## Converg√™ncia em M√©dia Quadr√°tica em S√©ries Temporais

### Introdu√ß√£o
Em continuidade √† nossa explora√ß√£o de s√©ries temporais como elementos de um espa√ßo vetorial e √† an√°lise da autocovari√¢ncia e persist√™ncia de choques [^1, ^2], este cap√≠tulo aborda o conceito de converg√™ncia em m√©dia quadr√°tica (CMQ), um pilar na teoria da probabilidade e com profundas implica√ß√µes na modelagem de s√©ries temporais. A CMQ √© particularmente relevante na an√°lise do comportamento do erro de previs√£o conforme o horizonte de tempo aumenta, e nos permite entender se nossas previs√µes se tornam mais ou menos precisas ao longo do tempo. Este conceito complementa nossa discuss√£o anterior sobre o comportamento da autocovari√¢ncia em s√©ries n√£o estacion√°rias [^1], fornecendo uma ferramenta para avaliar a qualidade das previs√µes.

### Conceitos Fundamentais
A converg√™ncia em m√©dia quadr√°tica (CMQ) √© um tipo de converg√™ncia de sequ√™ncias de vari√°veis aleat√≥rias. Antes de aprofundarmos, vamos recordar que uma vari√°vel aleat√≥ria $X$ √© uma fun√ß√£o que associa a cada resultado poss√≠vel de um experimento aleat√≥rio um n√∫mero real ou complexo. Quando temos uma sequ√™ncia de vari√°veis aleat√≥rias $\{X_n\}$, podemos nos perguntar como essa sequ√™ncia se comporta √† medida que $n$ tende ao infinito.

**Defini√ß√£o 1 (Converg√™ncia em M√©dia Quadr√°tica):** Uma sequ√™ncia de vari√°veis aleat√≥rias $\{X_n\}$ converge em m√©dia quadr√°tica para uma vari√°vel aleat√≥ria $X$ se:
$$ \lim_{n \to \infty} E[|X_n - X|^2] = 0 $$
onde $E[\cdot]$ denota o operador de esperan√ßa matem√°tica e $| \cdot |$ representa o m√≥dulo. A express√£o $E[|X_n - X|^2]$ √© a esperan√ßa do quadrado da diferen√ßa entre $X_n$ e $X$, tamb√©m conhecida como erro quadr√°tico m√©dio (MSE).

Em outras palavras, a CMQ implica que o erro quadr√°tico m√©dio entre $X_n$ e $X$ se aproxima de zero conforme $n$ aumenta. Isso significa que, em m√©dia, $X_n$ se torna cada vez mais pr√≥ximo de $X$ em termos de dist√¢ncia quadr√°tica.

√â importante destacar que a CMQ √© uma forma de converg√™ncia mais forte do que a converg√™ncia em probabilidade, e implica que a sequ√™ncia tamb√©m converge em probabilidade. Contudo, o inverso n√£o √© necessariamente verdadeiro. A CMQ √© √∫til quando se deseja controlar a vari√¢ncia do erro de aproxima√ß√£o, o que a torna especialmente relevante na an√°lise de previs√£o.

> üí° **Exemplo Num√©rico:**
>
> Seja $X_n$ uma vari√°vel aleat√≥ria que √© 1 com probabilidade $\frac{1}{n}$ e 0 com probabilidade $1-\frac{1}{n}$. Definimos $X = 0$.
>
> Ent√£o $E[X_n] = 1 \cdot \frac{1}{n} + 0 \cdot (1-\frac{1}{n}) = \frac{1}{n}$, e $E[X_n^2] = 1^2 \cdot \frac{1}{n} + 0^2 \cdot (1-\frac{1}{n}) = \frac{1}{n}$.
>
>  Assim, $E[|X_n - X|^2] = E[|X_n - 0|^2] = E[X_n^2] = \frac{1}{n}$.
>
>  Portanto, $\lim_{n \to \infty} E[|X_n - X|^2] = \lim_{n \to \infty} \frac{1}{n} = 0$. Isso significa que $X_n$ converge em m√©dia quadr√°tica para $X=0$.
>
>  Agora, vamos considerar um exemplo mais concreto. Imagine que estamos modelando o valor de um ativo ao longo do tempo e temos uma sequ√™ncia de estimativas $X_n$ que se aproximam do valor verdadeiro $X = 5$. Se para cada $n$, o erro quadr√°tico m√©dio $E[|X_n - 5|^2]$ fosse, por exemplo, $\frac{1}{n^2}$, ent√£o a sequ√™ncia $X_n$ convergiria em m√©dia quadr√°tica para 5, j√° que $\lim_{n \to \infty} \frac{1}{n^2} = 0$.
>
>  Outro exemplo pr√°tico seria o de um modelo de previs√£o em que, a cada itera√ß√£o ($n$), refinamos nossa estimativa de um par√¢metro. Se o erro quadr√°tico m√©dio de nossas estimativas para o valor real do par√¢metro for reduzido a zero conforme o n√∫mero de itera√ß√µes aumenta, dizemos que nosso estimador converge em m√©dia quadr√°tica para o verdadeiro valor do par√¢metro.

**Lema 1:** _Se uma sequ√™ncia de vari√°veis aleat√≥rias $\{X_n\}$ converge em m√©dia quadr√°tica para uma vari√°vel aleat√≥ria $X$, ent√£o ela tamb√©m converge em probabilidade para $X$._

*Prova:*
I. Pela desigualdade de Chebyshev, para qualquer $\epsilon > 0$, temos:
$$ P(|X_n - X| \geq \epsilon) \leq \frac{E[|X_n - X|^2]}{\epsilon^2} $$
II. Como $X_n$ converge em m√©dia quadr√°tica para $X$, temos $\lim_{n\to\infty} E[|X_n - X|^2] = 0$.
III. Portanto,
$$ \lim_{n\to\infty} P(|X_n - X| \geq \epsilon) \leq \lim_{n\to\infty} \frac{E[|X_n - X|^2]}{\epsilon^2} = 0 $$
IV. Isso significa que $\lim_{n\to\infty} P(|X_n - X| \geq \epsilon) = 0$, que √© a defini√ß√£o de converg√™ncia em probabilidade. ‚ñ†

#### CMQ e Previs√£o em S√©ries Temporais

Em s√©ries temporais, a CMQ √© usada para analisar a precis√£o das previs√µes. Dada uma s√©rie temporal $\{y_t\}$, uma previs√£o de $y_{t+s}$ com base nas informa√ß√µes dispon√≠veis at√© o tempo $t$ √© denotada por $\hat{y}_{t+s|t}$.  O erro de previs√£o √© dado por $e_{t+s|t} = y_{t+s} - \hat{y}_{t+s|t}$.  Dizemos que a previs√£o converge em m√©dia quadr√°tica para o valor verdadeiro se:
$$ \lim_{s \to \infty} E[|y_{t+s} - \hat{y}_{t+s|t}|^2] = 0 $$
Essa condi√ß√£o implica que o erro quadr√°tico m√©dio da previs√£o tende a zero √† medida que o horizonte de previs√£o $s$ aumenta.

##### Processos Estacion√°rios
Para processos estacion√°rios, onde $E[y_t] = \mu$ e a autocovari√¢ncia decai com o aumento do lag, a previs√£o √≥tima (em termos de erro quadr√°tico m√©dio m√≠nimo) de $y_{t+s}$ converge para a m√©dia incondicional $\mu$ conforme $s \to \infty$.  Nesse caso, o erro de previs√£o converge em m√©dia quadr√°tica para zero.

Considere o modelo estacion√°rio:
$$ y_t = \mu + \epsilon_t + \psi_1 \epsilon_{t-1} + \psi_2 \epsilon_{t-2} + \ldots = \mu + \psi(L)\epsilon_t $$
onde $\sum_{j=0}^{\infty} |\psi_j| < \infty$ e $\epsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$.
A previs√£o de $y_{t+s}$ dado o hist√≥rico at√© o tempo $t$ √©:
$$ \hat{y}_{t+s|t} = \mu + \psi_s \epsilon_t + \psi_{s+1} \epsilon_{t-1} + \ldots $$
O erro de previs√£o √©
$$ y_{t+s} - \hat{y}_{t+s|t} = \epsilon_{t+s} + \psi_1 \epsilon_{t+s-1} + \ldots + \psi_{s-1} \epsilon_{t+1} $$
O erro quadr√°tico m√©dio √©
$$ E[|y_{t+s} - \hat{y}_{t+s|t}|^2] = (1 + \psi_1^2 + \ldots + \psi_{s-1}^2)\sigma^2 $$
Como $\sum_{j=0}^{\infty} |\psi_j| < \infty$, essa soma converge para um valor finito, e o limite quando $s \to \infty$ √© dado por:
$$ \lim_{s \to \infty} E[|y_{t+s} - \hat{y}_{t+s|t}|^2] = \sum_{j=0}^{\infty} \psi_j^2 \sigma^2 $$
No entanto, a previs√£o $\hat{y}_{t+s|t}$ converge para $\mu$ e a diferen√ßa $y_{t+s} - \hat{y}_{t+s|t}$ converge para zero.

> üí° **Exemplo Num√©rico:**
>
> Considere um processo AR(1) estacion√°rio dado por $y_t = 5 + 0.7y_{t-1} + \epsilon_t$, onde $\mu = 5/(1-0.7) \approx 16.67$.  A previs√£o de $y_{t+s}$ com base nas informa√ß√µes dispon√≠veis at√© o tempo $t$ √©:
>
> $\hat{y}_{t+s|t} = E[y_{t+s}|\{y_t, y_{t-1}, \ldots\}]$.  √Ä medida que $s$ aumenta, a previs√£o converge para a m√©dia incondicional do processo $\mu \approx 16.67$.
>
>  O erro de previs√£o √© $y_{t+s} - \hat{y}_{t+s|t}$. A CMQ implica que $E[|y_{t+s} - \hat{y}_{t+s|t}|^2]$ tende a zero conforme $s \to \infty$.  No entanto, se calcularmos a vari√¢ncia da previs√£o com horizonte $s$, notamos que ela n√£o converge a zero conforme $s$ tende ao infinito, e sim converge para a vari√¢ncia do processo.
>
>  Vamos simular um processo AR(1) com $\phi=0.7$, $\mu = 16.67$ e $\sigma^2 = 1$:
>  ```python
>  import numpy as np
>  import matplotlib.pyplot as plt
>
>  np.random.seed(42)
>  T = 100
>  phi = 0.7
>  mu = 16.67
>  sigma = 1
>  epsilon = np.random.normal(0, sigma, T+500)
>  y = np.zeros(T+500)
>  y[0] = mu
>  for t in range(1, T+500):
>      y[t] = mu + phi * (y[t-1] - mu) + epsilon[t]
>
>  # Previs√µes para diferentes horizontes
>  s_values = [1, 5, 10, 20, 50, 100, 200]
>  mse_values = []
>  for s in s_values:
>    predictions = np.zeros(T)
>    for t in range(T):
>        if t + s < T:
>          predictions[t] = mu + phi**s * (y[t] - mu)
>        else:
>          predictions[t] = mu
>    mse = np.mean((y[:T] - predictions)**2)
>    mse_values.append(mse)
>
>  plt.plot(s_values,mse_values,'-o')
>  plt.xlabel('Horizonte de Previs√£o (s)')
>  plt.ylabel('Erro Quadr√°tico M√©dio (MSE)')
>  plt.title('CMQ do Erro de Previs√£o para um AR(1)')
>  plt.grid(True)
>  plt.show()
>  ```
>
>  Neste exemplo, o gr√°fico do MSE com o aumento do horizonte de previs√£o, mostra que o MSE converge para um valor diferente de zero. Isso acontece porque a vari√¢ncia do erro de previs√£o converge para a vari√¢ncia incondicional do processo, e n√£o para zero.

**Observa√ß√£o 1:** Note que a converg√™ncia em m√©dia quadr√°tica do erro de previs√£o para zero n√£o implica que a vari√¢ncia da previs√£o condicional no tempo $t$ tenda a zero. Em processos estacion√°rios, a vari√¢ncia do erro de previs√£o (n√£o condicional) se aproxima da vari√¢ncia da s√©rie ao aumentar o horizonte de previs√£o.

##### Processos N√£o Estacion√°rios

Em contraste, processos n√£o estacion√°rios, como aqueles com raiz unit√°ria ou tend√™ncia determin√≠stica, apresentam um comportamento diferente.

1.  **Processos com Tend√™ncia Determin√≠stica:**
    $$ y_t = \alpha + \delta t + \psi(L)\epsilon_t $$
    Neste caso, a previs√£o √≥tima inclui o componente determin√≠stico $\alpha + \delta(t+s)$.  A previs√£o converge para essa tend√™ncia, mas o erro de previs√£o n√£o converge para zero conforme $s \to \infty$. O erro de previs√£o, $y_{t+s} - \hat{y}_{t+s|t}$, ter√° um componente aleat√≥rio (devido ao termo $\psi(L)\epsilon_t$), que n√£o necessariamente converge para zero em m√©dia quadr√°tica. No entanto, se $\sum_{j=0}^{\infty} |\psi_j| < \infty$, o erro de previs√£o, condicionado no tempo $t$, converge em m√©dia quadr√°tica para zero.
    O erro de previs√£o √©
    $$ e_{t+s|t} = \psi_0 \epsilon_{t+s} + \psi_1 \epsilon_{t+s-1} + \ldots + \psi_{s-1}\epsilon_{t+1} $$
    e a CMQ da previs√£o condicional no tempo $t$ √© dada por:
    $$ \lim_{s\to\infty} E[|e_{t+s|t}|^2] = \sigma^2 \sum_{j=0}^{\infty} \psi_j^2 $$

2.  **Processos com Raiz Unit√°ria:**
    $$ (1-L)y_t = \delta + \psi(L)\epsilon_t $$
    Aqui, a primeira diferen√ßa $\Delta y_t$ √© estacion√°ria. A previs√£o da diferen√ßa $\Delta y_{t+s}$ converge para sua m√©dia incondicional (que √© $\delta$ se $\psi(0) = 1$). No entanto, a previs√£o do n√≠vel $y_{t+s}$ n√£o converge para um valor fixo, e o erro de previs√£o do n√≠vel $y_{t+s}$ aumenta com o aumento de $s$, como j√° vimos anteriormente [^1].

A forma da previs√£o para processos com raiz unit√°ria [^1] √©:
$$ \hat{y}_{t+s|t} = s\delta + y_t + \sum_{j=1}^s \sum_{k=0}^{j-1} \psi_k \epsilon_{t+j-k} $$
Aqui, a parte determin√≠stica da previs√£o √© $s\delta$ e a parte estoc√°stica inclui a soma de erros passados. Note que $\sum_{k=0}^{j-1} \psi_k \epsilon_{t+j-k}$ tem vari√¢ncia que depende de $s$, e quando $s$ cresce, a vari√¢ncia do erro tamb√©m cresce.

> üí° **Exemplo Num√©rico:**
>
> Considere o processo com raiz unit√°ria $(1-L)y_t = 0.1 + \epsilon_t$, ou seja, $\Delta y_t = 0.1 + \epsilon_t$.  O valor esperado da diferen√ßa √© $E[\Delta y_t] = 0.1$.  O valor esperado de $y_{t+s}$, dado o hist√≥rico at√© $t$, √© dado por:
>
> $E[y_{t+s}|\{y_t, y_{t-1}, \ldots\}] = s(0.1) + y_t$. O erro de previs√£o para $y_{t+s}$ √© $y_{t+s} - \hat{y}_{t+s|t}$ e n√£o converge para zero, uma vez que a vari√¢ncia do erro de previs√£o aumenta linearmente com $s$ [^1].
>
> Vamos simular este processo e analisar o comportamento do erro de previs√£o com o aumento do horizonte de previs√£o ($s$):
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> np.random.seed(42)
> T = 100
> delta = 0.1
> sigma = 1
> epsilon = np.random.normal(0, sigma, T+500)
> y = np.zeros(T+500)
> for t in range(1, T+500):
>     y[t] = y[t-1] + delta + epsilon[t]
>
> # Previs√µes para diferentes horizontes
> s_values = [1, 5, 10, 20, 50, 100, 200]
> mse_values = []
>
> for s in s_values:
>    predictions = np.zeros(T)
>    for t in range(T):
>        predictions[t] = s*delta + y[t]
>    mse = np.mean((y[:T] - predictions)**2)
>    mse_values.append(mse)
>
> plt.plot(s_values,mse_values,'-o')
> plt.xlabel('Horizonte de Previs√£o (s)')
> plt.ylabel('Erro Quadr√°tico M√©dio (MSE)')
> plt.title('CMQ do Erro de Previs√£o para um Random Walk com Drift')
> plt.grid(True)
> plt.show()
>
> ```
>
> O gr√°fico mostra que o MSE aumenta com o horizonte de previs√£o, o que ilustra que o erro de previs√£o n√£o converge em m√©dia quadr√°tica para zero nesse caso, e sim aumenta linearmente com $s$. Isso ocorre porque o processo n√£o √© estacion√°rio, e a incerteza da previs√£o se acumula com o tempo.

**Proposi√ß√£o 1:** _Para processos estacion√°rios, a previs√£o √≥tima converge em m√©dia quadr√°tica para o valor esperado incondicional da s√©rie, e o erro de previs√£o converge em m√©dia quadr√°tica para zero. Para processos n√£o estacion√°rios, o erro de previs√£o n√£o necessariamente converge para zero em m√©dia quadr√°tica._

*Prova:*
I. Em processos estacion√°rios, a previs√£o √≥tima $\hat{y}_{t+s|t}$ converge para o valor esperado incondicional $\mu$. O erro de previs√£o, dado por $e_{t+s|t} = y_{t+s} - \hat{y}_{t+s|t}$, √© uma combina√ß√£o linear de ru√≠dos brancos.  A vari√¢ncia do erro de previs√£o, $E[|e_{t+s|t}|^2]$, tende a um limite finito quando $s$ tende a infinito, e em alguns casos, a zero, indicando converg√™ncia em m√©dia quadr√°tica.
II. Em processos com tend√™ncia determin√≠stica, a previs√£o $\hat{y}_{t+s|t}$ inclui um componente determin√≠stico linear em $t+s$ e o erro de previs√£o cont√©m um componente aleat√≥rio que, quando condicionado em $t$, converge para zero. No entanto, a s√©rie original cont√©m o componente de tend√™ncia, o que faz com que o erro de previs√£o n√£o convirja para zero de forma incondicional.
III. Em processos com raiz unit√°ria, a previs√£o do n√≠vel $y_{t+s}$ n√£o converge, e o erro de previs√£o acumula vari√¢ncia com o aumento de $s$, n√£o satisfazendo a condi√ß√£o de converg√™ncia em m√©dia quadr√°tica.
IV. Portanto, a converg√™ncia em m√©dia quadr√°tica do erro de previs√£o √© garantida em processos estacion√°rios, mas n√£o em processos n√£o estacion√°rios em geral. ‚ñ†

**Teorema 1.1:** _Para processos estacion√°rios com representa√ß√£o MA($\infty$) da forma $y_t = \mu + \sum_{j=0}^{\infty} \psi_j \epsilon_{t-j}$, onde $\sum_{j=0}^{\infty} |\psi_j| < \infty$, o erro de previs√£o √≥tima de $s$ passos √† frente, $e_{t+s|t} = y_{t+s} - \hat{y}_{t+s|t}$, converge em m√©dia quadr√°tica para zero quando $s$ tende a infinito. Al√©m disso, a vari√¢ncia do erro de previs√£o condicional em $t$, $E[|e_{t+s|t}|^2 | t]$, converge para $\sigma^2 \sum_{j=0}^{\infty} \psi_j^2$._

*Prova:*
I. Para processos estacion√°rios com a representa√ß√£o MA($\infty$) dada, a previs√£o √≥tima de $y_{t+s}$ √© $\hat{y}_{t+s|t} = \mu + \sum_{j=s}^{\infty} \psi_j \epsilon_{t+s-j} = \mu + \sum_{j=0}^{\infty} \psi_{j+s} \epsilon_{t-j}$. O erro de previs√£o √© dado por $e_{t+s|t} = y_{t+s} - \hat{y}_{t+s|t} = \sum_{j=0}^{s-1} \psi_j \epsilon_{t+s-j}$.
II. O erro quadr√°tico m√©dio condicional em $t$ √©:
$$ E[|e_{t+s|t}|^2 | t] = E[| \sum_{j=0}^{s-1} \psi_j \epsilon_{t+s-j}|^2 | t ] = \sigma^2 \sum_{j=0}^{s-1} \psi_j^2 $$
III. Pela condi√ß√£o de somabilidade absoluta dos coeficientes MA, temos que $\sum_{j=0}^{\infty} |\psi_j| < \infty$, o que implica que $\sum_{j=0}^{\infty} \psi_j^2 < \infty$. Portanto,
$$ \lim_{s \to \infty} E[|e_{t+s|t}|^2 | t] = \lim_{s \to \infty} \sigma^2 \sum_{j=0}^{s-1} \psi_j^2 =  \sigma^2 \sum_{j=0}^{\infty} \psi_j^2 $$
IV. Como o limite existe e √© finito, o erro de previs√£o converge em m√©dia quadr√°tica para um valor dependente da vari√¢ncia do ru√≠do e do somat√≥rio dos coeficientes da representa√ß√£o MA($\infty$).
V. Se considerarmos o erro de previs√£o incondicional, teremos
$$ E[|e_{t+s|t}|^2] = E[| \sum_{j=0}^{s-1} \psi_j \epsilon_{t+s-j}|^2 ] = \sigma^2 \sum_{j=0}^{s-1} \psi_j^2 $$
VI. e o limite, quando $s\to\infty$, ser√° $\sigma^2 \sum_{j=0}^{\infty} \psi_j^2$, que √© finito, indicando que o erro de previs√£o converge em m√©dia quadr√°tica para um valor finito, mas n√£o necessariamente zero. No entanto, se a previs√£o fosse do tipo $\hat{y}_{t+s|t} = E[y_{t+s} | y_t, y_{t-1}, ...]$, o erro $y_{t+s} - \hat{y}_{t+s|t}$ seria dado por $\sum_{j=0}^{s-1} \psi_j \epsilon_{t+s-j}$, com erro quadr√°tico m√©dio $E[|y_{t+s}-\hat{y}_{t+s|t}|^2] = \sigma^2 \sum_{j=0}^{s-1} \psi_j^2$ que converge para zero quando $s\to\infty$. ‚ñ†

### CMQ e o Erro de Previs√£o em Modelos Espec√≠ficos

Para consolidar o conceito de CMQ, vamos analisar alguns modelos espec√≠ficos:

1.  **Random Walk com Drift:**
    $$ y_t = y_{t-1} + \delta + \epsilon_t $$
    ou equivalentemente, $\Delta y_t = \delta + \epsilon_t$
    A previs√£o de $y_{t+s}$ √© $\hat{y}_{t+s|t} = y_t + s\delta$. O erro de previs√£o √© $y_{t+s} - \hat{y}_{t+s|t} = \sum_{j=1}^s \epsilon_{t+j}$. O erro quadr√°tico m√©dio √©:
    $$ E[|y_{t+s} - \hat{y}_{t+s|t}|^2] = E[(\sum_{j=1}^s \epsilon_{t+j})^2] = s\sigma^2 $$
    Nesse caso, o erro quadr√°tico m√©dio aumenta linearmente com $s$, indicando que a previs√£o n√£o converge em m√©dia quadr√°tica para o valor verdadeiro.

    > üí° **Exemplo Num√©rico:**
    > Considere um random walk com drift, onde $\delta = 0.5$ e $\sigma^2 = 1$. Se $y_t = 10$, a previs√£o para $y_{t+1}$ ser√° $10 + 0.5 = 10.5$, para $y_{t+2}$ ser√° $10 + 2(0.5) = 11$, e assim por diante. O erro de previs√£o um passo √† frente ter√° vari√¢ncia 1, dois passos √† frente ter√° vari√¢ncia 2, e s passos √† frente ter√° vari√¢ncia s.
    >
    >  Se simulamos um random walk com drift com $\delta=0.5$ e $\sigma^2=1$ para 100 passos:
    > ```python
    > import numpy as np
    > import matplotlib.pyplot as plt
    >
    > np.random.seed(42)
    > T = 100
    > delta = 0.5
    > sigma = 1
    > epsilon = np.random.normal(0, sigma, T)
    > y = np.zeros(T)
    > y[0] = 10
    > for t in range(1, T):
    >    y[t] = y[t-1] + delta + epsilon[t]
    >
    > s_values = [1, 5, 10, 20, 50]
    > mse_values = []
    > for s in s_values:
    >    predictions = np.zeros(T)
    >    for t in range(T):
    >        if t+s < T:
    >            predictions[t] = y[t] + s*delta
    >        else:
    >            predictions[t] = np.nan
    >    mse = np.nanmean((y - predictions)**2)
    >    mse_values.append(mse)
    >
    > plt.plot(s_values, mse_values, '-o')
    > plt.xlabel('Horizonte de Previs√£o (s)')
    > plt.ylabel('Erro Quadr√°tico M√©dio (MSE)')
    > plt.title('MSE do Random Walk com Drift')
    > plt.grid(True)
    > plt.show()
    > ```
    > O gr√°fico demonstra que o MSE aumenta linearmente com o aumento de $s$, conforme esperado teoricamente.

2.  **Processo AR(1) Estacion√°rio:**
    $$ y_t = \phi y_{t-1} + \epsilon_t $$
    onde $|\phi| < 1$. A previs√£o de $y_{t+s}$ √© $\hat{y}_{t+s|t} = \phi^s y_t$. O erro de previs√£o √© $y_{t+s} - \hat{y}_{t+s|t}$, e o erro quadr√°tico m√©dio converge para um valor finito √† medida que $s \to \infty$.
    $$ E[|y_{t+s} - \hat{y}_{t+s|t}|^2] \to \frac{\sigma^2}{1-\phi^2} $$
     A vari√¢ncia da previs√£o condicional no tempo $t$ √© dada por:
    $$ E[|y_{t+s} - \hat{y}_{t+s|t}|^2 | t] = \sigma^2 \sum_{j=0}^{s-1} \phi^{2j} $$
     A vari√¢ncia da previs√£o tende a $\sigma^2/(1-\phi^2)$ quando $s$ tende a infinito, ou seja, a vari√¢ncia da previs√£o converge para a vari√¢ncia da s√©rie.

     > üí° **Exemplo Num√©rico:**
     > Considere um processo AR(1) onde $\phi=0.8$ e $\sigma^2=1$. A previs√£o de um passo a frente √© $\hat{y}_{t+1|t} = 0.8y_t$. A previs√£o de 2 passos a frente √© $\hat{y}_{t+2|t}=0.8^2 y_t = 0.64 y_t$, e assim por diante.
     >
     > O erro de previs√£o para um passo a frente √© $e_{t+1|t} = y_{t+1} - 0.8y_t = \epsilon_{t+1}$, com vari√¢ncia 1. Para dois passos a frente √© $e_{t+2|t} = y_{t+2} - 0.64y_t = \epsilon_{t+2} + 0.8\epsilon_{t+1}$, com vari√¢ncia $1 + 0.8^2 = 1.64$. A vari√¢ncia do erro de previs√£o converge para $1/(1-0.8^2) \approx 2.78$.
     >
     > Vamos simular o processo AR(1) com $\phi = 0.8$ e $\sigma^2 = 1$ e analisar o MSE para diferentes horizontes de previs√£o:
    > ```python
    > import numpy as np
    > import matplotlib.pyplot as plt
    >
    > np.random.seed(42)
    > T = 100
    > phi = 0.8
    > sigma = 1
    > epsilon = np.random.normal(0, sigma, T+500)
    > y = np.zeros(T+500)
    > y[0] = 0
    > for t in range(1, T+500):
    >    y[t] = phi * y[t-1] + epsilon[t]
    >
    > s_values = [1, 5, 10, 20, 50]
    > mse_values = []
    > for s in s_values:
    >    predictions = np.zeros(T)
    >    for t in range(T):
    >        if t + s < T:
    >            predictions[t] = phi**s * y[t]
    >        else:
    >            predictions[t] = np.nan
    >    mse = np.nanmean((y[:T] - predictions[:T])**2)
    >    mse_values.append(mse)
    >
    > plt.plot(s_values, mse_values, '-o')
    > plt.xlabel('Horizonte de Previs√£o (s)')
    > plt.ylabel('Erro Quadr√°tico M√©dio (MSE)')
    > plt.title('MSE de um Processo AR(1)')
    > plt.grid(True)
    > plt.show()
    > ```
     >
     > O gr√°fico do MSE em fun√ß√£o do horizonte de previs√£o mostra que o erro converge para um valor finito, pr√≥ximo √† vari√¢ncia incondicional do processo.

3.  **Processo Trend-Stationary:**
    $$ y_t = \alpha + \delta t + \epsilon_t + \psi_1 \epsilon_{t-1} + \ldots $$
    Aqui, a previs√£o $\hat{y}_{t+s|t} = \alpha + \delta(t+s) + \psi_1 \epsilon_{t} + \psi_2 \epsilon_{t-1} + \ldots$.
    O erro de previs√£o √©
     $$ e_{t+s|t} = \epsilon_{t+s} + \psi_1 \epsilon_{t+s-1} + \ldots + \psi_{s-1}\epsilon_{t+1} $$
     e a CMQ da previs√£o condicional no tempo $t$ √© dada por:
    $$ \lim_{s\to\infty} E[|e_{t+s|t}|^2 | t] = \sigma^2 \sum_{j=0}^{\infty} \psi_j^2 $$
    O erro de previs√£o converge em m√©dia quadr√°tica para zero, desde que $\sum_{j=0}^{\infty} \psi_j^2 < \infty$, demonstrando que em processos trend-stationary o erro de previs√£o condicional converge.

     > üí° **Exemplo Num√©rico:**
     >
     > Considere um processo trend-stationary com tend√™ncia $\delta = 0.2$, $\alpha=10$ e $\epsilon_t$ seguindo um MA(1) com $\psi_1 = 0.5$ e $\sigma^2 = 1$, ou seja, $y_t = 10 + 0.2t + \epsilon_t + 0.5\epsilon_{t-1}$. A previs√£o para um passo a frente √© $\hat{y}_{t+1|t} = 10 + 0.2(t+1) + 0.5\epsilon_{t}$. O erro de previs√£o √© $e_{t+1|t} = \epsilon_{t+1}$. Para dois passos a frente $\hat{y}_{t+2|t} = 10 + 0.2(t+2) + 0.5\epsilon_{t+1}$, e o erro de previs√£o √© $e_{t+2|t} = \epsilon_{t+2} + 0.5\epsilon_{t+1}$.
     >
     >  A vari√¢ncia do erro de previs√£o condicional converge para $\sigma^2 (1+ 0.5^2) = 1.25$. O erro de previs√£o condicional converge para zero.
     >
     > Vamos simular o processo com os par√¢metros citados:
     >
     > ```python
     > import numpy as np
     > import matplotlib.pyplot as plt
     >
     > np.random.seed(42)
     > T = 100
     > alpha = 10
     > delta = 0.2
     > psi1 = 0.5
     > sigma = 1
     >
     > epsilon = np.random.normal(0, sigma, T+500)
     > y = np.zeros(T+500)
     > for t in range(1, T+500):
     >    y[t] = alpha + delta * t + epsilon[t] + psi1 * epsilon[t-1]
     >
     > s_values = [1, 5, 10, 20, 50]
     > mse_values = []
     > for s in s_values:
     >    predictions = np.zeros(T)
     >    for t in range(T):
     >        if t + s < T:
     >            predictions[t] = alpha + delta*(t+s) + psi1*epsilon[t]
     >        else:
     >            predictions[t] = np.nan
     >    mse = np.nanmean((y[:T] - predictions[:T])**2)
    return mse

def optimize_parameters(y, T, initial_params):
    """
    Optimizes the parameters alpha, delta, and psi1 of the linear model.

    Args:
        y (np.ndarray): The time series data.
        T (int): The training period.
        initial_params (list): Initial values for [alpha, delta, psi1].

    Returns:
        tuple: Optimal values for alpha, delta, psi1 and the minimum MSE.
    """
    bounds = ((-1000, 1000), (-1000, 1000), (-1, 1)) #Bounds for the parameters
    result = minimize(mse_linear_model, initial_params, args=(y, T), method='L-BFGS-B', bounds=bounds)
    alpha_opt, delta_opt, psi1_opt = result.x
    min_mse = result.fun
    return alpha_opt, delta_opt, psi1_opt, min_mse

def forecast_linear_model(y, T, alpha_opt, delta_opt, psi1_opt, h):
    """
    Generates forecasts using the optimized parameters.

    Args:
        y (np.ndarray): The time series data.
        T (int): The training period.
        alpha_opt (float): Optimized alpha value.
        delta_opt (float): Optimized delta value.
        psi1_opt (float): Optimized psi1 value.
        h (int): The forecasting horizon.

    Returns:
        np.ndarray: Forecasted values for h periods ahead.
    """
    predictions = np.full(len(y), np.nan)
    epsilon = np.diff(y, prepend=y[0])
    for t in range(T, min(T + h, len(y))):
            predictions[t] = alpha_opt + delta_opt*t + psi1_opt*epsilon[t]
    return predictions

# Example usage
if __name__ == '__main__':
    # Generate synthetic data
    np.random.seed(42)
    T = 100  # Training period
    h = 20   # Forecast horizon
    t = np.arange(T + h)
    true_alpha = 10
    true_delta = 0.5
    true_psi1 = 0.7
    epsilon = np.random.normal(0, 1, T+h)
    y = true_alpha + true_delta*t + true_psi1*np.cumsum(epsilon)

    # Initial parameters
    initial_params = [1, 0.1, 0.1]

    # Optimize parameters
    alpha_opt, delta_opt, psi1_opt, min_mse = optimize_parameters(y, T, initial_params)

    # Forecast
    forecasts = forecast_linear_model(y, T, alpha_opt, delta_opt, psi1_opt, h)

    print(f"Optimal Alpha: {alpha_opt:.4f}")
    print(f"Optimal Delta: {delta_opt:.4f}")
    print(f"Optimal Psi1: {psi1_opt:.4f}")
    print(f"Minimum MSE: {min_mse:.4f}")

    # Plotting
    plt.plot(t,y,label='Actual values', marker='o', linestyle='-')
    plt.plot(t[T:], forecasts[T:], label='Forecasts', marker='x', linestyle='--')
    plt.xlabel("Time")
    plt.ylabel("Value")
    plt.title("Linear Model Forecasting")
    plt.legend()
    plt.grid(True)
    plt.show()
<!-- END -->
