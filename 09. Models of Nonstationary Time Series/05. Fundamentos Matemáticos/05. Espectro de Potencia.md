## AnÃ¡lise Espectral e a DecomposiÃ§Ã£o da VariÃ¢ncia em SÃ©ries Temporais

### IntroduÃ§Ã£o

Em continuidade Ã  nossa exploraÃ§Ã£o sobre modelos de sÃ©ries temporais nÃ£o estacionÃ¡rias, e apÃ³s discutirmos a autocovariÃ¢ncia, persistÃªncia de choques, convergÃªncia em mÃ©dia quadrÃ¡tica e as propriedades do espectro de potÃªncia [^1, ^2, ^3, ^4], este capÃ­tulo aborda a **anÃ¡lise espectral** como uma ferramenta crucial para decompor a variÃ¢ncia de uma sÃ©rie temporal em componentes de frequÃªncia. O espectro de potÃªncia, que Ã© a transformada de Fourier da funÃ§Ã£o de autocovariÃ¢ncia, permite-nos examinar a estrutura de dependÃªncia da sÃ©rie em termos de ciclos e frequÃªncias, oferecendo uma perspectiva alternativa e complementar Ã  anÃ¡lise no domÃ­nio do tempo. Essa anÃ¡lise Ã© particularmente Ãºtil para entender a natureza das oscilaÃ§Ãµes em sÃ©ries temporais e para distinguir entre processos estacionÃ¡rios e nÃ£o estacionÃ¡rios, complementando nossa discussÃ£o anterior sobre a persistÃªncia de choques e a distinÃ§Ã£o entre processos com raiz unitÃ¡ria e trend-stationary [^1].

### Conceitos Fundamentais

A anÃ¡lise espectral Ã© uma metodologia que permite decompor uma sÃ©rie temporal em componentes de diferentes frequÃªncias, revelando os padrÃµes oscilatÃ³rios presentes nos dados. Essa anÃ¡lise Ã© baseada no conceito de que qualquer sÃ©rie temporal pode ser representada como uma soma de componentes senoidais de diferentes frequÃªncias, amplitudes e fases. A principal ferramenta para essa decomposiÃ§Ã£o Ã© a **Transformada de Fourier**, que transforma uma funÃ§Ã£o no domÃ­nio do tempo para o domÃ­nio da frequÃªncia.

#### Transformada de Fourier

A Transformada de Fourier (TF) Ã© uma ferramenta matemÃ¡tica que decompÃµe uma funÃ§Ã£o em uma soma ponderada de componentes senoidais, cada uma com sua frequÃªncia, amplitude e fase. Para uma sÃ©rie temporal discreta $x_t$, a transformada de Fourier Ã© definida como:

$$ X(\omega) = \sum_{t=-\infty}^{\infty} x_t e^{-i\omega t} $$

onde $X(\omega)$ Ã© a transformada de Fourier de $x_t$, $\omega$ representa a frequÃªncia angular (em radianos), e $i$ Ã© a unidade imaginÃ¡ria. A funÃ§Ã£o $X(\omega)$ Ã© uma funÃ§Ã£o complexa que descreve a amplitude e a fase dos componentes senoidais em cada frequÃªncia.

A inversa da Transformada de Fourier (ITF) Ã© dada por:

$$ x_t = \frac{1}{2\pi} \int_{-\pi}^{\pi} X(\omega) e^{i\omega t} d\omega $$

que permite reconstruir a sÃ©rie temporal $x_t$ a partir de sua representaÃ§Ã£o na frequÃªncia.

Em aplicaÃ§Ãµes prÃ¡ticas, a transformada de Fourier Ã© aplicada a um nÃºmero finito de dados ($T$ amostras). A transformada de Fourier Discreta (DFT) Ã© definida como:

$$ X_k = \sum_{t=0}^{T-1} x_t e^{-i2\pi k t/T}, \quad k=0,1,\ldots,T-1 $$

onde $X_k$ representa a amplitude e fase dos componentes de frequÃªncia $\omega_k = 2\pi k/T$.

#### Espectro de PotÃªncia

O **espectro de potÃªncia**, ou densidade espectral de potÃªncia (PSD), Ã© o quadrado do mÃ³dulo da Transformada de Fourier, e representa a distribuiÃ§Ã£o da variÃ¢ncia da sÃ©rie em diferentes frequÃªncias:

$$ S(\omega) = \frac{1}{2\pi} |X(\omega)|^2 $$

Para uma sÃ©rie temporal estacionÃ¡ria com funÃ§Ã£o de autocovariÃ¢ncia $\gamma_k$, o espectro de potÃªncia tambÃ©m pode ser definido como a transformada de Fourier da funÃ§Ã£o de autocovariÃ¢ncia:

$$ S(\omega) = \frac{1}{2\pi} \sum_{k=-\infty}^{\infty} \gamma_k e^{-i\omega k} $$

A relaÃ§Ã£o entre o espectro de potÃªncia e a funÃ§Ã£o geradora de autocovariÃ¢ncia $g(z)$ Ã© dada por [^4]:
$$ S(\omega) = \frac{1}{2\pi} g(e^{-i\omega}) $$

O espectro de potÃªncia Ã© uma funÃ§Ã£o real e nÃ£o negativa, que descreve a distribuiÃ§Ã£o da potÃªncia (variÃ¢ncia) da sÃ©rie nas diferentes frequÃªncias. Em outras palavras, ele nos informa quais componentes de frequÃªncia contribuem mais para a variÃ¢ncia total da sÃ©rie. A Ã¡rea sob o espectro de potÃªncia integrada sobre todas as frequÃªncias Ã© igual Ã  variÃ¢ncia da sÃ©rie, como discutido anteriormente [^4]:
$$ \int_{-\pi}^{\pi} S(\omega) d\omega = \gamma_0 = Var(x_t) $$

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos considerar uma sÃ©rie temporal que consiste de uma onda senoidal com frequÃªncia $\omega_0$ e amplitude $A$, dada por $x_t = A \cos(\omega_0 t)$. A transformada de Fourier dessa sÃ©rie terÃ¡ picos em $\pm \omega_0$. O espectro de potÃªncia serÃ¡ concentrado nessas frequÃªncias, indicando que a maior parte da variÃ¢ncia da sÃ©rie estÃ¡ associada a essas oscilaÃ§Ãµes.
>
> Agora, vamos considerar a sÃ©rie $y_t = \cos(0.1t) + 0.5\cos(0.3t) + \epsilon_t$, onde $\epsilon_t$ Ã© ruÃ­do branco com $\sigma^2 = 0.1$.  Essa sÃ©rie Ã© composta por duas ondas senoidais de frequÃªncias diferentes (0.1 e 0.3 radianos), adicionadas a um ruÃ­do branco. A transformada de Fourier (e, consequentemente, o espectro de potÃªncia) dessa sÃ©rie terÃ¡ dois picos em torno de 0.1 e 0.3, alÃ©m de um nÃ­vel de base nÃ£o nulo devido ao ruÃ­do branco.
>
> Vamos simular esse processo e exibir o espectro:
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def autocovariance(x, k):
>    n = len(x)
>    if k >= n:
>        return 0
>    x_mean = np.mean(x)
>    cov = np.mean((x[:n-k] - x_mean) * (x[k:] - x_mean))
>    return cov
>
> def calculate_spectrum(x, lags):
>   gamma_k = np.array([autocovariance(x, k) for k in lags])
>   omega = np.linspace(-np.pi, np.pi, 100)
>   spectrum = np.zeros_like(omega, dtype=complex)
>   for k, gamma in zip(lags, gamma_k):
>        spectrum += gamma * np.exp(-1j * omega * k)
>   spectrum /= (2*np.pi)
>   return omega, spectrum
>
> np.random.seed(42)
> T = 500
> sigma = np.sqrt(0.1)
> epsilon = np.random.normal(0, sigma, T)
> t = np.arange(T)
> y = np.cos(0.1 * t) + 0.5 * np.cos(0.3*t) + epsilon
> lags = np.arange(-50, 51)
> omega, spectrum = calculate_spectrum(y, lags)
>
> plt.figure(figsize=(8, 6))
> plt.plot(omega, np.abs(spectrum))
> plt.title("Espectro de PotÃªncia de uma sÃ©rie com dois componentes senoidais")
> plt.xlabel("FrequÃªncia ($\\omega$)")
> plt.ylabel("Densidade Espectral")
> plt.grid(True)
> plt.show()
>
> ```
> O grÃ¡fico resultante exibirÃ¡ dois picos, correspondendo Ã s frequÃªncias das duas ondas senoidais, demonstrando como a anÃ¡lise espectral revela os componentes de frequÃªncia na sÃ©rie temporal.

#### DecomposiÃ§Ã£o da VariÃ¢ncia

A principal utilidade da anÃ¡lise espectral Ã© que ela nos permite **decompor a variÃ¢ncia** de uma sÃ©rie temporal em contribuiÃ§Ãµes de diferentes frequÃªncias. A potÃªncia do espectro em uma determinada frequÃªncia indica a quantidade de variÃ¢ncia da sÃ©rie associada Ã s oscilaÃ§Ãµes nessa frequÃªncia.

Para um processo estacionÃ¡rio, o espectro de potÃªncia Ã© uma funÃ§Ã£o nÃ£o negativa e finita, e a variÃ¢ncia total da sÃ©rie Ã© dada pela integral do espectro sobre todas as frequÃªncias. Por exemplo, em processos AR, a variÃ¢ncia pode ser expressa como:
$$Var(y_t) = \int_{-\pi}^{\pi} S(\omega) d\omega$$

Em termos da funÃ§Ã£o geradora de autocovariÃ¢ncia, essa integral Ã© equivalente a:
$$Var(y_t) = \gamma_0 = \frac{1}{2\pi} \int_{-\pi}^{\pi} g(e^{-i\omega}) d\omega $$

Em processos com raiz unitÃ¡ria, como o espectro de potÃªncia Ã© indefinido na frequÃªncia zero, a variÃ¢ncia da sÃ©rie original tende a infinito quando o horizonte temporal se aproxima do infinito, o que reflete a nÃ£o estacionariedade do processo. No entanto, o espectro da primeira diferenÃ§a $\Delta y_t$ Ã© bem definido e pode ser usado para decompor a variÃ¢ncia das mudanÃ§as na sÃ©rie original. A anÃ¡lise do espectro de $\Delta y_t$ tambÃ©m revela a persistÃªncia dos choques no processo, como visto anteriormente [^4].

A decomposiÃ§Ã£o da variÃ¢ncia usando o espectro de potÃªncia oferece insights valiosos sobre a estrutura de dependÃªncia e a persistÃªncia dos choques em sÃ©ries temporais, complementando nossa anÃ¡lise prÃ©via baseada na funÃ§Ã£o de autocovariÃ¢ncia e na convergÃªncia em mÃ©dia quadrÃ¡tica. A anÃ¡lise espectral permite identificar as frequÃªncias que contribuem mais para a variabilidade da sÃ©rie, o que Ã© crucial para a modelagem e previsÃ£o.

### AnÃ¡lise Espectral em Processos EstacionÃ¡rios e NÃ£o EstacionÃ¡rios

A anÃ¡lise espectral permite distinguir entre processos estacionÃ¡rios e nÃ£o estacionÃ¡rios com base nas caracterÃ­sticas de seus espectros de potÃªncia:

1.  **Processos EstacionÃ¡rios:**
    *   O espectro de potÃªncia Ã© nÃ£o negativo e finito em todas as frequÃªncias.
    *   A Ã¡rea sob o espectro representa a variÃ¢ncia total da sÃ©rie.
    *   Componentes cÃ­clicos sÃ£o identificados como picos no espectro nas frequÃªncias correspondentes.
    *   A persistÃªncia de choques, se presente, Ã© capturada pela concentraÃ§Ã£o de potÃªncia em baixas frequÃªncias (mas ainda finita no zero).

2.  **Processos com Raiz UnitÃ¡ria:**
    *   O espectro de potÃªncia Ã© indefinido na frequÃªncia zero, apresentando um pico ou singularidade nessa frequÃªncia.
    *   A maior parte da potÃªncia se concentra em baixas frequÃªncias, refletindo a persistÃªncia dos choques e a nÃ£o estacionariedade.
    *   A primeira diferenÃ§a, $\Delta y_t$, tem um espectro bem definido, que pode ser utilizado para analisar a variÃ¢ncia das mudanÃ§as na sÃ©rie.
    *   O espectro de $\Delta y_t$ Ã© positivo em frequÃªncia zero, indicando que os choques impactam o longo prazo.

3.  **Processos Trend-Stationary:**
    *   O espectro da sÃ©rie original Ã© bem definido, mas a sÃ©rie apresenta uma tendÃªncia.
    *   O espectro da sÃ©rie apÃ³s a remoÃ§Ã£o da tendÃªncia Ã© bem definido e finito.
     *   O espectro de $\Delta y_t$ Ã© nulo em frequÃªncia zero, o que reflete que, apÃ³s a diferenciaÃ§Ã£o, o impacto dos choques no longo prazo se dissipa.
    > ðŸ’¡ **Exemplo NumÃ©rico:**
   > Para ilustrar a diferenÃ§a nos espectros de processos estacionÃ¡rios e trend-stationary, vamos simular um processo AR(1) estacionÃ¡rio e um processo trend-stationary e comparar seus espectros.
   >
   > Um processo AR(1) Ã© dado por $y_t = \phi y_{t-1} + \epsilon_t$, onde $|\phi| < 1$ para garantir a estacionariedade. Vamos escolher $\phi = 0.7$ e $\epsilon_t$ como ruÃ­do branco com $\sigma^2 = 1$.
   >
   > Um processo trend-stationary pode ser simulado como $y_t = 0.1t + x_t$, onde $x_t$ segue um processo AR(1) como descrito acima. O termo $0.1t$ representa uma tendÃªncia linear.
   >
   > ```python
   > import numpy as np
   > import matplotlib.pyplot as plt
   > from scipy.signal import welch
   >
   > def simulate_ar1(T, phi, sigma):
   >   epsilon = np.random.normal(0, sigma, T)
   >   y = np.zeros(T)
   >   for t in range(1, T):
   >       y[t] = phi*y[t-1] + epsilon[t]
   >   return y
   >
   > def simulate_trend_stationary(T, phi, sigma, trend_coef):
   >   x = simulate_ar1(T, phi, sigma)
   >   t = np.arange(T)
   >   y = trend_coef * t + x
   >   return y
   >
   > np.random.seed(42)
   > T = 500
   > phi = 0.7
   > sigma = 1
   > trend_coef = 0.1
   >
   > y_ar1 = simulate_ar1(T, phi, sigma)
   > y_ts = simulate_trend_stationary(T, phi, sigma, trend_coef)
   >
   > # Compute the spectra
   > f_ar1, Pxx_ar1 = welch(y_ar1, window='hann', nperseg=256)
   > f_ts, Pxx_ts = welch(y_ts, window='hann', nperseg=256)
   >
   > # Plotting
   > plt.figure(figsize=(12, 6))
   >
   > plt.subplot(1, 2, 1)
   > plt.semilogy(f_ar1, Pxx_ar1)
   > plt.title("Espectro de PotÃªncia do AR(1) EstacionÃ¡rio")
   > plt.xlabel("FrequÃªncia (f)")
   > plt.ylabel("Densidade Espectral")
   > plt.grid(True)
   >
   > plt.subplot(1, 2, 2)
   > plt.semilogy(f_ts, Pxx_ts)
   > plt.title("Espectro de PotÃªncia do Trend-Stationary")
   > plt.xlabel("FrequÃªncia (f)")
   > plt.ylabel("Densidade Espectral")
   > plt.grid(True)
   >
   > plt.tight_layout()
   > plt.show()
   > ```
   >
   > O espectro do processo AR(1) estacionÃ¡rio mostrarÃ¡ uma distribuiÃ§Ã£o de potÃªncia finita, enquanto o espectro do processo trend-stationary mostrarÃ¡ um comportamento diferente, influenciado pela tendÃªncia linear, com maior potÃªncia nas frequÃªncias mais baixas. O espectro do processo trend-stationary apÃ³s a remoÃ§Ã£o da tendÃªncia (que pode ser feito, por exemplo, usando a primeira diferenÃ§a ou removendo a tendÃªncia estimada) seria similar ao do processo AR(1) estacionÃ¡rio, mas com a remoÃ§Ã£o de potÃªncia adicional em baixas frequÃªncias.

A anÃ¡lise do espectro de potÃªncia e a decomposiÃ§Ã£o da variÃ¢ncia fornecem uma visÃ£o complementar Ã  anÃ¡lise no domÃ­nio do tempo, ajudando a identificar padrÃµes oscilatÃ³rios, frequÃªncias relevantes e a natureza da persistÃªncia em sÃ©ries temporais.

**Teorema 1:** _Se $x_t$ Ã© uma sÃ©rie temporal estacionÃ¡ria com mÃ©dia zero e funÃ§Ã£o de autocovariÃ¢ncia $\gamma_k$, entÃ£o a variÃ¢ncia de $x_t$ Ã© dada por $\gamma_0$, e pode ser decomposta como a integral de seu espectro de potÃªncia sobre todas as frequÃªncias:_
$$Var(x_t) = \gamma_0 = \int_{-\pi}^{\pi} S(\omega) d\omega$$

*Prova:*
I. Sabemos que o espectro de potÃªncia Ã© dado por $S(\omega) = \frac{1}{2\pi} \sum_{k=-\infty}^{\infty} \gamma_k e^{-i\omega k}$.
II. Multiplicando ambos os lados por $e^{i\omega t}$ e integrando sobre o intervalo $[-\pi, \pi]$, temos:
   $$\int_{-\pi}^{\pi} S(\omega) e^{i\omega t} d\omega =  \int_{-\pi}^{\pi} \frac{1}{2\pi} \sum_{k=-\infty}^{\infty} \gamma_k e^{-i\omega k} e^{i\omega t} d\omega$$
III.  Trocando a ordem da integral e do somatÃ³rio:
    $$ \int_{-\pi}^{\pi} S(\omega) e^{i\omega t} d\omega =  \frac{1}{2\pi} \sum_{k=-\infty}^{\infty} \gamma_k \int_{-\pi}^{\pi} e^{-i\omega k} e^{i\omega t} d\omega$$
IV.  A integral no lado direito Ã© nula quando $k \neq t$ e igual a $2\pi$ quando $k = t$:
    $$ \int_{-\pi}^{\pi} e^{-i\omega k} e^{i\omega t} d\omega = \begin{cases} 0 & \text{se } k \neq t \\ 2\pi & \text{se } k = t \end{cases} $$
V.  Portanto, quando $t=0$:
    $$ \int_{-\pi}^{\pi} S(\omega) d\omega =  \frac{1}{2\pi} \sum_{k=-\infty}^{\infty} \gamma_k  \begin{cases} 0 & \text{se } k \neq 0 \\ 2\pi & \text{se } k = 0 \end{cases} = \gamma_0$$
VI.  Como $\gamma_0 = Cov(x_t, x_t) = Var(x_t)$, temos que:
    $$ Var(x_t) = \int_{-\pi}^{\pi} S(\omega) d\omega $$
    o que demonstra a decomposiÃ§Ã£o da variÃ¢ncia pelo espectro de potÃªncia. $\blacksquare$

**Teorema 1.1** _Se $x_t$ Ã© uma sÃ©rie temporal estacionÃ¡ria com mÃ©dia zero e espectro de potÃªncia $S(\omega)$, entÃ£o a funÃ§Ã£o de autocovariÃ¢ncia $\gamma_k$ pode ser recuperada atravÃ©s da transformada inversa de Fourier do espectro de potÃªncia:_

$$\gamma_k = \int_{-\pi}^{\pi} S(\omega) e^{i\omega k} d\omega$$

*Prova:*
I. Sabemos que o espectro de potÃªncia Ã© dado por $S(\omega) = \frac{1}{2\pi} \sum_{k=-\infty}^{\infty} \gamma_k e^{-i\omega k}$.
II. Multiplicando ambos os lados por $e^{i\omega j}$ e integrando sobre o intervalo $[-\pi, \pi]$, temos:
   $$\int_{-\pi}^{\pi} S(\omega) e^{i\omega j} d\omega =  \int_{-\pi}^{\pi} \frac{1}{2\pi} \sum_{k=-\infty}^{\infty} \gamma_k e^{-i\omega k} e^{i\omega j} d\omega$$
III.  Trocando a ordem da integral e do somatÃ³rio:
    $$ \int_{-\pi}^{\pi} S(\omega) e^{i\omega j} d\omega =  \frac{1}{2\pi} \sum_{k=-\infty}^{\infty} \gamma_k \int_{-\pi}^{\pi} e^{-i\omega k} e^{i\omega j} d\omega$$
IV. A integral no lado direito Ã© nula quando $k \neq j$ e igual a $2\pi$ quando $k = j$:
    $$ \int_{-\pi}^{\pi} e^{-i\omega k} e^{i\omega j} d\omega = \begin{cases} 0 & \text{se } k \neq j \\ 2\pi & \text{se } k = j \end{cases} $$
V. Portanto:
    $$ \int_{-\pi}^{\pi} S(\omega) e^{i\omega j} d\omega = \frac{1}{2\pi} \sum_{k=-\infty}^{\infty} \gamma_k \begin{cases} 0 & \text{se } k \neq j \\ 2\pi & \text{se } k = j \end{cases} = \gamma_j$$
VI.  Substituindo $j$ por $k$, temos que
$$ \gamma_k = \int_{-\pi}^{\pi} S(\omega) e^{i\omega k} d\omega $$
    o que demonstra que a funÃ§Ã£o de autocovariÃ¢ncia pode ser obtida a partir da transformada inversa de Fourier do espectro de potÃªncia. $\blacksquare$

### ImplementaÃ§Ã£o e Estimativa do Espectro

Em aplicaÃ§Ãµes prÃ¡ticas, o espectro de potÃªncia de uma sÃ©rie temporal Ã© frequentemente estimado a partir de uma amostra finita de dados. O estimador mais comum Ã© o **periodograma**, que Ã© definido como:

$$ \hat{S}(\omega) = \frac{1}{T} \left| \sum_{t=1}^{T} x_t e^{-i\omega t} \right|^2 $$

onde $T$ Ã© o tamanho da amostra. O periodograma Ã© uma estimativa consistente, mas possui uma alta variÃ¢ncia e pode apresentar picos espÃºrios devido Ã  natureza finita da amostra. Para mitigar esse problema, sÃ£o utilizadas tÃ©cnicas de suavizaÃ§Ã£o, como o uso de janelas espectrais ou mÃ©todos nÃ£o paramÃ©tricos.

O mÃ©todo de Welch, por exemplo, divide a sÃ©rie em segmentos sobrepostos e calcula o periodograma de cada segmento, posteriormente fazendo a mÃ©dia desses periodogramas. Isso resulta em uma estimativa mais suave e com menor variÃ¢ncia do espectro de potÃªncia.

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Vamos simular um processo AR(2) estacionÃ¡rio e um processo random walk e analisar seus espectros utilizando o mÃ©todo de Welch:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy.signal import welch
>
> def simulate_ar2(T, phi1, phi2, sigma):
>    epsilon = np.random.normal(0, sigma, T)
>    y = np.zeros(T)
>    for t in range(2, T):
>        y[t] = phi1*y[t-1] + phi2*y[t-2] + epsilon[t]
>    return y
>
> def simulate_random_walk(T, sigma):
>  epsilon = np.random.normal(0, sigma, T)
>  y = np.cumsum(epsilon)
>  return y
>
> np.random.seed(42)
> T = 1000
> sigma = 1
>
> # AR(2) parameters
> phi1 = 0.8
> phi2 = -0.3
>
> # Simulate the processes
> y_ar2 = simulate_ar2(T, phi1, phi2, sigma)
> y_rw = simulate_random_walk(T, sigma)
>
> # Estimate the Power Spectral Density using Welch's method
> f_ar2, Pxx_ar2 = welch(y_ar2, window='hann', nperseg=256)
> f_rw, Pxx_rw = welch(y_rw, window='hann', nperseg=256)
>
> # Plotting
> plt.figure(figsize=(12, 6))
>
> plt.subplot(1, 2, 1)
> plt.semilogy(f_ar2, Pxx_ar2)
> plt.title("Espectro de PotÃªncia do AR(2)")
> plt.xlabel("FrequÃªncia (f)")
> plt.ylabel("Densidade Espectral")
> plt.grid(True)
>
> plt.subplot(1, 2, 2)
> plt.semilogy(f_rw, Pxx_rw)
> plt.title("Espectro de PotÃªncia do Random Walk")
> plt.xlabel("FrequÃªncia (f)")
> plt.ylabel("Densidade Espectral")
> plt.grid(True)
>
> plt.tight_layout()
> plt.show()
> ```
> O cÃ³digo simula um processo AR(2) e um random walk, e calcula seus espectros usando o mÃ©todo de Welch. O grÃ¡fico resultante mostrarÃ¡ o espectro do AR(2) bem definido, com picos em frequÃªncias caracterÃ­sticas, e o espectro do random walk com um comportamento divergente em frequÃªncias baixas.

### ConclusÃ£o

Neste capÃ­tulo, exploramos a anÃ¡lise espectral como uma ferramenta poderosa para decompor a variÃ¢ncia de uma sÃ©rie temporal em componentes de frequÃªncia. Vimos como a Transformada de Fourier e o espectro de potÃªncia nos permitem analisar a estrutura de dependÃªncia da sÃ©rie, identificar ciclos, e distinguir entre processos estacionÃ¡rios e nÃ£o estacionÃ¡rios. Em particular, os processos com raiz unitÃ¡ria apresentam um espectro de potÃªncia indefinido na frequÃªncia zero, o que reflete a persistÃªncia dos choques e a nÃ£o estacionariedade da sÃ©rie original.

A anÃ¡lise espectral oferece uma visÃ£o complementar Ã  anÃ¡lise no domÃ­nio do tempo, e permite entender a natureza das oscilaÃ§Ãµes e da persistÃªncia em sÃ©ries temporais, complementando nossa discussÃ£o anterior sobre autocovariÃ¢ncia, convergÃªncia em mÃ©dia quadrÃ¡tica, e as propriedades do espectro [^1, ^2, ^3, ^4]. A compreensÃ£o desses conceitos Ã© fundamental para a modelagem e previsÃ£o de sÃ©ries temporais, e para a construÃ§Ã£o de modelos que capturem adequadamente as propriedades nÃ£o estacionÃ¡rias dos dados. A decomposiÃ§Ã£o da variÃ¢ncia utilizando o espectro de potÃªncia, e a anÃ¡lise de seu comportamento na frequÃªncia zero, sÃ£o ferramentas essenciais para a anÃ¡lise de sÃ©ries temporais com tendÃªncias e com memÃ³ria de longo prazo.

**ProposiÃ§Ã£o 1:** _Se $x_t$ Ã© uma sÃ©rie temporal com espectro de potÃªncia $S(\omega)$, entÃ£o o espectro da sÃ©rie $ax_t$, onde $a$ Ã© uma constante, Ã© dado por $a^2 S(\omega)$._

*Prova:*
I. Seja $X(\omega)$ a transformada de Fourier de $x_t$.
II. A transformada de Fourier de $ax_t$ Ã© $aX(\omega)$.
III. O espectro de potÃªncia de $ax_t$ Ã© dado por:
  $$S_{ax}(\omega) = \frac{1}{2\pi}|aX(\omega)|^2 = \frac{1}{2\pi}a^2|X(\omega)|^2 = a^2 \frac{1}{2\pi}|X(\omega)|^2 = a^2 S(\omega)$$
IV. Portanto, o espectro da sÃ©rie $ax_t$ Ã© $a^2 S(\omega)$. $\blacksquare$

### ReferÃªncias

[^1]: 15.1. Introduction
[^2]: 15.3. Comparison of Trend-Stationary and Unit Root Processes
[^3]: 15.2. Why Linear Time Trends and Unit Roots?
[^4]: 15.4. The Meaning of Tests for Unit Roots
<!-- END -->
