## O Conceito de Processo Integrado de Ordem 1 (I(1))

### IntroduÃ§Ã£o
Em continuidade Ã  discussÃ£o sobre modelos de sÃ©ries temporais nÃ£o estacionÃ¡rias e, especificamente, Ã  anÃ¡lise de raÃ­zes unitÃ¡rias e diferenciaÃ§Ã£o fracionÃ¡ria [^1], este capÃ­tulo foca no conceito de *processos integrados de ordem 1*, denotados por I(1). Esta classe de processos Ã© crucial na modelagem de sÃ©ries temporais que exibem uma tendÃªncia estocÃ¡stica, ou seja, uma trajetÃ³ria que nÃ£o Ã© previsÃ­vel com base em uma tendÃªncia determinÃ­stica, mas que precisa ser diferenciada uma vez para se tornar estacionÃ¡ria. A compreensÃ£o do conceito de I(1) Ã© fundamental para a modelagem e anÃ¡lise de muitas sÃ©ries econÃ´micas e financeiras.

### Conceitos Fundamentais
Como vimos anteriormente [^1], um processo Ã© considerado *estacionÃ¡rio* se sua mÃ©dia, variÃ¢ncia e autocovariÃ¢ncia nÃ£o variam ao longo do tempo. Muitos processos econÃ´micos e financeiros nÃ£o atendem a esta condiÃ§Ã£o, exibindo tendÃªncias e comportamentos nÃ£o estacionÃ¡rios [^1]. A modelagem desses processos muitas vezes envolve a aplicaÃ§Ã£o de transformaÃ§Ãµes, como a diferenciaÃ§Ã£o, para tornÃ¡-los estacionÃ¡rios.

Um processo *integrado de ordem 1*, ou I(1), Ã© um tipo especÃ­fico de processo nÃ£o estacionÃ¡rio que, ao ser diferenciado uma vez, torna-se um processo estacionÃ¡rio [^1].  Em outras palavras, se $y_t$ Ã© um processo I(1), entÃ£o $\Delta y_t = y_t - y_{t-1}$ Ã© um processo estacionÃ¡rio. A notaÃ§Ã£o $\Delta$ representa o operador de primeira diferenÃ§a, ou $(1-L)$ [^1].

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um processo de passeio aleatÃ³rio com deriva (random walk with drift):  $y_t = \delta + y_{t-1} + \epsilon_t$, onde $\epsilon_t$ Ã© um ruÃ­do branco. Este processo Ã© nÃ£o estacionÃ¡rio porque a sua variÃ¢ncia aumenta com o tempo e a sua mÃ©dia tambÃ©m depende do tempo. No entanto, ao aplicar o operador de primeira diferenÃ§a, temos $\Delta y_t = y_t - y_{t-1} = \delta + \epsilon_t$, que Ã© um processo estacionÃ¡rio, pois tem mÃ©dia constante ($\delta$) e variÃ¢ncia constante ($\sigma^2$). Portanto, o processo original $y_t$ Ã© um processo I(1).
>
> Para ilustrar com nÃºmeros, vamos supor que $\delta = 0.1$ e $\epsilon_t$ Ã© um ruÃ­do branco com mÃ©dia 0 e variÃ¢ncia 1. Geramos 100 pontos de dados para $y_t$, comeÃ§ando de $y_0 = 0$.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> np.random.seed(42) # For reproducibility
> delta = 0.1
> num_points = 100
> epsilon = np.random.normal(0, 1, num_points)
> y = np.zeros(num_points)
> y[0] = 0
> for t in range(1, num_points):
>     y[t] = delta + y[t-1] + epsilon[t]
>
> plt.figure(figsize=(10,5))
> plt.plot(y)
> plt.title('Processo I(1): Passeio AleatÃ³rio com Deriva')
> plt.xlabel('Tempo')
> plt.ylabel('Valor de y_t')
> plt.show()
>
> dy = np.diff(y)
> plt.figure(figsize=(10,5))
> plt.plot(dy)
> plt.title('Primeira DiferenÃ§a do Processo I(1): dy_t')
> plt.xlabel('Tempo')
> plt.ylabel('Valor de dy_t')
> plt.show()
>
> print(f'MÃ©dia de y_t: {np.mean(y):.2f}')
> print(f'VariÃ¢ncia de y_t: {np.var(y):.2f}')
> print(f'MÃ©dia de dy_t: {np.mean(dy):.2f}')
> print(f'VariÃ¢ncia de dy_t: {np.var(dy):.2f}')
> ```
>
> A plotagem da sÃ©rie original $y_t$ demonstra uma tendÃªncia crescente (estocÃ¡stica), confirmando a sua nÃ£o estacionariedade. Em contrapartida, o grÃ¡fico de $\Delta y_t$  exibe uma trajetÃ³ria sem tendÃªncia e com variabilidade constante, indicando sua estacionariedade. A mÃ©dia e a variÃ¢ncia de y_t nÃ£o sÃ£o constantes, enquanto a de dy_t o sÃ£o.

O conceito de I(1) estÃ¡ intimamente relacionado com a ideia de *raiz unitÃ¡ria* [^1]. Um processo possui uma raiz unitÃ¡ria quando o polinÃ´mio caracterÃ­stico do operador autoregressivo (AR) tem pelo menos uma raiz igual a 1 [^1].  Como vimos, um modelo AR(p) pode ser expresso como:

$$ (1 - \phi_1 L - \phi_2 L^2 - \ldots - \phi_p L^p)y_t = \epsilon_t $$

ou equivalentemente como $\phi(L)y_t = \epsilon_t$.
Se o polinÃ´mio $\phi(L)$ tem uma raiz unitÃ¡ria, entÃ£o podemos fatorÃ¡-lo como:

$$ \phi(L) = (1-L)(1-\lambda_2 L) \ldots (1-\lambda_p L) $$

onde $\lambda_2, \ldots, \lambda_p$ sÃ£o as outras raÃ­zes do polinÃ´mio.  Nesse caso, a sÃ©rie temporal $y_t$ Ã© um processo I(1) porque, ao aplicar o operador de primeira diferenÃ§a $(1-L)$, obtemos:

$$ (1-\lambda_2 L) \ldots (1-\lambda_p L)\Delta y_t = \epsilon_t $$
que Ã© um processo estacionÃ¡rio.

A representaÃ§Ã£o geral de um processo I(1) pode ser escrita como [^1]:

$$ (1-L)y_t = \delta + \psi(L)\epsilon_t $$

onde $\delta$ Ã© uma constante (a deriva) e $\psi(L)$ Ã© um operador de mÃ©dia mÃ³vel (MA).  Se $\delta=0$, o processo Ã© um passeio aleatÃ³rio.  Se $\delta \ne 0$, o processo tem uma deriva e sua mÃ©dia varia ao longo do tempo.

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que temos o modelo $y_t = 1.3y_{t-1} - 0.3y_{t-2} + \epsilon_t$. O polinÃ´mio caracterÃ­stico Ã© $1 - 1.3L + 0.3L^2 = (1-L)(1-0.3L)=0$. A raiz $L=1$ indica a presenÃ§a de uma raiz unitÃ¡ria. A raiz $L = 1/0.3$ Ã© estÃ¡vel. Assim, o processo original $y_t$ Ã© um processo I(1) e $\Delta y_t = y_t - y_{t-1}$ serÃ¡ estacionÃ¡rio, obedecendo a um modelo AR(1), $(1 - 0.3L) \Delta y_t = \epsilon_t$.
>
> Para demonstrar este caso, vamos gerar uma sÃ©rie temporal com este modelo e depois diferenciÃ¡-la.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> np.random.seed(42)
> num_points = 100
> epsilon = np.random.normal(0, 1, num_points)
> y = np.zeros(num_points)
> y[0] = 0
> y[1] = 0
> for t in range(2, num_points):
>   y[t] = 1.3*y[t-1] - 0.3*y[t-2] + epsilon[t]
>
> plt.figure(figsize=(10,5))
> plt.plot(y)
> plt.title('Processo I(1) com AR(2)')
> plt.xlabel('Tempo')
> plt.ylabel('Valor de y_t')
> plt.show()
>
> dy = np.diff(y)
> plt.figure(figsize=(10,5))
> plt.plot(dy)
> plt.title('Primeira DiferenÃ§a do Processo I(1)')
> plt.xlabel('Tempo')
> plt.ylabel('Valor de dy_t')
> plt.show()
>
> print(f'MÃ©dia de y_t: {np.mean(y):.2f}')
> print(f'VariÃ¢ncia de y_t: {np.var(y):.2f}')
> print(f'MÃ©dia de dy_t: {np.mean(dy):.2f}')
> print(f'VariÃ¢ncia de dy_t: {np.var(dy):.2f}')
> ```
> A plotagem da sÃ©rie $y_t$ revela um comportamento nÃ£o estacionÃ¡rio, com oscilaÃ§Ãµes cuja amplitude aumenta ao longo do tempo. ApÃ³s a diferenciaÃ§Ã£o, $\Delta y_t$ exibe uma trajetÃ³ria estacionÃ¡ria.

Ã‰ importante notar que a presenÃ§a de uma raiz unitÃ¡ria implica que choques passados tÃªm efeitos permanentes sobre o nÃ­vel da sÃ©rie [^1]. Em contraste, em um processo estacionÃ¡rio, o efeito dos choques passados diminui com o tempo. Em um processo I(1), a sÃ©rie nÃ£o converge para uma mÃ©dia constante, mas acumula os efeitos dos choques ao longo do tempo [^1].

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere dois processos: (a) $y_t = 0.8y_{t-1} + \epsilon_t$ e (b) $y_t = y_{t-1} + \epsilon_t$. No processo (a), que Ã© estacionÃ¡rio, um choque $\epsilon_t$ afeta $y_t$, mas o efeito desse choque diminui exponencialmente com o tempo (o efeito em $y_{t+n}$ serÃ¡ $0.8^n \epsilon_t$). No processo (b), que Ã© I(1), o choque $\epsilon_t$ afeta $y_t$, e o efeito desse choque permanece constante ao longo do tempo (o efeito em $y_{t+n}$ serÃ¡ $\epsilon_t$ para todo $n>0$).
>
> Vamos simular os dois processos e analisar o efeito de um choque Ãºnico. Suponha que um choque de tamanho 1 ocorre no tempo t=5.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> np.random.seed(42)
> num_points = 20
> epsilon_a = np.random.normal(0, 1, num_points)
> epsilon_b = np.random.normal(0, 1, num_points)
>
> # Choque no tempo 5
> epsilon_a[5] = 1
> epsilon_b[5] = 1
>
> y_a = np.zeros(num_points)
> y_b = np.zeros(num_points)
>
> for t in range(1, num_points):
>  y_a[t] = 0.8 * y_a[t-1] + epsilon_a[t]
>  y_b[t] = y_b[t-1] + epsilon_b[t]
>
> plt.figure(figsize=(10,5))
> plt.plot(y_a, label = 'Processo EstacionÃ¡rio')
> plt.plot(y_b, label = 'Processo I(1)')
> plt.axvline(x=5, color='r', linestyle='--', label='Choque')
> plt.title('Efeito de um Choque em Processos EstacionÃ¡rios e I(1)')
> plt.xlabel('Tempo')
> plt.ylabel('Valor de y_t')
> plt.legend()
> plt.show()
> ```
> A plotagem ilustra como o choque de tamanho 1 no tempo t=5 afeta os dois processos. No processo estacionÃ¡rio, o efeito do choque diminui com o tempo, enquanto no processo I(1), o efeito do choque Ã© permanente, deslocando o nÃ­vel da sÃ©rie.

A distinÃ§Ã£o entre processos I(1) e processos *trend-stationary* Ã© crucial. Em um processo *trend-stationary*, a sÃ©rie tem uma tendÃªncia determinÃ­stica (por exemplo, uma linha reta) em torno da qual a sÃ©rie flutua [^1]. Ao remover a tendÃªncia, o que resta Ã© um processo estacionÃ¡rio.  Em um processo I(1), a sÃ©rie nÃ£o tem uma tendÃªncia determinÃ­stica, mas tem uma tendÃªncia estocÃ¡stica, que sÃ³ Ã© removida aplicando a diferenciaÃ§Ã£o [^1].

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que temos duas sÃ©ries:
> (a) *trend-stationary*: $y_t = 2 + 0.5t + u_t$, onde $u_t$ Ã© um processo AR(1) estacionÃ¡rio.
> (b) *I(1)*: $y_t = y_{t-1} + \epsilon_t$.
>
> No caso (a), podemos remover a tendÃªncia (2 + 0.5t) para obter um processo estacionÃ¡rio ($u_t$).  No caso (b), precisamos aplicar o operador de primeira diferenÃ§a, $\Delta y_t = \epsilon_t$, para obter um processo estacionÃ¡rio.
>
> Vamos gerar e visualizar ambas as sÃ©ries:
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> np.random.seed(42)
> num_points = 100
> t = np.arange(num_points)
> epsilon_a = np.random.normal(0, 1, num_points)
> epsilon_b = np.random.normal(0, 1, num_points)
> u_t = np.zeros(num_points)
>
> for i in range(1,num_points):
>  u_t[i] = 0.5*u_t[i-1] + epsilon_a[i]
>
> y_a = 2 + 0.5*t + u_t
> y_b = np.zeros(num_points)
> for i in range(1, num_points):
>  y_b[i] = y_b[i-1] + epsilon_b[i]
>
> plt.figure(figsize=(10,5))
> plt.plot(y_a, label='Trend-Stationary')
> plt.plot(y_b, label='I(1)')
> plt.title('ComparaÃ§Ã£o entre Processos Trend-Stationary e I(1)')
> plt.xlabel('Tempo')
> plt.ylabel('Valor de y_t')
> plt.legend()
> plt.show()
>
> plt.figure(figsize=(10,5))
> plt.plot(y_a - (2 + 0.5*t), label = 'Removendo a tendÃªncia de y_a')
> plt.plot(np.diff(y_b), label='Primeira diferenÃ§a de y_b')
> plt.title('Processos Transformados (EstacionÃ¡rios)')
> plt.xlabel('Tempo')
> plt.ylabel('Valor de y_t')
> plt.legend()
> plt.show()
>
> print(f"MÃ©dia da sÃ©rie trend-stationary original: {np.mean(y_a):.2f}")
> print(f"VariÃ¢ncia da sÃ©rie trend-stationary original: {np.var(y_a):.2f}")
> print(f"MÃ©dia da sÃ©rie I(1) original: {np.mean(y_b):.2f}")
> print(f"VariÃ¢ncia da sÃ©rie I(1) original: {np.var(y_b):.2f}")
> print(f"MÃ©dia da sÃ©rie trend-stationary apÃ³s remoÃ§Ã£o da tendÃªncia: {np.mean(y_a - (2 + 0.5*t)):.2f}")
> print(f"VariÃ¢ncia da sÃ©rie trend-stationary apÃ³s remoÃ§Ã£o da tendÃªncia: {np.var(y_a - (2 + 0.5*t)):.2f}")
> print(f"MÃ©dia da primeira diferenÃ§a da sÃ©rie I(1): {np.mean(np.diff(y_b)):.2f}")
> print(f"VariÃ¢ncia da primeira diferenÃ§a da sÃ©rie I(1): {np.var(np.diff(y_b)):.2f}")
>
> ```
> A plotagem das sÃ©ries originais demonstra a diferenÃ§a entre uma sÃ©rie com tendÃªncia determinÃ­stica (trend-stationary) e uma sÃ©rie com tendÃªncia estocÃ¡stica (I(1)). A remoÃ§Ã£o da tendÃªncia da sÃ©rie trend-stationary e a diferenciaÃ§Ã£o da sÃ©rie I(1) a transforma em processos estacionÃ¡rios.

**Lema 1**
Um processo I(1) Ã© um caso particular de um processo com raiz unitÃ¡ria, onde o polinÃ´mio caracterÃ­stico do modelo AR possui uma raiz igual a 1, ou seja, $\lambda_1 = 1$.

*DemonstraÃ§Ã£o:*
I.   Um processo com raiz unitÃ¡ria Ã© definido como um processo onde o polinÃ´mio caracterÃ­stico $\phi(z) = 1 - \phi_1 z - \phi_2 z^2 - \ldots - \phi_p z^p = 0$ tem pelo menos uma raiz igual a 1.
II.  Um processo I(1) Ã© definido como um processo que, apÃ³s ser diferenciado uma vez, se torna estacionÃ¡rio.
III. Se um processo $y_t$ Ã© I(1), entÃ£o $(1-L)y_t = \Delta y_t$ Ã© estacionÃ¡rio.
IV.  A relaÃ§Ã£o entre as raÃ­zes do polinÃ´mio caracterÃ­stico e a estacionariedade de um processo AR(p) Ã© dada pela fatoraÃ§Ã£o do operador $\phi(L) = (1-\lambda_1 L)(1-\lambda_2 L) \ldots (1-\lambda_p L)$ onde $\lambda_i$ sÃ£o as raÃ­zes. Se qualquer das raÃ­zes, $\lambda_i$ for igual a 1,  entÃ£o o processo Ã© nÃ£o-estacionÃ¡rio, e  precisa ser diferenciado para se tornar estacionÃ¡rio.
V.  Se o polinÃ´mio caracterÃ­stico tem uma raiz igual a 1, entÃ£o ele pode ser fatorado como $\phi(L) = (1-L)(1-\lambda_2 L) \ldots (1-\lambda_p L)$, o que implica que o processo original $y_t$ Ã© um processo I(1), pois $(1-L)y_t$ Ã© estacionÃ¡rio.
VI. Portanto, um processo I(1) Ã© um caso particular de processo com raiz unitÃ¡ria onde $\lambda_1 = 1$, completando a prova. $\blacksquare$

**Teorema 1.1**
Se $y_t$ Ã© um processo I(1) e $\Delta y_t$ Ã© estacionÃ¡rio, entÃ£o a variÃ¢ncia de $y_t$ cresce linearmente com o tempo.

*DemonstraÃ§Ã£o:*
I.  Um processo I(1) pode ser expresso como $y_t = \sum_{i=1}^t \Delta y_i$, onde $\Delta y_i$ sÃ£o os incrementos da sÃ©rie.
II.   Como $\Delta y_t$ Ã© estacionÃ¡rio, a sua variÃ¢ncia $\text{Var}(\Delta y_t) = \sigma^2$ Ã© constante.
III. Assumindo que os incrementos $\Delta y_i$ sÃ£o nÃ£o correlacionados (o que Ã© razoÃ¡vel se $\Delta y_t$ Ã© um ruÃ­do branco ou um processo estacionÃ¡rio), a variÃ¢ncia de $y_t$ pode ser expressa como:
    $$ \text{Var}(y_t) = \text{Var}(\sum_{i=1}^t \Delta y_i) = \sum_{i=1}^t \text{Var}(\Delta y_i) $$
IV. Dado que $\text{Var}(\Delta y_i) = \sigma^2$ para todo $i$, temos:
$$ \text{Var}(y_t) = \sum_{i=1}^t \sigma^2 = t\sigma^2 $$
V. Isso mostra que a variÃ¢ncia de $y_t$ cresce linearmente com o tempo, o que Ã© uma caracterÃ­stica de processos nÃ£o estacionÃ¡rios com raiz unitÃ¡ria, completando a prova.  $\blacksquare$
> ğŸ’¡ **Exemplo NumÃ©rico:** Vamos simular um processo I(1) e calcular a sua variÃ¢ncia ao longo do tempo.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> np.random.seed(42)
> num_points = 100
> epsilon = np.random.normal(0, 1, num_points)
> y = np.zeros(num_points)
> for t in range(1, num_points):
>  y[t] = y[t-1] + epsilon[t]
>
> variances = [np.var(y[:t]) for t in range(2, num_points)]
> plt.figure(figsize=(10,5))
> plt.plot(range(2, num_points), variances)
> plt.title('VariÃ¢ncia de um Processo I(1) ao Longo do Tempo')
> plt.xlabel('Tempo')
> plt.ylabel('VariÃ¢ncia de y_t')
> plt.show()
>
> # Linear Regression to check the growth
> from sklearn.linear_model import LinearRegression
> time = np.array(range(2, num_points)).reshape(-1,1)
> model = LinearRegression().fit(time, variances)
> print(f"Coeficiente da regressÃ£o linear: {model.coef_[0]:.4f}")
> ```
> A plotagem mostra que a variÃ¢ncia da sÃ©rie I(1) cresce de forma aproximadamente linear com o tempo. O coeficiente da regressÃ£o linear quantifica a taxa de crescimento da variÃ¢ncia ao longo do tempo.

Este teorema demonstra que a variÃ¢ncia de um processo I(1) nÃ£o Ã© constante ao longo do tempo, mas cresce linearmente com o tempo. Essa caracterÃ­stica Ã© uma das formas de distinguir processos I(1) de processos estacionÃ¡rios ou *trend-stationary*, nos quais a variÃ¢ncia Ã© constante ou, no caso de *trend-stationary*, o desvio em relaÃ§Ã£o Ã  tendÃªncia Ã© constante.

**ProposiÃ§Ã£o 1**
Se $y_t$ Ã© um processo I(1) com representaÃ§Ã£o $(1-L)y_t = \delta + \psi(L)\epsilon_t$ onde $\epsilon_t$ Ã© um ruÃ­do branco com variÃ¢ncia $\sigma^2$, entÃ£o a autocovariÃ¢ncia de $y_t$ para lags grandes $(k \rightarrow \infty)$ Ã© aproximadamente proporcional a $t$.

*DemonstraÃ§Ã£o:*
I.   Para um processo I(1), a autocovariÃ¢ncia de $y_t$ Ã© dada por $\text{Cov}(y_t, y_{t-k})$.
II.  Sabemos que $y_t = \sum_{i=1}^t \Delta y_i$, onde $\Delta y_i$ representa os incrementos estacionÃ¡rios.
III.  Se $y_t$ Ã© um processo I(1), $(1-L)y_t = \Delta y_t$ Ã© estacionÃ¡rio, podendo ser modelado como $\Delta y_t = \delta + \psi(L)\epsilon_t$.
IV. Para lags grandes, $k$,  $\text{Cov}(y_t, y_{t-k})$ aproxima-se do termo que descreve a acumulaÃ§Ã£o de choques no tempo.
V.  Podemos escrever $y_t = \sum_{i=1}^{t} (\delta + \sum_{j=0}^{\infty} \psi_j \epsilon_{i-j})$ e $y_{t-k} = \sum_{i=1}^{t-k} (\delta + \sum_{j=0}^{\infty} \psi_j \epsilon_{i-j})$, onde $\psi_j$ sÃ£o os coeficientes do operador $\psi(L)$.
VI.  Para $k$ grande, as sÃ©ries $y_t$ e $y_{t-k}$ sÃ£o mais influenciadas pela acumulaÃ§Ã£o de choques passados que por covariÃ¢ncias internas, entÃ£o a covariÃ¢ncia $\text{Cov}(y_t, y_{t-k})$ Ã© aproximada pela variÃ¢ncia comum aos dois somatÃ³rios, que cresce linearmente com o tempo.
VII.  Como os choques $\epsilon_t$ sÃ£o independentes, a autocovariÃ¢ncia para $k$ grande serÃ¡ dominada pelo termo associado ao crescimento da variÃ¢ncia.
VIII. Assim,  $\text{Cov}(y_t, y_{t-k})$ serÃ¡ aproximadamente proporcional a $t$ quando $k$ Ã© grande. $\blacksquare$
> ğŸ’¡ **Exemplo NumÃ©rico:** Vamos simular um processo I(1) e verificar como a autocovariÃ¢ncia varia com o lag e o tempo.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> np.random.seed(42)
> num_points = 100
> epsilon = np.random.normal(0, 1, num_points)
> y = np.zeros(num_points)
> for t in range(1, num_points):
>     y[t] = y[t-1] + epsilon[t]
>
> # Calcular as autocovariÃ¢ncias para diferentes lags e tempos
> max_lag = 10
> autocovariances = np.zeros((num_points - max_lag, max_lag))
>
> for t in range(max_lag, num_points):
>  for k in range(max_lag):
>    autocovariances[t-max_lag,k] = np.cov(y[t-max_lag:t], y[t-max_lag-k:t-k])[0,1]
>
> # Visualizar o comportamento da autocovariÃ¢ncia em diferentes lags e tempo
> plt.figure(figsize=(10,6))
> for lag in range(max_lag):
>    plt.plot(range(max_lag,num_points), autocovariances[:,lag], label=f"Lag {lag+1}")
> plt.title('AutocovariÃ¢ncia de um Processo I(1) com Diferentes Lags ao Longo do Tempo')
> plt.xlabel('Tempo')
> plt.ylabel('AutocovariÃ¢ncia')
> plt.legend()
> plt.show()
>
> # Observe o comportamento da autocovariÃ¢ncia para um lag fixo (lag = 5)
> plt.figure(figsize=(10,6))
> plt.plot(range(max_lag, num_points), autocovariances[:, 4], label = 'AutocovariÃ¢ncia para lag=5')
> plt.title('AutocovariÃ¢ncia de um Processo I(1) com Lag 5 ao Longo do Tempo')
> plt.xlabel('Tempo')
> plt.ylabel('AutocovariÃ¢ncia')
> plt.legend()
> plt.show()
>
>
>
> # Para demonstrar que o comportamento da autocovariancia Ã© proporcional ao tempo para grandes lags
> time = np.array(range(max_lag, num_points)).reshape(-1,1)
> model = LinearRegression().fit(time, autocovariances[:, max_lag-1])
> print(f"Coeficiente da regressÃ£o linear para autocovariancia no lag {max_lag}: {model.coef_[0]:.4f}")
> ```
>
> A visualizaÃ§Ã£o das autocovariÃ¢ncias mostra que para lags maiores, a autocovariÃ¢ncia cresce aproximadamente linearmente com o tempo, conforme demonstrado pelo modelo de regressÃ£o.

Esta proposiÃ§Ã£o complementa o Teorema 1.1, mostrando que nÃ£o apenas a variÃ¢ncia, mas tambÃ©m a autocovariÃ¢ncia de um processo I(1) cresce com o tempo para defasagens grandes, embora de maneira diferente. Este crescimento da autocovariÃ¢ncia destaca o comportamento nÃ£o estacionÃ¡rio desses processos e o efeito acumulativo de choques ao longo do tempo.

**Teorema 1.2**
Se $y_t$ Ã© um processo I(1) e $x_t$ Ã© um processo estacionÃ¡rio, entÃ£o a soma $z_t = y_t + x_t$ Ã© um processo I(1).

*DemonstraÃ§Ã£o:*
I.  Se $y_t$ Ã© I(1), entÃ£o $\Delta y_t$ Ã© estacionÃ¡rio.
II.  Se $x_t$ Ã© estacionÃ¡rio, entÃ£o $\Delta x_t = x_t - x_{t-1}$ Ã© estacionÃ¡rio.
III.  Considere $z_t = y_t + x_t$. Aplicando o operador de primeira diferenÃ§a, obtemos:
    $$ \Delta z_t = \Delta (y_t + x_t) = \Delta y_t + \Delta x_t $$
IV.  A soma de dois processos estacionÃ¡rios Ã© um processo estacionÃ¡rio. Portanto, $\Delta z_t$ Ã© estacionÃ¡rio, pois Ã© a soma de $\Delta y_t$ (estacionÃ¡rio) e $\Delta x_t$ (estacionÃ¡rio).
V. Isso significa que $z_t$ Ã© um processo que, quando diferenciado uma vez, se torna estacionÃ¡rio, e portanto, $z_t$ Ã© um processo I(1). $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:** Vamos simular um processo I(1) e um processo AR(1) estacionÃ¡rio, somÃ¡-los e analisar o resultado.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> np.random.seed(42)
> num_points = 100
> epsilon_y = np.random.normal(0, 1, num_points)
> epsilon_x = np.random.normal(0, 1, num_points)
>
> y = np.zeros(num_points)
> x = np.zeros(num_points)
>
> for t in range(1, num_points):
>  y[t] = y[t-1] + epsilon_y[t]
>  x[t] = 0.7 * x[t-1] + epsilon_x[t]
>
> z = y + x
>
> plt.figure(figsize=(10,5))
> plt.plot(y, label='I(1) Process')
> plt.plot(x, label = 'Estacionary AR(1) Process')
> plt.plot(z, label='Soma dos Processos')
> plt.title('Soma de um Processo I(1) e um Processo EstacionÃ¡rio')
> plt.xlabel('Tempo')
> plt.ylabel('Valor')
> plt.legend()
> plt.show()
>
> plt.figure(figsize=(10,5))
> plt.plot(np.diff(z), label='Primeira DiferenÃ§a da Soma')
> plt.title('Primeira DiferenÃ§a da Soma dos Processos')
> plt.xlabel('Tempo')
> plt.ylabel('Valor')
> plt.legend()
> plt.show()
>
> print(f"MÃ©dia da sÃ©rie I(1) original: {np.mean(y):.2f}")
> print(f"VariÃ¢ncia da sÃ©rie I(1) original: {np.var(y):.2f}")
> print(f"MÃ©dia da sÃ©rie estacionÃ¡ria original: {np.mean(x):.2f}")
> print(f"VariÃ¢ncia da sÃ©rie estacionÃ¡ria original: {np.var(x):.2f}")
> print(f"MÃ©dia da sÃ©rie resultante original: {np.mean(z):.2f}")
> print(f"VariÃ¢ncia da sÃ©rie resultante original: {np.var(z):.2f}")
> print(f"MÃ©dia da primeira diferenÃ§a da soma dos processos: {np.mean(np.diff(z)):.2f}")
> print(f"VariÃ¢ncia da primeira diferenÃ§a da soma dos processos: {np.var(np.diff(z)):.2f}")
> ```
> A plotagem da soma dos dois processos mostra um comportamento nÃ£o estacionÃ¡rio similar ao do processo I(1). Ao diferenciarmos a soma, obtemos uma sÃ©rie estacionÃ¡ria. A mÃ©dia e variÃ¢ncia da sÃ©rie I(1) e da soma nÃ£o sÃ£o constantes.

Este resultado destaca que a propriedade de ser I(1) Ã© preservada quando somamos um processo I(1) com um processo estacionÃ¡rio. Isso Ã© fundamental para entender como a nÃ£o estacionariedade pode se propagar atravÃ©s de combinaÃ§Ãµes de sÃ©ries temporais.

### ConclusÃ£o
O conceito de processo integrado de ordem 1 (I(1)) Ã© fundamental para a modelagem e anÃ¡lise de sÃ©ries temporais nÃ£o estacionÃ¡rias que exibem uma tendÃªncia estocÃ¡stica. Esses processos precisam ser diferenciados uma vez para se tornarem estacionÃ¡rios, e a sua caracterÃ­stica de raiz unitÃ¡ria implica que os choques passados tÃªm efeitos permanentes no nÃ­vel da sÃ©rie. A compreensÃ£o das diferenÃ§as entre processos I(1) e *trend-stationary* Ã© crucial para a escolha apropriada de modelos de sÃ©ries temporais e para a correta interpretaÃ§Ã£o dos resultados empÃ­ricos. A classe de processos I(1) representa uma ferramenta essencial para a anÃ¡lise e modelagem de sÃ©ries temporais com comportamento complexo, especialmente em dados econÃ´micos e financeiros.

### ReferÃªncias
[^1]: Modelos de SÃ©ries Temporais NÃ£o EstacionÃ¡rias. *[CapÃ­tulo 15 do livro]*
<!-- END -->
