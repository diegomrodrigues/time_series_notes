## Modelagem de Memória Longa em Séries Temporais Não Estacionárias: Uma Análise Detalhada da Integração Fracionária

### Introdução
Em continuidade à exploração de modelos para séries temporais não estacionárias, este capítulo se dedica à análise detalhada da **integração fracionária**, com foco em como este método possibilita a modelagem de **memória longa**. Como discutido em capítulos anteriores, modelos tradicionais de raiz unitária e tendência determinística podem não ser adequados para capturar as características de séries temporais com dependência de longo prazo [^1]. A integração fracionária surge como uma alternativa, permitindo que o parâmetro de integração ($d$) assuma valores não inteiros. Este capítulo irá explorar como valores não inteiros de $d$ podem modelar a persistência de inovações e a dependência de longo prazo, detalhando as propriedades de processos fracionalmente integrados, suas representações matemáticas e sua aplicabilidade na modelagem de séries temporais com memória longa [^1].

### Conceitos Fundamentais

#### Integração Fracionária: Uma Generalização da Diferenciação
A integração fracionária é uma generalização da diferenciação tradicional que permite que o parâmetro de integração $d$ assuma valores não inteiros. Um processo integrado de ordem $d$ pode ser representado pela seguinte equação:
$$(1-L)^d y_t = \psi(L)\epsilon_t$$ [15.5.1]
onde $L$ é o operador de defasagem, $y_t$ é a série temporal, $\psi(L)$ é um polinômio em $L$ representando uma estrutura de média móvel (MA), e $\epsilon_t$ é um ruído branco [^1]. Quando $d$ é um inteiro positivo, temos a diferenciação tradicional (ex: $d=1$ corresponde à primeira diferença). No entanto, quando $d$ é um valor não inteiro, o operador $(1-L)^d$ define uma operação de diferenciação ou integração fracionária.

Para entender o significado de valores não inteiros de $d$, podemos reescrever a equação [15.5.1] como:
$$y_t = (1-L)^{-d} \psi(L)\epsilon_t$$
Expandindo $(1-L)^{-d}$ em uma série de potências, obtemos:
$$(1-L)^{-d} = \sum_{j=0}^{\infty} h_jL^j$$ [15.5.3]
onde os coeficientes $h_j$ são dados por:
$$h_j = \frac{\Gamma(d+j)}{\Gamma(j+1)\Gamma(d)}$$
que pode ser calculada usando a relação recursiva $h_0=1$ e $h_j = h_{j-1}\frac{d+j-1}{j}$. Assim, a série temporal $y_t$ pode ser escrita como uma média móvel de ordem infinita:
$$y_t = \sum_{j=0}^{\infty} h_j\epsilon_{t-j}$$
onde os coeficientes $h_j$ representam o impacto das inovações passadas na série. O comportamento desses coeficientes é crucial para entender a dependência de longo prazo e a persistência das inovações [^1].

> A principal diferença entre diferenciação inteira e fracionária está na forma como as inovações passadas são ponderadas. Na diferenciação inteira, os pesos são discretos (ex: 1, -1, ou 1, -2, 1), enquanto na diferenciação fracionária, os pesos decrescem de forma hiperbólica, permitindo um efeito de longo prazo das inovações [^1].

> 💡 **Exemplo Numérico:** Considere um processo com $d = 0.3$ e $\psi(L) = 1$. Vamos calcular os primeiros cinco coeficientes $h_j$ usando a relação recursiva:
>
> $\text{Step 1: } h_0 = 1$
>
> $\text{Step 2: } h_1 = h_0 \frac{d+1-1}{1} = 1 \times \frac{0.3}{1} = 0.3$
>
> $\text{Step 3: } h_2 = h_1 \frac{d+2-1}{2} = 0.3 \times \frac{1.3}{2} = 0.195$
>
> $\text{Step 4: } h_3 = h_2 \frac{d+3-1}{3} = 0.195 \times \frac{2.3}{3} = 0.1495$
>
> $\text{Step 5: } h_4 = h_3 \frac{d+4-1}{4} = 0.1495 \times \frac{3.3}{4} = 0.1233$
>
> Estes coeficientes mostram como as inovações passadas, $\epsilon_{t-j}$, afetam $y_t$. A inovação em $t-1$ tem um peso de $0.3$, a de $t-2$ tem um peso de $0.195$, e assim por diante. Note como os pesos decrescem mais lentamente em relação a modelos AR ou MA. Se, por outro lado, usássemos $d = 1$ (diferenciação tradicional), teríamos $h_0 = 1, h_1 = -1$ e $h_j=0$ para $j > 1$. Os pesos em diferenciação fracionária decaem mais suavemente, permitindo a modelagem de dependência de longo prazo.

#### Dependência de Longo Prazo e o Parâmetro $d$
A integração fracionária permite modelar dependências de longo prazo por meio de valores não inteiros do parâmetro $d$ [^1]. Em particular, quando $0 < d < 0.5$, a série temporal $y_t$ é **estacionária** e exibe dependência de longo prazo, também conhecida como "memória longa". O termo "memória longa" significa que as autocorrelações da série decaem lentamente, de forma que a influência de inovações passadas se mantém por um período mais longo do que em modelos tradicionais.  Neste caso, a soma dos quadrados dos coeficientes da representação MA(∞) converge, garantindo que a série é estacionária.

A taxa de decaimento dos coeficientes $h_j$ com o aumento de $j$ é dada pela aproximação:
$$h_j \approx (j+1)^{d-1}$$ [15.5.5]
Essa relação mostra que, para $0<d<0.5$, o decaimento de $h_j$ é hiperbólico, ou seja, mais lento do que o decaimento exponencial observado em modelos ARMA estacionários, mas mais rápido do que a ausência de decaimento em modelos com raiz unitária.

*   **Para $d=0$:** O processo é um ruído branco, sem dependência de longo prazo.
*   **Para $0<d<0.5$:** O processo é estacionário com memória longa, e as autocorrelações decaem hiperbolicamente.
*   **Para $d=0.5$:** O processo é marginalmente estacionário.
*   **Para $0.5 < d < 1$:** O processo é não estacionário e necessita ser diferenciado para modelagem.
*   **Para $d=1$:** O processo é um passeio aleatório com raiz unitária.
*   **Para $d>1$:** O processo é não estacionário e necessita ser diferenciado mais de uma vez.

> A flexibilidade da integração fracionária está na capacidade de escolher o valor de $d$ para modelar diferentes níveis de persistência e dependência de longo prazo, oferecendo uma alternativa aos modelos tradicionais que impõem restrições mais rígidas sobre a persistência das inovações [^1].

> 💡 **Exemplo Numérico:** Vamos analisar como a taxa de decaimento dos coeficientes $h_j$ varia com diferentes valores de $d$. Suponha que estamos interessados em $h_{100}$. Usando a aproximação $h_j \approx (j+1)^{d-1}$:
>
> - Para $d=0.1$: $h_{100} \approx (100+1)^{0.1-1} = 101^{-0.9} \approx 0.0099$
> - Para $d=0.3$: $h_{100} \approx (100+1)^{0.3-1} = 101^{-0.7} \approx 0.0257$
> - Para $d=0.5$: $h_{100} \approx (100+1)^{0.5-1} = 101^{-0.5} \approx 0.0995$
>
> Observe que, quanto maior o valor de $d$ (dentro do intervalo de estacionariedade), mais lento é o decaimento de $h_j$, e maior será a persistência das inovações na série temporal. Se compararmos com um processo AR(1) estacionário, por exemplo, onde os coeficientes decaem exponencialmente (e.g., $\phi^j$), o decaimento hiperbólico de $h_j$ é muito mais lento, permitindo que as inovações tenham impacto de longo prazo.
>
> Vamos comparar os coeficientes $h_j$ para d=0.3 e d=0.8 com um AR(1) com $\phi=0.8$. Para os 10 primeiros valores:
>
> | j | h_j (d=0.3) | h_j (d=0.8) | AR(1) com $\phi=0.8$|
> |---|---|---|---|
> | 0 | 1.000 | 1.000 | 1.000|
> | 1 | 0.300 | 0.800 | 0.800|
> | 2 | 0.195 | 0.680 | 0.640|
> | 3 | 0.149 | 0.629 | 0.512|
> | 4 | 0.123 | 0.598 | 0.410|
> | 5 | 0.106 | 0.576 | 0.328|
> | 6 | 0.094 | 0.560 | 0.262|
> | 7 | 0.085 | 0.547 | 0.210|
> | 8 | 0.078 | 0.537 | 0.168|
> | 9 | 0.072 | 0.529 | 0.134|
>
> Note como os valores de $h_j$ decaem de forma muito mais lenta que os de um AR(1) e que para d>0.5, a queda é ainda mais suave.

**Teorema 1:** Para um processo fracionalmente integrado, a soma dos quadrados dos coeficientes $h_j$ da representação MA(∞) converge se e somente se $d < \frac{1}{2}$.

*Proof:*
I.  Um processo fracionalmente integrado é dado por $y_t = (1-L)^{-d} \psi(L) \epsilon_t$ e pode ser expandido em uma representação MA(∞) da forma $y_t = \sum_{j=0}^\infty a_j \epsilon_{t-j}$, onde os coeficientes $a_j$ incorporam tanto os coeficientes $h_j$ de $(1-L)^{-d}$ quanto os coeficientes de $\psi(L)$.
II.  A condição para que um processo seja estacionário é que a variância da série, e portanto a soma dos quadrados dos coeficientes $a_j$, seja finita, ou seja $\sum_{j=0}^{\infty} a_j^2 < \infty$.
III. Assumindo que o processo $\psi(L)\epsilon_t$ seja estacionário, o comportamento de $a_j$ para grandes valores de $j$ é dominado pelos coeficientes $h_j$ de $(1-L)^{-d}$, que têm a aproximação $h_j \approx (j+1)^{d-1}$.
IV. Portanto, a soma dos quadrados dos coeficientes da representação MA(∞) é dada por $\sum_{j=0}^{\infty} a_j^2 \approx \sum_{j=0}^{\infty} h_j^2 \approx \sum_{j=0}^{\infty} (j+1)^{2(d-1)}$.
V. A convergência da série $\sum_{j=0}^{\infty} (j+1)^{2(d-1)}$ depende do comportamento da integral $\int_{1}^{\infty} x^{2(d-1)}dx$.
VI. Essa integral converge se $2(d-1) < -1$, que é equivalente a $2d-2 < -1$, ou seja, $d < \frac{1}{2}$.
VII. Portanto, a soma dos quadrados dos coeficientes $h_j$ converge se e somente se $d < \frac{1}{2}$. $\blacksquare$

> 💡 **Exemplo Numérico:** Para ilustrar o Teorema 1, vamos analisar a soma dos quadrados dos coeficientes $h_j$ para alguns valores de $d$. Para simplificar, vamos usar a aproximação $h_j \approx (j+1)^{d-1}$ e calcular a soma dos quadrados dos 10 primeiros coeficientes: $\sum_{j=0}^{9} h_j^2$.
>
> - Para $d = 0.2$:
>
> ```
> import numpy as np
> d = 0.2
> h_squared_sum = 0
> for j in range(10):
>   h_j = (j+1)**(d-1)
>   h_squared_sum += h_j**2
> print(f"Soma dos quadrados de h_j para d=0.2: {h_squared_sum}")
> ```
>
> Resultado: Soma dos quadrados de h_j para d=0.2: 1.363
>
>
> - Para $d = 0.4$:
>
> ```python
> import numpy as np
> d = 0.4
> h_squared_sum = 0
> for j in range(10):
>   h_j = (j+1)**(d-1)
>   h_squared_sum += h_j**2
> print(f"Soma dos quadrados de h_j para d=0.4: {h_squared_sum}")
> ```
>
> Resultado: Soma dos quadrados de h_j para d=0.4: 1.173
>
>
> - Para $d = 0.6$:
>
> ```python
> import numpy as np
> d = 0.6
> h_squared_sum = 0
> for j in range(10):
>   h_j = (j+1)**(d-1)
>   h_squared_sum += h_j**2
> print(f"Soma dos quadrados de h_j para d=0.6: {h_squared_sum}")
> ```
>
> Resultado: Soma dos quadrados de h_j para d=0.6: 1.067
>
> Embora para os 10 primeiros valores as somas pareçam convergir, se aumentarmos o número de termos na soma, para d>=0.5, a soma começa a divergir. Por exemplo:
>
> - Para d = 0.6 e 100 termos:
>
> ```python
> import numpy as np
> d = 0.6
> h_squared_sum = 0
> for j in range(100):
>   h_j = (j+1)**(d-1)
>   h_squared_sum += h_j**2
> print(f"Soma dos quadrados de h_j para d=0.6 (100 termos): {h_squared_sum}")
> ```
>
> Resultado: Soma dos quadrados de h_j para d=0.6 (100 termos): 1.739. Note como a soma aumenta quando aumentamos o número de termos, indicando que ela não converge. Isso demonstra que o Teorema 1 é válido: para d < 0.5 a soma converge, garantindo estacionariedade, enquanto para d >= 0.5 a série não é estacionária.
>
>  Este exemplo numérico ilustra que para $d < 0.5$, a soma dos quadrados dos coeficientes $h_j$ tende a convergir, o que é uma condição para a estacionariedade do processo. Já para $d \ge 0.5$, a soma dos quadrados dos coeficientes $h_j$ diverge, o que significa que o processo não é estacionário.

#### Representação MA(∞) e Decaimento dos Coeficientes
Como já vimos, um processo integrado fracionário pode ser expresso como:
$$y_t = (1-L)^{-d} \psi(L)\epsilon_t = \sum_{j=0}^{\infty} a_j\epsilon_{t-j}$$
Esta representação MA(∞) é fundamental para entender o comportamento de séries com memória longa. Os coeficientes $a_j$ são obtidos pela convolução dos coeficientes $h_j$ do operador $(1-L)^{-d}$ com os coeficientes do polinômio $\psi(L)$.

No caso em que $\psi(L)=1$, a série temporal é dada por:
$$y_t = \sum_{j=0}^{\infty} h_j\epsilon_{t-j}$$
O decaimento dos coeficientes $h_j$ determina o nível de persistência e a dependência de longo prazo. Como já discutido, $h_j \approx (j+1)^{d-1}$, para grandes valores de $j$, o que indica um decaimento hiperbólico.

A taxa de decaimento dos coeficientes $h_j$ influencia diretamente o comportamento das autocorrelações da série temporal. Em modelos com memória longa, as autocorrelações decaem para zero mais lentamente do que em modelos tradicionais ARMA [^1]. O decaimento hiperbólico dos coeficientes $h_j$ implica que as autocorrelações também decaem de forma hiperbólica, em contraste com o decaimento exponencial dos modelos ARMA.

> 💡 **Exemplo Numérico:**  Suponha um modelo com $d=0.3$ e $\psi(L) = 1$. Vamos simular uma série temporal com 1000 observações:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from math import gamma
>
> def fractional_diff_weights(d, length):
>    weights = []
>    for j in range(length):
>      weights.append(gamma(d+j) / (gamma(j+1) * gamma(d)))
>    return np.array(weights)
>
> # Define os parâmetros
> T = 1000
> d = 0.3
>
> # Gera os pesos
> weights = fractional_diff_weights(d, T)
>
> # Gera ruído branco
> white_noise = np.random.normal(0, 1, T)
>
> # Calcula a série temporal
> y = np.convolve(white_noise, weights, mode='full')[:T]
>
> # Plota a série
> plt.plot(y)
> plt.xlabel('Tempo')
> plt.ylabel('y_t')
> plt.title('Série Temporal Fracionalmente Integrada (d=0.3)')
> plt.show()
> ```
>
> Esta série simulada demonstra como o efeito das inovações é persistente ao longo do tempo. As oscilações na série não desaparecem rapidamente, o que é característico de processos com memória longa. O histograma da série mostra que ela é aproximadamente normal, mas suas autocorrelações (como veremos no próximo exemplo) indicam uma dependência de longo prazo.

> 💡 **Exemplo Numérico:** Vamos comparar o comportamento das autocorrelações para uma série temporal fracionalmente integrada ($d=0.3$) e um processo AR(1) estacionário ($\phi=0.7$).
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from math import gamma
>
> def fractional_diff_weights(d, length):
>    weights = []
>    for j in range(length):
>      weights.append(gamma(d+j) / (gamma(j+1) * gamma(d)))
>    return np.array(weights)
>
> def generate_frac_integrated_series(d, T):
>  white_noise = np.random.normal(0, 1, T)
>  weights = fractional_diff_weights(d, T)
>  frac_integrated_series = np.convolve(white_noise, weights, mode='full')[:T]
>  return frac_integrated_series
>
> def generate_ar1_series(phi, T):
>     white_noise = np.random.normal(0, 1, T)
>     ar1_series = np.zeros(T)
>     ar1_series[0] = white_noise[0]
>     for t in range(1, T):
>         ar1_series[t] = phi * ar1_series[t-1] + white_noise[t]
>     return ar1_series
>
> def autocorr(x, max_lag):
>  n = len(x)
>  result = []
>  for lag in range(max_lag + 1):
>      if lag == 0:
>        corr = np.corrcoef(x[:-lag],x[:-lag])[0,1]
>      else:
>          corr = np.corrcoef(x[:-lag],x[lag:])[0,1]
>
>      result.append(corr)
>  return np.array(result)
>
> # Define os parâmetros
> T = 1000
> max_lag = 100
> d = 0.3
> phi = 0.7
>
> # Gera as séries
> frac_integrated_series = generate_frac_integrated_series(d, T)
> ar1_series = generate_ar1_series(phi, T)
>
> # Calcula as autocorrelações
> acorr_frac = autocorr(frac_integrated_series, max_lag)
> acorr_ar1 = autocorr(ar1_series, max_lag)
>
> # Plota as autocorrelações
> plt.figure(figsize=(10, 6))
> plt.plot(acorr_frac, label='Frac. Integrated (d=0.3)')
> plt.plot(acorr_ar1, label='AR(1) (phi=0.7)')
> plt.xlabel('Lag')
> plt.ylabel('Autocorrelação')
> plt.title('Comparação das Autocorrelações')
> plt.legend()
> plt.show()
> ```
>
> O gráfico mostra que as autocorrelações do processo fracionalmente integrado (d=0.3) decaem mais lentamente do que as autocorrelações do processo AR(1) (phi=0.7). Enquanto no processo AR(1) as autocorrelações diminuem exponencialmente, no processo fracionalmente integrado, elas decaem de forma mais gradual, evidenciando a presença de memória longa.

**Teorema 2:** Se um processo fracionalmente integrado com $0 < d < \frac{1}{2}$ é definido como $y_t = (1-L)^{-d}\epsilon_t$, a função de autocorrelação $\rho_k$ decai hiperbolicamente como $\rho_k \approx k^{2d-1}$ para grandes valores de $k$.

*Proof:*
I. Sabemos que para grandes valores de $j$, $h_j \approx (j+1)^{d-1}$.
II. A autocorrelação no lag $k$, $\rho_k$, pode ser aproximada pela relação $\rho_k \approx \frac{\Gamma(1-d)\Gamma(k+d)}{\Gamma(d)\Gamma(k+1-d)}$.
III. Usando a aproximação $\Gamma(k+a) \approx k^a$, para grandes valores de $k$, temos:
$$\rho_k \approx \frac{\Gamma(1-d)}{\Gamma(d)} \frac{k^d}{k^{1-d}} = \frac{\Gamma(1-d)}{\Gamma(d)} k^{2d-1}$$
IV. Portanto, para grandes valores de $k$, a autocorrelação decai hiperbolicamente como $\rho_k \approx k^{2d-1}$. $\blacksquare$

> 💡 **Exemplo Numérico:** Para ilustrar o Teorema 2, vamos calcular a autocorrelação para $d=0.3$ e diferentes lags:
>
> $\text{Step 1: } \rho_k \approx k^{2d-1} = k^{2*0.3-1} = k^{-0.4}$
>
> $\text{Step 2: Para k=10 } \rho_{10} \approx 10^{-0.4} \approx 0.398$
>
> $\text{Step 3: Para k=50 } \rho_{50} \approx 50^{-0.4} \approx 0.174$
>
> $\text{Step 4: Para k=100 } \rho_{100} \approx 100^{-0.4} \approx 0.126$
>
> Estes cálculos mostram que a autocorrelação decai lentamente à medida que o lag $k$ aumenta, o que é característico de processos com memória longa. A taxa de decaimento é determinada pelo valor de $d$.

**Lema 2.1:**  A função de autocovariância $\gamma_k$ de um processo fracionalmente integrado com $0<d<0.5$,  $y_t = (1-L)^{-d}\epsilon_t$, decai hiperbolicamente como $\gamma_k \approx k^{2d-1}$ para grandes valores de $k$, assumindo que $\epsilon_t$ tenha variância constante $\sigma^2$.

*Proof:*
I. A autocovariância $\gamma_k$ é definida como $Cov(y_t, y_{t-k})$.
II. Para um processo fracionalmente integrado $y_t = \sum_{j=0}^{\infty} h_j \epsilon_{t-j}$, a autocovariância pode ser escrita como $\gamma_k = E[( \sum_{j=0}^{\infty} h_j \epsilon_{t-j})( \sum_{m=0}^{\infty} h_m \epsilon_{t-k-m})]$.
III. Usando a propriedade de que $E[\epsilon_t \epsilon_{t-j}] = \sigma^2$ se $j=0$ e $0$ caso contrário, temos $\gamma_k = \sigma^2 \sum_{j=0}^{\infty} h_j h_{j+k}$.
IV.  Para grandes valores de $j$, temos a aproximação  $h_j \approx (j+1)^{d-1}$. Substituindo na expressão para $\gamma_k$, obtemos:
$$ \gamma_k \approx \sigma^2 \sum_{j=0}^{\infty} (j+1)^{d-1}(j+k+1)^{d-1}$$
V. Para grandes valores de $k$, a soma acima pode ser aproximada por uma integral:
$$\gamma_k \approx  \sigma^2 \int_{0}^{\infty} x^{d-1}(x+k)^{d-1} dx $$
VI.  Esta integral pode ser resolvida e seu comportamento assintótico para grandes $k$ é $\gamma_k \approx c k^{2d-1}$, onde $c$ é uma constante que depende de $d$ e da variância $\sigma^2$..
VII. Assim, a autocovariância $\gamma_k$ decai hiperbolicamente como $\gamma_k \approx k^{2d-1}$ para grandes valores de $k$.  $\blacksquare$

### Implicações para a Modelagem de Séries Temporais com Memória Longa

A modelagem de séries temporais com memória longa usando integração fracionária tem implicações importantes para a análise de dados:
1. **Captura da Persistência:** A integração fracionária permite capturar a persistência das inovações, o que é crucial para modelar fenômenos que exibem dependência de longo prazo. Modelos tradicionais podem não ser adequados para capturar corretamente a dinâmica desses processos.
2. **Flexibilidade na Modelagem:** O parâmetro $d$ oferece flexibilidade na modelagem da dependência de longo prazo, permitindo que o analista ajuste o modelo à dinâmica específica de cada série temporal.
3. **Melhora na Previsão:** Modelos com integração fracionária podem levar a previsões mais precisas em séries com memória longa, pois levam em consideração o efeito persistente das inovações passadas.
4. **Teste de Hipóteses:** Os testes de hipóteses devem ser ajustados para levar em consideração a não normalidade de processos com memória longa. Os testes tradicionais podem gerar resultados enviesados.
5. **Estimação de Parâmetros:** A estimação de parâmetros em modelos com integração fracionária exige métodos específicos, tais como estimação de máxima verossimilhança ou métodos bayesianos, que não assumam que as inovações sejam ruídos brancos.
6. **Análise do parâmetro d:** A estimação do parâmetro $d$ oferece informações importantes sobre o grau de persistência da série. Este parâmetro pode ser usado para comparar o comportamento de séries temporais diferentes.

> 💡 **Exemplo Numérico:** Suponha que temos dois conjuntos de dados: um com $d \approx 0.1$ e outro com $d \approx 0.4$. Podemos inferir que:
>
> - O conjunto com $d \approx 0.1$ terá uma dependência de longo prazo relativamente fraca, as autocorrelações vão decair rapidamente.
>
> - O conjunto com $d \approx 0.4$ terá uma dependência de longo prazo mais forte, as autocorrelações vão persistir por um período mais longo.
>
> A estimativa de $d$ pode nos dar informações sobre a estrutura da série temporal. Por exemplo, $d=0.4$ pode ser comum em dados financeiros, enquanto $d=0.1$ pode ser mais comum em dados meteorológicos com menos persistência.

**Proposição 1:** Modelos com integração fracionária podem levar a previsões mais precisas em séries temporais com memória longa em comparação com modelos ARMA.

*Proof Outline:*
I. Modelos ARMA têm coeficientes que decaem exponencialmente, o que limita sua capacidade de capturar a memória longa.
II. Em modelos com memória longa, as autocorrelações decaem hiperbolicamente.
III. A integração fracionária permite modelar o decaimento hiperbólico das autocorrelações, por meio da manipulação do parâmetro $d$.
IV.  Como modelos com integração fracionária capturam a persistência e a dependência de longo prazo de forma mais adequada, seus modelos de previsão tendem a ser mais precisos em séries com memória longa. $\blacksquare$

> 💡 **Exemplo Numérico:** Para demonstrar a Proposição 1, considere uma série temporal gerada por um processo de integração fracionária com $d=0.3$ e vamos compará-la com uma previsão usando um modelo AR(1).
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from math import gamma
> from sklearn.linear_model import LinearRegression
>
> def fractional_diff_weights(d, length):
>    weights = []
>    for j in range(length):
>      weights.append(gamma(d+j) / (gamma(j+1) * gamma(d)))
>    return np.array(weights)
>
> def generate_frac_integrated_series(d, T):
>  white_noise = np.random.normal(0, 1, T)
>  weights = fractional_diff_weights(d, T)
>  frac_integrated_series = np.convolve(white_noise, weights, mode='full')[:T]
>  return frac_integrated_series
>
> def generate_ar1_series(phi, T):
>     white_noise = np.random.normal(0, 1, T)
>     ar1_series = np.zeros(T)
>     ar1_series[0] = white_noise[0]
>     for t in range(1, T):
>         ar1_series[t] = phi * ar1_series[t-1] + white_noise[t]
>     return ar1_series
>
> def fit_ar1(data):
>    X = data[:-1].reshape(-1, 1)
>    y = data[1:]
>    model = LinearRegression()
>    model.fit(X, y)
>    return model.coef_[0]
>
> # Define os parâmetros
> T = 500
> d = 0.3
> phi_ar1 = 0.7
> training_size = int(T*0.8)
>
> # Gera a série fracionalmente integrada
> frac_series = generate_frac_integrated_series(d, T)
>
> # Estima o parâmetro AR(1) nos dados iniciais
> phi_est = fit_ar1(frac_series[:training_size])
>
> # Faz a previsão
> ar1_pred = np.zeros(T-training_size)
> ar1_pred[0] = frac_series[training_size-1] * phi_est
> for t in range(1, T-training_size):
>   ar1_pred[t] = ar1_pred[t-1] * phi_est
>
> frac_pred = np.zeros(T-training_size)
> last_value = frac_series[training_size-1]
> for i in range(T - training_size):
>   frac_pred[i] = last_value
>
> # Calcula os erros de previsão
> ar1_error = np.sqrt(np.mean((frac_series[training_size:] - ar1_pred)**2))
> frac_error = np.sqrt(np.mean((frac_series[training_size:] - frac_pred)**2))
>
> print(f"RMSE para AR(1): {ar1_error}")
> print(f"RMSE para previsão da última observação: {frac_error}")
>
> # Gera os dados para o AR(1)
> ar1_series = generate_ar1_series(phi_ar1, T)
> phi_est_ar1 = fit_ar1(```python
ar```python
ar1_series)
> print(f"Coeficiente estimado para AR(1): {phi_est_ar1}")
>
> # Gera os dados para o AR(2)
> ar2_series = generate_ar2_series(phi_1_ar2, phi_2_ar2, T)
> phi_est_ar2 = fit_ar2(ar2_series)
> print(f"Coeficientes estimados para AR(2): {phi_est_ar2}")
>
> # Gera os dados para o MA(1)
> ma1_series = generate_ma1_series(theta_ma1, T)
> theta_est_ma1 = fit_ma1(ma1_series)
> print(f"Coeficiente estimado para MA(1): {theta_est_ma1}")
>
> # Gera os dados para o MA(2)
> ma2_series = generate_ma2_series(theta_1_ma2, theta_2_ma2, T)
> theta_est_ma2 = fit_ma2(ma2_series)
> print(f"Coeficientes estimados para MA(2): {theta_est_ma2}")

```
<!-- END -->
