## Modelos de SÃ©ries Temporais NÃ£o EstacionÃ¡rias: Uma AnÃ¡lise Detalhada

### IntroduÃ§Ã£o
Este capÃ­tulo aprofunda o estudo de **modelos de sÃ©ries temporais nÃ£o estacionÃ¡rias**, abordando suas caracterÃ­sticas, representaÃ§Ãµes matemÃ¡ticas e abordagens para anÃ¡lise [^1]. Como vimos anteriormente [^2], [^3], [^4], sÃ©ries temporais nÃ£o estacionÃ¡rias sÃ£o aquelas cujas propriedades estatÃ­sticas, como mÃ©dia e variÃ¢ncia, variam ao longo do tempo, em contraste com as sÃ©ries estacionÃ¡rias, que possuem essas propriedades constantes. A modelagem de sÃ©ries nÃ£o estacionÃ¡rias demanda abordagens especÃ­ficas que buscam capturar essas variaÃ§Ãµes dinÃ¢micas ao longo do tempo, e que permitem fazer previsÃµes mais precisas em modelos com dependÃªncia temporal. Este capÃ­tulo irÃ¡ explorar os principais modelos utilizados para anÃ¡lise de sÃ©ries nÃ£o estacionÃ¡rias, incluindo modelos trend-stationary, processos de raiz unitÃ¡ria, modelos com integraÃ§Ã£o fracionÃ¡ria e modelos com quebras estruturais, fornecendo uma visÃ£o abrangente das tÃ©cnicas e ferramentas disponÃ­veis para anÃ¡lise de dados nÃ£o estacionÃ¡rios.

### DefiniÃ§Ã£o e CaracterÃ­sticas de SÃ©ries Temporais NÃ£o EstacionÃ¡rias
Uma sÃ©rie temporal Ã© considerada nÃ£o estacionÃ¡ria se suas propriedades estatÃ­sticas (mÃ©dia, variÃ¢ncia e autocovariÃ¢ncia) variam ao longo do tempo. Essa variaÃ§Ã£o implica que a sÃ©rie nÃ£o apresenta um padrÃ£o constante, dificultando a aplicaÃ§Ã£o de modelos estatÃ­sticos tradicionais. As principais caracterÃ­sticas das sÃ©ries temporais nÃ£o estacionÃ¡rias sÃ£o:

1.  **MÃ©dia NÃ£o Constante:** A mÃ©dia da sÃ©rie varia com o tempo, apresentando tendÃªncias de crescimento ou decrescimento, ou padrÃµes cÃ­clicos que nÃ£o se repetem de maneira constante.
2.  **VariÃ¢ncia NÃ£o Constante:** A variÃ¢ncia da sÃ©rie pode aumentar ou diminuir com o tempo, indicando uma mudanÃ§a na dispersÃ£o dos dados.
3.  **AutocorrelaÃ§Ã£o NÃ£o EstacionÃ¡ria:** A autocorrelaÃ§Ã£o, que mede a dependÃªncia entre valores da sÃ©rie em diferentes instantes de tempo, nÃ£o Ã© constante ao longo do tempo, o que indica que os padrÃµes de dependÃªncia temporal tambÃ©m se alteram com o tempo.
4.  **PersistÃªncia de Choques:** Choques aleatÃ³rios na sÃ©rie podem ter efeitos duradouros e persistentes no nÃ­vel da sÃ©rie, o que Ã© uma caracterÃ­stica dos processos com raiz unitÃ¡ria.

#### Tipos de NÃ£o Estacionaridade
Existem diferentes tipos de nÃ£o estacionaridade, que podem ser modelados com abordagens especÃ­ficas:
1. **NÃ£o Estacionaridade em MÃ©dia:** A mÃ©dia da sÃ©rie varia ao longo do tempo, o que pode ser causado por uma tendÃªncia determinÃ­stica (como em modelos trend-stationary) ou por uma tendÃªncia estocÃ¡stica (como em modelos com raiz unitÃ¡ria).
2. **NÃ£o Estacionaridade em VariÃ¢ncia:** A variÃ¢ncia da sÃ©rie varia ao longo do tempo, o que pode ser modelado com processos heterocedÃ¡sticos (como modelos ARCH/GARCH).
3. **NÃ£o Estacionaridade em AutocorrelaÃ§Ã£o:** A autocorrelaÃ§Ã£o da sÃ©rie varia ao longo do tempo, indicando que a dependÃªncia temporal da sÃ©rie se modifica.
4. **Quebras Estruturais:** MudanÃ§as abruptas nos parÃ¢metros da sÃ©rie (mÃ©dia, variÃ¢ncia ou autocovariÃ¢ncia) em determinados pontos do tempo, o que indica que o modelo estatÃ­stico precisa de quebras no modelo, para se adequar a esses regimes diferentes.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
>   Um exemplo de sÃ©rie nÃ£o estacionÃ¡ria Ã© o preÃ§o de uma aÃ§Ã£o ao longo de vÃ¡rios anos, pois o preÃ§o mÃ©dio da aÃ§Ã£o tende a aumentar ao longo do tempo, a variÃ¢ncia do preÃ§o tende a mudar, e os padrÃµes de dependÃªncia temporal tambÃ©m se modificam.
>
>   ```python
>   import numpy as np
>   import matplotlib.pyplot as plt
>
>   np.random.seed(42)
>   T = 200
>   y = np.cumsum(np.random.normal(0.05, 0.8, T))
>   t = np.arange(T)
>
>   plt.figure(figsize=(10, 6))
>   plt.plot(t, y)
>   plt.title('SÃ©rie Temporal NÃ£o EstacionÃ¡ria (simulaÃ§Ã£o de um passeio aleatÃ³rio com deriva)')
>   plt.xlabel('Tempo')
>   plt.ylabel('Valor')
>   plt.grid(True)
>   plt.show()
>   print(f"MÃ©dia da sÃ©rie: {np.mean(y):.4f}")
>   print(f"VariÃ¢ncia da sÃ©rie: {np.var(y):.4f}")
>
>   ```
>  O grÃ¡fico mostra que a mÃ©dia da sÃ©rie aumenta ao longo do tempo, e a sua variÃ¢ncia tambÃ©m aumenta, o que caracteriza a nÃ£o estacionaridade da sÃ©rie.
>   As propriedades estatÃ­sticas da sÃ©rie variam ao longo do tempo.

**ProposiÃ§Ã£o 1:** Um processo estocÃ¡stico Ã© estacionÃ¡rio (em sentido fraco ou de segunda ordem) se sua mÃ©dia e autocovariÃ¢ncia forem constantes ao longo do tempo.
*Prova:*
I. Um processo estocÃ¡stico $y_t$ Ã© considerado estacionÃ¡rio (em sentido fraco ou de segunda ordem) se:
    - A mÃ©dia do processo Ã© constante ao longo do tempo: $E[y_t] = \mu$ para todo $t$, onde $\mu$ Ã© uma constante.
    - A autocovariÃ¢ncia entre dois valores da sÃ©rie $y_t$ e $y_{t-k}$ depende apenas da diferenÃ§a de tempo (lag) $k$, e nÃ£o dos valores absolutos de $t$, isto Ã©, $Cov(y_t, y_{t-k}) = \gamma(k)$, onde $\gamma$ Ã© uma funÃ§Ã£o que depende apenas de $k$.
II. Uma sÃ©rie temporal nÃ£o estacionÃ¡ria nÃ£o satisfaz uma dessas condiÃ§Ãµes, isto Ã©, sua mÃ©dia e/ou autocovariÃ¢ncia variam ao longo do tempo, e nÃ£o sÃ£o constantes.
III. Portanto, para que um processo seja estacionÃ¡rio, sua mÃ©dia e autocovariÃ¢ncia devem ser constantes ao longo do tempo, e um processo nÃ£o estacionÃ¡rio nÃ£o satisfaz uma dessas propriedades. $\blacksquare$

**ProposiÃ§Ã£o 1.1:** Se um processo estocÃ¡stico nÃ£o Ã© estacionÃ¡rio em mÃ©dia ou variÃ¢ncia, entÃ£o ele tambÃ©m nÃ£o Ã© estacionÃ¡rio em sentido forte.
*Prova:*
I. A estacionaridade forte requer que a distribuiÃ§Ã£o conjunta de $(y_{t_1}, y_{t_2}, ..., y_{t_n})$ seja a mesma que a distribuiÃ§Ã£o conjunta de $(y_{t_1+h}, y_{t_2+h}, ..., y_{t_n+h})$ para qualquer $t_1, t_2, ..., t_n$ e qualquer $h$.
II. Se a mÃ©dia ou variÃ¢ncia de um processo nÃ£o Ã© constante ao longo do tempo, entÃ£o a sua distribuiÃ§Ã£o conjunta tambÃ©m muda com o tempo, e portanto, o processo nÃ£o Ã© estacionÃ¡rio em sentido forte.
III. Assim, se um processo nÃ£o Ã© estacionÃ¡rio em mÃ©dia ou variÃ¢ncia, ele tambÃ©m nÃ£o pode ser estacionÃ¡rio em sentido forte. $\blacksquare$

### Modelos Trend-Stationary
Modelos trend-stationary assumem que a sÃ©rie temporal Ã© composta por uma tendÃªncia determinÃ­stica, e um componente estocÃ¡stico estacionÃ¡rio. A equaÃ§Ã£o bÃ¡sica de um modelo trend-stationary Ã©:
$$ y_t = \alpha + \delta t + u_t $$ [^1]
onde:
*   $y_t$ Ã© a sÃ©rie temporal observada.
*   $\alpha$ Ã© uma constante que representa o intercepto da tendÃªncia.
*   $\delta$ Ã© uma constante que representa a inclinaÃ§Ã£o da tendÃªncia.
*   $t$ Ã© o Ã­ndice de tempo.
*   $u_t$ Ã© um processo estocÃ¡stico estacionÃ¡rio com mÃ©dia zero e variÃ¢ncia constante.
O modelo assume que a nÃ£o estacionaridade da sÃ©rie Ã© devido Ã  tendÃªncia linear, e que, apÃ³s remover essa tendÃªncia, a sÃ©rie resultante Ã© estacionÃ¡ria, de forma que um modelo ARMA pode ser aplicado aos resÃ­duos $u_t$.

#### GeneralizaÃ§Ãµes de Modelos Trend-Stationary
O modelo trend-stationary pode ser generalizado para incluir tendÃªncias polinomiais de ordem superior:
$$ y_t = \alpha_0 + \alpha_1 t + \alpha_2 t^2 + \ldots + \alpha_p t^p + u_t $$
onde:
*   $\alpha_0, \alpha_1, \ldots, \alpha_p$ sÃ£o coeficientes que determinam a forma da tendÃªncia polinomial.
*   $p$ Ã© a ordem do polinÃ´mio.
* $u_t$ Ã© um processo estocÃ¡stico estacionÃ¡rio com mÃ©dia zero.

Outra generalizaÃ§Ã£o Ã© a inclusÃ£o de componentes sazonais determinÃ­sticos:
$$ y_t = \alpha + \delta t + \sum_{i=1}^k \beta_i S_i(t) + u_t $$
onde:
*   $S_i(t)$ sÃ£o funÃ§Ãµes que capturam a componente sazonal de perÃ­odo $i$.
*   $\beta_i$ sÃ£o os coeficientes das funÃ§Ãµes sazonais.
*   $u_t$ Ã© um processo estocÃ¡stico estacionÃ¡rio com mÃ©dia zero.
> ðŸ’¡ **Exemplo NumÃ©rico:**
>
>   Um exemplo de modelo trend-stationary Ã©:
>   $y_t = 10 + 0.5t + \epsilon_t$, onde $\epsilon_t$ Ã© um ruÃ­do branco com mÃ©dia zero e variÃ¢ncia 1.
>
>  Neste modelo, a sÃ©rie $y_t$ Ã© composta por uma tendÃªncia linear dada por $10+0.5t$, mais um termo de erro que representa as flutuaÃ§Ãµes em torno da tendÃªncia. A sÃ©rie $y_t$ Ã© nÃ£o estacionÃ¡ria por causa da tendÃªncia, mas a sÃ©rie $y_t - (10+0.5t) = \epsilon_t$ Ã© estacionÃ¡ria.
>
>   Vamos simular essa sÃ©rie por 100 perÃ­odos:
>   ```python
>   import numpy as np
>   import matplotlib.pyplot as plt
>
>   np.random.seed(42)
>   T = 100
>   t = np.arange(T)
>   y = 10 + 0.5 * t + np.random.normal(0, 1, T)
>   plt.figure(figsize=(10, 6))
>   plt.plot(t, y)
>   plt.title('Modelo Trend-Stationary')
>   plt.xlabel('Tempo')
>   plt.ylabel('Valor')
>   plt.grid(True)
>   plt.show()
>   ```
>   O grÃ¡fico mostra uma sÃ©rie com tendÃªncia linear crescente ao longo do tempo, que demonstra a nÃ£o estacionaridade, e flutuaÃ§Ãµes em torno da tendÃªncia, que Ã© a componente estacionÃ¡ria.
>
> Para modelar a sÃ©rie, podemos obter um ajuste da tendÃªncia com regressÃ£o linear e analisar os resÃ­duos, que deveriam ser estacionÃ¡rios.
>
>  Vamos ajustar uma regressÃ£o linear aos dados simulados e analisar os resÃ­duos:
>
>  ```python
>  import numpy as np
>  import matplotlib.pyplot as plt
>  from sklearn.linear_model import LinearRegression
>
>  np.random.seed(42)
>  T = 100
>  t = np.arange(T).reshape(-1, 1)
>  y = 10 + 0.5 * t.flatten() + np.random.normal(0, 1, T)
>
>  model = LinearRegression()
>  model.fit(t, y)
>  y_pred = model.predict(t)
>  residuals = y - y_pred
>
>  plt.figure(figsize=(12, 6))
>  plt.subplot(1, 2, 1)
>  plt.plot(t, y, label='SÃ©rie Temporal')
>  plt.plot(t, y_pred, color='red', label='TendÃªncia Estimada')
>  plt.title('SÃ©rie Temporal e TendÃªncia Estimada')
>  plt.xlabel('Tempo')
>  plt.ylabel('Valor')
>  plt.legend()
>  plt.grid(True)
>
>  plt.subplot(1, 2, 2)
>  plt.plot(t, residuals)
>  plt.title('ResÃ­duos do Modelo')
>  plt.xlabel('Tempo')
>  plt.ylabel('Valor')
>  plt.grid(True)
>
>  plt.tight_layout()
>  plt.show()
>
>  print(f"Coeficiente da tendÃªncia: {model.coef_[0]:.4f}")
>  print(f"Intercepto: {model.intercept_:.4f}")
>  print(f"MÃ©dia dos resÃ­duos: {np.mean(residuals):.4f}")
>  print(f"VariÃ¢ncia dos resÃ­duos: {np.var(residuals):.4f}")
>  ```
> O primeiro grÃ¡fico mostra a sÃ©rie original e a tendÃªncia estimada, que Ã© bastante prÃ³xima da tendÃªncia verdadeira, e o segundo grÃ¡fico mostra os resÃ­duos do modelo, que sÃ£o estacionÃ¡rios em mÃ©dia e variÃ¢ncia, como esperado. A mÃ©dia dos resÃ­duos Ã© prÃ³xima de zero, e a variÃ¢ncia Ã© prÃ³xima de 1, como no ruÃ­do branco simulado.

#### LimitaÃ§Ãµes dos Modelos Trend-Stationary
Os modelos trend-stationary sÃ£o simples, mas apresentam algumas limitaÃ§Ãµes:
1.  **Rigidez da TendÃªncia:** A forma da tendÃªncia Ã© determinada pelos parÃ¢metros do modelo, e nÃ£o se altera ao longo do tempo. Choques na sÃ©rie, se forem permanentes, podem levar a uma inadequaÃ§Ã£o do modelo, pois os resÃ­duos deverÃ£o conter autocorrelaÃ§Ã£o, o que implica que a componente estacionÃ¡ria nÃ£o Ã© adequada para modelar os resÃ­duos.
2.  **Choques TransitÃ³rios:** Modelos trend-stationary assumem que os choques aleatÃ³rios tÃªm um efeito transitÃ³rio sobre a sÃ©rie, ou seja, que a sÃ©rie tende a voltar para sua tendÃªncia de longo prazo, o que nÃ£o ocorre em processos com raiz unitÃ¡ria, onde os choques tÃªm efeito persistente.
3.  **NÃ£o AdequaÃ§Ã£o para SÃ©ries com Raiz UnitÃ¡ria:** SÃ©ries temporais que exibem comportamento de raiz unitÃ¡ria (onde os choques tÃªm efeito permanente) nÃ£o sÃ£o adequadamente representadas por modelos trend-stationary, pois estes nÃ£o modelam a persistÃªncia dos choques.
4. **ViÃ©s na Estimativa da TendÃªncia:** Se o modelo trend-stationary for aplicado a uma sÃ©rie com raiz unitÃ¡ria, a tendÃªncia estimada pode ser viesada, o que leva a previsÃµes imprecisas.
5. **Resultados EspÃºrios:** A aplicaÃ§Ã£o de modelos trend-stationary a sÃ©ries com raiz unitÃ¡ria pode levar a resultados estatÃ­sticos espÃºrios, que nÃ£o refletem a verdadeira dinÃ¢mica da sÃ©rie.

**Lema 1:** A remoÃ§Ã£o da tendÃªncia em modelos trend-stationary produz uma sÃ©rie estacionÃ¡ria, enquanto que a remoÃ§Ã£o da tendÃªncia determinÃ­stica em modelos de raiz unitÃ¡ria nÃ£o resulta em um processo estacionÃ¡rio.
*Prova:*
I. Em um modelo trend-stationary, a sÃ©rie Ã© modelada como $y_t = f(t) + u_t$, onde $f(t)$ representa a tendÃªncia determinÃ­stica (linear ou nÃ£o linear) e $u_t$ Ã© um processo estacionÃ¡rio.  Ao remover a tendÃªncia, obtemos $y_t - f(t) = u_t$, que Ã© estacionÃ¡rio por definiÃ§Ã£o.
II. Em modelos de raiz unitÃ¡ria, como o passeio aleatÃ³rio com deriva,  a sÃ©rie Ã© modelada como $y_t = \delta + y_{t-1} + \epsilon_t$. Expandindo a equaÃ§Ã£o, temos $y_t = y_0 + \delta t + \sum_{i=1}^{t} \epsilon_i$, onde $\delta t$ Ã© uma tendÃªncia determinÃ­stica linear.
III. Ao remover a tendÃªncia linear determinÃ­stica $\delta t$, obtemos: $y_t - \delta t = y_0 + \sum_{i=1}^{t} \epsilon_i$.  Este processo nÃ£o Ã© estacionÃ¡rio, pois a variÃ¢ncia cresce com o tempo: $Var(y_t - \delta t) = Var(y_0 + \sum_{i=1}^{t} \epsilon_i) = t \sigma^2$.
IV. Portanto, a remoÃ§Ã£o da tendÃªncia em modelos trend-stationary resulta em um processo estacionÃ¡rio, enquanto que a remoÃ§Ã£o da tendÃªncia determinÃ­stica em modelos de raiz unitÃ¡ria nÃ£o resulta em um processo estacionÃ¡rio. $\blacksquare$

**Lema 1.1:** Um modelo trend-stationary nÃ£o Ã© adequado para sÃ©ries que apresentam comportamento de passeio aleatÃ³rio com deriva, dado que a remoÃ§Ã£o da tendÃªncia linear resulta em uma sÃ©rie com variÃ¢ncia crescente, o que caracteriza a nÃ£o estacionaridade.
*Prova:*
I. Um passeio aleatÃ³rio com deriva Ã© modelado como $y_t = y_{t-1} + \delta + \epsilon_t$, onde $\delta$ Ã© a deriva e $\epsilon_t$ Ã© um ruÃ­do branco. Expandindo a equaÃ§Ã£o, temos $y_t = y_0 + \delta t + \sum_{i=1}^t \epsilon_i$.
II. O modelo trend-stationary assume que a sÃ©rie Ã© composta por uma tendÃªncia determinÃ­stica, e um componente estacionÃ¡rio.
III. Ao tentar modelar o passeio aleatÃ³rio com deriva como trend-stationary, a tendÃªncia linear  $\delta t$ serÃ¡ estimada.  Removendo essa tendÃªncia, a sÃ©rie resultante Ã© $y_t - \delta t = y_0 + \sum_{i=1}^t \epsilon_i$.  Esta sÃ©rie nÃ£o Ã© estacionÃ¡ria, pois sua variÃ¢ncia cresce com o tempo, $Var(y_0 + \sum_{i=1}^t \epsilon_i) = t \sigma^2$.
IV.  Portanto, um modelo trend-stationary nÃ£o Ã© adequado para sÃ©ries que apresentam comportamento de passeio aleatÃ³rio com deriva. $\blacksquare$

**Lema 1.2:** Se um modelo trend-stationary Ã© aplicado a uma sÃ©rie temporal com raiz unitÃ¡ria, os resÃ­duos do modelo apresentarÃ£o autocorrelaÃ§Ã£o significativa, indicando uma inadequaÃ§Ã£o do modelo.
*Prova:*
I. Uma sÃ©rie temporal com raiz unitÃ¡ria nÃ£o pode ser tornada estacionÃ¡ria pela remoÃ§Ã£o de uma tendÃªncia determinÃ­stica.
II. Se um modelo trend-stationary Ã© aplicado a uma sÃ©rie com raiz unitÃ¡ria, o modelo assume que os resÃ­duos sÃ£o estacionÃ¡rios apÃ³s a remoÃ§Ã£o da tendÃªncia determinÃ­stica.
III. No entanto, os resÃ­duos resultantes ainda conterÃ£o a componente nÃ£o estacionÃ¡ria da sÃ©rie com raiz unitÃ¡ria, que apresentarÃ¡ autocorrelaÃ§Ã£o significativa.
IV. Portanto, a presenÃ§a de autocorrelaÃ§Ã£o nos resÃ­duos indica que o modelo trend-stationary nÃ£o Ã© adequado para modelar uma sÃ©rie temporal com raiz unitÃ¡ria. $\blacksquare$

### Processos com Raiz UnitÃ¡ria
Processos com raiz unitÃ¡ria sÃ£o caracterizados pela presenÃ§a de uma raiz unitÃ¡ria no polinÃ´mio caracterÃ­stico do operador autoregressivo (AR). A forma mais simples de um processo com raiz unitÃ¡ria Ã© o passeio aleatÃ³rio com deriva, que vimos anteriormente [^2]. A forma geral de um processo com raiz unitÃ¡ria Ã©:
$$ (1-L)y_t = \delta + \psi(L)\epsilon_t $$ [^1]
onde:
*   $(1-L)$ Ã© o operador de primeira diferenÃ§a, tal que $(1-L)y_t = y_t - y_{t-1}$.
*   $\delta$ Ã© a deriva ou o crescimento mÃ©dio da sÃ©rie apÃ³s a diferenciaÃ§Ã£o.
*   $\psi(L)\epsilon_t$ Ã© um processo estacionÃ¡rio que modela a dependÃªncia temporal dos resÃ­duos apÃ³s a diferenciaÃ§Ã£o.
*   $\epsilon_t$ Ã© um ruÃ­do branco com mÃ©dia zero e variÃ¢ncia constante $\sigma^2$.

#### RepresentaÃ§Ãµes Equivalentes
O modelo de raiz unitÃ¡ria pode ser expresso de forma equivalente atravÃ©s da forma autoregressiva:
$$ y_t = y_{t-1} + \delta + \psi(L)\epsilon_t $$
Esta forma enfatiza a dependÃªncia da sÃ©rie temporal no seu valor anterior, e como a presenÃ§a do componente $\psi(L)\epsilon_t$ pode ser interpretada como uma perturbaÃ§Ã£o com autocorrelaÃ§Ã£o que se acumula ao longo do tempo, e que impede a sÃ©rie de voltar para um valor mÃ©dio de longo prazo.
Expandindo recursivamente o modelo, temos:
$$ y_t = y_0 + \delta t + \sum_{i=1}^t \psi(L)\epsilon_i $$
onde $y_0$ Ã© o valor inicial da sÃ©rie. Esta forma revela que a sÃ©rie Ã© composta por uma tendÃªncia linear (drift), e por uma soma acumulada de choques que se acumulam ao longo do tempo.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
>  Um exemplo de passeio aleatÃ³rio com deriva (raiz unitÃ¡ria) Ã© dado por:
>   $y_t = y_{t-1} + 0.1 + \epsilon_t$, onde $\epsilon_t$ Ã© um ruÃ­do branco.
>   Nesse caso,  a sÃ©rie Ã© nÃ£o estacionÃ¡ria, porque a sua variÃ¢ncia aumenta linearmente com o tempo.
>   Aplicando a primeira diferenÃ§a, obtemos:
>   $(1-L)y_t = 0.1 + \epsilon_t$.
>   A sÃ©rie diferenciada Ã© estacionÃ¡ria, com mÃ©dia 0.1 e variÃ¢ncia $\sigma^2$.
>
>   Para simular essa sÃ©rie, podemos implementar o seguinte cÃ³digo:
>   ```python
>   import numpy as np
>   import matplotlib.pyplot as plt
>
>   np.random.seed(42)
>   T = 100
>   y = np.zeros(T)
>   delta = 0.1
>   epsilon = np.random.normal(0, 1, T)
>   for t in range(1,T):
>      y[t] = y[t-1] + delta + epsilon[t]
>   t = np.arange(T)
>   plt.figure(figsize=(10, 6))
>   plt.plot(t, y)
>   plt.title('Passeio AleatÃ³rio com Deriva')
>   plt.xlabel('Tempo')
>   plt.ylabel('Valor')
>   plt.grid(True)
>   plt.show()
>   ```
>  O grÃ¡fico resultante mostra uma trajetÃ³ria de uma sÃ©rie com raiz unitÃ¡ria, com crescimento ao longo do tempo, e choques aleatÃ³rios.  Aplicando a diferenÃ§a, obtemos uma sÃ©rie estacionÃ¡ria com mÃ©dia diferente de zero.
>
>   Vamos calcular a sÃ©rie diferenciada, para verificar a sua estacionaridade:
>
>   ```python
>   import numpy as np
>   import matplotlib.pyplot as plt
>
>   np.random.seed(42)
>   T = 100
>   y = np.zeros(T)
>   delta = 0.1
>   epsilon = np.random.normal(0, 1, T)
>   for t in range(1,T):
>       y[t] = y[t-1] + delta + epsilon[t]
>
>   y_diff = np.diff(y)
>   t_diff = np.arange(T-1)
>
>   plt.figure(figsize=(12, 6))
>   plt.subplot(1, 2, 1)
>   plt.plot(np.arange(T), y)
>   plt.title('SÃ©rie Original (Passeio AleatÃ³rio com Deriva)')
>   plt.xlabel('Tempo')
>   plt.ylabel('Valor')
>   plt.grid(True)
>
>   plt.subplot(1, 2, 2)
>   plt.plot(t_diff, y_diff)
>   plt.title('SÃ©rie Diferenciada')
>   plt.xlabel('Tempo')
>   plt.ylabel('Valor')
>   plt.grid(True)
>   plt.tight_layout()
>   plt.show()
>   print(f"MÃ©dia da sÃ©rie diferenciada: {np.mean(y_diff):.4f}")
>   print(f"VariÃ¢ncia da sÃ©rie diferenciada: {np.var(y_diff):.4f}")
>   ```
>
>   O primeiro grÃ¡fico mostra a sÃ©rie original com raiz unitÃ¡ria, e o segundo grÃ¡fico mostra a sÃ©rie diferenciada, que Ã© estacionÃ¡ria em mÃ©dia e variÃ¢ncia.  A mÃ©dia da sÃ©rie diferenciada Ã© prÃ³xima de 0.1 e a sua variÃ¢ncia Ã© prÃ³xima de 1, como na simulaÃ§Ã£o original.

#### CaracterÃ­sticas dos Processos com Raiz UnitÃ¡ria
Os processos com raiz unitÃ¡ria apresentam as seguintes caracterÃ­sticas:
1.  **NÃ£o Estacionaridade:** A mÃ©dia e variÃ¢ncia do processo nÃ£o sÃ£o constantes ao longo do tempo. A variÃ¢ncia da sÃ©rie aumenta linearmente com o tempo.
2. **PersistÃªncia dos Choques:** Choques aleatÃ³rios tÃªm efeitos persistentes no nÃ­vel da sÃ©rie. Um choque nÃ£o se dissipa ao longo do tempo, e afeta o valor da sÃ©rie permanentemente, que Ã© a caracterÃ­stica da raiz unitÃ¡ria.
3. **Necessidade de DiferenciaÃ§Ã£o:** A sÃ©rie original deve ser diferenciada para se tornar estacionÃ¡ria, o que indica a presenÃ§a de uma raiz unitÃ¡ria no operador autoregressivo.
4. **AutocorrelaÃ§Ã£o:** A autocorrelaÃ§Ã£o da sÃ©rie nÃ£o decai rapidamente com o aumento do lag, indicando uma forte persistÃªncia da dependÃªncia temporal.

**Lema 2:** A aplicaÃ§Ã£o do operador de primeira diferenÃ§a em um processo com raiz unitÃ¡ria resulta em um processo estacionÃ¡rio, ao remover a raiz unitÃ¡ria do operador autoregressivo.
*Prova:*
I. Um processo com raiz unitÃ¡ria pode ser expresso como: $(1-L)y_t = \delta + \psi(L)\epsilon_t$, onde $(1-L)$ Ã© o operador de primeira diferenÃ§a.
II. O operador $(1-L)$ remove a raiz unitÃ¡ria da sÃ©rie original $y_t$.
III. A sÃ©rie resultante $(1-L)y_t$ Ã© igual a uma constante $\delta$ mais um processo estacionÃ¡rio $\psi(L)\epsilon_t$, o que torna a sÃ©rie resultante estacionÃ¡ria.
IV. Portanto, a aplicaÃ§Ã£o do operador de primeira diferenÃ§a remove a raiz unitÃ¡ria da sÃ©rie original e resulta em um processo estacionÃ¡rio. $\blacksquare$

**Lema 2.1:** A variÃ¢ncia de um passeio aleatÃ³rio com deriva $y_t = y_{t-1} + \delta + \epsilon_t$ aumenta linearmente com o tempo, dada por $t\sigma^2$, onde $\sigma^2$ Ã© a variÃ¢ncia do ruÃ­do branco.
*Prova:*
I. Expandindo a sÃ©rie, temos $y_t = y_0 + \delta t + \sum_{i=1}^t \epsilon_i$.
II. A variÃ¢ncia da sÃ©rie Ã© $Var(y_t) = Var(y_0 + \delta t + \sum_{i=1}^t \epsilon_i) = Var(\sum_{i=1}^t \epsilon_i)$.
III. Como os $\epsilon_i$ sÃ£o independentes e identicamente distribuÃ­dos com variÃ¢ncia $\sigma^2$, temos $Var(\sum_{i=1}^t \epsilon_i) = \sum_{i=1}^t Var(\epsilon_i) = t\sigma^2$.
IV. Portanto, a variÃ¢ncia de um passeio aleatÃ³rio com deriva aumenta linearmente com o tempo. $\blacksquare$

**Lema 2.2:** Se a sÃ©rie temporal $y_t$ segue um passeio aleatÃ³rio com deriva, entÃ£o sua autocorrelaÃ§Ã£o nÃ£o decai rapidamente com o aumento do lag $k$.
*Prova:*
I. Um passeio aleatÃ³rio com deriva Ã© definido como $y_t = y_{t-1} + \delta + \epsilon_t$.
II. A autocovariÃ¢ncia entre $y_t$ e $y_{t-k}$ Ã© dada por $Cov(y_t, y_{t-k}) = Cov(y_0 + \delta t + \sum_{i=1}^t \epsilon_i, y_0 + \delta(t-k) + \sum_{i=1}^{t-k} \epsilon_i)$.
III. Para $k>0$, essa autocovariÃ¢ncia Ã© dada por $Cov(\sum_{i=t-k+1}^{t} \epsilon_i, \sum_{i=1}^{t-k} \epsilon_i ) + Var(\sum_{i=1}^{t-k} \epsilon_i) = (t-k)\sigma^2$, onde $\sigma^2$ Ã© a variÃ¢ncia do ruÃ­do branco $\epsilon_i$.
IV. Como $Var(y_t) = t\sigma^2$, a autocorrelaÃ§Ã£o $\rho(k) = \frac{Cov(y_t, y_{t-k})}{Var(y_t)} = \frac{(t-k)\sigma^2}{t\sigma^2} = \frac{t-k}{t}$ .
V. Para $k$ pequeno em relaÃ§Ã£o a $t$, a autocorrelaÃ§Ã£o $\rho(k)$ serÃ¡ prÃ³xima de 1, e decai lentamente quando $k$ aumenta. Portanto, a autocorrelaÃ§Ã£o nÃ£o decai rapidamente. $\blacksquare$

### Modelos com IntegraÃ§Ã£o FracionÃ¡ria
Modelos com integraÃ§Ã£o fracionÃ¡ria generalizam o conceito de integraÃ§Ã£o para valores nÃ£o inteiros, permitindo modelar a dependÃªncia de longo prazo em sÃ©ries temporais. Em modelos ARIMA, a sÃ©rie Ã© diferenciada um nÃºmero inteiro de vezes (por exemplo, uma ou duas vezes) para se tornar estacionÃ¡ria. Modelos com integraÃ§Ã£o fracionÃ¡ria podem ser Ãºteis quando a sÃ©rie temporal nÃ£o Ã© totalmente estacionÃ¡ria, mas tambÃ©m nÃ£o Ã© adequadamente modelada por uma diferenÃ§a inteira.
A representaÃ§Ã£o de um modelo com integraÃ§Ã£o fracionÃ¡ria Ã© dada por:
$$ (1-L)^d y_t = \psi(L)\epsilon_t $$
onde:
*   $d$ Ã© um nÃºmero real, que pode ser fracionÃ¡rio, que indica a ordem de integraÃ§Ã£o.
*   $\psi(L)\epsilon_t$ Ã© um processo estacionÃ¡rio (ARMA) que modela os resÃ­duos apÃ³s a aplicaÃ§Ã£o do operador de diferenÃ§a fracionÃ¡ria.

#### Propriedades dos Modelos com IntegraÃ§Ã£o FracionÃ¡ria
As principais propriedades dos modelos com integraÃ§Ã£o fracionÃ¡ria sÃ£o:
1.  **DependÃªncia de Longo Prazo:** A autocorrelaÃ§Ã£o da sÃ©rie decai lentamente, capturando uma memÃ³ria de longo prazo nos dados.
2.  **Flexibilidade:** A ordem de integraÃ§Ã£o $d$ pode assumir qualquer valor real, o que permite ajustar a modelagem da dependÃªncia temporal de forma mais flexÃ­vel.
3.  **GeneralizaÃ§Ã£o dos Modelos ARMA e ARIMA:** Modelos com integraÃ§Ã£o fracionÃ¡ria generalizam os modelos ARMA e ARIMA, sendo que, se $d=1$ ou $d=2$, teremos um modelo ARIMA padrÃ£o.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
>  Um modelo com integraÃ§Ã£o fracionÃ¡ria pode ser modelado como
>  $(1-L)^{0.7} y_t = \epsilon_t$. Nesse modelo, a ordem de integraÃ§Ã£o Ã© 0.7, que nÃ£o Ã© um nÃºmero inteiro. O operador $(1-L)^{0.7}$ pode ser expandido utilizando a expansÃ£o binomial, o que requer uma implementaÃ§Ã£o computacional mais complexa do que para a primeira diferenÃ§a.
>
> A implementaÃ§Ã£o desse tipo de modelo pode ser feita em Python, utilizando a biblioteca statsmodels, ou em R, utilizando o pacote `fracdiff`.
>
>  Vamos simular um processo de integraÃ§Ã£o fracionÃ¡ria para fins de ilustraÃ§Ã£o, usando um mÃ©todo de aproximaÃ§Ã£o da diferenÃ§a fracionÃ¡ria:
>   ```python
>   import numpy as np
>   import matplotlib.pyplot as plt
>   from scipy.special import gamma
>
>   def frac_diff(series, d):
>        weights = [(gamma(i - d) / (gamma(i + 1) * gamma(-d))) for i in range(len(series))]
>        return np.convolve(series, weights, mode='full')[:len(series)]
>
>   np.random.seed(42)
>   T = 100
>   d = 0.7
>   epsilon = np.random.normal(0, 1, T)
>   y = frac_diff(epsilon, -d)
>   t = np.arange(T)
>
>   plt.figure(figsize=(10, 6))
>   plt.plot(t, y)
>   plt.title(f'Processo de IntegraÃ§Ã£o FracionÃ¡ria (d={d})')
>   plt.xlabel('Tempo')
>   plt.ylabel('Valor')
>   plt.grid(True)
>   plt.show()
>   ```
> O grÃ¡fico mostra uma sÃ©rie simulada com integraÃ§Ã£o fracionÃ¡ria, com dependÃªncia de longo prazo, caracterizada por flutuaÃ§Ãµes que persistem por longos perÃ­odos.  Note que a simulaÃ§Ã£o do processo de integraÃ§Ã£o fracionÃ¡ria exige um algoritmo diferente daquele utilizado para o processo de raiz unitÃ¡ria.
>
>  A simulaÃ§Ã£o apresentada Ã© uma simplificaÃ§Ã£o da implementaÃ§Ã£o da diferenÃ§a fracionÃ¡ria, jÃ¡ que o operador $(1-L)^d$ Ã© um operador nÃ£o local, ou seja, depende de todo o histÃ³rico da sÃ©rie temporal, o que implica em um algoritmo que armazena toda a histÃ³ria da sÃ©rie temporal.

**Lema 3:** Modelos com integraÃ§Ã£o fracionÃ¡ria com $0 < d < 1$ exibem um comportamento de memÃ³ria longa, onde a autocorrelaÃ§Ã£o decai hiperbolicamente, o que Ã© mais lento do que o decaimento exponencial em modelos estacionÃ¡rios.
*Prova:*
I. A funÃ§Ã£o de autocorrelaÃ§Ã£o de um processo com integraÃ§Ã£o fracionÃ¡ria decai como $\gamma(k) \approx k^{2d-1}$ para grandes $k$.
II. Quando $0<d<0.5$, a autocorrelaÃ§Ã£o decai para zero, mas de forma lenta (memÃ³ria longa).
III. Quando $0.5 < d < 1$, o processo Ã© nÃ£o estacionÃ¡rio, mas a sÃ©rie Ã© considerada de memÃ³ria longa, e Ã© chamada de persistente.
IV. Modelos estacionÃ¡rios ARMA tem decaimento exponencial na sua funÃ§Ã£o de autocorrelaÃ§Ã£o, o que implica que a dependÃªncia temporal se dissipa rapidamente, diferente do decaimento hiperbÃ³lico dos processos com integraÃ§Ã£o fracionÃ¡ria, que indica memÃ³ria longa. $\blacksquare$

### Modelos com Quebras Estruturais
Modelos com quebras estruturais admitem mudanÃ§as abruptas nos parÃ¢metros do modelo (mÃ©dia, variÃ¢ncia ou autocovariÃ¢ncia) em determinados pontos do tempo. Essas quebras podem ser causadas por eventos externos, como mudanÃ§as de polÃ­tica econÃ´mica, crises financeiras ou choques tecnolÃ³gicos. Um modelo simples de quebra estrutural na tendÃªncia Ã© dado por:
$$ y_t = \alpha_1 + \delta_1 t + \epsilon_t, \text{  para } t < T_b $$
$$ y_t = \alpha_2 + \delta_2 t + \epsilon_t, \text{  para } t \ge T_b $$
onde:
*   $T_b$ representa o ponto de quebra.
*   $\alpha_1$, $\delta_1$ e $\alpha_2$, $\delta_2$ representam os parÃ¢metros da tendÃªncia antes e depois da quebra.
*  $\epsilon_t$ representa um ruÃ­do branco com mÃ©dia zero.

#### Outras formas de quebras estruturais:
* Quebras na variÃ¢ncia
* Quebras na autocovariÃ¢ncia
* Quebras nos coeficientes de modelos AR ou MA.

####  DetecÃ§Ã£o de Quebras Estruturais
A detecÃ§Ã£o de quebras estruturais pode ser feita atravÃ©s de testes estatÃ­sticos, como o teste de Chow, ou mÃ©todos de estimaÃ§Ã£o de parÃ¢metros com mudanÃ§as de regime, como modelos Markov Switching.
> ðŸ’¡ **Exemplo NumÃ©rico:**
>
>   Suponha que temos uma sÃ©rie temporal com uma quebra na tendÃªncia no tempo T=100, simulada da seguinte forma:
>   $y_t = 5 + 0.3t + \epsilon_t$, para t<100
>   $y_t = 2 + 0.7t + \epsilon_t$, para t>=100
>   Onde $\epsilon_t$ Ã© ruÃ­do branco com mÃ©dia zero e variÃ¢ncia 1.
>  Nesse caso, a tendÃªncia da sÃ©rie muda abruptamente no ponto T=100, indicando uma quebra estrutural.

*   **ImplementaÃ§Ã£o em Python**

Para simular e visualizar essa sÃ©rie temporal com quebra estrutural, podemos usar as seguintes bibliotecas em Python:

```python
import numpy as np
import matplotlib.pyplot as plt

# ParÃ¢metros
T = 200
t = np.arange(1, T+1)
epsilon = np.random.normal(0, 1, T)

# SimulaÃ§Ã£o da sÃ©rie temporal
yt = np.zeros(T)
for i in range(T):
    if i < 100:
        yt[i] = 1 + 0.5 * (i+1) + epsilon[i]
    else:
        yt[i] = 2 + 0.7 * (i+1) + epsilon[i]

# Plot
plt.figure(figsize=(10, 6))
plt.plot(t, yt)
plt.xlabel('Tempo (t)')
plt.ylabel('y(t)')
plt.title('SÃ©rie Temporal com Quebra Estrutural')
plt.grid(True)
plt.show()
```

Este cÃ³digo simula uma sÃ©rie temporal com uma quebra estrutural no tempo t=100. A visualizaÃ§Ã£o grÃ¡fica mostra claramente a mudanÃ§a na tendÃªncia da sÃ©rie neste ponto.

*   **Outro exemplo: MudanÃ§a na Sazonalidade**

Considere uma sÃ©rie temporal com uma sazonalidade anual, onde a amplitude da sazonalidade muda apÃ³s um determinado ponto. Por exemplo:

$y_t =  cos(2\pi t/12) + \epsilon_t$, para t < 100

$y_t =  2 cos(2\pi t/12) + \epsilon_t$, para t>=100

>   Onde $\epsilon_t$ Ã© ruÃ­do branco com mÃ©dia zero e variÃ¢ncia 1.

Nesse caso, a amplitude da sazonalidade da sÃ©rie temporal dobra no ponto T=100.

*   **ImplementaÃ§Ã£o em Python**

```python
import numpy as np
import matplotlib.pyplot as plt

# ParÃ¢metros
T = 200
t = np.arange(1, T+1)
epsilon = np.random.normal(0, 0.5, T)

# SimulaÃ§Ã£o da sÃ©rie temporal
yt = np.zeros(T)
for i in range(T):
    if i < 100:
        yt[i] = np.cos(2 * np.pi * (i+1)/12) + epsilon[i]
    else:
        yt[i] = 2 * np.cos(2 * np.pi * (i+1)/12) + epsilon[i]


# Plot
plt.figure(figsize=(10, 6))
plt.plot(t, yt)
plt.xlabel('Tempo (t)')
plt.ylabel('y(t)')
plt.title('SÃ©rie Temporal com MudanÃ§a na Sazonalidade')
plt.grid(True)
plt.show()
```

Este cÃ³digo demonstra uma mudanÃ§a na sazonalidade, onde a amplitude do componente sazonal aumenta apÃ³s o tempo t=100.

### Testes de Quebra Estrutural

Existem diversos testes estatÃ­sticos que podem ser usados para identificar a ocorrÃªncia de quebras estruturais em sÃ©ries temporais. Alguns dos testes mais utilizados incluem:

*   **Teste de Chow:** Este Ã© um teste clÃ¡ssico para detectar quebras estruturais, que assume que a quebra ocorre em um ponto conhecido a priori. Ele compara a soma dos quadrados dos resÃ­duos de dois modelos: um modelo que assume que a sÃ©rie temporal Ã© homogÃªnea e um modelo que assume que a sÃ©rie Ã© dividida em duas partes com parÃ¢metros diferentes.

    $$ F = \frac{(RSS_{homogeneo} - (RSS_1 + RSS_2)) / k}{((RSS_1 + RSS_2)/(n_1 + n_2 - 2k))}$$

    Onde:
    *   $RSS_{homogeneo}$ Ã© a soma dos quadrados dos resÃ­duos do modelo sem quebra.
    *   $RSS_1$ Ã© a soma dos quadrados dos resÃ­duos do modelo antes da quebra.
    *   $RSS_2$ Ã© a soma dos quadrados dos resÃ­duos do modelo depois da quebra.
    *   $k$ Ã© o nÃºmero de parÃ¢metros estimados em cada modelo.
    *   $n_1$ Ã© o nÃºmero de observaÃ§Ãµes antes da quebra.
    *   $n_2$ Ã© o nÃºmero de observaÃ§Ãµes apÃ³s a quebra.

*   **Teste de Quandt:** Este teste Ã© utilizado quando o ponto da quebra nÃ£o Ã© conhecido. Ele avalia a presenÃ§a de uma quebra estrutural, testando todas as possÃ­veis datas de quebra e tomando o valor mÃ¡ximo da estatÃ­stica de teste de Chow.

*   **Teste de Cusum:** Este teste acumula os resÃ­duos do modelo e verifica se eles se desviam significativamente do zero, indicando uma possÃ­vel quebra estrutural.

*   **Teste de Bai-Perron:** Este teste permite a detecÃ§Ã£o de mÃºltiplas quebras estruturais em uma sÃ©rie temporal.

O teste de Chow e o teste de Quandt sÃ£o adequados para detectar quebras estruturais em modelos de regressÃ£o, enquanto o teste de Cusum e o teste de Bai-Perron sÃ£o mais adequados para modelos de sÃ©ries temporais.

<!-- END -->
