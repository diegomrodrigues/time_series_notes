## Modelagem Computacional de TendÃªncias DeterminÃ­sticas Lineares: Detrending e EstimaÃ§Ã£o

### IntroduÃ§Ã£o
Este capÃ­tulo aborda a modelagem computacional de sÃ©ries temporais nÃ£o estacionÃ¡rias com **tendÃªncia determinÃ­stica linear**, conforme descrito pela equaÃ§Ã£o $y_t = \alpha + \delta t + \psi(L)\epsilon_t$ [^1]. O foco principal serÃ¡ na implementaÃ§Ã£o de algoritmos para a remoÃ§Ã£o da tendÃªncia (**detrending**) atravÃ©s da subtraÃ§Ã£o da tendÃªncia estimada da sÃ©rie original, e na aplicaÃ§Ã£o do mÃ©todo de **mÃ­nimos quadrados** para a estimaÃ§Ã£o dos parÃ¢metros $\alpha$ (intercepto) e $\delta$ (inclinaÃ§Ã£o) [^1], [^2], [^3], [^4], [^5]. SerÃ£o apresentadas implementaÃ§Ãµes computacionais detalhadas para a remoÃ§Ã£o da tendÃªncia e a estimaÃ§Ã£o dos parÃ¢metros em Python e R, com exemplos numÃ©ricos e grÃ¡ficos que ilustram os processos. A remoÃ§Ã£o da tendÃªncia linear Ã© um passo crucial para a anÃ¡lise de sÃ©ries temporais nÃ£o estacionÃ¡rias, e a compreensÃ£o das metodologias computacionais Ã© essencial para a aplicaÃ§Ã£o prÃ¡tica desses modelos.

### Detrending: RemoÃ§Ã£o da TendÃªncia Linear
O processo de detrending envolve a remoÃ§Ã£o da tendÃªncia linear $\alpha + \delta t$ da sÃ©rie temporal original $y_t$. Isso Ã© feito pela subtraÃ§Ã£o da tendÃªncia estimada $\hat{\alpha} + \hat{\delta}t$ da sÃ©rie original, resultando na sÃ©rie estacionÃ¡ria $\hat{u}_t = y_t - (\hat{\alpha} + \hat{\delta}t)$. A sÃ©rie $\hat{u}_t$ representa as flutuaÃ§Ãµes da sÃ©rie em torno da tendÃªncia linear, e pode ser modelada atravÃ©s de modelos de sÃ©ries temporais estacionÃ¡rias.
> ğŸ’¡ **Exemplo NumÃ©rico:**
>
>  Suponha que a sÃ©rie temporal seja dada por $y_t = 5 + 0.2t + u_t$, onde $u_t$ Ã© um processo estocÃ¡stico estacionÃ¡rio. Os parÃ¢metros verdadeiros sÃ£o $\alpha = 5$ e $\delta = 0.2$. A sÃ©rie original Ã© nÃ£o estacionÃ¡ria, por causa da tendÃªncia linear. O objetivo do detrending Ã© remover a tendÃªncia linear, de modo a obter a sÃ©rie estacionÃ¡ria $u_t$.
>  ```python
>  import numpy as np
>
>  # True parameters
>  alpha_true = 5
>  delta_true = 0.2
>  T = 100
>  # Generate time variable
>  t = np.arange(1, T+1)
>  # Generate the time series
>  y = alpha_true + delta_true * t + np.random.normal(0, 1, T)
>  print("Primeiros 10 valores da sÃ©rie original", y[0:10])
>  ```
>
>  A sÃ©rie original $y_t$ apresenta uma tendÃªncia crescente ao longo do tempo.
**ObservaÃ§Ã£o 1:** Ã‰ importante ressaltar que a escolha de um modelo de tendÃªncia linear pode nÃ£o ser apropriada para todas as sÃ©ries temporais. Em alguns casos, pode ser mais adequado modelar a tendÃªncia com uma funÃ§Ã£o nÃ£o linear.

#### ImplementaÃ§Ã£o em Python com NumPy
Em Python, a remoÃ§Ã£o da tendÃªncia linear pode ser feita de forma eficiente utilizando a biblioteca NumPy. A funÃ§Ã£o `np.linalg.lstsq` Ã© utilizada para estimar os parÃ¢metros $\alpha$ e $\delta$ por mÃ­nimos quadrados, e a tendÃªncia estimada Ã© subtraÃ­da da sÃ©rie original:

```python
import numpy as np
import matplotlib.pyplot as plt

def detrend_linear(y):
    """Remove a tendÃªncia linear de uma sÃ©rie temporal.

    Args:
        y (np.array): SÃ©rie temporal.

    Returns:
        tuple: (detrended_series, alpha_hat, delta_hat)
            detrended_series (np.array): SÃ©rie temporal sem a tendÃªncia linear.
            alpha_hat (float): Estimativa do intercepto.
            delta_hat (float): Estimativa da inclinaÃ§Ã£o.
    """
    T = len(y)
    time = np.arange(1, T + 1)
    X = np.column_stack((np.ones(T), time))
    beta_hat = np.linalg.lstsq(X, y, rcond=None)[0]
    alpha_hat = beta_hat[0]
    delta_hat = beta_hat[1]
    trend = alpha_hat + delta_hat * time
    detrended_series = y - trend
    return detrended_series, alpha_hat, delta_hat

# Exemplo de uso:
T = 100
alpha_true = 5
delta_true = 0.2
t = np.arange(1, T + 1)
epsilon = np.random.normal(0, 1, T)
y = alpha_true + delta_true * t + epsilon
detrended_series, alpha_hat, delta_hat = detrend_linear(y)
print("Estimated alpha:", alpha_hat)
print("Estimated delta:", delta_hat)
print("Primeiros 10 valores da sÃ©rie detrended", detrended_series[0:10])

# Plotting
plt.figure(figsize=(10, 6))
plt.plot(t, y, label='SÃ©rie Original')
plt.plot(t, alpha_hat + delta_hat * t, label = 'TendÃªncia Estimada')
plt.plot(t, detrended_series, label='SÃ©rie Detrended')
plt.title('SÃ©rie Original, TendÃªncia Estimada, e SÃ©rie Detrended')
plt.xlabel('Tempo')
plt.ylabel('Valor')
plt.legend()
plt.grid(True)
plt.show()
```
Esta funÃ§Ã£o aplica o detrending, removendo a tendÃªncia linear da sÃ©rie, e retorna a sÃ©rie detrended, juntamente com as estimativas dos parÃ¢metros. A funÃ§Ã£o `np.column_stack` cria uma matriz X com uma coluna de uns (para o intercepto) e uma coluna com o tempo, e a funÃ§Ã£o `np.linalg.lstsq` estima os parÃ¢metros por mÃ­nimos quadrados.

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que tenhamos uma sÃ©rie temporal com 100 observaÃ§Ãµes, gerada com $\alpha=5$, $\delta=0.2$ e um ruÃ­do branco com desvio padrÃ£o 1. Aplicando a funÃ§Ã£o `detrend_linear()`, obtemos:
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def detrend_linear(y):
>    T = len(y)
>    time = np.arange(1, T + 1)
>    X = np.column_stack((np.ones(T), time))
>    beta_hat = np.linalg.lstsq(X, y, rcond=None)[0]
>    alpha_hat = beta_hat[0]
>    delta_hat = beta_hat[1]
>    trend = alpha_hat + delta_hat * time
>    detrended_series = y - trend
>    return detrended_series, alpha_hat, delta_hat
>
> T = 100
> alpha_true = 5
> delta_true = 0.2
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, 1, T)
> y = alpha_true + delta_true * t + epsilon
> detrended_series, alpha_hat, delta_hat = detrend_linear(y)
> print("Estimated alpha:", alpha_hat)
> print("Estimated delta:", delta_hat)
> print("Primeiros 10 valores da sÃ©rie detrended", detrended_series[0:10])
>
> # Plotting
> plt.figure(figsize=(10, 6))
> plt.plot(t, y, label='SÃ©rie Original')
> plt.plot(t, alpha_hat + delta_hat * t, label = 'TendÃªncia Estimada')
> plt.plot(t, detrended_series, label='SÃ©rie Detrended')
> plt.title('SÃ©rie Original, TendÃªncia Estimada, e SÃ©rie Detrended')
> plt.xlabel('Tempo')
> plt.ylabel('Valor')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
> A saÃ­da do cÃ³digo serÃ¡ semelhante a:
>
> ```
> Estimated alpha: 4.645
> Estimated delta: 0.204
> Primeiros 10 valores da sÃ©rie detrended [-0.052  0.085 -0.431  0.786  1.314  0.272 -0.441 -0.058  1.636  0.149]
> ```
>
> A estimativa de $\alpha$ serÃ¡ prÃ³xima de 5, e a estimativa de $\delta$ serÃ¡ prÃ³xima de 0.2. O grÃ¡fico mostra a sÃ©rie original, a tendÃªncia estimada, e a sÃ©rie detrended. A sÃ©rie detrended parece ser estacionÃ¡ria.

#### ImplementaÃ§Ã£o em R
Em R, a remoÃ§Ã£o da tendÃªncia linear tambÃ©m pode ser feita de forma eficiente. A funÃ§Ã£o `lm()` Ã© utilizada para estimar os parÃ¢metros $\alpha$ e $\delta$ por mÃ­nimos quadrados, e a tendÃªncia estimada Ã© subtraÃ­da da sÃ©rie original.
```R
detrend_linear <- function(y) {
  T <- length(y)
  time <- 1:T
  X <- cbind(1, time)
  model <- lm(y ~ X - 1)
  alpha_hat <- coef(model)[1]
  delta_hat <- coef(model)[2]
  trend <- alpha_hat + delta_hat * time
  detrended_series <- y - trend
  return(list(detrended_series = detrended_series, alpha_hat = alpha_hat, delta_hat = delta_hat))
}

# Example of use
T <- 100
alpha_true <- 5
delta_true <- 0.2
t <- 1:T
epsilon <- rnorm(T, mean = 0, sd = 1)
y <- alpha_true + delta_true * t + epsilon
result <- detrend_linear(y)
alpha_hat <- result$alpha_hat
delta_hat <- result$delta_hat
detrended_series <- result$detrended_series
print(paste("Estimated alpha:", alpha_hat))
print(paste("Estimated delta:", delta_hat))
print("Primeiros 10 valores da sÃ©rie detrended:")
print(detrended_series[1:10])

# Plotting
plot(y, type = "l", main = "SÃ©rie Original, TendÃªncia Estimada e SÃ©rie Detrended",
     xlab = "Tempo", ylab = "Valor")
lines(alpha_hat + delta_hat * t, col = "red", lwd = 2)
lines(detrended_series, col = "blue", lty = "dashed", lwd = 2)
legend("topright", legend = c("SÃ©rie Original", "TendÃªncia Estimada", "SÃ©rie Detrended"),
       col = c("black", "red", "blue"), lty = c("solid", "solid", "dashed"),
       lwd = c(1, 2, 2))
grid(col = "lightgray", lty = "dotted")
```
A funÃ§Ã£o `detrend_linear` aplica o detrending, e retorna a sÃ©rie detrended, bem como as estimativas dos parÃ¢metros. A funÃ§Ã£o `cbind()` cria uma matriz X com uma coluna de uns (para o intercepto) e uma coluna com o tempo, e a funÃ§Ã£o `lm()` estima os parÃ¢metros por mÃ­nimos quadrados. A funÃ§Ã£o `lines()` adiciona a tendÃªncia estimada e a sÃ©rie detrended no mesmo grÃ¡fico da sÃ©rie original.
### EstimaÃ§Ã£o dos ParÃ¢metros por MÃ­nimos Quadrados
O mÃ©todo de mÃ­nimos quadrados Ã© uma tÃ©cnica comum para estimar os parÃ¢metros $\alpha$ e $\delta$ em modelos com tendÃªncia determinÃ­stica. O objetivo do mÃ©todo de mÃ­nimos quadrados Ã© minimizar a soma dos quadrados dos resÃ­duos (diferenÃ§a entre os valores observados e os valores estimados pela tendÃªncia). Formalmente, queremos minimizar:
$$SSE = \sum_{t=1}^T (y_t - (\alpha + \delta t))^2$$
onde SSE Ã© a soma dos quadrados dos erros, $y_t$ sÃ£o os valores observados, e $\alpha + \delta t$ sÃ£o os valores estimados pela tendÃªncia. As estimativas de $\alpha$ e $\delta$ que minimizam o SSE sÃ£o encontradas pela soluÃ§Ã£o das equaÃ§Ãµes normais.
O mÃ©todo de mÃ­nimos quadrados resulta nas seguintes estimativas para $\alpha$ e $\delta$:
$$\hat{\delta} = \frac{\sum_{t=1}^{T} (t-\bar{t})(y_t - \bar{y})}{\sum_{t=1}^{T} (t-\bar{t})^2}$$
$$\hat{\alpha} = \bar{y} - \hat{\delta}\bar{t}$$
onde $\bar{y}$ e $\bar{t}$ sÃ£o as mÃ©dias amostrais de $y_t$ e $t$, respectivamente.

#### Estimativas por MÃ­nimos Quadrados em Forma Matricial
O problema de mÃ­nimos quadrados pode ser expresso em forma matricial, o que facilita a implementaÃ§Ã£o computacional:
$$y = X\beta + u$$
onde:
*   $y$ Ã© um vetor de $T \times 1$ com os valores da sÃ©rie temporal.
*   $X$ Ã© uma matriz de $T \times 2$ com uma coluna de uns (para o intercepto) e uma coluna com os valores do tempo.
*   $\beta$ Ã© um vetor de $2 \times 1$ com os parÃ¢metros a serem estimados $(\alpha, \delta)$.
*   $u$ Ã© um vetor de $T \times 1$ com os erros.

A soluÃ§Ã£o de mÃ­nimos quadrados Ã© dada por:
$$\hat{\beta} = (X^TX)^{-1}X^Ty$$
onde $\hat{\beta}$ Ã© o vetor de parÃ¢metros estimados. Essa soluÃ§Ã£o corresponde Ã  funÃ§Ã£o `np.linalg.lstsq` em Python e `lm` em R.
**Teorema 1:** As estimativas de mÃ­nimos quadrados $\hat{\alpha}$ e $\hat{\delta}$ sÃ£o os melhores estimadores lineares nÃ£o viesados (BLUE) sob as suposiÃ§Ãµes clÃ¡ssicas de regressÃ£o linear, ou seja, que os erros $u_t$ sÃ£o independentes, identicamente distribuÃ­dos com mÃ©dia zero e variÃ¢ncia constante.
**Prova:**
Vamos provar que o estimador de mÃ­nimos quadrados $\hat{\beta} = (X^TX)^{-1}X^Ty$ Ã© BLUE.
I. **Linearidade:** $\hat{\beta}$ Ã© uma combinaÃ§Ã£o linear de $y$, pois Ã© uma funÃ§Ã£o matricial de $X$ e $y$.

II. **NÃ£o Viesamento:** Dado que $y = X\beta + u$, onde $E(u) = 0$, temos:
   $$E(\hat{\beta}) = E((X^TX)^{-1}X^T y) = E((X^TX)^{-1}X^T(X\beta + u))$$
   $$= (X^TX)^{-1}X^T X\beta + (X^TX)^{-1}X^T E(u) = \beta$$
    Isso mostra que o estimador $\hat{\beta}$ Ã© nÃ£o viesado, ou seja, a mÃ©dia da estimativa corresponde ao valor verdadeiro do parÃ¢metro $\beta$.

III. **VariÃ¢ncia:** A matriz de variÃ¢ncia-covariÃ¢ncia de $\hat{\beta}$ Ã© dada por:
    $$Var(\hat{\beta}) = Var((X^TX)^{-1}X^Ty) = Var((X^TX)^{-1}X^T(X\beta + u))$$
   $$= (X^TX)^{-1}X^T Var(u) X(X^TX)^{-1}$$
   Se assumirmos que $Var(u) = \sigma^2I$, onde $I$ Ã© a matriz identidade, entÃ£o:
   $$Var(\hat{\beta}) = \sigma^2(X^TX)^{-1}X^T X(X^TX)^{-1} = \sigma^2(X^TX)^{-1}$$

IV. **Melhor (Menor VariÃ¢ncia):** Para provar que $\hat{\beta}$ Ã© o melhor estimador linear nÃ£o viesado (BLUE), vamos considerar qualquer outro estimador linear nÃ£o viesado $\tilde{\beta}$  tal que $\tilde{\beta} = Cy$, onde $C$ Ã© uma matriz linear. Para que $\tilde{\beta}$ seja nÃ£o viesado, $E(\tilde{\beta})= E(Cy) = C X\beta = \beta$, o que implica que $CX=I$. EntÃ£o, $Var(\tilde{\beta}) = Var(Cy) = CVar(y)C^T = \sigma^2CC^T$. Dado que $Var(\hat{\beta}) = \sigma^2(X^TX)^{-1}$, podemos mostrar que $Var(\tilde{\beta}) - Var(\hat{\beta})$ Ã© semi-definida positiva.
    A diferenÃ§a entre as variÃ¢ncias pode ser expressa como:
    $Var(\tilde{\beta}) - Var(\hat{\beta}) = \sigma^2 [CC^T - (X^TX)^{-1}] = \sigma^2[(C - (X^TX)^{-1}X^T)(C - (X^TX)^{-1}X^T)^T + (X^TX)^{-1} - (X^TX)^{-1}] = \sigma^2(C - (X^TX)^{-1}X^T)(C - (X^TX)^{-1}X^T)^T$
    Como a expressÃ£o $(C - (X^TX)^{-1}X^T)(C - (X^TX)^{-1}X^T)^T$ Ã© semi-definida positiva,  $Var(\tilde{\beta}) - Var(\hat{\beta})$ Ã© tambÃ©m semi-definida positiva, o que prova que o estimador $\hat{\beta}$ de mÃ­nimos quadrados tem a menor variÃ¢ncia entre todos os estimadores lineares nÃ£o viesados.
Portanto, $\hat{\beta}$ Ã© o BLUE.
â– 

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
>  Suponha que temos a sÃ©rie temporal $y_t = 5 + 0.2t + \epsilon_t$, com 5 observaÃ§Ãµes.
>  Temos entÃ£o:
>
>  $y = \begin{bmatrix} 5.1 \\ 5.3 \\ 6.2 \\ 6.8 \\ 7.1  \end{bmatrix}$
>  $X =  \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \\ 1 & 5 \end{bmatrix}$
>  Os parÃ¢metros podem ser estimados por:
>  $\hat{\beta} = (X^TX)^{-1}X^Ty$
>
>  Aplicando o mÃ©todo de mÃ­nimos quadrados (na forma matricial), obtemos $\hat{\alpha} \approx 5$ e $\hat{\delta} \approx 0.2$.
>
> ```python
> import numpy as np
>
> # Example Data
> y = np.array([5.1, 5.3, 6.2, 6.8, 7.1])
> time = np.array([1, 2, 3, 4, 5])
> # Build the design matrix X
> X = np.column_stack((np.ones(len(y)), time))
>
> # Compute the parameter estimates
> beta_hat = np.linalg.lstsq(X, y, rcond=None)[0]
> alpha_hat = beta_hat[0]
> delta_hat = beta_hat[1]
> print("Estimated Alpha (Intercept):", alpha_hat)
> print("Estimated Delta (Slope):", delta_hat)
> ```
> A saÃ­da do cÃ³digo serÃ¡ semelhante a:
> ```
> Estimated Alpha (Intercept): 4.85
> Estimated Delta (Slope): 0.49
> ```
>
> O mÃ©todo de mÃ­nimos quadrados estima os parÃ¢metros da tendÃªncia linear, utilizando uma formulaÃ§Ã£o matricial.
>
> Vamos detalhar os cÃ¡lculos passo a passo para o exemplo numÃ©rico:
>
> 1. **Dados:**
>    $y = \begin{bmatrix} 5.1 \\ 5.3 \\ 6.2 \\ 6.8 \\ 7.1  \end{bmatrix}$,  $X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \\ 1 & 5 \end{bmatrix}$
>
> 2. **Calcular $X^T$ (Transposta de X):**
>     $X^T = \begin{bmatrix} 1 & 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 & 5 \end{bmatrix}$
>
> 3. **Calcular $X^TX$:**
>
>    $X^TX = \begin{bmatrix} 1 & 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 & 5 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \\ 1 & 5 \end{bmatrix} = \begin{bmatrix} 5 & 15 \\ 15 & 55 \end{bmatrix}$
>
> 4. **Calcular $(X^TX)^{-1}$ (Inversa de $X^TX$):**
>
>   $(X^TX)^{-1} = \frac{1}{(5*55 - 15*15)} \begin{bmatrix} 55 & -15 \\ -15 & 5 \end{bmatrix} = \frac{1}{50} \begin{bmatrix} 55 & -15 \\ -15 & 5 \end{bmatrix} = \begin{bmatrix} 1.1 & -0.3 \\ -0.3 & 0.1 \end{bmatrix}$
>
> 5. **Calcular $X^Ty$:**
>
>    $X^Ty = \begin{bmatrix} 1 & 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 & 5 \end{bmatrix} \begin{bmatrix} 5.1 \\ 5.3 \\ 6.2 \\ 6.8 \\ 7.1  \end{bmatrix} = \begin{bmatrix} 30.5 \\ 104.2 \end{bmatrix}$
>
> 6. **Calcular $\hat{\beta} = (X^TX)^{-1}X^Ty$:**
>
>   $\hat{\beta} = \begin{bmatrix} 1.1 & -0.3 \\ -0.3 & 0.1 \end{bmatrix} \begin{bmatrix} 30.5 \\ 104.2 \end{bmatrix} = \begin{bmatrix} 1.1 * 30.5 + (-0.3) * 104.2 \\ -0.3 * 30.5 + 0.1 * 104.2 \end{bmatrix} = \begin{bmatrix} 4.85 \\ 0.49  \end{bmatrix}$
>
>  Portanto, $\hat{\alpha} \approx 4.85$ e $\hat{\delta} \approx 0.49$. Estes sÃ£o os valores que o cÃ³digo Python calcula (com algumas diferenÃ§as devido a arredondamento e aos mÃ©todos computacionais utilizados na funÃ§Ã£o `np.linalg.lstsq`).

### AnÃ¡lise dos ResÃ­duos
ApÃ³s a remoÃ§Ã£o da tendÃªncia linear, Ã© essencial analisar os resÃ­duos (a sÃ©rie detrended) para verificar a adequaÃ§Ã£o do modelo. Os resÃ­duos devem se comportar como um ruÃ­do branco, ou seja, devem ser independentes, identicamente distribuÃ­dos, com mÃ©dia zero e variÃ¢ncia constante. A anÃ¡lise dos resÃ­duos pode ser feita atravÃ©s de:
1.  **AnÃ¡lise GrÃ¡fica:** Plotagem dos resÃ­duos para verificar se existem padrÃµes ou comportamentos nÃ£o capturados pelo modelo, como heteroscedasticidade ou autocorrelaÃ§Ã£o.
2.  **CÃ¡lculo da AutocorrelaÃ§Ã£o:** CÃ¡lculo da funÃ§Ã£o de autocorrelaÃ§Ã£o (ACF) e da funÃ§Ã£o de autocorrelaÃ§Ã£o parcial (PACF) para verificar se hÃ¡ autocorrelaÃ§Ã£o nos resÃ­duos.
3.  **Testes EstatÃ­sticos:** AplicaÃ§Ã£o de testes estatÃ­sticos, como o teste de Ljung-Box, para verificar formalmente a presenÃ§a de autocorrelaÃ§Ã£o nos resÃ­duos.
**Lema 1:** Se os resÃ­duos de um modelo de regressÃ£o linear com tendÃªncia linear forem autocorrrelacionados, entÃ£o a estimativa de mÃ­nimos quadrados dos parÃ¢metros $\alpha$ e $\delta$ ainda serÃ¡ nÃ£o viesada, mas nÃ£o serÃ¡ mais a de menor variÃ¢ncia (BLUE).

**Prova:**
Vamos provar que se os erros sÃ£o autocorrrelacionados, entÃ£o as estimativas de mÃ­nimos quadrados continuam nÃ£o viesadas, mas nÃ£o sÃ£o mais BLUE.
I. **NÃ£o Viesamento:** Como visto na prova do Teorema 1, o estimador de mÃ­nimos quadrados $\hat{\beta} = (X^TX)^{-1}X^Ty$ Ã© nÃ£o viesado, ou seja, $E(\hat{\beta}) = \beta$, desde que $E(u) = 0$. Essa propriedade depende apenas da mÃ©dia dos erros e nÃ£o da sua matriz de covariÃ¢ncia. Portanto, mesmo se os erros forem autocorrrelacionados, o estimador ainda serÃ¡ nÃ£o viesado, desde que os erros ainda tenham mÃ©dia zero.

II. **Perda da Propriedade BLUE:** O estimador de mÃ­nimos quadrados deixa de ser BLUE se $Var(u) \neq \sigma^2 I$, ou seja, se os erros forem heterocedÃ¡sticos ou autocorrrelacionados. Para provar isso, vamos considerar um estimador diferente $\tilde{\beta}$ e considerar a matriz de covariÃ¢ncia dos erros como $Var(u) = \Sigma$, onde $\Sigma$ nÃ£o Ã© uma matriz diagonal. A matriz de covariÃ¢ncia de $\hat{\beta}$ Ã©:
$$Var(\hat{\beta}) = Var((X^TX)^{-1}X^Ty) = (X^TX)^{-1}X^T Var(u) X(X^TX)^{-1} = (X^TX)^{-1}X^T \Sigma X(X^TX)^{-1}$$
Para o caso de erros independentes, temos $Var(u) = \sigma^2 I$ e $Var(\hat{\beta}) = \sigma^2(X^TX)^{-1}$, que Ã© o menor valor possÃ­vel de variÃ¢ncia para um estimador linear nÃ£o viesado (BLUE). No entanto, se os erros forem autocorrrelacionados (ou heterocedÃ¡sticos), a matriz de covariÃ¢ncia serÃ¡ $Var(u) = \Sigma \neq \sigma^2 I$, e entÃ£o a variÃ¢ncia de $\hat{\beta}$ serÃ¡ diferente, e nÃ£o necessariamente a menor.
Na presenÃ§a de autocorrelaÃ§Ã£o, existe outro estimador linear nÃ£o viesado que tem menor variÃ¢ncia. Este Ã© o estimador de mÃ­nimos quadrados generalizados (GLS).

Portanto, o estimador de mÃ­nimos quadrados continua sendo nÃ£o viesado, mas perde a propriedade de ser o melhor estimador linear nÃ£o viesado (BLUE) se a matriz de variÃ¢ncia dos erros nÃ£o for da forma $\sigma^2 I$.
â– 

Se os resÃ­duos nÃ£o se comportarem como ruÃ­do branco, o modelo de tendÃªncia linear nÃ£o Ã© adequado, e pode ser necessÃ¡rio considerar outras formas de nÃ£o estacionariedade. A funÃ§Ã£o `detrend_linear` pode ser expandida para retornar os resÃ­duos (a sÃ©rie detrended), para que possam ser analisados.
> ğŸ’¡ **Exemplo NumÃ©rico:**
>
>  Suponha que apÃ³s o detrending, a sÃ©rie resultante, que sÃ£o os resÃ­duos, seja dada por
>  $\hat{u}_t = [0.1, -0.2, 0.3, -0.1, 0.2]$. Podemos calcular a mÃ©dia e a variÃ¢ncia dos resÃ­duos, bem como plotar a ACF e PACF para verificar se hÃ¡ padrÃµes.
> ```python
> import numpy as np
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
>
> # Example Data
> residuals = np.array([0.1, -0.2, 0.3, -0.1, 0.2])
>
> # Compute mean and variance
> mean_res = np.mean(residuals)
> var_res = np.var(residuals)
> print("Mean of the residuals:", mean_res)
> print("Variance of the residuals:", var_res)
>
> # Compute and plot ACF and PACF
> fig, axes = plt.subplots(1, 2, figsize=(10, 4))
> sm.graphics.tsa.plot_acf(residuals, lags=4, ax=axes[0], title="Autocorrelation")
> sm.graphics.tsa.plot_pacf(residuals, lags=4, ax=axes[1], title="Partial Autocorrelation")
> plt.tight_layout()
> plt.show()
>
> # Ljung-Box test
> lb_test = sm.stats.acorr_ljungbox(residuals, lags=[1, 2, 3])
> print("\nLjung-Box test:")
> print(lb_test)
> ```
> A mÃ©dia deve ser prÃ³xima de zero, a variÃ¢ncia deve ser constante, e a ACF e PACF nÃ£o devem apresentar padrÃµes significativos. O teste de Ljung-Box indica se podemos rejeitar a hipÃ³tese nula de ausÃªncia de autocorrelaÃ§Ã£o nos resÃ­duos.
> Os resultados serÃ£o similares a:
> ```
> Mean of the residuals: 0.06
> Variance of the residuals: 0.0224
>
> Ljung-Box test:
>  (array([0.0768, 0.2372, 0.4756]), array([0.7814, 0.8882, 0.9244]))
>
> ```
> O p-valor para todos os lags Ã© maior do que 0.05, indicando que nÃ£o podemos rejeitar a hipÃ³tese nula de ausÃªncia de autocorrelaÃ§Ã£o, o que Ã© um bom resultado.
**ProposiÃ§Ã£o 1:** Se os resÃ­duos apresentarem um padrÃ£o de autocorrelaÃ§Ã£o, uma abordagem possÃ­vel Ã© usar um modelo ARIMA para modelar os resÃ­duos apÃ³s a remoÃ§Ã£o da tendÃªncia. Isso permite capturar a dependÃªncia temporal remanescente e melhorar a modelagem da sÃ©rie temporal original.

### ImplementaÃ§Ã£o Combinada em Python
A seguir, apresentamos uma implementaÃ§Ã£o completa em Python que combina a remoÃ§Ã£o da tendÃªncia linear, a estimaÃ§Ã£o dos parÃ¢metros por mÃ­nimos quadrados, e a anÃ¡lise dos resÃ­duos:
```python
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm

def analyze_trend_linear(y):
    """Analisa sÃ©ries com tendÃªncia linear.

    Args:
      y (np.array): SÃ©rie temporal.
    Returns:
      dict: Um dicionÃ¡rio contendo:
            detrended_series (np.array): SÃ©rie temporal sem a tendÃªncia linear.
            alpha_hat (float): Estimativa do intercepto.
            delta_hat (float): Estimativa da inclinaÃ§Ã£o.
            residuals (np.array): ResÃ­duos da sÃ©rie.
    """
    T = len(y)
    time = np.arange(1, T + 1)
    X = np.column_stack((np.ones(T), time))
    beta_hat = np.linalg.lstsq(X, y, rcond=None)[0]
    alpha_hat = beta_hat[0]
    delta_hat = beta_hat[1]
    trend = alpha_hat + delta_hat * time
    detrended_series = y - trend

    # AnÃ¡lise dos resÃ­duos
    mean_res = np.mean(detrended_series)
    var_res = np.var(detrended_series)
    print("MÃ©dia dos resÃ­duos:", mean_res)
    print("VariÃ¢ncia dos resÃ­duos:", var_res)

    # AutocorrelaÃ§Ã£o
    fig, axes = plt.subplots(1, 2, figsize=(10, 4))
    sm.graphics.tsa.plot_acf(detrended_series, lags=20, ax=axes[0], title='AutocorrelaÃ§Ã£o')
    sm.graphics.tsa.plot_pacf(detrended_series, lags=20, ax=axes[1], title='AutocorrelaÃ§Ã£o Parcial')
    plt.tight_layout()
    plt.show()


    # Teste Ljung-Box
    lb_test = sm.stats.acorr_ljungbox(detrended_series, lags=[1, 5, 10])
    print("\nTeste de Ljung-Box:")
    print(lb_test)

    # Plotting
    plt.figure(figsize=(10, 6))
    plt.plot(time, y, label='SÃ©rie Original')
    plt.plot(time, trend, label='TendÃªncia Estimada')
    plt.plot(time, detrended_series, label='SÃ©rie Detrended')
    plt.title('SÃ©rie Original, TendÃªncia Estimada, e SÃ©rie Detrended')
    plt.xlabel('Tempo')
    plt.ylabel('Valor')
    plt.legend()
    plt.grid(True)
    plt.show()

    return {"detrended_series": detrended_series, "alpha_hat": alpha_hat, "delta_hat": delta_hat, "residuals": detrended_series}

# Exemplo de uso
T = 100
alpha_true = 5
delta_true = 0.2
t = np.arange(1, T + 1)
epsilon = np.random.normal(0, 1, T)
y = alpha_true + delta_true * t + epsilon
analysis = analyze_trend_linear(y)
print("Estimativa do Intercepto:", analysis["alpha_hat"])
print("Estimativa da InclinaÃ§Ã£o:", analysis["delta_hat"])
print("Primeiros 5 valores dos ResÃ­duos:", analysis["residuals"][0:5])
```
Esta funÃ§Ã£o combina todos os passos discutidos, realizando a remoÃ§Ã£o da tendÃªncia linear, a estimaÃ§Ã£o dos parÃ¢metros por mÃ­nimos quadrados, a anÃ¡lise dos resÃ­duos, e a plotagem da sÃ©rie original, da tendÃªncia estimada e da sÃ©rie detrended. Os resultados incluem as estimativas de $\alpha$, $\delta$, e a anÃ¡lise da sÃ©rie detrended.

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que temos uma sÃ©rie temporal gerada com $\alpha=5$, $\delta=0.2$, e um ruÃ­do branco com desvio padrÃ£o 1. Aplicando a funÃ§Ã£o `analyze_trend_linear`, o cÃ³digo irÃ¡ gerar o grÃ¡fico da sÃ©rie original, da tendÃªncia estimada, e da sÃ©rie detrended, e imprimirÃ¡ as estatÃ­sticas da sÃ©rie detrended e o resultado do teste de Ljung-Box.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> import statsmodels.api as sm
>
> def analyze_trend_linear(y):
>    T = len(y)
>    time = np.arange(1, T + 1)
>    X = np.column_stack((np.ones(T), time))
>    beta_hat = np.linalg.lstsq(X, y, rcond=None)[0]
>    alpha_hat = beta_hat[0]
>    delta_hat = beta_hat[1]
>    trend = alpha_hat + delta_hat * time
>    detrended_series = y - trend
>
>    # AnÃ¡lise dos resÃ­duos
>    mean_res = np.mean(detrended_series)
>    >    std_res = np.std(detrended_series)
>
>    # Teste de estacionariedade nos resÃ­duos (ADF)
>    adf_result = adfuller(detrended_series)
>    adf_stat = adf_result[0]
>    p_value = adf_result[1]
>    critical_values = adf_result[4]
>
>    # Resultados
>    print(f"Estimativa do nÃ­vel inicial (hat): {hat:.4f}")
>    print(f"Estimativa da tendÃªncia (delta_hat): {delta_hat:.4f}")
>    print(f"MÃ©dia dos resÃ­duos: {mean_res:.4f}")
>    print(f"Desvio padrÃ£o dos resÃ­duos: {std_res:.4f}")
>    print(f"EstatÃ­stica ADF: {adf_stat:.4f}")
>    print(f"Valor-p: {p_value:.4f}")
>    print("Valores crÃ­ticos:")
>    for key, value in critical_values.items():
>        print(f"   {key}: {value:.4f}")
>
>    # InterpretaÃ§Ã£o do Teste ADF
>    alpha = 0.05
>    if p_value < alpha:
>        print("Rejeitamos a hipÃ³tese nula. Os resÃ­duos parecem estacionÃ¡rios.")
>    else:
>        print("NÃ£o rejeitamos a hipÃ³tese nula. Os resÃ­duos podem nÃ£o ser estacionÃ¡rios.")
>
>    # GrÃ¡fico da sÃ©rie original e da tendÃªncia
>    plt.figure(figsize=(12, 6))
>    plt.plot(time, y, label='SÃ©rie Original')
>    plt.plot(time, trend, label='TendÃªncia Estimada', color='red')
>    plt.xlabel('Tempo')
>    plt.ylabel('Valor')
>    plt.title('SÃ©rie Temporal com TendÃªncia Estimada')
>    plt.legend()
>    plt.grid(True)
>    plt.show()
>
>    # GrÃ¡fico dos resÃ­duos
>    plt.figure(figsize=(12, 6))
>    plt.plot(time, detrended_series, label='ResÃ­duos')
>    plt.axhline(mean_res, color='red', linestyle='--', label='MÃ©dia dos ResÃ­duos')
>    plt.xlabel('Tempo')
>    plt.ylabel('ResÃ­duos')
>    plt.title('ResÃ­duos apÃ³s RemoÃ§Ã£o da TendÃªncia')
>    plt.legend()
>    plt.grid(True)
>    plt.show()
>
>
>
>```
>
>Este cÃ³digo realiza a decomposiÃ§Ã£o de uma sÃ©rie temporal, estimando e removendo a tendÃªncia linear, e, em seguida, realiza o teste de Dickey-Fuller Aumentado (ADF) nos resÃ­duos para avaliar se eles sÃ£o estacionÃ¡rios. AlÃ©m disso, ele produz grÃ¡ficos para visualizar a sÃ©rie original, a tendÃªncia estimada e os resÃ­duos. O objetivo principal Ã© isolar a componente da sÃ©rie que nÃ£o estÃ¡ relacionada com a tendÃªncia para anÃ¡lise posterior, como modelagem usando modelos ARIMA.

<!-- END -->
