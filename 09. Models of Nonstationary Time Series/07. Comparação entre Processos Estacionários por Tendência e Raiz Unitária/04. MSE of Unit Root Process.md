## An√°lise do Erro Quadr√°tico M√©dio (MSE) em Processos N√£o Estacion√°rios

### Introdu√ß√£o
Nos cap√≠tulos anteriores, exploramos a proje√ß√£o linear em processos estacion√°rios por tend√™ncia e de raiz unit√°ria, com particular aten√ß√£o ao modelo ARIMA(0,1,1) e ao passeio aleat√≥rio com deriva [^1, ^2, ^3]. Agora, vamos focar na an√°lise do **Erro Quadr√°tico M√©dio (MSE)** das previs√µes em processos n√£o estacion√°rios, especialmente no que se refere ao seu comportamento em rela√ß√£o ao horizonte de previs√£o. Esta an√°lise √© crucial para entender a precis√£o das previs√µes e como ela se comporta conforme nos movemos para o futuro. Vamos examinar como o MSE se comporta em processos de raiz unit√°ria e comparar com o comportamento em modelos estacion√°rios por tend√™ncia. Especificamente, vamos derivar o MSE para um modelo ARIMA(0,1,1) e demonstrar como o MSE se comporta com o horizonte de previs√£o $s$.

### MSE em Processos Estacion√°rios por Tend√™ncia
Para um processo estacion√°rio por tend√™ncia, a proje√ß√£o linear √© dada por:
$$ \hat{y}_{t+s|t} = \alpha + \delta(t+s) + \psi_s\epsilon_t + \psi_{s+1}\epsilon_{t-1} + \psi_{s+2}\epsilon_{t-2} + \ldots $$
O erro de previs√£o √© definido como a diferen√ßa entre o valor real $y_{t+s}$ e a previs√£o $\hat{y}_{t+s|t}$. O MSE √© a esperan√ßa matem√°tica do quadrado desse erro:
$$ MSE = E[(y_{t+s} - \hat{y}_{t+s|t})^2] $$
Para um processo estacion√°rio por tend√™ncia, como os choques tem efeito transit√≥rio, o MSE converge para um valor fixo √† medida que o horizonte de previs√£o ($s$) aumenta [^2]. O MSE converge para a vari√¢ncia incondicional do componente estacion√°rio  $\psi(L)\epsilon_t$, como visto no cap√≠tulo 15 [^1]. Ou seja, quando o horizonte de previs√£o se torna muito grande, o efeito das inova√ß√µes passadas na previs√£o se torna desprez√≠vel, e a √∫nica fonte de incerteza √© a variabilidade inerente ao componente estoc√°stico, como demonstrado no Lema 1 do cap√≠tulo anterior [^2].

**Lema 0.1:** O MSE de um processo estacion√°rio por tend√™ncia converge para a vari√¢ncia incondicional do componente estoc√°stico quando $s \to \infty$.

*Proof:*
I. A proje√ß√£o linear para um processo estacion√°rio por tend√™ncia √© dada por $\hat{y}_{t+s|t} = \alpha + \delta(t+s) + \sum_{i=0}^\infty \psi_{s+i}\epsilon_{t-i}$.
II. O erro de previs√£o √© $e_{t+s|t} = y_{t+s} - \hat{y}_{t+s|t}$.
III. Para um processo estacion√°rio, o efeito das inova√ß√µes passadas diminui √† medida que o horizonte de previs√£o $s$ aumenta, ou seja, $\psi_{s+i}$ tende a 0 quando $s \to \infty$.
IV. Consequentemente, o MSE, que √© dado por $E[e_{t+s|t}^2]$, converge para a vari√¢ncia do componente estoc√°stico quando $s \to \infty$. A vari√¢ncia do componente estoc√°stico $\psi(L)\epsilon_t$  √© dada por $\sigma^2 \sum_{i=0}^{\infty} \psi_i^2 $, que √© um valor finito devido √† estacionariedade do processo.
V. Portanto, o MSE converge para um valor fixo, que representa a incerteza inerente do processo, quando $s \to \infty$.
‚ñ†

Este lema refor√ßa a intui√ß√£o de que, em processos estacion√°rios por tend√™ncia, a capacidade de previs√£o n√£o se deteriora indefinidamente com o aumento do horizonte de previs√£o, e o erro de previs√£o tende a se estabilizar na variabilidade do componente estoc√°stico. Agora, vamos contrastar esse comportamento com o que observamos em processos de raiz unit√°ria.

> üí° **Exemplo Num√©rico:**
> Suponha que temos um processo estacion√°rio por tend√™ncia com um componente estoc√°stico modelado por um MA(1) onde $\psi_1 = 0.7$ e $\epsilon_t \sim N(0,1)$. O MSE, quando $s$ tende ao infinito, se aproxima da vari√¢ncia incondicional do componente estoc√°stico, que √© $\sigma^2(1 + \psi_1^2)$. Nesse caso, temos:
>
> $Var(\psi(L)\epsilon_t) = 1 * (1 + 0.7^2) = 1.49$.
>
> Isso significa que o MSE se aproximar√° de 1.49 quando o horizonte de previs√£o ($s$) for grande.
>
> ```mermaid
> graph LR
>     A[s=0] --> B(s=1, MSE=...)
>     B --> C(s=5, MSE=...)
>     C --> D(s=10, MSE=...)
>     D --> E(s=inf, MSE=1.49)
>     style A fill:#f9f,stroke:#333,stroke-width:2px
> ```
> O gr√°fico ilustra que, √† medida que *s* aumenta, o MSE se estabiliza em um valor pr√≥ximo a 1.49. Para valores pequenos de *s*, o MSE pode ser menor ou maior dependendo do efeito inicial dos erros passados, mas √† medida que *s* se torna grande, o efeito desses erros se dissipa, e a vari√¢ncia do componente estoc√°stico domina o valor do MSE.
>
> Note que em um processo estacion√°rio por tend√™ncia a capacidade de previs√£o n√£o se deteriora indefinidamente com o aumento do horizonte de previs√£o, diferentemente do que acontece em processos de raiz unit√°ria.

### MSE em Processos de Raiz Unit√°ria
Em contraste, para um processo de raiz unit√°ria, o MSE se comporta de maneira diferente. Para um processo geral de raiz unit√°ria, a proje√ß√£o linear √© dada por:
$$ \hat{y}_{t+s|t} = s\delta + y_t + (\psi_1 + \psi_2 + \ldots + \psi_s)\epsilon_t + (\psi_2 + \ldots + \psi_{s+1})\epsilon_{t-1} + \ldots $$
O erro de previs√£o √© dado por:
$$ e_{t+s|t} = y_{t+s} - \hat{y}_{t+s|t} $$
e o MSE √©:
$$ MSE = E[ (y_{t+s} - \hat{y}_{t+s|t})^2 ] $$
No caso de processos de raiz unit√°ria, o MSE n√£o converge para um valor fixo quando $s \to \infty$. Em vez disso, o MSE *aumenta* com o horizonte de previs√£o $s$. Esse aumento no MSE ocorre devido √† acumula√ß√£o dos efeitos dos choques passados, que afetam a trajet√≥ria da s√©rie ao longo do tempo. √Ä medida que o horizonte de previs√£o aumenta, mais choques impactam a previs√£o, e essa incerteza cumulativa resulta no aumento do MSE. Como visto no cap√≠tulo 15, a proje√ß√£o linear para um processo de raiz unit√°ria cresce linearmente com s devido √† componente $s\delta$, enquanto o MSE tamb√©m cresce linearmente com $s$, embora com uma inclina√ß√£o diferente [^1].

**Proposi√ß√£o 1:** O MSE para um processo de raiz unit√°ria n√£o converge para um valor fixo quando $s \to \infty$, mas aproxima-se assintoticamente de uma fun√ß√£o linear de $s$.
*Proof:*
I. A proje√ß√£o linear para o n√≠vel da s√©rie em um processo de raiz unit√°ria √© dada por:
    $$ \hat{y}_{t+s|t} = s\delta + y_t + (\psi_1 + \psi_2 + \ldots + \psi_s)\epsilon_t + (\psi_2 + \ldots + \psi_{s+1})\epsilon_{t-1} + \ldots $$
II.  O erro de previs√£o √© $ e_{t+s|t} = y_{t+s} - \hat{y}_{t+s|t} $.
III.  A vari√¢ncia do erro de previs√£o, MSE, √© dada por $ E[ (y_{t+s} - \hat{y}_{t+s|t})^2 ] $.
IV. Do cap√≠tulo 15, sabemos que a vari√¢ncia do erro de previs√£o para um processo de raiz unit√°ria √©
$$ E[ (y_{t+s} - \hat{y}_{t+s|t})^2 ] = \left\{1 + \left(\sum_{i=1}^{s} \psi_i \right)^2 + \left(\sum_{i=1}^{s-1} \psi_i \right)^2 + \ldots + \psi_1^2  \right\} \sigma^2  $$
V.  Quando $s$ tende ao infinito, a vari√¢ncia do erro de previs√£o n√£o converge para um valor fixo, a menos que $\psi_i=0$. Os coeficientes se acumulam atrav√©s das somas, o que faz com que o MSE cres√ßa linearmente com $s$. Como a soma dos coeficientes $\psi_i$ n√£o converge para zero, devido √† natureza n√£o estacion√°ria do processo, a vari√¢ncia do erro de previs√£o cresce com $s$.
VI.  O comportamento assint√≥tico do MSE √© dominado pelos termos que incluem $s$. Portanto, para $s$ grande, o MSE aproxima-se de uma fun√ß√£o linear de $s$.
‚ñ†
Essa proposi√ß√£o demonstra formalmente que o MSE para um processo de raiz unit√°ria n√£o se estabiliza em um valor fixo, ao contr√°rio do que ocorre em processos estacion√°rios por tend√™ncia. A demonstra√ß√£o destaca a import√¢ncia da acumula√ß√£o das inova√ß√µes passadas na determina√ß√£o do erro de previs√£o.

**Lema 1.1:** Para um processo de raiz unit√°ria com $\psi_i = 1$ para todo $i$, o MSE √© dado por $ MSE = \frac{s(s+1)(2s+1)}{6}\sigma^2$
*Proof:*
I. Da proposi√ß√£o 1, sabemos que $MSE = \left\{1 + \left(\sum_{i=1}^{s} \psi_i \right)^2 + \left(\sum_{i=1}^{s-1} \psi_i \right)^2 + \ldots + \psi_1^2  \right\} \sigma^2$
II. Se $\psi_i = 1$ para todo $i$, ent√£o $\sum_{i=1}^{k} \psi_i = k$
III. Portanto, $MSE = \left\{1 + s^2 + (s-1)^2 + ... + 1^2 \right\}\sigma^2 =  \left\{ \sum_{i=1}^s i^2 \right\}\sigma^2$
IV. Sabemos que a soma dos quadrados dos primeiros $s$ inteiros √© dada por $\sum_{i=1}^s i^2 = \frac{s(s+1)(2s+1)}{6}$.
V. Assim, $MSE = \frac{s(s+1)(2s+1)}{6}\sigma^2$.
‚ñ†

Este lema oferece um caso especial para o comportamento do MSE num processo de raiz unit√°ria, onde todos os coeficientes $\psi_i$ s√£o iguais a 1. Ele demonstra que o MSE cresce com o cubo de $s$ nesse caso espec√≠fico, indicando que o erro de previs√£o aumenta muito mais rapidamente do que no caso geral discutido na Proposi√ß√£o 1.
> üí° **Exemplo Num√©rico:**
> Considere um processo de raiz unit√°ria com $\psi_i = 1$ para todo $i$ e $\sigma^2 = 1$. Vamos calcular o MSE para alguns valores de $s$ usando a f√≥rmula derivada no Lema 1.1.
>
> *   **Para $s=1$:**
>     $MSE = \frac{1(1+1)(2(1)+1)}{6} \times 1 = \frac{1 \times 2 \times 3}{6} = 1$
>
> *   **Para $s=5$:**
>     $MSE = \frac{5(5+1)(2(5)+1)}{6} \times 1 = \frac{5 \times 6 \times 11}{6} = 55$
>
> *   **Para $s=10$:**
>     $MSE = \frac{10(10+1)(2(10)+1)}{6} \times 1 = \frac{10 \times 11 \times 21}{6} = 385$
>
> Observe como o MSE aumenta drasticamente com o aumento de $s$. Para $s=10$, o MSE √© 385, que √© muito maior que o MSE para $s=1$, que √© 1. Isso ilustra o r√°pido crescimento do MSE em processos de raiz unit√°ria quando os coeficientes $\psi_i$ s√£o todos iguais a 1.
>
> ```mermaid
>  graph LR
>      A[s=0] --> B(s=1, MSE=1)
>      A --> C(s=5, MSE=55)
>      A --> D(s=10, MSE=385)
>      style A fill:#f9f,stroke:#333,stroke-width:2px
> ```
>
> Este exemplo demonstra a r√°pida deteriora√ß√£o da previsibilidade em processos de raiz unit√°ria com coeficientes $\psi_i = 1$ √† medida que o horizonte de previs√£o aumenta.

### MSE para um Modelo ARIMA(0,1,1)
Vamos agora derivar o MSE para um modelo ARIMA(0,1,1). A proje√ß√£o linear para o modelo ARIMA(0,1,1) √© dada por [^3]:
$$ \hat{y}_{t+s|t} = s\delta + y_t + \theta \epsilon_t $$
O erro de previs√£o para o modelo ARIMA(0,1,1) √©:
$$ e_{t+s|t} = y_{t+s} - \hat{y}_{t+s|t} $$
Como visto no Lema 1.1 do cap√≠tulo anterior [^3], o erro de previs√£o pode ser reescrito como:
$$ e_{t+s|t} =  \sum_{i=1}^s \epsilon_{t+i} + \theta\epsilon_{t+s-1} - \theta\epsilon_t $$
O MSE √© o valor esperado do quadrado desse erro, e como os $\epsilon$ s√£o independentes com vari√¢ncia $\sigma^2$:
$$ MSE = E[(e_{t+s|t})^2] = E\left[\left(\sum_{i=1}^s \epsilon_{t+i} + \theta\epsilon_{t+s-1} - \theta\epsilon_t\right)^2\right] $$
Expandindo e usando a propriedade de independ√™ncia das inova√ß√µes, obtemos:
$$ MSE =  \sum_{i=1}^s E[\epsilon_{t+i}^2]  + \theta^2 E[\epsilon_{t+s-1}^2] + \theta^2 E[\epsilon_t^2] = s\sigma^2 + \theta^2 \sigma^2 + \theta^2\sigma^2 $$
$$ MSE = (s + 2\theta^2)\sigma^2 $$
Esta √© a f√≥rmula para o MSE para um processo ARIMA(0,1,1), que coincide com o resultado obtido no Lema 1.1 do cap√≠tulo anterior.

**Lema 2:** O MSE para o modelo ARIMA(0,1,1) √© dado por:
$$ MSE = (s + 2\theta^2)\sigma^2  $$
*Proof:*
I. O erro de previs√£o para o modelo ARIMA(0,1,1) √© dado por $ e_{t+s|t} = y_{t+s} - \hat{y}_{t+s|t} $.
II.  Sabemos que $\hat{y}_{t+s|t} = s\delta + y_t + \theta \epsilon_t$.
III. Do modelo ARIMA(0,1,1) temos $y_t - y_{t-1} = \delta + \epsilon_t + \theta\epsilon_{t-1}$ ou seja $y_{t+s} = y_t + s\delta + \sum_{i=1}^s\epsilon_{t+i} + \theta\epsilon_{t+s-1}$
IV. Portanto, $ e_{t+s|t} = y_{t+s} - \hat{y}_{t+s|t} = \sum_{i=1}^s\epsilon_{t+i} + \theta\epsilon_{t+s-1} - \theta\epsilon_t $.
V. Elevando ao quadrado e tomando a esperan√ßa, e lembrando que os $\epsilon$ s√£o independentes, obtemos
$MSE = E[ e_{t+s|t}^2] = E[ \left(\sum_{i=1}^s\epsilon_{t+i} + \theta\epsilon_{t+s-1} - \theta\epsilon_t \right)^2 ] = E[ (\sum_{i=1}^s\epsilon_{t+i})^2 ] + E[(\theta\epsilon_{t+s-1})^2] + E[(\theta\epsilon_t)^2 ]$
VI. Como $E[\epsilon_i^2]=\sigma^2$ e as inova√ß√µes s√£o independentes, $MSE=s\sigma^2 + \theta^2\sigma^2 + \theta^2\sigma^2 = (s+2\theta^2)\sigma^2$.
‚ñ†
Este lema formaliza a express√£o do MSE para o modelo ARIMA(0,1,1), mostrando que o MSE cresce linearmente com o horizonte de previs√£o $s$. O termo $2\theta^2\sigma^2$ √© uma constante que representa um aumento na incerteza devido √† inclus√£o do componente MA(1), e faz com que o MSE seja sempre maior do que em um passeio aleat√≥rio com deriva.

> üí° **Exemplo Num√©rico:**
> Considere um modelo ARIMA(0,1,1) com $\theta=0.7$ e $\sigma^2=0.5$. Vamos calcular o MSE para diferentes valores de $s$.
>
> *   **Para $s=1$:**
>     $MSE = (1 + 2(0.7)^2) \times 0.5 = (1 + 2(0.49)) \times 0.5 = 1.98 \times 0.5 = 0.99$
>
> *   **Para $s=5$:**
>     $MSE = (5 + 2(0.7)^2) \times 0.5 = (5 + 0.98) \times 0.5 = 5.98 \times 0.5 = 2.99$
>
> *   **Para $s=10$:**
>     $MSE = (10 + 2(0.7)^2) \times 0.5 = (10 + 0.98) \times 0.5 = 10.98 \times 0.5 = 5.49$
>
> Note que o MSE cresce linearmente com $s$, com uma taxa de crescimento igual a $\sigma^2 = 0.5$. O termo $2\theta^2\sigma^2 = 2(0.7)^2 \times 0.5 = 0.49$ adiciona um componente constante ao MSE, aumentando a incerteza em todos os horizontes de previs√£o.
>
> ```mermaid
>  graph LR
>      A[s=0] --> B(s=1, MSE=0.99)
>      A --> C(s=5, MSE=2.99)
>      A --> D(s=10, MSE=5.49)
>      style A fill:#f9f,stroke:#333,stroke-width:2px
> ```
>
> Este exemplo mostra o comportamento do MSE em um modelo ARIMA(0,1,1). O gr√°fico ilustra que o erro de previs√£o aumenta linearmente com o horizonte de previs√£o ($s$), o que √© caracter√≠stico de processos de raiz unit√°ria. O componente MA(1) do modelo adiciona um fator constante ao MSE, aumentando a incerteza para todos os horizontes.

**Observa√ß√£o 1:** O MSE do modelo ARIMA(0,1,1) pode ser interpretado como a soma do MSE de um passeio aleat√≥rio com deriva mais um termo constante que depende do par√¢metro $\theta$.
*Justification:*
Do Lema 2, sabemos que $MSE_{ARIMA(0,1,1)} = (s + 2\theta^2)\sigma^2$. Separando os termos, temos $MSE_{ARIMA(0,1,1)} = s\sigma^2 + 2\theta^2\sigma^2$. O primeiro termo, $s\sigma^2$, √© o MSE de um passeio aleat√≥rio com deriva, como veremos a seguir. O segundo termo, $2\theta^2\sigma^2$, √© um termo constante que aumenta o MSE devido √† inclus√£o do componente MA(1) no modelo. Isso significa que o modelo ARIMA(0,1,1) √© sempre menos preciso, em termos de MSE, do que um passeio aleat√≥rio com deriva para o mesmo horizonte de previs√£o $s$.

### Compara√ß√£o com o Passeio Aleat√≥rio com Deriva
Para um passeio aleat√≥rio com deriva, a proje√ß√£o linear √©:
$$ \hat{y}_{t+s|t} = s\delta + y_t $$
O erro de previs√£o √©:
$$ e_{t+s|t} = y_{t+s} - \hat{y}_{t+s|t} $$
Como $y_{t+s} = y_t + s\delta + \sum_{i=1}^s \epsilon_{t+i}$, o erro de previs√£o √© simplificado para
$$ e_{t+s|t} = \sum_{i=1}^s \epsilon_{t+i} $$
O MSE √© dado por:
$$ MSE = E\left[ \left( \sum_{i=1}^s \epsilon_{t+i} \right)^2 \right] = \sum_{i=1}^s E[\epsilon_{t+i}^2] = s\sigma^2 $$
Comparando com o MSE do ARIMA(0,1,1), vemos que o passeio aleat√≥rio com deriva tem um MSE menor para qualquer horizonte de previs√£o, pois o termo $2\theta^2\sigma^2$ n√£o est√° presente. Isso significa que o passeio aleat√≥rio com deriva √© mais previs√≠vel, em termos de MSE, do que o ARIMA(0,1,1) para um mesmo horizonte $s$.

**Lema 3:** O MSE do passeio aleat√≥rio com deriva √© $MSE = s\sigma^2$
*Proof:*
I. A proje√ß√£o linear para um passeio aleat√≥rio com deriva √© $\hat{y}_{t+s|t} = y_t + s\delta$.
II. Do modelo passeio aleat√≥rio com deriva, temos $y_{t+s} = y_t + s\delta + \sum_{i=1}^s \epsilon_{t+i}$.
III. O erro de previs√£o √© $e_{t+s|t} = y_{t+s} - \hat{y}_{t+s|t} = \sum_{i=1}^s \epsilon_{t+i}$.
IV.  O MSE √© dado por $MSE = E[e_{t+s|t}^2] = E[(\sum_{i=1}^s \epsilon_{t+i})^2]$.
V. Como as inova√ß√µes s√£o independentes e $E[\epsilon_i^2] = \sigma^2$, temos $MSE = \sum_{i=1}^s E[\epsilon_{t+i}^2] = \sum_{i=1}^s \sigma^2 = s\sigma^2$.
‚ñ†
Este lema formaliza o c√°lculo do MSE para o passeio aleat√≥rio com deriva, demonstrando que ele cresce linearmente com o horizonte de previs√£o, com uma inclina√ß√£o de $\sigma^2$. Este resultado serve como uma base de compara√ß√£o para modelos mais complexos, como o ARIMA(0,1,1).

> üí° **Exemplo Num√©rico:**
> Suponha um passeio aleat√≥rio com deriva com $\sigma^2 = 0.8$. Vamos calcular o MSE para diferentes valores de $s$.
>
> *   **Para $s=1$:**
>     $MSE = 1 \times 0.8 = 0.8$
>
> *   **Para $s=5$:**
>     $MSE = 5 \times 0.8 = 4$
>
> *   **Para $s=10$:**
>     $MSE = 10 \times 0.8 = 8$
>
> Note que o MSE cresce linearmente com $s$ com uma taxa de crescimento de $\sigma^2 = 0.8$.
>
> ```mermaid
> graph LR
>     A[s=0] --> B(s=1, MSE=0.8)
>     A --> C(s=5, MSE=4)
>     A --> D(s=10, MSE=8)
>     style A fill:#f9f,stroke:#333,stroke-width:2px
> ```
>
> Comparando com o exemplo anterior do modelo ARIMA(0,1,1), o MSE para o passeio aleat√≥rio com deriva √© sempre menor para o mesmo horizonte de previs√£o, o que ilustra a ideia de que o passeio aleat√≥rio com deriva √© mais previs√≠vel, em termos de MSE, do que o modelo ARIMA(0,1,1).

### Conclus√£o
Este cap√≠tulo focou na an√°lise do MSE em processos n√£o estacion√°rios, demonstrando que o MSE se comporta de forma diferente em processos estacion√°rios por tend√™ncia e processos de raiz unit√°ria. Enquanto o MSE converge para um valor fixo em modelos estacion√°rios por tend√™ncia, ele aumenta com o horizonte de previs√£o $s$ em modelos de raiz unit√°ria. Especificamente, mostramos que para o modelo ARIMA(0,1,1), o MSE cresce linearmente com $s$, e √© sempre maior que o MSE do passeio aleat√≥rio com deriva para o mesmo $s$. A an√°lise do MSE √© crucial para entender a incerteza associada √†s previs√µes, e como essa incerteza se comporta com o tempo. Os resultados demonstrados neste cap√≠tulo refor√ßam a ideia de que a modelagem de s√©ries temporais n√£o estacion√°rias exige aten√ß√£o especial ao comportamento do MSE e como este impacta a precis√£o das previs√µes.

### Refer√™ncias
[^1]: Modelos de S√©ries Temporais N√£o Estacion√°rias: T√≥picos introdut√≥rios.
[^2]: Compara√ß√£o da Proje√ß√£o Linear em Processos Estacion√°rios por Tend√™ncia e Raiz Unit√°ria.
[^3]: Compara√ß√£o da Proje√ß√£o Linear em Processos Estacion√°rios por Tend√™ncia e Raiz Unit√°ria: An√°lise do Modelo ARIMA(0,1,1).
<!-- END -->
