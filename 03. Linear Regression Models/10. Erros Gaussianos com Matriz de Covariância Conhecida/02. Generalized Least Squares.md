## Distribui√ß√£o do Estimador GLS Sob Erros Gaussianos com Matriz de Covari√¢ncia Conhecida

### Introdu√ß√£o
Este cap√≠tulo expande a an√°lise do estimador de **M√≠nimos Quadrados Generalizados (GLS)**, focando especificamente no cen√°rio em que os erros seguem uma distribui√ß√£o Gaussiana com uma matriz de covari√¢ncia conhecida. Construindo sobre os conceitos estabelecidos nos cap√≠tulos anteriores[^0], vamos detalhar a distribui√ß√£o condicional do estimador GLS e estabelecer sua propriedade como o melhor estimador linear n√£o viesado (BLUE) nesse contexto. Adicionalmente, ser√° demonstrado que, sob essa suposi√ß√£o, o estimador GLS corresponde ao estimador de m√°xima verossimilhan√ßa, refor√ßando sua import√¢ncia em infer√™ncia estat√≠stica. O objetivo √© fornecer uma compreens√£o detalhada das propriedades do estimador GLS quando os erros s√£o gaussianos e a matriz de covari√¢ncia √© conhecida, mostrando sua relev√¢ncia em modelagem econom√©trica e an√°lise de s√©ries temporais.

### Conceitos Fundamentais
Como vimos anteriormente[^0], o estimador de m√≠nimos quadrados generalizados (GLS) √© uma alternativa ao estimador de m√≠nimos quadrados ordin√°rios (OLS) quando a suposi√ß√£o de esfericidade dos erros √© violada. Especificamente, o modelo de regress√£o que estamos considerando √© da forma
$$y = X\beta + u$$
onde $y$ √© um vetor de vari√°veis dependentes, $X$ √© uma matriz de vari√°veis explicativas, $\beta$ √© um vetor de par√¢metros a serem estimados e $u$ √© um vetor de erros. Em contraste com o modelo OLS, onde sup√µe-se que $E(uu'|X) = \sigma^2I_T$, aqui consideramos que o vetor de erros $u$ segue uma distribui√ß√£o normal com m√©dia zero e matriz de covari√¢ncia dada por $E(uu'|X) = \sigma^2V$, onde $V$ √© uma matriz positiva definida conhecida. Al√©m disso, assume-se que
$$u|X \sim N(0, \sigma^2V)$$
Sob essas condi√ß√µes, o estimador GLS √© dado por
$$\hat{\beta}_{GLS} = (X'V^{-1}X)^{-1}X'V^{-1}y$$ [^8.3.5]
Este estimador √© obtido pela transforma√ß√£o do modelo original por uma matriz $L$, tal que $V^{-1} = L'L$, resultando em um modelo transformado com erros esf√©ricos. O modelo transformado √© dado por
$$\tilde{y} = \tilde{X}\beta + \tilde{u}$$
onde $\tilde{y} = Ly$, $\tilde{X} = LX$, e $\tilde{u} = Lu$ e $E(\tilde{u}\tilde{u}'|X) = \sigma^2I_T$. A aplica√ß√£o do estimador OLS ao modelo transformado √© equivalente a obter o estimador GLS.

> üí° **Exemplo Num√©rico:** Vamos considerar um exemplo simples com $T=3$ observa√ß√µes, duas vari√°veis explicativas e um termo constante. Suponha que temos:
> $$ y = \begin{bmatrix} 2 \\ 4 \\ 6 \end{bmatrix}, \quad X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \end{bmatrix}, \quad V = \begin{bmatrix} 2 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 4 \end{bmatrix}, \quad \sigma^2 = 1 $$
>
> Primeiro, calculamos $V^{-1}$:
> $$ V^{-1} = \begin{bmatrix} 1/2 & 0 & 0 \\ 0 & 1/3 & 0 \\ 0 & 0 & 1/4 \end{bmatrix} $$
>
> Em seguida, calculamos $X'V^{-1}X$:
> $$ X'V^{-1}X = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 1/2 & 0 & 0 \\ 0 & 1/3 & 0 \\ 0 & 0 & 1/4 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \end{bmatrix} = \begin{bmatrix} 1.083 & 1.833 \\ 1.833 & 3.5 \end{bmatrix} $$
>
> Agora, calculamos a inversa de $(X'V^{-1}X)$:
> $$ (X'V^{-1}X)^{-1} \approx \begin{bmatrix} 7.727 & -3.864 \\ -3.864 & 2.468 \end{bmatrix} $$
>
> Depois, calculamos $X'V^{-1}y$:
> $$ X'V^{-1}y = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 1/2 & 0 & 0 \\ 0 & 1/3 & 0 \\ 0 & 0 & 1/4 \end{bmatrix} \begin{bmatrix} 2 \\ 4 \\ 6 \end{bmatrix} = \begin{bmatrix} 4.167 \\ 9.5 \end{bmatrix} $$
>
> Finalmente, calculamos $\hat{\beta}_{GLS}$:
> $$ \hat{\beta}_{GLS} = (X'V^{-1}X)^{-1}X'V^{-1}y = \begin{bmatrix} 7.727 & -3.864 \\ -3.864 & 2.468 \end{bmatrix} \begin{bmatrix} 4.167 \\ 9.5 \end{bmatrix} = \begin{bmatrix} -1.33 \\ 1.82 \end{bmatrix} $$
>
>  Assim, $\hat{\beta}_{GLS} \approx \begin{bmatrix} -1.33 \\ 1.82 \end{bmatrix}$ √© o vetor de par√¢metros estimado pelo m√©todo GLS.

#### Distribui√ß√£o Condicional do Estimador GLS
Sob a suposi√ß√£o de que os erros s√£o Gaussianos, a distribui√ß√£o condicional do estimador GLS pode ser derivada. Dado que $u|X \sim N(0, \sigma^2V)$, e $\hat{\beta}_{GLS}$ √© uma combina√ß√£o linear de $y$ e portanto de $u$, ent√£o o estimador GLS tamb√©m segue uma distribui√ß√£o normal.  A m√©dia condicional do estimador GLS √©
$$E(\hat{\beta}_{GLS}|X) = E[(X'V^{-1}X)^{-1}X'V^{-1}(X\beta + u)|X] = (X'V^{-1}X)^{-1}X'V^{-1}X\beta + (X'V^{-1}X)^{-1}X'V^{-1}E(u|X) = \beta$$
Portanto, $\hat{\beta}_{GLS}$ √© n√£o viesado condicional em $X$.

A matriz de covari√¢ncia condicional de $\hat{\beta}_{GLS}$ √©:
$$Var(\hat{\beta}_{GLS}|X) = Var[(X'V^{-1}X)^{-1}X'V^{-1}(X\beta + u)|X]$$
$$= (X'V^{-1}X)^{-1}X'V^{-1}Var(u|X)V^{-1}X(X'V^{-1}X)^{-1}$$
$$=(X'V^{-1}X)^{-1}X'V^{-1}\sigma^2V V^{-1}X(X'V^{-1}X)^{-1}$$
$$=\sigma^2(X'V^{-1}X)^{-1}X'V^{-1}V V^{-1}X(X'V^{-1}X)^{-1}$$
$$=\sigma^2(X'V^{-1}X)^{-1}X'V^{-1}X(X'V^{-1}X)^{-1}$$
$$=\sigma^2(X'V^{-1}X)^{-1}$$

Portanto, a distribui√ß√£o condicional do estimador GLS √© dada por
$$\hat{\beta}_{GLS}|X \sim N(\beta, \sigma^2(X'V^{-1}X)^{-1})$$ [^8.2.31]
Essa distribui√ß√£o condicional √© crucial para a infer√™ncia estat√≠stica e testes de hip√≥teses sob a suposi√ß√£o de erros Gaussianos com matriz de covari√¢ncia conhecida.

> üí° **Exemplo Num√©rico (Cont.):**  Usando os mesmos valores do exemplo anterior e considerando $\sigma^2 = 1$, a matriz de covari√¢ncia de $\hat{\beta}_{GLS}$ √©:
>
> $$Var(\hat{\beta}_{GLS}|X) = \sigma^2(X'V^{-1}X)^{-1} = 1 \cdot \begin{bmatrix} 7.727 & -3.864 \\ -3.864 & 2.468 \end{bmatrix} = \begin{bmatrix} 7.727 & -3.864 \\ -3.864 & 2.468 \end{bmatrix}$$
>
> Isso significa que a vari√¢ncia do primeiro coeficiente estimado ($\hat{\beta}_{GLS,1}$) √© aproximadamente 7.727 e a vari√¢ncia do segundo coeficiente estimado ($\hat{\beta}_{GLS,2}$) √© aproximadamente 2.468. A covari√¢ncia entre os dois estimadores √© -3.864. Como os erros s√£o gaussianos, temos que:
> $$\hat{\beta}_{GLS}|X \sim N\left(\begin{bmatrix} \beta_1 \\ \beta_2 \end{bmatrix}, \begin{bmatrix} 7.727 & -3.864 \\ -3.864 & 2.468 \end{bmatrix}\right)$$
> Podemos usar essa distribui√ß√£o para construir intervalos de confian√ßa e realizar testes de hip√≥tese para os coeficientes.

**Lema 1:** *O estimador GLS √© um estimador linear de y*
*Prova:*
I. O estimador GLS √© definido como: $\hat{\beta}_{GLS} = (X'V^{-1}X)^{-1}X'V^{-1}y$.
II. Podemos reescrever $\hat{\beta}_{GLS}$ como: $\hat{\beta}_{GLS} = A y$, onde $A = (X'V^{-1}X)^{-1}X'V^{-1}$.
III. Dado que $A$ √© uma matriz que n√£o depende de $y$, e $\hat{\beta}_{GLS}$ √© um produto de uma matriz por um vetor $y$, ent√£o $\hat{\beta}_{GLS}$ √© uma combina√ß√£o linear de $y$. $\blacksquare$

**Corol√°rio 1:** *Sob a suposi√ß√£o de erros gaussianos com matriz de covari√¢ncia conhecida, a distribui√ß√£o condicional do estimador GLS √© normal.*
*Prova:*
I. Vimos que o estimador GLS √© uma combina√ß√£o linear de y, onde $\hat{\beta}_{GLS} = A y$ e $A = (X'V^{-1}X)^{-1}X'V^{-1}$.
II. Dado que $u|X \sim N(0, \sigma^2V)$, ent√£o $y|X \sim N(X\beta, \sigma^2V)$.
III. Uma combina√ß√£o linear de vari√°veis gaussianas tamb√©m √© uma vari√°vel gaussiana.
IV. Portanto $\hat{\beta}_{GLS}|X$ √© normal, com m√©dia $\beta$ e matriz de covari√¢ncia $\sigma^2(X'V^{-1}X)^{-1}$. $\blacksquare$

**Lema 1.1:** *O estimador $\hat{\beta}_{GLS}$ pode ser expresso como uma combina√ß√£o linear dos erros $u$ e do par√¢metro $\beta$.*
*Prova:*
I. Sabemos que $y = X\beta + u$.
II. Substituindo na express√£o de $\hat{\beta}_{GLS}$:
$\hat{\beta}_{GLS} = (X'V^{-1}X)^{-1}X'V^{-1}(X\beta + u)$
III. Expandindo:
$\hat{\beta}_{GLS} = (X'V^{-1}X)^{-1}X'V^{-1}X\beta + (X'V^{-1}X)^{-1}X'V^{-1}u$
IV. Simplificando:
$\hat{\beta}_{GLS} = \beta + (X'V^{-1}X)^{-1}X'V^{-1}u$
V. Portanto, $\hat{\beta}_{GLS}$ √© uma combina√ß√£o linear de $\beta$ e $u$. $\blacksquare$

#### BLUE: Best Linear Unbiased Estimator
Uma propriedade fundamental do estimador GLS sob a suposi√ß√£o de erros gaussianos com matriz de covari√¢ncia conhecida √© que ele √© o melhor estimador linear n√£o viesado (BLUE). A propriedade de n√£o vi√©s j√° foi demonstrada acima, e agora vamos mostrar que, dentro da classe de estimadores lineares n√£o viesados, o estimador GLS possui a menor vari√¢ncia.
Seja $\hat{\beta}$ qualquer outro estimador linear n√£o viesado de $\beta$. Como $\hat{\beta}$ √© linear em $y$, podemos escrever
$$\hat{\beta} = Cy$$
Para que $\hat{\beta}$ seja n√£o viesado, √© necess√°rio que
$$E(\hat{\beta}|X) = E(Cy|X) = C X \beta = \beta$$
isso implica que
$$CX = I$$
onde $I$ √© a matriz identidade. A vari√¢ncia condicional de $\hat{\beta}$ √© dada por
$$Var(\hat{\beta}|X) = Var(Cy|X) = CV(C') = \sigma^2CVC'$$
Para provar que o estimador GLS √© BLUE, precisamos demonstrar que a diferen√ßa entre $Var(\hat{\beta}|X)$ e $Var(\hat{\beta}_{GLS}|X)$ √© uma matriz semidefinida positiva.  Poderemos usar a fatora√ß√£o de Cholesky. Como $V$ √© positiva definida, $V^{-1}$ tamb√©m √©. Podemos fatorar $V^{-1}$ da seguinte forma $V^{-1}=L'L$. Assim, a vari√¢ncia do estimador GLS √©:
$$Var(\hat{\beta}_{GLS}|X) = \sigma^2(X'V^{-1}X)^{-1} = \sigma^2(X'L'LX)^{-1}$$
Por outro lado, dado que $CX = I$, podemos reescrever $Var(\hat{\beta}|X)$ como:
$$Var(\hat{\beta}|X) = \sigma^2CVC' = \sigma^2(CXV)(CX)'$$
$$= \sigma^2(CXV)(CX)' =  \sigma^2(X'L'L)(X'L'L)^{-1}(X'L'L) V (X'L'L)^{-1'}(X'L'L)' =  \sigma^2(X'V^{-1}X)^{-1}X'V^{-1}V(V^{-1})' X(X'V^{-1}X)^{-1}$$
$$= \sigma^2(X'V^{-1}X)^{-1}X'V^{-1}XX'V^{-1}X(X'V^{-1}X)^{-1}$$
Usando o fato que $CX=I$, temos:
$$Var(\hat{\beta}|X) = \sigma^2(X'V^{-1}X)^{-1} + \sigma^2(C - (X'V^{-1}X)^{-1}X'V^{-1})V(C - (X'V^{-1}X)^{-1}X'V^{-1})'$$
A segunda parte da igualdade √© sempre positiva, e portanto
$$Var(\hat{\beta}|X) \geq \sigma^2(X'V^{-1}X)^{-1} = Var(\hat{\beta}_{GLS}|X)$$
Portanto, $\hat{\beta}_{GLS}$ √© o estimador linear n√£o viesado de vari√¢ncia m√≠nima.

**Teorema 1** *Sob a suposi√ß√£o de erros gaussianos com matriz de covari√¢ncia conhecida, o estimador GLS √© o estimador BLUE.*
*Prova:*
I.  J√° provamos que $\hat{\beta}_{GLS}$ √© n√£o viesado: $E(\hat{\beta}_{GLS}|X) = \beta$.
    
II.  J√° provamos que $\hat{\beta}_{GLS}$ √© linear em $y$.
    
III.  A diferen√ßa entre a vari√¢ncia de qualquer estimador linear n√£o viesado e a vari√¢ncia de $\hat{\beta}_{GLS}$ √© positiva semidefinida.
    
IV.  Portanto,  $\hat{\beta}_{GLS}$ √© o estimador BLUE, que significa que √© o melhor (menor vari√¢ncia) entre todos os estimadores lineares n√£o viesados. $\blacksquare$

> üí° **Exemplo Num√©rico:** Para ilustrar o conceito de BLUE, vamos supor que temos um estimador alternativo $\hat{\beta}_{alt} = C y$ e que $CX = I$. Vamos definir uma matriz $C$ que satisfa√ßa essa condi√ß√£o e calcular sua vari√¢ncia, comparando-a com a vari√¢ncia do estimador GLS.
> Supondo que $X$ √© uma matriz $T \times k$ e $C$ √© $k \times T$. Para garantir que $CX = I_k$, vamos definir $C = (X'X)^{-1}X'$. Essa √© a matriz que nos daria o estimador OLS.
>  Vamos usar os dados do exemplo anterior, mas vamos calcular o estimador OLS para fins de compara√ß√£o. Primeiro calculamos:
>
>  $$ (X'X)^{-1} = \left(\begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \end{bmatrix} \right)^{-1} = \begin{bmatrix} 3 & 6 \\ 6 & 14 \end{bmatrix}^{-1} = \begin{bmatrix} 2.333 & -1 \\ -1 & 0.5 \end{bmatrix} $$
>  
>  $$ (X'X)^{-1}X' = \begin{bmatrix} 2.333 & -1 \\ -1 & 0.5 \end{bmatrix} \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 3 \end{bmatrix} = \begin{bmatrix} 1.333 & 0.333 & -0.667 \\ -0.5 & 0 & 0.5 \end{bmatrix} $$
>
> E o estimador OLS √©:
> $$ \hat{\beta}_{OLS} = (X'X)^{-1}X'y = \begin{bmatrix} 1.333 & 0.333 & -0.667 \\ -0.5 & 0 & 0.5 \end{bmatrix} \begin{bmatrix} 2 \\ 4 \\ 6 \end{bmatrix} = \begin{bmatrix} 0 \\ 2 \end{bmatrix} $$
>
> E a vari√¢ncia desse estimador √©:
>
> $$ Var(\hat{\beta}_{OLS}|X) = \sigma^2 (X'X)^{-1}X'V X(X'X)^{-1} = \begin{bmatrix} 2.333 & -1 \\ -1 & 0.5 \end{bmatrix} \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 2 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 4 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \end{bmatrix} \begin{bmatrix} 2.333 & -1 \\ -1 & 0.5 \end{bmatrix}$$
> $$ = \begin{bmatrix} 2.333 & -1 \\ -1 & 0.5 \end{bmatrix} \begin{bmatrix} 9 & 17 \\ 17 & 37 \end{bmatrix} \begin{bmatrix} 2.333 & -1 \\ -1 & 0.5 \end{bmatrix} = \begin{bmatrix} 4.58 & 8.58 \\ 0 & 0 \end{bmatrix} $$
>
> Ao comparar as vari√¢ncias dos estimadores, vemos que a vari√¢ncia dos coeficientes estimados pelo m√©todo GLS s√£o menores do que os do estimador OLS para os dois primeiros coeficientes (7.727 contra 13.58 e 2.468 contra 3.5) e podemos verificar que o estimador GLS √©, de fato, o melhor estimador linear n√£o viesado.

**Lema 2:** *Se $\hat{\beta}$ √© um estimador linear n√£o viesado de $\beta$ tal que $\hat{\beta} = Cy$, ent√£o $CX = I$, onde $I$ √© a matriz identidade.*
*Prova:*
I. Para que $\hat{\beta}$ seja n√£o viesado, temos que $E(\hat{\beta}|X) = \beta$.
II. Como $\hat{\beta} = Cy$, ent√£o $E(Cy|X) = \beta$.
III. Sabemos que $E(y|X) = X\beta$, logo $E(Cy|X) = CX\beta$.
IV. Portanto, $CX\beta = \beta$ para todo $\beta$. Isso implica que $CX=I$.  $\blacksquare$

#### Estimador de M√°xima Verossimilhan√ßa
Quando os erros seguem uma distribui√ß√£o normal, o estimador GLS √© tamb√©m o estimador de m√°xima verossimilhan√ßa.
A fun√ß√£o de verossimilhan√ßa para o modelo $y = X\beta + u$ com $u \sim N(0, \sigma^2V)$ √© dada por:
$$L(\beta, \sigma^2 | y, X) = (2\pi\sigma^2)^{-T/2}|V|^{-1/2} \exp\left(-\frac{1}{2\sigma^2}(y-X\beta)'V^{-1}(y-X\beta)\right)$$
Tomando o logaritmo natural, obtemos a log-verossimilhan√ßa:
$$\ln L(\beta, \sigma^2 | y, X) = -\frac{T}{2}\ln(2\pi) - \frac{T}{2}\ln(\sigma^2) - \frac{1}{2}\ln|V| - \frac{1}{2\sigma^2}(y-X\beta)'V^{-1}(y-X\beta)$$
Para maximizar a log-verossimilhan√ßa em rela√ß√£o a $\beta$, basta minimizar o termo $(y-X\beta)'V^{-1}(y-X\beta)$, que √© equivalente a minimizar a soma ponderada dos quadrados dos erros, com pesos dados pela matriz $V^{-1}$. A solu√ß√£o para este problema de minimiza√ß√£o √© o estimador GLS.
$$\hat{\beta}_{ML} = (X'V^{-1}X)^{-1}X'V^{-1}y = \hat{\beta}_{GLS}$$
O estimador de m√°xima verossimilhan√ßa para $\sigma^2$ √© dado por:
$${\hat{\sigma}^2}_{ML} = \frac{1}{T}(y-X\hat{\beta}_{GLS})'V^{-1}(y-X\hat{\beta}_{GLS})$$
Note que este estimador √© viesado. Um estimador n√£o viesado √© dado por:
$${\hat{\sigma}^2}_{GLS} = \frac{1}{T-k}(y-X\hat{\beta}_{GLS})'V^{-1}(y-X\hat{\beta}_{GLS})$$

> üí° **Exemplo Num√©rico:**  Usando os valores do exemplo anterior, vamos calcular o estimador de m√°xima verossimilhan√ßa para $\sigma^2$.
>
> Primeiro, calculamos os res√≠duos:
>
>  $$ \hat{u} = y - X \hat{\beta}_{GLS} = \begin{bmatrix} 2 \\ 4 \\ 6 \end{bmatrix} - \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \end{bmatrix} \begin{bmatrix} -1.33 \\ 1.82 \end{bmatrix} = \begin{bmatrix} 1.51 \\ 0.31 \\ -0.89 \end{bmatrix} $$
>
> Em seguida, calculamos $(y-X\hat{\beta}_{GLS})'V^{-1}(y-X\hat{\beta}_{GLS}) = \hat{u}'V^{-1}\hat{u}$:
>
>  $$ \hat{u}'V^{-1}\hat{u} = \begin{bmatrix} 1.51 & 0.31 & -0.89 \end{bmatrix} \begin{bmatrix} 1/2 & 0 & 0 \\ 0 & 1/3 & 0 \\ 0 & 0 & 1/4 \end{bmatrix} \begin{bmatrix} 1.51 \\ 0.31 \\ -0.89 \end{bmatrix} = 1.137 + 0.032 + 0.198 = 1.367$$
>
> O estimador de m√°xima verossimilhan√ßa para $\sigma^2$ √©:
>
>  $$ {\hat{\sigma}^2}_{ML} = \frac{1}{T}(y-X\hat{\beta}_{GLS})'V^{-1}(y-X\hat{\beta}_{GLS}) = \frac{1}{3} * 1.367 = 0.456 $$
>
> E o estimador n√£o viesado para $\sigma^2$ √©:
>
>  $$ {\hat{\sigma}^2}_{GLS} = \frac{1}{T-k}(y-X\hat{\beta}_{GLS})'V^{-1}(y-X\hat{\beta}_{GLS}) = \frac{1}{3-2} * 1.367 = 1.367 $$
>
>  Note que ${\hat{\sigma}^2}_{ML}$ √© viesado, e no nosso caso subestima a vari√¢ncia populacional $\sigma^2 = 1$ enquanto que ${\hat{\sigma}^2}_{GLS}$ n√£o √© viesado, mas tem uma vari√¢ncia maior.

**Teorema 2:** *Se os erros $u$ s√£o normalmente distribu√≠dos, o estimador GLS √© o estimador de m√°xima verossimilhan√ßa.*
*Prova:*
I.  A fun√ß√£o de verossimilhan√ßa para o modelo gaussiano √© dada por
$$L(\beta, \sigma^2 | y, X) = (2\pi\sigma^2)^{-T/2}|V|^{-1/2} \exp\left(-\frac{1}{2\sigma^2}(y-X\beta)'V^{-1}(y-X\beta)\right)$$
II.  A maximiza√ß√£o desta fun√ß√£o em rela√ß√£o a $\beta$ leva √† minimiza√ß√£o de $(y-X\beta)'V^{-1}(y-X\beta)$.
III. Como visto anteriormente, o estimador GLS √© obtido pela minimiza√ß√£o desta express√£o.
IV. Portanto, o estimador GLS √© tamb√©m o estimador de m√°xima verossimilhan√ßa sob erros gaussianos. $\blacksquare$

**Teorema 2.1:** *O estimador de m√°xima verossimilhan√ßa de $\sigma^2$ √© viesado, enquanto o estimador ${\hat{\sigma}^2}_{GLS}$ √© n√£o viesado.*
*Prova:*
I.  O estimador de m√°xima verossimilhan√ßa para $\sigma^2$ √© dado por:
    $${\hat{\sigma}^2}_{ML} = \frac{1}{T}(y-X\hat{\beta}_{GLS})'V^{-1}(y-X\hat{\beta}_{GLS})$$
II. O valor esperado de ${\hat{\sigma}^2}_{ML}$ √©:
    $$E({\hat{\sigma}^2}_{ML}|X) = \frac{1}{T}E((y-X\hat{\beta}_{GLS})'V^{-1}(y-X\hat{\beta}_{GLS})|X)$$
    $$= \frac{1}{T}E((u - X(\hat{\beta}_{GLS} - \beta))'V^{-1}(u - X(\hat{\beta}_{GLS} - \beta))|X)$$
    $$= \frac{1}{T}E(u'V^{-1}u - 2u'V^{-1}X(\hat{\beta}_{GLS} - \beta) + (\hat{\beta}_{GLS} - \beta)'X'V^{-1}X(\hat{\beta}_{GLS} - \beta)|X)$$
    $$= \frac{1}{T}[E(u'V^{-1}u|X) - 2E(u'V^{-1}X(\hat{\beta}_{GLS} - \beta)|X) + E((\hat{\beta}_{GLS} - \beta)'X'V^{-1}X(\hat{\beta}_{GLS} - \beta)|X)]$$
III. Como $E(u|X)=0$, o segundo termo √© zero. Substituindo $\hat{\beta}_{GLS} - \beta = (X'V^{-1}X)^{-1}X'V^{-1}u$ e  usando que $E(u'V^{-1}u|X)=Tr(V^{-1}E(uu'|X))=Tr(V^{-1}\sigma^2V)=\sigma^2Tr(I) = \sigma^2T$, temos:
     $$E({\hat{\sigma}^2}_{ML}|X) =  \frac{1}{T}[\sigma^2T + E(u'V^{-1}X(X'V^{-1}X)^{-1}X'V^{-1}X(X'V^{-1}X)^{-1}X'V^{-1}u|X)]$$
    $$= \frac{1}{T}[\sigma^2T  + \sigma^2 Tr(X(X'V^{-1}X)^{-1}X'V^{-1}V^{-1}X(X'V^{-1}X)^{-1}X'V^{-1}X)]$$
  $$=\sigma^2 + \frac{\sigma^2}{T}Tr((X'V^{-1}X)^{-1}X'V^{-1}X) = \sigma^2 + \frac{\sigma^2}{T}Tr(I_k) = \sigma^2 + \frac{\sigma^2k}{T}$$
    
    $$E({\hat{\sigma}^2}_{ML}|X) = \sigma^2(1-\frac{k}{T})$$
IV.  O estimador n√£o viesado ${\hat{\sigma}^2}_{GLS} = \frac{1}{T-k}(y-X\hat{\beta}_{GLS})'V^{-1}(y-X\hat{\beta}_{GLS})$ ter√° valor esperado igual a $\sigma^2$
    $$E({\hat{\sigma}^2}_{GLS}|X) = \frac{T}{T-k} E({\hat{\sigma}^2}_{ML}|X)= \frac{T}{T-k} \frac{T-k}{T} \sigma^2=\sigma^2$$
    V. Portanto, ${\hat{\sigma}^2}_{ML}$ √© viesado enquanto que ${\hat{\sigma}^2}_{GLS}$ √© n√£o viesado. $\blacksquare$

### Conclus√£o
Neste cap√≠tulo, exploramos as propriedades do estimador GLS sob a suposi√ß√£o de erros gaussianos com matriz de covari√¢ncia conhecida. Derivamos formalmente sua distribui√ß√£o condicional, demonstrando que ela √© normal, e estabelecemos que o estimador GLS √© o melhor estimador linear n√£o viesado (BLUE) nesse contexto. Adicionalmente, demonstramos que, com erros gaussianos, o estimador GLS √© equivalente ao estimador de m√°xima verossimilhan√ßa.  Essas propriedades refor√ßam a import√¢ncia do estimador GLS em aplica√ß√µes econom√©tricas e de an√°lise de s√©ries temporais, quando os erros n√£o s√£o esf√©ricos e quando a matriz de covari√¢ncia √© conhecida. A compreens√£o dessas propriedades √© essencial para realizar infer√™ncias estat√≠sticas robustas e obter estimativas eficientes dos par√¢metros de interesse.

### Refer√™ncias
[^0]: Previous Topics
[^8.1.1]:  ...
[^8.1.2]:  ...
[^8.1.5]:  ...
[^8.1.6]: ...
[^8.1.10]: ...
[^8.1.17]: ...
[^8.2.31]:  ...
[^8.3.1]: ...
[^8.3.2]: ...
[^8.3.3]: ...
[^8.3.4]: ...
[^8.3.5]: ...
[^8.3.6]: ...
<!-- END -->
