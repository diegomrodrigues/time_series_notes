## AnÃ¡lise Detalhada dos ResÃ­duos Populacionais em RegressÃ£o Linear

### IntroduÃ§Ã£o
Este capÃ­tulo aprofundarÃ¡ nossa compreensÃ£o dos **resÃ­duos populacionais** no contexto da regressÃ£o linear. Como vimos anteriormente, a anÃ¡lise dos resÃ­duos Ã© crucial para avaliar a adequaÃ§Ã£o de um modelo e identificar possÃ­veis problemas de especificaÃ§Ã£o. Partindo dos conceitos de estimador OLS, parÃ¢metros populacionais, e medidas de ajuste, como o $R^2$ [^Previous Topic 2], exploraremos como os resÃ­duos populacionais podem ser expressos em termos dos dados observados e dos erros populacionais, e examinaremos a sua relaÃ§Ã£o com a matriz de projeÃ§Ã£o. Esta anÃ¡lise Ã© essencial para a inferÃªncia estatÃ­stica e para a validaÃ§Ã£o dos resultados da regressÃ£o. Em continuidade com a exploraÃ§Ã£o da relaÃ§Ã£o entre o estimador OLS e o parÃ¢metro populacional [^Previous Topic 1], aqui focaremos em derivar e interpretar a expressÃ£o dos resÃ­duos populacionais.

### DerivaÃ§Ã£o dos ResÃ­duos Populacionais
Em um modelo de regressÃ£o linear, os **resÃ­duos amostrais**, denotados por $\hat{u}$, sÃ£o a diferenÃ§a entre os valores observados da variÃ¡vel dependente $y$ e os valores preditos $\hat{y}$ pela regressÃ£o [^8.1.4]:
$$\hat{u} = y - \hat{y}$$
onde $\hat{y} = Xb$ e $b$ Ã© o estimador OLS, dado por $b = (X'X)^{-1}X'y$.  O modelo populacional Ã© dado por:
$$y = X\beta + u$$
Para obter os **resÃ­duos populacionais** (e nÃ£o amostrais), que sÃ£o as diferenÃ§as entre os valores de y e seus valores preditos a partir dos parÃ¢metros populacionais (ou seja, os verdadeiros valores), substituÃ­mos o modelo populacional na expressÃ£o dos resÃ­duos amostrais. Os resÃ­duos amostrais sÃ£o obtidos utilizando os parÃ¢metros estimados $b$, enquanto os resÃ­duos populacionais sÃ£o obtidos usando os verdadeiros parÃ¢metros $\beta$.

Substituindo a expressÃ£o para o vetor de valores preditos $\hat{y}$, temos:
$$\hat{u} = y - Xb$$
Substituindo agora o modelo populacional $y = X\beta + u$, obtemos:
$$\hat{u} = (X\beta + u) - Xb$$
Podemos substituir a expressÃ£o de $b$, onde $b = \beta + (X'X)^{-1}X'u$ [^Previous Topic 1], na equaÃ§Ã£o dos resÃ­duos amostrais, e obtemos:
$$\hat{u} = X\beta + u - X(\beta + (X'X)^{-1}X'u)$$
$$\hat{u} = X\beta + u - X\beta - X(X'X)^{-1}X'u$$
Simplificando, temos:
$$\hat{u} = u - X(X'X)^{-1}X'u$$
Reescrevendo como uma operaÃ§Ã£o de matriz, obtemos:
$$\hat{u} = (I - X(X'X)^{-1}X')u$$
onde $I$ Ã© a matriz identidade. O termo $M_X = I - X(X'X)^{-1}X'$ Ã© conhecido como **matriz de projeÃ§Ã£o**, que possui propriedades notÃ¡veis que exploraremos adiante. Assim, a expressÃ£o final para os resÃ­duos populacionais Ã©:
$$\hat{u} = M_X u$$
Esta equaÃ§Ã£o [^8.1.11] mostra que os resÃ­duos populacionais $\hat{u}$ sÃ£o uma transformaÃ§Ã£o linear dos erros populacionais $u$, por meio da matriz de projeÃ§Ã£o $M_X$. O estimador do resÃ­duo Ã© dado por:
$$\hat{u} = y - Xb = M_Xy = M_X(X\beta+u) = M_Xu$$
pois $M_X X = 0$.

> ðŸ’¡ **InterpretaÃ§Ã£o:**
>  A matriz de projeÃ§Ã£o $M_X$ tem um papel crucial na anÃ¡lise da regressÃ£o linear, transformando os erros populacionais em resÃ­duos. Ela Ã© uma matriz idempotente (ou seja, $M_X M_X = M_X$) e simÃ©trica (ou seja, $M_X = M'_X$). AlÃ©m disso, ela projeta qualquer vetor em um subespaÃ§o ortogonal ao espaÃ§o coluna da matriz $X$.
>
> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Usando o exemplo numÃ©rico anterior onde temos o vetor de erros $u = \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}$ e a matriz $X = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 5 \end{bmatrix}$, vamos computar a matriz $M_X$. Usamos que $(X'X)^{-1} =  \begin{bmatrix} 2.71 & -0.71 \\ -0.71 & 0.21 \end{bmatrix}$
>
> $$ M_X = I - X(X'X)^{-1}X' $$
>
> Primeiro, calculemos $X(X'X)^{-1}X'$:
>
> $$ X(X'X)^{-1}X' = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 5 \end{bmatrix}  \begin{bmatrix} 2.71 & -0.71 \\ -0.71 & 0.21 \end{bmatrix} \begin{bmatrix} 1 & 1 & 1 \\ 2 & 3 & 5 \end{bmatrix} $$
> $$ X(X'X)^{-1}X' =  \begin{bmatrix} 1.42 & 0 \\ 0 & 0.42 \\ 0 & 0 \end{bmatrix}  \begin{bmatrix} 1 & 1 & 1 \\ 2 & 3 & 5 \end{bmatrix} = \begin{bmatrix}  0.64 & 0.5 & -0.21 \\  0.5 & 0.5 & 0.5 \\ -0.21 & 0.5 & 0.79 \end{bmatrix} $$
>
> EntÃ£o,
>
> $$ M_X = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} - \begin{bmatrix}  0.64 & 0.5 & -0.21 \\  0.5 & 0.5 & 0.5 \\ -0.21 & 0.5 & 0.79 \end{bmatrix} = \begin{bmatrix}  0.36 & -0.5 & 0.21 \\  -0.5 & 0.5 & -0.5 \\ 0.21 & -0.5 & 0.21 \end{bmatrix} $$
>
>  Agora, podemos computar os resÃ­duos populacionais:
>
> $$ \hat{u} = M_X u = \begin{bmatrix}  0.36 & -0.5 & 0.21 \\  -0.5 & 0.5 & -0.5 \\ 0.21 & -0.5 & 0.21 \end{bmatrix} \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix} = \begin{bmatrix} -0.86 \\ 1.5 \\ -0.71 \end{bmatrix} $$
>
>
> ```python
> import numpy as np
>
> # Definindo os dados do exemplo
> X = np.array([[1, 2], [1, 3], [1, 5]])
> u = np.array([-1, 1, 0])
>
> # Calculando a matriz (X'X)^-1
> XTX_inv = np.linalg.inv(X.T @ X)
>
> # Calculando a matriz de projeÃ§Ã£o M_X
> MX = np.eye(X.shape[0]) - X @ XTX_inv @ X.T
>
> # Calculando os resÃ­duos populacionais u_hat
> u_hat = MX @ u
>
> print(f"Matriz de ProjeÃ§Ã£o M_X:\n{MX}")
> print(f"ResÃ­duos Populacionais u_hat:\n{u_hat}")
> ```
>
> ðŸ’¡ **VisualizaÃ§Ã£o:**
> Podemos visualizar a aÃ§Ã£o da matriz de projeÃ§Ã£o em um espaÃ§o bidimensional. Imagine que o espaÃ§o coluna de X Ã© uma linha. A matriz de projeÃ§Ã£o pega qualquer vetor de erro $u$ e projeta ele no espaÃ§o ortogonal a essa linha, resultando no resÃ­duo $\hat{u}$.
> ```mermaid
>   graph LR
>      A[EspaÃ§o Total] --> B(EspaÃ§o Coluna de X);
>      A --> C(EspaÃ§o Ortogonal a X);
>      D[Vetor u] --> C;
>      C --> E[Vetor resÃ­duo u_hat];
>
> ```
>

**Caixa de Destaque:**

> A matriz de projeÃ§Ã£o $M_X = I - X(X'X)^{-1}X'$ Ã© simÃ©trica e idempotente e tem um papel fundamental na transformaÃ§Ã£o dos erros populacionais em resÃ­duos no modelo de regressÃ£o linear.

**Lema 1**
A matriz de projeÃ§Ã£o $M_X$ Ã© idempotente, ou seja, $M_X M_X = M_X$.

*Prova:*
I. ComeÃ§amos com a definiÃ§Ã£o da matriz de projeÃ§Ã£o: $M_X = I - X(X'X)^{-1}X'$.
II.  Calculamos $M_X M_X$:
$$M_X M_X = (I - X(X'X)^{-1}X')(I - X(X'X)^{-1}X')$$
III. Expandindo a expressÃ£o, temos:
$$M_X M_X = I - X(X'X)^{-1}X' - X(X'X)^{-1}X' + X(X'X)^{-1}X'X(X'X)^{-1}X'$$
IV. Dado que $X'X(X'X)^{-1} = I$, onde $I$ Ã© a matriz identidade, a expressÃ£o simplifica para:
$$M_X M_X = I - X(X'X)^{-1}X' - X(X'X)^{-1}X' + X(X'X)^{-1}X'$$
V.  Os dois Ãºltimos termos se cancelam, resultando em:
$$M_X M_X = I - X(X'X)^{-1}X'$$
VI.  Portanto, $M_X M_X = M_X$, o que demonstra que $M_X$ Ã© idempotente.  â– 

**Lema 2**
A matriz de projeÃ§Ã£o $M_X$ Ã© simÃ©trica, ou seja, $M_X = M'_X$.

*Prova:*
I.  ComeÃ§amos com a definiÃ§Ã£o da matriz de projeÃ§Ã£o: $M_X = I - X(X'X)^{-1}X'$.
II.  Tomamos a transposta de $M_X$:
$$M'_X = (I - X(X'X)^{-1}X')'$$
III. Usando as propriedades da transposta, temos:
$$M'_X = I' - (X(X'X)^{-1}X')'$$
IV.  Como $I' = I$, e usando a propriedade $(ABC)' = C'B'A'$, temos:
$$M'_X = I - (X')'((X'X)^{-1})'X'$$
V.  Como $(X')' = X$ e $(A^{-1})' = (A')^{-1}$, e sabendo que $(X'X)' = X'X$, temos:
$$M'_X = I - X((X'X)')^{-1}X' = I - X(X'X)^{-1}X'$$
VI.  Portanto, $M'_X = M_X$, o que demonstra que a matriz de projeÃ§Ã£o $M_X$ Ã© simÃ©trica.  â– 

**Lema 3**
Se multiplicarmos a matriz de projeÃ§Ã£o pela matriz de variÃ¡veis explicativas, o resultado Ã© 0, ou seja, $M_XX = 0$

*Prova:*
I. ComeÃ§amos com a definiÃ§Ã£o da matriz de projeÃ§Ã£o: $M_X = I - X(X'X)^{-1}X'$
II. Multiplicamos $M_X$ por $X$:
$$M_X X = (I - X(X'X)^{-1}X')X$$
III. Distribuindo, temos:
$$M_X X = X - X(X'X)^{-1}X'X$$
IV. Como $X'X(X'X)^{-1} = I$, entÃ£o
$$M_X X = X - X(I) = X - X = 0$$
V. Portanto, $M_X X = 0$. â– 

**Lema 4**
A matriz de projeÃ§Ã£o $M_X$ ortogonaliza o vetor $y$ em relaÃ§Ã£o ao espaÃ§o coluna de $X$, ou seja, a projeÃ§Ã£o de $y$ por $M_X$ estÃ¡ ortogonal ao espaÃ§o coluna de $X$.

*Prova:*
I.  Sabemos que $M_Xy = \hat{u}$ sÃ£o os resÃ­duos.
II.  Para mostrar que $\hat{u}$ Ã© ortogonal ao espaÃ§o coluna de $X$, precisamos mostrar que o produto escalar de $\hat{u}$ com qualquer vetor no espaÃ§o coluna de $X$ Ã© zero. Qualquer vetor no espaÃ§o coluna de $X$ pode ser expresso como $X\alpha$ para algum vetor $\alpha$.
III. Calculamos o produto escalar de $\hat{u}$ com $X\alpha$:
$$\hat{u}'(X\alpha) = (M_X u)'(X\alpha) = u'M_X'(X\alpha)$$
IV. Pelo Lema 2, $M_X$ Ã© simÃ©trica, entÃ£o $M_X' = M_X$. TambÃ©m sabemos do Lema 3 que $M_XX=0$, entÃ£o:
$$u'M_X'(X\alpha) = u'M_X(X\alpha) = u'(M_XX)\alpha = u'0\alpha = 0$$
V. Portanto, os resÃ­duos $\hat{u}$ sÃ£o ortogonais ao espaÃ§o coluna de $X$.  â– 

**Teorema 1**
O traÃ§o da matriz de projeÃ§Ã£o $M_X$ Ã© igual a $n-k$, onde $n$ Ã© o nÃºmero de observaÃ§Ãµes e $k$ Ã© o nÃºmero de regressores (incluindo a constante).

*Prova:*
I. ComeÃ§amos com a definiÃ§Ã£o da matriz de projeÃ§Ã£o: $M_X = I - X(X'X)^{-1}X'$.
II.  O traÃ§o de uma matriz Ã© a soma de seus elementos diagonais, e o traÃ§o de uma soma Ã© a soma dos traÃ§os. Portanto,
    $$tr(M_X) = tr(I - X(X'X)^{-1}X') = tr(I) - tr(X(X'X)^{-1}X')$$
III. O traÃ§o da matriz identidade $I_{n \times n}$ Ã© $n$. Pela propriedade do traÃ§o de um produto cÃ­clico ($tr(ABC) = tr(BCA) = tr(CAB)$), temos:
    $$tr(X(X'X)^{-1}X') = tr((X'X)^{-1}X'X) = tr((X'X)^{-1}(X'X)) = tr(I_k)$$
    Onde $I_k$ Ã© uma matriz identidade de dimensÃ£o $k \times k$.
IV. O traÃ§o de $I_k$ Ã© $k$.
V. Portanto:
   $$tr(M_X) = n - k$$ â– 

> ðŸ’¡ **Exemplo NumÃ©rico - TraÃ§o da Matriz de ProjeÃ§Ã£o:**
>
> Continuando com o exemplo numÃ©rico anterior onde $X = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 5 \end{bmatrix}$, temos $n = 3$ observaÃ§Ãµes e $k = 2$ regressores (incluindo a constante). Calculamos a matriz $M_X$ como:
>
> $$ M_X = \begin{bmatrix}  0.36 & -0.5 & 0.21 \\  -0.5 & 0.5 & -0.5 \\ 0.21 & -0.5 & 0.21 \end{bmatrix} $$
>
> O traÃ§o de $M_X$ Ã© a soma dos elementos da diagonal:
>
> $$tr(M_X) = 0.36 + 0.5 + 0.21 = 1.07$$
> Note que devido a erros de arredondamento, este valor deveria ser igual a 1, mas como estamos trabalhando com aproximaÃ§Ãµes decimais para os valores de $(X'X)^{-1}$, obtivemos o valor de 1.07.
>
> Usando o teorema, $tr(M_X)$ deve ser igual a $n - k = 3 - 2 = 1$.  Este exemplo ilustra como o traÃ§o da matriz de projeÃ§Ã£o estÃ¡ diretamente relacionado com o nÃºmero de observaÃ§Ãµes e regressores no modelo, fornecendo uma maneira adicional de verificar os resultados computacionais.
>
> ```python
> import numpy as np
>
> # Definindo os dados do exemplo
> X = np.array([[1, 2], [1, 3], [1, 5]])
>
> # Calculando a matriz (X'X)^-1
> XTX_inv = np.linalg.inv(X.T @ X)
>
> # Calculando a matriz de projeÃ§Ã£o M_X
> MX = np.eye(X.shape[0]) - X @ XTX_inv @ X.T
>
> # Calculando o traÃ§o de M_X
> trace_MX = np.trace(MX)
>
> # Calculando n e k
> n = X.shape[0]
> k = X.shape[1]
>
> print(f"TraÃ§o da Matriz de ProjeÃ§Ã£o M_X: {trace_MX}")
> print(f"n - k = {n - k}")
> ```

### ConclusÃ£o
Neste capÃ­tulo, derivamos a expressÃ£o para os resÃ­duos populacionais, $\hat{u} = M_X u$, e exploramos o papel crucial da matriz de projeÃ§Ã£o $M_X$ neste contexto. A matriz $M_X$ Ã© idempotente, simÃ©trica e transforma os erros populacionais em resÃ­duos, representando a projeÃ§Ã£o ortogonal dos erros no espaÃ§o ortogonal ao espaÃ§o coluna da matriz $X$. A compreensÃ£o dessas propriedades Ã© fundamental para a anÃ¡lise estatÃ­stica em modelos de regressÃ£o linear, permitindo avaliar o ajuste do modelo e realizar inferÃªncias sobre os parÃ¢metros. AlÃ©m disso, a demonstraÃ§Ã£o de que o traÃ§o de $M_X$ Ã© $n-k$ estabelece uma conexÃ£o importante com os graus de liberdade do modelo.

### ReferÃªncias
[^8.1.4]: Os resÃ­duos amostrais sÃ£o definidos como a diferenÃ§a entre os valores observados e os valores preditos pela regressÃ£o: $\hat{u} = y - Xb$.
[^8.1.11]: Os resÃ­duos populacionais sÃ£o dados por $\hat{u} = M_Xu$, onde $M_X$ Ã© a matriz de projeÃ§Ã£o.
[^Previous Topic 1]: A diferenÃ§a entre o estimador OLS e o parÃ¢metro populacional Ã© expressa como $b - \beta = (X'X)^{-1}X'u$.
[^Previous Topic 2]: O coeficiente de correlaÃ§Ã£o mÃºltipla ($R^2$) Ã© uma medida da proporÃ§Ã£o da variÃ¢ncia da variÃ¡vel dependente que Ã© explicada pelo modelo.
<!-- END -->
