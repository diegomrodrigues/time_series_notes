## An√°lise Detalhada dos Res√≠duos Populacionais em Regress√£o Linear

### Introdu√ß√£o
Este cap√≠tulo aprofundar√° nossa compreens√£o dos **res√≠duos populacionais** no contexto da regress√£o linear. Como vimos anteriormente, a an√°lise dos res√≠duos √© crucial para avaliar a adequa√ß√£o de um modelo e identificar poss√≠veis problemas de especifica√ß√£o. Partindo dos conceitos de estimador OLS, par√¢metros populacionais, e medidas de ajuste, como o $R^2$ [^Previous Topic 2], exploraremos como os res√≠duos populacionais podem ser expressos em termos dos dados observados e dos erros populacionais, e examinaremos a sua rela√ß√£o com a matriz de proje√ß√£o. Esta an√°lise √© essencial para a infer√™ncia estat√≠stica e para a valida√ß√£o dos resultados da regress√£o. Em continuidade com a explora√ß√£o da rela√ß√£o entre o estimador OLS e o par√¢metro populacional [^Previous Topic 1], aqui focaremos em derivar e interpretar a express√£o dos res√≠duos populacionais.

### Deriva√ß√£o dos Res√≠duos Populacionais
Em um modelo de regress√£o linear, os **res√≠duos amostrais**, denotados por $\hat{u}$, s√£o a diferen√ßa entre os valores observados da vari√°vel dependente $y$ e os valores preditos $\hat{y}$ pela regress√£o [^8.1.4]:
$$\hat{u} = y - \hat{y}$$
onde $\hat{y} = Xb$ e $b$ √© o estimador OLS, dado por $b = (X'X)^{-1}X'y$.  O modelo populacional √© dado por:
$$y = X\beta + u$$
Para obter os **res√≠duos populacionais** (e n√£o amostrais), que s√£o as diferen√ßas entre os valores de y e seus valores preditos a partir dos par√¢metros populacionais (ou seja, os verdadeiros valores), substitu√≠mos o modelo populacional na express√£o dos res√≠duos amostrais. Os res√≠duos amostrais s√£o obtidos utilizando os par√¢metros estimados $b$, enquanto os res√≠duos populacionais s√£o obtidos usando os verdadeiros par√¢metros $\beta$.

Substituindo a express√£o para o vetor de valores preditos $\hat{y}$, temos:
$$\hat{u} = y - Xb$$
Substituindo agora o modelo populacional $y = X\beta + u$, obtemos:
$$\hat{u} = (X\beta + u) - Xb$$
Podemos substituir a express√£o de $b$, onde $b = \beta + (X'X)^{-1}X'u$ [^Previous Topic 1], na equa√ß√£o dos res√≠duos amostrais, e obtemos:
$$\hat{u} = X\beta + u - X(\beta + (X'X)^{-1}X'u)$$
$$\hat{u} = X\beta + u - X\beta - X(X'X)^{-1}X'u$$
Simplificando, temos:
$$\hat{u} = u - X(X'X)^{-1}X'u$$
Reescrevendo como uma opera√ß√£o de matriz, obtemos:
$$\hat{u} = (I - X(X'X)^{-1}X')u$$
onde $I$ √© a matriz identidade. O termo $M_X = I - X(X'X)^{-1}X'$ √© conhecido como **matriz de proje√ß√£o**, que possui propriedades not√°veis que exploraremos adiante. Assim, a express√£o final para os res√≠duos populacionais √©:
$$\hat{u} = M_X u$$
Esta equa√ß√£o [^8.1.11] mostra que os res√≠duos populacionais $\hat{u}$ s√£o uma transforma√ß√£o linear dos erros populacionais $u$, por meio da matriz de proje√ß√£o $M_X$. O estimador do res√≠duo √© dado por:
$$\hat{u} = y - Xb = M_Xy = M_X(X\beta+u) = M_Xu$$
pois $M_X X = 0$.

> üí° **Interpreta√ß√£o:**
>  A matriz de proje√ß√£o $M_X$ tem um papel crucial na an√°lise da regress√£o linear, transformando os erros populacionais em res√≠duos. Ela √© uma matriz idempotente (ou seja, $M_X M_X = M_X$) e sim√©trica (ou seja, $M_X = M'_X$). Al√©m disso, ela projeta qualquer vetor em um subespa√ßo ortogonal ao espa√ßo coluna da matriz $X$.
>
> üí° **Exemplo Num√©rico:**
>
> Usando o exemplo num√©rico anterior onde temos o vetor de erros $u = \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}$ e a matriz $X = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 5 \end{bmatrix}$, vamos computar a matriz $M_X$. Usamos que $(X'X)^{-1} =  \begin{bmatrix} 2.71 & -0.71 \\ -0.71 & 0.21 \end{bmatrix}$
>
> $$ M_X = I - X(X'X)^{-1}X' $$
>
> Primeiro, calculemos $X(X'X)^{-1}X'$:
>
> $$ X(X'X)^{-1}X' = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 5 \end{bmatrix}  \begin{bmatrix} 2.71 & -0.71 \\ -0.71 & 0.21 \end{bmatrix} \begin{bmatrix} 1 & 1 & 1 \\ 2 & 3 & 5 \end{bmatrix} $$
> $$ X(X'X)^{-1}X' =  \begin{bmatrix} 1.42 & 0 \\ 0 & 0.42 \\ 0 & 0 \end{bmatrix}  \begin{bmatrix} 1 & 1 & 1 \\ 2 & 3 & 5 \end{bmatrix} = \begin{bmatrix}  0.64 & 0.5 & -0.21 \\  0.5 & 0.5 & 0.5 \\ -0.21 & 0.5 & 0.79 \end{bmatrix} $$
>
> Ent√£o,
>
> $$ M_X = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} - \begin{bmatrix}  0.64 & 0.5 & -0.21 \\  0.5 & 0.5 & 0.5 \\ -0.21 & 0.5 & 0.79 \end{bmatrix} = \begin{bmatrix}  0.36 & -0.5 & 0.21 \\  -0.5 & 0.5 & -0.5 \\ 0.21 & -0.5 & 0.21 \end{bmatrix} $$
>
>  Agora, podemos computar os res√≠duos populacionais:
>
> $$ \hat{u} = M_X u = \begin{bmatrix}  0.36 & -0.5 & 0.21 \\  -0.5 & 0.5 & -0.5 \\ 0.21 & -0.5 & 0.21 \end{bmatrix} \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix} = \begin{bmatrix} -0.86 \\ 1.5 \\ -0.71 \end{bmatrix} $$
>
>
> ```python
> import numpy as np
>
> # Definindo os dados do exemplo
> X = np.array([[1, 2], [1, 3], [1, 5]])
> u = np.array([-1, 1, 0])
>
> # Calculando a matriz (X'X)^-1
> XTX_inv = np.linalg.inv(X.T @ X)
>
> # Calculando a matriz de proje√ß√£o M_X
> MX = np.eye(X.shape[0]) - X @ XTX_inv @ X.T
>
> # Calculando os res√≠duos populacionais u_hat
> u_hat = MX @ u
>
> print(f"Matriz de Proje√ß√£o M_X:\n{MX}")
> print(f"Res√≠duos Populacionais u_hat:\n{u_hat}")
> ```
>
> üí° **Visualiza√ß√£o:**
> Podemos visualizar a a√ß√£o da matriz de proje√ß√£o em um espa√ßo bidimensional. Imagine que o espa√ßo coluna de X √© uma linha. A matriz de proje√ß√£o pega qualquer vetor de erro $u$ e projeta ele no espa√ßo ortogonal a essa linha, resultando no res√≠duo $\hat{u}$.
> ```mermaid
>   graph LR
>      A[Espa√ßo Total] --> B(Espa√ßo Coluna de X);
>      A --> C(Espa√ßo Ortogonal a X);
>      D[Vetor u] --> C;
>      C --> E[Vetor res√≠duo u_hat];
>
> ```
>

**Caixa de Destaque:**

> A matriz de proje√ß√£o $M_X = I - X(X'X)^{-1}X'$ √© sim√©trica e idempotente e tem um papel fundamental na transforma√ß√£o dos erros populacionais em res√≠duos no modelo de regress√£o linear.

**Lema 1**
A matriz de proje√ß√£o $M_X$ √© idempotente, ou seja, $M_X M_X = M_X$.

*Prova:*
I. Come√ßamos com a defini√ß√£o da matriz de proje√ß√£o: $M_X = I - X(X'X)^{-1}X'$.
II.  Calculamos $M_X M_X$:
$$M_X M_X = (I - X(X'X)^{-1}X')(I - X(X'X)^{-1}X')$$
III. Expandindo a express√£o, temos:
$$M_X M_X = I - X(X'X)^{-1}X' - X(X'X)^{-1}X' + X(X'X)^{-1}X'X(X'X)^{-1}X'$$
IV. Dado que $X'X(X'X)^{-1} = I$, onde $I$ √© a matriz identidade, a express√£o simplifica para:
$$M_X M_X = I - X(X'X)^{-1}X' - X(X'X)^{-1}X' + X(X'X)^{-1}X'$$
V.  Os dois √∫ltimos termos se cancelam, resultando em:
$$M_X M_X = I - X(X'X)^{-1}X'$$
VI.  Portanto, $M_X M_X = M_X$, o que demonstra que $M_X$ √© idempotente.  ‚ñ†

**Lema 2**
A matriz de proje√ß√£o $M_X$ √© sim√©trica, ou seja, $M_X = M'_X$.

*Prova:*
I.  Come√ßamos com a defini√ß√£o da matriz de proje√ß√£o: $M_X = I - X(X'X)^{-1}X'$.
II.  Tomamos a transposta de $M_X$:
$$M'_X = (I - X(X'X)^{-1}X')'$$
III. Usando as propriedades da transposta, temos:
$$M'_X = I' - (X(X'X)^{-1}X')'$$
IV.  Como $I' = I$, e usando a propriedade $(ABC)' = C'B'A'$, temos:
$$M'_X = I - (X')'((X'X)^{-1})'X'$$
V.  Como $(X')' = X$ e $(A^{-1})' = (A')^{-1}$, e sabendo que $(X'X)' = X'X$, temos:
$$M'_X = I - X((X'X)')^{-1}X' = I - X(X'X)^{-1}X'$$
VI.  Portanto, $M'_X = M_X$, o que demonstra que a matriz de proje√ß√£o $M_X$ √© sim√©trica.  ‚ñ†

**Lema 3**
Se multiplicarmos a matriz de proje√ß√£o pela matriz de vari√°veis explicativas, o resultado √© 0, ou seja, $M_XX = 0$

*Prova:*
I. Come√ßamos com a defini√ß√£o da matriz de proje√ß√£o: $M_X = I - X(X'X)^{-1}X'$
II. Multiplicamos $M_X$ por $X$:
$$M_X X = (I - X(X'X)^{-1}X')X$$
III. Distribuindo, temos:
$$M_X X = X - X(X'X)^{-1}X'X$$
IV. Como $X'X(X'X)^{-1} = I$, ent√£o
$$M_X X = X - X(I) = X - X = 0$$
V. Portanto, $M_X X = 0$. ‚ñ†

**Lema 4**
A matriz de proje√ß√£o $M_X$ ortogonaliza o vetor $y$ em rela√ß√£o ao espa√ßo coluna de $X$, ou seja, a proje√ß√£o de $y$ por $M_X$ est√° ortogonal ao espa√ßo coluna de $X$.

*Prova:*
I.  Sabemos que $M_Xy = \hat{u}$ s√£o os res√≠duos.
II.  Para mostrar que $\hat{u}$ √© ortogonal ao espa√ßo coluna de $X$, precisamos mostrar que o produto escalar de $\hat{u}$ com qualquer vetor no espa√ßo coluna de $X$ √© zero. Qualquer vetor no espa√ßo coluna de $X$ pode ser expresso como $X\alpha$ para algum vetor $\alpha$.
III. Calculamos o produto escalar de $\hat{u}$ com $X\alpha$:
$$\hat{u}'(X\alpha) = (M_X u)'(X\alpha) = u'M_X'(X\alpha)$$
IV. Pelo Lema 2, $M_X$ √© sim√©trica, ent√£o $M_X' = M_X$. Tamb√©m sabemos do Lema 3 que $M_XX=0$, ent√£o:
$$u'M_X'(X\alpha) = u'M_X(X\alpha) = u'(M_XX)\alpha = u'0\alpha = 0$$
V. Portanto, os res√≠duos $\hat{u}$ s√£o ortogonais ao espa√ßo coluna de $X$.  ‚ñ†

**Teorema 1**
O tra√ßo da matriz de proje√ß√£o $M_X$ √© igual a $n-k$, onde $n$ √© o n√∫mero de observa√ß√µes e $k$ √© o n√∫mero de regressores (incluindo a constante).

*Prova:*
I. Come√ßamos com a defini√ß√£o da matriz de proje√ß√£o: $M_X = I - X(X'X)^{-1}X'$.
II.  O tra√ßo de uma matriz √© a soma de seus elementos diagonais, e o tra√ßo de uma soma √© a soma dos tra√ßos. Portanto,
    $$tr(M_X) = tr(I - X(X'X)^{-1}X') = tr(I) - tr(X(X'X)^{-1}X')$$
III. O tra√ßo da matriz identidade $I_{n \times n}$ √© $n$. Pela propriedade do tra√ßo de um produto c√≠clico ($tr(ABC) = tr(BCA) = tr(CAB)$), temos:
    $$tr(X(X'X)^{-1}X') = tr((X'X)^{-1}X'X) = tr((X'X)^{-1}(X'X)) = tr(I_k)$$
    Onde $I_k$ √© uma matriz identidade de dimens√£o $k \times k$.
IV. O tra√ßo de $I_k$ √© $k$.
V. Portanto:
   $$tr(M_X) = n - k$$ ‚ñ†

> üí° **Exemplo Num√©rico - Tra√ßo da Matriz de Proje√ß√£o:**
>
> Continuando com o exemplo num√©rico anterior onde $X = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 5 \end{bmatrix}$, temos $n = 3$ observa√ß√µes e $k = 2$ regressores (incluindo a constante). Calculamos a matriz $M_X$ como:
>
> $$ M_X = \begin{bmatrix}  0.36 & -0.5 & 0.21 \\  -0.5 & 0.5 & -0.5 \\ 0.21 & -0.5 & 0.21 \end{bmatrix} $$
>
> O tra√ßo de $M_X$ √© a soma dos elementos da diagonal:
>
> $$tr(M_X) = 0.36 + 0.5 + 0.21 = 1.07$$
> Note que devido a erros de arredondamento, este valor deveria ser igual a 1, mas como estamos trabalhando com aproxima√ß√µes decimais para os valores de $(X'X)^{-1}$, obtivemos o valor de 1.07.
>
> Usando o teorema, $tr(M_X)$ deve ser igual a $n - k = 3 - 2 = 1$.  Este exemplo ilustra como o tra√ßo da matriz de proje√ß√£o est√° diretamente relacionado com o n√∫mero de observa√ß√µes e regressores no modelo, fornecendo uma maneira adicional de verificar os resultados computacionais.
>
> ```python
> import numpy as np
>
> # Definindo os dados do exemplo
> X = np.array([[1, 2], [1, 3], [1, 5]])
>
> # Calculando a matriz (X'X)^-1
> XTX_inv = np.linalg.inv(X.T @ X)
>
> # Calculando a matriz de proje√ß√£o M_X
> MX = np.eye(X.shape[0]) - X @ XTX_inv @ X.T
>
> # Calculando o tra√ßo de M_X
> trace_MX = np.trace(MX)
>
> # Calculando n e k
> n = X.shape[0]
> k = X.shape[1]
>
> print(f"Tra√ßo da Matriz de Proje√ß√£o M_X: {trace_MX}")
> print(f"n - k = {n - k}")
> ```

### Conclus√£o
Neste cap√≠tulo, derivamos a express√£o para os res√≠duos populacionais, $\hat{u} = M_X u$, e exploramos o papel crucial da matriz de proje√ß√£o $M_X$ neste contexto. A matriz $M_X$ √© idempotente, sim√©trica e transforma os erros populacionais em res√≠duos, representando a proje√ß√£o ortogonal dos erros no espa√ßo ortogonal ao espa√ßo coluna da matriz $X$. A compreens√£o dessas propriedades √© fundamental para a an√°lise estat√≠stica em modelos de regress√£o linear, permitindo avaliar o ajuste do modelo e realizar infer√™ncias sobre os par√¢metros. Al√©m disso, a demonstra√ß√£o de que o tra√ßo de $M_X$ √© $n-k$ estabelece uma conex√£o importante com os graus de liberdade do modelo.

### Refer√™ncias
[^8.1.4]: Os res√≠duos amostrais s√£o definidos como a diferen√ßa entre os valores observados e os valores preditos pela regress√£o: $\hat{u} = y - Xb$.
[^8.1.11]: Os res√≠duos populacionais s√£o dados por $\hat{u} = M_Xu$, onde $M_X$ √© a matriz de proje√ß√£o.
[^Previous Topic 1]: A diferen√ßa entre o estimador OLS e o par√¢metro populacional √© expressa como $b - \beta = (X'X)^{-1}X'u$.
[^Previous Topic 2]: O coeficiente de correla√ß√£o m√∫ltipla ($R^2$) √© uma medida da propor√ß√£o da vari√¢ncia da vari√°vel dependente que √© explicada pelo modelo.
<!-- END -->
