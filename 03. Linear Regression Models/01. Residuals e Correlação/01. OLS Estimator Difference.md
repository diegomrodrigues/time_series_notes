## Deriva√ß√£o da Diferen√ßa entre o Estimador OLS e o Par√¢metro Populacional

### Introdu√ß√£o
Neste cap√≠tulo, aprofundaremos a an√°lise do estimador de **m√≠nimos quadrados ordin√°rios (OLS)** e sua rela√ß√£o com o verdadeiro par√¢metro populacional, utilizando as ferramentas da √°lgebra matricial e infer√™ncia estat√≠stica apresentadas nos t√≥picos anteriores. Especificamente, exploraremos como a diferen√ßa entre o estimador OLS, denotado por $b$, e o par√¢metro populacional verdadeiro, denotado por $\beta$, pode ser expressa em termos das vari√°veis explicativas, $X$, e os erros populacionais, $u$. Esta deriva√ß√£o √© crucial para entender as propriedades estat√≠sticas do estimador OLS, como seu vi√©s e vari√¢ncia.

### Conceitos Fundamentais
Come√ßaremos recordando o modelo de **regress√£o linear** [^8.1.1]:
$$y = X\beta + u,$$
onde $y$ √© um vetor de observa√ß√µes da vari√°vel dependente, $X$ √© a matriz das vari√°veis explicativas, $\beta$ √© o vetor de par√¢metros a serem estimados e $u$ √© o vetor de erros populacionais. O estimador OLS para $\beta$, denotado por $b$, √© dado por [^8.1.6]:
$$b = (X'X)^{-1}X'y$$
Substituindo a equa√ß√£o do modelo de regress√£o na express√£o do estimador OLS, temos:
$$b = (X'X)^{-1}X'(X\beta + u)$$
$$b = (X'X)^{-1}X'X\beta + (X'X)^{-1}X'u$$
Como $(X'X)^{-1}X'X = I$, onde $I$ √© a matriz identidade, simplificamos para:
$$b = \beta + (X'X)^{-1}X'u$$
Rearranjando, obtemos a diferen√ßa entre o estimador OLS e o par√¢metro populacional:
$$b - \beta = (X'X)^{-1}X'u$$
Esta equa√ß√£o [^8.1.12] expressa a diferen√ßa entre o estimador OLS $b$ e o par√¢metro populacional verdadeiro $\beta$ em termos de $(X'X)^{-1}$ e $X'u$. Essa rela√ß√£o √© fundamental para analisar as propriedades estat√≠sticas do estimador OLS. O termo $(X'X)^{-1}X'$ atua como uma matriz de pesos que associa os erros populacionais $u$ √† diferen√ßa entre $b$ e $\beta$.

> üí° **Exemplo Num√©rico:** Suponha que temos um modelo com uma √∫nica vari√°vel explicativa (regress√£o simples) com a seguinte estrutura:
>
> $$ y_i = \beta_0 + \beta_1 x_i + u_i $$
>
> Para simplificar, considere que temos apenas 3 observa√ß√µes e, em forma matricial, temos:
>
> $$ y = \begin{bmatrix} 5 \\ 8 \\ 12 \end{bmatrix} \quad X = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 5 \end{bmatrix} \quad \beta = \begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix} \quad u = \begin{bmatrix} u_1 \\ u_2 \\ u_3 \end{bmatrix} $$
>
> Onde $y$ √© um vetor de valores observados, $X$ √© a matriz com uma coluna de 1s (para o intercepto) e uma coluna com os valores da vari√°vel explicativa, $\beta$ s√£o os par√¢metros a serem estimados, e $u$ s√£o os erros.
>
>  Vamos supor que o verdadeiro valor de $\beta$ seja $\beta = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$ e os erros populacionais sejam $u = \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}$.
>
>  Primeiro, vamos calcular $X'X$:
>
> $$ X'X = \begin{bmatrix} 1 & 1 & 1 \\ 2 & 3 & 5 \end{bmatrix} \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 5 \end{bmatrix} = \begin{bmatrix} 3 & 10 \\ 10 & 38 \end{bmatrix} $$
>
>  Agora, calculemos a inversa de $X'X$:
>
> $$ (X'X)^{-1} = \frac{1}{(3 \cdot 38 - 10 \cdot 10)} \begin{bmatrix} 38 & -10 \\ -10 & 3 \end{bmatrix} = \frac{1}{14} \begin{bmatrix} 38 & -10 \\ -10 & 3 \end{bmatrix} =  \begin{bmatrix} 2.71 & -0.71 \\ -0.71 & 0.21 \end{bmatrix} $$
>
>  Calculamos $X'u$:
>
> $$ X'u = \begin{bmatrix} 1 & 1 & 1 \\ 2 & 3 & 5 \end{bmatrix} \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 0 \\ 1 \end{bmatrix} $$
>
>  Agora, podemos calcular $(X'X)^{-1}X'u$:
>
> $$ (X'X)^{-1}X'u = \begin{bmatrix} 2.71 & -0.71 \\ -0.71 & 0.21 \end{bmatrix} \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} -0.71 \\ 0.21 \end{bmatrix} $$
>
>  Portanto, $b - \beta = \begin{bmatrix} -0.71 \\ 0.21 \end{bmatrix}$. Isso significa que o estimador OLS $b$ estar√° ligeiramente diferente do verdadeiro par√¢metro $\beta$ devido aos erros populacionais. O valor estimado de $b$ seria $b = \begin{bmatrix} 1 \\ 2 \end{bmatrix} + \begin{bmatrix} -0.71 \\ 0.21 \end{bmatrix} = \begin{bmatrix} 0.29 \\ 2.21 \end{bmatrix}$. Essa diferen√ßa ilustra que os erros populacionais afetam diretamente o valor do estimador OLS, e a matriz $X$ influencia essa rela√ß√£o.

**Caixa de Destaque:**
>A equa√ß√£o $$b - \beta = (X'X)^{-1}X'u$$ √© crucial para a infer√™ncia estat√≠stica na regress√£o linear. Ela demonstra que a diferen√ßa entre o estimador OLS e o par√¢metro verdadeiro √© uma fun√ß√£o dos erros populacionais e da matriz de vari√°veis explicativas.

√â importante notar que esta deriva√ß√£o n√£o imp√µe nenhuma restri√ß√£o sobre as propriedades estat√≠sticas dos erros, al√©m daquelas necess√°rias para a exist√™ncia da inversa $(X'X)^{-1}$, ou seja, que a matriz $X$ seja de *rank* coluna completo. Isso significa que a diferen√ßa entre o estimador OLS e o par√¢metro populacional pode ser analisada independentemente das particularidades da distribui√ß√£o dos erros populacionais.

**Lema 1**
A matriz $X'X$ √© invert√≠vel se e somente se a matriz $X$ tem posto de coluna completo.

*Prova:*
I. A matriz $X'X$ √© invert√≠vel se e somente se seu determinante √© diferente de zero.
II. A matriz $X'X$ √© uma matriz quadrada de dimens√£o $k \times k$, onde $k$ √© o n√∫mero de colunas de $X$.
III. Se a matriz $X$ tem posto de coluna completo, ent√£o suas colunas s√£o linearmente independentes.
IV. Consequentemente, a matriz $X'X$ tem posto m√°ximo, o que garante que seu determinante seja n√£o nulo e, portanto, que ela seja invert√≠vel.
V. Por outro lado, se a matriz $X$ n√£o tem posto de coluna completo, suas colunas s√£o linearmente dependentes.
VI. Isso implica que a matriz $X'X$ tamb√©m n√£o ter√° posto completo, e portanto, seu determinante ser√° nulo, o que a torna n√£o invert√≠vel. ‚ñ†

**Observa√ß√£o:**
A condi√ß√£o de posto de coluna completo para a matriz $X$ √© uma condi√ß√£o fundamental para a aplica√ß√£o do m√©todo de m√≠nimos quadrados ordin√°rios (OLS). Se essa condi√ß√£o n√£o for satisfeita, a matriz $X'X$ n√£o ser√° invert√≠vel, e, portanto, o estimador OLS $b = (X'X)^{-1}X'y$ n√£o estar√° definido. Isso ressalta a import√¢ncia de verificar a independ√™ncia linear das vari√°veis explicativas antes de aplicar o modelo de regress√£o linear.

### Conclus√£o
A diferen√ßa entre o estimador OLS e o par√¢metro populacional √© uma rela√ß√£o linear que expressa a influ√™ncia dos erros populacionais no estimador. Esta rela√ß√£o, dada por $b - \beta = (X'X)^{-1}X'u$, nos permite analisar as propriedades estat√≠sticas do estimador, como seu vi√©s, vari√¢ncia e consist√™ncia. Expandindo essa an√°lise, podemos explorar outros aspectos importantes do modelo de regress√£o, como a estima√ß√£o da vari√¢ncia dos erros e a constru√ß√£o de testes de hip√≥teses.

**Teorema 1**
Se $E[u|X] = 0$, ent√£o o estimador OLS √© n√£o viesado, ou seja, $E[b|X] = \beta$.

*Prova:*
I. Come√ßamos com a express√£o derivada para a diferen√ßa entre o estimador OLS e o par√¢metro populacional:
$$b - \beta = (X'X)^{-1}X'u$$
II. Tomando a esperan√ßa condicional em rela√ß√£o a $X$, temos:
$$E[b - \beta|X] = E[(X'X)^{-1}X'u|X]$$
III. Como $X$ √© considerado fixo na an√°lise condicional, $(X'X)^{-1}X'$ tamb√©m √© fixo. Portanto:
$$E[b - \beta|X] = (X'X)^{-1}X'E[u|X]$$
IV. Dado que $E[u|X] = 0$ por hip√≥tese, temos:
$$E[b - \beta|X] = (X'X)^{-1}X' \cdot 0 = 0$$
V. Assim, $E[b|X] - \beta = 0$, o que implica que $E[b|X] = \beta$. Portanto, o estimador OLS √© n√£o viesado sob a condi√ß√£o de que a esperan√ßa condicional dos erros dados as vari√°veis explicativas seja zero. ‚ñ†

> üí° **Exemplo Num√©rico:** Vamos usar o exemplo num√©rico anterior e demonstrar o Teorema 1. Usamos os mesmo valores para $X$ e para $\beta$
> $$ X = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 5 \end{bmatrix} \quad \beta = \begin{bmatrix} 1 \\ 2 \end{bmatrix} $$
> Vamos assumir que os erros $u$ s√£o de uma distribui√ß√£o com m√©dia zero, ou seja, $E[u|X]=0$, por exemplo, os valores de $u$ s√£o dados por $u = \begin{bmatrix} -0.5 \\ 0.2 \\ 0.3 \end{bmatrix}$. Usando o modelo populacional, temos os valores de $y$:
>
>  $$ y = X\beta + u = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 5 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix} + \begin{bmatrix} -0.5 \\ 0.2 \\ 0.3 \end{bmatrix} =  \begin{bmatrix} 5 \\ 7.2 \\ 11.3 \end{bmatrix} $$
>  Aplicando a f√≥rmula do estimador OLS, temos:
>
> $$ b = (X'X)^{-1}X'y =  \begin{bmatrix} 2.71 & -0.71 \\ -0.71 & 0.21 \end{bmatrix} \begin{bmatrix} 1 & 1 & 1 \\ 2 & 3 & 5 \end{bmatrix} \begin{bmatrix} 5 \\ 7.2 \\ 11.3 \end{bmatrix} = \begin{bmatrix} 0.82 \\ 2.06 \end{bmatrix} $$
>
> A diferen√ßa entre $b$ e $\beta$ √© $b - \beta = \begin{bmatrix} -0.18 \\ 0.06 \end{bmatrix}$.
>
>  Se repetirmos este processo m√∫ltiplas vezes, com diferentes amostras de $u$ (mantendo $E[u|X]=0$), vamos observar que a m√©dia de $b$ se aproxima de $\beta$, ilustrando o Teorema 1. Isso mostra que, em m√©dia, o estimador OLS √© n√£o viesado quando $E[u|X] = 0$.
>

### Refer√™ncias
[^8.1.1]:  O modelo de regress√£o linear √© definido como $y = X\beta + u$.
[^8.1.6]: O estimador OLS √© dado por $b = (X'X)^{-1}X'y$.
[^8.1.12]: A diferen√ßa entre o estimador OLS e o par√¢metro populacional √© $b = \beta + (X'X)^{-1}X'u$ ou $b - \beta = (X'X)^{-1}X'u$.
<!-- END -->
