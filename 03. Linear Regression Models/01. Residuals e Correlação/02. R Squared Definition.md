## Coeficiente de Correla√ß√£o M√∫ltipla e Ajuste da Regress√£o OLS

### Introdu√ß√£o
Em continuidade √† an√°lise do modelo de regress√£o linear e das propriedades do estimador de m√≠nimos quadrados ordin√°rios (OLS), abordaremos neste cap√≠tulo o conceito de **coeficiente de correla√ß√£o m√∫ltipla ($R^2$)**. Este coeficiente √© uma medida crucial para avaliar a qualidade do ajuste de uma regress√£o OLS, indicando a propor√ß√£o da variabilidade da vari√°vel dependente que √© explicada pelas vari√°veis explicativas do modelo. Apresentaremos tanto a forma n√£o centrada quanto a centrada do $R^2$, explorando suas defini√ß√µes e interpreta√ß√µes. Complementando o t√≥pico anterior sobre a diferen√ßa entre o estimador OLS e o par√¢metro populacional [^Previous Topic 1], este cap√≠tulo ir√° fornecer uma m√©trica para avaliar a performance do modelo, usando os conceitos de soma de quadrados e vari√¢ncia explicada.

### Conceitos Fundamentais

O **coeficiente de correla√ß√£o m√∫ltipla**, ou $R^2$, √© uma medida que quantifica o qu√£o bem o modelo de regress√£o linear ajusta os dados observados [^8.1.13]. Ele representa a propor√ß√£o da variabilidade total da vari√°vel dependente que √© explicada pelo modelo de regress√£o. Existem duas formas principais de $R^2$: a n√£o centrada e a centrada, cada uma com sua interpreta√ß√£o espec√≠fica.

#### $R^2$ N√£o Centrado
O $R^2$ n√£o centrado, denotado por $R^2_u$, √© definido como a raz√£o entre a soma dos quadrados dos valores ajustados (ou preditos) da regress√£o e a soma dos quadrados dos valores observados da vari√°vel dependente [^8.1.13]. Matematicamente, √© expresso como:

$$R_u^2 = \frac{\sum_{t=1}^{T} (b'x_t)^2}{\sum_{t=1}^{T} y_t^2} = \frac{b'X'Xb}{y'y} = \frac{y'X(X'X)^{-1}X'y}{y'y} $$

Nesta express√£o, $b$ representa o vetor de estimativas OLS dos coeficientes, $x_t$ √© o vetor das vari√°veis explicativas para a observa√ß√£o $t$, e $y_t$ √© o valor observado da vari√°vel dependente para a observa√ß√£o $t$. A nota√ß√£o matricial simplifica a f√≥rmula, onde $X$ √© a matriz de todas as vari√°veis explicativas, $y$ √© o vetor de todas as vari√°veis dependentes. Note que a express√£o $b'X'Xb$  representa a soma dos quadrados dos valores preditos $(\hat{y})$.

O $R^2_u$ quantifica a propor√ß√£o da varia√ß√£o total da vari√°vel dependente que √© explicada pela regress√£o, sem ajustar pela m√©dia da vari√°vel dependente. √â importante notar que $R^2_u$ pode assumir valores negativos se o modelo n√£o incluir um termo constante [^8.1.14].

> üí° **Interpreta√ß√£o:** Um $R^2_u$ pr√≥ximo de 1 indica que o modelo explica uma grande parte da varia√ß√£o da vari√°vel dependente. No entanto, um valor alto de $R^2_u$ n√£o necessariamente implica que o modelo seja o melhor ou que tenha um bom desempenho preditivo fora da amostra.
>
> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um modelo com apenas uma vari√°vel explicativa e os seguintes dados:
>
> $y = \begin{bmatrix} 2 \\ 4 \\ 5 \\ 4 \\ 5 \end{bmatrix}$, $X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \\ 1 & 5 \end{bmatrix}$
>
> Usando a regress√£o OLS, obtemos o vetor de coeficientes $b = \begin{bmatrix} 1.5 \\ 0.7 \end{bmatrix}$. Os valores preditos $\hat{y}$ s√£o obtidos por:
>
> $\hat{y} = Xb = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \\ 1 & 5 \end{bmatrix} \begin{bmatrix} 1.5 \\ 0.7 \end{bmatrix} = \begin{bmatrix} 2.2 \\ 2.9 \\ 3.6 \\ 4.3 \\ 5.0 \end{bmatrix}$
>
> A soma dos quadrados dos valores ajustados √©:
>
> $\sum_{t=1}^{T} (b'x_t)^2 = 2.2^2 + 2.9^2 + 3.6^2 + 4.3^2 + 5.0^2 = 61.7$
>
> A soma dos quadrados dos valores observados √©:
>
> $\sum_{t=1}^{T} y_t^2 = 2^2 + 4^2 + 5^2 + 4^2 + 5^2 = 78$
>
> Portanto, o $R^2_u$ √©:
>
> $R^2_u = \frac{61.7}{78} = 0.791$
>
> Isso indica que 79.1% da varia√ß√£o total da vari√°vel dependente √© explicada pelo modelo, sem levar em conta a m√©dia.
>
> ```python
> import numpy as np
>
> y = np.array([2, 4, 5, 4, 5])
> X = np.array([[1, 1], [1, 2], [1, 3], [1, 4], [1, 5]])
>
> # Calculating b using the normal equation
> b = np.linalg.solve(X.T @ X, X.T @ y)
> print(f"Estimated coefficients b: {b}")
>
> # Calculating predicted values y_hat
> y_hat = X @ b
> print(f"Predicted values y_hat: {y_hat}")
>
> # Calculating R_u^2
> R_u_squared = np.sum(y_hat**2) / np.sum(y**2)
> print(f"R^2_u: {R_u_squared}")
> ```

#### $R^2$ Centrado
O $R^2$ centrado, denotado por $R^2$, √© uma vers√£o modificada do $R^2$ que ajusta pela m√©dia da vari√°vel dependente [^8.1.14]. Esta forma √© mais comumente utilizada em softwares estat√≠sticos e √© definida como:

$$R^2 = \frac{y'X(X'X)^{-1}X'y - T\bar{y}^2}{y'y - T\bar{y}^2} = \frac{y'X(X'X)^{-1}X'y - Ty^2}{y'y - Ty^2}$$

onde $T$ √© o n√∫mero de observa√ß√µes e $\bar{y}$ √© a m√©dia amostral da vari√°vel dependente, $y$. Uma maneira alternativa de ver essa f√≥rmula √© usando $y'My$ e $y'M_Xy$ onde $M = I - \frac{1}{T}11'$ e $M_X=I-X(X'X)^{-1}X'$, onde $1$ √© um vetor de 1s de tamanho $T$:

$$R^2 = 1 - \frac{y'M_Xy}{y'My}$$

O termo $T\bar{y}^2$ subtra√≠do tanto do numerador quanto do denominador ajusta a soma dos quadrados dos valores preditos e da vari√°vel dependente pela variabilidade devida √† m√©dia amostral, respectivamente. O $R^2$ centrado compara a variabilidade explicada pelo modelo em rela√ß√£o √† variabilidade total da vari√°vel dependente em torno de sua m√©dia, o que o torna uma medida mais adequada para avaliar o ajuste do modelo.

> üí° **Interpreta√ß√£o:** O $R^2$ centrado varia entre 0 e 1, com valores pr√≥ximos de 1 indicando um bom ajuste do modelo aos dados. Um valor de 0 indica que o modelo n√£o explica nenhuma parte da variabilidade da vari√°vel dependente em torno da sua m√©dia, enquanto um valor de 1 indica que o modelo explica toda a variabilidade. Um valor de $R^2$ perto de 0,5 indica que o modelo explica metade da vari√¢ncia da vari√°vel dependente em torno da sua m√©dia.
>
> üí° **Exemplo Num√©rico:**
>
>  Usando o mesmo exemplo anterior, temos:
>
> $y = \begin{bmatrix} 2 \\ 4 \\ 5 \\ 4 \\ 5 \end{bmatrix}$, $X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \\ 1 & 5 \end{bmatrix}$, $\hat{y} = \begin{bmatrix} 2.2 \\ 2.9 \\ 3.6 \\ 4.3 \\ 5.0 \end{bmatrix}$, e $b = \begin{bmatrix} 1.5 \\ 0.7 \end{bmatrix}$.
>
> Calculamos a m√©dia de $y$:
>
> $\bar{y} = \frac{2+4+5+4+5}{5} = 4$
>
> $T\bar{y}^2 = 5 \cdot 4^2 = 80$
>
> Temos que $y'X(X'X)^{-1}X'y = \sum_{t=1}^{T} (b'x_t)^2 = 61.7$, como calculado anteriormente.
>
> E $y'y = \sum_{t=1}^{T} y_t^2 = 78$, tamb√©m calculado anteriormente.
>
> $$R^2 = \frac{61.7 - 80}{78 - 80} = \frac{-18.3}{-2} = 0.915$$
>
> Assim, $R^2=0.915$, mostrando que 91.5% da variabilidade da vari√°vel dependente em torno de sua m√©dia √© explicada pelo modelo. Este valor √© diferente do $R^2_u$ porque ele leva em conta a m√©dia de $y$.
>
> ```python
> import numpy as np
>
> y = np.array([2, 4, 5, 4, 5])
> X = np.array([[1, 1], [1, 2], [1, 3], [1, 4], [1, 5]])
>
> # Calculating b using the normal equation
> b = np.linalg.solve(X.T @ X, X.T @ y)
>
> # Calculating predicted values y_hat
> y_hat = X @ b
>
> # Calculating the mean of y
> y_mean = np.mean(y)
>
> # Calculating R^2
> R_squared = (np.sum(y_hat**2) - len(y)*y_mean**2) / (np.sum(y**2) - len(y)*y_mean**2)
> print(f"R^2: {R_squared}")
>
> # Alternatively using the matrix formulation
> T = len(y)
> I = np.eye(T)
> ones = np.ones((T, 1))
> M = I - (1/T) * ones @ ones.T
> M_X = I - X @ np.linalg.solve(X.T @ X, X.T)
> R_squared_alt = 1 - (y.T @ M_X @ y) / (y.T @ M @ y)
> print(f"R^2 (alternative): {R_squared_alt}")
> ```
>
> A diferen√ßa entre as duas f√≥rmulas de $R^2$ surge do ajuste da m√©dia. Enquanto $R^2_u$ mede a propor√ß√£o da varia√ß√£o total, $R^2$ mede a propor√ß√£o da varia√ß√£o *em torno da m√©dia*. √â geralmente prefer√≠vel usar o $R^2$ centrado, pois ele √© mais interpret√°vel e est√° mais alinhado com a ideia de um modelo de regress√£o linear.

**Caixa de Destaque:**

> O $R^2$ centrado √© frequentemente preferido ao $R^2$ n√£o centrado, pois ajusta pela m√©dia da vari√°vel dependente e √© mais interpret√°vel em termos de variabilidade explicada pelo modelo. O $R^2$ n√£o centrado pode ser negativo se a regress√£o n√£o incluir um termo constante, enquanto o $R^2$ centrado varia entre 0 e 1, quando a regress√£o inclui um termo constante.

**Lema 1**
Se o modelo de regress√£o inclui um termo constante, o $R^2$ centrado √© sempre n√£o negativo e menor ou igual a 1.

*Prova:*
I. O $R^2$ centrado √© definido como $R^2 = 1 - \frac{y'M_Xy}{y'My}$.
II. Se o modelo inclui um termo constante (um vetor de 1s), ent√£o a matriz $X$ cont√©m um vetor de 1s.
III. Sob esta condi√ß√£o, $y'M_Xy$ representa a soma dos quadrados dos res√≠duos da regress√£o OLS, que √© sempre n√£o negativa, uma vez que √© uma soma de quadrados.
IV.  Similarmente, $y'My$ representa a soma dos quadrados da vari√°vel dependente em torno de sua m√©dia, que tamb√©m √© n√£o negativa.
V.  Al√©m disso, $y'M_Xy$ √© sempre menor ou igual a $y'My$, j√° que a regress√£o OLS minimiza a soma dos quadrados dos res√≠duos.
VI. Portanto, $\frac{y'M_Xy}{y'My}$ √© sempre um n√∫mero n√£o negativo e menor ou igual a 1, implicando que $R^2$ estar√° entre 0 e 1, ou seja, $0 \leq R^2 \leq 1$. ‚ñ†

**Lema 1.1**
Se o modelo de regress√£o inclui um termo constante, ent√£o $y'M_Xy = \sum_{t=1}^T (y_t - \hat{y}_t)^2$, onde $\hat{y}_t$ s√£o os valores preditos pelo modelo OLS.

*Prova:*
I. Sabemos que $M_X = I - X(X'X)^{-1}X'$.
II. Portanto, $y'M_Xy = y'(I - X(X'X)^{-1}X')y = y'y - y'X(X'X)^{-1}X'y$.
III. O termo $y'X(X'X)^{-1}X'y$ √© igual a $\hat{y}'\hat{y}$, onde $\hat{y} = Xb = X(X'X)^{-1}X'y$ √© o vetor de valores preditos.
IV. Assim, $y'M_Xy = y'y - \hat{y}'\hat{y} = \sum_{t=1}^T y_t^2 - \sum_{t=1}^T \hat{y}_t^2$.
V. Como a soma dos quadrados totais (SST) pode ser decomposta como a soma dos quadrados explicada (SSE) e a soma dos quadrados dos res√≠duos (SSR), temos que $y'y - \hat{y}'\hat{y}$ √© equivalente a $\sum_{t=1}^T (y_t - \hat{y}_t)^2$.
VI. Portanto, $y'M_Xy = \sum_{t=1}^T (y_t - \hat{y}_t)^2$. ‚ñ†

**Lema 1.2**
Se o modelo de regress√£o inclui um termo constante, ent√£o $y'My = \sum_{t=1}^T (y_t - \bar{y})^2$, onde $\bar{y}$ √© a m√©dia amostral de $y$.

*Prova:*
I. Sabemos que $M = I - \frac{1}{T}11'$.
II. Portanto, $y'My = y'(I - \frac{1}{T}11')y = y'y - \frac{1}{T}y'11'y$.
III. O termo $1'y = \sum_{t=1}^T y_t$, ent√£o $\frac{1}{T}1'y = \bar{y}$.
IV. Assim, $y'11'y = (\sum_{t=1}^T y_t)^2 = T^2\bar{y}^2$.
V. Substituindo de volta, $y'My = y'y - \frac{1}{T}T^2\bar{y}^2 = y'y - T\bar{y}^2 = \sum_{t=1}^T y_t^2 - T\bar{y}^2$.
VI. A soma dos quadrados total centrada (SSTc) √© $\sum_{t=1}^T (y_t - \bar{y})^2 =  \sum_{t=1}^T y_t^2 - T\bar{y}^2$.
VII. Portanto, $y'My = \sum_{t=1}^T (y_t - \bar{y})^2$. ‚ñ†

**Observa√ß√£o:**
A presen√ßa de um termo constante no modelo de regress√£o √© crucial para garantir que o $R^2$ centrado seja uma medida significativa da propor√ß√£o da variabilidade explicada. Se o modelo n√£o incluir um termo constante, o $R^2$ centrado pode ser negativo, o que dificultaria sua interpreta√ß√£o como uma medida de ajuste.

> üí° **Exemplo Num√©rico:**
>
>  Vamos usar o mesmo exemplo num√©rico do t√≥pico anterior, com os valores de $y = \begin{bmatrix} 5 \\ 8 \\ 12 \end{bmatrix}$ e $X = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 5 \end{bmatrix}$
>
> Usando o estimador OLS $b = \begin{bmatrix} 0.29 \\ 2.21 \end{bmatrix}$, calculado anteriormente e $X$, podemos encontrar o vetor de valores preditos $\hat{y}$
>
> $$\hat{y} = Xb = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 5 \end{bmatrix}\begin{bmatrix} 0.29 \\ 2.21 \end{bmatrix} = \begin{bmatrix} 4.71 \\ 6.92 \\ 11.34 \end{bmatrix}$$
>
> A soma dos quadrados dos valores ajustados √©:
>
> $$\sum_{t=1}^{T} (b'x_t)^2 = 4.71^2 + 6.92^2 + 11.34^2 = 190.11$$
>
> A soma dos quadrados dos valores observados √©:
>
> $$\sum_{t=1}^{T} y_t^2 = 5^2 + 8^2 + 12^2 = 233 $$
>
> O $R^2_u$ √© dado por:
>
> $$R^2_u = \frac{190.11}{233} = 0.816$$
>
> Para calcular o $R^2$ centrado, precisamos da m√©dia de y: $\bar{y} = (5+8+12)/3=8.33$. Ent√£o $T\bar{y}^2 = 3 * 8.33^2 = 208.22$
>
> $$R^2 = \frac{190.11 - 208.22}{233 - 208.22} = \frac{-18.11}{24.78} = -0.73 $$
>
> Note que este resultado n√£o √© um bom exemplo, pois o estimador OLS n√£o √© √≥timo, pois estamos trabalhando com um exemplo ilustrativo com uma √∫nica amostra e com desvios entre $b$ e $\beta$.
>
> Se recomputarmos o $R^2$ com os valores preditos e os valores de $y$ corretos sem os erros u, teremos os valores:
>
> $$ y = \begin{bmatrix} 5 \\ 8 \\ 11 \end{bmatrix}  \quad \hat{y} =  \begin{bmatrix} 5 \\ 8 \\ 11 \end{bmatrix} $$
>
>  Nesse caso, o $R^2_u = \frac{5^2+8^2+11^2}{5^2+8^2+11^2}=1$, ou seja, o modelo explica toda a varia√ß√£o dos dados. A m√©dia $\bar{y} = 8$, ent√£o $T\bar{y}^2= 3 * 8^2 = 192$. O $R^2$ ser√° dado por
>
> $$R^2 = \frac{210-192}{210-192}=1$$
>
> Nesse caso, o R¬≤=1 indica um ajuste perfeito do modelo.
>
>
> ```python
> import numpy as np
>
> y = np.array([5, 8, 12])
> X = np.array([[1, 2], [1, 3], [1, 5]])
>
> # Calculating b using the normal equation
> b = np.linalg.solve(X.T @ X, X.T @ y)
>
> # Calculating predicted values y_hat
> y_hat = X @ b
>
> # Calculating R_u^2
> R_u_squared = np.sum(y_hat**2) / np.sum(y**2)
> print(f"R^2_u: {R_u_squared}")
>
> # Calculating the mean of y
> y_mean = np.mean(y)
>
> # Calculating R^2
> R_squared = (np.sum(y_hat**2) - len(y)*y_mean**2) / (np.sum(y**2) - len(y)*y_mean**2)
> print(f"R^2: {R_squared}")
>
> # Corrected calculation when the predicted value is correct
> y_correct = np.array([5, 8, 11])
> y_hat_correct = np.array([5, 8, 11])
>
> # Calculating R_u^2 for the perfect fit
> R_u_squared_correct = np.sum(y_hat_correct**2) / np.sum(y_correct**2)
> print(f"R^2_u (correct): {R_u_squared_correct}")
>
> # Calculating the mean of y for the perfect fit
> y_mean_correct = np.mean(y_correct)
>
> # Calculating R^2 for the perfect fit
> R_squared_correct = (np.sum(y_hat_correct**2) - len(y_correct)*y_mean_correct**2) / (np.sum(y_correct**2) - len(y_correct)*y_mean_correct**2)
> print(f"R^2 (correct): {R_squared_correct}")
>
> ```

**Teorema 1**
O $R^2$ centrado tamb√©m pode ser expresso como a raz√£o entre a soma dos quadrados explicada (SSE) e a soma dos quadrados total centrada (SSTc), onde $SSE = \sum_{t=1}^T (\hat{y}_t - \bar{y})^2$ e $SSTc = \sum_{t=1}^T (y_t - \bar{y})^2$.

*Prova:*
I.  Pelo lema 1.1, sabemos que $y'M_Xy = \sum_{t=1}^T (y_t - \hat{y}_t)^2$, que √© a soma dos quadrados dos res√≠duos (SSR).
II. Pelo lema 1.2, sabemos que $y'My = \sum_{t=1}^T (y_t - \bar{y})^2$, que √© a soma dos quadrados total centrada (SSTc).
III. O $R^2$ centrado √© definido como $R^2 = 1 - \frac{y'M_Xy}{y'My} = 1 - \frac{SSR}{SSTc}$.
IV. A soma dos quadrados total (SST) pode ser decomposta em soma dos quadrados explicada (SSE) e soma dos quadrados dos res√≠duos (SSR), onde $SST = SSE + SSR$.
V. Quando consideramos a soma dos quadrados total centrada, temos $SSTc = SSE + SSR$.
VI. Assim, $SSR = SSTc - SSE$, e $R^2 = 1 - \frac{SSTc - SSE}{SSTc} = \frac{SSTc - SSTc + SSE}{SSTc} = \frac{SSE}{SSTc}$.
VII. Portanto, $R^2 = \frac{SSE}{SSTc} = \frac{\sum_{t=1}^T (\hat{y}_t - \bar{y})^2}{\sum_{t=1}^T (y_t - \bar{y})^2}$. ‚ñ†

### Conclus√£o
O $R^2$ √© uma ferramenta indispens√°vel para avaliar o desempenho de um modelo de regress√£o OLS, oferecendo uma medida da propor√ß√£o da variabilidade da vari√°vel dependente que √© explicada pelo modelo. O $R^2$ n√£o centrado √© uma medida direta da vari√¢ncia explicada em rela√ß√£o √† vari√¢ncia total, enquanto o $R^2$ centrado ajusta pela m√©dia da vari√°vel dependente, proporcionando uma avalia√ß√£o mais precisa do ajuste do modelo. Apesar de sua import√¢ncia, o $R^2$ n√£o deve ser usado como a √∫nica m√©trica para avaliar um modelo, pois ele n√£o informa sobre a causalidade ou a validade do modelo fora da amostra. √â crucial complementar a an√°lise do $R^2$ com outras medidas e testes estat√≠sticos para uma avalia√ß√£o completa do modelo.

### Refer√™ncias
[^8.1.13]:  O $R^2$ n√£o centrado √© definido como a raz√£o entre a soma dos quadrados dos valores ajustados e a soma dos quadrados dos valores observados,  $R_u^2 = \frac{b'X'Xb}{y'y}$.
[^8.1.14]:  O $R^2$ centrado √© definido como a raz√£o entre a vari√¢ncia explicada ajustada pela m√©dia e a vari√¢ncia total ajustada pela m√©dia,  $R^2 = \frac{y'X(X'X)^{-1}X'y - Ty^2}{y'y - Ty^2}$.
[^Previous Topic 1]: A diferen√ßa entre o estimador OLS e o par√¢metro populacional √© expressa como $b - \beta = (X'X)^{-1}X'u$.
<!-- END -->
