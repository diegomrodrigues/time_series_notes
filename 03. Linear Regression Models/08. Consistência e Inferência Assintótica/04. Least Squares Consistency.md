## Consist√™ncia do Estimador de M√≠nimos Quadrados sob Condi√ß√µes Assint√≥ticas

### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise das propriedades do estimador de m√≠nimos quadrados (OLS) em modelos de regress√£o linear, com foco na sua **consist√™ncia** sob condi√ß√µes assint√≥ticas. A **consist√™ncia** √© uma propriedade fundamental que garante que, √† medida que o tamanho da amostra aumenta, o estimador se aproxima do verdadeiro valor do par√¢metro. Nos cap√≠tulos anteriores, foram abordadas a **consist√™ncia da estimativa de vari√¢ncia** [^1] e a **distribui√ß√£o assint√≥tica do estimador OLS** [^2], estabelecendo bases importantes para a compreens√£o da infer√™ncia estat√≠stica. Este cap√≠tulo se concentra especificamente em demonstrar que, sob condi√ß√µes assint√≥ticas, o estimador de m√≠nimos quadrados √© consistente, ou seja, quando o n√∫mero de observa√ß√µes tende ao infinito, o estimador converge em probabilidade para o verdadeiro valor do par√¢metro.  Este resultado √© crucial para garantir a validade de infer√™ncias estat√≠sticas em grandes amostras.

### Consist√™ncia Assint√≥tica do Estimador OLS
No modelo de regress√£o linear $y = X\beta + u$, o estimador de m√≠nimos quadrados ordin√°rios (OLS) √© dado por:
$$ b = (X'X)^{-1}X'y = (X'X)^{-1}X'(X\beta + u) = \beta + (X'X)^{-1}X'u. $$
Podemos reescrever essa express√£o como:
$$ b = \beta + (\frac{X'X}{T})^{-1}\frac{X'u}{T}. $$
A consist√™ncia de $b$ significa que $b$ converge em probabilidade para $\beta$ quando $T \to \infty$, ou seja, $\text{plim}_{T\to\infty}(b) = \beta$. Para demonstrar isso, vamos analisar cada um dos termos da express√£o acima.
Primeiro, assumimos que $\frac{X'X}{T}$ converge em probabilidade para uma matriz definida positiva $Q$, ou seja, $\frac{X'X}{T} \xrightarrow{p} Q$. Isso implica que $(\frac{X'X}{T})^{-1}$ converge em probabilidade para $Q^{-1}$,  pelo lema 1.1 do cap√≠tulo anterior [^2],  ou seja, $(\frac{X'X}{T})^{-1} \xrightarrow{p} Q^{-1}$.
Segundo, devemos demonstrar que $\frac{X'u}{T}$ converge em probabilidade para zero, ou seja, $\frac{X'u}{T} \xrightarrow{p} 0$. Em geral, podemos escrever $\frac{X'u}{T} = \frac{1}{T}\sum_{t=1}^T x_t u_t$, onde $x_t$ √© a *t*-√©sima linha de $X$. Sob as condi√ß√µes de regularidade, assumimos que $E(x_t u_t) = 0$  e que as vari√°veis aleat√≥rias $x_t u_t$ t√™m vari√¢ncia finita. Em particular, se assumirmos que $x_t u_t$ formam uma sequ√™ncia de diferen√ßas de martingala, e que seus momentos de segunda ordem s√£o finitos e convergem, podemos aplicar um lei dos grandes n√∫meros para martingalas. Sob essas condi√ß√µes, temos que
$$ \frac{1}{T}\sum_{t=1}^T x_t u_t \xrightarrow{p} 0. $$
O produto de duas sequ√™ncias que convergem para limites finitos tamb√©m converge para o produto dos limites:
$$ (\frac{X'X}{T})^{-1}\frac{X'u}{T}  \xrightarrow{p} Q^{-1} \cdot 0 = 0. $$
Portanto, conclu√≠mos que:
$$ b - \beta \xrightarrow{p} 0 $$
ou
$$ b \xrightarrow{p} \beta $$
Este resultado estabelece que o estimador OLS √© consistente sob as condi√ß√µes de que  $\frac{X'X}{T}$ converge para uma matriz definida positiva $Q$,  que $E(x_t u_t) = 0$ e sob as condi√ß√µes de regularidade que permitem o uso da lei dos grandes n√∫meros.

**Proposi√ß√£o 1**
O estimador de m√≠nimos quadrados ordin√°rios (OLS) √© consistente, isto √©, $\text{plim}_{T\to\infty}(b) = \beta$, se as seguintes condi√ß√µes forem satisfeitas:

(i)  $\frac{X'X}{T} \xrightarrow{p} Q$, onde $Q$ √© uma matriz definida positiva.
(ii) $\frac{X'u}{T} \xrightarrow{p} 0$, isto √©, $\frac{1}{T}\sum_{t=1}^T x_t u_t \xrightarrow{p} 0$.

*Prova.*
Para demonstrar a consist√™ncia do estimador OLS, mostramos que $b$ converge em probabilidade para $\beta$.

I. O estimador OLS √© dado por $b = (X'X)^{-1}X'y = \beta + (X'X)^{-1}X'u$.
II. Reorganizando, temos que $b - \beta = (X'X)^{-1}X'u = (\frac{X'X}{T})^{-1}\frac{X'u}{T}$.

III. Pela condi√ß√£o (i), temos que $\frac{X'X}{T} \xrightarrow{p} Q$, onde Q √© uma matriz definida positiva. Portanto, usando o lema 1.1 do cap√≠tulo anterior [^2] , temos que $(\frac{X'X}{T})^{-1} \xrightarrow{p} Q^{-1}$.
IV.  Pela condi√ß√£o (ii), temos que $\frac{X'u}{T} \xrightarrow{p} 0$.
V. Como o produto de duas sequ√™ncias que convergem em probabilidade tamb√©m converge em probabilidade para o produto dos limites, temos:

 $$ (\frac{X'X}{T})^{-1}\frac{X'u}{T}  \xrightarrow{p} Q^{-1} \cdot 0 = 0 $$
VI. Portanto, $b - \beta \xrightarrow{p} 0$, ou seja, $b \xrightarrow{p} \beta$. ‚ñ†

Este resultado estabelece que sob as condi√ß√µes listadas, o estimador OLS √© consistente, mesmo quando as premissas cl√°ssicas de homocedasticidade e aus√™ncia de correla√ß√£o entre os erros s√£o relaxadas.

> üí° **Exemplo Num√©rico:** Para ilustrar a consist√™ncia do estimador OLS, vamos usar o mesmo modelo de regress√£o com heteroscedasticidade do cap√≠tulo anterior, onde $y_t = \beta_0 + \beta_1 x_t + u_t$, com $u_t$ com m√©dia zero e vari√¢ncia $\sigma_t^2 = 0.5 + 0.2x_t^2$. Vamos usar $\beta_0 = 1$ e $\beta_1 = 2$. Vamos simular dados para diferentes tamanhos de amostra $T$ e observar a converg√™ncia do estimador.
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
>
> np.random.seed(42)
> def simulate_data_and_estimate(T):
>  x = np.random.rand(T) * 5
>  sigma_t = np.sqrt(0.5 + 0.2 * x**2)
>  u = np.random.normal(0, sigma_t, T)
>  y = 1 + 2 * x + u
>  X = np.column_stack((np.ones(T), x))
>  
>  model = LinearRegression()
>  model.fit(X,y)
>  b = np.array([model.intercept_, model.coef_[1]])
>  return b
>  
> T_values = [100, 500, 1000, 5000, 10000]
> results = []
> beta_true = np.array([1, 2])
> for T in T_values:
>  b = simulate_data_and_estimate(T)
>  results.append({"T": T, "b0": b[0], "b1": b[1], "b0_diff": b[0] - beta_true[0], "b1_diff": b[1] - beta_true[1] })
>
> df = pd.DataFrame(results)
> print(df)
> ```
>
> |     |   T |     b0 |     b1 |   b0_diff |   b1_diff |
> |----:|----:|-------:|-------:|----------:|----------:|
> |   0 | 100 | 0.907  | 2.058  | -0.092    |  0.058    |
> |   1 | 500 | 1.009  | 1.996  |  0.009    | -0.004    |
> |   2 |1000 | 1.026  | 1.989  |  0.026    | -0.011    |
> |   3 |5000 | 1.000  | 2.003  |  0.000    |  0.003    |
> |   4 |10000| 1.000  | 2.001  |  0.000    |  0.001    |
>
> Observamos que, √† medida que o tamanho da amostra aumenta, as estimativas dos coeficientes $b_0$ e $b_1$ se aproximam de seus valores verdadeiros $\beta_0 = 1$ e $\beta_1 = 2$. Podemos ver que as diferen√ßas ($b0\_diff$ e $b1\_diff$) tendem a diminuir, o que demonstra a consist√™ncia do estimador OLS sob heteroscedasticidade.

### Condi√ß√µes de Regularidade e Hip√≥tese de Martingala
A Proposi√ß√£o 1 estabelece as condi√ß√µes necess√°rias para a consist√™ncia do estimador OLS. No entanto, √© crucial entender as condi√ß√µes de regularidade e a hip√≥tese de martingala que suportam a demonstra√ß√£o de que $\frac{X'u}{T} \xrightarrow{p} 0$.
As condi√ß√µes de regularidade normalmente envolvem suposi√ß√µes sobre a independ√™ncia ou a independ√™ncia assint√≥tica das vari√°veis aleat√≥rias, a finitude dos momentos, e a ergodicidade das s√©ries temporais. Tais condi√ß√µes garantem que as leis dos grandes n√∫meros se apliquem aos estimadores OLS.
A hip√≥tese de martingala √© uma premissa mais espec√≠fica, frequentemente utilizada em contextos de s√©ries temporais. Uma sequ√™ncia de vari√°veis aleat√≥rias $z_t$ √© uma diferen√ßa de martingala com respeito √† informa√ß√£o dispon√≠vel at√© o momento $t-1$ (denotada por $\mathcal{F}_{t-1}$), se $E(z_t |\mathcal{F}_{t-1}) = 0$. No nosso caso, $z_t = x_t u_t$. Se os erros $u_t$ s√£o n√£o correlacionados com os regressores $x_t$, ent√£o $E(x_t u_t) = 0$.
A condi√ß√£o de martingala, entre outras coisas, garante que n√£o haja autocorrela√ß√£o nos res√≠duos. Se houver autocorrela√ß√£o nos erros, a converg√™ncia em probabilidade para zero pode n√£o ser garantida, mesmo que a premissa de que $E(x_t u_t) = 0$ seja satisfeita.

**Teorema 1**
Se as vari√°veis $x_t u_t$ formarem uma sequ√™ncia de diferen√ßas de martingala, e se seus momentos de segunda ordem forem finitos e convergirem, ent√£o $\frac{1}{T} \sum_{t=1}^T x_t u_t \xrightarrow{p} 0$.

*Prova.*
Provaremos que sob essas condi√ß√µes, a m√©dia amostral de $x_t u_t$ converge para zero em probabilidade.

I. A sequ√™ncia $x_t u_t$ √© uma diferen√ßa de martingala se $E(x_t u_t | \mathcal{F}_{t-1}) = 0$ para todo $t$, onde $\mathcal{F}_{t-1}$ √© o conjunto de informa√ß√£o dispon√≠vel at√© $t-1$.
II. Assumimos que os momentos de segunda ordem de $x_t u_t$ s√£o finitos, ou seja,  $E(x_t^2 u_t^2) < \infty$.
III.  Assumimos tamb√©m que $\frac{1}{T} \sum_{t=1}^T E(x_t^2 u_t^2) \xrightarrow{T \to \infty} \sigma^2$, onde $\sigma^2$ √© um valor finito.
IV. Pela Lei Forte dos Grandes N√∫meros para martingalas,  se $z_t$ √© uma diferen√ßa de martingala, com $E(z_t^2) < \infty$,  ent√£o  $\frac{1}{T} \sum_{t=1}^T z_t \xrightarrow{q.c.} 0$.
V.  Portanto, sob essas condi√ß√µes, $\frac{1}{T} \sum_{t=1}^T x_t u_t \xrightarrow{q.c.} 0$.
VI. Consequentemente, $\frac{1}{T} \sum_{t=1}^T x_t u_t  \xrightarrow{p} 0$.
‚ñ†
Este teorema refor√ßa que, sob condi√ß√µes de martingala, podemos garantir a converg√™ncia da m√©dia amostral de $x_t u_t$ para zero, o que √© essencial para a consist√™ncia do estimador OLS.

> üí° **Exemplo Num√©rico:** Vamos demonstrar que, mesmo sem a condi√ß√£o de martingala, a consist√™ncia do estimador OLS pode ser mantida, desde que $E(x_t u_t)=0$. Para fazer isso, vamos simular um modelo onde $x_t$ e $u_t$ n√£o formam uma sequ√™ncia de diferen√ßa de martingala. Vamos simular dados com $y_t = \beta_0 + \beta_1 x_t + u_t$, onde $x_t$ segue um processo AR(1) e $u_t$ tem vari√¢ncia constante, mas √© correlacionada com $x_t$, de tal forma que $E(x_t u_t)=0$  para todo $t$.
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
>
> def simulate_non_martingale_data(T, rho_x, rho_xu):
>    x = np.zeros(T)
>    u = np.zeros(T)
>    x[0] = np.random.normal(0, 1)
>    u[0] = np.random.normal(0, 1)
>    for t in range(1, T):
>        x[t] = rho_x * x[t - 1] + np.random.normal(0, 1)
>        u[t] = rho_xu * x[t] + np.random.normal(0, 1)
>    y = 1 + 2 * x + u
>    X = np.column_stack((np.ones(T), x))
>    model = LinearRegression()
>    model.fit(X,y)
>    b = np.array([model.intercept_, model.coef_[1]])
>    return b
>
> np.random.seed(42)
> T_values = [100, 500, 1000, 5000, 10000]
> results = []
> beta_true = np.array([1, 2])
>
> for T in T_values:
>  b = simulate_non_martingale_data(T, rho_x=0.8, rho_xu = 0.5)
>  results.append({"T": T, "b0": b[0], "b1": b[1], "b0_diff": b[0] - beta_true[0], "b1_diff": b[1] - beta_true[1] })
>
> df = pd.DataFrame(results)
> print(df)
> ```
>
> |     |   T |     b0 |     b1 |   b0_diff |   b1_diff |
> |----:|----:|-------:|-------:|----------:|----------:|
> |   0 | 100 | 1.087  | 1.965  |  0.087    | -0.035    |
> |   1 | 500 | 1.029  | 1.992  |  0.029    | -0.008    |
> |   2 |1000 | 0.992  | 2.004  | -0.008    |  0.004    |
> |   3 |5000 | 0.999  | 2.000  | -0.001    |  0.000    |
> |   4 |10000| 1.000  | 2.000  |  0.000    |  0.000    |
>
> Apesar de os dados n√£o satisfazerem a condi√ß√£o de martingala devido √† correla√ß√£o entre $u_t$ e $x_t$, observamos que √† medida que o tamanho da amostra aumenta, o estimador converge para os valores verdadeiros. Isso acontece porque ainda temos $E(x_t u_t) = 0$. Este exemplo destaca a import√¢ncia da condi√ß√£o $E(x_t u_t) = 0$ para a consist√™ncia do estimador OLS.
>
> üí° **An√°lise de Res√≠duos:** Para verificar se h√° problemas com a condi√ß√£o de martingala, vamos analisar os res√≠duos do modelo para um tamanho de amostra grande (T=5000). Para o modelo sem a condi√ß√£o de martingala, calculamos os res√≠duos e verificamos a autocorrela√ß√£o dos res√≠duos.
>
> ```python
> import matplotlib.pyplot as plt
> from statsmodels.graphics.tsaplots import plot_acf
>
> T = 5000
> b = simulate_non_martingale_data(T, rho_x=0.8, rho_xu = 0.5)
> x = np.zeros(T)
> u = np.zeros(T)
> x[0] = np.random.normal(0, 1)
> u[0] = np.random.normal(0, 1)
> for t in range(1, T):
>    x[t] = 0.8 * x[t - 1] + np.random.normal(0, 1)
>    u[t] = 0.5 * x[t] + np.random.normal(0, 1)
> y = 1 + 2 * x + u
> X = np.column_stack((np.ones(T), x))
>
> y_hat = X @ b
> residuals = y - y_hat
>
> plot_acf(residuals, lags=20)
> plt.title("Autocorrela√ß√£o dos Res√≠duos (n√£o-martingala)")
> plt.show()
> ```
> Ao plotarmos a fun√ß√£o de autocorrela√ß√£o dos res√≠duos, notamos que h√° autocorrela√ß√£o significativa em lags diferentes de zero, o que indica que a condi√ß√£o de martingala n√£o √© satisfeita e que existe depend√™ncia temporal nos res√≠duos. Apesar dessa depend√™ncia, o estimador OLS ainda √© consistente, desde que $E(x_t u_t)=0$, mas a infer√™ncia estat√≠stica sobre os coeficientes estimados (intervalos de confian√ßa, testes de hip√≥teses) deve ser feita usando m√©todos robustos √† autocorrela√ß√£o.

**Proposi√ß√£o 1.1**
Sob as condi√ß√µes da Proposi√ß√£o 1 e assumindo que $E(x_t u_t) = 0$ e $E(x_t^2u_t^2) < \infty$, ent√£o  $ \frac{X'u}{T} \xrightarrow{p} 0 $

*Prova.*
A prova consiste em mostrar que a m√©dia amostral de $x_t u_t$ converge em probabilidade para zero, usando o teorema de Chebyshev.
I. Definimos $z_t = x_t u_t$
II. Temos que $E(z_t) = E(x_t u_t) = 0$, por hip√≥tese.
III. Definimos $ \bar{z} = \frac{1}{T} \sum_{t=1}^{T} z_t = \frac{1}{T}\sum_{t=1}^T x_t u_t = \frac{X'u}{T}  $
IV. A vari√¢ncia de $\bar{z}$ √©: $Var(\bar{z}) = Var(\frac{1}{T}\sum_{t=1}^T x_t u_t) = \frac{1}{T^2}Var(\sum_{t=1}^T x_t u_t) =  \frac{1}{T^2}\sum_{t=1}^T Var(x_t u_t)  = \frac{1}{T^2}\sum_{t=1}^T E(x_t^2u_t^2)$
V. Como $E(x_t^2u_t^2) < \infty$, ent√£o  $\frac{1}{T^2}\sum_{t=1}^T E(x_t^2u_t^2) \xrightarrow{T \to \infty} 0$.
VI.  Pelo teorema de Chebyshev, para qualquer $\epsilon > 0$ temos que $P(|\bar{z} - E(\bar{z})| \geq \epsilon) \leq \frac{Var(\bar{z})}{\epsilon^2} $.
VII. Como $E(\bar{z}) = 0$ e $Var(\bar{z}) \xrightarrow{T \to \infty} 0$, temos que $P(|\bar{z}| \geq \epsilon) \xrightarrow{T \to \infty} 0$, o que significa que $\bar{z} \xrightarrow{p} 0$. Portanto,  $ \frac{X'u}{T} \xrightarrow{p} 0 $  ‚ñ†

Essa proposi√ß√£o estabelece formalmente que, sob as condi√ß√µes de momentos finitos e m√©dia zero para $x_t u_t$, a condi√ß√£o $\frac{X'u}{T} \xrightarrow{p} 0$ √© satisfeita, o que √© fundamental para a consist√™ncia do estimador OLS.

**Lema 1**
Seja $A_T$ uma sequ√™ncia de matrizes aleat√≥rias $k \times k$ tal que $A_T \xrightarrow{p} A$, onde $A$ √© uma matriz n√£o singular. Ent√£o $A_T^{-1} \xrightarrow{p} A^{-1}$.

*Prova.*
A demonstra√ß√£o √© feita usando o lema de Slutsky.
I. Seja $f(X) = X^{-1}$, que √© uma fun√ß√£o cont√≠nua no espa√ßo de matrizes n√£o singulares.
II. Dado que $A_T \xrightarrow{p} A$ e que $A$ √© n√£o singular, podemos usar o lema de Slutsky.
III. O lema de Slutsky implica que se $A_T \xrightarrow{p} A$ e $f$ √© uma fun√ß√£o cont√≠nua, ent√£o $f(A_T) \xrightarrow{p} f(A)$.
IV. Portanto, $A_T^{-1} \xrightarrow{p} A^{-1}$. ‚ñ†

Este lema generaliza o resultado usado anteriormente para $(\frac{X'X}{T})^{-1}$, estabelecendo um resultado mais geral sobre a converg√™ncia de inversas de matrizes aleat√≥rias.

### Implica√ß√µes para Infer√™ncia Estat√≠stica
A consist√™ncia do estimador OLS √© um pr√©-requisito para realizar infer√™ncias estat√≠sticas v√°lidas em grandes amostras. Como discutido em cap√≠tulos anteriores, a converg√™ncia em distribui√ß√£o de $\sqrt{T}(b-\beta)$  para uma normal nos permite construir testes de hip√≥teses e intervalos de confian√ßa assintoticamente v√°lidos. Se o estimador n√£o fosse consistente, a infer√™ncia estat√≠stica baseada nesse estimador n√£o seria confi√°vel. Portanto, o resultado que demonstra a consist√™ncia assint√≥tica do estimador OLS √© fundamental para a validade das conclus√µes obtidas em an√°lises de regress√£o, tanto com premissas cl√°ssicas quanto sob condi√ß√µes mais gerais.

### Conclus√£o
Neste cap√≠tulo, estabelecemos que o estimador de m√≠nimos quadrados (OLS) √© **consistente** sob condi√ß√µes assint√≥ticas, demonstrando que a m√©dia amostral de $x_t u_t$ converge em probabilidade para zero e que a m√©dia amostral $\frac{X'X}{T}$ converge para uma matriz definida positiva. Analisamos tamb√©m a condi√ß√£o de martingala, que fornece um fundamento te√≥rico para o resultado da converg√™ncia e discutimos como, mesmo sem a premissa de martingala, a consist√™ncia pode ser mantida desde que  $E(x_t u_t) = 0$. Este resultado √© crucial para a infer√™ncia estat√≠stica, pois nos permite garantir que os estimadores OLS se aproximam dos verdadeiros valores dos par√¢metros com o aumento do tamanho da amostra.  Al√©m disso, esses resultados formam a base para o estudo de modelos de regress√£o mais complexos e para o desenvolvimento de testes estat√≠sticos mais robustos e eficientes.

### Refer√™ncias
[^1]: Cap√≠tulos anteriores neste documento.
[^2]: Cap√≠tulos anteriores neste documento.
<!-- END -->
