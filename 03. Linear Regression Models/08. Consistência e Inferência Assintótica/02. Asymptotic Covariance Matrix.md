## Matriz de Covari√¢ncia Assint√≥tica e Infer√™ncia em Grandes Amostras

### Introdu√ß√£o
Em continuidade ao estudo da **consist√™ncia** e **distribui√ß√£o assint√≥tica** da **estimativa da vari√¢ncia** em modelos de regress√£o linear, este cap√≠tulo explora a **matriz de covari√¢ncia assint√≥tica** do estimador de par√¢metros. O conceito de **matriz de covari√¢ncia assint√≥tica** √© fundamental para a realiza√ß√£o de **infer√™ncia estat√≠stica** em amostras grandes, permitindo construir **testes de hip√≥teses** e **intervalos de confian√ßa** para os par√¢metros do modelo. Como j√° vimos, sob condi√ß√µes mais gerais do que as cl√°ssicas, a matriz de covari√¢ncia dos estimadores n√£o √© mais dada por  $\sigma^2(X'X)^{-1}$. Nesta se√ß√£o, vamos derivar a **matriz de covari√¢ncia assint√≥tica** e discutir como ela √© usada para realizar **infer√™ncias estat√≠sticas** v√°lidas, especialmente em cen√°rios com **heteroscedasticidade** ou **autocorrela√ß√£o**.

### Matriz de Covari√¢ncia Assint√≥tica
A **matriz de covari√¢ncia assint√≥tica** fornece uma aproxima√ß√£o da vari√¢ncia dos estimadores quando o tamanho da amostra tende ao infinito. Em outras palavras, ela descreve como os estimadores dos par√¢metros ($\beta$) se distribuem em torno de seus verdadeiros valores ($\beta_0$) para amostras grandes. A ideia central √© que, com um n√∫mero suficientemente grande de observa√ß√µes, a distribui√ß√£o do estimador de par√¢metros pode ser bem aproximada por uma distribui√ß√£o normal, e a vari√¢ncia dessa distribui√ß√£o √© dada pela **matriz de covari√¢ncia assint√≥tica**.

Como vimos anteriormente, no modelo de regress√£o linear, o estimador de m√≠nimos quadrados ordin√°rios (OLS) √© dado por
$$ b = (X'X)^{-1}X'y = (X'X)^{-1}X'(X\beta + u) = \beta + (X'X)^{-1}X'u. $$
Sob as premissas cl√°ssicas, a vari√¢ncia de $b$ √© dada por $Var(b) = \sigma^2(X'X)^{-1}$. No entanto, quando essas premissas s√£o relaxadas, a vari√¢ncia de $b$ pode ser diferente. Em geral, o estimador $b$ pode ser escrito como
$$ b = \beta + (\frac{X'X}{T})^{-1} \frac{X'u}{T}. $$
Considerando que a matriz $ \frac{X'X}{T} \xrightarrow{p} Q$ e que a distribui√ß√£o assint√≥tica de $\sqrt{T}(\frac{X'u}{T})$ √© normal com m√©dia zero e matriz de covari√¢ncia $\Omega$, a **matriz de covari√¢ncia assint√≥tica** de $\sqrt{T}(b-\beta)$ pode ser calculada como
$$ \text{Avar}(\sqrt{T}(b - \beta)) = \text{Avar} \left( \sqrt{T} (\frac{X'X}{T})^{-1} \frac{X'u}{T} \right) = Q^{-1}\Omega Q^{-1}. $$
Essa matriz de covari√¢ncia assint√≥tica √© uma generaliza√ß√£o da vari√¢ncia de b sob as premissas cl√°ssicas. Quando os erros s√£o homoced√°sticos e n√£o correlacionados, ou seja, $E(uu') = \sigma^2I_T$, temos que  $\Omega = \sigma^2 Q$  e  a **matriz de covari√¢ncia assint√≥tica** simplifica para
$$ Q^{-1}(\sigma^2 Q) Q^{-1} = \sigma^2 Q^{-1} . $$
Nesse caso, a matriz de covari√¢ncia assint√≥tica √© igual √† matriz de covari√¢ncia usual do estimador $b$ quando multiplicada por $T$. No entanto, quando h√° **heteroscedasticidade** ou **autocorrela√ß√£o**,  $\Omega \neq \sigma^2 Q$, e a matriz de covari√¢ncia assint√≥tica √© expressa na forma geral  $Q^{-1}\Omega Q^{-1}$.

> üí° **Exemplo Num√©rico:** Vamos considerar um modelo de regress√£o simples com erros heterosced√°sticos, onde a vari√¢ncia do erro depende de uma vari√°vel explicativa. Suponha que o verdadeiro modelo √© $y_t = 1 + 2x_t + u_t$, onde $u_t$ tem m√©dia zero e vari√¢ncia $\sigma_t^2 = 0.5 + 0.2x_t^2$. Vamos gerar um conjunto de dados com $T=1000$ observa√ß√µes e calcular a matriz de covari√¢ncia usual e a matriz de covari√¢ncia assint√≥tica.
```python
import numpy as np

# Gerar dados
np.random.seed(42)
T = 1000
x = np.random.rand(T) * 5  # Valores de x entre 0 e 5
sigma_t = np.sqrt(0.5 + 0.2 * x**2)
u = np.random.normal(0, sigma_t, T)
y = 1 + 2 * x + u
X = np.column_stack((np.ones(T), x)) # Matriz X com intercepto
k=2


# Estima√ß√£o por OLS
b = np.linalg.inv(X.T @ X) @ X.T @ y
y_hat = X @ b
residuals = y - y_hat

# Matriz de covari√¢ncia usual
sigma2_hat = np.sum(residuals**2) / (T-k) # Estimativa da vari√¢ncia do erro
var_b_usual = sigma2_hat * np.linalg.inv(X.T @ X)
print("Matriz de covari√¢ncia usual:\n", var_b_usual)

# Matriz de covari√¢ncia assint√≥tica (White)
Q_hat = (X.T @ X) / T
Omega_hat = np.zeros((k, k))
for t in range(T):
    Omega_hat += residuals[t]**2 * X[t,:].reshape(-1, 1) @ X[t,:].reshape(1, -1)
Omega_hat = Omega_hat / T
avar_b_het = np.linalg.inv(Q_hat) @ Omega_hat @ np.linalg.inv(Q_hat)

print("\nMatriz de covari√¢ncia assint√≥tica (White):\n", avar_b_het)
```
Observe que a matriz de covari√¢ncia assint√≥tica (White) difere da matriz de covari√¢ncia usual, o que √© esperado dado que temos heteroscedasticidade. A matriz assint√≥tica √© uma estimativa mais adequada da verdadeira vari√¢ncia do estimador OLS nesse caso.

**Lema 1**
Seja $A_T$ uma sequ√™ncia de matrizes aleat√≥rias tais que $A_T \xrightarrow{p} A$ onde A √© uma matriz n√£o aleat√≥ria e invert√≠vel. Ent√£o, $A_T^{-1} \xrightarrow{p} A^{-1}$.

*Proof.* A demonstra√ß√£o segue do fato de que a fun√ß√£o que associa uma matriz √† sua inversa √© cont√≠nua em um dom√≠nio de matrizes invert√≠veis e do teorema de mapeamento cont√≠nuo.

**Lema 2**
Seja $z_T$ uma sequ√™ncia de vetores aleat√≥rios tais que $\sqrt{T}z_T \xrightarrow{d} N(0, \Omega)$. Considere tamb√©m uma sequ√™ncia de matrizes aleat√≥rias $A_T$ tais que $A_T \xrightarrow{p} A$, onde A √© uma matriz n√£o aleat√≥ria. Ent√£o $\sqrt{T}A_T z_T \xrightarrow{d} N(0, A \Omega A')$.

*Proof.*
I.  Pelo teorema de Slutsky, se $A_T \xrightarrow{p} A$ e $\sqrt{T}z_T \xrightarrow{d} Z$, ent√£o $\sqrt{T}A_T z_T$ tem a mesma distribui√ß√£o assint√≥tica que $\sqrt{T}A z_T$.
II. Dado que $\sqrt{T}z_T \xrightarrow{d} N(0, \Omega)$, definimos $Z \sim N(0, \Omega)$.
III. Consequentemente, $\sqrt{T}Az_T$ converge em distribui√ß√£o para $AZ$.
IV. Se $Z \sim N(0, \Omega)$, ent√£o $AZ \sim N(0, A\Omega A')$.
V. Portanto, $\sqrt{T}A_T z_T \xrightarrow{d} N(0, A \Omega A')$.
‚ñ†

**Proposi√ß√£o 1**
A matriz de covari√¢ncia assint√≥tica do estimador OLS, dada por $Avar(\sqrt{T}(b - \beta)) = Q^{-1}\Omega Q^{-1}$, √© consistente.

*Proof.*  Para provar que essa matriz √© consistente,  devemos mostrar que os estimadores $\hat{Q}$ e $\hat{\Omega}$ convergem em probabilidade para $Q$ e $\Omega$, respectivamente.  Como $\frac{X'X}{T} \xrightarrow{p} Q$, temos que $\hat{Q} = \frac{X'X}{T}$ √© um estimador consistente de $Q$. Para $\hat{\Omega}$, como $\Omega = E[x_t x_t' u_t^2]$, podemos usar o estimador de White: $\hat{\Omega} = \frac{1}{T} \sum_{t=1}^T x_t x_t' \hat{u_t}^2$. Sob as condi√ß√µes de consist√™ncia do estimador OLS, $\hat{u}_t$ converge para $u_t$ em probabilidade, e portanto $\hat{\Omega} \xrightarrow{p} \Omega$. Usando o lema 1 e o lema 2, e o fato que a multiplica√ß√£o de matrizes e invers√£o s√£o opera√ß√µes cont√≠nuas, conclu√≠mos que $Q^{-1}\Omega Q^{-1}$ √© um estimador consistente de $\text{Avar}(\sqrt{T}(b - \beta))$.

> üí° **Exemplo Num√©rico:** Para demonstrar a consist√™ncia da matriz de covari√¢ncia assint√≥tica, vamos gerar dados de um modelo com heteroscedasticidade como no exemplo anterior, mas desta vez vamos aumentar o tamanho da amostra e comparar as estimativas da vari√¢ncia dos coeficientes com a matriz de covari√¢ncia assint√≥tica. Vamos variar o tamanho da amostra $T$ de 500 para 5000.
```python
import numpy as np
import pandas as pd

def simulate_data_and_estimate(T):
    x = np.random.rand(T) * 5
    sigma_t = np.sqrt(0.5 + 0.2 * x**2)
    u = np.random.normal(0, sigma_t, T)
    y = 1 + 2 * x + u
    X = np.column_stack((np.ones(T), x))
    k=2
    
    b = np.linalg.inv(X.T @ X) @ X.T @ y
    y_hat = X @ b
    residuals = y - y_hat

    Q_hat = (X.T @ X) / T
    Omega_hat = np.zeros((k, k))
    for t in range(T):
       Omega_hat += residuals[t]**2 * X[t,:].reshape(-1, 1) @ X[t,:].reshape(1, -1)
    Omega_hat = Omega_hat / T
    avar_b_het = np.linalg.inv(Q_hat) @ Omega_hat @ np.linalg.inv(Q_hat)
    return b, avar_b_het

T_values = [500, 1000, 2000, 5000]
results = []

for T in T_values:
  b, avar_b_het = simulate_data_and_estimate(T)
  results.append({"T": T,
                  "Avar_b0": avar_b_het[0,0],
                  "Avar_b1": avar_b_het[1,1]})
df = pd.DataFrame(results)
print(df)
```
Este exemplo ilustra que, √† medida que o tamanho da amostra aumenta, a vari√¢ncia estimada dos coeficientes ($Avar\_b0$ e $Avar\_b1$) tendem a diminuir, o que reflete a consist√™ncia da matriz de covari√¢ncia assint√≥tica. Os valores da matriz de covari√¢ncia assint√≥tica est√£o convergindo para um valor mais est√°vel √† medida que T aumenta.

### Infer√™ncia Estat√≠stica Usando a Matriz de Covari√¢ncia Assint√≥tica
Com a **matriz de covari√¢ncia assint√≥tica** em m√£os, podemos realizar testes de hip√≥teses e construir intervalos de confian√ßa para os par√¢metros. Para um teste de hip√≥tese sobre um √∫nico par√¢metro, digamos  $H_0 : \beta_i = \beta_{i0}$, podemos usar a estat√≠stica
$$ t = \frac{b_i - \beta_{i0}}{\sqrt{\widehat{\text{Avar}(b_i)}}}, $$
onde $\widehat{\text{Avar}(b_i)}$ √© a *i*-√©sima entrada diagonal da **matriz de covari√¢ncia assint√≥tica** estimada. Sob a hip√≥tese nula e para amostras grandes, a estat√≠stica $t$ se aproxima de uma distribui√ß√£o normal padr√£o. Se desejarmos testar m√∫ltiplas restri√ß√µes sobre os par√¢metros, como $H_0 : R\beta = r$, onde $R$ √© uma matriz de restri√ß√µes e $r$ √© um vetor de constantes, podemos usar a estat√≠stica de Wald
$$ W = (Rb - r)'(R\widehat{\text{Avar}(b)}R')^{-1}(Rb - r). $$
Sob a hip√≥tese nula, a estat√≠stica $W$ se aproxima de uma distribui√ß√£o $\chi^2$ com um n√∫mero de graus de liberdade igual ao n√∫mero de restri√ß√µes.
Al√©m dos testes de hip√≥teses, podemos construir intervalos de confian√ßa usando a matriz de covari√¢ncia assint√≥tica. Um intervalo de confian√ßa para um par√¢metro $\beta_i$ √© dado por
$$ [b_i - z_{\alpha/2}\sqrt{\widehat{\text{Avar}(b_i)}}, b_i + z_{\alpha/2}\sqrt{\widehat{\text{Avar}(b_i)}}], $$
onde $z_{\alpha/2}$ √© o quantil da distribui√ß√£o normal padr√£o para um n√≠vel de signific√¢ncia $\alpha$.

**Teorema 1**
Sob as condi√ß√µes de regularidade padr√£o para infer√™ncia assint√≥tica e  a hip√≥tese nula $H_0: \beta_i = \beta_{i0}$, a estat√≠stica  $t = \frac{b_i - \beta_{i0}}{\sqrt{\widehat{\text{Avar}(b_i)}}}$ converge em distribui√ß√£o para uma normal padr√£o.

*Proof.*
I. Sob as condi√ß√µes de regularidade, temos que $\sqrt{T}(b - \beta_0) \xrightarrow{d} N(0, Q^{-1}\Omega Q^{-1})$.
II. Isso implica que $\sqrt{T}(b_i - \beta_{i0})$ converge para uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia dada pela i-√©sima entrada diagonal de $Q^{-1}\Omega Q^{-1}$, que denotamos por $Avar(\sqrt{T}b_i)$.
III.  Como $\widehat{Avar(b_i)}$ √© um estimador consistente de $\frac{1}{T}Q^{-1}\Omega Q^{-1}$ (Proposi√ß√£o 1), ent√£o $\frac{1}{\sqrt{T}}\sqrt{Avar(\sqrt{T}b_i)} \xrightarrow{p} \sqrt{\widehat{\text{Avar}(b_i)}}$.
IV. Pelo teorema de Slutsky, podemos dividir $\sqrt{T}(b_i - \beta_{i0})$ por $\sqrt{T \widehat{\text{Avar}(b_i)}}$ sem alterar a distribui√ß√£o assint√≥tica:
$$\frac{\sqrt{T}(b_i - \beta_{i0})}{\sqrt{T}\sqrt{\widehat{\text{Avar}(b_i)}}} = \frac{b_i - \beta_{i0}}{\sqrt{\widehat{\text{Avar}(b_i)}}} \xrightarrow{d} N(0, 1)$$
V. Portanto, a estat√≠stica t converge em distribui√ß√£o para uma normal padr√£o.
‚ñ†

**Teorema 2**
Sob as condi√ß√µes de regularidade padr√£o para infer√™ncia assint√≥tica, e a hip√≥tese nula $H_0: R\beta = r$, a estat√≠stica de Wald  $W = (Rb - r)'(R\widehat{\text{Avar}(b)}R')^{-1}(Rb - r)$  converge em distribui√ß√£o para uma $\chi^2$ com um n√∫mero de graus de liberdade igual ao n√∫mero de restri√ß√µes.

*Proof.*
I.  Sob as condi√ß√µes de regularidade, $\sqrt{T}(b-\beta) \xrightarrow{d} N(0, Q^{-1}\Omega Q^{-1})$.
II. Pelo teorema do mapeamento cont√≠nuo, $\sqrt{T}(Rb-R\beta) \xrightarrow{d} N(0, RQ^{-1}\Omega Q^{-1}R')$.
III.  Sob a hip√≥tese nula, temos $R\beta=r$,  e  $\sqrt{T}(Rb-r) \xrightarrow{d} N(0, RQ^{-1}\Omega Q^{-1}R')$.
IV. Seja $\Sigma = RQ^{-1}\Omega Q^{-1}R'$. A estat√≠stica de Wald √© constru√≠da de tal forma que $W = (\sqrt{T}(Rb - r))' (\frac{1}{T}R\widehat{\text{Avar}(b)}R')^{-1}(\sqrt{T}(Rb - r))$.
V. Pelo teorema de Slutsky, e utilizando um estimador consistente da matriz de vari√¢ncia assint√≥tica de $\sqrt{T}(Rb-r)$, temos que $W \xrightarrow{d} \chi^2(k)$ onde $k$ √© o n√∫mero de restri√ß√µes, ou seja, o n√∫mero de linhas da matriz $R$.
‚ñ†

> üí° **Exemplo Num√©rico:** Vamos usar o mesmo modelo de regress√£o com heteroscedasticidade para realizar um teste de hip√≥teses. Suponha que queremos testar $H_0: \beta_1 = 2$, ou seja, se o coeficiente de inclina√ß√£o √© igual a 2. Vamos usar tanto a matriz de covari√¢ncia usual quanto a assint√≥tica para calcular a estat√≠stica t e o p-valor.

```python
import numpy as np
import pandas as pd
from scipy.stats import t, norm

# Gerar dados (mesmo modelo anterior)
np.random.seed(42)
T = 1000
x = np.random.rand(T) * 5
sigma_t = np.sqrt(0.5 + 0.2 * x**2)
u = np.random.normal(0, sigma_t, T)
y = 1 + 2 * x + u
X = np.column_stack((np.ones(T), x))
k=2


# Estima√ß√£o por OLS
b = np.linalg.inv(X.T @ X) @ X.T @ y
y_hat = X @ b
residuals = y - y_hat

# Matriz de covari√¢ncia usual
sigma2_hat = np.sum(residuals**2) / (T-k)
var_b_usual = sigma2_hat * np.linalg.inv(X.T @ X)
se_b_usual = np.sqrt(np.diag(var_b_usual))


# Matriz de covari√¢ncia assint√≥tica
Q_hat = (X.T @ X) / T
Omega_hat = np.zeros((k, k))
for t in range(T):
    Omega_hat += residuals[t]**2 * X[t,:].reshape(-1, 1) @ X[t,:].reshape(1, -1)
Omega_hat = Omega_hat / T
avar_b_het = np.linalg.inv(Q_hat) @ Omega_hat @ np.linalg.inv(Q_hat)
se_b_het = np.sqrt(np.diag(avar_b_het))


# Teste de hip√≥teses
beta_1_null = 2
t_stat_usual = (b[1] - beta_1_null) / se_b_usual[1]
t_stat_het = (b[1] - beta_1_null) / se_b_het[1]

p_value_usual = 2 * (1 - norm.cdf(abs(t_stat_usual))) # p valor para teste bicaudal
p_value_het = 2 * (1 - norm.cdf(abs(t_stat_het))) # p valor para teste bicaudal


print(f"Estat√≠stica t (matriz usual): {t_stat_usual:.3f}, p-valor: {p_value_usual:.3f}")
print(f"Estat√≠stica t (matriz assint√≥tica): {t_stat_het:.3f}, p-valor: {p_value_het:.3f}")
```
Este exemplo mostra como o uso da matriz de covari√¢ncia assint√≥tica pode influenciar o resultado de um teste de hip√≥teses. O p-valor utilizando a matriz assint√≥tica √© mais adequado para infer√™ncia quando a heteroscedasticidade est√° presente.  Se usarmos a matriz de covari√¢ncia usual, o p-valor estar√° incorreto.

> üí° **Exemplo Num√©rico:** Vamos construir um intervalo de confian√ßa de 95% para o coeficiente de inclina√ß√£o ($\beta_1$) usando a matriz de covari√¢ncia assint√≥tica.

```python
import numpy as np
import pandas as pd
from scipy.stats import norm

# Dados e estimativas (mesmo c√≥digo anterior)
np.random.seed(42)
T = 1000
x = np.random.rand(T) * 5
sigma_t = np.sqrt(0.5 + 0.2 * x**2)
u = np.random.normal(0, sigma_t, T)
y = 1 + 2 * x + u
X = np.column_stack((np.ones(T), x))
k=2

b = np.linalg.inv(X.T @ X) @ X.T @ y
y_hat = X @ b
residuals = y - y_hat

Q_hat = (X.T @ X) / T
Omega_hat = np.zeros((k, k))
for t in range(T):
    Omega_hat += residuals[t]**2 * X[t,:].reshape(-1, 1) @ X[t,:].reshape(1, -1)
Omega_hat = Omega_hat / T
avar_b_het = np.linalg.inv(Q_hat) @ Omega_hat @ np.linalg.inv(Q_hat)
se_b_het = np.sqrt(np.diag(avar_b_het))

# Intervalo de confian√ßa
alpha = 0.05
z = norm.ppf(1-alpha/2)

interval_b1 = (b[1] - z * se_b_het[1], b[1] + z * se_b_het[1])
print(f"Intervalo de confian√ßa de 95% para beta_1: {interval_b1}")

```
O intervalo de confian√ßa de 95% para $\beta_1$ obtido utilizando a matriz de covari√¢ncia assint√≥tica permite que fa√ßamos afirma√ß√µes sobre a incerteza da estimativa do par√¢metro.

### Rela√ß√£o com a Matriz de Informa√ß√£o de Fisher
Em modelos de estima√ß√£o por m√°xima verossimilhan√ßa, a **matriz de covari√¢ncia assint√≥tica** √© muitas vezes dada pelo inverso da **matriz de informa√ß√£o de Fisher**. No caso de regress√£o linear com erros Gaussianos, a fun√ß√£o de verossimilhan√ßa √©
$$ L(\beta, \sigma^2 | y, X) = (2\pi \sigma^2)^{-T/2} \text{exp} (-\frac{1}{2\sigma^2} (y - X\beta)'(y - X\beta)). $$
A **matriz de informa√ß√£o de Fisher** √© dada pela esperan√ßa do hessiano negativo da fun√ß√£o de log-verossimilhan√ßa em rela√ß√£o aos par√¢metros. Para o caso da regress√£o linear, a matriz de informa√ß√£o de Fisher √© dada por
$$ I(\beta, \sigma^2) = \begin{bmatrix} \frac{X'X}{\sigma^2} & 0 \\ 0 & \frac{T}{2\sigma^4} \end{bmatrix}. $$
Invertendo essa matriz, obtemos a **matriz de covari√¢ncia assint√≥tica** dos estimadores de m√°xima verossimilhan√ßa. Note que para $\beta$, obtemos $Cov(\beta) = \sigma^2(X'X)^{-1}$, que √© a matriz de covari√¢ncia usual, enquanto que para $\sigma^2$, obtemos $Cov(\sigma^2) = 2\sigma^4/T$. No entanto, note que se as premissas de homoscedasticidade s√£o violadas, a forma da matriz de informa√ß√£o de Fisher √© diferente e a matriz de covari√¢ncia assint√≥tica deve ser calculada de forma adequada. Sob erros n√£o gaussianos, a rela√ß√£o com a matriz de informa√ß√£o de Fisher n√£o √© t√£o direta, mas o conceito de matriz de covari√¢ncia assint√≥tica ainda se aplica.

### Conclus√£o
Neste cap√≠tulo, exploramos a **matriz de covari√¢ncia assint√≥tica** do estimador de par√¢metros em modelos de regress√£o linear sob condi√ß√µes gerais. Demonstramos como essa matriz √© calculada e como ela √© usada para realizar **infer√™ncia estat√≠stica** v√°lida, atrav√©s de **testes de hip√≥teses** e constru√ß√£o de **intervalos de confian√ßa**. Mostramos que a matriz de covari√¢ncia usual pode ser inadequada em presen√ßa de **heteroscedasticidade**, enquanto a matriz de covari√¢ncia assint√≥tica fornece uma aproxima√ß√£o mais precisa da variabilidade do estimador, sendo crucial para realizar infer√™ncia em grandes amostras. Finalmente, discutimos a conex√£o da **matriz de covari√¢ncia assint√≥tica** com a **matriz de informa√ß√£o de Fisher**, ressaltando sua import√¢ncia para a estima√ß√£o por m√°xima verossimilhan√ßa. Em cap√≠tulos seguintes, esses conceitos ser√£o aplicados para analisar modelos de regress√£o mais complexos, como modelos com **autocorrela√ß√£o** e **vari√°veis end√≥genas**.

### Refer√™ncias
[^1]: [8.1.12], [8.1.18] , [8.2.33], [8.2.34] do texto fornecido.
<!-- END -->
