## Distribui√ß√£o Assint√≥tica do Estimador de M√≠nimos Quadrados

### Introdu√ß√£o
Dando sequ√™ncia √† discuss√£o sobre **consist√™ncia** e **matriz de covari√¢ncia assint√≥tica** dos estimadores de regress√£o linear, este cap√≠tulo tem como objetivo principal derivar a **distribui√ß√£o assint√≥tica do estimador de m√≠nimos quadrados (OLS)**. Sob condi√ß√µes assint√≥ticas, demonstraremos que a raiz quadrada do n√∫mero de observa√ß√µes multiplicada pela diferen√ßa entre o estimador OLS e o verdadeiro par√¢metro converge para uma distribui√ß√£o normal com m√©dia zero. Este resultado √© fundamental para realizar infer√™ncia estat√≠stica em grandes amostras e construir testes de hip√≥teses e intervalos de confian√ßa. Este cap√≠tulo constr√≥i sobre o conhecimento pr√©vio de **consist√™ncia**, **matriz de covari√¢ncia assint√≥tica**, e as propriedades do estimador OLS e da matriz de proje√ß√£o discutidas anteriormente [^1], [^2]. Vamos explorar como a converg√™ncia em distribui√ß√£o do estimador √© afetada quando as premissas cl√°ssicas de regress√£o s√£o relaxadas.

### Deriva√ß√£o da Distribui√ß√£o Assint√≥tica
O ponto de partida para derivar a distribui√ß√£o assint√≥tica do estimador OLS √© a express√£o do estimador em termos dos dados e dos res√≠duos:
$$b = (X'X)^{-1}X'y = (X'X)^{-1}X'(X\beta + u) = \beta + (X'X)^{-1}X'u $$
Rearranjando, obtemos a diferen√ßa entre o estimador e o par√¢metro verdadeiro:
$$ b - \beta = (X'X)^{-1}X'u. $$
Para analisar o comportamento assint√≥tico, multiplicamos ambos os lados por $\sqrt{T}$, onde $T$ √© o n√∫mero de observa√ß√µes:
$$ \sqrt{T}(b - \beta) = \sqrt{T}(X'X)^{-1}X'u = (\frac{X'X}{T})^{-1}\frac{X'u}{\sqrt{T}} $$
Podemos analisar essa express√£o separadamente.
Primeiro,  sabemos que $\frac{X'X}{T}$ converge em probabilidade para uma matriz definida positiva $Q$, ou seja, $\frac{X'X}{T} \xrightarrow{p} Q$. Portanto, pelo lema 1 do cap√≠tulo anterior, temos que $(\frac{X'X}{T})^{-1} \xrightarrow{p} Q^{-1}$.
Segundo, vamos analisar $\frac{X'u}{\sqrt{T}}$. Em geral, podemos escrever $\frac{X'u}{\sqrt{T}} = \frac{1}{\sqrt{T}}\sum_{t=1}^T x_t u_t$, onde $x_t$ √© a $t$-√©sima linha de $X$. Sob as premissas de que $x_t u_t$ formam uma sequ√™ncia de diferen√ßas de martingala e de que seus momentos de segunda ordem s√£o finitos e convergem, podemos aplicar um teorema central do limite para vari√°veis aleat√≥rias dependentes. De maneira geral, sob condi√ß√µes de regularidade, temos que
$$\frac{X'u}{\sqrt{T}} \xrightarrow{d} N(0, \Omega),$$
onde $\Omega = \lim_{T \to \infty} \frac{1}{T} E(X'uu'X)$. A matriz $\Omega$ representa a vari√¢ncia assint√≥tica da soma ponderada dos res√≠duos, que pode ser uma matriz geral no caso de heteroscedasticidade ou autocorrela√ß√£o.
Combinando os dois resultados e aplicando o lema 2 do cap√≠tulo anterior, obtemos:
$$ \sqrt{T}(b - \beta) = (\frac{X'X}{T})^{-1}\frac{X'u}{\sqrt{T}} \xrightarrow{d} N(0, Q^{-1}\Omega Q^{-1}) $$
Este resultado fundamental estabelece que, sob condi√ß√µes assint√≥ticas, a distribui√ß√£o de $\sqrt{T}(b - \beta)$ converge para uma distribui√ß√£o normal com m√©dia zero e matriz de covari√¢ncia $Q^{-1}\Omega Q^{-1}$. Esta matriz, como discutido anteriormente, √© uma generaliza√ß√£o da matriz de covari√¢ncia usual, acomodando cen√°rios em que as premissas cl√°ssicas de homocedasticidade e aus√™ncia de autocorrela√ß√£o n√£o s√£o satisfeitas.

**Lema 1.1**
Se $\frac{X'X}{T} \xrightarrow{p} Q$ e $Q$ √© uma matriz definida positiva, ent√£o $(\frac{X'X}{T})^{-1} \xrightarrow{p} Q^{-1}$.

*Prova.*
Este lema √© uma aplica√ß√£o direta do lema 1 do cap√≠tulo anterior, que afirma que a inversa de uma matriz que converge em probabilidade para uma matriz invers√≠vel tamb√©m converge em probabilidade para a inversa dessa matriz.
‚ñ†

**Proposi√ß√£o 1.1**
Sob as condi√ß√µes de regularidade, incluindo que $x_t u_t$ formam uma sequ√™ncia de diferen√ßas de martingala e que seus momentos de segunda ordem s√£o finitos e convergem, temos que $\frac{X'u}{\sqrt{T}} \xrightarrow{d} N(0, \Omega)$, onde $\Omega = \lim_{T \to \infty} \frac{1}{T} E(X'uu'X)$.

*Prova.*
Esta proposi√ß√£o √© um resultado padr√£o de Teoremas do Limite Central para vari√°veis aleat√≥rias dependentes, onde a condi√ß√£o de martingala garante a aus√™ncia de autocorrela√ß√£o sistem√°tica no tempo, permitindo que se utilize o resultado assint√≥tico de uma soma ponderada de vari√°veis aleat√≥rias.
‚ñ†

> üí° **Exemplo Num√©rico:** Para ilustrar a distribui√ß√£o assint√≥tica do estimador de m√≠nimos quadrados, vamos simular dados de um modelo de regress√£o simples com heteroscedasticidade. O modelo √© dado por $y_t = \beta_0 + \beta_1 x_t + u_t$, onde $u_t$ tem m√©dia zero e vari√¢ncia $\sigma_t^2 = 0.5 + 0.2x_t^2$. Vamos considerar $\beta_0 = 1$ e $\beta_1 = 2$. Para isso, vamos simular um conjunto de dados com $T = 1000$ observa√ß√µes para $x$ seguindo uma distribui√ß√£o uniforme entre 0 e 5. Os erros ser√£o simulados de uma distribui√ß√£o normal com m√©dia zero e desvio padr√£o dado por $\sigma_t$. Em seguida, vamos estimar os coeficientes usando OLS.
>
>  ```python
>  import numpy as np
>  import pandas as pd
>  from sklearn.linear_model import LinearRegression
>  import matplotlib.pyplot as plt
>  import scipy.stats as st
>
>  np.random.seed(42)
>  T = 1000
>  x = np.random.rand(T) * 5
>  sigma_t = np.sqrt(0.5 + 0.2 * x**2)
>  u = np.random.normal(0, sigma_t, T)
>  y = 1 + 2 * x + u
>  X = np.column_stack((np.ones(T), x))
>  
>  # Estima√ß√£o por OLS
>  model = LinearRegression()
>  model.fit(X,y)
>  b = np.array([model.intercept_, model.coef_[1]])
>  
>  print(f"Estimativas dos coeficientes: b0 = {b[0]:.3f}, b1 = {b[1]:.3f}")
>  ```
> Ap√≥s a estima√ß√£o, vamos simular a distribui√ß√£o de $\sqrt{T}(b-\beta)$ atrav√©s de um experimento de Monte Carlo. Vamos simular 1000 amostras de dados, reestimando os par√¢metros para cada amostra e coletando os resultados. Para cada conjunto de dados simulado, os erros s√£o recalculados com a mesma estrutura de heterocedasticidade.
> ```python
>  num_simulations = 1000
>  b_simulations = np.zeros((num_simulations, 2))
>  
>  for i in range(num_simulations):
>    x = np.random.rand(T) * 5
>    sigma_t = np.sqrt(0.5 + 0.2 * x**2)
>    u = np.random.normal(0, sigma_t, T)
>    y = 1 + 2 * x + u
>    X = np.column_stack((np.ones(T), x))
>    model = LinearRegression()
>    model.fit(X,y)
>    b_sim = np.array([model.intercept_, model.coef_[1]])
>    b_simulations[i, :] = b_sim
>
>  b_diff = np.sqrt(T) * (b_simulations - b)
>  mean_b_diff = np.mean(b_diff, axis=0)
>  cov_b_diff = np.cov(b_diff, rowvar = False)
>  print("\nM√©dia das diferen√ßas simuladas:", mean_b_diff)
>  print("\nMatriz de covari√¢ncia das diferen√ßas simuladas:\n", cov_b_diff)
>  ```
> Este c√≥digo calcula a diferen√ßa entre as estimativas simuladas e as verdadeiras, multiplicada por $\sqrt{T}$. Observamos que as m√©dias est√£o pr√≥ximas de zero e que a matriz de covari√¢ncia assint√≥tica estimada se aproxima da matriz de covari√¢ncia calculada pelas simula√ß√µes. Isso confirma que a distribui√ß√£o de $\sqrt{T}(b-\beta)$ converge para uma normal com m√©dia zero.
>
> Agora, vamos visualizar a distribui√ß√£o simulada comparando com a distribui√ß√£o normal te√≥rica:
> ```python
>  Q_hat = (X.T @ X) / T
>  residuals = y - X @ b
>  Omega_hat = np.zeros((2, 2))
>  for t in range(T):
>    Omega_hat += residuals[t]**2 * X[t,:].reshape(-1, 1) @ X[t,:].reshape(1, -1)
>  Omega_hat = Omega_hat / T
>  avar_b_het = np.linalg.inv(Q_hat) @ Omega_hat @ np.linalg.inv(Q_hat)
>  x_axis_b0 = np.linspace(mean_b_diff[0] - 3*np.sqrt(avar_b_het[0,0]), mean_b_diff[0] + 3*np.sqrt(avar_b_het[0,0]), 100)
>  pdf_theoretical_b0 = st.norm.pdf(x_axis_b0, mean_b_diff[0], np.sqrt(avar_b_het[0,0]))
>  x_axis_b1 = np.linspace(mean_b_diff[1] - 3*np.sqrt(avar_b_het[1,1]), mean_b_diff[1] + 3*np.sqrt(avar_b_het[1,1]), 100)
>  pdf_theoretical_b1 = st.norm.pdf(x_axis_b1, mean_b_diff[1], np.sqrt(avar_b_het[1,1]))
>
>  fig, axs = plt.subplots(1, 2, figsize = (12, 6))
>  axs[0].hist(b_diff[:,0], bins=30, density=True, alpha=0.6, label='Simula√ß√µes')
>  axs[0].plot(x_axis_b0, pdf_theoretical_b0, 'r', label='Normal Te√≥rica')
>  axs[0].set_xlabel('sqrt(T)(b_0 - beta_0)')
>  axs[0].set_ylabel('Densidade')
>  axs[0].set_title('Distribui√ß√£o Assint√≥tica do Estimador b_0')
>  axs[0].legend()
>  axs[1].hist(b_diff[:,1], bins=30, density=True, alpha=0.6, label='Simula√ß√µes')
>  axs[1].plot(x_axis_b1, pdf_theoretical_b1, 'r', label='Normal Te√≥rica')
>  axs[1].set_xlabel('sqrt(T)(b_1 - beta_1)')
>  axs[1].set_ylabel('Densidade')
>  axs[1].set_title('Distribui√ß√£o Assint√≥tica do Estimador b_1')
>  axs[1].legend()
>  plt.show()
> ```
> Os histogramas das simula√ß√µes se assemelham √† distribui√ß√£o normal te√≥rica, confirmando que a distribui√ß√£o de $\sqrt{T}(b-\beta)$ converge para uma distribui√ß√£o normal com m√©dia zero.

### Implica√ß√µes para Infer√™ncia Estat√≠stica
O resultado de que $\sqrt{T}(b-\beta)$ converge para uma distribui√ß√£o normal permite que realizemos testes de hip√≥teses e construamos intervalos de confian√ßa para os par√¢metros. Em particular, se desejamos testar a hip√≥tese nula de que um par√¢metro $\beta_i$ √© igual a um valor espec√≠fico $\beta_{i0}$, podemos usar a estat√≠stica
$$ t = \frac{b_i - \beta_{i0}}{\sqrt{\widehat{\text{Avar}(b_i)}}} $$
onde $\widehat{\text{Avar}(b_i)}$ √© o *i*-√©simo elemento diagonal da matriz de covari√¢ncia assint√≥tica estimada. Sob a hip√≥tese nula, a distribui√ß√£o dessa estat√≠stica se aproxima de uma distribui√ß√£o normal padr√£o para amostras grandes. Este resultado √© especialmente importante porque, na pr√°tica, a matriz de covari√¢ncia assint√≥tica √© estimada usando os dados, como demonstrado no cap√≠tulo anterior.

**Teorema 1**
Sob as condi√ß√µes de regularidade padr√£o e a premissa de que o estimador OLS √© consistente, a estat√≠stica $t = \frac{b_i - \beta_{i0}}{\sqrt{\widehat{\text{Avar}(b_i)}}}$ converge em distribui√ß√£o para uma normal padr√£o.

*Prova.*
Vamos provar que sob as condi√ß√µes dadas, a estat√≠stica t converge em distribui√ß√£o para uma normal padr√£o.

I. Como demonstrado anteriormente, temos $\sqrt{T}(b - \beta) \xrightarrow{d} N(0, Q^{-1}\Omega Q^{-1})$.
   
II. Isso implica que $\sqrt{T}(b_i - \beta_i)$ converge para uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia dada pela *i*-√©sima entrada diagonal de $Q^{-1}\Omega Q^{-1}$. Denotaremos essa vari√¢ncia por $[Q^{-1}\Omega Q^{-1}]_{ii}$.
    
III.  Como vimos no cap√≠tulo anterior, a matriz de covari√¢ncia assint√≥tica estimada  $\widehat{Avar}(b)$  √© um estimador consistente para a matriz de covari√¢ncia assint√≥tica verdadeira, ou seja, $\widehat{Avar}(b) \xrightarrow{p}  \frac{1}{T} Q^{-1}\Omega Q^{-1}$.
   
IV. Consequentemente, $\widehat{\text{Avar}(b_i)} \xrightarrow{p} \frac{1}{T} [Q^{-1}\Omega Q^{-1}]_{ii}$, onde $[.]_{ii}$  denota o i-√©simo elemento diagonal de uma matriz. Tomando a raiz quadrada, tamb√©m temos $\sqrt{\widehat{\text{Avar}(b_i)}} \xrightarrow{p} \frac{1}{\sqrt{T}} \sqrt{[Q^{-1}\Omega Q^{-1}]_{ii}}$.

V. Definimos a estat√≠stica t como: 
    $$ t = \frac{b_i - \beta_{i0}}{\sqrt{\widehat{\text{Avar}(b_i)}}} = \frac{\sqrt{T}(b_i - \beta_{i0})}{\sqrt{T}\sqrt{\widehat{\text{Avar}(b_i)}}} $$
  
VI. Pelo teorema de Slutsky, temos que
$$ \frac{b_i - \beta_{i0}}{\sqrt{\widehat{\text{Avar}(b_i)}}}  = \frac{\frac{1}{\sqrt{T}}\sqrt{T}(b_i - \beta_{i0})}{\sqrt{\widehat{\text{Avar}(b_i)}}} \xrightarrow{d} N(0,1). $$
    Isso ocorre porque $\sqrt{T}(b_i - \beta_{i0})$ converge em distribui√ß√£o para uma normal com m√©dia 0 e vari√¢ncia $[Q^{-1}\Omega Q^{-1}]_{ii}$ e o denominador $\sqrt{\widehat{\text{Avar}(b_i)}}$ converge em probabilidade para $\frac{1}{\sqrt{T}} \sqrt{[Q^{-1}\Omega Q^{-1}]_{ii}}$.  Quando dividimos uma vari√°vel que converge em distribui√ß√£o para uma normal padr√£o por uma vari√°vel que converge em probabilidade para uma constante, o resultado converge para uma normal padr√£o.
‚ñ†

Este teorema estabelece a base para a infer√™ncia em grandes amostras com o uso da matriz de covari√¢ncia assint√≥tica.

**Corol√°rio 1.1**
Sob as mesmas condi√ß√µes do Teorema 1, podemos construir um intervalo de confian√ßa assint√≥tico para $\beta_i$ da seguinte forma:
$$ IC_{1-\alpha} (\beta_i) = \left[b_i - z_{\alpha/2} \sqrt{\widehat{\text{Avar}(b_i)}}, b_i + z_{\alpha/2} \sqrt{\widehat{\text{Avar}(b_i)}} \right]$$
onde $z_{\alpha/2}$ √© o valor cr√≠tico da distribui√ß√£o normal padr√£o correspondente ao n√≠vel de confian√ßa $1-\alpha$.

*Prova.*
Vamos mostrar como construir o intervalo de confian√ßa assint√≥tico para $\beta_i$.

I. Pelo Teorema 1, temos que a estat√≠stica $t = \frac{b_i - \beta_{i}}{\sqrt{\widehat{\text{Avar}(b_i)}}}$ converge em distribui√ß√£o para uma normal padr√£o.

II. Para um n√≠vel de confian√ßa $1-\alpha$, procuramos o valor cr√≠tico $z_{\alpha/2}$ tal que $P(-z_{\alpha/2} < t < z_{\alpha/2}) \approx 1 - \alpha$.

III.  Substituindo a express√£o da estat√≠stica $t$, temos
$$P\left(-z_{\alpha/2} < \frac{b_i - \beta_{i}}{\sqrt{\widehat{\text{Avar}(b_i)}}} < z_{\alpha/2}\right) \approx 1 - \alpha$$

IV. Rearranjando a desigualdade, obtemos:
$$P\left(b_i - z_{\alpha/2} \sqrt{\widehat{\text{Avar}(b_i)}} < \beta_{i} < b_i + z_{\alpha/2} \sqrt{\widehat{\text{Avar}(b_i)}}\right) \approx 1 - \alpha$$

V. Portanto, o intervalo de confian√ßa assint√≥tico para $\beta_i$ √© dado por:
$$ IC_{1-\alpha} (\beta_i) = \left[b_i - z_{\alpha/2} \sqrt{\widehat{\text{Avar}(b_i)}}, b_i + z_{\alpha/2} \sqrt{\widehat{\text{Avar}(b_i)}} \right]$$
‚ñ†

> üí° **Exemplo Num√©rico:**  Vamos usar os mesmos dados simulados para calcular o intervalo de confian√ßa para o coeficiente de inclina√ß√£o $\beta_1$ e realizar um teste de hip√≥tese para $H_0: \beta_1=2$. Usaremos um n√≠vel de confian√ßa de 95%, para o qual $z_{\alpha/2} \approx 1.96$.
> ```python
> import numpy as np
> import pandas as pd
> from scipy.stats import norm
>
> # Gerar dados (mesmo modelo anterior)
> np.random.seed(42)
> T = 1000
> x = np.random.rand(T) * 5
> sigma_t = np.sqrt(0.5 + 0.2 * x**2)
> u = np.random.normal(0, sigma_t, T)
> y = 1 + 2 * x + u
> X = np.column_stack((np.ones(T), x))
> k = 2
>
>
> # Estima√ß√£o por OLS
> model = LinearRegression()
> model.fit(X,y)
> b = np.array([model.intercept_, model.coef_[1]])
> y_hat = X @ b
> residuals = y - y_hat
>
> # Matriz de covari√¢ncia assint√≥tica
> Q_hat = (X.T @ X) / T
> Omega_hat = np.zeros((k, k))
> for t in range(T):
>     Omega_hat += residuals[t]**2 * X[t,:].reshape(-1, 1) @ X[t,:].reshape(1, -1)
> Omega_hat = Omega_hat / T
> avar_b_het = np.linalg.inv(Q_hat) @ Omega_hat @ np.linalg.inv(Q_hat)
> se_b_het = np.sqrt(np.diag(avar_b_het))
>
>
> # Intervalo de confian√ßa
> alpha = 0.05
> z_alpha_2 = norm.ppf(1-alpha/2)
> ic_lower = b[1] - z_alpha_2 * se_b_het[1]
> ic_upper = b[1] + z_alpha_2 * se_b_het[1]
> print(f"Intervalo de confian√ßa de 95% para beta_1: [{ic_lower:.3f}, {ic_upper:.3f}]")
>
> # Teste de hip√≥tese
> beta_1_null = 2
> t_stat_het = (b[1] - beta_1_null) / se_b_het[1]
> p_value_het = 2 * (1 - norm.cdf(abs(t_stat_het))) # p valor para teste bicaudal
> print(f"Estat√≠stica t: {t_stat_het:.3f}, p-valor: {p_value_het:.3f}")
> ```
>
> Observamos que o intervalo de confian√ßa cont√©m o valor verdadeiro de $\beta_1=2$ na maioria dos casos. O p-valor fornece evid√™ncias contra a hip√≥tese nula se for menor que o n√≠vel de signific√¢ncia, por exemplo, 0.05. Caso o p-valor seja menor que 0.05, rejeitamos a hip√≥tese nula de que $\beta_1=2$.

### Conclus√£o
Neste cap√≠tulo, derivamos a **distribui√ß√£o assint√≥tica do estimador de m√≠nimos quadrados** sob condi√ß√µes gerais. Demonstramos que $\sqrt{T}(b - \beta)$ converge para uma distribui√ß√£o normal com m√©dia zero e matriz de covari√¢ncia $Q^{-1}\Omega Q^{-1}$. Este resultado permite que fa√ßamos **infer√™ncias estat√≠sticas v√°lidas** usando as propriedades assint√≥ticas dos estimadores, como a constru√ß√£o de testes de hip√≥teses e intervalos de confian√ßa, mesmo quando as premissas cl√°ssicas de homocedasticidade e aus√™ncia de autocorrela√ß√£o s√£o violadas. Os resultados deste cap√≠tulo consolidam os conceitos de **consist√™ncia**, **matriz de covari√¢ncia assint√≥tica**, e **distribui√ß√£o assint√≥tica**, que juntos fornecem as ferramentas necess√°rias para an√°lise de regress√£o em grandes amostras sob diversas condi√ß√µes.

### Refer√™ncias
[^1]: [8.1.12], [8.2.33] do texto fornecido.
[^2]: Cap√≠tulos anteriores neste documento.
<!-- END -->
