## Consist√™ncia e Distribui√ß√£o Assint√≥tica da Estimativa de Vari√¢ncia

### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise de **modelos de regress√£o linear** sob condi√ß√µes mais gerais do que as cl√°ssicas, como **heteroscedasticidade** e **autocorrela√ß√£o**. Nos cap√≠tulos anteriores, foram introduzidas as **premissas cl√°ssicas de regress√£o**, onde os erros s√£o i.i.d. Gaussianos e as vari√°veis explicativas s√£o determin√≠sticas [^1]. Agora, abordaremos cen√°rios em que essas premissas s√£o relaxadas, levando a estimadores que, embora ainda sejam n√£o viesados, podem n√£o ser eficientes sob as premissas cl√°ssicas. O foco desta se√ß√£o √© demonstrar a **consist√™ncia da estimativa de vari√¢ncia**, essencial para **infer√™ncias estat√≠sticas** v√°lidas, como **testes de hip√≥teses**. Expandindo o estudo de estimadores **consistentes** para a vari√¢ncia, que se mostrou necess√°rio para infer√™ncias sob condi√ß√µes mais gerais que as cl√°ssicas, exploramos em detalhes sua distribui√ß√£o assint√≥tica.

### Consist√™ncia da Estimativa da Vari√¢ncia
No contexto de **regress√£o linear**, a **estimativa da vari√¢ncia** dos res√≠duos, denotada por $s^2$, desempenha um papel crucial em infer√™ncia. Para que as infer√™ncias sejam v√°lidas, a **estimativa de vari√¢ncia** precisa ser consistente, ou seja, convergir em probabilidade para a verdadeira vari√¢ncia $\sigma^2$ quando o tamanho da amostra aumenta.

**Retomando a discuss√£o do modelo de regress√£o linear** $y = X\beta + u$, vimos que o estimador de m√≠nimos quadrados ordin√°rios (OLS), denotado por $b$, √© n√£o viesado sob a premissa de que $E(u)=0$ e $E(uu') = \sigma^2 I_T$ [^1]. Contudo, quando essa √∫ltima premissa √© violada, como em casos de **heteroscedasticidade** ou **autocorrela√ß√£o**, a **distribui√ß√£o amostral de $b$** n√£o √© mais dada por $N(\beta, \sigma^2(X'X)^{-1})$. O estimador da **vari√¢ncia dos res√≠duos**, $s^2$, definido como $s^2 = RSS/(T-k) = \hat{u}'\hat{u}/(T-k)$, continua sendo n√£o viesado sob as premissas de que $u_i$ s√£o i.i.d. com m√©dia zero e vari√¢ncia $\sigma^2$ [^1]. No entanto, sob condi√ß√µes mais gerais, este estimador pode n√£o ser consistente.

Para verificar a consist√™ncia de $s^2$, come√ßamos analisando a soma dos quadrados dos res√≠duos populacionais, que pode ser escrita como:
$$ (y - X\beta)'(y - X\beta) = (y - Xb + Xb - X\beta)'(y - Xb + Xb - X\beta) = (y - Xb)'(y-Xb) + (Xb - X\beta)'(Xb - X\beta) $$
$$= (y - Xb)'(y-Xb) + (b - \beta)'X'X(b - \beta) $$
Dividindo por $T$ e usando o fato que $(y-Xb)'X = 0$ (ortogonalidade dos res√≠duos OLS) [^1], obtemos
$$ \frac{1}{T}(y - X\beta)'(y-X\beta) = \frac{1}{T}(y-Xb)'(y-Xb) + \frac{1}{T}(b-\beta)'X'X(b-\beta)  $$
ou
$$ \frac{1}{T}(y - Xb)'(y-Xb) = \frac{1}{T}u'u - (b - \beta)'\frac{X'X}{T}(b - \beta). $$
Onde $(1/T)(u'u) = (1/T) \sum_{t=1}^T u_t^2$ e a sequ√™ncia $\{u_t^2\}$ √© i.i.d. com m√©dia $\sigma^2$. Pela lei dos grandes n√∫meros, $(1/T) \sum_{t=1}^T u_t^2 \xrightarrow{p} \sigma^2$.
Para o segundo termo, temos que $(1/T)X'X \xrightarrow{p} Q$ e $(b-\beta) \xrightarrow{p} 0$ onde $Q$ √© uma matriz definida positiva [^1]. Logo, o segundo termo converge em probabilidade a zero:
$$ (b-\beta)'\frac{X'X}{T}(b-\beta) \xrightarrow{p} 0'Q0 = 0. $$
Portanto, conclu√≠mos que
$$ \frac{1}{T}(y - Xb)'(y-Xb) = \frac{1}{T}\hat{u}'\hat{u} \xrightarrow{p} \sigma^2 $$
O estimador OLS para a vari√¢ncia, definido em [8.1.18] [^1], √© dado por $s^2 = \frac{\hat{u}'\hat{u}}{T-k}$. Note que
$$ s^2 = \frac{T}{T-k}\frac{\hat{u}'\hat{u}}{T} $$
Definindo $a_T = \frac{T}{T-k}$, sabemos que $\lim_{T \to \infty} a_T = 1$. Portanto, por regras de limites de probabilidade, temos
$$  s^2 = a_T\frac{\hat{u}'\hat{u}}{T} \xrightarrow{p} 1\cdot\sigma^2 = \sigma^2  $$
Logo, $s^2$ √© um estimador consistente para $\sigma^2$. Esta an√°lise demonstra que mesmo com premissas mais gerais que as cl√°ssicas, como quando X √© estoc√°stica e independente de u, o estimador OLS para a vari√¢ncia continua consistente [^1].

> üí° **Exemplo Num√©rico:** Considere um modelo de regress√£o simples com uma vari√°vel explicativa e um intercepto, onde a verdadeira vari√¢ncia do erro √© $\sigma^2 = 4$. Vamos simular um conjunto de dados com $T = 100$ observa√ß√µes, onde $y = 2 + 3x + u$, com $u \sim N(0, 4)$. Podemos calcular o estimador $s^2$ e verificar sua converg√™ncia para $\sigma^2$ ao aumentar o n√∫mero de observa√ß√µes.
```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

np.random.seed(42)
T = 100
x = np.random.rand(T) * 10  # Gera valores de x
u = np.random.normal(0, 2, T) # Erros com sigma=2, variancia=4
y = 2 + 3 * x + u
X = np.column_stack((np.ones(T), x))
model = LinearRegression(fit_intercept=False) # O intercepto j√° foi adicionado na matriz X
model.fit(X, y)
y_pred = model.predict(X)
residuals = y - y_pred
rss = np.sum(residuals**2)
k=2 # N√∫mero de par√¢metros (intercepto e inclina√ß√£o)
s2 = rss / (T - k)
print(f"Estimativa s¬≤ (T=100): {s2:.4f}")

# Verificar a consist√™ncia aumentando o tamanho da amostra
sample_sizes = [100, 500, 1000, 5000]
s2_values = []

for t_size in sample_sizes:
    x = np.random.rand(t_size) * 10
    u = np.random.normal(0, 2, t_size)
    y = 2 + 3 * x + u
    X = np.column_stack((np.ones(t_size), x))
    model = LinearRegression(fit_intercept=False)
    model.fit(X, y)
    y_pred = model.predict(X)
    residuals = y - y_pred
    rss = np.sum(residuals**2)
    s2 = rss / (t_size - k)
    s2_values.append(s2)

df_results = pd.DataFrame({'Tamanho da Amostra': sample_sizes, 's¬≤': s2_values})
print("\nConsist√™ncia da estimativa s¬≤:")
print(df_results)
```
Este c√≥digo demonstra que, √† medida que o tamanho da amostra aumenta, a estimativa da vari√¢ncia $s^2$ se aproxima da verdadeira vari√¢ncia populacional ($\sigma^2 = 4$).

**Proposi√ß√£o 1** (Consist√™ncia de $s^2$ sob erros heterosced√°sticos):
A consist√™ncia do estimador $s^2$ se mant√©m mesmo sob heteroscedasticidade, desde que $E(u_t^2) = \sigma_t^2$, onde $\sigma_t^2$ pode variar com $t$, mas existe uma $\sigma^2$ tal que $\frac{1}{T}\sum_{t=1}^{T}\sigma_t^2 \xrightarrow{p} \sigma^2$ e que $\frac{1}{T}\sum_{t=1}^{T} u_t^2  \xrightarrow{p}  \frac{1}{T}\sum_{t=1}^{T}\sigma_t^2 $. Isso implica que $s^2$ converge para a m√©dia das vari√¢ncias condicionais.

*Prova:*
Provaremos que sob as condi√ß√µes fornecidas, $s^2$ √© um estimador consistente para $\sigma^2$.

I. Sob heteroscedasticidade, temos que $E(u_t u_s) = 0$ para $t \neq s$ e $E(u_t^2) = \sigma_t^2$, e $E(u_t) = 0$.

II. O termo $\frac{1}{T}u'u = \frac{1}{T}\sum_{t=1}^T u_t^2$ converge em probabilidade para $\frac{1}{T}\sum_{t=1}^T \sigma_t^2$ pela lei dos grandes n√∫meros, dado que $u_t^2$ s√£o independentes.

III. Assumimos que $\frac{1}{T}\sum_{t=1}^T \sigma_t^2 \xrightarrow{p} \sigma^2$, onde  $\sigma^2$  √© um valor fixo.

IV. Portanto, $\frac{1}{T}u'u \xrightarrow{p} \sigma^2$.

V. Como demonstrado anteriormente, $\frac{1}{T}\hat{u}'\hat{u} \xrightarrow{p} \sigma^2$.

VI. E tamb√©m temos que $s^2 = \frac{T}{T-k}\frac{\hat{u}'\hat{u}}{T}$ e  $\frac{T}{T-k} \xrightarrow{p} 1$.

VII. Portanto, pela propriedade do limite de probabilidade, conclu√≠mos que $s^2 \xrightarrow{p} \sigma^2$. ‚ñ†

Esta proposi√ß√£o generaliza o resultado de consist√™ncia para o caso de heteroscedasticidade, onde a vari√¢ncia dos erros pode variar ao longo das observa√ß√µes, mas com a m√©dia dessas vari√¢ncias convergindo para um valor constante.

> üí° **Exemplo Num√©rico:**  Para ilustrar a consist√™ncia de $s^2$ sob heteroscedasticidade, vamos modificar o exemplo anterior, onde a vari√¢ncia do erro agora depende de $x$. Suponha que $\sigma_t^2 = 0.5 + 0.1x_t^2$. O c√≥digo a seguir gera os dados e calcula $s^2$:
```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

np.random.seed(42)
T = 100
x = np.random.rand(T) * 10
sigma_t = np.sqrt(0.5 + 0.1 * x**2)  # Desvio padr√£o dependente de x
u = np.random.normal(0, sigma_t, T)
y = 2 + 3 * x + u
X = np.column_stack((np.ones(T), x))
model = LinearRegression(fit_intercept=False)
model.fit(X, y)
y_pred = model.predict(X)
residuals = y - y_pred
rss = np.sum(residuals**2)
k=2
s2 = rss / (T - k)

print(f"Estimativa s¬≤ (T=100, heteroscedasticidade): {s2:.4f}")

# Verificar a consist√™ncia aumentando o tamanho da amostra
sample_sizes = [100, 500, 1000, 5000]
s2_values = []
for t_size in sample_sizes:
    x = np.random.rand(t_size) * 10
    sigma_t = np.sqrt(0.5 + 0.1 * x**2)
    u = np.random.normal(0, sigma_t, t_size)
    y = 2 + 3 * x + u
    X = np.column_stack((np.ones(t_size), x))
    model = LinearRegression(fit_intercept=False)
    model.fit(X, y)
    y_pred = model.predict(X)
    residuals = y - y_pred
    rss = np.sum(residuals**2)
    s2 = rss / (t_size - k)
    s2_values.append(s2)

df_results = pd.DataFrame({'Tamanho da Amostra': sample_sizes, 's¬≤': s2_values})
print("\nConsist√™ncia da estimativa s¬≤ (heteroscedasticidade):")
print(df_results)
```
Este exemplo demonstra que mesmo com heteroscedasticidade, $s^2$ ainda converge para um valor, que neste caso, √© a m√©dia das vari√¢ncias condicionais, como demonstrado na proposi√ß√£o.

### Distribui√ß√£o Assint√≥tica da Estimativa da Vari√¢ncia

Embora a consist√™ncia de $s^2$ seja uma propriedade fundamental, para infer√™ncias estat√≠sticas precisas, precisamos conhecer sua **distribui√ß√£o assint√≥tica**. Especificamente, queremos saber para qual distribui√ß√£o converge a distribui√ß√£o de $s^2$ quando o tamanho da amostra $T$ tende ao infinito.

Para isso, analisamos $\sqrt{T}(s^2 - \sigma^2)$. Partindo de [8.2.11] [^1], temos que
$$ \frac{1}{T}(y - X\beta)'(y-X\beta) =  \frac{1}{T}\hat{u}'\hat{u} + (b - \beta)'\frac{X'X}{T}(b - \beta). $$
Como $\hat{u} = y - Xb = y - X\beta - X(b - \beta)$, podemos expressar
$$ \frac{1}{T}\hat{u}'\hat{u} = \frac{1}{T} u'u - (b-\beta)'\frac{X'X}{T}(b-\beta). $$
Multiplicando por $\sqrt{T}$ e subtraindo $\sqrt{T}\sigma^2$, temos
$$ \sqrt{T}(\frac{1}{T}\hat{u}'\hat{u} - \sigma^2) =  \sqrt{T}(\frac{1}{T}u'u - \sigma^2) - \sqrt{T}(b - \beta)'\frac{X'X}{T}(b - \beta) . $$
Como j√° discutido, o segundo termo converge em probabilidade a zero. Ent√£o,
$$ \sqrt{T}(\frac{1}{T}\hat{u}'\hat{u} - \sigma^2) \approx \sqrt{T}(\frac{1}{T}u'u - \sigma^2) =  \frac{1}{\sqrt{T}}\sum_{t=1}^T (u_t^2 - \sigma^2). $$

Onde  $\{u_t^2 - \sigma^2\}$ √© uma sequ√™ncia de vari√°veis aleat√≥rias i.i.d. com m√©dia zero e vari√¢ncia $\mu_4 - \sigma^4$, onde $\mu_4 = E(u_t^4)$. Pelo teorema central do limite, temos que
$$ \frac{1}{\sqrt{T}}\sum_{t=1}^T (u_t^2 - \sigma^2) \xrightarrow{L} N(0, \mu_4 - \sigma^4). $$
Portanto
$$ \sqrt{T}(\frac{1}{T}\hat{u}'\hat{u} - \sigma^2) \xrightarrow{L} N(0, \mu_4 - \sigma^4). $$
Agora, lembrando que  $s^2 = \frac{T}{T-k}\frac{\hat{u}'\hat{u}}{T}$ e que $\lim_{T \to \infty}\frac{T}{T-k} = 1$,  temos que
$$ \sqrt{T}(s^2 - \sigma^2) = \sqrt{T}(\frac{T}{T-k}\frac{\hat{u}'\hat{u}}{T} - \sigma^2) \approx \sqrt{T}(\frac{\hat{u}'\hat{u}}{T} - \sigma^2) \xrightarrow{L} N(0, \mu_4 - \sigma^4). $$
Este resultado mostra que a distribui√ß√£o assint√≥tica de $s^2$ √© dada por
$$ \sqrt{T}(s^2 - \sigma^2) \xrightarrow{L} N(0, \mu_4 - \sigma^4). $$

Onde $\mu_4$ √© o quarto momento central dos res√≠duos $u_t$. Este resultado, al√©m de estabelecer a distribui√ß√£o assint√≥tica da estimativa de vari√¢ncia, fornece uma justificativa para o uso de testes estat√≠sticos, como o teste *t*, que dependem da estimativa de vari√¢ncia. Em particular, demonstra que a estat√≠stica $\sqrt{T}(\frac{s^2 - \sigma^2}{\sqrt{\mu_4 - \sigma^4}})$ converge para uma distribui√ß√£o normal padr√£o.

> üí° **Exemplo Num√©rico:**  Vamos simular dados com erros normalmente distribu√≠dos e calcular a distribui√ß√£o assint√≥tica de $s^2$. Vamos usar os mesmos dados do primeiro exemplo num√©rico, onde temos um modelo com erros homoced√°sticos. Vamos estimar $\mu_4$ a partir dos dados.
```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import scipy.stats as st

np.random.seed(42)
T = 5000
x = np.random.rand(T) * 10
u = np.random.normal(0, 2, T)
y = 2 + 3 * x + u
X = np.column_stack((np.ones(T), x))

model = LinearRegression(fit_intercept=False)
model.fit(X, y)
y_pred = model.predict(X)
residuals = y - y_pred
rss = np.sum(residuals**2)
k = 2
s2 = rss / (T - k)
sigma2 = 4
mu4 = np.mean(residuals**4) # Estimativa do quarto momento dos erros
variance_s2_asymptotic = mu4 - sigma2**2
print(f"Estimativa s¬≤: {s2:.4f}")
print(f"Estimativa do quarto momento dos res√≠duos (mu4): {mu4:.4f}")
print(f"Vari√¢ncia assint√≥tica de sqrt(T)(s¬≤ - sigma¬≤): {variance_s2_asymptotic:.4f}")

# Simula√ß√£o para verificar a distribui√ß√£o assint√≥tica
num_simulations = 1000
s2_simulations = []

for _ in range(num_simulations):
    x = np.random.rand(T) * 10
    u = np.random.normal(0, 2, T)
    y = 2 + 3 * x + u
    X = np.column_stack((np.ones(T), x))
    model = LinearRegression(fit_intercept=False)
    model.fit(X, y)
    y_pred = model.predict(X)
    residuals = y - y_pred
    rss = np.sum(residuals**2)
    s2_sim = rss / (T - k)
    s2_simulations.append(s2_sim)

s2_diff = np.sqrt(T) * (np.array(s2_simulations) - sigma2)
mean_s2_diff = np.mean(s2_diff)
std_s2_diff = np.std(s2_diff)


print(f"M√©dia da diferen√ßa simulada: {mean_s2_diff:.4f}")
print(f"Desvio padr√£o da diferen√ßa simulada: {std_s2_diff:.4f}")

# Compara√ß√£o com a distribui√ß√£o normal te√≥rica
x_axis = np.linspace(mean_s2_diff - 3*std_s2_diff, mean_s2_diff + 3*std_s2_diff, 100)
pdf_theoretical = st.norm.pdf(x_axis, mean_s2_diff, np.sqrt(variance_s2_asymptotic))

plt.hist(s2_diff, bins=30, density=True, alpha=0.6, label='Simula√ß√µes')
plt.plot(x_axis, pdf_theoretical, 'r', label='Normal Te√≥rica')
plt.xlabel('sqrt(T)(s¬≤ - sigma¬≤)')
plt.ylabel('Densidade')
plt.title('Distribui√ß√£o Assint√≥tica da Estimativa de Vari√¢ncia')
plt.legend()
plt.show()
```

Este c√≥digo simula v√°rias amostras e calcula a diferen√ßa entre $s^2$ e $\sigma^2$, multiplicada por $\sqrt{T}$, plotando o histograma das simula√ß√µes e comparando com a distribui√ß√£o normal te√≥rica. Podemos ver que a distribui√ß√£o simulada se aproxima da normal te√≥rica com m√©dia zero e vari√¢ncia $\mu_4 - \sigma^4$

**Teorema 1** (Distribui√ß√£o Assint√≥tica de $s^2$ sob Heteroscedasticidade):
Sob a condi√ß√£o de heteroscedasticidade descrita na Proposi√ß√£o 1, a distribui√ß√£o assint√≥tica de $s^2$ √© dada por:

$$ \sqrt{T}(s^2 - \sigma^2) \xrightarrow{L} N(0, \lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^T (\mu_{4t} - \sigma_t^4) ) $$

onde $\mu_{4t} = E(u_t^4)$ e $\sigma_t^2 = E(u_t^2)$.

*Prova:*
Provaremos que sob heteroscedasticidade, a distribui√ß√£o assint√≥tica de $s^2$ √© dada pela express√£o fornecida.

I. Sob heteroscedasticidade, $E(u_t^2) = \sigma_t^2$, ent√£o, a prova segue de maneira an√°loga ao caso homosced√°stico, com a diferen√ßa que $E(u_t^2 - \sigma_t^2)^2 = \mu_{4t} - \sigma_t^4$, onde $\mu_{4t} = E(u_t^4)$.

II. Expandindo a express√£o, temos:
$$ \sqrt{T}(\frac{1}{T}u'u - \sigma^2) = \sqrt{T}(\frac{1}{T}\sum_{t=1}^T u_t^2 - \frac{1}{T}\sum_{t=1}^T \sigma_t^2 ) = \frac{1}{\sqrt{T}}\sum_{t=1}^T (u_t^2 - \sigma_t^2) $$

III. Pelo Teorema Central do Limite para vari√°veis independentes,
$$\frac{1}{\sqrt{T}}\sum_{t=1}^T (u_t^2 - \sigma_t^2) \xrightarrow{L} N(0, \lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^T (\mu_{4t} - \sigma_t^4)) $$
j√° que $E(u_t^2 - \sigma_t^2) = 0$ e $Var(u_t^2 - \sigma_t^2) = E(u_t^2 - \sigma_t^2)^2 = \mu_{4t} - \sigma_t^4$.

IV. Como no caso homosced√°stico, temos que $\sqrt{T}(s^2 - \sigma^2) \approx \sqrt{T}(\frac{1}{T}\hat{u}'\hat{u} - \sigma^2)$.

V. Portanto, combinando os passos anteriores, obtemos:
$$ \sqrt{T}(s^2 - \sigma^2) \xrightarrow{L} N(0, \lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^T (\mu_{4t} - \sigma_t^4) ) $$ ‚ñ†
Este resultado generaliza a distribui√ß√£o assint√≥tica para o caso de heteroscedasticidade, demonstrando que a vari√¢ncia assint√≥tica √© a m√©dia das vari√¢ncias dos $u_t^2$.

> üí° **Exemplo Num√©rico:** Para ilustrar a distribui√ß√£o assint√≥tica sob heteroscedasticidade, usamos novamente o exemplo do caso heteroced√°stico, e simulamos repetidas amostras para verificar a distribui√ß√£o de  $\sqrt{T}(s^2 - \sigma^2)$.

```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import scipy.stats as st


np.random.seed(42)
T = 5000
x = np.random.rand(T) * 10
sigma_t = np.sqrt(0.5 + 0.1 * x**2)
u = np.random.normal(0, sigma_t, T)
y = 2 + 3 * x + u
X = np.column_stack((np.ones(T), x))

model = LinearRegression(fit_intercept=False)
model.fit(X, y)
y_pred = model.predict(X)
residuals = y - y_pred
rss = np.sum(residuals**2)
k = 2
s2 = rss / (T - k)
sigma2_avg = np.mean(0.5 + 0.1*x**2) #M√©dia das vari√¢ncias condicionais
mu4_t = 3*(0.5 + 0.1 * x**2)**2 # Estimativa do quarto momento condicional para erros normais
mu4_avg = np.mean(mu4_t)
variance_s2_asymptotic = mu4_avg - sigma2_avg**2

print(f"Estimativa s¬≤: {s2:.4f}")
print(f"Estimativa da m√©dia das vari√¢ncias condicionais (sigma2_avg): {sigma2_avg:.4f}")
print(f"Estimativa do quarto momento dos res√≠duos (mu4_avg): {mu4_avg:.4f}")
print(f"Vari√¢ncia assint√≥tica de sqrt(T)(s¬≤ - sigma¬≤): {variance_s2_asymptotic:.4f}")


# Simula√ß√£o para verificar a distribui√ß√£o assint√≥tica
num_simulations = 1000
s2_simulations = []

for _ in range(num_simulations):
    x = np.random.rand(T) * 10
    sigma_t = np.sqrt(0.5 + 0.1 * x**2)
    u = np.random.normal(0, sigma_t, T)
    y = 2 + 3 * x + u
    X = np.column_stack((np.ones(T), x))
    model = LinearRegression(fit_intercept=False)
    model.fit(X, y)
    y_pred = model.predict(X)
    residuals = y - y_pred
    rss = np.sum(residuals**2)
    s2_sim = rss / (T - k)
    s2_simulations.append(s2_sim)

s2_diff = np.sqrt(T) * (np.array(s2_simulations) - sigma2_avg)

mean_s2_diff = np.mean(s2_diff)
std_s2_diff = np.std(s2_diff)

print(f"M√©dia da diferen√ßa simulada: {mean_s2_diff:.4f}")
print(f"Desvio padr√£o da diferen√ßa simulada: {std_s2_diff:.4f}")

# Compara√ß√£o com a distribui√ß√£o normal te√≥rica
x_axis = np.linspace(mean_s2_diff - 3*std_s2_diff, mean_s2_diff + 3*std_s2_diff, 100)
pdf_theoretical = st.norm.pdf(x_axis, mean_s2_diff, np.sqrt(variance_s2_asymptotic))

plt.hist(s2_diff, bins=30, density=True, alpha=0.6, label='Simula√ß√µes')
plt.plot(x_axis, pdf_theoretical, 'r', label='Normal Te√≥rica')
plt.xlabel('sqrt(T)(s¬≤ - sigma¬≤)')
plt.ylabel('Densidade')
plt.title('Distribui√ß√£o Assint√≥tica da Estimativa de Vari√¢ncia sob Heteroscedasticidade')
plt.legend()
plt.show()
```
Este exemplo demonstra que, mesmo sob heteroscedasticidade, a distribui√ß√£o de $\sqrt{T}(s^2 - \sigma^2)$ se aproxima de uma distribui√ß√£o normal, com a vari√¢ncia dada pela m√©dia dos momentos $\mu_{4t} - \sigma_t^4$, conforme o teorema.

### Conclus√£o
Neste cap√≠tulo, aprofundamos a an√°lise da **consist√™ncia** e da **distribui√ß√£o assint√≥tica** da **estimativa da vari√¢ncia** no contexto de modelos de **regress√£o linear**. Vimos que mesmo sob premissas mais gerais do que as cl√°ssicas, o estimador OLS para a vari√¢ncia $s^2$ √© **consistente**, ou seja, converge em probabilidade para a verdadeira vari√¢ncia $\sigma^2$. Al√©m disso, determinamos a **distribui√ß√£o assint√≥tica de $\sqrt{T}(s^2 - \sigma^2)$**, mostrando que esta converge para uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\mu_4 - \sigma^4$ no caso homosced√°stico e  $\lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^T (\mu_{4t} - \sigma_t^4)$ no caso heterosced√°stico. Esses resultados s√£o cruciais para a **infer√™ncia estat√≠stica**, em particular para a constru√ß√£o de **testes de hip√≥teses** e **intervalos de confian√ßa** v√°lidos em uma variedade de cen√°rios. Em cap√≠tulos subsequentes, utilizaremos esses resultados para avaliar a validade de infer√™ncias sob diferentes modelos e condi√ß√µes.

### Refer√™ncias
[^1]:  [8.1.12], [8.1.18], [8.1.10], [8.1.17], [8.2.11] do texto fornecido.
<!-- END -->
