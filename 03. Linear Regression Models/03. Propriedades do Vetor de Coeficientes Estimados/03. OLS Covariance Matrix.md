## Matriz de Covari√¢ncia do Estimador OLS e a Matriz de Informa√ß√£o de Fisher

### Introdu√ß√£o
Este cap√≠tulo expande o entendimento sobre as propriedades do estimador de M√≠nimos Quadrados Ordin√°rios (OLS), com foco na deriva√ß√£o detalhada da matriz de covari√¢ncia do estimador e sua conex√£o com a matriz de informa√ß√£o de Fisher. Anteriormente, exploramos a n√£o-tendenciosidade do estimador OLS [^8.1.15], a estima√ß√£o da vari√¢ncia dos res√≠duos, e testes de hip√≥teses [^8.1.18, ^8.2.23]. Agora, vamos analisar a estrutura da variabilidade dos coeficientes estimados, o que √© crucial para infer√™ncias estat√≠sticas e constru√ß√£o de intervalos de confian√ßa.

### Conceitos Fundamentais
A matriz de covari√¢ncia do estimador OLS, denotada por $Var(b)$, quantifica a variabilidade conjunta dos coeficientes estimados. Matematicamente, √© definida como a esperan√ßa do produto externo dos desvios do estimador em rela√ß√£o ao par√¢metro populacional:

$$ Var(b) = E[(b-\beta)(b-\beta)'] $$ [^8.1.16]

Utilizando a express√£o para o estimador OLS  $b = \beta + (X'X)^{-1}X'u$ [^8.1.12], podemos reescrever a equa√ß√£o acima como:

$$ Var(b) = E\left[\left((X'X)^{-1}X'u\right)\left((X'X)^{-1}X'u\right)'\right] $$

$$ Var(b) = E\left[(X'X)^{-1}X'uu'X(X'X)^{-1}\right] $$

Sob a premissa de que $E(uu') = \sigma^2 I_T$ [^8.1.15], onde $I_T$ √© uma matriz identidade de dimens√£o T, a matriz de covari√¢ncia √© dada por:
$$ Var(b) = (X'X)^{-1}X'E(uu')X(X'X)^{-1} = \sigma^2 (X'X)^{-1}X'X(X'X)^{-1} $$
$$ Var(b) = \sigma^2(X'X)^{-1}  $$ [^8.1.16]

Essa express√£o representa a matriz de covari√¢ncia dos coeficientes estimados do modelo de regress√£o linear. Ela indica como a vari√¢ncia de cada estimativa e a covari√¢ncia entre os estimadores dos par√¢metros s√£o influenciados pela vari√¢ncia dos erros e pela matriz de vari√°veis explicativas. A matriz $(X'X)^{-1}$ √© uma medida da multicolinearidade das vari√°veis independentes, com valores maiores implicando em maiores vari√¢ncias nas estimativas.

*Matriz de Informa√ß√£o de Fisher*

A matriz de informa√ß√£o de Fisher, por sua vez, √© um conceito fundamental na teoria da estima√ß√£o. Ela quantifica a quantidade de informa√ß√£o que uma amostra aleat√≥ria fornece sobre um par√¢metro desconhecido. Em contextos de regress√£o, a matriz de informa√ß√£o de Fisher pode ser relacionada √† matriz de covari√¢ncia do estimador OLS sob certas premissas. No caso de um modelo de regress√£o com erros Gaussianos, ela √© dada por:
$$ I(\beta) = E\left[-\frac{\partial^2 \log L(\beta; y)}{\partial \beta \partial \beta'}\right] $$
onde $L(\beta; y)$ √© a fun√ß√£o de verossimilhan√ßa do modelo. Para o modelo de regress√£o linear com erros Gaussianos, a fun√ß√£o de verossimilhan√ßa pode ser escrita como:
$$ L(\beta; y) = \frac{1}{(2\pi \sigma^2)^{T/2}} \exp\left[-\frac{1}{2\sigma^2} (y-X\beta)'(y-X\beta)\right] $$
O logaritmo da fun√ß√£o de verossimilhan√ßa √©:
$$ \log L(\beta; y) = -\frac{T}{2}\log(2\pi \sigma^2) - \frac{1}{2\sigma^2} (y-X\beta)'(y-X\beta) $$
As derivadas da fun√ß√£o de log-verossimilhan√ßa em rela√ß√£o a $\beta$ s√£o:
$$ \frac{\partial \log L(\beta; y)}{\partial \beta} = \frac{1}{\sigma^2}X'(y-X\beta) $$
e
$$ \frac{\partial^2 \log L(\beta; y)}{\partial \beta \partial \beta'} = -\frac{1}{\sigma^2}X'X $$
Assim, a matriz de informa√ß√£o de Fisher √©:
$$ I(\beta) =  -\mathbb{E}\left[-\frac{1}{\sigma^2}X'X\right] = \frac{1}{\sigma^2} X'X$$

Comparando com a matriz de covari√¢ncia de $b$, vemos que a matriz de informa√ß√£o de Fisher √© inversamente proporcional √† matriz de covari√¢ncia do estimador:
$$ Var(b) =  \sigma^2(X'X)^{-1} = I(\beta)^{-1} $$
Essa rela√ß√£o √© uma manifesta√ß√£o do princ√≠pio da efici√™ncia na estima√ß√£o: o estimador OLS, sendo o estimador de m√°xima verossimilhan√ßa, atinge o limite de Cram√©r-Rao, tendo a menor vari√¢ncia poss√≠vel dentre os estimadores n√£o viesados.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um modelo de regress√£o simples para ilustrar essa rela√ß√£o. Suponha que temos dados sobre o n√∫mero de horas de estudo ($X$) e as notas obtidas em um teste ($y$).
>
> ```python
> import numpy as np
> import pandas as pd
>
> # Dados de exemplo
> data = {'horas_estudo': [2, 3, 4, 5, 6, 7],
>         'nota': [60, 65, 75, 78, 85, 90]}
> df = pd.DataFrame(data)
>
> # Matriz X com intercepto
> X = np.array(df['horas_estudo'])
> X = np.column_stack((np.ones(len(X)), X)) # Adiciona uma coluna de 1s para o intercepto
>
> # Vetor y
> y = np.array(df['nota'])
>
> # Calculando X'X
> XtX = X.T @ X
>
> # Calculando (X'X)^-1
> XtX_inv = np.linalg.inv(XtX)
>
> # Estimando os coeficientes com OLS (para estimar sigma^2)
> b = np.linalg.inv(X.T @ X) @ X.T @ y
>
> # Calculando os res√≠duos
> u = y - X @ b
>
> # Estimando sigma^2 (vari√¢ncia dos res√≠duos)
> sigma2_hat = (u.T @ u) / (len(y) - X.shape[1]) # T-k graus de liberdade
>
> # Calculando a matriz de covari√¢ncia de b
> cov_b = sigma2_hat * XtX_inv
>
> # Calculando a matriz de informa√ß√£o de Fisher
> fisher_info = (1/sigma2_hat) * XtX
>
> print("Matriz de Covari√¢ncia de b:\n", cov_b)
> print("\nMatriz de Informa√ß√£o de Fisher:\n", fisher_info)
> print("\nInversa da Matriz de Informa√ß√£o de Fisher:\n", np.linalg.inv(fisher_info))
>
> ```
>
> **Interpreta√ß√£o:**
>
> A matriz `cov_b` representa a variabilidade dos coeficientes estimados. A diagonal principal indica a vari√¢ncia de cada coeficiente (intercepto e inclina√ß√£o), enquanto os elementos fora da diagonal indicam a covari√¢ncia entre eles. A matriz `fisher_info` √© a matriz de informa√ß√£o de Fisher, que √© inversamente proporcional √† matriz de covari√¢ncia de `b`. Observe que a inversa da matriz de informa√ß√£o de Fisher √© aproximadamente igual √† matriz de covari√¢ncia, multiplicada por sigma^2. Este exemplo ilustra numericamente a rela√ß√£o te√≥rica entre as matrizes de covari√¢ncia do estimador OLS e a matriz de informa√ß√£o de Fisher.
>
> Para a matriz de covari√¢ncia `cov_b`, vamos calcular os desvios-padr√£o dos coeficientes e usar para construir um intervalo de confian√ßa de 95% para o par√¢metro de inclina√ß√£o $\beta_1$
> ```python
> import scipy.stats as st
> # Extraindo os desvios padr√£o da matriz de covari√¢ncia
> std_err_b = np.sqrt(np.diag(cov_b))
>
> # Intervalo de confian√ßa de 95% para o par√¢metro de inclina√ß√£o (beta_1)
> beta_1_mean = b[1] # A inclina√ß√£o corresponde ao segundo elemento do vetor b
> beta_1_std_err = std_err_b[1]
> confidence_interval = st.t.interval(0.95, len(y) - X.shape[1], loc=beta_1_mean, scale=beta_1_std_err)
>
> print(f"Intervalo de confian√ßa de 95% para o par√¢metro de inclina√ß√£o (beta_1): {confidence_interval}")
> ```
> O intervalo de confian√ßa nos permite quantificar a incerteza na estimativa do par√¢metro de inclina√ß√£o. Quanto maior a vari√¢ncia do estimador, maior o intervalo de confian√ßa, e maior a incerteza sobre o valor verdadeiro do par√¢metro.

**Lema 1** Se $\hat{\sigma}^2 = \frac{u'u}{T-k}$ √© o estimador n√£o-viesado da vari√¢ncia dos erros, ent√£o $E[\hat{\sigma}^2]=\sigma^2$.

*Proof:* Conforme demonstrado em [^8.1.18], o estimador de vari√¢ncia dos res√≠duos, $\hat{\sigma}^2$, √© n√£o viesado, ou seja, seu valor esperado coincide com a vari√¢ncia populacional dos erros. A demonstra√ß√£o envolve o uso da matriz de proje√ß√£o e da propriedade do tra√ßo de uma matriz.
‚ñ†

**Lema 1.1**  A vari√¢ncia do estimador $\hat{\sigma}^2$ √© dada por $Var(\hat{\sigma}^2) = \frac{2\sigma^4}{T-k}$ quando os erros s√£o normalmente distribu√≠dos.

*Proof:* 
I. Se os erros s√£o normalmente distribu√≠dos, ent√£o $(T-k)\hat{\sigma}^2/\sigma^2$ segue uma distribui√ß√£o $\chi^2$ com $T-k$ graus de liberdade.
II. A vari√¢ncia de uma vari√°vel $\chi^2$ com $n$ graus de liberdade √© $2n$. 
III. Portanto, $Var((T-k)\hat{\sigma}^2/\sigma^2) = 2(T-k)$.
IV. Usando a propriedade da vari√¢ncia de uma constante multiplicada por uma vari√°vel aleat√≥ria, temos que $Var(aX) = a^2 Var(X)$. Logo, 
$Var\left(\frac{(T-k)\hat{\sigma}^2}{\sigma^2}\right) = \frac{(T-k)^2}{\sigma^4} Var(\hat{\sigma}^2) = 2(T-k)$.
V. Resolvendo para $Var(\hat{\sigma}^2)$, obtemos $Var(\hat{\sigma}^2) = \frac{2\sigma^4}{T-k}$.
‚ñ†

*Conex√£o com a Qualidade do Ajuste*

A matriz de covari√¢ncia de $b$ n√£o s√≥ mede a precis√£o dos estimadores, mas tamb√©m se relaciona com a qualidade geral do ajuste do modelo. Observando a matriz $(X'X)^{-1}$, nota-se que quanto mais ortogonais forem as colunas de $X$ (ou seja, quanto menos multicolinearidade), menor ser√° a vari√¢ncia das estimativas. A multicolinearidade aumenta a vari√¢ncia, tornando as estimativas menos precisas e mais sens√≠veis a pequenas varia√ß√µes nos dados.

> üí° **Exemplo Num√©rico:**
>
> Para demonstrar o efeito da multicolinearidade, vamos modificar o exemplo anterior, adicionando uma vari√°vel que √© altamente correlacionada com a vari√°vel horas de estudo. Suponha que agora temos dados sobre horas de estudo (X1) e horas de revis√£o (X2), onde X2 = 0.9*X1 + ru√≠do, e queremos avaliar o efeito na matriz de covari√¢ncia.
>
> ```python
> import numpy as np
> import pandas as pd
>
> # Dados de exemplo
> data = {'horas_estudo': [2, 3, 4, 5, 6, 7],
>         'nota': [60, 65, 75, 78, 85, 90]}
> df = pd.DataFrame(data)
>
> # Matriz X original com intercepto
> X1 = np.array(df['horas_estudo'])
> X1 = np.column_stack((np.ones(len(X1)), X1))
>
> # Vetor y
> y = np.array(df['nota'])
>
> # Calculando (X'X)^-1 original
> XtX_inv_original = np.linalg.inv(X1.T @ X1)
>
> # Estimando sigma^2 original
> b_original = np.linalg.inv(X1.T @ X1) @ X1.T @ y
> u_original = y - X1 @ b_original
> sigma2_hat_original = (u_original.T @ u_original) / (len(y) - X1.shape[1])
>
> # Calculando a matriz de covari√¢ncia original
> cov_b_original = sigma2_hat_original * XtX_inv_original
>
> # Criando a nova coluna com multicolinearidade
> X2 = 0.9 * df['horas_estudo'] + np.random.normal(0,0.5, len(df['horas_estudo']))
>
> # Matriz X com multicolinearidade
> X_multi = np.column_stack((np.ones(len(X1)),df['horas_estudo'], X2 ))
>
> # Calculando (X'X)^-1 com multicolinearidade
> XtX_inv_multi = np.linalg.inv(X_multi.T @ X_multi)
>
> # Estimando sigma^2 com multicolinearidade
> b_multi = np.linalg.inv(X_multi.T @ X_multi) @ X_multi.T @ y
> u_multi = y - X_multi @ b_multi
> sigma2_hat_multi = (u_multi.T @ u_multi) / (len(y) - X_multi.shape[1])
>
> # Calculando a matriz de covari√¢ncia com multicolinearidade
> cov_b_multi = sigma2_hat_multi * XtX_inv_multi
>
> print("Matriz de Covari√¢ncia original:\n", cov_b_original)
> print("\nMatriz de Covari√¢ncia com Multicolinearidade:\n", cov_b_multi)
>
> ```
>
> **Interpreta√ß√£o:**
>
> Observe como a matriz de covari√¢ncia aumentou quando adicionamos uma vari√°vel que √© altamente correlacionada com a outra vari√°vel explicativa. As vari√¢ncias dos coeficientes (elementos da diagonal) aumentaram, indicando que as estimativas s√£o menos precisas devido √† multicolinearidade.

**Lema 3** Se os erros $u$ seguem uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\sigma^2$, ent√£o o estimador OLS $b$ √© tamb√©m normalmente distribu√≠do.

*Proof:*
I. Sabemos que $b = \beta + (X'X)^{-1}X'u$ [^8.1.12].
II. Aqui, $b$ √© uma transforma√ß√£o linear de $u$.
III. Como $u$ √© normalmente distribu√≠do, qualquer transforma√ß√£o linear de $u$ tamb√©m ser√° normalmente distribu√≠da.
IV. Portanto, $b$ tamb√©m √© normalmente distribu√≠do.
V. A m√©dia de $b$ √© $E(b) = \beta$.
VI. A vari√¢ncia de $b$ √© $Var(b) = \sigma^2(X'X)^{-1}$.
VII. Assim, conclu√≠mos que $b \sim \mathcal{N}(\beta, \sigma^2(X'X)^{-1})$.
‚ñ†

**Proposi√ß√£o 3** A matriz de informa√ß√£o de Fisher para o modelo de regress√£o linear √© dada por $I(\beta) = \frac{1}{\sigma^2} X'X$, sob a premissa que os erros sejam normalmente distribu√≠dos.

*Proof:*
I.  A matriz de informa√ß√£o de Fisher √© definida como $I(\beta) = E\left[-\frac{\partial^2 \log L(\beta; y)}{\partial \beta \partial \beta'}\right]$.
II.  Para um modelo de regress√£o linear com erros gaussianos, temos que  $\frac{\partial^2 \log L(\beta; y)}{\partial \beta \partial \beta'} = -\frac{1}{\sigma^2}X'X$.
III.  Tomando o valor esperado, obtemos $I(\beta) = -\mathbb{E}\left[-\frac{1}{\sigma^2}X'X\right]$.
IV. Como $X$ √© considerado n√£o estoc√°stico, podemos retirar da esperan√ßa e conclu√≠mos que $I(\beta) = \frac{1}{\sigma^2}X'X$
‚ñ†

**Proposi√ß√£o 3.1** Sob as condi√ß√µes de normalidade dos erros, o estimador OLS $b$ √© o estimador de m√°xima verossimilhan√ßa (MV).

*Proof:*
I. A fun√ß√£o de verossimilhan√ßa para um modelo linear com erros normais √©:
$$L(\beta; y) = \frac{1}{(2\pi \sigma^2)^{T/2}} \exp\left[-\frac{1}{2\sigma^2} (y-X\beta)'(y-X\beta)\right]$$
II. Maximizar $L(\beta; y)$ √© equivalente a minimizar $ (y-X\beta)'(y-X\beta)$, que √© a soma dos erros quadrados.
III. A condi√ß√£o de primeira ordem para minimizar essa soma √© exatamente a mesma para o estimador OLS:
$$\frac{\partial}{\partial \beta}(y-X\beta)'(y-X\beta) = -2X'(y-X\beta) = 0$$
IV. Como o estimador de m√°xima verossimilhan√ßa √© √∫nico, e essa √© a condi√ß√£o do OLS, conclu√≠mos que o estimador OLS √© tamb√©m o estimador de m√°xima verossimilhan√ßa sob as condi√ß√µes de normalidade dos erros.
‚ñ†

**Corol√°rio 3.2** O estimador OLS $b$ √© assintoticamente normal, mesmo que os erros n√£o sejam estritamente normais, desde que as condi√ß√µes de regularidade sejam atendidas.

*Proof:*
I.  O estimador OLS pode ser escrito como uma m√©dia amostral ponderada dos erros: $b = \beta + (X'X)^{-1}X'u$.
II.  O Teorema do Limite Central (TLC) afirma que a distribui√ß√£o da m√©dia de um n√∫mero grande de vari√°veis aleat√≥rias independentes e identicamente distribu√≠das (i.i.d.) converge para uma distribui√ß√£o normal, sob certas condi√ß√µes de regularidade.
III.  Embora os erros $u$ n√£o sejam necessariamente i.i.d., se as condi√ß√µes de regularidade do TLC para sequ√™ncias n√£o i.i.d. forem satisfeitas (como a exist√™ncia de momentos finitos e que a influ√™ncia de cada erro individual sobre a estimativa do coeficiente tenda a zero √† medida que o tamanho da amostra aumenta), ent√£o $(X'X)^{-1}X'u$ √© assintoticamente normal.
IV. Portanto, o estimador OLS $b$ converge em distribui√ß√£o para uma normal, mesmo que os erros n√£o sejam estritamente normais, desde que as condi√ß√µes de regularidade padr√£o sejam v√°lidas e a matriz $X$ n√£o seja estoc√°stica.
‚ñ†
### Conclus√£o
A matriz de covari√¢ncia do estimador OLS, expressa como $Var(b) = \sigma^2(X'X)^{-1}$, √© fundamental para a infer√™ncia estat√≠stica em modelos de regress√£o. Ela quantifica a precis√£o das estimativas dos coeficientes e as correla√ß√µes entre eles, e sua rela√ß√£o com a matriz de informa√ß√£o de Fisher refor√ßa a efici√™ncia do estimador OLS no contexto de modelos com erros Gaussianos.  Entender sua deriva√ß√£o e seus componentes √© crucial para uma an√°lise rigorosa de modelos de regress√£o linear e para a constru√ß√£o de infer√™ncias confi√°veis e testes de hip√≥teses.

### Refer√™ncias
[^8.1.12]: Express√£o para o estimador OLS.
[^8.1.15]:  Vetor de coeficientes estimados OLS √© n√£o viesado.
[^8.1.16]:  Deriva√ß√£o da matriz de covari√¢ncia do estimador OLS.
[^8.1.18]: Defini√ß√£o e representa√ß√£o do estimador de vari√¢ncia do OLS como fun√ß√£o da matriz de proje√ß√£o.
[^8.2.23]: Testes de hip√≥teses com restri√ß√µes n√£o lineares.
<!-- END -->
