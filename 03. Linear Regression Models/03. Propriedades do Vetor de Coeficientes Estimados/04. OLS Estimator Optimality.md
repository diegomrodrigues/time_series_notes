## Distribui√ß√£o e Otimalidade do Estimador OLS Sob Erros Gaussianos

### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise das propriedades do estimador de M√≠nimos Quadrados Ordin√°rios (OLS), focando na sua distribui√ß√£o sob a suposi√ß√£o de erros Gaussianos e na demonstra√ß√£o de sua otimalidade nesse contexto espec√≠fico. Em cap√≠tulos anteriores, exploramos a n√£o-tendenciosidade, a matriz de covari√¢ncia do estimador e o Teorema de Gauss-Markov, que estabelece sua otimalidade dentro da classe de estimadores lineares n√£o viesados [^8.1.15, ^8.1.16, ^8.2.23]. Agora, vamos examinar como a distribui√ß√£o gaussiana dos erros implica em resultados ainda mais fortes sobre a distribui√ß√£o e a efici√™ncia do estimador OLS.

### Conceitos Fundamentais

Sob a premissa adicional de que os res√≠duos $u_t$ s√£o independentes e identicamente distribu√≠dos (i.i.d) seguindo uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\sigma^2$ (ou seja, $u_t \sim N(0, \sigma^2)$), temos resultados mais fortes sobre a distribui√ß√£o do estimador OLS.  Relembrando que o estimador OLS √© dado por:
$$ b = (X'X)^{-1}X'y = \beta + (X'X)^{-1}X'u$$ [^8.1.12]

Como $b$ √© uma combina√ß√£o linear dos erros $u$, e os erros s√£o gaussianos, $b$ tamb√©m ser√° gaussianamente distribu√≠do. Isso √© uma consequ√™ncia de que transforma√ß√µes lineares de vari√°veis aleat√≥rias gaussianas tamb√©m resultam em vari√°veis aleat√≥rias gaussianas. A m√©dia e a vari√¢ncia do estimador OLS j√° foram estabelecidas como:

$$ E(b) = \beta $$ [^8.1.15]

$$ Var(b) = \sigma^2 (X'X)^{-1} $$ [^8.1.16]

Assim, sob a suposi√ß√£o de erros gaussianos, a distribui√ß√£o do estimador OLS pode ser caracterizada como:

$$ b \sim N(\beta, \sigma^2(X'X)^{-1}) $$ [^8.1.17]

Este resultado √© fundamental para testes de hip√≥teses e constru√ß√£o de intervalos de confian√ßa para os par√¢metros do modelo de regress√£o.

*Otimilidade do Estimador OLS com Erros Gaussianos*

Sob a premissa de erros Gaussianos, o estimador OLS n√£o √© apenas o melhor estimador linear n√£o viesado (BLUE), conforme estabelecido pelo Teorema de Gauss-Markov, mas tamb√©m o estimador n√£o viesado de vari√¢ncia m√≠nima dentro de *todos* os estimadores n√£o viesados (n√£o apenas lineares). Isso √© um resultado mais forte do que o estabelecido pelo teorema de Gauss-Markov. Para demonstrar esta propriedade, precisamos recorrer ao conceito de informa√ß√£o de Fisher e ao limite de Cram√©r-Rao.

A *matriz de informa√ß√£o de Fisher* (j√° introduzida no cap√≠tulo anterior) para o modelo de regress√£o linear com erros gaussianos √© dada por:

$$ I(\beta) = \frac{1}{\sigma^2} X'X$$

O *limite de Cram√©r-Rao* estabelece um limite inferior para a vari√¢ncia de qualquer estimador n√£o viesado de um par√¢metro.  De acordo com o limite de Cram√©r-Rao, a vari√¢ncia de qualquer estimador n√£o viesado $\hat{\beta}$ de $\beta$ deve satisfazer a seguinte desigualdade:

$$ Var(\hat{\beta}) \geq I(\beta)^{-1} = \sigma^2 (X'X)^{-1} $$

Como a vari√¢ncia do estimador OLS $b$ √© exatamente igual a $\sigma^2 (X'X)^{-1}$, ele atinge o limite de Cram√©r-Rao. Isso significa que, dentro de todos os estimadores n√£o viesados (lineares e n√£o lineares), o estimador OLS √© o estimador de vari√¢ncia m√≠nima. Em outras palavras, n√£o existe nenhum outro estimador n√£o viesado de $\beta$ que tenha uma vari√¢ncia menor do que o estimador OLS. Isso demonstra a otimalidade do estimador OLS sob a suposi√ß√£o de erros gaussianos. Este resultado √© mais forte do que o Teorema de Gauss-Markov, que apenas estabelece a otimalidade entre estimadores lineares n√£o viesados.

*Implica√ß√µes para Infer√™ncia Estat√≠stica*

A distribui√ß√£o gaussiana do estimador OLS sob erros gaussianos permite a constru√ß√£o de testes de hip√≥teses com distribui√ß√µes exatas. Como $b \sim N(\beta, \sigma^2(X'X)^{-1})$, podemos padronizar as estimativas de forma que sigam uma distribui√ß√£o normal padr√£o. Especificamente, sob a hip√≥tese nula de que $\beta_i = \beta_i^0$, onde $\beta_i$ √© o i-√©simo elemento de $\beta$ e $\beta_i^0$ √© um valor especificado, a estat√≠stica $t$ definida como:
$$ t = \frac{b_i - \beta_i^0}{\sqrt{s^2\xi^{ii}}} $$
segue uma distribui√ß√£o $t$ de Student com $T-k$ graus de liberdade [^8.1.26], onde $s^2$ √© o estimador n√£o viesado da vari√¢ncia dos res√≠duos e $\xi^{ii}$ √© o elemento na linha i e coluna i da matriz $(X'X)^{-1}$. Esta distribui√ß√£o t exata permite infer√™ncias precisas sobre cada par√¢metro do modelo.

> üí° **Exemplo Num√©rico:**
>
> Suponha um modelo de regress√£o simples com uma vari√°vel explicativa e um intercepto, onde o verdadeiro valor de $\beta$ √© $\beta = \begin{bmatrix} 2 \\ 3 \end{bmatrix}$, ou seja, $\beta_0=2$ e $\beta_1=3$. O erro tem distribui√ß√£o $u_t \sim N(0, 2^2)$. Temos $T=100$ observa√ß√µes, onde a vari√°vel explicativa $x_t$ s√£o valores aleat√≥rios entre 0 e 1.
>
> ```python
> import numpy as np
> import scipy.stats as st
>
> # Par√¢metros do modelo
> np.random.seed(42) # para reprodutibilidade
> T = 100
> X = np.column_stack((np.ones(T), np.random.rand(T))) # Matriz X com intercepto
> beta_true = np.array([2, 3]) # Valor verdadeiro de beta
> sigma_sq = 2**2 # Vari√¢ncia dos erros
>
> # Gerando erros aleat√≥rios
> u = np.random.normal(0, np.sqrt(sigma_sq), T)
>
> # Gerando a vari√°vel dependente
> y = X @ beta_true + u
>
> # Calculando o estimador OLS
> b = np.linalg.inv(X.T @ X) @ X.T @ y
>
> # Estimador da vari√¢ncia dos res√≠duos
> y_hat = X @ b
> u_hat = y - y_hat
> s_sq = (u_hat.T @ u_hat) / (T - X.shape[1])
>
> # Vari√¢ncia estimada do estimador OLS
> var_b = s_sq * np.linalg.inv(X.T @ X)
> std_err_b = np.sqrt(np.diag(var_b))
>
> # Imprimindo resultados
> print("Estimativa de beta:", b)
> print("Vari√¢ncia estimada de beta:", var_b)
> print("Desvio padr√£o dos estimadores:", std_err_b)
>
> # Calculando intervalo de confian√ßa de 95% para beta_1
> confidence_interval = st.t.interval(0.95, len(y) - X.shape[1], loc=b[1], scale=std_err_b[1])
> print(f"Intervalo de confian√ßa de 95% para beta_1: {confidence_interval}")
>
> # Teste de hip√≥tese para beta_1 = 3
> t_stat = (b[1] - 3) / std_err_b[1]
> p_value = 2 * (1 - st.t.cdf(np.abs(t_stat), T - X.shape[1]))
>
> print(f"Estat√≠stica t para beta_1 = 3: {t_stat}")
> print(f"P-valor para beta_1 = 3: {p_value}")
>
> ```
>
> **Interpreta√ß√£o:**
> O c√≥digo acima simula dados de um modelo de regress√£o com erros gaussianos, calcula o estimador OLS ($b$), sua matriz de covari√¢ncia, seus desvios padr√µes e um intervalo de confian√ßa para $\beta_1$. O intervalo de confian√ßa mostra que, com 95% de certeza, o verdadeiro valor de $\beta_1$ est√° entre os limites inferior e superior do intervalo calculado.  O teste de hip√≥tese avalia se $\beta_1$ √© estatisticamente diferente de 3. Um p-valor menor que 0.05 indica que rejeitamos a hip√≥tese nula.

O mesmo vale para testes conjuntos de hip√≥teses. Sob a hip√≥tese nula $R\beta = r$, onde $R$ √© uma matriz de restri√ß√µes e $r$ √© um vetor de constantes, a estat√≠stica $F$ definida como:

$$ F = \frac{(Rb - r)'(R(X'X)^{-1}R')^{-1}(Rb - r)/m}{s^2} $$ [^8.1.32]

segue uma distribui√ß√£o $F$ de Fisher com $m$ e $T-k$ graus de liberdade, onde $m$ √© o n√∫mero de restri√ß√µes imposta. Essas distribui√ß√µes exatas fornecem uma base s√≥lida para testes de hip√≥teses precisos em modelos de regress√£o linear sob a suposi√ß√£o de erros gaussianos.

> üí° **Exemplo Num√©rico:**
>
> Continuando o exemplo anterior, suponha que queremos testar a hip√≥tese conjunta de que $\beta_0 = 2$ e $\beta_1 = 3$.
>
> ```python
> import numpy as np
> import scipy.stats as st
>
> # Par√¢metros do modelo (mesmos do exemplo anterior)
> np.random.seed(42)
> T = 100
> X = np.column_stack((np.ones(T), np.random.rand(T)))
> beta_true = np.array([2, 3])
> sigma_sq = 2**2
> u = np.random.normal(0, np.sqrt(sigma_sq), T)
> y = X @ beta_true + u
>
> # Calculando o estimador OLS
> b = np.linalg.inv(X.T @ X) @ X.T @ y
>
> # Estimador da vari√¢ncia dos res√≠duos
> y_hat = X @ b
> u_hat = y - y_hat
> s_sq = (u_hat.T @ u_hat) / (T - X.shape[1])
>
> # Matriz de restri√ß√µes
> R = np.array([[1, 0], [0, 1]])
> r = np.array([2, 3])
>
> # Calculando a estat√≠stica F
> F_stat = ((R @ b - r).T @ np.linalg.inv(R @ np.linalg.inv(X.T @ X) @ R.T) @ (R @ b - r)) / (R.shape[0] * s_sq)
>
> # Calculando o p-valor
> p_value = 1 - st.f.cdf(F_stat, R.shape[0], T - X.shape[1])
>
> print(f"Estat√≠stica F: {F_stat}")
> print(f"P-valor: {p_value}")
>
> ```
>
> **Interpreta√ß√£o:**
>  Neste exemplo, a hip√≥tese nula √© que ambos os par√¢metros do modelo s√£o iguais aos seus valores verdadeiros.  O p-valor para o teste F indica a probabilidade de obtermos a estat√≠stica F observada ou um valor mais extremo, sob a hip√≥tese nula. Se esse p-valor for menor do que um n√≠vel de signific√¢ncia (por exemplo, 0.05), rejeitamos a hip√≥tese nula, o que significa que pelo menos um dos par√¢metros estimados √© estatisticamente diferente do valor especificado.

**Lema 1**  Se $u$ √© uma vari√°vel aleat√≥ria gaussiana com m√©dia $\mu$ e vari√¢ncia $\Sigma$, ent√£o qualquer transforma√ß√£o linear de $u$, dada por $Au+b$, √© tamb√©m uma vari√°vel aleat√≥ria gaussiana com m√©dia $A\mu+b$ e vari√¢ncia $A\Sigma A'$.

*Proof:* Verifique a prova em qualquer livro texto sobre distribui√ß√µes estat√≠sticas.
‚ñ†
**Lema 1.1** Se $u \sim N(0,\sigma^2I)$ e $A$ √© uma matriz de dimens√£o $k \times T$, ent√£o $Au \sim N(0, \sigma^2 AA')$.
*Proof:* 
I. Pelo Lema 1,  se $u$ √© gaussiano com m√©dia 0 e vari√¢ncia $\sigma^2I$, ent√£o $Au$ √© gaussiano.
II. A m√©dia de $Au$ √© $A(0)=0$.
III. A vari√¢ncia de $Au$ √© $A(\sigma^2I)A' = \sigma^2AA'$.
IV. Logo, $Au \sim N(0, \sigma^2 AA')$.
‚ñ†
**Lema 2** Se os erros $u$ s√£o i.i.d normais com m√©dia zero e vari√¢ncia $\sigma^2$, ent√£o o estimador OLS $b$ √© o estimador de m√°xima verossimilhan√ßa (MV).

*Proof:*
I. J√° mostramos que o estimador OLS √© o estimador que minimiza a soma dos erros ao quadrado.
II. Sob a hip√≥tese de normalidade dos erros, o estimador de MV √© aquele que maximiza a fun√ß√£o de log verossimilhan√ßa, ou seja, o estimador que minimiza a soma dos erros ao quadrado.
III. Como o estimador OLS minimiza essa soma, ele √© tamb√©m o estimador de m√°xima verossimilhan√ßa sob a hip√≥tese de normalidade.
IV. O estimador OLS √© tamb√©m o melhor estimador linear n√£o viesado (BLUE) segundo o Teorema de Gauss-Markov.
V. Como o estimador de MV tem vari√¢ncia m√≠nima dentre todos os estimadores n√£o viesados, conclu√≠mos que o estimador OLS, sob as condi√ß√µes de normalidade, tem vari√¢ncia m√≠nima dentro da classe de todos os estimadores n√£o viesados (lineares ou n√£o).
‚ñ†

**Lema 3** Se $X$ √© uma matriz de vari√°veis explicativas n√£o estoc√°sticas e $u \sim N(0, \sigma^2I)$, ent√£o o estimador OLS $b \sim N(\beta, \sigma^2(X'X)^{-1})$.

*Proof:*
I. Sabemos que $b = \beta + (X'X)^{-1}X'u$.
II. Pelo Lema 1, sabemos que uma combina√ß√£o linear de vari√°veis normais tamb√©m √© normal.
III. Como $u$ √© normal, $(X'X)^{-1}X'u$ √© normal e, portanto, $b$ tamb√©m √© normal.
IV. Calculamos $E(b)=\beta$ e $Var(b)=\sigma^2(X'X)^{-1}$ nos cap√≠tulos anteriores.
V. Logo, podemos concluir que $b \sim N(\beta, \sigma^2(X'X)^{-1})$.
‚ñ†

**Proposi√ß√£o 1** Sob a premissa de erros Gaussianos, o estimador OLS √© o estimador de vari√¢ncia m√≠nima dentro de todos os estimadores n√£o viesados, alcan√ßando o limite de Cram√©r-Rao.

*Proof:*
I. A matriz de informa√ß√£o de Fisher para o modelo de regress√£o linear com erros gaussianos √© $I(\beta) = \frac{1}{\sigma^2} X'X$.
II. O limite de Cram√©r-Rao estabelece que a vari√¢ncia de qualquer estimador n√£o viesado $\hat{\beta}$ de $\beta$ deve satisfazer a desigualdade $Var(\hat{\beta}) \geq I(\beta)^{-1} = \sigma^2 (X'X)^{-1}$.
III. A vari√¢ncia do estimador OLS √© $Var(b) = \sigma^2(X'X)^{-1}$.
IV. Como a vari√¢ncia do estimador OLS atinge o limite de Cram√©r-Rao, ele √© o estimador n√£o viesado de vari√¢ncia m√≠nima dentre todos os estimadores n√£o viesados.
‚ñ†
**Proposi√ß√£o 1.1** Se $u \sim N(0,\sigma^2 I)$ ent√£o $(y-X\beta)'(y-X\beta)/\sigma^2$ tem distribui√ß√£o $\chi^2$ com $T$ graus de liberdade, onde $T$ √© o n√∫mero de observa√ß√µes.
*Proof:*
I. Sabemos que $u=y-X\beta \sim N(0,\sigma^2I)$.
II. Definimos $z = u/\sigma \sim N(0,I)$.
III. Sabemos que $z'z = \frac{u'u}{\sigma^2} = \frac{(y-X\beta)'(y-X\beta)}{\sigma^2} $.
IV. Como $z_i \sim N(0,1)$, ent√£o $z_i^2 \sim \chi^2(1)$.
V. Como $z'z = \sum_{i=1}^T z_i^2$,  e as vari√°veis s√£o i.i.d, ent√£o $z'z \sim \chi^2(T)$.
VI. Logo, $(y-X\beta)'(y-X\beta)/\sigma^2 \sim \chi^2(T)$.
‚ñ†

### Conclus√£o

A suposi√ß√£o de erros gaussianos no modelo de regress√£o linear leva a resultados mais fortes do que aqueles estabelecidos pelo teorema de Gauss-Markov.  Sob essas condi√ß√µes, a distribui√ß√£o do estimador OLS √© tamb√©m gaussiana, e ele atinge o limite de Cram√©r-Rao, o que significa que ele √© o estimador n√£o viesado de vari√¢ncia m√≠nima dentro da classe de todos os estimadores n√£o viesados. Este resultado estabelece a otimalidade do estimador OLS em um sentido mais amplo do que o que havia sido estabelecido anteriormente com o Teorema de Gauss-Markov. Al√©m disso, as distribui√ß√µes exatas (t e F) das estat√≠sticas de teste tornam poss√≠vel a infer√™ncia estat√≠stica precisa sobre os par√¢metros do modelo. A compreens√£o desses resultados √© essencial para a aplica√ß√£o rigorosa do m√©todo OLS em contextos onde a normalidade dos erros √© uma premissa razo√°vel.

### Refer√™ncias
[^8.1.12]: Express√£o para o estimador OLS.
[^8.1.15]:  Vetor de coeficientes estimados OLS √© n√£o viesado.
[^8.1.16]:  Deriva√ß√£o da matriz de covari√¢ncia do estimador OLS.
[^8.1.17]: Distribui√ß√£o do estimador OLS sob a premissa de erros gaussianos
[^8.1.26]: Estat√≠stica t para teste de hip√≥teses.
[^8.1.32]: Estat√≠stica F para teste de hip√≥teses conjuntas
[^8.2.23]: Testes de hip√≥teses com restri√ß√µes n√£o lineares.
<!-- END -->
