## O Teorema de Gauss-Markov e a Otimalidade do Estimador OLS

### Introdu√ß√£o
Este cap√≠tulo se dedica a explorar o Teorema de Gauss-Markov, um pilar fundamental na teoria da regress√£o linear. Anteriormente, discutimos as propriedades da vari√¢ncia dos res√≠duos e testes de hip√≥teses, incluindo restri√ß√µes n√£o lineares [^8.1.18, ^8.2.23]. Expandindo sobre esses conhecimentos, o Teorema de Gauss-Markov estabelece a otimalidade do estimador de M√≠nimos Quadrados Ordin√°rios (OLS) dentro de uma classe espec√≠fica de estimadores, garantindo que, sob certas condi√ß√µes, o estimador OLS √© o melhor estimador linear n√£o viesado (BLUE).

### Conceitos Fundamentais

O Teorema de Gauss-Markov fornece uma justificativa te√≥rica forte para o uso do estimador OLS em modelos de regress√£o linear. O teorema afirma que, dadas as premissas cl√°ssicas do modelo de regress√£o, o estimador OLS √© o melhor estimador linear n√£o viesado (BLUE) para os par√¢metros do modelo. Especificamente, o teorema estabelece que:
1.  **Linearidade:** O estimador √© uma fun√ß√£o linear da vari√°vel dependente $y$.
2.  **N√£o Viesado:** O valor esperado do estimador √© igual ao valor verdadeiro do par√¢metro que ele est√° estimando, isto √©, $E(b) = \beta$.
3. **Melhor (Eficiente):** O estimador tem a menor vari√¢ncia poss√≠vel dentro da classe de estimadores lineares e n√£o viesados.

As premissas cl√°ssicas do modelo de regress√£o linear que s√£o necess√°rias para a validade do Teorema de Gauss-Markov s√£o:
1.  O modelo √© linear nos par√¢metros: $y = X\beta + u$.
2.  As vari√°veis explicativas ($X$) s√£o n√£o estoc√°sticas (fixas em amostras repetidas) e de posto completo (as colunas s√£o linearmente independentes).
3.  O valor esperado dos res√≠duos √© zero: $E(u) = 0$.
4.  Os res√≠duos s√£o homoced√°sticos e n√£o correlacionados: $E(uu') = \sigma^2 I_T$.

Para formalizar a ideia de otimalidade, vamos considerar um estimador alternativo de $\beta$, que chamaremos de $\hat{\beta}$, que tamb√©m seja linear e n√£o viesado:

$$\hat{\beta} = Cy$$

onde $C$ √© uma matriz de dimens√£o $(k \times T)$. A linearidade do estimador √© clara por ser uma transforma√ß√£o linear de $y$. Para que $\hat{\beta}$ seja n√£o viesado, temos:

$$E(\hat{\beta}) = E(Cy) = E(C(X\beta + u)) = CX\beta + CE(u) = CX\beta$$

Para que $E(\hat{\beta}) = \beta$, devemos ter $CX = I_k$. A vari√¢ncia do estimador $\hat{\beta}$ √© dada por:

$$Var(\hat{\beta}) = Var(Cy) = Var(C(X\beta + u)) = Var(Cu) = E[(Cu)(Cu)'] = CE(uu')C' = \sigma^2 CC'$$

J√° a vari√¢ncia do estimador OLS √© dada por:

$$Var(b) = \sigma^2(X'X)^{-1}$$ [^8.1.16]

O Teorema de Gauss-Markov estabelece que, a diferen√ßa entre a matriz de covari√¢ncia de qualquer estimador linear e n√£o viesado $\hat{\beta}$ e a matriz de covari√¢ncia do estimador OLS, $Var(\hat{\beta}) - Var(b)$, √© sempre uma matriz semidefinida positiva. Isso significa que as vari√¢ncias dos elementos de $\hat{\beta}$ s√£o sempre maiores ou iguais √†s vari√¢ncias dos elementos de $b$. Matematicamente,

$$Var(\hat{\beta}) - Var(b) =  \sigma^2 (CC' - (X'X)^{-1}) \geq 0$$

Para demonstrar essa propriedade, vamos reescrever a matriz $C$ da seguinte forma:

$$ C = (X'X)^{-1}X' + D $$

onde $D$ √© uma matriz qualquer que satisfaz $DX = 0$. Esta condi√ß√£o garante que $CX = (X'X)^{-1}X'X + DX = I$, satisfazendo a condi√ß√£o de n√£o vi√©s.
Assim,
$$ CC' = [(X'X)^{-1}X' + D][X(X'X)^{-1} + D'] = (X'X)^{-1}X'X(X'X)^{-1} + D(X'X)^{-1}X' + (X'X)^{-1}X'D' + DD' $$

$$ CC' = (X'X)^{-1} + DD' $$
Ent√£o
$$  Var(\hat{\beta}) = \sigma^2 CC' = \sigma^2 ( (X'X)^{-1} + DD' ) = \sigma^2(X'X)^{-1} + \sigma^2 DD'  =  Var(b) + \sigma^2 DD' $$
Logo,
$$ Var(\hat{\beta}) - Var(b) = \sigma^2 DD' $$

Como $DD'$ √© uma matriz semidefinida positiva, conclu√≠mos que a vari√¢ncia do estimador OLS √© sempre menor ou igual a vari√¢ncia de qualquer estimador linear n√£o viesado $\hat{\beta}$, ou seja, o estimador OLS √© BLUE.

**Lema 1** Uma matriz da forma $DD'$ √© sempre semidefinida positiva.

*Proof:* Para qualquer vetor $z$, temos que $z'(DD')z = (D'z)'(D'z) = ||D'z||^2 \geq 0$. Logo, $DD'$ √© semidefinida positiva.
‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar o Teorema de Gauss-Markov, vamos comparar a vari√¢ncia do estimador OLS com a de um estimador linear alternativo que n√£o seja o estimador OLS. Suponha que temos um modelo de regress√£o simples com um intercepto e um preditor.
>
> ```python
> import numpy as np
>
> # Dados de exemplo
> np.random.seed(42)
> T = 100
> X = np.column_stack((np.ones(T), np.random.rand(T)))
> beta_true = np.array([2, 3])
> sigma_sq = 2
> u = np.random.normal(0, np.sqrt(sigma_sq), T)
> y = X @ beta_true + u
>
> # Estimador OLS
> b_ols = np.linalg.inv(X.T @ X) @ X.T @ y
>
> # Calculando a matriz de covari√¢ncia do estimador OLS
> cov_b_ols = sigma_sq * np.linalg.inv(X.T @ X)
>
> # Estimador alternativo (linear e n√£o viesado)
> D = np.random.rand(2,100) # Matriz aleatoria com dimens√µes (k,T)
> C_alt = (np.linalg.inv(X.T @ X) @ X.T) +  D - D @ X @ np.linalg.inv(X.T @ X) @ X.T # Garante que DX = 0 e C_altX = I_k
>
> b_alt = C_alt @ y
>
> # Calculando a matriz de covari√¢ncia do estimador alternativo
> cov_b_alt = sigma_sq * C_alt @ C_alt.T
>
> # Diferen√ßa das matrizes de covari√¢ncia
> cov_diff = cov_b_alt - cov_b_ols
>
> print("Matriz de covari√¢ncia do OLS:\n", cov_b_ols)
> print("\nMatriz de covari√¢ncia do estimador alternativo:\n", cov_b_alt)
> print("\nDiferen√ßa (cov_b_alt - cov_b_ols):\n", cov_diff)
>
> # Verificar se a diferen√ßa √© semidefinida positiva
> eigen_values = np.linalg.eigvals(cov_diff)
> print("\nAutovalores da diferen√ßa:\n", eigen_values)
>
> if np.all(eigen_values >= -1e-10):  # Testar se todos s√£o maiores ou iguais a 0
>     print("\nA diferen√ßa √© semidefinida positiva, como esperado pelo Teorema de Gauss-Markov.")
> else:
>      print("\nA diferen√ßa n√£o √© semidefinida positiva, algo est√° errado.")
>
>
> # Extraindo as vari√¢ncias dos coeficientes
> var_b_ols = np.diag(cov_b_ols)
> var_b_alt = np.diag(cov_b_alt)
>
> print("\nVari√¢ncias do estimador OLS:", var_b_ols)
> print("\nVari√¢ncias do estimador alternativo:", var_b_alt)
>
> # Comparando a vari√¢ncia dos estimadores
> diff = var_b_alt - var_b_ols
> print("\nDiferen√ßa nas vari√¢ncias (alt - ols):", diff)
>
>
> ```
>
> **Interpreta√ß√£o:**
>
> No exemplo acima, vemos que a matriz `cov_diff` √© semidefinida positiva, indicando que as vari√¢ncias dos coeficientes estimados com o m√©todo alternativo n√£o podem ser menores do que as do estimador OLS. Isso demonstra o Teorema de Gauss-Markov numericamente.
>
> **C√°lculos Detalhados:**
> 1. **Dados Simulados:** Geramos um conjunto de dados simples para uma regress√£o linear com um intercepto e um preditor. Os valores de $\beta$ s√£o definidos como [2, 3] e adicionamos um erro aleat√≥rio com $\sigma^2 = 2$.
> 2. **Estimador OLS:** Calculamos $\hat{\beta}_{OLS}$ usando a f√≥rmula $\hat{\beta}_{OLS} = (X'X)^{-1}X'y$.
> 3. **Matriz de Covari√¢ncia OLS:** Calculamos $Var(\hat{\beta}_{OLS}) = \sigma^2(X'X)^{-1}$.
> 4. **Estimador Alternativo:** Constru√≠mos um estimador linear $\hat{\beta}_{alt} = Cy$ onde $C = (X'X)^{-1}X' + D$.
> 5. **Matriz de Covari√¢ncia Alternativo:** Calculamos $Var(\hat{\beta}_{alt}) = \sigma^2CC'$.
> 6. **Diferen√ßa de Vari√¢ncias:** Subtra√≠mos a matriz de covari√¢ncia do OLS da matriz de covari√¢ncia do estimador alternativo $Var(\hat{\beta}_{alt}) - Var(\hat{\beta}_{OLS})$ e verificamos que essa matriz diferen√ßa √© semidefinida positiva, garantindo que a vari√¢ncia do OLS √© menor ou igual √† vari√¢ncia de qualquer outro estimador linear n√£o viesado.
>
> O c√≥digo tamb√©m calcula as vari√¢ncias individuais dos coeficientes para ambos os estimadores, que demostra o Teorema de Gauss-Markov de forma pr√°tica.

**Lema 2**  Seja $b$ o estimador OLS do modelo de regress√£o linear e $\hat{\beta}$ um estimador linear n√£o viesado qualquer. Ent√£o $Var(\hat{\beta}) - Var(b)$ √© uma matriz semidefinida positiva.

*Proof:*
I.  Do Teorema de Gauss-Markov, sabemos que $Var(\hat{\beta}) - Var(b) = \sigma^2 DD'$.
II. Pelo Lema 1, sabemos que $DD'$ √© uma matriz semidefinida positiva.
III. Multiplicar uma matriz semidefinida positiva por um escalar positivo ($\sigma^2$) resulta em outra matriz semidefinida positiva.
IV. Portanto, $Var(\hat{\beta}) - Var(b) = \sigma^2 DD'$ √© uma matriz semidefinida positiva.
‚ñ†

**Proposi√ß√£o 3**  A condi√ß√£o $DX = 0$ na matriz $C$ √© uma condi√ß√£o necess√°ria e suficiente para que o estimador $\hat{\beta} = Cy$, onde $C = (X'X)^{-1}X' + D$, seja n√£o viesado.

*Proof:*
I.  J√° mostramos que se $DX=0$ ent√£o $E(\hat{\beta})=\beta$.
II. Para provar que a condi√ß√£o √© necess√°ria, suponha que $\hat{\beta}$ seja n√£o viesado, ent√£o $CX=I_k$.
III. Substituindo $C = (X'X)^{-1}X' + D$, temos $((X'X)^{-1}X'+D)X = (X'X)^{-1}X'X + DX = I_k + DX=I_k$.
IV. Isso implica que $DX=0$.
V. Portanto, a condi√ß√£o √© necess√°ria e suficiente.
‚ñ†

**Proposi√ß√£o 4** A condi√ß√£o $DX = 0$ na matriz $C$ garante que o estimador $\hat{\beta}$ seja n√£o viesado.

*Proof:*
I.  Conforme demonstrado anteriormente, $E(\hat{\beta}) = CX\beta$.
II. Se $C = (X'X)^{-1}X' + D$ e $DX = 0$, ent√£o $CX = (X'X)^{-1}X'X + DX = I + 0 = I$.
III. Logo $E(\hat{\beta}) = I\beta = \beta$.
‚ñ†

**Proposi√ß√£o 4.1**  Se os erros n√£o s√£o homoced√°sticos e n√£o correlacionados, o estimador OLS n√£o √© mais o BLUE, e o Teorema de Gauss-Markov n√£o se aplica.

*Proof:*
I. Se $E(uu') = \Omega \neq \sigma^2 I_T$, a demonstra√ß√£o de que $Var(\hat{\beta}) - Var(b)$ √© semidefinida positiva n√£o √© mais v√°lida, pois n√£o temos $E(uu')=\sigma^2 I$.
II. Nesse caso, o estimador OLS n√£o √© mais o estimador BLUE. O estimador de M√≠nimos Quadrados Generalizados (GLS) √© o estimador BLUE neste caso.
III. O estimador OLS continua sendo n√£o-viesado, mas n√£o √© mais eficiente na classe dos estimadores lineares n√£o-viesados, ou seja, deixa de ser BLUE.
‚ñ†

**Observa√ß√£o 1** A demonstra√ß√£o do Teorema de Gauss-Markov, ao utilizar a decomposi√ß√£o $C = (X'X)^{-1}X' + D$, n√£o depende da normalidade dos erros, apenas das premissas de linearidade, exogeneidade das vari√°veis, esperan√ßa condicional dos erros igual a zero e homocedasticidade e n√£o correla√ß√£o dos erros. Essa √© uma caracter√≠stica importante do teorema, indicando que a otimalidade do OLS n√£o requer a normalidade dos erros.

### Conclus√£o
O Teorema de Gauss-Markov √© um resultado crucial na teoria da regress√£o linear, pois garante que o estimador OLS √© o melhor estimador linear n√£o viesado (BLUE) sob as premissas cl√°ssicas do modelo. Este resultado fornece uma base te√≥rica s√≥lida para o uso do OLS em muitas aplica√ß√µes econom√©tricas. No entanto, √© fundamental estar ciente de que o Teorema de Gauss-Markov depende das premissas do modelo. Quando essas premissas n√£o s√£o v√°lidas, o estimador OLS pode n√£o ser o melhor estimador, e outros m√©todos de estima√ß√£o podem ser mais apropriados. A compreens√£o das condi√ß√µes sob as quais o teorema se aplica e suas limita√ß√µes √© essencial para uma an√°lise econom√©trica rigorosa.

### Refer√™ncias
[^8.1.15]:  Vetor de coeficientes estimados OLS √© n√£o viesado.
[^8.1.16]:  Deriva√ß√£o da matriz de covari√¢ncia do estimador OLS.
[^8.1.18]: Defini√ß√£o e representa√ß√£o do estimador de vari√¢ncia do OLS como fun√ß√£o da matriz de proje√ß√£o.
[^8.2.23]: Testes de hip√≥teses com restri√ß√µes n√£o lineares.
<!-- END -->
