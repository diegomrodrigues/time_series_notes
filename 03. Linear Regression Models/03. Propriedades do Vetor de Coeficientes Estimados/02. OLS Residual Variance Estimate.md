## Estimativa da Vari√¢ncia dos Res√≠duos OLS e a Matriz de Proje√ß√£o

### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise das propriedades do estimador de M√≠nimos Quadrados Ordin√°rios (OLS), focando na estimativa da vari√¢ncia dos res√≠duos. Anteriormente, estabelecemos as propriedades do vetor de coeficientes estimados sob diferentes conjuntos de premissas, culminando na discuss√£o da consist√™ncia e distribui√ß√£o assint√≥tica do estimador OLS [^8.1.15, ^8.2.9]. Agora, exploraremos como a vari√¢ncia dos res√≠duos √© estimada e como essa estimativa se relaciona com a matriz de proje√ß√£o.

### Conceitos Fundamentais

A estimativa da vari√¢ncia dos res√≠duos OLS, denotada por $s^2$, desempenha um papel crucial na infer√™ncia estat√≠stica. Sob as premissas cl√°ssicas de regress√£o, esta estimativa √© dada pela soma dos quadrados dos res√≠duos (RSS) dividida pelos graus de liberdade, ajustada para o n√∫mero de par√¢metros estimados no modelo [^8.1.18]. A formula√ß√£o matem√°tica para a estimativa da vari√¢ncia √© apresentada como:
$$ s^2 = \frac{RSS}{T-k} = \frac{\hat{u}'\hat{u}}{T-k} = \frac{u'M_X M_X u}{T-k} $$ [^8.1.18]

onde:
- $RSS$ representa a soma dos quadrados dos res√≠duos.
- $T$ √© o n√∫mero de observa√ß√µes.
- $k$ √© o n√∫mero de par√¢metros no modelo, incluindo o intercepto.
- $\hat{u}$ √© o vetor de res√≠duos OLS.
- $u$ √© o vetor de res√≠duos populacionais.
- $M_X$ √© a matriz de proje√ß√£o definida como $M_X = I - X(X'X)^{-1}X'$ [^8.1.8], que projeta um vetor no espa√ßo ortogonal ao espa√ßo coluna de $X$.

A matriz $M_X$ tem propriedades not√°veis: ela √© sim√©trica ($M_X = M_X'$) e idempotente ($M_X M_X = M_X$) [^8.1.18]. Essas propriedades s√£o fundamentais para derivar as caracter√≠sticas do estimador da vari√¢ncia.  Em particular, o fato de $M_X$ ser idempotente permite simplificar a express√£o para $s^2$, como mostrado a seguir:

$$s^2 = \frac{u'M_Xu}{T-k}$$ [^8.1.19]

Adicionalmente, por $M_X$ ser sim√©trica, existe uma matriz $P$ de dimens√£o $(T \times T)$ tal que $M_X = PAP'$, onde $A$ √© uma matriz diagonal contendo os autovalores de $M_X$ e $P'P=I_T$ [^8.1.20, ^8.1.21].

√â importante notar que $M_Xv = 0$ se $v$ √© um dos $k$ vetores colunas de $X$ [^8.1.9]. Assumindo que as colunas de $X$ s√£o linearmente independentes, ent√£o, os $k$ vetores colunas de $X$ representam $k$ autovetores distintos de $M_X$, cada um associado a um autovalor igual a zero. Em outras palavras, $k$ dos autovalores de $M_X$ s√£o iguais a zero. Os demais $T-k$ autovalores s√£o iguais a um [^8.1.20].

Com estas considera√ß√µes, podemos transformar $u'M_X u$ em uma soma de quadrados de vari√°veis n√£o correlacionadas. Definindo $w = P'u$, temos que $E(ww') = E(P'uu'P) = P'E(uu')P = \sigma^2 P'P = \sigma^2 I_T$ [^8.1.22]. Assim, os elementos de $w$ s√£o n√£o correlacionados, com m√©dia zero e vari√¢ncia $\sigma^2$. Ent√£o:

$$u'M_Xu = u'PAP'u = w'Aw = w_1^2 \lambda_1 + w_2^2 \lambda_2 + \ldots + w_T^2 \lambda_T$$ [^8.1.22]

Como $k$ autovalores s√£o zero e os restantes $T-k$ s√£o iguais a um, [^8.1.22] simplifica-se para:
$$u'M_Xu = w_1^2 + w_2^2 + \ldots + w_{T-k}^2$$ [^8.1.23]

Al√©m disso, cada $w_i^2$ tem esperan√ßa $\sigma^2$. Portanto:
$$ E(u'M_X u) = (T-k)\sigma^2$$ [^8.1.23]
Resultando em $E(s^2) = \sigma^2$, ou seja, $s^2$ √© um estimador n√£o viesado de $\sigma^2$ [^8.1.19].

Sob a premissa de que $u$ √© Gaussiano, os $w_i$ tamb√©m s√£o Gaussianos e a express√£o $u'M_Xu$ √© uma soma de quadrados de $T-k$ vari√°veis normais independentes com m√©dia zero e vari√¢ncia $\sigma^2$ [^8.1.23]. Isso implica que:
$$ \frac{RSS}{\sigma^2} = \frac{u'M_X u}{\sigma^2} \sim \chi^2(T-k) $$ [^8.1.24]

Ou seja, o $RSS$ escalonado por $\sigma^2$ segue uma distribui√ß√£o qui-quadrado com $T-k$ graus de liberdade. Isso √© crucial para testes de hip√≥teses sobre os par√¢metros da regress√£o.

Al√©m disso, sob as premissas cl√°ssicas, b e $\hat{u}$ s√£o n√£o correlacionados:
$$E[\hat{u}(b-\beta)'] = E[M_Xuu'X(X'X)^{-1}] = \sigma^2 M_XX(X'X)^{-1}=0$$ [^8.1.25]

Com ambos $b$ e $\hat{u}$ gaussianos, a aus√™ncia de correla√ß√£o implica independ√™ncia. Isso significa que $b$ e $s^2$ s√£o independentes, um resultado essencial para a constru√ß√£o de testes t e F.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo pr√°tico para ilustrar o c√°lculo de $s^2$ e a matriz de proje√ß√£o $M_X$. Suponha que temos um modelo de regress√£o simples com um intercepto e um preditor, com $T=5$ observa√ß√µes e, portanto, $k=2$ par√¢metros.
>
> **Dados:**
>
> ```python
> import numpy as np
>
> # Dados de exemplo para Y e X
> y = np.array([2, 4, 5, 4, 5])
> X = np.array([[1, 1], [1, 2], [1, 3], [1, 4], [1, 5]])  # Coluna de 1s para o intercepto e um preditor
> ```
>
> **C√°lculo de $M_X$:**
>
> ```python
> # Calcula X'X
> XtX = X.T @ X
>
> # Calcula (X'X)^-1
> XtX_inv = np.linalg.inv(XtX)
>
> # Calcula X(X'X)^-1X'
> X_XtX_inv_Xt = X @ XtX_inv @ X.T
>
> # Calcula Mx
> I = np.eye(X.shape[0])
> Mx = I - X_XtX_inv_Xt
>
> print("Matriz Mx:\n", Mx)
> ```
>
> **C√°lculo dos Res√≠duos e $s^2$:**
>
> ```python
> # Calcula os coeficientes beta
> beta = XtX_inv @ X.T @ y
>
> # Calcula os valores preditos
> y_hat = X @ beta
>
> # Calcula os res√≠duos
> u_hat = y - y_hat
>
> # Calcula RSS
> RSS = u_hat.T @ u_hat
>
> # Calcula s^2
> s2 = RSS / (X.shape[0] - X.shape[1])
>
> print("Res√≠duos:\n", u_hat)
> print("Estimativa da vari√¢ncia dos res√≠duos (s^2):", s2)
> ```
>
> **Interpreta√ß√£o:**
> A matriz $M_X$ √© uma matriz de proje√ß√£o que projeta o vetor de res√≠duos no espa√ßo ortogonal ao espa√ßo coluna de $X$. A estimativa da vari√¢ncia dos res√≠duos, $s^2$, √© uma medida da dispers√£o dos res√≠duos em torno da linha de regress√£o. Neste exemplo, $s^2$ foi calculado para um conjunto de dados simulados. O resultado $s^2$ √© um estimador n√£o viesado da verdadeira vari√¢ncia dos res√≠duos populacionais sob as premissas do modelo de regress√£o linear.

**Lema 1** A propriedade de idempot√™ncia da matriz $M_X$, ou seja, $M_XM_X=M_X$, implica que $M_X$ √© uma matriz de proje√ß√£o ortogonal.

*Proof:* A idempot√™ncia de $M_X$ √© dada por $M_X M_X = M_X$. Sendo $M_X$ tamb√©m sim√©trica, $M_X = M_X'$, ent√£o $M_X$ √© uma proje√ß√£o ortogonal no espa√ßo ortogonal ao espa√ßo coluna de $X$. Esta propriedade √© crucial para entender o efeito da transforma√ß√£o realizada por $M_X$ sobre os res√≠duos $u$.
‚ñ†

**Corol√°rio 1.1** A matriz $M_X$ anula qualquer vetor que pertence ao espa√ßo coluna de $X$.

*Proof:* Seja $v$ um vetor no espa√ßo coluna de $X$, ent√£o $v = Xa$ para algum vetor $a$. Assim,
$M_X v = (I - X(X'X)^{-1}X')Xa = Xa - X(X'X)^{-1}X'Xa = Xa - X(X'X)^{-1}(X'X)a = Xa - Xa = 0$. Logo, a matriz $M_X$ anula qualquer vetor que seja uma combina√ß√£o linear das colunas de $X$.
‚ñ†

**Proposi√ß√£o 1** A matriz $M_X$ √© semidefinida positiva.

*Proof:* Para qualquer vetor $z$, temos que $z'M_Xz = z'M_X'M_Xz = (M_X z)' (M_X z) = ||M_Xz||^2 \geq 0$. Assim, $M_X$ √© semidefinida positiva.
‚ñ†

**Teorema 1** (Teorema de Frisch-Waugh-Lovell) Dados os vetores $y$, $X$ e uma matriz $Z$, seja $\hat{\beta}$ o estimador OLS de $y$ em $X$ e $\hat{\gamma}$ o estimador OLS de $y$ em $[X, Z]$. Ent√£o o estimador OLS de $y$ em $M_XZ$, ou seja, $\gamma_{M_X Z}$ , √© igual a parte dos estimadores de $\hat{\gamma}$ correspondente a matriz $Z$.

*Proof:* 
I. O estimador OLS de $\gamma$ na regress√£o de $y$ em $[X, Z]$ √© dado por:
$$ \begin{bmatrix} \hat{\beta} \\ \hat{\gamma} \end{bmatrix} = \Big( \begin{bmatrix} X' \\ Z' \end{bmatrix} \begin{bmatrix} X & Z \end{bmatrix} \Big)^{-1} \begin{bmatrix} X' \\ Z' \end{bmatrix} y = \begin{bmatrix} X'X & X'Z \\ Z'X & Z'Z \end{bmatrix}^{-1} \begin{bmatrix} X'y \\ Z'y \end{bmatrix} $$

II. O estimador de $\gamma$ da regress√£o de $y$ em $M_X Z$ √©:
$$ \hat{\gamma}_{M_XZ} = (Z'M_X'M_XZ)^{-1}Z'M_X'y = (Z'M_XZ)^{-1}Z'M_Xy $$
III. Usando o resultado de invers√£o de matrizes particionadas, pode-se mostrar que o estimador de $\gamma$ √© o mesmo nas duas regress√µes acima.
IV. Seja $P = \begin{bmatrix} X'X & X'Z \\ Z'X & Z'Z \end{bmatrix}$ e $P^{-1} = \begin{bmatrix} A & B \\ C & D \end{bmatrix}$. Ent√£o, $\hat{\gamma} = C X'y + D Z'y$. Note que $D = (Z'M_X Z)^{-1}$.
V. Al√©m disso,  $C = -(Z'M_X Z)^{-1} Z'X (X'X)^{-1}$, e $\hat{\beta} = (X'X)^{-1}X'y-(X'X)^{-1} X'Z \hat{\gamma}$.
VI. Substituindo as express√µes acima, $\hat{\gamma}_{M_XZ} = (Z'M_XZ)^{-1}Z'M_Xy = (Z'M_XZ)^{-1}Z'(I - X(X'X)^{-1}X')y = (Z'M_XZ)^{-1}Z'y - (Z'M_XZ)^{-1}Z'X(X'X)^{-1}X'y$.
VII. Com um pouco de √°lgebra, √© poss√≠vel mostrar que $\hat{\gamma} = \hat{\gamma}_{M_XZ}$
‚ñ†

### Conclus√£o
A estimativa da vari√¢ncia dos res√≠duos OLS, $s^2$, desempenha um papel fundamental na infer√™ncia estat√≠stica. A conex√£o com a matriz de proje√ß√£o $M_X$, atrav√©s de sua idempot√™ncia e simetria, proporciona uma base te√≥rica para entender a distribui√ß√£o de $s^2$ e sua propriedade de n√£o vi√©s. A independ√™ncia entre $b$ e $s^2$ sob a premissa de Gaussianidade tamb√©m √© fundamental para realizar testes de hip√≥teses sobre os coeficientes de regress√£o. A compreens√£o detalhada dessas propriedades e deriva√ß√µes √© crucial para a aplica√ß√£o rigorosa da an√°lise de regress√£o linear em contextos acad√™micos e de pesquisa.

### Refer√™ncias
[^8.1.8]:  Defini√ß√£o da matriz de proje√ß√£o Mx.
[^8.1.9]: Propriedade da matriz de proje√ß√£o Mx quando aplicada a um vetor no espa√ßo coluna de X.
[^8.1.15]:  Vetor de coeficientes estimados OLS √© n√£o viesado.
[^8.1.18]: Defini√ß√£o e representa√ß√£o do estimador de vari√¢ncia do OLS como fun√ß√£o da matriz de proje√ß√£o.
[^8.1.19]:  Simplifica√ß√£o da estimativa da vari√¢ncia dos res√≠duos usando a propriedade de idempot√™ncia da matriz Mx.
[^8.1.20]:  Propriedade da matriz sim√©trica Mx e sua decomposi√ß√£o.
[^8.1.21]:  Propriedade da matriz P que auxilia na decomposi√ß√£o de Mx.
[^8.1.22]:  Transforma√ß√£o do res√≠duo u usando a matriz P e distribui√ß√£o resultante.
[^8.1.23]:  Simplifica√ß√£o e esperan√ßa da soma de quadrados dos res√≠duos transformados.
[^8.1.24]:  Distribui√ß√£o da soma de quadrados dos res√≠duos.
[^8.1.25]:  N√£o correla√ß√£o entre b e √ª sob a premissa de normalidade dos res√≠duos.
[^8.2.9]:  Distribui√ß√£o assint√≥tica do estimador OLS.
## Testes de Hip√≥teses com Restri√ß√µes N√£o Lineares
### Introdu√ß√£o
Em continuidade √† discuss√£o anterior sobre testes de hip√≥teses, vamos agora nos aprofundar nos testes que envolvem restri√ß√µes n√£o lineares sobre os par√¢metros do modelo.  Como vimos, testes como o teste *t* e o teste *F* s√£o ferramentas valiosas para avaliar hip√≥teses lineares. No entanto, muitas vezes, as hip√≥teses que desejamos testar n√£o se enquadram nessa estrutura linear. Essa se√ß√£o expande a an√°lise para contemplar essas situa√ß√µes.

### Conceitos Fundamentais
Conforme discutido anteriormente [^8.2.23], o teste de Wald √© uma ferramenta poderosa para testar hip√≥teses, e sua forma assint√≥tica pode ser generalizada para lidar com restri√ß√µes n√£o lineares. A ideia central √© aproximar as restri√ß√µes n√£o lineares por meio de uma expans√£o de Taylor de primeira ordem em torno do valor verdadeiro dos par√¢metros, permitindo a aplica√ß√£o de m√©todos assint√≥ticos para realizar infer√™ncias.

*Expans√£o da Restri√ß√£o N√£o Linear*

Seja a hip√≥tese nula definida por $g(\beta) = 0$, onde $g: \mathbb{R}^k \rightarrow \mathbb{R}^m$ representa $m$ restri√ß√µes n√£o lineares sobre o vetor de par√¢metros $\beta \in \mathbb{R}^k$. A expans√£o de Taylor de primeira ordem de $g(\beta_T)$ em torno do verdadeiro valor $\beta_0$ √© dada por:

$$ g(\beta_T) \approx g(\beta_0) + \frac{\partial g}{\partial \beta'}\Big|_{\beta=\beta_0} (\beta_T - \beta_0) $$

Sob a hip√≥tese nula, $g(\beta_0) = 0$, e a express√£o acima se torna:

$$ g(\beta_T) \approx \frac{\partial g}{\partial \beta'}\Big|_{\beta=\beta_0} (\beta_T - \beta_0) $$

onde $\beta_T$ √© o estimador OLS de $\beta$, e  $\frac{\partial g}{\partial \beta'}\Big|_{\beta=\beta_0}$ representa a matriz de derivadas parciais de $g$ em rela√ß√£o a $\beta$, avaliada no valor verdadeiro $\beta_0$.

*Distribui√ß√£o Assint√≥tica*
Como $\sqrt{T}(\beta_T - \beta_0)$ converge em distribui√ß√£o para uma normal com m√©dia zero e matriz de covari√¢ncia $\sigma^2 Q^{-1}$ [^8.2.9], podemos inferir que:
$$ \sqrt{T}[g(\beta_T) - g(\beta_0)] \xrightarrow{d}  \mathcal{N}\Big(0, \sigma^2 \frac{\partial g}{\partial \beta'}\Big|_{\beta=\beta_0} Q^{-1} \Big(\frac{\partial g}{\partial \beta'}\Big|_{\beta=\beta_0}\Big)' \Big) $$

Utilizando este resultado e o lema 8.1, podemos estabelecer o seguinte resultado sobre a distribui√ß√£o do teste de Wald para restri√ß√µes n√£o lineares:

**Teorema:** Sob a hip√≥tese nula e certas condi√ß√µes de regularidade, a estat√≠stica de Wald definida como
$$ \left[g(\beta_T)\right]'  \left[\frac{\partial g}{\partial \beta'} S_T \Big(\frac{\partial g}{\partial \beta'}\Big)'\right]^{-1}  g(\beta_T) $$
converge em distribui√ß√£o para uma qui-quadrado com *m* graus de liberdade, onde $S_T$ √© um estimador consistente da matriz de covari√¢ncia assint√≥tica de $\sqrt{T} (\beta_T - \beta_0)$, ou seja, $\sigma^2 Q^{-1}$.

Essa estat√≠stica nos permite testar hip√≥teses n√£o lineares sobre os par√¢metros do modelo usando a distribui√ß√£o assint√≥tica qui-quadrado.

*Desafios e Cuidados*
√â importante observar que a estat√≠stica de Wald pode ser sens√≠vel √† forma como a restri√ß√£o n√£o linear √© expressa [^8.2.23]. Pequenas mudan√ßas na forma da restri√ß√£o podem levar a conclus√µes diferentes em amostras finitas, embora assintoticamente a conclus√£o deva ser a mesma. √â recomend√°vel ter cuidado ao aplicar testes de Wald para restri√ß√µes n√£o lineares e considerar alternativas como o teste de raz√£o de verossimilhan√ßa (LR) ou o teste de multiplicadores de Lagrange (LM) quando apropriado.

*Teste F e sua Rela√ß√£o com Teste Qui-Quadrado*

Quando as restri√ß√µes s√£o lineares, o teste Wald se reduz ao teste F apresentado anteriormente, e a estat√≠stica de teste resultante possui uma distribui√ß√£o F sob a hip√≥tese nula.

*Exemplo de Aplica√ß√£o*
Considere um modelo onde se deseja testar a hip√≥tese de que o produto dos coeficientes $\beta_1$ e $\beta_2$ √© igual a 1, ou seja, $\beta_1\beta_2 = 1$. Aqui, $g(\beta) = \beta_1\beta_2 - 1$, que claramente √© uma restri√ß√£o n√£o linear. A estat√≠stica de Wald pode ser constru√≠da aplicando os conceitos desenvolvidos acima, e assim, realizar o teste de hip√≥tese de forma apropriada.

> üí° **Exemplo Num√©rico:**
>
> Vamos usar o exemplo da restri√ß√£o n√£o linear $\beta_1\beta_2 = 1$ para ilustrar o teste de Wald. Suponha que temos um modelo de regress√£o com dois preditores, e ap√≥s a estima√ß√£o, obtivemos os seguintes resultados:
>
> ```python
> import numpy as np
> from scipy.stats import chi2
>
> # Resultados da estima√ß√£o OLS
> beta_hat = np.array([0.5, 2.5]) # Estimativas dos par√¢metros beta1 e beta2
> cov_beta = np.array([[0.05, 0.02], [0.02, 0.10]]) # Matriz de covari√¢ncia das estimativas
> T = 100  # Tamanho da amostra
>
> # Defini√ß√£o da restri√ß√£o n√£o linear
> def g(beta):
>     return beta[0] * beta[1] - 1
>
> # Derivadas parciais de g em rela√ß√£o a beta
> def dg_dbeta(beta):
>     return np.array([beta[1], beta[0]])
>
> # Avaliando g no valor estimado de beta
> g_beta_hat = g(beta_hat)
>
> # Avaliando as derivadas parciais no valor estimado de beta
> dg_dbeta_hat = dg_dbeta(beta_hat)
>
> # Estimador da matriz de covari√¢ncia assint√≥tica
> S_T = cov_beta
>
> # Calcula a estat√≠stica de Wald
> Wald_stat = g_beta_hat * np.linalg.inv(dg_dbeta_hat @ S_T @ dg_dbeta_hat.T) * g_beta_hat
>
> # Calcula o p-valor usando a distribui√ß√£o qui-quadrado com 1 grau de liberdade
> p_valor = 1 - chi2.cdf(Wald_stat, 1)
>
> print("Estat√≠stica de Wald:", Wald_stat)
> print("P-valor:", p_valor)
>
> # Interpreta√ß√£o:
> # Um p-valor abaixo de um limiar comum (ex: 0.05) sugere que a hip√≥tese nula
> # de que beta1*beta2 = 1 deve ser rejeitada.
> ```
>
> **Interpreta√ß√£o:**
> No exemplo acima, calculamos a estat√≠stica de Wald e o p-valor para testar a hip√≥tese n√£o linear $\beta_1\beta_2 = 1$. O resultado do teste de Wald indica se h√° evid√™ncia suficiente para rejeitar a hip√≥tese nula com base nos dados. Se o p-valor for baixo (menor que um n√≠vel de signific√¢ncia predefinido), rejeitamos a hip√≥tese nula, concluindo que o produto dos coeficientes $\beta_1$ e $\beta_2$ √© significativamente diferente de 1. Caso contr√°rio, n√£o h√° evid√™ncia suficiente para rejeitar a hip√≥tese nula.

**Proposi√ß√£o 2** Se as restri√ß√µes n√£o lineares forem lineares, isto √©, se $g(\beta) = R\beta - q$, o teste de Wald para restri√ß√µes lineares √© equivalente ao teste F usual.

*Proof:*
I. Se $g(\beta) = R\beta - q$, ent√£o $\frac{\partial g}{\partial \beta'} = R$.
II. Substituindo isso na estat√≠stica de Wald, temos que
$$ (R\beta_T-q)' \left[R S_T R'\right]^{-1} (R\beta_T-q) $$
que √© a forma quadr√°tica do teste F para restri√ß√µes lineares.
‚ñ†

**Lema 2** (Lema de Slutsky) Sejam $\{X_n\}$ e $\{Y_n\}$ sequ√™ncias de vari√°veis aleat√≥rias. Se $X_n \xrightarrow{d} X$ e $Y_n \xrightarrow{p} c$, onde $c$ √© uma constante, ent√£o $X_n + Y_n \xrightarrow{d} X + c$, $X_n Y_n \xrightarrow{d} cX$, e $X_n/Y_n \xrightarrow{d} X/c$ se $c \neq 0$.

*Proof:* Verifique o resultado em qualquer livro texto de infer√™ncia estat√≠stica.
‚ñ†

### Conclus√£o
A an√°lise de testes de hip√≥teses com restri√ß√µes n√£o lineares √© crucial para muitos problemas econom√©tricos. A estat√≠stica de Wald, com suas propriedades assint√≥ticas, oferece uma abordagem geral para lidar com essas situa√ß√µes. No entanto, √© importante estar ciente das limita√ß√µes dessa abordagem e considerar outras alternativas quando necess√°rio. Os testes de raz√£o de verossimilhan√ßa (LR) e multiplicadores de Lagrange (LM) se apresentam como alternativas v√°lidas e ser√£o abordados em cap√≠tulos subsequentes. Os conceitos apresentados at√© aqui, sobre o m√©todo dos m√≠nimos quadrados ordin√°rios (OLS) e seus testes, incluindo o teste de Wald, s√£o fundamentais para a compreens√£o e desenvolvimento de modelos econom√©tricos.

### Refer√™ncias
[^8.1.24]:  Distribui√ß√£o da soma de quadrados dos res√≠duos.
[^8.1.25]:  N√£o correla√ß√£o entre b e √ª sob a premissa de normalidade dos res√≠duos.
[^8.2.9]:  Distribui√ß√£o assint√≥tica do estimador OLS.
<!-- END -->
