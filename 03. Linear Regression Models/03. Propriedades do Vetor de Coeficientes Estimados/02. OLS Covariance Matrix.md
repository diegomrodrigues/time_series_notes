## Matriz de CovariÃ¢ncia do Estimador OLS e a Matriz de InformaÃ§Ã£o de Fisher

### IntroduÃ§Ã£o
Este capÃ­tulo expande o entendimento sobre as propriedades do estimador de MÃ­nimos Quadrados OrdinÃ¡rios (OLS), com foco na derivaÃ§Ã£o detalhada da matriz de covariÃ¢ncia do estimador e sua conexÃ£o com a matriz de informaÃ§Ã£o de Fisher. Anteriormente, exploramos a nÃ£o-tendenciosidade do estimador OLS [^8.1.15], a estimaÃ§Ã£o da variÃ¢ncia dos resÃ­duos, e testes de hipÃ³teses [^8.1.18, ^8.2.23]. Agora, vamos analisar a estrutura da variabilidade dos coeficientes estimados, o que Ã© crucial para inferÃªncias estatÃ­sticas e construÃ§Ã£o de intervalos de confianÃ§a.

### Conceitos Fundamentais
A matriz de covariÃ¢ncia do estimador OLS, denotada por $Var(b)$, quantifica a variabilidade conjunta dos coeficientes estimados. Matematicamente, Ã© definida como a esperanÃ§a do produto externo dos desvios do estimador em relaÃ§Ã£o ao parÃ¢metro populacional:

$$ Var(b) = E[(b-\beta)(b-\beta)'] $$ [^8.1.16]

Utilizando a expressÃ£o para o estimador OLS  $b = \beta + (X'X)^{-1}X'u$ [^8.1.12], podemos reescrever a equaÃ§Ã£o acima como:

$$ Var(b) = E\left[\left((X'X)^{-1}X'u\right)\left((X'X)^{-1}X'u\right)'\right] $$

$$ Var(b) = E\left[(X'X)^{-1}X'uu'X(X'X)^{-1}\right] $$

Sob a premissa de que $E(uu') = \sigma^2 I_T$ [^8.1.15], onde $I_T$ Ã© uma matriz identidade de dimensÃ£o T, a matriz de covariÃ¢ncia Ã© dada por:
$$ Var(b) = (X'X)^{-1}X'E(uu')X(X'X)^{-1} = \sigma^2 (X'X)^{-1}X'X(X'X)^{-1} $$
$$ Var(b) = \sigma^2(X'X)^{-1}  $$ [^8.1.16]

Essa expressÃ£o representa a matriz de covariÃ¢ncia dos coeficientes estimados do modelo de regressÃ£o linear. Ela indica como a variÃ¢ncia de cada estimativa e a covariÃ¢ncia entre os estimadores dos parÃ¢metros sÃ£o influenciados pela variÃ¢ncia dos erros e pela matriz de variÃ¡veis explicativas. A matriz $(X'X)^{-1}$ Ã© uma medida da multicolinearidade das variÃ¡veis independentes, com valores maiores implicando em maiores variÃ¢ncias nas estimativas.

*Matriz de InformaÃ§Ã£o de Fisher*

A matriz de informaÃ§Ã£o de Fisher, por sua vez, Ã© um conceito fundamental na teoria da estimaÃ§Ã£o. Ela quantifica a quantidade de informaÃ§Ã£o que uma amostra aleatÃ³ria fornece sobre um parÃ¢metro desconhecido. Em contextos de regressÃ£o, a matriz de informaÃ§Ã£o de Fisher pode ser relacionada Ã  matriz de covariÃ¢ncia do estimador OLS sob certas premissas. No caso de um modelo de regressÃ£o com erros Gaussianos, ela Ã© dada por:
$$ I(\beta) = E\left[-\frac{\partial^2 \log L(\beta; y)}{\partial \beta \partial \beta'}\right] $$
onde $L(\beta; y)$ Ã© a funÃ§Ã£o de verossimilhanÃ§a do modelo. Para o modelo de regressÃ£o linear com erros Gaussianos, a funÃ§Ã£o de verossimilhanÃ§a pode ser escrita como:
$$ L(\beta; y) = \frac{1}{(2\pi \sigma^2)^{T/2}} \exp\left[-\frac{1}{2\sigma^2} (y-X\beta)'(y-X\beta)\right] $$
O logaritmo da funÃ§Ã£o de verossimilhanÃ§a Ã©:
$$ \log L(\beta; y) = -\frac{T}{2}\log(2\pi \sigma^2) - \frac{1}{2\sigma^2} (y-X\beta)'(y-X\beta) $$
As derivadas da funÃ§Ã£o de log-verossimilhanÃ§a em relaÃ§Ã£o a $\beta$ sÃ£o:
$$ \frac{\partial \log L(\beta; y)}{\partial \beta} = \frac{1}{\sigma^2}X'(y-X\beta) $$
e
$$ \frac{\partial^2 \log L(\beta; y)}{\partial \beta \partial \beta'} = -\frac{1}{\sigma^2}X'X $$
Assim, a matriz de informaÃ§Ã£o de Fisher Ã©:
$$ I(\beta) =  -\mathbb{E}\left[-\frac{1}{\sigma^2}X'X\right] = \frac{1}{\sigma^2} X'X$$

Comparando com a matriz de covariÃ¢ncia de $b$, vemos que a matriz de informaÃ§Ã£o de Fisher Ã© inversamente proporcional Ã  matriz de covariÃ¢ncia do estimador:
$$ Var(b) =  \sigma^2(X'X)^{-1} = I(\beta)^{-1} $$
Essa relaÃ§Ã£o Ã© uma manifestaÃ§Ã£o do princÃ­pio da eficiÃªncia na estimaÃ§Ã£o: o estimador OLS, sendo o estimador de mÃ¡xima verossimilhanÃ§a, atinge o limite de CramÃ©r-Rao, tendo a menor variÃ¢ncia possÃ­vel dentre os estimadores nÃ£o viesados.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos considerar um modelo de regressÃ£o simples para ilustrar essa relaÃ§Ã£o. Suponha que temos dados sobre o nÃºmero de horas de estudo ($X$) e as notas obtidas em um teste ($y$).
>
> ```python
> import numpy as np
> import pandas as pd
>
> # Dados de exemplo
> data = {'horas_estudo': [2, 3, 4, 5, 6, 7],
>         'nota': [60, 65, 75, 78, 85, 90]}
> df = pd.DataFrame(data)
>
> # Matriz X com intercepto
> X = np.array(df['horas_estudo'])
> X = np.column_stack((np.ones(len(X)), X)) # Adiciona uma coluna de 1s para o intercepto
>
> # Vetor y
> y = np.array(df['nota'])
>
> # Calculando X'X
> XtX = X.T @ X
>
> # Calculando (X'X)^-1
> XtX_inv = np.linalg.inv(XtX)
>
> # Estimando os coeficientes com OLS (para estimar sigma^2)
> b = np.linalg.inv(X.T @ X) @ X.T @ y
>
> # Calculando os resÃ­duos
> u = y - X @ b
>
> # Estimando sigma^2 (variÃ¢ncia dos resÃ­duos)
> sigma2_hat = (u.T @ u) / (len(y) - X.shape[1]) # T-k graus de liberdade
>
> # Calculando a matriz de covariÃ¢ncia de b
> cov_b = sigma2_hat * XtX_inv
>
> # Calculando a matriz de informaÃ§Ã£o de Fisher
> fisher_info = (1/sigma2_hat) * XtX
>
> print("Matriz de CovariÃ¢ncia de b:\n", cov_b)
> print("\nMatriz de InformaÃ§Ã£o de Fisher:\n", fisher_info)
> print("\nInversa da Matriz de InformaÃ§Ã£o de Fisher:\n", np.linalg.inv(fisher_info))
>
> ```
>
> **InterpretaÃ§Ã£o:**
>
> A matriz `cov_b` representa a variabilidade dos coeficientes estimados. A diagonal principal indica a variÃ¢ncia de cada coeficiente (intercepto e inclinaÃ§Ã£o), enquanto os elementos fora da diagonal indicam a covariÃ¢ncia entre eles. A matriz `fisher_info` Ã© a matriz de informaÃ§Ã£o de Fisher, que Ã© inversamente proporcional Ã  matriz de covariÃ¢ncia de `b`. Observe que a inversa da matriz de informaÃ§Ã£o de Fisher Ã© aproximadamente igual Ã  matriz de covariÃ¢ncia, multiplicada por sigma^2. Este exemplo ilustra numericamente a relaÃ§Ã£o teÃ³rica entre as matrizes de covariÃ¢ncia do estimador OLS e a matriz de informaÃ§Ã£o de Fisher.
>
> Para a matriz de covariÃ¢ncia `cov_b`, vamos calcular os desvios-padrÃ£o dos coeficientes e usar para construir um intervalo de confianÃ§a de 95% para o parÃ¢metro de inclinaÃ§Ã£o $\beta_1$
> ```python
> import scipy.stats as st
> # Extraindo os desvios padrÃ£o da matriz de covariÃ¢ncia
> std_err_b = np.sqrt(np.diag(cov_b))
>
> # Intervalo de confianÃ§a de 95% para o parÃ¢metro de inclinaÃ§Ã£o (beta_1)
> beta_1_mean = b[1] # A inclinaÃ§Ã£o corresponde ao segundo elemento do vetor b
> beta_1_std_err = std_err_b[1]
> confidence_interval = st.t.interval(0.95, len(y) - X.shape[1], loc=beta_1_mean, scale=beta_1_std_err)
>
> print(f"Intervalo de confianÃ§a de 95% para o parÃ¢metro de inclinaÃ§Ã£o (beta_1): {confidence_interval}")
> ```
> O intervalo de confianÃ§a nos permite quantificar a incerteza na estimativa do parÃ¢metro de inclinaÃ§Ã£o. Quanto maior a variÃ¢ncia do estimador, maior o intervalo de confianÃ§a, e maior a incerteza sobre o valor verdadeiro do parÃ¢metro.

**Lema 1** Se $\hat{\sigma}^2 = \frac{u'u}{T-k}$ Ã© o estimador nÃ£o-viesado da variÃ¢ncia dos erros, entÃ£o $E[\hat{\sigma}^2]=\sigma^2$.

*Proof:* Conforme demonstrado em [^8.1.18], o estimador de variÃ¢ncia dos resÃ­duos, $\hat{\sigma}^2$, Ã© nÃ£o viesado, ou seja, seu valor esperado coincide com a variÃ¢ncia populacional dos erros. A demonstraÃ§Ã£o envolve o uso da matriz de projeÃ§Ã£o e da propriedade do traÃ§o de uma matriz.
â– 

**Lema 1.1**  A variÃ¢ncia do estimador $\hat{\sigma}^2$ Ã© dada por $Var(\hat{\sigma}^2) = \frac{2\sigma^4}{T-k}$ quando os erros sÃ£o normalmente distribuÃ­dos.

*Proof:* 
I. Se os erros sÃ£o normalmente distribuÃ­dos, entÃ£o $(T-k)\hat{\sigma}^2/\sigma^2$ segue uma distribuiÃ§Ã£o $\chi^2$ com $T-k$ graus de liberdade.
II. A variÃ¢ncia de uma variÃ¡vel $\chi^2$ com $n$ graus de liberdade Ã© $2n$. 
III. Portanto, $Var((T-k)\hat{\sigma}^2/\sigma^2) = 2(T-k)$.
IV. Usando a propriedade da variÃ¢ncia de uma constante multiplicada por uma variÃ¡vel aleatÃ³ria, temos que $Var(aX) = a^2 Var(X)$. Logo, 
$Var\left(\frac{(T-k)\hat{\sigma}^2}{\sigma^2}\right) = \frac{(T-k)^2}{\sigma^4} Var(\hat{\sigma}^2) = 2(T-k)$.
V. Resolvendo para $Var(\hat{\sigma}^2)$, obtemos $Var(\hat{\sigma}^2) = \frac{2\sigma^4}{T-k}$.
â– 

*ConexÃ£o com a Qualidade do Ajuste*

A matriz de covariÃ¢ncia de $b$ nÃ£o sÃ³ mede a precisÃ£o dos estimadores, mas tambÃ©m se relaciona com a qualidade geral do ajuste do modelo. Observando a matriz $(X'X)^{-1}$, nota-se que quanto mais ortogonais forem as colunas de $X$ (ou seja, quanto menos multicolinearidade), menor serÃ¡ a variÃ¢ncia das estimativas. A multicolinearidade aumenta a variÃ¢ncia, tornando as estimativas menos precisas e mais sensÃ­veis a pequenas variaÃ§Ãµes nos dados.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Para demonstrar o efeito da multicolinearidade, vamos modificar o exemplo anterior, adicionando uma variÃ¡vel que Ã© altamente correlacionada com a variÃ¡vel horas de estudo. Suponha que agora temos dados sobre horas de estudo (X1) e horas de revisÃ£o (X2), onde X2 = 0.9*X1 + ruÃ­do, e queremos avaliar o efeito na matriz de covariÃ¢ncia.
>
> ```python
> import numpy as np
> import pandas as pd
>
> # Dados de exemplo
> data = {'horas_estudo': [2, 3, 4, 5, 6, 7],
>         'nota': [60, 65, 75, 78, 85, 90]}
> df = pd.DataFrame(data)
>
> # Matriz X original com intercepto
> X1 = np.array(df['horas_estudo'])
> X1 = np.column_stack((np.ones(len(X1)), X1))
>
> # Vetor y
> y = np.array(df['nota'])
>
> # Calculando (X'X)^-1 original
> XtX_inv_original = np.linalg.inv(X1.T @ X1)
>
> # Estimando sigma^2 original
> b_original = np.linalg.inv(X1.T @ X1) @ X1.T @ y
> u_original = y - X1 @ b_original
> sigma2_hat_original = (u_original.T @ u_original) / (len(y) - X1.shape[1])
>
> # Calculando a matriz de covariÃ¢ncia original
> cov_b_original = sigma2_hat_original * XtX_inv_original
>
> # Criando a nova coluna com multicolinearidade
> X2 = 0.9 * df['horas_estudo'] + np.random.normal(0,0.5, len(df['horas_estudo']))
>
> # Matriz X com multicolinearidade
> X_multi = np.column_stack((np.ones(len(X1)),df['horas_estudo'], X2 ))
>
> # Calculando (X'X)^-1 com multicolinearidade
> XtX_inv_multi = np.linalg.inv(X_multi.T @ X_multi)
>
> # Estimando sigma^2 com multicolinearidade
> b_multi = np.linalg.inv(X_multi.T @ X_multi) @ X_multi.T @ y
> u_multi = y - X_multi @ b_multi
> sigma2_hat_multi = (u_multi.T @ u_multi) / (len(y) - X_multi.shape[1])
>
> # Calculando a matriz de covariÃ¢ncia com multicolinearidade
> cov_b_multi = sigma2_hat_multi * XtX_inv_multi
>
> print("Matriz de CovariÃ¢ncia original:\n", cov_b_original)
> print("\nMatriz de CovariÃ¢ncia com Multicolinearidade:\n", cov_b_multi)
>
> ```
>
> **InterpretaÃ§Ã£o:**
>
> Observe como a matriz de covariÃ¢ncia aumentou quando adicionamos uma variÃ¡vel que Ã© altamente correlacionada com a outra variÃ¡vel explicativa. As variÃ¢ncias dos coeficientes (elementos da diagonal) aumentaram, indicando que as estimativas sÃ£o menos precisas devido Ã  multicolinearidade.

**Lema 3** Se os erros $u$ seguem uma distribuiÃ§Ã£o normal com mÃ©dia zero e variÃ¢ncia $\sigma^2$, entÃ£o o estimador OLS $b$ Ã© tambÃ©m normalmente distribuÃ­do.

*Proof:*
I. Sabemos que $b = \beta + (X'X)^{-1}X'u$ [^8.1.12].
II. Aqui, $b$ Ã© uma transformaÃ§Ã£o linear de $u$.
III. Como $u$ Ã© normalmente distribuÃ­do, qualquer transformaÃ§Ã£o linear de $u$ tambÃ©m serÃ¡ normalmente distribuÃ­da.
IV. Portanto, $b$ tambÃ©m Ã© normalmente distribuÃ­do.
V. A mÃ©dia de $b$ Ã© $E(b) = \beta$.
VI. A variÃ¢ncia de $b$ Ã© $Var(b) = \sigma^2(X'X)^{-1}$.
VII. Assim, concluÃ­mos que $b \sim \mathcal{N}(\beta, \sigma^2(X'X)^{-1})$.
â– 

**ProposiÃ§Ã£o 3** A matriz de informaÃ§Ã£o de Fisher para o modelo de regressÃ£o linear Ã© dada por $I(\beta) = \frac{1}{\sigma^2} X'X$, sob a premissa que os erros sejam normalmente distribuÃ­dos.

*Proof:*
I.  A matriz de informaÃ§Ã£o de Fisher Ã© definida como $I(\beta) = E\left[-\frac{\partial^2 \log L(\beta; y)}{\partial \beta \partial \beta'}\right]$.
II.  Para um modelo de regressÃ£o linear com erros gaussianos, temos que  $\frac{\partial^2 \log L(\beta; y)}{\partial \beta \partial \beta'} = -\frac{1}{\sigma^2}X'X$.
III.  Tomando o valor esperado, obtemos $I(\beta) = -\mathbb{E}\left[-\frac{1}{\sigma^2}X'X\right]$.
IV. Como $X$ Ã© considerado nÃ£o estocÃ¡stico, podemos retirar da esperanÃ§a e concluÃ­mos que $I(\beta) = \frac{1}{\sigma^2}X'X$
â– 

**ProposiÃ§Ã£o 3.1** Sob as condiÃ§Ãµes de normalidade dos erros, o estimador OLS $b$ Ã© o estimador de mÃ¡xima verossimilhanÃ§a (MV).

*Proof:*
I. A funÃ§Ã£o de verossimilhanÃ§a para um modelo linear com erros normais Ã©:
$$L(\beta; y) = \frac{1}{(2\pi \sigma^2)^{T/2}} \exp\left[-\frac{1}{2\sigma^2} (y-X\beta)'(y-X\beta)\right]$$
II. Maximizar $L(\beta; y)$ Ã© equivalente a minimizar $ (y-X\beta)'(y-X\beta)$, que Ã© a soma dos erros quadrados.
III. A condiÃ§Ã£o de primeira ordem para minimizar essa soma Ã© exatamente a mesma para o estimador OLS:
$$\frac{\partial}{\partial \beta}(y-X\beta)'(y-X\beta) = -2X'(y-X\beta) = 0$$
IV. Como o estimador de mÃ¡xima verossimilhanÃ§a Ã© Ãºnico, e essa Ã© a condiÃ§Ã£o do OLS, concluÃ­mos que o estimador OLS Ã© tambÃ©m o estimador de mÃ¡xima verossimilhanÃ§a sob as condiÃ§Ãµes de normalidade dos erros.
â– 

**CorolÃ¡rio 3.2** O estimador OLS $b$ Ã© assintoticamente normal, mesmo que os erros nÃ£o sejam estritamente normais, desde que as condiÃ§Ãµes de regularidade sejam atendidas.

*Proof:*
I.  O estimador OLS pode ser escrito como uma mÃ©dia amostral ponderada dos erros: $b = \beta + (X'X)^{-1}X'u$.
II.  O Teorema do Limite Central (TLC) afirma que a distribuiÃ§Ã£o da mÃ©dia de um nÃºmero grande de variÃ¡veis aleatÃ³rias independentes e identicamente distribuÃ­das (i.i.d.) converge para uma distribuiÃ§Ã£o normal, sob certas condiÃ§Ãµes de regularidade.
III.  Embora os erros $u$ nÃ£o sejam necessariamente i.i.d., se as condiÃ§Ãµes de regularidade do TLC para sequÃªncias nÃ£o i.i.d. forem satisfeitas (como a existÃªncia de momentos finitos e que a influÃªncia de cada erro individual sobre a estimativa do coeficiente tenda a zero Ã  medida que o tamanho da amostra aumenta), entÃ£o $(X'X)^{-1}X'u$ Ã© assintoticamente normal.
IV. Portanto, o estimador OLS $b$ converge em distribuiÃ§Ã£o para uma normal, mesmo que os erros nÃ£o sejam estritamente normais, desde que as condiÃ§Ãµes de regularidade padrÃ£o sejam vÃ¡lidas e a matriz $X$ nÃ£o seja estocÃ¡stica.
â– 
### ConclusÃ£o
A matriz de covariÃ¢ncia do estimador OLS, expressa como $Var(b) = \sigma^2(X'X)^{-1}$, Ã© fundamental para a inferÃªncia estatÃ­stica em modelos de regressÃ£o. Ela quantifica a precisÃ£o das estimativas dos coeficientes e as correlaÃ§Ãµes entre eles, e sua relaÃ§Ã£o com a matriz de informaÃ§Ã£o de Fisher reforÃ§a a eficiÃªncia do estimador OLS no contexto de modelos com erros Gaussianos.  Entender sua derivaÃ§Ã£o e seus componentes Ã© crucial para uma anÃ¡lise rigorosa de modelos de regressÃ£o linear e para a construÃ§Ã£o de inferÃªncias confiÃ¡veis e testes de hipÃ³teses.

### ReferÃªncias
[^8.1.12]: ExpressÃ£o para o estimador OLS.
[^8.1.15]:  Vetor de coeficientes estimados OLS Ã© nÃ£o viesado.
[^8.1.16]:  DerivaÃ§Ã£o da matriz de covariÃ¢ncia do estimador OLS.
[^8.1.18]: DefiniÃ§Ã£o e representaÃ§Ã£o do estimador de variÃ¢ncia do OLS como funÃ§Ã£o da matriz de projeÃ§Ã£o.
[^8.2.23]: Testes de hipÃ³teses com restriÃ§Ãµes nÃ£o lineares.
<!-- END -->
