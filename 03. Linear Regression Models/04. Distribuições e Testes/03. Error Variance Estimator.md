## Distribui√ß√µes de Estimadores e Estat√≠sticas de Teste Sob Normalidade

### Introdu√ß√£o
Este cap√≠tulo aborda as distribui√ß√µes dos estimadores de m√≠nimos quadrados ordin√°rios (OLS) e das estat√≠sticas de teste *t* e *F* sob a suposi√ß√£o de normalidade dos erros. Como vimos anteriormente, a estat√≠stica *F* serve para testar restri√ß√µes lineares conjuntas sobre os coeficientes de um modelo de regress√£o e, no caso especial de apenas uma restri√ß√£o, √© equivalente ao quadrado da estat√≠stica *t* [^8.1.33]. Aqui, exploramos as distribui√ß√µes dessas estat√≠sticas em detalhe, incluindo a distribui√ß√£o do estimador da vari√¢ncia do erro. Expandindo o que vimos sobre testes de hip√≥teses, agora focaremos nas distribui√ß√µes exatas sob a condi√ß√£o de erros normalmente distribu√≠dos.

### Distribui√ß√£o do Estimador da Vari√¢ncia do Erro

Sob a premissa de que os erros $u_t$ s√£o independentes e identicamente distribu√≠dos (i.i.d.) com m√©dia zero e vari√¢ncia $\sigma^2$, e que seguem uma distribui√ß√£o normal, ou seja, $u_t \sim \text{N}(0, \sigma^2)$ [^8.1], podemos estabelecer resultados importantes sobre a distribui√ß√£o do estimador da vari√¢ncia do erro.

O estimador OLS da vari√¢ncia do erro √© dado por:
$$s^2 = \frac{RSS}{T-k} = \frac{\hat{u}'\hat{u}}{T-k}$$
onde $RSS$ √© a soma dos quadrados dos res√≠duos, $T$ √© o n√∫mero de observa√ß√µes e $k$ √© o n√∫mero de par√¢metros no modelo.

**Teorema 3.** Sob as condi√ß√µes cl√°ssicas de regress√£o linear, incluindo a suposi√ß√£o de normalidade dos erros, a estat√≠stica $\frac{(T-k)s^2}{\sigma^2}$ segue uma distribui√ß√£o qui-quadrado com $T-k$ graus de liberdade [^8.1.24].

*Prova:*
I.  Come√ßamos com a express√£o para a soma dos quadrados dos res√≠duos (RSS):
   $$RSS = \hat{u}'\hat{u} = u'M_Xu$$
    onde $M_X = I - X(X'X)^{-1}X'$ √© a matriz de proje√ß√£o ortogonal.

II.  Como $M_X$ √© sim√©trica, existe uma matriz $P$ tal que $M_X = PAP'$ [^8.1.20], onde $P'P = I_T$ [^8.1.21] e $A$ √© uma matriz diagonal contendo os autovalores de $M_X$.

III. Substitu√≠mos $M_X$ na express√£o para RSS:
    $$RSS = u'PAP'u = (P'u)'A(P'u) = w'Aw$$
    onde $w = P'u$.

IV. Como $E(uu') = \sigma^2 I_T$, temos $E(ww') = E(P'uu'P) = \sigma^2 P'P = \sigma^2 I_T$. Isso significa que os elementos de $w$ s√£o n√£o correlacionados com m√©dia zero e vari√¢ncia $\sigma^2$.

V. Al√©m disso, como $u$ √© normal, ent√£o $w$ tamb√©m √© normal. Portanto, $w_i \sim \text{N}(0, \sigma^2)$.

VI. De [^8.1.9], sabemos que $M_Xv = 0$ se $v$ for um vetor no espa√ßo coluna de $X$, e que existem $k$ autovalores de $M_X$ iguais a zero e $T-k$ autovalores iguais a um.

VII. Assim, $A$ tem $k$ zeros e $T-k$ uns na diagonal, e ent√£o:
    $$RSS = \sum_{i=1}^{T-k} w_i^2$$

VIII. Definindo $z_i = w_i/\sigma$, temos $z_i \sim \text{N}(0,1)$. Portanto,
    $$\frac{RSS}{\sigma^2} = \sum_{i=1}^{T-k} z_i^2$$
    segue uma distribui√ß√£o qui-quadrado com $T-k$ graus de liberdade, denotada por $\chi^2(T-k)$ [^8.1.24].

IX. Finalmente, multiplicando ambos os lados por $\frac{T-k}{T-k}$ obtemos:
  $$\frac{(T-k)s^2}{\sigma^2} = \frac{RSS}{\sigma^2} \sim \chi^2(T-k)$$
  que prova o teorema. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Suponha que temos um modelo de regress√£o com $T=100$ observa√ß√µes e $k=5$ par√¢metros. Ap√≥s ajustar o modelo, obtivemos uma soma dos quadrados dos res√≠duos (RSS) igual a 250. A vari√¢ncia do erro estimada √© ent√£o:
>
> $s^2 = \frac{RSS}{T-k} = \frac{250}{100-5} = \frac{250}{95} \approx 2.63$.
>
> Se assumirmos que a verdadeira vari√¢ncia do erro $\sigma^2$ √© 2, ent√£o a estat√≠stica $\frac{(T-k)s^2}{\sigma^2}$ √©:
>
> $\frac{(100-5) \times 2.63}{2} = \frac{95 \times 2.63}{2} = 125.375$.
>
> Pelo Teorema 3, essa estat√≠stica segue uma distribui√ß√£o qui-quadrado com $100-5 = 95$ graus de liberdade, ou seja, $\chi^2(95)$. Este valor calculado pode ser comparado com a distribui√ß√£o qui-quadrado para verificar se a varia√ß√£o observada √© esperada sob a suposi√ß√£o de normalidade dos erros.

Este teorema estabelece que o estimador da vari√¢ncia do erro, quando escalonado, segue uma distribui√ß√£o qui-quadrado, o que √© crucial para infer√™ncia estat√≠stica sobre os par√¢metros do modelo.

**Lema 3.1.** Sob as mesmas condi√ß√µes do Teorema 3, $s^2$ √© um estimador n√£o viesado de $\sigma^2$.

*Prova:*
I. Pelo Teorema 3, temos que $\frac{(T-k)s^2}{\sigma^2} \sim \chi^2(T-k)$. A esperan√ßa de uma vari√°vel aleat√≥ria qui-quadrado com $\nu$ graus de liberdade √© $\nu$. Portanto,
    $$E\left[\frac{(T-k)s^2}{\sigma^2}\right] = T-k$$

II. Multiplicando ambos os lados por $\sigma^2$ temos
   $$E[(T-k)s^2] = (T-k)\sigma^2$$

III. Dividindo ambos os lados por $(T-k)$ temos
    $$E[s^2] = \sigma^2$$
    Logo, $s^2$ √© n√£o viesado para $\sigma^2$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Continuando o exemplo anterior, sabemos que $s^2 \approx 2.63$. O lema 3.1 garante que este valor √© um estimador n√£o viesado da verdadeira vari√¢ncia do erro $\sigma^2$. Se repet√≠ssemos esse processo de amostragem e estimativa v√°rias vezes, a m√©dia de todos os valores de $s^2$ obtidos se aproximaria do valor real de $\sigma^2$.

Este lema complementa o Teorema 3, demonstrando uma propriedade importante do estimador da vari√¢ncia do erro: sua n√£o-viesabilidade. Este resultado √© essencial para garantir a validade dos testes de hip√≥teses e intervalos de confian√ßa baseados neste estimador.

### Distribui√ß√£o da Estat√≠stica de Teste *t*
A estat√≠stica *t* √© utilizada para testar hip√≥teses sobre um √∫nico coeficiente em um modelo de regress√£o linear. Ela √© definida como a raz√£o entre o desvio do estimador do coeficiente em rela√ß√£o ao seu valor sob a hip√≥tese nula, e o erro padr√£o do estimador.

Sob a hip√≥tese nula $H_0: \beta_i = \beta_i^0$ [^8.1.26], a estat√≠stica *t* √© dada por:
$$t = \frac{b_i - \beta_i^0}{\sqrt{s^2 \xi^{ii}}}$$
onde:
* $b_i$ √© o estimador OLS do coeficiente $\beta_i$.
* $\beta_i^0$ √© o valor de $\beta_i$ sob a hip√≥tese nula.
* $s^2$ √© o estimador da vari√¢ncia do erro.
* $\xi^{ii}$ √© o i-√©simo elemento da diagonal da matriz $(X'X)^{-1}$ [^8.1.26].

**Teorema 4.** Sob as condi√ß√µes cl√°ssicas de regress√£o linear, incluindo a suposi√ß√£o de normalidade dos erros, e assumindo que $H_0$ seja verdadeira, a estat√≠stica *t* segue uma distribui√ß√£o t de Student com $T-k$ graus de liberdade [^8.1.26].

*Prova:*
I. Sabemos que $b_i$ √© um estimador n√£o-viesado de $\beta_i$, e que sob a suposi√ß√£o de normalidade, $b_i$ segue uma distribui√ß√£o normal, i.e., $b_i \sim \text{N}(\beta_i, \sigma^2 \xi^{ii})$ [^8.1.17]. Assim, $\frac{b_i - \beta_i}{\sqrt{\sigma^2 \xi^{ii}}} \sim \text{N}(0, 1)$.

II. Sob a hip√≥tese nula $H_0: \beta_i = \beta_i^0$, temos que $\frac{b_i - \beta_i^0}{\sqrt{\sigma^2 \xi^{ii}}} \sim \text{N}(0, 1)$.

III. Tamb√©m sabemos que $(T-k)s^2/\sigma^2 \sim \chi^2(T-k)$, como demonstrado no Teorema 3.

IV. Al√©m disso, o estimador $b$ e o estimador $s^2$ s√£o independentes sob a suposi√ß√£o de normalidade dos erros [^8.1.25].

V. A estat√≠stica $t$ pode ser reescrita como:
   $$t = \frac{\frac{b_i - \beta_i^0}{\sqrt{\sigma^2 \xi^{ii}}}}{\sqrt{\frac{s^2}{\sigma^2}}} = \frac{\frac{b_i - \beta_i^0}{\sqrt{\sigma^2 \xi^{ii}}}}{\sqrt{\frac{(T-k)s^2/\sigma^2}{T-k}}}$$

VI. A express√£o acima √© a raz√£o entre uma vari√°vel aleat√≥ria normal padronizada e a raiz quadrada de uma vari√°vel qui-quadrado dividida por seus graus de liberdade, o que, por defini√ß√£o, segue uma distribui√ß√£o t de Student com $T-k$ graus de liberdade.
$\blacksquare$

> üí° **Exemplo Num√©rico:**
> Considere um modelo de regress√£o com $T=50$, $k=4$, e que estamos testando a hip√≥tese nula de que o coeficiente $\beta_2$ √© igual a zero, i.e., $H_0: \beta_2 = 0$. Suponha que o estimador OLS do coeficiente seja $b_2 = 0.5$, a vari√¢ncia estimada do erro seja $s^2 = 4$, e o elemento correspondente da diagonal de $(X'X)^{-1}$ seja $\xi^{22} = 0.04$. A estat√≠stica *t* √© calculada como:
>
> $t = \frac{0.5 - 0}{\sqrt{4 \times 0.04}} = \frac{0.5}{\sqrt{0.16}} = \frac{0.5}{0.4} = 1.25$.
>
> Pelo Teorema 4, sob a hip√≥tese nula, essa estat√≠stica *t* segue uma distribui√ß√£o t de Student com $50-4 = 46$ graus de liberdade. Podemos comparar o valor observado de 1.25 com os valores cr√≠ticos da distribui√ß√£o t de Student com 46 graus de liberdade para determinar se devemos rejeitar a hip√≥tese nula.

Este resultado √© fundamental para realizar testes de hip√≥teses sobre um √∫nico coeficiente. Ele nos permite calcular os valores-p, comparando o valor observado da estat√≠stica *t* com os valores cr√≠ticos da distribui√ß√£o *t* de Student.

**Lema 4.1.** O estimador OLS $b_i$ √© independente de $\hat{u}$ sob a suposi√ß√£o de normalidade dos erros.

*Prova:*
I. O estimador OLS $b$ pode ser expresso como $b = (X'X)^{-1}X'y$. Substituindo $y$ por $X\beta + u$, temos: $b = (X'X)^{-1}X'(X\beta + u) = \beta + (X'X)^{-1}X'u$.

II. O vetor de res√≠duos √© dado por $\hat{u} = y - Xb = X\beta + u - X(\beta + (X'X)^{-1}X'u) = u - X(X'X)^{-1}X'u = Mu$.

III. Assim, $b = \beta + (X'X)^{-1}X'u$ e $\hat{u} = Mu$.

IV. Como $u$ segue uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\sigma^2I$, ou seja, $u \sim \text{N}(0, \sigma^2I)$, ent√£o $b$ e $\hat{u}$ s√£o fun√ß√µes lineares de $u$ e seguem uma distribui√ß√£o normal. Para provar que $b$ e $\hat{u}$ s√£o independentes, basta mostrar que sua covari√¢ncia √© zero.
$$Cov(b, \hat{u}) = Cov(\beta + (X'X)^{-1}X'u, Mu) = Cov((X'X)^{-1}X'u, Mu) = (X'X)^{-1}X' Cov(u, u')M' = (X'X)^{-1}X'\sigma^2IM' = \sigma^2(X'X)^{-1}X'M'$$

V. Sabemos que $M = I - X(X'X)^{-1}X'$, ent√£o $M' = M$.
$$Cov(b, \hat{u}) = \sigma^2(X'X)^{-1}X'(I - X(X'X)^{-1}X') = \sigma^2(X'X)^{-1}X' - \sigma^2(X'X)^{-1}X'X(X'X)^{-1}X' = \sigma^2(X'X)^{-1}X' - \sigma^2(X'X)^{-1}X' = 0$$

VI. Como a covari√¢ncia entre $b$ e $\hat{u}$ √© zero e ambos seguem uma distribui√ß√£o normal, eles s√£o independentes. Consequentemente, $b_i$ e $\hat{u}$ tamb√©m s√£o independentes. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Este lema √© crucial para as dedu√ß√µes de distribui√ß√µes das estat√≠sticas de teste. Para ilustrar, suponha que $X$ √© uma matriz de design, $y$ √© um vetor de resposta e $u$ √© o vetor de erro. Com a normalidade dos erros, $b$ e $\hat{u}$ s√£o independentes, o que permite, por exemplo, usar a distribui√ß√£o t de Student para construir intervalos de confian√ßa para os coeficientes.

Este lema formaliza a independ√™ncia entre o estimador dos coeficientes e os res√≠duos, um resultado crucial para a distribui√ß√£o da estat√≠stica *t* e, posteriormente, da estat√≠stica *F*, sob a hip√≥tese de normalidade dos erros. A independ√™ncia √© fundamental para que as distribui√ß√µes das estat√≠sticas de teste sejam v√°lidas sob a hip√≥tese nula.

### Rela√ß√£o com Testes F
Como vimos, a estat√≠stica *F* √© um caso geral da estat√≠stica *t*. Especificamente, para testar a hip√≥tese nula de que um √∫nico coeficiente √© igual a um determinado valor ($H_0: \beta_i = \beta_i^0$), a estat√≠stica *F* √© o quadrado da estat√≠stica *t* [^8.1.33], como explicitado no Lema 1.1.

Se o valor calculado da estat√≠stica *t* √© $t$, ent√£o o valor da estat√≠stica *F* correspondente √© $F = t^2$. Sob a hip√≥tese nula, $t$ segue uma distribui√ß√£o t de Student com $T-k$ graus de liberdade, e *F* segue uma distribui√ß√£o F com 1 e $T-k$ graus de liberdade.

**Teorema 4.1.** Sob as mesmas condi√ß√µes do Teorema 4, a estat√≠stica $F$ para testar a restri√ß√£o $H_0: \beta_i = \beta_i^0$ segue uma distribui√ß√£o $F(1, T-k)$.

*Prova:*
I. Como $F=t^2$, e $t$ segue uma distribui√ß√£o $t(T-k)$, sabemos da defini√ß√£o da distribui√ß√£o F que o quadrado de uma vari√°vel aleat√≥ria com distribui√ß√£o t de Student com $\nu$ graus de liberdade tem distribui√ß√£o $F(1, \nu)$. Portanto, como $t \sim t(T-k)$, ent√£o $F = t^2 \sim F(1, T-k)$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Retomando o exemplo anterior da estat√≠stica *t*, onde calculamos $t = 1.25$. A estat√≠stica *F* correspondente seria $F = t^2 = (1.25)^2 = 1.5625$. Pelo Teorema 4.1, essa estat√≠stica *F* segue uma distribui√ß√£o F com 1 e 46 graus de liberdade (j√° que t√≠nhamos $T-k=46$ no exemplo anterior). Usar√≠amos essa distribui√ß√£o para encontrar o valor-p para um teste *F* e verificar se rejeitamos a hip√≥tese nula.
>
> Em resumo, a estat√≠stica $F$ √© √∫til ao testarmos m√∫ltiplas restri√ß√µes sobre os coeficientes de uma regress√£o linear, enquanto a estat√≠stica $t$ √© usada para testar apenas um √∫nico coeficiente.

Este teorema estabelece a distribui√ß√£o exata da estat√≠stica F quando usada para testar uma √∫nica restri√ß√£o, mostrando que a rela√ß√£o entre a estat√≠stica t e F se mant√©m sob a suposi√ß√£o de normalidade dos erros.

### Conclus√£o
Este cap√≠tulo estabeleceu a distribui√ß√£o do estimador da vari√¢ncia do erro e da estat√≠stica de teste *t* sob a suposi√ß√£o de normalidade dos erros em um modelo de regress√£o linear. Especificamente, mostramos que o estimador da vari√¢ncia do erro, escalonado, segue uma distribui√ß√£o qui-quadrado com $T-k$ graus de liberdade, e que a estat√≠stica *t* segue uma distribui√ß√£o t de Student com $T-k$ graus de liberdade quando a hip√≥tese nula √© verdadeira e o denominador √© um estimador da vari√¢ncia. Esses resultados s√£o cruciais para a infer√™ncia estat√≠stica em modelos de regress√£o linear e para o entendimento das propriedades das estat√≠sticas de teste t e F.

### Refer√™ncias
[^8.1]: Refere-se √†s condi√ß√µes iniciais para os erros do modelo.
[^8.1.24]: Apresenta a distribui√ß√£o da estat√≠stica RSS/œÉ¬≤ sob normalidade.
[^8.1.20]: Afirma a exist√™ncia de uma matriz P que diagonaliza a matriz Mx.
[^8.1.21]: Define a propriedade da matriz P.
[^8.1.9]: Apresenta a propriedade da matriz Mx.
[^8.1.17]: Define a distribui√ß√£o do estimador OLS b sob normalidade.
[^8.1.25]: Afirma a independ√™ncia de b e s¬≤ sob normalidade.
[^8.1.26]: Define a estat√≠stica t e apresenta sua distribui√ß√£o sob H0.
[^8.1.33]: Apresenta a rela√ß√£o entre a estat√≠stica t e F.
[^8.1.25] : Apresenta a independ√™ncia entre $b$ e $\hat{u}$ sob normalidade.
<!-- END -->
