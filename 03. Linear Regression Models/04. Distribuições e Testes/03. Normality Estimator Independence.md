## Independ√™ncia entre Estimadores e Res√≠duos sob Normalidade e suas Implica√ß√µes para Testes Estat√≠sticos

### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise das propriedades do estimador de m√≠nimos quadrados ordin√°rios (OLS) sob a premissa de normalidade dos erros, concentrando-se na demonstra√ß√£o da independ√™ncia entre o estimador dos coeficientes e os res√≠duos estimados. Esta independ√™ncia √© um pilar fundamental para a deriva√ß√£o das distribui√ß√µes das estat√≠sticas *$t$* e *$F$*, conforme explorado no cap√≠tulo anterior, e para a validade dos testes de hip√≥teses em modelos de regress√£o linear. Expandindo sobre o conceito de distribui√ß√µes exatas sob normalidade, detalharemos a prova formal dessa independ√™ncia e suas implica√ß√µes.

### Independ√™ncia entre Estimadores e Res√≠duos Estimados

Como discutido anteriormente, a suposi√ß√£o de que os erros do modelo s√£o normalmente distribu√≠dos, ou seja, $u_t \sim N(0, \sigma^2)$, desempenha um papel crucial na determina√ß√£o das distribui√ß√µes exatas dos estimadores e das estat√≠sticas de teste. No contexto do modelo de regress√£o linear $y = X\beta + u$, um resultado fundamental sob essa premissa √© a independ√™ncia entre o estimador dos coeficientes $\hat{\beta}$ e o vetor de res√≠duos estimados $\hat{u}$ [^8.1.25].

Formalmente, o estimador OLS dos coeficientes √© dado por:
$$\hat{\beta} = (X'X)^{-1}X'y$$
E o vetor de res√≠duos estimados √© dado por:
$$\hat{u} = y - X\hat{\beta}$$

**Teorema 5.** Sob as condi√ß√µes cl√°ssicas de regress√£o linear, incluindo a suposi√ß√£o de que os erros s√£o normalmente distribu√≠dos, o estimador OLS dos coeficientes $\hat{\beta}$ √© independente dos res√≠duos estimados $\hat{u}$.

*Prova:*
I.  Substitu√≠mos $y = X\beta + u$ na express√£o de $\hat{\beta}$:
    $$\hat{\beta} = (X'X)^{-1}X'(X\beta + u) = \beta + (X'X)^{-1}X'u$$

II.  O vetor de res√≠duos √© dado por:
    $$\hat{u} = y - X\hat{\beta} = X\beta + u - X(\beta + (X'X)^{-1}X'u) = u - X(X'X)^{-1}X'u = Mu$$
    Onde $M = I - X(X'X)^{-1}X'$ √© a matriz de proje√ß√£o ortogonal, que √© sim√©trica e idempotente.

III.  Podemos expressar $\hat{\beta}$ e $\hat{u}$ como fun√ß√µes lineares dos erros $u$:
    $$\hat{\beta} = \beta + (X'X)^{-1}X'u$$
    $$\hat{u} = Mu$$

IV.  Para mostrar que $\hat{\beta}$ e $\hat{u}$ s√£o independentes, √© suficiente mostrar que sua covari√¢ncia √© zero, dado que ambos s√£o fun√ß√µes lineares de uma vari√°vel aleat√≥ria normal, e portanto tamb√©m seguem uma distribui√ß√£o normal.
     $$Cov(\hat{\beta}, \hat{u}) = Cov(\beta + (X'X)^{-1}X'u, Mu) = Cov((X'X)^{-1}X'u, Mu)$$

V. Expandindo a express√£o da covari√¢ncia:
     $$Cov(\hat{\beta}, \hat{u}) = (X'X)^{-1}X'Cov(u, u')M' = (X'X)^{-1}X'E[(u - E[u])(u - E[u])']M'$$

VI.  Como os erros $u$ s√£o i.i.d. com m√©dia zero e vari√¢ncia $\sigma^2$, temos que $E(uu') = \sigma^2I$, e $E[u] = 0$, logo, $Cov(u, u') = E(uu') - E(u)E(u') = \sigma^2I$.
    $$Cov(\hat{\beta}, \hat{u}) = (X'X)^{-1}X'(\sigma^2I)M' = \sigma^2(X'X)^{-1}X'M'$$
VII.  Como a matriz de proje√ß√£o ortogonal $M$ √© sim√©trica, $M' = M$. Substituindo, temos:
    $$Cov(\hat{\beta}, \hat{u}) = \sigma^2(X'X)^{-1}X'M = \sigma^2(X'X)^{-1}X'(I - X(X'X)^{-1}X')$$

VIII. Expandindo a express√£o, obtemos:
      $$Cov(\hat{\beta}, \hat{u}) = \sigma^2(X'X)^{-1}X' - \sigma^2(X'X)^{-1}X'X(X'X)^{-1}X' = \sigma^2(X'X)^{-1}X' - \sigma^2(X'X)^{-1}X' = 0$$

IX.  Como a covari√¢ncia entre $\hat{\beta}$ e $\hat{u}$ √© zero, e ambos seguem uma distribui√ß√£o normal, podemos concluir que $\hat{\beta}$ e $\hat{u}$ s√£o independentes. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Para ilustrar este teorema, considere um modelo de regress√£o com duas vari√°veis preditoras e 100 observa√ß√µes. Suponha que, ap√≥s ajustar o modelo, obtivemos os estimadores dos coeficientes $\hat{\beta} = \begin{bmatrix} 1.5 \\ 0.8 \\ -0.5 \end{bmatrix}$ e um vetor de res√≠duos $\hat{u}$.
>
> ```python
> import numpy as np
> from numpy.linalg import inv
>
> # Simula√ß√£o de dados
> np.random.seed(42)
> T = 100
> k = 3 # Inclui o intercepto
> X = np.concatenate([np.ones((T,1)), np.random.randn(T, k-1)], axis=1)
> beta_true = np.array([1, 0.5, -0.3])
> u = np.random.normal(0, 1, T) # Erros com desvio padr√£o 1
> y = X @ beta_true + u
>
> # C√°lculo dos estimadores OLS
> beta_hat = inv(X.T @ X) @ X.T @ y
> u_hat = y - X @ beta_hat
>
> print(f"Estimativas dos coeficientes: {beta_hat}")
>
> # Verifica√ß√£o da covari√¢ncia (aproximadamente zero devido √† natureza amostral)
> cov_beta_u = np.cov(beta_hat, u_hat.T)
> print(f"Covari√¢ncia entre beta_hat e u_hat: {cov_beta_u[0, 1:]}")
>
> ```
> Os resultados mostram as estimativas dos coeficientes $\hat{\beta}$ e a matriz de covari√¢ncia entre $\hat{\beta}$ e $\hat{u}$. A matriz de covari√¢ncia deve ser aproximadamente zero para confirmar a independ√™ncia. A independ√™ncia estabelecida por este teorema significa que o valor de $\hat{\beta}$ n√£o fornece nenhuma informa√ß√£o sobre o valor de $\hat{u}$ e vice-versa, o que √© fundamental para a validade de muitos resultados estat√≠sticos derivados da teoria de regress√£o linear.

**Teorema 5.1.** Uma caracteriza√ß√£o equivalente da independ√™ncia entre $\hat{\beta}$ e $\hat{u}$ √© que a matriz de covari√¢ncia entre eles √© nula.

*Prova:*
I. Do Teorema 5, demonstramos que $Cov(\hat{\beta}, \hat{u}) = 0$.

II.  A independ√™ncia entre vari√°veis aleat√≥rias implica que sua covari√¢ncia √© zero. No caso de vari√°veis aleat√≥rias com distribui√ß√£o normal multivariada, a condi√ß√£o de covari√¢ncia zero √© tamb√©m suficiente para garantir a independ√™ncia.

III. Portanto, demonstrar que a matriz de covari√¢ncia entre $\hat{\beta}$ e $\hat{u}$ √© a matriz nula √© equivalente a demonstrar a independ√™ncia entre esses dois vetores aleat√≥rios. $\blacksquare$

Este teorema formaliza a independ√™ncia entre os estimadores dos coeficientes e os res√≠duos estimados, o que √© um resultado crucial para a validade de muitos resultados estat√≠sticos na an√°lise de regress√£o linear. Essa independ√™ncia √© uma consequ√™ncia direta da suposi√ß√£o de normalidade dos erros, e √© utilizada para derivar as distribui√ß√µes exatas das estat√≠sticas *$t$* e *$F$*.

**Lema 5.1.** O estimador da vari√¢ncia do erro $s^2$ √© independente do estimador do coeficiente $\hat{\beta}$ sob normalidade dos erros.

*Prova:*
I. Vimos no Teorema 5 que os res√≠duos estimados $\hat{u}$ s√£o independentes do estimador do coeficiente $\hat{\beta}$.
    $$\hat{\beta} = \beta + (X'X)^{-1}X'u$$
    $$\hat{u} = Mu$$

II. O estimador da vari√¢ncia do erro $s^2$ √© uma fun√ß√£o dos res√≠duos estimados $\hat{u}$, dada por
    $$s^2 = \frac{\hat{u}'\hat{u}}{T-k} = \frac{u'M'Mu}{T-k} = \frac{u'Mu}{T-k}$$

III. Como $s^2$ √© uma fun√ß√£o de $\hat{u}$ e sabemos que $\hat{u}$ e $\hat{\beta}$ s√£o independentes, segue que $s^2$ e $\hat{\beta}$ tamb√©m s√£o independentes. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Considerando o mesmo exemplo num√©rico anterior, o Lema 5.1 garante que o valor de $s^2$ estimado a partir dos res√≠duos √© independente dos valores estimados dos coeficientes $\hat{\beta}$.
>
> ```python
> import numpy as np
> from numpy.linalg import inv
>
> # Simula√ß√£o de dados (reutilizando os dados do exemplo anterior)
> np.random.seed(42)
> T = 100
> k = 3 # Inclui o intercepto
> X = np.concatenate([np.ones((T,1)), np.random.randn(T, k-1)], axis=1)
> beta_true = np.array([1, 0.5, -0.3])
> u = np.random.normal(0, 1, T)
> y = X @ beta_true + u
>
> # C√°lculo dos estimadores OLS
> beta_hat = inv(X.T @ X) @ X.T @ y
> u_hat = y - X @ beta_hat
>
> # Estimador da vari√¢ncia do erro
> s2 = np.sum(u_hat**2) / (T - k)
>
> print(f"Estimativa da vari√¢ncia do erro (s^2): {s2}")
>
> # A independ√™ncia entre s2 e beta_hat √© demonstrada teoricamente.
> # Podemos verificar empiricamente que suas covari√¢ncias ser√£o pr√≥ximas de zero em v√°rias simula√ß√µes
>
> ```
> Por exemplo, o fato do estimador para um determinado coeficiente ser grande ou pequeno, n√£o implica que o erro quadr√°tico m√©dio seja maior ou menor.

**Lema 5.2.** Se $A$ √© uma matriz n√£o aleat√≥ria e $x$ e $y$ s√£o vetores aleat√≥rios independentes, ent√£o $Ax$ e $y$ s√£o tamb√©m independentes.

*Prova:*
I. A independ√™ncia entre $x$ e $y$ significa que a distribui√ß√£o conjunta $f(x,y)$ pode ser fatorada como o produto das distribui√ß√µes marginais, $f(x,y)=f(x)f(y)$.

II. Seja $z=Ax$. A distribui√ß√£o de $z$ √© uma transforma√ß√£o da distribui√ß√£o de $x$. Como $A$ √© n√£o aleat√≥ria, a depend√™ncia de $z$ de $x$ √© totalmente determin√≠stica e n√£o introduz nenhuma nova fonte de aleatoriedade.

III. Se $x$ e $y$ s√£o independentes, a distribui√ß√£o de $z$ ser√° independente de $y$, pois a transforma√ß√£o $Ax$ n√£o introduz nenhuma rela√ß√£o aleat√≥ria com $y$. Portanto, a distribui√ß√£o conjunta de $z$ e $y$ ainda ser√° o produto de suas distribui√ß√µes marginais. $\blacksquare$

Este lema estabelece a independ√™ncia entre o estimador da vari√¢ncia do erro e os estimadores dos coeficientes, um resultado essencial para demonstrar a distribui√ß√£o das estat√≠sticas t e F. A independ√™ncia permite que usemos o estimador da vari√¢ncia do erro para estimar a incerteza associada aos estimadores dos coeficientes, sem que um afete o outro.

### Implica√ß√µes para as Estat√≠sticas *$t$* e *$F$*
A independ√™ncia entre o estimador dos coeficientes $\hat{\beta}$ e os res√≠duos estimados $\hat{u}$, juntamente com a distribui√ß√£o normal dos erros, tem implica√ß√µes diretas para a distribui√ß√£o das estat√≠sticas *$t$* e *$F$*. Em particular:

*   **Distribui√ß√£o da Estat√≠stica *$t$***: A estat√≠stica *$t$* √© definida como a raz√£o entre o desvio do estimador de um coeficiente em rela√ß√£o ao seu valor sob a hip√≥tese nula e o seu erro padr√£o. A independ√™ncia de $\hat{\beta}$ e $\hat{u}$ (e, portanto, de $s^2$) permite que derivemos a distribui√ß√£o *$t$* de Student sob a hip√≥tese nula. Esta deriva√ß√£o foi apresentada no Teorema 4 no cap√≠tulo anterior e depende crucialmente da independ√™ncia do numerador ($b_i - \beta_i^0$) e do denominador ($s^2 \xi^{ii}$) da estat√≠stica t. [^8.1.26].
*   **Distribui√ß√£o da Estat√≠stica *$F$***: A estat√≠stica *$F$*, utilizada para testar hip√≥teses conjuntas sobre os coeficientes, √© constru√≠da a partir de raz√µes de estat√≠sticas qui-quadrado. A independ√™ncia entre $\hat{\beta}$ e $\hat{u}$ permite obter a distribui√ß√£o exata *$F$* sob as condi√ß√µes cl√°ssicas de regress√£o linear. Esta distribui√ß√£o foi apresentada no Teorema 4.1 no cap√≠tulo anterior, cuja deriva√ß√£o depende da independ√™ncia da estat√≠stica do numerador e do denominador da estat√≠stica *$F$*, ou seja, que os estimadores de coeficientes sob restri√ß√£o e o estimador de vari√¢ncia n√£o se influenciem mutuamente. [^8.1.32, 8.1.33]

**Lema 5.3.** A estat√≠stica t e a estat√≠stica F s√£o independentes de $s^2$.

*Prova:*
I. A estat√≠stica t √© dada por:
    $$t = \frac{b_i - \beta_i^0}{\sqrt{s^2 \xi^{ii}}}$$
    onde $b_i$ √© o i-√©simo elemento do vetor $\hat{\beta}$. Como $\hat{\beta}$ e $s^2$ s√£o independentes, e a estat√≠stica t √© uma fun√ß√£o de $\hat{\beta}$ e $s^2$, a estat√≠stica t √© independente de $s^2$.

II. Similarmente, a estat√≠stica F, dada por:
    $$F = \frac{(Rb - r)'[s^2R(X'X)^{-1}R']^{-1}(Rb - r)}{m}$$
    onde $b$ √© o vetor dos estimadores OLS $\hat{\beta}$ e $m$ o n√∫mero de restri√ß√µes. A independ√™ncia entre $\hat{\beta}$ e $s^2$ implica que a estat√≠stica F √© independente de $s^2$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> A independ√™ncia entre as estat√≠sticas de teste e $s^2$ √© fundamental para o teste de hip√≥teses. Por exemplo, considere testar a hip√≥tese nula de que o segundo coeficiente $\beta_2$ √© igual a 0 em nosso modelo.  Utilizando os dados simulados anteriormente, podemos calcular a estat√≠stica *$t$* para este teste:
>
> ```python
> import numpy as np
> from numpy.linalg import inv
>
> # Simula√ß√£o de dados (reutilizando os dados do exemplo anterior)
> np.random.seed(42)
> T = 100
> k = 3 # Inclui o intercepto
> X = np.concatenate([np.ones((T,1)), np.random.randn(T, k-1)], axis=1)
> beta_true = np.array([1, 0.5, -0.3])
> u = np.random.normal(0, 1, T)
> y = X @ beta_true + u
>
> # C√°lculo dos estimadores OLS
> beta_hat = inv(X.T @ X) @ X.T @ y
> u_hat = y - X @ beta_hat
>
> # Estimador da vari√¢ncia do erro
> s2 = np.sum(u_hat**2) / (T - k)
>
> # C√°lculo da estat√≠stica t
> se_beta2 = np.sqrt(s2 * inv(X.T @ X)[2,2]) # Erro padr√£o de beta_2
> t_stat = (beta_hat[2] - 0) / se_beta2
> print(f"Estat√≠stica t para beta_2 = 0: {t_stat}")
>
> # A independ√™ncia significa que o valor de t_stat n√£o √© influenciado diretamente pelo valor de s2
> # e permite que utilizemos a distribui√ß√£o t de Student para testes de hip√≥teses.
> ```
> O fato de termos um valor grande para a estat√≠stica *$t$*, o que poderia nos levar a rejeitar a hip√≥tese nula, √© independente do valor que $s^2$ toma. Essa independ√™ncia permite usar a distribui√ß√£o *$t$* de Student (ou F no caso do teste *$F$*) para avaliar a signific√¢ncia do resultado.

Este lema complementa a teoria, mostrando como a independ√™ncia entre os estimadores e os res√≠duos √© chave para a constru√ß√£o de testes v√°lidos. Ele demonstra que as estat√≠sticas de teste, t e F, s√£o independentes de $s^2$, e portanto seguem uma distribui√ß√£o t de Student ou F sob a hip√≥tese nula, respectivamente.

### Conclus√£o
Este cap√≠tulo estabeleceu formalmente a independ√™ncia entre o estimador OLS dos coeficientes e os res√≠duos estimados sob a suposi√ß√£o de normalidade dos erros em um modelo de regress√£o linear. Essa independ√™ncia √© um resultado fundamental, pois √© essencial para a deriva√ß√£o das distribui√ß√µes exatas das estat√≠sticas de teste *$t$* e *$F$*, e, portanto, para a validade da infer√™ncia estat√≠stica em modelos de regress√£o linear. A compreens√£o desses resultados √© crucial para a aplica√ß√£o correta e interpreta√ß√£o dos testes de hip√≥teses em econometria e outras disciplinas que utilizam modelos de regress√£o linear.

### Refer√™ncias
[^8.1.25]: Afirma a independ√™ncia de b e s¬≤ sob normalidade.
[^8.1.26]: Define a estat√≠stica t e apresenta sua distribui√ß√£o sob H0.
[^8.1.32]: Deriva a estat√≠stica F a partir de distribui√ß√µes qui-quadrado e especifica sua distribui√ß√£o sob a hip√≥tese nula.
[^8.1.33]: Mostra a rela√ß√£o entre a estat√≠stica F e a estat√≠stica t para um √∫nico coeficiente.
<!-- END -->
