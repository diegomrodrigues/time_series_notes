## IndependÃªncia entre Estimadores e ResÃ­duos sob Normalidade e suas ImplicaÃ§Ãµes para Testes EstatÃ­sticos

### IntroduÃ§Ã£o
Este capÃ­tulo aprofunda a anÃ¡lise das propriedades do estimador de mÃ­nimos quadrados ordinÃ¡rios (OLS) sob a premissa de normalidade dos erros, concentrando-se na demonstraÃ§Ã£o da independÃªncia entre o estimador dos coeficientes e os resÃ­duos estimados. Esta independÃªncia Ã© um pilar fundamental para a derivaÃ§Ã£o das distribuiÃ§Ãµes das estatÃ­sticas *$t$* e *$F$*, conforme explorado no capÃ­tulo anterior, e para a validade dos testes de hipÃ³teses em modelos de regressÃ£o linear. Expandindo sobre o conceito de distribuiÃ§Ãµes exatas sob normalidade, detalharemos a prova formal dessa independÃªncia e suas implicaÃ§Ãµes.

### IndependÃªncia entre Estimadores e ResÃ­duos Estimados

Como discutido anteriormente, a suposiÃ§Ã£o de que os erros do modelo sÃ£o normalmente distribuÃ­dos, ou seja, $u_t \sim N(0, \sigma^2)$, desempenha um papel crucial na determinaÃ§Ã£o das distribuiÃ§Ãµes exatas dos estimadores e das estatÃ­sticas de teste. No contexto do modelo de regressÃ£o linear $y = X\beta + u$, um resultado fundamental sob essa premissa Ã© a independÃªncia entre o estimador dos coeficientes $\hat{\beta}$ e o vetor de resÃ­duos estimados $\hat{u}$ [^8.1.25].

Formalmente, o estimador OLS dos coeficientes Ã© dado por:
$$\hat{\beta} = (X'X)^{-1}X'y$$
E o vetor de resÃ­duos estimados Ã© dado por:
$$\hat{u} = y - X\hat{\beta}$$

**Teorema 5.** Sob as condiÃ§Ãµes clÃ¡ssicas de regressÃ£o linear, incluindo a suposiÃ§Ã£o de que os erros sÃ£o normalmente distribuÃ­dos, o estimador OLS dos coeficientes $\hat{\beta}$ Ã© independente dos resÃ­duos estimados $\hat{u}$.

*Prova:*
I.  SubstituÃ­mos $y = X\beta + u$ na expressÃ£o de $\hat{\beta}$:
    $$\hat{\beta} = (X'X)^{-1}X'(X\beta + u) = \beta + (X'X)^{-1}X'u$$

II.  O vetor de resÃ­duos Ã© dado por:
    $$\hat{u} = y - X\hat{\beta} = X\beta + u - X(\beta + (X'X)^{-1}X'u) = u - X(X'X)^{-1}X'u = Mu$$
    Onde $M = I - X(X'X)^{-1}X'$ Ã© a matriz de projeÃ§Ã£o ortogonal, que Ã© simÃ©trica e idempotente.

III.  Podemos expressar $\hat{\beta}$ e $\hat{u}$ como funÃ§Ãµes lineares dos erros $u$:
    $$\hat{\beta} = \beta + (X'X)^{-1}X'u$$
    $$\hat{u} = Mu$$

IV.  Para mostrar que $\hat{\beta}$ e $\hat{u}$ sÃ£o independentes, Ã© suficiente mostrar que sua covariÃ¢ncia Ã© zero, dado que ambos sÃ£o funÃ§Ãµes lineares de uma variÃ¡vel aleatÃ³ria normal, e portanto tambÃ©m seguem uma distribuiÃ§Ã£o normal.
     $$Cov(\hat{\beta}, \hat{u}) = Cov(\beta + (X'X)^{-1}X'u, Mu) = Cov((X'X)^{-1}X'u, Mu)$$

V. Expandindo a expressÃ£o da covariÃ¢ncia:
     $$Cov(\hat{\beta}, \hat{u}) = (X'X)^{-1}X'Cov(u, u')M' = (X'X)^{-1}X'E[(u - E[u])(u - E[u])']M'$$

VI.  Como os erros $u$ sÃ£o i.i.d. com mÃ©dia zero e variÃ¢ncia $\sigma^2$, temos que $E(uu') = \sigma^2I$, e $E[u] = 0$, logo, $Cov(u, u') = E(uu') - E(u)E(u') = \sigma^2I$.
    $$Cov(\hat{\beta}, \hat{u}) = (X'X)^{-1}X'(\sigma^2I)M' = \sigma^2(X'X)^{-1}X'M'$$
VII.  Como a matriz de projeÃ§Ã£o ortogonal $M$ Ã© simÃ©trica, $M' = M$. Substituindo, temos:
    $$Cov(\hat{\beta}, \hat{u}) = \sigma^2(X'X)^{-1}X'M = \sigma^2(X'X)^{-1}X'(I - X(X'X)^{-1}X')$$

VIII. Expandindo a expressÃ£o, obtemos:
      $$Cov(\hat{\beta}, \hat{u}) = \sigma^2(X'X)^{-1}X' - \sigma^2(X'X)^{-1}X'X(X'X)^{-1}X' = \sigma^2(X'X)^{-1}X' - \sigma^2(X'X)^{-1}X' = 0$$

IX.  Como a covariÃ¢ncia entre $\hat{\beta}$ e $\hat{u}$ Ã© zero, e ambos seguem uma distribuiÃ§Ã£o normal, podemos concluir que $\hat{\beta}$ e $\hat{u}$ sÃ£o independentes. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Para ilustrar este teorema, considere um modelo de regressÃ£o com duas variÃ¡veis preditoras e 100 observaÃ§Ãµes. Suponha que, apÃ³s ajustar o modelo, obtivemos os estimadores dos coeficientes $\hat{\beta} = \begin{bmatrix} 1.5 \\ 0.8 \\ -0.5 \end{bmatrix}$ e um vetor de resÃ­duos $\hat{u}$.
>
> ```python
> import numpy as np
> from numpy.linalg import inv
>
> # SimulaÃ§Ã£o de dados
> np.random.seed(42)
> T = 100
> k = 3 # Inclui o intercepto
> X = np.concatenate([np.ones((T,1)), np.random.randn(T, k-1)], axis=1)
> beta_true = np.array([1, 0.5, -0.3])
> u = np.random.normal(0, 1, T) # Erros com desvio padrÃ£o 1
> y = X @ beta_true + u
>
> # CÃ¡lculo dos estimadores OLS
> beta_hat = inv(X.T @ X) @ X.T @ y
> u_hat = y - X @ beta_hat
>
> print(f"Estimativas dos coeficientes: {beta_hat}")
>
> # VerificaÃ§Ã£o da covariÃ¢ncia (aproximadamente zero devido Ã  natureza amostral)
> cov_beta_u = np.cov(beta_hat, u_hat.T)
> print(f"CovariÃ¢ncia entre beta_hat e u_hat: {cov_beta_u[0, 1:]}")
>
> ```
> Os resultados mostram as estimativas dos coeficientes $\hat{\beta}$ e a matriz de covariÃ¢ncia entre $\hat{\beta}$ e $\hat{u}$. A matriz de covariÃ¢ncia deve ser aproximadamente zero para confirmar a independÃªncia. A independÃªncia estabelecida por este teorema significa que o valor de $\hat{\beta}$ nÃ£o fornece nenhuma informaÃ§Ã£o sobre o valor de $\hat{u}$ e vice-versa, o que Ã© fundamental para a validade de muitos resultados estatÃ­sticos derivados da teoria de regressÃ£o linear.

**Teorema 5.1.** Uma caracterizaÃ§Ã£o equivalente da independÃªncia entre $\hat{\beta}$ e $\hat{u}$ Ã© que a matriz de covariÃ¢ncia entre eles Ã© nula.

*Prova:*
I. Do Teorema 5, demonstramos que $Cov(\hat{\beta}, \hat{u}) = 0$.

II.  A independÃªncia entre variÃ¡veis aleatÃ³rias implica que sua covariÃ¢ncia Ã© zero. No caso de variÃ¡veis aleatÃ³rias com distribuiÃ§Ã£o normal multivariada, a condiÃ§Ã£o de covariÃ¢ncia zero Ã© tambÃ©m suficiente para garantir a independÃªncia.

III. Portanto, demonstrar que a matriz de covariÃ¢ncia entre $\hat{\beta}$ e $\hat{u}$ Ã© a matriz nula Ã© equivalente a demonstrar a independÃªncia entre esses dois vetores aleatÃ³rios. $\blacksquare$

Este teorema formaliza a independÃªncia entre os estimadores dos coeficientes e os resÃ­duos estimados, o que Ã© um resultado crucial para a validade de muitos resultados estatÃ­sticos na anÃ¡lise de regressÃ£o linear. Essa independÃªncia Ã© uma consequÃªncia direta da suposiÃ§Ã£o de normalidade dos erros, e Ã© utilizada para derivar as distribuiÃ§Ãµes exatas das estatÃ­sticas *$t$* e *$F$*.

**Lema 5.1.** O estimador da variÃ¢ncia do erro $s^2$ Ã© independente do estimador do coeficiente $\hat{\beta}$ sob normalidade dos erros.

*Prova:*
I. Vimos no Teorema 5 que os resÃ­duos estimados $\hat{u}$ sÃ£o independentes do estimador do coeficiente $\hat{\beta}$.
    $$\hat{\beta} = \beta + (X'X)^{-1}X'u$$
    $$\hat{u} = Mu$$

II. O estimador da variÃ¢ncia do erro $s^2$ Ã© uma funÃ§Ã£o dos resÃ­duos estimados $\hat{u}$, dada por
    $$s^2 = \frac{\hat{u}'\hat{u}}{T-k} = \frac{u'M'Mu}{T-k} = \frac{u'Mu}{T-k}$$

III. Como $s^2$ Ã© uma funÃ§Ã£o de $\hat{u}$ e sabemos que $\hat{u}$ e $\hat{\beta}$ sÃ£o independentes, segue que $s^2$ e $\hat{\beta}$ tambÃ©m sÃ£o independentes. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Considerando o mesmo exemplo numÃ©rico anterior, o Lema 5.1 garante que o valor de $s^2$ estimado a partir dos resÃ­duos Ã© independente dos valores estimados dos coeficientes $\hat{\beta}$.
>
> ```python
> import numpy as np
> from numpy.linalg import inv
>
> # SimulaÃ§Ã£o de dados (reutilizando os dados do exemplo anterior)
> np.random.seed(42)
> T = 100
> k = 3 # Inclui o intercepto
> X = np.concatenate([np.ones((T,1)), np.random.randn(T, k-1)], axis=1)
> beta_true = np.array([1, 0.5, -0.3])
> u = np.random.normal(0, 1, T)
> y = X @ beta_true + u
>
> # CÃ¡lculo dos estimadores OLS
> beta_hat = inv(X.T @ X) @ X.T @ y
> u_hat = y - X @ beta_hat
>
> # Estimador da variÃ¢ncia do erro
> s2 = np.sum(u_hat**2) / (T - k)
>
> print(f"Estimativa da variÃ¢ncia do erro (s^2): {s2}")
>
> # A independÃªncia entre s2 e beta_hat Ã© demonstrada teoricamente.
> # Podemos verificar empiricamente que suas covariÃ¢ncias serÃ£o prÃ³ximas de zero em vÃ¡rias simulaÃ§Ãµes
>
> ```
> Por exemplo, o fato do estimador para um determinado coeficiente ser grande ou pequeno, nÃ£o implica que o erro quadrÃ¡tico mÃ©dio seja maior ou menor.

**Lema 5.2.** Se $A$ Ã© uma matriz nÃ£o aleatÃ³ria e $x$ e $y$ sÃ£o vetores aleatÃ³rios independentes, entÃ£o $Ax$ e $y$ sÃ£o tambÃ©m independentes.

*Prova:*
I. A independÃªncia entre $x$ e $y$ significa que a distribuiÃ§Ã£o conjunta $f(x,y)$ pode ser fatorada como o produto das distribuiÃ§Ãµes marginais, $f(x,y)=f(x)f(y)$.

II. Seja $z=Ax$. A distribuiÃ§Ã£o de $z$ Ã© uma transformaÃ§Ã£o da distribuiÃ§Ã£o de $x$. Como $A$ Ã© nÃ£o aleatÃ³ria, a dependÃªncia de $z$ de $x$ Ã© totalmente determinÃ­stica e nÃ£o introduz nenhuma nova fonte de aleatoriedade.

III. Se $x$ e $y$ sÃ£o independentes, a distribuiÃ§Ã£o de $z$ serÃ¡ independente de $y$, pois a transformaÃ§Ã£o $Ax$ nÃ£o introduz nenhuma relaÃ§Ã£o aleatÃ³ria com $y$. Portanto, a distribuiÃ§Ã£o conjunta de $z$ e $y$ ainda serÃ¡ o produto de suas distribuiÃ§Ãµes marginais. $\blacksquare$

Este lema estabelece a independÃªncia entre o estimador da variÃ¢ncia do erro e os estimadores dos coeficientes, um resultado essencial para demonstrar a distribuiÃ§Ã£o das estatÃ­sticas t e F. A independÃªncia permite que usemos o estimador da variÃ¢ncia do erro para estimar a incerteza associada aos estimadores dos coeficientes, sem que um afete o outro.

### ImplicaÃ§Ãµes para as EstatÃ­sticas *$t$* e *$F$*
A independÃªncia entre o estimador dos coeficientes $\hat{\beta}$ e os resÃ­duos estimados $\hat{u}$, juntamente com a distribuiÃ§Ã£o normal dos erros, tem implicaÃ§Ãµes diretas para a distribuiÃ§Ã£o das estatÃ­sticas *$t$* e *$F$*. Em particular:

*   **DistribuiÃ§Ã£o da EstatÃ­stica *$t$***: A estatÃ­stica *$t$* Ã© definida como a razÃ£o entre o desvio do estimador de um coeficiente em relaÃ§Ã£o ao seu valor sob a hipÃ³tese nula e o seu erro padrÃ£o. A independÃªncia de $\hat{\beta}$ e $\hat{u}$ (e, portanto, de $s^2$) permite que derivemos a distribuiÃ§Ã£o *$t$* de Student sob a hipÃ³tese nula. Esta derivaÃ§Ã£o foi apresentada no Teorema 4 no capÃ­tulo anterior e depende crucialmente da independÃªncia do numerador ($b_i - \beta_i^0$) e do denominador ($s^2 \xi^{ii}$) da estatÃ­stica t. [^8.1.26].
*   **DistribuiÃ§Ã£o da EstatÃ­stica *$F$***: A estatÃ­stica *$F$*, utilizada para testar hipÃ³teses conjuntas sobre os coeficientes, Ã© construÃ­da a partir de razÃµes de estatÃ­sticas qui-quadrado. A independÃªncia entre $\hat{\beta}$ e $\hat{u}$ permite obter a distribuiÃ§Ã£o exata *$F$* sob as condiÃ§Ãµes clÃ¡ssicas de regressÃ£o linear. Esta distribuiÃ§Ã£o foi apresentada no Teorema 4.1 no capÃ­tulo anterior, cuja derivaÃ§Ã£o depende da independÃªncia da estatÃ­stica do numerador e do denominador da estatÃ­stica *$F$*, ou seja, que os estimadores de coeficientes sob restriÃ§Ã£o e o estimador de variÃ¢ncia nÃ£o se influenciem mutuamente. [^8.1.32, 8.1.33]

**Lema 5.3.** A estatÃ­stica t e a estatÃ­stica F sÃ£o independentes de $s^2$.

*Prova:*
I. A estatÃ­stica t Ã© dada por:
    $$t = \frac{b_i - \beta_i^0}{\sqrt{s^2 \xi^{ii}}}$$
    onde $b_i$ Ã© o i-Ã©simo elemento do vetor $\hat{\beta}$. Como $\hat{\beta}$ e $s^2$ sÃ£o independentes, e a estatÃ­stica t Ã© uma funÃ§Ã£o de $\hat{\beta}$ e $s^2$, a estatÃ­stica t Ã© independente de $s^2$.

II. Similarmente, a estatÃ­stica F, dada por:
    $$F = \frac{(Rb - r)'[s^2R(X'X)^{-1}R']^{-1}(Rb - r)}{m}$$
    onde $b$ Ã© o vetor dos estimadores OLS $\hat{\beta}$ e $m$ o nÃºmero de restriÃ§Ãµes. A independÃªncia entre $\hat{\beta}$ e $s^2$ implica que a estatÃ­stica F Ã© independente de $s^2$. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:**
> A independÃªncia entre as estatÃ­sticas de teste e $s^2$ Ã© fundamental para o teste de hipÃ³teses. Por exemplo, considere testar a hipÃ³tese nula de que o segundo coeficiente $\beta_2$ Ã© igual a 0 em nosso modelo.  Utilizando os dados simulados anteriormente, podemos calcular a estatÃ­stica *$t$* para este teste:
>
> ```python
> import numpy as np
> from numpy.linalg import inv
>
> # SimulaÃ§Ã£o de dados (reutilizando os dados do exemplo anterior)
> np.random.seed(42)
> T = 100
> k = 3 # Inclui o intercepto
> X = np.concatenate([np.ones((T,1)), np.random.randn(T, k-1)], axis=1)
> beta_true = np.array([1, 0.5, -0.3])
> u = np.random.normal(0, 1, T)
> y = X @ beta_true + u
>
> # CÃ¡lculo dos estimadores OLS
> beta_hat = inv(X.T @ X) @ X.T @ y
> u_hat = y - X @ beta_hat
>
> # Estimador da variÃ¢ncia do erro
> s2 = np.sum(u_hat**2) / (T - k)
>
> # CÃ¡lculo da estatÃ­stica t
> se_beta2 = np.sqrt(s2 * inv(X.T @ X)[2,2]) # Erro padrÃ£o de beta_2
> t_stat = (beta_hat[2] - 0) / se_beta2
> print(f"EstatÃ­stica t para beta_2 = 0: {t_stat}")
>
> # A independÃªncia significa que o valor de t_stat nÃ£o Ã© influenciado diretamente pelo valor de s2
> # e permite que utilizemos a distribuiÃ§Ã£o t de Student para testes de hipÃ³teses.
> ```
> O fato de termos um valor grande para a estatÃ­stica *$t$*, o que poderia nos levar a rejeitar a hipÃ³tese nula, Ã© independente do valor que $s^2$ toma. Essa independÃªncia permite usar a distribuiÃ§Ã£o *$t$* de Student (ou F no caso do teste *$F$*) para avaliar a significÃ¢ncia do resultado.

Este lema complementa a teoria, mostrando como a independÃªncia entre os estimadores e os resÃ­duos Ã© chave para a construÃ§Ã£o de testes vÃ¡lidos. Ele demonstra que as estatÃ­sticas de teste, t e F, sÃ£o independentes de $s^2$, e portanto seguem uma distribuiÃ§Ã£o t de Student ou F sob a hipÃ³tese nula, respectivamente.

### ConclusÃ£o
Este capÃ­tulo estabeleceu formalmente a independÃªncia entre o estimador OLS dos coeficientes e os resÃ­duos estimados sob a suposiÃ§Ã£o de normalidade dos erros em um modelo de regressÃ£o linear. Essa independÃªncia Ã© um resultado fundamental, pois Ã© essencial para a derivaÃ§Ã£o das distribuiÃ§Ãµes exatas das estatÃ­sticas de teste *$t$* e *$F$*, e, portanto, para a validade da inferÃªncia estatÃ­stica em modelos de regressÃ£o linear. A compreensÃ£o desses resultados Ã© crucial para a aplicaÃ§Ã£o correta e interpretaÃ§Ã£o dos testes de hipÃ³teses em econometria e outras disciplinas que utilizam modelos de regressÃ£o linear.

### ReferÃªncias
[^8.1.25]: Afirma a independÃªncia de b e sÂ² sob normalidade.
[^8.1.26]: Define a estatÃ­stica t e apresenta sua distribuiÃ§Ã£o sob H0.
[^8.1.32]: Deriva a estatÃ­stica F a partir de distribuiÃ§Ãµes qui-quadrado e especifica sua distribuiÃ§Ã£o sob a hipÃ³tese nula.
[^8.1.33]: Mostra a relaÃ§Ã£o entre a estatÃ­stica F e a estatÃ­stica t para um Ãºnico coeficiente.
<!-- END -->
