## DistribuiÃ§Ãµes de Estimadores e EstatÃ­sticas de Teste Sob Normalidade

### IntroduÃ§Ã£o
Este capÃ­tulo aborda as distribuiÃ§Ãµes dos estimadores de mÃ­nimos quadrados ordinÃ¡rios (OLS) e das estatÃ­sticas de teste *t* e *F* sob a suposiÃ§Ã£o de normalidade dos erros. Como vimos anteriormente, a estatÃ­stica *F* serve para testar restriÃ§Ãµes lineares conjuntas sobre os coeficientes de um modelo de regressÃ£o e, no caso especial de apenas uma restriÃ§Ã£o, Ã© equivalente ao quadrado da estatÃ­stica *t* [^8.1.33]. Aqui, exploramos as distribuiÃ§Ãµes dessas estatÃ­sticas em detalhe, incluindo a distribuiÃ§Ã£o do estimador da variÃ¢ncia do erro. Expandindo o que vimos sobre testes de hipÃ³teses, agora focaremos nas distribuiÃ§Ãµes exatas sob a condiÃ§Ã£o de erros normalmente distribuÃ­dos.

### DistribuiÃ§Ã£o do Estimador da VariÃ¢ncia do Erro

Sob a premissa de que os erros $u_t$ sÃ£o independentes e identicamente distribuÃ­dos (i.i.d.) com mÃ©dia zero e variÃ¢ncia $\sigma^2$, e que seguem uma distribuiÃ§Ã£o normal, ou seja, $u_t \sim \text{N}(0, \sigma^2)$ [^8.1], podemos estabelecer resultados importantes sobre a distribuiÃ§Ã£o do estimador da variÃ¢ncia do erro.

O estimador OLS da variÃ¢ncia do erro Ã© dado por:
$$s^2 = \frac{RSS}{T-k} = \frac{\hat{u}'\hat{u}}{T-k}$$
onde $RSS$ Ã© a soma dos quadrados dos resÃ­duos, $T$ Ã© o nÃºmero de observaÃ§Ãµes e $k$ Ã© o nÃºmero de parÃ¢metros no modelo.

**Teorema 3.** Sob as condiÃ§Ãµes clÃ¡ssicas de regressÃ£o linear, incluindo a suposiÃ§Ã£o de normalidade dos erros, a estatÃ­stica $\frac{(T-k)s^2}{\sigma^2}$ segue uma distribuiÃ§Ã£o qui-quadrado com $T-k$ graus de liberdade [^8.1.24].

*Prova:*
I.  ComeÃ§amos com a expressÃ£o para a soma dos quadrados dos resÃ­duos (RSS):
   $$RSS = \hat{u}'\hat{u} = u'M_Xu$$
    onde $M_X = I - X(X'X)^{-1}X'$ Ã© a matriz de projeÃ§Ã£o ortogonal.

II.  Como $M_X$ Ã© simÃ©trica, existe uma matriz $P$ tal que $M_X = PAP'$ [^8.1.20], onde $P'P = I_T$ [^8.1.21] e $A$ Ã© uma matriz diagonal contendo os autovalores de $M_X$.

III. SubstituÃ­mos $M_X$ na expressÃ£o para RSS:
    $$RSS = u'PAP'u = (P'u)'A(P'u) = w'Aw$$
    onde $w = P'u$.

IV. Como $E(uu') = \sigma^2 I_T$, temos $E(ww') = E(P'uu'P) = \sigma^2 P'P = \sigma^2 I_T$. Isso significa que os elementos de $w$ sÃ£o nÃ£o correlacionados com mÃ©dia zero e variÃ¢ncia $\sigma^2$.

V. AlÃ©m disso, como $u$ Ã© normal, entÃ£o $w$ tambÃ©m Ã© normal. Portanto, $w_i \sim \text{N}(0, \sigma^2)$.

VI. De [^8.1.9], sabemos que $M_Xv = 0$ se $v$ for um vetor no espaÃ§o coluna de $X$, e que existem $k$ autovalores de $M_X$ iguais a zero e $T-k$ autovalores iguais a um.

VII. Assim, $A$ tem $k$ zeros e $T-k$ uns na diagonal, e entÃ£o:
    $$RSS = \sum_{i=1}^{T-k} w_i^2$$

VIII. Definindo $z_i = w_i/\sigma$, temos $z_i \sim \text{N}(0,1)$. Portanto,
    $$\frac{RSS}{\sigma^2} = \sum_{i=1}^{T-k} z_i^2$$
    segue uma distribuiÃ§Ã£o qui-quadrado com $T-k$ graus de liberdade, denotada por $\chi^2(T-k)$ [^8.1.24].

IX. Finalmente, multiplicando ambos os lados por $\frac{T-k}{T-k}$ obtemos:
  $$\frac{(T-k)s^2}{\sigma^2} = \frac{RSS}{\sigma^2} \sim \chi^2(T-k)$$
  que prova o teorema. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Suponha que temos um modelo de regressÃ£o com $T=100$ observaÃ§Ãµes e $k=5$ parÃ¢metros. ApÃ³s ajustar o modelo, obtivemos uma soma dos quadrados dos resÃ­duos (RSS) igual a 250. A variÃ¢ncia do erro estimada Ã© entÃ£o:
>
> $s^2 = \frac{RSS}{T-k} = \frac{250}{100-5} = \frac{250}{95} \approx 2.63$.
>
> Se assumirmos que a verdadeira variÃ¢ncia do erro $\sigma^2$ Ã© 2, entÃ£o a estatÃ­stica $\frac{(T-k)s^2}{\sigma^2}$ Ã©:
>
> $\frac{(100-5) \times 2.63}{2} = \frac{95 \times 2.63}{2} = 125.375$.
>
> Pelo Teorema 3, essa estatÃ­stica segue uma distribuiÃ§Ã£o qui-quadrado com $100-5 = 95$ graus de liberdade, ou seja, $\chi^2(95)$. Este valor calculado pode ser comparado com a distribuiÃ§Ã£o qui-quadrado para verificar se a variaÃ§Ã£o observada Ã© esperada sob a suposiÃ§Ã£o de normalidade dos erros.

Este teorema estabelece que o estimador da variÃ¢ncia do erro, quando escalonado, segue uma distribuiÃ§Ã£o qui-quadrado, o que Ã© crucial para inferÃªncia estatÃ­stica sobre os parÃ¢metros do modelo.

**Lema 3.1.** Sob as mesmas condiÃ§Ãµes do Teorema 3, $s^2$ Ã© um estimador nÃ£o viesado de $\sigma^2$.

*Prova:*
I. Pelo Teorema 3, temos que $\frac{(T-k)s^2}{\sigma^2} \sim \chi^2(T-k)$. A esperanÃ§a de uma variÃ¡vel aleatÃ³ria qui-quadrado com $\nu$ graus de liberdade Ã© $\nu$. Portanto,
    $$E\left[\frac{(T-k)s^2}{\sigma^2}\right] = T-k$$

II. Multiplicando ambos os lados por $\sigma^2$ temos
   $$E[(T-k)s^2] = (T-k)\sigma^2$$

III. Dividindo ambos os lados por $(T-k)$ temos
    $$E[s^2] = \sigma^2$$
    Logo, $s^2$ Ã© nÃ£o viesado para $\sigma^2$. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Continuando o exemplo anterior, sabemos que $s^2 \approx 2.63$. O lema 3.1 garante que este valor Ã© um estimador nÃ£o viesado da verdadeira variÃ¢ncia do erro $\sigma^2$. Se repetÃ­ssemos esse processo de amostragem e estimativa vÃ¡rias vezes, a mÃ©dia de todos os valores de $s^2$ obtidos se aproximaria do valor real de $\sigma^2$.

Este lema complementa o Teorema 3, demonstrando uma propriedade importante do estimador da variÃ¢ncia do erro: sua nÃ£o-viesabilidade. Este resultado Ã© essencial para garantir a validade dos testes de hipÃ³teses e intervalos de confianÃ§a baseados neste estimador.

### DistribuiÃ§Ã£o da EstatÃ­stica de Teste *t*
A estatÃ­stica *t* Ã© utilizada para testar hipÃ³teses sobre um Ãºnico coeficiente em um modelo de regressÃ£o linear. Ela Ã© definida como a razÃ£o entre o desvio do estimador do coeficiente em relaÃ§Ã£o ao seu valor sob a hipÃ³tese nula, e o erro padrÃ£o do estimador.

Sob a hipÃ³tese nula $H_0: \beta_i = \beta_i^0$ [^8.1.26], a estatÃ­stica *t* Ã© dada por:
$$t = \frac{b_i - \beta_i^0}{\sqrt{s^2 \xi^{ii}}}$$
onde:
* $b_i$ Ã© o estimador OLS do coeficiente $\beta_i$.
* $\beta_i^0$ Ã© o valor de $\beta_i$ sob a hipÃ³tese nula.
* $s^2$ Ã© o estimador da variÃ¢ncia do erro.
* $\xi^{ii}$ Ã© o i-Ã©simo elemento da diagonal da matriz $(X'X)^{-1}$ [^8.1.26].

**Teorema 4.** Sob as condiÃ§Ãµes clÃ¡ssicas de regressÃ£o linear, incluindo a suposiÃ§Ã£o de normalidade dos erros, e assumindo que $H_0$ seja verdadeira, a estatÃ­stica *t* segue uma distribuiÃ§Ã£o t de Student com $T-k$ graus de liberdade [^8.1.26].

*Prova:*
I. Sabemos que $b_i$ Ã© um estimador nÃ£o-viesado de $\beta_i$, e que sob a suposiÃ§Ã£o de normalidade, $b_i$ segue uma distribuiÃ§Ã£o normal, i.e., $b_i \sim \text{N}(\beta_i, \sigma^2 \xi^{ii})$ [^8.1.17]. Assim, $\frac{b_i - \beta_i}{\sqrt{\sigma^2 \xi^{ii}}} \sim \text{N}(0, 1)$.

II. Sob a hipÃ³tese nula $H_0: \beta_i = \beta_i^0$, temos que $\frac{b_i - \beta_i^0}{\sqrt{\sigma^2 \xi^{ii}}} \sim \text{N}(0, 1)$.

III. TambÃ©m sabemos que $(T-k)s^2/\sigma^2 \sim \chi^2(T-k)$, como demonstrado no Teorema 3.

IV. AlÃ©m disso, o estimador $b$ e o estimador $s^2$ sÃ£o independentes sob a suposiÃ§Ã£o de normalidade dos erros [^8.1.25].

V. A estatÃ­stica $t$ pode ser reescrita como:
   $$t = \frac{\frac{b_i - \beta_i^0}{\sqrt{\sigma^2 \xi^{ii}}}}{\sqrt{\frac{s^2}{\sigma^2}}} = \frac{\frac{b_i - \beta_i^0}{\sqrt{\sigma^2 \xi^{ii}}}}{\sqrt{\frac{(T-k)s^2/\sigma^2}{T-k}}}$$

VI. A expressÃ£o acima Ã© a razÃ£o entre uma variÃ¡vel aleatÃ³ria normal padronizada e a raiz quadrada de uma variÃ¡vel qui-quadrado dividida por seus graus de liberdade, o que, por definiÃ§Ã£o, segue uma distribuiÃ§Ã£o t de Student com $T-k$ graus de liberdade.
$\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Considere um modelo de regressÃ£o com $T=50$, $k=4$, e que estamos testando a hipÃ³tese nula de que o coeficiente $\beta_2$ Ã© igual a zero, i.e., $H_0: \beta_2 = 0$. Suponha que o estimador OLS do coeficiente seja $b_2 = 0.5$, a variÃ¢ncia estimada do erro seja $s^2 = 4$, e o elemento correspondente da diagonal de $(X'X)^{-1}$ seja $\xi^{22} = 0.04$. A estatÃ­stica *t* Ã© calculada como:
>
> $t = \frac{0.5 - 0}{\sqrt{4 \times 0.04}} = \frac{0.5}{\sqrt{0.16}} = \frac{0.5}{0.4} = 1.25$.
>
> Pelo Teorema 4, sob a hipÃ³tese nula, essa estatÃ­stica *t* segue uma distribuiÃ§Ã£o t de Student com $50-4 = 46$ graus de liberdade. Podemos comparar o valor observado de 1.25 com os valores crÃ­ticos da distribuiÃ§Ã£o t de Student com 46 graus de liberdade para determinar se devemos rejeitar a hipÃ³tese nula.

Este resultado Ã© fundamental para realizar testes de hipÃ³teses sobre um Ãºnico coeficiente. Ele nos permite calcular os valores-p, comparando o valor observado da estatÃ­stica *t* com os valores crÃ­ticos da distribuiÃ§Ã£o *t* de Student.

**Lema 4.1.** O estimador OLS $b_i$ Ã© independente de $\hat{u}$ sob a suposiÃ§Ã£o de normalidade dos erros.

*Prova:*
I. O estimador OLS $b$ pode ser expresso como $b = (X'X)^{-1}X'y$. Substituindo $y$ por $X\beta + u$, temos: $b = (X'X)^{-1}X'(X\beta + u) = \beta + (X'X)^{-1}X'u$.

II. O vetor de resÃ­duos Ã© dado por $\hat{u} = y - Xb = X\beta + u - X(\beta + (X'X)^{-1}X'u) = u - X(X'X)^{-1}X'u = Mu$.

III. Assim, $b = \beta + (X'X)^{-1}X'u$ e $\hat{u} = Mu$.

IV. Como $u$ segue uma distribuiÃ§Ã£o normal com mÃ©dia zero e variÃ¢ncia $\sigma^2I$, ou seja, $u \sim \text{N}(0, \sigma^2I)$, entÃ£o $b$ e $\hat{u}$ sÃ£o funÃ§Ãµes lineares de $u$ e seguem uma distribuiÃ§Ã£o normal. Para provar que $b$ e $\hat{u}$ sÃ£o independentes, basta mostrar que sua covariÃ¢ncia Ã© zero.
$$Cov(b, \hat{u}) = Cov(\beta + (X'X)^{-1}X'u, Mu) = Cov((X'X)^{-1}X'u, Mu) = (X'X)^{-1}X' Cov(u, u')M' = (X'X)^{-1}X'\sigma^2IM' = \sigma^2(X'X)^{-1}X'M'$$

V. Sabemos que $M = I - X(X'X)^{-1}X'$, entÃ£o $M' = M$.
$$Cov(b, \hat{u}) = \sigma^2(X'X)^{-1}X'(I - X(X'X)^{-1}X') = \sigma^2(X'X)^{-1}X' - \sigma^2(X'X)^{-1}X'X(X'X)^{-1}X' = \sigma^2(X'X)^{-1}X' - \sigma^2(X'X)^{-1}X' = 0$$

VI. Como a covariÃ¢ncia entre $b$ e $\hat{u}$ Ã© zero e ambos seguem uma distribuiÃ§Ã£o normal, eles sÃ£o independentes. Consequentemente, $b_i$ e $\hat{u}$ tambÃ©m sÃ£o independentes. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Este lema Ã© crucial para as deduÃ§Ãµes de distribuiÃ§Ãµes das estatÃ­sticas de teste. Para ilustrar, suponha que $X$ Ã© uma matriz de design, $y$ Ã© um vetor de resposta e $u$ Ã© o vetor de erro. Com a normalidade dos erros, $b$ e $\hat{u}$ sÃ£o independentes, o que permite, por exemplo, usar a distribuiÃ§Ã£o t de Student para construir intervalos de confianÃ§a para os coeficientes.

Este lema formaliza a independÃªncia entre o estimador dos coeficientes e os resÃ­duos, um resultado crucial para a distribuiÃ§Ã£o da estatÃ­stica *t* e, posteriormente, da estatÃ­stica *F*, sob a hipÃ³tese de normalidade dos erros. A independÃªncia Ã© fundamental para que as distribuiÃ§Ãµes das estatÃ­sticas de teste sejam vÃ¡lidas sob a hipÃ³tese nula.

### RelaÃ§Ã£o com Testes F
Como vimos, a estatÃ­stica *F* Ã© um caso geral da estatÃ­stica *t*. Especificamente, para testar a hipÃ³tese nula de que um Ãºnico coeficiente Ã© igual a um determinado valor ($H_0: \beta_i = \beta_i^0$), a estatÃ­stica *F* Ã© o quadrado da estatÃ­stica *t* [^8.1.33], como explicitado no Lema 1.1.

Se o valor calculado da estatÃ­stica *t* Ã© $t$, entÃ£o o valor da estatÃ­stica *F* correspondente Ã© $F = t^2$. Sob a hipÃ³tese nula, $t$ segue uma distribuiÃ§Ã£o t de Student com $T-k$ graus de liberdade, e *F* segue uma distribuiÃ§Ã£o F com 1 e $T-k$ graus de liberdade.

**Teorema 4.1.** Sob as mesmas condiÃ§Ãµes do Teorema 4, a estatÃ­stica $F$ para testar a restriÃ§Ã£o $H_0: \beta_i = \beta_i^0$ segue uma distribuiÃ§Ã£o $F(1, T-k)$.

*Prova:*
I. Como $F=t^2$, e $t$ segue uma distribuiÃ§Ã£o $t(T-k)$, sabemos da definiÃ§Ã£o da distribuiÃ§Ã£o F que o quadrado de uma variÃ¡vel aleatÃ³ria com distribuiÃ§Ã£o t de Student com $\nu$ graus de liberdade tem distribuiÃ§Ã£o $F(1, \nu)$. Portanto, como $t \sim t(T-k)$, entÃ£o $F = t^2 \sim F(1, T-k)$. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Retomando o exemplo anterior da estatÃ­stica *t*, onde calculamos $t = 1.25$. A estatÃ­stica *F* correspondente seria $F = t^2 = (1.25)^2 = 1.5625$. Pelo Teorema 4.1, essa estatÃ­stica *F* segue uma distribuiÃ§Ã£o F com 1 e 46 graus de liberdade (jÃ¡ que tÃ­nhamos $T-k=46$ no exemplo anterior). UsarÃ­amos essa distribuiÃ§Ã£o para encontrar o valor-p para um teste *F* e verificar se rejeitamos a hipÃ³tese nula.
>
> Em resumo, a estatÃ­stica $F$ Ã© Ãºtil ao testarmos mÃºltiplas restriÃ§Ãµes sobre os coeficientes de uma regressÃ£o linear, enquanto a estatÃ­stica $t$ Ã© usada para testar apenas um Ãºnico coeficiente.

Este teorema estabelece a distribuiÃ§Ã£o exata da estatÃ­stica F quando usada para testar uma Ãºnica restriÃ§Ã£o, mostrando que a relaÃ§Ã£o entre a estatÃ­stica t e F se mantÃ©m sob a suposiÃ§Ã£o de normalidade dos erros.

### ConclusÃ£o
Este capÃ­tulo estabeleceu a distribuiÃ§Ã£o do estimador da variÃ¢ncia do erro e da estatÃ­stica de teste *t* sob a suposiÃ§Ã£o de normalidade dos erros em um modelo de regressÃ£o linear. Especificamente, mostramos que o estimador da variÃ¢ncia do erro, escalonado, segue uma distribuiÃ§Ã£o qui-quadrado com $T-k$ graus de liberdade, e que a estatÃ­stica *t* segue uma distribuiÃ§Ã£o t de Student com $T-k$ graus de liberdade quando a hipÃ³tese nula Ã© verdadeira e o denominador Ã© um estimador da variÃ¢ncia. Esses resultados sÃ£o cruciais para a inferÃªncia estatÃ­stica em modelos de regressÃ£o linear e para o entendimento das propriedades das estatÃ­sticas de teste t e F.

### ReferÃªncias
[^8.1]: Refere-se Ã s condiÃ§Ãµes iniciais para os erros do modelo.
[^8.1.24]: Apresenta a distribuiÃ§Ã£o da estatÃ­stica RSS/ÏƒÂ² sob normalidade.
[^8.1.20]: Afirma a existÃªncia de uma matriz P que diagonaliza a matriz Mx.
[^8.1.21]: Define a propriedade da matriz P.
[^8.1.9]: Apresenta a propriedade da matriz Mx.
[^8.1.17]: Define a distribuiÃ§Ã£o do estimador OLS b sob normalidade.
[^8.1.25]: Afirma a independÃªncia de b e sÂ² sob normalidade.
[^8.1.26]: Define a estatÃ­stica t e apresenta sua distribuiÃ§Ã£o sob H0.
[^8.1.33]: Apresenta a relaÃ§Ã£o entre a estatÃ­stica t e F.
[^8.1.25] : Apresenta a independÃªncia entre $b$ e $\hat{u}$ sob normalidade.
<!-- END -->
