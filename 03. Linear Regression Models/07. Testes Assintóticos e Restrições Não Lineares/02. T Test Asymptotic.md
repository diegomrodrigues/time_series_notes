## Teste t Assintótico para Inferência em Regressão Linear

### Introdução
Este capítulo explora em detalhes o teste t, em sua forma assintótica, para inferência em modelos de regressão linear. A discussão baseia-se nos conceitos e resultados já apresentados nos capítulos anteriores, com foco em modelos que violam as premissas clássicas de regressão. Em particular, analisamos como a distribuição assintótica do teste t surge sob condições mais gerais, e como ela se relaciona com os testes de hipóteses clássicos. A análise de distribuições assintóticas é fundamental quando as premissas de normalidade e independência não são válidas ou quando temos amostras grandes.

### Conceitos Fundamentais
O teste t, conforme discutido anteriormente [^8.1.26], é uma ferramenta crucial para testar hipóteses sobre os coeficientes de um modelo de regressão linear. Em sua forma clássica, o teste t exige a suposição de que os erros são independentes e identicamente distribuídos (i.i.d.) e Gaussianos, resultando em uma distribuição t exata com $T-k$ graus de liberdade, onde $T$ é o tamanho da amostra e $k$ é o número de regressores. No entanto, quando essas premissas são violadas, a distribuição do teste t não é mais exata, e é necessário recorrer a resultados assintóticos [^8.2].

Sob a premissa de que o estimador de mínimos quadrados ordinários (OLS) *$b$* é consistente e assintoticamente normal, podemos derivar a distribuição assintótica do teste t. O teste t assintótico, então, baseia-se na seguinte estatística [^8.2.20]:

$$ t_i = \frac{b_i - \beta_i^0}{s \sqrt{\xi^{ii}}} $$

onde:
- $b_i$ é a i-ésima componente do estimador OLS dos coeficientes $\beta$.
- $\beta_i^0$ é o valor hipotético do coeficiente $\beta_i$.
- $s$ é o desvio padrão dos resíduos.
- $\xi^{ii}$ é o i-ésimo elemento diagonal da matriz $(X'X)^{-1}$ [^8.1.26].

A chave para derivar a distribuição assintótica do teste t está em reconhecer que, sob certas condições, o estimador OLS *$b$* é consistente e assintoticamente normal. Ou seja, $\sqrt{T}(b - \beta)$ converge em distribuição para uma normal com média zero e matriz de variância-covariância $\sigma^2 Q^{-1}$, como visto em [^8.2.8], onde $Q$ é o limite da matriz das segundas derivadas da função de verossimilhança.

**Lema 1**
A matriz $Q$ definida como o limite da matriz das segundas derivadas da função de verossimilhança pode também ser expressa como o limite de $\frac{X'X}{T}$, ou seja, $Q = \lim_{T \to \infty} \frac{X'X}{T}$. Este resultado é fundamental para conectar a teoria assintótica à estrutura da regressão.

*Proof:*
Provaremos que $Q = \lim_{T \to \infty} \frac{X'X}{T}$

I. A função de verossimilhança para o modelo de regressão linear com erros gaussianos é dada por
$L(\beta,\sigma^2|y,X) = (2\pi\sigma^2)^{-T/2} \exp{ \left( -\frac{1}{2\sigma^2} (y-X\beta)'(y-X\beta) \right) }$.

II. Tomando o logaritmo, obtemos
$\ell(\beta,\sigma^2|y,X) = -\frac{T}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} (y-X\beta)'(y-X\beta)$.

III. Derivando duas vezes em relação a $\beta$, temos
$\frac{\partial^2 \ell}{\partial\beta\partial\beta'} = -\frac{1}{\sigma^2} X'X$.

IV. Definindo $Q$ como o limite da matriz das segundas derivadas da função de verossimilhança, temos
$Q = \lim_{T \to \infty} -\frac{1}{T} E\left[\frac{\partial^2 \ell}{\partial\beta\partial\beta'}\right] = \lim_{T \to \infty} -\frac{1}{T} E\left[-\frac{1}{\sigma^2} X'X\right]$.

V. Simplificando e lembrando que $E\left[\frac{1}{\sigma^2}\right] = \frac{1}{\sigma^2}$:
$Q = \lim_{T \to \infty} \frac{X'X}{T}\frac{1}{\sigma^2} E[1] = \lim_{T \to \infty} \frac{X'X}{T}$.

Portanto, $Q = \lim_{T \to \infty} \frac{X'X}{T}$. ■

> 💡 **Exemplo Numérico:**
> Vamos supor que temos um modelo de regressão linear com uma variável explicativa e uma constante, onde $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$.
> Temos os seguintes dados de uma amostra pequena com T = 10:
> ```python
> import numpy as np
> import pandas as pd
>
> x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
> y = np.array([2, 4, 5, 4, 5, 7, 8, 9, 10, 11])
> X = np.column_stack((np.ones(10), x))
>
> # Calculando X'X
> XT_X = X.T @ X
> print("X'X:\n", XT_X)
>
> # Calculando (X'X)/T
> XT_X_T = XT_X / 10
> print("\n(X'X)/T:\n", XT_X_T)
>
> # Inversa de X'X
> XT_X_inv = np.linalg.inv(XT_X)
> print("\n(X'X)^-1:\n",XT_X_inv)
>
> # Inversa de (X'X)/T
> XT_X_T_inv = np.linalg.inv(XT_X_T)
> print("\n((X'X)/T)^-1:\n", XT_X_T_inv)
> ```
>
> Aqui, $X'X$ é uma matriz 2x2, e $\frac{X'X}{T}$ é o resultado de dividir cada elemento de $X'X$ pelo tamanho da amostra. No limite, essa matriz converge para a matriz Q.

Quando as suposições clássicas de regressão não se sustentam, a distribuição da estatística t não é mais uma distribuição *$t$* de Student com $T-k$ graus de liberdade, mas, em vez disso, converge para uma distribuição normal padrão *$N(0,1)$* à medida que o tamanho da amostra $T$ tende ao infinito. Essa convergência é uma consequência do teorema do limite central aplicado aos estimadores.

Formalmente, sob as condições da *Assumption 8.3* [^8.2], que incluem a independência entre as variáveis explicativas e os erros, além de algumas condições de regularidade dos momentos de x, a estatística t assintótica tem a seguinte propriedade:

$$ t_i = \frac{b_i - \beta_i^0}{s \sqrt{\xi^{ii}}} \xrightarrow{d} N(0, 1) $$

onde "$\xrightarrow{d}$" indica convergência em distribuição. A convergência em distribuição é uma propriedade crucial, pois permite utilizar a distribuição normal padrão para realizar testes de hipótese em amostras suficientemente grandes.

**Observação 1** A condição de independência entre as variáveis explicativas e os erros é crucial para garantir a consistência e normalidade assintótica do estimador OLS. A violação dessa premissa pode levar a estimadores viesados e inconsistentes. Em modelos com variáveis endógenas, é necessário o uso de métodos de variáveis instrumentais para obter estimadores consistentes.

### Teste t e a Distribuição Normal Padrão

A estatística do teste t [^8.2.20], ao ser analisada em termos de sua distribuição assintótica, depende do comportamento do estimador do coeficiente $b_i$ e da variância amostral do estimador. A estatística é construída de maneira que, sob a hipótese nula, o numerador ($b_i - \beta_i^0$) converge para zero, e o denominador, o erro padrão, converge para uma estimativa do desvio padrão do estimador.

A convergência do numerador é garantida pela consistência do estimador OLS, como demonstrado em [^8.2.5]. A convergência do denominador é garantida pelo fato de que a estimativa da variância, $s^2$, converge em probabilidade para a variância dos erros, $\sigma^2$ [^8.2.12], e também pela consistência da matriz $(X'X/T)^{-1}$ para o seu limite populacional $Q^{-1}$.

**Teorema 1.1**
A consistência de $(X'X/T)^{-1}$ implica que  $\xi^{ii}$, o i-ésimo elemento diagonal de $(X'X)^{-1}$, converge para o i-ésimo elemento diagonal da matriz $Q^{-1}/T$ .

*Proof:*
Provaremos que a consistência de $(X'X/T)^{-1}$ implica que $\xi^{ii}$ converge para o i-ésimo elemento diagonal de $Q^{-1}/T$.

I. Sabemos que $\frac{X'X}{T} \xrightarrow{p} Q$, onde $\xrightarrow{p}$ indica convergência em probabilidade.

II. Se $\frac{X'X}{T}$ converge em probabilidade para $Q$, então o inverso também converge:
$(\frac{X'X}{T})^{-1} \xrightarrow{p} Q^{-1}$.

III.  Podemos reescrever $(X'X)^{-1}$ como:
$(X'X)^{-1} = (\frac{X'X}{T}T)^{-1} = (\frac{X'X}{T})^{-1}\frac{1}{T}$.

IV. Como $(\frac{X'X}{T})^{-1} \xrightarrow{p} Q^{-1}$, segue que:
$(X'X)^{-1} \xrightarrow{p} Q^{-1}\frac{1}{T} = \frac{Q^{-1}}{T}$.

V. Portanto, o i-ésimo elemento diagonal de $(X'X)^{-1}$, denotado como $\xi^{ii}$, converge em probabilidade para o i-ésimo elemento diagonal de $\frac{Q^{-1}}{T}$.

> 💡 **Exemplo Numérico:**
> Usando os mesmos dados do exemplo anterior, vamos calcular $\xi^{ii}$. Primeiro, definimos X e calculamos $(X'X)^{-1}$:
> ```python
> import numpy as np
>
> x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
> y = np.array([2, 4, 5, 4, 5, 7, 8, 9, 10, 11])
> X = np.column_stack((np.ones(10), x))
> XT_X_inv = np.linalg.inv(X.T @ X)
> print(f"Inversa de X'X: \n {XT_X_inv}")
> ```
> $\xi^{ii}$ são os elementos diagonais de $(X'X)^{-1}$. Portanto, $\xi^{11} = 0.1467$, e $\xi^{22} = 0.02$. Estes são usados no cálculo da estatística t. Em grandes amostras, essas matrizes convergem para os seus valores populacionais divididos por T, o que garante a convergência assintótica do teste t.

Assim, como $b_i$ converge assintoticamente para a distribuição normal, e o erro padrão converge em probabilidade para o desvio padrão assintótico, a estatística do teste t converge para uma distribuição normal padrão. Isto implica que, em amostras grandes, podemos aproximar a distribuição da estatística do teste t pela normal padrão, permitindo inferências sobre os coeficientes do modelo de regressão.

### Implicações para Testes de Hipóteses
A abordagem assintótica para o teste t permite realizar testes de hipóteses sobre os coeficientes de um modelo de regressão sem depender das premissas clássicas de normalidade dos erros. O teste assintótico é essencial quando trabalhamos com dados em que a distribuição dos resíduos é desconhecida ou não normal, como frequentemente ocorre com dados financeiros ou dados de séries temporais.

**Corolário 1.1**
Em modelos de regressão com heterocedasticidade ou erros autocorrelacionados, a estatística do teste t assintótico ainda converge para uma distribuição normal padrão, desde que a matriz de variância-covariância dos erros seja estimada de forma consistente. No entanto, o estimador OLS não é mais o melhor estimador linear não viesado, e o uso de um estimador de mínimos quadrados generalizados (GLS) é mais eficiente.

> 💡 **Exemplo Numérico:**
> Suponha que ajustamos um modelo de regressão e obtivemos os seguintes resultados: $b_0 = 1.2$, $b_1 = 0.8$, com um desvio padrão dos resíduos $s=0.5$  e $\xi^{11} = 0.15$, e  $\xi^{22} = 0.02$ da matriz $(X'X)^{-1}$ (calculado anteriormente). Queremos testar a hipótese $H_0: \beta_1 = 0$ contra $H_1 : \beta_1 \neq 0$.
>
> A estatística t para $\beta_1$ é:
> $$ t_1 = \frac{b_1 - \beta_1^0}{s \sqrt{\xi^{22}}} = \frac{0.8 - 0}{0.5 \sqrt{0.02}} \approx \frac{0.8}{0.5 \times 0.141} \approx 11.35$$
>
> Usando a distribuição normal padrão, com um nível de significância $\alpha=0.05$, o valor crítico para um teste bicaudal é aproximadamente 1.96. Como $|t_1| > 1.96$, rejeitamos a hipótese nula. Isso significa que o coeficiente $\beta_1$ é estatisticamente diferente de zero.

Utilizando a distribuição normal padrão, podemos construir intervalos de confiança assintóticos para os coeficientes, bem como realizar testes de hipóteses para verificar se os coeficientes são significativamente diferentes de zero ou de qualquer outro valor especificado.

Por exemplo, ao testar a hipótese nula $H_0 : \beta_i = \beta_i^0$ contra a hipótese alternativa $H_1 : \beta_i \neq \beta_i^0$, podemos calcular a estatística de teste $t_i$, e se o valor absoluto desta estatística for maior do que um valor crítico da distribuição normal padrão, rejeitamos a hipótese nula. O valor crítico é normalmente definido com base em um nível de significância $\alpha$, como 5%.

### Conclusão
O teste t assintótico é uma ferramenta fundamental para inferência em modelos de regressão linear, especialmente em situações onde as premissas clássicas não são válidas. Ao reconhecer que o estimador OLS converge para uma distribuição normal e que sua variância pode ser estimada de forma consistente, podemos usar a distribuição normal padrão para realizar testes de hipóteses e construir intervalos de confiança. A distribuição assintótica para o teste t é essencial para o tratamento de modelos que utilizam séries temporais ou quando a amostra é grande, onde as premissas de normalidade raramente se sustentam, permitindo que a inferência estatística seja robusta e confiável.

**Proposição 1**
A utilização de erros padrão robustos à heterocedasticidade, como os erros padrão de White, permite que o teste t assintótico seja válido mesmo quando a variância dos erros não é constante. Esses erros padrão são calculados sem a necessidade de especificar a forma da heterocedasticidade, tornando o teste mais flexível em aplicações práticas.

> 💡 **Exemplo Numérico:**
>  Suponha que, após uma análise, descobrimos que nosso modelo sofre de heterocedasticidade. Usando os mesmos dados do exemplo anterior, e utilizando erros padrão robustos de White, podemos obter um novo valor para o erro padrão de $b_1$, por exemplo $se(b_1)=0.09$.
>
>  Se nosso $b_1$ continua sendo $0.8$, a estatística t usando o erro padrão robusto seria:
>  $$t_1 = \frac{0.8 - 0}{0.09} \approx 8.89$$
>
>  A conclusão ainda seria rejeitar a hipótese nula, dado que $|t_1| > 1.96$. O uso de erros padrão robustos à heterocedasticidade garante que nossos resultados de inferência sejam válidos mesmo na presença de variância não constante dos erros.

### Referências
[^8.1.26]:  Seção 8.1, página 204: Apresentação da estatística t clássica para testes de hipóteses sobre os coeficientes de um modelo de regressão linear.
[^8.2]: Seção 8.2: Exploração de modelos de regressão linear sob condições mais gerais, incluindo erros não Gaussianos e variáveis estocásticas.
[^8.2.8]: Seção 8.2, página 210: Apresentação da distribuição assintótica do estimador OLS dos coeficientes de regressão.
[^8.2.20]: Seção 8.2, página 212: Definição da estatística do teste t em sua forma assintótica.
[^8.2.5]: Seção 8.2, página 210: Discussão sobre a consistência do estimador OLS sob premissas mais gerais.
[^8.2.12]: Seção 8.2, página 211: Apresentação da convergência da estimativa da variância dos erros em modelos de regressão.
[^8.3]: Seção 8.3: Análise de estimadores de mínimos quadrados generalizados (GLS) quando a matriz de variância-covariância dos erros não é escalar.
[^8.3.5]: Seção 8.3, página 220: Apresentação do estimador de mínimos quadrados generalizados (GLS).
[^8.3.20]: Seção 8.3, página 225: Distribuição assintótica para a estimação da autocorrelação em resíduos de modelos de regressão.
[^8.3.19]: Seção 8.3, página 225: Expressão da estatística usada para a estimação de autocorrelação.
<!-- END -->
