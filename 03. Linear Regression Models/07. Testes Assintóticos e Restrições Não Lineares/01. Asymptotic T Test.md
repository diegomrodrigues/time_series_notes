## Teste t Assint√≥tico para Infer√™ncia em Regress√£o Linear

### Introdu√ß√£o
Este cap√≠tulo explora em detalhes o teste t, em sua forma assint√≥tica, para infer√™ncia em modelos de regress√£o linear. A discuss√£o baseia-se nos conceitos e resultados j√° apresentados nos cap√≠tulos anteriores, com foco em modelos que violam as premissas cl√°ssicas de regress√£o. Em particular, analisamos como a distribui√ß√£o assint√≥tica do teste t surge sob condi√ß√µes mais gerais, e como ela se relaciona com os testes de hip√≥teses cl√°ssicos. A an√°lise de distribui√ß√µes assint√≥ticas √© fundamental quando as premissas de normalidade e independ√™ncia n√£o s√£o v√°lidas ou quando temos amostras grandes.

### Conceitos Fundamentais
O teste t, conforme discutido anteriormente [^8.1.26], √© uma ferramenta crucial para testar hip√≥teses sobre os coeficientes de um modelo de regress√£o linear. Em sua forma cl√°ssica, o teste t exige a suposi√ß√£o de que os erros s√£o independentes e identicamente distribu√≠dos (i.i.d.) e Gaussianos, resultando em uma distribui√ß√£o t exata com $T-k$ graus de liberdade, onde $T$ √© o tamanho da amostra e $k$ √© o n√∫mero de regressores. No entanto, quando essas premissas s√£o violadas, a distribui√ß√£o do teste t n√£o √© mais exata, e √© necess√°rio recorrer a resultados assint√≥ticos [^8.2].

Sob a premissa de que o estimador de m√≠nimos quadrados ordin√°rios (OLS) *$b$* √© consistente e assintoticamente normal, podemos derivar a distribui√ß√£o assint√≥tica do teste t. O teste t assint√≥tico, ent√£o, baseia-se na seguinte estat√≠stica [^8.2.20]:

$$ t_i = \frac{b_i - \beta_i^0}{s \sqrt{\xi^{ii}}} $$

onde:
- $b_i$ √© a i-√©sima componente do estimador OLS dos coeficientes $\beta$.
- $\beta_i^0$ √© o valor hipot√©tico do coeficiente $\beta_i$.
- $s$ √© o desvio padr√£o dos res√≠duos.
- $\xi^{ii}$ √© o i-√©simo elemento diagonal da matriz $(X'X)^{-1}$ [^8.1.26].

A chave para derivar a distribui√ß√£o assint√≥tica do teste t est√° em reconhecer que, sob certas condi√ß√µes, o estimador OLS *$b$* √© consistente e assintoticamente normal. Ou seja, $\sqrt{T}(b - \beta)$ converge em distribui√ß√£o para uma normal com m√©dia zero e matriz de vari√¢ncia-covari√¢ncia $\sigma^2 Q^{-1}$, como visto em [^8.2.8], onde $Q$ √© o limite da matriz das segundas derivadas da fun√ß√£o de verossimilhan√ßa.

**Lema 1**
A matriz $Q$ definida como o limite da matriz das segundas derivadas da fun√ß√£o de verossimilhan√ßa pode tamb√©m ser expressa como o limite de $\frac{X'X}{T}$, ou seja, $Q = \lim_{T \to \infty} \frac{X'X}{T}$. Este resultado √© fundamental para conectar a teoria assint√≥tica √† estrutura da regress√£o.

*Proof:*
Provaremos que $Q = \lim_{T \to \infty} \frac{X'X}{T}$

I. A fun√ß√£o de verossimilhan√ßa para o modelo de regress√£o linear com erros gaussianos √© dada por
$L(\beta,\sigma^2|y,X) = (2\pi\sigma^2)^{-T/2} \exp{ \left( -\frac{1}{2\sigma^2} (y-X\beta)'(y-X\beta) \right) }$.

II. Tomando o logaritmo, obtemos
$\ell(\beta,\sigma^2|y,X) = -\frac{T}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} (y-X\beta)'(y-X\beta)$.

III. Derivando duas vezes em rela√ß√£o a $\beta$, temos
$\frac{\partial^2 \ell}{\partial\beta\partial\beta'} = -\frac{1}{\sigma^2} X'X$.

IV. Definindo $Q$ como o limite da matriz das segundas derivadas da fun√ß√£o de verossimilhan√ßa, temos
$Q = \lim_{T \to \infty} -\frac{1}{T} E\left[\frac{\partial^2 \ell}{\partial\beta\partial\beta'}\right] = \lim_{T \to \infty} -\frac{1}{T} E\left[-\frac{1}{\sigma^2} X'X\right]$.

V. Simplificando e lembrando que $E\left[\frac{1}{\sigma^2}\right] = \frac{1}{\sigma^2}$:
$Q = \lim_{T \to \infty} \frac{X'X}{T}\frac{1}{\sigma^2} E[1] = \lim_{T \to \infty} \frac{X'X}{T}$.

Portanto, $Q = \lim_{T \to \infty} \frac{X'X}{T}$. ‚ñ†

> üí° **Exemplo Num√©rico:**
> Vamos supor que temos um modelo de regress√£o linear com uma vari√°vel explicativa e uma constante, onde $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$.
> Temos os seguintes dados de uma amostra pequena com T = 10:
> ```python
> import numpy as np
> import pandas as pd
>
> x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
> y = np.array([2, 4, 5, 4, 5, 7, 8, 9, 10, 11])
> X = np.column_stack((np.ones(10), x))
>
> # Calculando X'X
> XT_X = X.T @ X
> print("X'X:\n", XT_X)
>
> # Calculando (X'X)/T
> XT_X_T = XT_X / 10
> print("\n(X'X)/T:\n", XT_X_T)
>
> # Inversa de X'X
> XT_X_inv = np.linalg.inv(XT_X)
> print("\n(X'X)^-1:\n",XT_X_inv)
>
> # Inversa de (X'X)/T
> XT_X_T_inv = np.linalg.inv(XT_X_T)
> print("\n((X'X)/T)^-1:\n", XT_X_T_inv)
> ```
>
> Aqui, $X'X$ √© uma matriz 2x2, e $\frac{X'X}{T}$ √© o resultado de dividir cada elemento de $X'X$ pelo tamanho da amostra. No limite, essa matriz converge para a matriz Q.

Quando as suposi√ß√µes cl√°ssicas de regress√£o n√£o se sustentam, a distribui√ß√£o da estat√≠stica t n√£o √© mais uma distribui√ß√£o *$t$* de Student com $T-k$ graus de liberdade, mas, em vez disso, converge para uma distribui√ß√£o normal padr√£o *$N(0,1)$* √† medida que o tamanho da amostra $T$ tende ao infinito. Essa converg√™ncia √© uma consequ√™ncia do teorema do limite central aplicado aos estimadores.

Formalmente, sob as condi√ß√µes da *Assumption 8.3* [^8.2], que incluem a independ√™ncia entre as vari√°veis explicativas e os erros, al√©m de algumas condi√ß√µes de regularidade dos momentos de x, a estat√≠stica t assint√≥tica tem a seguinte propriedade:

$$ t_i = \frac{b_i - \beta_i^0}{s \sqrt{\xi^{ii}}} \xrightarrow{d} N(0, 1) $$

onde "$\xrightarrow{d}$" indica converg√™ncia em distribui√ß√£o. A converg√™ncia em distribui√ß√£o √© uma propriedade crucial, pois permite utilizar a distribui√ß√£o normal padr√£o para realizar testes de hip√≥tese em amostras suficientemente grandes.

**Observa√ß√£o 1** A condi√ß√£o de independ√™ncia entre as vari√°veis explicativas e os erros √© crucial para garantir a consist√™ncia e normalidade assint√≥tica do estimador OLS. A viola√ß√£o dessa premissa pode levar a estimadores viesados e inconsistentes. Em modelos com vari√°veis end√≥genas, √© necess√°rio o uso de m√©todos de vari√°veis instrumentais para obter estimadores consistentes.

### Teste t e a Distribui√ß√£o Normal Padr√£o

A estat√≠stica do teste t [^8.2.20], ao ser analisada em termos de sua distribui√ß√£o assint√≥tica, depende do comportamento do estimador do coeficiente $b_i$ e da vari√¢ncia amostral do estimador. A estat√≠stica √© constru√≠da de maneira que, sob a hip√≥tese nula, o numerador ($b_i - \beta_i^0$) converge para zero, e o denominador, o erro padr√£o, converge para uma estimativa do desvio padr√£o do estimador.

A converg√™ncia do numerador √© garantida pela consist√™ncia do estimador OLS, como demonstrado em [^8.2.5]. A converg√™ncia do denominador √© garantida pelo fato de que a estimativa da vari√¢ncia, $s^2$, converge em probabilidade para a vari√¢ncia dos erros, $\sigma^2$ [^8.2.12], e tamb√©m pela consist√™ncia da matriz $(X'X/T)^{-1}$ para o seu limite populacional $Q^{-1}$.

**Teorema 1.1**
A consist√™ncia de $(X'X/T)^{-1}$ implica que  $\xi^{ii}$, o i-√©simo elemento diagonal de $(X'X)^{-1}$, converge para o i-√©simo elemento diagonal da matriz $Q^{-1}/T$ .

*Proof:*
Provaremos que a consist√™ncia de $(X'X/T)^{-1}$ implica que $\xi^{ii}$ converge para o i-√©simo elemento diagonal de $Q^{-1}/T$.

I. Sabemos que $\frac{X'X}{T} \xrightarrow{p} Q$, onde $\xrightarrow{p}$ indica converg√™ncia em probabilidade.

II. Se $\frac{X'X}{T}$ converge em probabilidade para $Q$, ent√£o o inverso tamb√©m converge:
$(\frac{X'X}{T})^{-1} \xrightarrow{p} Q^{-1}$.

III.  Podemos reescrever $(X'X)^{-1}$ como:
$(X'X)^{-1} = (\frac{X'X}{T}T)^{-1} = (\frac{X'X}{T})^{-1}\frac{1}{T}$.

IV. Como $(\frac{X'X}{T})^{-1} \xrightarrow{p} Q^{-1}$, segue que:
$(X'X)^{-1} \xrightarrow{p} Q^{-1}\frac{1}{T} = \frac{Q^{-1}}{T}$.

V. Portanto, o i-√©simo elemento diagonal de $(X'X)^{-1}$, denotado como $\xi^{ii}$, converge em probabilidade para o i-√©simo elemento diagonal de $\frac{Q^{-1}}{T}$.

> üí° **Exemplo Num√©rico:**
> Usando os mesmos dados do exemplo anterior, vamos calcular $\xi^{ii}$. Primeiro, definimos X e calculamos $(X'X)^{-1}$:
> ```python
> import numpy as np
>
> x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
> y = np.array([2, 4, 5, 4, 5, 7, 8, 9, 10, 11])
> X = np.column_stack((np.ones(10), x))
> XT_X_inv = np.linalg.inv(X.T @ X)
> print(f"Inversa de X'X: \n {XT_X_inv}")
> ```
> $\xi^{ii}$ s√£o os elementos diagonais de $(X'X)^{-1}$. Portanto, $\xi^{11} = 0.1467$, e $\xi^{22} = 0.02$. Estes s√£o usados no c√°lculo da estat√≠stica t. Em grandes amostras, essas matrizes convergem para os seus valores populacionais divididos por T, o que garante a converg√™ncia assint√≥tica do teste t.

Assim, como $b_i$ converge assintoticamente para a distribui√ß√£o normal, e o erro padr√£o converge em probabilidade para o desvio padr√£o assint√≥tico, a estat√≠stica do teste t converge para uma distribui√ß√£o normal padr√£o. Isto implica que, em amostras grandes, podemos aproximar a distribui√ß√£o da estat√≠stica do teste t pela normal padr√£o, permitindo infer√™ncias sobre os coeficientes do modelo de regress√£o.

### Implica√ß√µes para Testes de Hip√≥teses
A abordagem assint√≥tica para o teste t permite realizar testes de hip√≥teses sobre os coeficientes de um modelo de regress√£o sem depender das premissas cl√°ssicas de normalidade dos erros. O teste assint√≥tico √© essencial quando trabalhamos com dados em que a distribui√ß√£o dos res√≠duos √© desconhecida ou n√£o normal, como frequentemente ocorre com dados financeiros ou dados de s√©ries temporais.

**Corol√°rio 1.1**
Em modelos de regress√£o com heterocedasticidade ou erros autocorrelacionados, a estat√≠stica do teste t assint√≥tico ainda converge para uma distribui√ß√£o normal padr√£o, desde que a matriz de vari√¢ncia-covari√¢ncia dos erros seja estimada de forma consistente. No entanto, o estimador OLS n√£o √© mais o melhor estimador linear n√£o viesado, e o uso de um estimador de m√≠nimos quadrados generalizados (GLS) √© mais eficiente.

> üí° **Exemplo Num√©rico:**
> Suponha que ajustamos um modelo de regress√£o e obtivemos os seguintes resultados: $b_0 = 1.2$, $b_1 = 0.8$, com um desvio padr√£o dos res√≠duos $s=0.5$  e $\xi^{11} = 0.15$, e  $\xi^{22} = 0.02$ da matriz $(X'X)^{-1}$ (calculado anteriormente). Queremos testar a hip√≥tese $H_0: \beta_1 = 0$ contra $H_1 : \beta_1 \neq 0$.
>
> A estat√≠stica t para $\beta_1$ √©:
> $$ t_1 = \frac{b_1 - \beta_1^0}{s \sqrt{\xi^{22}}} = \frac{0.8 - 0}{0.5 \sqrt{0.02}} \approx \frac{0.8}{0.5 \times 0.141} \approx 11.35$$
>
> Usando a distribui√ß√£o normal padr√£o, com um n√≠vel de signific√¢ncia $\alpha=0.05$, o valor cr√≠tico para um teste bicaudal √© aproximadamente 1.96. Como $|t_1| > 1.96$, rejeitamos a hip√≥tese nula. Isso significa que o coeficiente $\beta_1$ √© estatisticamente diferente de zero.

Utilizando a distribui√ß√£o normal padr√£o, podemos construir intervalos de confian√ßa assint√≥ticos para os coeficientes, bem como realizar testes de hip√≥teses para verificar se os coeficientes s√£o significativamente diferentes de zero ou de qualquer outro valor especificado.

Por exemplo, ao testar a hip√≥tese nula $H_0 : \beta_i = \beta_i^0$ contra a hip√≥tese alternativa $H_1 : \beta_i \neq \beta_i^0$, podemos calcular a estat√≠stica de teste $t_i$, e se o valor absoluto desta estat√≠stica for maior do que um valor cr√≠tico da distribui√ß√£o normal padr√£o, rejeitamos a hip√≥tese nula. O valor cr√≠tico √© normalmente definido com base em um n√≠vel de signific√¢ncia $\alpha$, como 5%.

### Conclus√£o
O teste t assint√≥tico √© uma ferramenta fundamental para infer√™ncia em modelos de regress√£o linear, especialmente em situa√ß√µes onde as premissas cl√°ssicas n√£o s√£o v√°lidas. Ao reconhecer que o estimador OLS converge para uma distribui√ß√£o normal e que sua vari√¢ncia pode ser estimada de forma consistente, podemos usar a distribui√ß√£o normal padr√£o para realizar testes de hip√≥teses e construir intervalos de confian√ßa. A distribui√ß√£o assint√≥tica para o teste t √© essencial para o tratamento de modelos que utilizam s√©ries temporais ou quando a amostra √© grande, onde as premissas de normalidade raramente se sustentam, permitindo que a infer√™ncia estat√≠stica seja robusta e confi√°vel.

**Proposi√ß√£o 1**
A utiliza√ß√£o de erros padr√£o robustos √† heterocedasticidade, como os erros padr√£o de White, permite que o teste t assint√≥tico seja v√°lido mesmo quando a vari√¢ncia dos erros n√£o √© constante. Esses erros padr√£o s√£o calculados sem a necessidade de especificar a forma da heterocedasticidade, tornando o teste mais flex√≠vel em aplica√ß√µes pr√°ticas.

> üí° **Exemplo Num√©rico:**
>  Suponha que, ap√≥s uma an√°lise, descobrimos que nosso modelo sofre de heterocedasticidade. Usando os mesmos dados do exemplo anterior, e utilizando erros padr√£o robustos de White, podemos obter um novo valor para o erro padr√£o de $b_1$, por exemplo $se(b_1)=0.09$.
>
>  Se nosso $b_1$ continua sendo $0.8$, a estat√≠stica t usando o erro padr√£o robusto seria:
>  $$t_1 = \frac{0.8 - 0}{0.09} \approx 8.89$$
>
>  A conclus√£o ainda seria rejeitar a hip√≥tese nula, dado que $|t_1| > 1.96$. O uso de erros padr√£o robustos √† heterocedasticidade garante que nossos resultados de infer√™ncia sejam v√°lidos mesmo na presen√ßa de vari√¢ncia n√£o constante dos erros.

### Refer√™ncias
[^8.1.26]:  Se√ß√£o 8.1, p√°gina 204: Apresenta√ß√£o da estat√≠stica t cl√°ssica para testes de hip√≥teses sobre os coeficientes de um modelo de regress√£o linear.
[^8.2]: Se√ß√£o 8.2: Explora√ß√£o de modelos de regress√£o linear sob condi√ß√µes mais gerais, incluindo erros n√£o Gaussianos e vari√°veis estoc√°sticas.
[^8.2.8]: Se√ß√£o 8.2, p√°gina 210: Apresenta√ß√£o da distribui√ß√£o assint√≥tica do estimador OLS dos coeficientes de regress√£o.
[^8.2.20]: Se√ß√£o 8.2, p√°gina 212: Defini√ß√£o da estat√≠stica do teste t em sua forma assint√≥tica.
[^8.2.5]: Se√ß√£o 8.2, p√°gina 210: Discuss√£o sobre a consist√™ncia do estimador OLS sob premissas mais gerais.
[^8.2.12]: Se√ß√£o 8.2, p√°gina 211: Apresenta√ß√£o da converg√™ncia da estimativa da vari√¢ncia dos erros em modelos de regress√£o.
[^8.3]: Se√ß√£o 8.3: An√°lise de estimadores de m√≠nimos quadrados generalizados (GLS) quando a matriz de vari√¢ncia-covari√¢ncia dos erros n√£o √© escalar.
[^8.3.5]: Se√ß√£o 8.3, p√°gina 220: Apresenta√ß√£o do estimador de m√≠nimos quadrados generalizados (GLS).
[^8.3.20]: Se√ß√£o 8.3, p√°gina 225: Distribui√ß√£o assint√≥tica para a estima√ß√£o da autocorrela√ß√£o em res√≠duos de modelos de regress√£o.
[^8.3.19]: Se√ß√£o 8.3, p√°gina 225: Express√£o da estat√≠stica usada para a estima√ß√£o de autocorrela√ß√£o.
<!-- END -->
