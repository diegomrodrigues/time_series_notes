## Modelos com Erros Autoregressivos de Ordem Superior e M√°xima Verossimilhan√ßa

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre modelos de regress√£o linear e suas extens√µes, este cap√≠tulo aborda o cen√°rio em que os erros seguem um processo autoregressivo de ordem superior. Como vimos anteriormente, no caso de um processo AR(1), as estimativas de m√≠nimos quadrados ordin√°rios (OLS) podem ser inconsistentes, especialmente quando h√° vari√°veis end√≥genas defasadas [^8.2.22]. Este cen√°rio exige uma abordagem mais sofisticada, na qual a estrutura da verossimilhan√ßa do modelo √© ajustada para acomodar os par√¢metros do processo autoregressivo e, simultaneamente, obter estimativas eficientes para os par√¢metros de regress√£o.

### Conceitos Fundamentais
#### Fun√ß√£o de Verossimilhan√ßa Modificada
Quando lidamos com erros que seguem um processo autoregressivo de ordem $p$, a estrutura da matriz de covari√¢ncia dos erros, $V$, n√£o √© mais uma matriz identidade multiplicada por um escalar $\sigma^2$, mas uma matriz dependente dos par√¢metros autoregressivos $\rho_1, \rho_2, ..., \rho_p$. Nesse caso, a fun√ß√£o de verossimilhan√ßa condicional dada a matriz de regressores $X$ e assumindo que os erros s√£o gaussianos, assume a forma [^8.3.25]:

$$
\begin{aligned}
  L(\beta, \sigma^2, \rho | y, X) = & -(T/2) \log(2\pi) - (T/2) \log(\sigma^2) - (1/2) \log|V_p| \\
  & - [1/(2\sigma^2)] (y_p - X_p\beta)'V_p^{-1}(y_p - X_p\beta) \\
  & - [1/(2\sigma^2)] \sum_{t=p+1}^{T} [ (y_t - x_t'\beta) - \rho_1(y_{t-1} - x_{t-1}'\beta) - \ldots - \rho_p(y_{t-p} - x_{t-p}'\beta) ]^2
\end{aligned}
$$

onde $y_p$ denota o vetor das primeiras $p$ observa√ß√µes de $y$, $X_p$ √© a matriz de regressores correspondente a essas primeiras $p$ observa√ß√µes e $V_p$ √© a matriz de covari√¢ncia dessas primeiras $p$ observa√ß√µes, que depende dos par√¢metros $\rho_1, \rho_2, \ldots, \rho_p$. O termo $\log|V_p|$ √© o logaritmo do determinante de $V_p$, que entra na fun√ß√£o de verossimilhan√ßa em decorr√™ncia da densidade normal multivariada [^8.3.25].

Essa formula√ß√£o explicita a depend√™ncia da fun√ß√£o de verossimilhan√ßa tanto dos par√¢metros do modelo de regress√£o, $\beta$, como dos par√¢metros do processo autoregressivo, $\rho$.  A complexidade adicional reside no fato de que o determinante da matriz de covari√¢ncia dos erros,  $|V|$,  introduz  termos adicionais na fun√ß√£o de verossimilhan√ßa que dependem dos par√¢metros autorregressivos [^8.3.25].

> üí° **Exemplo Num√©rico:** Considere um modelo AR(2) onde $p=2$. Suponha que temos $T=100$ observa√ß√µes, e as primeiras duas observa√ß√µes de $y$ s√£o $y_1 = 1.5$ e $y_2 = 2.1$, e as correspondentes linhas da matriz de regressores s√£o $x_1 = [1, 0.5]$ e $x_2 = [1, 0.8]$.  Se $\beta = [0.5, 1.2]$, ent√£o $X_p\beta$ para as primeiras duas observa√ß√µes seria:
> $X_p\beta = \begin{bmatrix} 1 & 0.5 \\ 1 & 0.8 \end{bmatrix} \begin{bmatrix} 0.5 \\ 1.2 \end{bmatrix} = \begin{bmatrix} 0.5 + 0.6 \\ 0.5 + 0.96 \end{bmatrix} = \begin{bmatrix} 1.1 \\ 1.46 \end{bmatrix}$.
> Ent√£o, $y_p - X_p\beta = \begin{bmatrix} 1.5 \\ 2.1 \end{bmatrix} - \begin{bmatrix} 1.1 \\ 1.46 \end{bmatrix} = \begin{bmatrix} 0.4 \\ 0.64 \end{bmatrix}$.
>
> Al√©m disso, suponha que temos $\rho_1 = 0.6$ e $\rho_2 = 0.3$, e $\sigma^2 = 0.2$. A matriz $V_p$ para um processo AR(2) √© uma matriz $2 \times 2$ que precisa ser calculada com base na autocovari√¢ncia do processo AR(2), e vamos assumir que $V_p^{-1} = \begin{bmatrix} 2.5 & -1 \\ -1 & 2 \end{bmatrix}$.
>
> A primeira parte da fun√ß√£o de verossimilhan√ßa, com $T=100$, √©:
> $-(100/2) \log(2\pi) - (100/2) \log(0.2) = -50 \log(2\pi) - 50 \log(0.2) \approx -50(1.8379) - 50(-1.6094) \approx -91.895 + 80.47 \approx -11.425$.
> A segunda parte, usando $V_p^{-1}$ e $(y_p - X_p\beta)$, √©:
> $- [1/(2*0.2)] \begin{bmatrix} 0.4 & 0.64 \end{bmatrix} \begin{bmatrix} 2.5 & -1 \\ -1 & 2 \end{bmatrix} \begin{bmatrix} 0.4 \\ 0.64 \end{bmatrix}  = -2.5 \begin{bmatrix} 0.4 & 0.64 \end{bmatrix} \begin{bmatrix} 1 - 0.64 \\ -0.4 + 1.28 \end{bmatrix} = -2.5  \begin{bmatrix} 0.4 & 0.64 \end{bmatrix} \begin{bmatrix} 0.36 \\ 0.88 \end{bmatrix} = -2.5 (0.144 + 0.5632) = -2.5 * 0.7072 = -1.768$.
>
> Para o resto das observa√ß√µes, o termo $[ (y_t - x_t'\beta) - \rho_1(y_{t-1} - x_{t-1}'\beta) - \rho_2(y_{t-2} - x_{t-2}'\beta) ]^2$ √© calculado para cada $t$ de 3 a 100 e somado. A soma desses termos, depois de dividir por $-2\sigma^2$, completa a fun√ß√£o de verossimilhan√ßa.
>
> Este exemplo demonstra como os par√¢metros autoregressivos e os par√¢metros de regress√£o juntos afetam o valor da fun√ß√£o de verossimilhan√ßa.

**Observa√ß√£o 1.** √â importante notar que a matriz $V_p$ corresponde √† matriz de covari√¢ncia das primeiras $p$ observa√ß√µes dos erros, e a sua estrutura depende do modelo autoregressivo subjacente. Em geral, essa matriz n√£o √© uma matriz diagonal, refletindo a depend√™ncia serial dos erros. Para um processo AR(p), a estrutura de $V_p$ pode ser derivada a partir das autocovari√¢ncias do processo. Al√©m disso, √© preciso garantir que a matriz $V_p$ seja definida positiva, condi√ß√£o necess√°ria para a validade da fun√ß√£o de verossimilhan√ßa gaussiana.
 
#### Otimiza√ß√£o por M√°xima Verossimilhan√ßa
A obten√ß√£o de estimativas dos par√¢metros $\beta$, $\sigma^2$ e $\rho$ requer a maximiza√ß√£o da fun√ß√£o de verossimilhan√ßa acima, o que geralmente n√£o √© poss√≠vel de forma anal√≠tica. Em vez disso, m√©todos num√©ricos s√£o empregados. Uma abordagem √© maximizar a fun√ß√£o de verossimilhan√ßa diretamente, encontrando os valores dos par√¢metros que resultam no maior valor da fun√ß√£o [^8.3.13].

Outra abordagem, inspirada no m√©todo de Cochrane-Orcutt iterado para um processo AR(1), envolve uma estrat√©gia iterativa. Inicialmente, podemos estimar os par√¢metros de regress√£o $\beta$ por OLS. Com esses par√¢metros, estimamos os res√≠duos $\hat{u}_t = y_t - x_t'\hat{\beta}$. Usando esses res√≠duos, podemos estimar os par√¢metros do processo autoregressivo por OLS ao regredir os res√≠duos nos seus pr√≥prios valores defasados [^8.3.16], obtendo estimativas para $\rho_1, \rho_2, \ldots, \rho_p$.  Com esses par√¢metros do AR, podemos transformar a regress√£o e encontrar o estimador GLS de $\beta$ [^8.3.5]. Esse processo pode ser iterado at√© que os par√¢metros convirjam [^8.3.15].

> üí° **Exemplo Num√©rico:** Suponha que temos as seguintes observa√ß√µes para $y$ e um regressor $x$:
>
> | t | $y_t$ | $x_t$ |
> |---|---|---|
> | 1 | 2.5 | 1.2 |
> | 2 | 3.1 | 1.5 |
> | 3 | 3.8 | 1.8 |
> | 4 | 4.2 | 2.1 |
> | 5 | 4.9 | 2.4 |
>
> **Passo 1:** Estimativa inicial de $\beta$ por OLS. Usando a fun√ß√£o do numpy para regress√£o linear, obtemos uma estimativa de $\hat{\beta} \approx [0.78, 1.72]$:
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> y = np.array([2.5, 3.1, 3.8, 4.2, 4.9])
> X = np.array([[1, 1.2], [1, 1.5], [1, 1.8], [1, 2.1], [1, 2.4]])
> model = LinearRegression()
> model.fit(X, y)
> beta_hat = model.coef_
> print(f'Beta estimado (OLS): {beta_hat}')
> ```
>
> **Passo 2:** C√°lculo dos res√≠duos:
> $\hat{u}_t = y_t - x_t'\hat{\beta}$. Usando $\hat{\beta}$ do passo 1, calculamos:
>
>  | t | $y_t$ | $x_t$ | $\hat{y_t}$ | $\hat{u}_t$  |
>  |---|---|---|---|---|
>  | 1 | 2.5 | 1.2 | 0.78 + 1.72 * 1.2 = 2.84 | 2.5 - 2.84 = -0.34 |
>  | 2 | 3.1 | 1.5 | 0.78 + 1.72 * 1.5 = 3.36 | 3.1 - 3.36 = -0.26 |
>  | 3 | 3.8 | 1.8 | 0.78 + 1.72 * 1.8 = 3.876| 3.8 - 3.876 = -0.076|
>  | 4 | 4.2 | 2.1 | 0.78 + 1.72 * 2.1 = 4.392 | 4.2 - 4.392 = -0.192|
>  | 5 | 4.9 | 2.4 | 0.78 + 1.72 * 2.4 = 4.908 | 4.9 - 4.908 = -0.008|
>
> **Passo 3:** Estimativa de $\rho_1$ usando OLS nos res√≠duos. Supondo um modelo AR(1) para simplificar, regredimos $\hat{u}_t$ em $\hat{u}_{t-1}$.
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> residuals = np.array([-0.34, -0.26, -0.076, -0.192, -0.008])
> residuals_lagged = residuals[:-1]
> residuals_current = residuals[1:]
>
> X = residuals_lagged.reshape(-1, 1)
> model_ar = LinearRegression()
> model_ar.fit(X, residuals_current)
> rho_hat = model_ar.coef_[0]
> print(f'Rho estimado (AR(1)): {rho_hat}')
> ```
> $\rho_1 \approx 0.123$
>
> **Passo 4:** Transforma√ß√£o da regress√£o para obter um novo $y^*$ e $X^*$. Para um AR(1), a transforma√ß√£o √© dada por:
>
> $y_t^* = y_t - \rho_1 y_{t-1}$
> $x_t^* = x_t - \rho_1 x_{t-1}$
>
> Usando $\rho_1 \approx 0.123$, obtemos:
>
> | t | $y_t$ | $x_t$ |  $y_t^*$| $x_t^*$ |
> |---|---|---|---|---|
> | 2 | 3.1  | 1.5 |  3.1 - 0.123 * 2.5 = 2.7925 | 1.5 - 0.123*1.2 = 1.3524|
> | 3 | 3.8  | 1.8 | 3.8 - 0.123* 3.1 = 3.4183 | 1.8 - 0.123 * 1.5 = 1.6155|
> | 4 | 4.2  | 2.1 | 4.2 - 0.123 * 3.8 = 3.7326| 2.1 - 0.123 * 1.8 = 1.8786|
> | 5 | 4.9 | 2.4 | 4.9 - 0.123* 4.2 = 4.3834| 2.4 - 0.123 * 2.1 = 2.1417|
>
> **Passo 5:**  Reestimar $\beta$ usando OLS com $y^*$ e $X^*$.
>
> Este processo pode ser iterado at√© que os par√¢metros convirjam.
>
> Este exemplo demonstra como a estimativa de m√°xima verossimilhan√ßa para um modelo com erros AR(p) pode ser realizada de forma iterativa, alternando entre a estima√ß√£o dos par√¢metros de regress√£o e os par√¢metros do processo autoregressivo.

**Lema 1.** *A transforma√ß√£o da regress√£o utilizando os par√¢metros estimados do modelo AR(p) corresponde a aplicar um filtro aos dados que remove a autocorrela√ß√£o serial presente nos erros*.

*Prova:*
I. O modelo autoregressivo de ordem $p$ para os erros pode ser escrito como:
   $$u_t = \rho_1 u_{t-1} + \rho_2 u_{t-2} + \ldots + \rho_p u_{t-p} + \epsilon_t$$
   onde $\epsilon_t$ √© um erro ru√≠do branco.

II. Reorganizando a equa√ß√£o acima, temos:
    $$\epsilon_t = u_t - \rho_1 u_{t-1} - \rho_2 u_{t-2} - \ldots - \rho_p u_{t-p}$$

III. A transforma√ß√£o da regress√£o subtrai os componentes autoregressivos de $u_t$:
    $$u_t - \rho_1 u_{t-1} - \rho_2 u_{t-2} - \ldots - \rho_p u_{t-p} = \epsilon_t$$
   
IV. Aplicando esta transforma√ß√£o aos erros,  estamos isolando o componente ru√≠do branco $\epsilon_t$, que por defini√ß√£o n√£o apresenta autocorrela√ß√£o serial.
 
V. Portanto, ao aplicar a transforma√ß√£o, eliminamos a autocorrela√ß√£o nos erros, gerando um novo conjunto de erros que s√£o aproximadamente ru√≠do branco.  Assim, o modelo transformado pode ser adequadamente estimado por m√©todos de m√≠nimos quadrados generalizados (GLS). ‚ñ†

√â importante notar que, quando as vari√°veis explicativas incluem defasagens da vari√°vel dependente, os estimadores OLS para $\beta$ s√£o inconsistentes. Nesses casos, uma abordagem como a proposta por Durbin (1960) pode ser utilizada para obter estimadores consistentes dos par√¢metros do modelo e da autocorrela√ß√£o [^8.3.23, 8.3.24].

**Teorema 1.** *Sob condi√ß√µes de regularidade, e assumindo que o processo de erro √© estacion√°rio e erg√≥dico, o estimador de m√°xima verossimilhan√ßa para o modelo com erros AR(p), denotado por $\hat{\theta} = (\hat{\beta}, \hat{\sigma}^2, \hat{\rho})$, √© consistente e assintoticamente normal*.

*Prova:*
I. A consist√™ncia do estimador de m√°xima verossimilhan√ßa (EMV) significa que, √† medida que o tamanho da amostra tende ao infinito, o estimador converge em probabilidade para o verdadeiro valor do par√¢metro: $\hat{\theta} \xrightarrow{p} \theta$.

II. A normalidade assint√≥tica do EMV afirma que, para um tamanho de amostra grande, a distribui√ß√£o do estimador se aproxima de uma distribui√ß√£o normal. Mais formalmente, $\sqrt{T}(\hat{\theta} - \theta)$ converge em distribui√ß√£o para uma normal multivariada com m√©dia zero e matriz de covari√¢ncia dada pela inversa da matriz de informa√ß√£o de Fisher,  $I(\theta)$, ou seja, $\sqrt{T}(\hat{\theta} - \theta) \xrightarrow{d} N(0, I^{-1}(\theta))$.
    
III.  Para demonstrar esses resultados, precisamos verificar condi√ß√µes de regularidade que garantam que a fun√ß√£o de verossimilhan√ßa seja bem-comportada, tais como:
    * **Identificabilidade:** O verdadeiro valor do par√¢metro deve ser o √∫nico m√°ximo global da fun√ß√£o de verossimilhan√ßa.
    * **Continuidade e Diferenciabilidade:** A fun√ß√£o de verossimilhan√ßa deve ser cont√≠nua e duas vezes diferenci√°vel em rela√ß√£o aos par√¢metros.
    * **Exist√™ncia da Matriz de Informa√ß√£o:** A matriz de informa√ß√£o de Fisher, definida como o valor esperado do negativo da segunda derivada da fun√ß√£o de verossimilhan√ßa em rela√ß√£o aos par√¢metros, deve existir e ser definida positiva.
    * **Condi√ß√µes de Estacionariedade e Ergodicidade:** Para s√©ries temporais, o processo de erros deve ser estacion√°rio (suas propriedades estat√≠sticas n√£o variam no tempo) e erg√≥dico (m√©dias amostrais convergem para m√©dias populacionais).
    
IV. Se essas condi√ß√µes s√£o satisfeitas, os resultados assint√≥ticos do EMV podem ser aplicados. Em particular, a consist√™ncia decorre da converg√™ncia da fun√ß√£o de verossimilhan√ßa amostral para a fun√ß√£o de verossimilhan√ßa populacional, e a normalidade assint√≥tica decorre do teorema do limite central aplicado ao escore (a derivada da fun√ß√£o de log-verossimilhan√ßa).
 
V. A matriz de informa√ß√£o de Fisher √© usada para calcular a vari√¢ncia assint√≥tica do estimador de m√°xima verossimilhan√ßa, que √© dada por $I^{-1}(\theta) / T$ . A inversa da matriz de informa√ß√£o de Fisher representa a matriz de covari√¢ncia assint√≥tica dos estimadores. ‚ñ†

#### Considera√ß√µes Assint√≥ticas
A an√°lise assint√≥tica √© crucial em modelos com erros autoregressivos, especialmente quando os estimadores de m√°xima verossimilhan√ßa n√£o possuem distribui√ß√µes de amostra finita conhecidas. Nesses casos, demonstrar que os estimadores s√£o consistentes e que suas distribui√ß√µes convergem para distribui√ß√µes normais permite realizar testes de hip√≥tese e construir intervalos de confian√ßa.

### Conclus√£o
Modelos com erros que seguem um processo autoregressivo de ordem superior representam um avan√ßo significativo na an√°lise de s√©ries temporais. Eles permitem capturar din√¢micas complexas nos res√≠duos, que podem n√£o ser adequadamente modeladas com um processo AR(1) ou assumindo que os res√≠duos s√£o independentes e identicamente distribu√≠dos.

A complexidade da fun√ß√£o de verossimilhan√ßa exige m√©todos de otimiza√ß√£o num√©rica ou iterativos para a obten√ß√£o de estimativas eficientes dos par√¢metros.  Al√©m disso, a an√°lise assint√≥tica √© fundamental para validar a infer√™ncia estat√≠stica em modelos com estruturas de erro mais complexas. Os m√©todos discutidos neste cap√≠tulo, quando combinados com as t√©cnicas introduzidas previamente, fornecem um conjunto de ferramentas poderoso para lidar com modelos de regress√£o em s√©ries temporais com erros autorregressivos de ordem superior.

### Refer√™ncias
[^8.2.22]:  ... *Se√ß√£o do texto anterior sobre modelos com vari√°veis end√≥genas.*
[^8.3.25]:  *Equa√ß√£o da fun√ß√£o de verossimilhan√ßa condicional para um processo autoregressivo de ordem p.*
[^8.3.13]: *Discuss√£o sobre a maximiza√ß√£o da fun√ß√£o de verossimilhan√ßa.*
[^8.3.5]:  *Defini√ß√£o do estimador GLS.*
[^8.3.16]: *Estimativa do par√¢metro de autocorrela√ß√£o usando OLS nos res√≠duos.*
[^8.3.15]: *M√©todo de Cochrane-Orcutt iterado para processos AR(1).*
[^8.3.23, 8.3.24]: *Abordagem de Durbin (1960) para modelos com vari√°veis end√≥genas defasadas.*
<!-- END -->
