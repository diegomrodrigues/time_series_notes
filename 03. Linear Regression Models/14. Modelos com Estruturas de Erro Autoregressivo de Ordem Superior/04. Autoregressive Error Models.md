## Modelos com Erros Autoregressivos de Ordem Superior e M√©todos de Estima√ß√£o Iterativos e GLS

### Introdu√ß√£o
Este cap√≠tulo, dando continuidade √† nossa discuss√£o sobre modelos de regress√£o com erros autoregressivos de ordem superior e estacionariedade, explora m√©todos pr√°ticos para estimar esses modelos. Como vimos anteriormente, a estima√ß√£o por m√°xima verossimilhan√ßa (MV) √© uma abordagem geral, mas que pode envolver c√°lculos complexos [^8.3.13]. Abordaremos aqui dois m√©todos alternativos, o m√©todo de Cochrane-Orcutt iterado e a t√©cnica de m√≠nimos quadrados generalizados (GLS), que s√£o particularmente √∫teis para lidar com a estrutura de covari√¢ncia autoregressiva dos erros. Veremos tamb√©m como esses m√©todos se relacionam com a estima√ß√£o por MV.

### M√©todos de Estima√ß√£o Iterativos

#### M√©todo de Cochrane-Orcutt Iterado
O m√©todo de Cochrane-Orcutt iterado √© uma abordagem pr√°tica para estimar modelos com erros autoregressivos. Como vimos na se√ß√£o anterior, no caso de erros AR(1), esse m√©todo consiste em alternar entre a estima√ß√£o dos par√¢metros de regress√£o e do par√¢metro autorregressivo [^8.3.15]. Para um processo autoregressivo de ordem $p$, AR($p$), o m√©todo pode ser generalizado de forma a estimar os $p$ par√¢metros autoregressivos em conjunto.

O procedimento iterativo pode ser resumido nos seguintes passos:
1. **Estima√ß√£o Inicial por OLS:** Estimar os par√¢metros de regress√£o $\beta$ usando m√≠nimos quadrados ordin√°rios (OLS), desconsiderando a estrutura de autocorrela√ß√£o nos erros. Os res√≠duos obtidos dessa regress√£o ser√£o usados no pr√≥ximo passo.

2. **Estima√ß√£o dos Par√¢metros AR:** Usando os res√≠duos obtidos no passo 1, estimar os par√¢metros do processo autoregressivo $\rho_1, \rho_2, \ldots, \rho_p$ por meio de uma regress√£o OLS. Essa regress√£o envolve regredir os res√≠duos sobre seus pr√≥prios valores defasados. A escolha do n√∫mero de defasagens no modelo autoregressivo precisa ser definida a priori.

3. **Transforma√ß√£o da Regress√£o:** Usando os par√¢metros AR estimados no passo 2, transformar as vari√°veis do modelo de regress√£o para remover a autocorrela√ß√£o serial dos erros.  Essa transforma√ß√£o √© dada por:

    $$
    \begin{aligned}
    y_t^* &= y_t - \rho_1 y_{t-1} - \rho_2 y_{t-2} - \ldots - \rho_p y_{t-p} \\
    x_t^* &= x_t - \rho_1 x_{t-1} - \rho_2 x_{t-2} - \ldots - \rho_p x_{t-p}
     \end{aligned}
    $$
    Note que essa transforma√ß√£o envolve defasagens da vari√°vel dependente e dos regressores. Em um contexto em que existem poucas observa√ß√µes (como em muitos estudos econom√©tricos), uma abordagem comum √© a de iniciar a transforma√ß√£o a partir de $t = p+1$, onde $p$ √© a ordem do modelo autoregressivo.

4.  **Reestima√ß√£o por OLS:**  Regredir $y_t^*$ sobre $x_t^*$ usando OLS para obter uma nova estimativa para os par√¢metros de regress√£o, $\beta$.

5. **Itera√ß√£o:** Repetir os passos 2 a 4 at√© que os par√¢metros estimados convirjam, ou seja, a diferen√ßa entre as estimativas em sucessivas itera√ß√µes se torne menor do que um determinado limiar.

> üí° **Exemplo Num√©rico:** Vamos expandir o exemplo num√©rico do cap√≠tulo anterior e ilustrar a aplica√ß√£o do m√©todo de Cochrane-Orcutt iterado para um modelo com erros AR(2), usando os mesmos dados:
>
> | t | $y_t$ | $x_t$ |
> |---|---|---|
> | 1 | 2.5 | 1.2 |
> | 2 | 3.1 | 1.5 |
> | 3 | 3.8 | 1.8 |
> | 4 | 4.2 | 2.1 |
> | 5 | 4.9 | 2.4 |
>
> **Passo 1:** (J√° realizado no cap√≠tulo anterior). Obtemos uma estimativa inicial de $\beta$ por OLS: $\hat{\beta} \approx [0.78, 1.72]$ e calculamos os res√≠duos  $\hat{u}_t = y_t - x_t'\hat{\beta}$.
>
>
> | t | $y_t$ | $x_t$ | $\hat{y_t}$ | $\hat{u}_t$  |
>  |---|---|---|---|---|
>  | 1 | 2.5 | 1.2 | 0.78 + 1.72 * 1.2 = 2.84 | 2.5 - 2.84 = -0.34 |
>  | 2 | 3.1 | 1.5 | 0.78 + 1.72 * 1.5 = 3.36 | 3.1 - 3.36 = -0.26 |
>  | 3 | 3.8 | 1.8 | 0.78 + 1.72 * 1.8 = 3.876| 3.8 - 3.876 = -0.076|
>  | 4 | 4.2 | 2.1 | 0.78 + 1.72 * 2.1 = 4.392 | 4.2 - 4.392 = -0.192|
>  | 5 | 4.9 | 2.4 | 0.78 + 1.72 * 2.4 = 4.908 | 4.9 - 4.908 = -0.008|
>
>
> **Passo 2:** Estimativa dos par√¢metros do AR(2) usando OLS nos res√≠duos. Para um processo AR(2), precisamos regredir os res√≠duos $\hat{u}_t$ sobre $\hat{u}_{t-1}$ e $\hat{u}_{t-2}$. Note que para o c√°lculo dos par√¢metros AR(2), precisamos come√ßar do res√≠duo 3, j√° que o res√≠duo 1 n√£o tem defasagens, e o res√≠duo 2 tem somente 1 defasagem. O numpy n√£o tem uma fun√ß√£o padr√£o para regress√£o com mais de um regressor e intercepto, ent√£o podemos usar o statsmodels.api:
>
> ```python
> import numpy as np
> import statsmodels.api as sm
>
> residuals = np.array([-0.34, -0.26, -0.076, -0.192, -0.008])
> residuals_lagged_1 = residuals[:-1]
> residuals_lagged_2 = residuals[:-2]
> residuals_current = residuals[2:]
>
> X = np.column_stack((residuals_lagged_1[1:], residuals_lagged_2))
> X = sm.add_constant(X) # adicionar intercepto
> model_ar = sm.OLS(residuals_current, X)
> results_ar = model_ar.fit()
> rho_hat = results_ar.params[1:]
> print(f'Rho estimado (AR(2)): {rho_hat}')
> ```
>
>   $\rho \approx [-0.717, 0.543]$.
>
> **Passo 3:** Transforma√ß√£o da regress√£o usando $\rho_1$ e $\rho_2$. Para cada t, precisamos calcular:
>
> $y_t^* = y_t - \rho_1 y_{t-1} - \rho_2 y_{t-2}$
> $x_t^* = x_t - \rho_1 x_{t-1} - \rho_2 x_{t-2}$
>
> Aplicando a transforma√ß√£o nos dados, temos que come√ßar em t = 3. Para t=3, temos:
> $y_3^* = 3.8 - (-0.717) * 3.1 - 0.543 * 2.5 = 3.8 + 2.2227 - 1.3575 = 4.6652$
> $x_3^* = 1.8 - (-0.717) * 1.5 - 0.543 * 1.2 = 1.8 + 1.0755 - 0.6516 = 2.2239$
> Para t=4:
> $y_4^* = 4.2 - (-0.717) * 3.8 - 0.543 * 3.1 = 4.2 + 2.7246 - 1.6833 = 5.2413$
> $x_4^* = 2.1 - (-0.717) * 1.8 - 0.543 * 1.5 = 2.1 + 1.2906 - 0.8145 = 2.5761$
> E para t=5:
> $y_5^* = 4.9 - (-0.717) * 4.2 - 0.543 * 3.8 = 4.9 + 3.0114 - 2.0634 = 5.848$
> $x_5^* = 2.4 - (-0.717) * 2.1 - 0.543 * 1.8 = 2.4 + 1.5057 - 0.9774 = 2.9283$
>
> Os novos dados transformados s√£o:
>
> | t | $y_t^*$ | $x_t^*$ |
> |---|---|---|
> | 3 | 4.6652 | 2.2239 |
> | 4 | 5.2413 | 2.5761 |
> | 5 | 5.848  | 2.9283 |
>
>
> **Passo 4:** Reestimar $\beta$ usando OLS com $y^*$ e $X^*$:
>
> ```python
> from sklearn.linear_model import LinearRegression
>
> y_transformed = np.array([4.6652, 5.2413, 5.848])
> X_transformed = np.array([[1, 2.2239], [1, 2.5761], [1, 2.9283]])
>
> model_transformed = LinearRegression()
> model_transformed.fit(X_transformed, y_transformed)
> beta_hat_transformed = model_transformed.coef_
> print(f'Beta estimado (transformado): {beta_hat_transformed}')
> ```
>
>  $\beta \approx [0.253, 1.91]$.
>
>
> **Passo 5:** Repetir os passos 2-4 at√© convergir.
>
> Depois de algumas itera√ß√µes, o m√©todo de Cochrane-Orcutt convergiria para uma estimativa dos par√¢metros $\beta$ e $\rho$ que maximizam aproximadamente a fun√ß√£o de verossimilhan√ßa.

**Observa√ß√£o 1**:  O m√©todo de Cochrane-Orcutt √© conhecido por apresentar algumas limita√ß√µes. Por exemplo, ele pode convergir para um m√°ximo local da fun√ß√£o de verossimilhan√ßa, ao inv√©s do m√°ximo global. Para mitigar esse problema, √© recomend√°vel usar diferentes valores iniciais para os par√¢metros do modelo autoregressivo e comparar os resultados. Al√©m disso, o m√©todo pode apresentar um desempenho ruim em amostras pequenas e quando as ra√≠zes do polin√¥mio autoregressivo est√£o pr√≥ximas do c√≠rculo unit√°rio. No entanto, quando implementado com aten√ß√£o, o m√©todo pode fornecer estimativas razo√°veis e consistentes para os par√¢metros do modelo.

#### Rela√ß√£o com a Estimativa de M√°xima Verossimilhan√ßa
O m√©todo de Cochrane-Orcutt iterado √©, em ess√™ncia, uma aproxima√ß√£o para o m√©todo de m√°xima verossimilhan√ßa.  Como vimos anteriormente, a estima√ß√£o por m√°xima verossimilhan√ßa envolve encontrar os valores dos par√¢metros $\beta, \rho_1, \rho_2, \ldots, \rho_p$ que maximizam a fun√ß√£o de verossimilhan√ßa condicional [^8.3.25]. O m√©todo iterado busca um ponto pr√≥ximo ao m√°ximo da fun√ß√£o de verossimilhan√ßa ao alternar entre a estima√ß√£o dos par√¢metros de regress√£o (condicional nos par√¢metros AR) e a estima√ß√£o dos par√¢metros AR (condicional nos par√¢metros de regress√£o).

Formalmente, o m√©todo de Cochrane-Orcutt √© equivalente a maximizar o logaritmo da fun√ß√£o de verossimilhan√ßa condicional, dada as primeiras $p$ observa√ß√µes:
$$
\begin{aligned}
 L_c(\beta, \sigma^2, \rho | y, X) \approx & -[(T-p)/2]\log(2\pi) - [(T-p)/2]\log(\sigma^2)  \\
  & - [1/(2\sigma^2)] \sum_{t=p+1}^{T} [ (y_t - x_t'\beta) - \rho_1(y_{t-1} - x_{t-1}'\beta) - \ldots - \rho_p(y_{t-p} - x_{t-p}'\beta) ]^2
\end{aligned}
$$

Esta aproxima√ß√£o ignora o termo do log-determinante da matriz de covari√¢ncia $\log|V_p|$, e maximiza a fun√ß√£o condicional nos erros transformados a partir de $t = p + 1$ [^8.3.25]. Contudo, em amostras grandes, a diferen√ßa entre as abordagens assintoticamente desaparece.  Na pr√°tica, m√©todos que maximizam a verossimilhan√ßa diretamente, como m√©todos baseados no algoritmo de Newton-Raphson, podem apresentar uma converg√™ncia mais r√°pida.

**Proposi√ß√£o 1**: *Sob condi√ß√µes de regularidade, o estimador de Cochrane-Orcutt iterado √© consistente e assintoticamente normal*.

*Prova:*
A prova da consist√™ncia e normalidade assint√≥tica do estimador de Cochrane-Orcutt requer t√©cnicas de an√°lise assint√≥tica, incluindo o teorema de Slutsky e o teorema do limite central. A ideia geral √© mostrar que as itera√ß√µes do algoritmo convergem para os verdadeiros valores dos par√¢metros e que a distribui√ß√£o assint√≥tica do estimador converge para uma distribui√ß√£o normal.  Como o estimador √© uma aproxima√ß√£o para a estima√ß√£o por m√°xima verossimilhan√ßa, em condi√ß√µes de regularidade, ele herda as propriedades assint√≥ticas do estimador de m√°xima verossimilhan√ßa. Uma prova detalhada pode ser encontrada em livros avan√ßados sobre econometria.
‚ñ†

### M√≠nimos Quadrados Generalizados (GLS)
Outra abordagem para estimar modelos com erros autoregressivos √© usar a t√©cnica de m√≠nimos quadrados generalizados (GLS). A t√©cnica GLS leva em considera√ß√£o a estrutura de covari√¢ncia dos erros e √© eficiente quando essa estrutura √© conhecida ou bem estimada.

#### Deriva√ß√£o do Estimador GLS
A ideia central do GLS √© transformar as vari√°veis originais ($y$ e $X$) de forma que o modelo transformado apresente erros com uma matriz de covari√¢ncia escalar, o que permite que o modelo seja estimado por OLS.

Seja o modelo de regress√£o com erros com estrutura autoregressiva:
$$ y = X\beta + u $$

onde $u \sim N(0, \sigma^2V)$, e $V$ √© a matriz de covari√¢ncia dos erros que depende dos par√¢metros do processo autoregressivo.  Como $V$ √© uma matriz sim√©trica e definida positiva, existe uma matriz $L$ tal que $V^{-1} = L'L$.
Ent√£o, multiplicando o modelo original por $L$, obtemos:
$$ Ly = LX\beta + Lu$$
Definindo $\tilde{y} = Ly$, $\tilde{X} = LX$ e $\tilde{u} = Lu$, o modelo transformado pode ser escrito como:
$$ \tilde{y} = \tilde{X}\beta + \tilde{u} $$

Neste caso, $\tilde{u}$ tem m√©dia zero e matriz de covari√¢ncia dada por:
$$E(\tilde{u}\tilde{u}') = E(Lu u'L') = L E(uu') L' = L \sigma^2 V L' = \sigma^2 I_T$$
onde $I_T$ √© a matriz identidade $T \times T$. Assim, o modelo transformado apresenta erros com matriz de covari√¢ncia escalar, o que o torna adequado para estima√ß√£o por OLS. O estimador GLS √© ent√£o dado por:
$$ \hat{\beta}_{GLS} = (\tilde{X}'\tilde{X})^{-1}\tilde{X}'\tilde{y} = (X'L'LX)^{-1}X'L'Ly = (X'V^{-1}X)^{-1}X'V^{-1}y$$
[^8.3.5]

A matriz $V$ depende dos par√¢metros autoregressivos, que podem ser estimados usando o procedimento de Cochrane-Orcutt, ou por meio de qualquer outro m√©todo consistente.

> üí° **Exemplo Num√©rico:** Vamos considerar um exemplo simples com erros AR(1) e ilustrar como o estimador GLS √© calculado. Suponha que temos as seguintes matrizes $X$ e $y$ e que estimamos $\rho=0.7$:
>
> $$
> X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \\ 1 & 5 \end{bmatrix}, \quad
> y = \begin{bmatrix} 3 \\ 5 \\ 6 \\ 8 \\ 9 \end{bmatrix}
> $$
>
> Primeiro, precisamos construir a matriz de covari√¢ncia $V$ para um processo AR(1). Para um AR(1), a matriz V √© dada por:
>
> $$ V = \frac{1}{1-\rho^2} \begin{bmatrix} 1 & \rho & \rho^2 & \cdots & \rho^{T-1} \\ \rho & 1 & \rho & \cdots & \rho^{T-2} \\ \rho^2 & \rho & 1 & \cdots & \rho^{T-3} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ \rho^{T-1} & \rho^{T-2} & \rho^{T-3} & \cdots & 1 \end{bmatrix} $$
>
> Para T=5 e $\rho=0.7$, temos
>
> ```python
> import numpy as np
>
> def ar1_covariance_matrix(rho, T):
>  V = np.zeros((T, T))
>  for i in range(T):
>   for j in range(T):
>    V[i, j] = rho**abs(i-j)
>  V = V / (1 - rho**2)
>  return V
>
> rho = 0.7
> T = 5
> V = ar1_covariance_matrix(rho, T)
> print("Matriz de covari√¢ncia V:\n", V)
> ```
>
> $V \approx \begin{bmatrix} 2.04 & 1.428 & 1.000 & 0.700 & 0.490 \\ 1.428 & 2.04 & 1.428 & 1.000 & 0.700 \\ 1.000 & 1.428 & 2.04 & 1.428 & 1.000 \\ 0.700 & 1.000 & 1.428 & 2.04 & 1.428 \\ 0.490 & 0.700 & 1.000 & 1.428 & 2.04 \end{bmatrix}$
>
> Agora, calculamos $V^{-1}$:
> ```python
> V_inv = np.linalg.inv(V)
> print("Matriz inversa de V:\n", V_inv)
> ```
>
> $V^{-1} \approx \begin{bmatrix} 1.70 & -1.20 & 0.00 & 0.00 & 0.00 \\ -1.20 & 2.19 & -1.20 & 0.00 & 0.00 \\ 0.00 & -1.20 & 2.19 & -1.20 & 0.00 \\ 0.00 & 0.00 & -1.20 & 2.19 & -1.20 \\ 0.00 & 0.00 & 0.00 & -1.20 & 1.70 \end{bmatrix}$
>
> Com $V^{-1}$, podemos calcular o estimador GLS: $\hat{\beta}_{GLS} = (X'V^{-1}X)^{-1}X'V^{-1}y$
> ```python
> X = np.array([[1, 1], [1, 2], [1, 3], [1, 4], [1, 5]])
> y = np.array([3, 5, 6, 8, 9])
>
> beta_gls = np.linalg.inv(X.T @ V_inv @ X) @ X.T @ V_inv @ y
> print("Estimativa GLS de beta:\n", beta_gls)
> ```
>
> $\hat{\beta}_{GLS} \approx [2.05, 1.14]$
>
>  Isso significa que, considerando a estrutura de autocorrela√ß√£o dos erros, a inclina√ß√£o estimada da regress√£o √© de 1.14 e o intercepto √© de 2.05.

**Proposi√ß√£o 2**: *Se a matriz de covari√¢ncia $V$ √© conhecida, o estimador GLS, $\hat{\beta}_{GLS}$, √© o estimador linear n√£o viesado de m√≠nima vari√¢ncia condicional em X*.

*Prova:*
I. O estimador GLS pode ser obtido por meio de uma transforma√ß√£o linear do modelo original. Defina um estimador linear $\hat{\beta} = Ay$, onde $A$ √© uma matriz que define um estimador linear dos par√¢metros $\beta$.
II. Para que $\hat{\beta}$ seja n√£o viesado, precisamos que $E(\hat{\beta}|X) = \beta$. Substituindo $y = X\beta + u$, temos $E(\hat{\beta}|X) = E(A(X\beta + u)|X) = AX\beta$.  Assim, para que $AX\beta = \beta$ para qualquer valor de $\beta$, precisamos que $AX = I$, onde $I$ √© a matriz identidade.
III. A vari√¢ncia condicional do estimador linear √© dada por $Var(\hat{\beta}|X) = E[(\hat{\beta}-E(\hat{\beta}))(\hat{\beta}-E(\hat{\beta}))'|X] = E[Auu'A'|X]=E[A E[uu'|X] A' = \sigma^2 A V A'$.
IV. O problema de encontrar o melhor estimador linear n√£o viesado √© equivalente a minimizar $A V A'$ sujeito √† restri√ß√£o de que $AX=I$.
V. Usando o m√©todo dos multiplicadores de Lagrange, definimos a fun√ß√£o:
$J(A, \lambda) = tr(AVA') + 2tr(\lambda' (I-AX))$, onde $tr$ √© o tra√ßo de uma matriz, e $\lambda$ s√£o os multiplicadores de Lagrange.

VI. Otimizando em rela√ß√£o a A e $\lambda$, temos: $\frac{\partial J}{\partial A} = 2AVA' - 2\lambda X' = 0$, o que implica que $AVA' = \lambda X'$ , e tamb√©m temos $\frac{\partial J}{\partial \lambda} = 2(I-AX) = 0$.
VII. Combinando as equa√ß√µes, temos que $A = V^{-1}X (X'V^{-1}X)^{-1}$.
VIII. O melhor estimador linear n√£o viesado √© ent√£o definido por $\hat{\beta}_{GLS} = A y = (X'V^{-1}X)^{-1}X'V^{-1}y$.
‚ñ†

**Lema 1**: *O estimador GLS, $\hat{\beta}_{GLS}$, √© consistente se a matriz de covari√¢ncia V for estimada consistentemente*.

*Prova:*
A prova da consist√™ncia do estimador GLS quando V √© estimada consistentemente segue do fato de que, sob condi√ß√µes de regularidade, o estimador de m√°xima verossimilhan√ßa √© consistente. Se a matriz V √© estimada por um m√©todo consistente, como o m√©todo de Cochrane-Orcutt iterado, e essa estimativa converge em probabilidade para V, ent√£o o estimador GLS tamb√©m converge para o verdadeiro valor do par√¢metro, demonstrando sua consist√™ncia. Mais precisamente, se $\hat{V} \xrightarrow{p} V$ (converge em probabilidade), ent√£o $\hat{\beta}_{GLS} = (X'\hat{V}^{-1}X)^{-1}X'\hat{V}^{-1}y \xrightarrow{p} \beta$.
‚ñ†

#### Implica√ß√µes Pr√°ticas do GLS

1.  **Efici√™ncia:** O estimador GLS √© eficiente, o que significa que ele tem a menor vari√¢ncia entre todos os estimadores lineares n√£o viesados quando a matriz de covari√¢ncia dos erros √© conhecida.
2.  **Robustez:** O GLS √© mais robusto a heterocedasticidade e autocorrela√ß√£o do que OLS, pois ele leva em considera√ß√£o a estrutura de covari√¢ncia dos erros, quando essa √© conhecida ou estimada consistentemente.
3. **Complexidade Computacional:** A implementa√ß√£o do GLS requer a invers√£o da matriz $V$, que pode ser computacionalmente intensiva em modelos com muitas observa√ß√µes, apesar de existir uma vasta gama de algoritmos para obter invers√µes eficientemente, e em geral computadores atuais n√£o imp√µem um grande problema de tempo de processamento.
4. **Depend√™ncia da Matriz V:** A efici√™ncia do GLS depende da precis√£o com que a matriz de covari√¢ncia $V$ √© especificada ou estimada. Se $V$ for mal especificada, o estimador GLS pode perder efici√™ncia ou at√© mesmo se tornar viesado.

**Corol√°rio 1**:  *Sob as condi√ß√µes do Lema 1, o estimador GLS com uma matriz V estimada consistentemente √© assintoticamente equivalente ao estimador de m√°xima verossimilhan√ßa*.

*Prova:*
Este corol√°rio segue do fato de que, sob condi√ß√µes de regularidade, tanto o estimador GLS (com V estimada consistentemente) como o estimador de m√°xima verossimilhan√ßa s√£o consistentes e assintoticamente normais. Ambos os m√©todos convergem para o verdadeiro valor dos par√¢metros e possuem a mesma distribui√ß√£o assint√≥tica. A intui√ß√£o por tr√°s desse corol√°rio √© que, quando a matriz de covari√¢ncia V √© bem estimada, o estimador GLS √© capaz de capturar toda a informa√ß√£o relevante sobre a estrutura dos erros, assim como a estima√ß√£o por m√°xima verossimilhan√ßa.
‚ñ†

### Conclus√£o

Este cap√≠tulo abordou os m√©todos de Cochrane-Orcutt iterado e m√≠nimos quadrados generalizados (GLS) para estimar modelos com erros autoregressivos de ordem superior. O m√©todo de Cochrane-Orcutt, apesar de iterativo, √© f√°cil de implementar e fornece estimativas razo√°veis dos par√¢metros. O GLS, por outro lado, oferece um estimador mais eficiente, que leva em considera√ß√£o a estrutura de covari√¢ncia dos erros, quando essa √© conhecida ou estimada consistentemente.

A escolha entre esses m√©todos depender√° das caracter√≠sticas espec√≠ficas do modelo, do tamanho da amostra, da precis√£o com que a estrutura de covari√¢ncia dos erros pode ser especificada e da complexidade computacional. Em muitos casos, uma combina√ß√£o desses m√©todos pode ser a melhor abordagem, como o uso do m√©todo iterado para obter uma estimativa inicial dos par√¢metros autoregressivos, que ser√£o usados na t√©cnica de GLS. No entanto, √© importante lembrar que tanto o GLS como o m√©todo de Cochrane-Orcutt s√£o aproxima√ß√µes da m√°xima verossimilhan√ßa, e, portanto, todos os tr√™s m√©todos s√£o v√°lidos em contextos assint√≥ticos.

### Refer√™ncias

[^8.3.13]: *Discuss√£o sobre a maximiza√ß√£o da fun√ß√£o de verossimilhan√ßa.*
[^8.3.15]: *M√©todo de Cochrane-Orcutt iterado para processos AR(1).*
[^8.3.25]:  *Equa√ß√£o da fun√ß√£o de verossimilhan√ßa condicional para um processo autoregressivo de ordem p.*
[^8.3.5]:  *Defini√ß√£o do estimador GLS.*
<!-- END -->
