## Testes F e RestriÃ§Ãµes Lineares: Uma AnÃ¡lise Detalhada da ComparaÃ§Ã£o de Modelos
### IntroduÃ§Ã£o
Este capÃ­tulo aprofunda o conceito de **testes F** em regressÃ£o linear, com Ãªnfase em como comparar modelos com e sem restriÃ§Ãµes. Conectando com o tÃ³pico anterior de **inferÃªncia estatÃ­stica em modelos de regressÃ£o linear** e **testes t para hipÃ³teses sobre os coeficientes**, agora exploramos como avaliar a significÃ¢ncia conjunta de mÃºltiplas restriÃ§Ãµes lineares sobre os parÃ¢metros do modelo. O foco serÃ¡ na interpretaÃ§Ã£o da estatÃ­stica F como uma medida da melhoria na adequaÃ§Ã£o do modelo ao incorporar um conjunto de restriÃ§Ãµes. Este capÃ­tulo se baseia em conceitos jÃ¡ apresentados [^1], [^2], [^3], [^4], e expande sobre eles para fornecer uma compreensÃ£o mais profunda de como o teste F Ã© utilizado na prÃ¡tica.

### Conceitos Fundamentais
Em regressÃ£o linear, muitas vezes nos deparamos com a necessidade de testar hipÃ³teses que envolvem mÃºltiplas restriÃ§Ãµes sobre os coeficientes do modelo. Como vimos anteriormente, o **teste t** Ã© apropriado para testar hipÃ³teses sobre um Ãºnico coeficiente. No entanto, para testar restriÃ§Ãµes lineares conjuntas, como, por exemplo, a igualdade entre dois coeficientes, o **teste F** Ã© a ferramenta adequada.  O teste F nos permite comparar a qualidade do ajuste de um modelo irrestrito com a de um modelo que incorpora restriÃ§Ãµes lineares, determinando se a imposiÃ§Ã£o dessas restriÃ§Ãµes leva a uma perda significativa na qualidade do ajuste do modelo.

Formalmente, considere o modelo de regressÃ£o linear:
$$
y = X\beta + u,
$$
onde $y$ Ã© o vetor de variÃ¡veis dependentes, $X$ Ã© a matriz de regressores, $\beta$ Ã© o vetor de parÃ¢metros e $u$ Ã© o vetor de erros. Suponha que desejamos testar um conjunto de $m$ restriÃ§Ãµes lineares sobre $\beta$, expressas como:

$$
H_0: R\beta = r, \qquad [8.1.27]
$$

onde $R$ Ã© uma matriz conhecida de dimensÃ£o $(m \times k)$, e $r$ Ã© um vetor conhecido de dimensÃ£o $(m \times 1)$. Por exemplo, para testar a hipÃ³tese que $\beta_1 = \beta_2$, terÃ­amos $m = 1$,
$$R = \begin{bmatrix} 1 & -1 & 0 & \ldots & 0 \end{bmatrix}$$ e $r=0$ [^4].

O teste F compara a soma de quadrados dos resÃ­duos (RSS) de um modelo **nÃ£o-restringido**, ou seja, um modelo que estima $\beta$ sem nenhuma restriÃ§Ã£o ($RSS_1$), com a RSS de um modelo **restringido** ($RSS_0$), no qual $\beta$ Ã© estimado sob a restriÃ§Ã£o imposta por $H_0$.  A **estatÃ­stica F** Ã© definida como:

$$
F = \frac{(RSS_0 - RSS_1)/m}{RSS_1/(T - k)}, \qquad [8.1.37]
$$

onde $T$ Ã© o nÃºmero de observaÃ§Ãµes, e $k$ Ã© o nÃºmero de parÃ¢metros no modelo original.

Essa estatÃ­stica segue uma distribuiÃ§Ã£o $F$ com $m$ graus de liberdade no numerador e $T-k$ graus de liberdade no denominador, sob a hipÃ³tese nula de que as restriÃ§Ãµes sÃ£o verdadeiras [^4]. Uma grande estatÃ­stica F indica que a imposiÃ§Ã£o das restriÃ§Ãµes resulta em uma perda significativa na qualidade do ajuste, sugerindo que a hipÃ³tese nula deve ser rejeitada.

**Lema 1** (DecomposiÃ§Ã£o da Soma de Quadrados): A soma total de quadrados (TSS) em uma regressÃ£o linear pode ser decomposta na soma de quadrados explicada pelo modelo (ESS) e na soma de quadrados dos resÃ­duos (RSS), ou seja, $TSS = ESS + RSS$. AlÃ©m disso, no contexto de comparaÃ§Ã£o de modelos com e sem restriÃ§Ãµes, temos que $TSS$ Ã© constante. Portanto, minimizar $RSS$ Ã© equivalente a maximizar $ESS$.

*Proof Strategy*: This result is a standard property of linear regression and is a consequence of the orthogonality between fitted values and residuals. The fact that $TSS$ remains constant across different model specifications allows us to focus on changes in the $RSS$ when evaluating different models, as the increase in ESS can only be achieved by decreasing the RSS.

**Prova do Lema 1:**
I. A soma total de quadrados (TSS) Ã© definida como a soma dos quadrados das diferenÃ§as entre cada valor de $y_i$ e a mÃ©dia de $y$, $\bar{y}$:
    $$TSS = \sum_{i=1}^T (y_i - \bar{y})^2$$
II. A soma de quadrados explicada pelo modelo (ESS) Ã© a soma dos quadrados das diferenÃ§as entre os valores ajustados, $\hat{y}_i$, e a mÃ©dia de $y$, $\bar{y}$:
    $$ESS = \sum_{i=1}^T (\hat{y}_i - \bar{y})^2$$
III. A soma de quadrados dos resÃ­duos (RSS) Ã© a soma dos quadrados dos resÃ­duos:
    $$RSS = \sum_{i=1}^T (y_i - \hat{y}_i)^2$$
IV. Para provar que $TSS = ESS + RSS$, podemos usar o fato que $y_i - \bar{y} = (y_i - \hat{y}_i) + (\hat{y}_i - \bar{y})$ e entÃ£o expandir $TSS$:
    $$TSS = \sum_{i=1}^T [(y_i - \hat{y}_i) + (\hat{y}_i - \bar{y})]^2$$
V. Expandindo o quadrado, temos:
    $$TSS = \sum_{i=1}^T (y_i - \hat{y}_i)^2 + 2\sum_{i=1}^T (y_i - \hat{y}_i)(\hat{y}_i - \bar{y}) + \sum_{i=1}^T (\hat{y}_i - \bar{y})^2$$
VI. O termo do meio desaparece devido Ã  propriedade de ortogonalidade entre os resÃ­duos e os valores ajustados, ou seja, $\sum_{i=1}^T (y_i - \hat{y}_i)(\hat{y}_i - \bar{y}) = 0$. Assim,
$$TSS = RSS + ESS$$
VII. Dado que a $TSS$ depende apenas dos dados $y_i$ e $\bar{y}$, ela Ã© constante para diferentes modelos lineares estimados usando os mesmos dados $y$, ou seja, mesmo quando impomos restriÃ§Ãµes, a $TSS$ nÃ£o muda.
VIII. Consequentemente, como $TSS = ESS + RSS$, e $TSS$ Ã© constante, minimizar $RSS$ implica maximizar $ESS$, e vice-versa. â– 

**DerivaÃ§Ã£o da EstatÃ­stica F por meio da ComparaÃ§Ã£o de Somas de Quadrados Residuais:**

A estatÃ­stica F pode ser calculada atravÃ©s da comparaÃ§Ã£o das somas de quadrados residuais (RSS) dos modelos irrestrito e restrito. Para isso, consideremos $b$ como a estimativa de mÃ­nimos quadrados ordinÃ¡rios (OLS) irrestrita [8.1.6] e $b^*$ a estimativa sob as restriÃ§Ãµes [8.1.35], com os respectivos $RSS_1$ e $RSS_0$. As somas de quadrados residuais sÃ£o definidas como:

$$
RSS_1 = \sum_{t=1}^{T} (y_t - x_t'b)^2, \qquad [8.1.35]
$$
$$
RSS_0 = \sum_{t=1}^{T} (y_t - x_t'b^*)^2, \qquad [8.1.36]
$$

A estatÃ­stica F, de acordo com [8.1.37],  Ã© dada por:

$$
F = \frac{(RSS_0 - RSS_1)/m}{RSS_1/(T - k)}, \qquad [8.1.37]
$$

onde $m$ representa o nÃºmero de restriÃ§Ãµes impostas e $T-k$ os graus de liberdade.

O teorema fundamental para esse cÃ¡lculo (provado em detalhes no ApÃªndice 8.A [^2]) estabelece que:

$$
RSS_0 - RSS_1 = (b - b^*)'X'X(b - b^*), \qquad [8.A.6]
$$

onde $b$ Ã© o estimador OLS nÃ£o-restringido e $b^*$ Ã© o estimador OLS restrito. Esta relaÃ§Ã£o Ã© crucial porque nos permite expressar a diferenÃ§a nas somas de quadrados residuais como uma funÃ§Ã£o da diferenÃ§a nos coeficientes estimados.  A estatÃ­stica F baseia-se entÃ£o na razÃ£o entre a diferenÃ§a nas somas de quadrados residuais (normalizada pelo nÃºmero de restriÃ§Ãµes), e a soma de quadrados residuais do modelo irrestrito normalizada por seus graus de liberdade.

**ProposiÃ§Ã£o 1:** A estatÃ­stica F pode ser expressa alternativamente usando a matriz de projeÃ§Ã£o. Seja $P = X(X'X)^{-1}X'$ a matriz de projeÃ§Ã£o em $X$ e seja $M = I - P$ a matriz de projeÃ§Ã£o ortogonal. EntÃ£o, podemos escrever:
$RSS_1 = u'Mu$,  onde $u = y - Xb$ sÃ£o os resÃ­duos do modelo irrestrito,  e $RSS_0 - RSS_1 = (b-b^*)'X'X(b-b^*)$.
*Proof Strategy*: Using the properties of $P$ and $M$, and the fact that $u$ is orthogonal to the fitted values, we can rewrite the sum of squared errors in terms of matrix operations.
**Prova da ProposiÃ§Ã£o 1:**
I.  Definimos $P = X(X'X)^{-1}X'$ como a matriz de projeÃ§Ã£o no espaÃ§o coluna de $X$. Portanto, $Xb = Py$, onde $b = (X'X)^{-1}X'y$ Ã© o estimador OLS nÃ£o-restrito.
II. Definimos $M = I - P$ como a matriz de projeÃ§Ã£o ortogonal no espaÃ§o nulo de $X$. 
III. Os resÃ­duos do modelo irrestrito sÃ£o dados por $u = y - Xb = y - Py = (I-P)y = My$.
IV. A soma dos quadrados dos resÃ­duos (RSS) para o modelo irrestrito Ã© dada por $RSS_1 = u'u = (My)'(My) = y'M'My$.
V. Como $M$ Ã© uma matriz de projeÃ§Ã£o, Ã© idempotente, ou seja, $M'M = M^2 = M$. Portanto, $RSS_1 = y'My$.
VI. TambÃ©m, $u'Mu = (My)'M(My) = y'M'My = y'My$. Logo, $RSS_1 = u'Mu$.
VII.  Usando o resultado [8.A.6], temos que $RSS_0 - RSS_1 = (b - b^*)'X'X(b - b^*)$.â– 
**CorolÃ¡rio 1:** A estatÃ­stica F pode tambÃ©m ser escrita como:
$$F = \frac{ \frac{ (b-b^*)'X'X(b-b^*)}{m}}{\frac{u'Mu}{T-k}}$$
 Esta formulaÃ§Ã£o da estatÃ­stica F evidencia a comparaÃ§Ã£o entre a variaÃ§Ã£o explicada pelas restriÃ§Ãµes e a variaÃ§Ã£o inexplicada do modelo irrestrito.

**Exemplo PrÃ¡tico:**

Para ilustrar como usar este teste, consideremos a situaÃ§Ã£o na qual  o modelo de regressÃ£o inclui 4 regressores ($k=4$) e queremos testar a hipÃ³tese nula de que os coeficientes dos regressores 3 e 4 sÃ£o simultaneamente iguais a zero. Ou seja:

$$
H_0: \beta_3 = 0 \text{ e } \beta_4 = 0
$$
Nesse caso, o nÃºmero de restriÃ§Ãµes ($m$) Ã© 2. Primeiramente, estima-se um modelo OLS irrestrito ($RSS_1$) incluindo todos os 4 regressores e, em seguida, estima-se o modelo restrito ($RSS_0$) excluindo os regressores 3 e 4 do modelo. Substituindo os valores de $RSS_0$ e $RSS_1$ na equaÃ§Ã£o [8.1.37], teremos uma estatÃ­stica F que podemos comparar com a distribuiÃ§Ã£o F apropriada.

Por exemplo, se o tamanho da amostra for $T=50$ e a estatÃ­stica F calculada for maior que 3.20 (o valor crÃ­tico de 5\% para uma distribuiÃ§Ã£o $F(2, 46)$), rejeitarÃ­amos a hipÃ³tese nula e concluirÃ­amos que os regressores 3 e 4 juntos sÃ£o significativamente diferentes de zero.

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos considerar um exemplo com dados simulados para tornar o cÃ¡lculo da estatÃ­stica F mais concreto. Suponha que temos $T=100$ observaÃ§Ãµes e 4 regressores (incluindo a constante), ou seja $k=4$. O modelo de regressÃ£o linear Ã©:
>
> $$y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + u_i$$
>
> Queremos testar a hipÃ³tese nula de que $\beta_3 = 0$ e $\beta_4 = 0$, entÃ£o temos $m=2$ restriÃ§Ãµes.
>
> **Passo 1: SimulaÃ§Ã£o dos Dados**
>
> Primeiro, vamos criar dados simulados usando Python.
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> # Define a semente para reprodutibilidade
> np.random.seed(42)
>
> # Gera os dados simulados
> T = 100 # NÃºmero de observaÃ§Ãµes
> X = np.random.rand(T, 4) # Matriz de regressores (inclui a constante)
> X[:, 0] = 1  # Adicionando a coluna de 1s para a constante
> beta = np.array([2, 3, -1, 0.5]) # Define os verdadeiros valores dos parÃ¢metros
> u = np.random.normal(0, 1, T) # Erro aleatÃ³rio
> y = np.dot(X, beta) + u
> ```
>
> **Passo 2: Estimar o modelo irrestrito**
>
> Agora, vamos ajustar um modelo OLS irrestrito usando todos os regressores e calcular $RSS_1$.
>
> ```python
> # Modelo OLS irrestrito
> model_irrestrito = LinearRegression(fit_intercept=False)
> model_irrestrito.fit(X, y)
> y_hat_irrestrito = model_irrestrito.predict(X)
> residuos_irrestrito = y - y_hat_irrestrito
> RSS1 = np.sum(residuos_irrestrito**2)
> print(f"RSS_1: {RSS1:.2f}")
> ```
>
> **Passo 3: Estimar o modelo restrito**
>
> Em seguida, ajustamos o modelo restrito excluindo os regressores 3 e 4 ($x_3$ e $x_4$) e calculamos $RSS_0$.
>
> ```python
> # Modelo OLS restrito
> X_restrito = X[:, 0:3] # Remove as colunas 3 e 4 da matriz de regressores
> model_restrito = LinearRegression(fit_intercept=False)
> model_restrito.fit(X_restrito, y)
> y_hat_restrito = model_restrito.predict(X_restrito)
> residuos_restrito = y - y_hat_restrito
> RSS0 = np.sum(residuos_restrito**2)
> print(f"RSS_0: {RSS0:.2f}")
> ```
>
> **Passo 4: Calcular a EstatÃ­stica F**
>
> Agora, podemos calcular a estatÃ­stica F usando a fÃ³rmula [8.1.37].
>
> ```python
> # Calcula a estatÃ­stica F
> m = 2  # NÃºmero de restriÃ§Ãµes
> T = 100 # NÃºmero de observaÃ§Ãµes
> k = 4  # NÃºmero de parÃ¢metros no modelo irrestrito
> F_stat = ((RSS0 - RSS1) / m) / (RSS1 / (T - k))
> print(f"EstatÃ­stica F: {F_stat:.2f}")
> ```
>
> **Passo 5: Comparar com o valor crÃ­tico**
>
> O valor crÃ­tico da distribuiÃ§Ã£o F com $m=2$ e $T-k=96$ graus de liberdade para $\alpha = 0.05$ (nÃ­vel de significÃ¢ncia de 5\%) Ã© aproximadamente 3.09. Se a estatÃ­stica F calculada for maior que esse valor, rejeitamos a hipÃ³tese nula.
>
> **InterpretaÃ§Ã£o:**
> Se a estatÃ­stica F calculada for maior que 3.09, rejeitamos a hipÃ³tese nula de que $\beta_3 = 0$ e $\beta_4 = 0$. Isso sugere que os regressores 3 e 4, quando considerados em conjunto, contribuem significativamente para o modelo e nÃ£o devem ser omitidos. No caso contrÃ¡rio, nÃ£o rejeitamos a hipÃ³tese nula, o que nos leva a concluir que os regressores 3 e 4 nÃ£o adicionam informaÃ§Ã£o relevante ao modelo. Os valores exatos da estatÃ­stica F e o resultado do teste podem variar de simulaÃ§Ã£o para simulaÃ§Ã£o.
>
> **AnÃ¡lise dos ResÃ­duos:**
> Ã‰ crucial analisar os resÃ­duos dos modelos irrestrito e restrito para verificar se os pressupostos da regressÃ£o linear sÃ£o vÃ¡lidos (normalidade, homocedasticidade). Isso pode envolver a geraÃ§Ã£o de histogramas, grÃ¡ficos de dispersÃ£o dos resÃ­duos e testes estatÃ­sticos para normalidade e homocedasticidade. A anÃ¡lise dos resÃ­duos nos ajuda a avaliar a qualidade geral dos ajustes dos modelos e a identificar potenciais problemas, como heterocedasticidade.
>
> Ã‰ importante notar que, neste exemplo, os valores dos parÃ¢metros foram definidos de modo que $\beta_3 = 0.5$ e $\beta_4 = 0$ para fins ilustrativos. Em uma anÃ¡lise do mundo real, os valores dos parÃ¢metros sÃ£o desconhecidos e sÃ£o estimados a partir dos dados. Este exemplo ilustra como podemos utilizar um teste F para verificar se restriÃ§Ãµes impostas aos coeficientes levam a uma perda significativa na capacidade preditiva do modelo.

### ConclusÃ£o
A capacidade de avaliar restriÃ§Ãµes lineares sobre os parÃ¢metros de modelos de regressÃ£o Ã© fundamental na prÃ¡tica economÃ©trica. O teste F, conforme detalhado neste capÃ­tulo, fornece uma ferramenta robusta para esta avaliaÃ§Ã£o, permitindo uma anÃ¡lise comparativa entre modelos que incorpora restriÃ§Ãµes lineares e modelos mais flexÃ­veis (irrestritos). O uso apropriado da estatÃ­stica F, juntamente com o conhecimento de suas propriedades estatÃ­sticas e distribuiÃ§Ã£o, permite a tomada de decisÃµes informadas e a melhor compreensÃ£o das relaÃ§Ãµes entre variÃ¡veis. A conexÃ£o estabelecida neste capÃ­tulo com tÃ³picos anteriores, como testes t e propriedades dos estimadores OLS, evidencia a natureza cumulativa do conhecimento em econometria, destacando a importÃ¢ncia de cada conceito na construÃ§Ã£o de uma anÃ¡lise completa. A capacidade de comparar as somas de quadrados residuais e, portanto, a qualidade do ajuste de modelos alternativos, Ã© uma competÃªncia fundamental para qualquer pesquisador que utiliza modelos de regressÃ£o linear.

### ReferÃªncias
[^1]: *[8.1.11] The population residuals can be found...*
[^2]: *[8.A.6] RSS0 âˆ’ RSS1 = (b âˆ’ b*)'(X'X)(b âˆ’ b*)  [8.A.6]*
[^3]: *[8.1.37] Expressions [8.1.37] and [8.1.32] will generate...*
[^4]: *[8.1.27] More generally, suppose we want a joint test of m different linear restrictions about Î², as represented by Ho: RÎ² = r.*
<!-- END -->
