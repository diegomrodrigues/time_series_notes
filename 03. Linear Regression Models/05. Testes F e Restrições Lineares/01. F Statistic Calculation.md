## Testes F e Restri√ß√µes Lineares: Uma An√°lise Detalhada da Compara√ß√£o de Modelos
### Introdu√ß√£o
Este cap√≠tulo aprofunda o conceito de **testes F** em regress√£o linear, com √™nfase em como comparar modelos com e sem restri√ß√µes. Conectando com o t√≥pico anterior de **infer√™ncia estat√≠stica em modelos de regress√£o linear** e **testes t para hip√≥teses sobre os coeficientes**, agora exploramos como avaliar a signific√¢ncia conjunta de m√∫ltiplas restri√ß√µes lineares sobre os par√¢metros do modelo. O foco ser√° na interpreta√ß√£o da estat√≠stica F como uma medida da melhoria na adequa√ß√£o do modelo ao incorporar um conjunto de restri√ß√µes. Este cap√≠tulo se baseia em conceitos j√° apresentados [^1], [^2], [^3], [^4], e expande sobre eles para fornecer uma compreens√£o mais profunda de como o teste F √© utilizado na pr√°tica.

### Conceitos Fundamentais
Em regress√£o linear, muitas vezes nos deparamos com a necessidade de testar hip√≥teses que envolvem m√∫ltiplas restri√ß√µes sobre os coeficientes do modelo. Como vimos anteriormente, o **teste t** √© apropriado para testar hip√≥teses sobre um √∫nico coeficiente. No entanto, para testar restri√ß√µes lineares conjuntas, como, por exemplo, a igualdade entre dois coeficientes, o **teste F** √© a ferramenta adequada.  O teste F nos permite comparar a qualidade do ajuste de um modelo irrestrito com a de um modelo que incorpora restri√ß√µes lineares, determinando se a imposi√ß√£o dessas restri√ß√µes leva a uma perda significativa na qualidade do ajuste do modelo.

Formalmente, considere o modelo de regress√£o linear:
$$
y = X\beta + u,
$$
onde $y$ √© o vetor de vari√°veis dependentes, $X$ √© a matriz de regressores, $\beta$ √© o vetor de par√¢metros e $u$ √© o vetor de erros. Suponha que desejamos testar um conjunto de $m$ restri√ß√µes lineares sobre $\beta$, expressas como:

$$
H_0: R\beta = r, \qquad [8.1.27]
$$

onde $R$ √© uma matriz conhecida de dimens√£o $(m \times k)$, e $r$ √© um vetor conhecido de dimens√£o $(m \times 1)$. Por exemplo, para testar a hip√≥tese que $\beta_1 = \beta_2$, ter√≠amos $m = 1$,
$$R = \begin{bmatrix} 1 & -1 & 0 & \ldots & 0 \end{bmatrix}$$ e $r=0$ [^4].

O teste F compara a soma de quadrados dos res√≠duos (RSS) de um modelo **n√£o-restringido**, ou seja, um modelo que estima $\beta$ sem nenhuma restri√ß√£o ($RSS_1$), com a RSS de um modelo **restringido** ($RSS_0$), no qual $\beta$ √© estimado sob a restri√ß√£o imposta por $H_0$.  A **estat√≠stica F** √© definida como:

$$
F = \frac{(RSS_0 - RSS_1)/m}{RSS_1/(T - k)}, \qquad [8.1.37]
$$

onde $T$ √© o n√∫mero de observa√ß√µes, e $k$ √© o n√∫mero de par√¢metros no modelo original.

Essa estat√≠stica segue uma distribui√ß√£o $F$ com $m$ graus de liberdade no numerador e $T-k$ graus de liberdade no denominador, sob a hip√≥tese nula de que as restri√ß√µes s√£o verdadeiras [^4]. Uma grande estat√≠stica F indica que a imposi√ß√£o das restri√ß√µes resulta em uma perda significativa na qualidade do ajuste, sugerindo que a hip√≥tese nula deve ser rejeitada.

**Lema 1** (Decomposi√ß√£o da Soma de Quadrados): A soma total de quadrados (TSS) em uma regress√£o linear pode ser decomposta na soma de quadrados explicada pelo modelo (ESS) e na soma de quadrados dos res√≠duos (RSS), ou seja, $TSS = ESS + RSS$. Al√©m disso, no contexto de compara√ß√£o de modelos com e sem restri√ß√µes, temos que $TSS$ √© constante. Portanto, minimizar $RSS$ √© equivalente a maximizar $ESS$.

*Proof Strategy*: This result is a standard property of linear regression and is a consequence of the orthogonality between fitted values and residuals. The fact that $TSS$ remains constant across different model specifications allows us to focus on changes in the $RSS$ when evaluating different models, as the increase in ESS can only be achieved by decreasing the RSS.

**Prova do Lema 1:**
I. A soma total de quadrados (TSS) √© definida como a soma dos quadrados das diferen√ßas entre cada valor de $y_i$ e a m√©dia de $y$, $\bar{y}$:
    $$TSS = \sum_{i=1}^T (y_i - \bar{y})^2$$
II. A soma de quadrados explicada pelo modelo (ESS) √© a soma dos quadrados das diferen√ßas entre os valores ajustados, $\hat{y}_i$, e a m√©dia de $y$, $\bar{y}$:
    $$ESS = \sum_{i=1}^T (\hat{y}_i - \bar{y})^2$$
III. A soma de quadrados dos res√≠duos (RSS) √© a soma dos quadrados dos res√≠duos:
    $$RSS = \sum_{i=1}^T (y_i - \hat{y}_i)^2$$
IV. Para provar que $TSS = ESS + RSS$, podemos usar o fato que $y_i - \bar{y} = (y_i - \hat{y}_i) + (\hat{y}_i - \bar{y})$ e ent√£o expandir $TSS$:
    $$TSS = \sum_{i=1}^T [(y_i - \hat{y}_i) + (\hat{y}_i - \bar{y})]^2$$
V. Expandindo o quadrado, temos:
    $$TSS = \sum_{i=1}^T (y_i - \hat{y}_i)^2 + 2\sum_{i=1}^T (y_i - \hat{y}_i)(\hat{y}_i - \bar{y}) + \sum_{i=1}^T (\hat{y}_i - \bar{y})^2$$
VI. O termo do meio desaparece devido √† propriedade de ortogonalidade entre os res√≠duos e os valores ajustados, ou seja, $\sum_{i=1}^T (y_i - \hat{y}_i)(\hat{y}_i - \bar{y}) = 0$. Assim,
$$TSS = RSS + ESS$$
VII. Dado que a $TSS$ depende apenas dos dados $y_i$ e $\bar{y}$, ela √© constante para diferentes modelos lineares estimados usando os mesmos dados $y$, ou seja, mesmo quando impomos restri√ß√µes, a $TSS$ n√£o muda.
VIII. Consequentemente, como $TSS = ESS + RSS$, e $TSS$ √© constante, minimizar $RSS$ implica maximizar $ESS$, e vice-versa. ‚ñ†

**Deriva√ß√£o da Estat√≠stica F por meio da Compara√ß√£o de Somas de Quadrados Residuais:**

A estat√≠stica F pode ser calculada atrav√©s da compara√ß√£o das somas de quadrados residuais (RSS) dos modelos irrestrito e restrito. Para isso, consideremos $b$ como a estimativa de m√≠nimos quadrados ordin√°rios (OLS) irrestrita [8.1.6] e $b^*$ a estimativa sob as restri√ß√µes [8.1.35], com os respectivos $RSS_1$ e $RSS_0$. As somas de quadrados residuais s√£o definidas como:

$$
RSS_1 = \sum_{t=1}^{T} (y_t - x_t'b)^2, \qquad [8.1.35]
$$
$$
RSS_0 = \sum_{t=1}^{T} (y_t - x_t'b^*)^2, \qquad [8.1.36]
$$

A estat√≠stica F, de acordo com [8.1.37],  √© dada por:

$$
F = \frac{(RSS_0 - RSS_1)/m}{RSS_1/(T - k)}, \qquad [8.1.37]
$$

onde $m$ representa o n√∫mero de restri√ß√µes impostas e $T-k$ os graus de liberdade.

O teorema fundamental para esse c√°lculo (provado em detalhes no Ap√™ndice 8.A [^2]) estabelece que:

$$
RSS_0 - RSS_1 = (b - b^*)'X'X(b - b^*), \qquad [8.A.6]
$$

onde $b$ √© o estimador OLS n√£o-restringido e $b^*$ √© o estimador OLS restrito. Esta rela√ß√£o √© crucial porque nos permite expressar a diferen√ßa nas somas de quadrados residuais como uma fun√ß√£o da diferen√ßa nos coeficientes estimados.  A estat√≠stica F baseia-se ent√£o na raz√£o entre a diferen√ßa nas somas de quadrados residuais (normalizada pelo n√∫mero de restri√ß√µes), e a soma de quadrados residuais do modelo irrestrito normalizada por seus graus de liberdade.

**Proposi√ß√£o 1:** A estat√≠stica F pode ser expressa alternativamente usando a matriz de proje√ß√£o. Seja $P = X(X'X)^{-1}X'$ a matriz de proje√ß√£o em $X$ e seja $M = I - P$ a matriz de proje√ß√£o ortogonal. Ent√£o, podemos escrever:
$RSS_1 = u'Mu$,  onde $u = y - Xb$ s√£o os res√≠duos do modelo irrestrito,  e $RSS_0 - RSS_1 = (b-b^*)'X'X(b-b^*)$.
*Proof Strategy*: Using the properties of $P$ and $M$, and the fact that $u$ is orthogonal to the fitted values, we can rewrite the sum of squared errors in terms of matrix operations.
**Prova da Proposi√ß√£o 1:**
I.  Definimos $P = X(X'X)^{-1}X'$ como a matriz de proje√ß√£o no espa√ßo coluna de $X$. Portanto, $Xb = Py$, onde $b = (X'X)^{-1}X'y$ √© o estimador OLS n√£o-restrito.
II. Definimos $M = I - P$ como a matriz de proje√ß√£o ortogonal no espa√ßo nulo de $X$. 
III. Os res√≠duos do modelo irrestrito s√£o dados por $u = y - Xb = y - Py = (I-P)y = My$.
IV. A soma dos quadrados dos res√≠duos (RSS) para o modelo irrestrito √© dada por $RSS_1 = u'u = (My)'(My) = y'M'My$.
V. Como $M$ √© uma matriz de proje√ß√£o, √© idempotente, ou seja, $M'M = M^2 = M$. Portanto, $RSS_1 = y'My$.
VI. Tamb√©m, $u'Mu = (My)'M(My) = y'M'My = y'My$. Logo, $RSS_1 = u'Mu$.
VII.  Usando o resultado [8.A.6], temos que $RSS_0 - RSS_1 = (b - b^*)'X'X(b - b^*)$.‚ñ†
**Corol√°rio 1:** A estat√≠stica F pode tamb√©m ser escrita como:
$$F = \frac{ \frac{ (b-b^*)'X'X(b-b^*)}{m}}{\frac{u'Mu}{T-k}}$$
 Esta formula√ß√£o da estat√≠stica F evidencia a compara√ß√£o entre a varia√ß√£o explicada pelas restri√ß√µes e a varia√ß√£o inexplicada do modelo irrestrito.

**Exemplo Pr√°tico:**

Para ilustrar como usar este teste, consideremos a situa√ß√£o na qual  o modelo de regress√£o inclui 4 regressores ($k=4$) e queremos testar a hip√≥tese nula de que os coeficientes dos regressores 3 e 4 s√£o simultaneamente iguais a zero. Ou seja:

$$
H_0: \beta_3 = 0 \text{ e } \beta_4 = 0
$$
Nesse caso, o n√∫mero de restri√ß√µes ($m$) √© 2. Primeiramente, estima-se um modelo OLS irrestrito ($RSS_1$) incluindo todos os 4 regressores e, em seguida, estima-se o modelo restrito ($RSS_0$) excluindo os regressores 3 e 4 do modelo. Substituindo os valores de $RSS_0$ e $RSS_1$ na equa√ß√£o [8.1.37], teremos uma estat√≠stica F que podemos comparar com a distribui√ß√£o F apropriada.

Por exemplo, se o tamanho da amostra for $T=50$ e a estat√≠stica F calculada for maior que 3.20 (o valor cr√≠tico de 5\% para uma distribui√ß√£o $F(2, 46)$), rejeitar√≠amos a hip√≥tese nula e concluir√≠amos que os regressores 3 e 4 juntos s√£o significativamente diferentes de zero.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo com dados simulados para tornar o c√°lculo da estat√≠stica F mais concreto. Suponha que temos $T=100$ observa√ß√µes e 4 regressores (incluindo a constante), ou seja $k=4$. O modelo de regress√£o linear √©:
>
> $$y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + u_i$$
>
> Queremos testar a hip√≥tese nula de que $\beta_3 = 0$ e $\beta_4 = 0$, ent√£o temos $m=2$ restri√ß√µes.
>
> **Passo 1: Simula√ß√£o dos Dados**
>
> Primeiro, vamos criar dados simulados usando Python.
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> # Define a semente para reprodutibilidade
> np.random.seed(42)
>
> # Gera os dados simulados
> T = 100 # N√∫mero de observa√ß√µes
> X = np.random.rand(T, 4) # Matriz de regressores (inclui a constante)
> X[:, 0] = 1  # Adicionando a coluna de 1s para a constante
> beta = np.array([2, 3, -1, 0.5]) # Define os verdadeiros valores dos par√¢metros
> u = np.random.normal(0, 1, T) # Erro aleat√≥rio
> y = np.dot(X, beta) + u
> ```
>
> **Passo 2: Estimar o modelo irrestrito**
>
> Agora, vamos ajustar um modelo OLS irrestrito usando todos os regressores e calcular $RSS_1$.
>
> ```python
> # Modelo OLS irrestrito
> model_irrestrito = LinearRegression(fit_intercept=False)
> model_irrestrito.fit(X, y)
> y_hat_irrestrito = model_irrestrito.predict(X)
> residuos_irrestrito = y - y_hat_irrestrito
> RSS1 = np.sum(residuos_irrestrito**2)
> print(f"RSS_1: {RSS1:.2f}")
> ```
>
> **Passo 3: Estimar o modelo restrito**
>
> Em seguida, ajustamos o modelo restrito excluindo os regressores 3 e 4 ($x_3$ e $x_4$) e calculamos $RSS_0$.
>
> ```python
> # Modelo OLS restrito
> X_restrito = X[:, 0:3] # Remove as colunas 3 e 4 da matriz de regressores
> model_restrito = LinearRegression(fit_intercept=False)
> model_restrito.fit(X_restrito, y)
> y_hat_restrito = model_restrito.predict(X_restrito)
> residuos_restrito = y - y_hat_restrito
> RSS0 = np.sum(residuos_restrito**2)
> print(f"RSS_0: {RSS0:.2f}")
> ```
>
> **Passo 4: Calcular a Estat√≠stica F**
>
> Agora, podemos calcular a estat√≠stica F usando a f√≥rmula [8.1.37].
>
> ```python
> # Calcula a estat√≠stica F
> m = 2  # N√∫mero de restri√ß√µes
> T = 100 # N√∫mero de observa√ß√µes
> k = 4  # N√∫mero de par√¢metros no modelo irrestrito
> F_stat = ((RSS0 - RSS1) / m) / (RSS1 / (T - k))
> print(f"Estat√≠stica F: {F_stat:.2f}")
> ```
>
> **Passo 5: Comparar com o valor cr√≠tico**
>
> O valor cr√≠tico da distribui√ß√£o F com $m=2$ e $T-k=96$ graus de liberdade para $\alpha = 0.05$ (n√≠vel de signific√¢ncia de 5\%) √© aproximadamente 3.09. Se a estat√≠stica F calculada for maior que esse valor, rejeitamos a hip√≥tese nula.
>
> **Interpreta√ß√£o:**
> Se a estat√≠stica F calculada for maior que 3.09, rejeitamos a hip√≥tese nula de que $\beta_3 = 0$ e $\beta_4 = 0$. Isso sugere que os regressores 3 e 4, quando considerados em conjunto, contribuem significativamente para o modelo e n√£o devem ser omitidos. No caso contr√°rio, n√£o rejeitamos a hip√≥tese nula, o que nos leva a concluir que os regressores 3 e 4 n√£o adicionam informa√ß√£o relevante ao modelo. Os valores exatos da estat√≠stica F e o resultado do teste podem variar de simula√ß√£o para simula√ß√£o.
>
> **An√°lise dos Res√≠duos:**
> √â crucial analisar os res√≠duos dos modelos irrestrito e restrito para verificar se os pressupostos da regress√£o linear s√£o v√°lidos (normalidade, homocedasticidade). Isso pode envolver a gera√ß√£o de histogramas, gr√°ficos de dispers√£o dos res√≠duos e testes estat√≠sticos para normalidade e homocedasticidade. A an√°lise dos res√≠duos nos ajuda a avaliar a qualidade geral dos ajustes dos modelos e a identificar potenciais problemas, como heterocedasticidade.
>
> √â importante notar que, neste exemplo, os valores dos par√¢metros foram definidos de modo que $\beta_3 = 0.5$ e $\beta_4 = 0$ para fins ilustrativos. Em uma an√°lise do mundo real, os valores dos par√¢metros s√£o desconhecidos e s√£o estimados a partir dos dados. Este exemplo ilustra como podemos utilizar um teste F para verificar se restri√ß√µes impostas aos coeficientes levam a uma perda significativa na capacidade preditiva do modelo.

### Conclus√£o
A capacidade de avaliar restri√ß√µes lineares sobre os par√¢metros de modelos de regress√£o √© fundamental na pr√°tica econom√©trica. O teste F, conforme detalhado neste cap√≠tulo, fornece uma ferramenta robusta para esta avalia√ß√£o, permitindo uma an√°lise comparativa entre modelos que incorpora restri√ß√µes lineares e modelos mais flex√≠veis (irrestritos). O uso apropriado da estat√≠stica F, juntamente com o conhecimento de suas propriedades estat√≠sticas e distribui√ß√£o, permite a tomada de decis√µes informadas e a melhor compreens√£o das rela√ß√µes entre vari√°veis. A conex√£o estabelecida neste cap√≠tulo com t√≥picos anteriores, como testes t e propriedades dos estimadores OLS, evidencia a natureza cumulativa do conhecimento em econometria, destacando a import√¢ncia de cada conceito na constru√ß√£o de uma an√°lise completa. A capacidade de comparar as somas de quadrados residuais e, portanto, a qualidade do ajuste de modelos alternativos, √© uma compet√™ncia fundamental para qualquer pesquisador que utiliza modelos de regress√£o linear.

### Refer√™ncias
[^1]: *[8.1.11] The population residuals can be found...*
[^2]: *[8.A.6] RSS0 ‚àí RSS1 = (b ‚àí b*)'(X'X)(b ‚àí b*)  [8.A.6]*
[^3]: *[8.1.37] Expressions [8.1.37] and [8.1.32] will generate...*
[^4]: *[8.1.27] More generally, suppose we want a joint test of m different linear restrictions about Œ≤, as represented by Ho: RŒ≤ = r.*
<!-- END -->
