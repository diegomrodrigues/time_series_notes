## Um Teste F Alternativo via Comparação de Somas de Quadrados Residuais
### Introdução
Este capítulo expande a discussão sobre **testes F** em regressão linear, introduzindo uma abordagem alternativa para calcular a estatística F, focando na comparação direta das somas de quadrados residuais (RSS) dos modelos restrito e não restrito. Este método, complementa os conceitos já apresentados sobre a **equivalência dos testes F e t** [^2], a **derivação da estatística F** [^1], [^3] e a **aplicação de restrições lineares** [^4], proporcionando uma visão mais prática e intuitiva do teste F. O foco será na apresentação detalhada da metodologia, incluindo a prova formal da equivalência e exemplos para ilustrar a aplicação em cenários concretos.

### Metodologia do Teste F via Comparação de RSS
A abordagem alternativa para o teste F utiliza diretamente as somas de quadrados dos resíduos (RSS) dos modelos restrito e não restrito, conforme introduzido anteriormente [^1], [^3], [^4]. Este método se mostra especialmente útil em situações onde a imposição das restrições pode ser realizada através de uma regressão linear em variáveis transformadas.

**Definições:**
*   $RSS_1$: Soma dos quadrados dos resíduos do modelo irrestrito.
*   $RSS_0$: Soma dos quadrados dos resíduos do modelo restrito.
*   $m$: Número de restrições lineares impostas.
*   $T$: Número total de observações.
*   $k$: Número de parâmetros no modelo irrestrito.

A estatística F, como visto em [8.1.37] [^3],  é calculada como:
$$
F = \frac{(RSS_0 - RSS_1)/m}{RSS_1/(T - k)}, \qquad [8.1.37]
$$

Essa formulação explora a diferença entre as somas de quadrados residuais (RSS) dos modelos. Se as restrições impostas forem válidas, o aumento em RSS ao se passar do modelo irrestrito para o modelo restrito ($RSS_0 - RSS_1$) será relativamente pequeno. Por outro lado, se as restrições forem inválidas, esse aumento será substancial. A estatística F quantifica essa diferença, levando em conta os graus de liberdade relevantes.

**Lema 1:** (Decomposição da Soma de Quadrados Residuais) A soma de quadrados residuais do modelo restrito ($RSS_0$) pode ser decomposta como a soma de quadrados residuais do modelo irrestrito ($RSS_1$) e um termo adicional que reflete o custo da imposição das restrições, dado por:
$RSS_0 = RSS_1 + (b - b^*)'X'X(b-b^*)$, onde $b$ é o estimador OLS irrestrito e $b^*$ é o estimador OLS restrito.
*Proof Strategy*: This result is based on algebraic manipulations showing that the residual sum of squares in the restricted model can be expressed as the residual sum of squares in the unrestricted model plus a term involving the difference between the unrestricted and restricted estimators, and the design matrix.
**Prova do Lema 1:**
I. A soma de quadrados residuais para o modelo irrestrito ($RSS_1$) é definida como:
    $$RSS_1 = (y - Xb)'(y - Xb),$$ onde $b$ é o estimador OLS irrestrito.
II. A soma de quadrados residuais para o modelo restrito ($RSS_0$) é definida como:
   $$RSS_0 = (y - Xb^*)'(y - Xb^*),$$ onde $b^*$ é o estimador OLS restrito.
III. Podemos reescrever a expressão de $RSS_0$ como:
    $$RSS_0 = (y - Xb + Xb - Xb^*)'(y - Xb + Xb - Xb^*)$$
IV. Expandindo essa expressão, temos:
    $$RSS_0 = [(y - Xb) + X(b - b^*)]'[(y - Xb) + X(b - b^*)]$$
V. Ao expandir e simplificar:
    $$RSS_0 = (y - Xb)'(y - Xb) + (y - Xb)'X(b - b^*) + (b - b^*)'X'(y - Xb) + (b - b^*)'X'X(b - b^*)$$
VI. Como $(y - Xb)$ é ortogonal ao espaço coluna de $X$ (devido às propriedades do OLS), os termos do meio se anulam, o que nos dá:
   $$RSS_0 = (y - Xb)'(y - Xb) + (b - b^*)'X'X(b - b^*)$$
VII. Reconhecemos o primeiro termo como $RSS_1$, então:
    $$RSS_0 = RSS_1 + (b - b^*)'X'X(b - b^*)$$
VIII. Isso demonstra que a soma dos quadrados residuais do modelo restrito é igual à soma dos quadrados residuais do modelo irrestrito mais um termo que depende da diferença entre os estimadores irrestrito e restrito. ■

**Lema 1.1:** (Propriedade da Matriz de Projeção) A matriz de projeção ortogonal $M = I - X(X'X)^{-1}X'$ tem a propriedade de que $MX = 0$.
*Proof Strategy:* This can be demonstrated by direct multiplication of the matrices. This is a fundamental property of the projection matrix, ensuring that the residuals are orthogonal to the columns of the design matrix.

**Prova do Lema 1.1:**
I. A matriz de projeção ortogonal $M$ é definida como $M = I - X(X'X)^{-1}X'$.
II. Multiplicando $M$ por $X$, temos:
    $$MX = (I - X(X'X)^{-1}X')X$$
III. Expandindo a expressão:
    $$MX = IX - X(X'X)^{-1}X'X$$
IV. Dado que $IX = X$ e $(X'X)^{-1}X'X = I$, obtemos:
    $$MX = X - X(I)$$
V. Simplificando, temos:
    $$MX = X - X = 0$$
VI. Portanto, $MX = 0$, demonstrando a propriedade da matriz de projeção ortogonal. ■

**Teorema 1:** A estatística F, calculada a partir da comparação das somas de quadrados residuais, pode ser expressa como:
$$F = \frac{(RSS_0 - RSS_1)/m}{RSS_1/(T-k)} =  \frac{ \frac{ (b-b^*)'X'X(b-b^*)}{m}}{\frac{u'Mu}{T-k}}$$
onde $u$ são os resíduos do modelo irrestrito e $M = I-X(X'X)^{-1}X'$ é a matriz de projeção ortogonal.
*Proof Strategy*: This proof uses the relationship between the sum of squared errors in the restricted and unrestricted models, and establishes the link between the F-statistic, the design matrix, the difference between the OLS coefficients and the residuals. This formulation emphasizes how F-statistic measures improvement in fit due to relaxing restrictions.

**Prova do Teorema 1:**
I.  Pelo Lema 1, sabemos que
$$RSS_0 = RSS_1 + (b - b^*)'X'X(b - b^*).$$
II.  Rearranjando, obtemos:
$$RSS_0 - RSS_1 = (b - b^*)'X'X(b - b^*).$$
III. Substituindo esse resultado na fórmula da estatística F, temos:
$$F = \frac{(RSS_0 - RSS_1)/m}{RSS_1/(T - k)} = \frac{(b - b^*)'X'X(b - b^*)/m}{RSS_1/(T-k)}.$$
IV.  Pela Proposição 1 do capítulo anterior, sabemos que $RSS_1 = u'Mu$. Portanto:
$$F = \frac{(b - b^*)'X'X(b - b^*)/m}{u'Mu/(T-k)}.$$
V. Essa expressão estabelece a relação direta entre a diferença na soma dos quadrados dos resíduos, os estimadores OLS e a matriz de projeção ortogonal. ■

**Corolário 1:** A estatística F também pode ser expressa como:
$$
F =  \frac{ \frac{ (b-b^*)'X'X(b-b^*)}{m}}{\frac{s^2(T-k)}{T-k}} =  \frac{ \frac{ (b-b^*)'X'X(b-b^*)}{m}}{s^2}
$$
onde $s^2$ é o estimador da variância do erro do modelo irrestrito.
*Proof Strategy*: This alternative expression for the F-statistic is useful because it directly relates the F-statistic to the estimated variance of the error term and demonstrates how it can be interpreted as the ratio between the variability attributed to the imposed restrictions, scaled by the number of restrictions ($m$) and the unexplained variability in the unrestricted model ($s^2$).
**Prova do Corolário 1:**
I.  A estatística F é dada por
$$
F = \frac{(RSS_0 - RSS_1)/m}{RSS_1/(T - k)}.
$$
II.  Do Teorema 1, temos
$$RSS_0 - RSS_1 = (b - b^*)'X'X(b - b^*)$$ e $RSS_1 = u'Mu$.
III.  Reescrevendo a estatística F como
$$
F =  \frac{ \frac{ (b-b^*)'X'X(b-b^*)}{m}}{\frac{u'Mu}{T-k}}.
$$
IV.  Sabemos que a variância do erro do modelo irrestrito é estimada por
$$ s^2 = \frac{RSS_1}{T-k} = \frac{u'Mu}{T-k} $$
V. Portanto, podemos substituir $RSS_1/(T-k)$ por $s^2$ na expressão da estatística F, resultando em
$$F =  \frac{ \frac{ (b-b^*)'X'X(b-b^*)}{m}}{s^2}$$
VI. Isso demonstra que a estatística F pode ser expressa em termos da diferença dos estimadores OLS, a matriz de desenho, a variância estimada do erro, e o número de restrições. ■

**Teorema 1.1** (Relação com a Matriz de Restrições) Se as restrições lineares impostas puderem ser expressas como $R\beta = r$, onde $R$ é uma matriz $m \times k$ e $r$ é um vetor $m \times 1$, então a estatística F pode ser expressa como:
$$ F = \frac{(R\hat{\beta} - r)'[R(X'X)^{-1}R']^{-1}(R\hat{\beta} - r)/m}{s^2}$$
onde $\hat{\beta}$ é o estimador OLS irrestrito e $s^2$ é o estimador da variância do erro do modelo irrestrito.
*Proof Strategy*: This result connects the RSS-based F-statistic with the more common expression involving the restriction matrix $R$. It requires the fact that $(b-b^*)$ is directly related to $R\hat{\beta} - r$. This alternative form is useful to connect with standard textbook expressions for the F-statistic for testing linear restrictions.

**Prova do Teorema 1.1:**
I.  Considerando a restrição linear $R\beta = r$, onde $R$ é uma matriz $m \times k$ e $r$ é um vetor $m \times 1$.
II.  Sabemos que o estimador OLS restrito $\hat{\beta}^*$ satisfaz $R\hat{\beta}^* = r$.
III.  A diferença entre os estimadores restrito e irrestrito é dada por $\hat{\beta} - \hat{\beta}^*$.
IV.  A expressão para $\hat{\beta}^*$ sob restrição linear é $\hat{\beta}^* = \hat{\beta} - (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(R\hat{\beta} - r)$.
V.  Portanto, $\hat{\beta} - \hat{\beta}^* = (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(R\hat{\beta} - r)$.
VI.  Substituindo esta expressão em $(b-b^*)'X'X(b-b^*)$, temos:
     $(b - b^*)'X'X(b - b^*) = (R\hat{\beta} - r)'[R(X'X)^{-1}R']^{-1}(R\hat{\beta} - r)$.
VII. Substituindo esse resultado na expressão do Corolário 1:
$$F = \frac{(R\hat{\beta} - r)'[R(X'X)^{-1}R']^{-1}(R\hat{\beta} - r)/m}{s^2}$$
VIII. Esta demonstração estabelece a equivalência entre a abordagem baseada em RSS e a expressão padrão do teste F para restrições lineares.■

### Aplicação Prática da Metodologia
A metodologia do teste F via comparação de RSS é particularmente útil quando a imposição das restrições lineares simplifica a estimação do modelo restrito, e essa imposição é realizada por meio de uma regressão simples sobre variáveis transformadas. Esta abordagem evita o cálculo da matriz R e sua inversa, simplificando a computação. A ideia é estimar o modelo restrito por meio de uma transformação nas variáveis originais que incorpore as restrições.

**Exemplo 1: Restrição de Igualdade de Coeficientes**
Suponha que temos o modelo:
$$
y_t = \beta_0 + \beta_1 x_{1t} + \beta_2 x_{2t} + u_t
$$
e desejamos testar a hipótese nula $H_0: \beta_1 = \beta_2$. Sob essa restrição, o modelo se torna:
$$
y_t = \beta_0 + \beta_1 (x_{1t} + x_{2t}) + u_t
$$
Assim, basta definir uma nova variável $z_t = x_{1t} + x_{2t}$ e estimar o modelo $y_t = \beta_0 + \beta_1 z_t + u_t$. A soma de quadrados dos resíduos desse modelo restrito será usada para calcular $RSS_0$, enquanto o $RSS_1$ será obtido do modelo original sem restrição.

> 💡 **Exemplo Numérico:**
>
> Vamos supor que temos um modelo de regressão linear com duas variáveis explicativas:
>
> $$y_t = \beta_0 + \beta_1 x_{1t} + \beta_2 x_{2t} + u_t$$
>
> Queremos testar a hipótese nula $H_0: \beta_1 = \beta_2$.  Para isso, vamos simular dados.
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> # Define a semente para reprodutibilidade
> np.random.seed(42)
>
> # Simula dados
> T = 100 # número de observações
> X = np.random.rand(T, 3) # Matriz com intercepto, x1 e x2
> X[:, 0] = 1 # Adiciona intercepto
> beta = np.array([1, 2, 2]) # Define os coeficientes verdadeiros
> u = np.random.normal(0, 0.5, T) # Simula os erros
> y = np.dot(X, beta) + u # Gera y
> ```
>
> Agora, vamos calcular os RSS dos modelos irrestrito e restrito.
>
> **Modelo Irrestrito:**
>
> Estimamos o modelo com todas as variáveis:
>
> ```python
> # Estima o modelo irrestrito
> model_irrestrito = LinearRegression(fit_intercept=False)
> model_irrestrito.fit(X, y)
> y_hat_irrestrito = model_irrestrito.predict(X)
> residuos_irrestrito = y - y_hat_irrestrito
> RSS1 = np.sum(residuos_irrestrito**2)
> print(f"RSS_1: {RSS1:.2f}")
> ```
>
> **Modelo Restrito:**
>
> Criamos a variável $z_t = x_{1t} + x_{2t}$ e estimamos o modelo:
> $$y_t = \beta_0 + \beta_1 z_t + u_t$$
>
> ```python
> # Estima o modelo restrito
> z = X[:, 1] + X[:, 2]
> X_restrito = np.column_stack((X[:, 0], z)) # Matriz com intercepto e z
> model_restrito = LinearRegression(fit_intercept=False)
> model_restrito.fit(X_restrito, y)
> y_hat_restrito = model_restrito.predict(X_restrito)
> residuos_restrito = y - y_hat_restrito
> RSS0 = np.sum(residuos_restrito**2)
> print(f"RSS_0: {RSS0:.2f}")
> ```
>
> **Cálculo da Estatística F:**
>
> Com $m = 1$ (uma restrição), $T = 100$ observações e $k = 3$ parâmetros no modelo irrestrito, calculamos a estatística F:
>
> ```python
> # Calcula a estatística F
> m = 1
> T = 100
> k = 3
> F_stat = ((RSS0 - RSS1) / m) / (RSS1 / (T - k))
> print(f"Estatística F: {F_stat:.2f}")
> ```
>
> **Interpretação:**
>
> O valor da estatística F é comparado com um valor crítico da distribuição F com 1 e 97 graus de liberdade (para $\alpha=0.05$ o valor é aproximadamente 3.94). Se o valor da estatística F calculada for maior que o valor crítico, rejeitamos a hipótese nula de igualdade dos coeficientes. Caso contrário, não rejeitamos a hipótese nula.

**Exemplo 2: Exclusão de Variáveis**
Se quisermos testar a hipótese nula de que um conjunto de coeficientes é zero (exclusão de variáveis), podemos simplesmente estimar o modelo original ($RSS_1$) e, em seguida, estimar um novo modelo que não inclui as variáveis correspondentes à hipótese nula ($RSS_0$). A estatística F é então calculada usando $RSS_0$ e $RSS_1$ como descrito acima.

> 💡 **Exemplo Numérico:**
>
> Vamos considerar um modelo com três variáveis explicativas e testar se as duas últimas são conjuntamente insignificantes. O modelo é dado por:
>
> $$y_t = \beta_0 + \beta_1 x_{1t} + \beta_2 x_{2t} + \beta_3 x_{3t} + u_t$$
>
> A hipótese nula é $H_0: \beta_2 = \beta_3 = 0$.
>
> **Passo 1: Simulação de Dados**
>
> Simule os dados como no exemplo anterior, mas com três variáveis explicativas.
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> # Define a semente para reprodutibilidade
> np.random.seed(42)
>
> # Simula dados
> T = 100
> X = np.random.rand(T, 4) # Matriz com intercepto, x1, x2 e x3
> X[:, 0] = 1 # Adiciona intercepto
> beta = np.array([1, 2, 0, 0]) # Define os coeficientes verdadeiros (beta_2 e beta_3 = 0)
> u = np.random.normal(0, 0.5, T) # Simula erros
> y = np.dot(X, beta) + u # Gera y
> ```
>
> **Passo 2: Estimativa do Modelo Irrestrito**
>
> Estime o modelo com todas as variáveis e calcule $RSS_1$.
>
> ```python
> # Estima o modelo irrestrito
> model_irrestrito = LinearRegression(fit_intercept=False)
> model_irrestrito.fit(X, y)
> y_hat_irrestrito = model_irrestrito.predict(X)
> residuos_irrestrito = y - y_hat_irrestrito
> RSS1 = np.sum(residuos_irrestrito**2)
> print(f"RSS_1: {RSS1:.2f}")
> ```
>
> **Passo 3: Estimativa do Modelo Restrito**
>
> Estime o modelo apenas com a primeira variável explicativa (excluindo $x_2$ e $x_3$) e calcule $RSS_0$.
>
> ```python
> # Estima o modelo restrito
> X_restrito = X[:, [0, 1]] # Matriz com intercepto e x1
> model_restrito = LinearRegression(fit_intercept=False)
> model_restrito.fit(X_restrito, y)
> y_hat_restrito = model_restrito.predict(X_restrito)
> residuos_restrito = y - y_hat_restrito
> RSS0 = np.sum(residuos_restrito**2)
> print(f"RSS_0: {RSS0:.2f}")
> ```
>
> **Passo 4: Cálculo da Estatística F**
>
> Calcule a estatística F com $m=2$ restrições, $T=100$ e $k=4$.
>
> ```python
> # Calcula a estatística F
> m = 2 # Número de restrições
> T = 100 # Número de observações
> k = 4 # Número de parâmetros no modelo irrestrito
> F_stat = ((RSS0 - RSS1) / m) / (RSS1 / (T - k))
> print(f"Estatística F: {F_stat:.2f}")
> ```
>
> **Passo 5: Interpretação**
>
> O valor da estatística F deve ser comparado com um valor crítico da distribuição F com 2 e 96 graus de liberdade. Se o valor calculado for maior do que o valor crítico, rejeitamos a hipótese nula de que $\beta_2$ e $\beta_3$ são conjuntamente nulos. Caso contrário, não rejeitamos.
>
> **Considerações Adicionais:**
> Este exemplo ilustra como o teste F pode ser usado para testar a exclusão de variáveis. Em uma aplicação real, é essencial avaliar os pressupostos do modelo de regressão linear.

> 💡 **Exemplo Numérico Detalhado:**
>
> Vamos considerar um exemplo numérico para ilustrar o cálculo do teste F pela comparação das somas de quadrados residuais. Suponha que temos o modelo:
>
> $$y_t = \beta_0 + \beta_1 x_{1t} + \beta_2 x_{2t} + \beta_3 x_{3t} + u_t$$
>
> e queremos testar a hipótese nula $H_0: \beta_2 = \beta_3 = 0$, o que significa que estamos testando a hipótese de exclusão dos regressores $x_2$ e $x_3$.
>
> **Passo 1: Simulação dos Dados**
>
> Vamos gerar dados simulados com $T=100$ observações.
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> # Define a semente para reprodutibilidade
> np.random.seed(42)
>
> # Gera os dados simulados
> T = 100 # Número de observações
> X = np.random.rand(T, 4) # Matriz de regressores (inclui a constante)
> X[:, 0] = 1  # Adicionando a coluna de 1s para a constante
> beta = np.array([2, 3, 0, 0]) # Define os verdadeiros valores dos parâmetros, beta_2 e beta_3 = 0
> u = np.random.normal(0, 1, T) # Erro aleatório
> y = np.dot(X, beta) + u
> ```
>
> **Passo 2: Estimar o modelo irrestrito**
>
> Estimamos o modelo OLS irrestrito usando todos os regressores e calculamos $RSS_1$.
>
> ```python
> # Modelo OLS irrestrito
> model_irrestrito = LinearRegression(fit_intercept=False)
> model_irrestrito.fit(X, y)
> y_hat_irrestrito = model_irrestrito.predict(X)
> residuos_irrestrito = y - y_hat_irrestrito
> RSS1 = np.sum(residuos_irrestrito**2)
> print(f"RSS_1: {RSS1:.2f}")
> ```
>
> **Passo 3: Estimar o modelo restrito**
>
> Estimamos o modelo restrito excluindo os regressores $x_2$ e $x_3$ e calculamos $RSS_0$.
>
> ```python
> # Modelo OLS restrito
> X_restrito = X[:, 0:2] # Remove as colunas 2 e 3 da matriz de regressores
> model_restrito = LinearRegression(fit_intercept=False)
> model_restrito.fit(X_restrito, y)
> y_hat_restrito = model_restrito.predict(X_restrito)
> residuos_restrito = y - y_hat_restrito
> RSS0 = np.sum(residuos_restrito**2)
> print(f"RSS_0: {RSS0:.2f}")
> ```
>
> **Passo 4: Calcular a Estatística F**
>
> Calculamos a estatística F usando a fórmula:
> $$F = \frac{(RSS_0 - RSS_1)/m}{RSS_1/(T - k)} $$
> onde $m = 2$ (número de restrições), $T=100$ (número de observações) e $k=4$ (número de parâmetros no modelo irrestrito).
>
> ```python
> # Calcula a estatística F
> m = 2  # Número de restrições
> T = 100 # Número de observações
> k = 4  # Número de parâmetros no modelo irrestrito
> F_stat = ((RSS0 - RSS1) / m) / (RSS1 / (T - k))
> print(f"Estatística F: {F_stat:.2f}")
> ```
>
> **Passo 5: Comparar com o valor crítico**
>
> O valor crítico para um teste F com 2 graus de liberdade no numerador e 96 graus de liberdade no denominador é aproximadamente 3.09 (para um nível de significância de 5\%). Se a estatística F calculada for maior que esse valor, rejeitamos a hipótese nula de que $\beta_2 = \beta_3 = 0$.
>
> **Interpretação:**
> Neste exemplo, se o valor calculado para a estatística F for maior que o valor crítico, concluímos que a omissão dos regressores $x_2$ e $x_3$ causa uma perda significativa no poder explicativo do modelo, o que nos leva a rejeitar a hipótese nula e a incluir as variáveis. O contrário se verifica se o valor da estatística F for inferior ao valor crítico. O resultado numérico específico dependerá dos dados simulados e pode variar de simulação para simulação.
>
> **Considerações Adicionais:**
> É sempre crucial verificar se os pressupostos do modelo de regressão linear são válidos, o que envolve analisar os resíduos e verificar homocedasticidade e normalidade. Esta análise adicional permite validar a fiabilidade dos resultados do teste.

### Conclusão
Este capítulo detalhou uma abordagem alternativa para o cálculo da estatística F em regressão linear, através da comparação direta das somas de quadrados residuais dos modelos restrito e irrestrito. A equivalência entre essa abordagem e a formulação original foi demonstrada, proporcionando uma ferramenta prática e eficiente para testes de hipóteses lineares. O método, além de ser mais intuitivo para muitos casos, evita o cálculo da matriz $R$ e sua inversa. A aplicação desta metodologia, juntamente com a compreensão das provas matemáticas subjacentes, capacita o pesquisador a realizar inferências estatísticas robustas e a comparar modelos com e sem restrições lineares de forma eficiente. A capacidade de usar diferentes formulações da estatística F enriquece o arsenal de ferramentas disponíveis para a análise econométrica.

### Referências
[^1]: *[8.1.37] Expressions [8.1.37] and [8.1.32] will generate exactly the same number...*
[^2]: *Previous Topics: --- START O teste F e o teste t são equivalentes para hipóteses lineares simples. O teste F surge diretamente dos resultados sobre distribuições qui-quadrado e da razão entre as estatísticas do modelo não restrito e restrito. ---*
[^3]: *[8.1.32] The Wald form of the OLS F test of a linear hypothesis...*
[^4]: *[8.1.27] More generally, suppose we want a joint test of m different linear restrictions about β, as represented by Ho: Rβ = r.*
<!-- END -->
