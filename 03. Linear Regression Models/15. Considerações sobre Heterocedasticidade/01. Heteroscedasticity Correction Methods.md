## Abordagens para Heterocedasticidade e Corre√ß√£o via Estimadores Robustos e GLS

### Introdu√ß√£o
Como visto anteriormente [^1], a regress√£o linear cl√°ssica assume que os erros t√™m vari√¢ncia constante (homocedasticidade). No entanto, em muitas aplica√ß√µes pr√°ticas, especialmente em s√©ries temporais, essa suposi√ß√£o √© frequentemente violada, e os erros podem exibir heterocedasticidade, onde a vari√¢ncia dos erros n√£o √© constante [^1]. Este cap√≠tulo aprofunda a an√°lise de heterocedasticidade e apresenta m√©todos para lidar com ela, usando estimadores robustos como os propostos por White e estimadores de m√≠nimos quadrados generalizados (GLS) [^1].

### Estimadores Robustos para Heterocedasticidade
Quando a suposi√ß√£o de homocedasticidade √© violada, os estimadores de M√≠nimos Quadrados Ordin√°rios (OLS) ainda s√£o n√£o viesados, mas n√£o s√£o mais os melhores estimadores lineares n√£o viesados (BLUE) [^1]. Al√©m disso, os testes de hip√≥teses tradicionais e intervalos de confian√ßa baseados nos erros padr√£o do OLS podem ser inv√°lidos [^1].

Para resolver esse problema, estimadores robustos para a matriz de covari√¢ncia do estimador OLS foram desenvolvidos. O estimador proposto por White [^1] √© uma abordagem popular para obter erros padr√£o consistentes na presen√ßa de heterocedasticidade de forma desconhecida. Este estimador n√£o requer nenhuma suposi√ß√£o espec√≠fica sobre a forma da heterocedasticidade e √© definido como:

$$
\hat{\Omega} = \left( \frac{1}{T} \sum_{t=1}^{T} x_t x_t' \right)^{-1} \left( \frac{1}{T} \sum_{t=1}^{T} \hat{u}_t^2 x_t x_t' \right) \left( \frac{1}{T} \sum_{t=1}^{T} x_t x_t' \right)^{-1}
$$
onde $x_t$ s√£o os regressores e $\hat{u}_t$ s√£o os res√≠duos OLS [^1]. A raiz quadrada dos elementos da diagonal de $\hat{\Omega}$ fornece os erros padr√£o robustos para os coeficientes OLS [^1].

> üí° **Exemplo Num√©rico:**
> Suponha que temos um modelo de regress√£o com um √∫nico regressor e um intercepto, e que coletamos dados para $T=5$ observa√ß√µes. Os dados para o regressor $x_t$ e a vari√°vel dependente $y_t$ s√£o:
>
> | t | $x_t$ | $y_t$ |
> |---|---|---|
> | 1 | 1  | 2 |
> | 2 | 2  | 4 |
> | 3 | 3  | 5 |
> | 4 | 4  | 8 |
> | 5 | 5  | 9 |
>
> Primeiro, calculamos os coeficientes OLS ($ \hat{\beta}_{OLS} = (X'X)^{-1}X'y$). A matriz $X$ (incluindo uma coluna de 1s para o intercepto) e o vetor $y$ s√£o:
>
> $$ X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \\ 1 & 5 \end{bmatrix}, \quad y = \begin{bmatrix} 2 \\ 4 \\ 5 \\ 8 \\ 9 \end{bmatrix} $$
>
> Usando Python com `numpy`, podemos calcular $\hat{\beta}_{OLS}$:
>
> ```python
> import numpy as np
>
> X = np.array([[1, 1], [1, 2], [1, 3], [1, 4], [1, 5]])
> y = np.array([2, 4, 5, 8, 9])
>
> beta_ols = np.linalg.inv(X.T @ X) @ X.T @ y
> print(f"Beta OLS: {beta_ols}")
> ```
>
> Isso resulta em $\hat{\beta}_{OLS} \approx \begin{bmatrix} 1.2 \\ 1.5 \end{bmatrix}$. Os res√≠duos OLS, $\hat{u}_t = y_t - \hat{y}_t = y_t - (X\hat{\beta}_{OLS})_t$, s√£o:
>
> $$\hat{u} = \begin{bmatrix} -0.7 \\ -0.4 \\ -0.1 \\  0.8 \\ 0.4\end{bmatrix}$$
>
> Agora, para calcular a matriz de covari√¢ncia robusta de White, $\hat{\Omega}$, primeiro calculamos $X'X$, que j√° foi usado para o OLS, e o termo do meio:
>
> $$ \frac{1}{T} \sum_{t=1}^{T} \hat{u}_t^2 x_t x_t' = \frac{1}{5} \sum_{t=1}^{5} \hat{u}_t^2 x_t x_t' = \frac{1}{5}\left[\begin{bmatrix} (-0.7)^2 & (-0.7)^2 \\ (-0.7)^2 & (-0.7)^2\end{bmatrix} + \begin{bmatrix} (-0.4)^2 & 2(-0.4)^2 \\ 2(-0.4)^2 & 4(-0.4)^2\end{bmatrix} + \ldots \right] $$
>
> Calculando essa soma:
>
> $$ \frac{1}{5} \sum_{t=1}^{5} \hat{u}_t^2 x_t x_t' \approx \begin{bmatrix}0.214 & 0.756 \\ 0.756 & 2.82\end{bmatrix} $$
>
> Agora podemos calcular $\hat{\Omega}$:
>
> $$ \hat{\Omega} = (X'X/T)^{-1} \left( \frac{1}{T} \sum_{t=1}^{T} \hat{u}_t^2 x_t x_t' \right) (X'X/T)^{-1} $$
>
> ```python
> u_hat = y - X @ beta_ols
> omega_mid = np.zeros((2,2))
> for t in range(5):
>  xt = X[t,:].reshape(-1,1)
>  omega_mid += (u_hat[t]**2) * (xt @ xt.T)
> omega_mid = omega_mid/5
>
> omega_hat = np.linalg.inv(X.T @ X/5) @ omega_mid @ np.linalg.inv(X.T @ X/5)
> print(f"Matriz de Covari√¢ncia Robusta: \n{omega_hat}")
> ```
>
> Este c√°lculo resulta em:
>
>  $$
> \hat{\Omega} \approx \begin{bmatrix} 0.28 & -0.1 \\ -0.1 & 0.04 \end{bmatrix}
> $$
>
> Os erros padr√£o robustos de White s√£o as ra√≠zes quadradas dos elementos da diagonal de $\hat{\Omega}$, aproximadamente 0.52 para o intercepto e 0.2 para o coeficiente do regressor. Esses erros padr√£o s√£o utilizados para infer√™ncias estat√≠sticas que s√£o robustas √† heterocedasticidade.

**Propriedades Assint√≥ticas:** Sob condi√ß√µes de regularidade, o estimador de White √© consistente para a matriz de vari√¢ncia-covari√¢ncia assint√≥tica dos coeficientes OLS, mesmo na presen√ßa de heterocedasticidade [^1]. Ou seja, usando esses erros padr√£o robustos nos testes de hip√≥tese e nos intervalos de confian√ßa, obt√©m-se resultados assintoticamente v√°lidos [^1].

**Import√¢ncia Pr√°tica:** O uso de estimadores robustos √© crucial em an√°lises emp√≠ricas, pois a heterocedasticidade √© frequentemente encontrada em dados reais [^1]. A aplica√ß√£o desses estimadores evita infer√™ncias incorretas devido √† viola√ß√£o das suposi√ß√µes cl√°ssicas de regress√£o.

**Observa√ß√£o:** √â importante notar que o estimador de White, apesar de sua robustez, √© um estimador assint√≥tico. Em amostras pequenas, seu desempenho pode n√£o ser √≥timo, e outras alternativas podem ser consideradas, como estimadores jackknife ou bootstrap para obter erros padr√£o mais precisos.

### M√≠nimos Quadrados Generalizados (GLS)
O estimador GLS √© uma alternativa aos OLS quando a matriz de vari√¢ncia-covari√¢ncia dos erros n√£o √© escalar [^1]. Em outras palavras, quando $E(uu') \neq \sigma^2 I_T$, o estimador GLS se torna o melhor estimador linear n√£o viesado (BLUE). Este estimador aborda as limita√ß√µes do OLS, principalmente quando os erros s√£o correlacionados ou heteroced√°sticos [^1].

O modelo GLS pode ser expresso da seguinte forma:
$$y = X\beta + u$$
onde $u | X \sim N(0, \sigma^2 V)$, sendo $V$ uma matriz de vari√¢ncia-covari√¢ncia conhecida [^1]. Para implementar o GLS, transformamos o modelo multiplicando-o por $L$, onde $V^{-1} = L'L$ [^1]:
$$Ly = LX\beta + Lu$$
$$\tilde{y} = \tilde{X}\beta + \tilde{u}$$
Nesse novo modelo, temos $\tilde{u}|X \sim N(0, \sigma^2 I_T)$, assim, as suposi√ß√µes cl√°ssicas do modelo de regress√£o linear s√£o satisfeitas [^1]. O estimador GLS √© obtido aplicando OLS ao modelo transformado [^1]:
$$
\hat{\beta}_{GLS} = (\tilde{X}'\tilde{X})^{-1}\tilde{X}'\tilde{y} = (X'V^{-1}X)^{-1}X'V^{-1}y
$$
A matriz de vari√¢ncia-covari√¢ncia de $\hat{\beta}_{GLS}$ √© dada por:
$$
Var(\hat{\beta}_{GLS}|X) = \sigma^2 (X'V^{-1}X)^{-1}
$$
**Propriedades:** O estimador GLS √© o BLUE sob as suposi√ß√µes acima, ou seja, ele possui a menor vari√¢ncia dentre todos os estimadores lineares n√£o viesados [^1].
*   **Efici√™ncia:** Quando a matriz V √© conhecida, GLS √© mais eficiente que OLS. OLS continua sendo n√£o viesado, mas GLS tem uma vari√¢ncia menor [^1].
*   **Generaliza√ß√£o:** O GLS engloba o OLS como um caso especial quando V √© uma matriz identidade ou um m√∫ltiplo dela [^1].

**Lema 1:** *O estimador OLS √© um caso especial do estimador GLS*.
*Prova:*
I. O estimador GLS √© dado por:
    $$\hat{\beta}_{GLS} = (X'V^{-1}X)^{-1}X'V^{-1}y$$
II. Se $V = I$, onde $I$ √© a matriz identidade, ent√£o $V^{-1} = I^{-1} = I$.
III. Substituindo $V^{-1}$ por $I$ na f√≥rmula do estimador GLS:
    $$\hat{\beta}_{GLS} = (X'IX)^{-1}X'Iy$$
IV. Como $X'I = X'$ e $IX = X$:
    $$\hat{\beta}_{GLS} = (X'X)^{-1}X'y$$
V. O lado direito da equa√ß√£o acima √© a f√≥rmula do estimador OLS. Portanto, quando a matriz de vari√¢ncia-covari√¢ncia dos erros √© uma matriz identidade (homocedasticidade e n√£o autocorrela√ß√£o), o estimador GLS se reduz ao estimador OLS.
‚ñ†

#### Exemplos de Aplica√ß√£o GLS

*   **Heterocedasticidade:** Quando a vari√¢ncia dos erros √© proporcional ao quadrado de uma vari√°vel explicativa, podemos usar o GLS para obter estimadores mais eficientes. A matriz $V$ pode ser constru√≠da usando as informa√ß√µes sobre a forma da heterocedasticidade [^1].
> üí° **Exemplo Num√©rico:**
> Suponha que a vari√¢ncia do erro seja proporcional ao quadrado do regressor $x_t$, ou seja, $Var(u_t) = \sigma^2 x_t^2$. A matriz de covari√¢ncia $V$ seria uma matriz diagonal com elementos $x_t^2$. Usando os mesmos dados do exemplo anterior, a matriz $V$ seria:
>
> $$ V = \sigma^2 \begin{bmatrix} 1^2 & 0 & 0 & 0 & 0 \\ 0 & 2^2 & 0 & 0 & 0 \\ 0 & 0 & 3^2 & 0 & 0 \\ 0 & 0 & 0 & 4^2 & 0 \\ 0 & 0 & 0 & 0 & 5^2 \end{bmatrix} = \sigma^2 \begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\ 0 & 4 & 0 & 0 & 0 \\ 0 & 0 & 9 & 0 & 0 \\ 0 & 0 & 0 & 16 & 0 \\ 0 & 0 & 0 & 0 & 25 \end{bmatrix}$$
>
> Para aplicar GLS, precisamos encontrar $V^{-1}$ e calcular $\hat{\beta}_{GLS} = (X'V^{-1}X)^{-1}X'V^{-1}y$.
>
> $$ V^{-1} = \frac{1}{\sigma^2} \begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\ 0 & 1/4 & 0 & 0 & 0 \\ 0 & 0 & 1/9 & 0 & 0 \\ 0 & 0 & 0 & 1/16 & 0 \\ 0 & 0 & 0 & 0 & 1/25 \end{bmatrix} $$
>
> Usando Python para calcular $\hat{\beta}_{GLS}$:
>
> ```python
> import numpy as np
>
> X = np.array([[1, 1], [1, 2], [1, 3], [1, 4], [1, 5]])
> y = np.array([2, 4, 5, 8, 9])
>
> V = np.diag([1,4,9,16,25])
> V_inv = np.linalg.inv(V)
>
> beta_gls = np.linalg.inv(X.T @ V_inv @ X) @ X.T @ V_inv @ y
> print(f"Beta GLS: {beta_gls}")
> ```
>
> O resultado para $\hat{\beta}_{GLS}$ √© aproximadamente $\begin{bmatrix} 1.1 \\ 1.54 \end{bmatrix}$. Note que os coeficientes GLS s√£o diferentes dos coeficientes OLS, e o GLS √© mais eficiente neste caso de heterocedasticidade.

*   **Autocorrela√ß√£o:** Se os erros forem correlacionados ao longo do tempo, podemos usar o GLS para levar essa estrutura de correla√ß√£o em conta [^1]. A matriz $V$ ser√° constru√≠da de acordo com o padr√£o de autocorrela√ß√£o identificado nos dados. Um exemplo de matriz V para um erro com processo AR(1) √© apresentado em [^1].

**Proposi√ß√£o 1:** *Em situa√ß√µes onde a matriz de vari√¢ncia-covari√¢ncia V n√£o √© completamente conhecida, mas pode ser estimada consistentemente, o estimador FGLS (M√≠nimos Quadrados Generalizados Fact√≠veis) pode ser utilizado*.
*Prova:*
I. O estimador GLS √© dado por $\hat{\beta}_{GLS} = (X'V^{-1}X)^{-1}X'V^{-1}y$, onde $V$ √© a matriz de vari√¢ncia-covari√¢ncia dos erros.
II. Em situa√ß√µes onde $V$ √© desconhecida, mas podemos obter um estimador consistente $\hat{V}$ tal que $\hat{V} \xrightarrow{p} V$ (converge em probabilidade para $V$), podemos substituir $V$ por $\hat{V}$ na f√≥rmula do estimador GLS.
III. O estimador FGLS √© ent√£o definido como $\hat{\beta}_{FGLS} = (X'\hat{V}^{-1}X)^{-1}X'\hat{V}^{-1}y$.
IV. Sob condi√ß√µes de regularidade, e usando o teorema de Slutsky, quando $\hat{V}$ converge em probabilidade para $V$, as propriedades assint√≥ticas do estimador FGLS se aproximam das propriedades assint√≥ticas do estimador GLS. Isso significa que, assintoticamente, $\hat{\beta}_{FGLS}$ tem as mesmas propriedades de efici√™ncia do $\hat{\beta}_{GLS}$.
‚ñ†

### Conclus√£o
A heterocedasticidade √© uma quest√£o comum em an√°lises econom√©tricas e de s√©ries temporais [^1]. A corre√ß√£o dessa condi√ß√£o √© essencial para obter resultados v√°lidos e infer√™ncias precisas [^1]. Estimadores robustos, como o de White, fornecem erros padr√£o consistentes sem suposi√ß√µes espec√≠ficas sobre a forma da heterocedasticidade [^1]. J√° os estimadores de m√≠nimos quadrados generalizados (GLS) oferecem uma abordagem mais eficiente quando a estrutura da heterocedasticidade ou autocorrela√ß√£o √© conhecida ou pode ser estimada de forma consistente [^1]. A escolha entre esses m√©todos depende das especificidades do problema, e ambos desempenham um papel importante nas an√°lises emp√≠ricas [^1]. Em casos pr√°ticos, podemos estimar a matriz de covari√¢ncia $V(\theta)$, sendo $\theta$ um vetor de par√¢metros que podemos estimar a partir dos dados [^1].

### Refer√™ncias
[^1]: Trechos retirados do texto fornecido.
<!-- END -->
