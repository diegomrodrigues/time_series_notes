## A Premissa da Normalidade dos Resíduos na Regressão Linear Clássica: Análise Detalhada

### Introdução
Dando continuidade à análise das premissas clássicas do modelo de regressão linear, este capítulo se concentrará na suposição de que os resíduos populacionais seguem uma distribuição normal, também conhecida como distribuição Gaussiana. Como exploramos anteriormente, a regressão linear clássica assume que os regressores são determinísticos, o que simplifica a análise e a estimação dos parâmetros [^1, ^4]. Também já abordamos a premissa de que os resíduos são independentes e identicamente distribuídos (i.i.d.) com média zero e variância constante [^5]. Agora, vamos investigar em detalhe a importância da normalidade dos resíduos, suas implicações para a inferência estatística e como essa premissa complementa a natureza determinística dos regressores e a premissa de resíduos i.i.d..

### Conceitos Fundamentais
A premissa de que os **resíduos populacionais ($u_t$) seguem uma distribuição normal (Gaussiana)** é crucial para a validade da inferência estatística no modelo de regressão linear [^1]. Essa premissa, formalizada como parte da Assunção 8.1(c) [^1], estabelece que os erros aleatórios no modelo são distribuídos de acordo com uma distribuição normal, com média zero e variância constante ($\sigma^2$). Matematicamente, isso é expresso como: $u_t \sim N(0, \sigma^2)$.

A distribuição normal, dada sua simetria e propriedades bem estabelecidas, simplifica a análise e a interpretação dos resultados da regressão. A normalidade dos resíduos permite aplicar testes estatísticos paramétricos, como os testes *t* e *F*, que são mais poderosos e precisos quando a distribuição dos erros é conhecida [^1]. Além disso, essa premissa é fundamental para derivar as distribuições exatas dos estimadores e estatísticas de teste, o que é essencial para construir intervalos de confiança e realizar testes de hipótese.

> 💡 **Exemplo Numérico:** Suponha que estamos analisando o efeito da temperatura ($x$) no rendimento de uma colheita ($y$). Depois de ajustar um modelo de regressão linear, os resíduos ($u_t$) representam as diferenças entre os rendimentos observados e os rendimentos previstos pelo modelo. Se os resíduos seguem uma distribuição normal, isso indica que os erros de previsão do nosso modelo são aleatórios e distribuídos simetricamente em torno de zero, o que é um bom sinal.
>
> Por exemplo, consideremos 10 observações de temperatura e rendimento, com um modelo de regressão linear simples: $y_t = \beta_0 + \beta_1 x_t + u_t$.
>
> Usando dados hipotéticos:
>
> | Temperatura ($x_t$) | Rendimento ($y_t$) |
> |---------------------|-------------------|
> | 20                 | 50                |
> | 22                 | 55                |
> | 25                 | 62                |
> | 28                 | 70                |
> | 30                 | 75                |
> | 24                 | 60                |
> | 26                 | 65                |
> | 29                 | 72                |
> | 21                 | 53                |
> | 23                 | 58                |
>
> Após ajustar o modelo de regressão linear, obtemos os seguintes resíduos (diferenças entre os valores observados e os valores previstos):
>
> | Resíduo ($u_t$) |
> |-----------------|
> | -1.2 |
> | 0.8  |
> | -0.5 |
> | 1.5  |
> | -2.1  |
> | 0.2  |
> | 0.1  |
> | -0.3  |
> | 1.7  |
> | -0.2 |
>
> Se plotarmos um histograma ou um gráfico Q-Q dos resíduos, e eles se aproximarem de uma distribuição normal centrada em zero, a premissa de normalidade é razoavelmente satisfeita. Podemos usar testes como o Shapiro-Wilk para verificar a normalidade.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy.stats import shapiro, probplot
>
> residuals = np.array([-1.2, 0.8, -0.5, 1.5, -2.1, 0.2, 0.1, -0.3, 1.7, -0.2])
>
> # Histogram
> plt.figure(figsize=(10, 4))
> plt.subplot(1, 2, 1)
> plt.hist(residuals, bins='auto', edgecolor='black')
> plt.title("Histograma dos Resíduos")
>
> # Q-Q plot
> plt.subplot(1, 2, 2)
> probplot(residuals, dist="norm", plot=plt)
> plt.title("Gráfico Q-Q dos Resíduos")
>
> plt.tight_layout()
> plt.show()
>
> # Shapiro-Wilk test
> stat, p = shapiro(residuals)
> print(f"Shapiro-Wilk Test: Statistic={stat:.3f}, p-value={p:.3f}")
> if p > 0.05:
>   print("Os resíduos parecem seguir uma distribuição normal.")
> else:
>    print("Os resíduos não parecem seguir uma distribuição normal.")
> ```
>
> A saída deste código produzirá um histograma e um gráfico Q-Q dos resíduos, e também o resultado do teste de Shapiro-Wilk, que pode ajudar a verificar a normalidade dos resíduos de forma mais precisa. Um p-valor alto no teste de Shapiro-Wilk sugere que os resíduos podem ser considerados como seguindo uma distribuição normal.

A importância da premissa de normalidade se manifesta na forma como as propriedades estatísticas do estimador de coeficientes OLS ($b$) são derivadas. Conforme mencionado anteriormente, sob as premissas de 8.1(a) e 8.1(b) [^1, ^4, ^5], $b$ é um estimador não viesado com matriz de variância-covariância $\sigma^2(X'X)^{-1}$ [^1]. No entanto, a Assunção 8.1(c), que assume a normalidade dos resíduos, adiciona informação crucial sobre a distribuição amostral do estimador. Conforme a equação [8.1.17] [^1],  quando $u$ é gaussiano, o estimador $b$ também é gaussiano e segue a distribuição $b \sim N(\beta, \sigma^2(X'X)^{-1})$.

Essa normalidade de $b$ é crucial porque permite realizar testes de hipóteses sobre os parâmetros do modelo. Por exemplo, o teste t, detalhado na equação [8.1.26] [^1], é utilizado para verificar se um coeficiente individual ($\beta_i$) é estatisticamente diferente de zero ou de um valor específico ($ \beta_i^0$). Este teste é construído com base na premissa de que a distribuição do estimador $b_i$ é normal.

> 💡 **Exemplo Numérico:** Voltando ao exemplo do fertilizante e crescimento de plantas, se quisermos verificar se o fertilizante tem um efeito significativo no crescimento, realizamos um teste de hipóteses para o coeficiente correspondente. Se os resíduos forem normais, podemos utilizar o teste t com confiança para obter um valor p. Por exemplo, seja a hipótese nula $H_0:\beta_1 = 0$.  O teste t é:
>
> $$ t = \frac{b_1 - 0}{\sqrt{s^2 \xi^{ii}}} $$
> onde $b_1$ é o estimador do coeficiente do fertilizante, $s^2$ é o estimador da variância do resíduo e $\xi^{ii}$ é o elemento diagonal correspondente de $(X'X)^{-1}$. Sob a premissa de normalidade, $t$ segue uma distribuição t de Student com $T-k$ graus de liberdade [^1], permitindo-nos calcular o valor p para verificar se há evidências contra a hipótese nula.
>
> Suponha que, após rodar a regressão, obtivemos $b_1 = 0.5$, $s^2 = 0.1$, e $\xi^{ii} = 0.04$ (elemento correspondente ao nosso coeficiente de interesse). O número de amostras é $T=20$ e o número de regressores é $k=2$. Então, a estatística t é:
>
> $$ t = \frac{0.5 - 0}{\sqrt{0.1 \times 0.04}} = \frac{0.5}{\sqrt{0.004}} = \frac{0.5}{0.0632} \approx 7.91 $$
> O valor crítico para um teste bicaudal com $\alpha = 0.05$ e $T-k = 18$ graus de liberdade é aproximadamente $2.10$. Como o valor absoluto da estatística $t$ (7.91) é maior que o valor crítico, rejeitamos a hipótese nula de que o coeficiente é zero. Isso sugere que o fertilizante tem um efeito estatisticamente significativo no crescimento das plantas.
>
> ```python
> import numpy as np
> from scipy.stats import t
>
> b1 = 0.5
> s2 = 0.1
> x_ii = 0.04
> T = 20
> k = 2
>
> t_statistic = b1 / np.sqrt(s2 * x_ii)
> print(f"Estatística t: {t_statistic:.2f}")
>
> degrees_freedom = T - k
> critical_value = t.ppf(1-0.025, degrees_freedom) #Two-tailed
> print(f"Valor crítico t (alpha=0.05, two-tailed): {critical_value:.2f}")
>
> p_value = 2 * (1 - t.cdf(abs(t_statistic), degrees_freedom))
> print(f"Valor p: {p_value:.4f}")
>
> if abs(t_statistic) > critical_value:
>   print("Rejeita-se a hipótese nula")
> else:
>  print("Não se rejeita a hipótese nula")
> ```
> Este código calcula a estatística t, o valor crítico com um nível de significância de 5% (two-tailed), e o valor p para o teste. Ele então decide se a hipótese nula deve ser rejeitada com base no valor t calculado e no valor p.

A normalidade dos resíduos também impacta na distribuição da estatística F [^1], utilizada para realizar testes de hipóteses conjuntas. A estatística F é construída sob a premissa de que o resíduo tem distribuição Gaussiana e, quando essa premissa é satisfeita, a estatística F segue uma distribuição F com $m$ e $T-k$ graus de liberdade sob a hipótese nula. A distribuição exata da estatística $F$ é derivada sob a premissa de normalidade. Conforme a equação [8.1.32], a estatística $F$ é dada por:

$$ F = \frac{(Rb - r)'[s^2R(X'X)^{-1}R']^{-1}(Rb - r)}{m} $$
onde $R$ é uma matriz de restrições, $r$ é o vetor dos valores de restrição, $m$ é o número de restrições testadas, e $s^2$ é o estimador da variância residual.
> 💡 **Exemplo Numérico:** Continuando com o exemplo anterior, se temos mais de um regressor, poderíamos usar o teste F para testar a hipótese de que um grupo de coeficientes (por exemplo, β₁ e β₂) são conjuntamente iguais a zero.
>
> Suponha que temos o modelo $y_t = \beta_0 + \beta_1 x_{1t} + \beta_2 x_{2t} + u_t$, onde $x_{1t}$ é a quantidade de fertilizante e $x_{2t}$ é a quantidade de água utilizada. Queremos testar a hipótese conjunta de que ambos os coeficientes, $\beta_1$ e $\beta_2$, são iguais a zero.
>
>  Seja  $R = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$, e $r = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$, o número de restrições $m=2$.
>
> Suponha que, após rodar a regressão, obtemos:
> $b = \begin{bmatrix} 10 \\ 0.3 \\ 0.4 \end{bmatrix}$, $s^2=0.05$, e $(X'X)^{-1} = \begin{bmatrix} 0.2 & -0.05 & 0.01 \\ -0.05 & 0.03 & -0.005 \\ 0.01 & -0.005 & 0.02 \end{bmatrix}$.
>
> Então:
>
> $Rb = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 10 \\ 0.3 \\ 0.4 \end{bmatrix} = \begin{bmatrix} 0.3 \\ 0.4 \end{bmatrix}$
>
>  $R(X'X)^{-1}R' = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 0.2 & -0.05 & 0.01 \\ -0.05 & 0.03 & -0.005 \\ 0.01 & -0.005 & 0.02 \end{bmatrix} \begin{bmatrix} 0 & 0 \\ 1 & 0 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 0.03 & -0.005 \\ -0.005 & 0.02 \end{bmatrix}$
>
> $[R(X'X)^{-1}R']^{-1} = \begin{bmatrix} 34.88 & 8.72 \\ 8.72 & 52.33 \end{bmatrix} $
>
> $(Rb - r)' [s^2R(X'X)^{-1}R']^{-1} (Rb - r) = [0.3 \ 0.4] \ 0.05^{-1}\begin{bmatrix} 34.88 & 8.72 \\ 8.72 & 52.33 \end{bmatrix}  \begin{bmatrix} 0.3 \\ 0.4 \end{bmatrix} = 27.31$
>
> $F = \frac{27.31}{2} = 13.655$
>
> O valor crítico de $F$ com 2 e $T-k=17$ graus de liberdade (assumindo $T=20$) a um nível de significância de 0.05 é aproximadamente 3.59. Dado que nosso valor F calculado (13.655) é maior que o valor crítico, rejeitamos a hipótese nula de que ambos os coeficientes são conjuntamente iguais a zero, o que indica que pelo menos um deles tem um efeito estatisticamente significativo.
>
> ```python
> import numpy as np
> from scipy.stats import f
>
> b = np.array([10, 0.3, 0.4])
> s2 = 0.05
> x_prime_x_inv = np.array([[0.2, -0.05, 0.01],
>                         [-0.05, 0.03, -0.005],
>                         [0.01, -0.005, 0.02]])
> R = np.array([[0, 1, 0],
>               [0, 0, 1]])
> r = np.array([0, 0])
> T = 20
> k = 3 #number of regressors
> m = 2  # number of restrictions
>
> Rb = R @ b
>
> r_x_inv_r_prime = R @ x_prime_x_inv @ R.T
>
> f_statistic_numerator = (Rb - r).T @ np.linalg.inv(s2*r_x_inv_r_prime) @ (Rb - r)
>
> f_statistic =  f_statistic_numerator/m
>
> print(f"Estatística F: {f_statistic:.3f}")
>
> critical_value = f.ppf(1-0.05, m, T-k)
> print(f"Valor crítico F (alpha=0.05): {critical_value:.3f}")
>
> p_value = 1- f.cdf(f_statistic, m, T-k)
> print(f"Valor p: {p_value:.4f}")
>
> if f_statistic > critical_value:
>   print("Rejeita-se a hipótese nula")
> else:
>  print("Não se rejeita a hipótese nula")
>
> ```
> O código acima calcula a estatística F e o valor-p, e decide se a hipótese nula deve ser rejeitada com base na comparação com o valor crítico, ilustrando como o teste F é usado na prática e o papel da premissa de normalidade para sua validade.

Sob a premissa de normalidade, o numerador de $F$ segue uma distribuição $\chi^2(m)$, onde $m$ é o número de restrições lineares e o denominador segue uma distribuição $\chi^2(T-k)$ dividido por seus graus de liberdade,  $T-k$, sendo ambos independentes. Portanto, $F$ segue a distribuição de probabilidade $F$ sob a hipótese nula, permitindo-nos realizar testes estatísticos de hipóteses conjuntas.

**Lema 2**
Sob a premissa de que os resíduos populacionais são normalmente distribuídos, a distribuição do estimador OLS $b$ também é normal.
*Demonstração:*
I. A equação [8.1.12] nos mostra que $b = \beta + (X'X)^{-1}X'u$.
II. Se os resíduos $u$ são normalmente distribuídos, então qualquer combinação linear de $u$ também é normalmente distribuída, ou seja, $X'u$ também segue uma distribuição normal.
III. Consequentemente, como $(X'X)^{-1}X'$ é uma matriz de valores fixos, então $(X'X)^{-1}X'u$ segue uma distribuição normal, já que é uma combinação linear de variáveis normais.
IV. A soma de um vetor constante ($\beta$) com um vetor normal ($ (X'X)^{-1}X'u $) também tem uma distribuição normal, e logo $b$ é normalmente distribuído.
$\blacksquare$

**Proposição 2**
A premissa de normalidade dos resíduos implica que o teste t, dado em [8.1.26], segue uma distribuição t de Student com $T-k$ graus de liberdade sob a hipótese nula.
*Demonstração:*
I.  A estatística do teste t é dada por: $$ t = \frac{b_i - \beta^0_i}{\sqrt{s^2\xi^{ii}}} $$
II. Do Lema 2, sabemos que $b_i$ é normalmente distribuído sob a premissa de normalidade dos resíduos, com média $\beta_i$ e variância $\sigma^2\xi^{ii}$, onde $\xi^{ii}$ é o elemento diagonal correspondente de $(X'X)^{-1}$. Assim,  $\frac{b_i - \beta_i}{\sqrt{\sigma^2\xi^{ii}}}$ segue uma distribuição normal padrão $N(0,1)$.
III. O estimador da variância, $s^2$, é independente de $b$ sob a normalidade dos resíduos, como expresso na equação [8.1.25] [^1].
IV. Conforme a equação [8.1.24] [^1], $\frac{(T-k)s^2}{\sigma^2}$ segue uma distribuição $\chi^2(T-k)$.
V. O teste t é a razão entre a distribuição normal padrão e a raiz quadrada da distribuição $\chi^2$ dividida por seus graus de liberdade, o que define a distribuição t de Student com $T-k$ graus de liberdade.
$\blacksquare$
> 💡 **Exemplo Numérico:**
> Continuando com o exemplo anterior, vamos testar a hipótese de que o coeficiente $\beta_1$ é igual a zero, usando os mesmos dados e os resultados já obtidos para os estimadores e erros padrão.
>
> A estatística t é dada por:
> $t = \frac{b_1 - 0}{\sqrt{s^2 \xi^{11}}}$,
>
> onde $b_1$ = 0.5 é o coeficiente estimado para o regressor $x$,  $s^2$ = 0.1 é a variância do erro e $\xi^{11}$ = 0.04 é o elemento diagonal correspondente de $(X'X)^{-1}$.
>
> $t = \frac{0.5}{\sqrt{0.1 \times 0.04}} = \frac{0.5}{\sqrt{0.004}} \approx 7.9$.
>
> Como vimos anteriormente, sob a premissa de normalidade, essa estatística t segue uma distribuição t de Student com $20-2=18$ graus de liberdade. Com um valor t de 7.9, o valor-p (probabilidade de obter um valor t tão extremo quanto 7.9) é praticamente zero, dado que a distribuição t de Student tem uma cauda muito fina. Portanto, podemos rejeitar a hipótese nula de que o coeficiente da variável fertilizante é igual a zero, e concluir que o fertilizante tem um efeito estatisticamente significativo no rendimento da colheita.
>
> ```python
> import numpy as np
> from scipy.stats import t
>
> b1 = 0.5
> s2 = 0.1
> x_ii = 0.04
> T = 20
> k = 2
>
> t_statistic = b1 / np.sqrt(s2 * x_ii)
> degrees_freedom = T - k
> p_value = 2 * (1 - t.cdf(abs(t_statistic), degrees_freedom))
>
> print(f"Estatística t: {t_statistic:.2f}")
> print(f"Valor p: {p_value:.4f}")
>
> critical_value = t.ppf(1-0.025, degrees_freedom)
> print(f"Valor crítico: {critical_value:.2f}")
>
> if abs(t_statistic) > critical_value:
>   print("Rejeita-se a hipótese nula")
> else:
>  print("Não se rejeita a hipótese nula")
> ```
> Este código exemplifica como realizar um teste t, o qual depende da normalidade dos resíduos. Ele calcula a estatística t e o valor-p, e decide sobre a hipótese nula com base no valor p e no nível de significância estabelecido.

**Teorema 2.1**
Sob as premissas do modelo clássico de regressão linear, incluindo a normalidade dos resíduos, o estimador da variância residual $s^2 = \frac{RSS}{T-k}$ é um estimador não viesado da variância populacional $\sigma^2$.

*Demonstração:*
I. Sabemos que $RSS = u'M_Xu$, onde $M_X = I - X(X'X)^{-1}X'$ é a matriz idempotente que projeta os resíduos no espaço ortogonal ao espaço das colunas de $X$.
II.  Conforme a equação [8.1.24] [^1], $\frac{(T-k)s^2}{\sigma^2} = \frac{RSS}{\sigma^2}$ segue uma distribuição $\chi^2(T-k)$. O valor esperado de uma variável com distribuição $\chi^2(df)$ é $df$.
III. Assim, $E[\frac{RSS}{\sigma^2}] = T-k$.
IV.  Multiplicando ambos os lados por $\frac{\sigma^2}{T-k}$, obtemos:  $E[\frac{RSS}{T-k}] = \sigma^2$.
V. Portanto, $E[s^2] = \sigma^2$, o que demonstra que $s^2$ é um estimador não viesado de $\sigma^2$.
$\blacksquare$

A premissa de normalidade dos resíduos também tem implicações na distribuição da variância estimada ($s^2$). A equação [8.1.24] estabelece que  $RSS/\sigma^2 = u'M_Xu/\sigma^2$ segue uma distribuição qui-quadrado ($\chi^2$) com $T-k$ graus de liberdade [^1], onde $M_X$ é a matriz idempotente $I - X(X'X)^{-1}X'$. Esta propriedade é fundamental para a construção de testes estatísticos sobre a variância do erro.

> 💡 **Exemplo Numérico:** Se quisermos realizar um teste de hipótese sobre a variância do erro, por exemplo, se a variância é diferente de um valor determinado, a distribuição qui-quadrado é utilizada para realizar a inferência estatística.
>
> Por exemplo, suponha que em nosso modelo de rendimento da colheita temos $T = 50$ observações e $k=2$ regressores e calculamos $RSS = 10$. O nosso estimador de variância é $s^2 = \frac{RSS}{T-k} = \frac{10}{50-2} = \frac{10}{48} = 0.2083$.
>
> Se quisermos testar se a variância é igual a um valor específico (digamos, $\sigma^2_0 = 0.2$), podemos usar a estatística:
> $$ \frac{(T-k)s^2}{\sigma^2_0} = \frac{48 \times 0.2083}{0.2} = 50 $$
>
> Sob a premissa de normalidade dos resíduos, essa estatística segue uma distribuição $\chi^2$ com $T-k=48$ graus de liberdade.  Podemos consultar a tabela da distribuição $\chi^2$ para determinar o valor p e concluir se existe evidência para rejeitar a hipótese nula de que a variância é igual a 0.2. Se o valor p é abaixo de um certo nível de significância (ex: 0.05), rejeitamos a hipótese nula.
>
> ```python
> import numpy as np
> from scipy.stats import chi2
>
> T = 50
> k = 2
> RSS = 10
> s2 = RSS / (T-k)
> sigma2_0 = 0.2
>
> chi2_statistic = (T - k) * s2 / sigma2_0
> degrees_freedom = T - k
>
> p_value = 1 - chi2.cdf(chi2_statistic, degrees_freedom)
>
> print(f"Estatística Qui-Quadrado: {chi2_statistic:.2f}")
> print(f"Valor p: {p_value:.4f}")
>
> critical_value = chi2.ppf(1-0.05, degrees_freedom)
> print(f"Valor crítico: {critical_value:.2f}")
>
> if chi2_statistic > critical_value:
>     print("Rejeita-se a hipótese nula")
> else:
>     print("Não se rejeita a hipótese nula")
> ```
> Este código calcula a estatística do teste qui-quadrado e o valor p, o qual assume a normalidade dos resíduos. O valor p indica se há evidências estatísticas para rejeitar a hipótese nula de que a variância do erro é igual a um valor especificado.

**Observação 1:** A premissa de normalidade dos resíduos é crucial para que a distribuição exata do estimador OLS ($b$) e das estatísticas de teste t e F possam ser derivadas. Se essa premissa não for satisfeita, as distribuições dos estimadores e estatísticas podem não seguir as distribuições t e F, invalidando a inferência estatística. Em muitos casos, essa premissa pode ser relaxada utilizando o teorema do limite central quando o tamanho da amostra é grande. No entanto, em amostras pequenas, a violação dessa premissa pode levar a resultados pouco confiáveis. Em particular, é importante notar que o tamanho amostral necessário para que o teorema do limite central possa ser invocado varia dependendo do grau em que a distribuição dos resíduos se afasta da normalidade.

**Observação 2:** Embora o estimador OLS seja não viesado mesmo sob violações da normalidade (contanto que $E(u)=0$), os testes de hipóteses (como o teste t e o teste F) só são válidos sob normalidade dos resíduos (e sob homocedasticidade, e ausência de autocorrelação), para amostras pequenas. Sob amostras grandes, o teorema do limite central garante que a distribuição dos estimadores se aproxima da normal, e que a distribuição dos testes se aproxima da distribuição t ou F, mesmo sem a normalidade dos resíduos. Contudo, em amostras pequenas, a validade dos testes pode estar comprometida na ausência de resíduos normais.

**Teorema 2.2**
Sob as premissas do modelo clássico de regressão linear, incluindo a normalidade dos resíduos, o estimador da variância residual $s^2 = \frac{RSS}{T-k}$ é independente do estimador OLS $b$.

*Demonstração:*
I. Sabemos que $b = \beta + (X'X)^{-1}X'u$ e $RSS = u'M_Xu$.
II.  A independência entre $s^2$ e $b$ se mantém se a covariância entre eles for nula. Ou seja, precisamos mostrar que $Cov(b, s^2) = 0$.
III. $Cov(b, s^2) = Cov( (X'X)^{-1}X'u, \frac{u'M_Xu}{T-k}) =  \frac{1}{T-k}Cov( (X'X)^{-1}X'u, u'M_Xu)$.
IV.  Sob a premissa de normalidade dos resíduos, $Cov(u_i, u_j^2) = 0$ para todos $i$ e $j$ [^1]. Este resultado é uma propriedade da distribuição normal.
V. Dado que $Cov( (X'X)^{-1}X'u, u'M_Xu)$ é uma soma de termos que contêm esses tipos de covariâncias (entre os resíduos individuais e os quadrados dos resíduos), então a covariância é igual a zero.
VI. Portanto, $Cov(b, s^2) = 0$, mostrando que os estimadores são independentes.
$\blacksquare$

**Corolário 2.1**
Sob as premissas do modelo clássico de regressão linear, incluindo a normalidade dos resíduos, a estatística t dada por $\frac{b_i-\beta^0_i}{\sqrt{s^2\xi^{ii}}}$ é independente do estimador da variância $s^2$

*Demonstração:*
I.  Pelo Teorema 2.2, sabemos que $b$ e $s^2$ são independentes.
II.  A estatística t é uma função de $b$ e $s^2$.
III. Como $b$ e $s^2$ são independentes, qualquer função de $b$ será independente de qualquer função de $s^2$.
IV. Portanto, a estatística t é independente de $s^2$.
$\blacksquare$

### Conclusão
Em suma, a premissa de normalidade dos resíduos populacionais é uma pedra angular da regressão linear clássica [^1]. Essa premissa, em conjunto com a premissa de regressores determinísticos e a premissa de que os resíduos são i.i.d., possibilita a aplicação de ferramentas de inferência estatística, como os testes t e F, e a construção de intervalos de confiança para os parâmetros do modelo. Além disso, a distribuição normal também afeta a distribuição dos estimadores de variância. Sob normalidade e as demais premissas, os estimadores do modelo OLS têm distribuição normal, as estatísticas t seguem distribuição t de Student e as estatísticas F seguem distribuição F, com graus de liberdade bem definidos. Embora essa premissa simplifique a análise, é importante reconhecer que nem sempre ela se mantém na prática. Nas próximas seções, exploraremos situações onde essa premissa é relaxada e analisaremos as implicações para a inferência estatística, e também modelos que não requerem essa premissa tão restritiva. A compreensão da importância da premissa de normalidade dos resíduos permite usar modelos de regressão de maneira mais informada e eficaz, ao mesmo tempo em que entendemos suas limitações e as alternativas disponíveis.

### Referências
[^1]: Trecho do texto original fornecido.
[^2]: Capítulo anterior sobre a premissa de regressores determinísticos.
[^3]: Capítulo anterior sobre a premissa de resíduos i.i.d.
[^4]: Capítulo anterior sobre a premissa de regressores determinísticos.
[^5]: Capítulo anterior sobre a premissa de resíduos i.i.d.
<!-- END -->
