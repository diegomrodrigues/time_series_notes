## A Premissa de ResÃ­duos Independentes e Identicamente DistribuÃ­dos na RegressÃ£o Linear ClÃ¡ssica: ImplicaÃ§Ãµes Detalhadas

### IntroduÃ§Ã£o
Neste capÃ­tulo, aprofundamos o estudo das premissas do modelo de regressÃ£o linear clÃ¡ssico, com foco na suposiÃ§Ã£o de que os resÃ­duos populacionais sÃ£o independentes e identicamente distribuÃ­dos (i.i.d.), com mÃ©dia zero e variÃ¢ncia constante. Como discutimos nos capÃ­tulos anteriores, a regressÃ£o linear clÃ¡ssica assume que os regressores sÃ£o determinÃ­sticos e que os resÃ­duos sÃ£o normalmente distribuÃ­dos, premissas essas que permitem derivar resultados estatÃ­sticos [^1, ^2, ^3]. Exploraremos agora a importÃ¢ncia da independÃªncia e distribuiÃ§Ã£o idÃªntica dos resÃ­duos, suas implicaÃ§Ãµes e como essa premissa se encaixa no arcabouÃ§o da anÃ¡lise de regressÃ£o. Este capÃ­tulo irÃ¡ detalhar as implicaÃ§Ãµes desta premissa para a variÃ¢ncia do estimador OLS, e como esta premissa se relaciona com a propriedade de nÃ£o viÃ©s do estimador OLS, e como a violaÃ§Ã£o desta premissa afeta as propriedades dos testes de hipÃ³tese.

### Conceitos Fundamentais
A premissa de que os **resÃ­duos populacionais ($u_t$) sÃ£o independentes e identicamente distribuÃ­dos (i.i.d.), com mÃ©dia zero e variÃ¢ncia constante ($\sigma^2$)**, Ã© um dos pilares do modelo de regressÃ£o linear clÃ¡ssico [^1]. Formalmente expressa na AssunÃ§Ã£o 8.1(b) [^1], essa premissa estabelece que:
1.  **IndependÃªncia:** Os resÃ­duos ($u_t$) sÃ£o independentes entre si, ou seja, o valor de um resÃ­duo em um ponto de tempo $t$ nÃ£o tem correlaÃ§Ã£o com o valor de outro resÃ­duo em um ponto de tempo $s$, onde $t \neq s$.
2.  **DistribuiÃ§Ã£o IdÃªntica:** Os resÃ­duos ($u_t$) seguem a mesma distribuiÃ§Ã£o de probabilidade para todos os pontos de tempo, com mÃ©dia zero e variÃ¢ncia $\sigma^2$.
3.  **MÃ©dia Zero:** O valor esperado de cada resÃ­duo Ã© zero, $E(u_t) = 0$.
4.  **VariÃ¢ncia Constante (Homocedasticidade):** A variÃ¢ncia de cada resÃ­duo Ã© a mesma, ou seja, $Var(u_t) = \sigma^2$, para todo $t$.

Em notaÃ§Ã£o matemÃ¡tica, essa premissa Ã© expressa como $u_t \sim i.i.d.(0, \sigma^2)$ [^1].

Essa premissa Ã© crucial para a validade das inferÃªncias estatÃ­sticas no modelo de regressÃ£o linear. Quando os resÃ­duos sÃ£o independentes e identicamente distribuÃ­dos, a variabilidade dos dados nÃ£o Ã© sistematicamente influenciada por fatores nÃ£o incluÃ­dos no modelo, e os testes de hipÃ³teses e intervalos de confianÃ§a podem ser interpretados com mais confianÃ§a [^1].

> ðŸ’¡ **Exemplo NumÃ©rico:** Imagine que estamos analisando a relaÃ§Ã£o entre o gasto com publicidade ($x$) e as vendas de um produto ($y$) ao longo de vÃ¡rios meses. Os resÃ­duos ($u_t$) representam a diferenÃ§a entre as vendas observadas e as vendas previstas pelo modelo.
>
> *   **IndependÃªncia:** Se os resÃ­duos sÃ£o i.i.d., entÃ£o, o erro de previsÃ£o para o mÃªs de janeiro nÃ£o deve influenciar o erro de previsÃ£o para o mÃªs de fevereiro. NÃ£o deve haver padrÃµes de erro se repetindo ao longo do tempo, ou seja, um resÃ­duo positivo nÃ£o implica que o prÃ³ximo serÃ¡ negativo (ou positivo).
> *   **DistribuiÃ§Ã£o IdÃªntica:**  A distribuiÃ§Ã£o dos erros deve ser consistente ao longo dos meses. A variabilidade dos erros deve ser aproximadamente a mesma ao longo do tempo e nÃ£o deve aumentar ou diminuir sistematicamente. NÃ£o devem haver meses em que o modelo gera erros maiores sistematicamente.
> *   **MÃ©dia Zero:** Em mÃ©dia, o modelo deve gerar previsÃµes corretas, com erros se distribuindo de maneira balanceada (em torno de zero). NÃ£o deve haver um viÃ©s sistemÃ¡tico, onde o modelo superestima ou subestima os valores de $y$ sistematicamente.
> *   **VariÃ¢ncia Constante (Homocedasticidade):** Os erros do modelo devem ser igualmente dispersos ao longo de todos os valores de $x$ e ao longo dos perÃ­odos de tempo. A variabilidade dos erros nÃ£o deve ser menor (ou maior) em diferentes nÃ­veis de gastos com publicidade.
>
> Suponha que temos os seguintes dados de gastos com publicidade (em milhares de reais) e vendas (em milhares de unidades) em 6 meses:
>
> | MÃªs  | Publicidade (x) | Vendas (y) |
> |------|-----------------|------------|
> | Jan  | 2               | 5          |
> | Fev  | 3               | 7          |
> | Mar  | 4               | 8          |
> | Abr  | 5               | 9          |
> | Mai  | 6               | 11         |
> | Jun  | 7               | 12         |
>
> Um modelo de regressÃ£o linear simples ajustado a esses dados resulta na equaÃ§Ã£o $\hat{y} = 2.5 + 1.3x$. Os resÃ­duos para cada mÃªs sÃ£o entÃ£o:
>
> | MÃªs  | Publicidade (x) | Vendas (y) | Vendas Previstas ($\hat{y}$) | ResÃ­duo ($u_t$) |
> |------|-----------------|------------|----------------------------|-----------------|
> | Jan  | 2               | 5          | 5.1                        | -0.1            |
> | Fev  | 3               | 7          | 6.4                        | 0.6             |
> | Mar  | 4               | 8          | 7.7                        | 0.3             |
> | Abr  | 5               | 9          | 9.0                        | 0.0             |
> | Mai  | 6               | 11         | 10.3                       | 0.7             |
> | Jun  | 7               | 12         | 11.6                       | 0.4             |
>
> Para que a premissa de resÃ­duos i.i.d. seja vÃ¡lida, estes resÃ­duos nÃ£o devem apresentar padrÃµes.  Por exemplo, nÃ£o deverÃ­amos ver todos os resÃ­duos positivos no comeÃ§o e todos negativos no final, ou resÃ­duos maiores nos meses de publicidade mais alta. AlÃ©m disso, a variabilidade dos resÃ­duos deve ser aproximadamente a mesma ao longo dos meses. Um grÃ¡fico dos resÃ­duos vs. os valores de $x$ (ou vs. o tempo) ajudaria a identificar padrÃµes e possÃ­veis problemas com esta premissa.
>
> ```mermaid
>  graph LR
>      A[Publicidade (x)] --> B(ResÃ­duos);
>      C[Tempo] --> B;
>  style B fill:#f9f,stroke:#333,stroke-width:2px
> ```
> Este grÃ¡fico ajudaria a visualizar se a variabilidade dos resÃ­duos estÃ¡ relacionada a $x$ ou ao tempo.

A independÃªncia dos resÃ­duos Ã© especialmente importante em anÃ¡lises de sÃ©ries temporais, onde os dados sÃ£o coletados sequencialmente ao longo do tempo. Em tais casos, a violaÃ§Ã£o da independÃªncia pode levar a estimativas viesadas e testes de hipÃ³teses incorretos. Essa propriedade Ã© frequentemente testada usando estatÃ­sticas como o teste de Durbin-Watson.

> ðŸ’¡ **Exemplo NumÃ©rico:**  Voltando ao exemplo de gastos com publicidade e vendas, se os resÃ­duos sÃ£o dependentes, podemos encontrar um padrÃ£o como o seguinte: meses com resÃ­duos positivos tendem a ser seguidos por meses com resÃ­duos positivos, e vice-versa. Isso indicaria que hÃ¡ alguma estrutura temporal nos resÃ­duos que o modelo nÃ£o estÃ¡ capturando, e que a premissa de independÃªncia Ã© violada. A presenÃ§a de correlaÃ§Ã£o serial nos resÃ­duos pode ser um indicativo de que variÃ¡veis relevantes estÃ£o faltando no modelo ou que a especificaÃ§Ã£o do modelo estÃ¡ inadequada, seja por um erro na escolha da forma funcional do modelo (linear, logarÃ­tmica, etc) ou por nÃ£o considerar variÃ¡veis importantes.
>
> Por exemplo, se em nosso exemplo de gastos com publicidade, os resÃ­duos dos primeiros trÃªs meses fossem -0.5, -0.3, -0.1 e os resÃ­duos dos trÃªs Ãºltimos meses fossem 0.5, 0.3, 0.1, isso indicaria uma correlaÃ§Ã£o positiva serial nos erros, sugerindo que o erro em um mÃªs afeta o erro no prÃ³ximo. Se a correlaÃ§Ã£o serial for alta, as estimativas dos coeficientes podem ser imprecisas e os testes de hipÃ³tese podem nÃ£o ser vÃ¡lidos. Podemos quantificar isso calculando a autocorrelaÃ§Ã£o dos resÃ­duos. A autocorrelaÃ§Ã£o de primeira ordem, $\rho_1$, Ã© a correlaÃ§Ã£o entre $u_t$ e $u_{t-1}$. Em nosso exemplo,
>
> ```python
> import numpy as np
> residuals = np.array([-0.5, -0.3, -0.1, 0.5, 0.3, 0.1])
> # Calcula autocorrelaÃ§Ã£o de primeira ordem (usando uma aproximaÃ§Ã£o)
> def autocorr(x, t=1):
>     return np.corrcoef(x[:-t], x[t:])[0, 1]
> rho_1 = autocorr(residuals)
> print(f"AutocorrelaÃ§Ã£o de primeira ordem: {rho_1:.2f}")
> ```
> Um valor alto (prÃ³ximo de 1 ou -1) de $\rho_1$ indicaria forte autocorrelaÃ§Ã£o, o que invalidaria a premissa de resÃ­duos independentes.

A propriedade de distribuiÃ§Ã£o idÃªntica tambÃ©m Ã© importante. Se a variÃ¢ncia dos resÃ­duos nÃ£o for constante, entÃ£o dizemos que existe heterocedasticidade, onde a qualidade da previsÃ£o do modelo varia ao longo do tempo ou ao longo dos valores de $x$. A presenÃ§a de heterocedasticidade tambÃ©m pode levar a inferÃªncias estatÃ­sticas incorretas.

A premissa de que $E(u_t) = 0$ garante que o modelo de regressÃ£o nÃ£o tenha um viÃ©s sistemÃ¡tico. Essa premissa garante que os parÃ¢metros do modelo nÃ£o estejam sistematicamente sobre ou subestimados devido a erros que nÃ£o tÃªm mÃ©dia zero.  O estimador OLS $(b)$ Ã© um estimador nÃ£o-viesado sob a premissa 8.1(a) e 8.1(b) [^1]. A equaÃ§Ã£o [8.1.15] [^1] demonstra formalmente que $E(b) = \beta$ quando a mÃ©dia dos erros Ã© zero, $E(u)=0$, com a derivaÃ§Ã£o:
$$ E(b) = E(\beta + (X'X)^{-1}X'u) = \beta + (X'X)^{-1}X'E(u) = \beta $$

A premissa de variÃ¢ncia constante, ou homocedasticidade, permite que a variÃ¢ncia dos erros seja estimada de forma precisa e utilizada para construir intervalos de confianÃ§a e realizar testes de hipÃ³teses. A matriz de variÃ¢ncia-covariÃ¢ncia dos estimadores OLS Ã© dada por $\sigma^2(X'X)^{-1}$ [^1], conforme expresso na equaÃ§Ã£o [8.1.16]. A pressuposiÃ§Ã£o de variÃ¢ncia constante ($\sigma^2$) Ã© crÃ­tica para a derivaÃ§Ã£o desta matriz e para a validade das inferÃªncias estatÃ­sticas.
> ðŸ’¡ **Exemplo NumÃ©rico:** Retornando ao exemplo de publicidade e vendas, suponha que a variabilidade dos erros Ã© muito menor para valores de gastos com publicidade mais baixos do que para valores mais altos. Isso indicaria que a premissa de homocedasticidade Ã© violada.
>
> Vamos supor que, analisando nossos dados de publicidade e vendas, os resÃ­duos para os meses com publicidade baixa (2 e 3 mil) sÃ£o -0.1 e 0.2, respectivamente, enquanto os resÃ­duos para os meses com publicidade alta (6 e 7 mil) sÃ£o -1.5 e 1.8. A variabilidade dos resÃ­duos (em valor absoluto) parece aumentar com o aumento do gasto com publicidade. Isso sugere que a variÃ¢ncia do erro nÃ£o Ã© constante (heterocedasticidade) e que as inferÃªncias baseadas no modelo podem ser invÃ¡lidas.
>
>  Vamos considerar um exemplo numÃ©rico mais especÃ­fico. Suponha que temos um modelo de regressÃ£o simples, $y_i = \beta_0 + \beta_1 x_i + u_i$, e as seguintes duas amostras:
>
> *   **Amostra 1:** $x$ = \[1, 2, 3] e resÃ­duos $u$ = \[0.1, -0.1, 0.2]
> *   **Amostra 2:** $x$ = \[4, 5, 6] e resÃ­duos $u$ = \[0.5, -0.4, 0.6]
>
> Em ambos os casos, a mÃ©dia dos resÃ­duos Ã© aproximadamente zero. Contudo, na amostra 2, a variabilidade dos erros Ã© muito maior que na amostra 1. Se a variÃ¢ncia dos erros Ã© dependente dos regressores, como mostrado nesse exemplo, a premissa da homocedasticidade Ã© violada, o que impacta a validade das inferÃªncias estatÃ­sticas. Em particular, o uso de um estimador de variÃ¢ncia $\sigma^2$ que presume a homocedasticidade pode levar a testes de hipÃ³tese que sÃ£o muito otimistas, ou seja, que rejeitam a hipÃ³tese nula com muita frequÃªncia, ou a intervalos de confianÃ§a muito estreitos, o que leva a conclusÃµes falsas sobre a significÃ¢ncia dos coeficientes.
>
> Formalmente, em um modelo com heterocedasticidade, a matriz de variÃ¢ncia-covariÃ¢ncia do erro Ã© dada por $E(uu') = \Sigma$, onde $\Sigma$ Ã© uma matriz nÃ£o diagonal com elementos nÃ£o constantes na diagonal principal (indicando a heterocedasticidade). A matriz de variÃ¢ncia-covariÃ¢ncia do estimador OLS sob heterocedasticidade serÃ¡ dada por: $$Var(b) = (X'X)^{-1} X' \Sigma X (X'X)^{-1}$$, que Ã© diferente da matriz $\sigma^2(X'X)^{-1}$ sob homocedasticidade.
>
> Vamos calcular a variÃ¢ncia dos resÃ­duos nas duas amostras:
>
> ```python
> import numpy as np
> residuals1 = np.array([0.1, -0.1, 0.2])
> residuals2 = np.array([0.5, -0.4, 0.6])
> variance1 = np.var(residuals1, ddof=1)
> variance2 = np.var(residuals2, ddof=1)
> print(f"VariÃ¢ncia da amostra 1: {variance1:.3f}")
> print(f"VariÃ¢ncia da amostra 2: {variance2:.3f}")
> ```
> Vemos que a variÃ¢ncia da segunda amostra Ã© bem maior que a da primeira, o que evidencia a heterocedasticidade.

Sob as premissas de independÃªncia e distribuiÃ§Ã£o idÃªntica dos resÃ­duos, a matriz de variÃ¢ncia-covariÃ¢ncia do vetor de erros populacionais $u$ Ã© dada por $E(uu') = \sigma^2 I_T$ [^1], onde $I_T$ Ã© uma matriz identidade de dimensÃ£o $T \times T$ e $\sigma^2$ Ã© a variÃ¢ncia comum dos erros. Essa matriz expressa que os resÃ­duos tÃªm a mesma variÃ¢ncia e nÃ£o sÃ£o correlacionados entre si.

**ObservaÃ§Ã£o 1:** Uma consequÃªncia importante da premissa de que os resÃ­duos sÃ£o nÃ£o correlacionados, Ã© que a matriz de covariÃ¢ncia populacional $E(uu')$ Ã© uma matriz diagonal. Isto se deve ao fato de que os elementos fora da diagonal principal representam a covariÃ¢ncia entre diferentes resÃ­duos, $E(u_i u_j)$ para $i \neq j$, que Ã© zero sob a premissa de independÃªncia.

A combinaÃ§Ã£o da premissa de normalidade dos resÃ­duos com a premissa de independÃªncia e distribuiÃ§Ã£o idÃªntica possibilita derivar a distribuiÃ§Ã£o exata do estimador OLS e das estatÃ­sticas de teste. Se alÃ©m de serem i.i.d., os resÃ­duos forem Gaussianos (AssunÃ§Ã£o 8.1(c)) [^1], entÃ£o o estimador OLS tem uma distribuiÃ§Ã£o normal, $b \sim N(\beta, \sigma^2(X'X)^{-1})$ [^1]. As estatÃ­sticas de teste como o teste t e o teste F tambÃ©m tÃªm distribuiÃ§Ãµes exatas sob essas condiÃ§Ãµes, como discutido no capÃ­tulo anterior [^2].

> ðŸ’¡ **Exemplo NumÃ©rico:** Para o teste t, a estatÃ­stica $$ t = \frac{b_i - \beta^0_i}{\sqrt{s^2\xi^{ii}}} $$ tem distribuiÃ§Ã£o t de Student com $T-k$ graus de liberdade [^1], onde $s^2$ Ã© o estimador nÃ£o viesado da variÃ¢ncia residual e $\xi^{ii}$ Ã© o elemento correspondente de $(X'X)^{-1}$. Similarmente, para o teste F, a estatÃ­stica
>
> $$ F = \frac{(Rb - r)'[s^2R(X'X)^{-1}R']^{-1}(Rb - r)}{m} $$
>
> tem distribuiÃ§Ã£o $F$ com $m$ e $T-k$ graus de liberdade, onde $R$ Ã© a matriz de restriÃ§Ãµes e $r$ o vetor de restriÃ§Ãµes testadas. Essas distribuiÃ§Ãµes exatas sÃ³ sÃ£o vÃ¡lidas sob a premissa de que os resÃ­duos sÃ£o i.i.d. e gaussianos.
>
> Continuando com nosso exemplo, vamos supor que estimamos o coeficiente da variÃ¡vel publicidade ($b_1$) como 1.3, com um erro padrÃ£o de 0.2, e que o tamanho da amostra Ã© 6. Se estamos testando a hipÃ³tese de que o coeficiente da publicidade Ã© zero ($\beta_1^0 = 0$), entÃ£o a estatÃ­stica t Ã© calculada como:
>
> $t = \frac{1.3 - 0}{0.2} = 6.5$.
>
> Para realizar o teste, compararÃ­amos esse valor com a distribuiÃ§Ã£o t de Student com $6-2=4$ graus de liberdade. Se o valor p associado a essa estatÃ­stica t for menor que o nÃ­vel de significÃ¢ncia escolhido (digamos 0.05), rejeitarÃ­amos a hipÃ³tese nula de que o coeficiente da publicidade Ã© zero. A validade deste teste depende da premissa de que os resÃ­duos sÃ£o i.i.d. e normalmente distribuÃ­dos.
>
> Para o teste F, suponha que testamos a hipÃ³tese de que todos os coeficientes (exceto o intercepto) sÃ£o iguais a zero, temos que o valor da estatÃ­stica F resultante foi 42.2, e que temos $m=1$ restriÃ§Ã£o(Ãµes) e $T-k=4$ graus de liberdade. CompararÃ­amos esse valor com a distribuiÃ§Ã£o F com 1 e 4 graus de liberdade. Se o valor p associado a essa estatÃ­stica F for menor que o nÃ­vel de significÃ¢ncia escolhido, rejeitarÃ­amos a hipÃ³tese nula de que todos os coeficientes sÃ£o iguais a zero. Novamente, a validade deste teste depende da premissa de resÃ­duos i.i.d. e normalmente distribuÃ­dos.
>
> Vamos calcular o p-valor para o teste t no exemplo acima. A estatÃ­stica t foi calculada como 6.5, e temos 4 graus de liberdade:
>
> ```python
> from scipy import stats
> t_statistic = 6.5
> degrees_freedom = 4
> p_value = 2 * (1 - stats.t.cdf(abs(t_statistic), degrees_freedom))
> print(f"p-valor do teste t: {p_value:.4f}")
> ```
> O p-valor Ã© muito pequeno, o que nos leva a rejeitar a hipÃ³tese nula de que o coeficiente da publicidade Ã© zero.

**Lema 3**
A premissa de que os resÃ­duos sÃ£o i.i.d. com mÃ©dia zero implica que $E(u_t u_s) = 0$ para todo $t \neq s$ e $E(u_t^2) = \sigma^2$ para todo $t$.
*DemonstraÃ§Ã£o:*
I.  Por definiÃ§Ã£o, a variÃ¢ncia de $u_t$ Ã© $E(u_t^2) - (E(u_t))^2 = E(u_t^2)$, pois $E(u_t) = 0$.
II. Sendo os resÃ­duos identicamente distribuÃ­dos, a variÃ¢ncia Ã© constante para todos os $t$, portanto, $E(u_t^2) = \sigma^2$ para todo $t$.
III. Sendo os resÃ­duos independentes entre si, a covariÃ¢ncia entre dois resÃ­duos diferentes $u_t$ e $u_s$ Ã© zero, ou seja, $E(u_t u_s) - E(u_t)E(u_s) = 0$, e como $E(u_t) = 0$, entÃ£o $E(u_t u_s) = 0$ para todo $t \neq s$.
IV.  Em resumo, $E(u_t u_s) = 0$ para todo $t \neq s$ e $E(u_t^2) = \sigma^2$ para todo $t$.
$\blacksquare$

**ProposiÃ§Ã£o 3**
A premissa de que os resÃ­duos sÃ£o i.i.d. com mÃ©dia zero e variÃ¢ncia $\sigma^2$ implica que a matriz de variÃ¢ncia-covariÃ¢ncia do vetor de erros populacionais $u$ Ã© dada por $E(uu') = \sigma^2 I_T$.
*DemonstraÃ§Ã£o:*
I.  A matriz de variÃ¢ncia-covariÃ¢ncia $E(uu')$ Ã© uma matriz quadrada de dimensÃ£o $T \times T$, cujos elementos $(i,j)$ sÃ£o dados por $E(u_i u_j)$.
II. Do Lema 3, sabemos que $E(u_i u_j) = 0$ quando $i \neq j$, e $E(u_i^2) = \sigma^2$ quando $i=j$.
III. Portanto, a matriz de variÃ¢ncia-covariÃ¢ncia Ã© uma matriz diagonal, onde todos os elementos na diagonal sÃ£o $\sigma^2$, e todos os elementos fora da diagonal sÃ£o 0. Essa Ã© a definiÃ§Ã£o da matriz $\sigma^2 I_T$.
$\blacksquare$

**Teorema 3.1**
Sob a premissa de que os resÃ­duos sÃ£o i.i.d. com mÃ©dia zero e variÃ¢ncia $\sigma^2$, a matriz de variÃ¢ncia-covariÃ¢ncia dos estimadores OLS Ã© dada por $\sigma^2(X'X)^{-1}$.
*DemonstraÃ§Ã£o:*
I.  O estimador OLS Ã© dado por $b = (X'X)^{-1}X'y$.
II.  Sabemos que $y= X\beta + u$, logo $b = \beta + (X'X)^{-1}X'u$.
III.  A variÃ¢ncia de $b$, condicional em $X$, Ã© dada por $Var(b|X) = E[(b-E(b))(b-E(b))'|X] = E[((X'X)^{-1}X'u)((X'X)^{-1}X'u)'|X]$.
IV. Utilizando a propriedade de que $(AB)' = B'A'$ e $E(uu')=\sigma^2 I_T$, obtemos $E[((X'X)^{-1}X'u)((X'X)^{-1}X'u)'|X] = (X'X)^{-1}X'E(uu')X(X'X)^{-1} = (X'X)^{-1}X'\sigma^2 I_T X(X'X)^{-1}$.
V. Portanto $Var(b|X) =  \sigma^2(X'X)^{-1}$.
$\blacksquare$

**Lema 3.1**
Sob as premissas de que os resÃ­duos sÃ£o i.i.d. com mÃ©dia zero e variÃ¢ncia $\sigma^2$ e que os regressores sÃ£o determinÃ­sticos, a covariÃ¢ncia entre o estimador OLS $b$ e os resÃ­duos $u$ Ã© igual a zero, ou seja, $Cov(b, u|X) = 0$.

*DemonstraÃ§Ã£o:*
I.  O estimador OLS Ã© dado por $b = (X'X)^{-1}X'y$. Substituindo $y = X\beta + u$, temos $b = \beta + (X'X)^{-1}X'u$.
II. A covariÃ¢ncia entre $b$ e $u$, condicional em $X$, Ã© definida como $Cov(b, u|X) = E[(b-E(b))(u-E(u))'|X]$.
III. Como $E(b|X) = \beta$, temos $Cov(b, u|X) = E[( (X'X)^{-1}X'u)u'|X] =  (X'X)^{-1}X'E(uu'|X)$.
IV. Sabendo que $E(uu') = \sigma^2 I_T$, obtemos $Cov(b, u|X) = (X'X)^{-1}X'\sigma^2 I_T = (X'X)^{-1}X'\sigma^2 I_T = 0$
V. Portanto,  $Cov(b, u|X) = 0$.
$\blacksquare$

**Teorema 3.2**
Sob as premissas de que os resÃ­duos sÃ£o i.i.d. com mÃ©dia zero e variÃ¢ncia $\sigma^2$ e que os regressores sÃ£o determinÃ­sticos, o estimador OLS $b$ Ã© nÃ£o correlacionado com os resÃ­duos $u$.

*DemonstraÃ§Ã£o:*
I. Do Lema 3.1, sabemos que $Cov(b,u|X) = 0$.
II. Se a covariÃ¢ncia Ã© zero, isso implica que a correlaÃ§Ã£o entre $b$ e $u$ Ã© tambÃ©m zero.
III. Portanto, o estimador OLS $b$ Ã© nÃ£o correlacionado com os resÃ­duos $u$.
$\blacksquare$

**ProposiÃ§Ã£o 3.1**
Se a premissa de homocedasticidade Ã© violada, ou seja, existe heterocedasticidade, a matriz de variÃ¢ncia-covariÃ¢ncia do estimador OLS, $Var(b)$, nÃ£o serÃ¡ igual a $\sigma^2(X'X)^{-1}$.

*DemonstraÃ§Ã£o:*
I. Sob heterocedasticidade, $E(uu') = \Sigma$, onde $\Sigma$ Ã© uma matriz diagonal, mas com elementos diferentes de $\sigma^2$ na diagonal principal (i.e., $E(u_i^2) = \sigma_i^2 \neq \sigma^2$ para algum $i$).
II. A variÃ¢ncia do estimador OLS, condicionada em X, Ã© dada por:
$Var(b|X) = E[(b - E(b))(b - E(b))'|X] = E[((X'X)^{-1}X'u)((X'X)^{-1}X'u)'|X] = (X'X)^{-1}X'E(uu')X(X'X)^{-1}$
III. Substituindo $E(uu')$ por $\Sigma$, temos $Var(b|X) = (X'X)^{-1}X' \Sigma X(X'X)^{-1}$
IV. Como $\Sigma \neq \sigma^2 I_T$, entÃ£o $Var(b|X) \neq \sigma^2(X'X)^{-1}$.
$\blacksquare$

**CorolÃ¡rio 3.1**
A violaÃ§Ã£o da premissa de homocedasticidade leva a estimativas incorretas dos erros padrÃ£o dos coeficientes, o que invalida os testes de hipÃ³teses e os intervalos de confianÃ§a, caso o erro padrÃ£o calculado assuma homocedasticidade.

*DemonstraÃ§Ã£o:*
I.  Pela ProposiÃ§Ã£o 3.1, a matriz de variÃ¢ncia-covariÃ¢ncia do estimador OLS nÃ£o Ã© dada por $\sigma^2(X'X)^{-1}$ sob heterocedasticidade.
II. Os erros padrÃ£o dos coeficientes, que sÃ£o utilizados para testes de hipÃ³teses e construÃ§Ã£o de intervalos de confianÃ§a, sÃ£o derivados da raiz quadrada dos elementos na diagonal principal de $Var(b)$.
III. Se a variÃ¢ncia-covariÃ¢ncia do estimador OLS Ã© calculada incorretamente, os erros padrÃ£o tambÃ©m serÃ£o incorretos.
IV.  Consequentemente, os testes de hipÃ³teses e intervalos de confianÃ§a que usam os erros padrÃ£o calculados sob a premissa de homocedasticidade serÃ£o invÃ¡lidos.
$\blacksquare$

### A RelaÃ§Ã£o com a NÃ£o-Tendenciosidade do Estimador OLS
Ã‰ importante destacar que a premissa de resÃ­duos com mÃ©dia zero ($E(u_t) = 0$) Ã© a principal responsÃ¡vel pela nÃ£o-tendenciosidade do estimador OLS. A equaÃ§Ã£o [8.1.15] [^1] mostra que $E(b) = \beta$ sob essa condiÃ§Ã£o. O fato dos resÃ­duos serem i.i.d. nÃ£o impacta na nÃ£o-tendenciosidade do estimador, mas sim na variÃ¢ncia do estimador.

### ImplicaÃ§Ãµes da ViolaÃ§Ã£o da Premissa de ResÃ­duos i.i.d.
A violaÃ§Ã£o da premissa de resÃ­duos i.i.d. pode levar a resultados estatÃ­sticos incorretos. A heterocedasticidade, que Ã© a variÃ¢ncia nÃ£o constante dos resÃ­duos, leva a estimativas ineficientes da variÃ¢ncia dos coeficientes. JÃ¡ a autocorrelaÃ§Ã£o, ou dependÃªncia serial dos resÃ­duos, leva a uma matriz de variÃ¢ncia-covariÃ¢ncia dos erros que nÃ£o Ã© mais diagonal, o que impacta a variÃ¢ncia dos estimadores e a validade dos testes de hipÃ³teses. Nesses casos, o estimador OLS nÃ£o serÃ¡ mais BLUE (Best Linear Unbiased Estimator).

Ã‰ importante reconhecer que na prÃ¡tica esta premissa Ã© frequentemente violada, principalmente em dados de sÃ©ries temporais e painel. Em tais casos, Ã© necessÃ¡rio utilizar modelos e mÃ©todos de estimaÃ§Ã£o que relaxam essa premissa, como o modelo de mÃ­nimos quadrados generalizados (GLS), ou estimadores robustos Ã  heteroscedasticidade e/ou autocorrelaÃ§Ã£o [^1], que serÃ£o discutidos nos prÃ³ximos capÃ­tulos.

> ðŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar o impacto da heterocedasticidade na matriz de variÃ¢ncia-covariÃ¢ncia, suponha que temos um modelo de regressÃ£o com dois regressores, $y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + u_i$. Sob homocedasticidade, $Var(b) = \sigma^2 (X'X)^{-1}$. Vamos supor que $(X'X)^{-1}$ Ã© a seguinte matriz:
>
> $(X'X)^{-1} = \begin{bmatrix} 0.2 & -0.05 & 0.01 \\ -0.05 & 0.1 & 0.02 \\ 0.01 & 0.02 & 0.15 \end{bmatrix}$
>
> e que a variÃ¢ncia dos resÃ­duos Ã© $\sigma^2 = 4$. Nesse caso a matriz de covariÃ¢ncia dos estimadores OLS sob homocedasticidade seria:
>
> $Var(b) = 4 \times \begin{bmatrix} 0.2 & -0.05 & 0.01 \\ -0.05 & 0.1 & 0.02 \\ 0.01 & 0.02 & 0.15 \end{bmatrix} = \begin{bmatrix} 0.8 & -0.2 & 0.04 \\ -0.2 & 0.4 & 0.08 \\ 0.04 & 0.08 & 0.6 \end{bmatrix}$
>
> Agora, suponha que hÃ¡ heterocedasticidade. Nesse caso a matriz de variÃ¢ncia-covariÃ¢ncia dos resÃ­duos Ã© $\Sigma$, que nÃ£o Ã© proporcional Ã  identidade. A matriz de variÃ¢ncia-covariÃ¢ncia dos estimadores OLS serÃ¡ $Var(b|X) = (X'X)^{-1}X' \Sigma X(X'X)^{-1}$. Sem saber a forma de $\Sigma$, nÃ£o podemos calcular exatamente a nova matriz de variÃ¢ncia-covariÃ¢ncia. Mas Ã© garantido que serÃ¡ diferente de $\sigma^2(X'X)^{-1}$. Isso significa que os erros padrÃ£o dos coeficientes serÃ£o diferentes (e geralmente maiores) sob heterocedasticidade.
>
> Vamos supor que ao calcular a matriz $ (X'X)^{-1}X' \Sigma X(X'X)^{-1}$ chegamos a:
>
> $Var(b|X) =  \begin{bmatrix} 1.2 & -0.3 & 0.08 \\ -0.3 & 0.6 & 0.12 \\ 0.08 & 0.12 & 0.9 \end{bmatrix}$
>
> As variÃ¢ncias dos coeficientes, que sÃ£o os termos da diagonal principal, sÃ£o agora maiores (1.2, 0.6 e 0.9) do que as calculadas sob homocedasticidade (0.8, 0.4, e 0.6). Isso significa que os erros padrÃ£o (raiz quadrada da variÃ¢ncia) serÃ£o maiores, o que torna os testes de hipÃ³tese menos precisos e intervalos de confianÃ§a mais largos.

### ConclusÃ£o
Em suma, a premissa de que os resÃ­duos sÃ£o independentes e identicamente distribuÃ­dos com mÃ©dia zero e variÃ¢ncia constante Ã© uma das bases para as inferÃªncias estatÃ­sticas em modelos de regressÃ£o linear clÃ¡ssicos [^1]. Essa premissa, combinada com a normalidade dos resÃ­duos e o determinismo dos regressores, permite derivar a distribuiÃ§Ã£o exata do estimador OLS e das estatÃ­sticas de teste. No entanto, a aplicaÃ§Ã£o do modelo de regressÃ£o deve sempre considerar a validade desta premissa, dado que a sua violaÃ§Ã£o pode levar a conclusÃµes errÃ´neas. A compreensÃ£o da importÃ¢ncia e das implicaÃ§Ãµes desta premissa Ã© um passo fundamental para a aplicaÃ§Ã£o correta e eficaz de modelos de regressÃ£o.

### ReferÃªncias
[^1]: Trecho do texto original fornecido.
[^2]: CapÃ­tulo anterior sobre a premissa da normalidade dos resÃ­duos.
[^3]: CapÃ­tulo anterior sobre a premissa de regressores determinÃ­sticos.
<!-- END -->
