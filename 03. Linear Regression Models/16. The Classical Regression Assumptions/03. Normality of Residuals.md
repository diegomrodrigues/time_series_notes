## A Premissa da Normalidade dos Res√≠duos na Regress√£o Linear Cl√°ssica

### Introdu√ß√£o
Dando continuidade √† an√°lise das premissas cl√°ssicas do modelo de regress√£o linear, este cap√≠tulo se concentrar√° na suposi√ß√£o de que os res√≠duos populacionais seguem uma distribui√ß√£o normal, tamb√©m conhecida como distribui√ß√£o Gaussiana. Como exploramos anteriormente, a regress√£o linear cl√°ssica assume que os regressores s√£o determin√≠sticos, o que simplifica a an√°lise e a estima√ß√£o dos par√¢metros [^1]. Agora, vamos investigar a import√¢ncia da normalidade dos res√≠duos, suas implica√ß√µes para a infer√™ncia estat√≠stica e como essa premissa complementa a natureza determin√≠stica dos regressores.

### Conceitos Fundamentais
A premissa de que os **res√≠duos populacionais ($u_t$) seguem uma distribui√ß√£o normal (Gaussiana)** √© crucial para a validade da infer√™ncia estat√≠stica no modelo de regress√£o linear [^1]. Essa premissa, formalizada como parte da Assun√ß√£o 8.1(c) [^1], estabelece que os erros aleat√≥rios no modelo s√£o distribu√≠dos de acordo com uma distribui√ß√£o normal, com m√©dia zero e vari√¢ncia constante ($\sigma^2$). Matematicamente, isso √© expresso como: $u_t \sim N(0, \sigma^2)$.

A distribui√ß√£o normal, dada sua simetria e propriedades bem estabelecidas, simplifica a an√°lise e a interpreta√ß√£o dos resultados da regress√£o. A normalidade dos res√≠duos permite aplicar testes estat√≠sticos param√©tricos, como os testes *t* e *F*, que s√£o mais poderosos e precisos quando a distribui√ß√£o dos erros √© conhecida [^1]. Al√©m disso, essa premissa √© fundamental para derivar as distribui√ß√µes exatas dos estimadores e estat√≠sticas de teste, o que √© essencial para construir intervalos de confian√ßa e realizar testes de hip√≥tese.

> üí° **Exemplo Num√©rico:** Suponha que estamos analisando o efeito da temperatura ($x$) no rendimento de uma colheita ($y$). Depois de ajustar um modelo de regress√£o linear, os res√≠duos ($u_t$) representam as diferen√ßas entre os rendimentos observados e os rendimentos previstos pelo modelo. Se os res√≠duos seguem uma distribui√ß√£o normal, isso indica que os erros de previs√£o do nosso modelo s√£o aleat√≥rios e distribu√≠dos simetricamente em torno de zero, o que √© um bom sinal.
>
> Por exemplo, consideremos 10 observa√ß√µes de temperatura e rendimento, com um modelo de regress√£o linear simples: $y_t = \beta_0 + \beta_1 x_t + u_t$.
>
> Usando dados hipot√©ticos:
>
> | Temperatura ($x_t$) | Rendimento ($y_t$) |
> |---------------------|-------------------|
> | 20                 | 50                |
> | 22                 | 55                |
> | 25                 | 62                |
> | 28                 | 70                |
> | 30                 | 75                |
> | 24                 | 60                |
> | 26                 | 65                |
> | 29                 | 72                |
> | 21                 | 53                |
> | 23                 | 58                |
>
> Ap√≥s ajustar o modelo de regress√£o linear, obtemos os seguintes res√≠duos (diferen√ßas entre os valores observados e os valores previstos):
>
> | Res√≠duo ($u_t$) |
> |-----------------|
> | -1.2 |
> | 0.8  |
> | -0.5 |
> | 1.5  |
> | -2.1  |
> | 0.2  |
> | 0.1  |
> | -0.3  |
> | 1.7  |
> | -0.2 |
>
> Se plotarmos um histograma ou um gr√°fico Q-Q dos res√≠duos, e eles se aproximarem de uma distribui√ß√£o normal centrada em zero, a premissa de normalidade √© razoavelmente satisfeita. Podemos usar testes como o Shapiro-Wilk para verificar a normalidade.

A import√¢ncia da premissa de normalidade se manifesta na forma como as propriedades estat√≠sticas do estimador de coeficientes OLS ($b$) s√£o derivadas. Conforme mencionado anteriormente, sob as premissas de 8.1(a) e 8.1(b) [^1], $b$ √© um estimador n√£o viesado com matriz de vari√¢ncia-covari√¢ncia $\sigma^2(X'X)^{-1}$ [^1]. No entanto, a Assun√ß√£o 8.1(c), que assume a normalidade dos res√≠duos, adiciona informa√ß√£o crucial sobre a distribui√ß√£o amostral do estimador. Conforme a equa√ß√£o [8.1.17] [^1],  quando $u$ √© gaussiano, o estimador $b$ tamb√©m √© gaussiano e segue a distribui√ß√£o $b \sim N(\beta, \sigma^2(X'X)^{-1})$.

Essa normalidade de $b$ √© crucial porque permite realizar testes de hip√≥teses sobre os par√¢metros do modelo. Por exemplo, o teste t, detalhado na equa√ß√£o [8.1.26] [^1], √© utilizado para verificar se um coeficiente individual ($\beta_i$) √© estatisticamente diferente de zero ou de um valor espec√≠fico ($ \beta_i^0$). Este teste √© constru√≠do com base na premissa de que a distribui√ß√£o do estimador $b_i$ √© normal.

> üí° **Exemplo Num√©rico:** Voltando ao exemplo do fertilizante e crescimento de plantas, se quisermos verificar se o fertilizante tem um efeito significativo no crescimento, realizamos um teste de hip√≥teses para o coeficiente correspondente. Se os res√≠duos forem normais, podemos utilizar o teste t com confian√ßa para obter um valor p. Por exemplo, seja a hip√≥tese nula $H_0:\beta_1 = 0$.  O teste t √©:
>
> $$ t = \frac{b_1 - 0}{\sqrt{s^2 \xi^{ii}}} $$
> onde $b_1$ √© o estimador do coeficiente do fertilizante, $s^2$ √© o estimador da vari√¢ncia do res√≠duo e $\xi^{ii}$ √© o elemento diagonal correspondente de $(X'X)^{-1}$. Sob a premissa de normalidade, $t$ segue uma distribui√ß√£o t de Student com $T-k$ graus de liberdade [^1], permitindo-nos calcular o valor p para verificar se h√° evid√™ncias contra a hip√≥tese nula.
>
> Suponha que, ap√≥s rodar a regress√£o, obtivemos $b_1 = 0.5$, $s^2 = 0.1$, e $\xi^{ii} = 0.04$ (elemento correspondente ao nosso coeficiente de interesse). O n√∫mero de amostras √© $T=20$ e o n√∫mero de regressores √© $k=2$. Ent√£o, a estat√≠stica t √©:
>
> $$ t = \frac{0.5 - 0}{\sqrt{0.1 \times 0.04}} = \frac{0.5}{\sqrt{0.004}} = \frac{0.5}{0.0632} \approx 7.91 $$
> O valor cr√≠tico para um teste bicaudal com $\alpha = 0.05$ e $T-k = 18$ graus de liberdade √© aproximadamente $2.10$. Como o valor absoluto da estat√≠stica $t$ (7.91) √© maior que o valor cr√≠tico, rejeitamos a hip√≥tese nula de que o coeficiente √© zero. Isso sugere que o fertilizante tem um efeito estatisticamente significativo no crescimento das plantas.

A normalidade dos res√≠duos tamb√©m impacta na distribui√ß√£o da estat√≠stica F [^1], utilizada para realizar testes de hip√≥teses conjuntas. A estat√≠stica F √© constru√≠da sob a premissa de que o res√≠duo tem distribui√ß√£o Gaussiana e, quando essa premissa √© satisfeita, a estat√≠stica F segue uma distribui√ß√£o F com $m$ e $T-k$ graus de liberdade sob a hip√≥tese nula. A distribui√ß√£o exata da estat√≠stica $F$ √© derivada sob a premissa de normalidade. Conforme a equa√ß√£o [8.1.32], a estat√≠stica $F$ √© dada por:

$$ F = \frac{(Rb - r)'[s^2R(X'X)^{-1}R']^{-1}(Rb - r)}{m} $$
onde $R$ √© uma matriz de restri√ß√µes, $r$ √© o vetor dos valores de restri√ß√£o, $m$ √© o n√∫mero de restri√ß√µes testadas, e $s^2$ √© o estimador da vari√¢ncia residual.
> üí° **Exemplo Num√©rico:** Continuando com o exemplo anterior, se temos mais de um regressor, poder√≠amos usar o teste F para testar a hip√≥tese de que um grupo de coeficientes (por exemplo, Œ≤‚ÇÅ e Œ≤‚ÇÇ) s√£o conjuntamente iguais a zero.
>
> Suponha que temos o modelo $y_t = \beta_0 + \beta_1 x_{1t} + \beta_2 x_{2t} + u_t$, onde $x_{1t}$ √© a quantidade de fertilizante e $x_{2t}$ √© a quantidade de √°gua utilizada. Queremos testar a hip√≥tese conjunta de que ambos os coeficientes, $\beta_1$ e $\beta_2$, s√£o iguais a zero.
>
>  Seja  $R = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$, e $r = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$, o n√∫mero de restri√ß√µes $m=2$.
>
> Suponha que, ap√≥s rodar a regress√£o, obtemos:
> $b = \begin{bmatrix} 10 \\ 0.3 \\ 0.4 \end{bmatrix}$, $s^2=0.05$, e $(X'X)^{-1} = \begin{bmatrix} 0.2 & -0.05 & 0.01 \\ -0.05 & 0.03 & -0.005 \\ 0.01 & -0.005 & 0.02 \end{bmatrix}$.
>
> Ent√£o:
>
> $Rb = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 10 \\ 0.3 \\ 0.4 \end{bmatrix} = \begin{bmatrix} 0.3 \\ 0.4 \end{bmatrix}$
>
>  $R(X'X)^{-1}R' = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 0.2 & -0.05 & 0.01 \\ -0.05 & 0.03 & -0.005 \\ 0.01 & -0.005 & 0.02 \end{bmatrix} \begin{bmatrix} 0 & 0 \\ 1 & 0 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 0.03 & -0.005 \\ -0.005 & 0.02 \end{bmatrix}$
>
> $[R(X'X)^{-1}R']^{-1} = \begin{bmatrix} 34.88 & 8.72 \\ 8.72 & 52.33 \end{bmatrix} $
>
> $(Rb - r)' [s^2R(X'X)^{-1}R']^{-1} (Rb - r) = [0.3 \ 0.4] \ 0.05^{-1}\begin{bmatrix} 34.88 & 8.72 \\ 8.72 & 52.33 \end{bmatrix}  \begin{bmatrix} 0.3 \\ 0.4 \end{bmatrix} = 27.31$
>
> $F = \frac{27.31}{2} = 13.655$
>
> O valor cr√≠tico de $F$ com 2 e $T-k=17$ graus de liberdade (assumindo $T=20$) a um n√≠vel de signific√¢ncia de 0.05 √© aproximadamente 3.59. Dado que nosso valor F calculado (13.655) √© maior que o valor cr√≠tico, rejeitamos a hip√≥tese nula de que ambos os coeficientes s√£o conjuntamente iguais a zero, o que indica que pelo menos um deles tem um efeito estatisticamente significativo.

Sob a premissa de normalidade, o numerador de $F$ segue uma distribui√ß√£o $\chi^2(m)$, onde $m$ √© o n√∫mero de restri√ß√µes lineares e o denominador segue uma distribui√ß√£o $\chi^2(T-k)$ dividido por seus graus de liberdade,  $T-k$, sendo ambos independentes. Portanto, $F$ segue a distribui√ß√£o de probabilidade $F$ sob a hip√≥tese nula, permitindo-nos realizar testes estat√≠sticos de hip√≥teses conjuntas.

**Lema 2**
Sob a premissa de que os res√≠duos populacionais s√£o normalmente distribu√≠dos, a distribui√ß√£o do estimador OLS $b$ tamb√©m √© normal.
*Demonstra√ß√£o:*
I. A equa√ß√£o [8.1.12] nos mostra que $b = \beta + (X'X)^{-1}X'u$.
II. Se os res√≠duos $u$ s√£o normalmente distribu√≠dos, ent√£o qualquer combina√ß√£o linear de $u$ tamb√©m √© normalmente distribu√≠da, ou seja, $X'u$ tamb√©m segue uma distribui√ß√£o normal.
III. Consequentemente, como $(X'X)^{-1}X'$ √© uma matriz de valores fixos, ent√£o $(X'X)^{-1}X'u$ segue uma distribui√ß√£o normal, j√° que √© uma combina√ß√£o linear de vari√°veis normais.
IV. A soma de um vetor constante ($\beta$) com um vetor normal ($ (X'X)^{-1}X'u $) tamb√©m tem uma distribui√ß√£o normal, e logo $b$ √© normalmente distribu√≠do.
‚ñ†

**Proposi√ß√£o 2**
A premissa de normalidade dos res√≠duos implica que o teste t, dado em [8.1.26], segue uma distribui√ß√£o t de Student com $T-k$ graus de liberdade sob a hip√≥tese nula.
*Demonstra√ß√£o:*
I.  A estat√≠stica do teste t √© dada por: $$ t = \frac{b_i - \beta^0_i}{\sqrt{s^2\xi^{ii}}} $$
II. Do Lema 2, sabemos que $b_i$ √© normalmente distribu√≠do sob a premissa de normalidade dos res√≠duos, com m√©dia $\beta_i$ e vari√¢ncia $\sigma^2\xi^{ii}$, onde $\xi^{ii}$ √© o elemento diagonal correspondente de $(X'X)^{-1}$. Assim,  $\frac{b_i - \beta_i}{\sqrt{\sigma^2\xi^{ii}}}$ segue uma distribui√ß√£o normal padr√£o $N(0,1)$.
III. O estimador da vari√¢ncia, $s^2$, √© independente de $b$ sob a normalidade dos res√≠duos, como expresso na equa√ß√£o [8.1.25] [^1].
IV. Conforme a equa√ß√£o [8.1.24] [^1], $\frac{(T-k)s^2}{\sigma^2}$ segue uma distribui√ß√£o $\chi^2(T-k)$.
V. O teste t √© a raz√£o entre a distribui√ß√£o normal padr√£o e a raiz quadrada da distribui√ß√£o $\chi^2$ dividida por seus graus de liberdade, o que define a distribui√ß√£o t de Student com $T-k$ graus de liberdade.
‚ñ†

**Teorema 2.1**
Sob as premissas do modelo cl√°ssico de regress√£o linear, incluindo a normalidade dos res√≠duos, o estimador da vari√¢ncia residual $s^2 = \frac{RSS}{T-k}$ √© um estimador n√£o viesado da vari√¢ncia populacional $\sigma^2$.

*Demonstra√ß√£o:*
I. Sabemos que $RSS = u'M_Xu$, onde $M_X = I - X(X'X)^{-1}X'$ √© a matriz idempotente que projeta os res√≠duos no espa√ßo ortogonal ao espa√ßo das colunas de $X$.
II.  Conforme a equa√ß√£o [8.1.24] [^1], $\frac{(T-k)s^2}{\sigma^2} = \frac{RSS}{\sigma^2}$ segue uma distribui√ß√£o $\chi^2(T-k)$. O valor esperado de uma vari√°vel com distribui√ß√£o $\chi^2(df)$ √© $df$.
III. Assim, $E[\frac{RSS}{\sigma^2}] = T-k$.
IV.  Multiplicando ambos os lados por $\frac{\sigma^2}{T-k}$, obtemos:  $E[\frac{RSS}{T-k}] = \sigma^2$.
V. Portanto, $E[s^2] = \sigma^2$, o que demonstra que $s^2$ √© um estimador n√£o viesado de $\sigma^2$.
‚ñ†

A premissa de normalidade dos res√≠duos tamb√©m tem implica√ß√µes na distribui√ß√£o da vari√¢ncia estimada ($s^2$). A equa√ß√£o [8.1.24] estabelece que  $RSS/\sigma^2 = u'M_Xu/\sigma^2$ segue uma distribui√ß√£o qui-quadrado ($\chi^2$) com $T-k$ graus de liberdade [^1], onde $M_X$ √© a matriz idempotente $I - X(X'X)^{-1}X'$. Esta propriedade √© fundamental para a constru√ß√£o de testes estat√≠sticos sobre a vari√¢ncia do erro.

> üí° **Exemplo Num√©rico:** Se quisermos realizar um teste de hip√≥tese sobre a vari√¢ncia do erro, por exemplo, se a vari√¢ncia √© diferente de um valor determinado, a distribui√ß√£o qui-quadrado √© utilizada para realizar a infer√™ncia estat√≠stica.
>
> Por exemplo, suponha que em nosso modelo de rendimento da colheita temos $T = 50$ observa√ß√µes e $k=2$ regressores e calculamos $RSS = 10$. O nosso estimador de vari√¢ncia √© $s^2 = \frac{RSS}{T-k} = \frac{10}{50-2} = \frac{10}{48} = 0.2083$.
>
> Se quisermos testar se a vari√¢ncia √© igual a um valor espec√≠fico (digamos, $\sigma^2_0 = 0.2$), podemos usar a estat√≠stica:
> $$ \frac{(T-k)s^2}{\sigma^2_0} = \frac{48 \times 0.2083}{0.2} = 50 $$
>
> Sob a premissa de normalidade dos res√≠duos, essa estat√≠stica segue uma distribui√ß√£o $\chi^2$ com $T-k=48$ graus de liberdade.  Podemos consultar a tabela da distribui√ß√£o $\chi^2$ para determinar o valor p e concluir se existe evid√™ncia para rejeitar a hip√≥tese nula de que a vari√¢ncia √© igual a 0.2. Se o valor p √© abaixo de um certo n√≠vel de signific√¢ncia (ex: 0.05), rejeitamos a hip√≥tese nula.

**Observa√ß√£o 1:** A premissa de normalidade dos res√≠duos √© crucial para que a distribui√ß√£o exata do estimador OLS ($b$) e das estat√≠sticas de teste t e F possam ser derivadas. Se essa premissa n√£o for satisfeita, as distribui√ß√µes dos estimadores e estat√≠sticas podem n√£o seguir as distribui√ß√µes t e F, invalidando a infer√™ncia estat√≠stica. Em muitos casos, essa premissa pode ser relaxada utilizando o teorema do limite central quando o tamanho da amostra √© grande. No entanto, em amostras pequenas, a viola√ß√£o dessa premissa pode levar a resultados pouco confi√°veis.

### Conclus√£o
Em suma, a premissa de normalidade dos res√≠duos populacionais √© uma pedra angular da regress√£o linear cl√°ssica [^1]. Essa premissa, em conjunto com a premissa de regressores determin√≠sticos, possibilita a aplica√ß√£o de ferramentas de infer√™ncia estat√≠stica, como os testes t e F, e a constru√ß√£o de intervalos de confian√ßa para os par√¢metros do modelo. Al√©m disso, a distribui√ß√£o normal tamb√©m afeta a distribui√ß√£o dos estimadores de vari√¢ncia. Embora essa premissa simplifique a an√°lise, √© importante reconhecer que nem sempre ela se mant√©m na pr√°tica. Nas pr√≥ximas se√ß√µes, exploraremos situa√ß√µes onde essa premissa √© relaxada e analisaremos as implica√ß√µes para a infer√™ncia estat√≠stica, e tamb√©m modelos que n√£o requerem essa premissa t√£o restritiva. A compreens√£o da import√¢ncia da premissa de normalidade dos res√≠duos permite usar modelos de regress√£o de maneira mais informada e eficaz, ao mesmo tempo em que entendemos suas limita√ß√µes e as alternativas dispon√≠veis.

### Refer√™ncias
[^1]: Trecho do texto original fornecido.
<!-- END -->
