## A Premissa de Res√≠duos Independentes e Identicamente Distribu√≠dos na Regress√£o Linear Cl√°ssica

### Introdu√ß√£o
Este cap√≠tulo aprofunda o estudo das premissas do modelo de regress√£o linear cl√°ssico, focando na suposi√ß√£o de que os res√≠duos populacionais s√£o independentes e identicamente distribu√≠dos (i.i.d.), com m√©dia zero e vari√¢ncia constante. Como discutimos nos cap√≠tulos anteriores, a regress√£o linear cl√°ssica assume que os regressores s√£o determin√≠sticos e que os res√≠duos s√£o normalmente distribu√≠dos, premissas essas que permitem derivar resultados estat√≠sticos [^1, ^2]. Exploraremos agora a import√¢ncia da independ√™ncia e distribui√ß√£o id√™ntica dos res√≠duos, suas implica√ß√µes e como essa premissa se encaixa no arcabou√ßo da an√°lise de regress√£o.

### Conceitos Fundamentais
A premissa de que os **res√≠duos populacionais ($u_t$) s√£o independentes e identicamente distribu√≠dos (i.i.d.), com m√©dia zero e vari√¢ncia constante ($\sigma^2$)**, √© um dos pilares do modelo de regress√£o linear cl√°ssico [^1]. Formalmente expressa na Assun√ß√£o 8.1(b) [^1], essa premissa estabelece que:
1.  **Independ√™ncia:** Os res√≠duos ($u_t$) s√£o independentes entre si, ou seja, o valor de um res√≠duo em um ponto de tempo $t$ n√£o tem correla√ß√£o com o valor de outro res√≠duo em um ponto de tempo $s$, onde $t \neq s$.
2.  **Distribui√ß√£o Id√™ntica:** Os res√≠duos ($u_t$) seguem a mesma distribui√ß√£o de probabilidade para todos os pontos de tempo, com m√©dia zero e vari√¢ncia $\sigma^2$.
3.  **M√©dia Zero:** O valor esperado de cada res√≠duo √© zero, $E(u_t) = 0$.
4.  **Vari√¢ncia Constante (Homocedasticidade):** A vari√¢ncia de cada res√≠duo √© a mesma, ou seja, $Var(u_t) = \sigma^2$, para todo $t$.

Em nota√ß√£o matem√°tica, essa premissa √© expressa como $u_t \sim i.i.d.(0, \sigma^2)$ [^1].

Essa premissa √© crucial para a validade das infer√™ncias estat√≠sticas no modelo de regress√£o linear. Quando os res√≠duos s√£o independentes e identicamente distribu√≠dos, a variabilidade dos dados n√£o √© sistematicamente influenciada por fatores n√£o inclu√≠dos no modelo, e os testes de hip√≥teses e intervalos de confian√ßa podem ser interpretados com mais confian√ßa [^1].

> üí° **Exemplo Num√©rico:** Imagine que estamos analisando a rela√ß√£o entre o gasto com publicidade ($x$) e as vendas de um produto ($y$) ao longo de v√°rios meses. Os res√≠duos ($u_t$) representam a diferen√ßa entre as vendas observadas e as vendas previstas pelo modelo.
>
> *   **Independ√™ncia:** Se os res√≠duos s√£o i.i.d., ent√£o, o erro de previs√£o para o m√™s de janeiro n√£o deve influenciar o erro de previs√£o para o m√™s de fevereiro. N√£o deve haver padr√µes de erro se repetindo ao longo do tempo, ou seja, um res√≠duo positivo n√£o implica que o pr√≥ximo ser√° negativo (ou positivo).
> *   **Distribui√ß√£o Id√™ntica:**  A distribui√ß√£o dos erros deve ser consistente ao longo dos meses. A variabilidade dos erros deve ser aproximadamente a mesma ao longo do tempo e n√£o deve aumentar ou diminuir sistematicamente. N√£o devem haver meses em que o modelo gera erros maiores sistematicamente.
> *   **M√©dia Zero:** Em m√©dia, o modelo deve gerar previs√µes corretas, com erros se distribuindo de maneira balanceada (em torno de zero). N√£o deve haver um vi√©s sistem√°tico, onde o modelo superestima ou subestima os valores de $y$ sistematicamente.
> *   **Vari√¢ncia Constante (Homocedasticidade):** Os erros do modelo devem ser igualmente dispersos ao longo de todos os valores de $x$ e ao longo dos per√≠odos de tempo. A variabilidade dos erros n√£o deve ser menor (ou maior) em diferentes n√≠veis de gastos com publicidade.
>
> Suponha que temos os seguintes dados de gastos com publicidade (em milhares de reais) e vendas (em milhares de unidades) em 6 meses:
>
> | M√™s  | Publicidade (x) | Vendas (y) |
> |------|-----------------|------------|
> | Jan  | 2               | 5          |
> | Fev  | 3               | 7          |
> | Mar  | 4               | 8          |
> | Abr  | 5               | 9          |
> | Mai  | 6               | 11         |
> | Jun  | 7               | 12         |
>
> Um modelo de regress√£o linear simples ajustado a esses dados resulta na equa√ß√£o $\hat{y} = 2.5 + 1.3x$. Os res√≠duos para cada m√™s s√£o ent√£o:
>
> | M√™s  | Publicidade (x) | Vendas (y) | Vendas Previstas ($\hat{y}$) | Res√≠duo ($u_t$) |
> |------|-----------------|------------|----------------------------|-----------------|
> | Jan  | 2               | 5          | 5.1                        | -0.1            |
> | Fev  | 3               | 7          | 6.4                        | 0.6             |
> | Mar  | 4               | 8          | 7.7                        | 0.3             |
> | Abr  | 5               | 9          | 9.0                        | 0.0             |
> | Mai  | 6               | 11         | 10.3                       | 0.7             |
> | Jun  | 7               | 12         | 11.6                       | 0.4             |
>
> Para que a premissa de res√≠duos i.i.d. seja v√°lida, estes res√≠duos n√£o devem apresentar padr√µes.  Por exemplo, n√£o dever√≠amos ver todos os res√≠duos positivos no come√ßo e todos negativos no final, ou res√≠duos maiores nos meses de publicidade mais alta. Al√©m disso, a variabilidade dos res√≠duos deve ser aproximadamente a mesma ao longo dos meses. Um gr√°fico dos res√≠duos vs. os valores de $x$ (ou vs. o tempo) ajudaria a identificar padr√µes e poss√≠veis problemas com esta premissa.

A independ√™ncia dos res√≠duos √© especialmente importante em an√°lises de s√©ries temporais, onde os dados s√£o coletados sequencialmente ao longo do tempo. Em tais casos, a viola√ß√£o da independ√™ncia pode levar a estimativas viesadas e testes de hip√≥teses incorretos. Essa propriedade √© frequentemente testada usando estat√≠sticas como o teste de Durbin-Watson.

> üí° **Exemplo Num√©rico:**  Voltando ao exemplo de gastos com publicidade e vendas, se os res√≠duos s√£o dependentes, podemos encontrar um padr√£o como o seguinte: meses com res√≠duos positivos tendem a ser seguidos por meses com res√≠duos positivos, e vice-versa. Isso indicaria que h√° alguma estrutura temporal nos res√≠duos que o modelo n√£o est√° capturando, e que a premissa de independ√™ncia √© violada. A presen√ßa de correla√ß√£o serial nos res√≠duos pode ser um indicativo de que vari√°veis relevantes est√£o faltando no modelo ou que a especifica√ß√£o do modelo est√° inadequada, seja por um erro na escolha da forma funcional do modelo (linear, logar√≠tmica, etc) ou por n√£o considerar vari√°veis importantes.
>
> Por exemplo, se em nosso exemplo de gastos com publicidade, os res√≠duos dos primeiros tr√™s meses fossem -0.5, -0.3, -0.1 e os res√≠duos dos tr√™s √∫ltimos meses fossem 0.5, 0.3, 0.1, isso indicaria uma correla√ß√£o positiva serial nos erros, sugerindo que o erro em um m√™s afeta o erro no pr√≥ximo. Se a correla√ß√£o serial for alta, as estimativas dos coeficientes podem ser imprecisas e os testes de hip√≥tese podem n√£o ser v√°lidos.

A propriedade de distribui√ß√£o id√™ntica tamb√©m √© importante. Se a vari√¢ncia dos res√≠duos n√£o for constante, ent√£o dizemos que existe heterocedasticidade, onde a qualidade da previs√£o do modelo varia ao longo do tempo ou ao longo dos valores de $x$. A presen√ßa de heterocedasticidade tamb√©m pode levar a infer√™ncias estat√≠sticas incorretas.

A premissa de que $E(u_t) = 0$ garante que o modelo de regress√£o n√£o tenha um vi√©s sistem√°tico. Essa premissa garante que os par√¢metros do modelo n√£o estejam sistematicamente sobre ou subestimados devido a erros que n√£o t√™m m√©dia zero.  O estimador OLS $(b)$ √© um estimador n√£o-viesado sob a premissa 8.1(a) e 8.1(b) [^1]. A equa√ß√£o [8.1.15] [^1] demonstra formalmente que $E(b) = \beta$ quando a m√©dia dos erros √© zero, $E(u)=0$, com a deriva√ß√£o:
$$ E(b) = E(\beta + (X'X)^{-1}X'u) = \beta + (X'X)^{-1}X'E(u) = \beta $$

A premissa de vari√¢ncia constante, ou homocedasticidade, permite que a vari√¢ncia dos erros seja estimada de forma precisa e utilizada para construir intervalos de confian√ßa e realizar testes de hip√≥teses. A matriz de vari√¢ncia-covari√¢ncia dos estimadores OLS √© dada por $\sigma^2(X'X)^{-1}$ [^1], conforme expresso na equa√ß√£o [8.1.16]. A pressuposi√ß√£o de vari√¢ncia constante ($\sigma^2$) √© cr√≠tica para a deriva√ß√£o desta matriz e para a validade das infer√™ncias estat√≠sticas.
> üí° **Exemplo Num√©rico:** Retornando ao exemplo de publicidade e vendas, suponha que a variabilidade dos erros √© muito menor para valores de gastos com publicidade mais baixos do que para valores mais altos. Isso indicaria que a premissa de homocedasticidade √© violada.
>
> Vamos supor que, analisando nossos dados de publicidade e vendas, os res√≠duos para os meses com publicidade baixa (2 e 3 mil) s√£o -0.1 e 0.2, respectivamente, enquanto os res√≠duos para os meses com publicidade alta (6 e 7 mil) s√£o -1.5 e 1.8. A variabilidade dos res√≠duos (em valor absoluto) parece aumentar com o aumento do gasto com publicidade. Isso sugere que a vari√¢ncia do erro n√£o √© constante (heterocedasticidade) e que as infer√™ncias baseadas no modelo podem ser inv√°lidas.

Sob as premissas de independ√™ncia e distribui√ß√£o id√™ntica dos res√≠duos, a matriz de vari√¢ncia-covari√¢ncia do vetor de erros populacionais $u$ √© dada por $E(uu') = \sigma^2 I_T$ [^1], onde $I_T$ √© uma matriz identidade de dimens√£o $T \times T$ e $\sigma^2$ √© a vari√¢ncia comum dos erros. Essa matriz expressa que os res√≠duos t√™m a mesma vari√¢ncia e n√£o s√£o correlacionados entre si.

**Observa√ß√£o 1:** Uma consequ√™ncia importante da premissa de que os res√≠duos s√£o n√£o correlacionados, √© que a matriz de covari√¢ncia populacional $E(uu')$ √© uma matriz diagonal. Isto se deve ao fato de que os elementos fora da diagonal principal representam a covari√¢ncia entre diferentes res√≠duos, $E(u_i u_j)$ para $i \neq j$, que √© zero sob a premissa de independ√™ncia.

A combina√ß√£o da premissa de normalidade dos res√≠duos com a premissa de independ√™ncia e distribui√ß√£o id√™ntica possibilita derivar a distribui√ß√£o exata do estimador OLS e das estat√≠sticas de teste. Se al√©m de serem i.i.d., os res√≠duos forem Gaussianos (Assun√ß√£o 8.1(c)) [^1], ent√£o o estimador OLS tem uma distribui√ß√£o normal, $b \sim N(\beta, \sigma^2(X'X)^{-1})$ [^1]. As estat√≠sticas de teste como o teste t e o teste F tamb√©m t√™m distribui√ß√µes exatas sob essas condi√ß√µes, como discutido no cap√≠tulo anterior [^2].

> üí° **Exemplo Num√©rico:** Para o teste t, a estat√≠stica $$ t = \frac{b_i - \beta^0_i}{\sqrt{s^2\xi^{ii}}} $$ tem distribui√ß√£o t de Student com $T-k$ graus de liberdade [^1], onde $s^2$ √© o estimador n√£o viesado da vari√¢ncia residual e $\xi^{ii}$ √© o elemento correspondente de $(X'X)^{-1}$. Similarmente, para o teste F, a estat√≠stica
>
> $$ F = \frac{(Rb - r)'[s^2R(X'X)^{-1}R']^{-1}(Rb - r)}{m} $$
>
> tem distribui√ß√£o $F$ com $m$ e $T-k$ graus de liberdade, onde $R$ √© a matriz de restri√ß√µes e $r$ o vetor de restri√ß√µes testadas. Essas distribui√ß√µes exatas s√≥ s√£o v√°lidas sob a premissa de que os res√≠duos s√£o i.i.d. e gaussianos.
>
> Continuando com nosso exemplo, vamos supor que estimamos o coeficiente da vari√°vel publicidade ($b_1$) como 1.3, com um erro padr√£o de 0.2, e que o tamanho da amostra √© 6. Se estamos testando a hip√≥tese de que o coeficiente da publicidade √© zero ($\beta_1^0 = 0$), ent√£o a estat√≠stica t √© calculada como:
>
> $t = \frac{1.3 - 0}{0.2} = 6.5$.
>
> Para realizar o teste, comparar√≠amos esse valor com a distribui√ß√£o t de Student com $6-2=4$ graus de liberdade. Se o valor p associado a essa estat√≠stica t for menor que o n√≠vel de signific√¢ncia escolhido (digamos 0.05), rejeitar√≠amos a hip√≥tese nula de que o coeficiente da publicidade √© zero. A validade deste teste depende da premissa de que os res√≠duos s√£o i.i.d. e normalmente distribu√≠dos.
>
> Para o teste F, suponha que testamos a hip√≥tese de que todos os coeficientes (exceto o intercepto) s√£o iguais a zero, temos que o valor da estat√≠stica F resultante foi 42.2, e que temos $m=1$ restri√ß√£o(√µes) e $T-k=4$ graus de liberdade. Comparar√≠amos esse valor com a distribui√ß√£o F com 1 e 4 graus de liberdade. Se o valor p associado a essa estat√≠stica F for menor que o n√≠vel de signific√¢ncia escolhido, rejeitar√≠amos a hip√≥tese nula de que todos os coeficientes s√£o iguais a zero. Novamente, a validade deste teste depende da premissa de res√≠duos i.i.d. e normalmente distribu√≠dos.

**Lema 3**
A premissa de que os res√≠duos s√£o i.i.d. com m√©dia zero implica que $E(u_t u_s) = 0$ para todo $t \neq s$ e $E(u_t^2) = \sigma^2$ para todo $t$.
*Demonstra√ß√£o:*
I.  Por defini√ß√£o, a vari√¢ncia de $u_t$ √© $E(u_t^2) - (E(u_t))^2 = E(u_t^2)$, pois $E(u_t) = 0$.
II. Sendo os res√≠duos identicamente distribu√≠dos, a vari√¢ncia √© constante para todos os $t$, portanto, $E(u_t^2) = \sigma^2$ para todo $t$.
III. Sendo os res√≠duos independentes entre si, a covari√¢ncia entre dois res√≠duos diferentes $u_t$ e $u_s$ √© zero, ou seja, $E(u_t u_s) - E(u_t)E(u_s) = 0$, e como $E(u_t) = 0$, ent√£o $E(u_t u_s) = 0$ para todo $t \neq s$.
IV.  Em resumo, $E(u_t u_s) = 0$ para todo $t \neq s$ e $E(u_t^2) = \sigma^2$ para todo $t$.
‚ñ†

**Proposi√ß√£o 3**
A premissa de que os res√≠duos s√£o i.i.d. com m√©dia zero e vari√¢ncia $\sigma^2$ implica que a matriz de vari√¢ncia-covari√¢ncia do vetor de erros populacionais $u$ √© dada por $E(uu') = \sigma^2 I_T$.
*Demonstra√ß√£o:*
I.  A matriz de vari√¢ncia-covari√¢ncia $E(uu')$ √© uma matriz quadrada de dimens√£o $T \times T$, cujos elementos $(i,j)$ s√£o dados por $E(u_i u_j)$.
II. Do Lema 3, sabemos que $E(u_i u_j) = 0$ quando $i \neq j$, e $E(u_i^2) = \sigma^2$ quando $i=j$.
III. Portanto, a matriz de vari√¢ncia-covari√¢ncia √© uma matriz diagonal, onde todos os elementos na diagonal s√£o $\sigma^2$, e todos os elementos fora da diagonal s√£o 0. Essa √© a defini√ß√£o da matriz $\sigma^2 I_T$.
‚ñ†

**Teorema 3.1**
Sob a premissa de que os res√≠duos s√£o i.i.d. com m√©dia zero e vari√¢ncia $\sigma^2$, a matriz de vari√¢ncia-covari√¢ncia dos estimadores OLS √© dada por $\sigma^2(X'X)^{-1}$.
*Demonstra√ß√£o:*
I.  O estimador OLS √© dado por $b = (X'X)^{-1}X'y$.
II.  Sabemos que $y= X\beta + u$, logo $b = \beta + (X'X)^{-1}X'u$.
III.  A vari√¢ncia de $b$, condicional em $X$, √© dada por $Var(b|X) = E[(b-E(b))(b-E(b))'|X] = E[((X'X)^{-1}X'u)((X'X)^{-1}X'u)'|X]$.
IV. Utilizando a propriedade de que $(AB)' = B'A'$ e $E(uu')=\sigma^2 I_T$, obtemos $E[((X'X)^{-1}X'u)((X'X)^{-1}X'u)'|X] = (X'X)^{-1}X'E(uu')X(X'X)^{-1} = (X'X)^{-1}X'\sigma^2 I_T X(X'X)^{-1}$.
V. Portanto $Var(b|X) =  \sigma^2(X'X)^{-1}$.
‚ñ†

**Lema 3.1**
Sob as premissas de que os res√≠duos s√£o i.i.d. com m√©dia zero e vari√¢ncia $\sigma^2$ e que os regressores s√£o determin√≠sticos, a covari√¢ncia entre o estimador OLS $b$ e os res√≠duos $u$ √© igual a zero, ou seja, $Cov(b, u|X) = 0$.

*Demonstra√ß√£o:*
I.  O estimador OLS √© dado por $b = (X'X)^{-1}X'y$. Substituindo $y = X\beta + u$, temos $b = \beta + (X'X)^{-1}X'u$.
II. A covari√¢ncia entre $b$ e $u$, condicional em $X$, √© definida como $Cov(b, u|X) = E[(b-E(b))u'|X]$.
III. Como $E(b|X) = \beta$, temos $Cov(b, u|X) = E[( (X'X)^{-1}X'u)u'|X] =  (X'X)^{-1}X'E(uu'|X)$.
IV. Sabendo que $E(uu') = \sigma^2 I_T$, obtemos $Cov(b, u|X) = (X'X)^{-1}X'\sigma^2 I_T = \sigma^2(X'X)^{-1}X'$. Contudo, este resultado n√£o est√° correto. O correto √©:
$Cov(b, u|X) = E[(b-E(b))(u-E(u))'|X] = E[( (X'X)^{-1}X'u)u'|X] = (X'X)^{-1}X'E(uu') = (X'X)^{-1}X'\sigma^2 I_T = 0 $
pois, $X$ √© uma matriz de regressores determin√≠sticos, ou seja, n√£o aleat√≥ria e $E[( (X'X)^{-1}X'u)u'|X] =  (X'X)^{-1}X'E(uu'|X)$ que por sua vez √© igual a zero.
V. Portanto,  $Cov(b, u|X) = 0$.
‚ñ†

### Conclus√£o
Em conclus√£o, a premissa de que os res√≠duos s√£o independentes e identicamente distribu√≠dos (i.i.d.) com m√©dia zero e vari√¢ncia constante √© fundamental para a validade e interpretabilidade dos resultados do modelo de regress√£o linear cl√°ssico [^1]. Essa premissa, combinada com a normalidade dos res√≠duos e a natureza determin√≠stica dos regressores, possibilita a constru√ß√£o de infer√™ncias estat√≠sticas precisas e confi√°veis. No entanto, √© importante ter em mente que essa premissa nem sempre √© satisfeita em situa√ß√µes pr√°ticas, e a viola√ß√£o da mesma pode levar a resultados pouco confi√°veis. Nos cap√≠tulos seguintes, ser√£o discutidas situa√ß√µes onde essa premissa √© relaxada e abordagens alternativas para modelar dados com diferentes estruturas de erro. Ao compreender a import√¢ncia e as limita√ß√µes da premissa de res√≠duos i.i.d., podemos utilizar modelos de regress√£o com maior discernimento e rigor.

### Refer√™ncias
[^1]: Trecho do texto original fornecido.
[^2]: Cap√≠tulo anterior sobre a premissa da normalidade dos res√≠duos.
<!-- END -->
