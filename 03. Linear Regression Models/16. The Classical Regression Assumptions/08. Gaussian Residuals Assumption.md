## A Premissa da Normalidade dos Res√≠duos na Regress√£o Linear Cl√°ssica: An√°lise Detalhada

### Introdu√ß√£o
Dando continuidade √† an√°lise das premissas cl√°ssicas do modelo de regress√£o linear, este cap√≠tulo se concentrar√° na suposi√ß√£o de que os res√≠duos populacionais seguem uma distribui√ß√£o normal, tamb√©m conhecida como distribui√ß√£o Gaussiana. Como exploramos anteriormente, a regress√£o linear cl√°ssica assume que os regressores s√£o determin√≠sticos, o que simplifica a an√°lise e a estima√ß√£o dos par√¢metros [^1, ^4]. Tamb√©m j√° abordamos a premissa de que os res√≠duos s√£o independentes e identicamente distribu√≠dos (i.i.d.) com m√©dia zero e vari√¢ncia constante [^5]. Agora, vamos investigar em detalhe a import√¢ncia da normalidade dos res√≠duos, suas implica√ß√µes para a infer√™ncia estat√≠stica e como essa premissa complementa a natureza determin√≠stica dos regressores e a premissa de res√≠duos i.i.d..

### Conceitos Fundamentais
A premissa de que os **res√≠duos populacionais ($u_t$) seguem uma distribui√ß√£o normal (Gaussiana)** √© crucial para a validade da infer√™ncia estat√≠stica no modelo de regress√£o linear [^1]. Essa premissa, formalizada como parte da Assun√ß√£o 8.1(c) [^1], estabelece que os erros aleat√≥rios no modelo s√£o distribu√≠dos de acordo com uma distribui√ß√£o normal, com m√©dia zero e vari√¢ncia constante ($\sigma^2$). Matematicamente, isso √© expresso como: $u_t \sim N(0, \sigma^2)$.

A distribui√ß√£o normal, dada sua simetria e propriedades bem estabelecidas, simplifica a an√°lise e a interpreta√ß√£o dos resultados da regress√£o. A normalidade dos res√≠duos permite aplicar testes estat√≠sticos param√©tricos, como os testes *t* e *F*, que s√£o mais poderosos e precisos quando a distribui√ß√£o dos erros √© conhecida [^1]. Al√©m disso, essa premissa √© fundamental para derivar as distribui√ß√µes exatas dos estimadores e estat√≠sticas de teste, o que √© essencial para construir intervalos de confian√ßa e realizar testes de hip√≥tese.

> üí° **Exemplo Num√©rico:** Suponha que estamos analisando o efeito da temperatura ($x$) no rendimento de uma colheita ($y$). Depois de ajustar um modelo de regress√£o linear, os res√≠duos ($u_t$) representam as diferen√ßas entre os rendimentos observados e os rendimentos previstos pelo modelo. Se os res√≠duos seguem uma distribui√ß√£o normal, isso indica que os erros de previs√£o do nosso modelo s√£o aleat√≥rios e distribu√≠dos simetricamente em torno de zero, o que √© um bom sinal.
>
> Por exemplo, consideremos 10 observa√ß√µes de temperatura e rendimento, com um modelo de regress√£o linear simples: $y_t = \beta_0 + \beta_1 x_t + u_t$.
>
> Usando dados hipot√©ticos:
>
> | Temperatura ($x_t$) | Rendimento ($y_t$) |
> |---------------------|-------------------|
> | 20                 | 50                |
> | 22                 | 55                |
> | 25                 | 62                |
> | 28                 | 70                |
> | 30                 | 75                |
> | 24                 | 60                |
> | 26                 | 65                |
> | 29                 | 72                |
> | 21                 | 53                |
> | 23                 | 58                |
>
> Ap√≥s ajustar o modelo de regress√£o linear, obtemos os seguintes res√≠duos (diferen√ßas entre os valores observados e os valores previstos):
>
> | Res√≠duo ($u_t$) |
> |-----------------|
> | -1.2 |
> | 0.8  |
> | -0.5 |
> | 1.5  |
> | -2.1  |
> | 0.2  |
> | 0.1  |
> | -0.3  |
> | 1.7  |
> | -0.2 |
>
> Se plotarmos um histograma ou um gr√°fico Q-Q dos res√≠duos, e eles se aproximarem de uma distribui√ß√£o normal centrada em zero, a premissa de normalidade √© razoavelmente satisfeita. Podemos usar testes como o Shapiro-Wilk para verificar a normalidade.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy.stats import shapiro, probplot
>
> residuals = np.array([-1.2, 0.8, -0.5, 1.5, -2.1, 0.2, 0.1, -0.3, 1.7, -0.2])
>
> # Histogram
> plt.figure(figsize=(10, 4))
> plt.subplot(1, 2, 1)
> plt.hist(residuals, bins='auto', edgecolor='black')
> plt.title("Histograma dos Res√≠duos")
>
> # Q-Q plot
> plt.subplot(1, 2, 2)
> probplot(residuals, dist="norm", plot=plt)
> plt.title("Gr√°fico Q-Q dos Res√≠duos")
>
> plt.tight_layout()
> plt.show()
>
> # Shapiro-Wilk test
> stat, p = shapiro(residuals)
> print(f"Shapiro-Wilk Test: Statistic={stat:.3f}, p-value={p:.3f}")
> if p > 0.05:
>   print("Os res√≠duos parecem seguir uma distribui√ß√£o normal.")
> else:
>    print("Os res√≠duos n√£o parecem seguir uma distribui√ß√£o normal.")
> ```
>
> A sa√≠da deste c√≥digo produzir√° um histograma e um gr√°fico Q-Q dos res√≠duos, e tamb√©m o resultado do teste de Shapiro-Wilk, que pode ajudar a verificar a normalidade dos res√≠duos de forma mais precisa. Um p-valor alto no teste de Shapiro-Wilk sugere que os res√≠duos podem ser considerados como seguindo uma distribui√ß√£o normal.

A import√¢ncia da premissa de normalidade se manifesta na forma como as propriedades estat√≠sticas do estimador de coeficientes OLS ($b$) s√£o derivadas. Conforme mencionado anteriormente, sob as premissas de 8.1(a) e 8.1(b) [^1, ^4, ^5], $b$ √© um estimador n√£o viesado com matriz de vari√¢ncia-covari√¢ncia $\sigma^2(X'X)^{-1}$ [^1]. No entanto, a Assun√ß√£o 8.1(c), que assume a normalidade dos res√≠duos, adiciona informa√ß√£o crucial sobre a distribui√ß√£o amostral do estimador. Conforme a equa√ß√£o [8.1.17] [^1],  quando $u$ √© gaussiano, o estimador $b$ tamb√©m √© gaussiano e segue a distribui√ß√£o $b \sim N(\beta, \sigma^2(X'X)^{-1})$.

Essa normalidade de $b$ √© crucial porque permite realizar testes de hip√≥teses sobre os par√¢metros do modelo. Por exemplo, o teste t, detalhado na equa√ß√£o [8.1.26] [^1], √© utilizado para verificar se um coeficiente individual ($\beta_i$) √© estatisticamente diferente de zero ou de um valor espec√≠fico ($ \beta_i^0$). Este teste √© constru√≠do com base na premissa de que a distribui√ß√£o do estimador $b_i$ √© normal.

> üí° **Exemplo Num√©rico:** Voltando ao exemplo do fertilizante e crescimento de plantas, se quisermos verificar se o fertilizante tem um efeito significativo no crescimento, realizamos um teste de hip√≥teses para o coeficiente correspondente. Se os res√≠duos forem normais, podemos utilizar o teste t com confian√ßa para obter um valor p. Por exemplo, seja a hip√≥tese nula $H_0:\beta_1 = 0$.  O teste t √©:
>
> $$ t = \frac{b_1 - 0}{\sqrt{s^2 \xi^{ii}}} $$
> onde $b_1$ √© o estimador do coeficiente do fertilizante, $s^2$ √© o estimador da vari√¢ncia do res√≠duo e $\xi^{ii}$ √© o elemento diagonal correspondente de $(X'X)^{-1}$. Sob a premissa de normalidade, $t$ segue uma distribui√ß√£o t de Student com $T-k$ graus de liberdade [^1], permitindo-nos calcular o valor p para verificar se h√° evid√™ncias contra a hip√≥tese nula.
>
> Suponha que, ap√≥s rodar a regress√£o, obtivemos $b_1 = 0.5$, $s^2 = 0.1$, e $\xi^{ii} = 0.04$ (elemento correspondente ao nosso coeficiente de interesse). O n√∫mero de amostras √© $T=20$ e o n√∫mero de regressores √© $k=2$. Ent√£o, a estat√≠stica t √©:
>
> $$ t = \frac{0.5 - 0}{\sqrt{0.1 \times 0.04}} = \frac{0.5}{\sqrt{0.004}} = \frac{0.5}{0.0632} \approx 7.91 $$
> O valor cr√≠tico para um teste bicaudal com $\alpha = 0.05$ e $T-k = 18$ graus de liberdade √© aproximadamente $2.10$. Como o valor absoluto da estat√≠stica $t$ (7.91) √© maior que o valor cr√≠tico, rejeitamos a hip√≥tese nula de que o coeficiente √© zero. Isso sugere que o fertilizante tem um efeito estatisticamente significativo no crescimento das plantas.
>
> ```python
> import numpy as np
> from scipy.stats import t
>
> b1 = 0.5
> s2 = 0.1
> x_ii = 0.04
> T = 20
> k = 2
>
> t_statistic = b1 / np.sqrt(s2 * x_ii)
> print(f"Estat√≠stica t: {t_statistic:.2f}")
>
> degrees_freedom = T - k
> critical_value = t.ppf(1-0.025, degrees_freedom) #Two-tailed
> print(f"Valor cr√≠tico t (alpha=0.05, two-tailed): {critical_value:.2f}")
>
> p_value = 2 * (1 - t.cdf(abs(t_statistic), degrees_freedom))
> print(f"Valor p: {p_value:.4f}")
>
> if abs(t_statistic) > critical_value:
>   print("Rejeita-se a hip√≥tese nula")
> else:
>  print("N√£o se rejeita a hip√≥tese nula")
> ```
> Este c√≥digo calcula a estat√≠stica t, o valor cr√≠tico com um n√≠vel de signific√¢ncia de 5% (two-tailed), e o valor p para o teste. Ele ent√£o decide se a hip√≥tese nula deve ser rejeitada com base no valor t calculado e no valor p.

A normalidade dos res√≠duos tamb√©m impacta na distribui√ß√£o da estat√≠stica F [^1], utilizada para realizar testes de hip√≥teses conjuntas. A estat√≠stica F √© constru√≠da sob a premissa de que o res√≠duo tem distribui√ß√£o Gaussiana e, quando essa premissa √© satisfeita, a estat√≠stica F segue uma distribui√ß√£o F com $m$ e $T-k$ graus de liberdade sob a hip√≥tese nula. A distribui√ß√£o exata da estat√≠stica $F$ √© derivada sob a premissa de normalidade. Conforme a equa√ß√£o [8.1.32], a estat√≠stica $F$ √© dada por:

$$ F = \frac{(Rb - r)'[s^2R(X'X)^{-1}R']^{-1}(Rb - r)}{m} $$
onde $R$ √© uma matriz de restri√ß√µes, $r$ √© o vetor dos valores de restri√ß√£o, $m$ √© o n√∫mero de restri√ß√µes testadas, e $s^2$ √© o estimador da vari√¢ncia residual.
> üí° **Exemplo Num√©rico:** Continuando com o exemplo anterior, se temos mais de um regressor, poder√≠amos usar o teste F para testar a hip√≥tese de que um grupo de coeficientes (por exemplo, Œ≤‚ÇÅ e Œ≤‚ÇÇ) s√£o conjuntamente iguais a zero.
>
> Suponha que temos o modelo $y_t = \beta_0 + \beta_1 x_{1t} + \beta_2 x_{2t} + u_t$, onde $x_{1t}$ √© a quantidade de fertilizante e $x_{2t}$ √© a quantidade de √°gua utilizada. Queremos testar a hip√≥tese conjunta de que ambos os coeficientes, $\beta_1$ e $\beta_2$, s√£o iguais a zero.
>
>  Seja  $R = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$, e $r = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$, o n√∫mero de restri√ß√µes $m=2$.
>
> Suponha que, ap√≥s rodar a regress√£o, obtemos:
> $b = \begin{bmatrix} 10 \\ 0.3 \\ 0.4 \end{bmatrix}$, $s^2=0.05$, e $(X'X)^{-1} = \begin{bmatrix} 0.2 & -0.05 & 0.01 \\ -0.05 & 0.03 & -0.005 \\ 0.01 & -0.005 & 0.02 \end{bmatrix}$.
>
> Ent√£o:
>
> $Rb = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 10 \\ 0.3 \\ 0.4 \end{bmatrix} = \begin{bmatrix} 0.3 \\ 0.4 \end{bmatrix}$
>
>  $R(X'X)^{-1}R' = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 0.2 & -0.05 & 0.01 \\ -0.05 & 0.03 & -0.005 \\ 0.01 & -0.005 & 0.02 \end{bmatrix} \begin{bmatrix} 0 & 0 \\ 1 & 0 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 0.03 & -0.005 \\ -0.005 & 0.02 \end{bmatrix}$
>
> $[R(X'X)^{-1}R']^{-1} = \begin{bmatrix} 34.88 & 8.72 \\ 8.72 & 52.33 \end{bmatrix} $
>
> $(Rb - r)' [s^2R(X'X)^{-1}R']^{-1} (Rb - r) = [0.3 \ 0.4] \ 0.05^{-1}\begin{bmatrix} 34.88 & 8.72 \\ 8.72 & 52.33 \end{bmatrix}  \begin{bmatrix} 0.3 \\ 0.4 \end{bmatrix} = 27.31$
>
> $F = \frac{27.31}{2} = 13.655$
>
> O valor cr√≠tico de $F$ com 2 e $T-k=17$ graus de liberdade (assumindo $T=20$) a um n√≠vel de signific√¢ncia de 0.05 √© aproximadamente 3.59. Dado que nosso valor F calculado (13.655) √© maior que o valor cr√≠tico, rejeitamos a hip√≥tese nula de que ambos os coeficientes s√£o conjuntamente iguais a zero, o que indica que pelo menos um deles tem um efeito estatisticamente significativo.
>
> ```python
> import numpy as np
> from scipy.stats import f
>
> b = np.array([10, 0.3, 0.4])
> s2 = 0.05
> x_prime_x_inv = np.array([[0.2, -0.05, 0.01],
>                         [-0.05, 0.03, -0.005],
>                         [0.01, -0.005, 0.02]])
> R = np.array([[0, 1, 0],
>               [0, 0, 1]])
> r = np.array([0, 0])
> T = 20
> k = 3 #number of regressors
> m = 2  # number of restrictions
>
> Rb = R @ b
>
> r_x_inv_r_prime = R @ x_prime_x_inv @ R.T
>
> f_statistic_numerator = (Rb - r).T @ np.linalg.inv(s2*r_x_inv_r_prime) @ (Rb - r)
>
> f_statistic =  f_statistic_numerator/m
>
> print(f"Estat√≠stica F: {f_statistic:.3f}")
>
> critical_value = f.ppf(1-0.05, m, T-k)
> print(f"Valor cr√≠tico F (alpha=0.05): {critical_value:.3f}")
>
> p_value = 1- f.cdf(f_statistic, m, T-k)
> print(f"Valor p: {p_value:.4f}")
>
> if f_statistic > critical_value:
>   print("Rejeita-se a hip√≥tese nula")
> else:
>  print("N√£o se rejeita a hip√≥tese nula")
>
> ```
> O c√≥digo acima calcula a estat√≠stica F e o valor-p, e decide se a hip√≥tese nula deve ser rejeitada com base na compara√ß√£o com o valor cr√≠tico, ilustrando como o teste F √© usado na pr√°tica e o papel da premissa de normalidade para sua validade.

Sob a premissa de normalidade, o numerador de $F$ segue uma distribui√ß√£o $\chi^2(m)$, onde $m$ √© o n√∫mero de restri√ß√µes lineares e o denominador segue uma distribui√ß√£o $\chi^2(T-k)$ dividido por seus graus de liberdade,  $T-k$, sendo ambos independentes. Portanto, $F$ segue a distribui√ß√£o de probabilidade $F$ sob a hip√≥tese nula, permitindo-nos realizar testes estat√≠sticos de hip√≥teses conjuntas.

**Lema 2**
Sob a premissa de que os res√≠duos populacionais s√£o normalmente distribu√≠dos, a distribui√ß√£o do estimador OLS $b$ tamb√©m √© normal.
*Demonstra√ß√£o:*
I. A equa√ß√£o [8.1.12] nos mostra que $b = \beta + (X'X)^{-1}X'u$.
II. Se os res√≠duos $u$ s√£o normalmente distribu√≠dos, ent√£o qualquer combina√ß√£o linear de $u$ tamb√©m √© normalmente distribu√≠da, ou seja, $X'u$ tamb√©m segue uma distribui√ß√£o normal.
III. Consequentemente, como $(X'X)^{-1}X'$ √© uma matriz de valores fixos, ent√£o $(X'X)^{-1}X'u$ segue uma distribui√ß√£o normal, j√° que √© uma combina√ß√£o linear de vari√°veis normais.
IV. A soma de um vetor constante ($\beta$) com um vetor normal ($ (X'X)^{-1}X'u $) tamb√©m tem uma distribui√ß√£o normal, e logo $b$ √© normalmente distribu√≠do.
$\blacksquare$

**Proposi√ß√£o 2**
A premissa de normalidade dos res√≠duos implica que o teste t, dado em [8.1.26], segue uma distribui√ß√£o t de Student com $T-k$ graus de liberdade sob a hip√≥tese nula.
*Demonstra√ß√£o:*
I.  A estat√≠stica do teste t √© dada por: $$ t = \frac{b_i - \beta^0_i}{\sqrt{s^2\xi^{ii}}} $$
II. Do Lema 2, sabemos que $b_i$ √© normalmente distribu√≠do sob a premissa de normalidade dos res√≠duos, com m√©dia $\beta_i$ e vari√¢ncia $\sigma^2\xi^{ii}$, onde $\xi^{ii}$ √© o elemento diagonal correspondente de $(X'X)^{-1}$. Assim,  $\frac{b_i - \beta_i}{\sqrt{\sigma^2\xi^{ii}}}$ segue uma distribui√ß√£o normal padr√£o $N(0,1)$.
III. O estimador da vari√¢ncia, $s^2$, √© independente de $b$ sob a normalidade dos res√≠duos, como expresso na equa√ß√£o [8.1.25] [^1].
IV. Conforme a equa√ß√£o [8.1.24] [^1], $\frac{(T-k)s^2}{\sigma^2}$ segue uma distribui√ß√£o $\chi^2(T-k)$.
V. O teste t √© a raz√£o entre a distribui√ß√£o normal padr√£o e a raiz quadrada da distribui√ß√£o $\chi^2$ dividida por seus graus de liberdade, o que define a distribui√ß√£o t de Student com $T-k$ graus de liberdade.
$\blacksquare$
> üí° **Exemplo Num√©rico:**
> Continuando com o exemplo anterior, vamos testar a hip√≥tese de que o coeficiente $\beta_1$ √© igual a zero, usando os mesmos dados e os resultados j√° obtidos para os estimadores e erros padr√£o.
>
> A estat√≠stica t √© dada por:
> $t = \frac{b_1 - 0}{\sqrt{s^2 \xi^{11}}}$,
>
> onde $b_1$ = 0.5 √© o coeficiente estimado para o regressor $x$,  $s^2$ = 0.1 √© a vari√¢ncia do erro e $\xi^{11}$ = 0.04 √© o elemento diagonal correspondente de $(X'X)^{-1}$.
>
> $t = \frac{0.5}{\sqrt{0.1 \times 0.04}} = \frac{0.5}{\sqrt{0.004}} \approx 7.9$.
>
> Como vimos anteriormente, sob a premissa de normalidade, essa estat√≠stica t segue uma distribui√ß√£o t de Student com $20-2=18$ graus de liberdade. Com um valor t de 7.9, o valor-p (probabilidade de obter um valor t t√£o extremo quanto 7.9) √© praticamente zero, dado que a distribui√ß√£o t de Student tem uma cauda muito fina. Portanto, podemos rejeitar a hip√≥tese nula de que o coeficiente da vari√°vel fertilizante √© igual a zero, e concluir que o fertilizante tem um efeito estatisticamente significativo no rendimento da colheita.
>
> ```python
> import numpy as np
> from scipy.stats import t
>
> b1 = 0.5
> s2 = 0.1
> x_ii = 0.04
> T = 20
> k = 2
>
> t_statistic = b1 / np.sqrt(s2 * x_ii)
> degrees_freedom = T - k
> p_value = 2 * (1 - t.cdf(abs(t_statistic), degrees_freedom))
>
> print(f"Estat√≠stica t: {t_statistic:.2f}")
> print(f"Valor p: {p_value:.4f}")
>
> critical_value = t.ppf(1-0.025, degrees_freedom)
> print(f"Valor cr√≠tico: {critical_value:.2f}")
>
> if abs(t_statistic) > critical_value:
>   print("Rejeita-se a hip√≥tese nula")
> else:
>  print("N√£o se rejeita a hip√≥tese nula")
> ```
> Este c√≥digo exemplifica como realizar um teste t, o qual depende da normalidade dos res√≠duos. Ele calcula a estat√≠stica t e o valor-p, e decide sobre a hip√≥tese nula com base no valor p e no n√≠vel de signific√¢ncia estabelecido.

**Teorema 2.1**
Sob as premissas do modelo cl√°ssico de regress√£o linear, incluindo a normalidade dos res√≠duos, o estimador da vari√¢ncia residual $s^2 = \frac{RSS}{T-k}$ √© um estimador n√£o viesado da vari√¢ncia populacional $\sigma^2$.

*Demonstra√ß√£o:*
I. Sabemos que $RSS = u'M_Xu$, onde $M_X = I - X(X'X)^{-1}X'$ √© a matriz idempotente que projeta os res√≠duos no espa√ßo ortogonal ao espa√ßo das colunas de $X$.
II.  Conforme a equa√ß√£o [8.1.24] [^1], $\frac{(T-k)s^2}{\sigma^2} = \frac{RSS}{\sigma^2}$ segue uma distribui√ß√£o $\chi^2(T-k)$. O valor esperado de uma vari√°vel com distribui√ß√£o $\chi^2(df)$ √© $df$.
III. Assim, $E[\frac{RSS}{\sigma^2}] = T-k$.
IV.  Multiplicando ambos os lados por $\frac{\sigma^2}{T-k}$, obtemos:  $E[\frac{RSS}{T-k}] = \sigma^2$.
V. Portanto, $E[s^2] = \sigma^2$, o que demonstra que $s^2$ √© um estimador n√£o viesado de $\sigma^2$.
$\blacksquare$

A premissa de normalidade dos res√≠duos tamb√©m tem implica√ß√µes na distribui√ß√£o da vari√¢ncia estimada ($s^2$). A equa√ß√£o [8.1.24] estabelece que  $RSS/\sigma^2 = u'M_Xu/\sigma^2$ segue uma distribui√ß√£o qui-quadrado ($\chi^2$) com $T-k$ graus de liberdade [^1], onde $M_X$ √© a matriz idempotente $I - X(X'X)^{-1}X'$. Esta propriedade √© fundamental para a constru√ß√£o de testes estat√≠sticos sobre a vari√¢ncia do erro.

> üí° **Exemplo Num√©rico:** Se quisermos realizar um teste de hip√≥tese sobre a vari√¢ncia do erro, por exemplo, se a vari√¢ncia √© diferente de um valor determinado, a distribui√ß√£o qui-quadrado √© utilizada para realizar a infer√™ncia estat√≠stica.
>
> Por exemplo, suponha que em nosso modelo de rendimento da colheita temos $T = 50$ observa√ß√µes e $k=2$ regressores e calculamos $RSS = 10$. O nosso estimador de vari√¢ncia √© $s^2 = \frac{RSS}{T-k} = \frac{10}{50-2} = \frac{10}{48} = 0.2083$.
>
> Se quisermos testar se a vari√¢ncia √© igual a um valor espec√≠fico (digamos, $\sigma^2_0 = 0.2$), podemos usar a estat√≠stica:
> $$ \frac{(T-k)s^2}{\sigma^2_0} = \frac{48 \times 0.2083}{0.2} = 50 $$
>
> Sob a premissa de normalidade dos res√≠duos, essa estat√≠stica segue uma distribui√ß√£o $\chi^2$ com $T-k=48$ graus de liberdade.  Podemos consultar a tabela da distribui√ß√£o $\chi^2$ para determinar o valor p e concluir se existe evid√™ncia para rejeitar a hip√≥tese nula de que a vari√¢ncia √© igual a 0.2. Se o valor p √© abaixo de um certo n√≠vel de signific√¢ncia (ex: 0.05), rejeitamos a hip√≥tese nula.
>
> ```python
> import numpy as np
> from scipy.stats import chi2
>
> T = 50
> k = 2
> RSS = 10
> s2 = RSS / (T-k)
> sigma2_0 = 0.2
>
> chi2_statistic = (T - k) * s2 / sigma2_0
> degrees_freedom = T - k
>
> p_value = 1 - chi2.cdf(chi2_statistic, degrees_freedom)
>
> print(f"Estat√≠stica Qui-Quadrado: {chi2_statistic:.2f}")
> print(f"Valor p: {p_value:.4f}")
>
> critical_value = chi2.ppf(1-0.05, degrees_freedom)
> print(f"Valor cr√≠tico: {critical_value:.2f}")
>
> if chi2_statistic > critical_value:
>     print("Rejeita-se a hip√≥tese nula")
> else:
>     print("N√£o se rejeita a hip√≥tese nula")
> ```
> Este c√≥digo calcula a estat√≠stica do teste qui-quadrado e o valor p, o qual assume a normalidade dos res√≠duos. O valor p indica se h√° evid√™ncias estat√≠sticas para rejeitar a hip√≥tese nula de que a vari√¢ncia do erro √© igual a um valor especificado.

**Observa√ß√£o 1:** A premissa de normalidade dos res√≠duos √© crucial para que a distribui√ß√£o exata do estimador OLS ($b$) e das estat√≠sticas de teste t e F possam ser derivadas. Se essa premissa n√£o for satisfeita, as distribui√ß√µes dos estimadores e estat√≠sticas podem n√£o seguir as distribui√ß√µes t e F, invalidando a infer√™ncia estat√≠stica. Em muitos casos, essa premissa pode ser relaxada utilizando o teorema do limite central quando o tamanho da amostra √© grande. No entanto, em amostras pequenas, a viola√ß√£o dessa premissa pode levar a resultados pouco confi√°veis. Em particular, √© importante notar que o tamanho amostral necess√°rio para que o teorema do limite central possa ser invocado varia dependendo do grau em que a distribui√ß√£o dos res√≠duos se afasta da normalidade.

**Observa√ß√£o 2:** Embora o estimador OLS seja n√£o viesado mesmo sob viola√ß√µes da normalidade (contanto que $E(u)=0$), os testes de hip√≥teses (como o teste t e o teste F) s√≥ s√£o v√°lidos sob normalidade dos res√≠duos (e sob homocedasticidade, e aus√™ncia de autocorrela√ß√£o), para amostras pequenas. Sob amostras grandes, o teorema do limite central garante que a distribui√ß√£o dos estimadores se aproxima da normal, e que a distribui√ß√£o dos testes se aproxima da distribui√ß√£o t ou F, mesmo sem a normalidade dos res√≠duos. Contudo, em amostras pequenas, a validade dos testes pode estar comprometida na aus√™ncia de res√≠duos normais.

**Teorema 2.2**
Sob as premissas do modelo cl√°ssico de regress√£o linear, incluindo a normalidade dos res√≠duos, o estimador da vari√¢ncia residual $s^2 = \frac{RSS}{T-k}$ √© independente do estimador OLS $b$.

*Demonstra√ß√£o:*
I. Sabemos que $b = \beta + (X'X)^{-1}X'u$ e $RSS = u'M_Xu$.
II.  A independ√™ncia entre $s^2$ e $b$ se mant√©m se a covari√¢ncia entre eles for nula. Ou seja, precisamos mostrar que $Cov(b, s^2) = 0$.
III. $Cov(b, s^2) = Cov( (X'X)^{-1}X'u, \frac{u'M_Xu}{T-k}) =  \frac{1}{T-k}Cov( (X'X)^{-1}X'u, u'M_Xu)$.
IV.  Sob a premissa de normalidade dos res√≠duos, $Cov(u_i, u_j^2) = 0$ para todos $i$ e $j$ [^1]. Este resultado √© uma propriedade da distribui√ß√£o normal.
V. Dado que $Cov( (X'X)^{-1}X'u, u'M_Xu)$ √© uma soma de termos que cont√™m esses tipos de covari√¢ncias (entre os res√≠duos individuais e os quadrados dos res√≠duos), ent√£o a covari√¢ncia √© igual a zero.
VI. Portanto, $Cov(b, s^2) = 0$, mostrando que os estimadores s√£o independentes.
$\blacksquare$

**Corol√°rio 2.1**
Sob as premissas do modelo cl√°ssico de regress√£o linear, incluindo a normalidade dos res√≠duos, a estat√≠stica t dada por $\frac{b_i-\beta^0_i}{\sqrt{s^2\xi^{ii}}}$ √© independente do estimador da vari√¢ncia $s^2$

*Demonstra√ß√£o:*
I.  Pelo Teorema 2.2, sabemos que $b$ e $s^2$ s√£o independentes.
II.  A estat√≠stica t √© uma fun√ß√£o de $b$ e $s^2$.
III. Como $b$ e $s^2$ s√£o independentes, qualquer fun√ß√£o de $b$ ser√° independente de qualquer fun√ß√£o de $s^2$.
IV. Portanto, a estat√≠stica t √© independente de $s^2$.
$\blacksquare$

### Conclus√£o
Em suma, a premissa de normalidade dos res√≠duos populacionais √© uma pedra angular da regress√£o linear cl√°ssica [^1]. Essa premissa, em conjunto com a premissa de regressores determin√≠sticos e a premissa de que os res√≠duos s√£o i.i.d., possibilita a aplica√ß√£o de ferramentas de infer√™ncia estat√≠stica, como os testes t e F, e a constru√ß√£o de intervalos de confian√ßa para os par√¢metros do modelo. Al√©m disso, a distribui√ß√£o normal tamb√©m afeta a distribui√ß√£o dos estimadores de vari√¢ncia. Sob normalidade e as demais premissas, os estimadores do modelo OLS t√™m distribui√ß√£o normal, as estat√≠sticas t seguem distribui√ß√£o t de Student e as estat√≠sticas F seguem distribui√ß√£o F, com graus de liberdade bem definidos. Embora essa premissa simplifique a an√°lise, √© importante reconhecer que nem sempre ela se mant√©m na pr√°tica. Nas pr√≥ximas se√ß√µes, exploraremos situa√ß√µes onde essa premissa √© relaxada e analisaremos as implica√ß√µes para a infer√™ncia estat√≠stica, e tamb√©m modelos que n√£o requerem essa premissa t√£o restritiva. A compreens√£o da import√¢ncia da premissa de normalidade dos res√≠duos permite usar modelos de regress√£o de maneira mais informada e eficaz, ao mesmo tempo em que entendemos suas limita√ß√µes e as alternativas dispon√≠veis.

### Refer√™ncias
[^1]: Trecho do texto original fornecido.
[^2]: Cap√≠tulo anterior sobre a premissa de regressores determin√≠sticos.
[^3]: Cap√≠tulo anterior sobre a premissa de res√≠duos i.i.d.
[^4]: Cap√≠tulo anterior sobre a premissa de regressores determin√≠sticos.
[^5]: Cap√≠tulo anterior sobre a premissa de res√≠duos i.i.d.
<!-- END -->
