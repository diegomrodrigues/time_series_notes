## A Premissa de Res√≠duos Independentes e Identicamente Distribu√≠dos na Regress√£o Linear Cl√°ssica: Implica√ß√µes Detalhadas

### Introdu√ß√£o
Neste cap√≠tulo, aprofundamos o estudo das premissas do modelo de regress√£o linear cl√°ssico, com foco na suposi√ß√£o de que os res√≠duos populacionais s√£o independentes e identicamente distribu√≠dos (i.i.d.), com m√©dia zero e vari√¢ncia constante. Como discutimos nos cap√≠tulos anteriores, a regress√£o linear cl√°ssica assume que os regressores s√£o determin√≠sticos e que os res√≠duos s√£o normalmente distribu√≠dos, premissas essas que permitem derivar resultados estat√≠sticos [^1, ^2, ^3]. Exploraremos agora a import√¢ncia da independ√™ncia e distribui√ß√£o id√™ntica dos res√≠duos, suas implica√ß√µes e como essa premissa se encaixa no arcabou√ßo da an√°lise de regress√£o. Este cap√≠tulo ir√° detalhar as implica√ß√µes desta premissa para a vari√¢ncia do estimador OLS, e como esta premissa se relaciona com a propriedade de n√£o vi√©s do estimador OLS, e como a viola√ß√£o desta premissa afeta as propriedades dos testes de hip√≥tese.

### Conceitos Fundamentais
A premissa de que os **res√≠duos populacionais ($u_t$) s√£o independentes e identicamente distribu√≠dos (i.i.d.), com m√©dia zero e vari√¢ncia constante ($\sigma^2$)**, √© um dos pilares do modelo de regress√£o linear cl√°ssico [^1]. Formalmente expressa na Assun√ß√£o 8.1(b) [^1], essa premissa estabelece que:
1.  **Independ√™ncia:** Os res√≠duos ($u_t$) s√£o independentes entre si, ou seja, o valor de um res√≠duo em um ponto de tempo $t$ n√£o tem correla√ß√£o com o valor de outro res√≠duo em um ponto de tempo $s$, onde $t \neq s$.
2.  **Distribui√ß√£o Id√™ntica:** Os res√≠duos ($u_t$) seguem a mesma distribui√ß√£o de probabilidade para todos os pontos de tempo, com m√©dia zero e vari√¢ncia $\sigma^2$.
3.  **M√©dia Zero:** O valor esperado de cada res√≠duo √© zero, $E(u_t) = 0$.
4.  **Vari√¢ncia Constante (Homocedasticidade):** A vari√¢ncia de cada res√≠duo √© a mesma, ou seja, $Var(u_t) = \sigma^2$, para todo $t$.

Em nota√ß√£o matem√°tica, essa premissa √© expressa como $u_t \sim i.i.d.(0, \sigma^2)$ [^1].

Essa premissa √© crucial para a validade das infer√™ncias estat√≠sticas no modelo de regress√£o linear. Quando os res√≠duos s√£o independentes e identicamente distribu√≠dos, a variabilidade dos dados n√£o √© sistematicamente influenciada por fatores n√£o inclu√≠dos no modelo, e os testes de hip√≥teses e intervalos de confian√ßa podem ser interpretados com mais confian√ßa [^1].

> üí° **Exemplo Num√©rico:** Imagine que estamos analisando a rela√ß√£o entre o gasto com publicidade ($x$) e as vendas de um produto ($y$) ao longo de v√°rios meses. Os res√≠duos ($u_t$) representam a diferen√ßa entre as vendas observadas e as vendas previstas pelo modelo.
>
> *   **Independ√™ncia:** Se os res√≠duos s√£o i.i.d., ent√£o, o erro de previs√£o para o m√™s de janeiro n√£o deve influenciar o erro de previs√£o para o m√™s de fevereiro. N√£o deve haver padr√µes de erro se repetindo ao longo do tempo, ou seja, um res√≠duo positivo n√£o implica que o pr√≥ximo ser√° negativo (ou positivo).
> *   **Distribui√ß√£o Id√™ntica:**  A distribui√ß√£o dos erros deve ser consistente ao longo dos meses. A variabilidade dos erros deve ser aproximadamente a mesma ao longo do tempo e n√£o deve aumentar ou diminuir sistematicamente. N√£o devem haver meses em que o modelo gera erros maiores sistematicamente.
> *   **M√©dia Zero:** Em m√©dia, o modelo deve gerar previs√µes corretas, com erros se distribuindo de maneira balanceada (em torno de zero). N√£o deve haver um vi√©s sistem√°tico, onde o modelo superestima ou subestima os valores de $y$ sistematicamente.
> *   **Vari√¢ncia Constante (Homocedasticidade):** Os erros do modelo devem ser igualmente dispersos ao longo de todos os valores de $x$ e ao longo dos per√≠odos de tempo. A variabilidade dos erros n√£o deve ser menor (ou maior) em diferentes n√≠veis de gastos com publicidade.
>
> Suponha que temos os seguintes dados de gastos com publicidade (em milhares de reais) e vendas (em milhares de unidades) em 6 meses:
>
> | M√™s  | Publicidade (x) | Vendas (y) |
> |------|-----------------|------------|
> | Jan  | 2               | 5          |
> | Fev  | 3               | 7          |
> | Mar  | 4               | 8          |
> | Abr  | 5               | 9          |
> | Mai  | 6               | 11         |
> | Jun  | 7               | 12         |
>
> Um modelo de regress√£o linear simples ajustado a esses dados resulta na equa√ß√£o $\hat{y} = 2.5 + 1.3x$. Os res√≠duos para cada m√™s s√£o ent√£o:
>
> | M√™s  | Publicidade (x) | Vendas (y) | Vendas Previstas ($\hat{y}$) | Res√≠duo ($u_t$) |
> |------|-----------------|------------|----------------------------|-----------------|
> | Jan  | 2               | 5          | 5.1                        | -0.1            |
> | Fev  | 3               | 7          | 6.4                        | 0.6             |
> | Mar  | 4               | 8          | 7.7                        | 0.3             |
> | Abr  | 5               | 9          | 9.0                        | 0.0             |
> | Mai  | 6               | 11         | 10.3                       | 0.7             |
> | Jun  | 7               | 12         | 11.6                       | 0.4             |
>
> Para que a premissa de res√≠duos i.i.d. seja v√°lida, estes res√≠duos n√£o devem apresentar padr√µes.  Por exemplo, n√£o dever√≠amos ver todos os res√≠duos positivos no come√ßo e todos negativos no final, ou res√≠duos maiores nos meses de publicidade mais alta. Al√©m disso, a variabilidade dos res√≠duos deve ser aproximadamente a mesma ao longo dos meses. Um gr√°fico dos res√≠duos vs. os valores de $x$ (ou vs. o tempo) ajudaria a identificar padr√µes e poss√≠veis problemas com esta premissa.
>
> ```mermaid
>  graph LR
>      A[Publicidade (x)] --> B(Res√≠duos);
>      C[Tempo] --> B;
>  style B fill:#f9f,stroke:#333,stroke-width:2px
> ```
> Este gr√°fico ajudaria a visualizar se a variabilidade dos res√≠duos est√° relacionada a $x$ ou ao tempo.

A independ√™ncia dos res√≠duos √© especialmente importante em an√°lises de s√©ries temporais, onde os dados s√£o coletados sequencialmente ao longo do tempo. Em tais casos, a viola√ß√£o da independ√™ncia pode levar a estimativas viesadas e testes de hip√≥teses incorretos. Essa propriedade √© frequentemente testada usando estat√≠sticas como o teste de Durbin-Watson.

> üí° **Exemplo Num√©rico:**  Voltando ao exemplo de gastos com publicidade e vendas, se os res√≠duos s√£o dependentes, podemos encontrar um padr√£o como o seguinte: meses com res√≠duos positivos tendem a ser seguidos por meses com res√≠duos positivos, e vice-versa. Isso indicaria que h√° alguma estrutura temporal nos res√≠duos que o modelo n√£o est√° capturando, e que a premissa de independ√™ncia √© violada. A presen√ßa de correla√ß√£o serial nos res√≠duos pode ser um indicativo de que vari√°veis relevantes est√£o faltando no modelo ou que a especifica√ß√£o do modelo est√° inadequada, seja por um erro na escolha da forma funcional do modelo (linear, logar√≠tmica, etc) ou por n√£o considerar vari√°veis importantes.
>
> Por exemplo, se em nosso exemplo de gastos com publicidade, os res√≠duos dos primeiros tr√™s meses fossem -0.5, -0.3, -0.1 e os res√≠duos dos tr√™s √∫ltimos meses fossem 0.5, 0.3, 0.1, isso indicaria uma correla√ß√£o positiva serial nos erros, sugerindo que o erro em um m√™s afeta o erro no pr√≥ximo. Se a correla√ß√£o serial for alta, as estimativas dos coeficientes podem ser imprecisas e os testes de hip√≥tese podem n√£o ser v√°lidos. Podemos quantificar isso calculando a autocorrela√ß√£o dos res√≠duos. A autocorrela√ß√£o de primeira ordem, $\rho_1$, √© a correla√ß√£o entre $u_t$ e $u_{t-1}$. Em nosso exemplo,
>
> ```python
> import numpy as np
> residuals = np.array([-0.5, -0.3, -0.1, 0.5, 0.3, 0.1])
> # Calcula autocorrela√ß√£o de primeira ordem (usando uma aproxima√ß√£o)
> def autocorr(x, t=1):
>     return np.corrcoef(x[:-t], x[t:])[0, 1]
> rho_1 = autocorr(residuals)
> print(f"Autocorrela√ß√£o de primeira ordem: {rho_1:.2f}")
> ```
> Um valor alto (pr√≥ximo de 1 ou -1) de $\rho_1$ indicaria forte autocorrela√ß√£o, o que invalidaria a premissa de res√≠duos independentes.

A propriedade de distribui√ß√£o id√™ntica tamb√©m √© importante. Se a vari√¢ncia dos res√≠duos n√£o for constante, ent√£o dizemos que existe heterocedasticidade, onde a qualidade da previs√£o do modelo varia ao longo do tempo ou ao longo dos valores de $x$. A presen√ßa de heterocedasticidade tamb√©m pode levar a infer√™ncias estat√≠sticas incorretas.

A premissa de que $E(u_t) = 0$ garante que o modelo de regress√£o n√£o tenha um vi√©s sistem√°tico. Essa premissa garante que os par√¢metros do modelo n√£o estejam sistematicamente sobre ou subestimados devido a erros que n√£o t√™m m√©dia zero.  O estimador OLS $(b)$ √© um estimador n√£o-viesado sob a premissa 8.1(a) e 8.1(b) [^1]. A equa√ß√£o [8.1.15] [^1] demonstra formalmente que $E(b) = \beta$ quando a m√©dia dos erros √© zero, $E(u)=0$, com a deriva√ß√£o:
$$ E(b) = E(\beta + (X'X)^{-1}X'u) = \beta + (X'X)^{-1}X'E(u) = \beta $$

A premissa de vari√¢ncia constante, ou homocedasticidade, permite que a vari√¢ncia dos erros seja estimada de forma precisa e utilizada para construir intervalos de confian√ßa e realizar testes de hip√≥teses. A matriz de vari√¢ncia-covari√¢ncia dos estimadores OLS √© dada por $\sigma^2(X'X)^{-1}$ [^1], conforme expresso na equa√ß√£o [8.1.16]. A pressuposi√ß√£o de vari√¢ncia constante ($\sigma^2$) √© cr√≠tica para a deriva√ß√£o desta matriz e para a validade das infer√™ncias estat√≠sticas.
> üí° **Exemplo Num√©rico:** Retornando ao exemplo de publicidade e vendas, suponha que a variabilidade dos erros √© muito menor para valores de gastos com publicidade mais baixos do que para valores mais altos. Isso indicaria que a premissa de homocedasticidade √© violada.
>
> Vamos supor que, analisando nossos dados de publicidade e vendas, os res√≠duos para os meses com publicidade baixa (2 e 3 mil) s√£o -0.1 e 0.2, respectivamente, enquanto os res√≠duos para os meses com publicidade alta (6 e 7 mil) s√£o -1.5 e 1.8. A variabilidade dos res√≠duos (em valor absoluto) parece aumentar com o aumento do gasto com publicidade. Isso sugere que a vari√¢ncia do erro n√£o √© constante (heterocedasticidade) e que as infer√™ncias baseadas no modelo podem ser inv√°lidas.
>
>  Vamos considerar um exemplo num√©rico mais espec√≠fico. Suponha que temos um modelo de regress√£o simples, $y_i = \beta_0 + \beta_1 x_i + u_i$, e as seguintes duas amostras:
>
> *   **Amostra 1:** $x$ = \[1, 2, 3] e res√≠duos $u$ = \[0.1, -0.1, 0.2]
> *   **Amostra 2:** $x$ = \[4, 5, 6] e res√≠duos $u$ = \[0.5, -0.4, 0.6]
>
> Em ambos os casos, a m√©dia dos res√≠duos √© aproximadamente zero. Contudo, na amostra 2, a variabilidade dos erros √© muito maior que na amostra 1. Se a vari√¢ncia dos erros √© dependente dos regressores, como mostrado nesse exemplo, a premissa da homocedasticidade √© violada, o que impacta a validade das infer√™ncias estat√≠sticas. Em particular, o uso de um estimador de vari√¢ncia $\sigma^2$ que presume a homocedasticidade pode levar a testes de hip√≥tese que s√£o muito otimistas, ou seja, que rejeitam a hip√≥tese nula com muita frequ√™ncia, ou a intervalos de confian√ßa muito estreitos, o que leva a conclus√µes falsas sobre a signific√¢ncia dos coeficientes.
>
> Formalmente, em um modelo com heterocedasticidade, a matriz de vari√¢ncia-covari√¢ncia do erro √© dada por $E(uu') = \Sigma$, onde $\Sigma$ √© uma matriz n√£o diagonal com elementos n√£o constantes na diagonal principal (indicando a heterocedasticidade). A matriz de vari√¢ncia-covari√¢ncia do estimador OLS sob heterocedasticidade ser√° dada por: $$Var(b) = (X'X)^{-1} X' \Sigma X (X'X)^{-1}$$, que √© diferente da matriz $\sigma^2(X'X)^{-1}$ sob homocedasticidade.
>
> Vamos calcular a vari√¢ncia dos res√≠duos nas duas amostras:
>
> ```python
> import numpy as np
> residuals1 = np.array([0.1, -0.1, 0.2])
> residuals2 = np.array([0.5, -0.4, 0.6])
> variance1 = np.var(residuals1, ddof=1)
> variance2 = np.var(residuals2, ddof=1)
> print(f"Vari√¢ncia da amostra 1: {variance1:.3f}")
> print(f"Vari√¢ncia da amostra 2: {variance2:.3f}")
> ```
> Vemos que a vari√¢ncia da segunda amostra √© bem maior que a da primeira, o que evidencia a heterocedasticidade.

Sob as premissas de independ√™ncia e distribui√ß√£o id√™ntica dos res√≠duos, a matriz de vari√¢ncia-covari√¢ncia do vetor de erros populacionais $u$ √© dada por $E(uu') = \sigma^2 I_T$ [^1], onde $I_T$ √© uma matriz identidade de dimens√£o $T \times T$ e $\sigma^2$ √© a vari√¢ncia comum dos erros. Essa matriz expressa que os res√≠duos t√™m a mesma vari√¢ncia e n√£o s√£o correlacionados entre si.

**Observa√ß√£o 1:** Uma consequ√™ncia importante da premissa de que os res√≠duos s√£o n√£o correlacionados, √© que a matriz de covari√¢ncia populacional $E(uu')$ √© uma matriz diagonal. Isto se deve ao fato de que os elementos fora da diagonal principal representam a covari√¢ncia entre diferentes res√≠duos, $E(u_i u_j)$ para $i \neq j$, que √© zero sob a premissa de independ√™ncia.

A combina√ß√£o da premissa de normalidade dos res√≠duos com a premissa de independ√™ncia e distribui√ß√£o id√™ntica possibilita derivar a distribui√ß√£o exata do estimador OLS e das estat√≠sticas de teste. Se al√©m de serem i.i.d., os res√≠duos forem Gaussianos (Assun√ß√£o 8.1(c)) [^1], ent√£o o estimador OLS tem uma distribui√ß√£o normal, $b \sim N(\beta, \sigma^2(X'X)^{-1})$ [^1]. As estat√≠sticas de teste como o teste t e o teste F tamb√©m t√™m distribui√ß√µes exatas sob essas condi√ß√µes, como discutido no cap√≠tulo anterior [^2].

> üí° **Exemplo Num√©rico:** Para o teste t, a estat√≠stica $$ t = \frac{b_i - \beta^0_i}{\sqrt{s^2\xi^{ii}}} $$ tem distribui√ß√£o t de Student com $T-k$ graus de liberdade [^1], onde $s^2$ √© o estimador n√£o viesado da vari√¢ncia residual e $\xi^{ii}$ √© o elemento correspondente de $(X'X)^{-1}$. Similarmente, para o teste F, a estat√≠stica
>
> $$ F = \frac{(Rb - r)'[s^2R(X'X)^{-1}R']^{-1}(Rb - r)}{m} $$
>
> tem distribui√ß√£o $F$ com $m$ e $T-k$ graus de liberdade, onde $R$ √© a matriz de restri√ß√µes e $r$ o vetor de restri√ß√µes testadas. Essas distribui√ß√µes exatas s√≥ s√£o v√°lidas sob a premissa de que os res√≠duos s√£o i.i.d. e gaussianos.
>
> Continuando com nosso exemplo, vamos supor que estimamos o coeficiente da vari√°vel publicidade ($b_1$) como 1.3, com um erro padr√£o de 0.2, e que o tamanho da amostra √© 6. Se estamos testando a hip√≥tese de que o coeficiente da publicidade √© zero ($\beta_1^0 = 0$), ent√£o a estat√≠stica t √© calculada como:
>
> $t = \frac{1.3 - 0}{0.2} = 6.5$.
>
> Para realizar o teste, comparar√≠amos esse valor com a distribui√ß√£o t de Student com $6-2=4$ graus de liberdade. Se o valor p associado a essa estat√≠stica t for menor que o n√≠vel de signific√¢ncia escolhido (digamos 0.05), rejeitar√≠amos a hip√≥tese nula de que o coeficiente da publicidade √© zero. A validade deste teste depende da premissa de que os res√≠duos s√£o i.i.d. e normalmente distribu√≠dos.
>
> Para o teste F, suponha que testamos a hip√≥tese de que todos os coeficientes (exceto o intercepto) s√£o iguais a zero, temos que o valor da estat√≠stica F resultante foi 42.2, e que temos $m=1$ restri√ß√£o(√µes) e $T-k=4$ graus de liberdade. Comparar√≠amos esse valor com a distribui√ß√£o F com 1 e 4 graus de liberdade. Se o valor p associado a essa estat√≠stica F for menor que o n√≠vel de signific√¢ncia escolhido, rejeitar√≠amos a hip√≥tese nula de que todos os coeficientes s√£o iguais a zero. Novamente, a validade deste teste depende da premissa de res√≠duos i.i.d. e normalmente distribu√≠dos.
>
> Vamos calcular o p-valor para o teste t no exemplo acima. A estat√≠stica t foi calculada como 6.5, e temos 4 graus de liberdade:
>
> ```python
> from scipy import stats
> t_statistic = 6.5
> degrees_freedom = 4
> p_value = 2 * (1 - stats.t.cdf(abs(t_statistic), degrees_freedom))
> print(f"p-valor do teste t: {p_value:.4f}")
> ```
> O p-valor √© muito pequeno, o que nos leva a rejeitar a hip√≥tese nula de que o coeficiente da publicidade √© zero.

**Lema 3**
A premissa de que os res√≠duos s√£o i.i.d. com m√©dia zero implica que $E(u_t u_s) = 0$ para todo $t \neq s$ e $E(u_t^2) = \sigma^2$ para todo $t$.
*Demonstra√ß√£o:*
I.  Por defini√ß√£o, a vari√¢ncia de $u_t$ √© $E(u_t^2) - (E(u_t))^2 = E(u_t^2)$, pois $E(u_t) = 0$.
II. Sendo os res√≠duos identicamente distribu√≠dos, a vari√¢ncia √© constante para todos os $t$, portanto, $E(u_t^2) = \sigma^2$ para todo $t$.
III. Sendo os res√≠duos independentes entre si, a covari√¢ncia entre dois res√≠duos diferentes $u_t$ e $u_s$ √© zero, ou seja, $E(u_t u_s) - E(u_t)E(u_s) = 0$, e como $E(u_t) = 0$, ent√£o $E(u_t u_s) = 0$ para todo $t \neq s$.
IV.  Em resumo, $E(u_t u_s) = 0$ para todo $t \neq s$ e $E(u_t^2) = \sigma^2$ para todo $t$.
$\blacksquare$

**Proposi√ß√£o 3**
A premissa de que os res√≠duos s√£o i.i.d. com m√©dia zero e vari√¢ncia $\sigma^2$ implica que a matriz de vari√¢ncia-covari√¢ncia do vetor de erros populacionais $u$ √© dada por $E(uu') = \sigma^2 I_T$.
*Demonstra√ß√£o:*
I.  A matriz de vari√¢ncia-covari√¢ncia $E(uu')$ √© uma matriz quadrada de dimens√£o $T \times T$, cujos elementos $(i,j)$ s√£o dados por $E(u_i u_j)$.
II. Do Lema 3, sabemos que $E(u_i u_j) = 0$ quando $i \neq j$, e $E(u_i^2) = \sigma^2$ quando $i=j$.
III. Portanto, a matriz de vari√¢ncia-covari√¢ncia √© uma matriz diagonal, onde todos os elementos na diagonal s√£o $\sigma^2$, e todos os elementos fora da diagonal s√£o 0. Essa √© a defini√ß√£o da matriz $\sigma^2 I_T$.
$\blacksquare$

**Teorema 3.1**
Sob a premissa de que os res√≠duos s√£o i.i.d. com m√©dia zero e vari√¢ncia $\sigma^2$, a matriz de vari√¢ncia-covari√¢ncia dos estimadores OLS √© dada por $\sigma^2(X'X)^{-1}$.
*Demonstra√ß√£o:*
I.  O estimador OLS √© dado por $b = (X'X)^{-1}X'y$.
II.  Sabemos que $y= X\beta + u$, logo $b = \beta + (X'X)^{-1}X'u$.
III.  A vari√¢ncia de $b$, condicional em $X$, √© dada por $Var(b|X) = E[(b-E(b))(b-E(b))'|X] = E[((X'X)^{-1}X'u)((X'X)^{-1}X'u)'|X]$.
IV. Utilizando a propriedade de que $(AB)' = B'A'$ e $E(uu')=\sigma^2 I_T$, obtemos $E[((X'X)^{-1}X'u)((X'X)^{-1}X'u)'|X] = (X'X)^{-1}X'E(uu')X(X'X)^{-1} = (X'X)^{-1}X'\sigma^2 I_T X(X'X)^{-1}$.
V. Portanto $Var(b|X) =  \sigma^2(X'X)^{-1}$.
$\blacksquare$

**Lema 3.1**
Sob as premissas de que os res√≠duos s√£o i.i.d. com m√©dia zero e vari√¢ncia $\sigma^2$ e que os regressores s√£o determin√≠sticos, a covari√¢ncia entre o estimador OLS $b$ e os res√≠duos $u$ √© igual a zero, ou seja, $Cov(b, u|X) = 0$.

*Demonstra√ß√£o:*
I.  O estimador OLS √© dado por $b = (X'X)^{-1}X'y$. Substituindo $y = X\beta + u$, temos $b = \beta + (X'X)^{-1}X'u$.
II. A covari√¢ncia entre $b$ e $u$, condicional em $X$, √© definida como $Cov(b, u|X) = E[(b-E(b))(u-E(u))'|X]$.
III. Como $E(b|X) = \beta$, temos $Cov(b, u|X) = E[( (X'X)^{-1}X'u)u'|X] =  (X'X)^{-1}X'E(uu'|X)$.
IV. Sabendo que $E(uu') = \sigma^2 I_T$, obtemos $Cov(b, u|X) = (X'X)^{-1}X'\sigma^2 I_T = (X'X)^{-1}X'\sigma^2 I_T = 0$
V. Portanto,  $Cov(b, u|X) = 0$.
$\blacksquare$

**Teorema 3.2**
Sob as premissas de que os res√≠duos s√£o i.i.d. com m√©dia zero e vari√¢ncia $\sigma^2$ e que os regressores s√£o determin√≠sticos, o estimador OLS $b$ √© n√£o correlacionado com os res√≠duos $u$.

*Demonstra√ß√£o:*
I. Do Lema 3.1, sabemos que $Cov(b,u|X) = 0$.
II. Se a covari√¢ncia √© zero, isso implica que a correla√ß√£o entre $b$ e $u$ √© tamb√©m zero.
III. Portanto, o estimador OLS $b$ √© n√£o correlacionado com os res√≠duos $u$.
$\blacksquare$

**Proposi√ß√£o 3.1**
Se a premissa de homocedasticidade √© violada, ou seja, existe heterocedasticidade, a matriz de vari√¢ncia-covari√¢ncia do estimador OLS, $Var(b)$, n√£o ser√° igual a $\sigma^2(X'X)^{-1}$.

*Demonstra√ß√£o:*
I. Sob heterocedasticidade, $E(uu') = \Sigma$, onde $\Sigma$ √© uma matriz diagonal, mas com elementos diferentes de $\sigma^2$ na diagonal principal (i.e., $E(u_i^2) = \sigma_i^2 \neq \sigma^2$ para algum $i$).
II. A vari√¢ncia do estimador OLS, condicionada em X, √© dada por:
$Var(b|X) = E[(b - E(b))(b - E(b))'|X] = E[((X'X)^{-1}X'u)((X'X)^{-1}X'u)'|X] = (X'X)^{-1}X'E(uu')X(X'X)^{-1}$
III. Substituindo $E(uu')$ por $\Sigma$, temos $Var(b|X) = (X'X)^{-1}X' \Sigma X(X'X)^{-1}$
IV. Como $\Sigma \neq \sigma^2 I_T$, ent√£o $Var(b|X) \neq \sigma^2(X'X)^{-1}$.
$\blacksquare$

**Corol√°rio 3.1**
A viola√ß√£o da premissa de homocedasticidade leva a estimativas incorretas dos erros padr√£o dos coeficientes, o que invalida os testes de hip√≥teses e os intervalos de confian√ßa, caso o erro padr√£o calculado assuma homocedasticidade.

*Demonstra√ß√£o:*
I.  Pela Proposi√ß√£o 3.1, a matriz de vari√¢ncia-covari√¢ncia do estimador OLS n√£o √© dada por $\sigma^2(X'X)^{-1}$ sob heterocedasticidade.
II. Os erros padr√£o dos coeficientes, que s√£o utilizados para testes de hip√≥teses e constru√ß√£o de intervalos de confian√ßa, s√£o derivados da raiz quadrada dos elementos na diagonal principal de $Var(b)$.
III. Se a vari√¢ncia-covari√¢ncia do estimador OLS √© calculada incorretamente, os erros padr√£o tamb√©m ser√£o incorretos.
IV.  Consequentemente, os testes de hip√≥teses e intervalos de confian√ßa que usam os erros padr√£o calculados sob a premissa de homocedasticidade ser√£o inv√°lidos.
$\blacksquare$

### A Rela√ß√£o com a N√£o-Tendenciosidade do Estimador OLS
√â importante destacar que a premissa de res√≠duos com m√©dia zero ($E(u_t) = 0$) √© a principal respons√°vel pela n√£o-tendenciosidade do estimador OLS. A equa√ß√£o [8.1.15] [^1] mostra que $E(b) = \beta$ sob essa condi√ß√£o. O fato dos res√≠duos serem i.i.d. n√£o impacta na n√£o-tendenciosidade do estimador, mas sim na vari√¢ncia do estimador.

### Implica√ß√µes da Viola√ß√£o da Premissa de Res√≠duos i.i.d.
A viola√ß√£o da premissa de res√≠duos i.i.d. pode levar a resultados estat√≠sticos incorretos. A heterocedasticidade, que √© a vari√¢ncia n√£o constante dos res√≠duos, leva a estimativas ineficientes da vari√¢ncia dos coeficientes. J√° a autocorrela√ß√£o, ou depend√™ncia serial dos res√≠duos, leva a uma matriz de vari√¢ncia-covari√¢ncia dos erros que n√£o √© mais diagonal, o que impacta a vari√¢ncia dos estimadores e a validade dos testes de hip√≥teses. Nesses casos, o estimador OLS n√£o ser√° mais BLUE (Best Linear Unbiased Estimator).

√â importante reconhecer que na pr√°tica esta premissa √© frequentemente violada, principalmente em dados de s√©ries temporais e painel. Em tais casos, √© necess√°rio utilizar modelos e m√©todos de estima√ß√£o que relaxam essa premissa, como o modelo de m√≠nimos quadrados generalizados (GLS), ou estimadores robustos √† heteroscedasticidade e/ou autocorrela√ß√£o [^1], que ser√£o discutidos nos pr√≥ximos cap√≠tulos.

> üí° **Exemplo Num√©rico:** Para ilustrar o impacto da heterocedasticidade na matriz de vari√¢ncia-covari√¢ncia, suponha que temos um modelo de regress√£o com dois regressores, $y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + u_i$. Sob homocedasticidade, $Var(b) = \sigma^2 (X'X)^{-1}$. Vamos supor que $(X'X)^{-1}$ √© a seguinte matriz:
>
> $(X'X)^{-1} = \begin{bmatrix} 0.2 & -0.05 & 0.01 \\ -0.05 & 0.1 & 0.02 \\ 0.01 & 0.02 & 0.15 \end{bmatrix}$
>
> e que a vari√¢ncia dos res√≠duos √© $\sigma^2 = 4$. Nesse caso a matriz de covari√¢ncia dos estimadores OLS sob homocedasticidade seria:
>
> $Var(b) = 4 \times \begin{bmatrix} 0.2 & -0.05 & 0.01 \\ -0.05 & 0.1 & 0.02 \\ 0.01 & 0.02 & 0.15 \end{bmatrix} = \begin{bmatrix} 0.8 & -0.2 & 0.04 \\ -0.2 & 0.4 & 0.08 \\ 0.04 & 0.08 & 0.6 \end{bmatrix}$
>
> Agora, suponha que h√° heterocedasticidade. Nesse caso a matriz de vari√¢ncia-covari√¢ncia dos res√≠duos √© $\Sigma$, que n√£o √© proporcional √† identidade. A matriz de vari√¢ncia-covari√¢ncia dos estimadores OLS ser√° $Var(b|X) = (X'X)^{-1}X' \Sigma X(X'X)^{-1}$. Sem saber a forma de $\Sigma$, n√£o podemos calcular exatamente a nova matriz de vari√¢ncia-covari√¢ncia. Mas √© garantido que ser√° diferente de $\sigma^2(X'X)^{-1}$. Isso significa que os erros padr√£o dos coeficientes ser√£o diferentes (e geralmente maiores) sob heterocedasticidade.
>
> Vamos supor que ao calcular a matriz $ (X'X)^{-1}X' \Sigma X(X'X)^{-1}$ chegamos a:
>
> $Var(b|X) =  \begin{bmatrix} 1.2 & -0.3 & 0.08 \\ -0.3 & 0.6 & 0.12 \\ 0.08 & 0.12 & 0.9 \end{bmatrix}$
>
> As vari√¢ncias dos coeficientes, que s√£o os termos da diagonal principal, s√£o agora maiores (1.2, 0.6 e 0.9) do que as calculadas sob homocedasticidade (0.8, 0.4, e 0.6). Isso significa que os erros padr√£o (raiz quadrada da vari√¢ncia) ser√£o maiores, o que torna os testes de hip√≥tese menos precisos e intervalos de confian√ßa mais largos.

### Conclus√£o
Em suma, a premissa de que os res√≠duos s√£o independentes e identicamente distribu√≠dos com m√©dia zero e vari√¢ncia constante √© uma das bases para as infer√™ncias estat√≠sticas em modelos de regress√£o linear cl√°ssicos [^1]. Essa premissa, combinada com a normalidade dos res√≠duos e o determinismo dos regressores, permite derivar a distribui√ß√£o exata do estimador OLS e das estat√≠sticas de teste. No entanto, a aplica√ß√£o do modelo de regress√£o deve sempre considerar a validade desta premissa, dado que a sua viola√ß√£o pode levar a conclus√µes err√¥neas. A compreens√£o da import√¢ncia e das implica√ß√µes desta premissa √© um passo fundamental para a aplica√ß√£o correta e eficaz de modelos de regress√£o.

### Refer√™ncias
[^1]: Trecho do texto original fornecido.
[^2]: Cap√≠tulo anterior sobre a premissa da normalidade dos res√≠duos.
[^3]: Cap√≠tulo anterior sobre a premissa de regressores determin√≠sticos.
<!-- END -->
