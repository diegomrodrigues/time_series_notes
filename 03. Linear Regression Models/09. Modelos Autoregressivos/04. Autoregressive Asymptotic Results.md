## Resultados Assint√≥ticos para Modelos Autoregressivos

### Introdu√ß√£o
Como explorado anteriormente [^1, ^2], a estima√ß√£o de par√¢metros em modelos Autoregressivos de ordem *p* (AR(p)) por M√≠nimos Quadrados Ordin√°rios (OLS) apresenta desafios devido √† correla√ß√£o entre o regressor e o termo de erro. Esta correla√ß√£o invalida as propriedades de pequenas amostras do estimador OLS. No entanto, m√©todos assint√≥ticos permitem generalizar os resultados para grandes amostras. Este cap√≠tulo se concentra na deriva√ß√£o dos resultados assint√≥ticos para modelos AR(p), estabelecendo como a distribui√ß√£o da estimativa dos par√¢metros se aproxima de uma distribui√ß√£o normal √† medida que o tamanho da amostra aumenta. A compreens√£o destes resultados √© crucial para a infer√™ncia estat√≠stica em modelos autoregressivos, fornecendo uma base te√≥rica para realizar testes de hip√≥teses e construir intervalos de confian√ßa.

### Conceitos Fundamentais
Em um modelo AR(p), definido por [^43]:
$$y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t,$$
as dificuldades na an√°lise de pequenas amostras n√£o impedem que a teoria assint√≥tica seja utilizada para estabelecer propriedades de grandes amostras dos estimadores OLS. Sob condi√ß√µes adequadas, incluindo estacionaridade [^2], o estimador OLS para o modelo AR(p) √© consistente, e a sua distribui√ß√£o assint√≥tica √© normal, conforme a discuss√£o em [^47].

Para derivar esses resultados, √© crucial reconhecer que o regressor, composto pelos valores passados da vari√°vel, √© estoc√°stico e n√£o independente do erro, como √© assumido em modelos de regress√£o lineares padr√£o [^1]. Para lidar com essa complexidade, a distribui√ß√£o dos estimadores OLS √© derivada condicionalmente em X, e ent√£o integrada sobre a distribui√ß√£o de X, conforme mencionado em [^44]. Este processo permite obter a verdadeira distribui√ß√£o assint√≥tica dos estimadores.

> üí° **Exemplo Num√©rico:** Considere um modelo AR(1): $y_t = \phi y_{t-1} + \epsilon_t$. Mesmo que $\epsilon_t$ seja um ru√≠do branco com distribui√ß√£o normal, o estimador de $\phi$ obtido por OLS, denotado por $\hat{\phi}$, n√£o ter√° uma distribui√ß√£o *t* de Student em amostras pequenas. A raz√£o para isto √© a correla√ß√£o entre $y_{t-1}$ e $\epsilon_t$, que √© uma das consequ√™ncias da depend√™ncia temporal. No entanto, se o tamanho da amostra for grande, a distribui√ß√£o de $\hat{\phi}$ se aproxima de uma distribui√ß√£o normal. Essa aproxima√ß√£o permite aplicar testes de hip√≥teses e construir intervalos de confian√ßa em an√°lises com grandes amostras, embora as propriedades para amostras pequenas n√£o se mantenham.
>
> Vamos simular um modelo AR(1) com $\phi = 0.7$ e $\epsilon_t \sim N(0, 1)$ para ilustrar este ponto.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from statsmodels.regression.linear_model import OLS
>
> np.random.seed(42)
>
> def simulate_ar1(phi, n, burn_in=100):
>     y = np.zeros(n + burn_in)
>     errors = np.random.normal(0, 1, n + burn_in)
>     for t in range(1, n + burn_in):
>         y[t] = phi * y[t-1] + errors[t]
>     return y[burn_in:]
>
> phi_true = 0.7
> sample_sizes = [50, 200, 1000]
>
> fig, axes = plt.subplots(1, 3, figsize=(15, 5))
>
> for i, T in enumerate(sample_sizes):
>   y = simulate_ar1(phi_true, T)
>   X = y[:-1]
>   y_actual = y[1:]
>
>   model = OLS(y_actual, X)
>   results = model.fit()
>   phi_hat = results.params[0]
>
>   errors = y_actual - phi_hat * X
>   
>   axes[i].hist(errors, bins=20, density=True, alpha=0.6, label=f'T={T}')
>   
>   from scipy.stats import norm
>   xmin, xmax = plt.xlim()
>   x = np.linspace(xmin, xmax, 100)
>   p = norm.pdf(x, 0, np.std(errors))
>   axes[i].plot(x, p, 'k', linewidth=2, label='Normal Fit')
>
>   axes[i].set_title(f'Distribui√ß√£o dos Res√≠duos (T={T})')
>   axes[i].legend()
> plt.tight_layout()
> plt.show()
>
> ```
> O gr√°fico mostra a distribui√ß√£o dos res√≠duos para diferentes tamanhos de amostra. Para amostras pequenas, a distribui√ß√£o dos res√≠duos pode n√£o ser normal, mas √† medida que o tamanho da amostra aumenta, a distribui√ß√£o se aproxima de uma normal. Este exemplo ilustra a import√¢ncia dos resultados assint√≥ticos para modelos AR(p).

De acordo com [^57], a distribui√ß√£o assint√≥tica do estimador OLS ($\hat{b}$) no contexto de modelos AR(p) pode ser expressa como:
$$\sqrt{T}(\hat{b} - \beta) \xrightarrow{d} N(0, \sigma^2Q^{-1}),$$

onde:
* $\sqrt{T}$ √© o fator de escala, refletindo a diminui√ß√£o da vari√¢ncia do estimador com o aumento do tamanho da amostra ($T$).
* $\beta$ √© o vetor dos verdadeiros par√¢metros do modelo AR(p).
* $\sigma^2$ √© a vari√¢ncia do termo de erro $\epsilon_t$.
* $Q$ √© a matriz de covari√¢ncia das vari√°veis explicativas escalonada (normalizada) pelo tamanho da amostra $T$. A matriz $Q$ converge para uma matriz de autocovari√¢ncia sob condi√ß√µes de estacionaridade, como detalhado em cap√≠tulos anteriores.
* $\xrightarrow{d}$ denota a converg√™ncia em distribui√ß√£o para uma distribui√ß√£o normal.

A matriz $Q$ na distribui√ß√£o assint√≥tica do estimador √© de fundamental import√¢ncia, pois ela reflete a depend√™ncia temporal da s√©rie, expressa nas autocovari√¢ncias, conforme o Lema 1 [^48].
De acordo com o Lema 1 [^48], os elementos da matriz $Q$ para um modelo AR(p) s√£o dados por:

$$Q =  \begin{bmatrix}
\gamma_0 & \gamma_1 & \gamma_2 & \dots & \gamma_{p-1} \\
\gamma_1 & \gamma_0 & \gamma_1 & \dots & \gamma_{p-2} \\
\gamma_2 & \gamma_1 & \gamma_0 & \dots & \gamma_{p-3} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\gamma_{p-1} & \gamma_{p-2} & \gamma_{p-3} & \dots & \gamma_0
\end{bmatrix}$$

onde $\gamma_k = Cov(y_t, y_{t-k})$ representa a autocovari√¢ncia da s√©rie temporal em defasagem $k$. A matriz $Q$ √© uma matriz de Toeplitz, uma estrutura que se repete ao longo das diagonais.

> üí° **Exemplo Num√©rico:** Vamos considerar o modelo AR(2): $y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + \epsilon_t$. As autocovari√¢ncias do processo s√£o $\gamma_0$, $\gamma_1$, e $\gamma_2$. A matriz $Q$ seria dada por:
>
>  $$Q = \begin{bmatrix}
>  \gamma_0 & \gamma_1 \\
> \gamma_1 & \gamma_0
> \end{bmatrix}$$
>
>  Os valores de $\gamma_0$ (vari√¢ncia de $y_t$) e $\gamma_1$ (autocovari√¢ncia entre $y_t$ e $y_{t-1}$) s√£o estimados a partir da amostra, e a matriz $Q$ √© usada para obter a vari√¢ncia assint√≥tica dos estimadores OLS $\hat{\phi_1}$ e $\hat{\phi_2}$.
>
>   Suponha que, ap√≥s estimar um modelo AR(2) a partir de uma amostra grande de dados, obtemos as seguintes estimativas de autocovari√¢ncia: $\hat{\gamma}_0 = 4$, $\hat{\gamma}_1 = 2$. Ent√£o, a matriz $Q$ estimada seria:
>    $$\hat{Q} = \begin{bmatrix}
>   4 & 2 \\
>  2 & 4
>  \end{bmatrix}$$
>
>  Para obter a vari√¢ncia assint√≥tica dos estimadores OLS, precisamos da inversa de $\hat{Q}$:
>
> $$\hat{Q}^{-1} = \frac{1}{(4 \times 4) - (2 \times 2)} \begin{bmatrix}
> 4 & -2 \\
> -2 & 4
> \end{bmatrix} = \frac{1}{12} \begin{bmatrix}
> 4 & -2 \\
> -2 & 4
> \end{bmatrix} = \begin{bmatrix}
> 1/3 & -1/6 \\
> -1/6 & 1/3
> \end{bmatrix}$$
> Se a vari√¢ncia do erro estimada for $\hat{\sigma}^2 = 1$, a vari√¢ncia assint√≥tica dos estimadores $\hat{\phi}_1$ e $\hat{\phi}_2$ seria dada por $\sigma^2 Q^{-1}$. No caso de $\hat{\phi}_1$ e $\hat{\phi}_2$ a vari√¢ncia seria $1/3$, e a covari√¢ncia seria $-1/6$. Isso significa que os estimadores tem uma vari√¢ncia de $1/3$ e s√£o negativamente correlacionados.

**Proposi√ß√£o 3** (Consist√™ncia e Distribui√ß√£o Assint√≥tica): Sob as condi√ß√µes estabelecidas na *Assumption 8.3* [^45] e a estacionaridade da s√©rie temporal, o estimador OLS $\hat{b}$ de um modelo AR(p) √© consistente, ou seja, $\hat{b} \xrightarrow{p} \beta$ e a distribui√ß√£o assint√≥tica √© dada por
$$\sqrt{T}(\hat{b} - \beta) \xrightarrow{d} N(0, \sigma^2Q^{-1}).$$
A converg√™ncia em probabilidade garante que √† medida que o tamanho da amostra aumenta, o estimador $\hat{b}$ se aproxima do verdadeiro valor $\beta$, e a distribui√ß√£o assint√≥tica nos fornece uma maneira de calcular a vari√¢ncia aproximada, o que permite construir testes de hip√≥teses e intervalos de confian√ßa para os par√¢metros do modelo AR(p) em grandes amostras.

> üí° **Exemplo Num√©rico:** Suponha que temos um modelo AR(1) $y_t = 0.7y_{t-1} + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco. Com uma amostra de tamanho $T = 1000$, o estimador OLS $\hat{\phi}$ √© de 0.68, e a vari√¢ncia do erro estimada √© $\hat{\sigma}^2 = 1$. Se a autocovari√¢ncia $\gamma_0 = 2$, a matriz $Q$ ser√° igual a 2. Ent√£o a vari√¢ncia assint√≥tica do estimador ser√° $\sigma^2 Q^{-1} \approx 1 / 2= 0.5$. Aplicando a Proposi√ß√£o 3, $\sqrt{1000}(\hat{\phi} - 0.7)$  tem distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 0.5. Este resultado nos permite construir um intervalo de confian√ßa aproximado para $\phi$ usando a distribui√ß√£o normal.
>
> Usando os resultados do exemplo anterior, podemos calcular o intervalo de confian√ßa para $\phi$. A vari√¢ncia assint√≥tica do estimador √© $\frac{1}{2} = 0.5$, ent√£o o desvio padr√£o assint√≥tico √© $\sqrt{0.5} \approx 0.707$. Como $\sqrt{1000}(\hat{\phi} - 0.7) \sim N(0, 0.5)$, temos que $\hat{\phi} \sim N(0.7, \frac{0.5}{1000}) = N(0.7, 0.0005)$. Ent√£o, o desvio padr√£o de $\hat{\phi}$ √© $\sqrt{0.0005} \approx 0.022$.
>
> Para construir um intervalo de confian√ßa de 95%, usamos o valor cr√≠tico de 1.96 da distribui√ß√£o normal padr√£o. O intervalo de confian√ßa √© dado por:
> $$\hat{\phi} \pm 1.96 \times \sqrt{\frac{0.5}{1000}} = 0.68 \pm 1.96 \times 0.022 = [0.637, 0.723]$$
> Este intervalo de confian√ßa nos d√° uma ideia da precis√£o da nossa estimativa de $\phi$. Note que este resultado √© uma aproxima√ß√£o assint√≥tica e √© mais precisa com amostras maiores.

**Teorema 4** (Validade dos Testes *t* e *F* Assint√≥ticos): Sob as condi√ß√µes da *Proposi√ß√£o 3*, as estat√≠sticas *t* e *F* podem ser usadas para realizar testes de hip√≥teses sobre os par√¢metros de um modelo AR(p), desde que o tamanho da amostra seja suficientemente grande. Em amostras finitas, os testes *t* e *F* podem n√£o apresentar a distribui√ß√£o *t* e *F* como de praxe, mas seus resultados assint√≥ticos convergem para distribui√ß√µes qui-quadrado e normal padr√£o, que podem ser usadas para a constru√ß√£o de testes e intervalos de confian√ßa.

**Prova do Teorema 4:**
Para demonstrar a validade assint√≥tica dos testes t e F em modelos AR(p), √© necess√°rio mostrar como suas distribui√ß√µes se aproximam de distribui√ß√µes conhecidas sob certas condi√ß√µes, mesmo quando as distribui√ß√µes para amostras finitas n√£o s√£o as de praxe.

I. Come√ßamos com a estat√≠stica t, que √© utilizada para testar uma hip√≥tese sobre um √∫nico par√¢metro do modelo AR(p), por exemplo,  $H_0: \beta_i = \beta_{i0}$, o que leva a estat√≠stica t:
$$t = \frac{\hat{\beta_i} - \beta_{i0}}{\sqrt{\hat{s}^2(X'X)^{-1}_{ii}}}$$

Onde $\hat{\beta_i}$ √© a estimativa OLS do par√¢metro $\beta_i$, $\beta_{i0}$ √© o valor do par√¢metro sob a hip√≥tese nula, $\hat{s}^2$ √© a estimativa da vari√¢ncia dos erros, e $(X'X)^{-1}_{ii}$ √© o i-√©simo elemento da diagonal da matriz $(X'X)^{-1}$.
II. Sob as condi√ß√µes de regularidade (como *Assumption 8.3* [^45]) e para grandes amostras, vimos que o estimador $\hat{\beta}$ √© assintoticamente normal:  $\sqrt{T}(\hat{\beta} - \beta) \xrightarrow{d} N(0, \sigma^2Q^{-1})$. Al√©m disso, $\hat{s}^2 \xrightarrow{p} \sigma^2$.

III. Usando a lei das grandes n√∫meros, vemos que a matriz $(X'X)^{-1}$ converge para $Q^{-1}/T$, e que $(X'X)^{-1}_{ii} \rightarrow q_{ii} / T$, onde $q_{ii}$ √© o i-√©simo elemento da diagonal de $Q^{-1}$.

IV. Substituindo essas converg√™ncias assint√≥ticas na estat√≠stica t, temos:
$$
t = \frac{\hat{\beta_i} - \beta_{i0}}{\sqrt{\hat{s}^2(X'X)^{-1}_{ii}}} \approx \frac{\hat{\beta_i} - \beta_{i0}}{\sqrt{\sigma^2 q_{ii} / T}} = \frac{\sqrt{T}(\hat{\beta_i} - \beta_{i0})}{\sqrt{\sigma^2 q_{ii}}}
$$
Com a distribui√ß√£o assint√≥tica do numerador sendo normal com m√©dia zero e vari√¢ncia $\sigma^2 q_{ii}$  e $  \hat{s}^2$  converge para $\sigma^2$ sob hip√≥tese nula.
V.  Portanto, a estat√≠stica t converge para uma distribui√ß√£o normal padr√£o quando $T \to \infty$:
$$
t \xrightarrow{d}  N(0, 1)
$$

VI. A estat√≠stica F √© utilizada para testar hip√≥teses sobre m√∫ltiplas restri√ß√µes lineares nos par√¢metros, da forma $H_0: R\beta = r$, onde R √© uma matriz de restri√ß√µes e r √© um vetor constante. A estat√≠stica F √© dada por:
$$
F = \frac{(R\hat{\beta} - r)'[R(X'X)^{-1}R']^{-1}(R\hat{\beta} - r)/m}{\hat{s}^2}
$$
Onde m √© o n√∫mero de restri√ß√µes.
VII. Usando as converg√™ncias estabelecidas anteriormente e o fato de que o termo de erro converge para uma qui-quadrado, o numerador se aproxima de uma distribui√ß√£o qui-quadrado com m graus de liberdade, e o denominador se aproxima de uma constante, $\sigma^2$, conforme $T \to \infty$.
VIII. Portanto, a estat√≠stica F converge em distribui√ß√£o para uma qui-quadrado quando T tende ao infinito:
$$
mF  \xrightarrow{d}  \chi^2_m
$$

Portanto, as estat√≠sticas t e F n√£o possuem as distribui√ß√µes de pequenas amostras nos modelos AR(p), mas suas vers√µes assint√≥ticas convergem para distribui√ß√µes normais e qui-quadrado, respectivamente, sob condi√ß√µes de regularidade. Isso permite construir testes de hip√≥teses assintoticamente v√°lidos. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere um modelo AR(2). Para testar a hip√≥tese nula de que $\phi_2 = 0$ usando a estat√≠stica t, e com uma amostra de tamanho $T = 500$, podemos usar a aproxima√ß√£o normal para a distribui√ß√£o da estat√≠stica t, mesmo que o modelo AR(2) n√£o satisfa√ßa as condi√ß√µes de modelos lineares para pequenas amostras. Se o resultado da estat√≠stica t for maior que 1.96, podemos rejeitar a hip√≥tese nula a um n√≠vel de signific√¢ncia de 5%. De forma an√°loga, para testar a hip√≥tese nula de que $\phi_1 = \phi_2 = 0$, podemos usar a estat√≠stica F, cuja distribui√ß√£o assint√≥tica √© uma qui-quadrado com 2 graus de liberdade.
>
> Vamos supor que, em um modelo AR(2) estimado com $T=500$, obtivemos $\hat{\phi}_1 = 0.6$, $\hat{\phi}_2 = 0.2$, e o erro padr√£o de $\hat{\phi}_2$ √© $0.1$. Para testar $H_0: \phi_2 = 0$ contra $H_1: \phi_2 \neq 0$, calculamos a estat√≠stica t:
> $$t = \frac{\hat{\phi}_2 - 0}{SE(\hat{\phi}_2)} = \frac{0.2}{0.1} = 2$$
> Sob a hip√≥tese nula, a estat√≠stica t segue uma distribui√ß√£o normal padr√£o assintoticamente. Usando um n√≠vel de signific√¢ncia de 5%, rejeitamos a hip√≥tese nula, pois o valor cr√≠tico para um teste bicaudal √© 1.96. O valor-p √© $2 * (1 - \Phi(2)) = 0.045$ que √© menor que 0.05.
>
> Para testar a hip√≥tese nula $H_0: \phi_1 = \phi_2 = 0$ contra a hip√≥tese alternativa de que pelo menos um dos par√¢metros √© diferente de 0, usamos a estat√≠stica F. Suponha que a estat√≠stica F calculada a partir dos dados seja 4. A estat√≠stica F, multiplicada pelo n√∫mero de restri√ß√µes ($m=2$), converge assintoticamente para uma distribui√ß√£o qui-quadrado com 2 graus de liberdade.
>
> $$mF = 2 * 4 = 8$$
> O valor-p associado a uma qui-quadrado com 2 graus de liberdade e valor 8 √© aproximadamente 0.018. Como o valor-p √© menor que 0.05, rejeitamos a hip√≥tese nula de que ambos os par√¢metros s√£o iguais a zero.

**Teorema 4.1** (Consist√™ncia do Estimador da Vari√¢ncia do Erro): Sob as mesmas condi√ß√µes da *Proposi√ß√£o 3*, o estimador da vari√¢ncia do erro $\hat{\sigma}^2 = \frac{1}{T-p}\sum_{t=p+1}^{T} \hat{\epsilon}_t^2$ √© consistente, ou seja, $\hat{\sigma}^2 \xrightarrow{p} \sigma^2$.
**Prova do Teorema 4.1:**
A prova da consist√™ncia do estimador da vari√¢ncia do erro segue a mesma linha de racioc√≠nio da consist√™ncia do estimador OLS, utilizando a lei dos grandes n√∫meros e a estacionaridade da s√©rie temporal.
I. Partimos do res√≠duo estimado: $\hat{\epsilon}_t = y_t - \hat{c} - \hat{\phi}_1 y_{t-1} - \dots - \hat{\phi}_p y_{t-p}$.
II.  Podemos expressar $\hat{\epsilon}_t$ como $\epsilon_t - (\hat{b} - \beta)'X_t$, onde $X_t = [1, y_{t-1}, \dots, y_{t-p}]'$.
III.  O estimador da vari√¢ncia do erro √© dado por $\hat{\sigma}^2 = \frac{1}{T-p}\sum_{t=p+1}^{T} \hat{\epsilon}_t^2$.
IV. Expandindo o termo do quadrado, obtemos
    $\hat{\sigma}^2 = \frac{1}{T-p}\sum_{t=p+1}^{T} \left(\epsilon_t - (\hat{b} - \beta)'X_t\right)^2 =  \frac{1}{T-p}\sum_{t=p+1}^{T} \epsilon_t^2 - 2\frac{1}{T-p}\sum_{t=p+1}^{T}\epsilon_t(\hat{b} - \beta)'X_t + \frac{1}{T-p}\sum_{t=p+1}^{T} ((\hat{b} - \beta)'X_t)^2$.
V.  Sob a condi√ß√£o de estacionaridade e as *Assumption 8.3* [^45], usando a lei dos grandes n√∫meros, temos que  $\frac{1}{T-p}\sum_{t=p+1}^{T} \epsilon_t^2 \xrightarrow{p} E(\epsilon_t^2) = \sigma^2$.
VI. Como $\hat{b} \xrightarrow{p} \beta$ (pela *Proposi√ß√£o 3*), o segundo e terceiro termos tendem a zero em probabilidade. O termo central contem $(\hat{b}-\beta)$ que converge em probabilidade para 0, e o termo em $X_t$ converge para uma quantidade finita dada pela estacionariedade e condi√ß√µes de momento. O √∫ltimo termo cont√©m $(\hat{b}-\beta)^2$, e portanto converge para 0 em probabilidade.
VII. Portanto, $\hat{\sigma}^2 \xrightarrow{p} \sigma^2$. Isso significa que o estimador da vari√¢ncia do erro converge para a verdadeira vari√¢ncia, tornando-o um estimador consistente. ‚ñ†
> üí° **Exemplo Num√©rico:** Suponha que, ap√≥s estimar um modelo AR(1), obtemos a seguinte sequ√™ncia de res√≠duos $\hat{\epsilon}_t$: [0.1, -0.2, 0.3, -0.1, 0.2]. Calculamos a vari√¢ncia amostral:
> $$\hat{\sigma}^2 = \frac{1}{5-1} \sum_{t=2}^{5} \hat{\epsilon}_t^2 = \frac{0.1^2 + (-0.2)^2 + 0.3^2 + (-0.1)^2 + 0.2^2}{4} = \frac{0.01 + 0.04 + 0.09 + 0.01 + 0.04}{4} = \frac{0.19}{4} = 0.0475$$
> Para um conjunto de dados real, com $T$ grande, $\hat{\sigma}^2$ deve se aproximar de $\sigma^2$.

**Lema 4.1** (Converg√™ncia da Matriz de Covari√¢ncia Amostral): Sob as mesmas condi√ß√µes da *Proposi√ß√£o 3*, a matriz de covari√¢ncia amostral $\hat{Q} = \frac{1}{T} \sum_{t=p+1}^T X_t X_t'$ converge em probabilidade para a matriz de autocovari√¢ncia populacional $Q$, ou seja, $\hat{Q} \xrightarrow{p} Q$.
**Prova do Lema 4.1:**
A prova deste lema envolve a aplica√ß√£o da lei dos grandes n√∫meros aos elementos da matriz de covari√¢ncia amostral.
I.  A matriz $\hat{Q}$ √© definida como $\frac{1}{T} \sum_{t=p+1}^T X_t X_t'$, onde $X_t = [1, y_{t-1}, y_{t-2}, \dots, y_{t-p}]'$.
II. Cada elemento de $\hat{Q}$ √© da forma $\frac{1}{T}\sum_{t=p+1}^{T}x_{it}x_{jt}$, onde $x_{it}$ e $x_{jt}$ s√£o elementos de $X_t$.
III.  Sob as condi√ß√µes de estacionaridade e as *Assumption 8.3* [^45], cada elemento da matriz converge para sua esperan√ßa, ou seja, $\frac{1}{T} \sum_{t=p+1}^T x_{it}x_{jt} \xrightarrow{p} E(x_{it}x_{jt})$.
IV.  Como $E(x_{it}x_{jt})$ corresponde aos elementos da matriz $Q$, conclu√≠mos que $\hat{Q} \xrightarrow{p} Q$. ‚ñ†

> üí° **Exemplo Num√©rico:** Para o modelo AR(1)  $y_t = \phi y_{t-1} + \epsilon_t$, a matriz $\hat{Q}$ converge para $\gamma_0$. Se a vari√¢ncia amostral de $y_t$ for $\hat{\gamma}_0 = 2.1$ com uma amostra de $T = 1000$, esperamos que esse valor seja pr√≥ximo da vari√¢ncia te√≥rica $\gamma_0$ do processo. Similarmente, no modelo AR(2), a converg√™ncia de $\hat{Q}$ para $Q$ significa que as estimativas de $\gamma_0$, $\gamma_1$ e $\gamma_2$ obtidas da amostra convergem para suas contrapartidas te√≥ricas.
>
>  Vamos considerar um exemplo pr√°tico de um modelo AR(1), onde temos uma s√©rie temporal $y_t$ com 1000 observa√ß√µes. Ap√≥s estimar o modelo por OLS, calculamos a vari√¢ncia amostral $\hat{\gamma}_0$ da s√©rie, que √© igual a 2.3. Calculamos tamb√©m $\frac{1}{T} \sum_{t=2}^{T} y_{t-1}^2 = 2.2$. Em um modelo AR(1), a matriz $Q$ √© simplesmente $\gamma_0$. Ent√£o $\hat{Q} = 2.2$. Pelo Lema 4.1, quando $T \to \infty$, $\hat{Q}$ se aproxima da matriz de autocovari√¢ncia te√≥rica $Q$.

**Corol√°rio 4.1** (Distribui√ß√£o Assint√≥tica dos Res√≠duos): Sob as mesmas condi√ß√µes do *Teorema 4*, os res√≠duos estimados $\hat{\epsilon}_t$ convergem em distribui√ß√£o para o termo de erro verdadeiro $\epsilon_t$, ou seja $\hat{\epsilon}_t \xrightarrow{d} \epsilon_t$.
**Prova do Corol√°rio 4.1:**
Este resultado decorre da consist√™ncia do estimador OLS e da defini√ß√£o dos res√≠duos.
I. Definimos o res√≠duo estimado como $\hat{\epsilon}_t = y_t - \hat{c} - \hat{\phi}_1 y_{t-1} - \dots - \hat{\phi}_p y_{t-p}$.
II. Sabemos que $y_t = c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p} + \epsilon_t$.
III. Substituindo a segunda na primeira, temos $\hat{\epsilon}_t = \epsilon_t - (\hat{c} - c) - (\hat{\phi}_1 - \phi_1)y_{t-1} - \dots - (\hat{\phi}_p - \phi_p)y_{t-p}$.
IV. Como os estimadores OLS s√£o consistentes, temos $\hat{c} \xrightarrow{p} c$ e $\hat{\phi}_i \xrightarrow{p} \phi_i$ para $i=1, \dots, p$.
V. Portanto, os termos $(\hat{c} - c)$ e $(\hat{\phi}_i - \phi_i)$ convergem para zero em probabilidade.
VI. Consequentemente, $\hat{\epsilon}_t \xrightarrow{d} \epsilon_t$, indicando que os res√≠duos estimados se aproximam do termo de erro verdadeiro, tanto mais quando o tamanho amostral aumenta. ‚ñ†
> üí° **Exemplo Num√©rico:** Suponha que temos um modelo AR(1) com $\phi = 0.7$. Ap√≥s estimar o modelo usando OLS, obtemos estimativas para os par√¢metros $\hat{\phi} = 0.68$. Os res√≠duos obtidos pela diferen√ßa entre os valores preditos e os observados $\hat{\epsilon}_t$ s√£o uma aproxima√ß√£o dos erros verdadeiros $\epsilon_t$. Se a amostra for grande, $\hat{\epsilon}_t$ deve se comportar de forma similar ao erro $\epsilon_t$. Por exemplo, se $\epsilon_t$ for um ru√≠do branco normal, os res√≠duos dever√£o se comportar da mesma forma.
>
>  Para avaliar se os res√≠duos estimados $\hat{\epsilon}_t$ s√£o consistentes com um ru√≠do branco, podemos plotar o correlograma dos res√≠duos. Se os res√≠duos forem aleat√≥rios, o correlograma n√£o deve apresentar padr√µes.

### Conclus√£o
Em resumo, a deriva√ß√£o dos resultados assint√≥ticos para modelos AR(p) permite realizar infer√™ncias estat√≠sticas com amostras grandes, superando as limita√ß√µes associadas √†s propriedades de pequenas amostras. A distribui√ß√£o assint√≥tica dos estimadores OLS √© normal, com vari√¢ncia dada por $\sigma^2Q^{-1}$. Os testes *t* e *F* s√£o v√°lidos assintoticamente, o que permite testar hip√≥teses sobre os par√¢metros do modelo. A matriz $Q$ desempenha um papel fundamental, refletindo a estrutura de autocovari√¢ncia da s√©rie temporal. O entendimento desses resultados √© essencial para uma aplica√ß√£o robusta e precisa de modelos AR(p) em an√°lises de s√©ries temporais, especialmente quando o tamanho da amostra √© grande. Al√©m disso, a consist√™ncia do estimador da vari√¢ncia do erro, a converg√™ncia da matriz de covari√¢ncia amostral e a distribui√ß√£o assint√≥tica dos res√≠duos complementam o quadro te√≥rico para infer√™ncia em modelos AR(p).

### Refer√™ncias
[^1]: "Como explorado anteriormente, a estima√ß√£o de par√¢metros em modelos Autoregressivos de ordem *p* (AR(p)) por M√≠nimos Quadrados Ordin√°rios (OLS) apresenta desafios devido √† correla√ß√£o entre o regressor e o termo de erro."
[^2]:  "Al√©m disso, a consist√™ncia e a distribui√ß√£o assint√≥tica dos estimadores s√£o estabelecidas sob condi√ß√µes espec√≠ficas, como a estacionaridade da s√©rie temporal."
[^43]: "Assumption 8.4: The regression model is
  $y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t$"
[^44]: "The distribution of test statistics for this case can be found by a two-step procedure. The first step evaluates the distribution conditional on X; that is, it treats X as deterministic just as in the earlier analysis. The second step multiplies by the density of X and integrates over X to find the true unconditional distribution."
[^45]: "Assumption 8.3: (a) x, stochastic and independent of us for all t, s; (b) u, non-
Gaussian but i.i.d. with mean zero, variance o¬≤, and E(u‚Å¥) = ¬µ‚ÇÑ < ‚àû; (c) E(x‚Çúx‚Çú') = Q‚Çú, a positive definite matrix with (1/T)‚àëQ‚Çú‚Üí Q, a positive definite matrix; (d) E(x·µ¢‚Çúx‚±º‚Çúx‚Çó‚Çúx‚Çò‚Çú) < ‚àû for all i, j, l, m, and t; (e) (1/T)‚àëx‚Çúx‚Çú' ‚Üí Q."
[^47]: "To describe the asymptotic results, ... We first establish that the OLS coefficient estimator is consistent under Assumption 8.3, that is, that bT‚ÜíŒ≤."
[^48]: "Lema 1 (Matriz de covari√¢ncia): A matriz de covari√¢ncia $Q$ mencionada na distribui√ß√£o assint√≥tica do estimador OLS tem uma estrutura espec√≠fica para modelos AR(p). Dado que as vari√°veis explicativas s√£o os valores defasados da pr√≥pria s√©rie temporal, $Q$ est√° relacionada com as autocovari√¢ncias da s√©rie."
[^57]: "Next turn to the asymptotic distribution of b. Notice from [8.2.3] that:
  $‚àöT(b_T - \beta) = [(1/T) \sum_{t=1}^T x_tx_t']^{-1} [(1/ \sqrt{T}) \sum_{t=1}^T x_t u_t ]$"
<!-- END -->
