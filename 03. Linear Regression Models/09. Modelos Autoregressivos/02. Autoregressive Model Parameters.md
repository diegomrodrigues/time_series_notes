## Estimando Par√¢metros em Modelos Autoregressivos de Ordem p
### Introdu√ß√£o
Este cap√≠tulo se aprofunda na an√°lise de modelos de regress√£o, focando especificamente na estima√ß√£o de par√¢metros em modelos autoregressivos de ordem p (AR(p)). Como vimos anteriormente [^1], modelos AR(p) s√£o fundamentais para an√°lise de s√©ries temporais, pois descrevem a depend√™ncia de uma vari√°vel em seus valores passados. Uma peculiaridade desses modelos √© que, embora sejam tratados como problemas de regress√£o, as propriedades de pequenas amostras, derivadas no contexto de regress√£o linear padr√£o, n√£o se sustentam devido √† correla√ß√£o entre o regressor e o termo de erro.

### Conceitos Fundamentais
Como definido em [^43] o modelo AR(p) √© dado por:
$$y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t$$
onde $y_t$ √© a vari√°vel no tempo $t$, $c$ √© uma constante, $\phi_1, \phi_2, \ldots, \phi_p$ s√£o os coeficientes autoregressivos, e $\epsilon_t$ √© o termo de erro.

O m√©todo de M√≠nimos Quadrados Ordin√°rios (OLS) √© frequentemente empregado para estimar os par√¢metros deste modelo. No entanto, a natureza autoregressiva introduz uma complica√ß√£o crucial: o regressor, que inclui os valores passados da vari√°vel ($y_{t-1}, y_{t-2}, \ldots, y_{t-p}$), √© correlacionado com o termo de erro. Esta correla√ß√£o invalida as propriedades de amostras pequenas do OLS, que foram discutidas anteriormente [^2].

A intui√ß√£o dessa dificuldade reside no fato de que $y_{t-1}$ inclui a informa√ß√£o de $\epsilon_{t-1}$, o que faz com que $y_{t-1}$ e $\epsilon_t$ se tornem correlacionados.
Por exemplo, se $\epsilon_t$ √© positivo, $y_t$ ser√° relativamente grande, e isso afeta $y_{t+1}$.

> üí° **Exemplo Num√©rico:** Considere um modelo AR(1) simples: $y_t = 0.7y_{t-1} + \epsilon_t$. Se $\epsilon_t = 2$, ent√£o $y_t$ ser√° maior que o previsto pelo componente autoregressivo sozinho. Se $\epsilon_{t+1}$ √© um n√∫mero aleat√≥rio (por exemplo, 1), $y_{t+1}$ tamb√©m √© influenciado pelo valor anormalmente alto de $y_t$. Esta persist√™ncia na influ√™ncia dos erros passados ilustra a correla√ß√£o entre o regressor ($y_{t-1}$) e o erro no tempo $t+1$, que √© uma das dificuldades fundamentais do modelo AR.
>
> ```mermaid
> graph LR
>     A[Œµ(t-1)] --> B(y(t-1))
>     B --> C(y(t))
>     D[Œµ(t)] --> C
>     C --> E(y(t+1))
>     F[Œµ(t+1)] --> E
>     style B fill:#f9f,stroke:#333,stroke-width:2px
>     style C fill:#ccf,stroke:#333,stroke-width:2px
> ```
> O diagrama acima ilustra como um erro no per√≠odo t-1 afeta y(t-1) e, por sua vez, influencia y(t) e y(t+1), mostrando a depend√™ncia serial.

Como mencionado em [^44], a abordagem para derivar a distribui√ß√£o de estat√≠sticas de teste, neste caso, √© feita em duas etapas: primeiro, avaliando a distribui√ß√£o condicional em X (tratando X como determin√≠stico); e, em seguida, integrando essa distribui√ß√£o sobre a distribui√ß√£o de X. Essa abordagem √© necess√°ria para lidar com as propriedades estat√≠sticas quando o regressor √© estoc√°stico.

Apesar dessa dificuldade, a consist√™ncia e a distribui√ß√£o assint√≥tica dos estimadores OLS ainda podem ser estabelecidas sob certas condi√ß√µes, conforme discutido em [^46], que requer condi√ß√µes adicionais para resultados assint√≥ticos.
Sob condi√ß√µes adequadas (detalhadas na *Assumption 8.3* [^45]),  a distribui√ß√£o assint√≥tica do estimador OLS pode ser obtida. Especificamente, o estimador OLS para o modelo AR(p) ainda √© consistente, e as estat√≠sticas t e F, embora n√£o mais v√°lidas em amostras pequenas, podem ser justificadas assintoticamente.

**Proposi√ß√£o 1** (Estacionaridade e Invertibilidade): Para que um modelo AR(p) seja √∫til na modelagem de s√©ries temporais, √© essencial que ele seja estacion√°rio. A estacionaridade garante que as propriedades estat√≠sticas da s√©rie temporal (m√©dia, vari√¢ncia, autocovari√¢ncia) permane√ßam constantes ao longo do tempo. Al√©m disso, para que o processo seja invert√≠vel, todas as ra√≠zes do polin√¥mio autoregressivo devem estar fora do c√≠rculo unit√°rio. Estas condi√ß√µes asseguram que o modelo represente um processo est√°vel e que seus par√¢metros possam ser estimados de forma confi√°vel.

> üí° **Exemplo Num√©rico:** Considere um modelo AR(1) com $\phi_1 = 1.2$. A equa√ß√£o caracter√≠stica √© $1 - 1.2z = 0$, o que implica $z = 1/1.2 \approx 0.83$. Como $|z| < 1$, o modelo n√£o √© estacion√°rio. Agora, considere $\phi_1 = 0.8$. A raiz √© $z = 1/0.8 = 1.25$. Como $|z| > 1$, o modelo √© estacion√°rio. Intuitivamente, um modelo n√£o estacion√°rio com $\phi_1 > 1$ levaria a uma explos√£o da s√©rie temporal, o que seria inadequado para modelar dados reais.

  O estimador de m√≠nimos quadrados ordin√°rios, de acordo com [^56], pode ser expresso como:
  $$\hat{b} = (X'X)^{-1}X'y$$
  Para obter as propriedades assint√≥ticas, √© necess√°rio considerar a consist√™ncia e a distribui√ß√£o assint√≥tica do estimador.
  Sob *Assumption 8.3* [^45], a consist√™ncia √© estabelecida [^47]. A distribui√ß√£o assint√≥tica √© dada por:
  $$\sqrt{T}(\hat{b} - \beta) \xrightarrow{d} N(0, \sigma^2Q^{-1})$$
  onde Q √© o limite da matriz de covari√¢ncia das vari√°veis explicativas normalizada (escalonada) e T representa o tamanho da amostra.
    
> üí° **Exemplo Num√©rico:** Suponha que temos um modelo AR(1) com $y_t = 0.6y_{t-1} + \epsilon_t$ e um conjunto de dados com T=1000.  Ao aplicar OLS, obtemos $\hat{b} = 0.62$. A matriz $Q$ √© um escalar neste caso (j√° que s√≥ temos um regressor). A vari√¢ncia do erro $\sigma^2$ √© estimada como 1. A estimativa de $Q$ √© dada por $\frac{1}{T}\sum_{t=2}^T y_{t-1}^2 \approx 1.5$. Ent√£o a vari√¢ncia assint√≥tica de $\hat{b}$ ser√° $\sigma^2/Q \approx 1/1.5 \approx 0.67$. Portanto, $\sqrt{1000}(\hat{b} - 0.6) \xrightarrow{d} N(0, 0.67)$. Isso significa que a estimativa est√° pr√≥xima do valor real e a incerteza diminui com o aumento da amostra.

  Os detalhes da deriva√ß√£o da distribui√ß√£o assint√≥tica (de acordo com [^57]) come√ßam por definir o estimador OLS de um modelo AR(p), e a partir da√≠ construir as propriedades assint√≥ticas do estimador. A deriva√ß√£o da distribui√ß√£o assint√≥tica de $\sqrt{T}(\hat{b}-\beta)$ (onde $\beta$ representa os par√¢metros do modelo AR(p)) e a vari√¢ncia assint√≥tica $\sigma^2Q^{-1}$ usa os conceitos da teoria de s√©ries temporais.

**Lema 1** (Matriz de covari√¢ncia): A matriz de covari√¢ncia $Q$ mencionada na distribui√ß√£o assint√≥tica do estimador OLS tem uma estrutura espec√≠fica para modelos AR(p). Dado que as vari√°veis explicativas s√£o os valores defasados da pr√≥pria s√©rie temporal, $Q$ est√° relacionada com as autocovari√¢ncias da s√©rie. Especificamente, se $\gamma_k = Cov(y_t, y_{t-k})$, ent√£o a matriz $Q$ √© formada pelos limites dessas autocovari√¢ncias normalizadas. A estrutura precisa de $Q$ pode ser expressa utilizando as autocovari√¢ncias amostrais. O entendimento desta matriz √© crucial para entender a vari√¢ncia dos estimadores, especialmente quando se calcula intervalos de confian√ßa.

**Prova do Lema 1:**
Para demonstrar como a matriz de covari√¢ncia $Q$ se relaciona com as autocovari√¢ncias da s√©rie temporal, precisamos estabelecer como ela surge do contexto do modelo AR(p) e como ela √© formada.

I.   Considere o modelo AR(p) em forma matricial: $y = X\beta + \epsilon$, onde $y$ √© o vetor das observa√ß√µes da s√©rie temporal, $X$ √© a matriz de regressores (valores defasados de $y$), $\beta$ √© o vetor de par√¢metros, e $\epsilon$ √© o vetor de erros.

II.  O estimador OLS √© dado por $\hat{\beta} = (X'X)^{-1}X'y$. A distribui√ß√£o assint√≥tica do estimador OLS √© expressa como $\sqrt{T}(\hat{\beta}-\beta) \xrightarrow{d} N(0, \sigma^2Q^{-1})$.

III.  A matriz $Q$ √© definida como o limite da matriz de covari√¢ncia das vari√°veis explicativas normalizada, ou seja, $Q = \lim_{T \to \infty} \frac{1}{T} X'X$.

IV. Para um modelo AR(p), as linhas da matriz X incluem vetores de valores defasados. Especificamente, cada linha da matriz $X$ ter√° a forma: $[y_{t-1}, y_{t-2}, \ldots, y_{t-p}]$. Portanto, a matriz $X'X$ incluir√° termos da forma $\sum_{t=1}^T y_{t-i}y_{t-j}$.

V. Quando normalizamos essa matriz por $1/T$, os elementos de $Q$ s√£o da forma $\frac{1}{T}\sum_{t=1}^T y_{t-i}y_{t-j}$, que, quando $T \to \infty$, convergem para a autocovari√¢ncia $\gamma_{|i-j|}$. Isso acontece sob a suposi√ß√£o de estacionaridade, que garante que essas autocovari√¢ncias convirjam para valores bem definidos.

VI. Assim, a matriz $Q$ √© composta por limites de autocovari√¢ncias normalizadas, onde o elemento (i,j) de $Q$ √© dado por $\gamma_{|i-j|}$. Ou seja, $Q$ √© uma matriz Toeplitz com autocovari√¢ncias.

VII. Logo, $Q$ √© uma matriz que representa a estrutura de autocovari√¢ncia das vari√°veis explicativas, com a seguinte estrutura para um modelo AR(p):

$$Q =  \begin{bmatrix}
\gamma_0 & \gamma_1 & \gamma_2 & \dots & \gamma_{p-1} \\
\gamma_1 & \gamma_0 & \gamma_1 & \dots & \gamma_{p-2} \\
\gamma_2 & \gamma_1 & \gamma_0 & \dots & \gamma_{p-3} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\gamma_{p-1} & \gamma_{p-2} & \gamma_{p-3} & \dots & \gamma_0
\end{bmatrix}$$

Essa estrutura demonstra como $Q$ se relaciona com as autocovari√¢ncias da s√©rie temporal, justificando a afirma√ß√£o do Lema 1. ‚ñ†
> üí° **Exemplo Num√©rico:** Considere um modelo AR(2). Suponha que as autocovari√¢ncias amostrais de uma s√©rie temporal sejam $\gamma_0=2$, $\gamma_1=1$, e $\gamma_2=0.5$. Ent√£o, a matriz $Q$ seria:
>
> $$Q = \begin{bmatrix}
> 2 & 1 \\
> 1 & 2
> \end{bmatrix}$$
>
> Esta matriz de covari√¢ncia, formada pelas autocovari√¢ncias, √© crucial para calcular a vari√¢ncia dos estimadores OLS no modelo AR(2), e reflete a depend√™ncia temporal da s√©rie.

**Teorema 1** (Yule-Walker): Os coeficientes de um modelo AR(p) estacion√°rio podem ser estimados por meio das equa√ß√µes de Yule-Walker, que relacionam os coeficientes autoregressivos √†s autocovari√¢ncias da s√©rie temporal. Essas equa√ß√µes fornecem um m√©todo alternativo para a estima√ß√£o de par√¢metros em modelos AR(p), que, em certas situa√ß√µes, pode apresentar vantagens computacionais ou de interpretabilidade. Al√©m disso, as equa√ß√µes de Yule-Walker fornecem uma base te√≥rica para o c√°lculo da matriz $Q$, demonstrando a sua conex√£o intr√≠nseca com a estrutura de autocovari√¢ncia da s√©rie.

**Prova do Teorema 1:**
Para demonstrar como as equa√ß√µes de Yule-Walker relacionam os coeficientes autoregressivos √†s autocovari√¢ncias da s√©rie temporal, vamos derivar essas equa√ß√µes a partir do modelo AR(p).

I.   Considere o modelo AR(p) estacion√°rio: $y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t$.

II.  Vamos assumir que $c=0$ para simplificar a prova, sem perda de generalidade (j√° que podemos trabalhar com valores desviados da m√©dia da s√©rie temporal). Multiplicando ambos os lados da equa√ß√£o por $y_{t-k}$ (onde $k=0, 1, 2, \ldots p$), temos:
    $$y_t y_{t-k} = \phi_1 y_{t-1}y_{t-k} + \phi_2 y_{t-2}y_{t-k} + \ldots + \phi_p y_{t-p}y_{t-k} + \epsilon_t y_{t-k}$$

III. Tomando as expectativas de ambos os lados da equa√ß√£o acima, e utilizando o fato de que $E[\epsilon_t y_{t-k}] = 0$ para $k > 0$, pois os erros s√£o independentes dos valores passados, temos:

    $$E[y_t y_{t-k}] = \phi_1 E[y_{t-1}y_{t-k}] + \phi_2 E[y_{t-2}y_{t-k}] + \ldots + \phi_p E[y_{t-p}y_{t-k}]$$

IV. Definindo as autocovari√¢ncias como $\gamma_k = E[y_t y_{t-k}]$, podemos reescrever a equa√ß√£o acima como:

    $$\gamma_k = \phi_1 \gamma_{k-1} + \phi_2 \gamma_{k-2} + \ldots + \phi_p \gamma_{k-p}$$

V. Para $k=1, 2, \ldots, p$, temos um sistema de *p* equa√ß√µes:
    \begin{align*}
    \gamma_1 &= \phi_1 \gamma_0 + \phi_2 \gamma_1 + \ldots + \phi_p \gamma_{p-1} \\
    \gamma_2 &= \phi_1 \gamma_1 + \phi_2 \gamma_0 + \ldots + \phi_p \gamma_{p-2} \\
    & \vdots \\
    \gamma_p &= \phi_1 \gamma_{p-1} + \phi_2 \gamma_{p-2} + \ldots + \phi_p \gamma_{0}
    \end{align*}

VI. Este sistema de *p* equa√ß√µes com *p* inc√≥gnitas ($\phi_1, \phi_2, \ldots, \phi_p$) pode ser resolvido para encontrar os coeficientes autoregressivos, dados os valores de $\gamma_0, \gamma_1, \ldots, \gamma_p$ (as autocovari√¢ncias).

VII. Este sistema de equa√ß√µes √© conhecido como as equa√ß√µes de Yule-Walker. As equa√ß√µes mostram uma rela√ß√£o direta entre os par√¢metros do modelo AR(p) e as autocovari√¢ncias da s√©rie temporal. Elas tamb√©m estabelecem uma base para a estima√ß√£o de $\beta$ a partir dos momentos da s√©rie temporal, fornecendo um m√©todo alternativo para estimar par√¢metros em modelos AR(p).

VIII. As equa√ß√µes de Yule-Walker tamb√©m estabelecem que $Q$ √© uma matriz formada pelas autocovari√¢ncias da s√©rie temporal. A estrutura precisa de $Q$ √© consequ√™ncia das rela√ß√µes entre as autocovari√¢ncias de $y$ em diferentes defasagens ($y_{t-1}$, $y_{t-2}$...).

Portanto, demonstramos que os coeficientes de um modelo AR(p) podem ser calculados pelas equa√ß√µes de Yule-Walker, que relacionam os par√¢metros autoregressivos √†s autocovari√¢ncias. ‚ñ†

> üí° **Exemplo Num√©rico:**  Vamos usar o exemplo anterior do modelo AR(2) com autocovari√¢ncias $\gamma_0=2$, $\gamma_1=1$, e $\gamma_2=0.5$. As equa√ß√µes de Yule-Walker para este modelo s√£o:
>
>   $\gamma_1 = \phi_1 \gamma_0 + \phi_2 \gamma_1$
>
>   $\gamma_2 = \phi_1 \gamma_1 + \phi_2 \gamma_0$
>
> Substituindo os valores, temos:
>
>  $1 = 2\phi_1 + 1\phi_2$
>  $0.5 = 1\phi_1 + 2\phi_2$
>
> Resolvendo esse sistema, multiplicando a segunda equa√ß√£o por 2 e subtraindo da primeira, temos: $1-1=2\phi_1 + \phi_2 - (2\phi_1 + 4\phi_2)$, resultando em $0 = -3\phi_2$, portanto, $\phi_2 = 0$. Substituindo na primeira equa√ß√£o, obtemos $1 = 2\phi_1$, ent√£o $\phi_1 = 0.5$. Portanto, as equa√ß√µes de Yule-Walker nos fornecem os par√¢metros do modelo AR(2) $\phi_1 = 0.5$ e $\phi_2 = 0$.
>
> ```python
> import numpy as np
> # Autocovari√¢ncias
> gamma0 = 2
> gamma1 = 1
> gamma2 = 0.5
>
> # Montando o sistema de equa√ß√µes de Yule-Walker
> A = np.array([[gamma0, gamma1],
>               [gamma1, gamma0]])
> b = np.array([gamma1, gamma2])
>
> # Resolvendo o sistema
> phi = np.linalg.solve(A, b)
> print(f"phi1: {phi[0]}, phi2: {phi[1]}")
> ```
> Este exemplo mostra como as autocovari√¢ncias podem ser usadas para obter diretamente os coeficientes do modelo AR(2).

### Conclus√£o
Em resumo, a estimativa de par√¢metros em modelos AR(p) por OLS requer cautela devido √† correla√ß√£o entre os regressores e os erros, o que invalida as propriedades de amostras pequenas. No entanto, sob condi√ß√µes adequadas, a consist√™ncia dos estimadores e as propriedades assint√≥ticas s√£o garantidas, permitindo infer√™ncias v√°lidas com amostras grandes. A an√°lise assint√≥tica, por meio de aproxima√ß√µes estat√≠sticas, permite generalizar os resultados para grandes amostras, mesmo sob complexidades inerentes aos modelos autoregressivos. A compreens√£o dessas nuances √© essencial para uma aplica√ß√£o precisa e robusta dos modelos AR(p) na an√°lise de s√©ries temporais. Modelos de autocorrela√ß√£o s√£o discutidos mais adiante [^66], mas √© importante notar que o m√©todo de m√°xima verossimilhan√ßa e a abordagem de m√≠nimos quadrados generalizados s√£o alternativas que podem ser usadas para encontrar solu√ß√µes de forma mais robusta e precisa.

### Refer√™ncias
[^1]:  "Este cap√≠tulo se aprofunda na an√°lise de modelos de regress√£o, focando especificamente na estima√ß√£o de par√¢metros em modelos autoregressivos de ordem p (AR(p))."
[^2]:  "Uma peculiaridade desses modelos √© que, embora sejam tratados como problemas de regress√£o, as propriedades de pequenas amostras, derivadas no contexto de regress√£o linear padr√£o, n√£o se sustentam devido √† correla√ß√£o entre o regressor e o termo de erro."
[^43]: "Assumption 8.4: The regression model is
  $y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t$"
[^44]: "The distribution of test statistics for this case can be found by a two-step procedure. The first step evaluates the distribution conditional on X; that is, it treats X as deterministic just as in the earlier analysis. The second step multiplies by the density of X and integrates over X to find the true unconditional distribution."
[^45]: "Assumption 8.3: (a) x, stochastic and independent of us for all t, s; (b) u, non-
Gaussian but i.i.d. with mean zero, variance o¬≤, and E(u‚Å¥) = ¬µ‚ÇÑ < ‚àû; (c) E(x‚Çúx‚Çú') = Q‚Çú, a positive definite matrix with (1/T)‚àëQ‚Çú‚Üí Q, a positive definite matrix; (d) E(x·µ¢‚Çúx‚±º‚Çúx‚Çó‚Çúx‚Çò‚Çú) < ‚àû for all i, j, l, m, and t; (e) (1/T)‚àëx‚Çúx‚Çú' ‚Üí Q."
[^46]: "To justify the usual OLS inference rules, we have to appeal to asymptotic results, for which purpose Assumption 8.3 includes conditions (c) through (e)."
[^47]: "To describe the asymptotic results, ... We first establish that the OLS coefficient estimator is consistent under Assumption 8.3, that is, that bT‚ÜíŒ≤."
[^56]: "The OLS coefficient estimate b is given by [8.1.6]:
 $b = (X'X)^{-1}X'y$ "
[^57]: "Next turn to the asymptotic distribution of b. Notice from [8.2.3] that:
  $‚àöT(b_T - \beta) = [(1/T) \sum_{t=1}^T x_tx_t']^{-1} [(1/ \sqrt{T}) \sum_{t=1}^T x_t u_t ]$"
[^66]: "Many of the preceding results about serially correlated errors no longer hold if the regression contains lagged endogenous variables."
<!-- END -->
