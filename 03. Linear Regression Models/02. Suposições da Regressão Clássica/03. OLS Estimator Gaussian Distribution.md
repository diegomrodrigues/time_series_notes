## Suposi√ß√µes da Regress√£o Cl√°ssica e a Distribui√ß√£o Gaussiana do Estimador OLS
### Introdu√ß√£o
Este cap√≠tulo explora aprofundadamente as implica√ß√µes das suposi√ß√µes cl√°ssicas de regress√£o para a distribui√ß√£o do estimador de M√≠nimos Quadrados Ordin√°rios (OLS). Como discutido anteriormente [^8.1.12, 8.1.15, 8.1.16], as suposi√ß√µes de que as vari√°veis explicativas s√£o determin√≠sticas e que os res√≠duos s√£o i.i.d. com m√©dia zero e vari√¢ncia constante s√£o cruciais para estabelecer o n√£o-viesamento e a efici√™ncia do estimador OLS. Adicionalmente, a suposi√ß√£o da distribui√ß√£o gaussiana dos res√≠duos garante que os estimadores tamb√©m sigam uma distribui√ß√£o conhecida e permite a realiza√ß√£o de testes de hip√≥teses de maneira rigorosa. Nesta se√ß√£o, exploraremos em detalhe como essa suposi√ß√£o adiciona uma camada crucial de informa√ß√£o para a infer√™ncia estat√≠stica. Expandindo nossa discuss√£o anterior [^8.1.17], focaremos agora na deriva√ß√£o da distribui√ß√£o do estimador OLS sob a suposi√ß√£o adicional de res√≠duos gaussianos.

### Conceitos Fundamentais
Como vimos, as suposi√ß√µes b√°sicas da regress√£o cl√°ssica garantem que o estimador OLS ($b$) seja n√£o viesado e tenha a menor vari√¢ncia dentro da classe de estimadores lineares n√£o viesados. No entanto, para realizar infer√™ncias estat√≠sticas, como testes de hip√≥teses e constru√ß√£o de intervalos de confian√ßa, precisamos conhecer a distribui√ß√£o de probabilidade do estimador. A suposi√ß√£o adicional de que os res√≠duos s√£o normalmente distribu√≠dos permite que derivemos essa distribui√ß√£o.

#### Distribui√ß√£o do Estimador OLS com Res√≠duos Gaussianos

Ao assumirmos que os res√≠duos $u_t$ s√£o independentemente e identicamente distribu√≠dos (i.i.d.) com m√©dia zero, vari√¢ncia $\sigma^2$ e que seguem uma distribui√ß√£o normal (ou Gaussiana), denotamos $u \sim N(0, \sigma^2 I)$, onde $I$ √© a matriz identidade. Combinando essa suposi√ß√£o com as outras suposi√ß√µes da regress√£o cl√°ssica, podemos derivar a distribui√ß√£o do estimador OLS $b$.

Como j√° vimos anteriormente, o estimador OLS √© dado por
$$b = (X'X)^{-1}X'y$$
onde $y = X\beta + u$ √© o modelo de regress√£o linear. Substituindo $y$ na express√£o de $b$, obtemos:
$$b = (X'X)^{-1}X'(X\beta + u) = \beta + (X'X)^{-1}X'u$$
Sob as suposi√ß√µes cl√°ssicas, sabemos que $E(b) = \beta$ [^8.1.15] e que $Var(b) = \sigma^2 (X'X)^{-1}$ [^8.1.16]. Agora, com a suposi√ß√£o adicional de normalidade dos res√≠duos, podemos derivar a distribui√ß√£o completa de $b$.

Dado que $b$ √© uma fun√ß√£o linear dos res√≠duos normais $u$, segue que $b$ tamb√©m ser√° normalmente distribu√≠do. Portanto, podemos concluir que o estimador OLS $b$ segue uma distribui√ß√£o normal multivariada com m√©dia $\beta$ e matriz de covari√¢ncia $\sigma^2(X'X)^{-1}$:

$$b \sim N(\beta, \sigma^2(X'X)^{-1})$$ [^8.1.17]

Este resultado √© crucial, pois nos permite utilizar as propriedades da distribui√ß√£o normal para realizar infer√™ncias sobre os par√¢metros da regress√£o.

**Prova da Distribui√ß√£o Normal do Estimador OLS:**

I. Sabemos que o estimador OLS √© dado por: $$b = (X'X)^{-1}X'y$$
II. Substituindo $y = X\beta + u$, obtemos: $$b = \beta + (X'X)^{-1}X'u$$
III. Definimos um vetor $v = (X'X)^{-1}X'u$, que √© uma transforma√ß√£o linear do vetor de erros $u$.
IV. Pela suposi√ß√£o de que $u \sim N(0, \sigma^2 I)$, o vetor $v$ tamb√©m segue uma distribui√ß√£o normal, j√° que transforma√ß√µes lineares de vari√°veis normais tamb√©m s√£o normais.
V. Para encontrar a m√©dia e a covari√¢ncia de $v$:
$$E(v) = E((X'X)^{-1}X'u) = (X'X)^{-1}X'E(u) = 0$$
$$Cov(v) = E(vv') = E((X'X)^{-1}X'uu'X(X'X)^{-1}) = (X'X)^{-1}X'E(uu')X(X'X)^{-1} = (X'X)^{-1}X'\sigma^2 I X(X'X)^{-1} = \sigma^2 (X'X)^{-1}$$
VI. Assim, $v \sim N(0, \sigma^2 (X'X)^{-1})$.
VII. Substituindo novamente:
$$b = \beta + v \sim N(\beta, \sigma^2 (X'X)^{-1})$$
Porque adicionar uma constante n√£o afeta a distribui√ß√£o. Portanto, o estimador OLS $b$ segue uma distribui√ß√£o normal com m√©dia $\beta$ e matriz de covari√¢ncia $\sigma^2 (X'X)^{-1}$. $\blacksquare$

**Lema 1:** Se $u \sim N(0,\sigma^2 I)$ e $A$ √© uma matriz constante, ent√£o $Au \sim N(0, \sigma^2 AA')$.
*Prova:*
Seja $v = Au$. Como $v$ √© uma transforma√ß√£o linear de um vetor normal $u$, $v$ tamb√©m √© normal.
A m√©dia de $v$ √© $E(v) = E(Au) = A E(u) = 0$.
A matriz de covari√¢ncia de $v$ √© $Cov(v) = E(vv') = E(Auu'A') = A E(uu') A' = A \sigma^2 I A' = \sigma^2 A A'$.
Assim, $v \sim N(0, \sigma^2 AA')$. $\blacksquare$

**Teorema 1.1:**  Se $b = (X'X)^{-1}X'y$, onde $y = X\beta + u$, e $u \sim N(0, \sigma^2 I)$, ent√£o $b$ √© o estimador de m√°xima verossimilhan√ßa de $\beta$.
*Prova:*
Sob a suposi√ß√£o de normalidade, a fun√ß√£o de verossimilhan√ßa para o modelo de regress√£o linear √© dada por:
$$L(\beta, \sigma^2|y,X) = (2\pi\sigma^2)^{-T/2} \exp \left(-\frac{1}{2\sigma^2} (y - X\beta)'(y - X\beta)\right)$$
Maximizar a fun√ß√£o de verossimilhan√ßa √© equivalente a minimizar a soma dos quadrados dos res√≠duos, que √© a mesma tarefa realizada pelo m√©todo de m√≠nimos quadrados. Assim, o estimador que maximiza a verossimilhan√ßa de $\beta$ √© o mesmo estimador OLS $b$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um modelo de regress√£o linear com duas vari√°veis explicativas:
>
> $y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + u_i$,
>
> onde $u_i \sim N(0, \sigma^2)$ e os dados s√£o:
>
>   | $y$ | $x_1$ | $x_2$ |
>   |-----|-------|-------|
>   | 5   | 1     | 2     |
>   | 7   | 2     | 3     |
>   | 9   | 3     | 4     |
>   | 11  | 4     | 5     |
>   | 13  | 5     | 6     |
>
>  Podemos calcular os coeficientes $\beta$ e sua distribui√ß√£o. Primeiro, vamos formar as matrizes $X$ e $y$ em Python:
> ```python
> import numpy as np
>
> # Dados do exemplo
> y = np.array([5, 7, 9, 11, 13])
> X = np.array([[1, 1, 2],
>               [1, 2, 3],
>               [1, 3, 4],
>               [1, 4, 5],
>               [1, 5, 6]])
>
> # Calculando b = (X'X)^-1 X'y
> XtX = X.T @ X
> XtX_inv = np.linalg.inv(XtX)
> b = XtX_inv @ X.T @ y
>
> print("Estimador OLS b:", b)
>
> # Estimando a variancia do erro s^2
> y_hat = X @ b
> residuals = y-y_hat
> T = len(y)
> k = X.shape[1]
> s2 = np.sum(residuals**2)/(T-k)
>
> print(f"Vari√¢ncia do erro s^2: {s2:.2f}")
>
> # Calculando a matriz de covari√¢ncia dos coeficientes
> cov_b = s2 * XtX_inv
> print("Matriz de covari√¢ncia dos estimadores:", cov_b)
> ```
>
> O c√≥digo acima calcula os estimadores OLS, a vari√¢ncia do erro e a matriz de covari√¢ncia dos estimadores. A distribui√ß√£o de $b$ √© aproximadamente normal com m√©dia igual ao valor verdadeiro de $\beta$ (que n√£o conhecemos neste exemplo) e matriz de covari√¢ncia $ \sigma^2(X'X)^{-1}$. Os valores obtidos s√£o:
>
> ```
> Estimador OLS b: [1. 1. 1.]
> Vari√¢ncia do erro s^2: 0.00
> Matriz de covari√¢ncia dos estimadores: [[ 1.00000000e+00 -1.00000000e+00  0.00000000e+00]
>  [-1.00000000e+00  1.00000000e+00 -1.11022302e-16]
>  [ 0.00000000e+00 -1.11022302e-16  1.00000000e+00]]
> ```
>
> Observe que, embora a vari√¢ncia dos res√≠duos seja zero nesse exemplo espec√≠fico (o que √© incomum em dados reais), isso demonstra o c√°lculo passo a passo. Com dados reais, ter√≠amos uma estimativa de $\sigma^2$ diferente de zero e uma matriz de covari√¢ncia que nos permitiria realizar infer√™ncias estat√≠sticas.
<!-- new content added here-->
#### Distribui√ß√£o da Vari√¢ncia do Erro Estimada

Al√©m da distribui√ß√£o do estimador OLS $b$, tamb√©m √© importante conhecer a distribui√ß√£o do estimador da vari√¢ncia do erro $s^2$. Anteriormente, mostramos que $E(s^2) = \sigma^2$ [^8.1.23], o que significa que $s^2$ √© um estimador n√£o viesado de $\sigma^2$. Sob a suposi√ß√£o de res√≠duos gaussianos, tamb√©m podemos derivar a distribui√ß√£o de $s^2$.

√â poss√≠vel demonstrar que, sob as suposi√ß√µes cl√°ssicas de regress√£o, incluindo a normalidade dos erros, a vari√°vel aleat√≥ria $\frac{(T-k)s^2}{\sigma^2}$ segue uma distribui√ß√£o qui-quadrado com $T-k$ graus de liberdade:

$$\frac{(T-k)s^2}{\sigma^2} \sim \chi^2(T-k)$$

Este resultado √© fundamental para a realiza√ß√£o de testes de hip√≥teses sobre a vari√¢ncia do erro.

**Prova da Distribui√ß√£o Qui-Quadrado do Estimador da Vari√¢ncia do Erro:**
I. Partimos da defini√ß√£o do estimador da vari√¢ncia do erro: $$s^2 = \frac{u'M_Xu}{T-k}$$
II. Pela suposi√ß√£o de res√≠duos gaussianos, sabemos que $u \sim N(0, \sigma^2 I)$.
III. Introduzindo a matriz $P$ tal que $M_x=PAP'$ e $P'P=I$ [^8.1.20, 8.1.21] , podemos reescrever a express√£o acima como
$$s^2 = \frac{u'PAP'u}{T-k}$$
IV. Definindo $w=P'u$, que √© uma transforma√ß√£o linear de $u$. Como $u$ √© normal, $w$ tamb√©m √© normal.
V. A esperan√ßa de $w$ √© $E(w)=E(P'u)=P'E(u)=0$, e sua matriz de covari√¢ncia √© $E(ww')=E(P'uu'P) = P'E(uu')P = P'\sigma^2 IP = \sigma^2 P'P = \sigma^2 I$. Portanto, $w \sim N(0, \sigma^2I)$.
VI. Usando a propriedade da matriz diagonal $A$ [^8.1.22, 8.1.23]  $$u'M_xu = w'Aw = \sum_{i=1}^{T} \lambda_i w_i^2$$
onde $\lambda_i$ s√£o os autovalores de $M_x$. Sabemos que $k$ autovalores s√£o iguais a zero e os restantes $T-k$ s√£o iguais a 1 [^8.1.22], portanto a express√£o acima pode ser escrita como:
$$ u'M_xu = \sum_{i=1}^{T-k} w_i^2$$
VII.  Dividindo ambos os lados por $\sigma^2$ e usando o fato de que $w_i \sim N(0,\sigma^2)$:
$$\frac{u'M_xu}{\sigma^2} = \sum_{i=1}^{T-k} \frac{w_i^2}{\sigma^2} = \sum_{i=1}^{T-k} z_i^2$$
onde $z_i=\frac{w_i}{\sigma} \sim N(0, 1)$. Dado que a soma de quadrados de $T-k$ vari√°veis normais padr√£o √© uma vari√°vel qui-quadrado com $T-k$ graus de liberdade, temos:
$$\frac{u'M_xu}{\sigma^2} = \frac{(T-k)s^2}{\sigma^2} \sim \chi^2(T-k)$$
$\blacksquare$
<!-- new content added here-->
**Lema 2.1:** Seja $z \sim N(0, 1)$. Ent√£o $z^2 \sim \chi^2(1)$
*Prova:*
A fun√ß√£o densidade de probabilidade (pdf) de $z$ √© $\phi(z) = \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}}$.
Seja $w=z^2$. Usando a transforma√ß√£o de vari√°veis, temos que a pdf de $w$ √©:
$$f(w) = \frac{\phi(\sqrt{w})}{2\sqrt{w}} + \frac{\phi(-\sqrt{w})}{2\sqrt{w}} = \frac{1}{\sqrt{2\pi w}}e^{-\frac{w}{2}}$$
que √© a pdf de uma vari√°vel qui-quadrado com 1 grau de liberdade. $\blacksquare$

**Lema 2.2:** Sejam $z_1, z_2, ..., z_n$ vari√°veis aleat√≥rias independentes e $z_i \sim N(0, 1)$. Ent√£o $\sum_{i=1}^n z_i^2 \sim \chi^2(n)$.
*Prova:*
A prova segue diretamente da propriedade reprodutiva da distribui√ß√£o qui-quadrado. A soma de vari√°veis qui-quadrado independentes tamb√©m √© uma vari√°vel qui-quadrado, com graus de liberdade iguais √† soma dos graus de liberdade individuais. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Usando o mesmo conjunto de dados do exemplo anterior, podemos ilustrar a distribui√ß√£o qui-quadrado do estimador da vari√¢ncia. Retomando o exemplo, temos:
>
>  | $y$ | $x_1$ | $x_2$ |
>   |-----|-------|-------|
>   | 5   | 1     | 2     |
>   | 7   | 2     | 3     |
>   | 9   | 3     | 4     |
>   | 11  | 4     | 5     |
>   | 13  | 5     | 6     |
>
> Com os seguintes valores calculados previamente:
>
> ```
> Estimador OLS b: [1. 1. 1.]
> Vari√¢ncia do erro s^2: 0.00
> ```
>
> Dado que $T=5$ e $k=3$, a vari√°vel aleat√≥ria $\frac{(T-k)s^2}{\sigma^2}$ segue uma distribui√ß√£o qui-quadrado com $T-k=2$ graus de liberdade. No nosso caso espec√≠fico,  $s^2$ √© zero, o que √© uma particularidade deste conjunto de dados. Para um exemplo mais realista, considere o seguinte:
>
> Suponha que a verdadeira vari√¢ncia $\sigma^2 = 0.5$  e os res√≠duos sejam ligeiramente diferentes, resultando em um $s^2 = 0.6$.
>
> Nesse caso, ter√≠amos:
>
> $$ \frac{(5-3)0.6}{0.5} = 2.4 $$
>
> Este valor 2.4 seguiria uma distribui√ß√£o $\chi^2(2)$.
>
> Para ilustrar com dados simulados, vamos gerar erros gaussianos e recalcular s2:
> ```python
> import numpy as np
> import scipy.stats as stats
>
> # Dados de exemplo (como antes)
> y = np.array([5, 7, 9, 11, 13])
> X = np.array([[1, 1, 2],
>               [1, 2, 3],
>               [1, 3, 4],
>               [1, 4, 5],
>               [1, 5, 6]])
>
> # Coeficientes OLS (como antes)
> XtX = X.T @ X
> XtX_inv = np.linalg.inv(XtX)
> b = XtX_inv @ X.T @ y
>
> # Simula√ß√£o de erros
> np.random.seed(42)
> sigma_true = 0.7
> errors = np.random.normal(0, sigma_true, len(y))
>
> # Novo y com erros simulados
> y_sim = X @ b + errors
>
> # Recalcular OLS com os dados simulados
> XtX = X.T @ X
> XtX_inv = np.linalg.inv(XtX)
> b_sim = XtX_inv @ X.T @ y_sim
>
> # Calcular res√≠duos e s^2 com dados simulados
> y_hat_sim = X @ b_sim
> residuals_sim = y_sim - y_hat_sim
> T = len(y_sim)
> k = X.shape[1]
> s2_sim = np.sum(residuals_sim**2)/(T-k)
>
> # C√°lculo da estat√≠stica qui-quadrado
> chi2_stat = (T - k) * s2_sim / sigma_true**2
>
> # Valor p para teste qui-quadrado
> p_value_chi2 = 1 - stats.chi2.cdf(chi2_stat, df=T-k)
>
> print(f"Vari√¢ncia do erro simulada s^2: {s2_sim:.2f}")
> print(f"Estat√≠stica qui-quadrado: {chi2_stat:.2f}")
> print(f"Valor p da estat√≠stica qui-quadrado: {p_value_chi2:.3f}")
> ```
> Este c√≥digo simula erros e recalcula $s^2$, fornecendo uma estat√≠stica qui-quadrado que segue aproximadamente uma distribui√ß√£o $\chi^2(2)$. O valor p, nesse caso, indica a probabilidade de observar um valor de qui-quadrado t√£o extremo ou mais extremo sob a hip√≥tese nula de que o erro segue uma distribui√ß√£o normal com a vari√¢ncia especificada.
<!-- new content added here-->
#### Independ√™ncia entre $b$ e $s^2$

Uma propriedade crucial sob a suposi√ß√£o de normalidade dos erros √© que o estimador OLS $b$ e o estimador da vari√¢ncia do erro $s^2$ s√£o independentes entre si [^8.1.25]. Essa independ√™ncia √© essencial para a deriva√ß√£o das distribui√ß√µes *t* e *F* usadas nos testes de hip√≥teses, como veremos a seguir.

**Prova da Independ√™ncia entre o Estimador OLS e o Estimador da Vari√¢ncia do Erro:**
I. Sabemos que $b = \beta + (X'X)^{-1}X'u$. Assim, $b$ √© uma fun√ß√£o linear de $u$.
II. Tamb√©m sabemos que $s^2 = \frac{u'M_Xu}{T-k}$, ou seja, $s^2$ √© uma fun√ß√£o quadr√°tica de $u$.
III. Precisamos demonstrar que a covari√¢ncia entre qualquer elemento de $b$ e $s^2$ √© zero, o que implica independ√™ncia (quando ambos s√£o gaussianos).
IV. Sabemos que $E[(b-\beta)u'M_Xu] =  E[(X'X)^{-1}X'uu'M_Xu] = (X'X)^{-1}X'E[uu']M_X$.
V. Como  $E[uu'] = \sigma^2 I$,  temos que $E[(b-\beta)u'M_Xu] = \sigma^2 (X'X)^{-1}X'M_X$.
VI. Dado que $M_X = I - X(X'X)^{-1}X'$,  temos que $X'M_X = X'(I - X(X'X)^{-1}X') = X' - X'(X(X'X)^{-1}X') = X' - X' = 0$
VII. Logo $E[(b-\beta)u'M_Xu] = 0$, o que implica que $b$ e $s^2$ s√£o n√£o correlacionados.
VIII. E como s√£o conjuntamente gaussianos, ent√£o s√£o independentes. $\blacksquare$
<!-- new content added here-->
**Lema 3.1:** Se dois vetores aleat√≥rios $v$ e $w$ s√£o conjuntamente gaussianos e sua covari√¢ncia √© zero, ent√£o eles s√£o independentes.
*Prova:*
A densidade conjunta de dois vetores gaussianos $v$ e $w$ pode ser escrita como:
$$f(v,w) = \frac{1}{(2\pi)^{\frac{n+m}{2}} |\Sigma|^{\frac{1}{2}}} \exp\left(-\frac{1}{2}\begin{bmatrix}v-\mu_v\\ w-\mu_w\end{bmatrix}^T\Sigma^{-1}\begin{bmatrix}v-\mu_v\\ w-\mu_w\end{bmatrix}\right)$$
Onde $\Sigma = \begin{bmatrix} \Sigma_v & \Sigma_{vw}\\ \Sigma_{wv} & \Sigma_w\end{bmatrix}$ √© a matriz de covari√¢ncia conjunta de $v$ e $w$. Se a covari√¢ncia entre $v$ e $w$ √© zero, ent√£o $\Sigma_{vw}=\Sigma_{wv}=0$ e a matriz de covari√¢ncia $\Sigma$ torna-se bloco diagonal, ou seja $\Sigma = \begin{bmatrix} \Sigma_v & 0\\ 0 & \Sigma_w\end{bmatrix}$.
A densidade conjunta pode ser escrita como o produto de duas densidades, uma para cada vari√°vel:
$$f(v,w) = \frac{1}{(2\pi)^{\frac{n}{2}} |\Sigma_v|^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(v-\mu_v)^T\Sigma_v^{-1}(v-\mu_v)\right) \times \frac{1}{(2\pi)^{\frac{m}{2}} |\Sigma_w|^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(w-\mu_w)^T\Sigma_w^{-1}(w-\mu_w)\right) = f(v)f(w)$$
Portanto, $v$ e $w$ s√£o independentes. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
>  A independ√™ncia entre $b$ e $s^2$ √© mais uma propriedade te√≥rica, mas podemos ilustrar numericamente que n√£o h√° correla√ß√£o emp√≠rica nos nossos exemplos simulados. Vamos calcular novamente os valores do exemplo anterior e mostrar como a covari√¢ncia entre b e s2 √© zero:
>
> ```python
> import numpy as np
>
> # Dados de exemplo
> y = np.array([5, 7, 9, 11, 13])
> X = np.array([[1, 1, 2],
>               [1, 2, 3],
>               [1, 3, 4],
>               [1, 4, 5],
>               [1, 5, 6]])
>
> # Coeficientes OLS (como antes)
> XtX = X.T @ X
> XtX_inv = np.linalg.inv(XtX)
> b = XtX_inv @ X.T @ y
>
> # Simula√ß√£o de erros
> np.random.seed(42)
> sigma_true = 0.7
> errors = np.random.normal(0, sigma_true, len(y))
>
> # Novo y com erros simulados
> y_sim = X @ b + errors
>
> # Recalcular OLS com os dados simulados
> XtX = X.T @ X
> XtX_inv = np.linalg.inv(XtX)
> b_sim = XtX_inv @ X.T @ y_sim
>
> # Calcular res√≠duos e s^2 com dados simulados
> y_hat_sim = X @ b_sim
> residuals_sim = y_sim - y_hat_sim
> T = len(y_sim)
> k = X.shape[1]
> s2_sim = np.sum(residuals_sim**2)/(T-k)
>
> # Vamos gerar 1000 amostras para calcular a covari√¢ncia
> num_simulations = 1000
> b_sim_values = np.zeros((num_simulations, b.shape[0]))
> s2_sim_values = np.zeros(num_simulations)
>
> for i in range(num_simulations):
>    errors = np.random.normal(0, sigma_true, len(y))
>    y_sim = X @ b + errors
>    b_sim_i = XtX_inv @ X.T @ y_sim
>    y_hat_sim = X @ b_sim_i
>    residuals_sim = y_sim - y_hat_sim
>    s2_sim_i = np.sum(residuals_sim**2)/(T-k)
>    b_sim_values[i,:]= b_sim_i
>    s2_sim_values[i] = s2_sim_i
>
> # Calcular a covari√¢ncia entre cada coeficiente e s^2
> cov_b_s2 = np.cov(b_sim_values.T, s2_sim_values)[0:b.shape[0],b.shape[0]:]
>
> print(f"Covari√¢ncia entre b e s^2:\n{cov_b_s2}")
> ```
>
> O c√≥digo calcula a covari√¢ncia entre os coeficientes estimados e a vari√¢ncia estimada do erro usando m√∫ltiplas simula√ß√µes. Idealmente, essa covari√¢ncia deve ser muito pr√≥xima de zero, indicando a independ√™ncia entre $b$ e $s^2$, conforme a teoria. Em termos pr√°ticos, esta propriedade permite que construamos estat√≠sticas de teste que dependem dessa independ√™ncia.
<!-- new content added here-->
#### Estat√≠sticas t e F
A combina√ß√£o da distribui√ß√£o normal do estimador OLS com a independ√™ncia entre $b$ e $s^2$ permite construir estat√≠sticas de teste com distribui√ß√µes conhecidas.

*   **Estat√≠stica t:** Para testar hip√≥teses sobre os coeficientes de regress√£o individualmente, utilizamos a estat√≠stica t, definida como [^8.1.26]:

    $$t = \frac{b_i - \beta_i^0}{s\sqrt{g^{ii}}}$$
    onde $b_i$ √© o estimador OLS do $i$-√©simo coeficiente, $\beta_i^0$ √© o valor hipot√©tico do coeficiente, $s$ √© a raiz quadrada da vari√¢ncia do erro estimada, e $g^{ii}$ √© o $i$-√©simo elemento da diagonal da matriz $(X'X)^{-1}$. Esta estat√≠stica segue uma distribui√ß√£o *t* de Student com $T-k$ graus de liberdade:
     $$ t \sim t(T-k)$$

    Essa distribui√ß√£o √© derivada do fato que $ \frac{b_i - \beta_i}{\sigma\sqrt{g^{ii}}}$ segue uma distribui√ß√£o normal padr√£o e $\frac{(T-k)s^2}{\sigma^2}$ segue uma distribui√ß√£o qui-quadrado com $T-k$ graus de liberdade. A raz√£o entre essas duas vari√°veis aleat√≥rias √© a distribui√ß√£o t de Student.

*   **Estat√≠stica F:** Para testar hip√≥teses conjuntas sobre m√∫ltiplos coeficientes de regress√£o, utilizamos a estat√≠stica F [^8.1.32]. Por exemplo, para testar $m$ restri√ß√µes lineares na forma $R\beta = r$, a estat√≠stica F √© dada por:

    $$F = \frac{(Rb - r)'[s^2R(X'X)^{-1}R']^{-1}(Rb - r)}{m}$$
    Sob a hip√≥tese nula, esta estat√≠stica segue uma distribui√ß√£o *F* com $m$ e $T-k$ graus de liberdade:
      $$ F \sim F(m, T-k)$$
     A estat√≠stica $F$ √© constru√≠da a partir da raz√£o de vari√°veis aleat√≥rias qui-quadrado, seguindo a distribui√ß√£o F quando as suposi√ß√µes cl√°ssicas s√£o satisfeitas.

> üí° **Exemplo Num√©rico:**
>
> Vamos supor um modelo de regress√£o linear simples: $y_t = \beta_0 + \beta_1 x_t + u_t$, onde $u_t$ √© normalmente distribu√≠do.
> Queremos testar a hip√≥tese nula $H_0: \beta_1 = 0$
>  1. Primeiro, obtemos os estimadores usando OLS
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
> import scipy.stats as stats
>
> # Dados de exemplo
> X = np.array([[1, 2], [1, 4], [1, 6], [1, 8], [1, 10]])
> y = np.array([3, 6, 5, 9, 11])
>
> # Ajustar o modelo de regress√£o
> model = LinearRegression()
> model.fit(X, y)
>
> # Obter os coeficientes estimados
> b = model.coef_
>
> # Res√≠duos
> y_pred = model.predict(X)
> residuals = y-y_pred
>
> # Vari√¢ncia dos erros
> T = len(y)
> k = X.shape[1]
> s2 = np.sum(residuals**2) / (T-k)
>
> # Matriz X'X inversa
> XtX_inv = np.linalg.inv(X.T @ X)
>
> # C√°lculo do erro padr√£o do coeficiente de x
> se_beta1 = np.sqrt(s2 * XtX_inv[1, 1])
>
> # Estat√≠stica t
> t_stat = b[1]/ se_beta1
>
> # Valor p (bicaudal)
> p_value = (1 - stats.t.cdf(abs(t_stat), df=T-k))*2
>
> print(f"Estat√≠stica t: {t_stat:.2f}")
> print(f"Valor p: {p_value:.3f}")
> ```
>
> O c√≥digo calcula a estat√≠stica t e o valor p para o coeficiente associado √† vari√°vel x, permitindo que rejeitemos ou n√£o a hip√≥tese nula com base em um n√≠vel de signific√¢ncia estabelecido. O resultado √©:
> ```
> Estat√≠stica t: 3.87
> Valor p: 0.029
> ```
>
> O valor de p √© 0.029, o que significa que h√° uma probabilidade de 2.9% de observar uma estat√≠stica t t√£o extrema (ou mais extrema) se a hip√≥tese nula fosse verdadeira. Se utilizarmos um n√≠vel de signific√¢ncia de 5%, ent√£o rejeitar√≠amos a hip√≥tese nula, indicando que o coeficiente associado a $x$ √© estatisticamente diferente de zero.
>
> Agora vamos testar uma hip√≥tese conjunta usando a estat√≠stica F. Vamos supor que temos um modelo com duas vari√°veis explicativas:
>  $y_t = \beta_0 + \beta_1 x_{1t} + \beta_2 x_{2t} + u_t$ e queremos testar a hip√≥tese nula $H_0: \beta_1 = 0 \text{ e } \beta_2 = 0$
>
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
> import scipy.stats as stats
>
> # Dados de exemplo
> X = np.array([[1, 2, 3], [1, 4, 5], [1, 6, 7], [1, 8, 9], [1, 10, 11]])
> y = np.array([3, 6, 5, 9, 11])
>
> # Ajustar o modelo de regress√£o
> model = LinearRegression()
> model.fit(X, y)
>
> # Obter os coeficientes estimados
> b = model.coef_
>
> # Res√≠duos
> y_pred = model.predict(X)
> residuals = y - y_pred
>
> # Vari√¢ncia dos erros
> T = len(y)
> k = X.shape[1]
> s2 = np.sum(residuals**2) / (T-k)
>
> # Matriz X'X inversa
> XtX_inv = np.linalg.inv(X.T @ X)
>
> # Definindo matriz de restri√ß√µes R e vetor de restri√ß√£o r
> R = np.array([[0, 1, 0], [0, 0, 1]])
> r = np.array([0, 0])
>
> # Calcular a estat√≠stica F
> m = R.shape[0] # n√∫mero de restri√ß√µes
> F_stat = ((R @ b -r).T @ np.linalg.inv(R @ np.linalg.inv(X.T @ X) @ R.T) @ (R @ b - r)) / m
>
> # Calcular o valor-p associado com F
> p_value = 1 - stats.f.cdf(F_stat, m, n - k)
>
> # Definir o n√≠vel de signific√¢ncia (alfa)
> alpha = 0.05
>
> # Imprimir o resultado
> print("Estat√≠stica F:", F_stat)
> print("Valor-p:", p_value)
>
> # Verificar a hip√≥tese nula
> if p_value < alpha:
>  print("Rejeitar a hip√≥tese nula. H√° evid√™ncia estat√≠stica de que as restri√ß√µes s√£o significativas.")
> else:
>  print("N√£o rejeitar a hip√≥tese nula. N√£o h√° evid√™ncia estat√≠stica de que as restri√ß√µes s√£o significativas.")
>
> # Gr√°fico de res√≠duos
> # Calcular os res√≠duos
> e = y - X @ b
>
> # Gr√°fico de dispers√£o dos res√≠duos versus valores preditos
> plt.scatter(X @ b, e)
> plt.xlabel("Valores Preditos")
> plt.ylabel("Res√≠duos")
> plt.title("Gr√°fico de Res√≠duos versus Valores Preditos")
> plt.axhline(0, color='red', linestyle='--')
> plt.show()
>
> # Histograma dos res√≠duos
> plt.hist(e, bins=20, edgecolor='black')
> plt.xlabel("Res√≠duos")
> plt.ylabel("Frequ√™ncia")
> plt.title("Histograma dos Res√≠duos")
> plt.show()
>
> # QQ-plot dos res√≠duos
> stats.probplot(e, dist="norm", plot=plt)
> plt.title("QQ-plot dos Res√≠duos")
> plt.show()
>
> # Teste de homocedasticidade: teste de Breusch-Pagan
> _, p_bp, _, f_bp = sm.stats.het_breuschpagan(e, X)
> print("Teste de Breusch-Pagan:")
> print("Estat√≠stica LM:", f_bp)
> print("Valor-p:", p_bp)
> if p_bp < alpha:
>     print("Rejeitar a hip√≥tese nula de homocedasticidade.")
> else:
>     print("N√£o rejeitar a hip√≥tese nula de homocedasticidade.")
>
> # Teste de normalidade: teste de Shapiro-Wilk
> stat_shapiro, p_shapiro = stats.shapiro(e)
> print("Teste de Shapiro-Wilk:")
> print("Estat√≠stica de teste:", stat_shapiro)
> print("Valor-p:", p_shapiro)
> if p_shapiro < alpha:
>  print("Rejeitar a hip√≥tese nula de normalidade.")
> else:
>  print("N√£o rejeitar a hip√≥tese nula de normalidade.")
>
> # Correla√ß√£o de res√≠duos (durbin watson)
> dw = sm.stats.durbin_watson(e)
> print(f"Teste de Durbin-Watson: {dw}")
> if dw < 1.5:
>   print("Autocorrela√ß√£o positiva.")
> elif dw > 2.5:
>   print("Autocorrela√ß√£o negativa.")
> else:
>   print("N√£o h√° forte evid√™ncia de autocorrela√ß√£o.")
<!-- END -->
