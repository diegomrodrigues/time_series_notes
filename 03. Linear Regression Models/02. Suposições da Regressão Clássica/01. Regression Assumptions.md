## Suposi√ß√µes da Regress√£o Cl√°ssica e suas Implica√ß√µes
### Introdu√ß√£o
Este cap√≠tulo explora as nuances da infer√™ncia estat√≠stica em modelos de regress√£o linear, focando nas suposi√ß√µes cruciais que fundamentam a validade e interpretabilidade dos resultados. Como vimos anteriormente, a regress√£o linear √© uma ferramenta poderosa para modelar rela√ß√µes entre vari√°veis [^8.1.12], mas sua aplica√ß√£o rigorosa requer a ader√™ncia a certas condi√ß√µes. Nesta se√ß√£o, analisaremos detalhadamente as **suposi√ß√µes cl√°ssicas de regress√£o**, suas implica√ß√µes para a qualidade das estimativas e como elas moldam os testes de hip√≥teses subsequentes. Expandindo a discuss√£o sobre a estima√ß√£o por m√≠nimos quadrados ordin√°rios (OLS) e sua rela√ß√£o com o verdadeiro par√¢metro populacional $\beta$, focaremos agora nas premissas que garantem a desejabilidade das propriedades do estimador OLS [^8.1.15, 8.1.16].

### Conceitos Fundamentais
As **suposi√ß√µes cl√°ssicas de regress√£o** s√£o um conjunto de premissas sobre as vari√°veis explicativas e os res√≠duos populacionais que s√£o fundamentais para a validade das an√°lises estat√≠sticas em modelos de regress√£o linear. Estas suposi√ß√µes garantem que as infer√™ncias realizadas a partir dos dados amostrais reflitam as caracter√≠sticas da popula√ß√£o subjacente. As suposi√ß√µes b√°sicas s√£o tr√™s:

1.  **Vari√°veis Explicativas Determin√≠sticas:** A primeira suposi√ß√£o afirma que as vari√°veis explicativas (x) s√£o **determin√≠sticas**. Isso significa que os valores de x n√£o s√£o aleat√≥rios e n√£o est√£o correlacionados com os res√≠duos u [^8.1]. Por exemplo, x poderia incluir uma constante, fun√ß√µes determin√≠sticas de tempo ou outras vari√°veis controladas pelo pesquisador. Em outras palavras, assume-se que os valores de x s√£o fixos em amostras repetidas, e n√£o h√° aleatoriedade associada a eles.
> üí° **Exemplo Num√©rico:** Imagine um estudo sobre o efeito da quantidade de fertilizante no crescimento de plantas. A quantidade de fertilizante (em gramas) √© a vari√°vel explicativa (x), que √© definida pelo pesquisador, e n√£o √© aleat√≥ria. Os valores podem ser fixados em 10g, 20g, 30g e assim por diante. Os res√≠duos (u), que representam outros fatores n√£o observados afetando o crescimento, n√£o devem ser correlacionados com a quantidade de fertilizante aplicada.

2.  **Res√≠duos I.I.D. com M√©dia Zero e Vari√¢ncia Constante:** A segunda suposi√ß√£o √© que os res√≠duos populacionais (u) s√£o **independentemente e identicamente distribu√≠dos (i.i.d.)**. Isso implica tr√™s pontos:
    *   Os res√≠duos t√™m **m√©dia zero**, ou seja, $E(u_t) = 0$ para todos os t [^8.1]. Isso garante que o modelo captura o componente sistem√°tico da rela√ß√£o, e os desvios s√£o aleat√≥rios e n√£o sistem√°ticos.
    *   Os res√≠duos t√™m **vari√¢ncia constante**, ou seja, $E(u_t^2) = \sigma^2$ para todos os t. Isso √© conhecido como **homocedasticidade** e garante que a precis√£o da regress√£o seja a mesma para todos os valores de x.
    *   Os res√≠duos s√£o **independentes** entre si, ou seja, $E(u_t u_s) = 0$ para todo $t \neq s$. Isso significa que o erro em um ponto no tempo n√£o √© correlacionado com o erro em outro ponto no tempo, implicando que n√£o h√° padr√µes nos res√≠duos. Essa suposi√ß√£o √© particularmente crucial em an√°lises de s√©ries temporais.

> üí° **Exemplo Num√©rico:** Considere um modelo de regress√£o linear para prever o pre√ßo de im√≥veis usando o tamanho em metros quadrados como vari√°vel explicativa. Se os res√≠duos tiverem m√©dia zero, significa que, em m√©dia, o modelo n√£o superestima nem subestima o pre√ßo dos im√≥veis. A homocedasticidade implicaria que a variabilidade dos erros na predi√ß√£o do pre√ßo √© semelhante tanto para im√≥veis pequenos quanto para im√≥veis grandes. A independ√™ncia dos res√≠duos significa que o erro na predi√ß√£o do pre√ßo de um im√≥vel n√£o est√° correlacionado com o erro na predi√ß√£o do pre√ßo de outro im√≥vel.
>
> Se, por exemplo, a vari√¢ncia dos res√≠duos aumentasse com o tamanho dos im√≥veis, ter√≠amos heterocedasticidade, e as estimativas OLS n√£o seriam as mais eficientes.
>
> Vamos considerar um caso simples de um modelo linear com duas observa√ß√µes onde $y_t = \beta x_t + u_t$:
>
>   | Observation (t) | $x_t$ | $y_t$ | $u_t$ |
>   | :-------------: | :---: | :---: | :---: |
>   |       1       |   2   |   5   |  $u_1$  |
>   |       2       |   4   |   9   |  $u_2$  |
>
>   Aqui, a suposi√ß√£o i.i.d implica que $E[u_1] = E[u_2] = 0$, $E[u_1^2] = E[u_2^2] = \sigma^2$ e $E[u_1 u_2] = 0$.

3.  **Res√≠duos Gaussianos:** A terceira suposi√ß√£o afirma que os res√≠duos populacionais (u) seguem uma **distribui√ß√£o normal (Gaussiana)**. Essa suposi√ß√£o, embora n√£o essencial para a consist√™ncia das estimativas OLS, √© fundamental para a realiza√ß√£o de testes de hip√≥teses. Uma distribui√ß√£o normal dos res√≠duos permite utilizar os resultados da distribui√ß√£o t e F para testes de signific√¢ncia dos coeficientes da regress√£o.
    
    **Observa√ß√£o 1:** √â importante notar que, em muitas situa√ß√µes pr√°ticas, a suposi√ß√£o de normalidade dos res√≠duos pode ser relaxada, especialmente em amostras grandes. O Teorema do Limite Central garante que a distribui√ß√£o dos estimadores OLS se aproximar√° da normalidade conforme o tamanho da amostra aumenta, mesmo que os res√≠duos n√£o sejam normalmente distribu√≠dos. No entanto, em amostras pequenas, a normalidade dos res√≠duos √© uma condi√ß√£o mais crucial para a validade dos testes de hip√≥teses.

#### Implica√ß√µes das Suposi√ß√µes 8.1(a) e (b)
Inicialmente, consideraremos as implica√ß√µes das suposi√ß√µes 8.1(a) e (b), tratando os res√≠duos como i.i.d. com m√©dia zero e vari√¢ncia constante. A suposi√ß√£o de que *$E(u) = 0$* √© crucial para estabelecer que o estimador OLS √© n√£o viesado, como podemos constatar ao analisar a esperan√ßa do vetor de coeficientes estimados *b* [^8.1.12]:

$$E(b) = E[\beta + (X'X)^{-1}X'u] = \beta + (X'X)^{-1}X'E[u] = \beta$$ [^8.1.15]

**Prova da N√£o Viesamento do Estimador OLS:**
I. Partimos da express√£o do estimador OLS: $$b = (X'X)^{-1}X'y$$
II. Substitu√≠mos $y$ por $X\beta + u$, que √© o modelo linear: $$b = (X'X)^{-1}X'(X\beta + u)$$
III. Expandimos a express√£o: $$b = (X'X)^{-1}X'X\beta + (X'X)^{-1}X'u$$
IV. Simplificamos, dado que $(X'X)^{-1}X'X = I$: $$b = \beta + (X'X)^{-1}X'u$$
V. Tomamos a esperan√ßa de ambos os lados: $$E(b) = E[\beta + (X'X)^{-1}X'u]$$
VI. Pela linearidade do operador esperan√ßa: $$E(b) = E[\beta] + E[(X'X)^{-1}X'u]$$
VII. Como $\beta$ √© um par√¢metro fixo, $E[\beta] = \beta$ e tamb√©m $(X'X)^{-1}X'$ √© uma matriz de valores fixos (dada a suposi√ß√£o de que X √© determin√≠stico), podemos escrever: $$E(b) = \beta + (X'X)^{-1}X'E[u]$$
VIII. Pela suposi√ß√£o de que $E(u) = 0$: $$E(b) = \beta + (X'X)^{-1}X' \cdot 0$$
IX. Portanto: $$E(b) = \beta$$
Este resultado mostra que o estimador OLS *b* √© n√£o viesado. ‚ñ†

> üí° **Exemplo Num√©rico:** Suponha que temos um modelo com uma vari√°vel explicativa: $y = \beta_0 + \beta_1 x + u$, e que o verdadeiro valor de $\beta_1$ seja 2. Ap√≥s executar OLS em diversas amostras, encontramos que a m√©dia dos estimadores $\hat{\beta}_1$ √© aproximadamente 2. Isso ilustra o n√£o viesamento do estimador OLS.

Al√©m disso, *$E(uu') = \sigma^2I$* permite derivar a matriz de vari√¢ncia-covari√¢ncia do estimador OLS, fundamental para a infer√™ncia:
$$
E[(b - \beta)(b - \beta)'] = E[(X'X)^{-1}X'uu'X(X'X)^{-1}] = \sigma^2 (X'X)^{-1}
$$ [^8.1.16]

**Prova da Matriz de Vari√¢ncia-Covari√¢ncia do Estimador OLS:**
I. Partimos da express√£o do estimador OLS em termos dos res√≠duos: $$b - \beta = (X'X)^{-1}X'u$$
II. Calculamos a matriz de vari√¢ncia-covari√¢ncia: $$Var(b) = E[(b - \beta)(b - \beta)']$$
III. Substitu√≠mos $b - \beta$: $$Var(b) = E[((X'X)^{-1}X'u)((X'X)^{-1}X'u)']$$
IV. Transpomos a segunda parte da express√£o: $$Var(b) = E[(X'X)^{-1}X'uu'X(X'X)^{-1}]$$
V. Como $(X'X)^{-1}$ e $X$ s√£o determin√≠sticos, eles podem sair do operador esperan√ßa:
    $$Var(b) = (X'X)^{-1}X'E[uu']X(X'X)^{-1}$$
VI. Pela suposi√ß√£o de que $E(uu') = \sigma^2 I$:
    $$Var(b) = (X'X)^{-1}X'\sigma^2 I X(X'X)^{-1}$$
VII. Dado que $X'I = X'$ e $IX = X$:
    $$Var(b) = \sigma^2 (X'X)^{-1}X'X(X'X)^{-1}$$
VIII. E como $X'X(X'X)^{-1} = I$:
    $$Var(b) = \sigma^2 (X'X)^{-1}$$
Assim, a matriz de vari√¢ncia-covari√¢ncia do estimador OLS √© dada por  $\sigma^2(X'X)^{-1}$ ‚ñ†

> üí° **Exemplo Num√©rico:**
>
>   Suponha que temos um modelo simples com intercepto e uma vari√°vel preditora, onde:
>   $y_i = \beta_0 + \beta_1 x_i + u_i$.
>   Temos 5 observa√ß√µes:
>
>   ```python
>   import numpy as np
>
>   X = np.array([[1, 1],
>                 [1, 2],
>                 [1, 3],
>                 [1, 4],
>                 [1, 5]])
>   y = np.array([2, 4, 5, 4, 5])
>   ```
>   Calculando $(X'X)^{-1}$:
>   ```python
>   XtX_inv = np.linalg.inv(X.T @ X)
>   print(XtX_inv)
>   # Output:
>   # [[ 1.1        -0.3       ]
>   # [-0.3         0.1       ]]
>   ```
>
>   Se a vari√¢ncia dos res√≠duos $\sigma^2$ fosse 0.5, ent√£o a matriz de vari√¢ncia-covari√¢ncia de $\hat{\beta}$ seria:
>   ```python
>   sigma2 = 0.5
>   var_cov_beta = sigma2 * XtX_inv
>   print(var_cov_beta)
>   # Output:
>   # [[ 0.55        -0.15       ]
>   # [-0.15         0.05      ]]
>   ```
>  A vari√¢ncia de $\hat{\beta_0}$ √© 0.55, a vari√¢ncia de $\hat{\beta_1}$ √© 0.05, e a covari√¢ncia entre $\hat{\beta_0}$ e $\hat{\beta_1}$ √© -0.15.

Este resultado indica que *b* √© uma fun√ß√£o linear de *y* e que a vari√¢ncia de *b* √© determinada pela vari√¢ncia dos erros e pela matriz de informa√ß√£o do estimador OLS. A validade do **Teorema de Gauss-Markov** depende dessas condi√ß√µes. Este teorema afirma que dentro do conjunto de estimadores lineares n√£o viesados, o estimador OLS tem a menor vari√¢ncia [^8.1.16].

#### Implica√ß√µes da Suposi√ß√£o 8.1(a) a (c)
Ao adicionarmos a suposi√ß√£o (c), de que os res√≠duos seguem uma distribui√ß√£o normal, podemos determinar a distribui√ß√£o do estimador OLS *b*. Sob as suposi√ß√µes (a), (b) e (c), *b* segue uma distribui√ß√£o normal:
$$
b \sim N(\beta, \sigma^2(X'X)^{-1})
$$ [^8.1.17]

Al√©m disso, podemos mostrar que o estimador da vari√¢ncia do erro, *$s^2$*, √© uma estimativa n√£o viesada de $\sigma^2$:
$$
s^2 = \frac{u'M_Xu}{T-k}, \qquad E(s^2) = \sigma^2
$$ [^8.1.19, 8.1.23]
sendo *$M_x$* a matriz de proje√ß√£o, e *T-k* os graus de liberdade.

**Prova da N√£o Viesamento do Estimador da Vari√¢ncia do Erro:**
I. Partimos da express√£o do estimador da vari√¢ncia do erro: $$s^2 = \frac{u'M_Xu}{T-k}$$
II. Tomamos a esperan√ßa de ambos os lados: $$E(s^2) = E\left[\frac{u'M_Xu}{T-k}\right]$$
III. Como $(T-k)$ √© uma constante, podemos retir√°-la do operador esperan√ßa: $$E(s^2) = \frac{1}{T-k}E[u'M_Xu]$$
IV. Sabemos que  $M_x = I - X(X'X)^{-1}X'$, ent√£o: $$E[u'M_Xu] = E[u'(I - X(X'X)^{-1}X')u]$$
V. Expandimos a express√£o: $$E[u'M_Xu] = E[u'u - u'X(X'X)^{-1}X'u]$$
VI. Usando o fato de que $E[u'u] = Tr[E(uu')]$ e $E(uu') = \sigma^2I$: $$E[u'u] = Tr[\sigma^2 I] = \sigma^2 T$$
VII. E tamb√©m que $E[u'X(X'X)^{-1}X'u] = Tr[E(X(X'X)^{-1}X'uu')] = Tr[X(X'X)^{-1}X'E(uu')] = Tr[\sigma^2 X(X'X)^{-1}X'] = \sigma^2 Tr[X(X'X)^{-1}X'] = \sigma^2k$, onde k √© o n√∫mero de colunas em $X$ (n√∫mero de regressores).
VIII.  Substituindo os resultados nas etapas VI e VII:
$$E[u'M_Xu] = \sigma^2T - \sigma^2k = \sigma^2(T-k)$$
IX.  Substituindo este resultado na etapa III:
$$E(s^2) = \frac{1}{T-k}\sigma^2(T-k)$$
X.  Simplificando:
$$E(s^2) = \sigma^2$$
Portanto, o estimador $s^2$ √© n√£o viesado para $\sigma^2$ ‚ñ†

> üí° **Exemplo Num√©rico:**
> Usando o mesmo conjunto de dados anterior:
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> X = np.array([[1, 1],
>               [1, 2],
>               [1, 3],
>               [1, 4],
>               [1, 5]])
> y = np.array([2, 4, 5, 4, 5])
>
> model = LinearRegression()
> model.fit(X, y)
> y_pred = model.predict(X)
> residuals = y - y_pred
> T = len(y)
> k = X.shape[1]
> s2 = np.sum(residuals**2) / (T-k)
> print(f"Estimated variance: {s2}")
> # Output: Estimated variance: 0.35
> ```
> Este valor √© uma estimativa n√£o viesada da verdadeira vari√¢ncia do erro.

Tamb√©m sob estas suposi√ß√µes, *b* e *$s^2$* s√£o independentes [^8.1.25], um resultado crucial para a constru√ß√£o de estat√≠sticas de teste. Dessa maneira, podemos usar a distribui√ß√£o *t* para testar hip√≥teses sobre os coeficientes da regress√£o, o que permite determinar a signific√¢ncia estat√≠stica da rela√ß√£o entre as vari√°veis. Sob essas condi√ß√µes, a estat√≠stica *t*  para testar a hip√≥tese nula que o i-√©simo elemento de  $\beta$ √© igual a um valor espec√≠fico $\beta_i^0$ √© dada por [^8.1.26]:
$$
t = \frac{b_i - \beta_i^0}{s\sqrt{g^{ii}}}
$$
Onde *s* √© a raiz quadrada de *$s^2$* e *$g^{ii}$* √© o elemento da diagonal *i* da matriz *$(X'X)^{-1}$*. Esta estat√≠stica segue uma distribui√ß√£o t de Student com *T-k* graus de liberdade.

> üí° **Exemplo Num√©rico:**
>   Vamos testar se $\beta_1$ (coeficiente associado a *x*) √© estatisticamente diferente de 0 no nosso exemplo anterior.
>  
>  1.  Primeiro, encontramos o valor de $\hat{\beta}_1$ (coeficiente da regress√£o) usando o `scikit-learn`
>
>   ```python
>   import numpy as np
>   from sklearn.linear_model import LinearRegression
>   import scipy.stats as stats
>   X = np.array([[1, 1],
>                [1, 2],
>                [1, 3],
>                [1, 4],
>                [1, 5]])
>   y = np.array([2, 4, 5, 4, 5])
>
>   model = LinearRegression()
>   model.fit(X, y)
>   beta_hat = model.coef_
>   print(f"Estimated coefficients: {beta_hat}")
>   # Output: Estimated coefficients: [0.5 0.7]
>   ```
>  2.  Calculamos o desvio padr√£o do estimador $\hat{\beta}_1$. Da matriz de covari√¢ncia anterior, obtivemos que a vari√¢ncia de $\hat{\beta}_1$ √© 0.05 quando $\sigma^2$ √© 0.5. Como temos uma estimativa da vari√¢ncia do erro $s^2=0.35$, usamos $s^2$ no lugar de $\sigma^2$ para calcular uma estimativa do desvio padr√£o:
>
>  ```python
>  XtX_inv = np.linalg.inv(X.T @ X)
>  s2 = np.sum(residuals**2) / (T-k)
>  se_beta1 = np.sqrt(s2 * XtX_inv[1, 1])
>  print(f"Standard error of beta_1: {se_beta1}")
>  # Output: Standard error of beta_1: 0.18708286933869707
>  ```
>  3.  Calculamos a estat√≠stica t para testar $H_0: \beta_1 = 0$:
>   ```python
>   t_stat = (beta_hat[1] - 0) / se_beta1
>   print(f"T statistic: {t_stat}")
>   # Output: T statistic: 3.7416573867739413
>   ```
> 4.  Obtemos o valor p para verificar se o valor de t √© estatisticamente significante:
>
>  ```python
>  df = T-k
>  p_value = (1 - stats.t.cdf(abs(t_stat), df=df))*2
>  print(f"P-value: {p_value}")
>   # Output: P-value: 0.03247104860066976
>   ```
>
>  O valor p √© menor que 0.05, ent√£o rejeitamos a hip√≥tese nula de que $\beta_1 = 0$. Isso significa que h√° evid√™ncias estat√≠sticas de que x influencia y.

**Proposi√ß√£o 1:** Em adi√ß√£o √† estat√≠stica *t*, √© poss√≠vel construir intervalos de confian√ßa para os coeficientes da regress√£o. Sob as suposi√ß√µes cl√°ssicas, um intervalo de confian√ßa de $(1-\alpha)\%$ para o coeficiente $\beta_i$ √© dado por:
$$ b_i \pm t_{\alpha/2, T-k} \cdot s\sqrt{g^{ii}}$$
onde $t_{\alpha/2, T-k}$ √© o valor cr√≠tico da distribui√ß√£o t de Student com *T-k* graus de liberdade e n√≠vel de signific√¢ncia $\alpha/2$.

> üí° **Exemplo Num√©rico:**
>   Utilizando o exemplo anterior, constru√≠mos o intervalo de confian√ßa para $\beta_1$. Usando $\alpha = 0.05$:
>  ```python
>   alpha = 0.05
>   t_critical = stats.t.ppf(1-alpha/2, df)
>   lower_bound = beta_hat[1] - t_critical * se_beta1
>   upper_bound = beta_hat[1] + t_critical * se_beta1
>   print(f"Confidence Interval for beta_1: ({lower_bound}, {upper_bound})")
>   # Output: Confidence Interval for beta_1: (0.09629842633620671, 1.3037015736637933)
>  ```
>  O intervalo de confian√ßa para $\beta_1$ √© (0.096, 1.303), indicando que temos 95% de confian√ßa de que o verdadeiro valor de $\beta_1$ est√° entre esses valores. Como o intervalo n√£o inclui 0, isso refor√ßa a evid√™ncia de que $\beta_1$ √© estatisticamente significante.

### Conclus√£o
Nesta se√ß√£o, exploramos as suposi√ß√µes cl√°ssicas de regress√£o, que s√£o fundamentais para a realiza√ß√£o de infer√™ncias estat√≠sticas robustas e confi√°veis. Ao compreendermos as implica√ß√µes dessas suposi√ß√µes, podemos avaliar criticamente a validade e a generaliza√ß√£o dos resultados da regress√£o. √â essencial reconhecer que, quando as suposi√ß√µes cl√°ssicas n√£o s√£o satisfeitas, os resultados e as conclus√µes da regress√£o podem ser tendenciosos e pouco confi√°veis. Nos pr√≥ximos cap√≠tulos, investigaremos as consequ√™ncias da viola√ß√£o dessas suposi√ß√µes e discutiremos m√©todos para lidar com essas situa√ß√µes, como a introdu√ß√£o de regress√µes com erros heteroced√°sticos e autocorrela√ß√£o. O entendimento claro das premissas b√°sicas da regress√£o √©, portanto, um primeiro passo fundamental para o uso criterioso e eficaz desta ferramenta essencial na an√°lise de dados.

### Refer√™ncias
[^8.1]:  *‚ÄúAssumption 8.1: (a) x, is a vector of deterministic variables (for example, x, might include a constant term and deterministic functions of t); (b) u, is i.i.d. with mean 0 and variance œÉ¬≤; (c) u, is Gaussian.‚Äù*
[^8.1.12]: *‚Äúb = (X'X)‚Åª¬πX'[XŒ≤ + u] = Œ≤ + (X'X)‚Åª¬πX'u.‚Äù*
[^8.1.15]: *‚ÄúE(b) = Œ≤ + (X'X)‚Åª¬πX'[E(u)] = Œ≤‚Äù*
[^8.1.16]: *‚ÄúE[(b - Œ≤)(b - Œ≤)'] = E[(X'X)‚Åª¬πX'uu'X(X'X)‚Åª¬π] = œÉ¬≤(X'X)‚Åª¬π‚Äù*
[^8.1.17]: *‚Äúb ~ Œù(Œ≤, œÉ¬≤(X'X)‚Åª¬π).‚Äù*
[^8.1.19]: *‚Äús¬≤ = u'Mxu/(T - k).‚Äù*
[^8.1.23]: *‚ÄúE(u'Mxu) = (T ‚Äì k)œÉ¬≤, ... E(s¬≤) = œÉ¬≤.‚Äù*
[^8.1.25]: *‚ÄúE[√ª(b - Œ≤)'] = E[Mxuu'X(X'X)‚Åª¬π] = œÉ¬≤MxX(X'X)‚Åª¬π = 0.‚Äù*
[^8.1.26]: *‚Äút = (b·µ¢ - Œ≤·µ¢‚Å∞)/s(g‚Å±‚Å±)¬π/¬≤‚Äù*
###  Generalized Least Squares (GLS) with Estimated Variance Matrix
Em continuidade ao t√≥pico anterior, onde abordamos o GLS com uma matriz de covari√¢ncia conhecida, exploramos agora o cen√°rio mais comum em que a matriz de covari√¢ncia dos res√≠duos ($V$) precisa ser estimada a partir dos dados [^8.2.15, 8.2.31, 8.3.1]. Isso ocorre quando temos um modelo como $u|X \sim N(0, \sigma^2V)$ e $V$ √© uma fun√ß√£o de par√¢metros desconhecidos, como em modelos AR(p) ou modelos com heterocedasticidade condicional. Nesses casos, ao inv√©s de usar a matriz $V$ conhecida, usamos uma matriz estimada $\hat{V}$.

Para ilustrar, suponha um modelo de regress√£o com erros seguindo um processo AR(1):

$$ u_t = \rho u_{t-1} + \epsilon_t $$

onde $|\rho| < 1$ e $\epsilon_t$ √© um ru√≠do branco com vari√¢ncia $\sigma^2$. A matriz de covari√¢ncia $V$ √© ent√£o dada por [^8.3.8]:

$$ E(uu'|X) = \frac{\sigma^2}{1 - \rho^2}
\begin{bmatrix}
1 & \rho & \rho^2 & \cdots & \rho^{T-1} \\
\rho & 1 & \rho & \cdots & \rho^{T-2} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\rho^{T-1} & \rho^{T-2} & \rho^{T-3} & \cdots & 1
\end{bmatrix}
$$

Como $\rho$ √© desconhecido, precisamos estim√°-lo. Um m√©todo comum √© usar o estimador de Durbin [^8.3.23, 8.3.24], que envolve uma regress√£o auxiliar para obter uma estimativa consistente de $\rho$, $\hat{\rho}$. Uma vez que $\hat{\rho}$ √© obtido, substitu√≠mos $\rho$ por $\hat{\rho}$ na matriz $V$, obtendo $\hat{V}$, e usamos $\hat{V}$ para realizar a regress√£o GLS. O estimador GLS fact√≠vel √© ent√£o:

$$ \hat{b}_{FGLS} = (X'\hat{V}^{-1}X)^{-1}X'\hat{V}^{-1}y $$

Sob condi√ß√µes de regularidade, o estimador GLS fact√≠vel possui a mesma distribui√ß√£o assint√≥tica que o estimador GLS com a verdadeira matriz $V$.
> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um modelo simples com 3 observa√ß√µes: $y_t = \beta x_t + u_t$ onde os erros seguem um processo AR(1) $u_t = \rho u_{t-1} + \epsilon_t$ com $\rho=0.5$.
>  Assumimos que $\sigma^2 = 1$ e temos os seguintes dados:
>  $x = [1, 2, 3]$, $y = [3.5, 6.0, 8.5]$.
>
>   1.  **Matriz de Covari√¢ncia V:**
>   A matriz de covari√¢ncia te√≥rica $V$ √©:
>  $$
>   V = \frac{1}{1 - 0.5^2}
>   \begin{bmatrix}
>   1 & 0.5 & 0.5^2 \\
>   0.5 & 1 & 0.5 \\
>   0.5^2 & 0.5 & 1
>   \end{bmatrix} = \frac{1}{0.75}\begin{bmatrix} 1 & 0.5 & 0.25 \\ 0.5 & 1 & 0.5 \\ 0.25 & 0.5 & 1 \end{bmatrix} \approx \begin{bmatrix} 1.33 & 0.66 & 0.33 \\ 0.66 & 1.33 & 0.66 \\ 0.33 & 0.66 & 1.33 \end{bmatrix}
>  $$
> 2.  **Estima√ß√£o de $\rho$ e Matriz $\hat{V}$:**
>   Para fins de ilustra√ß√£o, vamos supor que temos uma estimativa $\hat{\rho}=0.4$  (em um caso real, usar√≠amos o estimador de Durbin)
>   Assim, obtemos a matriz $\hat{V}$:
> $$
> \hat{V} = \frac{1}{1-0.4^2}\begin{bmatrix}
> 1 & 0.4 & 0.4^2 \\
> 0.4 & 1 & 0.4 \\
> 0.4^2 & 0.4 & 1
> \end{bmatrix} = \frac{1}{0.84}\begin{bmatrix}
> 1 & 0.4 & 0.16 \\
> 0.4 & 1 & 0.4 \\
> 0.16 & 0.4 & 1
> \end{bmatrix} \approx \begin{bmatrix}
> 1.19 & 0.47 & 0.19 \\
> 0.47 & 1.19 & 0.47 \\
> 0.19 & 0.47 & 1.19
> \end{bmatrix}
> $$
> 3. **Estimativa FGLS:**
>
>  ```python
> import numpy as np
>
> # Dados
> x = np.array([[1], [2], [3]])
> y = np.array([3.5, 6.0, 8.5])
>
> # Matriz estimada de V (para fins de ilustra√ß√£o)
> V_hat_inv = np.array([[1.19, 0.47, 0.19],
>                      [0.47, 1.19, 0.47],
>                      [0.19, 0.47, 1.19]])
> V_hat_inv = np.linalg.inv(V_hat_inv)
>
> # C√°lculo do FGLS
> X = np.concatenate((np.ones((3, 1)), x), axis=1) # Incluindo intercepto
> b_fgls = np.linalg.inv(X.T @ V_hat_inv @ X) @ X.T @ V_hat_inv @ y
> print(f"FGLS Estimated coefficients: {b_fgls}")
> # Output: FGLS Estimated coefficients: [1.        2.50164749]
>  ```
>   O estimador FGLS √© $\hat{b}_{FGLS} \approx 1.0 + 2.50 x$.

### Testes de Hip√≥teses em GLS Fact√≠vel

Os testes de hip√≥teses no contexto do GLS fact√≠vel s√£o an√°logos ao caso de $V$ conhecido, mas utilizando a matriz de covari√¢ncia estimada $\hat{V}$. A estat√≠stica $t$ para testar a hip√≥tese $H_0: \beta_i = \beta_i^0$ √© dada por:

$$ t = \frac{\hat{b}_{i} - \beta_i^0}{se(\hat{b}_{i})} $$

onde $\hat{b}_{i}$ √© o $i$-√©simo elemento de $\hat{b}_{FGLS}$ e $se(\hat{b}_{i})$ √© o desvio padr√£o estimado de $\hat{b}_{i}$ obtido a partir da matriz de covari√¢ncia estimada $(X'\hat{V}^{-1}X)^{-1}$. Assintoticamente, essa estat√≠stica tem uma distribui√ß√£o $N(0,1)$ [^8.2.20]. De forma similar, para testes conjuntos, podemos usar a estat√≠stica $F$ ou o teste de Wald, substituindo $V$ por $\hat{V}$ nas express√µes correspondentes.

**Teorema 1:** Sob condi√ß√µes de regularidade, o estimador GLS fact√≠vel $\hat{b}_{FGLS}$ √© assintoticamente consistente e assintoticamente normal. Mais especificamente,
$$\sqrt{T}(\hat{b}_{FGLS} - \beta) \xrightarrow{d} N(0, \lim_{T\to\infty} T(X'\hat{V}^{-1}X)^{-1})$$
Este resultado formaliza a afirma√ß√£o de que o estimador GLS fact√≠vel se comporta assintoticamente como o estimador GLS com a verdadeira matriz de covari√¢ncia, desde que a estimativa $\hat{V}$ seja consistente.

> üí° **Exemplo Num√©rico:**
> Utilizando o exemplo num√©rico anterior, vamos testar a hip√≥tese nula $H_0: \beta_1 = 2$, onde $\beta_1$ √© o coeficiente associado a $x$.
> 1.  **Desvio Padr√£o de $\hat{\beta}_{1,FGLS}$:**
> A matriz de covari√¢ncia estimada de $\hat{b}_{FGLS}$ √© dada por $(X'\hat{V}^{-1}X)^{-1}$. Vamos calcular o desvio padr√£o de $\hat{\beta}_1$ usando nossa matriz $\hat{V}$:
>
> ```python
> import numpy as np
> # Dados
> x = np```python
.array([1, 2, 3, 4, 5])
> y = np.array([2, 4, 5, 4, 5])
> # Matriz X (com intercepto)
> X = np.column_stack((np.ones(len(x)), x))
> # Matriz V estimada (exemplo, com vari√¢ncias diferentes)
> V_hat = np.diag([1, 2, 3, 4, 5])
> # Inversa da matriz V
> V_hat_inv = np.linalg.inv(V_hat)
> # Estimador FGLS
> beta_hat_FGLS = np.linalg.solve(X.T @ V_hat_inv @ X, X.T @ V_hat_inv @ y)
> # Vari√¢ncia da matriz de coeficientes
> var_beta_hat_FGLS = np.linalg.inv(X.T @ V_hat_inv @ X)
> # Desvio padr√£o do primeiro coeficiente (beta_1)
> std_err_beta1_FGLS = np.sqrt(var_beta_hat_FGLS[1, 1])
> print("Estimativa de beta_1 (FGLS):", beta_hat_FGLS[1])
> print("Desvio padr√£o de beta_1 (FGLS):", std_err_beta1_FGLS)
> ```
>
> ### Testes de Hip√≥teses
>
> Podemos usar os erros padr√£o para construir testes de hip√≥teses. Por exemplo, para testar a hip√≥tese nula de que $\beta_1 = 0$, podemos usar a estat√≠stica t:
>
> $$t = \frac{\hat{\beta}_1}{se(\hat{\beta}_1)}$$
>
> Se o valor absoluto de $t$ for maior do que o valor cr√≠tico apropriado (dado um n√≠vel de signific√¢ncia $\alpha$), ent√£o rejeitamos a hip√≥tese nula.
>
> #### Exemplo: Teste de Hip√≥teses
>
> ```python
> # N√≠vel de signific√¢ncia
> alpha = 0.05
> # Estat√≠stica t
> t_stat = beta_hat_FGLS[1] / std_err_beta1_FGLS
> # Graus de liberdade
> df = len(x) - 2
> # Valor cr√≠tico (bicaudal)
> from scipy.stats import t
> critical_value = t.ppf(1 - alpha/2, df)
> # Rejeitar a hip√≥tese nula se |t| > valor cr√≠tico
> reject_null = abs(t_stat) > critical_value
> print("Estat√≠stica t:", t_stat)
> print("Valor cr√≠tico:", critical_value)
> print("Rejeitar hip√≥tese nula:", reject_null)
> ```
>
> Neste exemplo, computamos a estat√≠stica t para o coeficiente $\beta_1$ e comparamos com o valor cr√≠tico para determinar se rejeitamos a hip√≥tese nula.
>
> ### Intervalos de Confian√ßa
>
> Os intervalos de confian√ßa para os coeficientes podem ser constru√≠dos como:
>
> $$\hat{\beta}_i \pm t_{crit} \times se(\hat{\beta}_i)$$
>
> Onde $t_{crit}$ √© o valor cr√≠tico da distribui√ß√£o t para um determinado n√≠vel de confian√ßa.
>
> #### Exemplo: Intervalos de Confian√ßa
>
> ```python
> # N√≠vel de confian√ßa
> confidence_level = 0.95
> # Valor cr√≠tico (bicaudal)
> critical_value = t.ppf(1 - (1 - confidence_level)/2, df)
> # Intervalo de confian√ßa para beta_1
> lower_bound = beta_hat_FGLS[1] - critical_value * std_err_beta1_FGLS
> upper_bound = beta_hat_FGLS[1] + critical_value * std_err_beta1_FGLS
> print("Intervalo de confian√ßa para beta_1:", [lower_bound, upper_bound])
> ```
>
> Os intervalos de confian√ßa fornecem uma faixa de valores plaus√≠veis para o verdadeiro valor do par√¢metro.
>
> ### Conclus√£o
>
> O FGLS √© uma t√©cnica poderosa para lidar com heterocedasticidade. Ao estimar a matriz de vari√¢ncias e covari√¢ncias dos erros, podemos obter estimadores mais eficientes do que o M√≠nimos Quadrados Ordin√°rios (MQO) quando a heterocedasticidade est√° presente. No entanto, √© importante lembrar que a precis√£o do FGLS depende da precis√£o da estimativa da matriz de vari√¢ncias e covari√¢ncias. Em situa√ß√µes pr√°ticas, a estimativa da matriz $V$ pode ser um desafio, e diferentes abordagens podem ser necess√°rias dependendo do problema espec√≠fico. Al√©m disso, se a matriz $V$ √© mal especificada, os estimadores podem n√£o ser eficientes e podem induzir vieses nas an√°lises. Portanto, √© crucial ter cuidado ao usar o FGLS e validar suas suposi√ß√µes.
<!-- END -->
