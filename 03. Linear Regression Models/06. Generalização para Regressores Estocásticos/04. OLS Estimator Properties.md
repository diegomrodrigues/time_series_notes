## OLS com Regressores Estoc√°sticos e N√£o Gaussianos: Distribui√ß√£o e Infer√™ncia
### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise do modelo de regress√£o linear, abordando um cen√°rio crucial onde os regressores s√£o estoc√°sticos e os erros n√£o seguem uma distribui√ß√£o Gaussiana. Construindo sobre os resultados anteriores [^1], que mostram que o estimador OLS permanece n√£o enviesado quando os regressores s√£o estoc√°sticos e independentes dos erros, e que sua distribui√ß√£o condicional √© Gaussiana [^1], agora investigaremos as implica√ß√µes da n√£o-gaussianidade dos erros [^2]. Veremos que, sob essas condi√ß√µes mais gerais, a distribui√ß√£o do estimador OLS deixa de ser Gaussiana, exigindo uma an√°lise baseada em resultados assint√≥ticos para a infer√™ncia estat√≠stica [^2].

### Conceitos Fundamentais
#### Regressores Estoc√°sticos, Erros N√£o Gaussianos e Independ√™ncia
Conforme explorado nos cap√≠tulos anteriores [^1], a suposi√ß√£o de que os erros s√£o gaussianos √© relaxada, enquanto mantemos que os regressores (representados por $X$) s√£o estoc√°sticos e independentes dos erros (representados por $u$). Esta configura√ß√£o √© comum em estudos econom√©tricos e s√©ries temporais, onde as vari√°veis explicativas podem ser end√≥genas ou ter distribui√ß√µes n√£o normais [^2]. Assumimos que os erros $u_t$ s√£o *i.i.d.* (independentes e identicamente distribu√≠dos), com m√©dia zero, vari√¢ncia $\sigma^2$ e um quarto momento finito ($E(u_t^4) = \mu_4 < \infty$), mas n√£o necessariamente gaussianos [^2].

**Assun√ß√£o 8.3:**
(a) $x_t$ √© estoc√°stico e independente de $u_s$ para todo $t$ e $s$.
(b) $u_t$ √© *i.i.d.*, n√£o Gaussiano, com m√©dia zero, vari√¢ncia $\sigma^2$ e $E(u_t^4) = \mu_4 < \infty$.
(c) $E(x_t x_t') = Q_t$, uma matriz definida positiva com $(1/T)\sum_{t=1}^{T} Q_t \xrightarrow{p} Q$, uma matriz definida positiva.
(d) $E(|x_{ti} x_{tj} x_{tl} x_{tm}|) < \infty$ para todo $i, j, l, m$ e $t$.
(e) $(1/T)\sum_{t=1}^{T} (x_t x_t') \xrightarrow{p} Q$.

As condi√ß√µes (c) a (e) da Assun√ß√£o 8.3 s√£o cruciais para garantir resultados assint√≥ticos v√°lidos, permitindo que derivemos resultados consistentes.

**Proposi√ß√£o 2:** *Sob a assun√ß√£o 8.3, o estimador de M√≠nimos Quadrados Ordin√°rios (OLS), denotado por b, continua n√£o enviesado, ou seja, E(b) = Œ≤.*

*Prova:*
I.  O estimador OLS √© definido como $b = (X'X)^{-1}X'y$. Substituindo $y = X\beta + u$, temos $b = (X'X)^{-1}X'(X\beta + u) = \beta + (X'X)^{-1}X'u$.
II. Tomando a esperan√ßa de b e usando a independ√™ncia entre $X$ e $u$, temos: $E(b) = E[\beta + (X'X)^{-1}X'u] = \beta + E[(X'X)^{-1}X']E(u)$.
III. Como $E(u) = 0$, segue que $E(b) = \beta$. Portanto, o estimador OLS continua n√£o enviesado mesmo com erros n√£o gaussianos e regressores estoc√°sticos, desde que mantenha a condi√ß√£o de independ√™ncia entre regressores e erros. ‚ñ†

#### Impacto da N√£o-Gaussianidade nos Resultados de Amostras Pequenas
Ao contr√°rio do cen√°rio onde os erros s√£o gaussianos, as distribui√ß√µes de $s^2$, das estat√≠sticas $t$ e $F$ n√£o s√£o mais as mesmas em pequenas amostras [^2]. Consequentemente, os testes de hip√≥teses tradicionais baseados nas distribui√ß√µes $t$ e $F$ n√£o s√£o v√°lidos em amostras pequenas quando os erros n√£o s√£o gaussianos. A distribui√ß√£o condicional do estimador OLS, $b$, √© dada por
$$b|X \sim N(\beta, \sigma^2(X'X)^{-1})$$
A distribui√ß√£o incondicional de $b$ n√£o √© Gaussiana sob a Assun√ß√£o 8.3 [^2].

**Observa√ß√£o 2:** *A aus√™ncia de distribui√ß√£o Gaussiana na distribui√ß√£o incondicional de $b$ surge do fato de que $X$ √© estoc√°stico, e, portanto, $X'X$ tamb√©m √© estoc√°stico. Consequentemente, a distribui√ß√£o de $(X'X)^{-1}$ √© n√£o trivial, o que implica que a distribui√ß√£o de $b$ √© uma mistura n√£o-normal de distribui√ß√µes condicionais*

**Lema 2.1:** *Sob as condi√ß√µes da Assun√ß√£o 8.3, a vari√¢ncia do estimador OLS, condicional a X, √© dada por $Var(b|X) = \sigma^2(X'X)^{-1}$.*

*Prova:*
I. Sabemos que $b = \beta + (X'X)^{-1}X'u$.
II. Assim, $Var(b|X) = Var(\beta + (X'X)^{-1}X'u|X) = Var((X'X)^{-1}X'u|X)$.
III. Como $X$ √© condicionalmente fixo, $Var(b|X) = (X'X)^{-1}X'Var(u|X)X(X'X)^{-1} = (X'X)^{-1}X'\sigma^2 I X(X'X)^{-1} = \sigma^2(X'X)^{-1}$.
$\blacksquare$

> üí° **Exemplo Num√©rico:** Vamos ilustrar o impacto da n√£o-gaussianidade dos erros atrav√©s da simula√ß√£o de v√°rias amostras e verifica√ß√£o da distribui√ß√£o do estimador OLS.
>
> ```python
> import numpy as np
> import pandas as pd
> import matplotlib.pyplot as plt
> from scipy import stats
>
> # Par√¢metros do modelo
> beta_0 = 2
> beta_1 = 1.5
> num_simulations = 1000
> sample_size = 50
>
> b1_estimates = []
> t_statistics = []
>
> for _ in range(num_simulations):
>     # Simula√ß√£o dos dados
>     x = np.random.rand(sample_size) * 10
>     u = np.random.standard_t(5, sample_size) # Erro n√£o gaussiano
>     y = beta_0 + beta_1 * x + u
>
>     # Construindo a matriz X
>     X = np.column_stack((np.ones(sample_size), x))
>
>     # Calculando o estimador OLS
>     b = np.linalg.inv(X.T @ X) @ X.T @ y
>
>     # Estimando a vari√¢ncia do erro
>     y_hat = X @ b
>     residuals = y - y_hat
>     RSS = np.sum(residuals**2)
>     sigma2_hat = RSS / (len(y) - X.shape[1])
>
>     # Calculando o erro padr√£o de b1
>     var_b = np.diag(sigma2_hat * np.linalg.inv(X.T @ X))
>     se_b = np.sqrt(var_b)
>
>     # Calculando a estat√≠stica t
>     t_statistic = (b[1] - beta_1) / se_b[1]
>
>     # Armazenando a estimativa do coeficiente b1
>     b1_estimates.append(b[1])
>     t_statistics.append(t_statistic)
>
> # Histograma das estimativas de b1
> plt.figure(figsize=(12, 6))
> plt.subplot(1, 2, 1)
> plt.hist(b1_estimates, bins=30)
> plt.title('Histograma das Estimativas de b1 (T=50)')
> plt.xlabel('Valores de b1')
> plt.ylabel('Frequ√™ncia')
>
> # Histograma da estat√≠stica t
> plt.subplot(1, 2, 2)
> plt.hist(t_statistics, bins=30)
> plt.title('Histograma da Estat√≠stica t (T=50)')
> plt.xlabel('Valores da Estat√≠stica t')
> plt.ylabel('Frequ√™ncia')
> plt.show()
>
> # Testando se a m√©dia amostral da estat√≠stica t √© pr√≥xima de zero
> print(f"M√©dia das estimativas de b1: {np.mean(b1_estimates):.4f}")
> print(f"Valor real de beta1: {beta_1}")
> print(f"M√©dia das estat√≠sticas t: {np.mean(t_statistics):.4f}")
> ```
> Este exemplo demonstra que, em pequenas amostras (T=50), a distribui√ß√£o do estimador $b_1$  e da estat√≠stica $t$ podem se desviar de uma distribui√ß√£o normal padr√£o. Este resultado refor√ßa a necessidade do uso de resultados assint√≥ticos para a realiza√ß√£o de testes de hip√≥teses v√°lidos.

#### Resultados Assint√≥ticos e Distribui√ß√µes Limitantes
Para aplicar as regras de infer√™ncia usuais de m√≠nimos quadrados,  recorremos aos resultados assint√≥ticos [^2]. As condi√ß√µes (c) a (e) da assun√ß√£o 8.3 s√£o cruciais para a validade dos resultados assint√≥ticos.

**Consist√™ncia do Estimador OLS:**
Como visto anteriormente [^2], o estimador OLS $b_T$ continua consistente sob a Assun√ß√£o 8.3, isto √©, $b_T \xrightarrow{p} \beta$ [^2]. A condi√ß√£o (e), que afirma que $(1/T)\sum_{t=1}^{T} x_t x_t' \xrightarrow{p} Q$, garante que o estimador convirja para o verdadeiro valor, sendo a matriz $Q$ definida positiva [^2].

**Distribui√ß√£o Assint√≥tica do Estimador OLS:**
A distribui√ß√£o assint√≥tica do estimador OLS √© dada por [^2]:
$$ \sqrt{T}(b_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1}). $$
Este resultado indica que o estimador OLS, multiplicado por $\sqrt{T}$, converge em distribui√ß√£o para uma normal com m√©dia zero e matriz de covari√¢ncia $\sigma^2 Q^{-1}$ [^2]. Esta propriedade √© fundamental para a constru√ß√£o de testes de hip√≥teses em amostras grandes [^2].

**Distribui√ß√µes Assint√≥ticas das Estat√≠sticas de Teste:**
Sob a assun√ß√£o 8.3 e sob a hip√≥tese nula, as estat√≠sticas t e F convergem para as mesmas distribui√ß√µes assint√≥ticas do caso gaussiano [^2]:
*   A estat√≠stica $t$ converge em distribui√ß√£o para uma normal padr√£o: $t \xrightarrow{d} N(0, 1)$ [^2].
*   A estat√≠stica $F$ converge em distribui√ß√£o para uma distribui√ß√£o $\chi^2$ com $m$ graus de liberdade: $mF \xrightarrow{d} \chi^2(m)$, onde $m$ √© o n√∫mero de restri√ß√µes lineares impostas pela hip√≥tese nula [^2].

**Teorema 2.1:**  *Sob a assun√ß√£o 8.3 e a hip√≥tese nula $H_0: \beta_j = \beta_{j0}$, a estat√≠stica t converge em distribui√ß√£o para uma distribui√ß√£o normal padr√£o.*

*Prova:*
I. A estat√≠stica $t$ √© definida por $t = \frac{b_j - \beta_j}{\sqrt{s^2 (X'X)^{-1}_{jj}}}$.
II. Sabemos que $\sqrt{T}(b_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1})$ e $s^2 \xrightarrow{p} \sigma^2$ e $\frac{X'X}{T} \xrightarrow{p} Q$.
III. Podemos reescrever a estat√≠stica t como $t = \frac{\sqrt{T}(b_j - \beta_j)}{\sqrt{s^2 (\frac{X'X}{T})^{-1}_{jj}}} $.
IV. Usando o resultado de converg√™ncia em distribui√ß√£o e probabilidade, $t \xrightarrow{d} \frac{N(0, \sigma^2 Q^{-1}_{jj})}{\sqrt{\sigma^2 Q^{-1}_{jj}}} = N(0, 1)$. Portanto, a estat√≠stica t converge em distribui√ß√£o para uma normal padr√£o. $\blacksquare$

**Teorema 2.2:** *Sob a assun√ß√£o 8.3 e a hip√≥tese nula $H_0: R\beta = r$, a estat√≠stica $F$ converge em distribui√ß√£o para uma distribui√ß√£o $\chi^2$ com m graus de liberdade, quando multiplicada por $m$, onde m √© o n√∫mero de restri√ß√µes lineares.*

*Prova:*
I. A estat√≠stica F √© dada por $F = \frac{(R b_T - r)'(R(X'X)^{-1}R')^{-1}(R b_T - r)/m}{RSS/(T-k)}$
II.  Sabemos que $ \sqrt{T}(R b_T - r) \xrightarrow{d} N(0, \sigma^2 R Q^{-1} R')$ e $s^2 \xrightarrow{p} \sigma^2$, onde $m$ √© o n√∫mero de restri√ß√µes lineares.
III. Podemos reescrever a estat√≠stica $F$ como  $mF = \frac{T(R b_T - r)'(R(X'X/T)^{-1}R')^{-1}(R b_T - r)}{ s^2}$.
IV.  Usando as propriedades de converg√™ncia,  $T(R b_T - r)'(R(X'X/T)^{-1}R')^{-1}(R b_T - r) \xrightarrow{d} \chi^2(m)$.
V.   Portanto $mF \xrightarrow{d} \chi^2(m)$, concluindo a prova. $\blacksquare$

**Corol√°rio 2.1:** *Sob as condi√ß√µes da Assun√ß√£o 8.3, o estimador da vari√¢ncia dos erros, $s^2$, √© um estimador consistente para $\sigma^2$. Ou seja, $s^2 \xrightarrow{p} \sigma^2$.*

*Prova:*
I. Sabemos que $s^2 = \frac{RSS}{T-k}$, onde $RSS = u'M_X u$, e $M_X$ √© a matriz de proje√ß√£o ortogonal.
II. Assim, $s^2 = \frac{u'M_X u}{T-k} = \frac{u'u - u'X(X'X)^{-1}X'u}{T-k} = \frac{\sum_{t=1}^T u_t^2 - \sum_{t=1}^T \hat{u}_t^2}{T-k}$.
III. Como $u_t$ s√£o *i.i.d* com vari√¢ncia $\sigma^2$, $\frac{1}{T} \sum_{t=1}^T u_t^2 \xrightarrow{p} \sigma^2$.
IV. Tamb√©m sabemos que $\frac{1}{T} \sum_{t=1}^T \hat{u}_t^2 \xrightarrow{p} 0$ quando $T \rightarrow \infty$, pois os res√≠duos convergem para 0 em probabilidade.
V. Portanto, $s^2 = \frac{T}{T-k}(\frac{1}{T}\sum_{t=1}^T u_t^2 - \frac{1}{T}\sum_{t=1}^T \hat{u}_t^2) \xrightarrow{p} \sigma^2$.
$\blacksquare$

> üí° **Exemplo Num√©rico:** Para demonstrar a aplica√ß√£o dos resultados assint√≥ticos, vamos simular dados, calcular o estimador OLS, a estat√≠stica t e o seu p-valor e verificar o comportamento da distribui√ß√£o do estimador √† medida que o tamanho da amostra cresce.
>
> ```python
> import numpy as np
> import pandas as pd
> import matplotlib.pyplot as plt
> from scipy import stats
>
> # Par√¢metros do modelo
> beta_0 = 2
> beta_1 = 1.5
> true_beta = np.array([beta_0, beta_1])
> sample_sizes = [50, 100, 500, 1000, 5000]
>
> estimated_betas = {size: [] for size in sample_sizes}
>
> for size in sample_sizes:
>    for _ in range(50): # 50 simula√ß√µes
>        x = np.random.rand(size) * 10
>        u = np.random.standard_t(5, size) # Erros n√£o gaussianos
>        y = beta_0 + beta_1 * x + u
>
>        X = np.column_stack((np.ones(size), x))
>        b = np.linalg.inv(X.T @ X) @ X.T @ y
>        estimated_betas[size].append(b)
>
> # Plotando a estimativa de beta1 em fun√ß√£o do tamanho da amostra
> plt.figure(figsize=(12, 6))
> for size, betas in estimated_betas.items():
>    beta1_values = [b[1] for b in betas]
>    plt.scatter([size] * len(beta1_values), beta1_values, label=f'Tamanho da Amostra: {size}', alpha=0.6)
>
> plt.axhline(y=beta_1, color='r', linestyle='-', label='Valor real de beta1')
> plt.xlabel('Tamanho da Amostra')
> plt.ylabel('Estimativa de Beta1')
> plt.title('Consist√™ncia do Estimador OLS para Erros N√£o Gaussianos')
> plt.legend()
> plt.show()
>
> # Calculando os res√≠duos, a estat√≠stica t e o p-valor para cada simula√ß√£o
> t_stats = {size: [] for size in sample_sizes}
> p_values = {size: [] for size in sample_sizes}
> for size in sample_sizes:
>    for _ in range(50):
>        x = np.random.rand(size) * 10
>        u = np.random.standard_t(5, size) # Erros n√£o gaussianos
>        y = beta_0 + beta_1 * x + u
>
>        X = np.column_stack((np.ones(size), x))
>        b = np.linalg.inv(X.T @ X) @ X.T @ y
>
>        # Estimando a vari√¢ncia do erro
>        y_hat = X @ b
>        residuals = y - y_hat
>        RSS = np.sum(residuals**2)
>        sigma2_hat = RSS / (len(y) - X.shape[1])
>
>        # Calculando o erro padr√£o de b1
>        var_b = np.diag(sigma2_hat * np.linalg.inv(X.T @ X))
>        se_b = np.sqrt(var_b)
>
>        # Calculando a estat√≠stica t
>        t_statistic = (b[1] - beta_1) / se_b[1]
>
>        # Calculando o p-valor
>        p_value = 2 * (1 - stats.norm.cdf(abs(t_statistic)))
>        t_stats[size].append(t_statistic)
>        p_values[size].append(p_value)
>
>
> # Verificando a distribui√ß√£o da estat√≠stica t para diferentes tamanhos de amostra
> plt.figure(figsize=(12, 8))
> for size in sample_sizes:
>    plt.subplot(2, 3, sample_sizes.index(size) + 1)
>    plt.hist(t_stats[size], bins=20, label=f"T = {size}")
>    plt.title(f"Distribui√ß√£o de t (T={size})")
>    plt.xlabel("Valores da Estat√≠stica t")
>    plt.ylabel("Frequ√™ncia")
>    plt.legend()
>
> plt.tight_layout()
> plt.show()
> ```
>
> Este exemplo demonstra que √† medida que o tamanho da amostra aumenta, as distribui√ß√µes das estat√≠sticas t se aproximam da distribui√ß√£o normal padr√£o. Isto demonstra a converg√™ncia assint√≥tica da estat√≠stica de teste, e refor√ßa a import√¢ncia dos resultados assint√≥ticos para realizar a infer√™ncia estat√≠stica v√°lida sob a Assun√ß√£o 8.3.

### Conclus√£o
Neste cap√≠tulo, exploramos o comportamento do estimador OLS em um cen√°rio onde os regressores s√£o estoc√°sticos e os erros s√£o n√£o-gaussianos. Embora o estimador permane√ßa n√£o enviesado, a sua distribui√ß√£o, em amostras pequenas, n√£o √© mais Gaussiana, invalidando os testes de hip√≥tese padr√£o baseados na distribui√ß√£o t e F. Ao inv√©s disso, recorremos aos resultados assint√≥ticos, que s√£o v√°lidos para amostras grandes. Em particular, estabelecemos que a estat√≠stica $t$ converge para uma distribui√ß√£o normal padr√£o, e a estat√≠stica $F$, multiplicada pelo n√∫mero de restri√ß√µes $m$, converge para uma distribui√ß√£o $\chi^2$ com $m$ graus de liberdade sob a hip√≥tese nula. Estes resultados nos permitem realizar testes de hip√≥tese v√°lidos em grandes amostras, mesmo quando os erros n√£o s√£o Gaussianos, sob as condi√ß√µes da Assun√ß√£o 8.3. O pr√≥ximo passo √© explorar cen√°rios onde a suposi√ß√£o de independ√™ncia entre regressores e erros n√£o se mant√©m.

### Refer√™ncias
[^1]: Refere-se a conte√∫dos e conceitos abordados anteriormente no contexto.
[^2]: Trecho retirado diretamente do texto original, explicitamente indicando as propriedades, teoremas e equa√ß√µes abordadas no caso de regressores estoc√°sticos n√£o-gaussianos.
<!-- END -->
