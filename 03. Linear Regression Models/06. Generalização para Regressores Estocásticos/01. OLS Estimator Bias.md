## Regress√£o Linear com Regressores Estoc√°sticos Independentes dos Erros
### Introdu√ß√£o
Este cap√≠tulo explora o modelo de regress√£o linear sob diferentes condi√ß√µes, expandindo os resultados apresentados anteriormente [^1]. Inicialmente, assumimos que os regressores eram determin√≠sticos e os erros eram i.i.d. Gaussianos [^1]. Agora, relaxamos essa suposi√ß√£o para considerar regressores que s√£o estoc√°sticos, mas independentes dos erros. Este cen√°rio √© comum em an√°lises de s√©ries temporais e requer uma abordagem mais cuidadosa da infer√™ncia estat√≠stica. Este t√≥pico se baseia nas propriedades dos estimadores OLS e testes estat√≠sticos explorados anteriormente [^1].

### Conceitos Fundamentais
#### Regressores Estoc√°sticos e Independ√™ncia dos Erros
Em muitos casos pr√°ticos, especialmente em s√©ries temporais, os regressores n√£o s√£o determin√≠sticos, mas sim vari√°veis aleat√≥rias. O primeiro caso a considerar √© aquele em que os regressores (representados por X) s√£o estoc√°sticos, mas independentes dos erros (representados por u). Formalmente, assumimos que X √© estoc√°stico e independente de $u_t$ para todos os $t$ e $s$. Al√©m disso, assumimos que $u_t$ √© *i.i.d* com m√©dia 0 e vari√¢ncia $\sigma^2$ [^2]. Em outras palavras,  $u_t \sim i.i.d. \ N(0, \sigma^2)$ [^2]. Esta condi√ß√£o de independ√™ncia entre regressores e erros √© crucial para garantir certas propriedades do estimador de M√≠nimos Quadrados Ordin√°rios (OLS).

**Proposi√ß√£o 1** *A condi√ß√£o de independ√™ncia entre regressores e erros implica que a esperan√ßa condicional do erro dado os regressores √© zero. Ou seja, $E(u_t | X) = 0$ para todos os t.*

*Prova:*
I. Dada a independ√™ncia entre $u_t$ e $X$, a esperan√ßa condicional de $u_t$ dado $X$ √© igual √† esperan√ßa incondicional de $u_t$. Isso √©: $E(u_t | X) = E(u_t)$.
II. Como assumimos que $E(u_t) = 0$, segue-se que $E(u_t | X) = 0$.
III. Portanto, a independ√™ncia entre regressores e erros implica que a esperan√ßa condicional do erro dado os regressores √© zero. ‚ñ†

> üí° **Exemplo Num√©rico:** Imagine um modelo de regress√£o onde o consumo ($y_t$) √© explicado pela renda ($x_t$) e um termo de erro ($u_t$). Em um cen√°rio ideal, assumimos que o erro ($u_t$), que pode incluir fatores n√£o observados que afetam o consumo, √© independente da renda ($x_t$). Matematicamente, isso significa que $E(u_t|x_t) = E(u_t) = 0$. Ou seja, n√£o importa qual seja o n√≠vel de renda, o erro m√©dio n√£o √© sistematicamente diferente de zero.
>
> ```mermaid
> graph LR
>     A[Renda (xt)] -->|Independente| B(Erro (ut))
>     B --> C[Consumo (yt)]
>     A --> C
> ```
>
> Se a renda for muito alta ou muito baixa, n√£o esperamos que isso cause um aumento ou diminui√ß√£o sistem√°tica nos fatores n√£o observados que afetam o consumo.

#### Propriedades do Estimador OLS sob Regressores Estoc√°sticos Independentes
Sob esta nova suposi√ß√£o, muitas das propriedades derivadas para regressores determin√≠sticos ainda se mant√™m. Por exemplo, o estimador OLS do vetor de coeficientes, $b$, continua n√£o enviesado [^2]. Matematicamente, isso √© demonstrado tomando as esperan√ßas de [8.1.12] e explorando a independ√™ncia entre X e u [^2]:
$$ E(b) = \beta + \{E[(X'X)^{-1}X']\} \{E(u)\} = \beta. $$
Este resultado mostra que, apesar da estocasticidade de X, o estimador OLS ainda fornece uma estimativa n√£o enviesada do verdadeiro par√¢metro $\beta$ [^2].

> üí° **Exemplo Num√©rico:** Considere um modelo de regress√£o simples com um regressor estoc√°stico: $y_t = \beta_0 + \beta_1 x_t + u_t$. Assumindo que $x_t$ √© independente de $u_t$, o estimador OLS para $\beta_1$, denotado por $b_1$, √© n√£o enviesado, ou seja, $E(b_1) = \beta_1$.
>
> Vamos supor que temos os seguintes dados simulados para 5 observa√ß√µes:
>
> ```python
> import numpy as np
>
> # Dados simulados
> np.random.seed(42)
> x = np.random.rand(5) * 10  # Regressor estoc√°stico
> u = np.random.normal(0, 1, 5)  # Erros i.i.d.
> beta_0 = 2
> beta_1 = 1.5
> y = beta_0 + beta_1 * x + u
>
> # Construindo a matriz X
> X = np.column_stack((np.ones(5), x))
>
> # Calculando o estimador OLS
> b = np.linalg.inv(X.T @ X) @ X.T @ y
>
> print(f"Estimativa de b0 (intercepto): {b[0]:.4f}")
> print(f"Estimativa de b1 (coeficiente de x): {b[1]:.4f}")
> print(f"Verdadeiro valor de beta_1: {beta_1}")
>
> # Repetindo a simula√ß√£o v√°rias vezes para verificar a m√©dia de b_1
> num_simulations = 1000
> b1_estimates = []
> for _ in range(num_simulations):
>    x_sim = np.random.rand(5) * 10
>    u_sim = np.random.normal(0, 1, 5)
>    y_sim = beta_0 + beta_1 * x_sim + u_sim
>    X_sim = np.column_stack((np.ones(5), x_sim))
>    b_sim = np.linalg.inv(X_sim.T @ X_sim) @ X_sim.T @ y_sim
>    b1_estimates.append(b_sim[1])
>
> print(f"M√©dia das estimativas de b1: {np.mean(b1_estimates):.4f}")
> ```
>
> Este exemplo mostra que, em uma √∫nica amostra, a estimativa de $b_1$ pode ser diferente do valor verdadeiro de $\beta_1$. No entanto, em m√©dia, a estimativa converge para o valor real quando repetimos a simula√ß√£o muitas vezes, o que ilustra a propriedade de n√£o-tendenciosidade do estimador OLS.

#### Distribui√ß√£o Condicional e Incondicional do Estimador OLS
A distribui√ß√£o dos testes estat√≠sticos sob regressores estoc√°sticos independentes requer uma abordagem em duas etapas [^2]. Primeiro, avaliamos a distribui√ß√£o do estimador **condicional** em X, o que significa tratar X como determin√≠stico na primeira etapa [^2]. Assim como na an√°lise anterior com regressores determin√≠sticos, a distribui√ß√£o **condicional** do estimador b √© dada por:
$$ b|X \sim N(\beta, \sigma^2(X'X)^{-1}). $$
Esta distribui√ß√£o condicional √© id√™ntica √†quela obtida com regressores determin√≠sticos [^2]. No entanto, a segunda etapa envolve a integra√ß√£o da densidade condicional sobre a densidade de X para obter a distribui√ß√£o **incondicional** de b [^2].
A distribui√ß√£o incondicional de $b$ n√£o √© mais Gaussiana sob a Assun√ß√£o 8.2, demonstrando que a distribui√ß√£o condicional de $b$ pode ser diferente da sua distribui√ß√£o incondicional [^2].

**Lema 1** *A n√£o-gaussianidade da distribui√ß√£o incondicional de $b$ surge da depend√™ncia entre $(X'X)^{-1}$ e $X$, que √© uma vari√°vel aleat√≥ria.*

*Prova:*
I. O estimador OLS √© dado por: $b = (X'X)^{-1}X'y$. Substituindo $y = X\beta + u$, temos: $b = (X'X)^{-1}X'(X\beta + u) = \beta + (X'X)^{-1}X'u$.
II. A distribui√ß√£o de $b$ depende de $(X'X)^{-1}X'u$. Se $X$ fosse determin√≠stico, $(X'X)^{-1}X'$ seria uma matriz constante e a distribui√ß√£o de $b$ seria uma combina√ß√£o linear de $u$, que √© normal.
III. No entanto, como $X$ √© estoc√°stico, $(X'X)^{-1}$ √© uma vari√°vel aleat√≥ria que depende de $X$. A rela√ß√£o n√£o √© linear e introduz n√£o-gaussianidade na distribui√ß√£o incondicional de $b$.
IV. Portanto, a n√£o-gaussianidade da distribui√ß√£o incondicional de $b$ surge da depend√™ncia entre $(X'X)^{-1}$ e $X$, que √© uma vari√°vel aleat√≥ria. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere a distribui√ß√£o condicional de $b_1$ (o estimador de $\beta_1$) dada uma matriz $X$ espec√≠fica. Suponha que, com um dado $X$, temos $b_1|X \sim N(\beta_1, \sigma^2 (X'X)^{-1}_{22})$, onde $(X'X)^{-1}_{22}$ √© o elemento na linha 2 e coluna 2 de $(X'X)^{-1}$. Isso significa que a distribui√ß√£o de $b_1$ √© uma normal com m√©dia $\beta_1$ e vari√¢ncia $\sigma^2(X'X)^{-1}_{22}$, condicionada a essa matriz $X$ espec√≠fica.
>
> Agora, considere que X pode variar, e cada X leva a uma matriz $(X'X)^{-1}$ diferente, e por sua vez, a uma vari√¢ncia diferente para o estimador OLS. A distribui√ß√£o incondicional de $b_1$ ser√° a m√©dia ponderada de todas as distribui√ß√µes condicionais, o que introduz uma mistura de distribui√ß√µes normais, que em geral, n√£o √© normal. Portanto, a distribui√ß√£o incondicional de $b_1$ n√£o √© mais Gaussiana.

#### Distribui√ß√µes de Testes Estat√≠sticos
Embora a distribui√ß√£o incondicional de $b$ n√£o seja Gaussiana, a distribui√ß√£o de algumas estat√≠sticas de teste √© **incondicionalmente** a mesma que no caso de regressores determin√≠sticos. Especificamente, a estat√≠stica $RSS/ \sigma^2$ segue uma distribui√ß√£o $\chi^2$ com $T-k$ graus de liberdade, mesmo com regressores estoc√°sticos independentes [^2]:
$$ RSS/ \sigma^2 \sim \chi^2(T-k). $$
Adicionalmente, a estat√≠stica t e a estat√≠stica F continuam a ter as mesmas distribui√ß√µes sob a hip√≥tese nula (distribui√ß√£o t com $T-k$ graus de liberdade e distribui√ß√£o F com $m$ e $T-k$ graus de liberdade, respectivamente) [^2]. Isso ocorre porque essas distribui√ß√µes s√£o derivadas *condicionalmente* em X e, como a distribui√ß√£o *condicional* √© igual √† obtida no caso determin√≠stico, ent√£o as distribui√ß√µes *incondicionais* ser√£o as mesmas [^2]. Portanto, os testes de hip√≥teses usando essas estat√≠sticas s√£o v√°lidos, mesmo com regressores estoc√°sticos independentes dos erros.

**Teorema 1.1**  *A estat√≠stica t, definida como $t = \frac{b_j - \beta_j}{se(b_j)}$, segue uma distribui√ß√£o t de Student com $T-k$ graus de liberdade sob a hip√≥tese nula $H_0: \beta_j = \beta_{j0}$, tanto condicionalmente em X quanto incondicionalmente, quando os regressores s√£o estoc√°sticos e independentes dos erros.*

*Prova:*
I. A estat√≠stica t √© definida como $t = \frac{b_j - \beta_j}{se(b_j)}$, onde $b_j$ √© o j-√©simo coeficiente estimado, $\beta_j$ √© o verdadeiro valor do coeficiente sob a hip√≥tese nula, e $se(b_j)$ √© o erro padr√£o do estimador.
II. Condicionalmente em $X$, o estimador OLS $b$ segue uma distribui√ß√£o normal: $b|X \sim N(\beta, \sigma^2(X'X)^{-1})$. Portanto, $(b_j - \beta_j)$ √© uma vari√°vel normal com m√©dia zero. O denominador, $se(b_j)$, √© uma estimativa do desvio padr√£o de $b_j$, com $se(b_j)^2 = \frac{\sigma^2 (X'X)^{-1}_{jj}}{RSS/(T-k)}$.
III. A estat√≠stica $\frac{RSS}{\sigma^2}$ segue uma distribui√ß√£o $\chi^2$ com $T-k$ graus de liberdade.
IV. Dado que a estat√≠stica t √© uma vari√°vel normal dividida pela raiz quadrada de uma vari√°vel qui-quadrado, onde cada um √© independente, condicionalmente em X, a estat√≠stica t segue uma distribui√ß√£o t de Student com $T-k$ graus de liberdade.
V. A distribui√ß√£o da estat√≠stica t *condicional* em X n√£o depende de X, portanto, a distribui√ß√£o *incondicional* √© a mesma, ou seja, segue tamb√©m uma distribui√ß√£o t de Student com $T-k$ graus de liberdade.
VI. Portanto, a estat√≠stica t segue uma distribui√ß√£o t de Student com $T-k$ graus de liberdade, tanto condicionalmente em X quanto incondicionalmente, quando os regressores s√£o estoc√°sticos e independentes dos erros. ‚ñ†

> üí° **Exemplo Num√©rico:** Vamos calcular a estat√≠stica t para testar a hip√≥tese $H_0: \beta_1 = 0$ usando os dados simulados anteriormente.
>
> ```python
> import numpy as np
> from scipy import stats
>
> # Dados simulados (usando os mesmos dados do exemplo anterior)
> np.random.seed(42)
> x = np.random.rand(5) * 10
> u = np.random.normal(0, 1, 5)
> beta_0 = 2
> beta_1 = 1.5
> y = beta_0 + beta_1 * x + u
>
> # Construindo a matriz X
> X = np.column_stack((np.ones(5), x))
>
> # Calculando o estimador OLS
> b = np.linalg.inv(X.T @ X) @ X.T @ y
>
> # Calculando os res√≠duos
> y_hat = X @ b
> residuals = y - y_hat
>
> # Calculando o RSS (Soma dos Quadrados dos Res√≠duos)
> RSS = np.sum(residuals**2)
>
> # Calculando a vari√¢ncia do erro
> sigma2_hat = RSS / (len(y) - X.shape[1])
>
> # Calculando o erro padr√£o de b1
> var_b = np.diag(sigma2_hat * np.linalg.inv(X.T @ X))
> se_b = np.sqrt(var_b)
>
> # Calculando a estat√≠stica t
> t_statistic = (b[1] - 0) / se_b[1] # Testando H0: beta1 = 0
>
> # Calculando o p-valor
> degrees_freedom = len(y) - X.shape[1]
> p_value = 2 * (1 - stats.t.cdf(abs(t_statistic), degrees_freedom))
>
> print(f"Estat√≠stica t: {t_statistic:.4f}")
> print(f"P-valor: {p_value:.4f}")
> print(f"Graus de liberdade: {degrees_freedom}")
>
> # Interpretando o resultado
> alpha = 0.05 # N√≠vel de signific√¢ncia
> if p_value < alpha:
>     print("Rejeitamos a hip√≥tese nula. H√° evid√™ncia estat√≠stica de que beta1 √© diferente de zero.")
> else:
>    print("N√£o rejeitamos a hip√≥tese nula. N√£o h√° evid√™ncia estat√≠stica de que beta1 √© diferente de zero.")
> ```
>
> Neste exemplo, a estat√≠stica t e o p-valor nos permitem testar a signific√¢ncia estat√≠stica do coeficiente $\beta_1$. O resultado ser√° usado para tomar decis√µes sobre a inclus√£o do regressor $x_t$ no modelo. Se rejeitarmos a hip√≥tese nula, significa que o regressor $x_t$ tem um efeito estatisticamente significativo sobre a vari√°vel dependente $y_t$.

#### Implica√ß√µes da Independ√™ncia
√â crucial notar que todos os resultados mencionados acima se baseiam na hip√≥tese de que os regressores s√£o independentes dos erros. Se essa condi√ß√£o n√£o for satisfeita, as propriedades do estimador OLS podem mudar significativamente, levando a vieses e distribui√ß√µes de teste incorretas.

**Observa√ß√£o 1** *A independ√™ncia entre regressores e erros √© uma condi√ß√£o suficiente, mas n√£o necess√°ria para a n√£o-tendenciosidade do estimador OLS. A condi√ß√£o fundamental √© que $E(u|X)=0$.*

### Conclus√£o
Ao considerar regressores estoc√°sticos, mas independentes dos erros, observamos que muitas propriedades importantes do estimador OLS permanecem v√°lidas. O estimador continua n√£o enviesado, e a estat√≠stica $RSS/\sigma^2$, a estat√≠stica $t$, e a estat√≠stica $F$, possuem as mesmas distribui√ß√µes das estat√≠sticas do modelo com regressores determin√≠sticos [^2]. √â crucial entender as distribui√ß√µes condicionais e incondicionais, para aplicar os testes de hip√≥tese adequadamente [^2]. Esta generaliza√ß√£o √© essencial para a an√°lise de s√©ries temporais e outros cen√°rios onde os regressores s√£o inerentemente vari√°veis aleat√≥rias. A pr√≥xima etapa envolve a explora√ß√£o dos casos em que tanto os regressores quanto os erros n√£o s√£o Gaussianos, ou onde h√° depend√™ncia entre regressores e erros [^3].

### Refer√™ncias
[^1]: Refere-se a conte√∫dos e conceitos abordados anteriormente no contexto.
[^2]: Trecho retirado diretamente do texto original, explicitamente indicando as propriedades e equa√ß√µes abordadas no caso de regressores estoc√°sticos independentes dos erros.
[^3]: Refere-se a t√≥picos que ser√£o explorados posteriormente no texto.
<!-- END -->
