## Correla√ß√£o Serial com Vari√°veis End√≥genas Defasadas: Estima√ß√£o via Forma Reduzida
### Introdu√ß√£o
Em continuidade aos t√≥picos anteriores sobre modelos de regress√£o linear, e em particular, ap√≥s analisarmos os casos onde os erros apresentam autocorrela√ß√£o, este cap√≠tulo aborda uma situa√ß√£o mais complexa: a estima√ß√£o de modelos que combinam erros serially correlacionados com a presen√ßa de vari√°veis end√≥genas defasadas. Como visto anteriormente, a presen√ßa de autocorrela√ß√£o nos erros, em conjunto com regressores defasados, viola as premissas b√°sicas para a consist√™ncia do estimador OLS padr√£o [^1], [^2]. Isso ocorre porque, nestes casos, os regressores tornam-se correlacionados com os erros, invalidando as propriedades desej√°veis do estimador. Este cap√≠tulo ir√° detalhar uma estrat√©gia espec√≠fica para lidar com esta quest√£o: a estima√ß√£o atrav√©s de uma forma reduzida do modelo, que, atrav√©s de uma reparametriza√ß√£o inteligente, transforma o modelo em um formato onde o OLS se torna um estimador consistente.

### Conceitos Fundamentais
Como explicitado anteriormente [^1], a viola√ß√£o da premissa de que os erros n√£o s√£o correlacionados com os regressores leva a estimadores OLS enviesados e inconsistentes. Em modelos com erros autocorrelacionados e vari√°veis end√≥genas defasadas, esta correla√ß√£o torna-se problem√°tica. Para ilustrar, considere um modelo onde a vari√°vel dependente $y_t$ √© afetada pela sua pr√≥pria defasagem e por um regressor $x_t$, com um termo de erro $u_t$ que segue um processo AR(1):

$$ y_t = \beta y_{t-1} + \gamma x_t + u_t $$
onde
$$ u_t = \rho u_{t-1} + \epsilon_t $$
com $|\rho| < 1$ e $\epsilon_t$ um ru√≠do branco Gaussiano [^1].
Neste cen√°rio, $y_{t-1}$ √© correlacionada com $u_t$ (via $u_{t-1}$), violando a condi√ß√£o de exogeneidade para o uso do OLS. Em particular, como discutido em [^1], o estimador de $\rho$ baseado em res√≠duos OLS n√£o √© consistente.

> üí° **Exemplo Num√©rico:** Para ilustrar, vamos supor que temos os seguintes dados simulados para $y_t$ e $x_t$ em 10 per√≠odos, onde $\beta = 0.5$, $\gamma = 0.8$, e $\rho = 0.7$. O termo de erro $\epsilon_t$ segue uma distribui√ß√£o normal com m√©dia 0 e desvio padr√£o 1.
```python
import numpy as np
import pandas as pd

np.random.seed(42)
T = 10
rho = 0.7
beta = 0.5
gamma = 0.8
epsilon = np.random.normal(0, 1, T)
u = np.zeros(T)
y = np.zeros(T)
x = np.random.normal(0, 2, T) # Regressor ex√≥geno

# Simulating u_t
u[0] = epsilon[0]
for t in range(1,T):
    u[t] = rho * u[t-1] + epsilon[t]

# Simulating y_t
y[0] = beta * 0 + gamma * x[0] + u[0]  # Assume y_0 = 0
for t in range(1,T):
   y[t] = beta * y[t-1] + gamma * x[t] + u[t]
data = pd.DataFrame({'y': y, 'x':x})
print(data)
```
Este c√≥digo ir√° gerar uma tabela com valores simulados de $y_t$ e $x_t$ que ilustram o tipo de dados com os quais estamos lidando. Note que, devido √† autocorrela√ß√£o no termo de erro e a defasagem de $y_t$, a premissa de exogeneidade para o uso do OLS √© violada.
<!-- Diagrama para ilustrar a correla√ß√£o -->
```mermaid
graph LR
    u_t_minus_1 --> u_t
    u_t --> y_t
    y_t_minus_1 --> y_t
    x_t --> y_t
    style u_t fill:#f9f,stroke:#333,stroke-width:2px
    style y_t_minus_1 fill:#ccf,stroke:#333,stroke-width:2px
```
Este diagrama mostra que $u_t$ influencia $y_t$, e $u_{t-1}$ influencia $u_t$, criando uma correla√ß√£o entre $y_{t-1}$ e $u_t$.

**Observa√ß√£o 1** √â importante notar que a endogeneidade de $y_{t-1}$ n√£o decorre diretamente do fato de ser uma vari√°vel defasada, mas sim da sua correla√ß√£o com o termo de erro $u_t$. Caso o termo de erro n√£o fosse autocorrelacionado (i.e., $\rho=0$), ent√£o $y_{t-1}$ seria ex√≥gena no modelo original.

Para contornar este problema, podemos recorrer a uma forma reduzida do modelo. Multiplicando a equa√ß√£o original por $(1 - \rho L)$, onde $L$ √© o operador de defasagem, obtemos:
<!-- Adicionando prova da equa√ß√£o da forma reduzida -->
*Prova:*
I. Come√ßamos com a equa√ß√£o original:
$$ y_t = \beta y_{t-1} + \gamma x_t + u_t $$
e a equa√ß√£o do termo de erro:
$$ u_t = \rho u_{t-1} + \epsilon_t $$

II. Defasando a primeira equa√ß√£o um per√≠odo, temos:
$$ y_{t-1} = \beta y_{t-2} + \gamma x_{t-1} + u_{t-1} $$

III. Multiplicamos a equa√ß√£o do termo de erro por $\rho$ e defasamos:
$$ \rho u_{t-1} = \rho^2 u_{t-2} + \rho \epsilon_{t-1} $$

IV. Subtraindo $\rho y_{t-1}$ da equa√ß√£o original, e usando o fato que $\rho u_{t-1} = u_t - \epsilon_t$, obtemos:
$$ y_t - \rho y_{t-1} = \beta y_{t-1} + \gamma x_t + u_t - \rho(\beta y_{t-2} + \gamma x_{t-1} + u_{t-1}) $$
$$ y_t - \rho y_{t-1} = \beta y_{t-1} + \gamma x_t + u_t - \rho \beta y_{t-2} - \rho \gamma x_{t-1} - \rho u_{t-1} $$
$$ y_t - \rho y_{t-1} = \beta y_{t-1} + \gamma x_t + \rho u_{t-1} + \epsilon_t - \rho \beta y_{t-2} - \rho \gamma x_{t-1} - \rho u_{t-1} $$
$$ y_t - \rho y_{t-1} = \beta y_{t-1} + \gamma x_t + \epsilon_t  - \rho \beta y_{t-2} - \rho \gamma x_{t-1}  $$

V. Reorganizando os termos, temos:
$$ y_t = (\rho + \beta) y_{t-1} - \rho \beta y_{t-2} + \gamma x_t - \rho \gamma x_{t-1} + \epsilon_t $$
\hfill $\square$
<!-- Fim da prova -->

$$ y_t = (\rho + \beta) y_{t-1} - \rho \beta y_{t-2} + \gamma x_t - \rho \gamma x_{t-1} + \epsilon_t $$
Esta equa√ß√£o pode ser reescrita como:

$$ y_t = \alpha_1 y_{t-1} + \alpha_2 y_{t-2} + \alpha_3 x_t + \alpha_4 x_{t-1} + \epsilon_t $$
onde
$$ \alpha_1 = \rho + \beta, \quad \alpha_2 = -\rho \beta, \quad \alpha_3 = \gamma, \quad \alpha_4 = -\rho \gamma $$
[8.3.23] [^1]

Este modelo reparametrizado, embora n√£o linear nas vari√°veis originais, possui a propriedade crucial de que o termo de erro $\epsilon_t$ √© n√£o correlacionado com os regressores. Desta forma, a estima√ß√£o por OLS desta forma reduzida resulta em estimadores consistentes dos coeficientes $\alpha_i$. Uma vez que os coeficientes $\alpha_i$ s√£o estimados consistentemente, pode-se obter uma estimativa de $\rho$ atrav√©s da rela√ß√£o $-\frac{\alpha_4}{\alpha_3}$ ou $-\frac{\alpha_2}{\alpha_1}$.

> üí° **Exemplo Num√©rico:** Continuando com o exemplo anterior, vamos gerar as vari√°veis defasadas e estimar a forma reduzida usando os dados simulados acima.
```python
from sklearn.linear_model import LinearRegression
# Generate lagged variables
data['y_lag1'] = data['y'].shift(1)
data['y_lag2'] = data['y'].shift(2)
data['x_lag1'] = data['x'].shift(1)
data = data.dropna()
# Prepare data for regression
X = data[['y_lag1', 'y_lag2', 'x', 'x_lag1']]
y = data['y']

# Fit OLS on the reduced form
model_reduced_form = LinearRegression()
model_reduced_form.fit(X, y)
alpha_1 = model_reduced_form.coef_[0]
alpha_2 = model_reduced_form.coef_[1]
alpha_3 = model_reduced_form.coef_[2]
alpha_4 = model_reduced_form.coef_[3]

print(f"Estimated alpha_1: {alpha_1:.3f}")
print(f"Estimated alpha_2: {alpha_2:.3f}")
print(f"Estimated alpha_3: {alpha_3:.3f}")
print(f"Estimated alpha_4: {alpha_4:.3f}")

# Estimate rho using the relationships
rho_est_1 = -alpha_4/alpha_3
rho_est_2 = -alpha_2/alpha_1
print(f"Estimated rho (from -alpha_4/alpha_3): {rho_est_1:.3f}")
print(f"Estimated rho (from -alpha_2/alpha_1): {rho_est_2:.3f}")

```
Este c√≥digo primeiro cria as vari√°veis defasadas ($y_{t-1}$, $y_{t-2}$, e $x_{t-1}$) e depois estima os coeficientes $\alpha_i$. As estimativas de $\rho$ usando as duas rela√ß√µes s√£o ent√£o calculadas e mostradas.

**Proposi√ß√£o 1** As duas formas de obter $\rho$,  $-\frac{\alpha_4}{\alpha_3}$ e  $-\frac{\alpha_2}{\alpha_1}$, s√£o equivalentes.

*Prova:*
Das defini√ß√µes dos $\alpha_i$, temos:
$$-\frac{\alpha_4}{\alpha_3} = -\frac{-\rho \gamma}{\gamma} = \rho$$
$$-\frac{\alpha_2}{\alpha_1} = -\frac{-\rho \beta}{\rho + \beta} = \frac{\rho \beta}{\rho + \beta}$$
A igualdade  $-\frac{\alpha_4}{\alpha_3}$ =  $-\frac{\alpha_2}{\alpha_1}$  implica  $\rho = \frac{\rho \beta}{\rho + \beta}$, ou seja,  $\rho(\rho + \beta) = \rho \beta$.  Se $\rho \neq 0$, ent√£o $\rho + \beta = \beta$ e portanto $\rho=0$, uma contradi√ß√£o.  Portanto, essas duas express√µes n√£o s√£o equivalentes em geral.  No entanto, se utilizarmos o fato que as estimativas de $\alpha_i$ obtidas por OLS s√£o consistentes, ent√£o a probabilidade de  $-\frac{\hat{\alpha}_4}{\hat{\alpha}_3}$ ser igual a $-\frac{\hat{\alpha}_2}{\hat{\alpha}_1}$ tende a 1 a medida que o tamanho da amostra tende a infinito.
\hfill $\square$
> üí° **Exemplo Num√©rico:** No exemplo anterior, temos $\rho = 0.7$. A partir dos resultados do c√≥digo, podemos verificar se as estimativas de $\rho$ convergem para o valor verdadeiro. Note que as estimativas de $\rho$ podem diferir ligeiramente, especialmente com amostras pequenas, mas, como a prova anterior indica, com amostras maiores ambas as estimativas tendem a convergir para o mesmo valor.

A vantagem desta abordagem √© que o estimador OLS sobre a forma reduzida √© consistente, o que √© crucial para a infer√™ncia estat√≠stica. Contudo, √© importante salientar que esta abordagem introduz restri√ß√µes n√£o lineares entre os coeficientes estimados, como discutido em [^1].

√â importante ressaltar que, a forma reduzida, por si s√≥, apenas contorna o problema da inconsist√™ncia causada pela correla√ß√£o serial e endogeneidade das vari√°veis defasadas. Ela n√£o lida com a possibilidade de erros n√£o serem homoced√°sticos. Para lidar com essa possibilidade, os m√©todos discutidos anteriormente, como os erros padr√£o robustos √† heterocedasticidade, s√£o essenciais [^1].

**Teorema 1** A consist√™ncia dos estimadores OLS na forma reduzida, combinada com as rela√ß√µes entre os coeficientes da forma reduzida e os par√¢metros do modelo original, garante a consist√™ncia dos estimadores dos par√¢metros originais, $\beta$, $\gamma$ e $\rho$.

*Prova*:
I. A consist√™ncia dos estimadores $\hat{\alpha}_1$, $\hat{\alpha}_2$, $\hat{\alpha}_3$, e $\hat{\alpha}_4$ implica que, √† medida que o tamanho da amostra aumenta, esses estimadores convergem em probabilidade para seus verdadeiros valores, $\alpha_1$, $\alpha_2$, $\alpha_3$ e $\alpha_4$ respectivamente.
II. As rela√ß√µes entre os coeficientes da forma reduzida e os par√¢metros originais s√£o:
$$ \alpha_1 = \rho + \beta, \quad \alpha_2 = -\rho \beta, \quad \alpha_3 = \gamma, \quad \alpha_4 = -\rho \gamma $$
III. Usando essas rela√ß√µes, podemos expressar os par√¢metros originais em termos dos coeficientes da forma reduzida:
$$ \hat{\rho} = -\frac{\hat{\alpha}_4}{\hat{\alpha}_3}, \quad \hat{\gamma} = \hat{\alpha}_3 $$
IV. O estimador para $\beta$ √© obtido como
$$ \hat{\beta} = \hat{\alpha}_1 - \hat{\rho} = \hat{\alpha}_1 + \frac{\hat{\alpha}_4}{\hat{\alpha}_3} $$
V.  Como as estimativas de $\alpha_i$ s√£o consistentes, e as opera√ß√µes em III e IV s√£o cont√≠nuas, ent√£o os estimadores para $\rho$, $\beta$ e $\gamma$ s√£o tamb√©m consistentes.
\hfill $\square$

> üí° **Exemplo Num√©rico:** Usando os resultados do exemplo anterior e as rela√ß√µes entre os $\alpha_i$ e os par√¢metros originais $\beta$, $\gamma$ e $\rho$, podemos obter estimativas para os par√¢metros originais:
```python
# Estimating the original parameters
beta_est = alpha_1 - rho_est_1
gamma_est = alpha_3
print(f"Estimated beta: {beta_est:.3f}")
print(f"Estimated gamma: {gamma_est:.3f}")
```
Este c√≥digo calcula as estimativas de $\beta$ e $\gamma$ usando os valores de $\alpha_i$ e $\rho$ obtidos anteriormente, mostrando que √© poss√≠vel obter estimativas consistentes para os par√¢metros de interesse usando a abordagem da forma reduzida.

### Conclus√£o
A estima√ß√£o de modelos com erros serially correlacionados e vari√°veis end√≥genas defasadas requer cuidados especiais. A estima√ß√£o por OLS direta torna-se inconsistente. A reparametriza√ß√£o do modelo numa forma reduzida, em que as vari√°veis defasadas e os seus efeitos s√£o combinados, permite obter um estimador OLS consistente para os par√¢metros. Atrav√©s da rela√ß√£o entre os par√¢metros da forma reduzida e os par√¢metros estruturais, √© poss√≠vel obter estimativas consistentes de todos os par√¢metros de interesse. A abordagem com a forma reduzida √© um m√©todo valioso para lidar com esta complexidade, proporcionando uma base s√≥lida para an√°lise e infer√™ncia em modelos com estas caracter√≠sticas. No entanto, a escolha entre esta abordagem, ou a estima√ß√£o da fun√ß√£o de verossimilhan√ßa (que exige otimiza√ß√£o num√©rica), √© determinada pela necessidade de se obter as melhores propriedades amostrais dos estimadores, dado o conjunto de dados dispon√≠vel [^1].

### Refer√™ncias
[^1]:  *Material fornecido no contexto.*
<!-- END -->
