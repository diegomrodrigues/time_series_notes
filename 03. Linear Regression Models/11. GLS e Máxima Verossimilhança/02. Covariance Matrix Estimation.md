## GLS com Matriz de Covari√¢ncia Desconhecida: Resultados Assint√≥ticos

### Introdu√ß√£o
No cap√≠tulo anterior, estabelecemos a equival√™ncia entre a estima√ß√£o por M√≠nimos Quadrados Generalizados (GLS) e a estima√ß√£o por M√°xima Verossimilhan√ßa (MLE) em modelos onde a matriz de covari√¢ncia dos res√≠duos, $V$, √© conhecida a priori. Contudo, na pr√°tica, essa matriz raramente √© conhecida e precisa ser estimada a partir dos dados. Expandindo o conceito apresentado, nesta se√ß√£o, exploramos o cen√°rio onde a matriz de covari√¢ncia dos res√≠duos, $V$, √© desconhecida e seus par√¢metros devem ser estimados conjuntamente com os par√¢metros de regress√£o, $\beta$.  Ainda que a otimalidade da abordagem GLS n√£o possa ser garantida nesse caso, veremos que sob hip√≥teses assint√≥ticas, √© poss√≠vel obter resultados semelhantes aos obtidos quando a matriz √© conhecida [^2, ^3, ^5]. Al√©m disso, revisaremos os resultados apresentados na se√ß√£o anterior e como eles se relacionam com a estima√ß√£o da vari√¢ncia da perturba√ß√£o aleat√≥ria.

### Conceitos Fundamentais
Como vimos anteriormente, a estima√ß√£o por GLS assume que a matriz de covari√¢ncia dos erros √© conhecida e pode ser expressa como $E(uu') = \sigma^2V$ [^2, ^19]. No entanto, quando $V$ √© desconhecida, ela precisa ser estimada juntamente com $\beta$.  Em geral, $V$ √© uma fun√ß√£o de um vetor de par√¢metros $\theta$, de modo que $V=V(\theta)$. Um exemplo √© o modelo AR(1) para os res√≠duos onde $V$ √© fun√ß√£o de $\rho$.

#### Estimando os Par√¢metros da Matriz de Covari√¢ncia
A log-verossimilhan√ßa condicional em $X$, quando $V$ √© fun√ß√£o de $\theta$, √© dada por:
$$ \mathcal{L}(\beta, \sigma^2, \theta | y, X) = -\frac{T}{2}\log(2\pi) -\frac{1}{2}\log|\sigma^2V(\theta)| - \frac{1}{2}(y-X\beta)'(\sigma^2V(\theta))^{-1}(y-X\beta) $$
[8.3.10]

Neste caso, a maximiza√ß√£o da log-verossimilhan√ßa requer a estima√ß√£o conjunta de $\beta$, $\sigma^2$ e $\theta$.  A solu√ß√£o para esse problema geralmente envolve m√©todos num√©ricos, como os discutidos em [^2].

**Lema 1**
A maximiza√ß√£o da fun√ß√£o de log-verossimilhan√ßa acima com respeito a $\sigma^2$ , considerando $\beta$ e $\theta$ como conhecidos, leva a:
$$ \hat{\sigma}^2(\beta, \theta) = \frac{1}{T}(y-X\beta)'V(\theta)^{-1}(y-X\beta) $$
*Prova:*
I. A fun√ß√£o de log-verossimilhan√ßa √© dada por:
    $$ \mathcal{L}(\beta, \sigma^2, \theta | y, X) = -\frac{T}{2}\log(2\pi) -\frac{1}{2}\log|\sigma^2V(\theta)| - \frac{1}{2}(y-X\beta)'(\sigma^2V(\theta))^{-1}(y-X\beta) $$
II. Usando a propriedade $\log|cA| = \log(c^n) + \log|A|$, para uma matriz $A$ de dimens√£o $n \times n$, e que $|cA| = c^n |A|$ temos que:
   $$ \mathcal{L}(\beta, \sigma^2, \theta | y, X) = -\frac{T}{2}\log(2\pi) -\frac{T}{2}\log(\sigma^2) -\frac{1}{2}\log|V(\theta)| - \frac{1}{2\sigma^2}(y-X\beta)'V(\theta)^{-1}(y-X\beta) $$
III. Derivando $\mathcal{L}$ com respeito a $\sigma^2$ e igualando a zero, obtemos:
    $$\frac{\partial \mathcal{L}}{\partial \sigma^2} = -\frac{T}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}(y-X\beta)'V(\theta)^{-1}(y-X\beta) = 0$$
IV. Multiplicando ambos os lados da equa√ß√£o por $2(\sigma^2)^2$ obtemos:
    $$ -T\sigma^2 + (y-X\beta)'V(\theta)^{-1}(y-X\beta) = 0 $$
V. Resolvendo para $\sigma^2$  resulta em:
     $$ \hat{\sigma}^2(\beta, \theta) = \frac{1}{T}(y-X\beta)'V(\theta)^{-1}(y-X\beta) $$
Note que o estimador de m√°xima verossimilhan√ßa de $\sigma^2$ depende de $\beta$ e $\theta$.  ‚ñ†

#### Abordagens Iterativas e Simplifica√ß√µes
Uma abordagem comum √© empregar um m√©todo iterativo, como o m√©todo Cochrane-Orcutt iterado, onde os par√¢metros de regress√£o e os par√¢metros da matriz de covari√¢ncia s√£o estimados alternadamente [^2, ^18]. Para iniciar o processo, usualmente os res√≠duos da regress√£o OLS s√£o utilizados para obter uma estimativa inicial de $\theta$ (e.g. usar os res√≠duos para estimar $\rho$ num modelo AR(1)). Com uma estimativa de $\theta$, $\hat{\theta}$, obtem-se uma estimativa de $V$, $\hat{V}$, e com este, uma estimativa de $\beta$ usando a metodologia GLS. Em seguida, o processo se repete usando o $\beta$ obtido para recalcular $V$ e assim por diante.

> üí° **Exemplo Num√©rico:**
>
> Vamos ilustrar o processo iterativo de Cochrane-Orcutt com um modelo simples de regress√£o com erros AR(1):
>
> $y_t = \beta_0 + \beta_1 x_t + u_t$
>
> $u_t = \rho u_{t-1} + \epsilon_t$, onde $\epsilon_t \sim N(0, \sigma^2)$
>
> Suponhamos que temos os seguintes dados simulados para $T=100$:
>
> ```python
> import numpy as np
> import pandas as pd
>
> np.random.seed(42)
> T = 100
> rho = 0.7
> beta_0 = 5
> beta_1 = 2
> sigma_sq = 1
>
> x = np.random.normal(10, 2, T)
> u = np.zeros(T)
> eps = np.random.normal(0, np.sqrt(sigma_sq), T)
>
> u[0] = eps[0]  # Initializing u_0 to epsilon_0 for simplicity
> for t in range(1, T):
>     u[t] = rho * u[t-1] + eps[t]
>
> y = beta_0 + beta_1 * x + u
> data = pd.DataFrame({'y':y,'x':x})
> ```
>
> **Itera√ß√£o 1:**
> 1.  *OLS Inicial*: Regredimos `y` em `x` usando OLS:
>     ```python
>     from sklearn.linear_model import LinearRegression
>
>     X = data[['x']]
>     y = data['y']
>     ols_model = LinearRegression().fit(X, y)
>     beta_ols = np.array([ols_model.intercept_, ols_model.coef_[0]])
>     residuals_ols = y - ols_model.predict(X)
>     print(f"OLS Beta Estimates: {beta_ols}")
>     ```
>     Resultados aproximados: $\hat{\beta}_{OLS} \approx \begin{bmatrix} 7.3 \\ 1.9 \end{bmatrix}$.
> 2.  *Estima√ß√£o de œÅ*: Regredimos $\hat{u}_t$ em $\hat{u}_{t-1}$:
>
>     ```python
>     lagged_residuals = residuals_ols[:-1]
>     current_residuals = residuals_ols[1:]
>     rho_model = LinearRegression().fit(lagged_residuals.reshape(-1,1), current_residuals)
>     rho_hat = rho_model.coef_[0]
>     print(f"Estimated rho: {rho_hat}")
>     ```
>      Resultado aproximado: $\hat{\rho} \approx 0.65$.
> 3.  *Constru√ß√£o de $\hat{V}$*:  A matriz $\hat{V}$ √© constru√≠da com base em $\hat{\rho}$. Como temos erros AR(1), $V$ tem uma estrutura Toeplitz:
>     $$\hat{V} = \begin{bmatrix}
>     1 & \hat{\rho} & \hat{\rho}^2 & \cdots & \hat{\rho}^{T-1} \\
>     \hat{\rho} & 1 & \hat{\rho} & \cdots & \hat{\rho}^{T-2} \\
>     \hat{\rho}^2 & \hat{\rho} & 1 & \cdots & \hat{\rho}^{T-3} \\
>     \vdots & \vdots & \vdots & \ddots & \vdots \\
>     \hat{\rho}^{T-1} & \hat{\rho}^{T-2} & \hat{\rho}^{T-3} & \cdots & 1
>     \end{bmatrix}$$
>     (A constru√ß√£o precisa de um pouco mais de c√≥digo para montar uma matriz Toeplitz, mas o conceito √© este)
> 4.  *GLS*: Estimamos $\beta$ usando GLS com $\hat{V}$:
>     ```python
>     from scipy.linalg import toeplitz
>
>     def create_AR1_covariance_matrix(rho, T):
>         first_col = np.array([rho**i for i in range(T)])
>         return toeplitz(first_col)
>
>     V_hat = create_AR1_covariance_matrix(rho_hat, T)
>     V_inv = np.linalg.inv(V_hat)
>     X_mat = np.concatenate([np.ones((T, 1)), X], axis=1)
>     beta_gls = np.linalg.solve(X_mat.T @ V_inv @ X_mat, X_mat.T @ V_inv @ y)
>     print(f"GLS Beta Estimates: {beta_gls}")
>     ```
>     Resultados aproximados: $\hat{\beta}_{GLS} \approx \begin{bmatrix} 5.6 \\ 1.8 \end{bmatrix}$.
>
> **Itera√ß√µes Seguintes:**
>  Repetimos os passos 2-4 at√© que $\hat{\rho}$ e $\hat{\beta}$ convirjam. Em cada itera√ß√£o, o res√≠duo usado para estimar $\hat{\rho}$ ser√° obtido com o $\hat{\beta}$ da itera√ß√£o anterior e assim por diante. Ap√≥s algumas itera√ß√µes, as estimativas devem convergir para valores pr√≥ximos aos verdadeiros. Os resultados assint√≥ticos garantem que essas estimativas convergir√£o para valores pr√≥ximos aos que seriam obtidos com a matriz $V$ conhecida, desde que o tamanho da amostra seja grande o suficiente.

Em algumas situa√ß√µes, m√©todos mais simples, como uma estimativa de $\theta$ atrav√©s de regress√µes OLS auxiliares, fornecem estimativas consistentes de $\theta$, com as mesmas propriedades assint√≥ticas da abordagem de m√°xima verossimilhan√ßa [^2]. A converg√™ncia desse processo √© um resultado assint√≥tico. Sob certas condi√ß√µes, essas estimativas t√™m a mesma distribui√ß√£o assint√≥tica do estimador GLS sob conhecimento de $V$, mas n√£o nos fornecem resultados exatos para amostras finitas.

#### Resultados Assint√≥ticos
Quando a matriz de covari√¢ncia n√£o √© conhecida e precisa ser estimada, n√£o √© poss√≠vel garantir que o estimador GLS seja de vari√¢ncia m√≠nima entre todos os estimadores n√£o viesados como no caso em que $V$ √© conhecida. No entanto, sob certas condi√ß√µes, principalmente quando o tamanho da amostra $T$ tende ao infinito, √© poss√≠vel obter resultados assint√≥ticos que justificam o uso da abordagem GLS. Em particular, o estimador GLS  $\hat{\beta}_{GLS} = (X'\hat{V}^{-1}X)^{-1}X'\hat{V}^{-1}y$ ter√° a mesma distribui√ß√£o assint√≥tica que o estimador GLS no caso em que $V$ √© conhecido [^2].

√â importante notar que a consist√™ncia das estimativas de $\theta$ e $\beta$ e a distribui√ß√£o assint√≥tica do estimador dependem de condi√ß√µes sobre a natureza dos regressores e os res√≠duos. Essas condi√ß√µes s√£o frequentemente expressas em termos de momentos finitos e ergodicidade.

### Vari√¢ncia Assint√≥tica do Estimador GLS
Como vimos no cap√≠tulo anterior, a vari√¢ncia condicional do estimador GLS quando $V$ √© conhecida √© dada por $\sigma^2(X'V^{-1}X)^{-1}$. No entanto, quando $V$ √© desconhecida e √© estimada por $\hat{V}$, a vari√¢ncia assint√≥tica do estimador GLS √© dada por:
$$ var(\hat{\beta}_{GLS}) = (X'\hat{V}^{-1}X)^{-1}X'\hat{V}^{-1} E(uu'|X) \hat{V}^{-1}X(X'\hat{V}^{-1}X)^{-1} $$
[8.2.34]

Sob as condi√ß√µes de assint√≥tica do modelo (como a exist√™ncia e converg√™ncia de plims), o estimador de White, ou o de Newey-West, podem ser usados para obter uma estimativa consistente da vari√¢ncia assint√≥tica de $\hat{\beta}_{GLS}$ [^2]. Em particular, quando h√° heterocedasticidade, a matriz de vari√¢ncia assint√≥tica do estimador GLS √© dada por $Q^{-1}\Omega Q^{-1}$ e uma estimativa consistente √© dada por $\hat{Q}_T^{-1}\hat{\Omega}_T\hat{Q}_T^{-1}$, onde $\hat{Q}_T = (1/T) \sum_{t=1}^{T} x_t x_t'$, $\hat{\Omega}_T = (1/T)\sum_{t=1}^T \hat{u}_t^2 x_t x_t'$ e $\hat{u}_t$ √© o res√≠duo do modelo OLS.

> üí° **Exemplo Num√©rico:**
>
> Retomando o exemplo do modelo com erros AR(1) e os dados simulados, podemos ilustrar como calcular a vari√¢ncia assint√≥tica do estimador GLS.
>
> 1. **Estimativa de Q:**
>   
>   ```python
>   Q_hat = (1/T) * (X_mat.T @ X_mat)
>   print("Estimated Q:\n", Q_hat)
>   ```
> 2. **Res√≠duos OLS e Estimativa de Œ©:**
>     
>   ```python
>   residuals_ols = y - ols_model.predict(X)
>   Omega_hat = np.zeros((2, 2))  # Initialize Omega_hat as a 2x2 matrix
>   for t in range(T):
>       x_t = X_mat[t,:].reshape(-1, 1)  # Ensure x_t is a column vector
>       Omega_hat += residuals_ols[t]**2 * x_t @ x_t.T
>   Omega_hat = (1/T) * Omega_hat
>   print("Estimated Omega:\n", Omega_hat)
>   ```
> 3. **Vari√¢ncia Assint√≥tica:**
>    A matriz de vari√¢ncia assint√≥tica do estimador GLS √© ent√£o dada por $\hat{Q}^{-1} \hat{\Omega} \hat{Q}^{-1}$:
>    ```python
>    Q_inv = np.linalg.inv(Q_hat)
>    asymptotic_variance = Q_inv @ Omega_hat @ Q_inv
>    print("Estimated asymptotic variance:\n", asymptotic_variance)
>   ```
>   Este exemplo mostra que a matriz de covari√¢ncia assint√≥tica √© obtida a partir das estimativas de $\hat{Q}$ e $\hat{\Omega}$. Na pr√°tica, utiliza-se a raiz quadrada dos elementos da diagonal para construir intervalos de confian√ßa e testes de hip√≥tese para os coeficientes.

**Lema 2.1** Se $\hat{V} \overset{p}{\longrightarrow} V$, ent√£o, sob condi√ß√µes de regularidade,  $(X'\hat{V}^{-1}X)^{-1} \overset{p}{\longrightarrow} (X'V^{-1}X)^{-1}$.
*Prova:*
I.  Dado que $\hat{V} \overset{p}{\longrightarrow} V$, isso significa que para qualquer $\epsilon > 0$, $\lim_{T \to \infty} P(|\hat{V} - V| > \epsilon) = 0$, onde $T$ √© o tamanho da amostra.
II. Se a fun√ß√£o inversa, denotada por $f(A) = A^{-1}$, √© cont√≠nua em $V$, ent√£o pela propriedade da converg√™ncia em probabilidade, temos que  $\hat{V}^{-1} \overset{p}{\longrightarrow} V^{-1}$. A continuidade da fun√ß√£o inversa √© uma condi√ß√£o de regularidade.
III.  $X$ √© uma matriz n√£o estoc√°stica. Portanto, $X'$ tamb√©m √© n√£o estoc√°stica. Assim,  $X'\hat{V}^{-1}X$ √© uma combina√ß√£o linear dos elementos de $\hat{V}^{-1}$, portanto, converge em probabilidade para $X'V^{-1}X$.
IV.  Novamente, se a fun√ß√£o inversa, $f(A) = A^{-1}$, √© cont√≠nua em $X'V^{-1}X$, ent√£o temos que $(X'\hat{V}^{-1}X)^{-1} \overset{p}{\longrightarrow} (X'V^{-1}X)^{-1}$. Esta √© uma outra condi√ß√£o de regularidade necess√°ria.
V. Portanto, sob as condi√ß√µes de regularidade, $(X'\hat{V}^{-1}X)^{-1}$ converge em probabilidade para $(X'V^{-1}X)^{-1}$.  ‚ñ†

#### Resultados de Testes de Hip√≥teses
Sob condi√ß√µes assint√≥ticas e a distribui√ß√£o dos erros ser normal, os testes $t$ e $F$ podem ser aplicados com as distribui√ß√µes aproximadas. Por exemplo, o teste $t$ para uma hip√≥tese de que $\beta_i=\beta_i^0$ √© dado por:
$$t = \frac{\hat{\beta}_i - \beta_i^0}{\sqrt{s^2 \hat{d}_{ii}}} $$
onde $\hat{d}_{ii}$ √© o i-√©simo elemento diagonal de $(X'\hat{V}^{-1}X)^{-1}$, e $s^2$ √© uma estimativa consistente de $\sigma^2$.
Sob as hip√≥teses nula, a distribui√ß√£o assint√≥tica deste teste $t$ √© $N(0,1)$. Similarmente, os testes $F$ (e $\chi^2$) utilizados na forma de Wald seguem a mesma l√≥gica.

> üí° **Exemplo Num√©rico:**
>
> Utilizando os resultados da regress√£o GLS do exemplo anterior, vamos realizar um teste de hip√≥tese para o coeficiente $\beta_1$. Suponhamos que queremos testar a hip√≥tese nula $H_0: \beta_1 = 2$ contra a hip√≥tese alternativa $H_1: \beta_1 \neq 2$.
>
> 1. **Estimativa de s¬≤:**
>   Utilizando a f√≥rmula de estimativa n√£o viesada para $\sigma^2$:
>   ```python
>   k = 2 # N√∫mero de par√¢metros no modelo
>   sigma_sq_hat = (1/(T - k))* (y - X_mat @ beta_gls).T @ V_inv @ (y - X_mat @ beta_gls)
>   print(f"Estimated sigma squared: {sigma_sq_hat}")
>   ```
>
> 2. **C√°lculo do desvio padr√£o:**
>  Calculamos a matriz de covari√¢ncia do estimador GLS:
>    ```python
>    var_beta_gls = sigma_sq_hat * np.linalg.inv(X_mat.T @ V_inv @ X_mat)
>    se_beta1 = np.sqrt(var_beta_gls[1, 1]) # Std error de beta_1 (elemento da diagonal)
>    print(f"Standard error of beta_1: {se_beta1}")
>    ```
> 3. **Estat√≠stica t:**
>    Calculamos a estat√≠stica t:
>   ```python
>   beta_1_0 = 2
>   t_stat = (beta_gls[1] - beta_1_0) / se_beta1
>   print(f"T-statistic: {t_stat}")
>   ```
>4. **Valor-p:**
>  Consultamos a distribui√ß√£o normal para obter o valor p:
> ```python
> import scipy.stats as stats
> p_value = 2 * (1 - stats.norm.cdf(np.abs(t_stat)))
> print(f"P-value: {p_value}")
> ```
>
> Se o valor p for menor do que o n√≠vel de signific√¢ncia desejado (por exemplo, 0.05), rejeitamos a hip√≥tese nula. Caso contr√°rio, n√£o h√° evid√™ncia estat√≠stica para rejeitar a hip√≥tese nula.

#### Vieses e Corre√ß√µes
√â importante ressaltar que as estimativas de $\beta$ e $\sigma^2$  em modelos com matriz de covari√¢ncia desconhecida podem apresentar vieses em amostras finitas. Como vimos anteriormente,  a estimativa de m√°xima verossimilhan√ßa de $\sigma^2$, $\hat{\sigma}^2_{MLE}$ √© viesada, sendo um estimador n√£o viesado dado por [^1, ^2]:
$$ \hat{\sigma}^2 = \frac{1}{T-K}(\bar{y} - \bar{X}\hat{\beta}_{GLS})'(\bar{y} - \bar{X}\hat{\beta}_{GLS}) = \frac{1}{T-K}(y - X\hat{\beta}_{GLS})'\hat{V}^{-1}(y - X\hat{\beta}_{GLS}) $$
[Corol√°rio 1.1]
Onde $K$ √© o n√∫mero de par√¢metros no modelo.
Em geral, sob hip√≥teses de amostra grande, a import√¢ncia dos vieses diminui.

**Corol√°rio 1.2** Se $V$ √© conhecida, ent√£o  a vari√¢ncia do estimador $\hat{\beta}_{GLS}$ √© dada por $Var(\hat{\beta}_{GLS}) = \sigma^2 (X'V^{-1}X)^{-1}$. Al√©m disso, $\hat{\sigma}^2 = \frac{1}{T-K}(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS})$ √© um estimador n√£o-viesado de $\sigma^2$.
*Prova:*
I. A vari√¢ncia do estimador GLS √© dada por:
    $$ Var(\hat{\beta}_{GLS}) = Var((X'V^{-1}X)^{-1}X'V^{-1}y) = (X'V^{-1}X)^{-1}X'V^{-1}Var(y)V^{-1}X(X'V^{-1}X)^{-1} $$
II. Como $y=X\beta + u$, e $Var(u)=\sigma^2V$, temos que $Var(y)=\sigma^2V$.
III. Substituindo $Var(y)$ na express√£o da vari√¢ncia:
$$ Var(\hat{\beta}_{GLS}) = (X'V^{-1}X)^{-1}X'V^{-1}(\sigma^2V)V^{-1}X(X'V^{-1}X)^{-1} $$
IV. Simplificando a express√£o, obtemos:
    $$ Var(\hat{\beta}_{GLS}) = \sigma^2 (X'V^{-1}X)^{-1} $$
V. Para demonstrar que $\hat{\sigma}^2$ √© n√£o-viesado, come√ßamos por notar que:
   $$ (y - X\hat{\beta}_{GLS}) = (I - X(X'V^{-1}X)^{-1}X'V^{-1})y $$
VI.  Substituindo $y=X\beta+u$:
 $$ (y - X\hat{\beta}_{GLS}) = (I - X(X'V^{-1}X)^{-1}X'V^{-1})(X\beta+u) = u - X(X'V^{-1}X)^{-1}X'V^{-1}u $$
VII.  Ent√£o:
  $$ (y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS}) = u'V^{-1}u - 2u'V^{-1}X(X'V^{-1}X)^{-1}X'V^{-1}u + u'V^{-1}X(X'V^{-1}X)^{-1}X'V^{-1}X(X'V^{-1}X)^{-1}X'V^{-1}u $$
VIII. Usando que $V^{-1}X(X'V^{-1}X)^{-1}X'V^{-1}X = V^{-1}X(X'V^{-1}X)^{-1}X' = P$ √© a matriz de proje√ß√£o, obtemos:
    $$ (y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS}) = u'V^{-1}u - 2u'V^{-1}Pu + u'V^{-1}Pu = u'V^{-1}u - u'V^{-1}Pu $$
IX.  Tomando esperan√ßa:
     $$ E[(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS})] = E(u'V^{-1}u) - E(u'V^{-1}Pu) $$
X. Como $E(u'Au) = tr(AE(uu'))$, e $E(uu') = \sigma^2V$
   $$ E(u'V^{-1}u) = tr(V^{-1}E(uu')) = tr(V^{-1}\sigma^2V) = \sigma^2 tr(I_T) = \sigma^2T $$
XI.  E:
     $$ E(u'V^{-1}Pu) = tr(V^{-1}PE(uu')) = tr(V^{-1}P\sigma^2V) = \sigma^2 tr(V^{-1}PV) = \sigma^2 tr(P) = \sigma^2K $$
XII.  Portanto:
   $$ E[(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS})] = \sigma^2T - \sigma^2K = \sigma^2(T-K) $$
XIII. Substituindo na express√£o de  $\hat{\sigma}^2$ temos:
     $$ E[\hat{\sigma}^2] = E[\frac{1}{T-K}(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS})] = \frac{1}{T-K} \sigma^2 (T-K) = \sigma^2 $$
  Logo,  $\hat{\sigma}^2$ √© um estimador n√£o-viesado de $\sigma^2$. ‚ñ†

### Conclus√£o
Quando a matriz de covari√¢ncia dos res√≠duos, $V$, n√£o √© conhecida, a estima√ß√£o conjunta dos par√¢metros de regress√£o e dos par√¢metros de covari√¢ncia requer abordagens mais complexas e resultados assint√≥ticos [^2, ^3, ^5].  Apesar da perda de otimalidade do estimador GLS em pequenas amostras, sob condi√ß√µes de amostras grandes, o estimador GLS, juntamente com estimativas consistentes da matriz de covari√¢ncia e distribui√ß√µes assint√≥ticas, ainda fornece resultados consistentes e eficientes para a estima√ß√£o de modelos de regress√£o. M√©todos iterativos e simplifica√ß√µes, como o m√©todo de Cochrane-Orcutt iterado e o uso de estimativas obtidas de regress√µes auxiliares, mostram-se ferramentas eficazes para essa tarefa [^2, ^18]. Al√©m disso, o conhecimento de como a matriz de covari√¢ncia afeta a vari√¢ncia das estimativas permite aos pesquisadores realizar infer√™ncias estat√≠sticas v√°lidas. √â importante considerar que a validade desses resultados depende das hip√≥teses assint√≥ticas do modelo.

### Refer√™ncias
[^1]: Se√ß√£o 8.1, "Review of Ordinary Least Squares".
[^2]: Se√ß√£o 8.3, "Generalized Least Squares".
[^3]: Se√ß√£o 8.2, "Ordinary Least Squares Under More General Conditions".
[^5]:  Se√ß√£o 8.2, "Case 6. Errors Serially Uncorrelated but with General Heteroskedasticity".
[^18]: Texto pr√≥ximo √† Eq. 8.3.15 e 8.3.16.
[^19]: Texto pr√≥ximo a Eq. 8.3.1 e a dedu√ß√£o do estimador GLS 8.3.5
<!-- END -->
