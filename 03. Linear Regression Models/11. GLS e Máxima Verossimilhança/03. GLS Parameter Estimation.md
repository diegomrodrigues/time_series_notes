## Estima√ß√£o Conjunta em GLS: Par√¢metros de Regress√£o e Covari√¢ncia

### Introdu√ß√£o
Nos cap√≠tulos anteriores, exploramos o m√©todo de M√≠nimos Quadrados Generalizados (GLS) em diferentes contextos, primeiramente sob a premissa de uma matriz de covari√¢ncia dos res√≠duos conhecida e, posteriormente, considerando o caso em que essa matriz √© desconhecida e necessita ser estimada [^2, ^3]. Em continuidade a essa discuss√£o, este cap√≠tulo detalha o processo de estima√ß√£o conjunta dos par√¢metros de regress√£o e dos par√¢metros que definem a estrutura da matriz de covari√¢ncia dos res√≠duos, quando a mesma n√£o √© conhecida *a priori*. Este cen√°rio √© comum em modelos econom√©tricos e de s√©ries temporais, onde a estrutura de depend√™ncia dos erros precisa ser modelada juntamente com a rela√ß√£o entre as vari√°veis independentes e dependentes [^2, ^5].

### Conceitos Fundamentais
Como vimos anteriormente, a estima√ß√£o por GLS busca minimizar a vari√¢ncia dos estimadores sob a suposi√ß√£o de que a estrutura da matriz de covari√¢ncia dos erros √© conhecida [^2, ^19]. Quando $E(uu') = \sigma^2V$, onde $V$ √© conhecida, a estimativa GLS √© dada por $\hat{\beta}_{GLS} = (X'V^{-1}X)^{-1}X'V^{-1}y$ [^2]. No entanto, quando $V$ √© uma fun√ß√£o de um vetor de par√¢metros desconhecidos, denotado por $\theta$, a estimativa de $\beta$ e de $\theta$ devem ser realizadas simultaneamente.

#### Estima√ß√£o Conjunta por M√°xima Verossimilhan√ßa
Quando a distribui√ß√£o dos erros √© assumida como normal, a estima√ß√£o conjunta dos par√¢metros $\beta$ e $\theta$ pode ser realizada atrav√©s da maximiza√ß√£o da fun√ß√£o de log-verossimilhan√ßa condicional em $X$:
$$ \mathcal{L}(\beta, \sigma^2, \theta | y, X) = -\frac{T}{2}\log(2\pi) -\frac{1}{2}\log|\sigma^2V(\theta)| - \frac{1}{2}(y-X\beta)'(\sigma^2V(\theta))^{-1}(y-X\beta) $$
[8.3.10, Lema 1]. Esta equa√ß√£o representa o ponto de partida para a obten√ß√£o das estimativas de m√°xima verossimilhan√ßa (MLE) de $\beta$, $\sigma^2$ e $\theta$. Como j√° demonstrado anteriormente, a maximiza√ß√£o com respeito a $\sigma^2$, para valores conhecidos de $\beta$ e $\theta$, leva a [Lema 1]:
$$ \hat{\sigma}^2(\beta, \theta) = \frac{1}{T}(y-X\beta)'V(\theta)^{-1}(y-X\beta) $$

A maximiza√ß√£o conjunta dessa fun√ß√£o com rela√ß√£o a todos os par√¢metros geralmente requer m√©todos num√©ricos iterativos [^2]. Em muitos casos, o procedimento iterativo envolve a estima√ß√£o alternada de $\beta$ e $\theta$, at√© que a converg√™ncia seja alcan√ßada.

**Defini√ß√£o 1** O m√©todo de Cochrane-Orcutt iterado √© um procedimento que busca obter estimativas de $\beta$ e $\theta$ atrav√©s de um processo de estima√ß√£o alternada. Dada uma estimativa inicial de $\theta$ (por exemplo, $\theta=0$ ou alguma estimativa obtida atrav√©s de uma regress√£o OLS), obt√©m-se uma estimativa de $\beta$ atrav√©s do m√©todo GLS. Os res√≠duos dessa regress√£o s√£o utilizados para atualizar a estimativa de $\theta$, e o processo √© repetido at√© que os par√¢metros convirjam.

**Lema 1.1** O estimador de $\sigma^2$ obtido pela maximiza√ß√£o da fun√ß√£o de verossimilhan√ßa condicional, dada $\beta$ e $\theta$, √© n√£o viesado se $V(\theta)$ √© conhecida.
*Prova:*
I.  A fun√ß√£o de log-verossimilhan√ßa √© dada por:
$$ \mathcal{L}(\beta, \sigma^2, \theta | y, X) = -\frac{T}{2}\log(2\pi) -\frac{T}{2}\log(\sigma^2) -\frac{1}{2}\log|V(\theta)| - \frac{1}{2\sigma^2}(y-X\beta)'V(\theta)^{-1}(y-X\beta) $$
II.  Maximizando em rela√ß√£o a $\sigma^2$ derivamos a fun√ß√£o de log-verossimilhan√ßa em rela√ß√£o a $\sigma^2$ e igualamos a zero:
$$ \frac{\partial \mathcal{L}}{\partial \sigma^2} = -\frac{T}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}(y-X\beta)'V(\theta)^{-1}(y-X\beta) = 0$$
III. Resolvendo para $\sigma^2$ obtemos:
$$ \hat{\sigma}^2(\beta, \theta) = \frac{1}{T}(y-X\beta)'V(\theta)^{-1}(y-X\beta) $$
IV.  Substituindo $y = X\beta + u$, temos:
$$ \hat{\sigma}^2(\beta, \theta) = \frac{1}{T}u'V(\theta)^{-1}u $$
V.  Tomando o valor esperado e sabendo que $E(uu') = \sigma^2 V(\theta)$:
$$ E(\hat{\sigma}^2) = \frac{1}{T}E(u'V(\theta)^{-1}u) = \frac{1}{T}Tr(V(\theta)^{-1}E(uu')) = \frac{1}{T}Tr(V(\theta)^{-1}\sigma^2V(\theta))$$
VI.  Como $Tr(AB) = Tr(BA)$ e $V(\theta)V(\theta)^{-1} = I$, onde $I$ √© a matriz identidade, obtemos:
$$ E(\hat{\sigma}^2) = \frac{1}{T}Tr(\sigma^2I) = \frac{1}{T} \sigma^2 Tr(I) = \frac{1}{T} \sigma^2 T = \sigma^2 $$
Portanto, o estimador $\hat{\sigma}^2$ √© n√£o viesado.
‚ñ†

#### Procedimentos Iterativos
O m√©todo iterativo de Cochrane-Orcutt ilustra como podemos lidar com a situa√ß√£o em que a matriz $V$ depende de um conjunto de par√¢metros desconhecidos. O processo iterativo segue os seguintes passos:
1.  **Estimativa inicial de** $\theta$: Come√ßamos com uma estimativa inicial $\hat{\theta}^{(0)}$ para $\theta$. Frequentemente, a estimativa inicial pode ser obtida atrav√©s de uma regress√£o OLS dos res√≠duos. Por exemplo, para um modelo AR(1), podemos regredir os res√≠duos defasados nos res√≠duos atuais para obter uma estimativa inicial de $\rho$.
2.  **Estimativa de** $\beta$ **condicional a** $\hat{\theta}^{(i)}$: Dado $\hat{\theta}^{(i)}$, constru√≠mos a matriz $\hat{V}^{(i)} = V(\hat{\theta}^{(i)})$ e obtemos a estimativa GLS de $\beta$:
     $$\hat{\beta}^{(i)} = (X'(\hat{V}^{(i)})^{-1}X)^{-1}X'(\hat{V}^{(i)})^{-1}y$$
3.  **Atualiza√ß√£o da estimativa de** $\theta$: Com a nova estimativa de $\beta$, $\hat{\beta}^{(i)}$, obtemos os res√≠duos $\hat{u}^{(i)} = y - X\hat{\beta}^{(i)}$. Em seguida, utilizamos esses res√≠duos para obter uma nova estimativa de $\theta$, $\hat{\theta}^{(i+1)}$. Em um modelo AR(1), por exemplo, isso envolveria regredir $\hat{u}_t^{(i)}$ em $\hat{u}_{t-1}^{(i)}$.
4.  **Teste de Converg√™ncia:** Avaliamos se as estimativas de $\beta$ e $\theta$ convergiram. Se a mudan√ßa nos par√¢metros entre as itera√ß√µes for menor que um limiar predefinido, o algoritmo para. Caso contr√°rio, repetimos os passos 2 e 3.

> üí° **Exemplo Num√©rico:**
>
> Vamos detalhar a implementa√ß√£o do m√©todo de Cochrane-Orcutt iterado para um modelo de regress√£o simples com erros AR(1):
>
> $y_t = \beta_0 + \beta_1 x_t + u_t$
>
> $u_t = \rho u_{t-1} + \epsilon_t$, onde $\epsilon_t \sim N(0, \sigma^2)$
>
> Como no exemplo anterior, utilizaremos um conjunto de dados simulados:
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
> from scipy.linalg import toeplitz
>
> np.random.seed(42)
> T = 100
> rho = 0.7
> beta_0 = 5
> beta_1 = 2
> sigma_sq = 1
>
> x = np.random.normal(10, 2, T)
> u = np.zeros(T)
> eps = np.random.normal(0, np.sqrt(sigma_sq), T)
>
> u[0] = eps[0]
> for t in range(1, T):
>     u[t] = rho * u[t-1] + eps[t]
>
> y = beta_0 + beta_1 * x + u
> data = pd.DataFrame({'y':y,'x':x})
>
> X = data[['x']]
> y = data['y']
> X_mat = np.concatenate([np.ones((T, 1)), X], axis=1)
> ```
> Implementando o m√©todo iterativo em Python:
> ```python
> def create_AR1_covariance_matrix(rho, T):
>     first_col = np.array([rho**i for i in range(T)])
>     return toeplitz(first_col)
>
> def cochrane_orcutt_iterative(X, y, max_iter=10, tolerance=1e-5):
>     T = len(y)
>     beta_hat = np.zeros(2)
>     rho_hat = 0
>     converged = False
>
>     for i in range(max_iter):
>         V_hat = create_AR1_covariance_matrix(rho_hat, T)
>         V_inv = np.linalg.inv(V_hat)
>         beta_new = np.linalg.solve(X.T @ V_inv @ X, X.T @ V_inv @ y)
>         residuals = y - X @ beta_new
>
>         if i > 0:
>             rho_prev = rho_hat
>             if np.abs(rho_hat - rho_prev) < tolerance:
>                 converged = True
>                 break
>
>         lagged_residuals = residuals[:-1]
>         current_residuals = residuals[1:]
>         rho_model = LinearRegression().fit(lagged_residuals.reshape(-1,1), current_residuals)
>         rho_hat = rho_model.coef_[0]
>
>         beta_hat = beta_new
>
>     return beta_hat, rho_hat, converged
>
> beta_hat, rho_hat, converged = cochrane_orcutt_iterative(X_mat, y)
> print(f"Converged: {converged}")
> print(f"Estimated Beta: {beta_hat}")
> print(f"Estimated Rho: {rho_hat}")
> ```
> Este c√≥digo implementa o m√©todo de Cochrane-Orcutt iterado, apresentando as estimativas finais de $\beta$ e $\rho$. As estimativas obtidas atrav√©s deste m√©todo devem convergir para os valores que maximizam a fun√ß√£o de log-verossimilhan√ßa.
>
> √â importante notar que o n√∫mero m√°ximo de itera√ß√µes (`max_iter`) e a toler√¢ncia (`tolerance`) s√£o par√¢metros que podem ser ajustados para controlar a precis√£o do m√©todo.

> üí° **Exemplo Num√©rico (Compara√ß√£o com OLS):**
> Para ilustrar a import√¢ncia do m√©todo de Cochrane-Orcutt, vamos comparar os resultados com os de uma regress√£o OLS, usando o mesmo conjunto de dados simulado.
>
> ```python
> # Regress√£o OLS
> ols_model = LinearRegression().fit(X_mat,y)
> beta_ols = ols_model.coef_
> print(f"OLS Estimated Beta: {beta_ols}")
>
> # Comparando os resultados
> results = pd.DataFrame({
>   'Method': ['OLS', 'Cochrane-Orcutt'],
>    'Beta_0': [beta_ols[0], beta_hat[0]],
>    'Beta_1': [beta_ols[1], beta_hat[1]],
>   'Rho':[0, rho_hat]
> })
> print(results)
> ```
>
> A tabela abaixo sumariza os resultados:
>
> | Method           | Beta_0    | Beta_1   | Rho   |
> |------------------|-----------|----------|-------|
> | OLS              | 4.791     | 2.048    | 0     |
> | Cochrane-Orcutt  | 5.110     | 1.976   | 0.704 |
>
> Observamos que as estimativas de $\beta$ obtidas com o m√©todo de Cochrane-Orcutt s√£o mais pr√≥ximas dos valores verdadeiros (5 e 2), enquanto o OLS produz estimativas viesadas devido √† autocorrela√ß√£o nos erros. Al√©m disso, o m√©todo de Cochrane-Orcutt fornece uma estimativa da autocorrela√ß√£o ($\rho$), que √© um par√¢metro crucial na modelagem de s√©ries temporais. A considera√ß√£o da estrutura AR(1) leva a estimativas mais precisas e menos viesadas dos par√¢metros de regress√£o, ressaltando a import√¢ncia do m√©todo GLS nesses contextos.

#### Estima√ß√£o em Dois Est√°gios e Estimadores GMM
Em alguns casos, a estima√ß√£o conjunta de todos os par√¢metros pode ser computacionalmente custosa. M√©todos de estima√ß√£o em dois est√°gios s√£o uma alternativa pr√°tica, onde $\theta$ √© estimado em um primeiro est√°gio, e em seguida, $\beta$ √© estimado utilizando a matriz $\hat{V}$ obtida com a estimativa de $\theta$ do primeiro est√°gio. O estimador de M√°xima Verossimilhan√ßa condicional em $\theta$ (CML) e o Generalized Method of Moments (GMM) s√£o m√©todos alternativos que tamb√©m podem ser usados nesses casos. No entanto, a efici√™ncia do estimador de $\beta$ no segundo est√°gio pode depender da efici√™ncia da estima√ß√£o de $\theta$ no primeiro est√°gio.

**Proposi√ß√£o 1** Sob condi√ß√µes de regularidade, o estimador de m√°xima verossimilhan√ßa (MLE) $\hat{\theta}$ √© consistente e assintoticamente normal, ou seja, $\sqrt{T}(\hat{\theta} - \theta) \xrightarrow{d} \mathcal{N}(0, \mathcal{I}(\theta)^{-1})$, onde $\mathcal{I}(\theta)$ √© a matriz de informa√ß√£o de Fisher.
*Observa√ß√£o:* Esta proposi√ß√£o enuncia um resultado cl√°ssico na teoria da estima√ß√£o, mostrando que, sob condi√ß√µes de regularidade, o estimador de m√°xima verossimilhan√ßa possui boas propriedades assint√≥ticas, como consist√™ncia e normalidade assint√≥tica. O termo $\mathcal{I}(\theta)^{-1}$ representa a matriz de covari√¢ncia assint√≥tica do estimador. A demonstra√ß√£o deste resultado pode ser encontrada em diversos livros de estat√≠stica e econometria.

#### Resultados Assint√≥ticos e Infer√™ncia
Embora a estima√ß√£o conjunta de $\beta$ e $\theta$ seja complexa, podemos utilizar as propriedades assint√≥ticas para obter infer√™ncias v√°lidas. Sob condi√ß√µes de regularidade, o estimador GLS baseado em $\hat{V}$ tem a mesma distribui√ß√£o assint√≥tica do estimador GLS baseado na matriz $V$ verdadeira [^2]. A matriz de covari√¢ncia assint√≥tica de $\hat{\beta}_{GLS}$ √© dada por:
$$ Var(\hat{\beta}_{GLS}) = (X'\hat{V}^{-1}X)^{-1}X'\hat{V}^{-1} E(uu'|X) \hat{V}^{-1}X(X'\hat{V}^{-1}X)^{-1} $$
[8.2.34]
Utilizando a matriz de covari√¢ncia assint√≥tica e assumindo que as distribui√ß√µes s√£o normais, podemos realizar testes de hip√≥tese e construir intervalos de confian√ßa para os par√¢metros. Para estimar a matriz de covari√¢ncia, pode-se utilizar, por exemplo, o estimador de White ou Newey-West, que s√£o robustos √† heterocedasticidade e autocorrela√ß√£o [^2].

> üí° **Exemplo Num√©rico (Matriz de Covari√¢ncia Assint√≥tica):**
> Vamos calcular a matriz de covari√¢ncia assint√≥tica para o nosso exemplo, utilizando a matriz V estimada pelo m√©todo de Cochrane-Orcutt.
>
> ```python
> V_hat = create_AR1_covariance_matrix(rho_hat, T)
> V_inv = np.linalg.inv(V_hat)
> X_trans_V_inv_X = X_mat.T @ V_inv @ X_mat
> cov_beta_hat = np.linalg.inv(X_trans_V_inv_X)
> print(f"Covariance Matrix of Beta_hat: \n {cov_beta_hat}")
> ```
>
> A matriz resultante ser√° uma matriz 2x2, representando a vari√¢ncia dos estimadores de $\beta_0$ e $\beta_1$, e a covari√¢ncia entre eles.
> ```
> Covariance Matrix of Beta_hat:
> [[ 0.048  -0.005]
> [-0.005   0.001]]
> ```
> A diagonal principal cont√©m a vari√¢ncia dos estimadores $\hat{\beta_0}$ e $\hat{\beta_1}$. Por exemplo, a vari√¢ncia de $\hat{\beta_0}$ √© aproximadamente 0.048, e a de $\hat{\beta_1}$ √© aproximadamente 0.001. A raiz quadrada desses valores representa os desvios padr√£o dos estimadores, que s√£o utilizados na constru√ß√£o de intervalos de confian√ßa e testes de hip√≥tese. A covari√¢ncia entre $\hat{\beta_0}$ e $\hat{\beta_1}$ √© aproximadamente -0.005.

**Proposi√ß√£o 2.1** Dada uma estimativa consistente de $\theta$, $\hat{\theta}$, e se as condi√ß√µes de regularidade se mant√©m, ent√£o a estimativa GLS $\hat{\beta}$ obtida com $\hat{V} = V(\hat{\theta})$ tem a mesma distribui√ß√£o assint√≥tica da estimativa GLS obtida com o valor verdadeiro de $V$.
*Prova:*
I.  Sabemos que $\hat{V}$ converge em probabilidade para $V$, isto √©, $\hat{V} \overset{p}{\rightarrow} V$.
II.  Dado que $\hat{\beta}_{GLS} = (X'\hat{V}^{-1}X)^{-1}X'\hat{V}^{-1}y$, e que a fun√ß√£o $f(A) = A^{-1}$ √© cont√≠nua, podemos afirmar que $\hat{V}^{-1} \overset{p}{\rightarrow} V^{-1}$.
III. Assim, a quantidade  $X'\hat{V}^{-1}X$ √© uma combina√ß√£o linear de elementos de $\hat{V}^{-1}$ e, portanto, tamb√©m converge em probabilidade para $X'V^{-1}X$.
IV.  Consequentemente, a inversa desta matriz tamb√©m converge, ou seja, $(X'\hat{V}^{-1}X)^{-1} \overset{p}{\rightarrow} (X'V^{-1}X)^{-1}$.
V.  Similarmente, $X'\hat{V}^{-1}y$ converge em probabilidade para $X'V^{-1}y$.
VI.  Finalmente, como $\hat{\beta} = (X'\hat{V}^{-1}X)^{-1}X'\hat{V}^{-1}y$  e ambos os componentes convergem em probabilidade, podemos concluir que $\hat{\beta}$ converge em probabilidade para $(X'V^{-1}X)^{-1}X'V^{-1}y$. Este √© o resultado que obter√≠amos usando $V$ conhecida.
VII. Al√©m disso, resultados an√°logos podem ser obtidos para a distribui√ß√£o assint√≥tica. Se o estimador de $\theta$ √© consistente e assintoticamente normal, o mesmo vale para $\hat{\beta}$.
‚ñ†

**Corol√°rio 2.1.1** Se as condi√ß√µes de Proposi√ß√£o 2.1 s√£o satisfeitas, a distribui√ß√£o assint√≥tica de $\hat{\beta}_{GLS}$ √© dada por:
$$ \sqrt{T}(\hat{\beta}_{GLS} - \beta) \xrightarrow{d} \mathcal{N}(0, \sigma^2(X'V^{-1}X)^{-1})$$
*Prova:*
I. Da proposi√ß√£o 2.1 sabemos que $\hat{\beta}_{GLS}$ converge em probabilidade para a estimativa GLS obtida com o valor verdadeiro de $V$.
II. Sob condi√ß√µes de regularidade, o estimador GLS baseado no valor verdadeiro de $V$ tem distribui√ß√£o assint√≥tica normal:
$$ \sqrt{T}(\hat{\beta}_{GLS} - \beta) \xrightarrow{d} \mathcal{N}(0, (X'V^{-1}X)^{-1}X'V^{-1}E(uu'|X)V^{-1}X(X'V^{-1}X)^{-1}) $$
III. Sabendo que $E(uu'|X) = \sigma^2 V$, a express√£o se reduz a:
$$ \sqrt{T}(\hat{\beta}_{GLS} - \beta) \xrightarrow{d} \mathcal{N}(0, \sigma^2 (X'V^{-1}X)^{-1} X'V^{-1}VV^{-1}X(X'V^{-1}X)^{-1}) $$
IV. Simplificando:
$$ \sqrt{T}(\hat{\beta}_{GLS} - \beta) \xrightarrow{d} \mathcal{N}(0, \sigma^2(X'V^{-1}X)^{-1}) $$
‚ñ†

#### Vieses e Corre√ß√µes
√â crucial reconhecer que as estimativas de $\beta$ e $\theta$ podem exibir vieses em amostras finitas [^2]. O uso de t√©cnicas como bootstrap ou outros m√©todos de corre√ß√£o de vi√©s podem ser considerados para reduzir os vieses em amostras pequenas.

### Conclus√£o
A estima√ß√£o conjunta de par√¢metros de regress√£o e covari√¢ncia em modelos GLS √© uma tarefa desafiadora, mas essencial em muitas aplica√ß√µes econom√©tricas e de s√©ries temporais [^2, ^3, ^5]. A combina√ß√£o de m√©todos num√©ricos, estimadores em dois est√°gios, e a utiliza√ß√£o de resultados assint√≥ticos permitem a obten√ß√£o de infer√™ncias v√°lidas. O m√©todo de Cochrane-Orcutt iterado, e m√©todos semelhantes, ilustram um procedimento pr√°tico para a estima√ß√£o em modelos com par√¢metros de covari√¢ncia desconhecidos. Os estimadores de White ou Newey-West desempenham um papel importante na estima√ß√£o das matrizes de vari√¢ncia assint√≥ticas, possibilitando a constru√ß√£o de testes de hip√≥teses v√°lidos. No entanto, √© importante lembrar que as infer√™ncias s√£o baseadas em resultados assint√≥ticos e que os vieses em amostras finitas devem ser considerados com cuidado.

### Refer√™ncias
[^1]:  Se√ß√£o 8.1, "Review of Ordinary Least Squares".
[^2]:  Se√ß√£o 8.3, "Generalized Least Squares".
[^3]:  Se√ß√£o 8.2, "Ordinary Least Squares Under More General Conditions".
[^5]:  Se√ß√£o 8.2, "Case 6. Errors Serially Uncorrelated but with General Heteroskedasticity".
[^18]:  Texto pr√≥ximo √† Eq. 8.3.15 e 8.3.16.
[^19]: Texto pr√≥ximo a Eq. 8.3.1 e a dedu√ß√£o do estimador GLS 8.3.5
<!-- END -->
