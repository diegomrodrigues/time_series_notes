## GLS e MÃ¡xima VerossimilhanÃ§a: EquivalÃªncia em Modelos com CovariÃ¢ncia Conhecida

### IntroduÃ§Ã£o
Neste capÃ­tulo, exploramos a fundo a estimaÃ§Ã£o por MÃ­nimos Quadrados OrdinÃ¡rios (OLS) sob diversas condiÃ§Ãµes. Como vimos anteriormente, a suposiÃ§Ã£o de que $E(uu') = \sigma^2I_T$ nem sempre Ã© vÃ¡lida, e embora a OLS ainda possa ser utilizada, os MÃ­nimos Quadrados Generalizados (GLS) se mostram uma alternativa preferÃ­vel em diversas situaÃ§Ãµes [^1, ^2, ^3, ^4, ^5, ^6]. Expandindo o conceito apresentado, vamos agora demonstrar que em modelos onde a matriz de covariÃ¢ncia Ã© conhecida, a estimativa GLS, que abordamos na seÃ§Ã£o anterior, tambÃ©m corresponde Ã  estimativa de MÃ¡xima VerossimilhanÃ§a (MLE), estabelecendo assim a otimalidade desta abordagem [^2].

### Conceitos Fundamentais
Revisitando o modelo de regressÃ£o bÃ¡sica, dado por $y = X\beta + u$, com $u|X \sim N(0, \sigma^2V)$, onde $V$ Ã© uma matriz conhecida $(T \times T)$ simÃ©trica e definida positiva [^2]. Conforme discutido, podemos encontrar uma matriz nÃ£o singular $L$ tal que $V^{-1} = L'L$ [^2, ^19]. Introduzimos a transformaÃ§Ã£o dos resÃ­duos da populaÃ§Ã£o, $\bar{u} = Lu$, o que leva a $\bar{u}$ com mÃ©dia 0 e variÃ¢ncia condicional em $X$ dada por:
$$E(\bar{u}\bar{u}'|X) = LE(uu'|X)L' = L\sigma^2VL'$$
Como $V = (V^{-1})^{-1} = (L'L)^{-1}$, temos que:
$$E(\bar{u}\bar{u}'|X) = \sigma^2L(L'L)^{-1}L' = \sigma^2I_T$$
Portanto, o modelo transformado $\bar{y} = \bar{X}\beta + \bar{u}$ possui resÃ­duos que satisfazem a suposiÃ§Ã£o 8.2, o que significa que todos os resultados daquela seÃ§Ã£o podem ser aplicados a este modelo transformado [^2].

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Suponha que temos um modelo com 3 observaÃ§Ãµes (T=3), um regressor (K=1) e a seguinte matriz de covariÃ¢ncia dos erros:
>
> $V = \begin{bmatrix} 2 & 0.5 & 0 \\ 0.5 & 1 & 0 \\ 0 & 0 & 3 \end{bmatrix}$
>
> Podemos obter $V^{-1}$ como:
>
> $V^{-1} = \begin{bmatrix} 0.571 & -0.286 & 0 \\ -0.286 & 1.143 & 0 \\ 0 & 0 & 0.333 \end{bmatrix}$
>
> E calcular a matriz $L$ tal que $V^{-1} = L'L$ usando a decomposiÃ§Ã£o de Cholesky, obtendo (aproximadamente):
>
> $L = \begin{bmatrix} 0.756 & 0 & 0 \\ -0.378 & 1.016 & 0 \\ 0 & 0 & 0.577 \end{bmatrix}$
>
> Para um vetor de erros $u = \begin{bmatrix} 1 \\ -1 \\ 2 \end{bmatrix}$, temos $\bar{u} = Lu \approx \begin{bmatrix} 0.756 \\ -1.394 \\ 1.154 \end{bmatrix}$.
>
> Note que $E(uu') = \sigma^2 V$ e $E(\bar{u}\bar{u}') \approx \sigma^2 I$. Se $\sigma^2 = 1$
>
> $E(uu') = \begin{bmatrix} 2 & 0.5 & 0 \\ 0.5 & 1 & 0 \\ 0 & 0 & 3 \end{bmatrix}$
>
> $E(\bar{u}\bar{u}') \approx \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$
>
> Onde $E(\bar{u}\bar{u}')$ Ã© a matriz identidade, o que ilustra que a transformaÃ§Ã£o usando L leva a erros com variÃ¢ncia constante.

#### DerivaÃ§Ã£o da Estimativa GLS
A estimativa GLS Ã© dada por:
$$ \hat{\beta}_{GLS} = (\bar{X}'\bar{X})^{-1}\bar{X}'\bar{y} = (X'L'LX)^{-1}X'L'Ly = (X'V^{-1}X)^{-1}X'V^{-1}y $$
[8.3.5].

**ProposiÃ§Ã£o 1** Uma caracterizaÃ§Ã£o alternativa para a matriz $L$ pode ser dada atravÃ©s da decomposiÃ§Ã£o de Cholesky de $V^{-1}$. Sendo $V^{-1}$ uma matriz simÃ©trica e definida positiva, existe uma matriz triangular inferior $L$ com elementos diagonais positivos tal que $V^{-1} = LL'$.

*Prova*: A decomposiÃ§Ã£o de Cholesky Ã© um resultado clÃ¡ssico da Ã¡lgebra linear, que garante a existÃªncia e unicidade da matriz $L$ sob estas condiÃ§Ãµes.

#### Log-VerossimilhanÃ§a Condicional
A log-verossimilhanÃ§a de $y$ condicional a $X$ sob a hipÃ³tese de que $y|X \sim N(X\beta, \sigma^2V)$ Ã© dada por [8.3.10]:
$$ \mathcal{L}(\beta, \sigma^2 | y, X) = -\frac{T}{2}\log(2\pi) -\frac{1}{2}\log|\sigma^2V| - \frac{1}{2}(y-X\beta)'(\sigma^2V)^{-1}(y-X\beta) $$

Utilizando o fato de que $V^{-1} = L'L$, podemos reescrever o Ãºltimo termo da equaÃ§Ã£o acima, como em [8.3.11]:
$$- \frac{1}{2}(y-X\beta)'(\sigma^2V)^{-1}(y-X\beta) = -\frac{1}{2\sigma^2}(y-X\beta)'L'L(y-X\beta) = -\frac{1}{2\sigma^2}(Ly-LX\beta)'(Ly-LX\beta) = -\frac{1}{2\sigma^2}(\bar{y}-\bar{X}\beta)'(\bar{y}-\bar{X}\beta)$$
O termo do meio da equaÃ§Ã£o da log-verossimilhanÃ§a Ã© dado por [8.3.12]:
$$ -\frac{1}{2} \log|\sigma^2V| = -\frac{T}{2} \log(\sigma^2) - \log|det(L)|$$
O determinante de L, que denotamos por $|det(L)|$ Ã© um valor constante. Substituindo [8.3.11] e [8.3.12] na log-verossimilhanÃ§a condicional, temos [8.3.13]:
$$ \mathcal{L}(\beta, \sigma^2 | y, X) = -\frac{T}{2}\log(2\pi) -\frac{T}{2}\log(\sigma^2) + \log|det(L)| - \frac{1}{2\sigma^2}(\bar{y}-\bar{X}\beta)'(\bar{y}-\bar{X}\beta) $$
Maximizar a log-verossimilhanÃ§a com respeito a $\beta$ Ã© equivalente a minimizar o termo quadrÃ¡tico $(\bar{y} - \bar{X}\beta)'(\bar{y} - \bar{X}\beta)$. Isso Ã© atingido atravÃ©s da regressÃ£o de $\bar{y}$ em $\bar{X}$, o que resulta na estimativa GLS [8.3.5].

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Vamos considerar um caso simples com 3 observaÃ§Ãµes (T=3) e um regressor (K=1). Suponha que temos as seguintes matrizes e vetores:
>
> $X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \end{bmatrix}$, $y = \begin{bmatrix} 3 \\ 5 \\ 8 \end{bmatrix}$ e $V$ como no exemplo anterior.
>
>  $V^{-1} = \begin{bmatrix} 0.571 & -0.286 & 0 \\ -0.286 & 1.143 & 0 \\ 0 & 0 & 0.333 \end{bmatrix}$
>
>  EntÃ£o, $\hat{\beta}_{GLS} = (X'V^{-1}X)^{-1}X'V^{-1}y$.
>
>  Calculando $X'V^{-1}X$:
>
>  $X'V^{-1}X = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 0.571 & -0.286 & 0 \\ -0.286 & 1.143 & 0 \\ 0 & 0 & 0.333 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \end{bmatrix} = \begin{bmatrix} 1.73 & 2.60 \\ 2.60 & 4.60 \end{bmatrix}$
>
>  Calculando $(X'V^{-1}X)^{-1}$:
>
>  $(X'V^{-1}X)^{-1} = \begin{bmatrix} 4.00 & -2.26 \\ -2.26 & 1.50 \end{bmatrix}$
>
>  Calculando $X'V^{-1}y$:
>
>  $X'V^{-1}y =  \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 0.571 & -0.286 & 0 \\ -0.286 & 1.143 & 0 \\ 0 & 0 & 0.333 \end{bmatrix} \begin{bmatrix} 3 \\ 5 \\ 8 \end{bmatrix} = \begin{bmatrix} 4.87 \\ 12.33 \end{bmatrix} $
>
>  Finalmente, $\hat{\beta}_{GLS} = (X'V^{-1}X)^{-1}X'V^{-1}y = \begin{bmatrix} 4.00 & -2.26 \\ -2.26 & 1.50 \end{bmatrix} \begin{bmatrix} 4.87 \\ 12.33 \end{bmatrix} = \begin{bmatrix} -8.15 \\ 4.00 \end{bmatrix}$
>
>  Logo, $\hat{\beta}_{GLS} \approx \begin{bmatrix} -8.15 \\ 4.00 \end{bmatrix}$. Isso significa que, de acordo com o modelo GLS, a interceptaÃ§Ã£o Ã© aproximadamente -8.15 e o coeficiente angular do regressor Ã© 4.00.

**Teorema 1** A estimativa de mÃ¡xima verossimilhanÃ§a para $\sigma^2$ neste modelo com $V$ conhecida Ã© dada por:
$$ \hat{\sigma}^2_{MLE} = \frac{1}{T}(\bar{y} - \bar{X}\hat{\beta}_{GLS})'(\bar{y} - \bar{X}\hat{\beta}_{GLS}) = \frac{1}{T}(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS})$$
*Prova*: 
I.  A log-verossimilhanÃ§a Ã© dada por:
$$ \mathcal{L}(\beta, \sigma^2 | y, X) = -\frac{T}{2}\log(2\pi) -\frac{T}{2}\log(\sigma^2) + \log|det(L)| - \frac{1}{2\sigma^2}(\bar{y}-\bar{X}\beta)'(\bar{y}-\bar{X}\beta) $$
II. Derivamos a log-verossimilhanÃ§a com respeito a $\sigma^2$:
$$ \frac{\partial \mathcal{L}}{\partial \sigma^2} = -\frac{T}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}(\bar{y} - \bar{X}\beta)'(\bar{y} - \bar{X}\beta) $$
III. Igualamos a derivada a zero para encontrar o mÃ¡ximo:
$$ -\frac{T}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}(\bar{y} - \bar{X}\beta)'(\bar{y} - \bar{X}\beta) = 0 $$
IV. Multiplicamos ambos os lados por $2(\sigma^2)^2$:
$$ -T\sigma^2 + (\bar{y} - \bar{X}\beta)'(\bar{y} - \bar{X}\beta) = 0 $$
V. Resolvemos para $\sigma^2$:
$$ \sigma^2 = \frac{1}{T}(\bar{y} - \bar{X}\beta)'(\bar{y} - \bar{X}\beta) $$
VI. SubstituÃ­mos $\beta$ pela estimativa GLS $\hat{\beta}_{GLS}$ para obter $\hat{\sigma}^2_{MLE}$:
$$ \hat{\sigma}^2_{MLE} = \frac{1}{T}(\bar{y} - \bar{X}\hat{\beta}_{GLS})'(\bar{y} - \bar{X}\hat{\beta}_{GLS}) $$
VII. Usamos o fato de que $(\bar{y}-\bar{X}\hat{\beta}_{GLS})'(\bar{y}-\bar{X}\hat{\beta}_{GLS}) = (Ly-LX\hat{\beta}_{GLS})'(Ly-LX\hat{\beta}_{GLS}) = (y-X\hat{\beta}_{GLS})'L'L(y-X\hat{\beta}_{GLS}) = (y-X\hat{\beta}_{GLS})'V^{-1}(y-X\hat{\beta}_{GLS})$:
$$ \hat{\sigma}^2_{MLE} = \frac{1}{T}(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS}) $$
Portanto, demonstramos que a estimativa de mÃ¡xima verossimilhanÃ§a para $\sigma^2$ Ã© dada por
$$ \hat{\sigma}^2_{MLE} = \frac{1}{T}(\bar{y} - \bar{X}\hat{\beta}_{GLS})'(\bar{y} - \bar{X}\hat{\beta}_{GLS}) = \frac{1}{T}(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS})$$
â– 

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Usando os dados do exemplo anterior, onde $y = \begin{bmatrix} 3 \\ 5 \\ 8 \end{bmatrix}$, $X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \end{bmatrix}$ e $\hat{\beta}_{GLS} \approx \begin{bmatrix} -8.15 \\ 4.00 \end{bmatrix}$, e $V^{-1}$ dada anteriormente.
>
> $\hat{y} = X\hat{\beta}_{GLS} = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \end{bmatrix} \begin{bmatrix} -8.15 \\ 4.00 \end{bmatrix} = \begin{bmatrix} -4.15 \\ -0.15 \\ 3.85 \end{bmatrix}$
>
> Os resÃ­duos sÃ£o:
>
> $\hat{u} = y - \hat{y} = \begin{bmatrix} 3 \\ 5 \\ 8 \end{bmatrix} - \begin{bmatrix} -4.15 \\ -0.15 \\ 3.85 \end{bmatrix} = \begin{bmatrix} 7.15 \\ 5.15 \\ 4.15 \end{bmatrix}$
>
> Calculando $(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS})$:
>
> $(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS}) = \begin{bmatrix} 7.15 & 5.15 & 4.15 \end{bmatrix} \begin{bmatrix} 0.571 & -0.286 & 0 \\ -0.286 & 1.143 & 0 \\ 0 & 0 & 0.333 \end{bmatrix} \begin{bmatrix} 7.15 \\ 5.15 \\ 4.15 \end{bmatrix} = 29.72$
>
>  EntÃ£o $\hat{\sigma}^2_{MLE} = \frac{1}{T}(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS}) = \frac{1}{3} \times 29.72 \approx 9.91$.
>
>  Este Ã© o valor da estimativa de mÃ¡xima verossimilhanÃ§a para $\sigma^2$.

#### EquivalÃªncia entre GLS e MLE
A derivaÃ§Ã£o acima demonstra que a estimativa GLS Ã© idÃªntica Ã  estimativa de mÃ¡xima verossimilhanÃ§a quando a matriz de covariÃ¢ncia $V$ Ã© conhecida [^2, ^19]. Essa equivalÃªncia estabelece a otimalidade do estimador GLS sob as condiÃ§Ãµes de normalidade e conhecimento da estrutura de covariÃ¢ncia.
Em essÃªncia, o estimador de mÃ¡xima verossimilhanÃ§a (MLE) tambÃ©m maximiza a probabilidade de observarmos os dados, dadas as nossas hipÃ³teses sobre o modelo (distribuiÃ§Ã£o dos erros, por exemplo) e, por conseguinte, Ã© uma ferramenta de estimaÃ§Ã£o bastante Ãºtil e eficiente.

**Teorema 1.1** O estimador $\hat{\sigma}^2_{MLE}$ Ã© viesado.

*Prova*: 
I. Sabemos que $\hat{\sigma}^2_{MLE} = \frac{1}{T}(\bar{y} - \bar{X}\hat{\beta}_{GLS})'(\bar{y} - \bar{X}\hat{\beta}_{GLS})$.
II.  Definimos $\bar{u} = \bar{y} - \bar{X}\hat{\beta}_{GLS}$. Assim $\hat{\sigma}^2_{MLE} = \frac{1}{T}\bar{u}'\bar{u}$.
III. Sabemos que $\hat{\beta}_{GLS} = (X'V^{-1}X)^{-1}X'V^{-1}y$. Logo $\hat{\beta}_{GLS}$ Ã© funÃ§Ã£o de $y$.
IV. Como $\bar{y} = \bar{X}\beta + \bar{u}$, entÃ£o $\bar{u} = \bar{y} - \bar{X}\beta = Ly - LX\beta = L(X\beta + u) - LX\beta = Lu$.
V. Assim $\bar{u} = L(y - X\hat{\beta}_{GLS}) = L(X(\beta-\hat{\beta}_{GLS}) + u)$.
VI. Da seÃ§Ã£o 8.3, sabemos que $\hat{\beta}_{GLS}$ Ã© nÃ£o viesado, ou seja $E[\hat{\beta}_{GLS}] = \beta$.
VII. Consequentemente, $E(\bar{u}) = 0$.
VIII. TambÃ©m sabemos que  $E[\bar{u}\bar{u}'] = \sigma^2 I$.
IX. Assim, $E[\hat{\sigma}^2_{MLE}] = E[\frac{1}{T}\bar{u}'\bar{u}] = \frac{1}{T}E[tr(\bar{u}\bar{u}')] = \frac{1}{T}tr(E[\bar{u}\bar{u}']) = \frac{1}{T}tr(\sigma^2I) = \frac{1}{T} \sigma^2 T = \sigma^2$.
X. Contudo, o item anterior estÃ¡ incorreto porque nÃ³s usamos $E[\bar{u}\bar{u}'] = \sigma^2 I$. Isso sÃ³ Ã© verdade para $\bar{u} = Lu$. O que temos, no entanto, Ã© $\bar{u}$ definido como $ \bar{y} - \bar{X}\hat{\beta}_{GLS}$.
XI. A forma correta de derivar o valor esperado Ã©:
$E[\hat{\sigma}^2_{MLE}] = E[\frac{1}{T}(\bar{y} - \bar{X}\hat{\beta}_{GLS})'(\bar{y} - \bar{X}\hat{\beta}_{GLS})] = E[\frac{1}{T} \bar{u}'\bar{u}]$, onde $\bar{u}$ representa os resÃ­duos estimados e nÃ£o os resÃ­duos populacionais.
XII. Sabemos que $E[\bar{u}'\bar{u}] = \sigma^2 (T - K)$, onde $K$ Ã© o nÃºmero de regressores. Logo, $E[\hat{\sigma}^2_{MLE}] = \frac{1}{T} \sigma^2(T-K) = \frac{T-K}{T}\sigma^2$
XIII. Como $\frac{T-K}{T} < 1$, concluÃ­mos que o estimador Ã© viesado para baixo.
â– 

**CorolÃ¡rio 1.1** Um estimador nÃ£o viesado para $\sigma^2$ pode ser obtido atravÃ©s de:
$$\hat{\sigma}^2 = \frac{1}{T-K} (\bar{y} - \bar{X}\hat{\beta}_{GLS})'(\bar{y} - \bar{X}\hat{\beta}_{GLS}) = \frac{1}{T-K}(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS})$$
*Prova:*
I.  Sabemos que $E[\hat{\sigma}^2_{MLE}] = \frac{T-K}{T}\sigma^2$.
II.  Definimos $\hat{\sigma}^2 = c \cdot \hat{\sigma}^2_{MLE}$, onde $c$ Ã© uma constante.
III.  Queremos que $E[\hat{\sigma}^2] = \sigma^2$.
IV.  Assim, $E[\hat{\sigma}^2] = E[c \cdot \hat{\sigma}^2_{MLE}] = c \cdot E[\hat{\sigma}^2_{MLE}] = c \frac{T-K}{T}\sigma^2 = \sigma^2$.
V.  Portanto, $c \frac{T-K}{T} = 1$, o que implica $c = \frac{T}{T-K}$.
VI.  Consequentemente, $\hat{\sigma}^2 = \frac{T}{T-K} \hat{\sigma}^2_{MLE} = \frac{T}{T-K} \frac{1}{T}(\bar{y} - \bar{X}\hat{\beta}_{GLS})'(\bar{y} - \bar{X}\hat{\beta}_{GLS}) = \frac{1}{T-K}(\bar{y} - \bar{X}\hat{\beta}_{GLS})'(\bar{y} - \bar{X}\hat{\beta}_{GLS})$.
VII.  Substituindo $(\bar{y} - \bar{X}\hat{\beta}_{GLS})'(\bar{y} - \bar{X}\hat{\beta}_{GLS})$ por $(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS})$, obtemos o resultado desejado:
$$\hat{\sigma}^2 = \frac{1}{T-K} (\bar{y} - \bar{X}\hat{\beta}_{GLS})'(\bar{y} - \bar{X}\hat{\beta}_{GLS}) = \frac{1}{T-K}(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS})$$
â– 

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Usando o exemplo anterior, onde $\hat{\sigma}^2_{MLE} \approx 9.91$, e sabendo que temos T=3 e K=2, o estimador nÃ£o viesado para $\sigma^2$ Ã©:
>
> $\hat{\sigma}^2 = \frac{1}{T-K}(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS}) = \frac{1}{3-2} \times 29.72 \approx 29.72$
>
>  Observe que $\hat{\sigma}^2 = \frac{T}{T-K}\hat{\sigma}^2_{MLE} = \frac{3}{3-2} 9.91 = 3 \times 9.91 \approx 29.72$, o que confirma a correÃ§Ã£o pelo viÃ©s.

### ConclusÃ£o

A equivalÃªncia entre a estimativa GLS e MLE em modelos com matriz de covariÃ¢ncia conhecida consolida a importÃ¢ncia da abordagem GLS como uma ferramenta eficiente e otimizada para a estimaÃ§Ã£o de modelos de regressÃ£o quando a suposiÃ§Ã£o de homocedasticidade nÃ£o Ã© vÃ¡lida [^1, ^2, ^3, ^4, ^5, ^6]. Em situaÃ§Ãµes onde a matriz de covariÃ¢ncia Ã© conhecida a priori, ao invÃ©s de necessitar de estimaÃ§Ã£o atravÃ©s de mÃ©todos mais complexos, a abordagem GLS se torna uma forma direta e eficiente de obter os parÃ¢metros que maximizam a funÃ§Ã£o de verossimilhanÃ§a, ao mesmo tempo em que obtÃ©m um estimador de variÃ¢ncia mÃ­nima entre os estimadores nÃ£o viesados [^2, ^19]. Este resultado fornece uma forte justificativa teÃ³rica para a utilizaÃ§Ã£o de GLS em modelos de sÃ©ries temporais e outros contextos onde as caracterÃ­sticas dos resÃ­duos requerem uma modelagem mais complexa.

### ReferÃªncias
[^1]:  SeÃ§Ã£o 8.1, "Review of Ordinary Least Squares".
[^2]:  SeÃ§Ã£o 8.3, "Generalized Least Squares".
[^3]:  SeÃ§Ã£o 8.2, "Ordinary Least Squares Under More General Conditions".
[^4]: Tabela 8.1, "Properties of OLS Estimates and Test Statistics Under Various Assumptions".
[^5]:  SeÃ§Ã£o 8.2, "Case 6. Errors Serially Uncorrelated but with General Heteroskedasticity".
[^6]:  SeÃ§Ã£o 8.2, "Case 3. Error Term i.i.d. Non-Gaussian and Independent of Explanatory Variables".
[^19]: Texto prÃ³ximo a Eq. 8.3.1 e a deduÃ§Ã£o do estimador GLS 8.3.5
<!-- END -->
