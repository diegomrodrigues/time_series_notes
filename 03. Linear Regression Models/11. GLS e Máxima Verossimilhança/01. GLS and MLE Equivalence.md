## GLS e M√°xima Verossimilhan√ßa: Equival√™ncia em Modelos com Covari√¢ncia Conhecida

### Introdu√ß√£o
Neste cap√≠tulo, exploramos a fundo a estima√ß√£o por M√≠nimos Quadrados Ordin√°rios (OLS) sob diversas condi√ß√µes. Como vimos anteriormente, a suposi√ß√£o de que $E(uu') = \sigma^2I_T$ nem sempre √© v√°lida, e embora a OLS ainda possa ser utilizada, os M√≠nimos Quadrados Generalizados (GLS) se mostram uma alternativa prefer√≠vel em diversas situa√ß√µes [^1, ^2, ^3, ^4, ^5, ^6]. Expandindo o conceito apresentado, vamos agora demonstrar que em modelos onde a matriz de covari√¢ncia √© conhecida, a estimativa GLS, que abordamos na se√ß√£o anterior, tamb√©m corresponde √† estimativa de M√°xima Verossimilhan√ßa (MLE), estabelecendo assim a otimalidade desta abordagem [^2].

### Conceitos Fundamentais
Revisitando o modelo de regress√£o b√°sica, dado por $y = X\beta + u$, com $u|X \sim N(0, \sigma^2V)$, onde $V$ √© uma matriz conhecida $(T \times T)$ sim√©trica e definida positiva [^2]. Conforme discutido, podemos encontrar uma matriz n√£o singular $L$ tal que $V^{-1} = L'L$ [^2, ^19]. Introduzimos a transforma√ß√£o dos res√≠duos da popula√ß√£o, $\bar{u} = Lu$, o que leva a $\bar{u}$ com m√©dia 0 e vari√¢ncia condicional em $X$ dada por:
$$E(\bar{u}\bar{u}'|X) = LE(uu'|X)L' = L\sigma^2VL'$$
Como $V = (V^{-1})^{-1} = (L'L)^{-1}$, temos que:
$$E(\bar{u}\bar{u}'|X) = \sigma^2L(L'L)^{-1}L' = \sigma^2I_T$$
Portanto, o modelo transformado $\bar{y} = \bar{X}\beta + \bar{u}$ possui res√≠duos que satisfazem a suposi√ß√£o 8.2, o que significa que todos os resultados daquela se√ß√£o podem ser aplicados a este modelo transformado [^2].

> üí° **Exemplo Num√©rico:**
> Suponha que temos um modelo com 3 observa√ß√µes (T=3), um regressor (K=1) e a seguinte matriz de covari√¢ncia dos erros:
>
> $V = \begin{bmatrix} 2 & 0.5 & 0 \\ 0.5 & 1 & 0 \\ 0 & 0 & 3 \end{bmatrix}$
>
> Podemos obter $V^{-1}$ como:
>
> $V^{-1} = \begin{bmatrix} 0.571 & -0.286 & 0 \\ -0.286 & 1.143 & 0 \\ 0 & 0 & 0.333 \end{bmatrix}$
>
> E calcular a matriz $L$ tal que $V^{-1} = L'L$ usando a decomposi√ß√£o de Cholesky, obtendo (aproximadamente):
>
> $L = \begin{bmatrix} 0.756 & 0 & 0 \\ -0.378 & 1.016 & 0 \\ 0 & 0 & 0.577 \end{bmatrix}$
>
> Para um vetor de erros $u = \begin{bmatrix} 1 \\ -1 \\ 2 \end{bmatrix}$, temos $\bar{u} = Lu \approx \begin{bmatrix} 0.756 \\ -1.394 \\ 1.154 \end{bmatrix}$.
>
> Note que $E(uu') = \sigma^2 V$ e $E(\bar{u}\bar{u}') \approx \sigma^2 I$. Se $\sigma^2 = 1$
>
> $E(uu') = \begin{bmatrix} 2 & 0.5 & 0 \\ 0.5 & 1 & 0 \\ 0 & 0 & 3 \end{bmatrix}$
>
> $E(\bar{u}\bar{u}') \approx \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$
>
> Onde $E(\bar{u}\bar{u}')$ √© a matriz identidade, o que ilustra que a transforma√ß√£o usando L leva a erros com vari√¢ncia constante.

#### Deriva√ß√£o da Estimativa GLS
A estimativa GLS √© dada por:
$$ \hat{\beta}_{GLS} = (\bar{X}'\bar{X})^{-1}\bar{X}'\bar{y} = (X'L'LX)^{-1}X'L'Ly = (X'V^{-1}X)^{-1}X'V^{-1}y $$
[8.3.5].

**Proposi√ß√£o 1** Uma caracteriza√ß√£o alternativa para a matriz $L$ pode ser dada atrav√©s da decomposi√ß√£o de Cholesky de $V^{-1}$. Sendo $V^{-1}$ uma matriz sim√©trica e definida positiva, existe uma matriz triangular inferior $L$ com elementos diagonais positivos tal que $V^{-1} = LL'$.

*Prova*: A decomposi√ß√£o de Cholesky √© um resultado cl√°ssico da √°lgebra linear, que garante a exist√™ncia e unicidade da matriz $L$ sob estas condi√ß√µes.

#### Log-Verossimilhan√ßa Condicional
A log-verossimilhan√ßa de $y$ condicional a $X$ sob a hip√≥tese de que $y|X \sim N(X\beta, \sigma^2V)$ √© dada por [8.3.10]:
$$ \mathcal{L}(\beta, \sigma^2 | y, X) = -\frac{T}{2}\log(2\pi) -\frac{1}{2}\log|\sigma^2V| - \frac{1}{2}(y-X\beta)'(\sigma^2V)^{-1}(y-X\beta) $$

Utilizando o fato de que $V^{-1} = L'L$, podemos reescrever o √∫ltimo termo da equa√ß√£o acima, como em [8.3.11]:
$$- \frac{1}{2}(y-X\beta)'(\sigma^2V)^{-1}(y-X\beta) = -\frac{1}{2\sigma^2}(y-X\beta)'L'L(y-X\beta) = -\frac{1}{2\sigma^2}(Ly-LX\beta)'(Ly-LX\beta) = -\frac{1}{2\sigma^2}(\bar{y}-\bar{X}\beta)'(\bar{y}-\bar{X}\beta)$$
O termo do meio da equa√ß√£o da log-verossimilhan√ßa √© dado por [8.3.12]:
$$ -\frac{1}{2} \log|\sigma^2V| = -\frac{T}{2} \log(\sigma^2) - \log|det(L)|$$
O determinante de L, que denotamos por $|det(L)|$ √© um valor constante. Substituindo [8.3.11] e [8.3.12] na log-verossimilhan√ßa condicional, temos [8.3.13]:
$$ \mathcal{L}(\beta, \sigma^2 | y, X) = -\frac{T}{2}\log(2\pi) -\frac{T}{2}\log(\sigma^2) + \log|det(L)| - \frac{1}{2\sigma^2}(\bar{y}-\bar{X}\beta)'(\bar{y}-\bar{X}\beta) $$
Maximizar a log-verossimilhan√ßa com respeito a $\beta$ √© equivalente a minimizar o termo quadr√°tico $(\bar{y} - \bar{X}\beta)'(\bar{y} - \bar{X}\beta)$. Isso √© atingido atrav√©s da regress√£o de $\bar{y}$ em $\bar{X}$, o que resulta na estimativa GLS [8.3.5].

> üí° **Exemplo Num√©rico:**
> Vamos considerar um caso simples com 3 observa√ß√µes (T=3) e um regressor (K=1). Suponha que temos as seguintes matrizes e vetores:
>
> $X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \end{bmatrix}$, $y = \begin{bmatrix} 3 \\ 5 \\ 8 \end{bmatrix}$ e $V$ como no exemplo anterior.
>
>  $V^{-1} = \begin{bmatrix} 0.571 & -0.286 & 0 \\ -0.286 & 1.143 & 0 \\ 0 & 0 & 0.333 \end{bmatrix}$
>
>  Ent√£o, $\hat{\beta}_{GLS} = (X'V^{-1}X)^{-1}X'V^{-1}y$.
>
>  Calculando $X'V^{-1}X$:
>
>  $X'V^{-1}X = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 0.571 & -0.286 & 0 \\ -0.286 & 1.143 & 0 \\ 0 & 0 & 0.333 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \end{bmatrix} = \begin{bmatrix} 1.73 & 2.60 \\ 2.60 & 4.60 \end{bmatrix}$
>
>  Calculando $(X'V^{-1}X)^{-1}$:
>
>  $(X'V^{-1}X)^{-1} = \begin{bmatrix} 4.00 & -2.26 \\ -2.26 & 1.50 \end{bmatrix}$
>
>  Calculando $X'V^{-1}y$:
>
>  $X'V^{-1}y =  \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 0.571 & -0.286 & 0 \\ -0.286 & 1.143 & 0 \\ 0 & 0 & 0.333 \end{bmatrix} \begin{bmatrix} 3 \\ 5 \\ 8 \end{bmatrix} = \begin{bmatrix} 4.87 \\ 12.33 \end{bmatrix} $
>
>  Finalmente, $\hat{\beta}_{GLS} = (X'V^{-1}X)^{-1}X'V^{-1}y = \begin{bmatrix} 4.00 & -2.26 \\ -2.26 & 1.50 \end{bmatrix} \begin{bmatrix} 4.87 \\ 12.33 \end{bmatrix} = \begin{bmatrix} -8.15 \\ 4.00 \end{bmatrix}$
>
>  Logo, $\hat{\beta}_{GLS} \approx \begin{bmatrix} -8.15 \\ 4.00 \end{bmatrix}$. Isso significa que, de acordo com o modelo GLS, a intercepta√ß√£o √© aproximadamente -8.15 e o coeficiente angular do regressor √© 4.00.

**Teorema 1** A estimativa de m√°xima verossimilhan√ßa para $\sigma^2$ neste modelo com $V$ conhecida √© dada por:
$$ \hat{\sigma}^2_{MLE} = \frac{1}{T}(\bar{y} - \bar{X}\hat{\beta}_{GLS})'(\bar{y} - \bar{X}\hat{\beta}_{GLS}) = \frac{1}{T}(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS})$$
*Prova*: 
I.  A log-verossimilhan√ßa √© dada por:
$$ \mathcal{L}(\beta, \sigma^2 | y, X) = -\frac{T}{2}\log(2\pi) -\frac{T}{2}\log(\sigma^2) + \log|det(L)| - \frac{1}{2\sigma^2}(\bar{y}-\bar{X}\beta)'(\bar{y}-\bar{X}\beta) $$
II. Derivamos a log-verossimilhan√ßa com respeito a $\sigma^2$:
$$ \frac{\partial \mathcal{L}}{\partial \sigma^2} = -\frac{T}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}(\bar{y} - \bar{X}\beta)'(\bar{y} - \bar{X}\beta) $$
III. Igualamos a derivada a zero para encontrar o m√°ximo:
$$ -\frac{T}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}(\bar{y} - \bar{X}\beta)'(\bar{y} - \bar{X}\beta) = 0 $$
IV. Multiplicamos ambos os lados por $2(\sigma^2)^2$:
$$ -T\sigma^2 + (\bar{y} - \bar{X}\beta)'(\bar{y} - \bar{X}\beta) = 0 $$
V. Resolvemos para $\sigma^2$:
$$ \sigma^2 = \frac{1}{T}(\bar{y} - \bar{X}\beta)'(\bar{y} - \bar{X}\beta) $$
VI. Substitu√≠mos $\beta$ pela estimativa GLS $\hat{\beta}_{GLS}$ para obter $\hat{\sigma}^2_{MLE}$:
$$ \hat{\sigma}^2_{MLE} = \frac{1}{T}(\bar{y} - \bar{X}\hat{\beta}_{GLS})'(\bar{y} - \bar{X}\hat{\beta}_{GLS}) $$
VII. Usamos o fato de que $(\bar{y}-\bar{X}\hat{\beta}_{GLS})'(\bar{y}-\bar{X}\hat{\beta}_{GLS}) = (Ly-LX\hat{\beta}_{GLS})'(Ly-LX\hat{\beta}_{GLS}) = (y-X\hat{\beta}_{GLS})'L'L(y-X\hat{\beta}_{GLS}) = (y-X\hat{\beta}_{GLS})'V^{-1}(y-X\hat{\beta}_{GLS})$:
$$ \hat{\sigma}^2_{MLE} = \frac{1}{T}(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS}) $$
Portanto, demonstramos que a estimativa de m√°xima verossimilhan√ßa para $\sigma^2$ √© dada por
$$ \hat{\sigma}^2_{MLE} = \frac{1}{T}(\bar{y} - \bar{X}\hat{\beta}_{GLS})'(\bar{y} - \bar{X}\hat{\beta}_{GLS}) = \frac{1}{T}(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS})$$
‚ñ†

> üí° **Exemplo Num√©rico:**
> Usando os dados do exemplo anterior, onde $y = \begin{bmatrix} 3 \\ 5 \\ 8 \end{bmatrix}$, $X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \end{bmatrix}$ e $\hat{\beta}_{GLS} \approx \begin{bmatrix} -8.15 \\ 4.00 \end{bmatrix}$, e $V^{-1}$ dada anteriormente.
>
> $\hat{y} = X\hat{\beta}_{GLS} = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \end{bmatrix} \begin{bmatrix} -8.15 \\ 4.00 \end{bmatrix} = \begin{bmatrix} -4.15 \\ -0.15 \\ 3.85 \end{bmatrix}$
>
> Os res√≠duos s√£o:
>
> $\hat{u} = y - \hat{y} = \begin{bmatrix} 3 \\ 5 \\ 8 \end{bmatrix} - \begin{bmatrix} -4.15 \\ -0.15 \\ 3.85 \end{bmatrix} = \begin{bmatrix} 7.15 \\ 5.15 \\ 4.15 \end{bmatrix}$
>
> Calculando $(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS})$:
>
> $(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS}) = \begin{bmatrix} 7.15 & 5.15 & 4.15 \end{bmatrix} \begin{bmatrix} 0.571 & -0.286 & 0 \\ -0.286 & 1.143 & 0 \\ 0 & 0 & 0.333 \end{bmatrix} \begin{bmatrix} 7.15 \\ 5.15 \\ 4.15 \end{bmatrix} = 29.72$
>
>  Ent√£o $\hat{\sigma}^2_{MLE} = \frac{1}{T}(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS}) = \frac{1}{3} \times 29.72 \approx 9.91$.
>
>  Este √© o valor da estimativa de m√°xima verossimilhan√ßa para $\sigma^2$.

#### Equival√™ncia entre GLS e MLE
A deriva√ß√£o acima demonstra que a estimativa GLS √© id√™ntica √† estimativa de m√°xima verossimilhan√ßa quando a matriz de covari√¢ncia $V$ √© conhecida [^2, ^19]. Essa equival√™ncia estabelece a otimalidade do estimador GLS sob as condi√ß√µes de normalidade e conhecimento da estrutura de covari√¢ncia.
Em ess√™ncia, o estimador de m√°xima verossimilhan√ßa (MLE) tamb√©m maximiza a probabilidade de observarmos os dados, dadas as nossas hip√≥teses sobre o modelo (distribui√ß√£o dos erros, por exemplo) e, por conseguinte, √© uma ferramenta de estima√ß√£o bastante √∫til e eficiente.

**Teorema 1.1** O estimador $\hat{\sigma}^2_{MLE}$ √© viesado.

*Prova*: 
I. Sabemos que $\hat{\sigma}^2_{MLE} = \frac{1}{T}(\bar{y} - \bar{X}\hat{\beta}_{GLS})'(\bar{y} - \bar{X}\hat{\beta}_{GLS})$.
II.  Definimos $\bar{u} = \bar{y} - \bar{X}\hat{\beta}_{GLS}$. Assim $\hat{\sigma}^2_{MLE} = \frac{1}{T}\bar{u}'\bar{u}$.
III. Sabemos que $\hat{\beta}_{GLS} = (X'V^{-1}X)^{-1}X'V^{-1}y$. Logo $\hat{\beta}_{GLS}$ √© fun√ß√£o de $y$.
IV. Como $\bar{y} = \bar{X}\beta + \bar{u}$, ent√£o $\bar{u} = \bar{y} - \bar{X}\beta = Ly - LX\beta = L(X\beta + u) - LX\beta = Lu$.
V. Assim $\bar{u} = L(y - X\hat{\beta}_{GLS}) = L(X(\beta-\hat{\beta}_{GLS}) + u)$.
VI. Da se√ß√£o 8.3, sabemos que $\hat{\beta}_{GLS}$ √© n√£o viesado, ou seja $E[\hat{\beta}_{GLS}] = \beta$.
VII. Consequentemente, $E(\bar{u}) = 0$.
VIII. Tamb√©m sabemos que  $E[\bar{u}\bar{u}'] = \sigma^2 I$.
IX. Assim, $E[\hat{\sigma}^2_{MLE}] = E[\frac{1}{T}\bar{u}'\bar{u}] = \frac{1}{T}E[tr(\bar{u}\bar{u}')] = \frac{1}{T}tr(E[\bar{u}\bar{u}']) = \frac{1}{T}tr(\sigma^2I) = \frac{1}{T} \sigma^2 T = \sigma^2$.
X. Contudo, o item anterior est√° incorreto porque n√≥s usamos $E[\bar{u}\bar{u}'] = \sigma^2 I$. Isso s√≥ √© verdade para $\bar{u} = Lu$. O que temos, no entanto, √© $\bar{u}$ definido como $ \bar{y} - \bar{X}\hat{\beta}_{GLS}$.
XI. A forma correta de derivar o valor esperado √©:
$E[\hat{\sigma}^2_{MLE}] = E[\frac{1}{T}(\bar{y} - \bar{X}\hat{\beta}_{GLS})'(\bar{y} - \bar{X}\hat{\beta}_{GLS})] = E[\frac{1}{T} \bar{u}'\bar{u}]$, onde $\bar{u}$ representa os res√≠duos estimados e n√£o os res√≠duos populacionais.
XII. Sabemos que $E[\bar{u}'\bar{u}] = \sigma^2 (T - K)$, onde $K$ √© o n√∫mero de regressores. Logo, $E[\hat{\sigma}^2_{MLE}] = \frac{1}{T} \sigma^2(T-K) = \frac{T-K}{T}\sigma^2$
XIII. Como $\frac{T-K}{T} < 1$, conclu√≠mos que o estimador √© viesado para baixo.
‚ñ†

**Corol√°rio 1.1** Um estimador n√£o viesado para $\sigma^2$ pode ser obtido atrav√©s de:
$$\hat{\sigma}^2 = \frac{1}{T-K} (\bar{y} - \bar{X}\hat{\beta}_{GLS})'(\bar{y} - \bar{X}\hat{\beta}_{GLS}) = \frac{1}{T-K}(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS})$$
*Prova:*
I.  Sabemos que $E[\hat{\sigma}^2_{MLE}] = \frac{T-K}{T}\sigma^2$.
II.  Definimos $\hat{\sigma}^2 = c \cdot \hat{\sigma}^2_{MLE}$, onde $c$ √© uma constante.
III.  Queremos que $E[\hat{\sigma}^2] = \sigma^2$.
IV.  Assim, $E[\hat{\sigma}^2] = E[c \cdot \hat{\sigma}^2_{MLE}] = c \cdot E[\hat{\sigma}^2_{MLE}] = c \frac{T-K}{T}\sigma^2 = \sigma^2$.
V.  Portanto, $c \frac{T-K}{T} = 1$, o que implica $c = \frac{T}{T-K}$.
VI.  Consequentemente, $\hat{\sigma}^2 = \frac{T}{T-K} \hat{\sigma}^2_{MLE} = \frac{T}{T-K} \frac{1}{T}(\bar{y} - \bar{X}\hat{\beta}_{GLS})'(\bar{y} - \bar{X}\hat{\beta}_{GLS}) = \frac{1}{T-K}(\bar{y} - \bar{X}\hat{\beta}_{GLS})'(\bar{y} - \bar{X}\hat{\beta}_{GLS})$.
VII.  Substituindo $(\bar{y} - \bar{X}\hat{\beta}_{GLS})'(\bar{y} - \bar{X}\hat{\beta}_{GLS})$ por $(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS})$, obtemos o resultado desejado:
$$\hat{\sigma}^2 = \frac{1}{T-K} (\bar{y} - \bar{X}\hat{\beta}_{GLS})'(\bar{y} - \bar{X}\hat{\beta}_{GLS}) = \frac{1}{T-K}(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS})$$
‚ñ†

> üí° **Exemplo Num√©rico:**
> Usando o exemplo anterior, onde $\hat{\sigma}^2_{MLE} \approx 9.91$, e sabendo que temos T=3 e K=2, o estimador n√£o viesado para $\sigma^2$ √©:
>
> $\hat{\sigma}^2 = \frac{1}{T-K}(y - X\hat{\beta}_{GLS})'V^{-1}(y - X\hat{\beta}_{GLS}) = \frac{1}{3-2} \times 29.72 \approx 29.72$
>
>  Observe que $\hat{\sigma}^2 = \frac{T}{T-K}\hat{\sigma}^2_{MLE} = \frac{3}{3-2} 9.91 = 3 \times 9.91 \approx 29.72$, o que confirma a corre√ß√£o pelo vi√©s.

### Conclus√£o

A equival√™ncia entre a estimativa GLS e MLE em modelos com matriz de covari√¢ncia conhecida consolida a import√¢ncia da abordagem GLS como uma ferramenta eficiente e otimizada para a estima√ß√£o de modelos de regress√£o quando a suposi√ß√£o de homocedasticidade n√£o √© v√°lida [^1, ^2, ^3, ^4, ^5, ^6]. Em situa√ß√µes onde a matriz de covari√¢ncia √© conhecida a priori, ao inv√©s de necessitar de estima√ß√£o atrav√©s de m√©todos mais complexos, a abordagem GLS se torna uma forma direta e eficiente de obter os par√¢metros que maximizam a fun√ß√£o de verossimilhan√ßa, ao mesmo tempo em que obt√©m um estimador de vari√¢ncia m√≠nima entre os estimadores n√£o viesados [^2, ^19]. Este resultado fornece uma forte justificativa te√≥rica para a utiliza√ß√£o de GLS em modelos de s√©ries temporais e outros contextos onde as caracter√≠sticas dos res√≠duos requerem uma modelagem mais complexa.

### Refer√™ncias
[^1]:  Se√ß√£o 8.1, "Review of Ordinary Least Squares".
[^2]:  Se√ß√£o 8.3, "Generalized Least Squares".
[^3]:  Se√ß√£o 8.2, "Ordinary Least Squares Under More General Conditions".
[^4]: Tabela 8.1, "Properties of OLS Estimates and Test Statistics Under Various Assumptions".
[^5]:  Se√ß√£o 8.2, "Case 6. Errors Serially Uncorrelated but with General Heteroskedasticity".
[^6]:  Se√ß√£o 8.2, "Case 3. Error Term i.i.d. Non-Gaussian and Independent of Explanatory Variables".
[^19]: Texto pr√≥ximo a Eq. 8.3.1 e a dedu√ß√£o do estimador GLS 8.3.5
<!-- END -->
