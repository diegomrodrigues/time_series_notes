## Estima√ß√£o da Autocorrela√ß√£o de Primeira Ordem via Res√≠duos
### Introdu√ß√£o
Em continuidade √† discuss√£o sobre estima√ß√£o de modelos de regress√£o linear com erros correlacionados, focamos agora na estima√ß√£o do par√¢metro de autocorrela√ß√£o, especificamente, em um modelo de primeira ordem AR(1). Conforme visto anteriormente, ao lidarmos com modelos onde os res√≠duos apresentam autocorrela√ß√£o, os estimadores de M√≠nimos Quadrados Ordin√°rios (OLS) podem perder a efici√™ncia. Uma abordagem para lidar com esta quest√£o envolve o uso de estimadores de M√≠nimos Quadrados Generalizados (GLS), que levam em conta a estrutura de covari√¢ncia dos erros. Exploraremos m√©todos alternativos para estimar o par√¢metro de autocorrela√ß√£o, com foco em abordagens que utilizam os res√≠duos da regress√£o OLS. Em particular, discutiremos como a estimativa direta da autocorrela√ß√£o dos res√≠duos pode fornecer resultados assintoticamente equivalentes ao m√©todo iterativo de Cochrane-Orcutt, apresentado em [^8.3.15].

### Conceitos Fundamentais
Como abordado em [^8.3.16], o par√¢metro de autocorrela√ß√£o $\rho$ pode ser estimado por:
$$
\hat{\rho} = \frac{\sum_{t=1}^{T} \hat{u}_t \hat{u}_{t-1}}{\sum_{t=1}^{T} \hat{u}_{t-1}^2},
$$
onde $\hat{u}_t = y_t - x_t'b$ s√£o os res√≠duos da regress√£o OLS, e $b$ √© o estimador OLS de $\beta$. O texto menciona uma forma de simplificar as express√µes utilizando uma amostra com T+1 observa√ß√µes renomeadas como $y_0, y_1, ..., y_T$, com o objetivo de usar $T$ observa√ß√µes na estimativa condicional da m√°xima verossimilhan√ßa. Note que, ao aplicar essa reformula√ß√£o, temos:
$$\hat{u}_t = (y_t - \beta'x_t) + (\beta'x_t - b'x_t) = u_t + (\beta - b)'x_t$$
Assim, ao derivarmos o numerador da equa√ß√£o da autocorrela√ß√£o temos que:
$$ \sum_{t=1}^{T} \hat{u}_t \hat{u}_{t-1} = \sum_{t=1}^{T} [u_t + (\beta - b)'x_t][u_{t-1} + (\beta - b)'x_{t-1}] $$
Expandindo esta express√£o, obtemos:
$$ \sum_{t=1}^{T} \hat{u}_t \hat{u}_{t-1} =  \sum_{t=1}^{T} u_t u_{t-1} + \sum_{t=1}^{T}(\beta-b)'x_t u_{t-1} + \sum_{t=1}^{T} u_t (\beta-b)'x_{t-1} + \sum_{t=1}^{T} (\beta-b)'x_t x_{t-1}'(\beta-b) $$
Ao analisarmos essa expans√£o, e considerando que as premissas do modelo garantem que as covari√¢ncias entre $u_t$ e $x_t$ sejam zero, e sob as condi√ß√µes de que $b$ seja um estimador consistente para $\beta$ e que os limites de $\frac{1}{T}\sum_{t=1}^{T} x_t u_{t-1}$, $\frac{1}{T}\sum_{t=1}^{T} u_t x_{t-1}$ e $\frac{1}{T}\sum_{t=1}^{T} x_t x_{t-1}$ existam, chegamos a:
$$ \frac{1}{T} \sum_{t=1}^{T} \hat{u}_t \hat{u}_{t-1} \approx \frac{1}{T} \sum_{t=1}^{T} u_t u_{t-1} $$
> üí° **Exemplo Num√©rico:**
> Vamos considerar um cen√°rio simplificado com $T=5$ para ilustrar a aproxima√ß√£o. Suponha que temos os seguintes res√≠duos verdadeiros ($u_t$) e os regressores ($x_t$):
>
> | t | $u_t$ | $x_t$ |
> |---|-------|-------|
> | 0 | 0.5   | 1     |
> | 1 | -0.3  | 2     |
> | 2 | 0.2   | 1.5   |
> | 3 | 0.1   | 3     |
> | 4 | -0.4  | 2.5   |
> | 5 | 0.25  | 1.8   |
>
> E suponha que o estimador OLS $b$ seja pr√≥ximo do valor verdadeiro $\beta$, de tal forma que $(\beta-b)$ √© pequeno (por exemplo, 0.05). Para simplificar, considere que $x_t$ √© um escalar.  Vamos calcular $\frac{1}{T} \sum_{t=1}^{T} u_t u_{t-1}$ e $\frac{1}{T}\sum_{t=1}^{T} \hat{u}_t \hat{u}_{t-1}$ e comparar.
>
> Primeiro, calculamos $\frac{1}{T}\sum_{t=1}^{T} u_t u_{t-1}$:
>
>  $\frac{1}{5} [(-0.3)(0.5) + (0.2)(-0.3) + (0.1)(0.2) + (-0.4)(0.1) + (0.25)(-0.4)] = \frac{1}{5} [-0.15 -0.06 + 0.02 -0.04 -0.1] = -0.066$
>
>Agora, vamos calcular os res√≠duos estimados $\hat{u}_t$. Usando a f√≥rmula $\hat{u}_t = u_t + (\beta - b)x_t$:
>
>| t | $u_t$ | $x_t$ | $(\beta-b)x_t$ |  $\hat{u}_t$ |
>|---|-------|-------|----------------|-------------|
>| 0 | 0.5   | 1     | 0.05           |   0.55        |
>| 1 | -0.3  | 2     | 0.10           |   -0.20       |
>| 2 | 0.2   | 1.5   | 0.075           |   0.275       |
>| 3 | 0.1   | 3     | 0.15           |   0.25        |
>| 4 | -0.4  | 2.5   | 0.125           |   -0.275       |
>| 5 | 0.25  | 1.8   | 0.09           |   0.34        |
>
> Calculando $\frac{1}{T}\sum_{t=1}^{T} \hat{u}_t \hat{u}_{t-1}$:
>
>  $\frac{1}{5} [(-0.20)(0.55) + (0.275)(-0.20) + (0.25)(0.275) + (-0.275)(0.25) + (0.34)(-0.275)] = \frac{1}{5} [-0.11 - 0.055 + 0.06875 -0.06875 -0.0935] = -0.05165$
>
>Como podemos ver, -0.066 √© relativamente pr√≥ximo de -0.05165, o que ilustra a aproxima√ß√£o te√≥rica. Em amostras maiores, a aproxima√ß√£o se torna ainda mais precisa.
>
>Este exemplo demonstra numericamente como a aproxima√ß√£o $\frac{1}{T} \sum_{t=1}^{T} \hat{u}_t \hat{u}_{t-1} \approx \frac{1}{T} \sum_{t=1}^{T} u_t u_{t-1}$ funciona na pr√°tica.

**Prova da aproxima√ß√£o do numerador:**
Vamos provar que $\frac{1}{T} \sum_{t=1}^{T} \hat{u}_t \hat{u}_{t-1} \approx \frac{1}{T} \sum_{t=1}^{T} u_t u_{t-1}$

I.  Come√ßamos com a express√£o expandida:
    $$ \frac{1}{T}\sum_{t=1}^{T} \hat{u}_t \hat{u}_{t-1} = \frac{1}{T}\sum_{t=1}^{T} u_t u_{t-1} + \frac{1}{T}\sum_{t=1}^{T}(\beta-b)'x_t u_{t-1} + \frac{1}{T}\sum_{t=1}^{T} u_t (\beta-b)'x_{t-1} + \frac{1}{T}\sum_{t=1}^{T} (\beta-b)'x_t x_{t-1}'(\beta-b) $$

II. Usando o fato de que $b$ √© um estimador consistente para $\beta$, temos que $(\beta - b)$ converge em probabilidade para zero, ou seja, $(\beta - b) \stackrel{p}{\longrightarrow} 0$.

III.  Assumindo que os limites $\frac{1}{T}\sum_{t=1}^{T} x_t u_{t-1}$, $\frac{1}{T}\sum_{t=1}^{T} u_t x_{t-1}$ e $\frac{1}{T}\sum_{t=1}^{T} x_t x_{t-1}$ existem e s√£o finitos, ent√£o:
     $$ \frac{1}{T}\sum_{t=1}^{T}(\beta-b)'x_t u_{t-1} \stackrel{p}{\longrightarrow} 0 $$
      $$ \frac{1}{T}\sum_{t=1}^{T} u_t (\beta-b)'x_{t-1} \stackrel{p}{\longrightarrow} 0 $$
      $$ \frac{1}{T}\sum_{t=1}^{T} (\beta-b)'x_t x_{t-1}'(\beta-b) \stackrel{p}{\longrightarrow} 0 $$
     
IV. Portanto, √† medida que $T$ cresce, os termos adicionais na expans√£o convergem em probabilidade para zero, e a aproxima√ß√£o torna-se cada vez mais precisa:
$$ \frac{1}{T} \sum_{t=1}^{T} \hat{u}_t \hat{u}_{t-1} \approx \frac{1}{T} \sum_{t=1}^{T} u_t u_{t-1} $$
‚ñ†

**Lema 1**
A premissa de que as covari√¢ncias entre $u_t$ e $x_t$ sejam zero, isto √©, $Cov(u_t, x_t) = 0$ para todo $t$, √© uma condi√ß√£o fundamental para a consist√™ncia do estimador OLS. Essa premissa assegura que os regressores n√£o sejam correlacionados com os erros, uma das hip√≥teses b√°sicas do modelo linear cl√°ssico. Essa condi√ß√£o garante que o estimador OLS $b$ converge em probabilidade para $\beta$.

Da mesma forma, o denominador pode ser expresso como:
$$
\sum_{t=1}^{T} \hat{u}_{t-1}^2 = \sum_{t=1}^{T} [u_{t-1} + (\beta - b)'x_{t-1}]^2
$$
Expandindo essa express√£o, obtemos:
$$ \sum_{t=1}^{T} \hat{u}_{t-1}^2 = \sum_{t=1}^{T} u_{t-1}^2 + 2 \sum_{t=1}^{T} u_{t-1}(\beta - b)'x_{t-1} + \sum_{t=1}^{T} [(\beta - b)'x_{t-1}]^2 $$
Assumindo as mesmas premissas para o caso anterior, chegamos a:
$$ \frac{1}{T} \sum_{t=1}^{T} \hat{u}_{t-1}^2 \approx \frac{1}{T} \sum_{t=1}^{T} u_{t-1}^2$$

> üí° **Exemplo Num√©rico:**
> Utilizando os mesmos dados do exemplo anterior, vamos calcular $\frac{1}{T} \sum_{t=1}^{T} u_{t-1}^2$ e $\frac{1}{T} \sum_{t=1}^{T} \hat{u}_{t-1}^2$ para ilustrar a aproxima√ß√£o do denominador:
>
>  $\frac{1}{T} \sum_{t=1}^{T} u_{t-1}^2 = \frac{1}{5} [(0.5)^2 + (-0.3)^2 + (0.2)^2 + (0.1)^2 + (-0.4)^2 ] = \frac{1}{5} [0.25 + 0.09 + 0.04 + 0.01 + 0.16] = 0.11$
>
> Usando os valores de $\hat{u}_t$ calculados no exemplo anterior:
>
>  $\frac{1}{T} \sum_{t=1}^{T} \hat{u}_{t-1}^2 = \frac{1}{5} [(0.55)^2 + (-0.2)^2 + (0.275)^2 + (0.25)^2 + (-0.275)^2] = \frac{1}{5} [0.3025 + 0.04 + 0.075625 + 0.0625 + 0.075625] = 0.11125$
>
> Novamente, vemos que $0.11$ e $0.11125$ s√£o muito pr√≥ximos, o que ilustra a aproxima√ß√£o te√≥rica do denominador.

**Prova da aproxima√ß√£o do denominador:**
Vamos provar que $\frac{1}{T} \sum_{t=1}^{T} \hat{u}_{t-1}^2 \approx \frac{1}{T} \sum_{t=1}^{T} u_{t-1}^2$

I. Come√ßamos com a express√£o expandida:
    $$ \frac{1}{T}\sum_{t=1}^{T} \hat{u}_{t-1}^2 = \frac{1}{T}\sum_{t=1}^{T} u_{t-1}^2 + \frac{2}{T}\sum_{t=1}^{T} u_{t-1}(\beta - b)'x_{t-1} + \frac{1}{T}\sum_{t=1}^{T} [(\beta - b)'x_{t-1}]^2 $$

II. Usando o fato de que $b$ √© um estimador consistente para $\beta$, temos que $(\beta - b)$ converge em probabilidade para zero, ou seja, $(\beta - b) \stackrel{p}{\longrightarrow} 0$.

III. Assumindo que os limites $\frac{1}{T}\sum_{t=1}^{T} u_{t-1}x_{t-1}$ e $\frac{1}{T}\sum_{t=1}^{T} x_{t-1}x_{t-1}'$ existem e s√£o finitos, ent√£o:
    $$ \frac{2}{T}\sum_{t=1}^{T} u_{t-1}(\beta - b)'x_{t-1} \stackrel{p}{\longrightarrow} 0 $$
     $$ \frac{1}{T}\sum_{t=1}^{T} [(\beta - b)'x_{t-1}]^2 \stackrel{p}{\longrightarrow} 0 $$

IV. Portanto, √† medida que $T$ cresce, os termos adicionais na expans√£o convergem em probabilidade para zero, e a aproxima√ß√£o torna-se cada vez mais precisa:
$$ \frac{1}{T} \sum_{t=1}^{T} \hat{u}_{t-1}^2 \approx \frac{1}{T} \sum_{t=1}^{T} u_{t-1}^2 $$
‚ñ†

**Lema 1.1**
A converg√™ncia em probabilidade de $\frac{1}{T}\sum_{t=1}^{T} \hat{u}_{t-1}^2$ para $\frac{1}{T}\sum_{t=1}^{T} u_{t-1}^2$ √© crucial para estabelecer a consist√™ncia do estimador $\hat{\rho}$. Essa converg√™ncia depende da consist√™ncia do estimador OLS $b$ e das condi√ß√µes de momento finito sobre os regressores e os erros.

A partir dessas aproxima√ß√µes, podemos observar que:
$$ \frac{\frac{1}{T}\sum_{t=1}^{T} \hat{u}_t \hat{u}_{t-1}}{\frac{1}{T}\sum_{t=1}^{T} \hat{u}_{t-1}^2} \approx \frac{\frac{1}{T}\sum_{t=1}^{T} u_t u_{t-1}}{\frac{1}{T}\sum_{t=1}^{T} u_{t-1}^2} $$

Essa express√£o demonstra que a estimativa de $\rho$ via res√≠duos OLS converge em probabilidade para a estimativa utilizando a popula√ß√£o de erros, ou seja:
$$ \hat{\rho} \stackrel{p}{\longrightarrow} \rho $$

> üí° **Exemplo Num√©rico:**
> Continuando com o mesmo exemplo, vamos calcular $\hat{\rho}$ usando os res√≠duos estimados e os res√≠duos verdadeiros:
>
> Usando os resultados anteriores:
>
> $\hat{\rho}_{res} = \frac{\frac{1}{T}\sum_{t=1}^{T} \hat{u}_t \hat{u}_{t-1}}{\frac{1}{T}\sum_{t=1}^{T} \hat{u}_{t-1}^2} = \frac{-0.05165}{0.11125} \approx -0.464$
>
> $\hat{\rho}_{true} = \frac{\frac{1}{T}\sum_{t=1}^{T} u_t u_{t-1}}{\frac{1}{T}\sum_{t=1}^{T} u_{t-1}^2} = \frac{-0.066}{0.11} = -0.6$
>
>Como podemos observar, os valores s√£o pr√≥ximos.  Em amostras maiores a converg√™ncia ser√° ainda maior. Este exemplo num√©rico mostra que a estimativa de $\rho$ usando os res√≠duos da regress√£o OLS se aproxima da estimativa usando os erros verdadeiros.

**Prova da converg√™ncia em probabilidade de $\hat{\rho}$ para $\rho$:**

I. Sabemos que:
    $$\hat{\rho} = \frac{\sum_{t=1}^{T} \hat{u}_t \hat{u}_{t-1}}{\sum_{t=1}^{T} \hat{u}_{t-1}^2} = \frac{\frac{1}{T}\sum_{t=1}^{T} \hat{u}_t \hat{u}_{t-1}}{\frac{1}{T}\sum_{t=1}^{T} \hat{u}_{t-1}^2}$$

II. Das provas anteriores, temos as seguintes aproxima√ß√µes:
     $$\frac{1}{T}\sum_{t=1}^{T} \hat{u}_t \hat{u}_{t-1} \approx \frac{1}{T}\sum_{t=1}^{T} u_t u_{t-1}$$
     $$\frac{1}{T}\sum_{t=1}^{T} \hat{u}_{t-1}^2 \approx \frac{1}{T}\sum_{t=1}^{T} u_{t-1}^2$$

III. Portanto,
    $$ \hat{\rho} \approx \frac{\frac{1}{T}\sum_{t=1}^{T} u_t u_{t-1}}{\frac{1}{T}\sum_{t=1}^{T} u_{t-1}^2} $$
IV. Dado que $\frac{1}{T}\sum_{t=1}^{T} u_t u_{t-1}$ converge em probabilidade para $Cov(u_t, u_{t-1})$ e $\frac{1}{T}\sum_{t=1}^{T} u_{t-1}^2$ converge em probabilidade para $Var(u_t)$, podemos concluir:
    $$ \hat{\rho} \stackrel{p}{\longrightarrow} \frac{Cov(u_t, u_{t-1})}{Var(u_t)} = \rho$$
    Onde $\rho$ √© o verdadeiro par√¢metro de autocorrela√ß√£o.
‚ñ†

Al√©m disso, o texto afirma em [^8.3.19], que a converg√™ncia em probabilidade de  $\frac{1}{T} \sum_{t=1}^{T} \hat{u}_t\hat{u}_{t-1}$  para  $\rho Var(u)$ pode ser demonstrada, assumindo que $b$ √© um estimador consistente para $\beta$, e que os limites  $\frac{1}{T} \sum_{t=1}^{T} u_t u_{t-1}$, $\frac{1}{T} \sum_{t=1}^{T} u_t x_{t-1}$, $\frac{1}{T} \sum_{t=1}^{T} x_t u_{t-1}$, e $\frac{1}{T} \sum_{t=1}^{T} x_t x_{t-1}$ existem. Em [^8.3.20], o texto apresenta a distribui√ß√£o assint√≥tica da estimativa de $\rho$ quando calculada com a popula√ß√£o de erros, e em [^8.3.21] demonstra que a estimativa de $\rho$ calculada com res√≠duos da regress√£o OLS tem a mesma distribui√ß√£o assint√≥tica:
$$ \sqrt{T}(\hat{\rho} - \rho) \xrightarrow{d} N(0, 1-\rho^2) $$
> üí° **Exemplo Num√©rico:**
> Suponha que temos uma amostra grande (T=200) e que a estimativa de $\rho$ pelos res√≠duos seja $\hat{\rho} = 0.6$.  O verdadeiro valor de $\rho$ √© desconhecido. Podemos usar a distribui√ß√£o assint√≥tica para construir um intervalo de confian√ßa para o verdadeiro valor de $\rho$. A vari√¢ncia assint√≥tica de $\hat{\rho}$ √© estimada por $(1-\hat{\rho}^2)/T = (1-0.6^2)/200 = 0.0032$. O desvio padr√£o assint√≥tico √© a raiz quadrada da vari√¢ncia, ou seja, $\sqrt{0.0032} \approx 0.0566$. Usando um n√≠vel de confian√ßa de 95%,  o valor cr√≠tico de $Z$ √© de aproximadamente $1.96$. Portanto o intervalo de confian√ßa para $\rho$ √©:
> $$ 0.6 \pm 1.96 \times 0.0566$$
> $$ 0.6 \pm 0.111$$
> Portanto o intervalo de confian√ßa ser√° aproximadamente [0.489, 0.711]
> Isso significa que, sob as premissas do modelo, temos 95% de confian√ßa que o verdadeiro valor de $\rho$ est√° dentro desse intervalo.
> Al√©m disso, podemos realizar testes de hip√≥teses sobre $\rho$. Por exemplo, testar $H_0 : \rho = 0$ contra $H_1 : \rho \neq 0$. A estat√≠stica de teste seria:
> $$ Z = \frac{\hat{\rho} - 0}{\sqrt{(1-\hat{\rho}^2)/T}} = \frac{0.6}{\sqrt{0.0032}} \approx 10.6 $$
>
> Como o valor de Z √© muito maior que 1.96, rejeitamos a hip√≥tese nula com um n√≠vel de signific√¢ncia de 5%. Isso nos da evid√™ncia que h√° autocorrela√ß√£o nos res√≠duos.

**Teorema 1**
A distribui√ß√£o assint√≥tica de $\hat{\rho}$  nos mostra que para amostras grandes, $\hat{\rho}$  √© aproximadamente normalmente distribu√≠do com m√©dia $\rho$ e vari√¢ncia $(1-\rho^2)/T$. Esta distribui√ß√£o √© fundamental para a infer√™ncia estat√≠stica, permitindo construir intervalos de confian√ßa e testar hip√≥teses sobre o par√¢metro de autocorrela√ß√£o $\rho$.

**Corol√°rio 1**
A converg√™ncia em distribui√ß√£o de $\sqrt{T}(\hat{\rho} - \rho)$ para $N(0, 1-\rho^2)$ implica que, para amostras grandes, a diferen√ßa entre a estimativa $\hat{\rho}$ e o verdadeiro valor $\rho$, multiplicada por $\sqrt{T}$, √© aproximadamente normalmente distribu√≠da. Este resultado √© crucial para testes de hip√≥teses e constru√ß√£o de intervalos de confian√ßa para $\rho$.
### Conclus√£o
Em resumo, a estimativa do par√¢metro de autocorrela√ß√£o $\rho$ utilizando diretamente os res√≠duos da regress√£o OLS √© consistente e assintoticamente equivalente √† estimativa obtida pelo m√©todo iterativo de Cochrane-Orcutt. Embora este m√©todo iterativo possa convergir para um m√°ximo local da fun√ß√£o de verossimilhan√ßa, a estimativa direta baseada nos res√≠duos do OLS √© computacionalmente mais simples e ainda fornece resultados assintoticamente v√°lidos. Portanto, a abordagem de estimar a autocorrela√ß√£o diretamente a partir dos res√≠duos OLS oferece uma alternativa eficiente e pr√°tica para lidar com modelos de regress√£o com erros autocorrelacionados, simplificando a implementa√ß√£o em cen√°rios onde estimativas r√°pidas e precisas s√£o desejadas.
**Proposi√ß√£o 1**
A simplicidade computacional da estimativa direta de $\rho$ baseada nos res√≠duos OLS, em compara√ß√£o com o m√©todo iterativo de Cochrane-Orcutt, torna-a prefer√≠vel em cen√°rios que exigem rapidez e efici√™ncia no c√°lculo. No entanto, a escolha do m√©todo mais adequado deve considerar o compromisso entre simplicidade e a necessidade de resultados com melhor precis√£o em casos espec√≠ficos.
### Refer√™ncias
[^8.3.15]:  Discuss√£o sobre o m√©todo iterativo de Cochrane-Orcutt
[^8.3.16]: Apresenta√ß√£o da f√≥rmula para estimativa da autocorrela√ß√£o dos res√≠duos.
[^8.3.19]: Demonstra√ß√£o da converg√™ncia em probabilidade do numerador da autocorrela√ß√£o.
[^8.3.20]: Distribui√ß√£o assint√≥tica da estimativa do par√¢metro de autocorrela√ß√£o usando a popula√ß√£o de erros.
[^8.3.21]:  Distribui√ß√£o assint√≥tica da estimativa do par√¢metro de autocorrela√ß√£o usando os res√≠duos OLS.
<!-- END -->
