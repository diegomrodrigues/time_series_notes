## Fatora√ß√£o Triangular e Proje√ß√µes Lineares

### Introdu√ß√£o
Neste cap√≠tulo, aprofundaremos o conceito de **fatora√ß√£o triangular** de uma matriz de segundos momentos e como ela se relaciona com as **proje√ß√µes lineares**. Como vimos anteriormente, a proje√ß√£o linear √© uma ferramenta fundamental para a constru√ß√£o de previs√µes √≥timas. [^4.1.2], [^4.1.9]. Expandindo esses conceitos, exploraremos como a fatora√ß√£o triangular nos permite transformar vari√°veis originais em um conjunto de vari√°veis n√£o correlacionadas, facilitando a an√°lise e a constru√ß√£o de modelos preditivos mais eficientes. Essa t√©cnica, que √© crucial na an√°lise de s√©ries temporais, tem uma liga√ß√£o direta com a regress√£o de m√≠nimos quadrados ordin√°rios, conforme discutido no ap√™ndice 4.A [^4.A].
Al√©m disso, vamos introduzir uma extens√£o do conceito de fatora√ß√£o triangular, mostrando que uma matriz sim√©trica definida positiva pode ser fatorada usando uma matriz triangular superior em vez de inferior, o que oferece flexibilidade adicional dependendo do contexto da aplica√ß√£o.

### Conceitos Fundamentais

A **fatora√ß√£o triangular**, tamb√©m conhecida como decomposi√ß√£o de Cholesky, de uma matriz sim√©trica definida positiva $\Omega$, permite express√°-la como o produto de uma matriz triangular inferior $A$, uma matriz diagonal $D$ e a transposta de $A$. Essa representa√ß√£o √© dada por [^4.4.1]:
$$ \Omega = ADA' $$
onde $A$ √© uma matriz triangular inferior com 1s na diagonal principal, e $D$ √© uma matriz diagonal com entradas positivas. [^4.4.1]. Esta decomposi√ß√£o √© √∫nica para uma matriz $\Omega$ dada, o que a torna uma ferramenta poderosa para an√°lise. [^4.4.14].

> üí° **Exemplo Num√©rico:** Considere a matriz de segundos momentos $\Omega$:
> $$ \Omega = \begin{bmatrix} 4 & 2 & 1 \\ 2 & 5 & 3 \\ 1 & 3 & 6 \end{bmatrix} $$
> Podemos fator√°-la em $ADA'$ da seguinte forma:
> $$ A = \begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.25 & 0.714 & 1 \end{bmatrix} $$
> $$ D = \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 4.285 \end{bmatrix} $$
> Onde $A$ √© triangular inferior com 1s na diagonal e $D$ √© diagonal. Calculando $ADA'$, obtemos aproximadamente $\Omega$.
> Este exemplo ilustra como uma matriz de covari√¢ncia pode ser decomposta em matrizes mais simples, facilitando o c√°lculo e a an√°lise.

**Teorema 1** (Fatora√ß√£o Triangular com Matriz Superior) Uma matriz sim√©trica definida positiva $\Omega$ tamb√©m pode ser expressa como:
$$ \Omega = UDU' $$
onde $U$ √© uma matriz triangular superior com 1s na diagonal principal, e $D$ √© a mesma matriz diagonal com entradas positivas da decomposi√ß√£o com matriz triangular inferior.

*Prova*: Dada a decomposi√ß√£o $\Omega = ADA'$ com $A$ triangular inferior, podemos observar que $\Omega = \Omega'$. Assim, $\Omega = (ADA')' = AD'A'$. Dado que $D$ √© diagonal ($D=D'$), e que $\Omega$ √© definida positiva, $A'$ tem inversa. Definindo $U = (A')^{-1}$ e $D^* = (A')DA'$, temos $\Omega = (A')^{-1}D^*(A^{-1}) = U(A')DA'(A^{-1}) = U(A')DA'(U^{-1})'$. Note que $U$ √© triangular superior, pois $(A')^{-1}$ o √©. Como $A'$ √© triangular superior, a inversa $(A')^{-1}$ √© tamb√©m triangular superior. Definindo $D = D^*$ temos a fatora√ß√£o com matriz triangular superior.
A singularidade desta decomposi√ß√£o tamb√©m pode ser provada de forma similar a [^4.4.14].
 
I. Dado que $\Omega$ √© sim√©trica, temos $\Omega = \Omega'$.
II.  A decomposi√ß√£o triangular inferior nos fornece $\Omega = ADA'$.
III. Tomando a transposta de $\Omega$, temos $\Omega' = (ADA')' = A'D'A$.
IV. Como D √© diagonal, $D' = D$, ent√£o $\Omega' = A'DA$.
V.  Igualando as duas express√µes para $\Omega$, temos $ADA' = A'DA$.
VI.  Definimos $U = (A')^{-1}$, que √© uma matriz triangular superior com 1s na diagonal principal, pois a inversa da transposta de uma matriz triangular inferior com 1s na diagonal principal √© uma matriz triangular superior com 1s na diagonal principal.
VII. Definimos tamb√©m $D^* = (A')DA'$.
VIII.  Substituindo $U$ em $\Omega$, temos $\Omega = (A')^{-1}D^*(A^{-1}) = U(A')DA'(A^{-1})$.
IX.  Como $(U^{-1})' = A'$, temos $A'(A^{-1}) = (U^{-1})' (A^{-1}) = (A U^{-1})' = (A (A')^{-1})'$.
X.  Observando que $A (A')^{-1} = I$, temos  $A (A')^{-1} = I$, ent√£o $(A (A')^{-1})' = I'= I$.
XI. Consequentemente temos $\Omega = U(A')DA'(U^{-1})' = U(A')DA'(A'^{-1})' = U(A')DA'(A^{-1}) = UDU'$.
XII. Portanto, $\Omega = UDU'$, onde $U$ √© uma matriz triangular superior com 1s na diagonal principal e $D$ √© a mesma matriz diagonal com entradas positivas da decomposi√ß√£o com matriz triangular inferior.
‚ñ†

A fatora√ß√£o triangular √© especialmente √∫til quando se trabalha com um vetor de vari√°veis aleat√≥rias $Y$. Seja $Y$ um vetor de vari√°veis aleat√≥rias (n x 1), com matriz de segundos momentos dada por:
$$ \Omega = E(YY') $$ [^4.5.1].
Podemos transformar as vari√°veis originais por meio de uma matriz $A$ e obter um novo conjunto de vari√°veis n√£o correlacionadas:
$$ \tilde{Y} = A^{-1}Y $$ [^4.5.2]
A matriz de segundos momentos das vari√°veis transformadas √©:
$$ E(\tilde{Y}\tilde{Y}') = E(A^{-1}YY'(A^{-1})') = A^{-1}E(YY')(A^{-1})' = A^{-1}\Omega(A^{-1})' = A^{-1}ADA'(A^{-1})' = D $$ [^4.5.3], [^4.5.4]
Este resultado mostra que as novas vari√°veis em $\tilde{Y}$ s√£o n√£o correlacionadas, pois a matriz de segundos momentos resultante √© diagonal. [^4.5.5].

> üí° **Exemplo Num√©rico:**  Suponha que temos um vetor de vari√°veis aleat√≥rias $Y = \begin{bmatrix} Y_1 \\ Y_2 \\ Y_3 \end{bmatrix}$ com a matriz de segundos momentos $\Omega$ como no exemplo anterior. A matriz $A$ e $D$ s√£o tamb√©m as do exemplo anterior.
>  Calculando a inversa de $A$:
> $$ A^{-1} = \begin{bmatrix} 1 & 0 & 0 \\ -0.5 & 1 & 0 \\ 0.107 & -0.714 & 1 \end{bmatrix} $$
> A transforma√ß√£o $\tilde{Y} = A^{-1}Y$ resulta em um novo vetor de vari√°veis $\tilde{Y}$ n√£o correlacionadas. A matriz de segundos momentos de $\tilde{Y}$ √© $D$, como demonstrado na teoria.

**Lema 1** (Propriedade da Inversa de Matrizes Triangulares) A inversa de uma matriz triangular inferior (superior) com 1s na diagonal principal √© tamb√©m uma matriz triangular inferior (superior) com 1s na diagonal principal.

*Prova*: A prova pode ser feita por indu√ß√£o no tamanho da matriz.
A inversa de uma matriz triangular inferior (superior) com 1s na diagonal principal √© tamb√©m triangular inferior (superior) com 1s na diagonal principal.
A prova pode ser feita por indu√ß√£o.

I. Caso base: Para uma matriz 1x1, o inverso de [1] √© [1], que √© uma matriz triangular inferior e superior com 1 na diagonal.
II. Hip√≥tese indutiva: Assumimos que o inverso de uma matriz triangular inferior (superior) de tamanho k x k com 1s na diagonal principal √© tamb√©m uma matriz triangular inferior (superior) com 1s na diagonal principal.
III. Passo indutivo: Considere uma matriz triangular inferior (superior) de tamanho (k+1) x (k+1) com 1s na diagonal principal, denotada por $T_{k+1}$. Podemos particionar esta matriz da seguinte forma:
    $$T_{k+1} = \begin{bmatrix} T_k & 0 \\ v' & 1 \end{bmatrix}$$
    para matrizes triangulares inferiores e
    $$T_{k+1} = \begin{bmatrix} 1 & v' \\ 0 & T_k \end{bmatrix}$$
    para matrizes triangulares superiores, onde $T_k$ √© uma matriz triangular inferior (superior) k x k com 1s na diagonal, 0 representa um vetor de zeros e $v$ √© um vetor coluna.
IV.  O inverso de $T_{k+1}$ pode ser calculado por blocos:
    $$T_{k+1}^{-1} = \begin{bmatrix} T_k^{-1} & 0 \\ -v'T_k^{-1} & 1 \end{bmatrix}$$
    para matrizes triangulares inferiores e
    $$T_{k+1}^{-1} = \begin{bmatrix} 1 & -v'T_k^{-1} \\ 0 & T_k^{-1} \end{bmatrix}$$
    para matrizes triangulares superiores.
V.  Pela hip√≥tese indutiva, $T_k^{-1}$ √© uma matriz triangular inferior (superior) com 1s na diagonal principal.
VI.  Portanto, $T_{k+1}^{-1}$ √© uma matriz triangular inferior (superior) com 1s na diagonal principal.
VII. Conclus√£o: Por indu√ß√£o, o inverso de uma matriz triangular inferior (superior) com 1s na diagonal principal √© tamb√©m uma matriz triangular inferior (superior) com 1s na diagonal principal. ‚ñ†

Para entender melhor a rela√ß√£o com as proje√ß√µes lineares, podemos analisar o processo de transforma√ß√£o:
$$  A\tilde{Y} = Y $$ [^4.5.6]
Essa express√£o nos diz como transformar as vari√°veis n√£o correlacionadas $\tilde{Y}$ para obter as vari√°veis originais $Y$. Em termos de proje√ß√£o linear, o primeiro elemento de $\tilde{Y}$ √© igual ao primeiro elemento de $Y$, o segundo elemento de $\tilde{Y}$ √© o res√≠duo de $Y_2$ ap√≥s projetar em $Y_1$, e assim por diante. Essa decomposi√ß√£o em res√≠duos permite a an√°lise de como as vari√°veis se relacionam entre si em termos de proje√ß√µes lineares.
Por exemplo, usando a express√£o [^4.4.11], podemos explicitar [^4.5.6] como:

$$
\begin{bmatrix}
    1 & 0 & 0 & \cdots & 0 \\
    \Omega_{21}\Omega_{11}^{-1} & 1 & 0 & \cdots & 0 \\
    \Omega_{31}\Omega_{11}^{-1} &  h_{32}h_{22}^{-1} & 1 & \cdots & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    \Omega_{n1}\Omega_{11}^{-1} & \cdots & \cdots & \cdots & 1
    \end{bmatrix}
    \begin{bmatrix}
        \tilde{Y_1} \\
        \tilde{Y_2} \\
        \tilde{Y_3} \\
        \vdots \\
        \tilde{Y_n}
    \end{bmatrix} =
    \begin{bmatrix}
        Y_1 \\
        Y_2 \\
        Y_3 \\
        \vdots \\
        Y_n
    \end{bmatrix}
$$
$$
\begin{cases}
    \tilde{Y_1} = Y_1 \\
    \Omega_{21}\Omega_{11}^{-1}\tilde{Y_1} + \tilde{Y_2} = Y_2 \implies \tilde{Y_2} = Y_2 - \Omega_{21}\Omega_{11}^{-1}Y_1 = Y_2 - \alpha Y_1 \\
    \Omega_{31}\Omega_{11}^{-1}\tilde{Y_1} +  h_{32}h_{22}^{-1}\tilde{Y_2} + \tilde{Y_3} = Y_3 \\
    \vdots
\end{cases}
$$
Onde $\alpha$ √© o coeficiente da proje√ß√£o linear de $Y_2$ em $Y_1$.
Notavelmente, a diagonal de $D$ representa o erro quadr√°tico m√©dio (MSE) associado com cada componente de $\tilde{Y}$, conforme equa√ß√£o [4.5.5]. Em particular, [^4.5.9] demonstra que o segundo elemento de  $\tilde{Y}$ corresponde ao res√≠duo resultante da proje√ß√£o de $Y_2$ sobre $Y_1$ e [^4.5.10] mostra que este res√≠duo √© ortogonal a $Y_1$ e que $E(\tilde{Y_2}Y_1) = 0$. O MSE desta proje√ß√£o linear √© dado por $d_{22} = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}$. [^4.5.12], [^4.5.13].

> üí° **Exemplo Num√©rico:** Usando a matriz $\Omega$ do exemplo anterior, vamos calcular alguns dos elementos da transforma√ß√£o.
>  - $\tilde{Y_1} = Y_1$.
>  - $\alpha = \Omega_{21}\Omega_{11}^{-1} = 2 / 4 = 0.5$.
>  - $\tilde{Y_2} = Y_2 - 0.5Y_1$.  Este √© o res√≠duo da proje√ß√£o de $Y_2$ sobre $Y_1$.
>  - $d_{22} = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12} = 5 - (2 * (1/4) * 2) = 5 - 1 = 4$ (o segundo elemento diagonal de $D$, como esperado).
> Este exemplo mostra concretamente como os elementos da matriz $A$ e $D$ se relacionam com as proje√ß√µes lineares e seus erros quadr√°ticos m√©dios.

Essa estrutura se estende a proje√ß√µes lineares que envolvem mais vari√°veis [^4.5.11]. Por exemplo, a equa√ß√£o [^4.5.12] demonstra como obter a proje√ß√£o linear de $Y_3$ dado $Y_1$ e $Y_2$, combinando a proje√ß√£o de $Y_3$ dado $Y_1$ com a parte n√£o prevista de $Y_2$ dado $Y_1$.

**Proposi√ß√£o 1** (Caracteriza√ß√£o do MSE via Decomposi√ß√£o Triangular) Os elementos diagonais da matriz $D$ na fatora√ß√£o triangular de $\Omega$, correspondem ao erro quadr√°tico m√©dio das proje√ß√µes lineares de cada vari√°vel $Y_i$ sobre as vari√°veis $Y_1, ..., Y_{i-1}$. Ou seja, $d_{ii}$ √© o erro quadr√°tico m√©dio da proje√ß√£o linear de $Y_i$ sobre as vari√°veis $Y_1, ..., Y_{i-1}$.

*Prova:* Como visto na discuss√£o anterior, $\tilde{Y_i}$ √© o res√≠duo da proje√ß√£o linear de $Y_i$ sobre $Y_1, ..., Y_{i-1}$. Portanto, $d_{ii} = E(\tilde{Y_i}^2)$ corresponde ao erro quadr√°tico m√©dio dessa proje√ß√£o.

I. A fatora√ß√£o triangular de $\Omega$ √© dada por $\Omega = ADA'$, onde $A$ √© uma matriz triangular inferior com 1s na diagonal, e $D$ √© uma matriz diagonal.
II.  As vari√°veis transformadas s√£o definidas como $\tilde{Y} = A^{-1}Y$.
III. A matriz de covari√¢ncia de $\tilde{Y}$ √© $E(\tilde{Y}\tilde{Y}') = D$, que √© uma matriz diagonal. Isso significa que os elementos de $\tilde{Y}$ s√£o n√£o correlacionados.
IV.  Da rela√ß√£o $A\tilde{Y} = Y$, podemos expressar cada $Y_i$ em fun√ß√£o de $\tilde{Y}_1, \ldots, \tilde{Y}_i$.
V.  Em particular, o elemento $\tilde{Y_i}$ representa o res√≠duo da proje√ß√£o linear de $Y_i$ sobre $Y_1, \ldots, Y_{i-1}$.
VI.  Como $D$ √© a matriz de covari√¢ncia de $\tilde{Y}$, o elemento diagonal $d_{ii}$ representa a vari√¢ncia de $\tilde{Y_i}$, ou seja, $d_{ii} = E(\tilde{Y_i}^2)$.
VII.  Portanto, $d_{ii}$ corresponde ao erro quadr√°tico m√©dio da proje√ß√£o linear de $Y_i$ sobre as vari√°veis $Y_1, ..., Y_{i-1}$.
‚ñ†

> üí° **Exemplo Num√©rico:** No exemplo anterior, $d_{11} = 4$, $d_{22} = 4$ e $d_{33} = 4.285$.
> - $d_{11}$ √© o MSE da proje√ß√£o de $Y_1$ sobre um espa√ßo vazio (ou seja, $Y_1$ em si mesmo), que √© simplesmente a sua vari√¢ncia.
> - $d_{22}$ √© o MSE da proje√ß√£o de $Y_2$ sobre $Y_1$, que j√° calculamos anteriormente.
> - $d_{33}$ √© o MSE da proje√ß√£o de $Y_3$ sobre $Y_1$ e $Y_2$.
>   Este exemplo ilustra como os elementos diagonais de D representam o MSE das proje√ß√µes lineares sequenciais.

### Conclus√£o
A **fatora√ß√£o triangular** oferece uma forma sistem√°tica de transformar um conjunto de vari√°veis correlacionadas em um conjunto de vari√°veis n√£o correlacionadas, onde as matrizes de transforma√ß√£o s√£o compostas pelos coeficientes das proje√ß√µes lineares. Essa t√©cnica se relaciona com a proje√ß√£o linear, onde os res√≠duos resultantes da proje√ß√£o s√£o ortogonais aos regressores.
Em termos pr√°ticos, essa decomposi√ß√£o permite uma atualiza√ß√£o eficiente das proje√ß√µes lineares, onde podemos calcular novos coeficientes ao adicionarmos novas informa√ß√µes, como foi demonstrado nas equa√ß√µes [^4.5.14] e [^4.5.16]. Al√©m disso, o conhecimento de como a fatora√ß√£o triangular transforma as vari√°veis originais pode ser usado para construir previs√µes mais precisas e otimizadas, especialmente em modelos ARMA e de regress√£o. Ao final, a fatora√ß√£o triangular permite construir modelos onde as vari√°veis originais sejam expressas em termos de vari√°veis n√£o correlacionadas e seus coeficientes de proje√ß√£o, o que torna a an√°lise de sistemas complexos mais trat√°vel.
Em resumo, a fatora√ß√£o triangular √© uma ferramenta fundamental para a an√°lise de s√©ries temporais, permitindo transformar dados de forma a obter novas vari√°veis n√£o correlacionadas, cujas rela√ß√µes com os dados originais sejam bem compreendidas. O uso da decomposi√ß√£o de Cholesky simplifica o c√°lculo de erros quadr√°ticos m√©dios de proje√ß√£o e permite uma melhor compreens√£o das rela√ß√µes entre as vari√°veis.
### Refer√™ncias
[^4.1.2]: Expression [4.1.1] is known as the mean squared error associated with the forecast ... The forecast with the smallest mean squared error turns out to be the ex- pectation of $Y_{t+1}$ conditional on $X_t$: $Y_{t+1}$ = $E(Y_{t+1}|X_t)$.
[^4.1.9]: We now restrict the class of forecasts considered by requiring the forecast $Y_{t+1}$ to be a linear function of $X_t$: $Y^*_{t+1}$ = $\alpha'X_t$.
[^4.4.1]: Any positive definite symmetric (n √ó n) matrix $\Omega$ has a unique representation of the form $\Omega = ADA'$, where $A$ is a lower triangular matrix with 1s along the principal diagonal...
[^4.4.14]: We next establish that the triangular factorization is unique. Suppose that $\Omega = A_1D_1A'_1 = A_2D_2A'_2$, where $A_1$ and $A_2$ are both lower triangular with 1s along the principal diagonal and $D_1$ and $D_2$ are both diagonal with positive entries along the principal diagonal.
[^4.5.1]: Let $Y = (Y_1, Y_2, \ldots, Y_n)'$ be an (n √ó 1) vector of random variables whose second-moment matrix is given by $\Omega = E(YY')$.
[^4.5.2]: Let $\Omega = ADA'$ be the triangular factorization of $\Omega$, and define $\tilde{Y}$ = $A^{-1}Y$.
[^4.5.3]: The second-moment matrix of these transformed variables is given by $E(\tilde{Y}\tilde{Y}')$ = $E(A^{-1}YY'[A']^{-1})$ = $A^{-1}E(YY')[A']^{-1}$.
[^4.5.4]: Substituting [4.5.1] into [4.5.3], the second-moment matrix of $\tilde{Y}$ is seen to be diagonal: $E(\tilde{Y}\tilde{Y}')$ = $A^{-1}\Omega[A']^{-1} = A^{-1}ADA'[A']^{-1} = D$.
[^4.5.5]: That is, $E(\tilde{Y_i}\tilde{Y_j}) = \begin{cases} d_{ii} & \text{for } i=j \\ 0 & \text{for } i \neq j \end{cases}$.
[^4.5.6]: Thus the $\tilde{Y}$'s form a series of random variables that are uncorrelated with one another. To see the implication of this, premultiply [4.5.2] by $A$: $A\tilde{Y} = Y$.
[^4.5.9]: or, using [4.5.8], $\tilde{Y_2}$ = $Y_2$ - $\Omega_{21}\Omega_{11}^{-1}Y_1$ = $Y_2$ - $\alpha Y_1$, where we have defined $\alpha$ = $\Omega_{21}\Omega_{11}^{-1}$. The fact that $\tilde{Y_2}$ is uncorrelated with $Y_1$ implies $E(\tilde{Y_2}Y_1)$ = $E[(Y_2 - \alpha Y_1)Y_1] = 0$.
[^4.5.10]: But, recalling [4.1.10], the value of $\alpha$ that satisfies [4.5.10] is defined as the coefficient of the linear projection of $Y_2$ on $Y_1$.
[^4.5.11]: Substituting in from [4.5.8] and [4.5.9] and rearranging, $Y_3$ = $Y_3$ - $\Omega_{31}\Omega_{11}^{-1}Y_1$ - $h_{32}h_{22}^{-1}$ ($Y_2$ - $\Omega_{21}\Omega_{11}^{-1}Y_1$).
[^4.5.12]:  Thus the residual is uncorrelated with either $Y_1$ or $\tilde{Y_2}$, meaning that $\tilde{Y_3}$ has the interpretation as the residual from a linear projection of $Y_3$ on $Y_1$ and $Y_2$. According to [4.5.11], the linear projection is given by $P(Y_3|Y_2,Y_1) = \Omega_{31}\Omega_{11}^{-1}Y_1 + h_{32}h_{22}^{-1}(Y_2 - \Omega_{21}\Omega_{11}^{-1}Y_1)$.
[^4.5.13]: The MSE of the linear projection is the variance of $\tilde{Y_3}$, which from [4.5.5] is given by $d_{33}$: $E[\tilde{Y_3} - P(\tilde{Y_3}|Y_2,Y_1)]^2 = h_{33} - h_{32}h_{22}^{-1}h_{23}$.
[^4.5.14]: $P(Y_3|Y_1) = \Omega_{31}\Omega_{11}^{-1}Y_1$.
       Equation [4.5.12] states that $P(Y_3|Y_2,Y_1) = P(Y_3|Y_1) + h_{32}h_{22}^{-1}[Y_2 - P(Y_2|Y_1)]$.
[^4.5.16]: $P(Y_3|Y_2,Y_1) = P(Y_3|Y_1) + \{E[Y_3 - P(Y_3|Y_1)][Y_2 - P(Y_2|Y_1)]\} \{E[Y_2 - P(Y_2|Y_1)]^2\}^{-1}[Y_2 - P(Y_2|Y_1)]$.
[^4.A]: Although linear projection describes population moments and ordinary least squares describes sample moments, there is a formal mathematical sense in which the two operations are the same. Appendix 4.A to this chapter discusses this parallel and shows how the formulas for an OLS regression can be viewed as a special case of the formulas for a linear projection.
<!-- END -->
