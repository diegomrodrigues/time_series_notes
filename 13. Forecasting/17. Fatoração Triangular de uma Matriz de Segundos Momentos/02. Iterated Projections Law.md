## A Lei das Proje√ß√µes Iteradas e sua Rela√ß√£o com a Fatora√ß√£o Triangular

### Introdu√ß√£o
Neste cap√≠tulo, vamos aprofundar nosso entendimento sobre a **lei das proje√ß√µes iteradas** e sua rela√ß√£o intr√≠nseca com a **fatora√ß√£o triangular**. Como vimos anteriormente, a fatora√ß√£o triangular nos permite transformar um conjunto de vari√°veis correlacionadas em um conjunto de vari√°veis n√£o correlacionadas, por meio de matrizes triangulares. [^4.5.4]. Agora, exploraremos como a lei das proje√ß√µes iteradas se encaixa nesse cen√°rio, particularmente quando lidamos com a atualiza√ß√£o de previs√µes em modelos de s√©ries temporais. Essa lei √© fundamental para entender como a informa√ß√£o se propaga atrav√©s das proje√ß√µes sucessivas e como podemos simplificar c√°lculos complexos em modelos preditivos.

### Conceitos Fundamentais

A **lei das proje√ß√µes iteradas** estabelece que a proje√ß√£o de uma proje√ß√£o em um espa√ßo de informa√ß√£o menor corresponde √† proje√ß√£o direta nesse espa√ßo. Em termos mais formais, se tivermos tr√™s conjuntos de vari√°veis, $Y_1$, $Y_2$, e $Y_3$, a proje√ß√£o de $Y_3$ em $Y_1$ e $Y_2$, e, ent√£o, a proje√ß√£o do resultado dessa proje√ß√£o sobre $Y_1$, √© equivalente a proje√ß√£o de $Y_3$ diretamente sobre $Y_1$. [^4.5.32]. Matematicamente, isso √© expresso como:
$$ P[P(Y_3|Y_2, Y_1)|Y_1] = P(Y_3|Y_1) $$
onde $P(Y_3|Y_2, Y_1)$ √© a proje√ß√£o linear de $Y_3$ sobre $Y_1$ e $Y_2$, e $P(Y_3|Y_1)$ √© a proje√ß√£o linear de $Y_3$ sobre $Y_1$.

> üí° **Exemplo Ilustrativo:** Imagine que voc√™ est√° tentando prever o pre√ßo de um ativo ($Y_3$) com base em duas vari√°veis: o pre√ßo de um ativo relacionado ($Y_2$) e uma vari√°vel macroecon√¥mica ($Y_1$). A lei das proje√ß√µes iteradas nos diz que, em vez de calcular a proje√ß√£o de $Y_3$ em ambos $Y_1$ e $Y_2$ e depois projetar esse resultado em $Y_1$ novamente, podemos obter o mesmo resultado projetando $Y_3$ diretamente em $Y_1$.

A import√¢ncia dessa lei reside em sua capacidade de simplificar c√°lculos complexos, especialmente em modelos de s√©ries temporais onde m√∫ltiplas vari√°veis e defasagens podem estar envolvidas. Ela garante que proje√ß√µes lineares sequenciais s√£o consistentes com proje√ß√µes diretas no espa√ßo de informa√ß√£o desejado.

A **fatora√ß√£o triangular** desempenha um papel fundamental na compreens√£o da lei das proje√ß√µes iteradas. Como vimos, a fatora√ß√£o triangular nos permite decompor a matriz de segundos momentos $\Omega$ em um produto de matrizes mais simples: $\Omega = ADA'$. [^4.4.1]. Al√©m disso, a transforma√ß√£o $\tilde{Y} = A^{-1}Y$ gera um vetor de vari√°veis $\tilde{Y}$ n√£o correlacionadas.
A rela√ß√£o entre a fatora√ß√£o triangular e a lei das proje√ß√µes iteradas se manifesta na forma como as proje√ß√µes lineares s√£o constru√≠das e atualizadas, como na equa√ß√£o [^4.5.30], que ilustra a proje√ß√£o de $Y_3$ condicional a $Y_2$ e $Y_1$. Essa equa√ß√£o √© uma forma da lei das proje√ß√µes iteradas:
$$P(Y_3|Y_2,Y_1) = P(Y_3|Y_1) + H_{32}H_{22}^{-1}[Y_2 - P(Y_2|Y_1)], $$
onde $H_{32}$ √© a covari√¢ncia entre o erro da proje√ß√£o de $Y_3$ em $Y_1$ e o erro da proje√ß√£o de $Y_2$ em $Y_1$, e $H_{22}$ √© a vari√¢ncia do erro da proje√ß√£o de $Y_2$ em $Y_1$.

**Teorema 2** (Lei das Proje√ß√µes Iteradas via Decomposi√ß√£o Triangular) Dado um vetor de vari√°veis aleat√≥rias Y, cuja matriz de covari√¢ncia √© expressa como $\Omega = ADA'$, onde A √© uma matriz triangular inferior, a lei das proje√ß√µes iteradas √© implicitamente utilizada no processo de atualiza√ß√£o das proje√ß√µes lineares, com o uso da fatora√ß√£o triangular.
*Prova:*
I. A proje√ß√£o de $Y_3$ dado $Y_1$ √© expressa por $P(Y_3|Y_1) = \Omega_{31}\Omega_{11}^{-1}Y_1$. [^4.5.30].
II.  O res√≠duo da proje√ß√£o de $Y_2$ sobre $Y_1$ √© dado por $\tilde{Y_2} = Y_2 - P(Y_2|Y_1) = Y_2 - \Omega_{21}\Omega_{11}^{-1}Y_1$. [^4.5.9].
III. A proje√ß√£o de $Y_3$ dado $Y_1$ e $Y_2$ pode ser expressa como $P(Y_3|Y_2, Y_1) = P(Y_3|Y_1) + H_{32}H_{22}^{-1}[Y_2 - P(Y_2|Y_1)]$. [^4.5.30].
IV.  A lei das proje√ß√µes iteradas afirma que $P[P(Y_3|Y_2, Y_1)|Y_1] = P(Y_3|Y_1)$.
V.  Aplicando a proje√ß√£o em $P(Y_3|Y_2, Y_1)$ sobre $Y_1$:
$P[P(Y_3|Y_2, Y_1)|Y_1] = P[P(Y_3|Y_1) + H_{32}H_{22}^{-1}[Y_2 - P(Y_2|Y_1)]|Y_1] = P(Y_3|Y_1) + H_{32}H_{22}^{-1}P[Y_2 - P(Y_2|Y_1)|Y_1]$.
VI.  O termo $Y_2 - P(Y_2|Y_1)$ √© o res√≠duo da proje√ß√£o de $Y_2$ sobre $Y_1$, que √© ortogonal a $Y_1$, e, portanto, a proje√ß√£o em $Y_1$ √© nula: $P[Y_2 - P(Y_2|Y_1)|Y_1] = 0$.
VII.  Assim, $P[P(Y_3|Y_2, Y_1)|Y_1] = P(Y_3|Y_1)$, que confirma a lei das proje√ß√µes iteradas.
VIII. O processo de fatora√ß√£o triangular garante que os res√≠duos s√£o ortogonais entre si. Isso √© utilizado na etapa V para simplificar a proje√ß√£o iterada.
‚ñ†

> üí° **Exemplo Num√©rico:** Para ilustrar a fatora√ß√£o triangular e a ortogonalidade dos res√≠duos, suponha que temos tr√™s vari√°veis $Y_1$, $Y_2$ e $Y_3$ com a seguinte matriz de covari√¢ncia:
>
> $$\Omega = \begin{bmatrix} 4 & 2 & 1 \\ 2 & 5 & 3 \\ 1 & 3 & 6 \end{bmatrix}$$
>
> O objetivo da fatora√ß√£o triangular $\Omega = ADA'$ √© encontrar uma matriz triangular inferior $A$ e uma matriz diagonal $D$ tal que essa igualdade seja satisfeita. O processo envolve projetar cada vari√°vel nas anteriores e obter os res√≠duos.
>
> **Passo 1: Projetar $Y_2$ em $Y_1$**
>
> A proje√ß√£o de $Y_2$ em $Y_1$ √©:
>  $P(Y_2|Y_1) = \frac{\Omega_{21}}{\Omega_{11}}Y_1 = \frac{2}{4}Y_1 = 0.5Y_1$.
>
> O res√≠duo $\tilde{Y_2}$ √© ent√£o:
> $\tilde{Y_2} = Y_2 - P(Y_2|Y_1) = Y_2 - 0.5Y_1$.
>
> **Passo 2: Projetar $Y_3$ em $Y_1$**
>
> A proje√ß√£o de $Y_3$ em $Y_1$ √©:
> $P(Y_3|Y_1) = \frac{\Omega_{31}}{\Omega_{11}}Y_1 = \frac{1}{4}Y_1 = 0.25Y_1$.
>
> **Passo 3: Projetar $Y_3$ no res√≠duo $\tilde{Y_2}$**
>
> Primeiro, precisamos da covari√¢ncia entre $Y_3$ e $\tilde{Y_2}$:
> $Cov(Y_3, \tilde{Y_2}) = Cov(Y_3, Y_2 - 0.5Y_1) = \Omega_{32} - 0.5\Omega_{31} = 3 - 0.5 = 2.5$.
>
>  A vari√¢ncia de $\tilde{Y_2}$ √©:
> $Var(\tilde{Y_2}) = Var(Y_2 - 0.5Y_1) = \Omega_{22} - 0.5^2\Omega_{11} = 5 - 0.25 \times 4 = 4 $.
>
> A proje√ß√£o de $Y_3$ em $\tilde{Y_2}$ √©:
> $P(Y_3|\tilde{Y_2}) = \frac{Cov(Y_3, \tilde{Y_2})}{Var(\tilde{Y_2})}\tilde{Y_2} = \frac{2.5}{4}\tilde{Y_2} = 0.625\tilde{Y_2}$.
>
> O res√≠duo $\tilde{Y_3}$ √©:
> $\tilde{Y_3} = Y_3 - P(Y_3|Y_1) - P(Y_3|\tilde{Y_2}) = Y_3 - 0.25Y_1 - 0.625(Y_2 - 0.5Y_1) = Y_3 - 0.625Y_2 + 0.0625Y_1$.
>
> **Matriz A e D**
> Com os coeficientes obtidos, a matriz A (triangular inferior com 1 na diagonal principal) e a matriz D (diagonal com as vari√¢ncias dos res√≠duos) s√£o:
>
> $$A = \begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.25-0.625*0.5 & 0.625 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ -0.0625 & 0.625 & 1 \end{bmatrix} $$
>
> $$D = \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 6 - 1/4 - 2.5*2.5/4 = 4.375\end{bmatrix}$$
>
>
> Observe que a matriz A captura os coeficientes das proje√ß√µes sequenciais, e D cont√©m as vari√¢ncias dos res√≠duos. A matriz $A$ nos mostra como $Y_2$ e $Y_3$ dependem de $Y_1$, e como $Y_3$ depende do res√≠duo de $Y_2$. A matriz $D$ nos mostra a vari√¢ncia dos res√≠duos de cada proje√ß√£o. Os res√≠duos $\tilde{Y_1}, \tilde{Y_2}, \tilde{Y_3}$ s√£o ortogonais entre si, o que √© crucial para a lei das proje√ß√µes iteradas. Este exemplo ilustra como a fatora√ß√£o triangular transforma vari√°veis correlacionadas em vari√°veis n√£o correlacionadas, que s√£o os res√≠duos das proje√ß√µes sequenciais.

A rela√ß√£o entre a fatora√ß√£o triangular e a lei das proje√ß√µes iteradas se torna evidente quando notamos que cada etapa do processo de fatora√ß√£o triangular e das proje√ß√µes lineares envolve a identifica√ß√£o de um res√≠duo que √© ortogonal √†s vari√°veis utilizadas na proje√ß√£o anterior. Ou seja, cada novo elemento da matriz triangular inferior $A$ representa a remo√ß√£o da parte de uma vari√°vel que pode ser prevista linearmente pelas vari√°veis anteriores.

### Desenvolvimento

O processo de atualiza√ß√£o das proje√ß√µes lineares atrav√©s da fatora√ß√£o triangular nos fornece uma compreens√£o clara da lei das proje√ß√µes iteradas. Quando calculamos a proje√ß√£o de $Y_3$ dado $Y_1$ e $Y_2$, usamos a seguinte f√≥rmula [^4.5.30]:
$$
    P(Y_3|Y_2, Y_1) = P(Y_3|Y_1) + H_{32}H_{22}^{-1}[Y_2 - P(Y_2|Y_1)]
$$
Essa f√≥rmula nos mostra que a proje√ß√£o de $Y_3$ √© uma combina√ß√£o da proje√ß√£o de $Y_3$ sobre $Y_1$ e a parte n√£o prevista de $Y_2$ dado $Y_1$. A parte n√£o prevista de $Y_2$ √© o res√≠duo que √© ortogonal a $Y_1$, o que √© fundamental para a validade da lei das proje√ß√µes iteradas. Se projet√°ssemos novamente o resultado de $P(Y_3|Y_2, Y_1)$ sobre $Y_1$, o segundo termo desapareceria pois √© ortogonal a $Y_1$, e o resultado seria exatamente $P(Y_3|Y_1)$, conforme a lei das proje√ß√µes iteradas.

A fatora√ß√£o triangular da matriz de segundos momentos $\Omega$ garante que os res√≠duos das proje√ß√µes s√£o ortogonais. [^4.5.5]. Isso simplifica as contas ao construir proje√ß√µes lineares, pois cada nova vari√°vel $\tilde{Y_i}$ √© constru√≠da de forma a ser ortogonal a todas as vari√°veis anteriores, $\tilde{Y_1}, \dots, \tilde{Y}_{i-1}$.  A diagonal da matriz $D$ reflete o erro quadr√°tico m√©dio (MSE) de cada uma dessas proje√ß√µes sequenciais, como demostrado na proposi√ß√£o 1.

**Proposi√ß√£o 1.1** (MSE e a Matriz Diagonal D)
Os elementos da diagonal da matriz D, obtida pela fatora√ß√£o triangular $\Omega = ADA'$, representam o Erro Quadr√°tico M√©dio (MSE) da proje√ß√£o linear sequencial de cada vari√°vel sobre as anteriores.
*Prova:*
I. Seja $\tilde{Y} = A^{-1}Y$. Ent√£o, $E[\tilde{Y}\tilde{Y}'] = A^{-1}E[YY'](A^{-1})' = A^{-1}\Omega(A^{-1})' = A^{-1}ADA'(A^{-1})' = D$.
II. O elemento $d_{ii}$ da matriz diagonal $D$ √© dado por $E[\tilde{Y_i}^2]$.
III. $\tilde{Y_i}$ √© o res√≠duo da proje√ß√£o linear de $Y_i$ sobre $Y_1, \dots, Y_{i-1}$.
IV. Portanto, $d_{ii} = E[\tilde{Y_i}^2]$ √© o MSE da proje√ß√£o linear de $Y_i$ sobre $Y_1, \dots, Y_{i-1}$.
‚ñ†

**Exemplo Num√©rico:**
Para demonstrar a aplica√ß√£o da lei das proje√ß√µes iteradas, vamos usar os mesmos vetores $Y_1$, $Y_2$, e $Y_3$ do exemplo da se√ß√£o anterior. Primeiro, vamos projetar $Y_3$ sobre $Y_1$. Usando os elementos da matriz de covari√¢ncia $\Omega$, a proje√ß√£o √©:
$$ P(Y_3|Y_1) = \Omega_{31}\Omega_{11}^{-1}Y_1 = \frac{1}{4}Y_1  $$

Agora, vamos calcular a proje√ß√£o de $Y_2$ sobre $Y_1$:
$$ P(Y_2|Y_1) = \Omega_{21}\Omega_{11}^{-1}Y_1 = \frac{2}{4}Y_1 = 0.5Y_1  $$
O res√≠duo da proje√ß√£o de $Y_2$ sobre $Y_1$ √©:
$$  Y_2 - P(Y_2|Y_1) = Y_2 - 0.5Y_1  $$
A covari√¢ncia $H_{32}$ entre o erro da proje√ß√£o de $Y_3$ sobre $Y_1$ e o res√≠duo da proje√ß√£o de $Y_2$ sobre $Y_1$ √© dada por:
$$ H_{32} = E[(Y_3 - \frac{1}{4}Y_1)(Y_2 - 0.5Y_1)] = \Omega_{23} -  \frac{1}{4}\Omega_{12} - \frac{1}{2}\Omega_{31} + \frac{1}{8}\Omega_{11} = 3 - 0.5 - 0.5 + 0.5 = 2.5 $$
A vari√¢ncia do res√≠duo da proje√ß√£o de $Y_2$ sobre $Y_1$ √© dada por:
$$ H_{22} = E[(Y_2 - 0.5Y_1)^2] = \Omega_{22} - \frac{2}{4} \Omega_{12} = 5 - 1 = 4 $$
Agora, a proje√ß√£o de $Y_3$ sobre $Y_1$ e $Y_2$ √©:
$$ P(Y_3|Y_1, Y_2) =  \frac{1}{4}Y_1 + \frac{2.5}{4}(Y_2 - 0.5Y_1) = \frac{1}{4}Y_1 + 0.625(Y_2 - 0.5Y_1)  $$
Se projetarmos novamente $P(Y_3|Y_1, Y_2)$ sobre $Y_1$, o segundo termo desaparece pois √© ortogonal a $Y_1$ e obtemos:
$$P[P(Y_3|Y_1, Y_2)|Y_1] =  \frac{1}{4}Y_1 = P(Y_3|Y_1) $$
Conforme esperado pela lei das proje√ß√µes iteradas.

### Conclus√£o

A **lei das proje√ß√µes iteradas** √© um conceito fundamental na an√°lise de s√©ries temporais, assegurando que as proje√ß√µes lineares sejam consistentes mesmo quando iteradas sobre subconjuntos de vari√°veis. A **fatora√ß√£o triangular** fornece uma estrutura para a implementa√ß√£o dessa lei, garantindo a ortogonalidade entre os res√≠duos das proje√ß√µes sequenciais e simplificando a constru√ß√£o de modelos preditivos.
A rela√ß√£o entre a lei das proje√ß√µes iteradas e a fatora√ß√£o triangular permite uma compreens√£o mais profunda de como a informa√ß√£o se propaga em sistemas complexos. A fatora√ß√£o triangular nos permite transformar um conjunto de vari√°veis correlacionadas em um conjunto de vari√°veis n√£o correlacionadas, cujas rela√ß√µes com os dados originais s√£o bem compreendidas, e a lei das proje√ß√µes iteradas assegura que as opera√ß√µes sobre esses res√≠duos sejam consistentes. Essa combina√ß√£o √© essencial para o desenvolvimento de modelos preditivos eficientes e robustos em an√°lise de s√©ries temporais.
Em resumo, a lei das proje√ß√µes iteradas e a fatora√ß√£o triangular s√£o ferramentas complementares que, quando combinadas, oferecem um poderoso arcabou√ßo te√≥rico e pr√°tico para o desenvolvimento de modelos preditivos em s√©ries temporais, com uma forte base matem√°tica.

**Observa√ß√£o:** A lei das proje√ß√µes iteradas √© uma propriedade geral de proje√ß√µes lineares, e n√£o depende da distribui√ß√£o espec√≠fica das vari√°veis. No entanto, para processos Gaussianos, essa propriedade tem uma interpreta√ß√£o ainda mais forte. Como ser√° discutido na pr√≥xima se√ß√£o, a proje√ß√£o linear √© a melhor previs√£o poss√≠vel quando lidamos com vari√°veis que seguem uma distribui√ß√£o normal multivariada.

### Refer√™ncias
[^4.4.1]: Any positive definite symmetric (n √ó n) matrix $\Omega$ has a unique representation of the form $\Omega = ADA'$, where $A$ is a lower triangular matrix with 1s along the principal diagonal...
[^4.5.4]: Substituting [4.5.1] into [4.5.3], the second-moment matrix of $\tilde{Y}$ is seen to be diagonal: $E(\tilde{Y}\tilde{Y}')$ = $A^{-1}\Omega[A']^{-1} = A^{-1}ADA'[A']^{-1} = D$.
[^4.5.5]: That is, $E(\tilde{Y_i}\tilde{Y_j}) = \begin{cases} d_{ii} & \text{for } i=j \\ 0 & \text{for } i \neq j \end{cases}$.
[^4.5.9]: or, using [4.5.8], $\tilde{Y_2}$ = $Y_2$ - $\Omega_{21}\Omega_{11}^{-1}Y_1$ = $Y_2$ - $\alpha Y_1$, where we have defined $\alpha$ = $\Omega_{21}\Omega_{11}^{-1}$. The fact that $\tilde{Y_2}$ is uncorrelated with $Y_1$ implies $E(\tilde{Y_2}Y_1)$ = $E[(Y_2 - \alpha Y_1)Y_1] = 0$.
[^4.5.30]: $P(Y_3|Y_2,Y_1) = \Omega_{31}\Omega_{11}^{-1}Y_1 + H_{32}H_{22}^{-1}(Y_2 - \Omega_{21}\Omega_{11}^{-1}Y_1)
        = P(Y_3|Y_1) + H_{32}H_{22}^{-1}[Y_2 - P(Y_2|Y_1)]$, where $H_{32}$ = $E\{[Y_3 - P(Y_3|Y_1)][Y_2 - P(Y_2|Y_1)]'\}$.
[^4.5.32]: Another useful result, the law of iterated projections, can be inferred im- mediately from [4.5.30]. What happens if the projection $P(Y_3|Y_2,Y_1)$ is itself projected on $Y_1$? The law of iterated projections says that this projection is equal to the simple projection of $Y_3$ on $Y_1$: $P[P(Y_3|Y_2,Y_1)|Y_1] = P(Y_3|Y_1)$.
To verify this claim, we need to show that the difference between $P(Y_3|Y_2,Y_1)$ and $P(Y_3|Y_1)$ is uncorrelated with $Y_1$. But from [4.5.30], this difference is given by

$P(Y_3|Y_2,Y_1) - P(Y_3|Y_1) = H_{32}H_{22}^{-1}[Y_2 - P(Y_2|Y_1)]$,

which indeed is uncorrelated with $Y_1$ by the definition of the linear projection $P(Y_2|Y_1)$.

## 4.6. Optimal Forecasts for Gaussian Processes

The forecasting rules developed in this chapter are optimal within the class of linear functions of the variables on which the forecast is based. For Gaussian processes, we can make the stronger claim that as long as a constant term is included among the variables on which the forecast is based, the optimal unrestricted forecast turns out to have a linear form and thus is given by the linear projection.

To verify this, let $Y_1$ be an $(n_1 \times 1)$ vector with mean $\mu_1$, and $Y_2$ an $(n_2 \times 1)$ vector with mean $\mu_2$, where the variance-covariance matrix is given by

$$
\begin{bmatrix}
E[(Y_1 - \mu_1)(Y_1 - \mu_1)'] & E[(Y_1 - \mu_1)(Y_2 - \mu_2)'] \\
E[(Y_2 - \mu_2)(Y_1 - \mu_1)'] & E[(Y_2 - \mu_2)(Y_2 - \mu_2)']
\end{bmatrix} =
\begin{bmatrix}
\Omega_{11} & \Omega_{12} \\
\Omega_{21} & \Omega_{22}
\end{bmatrix}.
$$

If $Y_1$ and $Y_2$ are Gaussian, then the joint probability density is
$$
f_{Y_1,Y_2}(y_1, y_2) = \frac{1}{(2\pi)^{(n_1+n_2)/2}|\Omega|^{1/2}} \exp \left\{ -\frac{1}{2}
\begin{bmatrix}
y_1 - \mu_1 \\
y_2 - \mu_2
\end{bmatrix}'
\begin{bmatrix}
\Omega_{11} & \Omega_{12} \\
\Omega_{21} & \Omega_{22}
\end{bmatrix}^{-1}
\begin{bmatrix}
y_1 - \mu_1 \\
y_2 - \mu_2
\end{bmatrix}
\right\}.
$$  [4.6.1]

The inverse of $\Omega$ is readily found by inverting [4.5.26]:

$$
\Omega^{-1} = [ADA']^{-1} = [A']^{-1}D^{-1}A^{-1} =
\begin{bmatrix}
I_{n_1} & -\Omega_{11}^{-1}\Omega_{12} \\
0 & I_{n_2}
\end{bmatrix}
\begin{bmatrix}
\Omega_{11}^{-1} & 0 \\
0 & (\Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12})^{-1}
\end{bmatrix}
\begin{bmatrix}
I_{n_1} & 0 \\
-\Omega_{21}\Omega_{11}^{-1} & I_{n_2}
\end{bmatrix}
$$
$$
=
\begin{bmatrix}
\Omega_{11}^{-1} + \Omega_{11}^{-1}\Omega_{12}(\Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12})^{-1}\Omega_{21}\Omega_{11}^{-1} & -\Omega_{11}^{-1}\Omega_{12}(\Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12})^{-1} \\
-(\Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12})^{-1}\Omega_{21}\Omega_{11}^{-1}  & (\Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12})^{-1}
\end{bmatrix}
$$  [4.6.2]
Likewise, the determinant of $\Omega$ can be found by taking the determinant of [4.5.26]:
$|\Omega| = |ADA'| = |A| |D| |A'| = |D|$.

But A is a lower triangular matrix. Its determinant is therefore given by the product of terms along the principal diagonal, all of which are unity. Hence $|A| = 1$ and $|\Omega| = |D|$:
$$
\begin{vmatrix}
\Omega_{11} & \Omega_{12} \\
\Omega_{21} & \Omega_{22}
\end{vmatrix} = \begin{vmatrix}
\Omega_{11} & 0 \\
0 & \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}
\end{vmatrix} =
|\Omega_{11}| |\Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}|.
$$  [4.6.3]

Substituting [4.6.2] and [4.6.3] into [4.6.1], the joint density can be written
$$
f_{Y_1,Y_2}(y_1, y_2) = \frac{1}{(2\pi)^{(n_1+n_2)/2}|\Omega_{11}|^{-1/2}|\Omega_{22}-\Omega_{21}\Omega_{11}^{-1}\Omega_{12}|^{-1/2}} \times
$$
$$
 \exp \left\{ -\frac{1}{2}
 \begin{bmatrix}
 y_1 - \mu_1 \\
 y_2 - \mu_2
 \end{bmatrix}'
 \begin{bmatrix}
 \Omega_{11}^{-1} + \Omega_{11}^{-1}\Omega_{12}(\Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12})^{-1}\Omega_{21}\Omega_{11}^{-1} & -\Omega_{11}^{-1}\Omega_{12}(\Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12})^{-1} \\
 -(\Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12})^{-1}\Omega_{21}\Omega_{11}^{-1} & (\Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12})^{-1}
 \end{bmatrix}
 \begin{bmatrix}
 y_1 - \mu_1 \\
 y_2 - \mu_2
 \end{bmatrix}
 \right\}
$$
$$
= \frac{1}{(2\pi)^{(n_1+n_2)/2}|\Omega_{11}|^{-1/2}|\Omega_{22}-\Omega_{21}\Omega_{11}^{-1}\Omega_{12}|^{-1/2}}
 \times
$$
$$
 \exp \left\{ -\frac{1}{2}
  \begin{bmatrix}
  y_1 - \mu_1 \\
  y_2 - m
  \end{bmatrix}'
  \begin{bmatrix}
  \Omega_{11}^{-1} & 0 \\
  0 & (\Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12})^{-1}
  \end{bmatrix}
  \begin{bmatrix}
  y_1 - \mu_1 \\
  y_2 - m
  \end{bmatrix}
  \right\}
$$
$$
= \frac{1}{(2\pi)^{(n_1+n_2)/2}|\Omega_{11}|^{-1/2}|\Omega_{22}-\Omega_{21}\Omega_{11}^{-1}\Omega_{12}|^{-1/2}}
$$
$$
\times \exp \left\{
-\frac{1}{2} (y_1 - \mu_1)'\Omega_{11}^{-1}(y_1 - \mu_1)
 - \frac{1}{2} (y_2 - m)'(\Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12})^{-1}(y_2 - m)
 \right\},
$$  [4.6.4]
where
$$
m = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(y_1 - \mu_1).
$$  [4.6.5]
The conditional density of $Y_2$ given $Y_1$ is found by dividing the joint density [4.6.4] by the marginal density:
$$
f_{Y_2|Y_1}(y_2|y_1) = \frac{f_{Y_1,Y_2}(y_1, y_2)}{f_{Y_1}(y_1)} = \frac{1}{(2\pi)^{n_2/2}|H|^{1/2}} \exp \left\{ -\frac{1}{2} (y_2 - m)' H^{-1} (y_2 - m) \right\},
$$
where
$$
H = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}.
$$  [4.6.6]

In other words,
$$
Y_2|Y_1 \sim N(m, H) \sim N([\mu_2 + \Omega_{21}\Omega_{11}^{-1}(y_1 - \mu_1)], [\Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}]).
$$  [4.6.7]

We saw in Section 4.1 that the optimal unrestricted forecast is given by the conditional expectation. For a Gaussian process, the optimal forecast is thus
$$
E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1 - \mu_1).
$$

On the other hand, for any distribution, the linear projection of the vector $Y_2$ on a vector $Y_1$ and a constant term is given by
$$
\hat{E}(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1 - \mu_1).
$$
Hence, for a Gaussian process, the linear projection gives the unrestricted optimal forecast.

**Teorema 3** (Optimalidade da Proje√ß√£o Linear em Processos Gaussianos) Para um processo Gaussiano, a proje√ß√£o linear de $Y_2$ sobre $Y_1$ √© a melhor previs√£o poss√≠vel, no sentido de minimizar o erro quadr√°tico m√©dio, dentro do conjunto de todas as fun√ß√µes de $Y_1$.
*Prova:*
I.  A esperan√ßa condicional $E(Y_2|Y_1)$ √© a melhor previs√£o de $Y_2$ dado $Y_1$, no sentido de minimizar o erro quadr√°tico m√©dio.
II.  Para um processo Gaussiano, a esperan√ßa condicional $E(Y_2|Y_1)$ √© uma fun√ß√£o linear de $Y_1$, como demonstrado na equa√ß√£o [4.6.7].
III.  A proje√ß√£o linear $\hat{E}(Y_2|Y_1)$ coincide com a esperan√ßa condicional $E(Y_2|Y_1)$ para processos Gaussianos.
IV.  Portanto, a proje√ß√£o linear √© a melhor previs√£o poss√≠vel para processos Gaussianos.
‚ñ†

> üí° **Exemplo Num√©rico:** Suponha que temos duas vari√°veis aleat√≥rias Gaussianas $Y_1$ e $Y_2$, com m√©dias $\mu_1 = 2$ e $\mu_2 = 5$, e a seguinte matriz de covari√¢ncia:
>
> $$\Omega = \begin{bmatrix} 4 & 2 \\ 2 & 9 \end{bmatrix}$$
>
> Queremos encontrar a melhor previs√£o de $Y_2$ dado $Y_1$. Pela teoria de processos Gaussianos, a melhor previs√£o √© dada pela esperan√ßa condicional, que neste caso √© uma fun√ß√£o linear:
>
> $$E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1 - \mu_1)$$
>
> Substituindo os valores:
>
>$$E(Y_2|Y_1=10) = 10 + \frac{2}{4}(10 - 5) = 10 + \frac{2}{4}5 = 10 + 2.5 = 12.5$$

Isso significa que, dado que $Y_1 = 10$, a melhor estimativa para $Y_2$ √© 12.5.

**Regress√£o Linear com M√∫ltiplas Vari√°veis**

Agora, considere um cen√°rio com m√∫ltiplas vari√°veis. Seja $Y$ um vetor de vari√°veis aleat√≥rias, dividido em duas partes: $Y_1$ (com dimens√£o $n_1$) e $Y_2$ (com dimens√£o $n_2$). A m√©dia e a matriz de covari√¢ncia de $Y$ podem ser escritas como:

$$
\mu = \begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix}, \quad \Omega = \begin{bmatrix} \Omega_{11} & \Omega_{12} \\ \Omega_{21} & \Omega_{22} \end{bmatrix}
$$

onde:
* $\mu_1$ √© o vetor de m√©dias de $Y_1$.
* $\mu_2$ √© o vetor de m√©dias de $Y_2$.
* $\Omega_{11}$ √© a matriz de covari√¢ncia de $Y_1$.
* $\Omega_{22}$ √© a matriz de covari√¢ncia de $Y_2$.
* $\Omega_{12} = \Omega_{21}^T$ √© a matriz de covari√¢ncia entre $Y_1$ e $Y_2$.

A esperan√ßa condicional de $Y_2$ dado $Y_1$ √© dada por:

$$E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1 - \mu_1)$$

Esta f√≥rmula √© uma generaliza√ß√£o da regress√£o linear simples para m√∫ltiplas vari√°veis, onde $\Omega_{21}\Omega_{11}^{-1}$ desempenha o papel dos coeficientes de regress√£o.

**Exemplo com Tr√™s Vari√°veis**

Vamos supor que temos tr√™s vari√°veis aleat√≥rias $Y_1$, $Y_2$ e $Y_3$ com as seguintes caracter√≠sticas:

$$
\mu = \begin{bmatrix} 2 \\ 4 \\ 6 \end{bmatrix}, \quad \Omega = \begin{bmatrix} 1 & 0.5 & 0.2 \\ 0.5 & 2 & 0.8 \\ 0.2 & 0.8 & 3 \end{bmatrix}
$$

Queremos encontrar $E(Y_3|Y_1=3, Y_2=5)$. Podemos particionar $Y$ em $Y_1 = [Y_1, Y_2]$ e $Y_2 = [Y_3]$, com:

$$
\mu_1 = \begin{bmatrix} 2 \\ 4 \end{bmatrix}, \quad \mu_2 = [6]
$$

$$
\Omega_{11} = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 2 \end{bmatrix}, \quad \Omega_{12} = \begin{bmatrix} 0.2 \\ 0.8 \end{bmatrix}, \quad \Omega_{21} = \begin{bmatrix} 0.2 & 0.8 \end{bmatrix}, \quad \Omega_{22} = [3]
$$

Primeiro, calculamos $\Omega_{11}^{-1}$. A inversa de uma matriz 2x2 $\begin{bmatrix} a & b \\ c & d \end{bmatrix}$ √© $\frac{1}{ad-bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$. Portanto:

$$
\Omega_{11}^{-1} = \frac{1}{1\cdot2 - 0.5\cdot0.5} \begin{bmatrix} 2 & -0.5 \\ -0.5 & 1 \end{bmatrix} = \frac{1}{1.75} \begin{bmatrix} 2 & -0.5 \\ -0.5 & 1 \end{bmatrix} = \begin{bmatrix} 1.143 & -0.286 \\ -0.286 & 0.571 \end{bmatrix}
$$

Agora, calculamos $\Omega_{21}\Omega_{11}^{-1}$:

$$
\Omega_{21}\Omega_{11}^{-1} = \begin{bmatrix} 0.2 & 0.8 \end{bmatrix} \begin{bmatrix} 1.143 & -0.286 \\ -0.286 & 0.571 \end{bmatrix} = \begin{bmatrix} 0.2 \cdot 1.143 + 0.8 \cdot (-0.286) & 0.2 \cdot (-0.286) + 0.8 \cdot 0.571 \end{bmatrix} = \begin{bmatrix} 0.0002 & 0.3996 \end{bmatrix}
$$

Finalmente, substitu√≠mos na f√≥rmula:

$$
E(Y_3|Y_1=3, Y_2=5) = 6 + \begin{bmatrix} 0.0002 & 0.3996 \end{bmatrix} \left( \begin{bmatrix} 3 \\ 5 \end{bmatrix} - \begin{bmatrix} 2 \\ 4 \end{bmatrix} \right) = 6 + \begin{bmatrix} 0.0002 & 0.3996 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = 6 + 0.0002 + 0.3996 = 6.3998
$$

Portanto, a melhor estimativa para $Y_3$, dado que $Y_1 = 3$ e $Y_2 = 5$, √© aproximadamente 6.4.

<!-- END -->
