## O Erro de ProjeÃ§Ã£o Linear e sua RelaÃ§Ã£o com a FatoraÃ§Ã£o Triangular

### IntroduÃ§Ã£o
Neste capÃ­tulo, vamos explorar como o **erro de projeÃ§Ã£o linear** pode ser expresso utilizando as variÃ¡veis transformadas atravÃ©s da **fatoraÃ§Ã£o triangular**, revelando a natureza da distribuiÃ§Ã£o desses erros. Como vimos anteriormente, a fatoraÃ§Ã£o triangular nos permite decompor uma matriz de segundos momentos em componentes que facilitam a anÃ¡lise de projeÃ§Ãµes lineares e a geraÃ§Ã£o de variÃ¡veis nÃ£o correlacionadas. [^4.5.4]. Agora, vamos examinar como o erro de projeÃ§Ã£o, que Ã© a diferenÃ§a entre o valor real e a previsÃ£o linear, se relaciona com as variÃ¡veis transformadas e como a matriz de covariÃ¢ncia diagonal dessas variÃ¡veis simplifica essa anÃ¡lise.

### Conceitos Fundamentais

O **erro de projeÃ§Ã£o linear** Ã© definido como a diferenÃ§a entre o valor observado de uma variÃ¡vel e sua previsÃ£o obtida por meio de uma projeÃ§Ã£o linear. Seja $Y_i$ uma variÃ¡vel aleatÃ³ria e $\hat{Y_i}$ a sua previsÃ£o linear baseada em outras variÃ¡veis, o erro de projeÃ§Ã£o $\epsilon_i$ Ã© dado por:
$$ \epsilon_i = Y_i - \hat{Y_i} $$
onde $\hat{Y_i}$ Ã© a projeÃ§Ã£o linear de $Y_i$ em um conjunto de outras variÃ¡veis. [^4.1.2], [^4.1.9]. A anÃ¡lise do erro de projeÃ§Ã£o Ã© crucial em estatÃ­stica e econometria, pois ele revela a qualidade da previsÃ£o. A **fatoraÃ§Ã£o triangular**, como demonstrado, nos fornece uma forma de transformar variÃ¡veis originais em variÃ¡veis nÃ£o correlacionadas $\tilde{Y}$, onde $\tilde{Y} = A^{-1}Y$. [^4.5.2]. Essa transformaÃ§Ã£o, crucial para o entendimento das projeÃ§Ãµes lineares, permite reescrever o erro de projeÃ§Ã£o em termos dessas novas variÃ¡veis.

> ðŸ’¡ **Exemplo Ilustrativo:** Suponha que estamos tentando prever a variÃ¡vel $Y_3$ usando $Y_1$ e $Y_2$. A projeÃ§Ã£o linear nos fornece uma previsÃ£o $\hat{Y_3}$ que Ã© uma combinaÃ§Ã£o linear de $Y_1$ e $Y_2$. O erro de projeÃ§Ã£o $\epsilon_3$ Ã© a diferenÃ§a entre o valor real de $Y_3$ e a previsÃ£o $\hat{Y_3}$. O objetivo da anÃ¡lise do erro Ã© entender sua natureza e como ele se relaciona com as variÃ¡veis usadas para gerar a previsÃ£o.
> Se o erro estiver correlacionado com as variÃ¡veis usadas na projeÃ§Ã£o, isso indica que a projeÃ§Ã£o linear nÃ£o captura toda a informaÃ§Ã£o relevante do sistema.

A fatoraÃ§Ã£o triangular e a transformaÃ§Ã£o $\tilde{Y} = A^{-1}Y$ desempenham um papel essencial nesta anÃ¡lise, pois transformam o problema de analisar erros correlacionados em um problema de analisar erros nÃ£o correlacionados.

**Teorema 4** (O Erro de ProjeÃ§Ã£o Linear e as VariÃ¡veis Transformadas)
O erro de projeÃ§Ã£o linear para cada variÃ¡vel $Y_i$ pode ser expresso como uma combinaÃ§Ã£o linear das variÃ¡veis transformadas $\tilde{Y}_j$, onde $j \geq i$, e que a matriz de covariÃ¢ncia do vetor de erros tem a mesma forma diagonal $D$ da matriz de covariÃ¢ncia das variÃ¡veis transformadas.
*Prova:*
I. A fatoraÃ§Ã£o triangular nos dÃ¡ $\Omega = ADA'$, onde $\Omega$ Ã© a matriz de covariÃ¢ncia do vetor $Y$. [^4.4.1].
II. As variÃ¡veis transformadas sÃ£o $\tilde{Y} = A^{-1}Y$, e tÃªm matriz de covariÃ¢ncia $E(\tilde{Y}\tilde{Y}') = D$, que Ã© diagonal. [^4.5.4].
III. A relaÃ§Ã£o entre $Y$ e $\tilde{Y}$ Ã© dada por $Y = A\tilde{Y}$. [^4.5.6].
IV. Seja $\epsilon$ o vetor de erros de projeÃ§Ã£o linear de cada variÃ¡vel $Y_i$ sobre as variÃ¡veis precedentes $Y_1, \ldots, Y_{i-1}$.
V. Pela lei das projeÃ§Ãµes iteradas, o erro de projeÃ§Ã£o de $Y_i$ sobre as variÃ¡veis anteriores Ã© o mesmo que o i-Ã©simo elemento da transformaÃ§Ã£o $A^{-1}Y$, ou seja, o i-Ã©simo elemento de $\tilde{Y}$.
VI. Podemos escrever o vetor de erros $\epsilon = \tilde{Y}$
VII. Como $E(\tilde{Y}\tilde{Y}') = D$, a matriz de covariÃ¢ncia do vetor de erros Ã© $E(\epsilon\epsilon')=D$. Isso prova que os erros de projeÃ§Ã£o sÃ£o nÃ£o correlacionados e tÃªm variÃ¢ncias dadas pelos elementos diagonais de $D$.
VIII. O erro de projeÃ§Ã£o para cada variÃ¡vel $Y_i$ pode ser expressa como uma combinaÃ§Ã£o linear de variÃ¡veis transformadas: $\epsilon_i = \tilde{Y_i}$.
â– 

**Teorema 4.1** (RelaÃ§Ã£o entre a Matriz de CovariÃ¢ncia dos Erros de ProjeÃ§Ã£o e a Matriz D)
A matriz de covariÃ¢ncia dos erros de projeÃ§Ã£o $\epsilon$, denotada por $E(\epsilon \epsilon')$, Ã© igual Ã  matriz diagonal $D$ obtida na fatoraÃ§Ã£o triangular da matriz de covariÃ¢ncia $\Omega$.
*Prova:*
I. Do Teorema 4, temos que $\epsilon = \tilde{Y}$.
II. A matriz de covariÃ¢ncia de $\tilde{Y}$ Ã© dada por $E(\tilde{Y} \tilde{Y}') = D$ (pela propriedade da fatoraÃ§Ã£o triangular).
III. Substituindo $\epsilon$ por $\tilde{Y}$, obtemos $E(\epsilon \epsilon') = E(\tilde{Y} \tilde{Y}') = D$.
IV. Portanto, a matriz de covariÃ¢ncia dos erros de projeÃ§Ã£o Ã© igual Ã  matriz diagonal $D$.
â– 

Este teorema demonstra que o erro de projeÃ§Ã£o linear para cada variÃ¡vel $Y_i$ pode ser expressa como uma variÃ¡vel $\tilde{Y_i}$ e, portanto, a matriz de covariÃ¢ncia dos erros de projeÃ§Ã£o Ã© diagonal, cujos elementos diagonais sÃ£o os erros quadrÃ¡ticos mÃ©dios das projeÃ§Ãµes. Ou seja, os elementos diagonais da matriz D representam o erro quadrÃ¡tico mÃ©dio associado a cada resÃ­duo obtido pelas projeÃ§Ãµes lineares sequenciais, como observado na proposiÃ§Ã£o 1.1 do capÃ­tulo anterior.

> ðŸ’¡ **Exemplo NumÃ©rico:** Retomando o exemplo de projeÃ§Ã£o de $Y_3$ usando $Y_1$ e $Y_2$, vimos que o erro da projeÃ§Ã£o de $Y_3$ sobre $Y_1$ e $Y_2$ Ã© dado por:
>$$
\tilde{Y_3} = Y_3 - P(Y_3|Y_2,Y_1)
$$
>Da mesma forma, $\tilde{Y_2} = Y_2 - P(Y_2|Y_1)$. Note que $\tilde{Y_1} = Y_1$.
>A transformaÃ§Ã£o $\tilde{Y} = A^{-1}Y$, com a matriz $A$ obtida pela fatoraÃ§Ã£o triangular, gera as variÃ¡veis nÃ£o correlacionadas $\tilde{Y_i}$. O erro de projeÃ§Ã£o linear de $Y_i$ equivale a $\tilde{Y_i}$.
> Suponha que temos trÃªs variÃ¡veis $Y_1, Y_2, Y_3$ com a seguinte matriz de covariÃ¢ncia $\Omega$:
>
> $$\Omega = \begin{bmatrix} 1 & 0.5 & 0.3 \\ 0.5 & 1 & 0.6 \\ 0.3 & 0.6 & 1 \end{bmatrix}$$
>
> A fatoraÃ§Ã£o triangular $\Omega = ADA'$ nos dÃ¡:
>
> $$A = \begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.3 & 0.47 & 1 \end{bmatrix}$$
>
> $$D = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0.75 & 0 \\ 0 & 0 & 0.33 \end{bmatrix}$$
>
> Aqui, $d_{11} = 1$, $d_{22} = 0.75$, e $d_{33} = 0.33$. Observe que $d_{11}$ Ã© a variÃ¢ncia de $Y_1$, $d_{22}$ Ã© o erro quadrÃ¡tico mÃ©dio da projeÃ§Ã£o de $Y_2$ sobre $Y_1$ e $d_{33}$ Ã© o erro quadrÃ¡tico mÃ©dio da projeÃ§Ã£o de $Y_3$ sobre $Y_1$ e $Y_2$. As variÃ¡veis transformadas $\tilde{Y}$ sÃ£o obtidas como $\tilde{Y} = A^{-1}Y$ e os erros de projeÃ§Ã£o sÃ£o iguais a $\tilde{Y}$. Portanto, $\tilde{Y_1}$ Ã© o erro de projeÃ§Ã£o para $Y_1$ (que Ã© apenas $Y_1$ pois nÃ£o hÃ¡ projeÃ§Ã£o), $\tilde{Y_2}$ Ã© o erro de projeÃ§Ã£o para $Y_2$ dado $Y_1$ e $\tilde{Y_3}$ Ã© o erro de projeÃ§Ã£o para $Y_3$ dado $Y_1$ e $Y_2$.

**Lema 2** (RelaÃ§Ã£o entre o Erro de ProjeÃ§Ã£o Linear e as VariÃ¡veis Transformadas) O erro de projeÃ§Ã£o linear de cada variÃ¡vel $Y_i$ sobre as variÃ¡veis anteriores $Y_1, ..., Y_{i-1}$ corresponde ao i-Ã©simo elemento da transformaÃ§Ã£o $A^{-1}Y$.

*Prova*:
I. Definimos $\tilde{Y} = A^{-1}Y$.
II. Expandindo esta equaÃ§Ã£o:
$$
\begin{bmatrix} \tilde{Y_1} \\ \tilde{Y_2} \\ \tilde{Y_3} \\ \vdots \\ \tilde{Y_n} \end{bmatrix} =
A^{-1} \begin{bmatrix} Y_1 \\ Y_2 \\ Y_3 \\ \vdots \\ Y_n \end{bmatrix}
$$
III. Explicitando a transformaÃ§Ã£o do exemplo anterior
$$\tilde{Y} = \begin{bmatrix}
    1 & 0 & 0 & \cdots & 0 \\
    -\Omega_{21}\Omega_{11}^{-1} & 1 & 0 & \cdots & 0 \\
    \cdots & \cdots & \cdots & \ddots & \vdots \\
    \cdots & \cdots & \cdots & \cdots & 1
    \end{bmatrix}
    \begin{bmatrix}
        Y_1 \\
        Y_2 \\
        Y_3 \\
        \vdots \\
        Y_n
    \end{bmatrix}
$$
IV. O i-Ã©simo elemento de $\tilde{Y}$ Ã© o resultado da projeÃ§Ã£o de $Y_i$ sobre $Y_1$ a $Y_{i-1}$.
V. Portanto, $\tilde{Y_i}$ representa o erro de projeÃ§Ã£o de $Y_i$ sobre as variÃ¡veis precedentes, e equivale a $\epsilon_i$.
â– 

### Desenvolvimento
A relaÃ§Ã£o entre o erro de projeÃ§Ã£o linear e a fatoraÃ§Ã£o triangular se manifesta na estrutura da matriz $D$, resultante da transformaÃ§Ã£o $\tilde{Y} = A^{-1}Y$. Como a matriz $D$ Ã© diagonal, os elementos $\tilde{Y}_i$ sÃ£o nÃ£o correlacionados. Ou seja, o erro de projeÃ§Ã£o para cada variÃ¡vel, expresso como $\tilde{Y_i}$, nÃ£o estÃ¡ correlacionado com os erros de projeÃ§Ã£o para as outras variÃ¡veis. Isto simplifica a anÃ¡lise do erro, pois em vez de trabalhar com erros correlacionados em $Y$, temos agora um sistema de erros nÃ£o correlacionados em $\tilde{Y}$. [^4.5.5].
A propriedade diagonal da matriz $D$, como visto, significa que a covariÃ¢ncia entre quaisquer duas variÃ¡veis transformadas $\tilde{Y_i}$ e $\tilde{Y_j}$ (com $i \neq j$) Ã© zero, ou seja, $E(\tilde{Y_i}\tilde{Y_j}) = 0$. [^4.5.5]. Os elementos da diagonal de $D$ representam, como vimos, a variÃ¢ncia de cada $\tilde{Y_i}$, que tambÃ©m correspondem aos erros quadrÃ¡ticos mÃ©dios das projeÃ§Ãµes sequenciais, confirmando a proposiÃ§Ã£o 1.1 do capÃ­tulo anterior.

> ðŸ’¡ **Exemplo NumÃ©rico:** Usando a matriz $\Omega$ do exemplo anterior, temos a matriz $D$ com elementos diagonais que sÃ£o os erros quadrÃ¡ticos mÃ©dios da projeÃ§Ã£o linear correspondente. Ou seja:
> - $d_{11}$ Ã© o MSE da projeÃ§Ã£o de $Y_1$, que Ã© apenas a sua variÃ¢ncia, jÃ¡ que nÃ£o hÃ¡ projeÃ§Ã£o.
> - $d_{22}$ Ã© o MSE da projeÃ§Ã£o de $Y_2$ sobre $Y_1$.
> - $d_{33}$ Ã© o MSE da projeÃ§Ã£o de $Y_3$ sobre $Y_1$ e $Y_2$.
> E todos os elementos fora da diagonal de $D$ sÃ£o zero, o que significa que os resÃ­duos sÃ£o nÃ£o correlacionados.
> Este exemplo ilustra como a fatoraÃ§Ã£o triangular leva a variÃ¡veis nÃ£o correlacionadas e como os elementos da diagonal de $D$ revelam o erro quadrÃ¡tico mÃ©dio das projeÃ§Ãµes sequenciais.
>
> Numericamente, usando a matriz $D$ do exemplo anterior, onde
> $$D = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0.75 & 0 \\ 0 & 0 & 0.33 \end{bmatrix}$$
> Temos que:
> - $d_{11} = 1$: O erro quadrÃ¡tico mÃ©dio da projeÃ§Ã£o de $Y_1$ Ã© igual Ã  sua variÃ¢ncia, que Ã© 1.
> - $d_{22} = 0.75$: O erro quadrÃ¡tico mÃ©dio da projeÃ§Ã£o de $Y_2$ sobre $Y_1$ Ã© 0.75.
> - $d_{33} = 0.33$: O erro quadrÃ¡tico mÃ©dio da projeÃ§Ã£o de $Y_3$ sobre $Y_1$ e $Y_2$ Ã© 0.33.
>
> Como a matriz D Ã© diagonal, a covariÃ¢ncia entre $\tilde{Y_i}$ e $\tilde{Y_j}$ Ã© 0 para $i \neq j$. Por exemplo, $E(\tilde{Y_1}\tilde{Y_2}) = 0$ e $E(\tilde{Y_2}\tilde{Y_3}) = 0$, demonstrando que os resÃ­duos sÃ£o nÃ£o correlacionados.

A relaÃ§Ã£o entre a fatoraÃ§Ã£o triangular e os erros de projeÃ§Ã£o se torna mais clara ao analisar o processo de atualizaÃ§Ã£o de projeÃ§Ãµes lineares. A equaÃ§Ã£o [^4.5.16] mostra que a atualizaÃ§Ã£o da projeÃ§Ã£o de $Y_3$ ao se adicionar $Y_2$ envolve a parte nÃ£o prevista de $Y_2$ dado $Y_1$, que Ã© precisamente o resÃ­duo $\tilde{Y_2}$ obtido pela fatoraÃ§Ã£o triangular. Esse resÃ­duo Ã© ortogonal a $Y_1$, o que simplifica a anÃ¡lise e garante a consistÃªncia das projeÃ§Ãµes.

**Lema 2.1** (Ortogonalidade dos Erros de ProjeÃ§Ã£o) Os erros de projeÃ§Ã£o $\tilde{Y_i}$ e $\tilde{Y_j}$ sÃ£o ortogonais para $i \neq j$, ou seja, $E(\tilde{Y_i}\tilde{Y_j}) = 0$.

*Prova*:
I. Pelo Teorema 4, os erros de projeÃ§Ã£o sÃ£o as variÃ¡veis transformadas $\tilde{Y}_i$.
II. A matriz de covariÃ¢ncia das variÃ¡veis transformadas Ã© $E(\tilde{Y}\tilde{Y}') = D$, onde $D$ Ã© uma matriz diagonal.
III. Como $D$ Ã© diagonal, os elementos fora da diagonal sÃ£o zero, ou seja, $E(\tilde{Y_i}\tilde{Y_j}) = 0$ para $i \neq j$.
IV. Portanto, os erros de projeÃ§Ã£o $\tilde{Y_i}$ e $\tilde{Y_j}$ sÃ£o ortogonais quando $i \neq j$.
â– 

> ðŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar a ortogonalidade dos erros de projeÃ§Ã£o, vamos simular um conjunto de dados para as variÃ¡veis $Y_1, Y_2$ e $Y_3$, com a mesma matriz de covariÃ¢ncia $\Omega$ utilizada anteriormente.
>
> ```python
> import numpy as np
>
> # Matriz de covariÃ¢ncia
> omega = np.array([[1, 0.5, 0.3],
>                   [0.5, 1, 0.6],
>                   [0.3, 0.6, 1]])
>
> # FatoraÃ§Ã£o triangular
> A = np.array([[1, 0, 0],
>               [0.5, 1, 0],
>               [0.3, 0.47, 1]])
>
> A_inv = np.linalg.inv(A)
>
> # SimulaÃ§Ã£o de dados
> np.random.seed(42)
> num_samples = 1000
> Y = np.random.multivariate_normal(mean=[0, 0, 0], cov=omega, size=num_samples)
>
> # TransformaÃ§Ã£o para as variÃ¡veis nÃ£o correlacionadas
> Y_tilde = np.dot(Y, A_inv.T)
>
> # CÃ¡lculo das covariÃ¢ncias
> cov_Y1_Y2 = np.cov(Y_tilde[:, 0], Y_tilde[:, 1])[0, 1]
> cov_Y1_Y3 = np.cov(Y_tilde[:, 0], Y_tilde[:, 2])[0, 1]
> cov_Y2_Y3 = np.cov(Y_tilde[:, 1], Y_tilde[:, 2])[0, 1]
>
> print(f"Cov(Y_tilde_1, Y_tilde_2): {cov_Y1_Y2:.4f}")
> print(f"Cov(Y_tilde_1, Y_tilde_3): {cov_Y1_Y3:.4f}")
> print(f"Cov(Y_tilde_2, Y_tilde_3): {cov_Y2_Y3:.4f}")
> ```
>
> A saÃ­da do cÃ³digo acima demonstra que as covariÃ¢ncias entre os erros de projeÃ§Ã£o $\tilde{Y_i}$ e $\tilde{Y_j}$ sÃ£o prÃ³ximas de zero, comprovando a ortogonalidade.

### ConclusÃ£o
O **erro de projeÃ§Ã£o linear**, quando analisado no contexto da **fatoraÃ§Ã£o triangular**, pode ser expresso como uma combinaÃ§Ã£o linear de variÃ¡veis transformadas, cuja matriz de covariÃ¢ncia Ã© diagonal. Isso implica que os erros de projeÃ§Ã£o sequenciais (as variÃ¡veis transformadas) sÃ£o ortogonais entre si. Esta propriedade simplifica a anÃ¡lise de projeÃ§Ãµes lineares e permite uma compreensÃ£o mais clara de como a informaÃ§Ã£o se propaga entre diferentes variÃ¡veis.
A transformaÃ§Ã£o linear atravÃ©s da fatoraÃ§Ã£o triangular, permite expressar os dados em termos de variÃ¡veis nÃ£o correlacionadas, cujas variÃ¢ncias sÃ£o os elementos da matriz diagonal $D$. Em resumo, a combinaÃ§Ã£o da lei das projeÃ§Ãµes iteradas com a fatoraÃ§Ã£o triangular oferece um poderoso arcabouÃ§o teÃ³rico e prÃ¡tico para o desenvolvimento de modelos preditivos em sÃ©ries temporais, com uma forte base matemÃ¡tica, possibilitando simplificar o problema de previsÃ£o complexa.

**ProposiÃ§Ã£o 5** (ConexÃ£o com Modelos de SÃ©ries Temporais) A decomposiÃ§Ã£o de uma sÃ©rie temporal em seus resÃ­duos por meio da fatoraÃ§Ã£o triangular Ã© anÃ¡loga Ã  decomposiÃ§Ã£o de um processo AR em seus choques inovacionais.

*Justificativa:*
I. A fatoraÃ§Ã£o triangular transforma as variÃ¡veis originais $Y_i$ em um conjunto de resÃ­duos $\tilde{Y_i}$ que sÃ£o mutuamente nÃ£o correlacionados.
II. Em um modelo AR, os choques inovacionais (ruÃ­do branco) sÃ£o tambÃ©m mutuamente nÃ£o correlacionados.
III. A transformaÃ§Ã£o $\tilde{Y}=A^{-1}Y$ Ã©, em essÃªncia, uma representaÃ§Ã£o do processo $Y$ em termos de seus resÃ­duos de projeÃ§Ã£o.
IV. Portanto, o processo de fatoraÃ§Ã£o triangular de uma sÃ©rie temporal Ã© similar ao processo de decomposiÃ§Ã£o em choques inovacionais em modelos AR, destacando a importÃ¢ncia da fatoraÃ§Ã£o triangular na anÃ¡lise de sÃ©ries temporais.
> ðŸ’¡ **Exemplo NumÃ©rico:** Considere um processo AR(1) simples:
> $$Y_t = 0.8Y_{t-1} + \epsilon_t$$
> onde $\epsilon_t$ sÃ£o choques inovacionais (ruÃ­do branco). A fatoraÃ§Ã£o triangular pode ser vista como o processo de encontrar os resÃ­duos $\epsilon_t$ a partir da sÃ©rie $Y_t$. Em um modelo AR(1), o resÃ­duo no tempo t Ã© o erro de previsÃ£o linear usando as informaÃ§Ãµes de $Y_{t-1}$. A fatoraÃ§Ã£o triangular nos permite transformar as variÃ¡veis originais em um conjunto de resÃ­duos nÃ£o correlacionados que sÃ£o essencialmente esses choques inovacionais.
>
> Usando a transformaÃ§Ã£o $\tilde{Y} = A^{-1}Y$ na fatoraÃ§Ã£o triangular, podemos obter os $\tilde{Y_t}$ que correspondem aos choques inovacionais $\epsilon_t$. Esta conexÃ£o Ã© fundamental para entender como a fatoraÃ§Ã£o triangular se relaciona com modelos de sÃ©ries temporais como o AR, onde a modelagem dos resÃ­duos Ã© crucial para a previsÃ£o. A matriz de covariÃ¢ncia dos $\tilde{Y_t}$ serÃ¡ diagonal, demonstrando a ausÃªncia de correlaÃ§Ã£o entre os resÃ­duos, da mesma forma que os choques inovacionais em modelos AR.

### ReferÃªncias
[^4.1.2]: Expression [4.1.1] is known as the mean squared error associated with the forecast ... The forecast with the smallest mean squared error turns out to be the ex- pectation of $Y_{t+1}$ conditional on $X_t$: $Y_{t+1}$ = $E(Y_{t+1}|X_t)$.
[^4.1.9]: We now restrict the class of forecasts considered by requiring the forecast $Y_{t+1}$ to be a linear function of $X_t$: $Y^*_{t+1}$ = $\alpha'X_t$.
[^4.4.1]: Any positive definite symmetric (n Ã— n) matrix $\Omega$ has a unique representation of the form $\Omega = ADA'$, where $A$ is a lower triangular matrix with 1s along the principal diagonal...
[^4.5.2]: Let $\Omega = ADA'$ be the triangular factorization of $\Omega$, and define $\tilde{Y}$ = $A^{-1}Y$.
[^4.5.4]: Substituting [4.5.1] into [4.5.3], the second-moment matrix of $\tilde{Y}$ is seen to be diagonal: $E(\tilde{Y}\tilde{Y}')$ = $A^{-1}\Omega[A']^{-1} = A^{-1}ADA'[A']^{-1} = D$.
[^4.5.5]: That is, $E(\tilde{Y_i}\tilde{Y_j}) = \begin{cases} d_{ii} & \text{for } i=j \\ 0 & \text{for } i \neq j \end{cases}$.
[^4.5.16]: $P(Y_3|Y_2,Y_1) = P(Y_3|Y_1) + \{E[Y_3 - P(Y_3|Y_1)][Y_2 - P(Y_2|Y_1)]\} \{E[Y_2 - P(Y_2|Y_1)]^2\}^{-1}[Y_2 - P(Y_2|Y_1)]$.
5.2. Likelihood Function for an AR(1) Process

Consider an AR(1) model:
$$Y_t = c + \phi Y_{t-1} + \epsilon_t$$ [^5.2.1]
where $\epsilon_t$ is i.i.d. $N(0, \sigma^2)$. We wish to evaluate the likelihood of the parameters $\theta = (c, \phi, \sigma^2)$ conditional on a particular set of observed data $Y_1, Y_2, ..., Y_T$. The joint density of $\epsilon_1, \epsilon_2, ..., \epsilon_T$ is given by:
$$f_{\epsilon_1, ..., \epsilon_T}(\epsilon_1, ..., \epsilon_T; \sigma^2) = \prod_{t=1}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{\epsilon_t^2}{2\sigma^2} \right)$$ [^5.2.2]
The likelihood function, as a function of the parameters given the data, is the same as the joint density function of the data considered as a function of the parameters,
$$L(\theta; Y_1, ..., Y_T) = f_{Y_1, ..., Y_T}(Y_1, ..., Y_T; \theta)$$ [^5.2.3]
To evaluate this, we note that the joint distribution of $Y_1, ..., Y_T$ can be decomposed as:
$$f_{Y_1, ..., Y_T}(Y_1, ..., Y_T; \theta) = f_{Y_1}(Y_1; \theta) \prod_{t=2}^T f_{Y_t|Y_{t-1},...,Y_1}(Y_t|Y_{t-1},...,Y_1; \theta)$$ [^5.2.4]
Given the model, the conditional distribution of $Y_t$ given all past $Y$ is identical to the conditional distribution given only the most recent past value $Y_{t-1}$. Thus,
$$f_{Y_t|Y_{t-1},...,Y_1}(Y_t|Y_{t-1},...,Y_1; \theta) = f_{Y_t|Y_{t-1}}(Y_t|Y_{t-1}; \theta)$$ [^5.2.5]
Furthermore, we know that the conditional distribution of $Y_t$ given $Y_{t-1}$ is normal, since:
$$Y_t | Y_{t-1} \sim N(c + \phi Y_{t-1}, \sigma^2)$$ [^5.2.6]
The corresponding density function is
$$f_{Y_t|Y_{t-1}}(Y_t|Y_{t-1}; \theta) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(Y_t - c - \phi Y_{t-1})^2}{2\sigma^2} \right)$$ [^5.2.7]
Substituting [^5.2.7] in [^5.2.4] we obtain the likelihood as
$$L(\theta; Y_1, ..., Y_T) = f_{Y_1}(Y_1; \theta) \prod_{t=2}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(Y_t - c - \phi Y_{t-1})^2}{2\sigma^2} \right)$$ [^5.2.8]
This likelihood can be further simplified by noting that the marginal distribution of $Y_1$ is implied by the AR(1) model. When the process is stationary, the unconditional mean and variance of $Y_t$ is given by
$$E(Y_t) = \frac{c}{1-\phi}$$ [^5.2.9]
$$Var(Y_t) = \frac{\sigma^2}{1-\phi^2}$$ [^5.2.10]
If we assume stationarity, the distribution of $Y_1$ is given by
$$Y_1 \sim N\left(\frac{c}{1-\phi}, \frac{\sigma^2}{1-\phi^2} \right)$$ [^5.2.11]
The corresponding density function is
$$f_{Y_1}(Y_1; \theta) = \frac{1}{\sqrt{2\pi \frac{\sigma^2}{1-\phi^2}}} \exp\left( -\frac{(Y_1 - \frac{c}{1-\phi})^2}{2\frac{\sigma^2}{1-\phi^2}} \right)$$ [^5.2.12]
Using [^5.2.12] to substitute into [^5.2.8], the full likelihood for the AR(1) model can be expressed as:
$$L(\theta; Y_1, ..., Y_T) = \frac{1}{\sqrt{2\pi \frac{\sigma^2}{1-\phi^2}}} \exp\left( -\frac{(Y_1 - \frac{c}{1-\phi})^2}{2\frac{\sigma^2}{1-\phi^2}} \right) \prod_{t=2}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(Y_t - c - \phi Y_{t-1})^2}{2\sigma^2} \right)$$ [^5.2.13]
It is often easier to maximize the logarithm of the likelihood, which is given by:
$$logL(\theta; Y_1, ..., Y_T) = -\frac{1}{2}log(2\pi) -\frac{1}{2}log(\frac{\sigma^2}{1-\phi^2}) - \frac{(Y_1 - \frac{c}{1-\phi})^2}{2\frac{\sigma^2}{1-\phi^2}} - \frac{T-1}{2}log(2\pi) - \frac{T-1}{2}log(\sigma^2) - \sum_{t=2}^T \frac{(Y_t - c - \phi Y_{t-1})^2}{2\sigma^2}$$ [^5.2.14]
This can be simplified to
$$logL(\theta; Y_1, ..., Y_T) = -\frac{T}{2}log(2\pi) -\frac{T}{2}log(\sigma^2) + \frac{1}{2}log(1-\phi^2) - \frac{(Y_1 - \frac{c}{1-\phi})^2(1-\phi^2)}{2\sigma^2} - \sum_{t=2}^T \frac{(Y_t - c - \phi Y_{t-1})^2}{2\sigma^2}$$ [^5.2.15]
The first three terms are constant, except for the parameters themselves. The last term is related to the sum of squared residuals.

> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos considerar um exemplo prÃ¡tico para calcular a verossimilhanÃ§a de um modelo AR(1). Suponha que temos os seguintes dados da sÃ©rie temporal $Y_t$: [1.2, 1.5, 1.8, 2.0, 2.2]. Vamos assumir que $c=0.1$, $\phi=0.7$, e $\sigma^2=0.1$.
>
> Primeiro, calculamos os termos para $t \ge 2$. Os erros $\epsilon_t$ sÃ£o obtidos pela equaÃ§Ã£o $\epsilon_t = Y_t - c - \phi Y_{t-1}$.
> - $\epsilon_2 = 1.5 - 0.1 - 0.7 * 1.2 = 0.56$
> - $\epsilon_3 = 1.8 - 0.1 - 0.7 * 1.5 = 0.65$
> - $\epsilon_4 = 2.0 - 0.1 - 0.7 * 1.8 = 0.64$
> - $\epsilon_5 = 2.2 - 0.1 - 0.7 * 2.0 = 0.70$
>
> O primeiro termo da verossimilhanÃ§a requer o cÃ¡lculo do valor esperado e variÃ¢ncia de $Y_1$.
>
> $E(Y_t) = \frac{c}{1-\phi} = \frac{0.1}{1-0.7} = 0.333$
> $Var(Y_t) = \frac{\sigma^2}{1-\phi^2} = \frac{0.1}{1-0.7^2} = 0.196$
>
> Agora, calculamos a funÃ§Ã£o de verossimilhanÃ§a:
>
> $$logL(\theta; Y_1, ..., Y_T) = -\frac{T}{2}log(2\pi) -\frac{T}{2}log(\sigma^2) + \frac{1}{2}log(1-\phi^2) - \frac{(Y_1 - \frac{c}{1-\phi})^2(1-\phi^2)}{2\sigma^2} - \sum_{t=2}^T \frac{(Y_t - c - \phi Y_{t-1})^2}{2\sigma^2}$$
>
> $$logL(\theta; Y_1, ..., Y_T) = -\frac{5}{2}log(2\pi) -\frac{5}{2}log(0.1) + \frac{1}{2}log(1-0.7^2) - \frac{(1.2 - 0.333)^2(1-0.7^2)}{2*0.1} - \frac{0.56^2 + 0.65^2 + 0.64^2 + 0.7^2}{2*0.1}$$
>
> $$logL(\theta; Y_1, ..., Y_T) \approx -5.34 - (-5.75) + (-0.22) - 2.75 - 10.56 \approx -13.12$$
>
> O resultado Ã© o log da verossimilhanÃ§a para os parÃ¢metros e dados fornecidos. Este valor Ã© usado em algoritmos de otimizaÃ§Ã£o para encontrar os melhores valores de $c$, $\phi$, e $\sigma^2$ que maximizam a verossimilhanÃ§a.

5.3. Likelihood Function for an MA(1) Process

Now consider an MA(1) model:
$$Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$$ [^5.3.1]
where $\epsilon_t$ is i.i.d. $N(0, \sigma^2)$. We wish to evaluate the likelihood of the parameters $\theta = (\mu, \theta, \sigma^2)$ conditional on a particular set of observed data $Y_1, Y_2, ..., Y_T$. The basic idea is the same as in the previous section, however we need to treat the unobserved $\epsilon_t$ as latent variables.
It is important to note that in this context, the value of $\epsilon_1$ needs to be initialized using some method, and the initialization choice will affect the estimate of the likelihood function. A commonly used approach, also known as unconditional or approximate likelihood, is to use $\epsilon_0=0$ as an initialization. With this initialization, and with the parameter values specified in $\theta$, we can reconstruct the complete sequence of latent variables $\epsilon_1, \epsilon_2,..., \epsilon_T$ by the following recursive equation:
$$\epsilon_t = Y_t - \mu - \theta \epsilon_{t-1}$$ [^5.3.2]
Using the i.i.d. assumption, the joint probability density of the latent variables can be expressed as
$$f_{\epsilon_1, ..., \epsilon_T}(\epsilon_1, ..., \epsilon_T; \sigma^2) = \prod_{t=1}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{\epsilon_t^2}{2\sigma^2} \right)$$ [^5.3.3]
As this function depends on the $\epsilon$ values which are functions of the parameters and data,it is possible to find the parameters that maximize the likelihood function. This is equivalent to minimizing the negative log-likelihood function,which is often easier to work with numerically.

### Optimization Techniques

Several optimization techniques can be employed to find the parameters that minimize the negative log-likelihood. Some of the most common methods include:

*   **Gradient Descent:** This is an iterative optimization algorithm that starts with an initial guess for the parameters and updates them in the direction of the negative gradient of the negative log-likelihood function. The update rule is:
    $$\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t)$$
    where $\theta_t$ represents the parameters at iteration $t$, $\eta$ is the learning rate, and $\nabla L(\theta_t)$ is the gradient of the negative log-likelihood function with respect to the parameters.

*   **Stochastic Gradient Descent (SGD):**  SGD is a variant of gradient descent that updates parameters using the gradient computed on a small batch of data rather than the entire dataset. This can significantly speed up the optimization process, especially for large datasets.

*   **Adam:**  Adam is an adaptive learning rate optimization algorithm that combines the advantages of both momentum and RMSprop. It adapts the learning rate for each parameter based on its historical gradients.

*   **L-BFGS:**  L-BFGS is a quasi-Newton optimization algorithm that uses an approximation of the Hessian matrix to find the minimum of the function. It is generally more efficient than gradient descent for problems with a moderate number of parameters.

### Example: Linear Regression

Consider a linear regression model with a single feature:

$$y_i = \theta_0 + \theta_1 x_i + \epsilon_i$$

where $y_i$ is the dependent variable, $x_i$ is the independent variable, $\theta_0$ and $\theta_1$ are the parameters, and $\epsilon_i$ is the error term, assumed to be normally distributed with zero mean and a variance of $\sigma^2$.

The likelihood function for this model can be expressed as:
$$L(\theta_0, \theta_1, \sigma^2 | X, Y) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - (\theta_0 + \theta_1 x_i))^2}{2\sigma^2}\right)$$

The negative log-likelihood is then:

$$-\log L = \frac{n}{2}\log(2\pi\sigma^2) + \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \theta_0 - \theta_1 x_i)^2$$

Minimizing this expression with respect to $\theta_0$ and $\theta_1$ is equivalent to minimizing the sum of squared errors, which is a classic result in linear regression.

### Model Evaluation

After fitting a model using maximum likelihood estimation, it is crucial to evaluate its performance. Some common evaluation metrics include:

*   **Log-Likelihood:** The value of the log-likelihood function for the fitted parameters, a higher log-likelihood indicates a better fit.
*   **AIC and BIC:** Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are metrics that balance the goodness of fit with the model complexity. They are defined as:
    $$AIC = -2\log L + 2k$$
    $$BIC = -2\log L + k\log n$$
    where $k$ is the number of parameters and $n$ is the number of data points.  Lower values of AIC and BIC are generally preferred.
*   **Cross-Validation:** Using techniques such as k-fold cross-validation to estimate how well the model will generalize to unseen data.
*   **Residual Analysis:** Examining the residuals (the difference between the predicted values and the observed values) to check if the assumptions of the model are met.

### Summary

Maximum likelihood estimation is a powerful and widely used method for estimating parameters in statistical models. It involves finding the parameters that maximize the likelihood of observing the given data. The process involves constructing the likelihood function, often converted to negative log-likelihood for numerical optimization using techniques like gradient descent and its variants, L-BFGS, and others. Once parameters are obtained, model evaluation and validation using metrics like AIC, BIC, and cross-validation are crucial to ensure the model's quality.

<!-- END -->
