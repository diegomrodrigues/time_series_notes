## O Erro de Proje√ß√£o Linear e sua Rela√ß√£o com a Fatora√ß√£o Triangular

### Introdu√ß√£o
Neste cap√≠tulo, vamos explorar como o **erro de proje√ß√£o linear** pode ser expresso utilizando as vari√°veis transformadas atrav√©s da **fatora√ß√£o triangular**, revelando a natureza da distribui√ß√£o desses erros. Como vimos anteriormente, a fatora√ß√£o triangular nos permite decompor uma matriz de segundos momentos em componentes que facilitam a an√°lise de proje√ß√µes lineares e a gera√ß√£o de vari√°veis n√£o correlacionadas. [^4.5.4]. Agora, vamos examinar como o erro de proje√ß√£o, que √© a diferen√ßa entre o valor real e a previs√£o linear, se relaciona com as vari√°veis transformadas e como a matriz de covari√¢ncia diagonal dessas vari√°veis simplifica essa an√°lise.

### Conceitos Fundamentais

O **erro de proje√ß√£o linear** √© definido como a diferen√ßa entre o valor observado de uma vari√°vel e sua previs√£o obtida por meio de uma proje√ß√£o linear. Seja $Y_i$ uma vari√°vel aleat√≥ria e $\hat{Y_i}$ a sua previs√£o linear baseada em outras vari√°veis, o erro de proje√ß√£o $\epsilon_i$ √© dado por:
$$ \epsilon_i = Y_i - \hat{Y_i} $$
onde $\hat{Y_i}$ √© a proje√ß√£o linear de $Y_i$ em um conjunto de outras vari√°veis. [^4.1.2], [^4.1.9]. A an√°lise do erro de proje√ß√£o √© crucial em estat√≠stica e econometria, pois ele revela a qualidade da previs√£o. A **fatora√ß√£o triangular**, como demonstrado, nos fornece uma forma de transformar vari√°veis originais em vari√°veis n√£o correlacionadas $\tilde{Y}$, onde $\tilde{Y} = A^{-1}Y$. [^4.5.2]. Essa transforma√ß√£o, crucial para o entendimento das proje√ß√µes lineares, permite reescrever o erro de proje√ß√£o em termos dessas novas vari√°veis.

> üí° **Exemplo Ilustrativo:** Suponha que estamos tentando prever a vari√°vel $Y_3$ usando $Y_1$ e $Y_2$. A proje√ß√£o linear nos fornece uma previs√£o $\hat{Y_3}$ que √© uma combina√ß√£o linear de $Y_1$ e $Y_2$. O erro de proje√ß√£o $\epsilon_3$ √© a diferen√ßa entre o valor real de $Y_3$ e a previs√£o $\hat{Y_3}$. O objetivo da an√°lise do erro √© entender sua natureza e como ele se relaciona com as vari√°veis usadas para gerar a previs√£o.
> Se o erro estiver correlacionado com as vari√°veis usadas na proje√ß√£o, isso indica que a proje√ß√£o linear n√£o captura toda a informa√ß√£o relevante do sistema.

A fatora√ß√£o triangular e a transforma√ß√£o $\tilde{Y} = A^{-1}Y$ desempenham um papel essencial nesta an√°lise, pois transformam o problema de analisar erros correlacionados em um problema de analisar erros n√£o correlacionados.

**Teorema 4** (O Erro de Proje√ß√£o Linear e as Vari√°veis Transformadas)
O erro de proje√ß√£o linear para cada vari√°vel $Y_i$ pode ser expresso como uma combina√ß√£o linear das vari√°veis transformadas $\tilde{Y}_j$, onde $j \geq i$, e que a matriz de covari√¢ncia do vetor de erros tem a mesma forma diagonal $D$ da matriz de covari√¢ncia das vari√°veis transformadas.
*Prova:*
I. A fatora√ß√£o triangular nos d√° $\Omega = ADA'$, onde $\Omega$ √© a matriz de covari√¢ncia do vetor $Y$. [^4.4.1].
II. As vari√°veis transformadas s√£o $\tilde{Y} = A^{-1}Y$, e t√™m matriz de covari√¢ncia $E(\tilde{Y}\tilde{Y}') = D$, que √© diagonal. [^4.5.4].
III. A rela√ß√£o entre $Y$ e $\tilde{Y}$ √© dada por $Y = A\tilde{Y}$. [^4.5.6].
IV. Seja $\epsilon$ o vetor de erros de proje√ß√£o linear de cada vari√°vel $Y_i$ sobre as vari√°veis precedentes $Y_1, \ldots, Y_{i-1}$.
V. Pela lei das proje√ß√µes iteradas, o erro de proje√ß√£o de $Y_i$ sobre as vari√°veis anteriores √© o mesmo que o i-√©simo elemento da transforma√ß√£o $A^{-1}Y$, ou seja, o i-√©simo elemento de $\tilde{Y}$.
VI. Podemos escrever o vetor de erros $\epsilon = \tilde{Y}$
VII. Como $E(\tilde{Y}\tilde{Y}') = D$, a matriz de covari√¢ncia do vetor de erros √© $E(\epsilon\epsilon')=D$. Isso prova que os erros de proje√ß√£o s√£o n√£o correlacionados e t√™m vari√¢ncias dadas pelos elementos diagonais de $D$.
VIII. O erro de proje√ß√£o para cada vari√°vel $Y_i$ pode ser expressa como uma combina√ß√£o linear de vari√°veis transformadas: $\epsilon_i = \tilde{Y_i}$.
‚ñ†

**Teorema 4.1** (Rela√ß√£o entre a Matriz de Covari√¢ncia dos Erros de Proje√ß√£o e a Matriz D)
A matriz de covari√¢ncia dos erros de proje√ß√£o $\epsilon$, denotada por $E(\epsilon \epsilon')$, √© igual √† matriz diagonal $D$ obtida na fatora√ß√£o triangular da matriz de covari√¢ncia $\Omega$.
*Prova:*
I. Do Teorema 4, temos que $\epsilon = \tilde{Y}$.
II. A matriz de covari√¢ncia de $\tilde{Y}$ √© dada por $E(\tilde{Y} \tilde{Y}') = D$ (pela propriedade da fatora√ß√£o triangular).
III. Substituindo $\epsilon$ por $\tilde{Y}$, obtemos $E(\epsilon \epsilon') = E(\tilde{Y} \tilde{Y}') = D$.
IV. Portanto, a matriz de covari√¢ncia dos erros de proje√ß√£o √© igual √† matriz diagonal $D$.
‚ñ†

Este teorema demonstra que o erro de proje√ß√£o linear para cada vari√°vel $Y_i$ pode ser expressa como uma vari√°vel $\tilde{Y_i}$ e, portanto, a matriz de covari√¢ncia dos erros de proje√ß√£o √© diagonal, cujos elementos diagonais s√£o os erros quadr√°ticos m√©dios das proje√ß√µes. Ou seja, os elementos diagonais da matriz D representam o erro quadr√°tico m√©dio associado a cada res√≠duo obtido pelas proje√ß√µes lineares sequenciais, como observado na proposi√ß√£o 1.1 do cap√≠tulo anterior.

> üí° **Exemplo Num√©rico:** Retomando o exemplo de proje√ß√£o de $Y_3$ usando $Y_1$ e $Y_2$, vimos que o erro da proje√ß√£o de $Y_3$ sobre $Y_1$ e $Y_2$ √© dado por:
>$$
\tilde{Y_3} = Y_3 - P(Y_3|Y_2,Y_1)
$$
>Da mesma forma, $\tilde{Y_2} = Y_2 - P(Y_2|Y_1)$. Note que $\tilde{Y_1} = Y_1$.
>A transforma√ß√£o $\tilde{Y} = A^{-1}Y$, com a matriz $A$ obtida pela fatora√ß√£o triangular, gera as vari√°veis n√£o correlacionadas $\tilde{Y_i}$. O erro de proje√ß√£o linear de $Y_i$ equivale a $\tilde{Y_i}$.
> Suponha que temos tr√™s vari√°veis $Y_1, Y_2, Y_3$ com a seguinte matriz de covari√¢ncia $\Omega$:
>
> $$\Omega = \begin{bmatrix} 1 & 0.5 & 0.3 \\ 0.5 & 1 & 0.6 \\ 0.3 & 0.6 & 1 \end{bmatrix}$$
>
> A fatora√ß√£o triangular $\Omega = ADA'$ nos d√°:
>
> $$A = \begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.3 & 0.47 & 1 \end{bmatrix}$$
>
> $$D = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0.75 & 0 \\ 0 & 0 & 0.33 \end{bmatrix}$$
>
> Aqui, $d_{11} = 1$, $d_{22} = 0.75$, e $d_{33} = 0.33$. Observe que $d_{11}$ √© a vari√¢ncia de $Y_1$, $d_{22}$ √© o erro quadr√°tico m√©dio da proje√ß√£o de $Y_2$ sobre $Y_1$ e $d_{33}$ √© o erro quadr√°tico m√©dio da proje√ß√£o de $Y_3$ sobre $Y_1$ e $Y_2$. As vari√°veis transformadas $\tilde{Y}$ s√£o obtidas como $\tilde{Y} = A^{-1}Y$ e os erros de proje√ß√£o s√£o iguais a $\tilde{Y}$. Portanto, $\tilde{Y_1}$ √© o erro de proje√ß√£o para $Y_1$ (que √© apenas $Y_1$ pois n√£o h√° proje√ß√£o), $\tilde{Y_2}$ √© o erro de proje√ß√£o para $Y_2$ dado $Y_1$ e $\tilde{Y_3}$ √© o erro de proje√ß√£o para $Y_3$ dado $Y_1$ e $Y_2$.

**Lema 2** (Rela√ß√£o entre o Erro de Proje√ß√£o Linear e as Vari√°veis Transformadas) O erro de proje√ß√£o linear de cada vari√°vel $Y_i$ sobre as vari√°veis anteriores $Y_1, ..., Y_{i-1}$ corresponde ao i-√©simo elemento da transforma√ß√£o $A^{-1}Y$.

*Prova*:
I. Definimos $\tilde{Y} = A^{-1}Y$.
II. Expandindo esta equa√ß√£o:
$$
\begin{bmatrix} \tilde{Y_1} \\ \tilde{Y_2} \\ \tilde{Y_3} \\ \vdots \\ \tilde{Y_n} \end{bmatrix} =
A^{-1} \begin{bmatrix} Y_1 \\ Y_2 \\ Y_3 \\ \vdots \\ Y_n \end{bmatrix}
$$
III. Explicitando a transforma√ß√£o do exemplo anterior
$$\tilde{Y} = \begin{bmatrix}
    1 & 0 & 0 & \cdots & 0 \\
    -\Omega_{21}\Omega_{11}^{-1} & 1 & 0 & \cdots & 0 \\
    \cdots & \cdots & \cdots & \ddots & \vdots \\
    \cdots & \cdots & \cdots & \cdots & 1
    \end{bmatrix}
    \begin{bmatrix}
        Y_1 \\
        Y_2 \\
        Y_3 \\
        \vdots \\
        Y_n
    \end{bmatrix}
$$
IV. O i-√©simo elemento de $\tilde{Y}$ √© o resultado da proje√ß√£o de $Y_i$ sobre $Y_1$ a $Y_{i-1}$.
V. Portanto, $\tilde{Y_i}$ representa o erro de proje√ß√£o de $Y_i$ sobre as vari√°veis precedentes, e equivale a $\epsilon_i$.
‚ñ†

### Desenvolvimento
A rela√ß√£o entre o erro de proje√ß√£o linear e a fatora√ß√£o triangular se manifesta na estrutura da matriz $D$, resultante da transforma√ß√£o $\tilde{Y} = A^{-1}Y$. Como a matriz $D$ √© diagonal, os elementos $\tilde{Y}_i$ s√£o n√£o correlacionados. Ou seja, o erro de proje√ß√£o para cada vari√°vel, expresso como $\tilde{Y_i}$, n√£o est√° correlacionado com os erros de proje√ß√£o para as outras vari√°veis. Isto simplifica a an√°lise do erro, pois em vez de trabalhar com erros correlacionados em $Y$, temos agora um sistema de erros n√£o correlacionados em $\tilde{Y}$. [^4.5.5].
A propriedade diagonal da matriz $D$, como visto, significa que a covari√¢ncia entre quaisquer duas vari√°veis transformadas $\tilde{Y_i}$ e $\tilde{Y_j}$ (com $i \neq j$) √© zero, ou seja, $E(\tilde{Y_i}\tilde{Y_j}) = 0$. [^4.5.5]. Os elementos da diagonal de $D$ representam, como vimos, a vari√¢ncia de cada $\tilde{Y_i}$, que tamb√©m correspondem aos erros quadr√°ticos m√©dios das proje√ß√µes sequenciais, confirmando a proposi√ß√£o 1.1 do cap√≠tulo anterior.

> üí° **Exemplo Num√©rico:** Usando a matriz $\Omega$ do exemplo anterior, temos a matriz $D$ com elementos diagonais que s√£o os erros quadr√°ticos m√©dios da proje√ß√£o linear correspondente. Ou seja:
> - $d_{11}$ √© o MSE da proje√ß√£o de $Y_1$, que √© apenas a sua vari√¢ncia, j√° que n√£o h√° proje√ß√£o.
> - $d_{22}$ √© o MSE da proje√ß√£o de $Y_2$ sobre $Y_1$.
> - $d_{33}$ √© o MSE da proje√ß√£o de $Y_3$ sobre $Y_1$ e $Y_2$.
> E todos os elementos fora da diagonal de $D$ s√£o zero, o que significa que os res√≠duos s√£o n√£o correlacionados.
> Este exemplo ilustra como a fatora√ß√£o triangular leva a vari√°veis n√£o correlacionadas e como os elementos da diagonal de $D$ revelam o erro quadr√°tico m√©dio das proje√ß√µes sequenciais.
>
> Numericamente, usando a matriz $D$ do exemplo anterior, onde
> $$D = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0.75 & 0 \\ 0 & 0 & 0.33 \end{bmatrix}$$
> Temos que:
> - $d_{11} = 1$: O erro quadr√°tico m√©dio da proje√ß√£o de $Y_1$ √© igual √† sua vari√¢ncia, que √© 1.
> - $d_{22} = 0.75$: O erro quadr√°tico m√©dio da proje√ß√£o de $Y_2$ sobre $Y_1$ √© 0.75.
> - $d_{33} = 0.33$: O erro quadr√°tico m√©dio da proje√ß√£o de $Y_3$ sobre $Y_1$ e $Y_2$ √© 0.33.
>
> Como a matriz D √© diagonal, a covari√¢ncia entre $\tilde{Y_i}$ e $\tilde{Y_j}$ √© 0 para $i \neq j$. Por exemplo, $E(\tilde{Y_1}\tilde{Y_2}) = 0$ e $E(\tilde{Y_2}\tilde{Y_3}) = 0$, demonstrando que os res√≠duos s√£o n√£o correlacionados.

A rela√ß√£o entre a fatora√ß√£o triangular e os erros de proje√ß√£o se torna mais clara ao analisar o processo de atualiza√ß√£o de proje√ß√µes lineares. A equa√ß√£o [^4.5.16] mostra que a atualiza√ß√£o da proje√ß√£o de $Y_3$ ao se adicionar $Y_2$ envolve a parte n√£o prevista de $Y_2$ dado $Y_1$, que √© precisamente o res√≠duo $\tilde{Y_2}$ obtido pela fatora√ß√£o triangular. Esse res√≠duo √© ortogonal a $Y_1$, o que simplifica a an√°lise e garante a consist√™ncia das proje√ß√µes.

**Lema 2.1** (Ortogonalidade dos Erros de Proje√ß√£o) Os erros de proje√ß√£o $\tilde{Y_i}$ e $\tilde{Y_j}$ s√£o ortogonais para $i \neq j$, ou seja, $E(\tilde{Y_i}\tilde{Y_j}) = 0$.

*Prova*:
I. Pelo Teorema 4, os erros de proje√ß√£o s√£o as vari√°veis transformadas $\tilde{Y}_i$.
II. A matriz de covari√¢ncia das vari√°veis transformadas √© $E(\tilde{Y}\tilde{Y}') = D$, onde $D$ √© uma matriz diagonal.
III. Como $D$ √© diagonal, os elementos fora da diagonal s√£o zero, ou seja, $E(\tilde{Y_i}\tilde{Y_j}) = 0$ para $i \neq j$.
IV. Portanto, os erros de proje√ß√£o $\tilde{Y_i}$ e $\tilde{Y_j}$ s√£o ortogonais quando $i \neq j$.
‚ñ†

> üí° **Exemplo Num√©rico:** Para ilustrar a ortogonalidade dos erros de proje√ß√£o, vamos simular um conjunto de dados para as vari√°veis $Y_1, Y_2$ e $Y_3$, com a mesma matriz de covari√¢ncia $\Omega$ utilizada anteriormente.
>
> ```python
> import numpy as np
>
> # Matriz de covari√¢ncia
> omega = np.array([[1, 0.5, 0.3],
>                   [0.5, 1, 0.6],
>                   [0.3, 0.6, 1]])
>
> # Fatora√ß√£o triangular
> A = np.array([[1, 0, 0],
>               [0.5, 1, 0],
>               [0.3, 0.47, 1]])
>
> A_inv = np.linalg.inv(A)
>
> # Simula√ß√£o de dados
> np.random.seed(42)
> num_samples = 1000
> Y = np.random.multivariate_normal(mean=[0, 0, 0], cov=omega, size=num_samples)
>
> # Transforma√ß√£o para as vari√°veis n√£o correlacionadas
> Y_tilde = np.dot(Y, A_inv.T)
>
> # C√°lculo das covari√¢ncias
> cov_Y1_Y2 = np.cov(Y_tilde[:, 0], Y_tilde[:, 1])[0, 1]
> cov_Y1_Y3 = np.cov(Y_tilde[:, 0], Y_tilde[:, 2])[0, 1]
> cov_Y2_Y3 = np.cov(Y_tilde[:, 1], Y_tilde[:, 2])[0, 1]
>
> print(f"Cov(Y_tilde_1, Y_tilde_2): {cov_Y1_Y2:.4f}")
> print(f"Cov(Y_tilde_1, Y_tilde_3): {cov_Y1_Y3:.4f}")
> print(f"Cov(Y_tilde_2, Y_tilde_3): {cov_Y2_Y3:.4f}")
> ```
>
> A sa√≠da do c√≥digo acima demonstra que as covari√¢ncias entre os erros de proje√ß√£o $\tilde{Y_i}$ e $\tilde{Y_j}$ s√£o pr√≥ximas de zero, comprovando a ortogonalidade.

### Conclus√£o
O **erro de proje√ß√£o linear**, quando analisado no contexto da **fatora√ß√£o triangular**, pode ser expresso como uma combina√ß√£o linear de vari√°veis transformadas, cuja matriz de covari√¢ncia √© diagonal. Isso implica que os erros de proje√ß√£o sequenciais (as vari√°veis transformadas) s√£o ortogonais entre si. Esta propriedade simplifica a an√°lise de proje√ß√µes lineares e permite uma compreens√£o mais clara de como a informa√ß√£o se propaga entre diferentes vari√°veis.
A transforma√ß√£o linear atrav√©s da fatora√ß√£o triangular, permite expressar os dados em termos de vari√°veis n√£o correlacionadas, cujas vari√¢ncias s√£o os elementos da matriz diagonal $D$. Em resumo, a combina√ß√£o da lei das proje√ß√µes iteradas com a fatora√ß√£o triangular oferece um poderoso arcabou√ßo te√≥rico e pr√°tico para o desenvolvimento de modelos preditivos em s√©ries temporais, com uma forte base matem√°tica, possibilitando simplificar o problema de previs√£o complexa.

**Proposi√ß√£o 5** (Conex√£o com Modelos de S√©ries Temporais) A decomposi√ß√£o de uma s√©rie temporal em seus res√≠duos por meio da fatora√ß√£o triangular √© an√°loga √† decomposi√ß√£o de um processo AR em seus choques inovacionais.

*Justificativa:*
I. A fatora√ß√£o triangular transforma as vari√°veis originais $Y_i$ em um conjunto de res√≠duos $\tilde{Y_i}$ que s√£o mutuamente n√£o correlacionados.
II. Em um modelo AR, os choques inovacionais (ru√≠do branco) s√£o tamb√©m mutuamente n√£o correlacionados.
III. A transforma√ß√£o $\tilde{Y}=A^{-1}Y$ √©, em ess√™ncia, uma representa√ß√£o do processo $Y$ em termos de seus res√≠duos de proje√ß√£o.
IV. Portanto, o processo de fatora√ß√£o triangular de uma s√©rie temporal √© similar ao processo de decomposi√ß√£o em choques inovacionais em modelos AR, destacando a import√¢ncia da fatora√ß√£o triangular na an√°lise de s√©ries temporais.
> üí° **Exemplo Num√©rico:** Considere um processo AR(1) simples:
> $$Y_t = 0.8Y_{t-1} + \epsilon_t$$
> onde $\epsilon_t$ s√£o choques inovacionais (ru√≠do branco). A fatora√ß√£o triangular pode ser vista como o processo de encontrar os res√≠duos $\epsilon_t$ a partir da s√©rie $Y_t$. Em um modelo AR(1), o res√≠duo no tempo t √© o erro de previs√£o linear usando as informa√ß√µes de $Y_{t-1}$. A fatora√ß√£o triangular nos permite transformar as vari√°veis originais em um conjunto de res√≠duos n√£o correlacionados que s√£o essencialmente esses choques inovacionais.
>
> Usando a transforma√ß√£o $\tilde{Y} = A^{-1}Y$ na fatora√ß√£o triangular, podemos obter os $\tilde{Y_t}$ que correspondem aos choques inovacionais $\epsilon_t$. Esta conex√£o √© fundamental para entender como a fatora√ß√£o triangular se relaciona com modelos de s√©ries temporais como o AR, onde a modelagem dos res√≠duos √© crucial para a previs√£o. A matriz de covari√¢ncia dos $\tilde{Y_t}$ ser√° diagonal, demonstrando a aus√™ncia de correla√ß√£o entre os res√≠duos, da mesma forma que os choques inovacionais em modelos AR.

### Refer√™ncias
[^4.1.2]: Expression [4.1.1] is known as the mean squared error associated with the forecast ... The forecast with the smallest mean squared error turns out to be the ex- pectation of $Y_{t+1}$ conditional on $X_t$: $Y_{t+1}$ = $E(Y_{t+1}|X_t)$.
[^4.1.9]: We now restrict the class of forecasts considered by requiring the forecast $Y_{t+1}$ to be a linear function of $X_t$: $Y^*_{t+1}$ = $\alpha'X_t$.
[^4.4.1]: Any positive definite symmetric (n √ó n) matrix $\Omega$ has a unique representation of the form $\Omega = ADA'$, where $A$ is a lower triangular matrix with 1s along the principal diagonal...
[^4.5.2]: Let $\Omega = ADA'$ be the triangular factorization of $\Omega$, and define $\tilde{Y}$ = $A^{-1}Y$.
[^4.5.4]: Substituting [4.5.1] into [4.5.3], the second-moment matrix of $\tilde{Y}$ is seen to be diagonal: $E(\tilde{Y}\tilde{Y}')$ = $A^{-1}\Omega[A']^{-1} = A^{-1}ADA'[A']^{-1} = D$.
[^4.5.5]: That is, $E(\tilde{Y_i}\tilde{Y_j}) = \begin{cases} d_{ii} & \text{for } i=j \\ 0 & \text{for } i \neq j \end{cases}$.
[^4.5.16]: $P(Y_3|Y_2,Y_1) = P(Y_3|Y_1) + \{E[Y_3 - P(Y_3|Y_1)][Y_2 - P(Y_2|Y_1)]\} \{E[Y_2 - P(Y_2|Y_1)]^2\}^{-1}[Y_2 - P(Y_2|Y_1)]$.
5.2. Likelihood Function for an AR(1) Process

Consider an AR(1) model:
$$Y_t = c + \phi Y_{t-1} + \epsilon_t$$ [^5.2.1]
where $\epsilon_t$ is i.i.d. $N(0, \sigma^2)$. We wish to evaluate the likelihood of the parameters $\theta = (c, \phi, \sigma^2)$ conditional on a particular set of observed data $Y_1, Y_2, ..., Y_T$. The joint density of $\epsilon_1, \epsilon_2, ..., \epsilon_T$ is given by:
$$f_{\epsilon_1, ..., \epsilon_T}(\epsilon_1, ..., \epsilon_T; \sigma^2) = \prod_{t=1}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{\epsilon_t^2}{2\sigma^2} \right)$$ [^5.2.2]
The likelihood function, as a function of the parameters given the data, is the same as the joint density function of the data considered as a function of the parameters,
$$L(\theta; Y_1, ..., Y_T) = f_{Y_1, ..., Y_T}(Y_1, ..., Y_T; \theta)$$ [^5.2.3]
To evaluate this, we note that the joint distribution of $Y_1, ..., Y_T$ can be decomposed as:
$$f_{Y_1, ..., Y_T}(Y_1, ..., Y_T; \theta) = f_{Y_1}(Y_1; \theta) \prod_{t=2}^T f_{Y_t|Y_{t-1},...,Y_1}(Y_t|Y_{t-1},...,Y_1; \theta)$$ [^5.2.4]
Given the model, the conditional distribution of $Y_t$ given all past $Y$ is identical to the conditional distribution given only the most recent past value $Y_{t-1}$. Thus,
$$f_{Y_t|Y_{t-1},...,Y_1}(Y_t|Y_{t-1},...,Y_1; \theta) = f_{Y_t|Y_{t-1}}(Y_t|Y_{t-1}; \theta)$$ [^5.2.5]
Furthermore, we know that the conditional distribution of $Y_t$ given $Y_{t-1}$ is normal, since:
$$Y_t | Y_{t-1} \sim N(c + \phi Y_{t-1}, \sigma^2)$$ [^5.2.6]
The corresponding density function is
$$f_{Y_t|Y_{t-1}}(Y_t|Y_{t-1}; \theta) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(Y_t - c - \phi Y_{t-1})^2}{2\sigma^2} \right)$$ [^5.2.7]
Substituting [^5.2.7] in [^5.2.4] we obtain the likelihood as
$$L(\theta; Y_1, ..., Y_T) = f_{Y_1}(Y_1; \theta) \prod_{t=2}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(Y_t - c - \phi Y_{t-1})^2}{2\sigma^2} \right)$$ [^5.2.8]
This likelihood can be further simplified by noting that the marginal distribution of $Y_1$ is implied by the AR(1) model. When the process is stationary, the unconditional mean and variance of $Y_t$ is given by
$$E(Y_t) = \frac{c}{1-\phi}$$ [^5.2.9]
$$Var(Y_t) = \frac{\sigma^2}{1-\phi^2}$$ [^5.2.10]
If we assume stationarity, the distribution of $Y_1$ is given by
$$Y_1 \sim N\left(\frac{c}{1-\phi}, \frac{\sigma^2}{1-\phi^2} \right)$$ [^5.2.11]
The corresponding density function is
$$f_{Y_1}(Y_1; \theta) = \frac{1}{\sqrt{2\pi \frac{\sigma^2}{1-\phi^2}}} \exp\left( -\frac{(Y_1 - \frac{c}{1-\phi})^2}{2\frac{\sigma^2}{1-\phi^2}} \right)$$ [^5.2.12]
Using [^5.2.12] to substitute into [^5.2.8], the full likelihood for the AR(1) model can be expressed as:
$$L(\theta; Y_1, ..., Y_T) = \frac{1}{\sqrt{2\pi \frac{\sigma^2}{1-\phi^2}}} \exp\left( -\frac{(Y_1 - \frac{c}{1-\phi})^2}{2\frac{\sigma^2}{1-\phi^2}} \right) \prod_{t=2}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(Y_t - c - \phi Y_{t-1})^2}{2\sigma^2} \right)$$ [^5.2.13]
It is often easier to maximize the logarithm of the likelihood, which is given by:
$$logL(\theta; Y_1, ..., Y_T) = -\frac{1}{2}log(2\pi) -\frac{1}{2}log(\frac{\sigma^2}{1-\phi^2}) - \frac{(Y_1 - \frac{c}{1-\phi})^2}{2\frac{\sigma^2}{1-\phi^2}} - \frac{T-1}{2}log(2\pi) - \frac{T-1}{2}log(\sigma^2) - \sum_{t=2}^T \frac{(Y_t - c - \phi Y_{t-1})^2}{2\sigma^2}$$ [^5.2.14]
This can be simplified to
$$logL(\theta; Y_1, ..., Y_T) = -\frac{T}{2}log(2\pi) -\frac{T}{2}log(\sigma^2) + \frac{1}{2}log(1-\phi^2) - \frac{(Y_1 - \frac{c}{1-\phi})^2(1-\phi^2)}{2\sigma^2} - \sum_{t=2}^T \frac{(Y_t - c - \phi Y_{t-1})^2}{2\sigma^2}$$ [^5.2.15]
The first three terms are constant, except for the parameters themselves. The last term is related to the sum of squared residuals.

> üí° **Exemplo Num√©rico:** Vamos considerar um exemplo pr√°tico para calcular a verossimilhan√ßa de um modelo AR(1). Suponha que temos os seguintes dados da s√©rie temporal $Y_t$: [1.2, 1.5, 1.8, 2.0, 2.2]. Vamos assumir que $c=0.1$, $\phi=0.7$, e $\sigma^2=0.1$.
>
> Primeiro, calculamos os termos para $t \ge 2$. Os erros $\epsilon_t$ s√£o obtidos pela equa√ß√£o $\epsilon_t = Y_t - c - \phi Y_{t-1}$.
> - $\epsilon_2 = 1.5 - 0.1 - 0.7 * 1.2 = 0.56$
> - $\epsilon_3 = 1.8 - 0.1 - 0.7 * 1.5 = 0.65$
> - $\epsilon_4 = 2.0 - 0.1 - 0.7 * 1.8 = 0.64$
> - $\epsilon_5 = 2.2 - 0.1 - 0.7 * 2.0 = 0.70$
>
> O primeiro termo da verossimilhan√ßa requer o c√°lculo do valor esperado e vari√¢ncia de $Y_1$.
>
> $E(Y_t) = \frac{c}{1-\phi} = \frac{0.1}{1-0.7} = 0.333$
> $Var(Y_t) = \frac{\sigma^2}{1-\phi^2} = \frac{0.1}{1-0.7^2} = 0.196$
>
> Agora, calculamos a fun√ß√£o de verossimilhan√ßa:
>
> $$logL(\theta; Y_1, ..., Y_T) = -\frac{T}{2}log(2\pi) -\frac{T}{2}log(\sigma^2) + \frac{1}{2}log(1-\phi^2) - \frac{(Y_1 - \frac{c}{1-\phi})^2(1-\phi^2)}{2\sigma^2} - \sum_{t=2}^T \frac{(Y_t - c - \phi Y_{t-1})^2}{2\sigma^2}$$
>
> $$logL(\theta; Y_1, ..., Y_T) = -\frac{5}{2}log(2\pi) -\frac{5}{2}log(0.1) + \frac{1}{2}log(1-0.7^2) - \frac{(1.2 - 0.333)^2(1-0.7^2)}{2*0.1} - \frac{0.56^2 + 0.65^2 + 0.64^2 + 0.7^2}{2*0.1}$$
>
> $$logL(\theta; Y_1, ..., Y_T) \approx -5.34 - (-5.75) + (-0.22) - 2.75 - 10.56 \approx -13.12$$
>
> O resultado √© o log da verossimilhan√ßa para os par√¢metros e dados fornecidos. Este valor √© usado em algoritmos de otimiza√ß√£o para encontrar os melhores valores de $c$, $\phi$, e $\sigma^2$ que maximizam a verossimilhan√ßa.

5.3. Likelihood Function for an MA(1) Process

Now consider an MA(1) model:
$$Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$$ [^5.3.1]
where $\epsilon_t$ is i.i.d. $N(0, \sigma^2)$. We wish to evaluate the likelihood of the parameters $\theta = (\mu, \theta, \sigma^2)$ conditional on a particular set of observed data $Y_1, Y_2, ..., Y_T$. The basic idea is the same as in the previous section, however we need to treat the unobserved $\epsilon_t$ as latent variables.
It is important to note that in this context, the value of $\epsilon_1$ needs to be initialized using some method, and the initialization choice will affect the estimate of the likelihood function. A commonly used approach, also known as unconditional or approximate likelihood, is to use $\epsilon_0=0$ as an initialization. With this initialization, and with the parameter values specified in $\theta$, we can reconstruct the complete sequence of latent variables $\epsilon_1, \epsilon_2,..., \epsilon_T$ by the following recursive equation:
$$\epsilon_t = Y_t - \mu - \theta \epsilon_{t-1}$$ [^5.3.2]
Using the i.i.d. assumption, the joint probability density of the latent variables can be expressed as
$$f_{\epsilon_1, ..., \epsilon_T}(\epsilon_1, ..., \epsilon_T; \sigma^2) = \prod_{t=1}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{\epsilon_t^2}{2\sigma^2} \right)$$ [^5.3.3]
As this function depends on the $\epsilon$ values which are functions of the parameters and data,it is possible to find the parameters that maximize the likelihood function. This is equivalent to minimizing the negative log-likelihood function,which is often easier to work with numerically.

### Optimization Techniques

Several optimization techniques can be employed to find the parameters that minimize the negative log-likelihood. Some of the most common methods include:

*   **Gradient Descent:** This is an iterative optimization algorithm that starts with an initial guess for the parameters and updates them in the direction of the negative gradient of the negative log-likelihood function. The update rule is:
    $$\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t)$$
    where $\theta_t$ represents the parameters at iteration $t$, $\eta$ is the learning rate, and $\nabla L(\theta_t)$ is the gradient of the negative log-likelihood function with respect to the parameters.

*   **Stochastic Gradient Descent (SGD):**  SGD is a variant of gradient descent that updates parameters using the gradient computed on a small batch of data rather than the entire dataset. This can significantly speed up the optimization process, especially for large datasets.

*   **Adam:**  Adam is an adaptive learning rate optimization algorithm that combines the advantages of both momentum and RMSprop. It adapts the learning rate for each parameter based on its historical gradients.

*   **L-BFGS:**  L-BFGS is a quasi-Newton optimization algorithm that uses an approximation of the Hessian matrix to find the minimum of the function. It is generally more efficient than gradient descent for problems with a moderate number of parameters.

### Example: Linear Regression

Consider a linear regression model with a single feature:

$$y_i = \theta_0 + \theta_1 x_i + \epsilon_i$$

where $y_i$ is the dependent variable, $x_i$ is the independent variable, $\theta_0$ and $\theta_1$ are the parameters, and $\epsilon_i$ is the error term, assumed to be normally distributed with zero mean and a variance of $\sigma^2$.

The likelihood function for this model can be expressed as:
$$L(\theta_0, \theta_1, \sigma^2 | X, Y) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - (\theta_0 + \theta_1 x_i))^2}{2\sigma^2}\right)$$

The negative log-likelihood is then:

$$-\log L = \frac{n}{2}\log(2\pi\sigma^2) + \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \theta_0 - \theta_1 x_i)^2$$

Minimizing this expression with respect to $\theta_0$ and $\theta_1$ is equivalent to minimizing the sum of squared errors, which is a classic result in linear regression.

### Model Evaluation

After fitting a model using maximum likelihood estimation, it is crucial to evaluate its performance. Some common evaluation metrics include:

*   **Log-Likelihood:** The value of the log-likelihood function for the fitted parameters, a higher log-likelihood indicates a better fit.
*   **AIC and BIC:** Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are metrics that balance the goodness of fit with the model complexity. They are defined as:
    $$AIC = -2\log L + 2k$$
    $$BIC = -2\log L + k\log n$$
    where $k$ is the number of parameters and $n$ is the number of data points.  Lower values of AIC and BIC are generally preferred.
*   **Cross-Validation:** Using techniques such as k-fold cross-validation to estimate how well the model will generalize to unseen data.
*   **Residual Analysis:** Examining the residuals (the difference between the predicted values and the observed values) to check if the assumptions of the model are met.

### Summary

Maximum likelihood estimation is a powerful and widely used method for estimating parameters in statistical models. It involves finding the parameters that maximize the likelihood of observing the given data. The process involves constructing the likelihood function, often converted to negative log-likelihood for numerical optimization using techniques like gradient descent and its variants, L-BFGS, and others. Once parameters are obtained, model evaluation and validation using metrics like AIC, BIC, and cross-validation are crucial to ensure the model's quality.

<!-- END -->
