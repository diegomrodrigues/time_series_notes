## Previs√µes Otimizadas em Modelos MA(q): Converg√™ncia e MSE

### Introdu√ß√£o

Este cap√≠tulo aprofunda a an√°lise das **previs√µes otimizadas** para modelos **MA(q)**, com foco na converg√™ncia da previs√£o para a m√©dia incondicional e no comportamento do **Erro M√©dio Quadr√°tico de Previs√£o (MSE)** √† medida que o horizonte de previs√£o aumenta. Construindo sobre os conceitos de previs√µes lineares √≥timas e operadores de defasagem previamente introduzidos [^1], [^5], [^6], o objetivo deste cap√≠tulo √© investigar como as caracter√≠sticas finitas dos modelos MA(q) afetam o comportamento das previs√µes para diferentes horizontes, destacando a rela√ß√£o entre previs√µes lineares, a estrutura do modelo e os conceitos de converg√™ncia em s√©ries temporais.

### Conceitos Fundamentais

Como visto nos cap√≠tulos anteriores, a **previs√£o linear √≥tima** para um processo com representa√ß√£o **MA(‚àû)** √© constru√≠da como uma combina√ß√£o linear dos erros passados, onde os erros futuros s√£o substitu√≠dos por seus valores esperados, que s√£o zero. No contexto espec√≠fico de modelos **MA(q)**, este conceito assume uma forma particular devido √† natureza finita da depend√™ncia dos erros passados.

Consideremos um processo **MA(q)** definido como:
$$ (Y_t - \mu) = \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q} = \sum_{j=0}^q \theta_j \epsilon_{t-j} $$
onde $\epsilon_t$ √© um ru√≠do branco, com $E(\epsilon_t) = 0$, $Var(\epsilon_t) = \sigma^2$ e  $\theta_0=1$.

A previs√£o linear √≥tima para $Y_{t+s}$ baseada nas informa√ß√µes dispon√≠veis at√© o instante $t$ √© dada por [^6]:

$$ \hat{Y}_{t+s|t} = \mu + E[Y_{t+s} - \mu | \epsilon_t, \epsilon_{t-1}, \ldots] $$

Para um modelo MA(q), a previs√£o linear √≥tima $\hat{Y}_{t+s|t}$ pode ser expressa de forma concisa usando o operador de defasagem e o operador de aniquila√ß√£o [^6]:

$$ \hat{Y}_{t+s|t} = \mu + \left[ \frac{\theta(L)}{L^s} \right]_+ \epsilon_t $$
onde $\theta(L) = \sum_{j=0}^q \theta_j L^j$ √© o polin√¥mio de defasagem para o modelo MA(q), e $\left[ \frac{\theta(L)}{L^s} \right]_+$ √© o operador de aniquila√ß√£o que mant√©m apenas os termos com expoentes n√£o negativos de $L$.

Para um modelo **MA(q)**, a previs√£o linear √≥tima √© dada por [^6]:

$$
\hat{Y}_{t+s|t} = \begin{cases}
    \mu + \theta_s \epsilon_t + \theta_{s+1} \epsilon_{t-1} + \ldots + \theta_q \epsilon_{t-q+s}, & \text{ para } s = 1, 2, \ldots, q \\
    \mu, & \text{ para } s = q + 1, q + 2, \ldots
\end{cases}
$$

Esta express√£o crucial demonstra que, para horizontes de previs√£o $s$ menores ou iguais √† ordem $q$ do modelo, a previs√£o linear √≥tima utiliza os erros passados  $\epsilon_t, \epsilon_{t-1}, \ldots, \epsilon_{t-q+s}$. Contudo, para horizontes de previs√£o $s$ maiores do que $q$, a previs√£o linear √≥tima se torna simplesmente a m√©dia incondicional do processo, $\mu$. Essa converg√™ncia para a m√©dia incondicional para horizontes de previs√£o maiores que a ordem q √© uma caracter√≠stica fundamental dos modelos MA(q).

> üí° **Exemplo Num√©rico:** Considere um processo MA(2) com $\mu = 10$, $\theta_1 = 0.6$ e $\theta_2 = 0.3$, e que observamos os seguintes erros: $\epsilon_t=1$, $\epsilon_{t-1}= -0.5$ e $\epsilon_{t-2}= 0.2$.  Vamos calcular as previs√µes e analisar o seu comportamento.
>
> *   Para um passo √† frente ($s=1$):
>    $$\hat{Y}_{t+1|t} = \mu + \theta_1 \epsilon_t + \theta_2 \epsilon_{t-1} = 10 + 0.6(1) + 0.3(-0.5) = 10 + 0.6 - 0.15 = 10.45$$
> *   Para dois passos √† frente ($s=2$):
>     $$\hat{Y}_{t+2|t} = \mu + \theta_2 \epsilon_t = 10 + 0.3(1) = 10.3$$
> *   Para tr√™s passos √† frente ($s=3$):
>     $$\hat{Y}_{t+3|t} = \mu = 10$$
> *   Para qualquer $s>2$:
>      $$\hat{Y}_{t+s|t} = \mu = 10$$
>
> Note que para horizontes maiores que a ordem do modelo ($q=2$), a previs√£o se torna igual √† m√©dia do processo ($\mu=10$).
>
> üí° **Exemplo Num√©rico:** Vamos visualizar o comportamento da previs√£o com um exemplo simulado. Suponha um modelo MA(1) com $\mu=5$, $\theta_1 = 0.8$, e $\sigma^2 = 1$. Simulamos 100 pontos para $\epsilon_t \sim N(0,1)$ e geramos o processo $Y_t$. Abaixo temos o gr√°fico da s√©rie simulada com previs√µes para $s=1$ e $s=2$ a partir do ponto $t=80$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros do modelo
> mu = 5
> theta1 = 0.8
> sigma2 = 1
>
> # Simula√ß√£o do processo MA(1)
> np.random.seed(42)
> eps = np.random.normal(0, np.sqrt(sigma2), 100)
> y = np.zeros(100)
> y[0] = mu + eps[0]
> for t in range(1, 100):
>     y[t] = mu + eps[t] + theta1 * eps[t-1]
>
> # Previs√µes para s=1 e s=2 a partir do ponto t=80
> t_start = 80
> y_hat_1 = mu + theta1 * eps[t_start]
> y_hat_2 = mu
>
> # Plotando a s√©rie e as previs√µes
> plt.figure(figsize=(10, 6))
> plt.plot(y, label='S√©rie MA(1)', marker='o', linestyle='-', markersize=3)
> plt.plot([t_start + 1], [y_hat_1], marker='x', color='red', markersize=8, label='Previs√£o s=1')
> plt.plot([t_start + 2], [y_hat_2], marker='x', color='green', markersize=8, label='Previs√£o s=2')
> plt.axvline(x=t_start, color='gray', linestyle='--')
> plt.xlabel('Tempo (t)')
> plt.ylabel('Valor da S√©rie (Y_t)')
> plt.title('S√©rie MA(1) Simulada e Previs√µes')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> Este gr√°fico mostra que a previs√£o para s=1 usa a informa√ß√£o do √∫ltimo erro, enquanto a previs√£o para s=2 retorna para a m√©dia incondicional.
>

O **Erro M√©dio Quadr√°tico de Previs√£o (MSE)**, que mede a variabilidade dos erros de previs√£o, √© dado por:
$$
MSE = E[(Y_{t+s} - \hat{Y}_{t+s|t})^2]
$$
Para um processo MA(q), o MSE para diferentes horizontes de previs√£o $s$ √© dado por [^6]:

$$
MSE = \begin{cases}
    \sigma^2, & \text{ para } s = 1 \\
    (1 + \theta_1^2 + \ldots + \theta_{s-1}^2)\sigma^2, & \text{ para } s = 2, 3, \ldots, q \\
    (1 + \theta_1^2 + \ldots + \theta_q^2)\sigma^2, & \text{ para } s = q+1, q+2,\ldots
\end{cases}
$$
Este resultado evidencia que o MSE aumenta com o horizonte de previs√£o at√© $q$, e se torna constante para $s>q$. O MSE para um passo √† frente ($s=1$) √© simplesmente a vari√¢ncia do ru√≠do branco $\sigma^2$. Para horizontes de previs√£o entre $1$ e $q$, o MSE aumenta √† medida que mais termos do processo MA(q) entram no erro de previs√£o. Para previs√µes com horizontes maiores do que a ordem $q$ do processo, o MSE se torna constante e igual √† vari√¢ncia incondicional do processo, ou seja, √† vari√¢ncia do processo MA(q) com todos os termos de erro.
Essa caracter√≠stica reflete a natureza da depend√™ncia do modelo MA(q), onde a informa√ß√£o mais relevante para a previs√£o est√° concentrada em um n√∫mero finito de erros passados.

> üí° **Exemplo Num√©rico:** Vamos retomar o processo MA(2) com $\theta_1 = 0.6$ e $\theta_2 = 0.3$ e assumir que $\sigma^2 = 2$. O MSE para diferentes horizontes de previs√£o √©:
>
> *   **MSE para s=1**: $MSE = \sigma^2 = 2$.
>
> *   **MSE para s=2**: $MSE = (1 + \theta_1^2)\sigma^2 = (1 + 0.6^2)2 = (1 + 0.36)2 = 1.36 \times 2 = 2.72$.
>
> *   **MSE para s=3**: $MSE = (1 + \theta_1^2 + \theta_2^2)\sigma^2 = (1 + 0.6^2 + 0.3^2)2 = (1 + 0.36 + 0.09)2 = 1.45 \times 2 = 2.9$.
>
> *   **MSE para s>2**: $MSE =  (1 + \theta_1^2 + \theta_2^2)\sigma^2 = 2.9$
>
>  Note que o MSE aumenta at√© $s=2$ e se estabiliza a partir de $s=3$, ou seja, o MSE converge para a vari√¢ncia incondicional do processo.
>
> üí° **Exemplo Num√©rico:**  Para visualizar o comportamento do MSE, vamos calcular e plotar o MSE para diferentes horizontes de previs√£o para um modelo MA(2). Usaremos os mesmos par√¢metros $\theta_1 = 0.6$, $\theta_2 = 0.3$ e $\sigma^2 = 2$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros do modelo
> theta1 = 0.6
> theta2 = 0.3
> sigma2 = 2
>
> # C√°lculo do MSE para diferentes horizontes de previs√£o
> mse_values = []
> for s in range(1, 10):
>   if s == 1:
>       mse = sigma2
>   elif s <= 2:
>     mse = (1 + np.sum(np.array([theta1, theta2])[:(s-1)]**2))*sigma2
>   else:
>     mse = (1 + theta1**2 + theta2**2)*sigma2
>   mse_values.append(mse)
>
> # Plotando os valores de MSE
> plt.figure(figsize=(8, 5))
> plt.plot(range(1, 10), mse_values, marker='o', linestyle='-')
> plt.xlabel('Horizonte de Previs√£o (s)')
> plt.ylabel('MSE')
> plt.title('MSE para Diferentes Horizontes de Previs√£o - MA(2)')
> plt.grid(True)
> plt.show()
> ```
>
> Este gr√°fico ilustra como o MSE aumenta inicialmente e se estabiliza em um valor constante para horizontes de previs√£o maiores que a ordem do modelo (q=2). Este comportamento refor√ßa a teoria de que, para modelos MA(q), a vari√¢ncia do erro de previs√£o converge para a vari√¢ncia incondicional do processo quando s > q.
>

**Proposi√ß√£o 1.** *MSE para s=1.*
O Erro M√©dio Quadr√°tico de Previs√£o (MSE) para um horizonte de previs√£o de um passo √† frente (s=1) em um processo MA(q) √© igual √† vari√¢ncia do ru√≠do branco $\sigma^2$:
$$MSE = \sigma^2, \quad \text{para } s=1$$
*Prova:*
I. Para $s=1$, a previs√£o linear √≥tima √© dada por $\hat{Y}_{t+1|t} = \mu + \theta_1 \epsilon_t + \theta_2 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q+1}$.
II. O valor real do processo em $t+1$ √© $Y_{t+1} = \mu + \epsilon_{t+1} + \theta_1 \epsilon_t + \theta_2 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q+1}$.
III. O erro de previs√£o √© $e_{t+1|t} = Y_{t+1} - \hat{Y}_{t+1|t} = \epsilon_{t+1}$.
IV.  O MSE √© o valor esperado do quadrado do erro de previs√£o,  $MSE = E[e_{t+1|t}^2] = E[\epsilon_{t+1}^2] $.
V.  Dado que $\epsilon_t$ √© um ru√≠do branco com vari√¢ncia $\sigma^2$, ent√£o $E[\epsilon_{t+1}^2]=\sigma^2$.
Portanto, o MSE para $s=1$ √© $\sigma^2$. ‚ñ†

**Lema 1.** *Converg√™ncia da Previs√£o para a M√©dia Incondicional.*
Para um processo MA(q), quando o horizonte de previs√£o $s$ √© maior que a ordem do processo $q$ ($s > q$), a previs√£o linear √≥tima converge para a m√©dia incondicional $\mu$, ou seja,
$$ \hat{Y}_{t+s|t} = \mu, \quad \text{para } s > q $$

*Prova:*
I. A previs√£o linear √≥tima para um processo MA(q) √© dada por:
$$
\hat{Y}_{t+s|t} = \mu + \left[ \frac{\sum_{j=0}^q \theta_j L^j}{L^s} \right]_+ \epsilon_t
$$
II. Para $s > q$, todos os termos do polin√¥mio no numerador $\sum_{j=0}^q \theta_j L^j$ ter√£o um expoente negativo de $L$ quando divididos por $L^s$.
III.  Pela defini√ß√£o do operador de aniquila√ß√£o, $\left[ \frac{L^k}{L^s} \right]_+ = 0$ para $k<s$.
IV. Portanto, para $s>q$: $\left[ \frac{\sum_{j=0}^q \theta_j L^j}{L^s} \right]_+ = 0$
V. Consequentemente, $\hat{Y}_{t+s|t} = \mu$ para $s > q$.
O Lema 1 estabelece que, para horizontes de previs√£o maiores que a ordem do modelo, a previs√£o linear √≥tima converge para a m√©dia incondicional do processo, destacando uma propriedade essencial dos processos MA(q). ‚ñ†

**Teorema 1.** *Converg√™ncia do MSE para a Vari√¢ncia Incondicional.*
Para um processo MA(q), o erro quadr√°tico m√©dio de previs√£o (MSE) converge para a vari√¢ncia incondicional do processo para horizontes de previs√£o $s$ maiores do que a ordem $q$ ($s > q$), e o MSE √© dado por:
$$MSE =  (1 + \theta_1^2 + \ldots + \theta_q^2) \sigma^2 , \quad \text{para } s > q$$
*Prova:*
I. O erro de previs√£o para um processo MA(q) √© dado por:
$e_{t+s|t} = Y_{t+s} - \hat{Y}_{t+s|t}$.
II.  Como sabemos, do Lema 1 que $\hat{Y}_{t+s|t} = \mu$ para $s>q$, ent√£o
$e_{t+s|t} = Y_{t+s} - \mu$.
III.  O MSE √© a esperan√ßa do quadrado do erro de previs√£o:
$$ MSE = E[(Y_{t+s} - \mu)^2] $$
IV. A vari√¢ncia de um processo MA(q) √© dada por:
    $$Var(Y_t) = (1 + \theta_1^2 + \ldots + \theta_q^2)\sigma^2 $$
V. Portanto, para $s>q$, temos:
$$MSE =  (1 + \theta_1^2 + \ldots + \theta_q^2) \sigma^2 $$
que √© precisamente a vari√¢ncia incondicional do processo.
O Teorema 1 estabelece que o MSE converge para a vari√¢ncia incondicional do processo para previs√µes com horizontes maiores que a ordem do processo MA(q), refletindo a perda de informa√ß√£o preditiva sobre os valores futuros quando a ordem do modelo √© finita e os erros passados tornam-se menos relevantes com o aumento do horizonte de previs√£o. ‚ñ†

**Teorema 1.1** *MSE para horizontes de previs√£o intermedi√°rios.*
Para um processo MA(q), o Erro M√©dio Quadr√°tico de Previs√£o (MSE) para horizontes de previs√£o $s$ tal que $1 < s \leq q$ √© dado por:
$$MSE = (1 + \theta_1^2 + \ldots + \theta_{s-1}^2)\sigma^2, \quad \text{ para } 1 < s \leq q$$
*Prova:*
I. O erro de previs√£o para um processo MA(q) para $1 < s \leq q$ √© dado por:
$e_{t+s|t} = Y_{t+s} - \hat{Y}_{t+s|t}$.
II. A previs√£o linear √≥tima para $1 < s \leq q$ √© $\hat{Y}_{t+s|t} = \mu + \theta_s \epsilon_t + \theta_{s+1} \epsilon_{t-1} + \ldots + \theta_q \epsilon_{t-q+s}$.
III. O valor real do processo em $t+s$ √© $Y_{t+s} = \mu + \epsilon_{t+s} + \theta_1 \epsilon_{t+s-1} + \dots + \theta_{s-1}\epsilon_{t+1} + \theta_s \epsilon_t + \theta_{s+1} \epsilon_{t-1} + \ldots + \theta_q \epsilon_{t-q+s}$.
IV. O erro de previs√£o √© $e_{t+s|t} = \epsilon_{t+s} + \theta_1 \epsilon_{t+s-1} + \dots + \theta_{s-1}\epsilon_{t+1}$.
V. O MSE √© dado por $MSE = E[e_{t+s|t}^2] = E[(\epsilon_{t+s} + \theta_1 \epsilon_{t+s-1} + \dots + \theta_{s-1}\epsilon_{t+1})^2]$.
VI. Dado que os erros s√£o independentes e com m√©dia zero, $E[\epsilon_i\epsilon_j]=0$ para $i \neq j$, e $E[\epsilon_i^2]=\sigma^2$.
VII. Expandindo o quadrado e tomando a esperan√ßa, temos $MSE = E[\epsilon_{t+s}^2] + \theta_1^2E[\epsilon_{t+s-1}^2] + \dots + \theta_{s-1}^2E[\epsilon_{t+1}^2]  = (1 + \theta_1^2 + \ldots + \theta_{s-1}^2)\sigma^2$.

Este teorema complementa o Teorema 1, fornecendo uma express√£o para o MSE em horizontes de previs√£o menores ou iguais a ordem $q$ do processo, completando a an√°lise do comportamento do MSE para todos os poss√≠veis horizontes de previs√£o. ‚ñ†

### Conclus√£o

Este cap√≠tulo explorou o comportamento das **previs√µes lineares √≥timas** em modelos **MA(q)**, com foco na converg√™ncia da previs√£o para a m√©dia incondicional e no comportamento do MSE. Foi demonstrado que, para horizontes de previs√£o maiores que a ordem do modelo ($s>q$), a previs√£o linear √≥tima converge para a m√©dia incondicional do processo, e o MSE converge para a vari√¢ncia incondicional. Esses resultados aprofundam a nossa compreens√£o sobre as previs√µes em s√©ries temporais, particularmente no contexto dos modelos MA(q). A combina√ß√£o do operador de defasagem e do operador de aniquila√ß√£o fornece uma maneira concisa e poderosa de construir essas previs√µes.

A discuss√£o sobre a converg√™ncia da previs√£o para a m√©dia incondicional, que n√£o tem informa√ß√£o dos erros passados, e o MSE convergir para a vari√¢ncia incondicional, demonstram o limite de previs√£o da estrutura MA(q) e suas implica√ß√µes na pr√°tica. Essa propriedade da previs√£o tamb√©m est√° relacionada a ideia de que, para longos horizontes de previs√£o, o processo se torna mais determin√≠stico, no sentido que seu valor esperado (a m√©dia) n√£o utiliza informa√ß√£o dos erros do passado.
A rela√ß√£o com conceitos abordados em cap√≠tulos anteriores √© feita atrav√©s da utiliza√ß√£o das defini√ß√µes e resultados das previs√µes lineares √≥timas e do uso do operador de defasagem em contexto MA(q). Os teoremas aqui apresentados consolidam nosso entendimento sobre a constru√ß√£o de previs√µes √≥timas, e preparam o terreno para a an√°lise de modelos de s√©ries temporais mais complexos, bem como para a an√°lise de previs√µes quando o n√∫mero de observa√ß√µes √© finito.

### Refer√™ncias
[^1]: *Expression [4.1.1] is known as the mean squared error associated with the forecast... The forecast with the smallest mean squared error turns out to be the expectation of Y‚ÇÅ+1 conditional on X‚ÇÅ...*
[^5]: *Consider a process with an MA(‚àû) representation (Y, ‚Äì Œº) = œà(L)Œµ, with e, white noise and...*
[^6]: *...the optimal linear forecast is √ä[Y,+s|‚Ç¨, ‚Ç¨,-1,...] ... the optimal forecast could be written in lag operator notation as √ä[Y,+s|‚Ç¨, ‚Ç¨,-1,...] = Œº + [œà(L)/L^s]_+ Œµ‚ÇÅ*
<!-- END -->
