## Previs√µes Baseadas em um N√∫mero Infinito de Observa√ß√µes para Processos MA(‚àû)

### Introdu√ß√£o

Este cap√≠tulo tem como objetivo aprofundar o conceito de **previs√µes √≥timas** para s√©ries temporais, com foco em processos com representa√ß√£o **MA(‚àû)**, complementando os conceitos introduzidos em se√ß√µes anteriores sobre proje√ß√µes lineares e previs√µes condicionais [^1]. O objetivo principal √© desenvolver uma compreens√£o detalhada de como a informa√ß√£o passada √© utilizada para gerar previs√µes precisas, particularmente quando o n√∫mero de observa√ß√µes tende ao infinito. Exploraremos a constru√ß√£o da previs√£o √≥tima linear, seu erro associado e o uso do operador de defasagem, estabelecendo uma ponte entre a teoria e a pr√°tica.

### Conceitos Fundamentais

Expandindo o conceito de **previs√£o linear √≥tima** apresentado anteriormente, agora abordaremos sua aplica√ß√£o em processos com representa√ß√£o **MA(‚àû)**. Como visto, a previs√£o √≥tima no contexto de previs√£o com base na esperan√ßa condicional √© definida como a esperan√ßa de $Y_{t+1}$ condicional a $X_t$, denotada por $Y_{t+1|t} = E(Y_{t+1}|X_t)$ [^1]. Em particular, quando se trata de modelos baseados em s√©ries temporais, √© crucial considerar que a informa√ß√£o passada relevante se traduz em observa√ß√µes passadas dos erros ($\epsilon$) do processo.

Consideremos um processo com representa√ß√£o **MA(‚àû)** definido como [^5]:

$$(Y_t - \mu) = \psi(L) \epsilon_t$$

onde $\epsilon_t$ representa um ru√≠do branco e $\psi(L)$ √© um polin√¥mio de defasagem dado por $\psi(L) = \sum_{j=0}^\infty \psi_j L^j$, com $\psi_0 = 1$ e $\sum_{j=0}^\infty |\psi_j| < \infty$ [^5].

O objetivo √© gerar previs√µes para $Y_{t+s}$, ou seja, valores que $Y$ assumir√° $s$ per√≠odos √† frente no tempo, com base em toda a informa√ß√£o dispon√≠vel at√© o instante $t$. A previs√£o linear √≥tima neste contexto √© definida como a proje√ß√£o de $Y_{t+s}$ no espa√ßo vetorial gerado por $\epsilon_t, \epsilon_{t-1}, \epsilon_{t-2}, ...$, que ser√° denotada por $\hat{Y}_{t+s|t}$.
A representa√ß√£o do processo no instante $t+s$ √©:

$$Y_{t+s} = \mu + \epsilon_{t+s} + \psi_1\epsilon_{t+s-1} + \ldots + \psi_{s-1}\epsilon_{t+1} + \psi_s \epsilon_t + \psi_{s+1}\epsilon_{t-1} + \ldots$$

A **previs√£o linear √≥tima** de $Y_{t+s}$ no tempo $t$, $\hat{Y}_{t+s|t}$, √© obtida tomando a esperan√ßa condicional de $Y_{t+s}$ com rela√ß√£o √†s informa√ß√µes dispon√≠veis at√© o instante $t$, ou seja [^5]:

$$\hat{Y}_{t+s|t} = E[Y_{t+s} | \epsilon_t, \epsilon_{t-1}, \ldots] = \mu + \psi_s\epsilon_t + \psi_{s+1}\epsilon_{t-1} + \psi_{s+2}\epsilon_{t-2} + \ldots$$

Nesse ponto, √© importante destacar que os erros futuros ($\epsilon_{t+1}, \epsilon_{t+2}, ...$) s√£o substitu√≠dos por seu valor esperado, que √© zero. Esta substitui√ß√£o √© a chave para a constru√ß√£o da previs√£o linear √≥tima. Observe que o erro associado a esta previs√£o √©:

$$Y_{t+s} - \hat{Y}_{t+s|t} = \epsilon_{t+s} + \psi_1 \epsilon_{t+s-1} + \ldots + \psi_{s-1} \epsilon_{t+1}$$

O erro de previs√£o √© formado pelas inova√ß√µes que ainda n√£o foram observadas no tempo $t$, como demonstrado acima.
Ao trabalhar com processos **MA(‚àû)**, a previs√£o linear √≥tima √© obtida usando as observa√ß√µes passadas de $\epsilon$. A ideia √© expressar $\hat{Y}_{t+s|t}$ como uma fun√ß√£o linear dos erros passados:

$$ \hat{Y}_{t+s|t} = \mu + \sum_{j=0}^{\infty} a_j \epsilon_{t-j} $$

Para construir esta previs√£o, deve-se levar em conta que o modelo **MA(‚àû)** nos permite expressar o valor atual da vari√°vel ($Y_t$) como uma fun√ß√£o dos erros passados. Em outras palavras,  as previs√µes s√£o uma fun√ß√£o dos erros passados e n√£o das vari√°veis passadas ($Y_{t-1}$, $Y_{t-2}$, etc), como em modelos autoregressivos.

O uso do **operador de defasagem** $L$ facilita a representa√ß√£o da previs√£o. A express√£o para o forecast pode ser representada como:

$$\hat{Y}_{t+s|t} = \mu + \left[ \frac{\psi(L)}{L^s} \right]_+ \epsilon_t$$

onde $\left[ \frac{\psi(L)}{L^s} \right]_+$  denota o operador de aniquila√ß√£o, que descarta os termos com expoentes negativos de L, ou seja, os termos que se referem a valores futuros de $\epsilon$ [^6]. Isso √© consistente com o fato de que apenas erros passados s√£o usados para construir a previs√£o √≥tima.
A previs√£o linear √≥tima para um processo MA(q) √© dada por [^6]:

$$\hat{Y}_{t+s|t} = \begin{cases}
\mu + \theta_s \epsilon_t + \theta_{s+1} \epsilon_{t-1} + \ldots + \theta_q \epsilon_{t-q+s}, & \text{ para } s = 1, 2, \ldots, q \\
\mu, & \text{ para } s = q + 1, q + 2, \ldots
\end{cases}$$

> üí° **Exemplo Num√©rico:** Considere um processo MA(2) com $\mu = 10$, $\theta_1 = 0.8$, $\theta_2 = 0.5$ e $\sigma^2 = 1$. Se observamos os erros passados $\epsilon_t = 1$, $\epsilon_{t-1} = -0.5$, e $\epsilon_{t-2} = 0.2$. Vamos calcular a previs√£o para os pr√≥ximos 3 per√≠odos:
>
> *   **Previs√£o para s=1:** $\hat{Y}_{t+1|t} = \mu + \theta_1 \epsilon_t + \theta_2 \epsilon_{t-1} = 10 + 0.8(1) + 0.5(-0.5) = 10 + 0.8 - 0.25 = 10.55$
> *   **Previs√£o para s=2:** $\hat{Y}_{t+2|t} = \mu + \theta_2 \epsilon_t = 10 + 0.5(1) = 10.5$
> *   **Previs√£o para s=3:** $\hat{Y}_{t+3|t} = \mu = 10$
>
>  Observe como a previs√£o converge para a m√©dia do processo para horizontes de previs√£o maiores que a ordem do modelo MA(q)

**Lema 1.** *Rela√ß√£o entre coeficientes $\psi_j$ e $\theta_j$ para um MA(q).*
Para um processo MA(q), os coeficientes $\psi_j$ do processo MA(‚àû) est√£o relacionados com os coeficientes $\theta_j$ do processo MA(q) da seguinte maneira:
$$\psi_j = \begin{cases}
\theta_j, & \text{ para } j \leq q \\
0, & \text{ para } j > q
\end{cases}$$

*Prova:*
I.  A representa√ß√£o MA(q) √© um caso especial da representa√ß√£o MA(‚àû).
II. Em um MA(q), temos que $\theta_j = 0$ para $j > q$.
III.  A representa√ß√£o MA(‚àû) √© dada por $(Y_t - \mu) = \sum_{j=0}^\infty \psi_j \epsilon_{t-j}$.
IV.  A representa√ß√£o MA(q) √© dada por $(Y_t - \mu) = \sum_{j=0}^q \theta_j \epsilon_{t-j}$.
V.  Comparando as duas express√µes, podemos concluir que $\psi_j = \theta_j$ para $j \leq q$ e $\psi_j = 0$ para $j > q$.
VI. Portanto, os coeficientes $\psi_j$ para $j \leq q$ coincidem com os coeficientes $\theta_j$ do modelo MA(q).
Esta rela√ß√£o √© fundamental para conectar as representa√ß√µes de modelos MA(q) como um caso espec√≠fico dentro da classe MA(‚àû). ‚ñ†

O erro m√©dio quadr√°tico de previs√£o (MSE) associado a essa previs√£o √© dado por [^6]:
$$MSE = \begin{cases}
\sigma^2, & \text{ para } s = 1 \\
(1 + \theta_1^2 + \ldots + \theta_{s-1}^2)\sigma^2, & \text{ para } s = 2, 3, \ldots, q\\
(1 + \theta_1^2 + \ldots + \theta_q^2) \sigma^2, & \text{ para } s = q+1, q+2,\ldots
\end{cases}$$

> üí° **Exemplo Num√©rico:** Usando o mesmo processo MA(2) do exemplo anterior ($\theta_1 = 0.8$, $\theta_2 = 0.5$, $\sigma^2 = 1$), vamos calcular o MSE para diferentes horizontes de previs√£o:
>
> *   **MSE para s=1:** $MSE = \sigma^2 = 1$
> *   **MSE para s=2:** $MSE = (1 + \theta_1^2)\sigma^2 = (1 + 0.8^2) \times 1 = 1 + 0.64 = 1.64$
> *   **MSE para s=3:** $MSE = (1 + \theta_1^2 + \theta_2^2)\sigma^2 = (1 + 0.8^2 + 0.5^2) \times 1 = 1 + 0.64 + 0.25 = 1.89$
>
> Note que o MSE aumenta com o horizonte de previs√£o at√© atingir um valor m√°ximo para $s > q$.

**Proposi√ß√£o 1.** *Propriedades do erro de previs√£o em modelos MA(‚àû).*
O erro de previs√£o em modelos MA(‚àû) possui as seguintes propriedades:

1.  *M√©dia zero:* O valor esperado do erro de previs√£o √© zero, i.e., $E[Y_{t+s} - \hat{Y}_{t+s|t}] = 0$.
2.  *N√£o correla√ß√£o com as previs√µes:* O erro de previs√£o no instante $t+s$ n√£o √© correlacionado com a previs√£o $\hat{Y}_{t+s|t}$, i.e., $Cov(Y_{t+s} - \hat{Y}_{t+s|t}, \hat{Y}_{t+s|t}) = 0$.
3.  *N√£o correla√ß√£o com erros passados:* Os erros de previs√£o s√£o n√£o correlacionados com os erros passados $\epsilon_j$, para $j \leq t$.

*Prova:*
1.  *M√©dia zero:*
    I. A previs√£o √≥tima √© definida como a esperan√ßa condicional: $\hat{Y}_{t+s|t} = E[Y_{t+s}|\epsilon_t, \epsilon_{t-1}, \ldots]$.
    II. O erro de previs√£o √© definido como: $e_{t+s} = Y_{t+s} - \hat{Y}_{t+s|t}$.
    III. Tomando a esperan√ßa do erro, temos:
    $E[e_{t+s}] = E[Y_{t+s} - \hat{Y}_{t+s|t}] = E[Y_{t+s}] - E[\hat{Y}_{t+s|t}]$.
    IV. Como $\hat{Y}_{t+s|t}$ √© a esperan√ßa condicional de $Y_{t+s}$, $E[\hat{Y}_{t+s|t}] = E[Y_{t+s}]$, logo,
    $E[e_{t+s}] = E[Y_{t+s}] - E[Y_{t+s}] = 0$.
   Portanto, o valor esperado do erro de previs√£o √© zero.

2.  *N√£o correla√ß√£o com as previs√µes:*
    I. Precisamos mostrar que $Cov(Y_{t+s} - \hat{Y}_{t+s|t}, \hat{Y}_{t+s|t}) = 0$.
    II. Usando a defini√ß√£o de covari√¢ncia, temos:
        $Cov(Y_{t+s} - \hat{Y}_{t+s|t}, \hat{Y}_{t+s|t}) = E[(Y_{t+s} - \hat{Y}_{t+s|t})\hat{Y}_{t+s|t}] - E[Y_{t+s} - \hat{Y}_{t+s|t}]E[\hat{Y}_{t+s|t}]$.
    III. Do item 1, sabemos que $E[Y_{t+s} - \hat{Y}_{t+s|t}] = 0$, assim,
       $Cov(Y_{t+s} - \hat{Y}_{t+s|t}, \hat{Y}_{t+s|t}) = E[(Y_{t+s} - \hat{Y}_{t+s|t})\hat{Y}_{t+s|t}]$.
     IV. Como $\hat{Y}_{t+s|t}$ √© a proje√ß√£o de $Y_{t+s}$ no espa√ßo gerado por $\epsilon_t, \epsilon_{t-1}, ...$, o erro de previs√£o $Y_{t+s} - \hat{Y}_{t+s|t}$ √© ortogonal a esse espa√ßo.
    V.  Portanto, o produto interno entre o erro e a previs√£o √© zero, e $E[(Y_{t+s} - \hat{Y}_{t+s|t})\hat{Y}_{t+s|t}] = 0$.
     VI.  Consequentemente, $Cov(Y_{t+s} - \hat{Y}_{t+s|t}, \hat{Y}_{t+s|t}) = 0$.
     A n√£o correla√ß√£o com a previs√£o √© resultado da propriedade de ortogonalidade da proje√ß√£o linear.

3.  *N√£o correla√ß√£o com erros passados:*
    I. Devemos mostrar que $Cov(Y_{t+s} - \hat{Y}_{t+s|t}, \epsilon_j) = 0$ para $j \leq t$.
    II. Usando a defini√ß√£o do erro de previs√£o:
    $Y_{t+s} - \hat{Y}_{t+s|t} = \epsilon_{t+s} + \psi_1 \epsilon_{t+s-1} + \ldots + \psi_{s-1}\epsilon_{t+1}$.
    III. Como o ru√≠do branco $\epsilon_j$ tem m√©dia zero e n√£o √© correlacionado entre diferentes instantes de tempo, todos os $\epsilon_j$ para $j \le t$ n√£o se correlacionam com $\epsilon_{t+s}, \epsilon_{t+s-1}, ..., \epsilon_{t+1}$.
    IV. Portanto, $Cov(Y_{t+s} - \hat{Y}_{t+s|t}, \epsilon_j) = 0$ para $j \leq t$.
    A n√£o correla√ß√£o com os erros passados √© devido √† natureza do ru√≠do branco e √† constru√ß√£o da previs√£o, que utiliza apenas os erros passados para projetar o valor futuro de $Y$. ‚ñ†

O MSE aumenta com o horizonte de previs√£o $s$, at√© atingir um valor m√°ximo no horizonte $q$. Para previs√µes mais distantes no futuro, a previs√£o √≥tima converge para a m√©dia incondicional do processo e a vari√¢ncia do erro converge para a vari√¢ncia incondicional do processo MA(q).

**Teorema 1.** *Converg√™ncia da previs√£o e do erro para processos MA(‚àû).*
Para um processo MA(‚àû) com $\sum_{j=0}^{\infty} |\psi_j| < \infty$, quando o horizonte de previs√£o $s$ tende ao infinito, a previs√£o linear √≥tima converge para a m√©dia incondicional do processo e o erro de previs√£o converge para a vari√¢ncia incondicional. Especificamente:
$$\lim_{s\to \infty} \hat{Y}_{t+s|t} = \mu$$
$$\lim_{s\to \infty} MSE = \sigma^2 \sum_{j=0}^{\infty} \psi_j^2$$

*Prova:*
I. A previs√£o linear √≥tima para $Y_{t+s}$ √© dada por:
    $$\hat{Y}_{t+s|t} = \mu + \psi_s\epsilon_t + \psi_{s+1}\epsilon_{t-1} + \psi_{s+2}\epsilon_{t-2} + \ldots$$
II. Como $\sum_{j=0}^{\infty} |\psi_j| < \infty$, segue que $\lim_{s \to \infty} \psi_s = 0$.
III.  Conforme $s$ aumenta, os coeficientes $\psi_s, \psi_{s+1}, \psi_{s+2}, ...$ tendem a zero.
IV. Assim, para $s \to \infty$, temos $\hat{Y}_{t+s|t} \to \mu$. Logo,
    $$\lim_{s\to \infty} \hat{Y}_{t+s|t} = \mu$$
V. O erro quadr√°tico m√©dio √© definido como $MSE = E[(Y_{t+s}-\hat{Y}_{t+s|t})^2]$.
VI.  No caso de um MA(‚àû),  o erro de previs√£o √© $Y_{t+s}-\hat{Y}_{t+s|t} =  \epsilon_{t+s} + \psi_1 \epsilon_{t+s-1} + \ldots + \psi_{s-1} \epsilon_{t+1}$.
VII. Logo, o MSE √© dado por:
   $MSE = E[(\epsilon_{t+s} + \psi_1 \epsilon_{t+s-1} + \ldots + \psi_{s-1} \epsilon_{t+1})^2]$.
VIII. Como os erros s√£o n√£o correlacionados e t√™m vari√¢ncia $\sigma^2$, temos:
     $MSE = (1 + \psi_1^2 + \psi_2^2 + \ldots + \psi_{s-1}^2)\sigma^2$.
IX. Quando $s$ tende ao infinito, o MSE se aproxima da vari√¢ncia incondicional do processo.
     $$\lim_{s\to \infty} MSE = \sigma^2 \sum_{j=0}^{\infty} \psi_j^2$$
A prova segue diretamente da express√£o para a previs√£o linear √≥tima em um processo MA(‚àû), onde os coeficientes $\psi_s, \psi_{s+1}, ...$ se tornam desprez√≠veis √† medida que $s$ cresce, levando √† converg√™ncia da previs√£o para a m√©dia $\mu$. A converg√™ncia do MSE segue da propriedade $\sum_{j=0}^{\infty} |\psi_j| < \infty$ e da express√£o para o MSE, que acumula as vari√¢ncias dos coeficientes at√© o horizonte $s$. Conforme $s$ tende ao infinito, o MSE se aproxima da vari√¢ncia incondicional do processo. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere um processo MA(‚àû) onde os coeficientes $\psi_j$ decaem exponencialmente, $\psi_j = 0.7^j$, e $\sigma^2 = 1$. Nesse caso, a m√©dia do processo √© $\mu$, e a vari√¢ncia do processo √©:
>
> $$\text{Var}(Y_t) = \sigma^2 \sum_{j=0}^{\infty} \psi_j^2 = 1 \sum_{j=0}^{\infty} (0.7^j)^2 =  \sum_{j=0}^{\infty} (0.49)^j$$
>
> Esta √© a soma de uma s√©rie geom√©trica infinita, que converge para $\frac{1}{1 - 0.49} = \frac{1}{0.51} \approx 1.96$. Isso significa que, √† medida que o horizonte de previs√£o $s$ aumenta, o MSE se aproximar√° de 1.96.
>
>  De acordo com o teorema, a previs√£o $\hat{Y}_{t+s|t}$  converge para $\mu$ quando $s$ tende ao infinito, e o erro de previs√£o converge para a vari√¢ncia incondicional do processo, que √© aproximadamente 1.96 nesse caso.

### Conclus√£o

Este cap√≠tulo detalhou como obter as **previs√µes √≥timas lineares** para processos com representa√ß√£o **MA(‚àû)** quando se tem um n√∫mero infinito de observa√ß√µes passadas. A substitui√ß√£o dos erros futuros por seus valores esperados (zero) e o uso do operador de defasagem s√£o elementos chave na constru√ß√£o dessas previs√µes. Foi demonstrado que a previs√£o linear √≥tima pode ser expressa como uma fun√ß√£o das observa√ß√µes passadas dos erros e que, para previs√µes mais distantes no futuro, a previs√£o converge para a m√©dia incondicional. Esses resultados complementam o desenvolvimento dos cap√≠tulos anteriores, solidificando o entendimento sobre a teoria de previs√µes em s√©ries temporais. Em particular, a an√°lise da estrutura do erro de previs√£o e a evolu√ß√£o do MSE com o horizonte de previs√£o permite uma compreens√£o mais profunda sobre as limita√ß√µes e potencialidades dos modelos de s√©ries temporais.

### Refer√™ncias
[^1]: *Expression [4.1.1] is known as the mean squared error associated with the forecast... The forecast with the smallest mean squared error turns out to be the expectation of Y‚ÇÅ+1 conditional on X‚ÇÅ...*
[^5]: *Consider a process with an MA(‚àû) representation (Y, ‚Äì Œº) = œà(L)Œµ, with e, white noise and...*
[^6]: *...the optimal linear forecast is √ä[Y,+s|‚Ç¨, ‚Ç¨,-1,...] ... the optimal forecast could be written in lag operator notation as √ä[Y,+s|‚Ç¨, ‚Ç¨,-1,...] = Œº + [œà(L)/L^s]_+ Œµ‚ÇÅ*
<!-- END -->
