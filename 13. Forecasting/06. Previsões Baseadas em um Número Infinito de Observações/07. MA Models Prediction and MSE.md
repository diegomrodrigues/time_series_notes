## Previsões Otimizadas em Modelos MA(q): Convergência e MSE

### Introdução

Este capítulo aprofunda a análise das **previsões otimizadas** para modelos **MA(q)**, com foco na convergência da previsão para a média incondicional e no comportamento do **Erro Médio Quadrático de Previsão (MSE)** à medida que o horizonte de previsão aumenta. Construindo sobre os conceitos de previsões lineares ótimas e operadores de defasagem previamente introduzidos [^1], [^5], [^6], o objetivo deste capítulo é investigar como as características finitas dos modelos MA(q) afetam o comportamento das previsões para diferentes horizontes, destacando a relação entre previsões lineares, a estrutura do modelo e os conceitos de convergência em séries temporais.

### Conceitos Fundamentais

Como visto nos capítulos anteriores, a **previsão linear ótima** para um processo com representação **MA(∞)** é construída como uma combinação linear dos erros passados, onde os erros futuros são substituídos por seus valores esperados, que são zero. No contexto específico de modelos **MA(q)**, este conceito assume uma forma particular devido à natureza finita da dependência dos erros passados.

Consideremos um processo **MA(q)** definido como:
$$ (Y_t - \mu) = \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q} = \sum_{j=0}^q \theta_j \epsilon_{t-j} $$
onde $\epsilon_t$ é um ruído branco, com $E(\epsilon_t) = 0$, $Var(\epsilon_t) = \sigma^2$ e  $\theta_0=1$.

A previsão linear ótima para $Y_{t+s}$ baseada nas informações disponíveis até o instante $t$ é dada por [^6]:

$$ \hat{Y}_{t+s|t} = \mu + E[Y_{t+s} - \mu | \epsilon_t, \epsilon_{t-1}, \ldots] $$

Para um modelo MA(q), a previsão linear ótima $\hat{Y}_{t+s|t}$ pode ser expressa de forma concisa usando o operador de defasagem e o operador de aniquilação [^6]:

$$ \hat{Y}_{t+s|t} = \mu + \left[ \frac{\theta(L)}{L^s} \right]_+ \epsilon_t $$
onde $\theta(L) = \sum_{j=0}^q \theta_j L^j$ é o polinômio de defasagem para o modelo MA(q), e $\left[ \frac{\theta(L)}{L^s} \right]_+$ é o operador de aniquilação que mantém apenas os termos com expoentes não negativos de $L$.

Para um modelo **MA(q)**, a previsão linear ótima é dada por [^6]:

$$
\hat{Y}_{t+s|t} = \begin{cases}
    \mu + \theta_s \epsilon_t + \theta_{s+1} \epsilon_{t-1} + \ldots + \theta_q \epsilon_{t-q+s}, & \text{ para } s = 1, 2, \ldots, q \\
    \mu, & \text{ para } s = q + 1, q + 2, \ldots
\end{cases}
$$

Esta expressão crucial demonstra que, para horizontes de previsão $s$ menores ou iguais à ordem $q$ do modelo, a previsão linear ótima utiliza os erros passados  $\epsilon_t, \epsilon_{t-1}, \ldots, \epsilon_{t-q+s}$. Contudo, para horizontes de previsão $s$ maiores do que $q$, a previsão linear ótima se torna simplesmente a média incondicional do processo, $\mu$. Essa convergência para a média incondicional para horizontes de previsão maiores que a ordem q é uma característica fundamental dos modelos MA(q).

> 💡 **Exemplo Numérico:** Considere um processo MA(2) com $\mu = 10$, $\theta_1 = 0.6$ e $\theta_2 = 0.3$, e que observamos os seguintes erros: $\epsilon_t=1$, $\epsilon_{t-1}= -0.5$ e $\epsilon_{t-2}= 0.2$.  Vamos calcular as previsões e analisar o seu comportamento.
>
> *   Para um passo à frente ($s=1$):
>    $$\hat{Y}_{t+1|t} = \mu + \theta_1 \epsilon_t + \theta_2 \epsilon_{t-1} = 10 + 0.6(1) + 0.3(-0.5) = 10 + 0.6 - 0.15 = 10.45$$
> *   Para dois passos à frente ($s=2$):
>     $$\hat{Y}_{t+2|t} = \mu + \theta_2 \epsilon_t = 10 + 0.3(1) = 10.3$$
> *   Para três passos à frente ($s=3$):
>     $$\hat{Y}_{t+3|t} = \mu = 10$$
> *   Para qualquer $s>2$:
>      $$\hat{Y}_{t+s|t} = \mu = 10$$
>
> Note que para horizontes maiores que a ordem do modelo ($q=2$), a previsão se torna igual à média do processo ($\mu=10$).
>
> 💡 **Exemplo Numérico:** Vamos visualizar o comportamento da previsão com um exemplo simulado. Suponha um modelo MA(1) com $\mu=5$, $\theta_1 = 0.8$, e $\sigma^2 = 1$. Simulamos 100 pontos para $\epsilon_t \sim N(0,1)$ e geramos o processo $Y_t$. Abaixo temos o gráfico da série simulada com previsões para $s=1$ e $s=2$ a partir do ponto $t=80$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Parâmetros do modelo
> mu = 5
> theta1 = 0.8
> sigma2 = 1
>
> # Simulação do processo MA(1)
> np.random.seed(42)
> eps = np.random.normal(0, np.sqrt(sigma2), 100)
> y = np.zeros(100)
> y[0] = mu + eps[0]
> for t in range(1, 100):
>     y[t] = mu + eps[t] + theta1 * eps[t-1]
>
> # Previsões para s=1 e s=2 a partir do ponto t=80
> t_start = 80
> y_hat_1 = mu + theta1 * eps[t_start]
> y_hat_2 = mu
>
> # Plotando a série e as previsões
> plt.figure(figsize=(10, 6))
> plt.plot(y, label='Série MA(1)', marker='o', linestyle='-', markersize=3)
> plt.plot([t_start + 1], [y_hat_1], marker='x', color='red', markersize=8, label='Previsão s=1')
> plt.plot([t_start + 2], [y_hat_2], marker='x', color='green', markersize=8, label='Previsão s=2')
> plt.axvline(x=t_start, color='gray', linestyle='--')
> plt.xlabel('Tempo (t)')
> plt.ylabel('Valor da Série (Y_t)')
> plt.title('Série MA(1) Simulada e Previsões')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> Este gráfico mostra que a previsão para s=1 usa a informação do último erro, enquanto a previsão para s=2 retorna para a média incondicional.
>

O **Erro Médio Quadrático de Previsão (MSE)**, que mede a variabilidade dos erros de previsão, é dado por:
$$
MSE = E[(Y_{t+s} - \hat{Y}_{t+s|t})^2]
$$
Para um processo MA(q), o MSE para diferentes horizontes de previsão $s$ é dado por [^6]:

$$
MSE = \begin{cases}
    \sigma^2, & \text{ para } s = 1 \\
    (1 + \theta_1^2 + \ldots + \theta_{s-1}^2)\sigma^2, & \text{ para } s = 2, 3, \ldots, q \\
    (1 + \theta_1^2 + \ldots + \theta_q^2)\sigma^2, & \text{ para } s = q+1, q+2,\ldots
\end{cases}
$$
Este resultado evidencia que o MSE aumenta com o horizonte de previsão até $q$, e se torna constante para $s>q$. O MSE para um passo à frente ($s=1$) é simplesmente a variância do ruído branco $\sigma^2$. Para horizontes de previsão entre $1$ e $q$, o MSE aumenta à medida que mais termos do processo MA(q) entram no erro de previsão. Para previsões com horizontes maiores do que a ordem $q$ do processo, o MSE se torna constante e igual à variância incondicional do processo, ou seja, à variância do processo MA(q) com todos os termos de erro.
Essa característica reflete a natureza da dependência do modelo MA(q), onde a informação mais relevante para a previsão está concentrada em um número finito de erros passados.

> 💡 **Exemplo Numérico:** Vamos retomar o processo MA(2) com $\theta_1 = 0.6$ e $\theta_2 = 0.3$ e assumir que $\sigma^2 = 2$. O MSE para diferentes horizontes de previsão é:
>
> *   **MSE para s=1**: $MSE = \sigma^2 = 2$.
>
> *   **MSE para s=2**: $MSE = (1 + \theta_1^2)\sigma^2 = (1 + 0.6^2)2 = (1 + 0.36)2 = 1.36 \times 2 = 2.72$.
>
> *   **MSE para s=3**: $MSE = (1 + \theta_1^2 + \theta_2^2)\sigma^2 = (1 + 0.6^2 + 0.3^2)2 = (1 + 0.36 + 0.09)2 = 1.45 \times 2 = 2.9$.
>
> *   **MSE para s>2**: $MSE =  (1 + \theta_1^2 + \theta_2^2)\sigma^2 = 2.9$
>
>  Note que o MSE aumenta até $s=2$ e se estabiliza a partir de $s=3$, ou seja, o MSE converge para a variância incondicional do processo.
>
> 💡 **Exemplo Numérico:**  Para visualizar o comportamento do MSE, vamos calcular e plotar o MSE para diferentes horizontes de previsão para um modelo MA(2). Usaremos os mesmos parâmetros $\theta_1 = 0.6$, $\theta_2 = 0.3$ e $\sigma^2 = 2$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Parâmetros do modelo
> theta1 = 0.6
> theta2 = 0.3
> sigma2 = 2
>
> # Cálculo do MSE para diferentes horizontes de previsão
> mse_values = []
> for s in range(1, 10):
>   if s == 1:
>       mse = sigma2
>   elif s <= 2:
>     mse = (1 + np.sum(np.array([theta1, theta2])[:(s-1)]**2))*sigma2
>   else:
>     mse = (1 + theta1**2 + theta2**2)*sigma2
>   mse_values.append(mse)
>
> # Plotando os valores de MSE
> plt.figure(figsize=(8, 5))
> plt.plot(range(1, 10), mse_values, marker='o', linestyle='-')
> plt.xlabel('Horizonte de Previsão (s)')
> plt.ylabel('MSE')
> plt.title('MSE para Diferentes Horizontes de Previsão - MA(2)')
> plt.grid(True)
> plt.show()
> ```
>
> Este gráfico ilustra como o MSE aumenta inicialmente e se estabiliza em um valor constante para horizontes de previsão maiores que a ordem do modelo (q=2). Este comportamento reforça a teoria de que, para modelos MA(q), a variância do erro de previsão converge para a variância incondicional do processo quando s > q.
>

**Proposição 1.** *MSE para s=1.*
O Erro Médio Quadrático de Previsão (MSE) para um horizonte de previsão de um passo à frente (s=1) em um processo MA(q) é igual à variância do ruído branco $\sigma^2$:
$$MSE = \sigma^2, \quad \text{para } s=1$$
*Prova:*
I. Para $s=1$, a previsão linear ótima é dada por $\hat{Y}_{t+1|t} = \mu + \theta_1 \epsilon_t + \theta_2 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q+1}$.
II. O valor real do processo em $t+1$ é $Y_{t+1} = \mu + \epsilon_{t+1} + \theta_1 \epsilon_t + \theta_2 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q+1}$.
III. O erro de previsão é $e_{t+1|t} = Y_{t+1} - \hat{Y}_{t+1|t} = \epsilon_{t+1}$.
IV.  O MSE é o valor esperado do quadrado do erro de previsão,  $MSE = E[e_{t+1|t}^2] = E[\epsilon_{t+1}^2] $.
V.  Dado que $\epsilon_t$ é um ruído branco com variância $\sigma^2$, então $E[\epsilon_{t+1}^2]=\sigma^2$.
Portanto, o MSE para $s=1$ é $\sigma^2$. ■

**Lema 1.** *Convergência da Previsão para a Média Incondicional.*
Para um processo MA(q), quando o horizonte de previsão $s$ é maior que a ordem do processo $q$ ($s > q$), a previsão linear ótima converge para a média incondicional $\mu$, ou seja,
$$ \hat{Y}_{t+s|t} = \mu, \quad \text{para } s > q $$

*Prova:*
I. A previsão linear ótima para um processo MA(q) é dada por:
$$
\hat{Y}_{t+s|t} = \mu + \left[ \frac{\sum_{j=0}^q \theta_j L^j}{L^s} \right]_+ \epsilon_t
$$
II. Para $s > q$, todos os termos do polinômio no numerador $\sum_{j=0}^q \theta_j L^j$ terão um expoente negativo de $L$ quando divididos por $L^s$.
III.  Pela definição do operador de aniquilação, $\left[ \frac{L^k}{L^s} \right]_+ = 0$ para $k<s$.
IV. Portanto, para $s>q$: $\left[ \frac{\sum_{j=0}^q \theta_j L^j}{L^s} \right]_+ = 0$
V. Consequentemente, $\hat{Y}_{t+s|t} = \mu$ para $s > q$.
O Lema 1 estabelece que, para horizontes de previsão maiores que a ordem do modelo, a previsão linear ótima converge para a média incondicional do processo, destacando uma propriedade essencial dos processos MA(q). ■

**Teorema 1.** *Convergência do MSE para a Variância Incondicional.*
Para um processo MA(q), o erro quadrático médio de previsão (MSE) converge para a variância incondicional do processo para horizontes de previsão $s$ maiores do que a ordem $q$ ($s > q$), e o MSE é dado por:
$$MSE =  (1 + \theta_1^2 + \ldots + \theta_q^2) \sigma^2 , \quad \text{para } s > q$$
*Prova:*
I. O erro de previsão para um processo MA(q) é dado por:
$e_{t+s|t} = Y_{t+s} - \hat{Y}_{t+s|t}$.
II.  Como sabemos, do Lema 1 que $\hat{Y}_{t+s|t} = \mu$ para $s>q$, então
$e_{t+s|t} = Y_{t+s} - \mu$.
III.  O MSE é a esperança do quadrado do erro de previsão:
$$ MSE = E[(Y_{t+s} - \mu)^2] $$
IV. A variância de um processo MA(q) é dada por:
    $$Var(Y_t) = (1 + \theta_1^2 + \ldots + \theta_q^2)\sigma^2 $$
V. Portanto, para $s>q$, temos:
$$MSE =  (1 + \theta_1^2 + \ldots + \theta_q^2) \sigma^2 $$
que é precisamente a variância incondicional do processo.
O Teorema 1 estabelece que o MSE converge para a variância incondicional do processo para previsões com horizontes maiores que a ordem do processo MA(q), refletindo a perda de informação preditiva sobre os valores futuros quando a ordem do modelo é finita e os erros passados tornam-se menos relevantes com o aumento do horizonte de previsão. ■

**Teorema 1.1** *MSE para horizontes de previsão intermediários.*
Para um processo MA(q), o Erro Médio Quadrático de Previsão (MSE) para horizontes de previsão $s$ tal que $1 < s \leq q$ é dado por:
$$MSE = (1 + \theta_1^2 + \ldots + \theta_{s-1}^2)\sigma^2, \quad \text{ para } 1 < s \leq q$$
*Prova:*
I. O erro de previsão para um processo MA(q) para $1 < s \leq q$ é dado por:
$e_{t+s|t} = Y_{t+s} - \hat{Y}_{t+s|t}$.
II. A previsão linear ótima para $1 < s \leq q$ é $\hat{Y}_{t+s|t} = \mu + \theta_s \epsilon_t + \theta_{s+1} \epsilon_{t-1} + \ldots + \theta_q \epsilon_{t-q+s}$.
III. O valor real do processo em $t+s$ é $Y_{t+s} = \mu + \epsilon_{t+s} + \theta_1 \epsilon_{t+s-1} + \dots + \theta_{s-1}\epsilon_{t+1} + \theta_s \epsilon_t + \theta_{s+1} \epsilon_{t-1} + \ldots + \theta_q \epsilon_{t-q+s}$.
IV. O erro de previsão é $e_{t+s|t} = \epsilon_{t+s} + \theta_1 \epsilon_{t+s-1} + \dots + \theta_{s-1}\epsilon_{t+1}$.
V. O MSE é dado por $MSE = E[e_{t+s|t}^2] = E[(\epsilon_{t+s} + \theta_1 \epsilon_{t+s-1} + \dots + \theta_{s-1}\epsilon_{t+1})^2]$.
VI. Dado que os erros são independentes e com média zero, $E[\epsilon_i\epsilon_j]=0$ para $i \neq j$, e $E[\epsilon_i^2]=\sigma^2$.
VII. Expandindo o quadrado e tomando a esperança, temos $MSE = E[\epsilon_{t+s}^2] + \theta_1^2E[\epsilon_{t+s-1}^2] + \dots + \theta_{s-1}^2E[\epsilon_{t+1}^2]  = (1 + \theta_1^2 + \ldots + \theta_{s-1}^2)\sigma^2$.

Este teorema complementa o Teorema 1, fornecendo uma expressão para o MSE em horizontes de previsão menores ou iguais a ordem $q$ do processo, completando a análise do comportamento do MSE para todos os possíveis horizontes de previsão. ■

### Conclusão

Este capítulo explorou o comportamento das **previsões lineares ótimas** em modelos **MA(q)**, com foco na convergência da previsão para a média incondicional e no comportamento do MSE. Foi demonstrado que, para horizontes de previsão maiores que a ordem do modelo ($s>q$), a previsão linear ótima converge para a média incondicional do processo, e o MSE converge para a variância incondicional. Esses resultados aprofundam a nossa compreensão sobre as previsões em séries temporais, particularmente no contexto dos modelos MA(q). A combinação do operador de defasagem e do operador de aniquilação fornece uma maneira concisa e poderosa de construir essas previsões.

A discussão sobre a convergência da previsão para a média incondicional, que não tem informação dos erros passados, e o MSE convergir para a variância incondicional, demonstram o limite de previsão da estrutura MA(q) e suas implicações na prática. Essa propriedade da previsão também está relacionada a ideia de que, para longos horizontes de previsão, o processo se torna mais determinístico, no sentido que seu valor esperado (a média) não utiliza informação dos erros do passado.
A relação com conceitos abordados em capítulos anteriores é feita através da utilização das definições e resultados das previsões lineares ótimas e do uso do operador de defasagem em contexto MA(q). Os teoremas aqui apresentados consolidam nosso entendimento sobre a construção de previsões ótimas, e preparam o terreno para a análise de modelos de séries temporais mais complexos, bem como para a análise de previsões quando o número de observações é finito.

### Referências
[^1]: *Expression [4.1.1] is known as the mean squared error associated with the forecast... The forecast with the smallest mean squared error turns out to be the expectation of Y₁+1 conditional on X₁...*
[^5]: *Consider a process with an MA(∞) representation (Y, – μ) = ψ(L)ε, with e, white noise and...*
[^6]: *...the optimal linear forecast is Ê[Y,+s|€, €,-1,...] ... the optimal forecast could be written in lag operator notation as Ê[Y,+s|€, €,-1,...] = μ + [ψ(L)/L^s]_+ ε₁*
<!-- END -->
