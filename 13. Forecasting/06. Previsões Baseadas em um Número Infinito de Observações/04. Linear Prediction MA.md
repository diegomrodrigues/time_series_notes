## Previs√£o Linear √ìtima em Processos MA(‚àû): Uma Abordagem Detalhada

### Introdu√ß√£o

Este cap√≠tulo se aprofunda na deriva√ß√£o da **previs√£o linear √≥tima** para processos com representa√ß√£o **MA(‚àû)**, com foco na utiliza√ß√£o das informa√ß√µes dos erros passados ($\epsilon$) e na exclus√£o dos erros futuros. Conectando-se com os conceitos apresentados anteriormente sobre previs√µes √≥timas condicionais e operadores de defasagem [^1], aqui exploraremos os fundamentos te√≥ricos e pr√°ticos que sustentam a modelagem de s√©ries temporais com base em um n√∫mero infinito de observa√ß√µes. O objetivo principal √© fornecer uma compreens√£o rigorosa de como a estrutura MA(‚àû) influencia a constru√ß√£o da previs√£o linear √≥tima, destacando a import√¢ncia do passado e a irrelev√¢ncia do futuro nesse contexto espec√≠fico.

### Conceitos Fundamentais

Como visto anteriormente, a previs√£o √≥tima para um processo com representa√ß√£o **MA(‚àû)** √© obtida atrav√©s da **esperan√ßa condicional** de $Y_{t+s}$ em rela√ß√£o ao seu passado [^1]. Em particular, nos modelos **MA(‚àû)**, esta esperan√ßa condicional se traduz na proje√ß√£o de $Y_{t+s}$ sobre o espa√ßo vetorial gerado pelos erros passados $\epsilon_t, \epsilon_{t-1}, \epsilon_{t-2},...$ [^5]. O foco desta se√ß√£o √© derivar precisamente esta proje√ß√£o linear, detalhando como os erros passados ($\epsilon$) s√£o combinados para formar a melhor previs√£o poss√≠vel, sem a utiliza√ß√£o dos erros futuros.

Retomando a representa√ß√£o **MA(‚àû)** [^5]:
$$ (Y_t - \mu) = \psi(L) \epsilon_t = \sum_{j=0}^{\infty} \psi_j L^j \epsilon_t $$
onde $\epsilon_t$ √© ru√≠do branco e  $\sum_{j=0}^\infty |\psi_j| < \infty$ e $\psi_0=1$.

Para prever $Y_{t+s}$, o objetivo √© construir a proje√ß√£o linear $\hat{Y}_{t+s|t}$ que minimize o erro quadr√°tico m√©dio, utilizando apenas as informa√ß√µes dispon√≠veis at√© o tempo $t$, ou seja, os erros passados $\epsilon_t, \epsilon_{t-1}, \epsilon_{t-2}, ...$. A representa√ß√£o de $Y_{t+s}$ no instante $t+s$ √© [^5]:

$$Y_{t+s} = \mu + \epsilon_{t+s} + \psi_1\epsilon_{t+s-1} + \ldots + \psi_{s-1}\epsilon_{t+1} + \psi_s \epsilon_t + \psi_{s+1}\epsilon_{t-1} + \ldots$$

Ao tomar a esperan√ßa condicional, os erros futuros ($\epsilon_{t+s}, \epsilon_{t+s-1}, \ldots, \epsilon_{t+1}$) s√£o substitu√≠dos por seu valor esperado, que √© zero, resultando na previs√£o linear √≥tima [^5]:
$$
\hat{Y}_{t+s|t} = E[Y_{t+s} | \epsilon_t, \epsilon_{t-1}, \ldots] = \mu + \psi_s\epsilon_t + \psi_{s+1}\epsilon_{t-1} + \psi_{s+2}\epsilon_{t-2} + \ldots
$$
Esta express√£o evidencia um ponto central: a previs√£o √≥tima para processos MA(‚àû) √© constru√≠da como uma combina√ß√£o linear dos erros passados $\epsilon_t, \epsilon_{t-1}, \epsilon_{t-2},...$, com os coeficientes $\psi_j$ associados aos respectivos erros passados. Note que, por defini√ß√£o, $\psi_0 = 1$, garantindo que o erro mais recente tem peso 1 no instante atual.

> üí° **Exemplo Num√©rico:** Suponha que temos um processo MA(‚àû) com $\mu = 10$ e coeficientes $\psi_j = (0.8)^j$. Queremos prever $Y_{t+2}$ no tempo $t$. Assumindo que os erros passados s√£o $\epsilon_t = 1$, $\epsilon_{t-1} = -0.5$, e $\epsilon_{t-2} = 0.25$. Ent√£o:
>
> $$ \hat{Y}_{t+2|t} = \mu + \psi_2 \epsilon_t + \psi_3 \epsilon_{t-1} + \psi_4 \epsilon_{t-2} + \dots $$
>
>  Calculando alguns termos:
>
> $$ \hat{Y}_{t+2|t} = 10 + (0.8)^2 (1) + (0.8)^3 (-0.5) + (0.8)^4 (0.25) + \dots $$
> $$ \hat{Y}_{t+2|t} = 10 + 0.64 - 0.256 + 0.1024 + \dots $$
>
> Truncando ap√≥s o terceiro termo, temos $\hat{Y}_{t+2|t} \approx 10.486$. Isso demonstra como os erros passados s√£o ponderados pelos coeficientes $\psi_j$ para gerar a previs√£o.
>

A previs√£o linear √≥tima, neste contexto, √© uma fun√ß√£o linear dos erros passados $\epsilon_t, \epsilon_{t-1}, \epsilon_{t-2},...$ e pode ser expressa como [^6]:
$$
\hat{Y}_{t+s|t} = \mu + \sum_{j=0}^{\infty} a_j \epsilon_{t-j}
$$
onde $a_j$ s√£o coeficientes que precisam ser determinados. A chave para expressar a proje√ß√£o linear, √© realizar que $a_j = \psi_{s+j}$, para $j \ge 0$. Ou seja:
$$
\hat{Y}_{t+s|t} = \mu + \sum_{j=0}^{\infty} \psi_{s+j} \epsilon_{t-j}
$$
√â importante notar que, na pr√°tica, a s√©rie de erros √© truncada em um n√∫mero finito de termos. Essa truncagem √© baseada no princ√≠pio de que os erros mais distantes no passado t√™m um impacto cada vez menor sobre a previs√£o. Assim, a previs√£o torna-se vi√°vel na pr√°tica, mesmo com infinitas observa√ß√µes.

A utiliza√ß√£o do **operador de defasagem** $L$ oferece uma representa√ß√£o compacta e elegante para a previs√£o linear √≥tima. Como demonstrado anteriormente [^6], a previs√£o pode ser expressa como:
$$
\hat{Y}_{t+s|t} = \mu + \left[ \frac{\psi(L)}{L^s} \right]_+ \epsilon_t
$$

Onde $\left[ \frac{\psi(L)}{L^s} \right]_+$  √© o operador de aniquila√ß√£o que exclui os termos com expoentes negativos de $L$,  ou seja, aqueles que se referem a erros futuros, garantindo que a previs√£o seja constru√≠da apenas com informa√ß√µes do passado [^6]. Isso tamb√©m garante que a proje√ß√£o linear √© uma fun√ß√£o apenas dos erros passados $\epsilon_t, \epsilon_{t-1},...$ e n√£o dos erros futuros $\epsilon_{t+1}, \epsilon_{t+2},...$. O operador de aniquila√ß√£o formaliza a intui√ß√£o de que os erros futuros s√£o substitu√≠dos por seus valores esperados, que s√£o zero.

> üí° **Exemplo Num√©rico:** Vamos usar o mesmo exemplo com $\psi_j = (0.8)^j$ e queremos prever $Y_{t+1}$. Assim, temos que $s=1$. A representa√ß√£o em termos do operador de defasagem seria:
>
> $$ \psi(L) = \sum_{j=0}^{\infty} (0.8)^j L^j = 1 + 0.8L + 0.64L^2 + 0.512L^3 + ... $$
>
>  Ent√£o, $\frac{\psi(L)}{L^1} = \frac{1}{L} + 0.8 + 0.64L + 0.512L^2 + ...$. Aplicando o operador de aniquila√ß√£o $[\cdot]_+$, eliminamos o termo com pot√™ncia negativa de L e temos:
>
>  $$ \left[\frac{\psi(L)}{L^1}\right]_+ =  0.8 + 0.64L + 0.512L^2 + ... $$
>
>  Logo, a previs√£o para um passo √† frente √©:
>  $$ \hat{Y}_{t+1|t} = \mu + (0.8\epsilon_t + 0.64\epsilon_{t-1} + 0.512\epsilon_{t-2} + ...) $$
>
> Usando os mesmos valores de $\epsilon$ do exemplo anterior:
>
> $$ \hat{Y}_{t+1|t} = 10 + (0.8)(1) + (0.64)(-0.5) + (0.512)(0.25) + \dots \approx 10.518 $$
>
> Este exemplo demonstra como o operador de defasagem pode ser usado para construir a previs√£o. Note que estamos sempre utilizando os erros passados.

Para um processo MA(q), a previs√£o linear √≥tima √© dada por [^6]:
$$
\hat{Y}_{t+s|t} = \begin{cases}
    \mu + \theta_s \epsilon_t + \theta_{s+1} \epsilon_{t-1} + \ldots + \theta_q \epsilon_{t-q+s}, & \text{ para } s = 1, 2, \ldots, q \\
    \mu, & \text{ para } s = q + 1, q + 2, \ldots
\end{cases}
$$

> üí° **Exemplo Num√©rico:** Considere um processo MA(2) com $\mu = 2$, $\theta_1 = 0.7$, e $\theta_2 = 0.4$. Se tivermos os erros $\epsilon_t = 0.5$, $\epsilon_{t-1} = -0.2$, e $\epsilon_{t-2} = 0.1$.
>
> Para uma previs√£o de um passo √† frente ($s=1$):
>
> $$\hat{Y}_{t+1|t} = \mu + \theta_1 \epsilon_t + \theta_2 \epsilon_{t-1} = 2 + 0.7(0.5) + 0.4(-0.2) = 2 + 0.35 - 0.08 = 2.27$$
>
> Para uma previs√£o de dois passos √† frente ($s=2$):
>
>  $$\hat{Y}_{t+2|t} = \mu + \theta_2 \epsilon_t = 2 + 0.4(0.5) = 2.2$$
>
>  Para qualquer previs√£o com $s > 2$, $\hat{Y}_{t+s|t} = \mu = 2$. Este exemplo ilustra como os coeficientes do MA(q) s√£o usados na previs√£o.

Este resultado √© consequ√™ncia direta do Lema 1, que mostra que a proje√ß√£o linear √≥tima √© uma combina√ß√£o linear dos erros passados, onde os coeficientes $\psi_j$ s√£o iguais aos $\theta_j$ para $j \le q$ e zero caso contr√°rio. Os erros futuros n√£o entram na equa√ß√£o, mostrando como a estrutura do processo MA(‚àû) √© intrinsecamente ligada √† sua capacidade de previs√£o.

**Lema 1**: A proje√ß√£o linear √≥tima de $Y_{t+s}$ sobre o espa√ßo gerado por $\{\epsilon_t, \epsilon_{t-1}, \epsilon_{t-2}, ...\}$ √© dada por $\hat{Y}_{t+s|t} = \mu + \sum_{j=0}^{\infty} \psi_{s+j} \epsilon_{t-j}$.

*Prova:*
I.  A representa√ß√£o MA(‚àû) de $Y_{t+s}$ √©:
    $$Y_{t+s} = \mu + \sum_{j=0}^{\infty} \psi_j \epsilon_{t+s-j}$$
II. A previs√£o linear √≥tima $\hat{Y}_{t+s|t}$ √© dada pela esperan√ßa condicional de $Y_{t+s}$ em rela√ß√£o ao passado, ou seja, em rela√ß√£o a $\epsilon_t, \epsilon_{t-1}, \epsilon_{t-2}, ...$.
    $$\hat{Y}_{t+s|t} = E[Y_{t+s} | \epsilon_t, \epsilon_{t-1}, \epsilon_{t-2}, ...]$$
III. Substituindo a representa√ß√£o MA(‚àû) em II, temos:
   $$\hat{Y}_{t+s|t} = E\left[\mu + \sum_{j=0}^{\infty} \psi_j \epsilon_{t+s-j} | \epsilon_t, \epsilon_{t-1}, \epsilon_{t-2}, ...\right]$$
IV. Pela linearidade da esperan√ßa condicional:
    $$\hat{Y}_{t+s|t} = \mu + E\left[\sum_{j=0}^{\infty} \psi_j \epsilon_{t+s-j} | \epsilon_t, \epsilon_{t-1}, \epsilon_{t-2}, ...\right]$$
V. Separando a soma em termos passados e futuros:
    $$\hat{Y}_{t+s|t} = \mu + E\left[\sum_{j=s}^{\infty} \psi_j \epsilon_{t+s-j} + \sum_{j=0}^{s-1} \psi_j \epsilon_{t+s-j} | \epsilon_t, \epsilon_{t-1}, \epsilon_{t-2}, ...\right]$$
VI.  Observando que para $j \ge s$, temos $t+s-j \le t$ e para $j < s$, temos $t+s-j > t$. Portanto:
    $$\hat{Y}_{t+s|t} = \mu + \sum_{j=s}^{\infty} \psi_j \epsilon_{t+s-j} + \sum_{j=0}^{s-1} \psi_j E[\epsilon_{t+s-j} | \epsilon_t, \epsilon_{t-1}, \epsilon_{t-2}, ...]$$
VII. Como os erros s√£o ru√≠do branco, $E[\epsilon_{t+k}|\epsilon_t,\epsilon_{t-1},\dots]=0$ se $k>0$. Assim, todos os termos da segunda somat√≥ria s√£o zero, pois $t+s-j > t$. Portanto,
    $$\hat{Y}_{t+s|t} = \mu + \sum_{j=s}^{\infty} \psi_j \epsilon_{t+s-j}$$
VIII. Fazendo a mudan√ßa de vari√°vel $k=j-s$, temos $j = k+s$:
     $$\hat{Y}_{t+s|t} = \mu + \sum_{k=0}^{\infty} \psi_{k+s} \epsilon_{t-k}$$
IX. Renomeando o √≠ndice $k$ para $j$:
    $$\hat{Y}_{t+s|t} = \mu + \sum_{j=0}^{\infty} \psi_{s+j} \epsilon_{t-j}$$
Portanto, a proje√ß√£o linear √≥tima de $Y_{t+s}$ sobre o espa√ßo gerado por $\{\epsilon_t, \epsilon_{t-1}, \epsilon_{t-2}, ...\}$ √© dada por $\hat{Y}_{t+s|t} = \mu + \sum_{j=0}^{\infty} \psi_{s+j} \epsilon_{t-j}$ ‚ñ†

**Teorema 1**: Para um processo MA(‚àû) com representa√ß√£o $(Y_t - \mu) = \psi(L)\epsilon_t$, o erro de previs√£o $e_{t+s|t} = Y_{t+s} - \hat{Y}_{t+s|t}$ √© dado por $e_{t+s|t} = \sum_{j=0}^{s-1} \psi_j \epsilon_{t+s-j}$.

*Prova:*
I. A representa√ß√£o MA(‚àû) de $Y_{t+s}$ √©:
$$Y_{t+s} = \mu + \sum_{j=0}^{\infty} \psi_j \epsilon_{t+s-j}$$
II. A previs√£o linear √≥tima $\hat{Y}_{t+s|t}$ √©, do Lema 1:
$$ \hat{Y}_{t+s|t} = \mu + \sum_{j=0}^{\infty} \psi_{s+j} \epsilon_{t-j} $$
III. Para calcular o erro de previs√£o, subtra√≠mos a previs√£o da vari√°vel observada:
$$ e_{t+s|t} = Y_{t+s} - \hat{Y}_{t+s|t} $$
IV. Substituindo as express√µes de I e II na equa√ß√£o do erro:
$$ e_{t+s|t} = \mu + \sum_{j=0}^{\infty} \psi_j \epsilon_{t+s-j} - \left(\mu + \sum_{j=0}^{\infty} \psi_{s+j} \epsilon_{t-j}\right) $$
V. Cancelando os termos $\mu$:
$$ e_{t+s|t} = \sum_{j=0}^{\infty} \psi_j \epsilon_{t+s-j} - \sum_{j=0}^{\infty} \psi_{s+j} \epsilon_{t-j} $$
VI. Separando a primeira somat√≥ria nos termos de $j=0$ at√© $s-1$ e $j=s$ at√© $\infty$
$$ e_{t+s|t} = \sum_{j=0}^{s-1} \psi_j \epsilon_{t+s-j} + \sum_{j=s}^{\infty} \psi_j \epsilon_{t+s-j} - \sum_{j=0}^{\infty} \psi_{s+j} \epsilon_{t-j} $$
VII. Fazendo a mudan√ßa de √≠ndice na segunda e terceira somat√≥ria, com $k=j-s$, na segunda e terceira somat√≥ria temos:
$$ e_{t+s|t} = \sum_{j=0}^{s-1} \psi_j \epsilon_{t+s-j} + \sum_{k=0}^{\infty} \psi_{k+s} \epsilon_{t-k} - \sum_{j=0}^{\infty} \psi_{s+j} \epsilon_{t-j} $$
VIII. As duas √∫ltimas somat√≥rias s√£o iguais, e se cancelam:
$$ e_{t+s|t} = \sum_{j=0}^{s-1} \psi_j \epsilon_{t+s-j} $$
Que representa a soma ponderada dos erros futuros, n√£o utilizados na previs√£o $\hat{Y}_{t+s|t}$.  ‚ñ†

> üí° **Exemplo Num√©rico:** Usando o mesmo processo MA(‚àû) do exemplo anterior, com $\psi_j = (0.8)^j$. Queremos calcular o erro de previs√£o para 2 passos √† frente ($s=2$). Pelo Teorema 1, temos:
>
> $$ e_{t+2|t} = \sum_{j=0}^{2-1} \psi_j \epsilon_{t+2-j} = \psi_0 \epsilon_{t+2} + \psi_1 \epsilon_{t+1} =  \epsilon_{t+2} + 0.8\epsilon_{t+1} $$
>
> Note que o erro de previs√£o √© uma fun√ß√£o dos erros futuros, que n√£o s√£o utilizados na previs√£o $\hat{Y}_{t+2|t}$. Suponha que $\epsilon_{t+1} = 0.3$ e $\epsilon_{t+2} = -0.1$. Ent√£o
>
> $$ e_{t+2|t} = -0.1 + 0.8(0.3) = -0.1 + 0.24 = 0.14 $$
>
> Isso indica que a previs√£o $\hat{Y}_{t+2|t}$ est√° 0.14 unidades abaixo do valor real $Y_{t+2}$.

**Corol√°rio 1.1**: O erro de previs√£o para um passo √† frente (s=1) √© $e_{t+1|t} = \epsilon_{t+1}$.

*Prova:*
I. Pelo Teorema 1, o erro de previs√£o √© dado por:
$$e_{t+s|t} = \sum_{j=0}^{s-1} \psi_j \epsilon_{t+s-j}$$
II. Para uma previs√£o de um passo √† frente, $s=1$:
$$e_{t+1|t} = \sum_{j=0}^{1-1} \psi_j \epsilon_{t+1-j} = \sum_{j=0}^{0} \psi_j \epsilon_{t+1-j} $$
III. Isso resulta em apenas um termo na somat√≥ria, com $j=0$:
$$e_{t+1|t} = \psi_0 \epsilon_{t+1-0} = \psi_0 \epsilon_{t+1}$$
IV. Dado que $\psi_0 = 1$:
$$e_{t+1|t} = \epsilon_{t+1}$$
Portanto, o erro de previs√£o para um passo √† frente √© $e_{t+1|t} = \epsilon_{t+1}$ ‚ñ†

> üí° **Exemplo Num√©rico:** Usando os mesmos par√¢metros, o erro de previs√£o para um passo a frente √© simplesmente $e_{t+1|t} = \epsilon_{t+1}$.  Se $\epsilon_{t+1} = 0.5$, ent√£o o erro de previs√£o √© 0.5.

**Teorema 1.1**: A vari√¢ncia do erro de previs√£o $Var(e_{t+s|t})$ √© dada por $\sigma^2 \sum_{j=0}^{s-1} \psi_j^2$, onde $\sigma^2 = Var(\epsilon_t)$.

*Prova:*
I. Do Teorema 1, sabemos que o erro de previs√£o √©:
$$e_{t+s|t} = \sum_{j=0}^{s-1} \psi_j \epsilon_{t+s-j}$$
II. A vari√¢ncia do erro de previs√£o √©:
$$Var(e_{t+s|t}) = Var\left(\sum_{j=0}^{s-1} \psi_j \epsilon_{t+s-j}\right)$$
III. Usando a propriedade da vari√¢ncia de uma soma de vari√°veis independentes:
$$Var(e_{t+s|t}) = \sum_{j=0}^{s-1} Var(\psi_j \epsilon_{t+s-j})$$
IV. Como $\psi_j$ s√£o constantes:
$$Var(e_{t+s|t}) = \sum_{j=0}^{s-1} \psi_j^2 Var(\epsilon_{t+s-j})$$
V. Sabendo que $\epsilon_t$ s√£o i.i.d com vari√¢ncia $\sigma^2$, temos $Var(\epsilon_{t+s-j}) = \sigma^2$:
$$Var(e_{t+s|t}) = \sum_{j=0}^{s-1} \psi_j^2 \sigma^2$$
VI. Fatorando $\sigma^2$:
$$Var(e_{t+s|t}) = \sigma^2 \sum_{j=0}^{s-1} \psi_j^2$$
Portanto, a vari√¢ncia do erro de previs√£o √© dada por $\sigma^2 \sum_{j=0}^{s-1} \psi_j^2$ ‚ñ†

> üí° **Exemplo Num√©rico:** Considere novamente o processo MA(‚àû) com $\psi_j = (0.8)^j$ e $\sigma^2 = 1$. Para calcular a vari√¢ncia do erro de previs√£o para 2 passos √† frente ($s=2$):
>
> $$ Var(e_{t+2|t}) = \sigma^2 \sum_{j=0}^{2-1} \psi_j^2 = 1 \cdot (\psi_0^2 + \psi_1^2) = 1 \cdot (1^2 + (0.8)^2) = 1 + 0.64 = 1.64 $$
>
> E para um passo √† frente ($s=1$):
>
> $$ Var(e_{t+1|t}) = \sigma^2 \sum_{j=0}^{1-1} \psi_j^2 = 1 \cdot \psi_0^2 = 1 \cdot 1^2 = 1 $$
>
>  Isso mostra que a vari√¢ncia do erro aumenta com o n√∫mero de passos de previs√£o, e que o erro da previs√£o para um passo √† frente √© igual a vari√¢ncia de $\epsilon_t$.

### Conclus√£o

Nesta se√ß√£o, exploramos em detalhes como obter a **previs√£o linear √≥tima** para processos **MA(‚àû)**, focando na import√¢ncia de express√°-la como uma combina√ß√£o linear dos erros passados ($\epsilon$). Demonstramos como o uso do operador de defasagem e a substitui√ß√£o dos erros futuros por seus valores esperados (zero) s√£o fundamentais para esta constru√ß√£o. Em particular, o resultado fundamental deste t√≥pico √© que, na previs√£o linear √≥tima para um processo MA(‚àû), os erros futuros n√£o entram na equa√ß√£o da previs√£o, pois s√£o substitu√≠dos pelos seus valores esperados, que s√£o zero, destacando a relev√¢ncia apenas dos erros passados nesse contexto. Al√©m disso, derivamos formalmente o papel dos coeficientes $\psi_j$ na forma√ß√£o da previs√£o, e como o processo MA(q) √© um caso espec√≠fico do processo MA(‚àû).
A conex√£o com t√≥picos anteriores [^1] foi feita atrav√©s da retomada do conceito de previs√£o √≥tima condicional, que √© agora aplicado especificamente √† classe de processos MA(‚àû). Essa constru√ß√£o te√≥rica estabelece uma base s√≥lida para o estudo de modelos de s√©ries temporais mais complexos e a discuss√£o sobre como realizar previs√µes com um n√∫mero finito de observa√ß√µes, que ser√° explorada em t√≥picos posteriores.

### Refer√™ncias
[^1]: *Expression [4.1.1] is known as the mean squared error associated with the forecast... The forecast with the smallest mean squared error turns out to be the expectation of Y‚ÇÅ+1 conditional on X‚ÇÅ...*
[^5]: *Consider a process with an MA(‚àû) representation (Y, ‚Äì Œº) = œà(L)Œµ, with e, white noise and...*
[^6]: *...the optimal linear forecast is √ä[Y,+s|‚Ç¨, ‚Ç¨,-1,...] ... the optimal forecast could be written in lag operator notation as √ä[Y,+s|‚Ç¨, ‚Ç¨,-1,...] = Œº + [œà(L)/L^s]_+ Œµ‚ÇÅ*
<!-- END -->
