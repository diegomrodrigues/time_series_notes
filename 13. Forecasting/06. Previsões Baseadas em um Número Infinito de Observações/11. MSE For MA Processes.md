## An√°lise do Erro M√©dio Quadr√°tico de Previs√£o (MSE) em Processos MA(‚àû) e sua Converg√™ncia

### Introdu√ß√£o

Este cap√≠tulo se aprofunda na an√°lise do **Erro M√©dio Quadr√°tico de Previs√£o (MSE)** em modelos **MA(‚àû)**, explorando seu comportamento em rela√ß√£o ao horizonte de previs√£o e demonstrando sua converg√™ncia para a vari√¢ncia incondicional do processo sob certas condi√ß√µes. Baseando-se nos conceitos de previs√£o linear √≥tima, operadores de defasagem e erros de previs√£o previamente introduzidos [^1], [^5], [^6], o objetivo central √© fornecer uma compreens√£o rigorosa de como o MSE se comporta em modelos MA(‚àû) e como ele se relaciona com a vari√¢ncia do processo. Este cap√≠tulo tem um papel fundamental no entendimento dos limites da previsibilidade em modelos MA(‚àû) e na avalia√ß√£o da precis√£o das previs√µes ao longo do tempo.

### Conceitos Fundamentais

Como abordado em cap√≠tulos anteriores, um processo **MA(‚àû)** √© definido como [^5]:
$$ (Y_t - \mu) = \psi(L) \epsilon_t = \sum_{j=0}^{\infty} \psi_j L^j \epsilon_t $$
onde $\epsilon_t$ representa um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, e $\psi(L)$ √© um polin√¥mio de defasagem com $\psi_0 = 1$ e $\sum_{j=0}^\infty |\psi_j| < \infty$. A **previs√£o linear √≥tima** de $Y_{t+s}$ baseada em informa√ß√µes at√© o tempo $t$ √© dada por [^1]:
$$ \hat{Y}_{t+s|t} = \mu + \sum_{j=0}^{\infty} \psi_{s+j} \epsilon_{t-j} $$
que pode ser expressa em termos do operador de aniquila√ß√£o como [^6]:
$$
\hat{Y}_{t+s|t} = \mu + \left[ \frac{\psi(L)}{L^s} \right]_+ \epsilon_t
$$

O **erro de previs√£o** $e_{t+s|t}$ √© a diferen√ßa entre o valor real e sua previs√£o linear √≥tima:
$$ e_{t+s|t} = Y_{t+s} - \hat{Y}_{t+s|t} = \sum_{j=0}^{s-1} \psi_j \epsilon_{t+s-j} $$
Este erro √© composto apenas pelos erros futuros, n√£o utilizados na previs√£o, evidenciando a rela√ß√£o entre a estrutura do modelo e a natureza do seu erro de previs√£o.

O **Erro M√©dio Quadr√°tico de Previs√£o (MSE)**, que quantifica a precis√£o das previs√µes, √© o valor esperado do quadrado do erro de previs√£o:
$$ MSE = E[e_{t+s|t}^2] $$
Para o processo MA(‚àû), o MSE pode ser expresso como [^1]:
$$ MSE = \sigma^2 \sum_{j=0}^{s-1} \psi_j^2 $$
Este resultado crucial estabelece que o MSE √© dado pela soma ponderada dos quadrados dos coeficientes $\psi_j$ do modelo MA(‚àû), multiplicada pela vari√¢ncia do ru√≠do branco. Para um horizonte de previs√£o $s=1$, temos que o MSE √© igual a $\sigma^2$ , a vari√¢ncia do ru√≠do branco.

> üí° **Exemplo Num√©rico:** Considere um processo MA(‚àû) com $\psi_j = 0.7^j$ e $\sigma^2 = 0.5$. Vamos calcular o MSE para alguns horizontes de previs√£o:
>
> *   Para $s=1$: $MSE = 0.5 \cdot (0.7^0)^2 = 0.5$. Isso significa que para um passo √† frente, o MSE √© igual √† vari√¢ncia do ru√≠do branco.
> *   Para $s=2$: $MSE = 0.5 \cdot [(0.7^0)^2 + (0.7^1)^2] = 0.5 \cdot (1 + 0.49) = 0.745$. O MSE aumenta, indicando maior incerteza na previs√£o para dois passos √† frente.
> *   Para $s=3$: $MSE = 0.5 \cdot [(0.7^0)^2 + (0.7^1)^2 + (0.7^2)^2] = 0.5 \cdot (1 + 0.49 + 0.2401) = 0.86505$. O MSE continua a aumentar, mas o incremento √© menor do que no passo anterior.
>
> Este exemplo demonstra como o MSE aumenta com o horizonte de previs√£o, refletindo a crescente incerteza em prever eventos mais distantes no tempo.
>
> üí° **Exemplo Num√©rico:** Suponha um processo MA(1) com $\theta_1=0.4$ e $\sigma^2=1$. Lembre-se que para este processo $\psi_0=1$ e $\psi_1=\theta_1=0.4$ e $\psi_j=0$ para $j>1$. O MSE √© dado por $MSE = \sigma^2 \sum_{j=0}^{s-1} \psi_j^2$. Calculando o MSE para diferentes passos √† frente, temos:
> * Para $s=1$: $MSE = 1\cdot (1^2) = 1$.
> * Para $s=2$: $MSE = 1\cdot (1^2 + 0.4^2) = 1.16$.
> * Para $s=3$: $MSE = 1\cdot (1^2 + 0.4^2 + 0^2) = 1.16$.
> * Para $s=4$: $MSE = 1\cdot (1^2 + 0.4^2 + 0^2 + 0^2) = 1.16$.
>
> Note que para processos MA(1), o MSE se estabiliza para $s>1$, pois $\psi_j=0$ para $j>1$.

Um aspecto importante do MSE em processos MA(‚àû) √© a sua converg√™ncia para um valor limite √† medida que o horizonte de previs√£o se torna grande.  Este valor limite √© a vari√¢ncia incondicional do processo, como ser√° demonstrado a seguir.

> üí° **Exemplo Num√©rico:** Suponha um processo MA(‚àû) onde $\psi_j = (0.8)^j$ e $\sigma^2 = 1$. Vamos analisar o comportamento do MSE para diferentes horizontes de previs√£o:
>
> *   Para $s=1$: $MSE = 1 \cdot (0.8^0)^2 = 1$.
> *   Para $s=2$: $MSE = 1 \cdot (0.8^0)^2 + 1 \cdot (0.8^1)^2 = 1 + 0.64 = 1.64$.
> *   Para $s=3$: $MSE = 1 \cdot (0.8^0)^2 + 1 \cdot (0.8^1)^2 + 1 \cdot (0.8^2)^2 = 1 + 0.64 + 0.4096 = 2.0496$.
>
> Note que o MSE aumenta √† medida que o horizonte de previs√£o $s$ se torna maior. Se $s \rightarrow \infty$ o MSE tende a $1/(1-0.8^2)=2.7777$, como ser√° demonstrado formalmente a seguir.
>
> üí° **Exemplo Num√©rico:** Suponha um processo MA(1) com $\theta_1=0.6$ e $\sigma^2=2$. A express√£o do MSE √© dada por $MSE(s)=\sigma^2 \sum_{j=0}^{s-1} \psi_j^2$. Para este processo, temos  $\psi_j = \theta_1^j$ para $j=0, 1$. Assim, $\psi_0=1$ e $\psi_1=0.6$ . Para um passo a frente (s=1), o MSE ser√° igual a vari√¢ncia do erro: $MSE = 2$. Para dois passos a frente, $MSE=2(1^2 + 0.6^2)=2(1.36)=2.72$. Para tres passos a frente, o MSE ser√° igual a  $MSE=2(1^2 + 0.6^2 + 0^2)=2.72$. Note que, para o processo MA(1), o MSE √© constante para $s>1$.

**Lema 1.** *MSE e a Vari√¢ncia do Erro de Previs√£o.*
O Erro M√©dio Quadr√°tico de Previs√£o (MSE) √© igual √† vari√¢ncia do erro de previs√£o.
$$ MSE = E[e_{t+s|t}^2] = Var(e_{t+s|t}) $$
*Prova:*
I. O erro de previs√£o √© dado por:
$$ e_{t+s|t} = Y_{t+s} - \hat{Y}_{t+s|t} $$
II.  Como o erro de previs√£o tem m√©dia zero (a previs√£o √© n√£o viesada), temos que:
$$ E[e_{t+s|t}] = E[Y_{t+s} - \hat{Y}_{t+s|t}] = E[Y_{t+s}] - E[\hat{Y}_{t+s|t}] = 0 $$
III.  Por defini√ß√£o, a vari√¢ncia de $e_{t+s|t}$ √©:
 $$ Var(e_{t+s|t}) = E[(e_{t+s|t} - E[e_{t+s|t}])^2] $$
IV.  Como $E[e_{t+s|t}] = 0$, temos:
 $$ Var(e_{t+s|t}) = E[e_{t+s|t}^2] = MSE $$
Portanto, o MSE √© igual √† vari√¢ncia do erro de previs√£o. $\blacksquare$

O Lema 1 demonstra que, em termos pr√°ticos, calcular o MSE equivale a calcular a vari√¢ncia do erro de previs√£o. Este resultado √© uma consequ√™ncia direta da propriedade de n√£o-viesamento das previs√µes lineares √≥timas, o que implica que o erro de previs√£o tem m√©dia zero.

**Lema 1.1** *Ortogonalidade entre o erro de previs√£o e a previs√£o.* O erro de previs√£o $e_{t+s|t}$ √© ortogonal √† previs√£o $\hat{Y}_{t+s|t}$, ou seja, $Cov(e_{t+s|t}, \hat{Y}_{t+s|t}) = 0$.

*Prova:*
I. A previs√£o linear √≥tima $\hat{Y}_{t+s|t}$ √© baseada em informa√ß√µes at√© o tempo $t$ e o erro de previs√£o $e_{t+s|t}$ √© composto por inova√ß√µes futuras.
II.  A covari√¢ncia entre o erro de previs√£o e a previs√£o pode ser expressa como:
$$Cov(e_{t+s|t}, \hat{Y}_{t+s|t}) = E[(e_{t+s|t} - E[e_{t+s|t}])(\hat{Y}_{t+s|t} - E[\hat{Y}_{t+s|t}])]$$
III. Como $E[e_{t+s|t}] = 0$ e usando o fato de que  $\hat{Y}_{t+s|t}$  √© fun√ß√£o das inova√ß√µes at√© o tempo $t$ e $e_{t+s|t}$ √© fun√ß√£o das inova√ß√µes de $t+1$ at√© $t+s$:
$$Cov(e_{t+s|t}, \hat{Y}_{t+s|t}) = E[e_{t+s|t}\hat{Y}_{t+s|t}]$$
IV.  Substituindo as express√µes para $e_{t+s|t}$ e $\hat{Y}_{t+s|t}$:
$$Cov(e_{t+s|t}, \hat{Y}_{t+s|t}) = E\left[\left( \sum_{j=0}^{s-1} \psi_j \epsilon_{t+s-j} \right) \left( \mu + \sum_{k=0}^{\infty} \psi_{s+k} \epsilon_{t-k} \right)\right] $$
V. Como os erros $\epsilon_t$ s√£o n√£o correlacionados (ru√≠do branco), temos que a esperan√ßa do produto das somas √© zero:
$$ E[\epsilon_{t+i} \epsilon_{t+j}]=0 \quad  \forall i\neq j$$
VI. Logo, todos os termos da esperan√ßa acima se anulam. Portanto:
$$Cov(e_{t+s|t}, \hat{Y}_{t+s|t}) = 0$$
O que demonstra que o erro de previs√£o e a previs√£o s√£o ortogonais.  $\blacksquare$

**Teorema 1.** *Converg√™ncia do MSE para a Vari√¢ncia Incondicional em Modelos MA(‚àû)*
Se $\sum_{j=0}^{\infty} \psi_j^2 < \infty$, ent√£o o Erro M√©dio Quadr√°tico de Previs√£o (MSE) converge para a vari√¢ncia incondicional do processo quando o horizonte de previs√£o $s$ tende ao infinito. Este limite √© dado por:
$$ \lim_{s \to \infty} MSE(s) = \sigma^2 \sum_{j=0}^{\infty} \psi_j^2 $$

*Prova:*
I.  O MSE para um horizonte de previs√£o s √© dado por:
$$MSE(s) = \sigma^2 \sum_{j=0}^{s-1} \psi_j^2$$
II.  Para analisar a converg√™ncia do MSE quando $s \to \infty$, tomamos o limite da express√£o acima:
$$ \lim_{s \to \infty} MSE(s) = \lim_{s \to \infty} \sigma^2 \sum_{j=0}^{s-1} \psi_j^2 $$
III. Pela propriedade do limite de uma soma, podemos levar o limite para dentro da somat√≥ria:
$$ \lim_{s \to \infty} MSE(s) = \sigma^2 \lim_{s \to \infty} \sum_{j=0}^{s-1} \psi_j^2 $$
IV. Como a soma dos quadrados dos coeficientes $\psi_j$ converge para um valor finito (por hip√≥tese $\sum_{j=0}^{\infty} \psi_j^2 < \infty$), o limite da soma parcial √© a soma infinita:
$$ \lim_{s \to \infty} \sum_{j=0}^{s-1} \psi_j^2 = \sum_{j=0}^{\infty} \psi_j^2 $$
V. Portanto, substituindo esse resultado, obtemos:
$$ \lim_{s \to \infty} MSE(s) = \sigma^2 \sum_{j=0}^{\infty} \psi_j^2 $$
O Teorema 1 estabelece que o MSE converge para um valor finito quando o horizonte de previs√£o tende ao infinito. Este valor limite √© precisamente a vari√¢ncia incondicional do processo.  Note que a converg√™ncia do MSE para a vari√¢ncia incondicional √© uma condi√ß√£o de estabilidade do processo MA(‚àû). Para esta converg√™ncia ser v√°lida, √© necess√°rio que $\sum_{j=0}^\infty \psi_j^2 < \infty$, o que √© uma restri√ß√£o menos forte que a condi√ß√£o que garante que $\sum_{j=0}^\infty |\psi_j| < \infty$. $\blacksquare$

**Teorema 1.1** *Rela√ß√£o entre MSE, vari√¢ncia do processo e covari√¢ncia entre $Y_{t+s}$ e $\hat{Y}_{t+s|t}$*
A vari√¢ncia do processo $Y_{t+s}$ pode ser decomposta em duas partes: a vari√¢ncia do erro de previs√£o e a vari√¢ncia da previs√£o. Formalmente:
$$Var(Y_{t+s})=MSE + Var(\hat{Y}_{t+s|t})$$

*Prova:*
I. A vari√¢ncia de $Y_{t+s}$ pode ser expressa como:
$$Var(Y_{t+s}) = E[(Y_{t+s} - E[Y_{t+s}])^2] = E[(Y_{t+s} - \mu)^2]$$
II. Usando a defini√ß√£o do erro de previs√£o, temos que $Y_{t+s} = \hat{Y}_{t+s|t} + e_{t+s|t}$. Substituindo na equa√ß√£o da vari√¢ncia, temos:
$$Var(Y_{t+s}) = E[(\hat{Y}_{t+s|t} + e_{t+s|t} - \mu)^2] = E[(\hat{Y}_{t+s|t} - \mu + e_{t+s|t})^2]$$
III. Expandindo o quadrado e usando que o erro de previs√£o √© n√£o viesado (E[$e_{t+s|t}$]=0):
$$Var(Y_{t+s}) = E[(\hat{Y}_{t+s|t} - \mu)^2 + 2(\hat{Y}_{t+s|t}-\mu)e_{t+s|t} + e_{t+s|t}^2]$$
IV. Pelo Lema 1.1, o erro e a previs√£o s√£o ortogonais,  $Cov(e_{t+s|t}, \hat{Y}_{t+s|t}) = 0$, o que implica que o termo cruzado na esperan√ßa √© zero. Assim:
$$Var(Y_{t+s}) =  E[(\hat{Y}_{t+s|t} - \mu)^2] + E[e_{t+s|t}^2]$$
V. Lembrando que  $MSE=E[e_{t+s|t}^2]$ e que  $Var(\hat{Y}_{t+s|t}) = E[(\hat{Y}_{t+s|t} - E[\hat{Y}_{t+s|t}])^2]= E[(\hat{Y}_{t+s|t} - \mu)^2]$, obtemos:
$$Var(Y_{t+s})=MSE + Var(\hat{Y}_{t+s|t})$$
O Teorema 1.1 mostra que a vari√¢ncia do processo √© a soma do MSE (vari√¢ncia do erro de previs√£o) e da vari√¢ncia da previs√£o, mostrando a decomposi√ß√£o da vari√¢ncia total do processo em partes relacionadas com a informa√ß√£o dispon√≠vel no tempo $t$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um processo MA(‚àû) onde $\psi_j = (0.5)^j$ e $\sigma^2 = 1$. A vari√¢ncia incondicional do processo √© dada por:
> $$Var(Y_t) = \sigma^2 \sum_{j=0}^\infty \psi_j^2 = 1 \sum_{j=0}^\infty (0.5)^{2j} = \sum_{j=0}^\infty (0.25)^j = \frac{1}{1 - 0.25} = \frac{1}{0.75} = \frac{4}{3}$$
> Ent√£o, o limite do MSE √© 4/3, o que implica que o erro de previs√£o se estabiliza em um valor m√°ximo bem definido.
>
> üí° **Exemplo Num√©rico:**  Vamos ilustrar a converg√™ncia do MSE com um exemplo simulado. Suponha um modelo MA(‚àû) com $\psi_j=0.9^j$, $\mu=0$ e $\sigma^2=1$. Vamos calcular o MSE para horizontes de previs√£o $s=1$ a $s=20$ e mostrar que o MSE converge para um valor finito quando $s \to \infty$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros do modelo
> sigma2 = 1
> psi = lambda j: 0.9**j
>
> # Calculando o MSE para diferentes horizontes de previs√£o
> horizons = np.arange(1, 21)
> mse_values = [sigma2 * np.sum(psi(np.arange(0, s))**2) for s in horizons]
>
> # C√°lculo do valor limite do MSE
> mse_limit = sigma2/(1-0.9**2)
>
> # Plotando os resultados
> plt.figure(figsize=(10, 6))
> plt.plot(horizons, mse_values, marker='o', linestyle='-', label='MSE(s)')
> plt.axhline(y=mse_limit, color='r', linestyle='--', label=f'Limite MSE: {mse_limit:.4f}')
> plt.xlabel('Horizonte de Previs√£o (s)')
> plt.ylabel('MSE')
> plt.title('Comportamento do MSE em um Modelo MA(‚àû) com a Converg√™ncia')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> Este gr√°fico demonstra como o MSE aumenta para horizontes de previs√£o pequenos e se estabiliza em um valor m√°ximo ao redor de 5.26 quando o horizonte de previs√£o se torna muito grande, mostrando como a vari√¢ncia do erro de previs√£o se comporta em modelos MA(‚àû).

O Teorema 1 formaliza a intui√ß√£o de que, em modelos MA(‚àû), o erro de previs√£o n√£o cresce indefinidamente e o MSE converge para um limite bem definido que √© a vari√¢ncia incondicional do processo, mostrando o limite de previsibilidade da estrutura MA(‚àû). A converg√™ncia do MSE tem implica√ß√µes pr√°ticas importantes para a an√°lise da precis√£o das previs√µes em modelos MA(‚àû), indicando que a variabilidade dos erros de previs√£o n√£o cresce indefinidamente.

### Conclus√£o

Neste cap√≠tulo, exploramos o comportamento do **Erro M√©dio Quadr√°tico de Previs√£o (MSE)** em modelos **MA(‚àû)**, enfatizando sua rela√ß√£o com o horizonte de previs√£o e sua converg√™ncia para a vari√¢ncia incondicional do processo. Foi demonstrado formalmente que o MSE √© igual a vari√¢ncia do erro de previs√£o, dado pela soma dos quadrados dos coeficientes do modelo multiplicados pela vari√¢ncia do ru√≠do branco. Atrav√©s da aplica√ß√£o de resultados de √°lgebra e do uso de operadores de defasagem e aniquila√ß√£o, foi demonstrado que, mesmo que o MSE aumente com o horizonte de previs√£o, ele converge para um valor finito quando o horizonte de previs√£o tende ao infinito, desde que a soma dos quadrados dos coeficientes $\psi_j$ seja finita.
A rela√ß√£o com conceitos de cap√≠tulos anteriores foi feita atrav√©s da utiliza√ß√£o das defini√ß√µes de previs√£o linear √≥tima, do operador de defasagem, operador de aniquila√ß√£o e do erro de previs√£o em modelos MA(‚àû). Os resultados apresentados s√£o de fundamental import√¢ncia para a compreens√£o das propriedades de previsibilidade dos modelos MA(‚àû) e a constru√ß√£o de previs√µes mais precisas e robustas, e ser√£o usados em cap√≠tulos posteriores no desenvolvimento de resultados para modelos mais complexos.

### Refer√™ncias
[^1]: *Expression [4.1.1] is known as the mean squared error associated with the forecast... The forecast with the smallest mean squared error turns out to be the expectation of Y‚ÇÅ+1 conditional on X‚ÇÅ...*
[^5]: *Consider a process with an MA(‚àû) representation (Y, ‚Äì Œº) = œà(L)Œµ, with e, white noise and...*
[^6]: *...the optimal linear forecast is √ä[Y,+s|‚Ç¨, ‚Ç¨,-1,...] ... the optimal forecast could be written in lag operator notation as √ä[Y,+s|‚Ç¨, ‚Ç¨,-1,...] = Œº + [œà(L)/L^s]_+ Œµ‚ÇÅ*
<!-- END -->
