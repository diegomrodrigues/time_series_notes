## Previs√£o Linear √ìtima e Erro de Previs√£o em Modelos MA(‚àû)

### Introdu√ß√£o

Este cap√≠tulo se dedica √† an√°lise detalhada da **previs√£o linear √≥tima** em modelos **MA(‚àû)**, com foco particular na constru√ß√£o da previs√£o e na an√°lise do seu **erro de previs√£o** e o **Erro M√©dio Quadr√°tico de Previs√£o (MSE)**. Expandindo os conceitos previamente introduzidos sobre modelos MA(‚àû), operadores de defasagem e aniquila√ß√£o [^1], [^5], [^6], o objetivo central √© fornecer uma compreens√£o rigorosa de como a informa√ß√£o passada √© utilizada para construir a previs√£o √≥tima e como o erro de previs√£o evolui com o horizonte de previs√£o. Atrav√©s da aplica√ß√£o do operador de aniquila√ß√£o e de ferramentas de √°lgebra, vamos demonstrar formalmente como o erro de previs√£o √© uma fun√ß√£o dos ru√≠dos futuros e como a vari√¢ncia deste erro pode ser calculada.

### Conceitos Fundamentais

Retomando os conceitos de cap√≠tulos anteriores, um processo **MA(‚àû)** pode ser definido como [^5]:
$$ (Y_t - \mu) = \psi(L) \epsilon_t = \sum_{j=0}^{\infty} \psi_j L^j \epsilon_t $$
onde $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, e $\psi(L)$ √© um polin√¥mio de defasagem tal que  $\psi_0 = 1$ e $\sum_{j=0}^\infty |\psi_j| < \infty$ [^5].

A previs√£o linear √≥tima de $Y_{t+s}$, denotada por $\hat{Y}_{t+s|t}$, √© obtida projetando $Y_{t+s}$ sobre o espa√ßo gerado por $\epsilon_t, \epsilon_{t-1}, \epsilon_{t-2},...$ ou, equivalentemente, tomando a esperan√ßa condicional de $Y_{t+s}$ dadas as informa√ß√µes dispon√≠veis at√© o instante $t$ [^1]. A express√£o para $Y_{t+s}$ √© [^5]:
$$Y_{t+s} = \mu + \epsilon_{t+s} + \psi_1\epsilon_{t+s-1} + \ldots + \psi_{s-1}\epsilon_{t+1} + \psi_s \epsilon_t + \psi_{s+1}\epsilon_{t-1} + \ldots$$

A **previs√£o linear √≥tima** √© obtida ao substituir todos os valores futuros de $\epsilon$ por seu valor esperado, que √© zero, resultando em [^5]:
$$
\hat{Y}_{t+s|t} = \mu + \psi_s\epsilon_t + \psi_{s+1}\epsilon_{t-1} + \psi_{s+2}\epsilon_{t-2} + \ldots
$$

Nesse ponto √© importante ressaltar que a previs√£o linear √≥tima expressa acima, utiliza todos os valores de Œµ dispon√≠veis no passado. Para expressar a previs√£o √≥tima usando a nota√ß√£o do operador de defasagem e do operador de aniquila√ß√£o,  podemos escrever [^6]:
$$
\hat{Y}_{t+s|t} = \mu + \left[ \frac{\psi(L)}{L^s} \right]_+ \epsilon_t
$$

Como visto anteriormente, o operador $\left[ \frac{\psi(L)}{L^s} \right]_+$ descarta os termos com expoentes negativos de $L$, que correspondem a valores futuros de $\epsilon$, substituindo-os por zero, conforme a constru√ß√£o da proje√ß√£o linear. Isso √© essencial para garantir que a previs√£o use apenas as informa√ß√µes dispon√≠veis at√© o instante $t$ e, portanto, o erro de previs√£o seja composto somente por erros futuros.

O **erro de previs√£o** $e_{t+s|t}$ √© definido como a diferen√ßa entre o valor real de $Y_{t+s}$ e sua previs√£o linear √≥tima $\hat{Y}_{t+s|t}$:
$$ e_{t+s|t} = Y_{t+s} - \hat{Y}_{t+s|t} $$

Substituindo as express√µes de $Y_{t+s}$ e $\hat{Y}_{t+s|t}$ na equa√ß√£o do erro de previs√£o, temos:

$$ e_{t+s|t} =  \epsilon_{t+s} + \psi_1 \epsilon_{t+s-1} + \dots + \psi_{s-1}\epsilon_{t+1} $$
Note que o erro de previs√£o √© composto apenas por uma soma ponderada de erros futuros, que, por defini√ß√£o, n√£o fazem parte das informa√ß√µes utilizadas para construir a previs√£o. Este resultado evidencia a rela√ß√£o entre a estrutura do modelo MA(‚àû) e a natureza do seu erro de previs√£o.

O **Erro M√©dio Quadr√°tico de Previs√£o (MSE)** √© uma m√©trica usada para avaliar a precis√£o de uma previs√£o, sendo definido como o valor esperado do quadrado do erro de previs√£o:

$$MSE = E[e_{t+s|t}^2]$$

No caso de um processo MA(‚àû), ao substituir a express√£o para o erro de previs√£o e utilizar a propriedade de que os erros s√£o n√£o correlacionados e t√™m vari√¢ncia $\sigma^2$, obtemos:
$$ MSE = E[(\epsilon_{t+s} + \psi_1 \epsilon_{t+s-1} + \ldots + \psi_{s-1} \epsilon_{t+1})^2] = (1 + \psi_1^2 + \ldots + \psi_{s-1}^2) \sigma^2  $$

Esta express√£o para o MSE mostra que ele √© igual √† vari√¢ncia do erro de previs√£o e que aumenta com o horizonte de previs√£o $s$, pois acumula os quadrados dos coeficientes $\psi_j$ at√© $s-1$. Isso significa que o erro de previs√£o tende a ser maior para horizontes de previs√£o maiores. A converg√™ncia deste MSE ser√° analisada em detalhes mais √† frente neste texto.

> üí° **Exemplo Num√©rico:** Considere um processo MA(‚àû) com $\mu = 10$, onde $\psi_j = (0.7)^j$ e $\sigma^2 = 2$. Vamos calcular a previs√£o, o erro de previs√£o e o MSE para $s=2$.
>
> Primeiro, vamos explicitar $Y_{t+2}$:
> $$Y_{t+2} = 10 + \epsilon_{t+2} + 0.7 \epsilon_{t+1} + (0.7)^2 \epsilon_t + (0.7)^3 \epsilon_{t-1} + \dots$$
>
> A previs√£o linear √≥tima $\hat{Y}_{t+2|t}$ √©:
> $$ \hat{Y}_{t+2|t} = 10 + (0.7)^2 \epsilon_t + (0.7)^3 \epsilon_{t-1} +  \dots $$
>
> O erro de previs√£o $e_{t+2|t}$ √© dado por:
> $$ e_{t+2|t} = \epsilon_{t+2} + 0.7 \epsilon_{t+1} $$
>
> O MSE para $s=2$ √©:
> $$ MSE =  (1 + 0.7^2)\sigma^2 = (1 + 0.49) \times 2 = 2.98 $$
>
> Note que $\mu$ aparece na express√£o de $Y_{t+s}$ e $\hat{Y}_{t+s|t}$, mas n√£o influencia o erro de previs√£o nem o MSE.
>
> üí° **Exemplo Num√©rico:**  Para o mesmo processo do exemplo anterior, considere que  $\epsilon_{t+1} = -1$ e $\epsilon_{t+2} = 2$. O erro de previs√£o para $s=2$ ser√° $e_{t+2|t} = \epsilon_{t+2} + 0.7 \epsilon_{t+1} = 2 + 0.7(-1) = 1.3$. Usando o MSE do exemplo anterior (2.98) e o erro de previs√£o (1.3), podemos dizer que o erro est√° dentro do esperado, no sentido de que, em m√©dia, o quadrado do erro de previs√£o deve ser 2.98.  Se, por exemplo, tiv√©ssemos 100 erros de previs√£o, o valor m√©dio de seus quadrados deveria se aproximar de 2.98.

**Lema 1.** *Erro de Previs√£o para um MA(‚àû)*
O erro de previs√£o $e_{t+s|t}$ em um modelo MA(‚àû) √© dado por uma combina√ß√£o linear dos erros futuros  $\epsilon_{t+1}$ at√© $\epsilon_{t+s}$, com coeficientes correspondentes aos pesos do modelo MA(‚àû) de 1 a s-1:

$$ e_{t+s|t} =  \epsilon_{t+s} + \psi_1 \epsilon_{t+s-1} + \dots + \psi_{s-1}\epsilon_{t+1} =  \sum_{j=0}^{s-1} \psi_j \epsilon_{t+s-j} $$

*Prova:*
I. A defini√ß√£o do erro de previs√£o √© $e_{t+s|t} = Y_{t+s} - \hat{Y}_{t+s|t}$.
II. A representa√ß√£o de $Y_{t+s}$ √© dada por:
   $$Y_{t+s} = \mu + \sum_{j=0}^{\infty} \psi_j \epsilon_{t+s-j} = \mu + \sum_{j=0}^{s-1} \psi_j \epsilon_{t+s-j} + \sum_{j=s}^{\infty} \psi_j \epsilon_{t+s-j}$$
III. A previs√£o linear √≥tima  $\hat{Y}_{t+s|t}$ √©:
    $$\hat{Y}_{t+s|t} = \mu + \sum_{j=s}^{\infty} \psi_j \epsilon_{t+s-j} = \mu + \psi_s \epsilon_t + \psi_{s+1} \epsilon_{t-1} + \ldots $$
IV. Substituindo as express√µes de $Y_{t+s}$ e $\hat{Y}_{t+s|t}$ na defini√ß√£o do erro de previs√£o:
$$e_{t+s|t} = \left[ \mu + \sum_{j=0}^{s-1} \psi_j \epsilon_{t+s-j} + \sum_{j=s}^{\infty} \psi_j \epsilon_{t+s-j} \right] - \left[ \mu + \sum_{j=s}^{\infty} \psi_j \epsilon_{t+s-j}\right]$$
V. Cancelando os termos $\mu$ e a segunda somat√≥ria, obtemos:
$$ e_{t+s|t} = \sum_{j=0}^{s-1} \psi_j \epsilon_{t+s-j}  = \epsilon_{t+s} + \psi_1 \epsilon_{t+s-1} + \ldots + \psi_{s-1}\epsilon_{t+1} $$
Portanto, o erro de previs√£o √© uma soma ponderada dos erros futuros $\epsilon_{t+1},...,\epsilon_{t+s}$. ‚ñ†

**Teorema 1.** *MSE em um modelo MA(‚àû)*
O Erro M√©dio Quadr√°tico de Previs√£o (MSE) em um modelo MA(‚àû) para um horizonte de previs√£o s √© dado por:
$$ MSE = E[e_{t+s|t}^2] = \sigma^2\sum_{j=0}^{s-1} \psi_j^2 $$
*Prova:*
I. O erro de previs√£o √© dado pelo Lema 1:
$$e_{t+s|t} = \sum_{j=0}^{s-1} \psi_j \epsilon_{t+s-j}$$
II. O MSE √© definido como $MSE = E[e_{t+s|t}^2]$
III. Substituindo a express√£o do erro de previs√£o temos:
$$MSE = E\left[\left(\sum_{j=0}^{s-1} \psi_j \epsilon_{t+s-j}\right)^2\right]$$
IV. Expans√£o do termo ao quadrado:
$$MSE = E\left[\sum_{j=0}^{s-1}\sum_{k=0}^{s-1} \psi_j \psi_k \epsilon_{t+s-j}\epsilon_{t+s-k}\right]$$
V. Pela linearidade do operador de esperan√ßa:
$$MSE = \sum_{j=0}^{s-1}\sum_{k=0}^{s-1} \psi_j \psi_k E\left[\epsilon_{t+s-j}\epsilon_{t+s-k}\right]$$
VI. Dado que o ru√≠do branco $\epsilon$ √© n√£o correlacionado, $E\left[\epsilon_{t+s-j}\epsilon_{t+s-k}\right] = 0$ para $j \neq k$. Tamb√©m sabemos que a vari√¢ncia de $\epsilon_t$ √© $\sigma^2$, portanto $E\left[\epsilon_{t+s-j}^2\right] = \sigma^2$:
$$MSE = \sum_{j=0}^{s-1} \psi_j^2 \sigma^2$$
VII. Fatorando $\sigma^2$, temos
$$MSE = \sigma^2 \sum_{j=0}^{s-1} \psi_j^2$$
O resultado final demonstra que o MSE √© dado pela soma ponderada (pelos quadrados dos coeficientes do modelo) da vari√¢ncia dos erros futuros, e aumenta com o horizonte de previs√£o, dado que esta soma √© feita at√© $s-1$. ‚ñ†

> üí° **Exemplo Num√©rico:**  Vamos retomar o processo MA(‚àû) do exemplo anterior, onde $\psi_j = (0.7)^j$ e $\sigma^2 = 2$. Para calcular o MSE para $s=1$, $s=2$ e $s=3$.
> *   **MSE para s=1**: $MSE = \sigma^2 \sum_{j=0}^{0} \psi_j^2  = 2 (0.7^0)^2 = 2(1) = 2$.
> *   **MSE para s=2**:  $MSE = \sigma^2 \sum_{j=0}^{1} \psi_j^2 = 2 [(0.7^0)^2 + (0.7^1)^2]  = 2 (1 + 0.49) = 2.98 $.
> *   **MSE para s=3**: $MSE = \sigma^2 \sum_{j=0}^{2} \psi_j^2 = 2 [(0.7^0)^2 + (0.7^1)^2 +(0.7^2)^2] = 2(1 + 0.49 + 0.2401) = 3.4602 $.
>
> Observe que o MSE aumenta com o horizonte de previs√£o, conforme demonstrado no Teorema 1. Para s=1, o MSE √© igual √† vari√¢ncia do ru√≠do branco, como esperado.

O teorema 1 estabelece a express√£o para o MSE, demonstrando como a vari√¢ncia do erro de previs√£o √© influenciada pelos coeficientes do modelo $\psi_j$ e pelo horizonte de previs√£o $s$.

**Teorema 1.1** *Converg√™ncia do MSE em um Modelo MA(‚àû)*
Se $\sum_{j=0}^{\infty} \psi_j^2 < \infty$, ent√£o o Erro M√©dio Quadr√°tico de Previs√£o (MSE) converge para um limite finito quando o horizonte de previs√£o $s$ tende ao infinito. Este limite √© dado por:
$$ \lim_{s \to \infty} MSE(s) = \sigma^2 \sum_{j=0}^{\infty} \psi_j^2 $$

*Prova:*
I. Do Teorema 1, sabemos que o MSE para um horizonte de previs√£o $s$ √© dado por:
$$MSE(s) = \sigma^2 \sum_{j=0}^{s-1} \psi_j^2$$
II. Para analisar a converg√™ncia do MSE quando $s \to \infty$, tomamos o limite da express√£o acima:
$$ \lim_{s \to \infty} MSE(s) = \lim_{s \to \infty} \sigma^2 \sum_{j=0}^{s-1} \psi_j^2 $$
III. Pela propriedade do limite de uma soma, podemos levar o limite para dentro da somat√≥ria:
$$ \lim_{s \to \infty} MSE(s) = \sigma^2 \lim_{s \to \infty} \sum_{j=0}^{s-1} \psi_j^2 $$
IV. Como $\sum_{j=0}^{\infty} \psi_j^2$ converge (por hip√≥tese), o limite da soma parcial √© a soma infinita:
$$\lim_{s \to \infty} \sum_{j=0}^{s-1} \psi_j^2 = \sum_{j=0}^{\infty} \psi_j^2 $$
V. Portanto, substituindo esse resultado, obtemos:
$$ \lim_{s \to \infty} MSE(s) = \sigma^2 \sum_{j=0}^{\infty} \psi_j^2 $$

Este teorema demonstra que mesmo que o MSE aumente com o horizonte de previs√£o, ele converge para um valor finito se a soma dos quadrados dos coeficientes $\psi_j$ for convergente. Isso √© uma caracter√≠stica importante dos modelos MA(‚àû) e garante que, mesmo com horizontes de previs√£o muito longos, o erro de previs√£o n√£o cres√ßa indefinidamente, mas se estabilize em um valor m√°ximo.

> üí° **Exemplo Num√©rico:** Retomando o exemplo com $\psi_j = (0.7)^j$ e $\sigma^2 = 2$, podemos calcular o limite do MSE quando $s \to \infty$:
>
> $$\lim_{s \to \infty} MSE(s) = \sigma^2 \sum_{j=0}^{\infty} (0.7)^{2j} = 2 \sum_{j=0}^{\infty} (0.49)^j $$
>
> Esta √© uma s√©rie geom√©trica com raz√£o $0.49$, portanto sua soma √©:
>
> $$ \lim_{s \to \infty} MSE(s) = 2 \times \frac{1}{1-0.49} = \frac{2}{0.51} \approx 3.9216 $$
>
> Isso significa que, para este modelo espec√≠fico, o MSE se estabiliza em torno de 3.9216 quando o horizonte de previs√£o se torna muito grande.

A converg√™ncia do MSE √© uma propriedade crucial para garantir a aplicabilidade pr√°tica dos modelos MA(‚àû) em cen√°rios de longo prazo, pois demonstra que, embora o erro aumente com o horizonte de previs√£o, ele n√£o explode para o infinito, e pode ser calculado um limite superior para o erro de previs√£o.

### Conclus√£o

Neste cap√≠tulo, exploramos a previs√£o linear √≥tima para processos **MA(‚àû)**, com um foco particular na natureza do erro de previs√£o e na deriva√ß√£o da express√£o para o MSE. Demonstramos que a previs√£o linear √≥tima √© obtida substituindo os erros futuros por seu valor esperado, e o erro de previs√£o resultante √© uma soma ponderada dos erros futuros. Derivamos formalmente o MSE e mostramos como ele aumenta com o horizonte de previs√£o. A an√°lise detalhada da proje√ß√£o linear e do erro de previs√£o fornece uma compreens√£o mais profunda da din√¢mica preditiva em modelos MA(‚àû).
O uso do operador de aniquila√ß√£o para expressar a previs√£o e o erro de previs√£o em termos dos ru√≠dos futuros simplifica a an√°lise.
Esses resultados complementam as discuss√µes anteriores sobre previs√µes e operadores de defasagem, consolidando a teoria necess√°ria para o estudo de modelos de s√©ries temporais mais avan√ßados, bem como a sua aplica√ß√£o em contextos onde o n√∫mero de observa√ß√µes √© infinito. A deriva√ß√£o e an√°lise da express√£o do MSE, que ser√° usada em outros cap√≠tulos, √© de fundamental import√¢ncia para construir modelos preditivos em s√©ries temporais.

### Refer√™ncias
[^1]: *Expression [4.1.1] is known as the mean squared error associated with the forecast... The forecast with the smallest mean squared error turns out to be the expectation of Y‚ÇÅ+1 conditional on X‚ÇÅ...*
[^5]: *Consider a process with an MA(‚àû) representation (Y, ‚Äì Œº) = œà(L)Œµ, with e, white noise and...*
[^6]: *...the optimal linear forecast is √ä[Y,+s|‚Ç¨, ‚Ç¨,-1,...] ... the optimal forecast could be written in lag operator notation as √ä[Y,+s|‚Ç¨, ‚Ç¨,-1,...] = Œº + [œà(L)/L^s]_+ Œµ‚ÇÅ*
<!-- END -->
