## Previs√µes √ìtimas em Modelos MA(q): Uma An√°lise Detalhada com Erros Passados

### Introdu√ß√£o

Este cap√≠tulo se aprofunda na an√°lise da **previs√£o linear √≥tima** para modelos **MA(q)**, com um foco particular em como a previs√£o √© constru√≠da utilizando todos os erros conhecidos do passado, e como a previs√£o converge para a m√©dia incondicional da s√©rie para horizontes de previs√£o al√©m de $q$. Expandindo sobre os conceitos de previs√£o linear √≥tima, operadores de defasagem e operador de aniquila√ß√£o apresentados em cap√≠tulos anteriores [^1], [^5], [^6], o objetivo principal √© fornecer uma compreens√£o completa do processo de previs√£o em modelos MA(q), com uma √™nfase na utiliza√ß√£o de todos os erros passados na constru√ß√£o da previs√£o e na an√°lise do comportamento da previs√£o para horizontes maiores que a ordem do modelo, culminando com a converg√™ncia para a m√©dia da s√©rie.

### Conceitos Fundamentais

Como visto em cap√≠tulos anteriores, a previs√£o linear √≥tima para um processo com representa√ß√£o **MA(‚àû)** √© constru√≠da usando todos os erros passados, expressa como uma combina√ß√£o linear dos erros, onde os erros futuros s√£o substitu√≠dos por seus valores esperados, que √© zero. Em particular, para modelos **MA(q)**, esta combina√ß√£o linear assume uma forma finita devido √† natureza limitada da depend√™ncia temporal.

Consideremos um processo **MA(q)** dado por:
$$ (Y_t - \mu) = \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q} = \sum_{j=0}^q \theta_j \epsilon_{t-j} $$
onde $\epsilon_t$ representa um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, e $\theta_0 = 1$.

A representa√ß√£o para um instante futuro $t+s$ √©:
$$ Y_{t+s} = \mu + \epsilon_{t+s} + \theta_1 \epsilon_{t+s-1} + \theta_2 \epsilon_{t+s-2} + \dots + \theta_q \epsilon_{t+s-q} $$

A previs√£o linear √≥tima de $Y_{t+s}$ no instante $t$, denotada como $\hat{Y}_{t+s|t}$, √© dada pela esperan√ßa condicional de $Y_{t+s}$ com rela√ß√£o a todas as informa√ß√µes dispon√≠veis at√© o instante $t$. Especificamente, o modelo MA(q) representa um processo que √© fun√ß√£o dos erros de inova√ß√£o $\epsilon$ at√© o tempo $t$. Assim, a previs√£o de $Y_{t+s}$ ser√° fun√ß√£o de $\epsilon_t, \epsilon_{t-1},...$ que comp√µem a hist√≥ria do processo at√© o instante $t$. Note que os erros futuros n√£o entram na equa√ß√£o da previs√£o. Matematicamente, isso pode ser representado como [^6]:

$$ \hat{Y}_{t+s|t} = E[Y_{t+s} | \epsilon_t, \epsilon_{t-1}, \dots ] = \mu + \sum_{j=s}^q \theta_j \epsilon_{t+s-j} = \mu + \sum_{k=0}^{q-s} \theta_{s+k}\epsilon_{t-k}$$
Onde se $s>q$, temos $\hat{Y}_{t+s|t} = \mu$.

√â importante destacar que, para horizontes de previs√£o $s$ maiores que a ordem $q$ do modelo, os erros passados n√£o mais contribuem para a previs√£o, e a melhor previs√£o linear se torna simplesmente a m√©dia incondicional do processo, $\mu$. Isso se deve √† caracter√≠stica da depend√™ncia temporal finita dos modelos MA(q).

Podemos expressar a previs√£o linear √≥tima de uma forma mais compacta usando o operador de defasagem $L$ e o operador de aniquila√ß√£o [^6]:
$$
\hat{Y}_{t+s|t} = \mu + \left[ \frac{\theta(L)}{L^s} \right]_+ \epsilon_t
$$
onde $\theta(L) = \sum_{j=0}^q \theta_j L^j$ √© o polin√¥mio de defasagem, e $\left[ \frac{\theta(L)}{L^s} \right]_+$ √© o operador de aniquila√ß√£o que elimina os termos com expoentes negativos de $L$, o que garante que a previs√£o use apenas erros passados e n√£o os futuros.

Em termos pr√°ticos, a express√£o para a previs√£o linear √≥tima pode ser escrita como:
$$
\hat{Y}_{t+s|t} =
\begin{cases}
\mu + \theta_s \epsilon_t + \theta_{s+1} \epsilon_{t-1} + \ldots + \theta_q \epsilon_{t-q+s}, & \text{ para } s = 1, 2, \ldots, q \\
\mu, & \text{ para } s = q + 1, q + 2, \ldots
\end{cases}
$$
Este resultado demonstra que para modelos MA(q), a previs√£o utiliza os erros passados at√© o horizonte q. Para horizontes de previs√£o $s$ maiores do que $q$, a previs√£o linear √≥tima se torna simplesmente a m√©dia incondicional do processo, $\mu$.

> üí° **Exemplo Num√©rico:** Considere um processo MA(2) com $\mu = 10$, $\theta_1 = 0.7$ e $\theta_2 = 0.4$. Suponha que os erros passados s√£o: $\epsilon_t = 2$, $\epsilon_{t-1} = -1$, e $\epsilon_{t-2} = 0.5$. Calculemos as previs√µes para $s=1, 2, 3$:
>
>  *   **Previs√£o para s=1:**
>  $$ \hat{Y}_{t+1|t} = \mu + \theta_1 \epsilon_t + \theta_2 \epsilon_{t-1} = 10 + 0.7(2) + 0.4(-1) = 10 + 1.4 - 0.4 = 11 $$
>  *   **Previs√£o para s=2:**
>  $$ \hat{Y}_{t+2|t} = \mu + \theta_2 \epsilon_t = 10 + 0.4(2) = 10 + 0.8 = 10.8 $$
>  *  **Previs√£o para s=3:**
>  $$ \hat{Y}_{t+3|t} = \mu = 10 $$
>
> Note que para $s>2$, a previs√£o √© igual √† m√©dia do processo.

O erro m√©dio quadr√°tico de previs√£o (MSE) associado √† previs√£o linear √≥tima, para um modelo MA(q), √© dado por:

$$MSE = \begin{cases}
    \sigma^2, & \text{ para } s = 1 \\
    (1 + \theta_1^2 + \ldots + \theta_{s-1}^2) \sigma^2, & \text{ para } s = 2, 3, \ldots, q \\
    (1 + \theta_1^2 + \ldots + \theta_q^2) \sigma^2, & \text{ para } s = q+1, q+2, \ldots
\end{cases}$$
Para um passo √† frente ($s=1$), o MSE √© simplesmente a vari√¢ncia do ru√≠do branco ($\sigma^2$). Para horizontes de previs√£o entre 2 e q, o MSE aumenta com o horizonte, incorporando os coeficientes $\theta_i$ do processo. Para $s > q$, o MSE se torna constante, igual a vari√¢ncia incondicional do processo.

> üí° **Exemplo Num√©rico:**  Vamos calcular o MSE para o processo MA(2) do exemplo anterior, com $\theta_1 = 0.7$, $\theta_2 = 0.4$, e $\sigma^2 = 0.25$:
>
> * **MSE para s=1:** $MSE = \sigma^2 = 0.25$
> * **MSE para s=2:** $MSE = (1 + \theta_1^2)\sigma^2 = (1 + 0.7^2) \cdot 0.25 = 1.49 \cdot 0.25 = 0.3725$
> * **MSE para s=3:** $MSE = (1 + \theta_1^2 + \theta_2^2)\sigma^2 = (1 + 0.7^2 + 0.4^2) \cdot 0.25 = 1.65 \cdot 0.25 = 0.4125$
>
> Note como o MSE aumenta com o horizonte de previs√£o at√© $q=2$ e depois se torna constante. Para $s>2$, o MSE ser√° sempre 0.4125.

**Lema 1.** *Previs√£o Linear √ìtima em MA(q) para s>q.*
Para um processo MA(q), a previs√£o linear √≥tima $\hat{Y}_{t+s|t}$ para um horizonte de previs√£o $s$ maior que a ordem do modelo $q$ √© igual √† m√©dia incondicional do processo, $\mu$. Ou seja, $\hat{Y}_{t+s|t} = \mu$ para $s > q$.

*Prova:*
I. A previs√£o linear √≥tima para um MA(q) √© dada por:
$$
\hat{Y}_{t+s|t} = \mu + \left[\frac{\sum_{j=0}^q \theta_j L^j}{L^s}\right]_+ \epsilon_t
$$
II. Se $s > q$, todos os termos de $\sum_{j=0}^q \theta_j L^j$ ter√£o um expoente negativo de $L$ quando divididos por $L^s$.
III. Pela defini√ß√£o do operador de aniquila√ß√£o, todos os termos com expoente negativo de $L$ s√£o eliminados, ou seja,  $\left[ \frac{L^k}{L^s} \right]_+ = 0$ para $k<s$.
IV. Portanto, para $s>q$, $\left[\frac{\sum_{j=0}^q \theta_j L^j}{L^s}\right]_+ = 0$
V. Consequentemente, $\hat{Y}_{t+s|t} = \mu$ para $s>q$.
O Lema 1 demonstra formalmente que a previs√£o linear √≥tima para horizontes maiores que a ordem q do modelo MA(q) √© simplesmente a m√©dia incondicional do processo, confirmando um resultado j√° discutido em cap√≠tulos anteriores. ‚ñ†

**Teorema 1.** *MSE para Modelos MA(q) com s > q.*
O Erro M√©dio Quadr√°tico de Previs√£o (MSE) em um modelo MA(q) para um horizonte de previs√£o $s$ maior que a ordem do modelo $q$ √© igual √† vari√¢ncia incondicional do processo, dada por:
$$MSE =  (1 + \theta_1^2 + \ldots + \theta_q^2) \sigma^2 , \quad \text{para } s > q$$

*Prova:*
I. Do Lema 1, para $s>q$, sabemos que $\hat{Y}_{t+s|t} = \mu$.
II. O erro de previs√£o √© definido como $e_{t+s|t} = Y_{t+s} - \hat{Y}_{t+s|t}$.
III. Portanto, para $s>q$, $e_{t+s|t} = Y_{t+s} - \mu$.
IV. O MSE √© definido como o valor esperado do erro ao quadrado, $MSE = E[(Y_{t+s} - \mu)^2]$.
V. Sabemos que $Var(Y_t) = E[(Y_t - \mu)^2]$
VI. E que a vari√¢ncia de um processo MA(q) √© dada por $Var(Y_t) = (1 + \theta_1^2 + \ldots + \theta_q^2) \sigma^2$.
VII. Assim, para $s>q$, temos:
$MSE =  (1 + \theta_1^2 + \ldots + \theta_q^2) \sigma^2$
Este teorema formaliza o comportamento do MSE para horizontes de previs√£o maiores do que a ordem do modelo MA(q), demonstrando que o MSE converge para a vari√¢ncia incondicional do processo, refletindo a perda da capacidade de previs√£o al√©m desse horizonte. ‚ñ†

> üí° **Exemplo Num√©rico:** Usando os valores do exemplo anterior $\theta_1 = 0.7$, $\theta_2 = 0.4$ e $\sigma^2 = 0.25$, a vari√¢ncia incondicional do processo MA(2) √©:
>
> $$Var(Y_t) = (1 + 0.7^2 + 0.4^2) \times 0.25 = 1.65 \times 0.25 = 0.4125$$
>
> Logo, o MSE para $s>2$ √© igual a $0.4125$, confirmando que o MSE converge para a vari√¢ncia incondicional do processo para horizontes de previs√£o maiores que a ordem do modelo.

**Observa√ß√£o 1.** *Propriedade da Ortogonalidade dos Erros de Previs√£o.* Em modelos MA(q), os erros de previs√£o $e_{t+s|t}$ s√£o ortogonais aos erros passados $\epsilon_{t-j}$ para $j \geq 0$ quando $s > 0$. Esta propriedade √© fundamental para a otimalidade da previs√£o linear e √© uma consequ√™ncia direta da proje√ß√£o ortogonal do processo.

*Prova:*
I. O erro de previs√£o para o horizonte s √© definido como $e_{t+s|t} = Y_{t+s} - \hat{Y}_{t+s|t}$
II. Para um processo MA(q), temos $Y_{t+s} = \mu + \sum_{j=0}^q \theta_j \epsilon_{t+s-j}$.
III.  A previs√£o linear √≥tima √© dada por  $\hat{Y}_{t+s|t} = \mu + \sum_{k=0}^{q-s} \theta_{s+k} \epsilon_{t-k}$ se $s \leq q$ e $\hat{Y}_{t+s|t} = \mu$ se $s > q$
IV. Ent√£o, $e_{t+s|t} = \sum_{j=0}^q \theta_j \epsilon_{t+s-j} - \sum_{k=0}^{q-s} \theta_{s+k} \epsilon_{t-k}$ para $s \leq q$ e $e_{t+s|t} = \sum_{j=0}^q \theta_j \epsilon_{t+s-j}$ para $s > q$.
V. Considere $s>0$.  Para $s \leq q$, $e_{t+s|t}$ √© uma combina√ß√£o linear de erros $\epsilon$ no futuro (em rela√ß√£o a t) e no presente e no passado (at√© t-q). Para $s > q$, $e_{t+s|t}$ envolve apenas erros futuros.
VI. Os erros $\epsilon_t$ s√£o n√£o correlacionados, $E[\epsilon_t\epsilon_s] = 0$ para $t \neq s$.
VII. Portanto $E[e_{t+s|t} \epsilon_{t-j}] = 0$ para $j \geq 0$. Isso significa que os erros de previs√£o s√£o ortogonais aos erros passados.

Esta observa√ß√£o destaca uma propriedade essencial da previs√£o linear √≥tima em modelos MA(q), a ortogonalidade do erro de previs√£o em rela√ß√£o aos erros passados, o que √© uma caracter√≠stica de proje√ß√µes lineares √≥timas.

**Lema 1.1.** *Forma Alternativa para a Previs√£o Linear √ìtima.*
A previs√£o linear √≥tima $\hat{Y}_{t+s|t}$ para um processo MA(q) tamb√©m pode ser expressa em termos do polin√¥mio de defasagem $\theta(L)$ e do operador de defasagem $L$ como:
$$
\hat{Y}_{t+s|t} = \mu + \left( \sum_{j=s}^q \theta_j L^{j-s} \right) \epsilon_t
$$
*Prova:*
I. A express√£o original para a previs√£o linear √≥tima √© $\hat{Y}_{t+s|t} = \mu + \sum_{k=0}^{q-s} \theta_{s+k}\epsilon_{t-k}$
II. Substituindo $j = s + k$, temos $k=j-s$. Quando $k=0$, $j=s$, e quando $k=q-s$, $j=q$. Logo, a soma se torna $\sum_{j=s}^q \theta_j \epsilon_{t-(j-s)}$
III. Utilizando o operador de defasagem $L^k \epsilon_t = \epsilon_{t-k}$, podemos escrever $\epsilon_{t-(j-s)} = L^{j-s} \epsilon_t$
IV. Portanto, $\hat{Y}_{t+s|t} = \mu + \sum_{j=s}^q \theta_j L^{j-s} \epsilon_t$, que pode ser escrito como $\hat{Y}_{t+s|t} = \mu + \left( \sum_{j=s}^q \theta_j L^{j-s} \right) \epsilon_t$

Esta forma alternativa destaca a depend√™ncia da previs√£o linear √≥tima em rela√ß√£o aos coeficientes do modelo e aos erros passados, e pode ser √∫til em deriva√ß√µes e c√°lculos posteriores.

### Conclus√£o

Este cap√≠tulo proporcionou uma an√°lise detalhada das **previs√µes lineares √≥timas** em modelos **MA(q)**, com foco na utiliza√ß√£o dos erros passados at√© a ordem q, e na converg√™ncia da previs√£o para a m√©dia incondicional do processo para horizontes maiores que q. Atrav√©s da aplica√ß√£o do operador de defasagem e do operador de aniquila√ß√£o, foi poss√≠vel expressar a previs√£o linear √≥tima de forma compacta e elegante, e demonstrar como os erros futuros n√£o entram na equa√ß√£o da previs√£o, sendo substitu√≠dos por seus valores esperados, que s√£o zero.  Foi formalmente demonstrado como o MSE se comporta em rela√ß√£o ao horizonte de previs√£o, aumentando inicialmente e se estabilizando na vari√¢ncia incondicional do processo para horizontes al√©m da ordem do modelo, com o MSE para um passo √† frente sendo igual a vari√¢ncia do ru√≠do branco. Foi tamb√©m demonstrada a propriedade de ortogonalidade dos erros de previs√£o em rela√ß√£o aos erros passados e fornecida uma forma alternativa para a previs√£o linear √≥tima em fun√ß√£o do polin√¥mio de defasagem.
A rela√ß√£o com t√≥picos anteriores foi estabelecida atrav√©s da utiliza√ß√£o do conceito de proje√ß√£o linear, da defini√ß√£o de erro m√©dio quadr√°tico de previs√£o, do uso do operador de defasagem e do operador de aniquila√ß√£o. Esses resultados s√£o fundamentais para aprofundar a compreens√£o dos modelos de s√©ries temporais e para desenvolver ferramentas mais sofisticadas na constru√ß√£o de previs√µes precisas em modelos MA(q), abrindo caminho para o estudo de modelos mais complexos, como os modelos ARMA.

### Refer√™ncias
[^1]: *Expression [4.1.1] is known as the mean squared error associated with the forecast... The forecast with the smallest mean squared error turns out to be the expectation of Y‚ÇÅ+1 conditional on X‚ÇÅ...*
[^5]: *Consider a process with an MA(‚àû) representation (Y, ‚Äì Œº) = œà(L)Œµ, with e, white noise and...*
[^6]: *...the optimal linear forecast is √ä[Y,+s|‚Ç¨, ‚Ç¨,-1,...] ... the optimal forecast could be written in lag operator notation as √ä[Y,+s|‚Ç¨, ‚Ç¨,-1,...] = Œº + [œà(L)/L^s]_+ Œµ‚ÇÅ*
<!-- END -->
