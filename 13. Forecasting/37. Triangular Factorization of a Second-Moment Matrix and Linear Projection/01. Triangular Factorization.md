## Fatora√ß√£o Triangular e Proje√ß√µes Lineares
### Introdu√ß√£o
Este cap√≠tulo explora a aplica√ß√£o da fatora√ß√£o triangular na an√°lise de proje√ß√µes lineares, com foco em como essa t√©cnica transforma dados originais em vari√°veis n√£o correlacionadas, simplificando o processo. Conforme introduzido nas se√ß√µes anteriores, a an√°lise de proje√ß√µes lineares √© fundamental para a previs√£o em s√©ries temporais [^4], e a capacidade de manipular os dados de forma eficiente √© crucial para obter resultados significativos. Ao construir sobre os conceitos introduzidos em [^4], este cap√≠tulo detalha como a fatora√ß√£o triangular se encaixa nesse contexto, complementando e expandindo os fundamentos de proje√ß√£o linear previamente estabelecidos.

### Conceitos Fundamentais
A **fatora√ß√£o triangular** de uma matriz de momentos segundos √© uma t√©cnica que decomp√µe uma matriz sim√©trica definida positiva $\Omega$ em um produto de tr√™s matrizes: $\Omega = ADA'$, onde A √© uma matriz triangular inferior com 1s na diagonal principal, e D √© uma matriz diagonal [^4]. Esta decomposi√ß√£o possui propriedades √∫nicas que s√£o exploradas para simplificar a an√°lise de proje√ß√µes lineares.

> üí° **Exemplo Num√©rico:**
> Suponha que temos uma matriz de covari√¢ncia $\Omega$:
>
> $$\Omega = \begin{bmatrix} 4 & 2 & 1 \\ 2 & 5 & 3 \\ 1 & 3 & 6 \end{bmatrix}$$
>
> Atrav√©s da fatora√ß√£o triangular (Cholesky, por exemplo), podemos decompor $\Omega$ em A, D e A'. Em Python usando NumPy:
> ```python
> import numpy as np
> from scipy.linalg import cholesky
>
> Omega = np.array([[4, 2, 1], [2, 5, 3], [1, 3, 6]])
> L = cholesky(Omega, lower=True) # Get the lower triangular matrix
> A = L / np.diag(L)[:, None]      # Divide by the diagonal to get 1s on the diagonal
> D = np.diag(np.diag(L)**2)       # Diagonal matrix with variances
>
> print("Matriz A:\n", A)
> print("Matriz D:\n", D)
> print("A * D * A.T:\n", A @ D @ A.T)
> ```
> A matriz A (retornada como `A` no c√≥digo) √© triangular inferior com 1s na diagonal, D (retornada como `D`) √© a matriz diagonal e `A @ D @ A.T` deve resultar na matriz original $\Omega$. Os resultados devem ser aproximadamente:
> ```
> Matriz A:
> [[1.         0.         0.        ]
> [0.5        1.         0.        ]
> [0.25       0.71428571 1.        ]]
> Matriz D:
> [[4.         0.         0.        ]
> [0.         4.         0.        ]
> [0.         0.         4.28571429]]
> A * D * A.T:
> [[4. 2. 1.]
> [2. 5. 3.]
> [1. 3. 6.]]
> ```

#### Transforma√ß√£o de Vari√°veis
O ponto chave desta abordagem reside na transforma√ß√£o dos dados originais, Y, em um novo conjunto de vari√°veis n√£o correlacionadas, $Y = A^{-1}Y$ [^4]. A matriz $A^{-1}$ √© constru√≠da de forma que, quando aplicada aos dados originais, resulta em novas vari√°veis $Y$ que s√£o mutuamente n√£o correlacionadas [^4]. O processo de decomposi√ß√£o triangular garante que a matriz de covari√¢ncia de $Y$, dada por $E(YY')$, seja diagonal, ou seja, as vari√°veis em $Y$ s√£o n√£o correlacionadas. Esta propriedade √© fundamental, pois simplifica muitas opera√ß√µes e c√°lculos em proje√ß√µes lineares.
**Proposi√ß√£o 1**
A matriz $A^{-1}$ √© tamb√©m triangular inferior com 1s na diagonal principal.
*Prova*:
I. Seja A uma matriz triangular inferior com 1s na diagonal principal. Pela defini√ß√£o de matriz inversa, temos $AA^{-1}=I$, onde I √© a matriz identidade.

II. Ao analisar o produto $AA^{-1}$ linha por linha,  observamos que a primeira linha de $A^{-1}$ deve ser [1 0 0 ... 0], pois esta √© a √∫nica forma de obter a primeira linha da matriz identidade.

III. Prosseguindo, a segunda linha de $A^{-1}$ deve ter um valor na primeira posi√ß√£o, seguido de 1 na segunda e 0 nas restantes, pois ao multiplicar pela segunda linha de A, obtemos o segundo vetor da matriz identidade.

IV. Continuando de forma an√°loga para cada linha, conclu√≠mos que $A^{-1}$ √© tamb√©m uma matriz triangular inferior com 1s na diagonal principal. ‚ñ†
> üí° **Exemplo Num√©rico:**
> Usando a matriz A do exemplo anterior, podemos encontrar $A^{-1}$ usando `numpy.linalg.inv`.
> ```python
> A_inv = np.linalg.inv(A)
> print("Matriz A^-1:\n", A_inv)
> print("A * A^-1:\n", A @ A_inv)
> ```
> Resultado:
> ```
> Matriz A^-1:
> [[ 1.          0.          0.        ]
> [-0.5         1.          0.        ]
> [ 0.10714286 -0.71428571  1.        ]]
> A * A^-1:
> [[1.00000000e+00 0.00000000e+00 0.00000000e+00]
> [1.11022302e-16 1.00000000e+00 0.00000000e+00]
> [0.00000000e+00 0.00000000e+00 1.00000000e+00]]
> ```
> Note que $A^{-1}$ √© tamb√©m triangular inferior com 1s na diagonal e que $A * A^{-1}$ √© (aproximadamente) a matriz identidade, confirmando a Proposi√ß√£o 1.
#### C√°lculo de Proje√ß√µes Lineares
O uso da fatora√ß√£o triangular para o c√°lculo de proje√ß√µes lineares √© not√°vel por v√°rias raz√µes [^4]. Primeiro, a transforma√ß√£o $Y = A^{-1}Y$ permite que a matriz de covari√¢ncia dos dados transformados seja diagonal, o que significa que as vari√°veis se tornam n√£o correlacionadas. Isso simplifica o c√°lculo das proje√ß√µes lineares, pois podemos analisar cada componente de $Y$ separadamente sem nos preocuparmos com a correla√ß√£o entre elas. Segundo, como vimos em [^4], a matriz de proje√ß√£o linear $\alpha$, pode ser obtida a partir da decomposi√ß√£o triangular da matriz de covari√¢ncia dos dados originais, conforme a seguinte express√£o: $\alpha =  [E(Y_{t+1}X)][E(X,X')]$. Em particular, a matriz A cont√©m informa√ß√µes cruciais sobre os coeficientes de proje√ß√£o linear, o que facilita o c√°lculo.
**Lema 2**
A matriz de covari√¢ncia das vari√°veis transformadas, $Y$, √© dada por D, onde D √© a matriz diagonal da decomposi√ß√£o triangular.
*Prova:*
I. Seja $\Omega$ a matriz de covari√¢ncia de $Y$, ou seja, $\Omega = E(YY')$. Pela fatora√ß√£o triangular, sabemos que $\Omega = ADA'$.

II. Agora, considere a matriz de covari√¢ncia de $Y=A^{-1}Y$:
    $$E(YY') = E((A^{-1}Y)(A^{-1}Y)') = E(A^{-1}YY'(A^{-1})') = A^{-1}E(YY')(A^{-1})'$$

III. Substituindo $\Omega$ por $ADA'$, temos:
$$A^{-1}(ADA')(A^{-1})' = A^{-1}AD(A'(A^{-1})') = IDI' = D$$

IV. Portanto, $$E(YY') = D$$, mostrando que a matriz de covari√¢ncia de $Y$ √© a matriz diagonal D. ‚ñ†
> üí° **Exemplo Num√©rico:**
> Suponha que temos um vetor de dados originais $Y = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$. Ap√≥s a transforma√ß√£o, temos $Y = A^{-1}Y$
> ```python
> Y = np.array([[1], [2], [3]])
> Y_transformed = A_inv @ Y
> print("Y transformado:\n", Y_transformed)
> ```
> Resultado:
> ```
> Y transformado:
> [[ 1.        ]
> [ 1.5       ]
> [ 0.35714286]]
> ```
> A matriz de covari√¢ncia de $Y$ √© dada por D, que √© diagonal. Isso demonstra que as vari√°veis transformadas s√£o n√£o correlacionadas.
#### Interpreta√ß√£o das Matrizes
A matriz A, utilizada na fatora√ß√£o triangular, tem uma interpreta√ß√£o espec√≠fica no contexto de proje√ß√µes lineares [^4]. Cada coluna de A corresponde aos coeficientes de proje√ß√£o de uma vari√°vel $Y_i$ sobre as vari√°veis $Y_1, Y_2, \ldots, Y_{i-1}$ [^4]. O elemento na linha *i* e coluna *j* de A, que denotaremos por $a_{ij}$, representa o coeficiente na proje√ß√£o linear da vari√°vel $Y_i$ sobre a vari√°vel $Y_j$, onde $j < i$ [^4]. A matriz diagonal D, por sua vez, cont√©m as vari√¢ncias das vari√°veis transformadas (res√≠duos das proje√ß√µes lineares) [^4]. Cada elemento $d_{ii}$ representa a vari√¢ncia do res√≠duo da proje√ß√£o de $Y_i$ sobre as vari√°veis anteriores, $Y_1, Y_2, \ldots, Y_{i-1}$ [^4].

> üí° **Exemplo Num√©rico:**
> Na matriz A obtida anteriormente:
> ```
> Matriz A:
> [[1.         0.         0.        ]
> [0.5        1.         0.        ]
> [0.25       0.71428571 1.        ]]
> ```
> O elemento $a_{21} = 0.5$ representa o coeficiente na proje√ß√£o linear de $Y_2$ sobre $Y_1$. O elemento $a_{31} = 0.25$ representa o coeficiente na proje√ß√£o linear de $Y_3$ sobre $Y_1$, e $a_{32} = 0.714...$ representa o coeficiente na proje√ß√£o linear de $Y_3$ sobre $Y_2$ ap√≥s remover o efeito de $Y_1$.
> Na matriz D:
> ```
> Matriz D:
> [[4.         0.         0.        ]
> [0.         4.         0.        ]
> [0.         0.         4.28571429]]
> ```
> O elemento $d_{11} = 4$ √© a vari√¢ncia de $Y_1$, $d_{22} = 4$ √© a vari√¢ncia do res√≠duo da proje√ß√£o de $Y_2$ sobre $Y_1$, e $d_{33} = 4.285...$ √© a vari√¢ncia do res√≠duo da proje√ß√£o de $Y_3$ sobre $Y_1$ e $Y_2$.

#### Atualiza√ß√£o de Proje√ß√µes Lineares
A fatora√ß√£o triangular facilita a atualiza√ß√£o de proje√ß√µes lineares. O processo de atualiza√ß√£o √© descrito pela express√£o [4.5.16] [^4]:

$$P(Y_3|Y_2,Y_1) = P(Y_3|Y_1) +  \{E[Y_3 - P(Y_3|Y_1)][Y_2 - P(Y_2|Y_1)]\} \{E[Y_2 - P(Y_2|Y_1)]^2\}^{-1} [Y_2 - P(Y_2|Y_1)]$$

Esta f√≥rmula mostra como a inclus√£o de uma nova vari√°vel $Y_2$ melhora a proje√ß√£o original de $Y_3$ baseada apenas em $Y_1$. A fatora√ß√£o triangular permite o c√°lculo eficiente desses componentes da atualiza√ß√£o, com a matriz H interpretada como a matriz de covari√¢ncia das vari√°veis transformadas. Os termos $h_{ij}$ s√£o calculados a partir da decomposi√ß√£o triangular da matriz de covari√¢ncia e representam covari√¢ncias entre res√≠duos de proje√ß√µes, que s√£o √∫teis para a atualiza√ß√£o.
**Teorema 3**
A matriz H mencionada na se√ß√£o sobre atualiza√ß√£o de proje√ß√µes, que representa as covari√¢ncias dos res√≠duos de proje√ß√µes lineares, √© equivalente √† matriz D da decomposi√ß√£o triangular, isto √©, H = D.
*Prova:*
I. Seja Y o vetor original de vari√°veis e $Y=A^{-1}Y$ o vetor transformado. Os res√≠duos da proje√ß√£o de $Y_i$ sobre as vari√°veis anteriores s√£o exatamente as vari√°veis $Y_i$ transformadas.

II. Como vimos no Lema 2, a matriz de covari√¢ncia das vari√°veis transformadas √© D, que √© diagonal.

III. Por defini√ß√£o, H √© a matriz de covari√¢ncia dos res√≠duos das proje√ß√µes, ou seja, a matriz de covari√¢ncia das vari√°veis transformadas.

IV. Portanto, $$H = E(YY') = D$$. Isto estabelece que a matriz H,  que surge no contexto de atualiza√ß√£o de proje√ß√µes lineares √©, de fato, a matriz diagonal D da decomposi√ß√£o triangular. ‚ñ†
> üí° **Exemplo Num√©rico:**
> Vamos ilustrar a atualiza√ß√£o de proje√ß√µes lineares. Suponha que tenhamos as seguintes proje√ß√µes iniciais:
>
> $P(Y_2|Y_1) = 0.5Y_1$ (coeficiente retirado da matriz A do exemplo num√©rico anterior)
> $Y_3 - P(Y_3|Y_1) = (0.25 Y_1 + 0.714Y_2)$, e $P(Y_3|Y_1) = 0.25Y_1$.
>
> $P(Y_3|Y_2,Y_1)$ √© dado por:
> $$P(Y_3|Y_2,Y_1) = P(Y_3|Y_1) +  \{E[Y_3 - P(Y_3|Y_1)][Y_2 - P(Y_2|Y_1)]\} \{E[Y_2 - P(Y_2|Y_1)]^2\}^{-1} [Y_2 - P(Y_2|Y_1)]$$
>
> Suponha que temos os seguintes dados: $Y_1 = 1$, $Y_2 = 2$, e $Y_3 = 3$.
>
> $\text{Step 1: } P(Y_2|Y_1) = 0.5 * 1 = 0.5$
>
> $\text{Step 2: } Y_2 - P(Y_2|Y_1) = 2 - 0.5 = 1.5$
>
> $\text{Step 3: } Y_3 - P(Y_3|Y_1) = 3-0.25*1= 2.75$
>
> $\text{Step 4: } E[Y_3 - P(Y_3|Y_1)][Y_2 - P(Y_2|Y_1)] =  cov(Y_3,Y_2) - cov(P(Y_3|Y_1), Y_2)=  3-0.25*2=2.5$
>
> $\text{Step 5: } E[Y_2 - P(Y_2|Y_1)]^2 = var(Y_2-P(Y_2|Y_1)) = var(Y_2)-var(P(Y_2|Y_1))= 5 - 0.5*4*0.5=4$
>
> $\text{Step 6: }  P(Y_3|Y_2,Y_1) = 0.25*1 + 2.5/4*1.5 = 1.1875 $
>
> No caso da atualiza√ß√£o, a matriz H = D, ou seja, as vari√¢ncias dos res√≠duos obtidas na decomposi√ß√£o triangular, que s√£o 4 e 4.2857, s√£o usadas para calcular as proje√ß√µes lineares atualizadas.

#### Aplica√ß√£o Pr√°tica
Em termos pr√°ticos, a fatora√ß√£o triangular e a transforma√ß√£o resultante permitem analisar o efeito de cada vari√°vel sobre a vari√°vel alvo em etapas separadas, em vez de tentar modelar todas as rela√ß√µes simultaneamente [^4]. Isso facilita o processamento e permite obter previs√µes lineares de forma mais eficiente. Al√©m disso, o processo de fatora√ß√£o triangular tamb√©m leva √† forma√ß√£o de um conjunto de vari√°veis n√£o correlacionadas, o que simplifica outros c√°lculos estat√≠sticos e econom√©tricos subsequentes.
**Corol√°rio 3.1**
A propriedade de que a matriz de covari√¢ncia dos res√≠duos de proje√ß√µes √© diagonal, implicando que os res√≠duos s√£o n√£o correlacionados,  permite aplicar m√©todos de an√°lise estat√≠stica que assumem independ√™ncia, simplificando a modelagem de processos estoc√°sticos.
*Prova:*
I. Dado que H=D, como demonstrado no Teorema 3, e que D √© uma matriz diagonal, ent√£o a matriz de covari√¢ncia dos res√≠duos de proje√ß√µes √© diagonal.

II. Uma matriz de covari√¢ncia diagonal implica que as vari√°veis (neste caso, os res√≠duos) s√£o n√£o correlacionadas.

III. Assim, em an√°lises subsequentes, podemos tratar estes res√≠duos como vari√°veis estatisticamente independentes, o que simplifica muitos m√©todos estat√≠sticos que assumem essa independ√™ncia. ‚ñ†
### Conclus√£o
A fatora√ß√£o triangular da matriz de momentos segundos √© uma t√©cnica poderosa que permite transformar dados originais em vari√°veis n√£o correlacionadas, facilitando o c√°lculo de proje√ß√µes lineares e a atualiza√ß√£o de proje√ß√µes com novas informa√ß√µes. A matriz A e a matriz D derivadas da fatora√ß√£o triangular proporcionam uma compreens√£o estruturada da rela√ß√£o entre as vari√°veis e os coeficientes de proje√ß√£o, enquanto a atualiza√ß√£o de proje√ß√µes lineares torna o processo din√¢mico e adapt√°vel. Ao criar conex√µes naturais entre o conte√∫do apresentado aqui e os t√≥picos anteriores, fica evidente o valor da fatora√ß√£o triangular no arsenal de um analista de s√©ries temporais [^4]. A transforma√ß√£o dos dados em vari√°veis n√£o correlacionadas e as propriedades das matrizes A e D s√£o de grande utilidade para a modelagem e previs√£o.
### Refer√™ncias
[^4]:  Express√£o [4.1.1] √© conhecida como o erro quadr√°tico m√©dio associado √† previs√£o $Y^*_{t+1}$, denotado $MSE(Y^*_{t+1}|t) = E(Y_{t+1} - Y^*_{t+1}|t)^2$. A previs√£o com o menor erro quadr√°tico m√©dio √© a expectativa de $Y_{t+1}$ condicional em X. Para verificar essa alega√ß√£o, considere basear $Y^*_{t+1}$ em qualquer fun√ß√£o g($X_t$). A representa√ß√£o da proje√ß√£o linear. Express√£o [4.1.1] √© conhecida como o erro quadr√°tico m√©dio associado √† previs√£o $Y^*_{t+1}$, denotado $MSE(Y^*_{t+1}|t) = E(Y_{t+1} - Y^*_{t+1}|t)^2$. A previs√£o com o menor erro quadr√°tico m√©dio √© a expectativa de $Y_{t+1}$ condicional em $X_t$. Para verificar essa alega√ß√£o, considere basear $Y^*_{t+1}$ em qualquer fun√ß√£o g($X_t$). A representa√ß√£o da proje√ß√£o linear e as f√≥rmulas para calcular a proje√ß√£o e seu erro. O conceito de proje√ß√£o linear e como ele se relaciona com a regress√£o de m√≠nimos quadrados ordin√°rios. C√°lculo dos coeficientes de proje√ß√£o. Matriz de proje√ß√£o e seu MSE. A formula√ß√£o do problema de proje√ß√£o e sua solu√ß√£o quando a proje√ß√£o √© realizada em um vetor.  A previs√£o como uma fun√ß√£o de e's defasados e a aplica√ß√£o do operador de aniquila√ß√£o.  A previs√£o como uma fun√ß√£o de Y's defasados, com a aplica√ß√£o da f√≥rmula de previs√£o de Wiener-Kolmogorov.  A previs√£o de um processo AR(1) e um processo AR(p). O conceito de proje√ß√µes iteradas. O processo de previs√£o de MA(1), MA(q) e ARMA(1,1). O problema da previs√£o com um n√∫mero finito de observa√ß√µes. A discuss√£o sobre como lidar com essa quest√£o.  A defini√ß√£o de proje√ß√µes lineares exatas para amostras finitas, as propriedades dessas proje√ß√µes e como calcular os coeficientes.  A representa√ß√£o de uma matriz sim√©trica definida positiva, juntamente com a deriva√ß√£o das matrizes A, D, o inverso e a unicidade.  A representa√ß√£o de uma matriz sim√©trica definida positiva, juntamente com a deriva√ß√£o das matrizes A, D, o inverso e a unicidade. Uma discuss√£o sobre o que a matriz triangular A significa no contexto de proje√ß√µes e como usar a fatora√ß√£o para atualizar proje√ß√µes lineares e sobre como as proje√ß√µes funcionam em combina√ß√£o com o conceito de proje√ß√£o iterada. O uso da fatora√ß√£o triangular da matriz de covari√¢ncia na previs√£o de um processo MA(1), com uma discuss√£o sobre o resultado da previs√£o, como as mudan√ßas e como lidar com processos n√£o invert√≠veis. O uso da fatora√ß√£o triangular de um segundo momento de uma matriz com proje√ß√µes lineares. A discuss√£o sobre como projetar vari√°veis e o significado da matriz H.
<!-- END -->
