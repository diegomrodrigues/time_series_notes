## A Estrutura Diagonal da Matriz de Covari√¢ncia Transformada e sua Implica√ß√£o na An√°lise do MSE

### Introdu√ß√£o

Este cap√≠tulo explora a estrutura diagonal da matriz de covari√¢ncia obtida ap√≥s a transforma√ß√£o de dados por meio da fatora√ß√£o triangular, e detalha suas implica√ß√µes para o c√°lculo do Erro Quadr√°tico M√©dio (MSE) e a an√°lise da vari√¢ncia dos res√≠duos. Conforme discutido nos cap√≠tulos anteriores, a fatora√ß√£o triangular decomp√µe a matriz de covari√¢ncia original ($\Omega$) em um produto de tr√™s matrizes ($ \Omega = ADA'$) [^4]. Uma das vantagens dessa decomposi√ß√£o √© a capacidade de transformar os dados originais em um novo conjunto de vari√°veis n√£o correlacionadas ($Y=A^{-1}Y$). Este cap√≠tulo foca em como a natureza diagonal da matriz de covari√¢ncia dos dados transformados simplifica a an√°lise da vari√¢ncia dos res√≠duos e o c√°lculo do MSE, complementando e aprofundando nossa compreens√£o sobre proje√ß√µes lineares.

### A Matriz de Covari√¢ncia Diagonal dos Dados Transformados

A fatora√ß√£o triangular, como vimos, transforma as vari√°veis originais $Y$ em vari√°veis transformadas $Y$, cuja matriz de covari√¢ncia √© uma matriz diagonal $D$ [^4]. A matriz $D$ √© constru√≠da de forma que suas entradas diagonais representam as vari√¢ncias dos res√≠duos das proje√ß√µes sequenciais e s√£o n√£o negativas [^4]. Esta propriedade √© fundamental para a simplifica√ß√£o de v√°rios c√°lculos e an√°lises, particularmente aqueles relacionados ao MSE e √† an√°lise da vari√¢ncia dos res√≠duos.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos uma matriz de covari√¢ncia $\Omega$ de tr√™s vari√°veis $Y_1, Y_2, Y_3$ e que atrav√©s da fatora√ß√£o triangular, obtivemos as matrizes A e D:
>
> $$ A = \begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.25 & 0.714 & 1 \end{bmatrix} $$
>
> $$ D = \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 4.2857 \end{bmatrix} $$
>
> A transforma√ß√£o $Y = A^{-1}Y$ garante que a matriz de covari√¢ncia de $Y$, denotada como $E(YY')$, seja igual √† matriz D, a matriz diagonal. Isso implica que as vari√°veis transformadas em $Y$ s√£o n√£o correlacionadas, como demonstrado no Lema 2 do cap√≠tulo anterior [^4].
>
> No c√≥digo abaixo demonstramos que $E(YY')$ √© a matriz diagonal $D$.
> ```python
> import numpy as np
>
> # Matriz de covari√¢ncia de exemplo
> Omega = np.array([[4, 2, 1],
>                   [2, 5, 3],
>                   [1, 3, 6]])
>
> # Simula√ß√£o da fatora√ß√£o triangular
> A = np.array([[1, 0, 0],
>               [0.5, 1, 0],
>               [0.25, 0.714, 1]])
> D = np.array([[4, 0, 0],
>               [0, 4, 0],
>               [0, 0, 4.2857]])
>
> # Vetor de dados original Y
> Y = np.array([1, 2, 3]).reshape(-1, 1)
>
> # Inversa de A
> A_inv = np.linalg.inv(A)
>
> # Transforma√ß√£o dos dados
> Y_transformed = A_inv @ Y
>
> # C√°lculo da matriz de covari√¢ncia dos dados transformados
> # Usando a defini√ß√£o, a matriz de covari√¢ncia √© E(YY')
> # Dado que E(YY') = D, como demonstrado no Lema 2 do cap√≠tulo anterior, isso se verifica
> Y_transformed_mean = np.mean(Y_transformed, axis=0)
> Y_transformed_centered = Y_transformed - Y_transformed_mean
> Cov_Y_transformed = Y_transformed_centered.T @ Y_transformed_centered / (Y_transformed.shape[0]-1)
>
> print("Matriz de covari√¢ncia dos dados transformados E(YY'):")
> print(Cov_Y_transformed)
> print("\nMatriz D obtida da fatora√ß√£o triangular:")
> print(D)
> ```
> O c√≥digo produzir√° resultados com `Cov_Y_transformed` e D aproximadamente iguais. Isto mostra que as vari√°veis em $Y$ s√£o n√£o correlacionadas e que suas vari√¢ncias s√£o dadas pelas entradas diagonais de D.
>
> üí° **Exemplo Num√©rico:**
>
> Para ilustrar com um exemplo num√©rico completo, considere uma matriz de covari√¢ncia $\Omega$ de duas vari√°veis $Y_1$ e $Y_2$:
>
> $\Omega = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}$
>
> Aplicando a fatora√ß√£o triangular, encontramos:
>
> $A = \begin{bmatrix} 1 & 0 \\ 0.5 & 1 \end{bmatrix}$
>
> $D = \begin{bmatrix} 2 & 0 \\ 0 & 2.5 \end{bmatrix}$
>
> Onde $A$ √© uma matriz triangular inferior com diagonal unit√°ria e $D$ √© uma matriz diagonal.
>
> Vamos verificar:
>
> 1.  $A' = \begin{bmatrix} 1 & 0.5 \\ 0 & 1 \end{bmatrix}$
> 2.  $ADA' = \begin{bmatrix} 1 & 0 \\ 0.5 & 1 \end{bmatrix} \begin{bmatrix} 2 & 0 \\ 0 & 2.5 \end{bmatrix} \begin{bmatrix} 1 & 0.5 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix} = \Omega$
>
> Agora, vamos calcular $A^{-1}$:
>
> $A^{-1} = \begin{bmatrix} 1 & 0 \\ -0.5 & 1 \end{bmatrix}$
>
> Vamos simular dados para $Y$:
>
> $Y = \begin{bmatrix} 3 \\ 5 \end{bmatrix}$
>
> Os dados transformados ser√£o:
>
> $Y = A^{-1}Y = \begin{bmatrix} 1 & 0 \\ -0.5 & 1 \end{bmatrix} \begin{bmatrix} 3 \\ 5 \end{bmatrix} = \begin{bmatrix} 3 \\ 3.5 \end{bmatrix}$
>
> A matriz de covari√¢ncia dos dados transformados, $E(YY')$, √© igual a $D$:
>
> $E(YY') = \begin{bmatrix} 2 & 0 \\ 0 & 2.5 \end{bmatrix}$
>
> Isso confirma que as vari√°veis transformadas s√£o n√£o correlacionadas e que a matriz de covari√¢ncia dos dados transformados √© a matriz diagonal D.

**Teorema 9**
A matriz de covari√¢ncia das vari√°veis transformadas Y, denotada como $E(YY')$, √© diagonal e √© igual √† matriz $D$ da fatora√ß√£o triangular. Ou seja $E(YY') = D$.
*Prova*:
I. Seja $\Omega$ a matriz de covari√¢ncia de Y, tal que $\Omega = E(YY')$. Pela fatora√ß√£o triangular, sabemos que $\Omega=ADA'$.
II. Se $Y=A^{-1}Y$, a matriz de covari√¢ncia das vari√°veis transformadas $Y$ √© dada por $E(YY') = E((A^{-1}Y)(A^{-1}Y)') = E(A^{-1}YY'(A^{-1})') = A^{-1}E(YY')(A^{-1})' = A^{-1}\Omega(A^{-1})'$.
III. Substituindo $\Omega$ por $ADA'$ temos $E(YY')=A^{-1}(ADA')(A^{-1})' = A^{-1}AD(A'(A^{-1})')= I D I' = D$.
IV. Portanto, $E(YY') = D$, comprovando que a matriz de covari√¢ncia das vari√°veis transformadas √© a matriz diagonal $D$. $\blacksquare$

**Lema 9.1**
A matriz A obtida atrav√©s da fatora√ß√£o triangular √© triangular inferior com diagonal unit√°ria. Ou seja, $a_{ii}=1$ para todo $i$ e $a_{ij}=0$ para todo $j>i$.
*Prova*:
I. A fatora√ß√£o triangular, como apresentada em cap√≠tulos anteriores [^4], define a matriz A como sendo resultante de opera√ß√µes de elimina√ß√£o gaussiana, onde cada passo elimina componentes de vari√°veis anteriores de cada linha da matriz.
II. As opera√ß√µes de elimina√ß√£o gaussiana, ao subtrair m√∫ltiplos de linhas anteriores de linhas atuais, transformam a matriz original em uma matriz triangular superior, e o registro dos multiplicadores na matriz A gera uma matriz triangular inferior.
III. A inicializa√ß√£o do processo, que n√£o modifica as linhas da matriz de covari√¢ncia original, garante que a diagonal de A seja unit√°ria.
IV. Portanto, a matriz A, por constru√ß√£o, √© triangular inferior com diagonal unit√°ria. $\blacksquare$

**Proposi√ß√£o 9.2**
A matriz $A^{-1}$ tamb√©m √© triangular inferior com diagonal unit√°ria.
*Prova*:
I. Pelo Lema 9.1, a matriz A √© triangular inferior com diagonal unit√°ria.
II. A inversa de uma matriz triangular inferior √© tamb√©m uma matriz triangular inferior.
III. A inversa de uma matriz com diagonal unit√°ria tamb√©m ter√° diagonal unit√°ria.
IV. Portanto, $A^{-1}$ √© uma matriz triangular inferior com diagonal unit√°ria. $\blacksquare$

### Simplifica√ß√£o do C√°lculo do MSE
A estrutura diagonal da matriz de covari√¢ncia $E(YY')$ simplifica significativamente o c√°lculo do Erro Quadr√°tico M√©dio (MSE) das proje√ß√µes lineares. Como visto no cap√≠tulo anterior [^4], o MSE da proje√ß√£o linear de uma vari√°vel $Y_i$ sobre as vari√°veis $Y_1, Y_2,..., Y_{i-1}$ √© dado pelo elemento $d_{ii}$ na diagonal da matriz $D$. Isso ocorre porque as vari√°veis transformadas $Y$ s√£o n√£o correlacionadas, o que reduz a necessidade de c√°lculos complexos de covari√¢ncia e vari√¢ncia.

> üí° **Exemplo Num√©rico:**
>
> Usando o exemplo num√©rico anterior, temos:
>
> $$ D = \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 4.2857 \end{bmatrix} $$
>
> O MSE da proje√ß√£o de $Y_1$ sobre um constante √© igual a $d_{11}=4$. O MSE da proje√ß√£o de $Y_2$ sobre $Y_1$ √© igual a $d_{22} = 4$. E o MSE da proje√ß√£o de $Y_3$ sobre $Y_1$ e $Y_2$ √© $d_{33}= 4.2857$. Estes valores s√£o obtidos diretamente da matriz D, sem necessidade de c√°lculos adicionais.
>
> Em geral, a matriz D nos oferece uma forma concisa de obter os MSEs de diferentes proje√ß√µes.
>
> üí° **Exemplo Num√©rico:**
>
> Utilizando a matriz D do exemplo anterior ($\begin{bmatrix} 2 & 0 \\ 0 & 2.5 \end{bmatrix}$), podemos facilmente calcular os MSEs:
>
> 1. O MSE da proje√ß√£o de $Y_1$ sobre uma constante √© $d_{11} = 2$. Isso significa que a vari√¢ncia de $Y_1$ sem nenhum preditor √© 2.
> 2. O MSE da proje√ß√£o de $Y_2$ sobre $Y_1$ √© $d_{22} = 2.5$. Este valor representa a vari√¢ncia do res√≠duo da proje√ß√£o de $Y_2$ sobre $Y_1$.
>
> A simplicidade de obter o MSE diretamente da matriz D, sem necessidade de c√°lculos adicionais, mostra o poder da fatora√ß√£o triangular.

**Proposi√ß√£o 10**
O Erro Quadr√°tico M√©dio (MSE) da proje√ß√£o linear de $Y_i$ sobre $Y_1, Y_2,..., Y_{i-1}$ √© dado pelo elemento $d_{ii}$ na diagonal da matriz D obtida da fatora√ß√£o triangular $\Omega = ADA'$.

*Prova:*
I. Como vimos no cap√≠tulo anterior, o elemento diagonal $d_{ii}$ representa a vari√¢ncia do res√≠duo da proje√ß√£o de $Y_i$ sobre as vari√°veis anteriores.
II. A proje√ß√£o linear $P(Y_i|Y_1, Y_2, \ldots, Y_{i-1})$ corresponde a um valor esperado, e a vari√¢ncia do res√≠duo dessa proje√ß√£o √© definida como o MSE.
III. Portanto, o MSE da proje√ß√£o de $Y_i$ sobre as vari√°veis anteriores √© igual a $d_{ii}$, o elemento diagonal da matriz D. $\blacksquare$

**Corol√°rio 10.1**
O MSE da proje√ß√£o de $Y_1$ sobre um constante √© igual a $d_{11}$.
*Prova:*
I. Segue diretamente da Proposi√ß√£o 10, onde $Y_1$ √© o primeiro elemento da s√©rie.
II. Assim, o MSE da proje√ß√£o de $Y_1$ sobre um valor constante (que √© sua pr√≥pria m√©dia) √© igual a $d_{11}$. $\blacksquare$

**Corol√°rio 10.2**
O MSE da proje√ß√£o de $Y_2$ sobre $Y_1$ √© igual a $d_{22}$.
*Prova:*
I. Segue diretamente da Proposi√ß√£o 10, onde $Y_2$ √© o segundo elemento da s√©rie.
II. Assim, o MSE da proje√ß√£o de $Y_2$ sobre $Y_1$ √© igual a $d_{22}$. $\blacksquare$

### An√°lise da Vari√¢ncia dos Res√≠duos

A natureza diagonal da matriz D tamb√©m facilita a an√°lise da vari√¢ncia dos res√≠duos das proje√ß√µes lineares. Os elementos diagonais $d_{ii}$ de D representam as vari√¢ncias dos res√≠duos $\hat{Y}_i = Y_i - P(Y_i|Y_1, Y_2, ..., Y_{i-1})$ [^4].  Estas informa√ß√µes s√£o cruciais para avaliar o ajuste de modelos de proje√ß√£o e identificar as vari√°veis que explicam melhor a vari√¢ncia dos dados.

> üí° **Exemplo Num√©rico:**
>
> No exemplo anterior, temos:
>
> $$ D = \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 4.2857 \end{bmatrix} $$
>
> A vari√¢ncia do res√≠duo da proje√ß√£o de $Y_1$ sobre um constante √© $d_{11} = 4$, da proje√ß√£o de $Y_2$ sobre $Y_1$ √© $d_{22} = 4$ e da proje√ß√£o de $Y_3$ sobre $Y_1$ e $Y_2$ √© $d_{33} = 4.2857$. Estes valores indicam quanto da variabilidade de cada vari√°vel permanece ap√≥s realizar as proje√ß√µes.
>
> Em geral, a matriz D nos oferece uma forma concisa de entender o grau de explica√ß√£o obtida por proje√ß√µes lineares sequenciais, indicando quais vari√°veis cont√™m a maior parte da informa√ß√£o relevante.
>
> üí° **Exemplo Num√©rico:**
>
> Usando nossa matriz D do exemplo anterior ($\begin{bmatrix} 2 & 0 \\ 0 & 2.5 \end{bmatrix}$):
>
> 1.  $d_{11} = 2$ representa a vari√¢ncia dos res√≠duos quando $Y_1$ √© projetado sobre uma constante (sua m√©dia). Em outras palavras, a variabilidade n√£o explicada de $Y_1$ √© 2.
> 2.  $d_{22} = 2.5$ representa a vari√¢ncia dos res√≠duos quando $Y_2$ √© projetado sobre $Y_1$. Isso indica que, mesmo usando $Y_1$ para predizer $Y_2$, ainda h√° uma variabilidade de 2.5 nos res√≠duos.
>
> Esta an√°lise da vari√¢ncia dos res√≠duos √© essencial para entender quanto cada vari√°vel explica de forma incremental, demonstrando mais uma aplica√ß√£o do uso da matriz D.

**Proposi√ß√£o 11**
Os elementos diagonais $d_{ii}$ da matriz D obtida da fatora√ß√£o triangular $\Omega = ADA'$ representam as vari√¢ncias dos res√≠duos da proje√ß√£o de $Y_i$ sobre $Y_1, Y_2, ..., Y_{i-1}$.
*Prova*:
I. Como j√° estabelecemos, a matriz de covari√¢ncia das vari√°veis transformadas $Y$ √© a matriz diagonal $D$.
II. Cada vari√°vel transformada $Y_i$ representa o res√≠duo da proje√ß√£o linear da vari√°vel original $Y_i$ sobre as vari√°veis anteriores $Y_1, Y_2, ..., Y_{i-1}$.
III. Portanto, as vari√¢ncias de cada vari√°vel transformada, que s√£o dadas pelos elementos diagonais $d_{ii}$, s√£o equivalentes √†s vari√¢ncias dos res√≠duos dessas proje√ß√µes sequenciais. $\blacksquare$

**Observa√ß√£o 11.1**
Note que, se todas as vari√°veis $Y_i$ forem utilizadas na proje√ß√£o de $Y_k$, para $k>i$, ent√£o o res√≠duo $\hat{Y}_k$ ser√° zero. Nesse caso, o elemento diagonal $d_{kk}$ na matriz $D$ seria zero.

**Observa√ß√£o 11.2**
A soma dos elementos diagonais da matriz $D$, $\sum_{i=1}^n d_{ii}$, √© igual √† soma das vari√¢ncias das vari√°veis originais $Y_i$, ou seja, o tra√ßo da matriz $\Omega$, denotado como $tr(\Omega)$. Isso ocorre porque a transforma√ß√£o por fatora√ß√£o triangular preserva a soma das vari√¢ncias.

### Conex√£o com a Atualiza√ß√£o de Proje√ß√µes Lineares

Como vimos no cap√≠tulo anterior [^4], a matriz $H$ usada na atualiza√ß√£o de proje√ß√µes lineares √© igual √† matriz $D$ obtida pela fatora√ß√£o triangular, ou seja, $H = D$. Isso significa que a fatora√ß√£o triangular n√£o apenas fornece uma maneira de transformar os dados em vari√°veis n√£o correlacionadas, mas tamb√©m fornece as ferramentas para atualizar as proje√ß√µes de forma eficiente. Os elementos da matriz $D$ (que s√£o as vari√¢ncias dos res√≠duos das proje√ß√µes sequenciais) s√£o usados diretamente no processo de atualiza√ß√£o [4.5.16] [^4]:
$$P(Y_3|Y_2,Y_1) = P(Y_3|Y_1) + \{E[Y_3 - P(Y_3|Y_1)][Y_2 - P(Y_2|Y_1)]\} \{E[Y_2 - P(Y_2|Y_1)]^2\}^{-1} [Y_2 - P(Y_2|Y_1)]$$
Aqui, o termo $\{E[Y_2 - P(Y_2|Y_1)]^2\}^{-1}$ √© exatamente o inverso do elemento $d_{22}$ da matriz $D$. A rela√ß√£o entre a fatora√ß√£o triangular e a atualiza√ß√£o de proje√ß√µes lineares √© mais um exemplo de como essa t√©cnica matem√°tica n√£o √© apenas uma abstra√ß√£o, mas sim uma ferramenta computacionalmente eficiente para diversas opera√ß√µes estat√≠sticas e econom√©tricas.

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar a conex√£o com a atualiza√ß√£o de proje√ß√µes, vamos usar os resultados do nosso exemplo anterior:
>
> Temos $d_{11} = 2$ e $d_{22} = 2.5$.  Na atualiza√ß√£o da proje√ß√£o, o termo $\{E[Y_2 - P(Y_2|Y_1)]^2\}^{-1}$ √© igual a $\frac{1}{d_{22}} = \frac{1}{2.5} = 0.4$.
>
> Assim, ao adicionar $Y_2$ no nosso modelo para projetar $Y_3$ (se houvesse), usar√≠amos a matriz $D$ para obter este termo, mostrando a aplica√ß√£o direta da fatora√ß√£o triangular na atualiza√ß√£o de proje√ß√µes lineares.
>
> A intui√ß√£o aqui √© que quanto maior a vari√¢ncia residual de $Y_2$ ($d_{22}$), menor ser√° o peso dado √† atualiza√ß√£o da proje√ß√£o utilizando $Y_2$.

### Aplica√ß√µes Pr√°ticas
As implica√ß√µes da diagonalidade da matriz de covari√¢ncia s√£o vastas, especialmente em:
1.  **Modelagem de S√©ries Temporais**: Facilita a an√°lise do erro de previs√£o ao longo do tempo, permitindo a identifica√ß√£o r√°pida de vari√°veis que podem n√£o estar sendo adequadamente modeladas.
2.  **An√°lise de Dados de Alta Dimensionalidade**: Reduz a complexidade do c√°lculo do MSE e an√°lise de vari√¢ncia, tornando vi√°vel o estudo de sistemas complexos.
3.  **Atualiza√ß√£o de Proje√ß√µes Lineares**: Fornece uma maneira direta de incluir novas informa√ß√µes, refinando as proje√ß√µes de forma sequencial e iterativa.
4.  **Valida√ß√£o de Modelos Estat√≠sticos**: Permite analisar os res√≠duos de proje√ß√£o de forma mais simples, garantindo que o modelo esteja capturando a variabilidade essencial dos dados.

### Conclus√£o

A estrutura diagonal da matriz de covari√¢ncia das vari√°veis transformadas, resultante da fatora√ß√£o triangular, √© uma caracter√≠stica poderosa que simplifica o c√°lculo do MSE e a an√°lise da vari√¢ncia dos res√≠duos. A matriz D, por si s√≥, resume informa√ß√µes cruciais sobre a variabilidade dos dados ap√≥s proje√ß√µes sequenciais, e facilita a atualiza√ß√£o de proje√ß√µes com novas informa√ß√µes [^4]. Ao criar conex√µes naturais com os t√≥picos abordados anteriormente, este cap√≠tulo demonstra a import√¢ncia da fatora√ß√£o triangular n√£o apenas como uma ferramenta de transforma√ß√£o, mas como um meio para obter proje√ß√µes lineares eficientes e com significado estat√≠stico, mostrando como as propriedades matem√°ticas da decomposi√ß√£o se traduzem em aplica√ß√µes pr√°ticas.

### Refer√™ncias
[^4]:  Express√£o [4.1.1] √© conhecida como o erro quadr√°tico m√©dio associado √† previs√£o $Y^*_{t+1}$, denotado $MSE(Y^*_{t+1}|t) = E(Y_{t+1} - Y^*_{t+1}|t)^2$. A previs√£o com o menor erro quadr√°tico m√©dio √© a expectativa de $Y_{t+1}$ condicional em X. Para verificar essa alega√ß√£o, considere basear $Y^*_{t+1}$ em qualquer fun√ß√£o g($X_t$). A representa√ß√£o da proje√ß√£o linear. Express√£o [4.1.1] √© conhecida como o erro quadr√°tico m√©dio associado √† previs√£o $Y^*_{t+1}$, denotado $MSE(Y^*_{t+1}|t) = E(Y_{t+1} - Y^*_{t+1}|t)^2$. A previs√£o com o menor erro quadr√°tico m√©dio √© a expectativa de $Y_{t+1}$ condicional em $X_t$. Para verificar essa alega√ß√£o, considere basear $Y^*_{t+1}$ em qualquer fun√ß√£o g($X_t$). A representa√ß√£o da proje√ß√£o linear e as f√≥rmulas para calcular a proje√ß√£o e seu erro. O conceito de proje√ß√£o linear e como ele se relaciona com a regress√£o de m√≠nimos quadrados ordin√°rios. C√°lculo dos coeficientes de proje√ß√£o. Matriz de proje√ß√£o e seu MSE. A formula√ß√£o do problema de proje√ß√£o e sua solu√ß√£o quando a proje√ß√£o √© realizada em um vetor.  A previs√£o como uma fun√ß√£o de e's defasados e a aplica√ß√£o do operador de aniquila√ß√£o.  A previs√£o como uma fun√ß√£o de Y's defasados, com a aplica√ß√£o da f√≥rmula de previs√£o de Wiener-Kolmogorov.  A previs√£o de um processo AR(1) e um processo AR(p). O conceito de proje√ß√µes iteradas. O processo de previs√£o de MA(1), MA(q) e ARMA(1,1). O problema da previs√£o com um n√∫mero finito de observa√ß√µes. A discuss√£o sobre como lidar com essa quest√£o.  A defini√ß√£o de proje√ß√µes lineares exatas para amostras finitas, as propriedades dessas proje√ß√µes e como calcular os coeficientes.  A representa√ß√£o de uma matriz sim√©trica definida positiva, juntamente com a deriva√ß√£o das matrizes A, D, o inverso e a unicidade.  A representa√ß√£o de uma matriz sim√©trica definida positiva, juntamente com a deriva√ß√£o das matrizes A, D, o inverso e a unicidade. Uma discuss√£o sobre o que a matriz triangular A significa no contexto de proje√ß√µes e como usar a fatora√ß√£o para atualizar proje√ß√µes lineares e sobre como as proje√ß√µes funcionam em combina√ß√£o com o conceito de proje√ß√£o iterada. O uso da fatora√ß√£o triangular da matriz de covari√¢ncia na previs√£o de um processo MA(1), com uma discuss√£o sobre o resultado da previs√£o, como as mudan√ßas e como lidar com processos n√£o invert√≠veis. O uso da fatora√ß√£o triangular de um segundo momento de uma matriz com proje√ß√µes lineares. A discuss√£o sobre como projetar vari√°veis e o significado da matriz H.
<!-- END -->
