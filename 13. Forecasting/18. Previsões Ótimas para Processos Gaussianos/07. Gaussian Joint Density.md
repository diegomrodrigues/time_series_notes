## A Densidade Conjunta de Vari√°veis Gaussianas e sua Aplica√ß√£o em Proje√ß√µes Lineares √ìtimas

### Introdu√ß√£o
Este cap√≠tulo explora a **densidade de probabilidade conjunta** de **vari√°veis Gaussianas** e como ela √© fundamental para derivar a **densidade condicional** e, consequentemente, estabelecer a equival√™ncia entre a **proje√ß√£o linear** e a **esperan√ßa condicional** em processos Gaussianos [^4.6]. O foco ser√° demonstrar como a estrutura espec√≠fica da densidade conjunta Gaussiana permite obter resultados t√£o importantes na teoria de previs√£o, e refor√ßar a ideia de que a proje√ß√£o linear fornece a previs√£o √≥tima irrestrita para tais processos.

### Conceitos Fundamentais
Nesta se√ß√£o, exploraremos as propriedades da densidade conjunta de vari√°veis Gaussianas, como us√°-la para derivar a densidade condicional, e como essa conex√£o leva √† equival√™ncia entre proje√ß√µes lineares e esperan√ßas condicionais.

#### Densidade Conjunta de Vari√°veis Gaussianas
Considere dois vetores aleat√≥rios, $Y_1$ de dimens√£o $n_1$ e $Y_2$ de dimens√£o $n_2$, cada um com suas m√©dias, $\mu_1$ e $\mu_2$, respectivamente. Se esses vetores s√£o gaussianos, a **densidade de probabilidade conjunta** do vetor combinado $Y = \begin{bmatrix} Y_1 \\ Y_2 \end{bmatrix}$ √© dada por [^4.6]:
$$
f_{Y_1,Y_2}(y_1,y_2) = \frac{1}{(2\pi)^{\frac{n_1+n_2}{2}} |\Omega|^{1/2}} \exp \left\{ -\frac{1}{2} \begin{bmatrix} y_1 - \mu_1 \\ y_2 - \mu_2 \end{bmatrix}' \Omega^{-1} \begin{bmatrix} y_1 - \mu_1 \\ y_2 - \mu_2 \end{bmatrix} \right\}
$$
onde $\Omega$ √© a matriz de covari√¢ncia conjunta:
$$
\Omega = \begin{bmatrix}
  \Omega_{11} & \Omega_{12} \\
  \Omega_{21} & \Omega_{22} \\
\end{bmatrix}
$$
Na qual $\Omega_{11}$ √© a matriz de covari√¢ncia de $Y_1$, $\Omega_{22}$ √© a matriz de covari√¢ncia de $Y_2$ e $\Omega_{12} = \Omega_{21}'$ √© a matriz de covari√¢ncia entre $Y_1$ e $Y_2$ [^4.6].  A express√£o para a densidade conjunta Gaussiana √© fundamental porque ela descreve completamente a probabilidade de observa√ß√£o de qualquer combina√ß√£o de valores de $Y_1$ e $Y_2$.

> üí° **Exemplo Num√©rico:**
> Considere duas vari√°veis Gaussianas, $Y_1$ e $Y_2$, onde $Y_1$ representa o retorno de um ativo no primeiro per√≠odo e $Y_2$ representa o retorno no segundo per√≠odo. Assume-se que as m√©dias s√£o $\mu_1 = 0.05$ e $\mu_2 = 0.08$. A matriz de covari√¢ncia √©:
> $$
> \Omega = \begin{bmatrix}
>   0.0025 & 0.001 \\
>   0.001 & 0.0049 \\
> \end{bmatrix}
> $$
> A densidade conjunta √© dada por:
> $$
> f_{Y_1,Y_2}(y_1,y_2) = \frac{1}{2\pi \sqrt{|\Omega|}} \exp \left\{ -\frac{1}{2} \begin{bmatrix} y_1 - 0.05 \\ y_2 - 0.08 \end{bmatrix}' \Omega^{-1} \begin{bmatrix} y_1 - 0.05 \\ y_2 - 0.08 \end{bmatrix} \right\}
> $$
> onde
> $$
>  |\Omega| = (0.0025 * 0.0049) - (0.001 * 0.001) = 0.00001225 - 0.000001 = 0.00001125
> $$
> e
> $$
> \Omega^{-1} = \frac{1}{0.00001125}\begin{bmatrix}
>   0.0049 & -0.001 \\
>   -0.001 & 0.0025 \\
> \end{bmatrix}
> $$
> Esta fun√ß√£o descreve a distribui√ß√£o conjunta dos retornos dos dois ativos em qualquer ponto do espa√ßo amostral. Ela √© fundamental para derivar as distribui√ß√µes marginais e condicionais.
>
> Para visualizar, podemos gerar amostras aleat√≥rias dessa distribui√ß√£o bivariada e plotar um gr√°fico de dispers√£o.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> mu = np.array([0.05, 0.08])
> cov = np.array([[0.0025, 0.001], [0.001, 0.0049]])
>
> samples = np.random.multivariate_normal(mu, cov, size=500)
>
> plt.figure(figsize=(8, 6))
> plt.scatter(samples[:, 0], samples[:, 1], alpha=0.6)
> plt.xlabel("Retorno do Ativo 1 (\$Y_1\$)")
> plt.ylabel("Retorno do Ativo 2 (\$Y_2\$)")
> plt.title("Distribui√ß√£o Conjunta de Retornos de Ativos")
> plt.grid(True)
> plt.show()
> ```
> O gr√°fico resultante mostrar√° uma elipse de pontos, refletindo a correla√ß√£o entre as duas vari√°veis.
> A forma da elipse √© determinada pela matriz de covari√¢ncia $\Omega$.

**Proposi√ß√£o 1** Se $Y_1$ e $Y_2$ s√£o vetores gaussianos, ent√£o qualquer combina√ß√£o linear deles, $a'Y_1 + b'Y_2$, onde $a$ e $b$ s√£o vetores de constantes, tamb√©m √© uma vari√°vel aleat√≥ria gaussiana.
*Proof:* Uma combina√ß√£o linear de vari√°veis gaussianas √© tamb√©m gaussiana. Esta propriedade √© amplamente conhecida na teoria de probabilidade e pode ser demonstrada usando fun√ß√µes caracter√≠sticas ou atrav√©s da forma quadr√°tica presente na defini√ß√£o da distribui√ß√£o gaussiana. $\blacksquare$

#### Deriva√ß√£o da Densidade Condicional
A **densidade condicional** de $Y_2$ dado $Y_1$, denotada por $f_{Y_2|Y_1}(y_2|y_1)$, descreve a distribui√ß√£o de probabilidade de $Y_2$ quando sabemos o valor de $Y_1$. Essa densidade condicional √© crucial para realizar previs√µes e √© obtida pela divis√£o da densidade conjunta pela densidade marginal de $Y_1$. A densidade marginal $f_{Y_1}(y_1)$ √© obtida da densidade conjunta $f_{Y_1, Y_2}(y_1, y_2)$ atrav√©s de integra√ß√£o sobre todo o espa√ßo de $Y_2$ [^4.6].
$$
f_{Y_1}(y_1) = \int f_{Y_1, Y_2}(y_1, y_2) \, dy_2
$$

A densidade condicional √© dada por [^4.6]:
$$
f_{Y_2|Y_1}(y_2|y_1) = \frac{f_{Y_1,Y_2}(y_1,y_2)}{f_{Y_1}(y_1)} = \frac{1}{(2\pi)^{n_2/2} |H|^{1/2}} \exp \left\{ -\frac{1}{2} (y_2 - m)'H^{-1}(y_2 - m) \right\}
$$
onde
$$
m = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(y_1-\mu_1)
$$
e
$$
H = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}
$$
A import√¢ncia desta express√£o reside no fato de que ela nos d√° a distribui√ß√£o de $Y_2$ dado um valor espec√≠fico de $Y_1$. Ela nos mostra que a distribui√ß√£o condicional tamb√©m √© normal, e que a m√©dia condicional √© uma fun√ß√£o linear de $Y_1$.

> üí° **Exemplo Num√©rico:**
> Usando o exemplo anterior, a densidade marginal de $Y_1$ √©:
> $$
> f_{Y_1}(y_1) = \frac{1}{\sqrt{2\pi \Omega_{11}}} \exp\left\{-\frac{(y_1 - \mu_1)^2}{2 \Omega_{11}}\right\} = \frac{1}{\sqrt{2\pi (0.0025)}} \exp\left\{-\frac{(y_1 - 0.05)^2}{2 (0.0025)}\right\}
> $$
> A densidade condicional de $Y_2$ dado $Y_1$ pode ser derivada da densidade conjunta e marginal, resultando numa distribui√ß√£o normal com m√©dia:
> $$
> m = 0.08 + \frac{0.001}{0.0025}(y_1 - 0.05) = 0.08 + 0.4(y_1 - 0.05)
> $$
> e vari√¢ncia
> $$
> H = 0.0049 - \frac{0.001^2}{0.0025} = 0.0049 - 0.0004 = 0.0045
> $$
> Desta forma,
> $$
> f_{Y_2|Y_1}(y_2|y_1) = \frac{1}{\sqrt{2\pi (0.0045)}} \exp \left\{-\frac{(y_2 - m)^2}{2 (0.0045)} \right\}
> $$
> Isso nos mostra a distribui√ß√£o condicional do retorno do ativo no segundo per√≠odo, dado o retorno do primeiro per√≠odo. Por exemplo, se observarmos $Y_1 = 0.06$, a m√©dia condicional de $Y_2$ ser√°:
> $$
> m = 0.08 + 0.4(0.06 - 0.05) = 0.08 + 0.4(0.01) = 0.084
> $$
> Assim, a distribui√ß√£o de $Y_2$ dado que $Y_1 = 0.06$ √© normal com m√©dia 0.084 e vari√¢ncia 0.0045.
>
> Podemos gerar amostras da densidade condicional, assumindo um valor espec√≠fico para $Y_1$ e visualizar essa distribui√ß√£o:
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy.stats import norm
>
> # Par√¢metros do exemplo
> mu_1 = 0.05
> mu_2 = 0.08
> omega_11 = 0.0025
> omega_12 = 0.001
> omega_21 = 0.001
> omega_22 = 0.0049
>
> # Valor espec√≠fico para Y1
> y1_value = 0.06
>
> # C√°lculo da m√©dia e vari√¢ncia condicionais
> m = mu_2 + (omega_12 / omega_11) * (y1_value - mu_1)
> H = omega_22 - (omega_12**2 / omega_11)
>
> # Gerando amostras da distribui√ß√£o condicional
> samples_conditional = np.random.normal(m, np.sqrt(H), size=500)
>
> # Visualizando a distribui√ß√£o condicional
> plt.figure(figsize=(8, 6))
> plt.hist(samples_conditional, bins=30, density=True, alpha=0.6, label='Distribui√ß√£o Condicional de $Y_2|Y_1$')
>
> # Sobrepondo a fun√ß√£o densidade de probabilidade
> x = np.linspace(m - 3*np.sqrt(H), m + 3*np.sqrt(H), 100)
> plt.plot(x, norm.pdf(x, m, np.sqrt(H)), 'r', label='PDF da Distribui√ß√£o Condicional')
>
> plt.xlabel("Retorno do Ativo 2 (\$Y_2\$)")
> plt.ylabel("Densidade de Probabilidade")
> plt.title("Distribui√ß√£o Condicional de $Y_2$ dado $Y_1 = 0.06$")
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
> Este histograma mostra como a distribui√ß√£o de $Y_2$ se concentra em torno da m√©dia condicional, e tem uma vari√¢ncia menor do que a vari√¢ncia marginal de $Y_2$.

**Lema 1** A densidade condicional $f_{Y_2|Y_1}(y_2|y_1)$ √© uma densidade normal com m√©dia e vari√¢ncia dadas por: $m = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(y_1-\mu_1)$ e $H = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}$, respectivamente.
*Proof:*
I. Sabemos que a densidade conjunta $f_{Y_1,Y_2}(y_1, y_2)$ de um vetor Gaussiano √© da forma:
$$
f_{Y_1,Y_2}(y_1,y_2) = \frac{1}{(2\pi)^{\frac{n_1+n_2}{2}} |\Omega|^{1/2}} \exp \left\{ -\frac{1}{2} \begin{bmatrix} y_1 - \mu_1 \\ y_2 - \mu_2 \end{bmatrix}' \Omega^{-1} \begin{bmatrix} y_1 - \mu_1 \\ y_2 - \mu_2 \end{bmatrix} \right\}
$$
II. A densidade marginal de $Y_1$ tamb√©m ser√° normal, com m√©dia $\mu_1$ e vari√¢ncia $\Omega_{11}$, e dada por:
$$
f_{Y_1}(y_1) = \frac{1}{(2\pi)^{\frac{n_1}{2}} |\Omega_{11}|^{1/2}} \exp \left\{ -\frac{1}{2} (y_1 - \mu_1)' \Omega_{11}^{-1} (y_1 - \mu_1) \right\}
$$
III. A densidade condicional $f_{Y_2|Y_1}(y_2|y_1)$ √© definida como a raz√£o entre a densidade conjunta e a densidade marginal:
$$
f_{Y_2|Y_1}(y_2|y_1) = \frac{f_{Y_1,Y_2}(y_1,y_2)}{f_{Y_1}(y_1)}
$$
IV. Substituindo as express√µes das densidades conjunta e marginal, e atrav√©s de manipula√ß√µes alg√©bricas para completar o quadrado na exponencial, obtemos que a densidade condicional tamb√©m √© normal e possui m√©dia $m = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(y_1-\mu_1)$ e vari√¢ncia $H = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}$.  $\blacksquare$

**Lema 1.1** A esperan√ßa condicional $E[Y_2|Y_1]$ √© uma fun√ß√£o linear de $Y_1$.
*Proof:*
I. Do Lema 1, a m√©dia da distribui√ß√£o condicional $f_{Y_2|Y_1}(y_2|y_1)$ √© dada por:
$$m = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(y_1-\mu_1)$$
II. Por defini√ß√£o, a esperan√ßa condicional $E[Y_2|Y_1]$ √© igual √† m√©dia da distribui√ß√£o condicional:
$$E[Y_2|Y_1] = m$$
III. Logo, $E[Y_2|Y_1] = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)$, que √© uma fun√ß√£o linear de $Y_1$, pois $\mu_2$, $\Omega_{21}$, $\Omega_{11}$ e $\mu_1$ s√£o constantes. $\blacksquare$

#### Equival√™ncia entre Proje√ß√£o Linear e Esperan√ßa Condicional
A partir da densidade condicional, a **esperan√ßa condicional** de $Y_2$ dado $Y_1$ √© dada pela m√©dia da distribui√ß√£o condicional [^4.6]:
$$
E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
Por outro lado, a **proje√ß√£o linear** de $Y_2$ sobre $Y_1$ √© dada por [^4.5]:
$$
\hat{E}(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
Comparando as duas express√µes, vemos que elas s√£o id√™nticas. Portanto, para processos Gaussianos, a esperan√ßa condicional coincide com a proje√ß√£o linear. Este resultado crucial simplifica enormemente o processo de previs√£o, pois a proje√ß√£o linear, mais simples computacionalmente, fornece a melhor previs√£o poss√≠vel.

> üí° **Exemplo Num√©rico:**
> Usando o exemplo anterior, podemos calcular a esperan√ßa condicional da seguinte maneira:
> $$
> E(Y_2|Y_1) = m = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1) = 0.08 + \frac{0.001}{0.0025}(Y_1-0.05) = 0.08 + 0.4(Y_1-0.05)
> $$
> Note que a esperan√ßa condicional √© uma fun√ß√£o linear de $Y_1$, um resultado t√≠pico de processos Gaussianos. Al√©m disso, como j√° demonstrado, ela coincide com a proje√ß√£o linear.
>
> Podemos ilustrar essa equival√™ncia com um exemplo concreto. Suponha que observamos $Y_1 = 0.06$. Ent√£o:
>
> Esperan√ßa Condicional:
> $$ E[Y_2|Y_1 = 0.06] = 0.08 + 0.4(0.06 - 0.05) = 0.08 + 0.004 = 0.084 $$
> Proje√ß√£o Linear:
> $$ \hat{E}[Y_2|Y_1 = 0.06] = 0.08 + 0.4(0.06 - 0.05) = 0.08 + 0.004 = 0.084 $$
>
> Ambos os valores s√£o id√™nticos, demonstrando a equival√™ncia entre a esperan√ßa condicional e a proje√ß√£o linear para este caso.

**Teorema 2** (Equival√™ncia entre Proje√ß√£o Linear e Esperan√ßa Condicional)
Para processos Gaussianos, a esperan√ßa condicional $E(Y_2|Y_1)$, que √© a previs√£o √≥tima, √© id√™ntica √† proje√ß√£o linear $\hat{E}(Y_2|Y_1)$.
*Proof:*
I. Foi estabelecido que a esperan√ßa condicional √© dada por:
$$E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)$$
II. A proje√ß√£o linear √© dada por:
$$\hat{E}(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)$$
III. As express√µes s√£o id√™nticas, logo $E(Y_2|Y_1) = \hat{E}(Y_2|Y_1)$.
Portanto, para processos Gaussianos, a esperan√ßa condicional e a proje√ß√£o linear coincidem, ou seja, a proje√ß√£o linear √© a previs√£o √≥tima. ‚ñ†

**Teorema 2.1** (Propriedade da Ortogonalidade)
O erro da proje√ß√£o linear, $Y_2 - \hat{E}(Y_2|Y_1)$, √© ortogonal a qualquer fun√ß√£o linear de $Y_1$. Em particular, $E[(Y_2 - \hat{E}(Y_2|Y_1))Y_1] = 0$.
*Proof:*
I. Definimos o erro da proje√ß√£o linear como $\epsilon = Y_2 - \hat{E}(Y_2|Y_1) = Y_2 - [\mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)]$.
II. Calculando a covari√¢ncia entre o erro e $Y_1$, temos:
    $Cov(\epsilon, Y_1) = E[(\epsilon - E[\epsilon])(Y_1 - E[Y_1])]$. Como $E[\epsilon] = 0$, pois $E[\hat{E}(Y_2|Y_1)] = E[Y_2]$ devido √† propriedade de n√£o-viesamento da proje√ß√£o,
    $Cov(\epsilon, Y_1) = E[\epsilon (Y_1-\mu_1)] = E[(Y_2 - \mu_2 - \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1))(Y_1-\mu_1)]$.
    $Cov(\epsilon, Y_1) = E[(Y_2-\mu_2)(Y_1-\mu_1)] - \Omega_{21}\Omega_{11}^{-1}E[(Y_1-\mu_1)(Y_1-\mu_1)]$
    $Cov(\epsilon, Y_1) = \Omega_{21} - \Omega_{21}\Omega_{11}^{-1}\Omega_{11} = \Omega_{21} - \Omega_{21} = 0$.
III. Como a covari√¢ncia √© zero, o erro da proje√ß√£o linear √© ortogonal a $Y_1$. De modo mais geral, como a proje√ß√£o linear √© a melhor previs√£o dentro do espa√ßo de fun√ß√µes lineares de $Y_1$, o erro √© ortogonal a qualquer fun√ß√£o linear de $Y_1$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Vamos ilustrar a propriedade de ortogonalidade usando nosso exemplo. Assumindo que $Y_1 = 0.06$, calculamos $\hat{E}(Y_2|Y_1 = 0.06) = 0.084$.
>
> O erro da proje√ß√£o √© $\epsilon = Y_2 - 0.084$.  Vamos gerar 1000 amostras de $Y_2$ condicionadas a $Y_1 = 0.06$, calcular o erro para cada amostra e a covari√¢ncia amostral entre o erro e $Y_1$:
> ```python
> import numpy as np
>
> # Par√¢metros do exemplo
> mu_1 = 0.05
> mu_2 = 0.08
> omega_11 = 0.0025
> omega_12 = 0.001
> omega_21 = 0.001
> omega_22 = 0.0049
>
> # Valor espec√≠fico para Y1
> y1_value = 0.06
>
> # C√°lculo da m√©dia e vari√¢ncia condicionais
> m = mu_2 + (omega_12 / omega_11) * (y1_value - mu_1)
> H = omega_22 - (omega_12**2 / omega_11)
>
> # Gerando amostras condicionais de Y2
> num_samples = 1000
> samples_conditional_y2 = np.random.normal(m, np.sqrt(H), size=num_samples)
>
> # Calculando o erro de proje√ß√£o
> prediction_error = samples_conditional_y2 - m
>
> # Calculando a covari√¢ncia amostral
> sample_covariance = np.cov(prediction_error, np.full(num_samples, y1_value))[0,1]
>
> print(f"Covari√¢ncia amostral entre o erro e Y1: {sample_covariance:.6f}")
> ```
> O resultado impresso demonstrar√° que a covari√¢ncia amostral √© muito pr√≥xima de zero, confirmando a propriedade de ortogonalidade.

### Conclus√£o
A an√°lise da **densidade de probabilidade conjunta** de **vari√°veis Gaussianas** nos permite derivar a **densidade condicional** e, consequentemente, estabelecer que, em processos Gaussianos, a **esperan√ßa condicional**, que √© a previs√£o √≥tima, coincide com a **proje√ß√£o linear** [^4.6]. Essa equival√™ncia √© fundamental na teoria de previs√£o, pois simplifica os c√°lculos e justifica o uso das proje√ß√µes lineares como ferramenta para fazer infer√™ncias sobre o comportamento futuro de vari√°veis. A utiliza√ß√£o da estrutura das distribui√ß√µes Gaussianas combinada com a √°lgebra linear fornece um forte arcabou√ßo para an√°lise e previs√£o em s√©ries temporais, e a rela√ß√£o entre densidade conjunta, condicional, proje√ß√µes e esperan√ßas condicionais √© crucial em teoria de probabilidade e estat√≠stica.

### Refer√™ncias
[^4.6]: Se√ß√£o 4.6 do texto, incluindo as equa√ß√µes [4.6.1] at√© [4.6.7].
[^4.5]: Se√ß√£o 4.5 do texto, incluindo as equa√ß√µes [4.5.1] at√© [4.5.16].
<!-- END -->
