## Previs√µes √ìtimas para Processos Gaussianos

### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise das previs√µes √≥timas, explorando especificamente o caso de **processos Gaussianos**. Em cap√≠tulos anteriores, estabelecemos m√©todos para calcular previs√µes √≥timas dentro da classe de fun√ß√µes lineares das vari√°veis nas quais a previs√£o se baseia [^4.1, ^4.2, ^4.3].  Agora, investigaremos como as propriedades espec√≠ficas dos processos Gaussianos afetam a natureza dessas previs√µes. O foco ser√° demonstrar que, para esses processos, a previs√£o √≥tima irrestrita possui uma forma linear e √©, portanto, equivalente √† proje√ß√£o linear.

### Conceitos Fundamentais
Inicialmente, definiremos formalmente o conceito de **processos Gaussianos** e como a esperan√ßa condicional se comporta neste contexto. Em seguida, demonstraremos que essa esperan√ßa condicional, que √© a previs√£o √≥tima irrestrita, coincide com a proje√ß√£o linear quando o processo √© Gaussiano.

#### Processos Gaussianos
Um **processo Gaussiano** √© um processo estoc√°stico onde qualquer combina√ß√£o linear de suas vari√°veis aleat√≥rias tem uma distribui√ß√£o normal. Especificamente, se temos um vetor aleat√≥rio $Y = (Y_1, Y_2, ..., Y_n)$, ele √© Gaussiano se qualquer combina√ß√£o linear $a'Y$, onde $a$ √© um vetor de constantes, tem uma distribui√ß√£o normal.
**Lema 1**
Se $Y$ √© um vetor aleat√≥rio Gaussiano, ent√£o qualquer subvetor de $Y$ tamb√©m √© Gaussiano.
*Proof:* Seja $Y = (Y_1, Y_2, \ldots, Y_n)$ um vetor Gaussiano, e seja $Z$ um subvetor de $Y$, por exemplo $Z=(Y_{i_1},Y_{i_2},\ldots,Y_{i_k})$ com $1 \leq i_1 < i_2 < \ldots< i_k \leq n$.  Qualquer combina√ß√£o linear de $Z$, dada por $b'Z$, pode ser escrita como $a'Y$ onde o vetor $a$ tem os elementos de $b$ nas posi√ß√µes correspondentes a $Z$ e 0 nas restantes. Como $Y$ √© gaussiano, $a'Y$ tem distribui√ß√£o normal, logo $b'Z$ tamb√©m, portanto $Z$ √© gaussiano. $\blacksquare$

#### Esperan√ßa Condicional e Processos Gaussianos
A **esperan√ßa condicional** $E[Y_2|Y_1]$ representa a melhor estimativa de $Y_2$ dado o conhecimento de $Y_1$ e, como vimos em se√ß√µes anteriores, √© a previs√£o √≥tima irrestrita. Para um processo Gaussiano, essa esperan√ßa condicional assume uma forma particularmente simples, que se revela ser linear.

Considere $Y_1$ um vetor $(n_1 \times 1)$ com m√©dia $\mu_1$ e $Y_2$ um vetor $(n_2 \times 1)$ com m√©dia $\mu_2$. A matriz de vari√¢ncia-covari√¢ncia do vetor combinado $(Y_1, Y_2)$ √© dada por [^4.6]:
$$
\begin{bmatrix}
  E[(Y_1 - \mu_1)(Y_1 - \mu_1)'] & E[(Y_1 - \mu_1)(Y_2 - \mu_2)'] \\
  E[(Y_2 - \mu_2)(Y_1 - \mu_1)'] & E[(Y_2 - \mu_2)(Y_2 - \mu_2)'] \\
\end{bmatrix}
=
\begin{bmatrix}
  \Omega_{11} & \Omega_{12} \\
  \Omega_{21} & \Omega_{22} \\
\end{bmatrix}
$$

Se $Y_1$ e $Y_2$ s√£o Gaussianos, a densidade de probabilidade conjunta √© dada por [^4.6]:
$$
f_{Y_1,Y_2}(y_1,y_2) = \frac{1}{(2\pi)^{\frac{n_1+n_2}{2}} |\Omega|^{1/2}} \exp \left\{ -\frac{1}{2} \begin{bmatrix} y_1 - \mu_1 \\ y_2 - \mu_2 \end{bmatrix}' \Omega^{-1} \begin{bmatrix} y_1 - \mu_1 \\ y_2 - \mu_2 \end{bmatrix} \right\}
$$
Onde  $\Omega$ √© a matriz de covari√¢ncia conjunta. A densidade condicional de $Y_2$ dado $Y_1$ pode ser escrita como [^4.6]:
$$
f_{Y_2|Y_1}(y_2|y_1) =  \frac{f_{Y_1,Y_2}(y_1,y_2)}{f_{Y_1}(y_1)} = \frac{1}{(2\pi)^{n_2/2} |H|^{1/2}} \exp \left\{ -\frac{1}{2} (y_2 - m)'H^{-1}(y_2 - m) \right\}
$$
onde
$$
m = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(y_1-\mu_1)
$$
e
$$
H = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}
$$
Desta forma, a esperan√ßa condicional √© [^4.6]:
$$
E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
Que √© uma fun√ß√£o linear de $Y_1$ e este resultado √© uma caracter√≠stica crucial dos processos Gaussianos.

> üí° **Exemplo Num√©rico:**
> Suponha que temos duas vari√°veis aleat√≥rias Gaussianas, $Y_1$ e $Y_2$, onde $Y_1$ representa o pre√ßo de uma a√ß√£o no dia anterior e $Y_2$ o pre√ßo da a√ß√£o hoje. Assumimos que as m√©dias s√£o $\mu_1 = 10$ e $\mu_2 = 12$. A matriz de covari√¢ncia √©:
> $$
> \Omega = \begin{bmatrix}
>   \Omega_{11} & \Omega_{12} \\
>   \Omega_{21} & \Omega_{22} \\
> \end{bmatrix} = \begin{bmatrix}
>   4 & 2 \\
>   2 & 9 \\
> \end{bmatrix}
> $$
> Ent√£o, $\Omega_{11} = 4$, $\Omega_{12} = \Omega_{21} = 2$, e $\Omega_{22} = 9$. Se quisermos calcular $E[Y_2|Y_1]$, precisamos de $\Omega_{21}\Omega_{11}^{-1}$:
> $$
> \Omega_{21}\Omega_{11}^{-1} = 2 \cdot 4^{-1} = 2 \cdot 0.25 = 0.5
> $$
> A esperan√ßa condicional √© dada por:
> $$
> E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1 - \mu_1) = 12 + 0.5(Y_1 - 10)
> $$
> Portanto, se observarmos $Y_1 = 14$, nossa previs√£o para $Y_2$ seria:
> $$
> E(Y_2|Y_1 = 14) = 12 + 0.5(14 - 10) = 12 + 0.5(4) = 12 + 2 = 14
> $$
> Este exemplo demonstra como a esperan√ßa condicional fornece uma previs√£o linear baseada na observa√ß√£o de $Y_1$.

**Teorema 1** (Caracteriza√ß√£o da Esperan√ßa Condicional)
Se $Y_1$ e $Y_2$ s√£o vetores aleat√≥rios Gaussianos, ent√£o a esperan√ßa condicional $E[Y_2 | Y_1]$ √© uma fun√ß√£o linear de $Y_1$ e sua vari√¢ncia condicional √© constante, ou seja, n√£o depende do valor de $Y_1$.

*Proof:*
Vamos provar que a esperan√ßa condicional $E[Y_2 | Y_1]$ √© uma fun√ß√£o linear de $Y_1$ e sua vari√¢ncia condicional √© constante.
I. A partir da dedu√ß√£o anterior, temos que a esperan√ßa condicional √© dada por:
   $$E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)$$
II. Esta express√£o √© uma fun√ß√£o linear de $Y_1$, pois √© da forma $a + BY_1$, onde $a = \mu_2 - \Omega_{21}\Omega_{11}^{-1}\mu_1$ e $B = \Omega_{21}\Omega_{11}^{-1}$.
III. A matriz de covari√¢ncia condicional $H$ √© dada por:
   $$H = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}$$
IV. Observe que $H$ n√£o depende de $Y_1$. Portanto, a vari√¢ncia condicional √© constante e n√£o depende do valor de $Y_1$.
V. Assim, provamos que a esperan√ßa condicional $E[Y_2 | Y_1]$ √© uma fun√ß√£o linear de $Y_1$ e sua vari√¢ncia condicional √© constante. $\blacksquare$

#### Proje√ß√£o Linear e Processos Gaussianos
Por outro lado, a **proje√ß√£o linear** de $Y_2$ em $Y_1$ (com um termo constante) √© dada por [^4.6]:
$$
\hat{E}(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
Note que esta express√£o √© id√™ntica √† esperan√ßa condicional para processos Gaussianos. Assim, para processos Gaussianos, a esperan√ßa condicional, que representa a previs√£o √≥tima irrestrita, coincide com a proje√ß√£o linear.

> üí° **Exemplo Num√©rico:**
> Usando os mesmos dados do exemplo anterior, a proje√ß√£o linear de $Y_2$ em $Y_1$ √©:
> $$
> \hat{E}(Y_2|Y_1) = 12 + 0.5(Y_1 - 10)
> $$
> Novamente, se $Y_1 = 14$, ter√≠amos:
> $$
> \hat{E}(Y_2|Y_1 = 14) = 12 + 0.5(14 - 10) = 14
> $$
> Este resultado √© id√™ntico √† esperan√ßa condicional, ilustrando que, para processos Gaussianos, a proje√ß√£o linear coincide com a esperan√ßa condicional.

**Corol√°rio 1.1**
Para processos Gaussianos, a proje√ß√£o linear $\hat{E}(Y_2|Y_1)$ √© a melhor previs√£o de $Y_2$ dado $Y_1$ no sentido de minimizar o erro quadr√°tico m√©dio, ou seja, ela coincide com a esperan√ßa condicional $E(Y_2|Y_1)$.

*Proof:*
Vamos provar que para processos Gaussianos, a proje√ß√£o linear $\hat{E}(Y_2|Y_1)$ √© a melhor previs√£o de $Y_2$ dado $Y_1$.
I. Foi estabelecido que para processos Gaussianos, a esperan√ßa condicional √©:
   $$E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)$$
II. A proje√ß√£o linear √© definida como:
   $$\hat{E}(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)$$
III. Portanto, vemos que $E(Y_2|Y_1) = \hat{E}(Y_2|Y_1)$ para processos Gaussianos.
IV. Sabemos que a esperan√ßa condicional $E(Y_2|Y_1)$ √© a melhor previs√£o de $Y_2$ dado $Y_1$ no sentido de minimizar o erro quadr√°tico m√©dio.
V. Consequentemente, para processos Gaussianos, a proje√ß√£o linear $\hat{E}(Y_2|Y_1)$, que coincide com $E(Y_2|Y_1)$, tamb√©m √© a melhor previs√£o no sentido de minimizar o erro quadr√°tico m√©dio. $\blacksquare$

### Conclus√£o
Em resumo, para processos Gaussianos, a **previs√£o √≥tima irrestrita** √© dada pela **esperan√ßa condicional**, que se iguala √† **proje√ß√£o linear** [^4.6]. Este resultado implica que, ao lidar com processos Gaussianos, a proje√ß√£o linear, comumente utilizada pela sua simplicidade computacional, n√£o leva a nenhuma perda de informa√ß√£o, gerando a previs√£o √≥tima sem perda de generalidade [^4.6]. Este resultado fornece uma forte justificativa para o uso de m√©todos lineares no contexto de processos Gaussianos, onde a complexidade de calcular a esperan√ßa condicional total √© evitada sem sacrificar a qualidade da previs√£o.  A an√°lise destaca a import√¢ncia da estrutura espec√≠fica dos processos Gaussianos para simplificar e otimizar os processos de previs√£o, proporcionando uma ferramenta poderosa para modelagem de dados.

### Refer√™ncias
[^4.1]:  Express√£o [4.1.1] e seguintes do texto.
[^4.2]: Se√ß√µes 4.2 e seguintes do texto.
[^4.3]: Se√ß√µes 4.3 e seguintes do texto.
[^4.6]: Se√ß√£o 4.6 do texto, incluindo equa√ß√µes [4.6.1] at√© [4.6.7].
<!-- END -->
