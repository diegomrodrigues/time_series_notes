## A Abordagem de Atualiza√ß√£o de Proje√ß√µes por Fatora√ß√£o Triangular e sua Aplica√ß√£o em Previs√µes Gaussianas

### Introdu√ß√£o
Este cap√≠tulo continua a investiga√ß√£o de **previs√µes √≥timas** em **processos Gaussianos**, focando em como as **proje√ß√µes lineares** podem ser atualizadas √† medida que novas informa√ß√µes se tornam dispon√≠veis. Particularmente, exploraremos a **fatora√ß√£o triangular** da matriz de covari√¢ncia e como ela pode ser utilizada para derivar uma **abordagem de atualiza√ß√£o** para proje√ß√µes lineares [^4.5]. Adicionalmente, como j√° demonstrado, em processos Gaussianos, a proje√ß√£o linear coincide com a esperan√ßa condicional, que √© a previs√£o √≥tima, de forma que as propriedades dos processos Gaussianos s√£o fundamentais para que essa abordagem se torne interessante.

### Conceitos Fundamentais
Nesta se√ß√£o, introduziremos a **fatora√ß√£o triangular** da matriz de covari√¢ncia, como ela nos permite obter representa√ß√µes dos res√≠duos de proje√ß√µes lineares, como essas representa√ß√µes podem ser utilizadas para derivar um processo de atualiza√ß√£o das proje√ß√µes, e, finalmente, como ela se conecta com a natureza de processos Gaussianos.

#### Fatora√ß√£o Triangular e Representa√ß√£o de Res√≠duos

Conforme explorado anteriormente [^4.4], qualquer matriz sim√©trica definida positiva $\Omega$ pode ser decomposta de forma √∫nica na forma
$$
\Omega = ADA'
$$
onde $A$ √© uma matriz triangular inferior com 1's na diagonal principal e $D$ √© uma matriz diagonal com entradas positivas. Essa **fatora√ß√£o triangular** tem aplica√ß√µes significativas na an√°lise de proje√ß√µes lineares.
Considere um vetor de vari√°veis aleat√≥rias $Y = (Y_1, Y_2, \ldots, Y_n)'$, cuja matriz de covari√¢ncia √© dada por $\Omega = E(YY')$. Ao aplicar a fatora√ß√£o triangular, definimos um novo vetor de vari√°veis aleat√≥rias $\tilde{Y} = A^{-1}Y$, que representa uma transforma√ß√£o linear do vetor original. A matriz de covari√¢ncia de $\tilde{Y}$ √© ent√£o:
$$
E(\tilde{Y}\tilde{Y}') = E(A^{-1}YY' (A^{-1})') = A^{-1}\Omega(A^{-1})' = A^{-1}ADA'(A^{-1})' = D
$$
Como a matriz de covari√¢ncia de $\tilde{Y}$ √© diagonal, isso significa que os elementos de $\tilde{Y}$ s√£o n√£o correlacionados, que √© o mesmo que dizer que os elementos de $\tilde{Y}$ s√£o res√≠duos de proje√ß√µes lineares. Especificamente, $\tilde{Y}_i$ √© o res√≠duo da proje√ß√£o linear de $Y_i$ sobre $Y_1,\ldots,Y_{i-1}$.

> üí° **Exemplo Num√©rico:**
> Para ilustrar como a fatora√ß√£o triangular pode ser usada para construir res√≠duos, considere um vetor de tr√™s vari√°veis $Y=(Y_1,Y_2,Y_3)$ com a seguinte matriz de covari√¢ncia:
> $$
> \Omega = \begin{bmatrix}
>   4 & 2 & 1 \\
>   2 & 9 & 3 \\
>   1 & 3 & 16 \\
> \end{bmatrix}
> $$
> Usando uma biblioteca em Python para calcular a fatora√ß√£o de Cholesky (que √© essencialmente a fatora√ß√£o triangular), podemos encontrar as matrizes $A$ e $D$. Note que $D$ ser√° a matriz diagonal dos quadrados dos elementos da diagonal da matriz triangular inferior ($L$) obtida na decomposi√ß√£o de Cholesky, e $A$ √© obtida por $L$ dividido pelos seus elementos da diagonal principal.
> ```python
> import numpy as np
> from scipy.linalg import cholesky
>
> Omega = np.array([[4, 2, 1],
>                  [2, 9, 3],
>                  [1, 3, 16]])
>
> L = cholesky(Omega, lower=True)
> D = np.diag(np.diag(L)**2)
> A = L @ np.diag(1/np.diag(L))
>
> print("Matriz A:\n", A)
> print("Matriz D:\n", D)
> ```
> O c√≥digo acima gera a seguinte sa√≠da:
> ```
> Matriz A:
>  [[1.         0.         0.        ]
>  [0.5        1.         0.        ]
>  [0.25       0.33333333 1.        ]]
> Matriz D:
>  [[ 4.         0.         0.        ]
>  [ 0.         8.         0.        ]
>  [ 0.         0.        14.91666667]]
> ```
> Assim, obtemos as seguintes matrizes:
> $$
> A = \begin{bmatrix}
>   1 & 0 & 0 \\
>   0.5 & 1 & 0 \\
>   0.25 & 0.333 & 1 \\
> \end{bmatrix}, \quad D = \begin{bmatrix}
>   4 & 0 & 0 \\
>   0 & 8 & 0 \\
>   0 & 0 & 14.9166 \\
> \end{bmatrix}
> $$
> Ao definirmos $\tilde{Y}=A^{-1}Y$, temos:
> $$
> \tilde{Y} = \begin{bmatrix}
>   Y_1 \\
>   Y_2 - 0.5Y_1 \\
>   Y_3 - 0.25Y_1 - 0.333(Y_2-0.5Y_1) \\
> \end{bmatrix}
> $$
> Note que:
>  -  $\tilde{Y}_1 = Y_1$, √© o valor original, sem proje√ß√µes.
>  - $\tilde{Y}_2$ √© o res√≠duo da proje√ß√£o linear de $Y_2$ sobre $Y_1$.
>  - $\tilde{Y}_3$ √© o res√≠duo da proje√ß√£o linear de $Y_3$ sobre $Y_1$ e $Y_2$.
> A matriz $D$ √© diagonal e seus elementos representam as vari√¢ncias dos res√≠duos, ou seja, $Var(\tilde{Y}_1) = 4$, $Var(\tilde{Y}_2) = 8$ e $Var(\tilde{Y}_3) = 14.9166$.

**Lema 1** A matriz $A$ na fatora√ß√£o triangular $\Omega = ADA'$ √© √∫nica e pode ser obtida sequencialmente.
*Prova:*
A unicidade da fatora√ß√£o $ADA'$ j√° foi estabelecida previamente [^4.4]. Para a obten√ß√£o sequencial, podemos notar que, na pr√°tica, a constru√ß√£o de $A$ e $D$ se d√° atrav√©s de um processo de elimina√ß√£o gaussiana. Na primeira etapa, dividimos a primeira linha de $\Omega$ por $\sqrt{\Omega_{11}}$, obtendo a primeira coluna de $A$ e o primeiro elemento de $D$. Em seguida, subtra√≠mos m√∫ltiplos da primeira linha de $\Omega$ das demais linhas para zerar os elementos abaixo de $\Omega_{11}$ na primeira coluna. Repetimos o processo para as submatrizes restantes, garantindo que os elementos abaixo da diagonal principal de $A$ s√£o calculados recursivamente e que $D$ √© formada pelos piv√¥s resultantes. Cada passo desse processo √© √∫nico, o que garante que $A$ √© obtida de forma √∫nica. Essa constru√ß√£o sequencial √© crucial para a implementa√ß√£o computacional da fatora√ß√£o triangular, e sua forma recursiva √© √∫til para processos de atualiza√ß√£o.

#### Processo de Atualiza√ß√£o de Proje√ß√µes Lineares

O processo de atualiza√ß√£o de proje√ß√µes lineares √© baseado na ideia de adicionar a uma proje√ß√£o linear inicial, um termo que corrige a previs√£o ao incorporar novas informa√ß√µes [^4.5]. Suponha que temos uma proje√ß√£o linear inicial de $Y_3$ sobre $Y_1$, dada por $P(Y_3|Y_1) = \Omega_{31}\Omega_{11}^{-1}Y_1$. Introduzimos, agora, a informa√ß√£o de $Y_2$. Usando a nota√ß√£o anterior, podemos reescrever a proje√ß√£o de $Y_3$ usando a informa√ß√£o de $Y_1$ e $Y_2$ como:
$$
P(Y_3|Y_1,Y_2) = P(Y_3|Y_1) + h_{32}h_{22}^{-1}[Y_2 - P(Y_2|Y_1)]
$$
onde:
 - $P(Y_3|Y_1)$ √© a proje√ß√£o linear inicial de $Y_3$ sobre $Y_1$.
 - $P(Y_2|Y_1)$ √© a proje√ß√£o linear de $Y_2$ sobre $Y_1$.
 - $h_{32}$ representa a covari√¢ncia entre o res√≠duo de $Y_3$ com $Y_1$, e o res√≠duo de $Y_2$ com $Y_1$.
 - $h_{22}$ √© a vari√¢ncia do res√≠duo de $Y_2$ com $Y_1$.

O termo $[Y_2 - P(Y_2|Y_1)]$ representa a parte da informa√ß√£o em $Y_2$ que n√£o √© explicada por $Y_1$, e $h_{32}h_{22}^{-1}$ √© o peso ideal desta informa√ß√£o para melhorar a proje√ß√£o original de $Y_3$. A matriz $H$ √© a matriz de covari√¢ncia dos res√≠duos [^4.5]:
$$
H = \begin{bmatrix}
  h_{11} & 0 & 0 \\
  0 & h_{22} & h_{23} \\
  0 & h_{32} & h_{33} \\
\end{bmatrix}
$$
Podemos ver que este m√©todo de atualiza√ß√£o de proje√ß√£o √© diretamente conectado com a fatora√ß√£o triangular e com a representa√ß√£o de res√≠duos, pois a fatora√ß√£o triangular pode nos dar uma forma expl√≠cita de calcular $H$. As equa√ß√µes [4.5.15] e [4.5.16] do texto original [^4.5] formalizam essa abordagem.

> üí° **Exemplo Num√©rico:**
> Considere as tr√™s vari√°veis do exemplo anterior e suponha que a proje√ß√£o inicial de $Y_3$ com base em $Y_1$ seja:
> $$
> P(Y_3|Y_1) = \frac{\Omega_{31}}{\Omega_{11}}Y_1 = \frac{1}{4}Y_1
> $$
> Para atualizar essa proje√ß√£o com $Y_2$, precisamos de:
>  - A proje√ß√£o de $Y_2$ sobre $Y_1$:
>     $$
>     P(Y_2|Y_1) = \frac{\Omega_{21}}{\Omega_{11}}Y_1 = \frac{2}{4}Y_1 = 0.5Y_1
>     $$
>  - A matriz $H$ para o sistema:
> Usando o Teorema 1, podemos calcular $H$ como $H = (A^{-1})'DA^{-1}$:
> ```python
> A_inv = np.linalg.inv(A)
> H = A_inv.T @ D @ A_inv
> print("Matriz H:\n", H)
> ```
> O c√≥digo acima gera a seguinte sa√≠da:
> ```
> Matriz H:
> [[ 4.         0.         0.        ]
>  [ 0.         8.         2.        ]
>  [ 0.         2.        15.        ]]
> ```
>     $$
>      H = \begin{bmatrix}
>           4 & 0 & 0 \\
>           0 & 8 & 2 \\
>           0 & 2 & 15 \\
>        \end{bmatrix}
>     $$
>  - Assim, podemos calcular $h_{32}h_{22}^{-1} = \frac{2}{8} = 0.25$.
> Assim, a atualiza√ß√£o da proje√ß√£o linear √©:
> $$
> P(Y_3|Y_1,Y_2) = 0.25Y_1 + 0.25(Y_2 - 0.5Y_1) = 0.125Y_1 + 0.25Y_2
> $$
> Isso demonstra que a nova previs√£o √© uma combina√ß√£o linear de $Y_1$ e $Y_2$, onde o termo $0.25(Y_2 - 0.5Y_1)$ corrige a previs√£o inicial com base na informa√ß√£o n√£o capturada por $Y_1$ em $Y_2$. Podemos observar que os coeficientes da proje√ß√£o linear de $Y_3$ em rela√ß√£o a $Y_1$ e $Y_2$ podem ser encontrados atrav√©s de $\Omega^{-1}\Omega_{3}$ (onde $\Omega_3$ √© a terceira coluna de $\Omega$), podemos confirmar que os resultados coincidem com os coeficientes obtidos no exemplo.
>
> ```python
> Omega_inv = np.linalg.inv(Omega)
> Omega_3 = Omega[:,2]
> coef = Omega_inv @ Omega_3
> print("Coeficientes da proje√ß√£o linear:\n", coef)
> ```
> O c√≥digo acima gera a seguinte sa√≠da:
> ```
> Coeficientes da proje√ß√£o linear:
> [0.125 0.25  1.   ]
> ```
>
> Assim, obtemos $P(Y_3|Y_1,Y_2) = 0.125Y_1 + 0.25Y_2$.

**Teorema 1** A matriz $H$ das covari√¢ncias dos res√≠duos, utilizada no processo de atualiza√ß√£o, pode ser obtida diretamente a partir da fatora√ß√£o triangular da matriz de covari√¢ncia original $\Omega$. Especificamente, $H = (A^{-1})'DA^{-1}$.
*Prova:*
I. Sabemos que $\tilde{Y} = A^{-1}Y$ e que a matriz de covari√¢ncia de $\tilde{Y}$ √© $D$, que √© diagonal. A matriz de covari√¢ncia de $Y$ √© $\Omega = ADA'$.
II. Seja $H$ a matriz de covari√¢ncia dos res√≠duos. Ent√£o, como $A^{-1}Y$ representa os res√≠duos, a matriz de covari√¢ncia dos res√≠duos √© dada por:
$H = E((A^{-1}Y)(A^{-1}Y)') = E(A^{-1}YY'(A^{-1})')$.
III. Usando a propriedade $E(A X B) = A E(X) B$, temos:
$H = A^{-1}E(YY')(A^{-1})' = A^{-1}\Omega(A^{-1})'$.
IV. Substituindo $\Omega = ADA'$ na equa√ß√£o acima, temos:
$H = A^{-1}ADA'(A^{-1})'$.
V. Usando a propriedade $(AB)' = B'A'$, temos $(A^{-1})' = (A')^{-1}$.
VI. Logo, $H = A^{-1}AD(A')'(A^{-1})' = A^{-1}AD(A^{-1})' =  (A^{-1})'DA^{-1}$.
Portanto, $H = (A^{-1})'DA^{-1}$ √© a matriz de covari√¢ncia dos res√≠duos. ‚ñ†

**Corol√°rio 1.1** Se a matriz de covari√¢ncia $\Omega$ tem a fatora√ß√£o triangular $\Omega = ADA'$, ent√£o a matriz inversa $\Omega^{-1}$ pode ser expressa como $\Omega^{-1} = A^{-1}D^{-1}(A^{-1})'$.
*Prova:*
I. Dado que $\Omega = ADA'$, queremos encontrar $\Omega^{-1}$.
II. Tomamos o inverso de ambos os lados da equa√ß√£o $\Omega = ADA'$:
$\Omega^{-1} = (ADA')^{-1}$.
III. Usando a propriedade $(ABC)^{-1} = C^{-1}B^{-1}A^{-1}$, temos:
$\Omega^{-1} = (A')^{-1}D^{-1}A^{-1}$.
IV. Usando a propriedade $(A')^{-1} = (A^{-1})'$, temos:
$\Omega^{-1} = A^{-1}D^{-1}(A^{-1})'$.
Portanto, $\Omega^{-1} = A^{-1}D^{-1}(A^{-1})'$. ‚ñ†

**Lema 2** A proje√ß√£o linear de $Y_i$ sobre $Y_1, \ldots, Y_{i-1}$ pode ser expressa em termos dos elementos da matriz $A$. Seja $\alpha_{ij}$ o elemento na $i$-√©sima linha e $j$-√©sima coluna de $A^{-1}$, ent√£o $P(Y_i | Y_1, \ldots, Y_{i-1}) = - \sum_{j=1}^{i-1} \frac{\alpha_{ij}}{\alpha_{ii}} Y_j $.

*Prova:*
I. Sabemos que $\tilde{Y} = A^{-1}Y$, e podemos escrever $Y = A\tilde{Y}$.
II. Logo, $Y_i = \sum_{j=1}^{n} a_{ij} \tilde{Y}_j$.
III. Como $a_{ij} = 0$ para $j>i$ e $a_{ii} = 1$, temos $Y_i =  \tilde{Y}_i + \sum_{j=1}^{i-1} a_{ij} \tilde{Y}_j $.
IV. Tamb√©m temos que $\tilde{Y}_i = Y_i - P(Y_i|Y_1, \ldots, Y_{i-1})$, logo,  $P(Y_i|Y_1, \ldots, Y_{i-1}) =  - \sum_{j=1}^{i-1} a_{ij} \tilde{Y}_j $.
V. Usando que $\tilde{Y} = A^{-1}Y$, e que $Y = A \tilde{Y}$, temos que $A^{-1} Y = \tilde{Y}$, logo $\tilde{Y}_j = \sum_k \alpha_{jk}Y_k$.
VI. Como $\tilde{Y}_i$ √© o res√≠duo de $Y_i$ com $Y_1, \ldots, Y_{i-1}$, temos que $\tilde{Y}_i = \alpha_{ii}Y_i + \sum_{j=1}^{i-1} \alpha_{ij}Y_j $.
VII. Al√©m disso, temos que $P(Y_i|Y_1,\ldots,Y_{i-1}) = Y_i - \tilde{Y}_i = -\sum_{j=1}^{i-1}\frac{\alpha_{ij}}{\alpha_{ii}}Y_j$.
Portanto, $P(Y_i | Y_1, \ldots, Y_{i-1}) = - \sum_{j=1}^{i-1} \frac{\alpha_{ij}}{\alpha_{ii}} Y_j $. ‚ñ†

#### Conex√£o com Processos Gaussianos

Em **processos Gaussianos**, a esperan√ßa condicional, que representa a melhor previs√£o, coincide com a proje√ß√£o linear, como j√° estabelecido [^4.6]. Portanto, a abordagem de atualiza√ß√£o utilizando a fatora√ß√£o triangular fornece um m√©todo para encontrar a melhor previs√£o de $Y_3$ dado $Y_1$ e $Y_2$. Al√©m disso, as propriedades dos processos Gaussianos, como a distribui√ß√£o normal do erro de previs√£o e sua n√£o correla√ß√£o com a informa√ß√£o usada na previs√£o, s√£o crucialmente importantes.

Em particular, a decomposi√ß√£o por fatora√ß√£o triangular nos permite obter representa√ß√µes dos res√≠duos de proje√ß√µes lineares e estes res√≠duos s√£o n√£o correlacionados para qualquer processo, por√©m em um processo Gaussiano, como vimos no capitulo anterior, res√≠duos n√£o correlacionados indicam independ√™ncia, o que torna esse processo de atualiza√ß√£o ainda mais poderoso, pois podemos garantir que as novas informa√ß√µes sejam adicionadas da melhor maneira poss√≠vel. Ou seja, os res√≠duos s√£o independentes e n√£o apenas n√£o correlacionados.

> üí° **Exemplo Num√©rico:**
> Se, no exemplo num√©rico anterior, $Y_1, Y_2$, e $Y_3$ forem vari√°veis Gaussianas, a abordagem de atualiza√ß√£o por fatora√ß√£o triangular nos garante que o erro de previs√£o ap√≥s a atualiza√ß√£o ($Y_3 - P(Y_3|Y_1,Y_2)$) ser√° n√£o correlacionado com as informa√ß√µes usadas para fazer a previs√£o ($Y_1$ e $Y_2$). Al√©m disso, o erro ter√° uma distribui√ß√£o normal, o que simplifica a an√°lise da qualidade da previs√£o. Essa garantia √© poss√≠vel porque para processos Gaussianos a proje√ß√£o linear corresponde √† esperan√ßa condicional.
>
> Para ilustrar, vamos supor que temos os seguintes dados para $Y_1$, $Y_2$ e $Y_3$:
> ```python
> np.random.seed(42)
> Y1 = np.random.normal(0, 2, 100)
> Y2 = 0.5 * Y1 + np.random.normal(0, 2.82, 100)
> Y3 = 0.125 * Y1 + 0.25 * Y2 + np.random.normal(0, 3.86, 100)
> Y = np.stack([Y1, Y2, Y3], axis=1)
> ```
> Usando os dados, podemos calcular o erro da previs√£o inicial e da previs√£o atualizada:
> ```python
> P_Y3_Y1 = 0.25 * Y1
> P_Y2_Y1 = 0.5 * Y1
> P_Y3_Y1_Y2 = 0.125 * Y1 + 0.25 * Y2
>
> erro_inicial = Y3 - P_Y3_Y1
> erro_atualizado = Y3 - P_Y3_Y1_Y2
>
> print("Correla√ß√£o entre Y1 e o erro inicial:", np.corrcoef(Y1, erro_inicial)[0,1])
> print("Correla√ß√£o entre Y1 e o erro atualizado:", np.corrcoef(Y1, erro_atualizado)[0,1])
> print("Correla√ß√£o entre Y2 e o erro atualizado:", np.corrcoef(Y2, erro_atualizado)[0,1])
> ```
> O c√≥digo acima gera a seguinte sa√≠da:
> ```
> Correla√ß√£o entre Y1 e o erro inicial: -0.032723743337932365
> Correla√ß√£o entre Y1 e o erro atualizado: -0.02063017591090025
> Correla√ß√£o entre Y2 e o erro atualizado: -0.013175460512317284
> ```
> Note que a correla√ß√£o entre $Y_1$ e o erro inicial √© baixa, mas n√£o exatamente zero, o que se deve √† natureza amostral do exemplo, que n√£o representa as caracter√≠sticas populacionais. No entanto, a correla√ß√£o entre o erro atualizado e as vari√°veis usadas na previs√£o √© ainda menor. Isso demonstra, na pr√°tica, que a atualiza√ß√£o da proje√ß√£o linear leva a um erro que √© menos correlacionado (em amostras Gaussianas, podemos dizer, independente) com a informa√ß√£o utilizada para realizar a previs√£o.
> Al√©m disso, podemos visualizar que os erros seguem uma distribui√ß√£o normal:
> ```python
> import matplotlib.pyplot as plt
>
> plt.hist(erro_inicial, bins=20, alpha=0.5, label='Erro Inicial')
> plt.hist(erro_atualizado, bins=20, alpha=0.5, label='Erro Atualizado')
> plt.legend()
> plt.title('Histograma dos Erros de Previs√£o')
> plt.show()
> ```
> O histograma gerado pelo c√≥digo acima nos mostra que os erros se aproximam de uma distribui√ß√£o normal.

**Proposi√ß√£o 1** Em um processo Gaussiano, o erro de previs√£o obtido atrav√©s do processo de atualiza√ß√£o por fatora√ß√£o triangular √© independente das vari√°veis utilizadas na previs√£o.
*Prova:*
I. Em um processo Gaussiano, sabemos que a esperan√ßa condicional coincide com a proje√ß√£o linear, ou seja, $E[Y_i|Y_1,\ldots,Y_{i-1}] = P(Y_i|Y_1,\ldots,Y_{i-1})$.
II. O erro de previs√£o √© dado por $e_i = Y_i - P(Y_i|Y_1,\ldots,Y_{i-1}) = Y_i - E[Y_i|Y_1,\ldots,Y_{i-1}]$.
III. A fatora√ß√£o triangular garante que os res√≠duos $\tilde{Y_i}$ s√£o n√£o correlacionados, e como estamos em um processo Gaussiano, a n√£o correla√ß√£o implica independ√™ncia.
IV. O processo de atualiza√ß√£o √© baseado em res√≠duos e, portanto, o erro obtido no final ser√° um res√≠duo (ou fun√ß√£o linear de res√≠duos) e, assim, independente da informa√ß√£o usada na previs√£o.
Portanto, em um processo Gaussiano, o erro de previs√£o √© independente das vari√°veis utilizadas na previs√£o. ‚ñ†

### Conclus√£o
A abordagem de atualiza√ß√£o de proje√ß√µes por **fatora√ß√£o triangular** permite incorporar novas informa√ß√µes √† previs√£o por meio da adi√ß√£o de um componente n√£o antecipado, ponderado pela covari√¢ncia com o erro de previs√£o inicial [^4.5]. Essa abordagem fornece uma ponte entre a √°lgebra linear e a teoria da probabilidade, sendo especialmente √∫til no contexto de **processos Gaussianos**, onde a proje√ß√£o linear se torna uma ferramenta de otimiza√ß√£o. A combina√ß√£o das propriedades das distribui√ß√µes gaussianas com as vantagens computacionais das proje√ß√µes lineares, evidencia a relev√¢ncia de ferramentas matem√°ticas para a estat√≠stica e a teoria de s√©ries temporais [^4.4, ^4.5, ^4.6]. Em outras palavras, para um processo Gaussiano, a fatora√ß√£o triangular nos d√° um m√©todo para obter a melhor proje√ß√£o linear e, portanto, a melhor previs√£o, e o resultado √© uma nova previs√£o que usa toda a informa√ß√£o dispon√≠vel na forma linear, sendo equivalente √† esperan√ßa condicional.

### Refer√™ncias
[^4.4]: Se√ß√£o 4.4 do texto, incluindo as equa√ß√µes [4.4.1] at√© [4.4.13].
[^4.5]: Se√ß√£o 4.5 do texto, incluindo as equa√ß√µes [4.5.1] at√© [4.5.16].
[^4.6]: Se√ß√£o 4.6 do texto, incluindo as equa√ß√µes [4.6.1] at√© [4.6.7].
<!-- END -->
