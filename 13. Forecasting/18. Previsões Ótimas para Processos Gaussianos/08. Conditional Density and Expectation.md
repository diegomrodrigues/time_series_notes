## A Densidade Condicional e a EsperanÃ§a Condicional em Processos Gaussianos

### IntroduÃ§Ã£o
Em continuidade Ã  discussÃ£o sobre **previsÃµes Ã³timas** em **processos Gaussianos**, e como a **projeÃ§Ã£o linear** e a **esperanÃ§a condicional** coincidem, este capÃ­tulo detalha o processo de derivaÃ§Ã£o da **densidade condicional** de $Y_2$ dado $Y_1$. Vamos explorar como a divisÃ£o da **densidade conjunta** pela **densidade marginal** resulta na **densidade condicional**, e como a partir desta, podemos obter a **esperanÃ§a condicional** $E(Y_2|Y_1)$ que coincide com a projeÃ§Ã£o linear, demonstrando que ela representa a previsÃ£o Ã³tima para tais processos [^4.6].

### DerivaÃ§Ã£o da Densidade Condicional
Partindo da **densidade conjunta** de $Y_1$ e $Y_2$ (vetores aleatÃ³rios gaussianos), que expressa a probabilidade de ocorrÃªncia simultÃ¢nea de valores desses vetores, podemos derivar a **densidade condicional** de $Y_2$ dado $Y_1$ como [^4.6]:
$$
f_{Y_2|Y_1}(y_2|y_1) = \frac{f_{Y_1,Y_2}(y_1,y_2)}{f_{Y_1}(y_1)}
$$
onde $f_{Y_1,Y_2}(y_1,y_2)$ Ã© a densidade conjunta de $Y_1$ e $Y_2$, e $f_{Y_1}(y_1)$ Ã© a densidade marginal de $Y_1$.

A **densidade conjunta** para variÃ¡veis Gaussianas Ã© dada por [^4.6]:
$$
f_{Y_1,Y_2}(y_1,y_2) = \frac{1}{(2\pi)^{\frac{n_1+n_2}{2}} |\Omega|^{1/2}} \exp \left\{ -\frac{1}{2} \begin{bmatrix} y_1 - \mu_1 \\ y_2 - \mu_2 \end{bmatrix}' \Omega^{-1} \begin{bmatrix} y_1 - \mu_1 \\ y_2 - \mu_2 \end{bmatrix} \right\}
$$
onde $\Omega$ Ã© a matriz de covariÃ¢ncia conjunta:
$$
\Omega = \begin{bmatrix}
  \Omega_{11} & \Omega_{12} \\
  \Omega_{21} & \Omega_{22} \\
\end{bmatrix}
$$
A **densidade marginal** de $Y_1$, por sua vez, Ã© dada por:
$$
f_{Y_1}(y_1) = \frac{1}{(2\pi)^{\frac{n_1}{2}} |\Omega_{11}|^{1/2}} \exp \left\{ -\frac{1}{2} (y_1 - \mu_1)' \Omega_{11}^{-1} (y_1 - \mu_1) \right\}
$$
Ao realizar a divisÃ£o, a densidade condicional Ã© obtida como [^4.6]:
$$
f_{Y_2|Y_1}(y_2|y_1) = \frac{1}{(2\pi)^{n_2/2} |H|^{1/2}} \exp \left\{ -\frac{1}{2} (y_2 - m)'H^{-1}(y_2 - m) \right\}
$$
onde:
 - $m = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(y_1-\mu_1)$ Ã© a mÃ©dia condicional de $Y_2$ dado $Y_1$
 - $H = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}$ Ã© a matriz de covariÃ¢ncia condicional de $Y_2$ dado $Y_1$

Essa forma da densidade condicional revela que, para processos Gaussianos, a distribuiÃ§Ã£o de $Y_2$ dado $Y_1$ tambÃ©m Ã© normal, e que a sua mÃ©dia Ã© uma funÃ§Ã£o linear de $Y_1$.

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Para ilustrar a derivaÃ§Ã£o da densidade condicional, considere dois vetores Gaussianos, $Y_1$ e $Y_2$, representando o retorno de um ativo em dois perÃ­odos diferentes. Suponha que as mÃ©dias sÃ£o $\mu_1 = 0.05$ e $\mu_2 = 0.10$ e a matriz de covariÃ¢ncia Ã©:
> $$
> \Omega = \begin{bmatrix}
>   0.0025 & 0.001 \\
>   0.001 & 0.0049 \\
> \end{bmatrix}
> $$
> A densidade conjunta Ã© dada por [^4.6]:
> $$
> f_{Y_1,Y_2}(y_1,y_2) = \frac{1}{2\pi \sqrt{|\Omega|}} \exp \left\{ -\frac{1}{2} \begin{bmatrix} y_1 - 0.05 \\ y_2 - 0.10 \end{bmatrix}' \Omega^{-1} \begin{bmatrix} y_1 - 0.05 \\ y_2 - 0.10 \end{bmatrix} \right\}
> $$
> onde $|\Omega| = 0.00001125$ (calculado no capÃ­tulo anterior).
> A densidade marginal de $Y_1$ Ã©:
> $$
> f_{Y_1}(y_1) = \frac{1}{\sqrt{2\pi \Omega_{11}}} \exp\left\{-\frac{(y_1 - \mu_1)^2}{2 \Omega_{11}}\right\} = \frac{1}{\sqrt{2\pi (0.0025)}} \exp\left\{-\frac{(y_1 - 0.05)^2}{2 (0.0025)}\right\}
> $$
> Para obter a densidade condicional de $Y_2$ dado $Y_1$, dividimos a densidade conjunta pela marginal:
> $$
> f_{Y_2|Y_1}(y_2|y_1) = \frac{f_{Y_1,Y_2}(y_1,y_2)}{f_{Y_1}(y_1)}
> $$
> ApÃ³s a simplificaÃ§Ã£o, obtemos a densidade condicional:
> $$
> f_{Y_2|Y_1}(y_2|y_1) = \frac{1}{\sqrt{2\pi H}} \exp \left\{ -\frac{1}{2} \frac{(y_2 - m)^2}{H} \right\}
> $$
> onde
> $$
> m = 0.10 + \frac{0.001}{0.0025}(y_1 - 0.05) = 0.10 + 0.4(y_1 - 0.05)
> $$
> e
> $$
> H = 0.0049 - \frac{0.001^2}{0.0025} = 0.0045
> $$
> Esta densidade condicional descreve a distribuiÃ§Ã£o de $Y_2$ dado um valor observado $y_1$ de $Y_1$, mostrando como a distribuiÃ§Ã£o de $Y_2$ muda com base na informaÃ§Ã£o disponÃ­vel de $Y_1$.
>
> Considere o caso especÃ­fico em que $y_1 = 0.06$. A mÃ©dia condicional de $Y_2$ passa a ser $m = 0.10 + 0.4(0.06 - 0.05) = 0.104$, e a variÃ¢ncia condicional Ã© $H=0.0045$. A densidade condicional especÃ­fica Ã© entÃ£o
> $$
> f_{Y_2|Y_1=0.06}(y_2|0.06) = \frac{1}{\sqrt{2\pi 0.0045}} \exp \left\{ -\frac{1}{2} \frac{(y_2 - 0.104)^2}{0.0045} \right\}
> $$
> Esta densidade Ã© uma normal com mÃ©dia 0.104 e variÃ¢ncia 0.0045.

**Lema 1**
A densidade condicional $f_{Y_2|Y_1}(y_2|y_1)$ obtida pela divisÃ£o da densidade conjunta $f_{Y_1, Y_2}(y_1, y_2)$ pela densidade marginal $f_{Y_1}(y_1)$ Ã© uma densidade normal com mÃ©dia $m = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(y_1-\mu_1)$ e variÃ¢ncia $H = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}$.
*Proof:*
I. A densidade conjunta para processos Gaussianos tem a forma:
$$
f_{Y_1,Y_2}(y_1,y_2) = C \exp \left\{ -\frac{1}{2} \begin{bmatrix} y_1 - \mu_1 \\ y_2 - \mu_2 \end{bmatrix}' \Omega^{-1} \begin{bmatrix} y_1 - \mu_1 \\ y_2 - \mu_2 \end{bmatrix} \right\}
$$
II. A densidade marginal para processos Gaussianos tem a forma:
$$
f_{Y_1}(y_1) = C_1 \exp \left\{ -\frac{1}{2} (y_1 - \mu_1)' \Omega_{11}^{-1} (y_1 - \mu_1) \right\}
$$
III. A densidade condicional Ã© a divisÃ£o da conjunta pela marginal:
$$
f_{Y_2|Y_1}(y_2|y_1) = \frac{f_{Y_1,Y_2}(y_1,y_2)}{f_{Y_1}(y_1)}
$$
IV. Ao realizar a divisÃ£o, os termos exponenciais se combinam, e, apÃ³s manipulaÃ§Ãµes algÃ©bricas, resulta em uma densidade da forma:
$$
f_{Y_2|Y_1}(y_2|y_1) = C_2 \exp \left\{ -\frac{1}{2} (y_2 - m)'H^{-1}(y_2 - m) \right\}
$$
onde $m = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(y_1-\mu_1)$ e $H = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}$. A expressÃ£o acima Ã© a forma de uma distribuiÃ§Ã£o normal com mÃ©dia $m$ e variÃ¢ncia $H$. $\blacksquare$

**Lema 1.1** (Propriedades da Matriz de CovariÃ¢ncia Condicional)
A matriz de covariÃ¢ncia condicional $H = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}$ Ã© sempre semidefinida positiva, garantindo que a densidade condicional seja uma funÃ§Ã£o de densidade vÃ¡lida.
*Proof:*
I. Dado que $\Omega$ Ã© uma matriz de covariÃ¢ncia, ela Ã© simÃ©trica e positiva semidefinida. Portanto, podemos decompor $\Omega$ usando a decomposiÃ§Ã£o de Schur, como
$$ \Omega = \begin{bmatrix} \Omega_{11} & \Omega_{12} \\ \Omega_{21} & \Omega_{22} \end{bmatrix} = \begin{bmatrix} I & \Omega_{12}\Omega_{22}^{-1} \\ 0 & I \end{bmatrix} \begin{bmatrix} \Omega_{11} - \Omega_{12} \Omega_{22}^{-1} \Omega_{21} & 0 \\ 0 & \Omega_{22} \end{bmatrix} \begin{bmatrix} I & 0 \\ \Omega_{22}^{-1}\Omega_{21} & I \end{bmatrix} $$
II.  A partir da decomposiÃ§Ã£o, vemos que $\Omega_{22}$ Ã© tambÃ©m positiva semidefinida.  AlÃ©m disso, a matriz $\Omega_{11} - \Omega_{12} \Omega_{22}^{-1} \Omega_{21}$ Ã© a matriz de covariÃ¢ncia marginal da variÃ¡vel $Y_1|Y_2$.
III. Pela propriedade de matrizes de covariÃ¢ncia, a matriz $ \Omega_{11} - \Omega_{12} \Omega_{22}^{-1} \Omega_{21}$ Ã© positiva semidefinida. Analogamente, temos que $\Omega_{22} - \Omega_{21} \Omega_{11}^{-1} \Omega_{12}$ tambÃ©m Ã© positiva semidefinida, que Ã© a mesma matriz $H$ do enunciado. $\blacksquare$

### A EsperanÃ§a Condicional e sua DerivaÃ§Ã£o
Uma vez que a densidade condicional Ã© obtida, a **esperanÃ§a condicional** de $Y_2$ dado $Y_1$, denotada por $E(Y_2|Y_1)$, pode ser obtida diretamente da mÃ©dia da distribuiÃ§Ã£o condicional [^4.6].
$$
E(Y_2|Y_1) = \int y_2 f_{Y_2|Y_1}(y_2|Y_1) \, dy_2 = m = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
Essa expressÃ£o demonstra que a esperanÃ§a condicional Ã© uma funÃ§Ã£o linear de $Y_1$, o que, como jÃ¡ vimos em capÃ­tulos anteriores, Ã© uma caracterÃ­stica fundamental para processos Gaussianos. Essa mÃ©dia condicional Ã©, por definiÃ§Ã£o, a previsÃ£o Ã³tima de $Y_2$ dado o conhecimento de $Y_1$.

> ğŸ’¡ **Exemplo NumÃ©rico:**
> No exemplo anterior, a esperanÃ§a condicional Ã© obtida como:
> $$
> E(Y_2|Y_1) = 0.10 + 0.4(Y_1 - 0.05)
> $$
> Esta funÃ§Ã£o linear de $Y_1$ representa a melhor previsÃ£o de $Y_2$, dada a informaÃ§Ã£o de $Y_1$. Se observarmos $Y_1 = 0.06$, a mÃ©dia condicional de $Y_2$ serÃ¡:
> $$
> E[Y_2|Y_1 = 0.06] = 0.10 + 0.4(0.06 - 0.05) = 0.10 + 0.004 = 0.104
> $$
> Este valor representa a melhor previsÃ£o do retorno do ativo no segundo perÃ­odo, dado que o retorno no primeiro perÃ­odo foi 0.06.
>
> Para simular, vamos criar amostras aleatÃ³rias de $Y_2$, dado que $Y_1$ Ã© 0.06 e calcular a mÃ©dia dessas amostras:
> ```python
> import numpy as np
>
> # ParÃ¢metros
> mu_1 = 0.05
> mu_2 = 0.10
> omega_11 = 0.0025
> omega_12 = 0.001
> omega_21 = 0.001
> omega_22 = 0.0049
>
> # Valor de Y1
> y1_value = 0.06
>
> # CÃ¡lculo da mÃ©dia e variÃ¢ncia condicionais
> m = mu_2 + (omega_12 / omega_11) * (y1_value - mu_1)
> H = omega_22 - (omega_12**2 / omega_11)
>
> # Gerando amostras condicionais
> n_samples = 1000
> samples_conditional = np.random.normal(m, np.sqrt(H), size=n_samples)
>
> # Calculando a mÃ©dia amostral
> sample_mean = np.mean(samples_conditional)
>
> print(f"EsperanÃ§a condicional teÃ³rica E(Y2|Y1): {m:.6f}")
> print(f"MÃ©dia amostral da distribuiÃ§Ã£o condicional de Y2 dado Y1: {sample_mean:.6f}")
> ```
> A saÃ­da do programa mostrarÃ¡ que a mÃ©dia amostral Ã© muito prÃ³xima da esperanÃ§a condicional, mostrando que a expectativa condicional Ã© uma mÃ©dia populacional, que Ã© bem aproximada pela mÃ©dia amostral.
>
> Vamos agora visualizar a distribuiÃ§Ã£o de amostras condicionais e a mÃ©dia condicional:
> ```python
> import matplotlib.pyplot as plt
> import seaborn as sns
>
> # Plot da distribuiÃ§Ã£o condicional
> sns.histplot(samples_conditional, kde=True, color='skyblue', label='DistribuiÃ§Ã£o Condicional de Y2|Y1=0.06')
> plt.axvline(m, color='red', linestyle='dashed', linewidth=1, label=f'MÃ©dia Condicional: {m:.4f}')
> plt.title('DistribuiÃ§Ã£o Condicional de Y2 dado Y1=0.06')
> plt.xlabel('Valor de Y2')
> plt.ylabel('FrequÃªncia')
> plt.legend()
> plt.show()
> ```
> Este grÃ¡fico mostrarÃ¡ um histograma dos valores simulados de $Y_2$ quando $Y_1=0.06$, juntamente com uma linha vertical vermelha indicando a mÃ©dia condicional, ilustrando visualmente que a mÃ©dia amostral se aproxima do valor esperado.

**Teorema 1** (EsperanÃ§a Condicional em Processos Gaussianos)
A esperanÃ§a condicional $E(Y_2|Y_1)$, obtida a partir da densidade condicional, Ã© dada por $E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)$, que Ã© uma funÃ§Ã£o linear de $Y_1$.
*Proof:*
I. Do Lema 1, a densidade condicional Ã© dada por
$$
f_{Y_2|Y_1}(y_2|y_1) = C \exp \left\{ -\frac{1}{2} (y_2 - m)'H^{-1}(y_2 - m) \right\}
$$
com $m = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(y_1-\mu_1)$ e $H = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}$.
II. A esperanÃ§a condicional $E(Y_2|Y_1)$ Ã© igual a mÃ©dia da distribuiÃ§Ã£o condicional, ou seja:
$$E(Y_2|Y_1) = m$$
III. Substituindo a expressÃ£o de $m$, obtemos:
$$E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)$$
IV. Logo, a esperanÃ§a condicional $E(Y_2|Y_1)$ Ã© uma funÃ§Ã£o linear de $Y_1$. $\blacksquare$

### EquivalÃªncia com a ProjeÃ§Ã£o Linear
Como visto anteriormente, a **projeÃ§Ã£o linear** de $Y_2$ sobre $Y_1$, que consiste na melhor previsÃ£o linear de $Y_2$ baseada em $Y_1$,  Ã© dada por [^4.5]:
$$
\hat{E}(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
Comparando esta expressÃ£o com a esperanÃ§a condicional, podemos verificar que elas sÃ£o idÃªnticas. Este resultado Ã© fundamental, pois ele nos diz que, em processos Gaussianos, a projeÃ§Ã£o linear, que Ã© computacionalmente mais simples, fornece a mesma previsÃ£o que a esperanÃ§a condicional, que Ã© a previsÃ£o Ã³tima irrestrita [^4.6].

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Usando o exemplo anterior, a projeÃ§Ã£o linear de $Y_2$ sobre $Y_1$ Ã© dada por:
> $$
> \hat{E}(Y_2|Y_1) = 0.10 + 0.4(Y_1 - 0.05)
> $$
> Esta Ã© exatamente a mesma funÃ§Ã£o linear que encontramos para a esperanÃ§a condicional. Isso demonstra, de forma prÃ¡tica, a equivalÃªncia entre a projeÃ§Ã£o linear e a esperanÃ§a condicional.
>
> Se considerarmos novamente $Y_1 = 0.06$, a projeÃ§Ã£o linear serÃ¡:
> $$
> \hat{E}(Y_2|Y_1=0.06) = 0.10 + 0.4(0.06 - 0.05) = 0.104
> $$
> O mesmo valor obtido para a esperanÃ§a condicional, reforÃ§ando a equivalÃªncia.

**Teorema 2** (EquivalÃªncia em Processos Gaussianos)
Para processos Gaussianos, a esperanÃ§a condicional $E(Y_2|Y_1)$ e a projeÃ§Ã£o linear $\hat{E}(Y_2|Y_1)$ sÃ£o idÃªnticas, e ambas sÃ£o a melhor previsÃ£o de $Y_2$ dado $Y_1$.
*Proof:*
I. Do Teorema 1 e da deduÃ§Ã£o anterior, temos:
$$
E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
II. A projeÃ§Ã£o linear Ã© dada por:
$$
\hat{E}(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
III. Comparando as duas expressÃµes, concluÃ­mos que:
$$
E(Y_2|Y_1) = \hat{E}(Y_2|Y_1)
$$
Portanto, em processos Gaussianos, a esperanÃ§a condicional e a projeÃ§Ã£o linear coincidem, e como a esperanÃ§a condicional Ã© a melhor previsÃ£o, a projeÃ§Ã£o linear tambÃ©m Ã© a melhor previsÃ£o. $\blacksquare$

**Teorema 2.1** (Propriedade da Melhor PrevisÃ£o)
Em processos Gaussianos, a projeÃ§Ã£o linear $\hat{E}(Y_2|Y_1)$, que coincide com a esperanÃ§a condicional $E(Y_2|Y_1)$, Ã© a melhor previsÃ£o no sentido de minimizar o erro quadrÃ¡tico mÃ©dio, ou seja, $\hat{E}(Y_2|Y_1) = \arg \min_{g(Y_1)} E[(Y_2-g(Y_1))^2]$, em que $g$ Ã© qualquer funÃ§Ã£o de $Y_1$.
*Proof:*
I.  Pela definiÃ§Ã£o da esperanÃ§a condicional, temos que $E(Y_2|Y_1)$ Ã© a melhor previsÃ£o de $Y_2$ dado $Y_1$ no sentido de minimizar o erro quadrÃ¡tico mÃ©dio $E[(Y_2 - g(Y_1))^2]$, em que $g$ Ã© qualquer funÃ§Ã£o de $Y_1$.
II. Pelo Teorema 2, sabemos que $\hat{E}(Y_2|Y_1) = E(Y_2|Y_1)$ para processos gaussianos.
III. Portanto, $\hat{E}(Y_2|Y_1)$ tambÃ©m minimiza o erro quadrÃ¡tico mÃ©dio $E[(Y_2 - g(Y_1))^2]$, sendo, portanto, a melhor previsÃ£o no sentido do erro quadrÃ¡tico mÃ©dio. $\blacksquare$

### ConclusÃ£o
A anÃ¡lise detalhada da **densidade conjunta** e sua utilizaÃ§Ã£o para derivar a **densidade condicional** demonstrou como obter a **esperanÃ§a condicional** $E(Y_2|Y_1)$ para **processos Gaussianos** [^4.6]. Este processo revela a fundamental caracterÃ­stica de que a esperanÃ§a condicional Ã© uma funÃ§Ã£o linear das variÃ¡veis condicionantes, e que ela coincide com a **projeÃ§Ã£o linear**.  A derivaÃ§Ã£o detalhada, e os exemplos numÃ©ricos, mostram que essa relaÃ§Ã£o Ã© central na teoria da previsÃ£o, pois permite que as projeÃ§Ãµes lineares sejam usadas como mÃ©todo computacionalmente simples para obter a melhor previsÃ£o em processos Gaussianos, combinando conceitos avanÃ§ados de probabilidade, estatÃ­stica e Ã¡lgebra linear.

### ReferÃªncias
[^4.6]: SeÃ§Ã£o 4.6 do texto, incluindo as equaÃ§Ãµes [4.6.1] atÃ© [4.6.7].
[^4.5]: SeÃ§Ã£o 4.5 do texto, incluindo as equaÃ§Ãµes [4.5.1] atÃ© [4.5.16].
<!-- END -->
