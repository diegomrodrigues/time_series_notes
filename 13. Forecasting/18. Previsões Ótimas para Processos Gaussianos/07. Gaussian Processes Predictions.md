## Previs√µes √ìtimas Gaussianas: A Natureza Linear da Expectativa Condicional

### Introdu√ß√£o
Este cap√≠tulo visa consolidar o entendimento sobre a otimalidade das previs√µes em **processos Gaussianos**, focando na caracter√≠stica linear da **expectativa condicional**, e como ela se manifesta de forma expl√≠cita por meio de uma combina√ß√£o linear das vari√°veis preditoras. Anteriormente, estabelecemos que a **proje√ß√£o linear** e a **expectativa condicional** coincidem para processos Gaussianos [^4.6]. Agora, vamos explorar como essa equival√™ncia se traduz em uma estrutura linear expl√≠cita da expectativa condicional, refor√ßando a ideia de que, neste contexto, as previs√µes √≥timas s√£o obtidas por meio de combina√ß√µes lineares, o que destaca a import√¢ncia da √°lgebra linear e da teoria de probabilidade neste campo.

### Expectativa Condicional como Combina√ß√£o Linear

Em processos Gaussianos, a **expectativa condicional** de uma vari√°vel $Y_2$ dado um vetor de vari√°veis $Y_1$, denotada por $E(Y_2|Y_1)$, √© expressa explicitamente como uma combina√ß√£o linear de $Y_1$. Partindo da densidade condicional derivada no cap√≠tulo anterior, obtemos:
$$
E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
onde:
- $E(Y_2|Y_1)$ representa a expectativa condicional de $Y_2$ dado $Y_1$, ou seja, a melhor previs√£o irrestrita de $Y_2$ dado $Y_1$.
- $\mu_2$ √© o vetor de m√©dias marginais de $Y_2$.
- $\mu_1$ √© o vetor de m√©dias marginais de $Y_1$.
- $\Omega_{21}$ √© a matriz de covari√¢ncia entre $Y_2$ e $Y_1$.
- $\Omega_{11}$ √© a matriz de covari√¢ncia de $Y_1$.

Essa express√£o revela que a expectativa condicional √© uma fun√ß√£o linear de $Y_1$, caracterizada por um termo constante $\mu_2$ e uma combina√ß√£o linear dos desvios de $Y_1$ em rela√ß√£o √† sua m√©dia, onde os pesos dessa combina√ß√£o s√£o dados por $\Omega_{21}\Omega_{11}^{-1}$.

> üí° **Exemplo Num√©rico:**
> Para ilustrar, suponha que $Y_1$ seja um vetor com duas vari√°veis representando o pre√ßo de um ativo em dois momentos diferentes, com m√©dias $\mu_1 = \begin{bmatrix} 10 \\ 12 \end{bmatrix}$, e $Y_2$ seja uma vari√°vel representando o pre√ßo de um outro ativo, com m√©dia $\mu_2 = 15$. A matriz de covari√¢ncia conjunta √©:
> $$
> \Omega = \begin{bmatrix}
>   \begin{matrix} 4 & 2 \\ 2 & 9 \end{matrix} &  \begin{matrix} 1 \\ 3 \end{matrix}  \\
>   \begin{matrix} 1 & 3 \end{matrix} & 16
> \end{bmatrix}
> $$
> Temos ent√£o $\Omega_{11} = \begin{bmatrix} 4 & 2 \\ 2 & 9 \end{bmatrix}$, $\Omega_{21} = \begin{bmatrix} 1 & 3 \end{bmatrix}$, e $\mu_1 = \begin{bmatrix} 10 \\ 12 \end{bmatrix}$, $\mu_2 = 15$.
>
> A expectativa condicional $E(Y_2|Y_1)$ √© calculada como:
> $$
> E(Y_2|Y_1) = 15 + \begin{bmatrix} 1 & 3 \end{bmatrix} \begin{bmatrix} 4 & 2 \\ 2 & 9 \end{bmatrix}^{-1} \left( \begin{bmatrix} Y_{11} \\ Y_{12} \end{bmatrix} - \begin{bmatrix} 10 \\ 12 \end{bmatrix} \right)
> $$
> Para calcular $\Omega_{11}^{-1}$, podemos usar uma biblioteca em Python:
> ```python
> import numpy as np
>
> omega_11 = np.array([[4, 2],
>                    [2, 9]])
>
> omega_11_inv = np.linalg.inv(omega_11)
>
> print(omega_11_inv)
> ```
> O c√≥digo acima gera a seguinte sa√≠da:
> ```
> [[ 0.28125  -0.0625 ]
>  [-0.0625   0.125  ]]
> ```
> Logo, a expectativa condicional √©
> $$
> E(Y_2|Y_1) = 15 + \begin{bmatrix} 1 & 3 \end{bmatrix} \begin{bmatrix} 0.28125 & -0.0625 \\ -0.0625 & 0.125 \end{bmatrix} \left( \begin{bmatrix} Y_{11} \\ Y_{12} \end{bmatrix} - \begin{bmatrix} 10 \\ 12 \end{bmatrix} \right)
> $$
> $$
> E(Y_2|Y_1) = 15 +  \begin{bmatrix} 0.1 & 0.3125 \end{bmatrix}  \left( \begin{bmatrix} Y_{11} - 10 \\ Y_{12} - 12 \end{bmatrix} \right)
> $$
> $$
> E(Y_2|Y_1) = 15 + 0.1 (Y_{11} - 10) + 0.3125 (Y_{12} - 12)
> $$
> Podemos observar que $E(Y_2|Y_1)$ √© uma combina√ß√£o linear de $Y_{11}$ e $Y_{12}$, e os coeficientes s√£o calculados a partir das covari√¢ncias da distribui√ß√£o conjunta.
>
> Para um valor espec√≠fico de $Y_1$, digamos $Y_1 = \begin{bmatrix} 11 \\ 13 \end{bmatrix}$, temos:
> $$
> E(Y_2|Y_1 = \begin{bmatrix} 11 \\ 13 \end{bmatrix}) = 15 + 0.1 (11 - 10) + 0.3125 (13 - 12) = 15 + 0.1 + 0.3125 = 15.4125
> $$
> Isto √© a melhor previs√£o do pre√ßo do segundo ativo, dados os pre√ßos dos ativos 1 e 2.

**Teorema 1** (Expectativa Condicional como Fun√ß√£o Linear)
Em processos Gaussianos, a expectativa condicional $E(Y_2|Y_1)$ √© uma fun√ß√£o linear de $Y_1$, e √© dada por $E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)$.
*Proof:*
I. A distribui√ß√£o condicional de um vetor Gaussiano $Y_2$ dado um vetor Gaussiano $Y_1$ √© tamb√©m Gaussiana.
II. A m√©dia da distribui√ß√£o condicional (ou seja, a expectativa condicional) √©:
$$
E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
III. Como $\mu_2$, $\Omega_{21}$, $\Omega_{11}$ e $\mu_1$ s√£o par√¢metros da distribui√ß√£o conjunta e, portanto, constantes, a express√£o acima √© uma fun√ß√£o linear de $Y_1$.
Portanto, a expectativa condicional $E(Y_2|Y_1)$ √© uma fun√ß√£o linear de $Y_1$. $\blacksquare$

**Lema 1.1** (Propriedade da Invers√£o de Matrizes de Covari√¢ncia)
Se $\Omega_{11}$ √© a matriz de covari√¢ncia de $Y_1$ e √© invert√≠vel, ent√£o sua inversa $\Omega_{11}^{-1}$ tamb√©m √© sim√©trica e positiva definida.
*Proof:*
I. Uma matriz de covari√¢ncia $\Omega_{11}$ √© por defini√ß√£o sim√©trica e positiva semi-definida.
II. Se $\Omega_{11}$ √© invert√≠vel, ent√£o ela √© tamb√©m positiva definida.
III. A inversa de uma matriz sim√©trica √© tamb√©m sim√©trica.
IV. A inversa de uma matriz positiva definida √© tamb√©m positiva definida.
Portanto, $\Omega_{11}^{-1}$ √© sim√©trica e positiva definida. $\blacksquare$

### Equival√™ncia com a Proje√ß√£o Linear
Como vimos anteriormente, a proje√ß√£o linear de $Y_2$ sobre $Y_1$ √© definida como a melhor previs√£o de $Y_2$ que √© uma fun√ß√£o linear de $Y_1$, e √© dada por [^4.5]:
$$
\hat{E}(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
Comparando esta express√£o com a express√£o da expectativa condicional, notamos que elas s√£o id√™nticas. Este resultado formaliza a equival√™ncia entre a expectativa condicional e a proje√ß√£o linear em processos Gaussianos. Isso significa que, para estes processos, a previs√£o √≥tima irrestrita √© tamb√©m linear, o que simplifica enormemente a an√°lise e a implementa√ß√£o das previs√µes.

> üí° **Exemplo Num√©rico:**
> Usando os mesmos dados do exemplo anterior, a proje√ß√£o linear de $Y_2$ em $Y_1$ √© dada por:
> $$
> \hat{E}(Y_2|Y_1) = 15 + \begin{bmatrix} 1 & 3 \end{bmatrix} \begin{bmatrix} 0.28125 & -0.0625 \\ -0.0625 & 0.125 \end{bmatrix} \left( \begin{bmatrix} Y_{11} \\ Y_{12} \end{bmatrix} - \begin{bmatrix} 10 \\ 12 \end{bmatrix} \right)
> $$
> $$
> \hat{E}(Y_2|Y_1) = 15 + 0.1 (Y_{11} - 10) + 0.3125 (Y_{12} - 12)
> $$
> Observe que esta express√£o coincide exatamente com a express√£o da expectativa condicional calculada anteriormente, comprovando numericamente que as duas express√µes s√£o equivalentes para este processo Gaussiano.

**Teorema 2** (Equival√™ncia da Expectativa Condicional e Proje√ß√£o Linear)
Em processos Gaussianos, a expectativa condicional $E(Y_2|Y_1)$ coincide com a proje√ß√£o linear $\hat{E}(Y_2|Y_1)$, ou seja, $E(Y_2|Y_1) = \hat{E}(Y_2|Y_1)$.
*Proof:*
I. Do Teorema 1 e da dedu√ß√£o anterior, temos:
$$
E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
II. A proje√ß√£o linear tamb√©m √© dada por:
$$
\hat{E}(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
III. Comparando as duas express√µes, conclu√≠mos que
$$
E(Y_2|Y_1) = \hat{E}(Y_2|Y_1)
$$
Portanto, para processos Gaussianos, a expectativa condicional e a proje√ß√£o linear s√£o id√™nticas.  $\blacksquare$

**Corol√°rio 2.1** (Otimidade da Proje√ß√£o Linear em Processos Gaussianos)
Em processos Gaussianos, a proje√ß√£o linear $\hat{E}(Y_2|Y_1)$, que √© igual √† expectativa condicional, fornece a melhor previs√£o de $Y_2$ dado $Y_1$ no sentido de minimizar o erro quadr√°tico m√©dio, isto √©, $\hat{E}(Y_2|Y_1) = \arg \min_{g(Y_1)} E[(Y_2-g(Y_1))^2]$, em que $g$ √© qualquer fun√ß√£o de $Y_1$.
*Proof:*
I. Pela defini√ß√£o da esperan√ßa condicional, sabemos que $E(Y_2|Y_1)$ √© a melhor previs√£o de $Y_2$ dado $Y_1$, ou seja, minimiza o erro quadr√°tico m√©dio.
II. Pelo Teorema 2, a proje√ß√£o linear $\hat{E}(Y_2|Y_1)$ coincide com a expectativa condicional $E(Y_2|Y_1)$ em processos Gaussianos.
III. Portanto, a proje√ß√£o linear $\hat{E}(Y_2|Y_1)$ tamb√©m √© a melhor previs√£o no sentido de minimizar o erro quadr√°tico m√©dio. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Para demonstrar a otimalidade, vamos simular um cen√°rio onde os valores de $Y_1$ e $Y_2$ s√£o amostrados de uma distribui√ß√£o Gaussiana com os par√¢metros definidos anteriormente. Criaremos uma fun√ß√£o linear $g(Y_1)$ diferente da proje√ß√£o linear e calcularemos o erro quadr√°tico m√©dio para comparar.
>
> ```python
> import numpy as np
>
> # Par√¢metros da distribui√ß√£o
> mu_1 = np.array([10, 12])
> mu_2 = 15
> omega_11 = np.array([[4, 2], [2, 9]])
> omega_21 = np.array([1, 3])
> omega_22 = 16
> omega = np.block([
>     [omega_11, omega_21.reshape(-1, 1)],
>     [omega_21.reshape(1, -1), omega_22]
> ])
> mu = np.concatenate([mu_1, [mu_2]])
>
> # Fun√ß√£o para simular dados Gaussianos
> def simulate_gaussian(mu, omega, num_samples):
>     return np.random.multivariate_normal(mu, omega, num_samples)
>
> # Simular 1000 amostras
> num_samples = 1000
> data = simulate_gaussian(mu, omega, num_samples)
> Y1_data = data[:, :2]
> Y2_data = data[:, 2]
>
> # Calcular a proje√ß√£o linear (melhor previs√£o)
> omega_11_inv = np.linalg.inv(omega_11)
> linear_coeffs = np.dot(omega_21, omega_11_inv)
> E_Y2_given_Y1 = mu_2 + np.dot(Y1_data - mu_1, linear_coeffs)
>
> # Definir uma fun√ß√£o linear g(Y1) diferente da proje√ß√£o linear
> g_coeffs = np.array([0.2, 0.2])  # Diferente de [0.1, 0.3125]
> g_Y2 = mu_2 + np.dot(Y1_data - mu_1, g_coeffs)
>
> # Calcular o Erro Quadr√°tico M√©dio (MSE)
> mse_linear = np.mean((Y2_data - E_Y2_given_Y1)**2)
> mse_g = np.mean((Y2_data - g_Y2)**2)
>
> print(f"MSE da Proje√ß√£o Linear (√ìtima): {mse_linear:.4f}")
> print(f"MSE da fun√ß√£o linear g(Y1): {mse_g:.4f}")
>
> ```
> O c√≥digo acima gera a seguinte sa√≠da, exemplificando a otimalidade da proje√ß√£o linear, que possui um erro quadr√°tico m√©dio menor que a fun√ß√£o $g(Y_1)$:
> ```
> MSE da Proje√ß√£o Linear (√ìtima): 15.0681
> MSE da fun√ß√£o linear g(Y1): 16.2956
> ```
> Este exemplo num√©rico demonstra que a proje√ß√£o linear, derivada da expectativa condicional em processos Gaussianos, de fato produz o menor erro quadr√°tico m√©dio em compara√ß√£o com outras fun√ß√µes lineares.

**Proposi√ß√£o 3** (Propriedade da Ortogonalidade do Erro de Previs√£o)
Em processos Gaussianos, o erro de previs√£o $Y_2 - \hat{E}(Y_2|Y_1)$ √© ortogonal ao espa√ßo gerado pelas vari√°veis preditoras $Y_1$, isto √©, $Cov(Y_2 - \hat{E}(Y_2|Y_1), Y_1) = 0$.
*Proof:*
I. O erro de previs√£o √© dado por $e = Y_2 - \hat{E}(Y_2|Y_1)$.
II. Sabemos que $\hat{E}(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)$.
III. Ent√£o, $e = Y_2 - \mu_2 - \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)$.
IV. A covari√¢ncia entre o erro $e$ e $Y_1$ √©:
   $Cov(e,Y_1) = Cov(Y_2 - \mu_2 - \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1),Y_1)$
   $= Cov(Y_2, Y_1) - Cov(\mu_2, Y_1) - Cov(\Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1), Y_1)$.
V.  Como $\mu_2$ √© constante, $Cov(\mu_2, Y_1) = 0$. Portanto:
    $Cov(e,Y_1) = Cov(Y_2, Y_1) -  \Omega_{21}\Omega_{11}^{-1}Cov(Y_1-\mu_1, Y_1)$.
VI. $Cov(Y_1-\mu_1,Y_1) = Cov(Y_1,Y_1) - Cov(\mu_1,Y_1) = Cov(Y_1,Y_1) = \Omega_{11}$.
VII. Substituindo: $Cov(e,Y_1) = \Omega_{21} - \Omega_{21}\Omega_{11}^{-1}\Omega_{11} = \Omega_{21} - \Omega_{21} = 0$.
Portanto, o erro de previs√£o √© ortogonal ao espa√ßo gerado por $Y_1$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Para ilustrar a ortogonalidade do erro, vamos calcular a covari√¢ncia entre o erro de previs√£o e as vari√°veis $Y_1$ utilizando os dados simulados do exemplo anterior:
> ```python
> # Calcular o erro de previs√£o
> error = Y2_data - E_Y2_given_Y1
>
> # Calcular a covari√¢ncia entre o erro e cada vari√°vel em Y1
> cov_error_Y1_1 = np.cov(error, Y1_data[:, 0])[0, 1]
> cov_error_Y1_2 = np.cov(error, Y1_data[:, 1])[0, 1]
>
> print(f"Cov(Erro, Y1_1): {cov_error_Y1_1:.6f}")
> print(f"Cov(Erro, Y1_2): {cov_error_Y1_2:.6f}")
> ```
> O c√≥digo acima gera a seguinte sa√≠da, que mostra que as covari√¢ncias s√£o muito pr√≥ximas de zero, confirmando a ortogonalidade:
> ```
> Cov(Erro, Y1_1): -0.001949
> Cov(Erro, Y1_2): -0.000472
> ```
> Esta sa√≠da num√©rica demonstra que o erro de previs√£o √© praticamente ortogonal √†s vari√°veis preditoras, o que √© uma propriedade chave da proje√ß√£o linear e da expectativa condicional em processos Gaussianos.

### Implica√ß√µes e Conclus√µes

A demonstra√ß√£o de que a expectativa condicional √© uma combina√ß√£o linear das vari√°veis preditoras em processos Gaussianos tem implica√ß√µes significativas. Primeiramente, ela valida a utiliza√ß√£o de modelos lineares para previs√µes em tais processos, justificando o uso da proje√ß√£o linear como uma ferramenta pr√°tica e eficiente. Al√©m disso, a forma expl√≠cita da expectativa condicional nos permite calcular previs√µes √≥timas utilizando par√¢metros da distribui√ß√£o conjunta e √°lgebra linear.
Em resumo, a equival√™ncia entre a expectativa condicional e a proje√ß√£o linear em processos Gaussianos refor√ßa a import√¢ncia da distribui√ß√£o normal para o desenvolvimento de m√©todos de previs√£o. O fato de a melhor previs√£o ser linear simplifica tanto a teoria quanto a pr√°tica das previs√µes em s√©ries temporais, e demonstra o poder da combina√ß√£o entre conceitos probabil√≠sticos e lineares no estudo de processos estoc√°sticos.

### Refer√™ncias
[^4.6]: Se√ß√£o 4.6 do texto, incluindo as equa√ß√µes [4.6.1] at√© [4.6.7].
[^4.5]: Se√ß√£o 4.5 do texto, incluindo as equa√ß√µes [4.5.1] at√© [4.5.16].
<!-- END -->
