## Previs√µes √ìtimas Gaussianas e a An√°lise do Erro

### Introdu√ß√£o

Em continuidade ao t√≥pico anterior, onde estabelecemos que, para processos Gaussianos, a **proje√ß√£o linear** coincide com a **esperan√ßa condicional**, e ambas representam a **previs√£o √≥tima irrestrita** [^4.6], este cap√≠tulo se concentrar√° em analisar as propriedades do erro de previs√£o nesse contexto. Particularmente, examinaremos como o erro de previs√£o se comporta em rela√ß√£o √† informa√ß√£o utilizada para prever e como sua distribui√ß√£o normal simplifica a an√°lise da qualidade das previs√µes em processos Gaussianos.

### Conceitos Fundamentais

Nesta se√ß√£o, aprofundaremos as propriedades do erro de previs√£o em processos Gaussianos, demonstrando que este √© **n√£o correlacionado** com a informa√ß√£o utilizada na previs√£o e segue uma **distribui√ß√£o normal**. Essas caracter√≠sticas facilitam a avalia√ß√£o da qualidade das previs√µes.

#### Erro de Previs√£o em Processos Gaussianos

O erro de previs√£o, denotado por $e_t$, √© a diferen√ßa entre o valor real $Y_t$ e a previs√£o $\hat{Y}_t$ baseada em alguma informa√ß√£o passada. Em termos gerais, $e_t = Y_t - \hat{Y}_t$. No contexto de processos Gaussianos, a previs√£o √≥tima √© dada pela esperan√ßa condicional [^4.6]:

$$
\hat{Y}_{t+1} = E(Y_{t+1} | Y_t, Y_{t-1}, ... )
$$

Como estabelecido anteriormente, essa esperan√ßa condicional tamb√©m corresponde √† proje√ß√£o linear [^4.6]. Para simplificar, utilizaremos $Y_t$ para representar o conjunto de informa√ß√µes passadas em geral, e definiremos o erro de previs√£o como:

$$
e_{t+1} = Y_{t+1} - \hat{Y}_{t+1} = Y_{t+1} -  E(Y_{t+1} | Y_t)
$$

**Teorema 2** (N√£o Correla√ß√£o do Erro de Previs√£o)
Em processos Gaussianos, o erro de previs√£o $e_{t+1}$ √© n√£o correlacionado com qualquer fun√ß√£o da informa√ß√£o utilizada para a previs√£o, $Y_t$, ou seja,  $E[e_{t+1}h(Y_t)] = 0$ para qualquer fun√ß√£o $h$.

*Proof:*
Vamos provar que $e_{t+1}$ √© n√£o correlacionado com qualquer fun√ß√£o $h(Y_t)$ em processos Gaussianos.
I.   A esperan√ßa condicional da defini√ß√£o do erro de previs√£o √©:
   $$E[e_{t+1}|Y_t] = E[Y_{t+1} - E(Y_{t+1}|Y_t)|Y_t] = E[Y_{t+1}|Y_t] - E[E(Y_{t+1}|Y_t)|Y_t]$$
II. Como $E(Y_{t+1}|Y_t)$ √© uma fun√ß√£o de $Y_t$, temos que $E[E(Y_{t+1}|Y_t)|Y_t]=E(Y_{t+1}|Y_t)$, portanto
   $$E[e_{t+1}|Y_t] = 0$$
III. A n√£o correla√ß√£o √© ent√£o comprovada usando a lei da esperan√ßa iterada, temos:
    $$E[e_{t+1}h(Y_t)] = E\{E[e_{t+1}h(Y_t)|Y_t]\} = E\{h(Y_t)E[e_{t+1}|Y_t]\} = E\{h(Y_t) \cdot 0\} = 0$$
IV. Assim, $e_{t+1}$ √© n√£o correlacionado com qualquer fun√ß√£o de $Y_t$. $\blacksquare$

Especificamente, quando $h(Y_t) = Y_t$, o teorema 2 implica que o erro de previs√£o √© n√£o correlacionado com a informa√ß√£o utilizada para fazer a previs√£o.  Este √© um resultado importante que simplifica muito a an√°lise de modelos e de previs√£o.

> üí° **Exemplo Num√©rico:**
> Suponha que voc√™ esteja modelando o pre√ßo di√°rio de uma a√ß√£o $Y_t$ como um processo Gaussiano. Digamos que $Y_t$ represente o pre√ßo da a√ß√£o no dia $t$. Voc√™ usa o pre√ßo de ontem, $Y_{t-1}$, para prever o pre√ßo de hoje, $Y_t$. A sua previs√£o linear √© dada por $\hat{Y}_t = 10 + 0.9 Y_{t-1}$. O erro de previs√£o √© $e_t = Y_t - \hat{Y}_t$. De acordo com o Teorema 2, o erro $e_t$ deve ser n√£o correlacionado com o pre√ßo de ontem $Y_{t-1}$. Isso significa que n√£o h√° rela√ß√£o linear entre o erro da sua previs√£o e o pre√ßo de ontem. Por exemplo, se $Y_{t-1} = 100$ e $Y_t = 105$, ent√£o $\hat{Y}_t = 10 + 0.9 \times 100 = 100$ e o erro √© $e_t = 105 - 100 = 5$. Se $Y_{t-1} = 110$ e $Y_t = 108$, ent√£o $\hat{Y}_t = 10 + 0.9 \times 110 = 109$ e o erro √© $e_t = 108 - 109 = -1$. Teorema 2 garante que em um modelo Gaussiano, n√£o importa o valor de $Y_{t-1}$ os erros n√£o tem padr√£o.

**Lema 2.1** (N√£o Correla√ß√£o com Previs√µes Lineares)
Como consequ√™ncia direta do Teorema 2, o erro de previs√£o $e_{t+1}$ √© n√£o correlacionado com qualquer previs√£o linear $\hat{Y}_{t+1}$ baseada em $Y_t$. Ou seja,  $E[e_{t+1}\hat{Y}_{t+1}] = 0$.

*Proof:*
Como $\hat{Y}_{t+1}$ √© uma fun√ß√£o linear de $Y_t$, podemos express√°-la como $\hat{Y}_{t+1} = a + bY_t$, para algumas constantes $a$ e $b$. Assim, $E[e_{t+1}\hat{Y}_{t+1}] = E[e_{t+1}(a + bY_t)] = aE[e_{t+1}] + bE[e_{t+1}Y_t]$. Pelo Teorema 2, $E[e_{t+1}Y_t] = 0$ e como $E[e_{t+1}]=0$ (Teorema 3), conclu√≠mos que $E[e_{t+1}\hat{Y}_{t+1}] = 0$. $\blacksquare$

Este resultado refor√ßa a ideia de que o erro de previs√£o n√£o carrega informa√ß√µes adicionais que poderiam ser utilizadas para refinar a previs√£o linear.

> üí° **Exemplo Num√©rico:**
> Continuando com o exemplo do pre√ßo da a√ß√£o, considere que a previs√£o linear seja $\hat{Y}_t = 10 + 0.9Y_{t-1}$. Se os erros de previs√£o $e_t$ forem calculados como no exemplo anterior, o Lema 2.1 afirma que a covari√¢ncia entre esses erros e a previs√£o $\hat{Y}_t$ ser√° zero. Isso significa que os erros n√£o podem ser previstos usando a previs√£o $\hat{Y}_t$. Ou seja, os erros n√£o mostram padr√£o nenhum com base nos valores da previs√£o. N√£o existe rela√ß√£o linear entre o erro e a previs√£o.

#### Distribui√ß√£o Normal do Erro de Previs√£o

Em processos Gaussianos, o erro de previs√£o tamb√©m possui uma distribui√ß√£o normal. Este resultado √© uma consequ√™ncia direta da propriedade dos processos Gaussianos de que qualquer combina√ß√£o linear de vari√°veis Gaussianas tamb√©m √© normal.

**Teorema 3** (Distribui√ß√£o do Erro de Previs√£o)
Em processos Gaussianos, o erro de previs√£o  $e_{t+1}$ tem uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia igual √† vari√¢ncia condicional do processo.

*Proof:*
Vamos provar que $e_{t+1}$ tem uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia igual √† vari√¢ncia condicional.
I.  Dado que o processo √© Gaussiano, sabemos que a distribui√ß√£o condicional de $Y_{t+1}$ dado $Y_t$ √© normal:
   $$Y_{t+1} | Y_t \sim N(E(Y_{t+1}|Y_t),  Var(Y_{t+1}|Y_t))$$
II.  O erro de previs√£o √© definido como $e_{t+1} = Y_{t+1} - E(Y_{t+1}|Y_t)$.
III. Como $E(Y_{t+1}|Y_t)$ √© uma fun√ß√£o de $Y_t$ e $Y_{t+1}$ dado $Y_t$ √© normal, o erro √© uma combina√ß√£o linear de vari√°veis normais e tamb√©m possui distribui√ß√£o normal, e tamb√©m a esperan√ßa condicional tem uma vari√¢ncia constante (Teorema 1).
IV.  Sabemos que $E[e_{t+1}|Y_t] = 0$, portanto $E[e_{t+1}] = 0$.
V.  A vari√¢ncia do erro √© dada pela vari√¢ncia condicional, ou seja:
   $$Var(e_{t+1}) = Var(Y_{t+1} - E(Y_{t+1}|Y_t)) = Var(Y_{t+1}|Y_t)$$
VI. Portanto, o erro de previs√£o $e_{t+1}$ segue uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia igual √† vari√¢ncia condicional do processo. $\blacksquare$

Este resultado nos permite descrever completamente a distribui√ß√£o do erro, usando apenas a sua m√©dia e vari√¢ncia condicional.

> üí° **Exemplo Num√©rico:**
> Suponha que, no exemplo num√©rico anterior, a vari√¢ncia condicional do pre√ßo da a√ß√£o hoje, dado o pre√ßo de ontem, seja $Var(Y_t|Y_{t-1}) = 5$. Portanto, o erro de previs√£o ter√° distribui√ß√£o normal com m√©dia zero e vari√¢ncia 5:
> $$
> e_{t+1} \sim N(0, 5)
> $$
> Esta informa√ß√£o permite calcular intervalos de confian√ßa para o erro de previs√£o. Por exemplo, em 95\% dos casos, o erro estar√° no intervalo $\pm 1.96\sqrt{5} \approx \pm 4.38$. Se voc√™ previu o pre√ßo de hoje como 100, com 95\% de confian√ßa, o valor real do pre√ßo de hoje estar√° entre 100 - 4.38 = 95.62 e 100 + 4.38 = 104.38.

**Teorema 3.1** (Independ√™ncia do Erro e da Informa√ß√£o Passada)
Em processos Gaussianos, o erro de previs√£o $e_{t+1}$ √© independente da informa√ß√£o passada $Y_t$.

*Proof:*
Pelo Teorema 2, sabemos que o erro de previs√£o √© n√£o correlacionado com qualquer fun√ß√£o de $Y_t$. Al√©m disso, pelo Teorema 3, sabemos que $e_{t+1}$ tem distribui√ß√£o normal. Como o processo √© Gaussiano, a distribui√ß√£o conjunta de $e_{t+1}$ e $Y_t$ √© tamb√©m normal. Em distribui√ß√µes normais, n√£o correla√ß√£o implica independ√™ncia. Portanto, $e_{t+1}$ e $Y_t$ s√£o independentes.  $\blacksquare$

Este resultado √© uma extens√£o importante do Teorema 2 e do Teorema 3, demonstrando que o erro de previs√£o n√£o apenas n√£o √© correlacionado com a informa√ß√£o passada, mas √©, de fato, estatisticamente independente dela. Isso refor√ßa que a previs√£o √≥tima extraiu toda a informa√ß√£o √∫til de $Y_t$.

> üí° **Exemplo Num√©rico:**
> Usando o exemplo do pre√ßo da a√ß√£o, o Teorema 3.1 afirma que o erro de previs√£o de hoje, $e_t$, √© independente do pre√ßo da a√ß√£o de ontem, $Y_{t-1}$. Isso significa que, olhando para o pre√ßo de ontem, voc√™ n√£o pode obter nenhuma informa√ß√£o sobre o sinal ou tamanho do erro de hoje. Por exemplo, se o pre√ßo de ontem foi de 100, o erro de hoje pode ser positivo ou negativo com a mesma probabilidade, e o tamanho do erro n√£o depende se o pre√ßo de ontem foi de 100 ou 110. N√£o √© poss√≠vel usar o pre√ßo de ontem para refinar a previs√£o sobre o erro de hoje.
>
> Podemos simular essa situa√ß√£o com dados aleat√≥rios:
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> np.random.seed(42)
> n_samples = 200
> Y_prev = np.random.normal(100, 10, n_samples) # Pre√ßo de ontem
> errors = np.random.normal(0, 5, n_samples) # Erro com m√©dia 0 e desvio 5
>
> plt.figure(figsize=(8,6))
> plt.scatter(Y_prev, errors, alpha=0.6)
> plt.xlabel("Pre√ßo da A√ß√£o Ontem (\$Y_{t-1}\$)")
> plt.ylabel("Erro de Previs√£o (\$e_t\$)")
> plt.title("Scatter Plot: Erro vs. Pre√ßo da A√ß√£o de Ontem")
> plt.grid(True)
> plt.show()
>
> correlation = np.corrcoef(Y_prev, errors)[0, 1]
> print(f"Correla√ß√£o entre Y_prev e erros: {correlation:.4f}")
> ```
> A figura gerada mostrar√° uma nuvem de pontos sem padr√£o, e a correla√ß√£o ser√° muito pr√≥xima de zero, evidenciando a independ√™ncia entre o erro de previs√£o e a informa√ß√£o passada.

### Conclus√£o

A an√°lise do erro de previs√£o em processos Gaussianos revela propriedades que simplificam a avalia√ß√£o e aprimoramento de modelos de previs√£o. O fato de o erro ser **n√£o correlacionado** com as informa√ß√µes usadas para gerar a previs√£o indica que o modelo capturou toda a informa√ß√£o relevante dispon√≠vel e que o erro n√£o pode ser predito linearmente com base nessa informa√ß√£o. Adicionalmente, a **distribui√ß√£o normal** do erro, juntamente com a sua m√©dia zero, nos permite quantificar a incerteza associada √† previs√£o, possibilitando a constru√ß√£o de intervalos de confian√ßa. Em resumo, para processos Gaussianos, o erro de previs√£o se comporta de maneira ideal, o que faz com que o modelo de proje√ß√£o linear se torne uma ferramenta particularmente √∫til, fornecendo previs√µes √≥timas e f√°ceis de analisar. Esses resultados demonstram a import√¢ncia das propriedades Gaussianas para o desenvolvimento de modelos de previs√£o mais precisos e informativos.

### Refer√™ncias

[^4.6]: Se√ß√£o 4.6 do texto, incluindo as equa√ß√µes [4.6.1] a [4.6.7]
<!-- END -->
