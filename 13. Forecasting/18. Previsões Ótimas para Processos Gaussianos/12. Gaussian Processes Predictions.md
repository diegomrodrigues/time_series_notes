## PrevisÃµes Ã“timas Gaussianas: A Natureza Linear da Expectativa Condicional

### IntroduÃ§Ã£o
Este capÃ­tulo visa consolidar o entendimento sobre a otimalidade das previsÃµes em **processos Gaussianos**, focando na caracterÃ­stica linear da **expectativa condicional**, e como ela se manifesta de forma explÃ­cita por meio de uma combinaÃ§Ã£o linear das variÃ¡veis preditoras. Anteriormente, estabelecemos que a **projeÃ§Ã£o linear** e a **expectativa condicional** coincidem para processos Gaussianos [^4.6]. Agora, vamos explorar como essa equivalÃªncia se traduz em uma estrutura linear explÃ­cita da expectativa condicional, reforÃ§ando a ideia de que, neste contexto, as previsÃµes Ã³timas sÃ£o obtidas por meio de combinaÃ§Ãµes lineares, o que destaca a importÃ¢ncia da Ã¡lgebra linear e da teoria de probabilidade neste campo.

### Expectativa Condicional como CombinaÃ§Ã£o Linear

Em processos Gaussianos, a **expectativa condicional** de uma variÃ¡vel $Y_2$ dado um vetor de variÃ¡veis $Y_1$, denotada por $E(Y_2|Y_1)$, Ã© expressa explicitamente como uma combinaÃ§Ã£o linear de $Y_1$. Partindo da densidade condicional derivada no capÃ­tulo anterior, obtemos:
$$
E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
onde:
- $E(Y_2|Y_1)$ representa a expectativa condicional de $Y_2$ dado $Y_1$, ou seja, a melhor previsÃ£o irrestrita de $Y_2$ dado $Y_1$.
- $\mu_2$ Ã© o vetor de mÃ©dias marginais de $Y_2$.
- $\mu_1$ Ã© o vetor de mÃ©dias marginais de $Y_1$.
- $\Omega_{21}$ Ã© a matriz de covariÃ¢ncia entre $Y_2$ e $Y_1$.
- $\Omega_{11}$ Ã© a matriz de covariÃ¢ncia de $Y_1$.

Essa expressÃ£o revela que a expectativa condicional Ã© uma funÃ§Ã£o linear de $Y_1$, caracterizada por um termo constante $\mu_2$ e uma combinaÃ§Ã£o linear dos desvios de $Y_1$ em relaÃ§Ã£o Ã  sua mÃ©dia, onde os pesos dessa combinaÃ§Ã£o sÃ£o dados por $\Omega_{21}\Omega_{11}^{-1}$.

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Para ilustrar, suponha que $Y_1$ seja um vetor com duas variÃ¡veis representando o preÃ§o de um ativo em dois momentos diferentes, com mÃ©dias $\mu_1 = \begin{bmatrix} 10 \\ 12 \end{bmatrix}$, e $Y_2$ seja uma variÃ¡vel representando o preÃ§o de um outro ativo, com mÃ©dia $\mu_2 = 15$. A matriz de covariÃ¢ncia conjunta Ã©:
> $$
> \Omega = \begin{bmatrix}
>   \begin{matrix} 4 & 2 \\ 2 & 9 \end{matrix} &  \begin{matrix} 1 \\ 3 \end{matrix}  \\
>   \begin{matrix} 1 & 3 \end{matrix} & 16
> \end{bmatrix}
> $$
> Temos entÃ£o $\Omega_{11} = \begin{bmatrix} 4 & 2 \\ 2 & 9 \end{bmatrix}$, $\Omega_{21} = \begin{bmatrix} 1 & 3 \end{bmatrix}$, e $\mu_1 = \begin{bmatrix} 10 \\ 12 \end{bmatrix}$, $\mu_2 = 15$.
>
> A expectativa condicional $E(Y_2|Y_1)$ Ã© calculada como:
> $$
> E(Y_2|Y_1) = 15 + \begin{bmatrix} 1 & 3 \end{bmatrix} \begin{bmatrix} 4 & 2 \\ 2 & 9 \end{bmatrix}^{-1} \left( \begin{bmatrix} Y_{11} \\ Y_{12} \end{bmatrix} - \begin{bmatrix} 10 \\ 12 \end{bmatrix} \right)
> $$
> Para calcular $\Omega_{11}^{-1}$, podemos usar uma biblioteca em Python:
> ```python
> import numpy as np
>
> omega_11 = np.array([[4, 2],
>                    [2, 9]])
>
> omega_11_inv = np.linalg.inv(omega_11)
>
> print(omega_11_inv)
> ```
> O cÃ³digo acima gera a seguinte saÃ­da:
> ```
> [[ 0.28125  -0.0625 ]
>  [-0.0625   0.125  ]]
> ```
> Logo, a expectativa condicional Ã©
> $$
> E(Y_2|Y_1) = 15 + \begin{bmatrix} 1 & 3 \end{bmatrix} \begin{bmatrix} 0.28125 & -0.0625 \\ -0.0625 & 0.125 \end{bmatrix} \left( \begin{bmatrix} Y_{11} \\ Y_{12} \end{bmatrix} - \begin{bmatrix} 10 \\ 12 \end{bmatrix} \right)
> $$
> $$
> E(Y_2|Y_1) = 15 +  \begin{bmatrix} 0.1 & 0.3125 \end{bmatrix}  \left( \begin{bmatrix} Y_{11} - 10 \\ Y_{12} - 12 \end{bmatrix} \right)
> $$
> $$
> E(Y_2|Y_1) = 15 + 0.1 (Y_{11} - 10) + 0.3125 (Y_{12} - 12)
> $$
> Podemos observar que $E(Y_2|Y_1)$ Ã© uma combinaÃ§Ã£o linear de $Y_{11}$ e $Y_{12}$, e os coeficientes sÃ£o calculados a partir das covariÃ¢ncias da distribuiÃ§Ã£o conjunta.
>
> Para um valor especÃ­fico de $Y_1$, digamos $Y_1 = \begin{bmatrix} 11 \\ 13 \end{bmatrix}$, temos:
> $$
> E(Y_2|Y_1 = \begin{bmatrix} 11 \\ 13 \end{bmatrix}) = 15 + 0.1 (11 - 10) + 0.3125 (13 - 12) = 15 + 0.1 + 0.3125 = 15.4125
> $$
> Isto Ã© a melhor previsÃ£o do preÃ§o do segundo ativo, dados os preÃ§os dos ativos 1 e 2.

**Teorema 1** (Expectativa Condicional como FunÃ§Ã£o Linear)
Em processos Gaussianos, a expectativa condicional $E(Y_2|Y_1)$ Ã© uma funÃ§Ã£o linear de $Y_1$, e Ã© dada por $E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)$.
*Proof:*
I. A distribuiÃ§Ã£o condicional de um vetor Gaussiano $Y_2$ dado um vetor Gaussiano $Y_1$ Ã© tambÃ©m Gaussiana.
II. A mÃ©dia da distribuiÃ§Ã£o condicional (ou seja, a expectativa condicional) Ã©:
$$
E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
III. Como $\mu_2$, $\Omega_{21}$, $\Omega_{11}$ e $\mu_1$ sÃ£o parÃ¢metros da distribuiÃ§Ã£o conjunta e, portanto, constantes, a expressÃ£o acima Ã© uma funÃ§Ã£o linear de $Y_1$.
Portanto, a expectativa condicional $E(Y_2|Y_1)$ Ã© uma funÃ§Ã£o linear de $Y_1$. $\blacksquare$

**Lema 1.1** (Propriedade da InversÃ£o de Matrizes de CovariÃ¢ncia)
Se $\Omega_{11}$ Ã© a matriz de covariÃ¢ncia de $Y_1$ e Ã© invertÃ­vel, entÃ£o sua inversa $\Omega_{11}^{-1}$ tambÃ©m Ã© simÃ©trica e positiva definida.
*Proof:*
I. Uma matriz de covariÃ¢ncia $\Omega_{11}$ Ã© por definiÃ§Ã£o simÃ©trica e positiva semi-definida.
II. Se $\Omega_{11}$ Ã© invertÃ­vel, entÃ£o ela Ã© tambÃ©m positiva definida.
III. A inversa de uma matriz simÃ©trica Ã© tambÃ©m simÃ©trica.
IV. A inversa de uma matriz positiva definida Ã© tambÃ©m positiva definida.
Portanto, $\Omega_{11}^{-1}$ Ã© simÃ©trica e positiva definida. $\blacksquare$

### EquivalÃªncia com a ProjeÃ§Ã£o Linear
Como vimos anteriormente, a projeÃ§Ã£o linear de $Y_2$ sobre $Y_1$ Ã© definida como a melhor previsÃ£o de $Y_2$ que Ã© uma funÃ§Ã£o linear de $Y_1$, e Ã© dada por [^4.5]:
$$
\hat{E}(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
Comparando esta expressÃ£o com a expressÃ£o da expectativa condicional, notamos que elas sÃ£o idÃªnticas. Este resultado formaliza a equivalÃªncia entre a expectativa condicional e a projeÃ§Ã£o linear em processos Gaussianos. Isso significa que, para estes processos, a previsÃ£o Ã³tima irrestrita Ã© tambÃ©m linear, o que simplifica enormemente a anÃ¡lise e a implementaÃ§Ã£o das previsÃµes.

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Usando os mesmos dados do exemplo anterior, a projeÃ§Ã£o linear de $Y_2$ em $Y_1$ Ã© dada por:
> $$
> \hat{E}(Y_2|Y_1) = 15 + \begin{bmatrix} 1 & 3 \end{bmatrix} \begin{bmatrix} 0.28125 & -0.0625 \\ -0.0625 & 0.125 \end{bmatrix} \left( \begin{bmatrix} Y_{11} \\ Y_{12} \end{bmatrix} - \begin{bmatrix} 10 \\ 12 \end{bmatrix} \right)
> $$
> $$
> \hat{E}(Y_2|Y_1) = 15 + 0.1 (Y_{11} - 10) + 0.3125 (Y_{12} - 12)
> $$
> Observe que esta expressÃ£o coincide exatamente com a expressÃ£o da expectativa condicional calculada anteriormente, comprovando numericamente que as duas expressÃµes sÃ£o equivalentes para este processo Gaussiano.

**Teorema 2** (EquivalÃªncia da Expectativa Condicional e ProjeÃ§Ã£o Linear)
Em processos Gaussianos, a expectativa condicional $E(Y_2|Y_1)$ coincide com a projeÃ§Ã£o linear $\hat{E}(Y_2|Y_1)$, ou seja, $E(Y_2|Y_1) = \hat{E}(Y_2|Y_1)$.
*Proof:*
I. Do Teorema 1 e da deduÃ§Ã£o anterior, temos:
$$
E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
II. A projeÃ§Ã£o linear tambÃ©m Ã© dada por:
$$
\hat{E}(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
III. Comparando as duas expressÃµes, concluÃ­mos que
$$
E(Y_2|Y_1) = \hat{E}(Y_2|Y_1)
$$
Portanto, para processos Gaussianos, a expectativa condicional e a projeÃ§Ã£o linear sÃ£o idÃªnticas.  $\blacksquare$

**CorolÃ¡rio 2.1** (Otimidade da ProjeÃ§Ã£o Linear em Processos Gaussianos)
Em processos Gaussianos, a projeÃ§Ã£o linear $\hat{E}(Y_2|Y_1)$, que Ã© igual Ã  expectativa condicional, fornece a melhor previsÃ£o de $Y_2$ dado $Y_1$ no sentido de minimizar o erro quadrÃ¡tico mÃ©dio, isto Ã©, $\hat{E}(Y_2|Y_1) = \arg \min_{g(Y_1)} E[(Y_2-g(Y_1))^2]$, em que $g$ Ã© qualquer funÃ§Ã£o de $Y_1$.
*Proof:*
I. Pela definiÃ§Ã£o da esperanÃ§a condicional, sabemos que $E(Y_2|Y_1)$ Ã© a melhor previsÃ£o de $Y_2$ dado $Y_1$, ou seja, minimiza o erro quadrÃ¡tico mÃ©dio.
II. Pelo Teorema 2, a projeÃ§Ã£o linear $\hat{E}(Y_2|Y_1)$ coincide com a expectativa condicional $E(Y_2|Y_1)$ em processos Gaussianos.
III. Portanto, a projeÃ§Ã£o linear $\hat{E}(Y_2|Y_1)$ tambÃ©m Ã© a melhor previsÃ£o no sentido de minimizar o erro quadrÃ¡tico mÃ©dio. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Para demonstrar a otimalidade, vamos simular um cenÃ¡rio onde os valores de $Y_1$ e $Y_2$ sÃ£o amostrados de uma distribuiÃ§Ã£o Gaussiana com os parÃ¢metros definidos anteriormente. Criaremos uma funÃ§Ã£o linear $g(Y_1)$ diferente da projeÃ§Ã£o linear e calcularemos o erro quadrÃ¡tico mÃ©dio para comparar.
>
> ```python
> import numpy as np
>
> # ParÃ¢metros da distribuiÃ§Ã£o
> mu_1 = np.array([10, 12])
> mu_2 = 15
> omega_11 = np.array([[4, 2], [2, 9]])
> omega_21 = np.array([1, 3])
> omega_22 = 16
> omega = np.block([
>     [omega_11, omega_21.reshape(-1, 1)],
>     [omega_21.reshape(1, -1), omega_22]
> ])
> mu = np.concatenate([mu_1, [mu_2]])
>
> # FunÃ§Ã£o para simular dados Gaussianos
> def simulate_gaussian(mu, omega, num_samples):
>     return np.random.multivariate_normal(mu, omega, num_samples)
>
> # Simular 1000 amostras
> num_samples = 1000
> data = simulate_gaussian(mu, omega, num_samples)
> Y1_data = data[:, :2]
> Y2_data = data[:, 2]
>
> # Calcular a projeÃ§Ã£o linear (melhor previsÃ£o)
> omega_11_inv = np.linalg.inv(omega_11)
> linear_coeffs = np.dot(omega_21, omega_11_inv)
> E_Y2_given_Y1 = mu_2 + np.dot(Y1_data - mu_1, linear_coeffs)
>
> # Definir uma funÃ§Ã£o linear g(Y1) diferente da projeÃ§Ã£o linear
> g_coeffs = np.array([0.2, 0.2])  # Diferente de [0.1, 0.3125]
> g_Y2 = mu_2 + np.dot(Y1_data - mu_1, g_coeffs)
>
> # Calcular o Erro QuadrÃ¡tico MÃ©dio (MSE)
> mse_linear = np.mean((Y2_data - E_Y2_given_Y1)**2)
> mse_g = np.mean((Y2_data - g_Y2)**2)
>
> print(f"MSE da ProjeÃ§Ã£o Linear (Ã“tima): {mse_linear:.4f}")
> print(f"MSE da funÃ§Ã£o linear g(Y1): {mse_g:.4f}")
>
> ```
> O cÃ³digo acima gera a seguinte saÃ­da, exemplificando a otimalidade da projeÃ§Ã£o linear, que possui um erro quadrÃ¡tico mÃ©dio menor que a funÃ§Ã£o $g(Y_1)$:
> ```
> MSE da ProjeÃ§Ã£o Linear (Ã“tima): 15.0681
> MSE da funÃ§Ã£o linear g(Y1): 16.2956
> ```
> Este exemplo numÃ©rico demonstra que a projeÃ§Ã£o linear, derivada da expectativa condicional em processos Gaussianos, de fato produz o menor erro quadrÃ¡tico mÃ©dio em comparaÃ§Ã£o com outras funÃ§Ãµes lineares.

**ProposiÃ§Ã£o 3** (Propriedade da Ortogonalidade do Erro de PrevisÃ£o)
Em processos Gaussianos, o erro de previsÃ£o $Y_2 - \hat{E}(Y_2|Y_1)$ Ã© ortogonal ao espaÃ§o gerado pelas variÃ¡veis preditoras $Y_1$, isto Ã©, $Cov(Y_2 - \hat{E}(Y_2|Y_1), Y_1) = 0$.
*Proof:*
I. O erro de previsÃ£o Ã© dado por $e = Y_2 - \hat{E}(Y_2|Y_1)$.
II. Sabemos que $\hat{E}(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)$.
III. EntÃ£o, $e = Y_2 - \mu_2 - \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)$.
IV. A covariÃ¢ncia entre o erro $e$ e $Y_1$ Ã©:
   $Cov(e,Y_1) = Cov(Y_2 - \mu_2 - \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1),Y_1)$
   $= Cov(Y_2, Y_1) - Cov(\mu_2, Y_1) - Cov(\Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1), Y_1)$.
V.  Como $\mu_2$ Ã© constante, $Cov(\mu_2, Y_1) = 0$. Portanto:
    $Cov(e,Y_1) = Cov(Y_2, Y_1) -  \Omega_{21}\Omega_{11}^{-1}Cov(Y_1-\mu_1, Y_1)$.
VI. $Cov(Y_1-\mu_1,Y_1) = Cov(Y_1,Y_1) - Cov(\mu_1,Y_1) = Cov(Y_1,Y_1) = \Omega_{11}$.
VII. Substituindo: $Cov(e,Y_1) = \Omega_{21} - \Omega_{21}\Omega_{11}^{-1}\Omega_{11} = \Omega_{21} - \Omega_{21} = 0$.
Portanto, o erro de previsÃ£o Ã© ortogonal ao espaÃ§o gerado por $Y_1$. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Para ilustrar a ortogonalidade do erro, vamos calcular a covariÃ¢ncia entre o erro de previsÃ£o e as variÃ¡veis $Y_1$ utilizando os dados simulados do exemplo anterior:
> ```python
> # Calcular o erro de previsÃ£o
> error = Y2_data - E_Y2_given_Y1
>
> # Calcular a covariÃ¢ncia entre o erro e cada variÃ¡vel em Y1
> cov_error_Y1_1 = np.cov(error, Y1_data[:, 0])[0, 1]
> cov_error_Y1_2 = np.cov(error, Y1_data[:, 1])[0, 1]
>
> print(f"Cov(Erro, Y1_1): {cov_error_Y1_1:.6f}")
> print(f"Cov(Erro, Y1_2): {cov_error_Y1_2:.6f}")
> ```
> O cÃ³digo acima gera a seguinte saÃ­da, que mostra que as covariÃ¢ncias sÃ£o muito prÃ³ximas de zero, confirmando a ortogonalidade:
> ```
> Cov(Erro, Y1_1): -0.001949
> Cov(Erro, Y1_2): -0.000472
> ```
> Esta saÃ­da numÃ©rica demonstra que o erro de previsÃ£o Ã© praticamente ortogonal Ã s variÃ¡veis preditoras, o que Ã© uma propriedade chave da projeÃ§Ã£o linear e da expectativa condicional em processos Gaussianos.

### ImplicaÃ§Ãµes e ConclusÃµes

A demonstraÃ§Ã£o de que a expectativa condicional Ã© uma combinaÃ§Ã£o linear das variÃ¡veis preditoras em processos Gaussianos tem implicaÃ§Ãµes significativas. Primeiramente, ela valida a utilizaÃ§Ã£o de modelos lineares para previsÃµes em tais processos, justificando o uso da projeÃ§Ã£o linear como uma ferramenta prÃ¡tica e eficiente. AlÃ©m disso, a forma explÃ­cita da expectativa condicional nos permite calcular previsÃµes Ã³timas utilizando parÃ¢metros da distribuiÃ§Ã£o conjunta e Ã¡lgebra linear.
Em resumo, a equivalÃªncia entre a expectativa condicional e a projeÃ§Ã£o linear em processos Gaussianos reforÃ§a a importÃ¢ncia da distribuiÃ§Ã£o normal para o desenvolvimento de mÃ©todos de previsÃ£o. O fato de a melhor previsÃ£o ser linear simplifica tanto a teoria quanto a prÃ¡tica das previsÃµes em sÃ©ries temporais, e demonstra o poder da combinaÃ§Ã£o entre conceitos probabilÃ­sticos e lineares no estudo de processos estocÃ¡sticos.

### ReferÃªncias
[^4.6]: SeÃ§Ã£o 4.6 do texto, incluindo as equaÃ§Ãµes [4.6.1] atÃ© [4.6.7].
[^4.5]: SeÃ§Ã£o 4.5 do texto, incluindo as equaÃ§Ãµes [4.5.1] atÃ© [4.5.16].
<!-- END -->
