## A Expectativa Condicional Expressa Explicitamente e sua Equival√™ncia com a Proje√ß√£o Linear em Processos Gaussianos

### Introdu√ß√£o

Este cap√≠tulo tem como objetivo central apresentar uma deriva√ß√£o expl√≠cita para a **expectativa condicional** $E(Y_2|Y_1)$ em **processos Gaussianos**, e demonstrar sua equival√™ncia com a **proje√ß√£o linear** $\hat{E}(Y_2|Y_1)$ [^4.6]. Nos cap√≠tulos anteriores, vimos que a proje√ß√£o linear e a expectativa condicional fornecem a previs√£o √≥tima irrestrita em processos Gaussianos, mas agora vamos apresentar uma deriva√ß√£o expl√≠cita para a expectativa condicional usando par√¢metros da distribui√ß√£o conjunta e demonstrar como ela coincide com a proje√ß√£o linear. Essa an√°lise √© crucial para refor√ßar a compreens√£o da natureza das previs√µes √≥timas em modelos Gaussianos, e para destacar a praticidade e o poder das ferramentas lineares neste contexto.

### Deriva√ß√£o Expl√≠cita da Expectativa Condicional

Para derivar uma express√£o expl√≠cita para a expectativa condicional, vamos partir da **densidade condicional** de $Y_2$ dado $Y_1$ que j√° obtivemos no cap√≠tulo anterior.
A densidade condicional para um vetor Gaussiano $Y_2$ dado um vetor Gaussiano $Y_1$ √© dada por:
$$
f_{Y_2|Y_1}(y_2|y_1) = \frac{1}{(2\pi)^{n_2/2} |H|^{1/2}} \exp \left\{ -\frac{1}{2} (y_2 - m)'H^{-1}(y_2 - m) \right\}
$$
onde
$$
m = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(y_1-\mu_1)
$$
e
$$
H = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}
$$
Lembrando que $n_2$ √© a dimens√£o do vetor $Y_2$, $H$ √© a matriz de covari√¢ncia condicional, $m$ √© o vetor de m√©dias condicionais, $\mu_1$ e $\mu_2$ s√£o os vetores de m√©dias marginais de $Y_1$ e $Y_2$, respectivamente, e $\Omega$ √© a matriz de covari√¢ncia conjunta.

A **expectativa condicional** $E[Y_2|Y_1]$ √© a m√©dia da distribui√ß√£o condicional $f_{Y_2|Y_1}(y_2|y_1)$. Portanto, em processos Gaussianos, a expectativa condicional √© dada por:
$$
E[Y_2|Y_1] = m = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
Esta √© a express√£o expl√≠cita da expectativa condicional em processos Gaussianos, onde vemos que ela √© expressa em fun√ß√£o dos par√¢metros da distribui√ß√£o conjunta, isto √©, $\mu_1$, $\mu_2$, $\Omega_{11}$, e $\Omega_{21}$.

> üí° **Exemplo Num√©rico:**
> Para ilustrar, considere dois ativos onde $Y_1$ representa o retorno do ativo 1 e $Y_2$ representa o retorno do ativo 2. Suponha que as m√©dias s√£o $\mu_1 = 0.06$ e $\mu_2 = 0.09$, e a matriz de covari√¢ncia √©:
> $$
> \Omega = \begin{bmatrix}
>   0.0025 & 0.001 \\
>   0.001 & 0.0049 \\
> \end{bmatrix}
> $$
> A expectativa condicional $E[Y_2|Y_1]$ pode ser calculada como:
> $$
> E[Y_2|Y_1] = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1 - \mu_1)
> $$
> Substituindo os valores:
> $$
> E[Y_2|Y_1] = 0.09 + \frac{0.001}{0.0025}(Y_1 - 0.06) = 0.09 + 0.4(Y_1 - 0.06)
> $$
> Esta √© a express√£o expl√≠cita para a expectativa condicional neste caso. Para um valor espec√≠fico, por exemplo, $Y_1 = 0.07$, ter√≠amos:
> $$
> E[Y_2|Y_1 = 0.07] = 0.09 + 0.4(0.07 - 0.06) = 0.09 + 0.004 = 0.094
> $$
> Isto representa a previs√£o √≥tima para o retorno do ativo 2 quando o retorno do ativo 1 √© 0.07.
>
> Podemos usar Python para calcular isso:
> ```python
> import numpy as np
>
> # Par√¢metros
> mu_1 = 0.06
> mu_2 = 0.09
> omega_11 = 0.0025
> omega_21 = 0.001
>
> # Valor de Y1 para o qual vamos calcular a expectativa condicional
> y1_value = 0.07
>
> # C√°lculo da expectativa condicional
> conditional_expectation = mu_2 + (omega_21 / omega_11) * (y1_value - mu_1)
>
> print(f"E[Y2|Y1 = {y1_value}]: {conditional_expectation}")
> ```
> Este c√≥digo calcula e imprime o valor da expectativa condicional de $Y_2$ dado $Y_1=0.07$, demonstrando numericamente o resultado obtido.

**Lema 1** A expectativa condicional $E(Y_2|Y_1)$ de um processo Gaussiano pode ser expressa explicitamente em termos dos par√¢metros da distribui√ß√£o conjunta e √© uma fun√ß√£o linear de $Y_1$, dada por $E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)$.
*Proof:*
I. Do cap√≠tulo anterior, vimos que a m√©dia da distribui√ß√£o condicional √© dada por:
$$
m = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(y_1-\mu_1)
$$
II.  Por defini√ß√£o, a esperan√ßa condicional √© a m√©dia da distribui√ß√£o condicional, portanto:
$$E(Y_2|Y_1) = m = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)$$
III. Dado que $\mu_2$, $\Omega_{21}$, $\Omega_{11}$ e $\mu_1$ s√£o constantes, conclu√≠mos que $E(Y_2|Y_1)$ √© uma fun√ß√£o linear de $Y_1$. $\blacksquare$

**Lema 1.1** A covari√¢ncia condicional $Cov(Y_2|Y_1)$ de um processo Gaussiano √© dada por $H = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}$ e √© independente de $Y_1$.
*Proof:*
I. Do cap√≠tulo anterior, a matriz de covari√¢ncia condicional √© dada por:
$$
H = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}
$$
II. Observando a express√£o, notamos que ela n√£o depende de $Y_1$.
III. Portanto, a covari√¢ncia condicional $Cov(Y_2|Y_1) = H$ √© independente de $Y_1$. $\blacksquare$

### Equival√™ncia com a Proje√ß√£o Linear

Por outro lado, a **proje√ß√£o linear** de $Y_2$ sobre $Y_1$ √© dada por [^4.5]:
$$
\hat{E}(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
Comparando as express√µes para a expectativa condicional e para a proje√ß√£o linear, podemos notar que elas s√£o id√™nticas. Isso demonstra formalmente a equival√™ncia entre a expectativa condicional e a proje√ß√£o linear em processos Gaussianos.

> üí° **Exemplo Num√©rico:**
> Usando os mesmos dados do exemplo anterior, a proje√ß√£o linear de $Y_2$ em $Y_1$ √© dada por:
> $$
> \hat{E}(Y_2|Y_1) = 0.09 + 0.4(Y_1 - 0.06)
> $$
> Observe que essa express√£o coincide exatamente com a express√£o que obtivemos para a expectativa condicional. Se observarmos $Y_1 = 0.07$, a proje√ß√£o linear √©:
> $$
> \hat{E}[Y_2|Y_1 = 0.07] = 0.09 + 0.4(0.07 - 0.06) = 0.094
> $$
> Novamente, o mesmo valor obtido para a expectativa condicional.
>
> Podemos confirmar a equival√™ncia da expectativa condicional e da proje√ß√£o linear usando Python:
> ```python
> import numpy as np
>
> # Par√¢metros do exemplo anterior
> mu_1 = 0.06
> mu_2 = 0.09
> omega_11 = 0.0025
> omega_21 = 0.001
>
> # Valor de Y1 para o qual faremos a previs√£o
> y1_value = 0.07
>
> # C√°lculo da proje√ß√£o linear
> linear_projection = mu_2 + (omega_21 / omega_11) * (y1_value - mu_1)
>
> # C√°lculo da expectativa condicional (j√° feito no exemplo anterior)
> conditional_expectation = mu_2 + (omega_21 / omega_11) * (y1_value - mu_1)
>
> print(f"Proje√ß√£o Linear E^(Y2|Y1): {linear_projection:.6f}")
> print(f"Expectativa Condicional E(Y2|Y1): {conditional_expectation:.6f}")
>
> # Verifica√ß√£o
> print(f"Proje√ß√£o linear e expectativa condicional s√£o iguais: {np.isclose(linear_projection, conditional_expectation)}")
> ```
> Este c√≥digo calcula e imprime os valores da proje√ß√£o linear e da expectativa condicional para $Y_1 = 0.07$, al√©m de confirmar que os valores s√£o, de fato, id√™nticos.

**Teorema 1** (Equival√™ncia da Expectativa Condicional e Proje√ß√£o Linear)
Em processos Gaussianos, a expectativa condicional $E(Y_2|Y_1)$ √© equivalente √† proje√ß√£o linear $\hat{E}(Y_2|Y_1)$, ou seja, $E(Y_2|Y_1) = \hat{E}(Y_2|Y_1)$.
*Proof:*
I.  A expectativa condicional $E(Y_2|Y_1)$ √© dada por:
$$
E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
II. A proje√ß√£o linear $\hat{E}(Y_2|Y_1)$ √© dada por:
$$
\hat{E}(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
III. Como ambas express√µes s√£o id√™nticas, conclu√≠mos que:
$$
E(Y_2|Y_1) = \hat{E}(Y_2|Y_1)
$$
Portanto, em processos Gaussianos, a expectativa condicional e a proje√ß√£o linear coincidem. $\blacksquare$

**Corol√°rio 1.1** (Otimidade da Proje√ß√£o Linear)
Em processos Gaussianos, a proje√ß√£o linear $\hat{E}(Y_2|Y_1)$ fornece a previs√£o √≥tima de $Y_2$ dado $Y_1$ no sentido de minimizar o erro quadr√°tico m√©dio.
*Proof:*
I. Sabemos que a expectativa condicional $E(Y_2|Y_1)$ √© a melhor previs√£o de $Y_2$ dado $Y_1$ no sentido de minimizar o erro quadr√°tico m√©dio, isto √©, $E[(Y_2 - g(Y_1))^2]$, para qualquer fun√ß√£o $g$ de $Y_1$.
II. Pelo Teorema 1, sabemos que $E(Y_2|Y_1) = \hat{E}(Y_2|Y_1)$.
III. Logo, a proje√ß√£o linear $\hat{E}(Y_2|Y_1)$ tamb√©m minimiza o erro quadr√°tico m√©dio, sendo, portanto, a melhor previs√£o de $Y_2$ dado $Y_1$. $\blacksquare$

**Corol√°rio 1.2** (Erro da Proje√ß√£o Linear)
O erro da proje√ß√£o linear, $Y_2 - \hat{E}(Y_2|Y_1)$, √© um vetor Gaussiano com m√©dia zero e matriz de covari√¢ncia $H = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}$.
*Proof:*
I.  O erro da proje√ß√£o linear √© dado por $e = Y_2 - \hat{E}(Y_2|Y_1) = Y_2 - (\mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1 - \mu_1))$.
II. Como $Y_2$ e $Y_1$ s√£o Gaussianos, e $\hat{E}(Y_2|Y_1)$ √© uma combina√ß√£o linear de $Y_1$, o erro $e$ tamb√©m √© um vetor Gaussiano.
III. A m√©dia do erro √© $E[e] = E[Y_2 - \hat{E}(Y_2|Y_1)] = E[Y_2] - E[\hat{E}(Y_2|Y_1)] = \mu_2 - (\mu_2 + \Omega_{21}\Omega_{11}^{-1}(E[Y_1] - \mu_1)) = \mu_2 - (\mu_2 + \Omega_{21}\Omega_{11}^{-1}(\mu_1 - \mu_1)) = \mu_2 - \mu_2 = 0$.
IV. A matriz de covari√¢ncia do erro √© $Cov(e) = Cov(Y_2 - \hat{E}(Y_2|Y_1)) = Cov(Y_2 - (\mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1 - \mu_1))) = Cov(Y_2 - \Omega_{21}\Omega_{11}^{-1}Y_1)$.  Usando propriedades de covari√¢ncia, $Cov(Y_2 - \Omega_{21}\Omega_{11}^{-1}Y_1) = Cov(Y_2) - Cov(Y_2, \Omega_{21}\Omega_{11}^{-1}Y_1) - Cov(\Omega_{21}\Omega_{11}^{-1}Y_1, Y_2) + Cov(\Omega_{21}\Omega_{11}^{-1}Y_1) = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12} + \Omega_{21}\Omega_{11}^{-1}\Omega_{11}\Omega_{11}^{-1}\Omega_{12} = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12} = H$.
V. Portanto, o erro da proje√ß√£o linear √© um vetor Gaussiano com m√©dia zero e matriz de covari√¢ncia $H$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Voltando ao exemplo dos retornos dos ativos, ao usar a proje√ß√£o linear para prever o retorno do ativo 2 dado que o retorno do ativo 1 √© 0.07, obtemos:
> $$
> \hat{E}[Y_2|Y_1 = 0.07] = 0.09 + 0.4(0.07 - 0.06) = 0.094
> $$
> Este valor √© id√™ntico √† expectativa condicional. Isso demonstra que, neste caso, ao usarmos a proje√ß√£o linear, estamos obtendo a melhor previs√£o, que coincide com a expectativa condicional.
>
> Podemos simular o processo e confirmar este resultado.
> ```python
> import numpy as np
>
> # Par√¢metros do exemplo
> mu_1 = 0.06
> mu_2 = 0.09
> omega_11 = 0.0025
> omega_12 = 0.001
> omega_21 = 0.001
> omega_22 = 0.0049
>
> # Valor espec√≠fico para Y1
> y1_value = 0.07
>
> # C√°lculo da m√©dia condicional
> m_cond = mu_2 + (omega_12 / omega_11) * (y1_value - mu_1)
>
> # C√°lculo da proje√ß√£o linear
> projection = mu_2 + (omega_12 / omega_11) * (y1_value - mu_1)
>
> print(f"Expectativa condicional E(Y2|Y1): {m_cond:.6f}")
> print(f"Proje√ß√£o linear E^(Y2|Y1): {projection:.6f}")
>
> # C√°lculo do erro
> error = np.random.multivariate_normal([0, 0], [[omega_11, omega_12], [omega_21, omega_22]], size=10000)
>
> # C√°lculo da covariancia do erro
> H = omega_22 - (omega_21 * omega_12 / omega_11)
> print(f"Covari√¢ncia do erro H:{H:.6f}")
>
> # Calculo do erro amostral
> error_empirical = np.mean((error[:,1] - (mu_2 + (omega_21 / omega_11) * (error[:,0] - mu_1)))**2)
>
> print(f"Erro amostral:{error_empirical:.6f}")
> ```
> Ao executar o c√≥digo, vemos que a esperan√ßa condicional e a proje√ß√£o linear s√£o id√™nticas. O c√≥digo tamb√©m simula amostras de dados gaussianos e calcula o erro da proje√ß√£o linear, verificando empiricamente que a m√©dia √© pr√≥xima de zero e a vari√¢ncia √© igual ao valor de H. Al√©m disso, a propriedade de ortogonalidade do erro em rela√ß√£o a qualquer fun√ß√£o linear de $Y_1$ pode ser usada para derivar o Teorema 1.

**Teorema 2** (Ortogonalidade do Erro)
Em processos Gaussianos, o erro da proje√ß√£o linear, $e = Y_2 - \hat{E}(Y_2|Y_1)$, √© ortogonal a qualquer fun√ß√£o linear de $Y_1$, isto √©, $Cov(e, a + bY_1) = 0$, para quaisquer vetores $a$ e $b$ de dimens√µes apropriadas.
*Proof:*
I.  Vamos mostrar que $Cov(e, Y_1) = 0$, como o erro $e$ tem m√©dia zero, $Cov(e, a + bY_1) = Cov(e, bY_1) = bCov(e, Y_1)$, portanto, ao mostrarmos que $Cov(e, Y_1)=0$, demonstramos o teorema.
II. Sabemos que $e = Y_2 - \mu_2 - \Omega_{21}\Omega_{11}^{-1}(Y_1 - \mu_1)$.
III. Assim, $Cov(e, Y_1) = Cov(Y_2 - \mu_2 - \Omega_{21}\Omega_{11}^{-1}(Y_1 - \mu_1), Y_1) = Cov(Y_2, Y_1) - Cov(\Omega_{21}\Omega_{11}^{-1}Y_1, Y_1) = \Omega_{21} - \Omega_{21}\Omega_{11}^{-1}\Omega_{11} = \Omega_{21} - \Omega_{21} = 0$.
IV. Portanto, o erro √© ortogonal a qualquer fun√ß√£o linear de $Y_1$. $\blacksquare$

### Conclus√£o

Em conclus√£o, este cap√≠tulo estabeleceu uma formula√ß√£o expl√≠cita para a **expectativa condicional** $E(Y_2|Y_1)$ em **processos Gaussianos**, mostrando que ela pode ser expressa em fun√ß√£o dos par√¢metros da distribui√ß√£o conjunta como $E[Y_2|Y_1] = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)$ [^4.6].  Al√©m disso, demonstramos formalmente que esta express√£o coincide exatamente com a **proje√ß√£o linear** $\hat{E}(Y_2|Y_1)$, justificando o uso da proje√ß√£o linear como um m√©todo eficiente e √≥timo para previs√£o em processos Gaussianos. A combina√ß√£o dos resultados da distribui√ß√£o normal e a simplicidade da proje√ß√£o linear torna o modelo gaussiano bastante poderoso e √∫til na pr√°tica da an√°lise de s√©ries temporais.

### Refer√™ncias

[^4.6]: Se√ß√£o 4.6 do texto, incluindo as equa√ß√µes [4.6.1] at√© [4.6.7].
[^4.5]: Se√ß√£o 4.5 do texto, incluindo as equa√ß√µes [4.5.1] at√© [4.5.16].
<!-- END -->
