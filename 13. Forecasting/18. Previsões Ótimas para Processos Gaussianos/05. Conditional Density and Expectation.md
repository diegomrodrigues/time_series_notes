## A Densidade Condicional e a Esperan√ßa Condicional em Processos Gaussianos

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre **previs√µes √≥timas** em **processos Gaussianos**, e como a **proje√ß√£o linear** e a **esperan√ßa condicional** coincidem, este cap√≠tulo detalha o processo de deriva√ß√£o da **densidade condicional** de $Y_2$ dado $Y_1$. Vamos explorar como a divis√£o da **densidade conjunta** pela **densidade marginal** resulta na **densidade condicional**, e como a partir desta, podemos obter a **esperan√ßa condicional** $E(Y_2|Y_1)$ que coincide com a proje√ß√£o linear, demonstrando que ela representa a previs√£o √≥tima para tais processos [^4.6].

### Deriva√ß√£o da Densidade Condicional
Partindo da **densidade conjunta** de $Y_1$ e $Y_2$ (vetores aleat√≥rios gaussianos), que expressa a probabilidade de ocorr√™ncia simult√¢nea de valores desses vetores, podemos derivar a **densidade condicional** de $Y_2$ dado $Y_1$ como [^4.6]:
$$
f_{Y_2|Y_1}(y_2|y_1) = \frac{f_{Y_1,Y_2}(y_1,y_2)}{f_{Y_1}(y_1)}
$$
onde $f_{Y_1,Y_2}(y_1,y_2)$ √© a densidade conjunta de $Y_1$ e $Y_2$, e $f_{Y_1}(y_1)$ √© a densidade marginal de $Y_1$.

A **densidade conjunta** para vari√°veis Gaussianas √© dada por [^4.6]:
$$
f_{Y_1,Y_2}(y_1,y_2) = \frac{1}{(2\pi)^{\frac{n_1+n_2}{2}} |\Omega|^{1/2}} \exp \left\{ -\frac{1}{2} \begin{bmatrix} y_1 - \mu_1 \\ y_2 - \mu_2 \end{bmatrix}' \Omega^{-1} \begin{bmatrix} y_1 - \mu_1 \\ y_2 - \mu_2 \end{bmatrix} \right\}
$$
onde $\Omega$ √© a matriz de covari√¢ncia conjunta:
$$
\Omega = \begin{bmatrix}
  \Omega_{11} & \Omega_{12} \\
  \Omega_{21} & \Omega_{22} \\
\end{bmatrix}
$$
A **densidade marginal** de $Y_1$, por sua vez, √© dada por:
$$
f_{Y_1}(y_1) = \frac{1}{(2\pi)^{\frac{n_1}{2}} |\Omega_{11}|^{1/2}} \exp \left\{ -\frac{1}{2} (y_1 - \mu_1)' \Omega_{11}^{-1} (y_1 - \mu_1) \right\}
$$
Ao realizar a divis√£o, a densidade condicional √© obtida como [^4.6]:
$$
f_{Y_2|Y_1}(y_2|y_1) = \frac{1}{(2\pi)^{n_2/2} |H|^{1/2}} \exp \left\{ -\frac{1}{2} (y_2 - m)'H^{-1}(y_2 - m) \right\}
$$
onde:
 - $m = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(y_1-\mu_1)$ √© a m√©dia condicional de $Y_2$ dado $Y_1$
 - $H = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}$ √© a matriz de covari√¢ncia condicional de $Y_2$ dado $Y_1$

Essa forma da densidade condicional revela que, para processos Gaussianos, a distribui√ß√£o de $Y_2$ dado $Y_1$ tamb√©m √© normal, e que a sua m√©dia √© uma fun√ß√£o linear de $Y_1$.

> üí° **Exemplo Num√©rico:**
> Para ilustrar a deriva√ß√£o da densidade condicional, considere dois vetores Gaussianos, $Y_1$ e $Y_2$, representando o retorno de um ativo em dois per√≠odos diferentes. Suponha que as m√©dias s√£o $\mu_1 = 0.05$ e $\mu_2 = 0.10$ e a matriz de covari√¢ncia √©:
> $$
> \Omega = \begin{bmatrix}
>   0.0025 & 0.001 \\
>   0.001 & 0.0049 \\
> \end{bmatrix}
> $$
> A densidade conjunta √© dada por [^4.6]:
> $$
> f_{Y_1,Y_2}(y_1,y_2) = \frac{1}{2\pi \sqrt{|\Omega|}} \exp \left\{ -\frac{1}{2} \begin{bmatrix} y_1 - 0.05 \\ y_2 - 0.10 \end{bmatrix}' \Omega^{-1} \begin{bmatrix} y_1 - 0.05 \\ y_2 - 0.10 \end{bmatrix} \right\}
> $$
> onde $|\Omega| = 0.00001125$ (calculado no cap√≠tulo anterior).
> A densidade marginal de $Y_1$ √©:
> $$
> f_{Y_1}(y_1) = \frac{1}{\sqrt{2\pi \Omega_{11}}} \exp\left\{-\frac{(y_1 - \mu_1)^2}{2 \Omega_{11}}\right\} = \frac{1}{\sqrt{2\pi (0.0025)}} \exp\left\{-\frac{(y_1 - 0.05)^2}{2 (0.0025)}\right\}
> $$
> Para obter a densidade condicional de $Y_2$ dado $Y_1$, dividimos a densidade conjunta pela marginal:
> $$
> f_{Y_2|Y_1}(y_2|y_1) = \frac{f_{Y_1,Y_2}(y_1,y_2)}{f_{Y_1}(y_1)}
> $$
> Ap√≥s a simplifica√ß√£o, obtemos a densidade condicional:
> $$
> f_{Y_2|Y_1}(y_2|y_1) = \frac{1}{\sqrt{2\pi H}} \exp \left\{ -\frac{1}{2} \frac{(y_2 - m)^2}{H} \right\}
> $$
> onde
> $$
> m = 0.10 + \frac{0.001}{0.0025}(y_1 - 0.05) = 0.10 + 0.4(y_1 - 0.05)
> $$
> e
> $$
> H = 0.0049 - \frac{0.001^2}{0.0025} = 0.0045
> $$
> Esta densidade condicional descreve a distribui√ß√£o de $Y_2$ dado um valor observado $y_1$ de $Y_1$, mostrando como a distribui√ß√£o de $Y_2$ muda com base na informa√ß√£o dispon√≠vel de $Y_1$.
>
> Considere o caso espec√≠fico em que $y_1 = 0.06$. A m√©dia condicional de $Y_2$ passa a ser $m = 0.10 + 0.4(0.06 - 0.05) = 0.104$, e a vari√¢ncia condicional √© $H=0.0045$. A densidade condicional espec√≠fica √© ent√£o
> $$
> f_{Y_2|Y_1=0.06}(y_2|0.06) = \frac{1}{\sqrt{2\pi 0.0045}} \exp \left\{ -\frac{1}{2} \frac{(y_2 - 0.104)^2}{0.0045} \right\}
> $$
> Esta densidade √© uma normal com m√©dia 0.104 e vari√¢ncia 0.0045.

**Lema 1**
A densidade condicional $f_{Y_2|Y_1}(y_2|y_1)$ obtida pela divis√£o da densidade conjunta $f_{Y_1, Y_2}(y_1, y_2)$ pela densidade marginal $f_{Y_1}(y_1)$ √© uma densidade normal com m√©dia $m = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(y_1-\mu_1)$ e vari√¢ncia $H = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}$.
*Proof:*
I. A densidade conjunta para processos Gaussianos tem a forma:
$$
f_{Y_1,Y_2}(y_1,y_2) = C \exp \left\{ -\frac{1}{2} \begin{bmatrix} y_1 - \mu_1 \\ y_2 - \mu_2 \end{bmatrix}' \Omega^{-1} \begin{bmatrix} y_1 - \mu_1 \\ y_2 - \mu_2 \end{bmatrix} \right\}
$$
II. A densidade marginal para processos Gaussianos tem a forma:
$$
f_{Y_1}(y_1) = C_1 \exp \left\{ -\frac{1}{2} (y_1 - \mu_1)' \Omega_{11}^{-1} (y_1 - \mu_1) \right\}
$$
III. A densidade condicional √© a divis√£o da conjunta pela marginal:
$$
f_{Y_2|Y_1}(y_2|y_1) = \frac{f_{Y_1,Y_2}(y_1,y_2)}{f_{Y_1}(y_1)}
$$
IV. Ao realizar a divis√£o, os termos exponenciais se combinam, e, ap√≥s manipula√ß√µes alg√©bricas, resulta em uma densidade da forma:
$$
f_{Y_2|Y_1}(y_2|y_1) = C_2 \exp \left\{ -\frac{1}{2} (y_2 - m)'H^{-1}(y_2 - m) \right\}
$$
onde $m = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(y_1-\mu_1)$ e $H = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}$. A express√£o acima √© a forma de uma distribui√ß√£o normal com m√©dia $m$ e vari√¢ncia $H$. $\blacksquare$

**Lema 1.1** (Propriedades da Matriz de Covari√¢ncia Condicional)
A matriz de covari√¢ncia condicional $H = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}$ √© sempre semidefinida positiva, garantindo que a densidade condicional seja uma fun√ß√£o de densidade v√°lida.
*Proof:*
I. Dado que $\Omega$ √© uma matriz de covari√¢ncia, ela √© sim√©trica e positiva semidefinida. Portanto, podemos decompor $\Omega$ usando a decomposi√ß√£o de Schur, como
$$ \Omega = \begin{bmatrix} \Omega_{11} & \Omega_{12} \\ \Omega_{21} & \Omega_{22} \end{bmatrix} = \begin{bmatrix} I & \Omega_{12}\Omega_{22}^{-1} \\ 0 & I \end{bmatrix} \begin{bmatrix} \Omega_{11} - \Omega_{12} \Omega_{22}^{-1} \Omega_{21} & 0 \\ 0 & \Omega_{22} \end{bmatrix} \begin{bmatrix} I & 0 \\ \Omega_{22}^{-1}\Omega_{21} & I \end{bmatrix} $$
II.  A partir da decomposi√ß√£o, vemos que $\Omega_{22}$ √© tamb√©m positiva semidefinida.  Al√©m disso, a matriz $\Omega_{11} - \Omega_{12} \Omega_{22}^{-1} \Omega_{21}$ √© a matriz de covari√¢ncia marginal da vari√°vel $Y_1|Y_2$.
III. Pela propriedade de matrizes de covari√¢ncia, a matriz $ \Omega_{11} - \Omega_{12} \Omega_{22}^{-1} \Omega_{21}$ √© positiva semidefinida. Analogamente, temos que $\Omega_{22} - \Omega_{21} \Omega_{11}^{-1} \Omega_{12}$ tamb√©m √© positiva semidefinida, que √© a mesma matriz $H$ do enunciado. $\blacksquare$

### A Esperan√ßa Condicional e sua Deriva√ß√£o
Uma vez que a densidade condicional √© obtida, a **esperan√ßa condicional** de $Y_2$ dado $Y_1$, denotada por $E(Y_2|Y_1)$, pode ser obtida diretamente da m√©dia da distribui√ß√£o condicional [^4.6].
$$
E(Y_2|Y_1) = \int y_2 f_{Y_2|Y_1}(y_2|Y_1) \, dy_2 = m = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
Essa express√£o demonstra que a esperan√ßa condicional √© uma fun√ß√£o linear de $Y_1$, o que, como j√° vimos em cap√≠tulos anteriores, √© uma caracter√≠stica fundamental para processos Gaussianos. Essa m√©dia condicional √©, por defini√ß√£o, a previs√£o √≥tima de $Y_2$ dado o conhecimento de $Y_1$.

> üí° **Exemplo Num√©rico:**
> No exemplo anterior, a esperan√ßa condicional √© obtida como:
> $$
> E(Y_2|Y_1) = 0.10 + 0.4(Y_1 - 0.05)
> $$
> Esta fun√ß√£o linear de $Y_1$ representa a melhor previs√£o de $Y_2$, dada a informa√ß√£o de $Y_1$. Se observarmos $Y_1 = 0.06$, a m√©dia condicional de $Y_2$ ser√°:
> $$
> E[Y_2|Y_1 = 0.06] = 0.10 + 0.4(0.06 - 0.05) = 0.10 + 0.004 = 0.104
> $$
> Este valor representa a melhor previs√£o do retorno do ativo no segundo per√≠odo, dado que o retorno no primeiro per√≠odo foi 0.06.
>
> Para simular, vamos criar amostras aleat√≥rias de $Y_2$, dado que $Y_1$ √© 0.06 e calcular a m√©dia dessas amostras:
> ```python
> import numpy as np
>
> # Par√¢metros
> mu_1 = 0.05
> mu_2 = 0.10
> omega_11 = 0.0025
> omega_12 = 0.001
> omega_21 = 0.001
> omega_22 = 0.0049
>
> # Valor de Y1
> y1_value = 0.06
>
> # C√°lculo da m√©dia e vari√¢ncia condicionais
> m = mu_2 + (omega_12 / omega_11) * (y1_value - mu_1)
> H = omega_22 - (omega_12**2 / omega_11)
>
> # Gerando amostras condicionais
> n_samples = 1000
> samples_conditional = np.random.normal(m, np.sqrt(H), size=n_samples)
>
> # Calculando a m√©dia amostral
> sample_mean = np.mean(samples_conditional)
>
> print(f"Esperan√ßa condicional te√≥rica E(Y2|Y1): {m:.6f}")
> print(f"M√©dia amostral da distribui√ß√£o condicional de Y2 dado Y1: {sample_mean:.6f}")
> ```
> A sa√≠da do programa mostrar√° que a m√©dia amostral √© muito pr√≥xima da esperan√ßa condicional, mostrando que a expectativa condicional √© uma m√©dia populacional, que √© bem aproximada pela m√©dia amostral.
>
> Vamos agora visualizar a distribui√ß√£o de amostras condicionais e a m√©dia condicional:
> ```python
> import matplotlib.pyplot as plt
> import seaborn as sns
>
> # Plot da distribui√ß√£o condicional
> sns.histplot(samples_conditional, kde=True, color='skyblue', label='Distribui√ß√£o Condicional de Y2|Y1=0.06')
> plt.axvline(m, color='red', linestyle='dashed', linewidth=1, label=f'M√©dia Condicional: {m:.4f}')
> plt.title('Distribui√ß√£o Condicional de Y2 dado Y1=0.06')
> plt.xlabel('Valor de Y2')
> plt.ylabel('Frequ√™ncia')
> plt.legend()
> plt.show()
> ```
> Este gr√°fico mostrar√° um histograma dos valores simulados de $Y_2$ quando $Y_1=0.06$, juntamente com uma linha vertical vermelha indicando a m√©dia condicional, ilustrando visualmente que a m√©dia amostral se aproxima do valor esperado.

**Teorema 1** (Esperan√ßa Condicional em Processos Gaussianos)
A esperan√ßa condicional $E(Y_2|Y_1)$, obtida a partir da densidade condicional, √© dada por $E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)$, que √© uma fun√ß√£o linear de $Y_1$.
*Proof:*
I. Do Lema 1, a densidade condicional √© dada por
$$
f_{Y_2|Y_1}(y_2|y_1) = C \exp \left\{ -\frac{1}{2} (y_2 - m)'H^{-1}(y_2 - m) \right\}
$$
com $m = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(y_1-\mu_1)$ e $H = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}$.
II. A esperan√ßa condicional $E(Y_2|Y_1)$ √© igual a m√©dia da distribui√ß√£o condicional, ou seja:
$$E(Y_2|Y_1) = m$$
III. Substituindo a express√£o de $m$, obtemos:
$$E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)$$
IV. Logo, a esperan√ßa condicional $E(Y_2|Y_1)$ √© uma fun√ß√£o linear de $Y_1$. $\blacksquare$

### Equival√™ncia com a Proje√ß√£o Linear
Como visto anteriormente, a **proje√ß√£o linear** de $Y_2$ sobre $Y_1$, que consiste na melhor previs√£o linear de $Y_2$ baseada em $Y_1$,  √© dada por [^4.5]:
$$
\hat{E}(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
Comparando esta express√£o com a esperan√ßa condicional, podemos verificar que elas s√£o id√™nticas. Este resultado √© fundamental, pois ele nos diz que, em processos Gaussianos, a proje√ß√£o linear, que √© computacionalmente mais simples, fornece a mesma previs√£o que a esperan√ßa condicional, que √© a previs√£o √≥tima irrestrita [^4.6].

> üí° **Exemplo Num√©rico:**
> Usando o exemplo anterior, a proje√ß√£o linear de $Y_2$ sobre $Y_1$ √© dada por:
> $$
> \hat{E}(Y_2|Y_1) = 0.10 + 0.4(Y_1 - 0.05)
> $$
> Esta √© exatamente a mesma fun√ß√£o linear que encontramos para a esperan√ßa condicional. Isso demonstra, de forma pr√°tica, a equival√™ncia entre a proje√ß√£o linear e a esperan√ßa condicional.
>
> Se considerarmos novamente $Y_1 = 0.06$, a proje√ß√£o linear ser√°:
> $$
> \hat{E}(Y_2|Y_1=0.06) = 0.10 + 0.4(0.06 - 0.05) = 0.104
> $$
> O mesmo valor obtido para a esperan√ßa condicional, refor√ßando a equival√™ncia.

**Teorema 2** (Equival√™ncia em Processos Gaussianos)
Para processos Gaussianos, a esperan√ßa condicional $E(Y_2|Y_1)$ e a proje√ß√£o linear $\hat{E}(Y_2|Y_1)$ s√£o id√™nticas, e ambas s√£o a melhor previs√£o de $Y_2$ dado $Y_1$.
*Proof:*
I. Do Teorema 1 e da dedu√ß√£o anterior, temos:
$$
E(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
II. A proje√ß√£o linear √© dada por:
$$
\hat{E}(Y_2|Y_1) = \mu_2 + \Omega_{21}\Omega_{11}^{-1}(Y_1-\mu_1)
$$
III. Comparando as duas express√µes, conclu√≠mos que:
$$
E(Y_2|Y_1) = \hat{E}(Y_2|Y_1)
$$
Portanto, em processos Gaussianos, a esperan√ßa condicional e a proje√ß√£o linear coincidem, e como a esperan√ßa condicional √© a melhor previs√£o, a proje√ß√£o linear tamb√©m √© a melhor previs√£o. $\blacksquare$

**Teorema 2.1** (Propriedade da Melhor Previs√£o)
Em processos Gaussianos, a proje√ß√£o linear $\hat{E}(Y_2|Y_1)$, que coincide com a esperan√ßa condicional $E(Y_2|Y_1)$, √© a melhor previs√£o no sentido de minimizar o erro quadr√°tico m√©dio, ou seja, $\hat{E}(Y_2|Y_1) = \arg \min_{g(Y_1)} E[(Y_2-g(Y_1))^2]$, em que $g$ √© qualquer fun√ß√£o de $Y_1$.
*Proof:*
I.  Pela defini√ß√£o da esperan√ßa condicional, temos que $E(Y_2|Y_1)$ √© a melhor previs√£o de $Y_2$ dado $Y_1$ no sentido de minimizar o erro quadr√°tico m√©dio $E[(Y_2 - g(Y_1))^2]$, em que $g$ √© qualquer fun√ß√£o de $Y_1$.
II. Pelo Teorema 2, sabemos que $\hat{E}(Y_2|Y_1) = E(Y_2|Y_1)$ para processos gaussianos.
III. Portanto, $\hat{E}(Y_2|Y_1)$ tamb√©m minimiza o erro quadr√°tico m√©dio $E[(Y_2 - g(Y_1))^2]$, sendo, portanto, a melhor previs√£o no sentido do erro quadr√°tico m√©dio. $\blacksquare$

### Conclus√£o
A an√°lise detalhada da **densidade conjunta** e sua utiliza√ß√£o para derivar a **densidade condicional** demonstrou como obter a **esperan√ßa condicional** $E(Y_2|Y_1)$ para **processos Gaussianos** [^4.6]. Este processo revela a fundamental caracter√≠stica de que a esperan√ßa condicional √© uma fun√ß√£o linear das vari√°veis condicionantes, e que ela coincide com a **proje√ß√£o linear**.  A deriva√ß√£o detalhada, e os exemplos num√©ricos, mostram que essa rela√ß√£o √© central na teoria da previs√£o, pois permite que as proje√ß√µes lineares sejam usadas como m√©todo computacionalmente simples para obter a melhor previs√£o em processos Gaussianos, combinando conceitos avan√ßados de probabilidade, estat√≠stica e √°lgebra linear.

### Refer√™ncias
[^4.6]: Se√ß√£o 4.6 do texto, incluindo as equa√ß√µes [4.6.1] at√© [4.6.7].
[^4.5]: Se√ß√£o 4.5 do texto, incluindo as equa√ß√µes [4.5.1] at√© [4.5.16].
<!-- END -->
