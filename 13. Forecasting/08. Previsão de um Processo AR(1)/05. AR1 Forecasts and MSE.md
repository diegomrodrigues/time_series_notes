## Previs√£o √ìtima e MSE para Processos AR(1): Uma An√°lise Detalhada
### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise da previs√£o para um processo Autorregressivo de primeira ordem (AR(1)), com √™nfase no comportamento da previs√£o √≥tima linear de $s$ per√≠odos √† frente e no seu erro quadr√°tico m√©dio (MSE). Construindo sobre os resultados estabelecidos nos cap√≠tulos anteriores, exploraremos como a previs√£o decai geometricamente em dire√ß√£o √† m√©dia do processo √† medida que o horizonte de previs√£o ($s$) aumenta. Adicionalmente, vamos derivar a express√£o do MSE e analisar o seu comportamento em fun√ß√£o de $s$. Nosso objetivo √© apresentar uma vis√£o abrangente das propriedades de previs√£o de modelos AR(1) para um p√∫blico com conhecimento avan√ßado em matem√°tica, modelos estat√≠sticos, otimiza√ß√£o e an√°lise de dados. [^1] [^2] [^3]

### Conceitos Fundamentais e Revis√£o
Conforme definido anteriormente, um processo AR(1) estacion√°rio √© descrito pela equa√ß√£o:
$$
(1 - \phi L)(Y_t - \mu) = \epsilon_t,
$$
onde $Y_t$ √© o valor da s√©rie temporal no instante $t$, $\mu$ √© a m√©dia do processo, $\phi$ √© o coeficiente autorregressivo, e $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$. A representa√ß√£o MA(‚àû) para esse processo √©:
$$
Y_t - \mu = \sum_{j=0}^{\infty} \phi^j \epsilon_{t-j}.
$$
A previs√£o √≥tima de $s$ per√≠odos √† frente √© dada pela f√≥rmula de Wiener-Kolmogorov:
$$
\hat{Y}_{t+s|t} = \mu + \left[\frac{\psi(L)}{L^s}\right]_+ (Y_t - \mu),
$$
onde $\psi(L) = \frac{1}{1 - \phi L}$ e  $\left[\frac{\psi(L)}{L^s}\right]_+$ representa a parte do polin√≥mio com pot√™ncias n√£o negativas de $L$. Para um processo AR(1), a parte do polin√≥mio com pot√™ncias n√£o negativas simplifica-se para $\frac{\phi^s}{1-\phi L}$.
Portanto, a previs√£o √≥tima para $s$ passos √† frente √© dada por [^4] [^5]:
$$
\hat{Y}_{t+s|t} = \mu + \phi^s (Y_t - \mu).
$$
Podemos notar que esta previs√£o linear decai geometricamente em dire√ß√£o √† m√©dia $\mu$ √† medida que $s$ aumenta, com o fator de decaimento sendo $\phi^s$.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um processo AR(1) com $\mu = 5$, $\phi = 0.7$, e $Y_t = 12$. Vamos calcular as previs√µes para diferentes horizontes.
>
> Para $s=1$:
> $$
> \hat{Y}_{t+1|t} = 5 + 0.7^1 (12 - 5) = 5 + 0.7(7) = 9.9
> $$
> Para $s=2$:
> $$
> \hat{Y}_{t+2|t} = 5 + 0.7^2 (12 - 5) = 5 + 0.49(7) = 8.43
> $$
> Para $s=5$:
> $$
> \hat{Y}_{t+5|t} = 5 + 0.7^5 (12 - 5) = 5 + 0.16807(7) = 6.17649
> $$
> Observe como a previs√£o se aproxima de $\mu = 5$ √† medida que $s$ aumenta. O fator de decaimento √© $\phi^s = 0.7^s$, que diminui rapidamente.

Expandindo a f√≥rmula anterior, obtemos:
$$
\hat{Y}_{t+s|t} = \mu + \phi^s (Y_t - \mu)
$$
No caso de uma previs√£o de um passo √† frente, ou seja, com s=1, teremos:
$$
\hat{Y}_{t+1|t} = \mu + \phi (Y_t - \mu)
$$
que pode ser expressa de forma recursiva como:
$$
\hat{Y}_{t+s|t} = \mu + \phi (\hat{Y}_{t+s-1|t} - \mu)
$$
Esta express√£o mostra que a previs√£o para $s$ passos √† frente pode ser calculada recursivamente usando a previs√£o do passo anterior.
**Proposi√ß√£o 0.1:**  A previs√£o √≥tima para s passos √† frente pode ser reescrita em termos de inova√ß√µes como:
$$\hat{Y}_{t+s|t} = \mu + \phi^s \sum_{j=0}^{\infty} \phi^j \epsilon_{t-j}.$$
##### Demonstra√ß√£o
Substituindo a representa√ß√£o MA(‚àû) de $Y_t$ na express√£o da previs√£o √≥tima:
$$
\hat{Y}_{t+s|t} = \mu + \phi^s (Y_t - \mu) = \mu + \phi^s \left( \sum_{j=0}^{\infty} \phi^j \epsilon_{t-j} \right).
$$
O resultado segue.
$\blacksquare$

### An√°lise do Erro Quadr√°tico M√©dio (MSE)
O erro quadr√°tico m√©dio (MSE) da previs√£o √© definido como:
$$
MSE(\hat{Y}_{t+s|t}) = E[(Y_{t+s} - \hat{Y}_{t+s|t})^2].
$$
Dado que as previs√µes lineares s√£o n√£o viesadas, o MSE √© igual √† vari√¢ncia do erro de previs√£o, ou seja, $MSE(\hat{Y}_{t+s|t}) = Var(Y_{t+s} - \hat{Y}_{t+s|t})$. Do cap√≠tulo anterior, sabemos que a vari√¢ncia do erro de previs√£o para um processo AR(1) √© dada por [^6] [^7]:
$$
Var(Y_{t+s} - \hat{Y}_{t+s|t}) = \sigma^2 \sum_{j=0}^{s-1} \phi^{2j},
$$
Este resultado pode tamb√©m ser expresso em forma fechada como:
$$
Var(Y_{t+s} - \hat{Y}_{t+s|t}) = \sigma^2 \frac{1 - \phi^{2s}}{1 - \phi^2}.
$$
Por conseguinte, o MSE da previs√£o de $s$ passos √† frente √©:
$$
MSE(\hat{Y}_{t+s|t}) = \sigma^2 \frac{1 - \phi^{2s}}{1 - \phi^2}.
$$
Esta express√£o mostra que o MSE aumenta com $s$. Quando $s=1$, o MSE ser√°:
$$
MSE(\hat{Y}_{t+1|t}) = \sigma^2
$$
e, √† medida que $s$ tende para o infinito, o MSE converge para:
$$
\lim_{s \to \infty} MSE(\hat{Y}_{t+s|t}) = \frac{\sigma^2}{1 - \phi^2}
$$
que √© a vari√¢ncia incondicional do processo AR(1) e que representa a incerteza inerente √† previs√£o a longo prazo.

> üí° **Exemplo Num√©rico:**
>
> Considere o exemplo anterior com $\phi = 0.7$ e $\sigma^2 = 2$. Calculemos o MSE para diferentes valores de $s$.
>
> Para $s=1$:
> $$
> MSE(\hat{Y}_{t+1|t}) = 2
> $$
> Para $s=2$:
> $$
> MSE(\hat{Y}_{t+2|t}) = 2 \cdot \frac{1 - 0.7^{2 \cdot 2}}{1 - 0.7^2} = 2 \cdot \frac{1 - 0.2401}{1 - 0.49} \approx 2 \cdot \frac{0.7599}{0.51} \approx 2.98
> $$
> Para $s=5$:
> $$
> MSE(\hat{Y}_{t+5|t}) = 2 \cdot \frac{1 - 0.7^{2 \cdot 5}}{1 - 0.7^2} = 2 \cdot \frac{1 - 0.02824}{0.51} \approx 2 \cdot \frac{0.97176}{0.51} \approx 3.81
> $$
> Para $s \to \infty$:
> $$
> \lim_{s \to \infty} MSE(\hat{Y}_{t+s|t}) = \frac{2}{1 - 0.7^2} = \frac{2}{0.51} \approx 3.92
> $$
>
> Observa-se que o MSE aumenta com $s$ e se aproxima da vari√¢ncia incondicional, que √© aproximadamente 3.92 neste caso.

**Lema 1:** O erro de previs√£o para s passos √† frente pode ser expressa como:
$$
Y_{t+s} - \hat{Y}_{t+s|t} = \sum_{j=0}^{s-1} \phi^j \epsilon_{t+s-j}.
$$
##### Demonstra√ß√£o
I. Usando a representa√ß√£o MA(‚àû) para $Y_{t+s}$, temos:
$$
Y_{t+s} = \mu + \sum_{j=0}^{\infty} \phi^j \epsilon_{t+s-j}
$$
II. Substituindo a express√£o da previs√£o √≥tima
$$
Y_{t+s} - \hat{Y}_{t+s|t} = \mu + \sum_{j=0}^{\infty} \phi^j \epsilon_{t+s-j} - (\mu + \phi^s (Y_t - \mu))
$$
III. Substituindo $Y_t - \mu$ pela sua representa√ß√£o MA(‚àû):
$$
Y_{t+s} - \hat{Y}_{t+s|t} = \mu + \sum_{j=0}^{\infty} \phi^j \epsilon_{t+s-j} - \left( \mu + \phi^s \sum_{j=0}^{\infty} \phi^j \epsilon_{t-j} \right)
$$
IV. Simplificando:
$$
Y_{t+s} - \hat{Y}_{t+s|t} = \sum_{j=0}^{\infty} \phi^j \epsilon_{t+s-j} - \phi^s \sum_{j=0}^{\infty} \phi^j \epsilon_{t-j}
$$
V. Reescrevendo a primeira somat√≥ria como a soma de dois componentes:
$$
\sum_{j=0}^{\infty} \phi^j \epsilon_{t+s-j} = \sum_{j=0}^{s-1} \phi^j \epsilon_{t+s-j} + \sum_{j=s}^{\infty} \phi^j \epsilon_{t+s-j}
$$
VI.  Reindexando a segunda somat√≥ria com $k = j-s$
$$
\sum_{j=s}^{\infty} \phi^j \epsilon_{t+s-j} = \sum_{k=0}^{\infty} \phi^{k+s} \epsilon_{t-k} = \phi^s \sum_{k=0}^{\infty} \phi^k \epsilon_{t-k}
$$
VII. Substituindo na express√£o anterior:
$$
Y_{t+s} - \hat{Y}_{t+s|t} = \sum_{j=0}^{s-1} \phi^j \epsilon_{t+s-j} + \phi^s \sum_{k=0}^{\infty} \phi^k \epsilon_{t-k} - \phi^s \sum_{j=0}^{\infty} \phi^j \epsilon_{t-j}
$$
VIII. Como  $\sum_{k=0}^{\infty} \phi^k \epsilon_{t-k} = \sum_{j=0}^{\infty} \phi^j \epsilon_{t-j}$:
$$
Y_{t+s} - \hat{Y}_{t+s|t} = \sum_{j=0}^{s-1} \phi^j \epsilon_{t+s-j}
$$
$\blacksquare$

#### Proposi√ß√£o 1: Decaimento Geom√©trico da Previs√£o
A previs√£o √≥tima de $s$ per√≠odos √† frente, $\hat{Y}_{t+s|t}$, decai geometricamente para a m√©dia do processo, $\mu$, com taxa de decaimento $\phi^s$.

##### Demonstra√ß√£o
I. A previs√£o √≥tima de $s$ per√≠odos √† frente √© dada por
$$
\hat{Y}_{t+s|t} = \mu + \phi^s(Y_t - \mu).
$$
II. Reorganizando a equa√ß√£o, podemos escrever a previs√£o como
$$
\hat{Y}_{t+s|t} - \mu = \phi^s(Y_t - \mu).
$$
III. Esta express√£o mostra que a diferen√ßa entre a previs√£o e a m√©dia do processo, $\hat{Y}_{t+s|t} - \mu$, √© uma fra√ß√£o $\phi^s$ da diferen√ßa entre o valor atual e a m√©dia, $Y_t - \mu$.
IV. Dado que para um processo AR(1) estacion√°rio, $|\phi| < 1$, temos que $\lim_{s \to \infty} \phi^s = 0$.
V. Portanto, quando $s$ tende ao infinito, $\hat{Y}_{t+s|t}$ converge para $\mu$. A taxa de converg√™ncia √© controlada pelo fator $\phi^s$, o que implica um decaimento geom√©trico em dire√ß√£o √† m√©dia do processo.
$\blacksquare$

#### Proposi√ß√£o 2: MSE e Horizonte de Previs√£o
O erro quadr√°tico m√©dio (MSE) aumenta com o horizonte de previs√£o $s$ e converge para a vari√¢ncia incondicional do processo $\frac{\sigma^2}{1-\phi^2}$ quando $s \to \infty$.

##### Demonstra√ß√£o
I. Come√ßamos com a express√£o do MSE:
$$
MSE(\hat{Y}_{t+s|t}) = \sigma^2 \frac{1 - \phi^{2s}}{1 - \phi^2}
$$
II. Para demonstrar que o MSE aumenta com $s$, vamos analisar a diferen√ßa entre o MSE com horizonte $s+1$ e $s$:
$$
MSE(\hat{Y}_{t+s+1|t}) - MSE(\hat{Y}_{t+s|t}) = \sigma^2 \frac{1 - \phi^{2(s+1)}}{1 - \phi^2} -  \sigma^2 \frac{1 - \phi^{2s}}{1 - \phi^2}
$$
III. Simplificando:
$$
MSE(\hat{Y}_{t+s+1|t}) - MSE(\hat{Y}_{t+s|t}) = \frac{\sigma^2}{1 - \phi^2} \left(1 - \phi^{2s+2} - (1 - \phi^{2s})\right) = \frac{\sigma^2}{1 - \phi^2} (\phi^{2s} - \phi^{2s+2})
$$
IV. Fatorando:
$$
MSE(\hat{Y}_{t+s+1|t}) - MSE(\hat{Y}_{t+s|t}) =  \frac{\sigma^2}{1 - \phi^2} \phi^{2s} (1-\phi^2) = \sigma^2\phi^{2s}
$$
V. Dado que $\phi^{2s}$ √© sempre positivo, e  $\sigma^2$ √© a vari√¢ncia do ru√≠do branco, que tamb√©m √© positiva, o MSE aumenta com $s$.
VI. Para analisar a converg√™ncia, tomamos o limite quando $s \to \infty$:
$$
\lim_{s \to \infty} MSE(\hat{Y}_{t+s|t}) = \lim_{s \to \infty} \sigma^2 \frac{1 - \phi^{2s}}{1 - \phi^2}.
$$
VII. Como $|\phi| < 1$, ent√£o $\lim_{s \to \infty} \phi^{2s} = 0$. Portanto:
$$
\lim_{s \to \infty} MSE(\hat{Y}_{t+s|t}) = \frac{\sigma^2}{1 - \phi^2}.
$$
VIII. Este resultado mostra que, √† medida que o horizonte de previs√£o aumenta, o MSE converge para a vari√¢ncia incondicional do processo AR(1).
$\blacksquare$

**Teorema 2.1:** O MSE para um horizonte de previs√£o $s$ pode ser expresso como a soma do MSE do passo anterior e da vari√¢ncia da inova√ß√£o no instante $t+s$:
$$
MSE(\hat{Y}_{t+s|t}) = MSE(\hat{Y}_{t+s-1|t}) + \sigma^2\phi^{2(s-1)}
$$
##### Demonstra√ß√£o
I. A partir da proposi√ß√£o 2, sabemos que
$$
MSE(\hat{Y}_{t+s|t}) = \sigma^2 \frac{1 - \phi^{2s}}{1 - \phi^2}
$$
II. e tamb√©m que
$$
MSE(\hat{Y}_{t+s-1|t}) = \sigma^2 \frac{1 - \phi^{2(s-1)}}{1 - \phi^2}
$$
III. Ent√£o, usando as express√µes acima, temos:
$$
MSE(\hat{Y}_{t+s|t}) - MSE(\hat{Y}_{t+s-1|t}) = \sigma^2 \frac{1 - \phi^{2s}}{1 - \phi^2} - \sigma^2 \frac{1 - \phi^{2(s-1)}}{1 - \phi^2}
$$
IV. Simplificando:
$$
MSE(\hat{Y}_{t+s|t}) - MSE(\hat{Y}_{t+s-1|t}) = \sigma^2 \frac{1 - \phi^{2s} - (1 - \phi^{2(s-1)})}{1 - \phi^2} = \sigma^2 \frac{\phi^{2(s-1)} - \phi^{2s}}{1 - \phi^2}
$$
V. Fatorando:
$$
MSE(\hat{Y}_{t+s|t}) - MSE(\hat{Y}_{t+s-1|t}) = \sigma^2 \frac{\phi^{2(s-1)} (1 - \phi^2)}{1 - \phi^2} = \sigma^2\phi^{2(s-1)}
$$
VI. Portanto:
$$
MSE(\hat{Y}_{t+s|t}) = MSE(\hat{Y}_{t+s-1|t}) + \sigma^2\phi^{2(s-1)}
$$
$\blacksquare$

#### Corol√°rio 1
O MSE do estimador da m√©dia √© sempre menor do que ou igual √† vari√¢ncia incondicional de Y.

##### Demonstra√ß√£o
I. Relembrando o MSE do estimador de s passos a frente:
$$
MSE(\hat{Y}_{t+s|t}) = \frac{\sigma^2}{1-\phi^2}(1-\phi^{2s})
$$
II. Temos, pelo teorema anterior, que
$$
\lim_{s \to \infty} MSE(\hat{Y}_{t+s|t}) = \frac{\sigma^2}{1 - \phi^2}
$$
III. Sabemos tamb√©m que o MSE aumenta monotonamente com $s$.
IV. Portanto, o MSE para qualquer $s$ √© menor ou igual do que a vari√¢ncia incondicional, $\frac{\sigma^2}{1 - \phi^2}$, e o resultado segue.
$\blacksquare$

### Implica√ß√µes e Discuss√£o
Os resultados acima t√™m implica√ß√µes importantes na pr√°tica da previs√£o de s√©ries temporais utilizando modelos AR(1). O decaimento geom√©trico da previs√£o para a m√©dia do processo demonstra que previs√µes de longo prazo perdem gradualmente a depend√™ncia do valor atual e tendem a convergir para a m√©dia do processo. O MSE, que representa a incerteza da previs√£o, aumenta com o horizonte de previs√£o, mas n√£o cresce indefinidamente. Ele converge para a vari√¢ncia incondicional do processo, o que indica um limite para a incerteza da previs√£o a longo prazo.

> üí° **Exemplo Num√©rico:**
> Considere um processo AR(1) com $\phi = 0.8$, $\mu = 10$ e $\sigma^2 = 1$.
>
> Para uma previs√£o de um passo √† frente ($s=1$), temos:
> $$
> \hat{Y}_{t+1|t} = 10 + 0.8(Y_t - 10)
> $$
> $$
> MSE(\hat{Y}_{t+1|t}) = 1
> $$
> Para uma previs√£o de dois passos √† frente ($s=2$):
> $$
> \hat{Y}_{t+2|t} = 10 + 0.8^2(Y_t - 10) = 10 + 0.64(Y_t - 10)
> $$
> $$
> MSE(\hat{Y}_{t+2|t}) = 1 \cdot \frac{1 - 0.8^{2 \cdot 2}}{1 - 0.8^2} = 1.64
> $$
> Para uma previs√£o de longo prazo ($s \to \infty$):
> $$
> \hat{Y}_{t+s|t} = 10
> $$
> $$
> MSE(\hat{Y}_{t+s|t}) = \frac{1}{1 - 0.8^2} \approx 2.78
> $$
>
> Este exemplo ilustra como a previs√£o converge para a m√©dia do processo (10) e o MSE aumenta com o horizonte da previs√£o e se aproxima da vari√¢ncia incondicional (2.78).
>
> Vamos analisar o decaimento geom√©trico mais detalhadamente. Suponha que $Y_t = 20$.
>
> Para $s=1$:
>  $$
> \hat{Y}_{t+1|t} = 10 + 0.8(20 - 10) = 18
> $$
> Para $s=2$:
> $$
>  \hat{Y}_{t+2|t} = 10 + 0.8^2(20 - 10) = 16.4
> $$
> Para $s=3$:
>  $$
> \hat{Y}_{t+3|t} = 10 + 0.8^3(20 - 10) = 15.12
> $$
>
> Observe que a diferen√ßa entre a previs√£o e a m√©dia ($\hat{Y}_{t+s|t} - \mu$) est√° sendo reduzida geometricamente pelo fator $\phi^s = 0.8^s$. A previs√£o se aproxima rapidamente da m√©dia de 10, confirmando o decaimento geom√©trico.

### Conclus√£o
Neste cap√≠tulo, analisamos em detalhe as propriedades de previs√£o de processos AR(1). Derivamos a forma da previs√£o √≥tima de $s$ per√≠odos √† frente e demonstramos como essa previs√£o decai geometricamente em dire√ß√£o √† m√©dia do processo. Al√©m disso, exploramos a express√£o para o erro quadr√°tico m√©dio (MSE) e demonstramos que ele aumenta com o horizonte de previs√£o e converge para a vari√¢ncia incondicional do processo quando o horizonte de previs√£o tende para o infinito.
Os resultados apresentados fornecem uma vis√£o abrangente do comportamento da previs√£o em modelos AR(1), oferecendo um conjunto robusto de ferramentas para an√°lise de s√©ries temporais e para fundamentar a tomada de decis√µes em cen√°rios onde previs√µes s√£o necess√°rias. A compreens√£o da converg√™ncia da previs√£o e do MSE √© crucial para a interpreta√ß√£o de resultados e para a avalia√ß√£o da confiabilidade das previs√µes no longo prazo.

### Refer√™ncias
[^1]: Express√£o [4.1.1], p√°g 73
[^2]: Express√£o [4.1.2], p√°g 73
[^3]: Express√£o [4.1.9], p√°g 74
[^4]: Express√£o [4.2.17], p√°g 80
[^5]: Express√£o [4.2.16], p√°g 80
[^6]: Express√£o [4.2.18], p√°g 80
[^7]: Express√£o [4.2.19], p√°g 80
<!-- END -->
