## A Previs√£o Linear para um Processo AR(1) e a Converg√™ncia do MSE
### Introdu√ß√£o
Este cap√≠tulo, em continuidade √† nossa discuss√£o sobre a previs√£o de processos AR(1), explora o comportamento do erro quadr√°tico m√©dio (MSE) √† medida que o horizonte de previs√£o aumenta. No cap√≠tulo anterior, derivamos a f√≥rmula de previs√£o para processos AR(1) e demonstramos a converg√™ncia da previs√£o para a m√©dia do processo. Agora, examinaremos como a vari√¢ncia do erro de previs√£o se comporta e converge para a vari√¢ncia incondicional de Y quando o horizonte da previs√£o tende ao infinito. Utilizaremos os resultados apresentados anteriormente para desenvolver uma compreens√£o mais profunda das propriedades de longo prazo das previs√µes lineares para processos AR(1). [^1] [^2] [^3]

### Conceitos Fundamentais
Como vimos anteriormente, um processo AR(1) √© definido como:
$$
(1 - \phi L)(Y_t - \mu) = \epsilon_t,
$$
onde $Y_t$ √© o valor da s√©rie temporal no instante $t$, $\mu$ √© a m√©dia do processo, $\phi$ √© o coeficiente autorregressivo e $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$. [^4] A representa√ß√£o MA($\infty$) deste processo √© dada por:
$$
Y_t - \mu = \sum_{j=0}^{\infty} \phi^j \epsilon_{t-j}.
$$
A previs√£o √≥tima para *s* passos √† frente, derivada usando a f√≥rmula de previs√£o Wiener-Kolmogorov, √©:
$$
\hat{Y}_{t+s|t} = \mu + \phi^s(Y_t - \mu),
$$
e a vari√¢ncia do erro de previs√£o √©:
$$
Var(Y_{t+s} - \hat{Y}_{t+s|t}) = \sigma^2 \sum_{j=0}^{s-1} \phi^{2j}.
$$
Estas express√µes foram desenvolvidas no cap√≠tulo anterior e s√£o a base para a an√°lise que se segue. [^5] [^6] [^7]

#### Teorema 1 (Revis√£o)
Relembrando o Teorema 1 do cap√≠tulo anterior, quando $s \to \infty$, a previs√£o $\hat{Y}_{t+s|t}$ converge para a m√©dia do processo $\mu$, e a vari√¢ncia do erro de previs√£o, $Var(Y_{t+s} - \hat{Y}_{t+s|t})$, converge para a vari√¢ncia incondicional do processo, $\frac{\sigma^2}{1-\phi^2}$, dado que $|\phi|<1$.

> üí° **Converg√™ncia do MSE:**
> O teorema anterior nos mostra que, para um processo AR(1) estacion√°rio (onde $|\phi| < 1$), conforme aumentamos o horizonte da previs√£o, nosso "melhor palpite" para o valor futuro ($Y_{t+s}$) se aproxima da m√©dia do processo ($\mu$). Al√©m disso, a incerteza dessa previs√£o, medida pela vari√¢ncia do erro, n√£o cresce indefinidamente. Em vez disso, ela converge para um valor fixo, que √© a vari√¢ncia incondicional do processo.
>
> A intui√ß√£o por tr√°s disso √© que, quanto mais nos afastamos no futuro, mais a depend√™ncia do valor de hoje (Yt) diminui, e o processo se torna mais "m√©dio". A vari√¢ncia do erro, que representa a incerteza, tende √† vari√¢ncia natural desse processo quando se olha para um per√≠odo muito longo.

#### Lema 1
O erro quadr√°tico m√©dio (MSE) da previs√£o para *s* passos √† frente √© dado pela vari√¢ncia do erro de previs√£o:
$$
MSE(\hat{Y}_{t+s|t}) = E[(Y_{t+s} - \hat{Y}_{t+s|t})^2] = Var(Y_{t+s} - \hat{Y}_{t+s|t}) =  \sigma^2 \sum_{j=0}^{s-1} \phi^{2j}.
$$

##### Demonstra√ß√£o
Para um estimador n√£o viesado (como √© o caso da previs√£o linear), o MSE √© igual √† vari√¢ncia do erro de previs√£o. Como vimos no cap√≠tulo anterior, o erro de previs√£o √© dado por
$$
Y_{t+s} - \hat{Y}_{t+s|t} = \sum_{j=0}^{s-1} \phi^j \epsilon_{t+s-j}
$$
Como os ru√≠dos brancos $\epsilon_t$ t√™m m√©dia zero, a m√©dia do erro de previs√£o √© zero. Portanto, o MSE do estimador √©:
$$
MSE(\hat{Y}_{t+s|t}) = E[(Y_{t+s} - \hat{Y}_{t+s|t})^2] =  E \left[  \left( \sum_{j=0}^{s-1} \phi^j \epsilon_{t+s-j} \right)^2 \right]
$$
Dado que os ru√≠dos brancos t√™m vari√¢ncia $\sigma^2$ e s√£o independentes, podemos calcular o MSE como
$$
MSE(\hat{Y}_{t+s|t}) =  \sum_{j=0}^{s-1} \phi^{2j} E[\epsilon_{t+s-j}^2] = \sigma^2 \sum_{j=0}^{s-1} \phi^{2j}
$$
O que completa a demonstra√ß√£o do lema.
$\blacksquare$

> üí° **Exemplo Num√©rico:**
> Vamos considerar um processo AR(1) com $\phi = 0.8$ e $\sigma^2 = 1$. Isso significa que cada valor na s√©rie temporal depende do valor anterior, com uma correla√ß√£o de 0.8, mais um choque aleat√≥rio com vari√¢ncia 1. Vamos calcular o MSE para previs√µes de 1, 2 e 3 passos √† frente:
>
> Para $s = 1$:
> $$
> MSE(\hat{Y}_{t+1|t}) = \sigma^2 \sum_{j=0}^{0} \phi^{2j} = 1 \cdot (0.8^{2\cdot0}) = 1 \cdot 1 = 1
> $$
> Para $s = 2$:
> $$
> MSE(\hat{Y}_{t+2|t}) = \sigma^2 \sum_{j=0}^{1} \phi^{2j} = 1 \cdot (0.8^{2\cdot0} + 0.8^{2\cdot1}) = 1 \cdot (1 + 0.64) = 1.64
> $$
> Para $s = 3$:
> $$
> MSE(\hat{Y}_{t+3|t}) = \sigma^2 \sum_{j=0}^{2} \phi^{2j} = 1 \cdot (0.8^{2\cdot0} + 0.8^{2\cdot1} + 0.8^{2\cdot2}) = 1 \cdot (1 + 0.64 + 0.4096) = 2.0496
> $$
>
> Como podemos ver, o MSE aumenta √† medida que o horizonte de previs√£o aumenta. Isso significa que nossa incerteza sobre o valor futuro do processo aumenta quanto mais para frente tentamos prever.

**Lema 1.1**
A vari√¢ncia do erro de previs√£o para um processo AR(1) pode ser expressa de forma fechada como:
$$
Var(Y_{t+s} - \hat{Y}_{t+s|t}) = \sigma^2 \frac{1-\phi^{2s}}{1-\phi^2}
$$

**Demonstra√ß√£o**
I. A vari√¢ncia do erro de previs√£o √© dada por:
$$
Var(Y_{t+s} - \hat{Y}_{t+s|t}) = \sigma^2 \sum_{j=0}^{s-1} \phi^{2j}
$$
II. A soma $\sum_{j=0}^{s-1} \phi^{2j}$ √© uma soma parcial de uma s√©rie geom√©trica com raz√£o $\phi^2$.
III.  A f√≥rmula para a soma de uma s√©rie geom√©trica parcial √© $\frac{1-r^n}{1-r}$ onde $r$ √© a raz√£o e $n$ √© o n√∫mero de termos.
IV. Neste caso, $r = \phi^2$ e $n = s$. Substituindo estes valores na f√≥rmula, obtemos:
$$
\sum_{j=0}^{s-1} \phi^{2j} = \frac{1-(\phi^2)^s}{1-\phi^2} = \frac{1-\phi^{2s}}{1-\phi^2}.
$$
V. Portanto, a vari√¢ncia do erro de previs√£o pode ser reescrita como:
$$
Var(Y_{t+s} - \hat{Y}_{t+s|t}) = \sigma^2 \frac{1-\phi^{2s}}{1-\phi^2}
$$
$\blacksquare$

> üí° **Exemplo Num√©rico:**
> Usando o mesmo exemplo do Lema 1.1 com $\phi = 0.8$ e $\sigma^2 = 1$, vamos calcular a vari√¢ncia do erro de previs√£o usando a forma fechada:
>
> Para $s = 1$:
> $$
> Var(Y_{t+1} - \hat{Y}_{t+1|t}) = 1 \cdot \frac{1 - 0.8^{2 \cdot 1}}{1 - 0.8^2} = \frac{1 - 0.64}{1 - 0.64} = \frac{0.36}{0.36} = 1
> $$
> Para $s = 2$:
> $$
> Var(Y_{t+2} - \hat{Y}_{t+2|t}) = 1 \cdot \frac{1 - 0.8^{2 \cdot 2}}{1 - 0.8^2} = \frac{1 - 0.4096}{1 - 0.64} = \frac{0.5904}{0.36} \approx 1.64
> $$
> Para $s = 3$:
> $$
> Var(Y_{t+3} - \hat{Y}_{t+3|t}) = 1 \cdot \frac{1 - 0.8^{2 \cdot 3}}{1 - 0.8^2} = \frac{1 - 0.262144}{1 - 0.64} = \frac{0.737856}{0.36} \approx 2.0496
> $$
>
> Esses resultados correspondem aos calculados diretamente pela soma, demonstrando a equival√™ncia das duas formula√ß√µes.

### An√°lise da Converg√™ncia do MSE
Com o lema anterior em mente, podemos analisar o comportamento do MSE quando o horizonte de previs√£o (*s*) tende ao infinito. Tomando o limite quando $s \to \infty$, temos:
$$
\lim_{s \to \infty} MSE(\hat{Y}_{t+s|t}) = \lim_{s \to \infty} \sigma^2 \sum_{j=0}^{s-1} \phi^{2j}.
$$
A soma no lado direito da equa√ß√£o √© uma s√©rie geom√©trica, que converge para $\frac{1}{1 - \phi^2}$ quando $|\phi| < 1$. Portanto:
$$
\lim_{s \to \infty} MSE(\hat{Y}_{t+s|t}) = \frac{\sigma^2}{1 - \phi^2}.
$$
Este resultado demonstra formalmente que, para um processo AR(1) estacion√°rio, o MSE da previs√£o converge para a vari√¢ncia incondicional do processo quando o horizonte de previs√£o tende ao infinito.

> üí° **Implica√ß√µes Pr√°ticas:**
> A converg√™ncia do MSE tem v√°rias implica√ß√µes pr√°ticas. Primeiro, ela estabelece um limite para a incerteza da previs√£o a longo prazo. Mesmo que a nossa previs√£o se aproxime da m√©dia do processo, haver√° sempre uma variabilidade inerente √† natureza aleat√≥ria do processo. Esta variabilidade, que se torna uma constante quando o horizonte de previs√£o se torna muito grande, √© a vari√¢ncia incondicional do processo.
>
> Em segundo lugar, este resultado mostra que a escolha de um horizonte de previs√£o espec√≠fico √© importante. Se estivermos interessados em fazer uma previs√£o para o curto prazo, a incerteza da nossa previs√£o ser√° menor do que a incerteza de uma previs√£o a longo prazo. No entanto, uma previs√£o de longo prazo, embora tenha uma vari√¢ncia maior, tamb√©m tem a vantagem de convergir para a m√©dia do processo.

**Proposi√ß√£o 1**
O MSE da previs√£o de *s* passos a frente para um processo AR(1) pode ser reescrito como:
$$
MSE(\hat{Y}_{t+s|t}) = \frac{\sigma^2}{1-\phi^2}(1-\phi^{2s})
$$
**Demonstra√ß√£o**
I. Pelo Lema 1, temos que o MSE √© dado por
$$
MSE(\hat{Y}_{t+s|t}) = \sigma^2 \sum_{j=0}^{s-1} \phi^{2j}
$$
II. Utilizando o Lema 1.1, sabemos que
$$
\sigma^2 \sum_{j=0}^{s-1} \phi^{2j} =  \sigma^2 \frac{1-\phi^{2s}}{1-\phi^2}
$$
III. Portanto,
$$
MSE(\hat{Y}_{t+s|t}) = \frac{\sigma^2}{1-\phi^2}(1-\phi^{2s})
$$
$\blacksquare$

> üí° **Exemplo Num√©rico:**
> Novamente, com o processo AR(1) onde $\phi = 0.8$ e $\sigma^2 = 1$, vamos usar a Proposi√ß√£o 1 para verificar o comportamento do MSE conforme *s* aumenta e se aproxima da vari√¢ncia incondicional:
>
> A vari√¢ncia incondicional do processo √©:
> $$
> \frac{\sigma^2}{1 - \phi^2} = \frac{1}{1 - 0.8^2} = \frac{1}{1 - 0.64} = \frac{1}{0.36} \approx 2.7778
> $$
> Usando a f√≥rmula da Proposi√ß√£o 1, vamos calcular o MSE para alguns valores de *s*:
> Para $s = 1$:
> $$
> MSE(\hat{Y}_{t+1|t}) = \frac{1}{1 - 0.8^2}(1 - 0.8^{2 \cdot 1}) = \frac{1}{0.36}(1 - 0.64) = \frac{0.36}{0.36} = 1
> $$
> Para $s = 5$:
> $$
> MSE(\hat{Y}_{t+5|t}) = \frac{1}{0.36}(1 - 0.8^{2 \cdot 5}) = \frac{1}{0.36}(1 - 0.8^{10}) \approx \frac{1}{0.36}(1 - 0.10737) \approx 2.480
> $$
> Para $s = 10$:
> $$
> MSE(\hat{Y}_{t+10|t}) = \frac{1}{0.36}(1 - 0.8^{2 \cdot 10}) = \frac{1}{0.36}(1 - 0.8^{20}) \approx \frac{1}{0.36}(1 - 0.0115) \approx 2.745
> $$
>
> Como podemos ver, conforme *s* aumenta, o MSE se aproxima do valor da vari√¢ncia incondicional ($\approx 2.7778$). Isso demonstra a converg√™ncia do MSE para a vari√¢ncia incondicional com o aumento do horizonte de previs√£o.

### Exemplo Num√©rico
Para ilustrar a converg√™ncia do MSE, vamos usar o mesmo processo AR(1) do cap√≠tulo anterior, com $\phi = 0.7$ e $\sigma^2 = 4$. A vari√¢ncia incondicional do processo √©
$$
\frac{\sigma^2}{1 - \phi^2} = \frac{4}{1 - (0.7)^2} = \frac{4}{0.51} \approx 7.84.
$$
Calculamos o MSE para diferentes valores de *s*:
Para $s = 1$: $MSE = 4$.
Para $s = 2$: $MSE \approx 5.96$.
Para $s = 3$: $MSE \approx 6.9204$.
...
Para $s \to \infty$: $MSE \to 7.84$.
Podemos ver que o MSE aumenta com o aumento de *s*, mas converge para a vari√¢ncia incondicional do processo ($\approx 7.84$). Este exemplo ilustra a converg√™ncia do MSE, demonstrada no cap√≠tulo anterior e refor√ßada pela an√°lise atual.

### Conclus√£o
Este cap√≠tulo focou na an√°lise da previs√£o linear para um processo AR(1) e demonstrou que, √† medida que o horizonte de previs√£o aumenta, o erro quadr√°tico m√©dio (MSE) converge para a vari√¢ncia incondicional do processo. Este resultado consolida a nossa compreens√£o do comportamento das previs√µes em processos AR(1). O MSE, que √© a medida de incerteza da previs√£o, n√£o cresce indefinidamente. Em vez disso, ele converge para uma variabilidade inerente ao processo, que √© a vari√¢ncia incondicional. Este resultado demonstra que, para um modelo estacion√°rio, embora os efeitos de choques individuais sejam tempor√°rios e a previs√£o eventualmente se aproximar√° da m√©dia, o grau de incerteza a longo prazo ser√° sempre pelo menos a vari√¢ncia incondicional do processo. Este conceito √© fundamental para decis√µes de previs√£o e aloca√ß√£o de recursos em an√°lise de s√©ries temporais.

### Refer√™ncias
[^1]: Express√£o [4.1.1], p√°g 73
[^2]: Express√£o [4.1.2], p√°g 73
[^3]: Express√£o [4.1.9], p√°g 74
[^4]: Express√£o [4.2.17], p√°g 80
[^5]: Express√£o [4.2.16], p√°g 80
[^6]: Express√£o [4.2.18], p√°g 80
[^7]: Express√£o [4.2.19], p√°g 80
<!-- END -->
