## Previs√£o √ìtima para Processos AR(p)
### Introdu√ß√£o
Este cap√≠tulo se aprofunda na previs√£o de processos autorregressivos de ordem *p* (AR(p)), baseando-se em resultados de cap√≠tulos anteriores. Anteriormente, exploramos a proje√ß√£o linear e suas propriedades, al√©m de introduzir os conceitos de previs√£o com base em um n√∫mero infinito e finito de observa√ß√µes [^1, ^2]. Agora, vamos nos concentrar na aplica√ß√£o desses conceitos para derivar a previs√£o √≥tima para um processo AR(p), com √™nfase em sua natureza recursiva e efici√™ncia computacional.

### Conceitos Fundamentais
A previs√£o √≥tima para um processo AR(p) √© obtida utilizando o conceito da lei das proje√ß√µes iteradas, um princ√≠pio fundamental em an√°lise de s√©ries temporais [^2]. Essencialmente, a previs√£o de um passo √† frente do processo AR(p) no tempo *t* √© expressa como uma fun√ß√£o linear dos seus valores passados at√© a ordem *p*:
$$ (Y_{t+1} - \mu) = \phi_1(Y_t - \mu) + \phi_2(Y_{t-1} - \mu) + \dots + \phi_p(Y_{t-p+1} - \mu) $$ [^2]
onde $Y_t$ representa a s√©rie temporal no tempo *t*, $\mu$ √© a m√©dia da s√©rie, e $\phi_1, \phi_2, \dots, \phi_p$ s√£o os coeficientes autorregressivos. Esta equa√ß√£o [^2] estabelece a depend√™ncia linear de $Y_{t+1}$ em rela√ß√£o aos *p* valores anteriores.

> üí° **Exemplo Num√©rico:**
> Vamos considerar um processo AR(2) com $\mu = 10$, $\phi_1 = 0.8$, e $\phi_2 = 0.2$. Suponha que as observa√ß√µes nos tempos *t*, *t-1* sejam $Y_t = 12$ e $Y_{t-1} = 11$.
> A previs√£o de um passo √† frente $Y_{t+1}$ √© calculada como:
> $$ (Y_{t+1} - 10) = 0.8(12 - 10) + 0.2(11 - 10) $$
> $$ Y_{t+1} - 10 = 0.8(2) + 0.2(1) $$
> $$ Y_{t+1} - 10 = 1.6 + 0.2 = 1.8$$
> $$ Y_{t+1} = 10 + 1.8 = 11.8$$
> Portanto, a previs√£o de um passo √† frente para $Y_{t+1}$ √© 11.8.

Para previs√µes de dois per√≠odos √† frente, podemos empregar o mesmo princ√≠pio, utilizando a previs√£o de um passo √† frente $Y_{t+1}$ na equa√ß√£o anterior. Substituindo *t* por *t* + 1 na equa√ß√£o original e utilizando o resultado da previs√£o de um passo √† frente, obtemos:
$$ (Y_{t+2|t} - \mu) = \phi_1(Y_{t+1|t} - \mu) + \phi_2(Y_t - \mu) + \dots + \phi_p(Y_{t-p+2} - \mu) $$ [^2]
Esta equa√ß√£o demonstra a natureza recursiva da previs√£o. A previs√£o de dois per√≠odos √† frente, $Y_{t+2|t}$, utiliza a previs√£o de um per√≠odo √† frente, $Y_{t+1|t}$, al√©m dos valores passados observados.

> üí° **Exemplo Num√©rico (continua√ß√£o):**
> Utilizando o mesmo processo AR(2) do exemplo anterior, vamos calcular a previs√£o de dois passos √† frente $Y_{t+2|t}$. J√° calculamos que $Y_{t+1|t} = 11.8$.
> Temos $Y_t = 12$ e $Y_{t-1} = 11$.
> $$ (Y_{t+2|t} - 10) = 0.8(11.8 - 10) + 0.2(12 - 10) $$
> $$ Y_{t+2|t} - 10 = 0.8(1.8) + 0.2(2) $$
> $$ Y_{t+2|t} - 10 = 1.44 + 0.4 = 1.84$$
> $$ Y_{t+2|t} = 10 + 1.84 = 11.84$$
> Assim, a previs√£o de dois passos √† frente para $Y_{t+2}$ √© 11.84.

A generaliza√ß√£o para *s* per√≠odos √† frente (onde *s* > 1) pode ser expressa da seguinte forma:
$$ (Y_{t+s|t} - \mu) = \phi_1(Y_{t+s-1|t} - \mu) + \phi_2(Y_{t+s-2|t} - \mu) + \dots + \phi_p(Y_{t+s-p|t} - \mu) $$ [^2]
Aqui, $Y_{t+j|t}$ representa a previs√£o de $Y_{t+j}$ baseada nas informa√ß√µes at√© o tempo *t*. Se o √≠ndice *t + j* for menor ou igual a *t*, ent√£o $Y_{t+j|t}$ √© simplesmente o valor observado $Y_{t+j}$ [^2]. Este processo iterativo permite gerar previs√µes para qualquer horizonte temporal.

Formalmente, podemos expressar a previs√£o √≥tima de *s* per√≠odos √† frente como [^2]:
$$ \hat{Y}_{t+s|t} = \mu + f_1^{(s)}(Y_t - \mu) + f_2^{(s)}(Y_{t-1} - \mu) + \dots + f_p^{(s)}(Y_{t-p+1} - \mu) $$
onde $f_i^{(s)}$ s√£o os coeficientes de proje√ß√£o. Os coeficientes podem ser calculados iterativamente usando a lei de proje√ß√µes iteradas.
Os erros associados a estas proje√ß√µes s√£o:
$$Y_{t+s} - \hat{Y}_{t+s|t} = \epsilon_{t+s} + \psi_1 \epsilon_{t+s-1} + \psi_2 \epsilon_{t+s-2} + \dots + \psi_{s-1} \epsilon_{t+1}$$
onde $\epsilon_t$ s√£o as inova√ß√µes do processo. Este termo representa o erro de previs√£o, que consiste em uma combina√ß√£o linear das inova√ß√µes futuras.
Este resultado demonstra que, no processo AR(p), as previs√µes futuras dependem linearmente dos valores passados observados.

**Lema 1**
A esperan√ßa condicional da inova√ß√£o $\epsilon_t$ dado o passado √© zero, ou seja, $E[\epsilon_t | Y_{t-1}, Y_{t-2}, \ldots] = 0$.

*Proof.*
I. As inova√ß√µes $\epsilon_t$ s√£o definidas como a diferen√ßa entre o valor observado $Y_t$ e sua previs√£o √≥tima $\hat{Y}_{t|t-1}$ baseada no passado:
   $$\epsilon_t = Y_t - \hat{Y}_{t|t-1}$$
II. A previs√£o √≥tima $\hat{Y}_{t|t-1}$ √© a proje√ß√£o de $Y_t$ sobre o espa√ßo vetorial gerado por $Y_{t-1}, Y_{t-2}, \ldots$. Pela propriedade da proje√ß√£o ortogonal, o erro de previs√£o $\epsilon_t$ deve ser ortogonal (n√£o correlacionado) a qualquer fun√ß√£o dos valores passados.
III. Como $\epsilon_t$ √© ortogonal ao espa√ßo vetorial gerado por $Y_{t-1}, Y_{t-2}, \ldots$, sua esperan√ßa condicional dado o passado √© zero:
    $$E[\epsilon_t | Y_{t-1}, Y_{t-2}, \ldots] = 0$$
Portanto, a esperan√ßa condicional da inova√ß√£o $\epsilon_t$ dado o passado √© zero. ‚ñ†

**Teorema 1**
A previs√£o √≥tima de *s* per√≠odos √† frente, $\hat{Y}_{t+s|t}$, minimiza o erro quadr√°tico m√©dio de previs√£o,  $E[(Y_{t+s} - \hat{Y}_{t+s|t})^2 | Y_t, Y_{t-1}, \ldots]$.

*Proof.*
I. Seja $\hat{Y}_{t+s|t}$ a previs√£o √≥tima de *s* per√≠odos √† frente, que √© definida como a proje√ß√£o de $Y_{t+s}$ sobre o espa√ßo vetorial gerado por $Y_t, Y_{t-1}, \ldots$. Seja $Z_{t+s|t}$ qualquer outra previs√£o de *s* per√≠odos √† frente.

II. Podemos expressar o erro quadr√°tico m√©dio da previs√£o $Z_{t+s|t}$ como:
   $$E[(Y_{t+s} - Z_{t+s|t})^2 | Y_t, Y_{t-1}, \ldots]$$
III. Adicionando e subtraindo $\hat{Y}_{t+s|t}$ dentro do quadrado:
    $$E[(Y_{t+s} - \hat{Y}_{t+s|t} + \hat{Y}_{t+s|t} - Z_{t+s|t})^2 | Y_t, Y_{t-1}, \ldots]$$
IV. Expandindo o quadrado:
    $$E[(Y_{t+s} - \hat{Y}_{t+s|t})^2 + 2(Y_{t+s} - \hat{Y}_{t+s|t})(\hat{Y}_{t+s|t} - Z_{t+s|t}) + (\hat{Y}_{t+s|t} - Z_{t+s|t})^2 | Y_t, Y_{t-1}, \ldots]$$
V. Pela propriedade da proje√ß√£o ortogonal, $E[(Y_{t+s} - \hat{Y}_{t+s|t})(\hat{Y}_{t+s|t} - Z_{t+s|t}) | Y_t, Y_{t-1}, \ldots] = 0$, pois o erro de proje√ß√£o $Y_{t+s} - \hat{Y}_{t+s|t}$ √© ortogonal a qualquer fun√ß√£o do passado, incluindo $\hat{Y}_{t+s|t} - Z_{t+s|t}$.
VI. Portanto,
   $$E[(Y_{t+s} - Z_{t+s|t})^2 | Y_t, Y_{t-1}, \ldots] = E[(Y_{t+s} - \hat{Y}_{t+s|t})^2 | Y_t, Y_{t-1}, \ldots] + E[(\hat{Y}_{t+s|t} - Z_{t+s|t})^2 | Y_t, Y_{t-1}, \ldots]$$
VII. Como $E[(\hat{Y}_{t+s|t} - Z_{t+s|t})^2 | Y_t, Y_{t-1}, \ldots] \geq 0$, o erro quadr√°tico m√©dio √© minimizado quando $Z_{t+s|t} = \hat{Y}_{t+s|t}$.

Portanto, a previs√£o √≥tima $\hat{Y}_{t+s|t}$ minimiza o erro quadr√°tico m√©dio de previs√£o. ‚ñ†

**Corol√°rio 1.1**
O erro de previs√£o para *s* per√≠odos √† frente, $Y_{t+s} - \hat{Y}_{t+s|t}$, tem m√©dia condicional zero dado o passado, ou seja,
$E[Y_{t+s} - \hat{Y}_{t+s|t} | Y_t, Y_{t-1}, \ldots] = 0$.

*Proof.*
I. Pela defini√ß√£o de previs√£o √≥tima, o erro de previs√£o √© dado por:
    $$e_{t+s|t} = Y_{t+s} - \hat{Y}_{t+s|t}$$
II. Do Teorema 1, sabemos que $\hat{Y}_{t+s|t}$ √© a proje√ß√£o de $Y_{t+s}$ sobre o espa√ßo vetorial gerado por $Y_t, Y_{t-1}, \ldots$. Portanto, o erro de previs√£o $e_{t+s|t}$ √© ortogonal a esse espa√ßo, ou seja, n√£o correlacionado com o passado.
III. Da propriedade de proje√ß√£o ortogonal, a esperan√ßa condicional do erro de previs√£o dado o passado √© zero:
   $$E[Y_{t+s} - \hat{Y}_{t+s|t} | Y_t, Y_{t-1}, \ldots] = E[e_{t+s|t} | Y_t, Y_{t-1}, \ldots] = 0$$
Portanto, o erro de previs√£o para *s* per√≠odos √† frente tem m√©dia condicional zero dado o passado. ‚ñ†

### Vantagens da Abordagem Recursiva
1.  **Efici√™ncia Computacional**: A natureza recursiva da previs√£o para o processo AR(p) o torna computacionalmente trat√°vel [^2]. Em vez de recalcular as previs√µes do zero para cada horizonte, cada previs√£o utiliza o resultado do passo anterior.
2.  **Baseado em Dados Passados**: O modelo depende apenas de valores passados da pr√≥pria s√©rie temporal [^2], eliminando a necessidade de informa√ß√µes externas. Esta abordagem √© especialmente √∫til quando h√° dificuldade em obter dados de outros processos.
3.  **Otimiza√ß√£o**: A previs√£o resultante √© √≥tima no sentido de minimizar o erro quadr√°tico m√©dio, quando se considera a depend√™ncia linear dos dados [^1]. Esta propriedade garante que o modelo se adapta aos padr√µes nos dados, evitando o sobreajuste.
4.  **Interpretabilidade**: Os coeficientes autorregressivos $\phi_1, \phi_2, \dots, \phi_p$ fornecem informa√ß√µes sobre a influ√™ncia de cada valor passado na previs√£o atual, permitindo uma an√°lise profunda do comportamento da s√©rie temporal [^2].

**Proposi√ß√£o 1**
Os coeficientes $f_i^{(s)}$  na representa√ß√£o da previs√£o √≥tima para *s* per√≠odos √† frente s√£o fun√ß√µes dos coeficientes autorregressivos $\phi_1, \dots, \phi_p$ e do horizonte de previs√£o *s*.

*Proof.*
I. A previs√£o de um passo √† frente √© dada por:
    $$\hat{Y}_{t+1|t} = \mu + \phi_1(Y_t - \mu) + \phi_2(Y_{t-1} - \mu) + \dots + \phi_p(Y_{t-p+1} - \mu)$$
    onde os coeficientes s√£o $\phi_i$.
II. A previs√£o para dois passos √† frente √© dada recursivamente por:
   $$\hat{Y}_{t+2|t} = \mu + \phi_1(\hat{Y}_{t+1|t} - \mu) + \phi_2(Y_t - \mu) + \dots + \phi_p(Y_{t-p+2} - \mu)$$
III. Substituindo a previs√£o de um passo √† frente em II:
    $$\hat{Y}_{t+2|t} = \mu + \phi_1(\mu + \phi_1(Y_t - \mu) + \phi_2(Y_{t-1} - \mu) + \dots + \phi_p(Y_{t-p+1} - \mu) - \mu) + \phi_2(Y_t - \mu) + \dots + \phi_p(Y_{t-p+2} - \mu)$$
IV. Ao expandir e rearranjar, os coeficientes de $Y_t, Y_{t-1}, \dots, Y_{t-p+1}$ em $\hat{Y}_{t+2|t}$ ser√£o fun√ß√µes de $\phi_1, \phi_2, \dots, \phi_p$. Esses s√£o os $f_i^{(2)}$ na express√£o geral.
V. Generalizando, a previs√£o de s per√≠odos √† frente $\hat{Y}_{t+s|t}$ √© obtida pela aplica√ß√£o iterativa do mesmo processo, ent√£o seus coeficientes $f_i^{(s)}$ ser√£o sempre uma fun√ß√£o dos $\phi_i$ e do n√∫mero de passos recursivos *s*.
Portanto, os coeficientes $f_i^{(s)}$ s√£o fun√ß√µes dos coeficientes autorregressivos $\phi_1, \dots, \phi_p$ e do horizonte de previs√£o *s*. ‚ñ†

### Conclus√£o
A previs√£o √≥tima para um processo AR(p) √© um modelo recursivo que combina valores passados da s√©rie temporal para gerar previs√µes eficientes e confi√°veis [^2]. Sua natureza recursiva e efici√™ncia computacional o tornam uma ferramenta poderosa para an√°lise de s√©ries temporais. Al√©m disso, a abordagem se beneficia da otimiza√ß√£o do erro quadr√°tico m√©dio, o que o torna ideal para previs√µes lineares. Em resumo, os processos AR(p) fornecem uma estrutura para a previs√£o √≥tima de s√©ries temporais, especialmente quando os valores passados s√£o a principal fonte de informa√ß√£o.

> üí° **Exemplo Num√©rico (Simula√ß√£o e Erro):**
> Para ilustrar a qualidade da previs√£o, vamos simular um processo AR(1) com $\mu=5$, $\phi_1 = 0.7$ e um ru√≠do branco com $\sigma = 1$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> np.random.seed(42)
>
> mu = 5
> phi1 = 0.7
> sigma = 1
> T = 100
>
> errors = np.random.normal(0, sigma, T)
> Y = np.zeros(T)
> Y[0] = mu + errors[0]
> for t in range(1, T):
>  Y[t] = mu + phi1 * (Y[t-1] - mu) + errors[t]
>
> Y_pred = np.zeros(T)
>
> for t in range(1, T):
>   Y_pred[t] = mu + phi1*(Y[t-1]-mu)
>
>
> plt.plot(Y, label='Real')
> plt.plot(Y_pred, label='Previs√£o')
> plt.legend()
> plt.title('S√©rie Temporal AR(1) vs Previs√£o')
> plt.show()
>
> errors = Y[1:] - Y_pred[1:]
> print("Erro Quadr√°tico M√©dio:", np.mean(errors**2))
> plt.plot(errors)
> plt.title("Res√≠duos da Previs√£o")
> plt.show()
>
> ```
> A partir da simula√ß√£o, podemos observar como as previs√µes se aproximam dos valores reais, e calcular o Erro Quadr√°tico M√©dio (MSE) dos res√≠duos, que quantifica o qu√£o bem o modelo consegue prever os dados futuros. Visualizando os res√≠duos, podemos avaliar se o modelo est√° capturando a din√¢mica da s√©rie temporal. Em um modelo bem ajustado, os res√≠duos devem se comportar como ru√≠do branco.

### Refer√™ncias
[^1]: Express√£o [4.1.1] e seguintes
[^2]: Se√ß√µes 4.2 e seguintes
<!-- END -->
