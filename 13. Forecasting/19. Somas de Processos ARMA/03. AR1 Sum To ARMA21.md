## A Soma de Processos AR(1): Uma An√°lise Detalhada
### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise da soma de processos autorregressivos de primeira ordem (AR(1)), explorando em detalhe como a combina√ß√£o linear de dois desses processos resulta em um processo Autorregressivo de M√©dia M√≥vel (ARMA(2,1)) ou, em casos espec√≠ficos, em um processo AR(1). Construindo sobre os conceitos apresentados anteriormente sobre a combina√ß√£o de processos estoc√°sticos [^4.7.1], [^4.7.2], [^4.7.3], [^4.7.27] este cap√≠tulo oferece uma an√°lise aprofundada, com provas e exemplos, para um p√∫blico com s√≥lido conhecimento em matem√°tica, modelos estat√≠sticos e an√°lise de dados.

### Conceitos Fundamentais
#### Soma de Dois Processos AR(1) com Par√¢metros Distintos
Como introduzido anteriormente [^4.7.22], [^4.7.23] considere dois processos autorregressivos de primeira ordem (AR(1)), $X_t$ e $W_t$, definidos como:
$$(1-\pi L)X_t = u_t$$
$$(1-\rho L)W_t = v_t$$
onde $u_t$ e $v_t$ s√£o processos de ru√≠do branco n√£o correlacionados, $L$ denota o operador de retardo, e $\pi$ e $\rho$ s√£o os par√¢metros autorregressivos. Assumimos que a soma desses dois processos √© observada como $Y_t = X_t + W_t$. Para entender o comportamento de $Y_t$, precisamos analisar sua estrutura temporal.
No caso em que os par√¢metros autorregressivos dos dois processos s√£o distintos $(\pi \neq \rho)$, uma transforma√ß√£o nas equa√ß√µes originais se faz necess√°ria. Multiplicamos a primeira equa√ß√£o por $(1-\rho L)$ e a segunda por $(1-\pi L)$, resultando em:
$$(1-\rho L)(1-\pi L)X_t = (1-\rho L)u_t$$
$$(1-\pi L)(1-\rho L)W_t = (1-\pi L)v_t$$
Somando essas duas equa√ß√µes, obtemos a representa√ß√£o do processo $Y_t$:
$$(1-\rho L)(1-\pi L)(X_t + W_t) = (1-\rho L)u_t + (1-\pi L)v_t$$
Substituindo $Y_t = X_t + W_t$, temos
$$(1-\rho L)(1-\pi L)Y_t = (1-\rho L)u_t + (1-\pi L)v_t$$
O lado esquerdo dessa equa√ß√£o representa um processo autorregressivo de ordem 2, enquanto o lado direito √© um processo de m√©dia m√≥vel de ordem 1. Portanto, a soma $Y_t$ √© descrita como um processo ARMA(2,1):
$$(1 - \phi_1 L - \phi_2 L^2)Y_t = (1 + \theta L)\epsilon_t$$
onde $\phi_1 = \pi + \rho$, $\phi_2 = -\pi\rho$, e o lado direito representa um processo MA(1) composto por uma combina√ß√£o linear dos ru√≠dos brancos originais [^4.7.27]. Este resultado √© fundamental, pois demonstra que a soma de dois processos AR(1) com par√¢metros diferentes n√£o resulta em um processo AR puro, mas sim em um processo ARMA, introduzindo uma componente de m√©dia m√≥vel na estrutura.

> üí° **Exemplo Num√©rico:**
> Sejam $\pi = 0.6$ e $\rho = 0.4$. Ent√£o,
>  $(1 - 0.4L)(1 - 0.6L)Y_t = (1 - 0.4L)u_t + (1 - 0.6L)v_t$
> Expandindo:
>  $(1 - 0.4L - 0.6L + 0.24L^2)Y_t = u_t - 0.4u_{t-1} + v_t - 0.6v_{t-1}$
>  $(1 - 1.0L + 0.24L^2)Y_t = u_t + v_t - 0.4u_{t-1} - 0.6v_{t-1}$
>  Definindo $\epsilon_t$ como um processo MA(1) tal que
>  $(1+\theta L)\epsilon_t= u_t + v_t - 0.4u_{t-1} - 0.6v_{t-1}$
>  onde $\phi_1 = 1.0$, $\phi_2 = -0.24$. Os par√¢metros para $\theta$ e a vari√¢ncia de $\epsilon_t$ s√£o obtidos por um processo de igualdade das autocovari√¢ncias, como visto anteriormente. Isso ilustra como dois AR(1) com par√¢metros distintos resultam em um ARMA(2,1). Para ilustrar, vamos gerar um exemplo num√©rico usando Python:
> ```python
> import numpy as np
> import pandas as pd
> import matplotlib.pyplot as plt
> 
> # Par√¢metros
> pi = 0.6
> rho = 0.4
> n = 200 # n√∫mero de observa√ß√µes
> 
> # Ru√≠dos brancos
> np.random.seed(42)
> u = np.random.normal(0, 1, n)
> v = np.random.normal(0, 1, n)
> 
> # Inicializa√ß√£o dos processos AR(1)
> X = np.zeros(n)
> W = np.zeros(n)
> 
> # Gera√ß√£o dos processos AR(1)
> for t in range(1,n):
>     X[t] = pi*X[t-1] + u[t]
>     W[t] = rho*W[t-1] + v[t]
> 
> # Gera√ß√£o de Y_t
> Y = X + W
> 
> # Plot
> plt.figure(figsize=(10,6))
> plt.plot(Y, label='Y_t = X_t + W_t')
> plt.plot(X, label='X_t')
> plt.plot(W, label='W_t')
> plt.xlabel('Tempo')
> plt.ylabel('Valor')
> plt.title('Processos AR(1) e sua Soma')
> plt.legend()
> plt.grid(True)
> plt.show()
> 
> # Calculando os par√¢metros phi1 e phi2
> phi1 = pi + rho
> phi2 = -pi * rho
> print(f"phi1: {phi1}, phi2: {phi2}")
> ```
> Este c√≥digo gera s√©ries temporais $X_t$ e $W_t$ usando os par√¢metros $\pi = 0.6$ e $\rho = 0.4$, e mostra a s√©rie $Y_t$ resultante. O gr√°fico visualiza as s√©ries temporais individuais e sua soma, enquanto o c√≥digo calcula e exibe os valores te√≥ricos de $\phi_1$ e $\phi_2$.

#### Soma de Dois Processos AR(1) com Par√¢metros Id√™nticos
No caso particular em que os par√¢metros autorregressivos s√£o id√™nticos ($\pi = \rho$), a soma $Y_t$ resulta em um processo AR(1), e n√£o em um processo ARMA(2,1). Se $\pi = \rho$, ent√£o
$$(1-\pi L)X_t = u_t$$
$$(1-\pi L)W_t = v_t$$
Adicionando essas equa√ß√µes diretamente, obtemos:
$$(1-\pi L)(X_t + W_t) = u_t + v_t$$
Substituindo $Y_t = X_t + W_t$
$$(1-\pi L)Y_t = u_t + v_t$$
Aqui, a soma de ru√≠dos brancos, $u_t+v_t$, resulta em um novo ru√≠do branco $\epsilon_t$. Deste modo, a soma $Y_t$ segue um processo AR(1), com o mesmo par√¢metro autorregressivo $\pi$.
$$(1-\pi L)Y_t = \epsilon_t$$

> üí° **Exemplo Num√©rico:**
> Sejam $\pi = \rho = 0.5$. Temos que $X_t = 0.5X_{t-1} + u_t$ e $W_t = 0.5W_{t-1} + v_t$. Ent√£o
> $(1-0.5L)X_t = u_t$ e $(1-0.5L)W_t = v_t$
> Somando, temos
> $(1 - 0.5L)(X_t+W_t) = u_t + v_t$
> $(1 - 0.5L)Y_t = \epsilon_t$, onde $\epsilon_t = u_t + v_t$
> Aqui, $Y_t$ √© um processo AR(1) com par√¢metro autorregressivo $0.5$. Vamos simular esse processo usando Python:
> ```python
> import numpy as np
> import pandas as pd
> import matplotlib.pyplot as plt
> 
> # Par√¢metro
> pi = 0.5
> rho = 0.5
> n = 200 # n√∫mero de observa√ß√µes
> 
> # Ru√≠dos brancos
> np.random.seed(42)
> u = np.random.normal(0, 1, n)
> v = np.random.normal(0, 1, n)
> 
> # Inicializa√ß√£o dos processos AR(1)
> X = np.zeros(n)
> W = np.zeros(n)
> 
> # Gera√ß√£o dos processos AR(1)
> for t in range(1,n):
>     X[t] = pi*X[t-1] + u[t]
>     W[t] = rho*W[t-1] + v[t]
> 
> # Gera√ß√£o de Y_t
> Y = X + W
> 
> # Plot
> plt.figure(figsize=(10,6))
> plt.plot(Y, label='Y_t = X_t + W_t')
> plt.plot(X, label='X_t')
> plt.plot(W, label='W_t')
> plt.xlabel('Tempo')
> plt.ylabel('Valor')
> plt.title('Processos AR(1) com Par√¢metros Id√™nticos e sua Soma')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
> Este c√≥digo gera $X_t$ e $W_t$ com $\pi = \rho = 0.5$ e ent√£o plota $Y_t$, a soma dos dois processos, que visualmente se comporta como um processo AR(1).

#### Formaliza√ß√£o Matem√°tica
Para formalizar estes resultados, vamos considerar a fun√ß√£o de autocovari√¢ncia e a fun√ß√£o geradora de autocovari√¢ncia. Se $Y_t = X_t + W_t$ e $X_t$ e $W_t$ s√£o n√£o correlacionados, ent√£o
$$Cov(Y_t, Y_{t-k}) = Cov(X_t, X_{t-k}) + Cov(W_t, W_{t-k})$$
A fun√ß√£o geradora de autocovari√¢ncia de $Y_t$, $g_Y(z)$, ser√° a soma das fun√ß√µes geradoras de autocovari√¢ncia dos processos $X_t$ e $W_t$:
$$g_Y(z) = g_X(z) + g_W(z)$$
Esta rela√ß√£o √© importante para determinar a estrutura ARMA resultante da combina√ß√£o de processos.

**Proposi√ß√£o 1**
A soma de dois processos AR(1) n√£o correlacionados, onde seus par√¢metros autorregressivos s√£o id√™nticos, resulta em um processo AR(1) com o mesmo par√¢metro.
*Prova:*
I. Sejam $X_t$ e $W_t$ dois processos AR(1) com o mesmo par√¢metro autorregressivo $\pi$. Ent√£o, podemos escrever:
$$(1-\pi L)X_t = u_t$$
$$(1-\pi L)W_t = v_t$$
onde $u_t$ e $v_t$ s√£o ru√≠dos brancos n√£o correlacionados.
II. Seja $Y_t = X_t + W_t$.
III. Somando as equa√ß√µes (I), temos:
$$(1-\pi L)(X_t + W_t) = u_t + v_t$$
$$(1-\pi L)Y_t = \epsilon_t$$
onde $\epsilon_t = u_t + v_t$ √© um novo ru√≠do branco, resultante da soma de dois ru√≠dos brancos n√£o correlacionados.
IV. Portanto, $Y_t$ segue um processo AR(1) com o mesmo par√¢metro autorregressivo $\pi$. $\blacksquare$

**Teorema 1**
A soma de dois processos AR(1) n√£o correlacionados, onde seus par√¢metros autorregressivos s√£o distintos, resulta em um processo ARMA(2,1).
*Prova:*
I. Sejam $X_t$ e $W_t$ dois processos AR(1) com par√¢metros autorregressivos $\pi$ e $\rho$, respectivamente, com $\pi \neq \rho$, e definidos como:
$$(1-\pi L)X_t = u_t$$
$$(1-\rho L)W_t = v_t$$
onde $u_t$ e $v_t$ s√£o ru√≠dos brancos n√£o correlacionados.
II. Seja $Y_t = X_t + W_t$.
III. Multiplicando a primeira equa√ß√£o por $(1-\rho L)$ e a segunda por $(1-\pi L)$, temos:
$$(1-\rho L)(1-\pi L)X_t = (1-\rho L)u_t$$
$$(1-\pi L)(1-\rho L)W_t = (1-\pi L)v_t$$
IV. Somando, obtemos:
$$(1-\rho L)(1-\pi L)(X_t + W_t) = (1-\rho L)u_t + (1-\pi L)v_t$$
$$(1-\rho L)(1-\pi L)Y_t = (1-\rho L)u_t + (1-\pi L)v_t$$
V. Expandindo o lado esquerdo, e definindo  $\phi_1 = \pi + \rho$ e $\phi_2 = -\pi\rho$, e o lado direito como $(1 + \theta L)\epsilon_t$, temos:
$$(1 - \phi_1 L - \phi_2 L^2)Y_t = (1+\theta L)\epsilon_t$$
onde $\epsilon_t$ √© uma combina√ß√£o linear dos ru√≠dos brancos originais.
VI. Portanto, $Y_t$ segue um processo ARMA(2,1). $\blacksquare$
Este teorema formaliza a intui√ß√£o de que a soma de processos AR(1) com par√¢metros distintos leva √† componente de m√©dia m√≥vel no processo resultante.

**Lema 1**
A vari√¢ncia do processo ru√≠do branco resultante da soma de dois ru√≠dos brancos n√£o correlacionados √© a soma de suas vari√¢ncias individuais.
*Prova:*
I. Sejam $u_t$ e $v_t$ dois processos de ru√≠do branco n√£o correlacionados com vari√¢ncias $\sigma_u^2$ e $\sigma_v^2$, respectivamente.
II. Seja $\epsilon_t = u_t + v_t$.
III. Calculando a vari√¢ncia de $\epsilon_t$:
   $$Var(\epsilon_t) = Var(u_t + v_t) = Var(u_t) + Var(v_t) + 2Cov(u_t, v_t)$$
IV. Como $u_t$ e $v_t$ s√£o n√£o correlacionados, $Cov(u_t, v_t) = 0$.
V. Portanto,
   $$Var(\epsilon_t) = Var(u_t) + Var(v_t) = \sigma_u^2 + \sigma_v^2$$
VI. Isso mostra que a vari√¢ncia do ru√≠do branco resultante √© a soma das vari√¢ncias dos ru√≠dos brancos originais. $\blacksquare$

**Teorema 1.1**
A fun√ß√£o de autocovari√¢ncia do processo ARMA(2,1) resultante da soma de dois processos AR(1) com par√¢metros distintos, √© dada pela soma das fun√ß√µes de autocovari√¢ncia dos processos AR(1) originais apenas para lags maiores que zero.
*Prova:*
I. Do Teorema 1, sabemos que $Y_t$ √© um processo ARMA(2,1) dado por $(1 - \phi_1 L - \phi_2 L^2)Y_t = (1 + \theta L)\epsilon_t$.
II. Sabemos tamb√©m que  $Y_t = X_t + W_t$, onde $X_t$ e $W_t$ s√£o AR(1) n√£o correlacionados.
III.  A autocovari√¢ncia de $Y_t$ no lag $k$ √© dada por $Cov(Y_t, Y_{t-k}) = Cov(X_t + W_t, X_{t-k} + W_{t-k})$.
IV. Como $X_t$ e $W_t$ s√£o n√£o correlacionados:
$Cov(Y_t, Y_{t-k}) = Cov(X_t, X_{t-k}) + Cov(W_t, W_{t-k})$ para $k \ge 1$.
V. Para o lag $0$, $Cov(Y_t,Y_t) = Var(Y_t) = Var(X_t)+ Var(W_t)$. Contudo, a autocovari√¢ncia de $Y_t$ √© influenciada pelos par√¢metros do processo MA(1) que comp√µe o lado direito da equa√ß√£o do ARMA(2,1), ou seja, $(1 + \theta L)\epsilon_t$.
VI. Portanto, a fun√ß√£o de autocovari√¢ncia de $Y_t$ √© dada pela soma das fun√ß√µes de autocovari√¢ncia de $X_t$ e $W_t$ apenas para $k>0$, enquanto o lag zero e outros momentos ser√£o influenciados pelo processo MA(1) e seus par√¢metros. $\blacksquare$

### Conclus√£o
Este cap√≠tulo explorou em profundidade a soma de processos AR(1), revelando que o resultado √© sens√≠vel √† igualdade dos par√¢metros autorregressivos. Quando os par√¢metros s√£o distintos, a soma produz um processo ARMA(2,1), que inclui tanto uma componente autorregressiva de segunda ordem quanto uma componente de m√©dia m√≥vel de primeira ordem. Quando os par√¢metros s√£o id√™nticos, o resultado √© um processo AR(1). Esta an√°lise detalhada proporciona uma compreens√£o mais profunda da estrutura das s√©ries temporais e das consequ√™ncias da combina√ß√£o de diferentes modelos estoc√°sticos, enfatizando a import√¢ncia de analisar as propriedades dos processos e n√£o simplesmente som√°-los. A capacidade de prever como processos estoc√°sticos se comportam quando combinados √© fundamental para modelagem e previs√£o em diversas aplica√ß√µes.
### Refer√™ncias
[^4.7.1]: ... *[Defini√ß√£o de um processo MA(1)]*
[^4.7.2]: ... *[Autocovari√¢ncias de um processo MA(1)]*
[^4.7.3]: ... *[Defini√ß√£o de ru√≠do branco]*
[^4.7.22]: ... *[Defini√ß√£o de um processo AR(1) para X]*
[^4.7.23]: ... *[Defini√ß√£o de um processo AR(1) para W]*
[^4.7.27]: ... *[Soma de AR(1) com AR(1) resulta em um ARMA(2,1)]*
<!-- END -->
