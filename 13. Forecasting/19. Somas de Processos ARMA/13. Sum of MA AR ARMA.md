## A Soma de Processos ARMA: Generaliza√ß√£o e Complexidade

### Introdu√ß√£o
Este cap√≠tulo expande a an√°lise da combina√ß√£o linear de processos estoc√°sticos, concentrando-se na soma de dois processos ARMA (Autorregressivos de M√©dias M√≥veis), generalizando resultados anteriores para modelos MA e AR [^4.7.1], [^4.7.2], [^4.7.3], [^4.7.5], [^4.7.7], [^4.7.15], [^4.7.16], [^4.7.21], [^4.7.27], e enfatizando que a soma de processos ARMA resulta em outro processo ARMA, onde a ordem dos componentes autorregressivos e de m√©dias m√≥veis √© determinada pelos m√°ximos das ordens dos componentes individuais. Embora as deriva√ß√µes se tornem mais complexas do que para MA e AR, este cap√≠tulo visa fornecer uma compreens√£o s√≥lida e formal para um p√∫blico com forte base em matem√°tica, modelos estat√≠sticos e an√°lise de dados.

### Conceitos Fundamentais
#### Soma de Dois Processos ARMA
Considere dois processos ARMA, $X_t$ e $W_t$, com representa√ß√µes gerais:
$$ \phi_X(L) X_t = \theta_X(L) u_t$$
$$ \phi_W(L) W_t = \theta_W(L) v_t$$
onde $u_t$ e $v_t$ s√£o ru√≠dos brancos independentes, $\phi_X(L)$ e $\phi_W(L)$ s√£o os polin√¥mios autorregressivos de ordens $p_X$ e $p_W$ respectivamente, e $\theta_X(L)$ e $\theta_W(L)$ s√£o os polin√¥mios de m√©dias m√≥veis de ordens $q_X$ e $q_W$ respetivamente.  Assumimos que os processos $X_t$ e $W_t$ s√£o independentes, o que significa que $E(X_t W_{t-j}) = 0$ para todos os lags $j$. O processo resultante da soma √© definido como $Y_t = X_t + W_t$. O objetivo √© analisar as propriedades do processo resultante $Y_t$, em particular, a sua representa√ß√£o em termos de modelos ARMA.

#### O Polin√¥mio Caracter√≠stico e a Representa√ß√£o da Soma
Para entender a estrutura de $Y_t$, podemos multiplicar a equa√ß√£o de $X_t$ por $\phi_W(L)$ e a equa√ß√£o de $W_t$ por $\phi_X(L)$, obtendo:
$$\phi_W(L)\phi_X(L) X_t = \phi_W(L)\theta_X(L)u_t$$
$$\phi_X(L)\phi_W(L) W_t = \phi_X(L)\theta_W(L)v_t$$
Somando ambas as equa√ß√µes temos:
$$\phi_W(L)\phi_X(L) (X_t+W_t) = \phi_W(L)\theta_X(L)u_t + \phi_X(L)\theta_W(L)v_t$$
Substituindo $Y_t = X_t + W_t$, obtemos:
$$\phi_Y(L) Y_t =  \phi_W(L)\theta_X(L)u_t + \phi_X(L)\theta_W(L)v_t $$
onde $\phi_Y(L) = \phi_W(L)\phi_X(L)$ √© o polin√¥mio autorregressivo resultante, cuja ordem ser√° igual √† soma das ordens dos processos componentes AR: $p_Y = p_X + p_W$.
O lado direito da equa√ß√£o corresponde √† soma de dois processos de m√©dias m√≥veis ponderados, cujo resultado, como demonstrado em cap√≠tulos anteriores, √© outro processo MA, onde a ordem √© o m√°ximo das ordens dos componentes MA resultantes da multiplica√ß√£o dos polin√¥mios [^4.7.21], [^4.8.4].
O processo $Y_t$ pode ent√£o ser representado como:
$$\phi_Y(L) Y_t = \theta_Y(L) \epsilon_t$$
onde $\epsilon_t$ √© um novo ru√≠do branco e $\theta_Y(L)$ √© um novo polin√¥mio de m√©dias m√≥veis. A ordem do processo MA resultante √© dada por $q_Y = \max\{p_W + q_X, p_X + q_W \}$.  Neste caso, o processo resultante √© um ARMA com ordem $p_Y = p_X + p_W$ na parte AR e com ordem $q_Y = \max\{p_W + q_X, p_X + q_W \}$ na parte MA, o que √© igual √† ordem da parte MA da soma dos processos.
> üí° **Exemplo Num√©rico:** Considere um processo ARMA(1,1) $X_t$ dado por $(1-0.5L)X_t = (1+0.3L)u_t$, e um processo ARMA(2,1) $W_t$ dado por $(1-0.6L + 0.2L^2)W_t = (1-0.2L)v_t$.
> A soma $Y_t=X_t + W_t$ √© obtida por:
> $$\phi_W(L)\phi_X(L)Y_t = \phi_W(L)\theta_X(L)u_t + \phi_X(L)\theta_W(L)v_t$$
> onde $\phi_Y(L) = (1-0.5L)(1-0.6L + 0.2L^2) = 1 - 1.1L + 0.5L^2 - 0.1L^3$. O lado direito √© dado por:
> $$(1-0.6L + 0.2L^2)(1+0.3L)u_t + (1-0.5L)(1-0.2L)v_t$$
> A ordem do lado direito, como processo MA, √© o m√°ximo de $2+1$ e $1+1$, resultando em um processo de ordem 3. Portanto, a soma √© um processo ARMA(3, 2), como o polin√¥mio caracter√≠stico e as ordens dos processos MA indicam.
>
> Para concretizar, vamos supor $u_t$ e $v_t$ s√£o ru√≠dos brancos com vari√¢ncia unit√°ria. Podemos simular as s√©ries $X_t$ e $W_t$ e som√°-las para obter $Y_t$.
> ```python
> import numpy as np
> import pandas as pd
> import matplotlib.pyplot as plt
>
> np.random.seed(42)
>
> def simulate_arma(ar_params, ma_params, n_samples, noise_std=1):
>     p = len(ar_params)
>     q = len(ma_params)
>     errors = np.random.normal(0, noise_std, n_samples + max(p, q))
>     series = np.zeros(n_samples + max(p,q))
>
>     for t in range(max(p, q), n_samples + max(p,q)):
>         ar_part = np.sum(ar_params * series[t - np.arange(1, p + 1)])
>         ma_part = np.sum(ma_params * errors[t - np.arange(1, q + 1)])
>         series[t] = ar_part + ma_part + errors[t]
>     return series[max(p, q):]
>
> # Processo X_t
> ar_x = [0.5]
> ma_x = [0.3]
> x_series = simulate_arma(ar_x, ma_x, 500)
>
> # Processo W_t
> ar_w = [0.6, -0.2]
> ma_w = [-0.2]
> w_series = simulate_arma(ar_w, ma_w, 500)
>
> # Processo Y_t = X_t + W_t
> y_series = x_series + w_series
>
> # Plotting the time series
> plt.figure(figsize=(10, 6))
> plt.plot(x_series, label="X_t (ARMA(1,1))")
> plt.plot(w_series, label="W_t (ARMA(2,1))")
> plt.plot(y_series, label="Y_t (X_t + W_t)")
> plt.xlabel("Time")
> plt.ylabel("Value")
> plt.title("Simulated ARMA Processes and their Sum")
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
> Este c√≥digo simula os processos $X_t$, $W_t$ e a sua soma $Y_t$, permitindo a visualiza√ß√£o da complexidade da s√©rie resultante.
#### A Fun√ß√£o Geradora de Autocovari√¢ncia (FGAC) da Soma
Uma forma mais elegante de analisar a soma de processos ARMA √© usando suas fun√ß√µes geradoras de autocovari√¢ncia (FGAC). A FGAC de $Y_t$ √© dada pela soma das FGACs dos dois processos individuais:
$$g_Y(z) = g_X(z) + g_W(z)$$
onde $g_X(z)$ e $g_W(z)$ s√£o as FGACs dos processos $X_t$ e $W_t$, respetivamente. As FGACs dos processos ARMA $X_t$ e $W_t$ podem ser representadas como:
$$g_X(z) = \sigma_u^2\frac{\theta_X(z)\theta_X(z^{-1})}{\phi_X(z)\phi_X(z^{-1})}$$
$$g_W(z) = \sigma_v^2\frac{\theta_W(z)\theta_W(z^{-1})}{\phi_W(z)\phi_W(z^{-1})}$$
A FGAC do processo resultante $Y_t$ √©, ent√£o, a soma:
$$g_Y(z) = \sigma_u^2\frac{\theta_X(z)\theta_X(z^{-1})}{\phi_X(z)\phi_X(z^{-1})} + \sigma_v^2\frac{\theta_W(z)\theta_W(z^{-1})}{\phi_W(z)\phi_W(z^{-1})}$$
Essa representa√ß√£o formaliza como a estrutura temporal dos processos individuais, expressa em termos das suas FGACs, se combina para gerar a estrutura temporal do processo resultante. A ordem dos polin√¥mios resultantes do numerador e do denominador de $g_Y(z)$ revelam as ordens AR e MA do processo resultante.
> üí° **Exemplo Num√©rico:**  Considere o processo $X_t$ como um AR(1) com $\phi(L) = 1-0.5L$ e ru√≠do branco de vari√¢ncia $\sigma_u^2=1$, e o processo $W_t$ como um MA(1) com $\theta(L)=1+0.3L$ e ru√≠do branco com vari√¢ncia $\sigma_v^2=0.5$. Ent√£o,
> $$g_X(z) = \frac{1}{(1-0.5z)(1-0.5z^{-1})} = \frac{1}{1 - 0.5z -0.5z^{-1} + 0.25}$$
> $$g_W(z) = 0.5\frac{(1+0.3z)(1+0.3z^{-1})}{1} = 0.5(1+0.3z +0.3z^{-1} + 0.09)$$
> A FGAC da soma $Y_t= X_t+W_t$ ser√°:
> $$g_Y(z) = \frac{1}{1 - 0.5z -0.5z^{-1} + 0.25} + 0.5(1+0.3z +0.3z^{-1} + 0.09)$$
> Note que a FGAC de $X_t$ tem um denominador quadr√°tico, devido ao processo AR(1), enquanto que a FGAC de $W_t$ tem um numerador quadr√°tico, devido ao processo MA(1). A soma dessas fun√ß√µes resultar√° em uma fun√ß√£o racional mais complexa, onde os polin√¥mios no numerador e no denominador definir√£o as ordens do processo ARMA resultante.

#### Determina√ß√£o da Ordem do Processo ARMA Resultante
Embora a fun√ß√£o geradora de autocovari√¢ncia da soma seja a soma das FGACs dos processos componentes, a obten√ß√£o de uma express√£o polinomial para o processo resultante pode ser complexa. Como demonstrado em cap√≠tulos anteriores, a soma de processos MA resulta em um novo processo MA cuja ordem √© o m√°ximo das ordens dos processos componentes. Similarmente, a soma de processos AR resulta em um processo AR cuja ordem √© a soma das ordens dos processos componentes [^4.7.27].  Contudo, quando somamos dois processos ARMA, a determina√ß√£o da ordem dos polin√¥mios resultantes no numerador e denominador da FGAC √© mais complexa, pois a soma das FGACs envolve o somat√≥rio de raz√µes de polin√¥mios, e que resulta em uma nova raz√£o de polin√¥mios. O polin√¥mio autorregressivo resultante tem uma ordem igual √† soma das ordens autorregressivas dos componentes, $p_Y = p_X + p_W$. No entanto, a ordem do componente de m√©dias m√≥veis resultante, $q_Y$, √© mais complexa e √© dada por $q_Y = \max\{p_W + q_X, p_X + q_W \}$, como visto na dedu√ß√£o anterior.  Isso significa que a parte de m√©dias m√≥veis resultante pode ter uma ordem superior ao m√°ximo das ordens das partes de m√©dias m√≥veis dos processos originais.
Em termos pr√°ticos,  o processo resultante $Y_t$ ser√° outro processo ARMA, onde a ordem AR ser√° a soma das ordens AR e a ordem MA √© o m√°ximo das ordens resultantes da combina√ß√£o das ordens AR e MA dos componentes, $q_Y = \max\{p_W + q_X, p_X + q_W \}$.

> üí° **Exemplo Num√©rico:**  Considere que $X_t$ √© um ARMA(1,2), e $W_t$ √© um ARMA(2,1). Ent√£o a parte AR de $Y_t$ ser√° de ordem 3 ($1+2$) e a parte MA ser√° de ordem $\max(2+2, 1+1)=4$. Logo, o processo resultante √© um ARMA(3, 4).
>  Num caso mais simples, se  $X_t$ √© AR(1) e $W_t$ √© MA(1), ent√£o o processo resultante ser√° ARMA(1,1), pois $p_Y = 1 + 0 = 1$ e $q_Y = \max(1+0, 0+1) = 1$.  Se $X_t$ √© MA(1) e $W_t$ √© MA(2), ent√£o o processo resultante ser√° ARMA(0,2),  ou seja um MA(2) pois $p_Y=0+0=0$ e $q_Y = max(0+1,0+2)=2$.
>  A an√°lise das fun√ß√µes geradoras de autocovari√¢ncia, em combina√ß√£o com os resultados anteriores sobre a soma de processos AR e MA individuais,  permite determinar a forma do processo resultante. Contudo, os detalhes da obten√ß√£o das representa√ß√µes polinomiais exatas podem ser dif√≠ceis na pr√°tica.

### Formaliza√ß√£o Matem√°tica

**Defini√ß√£o 1.1:**
A fun√ß√£o geradora de autocovari√¢ncia (FGAC) de um processo estacion√°rio $X_t$, denotada por $g_X(z)$, √© definida como:
$$g_X(z) = \sum_{j=-\infty}^{\infty} \gamma_j^x z^j$$
onde $\gamma_j^x = E[(X_t - \mu_X)(X_{t-j} - \mu_X)]$ √© a autocovari√¢ncia de $X_t$ no lag $j$, e $\mu_X$ √© a m√©dia do processo.

**Teorema 1**
A fun√ß√£o geradora de autocovari√¢ncia de um processo $Y_t$, que √© a soma de dois processos ARMA independentes $X_t$ e $W_t$, √© a soma das fun√ß√µes geradoras de autocovari√¢ncia de $X_t$ e $W_t$, i.e.:
$$g_Y(z) = g_X(z) + g_W(z)$$
*Prova:*
I. Sejam $X_t$ e $W_t$ dois processos ARMA independentes, e seja $Y_t = X_t + W_t$.
II. A autocovari√¢ncia de $Y_t$ no lag $j$ √© dada por:
$$\gamma_j^y = E[(Y_t - \mu_Y)(Y_{t-j} - \mu_Y)] = E[(X_t - \mu_X + W_t - \mu_W)(X_{t-j} - \mu_X + W_{t-j} - \mu_W)]$$
III. Como $X_t$ e $W_t$ s√£o independentes, os termos cruzados se anulam:
$$\gamma_j^y = E[(X_t - \mu_X)(X_{t-j} - \mu_X)] + E[(W_t - \mu_W)(W_{t-j} - \mu_W)] = \gamma_j^x + \gamma_j^w$$
IV. A fun√ß√£o geradora de autocovari√¢ncia de $Y_t$ √© dada por:
$$g_Y(z) = \sum_{j=-\infty}^{\infty} \gamma_j^y z^j = \sum_{j=-\infty}^{\infty} (\gamma_j^x + \gamma_j^w) z^j = \sum_{j=-\infty}^{\infty} \gamma_j^x z^j + \sum_{j=-\infty}^{\infty} \gamma_j^w z^j = g_X(z) + g_W(z)$$
V. Portanto, a FGAC de $Y_t$ √© a soma das FGACs de $X_t$ e $W_t$.  $\blacksquare$
Esse teorema √© uma generaliza√ß√£o do resultado demonstrado para processos MA em outros cap√≠tulos.

**Teorema 1.1:**
Se $X_t$ e $W_t$ s√£o processos ARMA, com polin√¥mios AR $\phi_X(L)$ e $\phi_W(L)$ de ordens $p_X$ e $p_W$ e polin√¥mios MA $\theta_X(L)$ e $\theta_W(L)$ de ordens $q_X$ e $q_W$ respetivamente, e a soma $Y_t = X_t + W_t$ resulta em um processo ARMA, ent√£o a ordem do polin√¥mio AR resultante √© $p_Y = p_X + p_W$, e a ordem da parte MA √© $q_Y = \max\{p_W + q_X, p_X + q_W \}$.

*Prova:*
I. Sejam $X_t$ e $W_t$ processos ARMA dados por:
$$\phi_X(L) X_t = \theta_X(L) u_t$$
$$\phi_W(L) W_t = \theta_W(L) v_t$$
II.  Multiplicando a primeira equa√ß√£o por $\phi_W(L)$ e a segunda por $\phi_X(L)$:
$$\phi_W(L)\phi_X(L) X_t = \phi_W(L)\theta_X(L)u_t$$
$$\phi_X(L)\phi_W(L) W_t = \phi_X(L)\theta_W(L)v_t$$
III. Somando e definindo $Y_t = X_t + W_t$, temos:
$$\phi_W(L)\phi_X(L) Y_t = \phi_W(L)\theta_X(L)u_t + \phi_X(L)\theta_W(L)v_t$$
IV. Seja $\phi_Y(L) = \phi_W(L)\phi_X(L)$.  A ordem do polin√¥mio $\phi_Y(L)$ √© a soma das ordens de $\phi_W(L)$ e $\phi_X(L)$, logo $p_Y = p_W + p_X$.
V. O lado direito da equa√ß√£o √© uma soma de dois processos de m√©dias m√≥veis. Multiplicando os polin√¥mios, temos que a ordem de um dos termos √© $p_W + q_X$ e do outro $p_X+ q_W$, como demonstrado em outros cap√≠tulos, a ordem da soma √© o m√°ximo dessas ordens, $q_Y = \max\{p_W + q_X, p_X + q_W \}$.
VI.  Portanto, a soma $Y_t$ segue um processo ARMA com ordem $p_Y$ na parte AR e ordem $q_Y$ na parte MA, onde  $p_Y = p_X + p_W$ e $q_Y = \max\{p_W + q_X, p_X + q_W \}$. $\blacksquare$

**Lema 1**
Se $Y_t = X_t+W_t$, onde $X_t$ e $W_t$ s√£o processos ARMA com polin√¥mios autorregressivos $\phi_X(L)$ e $\phi_W(L)$, e $Y_t$ √© representado como um processo ARMA com polin√¥mio autorregressivo $\phi_Y(L)$, ent√£o $\phi_Y(L) = \phi_X(L) \phi_W(L)$.
*Prova:*
I. Sejam $X_t$ e $W_t$ dois processos ARMA dados por:
$$\phi_X(L) X_t = \theta_X(L) u_t$$
$$\phi_W(L) W_t = \theta_W(L) v_t$$
II. A soma $Y_t = X_t + W_t$ √© dada por:
$$ \phi_W(L)\phi_X(L) Y_t = \phi_W(L)\theta_X(L)u_t + \phi_X(L)\theta_W(L)v_t $$
III. Onde o polin√¥mio autorregressivo resultante de $Y_t$ √© $\phi_Y(L) = \phi_X(L)\phi_W(L)$. $\blacksquare$
Este lema formaliza como os polin√¥mios autorregressivos se combinam na soma de dois processos ARMA.

**Lema 1.1**
O polin√¥mio de m√©dias m√≥veis resultante da soma de dois processos ARMA,  $\theta_Y(L)$,  pode ser expresso como:
$$ \theta_Y(L) \epsilon_t = \phi_W(L)\theta_X(L)u_t + \phi_X(L)\theta_W(L)v_t$$
onde $u_t$ e $v_t$ s√£o ru√≠dos brancos independentes e $\epsilon_t$ √© o ru√≠do branco resultante do processo $Y_t$.

*Prova:*
I.  A partir da dedu√ß√£o anterior, a soma de dois processos ARMA  $X_t$ e $W_t$ pode ser expressa como:
$$ \phi_W(L)\phi_X(L) Y_t = \phi_W(L)\theta_X(L)u_t + \phi_X(L)\theta_W(L)v_t $$
II. O lado direito desta equa√ß√£o corresponde a um processo de m√©dias m√≥veis, com  ru√≠dos brancos $u_t$ e $v_t$.
III. Sendo $\phi_Y(L) = \phi_W(L)\phi_X(L)$, o processo resultante $Y_t$ pode ser representado como:
$$ \phi_Y(L) Y_t = \theta_Y(L)\epsilon_t$$
IV. Portanto, comparando as duas representa√ß√µes do processo $Y_t$, o polin√¥mio de m√©dias m√≥veis $\theta_Y(L)$ √© dado implicitamente pela seguinte igualdade:
$$\theta_Y(L) \epsilon_t = \phi_W(L)\theta_X(L)u_t + \phi_X(L)\theta_W(L)v_t$$ $\blacksquare$
Este lema apresenta uma representa√ß√£o formal do lado direito da equa√ß√£o da soma de dois processos ARMA, demonstrando que √© um processo de m√©dias m√≥veis.

**Proposi√ß√£o 1**
Se os processos $X_t$ e $W_t$ s√£o estacion√°rios e independentes, ent√£o a soma $Y_t = X_t + W_t$ tamb√©m √© estacion√°ria.
*Prova:*
I.  A estacionaridade de um processo requer que a sua m√©dia e a sua autocovari√¢ncia sejam independentes do tempo.
II.  A m√©dia de $Y_t$ √© dada por:
$$E[Y_t] = E[X_t + W_t] = E[X_t] + E[W_t] = \mu_X + \mu_W$$
Como $X_t$ e $W_t$ s√£o estacion√°rios, suas m√©dias s√£o constantes, logo a m√©dia de $Y_t$ tamb√©m √© constante.
III. A autocovari√¢ncia de $Y_t$ no lag $j$ √© dada por:
$$\gamma_j^y = E[(Y_t - \mu_Y)(Y_{t-j} - \mu_Y)] =  E[(X_t - \mu_X + W_t - \mu_W)(X_{t-j} - \mu_X + W_{t-j} - \mu_W)]$$
IV. Devido √† independ√™ncia de $X_t$ e $W_t$, os termos cruzados se anulam:
$$\gamma_j^y =  E[(X_t - \mu_X)(X_{t-j} - \mu_X)] +  E[(W_t - \mu_W)(W_{t-j} - \mu_W)] = \gamma_j^x + \gamma_j^w$$
V.  Como $X_t$ e $W_t$ s√£o estacion√°rios, as autocovari√¢ncias $\gamma_j^x$ e $\gamma_j^w$ dependem apenas do lag $j$ e n√£o do tempo $t$, logo a autocovari√¢ncia de $Y_t$ tamb√©m √© independente do tempo.
VI. Portanto, a soma $Y_t$ √© um processo estacion√°rio. $\blacksquare$
Esta proposi√ß√£o formaliza que a soma de processos estacion√°rios independentes √© tamb√©m um processo estacion√°rio, um resultado importante em an√°lise de s√©ries temporais.
> üí° **Exemplo Num√©rico:** Vamos usar os processos definidos no exemplo anterior. O processo $X_t$ √© um ARMA(1,1) dado por $(1-0.5L)X_t = (1+0.3L)u_t$, onde $u_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia 1. Logo, $E[X_t] = 0$ se a m√©dia do processo de ru√≠do branco for zero. O processo $W_t$ √© um ARMA(2,1) dado por $(1-0.6L+0.2L^2)W_t = (1-0.2L)v_t$, com $v_t$ tamb√©m com m√©dia zero e vari√¢ncia unit√°ria, logo $E[W_t] = 0$. Desta forma, $E[Y_t] = E[X_t] + E[W_t] = 0 + 0 = 0$, que √© uma constante e n√£o depende do tempo, satisfazendo a condi√ß√£o de estacionaridade. De forma similar, as autocovari√¢ncias ser√£o independentes do tempo, tal como demonstrado na proposi√ß√£o anterior, indicando que $Y_t$ √© estacion√°rio.
> ```python
> # Verifica√ß√£o emp√≠rica da estacionaridade
> mean_x = np.mean(x_series)
> mean_w = np.mean(w_series)
> mean_y = np.mean(y_series)
>
> print(f"Mean of X_t: {mean_x:.4f}")
> print(f"Mean of W_t: {mean_w:.4f}")
> print(f"Mean of Y_t: {mean_y:.4f}")
>
> # Plotting autocorrelations
> from statsmodels.graphics.tsaplots import plot_acf
>
> fig, axes = plt.subplots(3, 1, figsize=(10, 10))
> plot_acf(x_series, ax=axes[0], title='Autocorrelation of X_t')
> plot_acf(w_series, ax=axes[1], title='Autocorrelation of W_t')
> plot_acf(y_series, ax=axes[2], title='Autocorrelation of Y_t')
> plt.tight_layout()
> plt.show()
> ```
> Este c√≥digo calcula a m√©dia dos processos $X_t, W_t, Y_t$ e mostra que as m√©dias s√£o constantes e as fun√ß√µes de autocorrela√ß√£o convergem para zero, indicando que as s√©ries simuladas s√£o estacion√°rias.

### Implica√ß√µes e Interpreta√ß√µes
A propriedade de que a FGAC da soma de processos ARMA √© a soma das FGACs dos processos individuais fornece uma ferramenta poderosa para analisar a estrutura temporal de modelos resultantes da combina√ß√£o linear de outros modelos. Al√©m disso, a an√°lise da representa√ß√£o polinomial de um ARMA resultante de uma soma permite a determina√ß√£o das ordens AR e MA do processo, embora os par√¢metros dos polin√¥mios resultantes n√£o possam ser obtidos por simples soma dos par√¢metros dos processos originais, como no caso da soma de processos AR ou MA puros.
A an√°lise deste cap√≠tulo formaliza as intui√ß√µes sobre como processos ARMA combinam suas propriedades temporais na soma. Os resultados estabelecem que a soma de processos ARMA independentes resulta em um novo processo ARMA, com par√¢metros que dependem de forma complexa dos par√¢metros dos processos componentes. As FGACs proporcionam uma vis√£o detalhada da combina√ß√£o, enquanto a forma polinomial dos operadores de defasagem permite uma manipula√ß√£o e an√°lise mais concisa das propriedades de modelos de s√©ries temporais.

### Conclus√£o
Este cap√≠tulo detalhou a soma de processos ARMA, demonstrando que a estrutura ARMA √© preservada na combina√ß√£o linear, embora com uma ordem que √© determinada pelo m√°ximo das ordens dos componentes individuais.  O resultado estabelece que a fun√ß√£o geradora de autocovari√¢ncia do processo resultante √© a soma das fun√ß√µes geradoras dos processos componentes, enquanto a forma polinomial do processo resultante tem a ordem dos componentes autoregressivos dada pela soma das ordens dos componentes AR e o componente MA dado pelo m√°ximo da combina√ß√£o das ordens AR e MA dos processos individuais. Este cap√≠tulo conclui a an√°lise sobre a soma de processos estoc√°sticos e suas propriedades temporais, oferecendo uma base formal para avan√ßar para t√≥picos como a estima√ß√£o de par√¢metros e a aplica√ß√£o de filtros em s√©ries temporais.

### Refer√™ncias
[^4.7.1]: ... *[Defini√ß√£o de um processo MA(1)]*
[^4.7.2]: ... *[Autocovari√¢ncias de um processo MA(1)]*
[^4.7.3]: ... *[Defini√ß√£o de ru√≠do branco]*
[^4.7.5]: ... *[Defini√ß√£o da s√©rie Y como soma de MA(1) e ru√≠do branco]*
[^4.7.7]: ... *[Representa√ß√£o MA(1) para Y]*
[^4.7.15]: ... *[Reescrita da representa√ß√£o MA(1) com u e v]*
[^4.7.16]: ... *[Lag distribu√≠do da serie epsilon]*
[^4.7.21]: ... *[Soma de MA resulta em MA]*
[^4.7.27]: ... *[Soma de AR(1) com AR(1) resulta em um ARMA(2,1)]*
[^4.8.4]: ... *[Representa√ß√£o de processos com polin√¥mios]*
<!-- END -->
