## A Adi√ß√£o de Processos de M√©dias M√≥veis: Uma Generaliza√ß√£o
### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise da adi√ß√£o de processos de m√©dias m√≥veis (MA) independentes, generalizando o resultado apresentado anteriormente da soma de um processo MA(1) com ru√≠do branco [^4.7.1], [^4.7.2], [^4.7.3], [^4.7.5], [^4.7.7], [^4.7.15], [^4.7.16], [^4.7.21]. O foco principal √© demonstrar como a combina√ß√£o linear de tais processos resulta em um novo processo MA, cuja ordem √© determinada pela m√°xima ordem entre os processos originais.

### Conceitos Fundamentais
#### Adi√ß√£o de Processos MA Independentes
Como vimos, a adi√ß√£o de um processo MA(1) com ru√≠do branco resulta em outro processo MA(1), o que sugere uma poss√≠vel generaliza√ß√£o para processos MA de ordem superior. Considere dois processos de m√©dias m√≥veis, $X_t$ e $W_t$, de ordens $q_1$ e $q_2$, respectivamente, definidos como:
$$X_t = (1 + \delta_1 L + \delta_2 L^2 + \ldots + \delta_{q_1} L^{q_1})u_t = \delta(L)u_t$$
$$W_t = (1 + \kappa_1 L + \kappa_2 L^2 + \ldots + \kappa_{q_2} L^{q_2})v_t = \kappa(L)v_t$$
onde $u_t$ e $v_t$ s√£o processos de ru√≠do branco n√£o correlacionados, e $L$ √© o operador de retardo. As autocovari√¢ncias de $X_t$ e $W_t$ s√£o n√£o nulas apenas para lags at√© $q_1$ e $q_2$, respetivamente [^4.7.6]. Assumimos que os dois processos s√£o independentes, o que significa que $E(X_t W_{t-j}) = 0$ para todos os lags $j$.
O processo resultante da soma √© dado por $Y_t = X_t + W_t$. Para entender a estrutura de $Y_t$, precisamos analisar suas autocovari√¢ncias e sua fun√ß√£o geradora de autocovari√¢ncia. A autocovari√¢ncia de $Y_t$ em lag $j$, denotada por $\gamma_j^y$, √© dada por:
$$E(Y_t Y_{t-j}) = E[(X_t + W_t)(X_{t-j} + W_{t-j})] = E(X_t X_{t-j}) + E(W_t W_{t-j}) + E(X_t W_{t-j}) + E(W_t X_{t-j})$$
Como $X_t$ e $W_t$ s√£o n√£o correlacionados, os termos cruzados desaparecem, e obtemos:
$$E(Y_t Y_{t-j}) = E(X_t X_{t-j}) + E(W_t W_{t-j}) = \gamma_j^x + \gamma_j^w$$
onde $\gamma_j^x$ e $\gamma_j^w$ s√£o as autocovari√¢ncias dos processos $X_t$ e $W_t$, respetivamente. Isso implica que a fun√ß√£o geradora de autocovari√¢ncia de $Y_t$ √© a soma das fun√ß√µes geradoras de autocovari√¢ncia de $X_t$ e $W_t$:
$$g_Y(z) = g_X(z) + g_W(z)$$ [^4.7.19]
onde $g_X(z)$ e $g_W(z)$ s√£o as fun√ß√µes geradoras de autocovari√¢ncia de $X_t$ e $W_t$, respetivamente.

> üí° **Exemplo Num√©rico:**
>  Considere um processo MA(1) $X_t$ com $\delta_1 = 0.8$ e um processo MA(2) $W_t$ com $\kappa_1 = 0.5$ e $\kappa_2 = -0.3$.
>  $$X_t = u_t + 0.8u_{t-1}$$
>  $$W_t = v_t + 0.5v_{t-1} - 0.3v_{t-2}$$
>  A autocovari√¢ncia de $X_t$ para lag 0 √© $\gamma_0^x = \sigma_u^2(1 + 0.8^2) = 1.64\sigma_u^2$, para lag 1 √© $\gamma_1^x = 0.8\sigma_u^2$ e para lags maiores √© 0.
>  A autocovari√¢ncia de $W_t$ para lag 0 √© $\gamma_0^w = \sigma_v^2(1 + 0.5^2 + (-0.3)^2) = 1.34\sigma_v^2$, para lag 1 √© $\gamma_1^w = (0.5 - 0.5*(-0.3))\sigma_v^2 = 0.65\sigma_v^2$, para lag 2 √© $\gamma_2^w = -0.3\sigma_v^2$, e para lags maiores √© 0.
>  Se $Y_t = X_t + W_t$, a autocovari√¢ncia de $Y_t$ √© a soma das autocovari√¢ncias, ou seja $\gamma_0^y = 1.64\sigma_u^2 + 1.34\sigma_v^2$, $\gamma_1^y = 0.8\sigma_u^2 + 0.65\sigma_v^2$, $\gamma_2^y = -0.3\sigma_v^2$, e $\gamma_j^y = 0$ para $j > 2$. Note que se $\sigma_u^2 = \sigma_v^2 = 1$, ent√£o $\gamma_0^y = 2.98$, $\gamma_1^y = 1.45$ and $\gamma_2^y = -0.3$. A ordem do processo MA resultante $Y_t$ ser√° 2.

#### Determina√ß√£o da Ordem do Processo MA Resultante
Seja $q = \max\{q_1, q_2\}$ o m√°ximo das ordens dos dois processos originais. Como vimos [^4.7.6], as autocovari√¢ncias de um processo MA s√£o zero al√©m do lag de sua ordem, e as autocovari√¢ncias de $Y_t$ s√£o zero para lags maiores que $q$, pois os processos $X_t$ e $W_t$ t√™m autocovari√¢ncias nulas ap√≥s $q_1$ e $q_2$, respetivamente. Portanto, $Y_t$ √© um processo MA de ordem $q$:
$$Y_t = (1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q)\epsilon_t = \theta(L)\epsilon_t$$
onde $\epsilon_t$ √© um processo de ru√≠do branco e os par√¢metros $\theta_i$ s√£o definidos de forma a igualar as fun√ß√µes geradoras de autocovari√¢ncia.
Este resultado generaliza a observa√ß√£o anterior de que a soma de um processo MA(1) com um ru√≠do branco resulta em outro processo MA(1) [^4.7.7]. Agora, podemos afirmar que a soma de quaisquer dois processos MA resulta em um novo processo MA cuja ordem √© dada pelo m√°ximo das ordens dos processos originais.

> üí° **Exemplo Num√©rico:**
> Vamos considerar o caso da soma de um processo MA(1) e um processo MA(2):
>  $$X_t = u_t + 0.7u_{t-1}$$
>  $$W_t = v_t + 0.5v_{t-1} - 0.3v_{t-2}$$
>  Aqui, $q_1 = 1$ e $q_2 = 2$.  Portanto, $q = \max(1,2) = 2$. A soma $Y_t = X_t + W_t$ √© um processo MA(2), que pode ser expressa como $Y_t = \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2}$, onde $\epsilon_t$ √© um novo ru√≠do branco e os par√¢metros $\theta_1$ e $\theta_2$ podem ser calculados usando as fun√ß√µes geradoras de autocovari√¢ncia ou resolvendo um sistema de equa√ß√µes.
> Para encontrar os par√¢metros $\theta_1$ e $\theta_2$,  precisar√≠amos igualar as autocovari√¢ncias de $Y_t$ nas duas representa√ß√µes, $\gamma_Y(k)$ obtido pela soma, e $\gamma_Y(k)$ obtido da representa√ß√£o MA(2). Este procedimento envolveria um sistema de equa√ß√µes n√£o linear, pois as autocovari√¢ncias de um processo MA dependem dos seus par√¢metros de forma n√£o linear.

#### A Natureza do Processo de Ru√≠do Branco Resultante
√â importante notar que, embora a soma $Y_t$ seja tamb√©m um processo MA, o processo de ru√≠do branco $\epsilon_t$ resultante √© diferente dos ru√≠dos brancos originais $u_t$ e $v_t$.  O processo $\epsilon_t$ pode ser visto como uma combina√ß√£o de valores presentes e passados dos ru√≠dos brancos originais, e as suas autocovari√¢ncias devem ser definidas de tal forma que a fun√ß√£o geradora de autocovari√¢ncia de $Y_t$ corresponda √† soma das fun√ß√µes geradoras de $X_t$ e $W_t$. A rela√ß√£o entre $\epsilon_t$, $u_t$ e $v_t$ √© obtida expandindo $Y_t$:
$$Y_t = \theta(L)\epsilon_t = \delta(L)u_t + \kappa(L)v_t$$
onde $\theta(L)$, $\delta(L)$ e $\kappa(L)$ s√£o polin√¥mios do operador de retardo. Essa rela√ß√£o define como os ru√≠dos brancos s√£o misturados para criar o novo ru√≠do branco $\epsilon_t$.

> üí° **Exemplo Num√©rico:**
>  Vamos examinar um caso espec√≠fico:
>  Sejam $X_t = u_t + \delta u_{t-1}$ e $W_t = v_t + \kappa v_{t-1}$. Ent√£o:
>  $$Y_t = u_t + \delta u_{t-1} + v_t + \kappa v_{t-1}$$
>  Se a soma puder ser representada por $Y_t = \epsilon_t + \theta \epsilon_{t-1}$, ent√£o
>   $\epsilon_t = (1+\theta L)^{-1}(u_t + \delta u_{t-1} + v_t + \kappa v_{t-1})$
>    A natureza da s√©rie $\epsilon_t$ √©, por conseguinte, uma lag infinita distribu√≠da dos ru√≠dos originais. Embora seja um ru√≠do branco, n√£o √© o mesmo ru√≠do branco de $u_t$ ou de $v_t$ individualmente. As autocovari√¢ncias de $\epsilon_t$ devem corresponder √† forma MA(1) resultante e incluir uma combina√ß√£o de $u_t$ e $v_t$.
>  Para um exemplo concreto, suponha que $\delta = 0.5$ e $\kappa = -0.3$, e $u_t$ e $v_t$ s√£o ru√≠dos brancos com vari√¢ncia 1. As autocovari√¢ncias de $Y_t$ s√£o:
>  $\gamma_0^y = 1 + 0.5^2 + 1 + (-0.3)^2 = 2.34$
>  $\gamma_1^y = 0.5 - 0.3 = 0.2$
>  Se $Y_t$ for um processo MA(1), $Y_t = \epsilon_t + \theta\epsilon_{t-1}$. Ent√£o $\gamma_0^y = \sigma_\epsilon^2(1 + \theta^2)$ e $\gamma_1^y = \sigma_\epsilon^2 \theta$.
>  $\sigma_\epsilon^2\theta = 0.2$ e $\sigma_\epsilon^2(1+\theta^2) = 2.34$. Resolvendo este sistema, obtemos um valor para $\theta$ e $\sigma_\epsilon^2$.

#### Formaliza√ß√£o Matem√°tica
Para formalizar estes resultados, podemos expressar a fun√ß√£o geradora de autocovari√¢ncia do processo $Y_t$ como:
$$g_Y(z) = g_X(z) + g_W(z)$$
onde $g_X(z)$ e $g_W(z)$ s√£o as fun√ß√µes geradoras de autocovari√¢ncia de $X_t$ e $W_t$, respectivamente. Se $X_t$ e $W_t$ s√£o processos MA de ordem $q_1$ e $q_2$, respetivamente, podemos escrever:
$$g_X(z) = \sigma_u^2\delta(z)\delta(z^{-1})$$
$$g_W(z) = \sigma_v^2\kappa(z)\kappa(z^{-1})$$
onde $\sigma_u^2$ e $\sigma_v^2$ s√£o as vari√¢ncias dos processos de ru√≠do branco $u_t$ e $v_t$, e $\delta(z)$ e $\kappa(z)$ s√£o os polin√¥mios correspondentes aos processos MA. Ent√£o,
$$g_Y(z) = \sigma_u^2\delta(z)\delta(z^{-1}) + \sigma_v^2\kappa(z)\kappa(z^{-1}).$$
A representa√ß√£o de $Y_t$ como um processo MA de ordem $q = \max\{q_1, q_2\}$ com fun√ß√£o geradora de autocovari√¢ncia $g_Y(z) = \sigma_\epsilon^2\theta(z)\theta(z^{-1})$ implica que devemos determinar os par√¢metros $\theta_i$ e $\sigma_\epsilon^2$ de modo que as duas formas de $g_Y(z)$ sejam equivalentes.

**Proposi√ß√£o 1**
A soma de processos MA independentes resulta em um novo processo MA cuja ordem √© o m√°ximo das ordens dos componentes originais.
*Prova:*
I. Sejam $X_t$ e $W_t$ dois processos de m√©dias m√≥veis independentes, de ordens $q_1$ e $q_2$, respectivamente, definidos por:
$$X_t = \delta(L)u_t$$
$$W_t = \kappa(L)v_t$$
onde $u_t$ e $v_t$ s√£o ru√≠dos brancos, e $\delta(L)$ e $\kappa(L)$ s√£o polin√¥mios de ordem $q_1$ e $q_2$ respetivamente.
II. Seja $Y_t = X_t + W_t$.
III. A fun√ß√£o geradora de autocovari√¢ncia de $Y_t$ √© dada por:
$$g_Y(z) = g_X(z) + g_W(z) = \sigma_u^2\delta(z)\delta(z^{-1}) + \sigma_v^2\kappa(z)\kappa(z^{-1})$$
IV. Seja $q = \max\{q_1, q_2\}$.
V. Pelo resultado [^4.7.21], sabemos que o lado direito pode ser expresso como um processo de m√©dia m√≥vel de ordem q com vari√¢ncia $\sigma^2_\epsilon$ e polin√¥mio $\theta(L)$ que depende de $\delta(L)$ e $\kappa(L)$. Assim:
$$g_Y(z) = \sigma^2_\epsilon \theta(z)\theta(z^{-1})$$
VI. Portanto, $Y_t$ segue um processo de m√©dia m√≥vel de ordem q, onde $q = \max\{q_1, q_2\}$. $\blacksquare$

**Teorema 1**
A fun√ß√£o de autocovari√¢ncia da soma de dois processos MA independentes √© a soma das fun√ß√µes de autocovari√¢ncia dos processos individuais.
*Prova:*
I. Sejam $X_t$ e $W_t$ dois processos MA independentes.
II. A autocovari√¢ncia de $Y_t$ em lag $k$ √© dada por $Cov(Y_t, Y_{t-k}) = Cov(X_t + W_t, X_{t-k} + W_{t-k})$.
III. Devido √† independ√™ncia de $X_t$ e $W_t$, temos
$$Cov(Y_t, Y_{t-k}) = Cov(X_t, X_{t-k}) + Cov(W_t, W_{t-k}) = \gamma_X(k) + \gamma_W(k)$$
IV. Portanto, a fun√ß√£o de autocovari√¢ncia de $Y_t$ √© a soma das fun√ß√µes de autocovari√¢ncia de $X_t$ e $W_t$. $\blacksquare$
Este teorema estabelece formalmente que a fun√ß√£o de autocovari√¢ncia do processo somado $Y_t$ √© a soma das autocovari√¢ncias dos processos individuais, um resultado fundamental para a an√°lise de sua estrutura temporal.

> üí° **Exemplo Num√©rico:**
> Retomando o exemplo com $X_t = u_t + 0.8u_{t-1}$ e $W_t = v_t + 0.5v_{t-1} - 0.3v_{t-2}$, onde $u_t$ e $v_t$ s√£o ru√≠dos brancos com vari√¢ncia 1.
>  As autocovari√¢ncias $\gamma^x$ para lags 0, 1 e superiores s√£o 1.64, 0.8, e 0 respectivamente.
>  As autocovari√¢ncias $\gamma^w$ para lags 0, 1, 2 e superiores s√£o 1.34, 0.65, -0.3, e 0 respectivamente.
>  As autocovari√¢ncias do processo somado $Y_t$ ser√£o $\gamma^y(0) = 1.64 + 1.34 = 2.98$, $\gamma^y(1) = 0.8 + 0.65 = 1.45$, $\gamma^y(2) = 0 - 0.3 = -0.3$, e $\gamma^y(k) = 0$ para $k>2$.
>  Este exemplo ilustra o teorema de forma clara, mostrando que a autocovari√¢ncia da soma √©, de facto, a soma das autocovari√¢ncias individuais para cada lag.

**Lema 1**
Se $X_t$ √© um processo MA(q1) e $W_t$ √© um processo MA(q2), e a soma $Y_t = X_t + W_t$ √© um processo MA(q), ent√£o $q = \max\{q_1, q_2\}$.
*Prova:*
I. Como $X_t$ √© MA(q1), suas autocovari√¢ncias s√£o zero para todos os lags maiores que $q_1$.
II. Similarmente, as autocovari√¢ncias de $W_t$ s√£o zero para todos os lags maiores que $q_2$.
III. Dado que $Y_t = X_t + W_t$, a fun√ß√£o de autocovari√¢ncia de $Y_t$ √© a soma das fun√ß√µes de autocovari√¢ncia de $X_t$ e $W_t$.
IV. Assim, as autocovari√¢ncias de $Y_t$ ser√£o zero para lags maiores que o m√°ximo das ordens $q_1$ e $q_2$.
V. Portanto, a ordem do processo MA resultante $Y_t$ √© $q = \max\{q_1, q_2\}$. $\blacksquare$
Este lema formaliza o resultado principal da adi√ß√£o de processos MA, garantindo que a ordem do novo processo seja determinada pelo maior valor entre as ordens dos componentes individuais.

**Lema 1.1**
Seja $X_t$ um processo MA($q_1$) e $W_t$ um processo MA($q_2$). Se $q_1=q_2=q$, e $X_t$ e $W_t$ s√£o independentes, ent√£o $Y_t = X_t + W_t$ √© um processo MA($q$).
*Prova:*
I. Sendo $X_t$ um processo MA($q_1$), suas autocovari√¢ncias s√£o nulas para lags maiores que $q_1$. Similarmente, sendo $W_t$ um processo MA($q_2$), suas autocovari√¢ncias s√£o nulas para lags maiores que $q_2$.
II. Se $q_1=q_2=q$, ent√£o ambas as autocovari√¢ncias s√£o nulas para lags maiores que $q$.
III. Sabemos que a autocovari√¢ncia de $Y_t$ √© a soma das autocovari√¢ncias de $X_t$ e $W_t$ (Teorema 1).
IV. Portanto, as autocovari√¢ncias de $Y_t$ s√£o nulas para lags maiores que $q$, implicando que $Y_t$ √© um processo MA($q$). $\blacksquare$

**Observa√ß√£o 1**
A independ√™ncia dos processos $X_t$ e $W_t$ √© crucial para a validade do Teorema 1 e do Lema 1. Se $X_t$ e $W_t$ fossem correlacionados, os termos cruzados $E(X_t W_{t-j})$ e $E(W_t X_{t-j})$ n√£o seriam nulos, e a autocovari√¢ncia de $Y_t$ n√£o seria simplesmente a soma das autocovari√¢ncias de $X_t$ e $W_t$. Nesse caso, a estrutura do processo resultante seria mais complexa e n√£o necessariamente um processo MA.

**Teorema 1.1**
Se $X_t$ e $W_t$ s√£o processos MA independentes, ent√£o a fun√ß√£o geradora de autocovari√¢ncia da soma $Y_t = X_t + W_t$ √© a soma das fun√ß√µes geradoras de autocovari√¢ncia de $X_t$ e $W_t$.
*Prova:*
I. Pela defini√ß√£o, a fun√ß√£o geradora de autocovari√¢ncia de um processo $Z_t$ √© definida como $g_Z(z) = \sum_{k=-\infty}^{\infty} \gamma_Z(k)z^k$, onde $\gamma_Z(k)$ √© a autocovari√¢ncia de $Z_t$ no lag $k$.
II. Sabemos pelo Teorema 1 que a autocovari√¢ncia de $Y_t$ no lag $k$ √© $\gamma_Y(k) = \gamma_X(k) + \gamma_W(k)$.
III. Portanto, a fun√ß√£o geradora de autocovari√¢ncia de $Y_t$ √© dada por:
$$g_Y(z) = \sum_{k=-\infty}^{\infty} \gamma_Y(k)z^k = \sum_{k=-\infty}^{\infty} (\gamma_X(k) + \gamma_W(k))z^k$$
IV. Pela linearidade da soma, temos:
$$g_Y(z) = \sum_{k=-\infty}^{\infty} \gamma_X(k)z^k + \sum_{k=-\infty}^{\infty} \gamma_W(k)z^k = g_X(z) + g_W(z)$$
V. Assim, a fun√ß√£o geradora de autocovari√¢ncia de $Y_t$ √© a soma das fun√ß√µes geradoras de autocovari√¢ncia de $X_t$ e $W_t$. $\blacksquare$

### Conclus√£o
Este cap√≠tulo estabeleceu, atrav√©s de argumentos, provas e exemplos num√©ricos, que a soma de processos MA independentes resulta em um novo processo MA, cuja ordem √© igual ao m√°ximo das ordens dos processos originais. O cap√≠tulo tamb√©m explorou a natureza do processo de ru√≠do branco resultante da soma e destacou a equival√™ncia da fun√ß√£o geradora de autocovari√¢ncia do processo resultante com a soma das fun√ß√µes geradoras dos processos originais. Esses resultados formam uma base s√≥lida para a compreens√£o da combina√ß√£o de modelos de s√©ries temporais e para a aplica√ß√£o de t√©cnicas avan√ßadas de an√°lise de dados. A compreens√£o de tais mecanismos √© fundamental para previs√µes e modelagens precisas e robustas em diversas aplica√ß√µes.

### Refer√™ncias
[^4.7.1]: ... *[Defini√ß√£o de um processo MA(1)]*
[^4.7.2]: ... *[Autocovari√¢ncias de um processo MA(1)]*
[^4.7.3]: ... *[Defini√ß√£o de ru√≠do branco]*
[^4.7.5]: ... *[Defini√ß√£o da s√©rie Y como soma de MA(1) e ru√≠do branco]*
[^4.7.7]: ... *[Representa√ß√£o MA(1) para Y]*
[^4.7.15]: ... *[Reescrita da representa√ß√£o MA(1) com u e v]*
[^4.7.16]: ... *[Lag distribu√≠do da serie epsilon]*
[^4.7.21]: ... *[Soma de MA resulta em MA]*
<!-- END -->
