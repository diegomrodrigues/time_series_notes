## Somas de Processos Autorregressivos: Uma An√°lise Detalhada
### Introdu√ß√£o
Este cap√≠tulo explora as consequ√™ncias da adi√ß√£o de processos estoc√°sticos, com foco particular em processos autorregressivos (AR). Conforme visto anteriormente [^4.7.27], a soma de dois processos AR resulta em um processo Autorregressivo de M√©dia M√≥vel (ARMA). Este cap√≠tulo detalha essa deriva√ß√£o e explora as implica√ß√µes dessa combina√ß√£o linear, fornecendo uma an√°lise aprofundada para um p√∫blico com conhecimento avan√ßado em matem√°tica, modelos estat√≠sticos, otimiza√ß√£o e an√°lise de dados. Expandindo o conte√∫do previamente abordado, iremos detalhar os mecanismos matem√°ticos por tr√°s dessas combina√ß√µes de processos.

### Conceitos Fundamentais
#### Deriva√ß√£o da Soma de Processos AR
Como previamente estabelecido [^4.7.22], [^4.7.23], considere dois processos AR(1), $X_t$ e $W_t$, definidos como:
$$(1-\pi L)X_t = u_t$$
$$(1-\rho L)W_t = v_t$$
onde $u_t$ e $v_t$ s√£o ru√≠dos brancos n√£o correlacionados. A opera√ß√£o $L$ √© o operador de retardo.  Se observarmos a soma $Y_t = X_t + W_t$, e desejarmos prever $Y_{t+1}$ com base em seus valores atrasados, podemos analisar o que acontece com a soma desses dois processos. Se por acaso, os dois processos AR compartilham o mesmo par√¢metro autorregressivo $\pi = \rho$, ent√£o a soma $Y_t$ resulta diretamente em um processo AR(1), dado por:
$$(1-\pi L)Y_t = \epsilon_t$$
onde $\epsilon_t = u_t + v_t$ √© um novo ru√≠do branco [^4.7.21].

> üí° **Exemplo Num√©rico:** Suponha que temos dois processos AR(1), $X_t$ e $W_t$, com $\pi = \rho = 0.5$. Os ru√≠dos brancos s√£o $u_t$ e $v_t$. Se $X_t = 0.5X_{t-1} + u_t$ e $W_t = 0.5W_{t-1} + v_t$, ent√£o a soma $Y_t = X_t + W_t$ segue $Y_t = 0.5Y_{t-1} + \epsilon_t$ onde $\epsilon_t = u_t + v_t$. Para ilustrar, vamos considerar valores espec√≠ficos:
> - $X_0 = 10$, $W_0 = 5$, $u_1 = 2$, $v_1 = -1$,
> - $X_1 = 0.5 * 10 + 2 = 7$, $W_1 = 0.5 * 5 - 1 = 1.5$
> - $Y_0 = 10 + 5 = 15$, $Y_1 = 7 + 1.5 = 8.5$
>
> Se $Y_t = 0.5Y_{t-1} + \epsilon_t$, ent√£o $Y_1 = 0.5 * 15 + (2-1) = 7.5 + 1 = 8.5$. A soma $Y_t$ preserva a estrutura AR(1) com o mesmo par√¢metro.

No caso mais geral onde $\pi \neq \rho$, podemos multiplicar a primeira equa√ß√£o por $(1-\rho L)$ e a segunda por $(1-\pi L)$, resultando em:
$$(1-\rho L)(1-\pi L)X_t = (1-\rho L)u_t$$
$$(1-\pi L)(1-\rho L)W_t = (1-\pi L)v_t.$$
Somando as duas equa√ß√µes, obtemos:
$$(1-\rho L)(1-\pi L)(X_t + W_t) = (1-\rho L)u_t + (1-\pi L)v_t.$$
Substituindo $Y_t = X_t + W_t$, temos:
$$(1-\rho L)(1-\pi L)Y_t = (1-\rho L)u_t + (1-\pi L)v_t.$$
O lado direito da equa√ß√£o representa uma soma de dois processos MA(1), que, como visto anteriormente, resulta em um processo MA(1), assim:
$$(1 - \phi_1 L - \phi_2 L^2)Y_t = (1 + \theta L)\epsilon_t.$$
Expandindo a equa√ß√£o, temos:
$$(1 - (\pi+\rho)L + \pi \rho L^2)Y_t = (1 + \theta L)\epsilon_t.$$
Portanto, $Y_t$ segue um processo ARMA(2,1), onde o lado esquerdo representa a parte autorregressiva (AR) e o lado direito a parte da m√©dia m√≥vel (MA), onde
$\phi_1 = \pi + \rho$, $\phi_2 = -\pi\rho$, e os par√¢metros $\theta$ e $\sigma^2_\epsilon$ s√£o obtidos combinando os processos de ru√≠do branco $u_t$ e $v_t$.

> üí° **Exemplo Num√©rico:** Sejam $\pi = 0.8$ e $\rho = 0.3$. Ent√£o, $\phi_1 = 0.8 + 0.3 = 1.1$ e $\phi_2 = -(0.8 * 0.3) = -0.24$. A equa√ß√£o se torna: $(1 - 1.1L + 0.24L^2)Y_t = (1 + \theta L)\epsilon_t$. O par√¢metro $\theta$ depender√° das vari√¢ncias dos ru√≠dos brancos $u_t$ e $v_t$, e $\epsilon_t$ √© o novo ru√≠do resultante. Para calcular $\theta$, podemos usar as autocovari√¢ncias dos processos e resolver um sistema de equa√ß√µes, como discutido em [^4.7.10], [^4.7.11] e [^4.7.12].

#### Generaliza√ß√£o para Processos AR de Ordem Superior
A an√°lise acima pode ser generalizada para processos AR de ordem superior. Sejam $X_t$ e $W_t$ dois processos autorregressivos independentes de ordens $p_1$ e $p_2$, respectivamente, com
$$ \pi(L)X_t = u_t$$
$$ \rho(L)W_t = v_t$$
onde $\pi(L) = 1 - \pi_1L - \ldots - \pi_{p_1}L^{p_1}$ e $\rho(L) = 1 - \rho_1L - \ldots - \rho_{p_2}L^{p_2}$.
Para obter o processo resultante da soma $Y_t = X_t + W_t$, aplicamos $\rho(L)$ na primeira equa√ß√£o e $\pi(L)$ na segunda:
$$ \rho(L)\pi(L)X_t = \rho(L)u_t$$
$$ \pi(L)\rho(L)W_t = \pi(L)v_t$$
Somando as equa√ß√µes, temos:
$$ \pi(L)\rho(L)(X_t + W_t) = \rho(L)u_t + \pi(L)v_t$$
Portanto,
$$ \phi(L)Y_t = \theta(L)\epsilon_t$$
onde $\phi(L) = \pi(L)\rho(L)$ √© um polin√¥mio de ordem $p_1 + p_2$, e  $\theta(L)\epsilon_t = \rho(L)u_t + \pi(L)v_t$ √© um processo MA de ordem m√°xima $\max(p_1, p_2)$.  O resultado √© um processo ARMA com ordem $(p_1 + p_2, \max(p_1,p_2))$ .

> üí° **Exemplo Num√©rico:** Sejam $X_t$ um processo AR(2) e $W_t$ um processo AR(1). Ent√£o, $p_1=2$ e $p_2=1$. A soma $Y_t$ ser√° um processo ARMA(3,2). Especificamente, se $X_t = 0.5X_{t-1} - 0.2X_{t-2} + u_t$ e $W_t = 0.7W_{t-1} + v_t$, ent√£o
> $\pi(L) = 1 - 0.5L + 0.2L^2$
> $\rho(L) = 1 - 0.7L$
> $\phi(L) = \pi(L)\rho(L) = (1 - 0.5L + 0.2L^2)(1 - 0.7L) = 1 - 1.2L + 0.55L^2 - 0.14L^3$
> e o lado MA ser√° um processo MA(2).

#### Consequ√™ncias
A adi√ß√£o de processos AR independentes n√£o preserva as propriedades puramente AR dos processos originais. Em vez disso, introduz uma componente MA na estrutura do novo processo, levando a um processo ARMA. O Teorema 1 detalha esta propriedade.

**Teorema 1**
A soma de um processo ARMA($p_1, q_1$) e um processo ARMA($p_2, q_2$), que s√£o n√£o correlacionados entre si em todos os lags e leads, resulta em um processo ARMA de ordem no m√°ximo ($p_1 + p_2$, $\max\{p_1, p_2, q_1, q_2\}$).
*Prova:*
I. Sejam $X_t$ um processo ARMA($p_1, q_1$) e $W_t$ um processo ARMA($p_2, q_2$) n√£o correlacionados, expressos como:
$$\phi(B)X_t = \theta(B)a_t$$
$$\psi(B)W_t = \omega(B)b_t$$
onde $\{a_t\}$ e $\{b_t\}$ s√£o ru√≠dos brancos n√£o correlacionados.
II. Seja $Y_t = X_t + W_t$. Ent√£o,
$$Y_t = \frac{\theta(B)}{\phi(B)}a_t + \frac{\omega(B)}{\psi(B)}b_t$$
III. Seja $\gamma_Y(k)$ a fun√ß√£o de autocovari√¢ncia de $Y_t$. Ent√£o,
$$\gamma_Y(k) = Cov(Y_t, Y_{t-k}) = Cov(X_t + W_t, X_{t-k} + W_{t-k})$$
IV. Como $X_t$ e $W_t$ s√£o n√£o correlacionados, temos
$$\gamma_Y(k) = Cov(X_t, X_{t-k}) + Cov(W_t, W_{t-k}) = \gamma_X(k) + \gamma_W(k)$$
onde $\gamma_X(k)$ e $\gamma_W(k)$ s√£o as fun√ß√µes de autocovari√¢ncia de $X_t$ e $W_t$ respectivamente. ‚ñ†
Este teorema estabelece que a combina√ß√£o linear de dois processos ARMA resulta em um novo processo cuja estrutura se torna mais complexa, geralmente incluindo componentes AR e MA, sendo um processo ARMA.

**Lema 1.1**
Se $X_t$ e $W_t$ s√£o processos AR(1) como definidos acima, e s√£o independentes, ent√£o sua soma $Y_t = X_t + W_t$ tem fun√ß√£o de autocovari√¢ncia dada por $\gamma_Y(k) = \gamma_X(k) + \gamma_W(k)$, onde $\gamma_X(k)$ e $\gamma_W(k)$ s√£o as fun√ß√µes de autocovari√¢ncia de $X_t$ e $W_t$ respectivamente.
*Prova:*
I. Como $X_t$ e $W_t$ s√£o independentes, $Cov(X_t, W_s) = 0$ para todos $t$ e $s$.
II. Assim,
\begin{align*}
\gamma_Y(k) &= Cov(Y_t, Y_{t-k}) \\
&= Cov(X_t + W_t, X_{t-k} + W_{t-k}) \\
&= Cov(X_t, X_{t-k}) + Cov(X_t, W_{t-k}) + Cov(W_t, X_{t-k}) + Cov(W_t, W_{t-k}) \\
&= Cov(X_t, X_{t-k}) + Cov(W_t, W_{t-k}) \\
&= \gamma_X(k) + \gamma_W(k).
\end{align*}
‚ñ†
Este lema formaliza a ideia de que a fun√ß√£o de autocovari√¢ncia da soma de processos independentes √© a soma de suas autocovari√¢ncias, um resultado fundamental para a an√°lise da estrutura temporal de $Y_t$.

> üí° **Exemplo Num√©rico:** Seja $X_t$ um processo AR(1) com $\pi = 0.6$ e $\sigma^2_u = 1$, e $W_t$ um processo AR(1) com $\rho = 0.4$ e $\sigma^2_v = 2$. As autocovari√¢ncias de $X_t$ e $W_t$ s√£o $\gamma_X(k) = \frac{\sigma^2_u \pi^k}{1 - \pi^2}$ e $\gamma_W(k) = \frac{\sigma^2_v \rho^k}{1 - \rho^2}$. Para $k=1$, $\gamma_X(1) = \frac{1 * 0.6}{1 - 0.6^2} = 0.9375$ e $\gamma_W(1) = \frac{2 * 0.4}{1 - 0.4^2} = 0.9524$. Logo, $\gamma_Y(1) = 0.9375 + 0.9524 = 1.8899$. A autocovari√¢ncia de $Y_t$ no lag 1 √© a soma das autocovari√¢ncias dos dois processos individuais no lag 1.

**Teorema 1.1**
A soma de dois processos AR(p) e AR(q) resulta em um processo ARMA(p+q, max(p,q)).
*Prova:* Este teorema √© uma generaliza√ß√£o direta da an√°lise feita anteriormente com os processos AR(1) e a generaliza√ß√£o para AR(p) e AR(q) apresentada na se√ß√£o "Generaliza√ß√£o para Processos AR de Ordem Superior".  Sejam $X_t$ e $W_t$ processos AR(p) e AR(q), respectivamente, podemos escrever $\phi(L)X_t = u_t$ e $\psi(L)W_t = v_t$. O processo resultante da soma $Y_t = X_t + W_t$ tem representa√ß√£o dada por $\phi(L)\psi(L)Y_t = \psi(L)u_t + \phi(L)v_t$. O lado esquerdo tem ordem $p+q$, enquanto o lado direito tem ordem $\max(p,q)$ em termos dos polin√¥mios em $L$ (operador de retardo). Portanto, $Y_t$ √© um processo ARMA(p+q, max(p,q)). ‚ñ†

**Proposi√ß√£o 1.2**
Se os processos AR envolvidos na soma possuem ra√≠zes caracter√≠sticas id√™nticas, a ordem da componente AR do processo resultante pode ser reduzida.
*Prova:*
I. Considere o caso em que $\pi(L)$ e $\rho(L)$ compartilham algumas ra√≠zes. Isso significa que os polin√¥mios $\pi(L)$ e $\rho(L)$ possuem fatores comuns, digamos $\alpha(L)$.
II. Ent√£o $\pi(L) = \alpha(L)\pi'(L)$ e $\rho(L) = \alpha(L)\rho'(L)$.
III. Assim, $\pi(L)\rho(L) = \alpha(L)^2\pi'(L)\rho'(L)$, o que implica que a ordem do polin√¥mio do lado AR √© reduzida devido ao fator $\alpha(L)^2$.  O lado MA, entretanto, ainda √© dado por $\rho(L)u_t + \pi(L)v_t$. A ordem da MA depende das ordens de $\pi(L)$ e $\rho(L)$, sem ser afetada pela simplifica√ß√£o do lado AR.
IV. Por exemplo, se ambos $X_t$ e $W_t$ s√£o AR(1) com o mesmo par√¢metro $\pi$, ent√£o o polin√¥mio AR resultante ser√° $(1-\pi L)^2$. No entanto, a ordem do polin√¥mio AR ser√° efetivamente 1, e n√£o 2, uma vez que a equa√ß√£o se simplifica para $(1-\pi L)Y_t = \epsilon_t$, onde $\epsilon_t$ √© uma soma de ru√≠dos brancos, portanto um ru√≠do branco. ‚ñ†
Este resultado destaca a import√¢ncia de analisar as ra√≠zes caracter√≠sticas dos polin√¥mios AR ao somar processos.

> üí° **Exemplo Num√©rico:**  Se ambos os processos s√£o AR(1) com $\pi = \rho = 0.7$, ent√£o $\pi(L) = \rho(L) = 1 - 0.7L$. Logo, $\pi(L)\rho(L) = (1-0.7L)^2 = 1 - 1.4L + 0.49L^2$. Entretanto, ao somar os processos, o resultado ser√° um AR(1), e n√£o um AR(2), conforme explicado na introdu√ß√£o, pois $(1 - 0.7L)Y_t = \epsilon_t$, onde $\epsilon_t = u_t + v_t$, um ru√≠do branco.

### Conclus√£o
Este cap√≠tulo demonstrou como a adi√ß√£o de processos AR independentes resulta em um processo ARMA. Os par√¢metros do novo processo ARMA dependem dos par√¢metros dos processos AR originais, bem como das vari√¢ncias dos ru√≠dos brancos. Conforme discutido e provado anteriormente, a soma de processos estacion√°rios n√£o correlacionados resulta num novo processo, cuja fun√ß√£o geradora de autocovari√¢ncia √© a soma das fun√ß√µes geradoras de autocovari√¢ncia dos processos individuais. Expandindo os resultados previamente discutidos, os resultados deste cap√≠tulo enfatizam a complexidade inerente na modelagem de s√©ries temporais quando diferentes processos s√£o combinados, exigindo uma considera√ß√£o cuidadosa da ordem dos processos e da natureza das inova√ß√µes. A compreens√£o desses mecanismos √© crucial para a modelagem e previs√£o em diversas aplica√ß√µes.
### Refer√™ncias
[^4.7.1]: ... *[Defini√ß√£o de um processo MA(1)]*
[^4.7.2]: ... *[Autocovari√¢ncias de um processo MA(1)]*
[^4.7.3]: ... *[Defini√ß√£o de ru√≠do branco]*
[^4.7.4]: ... *[N√£o correla√ß√£o entre u e v]*
[^4.7.5]: ... *[Defini√ß√£o da s√©rie Y como soma de MA(1) e ru√≠do branco]*
[^4.7.6]: ... *[Autocovari√¢ncias de Y]*
[^4.7.7]: ... *[Representa√ß√£o MA(1) para Y]*
[^4.7.8]: ... *[Condi√ß√£o para autocovari√¢ncia de ordem 0]*
[^4.7.9]: ... *[Condi√ß√£o para autocovari√¢ncia de ordem 1]*
[^4.7.10]: ... *[Solu√ß√£o para sigma^2]*
[^4.7.11]: ... *[Equa√ß√£o quadr√°tica para theta]*
[^4.7.12]: ... *[F√≥rmula quadr√°tica]*
[^4.7.13]: ... *[Caso em que sigma_v √© igual a zero]*
[^4.7.14]: ... *[Solu√ß√£o invert√≠vel e n√£o invert√≠vel]*
[^4.7.15]: ... *[Reescrita da representa√ß√£o MA(1) com u e v]*
[^4.7.16]: ... *[Lag distribu√≠do da serie epsilon]*
[^4.7.17]: ... *[Fun√ß√£o geradora de autocovari√¢ncia de Y]*
[^4.7.18]: ... *[Fun√ß√£o geradora de autocovari√¢ncia de epsilon]*
[^4.7.19]: ... *[Fun√ß√£o geradora de autocovari√¢ncia da soma de dois processos]*
[^4.7.20]: ... *[Condi√ß√£o para que Y seja MA(q)]*
[^4.7.21]: ... *[Soma de MA resulta em MA]*
[^4.7.22]: ... *[Defini√ß√£o de um processo AR(1) para X]*
[^4.7.23]: ... *[Defini√ß√£o de um processo AR(1) para W]*
[^4.7.24]: ... *[Multiplica√ß√£o da equa√ß√£o por (1-rhoL)]*
[^4.7.25]: ... *[Multiplica√ß√£o da equa√ß√£o por (1-piL)]*
[^4.7.26]: ... *[Soma das equa√ß√µes resultando num MA(1)]*
[^4.7.27]: ... *[Soma de AR(1) com AR(1) resulta em um ARMA(2,1)]*
<!-- END -->
