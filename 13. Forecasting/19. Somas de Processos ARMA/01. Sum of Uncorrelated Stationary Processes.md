## Somas de Processos ARMA
### Introdu√ß√£o
Este cap√≠tulo aprofunda a natureza das s√©ries que resultam da adi√ß√£o de dois processos ARMA diferentes, come√ßando com um exemplo instrutivo. O objetivo √© entender como as propriedades de s√©ries temporais se comportam quando combinadas linearmente. Como vimos anteriormente [^4.7.1], [^4.7.2], [^4.7.3] processos de s√©ries temporais podem ser descritos por suas autocovari√¢ncias e atrav√©s de modelos MA, AR ou ARMA. Este cap√≠tulo visa explorar o que acontece quando somamos tais processos.

### Conceitos Fundamentais
#### Soma de um Processo MA(1) com Ru√≠do Branco
Considere uma s√©rie $X_t$ que segue um processo MA(1) com m√©dia zero:
$$X_t = u_t + \delta u_{t-1},$$ [^4.7.1]
onde $u_t$ √© ru√≠do branco. As autocovari√¢ncias de $X_t$ s√£o:
$$E(X_tX_{t-j}) = \begin{cases}
(1+\delta^2)\sigma^2 & \text{para } j=0 \\
\delta\sigma^2 & \text{para } j=\pm 1 \\
0 & \text{caso contr√°rio.}
\end{cases}$$ [^4.7.2]
Seja $v_t$ uma s√©rie de ru√≠do branco separada:
$$E(v_t v_{t-j}) = \begin{cases}
\sigma_v^2 & \text{para } j=0 \\
0 & \text{caso contr√°rio.}
\end{cases}$$ [^4.7.3]
Assumindo que $u_t$ e $v_t$ s√£o n√£o correlacionados em todos os leads e lags [^4.7.4], temos:
$$E(u_t v_{t-j})=0$$
e
$$E(X_t v_{t-j})=0,$$
para todo $j$. Uma s√©rie observada $Y_t$ representa a soma do processo MA(1) e do ru√≠do branco:
$$Y_t = X_t + v_t = u_t + \delta u_{t-1} + v_t.$$ [^4.7.5]
A quest√£o que se coloca √©: quais s√£o as propriedades da s√©rie temporal de $Y_t$? Claramente, $Y_t$ tem m√©dia zero, e suas autocovari√¢ncias podem ser deduzidas de [^4.7.2] atrav√©s de [^4.7.4]:
$$E(Y_t Y_{t-j}) = E(X_t + v_t)(X_{t-j} + v_{t-j}) = E(X_t X_{t-j}) + E(v_t v_{t-j}) = \begin{cases}
(1+\delta^2)\sigma^2 + \sigma_v^2 & \text{para } j=0 \\
\delta\sigma^2 & \text{para } j=\pm 1 \\
0 & \text{caso contr√°rio.}
\end{cases}$$ [^4.7.6]
Assim, a soma $X_t + v_t$ √© estacion√°ria por covari√¢ncia, e suas autocovari√¢ncias s√£o zero al√©m de um lag, assim como as de um MA(1). Poder√≠amos ent√£o perguntar se existe uma representa√ß√£o MA(1) com m√©dia zero para $Y_t$:
$$Y_t = \epsilon_t + \theta \epsilon_{t-1},$$ [^4.7.7]
com
$$E(\epsilon_t \epsilon_{t-j}) = \begin{cases}
\sigma_\epsilon^2 & \text{para } j=0 \\
0 & \text{caso contr√°rio.}
\end{cases}$$
cujas autocovari√¢ncias correspondem √†s obtidas em [^4.7.6]. As autocovari√¢ncias de [^4.7.7] seriam:
$$E(Y_t Y_{t-j}) = \begin{cases}
(1+\theta^2)\sigma_\epsilon^2 & \text{para } j=0 \\
\theta\sigma_\epsilon^2 & \text{para } j=\pm 1 \\
0 & \text{caso contr√°rio.}
\end{cases}$$
Para ser consistente com [^4.7.6], deve haver o caso em que:
$$(1+\theta^2)\sigma_\epsilon^2 = (1+\delta^2)\sigma^2 + \sigma_v^2$$ [^4.7.8]
e
$$\theta\sigma_\epsilon^2 = \delta\sigma^2.$$ [^4.7.9]
A equa√ß√£o [^4.7.9] pode ser resolvida para $\sigma_\epsilon^2$:
$$\sigma_\epsilon^2 = \delta\sigma^2/\theta,$$ [^4.7.10]
e ent√£o substitu√≠da em [^4.7.8] para deduzir:
$$(1+\theta^2)(\delta\sigma^2/\theta) = (1+\delta^2)\sigma^2 + \sigma_v^2$$
$$(1+\theta^2)\delta = [(1+\delta^2) + (\sigma_v^2/\sigma^2)]\theta$$
$$\delta\theta^2 - [(1+\delta^2) + (\sigma_v^2/\sigma^2)]\theta + \delta = 0.$$ [^4.7.11]
Para valores dados de $\delta$, $\sigma^2$ e $\sigma_v^2$, dois valores de $\theta$ que satisfazem [^4.7.11] podem ser encontrados a partir da f√≥rmula quadr√°tica [^4.7.12]. Se $\sigma_v^2$ fosse igual a zero, a equa√ß√£o quadr√°tica em [^4.7.11] seria simplesmente:
$$\delta\theta^2 - (1+\delta^2)\theta + \delta = \delta(\theta-\delta)(\theta-\delta^{-1}) = 0,$$ [^4.7.13]
cujas solu√ß√µes s√£o $\theta = \delta$ e $\theta = \delta^{-1}$, o par√¢metro da m√©dia m√≥vel para $X_t$ a partir das representa√ß√µes invert√≠vel e n√£o invert√≠vel, respectivamente. Para $\sigma_v^2 > 0$, a equa√ß√£o [^4.7.11] sempre apresentar√° duas solu√ß√µes reais para $\theta$, uma solu√ß√£o invert√≠vel $\theta^*$ que satisfaz:
$$0<|\theta^*|<|\delta|,$$ [^4.7.14]
e uma solu√ß√£o n√£o invert√≠vel $\bar{\theta}$ caracterizada por:
$$1<|\delta^{-1}|<|\bar{\theta}|.$$
Tomando os valores associados com a representa√ß√£o invert√≠vel ($\theta^*$, $\sigma^{*2}$), vamos considerar se [^4.7.7] poderia de fato caracterizar os dados $\{Y_t\}$ gerados por [^4.7.5]. Isso exigiria:
$$(1+\theta^*L)\epsilon_t = (1+\delta L)u_t + v_t$$ [^4.7.15]
ou
$$\epsilon_t = (1+\theta^*L)^{-1}[(1+\delta L)u_t + v_t]$$
$$= (u_t-\theta^*u_{t-1}+\theta^{*2}u_{t-2}-\theta^{*3}u_{t-3}+\ldots) + \delta(u_{t-1}-\theta^*u_{t-2}+\theta^{*2}u_{t-3}-\theta^{*3}u_{t-4}+\ldots) + (v_t-\theta^*v_{t-1}+\theta^{*2}v_{t-2}-\theta^{*3}v_{t-3}+\ldots).$$ [^4.7.16]
A s√©rie $\epsilon_t$ definida em [^4.7.16] √© um lag distribu√≠do em valores passados de $u$ e $v$, ent√£o parece possuir uma rica estrutura de autocorrela√ß√£o. No entanto, verifica-se que √© ru√≠do branco. Para ver isso, note de [^4.7.6] que a fun√ß√£o geradora de autocovari√¢ncia de $Y$ pode ser escrita como:
$$g_Y(z)=(1+\delta z)\sigma^2(1+\delta z^{-1})+\sigma_v^2,$$ [^4.7.17]
de modo que a fun√ß√£o geradora de autocovari√¢ncia de $\epsilon_t = (1+\theta^*L)^{-1}Y_t$ √©:
$$g_{\epsilon}(z) = \frac{(1+\delta z)\sigma^2(1+\delta z^{-1})+\sigma_v^2}{(1+\theta^*z)(1+\theta^*z^{-1})}.$$ [^4.7.18]
Mas $\theta^*$ e $\sigma^{*2}$ foram escolhidos de modo a tornar a fun√ß√£o geradora de autocovari√¢ncia de $(1+\theta^*L)\epsilon_t$, ou seja,
$$(1+\theta^{*2})\sigma^{*2}(1+\theta^*z^{-1}),$$
id√™ntica ao lado direito de [^4.7.17]. Assim, [^4.7.18] √© simplesmente igual a:
$$g_{\epsilon}(z) = \sigma^{*2},$$
uma s√©rie de ru√≠do branco.

Em resumo, adicionar um processo MA(1) a uma s√©rie de ru√≠do branco com a qual n√£o est√° correlacionada em todos os leads e lags produz um novo processo MA(1) caracterizado por [^4.7.7]. Observe que a s√©rie $\epsilon_t$ em [^4.7.16] n√£o poderia ser prevista como uma fun√ß√£o linear de $\epsilon$ atrasado ou $Y$ atrasado. Claramente, $\epsilon_t$ poderia ser prevista, no entanto, com base em $u$ ou $v$ atrasado. Os hist√≥ricos $\{u_t\}$ e $\{v_t\}$ cont√™m mais informa√ß√µes do que $\{\epsilon_t\}$ ou $\{Y_t\}$. A previs√£o √≥tima de $Y_{t+1}$ com base em $\{Y_t, Y_{t-1},\ldots\}$ seria $\mathbb{E}(Y_{t+1}|Y_t, Y_{t-1},\ldots)=\theta^*\epsilon_t$ com erro quadr√°tico m√©dio associado $\sigma^{*2}$. Em contraste, a previs√£o linear √≥tima de $Y_{t+1}$ com base em $\{u_t, u_{t-1},\ldots, v_t, v_{t-1},\ldots\}$ seria $\mathbb{E}(Y_{t+1}|u_t, u_{t-1},\ldots, v_t, v_{t-1},\ldots)=\delta u_t$ com erro quadr√°tico m√©dio associado $\sigma^2 + \sigma_v^2$. Recordando de [^4.7.14] que $|\theta^*| < |\delta|$, parece de [^4.7.9] que $\theta^{*2}\sigma^{*2} < \delta^2\sigma^2$, significando de [^4.7.8] que $\sigma_\epsilon^2 > \sigma^2 + \sigma_v^2$. Em outras palavras, os valores passados de $Y$ cont√™m menos informa√ß√µes do que os valores passados de $u$ e $v$. Este exemplo pode ser √∫til para pensar nas consequ√™ncias de diferentes conjuntos de informa√ß√µes. Pode-se sempre fazer uma previs√£o sensata com base no que se sabe, $\{Y_t, Y_{t-1},\ldots\}$, embora geralmente haja outras informa√ß√µes que poderiam ter ajudado mais. Uma caracter√≠stica importante de tais configura√ß√µes √© que, embora $\epsilon_t$, $u_t$ e $v_t$ sejam todos ru√≠dos brancos, existem correla√ß√µes complexas entre essas s√©ries de ru√≠do branco.

Outro ponto que vale a pena notar √© que tudo o que pode ser estimado com base em $\{Y_t\}$ s√£o os dois par√¢metros $\theta^*$ e $\sigma^{*2}$, enquanto o verdadeiro modelo "estrutural" [^4.7.5] tem tr√™s par√¢metros ($\delta$, $\sigma^2$ e $\sigma_v^2$). Assim, os par√¢metros do modelo estrutural s√£o n√£o identificados no sentido em que os econometristas usam este termo - existe uma fam√≠lia de configura√ß√µes alternativas de $\delta$, $\sigma^2$ e $\sigma_v^2$ com $|\delta| < 1$ que produziriam o valor id√™ntico para a fun√ß√£o de verossimilhan√ßa dos dados observados $\{Y_t\}$. Os processos que foram adicionados para este exemplo tinham ambos m√©dia zero. A adi√ß√£o de termos constantes aos processos n√£o mudar√° os resultados de forma interessante - se $X_t$ √© um processo MA(1) com m√©dia $\mu_X$ e se $v_t$ √© ru√≠do branco mais uma constante $\mu_v$, ent√£o $X_t + v_t$ ser√° um processo MA(1) com m√©dia dada por $\mu_X+\mu_v$. Portanto, nada se perde ao restringir a discuss√£o subsequente √†s somas de processos de m√©dia zero.

> üí° **Exemplo Num√©rico:**
> Vamos considerar um exemplo num√©rico para ilustrar a soma de um processo MA(1) com ru√≠do branco. Suponha que temos um processo MA(1) $X_t$ com $\delta = 0.7$ e $\sigma^2 = 1$, e um ru√≠do branco $v_t$ com $\sigma_v^2 = 0.5$. Portanto:
> $$X_t = u_t + 0.7u_{t-1}$$
> As autocovari√¢ncias de $X_t$ s√£o:
> $$E(X_tX_{t-j}) = \begin{cases}
> (1+0.7^2)(1) = 1.49 & \text{para } j=0 \\
> 0.7(1) = 0.7 & \text{para } j=\pm 1 \\
> 0 & \text{caso contr√°rio.}
> \end{cases}$$
> E as autocovari√¢ncias de $v_t$ s√£o:
> $$E(v_t v_{t-j}) = \begin{cases}
> 0.5 & \text{para } j=0 \\
> 0 & \text{caso contr√°rio.}
> \end{cases}$$
> A s√©rie observada √© $Y_t = X_t + v_t = u_t + 0.7u_{t-1} + v_t$. As autocovari√¢ncias de $Y_t$ s√£o:
> $$E(Y_t Y_{t-j}) = \begin{cases}
> 1.49 + 0.5 = 1.99 & \text{para } j=0 \\
> 0.7 & \text{para } j=\pm 1 \\
> 0 & \text{caso contr√°rio.}
> \end{cases}$$
> Agora, vamos encontrar a representa√ß√£o MA(1) de $Y_t$ como $Y_t = \epsilon_t + \theta \epsilon_{t-1}$. Devemos ter:
> $$(1+\theta^2)\sigma_\epsilon^2 = 1.99$$
> $$\theta\sigma_\epsilon^2 = 0.7$$
> Resolvendo para $\sigma_\epsilon^2$:
> $$\sigma_\epsilon^2 = \frac{0.7}{\theta}$$
> Substituindo na primeira equa√ß√£o:
> $$(1+\theta^2)\frac{0.7}{\theta} = 1.99$$
> $$0.7 + 0.7\theta^2 = 1.99\theta$$
> $$0.7\theta^2 - 1.99\theta + 0.7 = 0$$
> Resolvendo a equa√ß√£o quadr√°tica usando a f√≥rmula quadr√°tica:
> $$\theta = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} = \frac{1.99 \pm \sqrt{(-1.99)^2 - 4(0.7)(0.7)}}{2(0.7)}$$
> $$\theta = \frac{1.99 \pm \sqrt{3.9601 - 1.96}}{1.4} = \frac{1.99 \pm \sqrt{2.0001}}{1.4} = \frac{1.99 \pm 1.414}{1.4}$$
> Temos duas solu√ß√µes para $\theta$:
> $$\theta_1 = \frac{1.99 + 1.414}{1.4} \approx 2.431$$
> $$\theta_2 = \frac{1.99 - 1.414}{1.4} \approx 0.411$$
> A solu√ß√£o invert√≠vel √© $\theta^* = 0.411$.
> Agora, vamos calcular $\sigma_\epsilon^2$:
> $$\sigma_\epsilon^2 = \frac{0.7}{0.411} \approx 1.703$$
> Portanto, temos $Y_t \approx \epsilon_t + 0.411\epsilon_{t-1}$, onde $\epsilon_t$ tem vari√¢ncia $1.703$. Isso demonstra como adicionar ru√≠do branco a um MA(1) resulta em um novo MA(1) com diferentes par√¢metros.

#### Adicionando Dois Processos de M√©dia M√≥vel
Suponha agora que $X_t$ seja um processo MA($q_1$) de m√©dia zero:
$$X_t = (1 + \delta_1 L + \delta_2 L^2 + \ldots + \delta_{q_1}L^{q_1})u_t = \delta(L)u_t,$$
onde $u_t$ √© ru√≠do branco. Seja $W_t$ um processo MA($q_2$) de m√©dia zero:
$$W_t = (1 + \kappa_1 L + \kappa_2 L^2 + \ldots + \kappa_{q_2}L^{q_2})v_t = \kappa(L)v_t,$$
onde $v_t$ √© ru√≠do branco.  Assim, $X_t$ tem autocovari√¢ncias $\gamma_0^x, \gamma_1^x, \gamma_2^x,\ldots$ da forma de [3.3.12], enquanto $W_t$ tem autocovari√¢ncias $\gamma_0^w, \gamma_1^w, \gamma_2^w,\ldots$ da mesma estrutura b√°sica. Assuma que $X_t$ e $W_t$ s√£o n√£o correlacionados entre si em todos os leads e lags:
$$E(X_t W_{t-j})=0$$
para todo $j$. E suponha que observamos
$$Y_t = X_t + W_t.$$
Defina $q$ como o maior de $q_1$ ou $q_2$:
$$q = \max\{q_1, q_2\}.$$
Ent√£o a $j$-√©sima autocovari√¢ncia de $Y_t$ √© dada por:
$$E(Y_t Y_{t-j}) = E(X_t + W_t)(X_{t-j} + W_{t-j}) = E(X_t X_{t-j}) + E(W_t W_{t-j}) = \begin{cases}
\gamma_j^x + \gamma_j^w & \text{para } j=0,\pm 1,\pm 2,\ldots,\pm q \\
0 & \text{caso contr√°rio.}
\end{cases}$$
Assim, as autocovari√¢ncias s√£o zero al√©m de $q$ lags, sugerindo que $Y_t$ possa ser representado como um processo MA($q$). O que mais precisar√≠amos mostrar para estarmos totalmente convencidos de que $Y_t$ √© de fato um processo MA($q$)? Esta quest√£o pode ser colocada em termos de fun√ß√µes geradoras de autocovari√¢ncia. Dado que:
$$\gamma_j^y = \gamma_j^x + \gamma_j^w,$$
segue que:
$$\sum_{j=-\infty}^{\infty} \gamma_j^y z^j = \sum_{j=-\infty}^{\infty} \gamma_j^x z^j + \sum_{j=-\infty}^{\infty} \gamma_j^w z^j.$$
Mas estas s√£o apenas as defini√ß√µes das fun√ß√µes geradoras de autocovari√¢ncia respectivas:
$$g_Y(z) = g_X(z) + g_W(z).$$ [^4.7.19]
A equa√ß√£o [^4.7.19] √© um resultado bastante geral - se algu√©m adiciona dois processos estacion√°rios por covari√¢ncia que s√£o n√£o correlacionados entre si em todos os leads e lags, a fun√ß√£o geradora de autocovari√¢ncia da soma √© a soma das fun√ß√µes geradoras de autocovari√¢ncia das s√©ries individuais.
Se $Y_t$ for expresso como um processo MA($q$),
$$Y_t = (1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q)\epsilon_t = \theta(L)\epsilon_t$$
com
$$E(\epsilon_t \epsilon_{t-j}) = \begin{cases}
\sigma_\epsilon^2 & \text{para } j=0 \\
0 & \text{caso contr√°rio,}
\end{cases}$$
ent√£o a sua fun√ß√£o geradora de autocovari√¢ncia seria:
$$g_Y(z) = \theta(z)\theta(z^{-1})\sigma_\epsilon^2.$$
A quest√£o √© ent√£o se sempre existem valores de $(\theta_1, \theta_2, \ldots, \theta_q, \sigma^2)$ tais que [^4.7.19] seja satisfeita:
$$\theta(z)\theta(z^{-1})\sigma_\epsilon^2 = \delta(z)\delta(z^{-1})\sigma_u^2 + \kappa(z)\kappa(z^{-1})\sigma_v^2.$$ [^4.7.20]
Acontece que existem. Assim, a conjectura revela-se correta de que, se dois processos de m√©dia m√≥vel que n√£o s√£o correlacionados entre si em todos os leads e lags s√£o adicionados, o resultado √© um novo processo de m√©dia m√≥vel cuja ordem √© a maior das ordens das duas s√©ries originais:
$$\text{MA}(q_1) + \text{MA}(q_2) = \text{MA}(\max\{q_1, q_2\}).$$ [^4.7.21]
Uma prova dessa asser√ß√£o, juntamente com um algoritmo construtivo para alcan√ßar a fatora√ß√£o em [^4.7.20], ser√° fornecida no Cap√≠tulo 13.

> üí° **Exemplo Num√©rico:**
> Considere dois processos MA: $X_t$, um MA(1) e $W_t$, um MA(2).
>
>  $$X_t = u_t + 0.5u_{t-1}$$
> $$W_t = v_t + 0.3v_{t-1} - 0.2v_{t-2}$$
> Assumimos que $u_t$ e $v_t$ s√£o ru√≠dos brancos n√£o correlacionados com $\sigma_u^2 = 1$ e $\sigma_v^2 = 0.8$.
>
> As autocovari√¢ncias de $X_t$ s√£o:
>  $$E(X_tX_{t-j}) = \begin{cases}
> (1+0.5^2)(1) = 1.25 & \text{para } j=0 \\
> 0.5(1) = 0.5 & \text{para } j=\pm 1 \\
> 0 & \text{caso contr√°rio.}
> \end{cases}$$
> As autocovari√¢ncias de $W_t$ s√£o:
>  $$E(W_tW_{t-j}) = \begin{cases}
> (1+0.3^2+(-0.2)^2)(0.8) = 0.872 & \text{para } j=0 \\
> (0.3 - (0.3)(-0.2))(0.8) = 0.288 & \text{para } j= \pm 1 \\
> -0.2(0.8) = -0.16 & \text{para } j = \pm 2 \\
> 0 & \text{caso contr√°rio.}
> \end{cases}$$
> A s√©rie combinada $Y_t = X_t + W_t$ tem autocovari√¢ncias:
>   $$E(Y_t Y_{t-j}) = \begin{cases}
> 1.25 + 0.872 = 2.122 & \text{para } j=0 \\
> 0.5 + 0.288 = 0.788 & \text{para } j=\pm 1 \\
> -0.16 & \text{para } j = \pm 2 \\
> 0 & \text{caso contr√°rio.}
> \end{cases}$$
>
> Como $q = \max(1,2) = 2$, $Y_t$ √© um processo MA(2). Podemos expressar $Y_t$ como $Y_t = \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2}$. Os par√¢metros $\theta_1$, $\theta_2$ e $\sigma_\epsilon^2$ podem ser obtidos resolvendo as seguintes equa√ß√µes:
>
> $$ (1 + \theta_1^2 + \theta_2^2)\sigma_\epsilon^2 = 2.122$$
> $$ (\theta_1 + \theta_1 \theta_2)\sigma_\epsilon^2 = 0.788$$
> $$ \theta_2 \sigma_\epsilon^2 = -0.16$$
> Resolver este sistema de equa√ß√µes pode ser complexo e geralmente √© feito numericamente. No entanto, este exemplo demonstra que a soma de um MA(1) e um MA(2) resulta em um MA(2).

#### Adicionando Dois Processos Autorregressivos
Suponha agora que $X_t$ e $W_t$ s√£o dois processos AR(1):
$$(1-\pi L)X_t = u_t,$$ [^4.7.22]
$$(1-\rho L)W_t = v_t,$$ [^4.7.23]
onde $u_t$ e $v_t$ s√£o cada um ru√≠do branco com $u_t$ n√£o correlacionado com $v_r$ para todo $t$ e $r$. Suponha novamente que observamos
$$Y_t = X_t + W_t$$
e queremos prever $Y_{t+1}$ com base em seus pr√≥prios valores atrasados. Se, por acaso, $X_t$ e $W_t$ compartilham o mesmo par√¢metro autorregressivo, ou seja, $\pi=\rho$, ent√£o [^4.7.22] poderia ser simplesmente adicionado diretamente a [^4.7.23] para deduzir
$$(1-\pi L)X_t + (1-\pi L)W_t = u_t + v_t$$
ou
$$(1-\pi L)(X_t + W_t) = u_t + v_t.$$
Mas a soma $u_t + v_t$ √© ru√≠do branco (como um caso especial do resultado [^4.7.21]), significando que $Y_t$ tem uma representa√ß√£o AR(1):
$$(1-\pi L)Y_t = \epsilon_t.$$
No caso mais prov√°vel de que os par√¢metros autorregressivos $\pi$ e $\rho$ sejam diferentes, ent√£o [^4.7.22] pode ser multiplicado por $(1-\rho L)$:
$$(1-\rho L)(1-\pi L)X_t = (1-\rho L)u_t$$ [^4.7.24]
e, similarmente, [^4.7.23] poderia ser multiplicado por $(1-\pi L)$:
$$(1-\pi L)(1-\rho L)W_t = (1-\pi L)v_t.$$ [^4.7.25]
Adicionando [^4.7.24] a [^4.7.25], obtemos:
$$(1-\rho L)(1-\pi L)(X_t + W_t) = (1-\rho L)u_t + (1-\pi L)v_t.$$ [^4.7.26]
De [^4.7.21], o lado direito de [^4.7.26] tem uma representa√ß√£o MA(1). Assim, podemos escrever:
$$(1-\phi_1 L - \phi_2 L^2)Y_t = (1+\theta L)\epsilon_t,$$
onde
$$(1-\phi_1 L - \phi_2 L^2) = (1-\rho L)(1-\pi L)$$
e
$$(1+\theta L)\epsilon_t = (1-\rho L)u_t + (1-\pi L)v_t.$$
Em outras palavras:
$$\text{AR}(1) + \text{AR}(1) = \text{ARMA}(2,1).$$ [^4.7.27]
Em geral, adicionar um processo AR($p_1$)
$$\pi(L)X_t = u_t$$
a um processo AR($p_2$) com o qual n√£o est√° correlacionado em todos os leads e lags
$$\rho(L)W_t = v_t$$
produz um processo ARMA($p_1 + p_2$, $\max\{p_1, p_2\}$):
$$\phi(L)Y_t = \theta(L)\epsilon_t,$$
onde:
$$\phi(L) = \pi(L)\rho(L)$$
e
$$\theta(L)\epsilon_t = \rho(L)u_t + \pi(L)v_t.$$

**Proposi√ß√£o 1** Generaliza√ß√£o da Soma de Processos AR
    A soma de dois processos AR n√£o correlacionados, um de ordem $p_1$ e outro de ordem $p_2$, resulta em um processo ARMA de ordem $(p_1+p_2, \max\{p_1,p_2\})$.
    *Prova:*
    I. Sejam $X_t$ um processo AR($p_1$) e $W_t$ um processo AR($p_2$) n√£o correlacionados. Estes podem ser expressos como:
    $$\pi(L)X_t = u_t$$
    $$\rho(L)W_t = v_t$$
    onde $\pi(L)$ e $\rho(L)$ s√£o polin√¥mios de ordem $p_1$ e $p_2$, respectivamente, e $u_t$ e $v_t$ s√£o ru√≠dos brancos n√£o correlacionados.
    II. Seja $Y_t = X_t + W_t$.
    III. Multiplicando a primeira equa√ß√£o por $\rho(L)$ e a segunda por $\pi(L)$, obtemos:
    $$\rho(L)\pi(L)X_t = \rho(L)u_t$$
     $$\pi(L)\rho(L)W_t = \pi(L)v_t$$
    IV. Adicionando as duas equa√ß√µes:
    $$\pi(L)\rho(L)(X_t + W_t) = \rho(L)u_t + \pi(L)v_t$$
     $$\phi(L)Y_t = \theta(L)\epsilon_t$$
    onde $\phi(L) = \pi(L)\rho(L)$ √© um polin√¥mio de ordem $p_1 + p_2$ e $\theta(L)\epsilon_t = \rho(L)u_t + \pi(L)v_t$ representa um processo MA de ordem m√°xima $\max\{p_1, p_2\}$.
    V. Portanto, a soma de dois processos AR n√£o correlacionados resulta em um processo ARMA de ordem $(p_1 + p_2, \max\{p_1, p_2\})$. ‚ñ†

> üí° **Exemplo Num√©rico:**
> Considere dois processos AR(1):
> $$X_t = 0.7X_{t-1} + u_t$$
> $$W_t = 0.4W_{t-1} + v_t$$
> onde $u_t$ e $v_t$ s√£o ru√≠dos brancos n√£o correlacionados, cada um com vari√¢ncia 1. Aqui, $\pi=0.7$ e $\rho = 0.4$.
> A s√©rie combinada $Y_t = X_t + W_t$. Para obter a representa√ß√£o ARMA, multiplicamos a primeira equa√ß√£o por $(1-0.4L)$ e a segunda por $(1-0.7L)$:
> $$(1-0.4L)(1-0.7L)X_t = (1-0.4L)u_t$$
> $$(1-0.7L)(1-0.4L)W_t = (1-0.7L)v_t$$
> Somando as duas equa√ß√µes, temos:
> $$(1-0.4L)(1-0.7L)(X_t + W_t) = (1-0.4L)u_t + (1-0.7L)v_t$$
> $$(1 - 1.1L + 0.28L^2)Y_t = u_t - 0.4u_{t-1} + v_t - 0.7v_{t-1}$$
> Defina $ (1+\theta L)\epsilon_t =  u_t - 0.4u_{t-1} + v_t - 0.7v_{t-1}$. Portanto, $Y_t$ segue um modelo ARMA(2,1), onde $\phi_1 = 1.1$, $\phi_2 = -0.28$, e $\theta$ e $\sigma_\epsilon^2$ podem ser derivados a partir das autocovari√¢ncias do lado direito.
>  Neste caso:
> $$\phi(L) = (1 - 0.7L)(1 - 0.4L) = 1 - 1.1L + 0.28L^2$$
> $$\phi_1 = 1.1$$
> $$\phi_2 = -0.28$$
> e
> $$\theta(L)\epsilon_t = (1 - 0.4L)u_t + (1 - 0.7L)v_t$$
> Onde $\theta$ e a vari√¢ncia de $\epsilon_t$ ser√£o determinados pela estrutura do processo MA que √© a soma dos dois processos de ru√≠do branco.
> Isso demonstra que a soma de dois AR(1) resulta em um ARMA(2,1).

**Teorema 1** Soma de Processos ARMA
    A soma de um processo ARMA($p_1, q_1$) e um processo ARMA($p_2, q_2$), que s√£o n√£o correlacionados entre si em todos os lags e leads, resulta em um processo ARMA de ordem no m√°ximo ($p_1+p_2$, $\max\{p_1, p_2, q_1, q_2\}$).
    *Prova:*
    I. Seja $X_t$ um processo ARMA($p_1, q_1$) e $W_t$ um processo ARMA($p_2, q_2$), tal que $X_t$ e $W_t$ s√£o n√£o correlacionados para todo $t$. Ent√£o, estes podem ser expressos como:
    $$\phi    (B)X_t = \theta(B)a_t$$
    $$\psi(B)W_t = \omega(B)b_t$$
    onde $\{a_t\}$ e $\{b_t\}$ s√£o ru√≠dos brancos n√£o correlacionados, $\phi(B) = 1 - \phi_1B - ... - \phi_{p_1}B^{p_1}$, $\theta(B) = 1 + \theta_1B + ... + \theta_{q_1}B^{q_1}$, $\psi(B) = 1 - \psi_1B - ... - \psi_{p_2}B^{p_2}$, e $\omega(B) = 1 + \omega_1B + ... + \omega_{q_2}B^{q_2}$.
    II. Seja $Y_t = X_t + W_t$. Ent√£o,
    $$Y_t = \frac{\theta(B)}{\phi(B)}a_t + \frac{\omega(B)}{\psi(B)}b_t$$
    III. Seja $\gamma_Y(k)$ a fun√ß√£o de autocovari√¢ncia de $Y_t$. Ent√£o,
    $$\gamma_Y(k) = Cov(Y_t, Y_{t-k}) = Cov(X_t + W_t, X_{t-k} + W_{t-k})$$
    Como $X_t$ e $W_t$ s√£o n√£o correlacionados, temos
    $$\gamma_Y(k) = Cov(X_t, X_{t-k}) + Cov(W_t, W_{t-k}) = \gamma_X(k) + \gamma_W(k)$$
    onde $\gamma_X(k)$ e $\gamma_W(k)$ s√£o as fun√ß√µes de autocovari√¢ncia de $X_t$ e $W_t$ respectivamente.

**Consequ√™ncias:**
1.  Se $X_t$ e $W_t$ s√£o processos AR(1) independentes, com par√¢metros $\phi_1$ e $\psi_1$, ent√£o $Y_t$ n√£o √© necessariamente um processo AR(1), AR(2) ou MA.
2.  A fun√ß√£o de autocovari√¢ncia de $Y_t$ √© a soma das fun√ß√µes de autocovari√¢ncia de $X_t$ e $W_t$.
3.  O espectro de $Y_t$ √© a soma dos espectros de $X_t$ e $W_t$, uma vez que a fun√ß√£o de autocovari√¢ncia e o espectro s√£o transformadas de Fourier um do outro.

**Exemplo:**
Suponha que $X_t$ seja um processo AR(1) com par√¢metro $\phi = 0.7$ e ru√≠do branco com vari√¢ncia $\sigma^2_a = 1$, e $W_t$ seja um processo MA(1) com par√¢metro $\theta = 0.5$ e ru√≠do branco com vari√¢ncia $\sigma^2_b = 2$. Ent√£o,
$$\gamma_X(k) = \frac{\sigma^2_a}{1 - \phi^2} \phi^{|k|}$$
$$\gamma_X(0) = \frac{1}{1 - 0.7^2} = \frac{1}{0.51} \approx 1.96$$
$$\gamma_X(1) = 1.96 \times 0.7 = 1.37$$
$$\gamma_W(k) = \begin{cases} \sigma^2_b(1 + \theta^2), & \text{se } k = 0 \\ \sigma^2_b \theta, & \text{se } |k| = 1 \\ 0, & \text{se } |k| > 1 \end{cases}$$
$$\gamma_W(0) = 2(1 + 0.5^2) = 2(1.25) = 2.5$$
$$\gamma_W(1) = 2 \times 0.5 = 1$$
$$\gamma_Y(0) = \gamma_X(0) + \gamma_W(0) = 1.96 + 2.5 = 4.46$$
$$\gamma_Y(1) = \gamma_X(1) + \gamma_W(1) = 1.37 + 1 = 2.37$$
$$\gamma_Y(k) = \gamma_X(k) \text{ para } |k| > 1$$
<!-- END -->
