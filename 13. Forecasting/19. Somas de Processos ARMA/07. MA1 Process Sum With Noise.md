## A Soma de um Processo MA(1) com Ru√≠do Branco: Uma An√°lise Detalhada

### Introdu√ß√£o
Este cap√≠tulo explora em detalhe a adi√ß√£o de um processo de m√©dia m√≥vel de primeira ordem (MA(1)) a um ru√≠do branco, demonstrando como a combina√ß√£o linear desses processos resulta em um novo processo MA(1), com par√¢metros ajustados para refletir a combina√ß√£o das autocovari√¢ncias [^4.7.1], [^4.7.2], [^4.7.3]. O objetivo principal √© mostrar que a soma de processos estoc√°sticos pode preservar ou alterar a estrutura dos componentes originais, e que esta opera√ß√£o resulta em outro processo MA(1), com par√¢metros distintos, e que a rela√ß√£o entre as autocovari√¢ncias dos componentes originais e do resultante √© crucial para entender a natureza da nova s√©rie temporal [^4.7.5], [^4.7.7]. A an√°lise apresentada visa fornecer uma compreens√£o profunda para um p√∫blico com conhecimento avan√ßado em matem√°tica, modelos estat√≠sticos, otimiza√ß√£o e an√°lise de dados, expandindo o conhecimento previamente abordado.

### Conceitos Fundamentais
#### Defini√ß√£o dos Processos
Considere um processo MA(1), $X_t$, com m√©dia zero definido como:
$$X_t = u_t + \delta u_{t-1}$$ [^4.7.1]
onde $u_t$ √© um processo de ru√≠do branco com vari√¢ncia $\sigma^2$. As autocovari√¢ncias de $X_t$ s√£o dadas por:
$$E(X_t X_{t-j}) = \begin{cases}
(1 + \delta^2)\sigma^2 & \text{para } j=0 \\
\delta\sigma^2 & \text{para } j=\pm 1 \\
0 & \text{caso contr√°rio.}
\end{cases}$$ [^4.7.2]
Adicionamos a este processo um ru√≠do branco independente $v_t$ com m√©dia zero e vari√¢ncia $\sigma_v^2$:
$$E(v_t v_{t-j}) = \begin{cases}
\sigma_v^2 & \text{para } j=0 \\
0 & \text{caso contr√°rio.}
\end{cases}$$ [^4.7.3]
Assumimos que $u_t$ e $v_t$ s√£o n√£o correlacionados, ou seja, $E(u_t v_{t-j}) = 0$ para todo $j$ [^4.7.4]. O processo observado resultante, $Y_t$, √© a soma desses dois processos:
$$Y_t = X_t + v_t = u_t + \delta u_{t-1} + v_t$$ [^4.7.5]
> üí° **Exemplo Num√©rico:** Suponha que temos um processo MA(1) onde $\delta = 0.7$ e $\sigma^2 = 2$. As autocovari√¢ncias de $X_t$ seriam $E(X_t X_t) = (1 + 0.7^2) \times 2 = 3.98$ para $j=0$ e $E(X_t X_{t-1}) = 0.7 \times 2 = 1.4$ para $j= \pm 1$. Adicionando um ru√≠do branco $v_t$ com vari√¢ncia $\sigma_v^2 = 1$, ter√≠amos que $E(v_t v_t) = 1$ para $j=0$ e 0 para outros lags.

#### An√°lise da S√©rie Resultante
O objetivo agora √© analisar as propriedades da s√©rie $Y_t$, em particular, suas autocovari√¢ncias e a possibilidade de represent√°-la como um processo MA(1). A m√©dia de $Y_t$ √© zero, pois tanto $X_t$ quanto $v_t$ t√™m m√©dia zero. A autocovari√¢ncia de $Y_t$ em lag $j$ √© dada por:
$$E(Y_t Y_{t-j}) = E[(X_t + v_t)(X_{t-j} + v_{t-j})] = E(X_t X_{t-j}) + E(v_t v_{t-j})$$
Substituindo as autocovari√¢ncias de $X_t$ e $v_t$, obtemos:
$$E(Y_t Y_{t-j}) = \begin{cases}
(1 + \delta^2)\sigma^2 + \sigma_v^2 & \text{para } j=0 \\
\delta\sigma^2 & \text{para } j=\pm 1 \\
0 & \text{caso contr√°rio.}
\end{cases}$$ [^4.7.6]
Como as autocovari√¢ncias de $Y_t$ s√£o n√£o nulas apenas em lags 0 e ¬±1, a estrutura sugere que $Y_t$ poderia ser representado como um processo MA(1) [^4.7.7]:
$$Y_t = \epsilon_t + \theta\epsilon_{t-1}$$
onde $\epsilon_t$ √© um ru√≠do branco com vari√¢ncia $\sigma_\epsilon^2$, e $\theta$ √© o par√¢metro MA(1). As autocovari√¢ncias deste processo s√£o:
$$E(Y_t Y_{t-j}) = \begin{cases}
(1 + \theta^2)\sigma_\epsilon^2 & \text{para } j=0 \\
\theta\sigma_\epsilon^2 & \text{para } j=\pm 1 \\
0 & \text{caso contr√°rio.}
\end{cases}$$
> üí° **Exemplo Num√©rico:**  Continuando com o exemplo anterior, as autocovari√¢ncias de $Y_t$ s√£o $E(Y_t Y_t) = (1 + 0.7^2) \times 2 + 1 = 4.98$ para $j=0$ e $E(Y_t Y_{t-1}) = 0.7 \times 2 = 1.4$ para $j= \pm 1$. O objetivo √© encontrar um $\theta$ e $\sigma_\epsilon^2$ para um processo MA(1) que resulte nessas autocovari√¢ncias.

#### Ajuste de Par√¢metros
Para que as duas representa√ß√µes de $Y_t$ sejam equivalentes, as autocovari√¢ncias em cada lag devem ser iguais. Comparando as autocovari√¢ncias, obtemos as seguintes rela√ß√µes:
$$(1 + \theta^2)\sigma_\epsilon^2 = (1 + \delta^2)\sigma^2 + \sigma_v^2$$ [^4.7.8]
$$\theta\sigma_\epsilon^2 = \delta\sigma^2$$ [^4.7.9]
A partir da segunda equa√ß√£o, podemos expressar $\sigma_\epsilon^2$ em fun√ß√£o de $\theta$ como:
$$\sigma_\epsilon^2 = \frac{\delta\sigma^2}{\theta}$$ [^4.7.10]
Substituindo este resultado na primeira equa√ß√£o, obtemos:
$$(1 + \theta^2)\frac{\delta\sigma^2}{\theta} = (1 + \delta^2)\sigma^2 + \sigma_v^2$$
Reorganizando os termos, obtemos uma equa√ß√£o quadr√°tica em $\theta$:
$$\delta\theta^2 - [(1 + \delta^2)\sigma^2 + \sigma_v^2]\theta/\sigma^2 + \delta = 0$$
$$\delta\theta^2 - [(1 + \delta^2) + (\sigma_v^2/\sigma^2)]\theta + \delta = 0$$ [^4.7.11]
As solu√ß√µes para $\theta$ podem ser obtidas utilizando a f√≥rmula quadr√°tica:
$$\theta = \frac{[(1 + \delta^2) + (\sigma_v^2/\sigma^2)] \pm \sqrt{[(1 + \delta^2) + (\sigma_v^2/\sigma^2)]^2 - 4\delta^2}}{2\delta}$$
Essa equa√ß√£o quadr√°tica em $\theta$ tem duas solu√ß√µes reais, indicando a exist√™ncia de uma representa√ß√£o invert√≠vel e outra n√£o invert√≠vel para o processo MA(1) resultante [^4.7.12], [^4.7.13], [^4.7.14]. A solu√ß√£o invert√≠vel $\theta^*$ satisfaz $0 < |\theta^*| < |\delta|$, e a solu√ß√£o n√£o invert√≠vel $\bar{\theta}$ satisfaz $1 < |\delta^{-1}| < |\bar{\theta}|$.

> üí° **Exemplo Num√©rico:** Considere que $\delta = 0.6$, $\sigma^2 = 1$ e $\sigma_v^2 = 0.5$. As equa√ß√µes para $\theta$ e $\sigma_\epsilon^2$ se tornam:
>  $$(1 + \theta^2)\sigma_\epsilon^2 = (1 + 0.6^2) \cdot 1 + 0.5 = 1.86$$
>  $$\theta\sigma_\epsilon^2 = 0.6 \cdot 1 = 0.6$$
>  Substituindo, obtemos:
>  $$(1 + \theta^2)\frac{0.6}{\theta} = 1.86$$
>  $$0.6\theta^2 - 1.86\theta + 0.6 = 0$$
>  Resolvendo a equa√ß√£o quadr√°tica, obtemos:
>  $$\theta = \frac{1.86 \pm \sqrt{1.86^2 - 4(0.6)(0.6)}}{1.2} = \frac{1.86 \pm \sqrt{3.4596 - 1.44}}{1.2} = \frac{1.86 \pm \sqrt{2.0196}}{1.2}$$
>  $$\theta \approx \frac{1.86 \pm 1.421}{1.2}$$
>  As duas solu√ß√µes s√£o:
>  $\theta_1 \approx 2.734$ e $\theta_2 \approx 0.366$. A solu√ß√£o invert√≠vel √© $\theta^* \approx 0.366$, e a vari√¢ncia √© $\sigma_\epsilon^2 = \frac{0.6}{0.366} \approx 1.639$.
> Portanto, o processo MA(1) resultante √© aproximadamente  $Y_t = \epsilon_t + 0.366\epsilon_{t-1}$, com vari√¢ncia $\sigma_\epsilon^2 \approx 1.639$.
>
> üí° **Exemplo Num√©rico:** Agora, vamos analisar o caso quando $\delta = -0.5$, $\sigma^2=1$ e $\sigma_v^2 = 0.25$. As equa√ß√µes seriam:
>
> $$(1 + \theta^2)\sigma_\epsilon^2 = (1 + (-0.5)^2) \cdot 1 + 0.25 = 1.5$$
> $$\theta\sigma_\epsilon^2 = -0.5 \cdot 1 = -0.5$$
>
>  Substituindo, temos:
>
> $$(1 + \theta^2) \frac{-0.5}{\theta} = 1.5$$
> $$-0.5\theta^2 - 1.5\theta - 0.5 = 0$$
>
>  Multiplicando por -1, para simplificar:
>
> $$0.5\theta^2 + 1.5\theta + 0.5 = 0$$
>
>  Resolvendo a equa√ß√£o quadr√°tica:
>
> $$\theta = \frac{-1.5 \pm \sqrt{1.5^2 - 4(0.5)(0.5)}}{1} = \frac{-1.5 \pm \sqrt{2.25 - 1}}{1} = \frac{-1.5 \pm \sqrt{1.25}}{1}$$
>
>  $$\theta \approx \frac{-1.5 \pm 1.118}{1}$$
>
> As duas solu√ß√µes s√£o:
>
> $\theta_1 \approx -0.382$ e $\theta_2 \approx -2.618$.
>
> A solu√ß√£o invert√≠vel √© $\theta^* \approx -0.382$.  A vari√¢ncia seria $\sigma_\epsilon^2 = \frac{-0.5}{-0.382} \approx 1.309$.
> Portanto, o processo MA(1) resultante √© aproximadamente $Y_t = \epsilon_t - 0.382\epsilon_{t-1}$, com vari√¢ncia $\sigma_\epsilon^2 \approx 1.309$.
>
> Esses exemplos num√©ricos ilustram como a adi√ß√£o de ru√≠do branco afeta os par√¢metros do processo MA(1) resultante.

#### An√°lise do Ru√≠do Branco Resultante
A representa√ß√£o $Y_t = \epsilon_t + \theta \epsilon_{t-1}$ nos leva √† an√°lise do ru√≠do branco resultante $\epsilon_t$. Substituindo a express√£o para $Y_t$, temos:
$$(1 + \theta L)\epsilon_t = u_t + \delta u_{t-1} + v_t$$
Resolvendo para $\epsilon_t$:
$$\epsilon_t = (1 + \theta L)^{-1}(u_t + \delta u_{t-1} + v_t)$$
$$\epsilon_t = (1 - \theta L + \theta^2 L^2 - \theta^3 L^3 + \ldots)(u_t + \delta u_{t-1} + v_t)$$
Esta equa√ß√£o demonstra que $\epsilon_t$ √© uma combina√ß√£o linear infinita de valores presentes e passados de $u_t$ e $v_t$. Embora $\epsilon_t$ seja tamb√©m um ru√≠do branco, ele n√£o √© id√™ntico a $u_t$ ou $v_t$ individualmente, mas sim uma fun√ß√£o linear desses ru√≠dos.

**Lema 1**
A representa√ß√£o $\epsilon_t = (1 + \theta L)^{-1}(u_t + \delta u_{t-1} + v_t)$ converge quando $|\theta| < 1$, onde $L$ √© o operador de atraso.

*Proof:*
I.  Come√ßamos com a express√£o para $\epsilon_t$:
    $$\epsilon_t = (1 + \theta L)^{-1}(u_t + \delta u_{t-1} + v_t)$$
II. Expandimos $(1 + \theta L)^{-1}$ como uma s√©rie geom√©trica:
    $$(1 + \theta L)^{-1} = 1 - \theta L + \theta^2 L^2 - \theta^3 L^3 + \ldots = \sum_{k=0}^{\infty} (-\theta L)^k$$
III. A s√©rie geom√©trica converge se $|\theta L| < 1$, ou seja, $|\theta| < 1$.
IV. Portanto, se $|\theta| < 1$, a representa√ß√£o de $\epsilon_t$ como uma combina√ß√£o linear infinita de valores passados de $u_t$ e $v_t$ √© v√°lida, pois os coeficientes da combina√ß√£o linear s√£o limitados e decrescem geometricamente.
‚ñ†
> üí° **Exemplo Num√©rico:** Para o exemplo em que  $\delta = 0.6$, $\sigma^2 = 1$ e $\sigma_v^2 = 0.5$ e a solu√ß√£o invert√≠vel $\theta \approx 0.366$, temos: $\epsilon_t = (1 - 0.366L + 0.366^2L^2 - ...) (u_t + 0.6u_{t-1} + v_t)$. Os coeficientes da expans√£o de $(1 + \theta L)^{-1}$ decrescem geometricamente, garantindo a converg√™ncia da s√©rie. Isso significa que $\epsilon_t$ √© influenciado por $u_t, u_{t-1}, v_t$ e seus valores passados, ponderados pelos coeficientes que decrescem.

#### Implica√ß√µes e Interpreta√ß√µes
A transforma√ß√£o obtida ao somar um processo MA(1) com ru√≠do branco demonstra que a estrutura MA(1) √© preservada, mas com par√¢metros ajustados. Essa opera√ß√£o pode ser √∫til em situa√ß√µes onde a observa√ß√£o √© uma combina√ß√£o de processos estoc√°sticos individuais e as correla√ß√µes temporais presentes na observa√ß√£o resultam de uma combina√ß√£o de tais processos. A rela√ß√£o entre as autocovari√¢ncias dos processos originais e do resultante, descrita atrav√©s das equa√ß√µes [^4.7.8] e [^4.7.9], desempenha um papel fundamental na compreens√£o e modelagem de s√©ries temporais.
Em termos de informa√ß√£o, mesmo que a soma $Y_t$ seja um processo MA(1) com ru√≠do branco, o hist√≥rico de $Y_t$ cont√©m menos informa√ß√µes para prever os valores futuros do que o hist√≥rico combinado dos ru√≠dos brancos individuais $u_t$ e $v_t$, pois o hist√≥rico de $Y_t$ n√£o pode inferir diretamente os valores de $u_t$ ou $v_t$ [^4.7.14].

**Proposi√ß√£o 1**
A vari√¢ncia do processo $\epsilon_t$, dado por $\sigma_\epsilon^2$, pode ser expressa em termos das vari√¢ncias dos ru√≠dos brancos originais $\sigma^2$, $\sigma_v^2$ e o par√¢metro $\theta$ como:
$$\sigma_\epsilon^2 = \frac{(1 + \delta^2)\sigma^2 + \sigma_v^2}{1 + \theta^2} $$

*Proof:*
I.  Come√ßamos com a equa√ß√£o [^4.7.8]:
    $$(1 + \theta^2)\sigma_\epsilon^2 = (1 + \delta^2)\sigma^2 + \sigma_v^2$$
II. Dividimos ambos os lados da equa√ß√£o por $(1 + \theta^2)$ para isolar $\sigma_\epsilon^2$:
    $$\sigma_\epsilon^2 = \frac{(1 + \delta^2)\sigma^2 + \sigma_v^2}{1 + \theta^2}$$
III. Esta express√£o relaciona a vari√¢ncia do ru√≠do branco resultante com as vari√¢ncias originais e o par√¢metro do processo MA(1) resultante.
‚ñ†
> üí° **Exemplo Num√©rico:** Usando novamente os valores $\delta=0.6$, $\sigma^2=1$ e $\sigma_v^2=0.5$ e  $\theta \approx 0.366$, temos:
> $\sigma_\epsilon^2 = \frac{(1 + 0.6^2) \cdot 1 + 0.5}{1 + 0.366^2} = \frac{1.86}{1.133956} \approx 1.639$. Este resultado coincide com o c√°lculo feito anteriormente e mostra que a vari√¢ncia do novo ru√≠do branco $\epsilon_t$ depende dos par√¢metros do processo original e da adi√ß√£o de ru√≠do branco.

### Conclus√£o
Este cap√≠tulo detalhou a soma de um processo MA(1) com ru√≠do branco, comprovando que o resultado √© um novo processo MA(1) com par√¢metros ajustados para refletir a combina√ß√£o das autocovari√¢ncias. A opera√ß√£o de somar processos estoc√°sticos, embora pare√ßa simples, pode resultar em uma estrutura de s√©rie temporal mais complexa com diferentes n√≠veis de informa√ß√£o, demonstrando que o processo resultante mant√©m uma estrutura similar, mas com par√¢metros diferentes. Esta an√°lise proporciona uma compreens√£o mais profunda da natureza da combina√ß√£o linear de processos estoc√°sticos e destaca a import√¢ncia de considerar as propriedades individuais dos processos e suas rela√ß√µes para a modelagem de s√©ries temporais.

**Corol√°rio 1.1**
Se $\sigma_v^2 = 0$, o processo $Y_t$ se reduz ao processo $X_t$, e a equa√ß√£o quadr√°tica para $\theta$ tem as solu√ß√µes $\theta = \delta$ (invert√≠vel) e $\theta = \frac{1}{\delta}$ (n√£o invert√≠vel).

*Proof:*
I.  Come√ßamos com a equa√ß√£o [4.7.11]:
    $$ \delta\theta^2 - [(1 + \delta^2) + (\sigma_v^2/\sigma^2)]\theta + \delta = 0 $$
II. Se $\sigma_v^2 = 0$, a equa√ß√£o se torna:
    $$ \delta\theta^2 - (1 + \delta^2)\theta + \delta = 0 $$
III. Dividindo por $\delta$:
    $$ \theta^2 - \left(\frac{1}{\delta} + \delta \right) \theta + 1 = 0 $$
IV. Reorganizando os termos:
    $$ \theta^2 - \frac{1}{\delta} \theta - \delta \theta + 1 = 0 $$
V. Fatorando:
    $$ \theta \left(\theta - \frac{1}{\delta} \right) - \delta \left( \theta - \frac{1}{\delta}\right) = 0 $$
VI. Simplificando:
   $$ \left(\theta - \delta\right) \left(\theta - \frac{1}{\delta}\right) = 0 $$
VII. Portanto, as solu√ß√µes s√£o:
    $$ \theta = \delta \quad \text{e} \quad \theta = \frac{1}{\delta} $$
VIII. Se $|\delta| < 1$, a solu√ß√£o invert√≠vel √© $\theta = \delta$ e a n√£o invert√≠vel √© $\theta = \frac{1}{\delta}$. Se $|\delta| > 1$, a solu√ß√£o invert√≠vel √© $\theta = \frac{1}{\delta}$ e a n√£o invert√≠vel √© $\theta = \delta$.
‚ñ†
> üí° **Exemplo Num√©rico:**  Se $\sigma_v^2 = 0$ e $\delta=0.6$, a equa√ß√£o para $\theta$ se torna: $0.6\theta^2 - (1 + 0.6^2)\theta + 0.6 = 0$ ou $0.6\theta^2 - 1.36\theta + 0.6 = 0$. As solu√ß√µes s√£o $\theta = 0.6$ (invert√≠vel) e $\theta = 1/0.6 \approx 1.667$ (n√£o invert√≠vel), como esperado pelo corol√°rio. Isso confirma que, sem ru√≠do adicional, a solu√ß√£o invert√≠vel √© simplesmente o par√¢metro original do processo MA(1).

### Refer√™ncias
[^4.7.1]: ... *[Defini√ß√£o de um processo MA(1)]*
[^4.7.2]: ... *[Autocovari√¢ncias de um processo MA(1)]*
[^4.7.3]: ... *[Defini√ß√£o de ru√≠do branco]*
[^4.7.4]: ... *[N√£o correla√ß√£o entre u e v]*
[^4.7.5]: ... *[Defini√ß√£o da s√©rie Y como soma de MA(1) e ru√≠do branco]*
[^4.7.7]: ... *[Representa√ß√£o MA(1) para Y]*
[^4.7.8]: ... *[Condi√ß√£o para autocovari√¢ncia de ordem 0]*
[^4.7.9]: ... *[Condi√ß√£o para autocovari√¢ncia de ordem 1]*
[^4.7.10]: ... *[Solu√ß√£o para sigma^2]*
[^4.7.11]: ... *[Equa√ß√£o quadr√°tica para theta]*
[^4.7.12]: ... *[F√≥rmula quadr√°tica]*
[^4.7.13]: ... *[Caso em que sigma_v √© igual a zero]*
[^4.7.14]: ... *[Solu√ß√£o invert√≠vel e n√£o invert√≠vel]*
[^4.7.15]: ... *[Reescrita da representa√ß√£o MA(1) com u e v]*
[^4.7.16]: ... *[Lag distribu√≠do da serie epsilon]*
### 5.2. The Likelihood Function for an AR(1)
To illustrate the calculation of a likelihood function, consider the case of a zero-mean AR(1) process:
$$Y_t = \phi Y_{t-1} + \epsilon_t$$ [5.2.1]
where $\epsilon_t \sim i.i.d. N(0, \sigma^2)$. The joint density of the sample ($y_1, y_2, ..., y_T$) can be written as the product of conditional densities:
$$f_{Y_1, Y_2, \ldots, Y_T}(y_1, y_2, \ldots, y_T; \phi, \sigma^2) = f_{Y_1}(y_1; \phi, \sigma^2) \prod_{t=2}^{T} f_{Y_t|Y_{t-1}, \ldots, Y_1}(y_t | y_{t-1}, \ldots, y_1; \phi, \sigma^2).$$ [5.2.2]

Given the AR(1) structure, the conditional density simplifies to:
$$f_{Y_t|Y_{t-1}, \ldots, Y_1}(y_t | y_{t-1}, \ldots, y_1; \phi, \sigma^2) = f_{Y_t|Y_{t-1}}(y_t | y_{t-1}; \phi, \sigma^2).$$ [5.2.3]

Since $Y_t - \phi Y_{t-1} = \epsilon_t$ and $\epsilon_t$ is normally distributed with mean zero and variance $\sigma^2$, it follows that, conditional on $Y_{t-1}$, $Y_t$ is also normally distributed with mean $\phi Y_{t-1}$ and variance $\sigma^2$. Thus, we have:
$$f_{Y_t|Y_{t-1}}(y_t | y_{t-1}; \phi, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_t - \phi y_{t-1})^2}{2\sigma^2}\right).$$ [5.2.4]

The density of $Y_1$, $f_{Y_1}(y_1; \phi, \sigma^2)$, requires special treatment. In principle, the density of $Y_1$ depends on the initial conditions of the AR process, which is not explicitly specified. However, to make the likelihood calculation feasible we will treat it as though $Y_1$ comes from the unconditional distribution of $Y_t$. As discussed previously, the unconditional variance of $Y_t$ in a stationary AR(1) model is $\sigma^2/(1-\phi^2)$. Given that $E[Y_t] = 0$, $Y_1$ is normally distributed with mean zero and variance $\sigma^2/(1-\phi^2)$:
$$f_{Y_1}(y_1; \phi, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2/(1-\phi^2)}} \exp\left(-\frac{y_1^2}{2\sigma^2/(1-\phi^2)}\right).$$ [5.2.5]

Substituting [5.2.4] and [5.2.5] into [5.2.2], we obtain the likelihood function:
$$L(\phi, \sigma^2; y_1, \ldots, y_T) = \left(\frac{1-\phi^2}{2\pi\sigma^2}\right)^{1/2} \exp\left(-\frac{y_1^2(1-\phi^2)}{2\sigma^2}\right) \prod_{t=2}^{T} \left(\frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_t - \phi y_{t-1})^2}{2\sigma^2}\right)\right).$$ [5.2.6]

Taking logs of the likelihood function we get the log likelihood function:
$$ln L(\phi, \sigma^2; y_1, \ldots, y_T) = -\frac{T}{2} ln(2\pi\sigma^2) + \frac{1}{2}ln(1-\phi^2) - \frac{y_1^2(1-\phi^2)}{2\sigma^2} - \frac{1}{2\sigma^2}\sum_{t=2}^{T} (y_t - \phi y_{t-1})^2.$$ [5.2.7]

The maximum likelihood estimates of $\phi$ and $\sigma^2$ can be found by maximizing [5.2.7] with respect to $\phi$ and $\sigma^2$.
> üí° **Exemplo Num√©rico:** Suponha que temos uma s√©rie temporal $Y = [1.2, 0.8, 1.5, 1.1, 0.9]$ e vamos estimar os par√¢metros de um AR(1). Vamos considerar $\phi=0.5$ e $\sigma^2=0.3$ para ilustra√ß√£o.
> O log-likelihood seria:
>
> $$ln L(0.5, 0.3; Y) = -\frac{5}{2} ln(2\pi \cdot 0.3) + \frac{1}{2}ln(1-0.5^2) - \frac{1.2^2(1-0.5^2)}{2\cdot 0.3} - \frac{1}{2\cdot 0.3} \sum_{t=2}^{5} (y_t - 0.5 y_{t-1})^2$$
>
> $$ln L(0.5, 0.3; Y) \approx -4.58 -0.29 - 2.70 - \frac{1}{0.6}[(0.8 - 0.5\cdot 1.2)^2 + (1.5 - 0.5 \cdot 0.8)^2 + (1.1 - 0.5 \cdot 1.5)^2 + (0.9 - 0.5\cdot 1.1)^2]$$
>
> $$ln L(0.5, 0.3; Y) \approx -4.58 - 0.29 - 2.70 - \frac{1}{0.6}[0.04 + 1.21 + 0.1225 + 0.1225]$$
>
> $$ln L(0.5, 0.3; Y) \approx -4.58 - 0.29 - 2.70 - \frac{1.495}{0.6} \approx -4.58 - 0.29 - 2.70 - 2.49  \approx -10.06 $$
>
> Para obter a estimativa de m√°xima verossimilhan√ßa, precisar√≠amos otimizar essa fun√ß√£o para diferentes valores de $\phi$ e $\sigma^2$ usando m√©todos num√©ricos.

### 5.3 The Likelihood Function for an AR(p)

The logic in the previous section can be extended to an AR(p) process:
$$Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t$$ [5.3.1]
where $\epsilon_t \sim i.i.d. N(0, \sigma^2)$. The joint density of the sample ($y_1, y_2, ..., y_T$) can be written as
$$f_{Y_1, Y_2, \ldots, Y_T}(y_1, y_2, \ldots, y_T; \phi_1, \ldots, \phi_p, \sigma^2) = f_{Y_1, \ldots, Y_p}(y_1, \ldots, y_p; \phi_1, \ldots, \phi_p, \sigma^2) \prod_{t=p+1}^{T} f_{Y_t|Y_{t-1}, \ldots, Y_1}(y_t | y_{t-1}, \ldots, y_1; \phi_1, \ldots, \phi_p, \sigma^2).$$ [5.3.2]
Similar to the AR(1) case, the conditional density can be simplified to:
$$f_{Y_t|Y_{t-1}, \ldots, Y_1}(y_t | y_{t-1}, \ldots, y_1; \phi_1, \ldots, \phi_p, \sigma^2) = f_{Y_t|Y_{t-1}, \ldots, Y_{t-p}}(y_t | y_{t-1}, \ldots, y_{t-p}; \phi_1, \ldots, \phi_p, \sigma^2).$$ [5.3.3]

Since $Y_t - \phi_1 Y_{t-1} - \ldots - \phi_p Y_{t-p} = \epsilon_t$ and $\epsilon_t$ is normally distributed, $Y_t$ conditional on $Y_{t-1}, ..., Y_{t-p}$ is also normally distributed with mean $\phi_1 Y_{t-1} + \ldots + \phi_p Y_{t-p}$ and variance $\sigma^2$:
$$f_{Y_t|Y_{t-1}, \ldots, Y_{t-p}}(y_t | y_{t-1}, \ldots, y_{t-p}; \phi_1, \ldots, \phi_p, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_t - \phi_1 y_{t-1} - \ldots - \phi_p y_{t-p})^2}{2\sigma^2}\right).$$ [5.3.4]

The density of the first p observations, $f_{Y_1, \ldots, Y_p}(y_1, \ldots, y_p; \phi_1, \ldots, \phi_p, \sigma^2)$, requires special treatment similar to $f_{Y_1}$ in the AR(1) case, as it involves the joint density of the initial p values and depends on the initial condition of the process. Again, for simplicity, we will treat it as though the first p values came from the unconditional distribution of $Y_t$. This density becomes complex quickly, and often a simplified version assuming stationarity is used. In practice, the effect of this initial density is negligible for large samples. Therefore, we can write a simplified version of log likelihood function for a large T as
$$ln L(\phi_1, \ldots, \phi_p, \sigma^2; y_1, \ldots, y_T) \approx -\frac{T}{2} ln(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{t=p+1}^{T} (y_t - \phi_1 y_{t-1} - \ldots - \phi_p y_{t-p})^2.$$ [5.3.5]
Maximization of this simplified expression gives the maximum likelihood estimates of the parameters.
> üí° **Exemplo Num√©rico:**  Consideremos uma s√©rie temporal $Y=[1.2, 0.8, 1.5, 1.1, 0.9, 1.3, 1.0]$ e um modelo AR(2). Vamos assumir $\phi_1=0.4$, $\phi_2 = 0.3$, $\sigma^2=0.2$ para fins ilustrativos. O log-likelihood para T grande √©:
>
> $$ln L(\phi_1, \phi_2, \sigma^2; Y) = -\frac{7}{2}ln(2\pi \cdot 0.2) - \frac{1}{2 \cdot 0.2}\sum_{t=3}^{7}(y_t - 0.4y_{t-1> - 0.3y_{t-2})^2$$
A maximiza√ß√£o desta fun√ß√£o de log-verossimilhan√ßa nos leva a estimativas de m√°xima verossimilhan√ßa para $\phi_1$, $\phi_2$ e $\sigma^2$. Este √© um exemplo de como a fun√ß√£o de verossimilhan√ßa pode ser usada para estimar par√¢metros em modelos de s√©ries temporais.

#### Exemplo de um processo AR(1)

Considere um processo AR(1) definido por:

$$y_t = \phi y_{t-1} + \epsilon_t$$

onde $\epsilon_t \sim N(0, \sigma^2)$.

A fun√ß√£o de log-verossimilhan√ßa para um processo AR(1) √© dada por:

$$ln L(\phi, \sigma^2; Y) = -\frac{T}{2}ln(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{t=2}^{T}(y_t - \phi y_{t-1})^2$$

Para maximizar esta fun√ß√£o, podemos derivar em rela√ß√£o a $\phi$ e $\sigma^2$ e igualar a zero para encontrar as estimativas de m√°xima verossimilhan√ßa.

##### Derivada em rela√ß√£o a $\phi$

$$\frac{\partial ln L}{\partial \phi} = \frac{1}{\sigma^2}\sum_{t=2}^{T}(y_t - \phi y_{t-1})y_{t-1} = 0$$

Resolvendo para $\phi$:

$$\hat{\phi}_{ML} = \frac{\sum_{t=2}^{T}y_t y_{t-1}}{\sum_{t=2}^{T}y_{t-1}^2}$$

##### Derivada em rela√ß√£o a $\sigma^2$

$$\frac{\partial ln L}{\partial \sigma^2} = -\frac{T}{2\sigma^2} + \frac{1}{2\sigma^4}\sum_{t=2}^{T}(y_t - \phi y_{t-1})^2 = 0$$

Resolvendo para $\sigma^2$:

$$\hat{\sigma}_{ML}^2 = \frac{1}{T}\sum_{t=2}^{T}(y_t - \hat{\phi}_{ML}y_{t-1})^2$$

Estas s√£o as estimativas de m√°xima verossimilhan√ßa para os par√¢metros de um processo AR(1).

#### Estimativas Recursivas e o Filtro de Kalman

Para modelos de s√©ries temporais mais complexos, como modelos de espa√ßo de estados, a estima√ß√£o de par√¢metros e estados pode ser feita usando o Filtro de Kalman. O Filtro de Kalman √© um algoritmo recursivo que estima o estado de um sistema din√¢mico a partir de uma s√©rie de medi√ß√µes ruidosas.

A ideia b√°sica do Filtro de Kalman √© combinar uma previs√£o baseada no modelo com uma medi√ß√£o atual para obter uma estimativa do estado. O filtro opera em duas etapas: previs√£o e atualiza√ß√£o.

##### Etapa de Previs√£o

Nesta etapa, o filtro usa o modelo do sistema para prever o estado no pr√≥ximo instante de tempo:

$$\hat{x}_{t|t-1} = F\hat{x}_{t-1|t-1} + Bu_t$$
$$P_{t|t-1} = FP_{t-1|t-1}F^T + Q$$

onde:
* $\hat{x}_{t|t-1}$ √© a estimativa do estado no tempo *t* dado que se tem informa√ß√£o at√© o tempo *t-1*.
* $F$ √© a matriz de transi√ß√£o do estado.
* $B$ √© a matriz de controle.
* $u_t$ √© o vetor de controle.
* $P_{t|t-1}$ √© a matriz de covari√¢ncia do erro de previs√£o.
* $Q$ √© a matriz de covari√¢ncia do ru√≠do do processo.

##### Etapa de Atualiza√ß√£o

Nesta etapa, a previs√£o √© combinada com a medi√ß√£o atual para refinar a estimativa do estado:

$$K_t = P_{t|t-1}H^T(HP_{t|t-1}H^T + R)^{-1}$$
$$\hat{x}_{t|t} = \hat{x}_{t|t-1} + K_t(z_t - H\hat{x}_{t|t-1})$$
$$P_{t|t} = (I - K_tH)P_{t|t-1}$$

onde:
* $K_t$ √© o ganho de Kalman.
* $H$ √© a matriz de observa√ß√£o.
* $R$ √© a matriz de covari√¢ncia do ru√≠do de medi√ß√£o.
* $z_t$ √© a medi√ß√£o atual.
* $\hat{x}_{t|t}$ √© a estimativa do estado no tempo *t* dado que se tem informa√ß√£o at√© o tempo *t*.
* $P_{t|t}$ √© a matriz de covari√¢ncia do erro da estimativa.

O Filtro de Kalman √© uma ferramenta poderosa para estima√ß√£o de estados em sistemas din√¢micos e √© usado em muitas aplica√ß√µes, incluindo navega√ß√£o, controle e previs√£o de s√©ries temporais.

<!-- END -->
