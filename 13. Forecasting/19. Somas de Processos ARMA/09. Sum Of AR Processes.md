## A Soma de Processos Autorregressivos: Uma An√°lise Detalhada e Complexa

### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise da combina√ß√£o linear de processos estoc√°sticos, focando na soma de processos autorregressivos (AR) e demonstrando como essa opera√ß√£o resulta em um processo ARMA (Autorregressivo de M√©dias M√≥veis) [^4.7.22], [^4.7.23], [^4.7.27]. Ao contr√°rio da soma de processos de m√©dias m√≥veis (MA), onde a ordem resultante √© simplesmente o m√°ximo das ordens dos processos componentes, a soma de processos AR resulta num processo ARMA com uma estrutura mais complexa, que depender√° dos par√¢metros dos processos originais. O objetivo principal deste cap√≠tulo √© mostrar como as propriedades de autocorrela√ß√£o dependem dos par√¢metros dos processos componentes, e como a representa√ß√£o com operadores de defasagem simplifica a an√°lise da estrutura temporal do processo resultante. Este cap√≠tulo visa uma compreens√£o profunda, construindo sobre a base te√≥rica j√° estabelecida para um p√∫blico com conhecimento avan√ßado em matem√°tica, modelos estat√≠sticos e an√°lise de dados.

### Conceitos Fundamentais
#### Representa√ß√£o de Processos Autorregressivos
Considere dois processos autorregressivos, $X_t$ e $W_t$, de ordens $p_1$ e $p_2$, respectivamente:
$$(1 - \pi L)X_t = u_t$$
$$(1 - \rho L)W_t = v_t$$
onde $u_t$ e $v_t$ s√£o ru√≠dos brancos independentes, $L$ √© o operador de defasagem, $\pi$ e $\rho$ s√£o os par√¢metros autorregressivos. Formalmente, estes processos podem ser expressos como:
$$ X_t = \pi_1 X_{t-1} + \pi_2 X_{t-2} + \ldots + \pi_{p_1} X_{t-p_1} + u_t$$
$$ W_t = \rho_1 W_{t-1} + \rho_2 W_{t-2} + \ldots + \rho_{p_2} W_{t-p_2} + v_t$$
onde $\pi_1, \pi_2, \ldots, \pi_{p_1}$ s√£o os par√¢metros do processo AR de $X_t$, e $\rho_1, \rho_2, \ldots, \rho_{p_2}$ s√£o os par√¢metros do processo AR de $W_t$.  Assumimos que $u_t$ e $v_t$ s√£o ru√≠dos brancos, com m√©dia zero e vari√¢ncias $\sigma_u^2$ e $\sigma_v^2$ respectivamente, e que s√£o independentes em todos os lags.

> üí° **Exemplo Num√©rico:** Seja um processo AR(1) $X_t$ com $\pi_1 = 0.7$ e um ru√≠do branco $u_t$ com vari√¢ncia $\sigma_u^2=0.5$. Ent√£o, a representa√ß√£o √© $X_t = 0.7 X_{t-1} + u_t$. Isso significa que cada valor de $X_t$ √© 70% do valor anterior mais um choque aleat√≥rio $u_t$. Agora, considere um outro processo AR(2), $W_t = 0.5W_{t-1} + 0.3W_{t-2} + v_t$, com vari√¢ncia $\sigma_v^2 = 0.3$. Aqui, cada valor depende dos dois valores anteriores e de um ru√≠do branco $v_t$. As depend√™ncias temporais s√£o diferentes para os dois processos.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros do AR(1)
> pi = 0.7
> sigma_u = np.sqrt(0.5)
>
> # Par√¢metros do AR(2)
> rho1 = 0.5
> rho2 = 0.3
> sigma_v = np.sqrt(0.3)
>
> # Simula√ß√£o dos processos
> np.random.seed(42)
> T = 100
> X = np.zeros(T)
> W = np.zeros(T)
>
> X[0] = np.random.normal(0, 1) # Valor inicial arbitr√°rio
> W[0] = np.random.normal(0, 1)
> W[1] = np.random.normal(0, 1)
>
> for t in range(1, T):
>    X[t] = pi*X[t-1] + np.random.normal(0, sigma_u)
>    if t > 1:
>        W[t] = rho1*W[t-1] + rho2*W[t-2] + np.random.normal(0, sigma_v)
>    else:
>        W[t] = rho1*W[t-1] + np.random.normal(0, sigma_v)
>
> # Plot dos processos simulados
> plt.figure(figsize=(10, 4))
> plt.plot(X, label='Processo AR(1) - X_t')
> plt.plot(W, label='Processo AR(2) - W_t')
> plt.xlabel('Tempo (t)')
> plt.ylabel('Valor')
> plt.title('Simula√ß√£o de Processos AR')
> plt.legend()
> plt.show()
> ```
> Este exemplo num√©rico ilustra como a din√¢mica de processos AR de diferentes ordens pode se manifestar.

#### A Soma de Dois Processos AR
Definimos o processo resultante, $Y_t$, como a soma dos processos $X_t$ e $W_t$:
$$Y_t = X_t + W_t$$
Para analisar as propriedades de $Y_t$, precisamos considerar como a soma dos processos AR afeta a sua estrutura temporal. Ao contr√°rio dos processos MA, onde a soma √© direta, a soma de processos AR resulta em um processo ARMA com depend√™ncia temporal mais complexa.
Para analisar como a soma de processos AR resulta em um processo ARMA, podemos aplicar os polin√¥mios de defasagem a cada processo, e expressar o processo resultante em termos do ru√≠do branco. Multiplicando a primeira equa√ß√£o por $(1-\rho L)$, e a segunda por $(1-\pi L)$, obtemos:
$$(1 - \rho L)(1 - \pi L)X_t = (1 - \rho L) u_t$$
$$(1 - \pi L)(1 - \rho L)W_t = (1 - \pi L) v_t$$
Somando ambas as equa√ß√µes e definindo $Y_t = X_t + W_t$, temos:
$$(1 - \rho L)(1 - \pi L)(X_t+W_t) = (1 - \rho L)u_t + (1 - \pi L)v_t$$
$$(1 - \rho L)(1 - \pi L) Y_t = (1 - \rho L)u_t + (1 - \pi L)v_t$$
$$ (1 - (\rho + \pi)L + \rho \pi L^2)Y_t = u_t - \rho u_{t-1} + v_t - \pi v_{t-1}$$
$$ (1 - (\rho + \pi)L + \rho \pi L^2)Y_t = \epsilon_t  +  \theta_1  \epsilon_{t-1}$$

O lado direito da equa√ß√£o pode ser expresso como um processo MA(1). Assim, podemos expressar a soma de dois processos AR como um processo ARMA(2,1).  O polin√¥mio autorregressivo resultante tem ordem 2, enquanto o polin√¥mio de m√©dias m√≥veis resultante tem ordem 1, devido √† combina√ß√£o linear dos ru√≠dos brancos.

> üí° **Exemplo Num√©rico:**  Considere dois processos AR(1), $X_t$ com $\pi = 0.6$ e $W_t$ com $\rho = 0.8$:
> $$(1 - 0.6L)X_t = u_t$$
> $$(1 - 0.8L)W_t = v_t$$
> Ao somar, obtemos:
> $$(1 - 0.8L)(1 - 0.6L)(X_t + W_t) = (1 - 0.8L)u_t + (1 - 0.6L)v_t$$
> $$(1 - 1.4L + 0.48L^2)Y_t = u_t - 0.8u_{t-1} + v_t - 0.6v_{t-1}$$
> Essa representa√ß√£o indica que a soma $Y_t$ segue um processo ARMA(2,1).  Neste caso, a ordem AR √© a soma das ordens dos processos componentes AR.  No entanto, a parte MA tem ordem 1 devido √† combina√ß√£o linear dos ru√≠dos brancos.
> O lado esquerdo nos d√° os par√¢metros autorregressivos:
> $$\phi(L) = 1 - 1.4L + 0.48L^2$$
> O lado direito pode ser representado como um processo MA(1):
> $$\epsilon_t - 0.8\epsilon_{t-1} + \epsilon_t - 0.6 \epsilon_{t-1} = \epsilon_t + \theta\epsilon_{t-1}$$
> No caso geral, a parte de m√©dias m√≥veis da soma de processos AR n√£o corresponde aos par√¢metros $\rho$ e $\pi$, o que complica o c√°lculo, mas tamb√©m enriquece a flexibilidade da estrutura temporal da s√©rie resultante.
>
> üí° **Exemplo Num√©rico:**  Para ilustrar numericamente, suponha que temos $u_t$ e $v_t$ como ru√≠dos brancos gerados a partir de uma distribui√ß√£o normal padr√£o. Vamos gerar 100 valores para cada um e calcular $Y_t$:
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros dos processos AR(1)
> pi = 0.6
> rho = 0.8
>
> # Simula√ß√£o dos ru√≠dos brancos
> np.random.seed(42)
> T = 100
> u = np.random.normal(0, 1, T)
> v = np.random.normal(0, 1, T)
>
> # Inicializa√ß√£o de X e W
> X = np.zeros(T)
> W = np.zeros(T)
>
> # C√°lculo de X e W
> X[0] = u[0]
> W[0] = v[0]
> for t in range(1, T):
>    X[t] = pi * X[t-1] + u[t]
>    W[t] = rho * W[t-1] + v[t]
>
> # C√°lculo de Y
> Y = X + W
>
> # C√°lculo do termo MA
> epsilon = np.zeros(T)
> for t in range(1, T):
>  epsilon[t] = u[t] - rho*u[t-1] + v[t] - pi*v[t-1]
>
> # Plotagem dos processos
> plt.figure(figsize=(12, 6))
> plt.plot(Y, label='Soma Y_t = X_t + W_t (ARMA(2,1))')
> plt.plot(X, label='Processo AR(1) - X_t')
> plt.plot(W, label='Processo AR(1) - W_t')
> plt.xlabel('Tempo (t)')
> plt.ylabel('Valor')
> plt.title('Simula√ß√£o da Soma de Dois Processos AR(1)')
> plt.legend()
> plt.show()
> ```
> Este c√≥digo demonstra como simular e visualizar o processo ARMA(2,1) resultante da soma de dois processos AR(1). A visualiza√ß√£o permite observar o comportamento resultante, que √© mais complexo do que os processos AR(1) originais.
>
> üí° **Exemplo Num√©rico:**  Vamos calcular os primeiros valores de $Y_t$ para entender como a combina√ß√£o dos processos AR(1) resulta em um processo ARMA(2,1). Usaremos os mesmos par√¢metros $\pi = 0.6$ e $\rho = 0.8$. Para simplificar, vamos usar $u_t$ e $v_t$ com os mesmos valores para os primeiros tr√™s per√≠odos: $u_1=0.5, u_2=-0.2, u_3=0.3$ e $v_1=0.1, v_2=0.4, v_3=-0.3$. Inicialmente assumimos $X_0=0$ e $W_0=0$
>
> - **$X_1 = 0.6 * 0 + 0.5 = 0.5$**
> - **$W_1 = 0.8 * 0 + 0.1 = 0.1$**
> - **$Y_1 = X_1 + W_1 = 0.5 + 0.1 = 0.6$**
>
> - **$X_2 = 0.6 * 0.5 + (-0.2) = 0.1$**
> - **$W_2 = 0.8 * 0.1 + 0.4 = 0.48$**
> - **$Y_2 = X_2 + W_2 = 0.1 + 0.48 = 0.58$**
>
> - **$X_3 = 0.6 * 0.1 + 0.3 = 0.36$**
> - **$W_3 = 0.8 * 0.48 + (-0.3) = 0.084$**
> - **$Y_3 = X_3 + W_3 = 0.36 + 0.084 = 0.444$**
>
> Esses c√°lculos demonstram como a soma de processos AR(1) gera valores $Y_t$ que dependem da combina√ß√£o dos valores de $X_t$ e $W_t$. O processo ARMA(2,1) resultante √© mais complexo que seus componentes individuais e tem uma mem√≥ria mais longa.

#### Casos Especiais: AR(1) com AR(1)
No caso em que ambos os processos s√£o de primeira ordem, AR(1), a representa√ß√£o se simplifica um pouco. Assumindo os processos $X_t$ e $W_t$ definidos como:
$$(1 - \pi L)X_t = u_t$$
$$(1 - \rho L)W_t = v_t$$
A soma $Y_t=X_t + W_t$ pode ser expressa como:
$$(1 - \pi L)(1 - \rho L)Y_t = (1 - \rho L) u_t + (1 - \pi L)v_t$$
$$(1 - (\pi + \rho) L + \pi\rho L^2) Y_t = u_t - \rho u_{t-1} + v_t - \pi v_{t-1}$$
Definindo $\epsilon_t = u_t + v_t$, e $\theta L = - \rho u_{t-1} - \pi v_{t-1}$ resulta em:
$$(1 - (\pi + \rho) L + \pi\rho L^2) Y_t = \epsilon_t + \theta \epsilon_{t-1}$$
Esta representa√ß√£o demonstra que a soma de dois processos AR(1) √© um processo ARMA(2,1).  O par√¢metro $\theta$ √© um par√¢metro de m√©dias m√≥veis que depende de uma combina√ß√£o de $\rho$ e $\pi$, em termos gerais, e n√£o corresponde diretamente aos par√¢metros originais.

> üí° **Exemplo Num√©rico:**  Suponha que $\pi = 0.5$ e $\rho = 0.3$. A representa√ß√£o da soma de AR(1) como um ARMA(2,1) √©:
> $$(1 - 0.8L + 0.15L^2)Y_t = u_t - 0.3u_{t-1} + v_t - 0.5v_{t-1}$$
> O polin√¥mio autorregressivo √© $1 - 0.8L + 0.15L^2$, que corresponde a um processo AR(2). O lado direito, $u_t - 0.3u_{t-1} + v_t - 0.5v_{t-1}$, pode ser expresso como um processo MA(1), com um par√¢metro $\theta$ determinado pela combina√ß√£o dos par√¢metros dos componentes originais. No entanto, o lado direito n√£o √© um processo de m√©dias m√≥veis puro, mas sim uma combina√ß√£o de dois processos MA(1). A combina√ß√£o das ra√≠zes dos polin√¥mios AR e MA √© o que resulta na representa√ß√£o ARMA(2,1).

#### A Fun√ß√£o Geradora de Autocovari√¢ncia (FGAC) da Soma
A fun√ß√£o geradora de autocovari√¢ncia (FGAC) da soma, $Y_t = X_t + W_t$, √© dada pela soma das fun√ß√µes geradoras de autocovari√¢ncia dos processos individuais, como visto em outros cap√≠tulos:
$$g_Y(z) = g_X(z) + g_W(z)$$
onde $g_X(z)$ e $g_W(z)$ s√£o as FGACs dos processos $X_t$ e $W_t$, respetivamente. Para um processo AR(p) a FGAC √© dada por
$$g_X(z) = \sigma^2 \frac{1}{\phi(z)\phi(z^{-1})}$$
Aplicando esse resultado √† soma de processos AR, temos:
$$g_Y(z) = \sigma_u^2 \frac{1}{(1-\pi z)(1-\pi z^{-1})} + \sigma_v^2 \frac{1}{(1-\rho z)(1-\rho z^{-1})}$$
A FGAC do processo resultante √© dada pela soma das FGACs dos componentes. Essa representa√ß√£o formal demonstra como a estrutura temporal de processos individuais √© combinada na soma, e como os par√¢metros dos processos componentes influenciam a estrutura temporal do processo resultante. As propriedades de autocorrela√ß√£o, no entanto, s√£o mais complexas do que uma simples soma dos par√¢metros dos componentes individuais.

> üí° **Exemplo Num√©rico:**  Retomando o exemplo com $\pi = 0.6$ e $\rho = 0.8$ (e assumindo vari√¢ncias $\sigma_u^2 = 1$ e $\sigma_v^2 = 1$):
> $$g_Y(z) = \frac{1}{(1-0.6z)(1-0.6z^{-1})} + \frac{1}{(1-0.8z)(1-0.8z^{-1})}$$
> Esta representa√ß√£o, embora simples, mostra como as FGACs dos componentes individuais se combinam na soma. √â importante destacar que essa combina√ß√£o n√£o resulta necessariamente em um processo AR puro, mas sim em um processo ARMA, devido √† estrutura de defasagem dos ru√≠dos brancos resultantes.
>
> üí° **Exemplo Num√©rico:**  Para um processo AR(1) com $\pi = 0.7$ e vari√¢ncia do ru√≠do $\sigma^2 = 0.5$, a FGAC √©
> $$g_X(z) = 0.5 \frac{1}{(1-0.7z)(1-0.7z^{-1})} = 0.5 \frac{1}{1 - 0.7z - 0.7z^{-1} + 0.49}$$
> Esta express√£o descreve como as autocovari√¢ncias do processo decaem ao longo do tempo, com base no par√¢metro $\pi = 0.7$.
> Se somarmos outro processo AR(1) com $\rho = 0.4$ e vari√¢ncia 0.3:
> $$g_W(z) = 0.3 \frac{1}{(1-0.4z)(1-0.4z^{-1})} = 0.3 \frac{1}{1 - 0.4z - 0.4z^{-1} + 0.16}$$
> A FGAC da soma ser√° a soma dessas duas express√µes. A an√°lise da FGAC √© importante pois ela nos permite analisar as autocorrela√ß√µes do processo, que s√£o fundamentais para entender a sua estrutura temporal.

#### Generaliza√ß√£o para Processos AR de Ordem Superior
A an√°lise anterior pode ser generalizada para processos AR de ordens superiores.  Se $X_t$ √© um processo AR($p_1$) e $W_t$ √© um processo AR($p_2$), ent√£o a soma $Y_t = X_t + W_t$ pode ser representada como um processo ARMA, com a parte AR de ordem $p_1 + p_2$.  O resultado geral √© que a soma de dois processos AR resulta em um processo ARMA onde a parte AR tem uma ordem igual √† soma das ordens AR dos dois processos originais, e a parte MA tem uma ordem inferior √† soma das ordens dos processos originais. Em particular, se $p_1=p_2=1$, ent√£o, como visto anteriormente, a soma √© um processo ARMA(2,1). Em termos gerais, a soma de um processo AR(p) e um processo AR(q) resulta num processo ARMA de ordem (p+q, max(p, q)-1).

**Proposi√ß√£o 1**
A ordem do polin√¥mio MA resultante da soma de dois processos AR √© no m√°ximo $max(p_1,p_2)$, onde $p_1$ e $p_2$ s√£o as ordens dos processos AR originais.
*Prova:*
I. Seja $X_t$ um processo AR($p_1$): $\phi_X(L)X_t = u_t$.
II. Seja $W_t$ um processo AR($p_2$): $\phi_W(L)W_t = v_t$.
III. A soma $Y_t=X_t+W_t$ √©: $\phi_X(L)\phi_W(L)Y_t = \phi_W(L)u_t + \phi_X(L)v_t$.
IV. $\phi_W(L)u_t$ √© um processo MA de ordem $p_2$ e $\phi_X(L)v_t$ √© um processo MA de ordem $p_1$.
V. A soma de dois processos MA resulta num processo MA de ordem igual ao m√°ximo das ordens dos componentes. Portanto, a parte MA de $Y_t$ tem ordem no m√°ximo $max(p_1,p_2)$. $\blacksquare$

### Formaliza√ß√£o Matem√°tica

**Defini√ß√£o 1.1**: Um processo autorregressivo AR(p) pode ser representado como:
$$\phi(L) X_t = u_t$$
onde $u_t$ √© um ru√≠do branco, e $\phi(L)$ √© um polin√¥mio de ordem p no operador de retardo $L$:
$$\phi(L) = 1 - \phi_1 L - \phi_2 L^2 - \ldots - \phi_p L^p$$

**Teorema 1**:
A soma de dois processos autorregressivos independentes, $X_t$ de ordem $p_1$ e $W_t$ de ordem $p_2$, resulta em um processo ARMA, cuja representa√ß√£o geral √© dada por:
$$ \phi_Y(L) Y_t = \theta_Y(L) \epsilon_t $$
onde $Y_t = X_t + W_t$,  $\phi_Y(L)$ √© um polin√¥mio autorregressivo de ordem $p_1 + p_2$, e $\theta_Y(L)$ √© um polin√¥mio de m√©dias m√≥veis com uma ordem que √© menor ou igual a  $p_1 + p_2 - 1$, e  $\epsilon_t$ √© um ru√≠do branco.
*Prova:*
I. Sejam $X_t$ e $W_t$ dois processos AR, definidos por:
$$\phi_X(L)X_t = u_t$$
$$\phi_W(L)W_t = v_t$$
onde $u_t$ e $v_t$ s√£o ru√≠dos brancos.
II. A soma $Y_t = X_t + W_t$ pode ser obtida multiplicando a primeira equa√ß√£o por $\phi_W(L)$ e a segunda por $\phi_X(L)$:
$$\phi_W(L)\phi_X(L)X_t = \phi_W(L)u_t$$
$$\phi_X(L)\phi_W(L)W_t = \phi_X(L)v_t$$
III. Somando ambas as equa√ß√µes, obtemos:
$$\phi_W(L)\phi_X(L)(X_t + W_t) = \phi_W(L)u_t + \phi_X(L)v_t$$
IV.  Definindo $\phi_Y(L) = \phi_X(L)\phi_W(L)$ e $Y_t = X_t + W_t$, temos:
$$\phi_Y(L) Y_t = \phi_W(L)u_t + \phi_X(L)v_t$$
V. O polin√¥mio resultante $\phi_Y(L)$ tem ordem $p_Y = p_1 + p_2$.
VI. O lado direito da equa√ß√£o, $\phi_W(L)u_t + \phi_X(L)v_t$, corresponde a uma soma de processos de m√©dias m√≥veis (ponderadas), que pode ser expresso como um novo processo de m√©dias m√≥veis $\theta_Y(L)\epsilon_t$, de ordem no m√°ximo $p_1+p_2 -1$, mas que, em geral, pode ter uma ordem inferior.
VII. Portanto, a soma $Y_t$ pode ser expressa como um processo ARMA, onde a ordem do componente AR √© $p_Y = p_1+p_2$ e a ordem do componente MA √© menor ou igual a $p_1+p_2 -1$. $\blacksquare$

**Lema 1**
A fun√ß√£o geradora de autocovari√¢ncia da soma de dois processos AR independentes √© a soma das fun√ß√µes geradoras de autocovari√¢ncia dos processos individuais.
*Prova:*
I. Sejam $X_t$ e $W_t$ dois processos AR independentes.
II. Definimos a fun√ß√£o geradora de autocovari√¢ncia (FGAC) de um processo $Z_t$ como:
$$g_Z(z) = \sum_{j=-\infty}^{\infty} \gamma_j^z z^j$$
III. A autocovari√¢ncia da soma $Y_t = X_t + W_t$ no lag $j$ √© dada por:
$$\gamma_j^y = E[(X_t+W_t)(X_{t-j} + W_{t-j})] = \gamma_j^x + \gamma_j^w $$
devido √† independ√™ncia entre $X_t$ e $W_t$.
IV. A fun√ß√£o geradora de autocovari√¢ncia de $Y_t$ √©:
$$g_Y(z) = \sum_{j=-\infty}^{\infty} \gamma_j^y z^j = \sum_{j=-\infty}^{\infty} (\gamma_j^x + \gamma_j^w) z^j = \sum_{j=-\infty}^{\infty} \gamma_j^x z^j + \sum_{j=-\infty}^{\infty} \gamma_j^w z^j = g_X(z) + g_W(z)$$
V. Portanto, a FGAC da soma √© a soma das FGACs dos componentes individuais.  $\blacksquare$
Este lema estabelece que, mesmo na soma de processos AR, a propriedade da soma das FGACs se mant√©m.

**Lema 1.1**
Se $X_t$ √© um processo AR(1) com par√¢metro $\pi$ e $W_t$ √© um processo AR(1) com par√¢metro $\rho$, ent√£o a soma $Y_t=X_t+W_t$ pode ser expressa como um processo ARMA(2,1).
*Prova:*
I. Seja $X_t$ um processo AR(1):
$$(1 - \pi L)X_t = u_t$$
II. Seja $W_t$ um processo AR(1):
$$(1 - \rho L)W_t = v_t$$
III.  Multiplicando a primeira equa√ß√£o por $(1-\rho L)$ e a segunda por $(1-\pi L)$, e somando, temos:
$$(1-\pi L)(1-\rho L)(X_t + W_t) = (1-\rho L)u_t + (1-\pi L)v_t$$
IV. Substituindo $Y_t = X_t + W_t$:
$$(1 - (\pi+\rho)L + \pi \rho L^2)Y_t = u_t - \rho u_{t-1} + v_t - \pi v_{t-1}$$
V. O lado esquerdo √© um polin√¥mio de ordem dois em $L$, correspondente ao componente AR de ordem 2, enquanto que o lado direito √© um polin√¥mio de ordem 1 em $L$, que corresponde a um componente MA de ordem 1.
VI.  Portanto, a representa√ß√£o resultante √© um processo ARMA(2,1). $\blacksquare$

**Teorema 1.1**
A soma de $n$ processos autorregressivos independentes $X_{1t}, X_{2t}, ..., X_{nt}$ de ordens $p_1, p_2, ..., p_n$, resulta em um processo ARMA com parte AR de ordem $\sum_{i=1}^{n} p_i$.
*Prova:*
I. A prova segue por indu√ß√£o. O caso base ($n=2$) √© coberto pelo Teorema 1.
II. Assuma que a soma de $k$ processos AR resulta num processo ARMA com parte AR de ordem $\sum_{i=1}^{k} p_i$.
III. Considere a soma de $k+1$ processos AR. O resultado da soma dos primeiros $k$ processos AR, denotado como $Z_t$, √© um processo ARMA, com componente AR de ordem $\sum_{i=1}^{k} p_i$.
IV. Adicionando o processo $X_{(k+1)t}$ de ordem $p_{k+1}$ a $Z_t$, temos a soma de um processo ARMA com parte AR de ordem $\sum_{i=1}^{k} p_i$ e um processo AR de ordem $p_{k+1}$. Pelo Teorema 1, a parte AR do processo resultante ser√° de ordem $\sum_{i=1}^{k} p_i + p_{k+1} = \sum_{i=1}^{k+1} p_i$.
V. Portanto, por indu√ß√£o, a soma de $n$ processos autorregressivos independentes resulta em um processo ARMA com parte AR de ordem $\sum_{i=1}^{n} p_i$. $\blacksquare$

### Implica√ß√µes e Interpreta√ß√µes
A soma de processos AR resulta em um processo ARMA com caracter√≠sticas que combinam as propriedades dos processos originais. Os par√¢metros do processo ARMA resultante n√£o s√£o obtidos por simples soma dos par√¢metros dos processos componentes, mas sim por manipula√ß√µes e opera√ß√µes polinomiais. A FGAC da soma √© a soma das FGACs dos componentes individuais. A representa√ß√£o com operadores de defasagem oferece uma forma elegante e concisa de analisar as propriedades temporais de tais processos. A ordem do polin√¥mio AR resultante √© a soma das ordens AR dos processos componentes e, no caso geral, o polin√¥mio de m√©dias m√≥veis resultante n√£o tem ordem correspondente aos dos componentes individuais.
A an√°lise deste cap√≠tulo formaliza como a combina√ß√£o linear de processos autorregressivos pode levar a estruturas de s√©ries temporais mais complexas do que cada processo por si s√≥. A representa√ß√£o com operadores de defasagem permite o estudo da estabilidade e causalidade dos processos resultantes atrav√©s das ra√≠zes dos polin√¥mios em $L$.

### Conclus√£o
Este cap√≠tulo explorou a soma de processos autorregressivos, demonstrando que tal opera√ß√£o resulta em um processo ARMA, onde as propriedades de autocorrela√ß√£o dependem dos par√¢metros dos processos originais. A representa√ß√£o com operadores de defasagem foi utilizada para analisar os polin√¥mios AR e MA do processo resultante. Embora a deriva√ß√£o para modelos ARMA seja mais complexa do que para MA e AR individuais, esta an√°lise fornece uma compreens√£o mais profunda das propriedades de s√©ries temporais resultantes da combina√ß√£o linear de modelos, enriquecendo a caixa de ferramentas de modelagem para um p√∫blico com conhecimento avan√ßado em matem√°tica, modelos estat√≠sticos e an√°lise de dados. Os resultados estabelecem que a soma de processos AR resulta num processo ARMA com uma estrutura temporal mais rica.

### Refer√™ncias
[^4.7.1]: ... *[Defini√ß√£o de um processo MA(1)]*
[^4.7.2]: ... *[Autocovari√¢ncias de um processo MA(1)]*
[^4.7.3]: ... *[Defini√ß√£o de ru√≠do branco]*
[^4.7.5]: ... *[Defini√ß√£o da s√©rie Y como soma de MA(1) e ru√≠do branco]*
[^4.7.7]: ... *[Representa√ß√£o MA(1) para Y]*
[^4.7.15]: ... *[Reescrita da representa√ß√£o MA(1) com u e v]*
[^4.7.16]: ... *[Lag distribu√≠do da serie epsilon]*
[^4.7.21]: ... *[Soma de MA resulta em MA]*
[^4.7.22]: ... *[Representa√ß√£o AR(1)]*
[^4.7.23]: ... *[Representa√ß√£o AR(1)]*
[^4.7.27]: ... *[Soma de AR(1) com AR(1) resulta em um ARMA(2,1)]*
<!-- END -->
