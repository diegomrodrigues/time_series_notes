## Atualiza√ß√£o de Proje√ß√µes Lineares com Res√≠duos e Ortogonaliza√ß√£o

### Introdu√ß√£o
Este cap√≠tulo expande o conceito de atualiza√ß√£o de proje√ß√µes lineares, focando em como calcular a atualiza√ß√£o atrav√©s da proje√ß√£o em uma base de vari√°veis onde a nova informa√ß√£o √© n√£o correlacionada com a informa√ß√£o anterior, como derivado da fatora√ß√£o triangular [^4]. Construindo sobre os conceitos apresentados anteriormente [^4], vamos detalhar o processo de atualiza√ß√£o, que envolve a proje√ß√£o do res√≠duo da nova informa√ß√£o em uma base ortogonal, e a express√£o de atualiza√ß√£o, que utiliza uma pondera√ß√£o dos momentos do res√≠duo. O objetivo principal √© demonstrar como a proje√ß√£o em uma base ortogonal simplifica o processo de atualiza√ß√£o de proje√ß√µes lineares e fornece um entendimento mais profundo de como novas informa√ß√µes s√£o incorporadas em previs√µes existentes.

### Atualiza√ß√£o da Proje√ß√£o em uma Base Ortogonal
A atualiza√ß√£o de uma proje√ß√£o linear envolve o ajuste dos coeficientes com base em informa√ß√µes adicionais, e essa atualiza√ß√£o pode ser simplificada ao projetar os res√≠duos da nova informa√ß√£o em uma base ortogonal, ou seja, em um conjunto de vari√°veis onde a covari√¢ncia entre elas √© zero [^4]. Essa abordagem √© baseada na ideia de decompor a nova informa√ß√£o em componentes que s√£o ortogonais √† informa√ß√£o original. Esta estrat√©gia ajuda a isolar a informa√ß√£o adicional e us√°-la de forma mais eficaz para atualizar a previs√£o existente, e que tamb√©m foi utilizada em cap√≠tulos anteriores para derivar a forma recursiva das proje√ß√µes lineares.
Vamos considerar o vetor de vari√°veis aleat√≥rias $Y = (Y_1, \ldots, Y_n)'$.  Supomos que temos uma proje√ß√£o inicial $P(Y_{n+1}|Y_1, \ldots, Y_n)$, e que recebemos uma nova vari√°vel $Y_{n+2}$. Para atualizar a proje√ß√£o, projetamos $Y_{n+2}$ no espa√ßo de $Y_1,\ldots, Y_n$, obtendo o res√≠duo $\hat{Y}_{n+2}$, que, por constru√ß√£o, √© ortogonal a $Y_1,\ldots, Y_n$. Para atualizar a proje√ß√£o, adicionamos o termo $\frac{Cov(Y_{n+2},\hat{Y}_{n+1})}{Var(\hat{Y}_{n+1})}\hat{Y}_{n+1}$ √† proje√ß√£o anterior. A recurs√£o da proje√ß√£o linear explora essa propriedade para obter os novos coeficientes a partir das informa√ß√µes anteriores [^4, Teorema 2.1].

Em vez de usar diretamente $Y_{n+2}$, podemos projet√°-la em um espa√ßo onde ela √© ortogonal a $Y_1, \ldots, Y_n$. Este espa√ßo √© gerado pelos res√≠duos das proje√ß√µes sequenciais $\hat{Y_i}$ obtidos da fatora√ß√£o triangular da matriz de covari√¢ncia de $Y$.  A proje√ß√£o de $Y_{n+2}$ nos res√≠duos √© dada por:
$$ P(Y_{n+2}|\hat{Y_1}, \ldots, \hat{Y_n})  = \sum_{i=1}^n \frac{Cov(Y_{n+2}, \hat{Y_i})}{Var(\hat{Y_i})}\hat{Y_i} $$
onde os $\hat{Y_i}$ s√£o os res√≠duos das proje√ß√µes lineares sequenciais de $Y_i$ em $Y_1, \ldots, Y_{i-1}$ , obtidos atrav√©s da fatora√ß√£o triangular. Como os $\hat{Y_i}$ s√£o ortogonais entre si, o c√°lculo de cada coeficiente na combina√ß√£o linear se torna mais simples, e √© dado pela raz√£o entre a covari√¢ncia entre $Y_{n+2}$ e $\hat{Y_i}$ e a vari√¢ncia de $\hat{Y_i}$.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um vetor de vari√°veis aleat√≥rias $Y = (Y_1, Y_2, Y_3, Y_4)'$, onde as vari√°veis s√£o relacionadas entre si como visto em se√ß√µes anteriores. Para simplificar, vamos usar os valores de covari√¢ncia e fatora√ß√£o j√° calculados anteriormente:
>
> $$\Omega = \begin{bmatrix} 4 & 2 & 1 & 0.5 \\ 2 & 5 & 2 & 1 \\ 1 & 2 & 6 & 1.5 \\ 0.5 & 1 & 1.5 & 7 \end{bmatrix}$$
>
> As matrizes da fatora√ß√£o triangular s√£o dadas por:
>
> $$A = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0.5 & 1 & 0 & 0 \\ 0.25 & 0.375 & 1 & 0 \\ 0.125 & 0.1875 & 0.2108 & 1 \end{bmatrix}$$
>
> $$D = \begin{bmatrix} 4 & 0 & 0 & 0 \\ 0 & 4 & 0 & 0 \\ 0 & 0 & 5.1875 & 0 \\ 0 & 0 & 0 & 6.5666 \end{bmatrix}$$
>
>
> As vari√°veis transformadas s√£o dadas por:
>
>  - $\hat{Y_1} = Y_1$
>  - $\hat{Y_2} = Y_2 - 0.5Y_1$
>  - $\hat{Y_3} = Y_3 - 0.375Y_2 + 0.125Y_1$
>  - $\hat{Y_4} = Y_4 - 0.2108Y_3 -0.1875(Y_2 - 0.5Y_1) - 0.125Y_1 = Y_4 - 0.2108Y_3 -0.1875Y_2 + 0.09375Y_1 - 0.125Y_1= Y_4 - 0.2108Y_3 -0.1875Y_2-0.03125Y_1$.
>
> **Passo 1: Proje√ß√£o Inicial**
>
> Considere a proje√ß√£o de $Y_4$ em $Y_1$, $Y_2$ e $Y_3$ como obtida anteriormente:
>
>  $P(Y_4|Y_1, Y_2, Y_3) = 0.125 Y_1 + 0.1875 Y_2 + 0.2108 Y_3$.
>
> **Passo 2: Proje√ß√£o em uma Base Ortogonal**
>
>  Vamos agora projetar $Y_4$ nos res√≠duos $\hat{Y_1}, \hat{Y_2}, \hat{Y_3}$:
>
> $$ P(Y_4|\hat{Y_1}, \hat{Y_2}, \hat{Y_3}) = \frac{Cov(Y_4, \hat{Y_1})}{Var(\hat{Y_1})}\hat{Y_1} + \frac{Cov(Y_4, \hat{Y_2})}{Var(\hat{Y_2})}\hat{Y_2} +  \frac{Cov(Y_4, \hat{Y_3})}{Var(\hat{Y_3})}\hat{Y_3} $$
>
>  Os coeficientes dessas proje√ß√µes s√£o precisamente os coeficientes $a_{4i}$ da matriz $A$ que foram calculados no exemplo anterior, ponderados pelos erros da proje√ß√£o, que s√£o as ra√≠zes quadradas dos elementos da matriz diagonal $D$:
>  - $\frac{Cov(Y_4, \hat{Y_1})}{Var(\hat{Y_1})} = 0.125$
>  - $\frac{Cov(Y_4, \hat{Y_2})}{Var(\hat{Y_2})} = 0.1875$
>  - $\frac{Cov(Y_4, \hat{Y_3})}{Var(\hat{Y_3})} = 0.2108$
>
>   -  Substituindo os valores obtemos a proje√ß√£o em termos dos res√≠duos
>
>   $P(Y_4|\hat{Y_1}, \hat{Y_2}, \hat{Y_3}) = 0.125\hat{Y_1} + 0.1875\hat{Y_2} + 0.2108\hat{Y_3} $
>
>   E substituindo os valores de $\hat{Y_i}$, obtemos a proje√ß√£o final em termos de $Y_i$:
>
>    $P(Y_4|Y_1, Y_2, Y_3) = 0.125 Y_1 + 0.1875(Y_2 - 0.5 Y_1) + 0.2108(Y_3 - 0.375 Y_2 + 0.125 Y_1) $
>
>
> Este exemplo mostra como a fatora√ß√£o triangular nos permite projetar de forma iterativa utilizando os res√≠duos, e obter os coeficientes da proje√ß√£o nos res√≠duos sem a necessidade de opera√ß√µes matriciais complexas.

**Lema 5.1**
Os coeficientes da proje√ß√£o de $Y_{n+1}$ no espa√ßo gerado pelos res√≠duos $\hat{Y_1}, \ldots, \hat{Y_n}$ s√£o dados por $\frac{Cov(Y_{n+1}, \hat{Y_i})}{Var(\hat{Y_i})}$, e esses coeficientes podem ser obtidos a partir dos elementos da matriz triangular A e da matriz diagonal D.

*Prova:*
I. A proje√ß√£o de $Y_{n+1}$ no espa√ßo gerado por $\hat{Y_1}, \ldots, \hat{Y_n}$ √© dada por
 $$ P(Y_{n+1}|\hat{Y_1}, \ldots, \hat{Y_n}) = \sum_{i=1}^{n} \frac{Cov(Y_{n+1}, \hat{Y_i})}{Var(\hat{Y_i})}\hat{Y_i} $$
II.  Pela constru√ß√£o dos res√≠duos $\hat{Y_i}$, temos que $\hat{Y_i}$ √© ortogonal a todos os $\hat{Y_j}$ com $j < i$. Portanto, a proje√ß√£o sobre a base ortogonal √© obtida da proje√ß√£o sobre cada um dos res√≠duos separadamente.
III.  O coeficiente da proje√ß√£o √© dado por $\frac{Cov(Y_{n+1}, \hat{Y_i})}{Var(\hat{Y_i})}$, que, no contexto da fatora√ß√£o triangular, corresponde ao elemento (n+1,i) da matriz $A^{-1}$ ap√≥s a fatora√ß√£o, ponderado pela vari√¢ncia de $\hat{Y_i}$, que corresponde ao elemento $d_{ii}$ da matriz D.
IV.  Portanto, os coeficientes da proje√ß√£o de $Y_{n+1}$ nos res√≠duos s√£o obtidos diretamente dos elementos de $A$ e $D$.
‚ñ†
A proje√ß√£o em uma base ortogonal simplifica o processo de atualiza√ß√£o, pois as vari√°veis da base s√£o descorrelacionadas entre si. Como resultado, cada coeficiente √© calculado de forma independente dos outros, e a proje√ß√£o de uma vari√°vel nas informa√ß√µes anteriores pode ser feita sequencialmente. A fatora√ß√£o triangular fornece os ingredientes necess√°rios para fazer isso de forma eficiente.

**Lema 5.2**
A matriz de covari√¢ncia dos res√≠duos $\hat{Y} = (\hat{Y}_1, \dots, \hat{Y}_n)'$ √© uma matriz diagonal, onde os elementos da diagonal s√£o as vari√¢ncias dos res√≠duos, dadas por $Var(\hat{Y}_i) = D_{ii}$.
*Prova:*
I. Pela constru√ß√£o dos res√≠duos na fatora√ß√£o triangular, sabemos que $\hat{Y}_i$ √© ortogonal a $\hat{Y}_j$ para $i \neq j$.
II. Ortogonalidade implica que $Cov(\hat{Y}_i, \hat{Y}_j) = 0$ para $i \neq j$.
III. A matriz de covari√¢ncia de $\hat{Y}$ √© definida como a matriz cujos elementos s√£o $Cov(\hat{Y}_i, \hat{Y}_j)$.
IV. Portanto, a matriz de covari√¢ncia de $\hat{Y}$ tem zeros fora da diagonal, e os elementos da diagonal s√£o as vari√¢ncias $Var(\hat{Y}_i)$.
V. Pela fatora√ß√£o triangular, essas vari√¢ncias s√£o os elementos diagonais da matriz $D$. Portanto, a matriz de covari√¢ncia de $\hat{Y}$ √© a matriz diagonal $D$.
‚ñ†

### Express√£o para Atualiza√ß√£o de Proje√ß√µes Lineares
A fatora√ß√£o triangular tamb√©m auxilia a obter a express√£o para atualiza√ß√£o de proje√ß√µes lineares. Seja $P(Y_{n+1}|Y_1, \ldots, Y_n)$ a proje√ß√£o de $Y_{n+1}$ em $Y_1, \ldots, Y_n$. Quando uma nova vari√°vel $Y_{n+2}$ se torna dispon√≠vel, a proje√ß√£o atualizada √© dada por:
$$P(Y_{n+2}|Y_1, \ldots, Y_{n+1}) = P(Y_{n+2}|Y_1, \ldots, Y_n) + \frac{Cov(Y_{n+2}, \hat{Y}_{n+1})}{Var(\hat{Y}_{n+1})}\hat{Y}_{n+1}$$
onde $\hat{Y}_{n+1} = Y_{n+1} - P(Y_{n+1}|Y_1, \ldots, Y_n)$ √© o res√≠duo da proje√ß√£o de $Y_{n+1}$ em $Y_1, \ldots, Y_n$.  A fatora√ß√£o triangular nos permite obter os componentes dessa express√£o:
1.  A proje√ß√£o inicial $P(Y_{n+2}|Y_1, \ldots, Y_n)$ √© obtida iterativamente utilizando os elementos da matriz $A$.
2.  O res√≠duo $\hat{Y}_{n+1}$ √© obtido como $Y_{n+1} - P(Y_{n+1}|Y_1, \ldots, Y_n)$.
3.  A vari√¢ncia do res√≠duo, $Var(\hat{Y}_{n+1})$ √© obtida diretamente do elemento diagonal correspondente da matriz $D$.
4.  A covari√¢ncia $Cov(Y_{n+2}, \hat{Y}_{n+1})$ tamb√©m pode ser calculada a partir dos elementos da matriz A expandida.

Esta express√£o mostra como a atualiza√ß√£o da proje√ß√£o linear √© feita adicionando ao valor da proje√ß√£o anterior o res√≠duo da nova vari√°vel ponderado por uma raz√£o entre a covari√¢ncia da vari√°vel atual com o res√≠duo e o erro associado √† proje√ß√£o do res√≠duo, de forma an√°loga ao encontrado na atualiza√ß√£o de proje√ß√µes lineares (4.5.16) [^4, ^4.5.16]. Essa √© a forma generalizada do resultado do teorema 2.1, que exploramos no cap√≠tulo anterior.

> üí° **Exemplo Num√©rico:**
> Vamos considerar um cen√°rio onde temos as seguintes vari√°veis aleat√≥rias:
>
> $Y_1$: Temperatura m√©dia di√°ria em uma cidade (em graus Celsius).
> $Y_2$: N√∫mero de sorvetes vendidos na mesma cidade.
> $Y_3$: N√∫mero de protetores solares vendidos.
>
>  e seus valores ao longo de alguns dias. Suponha que j√° obtivemos as seguintes proje√ß√µes lineares iniciais com base em dados hist√≥ricos e que agora queremos incorporar dados de um novo dia ($Y_4$):
>
> $P(Y_2|Y_1) = 0.7Y_1 + 5$ (Mais temperatura, mais sorvetes)
> $P(Y_3|Y_1, Y_2) = 0.2Y_1 + 0.5Y_2 + 2$ (Mais temperatura e sorvetes, mais protetores)
>
> E tamb√©m calculamos as seguintes matrizes de fatora√ß√£o triangular, que nos fornecem as proje√ß√µes sequenciais em forma de res√≠duos ortogonais:
>
> $$A = \begin{bmatrix} 1 & 0 & 0 \\ 0.7 & 1 & 0 \\ 0.2 & 0.5 & 1 \end{bmatrix}$$
>
> $$D = \begin{bmatrix} 5 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 1 \end{bmatrix}$$
>
> Os res√≠duos s√£o:
> - $\hat{Y_1} = Y_1$
> - $\hat{Y_2} = Y_2 - 0.7Y_1$
> - $\hat{Y_3} = Y_3 - 0.2Y_1 - 0.5Y_2$
>
>
> Agora, um novo dia chega, e temos um novo valor para a temperatura $Y_1 = 25$, $Y_2 = 23$ e  $Y_3 = 21$.  Para fins de ilustra√ß√£o, vamos considerar uma proje√ß√£o fict√≠cia de $Y_4$ no espa√ßo gerado por $Y_1, Y_2, Y_3$:
>
> $P(Y_4|Y_1, Y_2, Y_3) = 0.1Y_1 + 0.3Y_2 + 0.4Y_3 + 1$
>
> A partir desses valores, podemos calcular o res√≠duo associado a $Y_3$
> $\hat{Y_3} = 21 - (0.2 * 25 + 0.5 * 23) = 21 - 16.5 = 4.5$
>
> Assuma que obtemos um novo valor de $Y_4 = 10$. Agora, para atualizar a proje√ß√£o de $Y_4$  com este novo dado, vamos simular uma covari√¢ncia $Cov(Y_4, \hat{Y_3}) = 1.5$. Sabemos tamb√©m que $Var(\hat{Y_3}) = D_{33} = 1$. Utilizando o teorema 5.1, a proje√ß√£o atualizada de $Y_4$ seria:
>
>  $P(Y_4|Y_1, Y_2, Y_3) = P(Y_4|Y_1, Y_2) + \frac{Cov(Y_4, \hat{Y_3})}{Var(\hat{Y_3})}\hat{Y_3} $
>
>  $P(Y_4|Y_1, Y_2, Y_3) = P(Y_4|Y_1, Y_2) + \frac{1.5}{1} \hat{Y_3} = P(Y_4|Y_1, Y_2) + 1.5\hat{Y_3}$
>
>  Note que $\hat{Y_3} = Y_3 - P(Y_3|Y_1,Y_2) = Y_3 - (0.2Y_1 + 0.5Y_2 + 2)$
>
>  Para fins de simplicidade e para ilustrar a atualiza√ß√£o com o res√≠duo, vamos assumir que a proje√ß√£o $P(Y_4|Y_1, Y_2) = 0.1Y_1 + 0.3Y_2 +1$ (que n√£o usa o valor de $Y_3$). Ent√£o, substituindo os valores:
>
>  $P(Y_4|Y_1, Y_2, Y_3) = (0.1 \times 25 + 0.3 \times 23 + 1) + 1.5 \times 4.5 = 2.5 + 6.9 + 1 + 6.75 = 17.15$
>
> Este exemplo mostra como a proje√ß√£o pode ser atualizada com novos dados e como a proje√ß√£o em uma base ortogonal facilita os c√°lculos, utilizando as propriedades de ortogonalidade para simplificar a atualiza√ß√£o. O novo valor de $Y_4$, que √© $10$, est√° distante do valor projetado, $17.15$, o que indica que a proje√ß√£o original n√£o estava boa, ou que existem outros fatores que influenciam $Y_4$.

**Teorema 5.1**
A atualiza√ß√£o de proje√ß√µes lineares, calculada atrav√©s da proje√ß√£o em uma base ortogonal utilizando fatores triangulares, √© dada pela express√£o:
$$P(Y_{n+2}|Y_1, \ldots, Y_{n+1}) = P(Y_{n+2}|Y_1, \ldots, Y_n) + \frac{Cov(Y_{n+2}, \hat{Y}_{n+1})}{Var(\hat{Y}_{n+1})}\hat{Y}_{n+1}$$
onde $\hat{Y}_{n+1}$ √© o res√≠duo da proje√ß√£o de $Y_{n+1}$ em $Y_1, \ldots, Y_n$, e a covari√¢ncia e a vari√¢ncia podem ser obtidas a partir das matrizes triangulares A e D.

*Prova:*
I.  Como vimos em se√ß√µes anteriores, o res√≠duo $\hat{Y}_{n+1}$ √© ortogonal ao espa√ßo gerado por $Y_1, \ldots, Y_n$.
II.  Seja $P(Y_{n+2}|Y_1, \ldots, Y_{n+1})$ a proje√ß√£o de $Y_{n+2}$ no espa√ßo gerado por $Y_1, \ldots, Y_{n+1}$.
III.  Usando a decomposi√ß√£o em base ortogonal, essa proje√ß√£o pode ser expressa como:
    $$P(Y_{n+2}|Y_1, \ldots, Y_{n+1}) = P(Y_{n+2}|Y_1, \ldots, Y_n) +  P(Y_{n+2}|\hat{Y}_{n+1})$$
    onde $P(Y_{n+2}|Y_1, \ldots, Y_n)$ √© a proje√ß√£o de $Y_{n+2}$ no espa√ßo gerado por $Y_1, \ldots, Y_n$, e $P(Y_{n+2}|\hat{Y}_{n+1})$ √© a proje√ß√£o do res√≠duo  $Y_{n+2}$ no res√≠duo $\hat{Y}_{n+1}$.
IV.  A proje√ß√£o $P(Y_{n+2}|\hat{Y}_{n+1})$ √© dada por $\frac{Cov(Y_{n+2}, \hat{Y}_{n+1})}{Var(\hat{Y}_{n+1})}\hat{Y}_{n+1}$.
V.  Portanto, substituindo na equa√ß√£o inicial, obtemos a express√£o para a atualiza√ß√£o da proje√ß√£o:
$$P(Y_{n+2}|Y_1, \ldots, Y_{n+1}) = P(Y_{n+2}|Y_1, \ldots, Y_n) + \frac{Cov(Y_{n+2}, \hat{Y}_{n+1})}{Var(\hat{Y}_{n+1})}\hat{Y}_{n+1}$$
VI. Como a fatora√ß√£o triangular fornece uma forma para calcular os res√≠duos e os coeficientes, o teorema acima demonstra como obter a proje√ß√£o atualizada utilizando os fatores triangulares de forma recursiva, e demonstra que o processo pode ser interpretado como a proje√ß√£o do res√≠duo da nova vari√°vel na base ortogonal, gerando os fatores ponderadores.
‚ñ†

**Teorema 5.2**
A proje√ß√£o de $Y_{n+2}$ no espa√ßo gerado por $Y_1, \dots, Y_{n+1}$ pode ser expressa como uma combina√ß√£o linear dos res√≠duos $\hat{Y}_1, \dots, \hat{Y}_{n+1}$, onde cada coeficiente √© dado por $\frac{Cov(Y_{n+2}, \hat{Y}_i)}{Var(\hat{Y}_i)}$.
$$ P(Y_{n+2}|Y_1, \dots, Y_{n+1}) = \sum_{i=1}^{n+1} \frac{Cov(Y_{n+2}, \hat{Y}_i)}{Var(\hat{Y}_i)} \hat{Y}_i $$
*Prova:*
I. Sabemos que o espa√ßo gerado por $Y_1, \dots, Y_{n+1}$ √© o mesmo que o espa√ßo gerado pelos res√≠duos $\hat{Y}_1, \dots, \hat{Y}_{n+1}$, uma vez que cada $Y_i$ √© uma combina√ß√£o linear dos res√≠duos at√© $\hat{Y}_i$.
II. Como os res√≠duos s√£o ortogonais entre si, a proje√ß√£o de $Y_{n+2}$ nesse espa√ßo pode ser escrita como uma soma das proje√ß√µes de $Y_{n+2}$ sobre cada res√≠duo.
III. A proje√ß√£o de $Y_{n+2}$ sobre o res√≠duo $\hat{Y}_i$ √© dada por  $\frac{Cov(Y_{n+2}, \hat{Y}_i)}{Var(\hat{Y}_i)} \hat{Y}_i$.
IV. Somando as proje√ß√µes sobre todos os res√≠duos, temos o resultado desejado.
$$ P(Y_{n+2}|Y_1, \dots, Y_{n+1}) = \sum_{i=1}^{n+1} \frac{Cov(Y_{n+2}, \hat{Y}_i)}{Var(\hat{Y}_i)} \hat{Y}_i $$
‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Utilizando o exemplo anterior, vamos obter a proje√ß√£o atualizada de forma recursiva. J√° sabemos que
>
>   $Y_1 = \epsilon_1$
>   $Y_2 = 0.5Y_1 + \epsilon_2$
>   $Y_3 = 0.25Y_1 + 0.375Y_2 + \epsilon_3$
>   $Y_4 = 0.125 Y_1 + 0.1875 Y_2 + 0.2108 Y_3 + \epsilon_4$
>   e tamb√©m que:
>   $\hat{Y_1} = Y_1$
>  $\hat{Y_2} = Y_2 - 0.5Y_1$
>  $\hat{Y_3} = Y_3 - 0.375Y_2 + 0.125Y_1$
>
> A nova proje√ß√£o de $Y_4$, a partir dos resultados anteriores, √©:
>  $P(Y_4|Y_1,Y_2,Y_3) = 0.125Y_1 + 0.1875Y_2 + 0.2108Y_3$
>
> Suponha que temos uma nova vari√°vel, $Y_5$. A proje√ß√£o de $Y_5$ no espa√ßo gerado por $Y_1,Y_2,Y_3,Y_4$ seria:
>  $P(Y_5|Y_1, Y_2, Y_3, Y_4) = P(Y_5|Y_1,Y_2,Y_3) + \frac{Cov(Y_5, \hat{Y}_4)}{Var(\hat{Y}_4)}\hat{Y}_4$
>
> O termo $P(Y_5|Y_1, Y_2, Y_3)$ seria obtido a partir da atualiza√ß√£o da matriz $A$ e da fatora√ß√£o triangular. O res√≠duo de $Y_4$ √© dado por $\hat{Y}_4 = Y_4 - P(Y_4|Y_1,Y_2,Y_3)$, onde $P(Y_4|Y_1,Y_2,Y_3) = 0.125Y_1 + 0.1875Y_2 + 0.2108Y_3$. Para usar a f√≥rmula acima, precisamos calcular a covari√¢ncia entre $Y_5$ e $\hat{Y_4}$ e o erro quadr√°tico m√©dio do res√≠duo $Var(\hat{Y_4})$.
>
> A principal vantagem √© que os coeficientes e os res√≠duos anteriores (para o c√°lculo de $\hat{Y_4}$) j√° foram computados, e podem ser reutilizados.
>
> Vamos assumir que temos valores para $Y_1 = 2$, $Y_2 = 3$, $Y_3 = 4$, e que $Y_4 = 5$. Vamos calcular o valor da proje√ß√£o de $Y_4$:
> $P(Y_4|Y_1,Y_2,Y_3) = 0.125(2) + 0.1875(3) + 0.2108(4) = 0.25 + 0.5625 + 0.8432 = 1.6557$
> O res√≠duo de $Y_4$ √© $\hat{Y}_4 = Y_4 - P(Y_4|Y_1,Y_2,Y_3) = 5 - 1.6557 = 3.3443$.
>
> Para atualizar a proje√ß√£o de $Y_5$, vamos assumir que $Cov(Y_5, \hat{Y}_4) = 2$ e que $Var(\hat{Y}_4) = D_{44} = 6.5666$.
>
> Ent√£o, a atualiza√ß√£o da proje√ß√£o ser√°:
> $P(Y_5|Y_1,Y_2,Y_3, Y_4) = P(Y_5|Y_1,Y_2,Y_3) + \frac{2}{6.5666}\hat{Y}_4$
>
> Vamos assumir que $P(Y_5|Y_1,Y_2,Y_3) = 0.2Y_1 + 0.3Y_2 + 0.1Y_3 = 0.2(2) + 0.3(3) + 0.1(4) = 0.4 + 0.9 + 0.4 = 1.7$.
>
> Ent√£o:
> $P(Y_5|Y_1,Y_2,Y_3, Y_4) = 1.7 + \frac{2}{6.5666}(3.3443) = 1.7 + 1.019 = 2.719$
>
> Assim, a proje√ß√£o de $Y_5$ √© atualizada considerando a informa√ß√£o adicional de $Y_4$ atrav√©s do seu res√≠duo $\hat{Y_4}$. Note que a proje√ß√£o inicial de $Y_5$ sem usar $Y_4$ era $1.7$.

### Conclus√£o
Este cap√≠tulo demonstrou como a fatora√ß√£o triangular pode ser usada para projetar sobre uma base ortogonal, obtendo express√µes que atualizam a proje√ß√£o linear de forma recursiva. A express√£o para atualiza√ß√£o da proje√ß√£o linear, derivada usando os fatores triangulares, √© dada pela soma da proje√ß√£o anterior mais um termo que depende da nova informa√ß√£o ponderada pelos momentos. A compreens√£o do processo de atualiza√ß√£o atrav√©s de proje√ß√µes em bases ortogonais permite obter uma express√£o para atualizar as proje√ß√µes de forma eficiente e recursiva, e esta atualiza√ß√£o pode ser interpretada como a proje√ß√£o do res√≠duo da nova informa√ß√£o em uma base de vari√°veis ortogonal. O resultado √© um processo eficiente e iterativo de atualiza√ß√£o de proje√ß√µes lineares, fundamental em diversas aplica√ß√µes de previs√£o e modelagem.

### Refer√™ncias
[^4]: Se√ß√µes do Cap√≠tulo 4 do livro texto fornecido.
<!-- END -->
