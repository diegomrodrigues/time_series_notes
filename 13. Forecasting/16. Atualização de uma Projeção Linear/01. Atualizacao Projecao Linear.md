## Atualiza√ß√£o de uma Proje√ß√£o Linear

### Introdu√ß√£o
Este cap√≠tulo tem como objetivo aprofundar o entendimento sobre a atualiza√ß√£o de uma proje√ß√£o linear. Expandindo os conceitos de previs√£o e proje√ß√£o linear introduzidos anteriormente, esta se√ß√£o se concentra em como incorporar novas informa√ß√µes para refinar as previs√µes existentes. Exploraremos como os res√≠duos de proje√ß√µes lineares iniciais podem ser utilizados para ajustar os coeficientes quando novas vari√°veis se tornam dispon√≠veis. O processo, que envolve o c√°lculo de proje√ß√µes iniciais e sua atualiza√ß√£o sequencial, √© fundamental tanto para a previs√£o de s√©ries temporais quanto para regress√µes de m√≠nimos quadrados ordin√°rios (OLS) em situa√ß√µes com informa√ß√µes incrementais.

### Conceitos Fundamentais
A atualiza√ß√£o de uma proje√ß√£o linear √© um processo iterativo que utiliza informa√ß√µes adicionais para refinar uma proje√ß√£o existente. Considere que desejamos prever o valor de $Y_3$, com base em uma informa√ß√£o inicial $Y_1$. Uma proje√ß√£o de $Y_3$ com base apenas em $Y_1$ assume a forma $\hat{P}(Y_3|Y_1) = \Omega_{31} \Omega_{11}^{-1} Y_1$ [^4, ^4.5.12].

Agora, suponha que obtemos uma nova informa√ß√£o, representada por $Y_2$. Para atualizar a previs√£o original, precisamos considerar a proje√ß√£o de $Y_2$ em $Y_1$: $\hat{P}(Y_2|Y_1) = \Omega_{21} \Omega_{11}^{-1} Y_1$. A atualiza√ß√£o da proje√ß√£o de $Y_3$ que leva em conta $Y_2$ √© dada por [^4, ^4.5.14]:

$$ \hat{P}(Y_3|Y_2, Y_1) = \hat{P}(Y_3|Y_1) + h_{32} h_{22}^{-1} [Y_2 - \hat{P}(Y_2|Y_1)], $$

onde $h_{32}$ representa o produto esperado dos erros de previs√£o de $Y_3$ e $Y_2$ com base em $Y_1$, e $h_{22}$ √© o erro quadr√°tico m√©dio (MSE) da proje√ß√£o de $Y_2$ em $Y_1$ [^4, ^4.5.14]. Esta formula√ß√£o expressa que a nova proje√ß√£o √© uma combina√ß√£o da proje√ß√£o inicial mais uma corre√ß√£o com base na nova informa√ß√£o $Y_2$. Essencialmente, o novo termo $Y_2 - \hat{P}(Y_2|Y_1)$ √© o que $Y_2$ "revela" de novo que n√£o foi levado em conta por $Y_1$.

> üí° **Exemplo Num√©rico:**
>
> Vamos supor que temos as seguintes informa√ß√µes sobre a√ß√µes:
> - $Y_1$: Pre√ßo da a√ß√£o A no dia 1 = 100
> - $Y_2$: Pre√ßo da a√ß√£o B no dia 1 = 150
> - $Y_3$: Pre√ßo da a√ß√£o A no dia 2 (o que queremos prever)
>
> E que as proje√ß√µes iniciais e os valores sejam:
>
> - $\hat{P}(Y_3|Y_1) = 1.1 * Y_1 = 110$ (Proje√ß√£o de $Y_3$ baseada em $Y_1$)
> - $\hat{P}(Y_2|Y_1) = 1.5 * Y_1 = 150$ (Proje√ß√£o de $Y_2$ baseada em $Y_1$)
> - $Y_2 = 160$ (Valor real de $Y_2$ no dia 1)
>
> Precisamos tamb√©m de:
>
> - $h_{32} = E[(Y_3 - \hat{P}(Y_3|Y_1))(Y_2 - \hat{P}(Y_2|Y_1))] = 0.8$ (Covari√¢ncia entre os erros de previs√£o de $Y_3$ e $Y_2$ baseados em $Y_1$).
> - $h_{22} = E[Y_2 - \hat{P}(Y_2|Y_1)]^2 = 10^2 = 100$ (MSE da proje√ß√£o de $Y_2$ em $Y_1$).
>
> Agora, vamos calcular a proje√ß√£o atualizada de $Y_3$ utilizando a f√≥rmula:
>
>  $\hat{P}(Y_3|Y_2, Y_1) = \hat{P}(Y_3|Y_1) + h_{32} h_{22}^{-1} [Y_2 - \hat{P}(Y_2|Y_1)]$
>
> $\hat{P}(Y_3|Y_2, Y_1) = 110 + (0.8 / 100) * (160 - 150)$
> $\hat{P}(Y_3|Y_2, Y_1) = 110 + 0.008 * 10$
> $\hat{P}(Y_3|Y_2, Y_1) = 110 + 0.8 = 110.8$
>
> A nova proje√ß√£o de $Y_3$, considerando $Y_2$, √© 110.8.  Observamos que a proje√ß√£o inicial (110) √© corrigida para cima (110.8), levando em conta a diferen√ßa entre o valor observado de $Y_2$ (160) e a previs√£o feita com $Y_1$ (150).
>
> Este exemplo mostra como novas informa√ß√µes podem ser usadas para ajustar uma previs√£o inicial, tornando-a mais precisa.

Para entender melhor a natureza do multiplicador ($h_{32}h_{22}^{-1}$), definimos o vetor $\hat{Y}(1)$ da seguinte forma [^4, ^4.5.15]:
$$\hat{Y}(1) = E_1Y,$$
onde $E_1$ √© uma matriz que transforma $Y$ em um vetor de res√≠duos, $Y_i - \hat{P}(Y_i | Y_1)$. √â importante notar que a matriz de segundo momento de $\hat{Y}(1)$ √© $E_1 \Omega E_1' = H$ [^4, ^4.5.15], onde $H$ √© uma matriz que cont√©m os MSEs das proje√ß√µes de cada $Y_i$ em $Y_1$.  Especificamente,  $h_{22} = E[Y_2 - \hat{P}(Y_2|Y_1)]^2$, e $h_{32} = E[(Y_3 - \hat{P}(Y_3|Y_1))(Y_2 - \hat{P}(Y_2|Y_1))]$, com $h_{22}$ sendo o MSE da proje√ß√£o de $Y_2$ em $Y_1$ [^4, ^4.5.15].

Com isso, a atualiza√ß√£o da proje√ß√£o linear √© dada por [^4, ^4.5.16]:
$$ \hat{P}(Y_3|Y_2, Y_1) = \hat{P}(Y_3|Y_1) +  \{ E[Y_3 - \hat{P}(Y_3|Y_1)][Y_2 - \hat{P}(Y_2|Y_1)] \} \{ E[Y_2 - \hat{P}(Y_2|Y_1)]^2 \}^{-1} \{ Y_2 - \hat{P}(Y_2|Y_1) \}. $$
Esta express√£o formaliza como a previs√£o inicial de $Y_3$ com base em $Y_1$ √© ajustada por um termo que considera a nova informa√ß√£o de $Y_2$.

#### Rela√ß√£o com Regress√£o OLS
A atualiza√ß√£o de uma proje√ß√£o linear tem uma liga√ß√£o direta com a regress√£o OLS em contextos onde os dados s√£o incrementais. Considere o problema de regredir $Y$ em $X$. Uma vez que calculamos os coeficientes de regress√£o $b$ utilizando uma amostra, e posteriormente recebemos mais informa√ß√µes, n√£o √© necess√°rio refazer o c√°lculo desde o come√ßo. Em vez disso, √© poss√≠vel atualizar os coeficientes de regress√£o com base nos res√≠duos da regress√£o anterior e a nova informa√ß√£o.

Por exemplo, ao realizar uma regress√£o de m√≠nimos quadrados ordin√°rios (OLS),  o vetor de coeficientes $b$ √© determinado de forma a minimizar a soma dos erros ao quadrado (SSE). O m√©todo de atualiza√ß√£o  utiliza os res√≠duos da regress√£o anterior para ajustar os coeficientes quando s√£o recebidas novas informa√ß√µes, de acordo com a l√≥gica da proje√ß√£o linear.

**Proposi√ß√£o 1**
A atualiza√ß√£o dos coeficientes da regress√£o OLS pode ser expressa de forma an√°loga √† atualiza√ß√£o da proje√ß√£o linear. Se $b_1$ s√£o os coeficientes obtidos usando a primeira amostra de dados e $e_1$ s√£o os res√≠duos, quando uma segunda amostra com os valores $X_2$ e $Y_2$ √© obtida, os coeficientes atualizados $b_2$ podem ser expressos como:
$$b_2 = b_1 + (X_2'M_1X_2)^{-1}X_2'M_1(Y_2-X_2b_1)$$
onde $M_1=I-X_1(X_1'X_1)^{-1}X_1'$ √© a matriz de proje√ß√£o ortogonal ao espa√ßo gerado por $X_1$.

*Proof strategy:* Esta proposi√ß√£o estende a l√≥gica da atualiza√ß√£o de proje√ß√µes lineares para a regress√£o OLS. Ela demonstra como os coeficientes podem ser atualizados de forma incremental sem a necessidade de recalcular toda a regress√£o desde o in√≠cio. Esta atualiza√ß√£o √© feita usando os res√≠duos da primeira amostra e as informa√ß√µes adicionais da segunda amostra. O termo $(X_2'M_1X_2)^{-1}X_2'M_1(Y_2-X_2b_1)$ representa a corre√ß√£o a ser adicionada aos coeficientes anteriores.

*Proof:*
Para provar a proposi√ß√£o 1, devemos mostrar que o estimador OLS atualizado, $b_2$, √© obtido por meio da combina√ß√£o dos coeficientes estimados usando a primeira amostra ($b_1$) e um termo que incorpora a nova informa√ß√£o, e este termo tem a forma que apresentamos.
I.  A regress√£o OLS com os dados combinados $(X_1, Y_1)$ e $(X_2, Y_2)$ pode ser escrita como:
    $$
    \begin{bmatrix}
    X_1 \\
    X_2
    \end{bmatrix} b_2 = 
    \begin{bmatrix}
    Y_1 \\
    Y_2
    \end{bmatrix}
    $$
    onde $b_2$ √© o vetor de coeficientes atualizado.

II.  A solu√ß√£o OLS para $b_2$ √© dada por:
$$
b_2 = \left( \begin{bmatrix} X_1' & X_2' \end{bmatrix} \begin{bmatrix} X_1 \\ X_2 \end{bmatrix} \right)^{-1} \begin{bmatrix} X_1' & X_2' \end{bmatrix}  \begin{bmatrix} Y_1 \\ Y_2 \end{bmatrix}
$$
$$
b_2 = (X_1'X_1 + X_2'X_2)^{-1}(X_1'Y_1 + X_2'Y_2)
$$

III.  Sabemos que $b_1 = (X_1'X_1)^{-1}X_1'Y_1$, ent√£o $X_1'Y_1 = X_1'X_1b_1$.  Substituindo isso na equa√ß√£o para $b_2$, temos:
$$b_2 = (X_1'X_1 + X_2'X_2)^{-1}(X_1'X_1b_1 + X_2'Y_2)$$

IV.  Usamos a identidade de Woodbury para a matriz inversa $(A + BC)^{-1} = A^{-1} - A^{-1}B(I + CA^{-1}B)^{-1}CA^{-1}$. Definindo $A = X_1'X_1$, $B = X_2'$, e $C= X_2$ , e usando a identidade:
$$(X_1'X_1 + X_2'X_2)^{-1} = (X_1'X_1)^{-1} - (X_1'X_1)^{-1}X_2'(I + X_2(X_1'X_1)^{-1}X_2')^{-1}X_2(X_1'X_1)^{-1}$$
    Assim,
    $$b_2 = \left[ (X_1'X_1)^{-1} - (X_1'X_1)^{-1}X_2'(I + X_2(X_1'X_1)^{-1}X_2')^{-1}X_2(X_1'X_1)^{-1} \right] (X_1'X_1b_1 + X_2'Y_2)$$

V. Definindo $M_1 = I - X_1(X_1'X_1)^{-1}X_1'$, notamos que $M_1 X_1 = 0$, e tamb√©m $M_1 X_2 = X_2 - X_1(X_1'X_1)^{-1}X_1'X_2$
    Reorganizando os termos na identidade de Woodbury e usando o fato de que $X_1'(I - X_1(X_1'X_1)^{-1}X_1') = X_1'M_1 = 0$ , obtemos
    $$(X_1'X_1 + X_2'X_2)^{-1} = (X_1'X_1)^{-1} - (X_1'X_1)^{-1}X_2'(X_2'M_1X_2+X_2'X_1(X_1'X_1)^{-1}X_1'X_2)^{-1}X_2(X_1'X_1)^{-1}$$
    $$(X_1'X_1 + X_2'X_2)^{-1} = (X_1'X_1)^{-1} - (X_1'X_1)^{-1}X_2'(X_2'M_1X_2)^{-1}X_2(X_1'X_1)^{-1}$$
VI. Multiplicando $(X_1'X_1 + X_2'X_2)^{-1}$ por $X_1'X_1b_1 + X_2'Y_2$, obtemos
$$b_2 = b_1 - (X_1'X_1)^{-1}X_2'(X_2'M_1X_2)^{-1}X_2(X_1'X_1)^{-1} X_1'X_1b_1 + (X_1'X_1 + X_2'X_2)^{-1} X_2'Y_2$$
$$b_2 = b_1 - (X_1'X_1)^{-1}X_2'(X_2'M_1X_2)^{-1}X_2 b_1 + (X_1'X_1 + X_2'X_2)^{-1} X_2'Y_2$$

VII.  A express√£o para $b_2$ pode ser reescrita como
$$b_2 = b_1 + (X_2'M_1X_2)^{-1}X_2'M_1(Y_2-X_2b_1)$$
    Portanto, o estimador $b_2$ √© a soma do estimador anterior $b_1$ mais um termo que corrige a estimativa inicial usando o erro entre o valor observado de $Y_2$ e o valor previsto com os par√¢metros do modelo da primeira amostra.
‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo de regress√£o linear em que queremos prever o pre√ßo de uma casa ($Y$) com base em sua √°rea em metros quadrados ($X$).
>
> **Dados Iniciais (Amostra 1):**
>
> | Casa | √Årea ($X_1$) | Pre√ßo ($Y_1$) |
> |------|---------------|-------------|
> | 1    | 50            | 200         |
> | 2    | 75            | 300         |
> | 3    | 100           | 380         |
>
> **C√°lculo de $b_1$ (Regress√£o OLS na Amostra 1):**
>
> Primeiro, montamos as matrizes $X_1$ (incluindo uma coluna de 1s para o intercepto) e $Y_1$:
>
>  $X_1 = \begin{bmatrix} 1 & 50 \\ 1 & 75 \\ 1 & 100 \end{bmatrix}$, $Y_1 = \begin{bmatrix} 200 \\ 300 \\ 380 \end{bmatrix}$
>
> Calculamos $b_1 = (X_1'X_1)^{-1}X_1'Y_1$:
>
> $\text{Step 1: } X_1'X_1 = \begin{bmatrix} 3 & 225 \\ 225 & 18125 \end{bmatrix}$
>
> $\text{Step 2: } (X_1'X_1)^{-1} \approx \begin{bmatrix} 2.6667 & -0.0333 \\ -0.0333 & 0.0004 \end{bmatrix}$
>
> $\text{Step 3: } X_1'Y_1 = \begin{bmatrix} 880 \\ 70250 \end{bmatrix}$
>
> $\text{Step 4: } b_1 = (X_1'X_1)^{-1}X_1'Y_1  \approx \begin{bmatrix} 40 \\ 3.4 \end{bmatrix}$
>
> Ent√£o, o modelo da primeira regress√£o √© $\hat{Y} = 40 + 3.4 * X$.
>
> **Novos Dados (Amostra 2):**
>
> | Casa | √Årea ($X_2$) | Pre√ßo ($Y_2$) |
> |------|---------------|-------------|
> | 4    | 120           | 450         |
> | 5    | 80            | 320         |
>
> $X_2 = \begin{bmatrix} 1 & 120 \\ 1 & 80 \end{bmatrix}$, $Y_2 = \begin{bmatrix} 450 \\ 320 \end{bmatrix}$
>
> **Atualiza√ß√£o de $b_1$ para $b_2$:**
>
> Precisamos calcular $M_1 = I - X_1(X_1'X_1)^{-1}X_1'$:
>
> $X_1(X_1'X_1)^{-1}X_1' \approx \begin{bmatrix} 0.2667 & 0.0667 & -0.1333 \\ 0.0667 & 0.3333 & 0.2000 \\ -0.1333 & 0.2000 & 0.7333 \end{bmatrix}$
>
> $M_1 \approx \begin{bmatrix} 0.7333 & -0.0667 & 0.1333 \\ -0.0667 & 0.6667 & -0.2000 \\ 0.1333 & -0.2000 & 0.2667 \end{bmatrix}$
>
>  Ent√£o, calculamos $b_2 = b_1 + (X_2'M_1X_2)^{-1}X_2'M_1(Y_2-X_2b_1)$
>
> $\text{Step 1: } M_1 X_2 \approx  \begin{bmatrix} -0.0134 & 0.0134 \\ 0.0266 & -0.0266 \\ 0.0400 & -0.0400 \end{bmatrix}$
>
>  $\text{Step 2: } X_2'M_1X_2 \approx \begin{bmatrix} 0.0001 & -0.0001 \\ -0.0001 & 0.0001  \end{bmatrix}$
>
> $\text{Step 3: } (X_2'M_1X_2)^{-1}$ does not exist. We must use the generalized inverse.
>
> $\text{Step 3: } (X_2'M_1X_2)^{\dagger} = \begin{bmatrix} 1.25 & -1.25 \\ -1.25 & 1.25 \end{bmatrix} * 1000$
>
> $\text{Step 4: } X_2b_1 =  \begin{bmatrix} 448 \\ 312 \end{bmatrix}$
>
> $\text{Step 5: } Y_2 - X_2b_1 = \begin{bmatrix} 450 - 448 \\ 320 - 312 \end{bmatrix} = \begin{bmatrix} 2 \\ 8 \end{bmatrix}$
>
> $\text{Step 6: } X_2'M_1(Y_2 - X_2b_1) =  \begin{bmatrix} 0.2266 \\ -0.2266 \end{bmatrix}$
>
> $\text{Step 7: } (X_2'M_1X_2)^{\dagger} X_2'M_1(Y_2 - X_2b_1) = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$
>
> $b_2 = b_1 + \begin{bmatrix} 0 \\ 0 \end{bmatrix} = \begin{bmatrix} 40 \\ 3.4 \end{bmatrix}$
>
> Neste exemplo, a atualiza√ß√£o n√£o altera os par√¢metros, o que ocorre devido a natureza das novas amostras, que n√£o trazem muita informa√ß√£o adicional.
>
> Este exemplo mostra como a atualiza√ß√£o de uma regress√£o OLS pode ser feita de forma incremental. Na pr√°tica, em um exemplo mais complexo, o termo de corre√ß√£o seria diferente de 0.

#### Lei das Proje√ß√µes Iteradas

A "lei das proje√ß√µes iteradas" estabelece que, se projetarmos a proje√ß√£o atualizada $P(Y_3|Y_2, Y_1)$ em $Y_1$, o resultado ser√° igual a proje√ß√£o de $Y_3$ diretamente em $Y_1$, ou seja $P[P(Y_3|Y_2, Y_1)|Y_1] = P(Y_3|Y_1)$. Para verificar essa afirma√ß√£o, demonstramos que a diferen√ßa entre as proje√ß√µes √© descorrelacionada com $Y_1$. [^4, ^4.5.32]

**Lema 1**
A lei das proje√ß√µes iteradas pode ser generalizada para mais de uma atualiza√ß√£o. Ou seja, se temos uma sequ√™ncia de informa√ß√µes $Y_1, Y_2, \ldots, Y_n$, ent√£o $P[P(Y_{n+1}|Y_n, Y_{n-1}, \ldots, Y_1)|Y_{n-1}, \ldots, Y_1] = P(Y_{n+1}|Y_{n-1}, \ldots, Y_1)$.

*Proof strategy:* Este lema generaliza a lei das proje√ß√µes iteradas por indu√ß√£o. Aplicamos o mesmo principio da lei original iterativamente para qualquer n√∫mero de atualiza√ß√µes, o que pode ser demonstrado utilizando as propriedades de proje√ß√£o linear. Este resultado garante a consist√™ncia das proje√ß√µes ao incorporar incrementalmente novas informa√ß√µes.
*Proof:*
Para provar o Lema 1, usaremos indu√ß√£o sobre o n√∫mero de atualiza√ß√µes.

I. **Caso Base:** Para $n=2$, o lema se reduz √† lei das proje√ß√µes iteradas original:
$P[P(Y_3|Y_2, Y_1)|Y_1] = P(Y_3|Y_1)$. J√° temos conhecimento que essa condi√ß√£o √© verdadeira pela lei original das proje√ß√µes iteradas.

II. **Hip√≥tese Indutiva:** Assumimos que o lema √© v√°lido para $n = k$, ou seja, $P[P(Y_{k+1}|Y_k, Y_{k-1}, \ldots, Y_1)|Y_{k-1}, \ldots, Y_1] = P(Y_{k+1}|Y_{k-1}, \ldots, Y_1)$.

III. **Passo Indutivo:** Devemos provar que o lema √© v√°lido para $n = k+1$. Queremos mostrar que:
$P[P(Y_{k+2}|Y_{k+1}, Y_k, \ldots, Y_1)|Y_k, \ldots, Y_1] = P(Y_{k+2}|Y_k, \ldots, Y_1)$.

IV.  Podemos escrever:
$P(Y_{k+2}|Y_{k+1}, Y_k, \ldots, Y_1) = P(Y_{k+2}|Y_k, \ldots, Y_1) + \alpha [Y_{k+1} - P(Y_{k+1}|Y_k, \ldots, Y_1)]$
    onde  $\alpha$  representa um vetor de coeficientes de proje√ß√£o correspondente a $Y_{k+1}$.

V. Ao projetarmos $P(Y_{k+2}|Y_{k+1}, Y_k, \ldots, Y_1)$ em $(Y_k, \ldots, Y_1)$ temos:
$P[P(Y_{k+2}|Y_{k+1}, Y_k, \ldots, Y_1)|Y_k, \ldots, Y_1] = P[P(Y_{k+2}|Y_k, \ldots, Y_1) + \alpha(Y_{k+1} - P(Y_{k+1}|Y_k, \ldots, Y_1))|Y_k, \ldots, Y_1]$

VI. Como a proje√ß√£o √© um operador linear, podemos separar os termos:
$P[P(Y_{k+2}|Y_{k+1}, Y_k, \ldots, Y_1)|Y_k, \ldots, Y_1] = P[P(Y_{k+2}|Y_k, \ldots, Y_1)|Y_k, \ldots, Y_1] + P[\alpha(Y_{k+1} - P(Y_{k+1}|Y_k, \ldots, Y_1))|Y_k, \ldots, Y_1]$

VII.  Pela hip√≥tese indutiva, o primeiro termo simplifica para $P(Y_{k+2}|Y_k, \ldots, Y_1)$.  O segundo termo,  $P[\alpha(Y_{k+1} - P(Y_{k+1}|Y_k, \ldots, Y_1))|Y_k, \ldots, Y_1]$, √© igual a zero, pois o res√≠duo $Y_{k+1} - P(Y_{k+1}|Y_k, \ldots, Y_1)$ √© ortogonal a $(Y_k, \ldots, Y_1)$.

VIII.  Portanto, $P[P(Y_{k+2}|Y_{k+1}, Y_k, \ldots, Y_1)|Y_k, \ldots, Y_1] = P(Y_{k+2}|Y_k, \ldots, Y_1)$, completando o passo indutivo e a prova.
‚ñ†

### Conclus√£o
A atualiza√ß√£o de uma proje√ß√£o linear √© uma ferramenta poderosa para incorporar novas informa√ß√µes em previs√µes e modelos de regress√£o, utilizando os res√≠duos para ajustar os coeficientes com dados incrementais, sendo fundamental tanto para a previs√£o de s√©ries temporais quanto para regress√µes OLS em cen√°rios din√¢micos. O processo de atualiza√ß√£o iterativa demonstra como a informa√ß√£o pode ser incorporada de maneira eficiente,  permitindo um refinamento cont√≠nuo dos resultados. A lei das proje√ß√µes iteradas fornece uma propriedade importante do m√©todo, garantindo que proje√ß√µes sobre diferentes conjuntos de informa√ß√µes sejam consistentes entre si.

### Refer√™ncias
[^4]:  Se√ß√µes do Cap√≠tulo 4 do livro texto fornecido.
<!-- END -->
