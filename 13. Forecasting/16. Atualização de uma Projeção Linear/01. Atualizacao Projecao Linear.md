## Atualização de uma Projeção Linear

### Introdução
Este capítulo tem como objetivo aprofundar o entendimento sobre a atualização de uma projeção linear. Expandindo os conceitos de previsão e projeção linear introduzidos anteriormente, esta seção se concentra em como incorporar novas informações para refinar as previsões existentes. Exploraremos como os resíduos de projeções lineares iniciais podem ser utilizados para ajustar os coeficientes quando novas variáveis se tornam disponíveis. O processo, que envolve o cálculo de projeções iniciais e sua atualização sequencial, é fundamental tanto para a previsão de séries temporais quanto para regressões de mínimos quadrados ordinários (OLS) em situações com informações incrementais.

### Conceitos Fundamentais
A atualização de uma projeção linear é um processo iterativo que utiliza informações adicionais para refinar uma projeção existente. Considere que desejamos prever o valor de $Y_3$, com base em uma informação inicial $Y_1$. Uma projeção de $Y_3$ com base apenas em $Y_1$ assume a forma $\hat{P}(Y_3|Y_1) = \Omega_{31} \Omega_{11}^{-1} Y_1$ [^4, ^4.5.12].

Agora, suponha que obtemos uma nova informação, representada por $Y_2$. Para atualizar a previsão original, precisamos considerar a projeção de $Y_2$ em $Y_1$: $\hat{P}(Y_2|Y_1) = \Omega_{21} \Omega_{11}^{-1} Y_1$. A atualização da projeção de $Y_3$ que leva em conta $Y_2$ é dada por [^4, ^4.5.14]:

$$ \hat{P}(Y_3|Y_2, Y_1) = \hat{P}(Y_3|Y_1) + h_{32} h_{22}^{-1} [Y_2 - \hat{P}(Y_2|Y_1)], $$

onde $h_{32}$ representa o produto esperado dos erros de previsão de $Y_3$ e $Y_2$ com base em $Y_1$, e $h_{22}$ é o erro quadrático médio (MSE) da projeção de $Y_2$ em $Y_1$ [^4, ^4.5.14]. Esta formulação expressa que a nova projeção é uma combinação da projeção inicial mais uma correção com base na nova informação $Y_2$. Essencialmente, o novo termo $Y_2 - \hat{P}(Y_2|Y_1)$ é o que $Y_2$ "revela" de novo que não foi levado em conta por $Y_1$.

> 💡 **Exemplo Numérico:**
>
> Vamos supor que temos as seguintes informações sobre ações:
> - $Y_1$: Preço da ação A no dia 1 = 100
> - $Y_2$: Preço da ação B no dia 1 = 150
> - $Y_3$: Preço da ação A no dia 2 (o que queremos prever)
>
> E que as projeções iniciais e os valores sejam:
>
> - $\hat{P}(Y_3|Y_1) = 1.1 * Y_1 = 110$ (Projeção de $Y_3$ baseada em $Y_1$)
> - $\hat{P}(Y_2|Y_1) = 1.5 * Y_1 = 150$ (Projeção de $Y_2$ baseada em $Y_1$)
> - $Y_2 = 160$ (Valor real de $Y_2$ no dia 1)
>
> Precisamos também de:
>
> - $h_{32} = E[(Y_3 - \hat{P}(Y_3|Y_1))(Y_2 - \hat{P}(Y_2|Y_1))] = 0.8$ (Covariância entre os erros de previsão de $Y_3$ e $Y_2$ baseados em $Y_1$).
> - $h_{22} = E[Y_2 - \hat{P}(Y_2|Y_1)]^2 = 10^2 = 100$ (MSE da projeção de $Y_2$ em $Y_1$).
>
> Agora, vamos calcular a projeção atualizada de $Y_3$ utilizando a fórmula:
>
>  $\hat{P}(Y_3|Y_2, Y_1) = \hat{P}(Y_3|Y_1) + h_{32} h_{22}^{-1} [Y_2 - \hat{P}(Y_2|Y_1)]$
>
> $\hat{P}(Y_3|Y_2, Y_1) = 110 + (0.8 / 100) * (160 - 150)$
> $\hat{P}(Y_3|Y_2, Y_1) = 110 + 0.008 * 10$
> $\hat{P}(Y_3|Y_2, Y_1) = 110 + 0.8 = 110.8$
>
> A nova projeção de $Y_3$, considerando $Y_2$, é 110.8.  Observamos que a projeção inicial (110) é corrigida para cima (110.8), levando em conta a diferença entre o valor observado de $Y_2$ (160) e a previsão feita com $Y_1$ (150).
>
> Este exemplo mostra como novas informações podem ser usadas para ajustar uma previsão inicial, tornando-a mais precisa.

Para entender melhor a natureza do multiplicador ($h_{32}h_{22}^{-1}$), definimos o vetor $\hat{Y}(1)$ da seguinte forma [^4, ^4.5.15]:
$$\hat{Y}(1) = E_1Y,$$
onde $E_1$ é uma matriz que transforma $Y$ em um vetor de resíduos, $Y_i - \hat{P}(Y_i | Y_1)$. É importante notar que a matriz de segundo momento de $\hat{Y}(1)$ é $E_1 \Omega E_1' = H$ [^4, ^4.5.15], onde $H$ é uma matriz que contém os MSEs das projeções de cada $Y_i$ em $Y_1$.  Especificamente,  $h_{22} = E[Y_2 - \hat{P}(Y_2|Y_1)]^2$, e $h_{32} = E[(Y_3 - \hat{P}(Y_3|Y_1))(Y_2 - \hat{P}(Y_2|Y_1))]$, com $h_{22}$ sendo o MSE da projeção de $Y_2$ em $Y_1$ [^4, ^4.5.15].

Com isso, a atualização da projeção linear é dada por [^4, ^4.5.16]:
$$ \hat{P}(Y_3|Y_2, Y_1) = \hat{P}(Y_3|Y_1) +  \{ E[Y_3 - \hat{P}(Y_3|Y_1)][Y_2 - \hat{P}(Y_2|Y_1)] \} \{ E[Y_2 - \hat{P}(Y_2|Y_1)]^2 \}^{-1} \{ Y_2 - \hat{P}(Y_2|Y_1) \}. $$
Esta expressão formaliza como a previsão inicial de $Y_3$ com base em $Y_1$ é ajustada por um termo que considera a nova informação de $Y_2$.

#### Relação com Regressão OLS
A atualização de uma projeção linear tem uma ligação direta com a regressão OLS em contextos onde os dados são incrementais. Considere o problema de regredir $Y$ em $X$. Uma vez que calculamos os coeficientes de regressão $b$ utilizando uma amostra, e posteriormente recebemos mais informações, não é necessário refazer o cálculo desde o começo. Em vez disso, é possível atualizar os coeficientes de regressão com base nos resíduos da regressão anterior e a nova informação.

Por exemplo, ao realizar uma regressão de mínimos quadrados ordinários (OLS),  o vetor de coeficientes $b$ é determinado de forma a minimizar a soma dos erros ao quadrado (SSE). O método de atualização  utiliza os resíduos da regressão anterior para ajustar os coeficientes quando são recebidas novas informações, de acordo com a lógica da projeção linear.

**Proposição 1**
A atualização dos coeficientes da regressão OLS pode ser expressa de forma análoga à atualização da projeção linear. Se $b_1$ são os coeficientes obtidos usando a primeira amostra de dados e $e_1$ são os resíduos, quando uma segunda amostra com os valores $X_2$ e $Y_2$ é obtida, os coeficientes atualizados $b_2$ podem ser expressos como:
$$b_2 = b_1 + (X_2'M_1X_2)^{-1}X_2'M_1(Y_2-X_2b_1)$$
onde $M_1=I-X_1(X_1'X_1)^{-1}X_1'$ é a matriz de projeção ortogonal ao espaço gerado por $X_1$.

*Proof strategy:* Esta proposição estende a lógica da atualização de projeções lineares para a regressão OLS. Ela demonstra como os coeficientes podem ser atualizados de forma incremental sem a necessidade de recalcular toda a regressão desde o início. Esta atualização é feita usando os resíduos da primeira amostra e as informações adicionais da segunda amostra. O termo $(X_2'M_1X_2)^{-1}X_2'M_1(Y_2-X_2b_1)$ representa a correção a ser adicionada aos coeficientes anteriores.

*Proof:*
Para provar a proposição 1, devemos mostrar que o estimador OLS atualizado, $b_2$, é obtido por meio da combinação dos coeficientes estimados usando a primeira amostra ($b_1$) e um termo que incorpora a nova informação, e este termo tem a forma que apresentamos.
I.  A regressão OLS com os dados combinados $(X_1, Y_1)$ e $(X_2, Y_2)$ pode ser escrita como:
    $$
    \begin{bmatrix}
    X_1 \\
    X_2
    \end{bmatrix} b_2 = 
    \begin{bmatrix}
    Y_1 \\
    Y_2
    \end{bmatrix}
    $$
    onde $b_2$ é o vetor de coeficientes atualizado.

II.  A solução OLS para $b_2$ é dada por:
$$
b_2 = \left( \begin{bmatrix} X_1' & X_2' \end{bmatrix} \begin{bmatrix} X_1 \\ X_2 \end{bmatrix} \right)^{-1} \begin{bmatrix} X_1' & X_2' \end{bmatrix}  \begin{bmatrix} Y_1 \\ Y_2 \end{bmatrix}
$$
$$
b_2 = (X_1'X_1 + X_2'X_2)^{-1}(X_1'Y_1 + X_2'Y_2)
$$

III.  Sabemos que $b_1 = (X_1'X_1)^{-1}X_1'Y_1$, então $X_1'Y_1 = X_1'X_1b_1$.  Substituindo isso na equação para $b_2$, temos:
$$b_2 = (X_1'X_1 + X_2'X_2)^{-1}(X_1'X_1b_1 + X_2'Y_2)$$

IV.  Usamos a identidade de Woodbury para a matriz inversa $(A + BC)^{-1} = A^{-1} - A^{-1}B(I + CA^{-1}B)^{-1}CA^{-1}$. Definindo $A = X_1'X_1$, $B = X_2'$, e $C= X_2$ , e usando a identidade:
$$(X_1'X_1 + X_2'X_2)^{-1} = (X_1'X_1)^{-1} - (X_1'X_1)^{-1}X_2'(I + X_2(X_1'X_1)^{-1}X_2')^{-1}X_2(X_1'X_1)^{-1}$$
    Assim,
    $$b_2 = \left[ (X_1'X_1)^{-1} - (X_1'X_1)^{-1}X_2'(I + X_2(X_1'X_1)^{-1}X_2')^{-1}X_2(X_1'X_1)^{-1} \right] (X_1'X_1b_1 + X_2'Y_2)$$

V. Definindo $M_1 = I - X_1(X_1'X_1)^{-1}X_1'$, notamos que $M_1 X_1 = 0$, e também $M_1 X_2 = X_2 - X_1(X_1'X_1)^{-1}X_1'X_2$
    Reorganizando os termos na identidade de Woodbury e usando o fato de que $X_1'(I - X_1(X_1'X_1)^{-1}X_1') = X_1'M_1 = 0$ , obtemos
    $$(X_1'X_1 + X_2'X_2)^{-1} = (X_1'X_1)^{-1} - (X_1'X_1)^{-1}X_2'(X_2'M_1X_2+X_2'X_1(X_1'X_1)^{-1}X_1'X_2)^{-1}X_2(X_1'X_1)^{-1}$$
    $$(X_1'X_1 + X_2'X_2)^{-1} = (X_1'X_1)^{-1} - (X_1'X_1)^{-1}X_2'(X_2'M_1X_2)^{-1}X_2(X_1'X_1)^{-1}$$
VI. Multiplicando $(X_1'X_1 + X_2'X_2)^{-1}$ por $X_1'X_1b_1 + X_2'Y_2$, obtemos
$$b_2 = b_1 - (X_1'X_1)^{-1}X_2'(X_2'M_1X_2)^{-1}X_2(X_1'X_1)^{-1} X_1'X_1b_1 + (X_1'X_1 + X_2'X_2)^{-1} X_2'Y_2$$
$$b_2 = b_1 - (X_1'X_1)^{-1}X_2'(X_2'M_1X_2)^{-1}X_2 b_1 + (X_1'X_1 + X_2'X_2)^{-1} X_2'Y_2$$

VII.  A expressão para $b_2$ pode ser reescrita como
$$b_2 = b_1 + (X_2'M_1X_2)^{-1}X_2'M_1(Y_2-X_2b_1)$$
    Portanto, o estimador $b_2$ é a soma do estimador anterior $b_1$ mais um termo que corrige a estimativa inicial usando o erro entre o valor observado de $Y_2$ e o valor previsto com os parâmetros do modelo da primeira amostra.
■

> 💡 **Exemplo Numérico:**
>
> Vamos considerar um exemplo de regressão linear em que queremos prever o preço de uma casa ($Y$) com base em sua área em metros quadrados ($X$).
>
> **Dados Iniciais (Amostra 1):**
>
> | Casa | Área ($X_1$) | Preço ($Y_1$) |
> |------|---------------|-------------|
> | 1    | 50            | 200         |
> | 2    | 75            | 300         |
> | 3    | 100           | 380         |
>
> **Cálculo de $b_1$ (Regressão OLS na Amostra 1):**
>
> Primeiro, montamos as matrizes $X_1$ (incluindo uma coluna de 1s para o intercepto) e $Y_1$:
>
>  $X_1 = \begin{bmatrix} 1 & 50 \\ 1 & 75 \\ 1 & 100 \end{bmatrix}$, $Y_1 = \begin{bmatrix} 200 \\ 300 \\ 380 \end{bmatrix}$
>
> Calculamos $b_1 = (X_1'X_1)^{-1}X_1'Y_1$:
>
> $\text{Step 1: } X_1'X_1 = \begin{bmatrix} 3 & 225 \\ 225 & 18125 \end{bmatrix}$
>
> $\text{Step 2: } (X_1'X_1)^{-1} \approx \begin{bmatrix} 2.6667 & -0.0333 \\ -0.0333 & 0.0004 \end{bmatrix}$
>
> $\text{Step 3: } X_1'Y_1 = \begin{bmatrix} 880 \\ 70250 \end{bmatrix}$
>
> $\text{Step 4: } b_1 = (X_1'X_1)^{-1}X_1'Y_1  \approx \begin{bmatrix} 40 \\ 3.4 \end{bmatrix}$
>
> Então, o modelo da primeira regressão é $\hat{Y} = 40 + 3.4 * X$.
>
> **Novos Dados (Amostra 2):**
>
> | Casa | Área ($X_2$) | Preço ($Y_2$) |
> |------|---------------|-------------|
> | 4    | 120           | 450         |
> | 5    | 80            | 320         |
>
> $X_2 = \begin{bmatrix} 1 & 120 \\ 1 & 80 \end{bmatrix}$, $Y_2 = \begin{bmatrix} 450 \\ 320 \end{bmatrix}$
>
> **Atualização de $b_1$ para $b_2$:**
>
> Precisamos calcular $M_1 = I - X_1(X_1'X_1)^{-1}X_1'$:
>
> $X_1(X_1'X_1)^{-1}X_1' \approx \begin{bmatrix} 0.2667 & 0.0667 & -0.1333 \\ 0.0667 & 0.3333 & 0.2000 \\ -0.1333 & 0.2000 & 0.7333 \end{bmatrix}$
>
> $M_1 \approx \begin{bmatrix} 0.7333 & -0.0667 & 0.1333 \\ -0.0667 & 0.6667 & -0.2000 \\ 0.1333 & -0.2000 & 0.2667 \end{bmatrix}$
>
>  Então, calculamos $b_2 = b_1 + (X_2'M_1X_2)^{-1}X_2'M_1(Y_2-X_2b_1)$
>
> $\text{Step 1: } M_1 X_2 \approx  \begin{bmatrix} -0.0134 & 0.0134 \\ 0.0266 & -0.0266 \\ 0.0400 & -0.0400 \end{bmatrix}$
>
>  $\text{Step 2: } X_2'M_1X_2 \approx \begin{bmatrix} 0.0001 & -0.0001 \\ -0.0001 & 0.0001  \end{bmatrix}$
>
> $\text{Step 3: } (X_2'M_1X_2)^{-1}$ does not exist. We must use the generalized inverse.
>
> $\text{Step 3: } (X_2'M_1X_2)^{\dagger} = \begin{bmatrix} 1.25 & -1.25 \\ -1.25 & 1.25 \end{bmatrix} * 1000$
>
> $\text{Step 4: } X_2b_1 =  \begin{bmatrix} 448 \\ 312 \end{bmatrix}$
>
> $\text{Step 5: } Y_2 - X_2b_1 = \begin{bmatrix} 450 - 448 \\ 320 - 312 \end{bmatrix} = \begin{bmatrix} 2 \\ 8 \end{bmatrix}$
>
> $\text{Step 6: } X_2'M_1(Y_2 - X_2b_1) =  \begin{bmatrix} 0.2266 \\ -0.2266 \end{bmatrix}$
>
> $\text{Step 7: } (X_2'M_1X_2)^{\dagger} X_2'M_1(Y_2 - X_2b_1) = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$
>
> $b_2 = b_1 + \begin{bmatrix} 0 \\ 0 \end{bmatrix} = \begin{bmatrix} 40 \\ 3.4 \end{bmatrix}$
>
> Neste exemplo, a atualização não altera os parâmetros, o que ocorre devido a natureza das novas amostras, que não trazem muita informação adicional.
>
> Este exemplo mostra como a atualização de uma regressão OLS pode ser feita de forma incremental. Na prática, em um exemplo mais complexo, o termo de correção seria diferente de 0.

#### Lei das Projeções Iteradas

A "lei das projeções iteradas" estabelece que, se projetarmos a projeção atualizada $P(Y_3|Y_2, Y_1)$ em $Y_1$, o resultado será igual a projeção de $Y_3$ diretamente em $Y_1$, ou seja $P[P(Y_3|Y_2, Y_1)|Y_1] = P(Y_3|Y_1)$. Para verificar essa afirmação, demonstramos que a diferença entre as projeções é descorrelacionada com $Y_1$. [^4, ^4.5.32]

**Lema 1**
A lei das projeções iteradas pode ser generalizada para mais de uma atualização. Ou seja, se temos uma sequência de informações $Y_1, Y_2, \ldots, Y_n$, então $P[P(Y_{n+1}|Y_n, Y_{n-1}, \ldots, Y_1)|Y_{n-1}, \ldots, Y_1] = P(Y_{n+1}|Y_{n-1}, \ldots, Y_1)$.

*Proof strategy:* Este lema generaliza a lei das projeções iteradas por indução. Aplicamos o mesmo principio da lei original iterativamente para qualquer número de atualizações, o que pode ser demonstrado utilizando as propriedades de projeção linear. Este resultado garante a consistência das projeções ao incorporar incrementalmente novas informações.
*Proof:*
Para provar o Lema 1, usaremos indução sobre o número de atualizações.

I. **Caso Base:** Para $n=2$, o lema se reduz à lei das projeções iteradas original:
$P[P(Y_3|Y_2, Y_1)|Y_1] = P(Y_3|Y_1)$. Já temos conhecimento que essa condição é verdadeira pela lei original das projeções iteradas.

II. **Hipótese Indutiva:** Assumimos que o lema é válido para $n = k$, ou seja, $P[P(Y_{k+1}|Y_k, Y_{k-1}, \ldots, Y_1)|Y_{k-1}, \ldots, Y_1] = P(Y_{k+1}|Y_{k-1}, \ldots, Y_1)$.

III. **Passo Indutivo:** Devemos provar que o lema é válido para $n = k+1$. Queremos mostrar que:
$P[P(Y_{k+2}|Y_{k+1}, Y_k, \ldots, Y_1)|Y_k, \ldots, Y_1] = P(Y_{k+2}|Y_k, \ldots, Y_1)$.

IV.  Podemos escrever:
$P(Y_{k+2}|Y_{k+1}, Y_k, \ldots, Y_1) = P(Y_{k+2}|Y_k, \ldots, Y_1) + \alpha [Y_{k+1} - P(Y_{k+1}|Y_k, \ldots, Y_1)]$
    onde  $\alpha$  representa um vetor de coeficientes de projeção correspondente a $Y_{k+1}$.

V. Ao projetarmos $P(Y_{k+2}|Y_{k+1}, Y_k, \ldots, Y_1)$ em $(Y_k, \ldots, Y_1)$ temos:
$P[P(Y_{k+2}|Y_{k+1}, Y_k, \ldots, Y_1)|Y_k, \ldots, Y_1] = P[P(Y_{k+2}|Y_k, \ldots, Y_1) + \alpha(Y_{k+1} - P(Y_{k+1}|Y_k, \ldots, Y_1))|Y_k, \ldots, Y_1]$

VI. Como a projeção é um operador linear, podemos separar os termos:
$P[P(Y_{k+2}|Y_{k+1}, Y_k, \ldots, Y_1)|Y_k, \ldots, Y_1] = P[P(Y_{k+2}|Y_k, \ldots, Y_1)|Y_k, \ldots, Y_1] + P[\alpha(Y_{k+1} - P(Y_{k+1}|Y_k, \ldots, Y_1))|Y_k, \ldots, Y_1]$

VII.  Pela hipótese indutiva, o primeiro termo simplifica para $P(Y_{k+2}|Y_k, \ldots, Y_1)$.  O segundo termo,  $P[\alpha(Y_{k+1} - P(Y_{k+1}|Y_k, \ldots, Y_1))|Y_k, \ldots, Y_1]$, é igual a zero, pois o resíduo $Y_{k+1} - P(Y_{k+1}|Y_k, \ldots, Y_1)$ é ortogonal a $(Y_k, \ldots, Y_1)$.

VIII.  Portanto, $P[P(Y_{k+2}|Y_{k+1}, Y_k, \ldots, Y_1)|Y_k, \ldots, Y_1] = P(Y_{k+2}|Y_k, \ldots, Y_1)$, completando o passo indutivo e a prova.
■

### Conclusão
A atualização de uma projeção linear é uma ferramenta poderosa para incorporar novas informações em previsões e modelos de regressão, utilizando os resíduos para ajustar os coeficientes com dados incrementais, sendo fundamental tanto para a previsão de séries temporais quanto para regressões OLS em cenários dinâmicos. O processo de atualização iterativa demonstra como a informação pode ser incorporada de maneira eficiente,  permitindo um refinamento contínuo dos resultados. A lei das projeções iteradas fornece uma propriedade importante do método, garantindo que projeções sobre diferentes conjuntos de informações sejam consistentes entre si.

### Referências
[^4]:  Seções do Capítulo 4 do livro texto fornecido.
<!-- END -->
