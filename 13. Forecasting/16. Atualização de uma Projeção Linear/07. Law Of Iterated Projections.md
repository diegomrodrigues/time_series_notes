## A Lei das Proje√ß√µes Iteradas e a Consist√™ncia de Proje√ß√µes Lineares

### Introdu√ß√£o
Este cap√≠tulo explora a lei das proje√ß√µes iteradas e sua import√¢ncia para a consist√™ncia das proje√ß√µes lineares, complementando os temas abordados nos cap√≠tulos anteriores [^4]. A lei das proje√ß√µes iteradas afirma que projetar uma proje√ß√£o sobre o mesmo espa√ßo de informa√ß√£o n√£o altera a proje√ß√£o original, ou seja, n√£o adiciona informa√ß√£o nova. Essa propriedade √© fundamental para garantir a consist√™ncia das proje√ß√µes em processos iterativos onde novas informa√ß√µes s√£o incorporadas sequencialmente. O objetivo deste cap√≠tulo √© formalizar essa lei e demonstrar suas implica√ß√µes para a atualiza√ß√£o e a interpreta√ß√£o de proje√ß√µes lineares em contextos de an√°lise de s√©ries temporais e regress√£o.

### A Lei das Proje√ß√µes Iteradas
Como visto em cap√≠tulos anteriores [^4], a proje√ß√£o linear de uma vari√°vel $Y$ em um conjunto de vari√°veis $X$ √© a melhor aproxima√ß√£o linear de $Y$ no espa√ßo gerado por $X$. A lei das proje√ß√µes iteradas, um conceito chave na an√°lise de proje√ß√µes lineares, estabelece uma rela√ß√£o fundamental sobre a consist√™ncia de m√∫ltiplas proje√ß√µes. Formalmente, ela afirma que, se projetarmos uma proje√ß√£o linear j√° existente sobre o mesmo conjunto de vari√°veis, o resultado √© igual √† proje√ß√£o original [^4, ^4.5.32]. Ou seja, dada uma vari√°vel $Y_3$ e um conjunto de vari√°veis $Y_1$ e $Y_2$, temos:
$$P[P(Y_3|Y_2, Y_1)|Y_1] = P(Y_3|Y_1),$$
onde $P(Y_3|Y_2, Y_1)$ representa a proje√ß√£o de $Y_3$ sobre o espa√ßo gerado por $Y_1$ e $Y_2$, e $P(Y_3|Y_1)$ representa a proje√ß√£o de $Y_3$ sobre o espa√ßo gerado apenas por $Y_1$. Essa lei estabelece que, se projetarmos a proje√ß√£o atualizada $P(Y_3|Y_2, Y_1)$ no espa√ßo gerado por $Y_1$, o resultado ser√° igual √† proje√ß√£o original $P(Y_3|Y_1)$. O que significa que, ap√≥s projetar em $Y_1$ e $Y_2$, e ao projetar esse resultado em $Y_1$ novamente, nenhuma informa√ß√£o adicional √© obtida.

> üí° **Exemplo Num√©rico:**
>
> Suponha que tenhamos as seguintes vari√°veis aleat√≥rias e suas rela√ß√µes:
>
>   $Y_1 = \epsilon_1$
>   $Y_2 = 0.5Y_1 + \epsilon_2$
>   $Y_3 = 0.25Y_1 + 0.375Y_2 + \epsilon_3$
>
> onde $\epsilon_1, \epsilon_2,$ e $\epsilon_3$ s√£o ru√≠dos brancos independentes com m√©dia zero e vari√¢ncia 1.
>
> **Passo 1: Proje√ß√£o de $Y_3$ em $Y_1$**
>
> A proje√ß√£o de $Y_3$ em $Y_1$ √© dada por $P(Y_3|Y_1) = \frac{Cov(Y_3, Y_1)}{Var(Y_1)}Y_1$.
>
>  Calculando a covari√¢ncia:
> $Cov(Y_3, Y_1) = Cov(0.25Y_1 + 0.375(0.5Y_1 + \epsilon_2) + \epsilon_3, Y_1) = Cov(0.25Y_1 + 0.1875Y_1 + 0.375\epsilon_2 + \epsilon_3, Y_1) = Cov(0.4375Y_1 + 0.375\epsilon_2 + \epsilon_3, Y_1) = 0.4375Cov(Y_1, Y_1) = 0.4375$.
>
> A vari√¢ncia de $Y_1$ √© $Var(Y_1) = Var(\epsilon_1) = 1$.
> Portanto, $P(Y_3|Y_1) = \frac{0.4375}{1}Y_1 = 0.4375Y_1$.
>
> **Passo 2: Proje√ß√£o de $Y_3$ em $Y_1$ e $Y_2$**
>
> A proje√ß√£o de $Y_3$ em $Y_1$ e $Y_2$ √© dada por $P(Y_3|Y_1, Y_2) = \beta_1Y_1 + \beta_2Y_2$.
> Substituindo o valor de $Y_2$ temos $P(Y_3|Y_1, Y_2) = 0.25Y_1 + 0.375Y_2$.
>
> **Passo 3: Proje√ß√£o de $P(Y_3|Y_1, Y_2)$ em $Y_1$**
>
> Agora, vamos projetar $P(Y_3|Y_1, Y_2)$ em $Y_1$. Substituindo $Y_2$ pela sua proje√ß√£o em $Y_1$, $P(Y_2|Y_1) = \frac{Cov(Y_2,Y_1)}{Var(Y_1)}Y_1= \frac{Cov(0.5Y_1+\epsilon_2, Y_1)}{Var(Y_1)}Y_1 = \frac{0.5}{1}Y_1 = 0.5Y_1$:
> $P(P(Y_3|Y_1, Y_2)|Y_1) = P(0.25Y_1 + 0.375Y_2 | Y_1) = 0.25Y_1 + 0.375P(Y_2|Y_1) = 0.25Y_1 + 0.375(0.5Y_1) = 0.25Y_1 + 0.1875Y_1 = 0.4375Y_1$
>
>
> **Resultado:**
>
> Vemos que $P(P(Y_3|Y_1, Y_2)|Y_1) = 0.4375Y_1 = P(Y_3|Y_1)$, o que confirma a lei das proje√ß√µes iteradas.
>
> Isso significa que projetar $Y_3$ em $Y_1$ e $Y_2$ e, em seguida, projetar o resultado em $Y_1$ nos d√° o mesmo resultado de projetar $Y_3$ diretamente em $Y_1$. A informa√ß√£o adicional de $Y_2$ j√° estava contida na proje√ß√£o quando considerada em conjunto com $Y_1$.

A lei das proje√ß√µes iteradas pode ser generalizada para m√∫ltiplos passos, como visto no Lema 1 do cap√≠tulo anterior. Ou seja, dada uma sequ√™ncia de informa√ß√µes $Y_1, Y_2, \ldots, Y_n$, a proje√ß√£o de $Y_{n+1}$ sobre $Y_1, \ldots, Y_n$ projetada sobre $Y_1, \ldots, Y_{n-1}$ resulta na proje√ß√£o original de $Y_{n+1}$ sobre $Y_1, \ldots, Y_{n-1}$:
$$P[P(Y_{n+1}|Y_n, Y_{n-1},\ldots, Y_1)|Y_{n-1},\ldots, Y_1] = P(Y_{n+1}|Y_{n-1},\ldots, Y_1)$$
Essa propriedade √© fundamental para garantir a consist√™ncia das proje√ß√µes quando novas informa√ß√µes s√£o incorporadas sequencialmente [^4, Lema 1].

**Lema 1**
A lei das proje√ß√µes iteradas tamb√©m pode ser expressa em termos de res√≠duos. Especificamente, o res√≠duo da proje√ß√£o de $Y_{n+1}$ sobre $Y_1, \ldots, Y_n$, projetado sobre o espa√ßo gerado por $Y_1, \ldots, Y_{n-1}$, resulta em zero. Ou seja,
$P(Y_{n+1} - P(Y_{n+1}|Y_1, \ldots, Y_n)|Y_1, \ldots, Y_{n-1}) = 0$

*Prova:*
I. Definimos o res√≠duo da proje√ß√£o de $Y_{n+1}$ sobre o espa√ßo gerado por $Y_1, \ldots, Y_n$ como $e_{n+1} = Y_{n+1} - P(Y_{n+1}|Y_1, \ldots, Y_n)$.
II. Pela defini√ß√£o de proje√ß√£o, $e_{n+1}$ √© ortogonal a todos os elementos do subespa√ßo gerado por $Y_1, \ldots, Y_n$, e portanto tamb√©m ortogonal ao subespa√ßo gerado por $Y_1, \ldots, Y_{n-1}$.
III. Como o res√≠duo $e_{n+1}$ √© ortogonal ao espa√ßo gerado por $Y_1, \ldots, Y_{n-1}$, a proje√ß√£o de $e_{n+1}$ nesse espa√ßo √© nula: $P(e_{n+1}|Y_1, \ldots, Y_{n-1}) = 0$.
IV. Substituindo $e_{n+1}$ por sua defini√ß√£o, obtemos $P(Y_{n+1} - P(Y_{n+1}|Y_1, \ldots, Y_n)|Y_1, \ldots, Y_{n-1}) = 0$.
‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Retomando o exemplo anterior, temos:
>
>  $Y_1 = \epsilon_1$
>  $Y_2 = 0.5Y_1 + \epsilon_2$
>  $Y_3 = 0.25Y_1 + 0.375Y_2 + \epsilon_3$
>
>  E as proje√ß√µes:
>  $P(Y_3|Y_1) = 0.4375Y_1$
>  $P(Y_3|Y_1, Y_2) = 0.25Y_1 + 0.375Y_2$.
>
> **Passo 1: C√°lculo do Res√≠duo**
>
> O res√≠duo da proje√ß√£o de $Y_3$ sobre $Y_1$ e $Y_2$ √©:
>
> $\hat{e}_3 = Y_3 - P(Y_3|Y_1, Y_2) = (0.25Y_1 + 0.375Y_2 + \epsilon_3) - (0.25Y_1 + 0.375Y_2) = \epsilon_3$.
>
> **Passo 2: Proje√ß√£o do Res√≠duo em $Y_1$**
>
> Agora, projetamos o res√≠duo $\hat{e}_3$ sobre $Y_1$:
>
> $P(\hat{e}_3|Y_1) = \frac{Cov(\hat{e}_3, Y_1)}{Var(Y_1)}Y_1 = \frac{Cov(\epsilon_3, \epsilon_1)}{Var(\epsilon_1)}\epsilon_1$.
>
> Como $\epsilon_3$ e $\epsilon_1$ s√£o independentes, sua covari√¢ncia √© zero. Portanto, $P(\hat{e}_3|Y_1) = 0$.
>
> **Resultado:**
>
> Este resultado confirma o Lema 1, pois a proje√ß√£o do res√≠duo da proje√ß√£o de $Y_3$ sobre $Y_1$ e $Y_2$, quando projetado em $Y_1$, resulta em zero. Isso significa que o res√≠duo $\epsilon_3$ n√£o cont√©m informa√ß√£o linear relevante para $Y_1$.

### Consist√™ncia e a Ortogonalidade dos Res√≠duos
A lei das proje√ß√µes iteradas garante a consist√™ncia das proje√ß√µes em processos iterativos, pois, uma vez que projetamos uma vari√°vel em um conjunto de informa√ß√µes, nenhuma informa√ß√£o adicional sobre a vari√°vel √© obtida ao projetar novamente sobre um subconjunto da mesma informa√ß√£o. Essa propriedade est√° diretamente ligada √† ortogonalidade dos res√≠duos da proje√ß√£o.  Quando projetamos $Y_3$ sobre $Y_1$, o res√≠duo $Y_3 - P(Y_3|Y_1)$ √© ortogonal a $Y_1$, e projetar esse res√≠duo novamente em $Y_1$ resulta em zero. A lei das proje√ß√µes iteradas implica que a informa√ß√£o sobre a vari√°vel que est√° contida em suas proje√ß√µes no espa√ßo gerado por outras vari√°veis j√° est√° totalmente incorporada.

A prova formal dessa lei se baseia em um argumento de ortogonalidade dos res√≠duos da proje√ß√£o:
Como a proje√ß√£o $P(Y_3|Y_1, Y_2)$ √© a melhor aproxima√ß√£o linear de $Y_3$ no subespa√ßo gerado por $Y_1$ e $Y_2$, o res√≠duo $Y_3 - P(Y_3|Y_1, Y_2)$ √© ortogonal a todas as vari√°veis do subespa√ßo, ou seja, √© ortogonal a $Y_1$ e $Y_2$ [^4, ^4.5.10].  Quando projetamos esse res√≠duo sobre um subespa√ßo menor que √© gerado por $Y_1$, n√£o temos uma nova informa√ß√£o sobre $Y_3$. De forma equivalente, a diferen√ßa entre as duas proje√ß√µes √© sempre ortogonal √† proje√ß√£o original: $P(Y_3|Y_2, Y_1) - P(Y_3|Y_1)$ √© ortogonal a $Y_1$.

**Proposi√ß√£o 5**
A lei das proje√ß√µes iteradas implica que o res√≠duo de uma proje√ß√£o √© sempre ortogonal ao subespa√ßo no qual foi projetado. Ou seja, se $P(Y_{n+1}|Y_1,\ldots,Y_n)$ √© a proje√ß√£o de $Y_{n+1}$ sobre o espa√ßo gerado por $Y_1,\ldots,Y_n$, e $P(Y_{n+1}|Y_1,\ldots,Y_{n-1})$ √© a proje√ß√£o de $Y_{n+1}$ sobre o espa√ßo gerado por $Y_1,\ldots,Y_{n-1}$, ent√£o, a diferen√ßa $P(Y_{n+1}|Y_1,\ldots,Y_n) - P(Y_{n+1}|Y_1,\ldots,Y_{n-1})$ √© ortogonal a $Y_1,\ldots,Y_{n-1}$.

*Prova:*
I. Definimos o res√≠duo da proje√ß√£o $Y_{n+1}$ sobre o espa√ßo gerado por $Y_1, \ldots, Y_n$ como $\hat{Y}_{n+1} = Y_{n+1} - P(Y_{n+1}|Y_1, \ldots, Y_n)$. Por defini√ß√£o de proje√ß√£o, o res√≠duo √© ortogonal a todos os elementos do subespa√ßo gerado por $Y_1, \ldots, Y_n$.
II. A proje√ß√£o $P(Y_{n+1}|Y_1, \ldots, Y_n)$ pode ser expressa como a soma da proje√ß√£o de $Y_{n+1}$ no espa√ßo gerado por $Y_1, \ldots, Y_{n-1}$, mais a proje√ß√£o do res√≠duo da vari√°vel $Y_n$ sobre o espa√ßo gerado por $Y_1, \ldots, Y_{n-1}$:
$P(Y_{n+1}|Y_1, \ldots, Y_n) = P(Y_{n+1}|Y_1, \ldots, Y_{n-1}) + \frac{Cov(Y_{n+1}, \hat{Y_n})}{Var(\hat{Y_n})}\hat{Y_n}$ onde $\hat{Y_n}$ √© o res√≠duo da proje√ß√£o de $Y_n$ em $Y_1,\ldots, Y_{n-1}$ [^4, Teorema 2.1].
III. Substituindo, a diferen√ßa entre as duas proje√ß√µes √© dada por:
    $P(Y_{n+1}|Y_1,\ldots,Y_n) - P(Y_{n+1}|Y_1,\ldots,Y_{n-1}) = \frac{Cov(Y_{n+1}, \hat{Y_n})}{Var(\hat{Y_n})}\hat{Y_n}$
IV. Pela defini√ß√£o de proje√ß√£o linear, o res√≠duo $\hat{Y_n}$ √© ortogonal ao espa√ßo gerado por $Y_1, \ldots, Y_{n-1}$, o que implica que $Cov(\hat{Y_n}, Y_j) = 0$ para $j = 1, \ldots, n-1$.
V. Como a diferen√ßa das proje√ß√µes √© proporcional ao res√≠duo $\hat{Y_n}$, ent√£o, essa diferen√ßa tamb√©m √© ortogonal ao espa√ßo gerado por $Y_1, \ldots, Y_{n-1}$.
VI. Conclu√≠mos que a diferen√ßa entre as proje√ß√µes $P(Y_{n+1}|Y_1, \ldots, Y_n) - P(Y_{n+1}|Y_1, \ldots, Y_{n-1})$ √© ortogonal a $Y_1, \ldots, Y_{n-1}$, demonstrando a propriedade de ortogonalidade dos res√≠duos e a lei das proje√ß√µes iteradas.
‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Usando o exemplo anterior:
>
>   $Y_1 = \epsilon_1$
>   $Y_2 = 0.5Y_1 + \epsilon_2$
>   $Y_3 = 0.25Y_1 + 0.375Y_2 + \epsilon_3$
>
> **Passo 1: C√°lculo das Proje√ß√µes**
>
> J√° calculamos:
>
>   $P(Y_3|Y_1) = 0.4375Y_1$
>   $P(Y_3|Y_1, Y_2) = 0.25Y_1 + 0.375Y_2$
>
> **Passo 2: Diferen√ßa entre as Proje√ß√µes**
>
>  A diferen√ßa entre as proje√ß√µes √©:
>
>  $P(Y_3|Y_1, Y_2) - P(Y_3|Y_1) = (0.25Y_1 + 0.375Y_2) - 0.4375Y_1 = 0.375Y_2 - 0.1875Y_1 = 0.375(0.5Y_1+\epsilon_2) - 0.1875Y_1 = 0.1875Y_1+0.375\epsilon_2 - 0.1875Y_1= 0.375\epsilon_2$
>
> **Passo 3: Verifica√ß√£o da Ortogonalidade**
>
>  Para verificar se a diferen√ßa √© ortogonal a $Y_1$, calculamos a covari√¢ncia:
>
>  $Cov(P(Y_3|Y_1, Y_2) - P(Y_3|Y_1), Y_1) = Cov(0.375\epsilon_2, \epsilon_1) = 0$, pois $\epsilon_1$ e $\epsilon_2$ s√£o independentes.
>
> **Resultado:**
>
> A covari√¢ncia √© zero, o que demonstra que a diferen√ßa entre as proje√ß√µes √© ortogonal a $Y_1$, conforme estabelecido na Proposi√ß√£o 5. Isso implica que a informa√ß√£o adicional de $Y_2$ n√£o est√° correlacionada linearmente com $Y_1$.

**Corol√°rio 5.1**
Um resultado direto da Proposi√ß√£o 5 √© que a proje√ß√£o de um res√≠duo sobre o mesmo espa√ßo de informa√ß√£o √© sempre zero. Isto √©, para qualquer conjunto de vari√°veis $Y_1, \ldots, Y_n$, se definirmos o res√≠duo $\hat{Y}_{n+1} = Y_{n+1} - P(Y_{n+1}|Y_1, \ldots, Y_n)$, ent√£o $P(\hat{Y}_{n+1}|Y_1, \ldots, Y_n) = 0$.

*Prova:*
I. Pela Proposi√ß√£o 5, sabemos que a diferen√ßa entre proje√ß√µes, e em particular o res√≠duo $\hat{Y}_{n+1}$, √© ortogonal ao espa√ßo gerado pelas vari√°veis sobre as quais a proje√ß√£o √© realizada.
II. A proje√ß√£o de um vetor ortogonal a um subespa√ßo sobre o mesmo subespa√ßo √© sempre o vetor nulo.
III. Portanto, $P(\hat{Y}_{n+1}|Y_1, \ldots, Y_n) = 0$.
‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Retomando nosso exemplo:
>
>   $Y_1 = \epsilon_1$
>   $Y_2 = 0.5Y_1 + \epsilon_2$
>   $Y_3 = 0.25Y_1 + 0.375Y_2 + \epsilon_3$
>   $P(Y_3|Y_1, Y_2) = 0.25Y_1 + 0.375Y_2$
>
> **Passo 1: C√°lculo do Res√≠duo**
>
> O res√≠duo da proje√ß√£o de $Y_3$ sobre $Y_1$ e $Y_2$ √©:
>
> $\hat{Y}_3 = Y_3 - P(Y_3|Y_1, Y_2) = (0.25Y_1 + 0.375Y_2 + \epsilon_3) - (0.25Y_1 + 0.375Y_2) = \epsilon_3$
>
> **Passo 2: Proje√ß√£o do Res√≠duo em $Y_1$ e $Y_2$**
>
> Projetamos $\hat{Y}_3$ em $Y_1$ e $Y_2$:
>
>  $P(\hat{Y}_3|Y_1, Y_2) = P(\epsilon_3|Y_1, Y_2)$.
>
>  Como $\epsilon_3$ √© independente de $Y_1$ e $Y_2$, a melhor proje√ß√£o linear de $\epsilon_3$ no espa√ßo gerado por $Y_1$ e $Y_2$ √© zero. Portanto:
>
>  $P(\hat{Y}_3|Y_1, Y_2) = 0$.
>
> **Resultado:**
>
> Este resultado demonstra o Corol√°rio 5.1. A proje√ß√£o do res√≠duo no mesmo espa√ßo de informa√ß√£o √© zero. Isso significa que o res√≠duo n√£o tem componente linear projet√°vel em $Y_1$ e $Y_2$.

### Implica√ß√µes para a Atualiza√ß√£o de Proje√ß√µes
A lei das proje√ß√µes iteradas tem implica√ß√µes diretas para a atualiza√ß√£o de proje√ß√µes lineares. Em um processo iterativo de atualiza√ß√£o, cada nova vari√°vel fornece uma contribui√ß√£o para a proje√ß√£o que √© ortogonal √†s proje√ß√µes j√° realizadas. Isso significa que, se projetarmos a proje√ß√£o atualizada em um conjunto de vari√°veis anterior, n√£o adicionamos ou retiramos informa√ß√£o sobre o valor a ser previsto.
Essa propriedade √© importante porque garante que as proje√ß√µes est√£o incorporando toda a informa√ß√£o relevante em cada etapa. Al√©m disso, a ortogonalidade dos res√≠duos permite decompor o processo de proje√ß√£o linear em partes independentes, simplificando os c√°lculos e tornando o processo computacionalmente eficiente [^4, Lema 4.1]. O entendimento da lei das proje√ß√µes iteradas √©, portanto, crucial para interpretar as rela√ß√µes entre as vari√°veis e garantir a consist√™ncia de proje√ß√µes lineares.

**Teorema 1**
A lei das proje√ß√µes iteradas permite decompor a proje√ß√£o de uma vari√°vel em um conjunto de outras vari√°veis em uma sequ√™ncia de proje√ß√µes, cada uma delas adicionando informa√ß√£o ortogonal √†s anteriores. Ou seja, dado um conjunto de vari√°veis $Y_1, \ldots, Y_n$, a proje√ß√£o de $Y_{n+1}$ pode ser expressa como:
$P(Y_{n+1}|Y_1, \ldots, Y_n) = P(Y_{n+1}|Y_1) + P(Y_{n+1} - P(Y_{n+1}|Y_1)|Y_2) + \ldots + P(Y_{n+1} - P(Y_{n+1}|Y_1, \ldots, Y_{n-1})|Y_n)$

*Prova:*
I. Pela defini√ß√£o da proje√ß√£o iterada e da Proposi√ß√£o 5, sabemos que a diferen√ßa entre a proje√ß√£o em um conjunto de vari√°veis e a proje√ß√£o em um subconjunto dessas vari√°veis √© ortogonal ao subespa√ßo do subconjunto.
II. Come√ßamos expandindo a proje√ß√£o de $Y_{n+1}$ sobre $Y_1$ e $Y_2$:
    $P(Y_{n+1}|Y_1, Y_2) = P(Y_{n+1}|Y_1) + P(Y_{n+1} - P(Y_{n+1}|Y_1)|Y_2)$
III. Aplicando recursivamente esse processo, chegamos √† decomposi√ß√£o geral:
  $P(Y_{n+1}|Y_1, \ldots, Y_n) = P(Y_{n+1}|Y_1) + P(Y_{n+1} - P(Y_{n+1}|Y_1)|Y_2) + \ldots + P(Y_{n+1} - P(Y_{n+1}|Y_1, \ldots, Y_{n-1})|Y_n)$.
IV. Cada termo nessa soma representa a contribui√ß√£o marginal de cada vari√°vel para a proje√ß√£o, e cada contribui√ß√£o √© ortogonal √†s anteriores.
‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Utilizando novamente nosso conjunto de vari√°veis:
>
>  $Y_1 = \epsilon_1$
>  $Y_2 = 0.5Y_1 + \epsilon_2$
>  $Y_3 = 0.25Y_1 + 0.375Y_2 + \epsilon_3$
>
> J√° calculamos:
>
>  $P(Y_3|Y_1) = 0.4375Y_1$
>  $P(Y_3|Y_1,Y_2) = 0.25Y_1 + 0.375Y_2$
>
>  **Passo 1: C√°lculo de $P(Y_3 - P(Y_3|Y_1)|Y_2)$**
>
>  Primeiro, calculamos o res√≠duo de $Y_3$ em rela√ß√£o a $Y_1$:
>  $Y_3 - P(Y_3|Y_1) = (0.25Y_1 + 0.375Y_2 + \epsilon_3) - 0.4375Y_1 = 0.375Y_2 - 0.1875Y_1 + \epsilon_3$
>
>  Agora, projetamos esse res√≠duo em $Y_2$:
>  $P(0.375Y_2 - 0.1875Y_1 + \epsilon_3 | Y_2) = 0.375Y_2 + P(-0.1875Y_1+\epsilon_3|Y_2)$
>
>  Para calcular $P(-0.1875Y_1+\epsilon_3|Y_2)$, notamos que $\epsilon_3$ √© ortogonal a $Y_2$, ent√£o $P(\epsilon_3|Y_2) = 0$. Agora, $P(-0.1875Y_1|Y_2) = \frac{Cov(-0.1875Y_1, Y_2)}{Var(Y_2)}Y_2 = \frac{Cov(-0.1875\epsilon_1, 0.5\epsilon_1+\epsilon_2)}{Var(0.5\epsilon_1+\epsilon_2)}Y_2 = \frac{-0.1875*0.5}{0.5^2+1}Y_2 = \frac{-0.09375}{1.25}Y_2 = -0.075Y_2$.
>
>  Portanto, $P(Y_3 - P(Y_3|Y_1)|Y_2) = 0.375Y_2 - 0.075Y_2 = 0.3Y_2$.
>
> **Passo 2: Verifica√ß√£o da Decomposi√ß√£o**
>
>  Somando as proje√ß√µes:
>
>  $P(Y_3|Y_1) + P(Y_3 - P(Y_3|Y_1)|Y_2) = 0.4375Y_1 + 0.3Y_2$.
>
>  O que √© diferente de $P(Y_3|Y_1,Y_2)=0.25Y_1 + 0.375Y_2$. Este exemplo mostra que os coeficientes s√£o recalculados em cada passo da decomposi√ß√£o.
>
>  A decomposi√ß√£o correta √©:
>
>   $P(Y_3|Y_1) + P(Y_3 - P(Y_3|Y_1)|Y_2) = 0.4375Y_1 + P(0.375Y_2 - 0.1875Y_1 + \epsilon_3 | Y_2) = 0.4375Y_1 + 0.375Y_2 -0.1875P(Y_1|Y_2) = 0.4375Y_1 + 0.375Y_2 -0.1875(0.5/1.25)Y_2 = 0.4375Y_1 + 0.375Y_2 -0.075Y_2 = 0.4375Y_1 + 0.3Y_2$. O que n√£o √© igual a $P(Y_3|Y_1,Y_2)$.
>
>
>  Na verdade, o correto √© decompor em termos dos res√≠duos:
>  $P(Y_3|Y_1, Y_2) = P(Y_3|Y_1) + P(Y_3 - P(Y_3|Y_1)|Y_2 - P(Y_2|Y_1)) = P(Y_3|Y_1) + P(Y_3 - P(Y_3|Y_1)|\hat{Y_2})$.
>   $P(Y_2|Y_1) = 0.5Y_1$
>   $\hat{Y_2} = Y_2 - P(Y_2|Y_1) = \epsilon_2$
>
>   Ent√£o $P(Y_3 - P(Y_3|Y_1)|\hat{Y_2}) = P(0.375Y_2 - 0.1875Y_1 + \epsilon_3|\epsilon_2) = P(0.375(0.5Y_1+\epsilon_2) - 0.1875Y_1 + \epsilon_3|\epsilon_2) = P(0.1875Y_1 + 0.375\epsilon_2 - 0.1875Y_1 + \epsilon_3|\epsilon_2) = P(0.375\epsilon_2+\epsilon_3|\epsilon_2) = 0.375\epsilon_2 = 0.375(Y_2 - 0.5Y_1)$.
>   $P(Y_3|Y_1, Y_2) = 0.4375Y_1 + 0.375(Y_2-0.5Y_1) = 0.4375Y_1 + 0.375Y_2 - 0.1875Y_1 = 0.25Y_1 + 0.375Y_2$
>
>
> **Resultado:**
>
>  A decomposi√ß√£o de proje√ß√£o funciona quando as vari√°veis s√£o escritas em termos dos res√≠duos.

Em termos pr√°ticos, ao projetar uma vari√°vel em um conjunto de informa√ß√µes e projetar novamente o resultado em um subconjunto dessas informa√ß√µes, a lei das proje√ß√µes iteradas assegura que nada √© ganho ou perdido nesse processo. Ela garante, portanto, que o processo de proje√ß√µes m√∫ltiplas √© consistente e que n√£o h√° informa√ß√£o duplicada sendo incorporada.

### Conclus√£o
Este cap√≠tulo formalizou a lei das proje√ß√µes iteradas e explorou suas implica√ß√µes para a consist√™ncia de proje√ß√µes lineares. A lei estabelece que projetar novamente uma proje√ß√£o em um subespa√ßo j√° considerado n√£o adiciona informa√ß√µes novas √† proje√ß√£o original. Essa propriedade, que se baseia na ortogonalidade dos res√≠duos, √© crucial para processos iterativos de atualiza√ß√£o, como os vistos em cap√≠tulos anteriores, garantindo a consist√™ncia e a efici√™ncia do processo de atualiza√ß√£o. A lei das proje√ß√µes iteradas fornece uma base te√≥rica s√≥lida para a utiliza√ß√£o de proje√ß√µes lineares em diversas √°reas, desde previs√£o de s√©ries temporais at√© an√°lise de regress√£o em contextos din√¢micos.

### Refer√™ncias
[^4]: Se√ß√µes do Cap√≠tulo 4 do livro texto fornecido.
<!-- END -->
