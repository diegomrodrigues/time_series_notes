## Fatora√ß√£o Triangular e Atualiza√ß√£o de Proje√ß√µes Lineares

### Introdu√ß√£o
Este cap√≠tulo explora a aplica√ß√£o da decomposi√ß√£o triangular na atualiza√ß√£o de proje√ß√µes lineares, complementando os t√≥picos previamente discutidos sobre a atualiza√ß√£o de proje√ß√µes e regress√£o OLS [^4]. Atrav√©s da fatora√ß√£o triangular de matrizes de covari√¢ncia, podemos entender como o erro de uma nova proje√ß√£o se relaciona com o erro da proje√ß√£o original e com a nova informa√ß√£o dispon√≠vel. Esta abordagem n√£o apenas oferece uma interpreta√ß√£o geom√©trica da atualiza√ß√£o, mas tamb√©m fornece um mecanismo eficiente para o c√°lculo das proje√ß√µes e seus respectivos erros. Expandindo o conceito de atualiza√ß√£o de proje√ß√µes lineares, esta se√ß√£o destaca a import√¢ncia da fatora√ß√£o triangular para a atualiza√ß√£o de previs√µes e o c√°lculo da vari√¢ncia do erro de previs√£o.

### Conceitos Fundamentais

A fatora√ß√£o triangular, apresentada na se√ß√£o 4.4 [^4], desempenha um papel crucial na compreens√£o da atualiza√ß√£o de proje√ß√µes lineares. Dada uma matriz de covari√¢ncia $\Omega$, a fatora√ß√£o triangular nos permite express√°-la como $\Omega = ADA'$, onde $A$ √© uma matriz triangular inferior com 1s na diagonal principal e $D$ √© uma matriz diagonal [^4, ^4.4.1]. Como vimos anteriormente [^4], o erro da proje√ß√£o linear pode ser interpretado como um res√≠duo, ou seja, a parte da vari√°vel que n√£o √© explicada pela proje√ß√£o linear. A fatora√ß√£o triangular permite decompor a matriz de covari√¢ncia em componentes que capturam as rela√ß√µes entre as vari√°veis e os res√≠duos de proje√ß√£o.
Para entender a rela√ß√£o entre a fatora√ß√£o triangular e a atualiza√ß√£o de proje√ß√µes lineares, vamos considerar o vetor de vari√°veis aleat√≥rias $Y = (Y_1, Y_2, \ldots, Y_n)'$, cuja matriz de covari√¢ncia √© $\Omega = E(YY')$. A fatora√ß√£o triangular de $\Omega$ permite definir um novo vetor de vari√°veis transformadas $\hat{Y} = A^{-1}Y$, de tal modo que $E(\hat{Y}\hat{Y}')=D$, ou seja, os elementos de $\hat{Y}$ s√£o descorrelacionados. Nesse contexto, $A$ e $D$ desempenham pap√©is espec√≠ficos. A matriz $A$ cont√©m os coeficientes das proje√ß√µes lineares sequenciais de cada $Y_i$ nos valores precedentes ($Y_1, Y_2, \ldots, Y_{i-1}$), e a matriz $D$ cont√©m os erros quadr√°ticos m√©dios (MSE) correspondentes a essas proje√ß√µes [^4, ^4.5.2, ^4, ^4.5.4]. Em outras palavras, cada elemento diagonal $d_{ii}$ de $D$ representa o MSE da proje√ß√£o de $Y_i$ em $Y_1, Y_2, \ldots, Y_{i-1}$ [^4, ^4.5.5].

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um vetor de vari√°veis aleat√≥rias $Y = (Y_1, Y_2, Y_3)'$ com a seguinte matriz de covari√¢ncia:
>
> $$\Omega = \begin{bmatrix} 4 & 2 & 1 \\ 2 & 5 & 2 \\ 1 & 2 & 6 \end{bmatrix}$$
>
> Nossa tarefa √© encontrar a fatora√ß√£o triangular $\Omega = ADA'$.
>
> **Passo 1: Calcular a matriz A e D.**
>
>  -  $d_{11} = \Omega_{11} = 4$
>  -  $a_{21} = \frac{\Omega_{21}}{d_{11}} = \frac{2}{4} = 0.5$
>  -  $d_{22} = \Omega_{22} - a_{21}^2 d_{11} = 5 - 0.5^2 \times 4 = 5 - 1 = 4$
>  -  $a_{31} = \frac{\Omega_{31}}{d_{11}} = \frac{1}{4} = 0.25$
>  -  $a_{32} = \frac{\Omega_{32} - a_{31} a_{21} d_{11}}{d_{22}} = \frac{2 - 0.25 \times 0.5 \times 4}{4} = \frac{2 - 0.5}{4} = \frac{1.5}{4} = 0.375$
>  -  $d_{33} = \Omega_{33} - a_{31}^2 d_{11} - a_{32}^2 d_{22} = 6 - 0.25^2 \times 4 - 0.375^2 \times 4 = 6 - 0.25 - 0.5625 = 5.1875$
>
> Portanto, temos as seguintes matrizes:
>
> $$A = \begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.25 & 0.375 & 1 \end{bmatrix}$$
>
> $$D = \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 5.1875 \end{bmatrix}$$
>
>
> **Passo 2: Interpreta√ß√£o.**
>
>  - A matriz $A$ fornece os coeficientes para as proje√ß√µes lineares sequenciais. Por exemplo, $a_{21} = 0.5$ √© o coeficiente da proje√ß√£o de $Y_2$ em $Y_1$. $a_{31} = 0.25$ e $a_{32} = 0.375$ s√£o os coeficientes da proje√ß√£o de $Y_3$ em $Y_1$ e $Y_2$
>  - A matriz $D$ cont√©m os erros quadr√°ticos m√©dios (MSEs) dessas proje√ß√µes. $d_{11} = 4$ √© a vari√¢ncia de $Y_1$, $d_{22} = 4$ √© o MSE da proje√ß√£o de $Y_2$ em $Y_1$, e $d_{33} = 5.1875$ √© o MSE da proje√ß√£o de $Y_3$ em $Y_1$ e $Y_2$.
>
> **Passo 3: C√°lculo das vari√°veis transformadas.**
>
> Podemos encontrar $\hat{Y} = A^{-1}Y$.  Para este exemplo, a inversa de A √©
> $$A^{-1} = \begin{bmatrix} 1 & 0 & 0 \\ -0.5 & 1 & 0 \\ 0 & -0.375 & 1 \end{bmatrix}$$
>
> Assim,
>
> - $\hat{Y_1} = Y_1$
> - $\hat{Y_2} = -0.5Y_1 + Y_2$
> - $\hat{Y_3} =  -0.375Y_2 + Y_3$
>
> **Passo 4: Verifica√ß√£o da Descorrela√ß√£o**
>
> As vari√°veis transformadas $\hat{Y}$ s√£o descorrelacionadas.  Para conferir, podemos calcular a matriz de covari√¢ncia de $\hat{Y}$:
>
> $$Cov(\hat{Y}) = A^{-1} \Omega (A^{-1})^T = D$$
>
> A matriz $D$ √© diagonal, comprovando que as vari√°veis transformadas s√£o descorrelacionadas.
>
> Este exemplo demonstra como a fatora√ß√£o triangular nos permite decompor a matriz de covari√¢ncia, obter os coeficientes das proje√ß√µes lineares sequenciais e os erros de proje√ß√£o. Essa informa√ß√£o √© fundamental para entender a atualiza√ß√£o de proje√ß√µes e previs√µes.

**Proposi√ß√£o 2**
A decomposi√ß√£o triangular de uma matriz de covari√¢ncia $\Omega$ fornece os coeficientes para atualizar proje√ß√µes lineares de forma sequencial, e os elementos diagonais da matriz D representam os erros de proje√ß√£o. O erro da nova proje√ß√£o, pode ser expresso como uma combina√ß√£o linear dos erros das proje√ß√µes originais e da nova informa√ß√£o dispon√≠vel.

*Proof strategy:* Esta proposi√ß√£o estabelece uma ponte entre a fatora√ß√£o triangular e a atualiza√ß√£o de proje√ß√µes lineares. Ela demonstra que a matriz A na decomposi√ß√£o cont√©m os coeficientes da proje√ß√£o linear e que a matriz D cont√©m os erros associados √†s proje√ß√µes. O erro de proje√ß√£o, ent√£o, pode ser interpretado como o res√≠duo da proje√ß√£o, demonstrando que a decomposi√ß√£o triangular captura essa informa√ß√£o.
*Proof:*
I.  Come√ßamos com a defini√ß√£o de $\hat{Y} = A^{-1}Y$ e $\Omega = E(YY')$. A matriz de covari√¢ncia de $\hat{Y}$ √© dada por $E(\hat{Y}\hat{Y}') = A^{-1} \Omega (A^{-1})'$. Como $\Omega = ADA'$, ent√£o $E(\hat{Y}\hat{Y}')= A^{-1} ADA' (A^{-1})' = D$.  Como $D$ √© uma matriz diagonal, as vari√°veis de $\hat{Y}$ s√£o descorrelacionadas entre si.
II.  Vamos expandir $\hat{Y}$ para entender a estrutura de $A^{-1}$.  O elemento $Y_1$ n√£o √© transformado, j√° que $\hat{Y_1} = Y_1$. O segundo elemento √© $\hat{Y_2} = Y_2 - \Omega_{21}\Omega_{11}^{-1}Y_1$ [^4, ^4.5.9], que √© o res√≠duo da proje√ß√£o de $Y_2$ em $Y_1$.  O terceiro elemento √© o res√≠duo da proje√ß√£o de $Y_3$ em $Y_1$ e $Y_2$. Em geral, cada elemento $\hat{Y_i}$ √© o res√≠duo da proje√ß√£o linear de $Y_i$ nos elementos precedentes de $Y$.
III.  Assim, a matriz $A^{-1}$ cont√©m os coeficientes usados nas proje√ß√µes sequenciais, onde a linha $i$ de $A^{-1}$ representa os coeficientes para obter a proje√ß√£o de $Y_i$ nos valores anteriores.   Explicitamente, temos:
    $$ \begin{bmatrix}
    \hat{Y_1} \\ \hat{Y_2} \\ \hat{Y_3} \\ \vdots \\ \hat{Y_n}
    \end{bmatrix}
    =
    \begin{bmatrix}
    1 & 0 & 0 & \ldots & 0 \\
    -\Omega_{21}\Omega_{11}^{-1} & 1 & 0 & \ldots & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    \end{bmatrix} \begin{bmatrix}
    Y_1 \\ Y_2 \\ Y_3 \\ \vdots \\ Y_n
    \end{bmatrix}
    $$
IV.  Como a matriz de covari√¢ncia das vari√°veis transformadas $\hat{Y}$ √© dada por $D$, $E(\hat{Y}\hat{Y}')=D$, segue que $E(\hat{Y_i}^2) = d_{ii}$.   E como $\hat{Y_i}$ representa o res√≠duo da proje√ß√£o de $Y_i$ em $Y_1, Y_2, \ldots, Y_{i-1}$, $d_{ii}$ √© o MSE dessa proje√ß√£o.
V.  Podemos derivar a atualiza√ß√£o da proje√ß√£o a partir da decomposi√ß√£o triangular. A diferen√ßa entre a vari√°vel observada $Y_i$ e sua proje√ß√£o com base em valores anteriores √© $\hat{Y_i}$. As vari√°veis $\hat{Y_i}$ s√£o ortogonais, ent√£o as proje√ß√µes e os res√≠duos s√£o independentes entre si.
VI. Conclu√≠mos que a fatora√ß√£o triangular de $\Omega$ fornece os coeficientes da proje√ß√£o linear sequencial em A e os erros correspondentes em D, e o erro da nova proje√ß√£o linear pode ser expresso em fun√ß√£o dos erros da proje√ß√£o original, com os coeficientes contidos em A.
‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Retomando o exemplo anterior, vamos calcular as proje√ß√µes sequenciais e seus erros.
>
> 1. **Proje√ß√£o de $Y_2$ em $Y_1$:**
>   - O coeficiente da proje√ß√£o √© $a_{21} = 0.5$.
>   - A proje√ß√£o √© $\hat{Y}_{2|1} = 0.5Y_1$.
>   - O erro da proje√ß√£o √© $d_{22} = 4$.
>   - O res√≠duo da proje√ß√£o √© $\hat{Y_2} = Y_2 - 0.5Y_1$
>
> 2.  **Proje√ß√£o de $Y_3$ em $Y_1$ e $Y_2$:**
>   - Os coeficientes da proje√ß√£o s√£o $a_{31} = 0.25$ e $a_{32} = 0.375$.
>   - A proje√ß√£o √© $\hat{Y}_{3|1,2} = 0.25Y_1 + 0.375Y_2$.
>   - O erro da proje√ß√£o √© $d_{33} = 5.1875$.
>
> 3. **Atualiza√ß√£o da Proje√ß√£o de $Y_3$:**
>
>   -  Inicialmente, a proje√ß√£o de $Y_3$ em $Y_1$ √© $\hat{Y}_{3|1} = 0.25Y_1$.
>   -  Para melhorar a proje√ß√£o, usamos o res√≠duo de $Y_2$, que √© $\hat{Y_2} = Y_2 - 0.5Y_1$.
>   -  A nova proje√ß√£o √© $\hat{Y}_{3|1,2} = \hat{Y}_{3|1} + 0.375(Y_2-0.5Y_1) = 0.25Y_1 + 0.375Y_2 -0.1875Y_1 =  0.0625Y_1 + 0.375Y_2$.
>   -  O erro associado a essa proje√ß√£o √© $d_{33} = 5.1875$.
>
> Este exemplo mostra como a fatora√ß√£o triangular nos permite construir as proje√ß√µes sequencialmente, utilizando os coeficientes da matriz A e calculando os erros das proje√ß√µes com a matriz D. Os res√≠duos obtidos atrav√©s de $A^{-1}$ s√£o usados para atualizar a proje√ß√£o.

#### Atualiza√ß√£o Sequencial
A decomposi√ß√£o triangular fornece um m√©todo para atualizar as proje√ß√µes lineares sequencialmente. Suponha que temos uma proje√ß√£o inicial de $Y_n$ nos valores anteriores ($Y_1, \ldots, Y_{n-1}$), e recebemos uma nova vari√°vel, $Y_{n+1}$. Para atualizar a proje√ß√£o, √© preciso calcular o res√≠duo $Y_{n+1}$ a partir dos dados anteriores e ent√£o adicionar esse res√≠duo √† proje√ß√£o inicial.
A atualiza√ß√£o sequencial √© obtida atrav√©s das matrizes $A$ e $D$. A matriz $A^{-1}$ indica como transformar as vari√°veis originais para obter os res√≠duos, e a matriz $D$ indica o erro associado com essas proje√ß√µes. Isso pode ser expresso de forma iterativa:
1.  Come√ßamos com $Y_1$ e temos $d_{11}$ como o erro da proje√ß√£o (a vari√¢ncia de $Y_1$ em si mesma).
2.  Calculamos o res√≠duo $Y_2 - P(Y_2|Y_1)$, com o erro correspondente $d_{22}$.
3.  Em cada etapa, calculamos o novo res√≠duo subtraindo da vari√°vel atual a proje√ß√£o linear baseada nos valores anteriores. O erro da proje√ß√£o correspondente √© dada pelo elemento diagonal da matriz $D$.
Assim, ao usarmos a fatora√ß√£o triangular, n√£o apenas decompomos a matriz de covari√¢ncia original, mas tamb√©m obtemos os elementos que permitem construir os res√≠duos sequencialmente e projet√°-los em novas informa√ß√µes, fornecendo uma abordagem eficiente para atualiza√ß√£o de proje√ß√µes lineares, o que se mostra fundamental em problemas de previs√£o e regress√£o com informa√ß√£o incremental.

**Lema 2.1**
A matriz $A^{-1}$ na decomposi√ß√£o triangular de $\Omega$ √© tal que seus elementos abaixo da diagonal principal, com sinal invertido, correspondem aos coeficientes das proje√ß√µes lineares sequenciais. Formalmente, o elemento $(i,j)$ de $A^{-1}$, denotado por $a^{-1}_{ij}$, para $i > j$, √© dado por $-\frac{Cov(Y_i, \hat{Y_j})}{Var(\hat{Y_j})}$, onde $\hat{Y_j}$ √© o res√≠duo da proje√ß√£o de $Y_j$ nos elementos anteriores $Y_1, \ldots, Y_{j-1}$.

*Proof strategy:* Este lema formaliza a interpreta√ß√£o dos elementos da matriz $A^{-1}$ como coeficientes de proje√ß√µes lineares, especificando que esses coeficientes s√£o obtidos atrav√©s da divis√£o da covari√¢ncia entre a vari√°vel de interesse e o res√≠duo da proje√ß√£o pelo erro quadr√°tico m√©dio do res√≠duo.

*Proof:*
I.  Da Proposi√ß√£o 2, sabemos que $\hat{Y} = A^{-1} Y$, onde $\hat{Y_i}$ √© o res√≠duo da proje√ß√£o linear de $Y_i$ em $Y_1, \ldots, Y_{i-1}$.
II. Expandindo a equa√ß√£o para um elemento gen√©rico, temos:
    $$ \hat{Y_i} = Y_i + \sum_{j=1}^{i-1} a^{-1}_{ij}Y_j $$
   onde $a^{-1}_{ij}$ s√£o os elementos da matriz $A^{-1}$. Note que os elementos diagonais de $A^{-1}$ s√£o todos 1 e os elementos acima da diagonal s√£o 0.
III.  Reescrevendo a equa√ß√£o, podemos expressar $Y_i$ como uma fun√ß√£o dos res√≠duos e dos coeficientes de $A^{-1}$:
    $$ Y_i = \hat{Y_i} - \sum_{j=1}^{i-1} a^{-1}_{ij}Y_j $$
IV.  Seja $P(Y_i|Y_1,\ldots,Y_{i-1})$ a proje√ß√£o linear de $Y_i$ no subespa√ßo gerado por $Y_1,\ldots,Y_{i-1}$. Ent√£o, $\hat{Y_i} = Y_i - P(Y_i|Y_1,\ldots,Y_{i-1})$.
V.  Da teoria de proje√ß√µes, sabemos que o coeficiente da proje√ß√£o de $Y_i$ em $Y_j$ (condicionado em $Y_1,\ldots,Y_{j-1}$) √© dado por $\frac{Cov(Y_i, \hat{Y_j})}{Var(\hat{Y_j})}$. Como $A^{-1}$ expressa os coeficientes para calcular o res√≠duo, e o res√≠duo √© a diferen√ßa entre a vari√°vel original e a proje√ß√£o linear, o elemento $(i,j)$ de $A^{-1}$, com $i>j$, deve ser o negativo desse coeficiente, isto √© $a^{-1}_{ij} = -\frac{Cov(Y_i, \hat{Y_j})}{Var(\hat{Y_j})}$. Note que $\hat{Y_j}$ √© ortogonal a $Y_1,\ldots,Y_{j-1}$, o que garante que $Cov(Y_i, \hat{Y_j})$ captura a contribui√ß√£o de $Y_j$ em $Y_i$.
VI. Portanto, $A^{-1}$ cont√©m os coeficientes negativos das proje√ß√µes lineares sequenciais, concluindo a demonstra√ß√£o.
‚ñ†
> üí° **Exemplo Num√©rico:**
>
> Utilizando o exemplo anterior, vamos explicitar a rela√ß√£o entre os elementos de $A^{-1}$ e os coeficientes de proje√ß√£o.
>
> A matriz $A^{-1}$ √©:
>
> $$A^{-1} = \begin{bmatrix} 1 & 0 & 0 \\ -0.5 & 1 & 0 \\ 0 & -0.375 & 1 \end{bmatrix}$$
>
>
> -  O elemento $a^{-1}_{21} = -0.5$. Isso significa que o res√≠duo da proje√ß√£o de $Y_2$ em $Y_1$ √© $\hat{Y_2} = Y_2 - 0.5Y_1$. O coeficiente de proje√ß√£o √© $0.5$, que √© o negativo de $a^{-1}_{21}$.
>
> - O elemento $a^{-1}_{32} = -0.375$.  O res√≠duo de $Y_3$ ap√≥s projetar em $Y_1$ √© $\hat{Y}_{3|1} = Y_3 - 0.25Y_1$.  A proje√ß√£o de $Y_3$ em $Y_2$ (ap√≥s projetar $Y_2$ em $Y_1$) adiciona  $0.375\hat{Y_2}$ onde $\hat{Y_2} = Y_2 - 0.5Y_1$. O coeficiente de proje√ß√£o √© $0.375$, que √© o negativo de $a^{-1}_{32}$.
>
> Este exemplo demonstra como os elementos abaixo da diagonal principal da matriz $A^{-1}$ fornecem os coeficientes de proje√ß√µes lineares sequenciais, com sinal invertido.

**Teorema 2.1**
Seja $P(Y_{n+1}|Y_1, \ldots, Y_n)$ a proje√ß√£o linear de $Y_{n+1}$ em $Y_1, \ldots, Y_n$, e seja $P(Y_{n+1}|Y_1, \ldots, Y_{n-1})$ a proje√ß√£o linear de $Y_{n+1}$ em $Y_1, \ldots, Y_{n-1}$.  Seja $\hat{Y_n}$ o res√≠duo da proje√ß√£o de $Y_n$ em $Y_1, \ldots, Y_{n-1}$.  Ent√£o, a proje√ß√£o atualizada √© dada por
$$ P(Y_{n+1}|Y_1, \ldots, Y_n) = P(Y_{n+1}|Y_1, \ldots, Y_{n-1}) + \frac{Cov(Y_{n+1}, \hat{Y_n})}{Var(\hat{Y_n})}\hat{Y_n} $$

*Proof strategy:* Este teorema formaliza o processo de atualiza√ß√£o da proje√ß√£o linear, mostrando que a proje√ß√£o atualizada √© igual √† proje√ß√£o anterior mais a contribui√ß√£o da nova vari√°vel, dada pelo seu res√≠duo ponderado pela raz√£o entre a covari√¢ncia entre a nova vari√°vel e o res√≠duo, e a vari√¢ncia do res√≠duo.
*Proof:*
I.   Da Proposi√ß√£o 2, sabemos que cada $\hat{Y_i}$ √© o res√≠duo da proje√ß√£o linear de $Y_i$ em $Y_1, \ldots, Y_{i-1}$. Portanto, $\hat{Y_n} = Y_n - P(Y_n|Y_1, \ldots, Y_{n-1})$.
II.  A proje√ß√£o $P(Y_{n+1}|Y_1, \ldots, Y_n)$ √© dada pela soma da proje√ß√£o de $Y_{n+1}$ nos elementos anteriores, mais a contribui√ß√£o de $Y_n$ condicional aos elementos anteriores.
III. Usando o resultado do Lema 2.1, o coeficiente da proje√ß√£o de $Y_{n+1}$ em $\hat{Y_n}$ √© dado por $\frac{Cov(Y_{n+1}, \hat{Y_n})}{Var(\hat{Y_n})}$.
IV.  Ent√£o, a proje√ß√£o de $Y_{n+1}$ em $Y_1, \ldots, Y_n$ pode ser escrita como:
$$ P(Y_{n+1}|Y_1, \ldots, Y_n) = P(Y_{n+1}|Y_1, \ldots, Y_{n-1}) + \frac{Cov(Y_{n+1}, \hat{Y_n})}{Var(\hat{Y_n})}\hat{Y_n} $$
V.  Isso mostra que a atualiza√ß√£o da proje√ß√£o envolve adicionar √† proje√ß√£o anterior uma parcela que depende da covari√¢ncia entre $Y_{n+1}$ e o res√≠duo de $Y_n$, escalado pela vari√¢ncia do res√≠duo, concluindo a demonstra√ß√£o.
‚ñ†
> üí° **Exemplo Num√©rico:**
>
> Suponha que tenhamos uma nova vari√°vel $Y_4$, e que a matriz de covari√¢ncia expandida seja:
>
> $$\Omega_{exp} = \begin{bmatrix} 4 & 2 & 1 & 0.5 \\ 2 & 5 & 2 & 1 \\ 1 & 2 & 6 & 1.5 \\ 0.5 & 1 & 1.5 & 7 \end{bmatrix}$$
>
> J√° calculamos a decomposi√ß√£o triangular para as tr√™s primeiras vari√°veis. Agora, vamos atualizar a proje√ß√£o com a nova vari√°vel $Y_4$.
>
> 1. **Fatora√ß√£o Triangular Estendida:**
>   -  Calculamos os novos coeficientes $a_{41}$, $a_{42}$, $a_{43}$ e o novo erro $d_{44}$.
>   - $a_{41} = \frac{\Omega_{41}}{d_{11}} = \frac{0.5}{4} = 0.125$
>   - $a_{42} = \frac{\Omega_{42} - a_{41} a_{21} d_{11}}{d_{22}} = \frac{1 - 0.125 \times 0.5 \times 4}{4} = \frac{1 - 0.25}{4} = 0.1875$
>   - $a_{43} = \frac{\Omega_{43} - a_{41} a_{31} d_{11} - a_{42} a_{32} d_{22}}{d_{33}} = \frac{1.5 - 0.125 \times 0.25 \times 4 - 0.1875 \times 0.375 \times 4}{5.1875} = \frac{1.5 - 0.125 - 0.28125}{5.1875} = \frac{1.09375}{5.1875} \approx 0.2108$
>   - $d_{44} = 7 - a_{41}^2 d_{11} - a_{42}^2 d_{22} - a_{43}^2 d_{33} = 7 - 0.125^2 \times 4 - 0.1875^2 \times 4 - 0.2108^2 \times 5.1875 = 7 - 0.0625 - 0.140625 - 0.2302 \approx 6.5666$
>
> 2. **Proje√ß√£o Atualizada:**
>   -  A proje√ß√£o de $Y_4$ em $Y_1, Y_2, Y_3$ √© dada por:
>      $$P(Y_4|Y_1, Y_2, Y_3) = 0.125Y_1 + 0.1875Y_2 + 0.2108Y_3$$
>   - O res√≠duo de $Y_3$ (j√° projetado em $Y_1$ e $Y_2$) √©  $\hat{Y_3} = Y_3 - 0.25Y_1 - 0.375(Y_2-0.5Y_1) = Y_3-0.0625Y_1-0.375Y_2$
>   -  A proje√ß√£o de $Y_4$ em $Y_1,Y_2$ √© $P(Y_4|Y_1, Y_2) = 0.125Y_1 + 0.1875Y_2$
>  - Usando o Teorema 2.1, a proje√ß√£o atualizada √©:
> $$P(Y_4|Y_1, Y_2, Y_3) = P(Y_4|Y_1, Y_2) + \frac{Cov(Y_4, \hat{Y_3})}{Var(\hat{Y_3})} \hat{Y_3} = 0.125Y_1 + 0.1875Y_2 + 0.2108\hat{Y_3}$$
>
> Este exemplo demonstra como a fatora√ß√£o triangular nos permite atualizar a proje√ß√£o sequencialmente, adicionando a contribui√ß√£o da nova vari√°vel com base em seu res√≠duo em rela√ß√£o √†s vari√°veis anteriores.

**Corol√°rio 2.1**
O erro quadr√°tico m√©dio (MSE) da proje√ß√£o atualizada, denotado por $E[(Y_{n+1} - P(Y_{n+1}|Y_1, \ldots, Y_n))^2]$, √© dado por $d_{n+1,n+1}$, o elemento diagonal correspondente na matriz $D$ da decomposi√ß√£o triangular de $\Omega$ aumentada com a nova vari√°vel $Y_{n+1}$.

*Proof strategy:* Este corol√°rio afirma que o erro da proje√ß√£o atualizada √© dado pelo elemento diagonal da matriz D ap√≥s a incorpora√ß√£o da nova vari√°vel. Isso √© uma consequ√™ncia direta da Proposi√ß√£o 2 e do Teorema 2.1.
*Proof:*
I.  Da Proposi√ß√£o 2, sabemos que os elementos diagonais da matriz $D$ representam os MSEs das proje√ß√µes sequenciais.
II.  Se adicionarmos uma nova vari√°vel $Y_{n+1}$ ao vetor $Y$, a matriz de covari√¢ncia $\Omega$ √© aumentada, e a fatora√ß√£o triangular de $\Omega$ resulta em uma nova matriz $D$ com um elemento adicional $d_{n+1,n+1}$.
III.  O elemento $d_{n+1,n+1}$ corresponde ao erro da proje√ß√£o de $Y_{n+1}$ em $Y_1, \ldots, Y_n$.
IV.  Pelo Teorema 2.1, a proje√ß√£o de $Y_{n+1}$ em $Y_1, \ldots, Y_n$ √© a proje√ß√£o atualizada, e o seu erro √© o elemento $d_{n+1,n+1}$.
V. Portanto, o MSE da proje√ß√£o atualizada √© igual a $d_{n+1,n+1}$, concluindo a demonstra√ß√£o.
‚ñ†
> üí° **Exemplo Num√©rico:**
>
> Continuando com o exemplo da se√ß√£o anterior, o MSE da proje√ß√£o de $Y_4$ em $Y_1, Y_2, Y_3$ √© dado por $d_{44} \approx 6.5666$. Isso significa que, ap√≥s a atualiza√ß√£o da proje√ß√£o de $Y_4$, o erro quadr√°tico m√©dio da previs√£o √© 6.5666. Este valor √© calculado diretamente durante a fatora√ß√£o triangular estendida e representa a vari√¢ncia do res√≠duo de $Y_4$ ap√≥s considerar a influ√™ncia de $Y_1, Y_2,$ e $Y_3$.

### Conclus√£o
A decomposi√ß√£o triangular de uma matriz de covari√¢ncia √© uma ferramenta poderosa para a atualiza√ß√£o de proje√ß√µes lineares, fornecendo uma maneira eficiente de entender e calcular como novas informa√ß√µes podem ser incorporadas para refinar previs√µes. A fatora√ß√£o triangular de $\Omega$ decomp√µe a matriz de covari√¢ncia, e fornece uma interpreta√ß√£o para o erro de proje√ß√£o como o res√≠duo sequencial das proje√ß√µes, al√©m dos coeficientes para a atualiza√ß√£o.  A conex√£o com a atualiza√ß√£o sequencial e os res√≠duos √© fundamental para aplica√ß√µes tanto em previs√£o quanto em modelos de regress√£o.

### Refer√™ncias
[^4]: Se√ß√µes do Cap√≠tulo 4 do livro texto fornecido.
<!-- END -->
