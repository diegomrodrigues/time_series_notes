## Fatora√ß√£o Triangular e Deriva√ß√£o da Proje√ß√£o Linear Exata

### Introdu√ß√£o
Este cap√≠tulo aprofunda a conex√£o entre a fatora√ß√£o triangular de uma matriz de covari√¢ncia e a deriva√ß√£o da proje√ß√£o linear exata. Expandindo os conceitos de proje√ß√£o linear e fatora√ß√£o triangular j√° introduzidos [^4], esta se√ß√£o explora como a fatora√ß√£o triangular pode ser utilizada para obter uma forma exata da proje√ß√£o linear, e tamb√©m como a matriz H, definida como o produto da matriz triangular e sua transposta, se relaciona com a matriz de covari√¢ncia das proje√ß√µes e com os erros de previs√£o. A rela√ß√£o entre a fatora√ß√£o triangular e o erro de proje√ß√£o, que resulta ser uma vari√°vel n√£o correlacionada, ser√° tamb√©m explorada em detalhe.

### Conceitos Fundamentais

Como explorado anteriormente, a fatora√ß√£o triangular de uma matriz de covari√¢ncia $\Omega$ nos permite decomp√¥-la como $\Omega = ADA'$, onde $A$ √© uma matriz triangular inferior com 1s na diagonal principal e $D$ √© uma matriz diagonal [^4, ^4.4.1]. Introduzimos tamb√©m o vetor transformado $\hat{Y} = A^{-1}Y$ tal que $E(\hat{Y} \hat{Y}') = D$, onde os elementos de $\hat{Y}$ s√£o os res√≠duos das proje√ß√µes lineares sequenciais e s√£o descorrelacionados [^4, ^4.5.2]. Agora, vamos analisar como essas vari√°veis transformadas e a matriz $H = E_1 \Omega E_1'$ (onde $E_1$ √© uma matriz de transforma√ß√£o que projeta em um subespa√ßo) se relacionam com a proje√ß√£o linear exata e seus erros.
A matriz $H$, definida como $E_1 \Omega E_1'$ [^4, ^4.5.15], onde $E_1$ √© a matriz de transforma√ß√£o que projeta o vetor original $Y$ no subespa√ßo de res√≠duos dados pelas proje√ß√µes sequenciais em um valor $Y_1$, desempenha um papel fundamental na deriva√ß√£o da proje√ß√£o linear exata. Explicitamente, $E_1$ transforma o vetor $Y$ em um vetor cujos elementos s√£o os res√≠duos das proje√ß√µes lineares em $Y_1$ [^4, ^4.5.15]. Como visto anteriormente [^4, ^4.5.15], a matriz $H$ √© a matriz de covari√¢ncia dos res√≠duos das proje√ß√µes em $Y_1$. Ou seja, se projetarmos $Y_2$ em $Y_1$ o res√≠duo ser√° $\hat{Y_2}$, a vari√¢ncia desse res√≠duo √© $h_{22}$ que √© um elemento de $H$, e a proje√ß√£o de $Y_3$ em $Y_1$ fornece o res√≠duo, $\hat{Y_3}$, que tamb√©m √© um elemento do vetor $\hat{Y}$, e que tem vari√¢ncia $h_{33}$, que tamb√©m √© elemento de $H$ [^4, ^4.5.15]. A matriz $H$, no contexto da fatora√ß√£o triangular, tem a seguinte forma:
$$ H = \begin{bmatrix}
    h_{11} & 0 & 0 & \ldots & 0 \\
    0 & h_{22} & h_{23} & \ldots & h_{2n} \\
    0 & h_{32} & h_{33} & \ldots & h_{3n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & h_{n2} & h_{n3} & \ldots & h_{nn}
\end{bmatrix} $$

√â importante ressaltar que $H$ tamb√©m pode ser decomposta da forma $H = K K'$, onde $K$ √© uma matriz triangular inferior [^4].

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um vetor de vari√°veis aleat√≥rias $Y = (Y_1, Y_2, Y_3)'$ com a matriz de covari√¢ncia $\Omega$ j√° utilizada no cap√≠tulo anterior:
>
> $$\Omega = \begin{bmatrix} 4 & 2 & 1 \\ 2 & 5 & 2 \\ 1 & 2 & 6 \end{bmatrix}$$
>
> J√° calculamos a fatora√ß√£o triangular de $\Omega$:
>
> $$A = \begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.25 & 0.375 & 1 \end{bmatrix}$$
>
> $$D = \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 5.1875 \end{bmatrix}$$
>
>
> Agora, vamos considerar a matriz $E_1$, que projeta em $Y_1$:
>
> $$E_1 = \begin{bmatrix} 1 & 0 & 0 \\ -0.5 & 1 & 0 \\  -0.25 & 0 & 1 \end{bmatrix}$$
>
> A matriz $H$ √© dada por $H = E_1 \Omega E_1'$:
>
> $$H = \begin{bmatrix} 1 & 0 & 0 \\ -0.5 & 1 & 0 \\  -0.25 & 0 & 1 \end{bmatrix}  \begin{bmatrix} 4 & 2 & 1 \\ 2 & 5 & 2 \\ 1 & 2 & 6 \end{bmatrix}  \begin{bmatrix} 1 & -0.5 & -0.25 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} =  \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 1.5 \\ 0 & 1.5 & 5.9375 \end{bmatrix}$$
>
> Note que $h_{11} = \Omega_{11} = 4$ √© a vari√¢ncia de $Y_1$, que corresponde tamb√©m √† vari√¢ncia do res√≠duo de $Y_1$ em si mesmo. O elemento $h_{22} = 4$ √© o erro quadr√°tico m√©dio da proje√ß√£o de $Y_2$ em $Y_1$, que √© tamb√©m o elemento $d_{22}$ da matriz $D$. O elemento $h_{33} = 5.9375$ √© a vari√¢ncia do res√≠duo da proje√ß√£o de $Y_3$ em $Y_1$. Note que esta matriz, $H$, corresponde a matriz de covari√¢ncia dos res√≠duos das proje√ß√µes em $Y_1$. Os elementos $h_{ij}$ para $i,j > 1$, correspondem √† covari√¢ncia dos res√≠duos.
>
> Podemos tamb√©m visualizar essa matriz $H$ como um diagrama de rela√ß√µes entre os res√≠duos:
>
> ```mermaid
> graph LR
>   Y1(Y1) -->|h11=4| Y1_res(Res√≠duo Y1);
>   Y2(Y2) -->|h22=4| Y2_res(Res√≠duo Y2);
>   Y3(Y3) -->|h33=5.9375| Y3_res(Res√≠duo Y3);
>   Y2_res -->|h23=1.5| Y3_res;
> ```

**Proposi√ß√£o 3**
A matriz $H$, calculada como $E_1 \Omega E_1'$, √© a matriz de covari√¢ncia dos res√≠duos da proje√ß√£o sequencial das vari√°veis $Y_i$ em $Y_1$. A matriz $H$ √© tamb√©m a matriz de covari√¢ncia do vetor $\hat{Y}(1)$, definido como $\hat{Y}(1) = E_1 Y$.

*Proof:*
I.  Definimos $\hat{Y}(1) = E_1Y$, onde $E_1$ √© uma matriz que transforma $Y$ em um vetor de res√≠duos em rela√ß√£o a $Y_1$.   Como vimos em se√ß√µes anteriores [^4, ^4.5.15], $E_1$ tem a forma
    $$
    E_1 = \begin{bmatrix}
    1 & 0 & 0 & \ldots & 0 \\
    - \Omega_{21} \Omega_{11}^{-1} & 1 & 0 & \ldots & 0 \\
    -\Omega_{31} \Omega_{11}^{-1} & 0 & 1 & \ldots & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    - \Omega_{n1} \Omega_{11}^{-1} & 0 & 0 & \ldots & 1
    \end{bmatrix}
    $$
   Em outras palavras, a matriz $E_1$ realiza as opera√ß√µes de projetar todas as vari√°veis $Y_i$ em $Y_1$, gerando os res√≠duos dessa proje√ß√£o.

II.  A matriz de covari√¢ncia de $\hat{Y}(1)$ √© dada por $E[\hat{Y}(1)\hat{Y}(1)'] = E[E_1YY'E_1'] = E_1 E(YY') E_1' = E_1\Omega E_1' = H$.   Assim, $H$ √© a matriz de covari√¢ncia do vetor $\hat{Y}(1)$, onde cada elemento $h_{ij}$ de $H$ representa a covari√¢ncia entre os res√≠duos das proje√ß√µes das vari√°veis $Y_i$ e $Y_j$ em $Y_1$.
III.  O elemento $h_{ii}$ de $H$ representa o MSE da proje√ß√£o de $Y_i$ em $Y_1$, enquanto $h_{ij}$ para $i \neq j$ representa a covari√¢ncia entre os res√≠duos. Assim, os elementos de $H$ quantificam como cada vari√°vel $Y_i$ varia em rela√ß√£o a $Y_1$ ap√≥s remover a depend√™ncia linear de $Y_1$ das demais vari√°veis.
IV. Portanto, a matriz $H = E_1 \Omega E_1'$ √© a matriz de covari√¢ncia dos res√≠duos da proje√ß√£o de $Y$ em $Y_1$, com cada $h_{ii}$ representando o erro quadr√°tico m√©dio do res√≠duo da proje√ß√£o de $Y_i$ em $Y_1$.
‚ñ†
> üí° **Exemplo Num√©rico:**
>
> Utilizando o exemplo num√©rico anterior, vamos explicitar a rela√ß√£o entre a matriz $H$ e os res√≠duos das proje√ß√µes.
>
>  J√° hav√≠amos calculado que:
>
>  $$ H = \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 1.5 \\ 0 & 1.5 & 5.9375 \end{bmatrix} $$
>
> Vamos agora calcular o vetor $\hat{Y}(1)$:
>
> $$ \hat{Y}(1) = E_1 Y  = \begin{bmatrix} 1 & 0 & 0 \\ -0.5 & 1 & 0 \\  -0.25 & 0 & 1 \end{bmatrix} \begin{bmatrix} Y_1 \\ Y_2 \\ Y_3 \end{bmatrix} = \begin{bmatrix} Y_1 \\ Y_2 - 0.5 Y_1 \\ Y_3 - 0.25 Y_1 \end{bmatrix}  $$
>
>  -  O primeiro elemento de $\hat{Y}(1)$ √© $Y_1$.
>  -  O segundo elemento √© o res√≠duo da proje√ß√£o de $Y_2$ em $Y_1$, $Y_2 - 0.5Y_1$.
>  -  O terceiro elemento √© o res√≠duo da proje√ß√£o de $Y_3$ em $Y_1$, $Y_3 - 0.25Y_1$.
>
>  As vari√¢ncias desses res√≠duos s√£o precisamente os elementos da diagonal de H:
>
>  - $Var(\hat{Y_1}) = 4$.
>  - $Var(\hat{Y_2}) = 4$.
>  - $Var(\hat{Y_3}) = 5.9375$.
>
>  Al√©m disso, a covari√¢ncia entre os res√≠duos de $Y_2$ e $Y_3$ em rela√ß√£o a $Y_1$, √© dada por $1.5$, que √© o elemento $h_{23}$ de $H$.  Este exemplo demonstra que a matriz $H$ de fato representa a matriz de covari√¢ncia dos res√≠duos das proje√ß√µes de $Y$ em $Y_1$.
>
>   Podemos verificar isso computacionalmente usando numpy:
>   ```python
>   import numpy as np
>
>   Omega = np.array([[4, 2, 1], [2, 5, 2], [1, 2, 6]])
>   E1 = np.array([[1, 0, 0], [-0.5, 1, 0], [-0.25, 0, 1]])
>   H = E1 @ Omega @ E1.T
>
>   print("Matriz H:")
>   print(H)
>
>   # Vamos simular valores para Y
>   np.random.seed(42) # para reprodutibilidade
>   Y = np.random.multivariate_normal([0, 0, 0], Omega, size=1000)
>
>   # Calcular Y_hat(1)
>   Y_hat_1 = E1 @ Y.T
>
>   # Calcular a matriz de covari√¢ncia de Y_hat(1)
>   cov_Y_hat_1 = np.cov(Y_hat_1)
>
>   print("\nCovari√¢ncia de Y_hat(1):")
>   print(cov_Y_hat_1)
>
>   # Os valores de H e a matriz de covariancia de Y_hat(1) devem ser muito pr√≥ximos
>   print("\n Diferen√ßa entre H e a covariancia de Y_hat(1):")
>   print(np.abs(H - cov_Y_hat_1))
>   ```
>
>   O c√≥digo acima calcula a matriz $H$ a partir da matriz de covari√¢ncia $\Omega$ e a matriz de transforma√ß√£o $E_1$. Depois, simula 1000 amostras de um vetor aleat√≥rio $Y$ com a covari√¢ncia $\Omega$ e calcula os res√≠duos $\hat{Y}(1)$. Finalmente, calcula a matriz de covari√¢ncia desses res√≠duos e verifica que ela √© aproximadamente igual a matriz $H$.

A matriz $H$ tem uma propriedade importante: ela pode ser escrita como o produto de uma matriz triangular inferior por sua transposta ($H = KK'$) [^4, ^4.4.7]. Essa fatora√ß√£o, como vimos anteriormente, auxilia na atualiza√ß√£o das proje√ß√µes lineares. Al√©m disso, a decomposi√ß√£o de $H$ em $KK'$ est√° diretamente relacionada com a decomposi√ß√£o de $\Omega$ em $ADA'$.

**Lema 3.1**
Se $\Omega = ADA'$ √© a fatora√ß√£o triangular da matriz de covari√¢ncia $\Omega$ e $H = E_1 \Omega E_1'$ √© a matriz de covari√¢ncia dos res√≠duos da proje√ß√£o de $Y$ em $Y_1$, ent√£o existe uma matriz triangular inferior $K$ tal que $H = KK'$ e a matriz $K$ pode ser obtida a partir da matriz $A$ e da matriz de proje√ß√£o $E_1$.

*Proof:*
I. Sabemos que $\Omega = ADA'$ e $H = E_1 \Omega E_1'$. Substituindo $\Omega$ em $H$, temos $H = E_1 (ADA') E_1' = (E_1A)D(E_1A)'$.
II. Note que $E_1A$ resulta em uma matriz triangular inferior, pois $A$ √© triangular inferior com 1s na diagonal e $E_1$ tem a forma
    $$
    E_1 = \begin{bmatrix}
    1 & 0 & 0 & \ldots & 0 \\
    - \Omega_{21} \Omega_{11}^{-1} & 1 & 0 & \ldots & 0 \\
    -\Omega_{31} \Omega_{11}^{-1} & 0 & 1 & \ldots & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    - \Omega_{n1} \Omega_{11}^{-1} & 0 & 0 & \ldots & 1
    \end{bmatrix}
    $$
   Assim, $E_1A$ √© uma matriz triangular inferior. Chamemos $E_1A = K_1$

III. Seja $K = K_1 D^{1/2}$, onde $D^{1/2}$ √© a raiz quadrada da matriz diagonal $D$. Ent√£o, $K$ tamb√©m √© triangular inferior. Substituindo, temos $H = K_1 D K_1' = K_1 D^{1/2} D^{1/2} K_1' = (K_1 D^{1/2}) (K_1 D^{1/2})' = K K'$.
IV. Portanto, $H$ pode ser fatorada como $H = KK'$, onde $K$ √© uma matriz triangular inferior obtida a partir de $A$, $E_1$ e $D$.
‚ñ†

**Teorema 3.1**
Dado o vetor $Y$ e sua matriz de covari√¢ncia $\Omega$, a fatora√ß√£o triangular de $\Omega$ pode ser utilizada para derivar a proje√ß√£o linear exata. O erro de previs√£o resultante √© uma vari√°vel n√£o correlacionada com as informa√ß√µes utilizadas para a previs√£o, e a vari√¢ncia do erro √© dada por um elemento na matriz diagonal $D$.

*Proof:*
I. Seja $\Omega = ADA'$ a fatora√ß√£o triangular da matriz de covari√¢ncia $\Omega$, onde $A$ √© uma matriz triangular inferior com 1s na diagonal principal, e $D$ √© uma matriz diagonal.
II.  Definimos o vetor $\hat{Y} = A^{-1} Y$. Da proposi√ß√£o 2,  sabemos que os elementos de $\hat{Y}$ s√£o os res√≠duos das proje√ß√µes lineares sequenciais. Especificamente, $\hat{Y_i}$ √© o res√≠duo da proje√ß√£o de $Y_i$ no subespa√ßo gerado por $Y_1, Y_2, \ldots, Y_{i-1}$, e as vari√°veis $\hat{Y_i}$ s√£o ortogonais entre si.
III.  A matriz de covari√¢ncia de $\hat{Y}$ √© $E(\hat{Y}\hat{Y}') = D$, que √© uma matriz diagonal, e cada elemento $d_{ii}$ da diagonal √© o erro quadr√°tico m√©dio (MSE) da proje√ß√£o de $Y_i$ em $Y_1, \ldots, Y_{i-1}$.
IV.  Para obter a proje√ß√£o linear exata, podemos usar o fato de que a proje√ß√£o de $Y_i$ em $Y_1, \ldots, Y_{i-1}$ √© a melhor aproxima√ß√£o linear de $Y_i$ no subespa√ßo gerado por $Y_1, \ldots, Y_{i-1}$.
V.  O res√≠duo dessa proje√ß√£o √© $\hat{Y_i}$. Do passo anterior, sabemos que $E(\hat{Y_i}^2) = d_{ii}$, e como os res√≠duos s√£o descorrelacionados, podemos dizer que $\hat{Y_i}$ √© uma vari√°vel n√£o correlacionada com as informa√ß√µes usadas para proje√ß√£o.
VI.  O processo para projetar $Y_i$ pode ser expresso usando a matriz $A^{-1}$, especificamente, cada linha $i$ de $A^{-1}$ d√° os coeficientes da proje√ß√£o linear de $Y_i$ em $Y_1, \ldots, Y_{i-1}$, e assim a proje√ß√£o exata pode ser obtida, iterativamente, utilizando $A^{-1}$.
VII. A matriz $D$ cont√©m, em cada elemento $d_{ii}$, o erro quadr√°tico m√©dio da proje√ß√£o exata de $Y_i$ nos elementos anteriores.
VIII. Portanto, a fatora√ß√£o triangular de $\Omega$ nos permite derivar a proje√ß√£o linear exata e identificar o erro de previs√£o resultante como uma vari√°vel n√£o correlacionada.
‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar o exemplo anterior onde:
>
> $$A = \begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.25 & 0.375 & 1 \end{bmatrix}$$
>
> $$D = \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 5.1875 \end{bmatrix}$$
>
> e $Y = (Y_1, Y_2, Y_3)'$.
>
> O vetor $\hat{Y}$ √© dado por $\hat{Y} = A^{-1} Y$. Primeiro, precisamos calcular $A^{-1}$. Usando um software de c√°lculo simb√≥lico ou m√©todos num√©ricos, encontramos:
>
> $$A^{-1} = \begin{bmatrix} 1 & 0 & 0 \\ -0.5 & 1 & 0 \\ 0.125 & -0.375 & 1 \end{bmatrix}$$
>
> Portanto, $\hat{Y}$ √©:
>
> $$\hat{Y} = \begin{bmatrix} 1 & 0 & 0 \\ -0.5 & 1 & 0 \\ 0.125 & -0.375 & 1 \end{bmatrix} \begin{bmatrix} Y_1 \\ Y_2 \\ Y_3 \end{bmatrix} = \begin{bmatrix} Y_1 \\ Y_2 - 0.5 Y_1 \\ Y_3 - 0.375 Y_2 + 0.125 Y_1 \end{bmatrix}$$
>
>
> Vamos analisar cada elemento:
>
> - $\hat{Y}_1 = Y_1$, o res√≠duo da proje√ß√£o de $Y_1$ nele mesmo, cuja vari√¢ncia √© $d_{11} = 4$.
>
> - $\hat{Y}_2 = Y_2 - 0.5 Y_1$, o res√≠duo da proje√ß√£o de $Y_2$ em $Y_1$, cuja vari√¢ncia √© $d_{22} = 4$.
>
> - $\hat{Y}_3 = Y_3 - 0.375 Y_2 + 0.125 Y_1$, o res√≠duo da proje√ß√£o de $Y_3$ em $Y_1$ e $Y_2$. A vari√¢ncia desse res√≠duo √© $d_{33} = 5.1875$.
>
> A proje√ß√£o linear exata de $Y_2$ em $Y_1$ √© $0.5 Y_1$ e o erro √© dado por $\hat{Y_2} = Y_2 - 0.5 Y_1$, que √© descorrelacionado com $Y_1$.
> A proje√ß√£o linear exata de $Y_3$ em $Y_1$ e $Y_2$ √© $0.375 Y_2 - 0.125 Y_1$ e o erro √© dado por $\hat{Y_3} = Y_3 - 0.375 Y_2 + 0.125 Y_1$, que √© descorrelacionado com $Y_1$ e $Y_2$.
>
>  Podemos verificar que a matriz de covari√¢ncia de $\hat{Y}$ √© de fato $D$ simulando amostras de $Y$ e calculando a covari√¢ncia de $\hat{Y}$:
>   ```python
>   import numpy as np
>
>   Omega = np.array([[4, 2, 1], [2, 5, 2], [1, 2, 6]])
>   A_inv = np.array([[1, 0, 0], [-0.5, 1, 0], [0.125, -0.375, 1]])
>   D = np.array([[4, 0, 0], [0, 4, 0], [0, 0, 5.1875]])
>
>   np.random.seed(42)
>   Y = np.random.multivariate_normal([0, 0, 0], Omega, size=1000)
>
>   Y_hat = A_inv @ Y.T
>   cov_Y_hat = np.cov(Y_hat)
>
>   print("Matriz de covari√¢ncia de Y_hat:")
>   print(cov_Y_hat)
>   print("\nMatriz D:")
>   print(D)
>   print("\nDiferen√ßa entre a covari√¢ncia de Y_hat e D:")
>   print(np.abs(cov_Y_hat - D))
>   ```
>
>   O c√≥digo acima mostra que a matriz de covari√¢ncia de $\hat{Y}$ √© aproximadamente a matriz $D$, como previsto pela teoria.

**Corol√°rio 3.1**
O vetor de res√≠duos $\hat{Y} = A^{-1}Y$ √© um conjunto de vari√°veis n√£o correlacionadas, e a vari√¢ncia de cada res√≠duo $\hat{Y_i}$ √© dada pelo elemento correspondente $d_{ii}$ da matriz diagonal $D$.

*Proof:*
I. Do Teorema 3.1, sabemos que $\hat{Y} = A^{-1}Y$ representa os res√≠duos das proje√ß√µes lineares sequenciais.
II. Tamb√©m do Teorema 3.1, temos que $E(\hat{Y}\hat{Y}') = D$, e como $D$ √© uma matriz diagonal, isso implica que a covari√¢ncia entre quaisquer dois res√≠duos $\hat{Y_i}$ e $\hat{Y_j}$ √© zero quando $i \neq j$. Logo, os res√≠duos s√£o n√£o correlacionados.
III. Al√©m disso, a vari√¢ncia de cada res√≠duo $\hat{Y_i}$ √© dada pelo elemento diagonal correspondente $d_{ii}$ da matriz $D$.
IV. Portanto, o vetor de res√≠duos $\hat{Y}$ √© um conjunto de vari√°veis n√£o correlacionadas, e a vari√¢ncia de cada res√≠duo √© dada pelos elementos da diagonal de $D$.
‚ñ†

### Conclus√£o
Este cap√≠tulo demonstrou como a fatora√ß√£o triangular da matriz de covari√¢ncia $\Omega$ √© fundamental para a deriva√ß√£o da proje√ß√£o linear exata. Ao conectar a matriz $H$ aos res√≠duos das proje√ß√µes sequenciais e ao interpretar os elementos diagonais da matriz $D$ como os erros de proje√ß√£o, mostramos a rela√ß√£o entre a fatora√ß√£o triangular, as proje√ß√µes lineares, e seus erros. Este entendimento √© fundamental para a atualiza√ß√£o de previs√µes em s√©ries temporais e em modelos de regress√£o, complementando os resultados vistos anteriormente.

### Refer√™ncias
[^4]: Se√ß√µes do Cap√≠tulo 4 do livro texto fornecido.
<!-- END -->
