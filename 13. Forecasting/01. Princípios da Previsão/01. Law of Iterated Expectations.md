## A Aplica√ß√£o da Lei das Expectativas Iteradas na Otimalidade da Previs√£o Condicional

### Introdu√ß√£o
Este cap√≠tulo explora os princ√≠pios de previs√£o em s√©ries temporais, aprofundando a ideia de que a **expectativa condicional** constitui o melhor preditor quando se busca minimizar o erro quadr√°tico m√©dio (MSE). Expandindo os conceitos introdut√≥rios, detalhamos a deriva√ß√£o matem√°tica que confirma essa afirma√ß√£o, utilizando a **lei das expectativas iteradas** para demonstrar que o termo cruzado na decomposi√ß√£o do MSE se anula, levando √† otimalidade da expectativa condicional.

### Conceitos Fundamentais
Como visto anteriormente, o **erro quadr√°tico m√©dio** (MSE) associado √† previs√£o $\hat{Y}_{t+1}$ √© definido como [^1]:
$$MSE(\hat{Y}_{t+1}) = E[(Y_{t+1} - \hat{Y}_{t+1})^2].$$
Para encontrar a previs√£o que minimiza o MSE, consideramos inicialmente uma fun√ß√£o arbitr√°ria $g(X_t)$ como previs√£o, denotada por $\hat{Y}_{t+1|t} = g(X_t)$ [^1]. O MSE desta previs√£o pode ser expresso como [^1]:
$$E[Y_{t+1} - g(X_t)]^2 = E[Y_{t+1} - E(Y_{t+1}|X_t) + E(Y_{t+1}|X_t) - g(X_t)]^2.$$
Expandindo esta express√£o, obtemos [^1]:
$$E[Y_{t+1} - g(X_t)]^2 = E[Y_{t+1} - E(Y_{t+1}|X_t)]^2 + 2E\{[Y_{t+1} - E(Y_{t+1}|X_t)][E(Y_{t+1}|X_t) - g(X_t)]\} + E[E(Y_{t+1}|X_t) - g(X_t)]^2.$$
O termo central do lado direito da equa√ß√£o acima pode ser escrito como $2E[\eta_{t+1}]$, onde [^1]:
$$\eta_{t+1} = [Y_{t+1} - E(Y_{t+1}|X_t)][E(Y_{t+1}|X_t) - g(X_t)].$$
Para demonstrar que esse termo se anula, condicionamos a esperan√ßa em $X_t$. Dado $X_t$, os termos $E(Y_{t+1}|X_t)$ e $g(X_t)$ s√£o constantes e podem ser fatorados da esperan√ßa condicional [^1]:
$$E[\eta_{t+1}|X_t] = [E(Y_{t+1}|X_t) - g(X_t)] \times E\{[Y_{t+1} - E(Y_{t+1}|X_t)]|X_t\}.$$
A esperan√ßa condicional do termo $[Y_{t+1} - E(Y_{t+1}|X_t)]$ dado $X_t$ √© zero, resultando em [^1]:
$$E[\eta_{t+1}|X_t] = [E(Y_{t+1}|X_t) - g(X_t)] \times 0 = 0.$$
Aplicando a **lei das expectativas iteradas**, ou seja, $E[\eta_{t+1}] = E_{X_t}[E[\eta_{t+1}|X_t]]$ [^1], obtemos [^1]:
$$E[\eta_{t+1}] = E_{X_t}(0) = 0.$$
Substituindo este resultado de volta na expans√£o do MSE, obtemos [^1]:
$$E[Y_{t+1} - g(X_t)]^2 = E[Y_{t+1} - E(Y_{t+1}|X_t)]^2 + E[E(Y_{t+1}|X_t) - g(X_t)]^2.$$
O segundo termo do lado direito da equa√ß√£o √© sempre n√£o negativo. Para minimizar o MSE, devemos escolher uma fun√ß√£o $g(X_t)$ que iguale esse termo a zero [^1], o que ocorre quando [^1]:
$$E(Y_{t+1}|X_t) = g(X_t).$$
Isso demonstra que a fun√ß√£o $g(X_t)$ que minimiza o MSE √© a **expectativa condicional** $E(Y_{t+1}|X_t)$. O MSE √≥timo √© dado por [^1]:
$$E[Y_{t+1} - E(Y_{t+1}|X_t)]^2.$$

> üí° **Exemplo Num√©rico:** Suponha que temos um modelo onde $Y_{t+1} = 0.5X_t + \epsilon_{t+1}$, onde $\epsilon_{t+1}$ √© um erro aleat√≥rio com m√©dia zero e vari√¢ncia $\sigma^2 = 1$. A expectativa condicional √© $E(Y_{t+1}|X_t) = 0.5X_t$. Se usarmos uma previs√£o $g(X_t) = 0.4X_t$, vamos calcular o MSE usando a decomposi√ß√£o:
>
> $\text{Step 1: }  E[Y_{t+1} - E(Y_{t+1}|X_t)]^2 = E[(0.5X_t + \epsilon_{t+1} - 0.5X_t)^2] = E[\epsilon_{t+1}^2] = \sigma^2 = 1$
>
> $\text{Step 2: } E[E(Y_{t+1}|X_t) - g(X_t)]^2 = E[(0.5X_t - 0.4X_t)^2] = E[(0.1X_t)^2] = 0.01E[X_t^2]$
>
> Assumindo que $E[X_t^2]=4$, temos:
>
> $\text{Step 3: } E[E(Y_{t+1}|X_t) - g(X_t)]^2 = 0.01 * 4 = 0.04$
>
> $\text{Step 4: } MSE(g(X_t)) = 1 + 0.04 = 1.04$
>
> Se tiv√©ssemos usado a previs√£o √≥tima $g(X_t) = 0.5X_t$, o MSE seria apenas o erro irredut√≠vel, que √© 1. A diferen√ßa de 0.04 √© o custo de usar uma fun√ß√£o de previs√£o sub√≥tima. Isto ilustra que usar uma fun√ß√£o diferente da expectativa condicional resulta em um MSE maior.

**Observa√ß√£o 1:** √â importante notar que o termo $E[Y_{t+1} - E(Y_{t+1}|X_t)]^2$ representa a vari√¢ncia do erro de previs√£o, tamb√©m conhecida como erro irredut√≠vel, j√° que √© a menor vari√¢ncia que pode ser obtida ao prever $Y_{t+1}$ com base em $X_t$. Este termo, por n√£o depender da escolha da fun√ß√£o $g(X_t)$, define o limite inferior do MSE que pode ser alcan√ßado.

**Teorema 1:** (Decomposi√ß√£o do MSE) Dado um preditor $\hat{Y}_{t+1} = g(X_t)$ para $Y_{t+1}$, o erro quadr√°tico m√©dio (MSE) pode ser decomposto em duas partes: a vari√¢ncia do erro de previs√£o da melhor previs√£o poss√≠vel e o erro adicional devido √† escolha de um preditor diferente da esperan√ßa condicional. Formalmente:
$$MSE(\hat{Y}_{t+1}) = E[Y_{t+1} - E(Y_{t+1}|X_t)]^2 + E[E(Y_{t+1}|X_t) - g(X_t)]^2$$
*Demonstra√ß√£o:*
I. Come√ßamos com a defini√ß√£o do MSE para um preditor arbitr√°rio $g(X_t)$:
   $$MSE(g(X_t)) = E[(Y_{t+1} - g(X_t))^2]$$
II. Adicionamos e subtra√≠mos $E(Y_{t+1}|X_t)$ dentro do quadrado:
   $$MSE(g(X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t) + E(Y_{t+1}|X_t) - g(X_t))^2]$$
III. Expandimos o quadrado:
   $$MSE(g(X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2 + 2(Y_{t+1} - E(Y_{t+1}|X_t))(E(Y_{t+1}|X_t) - g(X_t)) + (E(Y_{t+1}|X_t) - g(X_t))^2]$$
IV. Aplicamos a linearidade da esperan√ßa:
    $$MSE(g(X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] + 2E[(Y_{t+1} - E(Y_{t+1}|X_t))(E(Y_{t+1}|X_t) - g(X_t))] + E[(E(Y_{t+1}|X_t) - g(X_t))^2]$$
V. Definimos $\eta_{t+1} = [Y_{t+1} - E(Y_{t+1}|X_t)][E(Y_{t+1}|X_t) - g(X_t)]$ e mostramos que $E[\eta_{t+1}] = 0$. Condicionando em $X_t$:
    $$E[\eta_{t+1}|X_t] = [E(Y_{t+1}|X_t) - g(X_t)]E[Y_{t+1} - E(Y_{t+1}|X_t)|X_t]$$
VI. Usando a propriedade da esperan√ßa condicional, $E[Y_{t+1} - E(Y_{t+1}|X_t)|X_t] = E[Y_{t+1}|X_t] - E[E(Y_{t+1}|X_t)|X_t] = E[Y_{t+1}|X_t] - E(Y_{t+1}|X_t) = 0$.
    $$E[\eta_{t+1}|X_t] = [E(Y_{t+1}|X_t) - g(X_t)] \times 0 = 0$$
VII. Pela lei das expectativas iteradas:
    $$E[\eta_{t+1}] = E_{X_t}[E[\eta_{t+1}|X_t]] = E_{X_t}[0] = 0$$
VIII. Substitu√≠mos este resultado de volta na express√£o do MSE:
    $$MSE(g(X_t)) = E[Y_{t+1} - E(Y_{t+1}|X_t)]^2 + E[E(Y_{t+1}|X_t) - g(X_t)]^2$$
    ‚ñ†

> üí° **Exemplo Num√©rico:** Vamos usar um exemplo com dados simulados para ilustrar a decomposi√ß√£o do MSE. Suponha que temos um conjunto de dados onde $Y_{t+1} = 2X_t + \epsilon_{t+1}$ e $\epsilon_{t+1} \sim N(0, 1)$, $X_t \sim N(1, 0.5)$. Vamos calcular o MSE para dois preditores diferentes: $g_1(X_t) = 2X_t$ (o preditor √≥timo) e $g_2(X_t) = 1.5X_t$. Simulemos alguns dados para realizar este c√°lculo:
>
> ```python
> import numpy as np
>
> np.random.seed(42)
> num_samples = 1000
> X_t = np.random.normal(1, 0.5, num_samples)
> epsilon_t_plus_1 = np.random.normal(0, 1, num_samples)
> Y_t_plus_1 = 2 * X_t + epsilon_t_plus_1
>
> # Preditor √≥timo
> g1_X_t = 2 * X_t
>
> # Preditor sub-√≥timo
> g2_X_t = 1.5 * X_t
>
> # Erros
> error_g1 = Y_t_plus_1 - g1_X_t
> error_g2 = Y_t_plus_1 - g2_X_t
>
> # C√°lculo do MSE
> mse_g1 = np.mean(error_g1**2)
> mse_g2 = np.mean(error_g2**2)
>
> # Decomposi√ß√£o do MSE para g2
> expected_y_t_plus_1 = 2 * X_t
> variance_error = np.mean((Y_t_plus_1 - expected_y_t_plus_1)**2)
> bias_squared = np.mean((expected_y_t_plus_1 - g2_X_t)**2)
>
> print(f"MSE do preditor √≥timo (g1): {mse_g1:.4f}")
> print(f"MSE do preditor sub√≥timo (g2): {mse_g2:.4f}")
> print(f"Vari√¢ncia do erro (irredut√≠vel): {variance_error:.4f}")
> print(f"Erro devido a g2: {bias_squared:.4f}")
> print(f"Decomposi√ß√£o do MSE para g2: {variance_error + bias_squared:.4f}")
> ```
>
> A sa√≠da deste c√≥digo mostra:
>
> ```
> MSE do preditor √≥timo (g1): 1.0197
> MSE do preditor sub√≥timo (g2): 1.2651
> Vari√¢ncia do erro (irredut√≠vel): 1.0197
> Erro devido a g2: 0.2454
> Decomposi√ß√£o do MSE para g2: 1.2651
> ```
>
> Como podemos ver, o MSE do preditor √≥timo ($g_1$) √© muito pr√≥ximo da vari√¢ncia do erro irredut√≠vel. J√° o MSE do preditor sub√≥timo ($g_2$) √© maior e pode ser decomposto na vari√¢ncia do erro e no erro adicional devido √† escolha de um preditor diferente da esperan√ßa condicional. Este exemplo num√©rico confirma a decomposi√ß√£o do MSE e a otimalidade da esperan√ßa condicional.

**Corol√°rio 1.1:** A previs√£o condicional $E(Y_{t+1}|X_t)$ √© o √∫nico preditor (a menos de eventos de probabilidade zero) que minimiza o MSE. Ou seja, se $g(X_t) \neq E(Y_{t+1}|X_t)$, ent√£o $MSE(g(X_t)) > MSE(E(Y_{t+1}|X_t))$.
*Demonstra√ß√£o:*
I. Do Teorema 1, temos a decomposi√ß√£o do MSE:
    $$MSE(g(X_t)) = E[Y_{t+1} - E(Y_{t+1}|X_t)]^2 + E[E(Y_{t+1}|X_t) - g(X_t)]^2$$
II. O primeiro termo, $E[Y_{t+1} - E(Y_{t+1}|X_t)]^2$, √© o erro irredut√≠vel.
III. O segundo termo, $E[E(Y_{t+1}|X_t) - g(X_t)]^2$, √© um termo n√£o negativo pois √© o valor esperado de um quadrado.
IV. Este termo √© igual a zero se e somente se $E(Y_{t+1}|X_t) = g(X_t)$ quase certamente.
V. Se $g(X_t) \neq E(Y_{t+1}|X_t)$ em um conjunto de probabilidade n√£o nula, ent√£o $E[E(Y_{t+1}|X_t) - g(X_t)]^2 > 0$.
VI. Portanto, $MSE(g(X_t)) > MSE(E(Y_{t+1}|X_t))$ se $g(X_t) \neq E(Y_{t+1}|X_t)$ (a menos de eventos de probabilidade zero).
VII. Conclu√≠mos que a previs√£o condicional $E(Y_{t+1}|X_t)$ minimiza o MSE.
‚ñ†

Al√©m disso, podemos analisar como essa optimalidade se mant√©m quando expandimos a informa√ß√£o utilizada para prever $Y_{t+1}$.

**Teorema 2:** (Optimalidade da Previs√£o Condicional com Conjunto de Informa√ß√µes) Seja $\mathcal{F}_t$ um conjunto de informa√ß√µes dispon√≠veis no tempo $t$. Ent√£o, o preditor de $Y_{t+1}$ que minimiza o MSE √© dado pela esperan√ßa condicional $E(Y_{t+1}|\mathcal{F}_t)$.

*Demonstra√ß√£o:*
I. Seja $g(\mathcal{F}_t)$ um preditor qualquer de $Y_{t+1}$ utilizando o conjunto de informa√ß√µes $\mathcal{F}_t$.
II. O MSE do preditor √© dado por:
    $$MSE(g(\mathcal{F}_t)) = E[(Y_{t+1} - g(\mathcal{F}_t))^2]$$
III. Adicionando e subtraindo $E(Y_{t+1}|\mathcal{F}_t)$ dentro do quadrado:
    $$MSE(g(\mathcal{F}_t)) = E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t) + E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t))^2]$$
IV. Expandindo o quadrado:
    $$MSE(g(\mathcal{F}_t)) = E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))^2 + 2(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))(E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t)) + (E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t))^2]$$
V. Aplicando a linearidade da esperan√ßa:
    $$MSE(g(\mathcal{F}_t)) = E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))^2] + 2E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))(E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t))] + E[(E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t))^2]$$
VI. Definimos $\eta_{t+1} = [Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t)][E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t)]$ e mostramos que $E[\eta_{t+1}] = 0$. Condicionando em $\mathcal{F}_t$:
    $$E[\eta_{t+1}|\mathcal{F}_t] = [E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t)]E[Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t)|\mathcal{F}_t]$$
VII. Usando a propriedade da esperan√ßa condicional, $E[Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t)|\mathcal{F}_t] = E[Y_{t+1}|\mathcal{F}_t] - E[E(Y_{t+1}|\mathcal{F}_t)|\mathcal{F}_t] = E[Y_{t+1}|\mathcal{F}_t] - E(Y_{t+1}|\mathcal{F}_t) = 0$.
    $$E[\eta_{t+1}|\mathcal{F}_t] = [E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t)] \times 0 = 0$$
VIII. Pela lei das expectativas iteradas:
    $$E[\eta_{t+1}] = E_{\mathcal{F}_t}[E[\eta_{t+1}|\mathcal{F}_t]] = E_{\mathcal{F}_t}[0] = 0$$
IX. Substituindo este resultado de volta na express√£o do MSE:
    $$MSE(g(\mathcal{F}_t)) = E[Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t)]^2 + E[E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t)]^2$$
X. O primeiro termo, $E[Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t)]^2$, √© o erro irredut√≠vel.
XI. O segundo termo, $E[E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t)]^2$, √© n√£o negativo e igual a zero se e somente se $g(\mathcal{F}_t) = E(Y_{t+1}|\mathcal{F}_t)$ quase certamente.
XII. Portanto, a previs√£o condicional $E(Y_{t+1}|\mathcal{F}_t)$ minimiza o MSE.
‚ñ†
> üí° **Exemplo Num√©rico:** Considere que $\mathcal{F}_t$ inclui $X_t$ e $Z_t$, onde $Y_{t+1} = 0.5X_t + 0.3Z_t + \epsilon_{t+1}$. Suponha que a expectativa condicional $E(Y_{t+1}|\mathcal{F}_t) = 0.5X_t + 0.3Z_t$. Um preditor sub√≥timo poderia ser $g(\mathcal{F}_t) = 0.4X_t + 0.2Z_t$. Vamos simular os dados e calcular o MSE:
>
> ```python
> import numpy as np
>
> np.random.seed(42)
> num_samples = 1000
> X_t = np.random.normal(1, 0.5, num_samples)
> Z_t = np.random.normal(2, 1, num_samples)
> epsilon_t_plus_1 = np.random.normal(0, 1, num_samples)
> Y_t_plus_1 = 0.5 * X_t + 0.3 * Z_t + epsilon_t_plus_1
>
> # Preditor √≥timo
> g_opt = 0.5 * X_t + 0.3 * Z_t
>
> # Preditor sub-√≥timo
> g_subopt = 0.4 * X_t + 0.2 * Z_t
>
> # Erros
> error_opt = Y_t_plus_1 - g_opt
> error_subopt = Y_t_plus_1 - g_subopt
>
> # C√°lculo do MSE
> mse_opt = np.mean(error_opt**2)
> mse_subopt = np.mean(error_subopt**2)
>
> # Decomposi√ß√£o do MSE para o preditor sub-√≥timo
> variance_error = np.mean((Y_t_plus_1 - (0.5*X_t + 0.3*Z_t))**2)
> bias_squared = np.mean(((0.5*X_t + 0.3*Z_t) - (0.4*X_t + 0.2*Z_t))**2)
>
> print(f"MSE do preditor √≥timo: {mse_opt:.4f}")
> print(f"MSE do preditor sub-√≥timo: {mse_subopt:.4f}")
> print(f"Vari√¢ncia do erro (irredut√≠vel): {variance_error:.4f}")
> print(f"Erro devido ao preditor sub-√≥timo: {bias_squared:.4f}")
> print(f"Decomposi√ß√£o do MSE para o preditor sub-√≥timo: {variance_error+bias_squared:.4f}")
> ```
>
> A sa√≠da deste c√≥digo demonstra:
>
> ```
> MSE do preditor √≥timo: 1.0049
> MSE do preditor sub-√≥timo: 1.0494
> Vari√¢ncia do erro (irredut√≠vel): 1.0049
> Erro devido ao preditor sub-√≥timo: 0.0445
> Decomposi√ß√£o do MSE para o preditor sub-√≥timo: 1.0494
> ```
>
> Este exemplo ilustra que mesmo com mais informa√ß√µes, o preditor √≥timo (a esperan√ßa condicional) ainda possui o menor MSE. O preditor sub√≥timo tem um MSE maior devido a um termo adicional que surge quando se utiliza uma fun√ß√£o de previs√£o diferente da esperan√ßa condicional, confirmando o teorema 2.

### Conclus√£o
A demonstra√ß√£o apresentada neste cap√≠tulo, com o uso da **lei das expectativas iteradas**, valida formalmente que a **expectativa condicional** $E(Y_{t+1}|X_t)$ √© a previs√£o que minimiza o erro quadr√°tico m√©dio. Este resultado √© crucial para a teoria da previs√£o em s√©ries temporais, estabelecendo a base para a constru√ß√£o de modelos preditivos eficazes. A anula√ß√£o do termo cruzado na decomposi√ß√£o do MSE √© um resultado chave que justifica a otimalidade da esperan√ßa condicional como previs√£o em um contexto de minimiza√ß√£o do erro quadr√°tico m√©dio. Al√©m disso, a extens√£o para um conjunto de informa√ß√µes $\mathcal{F}_t$ refor√ßa a generalidade do resultado.

### Refer√™ncias
[^1]: Trechos do texto original fornecido.
<!-- END -->
