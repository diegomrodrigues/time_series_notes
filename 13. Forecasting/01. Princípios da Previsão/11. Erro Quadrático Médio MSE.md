## O Erro QuadrÃ¡tico MÃ©dio e a Expectativa Condicional na PrevisÃ£o de SÃ©ries Temporais

### IntroduÃ§Ã£o

Este capÃ­tulo foca na anÃ¡lise do **Erro QuadrÃ¡tico MÃ©dio (MSE)** como uma mÃ©trica fundamental para avaliaÃ§Ã£o de previsÃµes em sÃ©ries temporais e na demonstraÃ§Ã£o de que a **expectativa condicional**, $E(Y_{t+1}|X_t)$, Ã© a previsÃ£o Ã³tima sob essa mÃ©trica. Conectando com os conceitos de otimalidade e decomposiÃ§Ã£o do MSE explorados nos capÃ­tulos anteriores [^1], [^2], [^3], aprofundaremos nosso entendimento sobre a razÃ£o pela qual a **expectativa condicional** emerge como a melhor escolha para previsÃµes, consolidando a base teÃ³rica para a construÃ§Ã£o de modelos preditivos robustos. Este capÃ­tulo busca sintetizar os resultados anteriores e apresentar uma visÃ£o coesa da importÃ¢ncia do MSE e da expectativa condicional na previsÃ£o.

### O Erro QuadrÃ¡tico MÃ©dio (MSE) como MÃ©trica de AvaliaÃ§Ã£o
O **Erro QuadrÃ¡tico MÃ©dio (MSE)**, como jÃ¡ definido anteriormente [^1], Ã© a esperanÃ§a do quadrado da diferenÃ§a entre o valor real de uma variÃ¡vel $Y_{t+1}$ e sua previsÃ£o $\hat{Y}_{t+1}$. Matematicamente, o MSE Ã© expresso como:
$$MSE(\hat{Y}_{t+1}) = E[(Y_{t+1} - \hat{Y}_{t+1})^2].$$
Esta mÃ©trica pondera igualmente os erros positivos e negativos, e penaliza erros maiores de forma quadrÃ¡tica, o que faz com que o MSE seja uma mÃ©trica sensÃ­vel Ã  magnitude dos erros. Em contextos prÃ¡ticos, um MSE menor indica que o modelo de previsÃ£o estÃ¡ mais prÃ³ximo dos valores reais. A escolha do MSE como mÃ©trica de avaliaÃ§Ã£o se justifica pela sua ligaÃ§Ã£o com a variÃ¢ncia do erro de previsÃ£o e pelas suas propriedades matemÃ¡ticas, que facilitam a derivaÃ§Ã£o de resultados teÃ³ricos.

A interpretaÃ§Ã£o do MSE Ã© direta: representa o valor esperado do quadrado do erro de previsÃ£o. Um MSE igual a zero indicaria uma previsÃ£o perfeita, enquanto valores maiores indicam previsÃµes menos precisas. Dada sua definiÃ§Ã£o, o MSE Ã© sempre nÃ£o-negativo, permitindo uma comparaÃ§Ã£o direta entre diferentes modelos de previsÃ£o. AlÃ©m disso, a forma quadrÃ¡tica do MSE faz com que grandes erros tenham um impacto maior na mÃ©trica do que pequenos erros. Isso Ã© desejÃ¡vel em muitas aplicaÃ§Ãµes onde grandes erros podem ser mais prejudiciais do que erros pequenos.

> ðŸ’¡ **Exemplo NumÃ©rico:** Imagine que temos um modelo de previsÃ£o de vendas de um produto onde as vendas reais do prÃ³ximo mÃªs ($Y_{t+1}$) dependem das vendas do mÃªs atual ($X_t$). ApÃ³s rodar o modelo, obtivemos dois conjuntos de previsÃµes para diferentes abordagens, junto com os valores reais de vendas. O primeiro modelo, $\hat{Y}_{t+1}^1$, produziu os seguintes erros de previsÃ£o: -2, 3, 1, -4, 2. O segundo modelo, $\hat{Y}_{t+1}^2$, resultou em erros de previsÃ£o: -1, 2, 0.5, -2, 1.5. Vamos calcular o MSE para ambos os modelos:
>
> **Modelo 1:**
> $MSE(\hat{Y}_{t+1}^1) = \frac{(-2)^2 + 3^2 + 1^2 + (-4)^2 + 2^2}{5} = \frac{4+9+1+16+4}{5} = \frac{34}{5} = 6.8$
>
> **Modelo 2:**
> $MSE(\hat{Y}_{t+1}^2) = \frac{(-1)^2 + 2^2 + 0.5^2 + (-2)^2 + 1.5^2}{5} = \frac{1+4+0.25+4+2.25}{5} = \frac{11.5}{5} = 2.3$
>
> Nesse exemplo, o modelo 2 tem um MSE muito menor, indicando que suas previsÃµes sÃ£o, em mÃ©dia, mais prÃ³ximas dos valores reais quando comparado ao modelo 1. O modelo 1, com o erro de -4,  teve um impacto muito maior no MSE do que o modelo 2, que nÃ£o teve nenhum erro tÃ£o discrepante. Isso exemplifica como o MSE Ã© sensÃ­vel a erros maiores.
>
> ```python
> import numpy as np
>
> # Errors for model 1 and 2
> errors1 = np.array([-2, 3, 1, -4, 2])
> errors2 = np.array([-1, 2, 0.5, -2, 1.5])
>
> # Calculate MSE
> mse1 = np.mean(errors1**2)
> mse2 = np.mean(errors2**2)
>
> print(f"MSE for model 1: {mse1}")
> print(f"MSE for model 2: {mse2}")
> ```

### A Expectativa Condicional como PrevisÃ£o Ã“tima sob o MSE
Conforme demonstrado nos capÃ­tulos anteriores [^1], [^2], a **expectativa condicional** de $Y_{t+1}$ dado $X_t$, denotada por $E(Y_{t+1}|X_t)$, Ã© o preditor que minimiza o MSE. Este resultado Ã© uma consequÃªncia da decomposiÃ§Ã£o do MSE e da aplicaÃ§Ã£o da lei das expectativas iteradas. Vamos resumir aqui os argumentos principais.
Consideramos um preditor genÃ©rico $g(X_t)$ e calculamos seu MSE:
$$MSE(g(X_t)) = E[(Y_{t+1} - g(X_t))^2].$$
Adicionando e subtraindo a **expectativa condicional**, temos:
$$MSE(g(X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t) + E(Y_{t+1}|X_t) - g(X_t))^2].$$
Expandindo o quadrado e utilizando a lei da esperanÃ§a iterada, a decomposiÃ§Ã£o do MSE Ã© dada por [^1], [^2]:
$$MSE(g(X_t)) = E[Y_{t+1} - E(Y_{t+1}|X_t)]^2 + E[E(Y_{t+1}|X_t) - g(X_t)]^2.$$
O primeiro termo, $E[Y_{t+1} - E(Y_{t+1}|X_t)]^2$, representa o **erro irredutÃ­vel**, tambÃ©m conhecido como a variÃ¢ncia do erro de previsÃ£o condicional. O segundo termo, $E[E(Y_{t+1}|X_t) - g(X_t)]^2$, Ã© sempre nÃ£o negativo e representa o erro adicional resultante do uso de um preditor diferente da **expectativa condicional**. Para minimizar o MSE, devemos fazer com que o segundo termo seja zero, o que ocorre somente se $g(X_t) = E(Y_{t+1}|X_t)$ quase certamente [^1], [^2].
Assim, a **expectativa condicional** $E(Y_{t+1}|X_t)$ Ã© o preditor que minimiza o MSE, o que implica que ela representa a melhor previsÃ£o possÃ­vel sob essa mÃ©trica. Este resultado destaca a importÃ¢ncia da expectativa condicional como um pilar fundamental na construÃ§Ã£o de modelos preditivos.

> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos considerar um exemplo simplificado onde $Y_{t+1} = 3X_t + \epsilon_{t+1}$, onde $X_t$ Ã© uma variÃ¡vel aleatÃ³ria com mÃ©dia 2 e variÃ¢ncia 1, e $\epsilon_{t+1}$ Ã© um ruÃ­do branco com mÃ©dia zero e variÃ¢ncia 0.5. A expectativa condicional de $Y_{t+1}$ dado $X_t$ Ã© $E(Y_{t+1}|X_t) = 3X_t$. Vamos comparar o MSE desta previsÃ£o com uma funÃ§Ã£o linear diferente, por exemplo $g(X_t) = 2.5X_t$:
>
> 1.  **MSE da Expectativa Condicional:**
>     $MSE(3X_t) = E[(Y_{t+1} - 3X_t)^2] = E[(3X_t + \epsilon_{t+1} - 3X_t)^2] = E[\epsilon_{t+1}^2] = 0.5$.
>
> 2.  **MSE da FunÃ§Ã£o Linear:**
>     $MSE(2.5X_t) = E[(Y_{t+1} - 2.5X_t)^2] = E[(3X_t + \epsilon_{t+1} - 2.5X_t)^2] = E[(0.5X_t + \epsilon_{t+1})^2]$.
>     Expandindo, temos: $MSE(2.5X_t) = E[0.25X_t^2 + \epsilon_{t+1}^2 + X_t\epsilon_{t+1}] = 0.25E[X_t^2] + E[\epsilon_{t+1}^2]$.
>    Como $E[X_t^2] = Var(X_t) + E[X_t]^2 = 1 + 2^2 = 5$, temos que $MSE(2.5X_t) = 0.25 * 5 + 0.5 = 1.25 + 0.5 = 1.75$.
>
> Observe que o MSE do preditor $2.5X_t$ (1.75) Ã© maior do que o MSE da expectativa condicional (0.5). A diferenÃ§a no MSE Ã© devida ao termo adicional na decomposiÃ§Ã£o do MSE. Esse exemplo numÃ©rico demonstra que a expectativa condicional fornece a melhor previsÃ£o sob a mÃ©trica do MSE.
>
> ```python
> import numpy as np
>
> # Given parameters
> variance_epsilon = 0.5
> mu_x = 2
> variance_x = 1
>
> # Calculate the MSE for the optimal predictor
> mse_optimal = variance_epsilon
>
> # Calculate the mean square of x
> mean_sq_x = variance_x + mu_x**2
>
> # Calculate the MSE for the suboptimal predictor
> mse_suboptimal = 0.25 * mean_sq_x + variance_epsilon
>
> print(f"MSE for optimal predictor: {mse_optimal}")
> print(f"MSE for suboptimal predictor: {mse_suboptimal}")
> print(f"Difference in MSE: {mse_suboptimal-mse_optimal}")
> ```

**Teorema 1:** (Otimalidade da Expectativa Condicional) A **expectativa condicional** $E(Y_{t+1}|X_t)$ Ã© o Ãºnico preditor que minimiza o **erro quadrÃ¡tico mÃ©dio (MSE)**. Formalmente, se $g(X_t)$ Ã© qualquer outro preditor, entÃ£o:
$$E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] \leq E[(Y_{t+1} - g(X_t))^2]$$
*Proof:*
I.  ComeÃ§amos com a decomposiÃ§Ã£o do MSE para um preditor arbitrÃ¡rio $g(X_t)$:
  $$MSE(g(X_t)) = E[Y_{t+1} - E(Y_{t+1}|X_t)]^2 + E[E(Y_{t+1}|X_t) - g(X_t)]^2$$
II.  O primeiro termo, $E[Y_{t+1} - E(Y_{t+1}|X_t)]^2$, Ã© o MSE do preditor Ã³timo e Ã© fixo para todos os preditores que se baseiam em $X_t$.
III. O segundo termo, $E[E(Y_{t+1}|X_t) - g(X_t)]^2$, Ã© sempre nÃ£o negativo e sÃ³ Ã© igual a zero se $g(X_t) = E(Y_{t+1}|X_t)$ quase certamente.
IV. Portanto, para qualquer $g(X_t) \neq E(Y_{t+1}|X_t)$, o segundo termo serÃ¡ estritamente positivo, o que implica que $MSE(g(X_t)) > E[Y_{t+1} - E(Y_{t+1}|X_t)]^2$.
V.  ConcluÃ­mos que $E(Y_{t+1}|X_t)$ Ã© o Ãºnico preditor que minimiza o MSE. $\blacksquare$

**Lema 1.1:** (Unicidade do Preditor Ã“timo) Se dois preditores $g_1(X_t)$ e $g_2(X_t)$ minimizam o MSE, entÃ£o $g_1(X_t) = g_2(X_t)$ quase certamente.
*Proof:*
I.  Se ambos os preditores minimizam o MSE, eles devem ser iguais Ã  **expectativa condicional** $E(Y_{t+1}|X_t)$.
II. Portanto, $g_1(X_t) = E(Y_{t+1}|X_t)$ e $g_2(X_t) = E(Y_{t+1}|X_t)$ quase certamente.
III. Implica que $g_1(X_t) = g_2(X_t)$ quase certamente.  â– 

**Lema 1.2:** (DecomposiÃ§Ã£o da VariÃ¢ncia do Erro) O MSE da **expectativa condicional** Ã© igual Ã  variÃ¢ncia do erro de previsÃ£o, que Ã© o erro irredutÃ­vel:
$$MSE(E(Y_{t+1}|X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] = Var(Y_{t+1} - E(Y_{t+1}|X_t)).$$
*Proof:*
I.  ComeÃ§amos com a definiÃ§Ã£o do MSE da **expectativa condicional**:
 $$MSE(E(Y_{t+1}|X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2].$$
II.  Dado que $E[Y_{t+1} - E(Y_{t+1}|X_t)|X_t] = 0$ temos que $E[Y_{t+1} - E(Y_{t+1}|X_t)] = 0$, logo o MSE Ã© igual a variÃ¢ncia do erro de previsÃ£o condicional:
$$MSE(E(Y_{t+1}|X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] = Var(Y_{t+1} - E(Y_{t+1}|X_t)).$$
Este resultado demonstra que a variÃ¢ncia do erro de previsÃ£o condicional representa o menor erro que podemos obter ao prever $Y_{t+1}$ com base em $X_t$.  â– 

> ðŸ’¡ **Exemplo NumÃ©rico:** Considere novamente o cenÃ¡rio onde $Y_{t+1} = 3X_t + \epsilon_{t+1}$. Sabemos que $E(Y_{t+1}|X_t) = 3X_t$ e o erro de previsÃ£o Ã© $e_{t+1} = Y_{t+1} - E(Y_{t+1}|X_t) = \epsilon_{t+1}$. A variÃ¢ncia desse erro Ã© $Var(\epsilon_{t+1}) = 0.5$. Se calcularmos o MSE da expectativa condicional:
>
>  $MSE(E(Y_{t+1}|X_t)) = E[(Y_{t+1} - 3X_t)^2] = E[\epsilon_{t+1}^2] = Var(\epsilon_{t+1}) = 0.5$
>
> Isso ilustra o Lema 1.2, onde o MSE da expectativa condicional Ã© exatamente igual Ã  variÃ¢ncia do erro de previsÃ£o (o erro irredutÃ­vel), que Ã© 0.5 neste caso.
>
> ```python
> import numpy as np
>
> # Given parameters
> variance_epsilon = 0.5
>
> # Calculate MSE of conditional expectation
> mse_conditional_expectation = variance_epsilon
>
> print(f"MSE of conditional expectation: {mse_conditional_expectation}")
> ```

**Lema 1.3:** (Ortogonalidade do Erro de PrevisÃ£o) O erro de previsÃ£o associado Ã  **expectativa condicional**, definido como $e_{t+1} = Y_{t+1} - E(Y_{t+1}|X_t)$, Ã© ortogonal a qualquer funÃ§Ã£o de $X_t$, ou seja, $E[e_{t+1}h(X_t)] = 0$ para qualquer funÃ§Ã£o $h(X_t)$.
*Proof:*
I.  Usando a lei das expectativas iteradas, temos que:
   $$E[e_{t+1}h(X_t)] = E[E[e_{t+1}h(X_t)|X_t]]$$
II. Como $h(X_t)$ Ã© uma funÃ§Ã£o de $X_t$, podemos retirÃ¡-la da esperanÃ§a condicional:
  $$E[e_{t+1}h(X_t)] = E[h(X_t)E[e_{t+1}|X_t]]$$
III. A **esperanÃ§a condicional** do erro de previsÃ£o, $E[e_{t+1}|X_t] = E[Y_{t+1} - E(Y_{t+1}|X_t)|X_t] = 0$.
IV. Logo:
    $$E[e_{t+1}h(X_t)] = E[h(X_t) \times 0] = 0$$
  Isso demonstra que o erro de previsÃ£o Ã© ortogonal a qualquer funÃ§Ã£o de $X_t$.  â– 
> ðŸ’¡ **Exemplo NumÃ©rico:** Continuando com o exemplo onde $Y_{t+1} = 3X_t + \epsilon_{t+1}$ e  $e_{t+1} = \epsilon_{t+1}$. Se  $h(X_t) = X_t$, a ortogonalidade implica que $E[e_{t+1}X_t] = E[\epsilon_{t+1}X_t]=0$. Isso ocorre porque o ruÃ­do branco $\epsilon_{t+1}$ Ã© independente de $X_t$, entÃ£o a correlaÃ§Ã£o entre eles Ã© zero. Se $X_t$ tivesse alguma relaÃ§Ã£o linear com o erro, por exemplo $X_t = c\epsilon_{t+1}$,  entÃ£o a expectativa seria diferente de zero. A ortogonalidade significa que nÃ£o hÃ¡ informaÃ§Ã£o de $X_t$ que possa ser usada para prever o erro.
>
> ```python
> import numpy as np
>
> # Assuming independence between epsilon and x
> # In practice this is not true unless we generate synthetic data where this is enforced
> # We generate values that behave independently
> np.random.seed(42)
> num_samples = 1000
> X_t = np.random.normal(2,1, num_samples)
> epsilon = np.random.normal(0,np.sqrt(0.5), num_samples)
>
> # Calculate the expectation of the product
> expectation_product = np.mean(epsilon * X_t)
>
> print(f"Expectation of product between error and X_t: {expectation_product}")
> ```

**ProposiÃ§Ã£o 1:** (MSE e o Erro Absoluto MÃ©dio) Apesar da Ãªnfase no MSE, outras mÃ©tricas de erro existem. Em particular, o Erro Absoluto MÃ©dio (MAE), definido como $E[|Y_{t+1} - \hat{Y}_{t+1}|]$, Ã© uma alternativa comum. A **expectativa condicional** nÃ£o Ã© necessariamente a previsÃ£o Ã³tima sob o MAE, embora seja uma boa aproximaÃ§Ã£o em muitas aplicaÃ§Ãµes. A mediana condicional, $Med(Y_{t+1}|X_t)$, Ã© o preditor que minimiza o MAE.
*Proof:*
I.  A demonstraÃ§Ã£o da otimalidade da mediana condicional em relaÃ§Ã£o ao MAE segue uma linha semelhante Ã  da otimalidade da expectativa condicional em relaÃ§Ã£o ao MSE, utilizando a decomposiÃ§Ã£o do erro absoluto em termos da mediana condicional e a lei da esperanÃ§a iterada, sendo que o detalhamento nÃ£o serÃ¡ realizado aqui por questÃ£o de espaÃ§o.
II. A diferenÃ§a chave entre o MSE e o MAE Ã© como eles penalizam os erros. O MSE penaliza erros maiores de forma quadrÃ¡tica, enquanto o MAE penaliza todos os erros de forma linear, o que o torna menos sensÃ­vel a outliers. â– 

**ObservaÃ§Ã£o 1:**  A demonstraÃ§Ã£o da otimalidade da **expectativa condicional** se baseia na decomposiÃ§Ã£o do MSE e na aplicaÃ§Ã£o da lei das expectativas iteradas, o que garante que o preditor seja o mais preciso possÃ­vel, dado o conjunto de informaÃ§Ãµes disponÃ­vel.

**ObservaÃ§Ã£o 2:**  O MSE da **expectativa condicional** representa um limite inferior para o erro de previsÃ£o, e essa Ã© a razÃ£o pela qual ele Ã© chamado de **erro irredutÃ­vel**. NÃ£o Ã© possÃ­vel construir um preditor que, ao se basear nas informaÃ§Ãµes de $X_t$, obtenha um MSE menor.

**ObservaÃ§Ã£o 3:**  A propriedade da ortogonalidade do erro de previsÃ£o (Lema 1.3) Ã© uma caracterÃ­stica importante da **expectativa condicional** e indica que o erro de previsÃ£o nÃ£o tem relaÃ§Ã£o linear com as variÃ¡veis usadas na previsÃ£o. Isso Ã© o que faz com que a **expectativa condicional** seja uma previsÃ£o nÃ£o viesada.

**ObservaÃ§Ã£o 4:** A escolha entre MSE e MAE depende do contexto especÃ­fico. Se grandes erros sÃ£o particularmente indesejÃ¡veis, o MSE Ã© mais apropriado. Se a robustez a outliers Ã© uma prioridade, o MAE pode ser mais indicado. Ã‰ importante lembrar que a **expectativa condicional** Ã© apenas o preditor Ã³timo sob a mÃ©trica do MSE.

### ConclusÃ£o
Neste capÃ­tulo, foi estabelecido que a **expectativa condicional**, $E(Y_{t+1}|X_t)$, representa a previsÃ£o Ã³tima sob a mÃ©trica do **erro quadrÃ¡tico mÃ©dio (MSE)**. A demonstraÃ§Ã£o formal, baseada na decomposiÃ§Ã£o do MSE e na aplicaÃ§Ã£o da lei das expectativas iteradas, confirma que qualquer desvio da **expectativa condicional** leva a um aumento do MSE. Este resultado Ã© de fundamental importÃ¢ncia na teoria da previsÃ£o de sÃ©ries temporais e ressalta a importÃ¢ncia de utilizar a **expectativa condicional** como base para construÃ§Ã£o de modelos preditivos eficazes. A anÃ¡lise tambÃ©m consolida os conceitos de **erro irredutÃ­vel**, que define o limite da precisÃ£o que se pode alcanÃ§ar ao prever um processo estocÃ¡stico, e de ortogonalidade, que caracteriza a falta de relaÃ§Ã£o linear entre o erro da previsÃ£o Ã³tima e as informaÃ§Ãµes disponÃ­veis. Assim, a expectativa condicional, vista como a melhor previsÃ£o, representa um marco na modelagem estatÃ­stica para sÃ©ries temporais.

### ReferÃªncias
[^1]: Trechos do texto original fornecido.
[^2]: ReferÃªncia aos conceitos previamente estabelecidos no texto.
<!-- END -->
