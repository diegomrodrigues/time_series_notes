## Decomposi√ß√£o do Erro Quadr√°tico M√©dio e a Otimalidade da Expectativa Condicional

### Introdu√ß√£o

Este cap√≠tulo avan√ßa na discuss√£o sobre os fundamentos da previs√£o em s√©ries temporais, focando na decomposi√ß√£o do **erro quadr√°tico m√©dio (MSE)** e sua rela√ß√£o com a **expectativa condicional**. Expandindo os conceitos apresentados anteriormente [^1], exploramos como a lei das expectativas iteradas nos permite decompor o MSE, revelando que a **expectativa condicional** emerge como o preditor √≥timo ao minimizar o erro de previs√£o. Al√©m disso, analisamos como a otimalidade se mant√©m quando se expande o conjunto de informa√ß√µes usado na previs√£o.

### Conceitos Fundamentais

O **erro quadr√°tico m√©dio (MSE)**, uma m√©trica chave para avaliar a qualidade das previs√µes, √© definido como [^1]:
$$MSE(\hat{Y}_{t+1}) = E[(Y_{t+1} - \hat{Y}_{t+1})^2],$$
onde $\hat{Y}_{t+1}$ √© a previs√£o de $Y_{t+1}$. Buscando a previs√£o que minimize este erro, exploramos a possibilidade de usar uma fun√ß√£o arbitr√°ria $g(X_t)$ em vez da **expectativa condicional**  $E(Y_{t+1}|X_t)$, definindo $\hat{Y}_{t+1|t} = g(X_t)$ [^1]. Assim, o MSE desta previs√£o pode ser escrito como [^1]:
$$E[Y_{t+1} - g(X_t)]^2 = E[Y_{t+1} - E(Y_{t+1}|X_t) + E(Y_{t+1}|X_t) - g(X_t)]^2.$$
Expandindo o quadrado, obtemos [^1]:
$$E[Y_{t+1} - g(X_t)]^2 = E[Y_{t+1} - E(Y_{t+1}|X_t)]^2 + 2E\{[Y_{t+1} - E(Y_{t+1}|X_t)][E(Y_{t+1}|X_t) - g(X_t)]\} + E[E(Y_{t+1}|X_t) - g(X_t)]^2.$$
O termo central, que envolve o produto cruzado, pode ser reescrito como $2E[\eta_{t+1}]$, onde [^1]:
$$\eta_{t+1} = [Y_{t+1} - E(Y_{t+1}|X_t)][E(Y_{t+1}|X_t) - g(X_t)].$$
Condicionando a esperan√ßa em $X_t$, e notando que $E(Y_{t+1}|X_t)$ e $g(X_t)$ s√£o constantes dado $X_t$, podemos fator√°-los e obtemos [^1]:
$$E[\eta_{t+1}|X_t] = [E(Y_{t+1}|X_t) - g(X_t)] \times E\{[Y_{t+1} - E(Y_{t+1}|X_t)]|X_t\}.$$
A **esperan√ßa condicional** de $[Y_{t+1} - E(Y_{t+1}|X_t)]$ dado $X_t$ √© zero, pois $E[Y_{t+1}|X_t] - E[E(Y_{t+1}|X_t)|X_t] = 0$, resultando em [^1]:
$$E[\eta_{t+1}|X_t] = 0.$$
A **lei das expectativas iteradas** [^1], $E[\eta_{t+1}] = E_{X_t}[E[\eta_{t+1}|X_t]]$, leva-nos a [^1]:
$$E[\eta_{t+1}] = E_{X_t}(0) = 0.$$
Substituindo este resultado de volta no MSE, temos [^1]:
$$E[Y_{t+1} - g(X_t)]^2 = E[Y_{t+1} - E(Y_{t+1}|X_t)]^2 + E[E(Y_{t+1}|X_t) - g(X_t)]^2.$$
O segundo termo √© sempre n√£o negativo, e para minimizar o MSE, devemos escolher uma fun√ß√£o $g(X_t)$ que o anule [^1]. Isso ocorre quando [^1]:
$$E(Y_{t+1}|X_t) = g(X_t).$$
Assim, o preditor que minimiza o MSE √© a **expectativa condicional** $E(Y_{t+1}|X_t)$ e o MSE √≥timo √© dado por [^1]:
$$E[Y_{t+1} - E(Y_{t+1}|X_t)]^2.$$

> üí° **Exemplo Num√©rico:** Vamos considerar um cen√°rio simplificado onde temos uma vari√°vel $Y_{t+1}$ que depende de uma vari√°vel $X_t$ da seguinte forma:
> $Y_{t+1} = 2X_t + \epsilon_{t+1}$, onde $\epsilon_{t+1}$ √© um erro aleat√≥rio com m√©dia zero e vari√¢ncia $\sigma^2$.  Suponha que $X_t$ seja uma vari√°vel aleat√≥ria com m√©dia $\mu_X = 1$ e vari√¢ncia $\sigma_X^2 = 0.5$.
>
> A expectativa condicional √© dada por $E(Y_{t+1}|X_t) = 2X_t$. Vamos comparar o MSE desta previs√£o com uma fun√ß√£o arbitr√°ria, por exemplo, $g(X_t) = 1.5X_t$.
>
> 1.  **MSE da Expectativa Condicional:**
>     $MSE(E(Y_{t+1}|X_t)) = E[Y_{t+1} - E(Y_{t+1}|X_t)]^2 = E[2X_t + \epsilon_{t+1} - 2X_t]^2 = E[\epsilon_{t+1}^2] = \sigma^2$
>
> 2.  **MSE da Fun√ß√£o Arbitr√°ria:**
>     $MSE(g(X_t)) = E[Y_{t+1} - g(X_t)]^2 = E[2X_t + \epsilon_{t+1} - 1.5X_t]^2 = E[0.5X_t + \epsilon_{t+1}]^2$
>     Expandindo, obtemos:
>    $E[0.25X_t^2 + \epsilon_{t+1}^2 + X_t\epsilon_{t+1}] = 0.25E[X_t^2] + E[\epsilon_{t+1}^2] + E[X_t]E[\epsilon_{t+1}]$ (assumindo que $X_t$ e $\epsilon_{t+1}$ s√£o independentes).
>    Como $E[\epsilon_{t+1}] = 0$, o termo cruzado some, e temos:
>    $MSE(g(X_t)) = 0.25E[X_t^2] + \sigma^2$.
>
>    Como $Var(X_t) = E[X_t^2] - (E[X_t])^2$, temos que $E[X_t^2] = Var(X_t) + (E[X_t])^2 = 0.5 + 1^2 = 1.5$.
>    Portanto, $MSE(g(X_t)) = 0.25(1.5) + \sigma^2 = 0.375 + \sigma^2$.
>
>   Como $0.375 > 0$, vemos que $MSE(g(X_t)) > MSE(E(Y_{t+1}|X_t))$.  Este exemplo num√©rico mostra que o MSE da previs√£o baseada na expectativa condicional ($2X_t$) √© sempre menor do que o MSE de qualquer outra fun√ß√£o $g(X_t)$ como, por exemplo $1.5X_t$. A diferen√ßa √© um valor positivo, neste caso, 0.375, que resulta da vari√¢ncia de $X_t$ e da diferen√ßa entre o coeficiente correto e o coeficiente usado na fun√ß√£o arbitr√°ria $g(X_t)$.
>
>   Este exemplo ilustra a otimalidade da expectativa condicional na minimiza√ß√£o do MSE.
>
>   ```python
>    import numpy as np
>    # Define the variance of the error term
>    sigma_squared = 1
>
>    # Define the variance of the X variable
>    var_x = 0.5
>
>    # Calculate the MSE of the conditional expectation
>    mse_conditional_expectation = sigma_squared
>
>    # Calculate the MSE of the arbitrary function
>    mse_arbitrary_function = 0.25 * (var_x + 1) + sigma_squared
>
>    print(f"MSE of conditional expectation: {mse_conditional_expectation}")
>    print(f"MSE of arbitrary function: {mse_arbitrary_function}")
>    ```

> üí° **Demonstra√ß√£o Formal:** Seja $\mathcal{F}_t$ o conjunto de informa√ß√µes no tempo $t$, ent√£o o MSE de um preditor arbitr√°rio $g(\mathcal{F}_t)$ de $Y_{t+1}$ pode ser decomposto da seguinte forma:
> $$E[(Y_{t+1} - g(\mathcal{F}_t))^2] = E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t) + E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t))^2]$$
> Expandindo o quadrado, temos:
> $$= E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))^2 + 2(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))(E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t)) + (E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t))^2]$$
> Aplicando a linearidade da esperan√ßa:
> $$= E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))^2] + 2E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))(E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t))] + E[(E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t))^2]$$
> Condicionando na informa√ß√£o dispon√≠vel $\mathcal{F}_t$, temos que $E[Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t)|\mathcal{F}_t] = 0$, o que leva o termo cruzado a se anular pela lei das expectativas iteradas:
> $$E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))(E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t))] = E[E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))(E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t))|\mathcal{F}_t]] = 0$$
> Portanto, temos a decomposi√ß√£o do MSE:
> $$E[(Y_{t+1} - g(\mathcal{F}_t))^2] = E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))^2] + E[(E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t))^2]$$
> O segundo termo √© sempre n√£o negativo, e o MSE √© minimizado se, e somente se, $g(\mathcal{F}_t) = E(Y_{t+1}|\mathcal{F}_t)$. $\blacksquare$

Aqui est√° uma prova mais detalhada para a decomposi√ß√£o do MSE:

**Prova:**
Vamos provar que o MSE de um preditor arbitr√°rio $g(\mathcal{F}_t)$ pode ser decomposto como:
$$E[(Y_{t+1} - g(\mathcal{F}_t))^2] = E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))^2] + E[(E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t))^2]$$

I.   Come√ßamos adicionando e subtraindo $E(Y_{t+1}|\mathcal{F}_t)$ dentro do termo do MSE:
    $$E[(Y_{t+1} - g(\mathcal{F}_t))^2] = E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t) + E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t))^2]$$

II. Expandimos o quadrado:
$$E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))^2 + 2(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))(E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t)) + (E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t))^2]$$

III. Aplicando a linearidade da esperan√ßa:
$$= E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))^2] + 2E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))(E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t))] + E[(E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t))^2]$$

IV. Consideremos o termo do meio. Usando a lei da esperan√ßa iterada, podemos reescrever a esperan√ßa como:
$$E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))(E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t))] = E[E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))(E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t))|\mathcal{F}_t]]$$

V.  Dado que $E(Y_{t+1}|\mathcal{F}_t)$ e $g(\mathcal{F}_t)$ s√£o mensur√°veis com rela√ß√£o a $\mathcal{F}_t$, podemos retirar esses termos da esperan√ßa condicional:
$$E[ (E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t)) E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))|\mathcal{F}_t]]$$

VI. A esperan√ßa condicional $E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))|\mathcal{F}_t]$ √© zero por defini√ß√£o da esperan√ßa condicional:
$$E[Y_{t+1}|\mathcal{F}_t] - E[E(Y_{t+1}|\mathcal{F}_t)|\mathcal{F}_t] = E[Y_{t+1}|\mathcal{F}_t] - E[Y_{t+1}|\mathcal{F}_t] = 0$$

VII. Portanto, o termo do meio se anula:
$$E[ (E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t)) \times 0] = 0$$

VIII. Assim, obtemos a decomposi√ß√£o do MSE:
$$E[(Y_{t+1} - g(\mathcal{F}_t))^2] = E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))^2] + E[(E(Y_{t+1}|\mathcal{F}_t) - g(\mathcal{F}_t))^2]$$

IX. O segundo termo na decomposi√ß√£o √© sempre n√£o-negativo, e ele se anula se e somente se $g(\mathcal{F}_t) = E(Y_{t+1}|\mathcal{F}_t)$, o que prova que o preditor √≥timo no sentido do MSE √© $E(Y_{t+1}|\mathcal{F}_t)$. ‚ñ†

**Observa√ß√£o 2:** O termo $E[Y_{t+1} - E(Y_{t+1}|X_t)]^2$ representa a vari√¢ncia do erro de previs√£o da melhor previs√£o poss√≠vel, conhecida como **erro irredut√≠vel** [^1]. Este termo independe da escolha de $g(X_t)$ e define o limite inferior do MSE alcan√ß√°vel.

**Lema 1:** Se $Y_{t+1}$ e $X_t$ s√£o vari√°veis aleat√≥rias, e se $E[Y_{t+1}^2] < \infty$ , ent√£o $E[Y_{t+1}|X_t]$ existe e √© √∫nica (a menos de conjuntos de medida nula).
*Proof:* A exist√™ncia e unicidade da esperan√ßa condicional s√£o resultados cl√°ssicos da teoria de probabilidade, decorrentes do Teorema de Radon-Nikodym.  A condi√ß√£o $E[Y_{t+1}^2] < \infty$ garante a integrabilidade necess√°ria para definir a esperan√ßa condicional.

**Observa√ß√£o 2.1:** O Lema 1 garante que a esperan√ßa condicional, que √© o preditor √≥timo conforme demostrado, est√° bem definida sob condi√ß√µes razo√°veis. Isso refor√ßa a import√¢ncia da esperan√ßa condicional como a melhor previs√£o no sentido do MSE.

### A Otimalidade da Previs√£o Condicional com Conjunto de Informa√ß√µes

Expandindo o conceito de otimalidade, considere o caso onde a previs√£o de $Y_{t+1}$ √© feita com base em um conjunto de informa√ß√µes $\mathcal{F}_t$, que pode incluir v√°rias vari√°veis. O preditor de $Y_{t+1}$ que minimiza o MSE √© dado pela **esperan√ßa condicional** $E(Y_{t+1}|\mathcal{F}_t)$.

> üí° **Exemplo:** Suponha que $\mathcal{F}_t$ contenha informa√ß√µes sobre o valor de uma vari√°vel $X_t$ e $Z_t$, e que o verdadeiro modelo seja $Y_{t+1} = 0.5X_t + 0.3Z_t + \epsilon_{t+1}$. Ent√£o a melhor previs√£o que podemos fazer √© $E(Y_{t+1}|\mathcal{F}_t) = 0.5X_t + 0.3Z_t$. Usar qualquer outra fun√ß√£o levaria a um MSE maior.

A demonstra√ß√£o formal deste resultado segue uma l√≥gica an√°loga √†quela usada para o caso onde a informa√ß√£o √© resumida por uma √∫nica vari√°vel $X_t$. O MSE pode ser decomposto, e o termo que envolve o produto cruzado se anula pela aplica√ß√£o da lei das expectativas iteradas. Isso leva √† conclus√£o de que a **expectativa condicional** √© o preditor √≥timo dado qualquer conjunto de informa√ß√µes.

> üí° **Exemplo Num√©rico:** Vamos supor um modelo onde $Y_{t+1} = 0.7X_t + 0.4Z_t + \epsilon_{t+1}$, onde $X_t$ e $Z_t$ s√£o vari√°veis aleat√≥rias com m√©dia zero, vari√¢ncia 1 e correla√ß√£o 0.5, e $\epsilon_{t+1}$ √© um erro com m√©dia zero e vari√¢ncia 0.2, independente de $X_t$ e $Z_t$.  O conjunto de informa√ß√µes $\mathcal{F}_t$ cont√©m $X_t$ e $Z_t$.  Neste caso, $E(Y_{t+1}|\mathcal{F}_t) = 0.7X_t + 0.4Z_t$.
>
> Vamos calcular o MSE desta previs√£o.
>
> $MSE(E(Y_{t+1}|\mathcal{F}_t)) = E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))^2] = E[(0.7X_t + 0.4Z_t + \epsilon_{t+1} - (0.7X_t + 0.4Z_t))^2] = E[\epsilon_{t+1}^2] = 0.2$
>
> Agora, consideremos uma previs√£o alternativa, $g(\mathcal{F}_t) = 0.5X_t + 0.3Z_t$. O MSE desta previs√£o √©:
>
> $MSE(g(\mathcal{F}_t)) = E[(Y_{t+1} - g(\mathcal{F}_t))^2] = E[(0.7X_t + 0.4Z_t + \epsilon_{t+1} - (0.5X_t + 0.3Z_t))^2] = E[(0.2X_t + 0.1Z_t + \epsilon_{t+1})^2]$
>
> Expandindo o quadrado, temos:
>
> $E[0.04X_t^2 + 0.01Z_t^2 + 0.04X_tZ_t + 0.4X_t\epsilon_{t+1} + 0.2Z_t\epsilon_{t+1} + \epsilon_{t+1}^2]$
>
> Usando a linearidade da esperan√ßa e o fato de que a correla√ß√£o entre $X_t$ e $Z_t$ √© 0.5 e que $X_t$, $Z_t$, e $\epsilon_{t+1}$ t√™m m√©dia zero, temos:
>
> $MSE(g(\mathcal{F}_t)) = 0.04 E[X_t^2] + 0.01 E[Z_t^2] + 0.04 E[X_tZ_t] + E[\epsilon_{t+1}^2]$
>
> Como $E[X_t^2] = Var(X_t) + E[X_t]^2 = 1 + 0 = 1$ e $E[Z_t^2] = Var(Z_t) + E[Z_t]^2 = 1 + 0 = 1$ e $E[X_tZ_t] = Cov(X_t,Z_t) + E[X_t]E[Z_t] = 0.5 + 0 = 0.5$:
>
> $MSE(g(\mathcal{F}_t)) = 0.04(1) + 0.01(1) + 0.04(0.5) + 0.2 = 0.04 + 0.01 + 0.02 + 0.2 = 0.27$.
>
> Novamente, $MSE(g(\mathcal{F}_t)) = 0.27 > 0.2 = MSE(E(Y_{t+1}|\mathcal{F}_t))$, demonstrando que a expectativa condicional minimiza o MSE. A diferen√ßa entre os MSEs, 0.07, √© resultado da diferen√ßa entre os coeficientes verdadeiros e aqueles usados na fun√ß√£o arbitr√°ria $g(\mathcal{F}_t)$, assim como da covari√¢ncia entre $X_t$ e $Z_t$.
>
> ```python
> import numpy as np
>
> # Define the variance of the error term
> sigma_squared = 0.2
>
> # Define the variances of X and Z variables
> var_x = 1
> var_z = 1
>
> # Define the covariance between X and Z
> cov_xz = 0.5
>
> # Calculate the MSE of the conditional expectation
> mse_conditional_expectation = sigma_squared
>
> # Calculate the MSE of the arbitrary function
> mse_arbitrary_function = 0.04*var_x + 0.01*var_z + 0.04*cov_xz + sigma_squared
>
> print(f"MSE of conditional expectation: {mse_conditional_expectation}")
> print(f"MSE of arbitrary function: {mse_arbitrary_function}")
> ```

**Teorema 1:** (Teorema da Proje√ß√£o) Seja $\hat{Y}_{t+1} = E(Y_{t+1}|\mathcal{F}_t)$ a proje√ß√£o de $Y_{t+1}$ sobre o espa√ßo de vari√°veis aleat√≥rias mensur√°veis com respeito a $\mathcal{F}_t$. Ent√£o, para qualquer fun√ß√£o $g(\mathcal{F}_t)$, temos que $E[(Y_{t+1} - \hat{Y}_{t+1})g(\mathcal{F}_t)] = 0$. Ou seja, o erro de previs√£o $Y_{t+1} - \hat{Y}_{t+1}$ √© ortogonal a qualquer vari√°vel que √© uma fun√ß√£o das informa√ß√µes dispon√≠veis $\mathcal{F}_t$.
*Proof:* Usando a propriedade da esperan√ßa iterada, $E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))g(\mathcal{F}_t)] = E[E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))g(\mathcal{F}_t)|\mathcal{F}_t]]$. Uma vez que $g(\mathcal{F}_t)$ √© mensur√°vel com respeito a $\mathcal{F}_t$, podemos retir√°-la da esperan√ßa condicional, resultando em $E[g(\mathcal{F}_t)E[Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t)|\mathcal{F}_t]] = E[g(\mathcal{F}_t) \times 0] = 0$. Portanto, o erro de previs√£o √© ortogonal a qualquer vari√°vel que seja fun√ß√£o da informa√ß√£o dispon√≠vel.

Aqui est√° uma prova mais detalhada do Teorema 1:
**Prova do Teorema 1:**
Vamos provar que $E[(Y_{t+1} - \hat{Y}_{t+1})g(\mathcal{F}_t)] = 0$, onde $\hat{Y}_{t+1} = E(Y_{t+1}|\mathcal{F}_t)$ e $g(\mathcal{F}_t)$ √© uma fun√ß√£o mensur√°vel com rela√ß√£o a $\mathcal{F}_t$.

I. Come√ßamos substituindo $\hat{Y}_{t+1}$ por sua defini√ß√£o:
$$E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))g(\mathcal{F}_t)]$$

II. Usamos a lei da esperan√ßa iterada para reescrever a esperan√ßa:
$$= E[E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))g(\mathcal{F}_t)|\mathcal{F}_t]]$$

III. Dado que $g(\mathcal{F}_t)$ √© mensur√°vel com rela√ß√£o a $\mathcal{F}_t$, podemos retir√°-la da esperan√ßa condicional:
$$= E[g(\mathcal{F}_t)E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))|\mathcal{F}_t]]$$

IV. Pela linearidade da esperan√ßa condicional, podemos escrever:
$$= E[g(\mathcal{F}_t)(E[Y_{t+1}|\mathcal{F}_t] - E[E(Y_{t+1}|\mathcal{F}_t)|\mathcal{F}_t])]$$

V. Usando a propriedade de que $E[E(Y_{t+1}|\mathcal{F}_t)|\mathcal{F}_t] = E(Y_{t+1}|\mathcal{F}_t)$, temos:
$$= E[g(\mathcal{F}_t)(E[Y_{t+1}|\mathcal{F}_t] - E[Y_{t+1}|\mathcal{F}_t])]$$

VI. Portanto:
$$= E[g(\mathcal{F}_t) \times 0] = 0$$

VII. Isso prova que $E[(Y_{t+1} - \hat{Y}_{t+1})g(\mathcal{F}_t)] = 0$, ou seja, o erro de previs√£o √© ortogonal a qualquer fun√ß√£o da informa√ß√£o dispon√≠vel. ‚ñ†

**Teorema 1.1:** (Propriedade da Ortogonalidade do Erro) Se $Y_{t+1}$ √© um processo estoc√°stico e $\hat{Y}_{t+1} = E(Y_{t+1}|\mathcal{F}_t)$, ent√£o, o erro de previs√£o $e_{t+1} = Y_{t+1} - \hat{Y}_{t+1}$ √© n√£o correlacionado com qualquer vari√°vel em $\mathcal{F}_t$, ou seja, $Cov(e_{t+1}, X_t) = 0$ para qualquer $X_t \in \mathcal{F}_t$.
*Proof:* Dado que $Cov(e_{t+1}, X_t) = E[(e_{t+1} - E[e_{t+1}])(X_t - E[X_t])]$, e como $E[e_{t+1}] = E[Y_{t+1} - E[Y_{t+1}|\mathcal{F}_t]] = 0$, temos que $Cov(e_{t+1}, X_t) = E[e_{t+1}X_t]$. Usando o Teorema 1, temos que $E[e_{t+1}X_t] = 0$, portanto, a covari√¢ncia √© zero, o que implica que o erro de previs√£o √© n√£o correlacionado com qualquer vari√°vel em $\mathcal{F}_t$.

Aqui est√° a prova detalhada do Teorema 1.1
**Prova do Teorema 1.1:**
Vamos provar que $Cov(e_{t+1}, X_t) = 0$, onde $e_{t+1} = Y_{t+1} - \hat{Y}_{t+1}$, $\hat{Y}_{t+1} = E(Y_{t+1}|\mathcal{F}_t)$, e $X_t$ √© uma vari√°vel em $\mathcal{F}_t$.

I. Come√ßamos com a defini√ß√£o de covari√¢ncia:
$$Cov(e_{t+1}, X_t) = E[(e_{t+1} - E[e_{t+1}])(X_t - E[X_t])]$$

II. Dado que $e_{t+1} = Y_{t+1} - \hat{Y}_{t+1} = Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t)$, temos que $E[e_{t+1}] = E[Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t)] = E[Y_{t+1}] - E[E(Y_{t+1}|\mathcal{F}_t)] = E[Y_{t+1}] - E[Y_{t+1}] = 0$ usando a lei da esperan√ßa iterada. Portanto,
$$Cov(e_{t+1}, X_t) = E[e_{t+1}(X_t - E[X_t])]$$

III. Como $E[X_t]$ √© uma constante, podemos distribuir a esperan√ßa:
$$Cov(e_{t+1}, X_t) = E[e_{t+1}X_t] - E[e_{t+1}]E[X_t]$$

IV. Dado que $E[e_{t+1}] = 0$, temos que:
$$Cov(e_{t+1}, X_t) = E[e_{t+1}X_t]$$

V. Pelo Teorema 1, sabemos que $E[(Y_{t+1} - \hat{Y}_{t+1})g(\mathcal{F}_t)] = 0$ para qualquer fun√ß√£o $g(\mathcal{F}_t)$. Em particular, podemos escolher $g(\mathcal{F}_t) = X_t$:
$$E[e_{t+1}X_t] = E[(Y_{t+1} - E(Y_{t+1}|\mathcal{F}_t))X_t] = 0$$

VI. Portanto:
$$Cov(e_{t+1}, X_t) = 0$$

VII. Isso prova que o erro de previs√£o $e_{t+1}$ √© n√£o correlacionado com qualquer vari√°vel $X_t$ em $\mathcal{F}_t$. ‚ñ†

### Conclus√£o

A **decomposi√ß√£o do MSE** utilizando a **lei das expectativas iteradas** demonstra que a **expectativa condicional** √© o preditor que minimiza o **erro quadr√°tico m√©dio** [^1]. A demonstra√ß√£o apresentada, v√°lida para qualquer conjunto de informa√ß√µes $\mathcal{F}_t$, confirma que a esperan√ßa condicional n√£o s√≥ √© uma previs√£o √≥tima, mas tamb√©m define o limite inferior do erro de previs√£o alcan√ß√°vel. Este resultado √© fundamental para a constru√ß√£o de modelos preditivos robustos e eficazes, e serve como alicerce para a escolha de fun√ß√µes de previs√£o em modelos estat√≠sticos. O conceito de **erro irredut√≠vel** tamb√©m surge como um limite te√≥rico √† melhoria do MSE, e enfatiza a natureza inerentemente aleat√≥ria dos processos que buscamos prever.

### Refer√™ncias

[^1]: Trechos do texto original fornecido.
<!-- END -->
