## A Proje√ß√£o Linear como Abordagem para Previs√£o em S√©ries Temporais

### Introdu√ß√£o
Este cap√≠tulo avan√ßa no estudo das t√©cnicas de previs√£o em s√©ries temporais, focando na **proje√ß√£o linear** como uma alternativa √† **expectativa condicional** quando a rela√ß√£o entre as vari√°veis n√£o √© facilmente model√°vel ou quando se busca simplicidade computacional. Expandindo os conceitos introduzidos anteriormente [^1], exploramos como a proje√ß√£o linear, ao restringir a fun√ß√£o de previs√£o a uma forma linear, busca encontrar um compromisso entre a simplicidade do modelo e a minimiza√ß√£o do **erro quadr√°tico m√©dio (MSE)**. Detalhamos o c√°lculo do vetor de coeficientes para essa proje√ß√£o e demonstramos sua rela√ß√£o com a otimalidade da expectativa condicional em um contexto linear.

### Conceitos Fundamentais
Como vimos anteriormente, a **expectativa condicional** $E(Y_{t+1}|X_t)$ √© a previs√£o que minimiza o **MSE** [^1]. No entanto, em muitas situa√ß√µes pr√°ticas, a forma funcional da expectativa condicional pode ser desconhecida ou muito complexa para ser calculada. Em tais casos, uma aproxima√ß√£o linear, conhecida como **proje√ß√£o linear**, torna-se uma alternativa atraente. A proje√ß√£o linear busca expressar a previs√£o de $Y_{t+1}$ como uma fun√ß√£o linear de $X_t$, dada por [^2]:
$$\hat{Y}_{t+1|t} = \alpha'X_t,$$
onde $\alpha$ √© um vetor de coeficientes que desejamos determinar. O objetivo √© encontrar o vetor $\alpha$ que minimize o **MSE**, definido como [^1]:
$$MSE(\alpha) = E[(Y_{t+1} - \alpha'X_t)^2].$$
Para determinar $\alpha$, impomos a condi√ß√£o de que o erro de previs√£o, $Y_{t+1} - \alpha'X_t$, seja n√£o correlacionado com $X_t$ [^2], ou seja:
$$E[(Y_{t+1} - \alpha'X_t)X_t] = 0.$$
Esta condi√ß√£o √© equivalente a exigir que a proje√ß√£o linear seja a melhor aproxima√ß√£o linear de $Y_{t+1}$ em termos de $X_t$. Usando a linearidade da esperan√ßa, podemos reescrever a condi√ß√£o acima como:
$$E[Y_{t+1}X_t] - \alpha'E[X_tX_t'] = 0.$$
Resolvendo para $\alpha$, obtemos [^3]:
$$\alpha' = E[Y_{t+1}X_t'] [E[X_tX_t']]^{-1}.$$
Este resultado √© crucial, pois expressa o vetor de coeficientes $\alpha$ em termos dos momentos populacionais de $Y_{t+1}$ e $X_t$. √â importante notar que, ao contr√°rio da expectativa condicional, que busca minimizar o MSE considerando todas as poss√≠veis rela√ß√µes entre $Y_{t+1}$ e $X_t$, a proje√ß√£o linear se restringe a uma fun√ß√£o linear. Essa restri√ß√£o implica que a proje√ß√£o linear pode n√£o ser o preditor √≥timo no sentido do MSE, mas oferece uma solu√ß√£o de compromisso que minimiza o erro dentro da classe de preditores lineares.
> üí° **Exemplo Num√©rico:** Suponha que temos um modelo onde $Y_{t+1} = 2X_t + 3Z_t + \epsilon_{t+1}$, onde $X_t$ e $Z_t$ s√£o vari√°veis aleat√≥rias com m√©dia zero e $\epsilon_{t+1}$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$. Vamos calcular o vetor $\alpha$ na proje√ß√£o linear $\hat{Y}_{t+1} = \alpha' [X_t, Z_t]'$.
>
> Dado que $\alpha' = E[Y_{t+1}X_t'] [E[X_tX_t']]^{-1}$, e definindo $X_t^* = [X_t, Z_t]$, temos:
>
> $E[Y_{t+1}X_t^*] = E[(2X_t + 3Z_t + \epsilon_{t+1}) [X_t, Z_t]] = E[[2X_t^2 + X_t\epsilon_{t+1}, 3Z_t^2 + Z_t\epsilon_{t+1}]] = [2E[X_t^2], 3E[Z_t^2]]$ (dado que $X_t$, $Z_t$ e $\epsilon_{t+1}$ s√£o n√£o correlacionados).
>
> $E[X_t^* (X_t^*)'] = E[[X_t, Z_t]' [X_t, Z_t]] = E[[X_t^2, X_tZ_t], [Z_tX_t, Z_t^2]] = [[E[X_t^2], E[X_tZ_t]], [E[Z_tX_t], E[Z_t^2]]]$.
>
> Assumindo que $X_t$ e $Z_t$ s√£o n√£o correlacionadas e com vari√¢ncia 1, temos $E[X_tZ_t] = 0$, e $E[X_t^2] = E[Z_t^2] = 1$. Ent√£o,
>
> $E[Y_{t+1}X_t^*] = [2, 3]$ e $E[X_t^* (X_t^*)'] = [[1, 0], [0, 1]]$.
>
> Portanto, $\alpha' = [2, 3][[1, 0], [0, 1]]^{-1} = [2, 3]$.
>
> A proje√ß√£o linear de $Y_{t+1}$ √© $\hat{Y}_{t+1} = 2X_t + 3Z_t$. Note que este resultado √© o mesmo que a parte determin√≠stica da fun√ß√£o original.  Este exemplo ilustra como a proje√ß√£o linear recupera os par√¢metros da rela√ß√£o linear entre as vari√°veis em um caso particular.
>
> ```python
> import numpy as np
>
> # Define the number of samples
> num_samples = 1000
>
> # Define the parameters of the process
> mu_x = 0
> variance_x = 1
> mu_z = 0
> variance_z = 1
> sigma_squared = 1
>
> # Generate samples for the random variables
> np.random.seed(42)
> X_t = np.random.normal(mu_x, np.sqrt(variance_x), num_samples)
> Z_t = np.random.normal(mu_z, np.sqrt(variance_z), num_samples)
> epsilon_t_plus_1 = np.random.normal(0, np.sqrt(sigma_squared), num_samples)
>
> # Calculate Y_t+1 based on the model
> Y_t_plus_1 = 2 * X_t + 3 * Z_t + epsilon_t_plus_1
>
> # Create the design matrix
> X_t_matrix = np.vstack((X_t, Z_t)).T
>
> # Calculate the projection matrix using the pseudoinverse to handle singular matrix
> alpha_transpose = np.linalg.pinv(X_t_matrix.T @ X_t_matrix) @ (X_t_matrix.T @ Y_t_plus_1)
>
> # Print the parameters
> print(f"Projection parameters: {alpha_transpose}")
>
>
> # Calculate the projection matrix using the analytical formula
> # Assuming X and Z are not correlated
> E_Y_X = np.array([np.mean(Y_t_plus_1 * X_t), np.mean(Y_t_plus_1 * Z_t)])
> E_X_XT = np.array([[np.mean(X_t**2), np.mean(X_t*Z_t)], [np.mean(X_t*Z_t), np.mean(Z_t**2)]])
>
> alpha_transpose_analytical = E_Y_X @ np.linalg.inv(E_X_XT)
> print(f"Analytical Projection parameters: {alpha_transpose_analytical}")
>
>
> # Calculate predicted value and the MSE
> predicted_values = X_t_matrix @ alpha_transpose
> mse = np.mean((Y_t_plus_1 - predicted_values)**2)
> print(f"Mean squared error: {mse}")
> ```

**Teorema 1:** (Proje√ß√£o Linear e MSE) O vetor de coeficientes $\alpha$ que minimiza o MSE para a proje√ß√£o linear $\hat{Y}_{t+1|t} = \alpha'X_t$ √© dado por:
$$\alpha' = E[Y_{t+1}X_t'] [E[X_tX_t']]^{-1}.$$
*Proof:*
I.  O MSE da proje√ß√£o linear √© definido como:
 $$MSE(\alpha) = E[(Y_{t+1} - \alpha'X_t)^2]$$
II. Para encontrar o $\alpha$ que minimiza o MSE, devemos encontrar a condi√ß√£o de primeira ordem para o problema de minimiza√ß√£o do MSE.  Come√ßamos derivando o MSE em rela√ß√£o a $\alpha$.  Para isso, vamos primeiro expandir a express√£o:
$$MSE(\alpha) = E[(Y_{t+1} - \alpha'X_t)'(Y_{t+1} - \alpha'X_t)] = E[Y_{t+1}^2 - 2Y_{t+1}\alpha'X_t + \alpha'X_t X_t'\alpha]$$
Usando a linearidade da esperan√ßa, temos:
$$MSE(\alpha) = E[Y_{t+1}^2] - 2E[Y_{t+1}\alpha'X_t] + E[\alpha'X_t X_t'\alpha] = E[Y_{t+1}^2] - 2\alpha'E[Y_{t+1}X_t] + \alpha'E[X_t X_t']\alpha$$
III. Agora, derivamos o MSE em rela√ß√£o a $\alpha$:
$$\frac{\partial MSE}{\partial \alpha} = - 2E[Y_{t+1}X_t'] + 2\alpha'E[X_tX_t']$$
IV. Igualamos a derivada a zero para encontrar o ponto cr√≠tico:
$$- 2E[Y_{t+1}X_t'] + 2\alpha'E[X_tX_t'] = 0$$
V.  Resolvendo para $\alpha'$, temos:
$$\alpha'E[X_tX_t'] = E[Y_{t+1}X_t']$$
$$\alpha' = E[Y_{t+1}X_t'] [E[X_tX_t']]^{-1}$$
Esta √© a condi√ß√£o de primeira ordem para a minimiza√ß√£o do MSE, e ela nos fornece a f√≥rmula para o vetor de coeficientes $\alpha$. Para garantir que essa condi√ß√£o seja um m√≠nimo, devemos mostrar que a matriz Hessiana √© positiva definida.  A segunda derivada do MSE em rela√ß√£o a $\alpha$ √© dada por $2E[X_tX_t']$. Como essa matriz √© a matriz de vari√¢ncia-covari√¢ncia de $X_t$ e assumimos que ela √© positiva definida, o resultado √© um m√≠nimo para o MSE.  ‚ñ†

**Lema 1.1** (Condi√ß√£o de Ortogonalidade) O vetor de coeficientes $\alpha$ na proje√ß√£o linear $\hat{Y}_{t+1|t} = \alpha'X_t$ satisfaz a condi√ß√£o de que o erro de previs√£o √© n√£o correlacionado com $X_t$, ou seja, $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$.
*Proof:*
I.  Come√ßamos com a defini√ß√£o da proje√ß√£o linear:
  $$\hat{Y}_{t+1|t} = \alpha'X_t$$
II.  O erro de previs√£o √© definido como:
  $$e_{t+1} = Y_{t+1} - \alpha'X_t$$
III.  Para que $\alpha$ seja o vetor que minimiza o MSE dentro da classe dos preditores lineares, impomos que o erro de previs√£o seja ortogonal a $X_t$:
   $$E[e_{t+1}X_t'] = 0$$
IV. Substituindo a defini√ß√£o de $e_{t+1}$, temos:
   $$E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$$
V.  Usando a linearidade da esperan√ßa:
   $$E[Y_{t+1}X_t'] - \alpha'E[X_tX_t'] = 0$$
VI. Resolvendo para $\alpha'$, obtemos a mesma express√£o do Teorema 1:
    $$\alpha' = E[Y_{t+1}X_t'] [E[X_tX_t']]^{-1}$$
   Essa condi√ß√£o demonstra que a proje√ß√£o linear se baseia em uma escolha de $\alpha$ que torna o erro de previs√£o ortogonal √†s vari√°veis usadas na previs√£o. ‚ñ†

**Lema 1.2** (Decomposi√ß√£o do MSE) O MSE da proje√ß√£o linear pode ser decomposto em termos da vari√¢ncia do erro de previs√£o e da vari√¢ncia de $Y_{t+1}$. Mais especificamente, temos:
$$E[(Y_{t+1} - \hat{Y}_{t+1|t})^2] = E[Y_{t+1}^2] - E[\hat{Y}_{t+1|t}^2]$$
*Proof:*
I. Come√ßamos com a defini√ß√£o do MSE da proje√ß√£o linear e substitu√≠mos $\hat{Y}_{t+1|t}$ por $\alpha'X_t$:
$$MSE = E[(Y_{t+1} - \alpha'X_t)^2]$$
II. Expandindo o quadrado, temos:
$$MSE = E[Y_{t+1}^2 - 2Y_{t+1}\alpha'X_t + (\alpha'X_t)^2]$$
III. Usando a linearidade da esperan√ßa:
$$MSE = E[Y_{t+1}^2] - 2E[Y_{t+1}\alpha'X_t] + E[(\alpha'X_t)^2]$$
IV. Reconhecendo que $\alpha'E[Y_{t+1}X_t'] = \alpha'E[X_tX_t']\alpha$ a partir da condi√ß√£o de ortogonalidade, podemos substituir $E[Y_{t+1}\alpha'X_t]$ por $\alpha'E[X_tX_t']\alpha = E[\hat{Y}_{t+1|t}^2]$:
$$MSE = E[Y_{t+1}^2] - 2E[\hat{Y}_{t+1|t}^2] + E[\hat{Y}_{t+1|t}^2]$$
V. Simplificando, obtemos:
$$MSE = E[Y_{t+1}^2] - E[\hat{Y}_{t+1|t}^2]$$
Este resultado mostra que o MSE √© a diferen√ßa entre a vari√¢ncia de $Y_{t+1}$ e a vari√¢ncia da proje√ß√£o linear. Al√©m disso, esse resultado √© um caso particular da decomposi√ß√£o da vari√¢ncia total em vari√¢ncia explicada pelo modelo e vari√¢ncia do erro.  ‚ñ†

**Observa√ß√£o 1:** √â fundamental notar que a proje√ß√£o linear, ao impor a forma linear da previs√£o, pode n√£o capturar toda a complexidade das rela√ß√µes n√£o lineares entre $Y_{t+1}$ e $X_t$.  Em situa√ß√µes onde a rela√ß√£o verdadeira entre as vari√°veis √© n√£o linear, a **expectativa condicional** √© geralmente uma escolha melhor para previs√µes, desde que seja poss√≠vel estim√°-la.

**Observa√ß√£o 2:** Quando a rela√ß√£o entre $Y_{t+1}$ e $X_t$ √© linear, a proje√ß√£o linear e a **expectativa condicional** coincidem. Isso significa que a proje√ß√£o linear fornece a previs√£o √≥tima nesse cen√°rio espec√≠fico.

### A Proje√ß√£o Linear como um Compromisso
A proje√ß√£o linear representa uma abordagem de compromisso para a previs√£o. Ela simplifica o problema da previs√£o ao restringir o preditor a uma fun√ß√£o linear, o que pode ser vantajoso em termos computacionais e de interpretabilidade. No entanto, ao fazer essa escolha, corremos o risco de n√£o capturar toda a informa√ß√£o relevante que a **expectativa condicional** poderia utilizar. Em outras palavras, a proje√ß√£o linear oferece uma aproxima√ß√£o do preditor √≥timo dentro da classe dos preditores lineares, ao passo que a **expectativa condicional** busca o melhor preditor em um espa√ßo muito mais amplo de fun√ß√µes.

> üí° **Exemplo Num√©rico:** Consideremos o caso onde $Y_{t+1} = X_t^2 + \epsilon_{t+1}$, onde $\epsilon_{t+1}$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia 0.5, e $X_t \sim N(0,1)$. Nesse caso, a expectativa condicional √© $E(Y_{t+1}|X_t) = X_t^2$.  Se impusermos uma proje√ß√£o linear da forma $\hat{Y}_{t+1} = \alpha X_t$, o vetor $\alpha$ pode ser obtido por $\alpha = E[Y_{t+1}X_t] / E[X_t^2]$.  Como $E[Y_{t+1}X_t] = E[X_t^3 + \epsilon_{t+1}X_t] = E[X_t^3] + E[\epsilon_{t+1}X_t] = E[X_t^3] = 0$, e $E[X_t^2] = 1$, temos que $\alpha = 0$, e portanto, a proje√ß√£o linear de $Y_{t+1}$ em $X_t$ √© $\hat{Y}_{t+1} = 0$.
>
> Note que neste exemplo a proje√ß√£o linear ignora completamente a rela√ß√£o entre $Y_{t+1}$ e $X_t$. O MSE obtido com a proje√ß√£o linear ser√° $E[(Y_{t+1} - 0)^2] = E[Y_{t+1}^2] = E[(X_t^2 + \epsilon_{t+1})^2] = E[X_t^4] + 2E[X_t^2\epsilon_{t+1}] + E[\epsilon_{t+1}^2] = 3 + 0 + 0.5= 3.5$.  J√° o MSE da expectativa condicional √© $E[(Y_{t+1} - X_t^2)^2] = E[\epsilon_{t+1}^2] = 0.5$, demonstrando que o preditor linear √© muito pior do que a expectativa condicional.  Este exemplo demonstra que a proje√ß√£o linear pode levar a resultados sub√≥timos se a rela√ß√£o verdadeira entre as vari√°veis n√£o for linear.  No entanto, a proje√ß√£o linear tem a vantagem de ser mais f√°cil de obter em alguns casos.
>
>
>  ```python
> import numpy as np
>
> # Parameters
> variance_epsilon = 0.5
> num_samples = 1000
>
> # Generate random samples
> np.random.seed(42)
> X_t = np.random.normal(0, 1, num_samples)
> epsilon_t_plus_1 = np.random.normal(0, np.sqrt(variance_epsilon), num_samples)
>
> # Calculate Y_t+1
> Y_t_plus_1 = X_t**2 + epsilon_t_plus_1
>
> # Calculate the projection parameter
> alpha_hat = np.mean(Y_t_plus_1*X_t) / np.mean(X_t**2)
>
> # Obtain the linear projection
> linear_projection = alpha_hat * X_t
>
> # Obtain the conditional expectation
> conditional_expectation = X_t**2
>
> # Calculate the MSE of the linear projection
> mse_linear_projection = np.mean((Y_t_plus_1 - linear_projection)**2)
>
> # Calculate the MSE of the conditional expectation
> mse_conditional_expectation = np.mean((Y_t_plus_1 - conditional_expectation)**2)
>
> # Print the results
> print(f"Projection Parameter: {alpha_hat}")
> print(f"MSE of the linear projection: {mse_linear_projection}")
> print(f"MSE of the conditional expectation: {mse_conditional_expectation}")
> ```

### Conclus√£o
A **proje√ß√£o linear** oferece uma abordagem alternativa para a previs√£o em s√©ries temporais, que busca um vetor de coeficientes $\alpha$ que minimize o MSE dentro da classe de preditores lineares. Este m√©todo, que se baseia na condi√ß√£o de que o erro de previs√£o seja n√£o correlacionado com as vari√°veis preditoras, oferece um caminho para a constru√ß√£o de modelos preditivos mais simples em casos onde a **expectativa condicional** √© dif√≠cil de ser calculada ou quando a rela√ß√£o entre as vari√°veis √© aproximadamente linear. Embora a proje√ß√£o linear possa n√£o ser t√£o precisa quanto a **expectativa condicional** em todos os casos, ela representa uma solu√ß√£o de compromisso valiosa, combinando simplicidade e efici√™ncia na minimiza√ß√£o do **MSE**. Este cap√≠tulo demonstra a import√¢ncia de considerar diferentes abordagens de previs√£o, e como a escolha do m√©todo mais adequado deve depender tanto das propriedades estat√≠sticas dos dados quanto das necessidades pr√°ticas de modelagem.

### Refer√™ncias
[^1]: Trechos do texto original fornecido.
[^2]: Trecho [4.1.9] do texto original.
[^3]: Trecho [4.1.13] do texto original.
<!-- END -->
