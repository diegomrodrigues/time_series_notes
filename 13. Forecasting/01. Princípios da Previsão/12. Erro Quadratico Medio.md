## O Erro Quadr√°tico M√©dio e a Expectativa Condicional na Previs√£o de S√©ries Temporais

### Introdu√ß√£o

Este cap√≠tulo foca na an√°lise do **Erro Quadr√°tico M√©dio (MSE)** como uma m√©trica fundamental para avalia√ß√£o de previs√µes em s√©ries temporais e na demonstra√ß√£o de que a **expectativa condicional**, $E(Y_{t+1}|X_t)$, √© a previs√£o √≥tima sob essa m√©trica. Conectando com os conceitos de otimalidade e decomposi√ß√£o do MSE explorados nos cap√≠tulos anteriores [^1], [^2], [^3], aprofundaremos nosso entendimento sobre a raz√£o pela qual a **expectativa condicional** emerge como a melhor escolha para previs√µes, consolidando a base te√≥rica para a constru√ß√£o de modelos preditivos robustos. Este cap√≠tulo busca sintetizar os resultados anteriores e apresentar uma vis√£o coesa da import√¢ncia do MSE e da expectativa condicional na previs√£o.

### O Erro Quadr√°tico M√©dio (MSE) como M√©trica de Avalia√ß√£o
O **Erro Quadr√°tico M√©dio (MSE)**, como j√° definido anteriormente [^1], √© a esperan√ßa do quadrado da diferen√ßa entre o valor real de uma vari√°vel $Y_{t+1}$ e sua previs√£o $\hat{Y}_{t+1}$. Matematicamente, o MSE √© expresso como:
$$MSE(\hat{Y}_{t+1}) = E[(Y_{t+1} - \hat{Y}_{t+1})^2].$$
Esta m√©trica pondera igualmente os erros positivos e negativos, e penaliza erros maiores de forma quadr√°tica, o que faz com que o MSE seja uma m√©trica sens√≠vel √† magnitude dos erros. Em contextos pr√°ticos, um MSE menor indica que o modelo de previs√£o est√° mais pr√≥ximo dos valores reais. A escolha do MSE como m√©trica de avalia√ß√£o se justifica pela sua liga√ß√£o com a vari√¢ncia do erro de previs√£o e pelas suas propriedades matem√°ticas, que facilitam a deriva√ß√£o de resultados te√≥ricos.

A interpreta√ß√£o do MSE √© direta: representa o valor esperado do quadrado do erro de previs√£o. Um MSE igual a zero indicaria uma previs√£o perfeita, enquanto valores maiores indicam previs√µes menos precisas. Dada sua defini√ß√£o, o MSE √© sempre n√£o-negativo, permitindo uma compara√ß√£o direta entre diferentes modelos de previs√£o. Al√©m disso, a forma quadr√°tica do MSE faz com que grandes erros tenham um impacto maior na m√©trica do que pequenos erros. Isso √© desej√°vel em muitas aplica√ß√µes onde grandes erros podem ser mais prejudiciais do que erros pequenos.

> üí° **Exemplo Num√©rico:** Imagine que temos um modelo de previs√£o de vendas de um produto onde as vendas reais do pr√≥ximo m√™s ($Y_{t+1}$) dependem das vendas do m√™s atual ($X_t$). Ap√≥s rodar o modelo, obtivemos dois conjuntos de previs√µes para diferentes abordagens, junto com os valores reais de vendas. O primeiro modelo, $\hat{Y}_{t+1}^1$, produziu os seguintes erros de previs√£o: -2, 3, 1, -4, 2. O segundo modelo, $\hat{Y}_{t+1}^2$, resultou em erros de previs√£o: -1, 2, 0.5, -2, 1.5. Vamos calcular o MSE para ambos os modelos:
>
> **Modelo 1:**
> $MSE(\hat{Y}_{t+1}^1) = \frac{(-2)^2 + 3^2 + 1^2 + (-4)^2 + 2^2}{5} = \frac{4+9+1+16+4}{5} = \frac{34}{5} = 6.8$
>
> **Modelo 2:**
> $MSE(\hat{Y}_{t+1}^2) = \frac{(-1)^2 + 2^2 + 0.5^2 + (-2)^2 + 1.5^2}{5} = \frac{1+4+0.25+4+2.25}{5} = \frac{11.5}{5} = 2.3$
>
> Nesse exemplo, o modelo 2 tem um MSE muito menor, indicando que suas previs√µes s√£o, em m√©dia, mais pr√≥ximas dos valores reais quando comparado ao modelo 1. O modelo 1, com o erro de -4,  teve um impacto muito maior no MSE do que o modelo 2, que n√£o teve nenhum erro t√£o discrepante. Isso exemplifica como o MSE √© sens√≠vel a erros maiores.
>
> ```python
> import numpy as np
>
> # Errors for model 1 and 2
> errors1 = np.array([-2, 3, 1, -4, 2])
> errors2 = np.array([-1, 2, 0.5, -2, 1.5])
>
> # Calculate MSE
> mse1 = np.mean(errors1**2)
> mse2 = np.mean(errors2**2)
>
> print(f"MSE for model 1: {mse1}")
> print(f"MSE for model 2: {mse2}")
> ```

### A Expectativa Condicional como Previs√£o √ìtima sob o MSE
Conforme demonstrado nos cap√≠tulos anteriores [^1], [^2], a **expectativa condicional** de $Y_{t+1}$ dado $X_t$, denotada por $E(Y_{t+1}|X_t)$, √© o preditor que minimiza o MSE. Este resultado √© uma consequ√™ncia da decomposi√ß√£o do MSE e da aplica√ß√£o da lei das expectativas iteradas. Vamos resumir aqui os argumentos principais.
Consideramos um preditor gen√©rico $g(X_t)$ e calculamos seu MSE:
$$MSE(g(X_t)) = E[(Y_{t+1} - g(X_t))^2].$$
Adicionando e subtraindo a **expectativa condicional**, temos:
$$MSE(g(X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t) + E(Y_{t+1}|X_t) - g(X_t))^2].$$
Expandindo o quadrado e utilizando a lei da esperan√ßa iterada, a decomposi√ß√£o do MSE √© dada por [^1], [^2]:
$$MSE(g(X_t)) = E[Y_{t+1} - E(Y_{t+1}|X_t)]^2 + E[E(Y_{t+1}|X_t) - g(X_t)]^2.$$
O primeiro termo, $E[Y_{t+1} - E(Y_{t+1}|X_t)]^2$, representa o **erro irredut√≠vel**, tamb√©m conhecido como a vari√¢ncia do erro de previs√£o condicional. O segundo termo, $E[E(Y_{t+1}|X_t) - g(X_t)]^2$, √© sempre n√£o negativo e representa o erro adicional resultante do uso de um preditor diferente da **expectativa condicional**. Para minimizar o MSE, devemos fazer com que o segundo termo seja zero, o que ocorre somente se $g(X_t) = E(Y_{t+1}|X_t)$ quase certamente [^1], [^2].
Assim, a **expectativa condicional** $E(Y_{t+1}|X_t)$ √© o preditor que minimiza o MSE, o que implica que ela representa a melhor previs√£o poss√≠vel sob essa m√©trica. Este resultado destaca a import√¢ncia da expectativa condicional como um pilar fundamental na constru√ß√£o de modelos preditivos.

> üí° **Exemplo Num√©rico:** Vamos considerar um exemplo simplificado onde $Y_{t+1} = 3X_t + \epsilon_{t+1}$, onde $X_t$ √© uma vari√°vel aleat√≥ria com m√©dia 2 e vari√¢ncia 1, e $\epsilon_{t+1}$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia 0.5. A expectativa condicional de $Y_{t+1}$ dado $X_t$ √© $E(Y_{t+1}|X_t) = 3X_t$. Vamos comparar o MSE desta previs√£o com uma fun√ß√£o linear diferente, por exemplo $g(X_t) = 2.5X_t$:
>
> 1.  **MSE da Expectativa Condicional:**
>     $MSE(3X_t) = E[(Y_{t+1} - 3X_t)^2] = E[(3X_t + \epsilon_{t+1} - 3X_t)^2] = E[\epsilon_{t+1}^2] = 0.5$.
>
> 2.  **MSE da Fun√ß√£o Linear:**
>     $MSE(2.5X_t) = E[(Y_{t+1} - 2.5X_t)^2] = E[(3X_t + \epsilon_{t+1} - 2.5X_t)^2] = E[(0.5X_t + \epsilon_{t+1})^2]$.
>     Expandindo, temos: $MSE(2.5X_t) = E[0.25X_t^2 + \epsilon_{t+1}^2 + X_t\epsilon_{t+1}] = 0.25E[X_t^2] + E[\epsilon_{t+1}^2]$.
>    Como $E[X_t^2] = Var(X_t) + E[X_t]^2 = 1 + 2^2 = 5$, temos que $MSE(2.5X_t) = 0.25 * 5 + 0.5 = 1.25 + 0.5 = 1.75$.
>
> Observe que o MSE do preditor $2.5X_t$ (1.75) √© maior do que o MSE da expectativa condicional (0.5). A diferen√ßa no MSE √© devida ao termo adicional na decomposi√ß√£o do MSE. Esse exemplo num√©rico demonstra que a expectativa condicional fornece a melhor previs√£o sob a m√©trica do MSE.
>
> ```python
> import numpy as np
>
> # Given parameters
> variance_epsilon = 0.5
> mu_x = 2
> variance_x = 1
>
> # Calculate the MSE for the optimal predictor
> mse_optimal = variance_epsilon
>
> # Calculate the mean square of x
> mean_sq_x = variance_x + mu_x**2
>
> # Calculate the MSE for the suboptimal predictor
> mse_suboptimal = 0.25 * mean_sq_x + variance_epsilon
>
> print(f"MSE for optimal predictor: {mse_optimal}")
> print(f"MSE for suboptimal predictor: {mse_suboptimal}")
> print(f"Difference in MSE: {mse_suboptimal-mse_optimal}")
> ```

**Teorema 1:** (Otimalidade da Expectativa Condicional) A **expectativa condicional** $E(Y_{t+1}|X_t)$ √© o √∫nico preditor que minimiza o **erro quadr√°tico m√©dio (MSE)**. Formalmente, se $g(X_t)$ √© qualquer outro preditor, ent√£o:
$$E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] \leq E[(Y_{t+1} - g(X_t))^2]$$
*Proof:*
I.  Come√ßamos com a decomposi√ß√£o do MSE para um preditor arbitr√°rio $g(X_t)$:
  $$MSE(g(X_t)) = E[Y_{t+1} - E(Y_{t+1}|X_t)]^2 + E[E(Y_{t+1}|X_t) - g(X_t)]^2$$
II.  O primeiro termo, $E[Y_{t+1} - E(Y_{t+1}|X_t)]^2$, √© o MSE do preditor √≥timo e √© fixo para todos os preditores que se baseiam em $X_t$.
III. O segundo termo, $E[E(Y_{t+1}|X_t) - g(X_t)]^2$, √© sempre n√£o negativo e s√≥ √© igual a zero se $g(X_t) = E(Y_{t+1}|X_t)$ quase certamente.
IV. Portanto, para qualquer $g(X_t) \neq E(Y_{t+1}|X_t)$, o segundo termo ser√° estritamente positivo, o que implica que $MSE(g(X_t)) > E[Y_{t+1} - E(Y_{t+1}|X_t)]^2$.
V.  Conclu√≠mos que $E(Y_{t+1}|X_t)$ √© o √∫nico preditor que minimiza o MSE. $\blacksquare$

**Lema 1.1:** (Unicidade do Preditor √ìtimo) Se dois preditores $g_1(X_t)$ e $g_2(X_t)$ minimizam o MSE, ent√£o $g_1(X_t) = g_2(X_t)$ quase certamente.
*Proof:*
I.  Se ambos os preditores minimizam o MSE, eles devem ser iguais √† **expectativa condicional** $E(Y_{t+1}|X_t)$.
II. Portanto, $g_1(X_t) = E(Y_{t+1}|X_t)$ e $g_2(X_t) = E(Y_{t+1}|X_t)$ quase certamente.
III. Implica que $g_1(X_t) = g_2(X_t)$ quase certamente.  ‚ñ†

**Lema 1.2:** (Decomposi√ß√£o da Vari√¢ncia do Erro) O MSE da **expectativa condicional** √© igual √† vari√¢ncia do erro de previs√£o, que √© o erro irredut√≠vel:
$$MSE(E(Y_{t+1}|X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] = Var(Y_{t+1} - E(Y_{t+1}|X_t)).$$
*Proof:*
I.  Come√ßamos com a defini√ß√£o do MSE da **expectativa condicional**:
 $$MSE(E(Y_{t+1}|X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2].$$
II.  Dado que $E[Y_{t+1} - E(Y_{t+1}|X_t)|X_t] = 0$ temos que $E[Y_{t+1} - E(Y_{t+1}|X_t)] = 0$, logo o MSE √© igual a vari√¢ncia do erro de previs√£o condicional:
$$MSE(E(Y_{t+1}|X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] = Var(Y_{t+1} - E(Y_{t+1}|X_t)).$$
Este resultado demonstra que a vari√¢ncia do erro de previs√£o condicional representa o menor erro que podemos obter ao prever $Y_{t+1}$ com base em $X_t$.  ‚ñ†

> üí° **Exemplo Num√©rico:** Considere novamente o cen√°rio onde $Y_{t+1} = 3X_t + \epsilon_{t+1}$. Sabemos que $E(Y_{t+1}|X_t) = 3X_t$ e o erro de previs√£o √© $e_{t+1} = Y_{t+1} - E(Y_{t+1}|X_t) = \epsilon_{t+1}$. A vari√¢ncia desse erro √© $Var(\epsilon_{t+1}) = 0.5$. Se calcularmos o MSE da expectativa condicional:
>
>  $MSE(E(Y_{t+1}|X_t)) = E[(Y_{t+1} - 3X_t)^2] = E[\epsilon_{t+1}^2] = Var(\epsilon_{t+1}) = 0.5$
>
> Isso ilustra o Lema 1.2, onde o MSE da expectativa condicional √© exatamente igual √† vari√¢ncia do erro de previs√£o (o erro irredut√≠vel), que √© 0.5 neste caso.
>
> ```python
> import numpy as np
>
> # Given parameters
> variance_epsilon = 0.5
>
> # Calculate MSE of conditional expectation
> mse_conditional_expectation = variance_epsilon
>
> print(f"MSE of conditional expectation: {mse_conditional_expectation}")
> ```

**Lema 1.3:** (Ortogonalidade do Erro de Previs√£o) O erro de previs√£o associado √† **expectativa condicional**, definido como $e_{t+1} = Y_{t+1} - E(Y_{t+1}|X_t)$, √© ortogonal a qualquer fun√ß√£o de $X_t$, ou seja, $E[e_{t+1}h(X_t)] = 0$ para qualquer fun√ß√£o $h(X_t)$.
*Proof:*
I.  Usando a lei das expectativas iteradas, temos que:
   $$E[e_{t+1}h(X_t)] = E[E[e_{t+1}h(X_t)|X_t]]$$
II. Como $h(X_t)$ √© uma fun√ß√£o de $X_t$, podemos retir√°-la da esperan√ßa condicional:
  $$E[e_{t+1}h(X_t)] = E[h(X_t)E[e_{t+1}|X_t]]$$
III. A **esperan√ßa condicional** do erro de previs√£o, $E[e_{t+1}|X_t] = E[Y_{t+1} - E(Y_{t+1}|X_t)|X_t] = 0$.
IV. Logo:
    $$E[e_{t+1}h(X_t)] = E[h(X_t) \times 0] = 0$$
  Isso demonstra que o erro de previs√£o √© ortogonal a qualquer fun√ß√£o de $X_t$.  ‚ñ†
> üí° **Exemplo Num√©rico:** Continuando com o exemplo onde $Y_{t+1} = 3X_t + \epsilon_{t+1}$ e  $e_{t+1} = \epsilon_{t+1}$. Se  $h(X_t) = X_t$, a ortogonalidade implica que $E[e_{t+1}X_t] = E[\epsilon_{t+1}X_t]=0$. Isso ocorre porque o ru√≠do branco $\epsilon_{t+1}$ √© independente de $X_t$, ent√£o a correla√ß√£o entre eles √© zero. Se $X_t$ tivesse alguma rela√ß√£o linear com o erro, por exemplo $X_t = c\epsilon_{t+1}$,  ent√£o a expectativa seria diferente de zero. A ortogonalidade significa que n√£o h√° informa√ß√£o de $X_t$ que possa ser usada para prever o erro.
>
> ```python
> import numpy as np
>
> # Assuming independence between epsilon and x
> # In practice this is not true unless we generate synthetic data where this is enforced
> # We generate values that behave independently
> np.random.seed(42)
> num_samples = 1000
> X_t = np.random.normal(2,1, num_samples)
> epsilon = np.random.normal(0,np.sqrt(0.5), num_samples)
>
> # Calculate the expectation of the product
> expectation_product = np.mean(epsilon * X_t)
>
> print(f"Expectation of product between error and X_t: {expectation_product}")
> ```

**Proposi√ß√£o 1:** (MSE e o Erro Absoluto M√©dio) Apesar da √™nfase no MSE, outras m√©tricas de erro existem. Em particular, o Erro Absoluto M√©dio (MAE), definido como $E[|Y_{t+1} - \hat{Y}_{t+1}|]$, √© uma alternativa comum. A **expectativa condicional** n√£o √© necessariamente a previs√£o √≥tima sob o MAE, embora seja uma boa aproxima√ß√£o em muitas aplica√ß√µes. A mediana condicional, $Med(Y_{t+1}|X_t)$, √© o preditor que minimiza o MAE.
*Proof:*
I.  A demonstra√ß√£o da otimalidade da mediana condicional em rela√ß√£o ao MAE segue uma linha semelhante √† da otimalidade da expectativa condicional em rela√ß√£o ao MSE, utilizando a decomposi√ß√£o do erro absoluto em termos da mediana condicional e a lei da esperan√ßa iterada, sendo que o detalhamento n√£o ser√° realizado aqui por quest√£o de espa√ßo.
II. A diferen√ßa chave entre o MSE e o MAE √© como eles penalizam os erros. O MSE penaliza erros maiores de forma quadr√°tica, enquanto o MAE penaliza todos os erros de forma linear, o que o torna menos sens√≠vel a outliers. ‚ñ†

**Observa√ß√£o 1:**  A demonstra√ß√£o da otimalidade da **expectativa condicional** se baseia na decomposi√ß√£o do MSE e na aplica√ß√£o da lei das expectativas iteradas, o que garante que o preditor seja o mais preciso poss√≠vel, dado o conjunto de informa√ß√µes dispon√≠vel.

**Observa√ß√£o 2:**  O MSE da **expectativa condicional** representa um limite inferior para o erro de previs√£o, e essa √© a raz√£o pela qual ele √© chamado de **erro irredut√≠vel**. N√£o √© poss√≠vel construir um preditor que, ao se basear nas informa√ß√µes de $X_t$, obtenha um MSE menor.

**Observa√ß√£o 3:**  A propriedade da ortogonalidade do erro de previs√£o (Lema 1.3) √© uma caracter√≠stica importante da **expectativa condicional** e indica que o erro de previs√£o n√£o tem rela√ß√£o linear com as vari√°veis usadas na previs√£o. Isso √© o que faz com que a **expectativa condicional** seja uma previs√£o n√£o viesada.

**Observa√ß√£o 4:** A escolha entre MSE e MAE depende do contexto espec√≠fico. Se grandes erros s√£o particularmente indesej√°veis, o MSE √© mais apropriado. Se a robustez a outliers √© uma prioridade, o MAE pode ser mais indicado. √â importante lembrar que a **expectativa condicional** √© apenas o preditor √≥timo sob a m√©trica do MSE.

### Conclus√£o
Neste cap√≠tulo, foi estabelecido que a **expectativa condicional**, $E(Y_{t+1}|X_t)$, representa a previs√£o √≥tima sob a m√©trica do **erro quadr√°tico m√©dio (MSE)**. A demonstra√ß√£o formal, baseada na decomposi√ß√£o do MSE e na aplica√ß√£o da lei das expectativas iteradas, confirma que qualquer desvio da **expectativa condicional** leva a um aumento do MSE. Este resultado √© de fundamental import√¢ncia na teoria da previs√£o de s√©ries temporais e ressalta a import√¢ncia de utilizar a **expectativa condicional** como base para constru√ß√£o de modelos preditivos eficazes. A an√°lise tamb√©m consolida os conceitos de **erro irredut√≠vel**, que define o limite da precis√£o que se pode alcan√ßar ao prever um processo estoc√°stico, e de ortogonalidade, que caracteriza a falta de rela√ß√£o linear entre o erro da previs√£o √≥tima e as informa√ß√µes dispon√≠veis. Assim, a expectativa condicional, vista como a melhor previs√£o, representa um marco na modelagem estat√≠stica para s√©ries temporais.

### Refer√™ncias
[^1]: Trechos do texto original fornecido.
[^2]: Refer√™ncia aos conceitos previamente estabelecidos no texto.
<!-- END -->
