## A Expectativa Condicional como Previs√£o √ìtima

### Introdu√ß√£o

Este cap√≠tulo aprofunda a an√°lise da **expectativa condicional** $E(Y_{t+1}|X_t)$ como o preditor ideal para s√©ries temporais, detalhando as raz√µes pelas quais ela minimiza o **erro quadr√°tico m√©dio (MSE)**. Expandindo os conceitos previamente estabelecidos [^1], exploramos a liga√ß√£o entre a **expectativa condicional** e a minimiza√ß√£o do MSE, mostrando que, dado um conjunto de informa√ß√µes $X_t$, a expectativa condicional fornece a melhor previs√£o poss√≠vel. Este cap√≠tulo visa consolidar a compreens√£o da otimalidade da **expectativa condicional**, fornecendo uma base te√≥rica s√≥lida para a constru√ß√£o de modelos de previs√£o.

### Conceitos Fundamentais

A **expectativa condicional** $E(Y_{t+1}|X_t)$ representa o valor esperado de $Y_{t+1}$, dada a realiza√ß√£o espec√≠fica da vari√°vel $X_t$ [^1]. Em termos pr√°ticos, ela expressa a melhor estimativa de $Y_{t+1}$ baseada nas informa√ß√µes contidas em $X_t$. A qualidade de uma previs√£o √© frequentemente avaliada pelo **erro quadr√°tico m√©dio (MSE)**, que mede a m√©dia do quadrado das diferen√ßas entre os valores reais e as previs√µes [^1]:

$$MSE(\hat{Y}_{t+1}) = E[(Y_{t+1} - \hat{Y}_{t+1})^2],$$

onde $\hat{Y}_{t+1}$ √© a previs√£o de $Y_{t+1}$. Nosso objetivo √© demonstrar formalmente que a **expectativa condicional**, $E(Y_{t+1}|X_t)$, √© a previs√£o que minimiza o MSE.

### A Demonstra√ß√£o da Otimalidade da Expectativa Condicional

Para demonstrar a otimalidade da **expectativa condicional**, vamos considerar um preditor arbitr√°rio $g(X_t)$. O MSE deste preditor √© dado por:
$$MSE(g(X_t)) = E[(Y_{t+1} - g(X_t))^2].$$
Adicionando e subtraindo a **expectativa condicional** $E(Y_{t+1}|X_t)$ dentro do termo do MSE, obtemos [^1]:
$$MSE(g(X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t) + E(Y_{t+1}|X_t) - g(X_t))^2].$$
Expandindo o quadrado, temos [^1]:
$$MSE(g(X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2 + 2(Y_{t+1} - E(Y_{t+1}|X_t))(E(Y_{t+1}|X_t) - g(X_t)) + (E(Y_{t+1}|X_t) - g(X_t))^2].$$
Aplicando a linearidade da esperan√ßa, obtemos [^1]:
$$MSE(g(X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] + 2E[(Y_{t+1} - E(Y_{t+1}|X_t))(E(Y_{t+1}|X_t) - g(X_t))] + E[(E(Y_{t+1}|X_t) - g(X_t))^2].$$
O termo central pode ser reescrito como $2E[\eta_{t+1}]$, onde [^1]:
$$\eta_{t+1} = [Y_{t+1} - E(Y_{t+1}|X_t)][E(Y_{t+1}|X_t) - g(X_t)].$$
Para demonstrar que este termo se anula, condicionamos a esperan√ßa em $X_t$. Dado $X_t$, os termos $E(Y_{t+1}|X_t)$ e $g(X_t)$ s√£o constantes e podem ser fatorados da esperan√ßa condicional [^1]:
$$E[\eta_{t+1}|X_t] = [E(Y_{t+1}|X_t) - g(X_t)] \times E\{[Y_{t+1} - E(Y_{t+1}|X_t)]|X_t\}.$$
A **esperan√ßa condicional** do termo $[Y_{t+1} - E(Y_{t+1}|X_t)]$ dado $X_t$ √© zero, pois  $E[Y_{t+1}|X_t] - E[E(Y_{t+1}|X_t)|X_t] = 0$, resultando em [^1]:
$$E[\eta_{t+1}|X_t] = [E(Y_{t+1}|X_t) - g(X_t)] \times 0 = 0.$$
Aplicando a **lei das expectativas iteradas** [^1], que afirma que $E[\eta_{t+1}] = E_{X_t}[E[\eta_{t+1}|X_t]]$, obtemos [^1]:
$$E[\eta_{t+1}] = E_{X_t}(0) = 0.$$
Substituindo este resultado na equa√ß√£o do MSE, obtemos [^1]:
$$MSE(g(X_t)) = E[Y_{t+1} - E(Y_{t+1}|X_t)]^2 + E[E(Y_{t+1}|X_t) - g(X_t)]^2.$$
Esta equa√ß√£o demonstra que o MSE de qualquer preditor $g(X_t)$ pode ser decomposto em duas partes:
1.  A **vari√¢ncia do erro de previs√£o condicional**, dada por $E[Y_{t+1} - E(Y_{t+1}|X_t)]^2$. Este termo √© independente da escolha da fun√ß√£o $g(X_t)$ e representa o menor erro poss√≠vel ao prever $Y_{t+1}$ dado $X_t$.  √â tamb√©m conhecido como **erro irredut√≠vel**.
2.  Um termo n√£o-negativo, $E[E(Y_{t+1}|X_t) - g(X_t)]^2$, que quantifica o erro adicional incorrido ao usar um preditor $g(X_t)$ que difere da **expectativa condicional**.

Para minimizar o MSE, devemos escolher uma fun√ß√£o $g(X_t)$ que fa√ßa com que o termo adicional seja igual a zero, o que ocorre se, e somente se, $g(X_t) = E(Y_{t+1}|X_t)$ quase certamente [^1]. Isso demonstra formalmente que a **expectativa condicional** $E(Y_{t+1}|X_t)$ √© a previs√£o que minimiza o MSE, o qual se torna igual √† vari√¢ncia do erro de previs√£o condicional:
$$MSE(E(Y_{t+1}|X_t)) = E[Y_{t+1} - E(Y_{t+1}|X_t)]^2.$$
Este resultado confirma que a **expectativa condicional**, quando comparada a qualquer outro preditor, fornece a melhor previs√£o no sentido do MSE, fornecendo a previs√£o com o menor erro sob essa m√©trica.

> üí° **Exemplo Num√©rico:** Suponha que $Y_{t+1}$ represente o pre√ßo de uma a√ß√£o no dia seguinte, e $X_t$ represente o pre√ßo da a√ß√£o no dia atual. Assumimos que a rela√ß√£o entre esses valores possa ser aproximada por $Y_{t+1} = 0.8X_t + 5 + \epsilon_{t+1}$, onde $\epsilon_{t+1} \sim N(0, 2)$ representa um erro aleat√≥rio. A expectativa condicional, nesse caso, √© $E(Y_{t+1}|X_t) = 0.8X_t + 5$. Vamos comparar o MSE dessa previs√£o com o MSE de um preditor sub√≥timo, $g(X_t) = 0.6X_t + 6$.
>
> 1. **C√°lculo do MSE da Expectativa Condicional:**
>   *  Preditor: $\hat{Y}_{t+1} = E(Y_{t+1}|X_t) = 0.8X_t + 5$.
>   *  $MSE(0.8X_t + 5) = E[(Y_{t+1} - (0.8X_t + 5))^2] = E[(0.8X_t + 5 + \epsilon_{t+1} - 0.8X_t - 5)^2] = E[\epsilon_{t+1}^2] = 2$.
>
> 2.  **C√°lculo do MSE do Preditor Sub√≥timo:**
>    *   Preditor: $\hat{Y}_{t+1} = g(X_t) = 0.6X_t + 6$.
>    *   $MSE(0.6X_t + 6) = E[(Y_{t+1} - (0.6X_t + 6))^2] = E[(0.8X_t + 5 + \epsilon_{t+1} - 0.6X_t - 6)^2] = E[(0.2X_t - 1 + \epsilon_{t+1})^2]$
>    *   Expandindo o quadrado, obtemos: $E[(0.04X_t^2 - 0.4X_t + 1 + 2(0.2X_t - 1)\epsilon_{t+1} + \epsilon_{t+1}^2)]$.
>    *  Assumindo que $X_t$ e $\epsilon_{t+1}$ s√£o independentes e que $E[\epsilon_{t+1}] = 0$, $E[X_t \epsilon_{t+1}]=0$, $E[\epsilon_{t+1}^2]=2$ e $E[X_t] = 10$ e $E[X_t^2] = 102$ (Valores hipot√©ticos para $X_t$ que s√£o condizentes com a varia√ß√£o de pre√ßos de a√ß√µes)
>    *  $MSE(0.6X_t + 6) = 0.04 E[X_t^2] - 0.4E[X_t] + 1 + E[\epsilon_{t+1}^2] =  0.04 * 102 -0.4*10 + 1 + 2 = 4.08 - 4 + 1 + 2 = 3.08$
>
>
> Podemos verificar que o $MSE(0.8X_t + 5) = 2 < 3.08 = MSE(0.6X_t + 6)$, o que demonstra que a **expectativa condicional** fornece um MSE menor do que o preditor sub√≥timo. A diferen√ßa no MSE (1.08) representa a perda de precis√£o ao usar um preditor diferente da **expectativa condicional** e ilustra sua otimalidade.
>
> ```python
> import numpy as np
>
> # Define parameters
> variance_epsilon = 2
> num_samples = 1000
>
> # Generate sample data
> np.random.seed(42)
> X_t = np.random.normal(10, np.sqrt(2), num_samples)
> epsilon_t_plus_1 = np.random.normal(0, np.sqrt(variance_epsilon), num_samples)
> Y_t_plus_1 = 0.8 * X_t + 5 + epsilon_t_plus_1
>
> # Predictor optimal
> optimal_predictor = 0.8*X_t + 5
>
> # Predictor suboptimal
> suboptimal_predictor = 0.6*X_t + 6
>
> # Calculate errors
> error_optimal = Y_t_plus_1 - optimal_predictor
> error_suboptimal = Y_t_plus_1 - suboptimal_predictor
>
> # Calculate MSE
> mse_optimal = np.mean(error_optimal**2)
> mse_suboptimal = np.mean(error_suboptimal**2)
>
> # Print results
> print(f"MSE of the optimal predictor: {mse_optimal}")
> print(f"MSE of the suboptimal predictor: {mse_suboptimal}")
> ```

**Teorema 1:** (Otimalidade da Expectativa Condicional) A **expectativa condicional** $E(Y_{t+1}|X_t)$ √© o √∫nico preditor que minimiza o **erro quadr√°tico m√©dio (MSE)**. Mais formalmente, se $g(X_t)$ √© qualquer outro preditor, ent√£o:
$$E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] \le E[(Y_{t+1} - g(X_t))^2].$$
*Proof:*
I.   Come√ßamos com a decomposi√ß√£o do MSE de um preditor arbitr√°rio $g(X_t)$:
    $$MSE(g(X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] + E[(E(Y_{t+1}|X_t) - g(X_t))^2]$$
II.  O primeiro termo, $E[(Y_{t+1} - E(Y_{t+1}|X_t))^2]$, √© o MSE do preditor √≥timo, que √© a **expectativa condicional**.
III. O segundo termo, $E[(E(Y_{t+1}|X_t) - g(X_t))^2]$, √© sempre n√£o negativo, dado que √© o valor esperado de um quadrado.
IV. Este segundo termo √© zero se e somente se $g(X_t) = E(Y_{t+1}|X_t)$ (a menos de conjuntos de probabilidade zero).
V. Portanto, para qualquer preditor $g(X_t)$ diferente da **expectativa condicional**, o segundo termo ser√° estritamente positivo, o que implica que $MSE(g(X_t)) > E[(Y_{t+1} - E(Y_{t+1}|X_t))^2]$, ou seja, a **expectativa condicional** minimiza o MSE.
VI. Conclu√≠mos que a **expectativa condicional** $E(Y_{t+1}|X_t)$ √© o √∫nico preditor que minimiza o MSE. $\blacksquare$

**Lema 1.1:** (Unicidade da Expectativa Condicional) Se dois preditores $g_1(X_t)$ e $g_2(X_t)$ minimizam o MSE, ent√£o $g_1(X_t) = g_2(X_t)$ quase certamente.
*Proof:*
I.  Se ambos os preditores minimizam o MSE, eles devem ser iguais √† **expectativa condicional** $E(Y_{t+1}|X_t)$.
II.  Portanto, $g_1(X_t) = E(Y_{t+1}|X_t)$ e $g_2(X_t) = E(Y_{t+1}|X_t)$ quase certamente.
III. Implica que $g_1(X_t) = g_2(X_t)$ quase certamente. $\blacksquare$

**Lema 1.2:** (Propriedade da Ortogonalidade do Erro) O erro de previs√£o da **expectativa condicional**, definido como $e_{t+1} = Y_{t+1} - E(Y_{t+1}|X_t)$, √© ortogonal a qualquer fun√ß√£o de $X_t$, isto √©, $E[e_{t+1}h(X_t)] = 0$ para qualquer fun√ß√£o $h(X_t)$.
*Proof:*
I.  Usando a lei da esperan√ßa iterada, temos:
    $$E[e_{t+1}h(X_t)] = E[E[e_{t+1}h(X_t)|X_t]]$$
II.  Dado que $h(X_t)$ √© uma fun√ß√£o de $X_t$, ela pode ser retirada da esperan√ßa condicional:
    $$E[e_{t+1}h(X_t)] = E[h(X_t)E[e_{t+1}|X_t]]$$
III. Sabemos que a esperan√ßa condicional do erro de previs√£o √© zero, ou seja, $E[e_{t+1}|X_t] = E[Y_{t+1} - E(Y_{t+1}|X_t)|X_t] = E[Y_{t+1}|X_t] - E[Y_{t+1}|X_t] = 0$
IV. Logo, temos:
    $$E[e_{t+1}h(X_t)] = E[h(X_t) \times 0] = 0$$
  Portanto, o erro de previs√£o √© ortogonal a qualquer fun√ß√£o de $X_t$. $\blacksquare$

**Lema 1.3:** (Decomposi√ß√£o da Vari√¢ncia) A vari√¢ncia de $Y_{t+1}$ pode ser decomposta na vari√¢ncia da expectativa condicional mais a vari√¢ncia do erro de previs√£o condicional:
$$Var(Y_{t+1}) = Var(E(Y_{t+1}|X_t)) + E[Var(Y_{t+1}|X_t)].$$
*Proof:*
I. Come√ßamos com a defini√ß√£o da vari√¢ncia de $Y_{t+1}$:
  $$Var(Y_{t+1}) = E[(Y_{t+1} - E[Y_{t+1}])^2].$$
II. Adicionando e subtraindo $E[Y_{t+1}|X_t]$ dentro do termo do quadrado:
$$Var(Y_{t+1}) = E[(Y_{t+1} - E[Y_{t+1}|X_t] + E[Y_{t+1}|X_t] - E[Y_{t+1}])^2].$$
III. Expandindo o quadrado:
$$Var(Y_{t+1}) = E[(Y_{t+1} - E[Y_{t+1}|X_t])^2 + 2(Y_{t+1} - E[Y_{t+1}|X_t])(E[Y_{t+1}|X_t] - E[Y_{t+1}]) + (E[Y_{t+1}|X_t] - E[Y_{t+1}])^2].$$
IV. Aplicando a linearidade da esperan√ßa:
$$Var(Y_{t+1}) = E[(Y_{t+1} - E[Y_{t+1}|X_t])^2] + 2E[(Y_{t+1} - E[Y_{t+1}|X_t])(E[Y_{t+1}|X_t] - E[Y_{t+1}])] + E[(E[Y_{t+1}|X_t] - E[Y_{t+1}])^2].$$
V. O termo do meio, $2E[(Y_{t+1} - E[Y_{t+1}|X_t])(E[Y_{t+1}|X_t] - E[Y_{t+1}])]$, √© zero devido √† ortogonalidade do erro de previs√£o. Isso pode ser demonstrado usando a lei da esperan√ßa iterada:
$$E[(Y_{t+1} - E[Y_{t+1}|X_t])(E[Y_{t+1}|X_t] - E[Y_{t+1}])] = E[E[(Y_{t+1} - E[Y_{t+1}|X_t])(E[Y_{t+1}|X_t] - E[Y_{t+1}])|X_t]] = E[(E[Y_{t+1}|X_t]-E[Y_{t+1}])E[Y_{t+1} - E[Y_{t+1}|X_t]|X_t]] = E[(E[Y_{t+1}|X_t]-E[Y_{t+1}]) * 0] = 0$$
VI. O primeiro termo, $E[(Y_{t+1} - E[Y_{t+1}|X_t])^2]$, √© a vari√¢ncia condicional de $Y_{t+1}$ dado $X_t$, ou seja, $E[Var(Y_{t+1}|X_t)]$. O terceiro termo √© a vari√¢ncia da expectativa condicional, $Var(E[Y_{t+1}|X_t])$.
VII. Portanto:
    $$Var(Y_{t+1}) = E[Var(Y_{t+1}|X_t)] + Var(E[Y_{t+1}|X_t]).$$
Assim, a vari√¢ncia de $Y_{t+1}$ pode ser decomposta na vari√¢ncia da expectativa condicional e na vari√¢ncia do erro condicional. $\blacksquare$

> üí° **Exemplo Num√©rico da Decomposi√ß√£o da Vari√¢ncia:** Utilizando os dados do exemplo anterior, com $Y_{t+1} = 0.8X_t + 5 + \epsilon_{t+1}$, onde $\epsilon_{t+1} \sim N(0, 2)$, podemos calcular a decomposi√ß√£o da vari√¢ncia. Sabemos que $Var(Y_{t+1}) = Var(0.8X_t + 5 + \epsilon_{t+1})$. Como $X_t$ e $\epsilon_{t+1}$ s√£o independentes, temos:
>
>  $Var(Y_{t+1}) = Var(0.8X_t) + Var(\epsilon_{t+1})$.
>
>  Assumindo $Var(X_t) = 2$, $Var(0.8X_t) = (0.8)^2Var(X_t) = 0.64 * 2 = 1.28$ e  $Var(\epsilon_{t+1}) = 2$.
>
>  Portanto, $Var(Y_{t+1}) = 1.28 + 2 = 3.28$.
>
>  Agora, analisando a decomposi√ß√£o te√≥rica:
>
>  $Var(E(Y_{t+1}|X_t)) = Var(0.8X_t + 5) = Var(0.8X_t) = 1.28$
>
>  $E[Var(Y_{t+1}|X_t)] = E[Var(0.8X_t + 5 + \epsilon_{t+1}|X_t)] = E[Var(\epsilon_{t+1}|X_t)] = E[Var(\epsilon_{t+1})] = 2$.
>
>  Com isso, $Var(Y_{t+1}) = Var(E(Y_{t+1}|X_t)) + E[Var(Y_{t+1}|X_t)] = 1.28 + 2 = 3.28$.
>
>  Este exemplo num√©rico demonstra a decomposi√ß√£o da vari√¢ncia em seus componentes te√≥ricos, mostrando que a vari√¢ncia total de $Y_{t+1}$ pode ser entendida como a soma da vari√¢ncia explicada por $X_t$ e a vari√¢ncia do erro irredut√≠vel.

**Observa√ß√£o 1:** A demonstra√ß√£o da otimalidade da **expectativa condicional** se baseia na decomposi√ß√£o do MSE e na aplica√ß√£o da lei das expectativas iteradas, o que garante que o preditor seja o mais preciso poss√≠vel, dado o conjunto de informa√ß√µes dispon√≠vel.

**Observa√ß√£o 2:** A propriedade da ortogonalidade do erro de previs√£o (Lema 1.2) implica que o erro associado √† **expectativa condicional** n√£o tem rela√ß√£o linear com as vari√°veis usadas na previs√£o.

**Observa√ß√£o 3:** A decomposi√ß√£o da vari√¢ncia (Lema 1.3) fornece uma maneira √∫til de analisar a contribui√ß√£o da informa√ß√£o $X_t$ para reduzir a incerteza sobre $Y_{t+1}$. A vari√¢ncia da expectativa condicional $Var(E(Y_{t+1}|X_t))$ representa a por√ß√£o da vari√¢ncia de $Y_{t+1}$ que pode ser explicada por $X_t$, enquanto a esperan√ßa da vari√¢ncia condicional $E[Var(Y_{t+1}|X_t)]$ representa a incerteza inerente que n√£o pode ser eliminada por $X_t$.

### Conclus√£o

Este cap√≠tulo demonstrou formalmente que a **expectativa condicional** $E(Y_{t+1}|X_t)$ representa a melhor estimativa de $Y_{t+1}$ com base em $X_t$, no sentido de que ela minimiza o **erro quadr√°tico m√©dio (MSE)**. A decomposi√ß√£o do MSE e a aplica√ß√£o da **lei das expectativas iteradas** confirmam que qualquer desvio da **expectativa condicional** leva a um aumento do MSE. Este resultado √© crucial para a teoria da previs√£o em s√©ries temporais, justificando a import√¢ncia da **expectativa condicional** como um pilar fundamental na constru√ß√£o de modelos preditivos eficazes. O conceito de **erro irredut√≠vel**, tamb√©m enfatizado, destaca que, mesmo com o preditor √≥timo, ainda h√° uma incerteza inerente ao processo que buscamos prever, e refor√ßa que a meta da previs√£o n√£o √© eliminar o erro, mas sim, minimiz√°-lo.

### Refer√™ncias

[^1]: Trechos do texto original fornecido.
<!-- END -->
