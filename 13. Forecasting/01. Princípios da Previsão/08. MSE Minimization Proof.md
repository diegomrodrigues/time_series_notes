## A Idealidade da Previs√£o: Expectativa Condicional e Minimiza√ß√£o do MSE

### Introdu√ß√£o
Este cap√≠tulo dedica-se √† an√°lise da idealidade da **expectativa condicional** $E(Y_{t+1}|X_t)$ como o preditor √≥timo, sob a m√©trica do **erro quadr√°tico m√©dio (MSE)**, para s√©ries temporais. Aprofundando os conceitos previamente estabelecidos [^1], [^2], [^3], focamos na demonstra√ß√£o de que qualquer fun√ß√£o de previs√£o $g(X_t)$ diferente da **expectativa condicional** resultar√° em um MSE maior. A explora√ß√£o deste princ√≠pio √© fundamental para consolidar a **expectativa condicional** como a base te√≥rica para a constru√ß√£o de previs√µes √≥timas, oferecendo um entendimento mais profundo da minimiza√ß√£o do MSE.

### Metodologia para Avaliar a Idealidade da Previs√£o
Para verificar a idealidade da **expectativa condicional** como preditor √≥timo, vamos comparar o MSE obtido ao usar a **expectativa condicional**, $E(Y_{t+1}|X_t)$, com o MSE obtido usando qualquer outra fun√ß√£o de previs√£o $g(X_t)$. O **erro quadr√°tico m√©dio (MSE)**, como j√° definido [^1], √© dado por:
$$MSE(\hat{Y}_{t+1}) = E[(Y_{t+1} - \hat{Y}_{t+1})^2].$$
Onde $\hat{Y}_{t+1}$ representa a previs√£o de $Y_{t+1}$. Vamos analisar o MSE quando $\hat{Y}_{t+1} = g(X_t)$, isto √©:
$$MSE(g(X_t)) = E[(Y_{t+1} - g(X_t))^2].$$
Para demonstrar que a **expectativa condicional** √© o preditor √≥timo, precisamos mostrar que o MSE √© minimizado exclusivamente quando $g(X_t) = E(Y_{t+1}|X_t)$.

### An√°lise do MSE com uma Fun√ß√£o de Previs√£o Arbitr√°ria
Para comparar a performance da **expectativa condicional** com uma fun√ß√£o de previs√£o arbitr√°ria, $g(X_t)$, analisamos a diferen√ßa do MSE entre esses dois preditores. Inicialmente, expandimos o termo MSE para a previs√£o $g(X_t)$ de forma a introduzir a **expectativa condicional**:
$$MSE(g(X_t)) = E[(Y_{t+1} - g(X_t))^2].$$
Adicionamos e subtra√≠mos a **expectativa condicional** dentro do termo ao quadrado:
$$MSE(g(X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t) + E(Y_{t+1}|X_t) - g(X_t))^2].$$
Expandindo o quadrado, obtemos:
$$MSE(g(X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2 + 2(Y_{t+1} - E(Y_{t+1}|X_t))(E(Y_{t+1}|X_t) - g(X_t)) + (E(Y_{t+1}|X_t) - g(X_t))^2].$$
Aplicando a linearidade da esperan√ßa, temos:
$$MSE(g(X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] + 2E[(Y_{t+1} - E(Y_{t+1}|X_t))(E(Y_{t+1}|X_t) - g(X_t))] + E[(E(Y_{t+1}|X_t) - g(X_t))^2].$$
O termo do meio pode ser reescrito como $2E[\eta_{t+1}]$, onde:
$$\eta_{t+1} = [Y_{t+1} - E(Y_{t+1}|X_t)][E(Y_{t+1}|X_t) - g(X_t)].$$
Para verificar que este termo √© nulo, vamos condicionar a esperan√ßa em $X_t$. Dado $X_t$, os termos $E(Y_{t+1}|X_t)$ e $g(X_t)$ s√£o constantes, e podem ser fatorados da esperan√ßa condicional:
$$E[\eta_{t+1}|X_t] = [E(Y_{t+1}|X_t) - g(X_t)] \times E\{[Y_{t+1} - E(Y_{t+1}|X_t)]|X_t\}.$$
Por defini√ß√£o, a esperan√ßa condicional do termo $[Y_{t+1} - E(Y_{t+1}|X_t)]$ dado $X_t$ √© zero, ou seja:
$$E\{[Y_{t+1} - E(Y_{t+1}|X_t)]|X_t\} = 0.$$
Portanto:
$$E[\eta_{t+1}|X_t] = [E(Y_{t+1}|X_t) - g(X_t)] \times 0 = 0.$$
Aplicando a **lei das expectativas iteradas**, ou seja, $E[\eta_{t+1}] = E_{X_t}[E[\eta_{t+1}|X_t]]$, conclu√≠mos que [^1]:
$$E[\eta_{t+1}] = E_{X_t}(0) = 0.$$
Com isso, a express√£o do MSE se simplifica para [^1]:
$$MSE(g(X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] + E[(E(Y_{t+1}|X_t) - g(X_t))^2].$$
Essa decomposi√ß√£o mostra que o MSE de qualquer preditor $g(X_t)$ √© a soma da vari√¢ncia do erro de previs√£o condicional, $E[(Y_{t+1} - E(Y_{t+1}|X_t))^2]$, e um termo n√£o-negativo, $E[(E(Y_{t+1}|X_t) - g(X_t))^2]$. O primeiro termo representa o erro irredut√≠vel associado √† melhor previs√£o poss√≠vel e n√£o depende da escolha de $g(X_t)$. O segundo termo representa o erro adicional decorrente do uso de um preditor diferente da **expectativa condicional**.

> üí° **Exemplo Num√©rico:** Considere um cen√°rio em que $Y_{t+1}$ representa o pre√ßo de uma a√ß√£o no dia $t+1$, e $X_t$ representa o pre√ßo da mesma a√ß√£o no dia $t$. Assuma que a rela√ß√£o entre $Y_{t+1}$ e $X_t$ seja dada por $Y_{t+1} = 0.8X_t + \epsilon_{t+1}$, onde $\epsilon_{t+1}$ √© um choque aleat√≥rio com m√©dia zero e desvio padr√£o 0.2 (ou seja, $\epsilon_{t+1} \sim N(0, 0.2^2)$). Portanto, a **expectativa condicional** √© $E(Y_{t+1}|X_t) = 0.8X_t$. Vamos comparar o MSE usando a **expectativa condicional** com um preditor alternativo, $g(X_t) = 0.6X_t$.
>
> *   **MSE com a Expectativa Condicional ($E(Y_{t+1}|X_t) = 0.8X_t$):**
>
>     O erro de previs√£o √© $e_{t+1} = Y_{t+1} - E(Y_{t+1}|X_t) = 0.8X_t + \epsilon_{t+1} - 0.8X_t = \epsilon_{t+1}$.
>     $$MSE(0.8X_t) = E[(Y_{t+1} - 0.8X_t)^2] = E[\epsilon_{t+1}^2] = Var(\epsilon_{t+1}) = 0.2^2 = 0.04$$
>
> *   **MSE com a fun√ß√£o arbitr√°ria ($g(X_t) = 0.6X_t$):**
>
>     O erro de previs√£o √© $e_{t+1} = Y_{t+1} - 0.6X_t = 0.8X_t + \epsilon_{t+1} - 0.6X_t = 0.2X_t + \epsilon_{t+1}$.
>     $$MSE(0.6X_t) = E[(Y_{t+1} - 0.6X_t)^2] = E[(0.2X_t + \epsilon_{t+1})^2] = E[0.04X_t^2 + 0.4X_t\epsilon_{t+1} + \epsilon_{t+1}^2].$$
>     Assumindo que $X_t$ e $\epsilon_{t+1}$ s√£o independentes e $E[X_t] = 5$ e $Var(X_t) = 1$, ent√£o $E[X_t^2] = Var(X_t) + E[X_t]^2 = 1 + 5^2 = 26$.  Al√©m disso, $E[X_t\epsilon_{t+1}] = E[X_t]E[\epsilon_{t+1}] = 5 \cdot 0 = 0$. Assim:
>      $$MSE(0.6X_t) = 0.04 \times E[X_t^2] +  E[\epsilon_{t+1}^2] = 0.04 \times 26 + 0.04 = 1.04 + 0.04 = 1.08$$
>
> Como podemos ver, o MSE obtido com a **expectativa condicional** (0.04) √© menor do que o MSE com a fun√ß√£o arbitr√°ria (1.08).
>
> ```python
> import numpy as np
>
> # Parameters
> variance_epsilon = 0.2**2
> mean_x = 5
> variance_x = 1
>
> # Calculate MSE with conditional expectation
> mse_conditional = variance_epsilon
>
> # Calculate E[X^2]
> ex2 = variance_x + mean_x**2
>
> # Calculate MSE with the alternative predictor
> mse_suboptimal = 0.04*ex2 + variance_epsilon
>
> print(f"MSE with conditional expectation: {mse_conditional}")
> print(f"MSE with suboptimal predictor: {mse_suboptimal}")
> ```
>
> ```mermaid
> graph LR
>     A[X_t] --> B(E(Y_{t+1}|X_t) = 0.8X_t)
>     A --> C(g(X_t) = 0.6X_t)
>     B --> D[MSE = 0.04]
>     C --> E[MSE = 1.08]
>     D --> F{Menor MSE}
>     E --> F
> ```

### Demonstra√ß√£o da Idealidade da Expectativa Condicional
A decomposi√ß√£o do MSE demonstra que o segundo termo, $E[(E(Y_{t+1}|X_t) - g(X_t))^2]$, √© sempre n√£o-negativo, e √© igual a zero se, e somente se,  $g(X_t) = E(Y_{t+1}|X_t)$ quase certamente. Isso prova que a **expectativa condicional**, $E(Y_{t+1}|X_t)$, √© o √∫nico preditor (a menos de um conjunto de probabilidade nula) que minimiza o MSE. O valor m√≠nimo do MSE √© dado pela vari√¢ncia do erro de previs√£o condicional [^1], ou seja, o erro irredut√≠vel. Isso demonstra formalmente que, ao usar a **expectativa condicional** como preditor, obtemos o menor MSE poss√≠vel dado o conjunto de informa√ß√µes $X_t$.

**Teorema 1:** (Idealidade da Expectativa Condicional) A **expectativa condicional** $E(Y_{t+1}|X_t)$ √© o √∫nico preditor (a menos de conjuntos de probabilidade zero) que minimiza o **erro quadr√°tico m√©dio (MSE)**.
*Proof:*
I.  Come√ßamos com a decomposi√ß√£o do MSE para um preditor arbitr√°rio $g(X_t)$:
  $$MSE(g(X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] + E[(E(Y_{t+1}|X_t) - g(X_t))^2]$$
II.  O primeiro termo, $E[(Y_{t+1} - E(Y_{t+1}|X_t))^2]$, √© o MSE do preditor √≥timo e √© fixo para todos os preditores que se baseiam em $X_t$.
III. O segundo termo, $E[(E(Y_{t+1}|X_t) - g(X_t))^2]$, √© sempre n√£o negativo e s√≥ √© igual a zero se $g(X_t) = E(Y_{t+1}|X_t)$ quase certamente.
IV. Portanto, para qualquer $g(X_t) \neq E(Y_{t+1}|X_t)$, o segundo termo ser√° estritamente positivo, o que implica que $MSE(g(X_t)) > E[(Y_{t+1} - E(Y_{t+1}|X_t))^2]$.
V. Conclu√≠mos que $E(Y_{t+1}|X_t)$ √© o √∫nico preditor que minimiza o MSE.  $\blacksquare$

**Lema 1.1:** (Unicidade do Preditor √ìtimo) Se dois preditores, $g_1(X_t)$ e $g_2(X_t)$, minimizam o MSE, ent√£o $g_1(X_t) = g_2(X_t)$ quase certamente.
*Proof:*
I.  Se ambos os preditores minimizam o MSE, eles devem ser iguais √† **expectativa condicional** $E(Y_{t+1}|X_t)$.
II.  Portanto, $g_1(X_t) = E(Y_{t+1}|X_t)$ e $g_2(X_t) = E(Y_{t+1}|X_t)$ quase certamente.
III. Isso implica que $g_1(X_t) = g_2(X_t)$ quase certamente.  $\blacksquare$

**Lema 1.2:** (MSE da Expectativa Condicional) O **erro quadr√°tico m√©dio (MSE)** quando se utiliza a **expectativa condicional** como preditor, √© igual √† vari√¢ncia do erro de previs√£o condicional:
$$MSE(E(Y_{t+1}|X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2].$$
*Proof:*
I.  Por defini√ß√£o, o MSE de qualquer preditor $h(X_t)$ √© dado por $MSE(h(X_t))= E[(Y_{t+1} - h(X_t))^2]$.
II.  Substituindo $h(X_t)$ pela **expectativa condicional** $E(Y_{t+1}|X_t)$, temos:
$$MSE(E(Y_{t+1}|X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2]$$
III. Pela defini√ß√£o de vari√¢ncia, temos que $Var(Y_{t+1}|X_t) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2|X_t]$. Ao tomar a esperan√ßa em rela√ß√£o a $X_t$, temos que
$MSE(E(Y_{t+1}|X_t)) = E[Var(Y_{t+1}|X_t)] = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2]$.   $\blacksquare$
Este resultado enfatiza que, ao usar a **expectativa condicional**, o MSE atinge seu m√≠nimo, correspondendo √† vari√¢ncia do erro de previs√£o condicional, o que √© o limite inferior da precis√£o da previs√£o.

**Lema 1.3:** (Ortogonalidade do Erro de Previs√£o) O erro de previs√£o da **expectativa condicional**, definido como $e_{t+1} = Y_{t+1} - E(Y_{t+1}|X_t)$, √© ortogonal a qualquer fun√ß√£o de $X_t$. Em outras palavras, $E[e_{t+1}h(X_t)] = 0$ para qualquer fun√ß√£o $h(X_t)$.
*Proof:*
I.  Pela lei da esperan√ßa iterada, temos:
    $$E[e_{t+1}h(X_t)] = E[E[e_{t+1}h(X_t)|X_t]]$$
II. Como $h(X_t)$ √© uma fun√ß√£o de $X_t$, √© constante quando condicionada em $X_t$. Portanto:
   $$E[e_{t+1}h(X_t)] = E[h(X_t)E[e_{t+1}|X_t]]$$
III. Sabemos que a esperan√ßa condicional do erro de previs√£o $E[e_{t+1}|X_t] = 0$. Logo:
   $$E[e_{t+1}h(X_t)] = E[h(X_t) \times 0] = E[0] = 0$$
Este resultado demonstra que o erro de previs√£o obtido usando a **expectativa condicional** n√£o tem rela√ß√£o linear com qualquer fun√ß√£o das vari√°veis usadas para prever.   $\blacksquare$

**Corol√°rio 1.1:** (N√£o-correla√ß√£o do Erro de Previs√£o com o Preditor) O erro de previs√£o da **expectativa condicional**, $e_{t+1} = Y_{t+1} - E(Y_{t+1}|X_t)$, n√£o √© correlacionado com o pr√≥prio preditor, ou seja, $E[e_{t+1}E(Y_{t+1}|X_t)] = 0$.
*Proof:*
I.  Este corol√°rio √© um caso especial do Lema 1.3, onde $h(X_t)$ √© substitu√≠do por $E(Y_{t+1}|X_t)$.
II.  Assim, temos $E[e_{t+1}E(Y_{t+1}|X_t)] = 0$.   $\blacksquare$

**Proposi√ß√£o 1:** (Decomposi√ß√£o da Vari√¢ncia de $Y_{t+1}$) A vari√¢ncia de $Y_{t+1}$ pode ser decomposta como a soma da vari√¢ncia da expectativa condicional e a vari√¢ncia do erro de previs√£o condicional:
$$Var(Y_{t+1}) = Var(E(Y_{t+1}|X_t)) + E[Var(Y_{t+1}|X_t)].$$
*Proof:*
I. Come√ßamos com a identidade: $Y_{t+1} = E(Y_{t+1}|X_t) + (Y_{t+1} - E(Y_{t+1}|X_t)) = E(Y_{t+1}|X_t) + e_{t+1}$, onde $e_{t+1}$ √© o erro de previs√£o.
II. Tomando a vari√¢ncia de ambos os lados, e utilizando o fato de que $E[e_{t+1}E(Y_{t+1}|X_t)] = 0$ (Corol√°rio 1.1), temos:
    $$Var(Y_{t+1}) = Var(E(Y_{t+1}|X_t) + e_{t+1}) = Var(E(Y_{t+1}|X_t)) + Var(e_{t+1}) + 2Cov(E(Y_{t+1}|X_t), e_{t+1}).$$
III. Como a covari√¢ncia √© zero devido ao Corol√°rio 1.1, temos:
   $$Var(Y_{t+1}) = Var(E(Y_{t+1}|X_t)) + Var(e_{t+1}).$$
IV. Observando que $Var(e_{t+1}) = E[e_{t+1}^2] = E[E[e_{t+1}^2|X_t]] = E[Var(Y_{t+1}|X_t)]$, conclu√≠mos que:
  $$Var(Y_{t+1}) = Var(E(Y_{t+1}|X_t)) + E[Var(Y_{t+1}|X_t)].$$  $\blacksquare$

### Implica√ß√µes e Interpreta√ß√µes Pr√°ticas
O resultado fundamental deste cap√≠tulo √© que a **expectativa condicional**, $E(Y_{t+1}|X_t)$, √© o preditor ideal sob a m√©trica do MSE. Isso significa que, ao usar a **expectativa condicional**, obtemos o melhor compromisso entre precis√£o e simplicidade. Esta conclus√£o n√£o significa que sempre teremos uma previs√£o perfeita, uma vez que existe uma vari√¢ncia associada ao erro, e essa √© justamente a vari√¢ncia do erro de previs√£o condicional, que √© o MSE m√≠nimo poss√≠vel.
Em cen√°rios onde a **expectativa condicional** n√£o √© facilmente calcul√°vel, podemos recorrer a aproxima√ß√µes, tais como a proje√ß√£o linear [^3]. No entanto, a proje√ß√£o linear representa um compromisso, pois n√£o necessariamente captura toda a complexidade da rela√ß√£o entre $Y_{t+1}$ e $X_t$, mas √© a melhor aproxima√ß√£o linear poss√≠vel, ou seja, o melhor preditor linear sob a mesma m√©trica. A escolha do m√©todo de previs√£o deve, portanto, levar em conta a natureza dos dados e os objetivos espec√≠ficos da modelagem.

### Conclus√£o
Este cap√≠tulo refor√ßou que o **erro quadr√°tico m√©dio (MSE)** √© uma m√©trica fundamental para a avalia√ß√£o de previs√µes e que a **expectativa condicional**, $E(Y_{t+1}|X_t)$, √© a previs√£o que minimiza o MSE. Atrav√©s da decomposi√ß√£o do MSE e da aplica√ß√£o da lei das expectativas iteradas, demonstramos que nenhum outro preditor pode superar a **expectativa condicional** em termos de MSE. Al√©m disso, mostramos que o MSE m√≠nimo corresponde √† vari√¢ncia do erro de previs√£o condicional, e que o erro de previs√£o √© ortogonal a qualquer fun√ß√£o de $X_t$. Essa an√°lise consolidou nosso entendimento sobre a idealidade da expectativa condicional, fornecendo uma base s√≥lida para o desenvolvimento de modelos de previs√£o precisos e robustos.

### Refer√™ncias
[^1]: Trechos do texto original fornecido.
[^2]: Refer√™ncia aos conceitos previamente estabelecidos no texto.
[^3]: Trecho sobre proje√ß√£o linear do texto original fornecido.
<!-- END -->
