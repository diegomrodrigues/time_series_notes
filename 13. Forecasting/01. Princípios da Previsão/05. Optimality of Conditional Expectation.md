## Demonstra√ß√£o da Otimalidade da Expectativa Condicional via Decomposi√ß√£o do MSE e Lei das Expectativas Iteradas

### Introdu√ß√£o
Este cap√≠tulo visa apresentar uma demonstra√ß√£o detalhada da otimalidade da **expectativa condicional** como o melhor preditor no contexto da minimiza√ß√£o do **erro quadr√°tico m√©dio (MSE)**. Fundamentando-se em conceitos previamente estabelecidos [^1], exploraremos como a **decomposi√ß√£o do MSE**, combinada com a aplica√ß√£o da **lei das expectativas iteradas**, nos permite demonstrar formalmente que qualquer fun√ß√£o de previs√£o que difira da **expectativa condicional** resultar√° em um MSE maior. O foco aqui ser√° em derivar rigorosamente este resultado, enfatizando a import√¢ncia da **expectativa condicional** na teoria da previs√£o.

### Conceitos Fundamentais

A **expectativa condicional**, $E(Y_{t+1}|X_t)$, representa o valor esperado de $Y_{t+1}$ dado o conhecimento de $X_t$. √â uma ferramenta crucial na previs√£o de s√©ries temporais, pois busca capturar a rela√ß√£o entre $Y_{t+1}$ e as informa√ß√µes dispon√≠veis no tempo $t$. O **erro quadr√°tico m√©dio (MSE)**, definido como [^1]:
$$MSE(\hat{Y}_{t+1}) = E[(Y_{t+1} - \hat{Y}_{t+1})^2],$$
√© uma m√©trica comum para avaliar a qualidade de um preditor $\hat{Y}_{t+1}$. Nosso objetivo √© demonstrar que a **expectativa condicional** √© o preditor que minimiza este MSE. Para isso, vamos considerar um preditor gen√©rico $g(X_t)$ e comparar seu MSE com o MSE da **expectativa condicional**.

### Decomposi√ß√£o do MSE e Aplica√ß√£o da Lei das Expectativas Iteradas
Come√ßamos expressando o MSE de um preditor gen√©rico $g(X_t)$ como [^1]:
$$E[Y_{t+1} - g(X_t)]^2.$$
Adicionando e subtraindo a **expectativa condicional** $E(Y_{t+1}|X_t)$, obtemos [^1]:
$$E[Y_{t+1} - g(X_t)]^2 = E[Y_{t+1} - E(Y_{t+1}|X_t) + E(Y_{t+1}|X_t) - g(X_t)]^2.$$
Expandindo esta express√£o, temos [^1]:
$$E[Y_{t+1} - g(X_t)]^2 = E[Y_{t+1} - E(Y_{t+1}|X_t)]^2 + 2E\{[Y_{t+1} - E(Y_{t+1}|X_t)][E(Y_{t+1}|X_t) - g(X_t)]\} + E[E(Y_{t+1}|X_t) - g(X_t)]^2.$$
O termo central, que envolve o produto cruzado, pode ser escrito como $2E[\eta_{t+1}]$, onde [^1]:
$$\eta_{t+1} = [Y_{t+1} - E(Y_{t+1}|X_t)][E(Y_{t+1}|X_t) - g(X_t)].$$
Para demonstrar que este termo se anula, condicionamos a esperan√ßa em $X_t$. Dado $X_t$, os termos $E(Y_{t+1}|X_t)$ e $g(X_t)$ s√£o constantes e podem ser fatorados da esperan√ßa condicional [^1]:
$$E[\eta_{t+1}|X_t] = [E(Y_{t+1}|X_t) - g(X_t)] \times E\{[Y_{t+1} - E(Y_{t+1}|X_t)]|X_t\}.$$
A **esperan√ßa condicional** do termo $[Y_{t+1} - E(Y_{t+1}|X_t)]$ dado $X_t$ √© zero, pois por defini√ß√£o da esperan√ßa condicional,  $E[Y_{t+1}|X_t] - E[E(Y_{t+1}|X_t)|X_t] = E[Y_{t+1}|X_t] - E[Y_{t+1}|X_t] = 0$, resultando em [^1]:
$$E[\eta_{t+1}|X_t] = [E(Y_{t+1}|X_t) - g(X_t)] \times 0 = 0.$$
Aplicando a **lei das expectativas iteradas**, ou seja, $E[\eta_{t+1}] = E_{X_t}[E[\eta_{t+1}|X_t]]$ [^1], obtemos [^1]:
$$E[\eta_{t+1}] = E_{X_t}(0) = 0.$$
Substituindo este resultado de volta na expans√£o do MSE, obtemos [^1]:
$$E[Y_{t+1} - g(X_t)]^2 = E[Y_{t+1} - E(Y_{t+1}|X_t)]^2 + E[E(Y_{t+1}|X_t) - g(X_t)]^2.$$
Esta express√£o √© crucial. Ela mostra que o MSE de qualquer preditor $g(X_t)$ √© igual √† soma de dois termos: a **vari√¢ncia do erro de previs√£o condicional** $E[Y_{t+1} - E(Y_{t+1}|X_t)]^2$, que √© independente de $g(X_t)$, e um termo n√£o negativo $E[E(Y_{t+1}|X_t) - g(X_t)]^2$. O primeiro termo representa a vari√¢ncia do erro de previs√£o da melhor previs√£o poss√≠vel e √© tamb√©m conhecido como **erro irredut√≠vel**[^1]. Este erro √© um limite inerente √† capacidade de prever $Y_{t+1}$ e n√£o pode ser reduzido com nenhuma escolha de $g(X_t)$. O segundo termo √© sempre n√£o-negativo e representa o custo adicional em MSE resultante da utiliza√ß√£o de um preditor que se desvia da **expectativa condicional**.

### Demonstra√ß√£o da Otimalidade
Para minimizar o MSE, devemos escolher uma fun√ß√£o $g(X_t)$ que anule o termo adicional $E[E(Y_{t+1}|X_t) - g(X_t)]^2$. Isso ocorre se, e somente se [^1]:
$$E(Y_{t+1}|X_t) = g(X_t)$$
quase certamente.  Isso demonstra que a fun√ß√£o $g(X_t)$ que minimiza o MSE √© a **expectativa condicional** $E(Y_{t+1}|X_t)$. O MSE m√≠nimo que pode ser alcan√ßado √© dado por [^1]:
$$MSE_{min} = E[Y_{t+1} - E(Y_{t+1}|X_t)]^2.$$
Isso confirma formalmente que a **expectativa condicional** $E(Y_{t+1}|X_t)$ √© o preditor √≥timo no sentido de minimizar o erro quadr√°tico m√©dio (MSE).

> üí° **Exemplo Num√©rico:** Suponha que temos um modelo simplificado onde $Y_{t+1}$ √© linearmente dependente de $X_t$ com um erro aleat√≥rio, dado por $Y_{t+1} = 2X_t + \epsilon_{t+1}$, onde $\epsilon_{t+1}$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia 1, ou seja, $\epsilon_{t+1} \sim N(0, 1)$. Assumimos tamb√©m que $X_t$ tem uma distribui√ß√£o com m√©dia 3 e vari√¢ncia 2.  Vamos comparar o MSE quando usamos a expectativa condicional, que neste caso √© $E(Y_{t+1}|X_t) = 2X_t$, como preditor e quando usamos uma fun√ß√£o diferente, como $g(X_t) = 1.5X_t$.
>
> 1.  **C√°lculo para o preditor √≥timo (expectativa condicional):**
>    *   Preditor: $\hat{Y}_{t+1} = E(Y_{t+1}|X_t) = 2X_t$.
>    *   $MSE(2X_t) = E[(Y_{t+1} - 2X_t)^2] = E[(2X_t + \epsilon_{t+1} - 2X_t)^2] = E[\epsilon_{t+1}^2] = 1$.  Note que o MSE do preditor √≥timo √© igual √† vari√¢ncia do erro aleat√≥rio.
>
> 2.  **C√°lculo para o preditor sub√≥timo:**
>    *   Preditor: $\hat{Y}_{t+1} = g(X_t) = 1.5X_t$.
>    *   $MSE(1.5X_t) = E[(Y_{t+1} - 1.5X_t)^2] = E[(2X_t + \epsilon_{t+1} - 1.5X_t)^2] = E[(0.5X_t + \epsilon_{t+1})^2]$.
>    *   Expandindo: $MSE(1.5X_t) = E[0.25X_t^2 + \epsilon_{t+1}^2 + X_t\epsilon_{t+1}] = 0.25E[X_t^2] + E[\epsilon_{t+1}^2] + E[X_t\epsilon_{t+1}]$.
>    *   Dado que $E[X_t\epsilon_{t+1}] = 0$ (assumindo que o erro n√£o tem rela√ß√£o com $X_t$), e $E[X_t^2] = Var[X_t] + E[X_t]^2 = 2 + 3^2 = 11$, ent√£o:
>    *   $MSE(1.5X_t) = 0.25 \times 11 + 1 = 2.75 + 1 = 3.75$.
>
>   Podemos observar que o MSE do preditor sub√≥timo (3.75) √© maior que o MSE do preditor √≥timo (1).  A diferen√ßa no MSE (3.75 - 1 = 2.75) reflete o custo de usar um preditor que n√£o √© a expectativa condicional. A parte que corresponde ao erro irredut√≠vel √© a vari√¢ncia do erro: 1. A parte adicional do MSE, que resulta do uso do preditor sub-√≥timo, √© de 2.75.
>
>
>  ```python
>    import numpy as np
>
>    # Given parameters
>    variance_epsilon = 1
>    mean_x = 3
>    variance_x = 2
>
>    # Calculate the MSE for the optimal predictor (conditional expectation)
>    mse_optimal = variance_epsilon
>
>    # Calculate the mean square of x
>    mean_sq_x = variance_x + mean_x**2
>
>    # Calculate the MSE for the suboptimal predictor
>    mse_suboptimal = 0.25 * mean_sq_x + variance_epsilon
>
>    print(f"MSE for optimal predictor: {mse_optimal}")
>    print(f"MSE for suboptimal predictor: {mse_suboptimal}")
>    print(f"Difference in MSE: {mse_suboptimal-mse_optimal}")
>  ```

**Teorema 1:** (Otimilidade da Expectativa Condicional) Dado o MSE como m√©trica de avalia√ß√£o, a **expectativa condicional** $E(Y_{t+1}|X_t)$ √© o √∫nico preditor (a menos de um conjunto de probabilidade nula) que minimiza o MSE.
*Proof:*
I. Come√ßamos com a decomposi√ß√£o do MSE para um preditor arbitr√°rio $g(X_t)$:
   $$MSE(g(X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] + E[(E(Y_{t+1}|X_t) - g(X_t))^2]$$
II. O primeiro termo, $E[(Y_{t+1} - E(Y_{t+1}|X_t))^2]$, √© o erro irredut√≠vel, e √© fixo para todos os preditores que se baseiam em $X_t$.
III. O segundo termo, $E[(E(Y_{t+1}|X_t) - g(X_t))^2]$, √© n√£o negativo e s√≥ √© igual a zero se $g(X_t) = E(Y_{t+1}|X_t)$ quase certamente.
IV. Portanto, para qualquer $g(X_t) \neq E(Y_{t+1}|X_t)$ (a menos de conjuntos de probabilidade zero), temos que o segundo termo ser√° estritamente positivo, resultando em $MSE(g(X_t)) > MSE(E(Y_{t+1}|X_t))$.
V. Conclu√≠mos que $E(Y_{t+1}|X_t)$ √© o √∫nico preditor que minimiza o MSE. $\blacksquare$

**Lema 1.1:** (Propriedade do Erro de Previs√£o) O erro de previs√£o associado √† **expectativa condicional**, definido como $e_{t+1} = Y_{t+1} - E(Y_{t+1}|X_t)$, possui m√©dia zero, ou seja, $E[e_{t+1}] = 0$.
*Proof:* Pela lei da esperan√ßa iterada, temos:
$$E[e_{t+1}] = E[Y_{t+1} - E(Y_{t+1}|X_t)] = E[E[Y_{t+1} - E(Y_{t+1}|X_t)|X_t]]$$
Aplicando a linearidade da esperan√ßa condicional, temos:
$$E[E[Y_{t+1}|X_t] - E[E(Y_{t+1}|X_t)|X_t]] = E[E[Y_{t+1}|X_t] - E[Y_{t+1}|X_t]] = E[0] = 0$$
Portanto, a m√©dia do erro de previs√£o √© zero.  ‚ñ†

**Lema 1.2:** (Ortogonalidade do Erro de Previs√£o) O erro de previs√£o $e_{t+1} = Y_{t+1} - E(Y_{t+1}|X_t)$ √© ortogonal a qualquer fun√ß√£o $h(X_t)$ de $X_t$, ou seja, $E[e_{t+1}h(X_t)] = 0$.
*Proof:*
I. Pela lei da esperan√ßa iterada, temos:
$$E[e_{t+1}h(X_t)] = E[E[e_{t+1}h(X_t)|X_t]]$$
II. Como $h(X_t)$ √© uma fun√ß√£o de $X_t$, √© constante dado $X_t$ e podemos escrever:
$$E[e_{t+1}h(X_t)] = E[h(X_t)E[e_{t+1}|X_t]]$$
III. Sabemos do Lema 1.1 que $E[e_{t+1}|X_t] = E[Y_{t+1}|X_t] - E[E(Y_{t+1}|X_t)|X_t] = E[Y_{t+1}|X_t] - E[Y_{t+1}|X_t] = 0$
IV. Logo:
$$E[e_{t+1}h(X_t)] = E[h(X_t) \times 0] = E[0] = 0$$
Portanto, o erro de previs√£o $e_{t+1}$ √© ortogonal a qualquer fun√ß√£o de $X_t$. ‚ñ†

**Observa√ß√£o 1:** A decomposi√ß√£o do MSE ressalta que apenas o segundo termo, que envolve a diferen√ßa entre a **expectativa condicional** e o preditor alternativo $g(X_t)$, √© afetado pela escolha da fun√ß√£o de previs√£o. Isso implica que para minimizar o MSE, o foco deve ser em aproximar a fun√ß√£o de previs√£o da **expectativa condicional**.

**Observa√ß√£o 2:** O termo $E[Y_{t+1} - E(Y_{t+1}|X_t)]^2$ representa a vari√¢ncia do erro de previs√£o, tamb√©m conhecida como **erro irredut√≠vel**. Ele indica o limite inferior para o MSE, ou seja, a menor vari√¢ncia de erro que se pode alcan√ßar ao prever $Y_{t+1}$ usando informa√ß√µes de $X_t$. Este termo independe da escolha da fun√ß√£o $g(X_t)$.

**Proposi√ß√£o 1:** (Rela√ß√£o entre MSE e Vari√¢ncia do Erro) Se $g(X_t)$ √© a expectativa condicional, ent√£o o MSE √© igual √† vari√¢ncia do erro de previs√£o condicional. Mais precisamente, $MSE(E(Y_{t+1}|X_t)) = Var(Y_{t+1} - E(Y_{t+1}|X_t))$.
*Proof:*
I. Sabemos que o $MSE(g(X_t)) = E[Y_{t+1} - g(X_t)]^2$. Quando $g(X_t) = E(Y_{t+1}|X_t)$, temos:
$MSE(E(Y_{t+1}|X_t)) = E[Y_{t+1} - E(Y_{t+1}|X_t)]^2$.
II. Por defini√ß√£o, a vari√¢ncia do erro de previs√£o √©:
$Var(Y_{t+1} - E(Y_{t+1}|X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t) - E[Y_{t+1} - E(Y_{t+1}|X_t)])^2]$.
III. Como $E[Y_{t+1} - E(Y_{t+1}|X_t)] = 0$ (Lema 1.1), temos:
$Var(Y_{t+1} - E(Y_{t+1}|X_t)) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] = MSE(E(Y_{t+1}|X_t))$.
IV. Portanto, o MSE da expectativa condicional √© igual √† vari√¢ncia do erro de previs√£o.  ‚ñ†

### Conclus√£o
A demonstra√ß√£o apresentada neste cap√≠tulo, utilizando a **decomposi√ß√£o do MSE** e a **lei das expectativas iteradas**, estabelece de forma rigorosa que a **expectativa condicional** $E(Y_{t+1}|X_t)$ √© o preditor √≥timo no contexto de minimiza√ß√£o do **erro quadr√°tico m√©dio**. Qualquer outro preditor, que difira da **expectativa condicional**, incorrer√° em um MSE maior. Essa conclus√£o √© um pilar fundamental para a modelagem de previs√µes em s√©ries temporais, e sublinha a import√¢ncia de utilizar a **expectativa condicional** como base para constru√ß√£o de modelos preditivos eficazes. Esta an√°lise tamb√©m evidencia o conceito de erro irredut√≠vel, que limita a precis√£o de qualquer previs√£o, e enfatiza o papel da informa√ß√£o na redu√ß√£o da vari√¢ncia da previs√£o.
### Refer√™ncias
[^1]: Trechos do texto original fornecido.
<!-- END -->
