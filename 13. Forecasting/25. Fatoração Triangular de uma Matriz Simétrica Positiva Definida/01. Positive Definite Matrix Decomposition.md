## Fatoração Triangular de uma Matriz Simétrica Positiva Definida
### Introdução
Este capítulo explora em detalhes a fatoração triangular de uma matriz simétrica positiva definida, um conceito essencial em diversas áreas da matemática, estatística e, particularmente, na análise de séries temporais. A representação de uma matriz como o produto de uma matriz triangular inferior, uma matriz diagonal e a transposta da matriz triangular inferior, oferece uma poderosa ferramenta para simplificar cálculos e análises [^4.4.1]. Este tópico constrói sobre a discussão anterior de projeções lineares e estimação de mínimos quadrados ordinários (OLS), fornecendo uma base para entender métodos computacionais e teóricos mais avançados.

### Conceitos Fundamentais
A fatoração triangular, também conhecida como fatoração de Cholesky quando a matriz diagonal é expressa como o quadrado de uma outra matriz diagonal, é um método para decompor uma matriz simétrica positiva definida em um produto de três matrizes de uma forma específica [^4.4.1]. Formalmente, dada uma matriz **simétrica positiva definida** $\Omega$ de dimensão *n x n*, podemos expressá-la como:

$$ \Omega = ADA' $$,

onde:

1.  **A** é uma **matriz triangular inferior** de dimensão *n x n* com todos os elementos da diagonal principal iguais a 1. Isso significa que todos os elementos acima da diagonal principal são zero.
2.  **D** é uma **matriz diagonal** de dimensão *n x n* com todos os elementos da diagonal principal estritamente positivos, ou seja, $d_{ii} > 0$ para todo *i*. Todos os elementos fora da diagonal são zero.
3.  **A'** é a **transposta** da matriz A.

A importância desta fatoração reside na sua unicidade e na sua utilidade em simplificar cálculos envolvendo matrizes simétricas positivas definidas.

> 💡 **Exemplo Numérico:**
> Vamos considerar uma matriz simétrica positiva definida $\Omega$ de dimensão 3x3:
>
>  $$ \Omega = \begin{bmatrix} 4 & 2 & 2 \\ 2 & 5 & 3 \\ 2 & 3 & 6 \end{bmatrix} $$
>
>  O objetivo é encontrar matrizes A e D tal que $\Omega = ADA'$. Este processo será detalhado nos próximos passos, e demonstrará como a fatoração triangular funciona na prática.

**Construção da Fatoração Triangular**
A fatoração triangular pode ser construída de forma iterativa. Começamos transformando $\Omega$ numa matriz com zeros na primeira coluna, abaixo da diagonal principal [^4.4.2]. Isso é feito através da pré-multiplicação de $\Omega$ por uma matriz **E₁**:

$$
E_1 = \begin{bmatrix}
1 & 0 & 0 & \ldots & 0 \\
-\frac{\Omega_{21}}{\Omega_{11}} & 1 & 0 & \ldots & 0 \\
-\frac{\Omega_{31}}{\Omega_{11}} & 0 & 1 & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
-\frac{\Omega_{n1}}{\Omega_{11}} & 0 & 0 & \ldots & 1
\end{bmatrix}
$$
Onde $E_1$ é uma matriz com 1s na diagonal principal, e elementos da forma $-\frac{\Omega_{i1}}{\Omega_{11}}$ abaixo da diagonal principal na primeira coluna [^4.4.3].
A pré-multiplicação por *$E₁$* e a pós-multiplicação pela transposta *$E₁'$* resulta em uma matriz *H*:

$$
H = E_1\Omega E_1'
$$

onde *H* tem zeros na primeira coluna, abaixo da diagonal principal [^4.4.4]. O processo é então repetido para as colunas seguintes.
Em cada etapa, pré-multiplicamos e pós-multiplicamos com uma matriz $E_k$ construída de forma análoga, resultando em:

$$
E_{n-1} \ldots E_2 E_1 \Omega E_1' E_2' \ldots E_{n-1}' = D
$$

Onde *D* é uma matriz diagonal. A matriz *A* é então obtida pela inversão e multiplicação das matrizes *E* [^4.4.8].

> 💡 **Exemplo Numérico (continuação):**
>
> Para a matriz $\Omega$ do exemplo anterior, vamos calcular $E_1$:
>
> $$ E_1 = \begin{bmatrix} 1 & 0 & 0 \\ -\frac{2}{4} & 1 & 0 \\ -\frac{2}{4} & 0 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 \\ -0.5 & 1 & 0 \\ -0.5 & 0 & 1 \end{bmatrix} $$
>
> Agora, calculamos $H = E_1 \Omega E_1'$
>
> $$ H = \begin{bmatrix} 1 & 0 & 0 \\ -0.5 & 1 & 0 \\ -0.5 & 0 & 1 \end{bmatrix} \begin{bmatrix} 4 & 2 & 2 \\ 2 & 5 & 3 \\ 2 & 3 & 6 \end{bmatrix} \begin{bmatrix} 1 & -0.5 & -0.5 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 2 \\ 0 & 2 & 5 \end{bmatrix} $$
>
> Observe que a primeira coluna de $H$, abaixo da diagonal principal, agora possui zeros.

**Observação:** Note que cada matriz $E_k$ é uma matriz triangular inferior com 1s na diagonal e com elementos abaixo da diagonal na coluna *k* que são utilizados para zerar os elementos abaixo da diagonal na matriz resultante da operação $E_{k-1} \ldots E_1 \Omega E_1' \ldots E_{k-1}' $. Portanto, a matriz $E_k^{-1}$ é idêntica a $E_k$ exceto pelo sinal dos elementos fora da diagonal. Isso facilita o cálculo de $A$ como o produto das inversas das matrizes $E_k$.

**Lema 1**
A inversa de uma matriz $E_k$ como definida acima é obtida trocando o sinal dos elementos abaixo da diagonal principal.

*Proof*:
Seja $E_k$ uma matriz da forma:
$$
E_k = I - v_k e_k'
$$
onde $I$ é a matriz identidade, $e_k$ é o vetor coluna com 1 na posição *k* e 0 em outras posições, e $v_k$ é o vetor com elementos $-\frac{\Omega_{ik}}{\Omega_{kk}}$ para $i > k$ e 0 caso contrário. Então, a inversa de $E_k$ é dada por:
$$
E_k^{-1} = I + v_k e_k'
$$
Esta relação pode ser verificada multiplicando $E_k$ por $E_k^{-1}$ e observando que o resultado é a matriz identidade.

I. Seja $E_k = I - v_k e_k'$ e $E_k^{-1} = I + v_k e_k'$.

II. Multiplicando $E_k$ por $E_k^{-1}$:
$$ E_k E_k^{-1} = (I - v_k e_k')(I + v_k e_k') $$

III. Expandindo o produto:
$$ E_k E_k^{-1} = I + v_k e_k' - v_k e_k' - v_k e_k' v_k e_k' $$

IV. Observe que $e_k' v_k$ é um escalar (produto interno de dois vetores).  Como $e_k$ é um vetor com 1 na posição $k$ e 0 em outras posições, e os elementos de $v_k$ são da forma $-\frac{\Omega_{ik}}{\Omega_{kk}}$ para $i>k$ e zero caso contrário, $e_k' v_k = 0$.
$$ E_k E_k^{-1} = I + v_k e_k' - v_k e_k' - v_k (0) e_k' = I $$

V. Portanto, $E_k E_k^{-1} = I$, confirmando que $E_k^{-1} = I + v_k e_k'$ é a inversa de $E_k$. ■

Essa demonstração mostra que a inversa da matriz $E_k$ é facilmente obtida, confirmando a observação anterior sobre a facilidade de obter a matriz A.

> 💡 **Exemplo Numérico (continuação):**
>
>   A inversa de $E_1$ é obtida simplesmente trocando o sinal dos elementos abaixo da diagonal:
>
> $$ E_1^{-1} = \begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.5 & 0 & 1 \end{bmatrix} $$
>
>   Este resultado é útil para obter a matriz $A$.

**Unicidade da Fatoração**
A fatoração triangular é única. Para demonstrar isso, suponha que existam duas fatorações triangulares para a matriz $\Omega$:

$$ \Omega = A_1 D_1 A_1' = A_2 D_2 A_2' $$.

Multiplicando à esquerda por *$A₂⁻¹$* e à direita por *(A₁')⁻¹*, temos [^4.4.15]:

$$ A_2^{-1} A_1 D_1  = D_2 A_2' (A_1')^{-1} $$.

O lado esquerdo desta equação é uma matriz triangular superior, enquanto o lado direito é uma matriz triangular inferior. A única maneira de isso ser verdade é se ambas as matrizes forem iguais à matriz identidade. Isso implica que A₁ = A₂ e D₁ = D₂ [^4.4.15]. Portanto, a fatoração triangular é única.

*Proof*:
I.  Assuma que existem duas fatorações triangulares para $\Omega$:
    $$\Omega = A_1 D_1 A_1' = A_2 D_2 A_2'$$

II. Multiplicando à esquerda por $A_2^{-1}$:
    $$A_2^{-1} \Omega = A_2^{-1} A_1 D_1 A_1' = D_2 A_2'$$

III. Multiplicando à direita por $(A_1')^{-1}$:
    $$A_2^{-1} A_1 D_1 = D_2 A_2' (A_1')^{-1}$$

IV. O lado esquerdo, $A_2^{-1} A_1 D_1$, é o produto de três matrizes. $A_1$ e $A_2$ são triangulares inferiores com 1's na diagonal, e $D_1$ é diagonal, tornando o produto triangular inferior. A inversa de uma matriz triangular inferior também é triangular inferior, o que faz com que $A_2^{-1} A_1$ seja triangular inferior.
    O produto de uma matriz triangular inferior por uma diagonal continua sendo triangular inferior.

V. O lado direito, $D_2 A_2' (A_1')^{-1}$, é o produto de três matrizes. $A_2'$ e $(A_1')^{-1}$ são triangulares superiores, e $D_2$ é diagonal. Portanto, o lado direito é triangular superior.

VI. Uma matriz triangular inferior é igual a uma matriz triangular superior se e somente se ambas forem diagonais. Portanto, $A_2^{-1} A_1 D_1$ e $D_2 A_2' (A_1')^{-1}$ são ambas diagonais. Como $A_1$ e $A_2$ são triangulares inferiores com 1's na diagonal, $A_2^{-1} A_1$ deve ser igual a $I$.

VII. Se $A_2^{-1}A_1=I$, então $A_1 = A_2$, e consequentemente $D_1 = D_2$.

VIII. Portanto, a fatoração triangular é única. ■

**Fatoração de Cholesky**
A **fatoração de Cholesky** é uma forma particular da fatoração triangular em que a matriz diagonal *D* é expressa como o quadrado de outra matriz diagonal, **D¹/²** [^4.4.16]:

$$ \Omega = AD^{1/2} (AD^{1/2})' = PP' $$,

onde
*   P = AD¹/²
e D¹/² é uma matriz diagonal cujos elementos diagonais são as raízes quadradas dos elementos diagonais da matriz D. A matriz P também é uma matriz triangular inferior. Esta representação é útil em certas aplicações, especialmente para simulações e geração de números aleatórios com uma dada matriz de covariância.

> 💡 **Exemplo Numérico (continuação):**
> Continuando o exemplo, vamos encontrar a matriz diagonal $D$ e depois $D^{1/2}$ e finalmente a matriz $P$.
>
> Após aplicar a sequência de operações $E_k$ até obter uma matriz diagonal, teríamos:
>
> $$ D =  \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 4 \end{bmatrix} $$
>
> $$ D^{1/2} =  \begin{bmatrix} 2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 2 \end{bmatrix} $$
>
> E a matriz A seria o produto das inversas das matrizes $E_k$:
>
> $$ A = E_1^{-1} E_2^{-1} = \begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.5 & 0.5 & 1 \end{bmatrix}$$
>
> Finalmente, a matriz P na fatoração de Cholesky é dada por:
>
> $$ P = AD^{1/2} =  \begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.5 & 0.5 & 1 \end{bmatrix}  \begin{bmatrix} 2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 2 \end{bmatrix} = \begin{bmatrix} 2 & 0 & 0 \\ 1 & 2 & 0 \\ 1 & 1 & 2 \end{bmatrix}$$
>
> Note que $P$ é uma matriz triangular inferior.
>
>  Podemos verificar que $PP^T = \Omega$:
> $$ PP' = \begin{bmatrix} 2 & 0 & 0 \\ 1 & 2 & 0 \\ 1 & 1 & 2 \end{bmatrix} \begin{bmatrix} 2 & 1 & 1 \\ 0 & 2 & 1 \\ 0 & 0 & 2 \end{bmatrix} = \begin{bmatrix} 4 & 2 & 2 \\ 2 & 5 & 3 \\ 2 & 3 & 6 \end{bmatrix} = \Omega $$

**Lema 2**
A matriz P na fatoração de Cholesky de uma matriz simétrica positiva definida $\Omega$ é uma matriz triangular inferior com elementos diagonais positivos.

*Proof*:
A matriz $P$ é dada por $P = AD^{1/2}$, onde $A$ é uma matriz triangular inferior com 1s na diagonal principal e $D^{1/2}$ é uma matriz diagonal com elementos positivos na diagonal. O produto de uma matriz triangular inferior com uma matriz diagonal resulta em uma matriz triangular inferior. Adicionalmente, os elementos da diagonal de $P$ são o produto dos elementos diagonais correspondentes de $A$ e $D^{1/2}$, que são $1$ e $\sqrt{d_{ii}}$, respectivamente. Portanto, os elementos diagonais de $P$ são todos positivos.

I. Seja $P = AD^{1/2}$.

II. $A$ é uma matriz triangular inferior com 1s na diagonal principal, e $D^{1/2}$ é uma matriz diagonal com elementos diagonais positivos, $d_{ii}^{1/2} > 0$.

III. O produto de uma matriz triangular inferior ($A$) com uma matriz diagonal ($D^{1/2}$) é uma matriz triangular inferior.

IV. Os elementos diagonais de $P$ são dados pelo produto dos elementos diagonais correspondentes de $A$ e $D^{1/2}$. Como os elementos diagonais de $A$ são todos iguais a 1 e os elementos diagonais de $D^{1/2}$ são $\sqrt{d_{ii}}$, então os elementos diagonais de $P$ são $\sqrt{d_{ii}}$, que são todos positivos.

V. Portanto, $P$ é uma matriz triangular inferior com elementos diagonais positivos.■

**Aplicação em Projeções Lineares**
Como vimos na seção anterior, a fatoração triangular é muito útil na atualização de projeções lineares, fornecendo um método eficiente para calcular os coeficientes e os erros de projeções lineares de forma recursiva [^4.5.2]. Além disso, a fatorização é uma ferramenta muito poderosa na construção de projeções lineares [^4.5.6].

**Teorema 1**
A fatoração de Cholesky pode ser usada para determinar a projeção linear de uma variável aleatória $Y_i$ sobre um conjunto de variáveis aleatórias $Y_1, \ldots, Y_{i-1}$.

*Proof:*
Seja $\Omega$ a matriz de covariância das variáveis aleatórias $Y = [Y_1, Y_2, \ldots, Y_n]^T$, com $\Omega = E[YY^T]$. Usando a fatoração de Cholesky, podemos escrever $\Omega = PP^T$. Considere a projeção de $Y_i$ sobre $Y_1, \ldots, Y_{i-1}$, denotada por $\hat{Y_i}$. As variáveis transformadas $Z = P^{-1}Y$ têm matriz de covariância igual à identidade. Assim, a projeção de $Z_i$ em $Z_1, \ldots, Z_{i-1}$ é simplesmente $\hat{Z_i} = [Z_1, \ldots, Z_{i-1}]^T[Z_1, \ldots, Z_{i-1}] = [Z_1, \ldots, Z_{i-1}]$, onde os coeficientes da projeção são obtidos por meio da matriz triangular inferior $P$. Os elementos de $P$ permitem obter os coeficientes da projeção de $Y_i$ sobre os elementos de $Y_1, \ldots, Y_{i-1}$ e, consequentemente, a projeção linear de $Y_i$.

I. Seja $Y = [Y_1, Y_2, \ldots, Y_n]^T$ um vetor de variáveis aleatórias com matriz de covariância $\Omega = E[YY^T]$.

II. Usando a fatoração de Cholesky, podemos escrever $\Omega = PP^T$, onde $P$ é uma matriz triangular inferior com elementos diagonais positivos.

III. Defina $Z = P^{-1}Y$. Então, a matriz de covariância de $Z$ é:
    $$Cov(Z) = Cov(P^{-1}Y) = P^{-1} Cov(Y) (P^{-1})^T = P^{-1} \Omega (P^{-1})^T = P^{-1}PP^T (P^{-1})^T = I$$

IV. A projeção de $Z_i$ sobre $Z_1, \ldots, Z_{i-1}$ é dada por:
   $$\hat{Z_i} = \sum_{j=1}^{i-1} \beta_j Z_j $$
   Como $Cov(Z) = I$, as variáveis $Z_1, \ldots, Z_{i-1}$ são ortogonais e os coeficientes da projeção são nulos. A projeção de $Z_i$ sobre $Z_1, \ldots, Z_{i-1}$ é um vetor com os primeiros $i-1$ elementos iguais aos próprios elementos $Z_1, \ldots, Z_{i-1}$ e os restantes elementos são zero, ou seja, $\hat{Z_i}$ é o vetor formado pela projeção de $Z_i$ sobre o subespaço gerado por $Z_1,\ldots,Z_{i-1}$.
   
V.  Como $Z = P^{-1}Y$, temos $Y = PZ$. Logo, podemos expressar $Y_i$ como uma combinação linear das variáveis $Z_1, \ldots, Z_i$ dadas pelos elementos da $i$-ésima linha da matriz $P$. Os coeficientes desta combinação linear são os elementos da matriz $P$.

VI. Ao usar a matriz triangular inferior P, calculamos os coeficientes da projeção de $Y_i$ sobre $Y_1, \ldots, Y_{i-1}$. Como $P$ é triangular inferior, $Y_i$ é uma combinação linear de $Z_1, \ldots, Z_i$. Consequentemente, os coeficientes da projeção de $Y_i$ sobre $Y_1, \ldots, Y_{i-1}$ podem ser encontrados através da matriz $P$.

VII. Portanto, a fatoração de Cholesky pode ser usada para determinar a projeção linear de $Y_i$ sobre $Y_1, \ldots, Y_{i-1}$. ■

> 💡 **Exemplo Numérico (continuação):**
>
> Vamos supor que $Y$ é um vetor de variáveis aleatórias com a matriz de covariância $\Omega$ que usamos nos exemplos anteriores. Assim, temos:
>
> $$ \Omega = \begin{bmatrix} 4 & 2 & 2 \\ 2 & 5 & 3 \\ 2 & 3 & 6 \end{bmatrix} $$
> e a matriz P da fatoração de Cholesky:
>
>  $$ P = \begin{bmatrix} 2 & 0 & 0 \\ 1 & 2 & 0 \\ 1 & 1 & 2 \end{bmatrix} $$
>
> Se $Z=P^{-1}Y$, então $Y=PZ$. A primeira linha de $P$ diz que $Y_1=2Z_1$. A segunda linha de $P$ diz que $Y_2 = Z_1 + 2Z_2$. E a terceira linha de $P$ diz que $Y_3 = Z_1 + Z_2 + 2Z_3$. Desta forma, é possível obter os coeficientes da projeção de cada variável sobre as anteriores.
>
>Por exemplo, para projetar $Y_3$ em $Y_1$ e $Y_2$, observamos que:
>
>$$ Y_3 = Z_1 + Z_2 + 2Z_3 $$
>
> Da relação $Y=PZ$, temos $Z=P^{-1}Y$. Calculando $P^{-1}$:
>
>$$ P^{-1} = \begin{bmatrix} 1/2 & 0 & 0 \\ -1/4 & 1/2 & 0 \\ 0 & -1/4 & 1/2 \end{bmatrix}$$
>
> Então:
>
> $$ Z_1 = \frac{1}{2}Y_1 $$
> $$ Z_2 = -\frac{1}{4}Y_1 + \frac{1}{2}Y_2 $$
> $$ Z_3 = -\frac{1}{4}Y_2 + \frac{1}{2}Y_3 $$
>
> Substituindo, teríamos:
>
>$$ Y_3 = \frac{1}{2}Y_1 + (-\frac{1}{4}Y_1 + \frac{1}{2}Y_2) + 2(-\frac{1}{4}Y_2 + \frac{1}{2}Y_3) $$
>$$ Y_3 = \frac{1}{2}Y_1 -\frac{1}{4}Y_1 + \frac{1}{2}Y_2 - \frac{1}{2}Y_2 + Y_3 $$
>$$ 0 = \frac{1}{4}Y_1  $$
>
> Isto demonstra como a matriz P da fatoração de Cholesky ajuda a obter as relações de projeção entre as variáveis aleatórias.

### Conclusão
A fatoração triangular, incluindo a fatoração de Cholesky, é uma ferramenta poderosa e fundamental na matemática e estatística, com aplicações em diversas áreas. A capacidade de decompor matrizes simétricas positivas definidas desta forma fornece uma abordagem eficiente para simplificar cálculos, entender propriedades de modelos estatísticos e implementar algoritmos computacionais, como o filtro de Kalman. O uso dessa fatoração, como discutido neste capítulo, complementa a teoria das projeções lineares, conectando a teoria com aplicações práticas [^4.5.13].

### Referências
[^4.4.1]: Qualquer matriz simétrica positiva definida $\Omega$ pode ser expressa como $\Omega = ADA'$, onde A é uma matriz triangular inferior com 1s na diagonal principal, e D é uma matriz diagonal com elementos positivos.
[^4.4.2]: Para ver como a fatorização triangular pode ser calculada, considere...
[^4.4.3]: Esta matriz sempre existe, desde que $\Omega_{11} \neq 0$. Isso é garantido no caso atual, porque $\Omega_{11}$ é igual a $e_1\Omega e_1$, onde $e_1 = [1 \, 0 \, 0 \, \ldots \, 0]$. Como $\Omega$ é positiva definida, $e_1\Omega e_1$ deve ser maior que zero.
[^4.4.4]: Quando $\Omega$ é pré-multiplicado por $E_1$ e pós-multiplicado por $E_1'$, o resultado é...
[^4.4.8]: Assim, existe uma matriz A...
[^4.4.15]: Multiplicando à esquerda [4.4.14] por $D_1^{-1}A_1^{-1}$ e multiplicando à direita por $[A_2']^{-1}$ se obtém...
[^4.4.16]: Expressão [4.4.16] é conhecida como a fatoração de Cholesky de $\Omega$...
[^4.5.1]: Seja $Y = (Y_1, Y_2, \ldots, Y_n)'$ um vetor (n x 1) de variáveis aleatórias cuja matriz de segundo momento é dada por $\Omega = E(YY')$.
[^4.5.2]: Seja $\Omega = ADA'$ a fatoração triangular de $\Omega$, e defina $\tilde{Y} = A^{-1}Y$.
[^4.5.6]: Para ver a implicação disso, pré-multiplique [4.5.2] por A: $A\tilde{Y} = Y$.
[^4.5.13]: O MSE da projeção linear é a variância de $Y_3$, que de [4.5.5] é dada por $d_{33}$: $E[Y_3 - P(Y_3|Y_2,Y_1)]^2 = h_{33} - h_{32}h_{22}^{-1}h_{23}$
<!-- END -->
