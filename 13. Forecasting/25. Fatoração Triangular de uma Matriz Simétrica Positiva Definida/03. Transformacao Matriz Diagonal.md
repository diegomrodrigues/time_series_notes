## A Transforma√ß√£o de $\Omega$ para uma Matriz Diagonal via Multiplica√ß√£o por Matrizes Triangulares Inferiores

### Introdu√ß√£o
Este cap√≠tulo detalha o processo iterativo de transforma√ß√£o de uma matriz sim√©trica positiva definida $\Omega$ em uma matriz diagonal *D* atrav√©s de pr√© e p√≥s-multiplica√ß√µes sucessivas por matrizes triangulares inferiores *E* e suas transpostas [^4.4.2]. Este processo √© fundamental para entender a **fatora√ß√£o triangular** e a **fatora√ß√£o de Cholesky**, previamente discutidas. As opera√ß√µes sucessivas com as matrizes *E* resultam na decomposi√ß√£o de $\Omega$ em um produto de matrizes que facilita a an√°lise e manipula√ß√£o de $\Omega$ em diversas aplica√ß√µes, especialmente em estat√≠stica e computa√ß√£o.

Este cap√≠tulo se baseia nos conceitos anteriores sobre **matrizes sim√©tricas positivas definidas**, **fatora√ß√£o triangular** e **fatora√ß√£o de Cholesky**, fornecendo uma vis√£o detalhada de como essas fatora√ß√µes s√£o constru√≠das computacionalmente. Uma compreens√£o completa do processo iterativo de transforma√ß√£o √© crucial para entender o funcionamento dos algoritmos de decomposi√ß√£o matricial.

### O Processo Iterativo de Transforma√ß√£o

Como vimos anteriormente, a fatora√ß√£o triangular de uma matriz sim√©trica positiva definida $\Omega$ √© dada por:

$$ \Omega = ADA' $$,

onde A √© uma matriz triangular inferior com 1s na diagonal principal e D √© uma matriz diagonal com elementos positivos [^4.4.1]. O processo para transformar $\Omega$ em uma matriz diagonal *D* envolve a aplica√ß√£o iterativa de pr√© e p√≥s-multiplica√ß√µes por uma sequ√™ncia de matrizes triangulares inferiores que zeram os elementos fora da diagonal.

**Constru√ß√£o da Matriz E‚ÇÅ**

O processo iterativo come√ßa com a constru√ß√£o de uma matriz triangular inferior, *$E_1$*, que transforma a primeira coluna de $\Omega$, zerando todos os elementos abaixo da diagonal principal [^4.4.3]. A matriz *$E_1$* √© dada por:

$$
E_1 = \begin{bmatrix}
1 & 0 & 0 & \ldots & 0 \\
-\frac{\Omega_{21}}{\Omega_{11}} & 1 & 0 & \ldots & 0 \\
-\frac{\Omega_{31}}{\Omega_{11}} & 0 & 1 & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
-\frac{\Omega_{n1}}{\Omega_{11}} & 0 & 0 & \ldots & 1
\end{bmatrix}
$$

onde o elemento gen√©rico *$E_1$*, em que a primeira coluna abaixo da diagonal cont√©m os elementos $-\frac{\Omega_{i1}}{\Omega_{11}}$ para $i>1$, e zeros nas outras posi√ß√µes abaixo da diagonal [^4.4.3]. Todos os elementos na diagonal principal de *$E_1$* s√£o iguais a 1.

**Primeira Transforma√ß√£o: H = E‚ÇÅŒ©E‚ÇÅ'**

Ao pr√©-multiplicar $\Omega$ por *$E_1$* e p√≥s-multiplicar por sua transposta *$E_1'$*, obtemos uma nova matriz *H*:

$$
H = E_1\Omega E_1'
$$

A matriz *H* tem a propriedade de que todos os elementos da primeira coluna abaixo da diagonal principal s√£o zero, enquanto os elementos na diagonal principal e acima dela permanecem inalterados em rela√ß√£o a $\Omega$ [^4.4.4].

> üí° **Exemplo Num√©rico:**
> Vamos considerar a matriz $\Omega$ do cap√≠tulo anterior e aplicar a primeira transforma√ß√£o:
>
> $$ \Omega = \begin{bmatrix} 4 & 2 & 2 \\ 2 & 5 & 3 \\ 2 & 3 & 6 \end{bmatrix} $$
>
> A matriz $E_1$ √© dada por:
>
> $$ E_1 = \begin{bmatrix} 1 & 0 & 0 \\ -\frac{2}{4} & 1 & 0 \\ -\frac{2}{4} & 0 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 \\ -0.5 & 1 & 0 \\ -0.5 & 0 & 1 \end{bmatrix} $$
>
> Calculamos a nova matriz $H$:
>
> $$ H = E_1 \Omega E_1' = \begin{bmatrix} 1 & 0 & 0 \\ -0.5 & 1 & 0 \\ -0.5 & 0 & 1 \end{bmatrix} \begin{bmatrix} 4 & 2 & 2 \\ 2 & 5 & 3 \\ 2 & 3 & 6 \end{bmatrix} \begin{bmatrix} 1 & -0.5 & -0.5 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 2 \\ 0 & 2 & 5 \end{bmatrix} $$
>
> Observe que a primeira coluna de *H*, abaixo da diagonal principal, agora possui zeros.
>
> 
> Vamos detalhar o c√°lculo para obter $H$. Primeiro calculamos $E_1\Omega$:
>
> $ E_1\Omega = \begin{bmatrix} 1 & 0 & 0 \\ -0.5 & 1 & 0 \\ -0.5 & 0 & 1 \end{bmatrix} \begin{bmatrix} 4 & 2 & 2 \\ 2 & 5 & 3 \\ 2 & 3 & 6 \end{bmatrix} =  \begin{bmatrix} 4 & 2 & 2 \\ 0 & 4 & 2 \\ 0 & 2 & 5 \end{bmatrix}$
>
> Agora calculamos $(E_1\Omega)E_1'$:
>
> $ (E_1\Omega)E_1' = \begin{bmatrix} 4 & 2 & 2 \\ 0 & 4 & 2 \\ 0 & 2 & 5 \end{bmatrix}  \begin{bmatrix} 1 & -0.5 & -0.5 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 2 \\ 0 & 2 & 5 \end{bmatrix} $
>
>
> A transforma√ß√£o $E_1\Omega E_1'$ elimina os termos fora da diagonal na primeira coluna e linha, preservando a simetria da matriz. Este √© um passo crucial na fatora√ß√£o triangular.

**A Sequ√™ncia de Matrizes E**

O processo continua iterativamente para as colunas restantes. Para a segunda coluna, constru√≠mos uma matriz *$E_2$*, que transforma a segunda coluna de *H*, zerando os elementos abaixo da diagonal:

$$
E_2 = \begin{bmatrix}
1 & 0 & 0 & \ldots & 0 \\
0 & 1 & 0 & \ldots & 0 \\
0 & -\frac{H_{32}}{H_{22}} & 1 & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & -\frac{H_{n2}}{H_{22}} & 0 & \ldots & 1
\end{bmatrix}
$$

onde os elementos da segunda coluna abaixo da diagonal principal s√£o dados por $-\frac{H_{i2}}{H_{22}}$ para $i>2$. Aplicando a transforma√ß√£o em *H*:

$$ K = E_2 H E_2' = E_2 E_1 \Omega E_1' E_2' $$,

A matriz *K* possui zeros na primeira e segunda colunas, abaixo da diagonal.

Este processo √© repetido at√© que todas as colunas abaixo da diagonal sejam zeradas. Em geral, a matriz *$E_k$* √© constru√≠da com 1s na diagonal principal e elementos abaixo da diagonal na coluna *k*, dados por $-\frac{H_{ik}}{H_{kk}}$ para *i > k*.

> üí° **Exemplo Num√©rico (continua√ß√£o):**
>
> Para o exemplo anterior, vamos calcular a matriz $E_2$. Primeiro, identificamos $H_{22}=4$ e $H_{32}=2$. Assim:
>
> $$ E_2 = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & -\frac{2}{4} & 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & -0.5 & 1 \end{bmatrix} $$
>
> Calculamos a nova matriz $K$:
>
> $$ K = E_2 H E_2' =  \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & -0.5 & 1 \end{bmatrix} \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 2 \\ 0 & 2 & 5 \end{bmatrix}  \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & -0.5 \\ 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 4 \end{bmatrix} $$
>
> Observe que a primeira e segunda colunas de $K$, abaixo da diagonal principal, agora possuem zeros.
>
> Para detalhar o c√°lculo, primeiro calculamos $E_2H$:
>
>  $E_2H = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & -0.5 & 1 \end{bmatrix} \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 2 \\ 0 & 2 & 5 \end{bmatrix} = \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 2 \\ 0 & 0 & 4 \end{bmatrix}$
>
> Agora calculamos $(E_2H)E_2'$:
>
>  $(E_2H)E_2' = \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 2 \\ 0 & 0 & 4 \end{bmatrix}\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & -0.5 \\ 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 4 \end{bmatrix}$
>
> O resultado √© uma matriz diagonal, o que indica que alcan√ßamos a forma desejada.

**A Matriz Diagonal D**

Ap√≥s *n-1* etapas do processo iterativo, obtemos uma matriz diagonal **D**:

$$
E_{n-1} \ldots E_2 E_1 \Omega E_1' E_2' \ldots E_{n-1}' = D
$$

onde **D** √© uma matriz diagonal com elementos estritamente positivos, como demonstrado no t√≥pico anterior [^4.4.7].

**Constru√ß√£o da Matriz A**

A matriz *A* √© obtida como o inverso do produto das matrizes *$E_k$*:

$$ A = (E_{n-1} \ldots E_2 E_1)^{-1} = E_1^{-1} E_2^{-1} \ldots E_{n-1}^{-1} $$

Como cada matriz *$E_k$* √© uma matriz triangular inferior com 1s na diagonal, sua inversa *$E_k^{-1}$* √© obtida simplesmente trocando os sinais dos elementos abaixo da diagonal [Lema 1]. O produto das matrizes triangulares inferiores *$E_k^{-1}$* resulta tamb√©m em uma matriz triangular inferior *A* [^4.4.8].

> üí° **Exemplo Num√©rico (continua√ß√£o):**
>
> Para obter a matriz *A*, calculamos as inversas de $E_1$ e $E_2$:
>
> $$ E_1^{-1} = \begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.5 & 0 & 1 \end{bmatrix} $$
>
> $$ E_2^{-1} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0.5 & 1 \end{bmatrix} $$
>
> A matriz *A* √© o produto das matrizes inversas:
>
> $$ A = E_1^{-1} E_2^{-1} = \begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.5 & 0 & 1 \end{bmatrix}  \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0.5 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.5 & 0.5 & 1 \end{bmatrix} $$
>
> Podemos notar que *A* √© uma matriz triangular inferior com 1s na diagonal.
>
>
> Para detalhar, primeiro calculamos $E_1^{-1} E_2^{-1}$:
>
> $ E_1^{-1} E_2^{-1} = \begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.5 & 0 & 1 \end{bmatrix}  \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0.5 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.5 & 0.5 & 1 \end{bmatrix}$
>
> Essa matriz $A$ cont√©m os multiplicadores usados nas transforma√ß√µes sucessivas e √© fundamental na fatora√ß√£o de Cholesky.

**Lema 1**
A inversa de uma matriz triangular inferior com 1s na diagonal principal √© obtida trocando os sinais dos elementos abaixo da diagonal.

*Proof:*
Seja $E_k$ uma matriz triangular inferior com 1s na diagonal principal. Ent√£o, $E_k$ pode ser escrita como $I - L_k$, onde $I$ √© a matriz identidade e $L_k$ √© uma matriz triangular inferior com zeros na diagonal principal. Podemos verificar que $(I - L_k)^{-1} = I + L_k + L_k^2 + \ldots$. Como $L_k$ √© nilpotente (i.e., $L_k^m = 0$ para algum $m$), a expans√£o acima √© finita e  $ (I-L_k)^{-1} = I + L_k$. Portanto a inversa de $E_k$ √© uma matriz triangular inferior com 1s na diagonal e com os sinais dos elementos abaixo da diagonal invertidos. $\blacksquare$

*Prova do Lema 1:*
I.  Seja $E_k$ uma matriz triangular inferior com 1s na diagonal principal. Ent√£o, podemos escrever $E_k = I - L_k$, onde $I$ √© a matriz identidade e $L_k$ √© uma matriz triangular inferior com zeros na diagonal principal.

II. A inversa de $E_k$, denotada por $E_k^{-1}$, deve satisfazer $E_k E_k^{-1} = I$. Ent√£o, temos $(I - L_k)E_k^{-1} = I$.

III. Podemos expandir a inversa de $(I-L_k)$ como uma s√©rie:
   $$(I - L_k)^{-1} = I + L_k + L_k^2 + L_k^3 + \ldots$$

IV. Como $L_k$ √© uma matriz triangular inferior com zeros na diagonal, ela √© nilpotente. Isso significa que existe um inteiro positivo $m$ tal que $L_k^m = 0$.

V. Portanto, a expans√£o da inversa se torna uma soma finita:
  $$(I - L_k)^{-1} = I + L_k + L_k^2 + \ldots + L_k^{m-1}$$

VI.  Como todos os termos $L_k^j$ para $j \geq 2$ resultam em matrizes com zeros na diagonal e tamb√©m com zeros nas diagonais que se afastam da diagonal principal, para fins de obter a inversa de $E_k$ basta considerar $(I-L_k)^{-1} = I + L_k$.

VII. Assim, a inversa de $E_k$ √© uma matriz triangular inferior com 1s na diagonal principal e os sinais dos elementos abaixo da diagonal invertidos. $\blacksquare$

**Teorema 2**
O produto de matrizes triangulares inferiores com 1s na diagonal principal √© tamb√©m uma matriz triangular inferior com 1s na diagonal principal.

*Proof:*
Sejam $A$ e $B$ matrizes triangulares inferiores com 1s na diagonal principal. Ent√£o, o elemento $(AB)_{ij}$ do produto $AB$ √© dado por $\sum_{k} a_{ik}b_{kj}$. Se $i < j$, ent√£o $a_{ik} = 0$ para $k>i$ e $b_{kj}=0$ para $k<j$. Assim, se $i<j$,  todos os termos na soma $\sum_{k} a_{ik}b_{kj}$ s√£o nulos e  $(AB)_{ij} = 0$. Se $i=j$, $(AB)_{ii} = \sum_{k} a_{ik}b_{ki}$. Como $a_{ik}=0$ para $k>i$ e $b_{ki}=0$ para $k<i$ e $a_{ii}=b_{ii}=1$, ent√£o $(AB)_{ii} = a_{ii}b_{ii} = 1 \times 1 = 1$. Portanto, $AB$ √© uma matriz triangular inferior com 1s na diagonal principal. Por indu√ß√£o, o produto de um n√∫mero finito de matrizes triangulares inferiores com 1s na diagonal principal √© tamb√©m uma matriz triangular inferior com 1s na diagonal principal.  $\blacksquare$

*Prova do Teorema 2:*
I. Sejam $A$ e $B$ matrizes triangulares inferiores com 1s na diagonal principal. Isso significa que $a_{ij} = 0$ para $i < j$ e $a_{ii} = 1$, e similarmente para $B$.

II. O elemento $(AB)_{ij}$ do produto $AB$ √© dado por:
$$(AB)_{ij} = \sum_{k=1}^{n} a_{ik}b_{kj}$$

III. Analisamos o caso em que $i < j$:
    Se $k > i$, ent√£o $a_{ik} = 0$. Se $k < j$, ent√£o $b_{kj} = 0$. Para que o produto $a_{ik}b_{kj}$ seja n√£o-nulo, devemos ter $k \leq i$ e $k \geq j$. Como $i < j$, n√£o existe nenhum $k$ que satisfa√ßa ambas as condi√ß√µes simultaneamente. Portanto, todos os termos da soma s√£o nulos e $(AB)_{ij} = 0$ para $i < j$.

IV. Analisamos o caso em que $i = j$:
    $$(AB)_{ii} = \sum_{k=1}^{n} a_{ik}b_{ki}$$
    Quando $k < i$, temos que $b_{ki} = 0$. Quando $k > i$, temos que $a_{ik} = 0$. O √∫nico termo n√£o-nulo √© quando $k = i$, e nesse caso $a_{ii} = 1$ e $b_{ii} = 1$. Portanto, $(AB)_{ii} = a_{ii}b_{ii} = 1 \cdot 1 = 1$.

V. Juntando os dois casos, conclu√≠mos que $AB$ √© uma matriz triangular inferior com 1s na diagonal principal.

VI. Por indu√ß√£o, este resultado pode ser estendido para um n√∫mero finito de matrizes triangulares inferiores com 1s na diagonal principal. Portanto, o produto de um n√∫mero finito de matrizes triangulares inferiores com 1s na diagonal principal √© tamb√©m uma matriz triangular inferior com 1s na diagonal principal. $\blacksquare$

### Unicidade da Transforma√ß√£o

Como discutido anteriormente, a fatora√ß√£o triangular √© √∫nica [^4.4.15]. Portanto, a sequ√™ncia de matrizes *$E_k$* e suas inversas que transformam $\Omega$ em *D* s√£o √∫nicas, dadas a matriz sim√©trica positiva definida $\Omega$. A unicidade garante a consist√™ncia do processo e a aplicabilidade da fatora√ß√£o em diferentes contextos.

**Teorema 3**
A sequ√™ncia de matrizes $E_k$ utilizadas na transforma√ß√£o de uma matriz sim√©trica positiva definida $\Omega$ em uma matriz diagonal $D$ √© √∫nica.

*Proof:*
Seja $\Omega$ uma matriz sim√©trica positiva definida. Suponha que existam duas sequ√™ncias de matrizes triangulares inferiores $E_1, E_2, \ldots, E_{n-1}$ e $F_1, F_2, \ldots, F_{n-1}$ com 1s na diagonal principal que transformam $\Omega$ em matrizes diagonais $D_E$ e $D_F$, respectivamente, ou seja,
$$ E_{n-1} \ldots E_2 E_1 \Omega E_1' E_2' \ldots E_{n-1}' = D_E $$
$$ F_{n-1} \ldots F_2 F_1 \Omega F_1' F_2' \ldots F_{n-1}' = D_F $$
Pelas propriedades da fatora√ß√£o triangular, temos que $\Omega = A_E D_E A_E' = A_F D_F A_F'$, onde $A_E = (E_{n-1} \ldots E_1)^{-1}$ e $A_F = (F_{n-1} \ldots F_1)^{-1}$. Pela unicidade da fatora√ß√£o triangular, temos $A_E=A_F$ e $D_E = D_F$. Uma vez que $A_E=A_F$, temos que $E_1 = F_1, E_2 = F_2, \ldots, E_{n-1} = F_{n-1}$.
Portanto, a sequ√™ncia de matrizes $E_k$ utilizada na transforma√ß√£o de $\Omega$ em $D$ √© √∫nica. $\blacksquare$

*Prova do Teorema 3:*
I. Seja $\Omega$ uma matriz sim√©trica positiva definida. Suponha que existam duas sequ√™ncias de matrizes triangulares inferiores com 1s na diagonal principal, $E_1, E_2, \ldots, E_{n-1}$ e $F_1, F_2, \ldots, F_{n-1}$, que transformam $\Omega$ em matrizes diagonais $D_E$ e $D_F$, respectivamente. Assim:

$$E_{n-1} \cdots E_2 E_1 \Omega E_1' E_2' \cdots E_{n-1}' = D_E$$
$$F_{n-1} \cdots F_2 F_1 \Omega F_1' F_2' \cdots F_{n-1}' = D_F$$

II. Definimos $A_E = (E_{n-1} \cdots E_1)^{-1}$ e $A_F = (F_{n-1} \cdots F_1)^{-1}$. Ent√£o, podemos reescrever as equa√ß√µes acima como:
$$\Omega = A_E D_E A_E'$$
$$\Omega = A_F D_F A_F'$$

III. Pela unicidade da fatora√ß√£o triangular, sabemos que a representa√ß√£o de $\Omega$ como o produto de uma matriz triangular inferior, uma matriz diagonal e a transposta da matriz triangular √© √∫nica. Portanto, devemos ter $A_E = A_F$ e $D_E = D_F$.

IV. Como $A_E = A_F$, temos que:
$$(E_{n-1} \cdots E_1)^{-1} = (F_{n-1} \cdots F_1)^{-1}$$

V. Invertendo ambos os lados da equa√ß√£o:
$$E_{n-1} \cdots E_1 = F_{n-1} \cdots F_1$$

VI. Multiplicando ambos os lados pela inversa de $(E_1)$ pela direita, temos:
$$E_{n-1} \cdots E_2 = F_{n-1} \cdots F_2 (F_1 E_1^{-1})$$

VII. Como $E_1$ e $F_1$ s√£o matrizes que zeram a primeira coluna abaixo da diagonal de $\Omega$, temos que $F_1 E_1^{-1} = I$, ent√£o
$$E_{n-1} \cdots E_2 = F_{n-1} \cdots F_2$$
Repetindo o processo, obtemos $E_k = F_k$ para todo $k$.

VIII. Portanto, a sequ√™ncia de matrizes $E_k$ utilizada na transforma√ß√£o de $\Omega$ em $D$ √© √∫nica. $\blacksquare$

### Conclus√£o
A transforma√ß√£o de uma matriz sim√©trica positiva definida $\Omega$ em uma matriz diagonal *D* por meio de multiplica√ß√µes iterativas com matrizes triangulares inferiores *$E_k$* e suas transpostas √© um processo fundamental na fatora√ß√£o triangular e de Cholesky. Este processo fornece um meio computacionalmente eficiente para decompor $\Omega$, facilitando diversas opera√ß√µes, incluindo a gera√ß√£o de n√∫meros aleat√≥rios e a solu√ß√£o de sistemas lineares [^4.5.12]. A unicidade da sequ√™ncia de matrizes *$E_k$* garante que a transforma√ß√£o seja bem definida e consistente, refor√ßando a import√¢ncia dessas t√©cnicas em diversas √°reas da matem√°tica, estat√≠stica e computa√ß√£o.

### Refer√™ncias
[^4.4.1]: Qualquer matriz sim√©trica positiva definida $\Omega$ pode ser expressa como $\Omega = ADA'$, onde A √© uma matriz triangular inferior com 1s na diagonal principal, e D √© uma matriz diagonal com elementos positivos.
[^4.4.2]: Para ver como a fatoriza√ß√£o triangular pode ser calculada, considere...
[^4.4.3]: Esta matriz sempre existe, desde que $\Omega_{11} \neq 0$. Isso √© garantido no caso atual, porque $\Omega_{11}$ √© igual a $e_1\Omega e_1$, onde $e_1 = [1 \, 0 \, 0 \, \ldots \, 0]$. Como $\Omega$ √© positiva definida, $e_1\Omega e_1$ deve ser maior que zero.
[^4.4.4]: Quando $\Omega$ √© pr√©-multiplicado por $E_1$ e p√≥s-multiplicado por $E_1'$, o resultado √©...
[^4.4.7]: ...para qualquer matriz sim√©trica positiva definida $\Omega$, existem matrizes $E_1, E_2, \ldots, E_{n-1}$ tais que $E_{n-1} \ldots E_2 E_1 \Omega E_1' E_2' \ldots E_{n-1}' = D$,
[^4.4.8]: Assim, existe uma matriz A...
[^4.4.15]: Multiplicando √† esquerda [4.4.14] por $D_1^{-1}A_1^{-1}$ e multiplicando √† direita por $[A_2']^{-1}$ se obt√©m...
[^4.5.12]: ...o √≥timo forecast de Y3 condicional em Y2 e Y1 pode ser lido da √∫ltima linha em bloco de A: $\hat{P}(Y_3|Y_2,Y_1) = \Omega_{31} \Omega_{11}^{-1} Y_1 + H_{32}H_{22}^{-1} (Y_2 - \Omega_{21}\Omega_{11}^{-1} Y_1) = P(Y_3|Y_1) + H_{32}H_{22}^{-1} [Y_2-P(Y_2|Y_1)]$
<!-- END -->
