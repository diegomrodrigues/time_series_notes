## O Elemento (j,i) de $E^{-1}$ e sua Rela√ß√£o com a Matriz A na Fatora√ß√£o Triangular

### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise da **fatora√ß√£o triangular** de uma matriz sim√©trica positiva definida $\Omega$, explorando a rela√ß√£o entre os elementos das matrizes inversas $E^{-1}$ e os elementos da matriz triangular inferior $A$ [^4.4.8]. Como vimos anteriormente, a fatora√ß√£o triangular de $\Omega$ √© expressa como $\Omega = ADA'$, onde $A$ √© uma matriz triangular inferior com 1s na diagonal e $D$ √© uma matriz diagonal. A constru√ß√£o das matrizes $A$ e $D$ envolve uma s√©rie de pr√© e p√≥s-multiplica√ß√µes iterativas de $\Omega$ por matrizes triangulares inferiores $E_k$ e suas transpostas. As matrizes inversas $E_k^{-1}$ desempenham um papel crucial na defini√ß√£o da matriz $A$, sendo que seus elementos fornecem uma compreens√£o da estrutura da matriz resultante.

Este cap√≠tulo avan√ßa o conhecimento constru√≠do nos cap√≠tulos anteriores sobre a fatora√ß√£o triangular e sua constru√ß√£o iterativa, concentrando-se na an√°lise da rela√ß√£o entre as matrizes $E_k$ e a matriz $A$. Este aprofundamento √© fundamental para consolidar o conhecimento te√≥rico e a intui√ß√£o sobre os fundamentos da fatora√ß√£o triangular, preparando o terreno para aplica√ß√µes computacionais mais avan√ßadas.

### Rela√ß√£o entre $E_k^{-1}$ e a Matriz A

Na constru√ß√£o da fatora√ß√£o triangular, a matriz sim√©trica positiva definida $\Omega$ √© transformada em uma matriz diagonal $D$ atrav√©s de uma sequ√™ncia de pr√© e p√≥s-multiplica√ß√µes por matrizes triangulares inferiores $E_k$ e suas transpostas:

$$ D = E_{n-1} \ldots E_2 E_1 \Omega E_1' E_2' \ldots E_{n-1}' $$

A matriz triangular inferior $A$ √© obtida como o inverso do produto das matrizes $E_k$:

$$ A = (E_{n-1} \ldots E_2 E_1)^{-1} $$

Uma propriedade importante √© que a inversa de cada matriz $E_k$ √© facilmente calculada trocando o sinal dos elementos abaixo da diagonal principal, como estabelecido no **Lema 1** [^4.4.10]. Isso simplifica o c√°lculo da matriz $A$, pois a mesma pode ser obtida como o produto das inversas das matrizes $E_k$:

$$ A = E_1^{-1} E_2^{-1} \ldots E_{n-1}^{-1} $$

A matriz $A$, portanto, resulta do produto das matrizes $E_k^{-1}$. Uma rela√ß√£o fundamental √© que, como demonstrado a seguir, o elemento (j, i) da matriz $A$ corresponde ao elemento (j, i) da matriz $E_i^{-1}$.

**Teorema 1**
O elemento (j,i) da matriz A √© igual ao elemento (j,i) da matriz $E_i^{-1}$ para $i \leq j$.

*Proof*:
A matriz $A$ √© dada por $A = (E_{n-1} \ldots E_1)^{-1} = E_1^{-1} E_2^{-1} \ldots E_{n-1}^{-1}$. Sejam $A_k = E_1^{-1} E_2^{-1} \ldots E_k^{-1}$. Mostraremos por indu√ß√£o que o elemento (j,i) de $A_k$ √© igual ao elemento (j,i) de $E_i^{-1}$ para $i \leq j \leq k$.

1. Base: Para $k = 1$, $A_1 = E_1^{-1}$. O elemento (j,i) de $A_1$ √© igual ao elemento (j,i) de $E_1^{-1}$.

2. Passo indutivo: Suponha que o elemento (j,i) de $A_k$ √© igual ao elemento (j,i) de $E_i^{-1}$ para $i \leq j \leq k$. Vamos mostrar que o elemento (j,i) de $A_{k+1}$ √© igual ao elemento (j,i) de $E_i^{-1}$ para $i \leq j \leq k+1$. A matriz $A_{k+1}$ √© dada por $A_{k+1} = A_k E_{k+1}^{-1}$. O elemento (j, i) de $A_{k+1}$ √©:
$$ (A_{k+1})_{ji} = \sum_{l=1}^{n} (A_k)_{jl} (E_{k+1}^{-1})_{li} $$
Se $i>k+1$, ent√£o o elemento $(E_{k+1}^{-1})_{li}$ √© zero para todo $l$. Assim o elemento (j,i) de $A_{k+1}$ √© zero para $i>k+1$.
Como $E_{k+1}^{-1}$ √© uma matriz triangular inferior com 1s na diagonal, sabemos que $(E_{k+1}^{-1})_{li}=0$ se $l > i$ e $(E_{k+1}^{-1})_{li}=1$ se $l = i$. Se $i \le j \le k$, ent√£o pela hip√≥tese de indu√ß√£o, o elemento $(A_k)_{ji} = (E_i^{-1})_{ji}$. Assim, para $i \leq j \leq k$,
$$(A_{k+1})_{ji} = \sum_{l=1}^{n} (A_k)_{jl} (E_{k+1}^{-1})_{li} =  (A_k)_{ji} (E_{k+1}^{-1})_{ii} +  \sum_{l<i}^{n} (A_k)_{jl} (E_{k+1}^{-1})_{li}  = (A_k)_{ji} = (E_i^{-1})_{ji}  $$
Como $i \le j \le k+1$, ent√£o o elemento $(A_{k+1})_{ji}  = (E_i^{-1})_{ji}$ para $i \le j \le k+1$.
Para o caso $j=k+1$,  note que a matriz $E_{k+1}^{-1}$ tem o elemento $(k+1, k+1) = 1$ e os elementos em posi√ß√µes $(k+1, l)$ com $l<k+1$ (i.e, na coluna $k+1$) tem valores que correspondem aos multiplicadores que transformam a coluna de $k+1$ em zeros. Logo, o elemento $(A_{k+1})_{k+1, i}$ √© igual ao elemento $(E_{i}^{-1})_{k+1, i}$ por constru√ß√£o.
Portanto, por indu√ß√£o, para todo $i \le j$, o elemento $(j,i)$ de $A$ corresponde ao elemento $(j,i)$ de $E_i^{-1}$.  $\blacksquare$

*Proof do Teorema 1*:
I. A matriz $A$ √© dada pelo produto das inversas das matrizes $E_k$:
    $$A = E_1^{-1} E_2^{-1} \ldots E_{n-1}^{-1}$$

II. Definimos $A_k = E_1^{-1} E_2^{-1} \ldots E_k^{-1}$. Assim, $A = A_{n-1}$. Mostraremos que o elemento (j, i) de $A_k$ √© igual ao elemento (j, i) de $E_i^{-1}$ para $i \leq j \leq k$ utilizando indu√ß√£o.

III. **Base:** Para $k = 1$, temos $A_1 = E_1^{-1}$. Logo, o elemento (j, i) de $A_1$ √© igual ao elemento (j, i) de $E_1^{-1}$ para $i \leq j \leq 1$, ou seja, para $i=j=1$.

IV. **Passo Indutivo:** Suponhamos que o elemento (j, i) de $A_k$ √© igual ao elemento (j, i) de $E_i^{-1}$ para $i \leq j \leq k$. Mostraremos que o elemento (j, i) de $A_{k+1}$ √© igual ao elemento (j, i) de $E_i^{-1}$ para $i \leq j \leq k+1$.

V. A matriz $A_{k+1}$ √© dada por $A_{k+1} = A_k E_{k+1}^{-1}$. O elemento (j, i) de $A_{k+1}$ √©:
    $$(A_{k+1})_{ji} = \sum_{l=1}^{n} (A_k)_{jl} (E_{k+1}^{-1})_{li}$$

VI. Se $i > k+1$, ent√£o o elemento $(E_{k+1}^{-1})_{li}$ √© zero para todo $l$, e portanto, o elemento (j, i) de $A_{k+1}$ √© zero, confirmando o resultado.

VII. Como $E_{k+1}^{-1}$ √© uma matriz triangular inferior com 1s na diagonal, sabemos que $(E_{k+1}^{-1})_{li}=0$ se $l > i$ e $(E_{k+1}^{-1})_{ii}=1$. Se $i \le j \le k$, ent√£o pela hip√≥tese de indu√ß√£o, o elemento $(A_k)_{ji} = (E_i^{-1})_{ji}$. Assim, para $i \leq j \leq k$, temos:
$$(A_{k+1})_{ji} = \sum_{l=1}^{n} (A_k)_{jl} (E_{k+1}^{-1})_{li} =  (A_k)_{ji} (E_{k+1}^{-1})_{ii} +  \sum_{l<i}^{n} (A_k)_{jl} (E_{k+1}^{-1})_{li} = (A_k)_{ji} = (E_i^{-1})_{ji}$$
Como $i \le j \le k+1$, ent√£o o elemento $(A_{k+1})_{ji}  = (E_i^{-1})_{ji}$ para $i \le j \le k+1$.

VIII. Para o caso $j=k+1$, note que a matriz $E_{k+1}^{-1}$ tem o elemento $(k+1, k+1) = 1$ e os elementos em posi√ß√µes $(k+1, l)$ com $l<k+1$ (i.e, na coluna $k+1$) tem valores que correspondem aos multiplicadores que transformam a coluna de $k+1$ em zeros. Logo, o elemento $(A_{k+1})_{k+1, i}$ √© igual ao elemento $(E_{i}^{-1})_{k+1, i}$ por constru√ß√£o.

IX. Portanto, por indu√ß√£o, para todo $i \le j$, o elemento $(j,i)$ de $A$ corresponde ao elemento $(j,i)$ de $E_i^{-1}$. $\blacksquare$

Este resultado estabelece uma conex√£o direta entre os elementos das matrizes $E_k^{-1}$ e a estrutura da matriz $A$.

> üí° **Exemplo Num√©rico:**
>
> Vamos ilustrar este resultado utilizando o exemplo anterior, onde calculamos:
>
> $$ E_1^{-1} = \begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.5 & 0 & 1 \end{bmatrix} $$
>
> $$ E_2^{-1} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0.5 & 1 \end{bmatrix} $$
>
> E obtivemos a matriz *A*:
>
> $$ A = \begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.5 & 0.5 & 1 \end{bmatrix} $$
>
>  De acordo com o teorema 1, o elemento (2,1) de A deve ser igual ao elemento (2,1) de $E_1^{-1}$. Notamos que $(A)_{21} = 0.5 = (E_1^{-1})_{21}$. Similarmente, o elemento (3,1) de $A$ deve ser igual ao elemento (3,1) de $E_1^{-1}$, logo $(A)_{31} = 0.5 = (E_1^{-1})_{31}$. O elemento (3,2) de $A$ deve ser igual ao elemento (3,2) de $E_2^{-1}$, logo $(A)_{32} = 0.5 = (E_2^{-1})_{32}$. Por fim, o elemento (1,1) da matriz A corresponde ao elemento (1,1) da matriz $E_1^{-1}$, ou seja, $(A)_{11}=1=(E_1^{-1})_{11}$.
>
> Podemos verificar que a coluna 1 de A corresponde √† coluna 1 de $E_1^{-1}$, e que a coluna 2 de A corresponde √† coluna 2 de $E_2^{-1}$ abaixo da diagonal.

**Consequ√™ncias da Rela√ß√£o entre $E_k^{-1}$ e A**

A rela√ß√£o entre os elementos de $E_k^{-1}$ e $A$ √© muito √∫til na pr√°tica por diversas raz√µes:
1.  **C√°lculo eficiente de A**: Para calcular a matriz *A*, n√£o √© necess√°rio multiplicar todas as matrizes $E_k^{-1}$. √â poss√≠vel construir a matriz *A* coletando os elementos relevantes das matrizes $E_k^{-1}$ diretamente [^4.4.11].
2.  **Intui√ß√£o sobre a constru√ß√£o de A**: A rela√ß√£o explicita o papel de cada matriz $E_k^{-1}$ na constru√ß√£o de $A$, fornecendo uma melhor compreens√£o do processo de fatora√ß√£o triangular.
3.  **Implementa√ß√£o computacional**: A propriedade apresentada simplifica a implementa√ß√£o computacional de algoritmos de fatora√ß√£o triangular, uma vez que os elementos de *A* s√£o obtidos de forma direta a partir das matrizes $E_k$.
4.  **Generaliza√ß√£o**: O resultado apresentado se aplica √† fatora√ß√£o triangular de matrizes em blocos, o que torna o resultado ainda mais valioso.

**Teorema 1.1**
A matriz A √© a inversa do produto das matrizes $E_k$, ou seja, $A = (E_{n-1} \ldots E_1)^{-1}$. Al√©m disso, cada coluna *i* da matriz *A* corresponde √† coluna *i* da matriz $E_i^{-1}$.

*Proof:*

O Teorema 1 j√° estabelece que o elemento (j,i) de A √© igual ao elemento (j,i) de $E_i^{-1}$ para $i \leq j$. Como $A = E_1^{-1} E_2^{-1} \ldots E_{n-1}^{-1}$, a matriz A √© o produto das inversas das matrizes $E_k$. Al√©m disso, pela demonstra√ß√£o do Teorema 1, cada elemento (j, i) de A √© determinado pelo elemento (j, i) da matriz $E_i^{-1}$ quando $i \leq j$. Se tomarmos uma coluna *i* de *A*, seus elementos (j, i) para todos os valores poss√≠veis de *j* correspondem aos elementos da coluna *i* de $E_i^{-1}$. Deste modo, a coluna *i* de *A* coincide com a coluna *i* de $E_i^{-1}$. $\blacksquare$

**Lema 1.1**
Cada matriz $E_k^{-1}$ √© uma matriz triangular inferior com 1s na diagonal principal e os elementos abaixo da diagonal s√£o os negativos dos elementos correspondentes da matriz $E_k$.

*Proof*:
Esta propriedade foi estabelecida previamente no Lema 1 [^4.4.10] e √© fundamental na constru√ß√£o e manipula√ß√£o das matrizes $E_k$ e suas inversas.  Especificamente, as matrizes $E_k$ s√£o constru√≠das de forma que os elementos abaixo da diagonal principal s√£o os multiplicadores utilizados durante a elimina√ß√£o de Gauss, e ao inverter a matriz, estes elementos apenas trocam o sinal. $\blacksquare$

**Corol√°rio 1.1**
O elemento (j, i) da matriz A √© zero para $i>j$.

*Proof:*
A matriz *A* √© uma matriz triangular inferior. Isso significa que todos os elementos acima da diagonal principal s√£o zero, ou seja, $(A)_{ji}=0$ para $i>j$. O resultado segue diretamente do Teorema 1, j√° que os elementos de $A$ correspondem aos elementos de $E_i^{-1}$, que s√£o iguais a zero para $i>j$ por constru√ß√£o. $\blacksquare$

**Exemplo Num√©rico (Visualiza√ß√£o da Conex√£o):**

Retomando o exemplo num√©rico e as matrizes $E_1^{-1}$ e $E_2^{-1}$:

$$ E_1^{-1} = \begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.5 & 0 & 1 \end{bmatrix} \quad \text{e} \quad  E_2^{-1} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0.5 & 1 \end{bmatrix}$$

E a matriz $A$:

$$ A = \begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.5 & 0.5 & 1 \end{bmatrix} $$

Podemos observar que:

*   O elemento $A_{11} = 1$  corresponde ao elemento $ (E_1^{-1})_{11} = 1$.
*   O elemento $A_{21} = 0.5$  corresponde ao elemento $(E_1^{-1})_{21} = 0.5$.
*   O elemento $A_{31} = 0.5$ corresponde ao elemento $(E_1^{-1})_{31} = 0.5$.
*   O elemento $A_{32} = 0.5$ corresponde ao elemento $(E_2^{-1})_{32} = 0.5$.

Este exemplo ilustra a rela√ß√£o entre os elementos das matrizes inversas $E_k^{-1}$ e os elementos da matriz $A$.

### Conclus√£o
O elemento (j,i) das matrizes inversas $E_k^{-1}$ corresponde diretamente ao elemento (j,i) da matriz $A$ na fatora√ß√£o triangular. As matrizes $E_k^{-1}$ s√£o obtidas ao inverter as matrizes $E_k$, que s√£o matrizes triangulares inferiores com 1s na diagonal principal. Esta conex√£o simplifica os c√°lculos da matriz *A*, o que facilita a implementa√ß√£o computacional da fatora√ß√£o triangular e fornece uma compreens√£o mais profunda da estrutura da decomposi√ß√£o. A unicidade deste processo, juntamente com o Lema 1 e o Teorema 1 e 2, destaca a import√¢ncia da fatora√ß√£o triangular na an√°lise de matrizes sim√©tricas positivas definidas, sendo uma ferramenta computacional valiosa para diversas aplica√ß√µes em estat√≠stica e computa√ß√£o.

### Refer√™ncias
[^4.4.8]: Assim, existe uma matriz A...
[^4.4.10]: ...o inverso de $E_1$ √©...
[^4.4.11]: ...a j-√©sima coluna de $A$ √© a j-√©sima coluna de $E_j^{-1}$

5.2. Likelihood Function for an AR(1)
Process

Consider the AR(1) model:
$$Y_t = c + \phi Y_{t-1} + \epsilon_t,$$
where $\epsilon_t \sim i.i.d. N(0, \sigma^2)$ [^5.1.5]. The likelihood function is the joint density of the observed data, which in this case are $Y_1, Y_2, \ldots, Y_T$. Given that $\epsilon_t$ is normally distributed, $Y_t$ conditional on $Y_{t-1}$ is also normally distributed:
$$Y_t | Y_{t-1} \sim N(c + \phi Y_{t-1}, \sigma^2).$$

The joint density of the sample conditional on the initial value $Y_0$ can be expressed as the product of the conditional densities:
$$f_{Y_1, \ldots, Y_T} (y_1, \ldots, y_T | Y_0; \theta) = \prod_{t=1}^T f_{Y_t | Y_{t-1}} (y_t | y_{t-1}; \theta),$$
where $\theta = (c, \phi, \sigma^2)$ represents the vector of parameters. The conditional density of each $Y_t$ is given by
$$f_{Y_t | Y_{t-1}} (y_t | y_{t-1}; \theta) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(y_t - c - \phi y_{t-1})^2}{2\sigma^2} \right).$$
Substituting this into the joint density expression, we obtain the likelihood function:
$$L(\theta; y_1, \ldots, y_T | Y_0) = \prod_{t=1}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(y_t - c - \phi y_{t-1})^2}{2\sigma^2} \right).$$
It is often more convenient to work with the log-likelihood function, which is obtained by taking the natural logarithm of the likelihood function:
$$\log L(\theta; y_1, \ldots, y_T | Y_0) = \sum_{t=1}^T \left[ -\frac{1}{2} \log(2\pi\sigma^2) - \frac{(y_t - c - \phi y_{t-1})^2}{2\sigma^2} \right].$$
$$\log L(\theta; y_1, \ldots, y_T | Y_0) = -\frac{T}{2} \log(2\pi) - \frac{T}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=1}^T (y_t - c - \phi y_{t-1})^2.$$
This log-likelihood function is the objective function we will maximize with respect to the parameters $c$, $\phi$, and $\sigma^2$ to obtain the maximum likelihood estimates.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos uma s√©rie temporal com T=100 observa√ß√µes gerada por um processo AR(1) com par√¢metros verdadeiros $c=10$, $\phi=0.7$ e $\sigma^2=4$. Vamos gerar uma s√©rie temporal e calcular a log-verossimilhan√ßa para alguns par√¢metros diferentes.
>
> ```python
> import numpy as np
> import scipy.optimize as optimize
>
> # Par√¢metros verdadeiros
> true_c = 10
> true_phi = 0.7
> true_sigma2 = 4
> T = 100
>
> # Gerar dados AR(1)
> np.random.seed(42)
> errors = np.random.normal(0, np.sqrt(true_sigma2), T)
> Y = np.zeros(T)
> Y[0] = true_c + errors[0] # Inicializa√ß√£o usando a m√©dia do processo
> for t in range(1, T):
>     Y[t] = true_c + true_phi * Y[t-1] + errors[t]
>
> # Fun√ß√£o de log-verossimilhan√ßa
> def log_likelihood_ar1(params, data):
>     c, phi, sigma2 = params
>     T = len(data)
>     ll = -T/2 * np.log(2*np.pi) - T/2 * np.log(sigma2) - 1/(2*sigma2) * np.sum((data[1:] - c - phi*data[:-1])**2)
>     return -ll # Retornamos o negativo para maximiza√ß√£o
>
> # Testando a fun√ß√£o com alguns par√¢metros
> params1 = [10, 0.7, 4] # Par√¢metros verdadeiros
> params2 = [9, 0.6, 3]
> params3 = [11, 0.8, 5]
>
> ll1 = -log_likelihood_ar1(params1, Y)
> ll2 = -log_likelihood_ar1(params2, Y)
> ll3 = -log_likelihood_ar1(params3, Y)
>
> print(f"Log-verossimilhan√ßa para os par√¢metros verdadeiros: {ll1:.2f}")
> print(f"Log-verossimilhan√ßa para params2: {ll2:.2f}")
> print(f"Log-verossimilhan√ßa para params3: {ll3:.2f}")
>
> # Otimiza√ß√£o
> initial_params = [0,0,1]
> result = optimize.minimize(log_likelihood_ar1, initial_params, args=(Y,), method='BFGS')
> print("Resultados da otimiza√ß√£o:")
> print(result.x)
> print(f"Log-verossimilhan√ßa otimizada: {-result.fun:.2f}")
> ```
>
> Este exemplo ilustra como calculamos a log-verossimilhan√ßa para diferentes valores de par√¢metros e como a otimiza√ß√£o num√©rica pode encontrar os valores que maximizam essa fun√ß√£o. Note que a log-verossimilhan√ßa √© maior para os par√¢metros que est√£o mais pr√≥ximos dos par√¢metros verdadeiros. A otimiza√ß√£o resulta em estimativas pr√≥ximas dos par√¢metros verdadeiros e em um valor de log-verossimilhan√ßa maior que os testados.

5.3. Likelihood Function for an AR(p) Process
The generalization to an AR(p) model, given by
$$Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t,$$
is relatively straightforward. The conditional distribution of $Y_t$ given its past values is
$$Y_t | Y_{t-1}, Y_{t-2}, \ldots, Y_{t-p} \sim N(c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p}, \sigma^2).$$
The log-likelihood function is then given by
$$\log L(\theta; y_1, \ldots, y_T | Y_0, \ldots, Y_{1-p}) = -\frac{T}{2} \log(2\pi) - \frac{T}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=1}^T (y_t - c - \phi_1 y_{t-1} - \ldots - \phi_p y_{t-p})^2,$$
where $\theta = (c, \phi_1, \ldots, \phi_p, \sigma^2)$. Here, $Y_0, Y_{-1}, \ldots, Y_{1-p}$ represent the initial values required to start the process. The maximization of this log-likelihood function will provide the MLEs of the AR(p) parameters.

> üí° **Exemplo Num√©rico:**
>
> Vamos supor um processo AR(2) com $c = 5$, $\phi_1 = 0.6$, $\phi_2 = 0.2$ e $\sigma^2 = 1$. Para calcular a verossimilhan√ßa, precisamos de duas observa√ß√µes passadas ($Y_0$ e $Y_{-1}$). Criaremos um conjunto de dados simulados para exemplificar.
>
> ```python
> import numpy as np
> import scipy.optimize as optimize
>
> # Par√¢metros verdadeiros AR(2)
> true_c = 5
> true_phi1 = 0.6
> true_phi2 = 0.2
> true_sigma2 = 1
> T = 100
>
> # Gerar dados AR(2)
> np.random.seed(42)
> errors = np.random.normal(0, np.sqrt(true_sigma2), T + 2)
> Y = np.zeros(T + 2)
> Y[0] = true_c + errors[0]
> Y[1] = true_c + true_phi1*Y[0] + errors[1]
> for t in range(2, T + 2):
>     Y[t] = true_c + true_phi1 * Y[t-1] + true_phi2 * Y[t-2] + errors[t]
>
> data = Y[2:] # Excluimos as duas primeiras obs para o c√°lculo da verossimilhan√ßa
> # Fun√ß√£o de log-verossimilhan√ßa AR(2)
> def log_likelihood_ar2(params, data):
>     c, phi1, phi2, sigma2 = params
>     T = len(data)
>     ll = -T/2 * np.log(2*np.pi) - T/2 * np.log(sigma2) - 1/(2*sigma2) * np.sum((data[2:] - c - phi1*data[1:-1] - phi2*data[:-2])**2)
>     return -ll
>
> # Testando a fun√ß√£o com alguns par√¢metros
> params_true = [5, 0.6, 0.2, 1]
> params_test = [4, 0.5, 0.1, 0.9]
>
> ll_true = -log_likelihood_ar2(params_true, Y)
> ll_test = -log_likelihood_ar2(params_test, Y)
>
> print(f"Log-verossimilhan√ßa para par√¢metros verdadeiros: {ll_true:.2f}")
> print(f"Log-verossimilhan√ßa para par√¢metros de teste: {ll_test:.2f}")
>
> # Otimiza√ß√£o
> initial_params = [0,0,0,1]
> result = optimize.minimize(log_likelihood_ar2, initial_params, args=(Y,), method='BFGS')
> print("Resultados da otimiza√ß√£o:")
> print(result.x)
> print(f"Log-verossimilhan√ßa otimizada: {-result.fun:.2f}")
> ```
>
> Este exemplo mostra como a log-verossimilhan√ßa √© calculada para um processo AR(2), ilustrando a generaliza√ß√£o da fun√ß√£o para modelos de ordem superior. A otimiza√ß√£o resulta em estimativas pr√≥ximas aos par√¢metros verdadeiros.

5.4. Likelihood Function for an MA(q) Process
The MA(q) process is defined by
$$Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \ldots + \theta_q \epsilon_{t-q},$$
where $\epsilon_t \sim i.i.d. N(0, \sigma^2)$. The likelihood function for the MA(q) process is slightly more complex than for AR processes because we do not directly observe $\epsilon_t$.  Therefore, we need to express the likelihood in terms of observable quantities. We can write the innovation recursively as:
$$\epsilon_t = Y_t - \mu - \theta_1 \epsilon_{t-1} - \ldots - \theta_q \epsilon_{t-q}.$$
The log-likelihood function for a Gaussian MA(q) model can be written as:
$$\log L(\theta; y_1, \ldots, y_T | \epsilon_0, \ldots, \epsilon_{1-q}) = -\frac{T}{2} \log(2\pi) - \frac{T}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=1}^T \epsilon_t^2,$$
where $\theta = (\mu, \theta_1, \ldots, \theta_q, \sigma^2)$.  We typically need initial values $\epsilon_0, \epsilon_{-1}, \ldots, \epsilon_{1-q}$  to start this calculation.  A common practical approach is to set these initial errors to zero, and treat them as fixed constants rather than random variables: $\epsilon_0 = \epsilon_{-1} = \ldots = \epsilon_{1-q} = 0$.  The errors $\epsilon_t$ are then calculated recursively using the observed values for $Y_t$, and substituting the estimates of $\mu$ and $\theta_i$ obtained at each iteration of the optimization.

> üí° **Exemplo Num√©rico:**
>
> Suponha um processo MA(1) com $\mu = 2$, $\theta_1 = 0.8$ e $\sigma^2 = 0.5$.  Calcularemos a log-verossimilhan√ßa para uma s√©rie temporal simulada.
>
> ```python
> import numpy as np
> import scipy.optimize as optimize
>
> # Par√¢metros verdadeiros do MA(1)
> true_mu = 2
> true_theta1 = 0.8
> true_sigma2 = 0.5
> T = 100
>
> # Gerando os erros e a s√©rie MA(1)
> np.random.seed(42)
> errors = np.random.normal(0, np.sqrt(true_sigma2), T + 1) # um erro extra para inicializar
> Y = np.zeros(T)
>
> # Inicializando as s√©ries
> eps = np.zeros(T + 1)
>
> for t in range(1, T + 1):
>   Y[t-1] = true_mu + eps[t] + true_theta1 * eps[t-1]
>
>
> # Fun√ß√£o da log-verossimilhan√ßa para MA(1)
> def log_likelihood_ma1(params, data):
>   mu, theta1, sigma2 = params
>   T = len(data)
>   eps = np.zeros(T+1)
>   for t in range(1, T + 1):
>     eps[t] = data[t-1] - mu - theta1 * eps[t-1]
>   ll = -T/2 * np.log(2*np.pi) - T/2 * np.log(sigma2) - 1/(2*sigma2) * np.sum(eps[1:]**2)
>   return -ll
>
> # Testando a fun√ß√£o com par√¢metros
> params_true = [2, 0.8, 0.5]
> params_test = [1, 0.7, 0.4]
>
> ll_true = -log_likelihood_ma1(params_true, Y)
> ll_test = -log_likelihood_ma1(params_test, Y)
>
> print(f"Log-verossimilhan√ßa para par√¢metros verdadeiros: {ll_true:.2f}")
> print(f"Log-verossimilhan√ßa para par√¢metros de teste: {ll_test:.2f}")
>
> # Otimiza√ß√£o
> initial_params = [0, 0, 1]
> result = optimize.minimize(log_likelihood_ma1, initial_params, args=(Y,), method='BFGS')
> print("Resultados da otimiza√ß√£o:")
> print(result.x)
```python
# Par√¢metros √≥timos
phi_opt, theta_opt, sigma_opt = result.x

# Intervalo de confian√ßa
hessian_inv = result.hess_inv.todense()
std_errors = np.sqrt(np.diag(hessian_inv))
conf_intervals = []
for i, param_opt in enumerate(result.x):
    conf_intervals.append((param_opt - 1.96 * std_errors[i], param_opt + 1.96 * std_errors[i]))

print("Intervalos de confian√ßa:")
print(conf_intervals)


# AIC e BIC
n = len(Y)
k = len(initial_params)
aic = -2 * result.fun + 2 * k
bic = -2 * result.fun + k * np.log(n)

print(f"AIC: {aic:.2f}")
print(f"BIC: {bic:.2f}")

# Previs√µes
def predict_ma1(params, data, steps=1):
    phi, theta, sigma = params
    n = len(data)
    predictions = []
    errors = [0,0] # inicializar com dois erros 0, necess√°rios para o modelo MA(1)
    
    for _ in range(steps):
        predicted_value = phi * data[-1] + theta * errors[-1]
        predictions.append(predicted_value)
        errors.append(data[-1] - predicted_value)  # erro de previs√£o
        data = np.append(data, predicted_value)
    return np.array(predictions)


# Prever os pr√≥ximos 5 valores
future_predictions = predict_ma1(result.x, Y, steps=5)

print("Pr√≥ximas 5 previs√µes:")
print(future_predictions)


# Visualiza√ß√£o
plt.figure(figsize=(12, 6))
plt.plot(range(len(Y)), Y, label="Dados Observados")
plt.plot(range(len(Y), len(Y) + len(future_predictions)), future_predictions, marker='o', linestyle='--', label="Previs√µes")
plt.xlabel("Tempo")
plt.ylabel("Valor")
plt.title("S√©rie Temporal e Previs√µes MA(1)")
plt.legend()
plt.grid(True)
plt.show()
```
<!-- END -->
