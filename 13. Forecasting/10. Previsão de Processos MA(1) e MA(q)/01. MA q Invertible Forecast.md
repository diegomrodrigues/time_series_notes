## Previs√£o para Processos MA(q) Invert√≠veis
### Introdu√ß√£o
Este cap√≠tulo aborda a previs√£o em modelos de s√©ries temporais, especificamente os modelos de m√©dias m√≥veis (MA). Como vimos anteriormente, a previs√£o √≥tima para um processo estoc√°stico √© dada pela expectativa condicional do valor futuro, dado o hist√≥rico das observa√ß√µes [^4.1.2]. Aqui, exploraremos em detalhes como calcular essas previs√µes para processos MA(q) invert√≠veis, estendendo os conceitos j√° introduzidos para processos MA(1) [^4.2.28]. O uso do operador de retardo desempenha um papel crucial neste processo, nos permitindo expressar previs√µes de forma compacta e eficiente.

### Conceitos Fundamentais
Em um processo MA(q), a vari√°vel $Y_t$ √© expressa como uma combina√ß√£o linear de ru√≠dos brancos passados $\epsilon_t$:

$$Y_t - \mu = (1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q)\epsilon_t$$ [^4.2.1]

Onde $\mu$ √© a m√©dia do processo, $L$ √© o operador de retardo ($L^j \epsilon_t = \epsilon_{t-j}$), e $\theta_1, \theta_2, \ldots, \theta_q$ s√£o os par√¢metros do modelo. A invertibilidade √© uma propriedade crucial para realizar previs√µes com modelos MA, garantindo que o processo possa ser expresso em termos de seus valores passados.

**Observa√ß√£o 1:** A condi√ß√£o de invertibilidade para um processo MA(q) √© que as ra√≠zes do polin√¥mio caracter√≠stico $1 + \theta_1 z + \theta_2 z^2 + \ldots + \theta_q z^q = 0$ estejam fora do c√≠rculo unit√°rio no plano complexo. Essa condi√ß√£o √© an√°loga √† condi√ß√£o de estacionariedade para processos AR e garante que a representa√ß√£o em termos de valores passados seja convergente.

A previs√£o √≥tima de $Y_{t+s}$ com base em um hist√≥rico de ru√≠dos passados, denotado por $\hat{Y}_{t+s|t}$, √© obtida definindo os valores futuros de $\epsilon$ como zero [^4.2.4].  Para derivar a previs√£o para um processo MA(q) invert√≠vel, podemos utilizar a express√£o [^4.2.16]:

$$\hat{Y}_{t+s|t} = \mu + \left[\frac{1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q}{L^s}\right]_+ \frac{1}{1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q}(Y_t - \mu)$$ [^4.2.34]

Aqui, o operador $[ \cdot ]_+$ indica o operador de aniquila√ß√£o, que substitui os termos com pot√™ncias negativas de $L$ por zero [^4.2.8]. Essa opera√ß√£o garante que a previs√£o seja uma fun√ß√£o apenas dos valores passados e presentes, conforme os ru√≠dos futuros sejam desconhecidos e, portanto, definidos como zero.

> üí° **Exemplo Num√©rico:** Considere um processo MA(2) com par√¢metros $\theta_1 = 0.5$ e $\theta_2 = 0.3$, e m√©dia $\mu = 10$. O polin√¥mio caracter√≠stico √© $1 + 0.5z + 0.3z^2 = 0$. Para verificar a invertibilidade, precisamos encontrar as ra√≠zes deste polin√¥mio. As ra√≠zes s√£o aproximadamente $z_1 = -1.11$ e $z_2 = -0.55$. Como ambas as ra√≠zes est√£o fora do c√≠rculo unit√°rio ( $|z| > 1$), o processo √© invert√≠vel.

Para obter a previs√£o para um horizonte de $s$ per√≠odos √† frente, expandimos o operador de retardo:

$$\left[\frac{1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q}{L^s}\right]_+ = \begin{cases}
\theta_s + \theta_{s+1}L + \theta_{s+2}L^2 + \ldots + \theta_q L^{q-s}, & \text{para } s = 1, 2, \ldots, q \\
0, & \text{para } s = q+1, q+2, \ldots
\end{cases}$$

Note que os coeficientes $\theta_j$ para $j > q$ s√£o definidos como 0. Portanto, para horizontes de previs√£o $s$ maiores que $q$, a previs√£o √© simplesmente a m√©dia incondicional $\mu$ do processo [^4.2.35]. A express√£o para as previs√µes de $s$ per√≠odos √† frente, $\hat{Y}_{t+s|t}$ torna-se:

$$\hat{Y}_{t+s|t} = \mu + (\theta_s + \theta_{s+1}L + \theta_{s+2}L^2 + \ldots + \theta_q L^{q-s})\hat{\epsilon}_t$$ [^4.2.35]

onde $\hat{\epsilon}_t$ representa a estimativa do ru√≠do branco no tempo $t$ e pode ser obtida recursivamente a partir da equa√ß√£o:

$$\hat{\epsilon}_t = (Y_t - \mu) - \theta_1\hat{\epsilon}_{t-1} - \theta_2\hat{\epsilon}_{t-2} - \ldots - \theta_q\hat{\epsilon}_{t-q}$$ [^4.2.36]

> üí° **Exemplo Num√©rico:** Continuando com o MA(2) anterior, suponha que tenhamos os seguintes valores: $Y_t = 12$, $\hat{\epsilon}_{t-1} = 0.5$ e $\hat{\epsilon}_{t-2} = -0.2$. Ent√£o, podemos calcular $\hat{\epsilon}_t$:
>
> $$\hat{\epsilon}_t = (12 - 10) - (0.5)(0.5) - (0.3)(-0.2) = 2 - 0.25 + 0.06 = 1.81$$
>
> Agora, para prever $Y_{t+1}$, como $s=1$, temos:
> $$\hat{Y}_{t+1|t} = 10 + (0.5)\hat{\epsilon}_t + (0.3)\hat{\epsilon}_{t-1} = 10 + (0.5)(1.81) + (0.3)(0.5) = 10 + 0.905 + 0.15 = 11.055$$
> Para prever $Y_{t+2}$, como $s=2$, temos:
> $$\hat{Y}_{t+2|t} = 10 + (0.3)\hat{\epsilon}_t = 10 + (0.3)(1.81) = 10 + 0.543 = 10.543$$
> E para $s>2$, a previs√£o √© simplesmente $\hat{Y}_{t+s|t} = 10$.

**Lema 1:** A equa√ß√£o recursiva para $\hat{\epsilon}_t$ pode ser escrita em termos do operador de retardo como:
$$\hat{\epsilon}_t = \frac{1}{1+\theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q}(Y_t - \mu)$$
*Proof:*
I.  Come√ßamos com a equa√ß√£o recursiva para $\hat{\epsilon}_t$:
    $$(Y_t - \mu) = \hat{\epsilon}_t + \theta_1\hat{\epsilon}_{t-1} + \theta_2\hat{\epsilon}_{t-2} + \ldots + \theta_q\hat{\epsilon}_{t-q}$$
II.  Usando a propriedade do operador de retardo ($L^j \hat{\epsilon}_t = \hat{\epsilon}_{t-j}$), podemos reescrever a equa√ß√£o como:
    $$(Y_t - \mu) = \hat{\epsilon}_t + \theta_1 L\hat{\epsilon}_t + \theta_2 L^2\hat{\epsilon}_t + \ldots + \theta_q L^q\hat{\epsilon}_t$$
III. Fatoramos $\hat{\epsilon}_t$ do lado direito:
    $$(Y_t - \mu) = (1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q)\hat{\epsilon}_t$$
IV.  Para isolar $\hat{\epsilon}_t$, multiplicamos ambos os lados pela inversa do polin√¥mio:
    $$\hat{\epsilon}_t = \frac{1}{1+\theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q}(Y_t - \mu)$$
    
Portanto, demonstramos que a equa√ß√£o recursiva para $\hat{\epsilon}_t$ pode ser escrita em termos do operador de retardo como: $\hat{\epsilon}_t = \frac{1}{1+\theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q}(Y_t - \mu)$ ‚ñ†

A previs√£o com horizonte de $s$ periodos a frente √© portanto:
$$\hat{Y}_{t+s|t} = \mu + (\theta_s \hat{\epsilon}_t + \theta_{s+1} \hat{\epsilon}_{t-1} + \theta_{s+2} \hat{\epsilon}_{t-2} + \ldots + \theta_q \hat{\epsilon}_{t-q+s})$$

√â importante notar que a estimativa $\hat{\epsilon}_t$ √© o ru√≠do branco estimado no per√≠odo $t$, que √© dado pela diferen√ßa entre o valor observado e o valor estimado de $Y_t$ usando os ru√≠dos passados. Os ru√≠dos futuros s√£o desconhecidos, e seus valores s√£o definidos como zero ao calcular a previs√£o para $s>0$.

**Teorema 1:** Para um processo MA(q) invert√≠vel, a vari√¢ncia do erro de previs√£o para um horizonte *s* √© dada por:
$$Var(Y_{t+s} - \hat{Y}_{t+s|t}) = \sigma^2 \sum_{j=0}^{s-1} \psi_j^2$$
Onde $\sigma^2$ √© a vari√¢ncia do ru√≠do branco $\epsilon_t$ e $\psi_j$ s√£o os coeficientes da representa√ß√£o MA($\infty$) do processo, definidos pela expans√£o:
$$ \frac{1}{1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q} = \sum_{j=0}^{\infty} \psi_j L^j$$
*Proof:*
I.  O erro de previs√£o √© dado por $e_{t+s|t} = Y_{t+s} - \hat{Y}_{t+s|t}$.
II. Um processo MA(q) pode ser expresso como um MA($\infty$): $Y_t = \mu + \sum_{j=0}^\infty \psi_j \epsilon_{t-j}$. Portanto, $Y_{t+s} = \mu + \sum_{j=0}^\infty \psi_j \epsilon_{t+s-j}$.
III. A previs√£o $\hat{Y}_{t+s|t}$ usa a informa√ß√£o dispon√≠vel at√© o tempo $t$, logo ela √© dada por: $\hat{Y}_{t+s|t} = \mu + \sum_{j=s}^\infty \psi_j \epsilon_{t+s-j}$.
IV.  Substituindo em $e_{t+s|t}$, temos $e_{t+s|t} =  \left( \mu + \sum_{j=0}^\infty \psi_j \epsilon_{t+s-j} \right) - \left( \mu + \sum_{j=s}^\infty \psi_j \epsilon_{t+s-j} \right)$, que simplifica para:
$$ e_{t+s|t} = \sum_{j=0}^{s-1} \psi_j \epsilon_{t+s-j} $$
V. Como os $\epsilon_t$ s√£o ru√≠dos brancos com vari√¢ncia $\sigma^2$, a vari√¢ncia do erro de previs√£o √©:
$$Var(e_{t+s|t}) = Var \left( \sum_{j=0}^{s-1} \psi_j \epsilon_{t+s-j} \right) = \sum_{j=0}^{s-1} \psi_j^2 Var(\epsilon_{t+s-j}) = \sigma^2 \sum_{j=0}^{s-1} \psi_j^2 $$
Portanto, $Var(Y_{t+s} - \hat{Y}_{t+s|t}) = \sigma^2 \sum_{j=0}^{s-1} \psi_j^2$. ‚ñ†

> üí° **Exemplo Num√©rico:** Para nosso MA(2) com $\theta_1 = 0.5$ e $\theta_2 = 0.3$, vamos calcular os primeiros $\psi_j$ para verificar a vari√¢ncia do erro de previs√£o. Precisamos expandir $\frac{1}{1 + 0.5L + 0.3L^2} = \sum_{j=0}^{\infty} \psi_j L^j$.
>
> Fazendo a divis√£o polinomial ou usando outros m√©todos, podemos obter os primeiros $\psi$ valores:
>
> $\psi_0 = 1$
>
> $\psi_1 = -0.5$
>
> $\psi_2 = -0.3 - 0.5*(-0.5) = -0.05$
>
> $\psi_3 = -0.5*(-0.05) - 0.3*(-0.5) = 0.175$
>
> Se assumirmos que a vari√¢ncia do ru√≠do branco √© $\sigma^2 = 1$, a vari√¢ncia do erro de previs√£o para $s=1$ √©:
>
> $Var(Y_{t+1} - \hat{Y}_{t+1|t}) = 1 * (1^2) = 1$
>
> Para $s=2$:
>
> $Var(Y_{t+2} - \hat{Y}_{t+2|t}) = 1 * (1^2 + (-0.5)^2) = 1.25$
>
> Para $s=3$:
>
> $Var(Y_{t+3} - \hat{Y}_{t+3|t}) = 1 * (1^2 + (-0.5)^2 + (-0.05)^2) = 1.2525$
>
> Note que a vari√¢ncia do erro de previs√£o aumenta com o horizonte de previs√£o $s$, e converge para a vari√¢ncia total do processo quando $s \to \infty$.

### Conclus√£o
Em resumo, a previs√£o de processos MA(q) invert√≠veis envolve a aplica√ß√£o do operador de retardo e do conceito de aniquila√ß√£o para isolar os componentes relevantes do hist√≥rico passado. Este m√©todo nos permite gerar previs√µes √≥timas baseadas nas informa√ß√µes dispon√≠veis no momento da previs√£o e nos par√¢metros estimados. O uso de uma recurs√£o para o c√°lculo de $\hat{\epsilon}_t$ torna o processo de previs√£o pr√°tico e implement√°vel. A previs√£o para horizontes $s>q$ converge para a m√©dia incondicional $\mu$ do processo.

### Refer√™ncias
[^4.1.2]: *A previs√£o com o menor erro quadr√°tico m√©dio √© a expectativa de $Y_{t+1}$ condicional a $X_t$: $Y_{t+1}^* = E(Y_{t+1}|X_t)$*
[^4.2.4]: *O termo do meio no lado direito de [4.1.4] como $2E[\eta_{t+1}]$, onde $\eta_{t+1} = \{ [Y_{t+1} - E(Y_{t+1}|X_t)][E(Y_{t+1}|X_t) - g(X_t)] \}$*
[^4.2.8]: *O operador de aniquila√ß√£o (indicado por $[\cdot]_+$) substitui as pot√™ncias negativas de $L$ por zero; por exemplo, $\left[\frac{\psi(L)}{L^s}\right]_+ = \psi_s + \psi_{s+1}L^1 + \psi_{s+2}L^2 + \ldots$*
[^4.2.16]: *$\hat{E}[Y_{t+s}|\epsilon_t, \epsilon_{t-1}, \ldots] = \mu + \left[\frac{\psi(L)}{L^s}\right]_+ \frac{1}{\psi(L)} (Y_t - \mu)$*
[^4.2.28]: *Considere uma representa√ß√£o invert√≠vel MA(1), $Y_t - \mu = (1 + \theta L)\epsilon_t$, com $|\theta| < 1$.*
[^4.2.34]: *a previs√£o [4.2.16] torna-se $\hat{Y}_{t+s|t} = \mu + \left[\frac{1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q}{L^s}\right]_+ \times \frac{1}{1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q} (Y_t - \mu)$*
[^4.2.35]: *Assim, para horizontes de $s = 1, 2, \ldots, q$, a previs√£o √© dada por $\hat{Y}_{t+s|t} = \mu + (\theta_s + \theta_{s+1}L + \theta_{s+2}L^2 + \ldots + \theta_q L^{q-s})\hat{\epsilon}_t$, onde $\hat{\epsilon}_t$ pode ser caracterizado pela recurs√£o*
[^4.2.36]: *$\hat{\epsilon}_t = (Y_t - \mu) - \theta_1\hat{\epsilon}_{t-1} - \theta_2\hat{\epsilon}_{t-2} - \ldots - \theta_q\hat{\epsilon}_{t-q}$*
<!-- END -->
