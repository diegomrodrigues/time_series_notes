## Previs√£o para Processos MA(q) com Horizontes Longos: An√°lise da Converg√™ncia

### Introdu√ß√£o

Este cap√≠tulo visa aprofundar a discuss√£o sobre a previs√£o em modelos de m√©dias m√≥veis (MA), focando especificamente no comportamento das previs√µes quando o horizonte de tempo ($s$) excede a ordem do modelo ($q$). Expandindo os conceitos apresentados anteriormente sobre previs√µes de processos MA(1) [^4.2.28] e MA(q) [^1], exploraremos como as previs√µes se comportam assintoticamente e como o erro quadr√°tico m√©dio (MSE) se relaciona com a vari√¢ncia incondicional do processo. Como j√° vimos, a previs√£o √≥tima para um processo estoc√°stico √© dada pela expectativa condicional do valor futuro, dado o hist√≥rico das observa√ß√µes [^4.1.2]. Examinaremos em detalhes como essa expectativa se manifesta em horizontes longos.

### Converg√™ncia para a M√©dia Incondicional
Como demonstrado anteriormente [^1], a previs√£o para um processo MA(q) com horizonte $s$ √© dada por:

$$\hat{Y}_{t+s|t} = \mu + \left[\frac{1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q}{L^s}\right]_+ \frac{1}{1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q}(Y_t - \mu)$$ [^4.2.34]

Quando o horizonte de previs√£o $s$ √© maior do que a ordem do processo $q$ ($s > q$), o operador de aniquila√ß√£o elimina todos os termos com pot√™ncias positivas de $L$, resultando em:

$$\left[\frac{1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q}{L^s}\right]_+ = 0$$
para $s > q$

Consequentemente, a previs√£o para horizontes longos, onde $s > q$, se reduz a:
$$\hat{Y}_{t+s|t} = \mu$$ [^4.2.35]

Este resultado √© fundamental e demonstra que, √† medida que nos afastamos no futuro, a previs√£o √≥tima converge para a m√©dia incondicional do processo, $\mu$.  Isso implica que, para processos MA(q), as informa√ß√µes contidas nos dados passados perdem relev√¢ncia √† medida que o horizonte de previs√£o se estende. A intui√ß√£o por tr√°s deste comportamento √© que, para al√©m da mem√≥ria do processo, o melhor palpite que podemos fazer √© a m√©dia do processo.

> üí° **Exemplo Num√©rico:**  Consideremos um processo MA(2) com par√¢metros $\theta_1 = 0.6$, $\theta_2 = 0.4$ e m√©dia $\mu = 50$. Para previs√µes com horizonte de um e dois per√≠odos √† frente ($s=1$ e $s=2$, respectivamente), usamos as equa√ß√µes j√° definidas:
>
> $$\hat{Y}_{t+1|t} = 50 + 0.6 \hat{\epsilon}_t + 0.4 \hat{\epsilon}_{t-1}$$
>
> $$\hat{Y}_{t+2|t} = 50 + 0.4 \hat{\epsilon}_t$$
>
> Para um horizonte de previs√£o $s=3$, e todos os valores $s>2$, a previs√£o se torna:
> $$\hat{Y}_{t+s|t} = 50$$
> Independentemente dos valores observados passados de $Y$, a melhor previs√£o para um futuro distante √© a m√©dia incondicional do processo.
>
> Suponha que observamos a seguinte sequ√™ncia de erros $\epsilon$: $\epsilon_{t-2} = 0.5$, $\epsilon_{t-1} = -0.2$, $\epsilon_t = 0.8$.
>
> Ent√£o, a previs√£o para $t+1$ √©: $\hat{Y}_{t+1|t} = 50 + 0.6(0.8) + 0.4(-0.2) = 50.4$.
>
> A previs√£o para $t+2$ √©: $\hat{Y}_{t+2|t} = 50 + 0.4(0.8) = 50.32$.
>
> E para $t+3$ (e todos os $s>2$), a previs√£o √©: $\hat{Y}_{t+3|t} = 50$.
>
> Isso demonstra que, para horizontes de previs√£o maiores que a ordem do processo, as previs√µes convergem para a m√©dia.

**Lema 1:** A converg√™ncia da previs√£o para a m√©dia incondicional ($\mu$) √© uma consequ√™ncia direta da natureza de mem√≥ria finita dos processos MA(q).
*Proof:*
Como definido anteriormente, $\hat{Y}_{t+s|t}$ √© dado pela equa√ß√£o:
$$\hat{Y}_{t+s|t} = \mu + (\theta_s + \theta_{s+1}L + \theta_{s+2}L^2 + \ldots + \theta_q L^{q-s})\hat{\epsilon}_t$$
para  $s \leq q$, e $\hat{Y}_{t+s|t} = \mu$, para $s > q$.
Para $s > q$, todos os operadores de retardo de $\hat{\epsilon}_t$ s√£o eliminados pelo operador de aniquila√ß√£o, deixando a previs√£o como igual a $\mu$, independente dos valores passados de $\hat{\epsilon}$.
Esta √© uma caracter√≠stica importante de processos MA(q):  a previs√£o converge para a m√©dia incondicional √† medida que o horizonte de previs√£o se estende al√©m da ordem do modelo. ‚ñ†

**Lema 1.1:** Para um processo MA(q), o erro de previs√£o $e_{t+s|t} = Y_{t+s} - \hat{Y}_{t+s|t}$ para $s>q$ √© igual a $Y_{t+s} - \mu$.
*Proof:*
I. Pelo Lema 1, para $s>q$ temos que $\hat{Y}_{t+s|t} = \mu$.
II. Substituindo $\hat{Y}_{t+s|t}$ em $e_{t+s|t}$, obtemos $e_{t+s|t} = Y_{t+s} - \mu$. ‚ñ†

### Comportamento do Erro Quadr√°tico M√©dio (MSE)
Al√©m da converg√™ncia da previs√£o, o erro quadr√°tico m√©dio (MSE) associado a essas previs√µes tamb√©m apresenta um comportamento interessante. A vari√¢ncia do erro de previs√£o para um horizonte $s$ √© dada por [^1]:
$$Var(Y_{t+s} - \hat{Y}_{t+s|t}) = \sigma^2 \sum_{j=0}^{s-1} \psi_j^2$$
Onde $\sigma^2$ √© a vari√¢ncia do ru√≠do branco $\epsilon_t$ e $\psi_j$ s√£o os coeficientes da representa√ß√£o MA($\infty$) do processo, definidos pela expans√£o:
$$ \frac{1}{1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q} = \sum_{j=0}^{\infty} \psi_j L^j$$
Como o MSE √© igual a vari√¢ncia do erro de previs√£o, temos:

$$MSE(Y_{t+s|t}) = E[(Y_{t+s} - \hat{Y}_{t+s|t})^2] = \sigma^2 \sum_{j=0}^{s-1} \psi_j^2$$

Para horizontes de previs√£o $s > q$, a previs√£o se torna a m√©dia incondicional do processo, $\hat{Y}_{t+s|t}=\mu$.  Nesse caso, o erro de previs√£o √© dado por:
$$ e_{t+s|t} = Y_{t+s} - \mu $$
A vari√¢ncia desse erro, e portanto o MSE, torna-se:
$$MSE(Y_{t+s|t}) = Var(Y_{t+s} - \mu) = Var(Y_{t+s})$$

que √© a vari√¢ncia incondicional do processo, ou seja:

$$MSE(Y_{t+s|t}) = \sigma^2 \sum_{j=0}^{\infty} \psi_j^2$$

**Observa√ß√£o 2:**  A vari√¢ncia incondicional do processo √© obtida quando o horizonte de previs√£o se torna suficientemente grande de modo que o erro de previs√£o consiste apenas nas inova√ß√µes futuras, levando o MSE a convergir para a vari√¢ncia total do processo.

**Teorema 2:** Para um processo MA(q), o MSE converge para a vari√¢ncia incondicional do processo quando $s$ tende para o infinito:
$$ \lim_{s \to \infty} MSE(Y_{t+s|t}) = \sigma^2 \sum_{j=0}^{\infty} \psi_j^2 $$
*Proof:*
I. Como demonstrado anteriormente, $MSE(Y_{t+s|t}) = \sigma^2 \sum_{j=0}^{s-1} \psi_j^2$.
II. Quando $s \to \infty$, a soma $\sum_{j=0}^{s-1} \psi_j^2$ se torna a soma de todos os coeficientes da representa√ß√£o MA($\infty$), e, portanto, converge para a vari√¢ncia incondicional.
III. Assim, temos que $\lim_{s \to \infty} MSE(Y_{t+s|t}) = \lim_{s \to \infty} \sigma^2 \sum_{j=0}^{s-1} \psi_j^2 = \sigma^2 \sum_{j=0}^{\infty} \psi_j^2 $. ‚ñ†

Este resultado √© fundamental e indica que, para processos MA(q), o erro de previs√£o aumenta com o horizonte de previs√£o $s$ at√© atingir um valor m√°ximo, que √© a vari√¢ncia do processo, dada por $\sigma^2 \sum_{j=0}^{\infty} \psi_j^2$.

> üí° **Exemplo Num√©rico:** Retomando o processo MA(2) do exemplo anterior, com par√¢metros $\theta_1 = 0.6$, $\theta_2 = 0.4$ e assumindo $\sigma^2 = 1$, podemos calcular os primeiros coeficientes $\psi_j$:
>
> $\psi_0 = 1$
>
> $\psi_1 = -0.6$
>
> $\psi_2 = -0.4 - 0.6(-0.6) = -0.04$
>
> $\psi_3 = -0.6(-0.04) - 0.4(-0.6) = 0.264$
>
> $\psi_4 = -0.6(0.264) -0.4(-0.04) = -0.1424$
>
> Para os horizontes de previs√£o, a vari√¢ncia do erro de previs√£o (MSE) √©:
>
> *   $s=1$: $MSE(Y_{t+1|t}) = 1 * (1^2) = 1$
>
> *   $s=2$: $MSE(Y_{t+2|t}) = 1 * (1^2 + (-0.6)^2) = 1.36$
>
> *   $s=3$: $MSE(Y_{t+3|t}) = 1 * (1^2 + (-0.6)^2 + (-0.04)^2) = 1.3616$
>
> √Ä medida que $s$ aumenta, o MSE converge para $\sigma^2\sum_{j=0}^{\infty} \psi_j^2$. Calculando os primeiros termos da soma, vemos que a sequ√™ncia converge para um valor pr√≥ximo de 1.52.
>
> Para ilustrar numericamente, suponha que tenhamos observado os primeiros quatro valores de $\psi$: 1, -0.6, -0.04, e 0.264.
>
> Para $s=1$, o MSE seria $\sigma^2 * \psi_0^2 = 1 * 1^2 = 1$.
>
> Para $s=2$, o MSE seria $\sigma^2 * (\psi_0^2 + \psi_1^2) = 1 * (1^2 + (-0.6)^2) = 1.36$.
>
> Para $s=3$, o MSE seria $\sigma^2 * (\psi_0^2 + \psi_1^2 + \psi_2^2) = 1 * (1^2 + (-0.6)^2 + (-0.04)^2) = 1.3616$.
>
> Para $s=4$, o MSE seria $\sigma^2 * (\psi_0^2 + \psi_1^2 + \psi_2^2 + \psi_3^2) = 1 * (1^2 + (-0.6)^2 + (-0.04)^2 + 0.264^2) = 1.431296$.
>
> E para $s \to \infty$,  o MSE tender√° a vari√¢ncia incondicional do processo, que neste caso √© $\approx 1.52$

**Teorema 2.1:** O MSE para um processo MA(q) √© uma fun√ß√£o n√£o decrescente do horizonte de previs√£o s.
*Proof:*
I. O MSE √© dado por $MSE(Y_{t+s|t}) = \sigma^2 \sum_{j=0}^{s-1} \psi_j^2$.
II. √Ä medida que $s$ aumenta, um termo n√£o negativo ($\psi_{s-1}^2$) √© adicionado √† soma.
III. Como $\sigma^2 > 0$, o MSE n√£o pode diminuir com o aumento de $s$, e portanto √© uma fun√ß√£o n√£o decrescente. ‚ñ†

**Corol√°rio 2.1:** O MSE para previs√µes de um processo MA(q) nunca excede a vari√¢ncia incondicional do processo.
*Proof:*
I. Pelo Teorema 2, o MSE converge para a vari√¢ncia incondicional quando $s$ tende ao infinito.
II. Pelo Teorema 2.1 o MSE √© n√£o decrescente.
III. Portanto, o MSE sempre ser√° menor ou igual √† vari√¢ncia incondicional. ‚ñ†

### Conclus√£o

Em conclus√£o, a previs√£o para horizontes maiores do que a ordem do MA(q) resulta na m√©dia incondicional do processo. Este resultado √© uma consequ√™ncia da propriedade de mem√≥ria finita dos processos MA(q). A vari√¢ncia do erro de previs√£o (MSE), por sua vez, aumenta com o horizonte de previs√£o, convergindo para a vari√¢ncia incondicional do processo. Compreender o comportamento das previs√µes e seus erros para horizontes longos √© essencial para a aplica√ß√£o pr√°tica de modelos MA em previs√£o de s√©ries temporais.

### Refer√™ncias

[^4.1.2]: *A previs√£o com o menor erro quadr√°tico m√©dio √© a expectativa de $Y_{t+1}$ condicional a $X_t$: $Y_{t+1}^* = E(Y_{t+1}|X_t)$*
[^1]: *Para processos MA(q) invert√≠veis, a previs√£o envolve o uso do operador de retardo, resultando em um conjunto de coeficientes para previs√µes de s per√≠odos √† frente.*
[^4.2.28]: *Considere uma representa√ß√£o invert√≠vel MA(1), $Y_t - \mu = (1 + \theta L)\epsilon_t$, com $|\theta| < 1$.*
[^4.2.34]: *a previs√£o [4.2.16] torna-se $\hat{Y}_{t+s|t} = \mu + \left[\frac{1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q}{L^s}\right]_+ \times \frac{1}{1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q} (Y_t - \mu)$*
[^4.2.35]: *Assim, para horizontes de $s = 1, 2, \ldots, q$, a previs√£o √© dada por $\hat{Y}_{t+s|t} = \mu + (\theta_s + \theta_{s+1}L + \theta_{s+2}L^2 + \ldots + \theta_q L^{q-s})\hat{\epsilon}_t$, onde $\hat{\epsilon}_t$ pode ser caracterizado pela recurs√£o*
### 5.2. Likelihood Function for an AR(1) Process

Vamos iniciar com um modelo AR(1), onde a equa√ß√£o de observa√ß√£o √© dada por:
$$Y_t = c + \phi Y_{t-1} + \epsilon_t$$ [5.2.1]
onde $\epsilon_t \sim i.i.d. N(0, \sigma^2)$. O vetor de par√¢metros a ser estimado √© $\theta = (c, \phi, \sigma^2)'$. Para construir a fun√ß√£o de *likelihood*, assumimos que os valores iniciais s√£o dados, neste caso $Y_0$.
Ent√£o, a densidade condicional de $Y_1$ dado $Y_0$ √©:

$$f(Y_1|Y_0; \theta) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left[-\frac{(Y_1 - c - \phi Y_0)^2}{2\sigma^2}\right]$$ [5.2.2]

Similarmente, a densidade condicional de $Y_2$ dado $Y_1$ √©:
$$f(Y_2|Y_1; \theta) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left[-\frac{(Y_2 - c - \phi Y_1)^2}{2\sigma^2}\right]$$ [5.2.3]
E, em geral, a densidade condicional de $Y_t$ dado $Y_{t-1}$ √©:
$$f(Y_t|Y_{t-1}; \theta) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left[-\frac{(Y_t - c - \phi Y_{t-1})^2}{2\sigma^2}\right]$$ [5.2.4]
Assumindo independ√™ncia serial, a fun√ß√£o de *likelihood* √© o produto das densidades condicionais:
$$L(\theta; Y_1, \ldots, Y_T | Y_0) = \prod_{t=1}^T f(Y_t | Y_{t-1}; \theta)$$ [5.2.5]
Substituindo [5.2.4] em [5.2.5], temos:
$$L(\theta; Y_1, \ldots, Y_T | Y_0) = (2\pi\sigma^2)^{-T/2} \exp\left[-\frac{1}{2\sigma^2}\sum_{t=1}^T (Y_t - c - \phi Y_{t-1})^2\right]$$ [5.2.6]
√â mais conveniente trabalhar com o logaritmo da fun√ß√£o de *likelihood*, o que n√£o altera os valores de $\theta$ que maximizam a fun√ß√£o:
$$\ln L(\theta; Y_1, \ldots, Y_T | Y_0) = -\frac{T}{2} \ln(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{t=1}^T (Y_t - c - \phi Y_{t-1})^2$$ [5.2.7]
Esta fun√ß√£o, $\ln L(\theta; Y_1, \ldots, Y_T | Y_0)$, pode ser maximizada em rela√ß√£o aos par√¢metros $\theta = (c, \phi, \sigma^2)'$.

> üí° **Exemplo Num√©rico:** Considere um processo AR(1) com $c=10$, $\phi=0.7$, e $\sigma^2=4$. Vamos supor que temos as seguintes observa√ß√µes $Y_0=20$, $Y_1=25$, e $Y_2=30$.
>
> A densidade condicional de $Y_1$ dado $Y_0$ √©:
> $$f(Y_1|Y_0; \theta) = \frac{1}{\sqrt{2\pi(4)}} \exp\left[-\frac{(25 - 10 - 0.7 * 20)^2}{2 * 4}\right] = \frac{1}{\sqrt{8\pi}} \exp\left[-\frac{(1)^2}{8}\right] \approx 0.277$$
>
> A densidade condicional de $Y_2$ dado $Y_1$ √©:
> $$f(Y_2|Y_1; \theta) = \frac{1}{\sqrt{2\pi(4)}} \exp\left[-\frac{(30 - 10 - 0.7 * 25)^2}{2 * 4}\right] = \frac{1}{\sqrt{8\pi}} \exp\left[-\frac{(2.5)^2}{8}\right] \approx 0.226$$
>
> A fun√ß√£o de *likelihood* para as duas observa√ß√µes √©:
> $$L(\theta; Y_1, Y_2 | Y_0) = f(Y_1|Y_0; \theta) \times f(Y_2|Y_1; \theta) \approx 0.277 * 0.226 \approx 0.0626$$
>
> O logaritmo da fun√ß√£o de *likelihood* √©:
> $$\ln L(\theta; Y_1, Y_2 | Y_0) = \ln(0.0626) \approx -2.77$$
>
> A ideia √© encontrar os valores de $c$, $\phi$ e $\sigma^2$ que maximizam a fun√ß√£o de *likelihood* (ou o seu logaritmo), dado o conjunto de observa√ß√µes.

### 5.3. Likelihood Function for an MA(1) Process

Agora, vamos considerar um processo MA(1), onde a equa√ß√£o de observa√ß√£o √© dada por:
$$Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$$ [5.3.1]
onde $\epsilon_t \sim i.i.d. N(0, \sigma^2)$. O vetor de par√¢metros a ser estimado √© $\theta = (\mu, \theta, \sigma^2)'$. Diferentemente do caso AR(1), os termos de erro $\epsilon_t$ n√£o s√£o diretamente observ√°veis. Assim, a constru√ß√£o da fun√ß√£o de *likelihood* √© mais complexa. A ideia √© expressar a fun√ß√£o de *likelihood* em termos das vari√°veis observadas $Y_t$.

Vamos reescrever [5.3.1] na forma:
$$\epsilon_t = Y_t - \mu - \theta \epsilon_{t-1}$$ [5.3.2]
Podemos usar esta equa√ß√£o recursivamente para expressar o erro corrente $\epsilon_t$ em fun√ß√£o dos erros passados. Para iniciar a recurs√£o, precisamos de um valor inicial para o erro. Vamos assumir que $\epsilon_0 = 0$. Ent√£o,
$$\epsilon_1 = Y_1 - \mu$$ [5.3.3]
$$\epsilon_2 = Y_2 - \mu - \theta \epsilon_1 = Y_2 - \mu - \theta(Y_1 - \mu)$$ [5.3.4]
E, em geral, para $t > 1$:
$$\epsilon_t = Y_t - \mu - \theta \epsilon_{t-1}$$ [5.3.5]
O erro $\epsilon_t$ √© uma fun√ß√£o de par√¢metros $\mu$, $\theta$ e das observa√ß√µes at√© o tempo $t$. A fun√ß√£o de *likelihood* √© dada pelo produto das densidades condicionais:
$$L(\theta; Y_1, \ldots, Y_T) = \prod_{t=1}^T f(Y_t | Y_{t-1}, \ldots, Y_1; \theta)$$ [5.3.6]
A densidade condicional de $Y_t$ dado os valores anteriores √© equivalente √† densidade condicional do erro $\epsilon_t$ dado os erros anteriores, pois $Y_t$ √© fun√ß√£o de $\epsilon_t$ e os valores passados de $\epsilon$. Assim,
$$f(Y_t | Y_{t-1}, \ldots, Y_1; \theta) = f(\epsilon_t | \epsilon_{t-1}, \ldots, \epsilon_1; \theta)$$ [5.3.7]
Como os $\epsilon_t$ s√£o *i.i.d.*, podemos expressar a fun√ß√£o de *likelihood* como:

$$L(\theta; Y_1, \ldots, Y_T) = \prod_{t=1}^T f(\epsilon_t; \theta)$$ [5.3.8]
Substituindo a densidade gaussiana de $\epsilon_t$:
$$L(\theta; Y_1, \ldots, Y_T) = (2\pi\sigma^2)^{-T/2} \exp\left[-\frac{1}{2\sigma^2}\sum_{t=1}^T \epsilon_t^2\right]$$ [5.3.9]

Substituindo [5.3.2] em [5.3.9], encontramos que:
$$L(\theta; Y_1, \ldots, Y_T) = (2\pi\sigma^2)^{-T/2} \exp\left[-\frac{1}{2\sigma^2}\sum_{t=1}^T (Y_t - \mu - \theta \epsilon_{t-1})^2\right]$$ [5.3.10]
E o logaritmo da fun√ß√£o de *likelihood* √©:
$$\ln L(\theta; Y_1, \ldots, Y_T) = -\frac{T}{2} \ln(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{t=1}^T (Y_t - \mu - \theta \epsilon_{t-1})^2$$ [5.3.11]
onde $\epsilon_t$ √© calculado recursivamente de acordo com [5.3.2] e [5.3.3]. Esta fun√ß√£o, $\ln L(\theta; Y_1, \ldots, Y_T)$, pode ser maximizada em rela√ß√£o aos par√¢metros $\theta = (\mu, \theta, \sigma^2)'$.

> üí° **Exemplo Num√©rico:** Considere um processo MA(1) com $\mu=20$, $\theta=0.5$ e $\sigma^2=9$. Suponha que temos as seguintes observa√ß√µes: $Y_1=22$ e $Y_2=23$.
>
> Primeiro, calculamos $\epsilon_1$:
>
> $$\epsilon_1 = Y_1 - \mu = 22 - 20 = 2$$
>
> Em seguida, calculamos $\epsilon_2$ usando a equa√ß√£o recursiva:
>
> $$\epsilon_2 = Y_2 - \mu - \theta \epsilon_1 = 23 - 20 - 0.5 * 2 = 2$$
>
> A fun√ß√£o de *likelihood* √© ent√£o:
>
> $$L(\theta; Y_1, Y_2) = (2\pi(9))^{-2/2} \exp\left[-\frac{1}{2*9}(\epsilon_1^2 + \epsilon_2^2)\right] = (18\pi)^{-1} \exp\left[-\frac{1}{18}(2^2 + 2^2)\right] = \frac{1}{18\pi} \exp\left[-\frac{8}{18}\right] \approx 0.0127$$
>
> O logaritmo da fun√ß√£o de *likelihood* √©:
>
> $$\ln L(\theta; Y_1, Y_2) =  \ln(0.0127) \approx -4.36$$
>
> O processo de m√°xima verossimilhan√ßa busca os valores de $\mu$, $\theta$ e $\sigma^2$ que maximizam a fun√ß√£o de *likelihood* (ou o seu logaritmo) para os dados observados.

### 5.4. Likelihood Function for an ARMA(p, q) Process

Generalizando, a fun√ß√£o de *likelihood* de um processo ARMA(p, q) √© dada por:

$$Y_t = c + \phi_1 Y_{t-1} + \ldots + \phi_p Y_{t-p} + \epsilon_t + \theta_1 \epsilon_{t-1} + \ldots + \theta_q \epsilon_{t-q}$$ [5.4.1]
onde $\epsilon_t \sim i.i.d. N(0, \sigma^2)$. O vetor de par√¢metros a ser estimado √© $\theta = (c, \phi_1, \ldots, \phi_p, \theta_1, \ldots, \theta_q, \sigma^2)'$. De forma similar ao caso MA(1), expressamos o erro corrente $\epsilon_t$ em termos das observa√ß√µes e erros passados, sendo que necessitamos de valores iniciais para $Y_t$ e $\epsilon_t$:
$$\epsilon_t = Y_t - c - \phi_1 Y_{t-1} - \ldots - \phi_p Y_{t-p} - \theta_1 \epsilon_{t-1} - \ldots - \theta_q \epsilon_{t-q}$$ [5.4.2]
Para iniciar a recurs√£o, precisamos de $Y_0, Y_{-1}, \ldots, Y_{-p+1}$ e $\epsilon_0, \epsilon_{-1}, \ldots, \epsilon_{-q+1}$. Para fins de estima√ß√£o, usualmente definimos esses valores iniciais como iguais a zero ou a m√©dia amostral.

De modo an√°logo ao caso MA(1), a fun√ß√£o de *likelihood* √© dada por:

$$L(\theta; Y_1, \ldots, Y_T) = (2\pi\sigma^2)^{-T/2} \exp\left[-\frac{1}{2\sigma^2}\sum_{t=1}^T \epsilon_t^2\right]$$ [5.4.3]
Onde $\epsilon_t$ √© calculado recursivamente de [5.4.2]. O logaritmo da fun√ß√£o de *likelihood* √© ent√£o:

$$\ln L(\theta; Y_1, \ldots, Y_T) = -\frac{T}{2} \ln(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{t=1}^T \epsilon_t^2$$ [5.4.4]
Essa fun√ß√£o, $\ln L(\theta; Y_1, \ldots, Y_T)$, √© maximizada em rela√ß√£o ao vetor de par√¢metros $\theta$.

> üí° **Exemplo Num√©rico:**  Vamos considerar um processo ARMA(1,1) com $c=5$, $\phi_1=0.6$, $\theta_1=0.4$, e $\sigma^2=2$.  Suponhamos que temos as observa√ß√µes $Y_0=10$, $Y_1=12$, $Y_2=15$. Para iniciar a recurs√£o, definimos $\epsilon_0=0$.
>
> Primeiro, calculamos $\epsilon_1$:
>
> $$\epsilon_1 = Y_1 - c - \phi_1 Y_0 = 12 - 5 - 0.6 * 10 = 1$$
>
> Agora, calculamos $\epsilon_2$:
>
> $$\epsilon_2 = Y_2 - c - \phi_1 Y_1 - \theta_1 \epsilon_1 = 15 - 5 - 0.6 * 12 - 0.4 * 1 = 2.2$$
>
> A fun√ß√£o de *likelihood* √© ent√£o:
>
> $$L(\theta; Y_1, Y_2) = (2\pi(2))^{-2/2} \exp\left[-\frac{1}{2*2}(\epsilon_1^2 + \epsilon_2^2)\right] = (4\pi)^{-1} \exp\left[-\frac{1}{4}(1^2 + 2.2^2)\right] \approx 0.021$$
>
> O logaritmo da fun√ß√£o de *likelihood* √©:
>
> $$\ln L(\theta; Y_1, Y_2) =  \ln(0.021) \approx -3.86$$
>
>  O objetivo da estima√ß√£o por m√°xima verossimilhan√ßa √© encontrar os valores dos par√¢metros que maximizem essa fun√ß√£o para o conjunto de dados.

### 5.5. Conditional Sum of Squares

Para simplificar a maximiza√ß√£o da fun√ß√£o de *likelihood*, podemos usar o m√©todo de Conditional Sum of Squares (CSS). O CSS se concentra em minimizar a soma dos erros quadrados condicionais sem levar em conta o termo da fun√ß√£o de *likelihood* que envolve $\sigma^2$. Para ilustrar, considere a fun√ß√£o de *likelihood* [5.4.4], temos que maximizar a fun√ß√£o que √© equivalente a minimizar:

$$CSS(\theta) = \sum_{t=1}^T \epsilon_t^2$$ [5.5.1]

onde $\epsilon_t$ √© definido por [5.4.2]. Os par√¢metros √≥timos, $\hat{\theta}_{CSS}$, encontrados minimizando a soma dos quadrados condicionais, podem ser utilizados para encontrar um estimador para $\sigma^2$. O estimador da vari√¢ncia $\hat{\sigma}_{CSS}^2$ √© dado por:

$$\hat{\sigma}_{CSS}^2 = \frac{1}{T} CSS(\hat{\theta}_{CSS}) = \frac{1}{T}\sum_{t=1}^T \hat{\epsilon}_t^2$$ [5.5.2]

O estimador CSS n√£o √© o mesmo que o estimador *maximum likelihood*, mas √© computacionalmente mais eficiente e √© usado como ponto de partida em muitos algoritmos de estima√ß√£o de par√¢metros em modelos ARMA.

> üí° **Exemplo Num√©rico:** Retomando o exemplo do ARMA(1,1) com os mesmos par√¢metros e dados ($c=5$, $\phi_1=0.6$, $\theta_1=0.4$, $Y_0=10$, $Y_1=12$, $Y_2=15$ e $\epsilon_0=0$), calculamos $\epsilon_1=1$ e $\epsilon_2=2.2$ como no exemplo anterior.
>
> O CSS √© ent√£o:
>
> $$CSS(\theta) = \sum_{t=1}^2 \epsilon_t^2 = \epsilon_1^2 + \epsilon_2^2 = 1^2 + 2.2^2 = 1 + 4.84 = 5.84$$
>
> O estimador CSS da vari√¢ncia seria:
>
> $$\hat{\sigma}_{CSS}^2 = \frac{1}{2} CSS(\hat{\theta}_{CSS}) = \frac{1}{2} * 5.84 = 2.92$$
>
> O m√©todo CSS √© frequentemente usado para obter uma estimativa inicial dos par√¢metros, que depois √© refinada usando algoritmos de m√°xima verossimilhan√ßa.

### 5.6. Innovations Form of the Likelihood Function

Outra forma de derivar a fun√ß√£o de *likelihood* √© atrav√©s da chamada forma de inova√ß√µes, que utiliza as proje√ß√µes lineares para escrever a fun√ß√£o de *likelihood* de um processo geral de s√©rie temporal linear. Vamos considerar a representa√ß√£o de Wold de uma s√©rie temporal $Y_t$:

$$ Y_t = \mu + \sum_{j=0}^{\infty} \psi_j \epsilon_{t-j} $$

onde $\mu$ √© a m√©dia da s√©rie, $\psi_j$ s√£o os pesos do filtro, e $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$. Assumindo que podemos aproximar esta representa√ß√£o com uma ordem finita *p*, ou seja, que $\psi_j \approx 0$ para $j > p$, temos:

$$ Y_t \approx \mu + \sum_{j=0}^{p} \psi_j \epsilon_{t-j} $$

Podemos reescrever isso na forma de um modelo de m√©dia m√≥vel (MA) de ordem *p*:

$$ Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_p \epsilon_{t-p} $$

onde $\theta_j = \psi_j$. Agora, a fun√ß√£o de *likelihood* para um modelo MA(p) pode ser escrita como:

$$ L(\mu, \theta, \sigma^2 | Y) = (2\pi\sigma^2)^{-T/2} \exp\left(-\frac{1}{2\sigma^2}\sum_{t=1}^{T}\epsilon_t^2\right) $$

onde $Y = [Y_1, Y_2, \dots, Y_T]$ √© a s√©rie temporal observada, e os erros $\epsilon_t$ s√£o calculados recursivamente a partir da equa√ß√£o do modelo MA(p). Note que o c√°lculo dos erros $\epsilon_t$ envolve retroceder no tempo e que essa retroa√ß√£o, na pr√°tica, √© feita condicionando os erros iniciais em zero. Isso pode introduzir algumas complica√ß√µes, mas as aproxima√ß√µes que normalmente fazemos tendem a convergir para solu√ß√µes est√°veis quando o tamanho da s√©rie temporal aumenta.

O uso de proje√ß√µes lineares √© crucial aqui, pois, permite estimar os par√¢metros $\mu$ e $\theta_j$, bem como a vari√¢ncia do erro $\sigma^2$. Atrav√©s da maximiza√ß√£o da fun√ß√£o de *likelihood*, podemos obter estimadores para esses par√¢metros que s√£o consistentes e assintoticamente eficientes.

<!-- END -->
