## Proje√ß√£o Linear e Regress√£o de M√≠nimos Quadrados Ordin√°rios (OLS)

### Introdu√ß√£o
Este cap√≠tulo explora a conex√£o fundamental entre a **proje√ß√£o linear** e a **regress√£o de m√≠nimos quadrados ordin√°rios (OLS)**, dois m√©todos cruciais na an√°lise de s√©ries temporais e previs√£o. Como vimos anteriormente, a proje√ß√£o linear busca encontrar a melhor aproxima√ß√£o linear de uma vari√°vel aleat√≥ria em termos de outra [^4.1.9]. A regress√£o OLS, por outro lado, √© uma t√©cnica estat√≠stica para encontrar a rela√ß√£o linear que minimiza a soma dos quadrados dos erros entre os valores observados e preditos [^4.1.17]. Ao demonstrar como a regress√£o OLS surge como um caso especial da proje√ß√£o linear, este cap√≠tulo unifica essas abordagens e fornece uma base mais profunda para entender seus fundamentos te√≥ricos e pr√°ticos.

### Conceitos Fundamentais
Um **modelo de regress√£o linear** relaciona uma observa√ß√£o $y_{t+1}$ com um vetor de vari√°veis explicativas $x_t$:

$$y_{t+1} = \beta'x_t + u_t$$ [^4.1.16]

Aqui, $\beta$ √© um vetor de coeficientes, e $u_t$ √© um termo de erro aleat√≥rio. Dada uma amostra de $T$ observa√ß√µes de $y$ e $x$, a **soma amostral dos res√≠duos quadrados** √© definida como:

$$ \sum_{t=1}^T (y_{t+1} - \beta'x_t)^2 $$ [^4.1.17]

O **estimador de m√≠nimos quadrados ordin√°rios (OLS)**, denotado por $b$, √© o valor de $\beta$ que minimiza esta soma. A f√≥rmula para $b$ √© dada por:

$$ b = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_{t+1} $$ [^4.1.18]

Esta f√≥rmula pode ser expressa de forma equivalente como:

$$ b = \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t y_{t+1} \right] $$ [^4.1.19]

> üí° **Exemplo Num√©rico:** Vamos considerar um caso simples onde temos $T=3$ observa√ß√µes. Seja $x_t$ uma √∫nica vari√°vel explicativa e $y_{t+1}$ a vari√°vel dependente. Suponha que as observa√ß√µes sejam:
>
>  | t  | $x_t$ | $y_{t+1}$ |
>  |----|-------|-----------|
>  | 1  | 2     | 5         |
>  | 2  | 3     | 8         |
>  | 3  | 4     | 10        |
>
> Primeiro, calculamos $\sum_{t=1}^T x_t x_t' = (2^2) + (3^2) + (4^2) = 4 + 9 + 16 = 29$
>
> Em seguida, calculamos $\sum_{t=1}^T x_t y_{t+1} = (2 \times 5) + (3 \times 8) + (4 \times 10) = 10 + 24 + 40 = 74$
>
> Assim, o estimador OLS $b$ √©:
>
> $$b = \frac{74}{29} \approx 2.55$$
>
> Este valor de $b$ representa a inclina√ß√£o da reta que melhor se ajusta aos dados, no sentido de minimizar a soma dos quadrados dos erros.

Comparando o estimador OLS $b$ na equa√ß√£o [^4.1.19] com o coeficiente de proje√ß√£o linear $\alpha$ na equa√ß√£o [^4.1.13]:

$$ \alpha' = E(Y_{t+1} X_t') [E(X_t X_t')]^{-1} $$

Observamos que $b$ √© constru√≠do a partir de momentos amostrais, enquanto $\alpha$ √© constru√≠do a partir de momentos populacionais. Em outras palavras, a regress√£o OLS resume as observa√ß√µes da amostra, enquanto a proje√ß√£o linear resume as caracter√≠sticas populacionais do processo estoc√°stico $\{X_t, Y_{t+1}\}$.

Apesar de suas diferen√ßas em termos de uso de amostras ou popula√ß√£o, h√° uma rela√ß√£o matem√°tica formal entre as duas abordagens. O Ap√™ndice 4.A do texto detalha esse paralelismo e demonstra como a regress√£o OLS pode ser vista como um caso especial da proje√ß√£o linear. Especificamente, se o processo estoc√°stico $\{X_t, Y_{t+1}\}$ for **covari√¢ncia-estacion√°rio e erg√≥dico** para momentos de segunda ordem, ent√£o os momentos amostrais convergir√£o para os momentos populacionais quando o tamanho da amostra $T$ tende ao infinito:

$$ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t' \overset{p}{\rightarrow} E(X_t X_t') $$
$$ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t y_{t+1} \overset{p}{\rightarrow} E(X_t Y_{t+1}) $$

onde $\overset{p}{\rightarrow}$ denota converg√™ncia em probabilidade [^4.1.20]. Portanto, o estimador OLS $b$ converge em probabilidade para o coeficiente de proje√ß√£o linear $\alpha$:

$$ b \overset{p}{\rightarrow} \alpha $$ [^4.1.20]

Este resultado √© crucial porque ele estabelece que a regress√£o OLS, sob certas condi√ß√µes, fornece uma estimativa consistente do coeficiente de proje√ß√£o linear. √â importante notar que essa converg√™ncia requer apenas que o processo seja erg√≥dico para momentos de segunda ordem. Por outro lado, a an√°lise econom√©trica estrutural, que busca rela√ß√µes causais entre $X$ e $Y$, exige pressupostos muito mais fortes sobre a rela√ß√£o entre $X$ e $Y$. Em contextos de proje√ß√£o linear, o foco √© apenas na previs√£o, onde se busca a melhor previs√£o poss√≠vel, independentemente da causalidade [^4.1.20].

**Lema 1** (Consist√™ncia do estimador OLS): Dada a premissa de que o processo estoc√°stico $\{X_t, Y_{t+1}\}$ √© covari√¢ncia-estacion√°rio e erg√≥dico para momentos de segunda ordem, e que $E(X_tX_t')$ √© uma matriz n√£o-singular, o estimador OLS $b$ converge em probabilidade para o coeficiente de proje√ß√£o linear $\alpha$. Isso significa que, √† medida que o tamanho da amostra $T$ aumenta, o estimador OLS $b$ se aproxima cada vez mais do valor verdadeiro do coeficiente de proje√ß√£o linear $\alpha$.

*Proof Outline*: The proof follows directly from the convergence of sample moments to population moments and the continuous mapping theorem. Specifically, the sample covariance matrices, $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ and $\frac{1}{T} \sum_{t=1}^T x_t y_{t+1}$, converge in probability to their respective population counterparts, $E(X_t X_t')$ and $E(X_t Y_{t+1})$. The inverse operation and matrix multiplication are continuous functions, therefore, $b$ converges in probability to $\alpha$.

*Prova:*
Provaremos que se o processo estoc√°stico $\{X_t, Y_{t+1}\}$ √© covari√¢ncia-estacion√°rio e erg√≥dico para momentos de segunda ordem, e $E(X_t X_t')$ √© n√£o singular, ent√£o $b \overset{p}{\rightarrow} \alpha$.

I.  Pela defini√ß√£o do estimador OLS, temos:
    $$b = \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t y_{t+1} \right]$$

II.  Como $\{X_t, Y_{t+1}\}$ √© covari√¢ncia-estacion√°rio e erg√≥dico para momentos de segunda ordem, os momentos amostrais convergem em probabilidade para os momentos populacionais:
    $$\left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t' \overset{p}{\rightarrow} E(X_t X_t')$$
    $$\left( \frac{1}{T} \right) \sum_{t=1}^T x_t y_{t+1} \overset{p}{\rightarrow} E(X_t Y_{t+1})$$

III. Substituindo os limites na express√£o para $b$, obtemos:
    $$b \overset{p}{\rightarrow} [E(X_t X_t')]^{-1} E(X_t Y_{t+1})$$

IV. Pela defini√ß√£o do coeficiente de proje√ß√£o linear $\alpha$, temos:
    $$\alpha' = E(Y_{t+1} X_t') [E(X_t X_t')]^{-1}$$
    Que √© equivalente a
    $$\alpha = [E(X_t X_t')]^{-1} E(X_t Y_{t+1})$$

V.  Comparando as express√µes, vemos que:
    $$b \overset{p}{\rightarrow} \alpha$$
    Portanto, o estimador OLS $b$ converge em probabilidade para o coeficiente de proje√ß√£o linear $\alpha$. ‚ñ†

**Teorema 1** (Decomposi√ß√£o da Vari√¢ncia): A vari√¢ncia de $Y_{t+1}$ pode ser decomposta em uma parte explicada pela proje√ß√£o linear e uma parte n√£o explicada (erro). De forma mais precisa, se denotarmos a proje√ß√£o linear de $Y_{t+1}$ sobre $X_t$ como $\hat{Y}_{t+1} = \alpha'X_t$, e o erro de proje√ß√£o como $U_{t+1} = Y_{t+1} - \hat{Y}_{t+1}$, ent√£o temos:
$$Var(Y_{t+1}) = Var(\hat{Y}_{t+1}) + Var(U_{t+1})$$
Al√©m disso, $\hat{Y}_{t+1}$ e $U_{t+1}$ s√£o n√£o correlacionados.
*Proof Outline:*  The proof relies on the properties of linear projections and the orthogonality principle.  Since $U_{t+1}$ is the error from the best linear projection, it is orthogonal to the space spanned by $X_t$, which implies $Cov(\hat{Y}_{t+1}, U_{t+1})=0$. Thus, $Var(Y_{t+1}) = Var(\hat{Y}_{t+1} + U_{t+1}) = Var(\hat{Y}_{t+1}) + Var(U_{t+1}) + 2Cov(\hat{Y}_{t+1}, U_{t+1}) = Var(\hat{Y}_{t+1}) + Var(U_{t+1})$.

*Prova:*
Provaremos que $Var(Y_{t+1}) = Var(\hat{Y}_{t+1}) + Var(U_{t+1})$ e que $\hat{Y}_{t+1}$ e $U_{t+1}$ s√£o n√£o correlacionados.

I.  Por defini√ß√£o, temos $Y_{t+1} = \hat{Y}_{t+1} + U_{t+1}$.

II. A vari√¢ncia de $Y_{t+1}$ √© ent√£o:
    $$Var(Y_{t+1}) = Var(\hat{Y}_{t+1} + U_{t+1})$$

III.  Expandindo a vari√¢ncia, obtemos:
   $$Var(Y_{t+1}) = Var(\hat{Y}_{t+1}) + Var(U_{t+1}) + 2Cov(\hat{Y}_{t+1}, U_{t+1})$$

IV. Pela propriedade da proje√ß√£o linear, o erro $U_{t+1}$ √© ortogonal ao espa√ßo gerado por $X_t$, ou seja, $\hat{Y}_{t+1}$ e $U_{t+1}$ s√£o n√£o correlacionados, e sua covari√¢ncia √© zero:
   $$Cov(\hat{Y}_{t+1}, U_{t+1}) = 0$$

V.  Portanto, a vari√¢ncia de $Y_{t+1}$ se simplifica para:
    $$Var(Y_{t+1}) = Var(\hat{Y}_{t+1}) + Var(U_{t+1})$$
    E $\hat{Y}_{t+1}$ e $U_{t+1}$ s√£o n√£o correlacionados. ‚ñ†

> üí° **Exemplo Num√©rico:** Vamos usar o exemplo anterior onde $b \approx 2.55$. Considere que a m√©dia de $y$ √© $\bar{y} = (5 + 8 + 10)/3 = 7.67$ e a vari√¢ncia amostral de $y$ √© aproximadamente $Var(y) = \frac{(5-7.67)^2+(8-7.67)^2+(10-7.67)^2}{3-1} = 6.33$. As predi√ß√µes seriam $\hat{y}_{t+1} = 2.55x_t$.
> Ent√£o $\hat{y}_1 = 2.55 \times 2 = 5.1$, $\hat{y}_2 = 2.55 \times 3 = 7.65$, e $\hat{y}_3 = 2.55 \times 4 = 10.2$.
>
> Os res√≠duos s√£o $u_1 = 5 - 5.1 = -0.1$, $u_2 = 8 - 7.65 = 0.35$, $u_3 = 10 - 10.2 = -0.2$.
>
>  $Var(\hat{y}) =  \frac{(5.1 - 7.65)^2 + (7.65 - 7.65)^2 + (10.2 - 7.65)^2}{3-1}  = 6.71$ (aproximadamente). A vari√¢ncia dos erros seria $Var(u) = \frac{(-0.1-0.017)^2+(0.35-0.017)^2+(-0.2-0.017)^2}{3-1} = 0.067$ (aproximadamente), onde a m√©dia dos erros $u$ √©  $(-0.1 + 0.35 - 0.2)/3 = 0.017$. Note que devido √† quantidade pequena de amostras, a igualdade n√£o se verifica perfeitamente, mas conceitualmente:
>  $Var(y) \approx Var(\hat{y}) + Var(u)$, $6.33 \approx 6.71 + 0.067 $. A parte da vari√¢ncia de y explicada pelo modelo √© $Var(\hat{y})$.
>

**Proposi√ß√£o 1.1** (R-Squared): O $R^2$ da regress√£o OLS, que mede a propor√ß√£o da vari√¢ncia de $Y_{t+1}$ que √© explicada pela regress√£o, √© definido como:
$$R^2 = \frac{Var(\hat{Y}_{t+1})}{Var(Y_{t+1})} = 1 - \frac{Var(U_{t+1})}{Var(Y_{t+1})}$$
O $R^2$ varia entre 0 e 1, indicando a qualidade do ajuste da regress√£o linear. Um valor mais pr√≥ximo de 1 indica um ajuste melhor.

*Proof Outline:* The proof follows directly from the variance decomposition theorem. Since $Var(Y_{t+1}) = Var(\hat{Y}_{t+1}) + Var(U_{t+1})$, we can divide by $Var(Y_{t+1})$ to obtain $1 = \frac{Var(\hat{Y}_{t+1})}{Var(Y_{t+1})} + \frac{Var(U_{t+1})}{Var(Y_{t+1})}$, which gives us the desired definition of $R^2$.

*Prova:*
Provaremos que $R^2 = \frac{Var(\hat{Y}_{t+1})}{Var(Y_{t+1})} = 1 - \frac{Var(U_{t+1})}{Var(Y_{t+1})}$.

I. Do Teorema 1 (Decomposi√ß√£o da Vari√¢ncia), temos:
    $$Var(Y_{t+1}) = Var(\hat{Y}_{t+1}) + Var(U_{t+1})$$

II. Dividindo ambos os lados da equa√ß√£o por $Var(Y_{t+1})$, obtemos:
    $$1 = \frac{Var(\hat{Y}_{t+1})}{Var(Y_{t+1})} + \frac{Var(U_{t+1})}{Var(Y_{t+1})}$$

III. Rearranjando a equa√ß√£o, definimos o $R^2$ como:
    $$R^2 = \frac{Var(\hat{Y}_{t+1})}{Var(Y_{t+1})} = 1 - \frac{Var(U_{t+1})}{Var(Y_{t+1})}$$
    ‚ñ†

> üí° **Exemplo Num√©rico:** Usando os valores do exemplo anterior, podemos calcular o $R^2$ como:
> $$R^2 = \frac{Var(\hat{Y}_{t+1})}{Var(Y_{t+1})} \approx \frac{6.71}{6.33} \approx 1.06$$
> Ou, de forma equivalente
> $$R^2 = 1 - \frac{Var(U_{t+1})}{Var(Y_{t+1})} \approx 1 - \frac{0.067}{6.33} \approx 0.989 $$
> Note que devido √† pequena quantidade de amostras, $R^2$ ficou acima de 1 na primeira f√≥rmula, o que n√£o √© esperado. Com maiores amostras o valor deve ser inferior a 1.  O valor de 0.989 indica que o modelo explica cerca de 98.9% da variabilidade dos dados, o que significa um bom ajuste.
>

**Lema 1.1** (Erro da Proje√ß√£o): O erro da proje√ß√£o $U_{t+1}$ tem m√©dia zero, ou seja, $E[U_{t+1}] = 0$.
*Proof Outline:* This follows from the properties of projection. By definition, the projection $\hat{Y}_{t+1}$ minimizes the mean squared error, which is achieved when $E[U_{t+1}] = 0$. More formally, $E[Y_{t+1}] = E[\hat{Y}_{t+1} + U_{t+1}] = E[\hat{Y}_{t+1}] + E[U_{t+1}]$. And because $\hat{Y}_{t+1}$ is the best linear approximation, it implies that $E[U_{t+1}] = 0$.

*Prova:*
Provaremos que $E[U_{t+1}] = 0$.

I.  Por defini√ß√£o, $Y_{t+1} = \hat{Y}_{t+1} + U_{t+1}$.

II.  Tomando o valor esperado de ambos os lados, temos:
    $$E[Y_{t+1}] = E[\hat{Y}_{t+1} + U_{t+1}]$$

III.  Pela linearidade do valor esperado:
    $$E[Y_{t+1}] = E[\hat{Y}_{t+1}] + E[U_{t+1}]$$

IV.  Como $\hat{Y}_{t+1}$ √© a melhor aproxima√ß√£o linear de $Y_{t+1}$, o erro $U_{t+1}$ deve ter m√©dia zero:
    $$E[U_{t+1}] = E[Y_{t+1}] - E[\hat{Y}_{t+1}]$$

V.  Substituindo $\hat{Y}_{t+1} = \alpha' X_t$:
     $$ E[U_{t+1}] = E[Y_{t+1}] - E[\alpha' X_t] $$

VI. Usando a defini√ß√£o de $\alpha$, o erro de proje√ß√£o $U_{t+1}$ √© ortogonal a $X_t$.
      Portanto, o valor esperado do erro √© zero:
    $$E[U_{t+1}] = 0$$
     ‚ñ†

Al√©m disso, podemos analisar a distribui√ß√£o dos res√≠duos $u_t$ do modelo de regress√£o linear. Se assumirmos que os res√≠duos s√£o independentes e identicamente distribu√≠dos (i.i.d.) com m√©dia zero e vari√¢ncia constante (homoscedasticidade), ent√£o podemos aplicar os resultados assint√≥ticos da estat√≠stica para construir testes de hip√≥teses sobre os coeficientes da regress√£o $\beta$ e fazer infer√™ncia estat√≠stica.

### Conclus√£o

Este cap√≠tulo apresentou a liga√ß√£o intr√≠nseca entre a proje√ß√£o linear e a regress√£o OLS. A regress√£o OLS, que √© frequentemente usada para estimar modelos de s√©ries temporais e previs√£o, pode ser entendida como um caso particular da proje√ß√£o linear, onde os momentos populacionais s√£o substitu√≠dos pelos momentos amostrais. Esta conex√£o te√≥rica fornece uma base mais profunda para interpretar e usar essas t√©cnicas. Al√©m disso, a an√°lise da converg√™ncia em probabilidade do estimador OLS para o coeficiente de proje√ß√£o linear, sob condi√ß√µes de estacionariedade e ergodicidade, valida o uso da regress√£o OLS para a previs√£o, mesmo quando n√£o se est√° interessado em rela√ß√µes causais.

### Refer√™ncias
[^4.1.9]: A proje√ß√£o linear √© definida como a melhor aproxima√ß√£o linear de uma vari√°vel aleat√≥ria em termos de outra.
[^4.1.13]: A f√≥rmula para o coeficiente de proje√ß√£o linear $\alpha$ √© dada por  $\alpha' = E(Y_{t+1} X_t') [E(X_t X_t')]^{-1}$.
[^4.1.16]: Um modelo de regress√£o linear relaciona uma observa√ß√£o $y_{t+1}$ com um vetor de vari√°veis explicativas $x_t$: $y_{t+1} = \beta'x_t + u_t$.
[^4.1.17]: A soma amostral dos res√≠duos quadrados √© definida como $\sum_{t=1}^T (y_{t+1} - \beta'x_t)^2$.
[^4.1.18]: O estimador OLS √© dado por $b = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_{t+1}$.
[^4.1.19]: A formula para b pode ser escrita como $b = \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t y_{t+1} \right]$.
[^4.1.20]: Em condi√ß√µes de estacionariedade e ergodicidade, os momentos amostrais convergem para os momentos populacionais e o estimador OLS converge para o coeficiente de proje√ß√£o linear.
<!-- END -->
