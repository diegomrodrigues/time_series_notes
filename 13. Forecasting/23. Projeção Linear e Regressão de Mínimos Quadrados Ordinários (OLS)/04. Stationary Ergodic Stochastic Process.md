## Proje√ß√£o Linear e Regress√£o OLS sob Condi√ß√µes de Estacionariedade e Ergodicidade

### Introdu√ß√£o

Este cap√≠tulo explora as implica√ß√µes da **estacionariedade** e **ergodicidade** para o relacionamento entre **proje√ß√£o linear** e **regress√£o de m√≠nimos quadrados ordin√°rios (OLS)**. Nos cap√≠tulos anteriores, estabelecemos que a proje√ß√£o linear opera no dom√≠nio dos momentos populacionais, enquanto a regress√£o OLS opera no dom√≠nio dos momentos amostrais [^4.1.13, 4.1.18, 4.1.19]. A presente discuss√£o demonstra que, sob condi√ß√µes espec√≠ficas de estacionariedade e ergodicidade, a regress√£o OLS fornece uma estimativa consistente dos coeficientes de proje√ß√£o linear, sem a necessidade de informa√ß√µes populacionais completas.

### Estacionariedade e Ergodicidade

A **estacionariedade** em s√©ries temporais refere-se √† propriedade de que as caracter√≠sticas estat√≠sticas de um processo (como m√©dia e vari√¢ncia) n√£o se alteram ao longo do tempo. Formalmente, um processo estoc√°stico $\{X_t\}$ √© considerado **estritamente estacion√°rio** se a distribui√ß√£o conjunta de qualquer sequ√™ncia de observa√ß√µes $X_{t_1}, X_{t_2}, \ldots, X_{t_k}$ √© id√™ntica √† distribui√ß√£o conjunta de $X_{t_1+h}, X_{t_2+h}, \ldots, X_{t_k+h}$ para qualquer $h$ e qualquer sequ√™ncia de instantes de tempo $t_1, t_2, \ldots, t_k$. Um processo √© **fracamente estacion√°rio** (ou estacion√°rio em covari√¢ncia) se a m√©dia, a vari√¢ncia e a autocovari√¢ncia do processo s√£o constantes ao longo do tempo.

A **ergodicidade**, por sua vez, √© uma propriedade que permite inferir propriedades populacionais de um processo a partir de uma √∫nica realiza√ß√£o longa desse processo. Intuitivamente, um processo erg√≥dico 'visita' todos os seus estados poss√≠veis ao longo de um tempo suficientemente longo, permitindo que as m√©dias temporais se aproximem das m√©dias populacionais. Um processo erg√≥dico para momentos de segunda ordem √© tal que, para quaisquer fun√ß√µes $f$ e $g$ para as quais a esperan√ßa seja bem definida:

$$ \frac{1}{T}\sum_{t=1}^T f(X_t)g(X_{t-k}) \overset{p}{\rightarrow} E[f(X_t)g(X_{t-k})] $$

onde $\overset{p}{\rightarrow}$ denota converg√™ncia em probabilidade.
**Observa√ß√£o 2:** Note que a defini√ß√£o de ergodicidade para momentos de segunda ordem √© suficiente para os resultados que ser√£o apresentados neste cap√≠tulo, mas existem defini√ß√µes mais gerais de ergodicidade. Especificamente, a defini√ß√£o acima √© suficiente para garantir a validade da Lei dos Grandes N√∫meros (LLN) para os momentos amostrais que surgir√£o na an√°lise da regress√£o OLS.

A combina√ß√£o de estacionariedade e ergodicidade √© crucial, pois ela permite a conex√£o entre momentos populacionais e momentos amostrais. Em termos pr√°ticos, se o processo subjacente for estacion√°rio e erg√≥dico, a regress√£o OLS √© capaz de capturar as caracter√≠sticas do processo, aproximando-se da proje√ß√£o linear √† medida que o tamanho da amostra aumenta.
> üí° **Exemplo Num√©rico:** Vamos considerar um exemplo de um processo AR(1) estacion√°rio e erg√≥dico. Suponha que temos uma s√©rie temporal $X_t$ que segue o modelo $X_t = 0.7X_{t-1} + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2 = 1$.  Como o coeficiente autoregressivo (0.7) tem valor absoluto menor que 1, o processo √© estacion√°rio.  Para ilustrar a ergodicidade, vamos simular uma longa s√©rie temporal e calcular a m√©dia amostral e compar√°-la com a m√©dia populacional te√≥rica. Como o processo tem m√©dia zero, esperamos que a m√©dia amostral se aproxime de zero.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> phi = 0.7
> sigma = 1
> T = 10000 # Tamanho da amostra
>
> # Simula√ß√£o do AR(1)
> np.random.seed(42)
> epsilon = np.random.normal(0, sigma, T)
> X = np.zeros(T)
> X[0] = np.random.normal(0, sigma/(1-phi**2)**0.5) # inicializa√ß√£o
> for t in range(1, T):
>     X[t] = phi * X[t-1] + epsilon[t]
>
> # C√°lculo da m√©dia amostral
> media_amostral = np.mean(X)
> print(f"M√©dia Amostral: {media_amostral:.4f}")
>
> # Plot da s√©rie
> plt.plot(X)
> plt.title("S√©rie Temporal AR(1)")
> plt.xlabel("Tempo")
> plt.ylabel("X_t")
> plt.show()
> ```
> A m√©dia amostral calculada para a s√©rie simulada estar√° bem pr√≥xima de zero (o valor te√≥rico da m√©dia populacional). Isso demonstra que, devido √† ergodicidade, a m√©dia amostral de uma √∫nica longa realiza√ß√£o do processo se aproxima da m√©dia populacional.

### Converg√™ncia da Regress√£o OLS para a Proje√ß√£o Linear

Como vimos anteriormente, a proje√ß√£o linear √© definida como:
$$ \hat{Y}_{t+1} = \alpha' X_t $$
onde
$$ \alpha' = E(Y_{t+1} X_t') [E(X_t X_t')]^{-1} $$
e a regress√£o OLS √© definida como:
$$ b = \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t y_{t+1} \right] $$
Sob condi√ß√µes de estacionariedade e ergodicidade, a Lei dos Grandes N√∫meros garante que os momentos amostrais convergem para os momentos populacionais √† medida que o tamanho da amostra $T$ tende ao infinito [^4.1.20]:

$$ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t' \overset{p}{\rightarrow} E(X_t X_t') $$

$$ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t y_{t+1} \overset{p}{\rightarrow} E(X_t Y_{t+1}) $$

Consequentemente, o estimador OLS $b$ converge em probabilidade para o coeficiente de proje√ß√£o linear $\alpha$ [^4.1.20]:
$$ b \overset{p}{\rightarrow} \alpha $$
**Lema 1.1** (Converg√™ncia da Inversa de uma matriz): Seja $A_T$ uma sequ√™ncia de matrizes aleat√≥rias que convergem em probabilidade para uma matriz n√£o singular $A$, ent√£o $A_T^{-1}$ converge em probabilidade para $A^{-1}$.
*Prova:*
A prova desse lema pode ser encontrada em diversos textos de econometria e an√°lise matem√°tica. Ela utiliza o conceito de continuidade da fun√ß√£o inversa e o Teorema da Fun√ß√£o Cont√≠nua (Continuous Mapping Theorem).

Este resultado √© crucial, pois ele justifica o uso da regress√£o OLS como uma maneira pr√°tica de estimar a melhor aproxima√ß√£o linear entre vari√°veis, mesmo quando os momentos populacionais s√£o desconhecidos. Ou seja, mesmo que n√£o conhe√ßamos $E(Y_{t+1} X_t')$ e $E(X_t X_t')$, se os processos s√£o estacion√°rios e erg√≥dicos, e temos amostras grandes, ent√£o $b$ √© um bom estimador de $\alpha$.

**Teorema 1** (Converg√™ncia do estimador OLS sob estacionariedade e ergodicidade): Se o processo estoc√°stico $\{X_t, Y_{t+1}\}$ √© covari√¢ncia-estacion√°rio e erg√≥dico para momentos de segunda ordem, e se $E(X_tX_t')$ √© uma matriz n√£o-singular, ent√£o o estimador OLS $b$ converge em probabilidade para o coeficiente de proje√ß√£o linear $\alpha$.
*Prova:*
A prova deste teorema √© uma aplica√ß√£o direta da Lei dos Grandes N√∫meros e do Teorema da Fun√ß√£o Cont√≠nua.

I. Pela defini√ß√£o do estimador OLS:
$$ b = \left[ \frac{1}{T} \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \frac{1}{T} \sum_{t=1}^T x_t y_{t+1} \right] $$

II. Pela estacionariedade e ergodicidade, e pelo lema 1.1 do cap√≠tulo anterior, os momentos amostrais convergem em probabilidade para seus equivalentes populacionais:
    $$ \frac{1}{T} \sum_{t=1}^T x_t x_t' \overset{p}{\rightarrow} E(X_t X_t') $$
    $$ \frac{1}{T} \sum_{t=1}^T x_t y_{t+1} \overset{p}{\rightarrow} E(X_t Y_{t+1}) $$

III. Substituindo os limites na express√£o de $b$, e usando o Lema 1.1:
$$ b \overset{p}{\rightarrow} [E(X_t X_t')]^{-1} E(X_t Y_{t+1}) $$
IV.  O lado direito da equa√ß√£o acima √© a defini√ß√£o do coeficiente de proje√ß√£o linear $\alpha$:
$$ \alpha = [E(X_t X_t')]^{-1} E(X_t Y_{t+1}) $$
V. Portanto, conclu√≠mos que:
$$ b \overset{p}{\rightarrow} \alpha $$
Ou seja, o estimador OLS $b$ converge em probabilidade para o coeficiente de proje√ß√£o linear $\alpha$ sob as condi√ß√µes especificadas. ‚ñ†

**Lema 1** (Consist√™ncia da regress√£o OLS): Em um processo estoc√°stico que √© covari√¢ncia-estacion√°rio e erg√≥dico para momentos de segunda ordem, a regress√£o OLS fornece um estimador consistente do coeficiente de proje√ß√£o linear, ou seja, quando $T \to \infty$, o estimador OLS $b$ se aproxima do valor verdadeiro do coeficiente de proje√ß√£o linear $\alpha$.
*Prova:*
A demonstra√ß√£o desse lema est√° impl√≠cita na prova do Teorema 1. Uma vez que a converg√™ncia em probabilidade do estimador OLS para o coeficiente de proje√ß√£o linear j√° foi estabelecida no Teorema 1, o Lema 1 √© uma consequ√™ncia direta desse resultado.

Este lema destaca que, em condi√ß√µes de estacionariedade e ergodicidade, a regress√£o OLS n√£o apenas converge para o valor "correto", mas faz isso usando apenas informa√ß√µes da amostra. Isso √© fundamental na pr√°tica, pois raramente temos acesso aos momentos populacionais.
**Observa√ß√£o 1:** Uma condi√ß√£o fundamental para a converg√™ncia do estimador OLS para o coeficiente da proje√ß√£o linear √© que a matriz de covari√¢ncia populacional $E(X_t X_t')$ seja n√£o-singular. Isso garante que a inversa dessa matriz exista e que o coeficiente seja bem definido. Caso essa condi√ß√£o seja violada (por exemplo, se existe multicolinearidade perfeita entre as vari√°veis em $X_t$), a regress√£o OLS n√£o funcionar√° de forma correta e os coeficientes ser√£o indeterminados.

> üí° **Exemplo Num√©rico:** Considere um modelo em que queremos prever o pre√ßo de uma a√ß√£o ($Y_{t+1}$) usando o pre√ßo da a√ß√£o no dia anterior ($X_t$).  Assumimos que o processo conjunto √© estacion√°rio e erg√≥dico. O modelo √© dado por:
> $$Y_{t+1} = \alpha X_t + U_{t+1}$$
> onde $U_{t+1}$ √© um termo de erro.  Na realidade, o verdadeiro $\alpha$ (coeficiente de proje√ß√£o linear) √© 0.8. Isso significa que, em m√©dia, o pre√ßo da a√ß√£o no dia seguinte √© 80% do pre√ßo do dia anterior mais algum choque aleat√≥rio. Usaremos dados simulados para ilustrar como a regress√£o OLS estima consistentemente esse valor.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
>
> # Par√¢metros
> alpha_verdadeiro = 0.8
> sigma_erro = 0.2
> T = 1000
>
> # Simula√ß√£o de dados
> np.random.seed(42)
> X = np.random.normal(100, 10, T)  # Pre√ßo da a√ß√£o no dia t
> U = np.random.normal(0, sigma_erro, T) # Choques aleat√≥rios
> Y = alpha_verdadeiro * X[:-1] + U[1:] # Pre√ßo da a√ß√£o no dia t+1
>
> # Criando DataFrame
> data = pd.DataFrame({'X_t': X[:-1], 'Y_t1': Y})
>
> # Regress√£o OLS
> X_reg = sm.add_constant(data['X_t'])
> modelo = sm.OLS(data['Y_t1'], X_reg)
> resultados = modelo.fit()
>
> print(resultados.summary())
> print(f"Coeficiente estimado (b): {resultados.params['X_t']:.4f}")
>
> # Comparando com o valor verdadeiro
> print(f"Coeficiente verdadeiro (alpha): {alpha_verdadeiro}")
> ```
> Este c√≥digo gera uma s√©rie de dados simulados, ajusta uma regress√£o OLS e mostra que o coeficiente estimado ('X_t') se aproxima do valor verdadeiro de $\alpha$ (0.8) quando o tamanho da amostra √© grande. √Ä medida que $T$ aumenta, a estimativa de $b$ ($b$) se aproxima de $\alpha$.
>
> Em termos de converg√™ncia em probabilidade, quando $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ se aproxima de $E(X_t X_t')$ e $\frac{1}{T} \sum_{t=1}^T x_t y_{t+1}$ se aproxima de $E(X_t Y_{t+1})$, o estimador OLS $b$ converge para o valor do coeficiente de proje√ß√£o linear $\alpha$, que nesse caso √© 0.8.

### Implica√ß√µes Pr√°ticas

1.  **Previs√£o:** A converg√™ncia do estimador OLS para o coeficiente de proje√ß√£o linear justifica o uso da regress√£o OLS em contextos de previs√£o. Mesmo quando n√£o se tem um interesse em causalidade, ou seja, quando n√£o se busca o efeito de $X_t$ em $Y_{t+1}$, mas apenas em obter a melhor previs√£o de $Y_{t+1}$ a partir de $X_t$, a regress√£o OLS fornece uma aproxima√ß√£o consistente para o coeficiente de proje√ß√£o linear.

2.  **An√°lise de S√©ries Temporais:**  Para modelos de s√©ries temporais, onde muitas vezes se assume que o processo √© estacion√°rio e erg√≥dico, a regress√£o OLS torna-se uma ferramenta valiosa para estimar os coeficientes de modelos autorregressivos e de m√©dias m√≥veis, que s√£o formas espec√≠ficas de proje√ß√£o linear.

3.  **Robustez:** O requisito de ergodicidade apenas para momentos de segunda ordem √© relativamente fraco, permitindo que a regress√£o OLS seja utilizada em uma ampla gama de contextos pr√°ticos, mesmo quando os pressupostos mais fortes sobre a distribui√ß√£o dos dados s√£o violados. Isso torna a abordagem bastante robusta em termos de aplica√ß√£o emp√≠rica.
**Observa√ß√£o 3:** Apesar da robustez mencionada, √© importante notar que a validade dos resultados assint√≥ticos da regress√£o OLS depende crucialmente da correta especifica√ß√£o do modelo. Ou seja, a forma funcional da proje√ß√£o linear deve estar alinhada com a rela√ß√£o verdadeira entre as vari√°veis.
> üí° **Exemplo Num√©rico:**  Para ilustrar a aplica√ß√£o em s√©ries temporais, considere um modelo AR(1) para prever a infla√ß√£o:
> $$Infla√ß√£o_{t+1} = \alpha Infla√ß√£o_t + U_{t+1}$$
> Assumindo que a infla√ß√£o √© estacion√°ria e erg√≥dica, podemos usar OLS para estimar o coeficiente $\alpha$. Vamos gerar dados simulados de infla√ß√£o e aplicar a regress√£o OLS:
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> alpha_verdadeiro = 0.6
> sigma_erro = 0.1
> T = 500
>
> # Simula√ß√£o de dados
> np.random.seed(42)
> inflacao = np.zeros(T)
> inflacao[0] = np.random.normal(0, sigma_erro/(1-alpha_verdadeiro**2)**0.5)
>
> U = np.random.normal(0, sigma_erro, T)
>
> for t in range(1, T):
>    inflacao[t] = alpha_verdadeiro * inflacao[t-1] + U[t]
>
> # Cria√ß√£o do DataFrame
> data = pd.DataFrame({'Inflacao_t': inflacao[:-1],'Inflacao_t1': inflacao[1:] })
>
> # Regress√£o OLS
> X = sm.add_constant(data['Inflacao_t'])
> modelo = sm.OLS(data['Inflacao_t1'], X)
> resultados = modelo.fit()
>
> print(resultados.summary())
> print(f"Coeficiente Estimado (b): {resultados.params['Inflacao_t']:.4f}")
>
> # Compara√ß√£o com o valor verdadeiro
> print(f"Coeficiente verdadeiro (alpha): {alpha_verdadeiro}")
>
> # Plot da s√©rie
> plt.plot(data['Inflacao_t1'])
> plt.title("S√©rie Temporal da Infla√ß√£o (Simulada)")
> plt.xlabel("Tempo")
> plt.ylabel("Inflacao_t")
> plt.show()
> ```
> O exemplo acima mostra que a regress√£o OLS pode ser aplicada a dados de s√©ries temporais para estimar os par√¢metros do modelo, desde que os dados sejam estacion√°rios e erg√≥dicos. A m√©dia estimada pelo OLS ($b$) converge para o valor de $\alpha$.

### A Import√¢ncia da Ergodicidade para a Consist√™ncia do Estimador OLS

A ergodicidade desempenha um papel crucial na garantia de que as m√©dias amostrais convergem para as m√©dias populacionais, o que, por sua vez, assegura a consist√™ncia do estimador OLS. Sem essa propriedade, a regress√£o OLS n√£o necessariamente convergiria para o coeficiente de proje√ß√£o linear, tornando-a menos √∫til como um m√©todo para estimar as rela√ß√µes te√≥ricas subjacentes.

**Teorema 1.2** (Distribui√ß√£o Assint√≥tica do Estimador OLS):
Sob condi√ß√µes adicionais de regularidade, o estimador OLS $b$ tem distribui√ß√£o assint√≥tica normal:
$$ \sqrt{T}(b - \alpha) \overset{d}{\rightarrow} N(0, V) $$
Onde $V = [E(X_tX_t')]^{-1} E(X_t X_t'U_{t+1}^2) [E(X_tX_t')]^{-1}$ e $U_{t+1} = Y_{t+1}-\alpha'X_t$ √© o erro de proje√ß√£o.

*Proof:*
A prova desse teorema envolve a aplica√ß√£o do Teorema do Limite Central para vari√°veis dependentes.

I. Come√ßamos com a express√£o do estimador OLS:
    $$ b = \left[ \frac{1}{T} \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \frac{1}{T} \sum_{t=1}^T x_t y_{t+1} \right] $$

II. Substitu√≠mos $y_{t+1}$ pela sua proje√ß√£o linear mais o erro:
 $$ b = \left[ \frac{1}{T} \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \frac{1}{T} \sum_{t=1}^T x_t (\alpha'x_t + U_{t+1}) \right] $$

III. Reorganizando e subtraindo $\alpha$ de ambos os lados:
    $$ b - \alpha = \left[ \frac{1}{T} \sum_{t=1}^T x_t x_t' \right]^{-1}  \left[ \frac{1}{T} \sum_{t=1}^T x_t U_{t+1} \right] $$
Multiplicando por $\sqrt{T}$:
    $$ \sqrt{T}(b - \alpha) = \left[ \frac{1}{T} \sum_{t=1}^T x_t x_t' \right]^{-1}  \left[ \frac{1}{\sqrt{T}} \sum_{t=1}^T x_t U_{t+1} \right] $$
IV. Pelas condi√ß√µes de regularidade e ergodicidade, temos:
    $$ \left[ \frac{1}{T} \sum_{t=1}^T x_t x_t' \right]^{-1} \overset{p}{\rightarrow} [E(X_tX_t')]^{-1} $$
V. A partir do Teorema do Limite Central (TLC) e usando que as vari√°veis $x_tU_{t+1}$ s√£o estacion√°rias e ergodicidade, temos:
    $$ \frac{1}{\sqrt{T}} \sum_{t=1}^T x_t U_{t+1} \overset{d}{\rightarrow} N(0, E(X_t X_t'U_{t+1}^2)) $$

VI. Usando o m√©todo delta, combinamos os resultados:
$$ \sqrt{T}(b - \alpha) \overset{d}{\rightarrow} N(0, [E(X_tX_t')]^{-1} E(X_t X_t'U_{t+1}^2) [E(X_tX_t')]^{-1}) $$
VII. Portanto,
$$ \sqrt{T}(b - \alpha) \overset{d}{\rightarrow} N(0, V) $$
Onde $V = [E(X_tX_t')]^{-1} E(X_t X_t'U_{t+1}^2) [E(X_tX_t')]^{-1}$ e $U_{t+1} = Y_{t+1}-\alpha'X_t$ √© o erro de proje√ß√£o. ‚ñ†

Este teorema estabelece que, sob certas condi√ß√µes, o estimador OLS tem uma distribui√ß√£o assint√≥tica normal, permitindo que realizemos testes de hip√≥teses sobre os coeficientes e construamos intervalos de confian√ßa.
**Lema 1.3** (Estimador Consistente da Vari√¢ncia Assint√≥tica): Seja $V$ a vari√¢ncia assint√≥tica do estimador OLS dada no Teorema 1.2, ent√£o um estimador consistente de $V$ √© dado por:
$$ \hat{V} = \left[ \frac{1}{T} \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \frac{1}{T} \sum_{t=1}^T x_t x_t' \hat{U}_{t+1}^2 \right] \left[ \frac{1}{T} \sum_{t=1}^T x_t x_t' \right]^{-1} $$
onde $\hat{U}_{t+1} = Y_{t+1} - b'X_t$ s√£o os res√≠duos OLS.
*Prova:*
A prova desse lema segue diretamente da aplica√ß√£o da Lei dos Grandes N√∫meros e do Teorema da Fun√ß√£o Cont√≠nua. A converg√™ncia em probabilidade dos momentos amostrais para os momentos populacionais e a converg√™ncia em probabilidade do estimador OLS $b$ para o coeficiente de proje√ß√£o linear $\alpha$ implicam que $\hat{V}$ converge para $V$.

Este lema √© importante na pr√°tica, pois permite que se construam intervalos de confian√ßa e se realizem testes de hip√≥teses sobre os coeficientes da regress√£o OLS, utilizando apenas os dados da amostra e os res√≠duos obtidos da regress√£o OLS.
> üí° **Exemplo Num√©rico:** Vamos usar os dados simulados de infla√ß√£o do exemplo anterior para demonstrar o uso da vari√¢ncia assint√≥tica para a constru√ß√£o de um intervalo de confian√ßa.
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
>
> # Par√¢metros
> alpha_verdadeiro = 0.6
> sigma_erro = 0.1
> T = 500
>
> # Simula√ß√£o de dados
> np.random.seed(42)
> inflacao = np.zeros(T)
> inflacao[0] = np.random.normal(0, sigma_erro/(1-alpha_verdadeiro**2)**0.5)
>
> U = np.random.normal(0, sigma_erro, T)
>
> for t in range(1, T):
>    inflacao[t] = alpha_verdadeiro * inflacao[t-1] + U[t]
>
> # Cria√ß√£o do DataFrame
> data = pd.DataFrame({'Inflacao_t': inflacao[:-1],'Inflacao_t1': inflacao[1:] })
>
> # Regress√£o OLS
> X = sm.add_constant(data['Inflacao_t'])
> modelo = sm.OLS(data['Inflacao_t1'], X)
> resultados = modelo.fit()
>
> # C√°lculo do intervalo de confian√ßa
> intervalo_confianca = resultados.conf_int(alpha=0.05)
>
> print(f"Coeficiente Estimado (b): {resultados.params['Inflacao_t']:.4f}")
> print(f"Intervalo de Confian√ßa (95%): {intervalo_confianca.loc['Inflacao_t'].values}")
> print(f"Coeficiente verdadeiro (alpha): {alpha_verdadeiro}")
> ```
> O c√≥digo acima calcula o intervalo de confian√ßa de 95% para o coeficiente estimado. O intervalo √© constru√≠do usando a distribui√ß√£o assint√≥tica normal do estimador OLS, e usando a estimativa consistente da vari√¢ncia assint√≥tica dada pelo Lema 1.3. Se o processo √© estacion√°rio e erg√≥dico, e o tamanho da amostra √© suficientemente grande, este intervalo de confian√ßa tem uma probabilidade de 95% de conter o verdadeiro valor do coeficiente de proje√ß√£o linear.

### Conclus√£o

Este cap√≠tulo refor√ßou a import√¢ncia da estacionariedade e da ergodicidade na an√°lise de s√©ries temporais e, em particular, na rela√ß√£o entre proje√ß√£o linear e regress√£o OLS. Sob essas condi√ß√µes, a regress√£o OLS n√£o √© apenas um m√©todo de ajuste amostral, mas tamb√©m um estimador consistente do coeficiente de proje√ß√£o linear, um par√¢metro populacional fundamental. Esta converg√™ncia, aliada √† distribui√ß√£o assint√≥tica normal do estimador OLS, estabelece uma base s√≥lida para a sua utiliza√ß√£o em uma ampla variedade de aplica√ß√µes pr√°ticas, especialmente em contextos onde o objetivo √© a previs√£o e modelagem de s√©ries temporais.

### Refer√™ncias

[^4.1.9]: A proje√ß√£o linear √© definida como a melhor aproxima√ß√£o linear de uma vari√°vel aleat√≥ria em termos de outra.
[^4.1.13]: A f√≥rmula para o coeficiente de proje√ß√£o linear $\alpha$ √© dada por  $\alpha' = E(Y_{t+1} X_t') [E(X_t X_t')]^{-1}$.
[^4.1.18]: O estimador OLS √© dado por $b = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_{t+1}$.
[^4.1.19]: A formula para b pode ser escrita como $b = \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t y_{t+1} \right]$.
[^4.1.20]: Em condi√ß√µes de estacionariedade e ergodicidade, os momentos amostrais convergem para os momentos populacionais e o estimador OLS converge para o coeficiente de proje√ß√£o linear.
<!-- END -->
