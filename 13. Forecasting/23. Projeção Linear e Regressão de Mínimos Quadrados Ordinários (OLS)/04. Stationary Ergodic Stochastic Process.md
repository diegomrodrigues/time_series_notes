## ProjeÃ§Ã£o Linear e RegressÃ£o OLS sob CondiÃ§Ãµes de Estacionariedade e Ergodicidade

### IntroduÃ§Ã£o

Este capÃ­tulo explora as implicaÃ§Ãµes da **estacionariedade** e **ergodicidade** para o relacionamento entre **projeÃ§Ã£o linear** e **regressÃ£o de mÃ­nimos quadrados ordinÃ¡rios (OLS)**. Nos capÃ­tulos anteriores, estabelecemos que a projeÃ§Ã£o linear opera no domÃ­nio dos momentos populacionais, enquanto a regressÃ£o OLS opera no domÃ­nio dos momentos amostrais [^4.1.13, 4.1.18, 4.1.19]. A presente discussÃ£o demonstra que, sob condiÃ§Ãµes especÃ­ficas de estacionariedade e ergodicidade, a regressÃ£o OLS fornece uma estimativa consistente dos coeficientes de projeÃ§Ã£o linear, sem a necessidade de informaÃ§Ãµes populacionais completas.

### Estacionariedade e Ergodicidade

A **estacionariedade** em sÃ©ries temporais refere-se Ã  propriedade de que as caracterÃ­sticas estatÃ­sticas de um processo (como mÃ©dia e variÃ¢ncia) nÃ£o se alteram ao longo do tempo. Formalmente, um processo estocÃ¡stico $\{X_t\}$ Ã© considerado **estritamente estacionÃ¡rio** se a distribuiÃ§Ã£o conjunta de qualquer sequÃªncia de observaÃ§Ãµes $X_{t_1}, X_{t_2}, \ldots, X_{t_k}$ Ã© idÃªntica Ã  distribuiÃ§Ã£o conjunta de $X_{t_1+h}, X_{t_2+h}, \ldots, X_{t_k+h}$ para qualquer $h$ e qualquer sequÃªncia de instantes de tempo $t_1, t_2, \ldots, t_k$. Um processo Ã© **fracamente estacionÃ¡rio** (ou estacionÃ¡rio em covariÃ¢ncia) se a mÃ©dia, a variÃ¢ncia e a autocovariÃ¢ncia do processo sÃ£o constantes ao longo do tempo.

A **ergodicidade**, por sua vez, Ã© uma propriedade que permite inferir propriedades populacionais de um processo a partir de uma Ãºnica realizaÃ§Ã£o longa desse processo. Intuitivamente, um processo ergÃ³dico 'visita' todos os seus estados possÃ­veis ao longo de um tempo suficientemente longo, permitindo que as mÃ©dias temporais se aproximem das mÃ©dias populacionais. Um processo ergÃ³dico para momentos de segunda ordem Ã© tal que, para quaisquer funÃ§Ãµes $f$ e $g$ para as quais a esperanÃ§a seja bem definida:

$$ \frac{1}{T}\sum_{t=1}^T f(X_t)g(X_{t-k}) \overset{p}{\rightarrow} E[f(X_t)g(X_{t-k})] $$

onde $\overset{p}{\rightarrow}$ denota convergÃªncia em probabilidade.
**ObservaÃ§Ã£o 2:** Note que a definiÃ§Ã£o de ergodicidade para momentos de segunda ordem Ã© suficiente para os resultados que serÃ£o apresentados neste capÃ­tulo, mas existem definiÃ§Ãµes mais gerais de ergodicidade. Especificamente, a definiÃ§Ã£o acima Ã© suficiente para garantir a validade da Lei dos Grandes NÃºmeros (LLN) para os momentos amostrais que surgirÃ£o na anÃ¡lise da regressÃ£o OLS.

A combinaÃ§Ã£o de estacionariedade e ergodicidade Ã© crucial, pois ela permite a conexÃ£o entre momentos populacionais e momentos amostrais. Em termos prÃ¡ticos, se o processo subjacente for estacionÃ¡rio e ergÃ³dico, a regressÃ£o OLS Ã© capaz de capturar as caracterÃ­sticas do processo, aproximando-se da projeÃ§Ã£o linear Ã  medida que o tamanho da amostra aumenta.
> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos considerar um exemplo de um processo AR(1) estacionÃ¡rio e ergÃ³dico. Suponha que temos uma sÃ©rie temporal $X_t$ que segue o modelo $X_t = 0.7X_{t-1} + \epsilon_t$, onde $\epsilon_t$ Ã© um ruÃ­do branco com mÃ©dia zero e variÃ¢ncia $\sigma^2 = 1$.  Como o coeficiente autoregressivo (0.7) tem valor absoluto menor que 1, o processo Ã© estacionÃ¡rio.  Para ilustrar a ergodicidade, vamos simular uma longa sÃ©rie temporal e calcular a mÃ©dia amostral e comparÃ¡-la com a mÃ©dia populacional teÃ³rica. Como o processo tem mÃ©dia zero, esperamos que a mÃ©dia amostral se aproxime de zero.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # ParÃ¢metros
> phi = 0.7
> sigma = 1
> T = 10000 # Tamanho da amostra
>
> # SimulaÃ§Ã£o do AR(1)
> np.random.seed(42)
> epsilon = np.random.normal(0, sigma, T)
> X = np.zeros(T)
> X[0] = np.random.normal(0, sigma/(1-phi**2)**0.5) # inicializaÃ§Ã£o
> for t in range(1, T):
>     X[t] = phi * X[t-1] + epsilon[t]
>
> # CÃ¡lculo da mÃ©dia amostral
> media_amostral = np.mean(X)
> print(f"MÃ©dia Amostral: {media_amostral:.4f}")
>
> # Plot da sÃ©rie
> plt.plot(X)
> plt.title("SÃ©rie Temporal AR(1)")
> plt.xlabel("Tempo")
> plt.ylabel("X_t")
> plt.show()
> ```
> A mÃ©dia amostral calculada para a sÃ©rie simulada estarÃ¡ bem prÃ³xima de zero (o valor teÃ³rico da mÃ©dia populacional). Isso demonstra que, devido Ã  ergodicidade, a mÃ©dia amostral de uma Ãºnica longa realizaÃ§Ã£o do processo se aproxima da mÃ©dia populacional.

### ConvergÃªncia da RegressÃ£o OLS para a ProjeÃ§Ã£o Linear

Como vimos anteriormente, a projeÃ§Ã£o linear Ã© definida como:
$$ \hat{Y}_{t+1} = \alpha' X_t $$
onde
$$ \alpha' = E(Y_{t+1} X_t') [E(X_t X_t')]^{-1} $$
e a regressÃ£o OLS Ã© definida como:
$$ b = \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t y_{t+1} \right] $$
Sob condiÃ§Ãµes de estacionariedade e ergodicidade, a Lei dos Grandes NÃºmeros garante que os momentos amostrais convergem para os momentos populacionais Ã  medida que o tamanho da amostra $T$ tende ao infinito [^4.1.20]:

$$ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t' \overset{p}{\rightarrow} E(X_t X_t') $$

$$ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t y_{t+1} \overset{p}{\rightarrow} E(X_t Y_{t+1}) $$

Consequentemente, o estimador OLS $b$ converge em probabilidade para o coeficiente de projeÃ§Ã£o linear $\alpha$ [^4.1.20]:
$$ b \overset{p}{\rightarrow} \alpha $$
**Lema 1.1** (ConvergÃªncia da Inversa de uma matriz): Seja $A_T$ uma sequÃªncia de matrizes aleatÃ³rias que convergem em probabilidade para uma matriz nÃ£o singular $A$, entÃ£o $A_T^{-1}$ converge em probabilidade para $A^{-1}$.
*Prova:*
A prova desse lema pode ser encontrada em diversos textos de econometria e anÃ¡lise matemÃ¡tica. Ela utiliza o conceito de continuidade da funÃ§Ã£o inversa e o Teorema da FunÃ§Ã£o ContÃ­nua (Continuous Mapping Theorem).

Este resultado Ã© crucial, pois ele justifica o uso da regressÃ£o OLS como uma maneira prÃ¡tica de estimar a melhor aproximaÃ§Ã£o linear entre variÃ¡veis, mesmo quando os momentos populacionais sÃ£o desconhecidos. Ou seja, mesmo que nÃ£o conheÃ§amos $E(Y_{t+1} X_t')$ e $E(X_t X_t')$, se os processos sÃ£o estacionÃ¡rios e ergÃ³dicos, e temos amostras grandes, entÃ£o $b$ Ã© um bom estimador de $\alpha$.

**Teorema 1** (ConvergÃªncia do estimador OLS sob estacionariedade e ergodicidade): Se o processo estocÃ¡stico $\{X_t, Y_{t+1}\}$ Ã© covariÃ¢ncia-estacionÃ¡rio e ergÃ³dico para momentos de segunda ordem, e se $E(X_tX_t')$ Ã© uma matriz nÃ£o-singular, entÃ£o o estimador OLS $b$ converge em probabilidade para o coeficiente de projeÃ§Ã£o linear $\alpha$.
*Prova:*
A prova deste teorema Ã© uma aplicaÃ§Ã£o direta da Lei dos Grandes NÃºmeros e do Teorema da FunÃ§Ã£o ContÃ­nua.

I. Pela definiÃ§Ã£o do estimador OLS:
$$ b = \left[ \frac{1}{T} \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \frac{1}{T} \sum_{t=1}^T x_t y_{t+1} \right] $$

II. Pela estacionariedade e ergodicidade, e pelo lema 1.1 do capÃ­tulo anterior, os momentos amostrais convergem em probabilidade para seus equivalentes populacionais:
    $$ \frac{1}{T} \sum_{t=1}^T x_t x_t' \overset{p}{\rightarrow} E(X_t X_t') $$
    $$ \frac{1}{T} \sum_{t=1}^T x_t y_{t+1} \overset{p}{\rightarrow} E(X_t Y_{t+1}) $$

III. Substituindo os limites na expressÃ£o de $b$, e usando o Lema 1.1:
$$ b \overset{p}{\rightarrow} [E(X_t X_t')]^{-1} E(X_t Y_{t+1}) $$
IV.  O lado direito da equaÃ§Ã£o acima Ã© a definiÃ§Ã£o do coeficiente de projeÃ§Ã£o linear $\alpha$:
$$ \alpha = [E(X_t X_t')]^{-1} E(X_t Y_{t+1}) $$
V. Portanto, concluÃ­mos que:
$$ b \overset{p}{\rightarrow} \alpha $$
Ou seja, o estimador OLS $b$ converge em probabilidade para o coeficiente de projeÃ§Ã£o linear $\alpha$ sob as condiÃ§Ãµes especificadas. â– 

**Lema 1** (ConsistÃªncia da regressÃ£o OLS): Em um processo estocÃ¡stico que Ã© covariÃ¢ncia-estacionÃ¡rio e ergÃ³dico para momentos de segunda ordem, a regressÃ£o OLS fornece um estimador consistente do coeficiente de projeÃ§Ã£o linear, ou seja, quando $T \to \infty$, o estimador OLS $b$ se aproxima do valor verdadeiro do coeficiente de projeÃ§Ã£o linear $\alpha$.
*Prova:*
A demonstraÃ§Ã£o desse lema estÃ¡ implÃ­cita na prova do Teorema 1. Uma vez que a convergÃªncia em probabilidade do estimador OLS para o coeficiente de projeÃ§Ã£o linear jÃ¡ foi estabelecida no Teorema 1, o Lema 1 Ã© uma consequÃªncia direta desse resultado.

Este lema destaca que, em condiÃ§Ãµes de estacionariedade e ergodicidade, a regressÃ£o OLS nÃ£o apenas converge para o valor "correto", mas faz isso usando apenas informaÃ§Ãµes da amostra. Isso Ã© fundamental na prÃ¡tica, pois raramente temos acesso aos momentos populacionais.
**ObservaÃ§Ã£o 1:** Uma condiÃ§Ã£o fundamental para a convergÃªncia do estimador OLS para o coeficiente da projeÃ§Ã£o linear Ã© que a matriz de covariÃ¢ncia populacional $E(X_t X_t')$ seja nÃ£o-singular. Isso garante que a inversa dessa matriz exista e que o coeficiente seja bem definido. Caso essa condiÃ§Ã£o seja violada (por exemplo, se existe multicolinearidade perfeita entre as variÃ¡veis em $X_t$), a regressÃ£o OLS nÃ£o funcionarÃ¡ de forma correta e os coeficientes serÃ£o indeterminados.

> ðŸ’¡ **Exemplo NumÃ©rico:** Considere um modelo em que queremos prever o preÃ§o de uma aÃ§Ã£o ($Y_{t+1}$) usando o preÃ§o da aÃ§Ã£o no dia anterior ($X_t$).  Assumimos que o processo conjunto Ã© estacionÃ¡rio e ergÃ³dico. O modelo Ã© dado por:
> $$Y_{t+1} = \alpha X_t + U_{t+1}$$
> onde $U_{t+1}$ Ã© um termo de erro.  Na realidade, o verdadeiro $\alpha$ (coeficiente de projeÃ§Ã£o linear) Ã© 0.8. Isso significa que, em mÃ©dia, o preÃ§o da aÃ§Ã£o no dia seguinte Ã© 80% do preÃ§o do dia anterior mais algum choque aleatÃ³rio. Usaremos dados simulados para ilustrar como a regressÃ£o OLS estima consistentemente esse valor.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
>
> # ParÃ¢metros
> alpha_verdadeiro = 0.8
> sigma_erro = 0.2
> T = 1000
>
> # SimulaÃ§Ã£o de dados
> np.random.seed(42)
> X = np.random.normal(100, 10, T)  # PreÃ§o da aÃ§Ã£o no dia t
> U = np.random.normal(0, sigma_erro, T) # Choques aleatÃ³rios
> Y = alpha_verdadeiro * X[:-1] + U[1:] # PreÃ§o da aÃ§Ã£o no dia t+1
>
> # Criando DataFrame
> data = pd.DataFrame({'X_t': X[:-1], 'Y_t1': Y})
>
> # RegressÃ£o OLS
> X_reg = sm.add_constant(data['X_t'])
> modelo = sm.OLS(data['Y_t1'], X_reg)
> resultados = modelo.fit()
>
> print(resultados.summary())
> print(f"Coeficiente estimado (b): {resultados.params['X_t']:.4f}")
>
> # Comparando com o valor verdadeiro
> print(f"Coeficiente verdadeiro (alpha): {alpha_verdadeiro}")
> ```
> Este cÃ³digo gera uma sÃ©rie de dados simulados, ajusta uma regressÃ£o OLS e mostra que o coeficiente estimado ('X_t') se aproxima do valor verdadeiro de $\alpha$ (0.8) quando o tamanho da amostra Ã© grande. Ã€ medida que $T$ aumenta, a estimativa de $b$ ($b$) se aproxima de $\alpha$.
>
> Em termos de convergÃªncia em probabilidade, quando $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ se aproxima de $E(X_t X_t')$ e $\frac{1}{T} \sum_{t=1}^T x_t y_{t+1}$ se aproxima de $E(X_t Y_{t+1})$, o estimador OLS $b$ converge para o valor do coeficiente de projeÃ§Ã£o linear $\alpha$, que nesse caso Ã© 0.8.

### ImplicaÃ§Ãµes PrÃ¡ticas

1.  **PrevisÃ£o:** A convergÃªncia do estimador OLS para o coeficiente de projeÃ§Ã£o linear justifica o uso da regressÃ£o OLS em contextos de previsÃ£o. Mesmo quando nÃ£o se tem um interesse em causalidade, ou seja, quando nÃ£o se busca o efeito de $X_t$ em $Y_{t+1}$, mas apenas em obter a melhor previsÃ£o de $Y_{t+1}$ a partir de $X_t$, a regressÃ£o OLS fornece uma aproximaÃ§Ã£o consistente para o coeficiente de projeÃ§Ã£o linear.

2.  **AnÃ¡lise de SÃ©ries Temporais:**  Para modelos de sÃ©ries temporais, onde muitas vezes se assume que o processo Ã© estacionÃ¡rio e ergÃ³dico, a regressÃ£o OLS torna-se uma ferramenta valiosa para estimar os coeficientes de modelos autorregressivos e de mÃ©dias mÃ³veis, que sÃ£o formas especÃ­ficas de projeÃ§Ã£o linear.

3.  **Robustez:** O requisito de ergodicidade apenas para momentos de segunda ordem Ã© relativamente fraco, permitindo que a regressÃ£o OLS seja utilizada em uma ampla gama de contextos prÃ¡ticos, mesmo quando os pressupostos mais fortes sobre a distribuiÃ§Ã£o dos dados sÃ£o violados. Isso torna a abordagem bastante robusta em termos de aplicaÃ§Ã£o empÃ­rica.
**ObservaÃ§Ã£o 3:** Apesar da robustez mencionada, Ã© importante notar que a validade dos resultados assintÃ³ticos da regressÃ£o OLS depende crucialmente da correta especificaÃ§Ã£o do modelo. Ou seja, a forma funcional da projeÃ§Ã£o linear deve estar alinhada com a relaÃ§Ã£o verdadeira entre as variÃ¡veis.
> ðŸ’¡ **Exemplo NumÃ©rico:**  Para ilustrar a aplicaÃ§Ã£o em sÃ©ries temporais, considere um modelo AR(1) para prever a inflaÃ§Ã£o:
> $$InflaÃ§Ã£o_{t+1} = \alpha InflaÃ§Ã£o_t + U_{t+1}$$
> Assumindo que a inflaÃ§Ã£o Ã© estacionÃ¡ria e ergÃ³dica, podemos usar OLS para estimar o coeficiente $\alpha$. Vamos gerar dados simulados de inflaÃ§Ã£o e aplicar a regressÃ£o OLS:
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
>
> # ParÃ¢metros
> alpha_verdadeiro = 0.6
> sigma_erro = 0.1
> T = 500
>
> # SimulaÃ§Ã£o de dados
> np.random.seed(42)
> inflacao = np.zeros(T)
> inflacao[0] = np.random.normal(0, sigma_erro/(1-alpha_verdadeiro**2)**0.5)
>
> U = np.random.normal(0, sigma_erro, T)
>
> for t in range(1, T):
>    inflacao[t] = alpha_verdadeiro * inflacao[t-1] + U[t]
>
> # CriaÃ§Ã£o do DataFrame
> data = pd.DataFrame({'Inflacao_t': inflacao[:-1],'Inflacao_t1': inflacao[1:] })
>
> # RegressÃ£o OLS
> X = sm.add_constant(data['Inflacao_t'])
> modelo = sm.OLS(data['Inflacao_t1'], X)
> resultados = modelo.fit()
>
> print(resultados.summary())
> print(f"Coeficiente Estimado (b): {resultados.params['Inflacao_t']:.4f}")
>
> # ComparaÃ§Ã£o com o valor verdadeiro
> print(f"Coeficiente verdadeiro (alpha): {alpha_verdadeiro}")
>
> # Plot da sÃ©rie
> plt.plot(data['Inflacao_t1'])
> plt.title("SÃ©rie Temporal da InflaÃ§Ã£o (Simulada)")
> plt.xlabel("Tempo")
> plt.ylabel("Inflacao_t")
> plt.show()
> ```
> O exemplo acima mostra que a regressÃ£o OLS pode ser aplicada a dados de sÃ©ries temporais para estimar os parÃ¢metros do modelo, desde que os dados sejam estacionÃ¡rios e ergÃ³dicos. A mÃ©dia estimada pelo OLS ($b$) converge para o valor de $\alpha$.

### A ImportÃ¢ncia da Ergodicidade para a ConsistÃªncia do Estimador OLS

A ergodicidade desempenha um papel crucial na garantia de que as mÃ©dias amostrais convergem para as mÃ©dias populacionais, o que, por sua vez, assegura a consistÃªncia do estimador OLS. Sem essa propriedade, a regressÃ£o OLS nÃ£o necessariamente convergiria para o coeficiente de projeÃ§Ã£o linear, tornando-a menos Ãºtil como um mÃ©todo para estimar as relaÃ§Ãµes teÃ³ricas subjacentes.

**Teorema 1.2** (DistribuiÃ§Ã£o AssintÃ³tica do Estimador OLS):
Sob condiÃ§Ãµes adicionais de regularidade, o estimador OLS $b$ tem distribuiÃ§Ã£o assintÃ³tica normal:
$$ \sqrt{T}(b - \alpha) \overset{d}{\rightarrow} N(0, V) $$
Onde $V = [E(X_tX_t')]^{-1} E(X_t X_t'U_{t+1}^2) [E(X_tX_t')]^{-1}$ e $U_{t+1} = Y_{t+1}-\alpha'X_t$ Ã© o erro de projeÃ§Ã£o.

*Proof:*
A prova desse teorema envolve a aplicaÃ§Ã£o do Teorema do Limite Central para variÃ¡veis dependentes.

I. ComeÃ§amos com a expressÃ£o do estimador OLS:
    $$ b = \left[ \frac{1}{T} \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \frac{1}{T} \sum_{t=1}^T x_t y_{t+1} \right] $$

II. SubstituÃ­mos $y_{t+1}$ pela sua projeÃ§Ã£o linear mais o erro:
 $$ b = \left[ \frac{1}{T} \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \frac{1}{T} \sum_{t=1}^T x_t (\alpha'x_t + U_{t+1}) \right] $$

III. Reorganizando e subtraindo $\alpha$ de ambos os lados:
    $$ b - \alpha = \left[ \frac{1}{T} \sum_{t=1}^T x_t x_t' \right]^{-1}  \left[ \frac{1}{T} \sum_{t=1}^T x_t U_{t+1} \right] $$
Multiplicando por $\sqrt{T}$:
    $$ \sqrt{T}(b - \alpha) = \left[ \frac{1}{T} \sum_{t=1}^T x_t x_t' \right]^{-1}  \left[ \frac{1}{\sqrt{T}} \sum_{t=1}^T x_t U_{t+1} \right] $$
IV. Pelas condiÃ§Ãµes de regularidade e ergodicidade, temos:
    $$ \left[ \frac{1}{T} \sum_{t=1}^T x_t x_t' \right]^{-1} \overset{p}{\rightarrow} [E(X_tX_t')]^{-1} $$
V. A partir do Teorema do Limite Central (TLC) e usando que as variÃ¡veis $x_tU_{t+1}$ sÃ£o estacionÃ¡rias e ergodicidade, temos:
    $$ \frac{1}{\sqrt{T}} \sum_{t=1}^T x_t U_{t+1} \overset{d}{\rightarrow} N(0, E(X_t X_t'U_{t+1}^2)) $$

VI. Usando o mÃ©todo delta, combinamos os resultados:
$$ \sqrt{T}(b - \alpha) \overset{d}{\rightarrow} N(0, [E(X_tX_t')]^{-1} E(X_t X_t'U_{t+1}^2) [E(X_tX_t')]^{-1}) $$
VII. Portanto,
$$ \sqrt{T}(b - \alpha) \overset{d}{\rightarrow} N(0, V) $$
Onde $V = [E(X_tX_t')]^{-1} E(X_t X_t'U_{t+1}^2) [E(X_tX_t')]^{-1}$ e $U_{t+1} = Y_{t+1}-\alpha'X_t$ Ã© o erro de projeÃ§Ã£o. â– 

Este teorema estabelece que, sob certas condiÃ§Ãµes, o estimador OLS tem uma distribuiÃ§Ã£o assintÃ³tica normal, permitindo que realizemos testes de hipÃ³teses sobre os coeficientes e construamos intervalos de confianÃ§a.
**Lema 1.3** (Estimador Consistente da VariÃ¢ncia AssintÃ³tica): Seja $V$ a variÃ¢ncia assintÃ³tica do estimador OLS dada no Teorema 1.2, entÃ£o um estimador consistente de $V$ Ã© dado por:
$$ \hat{V} = \left[ \frac{1}{T} \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \frac{1}{T} \sum_{t=1}^T x_t x_t' \hat{U}_{t+1}^2 \right] \left[ \frac{1}{T} \sum_{t=1}^T x_t x_t' \right]^{-1} $$
onde $\hat{U}_{t+1} = Y_{t+1} - b'X_t$ sÃ£o os resÃ­duos OLS.
*Prova:*
A prova desse lema segue diretamente da aplicaÃ§Ã£o da Lei dos Grandes NÃºmeros e do Teorema da FunÃ§Ã£o ContÃ­nua. A convergÃªncia em probabilidade dos momentos amostrais para os momentos populacionais e a convergÃªncia em probabilidade do estimador OLS $b$ para o coeficiente de projeÃ§Ã£o linear $\alpha$ implicam que $\hat{V}$ converge para $V$.

Este lema Ã© importante na prÃ¡tica, pois permite que se construam intervalos de confianÃ§a e se realizem testes de hipÃ³teses sobre os coeficientes da regressÃ£o OLS, utilizando apenas os dados da amostra e os resÃ­duos obtidos da regressÃ£o OLS.
> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos usar os dados simulados de inflaÃ§Ã£o do exemplo anterior para demonstrar o uso da variÃ¢ncia assintÃ³tica para a construÃ§Ã£o de um intervalo de confianÃ§a.
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
>
> # ParÃ¢metros
> alpha_verdadeiro = 0.6
> sigma_erro = 0.1
> T = 500
>
> # SimulaÃ§Ã£o de dados
> np.random.seed(42)
> inflacao = np.zeros(T)
> inflacao[0] = np.random.normal(0, sigma_erro/(1-alpha_verdadeiro**2)**0.5)
>
> U = np.random.normal(0, sigma_erro, T)
>
> for t in range(1, T):
>    inflacao[t] = alpha_verdadeiro * inflacao[t-1] + U[t]
>
> # CriaÃ§Ã£o do DataFrame
> data = pd.DataFrame({'Inflacao_t': inflacao[:-1],'Inflacao_t1': inflacao[1:] })
>
> # RegressÃ£o OLS
> X = sm.add_constant(data['Inflacao_t'])
> modelo = sm.OLS(data['Inflacao_t1'], X)
> resultados = modelo.fit()
>
> # CÃ¡lculo do intervalo de confianÃ§a
> intervalo_confianca = resultados.conf_int(alpha=0.05)
>
> print(f"Coeficiente Estimado (b): {resultados.params['Inflacao_t']:.4f}")
> print(f"Intervalo de ConfianÃ§a (95%): {intervalo_confianca.loc['Inflacao_t'].values}")
> print(f"Coeficiente verdadeiro (alpha): {alpha_verdadeiro}")
> ```
> O cÃ³digo acima calcula o intervalo de confianÃ§a de 95% para o coeficiente estimado. O intervalo Ã© construÃ­do usando a distribuiÃ§Ã£o assintÃ³tica normal do estimador OLS, e usando a estimativa consistente da variÃ¢ncia assintÃ³tica dada pelo Lema 1.3. Se o processo Ã© estacionÃ¡rio e ergÃ³dico, e o tamanho da amostra Ã© suficientemente grande, este intervalo de confianÃ§a tem uma probabilidade de 95% de conter o verdadeiro valor do coeficiente de projeÃ§Ã£o linear.

### ConclusÃ£o

Este capÃ­tulo reforÃ§ou a importÃ¢ncia da estacionariedade e da ergodicidade na anÃ¡lise de sÃ©ries temporais e, em particular, na relaÃ§Ã£o entre projeÃ§Ã£o linear e regressÃ£o OLS. Sob essas condiÃ§Ãµes, a regressÃ£o OLS nÃ£o Ã© apenas um mÃ©todo de ajuste amostral, mas tambÃ©m um estimador consistente do coeficiente de projeÃ§Ã£o linear, um parÃ¢metro populacional fundamental. Esta convergÃªncia, aliada Ã  distribuiÃ§Ã£o assintÃ³tica normal do estimador OLS, estabelece uma base sÃ³lida para a sua utilizaÃ§Ã£o em uma ampla variedade de aplicaÃ§Ãµes prÃ¡ticas, especialmente em contextos onde o objetivo Ã© a previsÃ£o e modelagem de sÃ©ries temporais.

### ReferÃªncias

[^4.1.9]: A projeÃ§Ã£o linear Ã© definida como a melhor aproximaÃ§Ã£o linear de uma variÃ¡vel aleatÃ³ria em termos de outra.
[^4.1.13]: A fÃ³rmula para o coeficiente de projeÃ§Ã£o linear $\alpha$ Ã© dada por  $\alpha' = E(Y_{t+1} X_t') [E(X_t X_t')]^{-1}$.
[^4.1.18]: O estimador OLS Ã© dado por $b = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_{t+1}$.
[^4.1.19]: A formula para b pode ser escrita como $b = \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t y_{t+1} \right]$.
[^4.1.20]: Em condiÃ§Ãµes de estacionariedade e ergodicidade, os momentos amostrais convergem para os momentos populacionais e o estimador OLS converge para o coeficiente de projeÃ§Ã£o linear.
<!-- END -->
