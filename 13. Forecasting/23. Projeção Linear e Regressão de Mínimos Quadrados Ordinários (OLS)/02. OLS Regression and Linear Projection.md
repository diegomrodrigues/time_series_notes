## Proje√ß√£o Linear e Regress√£o de M√≠nimos Quadrados Ordin√°rios (OLS): Uma An√°lise Comparativa

### Introdu√ß√£o
Este cap√≠tulo aprofunda a rela√ß√£o entre **proje√ß√£o linear** e **regress√£o de m√≠nimos quadrados ordin√°rios (OLS)**, construindo sobre o entendimento pr√©vio de que ambos os m√©todos buscam uma aproxima√ß√£o linear √≥tima de uma vari√°vel dependente [^4.1.9]. Anteriormente, foi estabelecido que a proje√ß√£o linear opera no dom√≠nio dos momentos populacionais, enquanto a regress√£o OLS se baseia nos momentos amostrais [^4.1.13, 4.1.18, 4.1.19]. Aqui, exploramos as implica√ß√µes desta distin√ß√£o, examinando como cada m√©todo captura informa√ß√µes distintas sobre os dados e como estas informa√ß√µes s√£o utilizadas. Adicionalmente, formalizamos alguns conceitos importantes como o erro de proje√ß√£o e a decomposi√ß√£o da vari√¢ncia.

### Proje√ß√£o Linear: Uma Vis√£o Populacional
A **proje√ß√£o linear** √© um conceito fundamental na teoria de s√©ries temporais, buscando a melhor aproxima√ß√£o linear de uma vari√°vel aleat√≥ria $Y_{t+1}$ em termos de outra vari√°vel aleat√≥ria ou um vetor de vari√°veis $X_t$ [^4.1.9]. O objetivo √© encontrar um vetor $\alpha$ que minimiza o erro quadr√°tico m√©dio da proje√ß√£o, resultando na seguinte express√£o para a proje√ß√£o de $Y_{t+1}$ em $X_t$:

$$\hat{Y}_{t+1} = \alpha'X_t$$

O vetor de coeficientes $\alpha$ √© dado por:

$$\alpha' = E(Y_{t+1} X_t') [E(X_t X_t')]^{-1}$$ [^4.1.13]

A caracter√≠stica crucial desta formula√ß√£o √© o uso de **momentos populacionais** [^4.1.13]. $E(Y_{t+1} X_t')$ representa a covari√¢ncia entre $Y_{t+1}$ e $X_t$, enquanto $E(X_t X_t')$ representa a matriz de covari√¢ncia de $X_t$. Estes momentos s√£o caracter√≠sticas te√≥ricas do processo estoc√°stico subjacente, e sua obten√ß√£o requer conhecimento completo da distribui√ß√£o conjunta de $Y_{t+1}$ e $X_t$. Na pr√°tica, esses momentos s√£o muitas vezes desconhecidos e precisam ser estimados a partir de dados observados.
**Observa√ß√£o 1:** √â importante notar que a proje√ß√£o linear √© un√≠voca, ou seja, para um dado vetor de vari√°veis $X_t$, existe um √∫nico vetor $\alpha$ que minimiza o erro quadr√°tico m√©dio. Esta unicidade decorre da propriedade de que a matriz de covari√¢ncia $E(X_t X_t')$ √© n√£o-singular.

> üí° **Exemplo Num√©rico:** Suponha que temos um modelo populacional onde $Y_{t+1} = 0.5X_{t,1} + 1.2X_{t,2} + U_{t+1}$, e sabemos que:
>
> $E(X_t X_t') = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$
>
> $E(Y_{t+1} X_t') = \begin{bmatrix} E(Y_{t+1} X_{t,1}) & E(Y_{t+1} X_{t,2}) \end{bmatrix} = \begin{bmatrix} 0.5*1 + 1.2*0.5  & 0.5*0.5 + 1.2*1 \end{bmatrix} = \begin{bmatrix} 1.1 & 1.45 \end{bmatrix}$
>
> Para encontrar $\alpha$, primeiro precisamos calcular $(E(X_t X_t'))^{-1}$. Para uma matriz 2x2, a inversa √© dada por:
>
>  $A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$, $A^{-1} = \frac{1}{ad-bc}\begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$
>
>  $(E(X_t X_t'))^{-1} = \frac{1}{1*1 - 0.5*0.5} \begin{bmatrix} 1 & -0.5 \\ -0.5 & 1 \end{bmatrix} = \frac{1}{0.75} \begin{bmatrix} 1 & -0.5 \\ -0.5 & 1 \end{bmatrix} =  \begin{bmatrix} 1.333 & -0.666 \\ -0.666 & 1.333 \end{bmatrix}$
>
> Agora, calculamos $\alpha'$:
>
> $\alpha' =  \begin{bmatrix} 1.1 & 1.45 \end{bmatrix} \begin{bmatrix} 1.333 & -0.666 \\ -0.666 & 1.333 \end{bmatrix} =  \begin{bmatrix} 1.1*1.333 + 1.45*(-0.666) & 1.1*(-0.666) + 1.45*1.333  \end{bmatrix} = \begin{bmatrix} 0.5 & 1.2 \end{bmatrix}$
>
> Assim, $\alpha = \begin{bmatrix} 0.5 \\ 1.2 \end{bmatrix}$, que corresponde aos coeficientes verdadeiros do nosso modelo. Isso demonstra como a proje√ß√£o linear recupera os coeficientes populacionais quando conhecemos os momentos populacionais.

### Regress√£o OLS: Uma Vis√£o Amostral
A **regress√£o de m√≠nimos quadrados ordin√°rios (OLS)**, por sua vez, opera no dom√≠nio dos **momentos amostrais**. Dado um conjunto de $T$ observa√ß√µes, a regress√£o OLS estima o vetor $\beta$ que minimiza a soma dos quadrados dos erros:

$$ \sum_{t=1}^T (y_{t+1} - \beta'x_t)^2 $$ [^4.1.17]

O estimador OLS, denotado por $b$, √© calculado como:

$$ b = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_{t+1} $$ [^4.1.18]

ou equivalentemente,

$$ b = \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t y_{t+1} \right] $$ [^4.1.19]

Aqui, $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ √© a matriz de covari√¢ncia amostral de $X_t$ e $\frac{1}{T} \sum_{t=1}^T x_t y_{t+1}$ √© o vetor de covari√¢ncia amostral entre $X_t$ e $Y_{t+1}$.  A regress√£o OLS, portanto, resume as informa√ß√µes contidas nos dados da amostra para estimar a rela√ß√£o linear entre $Y_{t+1}$ e $X_t$.
**Lema 1.2** (Unicidade do estimador OLS): Se a matriz $\sum_{t=1}^T x_t x_t'$ √© n√£o-singular (o que acontece se n√£o houver multicolinearidade perfeita nas vari√°veis explicativas), o estimador OLS $b$ √© √∫nico.
*Prova:*
Vamos provar a unicidade do estimador OLS $b$.

I.  O problema de minimiza√ß√£o da soma dos quadrados dos erros √© dado por:
 $$ \min_{\beta} \sum_{t=1}^T (y_{t+1} - \beta'x_t)^2 $$
II.  Para encontrar o m√≠nimo, derivamos a fun√ß√£o objetivo em rela√ß√£o a $\beta$ e igualamos a zero. A fun√ß√£o objetivo pode ser escrita em nota√ß√£o matricial como:
$$ (Y - X\beta)'(Y - X\beta) $$
Onde $Y$ √© um vetor coluna de tamanho $T$ com os valores de $y_{t+1}$, e $X$ √© uma matriz $T \times k$ com cada linha sendo $x_t'$.
III. Expandindo a express√£o, obtemos:
$$ Y'Y - 2\beta'X'Y + \beta'X'X\beta $$
IV. Derivando em rela√ß√£o a $\beta$, obtemos a condi√ß√£o de primeira ordem:
$$ -2X'Y + 2X'X\beta = 0 $$
V.  Reorganizando, temos:
$$ X'X\beta = X'Y $$
VI. Se $X'X$ for n√£o-singular (o que equivale a dizer que a matriz $\sum_{t=1}^T x_t x_t'$ √© n√£o-singular), podemos pr√©-multiplicar ambos os lados por $(X'X)^{-1}$:
$$ \beta = (X'X)^{-1}X'Y $$
VII.  Isso √© equivalente a:
 $$ b = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_{t+1} $$
VIII. Como a inversa de uma matriz n√£o-singular √© √∫nica, o valor de $\beta$ que minimiza a soma dos quadrados dos erros √© √∫nico.
 Portanto, o estimador OLS $b$ √© √∫nico se a matriz $\sum_{t=1}^T x_t x_t'$ √© n√£o-singular. ‚ñ†

> üí° **Exemplo Num√©rico:** Vamos usar um conjunto de dados simulados para ilustrar o c√°lculo do estimador OLS. Suponha que temos $T=5$ observa√ß√µes, com um vetor de vari√°vel explicativa $X_t$ e uma vari√°vel dependente $Y_{t+1}$:
>
> $X = \begin{bmatrix} 1 & 2 \\ 2 & 1 \\ 3 & 3 \\ 4 & 2 \\ 5 & 4 \end{bmatrix}$
>
> $Y = \begin{bmatrix} 4 \\ 5 \\ 10 \\ 11 \\ 14 \end{bmatrix}$
>
>  **Passo 1: Calcular $X'X$**
>
>  $X' = \begin{bmatrix} 1 & 2 & 3 & 4 & 5 \\ 2 & 1 & 3 & 2 & 4 \end{bmatrix}$
>
> $X'X = \begin{bmatrix} 1 & 2 & 3 & 4 & 5 \\ 2 & 1 & 3 & 2 & 4 \end{bmatrix} \begin{bmatrix} 1 & 2 \\ 2 & 1 \\ 3 & 3 \\ 4 & 2 \\ 5 & 4 \end{bmatrix} = \begin{bmatrix} 55 & 45 \\ 45 & 34 \end{bmatrix}$
>
>  **Passo 2: Calcular $(X'X)^{-1}$**
>
> $(X'X)^{-1} = \frac{1}{55*34-45*45} \begin{bmatrix} 34 & -45 \\ -45 & 55 \end{bmatrix} = \frac{1}{170} \begin{bmatrix} 34 & -45 \\ -45 & 55 \end{bmatrix}  \approx \begin{bmatrix} 0.2 & -0.26 \\ -0.26 & 0.32 \end{bmatrix}$
>
> **Passo 3: Calcular $X'Y$**
>
> $X'Y = \begin{bmatrix} 1 & 2 & 3 & 4 & 5 \\ 2 & 1 & 3 & 2 & 4 \end{bmatrix} \begin{bmatrix} 4 \\ 5 \\ 10 \\ 11 \\ 14 \end{bmatrix} = \begin{bmatrix} 145 \\ 119 \end{bmatrix}$
>
>  **Passo 4: Calcular o estimador OLS $b$**
>
> $b = (X'X)^{-1}X'Y = \begin{bmatrix} 0.2 & -0.26 \\ -0.26 & 0.32 \end{bmatrix} \begin{bmatrix} 145 \\ 119 \end{bmatrix} = \begin{bmatrix} 2.31 \\ 1.41 \end{bmatrix}$
>
> Assim, o estimador OLS $b$ √© aproximadamente $\begin{bmatrix} 2.31 \\ 1.41 \end{bmatrix}$.  Isso nos d√° uma estimativa da rela√ß√£o linear entre $Y$ e $X$ com base nos dados amostrais.
>
> ```python
> import numpy as np
> from numpy.linalg import inv
> X = np.array([[1, 2], [2, 1], [3, 3], [4, 2], [5, 4]])
> Y = np.array([4, 5, 10, 11, 14])
> X_transpose = X.transpose()
> XtX = np.dot(X_transpose, X)
> inv_XtX = inv(XtX)
> XtY = np.dot(X_transpose, Y)
> b = np.dot(inv_XtX, XtY)
> print(b) # Output: [2.30588235 1.41176471]
> ```
> A implementa√ß√£o em Python fornece resultados similares.

> üí° **An√°lise Comparativa:** A principal distin√ß√£o entre a proje√ß√£o linear e a regress√£o OLS reside em como elas utilizam as informa√ß√µes sobre o processo estoc√°stico. A proje√ß√£o linear, atrav√©s do uso de momentos populacionais, captura as rela√ß√µes te√≥ricas subjacentes, enquanto a regress√£o OLS, atrav√©s de momentos amostrais, captura as rela√ß√µes emp√≠ricas observadas na amostra.  Enquanto a proje√ß√£o linear visa modelar a estrutura de longo prazo do processo, a regress√£o OLS foca em descrever a rela√ß√£o entre as vari√°veis dentro de uma amostra espec√≠fica.

### Rela√ß√£o entre Proje√ß√£o Linear e Regress√£o OLS

Apesar de suas diferen√ßas, existe uma rela√ß√£o profunda entre a proje√ß√£o linear e a regress√£o OLS. Sob condi√ß√µes de **covari√¢ncia-estacionariedade e ergodicidade** para momentos de segunda ordem, a Lei dos Grandes N√∫meros garante que os momentos amostrais convergem em probabilidade para seus equivalentes populacionais [^4.1.20]:

$$\left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t' \overset{p}{\rightarrow} E(X_t X_t')$$
$$\left( \frac{1}{T} \right) \sum_{t=1}^T x_t y_{t+1} \overset{p}{\rightarrow} E(X_t Y_{t+1})$$

Este resultado implica que, √† medida que o tamanho da amostra $T$ aumenta, o estimador OLS $b$ se aproxima do coeficiente de proje√ß√£o linear $\alpha$ [^4.1.20]. Esta converg√™ncia √© formalizada no Lema 1:

**Lema 1** (Consist√™ncia do estimador OLS): Dada a premissa de que o processo estoc√°stico $\{X_t, Y_{t+1}\}$ √© covari√¢ncia-estacion√°rio e erg√≥dico para momentos de segunda ordem, e que $E(X_tX_t')$ √© uma matriz n√£o-singular, o estimador OLS $b$ converge em probabilidade para o coeficiente de proje√ß√£o linear $\alpha$.

Esta converg√™ncia justifica o uso da regress√£o OLS como uma forma de estimar a rela√ß√£o linear subjacente entre as vari√°veis, mesmo quando n√£o se tem acesso aos momentos populacionais verdadeiros.  A converg√™ncia tamb√©m destaca um ponto crucial: a OLS √© uma aproxima√ß√£o amostral da proje√ß√£o linear populacional, e, portanto, sua precis√£o depende do tamanho da amostra.
**Teorema 1.1** (Distribui√ß√£o Assint√≥tica do Estimador OLS): Al√©m da converg√™ncia, sob condi√ß√µes adicionais de regularidade (como a exist√™ncia de momentos de quarta ordem e a independ√™ncia entre as vari√°veis), o estimador OLS $b$ tem distribui√ß√£o assint√≥tica normal:
$$\sqrt{T}(b - \alpha) \overset{d}{\rightarrow} N(0, V)$$
Onde $V = [E(X_tX_t')]^{-1} E(X_t X_t'U_{t+1}^2) [E(X_tX_t')]^{-1}$ e $U_{t+1} = Y_{t+1}-\alpha'X_t$ √© o erro de proje√ß√£o.
*Prova:*
Vamos demonstrar a distribui√ß√£o assint√≥tica do estimador OLS.

I.  Come√ßamos com o estimador OLS:
$$b = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_{t+1}$$
II.  Substitu√≠mos $y_{t+1}$ por sua proje√ß√£o linear mais o erro, $y_{t+1} = \alpha'x_t + U_{t+1}$:
$$b = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t (\alpha'x_t + U_{t+1})$$
III.  Distribu√≠mos a soma:
$$b = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \left(\sum_{t=1}^T x_t x_t'\alpha + \sum_{t=1}^T x_t U_{t+1}\right)$$
IV.  Reorganizando:
$$b = \alpha + \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t U_{t+1}$$
V.  Subtraindo $\alpha$ de ambos os lados e multiplicando por $\sqrt{T}$:
$$\sqrt{T}(b - \alpha) = \left( \frac{1}{T} \sum_{t=1}^T x_t x_t' \right)^{-1} \frac{1}{\sqrt{T}} \sum_{t=1}^T x_t U_{t+1}$$
VI. Pela Lei dos Grandes N√∫meros, $\frac{1}{T} \sum_{t=1}^T x_t x_t' \overset{p}{\rightarrow} E(X_t X_t')$, ent√£o:
$$\left( \frac{1}{T} \sum_{t=1}^T x_t x_t' \right)^{-1} \overset{p}{\rightarrow} [E(X_t X_t')]^{-1}$$
VII. Sob condi√ß√µes de regularidade, e usando o teorema do limite central (TLC) para a soma $\frac{1}{\sqrt{T}} \sum_{t=1}^T x_t U_{t+1}$, temos:
$$\frac{1}{\sqrt{T}} \sum_{t=1}^T x_t U_{t+1} \overset{d}{\rightarrow} N(0, E(X_t X_t' U_{t+1}^2))$$
VIII. Usando o m√©todo delta, e combinando os resultados:
$$\sqrt{T}(b - \alpha) \overset{d}{\rightarrow} N(0, [E(X_tX_t')]^{-1} E(X_t X_t'U_{t+1}^2) [E(X_tX_t')]^{-1})$$
IX.  Portanto,
$$\sqrt{T}(b - \alpha) \overset{d}{\rightarrow} N(0, V)$$
Onde $V = [E(X_tX_t')]^{-1} E(X_t X_t'U_{t+1}^2) [E(X_tX_t')]^{-1}$ e $U_{t+1} = Y_{t+1}-\alpha'X_t$ √© o erro de proje√ß√£o. ‚ñ†
> üí° **Exemplo Num√©rico:**  Continuando com o exemplo anterior onde $b = \begin{bmatrix} 2.31 \\ 1.41 \end{bmatrix}$ e assumindo que o verdadeiro valor de $\alpha = \begin{bmatrix} 2 \\ 1.5 \end{bmatrix}$, podemos simular diferentes tamanhos de amostra e observar a converg√™ncia do estimador OLS. Vamos considerar $T = 10, 100, 1000$.
>
> ```python
> import numpy as np
> from numpy.linalg import inv
> np.random.seed(42) # para reprodutibilidade
>
> def generate_data(T):
>     X = np.random.rand(T, 2) * 10  # Simula X
>     alpha = np.array([2, 1.5]) # define alpha
>     U = np.random.normal(0, 1, T) # Simula erro
>     Y = np.dot(X, alpha) + U # simula Y
>     return X, Y
>
> def ols_estimation(X, Y):
>     X_transpose = X.transpose()
>     XtX = np.dot(X_transpose, X)
>     inv_XtX = inv(XtX)
>     XtY = np.dot(X_transpose, Y)
>     b = np.dot(inv_XtX, XtY)
>     return b
>
> true_alpha = np.array([2, 1.5])
> sample_sizes = [10, 100, 1000]
>
> for T in sample_sizes:
>     X, Y = generate_data(T)
>     b = ols_estimation(X, Y)
>     print(f"Tamanho da Amostra: {T}")
>     print(f"Estimativa OLS (b): {b}")
>     print(f"Erro (b - alpha): {b - true_alpha}")
>
> ```
> Ao executar este c√≥digo, notaremos que, √† medida que o tamanho da amostra aumenta, o estimador OLS $b$ se aproxima cada vez mais do valor verdadeiro de $\alpha$.
>
> ```
> Tamanho da Amostra: 10
> Estimativa OLS (b): [1.98208239 1.63997101]
> Erro (b - alpha): [-0.01791761  0.13997101]
> Tamanho da Amostra: 100
> Estimativa OLS (b): [1.98492973 1.4967886 ]
> Erro (b - alpha): [-0.01507027 -0.0032114 ]
> Tamanho da Amostra: 1000
> Estimativa OLS (b): [2.00092764 1.50122577]
> Erro (b - alpha): [0.00092764 0.00122577]
> ```
> Isso ilustra a consist√™ncia do estimador OLS.

### Decomposi√ß√£o da Vari√¢ncia e o Erro de Proje√ß√£o
Outro conceito fundamental para entender a liga√ß√£o entre proje√ß√£o linear e OLS √© a **decomposi√ß√£o da vari√¢ncia**. Como estabelecido no Teorema 1, a vari√¢ncia da vari√°vel dependente $Y_{t+1}$ pode ser decomposta em duas partes: a vari√¢ncia explicada pela proje√ß√£o linear ($\hat{Y}_{t+1}$) e a vari√¢ncia do erro de proje√ß√£o ($U_{t+1}$):

$$Var(Y_{t+1}) = Var(\hat{Y}_{t+1}) + Var(U_{t+1})$$

Esta decomposi√ß√£o √© importante porque ela mostra que a proje√ß√£o linear, embora seja a melhor aproxima√ß√£o linear poss√≠vel, n√£o √© perfeita e ainda deixa alguma variabilidade n√£o explicada. Al√©m disso, $\hat{Y}_{t+1}$ e $U_{t+1}$ s√£o n√£o correlacionados, demonstrando que a proje√ß√£o linear captura a parte da variabilidade que pode ser explicada linearmente, deixando o restante no termo de erro.
**Lema 2** (N√£o-Correla√ß√£o entre Proje√ß√£o e Erro): A covari√¢ncia entre a proje√ß√£o linear $\hat{Y}_{t+1}$ e o erro de proje√ß√£o $U_{t+1}$ √© nula, isto √©, $Cov(\hat{Y}_{t+1}, U_{t+1}) = 0$.
*Prova:*
Vamos provar que a covari√¢ncia entre a proje√ß√£o e o erro de proje√ß√£o √© zero.
I.  Come√ßamos com as defini√ß√µes:
$$\hat{Y}_{t+1} = \alpha'X_t$$
$$U_{t+1} = Y_{t+1} - \hat{Y}_{t+1} = Y_{t+1} - \alpha'X_t$$
II. Queremos mostrar que $Cov(\hat{Y}_{t+1}, U_{t+1}) = 0$:
$$Cov(\hat{Y}_{t+1}, U_{t+1}) = E[(\hat{Y}_{t+1} - E(\hat{Y}_{t+1}))(U_{t+1} - E(U_{t+1}))]$$
III. Como $E(U_{t+1})=0$ (demonstrado no Lema 1.1 abaixo), ent√£o:
$$Cov(\hat{Y}_{t+1}, U_{t+1}) = E[(\hat{Y}_{t+1} - E(\hat{Y}_{t+1}))U_{t+1}]$$
IV.  Substituindo $\hat{Y}_{t+1}$:
$$Cov(\hat{Y}_{t+1}, U_{t+1}) = E[(\alpha'X_t - E(\alpha'X_t))U_{t+1}]$$
V.  Como $\alpha$ √© um vetor constante, podemos mover para fora da esperan√ßa:
$$Cov(\hat{Y}_{t+1}, U_{t+1}) = E[\alpha'(X_t - E(X_t))U_{t+1}]$$
VI. Como $\alpha$ √© um vetor constante, podemos mover para fora da esperan√ßa:
$$Cov(\hat{Y}_{t+1}, U_{t+1}) = \alpha'E[(X_t - E(X_t))U_{t+1}]$$
VII. Substitu√≠mos $U_{t+1}$ pela sua defini√ß√£o:
$$Cov(\hat{Y}_{t+1}, U_{t+1}) = \alpha'E[(X_t - E(X_t))(Y_{t+1} - \alpha'X_t)]$$
VIII. Expandindo:
$$Cov(\hat{Y}_{t+1}, U_{t+1}) = \alpha'[E((X_t - E(X_t))Y_{t+1}) - E((X_t - E(X_t))\alpha'X_t)]$$
IX. Usando a defini√ß√£o de covari√¢ncia:
$$Cov(\hat{Y}_{t+1}, U_{t+1}) = \alpha'[Cov(X_t, Y_{t+1}) - \alpha'Cov(X_t, X_t)]$$
X. Usando a defini√ß√£o de $\alpha$:
$$\alpha' = E(Y_{t+1} X_t') [E(X_t X_t')]^{-1}$$
XI.  Portanto,  $Cov(X_t, Y_{t+1}) = E(Y_{t+1}X_t') - E(Y_{t+1})E(X_t')$, e $Cov(X_t, X_t) = E(X_tX_t') - E(X_t)E(X_t')$, temos:
$$Cov(\hat{Y}_{t+1}, U_{t+1}) = \alpha'[E(Y_{t+1}X_t') - E(Y_{t+1})E(X_t') - \alpha'(E(X_tX_t') - E(X_t)E(X_t'))]$$
XII. Substituindo $\alpha'$:
$$Cov(\hat{Y}_{t+1}, U_{t+1}) = \alpha'[E(Y_{t+1}X_t') - E(Y_{t+1})E(X_t') - (E(Y_{t+1} X_t') [E(X_t X_t')]^{-1})(E(X_tX_t') - E(X_t)E(X_t'))]$$
XIII. Simplificando, observamos que $E(Y_{t+1}X_t') = \alpha'E(X_tX_t')$. Portanto:
$$Cov(\hat{Y}_{t+1}, U_{t+1}) = \alpha' E(Y_{t+1} X_t') -  \alpha' E(X_t X_t') \alpha' =  \alpha' E(X_t X_t') \alpha' -  \alpha' E(X_t X_t') \alpha' = 0$$
$$Cov(\hat{Y}_{t+1}, U_{t+1}) = 0$$
Portanto, a covari√¢ncia entre a proje√ß√£o linear e o erro de proje√ß√£o √© nula. ‚ñ†

Al√©m disso, o **erro de proje√ß√£o** tem m√©dia zero, como demonstrado no Lema 1.1:
**Lema 1.1** (Erro da Proje√ß√£o): O erro da proje√ß√£o $U_{t+1}$ tem m√©dia zero, ou seja, $E[U_{t+1}] = 0$.
*Prova:*
Vamos provar que o erro de proje√ß√£o $U_{t+1}$ tem m√©dia zero.

I.  Come√ßamos com a defini√ß√£o do erro de proje√ß√£o:
$$U_{t+1} = Y_{t+1} - \hat{Y}_{t+1}$$
II.  Substitu√≠mos $\hat{Y}_{t+1}$ pela sua defini√ß√£o:
$$U_{t+1} = Y_{t+1} - \alpha'X_t$$
III. Tomamos a esperan√ßa de ambos os lados:
$$E[U_{t+1}] = E[Y_{t+1} - \alpha'X_t]$$
IV. Pela linearidade da esperan√ßa:
$$E[U_{t+1}] = E[Y_{t+1}] - E[\alpha'X_t]$$
V.  Como $\alpha$ √© um vetor constante, podemos mover para fora da esperan√ßa:
$$E[U_{t+1}] = E[Y_{t+1}] - \alpha'E[X_t]$$
VI.  Lembrando que $\alpha' = E(Y_{t+1}X_t') [E(X_t X_t')]^{-1}$, e multiplicando pelo lado direito, temos:
$$ E(Y_{t+1} X_t') = \alpha' E(X_t X_t')$$
VII. A melhor previs√£o de $Y_{t+1}$ com base em $X_t$ √© tal que $E[U_{t+1}X_t] = 0$:
    $$E[U_{t+1}X_t] = E[(Y_{t+1} - \alpha'X_t)X_t] = E[Y_{t+1}X_t] - \alpha'E[X_t X_t] = 0$$
    $$E[Y_{t+1}X_t] = \alpha'E[X_t X_t]$$
VIII. Se pre-multiplicarmos pela inversa de $E(X_t X_t')$ obtemos que
    $$\alpha' = E[Y_{t+1} X_t'] E[X_t X_t']^{-1}$$
IX.  Portanto, o erro de proje√ß√£o tem esperan√ßa zero:
$$E[U_{t+1}] = 0$$‚ñ†

O $R^2$, discutido na Proposi√ß√£o 1.1, quantifica a propor√ß√£o da vari√¢ncia de $Y_{t+1}$ explicada pela proje√ß√£o linear. Ele fornece uma medida da qualidade do ajuste da regress√£o linear.

**Proposi√ß√£o 1.1** (R-Squared): O $R^2$ da regress√£o OLS, que mede a propor√ß√£o da vari√¢ncia de $Y_{t+1}$ que √© explicada pela regress√£o, √© definido como:
$$R^2 = \frac{Var(\hat{Y}_{t+1})}{Var(Y_{t+1})} = 1 - \frac{Var(U_{t+1})}{Var(Y_{t+1})}$$

Um $R^2$ pr√≥ximo de 1 indica que o modelo linear explica a maior parte da variabilidade de $Y_{t+1}$, enquanto um $R^2$ pr√≥ximo de 0 indica que o modelo linear √© pouco informativo.
**Proposi√ß√£o 1.2** (R-Squared ajustado): Uma vers√£o ajustada do R-Squared, que penaliza a inclus√£o de mais vari√°veis explicativas no modelo, √© dada por:

$$R_{adj}^2 = 1 - \frac{(1-R^2)(T-1)}{T-k-1}$$

Onde $k$ √© o n√∫mero de vari√°veis explicativas inclu√≠das em $X_t$. O $R_{adj}^2$ pode ser mais √∫til quando se compara modelos com diferentes n√∫meros de vari√°veis.

> üí° **Exemplo Num√©rico:** Vamos calcular o R¬≤ e o R¬≤ ajustado para os dados do exemplo anterior. Usando os dados do exemplo da regress√£o OLS com $b = \begin{bmatrix} 2.31 \\ 1.41 \end{bmatrix}$,  $X = \begin{bmatrix} 1 & 2 \\ 2 & 1 \\ 3 & 3 \\ 4 & 2 \\ 5 & 4 \end{bmatrix}$ e $Y = \begin{bmatrix} 4 \\ 5 \\ 10 \\ 11 \\ 14 \end{bmatrix}$,
>
> **1. Calcular os valores preditos $\hat{Y}_{t+1}$:**
>
> $\hat{Y} = Xb = \begin{bmatrix} 1 & 2 \\ 2 & 1 \\ 3 & 3 \\ 4 & 2 \\ 5 & 4 \end{bmatrix} \begin{bmatrix} 2.31 \\ 1.41 \end{bmatrix} = \begin{bmatrix} 5.13 \\ 6.03 \\ 11.16 \\ 12.06 \\ 16.19 \end{bmatrix}$
>
> **2. Calcular a vari√¢ncia de Y e de $\hat{Y}$**
>
> $Var(Y) = \frac{1}{5} \sum_{t=1}^{5} (y_t - \bar{y})^2$, onde $\bar{y} = \frac{4+5+10+11+14}{5} = 8.8$

$Var(Y) = \frac{1}{5} [(4-8.8)^2 + (5-8.8)^2 + (10-8.8)^2 + (11-8.8)^2 + (14-8.8)^2]$

$Var(Y) = \frac{1}{5} [(-4.8)^2 + (-3.8)^2 + (1.2)^2 + (2.2)^2 + (5.2)^2]$

$Var(Y) = \frac{1}{5} [23.04 + 14.44 + 1.44 + 4.84 + 27.04]$

$Var(Y) = \frac{1}{5} [70.8] = 14.16$

Para $\hat{Y}$:

$\bar{\hat{y}} = \frac{6.03 + 11.16 + 12.06 + 16.19}{4} = \frac{45.44}{4} = 11.36$

$Var(\hat{Y}) = \frac{1}{4} \sum_{t=1}^{4} (\hat{y}_t - \bar{\hat{y}})^2$

$Var(\hat{Y}) = \frac{1}{4} [(6.03-11.36)^2 + (11.16-11.36)^2 + (12.06 - 11.36)^2 + (16.19-11.36)^2]$

$Var(\hat{Y}) = \frac{1}{4} [(-5.33)^2 + (-0.2)^2 + (0.7)^2 + (4.83)^2]$

$Var(\hat{Y}) = \frac{1}{4} [28.4089 + 0.04 + 0.49 + 23.3289]$

$Var(\hat{Y}) = \frac{1}{4} [52.2678] = 13.06695 \approx 13.07$

> **3. Calcular o coeficiente de determina√ß√£o $R^2$**
>
$R^2 = 1 - \frac{Var(Y - \hat{Y})}{Var(Y)}$

Precisamos calcular a vari√¢ncia dos erros $Var(Y - \hat{Y})$:
$Y - \hat{Y} = \begin{bmatrix} 4 \\ 5 \\ 10 \\ 11 \\ 14 \end{bmatrix} - \begin{bmatrix} 6.03 \\ 11.16 \\ 12.06 \\ 16.19 \\  \end{bmatrix}$
$Y - \hat{Y} = \begin{bmatrix} -2.03 \\ -6.16 \\ -2.06 \\ -5.19 \\   \end{bmatrix} $

$\bar{e} = \frac{-2.03 - 6.16 - 2.06 - 5.19}{4} = -3.86$

$Var(Y-\hat{Y}) = \frac{1}{4} \sum_{t=1}^4 (e_t - \bar{e})^2 $

$Var(Y-\hat{Y}) = \frac{1}{4} [(-2.03 + 3.86)^2 + (-6.16+3.86)^2 + (-2.06 + 3.86)^2 + (-5.19+3.86)^2]$

$Var(Y-\hat{Y}) = \frac{1}{4} [(1.83)^2 + (-2.3)^2 + (1.8)^2 + (-1.33)^2]$

$Var(Y-\hat{Y}) = \frac{1}{4} [3.3489 + 5.29 + 3.24 + 1.7689]$

$Var(Y-\hat{Y}) = \frac{1}{4} [13.6478] \approx 3.41 $

Agora podemos calcular $R^2$:

$R^2 = 1 - \frac{3.41}{14.16} = 1 - 0.2408 = 0.7592$

Assim, $R^2 \approx 0.759$

> **4. Conclus√£o**

O modelo linear explica aproximadamente 75.9% da varia√ß√£o nos dados.
<!-- END -->
