## Compara√ß√£o Detalhada entre os Coeficientes OLS e de Proje√ß√£o Linear

### Introdu√ß√£o

Este cap√≠tulo aprofunda a distin√ß√£o entre os coeficientes estimados pela **regress√£o de m√≠nimos quadrados ordin√°rios (OLS)** e os coeficientes obtidos atrav√©s da **proje√ß√£o linear**, focando em como cada um captura informa√ß√µes distintas dos dados [^4.1.13, 4.1.18, 4.1.19]. Como estabelecido anteriormente, os coeficientes OLS, denotados por $b$, s√£o derivados de **momentos amostrais**, ou seja, informa√ß√µes espec√≠ficas da amostra de dados utilizada [^4.1.18, 4.1.19]. Por outro lado, os coeficientes de proje√ß√£o linear, denotados por $\alpha$, s√£o derivados de **momentos populacionais**, que representam as caracter√≠sticas te√≥ricas do processo estoc√°stico subjacente [^4.1.13]. Esta diferen√ßa fundamental na origem dos coeficientes leva a interpreta√ß√µes distintas e implica√ß√µes pr√°ticas que s√£o exploradas neste cap√≠tulo.

### Coeficientes OLS ($b$): Estimativas Amostrais

O estimador OLS $b$ √© obtido minimizando a soma dos quadrados dos res√≠duos para um conjunto de $T$ observa√ß√µes:
$$b = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_{t+1}$$[^4.1.18]
Esta f√≥rmula pode ser expressa em termos de momentos amostrais:
$$ b = \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t y_{t+1} \right] $$ [^4.1.19]

Aqui, $\left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t'$ representa a matriz de covari√¢ncia amostral das vari√°veis explicativas $X_t$, e $\left( \frac{1}{T} \right) \sum_{t=1}^T x_t y_{t+1}$ representa o vetor de covari√¢ncia amostral entre as vari√°veis explicativas $X_t$ e a vari√°vel dependente $Y_{t+1}$. √â crucial notar que esses momentos s√£o espec√≠ficos da amostra de dados utilizada para a estima√ß√£o. Portanto, diferentes amostras do mesmo processo estoc√°stico produzir√£o diferentes estimativas de $b$, devido a variabilidade inerente do processo.

> üí° **Exemplo Num√©rico:** Suponha que temos um modelo linear com uma √∫nica vari√°vel explicativa: $y_{t+1} = \beta x_t + u_{t+1}$. Coletamos uma amostra de $T = 5$ observa√ß√µes:
>
> | t  | $x_t$ | $y_{t+1}$ |
> |----|-------|-----------|
> | 1  | 1     | 3         |
> | 2  | 2     | 5         |
> | 3  | 3     | 7         |
> | 4  | 4     | 9         |
> | 5  | 5     | 12        |
>
> Calculamos os momentos amostrais:
>
> $\sum_{t=1}^5 x_t x_t = 1^2 + 2^2 + 3^2 + 4^2 + 5^2 = 55$
>
> $\sum_{t=1}^5 x_t y_{t+1} = (1 \times 3) + (2 \times 5) + (3 \times 7) + (4 \times 9) + (5 \times 12) = 3 + 10 + 21 + 36 + 60 = 130$
>
>  Assim, $b = \frac{\sum_{t=1}^5 x_t y_{t+1}}{\sum_{t=1}^5 x_t x_t} = \frac{130}{55} \approx 2.36$.
>
>  O estimador OLS para $\beta$ nesta amostra √© aproximadamente 2.36. Se colet√°ssemos uma nova amostra, obter√≠amos um valor diferente para $b$.
> ```python
> import numpy as np
>
> x = np.array([1, 2, 3, 4, 5])
> y = np.array([3, 5, 7, 9, 12])
>
> XtX = np.sum(x*x)
> XtY = np.sum(x*y)
> b = XtY / XtX
>
> print(f"Estimativa OLS (b): {b:.2f}")
> ```

**Propriedades Estat√≠sticas do Estimador OLS:**

*   **N√£o-Viesamento (sob condi√ß√µes ideais):**  Se os pressupostos do modelo linear cl√°ssico s√£o v√°lidos (como exogeneidade das vari√°veis explicativas, homocedasticidade, e n√£o autocorrelacao dos erros), o estimador OLS √© n√£o-viesado, ou seja, o valor esperado de $b$ √© igual ao verdadeiro valor do coeficiente $\beta$: $E(b) = \beta$. No entanto, na pr√°tica, esses pressupostos podem n√£o se verificar perfeitamente, resultando em vieses nos estimadores.
*   **Consist√™ncia (sob condi√ß√µes de estacionariedade e ergodicidade):**  Como estabelecido no Lema 1 do cap√≠tulo anterior, sob condi√ß√µes de estacionariedade e ergodicidade para momentos de segunda ordem, o estimador OLS $b$ converge em probabilidade para o verdadeiro coeficiente da proje√ß√£o linear $\alpha$, √† medida que o tamanho da amostra $T$ tende ao infinito:  $b \overset{p}{\rightarrow} \alpha$.
*   **Efici√™ncia (em rela√ß√£o a outros estimadores lineares):** Dentro da classe de estimadores lineares e n√£o viesados, o estimador OLS √© o mais eficiente, ou seja, possui a menor vari√¢ncia poss√≠vel. No entanto, esta propriedade √© v√°lida apenas quando os erros s√£o homoced√°sticos.
*   **Distribui√ß√£o Assint√≥tica Normal:** Sob condi√ß√µes de regularidade (como a exist√™ncia de momentos de quarta ordem e independ√™ncia entre vari√°veis), o estimador OLS $b$ possui distribui√ß√£o assint√≥tica normal. Esta propriedade √© crucial para construir intervalos de confian√ßa e realizar testes de hip√≥teses sobre os coeficientes.

**Lema 1.1** (Converg√™ncia dos Momentos Amostrais): Sob condi√ß√µes de estacionariedade e ergodicidade, os momentos amostrais convergem em probabilidade para seus respectivos momentos populacionais. Ou seja,
$$\frac{1}{T} \sum_{t=1}^T x_t x_t' \overset{p}{\rightarrow} E(X_t X_t')$$
e
$$\frac{1}{T} \sum_{t=1}^T x_t y_{t+1} \overset{p}{\rightarrow} E(X_t Y_{t+1})$$

*Proof:* A prova deste resultado segue diretamente das propriedades de ergodicidade e da lei dos grandes n√∫meros para processos estoc√°sticos. Em ess√™ncia, a ergodicidade garante que as m√©dias temporais, calculadas sobre uma √∫nica realiza√ß√£o do processo, convergem para as m√©dias populacionais quando o n√∫mero de observa√ß√µes tende ao infinito.
Este lema √© fundamental para estabelecer a consist√™ncia do estimador OLS, pois ele garante que os termos na f√≥rmula do estimador OLS convergem para os seus valores populacionais, como explicitado no item seguinte.

**Teorema 1.1** (Consist√™ncia do Estimador OLS):
Sob as condi√ß√µes de estacionariedade e ergodicidade, e assumindo que $E(X_tX_t')$ √© n√£o singular, o estimador OLS converge em probabilidade para o coeficiente de proje√ß√£o linear $\alpha$, i.e., $b \overset{p}{\rightarrow} \alpha$ quando $T \rightarrow \infty$.

*Proof:* Dada a equa√ß√£o do estimador OLS:
$$ b = \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t y_{t+1} \right] $$
pelo Lema 1.1, temos que:
$$ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t'  \overset{p}{\rightarrow}  E(X_t X_t')$$
e
$$ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t y_{t+1}  \overset{p}{\rightarrow} E(X_t Y_{t+1})$$

I. Pelo Lema 1.1, sabemos que
    $$\frac{1}{T} \sum_{t=1}^T x_t x_t'  \overset{p}{\rightarrow}  E(X_t X_t')$$
    e
    $$\frac{1}{T} \sum_{t=1}^T x_t y_{t+1}  \overset{p}{\rightarrow} E(X_t Y_{t+1})$$
II. Como a inversa de uma matriz √© uma opera√ß√£o cont√≠nua, se $ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t'$ converge em probabilidade para $E(X_t X_t')$, ent√£o $\left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t' \right]^{-1}$ converge em probabilidade para $[E(X_t X_t')]^{-1}$. Ou seja,
    $$\left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t' \right]^{-1} \overset{p}{\rightarrow} [E(X_t X_t')]^{-1}$$
III. Usando o fato de que o produto de duas sequ√™ncias que convergem em probabilidade tamb√©m converge em probabilidade para o produto dos seus limites, temos que:
     $$b = \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t y_{t+1} \right] \overset{p}{\rightarrow} [E(X_t X_t')]^{-1} E(X_t Y_{t+1}) = \alpha$$
Portanto, demonstramos que o estimador OLS $b$ converge em probabilidade para o coeficiente de proje√ß√£o linear $\alpha$. ‚ñ†

### Coeficientes de Proje√ß√£o Linear ($\alpha$): Par√¢metros Populacionais

Os coeficientes de proje√ß√£o linear, $\alpha$, s√£o derivados da express√£o:
$$\alpha' = E(Y_{t+1} X_t') [E(X_t X_t')]^{-1}$$ [^4.1.13]
Aqui, $E(Y_{t+1} X_t')$ e $E(X_t X_t')$ s√£o os momentos populacionais do processo estoc√°stico $\{X_t, Y_{t+1}\}$. A proje√ß√£o linear busca a melhor aproxima√ß√£o linear de $Y_{t+1}$ a partir de $X_t$, no sentido de minimizar o erro quadr√°tico m√©dio, $E[(Y_{t+1} - \alpha'X_t)^2]$.

Diferentemente do estimador OLS, que varia com diferentes amostras, o coeficiente de proje√ß√£o linear $\alpha$ √© um par√¢metro fixo do processo estoc√°stico, refletindo a rela√ß√£o te√≥rica subjacente entre $Y_{t+1}$ e $X_t$.

**Caracter√≠sticas dos Coeficientes de Proje√ß√£o Linear:**

*   **Par√¢metro Populacional:**  $\alpha$ representa uma caracter√≠stica do processo estoc√°stico subjacente, e n√£o da amostra espec√≠fica utilizada. Ele √© o valor que minimizar√° o erro quadr√°tico m√©dio na popula√ß√£o, n√£o se alterando com novas amostras.
*   **Unicidade:** Para um dado vetor de vari√°veis $X_t$ e sob a condi√ß√£o de que $E(X_tX_t')$ seja n√£o singular, existe um √∫nico valor para $\alpha$ que minimiza o erro quadr√°tico m√©dio na popula√ß√£o.
*   **Estabilidade:**  Em processos estacion√°rios, o valor de $\alpha$ permanece constante ao longo do tempo. Esta estabilidade √© fundamental para a an√°lise de s√©ries temporais e para a previs√£o de longo prazo.
*   **Interpreta√ß√£o:**  $\alpha$ quantifica o efeito m√©dio de $X_t$ em $Y_{t+1}$, dado o conhecimento da distribui√ß√£o conjunta de ambas.  Em contextos causais, $\alpha$ representa o efeito de uma mudan√ßa em $X_t$ em $Y_{t+1}$, *assumindo que n√£o h√° outras vari√°veis omitidas que influenciem a rela√ß√£o entre ambas*.

> üí° **Exemplo Num√©rico:** Considere um modelo populacional simples, onde $Y_{t+1} = 2X_t + U_{t+1}$, e onde $E(X_t^2) = 5$ e $E(Y_{t+1}X_t) = 10$. Ent√£o, $\alpha = E(Y_{t+1}X_t)/E(X_t^2) = 10/5 = 2$. Em uma amostra espec√≠fica, a estimativa OLS $b$ pode n√£o ser exatamente igual a 2, mas sob as condi√ß√µes de consist√™ncia, a estimativa OLS se aproximar√° do valor de 2 quando o tamanho da amostra aumenta.

### Compara√ß√£o Direta: $b$ vs. $\alpha$

1.  **Origem:**
    *   $b$:  Calculado a partir de momentos amostrais, utilizando informa√ß√µes de uma amostra espec√≠fica de dados.
    *   $\alpha$: Calculado a partir de momentos populacionais, que representam as caracter√≠sticas te√≥ricas do processo.
2.  **Variabilidade:**
    *   $b$: √â uma vari√°vel aleat√≥ria, cujo valor varia de amostra para amostra.
    *   $\alpha$: √â um par√¢metro fixo do processo estoc√°stico, n√£o variando com as amostras.
3.  **Consist√™ncia:**
    *   Sob as condi√ß√µes de covari√¢ncia-estacionariedade e ergodicidade, $b$ converge em probabilidade para $\alpha$ quando o tamanho da amostra tende ao infinito.
4.  **Precis√£o:**
    *   $b$: A precis√£o de $b$ depende do tamanho da amostra. Quanto maior a amostra, mais precisa √© a estimativa.
    *   $\alpha$:  A precis√£o de $\alpha$ depende do conhecimento das distribui√ß√µes subjacentes.
5.  **Interpreta√ß√£o:**
    *   $b$:  Quantifica a rela√ß√£o linear entre $Y_{t+1}$ e $X_t$ na amostra observada.
    *   $\alpha$:  Quantifica a rela√ß√£o linear entre $Y_{t+1}$ e $X_t$ na popula√ß√£o, representando a rela√ß√£o te√≥rica do processo estoc√°stico.

> üí° **Exemplo Num√©rico:** Vamos considerar um cen√°rio onde simulamos dados de um processo AR(1), definido por $y_{t+1} = 0.8y_t + u_{t+1}$, onde $u_{t+1}$ √© ru√≠do branco.  Simulamos 100 amostras com $T=100$, e calculamos os coeficientes da regress√£o de $y_{t+1}$ em $y_t$ para cada amostra e tamb√©m o coeficiente da proje√ß√£o linear quando conhecemos os momentos populacionais do modelo. O verdadeiro valor populacional de $\alpha$ nesse cen√°rio √© 0.8. A distribui√ß√£o dos coeficientes OLS amostrais ter√° um desvio padr√£o que diminui com o aumento de $T$, centrando em torno do valor de 0.8.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from numpy.linalg import inv
>
> def simulate_ar1(T, phi, sigma):
>     u = np.random.normal(0, sigma, T+1)
>     y = np.zeros(T+1)
>     for t in range(1, T+1):
>         y[t] = phi*y[t-1] + u[t]
>     return y[1:]
>
> def ols_estimation(x, y):
>     X = x.reshape(-1,1)
>     X_transpose = X.transpose()
>     XtX = np.dot(X_transpose, X)
>     inv_XtX = inv(XtX)
>     XtY = np.dot(X_transpose, y)
>     b = np.dot(inv_XtX, XtY)
>     return b[0]
>
> def alpha_population(phi, sigma):
>    return phi
>
>
> np.random.seed(42)
>
> T = 100
> num_samples = 100
> true_phi = 0.8
> true_sigma = 1
>
> b_values = []
> for _ in range(num_samples):
>     y = simulate_ar1(T, true_phi, true_sigma)
>     x = y[:-1]
>     b = ols_estimation(x, y[1:])
>     b_values.append(b)
>
> alpha_true = alpha_population(true_phi, true_sigma)
>
> print(f"Coeficiente verdadeiro da projecao linear (alpha): {alpha_true}")
> print(f"Media do estimador OLS (b): {np.mean(b_values)}")
> print(f"Desvio padrao do estimador OLS (b): {np.std(b_values)}")
>
> plt.hist(b_values, bins=20, edgecolor='black')
> plt.axvline(alpha_true, color='r', linestyle='dashed', linewidth=1, label='alpha_true')
> plt.title("Distribui√ß√£o dos estimadores OLS")
> plt.xlabel("Estimador OLS (b)")
> plt.ylabel("Frequ√™ncia")
> plt.legend()
> plt.show()
> ```
>
> ```
> Coeficiente verdadeiro da projecao linear (alpha): 0.8
> Media do estimador OLS (b): 0.7833301401124663
> Desvio padrao do estimador OLS (b): 0.09160689914352474
> ```
>
> Este exemplo demonstra que o estimador OLS √© uma vari√°vel aleat√≥ria e se aproxima do valor populacional de $\alpha$ a medida que aumentamos o tamanho da amostra.
>
> üí° **Exemplo Num√©rico:** Para ilustrar a diferen√ßa entre os coeficientes OLS e os coeficientes da proje√ß√£o linear em um caso multivariado, considere o seguinte processo:
> $Y_{t+1} = 1.5X_{1t} - 0.5X_{2t} + U_{t+1}$.
> Assuma que os momentos populacionais s√£o:
> $E(X_t X_t') = \begin{bmatrix} 2 & 0.5 \\ 0.5 & 1 \end{bmatrix}$
> $E(Y_{t+1} X_t') = \begin{bmatrix} 2.75 \\ -0.25 \end{bmatrix}$
> O coeficiente da proje√ß√£o linear √©:
>
> $\alpha = [E(X_t X_t')]^{-1} E(Y_{t+1} X_t')$
>
> Primeiro, calcule a inversa de $E(X_t X_t')$:
>
> $[E(X_t X_t')]^{-1} = \frac{1}{(2)(1) - (0.5)(0.5)} \begin{bmatrix} 1 & -0.5 \\ -0.5 & 2 \end{bmatrix} = \frac{1}{1.75} \begin{bmatrix} 1 & -0.5 \\ -0.5 & 2 \end{bmatrix} \approx \begin{bmatrix} 0.57 & -0.29 \\ -0.29 & 1.14 \end{bmatrix} $
>
> Agora, calcule $\alpha$:
>
> $\alpha = \begin{bmatrix} 0.57 & -0.29 \\ -0.29 & 1.14 \end{bmatrix} \begin{bmatrix} 2.75 \\ -0.25 \end{bmatrix} = \begin{bmatrix} (0.57 \times 2.75) + (-0.29 \times -0.25) \\ (-0.29 \times 2.75) + (1.14 \times -0.25) \end{bmatrix} = \begin{bmatrix} 1.57 + 0.07 \\ -0.80 - 0.29 \end{bmatrix} = \begin{bmatrix} 1.64 \\ -1.09 \end{bmatrix}$
>
> O coeficiente da proje√ß√£o linear √©, aproximadamente, $\alpha = \begin{bmatrix} 1.64 \\ -1.09 \end{bmatrix}$. Note que estes valores s√£o pr√≥ximos aos coeficientes verdadeiros do modelo populacional. O estimador OLS em uma amostra finita ser√° diferente, mas convergir√° para esse valor com o aumento de T.

### Conclus√£o

Este cap√≠tulo destacou a natureza distinta dos coeficientes estimados pela regress√£o OLS e da proje√ß√£o linear. Enquanto os coeficientes OLS s√£o constru√≠dos a partir de momentos amostrais e s√£o inerentemente vari√°veis devido √† natureza finita das amostras, os coeficientes de proje√ß√£o linear s√£o derivados de momentos populacionais, que representam par√¢metros fixos do processo estoc√°stico subjacente. Esta distin√ß√£o √© fundamental para entender a interpreta√ß√£o e o uso de cada m√©todo. O estimador OLS, sob condi√ß√µes de consist√™ncia, se aproxima do coeficiente da proje√ß√£o linear quando o tamanho da amostra aumenta, destacando o valor da regress√£o OLS como uma ferramenta √∫til na estima√ß√£o de modelos de s√©ries temporais e na previs√£o.

### Refer√™ncias

[^4.1.9]: A proje√ß√£o linear √© definida como a melhor aproxima√ß√£o linear de uma vari√°vel aleat√≥ria em termos de outra.
[^4.1.13]: A f√≥rmula para o coeficiente de proje√ß√£o linear $\alpha$ √© dada por  $\alpha' = E(Y_{t+1} X_t') [E(X_t X_t')]^{-1}$.
[^4.1.18]: O estimador OLS √© dado por $b = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_{t+1}$.
[^4.1.19]: A formula para b pode ser escrita como $b = \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t y_{t+1} \right]$.
[^4.1.20]: Em condi√ß√µes de estacionariedade e ergodicidade, os momentos amostrais convergem para os momentos populacionais e o estimador OLS converge para o coeficiente de proje√ß√£o linear.
<!-- END -->
