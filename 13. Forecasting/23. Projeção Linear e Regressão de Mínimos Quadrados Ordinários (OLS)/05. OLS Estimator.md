## O Estimador de OLS: Deriva√ß√£o e Propriedades

### Introdu√ß√£o

Este cap√≠tulo foca na deriva√ß√£o e nas propriedades do **estimador de m√≠nimos quadrados ordin√°rios (OLS)**, denotado por $b$, que √© um componente central na an√°lise de regress√£o e proje√ß√£o linear. Conforme vimos nos cap√≠tulos anteriores, a regress√£o OLS busca encontrar a melhor rela√ß√£o linear entre uma vari√°vel dependente, $Y_{t+1}$, e um conjunto de vari√°veis explicativas, $X_t$, minimizando a soma dos quadrados dos res√≠duos. Este cap√≠tulo apresenta a deriva√ß√£o formal do estimador OLS e explora suas propriedades estat√≠sticas, como n√£o-viesamento, consist√™ncia e efici√™ncia.

### Deriva√ß√£o do Estimador OLS

Considere o modelo de regress√£o linear:

$$Y_{t+1} = \beta' X_t + u_{t+1}$$ [^4.1.16]

onde $Y_{t+1}$ √© a vari√°vel dependente, $X_t$ √© um vetor de vari√°veis explicativas, $\beta$ √© um vetor de coeficientes a serem estimados e $u_{t+1}$ √© o termo de erro. O objetivo da regress√£o OLS √© encontrar o vetor $\beta$ que melhor ajusta os dados, minimizando a soma dos quadrados dos res√≠duos:

$$ \sum_{t=1}^T (y_{t+1} - \beta' x_t)^2 $$ [^4.1.17]

onde $y_{t+1}$ e $x_t$ representam os valores observados das vari√°veis dependente e explicativas no tempo $t$, respectivamente. O **estimador OLS**, denotado por $b$, √© o vetor de coeficientes que minimiza essa soma.

Para encontrar $b$, derivamos a soma dos quadrados dos res√≠duos em rela√ß√£o a $\beta$ e igualamos a zero. Seja $S(\beta)$ a soma dos quadrados dos res√≠duos:

$$ S(\beta) = \sum_{t=1}^T (y_{t+1} - \beta' x_t)^2 $$

Em nota√ß√£o matricial, podemos expressar o modelo como:

$$ Y = X\beta + U $$

onde $Y$ √© um vetor coluna $T \times 1$ de observa√ß√µes $y_{t+1}$, $X$ √© uma matriz $T \times k$ com cada linha sendo o vetor transposto $x_t'$ e $U$ √© o vetor coluna dos erros $u_{t+1}$.  A soma dos quadrados dos res√≠duos pode ser escrita como:

$$ S(\beta) = (Y - X\beta)'(Y - X\beta) = Y'Y - 2\beta'X'Y + \beta'X'X\beta $$
onde $'$ denota a transposi√ß√£o da matriz.

Derivando $S(\beta)$ em rela√ß√£o a $\beta$, obtemos:

$$ \frac{\partial S(\beta)}{\partial \beta} = -2X'Y + 2X'X\beta $$

Igualando essa derivada a zero (condi√ß√£o de primeira ordem para o m√≠nimo):

$$ -2X'Y + 2X'X\beta = 0 $$

Isolando $\beta$, obtemos o estimador OLS:

$$ 2X'X\beta = 2X'Y $$
$$ X'X\beta = X'Y $$
$$ b = (X'X)^{-1}X'Y $$
ou equivalentemente
$$b = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_{t+1}$$ [^4.1.18]
ou tamb√©m
$$ b = \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t y_{t+1} \right] $$ [^4.1.19]

Este √© o estimador OLS para o vetor de coeficientes $\beta$, tamb√©m denotado por $b$, que minimiza a soma dos quadrados dos res√≠duos.
**Lema 1** (Condi√ß√£o para a unicidade do estimador OLS): O estimador OLS $b$ √© √∫nico se a matriz $X'X = \sum_{t=1}^T x_t x_t'$ √© n√£o-singular. Isso significa que a matriz deve ter inversa, o que ocorre se n√£o houver multicolinearidade perfeita entre as vari√°veis explicativas.
*Prova:* A prova deste lema foi detalhada no cap√≠tulo anterior. Em resumo, se $X'X$ √© n√£o-singular, ent√£o a inversa $(X'X)^{-1}$ existe, o que garante que a solu√ß√£o da equa√ß√£o normal ($X'Xb = X'Y$) √© √∫nica, produzindo um √∫nico valor para o estimador OLS $b$.

**Lema 1.1** (Representa√ß√£o Alternativa do Estimador OLS): O estimador OLS pode ser expresso em termos dos desvios da m√©dia das vari√°veis. Definindo $\tilde{y}_{t+1} = y_{t+1} - \bar{y}$ e $\tilde{x}_t = x_t - \bar{x}$, onde $\bar{y}$ e $\bar{x}$ s√£o as m√©dias amostrais de $y_{t+1}$ e $x_t$, respectivamente, o estimador OLS pode ser escrito como:
$$ b = \left( \sum_{t=1}^T \tilde{x}_t \tilde{x}_t' \right)^{-1} \sum_{t=1}^T \tilde{x}_t \tilde{y}_{t+1} $$
*Prova:*
I.  Come√ßamos com a representa√ß√£o original do estimador OLS:
    $$b = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_{t+1}$$
II.  Substitu√≠mos $x_t$ por $\tilde{x}_t + \bar{x}$ e $y_{t+1}$ por $\tilde{y}_{t+1} + \bar{y}$:
    $$b = \left( \sum_{t=1}^T (\tilde{x}_t + \bar{x}) (\tilde{x}_t + \bar{x})' \right)^{-1} \sum_{t=1}^T (\tilde{x}_t + \bar{x}) (\tilde{y}_{t+1} + \bar{y})$$
III. Expandimos os produtos:
   $$b = \left( \sum_{t=1}^T (\tilde{x}_t \tilde{x}_t' + \tilde{x}_t \bar{x}' + \bar{x} \tilde{x}_t' + \bar{x} \bar{x}') \right)^{-1} \sum_{t=1}^T (\tilde{x}_t \tilde{y}_{t+1} + \tilde{x}_t \bar{y} + \bar{x} \tilde{y}_{t+1} + \bar{x} \bar{y})$$
IV. Usamos as propriedades $\sum_{t=1}^T \tilde{x}_t = 0$ e $\sum_{t=1}^T \tilde{y}_{t+1} = 0$. Isso implica que $\sum_{t=1}^T \tilde{x}_t \bar{x}' = \sum_{t=1}^T \bar{x} \tilde{x}_t' = 0$ e  $\sum_{t=1}^T \tilde{x}_t \bar{y} = \sum_{t=1}^T \bar{x} \tilde{y}_{t+1} = 0$:
    $$b = \left( \sum_{t=1}^T (\tilde{x}_t \tilde{x}_t' + \bar{x} \bar{x}') \right)^{-1} \sum_{t=1}^T (\tilde{x}_t \tilde{y}_{t+1} + \bar{x} \bar{y})$$
V.  No entanto, √© importante notar que:
    $$
    \sum_{t=1}^T x_t x_t' = \sum_{t=1}^T \tilde{x}_t \tilde{x}_t' + T \bar{x}\bar{x}'
    $$
    $$
     \sum_{t=1}^T x_t y_{t+1} = \sum_{t=1}^T \tilde{x}_t \tilde{y}_{t+1} + T \bar{x}\bar{y}
    $$
VI. Dado que, em uma regress√£o com constante, $\bar{y} = b_0 + \bar{x}b$, ent√£o a parte que envolve a m√©dia em ambos o numerador e denominador se cancelam, resultando na representa√ß√£o do estimador OLS em termos de desvios da m√©dia:
    $$ b = \left( \sum_{t=1}^T \tilde{x}_t \tilde{x}_t' \right)^{-1} \sum_{t=1}^T \tilde{x}_t \tilde{y}_{t+1} $$‚ñ†
Esta representa√ß√£o √© √∫til para destacar a rela√ß√£o entre o estimador OLS e a covari√¢ncia entre as vari√°veis, al√©m de facilitar algumas deriva√ß√µes te√≥ricas.
> üí° **Exemplo Num√©rico:** Vamos considerar um exemplo simplificado com duas vari√°veis, $X$ e $Y$, e aplicar a f√≥rmula do estimador OLS para calcular o coeficiente $\beta$. Suponha que temos os seguintes dados para 3 per√≠odos:
>
> | t   | $x_t$ | $y_{t+1}$ |
> |-----|-------|-----------|
> | 1   | 1     | 3         |
> | 2   | 2     | 5         |
> | 3   | 3     | 7         |
>
> Podemos calcular $\sum_{t=1}^3 x_t x_t'$ e $\sum_{t=1}^3 x_t y_{t+1}$:
>
>  $\sum_{t=1}^3 x_t x_t' = (1*1) + (2*2) + (3*3) = 1 + 4 + 9 = 14$
>
> $\sum_{t=1}^3 x_t y_{t+1} = (1*3) + (2*5) + (3*7) = 3 + 10 + 21 = 34$
>
> O estimador OLS $b$ √© dado por:
>
> $b = \left( \sum_{t=1}^3 x_t x_t' \right)^{-1} \sum_{t=1}^3 x_t y_{t+1} = 14^{-1} * 34 = \frac{34}{14} = 2.43$ (aproximadamente)
>
> Ent√£o, o coeficiente estimado de OLS √© de aproximadamente 2.43, o que significa que para cada aumento de uma unidade em $X$, esperamos um aumento de aproximadamente 2.43 unidades em $Y$.
>
> Agora, vamos fazer os c√°lculos usando a representa√ß√£o em termos de desvios da m√©dia:
>
>  $\bar{x} = (1+2+3)/3 = 2$
>  $\bar{y} = (3+5+7)/3 = 5$
>
> | t   | $\tilde{x}_t$ | $\tilde{y}_{t+1}$ |
> |-----|---------------|-------------------|
> | 1   | -1            | -2                |
> | 2   | 0             | 0                 |
> | 3   | 1             | 2                 |
>
> $\sum_{t=1}^3 \tilde{x}_t \tilde{x}_t' = (-1*-1) + (0*0) + (1*1) = 1+0+1 = 2$
>
> $\sum_{t=1}^3 \tilde{x}_t \tilde{y}_{t+1} = (-1*-2) + (0*0) + (1*2) = 2+0+2 = 4$
>
> $b = \left( \sum_{t=1}^3 \tilde{x}_t \tilde{x}_t' \right)^{-1} \sum_{t=1}^3 \tilde{x}_t \tilde{y}_{t+1} = 2^{-1} * 4 = \frac{4}{2} = 2$
>
> Note que este valor coincide com o coeficiente da regress√£o com constante, usando o mesmo conjunto de dados.

### Propriedades Estat√≠sticas do Estimador OLS

O estimador OLS possui diversas propriedades estat√≠sticas que o tornam uma ferramenta √∫til na econometria e na an√°lise de s√©ries temporais, quando seus pressupostos s√£o respeitados:

1.  **N√£o-Viesamento (sob condi√ß√µes ideais):** Sob os pressupostos do modelo linear cl√°ssico (exogeneidade, homocedasticidade, n√£o autocorrela√ß√£o dos erros e amostras aleat√≥rias), o estimador OLS √© n√£o-viesado. Isso significa que, em m√©dia, o valor esperado de $b$ √© igual ao verdadeiro valor do coeficiente $\beta$:

$$E[b] = \beta$$

   O n√£o-viesamento garante que o estimador OLS n√£o subestima ou superestima sistematicamente o verdadeiro valor dos coeficientes. No entanto, √© importante notar que essa propriedade √© v√°lida apenas sob os pressupostos do modelo linear cl√°ssico e que na pr√°tica esses pressupostos podem n√£o se verificar completamente.

   Para demonstrar o n√£o-viesamento, partimos do modelo linear $Y = X\beta + U$ e multiplicamos ambos os lados por $X'$ e pr√©-multiplicamos por $(X'X)^{-1}$:
   $$ (X'X)^{-1}X'Y = (X'X)^{-1}X'(X\beta + U) $$
   $$ b = \beta + (X'X)^{-1}X'U $$
   Aplicando o valor esperado:
   $$ E(b) = E[\beta + (X'X)^{-1}X'U] $$
   Como $\beta$ √© um valor constante e $E(U) = 0$ sob as condi√ß√µes de exogeneidade de $X$ e m√©dia zero de $U$, temos:
    $$ E(b) = \beta + E[(X'X)^{-1}X'U] $$
   Como $X$ √© ex√≥geno, $(X'X)^{-1}X'$ tamb√©m √©, e o valor esperado pode passar para dentro:
    $$ E(b) = \beta + (X'X)^{-1}X'E[U] $$
    Como o valor esperado de U √© zero,
      $$ E(b) = \beta  $$
   Portanto, o estimador OLS √© n√£o-viesado.
   *Prova:*
I.  Come√ßamos com o estimador OLS:
    $$ b = (X'X)^{-1}X'Y $$
II. Substitu√≠mos $Y$ pelo modelo verdadeiro $X\beta + U$:
    $$ b = (X'X)^{-1}X'(X\beta + U) $$
III. Expandimos a express√£o:
    $$ b = (X'X)^{-1}X'X\beta + (X'X)^{-1}X'U $$
IV. Simplificamos usando o fato de que $(X'X)^{-1}X'X = I$, onde $I$ √© a matriz identidade:
    $$ b = \beta + (X'X)^{-1}X'U $$
V. Aplicamos o operador de valor esperado:
    $$ E[b] = E[\beta + (X'X)^{-1}X'U] $$
VI. Pela linearidade do valor esperado e como $\beta$ √© constante, temos:
    $$ E[b] = \beta + E[(X'X)^{-1}X'U] $$
VII. Dado que $X$ √© ex√≥geno, $(X'X)^{-1}X'$ √© considerado constante em rela√ß√£o a $U$, logo:
   $$E[b] = \beta + (X'X)^{-1}X'E[U]$$
VIII. Pelo pressuposto de que $E[U] = 0$:
    $$ E[b] = \beta $$
Portanto, o estimador OLS √© n√£o-viesado. ‚ñ†

2.  **Consist√™ncia:** Como estabelecido no cap√≠tulo anterior, sob as condi√ß√µes de estacionariedade e ergodicidade, o estimador OLS $b$ √© um estimador consistente do coeficiente de proje√ß√£o linear $\alpha$. Isso significa que, √† medida que o tamanho da amostra $T$ tende ao infinito, o estimador OLS $b$ converge em probabilidade para $\alpha$:

$$ b \overset{p}{\rightarrow} \alpha $$ [^4.1.20]

   A consist√™ncia garante que, com um n√∫mero suficientemente grande de observa√ß√µes, o estimador OLS se aproxima do verdadeiro valor do coeficiente.
> üí° **Exemplo Num√©rico:** Para ilustrar a consist√™ncia, vamos simular o mesmo modelo linear da se√ß√£o anterior, mas com diferentes tamanhos de amostra e calcular as estimativas. Considere novamente o modelo $Y_{t+1} = 2X_t + U_{t+1}$. Vamos gerar dados para diferentes valores de $T$ e ver como as estimativas convergem para 2.
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> beta_verdadeiro = 2
> media_x = 5
> desvio_padrao_x = 2
> desvio_padrao_u = 1
> tamanhos_amostra = [100, 500, 1000, 5000, 10000]
>
> # Gerando dados e estimando o modelo
> estimativas_b = []
> for T in tamanhos_amostra:
>     np.random.seed(42)
>     X = np.random.normal(media_x, desvio_padrao_x, T)
>     U = np.random.normal(0, desvio_padrao_u, T)
>     Y = beta_verdadeiro * X + U
>     data = pd.DataFrame({'X':X, 'Y':Y})
>     X_reg = sm.add_constant(data['X'])
>     modelo = sm.OLS(data['Y'], X_reg)
>     res = modelo.fit()
>     estimativas_b.append(res.params['X'])
>
>
> # Plotando os resultados
> plt.figure(figsize=(10,6))
> plt.plot(tamanhos_amostra, estimativas_b, marker='o')
> plt.axhline(beta_verdadeiro, color='red', linestyle='dashed', linewidth=1, label='True beta')
> plt.xlabel('Tamanho da Amostra (T)')
> plt.ylabel('Estimativa do Coeficiente (b)')
> plt.title('Consist√™ncia do Estimador OLS')
> plt.legend()
> plt.grid(True)
> plt.show()
>
> for i, T in enumerate(tamanhos_amostra):
>     print(f"Tamanho da Amostra = {T}, Estimativa de b = {estimativas_b[i]:.4f}")
> ```
> Observamos que, √† medida que o tamanho da amostra $T$ aumenta, a estimativa de $b$ se aproxima do valor verdadeiro de $\beta$, que √© 2. Isso ilustra o conceito de consist√™ncia: quanto maior a amostra, mais precisa √© a estimativa do coeficiente.

3. **Efici√™ncia:** Dentro da classe de estimadores lineares n√£o viesados, o estimador OLS √© o mais eficiente, ou seja, possui a menor vari√¢ncia poss√≠vel. Este resultado, conhecido como Teorema de Gauss-Markov, garante que, sob os pressupostos do modelo linear cl√°ssico, o OLS fornece as melhores estimativas poss√≠veis na classe de estimadores lineares n√£o-viesados.
        A vari√¢ncia dos estimadores OLS √© dada por:
   $$Var(b) = \sigma^2 (X'X)^{-1}$$
    onde $\sigma^2$ √© a vari√¢ncia do termo de erro.

   **Teorema 1** (Teorema de Gauss-Markov): Dado o modelo cl√°ssico de regress√£o linear, o estimador OLS $b$ √© o melhor estimador linear n√£o viesado (BLUE). Isso significa que qualquer outro estimador linear n√£o viesado ter√° uma matriz de vari√¢ncia-covari√¢ncia que, quando subtra√≠da da matriz de vari√¢ncia-covari√¢ncia de $b$, resulta em uma matriz semi-definida positiva.
    *Prova:*
I. Seja $\hat{\beta}$ um estimador linear n√£o viesado qualquer: $\hat{\beta} = AY$, onde $A$ √© uma matriz de constantes.
II.  Para que $\hat{\beta}$ seja n√£o viesado, $E[\hat{\beta}] = \beta$. Substituindo $Y = X\beta + U$, obtemos $E[A(X\beta + U)] = AX\beta = \beta$. Para que isso seja verdade para qualquer valor de $\beta$, √© necess√°rio que $AX = I$, onde $I$ √© a matriz identidade.
III. A vari√¢ncia de $\hat{\beta}$ √© $Var(\hat{\beta}) = E[(\hat{\beta} - \beta)(\hat{\beta} - \beta)'] = E[A(Y-X\beta)(Y-X\beta)'A'] = E[AUU'A'] = \sigma^2 AA'$.
IV. Seja $A = (X'X)^{-1}X' + D$ onde $D$ √© uma matriz tal que $DX=0$.
V.  Ent√£o, $AA' = [(X'X)^{-1}X' + D][X(X'X)^{-1} + D'] = (X'X)^{-1}X'X(X'X)^{-1} + (X'X)^{-1}X'D' + DX(X'X)^{-1} + DD'$.
VI. Como $X'X(X'X)^{-1} = I$, temos que $X(X'X)^{-1} = I$ e como $DX=0$ os termos que envolvem D e X se cancelam, ent√£o $AA' = (X'X)^{-1} + DD'$.
VII. A diferen√ßa da vari√¢ncia de $\hat{\beta}$ e de $b$ √© dada por: $Var(\hat{\beta})-Var(b) = \sigma^2 [AA' - (X'X)^{-1}] = \sigma^2 DD'$.
VIII. Como $DD'$ √© uma matriz semi-definida positiva, $Var(\hat{\beta})-Var(b)$ tamb√©m √© uma matriz semi-definida positiva. Portanto o estimador OLS √© o melhor estimador linear n√£o-viesado (BLUE). ‚ñ†

4.  **Distribui√ß√£o Assint√≥tica Normal:** Sob certas condi√ß√µes de regularidade, o estimador OLS $b$ possui distribui√ß√£o assint√≥tica normal, como detalhado no cap√≠tulo anterior:
     $$ \sqrt{T}(b - \alpha) \overset{d}{\rightarrow} N(0, V) $$
    Onde $V$ √© a matriz de vari√¢ncia assint√≥tica:
    $V = [E(X_tX_t')]^{-1} E(X_t X_t'U_{t+1}^2) [E(X_tX_t')]^{-1}$.

   Esta propriedade √© fundamental para realizar testes de hip√≥teses sobre os coeficientes e construir intervalos de confian√ßa. A distribui√ß√£o normal garante que as conclus√µes estat√≠sticas baseadas no estimador OLS sejam v√°lidas quando o tamanho da amostra √© suficientemente grande.
    A distribui√ß√£o assint√≥tica normal se torna uma aproxima√ß√£o √∫til, especialmente em grandes amostras, onde a distribui√ß√£o exata do estimador pode ser dif√≠cil de determinar.
> üí° **Exemplo Num√©rico:** Para ilustrar a distribui√ß√£o assint√≥tica normal, vamos simular o modelo linear novamente e obter as estimativas de $b$ para diversas amostras. Em seguida, plotaremos um histograma das estimativas para verificar se a distribui√ß√£o se aproxima de uma normal. Vamos manter o modelo $Y_{t+1} = 2X_t + U_{t+1}$.
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
> import scipy.stats as st
>
> # Par√¢metros
> beta_verdadeiro = 2
> media_x = 5
> desvio_padrao_x = 2
> desvio_padrao_u = 1
> T = 1000
> num_simulacoes = 1000
>
> # Gerando dados e estimando o modelo para diversas simula√ß√µes
> estimativas_b = []
> for i in range(num_simulacoes):
>    np.random.seed(i)
>    X = np.random.normal(media_x, desvio_padrao_x, T)
>    U = np.random.normal(0, desvio_padrao_u, T)
>    Y = beta_verdadeiro * X + U
>    data = pd.DataFrame({'X':X, 'Y':Y})
>    X_reg = sm.add_constant(data['X'])
>    modelo = sm.OLS(data['Y'], X_reg)
>    res = modelo.fit()
>    estimativas_b.append(res.params['X'])
>
> # Plotando o histograma
> plt.figure(figsize=(10,6))
> plt.hist(estimativas_b, bins=30, density=True, alpha=0.6, color='skyblue', label='Histograma de b')
>
> # Plotando a curva da normal para compara√ß√£o
> mu = np.mean(estimativas_b)
> sigma = np.std(estimativas_b)
> x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)
> plt.plot(x, st.norm.pdf(x, mu, sigma), color='red', label='Distribui√ß√£o Normal')
>
> plt.xlabel('Estimativa de b')
> plt.ylabel('Densidade')
> plt.title('Distribui√ß√£o Assint√≥tica Normal do Estimador OLS')
> plt.legend()
> plt.grid(True)
> plt.show()
>
> print(f"M√©dia das estimativas (b): {mu:.4f}")
> print(f"Desvio Padr√£o das estimativas (b): {sigma:.4f}")
> ```
> Ao executar o c√≥digo, podemos observar que a distribui√ß√£o das estimativas de $b$ se assemelha a uma distribui√ß√£o normal, com m√©dia pr√≥xima ao valor verdadeiro de $\beta$. Isto ilustra a propriedade da distribui√ß√£o assint√≥tica normal.

5.  **Converg√™ncia:** Quando o processo √© estacion√°rio e erg√≥dico para momentos de segunda ordem, o estimador OLS se aproxima do coeficiente da proje√ß√£o linear na popula√ß√£o quando o tamanho da amostra tende ao infinito: $b \overset{p}{\rightarrow} \alpha$. Como vimos no cap√≠tulo anterior, este √© o caso quando os momentos amostrais se aproximam dos momentos populacionais.

> üí° **Exemplo Num√©rico:** A propriedade de converg√™ncia √© similar √† consist√™ncia, mas com um foco maior na rela√ß√£o com a proje√ß√£o linear na popula√ß√£o. J√° que demonstramos a consist√™ncia com exemplos anteriores, podemos visualizar como o estimador OLS se aproxima do valor da proje√ß√£o linear. Suponha que a verdadeira rela√ß√£o entre X e Y √© Y = 2X + U.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> beta_populacional = 2
> media_x = 5
> desvio_padrao_x = 2
> desvio_padrao_u = 1
>
> # Simula√ß√£o com diferentes tamanhos de amostra
> tamanhos_amostra = [100, 1000, 10000]
>
> plt.figure(figsize=(10, 6))
>
> for T in tamanhos_amostra:
>  np.random.seed(42)
>  X = np.random.normal(media_x, desvio_padrao_x, T)
>  U = np.random.normal(0, desvio_padrao_u, T)
>  Y = beta_populacional * X + U
>
>  data = pd.DataFrame({'X': X, 'Y': Y})
>  X_reg = sm.add_constant(data['X'])
>  model = sm.OLS(data['Y'], X_reg)
>  results = model.fit()
>  b_estimado = results.params['X']
>
>  plt.scatter(X, Y, label=f'T={T}, b={b_estimado:.2f}', alpha=0.5)
>  x_range = np.linspace(min(X), max(X), 100)
>  plt.plot(x_range, b_estimado * x_range, linestyle='--')
>
> plt.plot(x_range, beta_populacional* x_range, label=f'Proje√ß√£o Linear (b={beta_populacional})', color='black')
> plt.xlabel('X')
> plt.ylabel('Y')
> plt.title('Converg√™ncia do Estimador OLS para Diferentes Tamanhos de Amostra')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>  Neste gr√°fico, vemos que as retas estimadas pelo OLS se aproximam da reta da verdadeira proje√ß√£o linear √† medida que o tamanho da amostra aumenta. Isso demonstra que, com um n√∫mero grande o suficiente de observa√ß√µes, o estimador OLS converge para o verdadeiro valor da proje√ß√£o linear.

### Interpreta√ß√£o do Estimador OLS

O estimador OLS $b$ representa uma estimativa da rela√ß√£o linear entre as vari√°veis explicativas e a vari√°vel dependente. Cada componente de $b$ indica o efeito m√©dio de uma mudan√ßa unit√°ria na respectiva vari√°vel explicativa sobre a vari√°vel dependente, mantendo todas as outras vari√°veis constantes. Esta interpreta√ß√£o √© crucial para a an√°lise e previs√£o em modelos de s√©ries temporais.
> üí° **Exemplo Num√©rico:** Suponha que temos o seguinte modelo estimado usando dados de vendas de uma loja:
>
> $Vendas = 100 + 5 * Propaganda + 2 * Preco$
>
> onde *Vendas* √© a quantidade de produtos vendidos, *Propaganda* √© o gasto com propaganda e *Preco* √© o pre√ßo do produto.
>
> O estimador OLS neste caso fornece:
>
> *   Um intercepto de 100: Isso significa que se n√£o houver gastos com propaganda e o pre√ßo do produto for zero, ainda assim vender√≠amos 100 produtos.
> *   Um coeficiente de 5 para *Propaganda*: Isso indica que um aumento de 1 unidade no gasto com propaganda leva a um aumento de 5 unidades nas vendas, mantendo o pre√ßo constante.
> *   Um coeficiente de 2 para *Preco*: Isso indica que um aumento de 1 unidade no pre√ßo leva a um aumento de 2 unidades nas vendas, mantendo o gasto com propaganda constante.
>
> Estes valores fornecem uma interpreta√ß√£o direta de como cada vari√°vel explicativa influencia a vari√°vel dependente, permitindo aos gestores da loja tomar decis√µes baseadas em dados.

### Conclus√£o

Este cap√≠tulo apresentou a deriva√ß√£o formal do estimador OLS, demonstrando como a minimiza√ß√£o da soma dos quadrados dos res√≠duos leva √† f√≥rmula de $b$. Al√©m disso, foram exploradas as propriedades estat√≠sticas do estimador OLS, mostrando que ele √© n√£o-viesado sob condi√ß√µes ideais, consistente quando as amostras s√£o grandes, e eficiente na classe de estimadores lineares n√£o viesados. Essas propriedades justificam o uso do estimador OLS como uma ferramenta fundamental na an√°lise de s√©ries temporais e modelagem econom√©trica. O conhecimento da natureza e das propriedades do estimador OLS √© essencial para uma an√°lise estat√≠stica robusta e confi√°vel.

### Refer√™ncias

[^4.1.16]: Um modelo de regress√£o linear relaciona uma observa√ß√£o $y_{t+1}$ com um vetor de vari√°veis explicativas $x_t$: $y_{t+1} = \beta'x_t + u_t$.
[^4.1.17]: A soma amostral dos res√≠duos quadrados √© definida como $\sum_{t=1}^T (y_{t+1} - \beta'x_t)^2$.
[^4.1.18]: O estimador OLS √© dado por $b = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_{t+1}$.
[^4.1.19]: A formula para b pode ser escrita como $b = \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \left( \frac{1}{T} \right) \sum_{t=1}^T x_t y_{t+1} \right]$.
[^4.1.20]: Em condi√ß√µes de estacionariedade e ergodicidade, os momentos amostrais convergem para os momentos populacionais e o estimador OLS converge para o coeficiente de proje√ß√£o linear.
<!-- END -->
