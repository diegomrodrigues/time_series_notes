## Previs√µes Exatas de Amostra Finita
### Introdu√ß√£o
Como vimos anteriormente, a constru√ß√£o de previs√µes √≥timas para s√©ries temporais envolve projetar o valor futuro de uma s√©rie em seu hist√≥rico passado [^4.1]. Quando o n√∫mero de observa√ß√µes √© infinito, podemos calcular as proje√ß√µes usando esperan√ßas condicionais [^4.2]. No entanto, quando lidamos com amostras finitas, precisamos de m√©todos que nos permitam obter previs√µes precisas com base em um n√∫mero limitado de observa√ß√µes. Esta se√ß√£o explora como construir previs√µes exatas de amostra finita, utilizando uma abordagem baseada na proje√ß√£o linear e nas propriedades das matrizes de autocovari√¢ncia.

### Conceitos Fundamentais
A constru√ß√£o de previs√µes exatas de amostra finita se baseia na proje√ß√£o do valor futuro da s√©rie ($Y_{t+1}$) em seus $m$ valores mais recentes ($Y_t, Y_{t-1}, ..., Y_{t-m+1}$) [^4.3]. O objetivo √© encontrar um forecast linear que minimize o erro quadr√°tico m√©dio (MSE). Matematicamente, buscamos uma previs√£o da forma [^4.3.5]:
$$ \alpha^{(m)'}X_t = \alpha_0^{(m)} + \alpha_1^{(m)}Y_t + \alpha_2^{(m)}Y_{t-1} + \ldots + \alpha_m^{(m)}Y_{t-m+1} $$
onde $X_t$ √© um vetor que cont√©m os $m$ valores passados da s√©rie, junto com uma constante. O coeficiente $\alpha_i^{(m)}$ representa a influ√™ncia do valor $Y_{t-i+1}$ na previs√£o de $Y_{t+1}$. √â importante notar que, em geral, o coeficiente $\alpha_i^{(m)}$ ser√° diferente se usarmos $m+1$ valores passados em vez de $m$ [^4.3.5].

Para calcular os coeficientes $\alpha^{(m)}$, definimos a matriz de autocovari√¢ncia $\Omega$ dos valores passados da s√©rie e usamos a seguinte f√≥rmula [^4.3.6]:
$$ \alpha^{(m)'} = [\mu, (\gamma_1 + \mu^2), (\gamma_2 + \mu^2), \ldots, (\gamma_m + \mu^2)] \begin{bmatrix}
\mu & \mu & \mu & \cdots & \mu \\
\gamma_0 + \mu^2 & \gamma_1 + \mu^2 & \gamma_2 + \mu^2 & \cdots & \gamma_{m-1} + \mu^2 \\
\gamma_1 + \mu^2 & \gamma_0 + \mu^2 & \gamma_1 + \mu^2 & \cdots & \gamma_{m-2} + \mu^2 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\gamma_{m-1} + \mu^2 & \gamma_{m-2} + \mu^2 & \gamma_{m-3} + \mu^2 & \cdots & \gamma_0 + \mu^2
\end{bmatrix}^{-1} $$

onde $\mu$ √© a m√©dia da s√©rie e $\gamma_i$ √© a autocovari√¢ncia no lag $i$. Alternativamente, se considerarmos o vetor de vari√°veis em desvios da m√©dia, o c√°lculo dos coeficientes se simplifica [^4.3.7]. Nesse caso, a previs√£o √© expressa como:
$$ \hat{Y}_{t+1|t} - \mu = \alpha_1^{(m)}(Y_t - \mu) + \alpha_2^{(m)}(Y_{t-1} - \mu) + \ldots + \alpha_m^{(m)}(Y_{t-m+1} - \mu) $$
e os coeficientes podem ser calculados diretamente a partir da matriz de autocovari√¢ncia [^4.3.8]:
$$ [\alpha_1^{(m)}, \alpha_2^{(m)}, \ldots, \alpha_m^{(m)} ] = [\gamma_1, \gamma_2, \ldots, \gamma_m ]  \begin{bmatrix}
\gamma_0 & \gamma_1 & \cdots & \gamma_{m-1} \\
\gamma_1 & \gamma_0 & \cdots & \gamma_{m-2} \\
\vdots & \vdots & \ddots & \vdots \\
\gamma_{m-1} & \gamma_{m-2} & \cdots & \gamma_0
\end{bmatrix}^{-1} $$

> üí° **Exemplo Num√©rico:**
> Suponha que temos uma s√©rie temporal com os seguintes valores de autocovari√¢ncia e m√©dia: $\mu = 2$, $\gamma_0 = 5$, $\gamma_1 = 2$, $\gamma_2 = 1$, e queremos fazer uma previs√£o usando os dois valores passados ($m=2$).  O vetor de autocovari√¢ncia √© $[\gamma_1, \gamma_2] = [2, 1]$. A matriz de autocovari√¢ncia para $m=2$ √©:
> $$ \Omega_2 = \begin{bmatrix} \gamma_0 & \gamma_1 \\ \gamma_1 & \gamma_0 \end{bmatrix} = \begin{bmatrix} 5 & 2 \\ 2 & 5 \end{bmatrix} $$
>  Calculando a inversa da matriz:
> $$ \Omega_2^{-1} = \frac{1}{(5*5 - 2*2)} \begin{bmatrix} 5 & -2 \\ -2 & 5 \end{bmatrix} = \frac{1}{21} \begin{bmatrix} 5 & -2 \\ -2 & 5 \end{bmatrix} $$
> Os coeficientes $\alpha^{(2)}$ s√£o:
> $$ [\alpha_1^{(2)}, \alpha_2^{(2)}] = [2, 1] \frac{1}{21} \begin{bmatrix} 5 & -2 \\ -2 & 5 \end{bmatrix} = \frac{1}{21} [2*5 + 1*(-2), 2*(-2) + 1*5] = \frac{1}{21} [8, 1] \approx [0.38, 0.047] $$
> Assim, a previs√£o para $Y_{t+1}$ seria:
> $$ \hat{Y}_{t+1|t} = 2 + 0.38(Y_t - 2) + 0.047(Y_{t-1} - 2) $$
> Isso significa que o valor previsto de $Y_{t+1}$ √© uma combina√ß√£o ponderada dos valores passados, com um peso maior no valor mais recente ($Y_t$).

Para gerar previs√µes de $s$ per√≠odos √† frente, podemos usar uma generaliza√ß√£o da equa√ß√£o acima, que envolve o uso de matrizes de autocovari√¢ncia e um vetor de autocovari√¢ncias com um horizonte de previs√£o de $s$ [^4.3.9]:
$$ \hat{Y}_{t+s|t} = \mu + \alpha_1^{(m,s)}(Y_t - \mu) + \alpha_2^{(m,s)}(Y_{t-1} - \mu) + \ldots + \alpha_m^{(m,s)}(Y_{t-m+1} - \mu) $$

onde:
$$ [\alpha_1^{(m,s)}, \alpha_2^{(m,s)}, \ldots, \alpha_m^{(m,s)} ] = [\gamma_s, \gamma_{s+1}, \ldots, \gamma_{s+m-1} ]  \begin{bmatrix}
\gamma_0 & \gamma_1 & \cdots & \gamma_{m-1} \\
\gamma_1 & \gamma_0 & \cdots & \gamma_{m-2} \\
\vdots & \vdots & \ddots & \vdots \\
\gamma_{m-1} & \gamma_{m-2} & \cdots & \gamma_0
\end{bmatrix}^{-1} $$

> üí° **Exemplo Num√©rico:**
> Continuando o exemplo anterior, suponha que queremos fazer uma previs√£o de dois per√≠odos √† frente ($s=2$). Precisamos do vetor $[\gamma_2, \gamma_3]$. Como $\gamma_3 = 0$, o vetor √© $[\gamma_2, \gamma_3] = [1, 0]$. Usando a mesma matriz de autocovari√¢ncia $\Omega_2$ e sua inversa, temos:
> $$ [\alpha_1^{(2,2)}, \alpha_2^{(2,2)}] = [1, 0] \frac{1}{21} \begin{bmatrix} 5 & -2 \\ -2 & 5 \end{bmatrix} = \frac{1}{21} [1*5 + 0*(-2), 1*(-2) + 0*5] = \frac{1}{21} [5, -2] \approx [0.238, -0.095] $$
> A previs√£o para $Y_{t+2}$ seria:
> $$ \hat{Y}_{t+2|t} = 2 + 0.238(Y_t - 2) - 0.095(Y_{t-1} - 2) $$
> Note que os coeficientes para a previs√£o de 2 per√≠odos √† frente s√£o diferentes dos coeficientes de 1 per√≠odo √† frente, e que o valor de $Y_{t-1}$ tem um efeito negativo sobre a previs√£o.

**Observa√ß√£o 1:** A estrutura Toeplitz da matriz de autocovari√¢ncia √© crucial para a efici√™ncia computacional dos m√©todos apresentados. Essa estrutura permite o uso de algoritmos especializados que reduzem o custo de invers√£o da matriz, tornando o c√°lculo das previs√µes vi√°vel mesmo para valores de 'm' razoavelmente grandes.
### O Papel da Fatora√ß√£o Triangular
O c√°lculo direto dos coeficientes $\alpha^{(m)}$ envolve a invers√£o de uma matriz de autocovari√¢ncia, o que pode ser computacionalmente caro para $m$ grande. Felizmente, a fatora√ß√£o triangular da matriz de autocovari√¢ncia oferece uma forma eficiente de realizar esse c√°lculo [^4.4]. A fatora√ß√£o triangular de uma matriz sim√©trica definida positiva $\Omega$ permite escrever:
$$ \Omega = ADA' $$
onde $A$ √© uma matriz triangular inferior com 1s na diagonal principal, e $D$ √© uma matriz diagonal com elementos positivos na diagonal [^4.4.1]. Essa fatora√ß√£o √© √∫nica, e existem algoritmos para calcul√°-la, como o m√©todo de Cholesky [^4.4].

A fatora√ß√£o triangular nos permite calcular os coeficientes da proje√ß√£o linear de maneira recursiva, como demostrado em [^4.5.6]. Podemos usar a transforma√ß√£o $ \tilde{Y} = A^{-1}Y $ para decorrelacionar as componentes de $Y$. O uso da fatora√ß√£o triangular permite obter a matriz de proje√ß√£o que √© necess√°ria para obter as predi√ß√µes √≥timas, e, nesse sentido, tamb√©m est√° relacionada a resolu√ß√£o de problemas de m√≠nimos quadrados [^4.5]. O MSE das predi√ß√µes pode ser obtido usando a matriz diagonal D que surge na fatora√ß√£o triangular [^4.5.13].

**Lema 1**
A matriz $A$ obtida na decomposi√ß√£o $\Omega = ADA'$ pode ser usada para calcular os coeficientes da proje√ß√£o linear de forma recursiva. Isto √©, se $\alpha^{(m)}$ s√£o os coeficientes de proje√ß√£o usando m observa√ß√µes passadas e $A_m$ e $D_m$ s√£o as matrizes obtidas com m observa√ß√µes, os coeficientes $\alpha^{(m+1)}$ podem ser obtidos a partir de $\alpha^{(m)}$ e os elementos das matrizes $A_{m+1}$ e $D_{m+1}$.

*Prova*:
Para provar este lema, vamos mostrar como os coeficientes de proje√ß√£o podem ser atualizados recursivamente.
I. Seja $\Omega_m$ a matriz de autocovari√¢ncia usando $m$ observa√ß√µes passadas, e seja sua fatora√ß√£o triangular dada por $\Omega_m = A_m D_m A_m'$. Os coeficientes de proje√ß√£o s√£o ent√£o dados por $\alpha^{(m)} = \Omega_m^{-1} \gamma_m$, onde $\gamma_m = [\gamma_1, \gamma_2, \ldots, \gamma_m]'$.

II. Ao adicionar uma nova observa√ß√£o, a matriz de autocovari√¢ncia se torna $\Omega_{m+1}$, e podemos escrever sua fatora√ß√£o triangular como $\Omega_{m+1} = A_{m+1} D_{m+1} A_{m+1}'$. As matrizes $A_{m+1}$ e $D_{m+1}$ s√£o obtidas a partir de $A_m$ e $D_m$ com a adi√ß√£o de uma nova linha e coluna.

III. Como $A$ √© uma matriz triangular inferior com 1s na diagonal, sua inversa $A^{-1}$ tamb√©m √© triangular inferior e pode ser calculada recursivamente. Denotemos $A_m^{-1}$ por $B_m$. Ent√£o, $B_m A_m = I$.

IV. A nova matriz $A_{m+1}$ tem a forma
$$ A_{m+1} = \begin{bmatrix} A_m & 0 \\ a' & 1 \end{bmatrix} $$
onde $a$ √© um vetor de $m$ elementos. Portanto, a inversa de $A_{m+1}$ pode ser escrita como:
$$ A_{m+1}^{-1} = \begin{bmatrix} A_m^{-1} & 0 \\ -a'A_m^{-1} & 1 \end{bmatrix} $$

V. Similarmente, a matriz $D$ √© diagonal, e a matriz $D_{m+1}$ pode ser atualizada adicionando um novo elemento na diagonal.

VI. Os novos coeficientes $\alpha^{(m+1)}$ podem ser calculados usando $\Omega_{m+1}^{-1}$ e o vetor de autocovari√¢ncias correspondente, ou seja, $\alpha^{(m+1)} = \Omega_{m+1}^{-1} \gamma_{m+1}$.

VII. Como $\Omega_{m+1} = A_{m+1} D_{m+1} A_{m+1}'$, temos $\Omega_{m+1}^{-1} = (A_{m+1}')^{-1} D_{m+1}^{-1} A_{m+1}^{-1}$. Utilizando as express√µes para $A_{m+1}^{-1}$ e a atualiza√ß√£o de $D$, podemos computar $\alpha^{(m+1)}$ recursivamente sem precisar calcular a inversa de $\Omega_{m+1}$ diretamente.

VIII. Os elementos da nova linha da matriz $A$ e o novo elemento diagonal de $D$ podem ser calculados a partir dos elementos da matriz de autocovari√¢ncia original.

Este processo prova que os coeficientes $\alpha^{(m+1)}$ podem ser obtidos recursivamente a partir de $\alpha^{(m)}$ e dos elementos de $A_{m+1}$ e $D_{m+1}$. ‚ñ†

**Corol√°rio 1.1**  A fatora√ß√£o triangular tamb√©m pode ser usada para calcular o erro de previs√£o recursivamente. O erro de previs√£o de $Y_{t+1}$ usando m observa√ß√µes passadas est√° relacionado com os elementos da matriz $D_m$. Ao adicionar uma nova observa√ß√£o e obter $D_{m+1}$, √© poss√≠vel obter o novo erro de previs√£o de forma eficiente.

*Prova*:
Para provar este corol√°rio, vamos mostrar como o erro de previs√£o pode ser atualizado recursivamente usando a matriz D obtida na fatora√ß√£o triangular.
I. O erro de previs√£o de $Y_{t+1}$ usando $m$ observa√ß√µes passadas √© dado por $e_{t+1|t}^{(m)} = Y_{t+1} - \hat{Y}_{t+1|t}^{(m)}$. O erro quadr√°tico m√©dio (MSE) associado a esta previs√£o √© $MSE_m = E[(e_{t+1|t}^{(m)})^2]$.

II. A matriz $\Omega_m$ pode ser escrita como $\Omega_m = A_m D_m A_m'$, onde $D_m$ √© uma matriz diagonal com elementos positivos. A vari√¢ncia do erro de previs√£o est√° relacionada com os elementos da matriz $D_m$.

III.  Especificamente, o MSE da previs√£o √© dado pelo elemento da diagonal principal de $D_m$, que denotamos por $d_{m}$. Isto √©, $MSE_m = d_m$. Onde $d_m$ √© o √∫ltimo elemento diagonal da matriz $D_m$.

IV. Ao adicionar uma nova observa√ß√£o, a matriz de autocovari√¢ncia se torna $\Omega_{m+1}$, e sua fatora√ß√£o triangular √© dada por $\Omega_{m+1} = A_{m+1} D_{m+1} A_{m+1}'$. A nova matriz diagonal $D_{m+1}$ cont√©m um novo elemento diagonal, $d_{m+1}$.

V. O MSE associado a esta nova previs√£o, $MSE_{m+1}$, √© dado por o √∫ltimo elemento diagonal de $D_{m+1}$, $d_{m+1}$. Isto √©, $MSE_{m+1} = d_{m+1}$.

VI. Dado que as matrizes $D_m$ e $D_{m+1}$ s√£o obtidas na fatora√ß√£o triangular de $\Omega_m$ e $\Omega_{m+1}$, o c√°lculo do novo MSE, $MSE_{m+1}$ pode ser feito usando a informa√ß√£o dispon√≠vel de $D_m$ e dos novos elementos que aparecem em $A_{m+1}$.

VII. Portanto, ao adicionar uma nova observa√ß√£o, n√£o √© necess√°rio calcular todo o MSE novamente, pois ele √© obtido diretamente do novo elemento diagonal de $D_{m+1}$.

Assim, a fatora√ß√£o triangular permite atualizar o erro de previs√£o recursivamente, usando os elementos das matrizes $A$ e $D$ obtidas.‚ñ†

### Aplica√ß√£o em um Processo MA(1)
Para ilustrar o processo, considere a previs√£o de um processo MA(1) [^4.5.17]:
$$ Y_t = \mu + \epsilon_t + \theta\epsilon_{t-1} $$
onde $\epsilon_t$ √© um ru√≠do branco com vari√¢ncia $\sigma^2$. A matriz de autocovari√¢ncia $\Omega$ desse processo tem uma estrutura espec√≠fica [^4.5.17]. Ao aplicar a fatora√ß√£o triangular, obtemos matrizes $A$ e $D$ que nos permitem calcular os coeficientes da proje√ß√£o e a vari√¢ncia do erro de previs√£o de maneira precisa [^4.5.18], [^4.5.19]. As equa√ß√µes [4.5.20] e [4.5.21] mostram como usar a fatora√ß√£o triangular para obter o forecast ideal de amostra finita e o seu MSE associado.

> üí° **Exemplo Num√©rico:**
> Considere um processo MA(1) com $\mu = 5$, $\theta = 0.8$ e $\sigma^2 = 1$. Ent√£o, temos:
> - $\gamma_0 = (1 + 0.8^2) * 1 = 1.64$
> - $\gamma_1 = 0.8 * 1 = 0.8$
> - $\gamma_k = 0$ para $k > 1$
>
> Vamos calcular as previs√µes usando $m=2$.  A matriz de autocovari√¢ncia para $m=2$ √©:
> $$ \Omega_2 = \begin{bmatrix} 1.64 & 0.8 \\ 0.8 & 1.64 \end{bmatrix} $$
>
> A fatora√ß√£o de Cholesky $\Omega_2 = L L^T$ resulta em:
>
> $L = \begin{bmatrix}
>  1.2806 & 0 \\
>   0.6247 & 1.1804
> \end{bmatrix}$
>
>  Podemos reescrever isso como $L = A D^{1/2}$:
>
> $D = \begin{bmatrix}
>  1.64 & 0 \\
>   0 & 1.3934
> \end{bmatrix}$
>
>  e $A= \begin{bmatrix}
>  1 & 0 \\
>   0.48 & 1
> \end{bmatrix}$
>
> Os coeficientes da proje√ß√£o para $m=2$ s√£o calculados como:
>
> $[\alpha_1^{(2)}, \alpha_2^{(2)}] =  [\gamma_1, \gamma_2] \Omega_2^{-1} = [0.8, 0] \begin{bmatrix}
>  0.7374 & -0.3585 \\
>   -0.3585 & 0.7374
> \end{bmatrix} = [0.59, -0.286]$
>
> A previs√£o de um passo a frente √©:
> $$ \hat{Y}_{t+1|t} = 5 + 0.59(Y_t - 5) -0.286(Y_{t-1} - 5) $$
>
>  O MSE associado com essa previs√£o √©  o √∫ltimo elemento diagonal da matriz $D$ resultante da fatora√ß√£o triangular, que √©  1.3934.
>
> Para $m=1$, a matriz de autocovari√¢ncia √© apenas $\Omega_1 = [1.64]$, e $A_1 = [1]$, $D_1 = [1.64]$. O coeficiente $\alpha_1^{(1)}$ √©:
> $$ \alpha_1^{(1)} = \frac{\gamma_1}{\gamma_0} = \frac{0.8}{1.64} \approx 0.488 $$
> E a previs√£o √©:
> $$ \hat{Y}_{t+1|t} = 5 + 0.488(Y_t - 5) $$
> O MSE para $m=1$ √© $d_1 = 1.64$. Note que o MSE diminui quando adicionamos uma observa√ß√£o passada, o que indica que a previs√£o com $m=2$ √© mais precisa.

**Teorema 1**  Para um processo MA(1), a matriz de autocovari√¢ncia $\Omega$ √© Toeplitz, e ao usar a fatora√ß√£o triangular, as matrizes A e D possuem uma estrutura que permite uma atualiza√ß√£o recursiva dos coeficientes de proje√ß√£o e do erro de previs√£o.
*Prova*:
Para provar este teorema, vamos mostrar que para um processo MA(1), a matriz de autocovari√¢ncia tem uma estrutura Toeplitz, e como essa estrutura se reflete nas matrizes $A$ e $D$ obtidas pela fatora√ß√£o triangular.
I. Para um processo MA(1), $Y_t = \mu + \epsilon_t + \theta\epsilon_{t-1}$, onde $\epsilon_t$ √© um ru√≠do branco com vari√¢ncia $\sigma^2$. A fun√ß√£o de autocovari√¢ncia $\gamma_k$ √© dada por:
    - $\gamma_0 = Var(Y_t) = E[(Y_t - \mu)^2] = E[(\epsilon_t + \theta\epsilon_{t-1})^2] = (1+\theta^2)\sigma^2$
    - $\gamma_1 = Cov(Y_t, Y_{t-1}) = E[(Y_t - \mu)(Y_{t-1} - \mu)] = E[(\epsilon_t + \theta\epsilon_{t-1})(\epsilon_{t-1} + \theta\epsilon_{t-2})] = \theta\sigma^2$
    - $\gamma_k = 0$ para $k>1$

II. A matriz de autocovari√¢ncia $\Omega$ para um processo MA(1) usando $m$ observa√ß√µes passadas tem a forma:
$$ \Omega_m = \begin{bmatrix}
\gamma_0 & \gamma_1 & 0 & \cdots & 0 \\
\gamma_1 & \gamma_0 & \gamma_1 & \cdots & 0 \\
0 & \gamma_1 & \gamma_0 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & \gamma_0
\end{bmatrix} $$
Esta matriz $\Omega_m$ √© Toeplitz, pois os elementos ao longo de cada diagonal s√£o iguais.

III. Ao aplicar a fatora√ß√£o triangular $\Omega_m = A_m D_m A_m'$, a matriz $A_m$ ter√° a forma:
$$ A_m = \begin{bmatrix}
1 & 0 & 0 & \cdots & 0 \\
a_{21} & 1 & 0 & \cdots & 0 \\
0 & a_{32} & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1
\end{bmatrix} $$
onde $a_{i,i-1}$ s√£o os elementos abaixo da diagonal principal e $a_{ij} = 0$ se $j < i-1$. Estes elementos s√£o fun√ß√µes de $\theta$ e $\sigma^2$. A estrutura de $A$ reflete a estrutura do processo MA(1), com elementos n√£o nulos apenas na primeira subdiagonal.

IV. A matriz $D_m$ √© diagonal com elementos $d_i$ na diagonal, que tamb√©m s√£o fun√ß√µes de $\theta$ e $\sigma^2$.

V. A atualiza√ß√£o recursiva de $A$ e $D$ √© facilitada pela estrutura de Toeplitz de $\Omega$. Ao adicionar uma nova observa√ß√£o, as novas matrizes $A_{m+1}$ e $D_{m+1}$ podem ser obtidas a partir de $A_m$ e $D_m$ sem a necessidade de recalcular toda a fatora√ß√£o triangular do zero. A atualiza√ß√£o recursiva aproveita a estrutura simples e regular dessas matrizes.

VI. Portanto, a estrutura de Toeplitz da matriz de autocovari√¢ncia $\Omega$ de um processo MA(1) leva a uma estrutura espec√≠fica nas matrizes $A$ e $D$ obtidas pela fatora√ß√£o triangular, que permite a atualiza√ß√£o recursiva dos coeficientes de proje√ß√£o e do erro de previs√£o.

Este resultado prova que para um processo MA(1), a estrutura da matriz de autocovari√¢ncia se propaga para as matrizes $A$ e $D$ ap√≥s a fatora√ß√£o triangular, permitindo a atualiza√ß√£o recursiva dos coeficientes de proje√ß√£o e do erro de previs√£o. ‚ñ†

### Conclus√£o
Neste cap√≠tulo, exploramos como construir previs√µes exatas de amostra finita, com foco no conceito de proje√ß√£o linear e no uso da fatora√ß√£o triangular de matrizes de autocovari√¢ncia.  A abordagem apresentada √© fundamental para a an√°lise de s√©ries temporais quando o n√∫mero de observa√ß√µes √© limitado e oferece uma base s√≥lida para a constru√ß√£o de previs√µes em diversos contextos [^4.5]. A capacidade de calcular previs√µes exatas com um n√∫mero limitado de observa√ß√µes √© crucial para a aplica√ß√£o pr√°tica da an√°lise de s√©ries temporais, particularmente em econometria e outras √°reas onde os dados podem ser escassos. Os m√©todos apresentados aqui fornecem uma base s√≥lida para a an√°lise de s√©ries temporais, permitindo que analistas e pesquisadores construam modelos mais precisos e confi√°veis com base em dados limitados. A pr√≥xima se√ß√£o abordar√° o conceito de lei das proje√ß√µes iteradas, que completa as ferramentas para trabalhar com processos estoc√°sticos.

### Refer√™ncias
[^4.1]:  Express√£o [4.1.1]
[^4.2]:  Express√£o [4.1.2]
[^4.3]: Express√£o [4.1.9]
[^4.3.5]: Express√£o [4.3.5]
[^4.3.6]: Express√£o [4.3.6]
[^4.3.7]: Express√£o [4.3.7]
[^4.3.8]: Express√£o [4.3.8]
[^4.3.9]: Express√£o [4.3.9]
[^4.4]: Express√£o [4.4.1]
[^4.4.1]: Express√£o [4.4.1]
[^4.5.6]: Express√£o [4.5.6]
[^4.5]: Express√£o [4.5]
[^4.5.13]: Express√£o [4.5.13]
[^4.5.17]: Express√£o [4.5.17]
[^4.5.18]: Express√£o [4.5.18]
[^4.5.19]: Express√£o [4.5.19]
[^4.5.20]: Express√£o [4.5.20]
[^4.5.21]: Express√£o [4.5.21]
<!-- END -->
