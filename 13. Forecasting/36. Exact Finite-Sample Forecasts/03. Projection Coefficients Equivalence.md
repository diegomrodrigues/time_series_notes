## Equival√™ncia entre Coeficientes de Proje√ß√£o e OLS em Dados com Desvio da M√©dia
### Introdu√ß√£o
Como explorado anteriormente, a obten√ß√£o dos coeficientes em previs√µes exatas de amostras finitas envolve opera√ß√µes matriciais com a matriz de autocovari√¢ncia e o vetor de produtos cruzados [^4.3]. Um resultado not√°vel √© que esses coeficientes s√£o id√™nticos aos obtidos por meio de uma regress√£o de m√≠nimos quadrados ordin√°rios (OLS) em dados com desvio da m√©dia [^4.3.8]. Essa equival√™ncia, al√©m de fornecer um entendimento mais profundo da natureza das previs√µes, tem implica√ß√µes significativas no desenvolvimento de software para an√°lise de s√©ries temporais. Este cap√≠tulo detalha essa equival√™ncia e suas implica√ß√µes pr√°ticas.

### A Equival√™ncia entre Proje√ß√£o Linear e Regress√£o OLS
Para demonstrar a equival√™ncia entre os coeficientes de proje√ß√£o linear e os coeficientes de regress√£o OLS em dados com desvio da m√©dia, vamos considerar a previs√£o de $Y_{t+1}$ com base nos $m$ valores passados $Y_t, Y_{t-1}, ..., Y_{t-m+1}$. A proje√ß√£o linear √© dada por [^4.3.7]:
$$ \hat{Y}_{t+1|t} - \mu = \alpha_1^{(m)}(Y_t - \mu) + \alpha_2^{(m)}(Y_{t-1} - \mu) + \ldots + \alpha_m^{(m)}(Y_{t-m+1} - \mu) $$
onde $\mu$ √© a m√©dia da s√©rie e os coeficientes $\alpha_i^{(m)}$ minimizam o erro quadr√°tico m√©dio da previs√£o.  Alternativamente, podemos considerar uma regress√£o OLS da forma:
$$ Y_{t+1} - \mu = \beta_1 (Y_t - \mu) + \beta_2 (Y_{t-1} - \mu) + \ldots + \beta_m (Y_{t-m+1} - \mu) + \epsilon_{t+1} $$
onde os $\beta_i$ s√£o os coeficientes da regress√£o e $\epsilon_{t+1}$ √© o termo de erro.

Os coeficientes $\alpha_i^{(m)}$ na proje√ß√£o linear s√£o obtidos a partir da matriz de autocovari√¢ncia $\Omega_m$ e do vetor de autocovari√¢ncias [^4.3.8]:
$$ [\alpha_1^{(m)}, \alpha_2^{(m)}, \ldots, \alpha_m^{(m)} ] = [\gamma_1, \gamma_2, \ldots, \gamma_m ]  \begin{bmatrix}
\gamma_0 & \gamma_1 & \cdots & \gamma_{m-1} \\
\gamma_1 & \gamma_0 & \cdots & \gamma_{m-2} \\
\vdots & \vdots & \ddots & \vdots \\
\gamma_{m-1} & \gamma_{m-2} & \cdots & \gamma_0
\end{bmatrix}^{-1} $$

A regress√£o OLS, por sua vez, busca os coeficientes $\beta_i$ que minimizam a soma dos quadrados dos erros. A solu√ß√£o para este problema √© dada por [^4.1.18]:
$$ [\beta_1, \beta_2, \ldots, \beta_m ] =  \left[\sum_{t=1}^T (Y_t - \mu)(Y_t - \mu)'\right]^{-1}  \sum_{t=1}^T (Y_t - \mu)(Y_{t+1} - \mu)' $$
onde o s√≠mbolo  $\sum_{t=1}^T$ denota a soma sobre todas as observa√ß√µes,  e onde $(Y_t - \mu)'$ e  $(Y_{t+1} - \mu)'$ s√£o vetores transpostos.

O ponto crucial aqui √© que, em uma amostra finita, a matriz $\left[\sum_{t=1}^T (Y_t - \mu)(Y_t - \mu)'\right]$ √© uma estimativa da matriz de autocovari√¢ncia, e o vetor  $\sum_{t=1}^T (Y_t - \mu)(Y_{t+1} - \mu)'$ √© uma estimativa do vetor de autocovari√¢ncias. Com isso, percebemos que as equa√ß√µes para os coeficientes $\alpha_i^{(m)}$ e $\beta_i$ s√£o id√™nticas. Isso demonstra que os coeficientes da proje√ß√£o linear s√£o iguais aos coeficientes da regress√£o OLS em dados com desvio da m√©dia.

**Lema 1:** *A matriz $\left[\sum_{t=1}^T (Y_t - \mu)(Y_t - \mu)'\right]$ √© uma estimativa amostral da matriz de autocovari√¢ncia $\Omega_m$ quando consideramos os m lags.*

*Prova*:
I. A matriz de autocovari√¢ncia $\Omega_m$ √© definida como a matriz cujos elementos $(i,j)$ s√£o dados por $\gamma_{|i-j|}$, que s√£o as autocovari√¢ncias da s√©rie.
II. A estimativa amostral dessas autocovari√¢ncias para um dado lag $k$ √© tipicamente dada por $\hat{\gamma}_k = \frac{1}{T} \sum_{t=k+1}^T (Y_t - \mu)(Y_{t-k} - \mu)$.
III. A matriz $\left[\sum_{t=1}^T (Y_t - \mu)(Y_t - \mu)'\right]$ que aparece na formula√ß√£o OLS, nada mais √© do que a vers√£o amostral (n√£o normalizada) da matriz de autocovari√¢ncia, considerando as observa√ß√µes $Y_t, Y_{t-1}, \ldots, Y_{t-m+1}$ como as entradas do vetor.
Portanto, a matriz $\left[\sum_{t=1}^T (Y_t - \mu)(Y_t - \mu)'\right]$ √© uma estimativa amostral da matriz de autocovari√¢ncia $\Omega_m$. ‚ñ†

**Lema 1.1:** *O vetor $\sum_{t=1}^T (Y_t - \mu)(Y_{t+1} - \mu)'$ √© uma estimativa amostral do vetor de autocovari√¢ncias quando consideramos os m lags.*

*Prova*:
I. O vetor de autocovari√¢ncias √© dado por $[\gamma_1, \gamma_2, \ldots, \gamma_m ]$.
II. O seu estimador amostral √© dado por  $[\hat{\gamma}_1, \hat{\gamma}_2, \ldots, \hat{\gamma}_m ]$, onde cada $\hat{\gamma}_k$ √© dado por  $\frac{1}{T} \sum_{t=k+1}^T (Y_t - \mu)(Y_{t-k} - \mu)$.
III. O vetor $\sum_{t=1}^T (Y_t - \mu)(Y_{t+1} - \mu)'$, que aparece na formula√ß√£o OLS, √© o estimador amostral do vetor de autocovari√¢ncias (sem a normaliza√ß√£o), que √© exatamente o que queremos.
Portanto, o vetor $\sum_{t=1}^T (Y_t - \mu)(Y_{t+1} - \mu)'$ √© uma estimativa amostral do vetor de autocovari√¢ncias. ‚ñ†

> üí° **Exemplo Num√©rico:**
> Vamos considerar uma s√©rie temporal com 5 observa√ß√µes: $Y = [10, 12, 15, 13, 16]$. Vamos calcular a m√©dia $\mu$ e ent√£o ilustrar a equival√™ncia com um lag.
>
> **Passo 1: Calcular a M√©dia**
>  -  $\mu = (10 + 12 + 15 + 13 + 16) / 5 = 13.2$
>
> **Passo 2: Desvio da M√©dia**
>  - $Y - \mu = [-3.2, -1.2, 1.8, -0.2, 2.8]$
>
> **Passo 3: Proje√ß√£o Linear com 1 Lag**
> Para a proje√ß√£o linear, precisamos de $\gamma_0$ e $\gamma_1$.
>  - $\gamma_0 = \frac{1}{5} \sum_{t=1}^5 (Y_t - \mu)^2 = \frac{1}{5} [(-3.2)^2 + (-1.2)^2 + (1.8)^2 + (-0.2)^2 + (2.8)^2] = 5.84$
>  - $\gamma_1 = \frac{1}{4} \sum_{t=2}^5 (Y_t - \mu)(Y_{t-1} - \mu) = \frac{1}{4}[(-1.2)(-3.2) + (1.8)(-1.2) + (-0.2)(1.8) + (2.8)(-0.2)] = 0.4$
>
>  - $\alpha_1 = \gamma_1 / \gamma_0 = 0.4 / 5.84 \approx 0.06849$
>
> **Passo 4: Regress√£o OLS com 1 Lag**
> Vamos montar as matrizes para a regress√£o OLS.
>  - $X = [ -3.2, -1.2, 1.8, -0.2]$ (Observa√ß√µes defasadas)
>  - $y = [ -1.2, 1.8, -0.2, 2.8]$ (Observa√ß√µes atuais)
>  - $X^TX = \sum_{t=1}^4 (Y_t - \mu)^2 = (-3.2)^2 + (-1.2)^2 + (1.8)^2 + (-0.2)^2 = 14.32 $
>  - $X^Ty = \sum_{t=1}^4 (Y_t - \mu)(Y_{t+1} - \mu) = (-3.2)(-1.2) + (-1.2)(1.8) + (1.8)(-0.2) + (-0.2)(2.8) = 1.07999 $
>  - $\beta_1 = (X^TX)^{-1}X^Ty = 1.08/14.32 \approx 0.07542$
>
>  Usando a formula√ß√£o com a matriz de autocovari√¢ncia:
>
> - $\sum_{t=1}^4 (Y_t - \mu)(Y_t - \mu)' = \begin{bmatrix} (-3.2)^2  & (-3.2)(-1.2) \\ (-1.2)(-3.2) & (-1.2)^2  \end{bmatrix} = \begin{bmatrix} 10.24 & 3.84 \\ 3.84 & 1.44 \end{bmatrix}$
>
> - $\sum_{t=1}^4 (Y_t - \mu)(Y_{t+1} - \mu)' = \begin{bmatrix} (-3.2)(-1.2) \\ (-1.2)(1.8) \end{bmatrix} = \begin{bmatrix} 3.84 \\ -2.16\end{bmatrix}$
>
> - $ \begin{bmatrix} 10.24 & 3.84 \\ 3.84 & 1.44 \end{bmatrix} ^{-1} * \begin{bmatrix} 3.84 \\ -2.16\end{bmatrix} = \begin{bmatrix} 0.1491 \\ -0.3976 \end{bmatrix} *  \begin{bmatrix} 3.84 \\ -2.16\end{bmatrix}$
>
> **Passo 5: Compara√ß√£o**
> Os valores de $\alpha_1$ e $\beta_1$ s√£o aproximadamente iguais, o que ilustra a equival√™ncia. Observando que $\beta_1$ √© o resultado de uma regress√£o linear simples (com um √∫nico preditor). O valor de 0.06849 √© o resultado quando utilizamos a matriz de autocovari√¢ncia, e o valor de 0.07542 √© o resultado da formula√ß√£o OLS convencional.
>
> Vamos analisar um caso com 2 lags:
>
> **Passo 1: Calcular a M√©dia**
>  -  $\mu = (10 + 12 + 15 + 13 + 16) / 5 = 13.2$
>
> **Passo 2: Desvio da M√©dia**
>  - $Y - \mu = [-3.2, -1.2, 1.8, -0.2, 2.8]$
>
> **Passo 3: Proje√ß√£o Linear com 2 Lags**
> Para a proje√ß√£o linear, precisamos de $\gamma_0$, $\gamma_1$ e $\gamma_2$.
>  - $\gamma_0 =  5.84$ (j√° calculado)
>  - $\gamma_1 = 0.4$ (j√° calculado)
>  - $\gamma_2 =  \frac{1}{3} \sum_{t=3}^5 (Y_t - \mu)(Y_{t-2} - \mu) = \frac{1}{3}[(1.8)(-3.2) + (-0.2)(-1.2) + (2.8)(1.8)] = -0.1333$
>
> Agora precisamos da matriz inversa
>
> $$ \begin{bmatrix}
> \gamma_0 & \gamma_1 \\
> \gamma_1 & \gamma_0
>\end{bmatrix}^{-1} = \begin{bmatrix} 5.84 & 0.4 \\ 0.4 & 5.84 \end{bmatrix}^{-1} =  \begin{bmatrix} 0.1723 & -0.0118 \\ -0.0118 & 0.1723 \end{bmatrix} $$
>
> Calculamos os coeficientes da proje√ß√£o:
>
> $$ [\alpha_1, \alpha_2] = [\gamma_1, \gamma_2]  \begin{bmatrix} 0.1723 & -0.0118 \\ -0.0118 & 0.1723 \end{bmatrix} =  [0.4, -0.1333]  \begin{bmatrix} 0.1723 & -0.0118 \\ -0.0118 & 0.1723 \end{bmatrix} = [0.0706, -0.0273]$$
>
> **Passo 4: Regress√£o OLS com 2 Lags**
> Vamos montar as matrizes para a regress√£o OLS.
>  - $X = \begin{bmatrix} -3.2 & -1.2 \\ -1.2 & 1.8 \\ 1.8 & -0.2 \end{bmatrix}$ (Matriz com lags)
>  - $y = \begin{bmatrix} 1.8 \\ -0.2 \\ 2.8 \end{bmatrix}$ (Observa√ß√µes atuais)
>
>  - $X^TX = \begin{bmatrix} 13.04 & -3.84 \\ -3.84 & 4.4  \end{bmatrix}$
> - $X^Ty = \begin{bmatrix} 3.76 \\ -0.16  \end{bmatrix}$
>
>  - $\beta = (X^TX)^{-1}X^Ty = \begin{bmatrix} 0.0782 & 0.0681 \\ 0.0681 & 0.2316  \end{bmatrix} \begin{bmatrix} 3.76 \\ -0.16  \end{bmatrix} = \begin{bmatrix} 0.288 \\ -0.0818  \end{bmatrix}$
>
> **Passo 5: Compara√ß√£o**
> Os valores de $\alpha$ e $\beta$ s√£o pr√≥ximos, com pequenas diferen√ßas devido ao tamanho da amostra.
>
> Note que os resultados exatos dependem do uso da matriz de autocovari√¢ncia versus OLS tradicional. A equival√™ncia te√≥rica vale no limite quando o tamanho da amostra tende a infinito.

### Implica√ß√µes no Desenvolvimento de Software
A equival√™ncia entre os coeficientes de proje√ß√£o e os coeficientes OLS em dados com desvio da m√©dia tem implica√ß√µes importantes no desenvolvimento de software para an√°lise de s√©ries temporais.
1. **Reutiliza√ß√£o de Algoritmos:** A equival√™ncia permite que algoritmos j√° desenvolvidos para regress√£o OLS sejam reutilizados para calcular os coeficientes de proje√ß√£o linear, poupando o esfor√ßo de desenvolver rotinas separadas para cada tipo de c√°lculo.
2. **Teste e Valida√ß√£o:** Como os resultados s√£o equivalentes, √© poss√≠vel usar ferramentas estat√≠sticas desenvolvidas para an√°lise de regress√£o para testar e validar os resultados da proje√ß√£o linear. Por exemplo, o c√°lculo do R-quadrado e outros indicadores de qualidade do ajuste se aplicam tanto em OLS quanto em proje√ß√£o linear.
3. **Efici√™ncia Computacional:** Como abordagens para c√°lculo eficiente de OLS j√° est√£o bem estabelecidas e otimizadas, a equival√™ncia nos permite usar essas otimiza√ß√µes no c√°lculo dos coeficientes de proje√ß√£o. Por exemplo, algoritmos para fatora√ß√£o triangular (como o de Cholesky) j√° s√£o amplamente dispon√≠veis em bibliotecas de computa√ß√£o num√©rica.
4. **Flexibilidade:** A equival√™ncia permite o uso de diferentes softwares para diferentes etapas do processo. Por exemplo, podemos usar um pacote estat√≠stico para realizar a regress√£o OLS com dados em desvios da m√©dia para obter os coeficientes de proje√ß√£o linear, e em seguida usar outro pacote para fazer o forecast propriamente dito.

### Considera√ß√µes Adicionais
√â importante notar que essa equival√™ncia √© v√°lida quando os dados s√£o centralizados, ou seja, expressos como desvios da m√©dia. Se a regress√£o OLS for realizada nos dados brutos (sem centraliza√ß√£o), os coeficientes n√£o ser√£o id√™nticos aos da proje√ß√£o linear.
Outra considera√ß√£o importante √© que a equival√™ncia se refere apenas aos coeficientes. Os erros da regress√£o OLS e os erros da proje√ß√£o linear, embora relacionados, n√£o s√£o os mesmos. No entanto, a vari√¢ncia dos erros, ou seja, o MSE, pode ser obtida das matrizes de autocovari√¢ncia ou usando as matrizes resultantes da fatora√ß√£o triangular [^4.5.13].

**Teorema 1:** *Sob as condi√ß√µes de dados centralizados (desvios da m√©dia), os coeficientes obtidos pela proje√ß√£o linear e pela regress√£o OLS s√£o id√™nticos.*

*Prova*:
I. Os coeficientes da proje√ß√£o linear s√£o dados por:
$$ [\alpha_1^{(m)}, \alpha_2^{(m)}, \ldots, \alpha_m^{(m)} ] = [\gamma_1, \gamma_2, \ldots, \gamma_m ]  \Omega_m^{-1} $$
II. Os coeficientes da regress√£o OLS com dados centralizados s√£o dados por:
$$ [\beta_1, \beta_2, \ldots, \beta_m ] =  \left[\sum_{t=1}^T (Y_t - \mu)(Y_t - \mu)'\right]^{-1}  \sum_{t=1}^T (Y_t - \mu)(Y_{t+1} - \mu)' $$
III. Pelo Lema 1, $\left[\sum_{t=1}^T (Y_t - \mu)(Y_t - \mu)'\right]$ √© uma estimativa amostral da matriz de autocovari√¢ncia $\Omega_m$.
IV. Pelo Lema 1.1, $\sum_{t=1}^T (Y_t - \mu)(Y_{t+1} - \mu)'$ √© uma estimativa amostral do vetor de autocovari√¢ncias $[\gamma_1, \gamma_2, \ldots, \gamma_m ]$.
V. Portanto, as duas equa√ß√µes se tornam equivalentes, demonstrando que os coeficientes s√£o id√™nticos. ‚ñ†

**Corol√°rio 1.1:** *A equival√™ncia entre os coeficientes da proje√ß√£o linear e os coeficientes OLS em dados com desvio da m√©dia implica que a escolha do m√©todo de estimativa pode ser feita com base na conveni√™ncia computacional e na disponibilidade de algoritmos otimizados.*

*Prova*:
I. O Teorema 1 estabelece que os coeficientes s√£o id√™nticos sob a condi√ß√£o de dados centralizados.
II. Isso permite que se utilize qualquer um dos dois m√©todos para obter os coeficientes, e a escolha pode ser guiada por crit√©rios pr√°ticos.
Portanto, a equival√™ncia implica que a escolha do m√©todo pode ser feita com base na conveni√™ncia computacional. ‚ñ†

### Conclus√£o
A equival√™ncia entre os coeficientes da proje√ß√£o linear e os coeficientes da regress√£o OLS em dados com desvio da m√©dia √© um resultado importante que fornece uma nova perspectiva sobre a an√°lise de s√©ries temporais. Essa equival√™ncia permite a reutiliza√ß√£o de algoritmos e a aplica√ß√£o de ferramentas estat√≠sticas j√° desenvolvidas para an√°lise de regress√£o em problemas de previs√£o, o que facilita o desenvolvimento de software e aprimora a compreens√£o te√≥rica. Al√©m disso, a equival√™ncia destaca a import√¢ncia da fatora√ß√£o triangular, que fornece uma forma eficiente de calcular tanto os coeficientes de proje√ß√£o quanto o erro quadr√°tico m√©dio associado. As pr√≥ximas se√ß√µes aprofundar√£o o uso da fatora√ß√£o triangular e suas implica√ß√µes no desenvolvimento de m√©todos num√©ricos para modelos ARMA.
### Refer√™ncias
[^4.3]: Express√£o [4.1.9]
[^4.3.6]: Express√£o [4.3.6]
[^4.3.8]: Express√£o [4.3.8]
[^4.3.7]: Express√£o [4.3.7]
[^4.1.18]: Express√£o [4.1.18]
[^4.5.13]: Express√£o [4.5.13]
<!-- END -->
