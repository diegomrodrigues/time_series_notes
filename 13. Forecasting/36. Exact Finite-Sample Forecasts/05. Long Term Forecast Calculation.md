## Previs√µes de Longo Prazo em Amostras Finitas: Proje√ß√µes Iteradas, Atualiza√ß√£o de Coeficientes e Estabilidade Num√©rica
### Introdu√ß√£o
Como discutido em se√ß√µes anteriores, a constru√ß√£o de previs√µes em amostras finitas envolve a proje√ß√£o linear do valor futuro de uma s√©rie temporal em seu hist√≥rico passado [^4.3]. Enquanto as previs√µes de curto prazo podem ser obtidas com base em uma proje√ß√£o direta, previs√µes de longo prazo exigem uma abordagem iterativa, onde as previs√µes de um passo s√£o utilizadas como entradas para proje√ß√µes futuras [^4.2.24]. Essa abordagem, baseada na *Lei das Proje√ß√µes Iteradas*, implica um processo de atualiza√ß√£o recursiva dos coeficientes de proje√ß√£o e na propaga√ß√£o dos erros de previs√£o. Este cap√≠tulo explora em profundidade os desafios computacionais e te√≥ricos envolvidos na constru√ß√£o de previs√µes de longo prazo, destacando a import√¢ncia de abordagens num√©ricas computacionalmente eficazes e a necessidade de aten√ß√£o √† estabilidade num√©rica durante a propaga√ß√£o dos erros.

### Proje√ß√µes Iteradas para Previs√µes de Longo Prazo
A Lei das Proje√ß√µes Iteradas, formalizada na se√ß√£o anterior, estabelece um procedimento recursivo para a obten√ß√£o de previs√µes de horizonte m√∫ltiplo [^4.5.32]. Em vez de modelar diretamente a rela√ß√£o entre o valor presente e um valor distante no futuro, a lei possibilita usar a previs√£o de curto prazo como base para a previs√£o subsequente, permitindo, assim, construir uma sequ√™ncia de previs√µes para um horizonte de tempo mais extenso.

Para ilustrar, considere um processo AR(p):
$$ Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t $$
onde $c$ √© uma constante, $\phi_i$ s√£o os coeficientes autorregressivos e $\epsilon_t$ √© o termo de erro. A previs√£o de um passo √† frente √© dada por:
$$ \hat{Y}_{t+1|t} = c + \phi_1 Y_t + \phi_2 Y_{t-1} + \ldots + \phi_p Y_{t-p} $$
Para prever $Y_{t+2}$, podemos iterar a mesma equa√ß√£o, substituindo as observa√ß√µes futuras pelos seus valores projetados:
$$ \hat{Y}_{t+2|t} = c + \phi_1 \hat{Y}_{t+1|t} + \phi_2 Y_t + \ldots + \phi_p Y_{t-p+1} $$
Note que para $t+2$, utilizamos $\hat{Y}_{t+1|t}$ como substituto de $Y_{t+1}$. Esse processo de substitui√ß√£o √© iterado recursivamente at√© o horizonte de previs√£o desejado, permitindo obter previs√µes de longo prazo. Este processo gera uma sequ√™ncia de previs√µes, com cada previs√£o constru√≠da sobre a anterior, de modo que a precis√£o da previs√£o depender√° da precis√£o das previs√µes anteriores, com os erros de proje√ß√£o se propagando ao longo do tempo.

> üí° **Exemplo Num√©rico:**
> Considere um modelo AR(1) com $c = 0$, $\phi_1 = 0.7$. Se $Y_t = 10$, podemos prever os pr√≥ximos dois per√≠odos iterativamente:
>
> **Passo 1:** Prever $Y_{t+1}$:
> $$ \hat{Y}_{t+1|t} = 0.7 * 10 = 7 $$
>
> **Passo 2:** Prever $Y_{t+2}$ usando $\hat{Y}_{t+1|t}$:
> $$ \hat{Y}_{t+2|t} = 0.7 * \hat{Y}_{t+1|t} = 0.7 * 7 = 4.9 $$
>
> Assim, a previs√£o de dois passos √† frente √© 4.9, obtida iterando o processo de previs√£o de um passo.

A Lei das Proje√ß√µes Iteradas n√£o se limita a modelos AR(p). Ela tamb√©m se aplica a modelos MA(q) e ARMA(p,q), permitindo a constru√ß√£o de previs√µes de longo prazo para esses modelos, embora a necessidade de projetar tamb√©m os erros complica um pouco o c√°lculo, e a abordagem da fatora√ß√£o triangular se torna ainda mais vantajosa. **Em particular, para modelos MA(q), as previs√µes de longo prazo convergem para a m√©dia incondicional da s√©rie, dado que os erros passados n√£o t√™m efeito nas proje√ß√µes futuras ap√≥s um n√∫mero suficiente de passos.**

> üí° **Exemplo Num√©rico:**
> Considere um modelo MA(1) com $Y_t = \epsilon_t + 0.5\epsilon_{t-1}$, onde $\epsilon_t$ √© ru√≠do branco com m√©dia zero. Suponha que $\epsilon_t = 2$.
>
> **Passo 1:** Prever $Y_{t+1}$:
>  Como a previs√£o condicional de $\epsilon_{t+1}$ √© 0, temos:
> $$ \hat{Y}_{t+1|t} = 0 + 0.5\epsilon_t = 0.5 * 2 = 1 $$
>
> **Passo 2:** Prever $Y_{t+2}$:
> Novamente, a previs√£o condicional de $\epsilon_{t+2}$ √© 0, e o erro $\epsilon_{t+1}$ tamb√©m n√£o √© mais utilizado na proje√ß√£o, portanto:
> $$ \hat{Y}_{t+2|t} = 0 $$
>
> Para horizontes maiores, a previs√£o tamb√©m ser√° 0, que √© a m√©dia incondicional do processo. Isto ilustra a converg√™ncia para a m√©dia em modelos MA para previs√µes de longo prazo.

### Atualiza√ß√£o Recursiva dos Coeficientes de Proje√ß√£o
Como discutido anteriormente, os coeficientes de proje√ß√£o ($\alpha^{(m)}$) podem ser obtidos utilizando opera√ß√µes matriciais com a matriz de autocovari√¢ncia e o vetor de autocovari√¢ncias [^4.3.8]. No entanto, para previs√µes de longo prazo, √© necess√°rio atualizar esses coeficientes √† medida que novas observa√ß√µes se tornam dispon√≠veis e o horizonte de previs√£o se desloca [^4.5.16]. A fatora√ß√£o triangular da matriz de autocovari√¢ncia $\Omega_m$ desempenha um papel crucial nesse processo, j√° que, atrav√©s dela, podemos atualizar os coeficientes recursivamente sem precisar recalcular toda a matriz de autocovari√¢ncia e sua inversa [Teorema 2].

De acordo com o resultado anterior, a fatora√ß√£o triangular $\Omega_m = ADA'$ permite que se expresse os coeficientes de proje√ß√£o como:
$$ \alpha = A^{-1} D^{-1} (A')^{-1} \gamma $$
onde $\gamma$ √© o vetor de autocovari√¢ncias. A matriz $A$ √© triangular inferior, o que facilita a obten√ß√£o de sua inversa, e $D$ √© diagonal. Ao adicionar uma nova observa√ß√£o, podemos atualizar as matrizes $A$ e $D$ sem ter que refazer a fatora√ß√£o triangular toda desde o in√≠cio, o que resulta em uma economia computacional significativa.

**Teorema 1** A atualiza√ß√£o recursiva dos coeficientes de proje√ß√£o em modelos ARMA, utilizando a fatora√ß√£o triangular, permite calcular as previs√µes de longo prazo de maneira eficiente.

*Prova*:
I. Considere a previs√£o de $Y_{t+s}$ baseada nas observa√ß√µes passadas $Y_t, Y_{t-1}, \ldots, Y_{t-m+1}$, onde $s$ √© o horizonte de previs√£o.

II. Os coeficientes de proje√ß√£o s√£o dados por $\alpha^{(m,s)}$, que dependem da matriz de autocovari√¢ncia $\Omega_m$ e do vetor de autocovari√¢ncias correspondente ao horizonte de previs√£o $s$ [^4.3.9].

III. A fatora√ß√£o triangular da matriz de autocovari√¢ncia $\Omega_m$ √© dada por $\Omega_m = A_m D_m A_m'$. As matrizes $A_m$ e $D_m$ s√£o obtidas de forma recursiva, como demonstrado em cap√≠tulos anteriores.

IV. Quando o horizonte de previs√£o √© atualizado para $s+1$, o vetor de autocovari√¢ncias $\gamma_s$ √© alterado para $\gamma_{s+1}$.

V. A nova fatora√ß√£o triangular $\Omega_{m,s+1}$ pode ser obtida a partir de $\Omega_{m,s}$ com uma pequena atualiza√ß√£o, que se traduz em uma atualiza√ß√£o das matrizes $A$ e $D$, utilizando a estrutura de Toeplitz de $\Omega_m$.
VI. Consequentemente, os novos coeficientes de proje√ß√£o $\alpha^{(m,s+1)}$ podem ser calculados sem a necessidade de recalcular toda a fatora√ß√£o triangular do zero, usando a fatora√ß√£o recursiva.

VII. A abordagem iterativa permite calcular a previs√£o de longo prazo combinando a atualiza√ß√£o dos coeficientes e o uso das previs√µes dos passos anteriores.

VIII. Portanto, a combina√ß√£o da fatora√ß√£o triangular com a Lei das Proje√ß√µes Iteradas permite calcular previs√µes de longo prazo de maneira eficiente.  $\blacksquare$

**Lema 1.1** Em modelos ARMA estacion√°rios, os coeficientes de proje√ß√£o $\alpha^{(m,s)}$ convergem para valores constantes √† medida que o horizonte de previs√£o $s$ tende ao infinito.

*Prova:*
I. Em modelos ARMA estacion√°rios, as autocovari√¢ncias $\gamma_k$ decaem para zero √† medida que $k$ aumenta.
II. Os coeficientes de proje√ß√£o $\alpha^{(m,s)}$ s√£o fun√ß√µes das autocovari√¢ncias, logo, para previs√µes de longo prazo, eles dependem de autocovari√¢ncias para grandes valores de $s$, que s√£o pr√≥ximos de zero.
III. Como as autocovari√¢ncias convergem para zero, os coeficientes $\alpha^{(m,s)}$ convergem para um valor constante √† medida que $s$ tende ao infinito, o que significa que as previs√µes de longo prazo tendem a convergir para a m√©dia da s√©rie temporal.
IV.  Portanto, em modelos estacion√°rios, a propaga√ß√£o de erros tende a se estabilizar em n√≠veis pr√≥ximos de zero para horizontes de previs√£o muito grandes. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Considere um processo AR(2) com $Y_t = 0.8Y_{t-1} - 0.3Y_{t-2} + \epsilon_t$. Suponha que, com base em um conjunto de dados inicial, os coeficientes de proje√ß√£o $\alpha^{(2)} = [0.8, -0.3]$ s√£o estimados usando a fatora√ß√£o triangular da matriz de autocovari√¢ncia. Quando uma nova observa√ß√£o $Y_{t+1}$ √© obtida, a matriz de autocovari√¢ncia e o vetor de autocovari√¢ncias s√£o atualizados. Com a estrutura de Toeplitz, a fatora√ß√£o triangular $ADA'$ pode ser atualizada recursivamente:
>
> $\text{Passo 1: Fatora√ß√£o Inicial: } \Omega_2 = A_2 D_2 A_2^T$
>
> $\text{Passo 2: Nova observa√ß√£o: } Y_{t+1}$
>
> $\text{Passo 3: Atualiza√ß√£o: }  \Omega_3 = A_3 D_3 A_3^T$ com $A_3$ e $D_3$ calculados recursivamente a partir de $A_2$ e $D_2$. Os novos coeficientes $\alpha^{(3)}$ s√£o calculados com a nova fatora√ß√£o.
>
> A atualiza√ß√£o recursiva evita a necessidade de recalcular a fatora√ß√£o do zero a cada nova observa√ß√£o, economizando tempo computacional. Esta atualiza√ß√£o tamb√©m garante que a matriz de autocovari√¢ncias n√£o acumule erros numericos significativos quando muitas observa√ß√µes s√£o adicionadas, uma vez que a fatora√ß√£o √© atualizada de forma incremental, e n√£o calculada do zero a cada etapa.

### Propaga√ß√£o dos Erros e Estabilidade Num√©rica
A atualiza√ß√£o recursiva dos coeficientes e das previs√µes em previs√µes de longo prazo exige aten√ß√£o √† propaga√ß√£o dos erros de previs√£o e √† estabilidade num√©rica do processo. A propaga√ß√£o dos erros ocorre porque a previs√£o de um per√≠odo √© utilizada como entrada para a previs√£o do pr√≥ximo, o que faz com que eventuais erros de arredondamento se acumulem ao longo do tempo, e a estabilidade num√©rica dos algoritmos utilizados √© crucial para garantir que os resultados sejam precisos e confi√°veis.

Os erros de previs√£o em modelos autorregressivos tendem a se propagar ao longo do tempo de forma amplificada, especialmente quando os par√¢metros autorregressivos s√£o pr√≥ximos de 1. Isso pode resultar em intervalos de confian√ßa para as previs√µes de longo prazo muito amplos, indicando alta incerteza nas previs√µes. Em cen√°rios com dados ruidosos, esta propaga√ß√£o pode ter um efeito ainda maior nos resultados finais.

#### Estrat√©gias para Lidar com a Propaga√ß√£o dos Erros
1.  **An√°lise da Fun√ß√£o de Autocorrela√ß√£o:** Analisar a fun√ß√£o de autocorrela√ß√£o da s√©rie temporal permite identificar a estrutura temporal dos dados e escolher um modelo apropriado para as previs√µes. A an√°lise da fun√ß√£o de autocorrela√ß√£o parcial, por sua vez, ajuda na identifica√ß√£o da ordem de um modelo autorregressivo.

2.  **Regulariza√ß√£o:** Para matrizes mal condicionadas (pr√≥ximas da singularidade), o uso de t√©cnicas de regulariza√ß√£o, como a regulariza√ß√£o de Tikhonov, pode melhorar a estabilidade num√©rica da invers√£o e do c√°lculo dos coeficientes. A regulariza√ß√£o adiciona um pequeno termo √† matriz de autocovari√¢ncia, o que melhora seu n√∫mero de condi√ß√£o.
> üí° **Exemplo Num√©rico:**
> Em vez de inverter $\Omega$, calculamos $(\Omega + \lambda I)^{-1}$, onde $\lambda$ √© um par√¢metro de regulariza√ß√£o e $I$ √© a matriz identidade. A escolha de $\lambda$ √© um compromisso entre a estabilidade e o bias da estimativa.
>
> Suponha que temos uma matriz de autocovari√¢ncia $\Omega = \begin{bmatrix} 1 & 0.99 \\ 0.99 & 1 \end{bmatrix}$. Esta matriz est√° pr√≥xima da singularidade, o que pode levar a resultados inst√°veis se tentarmos calcular $\Omega^{-1}$ diretamente.
>
> Aplicando regulariza√ß√£o de Tikhonov com $\lambda = 0.01$, temos:
>
> $$\Omega_{\text{reg}} = \Omega + \lambda I = \begin{bmatrix} 1 & 0.99 \\ 0.99 & 1 \end{bmatrix} + \begin{bmatrix} 0.01 & 0 \\ 0 & 0.01 \end{bmatrix} = \begin{bmatrix} 1.01 & 0.99 \\ 0.99 & 1.01 \end{bmatrix}$$
>
> A matriz $\Omega_{\text{reg}}$ √© mais est√°vel e sua inversa pode ser calculada de forma mais confi√°vel, reduzindo problemas de instabilidade num√©rica.

3.  **Uso de Precis√£o Adequada:** A escolha da precis√£o dos n√∫meros de ponto flutuante usados nos c√°lculos pode influenciar a estabilidade num√©rica. O uso de tipos de dados de maior precis√£o (por exemplo, `float64` em vez de `float32`) pode reduzir os erros de arredondamento, ao custo de maior uso de mem√≥ria e tempo computacional. A escolha deve ser guiada por um balan√ßo entre esses dois fatores, avaliando a sensibilidade do resultado e os limites computacionais.

4. **Testes de Hip√≥teses:** Testes de hip√≥teses podem ser utilizados para verificar se os modelos utilizados para previs√£o s√£o adequados para os dados em quest√£o. Testes como os de Ljung-Box e Dickey-Fuller s√£o utilizados para validar as propriedades dos erros e avaliar a estacionariedade dos dados.
5. **An√°lise de Sensibilidade:** Para testar a sensibilidade do sistema, simula√ß√µes com diferentes conjuntos de dados e perturba√ß√µes nos par√¢metros podem ser utilizadas, ajudando a identificar a vulnerabilidade das previs√µes e dos erros.

#### Uso da Fatora√ß√£o Triangular para Estabilidade Num√©rica
Como discutido nas se√ß√µes anteriores, a fatora√ß√£o triangular de Cholesky √© uma ferramenta computacional importante para o c√°lculo eficiente das previs√µes de amostras finitas [^4.4], mas tamb√©m tem propriedades que auxiliam a estabilidade num√©rica. Em vez de calcular a inversa da matriz de autocovari√¢ncia diretamente, a fatora√ß√£o triangular permite trabalhar com as matrizes $A$ e $D$, que t√™m propriedades num√©ricas mais favor√°veis [^4.5.6]. Isso reduz a propaga√ß√£o de erros e permite obter previs√µes mais precisas e est√°veis. Al√©m disso, devido √† natureza recursiva da fatora√ß√£o triangular, √© poss√≠vel atualizar os c√°lculos √† medida que novas observa√ß√µes s√£o disponibilizadas, sem ter que recome√ßar o c√°lculo desde o in√≠cio.
    
    **Teorema 2** A fatora√ß√£o de Cholesky $\Omega = LL^T$ √© mais est√°vel numericamente do que a invers√£o direta de $\Omega$, pois ela lida melhor com problemas de singularidade e acumula√ß√£o de erros de arredondamento.

    *Prova:*
    I. A fatora√ß√£o de Cholesky decomp√µe uma matriz sim√©trica definida positiva $\Omega$ em um produto de uma matriz triangular inferior $L$ e sua transposta $L^T$.
    
    II. A invers√£o direta de $\Omega$ envolve opera√ß√µes que amplificam erros de arredondamento, especialmente quando a matriz √© mal condicionada (pr√≥xima da singularidade).
    
    III. A fatora√ß√£o de Cholesky envolve opera√ß√µes que s√£o mais est√°veis numericamente, pois n√£o incluem divis√£o por valores pr√≥ximos a zero, o que ocorre na invers√£o direta.

    IV. O n√∫mero de condi√ß√£o da matriz $L$ √© a raiz quadrada do n√∫mero de condi√ß√£o de $\Omega$, tornando os c√°lculos com $L$ mais est√°veis.

    V. Portanto, em problemas de alta sensibilidade num√©rica, a fatora√ß√£o de Cholesky √© mais adequada para obter resultados precisos e confi√°veis do que a invers√£o direta de matrizes. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Considere a matriz de autocovari√¢ncia $\Omega = \begin{bmatrix} 1 & 0.9 \\ 0.9 & 1 \end{bmatrix}$.
>
> **Invers√£o direta:** O c√°lculo de $\Omega^{-1}$ pode ser inst√°vel devido √† proximidade da matriz √† singularidade.
>
> **Fatora√ß√£o de Cholesky:** Encontramos uma matriz triangular inferior $L$ tal que $\Omega = LL^T$. Para este exemplo:
> $$ L = \begin{bmatrix} 1 & 0 \\ 0.9 & \sqrt{1 - 0.9^2} \end{bmatrix} \approx \begin{bmatrix} 1 & 0 \\ 0.9 & 0.4359 \end{bmatrix} $$
>
> As opera√ß√µes com a matriz $L$ s√£o mais est√°veis numericamente do que com a matriz $\Omega$ diretamente, pois o n√∫mero de condi√ß√£o de $L$ √© menor do que o de $\Omega$. A solu√ß√£o de sistemas lineares como $Lx=b$ √© muito mais precisa e confi√°vel computacionalmente se usarmos $L$ em vez de $\Omega$.

    
**Corol√°rio 1.1** O uso de m√©todos recursivos com fatora√ß√£o de Cholesky permite que as previs√µes e os erros sejam atualizados de forma incremental, reduzindo a propaga√ß√£o de erros e melhorando a estabilidade num√©rica para previs√µes de longo prazo.
*Prova:*
I. M√©todos recursivos aproveitam a estrutura da matriz de autocovari√¢ncia para evitar recalcular todos os par√¢metros quando uma nova observa√ß√£o √© adicionada.
II. A fatora√ß√£o de Cholesky permite expressar a matriz de autocovari√¢ncia em termos de uma matriz triangular, o que simplifica a atualiza√ß√£o recursiva dos c√°lculos.
III. Ao evitar o rec√°lculo completo a cada nova observa√ß√£o, o uso de fatora√ß√£o triangular reduz a acumula√ß√£o de erros de arredondamento e proporciona maior estabilidade num√©rica.
IV. Portanto, a combina√ß√£o de m√©todos recursivos e fatora√ß√£o de Cholesky permite a atualiza√ß√£o incremental de previs√µes e erros, reduzindo a propaga√ß√£o de erros em previs√µes de longo prazo. $\blacksquare$

### Conclus√£o
A constru√ß√£o de previs√µes de longo prazo em amostras finitas envolve proje√ß√µes iteradas e atualiza√ß√£o recursiva dos coeficientes de proje√ß√£o, o que exige aten√ß√£o √† estabilidade num√©rica e ao controle da propaga√ß√£o de erros. A Lei das Proje√ß√µes Iteradas fornece a base te√≥rica para este processo recursivo, enquanto a fatora√ß√£o triangular oferece uma maneira eficiente e est√°vel de realizar os c√°lculos. A escolha das bibliotecas de computa√ß√£o num√©rica, a implementa√ß√£o de algoritmos recursivos, a utiliza√ß√£o da computa√ß√£o paralela e a aplica√ß√£o de t√©cnicas de regulariza√ß√£o s√£o passos cruciais para o desenvolvimento de sistemas de previs√£o robustos e precisos. Ao dominar esses conceitos e t√©cnicas, √© poss√≠vel implementar modelos de previs√£o de s√©ries temporais mais eficazes, mesmo em cen√°rios desafiadores. As pr√≥ximas se√ß√µes abordar√£o a estima√ß√£o dos par√¢metros do modelo por m√°xima verossimilhan√ßa e as t√©cnicas de otimiza√ß√£o num√©rica para encontrar os par√¢metros que melhor se ajustam aos dados.

### Refer√™ncias
[^4.3]: Express√£o [4.1.9]
[^4.2.24]: Express√£o [4.2.24]
[^4.5.32]: Express√£o [4.5.32]
[^4.3.6]: Express√£o [4.3.6]
[^4.3.8]: Express√£o [4.3.8]
[^4.5.16]: Express√£o [4.5.16]
[^4.4]: Express√£o [4.4]
[^4.5.6]: Express√£o [4.5.6]
[Teorema 2]: Teorema 2 do cap√≠tulo anterior
[Lema 1]: Lema 1 do cap√≠tulo anterior
[Corol√°rio 1.1]: Corol√°rio 1.1 do cap√≠tulo anterior
<!-- END -->
