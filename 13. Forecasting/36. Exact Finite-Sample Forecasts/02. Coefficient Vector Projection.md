## O Vetor de Coeficientes em Proje√ß√µes de Amostras Finitas e Opera√ß√µes Matriciais
### Introdu√ß√£o
Como explorado anteriormente, a constru√ß√£o de previs√µes exatas para amostras finitas requer a proje√ß√£o do valor futuro de uma s√©rie em seus valores passados [^4.3]. Este processo, fundamental em an√°lise de s√©ries temporais, envolve o c√°lculo de coeficientes que ponderam a influ√™ncia de cada observa√ß√£o passada na previs√£o.  Esses coeficientes, representados pelo vetor $\alpha^{(m)}$, s√£o obtidos atrav√©s de opera√ß√µes matriciais que envolvem a matriz de autocovari√¢ncia e um vetor de produtos cruzados [^4.3.6], [^4.3.8]. No entanto, a complexidade computacional associada a essas opera√ß√µes exige uma an√°lise cuidadosa, especialmente quando o n√∫mero de observa√ß√µes passadas ($m$) aumenta. Este cap√≠tulo aprofunda essa tem√°tica, detalhando como o vetor de coeficientes √© derivado e explorando t√©cnicas para otimizar o processo computacional.

### Deriva√ß√£o do Vetor de Coeficientes $\alpha^{(m)}$
O vetor de coeficientes $\alpha^{(m)}$ surge da necessidade de encontrar os pesos que minimizam o erro quadr√°tico m√©dio (MSE) da previs√£o linear.  Como visto anteriormente, a previs√£o de $Y_{t+1}$ baseada em $m$ valores passados $Y_t, Y_{t-1}, ..., Y_{t-m+1}$ √© expressa como [^4.3.7]:
$$ \hat{Y}_{t+1|t} - \mu = \alpha_1^{(m)}(Y_t - \mu) + \alpha_2^{(m)}(Y_{t-1} - \mu) + \ldots + \alpha_m^{(m)}(Y_{t-m+1} - \mu) $$
onde $\mu$ √© a m√©dia da s√©rie. Para determinar os valores de $\alpha_i^{(m)}$, minimizamos o MSE, o que leva √† seguinte express√£o para o vetor de coeficientes [^4.3.8]:
$$ [\alpha_1^{(m)}, \alpha_2^{(m)}, \ldots, \alpha_m^{(m)} ] = [\gamma_1, \gamma_2, \ldots, \gamma_m ]  \begin{bmatrix}
\gamma_0 & \gamma_1 & \cdots & \gamma_{m-1} \\
\gamma_1 & \gamma_0 & \cdots & \gamma_{m-2} \\
\vdots & \vdots & \ddots & \vdots \\
\gamma_{m-1} & \gamma_{m-2} & \cdots & \gamma_0
\end{bmatrix}^{-1} $$

Nessa equa√ß√£o, $\gamma_i$ representa a autocovari√¢ncia da s√©rie no lag $i$, e a matriz invertida √© a matriz de autocovari√¢ncia  $\Omega_m$, com dimens√£o $m \times m$. A matriz $\Omega_m$ possui estrutura Toeplitz, o que significa que os elementos ao longo de cada diagonal s√£o iguais. Essa caracter√≠stica √© explorada em algoritmos computacionais eficientes para calcular a inversa. O vetor $[\gamma_1, \gamma_2, \ldots, \gamma_m]$ representa o vetor de produtos cruzados entre $Y_{t+1}$ e as $m$ observa√ß√µes passadas.

> üí° **Exemplo Num√©rico:**
> Suponha que temos uma s√©rie temporal com as seguintes autocovari√¢ncias: $\gamma_0 = 2$, $\gamma_1 = 1$, e $\gamma_2 = 0.5$. Queremos calcular o vetor de coeficientes $\alpha^{(2)}$ para prever $Y_{t+1}$ usando $Y_t$ e $Y_{t-1}$. A matriz de autocovari√¢ncia $\Omega_2$ √©:
> $$ \Omega_2 = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix} $$
> O vetor de produtos cruzados √© $[\gamma_1, \gamma_2] = [1, 0.5]$.
>
> **Passo 1:** Calcular a inversa de $\Omega_2$:
> $$ \Omega_2^{-1} = \frac{1}{(2*2) - (1*1)} \begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix} = \frac{1}{3} \begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix} $$
> **Passo 2:** Calcular o vetor de coeficientes $\alpha^{(2)}$:
> $$ [\alpha_1^{(2)}, \alpha_2^{(2)}] = [1, 0.5] \frac{1}{3} \begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix} = \frac{1}{3} [1.5, 0] = [0.5, 0] $$
> Portanto, $\alpha_1^{(2)} = 0.5$ e $\alpha_2^{(2)} = 0$. Isso significa que a previs√£o de $Y_{t+1}$ √© dada por $\hat{Y}_{t+1|t} = 0.5 (Y_t - \mu) + 0 (Y_{t-1} - \mu) + \mu = 0.5 Y_t + 0.5\mu$. Note que o valor de $\mu$ √© necess√°rio para fazer a previs√£o real.

A complexidade computacional dessa opera√ß√£o reside na invers√£o da matriz de autocovari√¢ncia $\Omega_m$. A invers√£o de matrizes √© uma opera√ß√£o que, em geral, possui complexidade $O(m^3)$, onde $m$ √© a dimens√£o da matriz. Portanto, √† medida que o n√∫mero de observa√ß√µes passadas consideradas ($m$) aumenta, o custo computacional do c√°lculo dos coeficientes $\alpha^{(m)}$ cresce rapidamente. √â crucial utilizar t√©cnicas que tornem esses c√°lculos vi√°veis, especialmente em cen√°rios com grande volume de dados.

### Otimiza√ß√£o Computacional Atrav√©s da Fatora√ß√£o Triangular
A fatora√ß√£o triangular da matriz de autocovari√¢ncia oferece uma alternativa para otimizar o c√°lculo do vetor de coeficientes. Como visto anteriormente, a fatora√ß√£o triangular de uma matriz sim√©trica definida positiva $\Omega$ permite express√°-la como [^4.4.1]:
$$ \Omega = ADA' $$
onde $A$ √© uma matriz triangular inferior com 1s na diagonal principal e $D$ √© uma matriz diagonal. Esta decomposi√ß√£o, √∫nica, pode ser calculada atrav√©s do m√©todo de Cholesky ou outros m√©todos relacionados [^4.4], que apresentam um custo computacional mais baixo que a invers√£o direta de $\Omega$.

Em vez de calcular a inversa de $\Omega$ diretamente, podemos usar as matrizes $A$ e $D$ para obter o vetor de coeficientes $\alpha^{(m)}$ de forma mais eficiente.  Primeiro, ao inv√©s de inverter $\Omega$, transformamos o vetor de produtos cruzados  $\gamma_m = [\gamma_1, \gamma_2, \ldots, \gamma_m]'$ em um vetor $\gamma_m^* = A^{-1} \gamma_m$. Em seguida, calculamos $D^{-1} \gamma_m^*$ e por fim multiplicamos $A'^{-1}D^{-1} \gamma_m^*$. Essa abordagem envolve apenas a invers√£o de matrizes triangulares, cuja complexidade computacional √© $O(m^2)$ em vez de $O(m^3)$, al√©m de opera√ß√µes de multiplica√ß√£o matriz-vetor que t√™m custo computacional $O(m^2)$. Dessa forma, a fatora√ß√£o triangular reduz significativamente o custo computacional total, tornando a previs√£o de amostras finitas mais vi√°vel.

> üí° **Exemplo Num√©rico:**
> Usando o mesmo exemplo anterior, vamos fatorar a matriz $\Omega_2$.
>
> **Passo 1:** Encontrar a fatora√ß√£o $ADA'$ de $\Omega_2$:
>
>  $A = \begin{bmatrix} 1 & 0 \\ 0.5 & 1 \end{bmatrix}$, $D = \begin{bmatrix} 2 & 0 \\ 0 & 1.5 \end{bmatrix}$, $A' = \begin{bmatrix} 1 & 0.5 \\ 0 & 1 \end{bmatrix}$
>
> Note que $ADA' = \begin{bmatrix} 1 & 0 \\ 0.5 & 1 \end{bmatrix} \begin{bmatrix} 2 & 0 \\ 0 & 1.5 \end{bmatrix} \begin{bmatrix} 1 & 0.5 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix} = \Omega_2$
>
> **Passo 2:** Calcular $A^{-1}$:
>
> $A^{-1} = \begin{bmatrix} 1 & 0 \\ -0.5 & 1 \end{bmatrix}$
>
> **Passo 3:** Calcular $\gamma_2^* = A^{-1} \gamma_2$:
>
> $\gamma_2^* = \begin{bmatrix} 1 & 0 \\ -0.5 & 1 \end{bmatrix} \begin{bmatrix} 1 \\ 0.5 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$
>
> **Passo 4:** Calcular $D^{-1} \gamma_2^*$:
>
> $D^{-1} = \begin{bmatrix} 0.5 & 0 \\ 0 & 2/3 \end{bmatrix}$, $D^{-1} \gamma_2^* =  \begin{bmatrix} 0.5 & 0 \\ 0 & 2/3 \end{bmatrix}  \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 0.5 \\ 0 \end{bmatrix}$
>
> **Passo 5:** Calcular  $A'^{-1} (D^{-1} \gamma_2^*)$:
>
>  $A'^{-1} = \begin{bmatrix} 1 & -0.5 \\ 0 & 1 \end{bmatrix}$, $A'^{-1} (D^{-1} \gamma_2^*) = \begin{bmatrix} 1 & -0.5 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 0.5 \\ 0 \end{bmatrix} =  \begin{bmatrix} 0.5 \\ 0 \end{bmatrix}$
>
> O resultado √© o mesmo que obtivemos anteriormente, $\alpha^{(2)} = [0.5, 0]$, mas com menos opera√ß√µes computacionalmente custosas.

O uso da fatora√ß√£o triangular n√£o se limita apenas √† otimiza√ß√£o do c√°lculo dos coeficientes, ela tamb√©m permite a atualiza√ß√£o recursiva do erro de previs√£o [Corol√°rio 1.1].  O MSE da previs√£o √© dado pelo √∫ltimo elemento diagonal da matriz D, o que significa que ao adicionar uma nova observa√ß√£o e recalcular a fatora√ß√£o triangular, √© poss√≠vel atualizar tanto o vetor de coeficientes quanto o erro de previs√£o sem a necessidade de recalcular todas as opera√ß√µes novamente.
**Teorema 1**
A fatora√ß√£o triangular de uma matriz de autocovari√¢ncia Toeplitz $\Omega_m$ pode ser expressa de forma recursiva, onde a fatora√ß√£o de $\Omega_{m+1}$ pode ser obtida a partir da fatora√ß√£o de $\Omega_m$.

*Prova:*
I. Para $m=1$, a fatora√ß√£o $\Omega_1 = A_1 D_1 A_1'$ √© trivial, pois $\Omega_1$ √© apenas um escalar.

II. Suponha que para $m=k$ a fatora√ß√£o triangular $\Omega_k = A_k D_k A_k'$ j√° foi obtida. 

III. Para $m=k+1$, podemos construir a matriz $\Omega_{k+1}$ a partir de $\Omega_k$ adicionando uma linha e coluna com as autocovari√¢ncias apropriadas. Dada a estrutura de Toeplitz:
$$
\Omega_{k+1} = 
\begin{bmatrix}
    \Omega_k & \gamma_k \\
    \gamma_k' & \gamma_0
\end{bmatrix}
$$
onde $\gamma_k = [\gamma_1, \gamma_2, ..., \gamma_k]'$.

IV. A fatora√ß√£o de $\Omega_{k+1}$ √© dada por $\Omega_{k+1} = A_{k+1} D_{k+1} A_{k+1}'$, onde
$$
A_{k+1} = 
\begin{bmatrix}
    A_k & 0 \\
    a_k' & 1
\end{bmatrix}
$$
e
$$
D_{k+1} = 
\begin{bmatrix}
    D_k & 0 \\
    0 & d_{k+1}
\end{bmatrix}
$$
onde $a_k$ e $d_{k+1}$ s√£o obtidos da fatora√ß√£o de $\Omega_{k+1}$.
V. Essa constru√ß√£o recursiva demonstra que a fatora√ß√£o triangular de matrizes Toeplitz pode ser realizada de forma incremental, aproveitando os resultados j√° obtidos em dimens√µes menores. A recursividade prov√©m da estrutura Toeplitz que garante que os elementos da diagonal s√£o todos iguais. A estrutura particular das matrizes $A$ e $D$ tamb√©m permite calcular a atualiza√ß√£o recursiva de forma eficiente. ‚ñ†

**Lema 1**
O vetor de coeficientes $\alpha^{(m)}$ pode ser atualizado recursivamente utilizando a fatora√ß√£o triangular.

*Prova:*
I.  Do Teorema 1, sabemos que a fatora√ß√£o triangular de $\Omega_m$ pode ser obtida recursivamente, ou seja, a fatora√ß√£o de $\Omega_{m+1}$ pode ser obtida a partir da fatora√ß√£o de $\Omega_m$.

II. O vetor de coeficientes $\alpha^{(m)}$ √© dado por $\alpha^{(m)} = \gamma_m' \Omega_m^{-1}$, onde $\gamma_m$ √© o vetor de produtos cruzados.

III. Usando a fatora√ß√£o triangular $\Omega_m = A_m D_m A_m'$, temos $\Omega_m^{-1} = (A_m')^{-1} D_m^{-1} A_m^{-1}$.

IV. Como a fatora√ß√£o triangular de $\Omega_m$ pode ser atualizada recursivamente, as matrizes $A_m$, $D_m$ e, portanto, $\Omega_m^{-1}$ tamb√©m podem ser atualizadas recursivamente.

V. Assim, ao adicionar uma nova observa√ß√£o, podemos usar a fatora√ß√£o triangular da matriz de autocovari√¢ncia de dimens√£o $m$ para calcular a fatora√ß√£o triangular da matriz de dimens√£o $m+1$, o que, por sua vez, nos permite atualizar o vetor de coeficientes $\alpha^{(m+1)}$ sem precisar recalcular toda a opera√ß√£o matricial desde o in√≠cio. ‚ñ†

**Corol√°rio 1.1**
O erro quadr√°tico m√©dio da previs√£o (MSE) pode ser atualizado recursivamente utilizando a fatora√ß√£o triangular.

*Prova:*
I. O erro quadr√°tico m√©dio da previs√£o (MSE) √© dado por $MSE_m = \gamma_0 - \gamma_m' \Omega_m^{-1} \gamma_m$, onde $\gamma_0$ √© a autocovari√¢ncia no lag 0.
II. O MSE pode ser expresso em termos da fatora√ß√£o triangular como o √∫ltimo elemento diagonal da matriz $D$, que chamamos $d_{m}$. Ou seja, $MSE_m = d_m$.
III. De acordo com o Teorema 1, podemos obter $D_{m+1}$ usando $D_m$, e o √∫ltimo elemento diagonal de $D_{m+1}$ ser√° $d_{m+1}$, que corresponde ao $MSE_{m+1}$.
IV. Portanto, o MSE da previs√£o de $m+1$ pode ser obtido usando o MSE da previs√£o de $m$ sem necessidade de recalcular todas as opera√ß√µes matriciais. ‚ñ†

**Teorema 2**
A fatora√ß√£o triangular pode ser usada para obter os coeficientes de proje√ß√£o de forma recursiva.

*Prova*:
I.  Pelo Lema 1, sabemos que o vetor de coeficientes $\alpha^{(m)}$ pode ser atualizado recursivamente.

II.  Pelo Corol√°rio 1.1, sabemos que o erro quadr√°tico m√©dio da previs√£o (MSE) tamb√©m pode ser atualizado recursivamente utilizando a fatora√ß√£o triangular.

III. A fatora√ß√£o triangular da matriz de autocovari√¢ncia $\Omega_m$ √© dada por $\Omega_m = A_m D_m A_m'$, onde $A_m$ √© uma matriz triangular inferior com 1s na diagonal principal e $D_m$ √© uma matriz diagonal.

IV.  A atualiza√ß√£o recursiva das matrizes $A$ e $D$ permite atualizar recursivamente o vetor de coeficientes $\alpha^{(m)}$ e o MSE da previs√£o, pois o vetor de coeficientes depende da fatora√ß√£o triangular de $\Omega_m$ e o MSE √© o √∫ltimo elemento diagonal da matriz $D$.

V. Quando uma nova observa√ß√£o $Y_{t-m}$ √© adicionada, em vez de calcular toda a matriz de autocovari√¢ncia $\Omega_{m+1}$ e inverter ela, √© poss√≠vel atualizar as matrizes $A$ e $D$ recursivamente. Isso se traduz em um vetor de coeficientes de proje√ß√£o $\alpha^{(m+1)}$ e um MSE de previs√£o que pode ser calculado utilizando informa√ß√µes do passo anterior. Ou seja, os elementos de $A_{m+1}$ e $D_{m+1}$ podem ser expressos em termos de $A_m$, $D_m$ e os novos elementos da matriz de autocovari√¢ncia $\Omega_{m+1}$.

VI. Portanto, a fatora√ß√£o triangular √© uma ferramenta que permite tanto a obten√ß√£o dos coeficientes de proje√ß√£o quanto a atualiza√ß√£o do erro de previs√£o de maneira eficiente e recursiva. ‚ñ†

> üí° **Exemplo Num√©rico:**
> Para ilustrar o processo de atualiza√ß√£o recursiva, considere o exemplo num√©rico do processo MA(1) na se√ß√£o anterior.  Vimos que para $m=1$ temos:
> - $\alpha_1^{(1)} = 0.488$
> - $MSE_1 = 1.64$
> e para $m=2$:
>  - $\alpha_1^{(2)} = 0.59$
> - $\alpha_2^{(2)} = -0.286$
> - $MSE_2 = 1.3934$
>
> Suponha que agora, queremos atualizar para $m=3$. A matriz de autocovari√¢ncia $\Omega_3$ √©:
>
> $$ \Omega_3 = \begin{bmatrix} 1.64 & 0.8 & 0 \\ 0.8 & 1.64 & 0.8 \\ 0 & 0.8 & 1.64 \end{bmatrix} $$
>
> A fatora√ß√£o triangular resulta em:
>
> $A_3 = \begin{bmatrix}
>  1 & 0 & 0 \\
>  0.488 & 1 & 0 \\
>  0 & 0.622 & 1
> \end{bmatrix}$
>
> $D_3 = \begin{bmatrix}
>  1.64 & 0 & 0 \\
>  0 & 1.3934 & 0 \\
>  0 & 0 & 1.2524
> \end{bmatrix}$
>
>  Os novos coeficientes s√£o calculados com:
>
> $[\alpha_1^{(3)}, \alpha_2^{(3)}, \alpha_3^{(3)}] = [0.8, 0, 0] \Omega_3^{-1}$
>
> Como a matriz $A$ √© triangular inferior, a sua inversa pode ser facilmente calculada:
>
> $A_3^{-1} = \begin{bmatrix}
>  1 & 0 & 0 \\
>  -0.488 & 1 & 0 \\
>   0.303 & -0.622 & 1
> \end{bmatrix}$
>
>  E podemos obter as previs√µes recursivamente, e o erro quadr√°tico m√©dio associado √© $MSE_3 = 1.2524$. Note que os c√°lculos s√£o feitos usando os valores das matrizes $A$ e $D$ obtidas em $m=2$ e adicionando novos elementos, em vez de recalcular tudo do zero.

### Conclus√£o
O vetor de coeficientes $\alpha^{(m)}$ desempenha um papel crucial na constru√ß√£o de previs√µes exatas de amostra finita, e a forma como esses coeficientes s√£o calculados impacta diretamente a efici√™ncia computacional do processo. O uso de opera√ß√µes matriciais, incluindo a invers√£o da matriz de autocovari√¢ncia, pode ser computacionalmente dispendioso. No entanto, a fatora√ß√£o triangular oferece um caminho para otimizar esses c√°lculos, reduzindo a complexidade computacional e permitindo a atualiza√ß√£o recursiva dos coeficientes e do erro de previs√£o.  Compreender esses aspectos √© essencial para a implementa√ß√£o eficaz de modelos de previs√£o em aplica√ß√µes pr√°ticas.  As t√©cnicas apresentadas neste cap√≠tulo fornecem uma base s√≥lida para o desenvolvimento de previs√µes precisas e eficientes, utilizando um n√∫mero finito de observa√ß√µes. Na pr√≥xima se√ß√£o, exploraremos a Lei das Proje√ß√µes Iteradas e como ela se relaciona com as previs√µes em amostras finitas.
## 5.2. A Lei das Proje√ß√µes Iteradas e Previs√µes em Amostras Finitas
### Lei das Proje√ß√µes Iteradas
A *Lei das Proje√ß√µes Iteradas*, j√° mencionada anteriormente [^4.2.24], desempenha um papel crucial na constru√ß√£o de previs√µes √≥timas, particularmente em situa√ß√µes onde o n√∫mero de observa√ß√µes √© finito. Essa lei estabelece que a proje√ß√£o de uma proje√ß√£o √© igual √† proje√ß√£o original. Formalmente, se tivermos tr√™s conjuntos de vari√°veis aleat√≥rias, $Y_t$, $Y_{t+1}$ e $Y_{t+2}$, e se o objetivo for prever $Y_{t+2}$ com base nas informa√ß√µes de $Y_t$ e $Y_{t+1}$, a lei afirma que a proje√ß√£o de $Y_{t+2}$ sobre $Y_t$ √© igual √† proje√ß√£o de $Y_{t+2}$ sobre $Y_{t+1}$ (que j√° incorpora informa√ß√µes de $Y_t$), projetada novamente sobre $Y_t$. Em outras palavras,
$$P[P(Y_{t+2}|Y_{t+1}, Y_t)|Y_t] = P(Y_{t+2}|Y_t).$$
Em termos pr√°ticos, essa lei implica que, ao construir previs√µes de horizonte m√∫ltiplo, podemos iterar o processo de previs√£o passo a passo, utilizando as previs√µes de um per√≠odo como entrada para a previs√£o do per√≠odo seguinte, em vez de realizar uma previs√£o direta para o horizonte desejado. Essa abordagem √© particularmente √∫til em modelos autorregressivos.

### Implica√ß√µes para Previs√µes em Amostras Finitas
A Lei das Proje√ß√µes Iteradas se torna particularmente relevante quando lidamos com amostras finitas, pois fornece uma maneira de construir previs√µes de horizonte m√∫ltiplo usando a estrutura de modelos autorregressivos. Em vez de tentar modelar diretamente as rela√ß√µes entre vari√°veis em per√≠odos muito distantes, a lei nos permite construir um modelo para um per√≠odo e usar recursivamente o modelo para projetar o futuro.
Por exemplo, para um processo AR(1),
$$Y_{t+1} = \phi Y_t + \epsilon_{t+1},$$
a previs√£o de $Y_{t+2}$ com base em $Y_t$ pode ser obtida iterando o modelo de um passo:
$$P(Y_{t+2}|Y_t) = P(\phi Y_{t+1} + \epsilon_{t+2}|Y_t) = \phi P(Y_{t+1}|Y_t) = \phi^2 Y_t.$$
Essa abordagem iterativa √© fundamental para derivar as express√µes para previs√µes em amostras finitas, conforme discutido em se√ß√µes anteriores [^4.2.20].

> üí° **Exemplo Num√©rico:**
> Considere um modelo AR(1) com $\phi = 0.8$ e $Y_t = 10$.  Queremos prever $Y_{t+2}$ usando a Lei das Proje√ß√µes Iteradas.
>
> **Passo 1:** Prever $Y_{t+1}$ dado $Y_t$:
>
> $$P(Y_{t+1}|Y_t) = \phi Y_t = 0.8 \times 10 = 8$$
>
> **Passo 2:** Prever $Y_{t+2}$ usando a previs√£o de $Y_{t+1}$:
>
> $$P(Y_{t+2}|Y_t) = P(Y_{t+2}|Y_{t+1}) = \phi P(Y_{t+1}|Y_t) = 0.8 \times 8 = 6.4 = \phi^2 Y_t$$
>
> Portanto, a previs√£o de $Y_{t+2}$ dada $Y_t$ √© 6.4. A Lei das Proje√ß√µes Iteradas permite obter essa previs√£o de forma iterativa, sem modelar diretamente a rela√ß√£o entre $Y_{t+2}$ e $Y_t$.

### Conex√£o com a Decomposi√ß√£o de Wold
A Lei das Proje√ß√µes Iteradas tamb√©m est√° intimamente ligada √† *Decomposi√ß√£o de Wold*, que afirma que qualquer processo estacion√°rio pode ser representado como a soma de um componente determin√≠stico e um componente indetermin√≠stico, sendo este √∫ltimo expresso como uma combina√ß√£o linear de choques brancos [^4.8.2]. A lei garante que a proje√ß√£o de um processo no futuro pode ser obtida iterando a proje√ß√£o de um passo, o que se alinha com a ideia de que previs√µes podem ser feitas a partir do hist√≥rico de choques passados.

### Pr√≥ximos Passos
Ao ter consolidado o entendimento das previs√µes √≥timas e de como a Lei das Proje√ß√µes Iteradas auxilia na sua constru√ß√£o, a pr√≥xima etapa √© a an√°lise da *fun√ß√£o de verossimilhan√ßa* em modelos ARMA.

## 5.3. A Fun√ß√£o de Verossimilhan√ßa para Modelos ARMA Gaussianos
O m√©todo de *m√°xima verossimilhan√ßa* (MV) busca encontrar os valores dos par√¢metros de um modelo que maximizam a probabilidade de observarmos os dados que temos em m√£os [^5.1.4]. Em modelos ARMA, essa probabilidade √© formalizada atrav√©s da *fun√ß√£o de verossimilhan√ßa*. A ideia central √© que os par√¢metros que melhor explicam os dados s√£o aqueles que tornam a amostra observada mais prov√°vel.
A constru√ß√£o da fun√ß√£o de verossimilhan√ßa envolve alguns passos cruciais:
### Distribui√ß√£o Condicional dos Erros
Inicialmente, √© fundamental assumir uma distribui√ß√£o para o termo de erro $\epsilon_t$. Conforme mencionado anteriormente [^5.1.5], na maioria das an√°lises, assume-se que os erros seguem uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\sigma^2$.
$$\epsilon_t \sim \text{i.i.d. N}(0, \sigma^2).$$
Essa suposi√ß√£o nos permite modelar a distribui√ß√£o condicional de $Y_t$ dados os valores anteriores. Para um modelo ARMA(p,q) geral, podemos escrever
$$Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + ... + \phi_p Y_{t-p} + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + ... + \theta_q \epsilon_{t-q} + \epsilon_t.$$
Dados os valores de $Y_{t-1}, Y_{t-2}, ..., Y_{t-p}$ e $\epsilon_{t-1}, \epsilon_{t-2}, ..., \epsilon_{t-q}$, a distribui√ß√£o condicional de $Y_t$ √© tamb√©m normal, com m√©dia
$$\mu_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + ... + \phi_p Y_{t-p} + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + ... + \theta_q \epsilon_{t-q}$$
e vari√¢ncia $\sigma^2$.
### Fun√ß√£o de Verossimilhan√ßa
Com a distribui√ß√£o condicional de $Y_t$ estabelecida, a fun√ß√£o de verossimilhan√ßa, que denotaremos como $L(\theta)$, onde $\theta$ √© o vetor de par√¢metros a serem estimados, √© dada pelo produto das densidades de probabilidade condicionais para cada observa√ß√£o na amostra:
$$L(\theta) = f(y_1, y_2, ..., y_T | \theta) = f(y_1 | \theta) \prod_{t=2}^{T} f(y_t | y_{t-1}, y_{t-2}, ..., y_1, \theta),$$
onde $f(y_t | y_{t-1}, y_{t-2}, ..., y_1, \theta)$ representa a densidade de probabilidade condicional de $Y_t$ dados os valores passados. Devido √† suposi√ß√£o de normalidade, a densidade condicional √© dada por:
$$f(y_t | y_{t-1}, y_{t-2}, ..., y_1, \theta) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(y_t - \mu_t)^2}{2\sigma^2}\right).$$
A fun√ß√£o de verossimilhan√ßa, portanto, se torna o produto de termos exponenciais, e para fins pr√°ticos, √© mais conveniente maximizar o seu logaritmo, ou seja, a *log-verossimilhan√ßa*:
$$\ln L(\theta) = -\frac{T}{2} \ln(2 \pi \sigma^2) - \sum_{t=1}^{T} \frac{(y_t - \mu_t)^2}{2 \sigma^2}.$$
O objetivo do m√©todo de m√°xima verossimilhan√ßa √© encontrar os valores de $\theta$ que maximizam essa log-verossimilhan√ßa.

### Modelos AR(p)
Para um processo AR(p), onde n√£o h√° termos de m√©dia m√≥vel, a fun√ß√£o de log-verossimilhan√ßa se torna
$$\ln L(\theta) = -\frac{T}{2} \ln(2 \pi \sigma^2) - \sum_{t=p+1}^{T} \frac{(y_t - c - \phi_1 y_{t-1} - \phi_2 y_{t-2} - ... - \phi_p y_{t-p})^2}{2 \sigma^2}.$$
Note que a soma come√ßa em $t = p+1$, uma vez que precisamos de $p$ observa√ß√µes anteriores para calcular a m√©dia condicional.

> üí° **Exemplo Num√©rico:**
> Considere um modelo AR(1) com $Y_t = 2 + 0.8Y_{t-1} + \epsilon_t$, onde $\epsilon_t \sim N(0, 1)$. Temos uma amostra de tamanho 3: $Y_1 = 5$, $Y_2 = 6$, $Y_3 = 7$.
>
> A log-verossimilhan√ßa para este modelo AR(1) √©:
> $$\ln L(\theta) = -\frac{T}{2} \ln(2 \pi \sigma^2) - \sum_{t=2}^{T} \frac{(y_t - c - \phi_1 y_{t-1})^2}{2 \sigma^2}$$
>
> Substituindo os valores, temos:
>
> $\ln L(\theta) = -\frac{3}{2} \ln(2 \pi) - \frac{1}{2} \left[  (6 - 2 - 0.8 * 5)^2 + (7 - 2 - 0.8 * 6)^2 \right] =  - \frac{3}{2} \ln(2 \pi) - \frac{1}{2} \left[  (6-6)^2 + (7 - 6.8)^2 \right]  = -5.4018 - \frac{1}{2} (0 + 0.04) = -5.4218 $
>
> O valor -5.4218 representa a log-verossimilhan√ßa para esses dados com os par√¢metros especificados. Note que para calcular a verossimilhan√ßa precisamos assumir um valor para $\sigma^2$, que neste caso usamos $\sigma^2=1$, e o objetivo da m√°xima verossimilhan√ßa seria encontrar os par√¢metros $c$ e $\phi_1$ que maximizam esse valor de $\ln L(\theta)$.

### Modelos MA(q)
Em um modelo MA(q), por outro lado,
$$Y_t = c + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + ... + \theta_q \epsilon_{t-q} + \epsilon_t.$$
A m√©dia condicional se torna
$$\mu_t = c + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + ... + \theta_q \epsilon_{t-q}.$$
A fun√ß√£o de log-verossimilhan√ßa √© dada por:
$$\ln L(\theta) = -\frac{T}{2} \ln(2 \pi \sigma^2) - \sum_{t=1}^{T} \frac{(y_t - c - \theta_1 \epsilon_{t-1} - \theta_2 \epsilon_{t-2} - ... - \theta_q \epsilon_{t-q})^2}{2 \sigma^2}.$$
Entretanto, como os $\epsilon_t$ n√£o s√£o observ√°veis, precisamos utilizar algum m√©todo para estim√°-los. Uma abordagem comum √© inicializ√°-los com zero (ou outro valor adequado) e iterar o processo de estima√ß√£o, que ser√° discutido nas pr√≥ximas se√ß√µes.
**Proposi√ß√£o 1**
A inicializa√ß√£o dos erros $\epsilon_t$ com zero √© equivalente a assumir que o processo come√ßou no infinito passado.

*Prova:*
I. Considere um processo MA(q) dado por $Y_t = c + \sum_{i=1}^q \theta_i \epsilon_{t-i} + \epsilon_t$.

II. A representa√ß√£o de m√©dia m√≥vel de um processo estacion√°rio pode ser expressa como uma soma infinita de choques passados:
    $$Y_t = \mu + \sum_{j=0}^{\infty} \psi_j \epsilon_{t-j}$$
   onde $\mu$ √© a m√©dia do processo e $\psi_j$ s√£o os coeficientes de m√©dia m√≥vel.
   
III. Se inicializarmos os erros com $\epsilon_t = 0$ para todo $t \leq 0$, estamos truncando essa soma infinita. Isso equivale a assumir que todos os choques passados antes do in√≠cio da amostra s√£o nulos, i.e.,  $\epsilon_t=0$ para $t<1$.

IV. Essa truncagem √© uma aproxima√ß√£o, pois estamos desconsiderando a parte infinita da representa√ß√£o do processo. No entanto, essa aproxima√ß√£o torna o c√°lculo da fun√ß√£o de verossimilhan√ßa computacionalmente trat√°vel.

V. Portanto, inicializar os erros com zero √© equivalente a assumir que o processo come√ßou no infinito passado com todos os erros anteriores iguais a zero, simplificando a estima√ß√£o e a an√°lise da fun√ß√£o de verossimilhan√ßa. ‚ñ†

### Modelos ARMA(p,q)
O caso geral de ARMA(p,q) √© uma combina√ß√£o dos dois casos anteriores, com a fun√ß√£o de verossimilhan√ßa incorporando ambas as partes autorregressiva e de m√©dia m√≥vel, o que adiciona complexidade aos algoritmos de estima√ß√£o.

### Pr√≥ximos Passos
Ap√≥s estabelecer a forma da fun√ß√£o de verossimilhan√ßa para diferentes modelos ARMA, a pr√≥xima se√ß√£o focar√° em como maximizar esta fun√ß√£o para obter as estimativas dos par√¢metros.
<!-- END -->
