## O MSE e a Otimiza√ß√£o da Proje√ß√£o Linear: Uma An√°lise Comparativa

### Introdu√ß√£o
Este cap√≠tulo explora a fundo a otimiza√ß√£o da **proje√ß√£o linear** atrav√©s da decomposi√ß√£o do erro quadr√°tico m√©dio (MSE), demonstrando como a proje√ß√£o linear, definida como $P(Y_{t+1}|X_t) = \alpha'X_t$, emerge como a solu√ß√£o √≥tima dentro da classe de previs√µes lineares. Construindo sobre os conceitos previamente apresentados, vamos analisar detalhadamente a estrutura do MSE, identificando um termo intermedi√°rio que se anula em virtude da condi√ß√£o de ortogonalidade, similar ao que ocorre com a expectativa condicional. Esse processo nos permitir√° estabelecer que a fun√ß√£o linear $g'X_t$ que minimiza o MSE √© precisamente a proje√ß√£o linear $\alpha'X_t$.

### Decomposi√ß√£o do MSE e a Otimiza√ß√£o
Considere uma fun√ß√£o linear arbitr√°ria $g'X_t$ como uma candidata para prever $Y_{t+1}$. O MSE associado a essa previs√£o √© dado por:
$$MSE = E[(Y_{t+1} - g'X_t)^2]$$
Para demonstrar que a proje√ß√£o linear $\alpha'X_t$ minimiza esse MSE dentro do espa√ßo de fun√ß√µes lineares, vamos adicionar e subtrair $\alpha'X_t$ na express√£o do MSE:
$$MSE = E[(Y_{t+1} - \alpha'X_t + \alpha'X_t - g'X_t)^2]$$
Expandindo o quadrado, obtemos:
$$MSE = E[(Y_{t+1} - \alpha'X_t)^2 + 2(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t) + (\alpha'X_t - g'X_t)^2]$$
Aplicando a propriedade da linearidade da esperan√ßa, podemos reescrever como:
$$MSE = E[(Y_{t+1} - \alpha'X_t)^2] + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t)] + E[(\alpha'X_t - g'X_t)^2]$$
O termo central nessa express√£o,  $2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t)]$, √© crucial para a demonstra√ß√£o da otimalidade. Definimos $\eta_{t+1} = (Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t)$. Pela condi√ß√£o de ortogonalidade da proje√ß√£o linear, sabemos que $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$ [^2]. Usando esse resultado e a lei da esperan√ßa iterada, podemos mostrar que a esperan√ßa do termo do meio se anula:
$$E[\eta_{t+1}] = E[E[\eta_{t+1}|X_t]] = E[(Y_{t+1} - \alpha'X_t)E[\alpha'X_t - g'X_t|X_t]] = E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t)] = 0$$
Dessa forma, o MSE simplifica-se para:
$$MSE = E[(Y_{t+1} - \alpha'X_t)^2] + E[(\alpha'X_t - g'X_t)^2]$$
O segundo termo dessa express√£o √© sempre n√£o negativo. Portanto, o MSE √© minimizado quando esse termo √© igual a zero, o que ocorre apenas se $g'X_t = \alpha'X_t$. Isso demonstra que a proje√ß√£o linear $\alpha'X_t$ minimiza o MSE dentro da classe de previs√µes lineares [^1].

> üí° **Exemplo Num√©rico:**
> Para ilustrar o conceito de decomposi√ß√£o do MSE, vamos considerar um exemplo pr√°tico onde queremos prever o desempenho de um produto ($Y_{t+1}$) com base nos gastos com publicidade digital ($X_t$). Temos alguns dados simulados:
>
> | t   | Publicidade ($X_t$) | Desempenho ($Y_{t+1}$) |
> |-----|--------------------|-----------------------|
> | 1   | 1                  | 3                     |
> | 2   | 2                  | 5                     |
> | 3   | 3                  | 8                     |
> | 4   | 4                  | 10                    |
> | 5   | 5                  | 12                    |
>
> Primeiro, vamos calcular a proje√ß√£o linear $\hat{Y}_{t+1} = \alpha'X_t$:
> 
> 1. **Calculamos** $E[X_t^2]$:
>     $$E[X_t^2] = \frac{1^2 + 2^2 + 3^2 + 4^2 + 5^2}{5} = \frac{1 + 4 + 9 + 16 + 25}{5} = \frac{55}{5} = 11$$
>
> 2. **Calculamos** $E[Y_{t+1}X_t]$:
>     $$E[Y_{t+1}X_t] = \frac{1\cdot3 + 2\cdot5 + 3\cdot8 + 4\cdot10 + 5\cdot12}{5} = \frac{3+10+24+40+60}{5} = \frac{137}{5} = 27.4$$
>
> 3.  **Calculamos** $\alpha$:
>     $$\alpha = \frac{E[Y_{t+1}X_t]}{E[X_t^2]} = \frac{27.4}{11} \approx 2.49$$
>
> A proje√ß√£o linear √© ent√£o $\hat{Y}_{t+1} = 2.49 X_t$. Agora, vamos analisar o MSE:
>
> | t   | $X_t$ | $Y_{t+1}$ | $\hat{Y}_{t+1} = 2.49 X_t$ | $Y_{t+1} - \hat{Y}_{t+1}$ | $(Y_{t+1} - \hat{Y}_{t+1})^2$ |
> |-----|-------|-----------|----------------------------|---------------------------|-------------------------------|
> | 1   | 1     | 3         | 2.49                       | 0.51                      | 0.26                       |
> | 2   | 2     | 5         | 4.98                       | 0.02                      | 0.00                       |
> | 3   | 3     | 8         | 7.47                       | 0.53                      | 0.28                       |
> | 4   | 4     | 10        | 9.96                       | 0.04                      | 0.00                       |
> | 5   | 5     | 12        | 12.45                      | -0.45                     | 0.20                       |
>
>  $$MSE_{\alpha} = \frac{0.26+0.00+0.28+0.00+0.20}{5} = \frac{0.74}{5} \approx 0.15$$
>
> Para demonstrar que o termo intermedi√°rio √© zero e a otimalidade da proje√ß√£o linear, vamos usar outra fun√ß√£o linear para prever $Y_{t+1}$, por exemplo, $g(X_t) = 2X_t$.
>
> | t   | $X_t$ | $Y_{t+1}$ | $g(X_t) = 2 X_t$ |  $Y_{t+1} - g(X_t)$ | $(Y_{t+1} - g(X_t))^2$ | $\hat{Y}_{t+1} - g(X_t)$ | $(Y_{t+1} - \hat{Y}_{t+1}) (\hat{Y}_{t+1} - g(X_t))$ |
> |-----|-------|-----------|-------------------|---------------------|------------------------|----------------------|-----------------------------------------------|
> | 1   | 1     | 3         | 2                 | 1                   | 1                      | 0.49                  |  0.25 |
> | 2   | 2     | 5         | 4                 | 1                   | 1                      | 0.98                  |  0.0196 |
> | 3   | 3     | 8         | 6                 | 2                   | 4                      | 1.47                  |  0.778 |
> | 4   | 4     | 10        | 8                 | 2                   | 4                      | 1.96                  |  0.078 |
> | 5   | 5     | 12        | 10                | 2                   | 4                      | 2.45                  |  -1.10 |
>
>  $$MSE_{g} = \frac{1 + 1 + 4 + 4 + 4}{5} = \frac{14}{5} = 2.8$$
>
>  $$E[(Y_{t+1} - \hat{Y}_{t+1}) (\hat{Y}_{t+1} - g(X_t))] = \frac{0.25+0.02+0.778+0.078-1.1}{5} = \frac{0.024}{5} \approx 0$$
>
>  O termo intermedi√°rio  $E[(Y_{t+1} - \hat{Y}_{t+1}) (\hat{Y}_{t+1} - g(X_t))]$  √© igual a zero, confirmando que a decomposi√ß√£o do MSE √© v√°lida. Al√©m disso, $MSE_{\alpha} < MSE_g$, refor√ßando a otimalidade da proje√ß√£o linear.
 
  **Lema 0.1:** (MSE da Proje√ß√£o Linear)
  *O erro quadr√°tico m√©dio (MSE) da proje√ß√£o linear* $P(Y_{t+1}|X_t) = \alpha'X_t$ *√© dado por*:
 $$MSE = E[(Y_{t+1} - \alpha'X_t)^2] = E[Y_{t+1}^2] - \alpha'E[X_tY_{t+1}] $$
*Proof:*
I.  Come√ßamos com a defini√ß√£o do MSE: $MSE = E[(Y_{t+1} - \alpha'X_t)^2]$.
II. Expandindo o quadrado, temos: $MSE = E[Y_{t+1}^2 - 2Y_{t+1}\alpha'X_t + (\alpha'X_t)^2]$.
III. Aplicando a linearidade da esperan√ßa, obtemos:
$MSE = E[Y_{t+1}^2] - 2\alpha'E[X_tY_{t+1}] + \alpha'E[X_tX_t']\alpha$.
IV.  Sabemos que $\alpha' = E[Y_{t+1}X_t][E(X_tX_t')]^{-1}$, ent√£o $\alpha'E[X_tX_t'] = E[Y_{t+1}X_t]$. Substituindo, temos:
$MSE = E[Y_{t+1}^2] - 2\alpha'E[X_tY_{t+1}] + \alpha'E[X_tY_{t+1}]$.
V.  Simplificando: $MSE = E[Y_{t+1}^2] - \alpha'E[X_tY_{t+1}]$. $\blacksquare$

### A Rela√ß√£o com a Expectativa Condicional
A proje√ß√£o linear $\alpha'X_t$ busca a melhor aproxima√ß√£o linear de $Y_{t+1}$ com base em $X_t$, dentro do espa√ßo de fun√ß√µes lineares. A condi√ß√£o de ortogonalidade entre o erro de previs√£o e as vari√°veis explicativas √© an√°loga √† condi√ß√£o de ortogonalidade que define a esperan√ßa condicional, $E[Y_{t+1} - E(Y_{t+1}|X_t)|h(X_t)] = 0$, onde $h(X_t)$ √© qualquer fun√ß√£o de $X_t$. A diferen√ßa √© que a proje√ß√£o linear se restringe a fun√ß√µes lineares de $X_t$, enquanto a esperan√ßa condicional abrange todas as fun√ß√µes poss√≠veis de $X_t$. No entanto, quando a esperan√ßa condicional  $E(Y_{t+1}|X_t)$ √© linear em $X_t$, ou seja,  $E(Y_{t+1}|X_t) = \gamma'X_t$, a proje√ß√£o linear coincide com a expectativa condicional, e a escolha de $\alpha$ minimiza o MSE entre todas as fun√ß√µes lineares de $X_t$.

A intui√ß√£o por tr√°s da minimiza√ß√£o do MSE por meio da proje√ß√£o linear √© que, ao for√ßar o erro de previs√£o a ser n√£o correlacionado com as vari√°veis explicativas, estamos garantindo que n√£o h√° nenhum componente linearmente relevante de $X_t$ que possamos usar para melhorar a previs√£o. Isso √© semelhante √† condi√ß√£o de ortogonalidade na proje√ß√£o em espa√ßos vetoriais, onde o vetor de proje√ß√£o √© ortogonal ao vetor de erro, o que representa a melhor aproxima√ß√£o linear do vetor original.

> üí° **Exemplo Num√©rico:**
> Suponha que a rela√ß√£o real entre o n√∫mero de horas de estudo ($X_t$) e a nota de um aluno em um exame ($Y_{t+1}$) seja expressa como $E(Y_{t+1}|X_t) = 2X_t + X_t^2$, ou seja, a rela√ß√£o entre as vari√°veis √© n√£o linear. A proje√ß√£o linear busca a melhor aproxima√ß√£o linear dessa rela√ß√£o. Suponha que temos os dados abaixo:
>
> | t   | $X_t$ | $Y_{t+1}$ |
> |-----|-------|-----------|
> | 1   | 1     | 3         |
> | 2   | 2     | 8         |
> | 3   | 3     | 15        |
> | 4   | 4     | 25        |
> | 5   | 5     | 35        |
>
> Calculamos o vetor $\alpha$ da proje√ß√£o linear $\hat{Y}_{t+1} = \alpha X_t$:
>
> 1.  **Calculamos** $E[X_t^2]$:
>  $$E[X_t^2] = \frac{1^2 + 2^2 + 3^2 + 4^2 + 5^2}{5} = \frac{1+4+9+16+25}{5} = 11$$
>
> 2.  **Calculamos** $E[Y_{t+1}X_t]$:
> $$E[Y_{t+1}X_t] = \frac{1\cdot3 + 2\cdot8 + 3\cdot15 + 4\cdot25 + 5\cdot35}{5} = \frac{3+16+45+100+175}{5} = 67.8$$
>
> 3. **Calculamos** $\alpha$:
> $$\alpha = \frac{E[Y_{t+1}X_t]}{E[X_t^2]} = \frac{67.8}{11} \approx 6.16$$
>
>  A proje√ß√£o linear √© $\hat{Y}_{t+1} = 6.16X_t$. O MSE dessa proje√ß√£o ser√° o menor poss√≠vel dentro da classe linear.  Entretanto, por ser a rela√ß√£o entre as vari√°veis n√£o linear, a proje√ß√£o linear n√£o captura a rela√ß√£o verdadeira e o MSE n√£o ser√° nulo.  A expectativa condicional, sendo n√£o linear, capturaria melhor a rela√ß√£o e obteria um MSE inferior, por√©m ela √© desconhecida.

###  Formaliza√ß√£o Matem√°tica
Para consolidar o entendimento da otimalidade da proje√ß√£o linear, vamos apresentar as provas formais.

  **Teorema 1:** *A proje√ß√£o linear* $P(Y_{t+1}|X_t) = \alpha'X_t$ *com* $\alpha' = E[Y_{t+1}X_t][E(X_tX_t')]^{-1}$ *minimiza o erro quadr√°tico m√©dio (MSE) dentro da classe de todas as fun√ß√µes lineares de* $X_t$.
  *Proof:*
    
    I. Considere uma fun√ß√£o linear arbitr√°ria $g'X_t$ como previs√£o de $Y_{t+1}$. O MSE associado a essa previs√£o √© $E[(Y_{t+1} - g'X_t)^2]$.
    II. Expandindo o MSE, adicionando e subtraindo a proje√ß√£o linear $\alpha'X_t$, obtemos:
   $$MSE = E[(Y_{t+1} - \alpha'X_t + \alpha'X_t - g'X_t)^2]$$
    III.  Expandindo o quadrado:
   $$MSE = E[(Y_{t+1} - \alpha'X_t)^2 + 2(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t) + (\alpha'X_t - g'X_t)^2]$$
    IV. Aplicando a linearidade da esperan√ßa:
   $$MSE = E[(Y_{t+1} - \alpha'X_t)^2] + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t)] + E[(\alpha'X_t - g'X_t)^2]$$
     V. O termo intermedi√°rio √© zero pela propriedade de ortogonalidade:
      $$2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t)] = 2E[E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t)|X_t]] = 2E[0] = 0$$
      j√° que $E[(Y_{t+1} - \alpha'X_t)X_t]=0'$
     VI.  Portanto:
    $$MSE = E[(Y_{t+1} - \alpha'X_t)^2] + E[(\alpha'X_t - g'X_t)^2]$$
    VII. O segundo termo da express√£o acima √© sempre n√£o negativo, portanto, o MSE √© minimizado quando o segundo termo √© zero, o que ocorre quando $\alpha'X_t = g'X_t$.
    VIII. Logo, a proje√ß√£o linear $\alpha'X_t$ minimiza o MSE dentro da classe de fun√ß√µes lineares de $X_t$. $\blacksquare$

**Lema 1:** (Equival√™ncia do MSE quando a Expectativa Condicional √© Linear) *Se a expectativa condicional* $E(Y_{t+1}|X_t)$ *√© uma fun√ß√£o linear de* $X_t$, *ent√£o a proje√ß√£o linear* $P(Y_{t+1}|X_t)$ *produz o mesmo erro quadr√°tico m√©dio (MSE) que a expectativa condicional.*

*Proof:*
 I. Assumimos que $E(Y_{t+1}|X_t) = \beta'X_t$. O MSE da expectativa condicional √© $MSE_{EC} = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2]$.
 II.  O MSE da proje√ß√£o linear √© $MSE_{PL} = E[(Y_{t+1} - \alpha'X_t)^2]$, onde $\alpha' = E[Y_{t+1}X_t] [E(X_tX_t')]^{-1}$.
 III. Se $E(Y_{t+1}|X_t) = \beta'X_t$, ent√£o $\alpha = \beta$ e $E(Y_{t+1}|X_t) = \alpha'X_t$, ent√£o,
 IV. $MSE_{EC} = E[(Y_{t+1} - \alpha'X_t)^2] = MSE_{PL}$. $\blacksquare$

**Lema 2:** (Decomposi√ß√£o da Vari√¢ncia e o MSE) *A vari√¢ncia de* $Y_{t+1}$ *pode ser decomposta como a soma da vari√¢ncia da proje√ß√£o linear* $\alpha'X_t$ *e do erro quadr√°tico m√©dio (MSE) da proje√ß√£o linear, representando o erro de previs√£o* $e_{t+1}$.
  
 $$Var(Y_{t+1}) = Var(\alpha'X_t) + MSE(Y_{t+1}|X_t)$$
*Proof:*
I.  Sabemos que $Y_{t+1} = \alpha'X_t + e_{t+1}$, onde $e_{t+1} = Y_{t+1} - \alpha'X_t$ √© o erro de previs√£o.
II.  Calculando a vari√¢ncia:
$$Var(Y_{t+1}) = Var(\alpha'X_t + e_{t+1})$$
III.  Expandindo:
$$Var(Y_{t+1}) = Var(\alpha'X_t) + Var(e_{t+1}) + 2Cov(\alpha'X_t, e_{t+1})$$
IV. Pela condi√ß√£o de ortogonalidade, o termo da covari√¢ncia √© igual a zero, ou seja, $Cov(\alpha'X_t, e_{t+1})=0$.
V.  Portanto:
 $$Var(Y_{t+1}) = Var(\alpha'X_t) + Var(e_{t+1})$$
VI. Dado que $Var(e_{t+1})$ √© o MSE da proje√ß√£o linear:
  $$Var(Y_{t+1}) = Var(\alpha'X_t) + MSE(Y_{t+1}|X_t)$$ $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Vamos usar os dados do primeiro exemplo num√©rico para demonstrar o lema 2. No exemplo, calculamos $\alpha \approx 2.49$. Com os dados de $X_t$: 1, 2, 3, 4, e 5, as proje√ß√µes $\hat{Y}_{t+1} = \alpha X_t$ s√£o: 2.49, 4.98, 7.47, 9.96, e 12.45. Os valores de $Y_{t+1}$ s√£o: 3, 5, 8, 10 e 12.
>
> 1. **Calculamos** $Var(Y_{t+1})$:
>  $$E[Y_{t+1}] = \frac{3+5+8+10+12}{5} = 7.6$$
>  $$Var(Y_{t+1}) = \frac{(3-7.6)^2 + (5-7.6)^2 + (8-7.6)^2 + (10-7.6)^2 + (12-7.6)^2}{5} = \frac{21.16+6.76+0.16+5.76+19.36}{5} = \frac{53.2}{5} = 10.64$$
>
> 2. **Calculamos** $Var(\alpha'X_t) = Var(\hat{Y}_{t+1})$:
> $$E[\hat{Y}_{t+1}] = \frac{2.49+4.98+7.47+9.96+12.45}{5} = 7.47$$
> $$Var(\hat{Y}_{t+1}) = \frac{(2.49-7.47)^2+(4.98-7.47)^2+(7.47-7.47)^2+(9.96-7.47)^2+(12.45-7.47)^2}{5} = \frac{24.80+6.20+0+6.20+24.80}{5} = \frac{62}{5} = 12.4 $$
>
> 3. **Calculamos** $MSE(Y_{t+1}|X_t)$ (j√° calculado no primeiro exemplo):
> $$MSE(Y_{t+1}|X_t) = 0.15$$
>
> 4. **Verificamos a decomposi√ß√£o da vari√¢ncia**:
> $$Var(Y_{t+1}) = 10.64$$
> $$Var(\alpha'X_t) + MSE(Y_{t+1}|X_t) = 12.4 + 0.15 = 12.55 $$
>
> Os valores n√£o s√£o exatamente iguais por causa dos arredondamentos, mas s√£o pr√≥ximos e demonstram o lema 2.

**Proposi√ß√£o 1:** (Unicidade do Vetor $\alpha$ na Proje√ß√£o Linear) *O vetor* $\alpha$ *que minimiza o MSE na proje√ß√£o linear* $P(Y_{t+1}|X_t) = \alpha'X_t$ *√© √∫nico se a matriz* $E(X_tX_t')$ *for n√£o singular.*
*Proof:*
I.  Partimos da defini√ß√£o do vetor $\alpha$ que minimiza o MSE: $\alpha' = E(Y_{t+1}X_t)[E(X_tX_t')]^{-1}$.
II.  Se a matriz $E(X_tX_t')$ for n√£o singular, ent√£o sua inversa $[E(X_tX_t')]^{-1}$ √© √∫nica.
III. Como a unicidade da inversa garante a unicidade da solu√ß√£o para $\alpha$, conclu√≠mos que o vetor  $\alpha$ √© √∫nico. $\blacksquare$
  
**Teorema 1.1:** (Decomposi√ß√£o da vari√¢ncia do erro da proje√ß√£o linear) *O MSE da proje√ß√£o linear pode ser decomposto na vari√¢ncia do erro da proje√ß√£o linear e o erro de previs√£o da expectativa condicional* $E(Y_{t+1}|X_t)$
$$ MSE(Y_{t+1}|X_t) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] + E[(E(Y_{t+1}|X_t) - \alpha'X_t)^2] $$

*Proof:*
I. Sabemos que $Y_{t+1} = E(Y_{t+1}|X_t) + \epsilon_{t+1}$, onde $\epsilon_{t+1}$ √© o erro de previs√£o da esperan√ßa condicional, tal que $E[\epsilon_{t+1}|X_t]=0$.
II. O MSE da proje√ß√£o linear √© dado por $MSE(Y_{t+1}|X_t) = E[(Y_{t+1} - \alpha'X_t)^2]$
III.  Substituindo $Y_{t+1}$ na equa√ß√£o do MSE:
$MSE(Y_{t+1}|X_t) = E[(E(Y_{t+1}|X_t) + \epsilon_{t+1} - \alpha'X_t)^2]$.
IV.  Adicionando e subtraindo $E(Y_{t+1}|X_t)$:
$MSE(Y_{t+1}|X_t) = E[(E(Y_{t+1}|X_t) - \alpha'X_t + \epsilon_{t+1})^2]$
V. Expandindo o quadrado:
$MSE(Y_{t+1}|X_t) = E[(E(Y_{t+1}|X_t) - \alpha'X_t)^2 + 2(E(Y_{t+1}|X_t) - \alpha'X_t)\epsilon_{t+1} + \epsilon_{t+1}^2]$
VI. Aplicando a linearidade da esperan√ßa:
$MSE(Y_{t+1}|X_t) = E[(E(Y_{t+1}|X_t) - \alpha'X_t)^2] + 2E[(E(Y_{t+1}|X_t) - \alpha'X_t)\epsilon_{t+1}] + E[\epsilon_{t+1}^2]$
VII. Dado que $E[\epsilon_{t+1}|X_t]=0$, o termo $2E[(E(Y_{t+1}|X_t) - \alpha'X_t)\epsilon_{t+1}]=0$.
VIII. Portanto:
$MSE(Y_{t+1}|X_t) = E[(E(Y_{t+1}|X_t) - \alpha'X_t)^2] + E[\epsilon_{t+1}^2]$
IX. O termo $E[\epsilon_{t+1}^2]$ corresponde a vari√¢ncia do erro de previs√£o da expectativa condicional, $E[(Y_{t+1} - E(Y_{t+1}|X_t))^2]$ e $E[(E(Y_{t+1}|X_t) - \alpha'X_t)^2]$ corresponde ao erro da aproxima√ß√£o linear da expectativa condicional. Assim:
$$ MSE(Y_{t+1}|X_t) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] + E[(E(Y_{t+1}|X_t) - \alpha'X_t)^2] $$
$\blacksquare$

### Conclus√£o
A decomposi√ß√£o do MSE associado √† proje√ß√£o linear demonstra que o vetor $\alpha$ que minimiza o MSE dentro da classe de previs√µes lineares √© precisamente o vetor $\alpha$ da proje√ß√£o linear. O termo intermedi√°rio na expans√£o do MSE se anula devido √† condi√ß√£o de n√£o correla√ß√£o entre o erro e as vari√°veis explicativas, um resultado que se assemelha ao caso da expectativa condicional. Embora a proje√ß√£o linear esteja restrita a fun√ß√µes lineares, a prova matem√°tica apresentada aqui, refor√ßada por exemplos num√©ricos, estabelece sua otimalidade dentro do seu dom√≠nio e sua relev√¢ncia pr√°tica como um m√©todo eficaz de previs√£o.

### Refer√™ncias
[^1]: Expression [4.1.1], [4.1.2], [4.1.3], [4.1.4], [4.1.5], [4.1.6], [4.1.7], [4.1.8]
[^2]: [4.1.9], [4.1.10], [4.1.11], [4.1.12], [4.1.13]
<!-- END -->
