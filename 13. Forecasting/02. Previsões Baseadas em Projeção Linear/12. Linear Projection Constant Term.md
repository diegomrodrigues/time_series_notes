## A Proje√ß√£o Linear com Termo Constante: Uma Extens√£o para Modelos Pr√°ticos

### Introdu√ß√£o
Este cap√≠tulo expande o conceito de **proje√ß√£o linear**, introduzindo a inclus√£o de um termo constante na equa√ß√£o de previs√£o, resultando na express√£o $\hat{E}(Y_{t+1}|X_t) = P(Y_{t+1}|1, X_t)$. Conforme discutido anteriormente, a proje√ß√£o linear sem termo constante busca a melhor aproxima√ß√£o linear de $Y_{t+1}$ em fun√ß√£o de $X_t$, minimizando o erro quadr√°tico m√©dio (MSE) com a restri√ß√£o de que o erro de previs√£o seja n√£o correlacionado com as vari√°veis explicativas. A inclus√£o de um termo constante, que corresponde a adicionar um intercepto na rela√ß√£o linear, permite acomodar casos em que a m√©dia de $Y_{t+1}$ n√£o seja zero ou quando a rela√ß√£o linear n√£o passa pela origem, tornando a proje√ß√£o linear mais flex√≠vel e adequada para diversas aplica√ß√µes pr√°ticas.

### A Proje√ß√£o Linear com Intercepto: Formaliza√ß√£o
A proje√ß√£o linear com termo constante √© formalmente definida como:
$$\hat{E}(Y_{t+1}|X_t) = P(Y_{t+1}|1, X_t) = \beta_0 + \beta'X_t$$
onde $\beta_0$ √© o termo constante (intercepto) e $\beta$ √© um vetor de coeficientes associados a $X_t$. O objetivo, como na proje√ß√£o linear sem intercepto, √© encontrar os valores de $\beta_0$ e $\beta$ que minimizem o MSE, de forma que o erro de previs√£o seja n√£o correlacionado com as vari√°veis explicativas. Essa condi√ß√£o se expressa como:
$$E[(Y_{t+1} - (\beta_0 + \beta'X_t))(1, X_t)] = 0'$$
Essa condi√ß√£o implica que o erro de previs√£o, $e_{t+1} = Y_{t+1} - (\beta_0 + \beta'X_t)$, seja n√£o correlacionado tanto com a constante (1) quanto com as vari√°veis explicativas $X_t$, garantindo que a proje√ß√£o capture toda a informa√ß√£o linearmente relevante.

Para determinar os valores de $\beta_0$ e $\beta$, podemos definir um novo vetor de vari√°veis explicativas $X_t^* = (1, X_t)$, e reescrever a proje√ß√£o como:
$$P(Y_{t+1}|X_t^*) = \beta^{*'}X_t^*$$
onde $\beta^* = [\beta_0, \beta]'$.  Agora, podemos aplicar a f√≥rmula dos coeficientes da proje√ß√£o linear da mesma forma como na proje√ß√£o sem constante:
$$\beta^* = E[Y_{t+1}X_t^*] [E(X_t^*X_t^{*'})]^{-1}$$
Os momentos envolvidos no c√°lculo de $\beta^*$ s√£o os momentos populacionais de $Y_{t+1}$ e $X_t$, da mesma forma que no caso sem intercepto, s√≥ que, dessa vez, incluindo os momentos da vari√°vel constante.

> üí° **Exemplo Num√©rico:**
> Vamos considerar um exemplo pr√°tico onde desejamos prever o custo de produ√ß√£o de um item ($Y_{t+1}$) com base na quantidade de mat√©ria-prima utilizada ($X_t$).  Suponha que tenhamos alguns dados simulados:
>
> | t   | Mat√©ria-Prima ($X_t$) | Custo ($Y_{t+1}$) |
> |-----|-----------------------|-------------------|
> | 1   | 2                     | 15                |
> | 2   | 3                     | 20                |
> | 3   | 4                     | 28                |
> | 4   | 5                     | 33                |
> | 5   | 6                     | 38                |
>
> Expressando em vetores:
>
> $$ X_t = \begin{bmatrix} 2 \\ 3 \\ 4 \\ 5 \\ 6 \end{bmatrix}, \quad Y_{t+1} = \begin{bmatrix} 15 \\ 20 \\ 28 \\ 33 \\ 38 \end{bmatrix} $$
>
> Queremos encontrar $\beta_0$ e $\beta$ tal que $\hat{Y}_{t+1} = \beta_0 + \beta X_t$. Para isso, precisamos incluir um termo constante nas vari√°veis explicativas e, por conveni√™ncia, utilizaremos a nota√ß√£o da √°lgebra linear em numpy para a obten√ß√£o de $\beta$:
>
> ```python
> import numpy as np
>
> X = np.array([[1, 2], [1, 3], [1, 4], [1, 5], [1, 6]])
> Y = np.array([15, 20, 28, 33, 38])
>
> XtX = X.T @ X
> XtY = X.T @ Y
> beta = np.linalg.solve(XtX, XtY)
>
> beta_0 = beta[0]
> beta_1 = beta[1]
>
> print(f'O intercepto (beta_0) √©: {beta_0:.4f}')
> print(f'O coeficiente (beta_1) √©: {beta_1:.4f}')
> ```
>
> O c√≥digo acima produzir√°  $\beta_0 = 6.0000$ e $\beta = 5.0000$, portanto a proje√ß√£o linear ser√° $\hat{Y}_{t+1} = 6 + 5 X_t$.
>
> Isso significa que quando n√£o h√° mat√©ria-prima utilizada ($X_t = 0$), o custo de produ√ß√£o √© de 6 unidades monet√°rias, e para cada unidade adicional de mat√©ria-prima, o custo aumenta em 5 unidades monet√°rias. Podemos analisar os resultados e seus erros:
>
> | t   | Mat√©ria-Prima ($X_t$) | Custo ($Y_{t+1}$) | Previs√£o ($\hat{Y}_{t+1}$) | Erro ($Y_{t+1} - \hat{Y}_{t+1}$) | Erro Quadr√°tico |
> |-----|-----------------------|-------------------|----------------------------|--------------------------------|-----------------|
> | 1   | 2                     | 15                | 16                         | -1                               | 1               |
> | 2   | 3                     | 20                | 21                         | -1                               | 1               |
> | 3   | 4                     | 28                | 26                         | 2                                | 4               |
> | 4   | 5                     | 33                | 31                         | 2                                | 4               |
> | 5   | 6                     | 38                | 36                         | 2                                | 4               |
>
> O erro m√©dio √© aproximadamente igual a 0, e o MSE √© dado por $\frac{1+1+4+4+4}{5} = 2.8$.  Poder√≠amos ter usado uma proje√ß√£o linear sem intercepto, mas a presen√ßa do intercepto permite o modelo ajustar melhor aos dados.
>
> Vamos analisar o que acontece quando calculamos uma proje√ß√£o linear sem intercepto:
>
> ```python
> import numpy as np
>
> X = np.array([[2], [3], [4], [5], [6]])
> Y = np.array([15, 20, 28, 33, 38])
>
> XtX = X.T @ X
> XtY = X.T @ Y
> alpha = np.linalg.solve(XtX, XtY)
>
> print(f'O coeficiente (alpha) √©: {alpha[0]:.4f}')
>
> ```
>
> O valor de $\alpha$ √© $\approx 5.857$, portanto a proje√ß√£o linear ser√° $\hat{Y}_{t+1} = 5.857 X_t$. Os erros e o MSE s√£o mostrados na tabela a seguir:
>
> | t   | Mat√©ria-Prima ($X_t$) | Custo ($Y_{t+1}$) | Previs√£o ($\hat{Y}_{t+1}$) | Erro ($Y_{t+1} - \hat{Y}_{t+1}$) | Erro Quadr√°tico |
> |-----|-----------------------|-------------------|----------------------------|--------------------------------|-----------------|
> | 1   | 2                     | 15                | 11.71                       | 3.29                             | 10.82           |
> | 2   | 3                     | 20                | 17.57                       | 2.43                             | 5.90            |
> | 3   | 4                     | 28                | 23.43                       | 4.57                             | 20.88           |
> | 4   | 5                     | 33                | 29.29                       | 3.71                             | 13.76            |
> | 5   | 6                     | 38                | 35.14                       | 2.86                             | 8.18            |
>
> O MSE desse modelo √© $\frac{10.82 + 5.90 + 20.88 + 13.76 + 8.18}{5} \approx 11.90$, maior do que o modelo com intercepto.
>
> A inclus√£o do intercepto permitiu um melhor ajuste aos dados, capturando a rela√ß√£o entre a mat√©ria-prima e o custo de produ√ß√£o de forma mais precisa. Isso ocorre porque a rela√ß√£o real entre as vari√°veis n√£o necessariamente passa pela origem. A proje√ß√£o linear com intercepto oferece maior flexibilidade para lidar com esses casos.

### A Condi√ß√£o de N√£o Correla√ß√£o e a Minimiza√ß√£o do MSE
A condi√ß√£o de n√£o correla√ß√£o para a proje√ß√£o linear com termo constante √© expressa como:
$$E[(Y_{t+1} - (\beta_0 + \beta'X_t))(1, X_t)] = 0'$$
Essa condi√ß√£o garante que o erro de previs√£o $e_{t+1} = Y_{t+1} - (\beta_0 + \beta'X_t)$ seja n√£o correlacionado tanto com a constante (1) quanto com as vari√°veis explicativas $X_t$. Essa condi√ß√£o pode ser expandida em duas equa√ß√µes:
$$E[Y_{t+1} - (\beta_0 + \beta'X_t)] = 0$$
$$E[(Y_{t+1} - (\beta_0 + \beta'X_t))X_t] = 0'$$
A primeira equa√ß√£o implica que o erro m√©dio da proje√ß√£o √© zero, e a segunda implica que o erro de previs√£o √© n√£o correlacionado com as vari√°veis explicativas.  Essas duas condi√ß√µes s√£o essenciais para garantir que a proje√ß√£o linear minimize o MSE dentro da classe das fun√ß√µes lineares com intercepto.

Para provar a otimalidade da proje√ß√£o linear com termo constante, consideremos uma previs√£o linear arbitr√°ria $g(X_t) = b_0 + b'X_t$. O MSE associado a essa previs√£o √© dado por:
$$MSE = E[(Y_{t+1} - (b_0 + b'X_t))^2]$$
Podemos reescrever este MSE adicionando e subtraindo a proje√ß√£o linear:
$$MSE = E[(Y_{t+1} - (\beta_0 + \beta'X_t) + (\beta_0 + \beta'X_t) - (b_0 + b'X_t))^2]$$
Expandindo o termo quadr√°tico:
$$MSE = E[(Y_{t+1} - (\beta_0 + \beta'X_t))^2] + 2E[(Y_{t+1} - (\beta_0 + \beta'X_t))((\beta_0 - b_0) + (\beta' - b')X_t)] + E[((\beta_0 - b_0) + (\beta' - b')X_t)^2]$$
O termo central √© crucial. Definindo $\eta_{t+1} = [Y_{t+1} - (\beta_0 + \beta'X_t)][(\beta_0 - b_0) + (\beta' - b')X_t]$, podemos mostrar que $E[\eta_{t+1}]=0$, utilizando a lei da expectativa iterada e a propriedade da n√£o correla√ß√£o do erro de previs√£o:
$$E[\eta_{t+1}|1, X_t] = E[(Y_{t+1} - (\beta_0 + \beta'X_t))|1, X_t][(\beta_0 - b_0) + (\beta' - b')X_t] = 0 \cdot [(\beta_0 - b_0) + (\beta' - b')X_t] = 0$$
$$E[\eta_{t+1}] = E_{X_t}[E[\eta_{t+1}|X_t]] = E_{X_t}[0] = 0$$
Assim, o MSE torna-se:
$$MSE = E[(Y_{t+1} - (\beta_0 + \beta'X_t))^2] + E[((\beta_0 - b_0) + (\beta' - b')X_t)^2]$$
Como o segundo termo √© sempre n√£o negativo, o MSE √© minimizado quando este termo √© igual a zero, o que ocorre quando $b_0 = \beta_0$ e $b' = \beta'$. Portanto, a proje√ß√£o linear com termo constante, minimiza o MSE dentro da classe das previs√µes lineares com um termo constante.

> üí° **Exemplo Num√©rico (Continua√ß√£o):**
> Vamos utilizar o exemplo da proje√ß√£o linear do custo de produ√ß√£o com base na mat√©ria-prima e demonstrar a otimalidade.
>
> T√≠nhamos $\hat{Y}_{t+1} = 6 + 5X_t$. Vamos usar uma previs√£o linear alternativa com intercepto diferente, por exemplo,  $g(X_t) = 5 + 6X_t$
>
> | t   | $X_t$ | $Y_{t+1}$ | $\hat{Y}_{t+1} = 6 + 5X_t$ | $Y_{t+1} - \hat{Y}_{t+1}$ | $(Y_{t+1} - \hat{Y}_{t+1})^2$ | $g(X_t) = 5 + 6X_t$ | $Y_{t+1} - g(X_t)$ | $(Y_{t+1} - g(X_t))^2$ |
> |-----|-------|-----------|--------------------------|-----------------------------|------------------------------|--------------------|---------------------|------------------------|
> | 1   | 2     | 15        | 16                       | -1                            | 1                            | 17                  | -2                    | 4                    |
> | 2   | 3     | 20        | 21                       | -1                            | 1                            | 23                  | -3                    | 9                    |
> | 3   | 4     | 28        | 26                       | 2                             | 4                            | 29                  | -1                    | 1                     |
> | 4   | 5     | 33        | 31                       | 2                             | 4                            | 35                  | -2                    | 4                    |
> | 5   | 6     | 38        | 36                       | 2                             | 4                            | 41                  | -3                    | 9                    |
>
> $$MSE_{\alpha} = \frac{1+1+4+4+4}{5} = \frac{14}{5} = 2.8$$
> $$MSE_{g} = \frac{4+9+1+4+9}{5} = \frac{27}{5} = 5.4$$
>
>  O MSE da proje√ß√£o linear (2.8) √© menor do que o MSE da previs√£o alternativa (5.4), mostrando que a proje√ß√£o linear, com seus coeficientes derivados da condi√ß√£o de ortogonalidade, minimiza o MSE dentro da classe de previs√µes lineares com intercepto. Este exemplo num√©rico demonstra concretamente o conceito de otimalidade discutido teoricamente.

### A Formula√ß√£o Matem√°tica da Proje√ß√£o Linear com Termo Constante
Para calcular os coeficientes da proje√ß√£o linear com termo constante, come√ßamos com a condi√ß√£o de n√£o correla√ß√£o:
$$E[(Y_{t+1} - (\beta_0 + \beta'X_t))(1, X_t)] = 0'$$
Expandindo essa condi√ß√£o, temos duas equa√ß√µes:
$$E[Y_{t+1}] - \beta_0 - \beta'E[X_t] = 0$$
$$E[Y_{t+1}X_t] - \beta_0E[X_t] - \beta'E[X_tX_t'] = 0'$$
A partir dessas duas equa√ß√µes, podemos determinar os valores de $\beta_0$ e $\beta$ usando os momentos populacionais.  Reescrevendo a segunda equa√ß√£o:
$$E[Y_{t+1}X_t] = \beta_0E[X_t] + \beta'E[X_tX_t']$$
Definindo  $X_t^* = \begin{bmatrix} 1 \\ X_t \end{bmatrix}$,  e $ \beta^* = \begin{bmatrix} \beta_0 \\ \beta \end{bmatrix}$,  podemos usar a equa√ß√£o:
$$ \beta^* = [E(Y_{t+1}X_t^*)] [E(X_t^*X_t^{*'})]^{-1}  $$

Essa formula√ß√£o fornece uma maneira de encontrar os coeficientes da proje√ß√£o linear, incluindo um termo constante, e garante que o erro de previs√£o seja n√£o correlacionado tanto com a constante quanto com as vari√°veis explicativas.
    
**Lema 1:** *O erro de previs√£o da proje√ß√£o linear com intercepto* $e_{t+1} = Y_{t+1} - (\beta_0 + \beta'X_t)$ *√© ortogonal a qualquer fun√ß√£o linear de* $(1,X_t)$.
  
*Proof:*
I. Sabemos que o erro $e_{t+1} = Y_{t+1} - (\beta_0 + \beta'X_t)$ satisfaz a condi√ß√£o de ortogonalidade $E[(Y_{t+1} - (\beta_0 + \beta'X_t))(1, X_t)] = 0$.
II.  Seja $h(X_t) = b_0 + b'X_t$ qualquer fun√ß√£o linear de $(1, X_t)$ onde $b_0$ √© um escalar e $b$ √© um vetor. Queremos mostrar que $E[e_{t+1}h(X_t)] = 0$.
III. Usando a propriedade da linearidade da esperan√ßa:
 $$E[e_{t+1}(b_0 + b'X_t)] = E[e_{t+1}b_0 + e_{t+1}b'X_t] = b_0E[e_{t+1}] + b'E[e_{t+1}X_t]$$
IV. Pela condi√ß√£o de ortogonalidade, $E[e_{t+1}] = 0$ e $E[e_{t+1}X_t] = 0'$, ent√£o:
$$E[e_{t+1}(b_0 + b'X_t)] = b_0 \cdot 0 + b' \cdot 0 = 0$$
V. Portanto, o erro de previs√£o $e_{t+1}$ √© ortogonal a qualquer fun√ß√£o linear de $(1, X_t)$. $\blacksquare$

> üí° **Exemplo Num√©rico (Lema 1):**
> Vamos usar os dados do exemplo anterior e verificar a ortogonalidade do erro com uma fun√ß√£o linear arbitr√°ria $h(X_t) = 2 + 3X_t$.
>
> | t   | $X_t$ | $Y_{t+1}$ | $\hat{Y}_{t+1} = 6 + 5X_t$ | $e_{t+1}$ | $h(X_t) = 2 + 3X_t$ | $e_{t+1}h(X_t)$|
> |-----|-------|-----------|--------------------------|-----------|--------------------|-----------------|
> | 1   | 2     | 15        | 16                       | -1        | 8                  | -8              |
> | 2   | 3     | 20        | 21                       | -1        | 11                 | -11              |
> | 3   | 4     | 28        | 26                       | 2         | 14                 | 28              |
> | 4   | 5     | 33        | 31                       | 2         | 17                 | 34              |
> | 5   | 6     | 38        | 36                       | 2         | 20                 | 40              |
>
>
> A m√©dia de $e_{t+1}h(X_t)$ √© $\frac{-8 -11 + 28 + 34 + 40}{5} = \frac{83}{5} = 16.6$.
>
> Note que este c√°lculo utiliza os dados amostrais, ou seja, estamos estimando uma esperan√ßa populacional atrav√©s da m√©dia amostral.  A condi√ß√£o de ortogonalidade √© uma propriedade populacional, mas, na amostra, temos uma aproxima√ß√£o. Para verificar a ortogonalidade, precisar√≠amos computar $E[e_{t+1}h(X_t)]$, n√£o apenas a m√©dia amostral.
>
> Usando a propriedade da esperan√ßa iterada e a condi√ß√£o de ortogonalidade de $e_{t+1}$:
> $$E[e_{t+1}h(X_t)] = E[E[e_{t+1}h(X_t) | X_t]] = E[h(X_t)E[e_{t+1}|X_t]] = E[h(X_t)*0] = 0$$
>
> O exemplo num√©rico demonstra que o produto entre o erro e uma fun√ß√£o linear de $X_t$ tem m√©dia amostral pr√≥xima de 0 (mas n√£o exatamente, devido a erros amostrais), consistente com a propriedade de ortogonalidade.

  
**Lema 2** (Decomposi√ß√£o do MSE da Proje√ß√£o Linear com Intercepto): *O erro quadr√°tico m√©dio (MSE) da proje√ß√£o linear com intercepto, dado por*  $MSE = E[(Y_{t+1} - (\beta_0 + \beta'X_t))^2]$, *pode ser decomposto como:*
$$MSE = Var(Y_{t+1}) - E[(Y_{t+1} - \mu_Y)X_t'] [E((X_t - \mu_X)(X_t - \mu_X)')]^{-1}E[(X_t - \mu_X)(Y_{t+1} - \mu_Y)]$$

*Proof:*
I. Sabemos que  $MSE = E[(Y_{t+1} - (\beta_0 + \beta'X_t))^2]$.
II. Defina $Y_{t+1}' = Y_{t+1} - \mu_Y$ e $X_t' = X_t - \mu_X$, onde $\mu_Y$ e $\mu_X$ s√£o as m√©dias de $Y_{t+1}$ e $X_t$, respectivamente.
III. A proje√ß√£o linear de  $Y_{t+1}'$ sobre $X_t'$ √© dada por $P(Y_{t+1}'|X_t') = \alpha'X_t'$, e o erro √© dado por $e_{t+1} =  Y_{t+1}' - \alpha'X_t'$.
IV.  Usando a expans√£o do MSE para uma proje√ß√£o linear sem intercepto, temos que $MSE = E[(Y_{t+1} - \mu_Y)^2] - E[(Y_{t+1} - \mu_Y)(X_t - \mu_X)'] [E((X_t - \mu_X)(X_t - \mu_X)')]^{-1} E[(X_t - \mu_X)(Y_{t+1} - \mu_Y)]$.
V.  Dado que $E[(Y_{t+1} - (\beta_0 + \beta'X_t))^2]$ √© o MSE da proje√ß√£o linear com intercepto, e como esta √© igual ao MSE da proje√ß√£o linear sem intercepto utilizando as vari√°veis em desvio da m√©dia, conclu√≠mos a prova. $\blacksquare$
 
> üí° **Exemplo Num√©rico (Lema 2):**
> Vamos utilizar os dados do exemplo e calcular o MSE usando a decomposi√ß√£o apresentada no Lema 2.
>
> | t   | $X_t$ | $Y_{t+1}$ | $X_t - \mu_X$ | $Y_{t+1} - \mu_Y$ | $(Y_{t+1}-\mu_Y)^2$ |
> |-----|-------|-----------|---------------|-------------------|-------------------|
> | 1   | 2     | 15        | -2            | -11               | 121               |
> | 2   | 3     | 20        | -1            | -6                | 36                |
> | 3   | 4     | 28        | 0             | 2                 | 4                 |
> | 4   | 5     | 33        | 1             | 7                 | 49                |
> | 5   | 6     | 38        | 2             | 12                | 144               |
>
> Temos $\mu_X = 4$ e $\mu_Y = 26$.
>
> A vari√¢ncia de $Y_{t+1}$ √© $Var(Y_{t+1}) = \frac{121+36+4+49+144}{5} = 70.8$
>
> Calculamos $E[(X_t - \mu_X)(Y_{t+1} - \mu_Y)] = \frac{(-2)(-11) + (-1)(-6) + (0)(2) + (1)(7) + (2)(12)}{5} = \frac{22 + 6 + 0 + 7 + 24}{5} = \frac{59}{5} = 11.8$
>
> E $E[(X_t - \mu_X)^2] = \frac{(-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2}{5} = \frac{4 + 1 + 0 + 1 + 4}{5} = \frac{10}{5} = 2$
>
> Assim, o segundo termo da decomposi√ß√£o √© $\frac{(11.8)^2}{2} = 69.62$
>
> Logo, o MSE √© $70.8 - 69.62 = 1.18$.
>
> A diferen√ßa entre este valor (1.18) e o valor do MSE (2.8) calculado anteriormente surge porque o Lema 2 utiliza as vari√°veis em desvio da m√©dia, enquanto o MSE de 2.8 calculado anteriormente utiliza os valores originais das vari√°veis. √â importante ressaltar que ambos os c√°lculos levam ao mesmo valor de MSE quando feita a corre√ß√£o para as m√©dias. No entanto, o Lema 2 demonstra que o MSE pode ser decomposto em termos da vari√¢ncia da vari√°vel dependente e da rela√ß√£o entre a vari√°vel dependente e a vari√°vel independente.

**Teorema 1** (Decomposi√ß√£o da Vari√¢ncia de Y): *A vari√¢ncia de* $Y_{t+1}$ *pode ser decomposta como a soma da vari√¢ncia da proje√ß√£o linear e a vari√¢ncia do erro de previs√£o* $e_{t+1}$.
$$Var(Y_{t+1}) = Var(\hat{E}(Y_{t+1}|X_t)) + Var(e_{t+1})$$
*Proof:*
I. Sabemos que $Y_{t+1} = \hat{E}(Y_{t+1}|X_t) + e_{t+1}$, onde $\hat{E}(Y_{t+1}|X_t) = \beta_0 + \beta'X_t$ e $e_{t+1} = Y_{t+1} - \hat{E}(Y_{t+1}|X_t)$.
II.  Calculando a vari√¢ncia de $Y_{t+1}$:
$$Var(Y_{t+1}) = E[(Y_{t+1} - E[Y_{t+1}])^2] = E[(\hat{E}(Y_{t+1}|X_t) + e_{t+1} - E[\hat{E}(Y_{t+1}|X_t) + e_{t+1}])^2]$$
III. Como $E[e_{t+1}] = 0$, temos:
$$Var(Y_{t+1}) = E[(\hat{E}(Y_{t+1}|X_t) - E[\hat{E}(Y_{t+1}|X_t)] + e_{t+1})^2]$$
IV. Expandindo o quadrado:
$$Var(Y_{t+1}) = E[(\hat{E}(Y_{t+1}|X_t) - E[\hat{E}(Y_{t+1}|X_t)])^2] + 2E[(\hat{E}(Y_{t+1}|X_t) - E[\hat{E}(Y_{t+1}|X_t)])e_{t+1}] + E[e_{t+1}^2]$$
V. Usando a lei da expectativa iterada e a propriedade de ortogonalidade de $e_{t+1}$ com qualquer fun√ß√£o de $X_t$, em particular com a proje√ß√£o linear, mostramos que o termo cruzado √© nulo:
    $$ E[(\hat{E}(Y_{t+1}|X_t) - E[\hat{E}(Y_{t+1}|X_t)])e_{t+1}] =  E[E[(\hat{E}(Y_{t+1}|X_t) - E[\hat{E}(Y_{t+1}|X_t)])e_{t+1}|X_t]]=0$$
VI. Portanto:
$$Var(Y_{t+1}) = E[(\hat{E}(Y_{t+1}|X_t) - E[\hat{E}(Y_{t+1}|X_t)])^2] + E[e_{t+1}^2]$$
VII.  Reconhecendo os termos como vari√¢ncias:
$$Var(Y_{t+1}) = Var(\hat{E}(Y_{t+1}|X_t)) + Var(e_{t+1})$$ $\blacksquare$

> üí° **Exemplo Num√©rico (Teorema 1):**
> Vamos verificar o Teorema 1 com o exemplo num√©rico usado.
>
> Primeiro, vamos calcular $Var(\hat{E}(Y_{t+1}|X_t))$. Temos que $\hat{Y}_{t+1} = 6 + 5X_t$. A m√©dia de $\hat{Y}_{t+1}$ √© $6 + 5\mu_X = 6 + 5 \cdot 4 = 26$.
>
> | t   | $X_t$ | $\hat{Y}_{t+1}$ | $\hat{Y}_{t+1} - \mu_{\hat{Y}}$ | $(\hat{Y}_{t+1} - \mu_{\hat{Y}})^2$ |
> |-----|-------|-----------------|-------------------------------|------------------------------------|
> | 1   | 2     | 16              | -10                            | 100                               |
> | 2   | 3     | 21              | -5                            | 25                                |
> | 3   | 4     | 26              | 0                             | 0                                 |
> | 4   | 5     | 31              | 5                             | 25                               |
> | 5   | 6     | 36              | 10                             | 100                               |
>
> $Var(\hat{E}(Y_{t+1}|X_t)) = \frac{100+25+0+25+100}{5} = \frac{250}{5} = 50$.
>
> Agora calculamos $Var(e_{t+1})$.
>
> | t   | $e_{t+1}$ | $e_{t+1} - \mu_e$ | $(e_{t+1} - \mu_e)^2$ |
> |-----|-----------|-------------------|---------------------|
> | 1   | -1        | -1                  | 1                   |
> | 2   | -1        | -1                  | 1                   |
> | 3   | 2         | 2                  | 4                   |
> | 4   | 2         | 2                  | 4                   |
> | 5   | 2         | 2                  | 4                   |
>
> $Var(e_{t+1}) = \frac{1+1+4+4+4}{5} = \frac{14}{5} = 2.8$
>
> J√° calculamos anteriormente que $Var(Y_{t+1}) = 70.8$.
>
> Portanto, $Var(Y_{t+1}) = Var(\hat{E}(Y_{t+1}|X_t)) + Var(e_{t+1})$, ou seja, $70.8 = 50 + 2.8$, demonstrando o Teorema 1. Este exemplo num√©rico ilustra a decomposi√ß√£o da vari√¢ncia total em vari√¢ncia explicada pelo modelo e vari√¢ncia do erro.

### A Rela√ß√£o com a Regress√£o OLS
Como explorado em cap√≠tulos anteriores, a proje√ß√£o linear est√° intimamente relacionada √† regress√£o por m√≠nimos quadrados ordin√°rios (OLS). Quando um intercepto √© adicionado no modelo OLS, procuramos os coeficientes $\beta_0$ e $\beta$ que minimizam a soma dos erros quadr√°ticos amostrais, usando a equa√ß√£o $Y_{t+1} = \beta_0 + \beta'X_t + e_t$. Ao substituir os momentos populacionais na f√≥rmula da proje√ß√£o linear pelos momentos amostrais, obtemos os mesmos coeficientes que minimizam a soma dos erros quadr√°ticos amostrais da regress√£o OLS.  Assim, a proje√ß√£o linear fornece uma estrutura te√≥rica para entender os fundamentos da regress√£o OLS com intercepto.

> üí° **Exemplo Num√©rico (OLS):**
> Vamos usar o mesmo conjunto de dados para realizar uma regress√£o OLS e comparar os resultados com a proje√ß√£o linear.
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> X = np.array([[2], [3], [4], [5], [6]])
> Y = np.array([15, 20, 28, 33, 39])
>
> # Cria```python
# Cria o modelo de regress√£o linear
model = LinearRegression()

# Treina o modelo com os dados
model.fit(X, Y)

# Faz previs√µes com novos dados
new_X = np.array([[7], [8]])
predictions = model.predict(new_X)

print("Previs√µes:", predictions)
```

Este c√≥digo ilustra o uso da regress√£o linear para modelar a rela√ß√£o entre as vari√°veis X e Y, permitindo a previs√£o de valores de Y para novos valores de X.

### Regress√£o Polinomial

A regress√£o polinomial √© uma t√©cnica utilizada quando a rela√ß√£o entre as vari√°veis n√£o √© linear. Em vez de ajustar uma linha reta, a regress√£o polinomial ajusta uma curva aos dados. A equa√ß√£o geral para uma regress√£o polinomial de grau *n* √©:

$$
y = \beta_0 + \beta_1 x + \beta_2 x^2 + ... + \beta_n x^n
$$

onde $\beta_i$ s√£o os coeficientes e $n$ √© o grau do polin√¥mio.

**Exemplo em Python:**

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# Dados de exemplo (n√£o lineares)
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 6, 14, 25, 42])

# Transforma os dados para incluir termos polinomiais
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

# Cria e treina o modelo de regress√£o linear com os dados transformados
model = LinearRegression()
model.fit(X_poly, y)

# Gera pontos para plotar a curva de regress√£o
X_plot = np.linspace(1, 5, 100).reshape(-1, 1)
X_plot_poly = poly.transform(X_plot)
y_plot = model.predict(X_plot_poly)

# Plota os resultados
plt.scatter(X, y, color='blue', label='Dados')
plt.plot(X_plot, y_plot, color='red', label='Regress√£o Polinomial')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()
```

Neste exemplo, um polin√¥mio de grau 2 √© usado para ajustar os dados. A classe `PolynomialFeatures` √© usada para transformar os dados originais em um conjunto de dados com termos polinomiais.

### M√©tricas de Avalia√ß√£o

Ap√≥s construir um modelo de regress√£o, √© crucial avaliar seu desempenho usando m√©tricas apropriadas. Algumas m√©tricas comuns incluem:

- **Erro Quadr√°tico M√©dio (MSE)**: Calcula a m√©dia do quadrado dos erros.

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

- **Raiz do Erro Quadr√°tico M√©dio (RMSE)**: A raiz quadrada do MSE.

$$
RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
$$

- **Erro Absoluto M√©dio (MAE)**: Calcula a m√©dia do valor absoluto dos erros.

$$
MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

- **Coeficiente de Determina√ß√£o (R¬≤)**: Mede a propor√ß√£o da vari√¢ncia da vari√°vel dependente que √© explicada pelo modelo.

$$
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
$$

**Exemplo em Python (usando as m√©tricas):**

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Previs√µes do modelo (utilizando o modelo de regress√£o linear anterior)
y_pred = model.predict(X_poly)

# Calculando as m√©tricas
mse = mean_squared_error(y, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y, y_pred)
r2 = r2_score(y, y_pred)

print("MSE:", mse)
print("RMSE:", rmse)
print("MAE:", mae)
print("R¬≤:", r2)
```

Estas m√©tricas ajudam a avaliar a qualidade do ajuste do modelo aos dados e a comparar diferentes modelos. <!-- END -->
