## Propriedades da Proje√ß√£o Linear: Linearidade e Invari√¢ncia

### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise da **proje√ß√£o linear**, explorando suas propriedades de **linearidade** e **invari√¢ncia**. Como vimos anteriormente, a proje√ß√£o linear busca a melhor aproxima√ß√£o linear de uma vari√°vel $Y_{t+1}$ em fun√ß√£o de um conjunto de vari√°veis explicativas $X_t$, minimizando o erro quadr√°tico m√©dio (MSE). Uma das propriedades mais √∫teis da proje√ß√£o linear √© sua **linearidade**, que permite tratar combina√ß√µes lineares de vari√°veis de maneira direta. Al√©m disso, a propriedade de **invari√¢ncia** garante que a proje√ß√£o de uma transforma√ß√£o linear da vari√°vel dependente seja uma transforma√ß√£o linear da proje√ß√£o original.

### Linearidade da Proje√ß√£o Linear
A propriedade de linearidade da proje√ß√£o linear afirma que a proje√ß√£o de uma combina√ß√£o linear de vari√°veis √© igual √† combina√ß√£o linear das proje√ß√µes individuais. Formalmente, se $a$ e $b$ s√£o constantes determin√≠sticas, ent√£o a proje√ß√£o de $aY_{t+1} + b$ sobre $X_t$ √© dada por:
$$P(aY_{t+1} + b|X_t) = aP(Y_{t+1}|X_t) + b$$
Essa propriedade simplifica muito a an√°lise e o c√°lculo das proje√ß√µes lineares, permitindo tratar transforma√ß√µes lineares de vari√°veis de forma direta.

> üí° **Exemplo Num√©rico:**
> Vamos considerar novamente o exemplo da previs√£o de pre√ßos de casas ($Y_{t+1}$) com base na √°rea ($X_t$). Suponha que recebemos um subs√≠dio do governo de 10000 unidades monet√°rias para cada casa e, devido a infla√ß√£o, o pre√ßo de cada casa aumentar√° 1.2 vezes (120%).
>
> J√° calculamos anteriormente, a proje√ß√£o linear para o pre√ßo da casa como $P(Y_{t+1}|X_t) = \alpha X_t$, onde $\alpha = 2782.22$.
>
> Agora, a vari√°vel de interesse passa a ser $Z_{t+1} = 1.2Y_{t+1} + 10000$. Usando a propriedade da linearidade, podemos escrever:
>
> $$P(Z_{t+1}|X_t) = P(1.2Y_{t+1} + 10000|X_t) = 1.2P(Y_{t+1}|X_t) + 10000$$
>
> $$P(Z_{t+1}|X_t) = 1.2(2782.22X_t) + 10000 = 3338.66X_t + 10000$$
>
> Isso significa que a proje√ß√£o linear do pre√ßo da casa corrigido pelo subsidio e pela infla√ß√£o √© simplesmente uma transforma√ß√£o linear da proje√ß√£o linear original.

> üí° **Exemplo Num√©rico:**
> Para ilustrar ainda mais a linearidade, vamos considerar um modelo com duas vari√°veis explicativas. Suponha que $Y_{t+1}$ representa o lucro de uma empresa, $X_{1,t}$ representa o investimento em publicidade e $X_{2,t}$ representa o investimento em pesquisa e desenvolvimento. Se a proje√ß√£o linear de $Y_{t+1}$ em fun√ß√£o de $X_{1,t}$ e $X_{2,t}$ √© dada por:
>
> $$P(Y_{t+1}|X_{1,t}, X_{2,t}) = 100X_{1,t} + 200X_{2,t}$$
>
> Agora, suponha que a empresa esteja interessada em analisar o efeito de um novo √≠ndice de desempenho $W_{t+1}$, onde $W_{t+1} = 0.5Y_{t+1} + 500$. Utilizando a propriedade de linearidade, podemos projetar $W_{t+1}$ sobre $X_{1,t}$ e $X_{2,t}$ da seguinte forma:
>
> $$P(W_{t+1}|X_{1,t}, X_{2,t}) = P(0.5Y_{t+1} + 500|X_{1,t}, X_{2,t}) = 0.5P(Y_{t+1}|X_{1,t}, X_{2,t}) + 500$$
>
> $$P(W_{t+1}|X_{1,t}, X_{2,t}) = 0.5(100X_{1,t} + 200X_{2,t}) + 500 = 50X_{1,t} + 100X_{2,t} + 500$$
>
> Observe como a proje√ß√£o de uma transforma√ß√£o linear da vari√°vel dependente √© simplesmente a transforma√ß√£o linear da proje√ß√£o original. Isso simplifica muito a an√°lise quando precisamos avaliar m√∫ltiplos cen√°rios com diferentes pondera√ß√µes da vari√°vel dependente.

**Teorema 1:** (Linearidade da Proje√ß√£o Linear) *A proje√ß√£o linear de uma combina√ß√£o linear de vari√°veis em rela√ß√£o a um conjunto de vari√°veis explicativas √© igual a combina√ß√£o linear das proje√ß√µes de cada vari√°vel individual, ou seja:*

$$P(aY_{t+1} + b|X_t) = aP(Y_{t+1}|X_t) + b$$
*onde a e b s√£o constantes determin√≠sticas.*

*Proof:*
I. Seja $P(Y_{t+1}|X_t) = \alpha'X_t$ a proje√ß√£o linear de $Y_{t+1}$ em $X_t$.
II. Queremos mostrar que $P(aY_{t+1} + b|X_t) = aP(Y_{t+1}|X_t) + b$.
III. Seja $P(aY_{t+1} + b|X_t) = \gamma'X_t$ a proje√ß√£o linear de $aY_{t+1} + b$ em $X_t$.
IV. Pela defini√ß√£o de proje√ß√£o linear, o erro de previs√£o $(aY_{t+1} + b - \gamma'X_t)$ deve ser n√£o correlacionado com $X_t$:
$$E[(aY_{t+1} + b - \gamma'X_t)X_t] = 0$$
V. Expandindo, temos:
$$aE[Y_{t+1}X_t] + bE[X_t] - \gamma'E[X_tX_t'] = 0$$
VI.  Sabemos que $E[Y_{t+1}X_t] = \alpha'E[X_tX_t']$, ent√£o:
$$a\alpha'E[X_tX_t'] + bE[X_t] - \gamma'E[X_tX_t'] = 0$$
VII. Resolvendo para $\gamma'$, temos:
$$\gamma' = a\alpha' + bE[X_t]E[X_tX_t']^{-1}$$
VIII. No entanto, para que a proje√ß√£o seja linear com termo constante,  devemos ter $E[aY_{t+1}+b - aP(Y_{t+1}|X_t) - b]=0$, o que nos leva a $E[aY_{t+1}  - a\alpha'X_t]=0$ ou $E[Y_{t+1}  - \alpha'X_t]=0$  o que √© a condi√ß√£o para a proje√ß√£o linear sem intercepto.
IX. A proje√ß√£o linear com termo constante √© $P(aY_{t+1}+b|1,X_t) = \beta_0 + \beta'X_t$.  Usando a propriedade de linearidade:
$$P(aY_{t+1}+b|1,X_t) = aP(Y_{t+1}|1,X_t) + b = a(\beta_0 + \beta'X_t) + b$$
X. Portanto, a proje√ß√£o linear de uma combina√ß√£o linear $aY_{t+1} + b$ √© a combina√ß√£o linear das proje√ß√µes individuais, ou seja, $P(aY_{t+1} + b|X_t) = aP(Y_{t+1}|X_t) + b$.$\blacksquare$

**Lema 1.1:** *A linearidade da proje√ß√£o linear se estende para combina√ß√µes lineares de m√∫ltiplas vari√°veis dependentes. Se $Y_{1,t+1}, Y_{2,t+1}, \ldots, Y_{n,t+1}$ s√£o vari√°veis aleat√≥rias e $a_1, a_2, \ldots, a_n, b$ s√£o constantes determin√≠sticas, ent√£o:*
$$P(\sum_{i=1}^{n} a_i Y_{i,t+1} + b|X_t) = \sum_{i=1}^{n} a_i P(Y_{i,t+1}|X_t) + b$$
*Proof:* A prova deste lema pode ser feita por indu√ß√£o sobre o n√∫mero de vari√°veis. Para o caso base $n=1$, o resultado √© trivialmente verdadeiro. Para $n=2$, o resultado foi estabelecido no Teorema 1. Suponha que o resultado seja v√°lido para $n=k$, isto √©, $P(\sum_{i=1}^{k} a_i Y_{i,t+1} + b|X_t) = \sum_{i=1}^{k} a_i P(Y_{i,t+1}|X_t) + b$. Ent√£o, para $n=k+1$, temos:
\begin{align*}
P(\sum_{i=1}^{k+1} a_i Y_{i,t+1} + b|X_t) &= P(\sum_{i=1}^{k} a_i Y_{i,t+1} + a_{k+1}Y_{k+1,t+1} + b|X_t) \\
&= P(\sum_{i=1}^{k} a_i Y_{i,t+1} +  a_{k+1}Y_{k+1,t+1} + b_1 + b_2 |X_t), \quad b_1+b_2=b\\
&= P(\sum_{i=1}^{k} a_i Y_{i,t+1} + b_1 |X_t) + P( a_{k+1}Y_{k+1,t+1} + b_2|X_t) \\
&=  \sum_{i=1}^{k} a_i P(Y_{i,t+1}|X_t) + b_1 + a_{k+1} P(Y_{k+1,t+1}|X_t) + b_2 \\
&= \sum_{i=1}^{k+1} a_i P(Y_{i,t+1}|X_t) + b.
\end{align*}
Assim, por indu√ß√£o, o lema √© v√°lido para todos $n$.$\blacksquare$

### Invari√¢ncia da Proje√ß√£o Linear
A propriedade de invari√¢ncia da proje√ß√£o linear diz que, se aplicarmos uma transforma√ß√£o linear na vari√°vel dependente ($Y_{t+1}$), a proje√ß√£o linear da vari√°vel transformada ser√° igual √† transforma√ß√£o linear da proje√ß√£o linear original. Mais formalmente, se $a$ e $b$ s√£o constantes, ent√£o:
$$P(aY_{t+1} + b | X_t) = a P(Y_{t+1}|X_t) + b$$

> üí° **Exemplo Num√©rico:**
> Continuando o exemplo anterior, suponha que decidimos converter os pre√ßos das casas de d√≥lares para reais, usando uma taxa de c√¢mbio de 5 reais por d√≥lar, e adicionar um valor fixo de 1000 reais para todos os pre√ßos.  Se $Y_{t+1}$ √© o pre√ßo em d√≥lares e $Z_{t+1}$ √© o pre√ßo em reais, ent√£o $Z_{t+1} = 5Y_{t+1} + 1000$.
>
> Pela propriedade de linearidade e invari√¢ncia, temos:
> $$P(Z_{t+1}|X_t) = P(5Y_{t+1} + 1000|X_t) = 5P(Y_{t+1}|X_t) + 1000$$
>
> Se a proje√ß√£o linear original era $P(Y_{t+1}|X_t) = 2782.22X_t$, ent√£o a proje√ß√£o linear para os pre√ßos em reais seria:
>
> $$P(Z_{t+1}|X_t) = 5(2782.22X_t) + 1000 = 13911.1X_t + 1000$$
>
> Isso demonstra a propriedade de invari√¢ncia: a proje√ß√£o linear da transforma√ß√£o linear √© a mesma transforma√ß√£o da proje√ß√£o linear original.

> üí° **Exemplo Num√©rico:**
> Considere agora um cen√°rio de previs√£o de vendas ($Y_{t+1}$) com base em gastos com marketing digital ($X_t$). Suponha que a rela√ß√£o linear entre vendas e marketing seja dada por:
>
> $$P(Y_{t+1}|X_t) = 500 + 10X_t$$
>
> Agora, imagine que a equipe de vendas decida analisar as vendas em uma escala diferente, multiplicando as vendas por um fator de 2 e subtraindo 100. Seja $W_{t+1} = 2Y_{t+1} - 100$.  Usando a propriedade de invari√¢ncia, podemos obter a proje√ß√£o linear de $W_{t+1}$ sobre $X_t$:
>
> $$P(W_{t+1}|X_t) = P(2Y_{t+1} - 100|X_t) = 2P(Y_{t+1}|X_t) - 100$$
>
> $$P(W_{t+1}|X_t) = 2(500 + 10X_t) - 100 = 1000 + 20X_t - 100 = 900 + 20X_t$$
>
> Assim, vemos que a proje√ß√£o de $W_{t+1}$ √© uma transforma√ß√£o linear da proje√ß√£o de $Y_{t+1}$, demonstrando a propriedade de invari√¢ncia.

**Teorema 2:** (Invari√¢ncia da Proje√ß√£o Linear) *Se a proje√ß√£o linear de* $Y_{t+1}$ *sobre* $X_t$ *√©* $P(Y_{t+1}|X_t)$, *ent√£o a proje√ß√£o linear de* $aY_{t+1} + b$ *sobre* $X_t$ *√© dada por*:

$$P(aY_{t+1} + b|X_t) = aP(Y_{t+1}|X_t) + b$$
*onde a e b s√£o constantes determin√≠sticas.*

*Proof:*
I. Seja $P(Y_{t+1}|X_t) = \alpha'X_t$.
II. Queremos mostrar que a proje√ß√£o de $aY_{t+1} + b$ sobre $X_t$ √© $aP(Y_{t+1}|X_t) + b$.
III. Seja $P(aY_{t+1} + b|X_t) = \gamma'X_t$  a proje√ß√£o linear de $aY_{t+1} + b$ em $X_t$.
IV. Pela defini√ß√£o de proje√ß√£o linear, o erro de previs√£o deve ser n√£o correlacionado com $X_t$:
$$E[(aY_{t+1} + b - \gamma'X_t)X_t] = 0$$
V. Expandindo, obtemos:
$$aE[Y_{t+1}X_t] + bE[X_t] - \gamma'E[X_tX_t'] = 0$$
VI. Como $E[Y_{t+1}X_t] = \alpha'E[X_tX_t']$ da defini√ß√£o de $\alpha$, temos:
$$a\alpha'E[X_tX_t'] + bE[X_t] - \gamma'E[X_tX_t'] = 0$$
VII. Resolvendo para $\gamma'$, temos:
$$\gamma' = a\alpha' + bE[X_t]E[X_tX_t']^{-1}$$
VIII. Para que a proje√ß√£o seja linear, devemos impor que $E[aY_{t+1}+b - (a\alpha'X_t+b) ] = aE[Y_{t+1}  - \alpha'X_t]=0$, que √© a condi√ß√£o de ortogonalidade.
IX.  Pela propriedade de linearidade, $P(aY_{t+1}+b|X_t)= aP(Y_{t+1}|X_t) + b$.
X. Portanto, a proje√ß√£o linear de $aY_{t+1}+b$ em $X_t$ √© igual a $aP(Y_{t+1}|X_t) + b$.$\blacksquare$

√â crucial notar que, para que a propriedade da invari√¢ncia se mantenha, a condi√ß√£o de ortogonalidade deve ser preservada para a nova vari√°vel dependente transformada linearmente.

**Proposi√ß√£o 2.1:** *A propriedade de invari√¢ncia da proje√ß√£o linear se mant√©m para transforma√ß√µes lineares do vetor de vari√°veis explicativas $X_t$. Se $P(Y_{t+1}|X_t) = \alpha'X_t$ √© a proje√ß√£o linear de $Y_{t+1}$ sobre $X_t$, e $Z_t = AX_t + c$, onde $A$ √© uma matriz de constantes e $c$ √© um vetor de constantes, ent√£o a proje√ß√£o linear de $Y_{t+1}$ sobre $Z_t$ √© dada por*
$$P(Y_{t+1}|Z_t) = \alpha'A^{-1}Z_t - \alpha'A^{-1}c $$

> üí° **Exemplo Num√©rico:**
> Vamos considerar um exemplo com duas vari√°veis explicativas. Suponha que tenhamos a seguinte rela√ß√£o:
>
> $$P(Y_{t+1}|X_{1,t}, X_{2,t}) = 2X_{1,t} + 3X_{2,t}$$
>
> Agora, vamos transformar as vari√°veis explicativas em novas vari√°veis $Z_{1,t}$ e $Z_{2,t}$ usando a seguinte transforma√ß√£o linear:
>
> $$Z_t = AX_t$$
>
> Onde $Z_t = \begin{bmatrix} Z_{1,t} \\ Z_{2,t} \end{bmatrix}$, $X_t = \begin{bmatrix} X_{1,t} \\ X_{2,t} \end{bmatrix}$ e $A = \begin{bmatrix} 1 & 0.5 \\ 0.2 & 1 \end{bmatrix}$. Portanto,
>
> $$Z_{1,t} = X_{1,t} + 0.5X_{2,t}$$
> $$Z_{2,t} = 0.2X_{1,t} + X_{2,t}$$
>
> Primeiro, encontramos a inversa de A:
>
> $$A^{-1} = \frac{1}{1 - (0.5)(0.2)} \begin{bmatrix} 1 & -0.5 \\ -0.2 & 1 \end{bmatrix} = \frac{1}{0.9} \begin{bmatrix} 1 & -0.5 \\ -0.2 & 1 \end{bmatrix} = \begin{bmatrix} 1.11 & -0.56 \\ -0.22 & 1.11 \end{bmatrix}$$
>
> Utilizando a proposi√ß√£o 2.1 temos que $P(Y_{t+1}|Z_t) = \alpha'A^{-1}Z_t$.
>
> $\alpha = \begin{bmatrix} 2 \\ 3 \end{bmatrix}$, logo
>
>  $\alpha'A^{-1} =  \begin{bmatrix} 2 & 3 \end{bmatrix} \begin{bmatrix} 1.11 & -0.56 \\ -0.22 & 1.11 \end{bmatrix} = \begin{bmatrix} 1.56 & 2.22 \end{bmatrix}$
>
> $$P(Y_{t+1}|Z_{1,t}, Z_{2,t}) = 1.56Z_{1,t} + 2.22Z_{2,t}$$
>
> Essa transforma√ß√£o mostra como a proje√ß√£o linear se ajusta a mudan√ßas nas vari√°veis explicativas, mantendo a mesma rela√ß√£o fundamental com a vari√°vel dependente.

*Proof:*
I. Temos que  $Z_t = AX_t + c$, ent√£o $X_t = A^{-1}(Z_t - c) = A^{-1}Z_t - A^{-1}c$.
II. A proje√ß√£o linear de $Y_{t+1}$ sobre $X_t$ √© dada por $P(Y_{t+1}|X_t) = \alpha'X_t$.
III. Substituindo a express√£o de $X_t$ em termos de $Z_t$ na proje√ß√£o, temos:
$$P(Y_{t+1}|Z_t) = \alpha'(A^{-1}Z_t - A^{-1}c) =  \alpha'A^{-1}Z_t - \alpha'A^{-1}c$$
IV. Portanto, a proje√ß√£o linear de $Y_{t+1}$ sobre $Z_t$ √© uma transforma√ß√£o linear da proje√ß√£o original em $X_t$, onde os coeficientes da proje√ß√£o s√£o transformados por $A^{-1}$ e um termo constante √© adicionado. $\blacksquare$
Essa proposi√ß√£o mostra que a proje√ß√£o linear √© invariante n√£o apenas em rela√ß√£o a transforma√ß√µes lineares da vari√°vel dependente, mas tamb√©m a transforma√ß√µes lineares das vari√°veis explicativas, contanto que a transforma√ß√£o seja invers√≠vel.

### Implica√ß√µes Pr√°ticas
As propriedades de linearidade e invari√¢ncia s√£o muito importantes na pr√°tica. Elas garantem que a proje√ß√£o linear seja uma ferramenta flex√≠vel e adapt√°vel, capaz de lidar com transforma√ß√µes lineares das vari√°veis de interesse de forma eficiente.
  - **Simplifica√ß√£o de C√°lculos:** A linearidade permite que transforma√ß√µes lineares nas vari√°veis dependentes sejam tratadas diretamente, sem a necessidade de recalcular a proje√ß√£o linear a partir do zero.
  - **Generaliza√ß√£o de Modelos:** A invari√¢ncia permite que resultados obtidos com uma escala de vari√°veis possam ser generalizados para outras escalas, contanto que a transforma√ß√£o seja linear.
  - **Robustez:** As propriedades garantem que o m√©todo de proje√ß√£o linear seja robusto a certas transforma√ß√µes comuns nos dados, o que √© essencial para aplica√ß√µes pr√°ticas.

### Conclus√£o
A proje√ß√£o linear, al√©m de ser uma ferramenta poderosa para previs√£o, apresenta as valiosas propriedades de linearidade e invari√¢ncia. A linearidade permite manipular combina√ß√µes lineares de vari√°veis de maneira simples, enquanto a invari√¢ncia garante que transforma√ß√µes lineares nas vari√°veis dependentes se reflitam de forma direta na proje√ß√£o linear. Essas propriedades simplificam os c√°lculos, generalizam os resultados e garantem a robustez do m√©todo. Juntas, essas propriedades tornam a proje√ß√£o linear uma ferramenta fundamental na an√°lise de s√©ries temporais e na modelagem de dados.
### Refer√™ncias
[^1]: Expression [4.1.1], [4.1.2], [4.1.3], [4.1.4], [4.1.5], [4.1.6], [4.1.7], [4.1.8]
[^2]: [4.1.9], [4.1.10], [4.1.11], [4.1.12], [4.1.13]
[^3]: [4.1.14], [4.1.15]
<!-- END -->
