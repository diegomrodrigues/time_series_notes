## Proje√ß√£o Linear e o Erro Quadr√°tico M√©dio √ìtimo
### Introdu√ß√£o
Neste cap√≠tulo, exploramos a **proje√ß√£o linear** como uma ferramenta fundamental na an√°lise de s√©ries temporais e na constru√ß√£o de previs√µes. A proje√ß√£o linear de $Y_{t+1}$ sobre $X_t$, denotada como $P(Y_{t+1}|X_t) = \alpha'X_t$, busca minimizar o erro quadr√°tico m√©dio (MSE) dentro da classe de previs√µes lineares [^2]. A condi√ß√£o de n√£o correla√ß√£o entre o erro de previs√£o e as vari√°veis explicativas, $E[(Y_{t+1} - \alpha'X_t)X_t] = 0$, garante a otimalidade desta proje√ß√£o dentro do espa√ßo linear [^2]. Este m√©todo, embora mais restritivo do que usar a expectativa condicional, apresenta uma solu√ß√£o pr√°tica e computacionalmente eficiente.

> üí° **Exemplo Num√©rico:**
> Suponha que temos dados de vendas ($Y_{t+1}$) e gastos com publicidade ($X_t$) ao longo de v√°rios per√≠odos. Para simplificar, vamos considerar que $Y_{t+1}$ √© o total de vendas no per√≠odo $t+1$ e $X_t$ √© o gasto com publicidade no per√≠odo $t$ (em milhares de unidades monet√°rias). Temos os seguintes dados simulados:
>
> | t   | $X_t$ (Publicidade) | $Y_{t+1}$ (Vendas) |
> |-----|---------------------|-------------------|
> | 1   | 2                   | 5                 |
> | 2   | 3                   | 7                 |
> | 3   | 4                   | 9                 |
> | 4   | 5                   | 10                |
> | 5   | 6                   | 12                |
>
>
> Nosso objetivo √© encontrar a melhor proje√ß√£o linear de $Y_{t+1}$ em $X_t$, ou seja, encontrar $\alpha$ tal que $P(Y_{t+1}|X_t) = \alpha X_t$. Para isso, vamos calcular $\alpha$ usando a f√≥rmula:
> $$\alpha = \frac{E[Y_{t+1}X_t]}{E[X_t^2]}$$
>
> Primeiro, calculamos as m√©dias e os momentos:
>
> $\text{M√©dia de } X_t$: $\bar{X} = \frac{2 + 3 + 4 + 5 + 6}{5} = 4$
>
> $\text{M√©dia de } Y_{t+1}$: $\bar{Y} = \frac{5 + 7 + 9 + 10 + 12}{5} = 8.6$
>
> $E[X_t^2] = \frac{2^2 + 3^2 + 4^2 + 5^2 + 6^2}{5} = \frac{4 + 9 + 16 + 25 + 36}{5} = \frac{90}{5} = 18$
>
> $E[Y_{t+1}X_t] = \frac{(2 \times 5) + (3 \times 7) + (4 \times 9) + (5 \times 10) + (6 \times 12)}{5} = \frac{10 + 21 + 36 + 50 + 72}{5} = \frac{189}{5} = 37.8$
>
> Agora, podemos calcular $\alpha$:
>
> $$\alpha = \frac{37.8}{18} = 2.1$$
>
> Portanto, a proje√ß√£o linear de $Y_{t+1}$ em $X_t$ √© dada por $P(Y_{t+1}|X_t) = 2.1 X_t$.
>
> Isso significa que, para cada unidade de gasto com publicidade ($X_t$), esperamos um aumento de 2.1 unidades nas vendas ($Y_{t+1}$).

### Conceitos Fundamentais
A nota√ß√£o $P(Y_{t+1}|X_t) = \alpha'X_t$ representa a **proje√ß√£o linear de $Y_{t+1}$ em $X_t$**, onde $\alpha$ √© um vetor de coeficientes que minimiza o MSE da previs√£o [^2]. Como vimos na se√ß√£o anterior, o erro de previs√£o √© definido como $Y_{t+1} - \alpha'X_t$, e a condi√ß√£o $E[(Y_{t+1} - \alpha'X_t)X_t] = 0$ implica que este erro √© n√£o correlacionado com as vari√°veis explicativas $X_t$. √â essencial notar que essa condi√ß√£o de n√£o correla√ß√£o √© fundamental para a otimalidade da proje√ß√£o linear [^2].

> üí° **Rela√ß√£o com a Expectativa Condicional:**
> Embora a expectativa condicional $E(Y_{t+1}|X_t)$ represente a previs√£o que minimiza o MSE para todas as fun√ß√µes de $X_t$, a proje√ß√£o linear restringe a previs√£o a uma fun√ß√£o linear de $X_t$, ou seja, $\alpha'X_t$. No entanto, √© um resultado not√°vel que *o MSE da proje√ß√£o linear √≥tima* $P(Y_{t+1}|X_t)$ *√© igual ao MSE da expectativa condicional* $E(Y_{t+1}|X_t)$ *quando a expectativa condicional √© uma fun√ß√£o linear de $X_t$*, o que geralmente √© o caso em modelos gaussianos.

A condi√ß√£o de n√£o correla√ß√£o  $E[(Y_{t+1} - \alpha'X_t)X_t] = 0$ pode ser reescrita como:
$$E[Y_{t+1}X_t] = \alpha' E[X_tX_t']$$
Resolvendo para $\alpha'$, obtemos os coeficientes da proje√ß√£o linear:
$$\alpha' = E[Y_{t+1}X_t] [E(X_tX_t')]^{-1}$$
Essa f√≥rmula expressa os coeficientes da proje√ß√£o linear em termos dos momentos populacionais de $Y_{t+1}$ e $X_t$ [^2]. A proje√ß√£o linear √© a melhor previs√£o linear poss√≠vel no sentido de que ela minimiza o MSE dentro da classe de fun√ß√µes lineares das vari√°veis explicativas [^2].

> üí° **Exemplo Num√©rico (Multivariado):**
>  Vamos expandir o exemplo anterior para incluir outra vari√°vel, digamos, o √≠ndice de confian√ßa do consumidor ($Z_t$). Agora temos duas vari√°veis explicativas $X_t$ (publicidade) e $Z_t$ (confian√ßa do consumidor).
>
> | t   | $X_t$ | $Z_t$ | $Y_{t+1}$ |
> |-----|-------|-------|-----------|
> | 1   | 2     | 10    | 5         |
> | 2   | 3     | 12    | 7         |
> | 3   | 4     | 11    | 9         |
> | 4   | 5     | 13    | 10        |
> | 5   | 6     | 15    | 12        |
>
> Queremos encontrar $\alpha = [\alpha_1, \alpha_2]$ tal que $P(Y_{t+1}|X_t, Z_t) = \alpha_1 X_t + \alpha_2 Z_t$.
>
> Primeiro, vamos construir as matrizes $X$ (vari√°veis explicativas) e $Y$ (vari√°vel dependente):
> $$ X = \begin{bmatrix} 2 & 10 \\ 3 & 12 \\ 4 & 11 \\ 5 & 13 \\ 6 & 15 \end{bmatrix} \quad Y = \begin{bmatrix} 5 \\ 7 \\ 9 \\ 10 \\ 12 \end{bmatrix}$$
>
> Usamos a f√≥rmula $\alpha = (X^T X)^{-1} X^T Y$
>
> Calculamos $X^T X$ :
>$$ X^T X = \begin{bmatrix} 2 & 3 & 4 & 5 & 6 \\ 10 & 12 & 11 & 13 & 15 \end{bmatrix} \begin{bmatrix} 2 & 10 \\ 3 & 12 \\ 4 & 11 \\ 5 & 13 \\ 6 & 15 \end{bmatrix} = \begin{bmatrix} 90 & 483 \\ 483 & 2679 \end{bmatrix} $$
>
> Calculamos $(X^T X)^{-1}$:
>
> $$ (X^T X)^{-1} = \frac{1}{(90*2679 - 483*483)} \begin{bmatrix} 2679 & -483 \\ -483 & 90 \end{bmatrix} \approx \begin{bmatrix} 0.469 & -0.085 \\ -0.085 & 0.016 \end{bmatrix} $$
>
> Calculamos $X^T Y$:
>$$ X^T Y = \begin{bmatrix} 2 & 3 & 4 & 5 & 6 \\ 10 & 12 & 11 & 13 & 15 \end{bmatrix} \begin{bmatrix} 5 \\ 7 \\ 9 \\ 10 \\ 12 \end{bmatrix} = \begin{bmatrix} 189 \\ 1047 \end{bmatrix} $$
>
> Agora, calculamos $\alpha$:
> $$\alpha = (X^T X)^{-1} X^T Y = \begin{bmatrix} 0.469 & -0.085 \\ -0.085 & 0.016 \end{bmatrix} \begin{bmatrix} 189 \\ 1047 \end{bmatrix} \approx \begin{bmatrix} 0.032 \\ 0.028 \end{bmatrix}$$
>
> Assim, a proje√ß√£o linear √© aproximadamente $P(Y_{t+1}|X_t, Z_t) = 0.032 X_t + 0.028 Z_t$. Isso indica que tanto a publicidade quanto a confian√ßa do consumidor t√™m um efeito positivo nas vendas, mas a publicidade tem um efeito ligeiramente maior.

Uma propriedade fundamental da proje√ß√£o linear √© que o erro da proje√ß√£o, $Y_{t+1} - \alpha'X_t$, √© n√£o correlacionado com qualquer combina√ß√£o linear de $X_t$. Ou seja, para qualquer vetor $c$, temos:
$$E[(Y_{t+1} - \alpha'X_t)c'X_t] = 0$$
Esta propriedade, conhecida como **propriedade de ortogonalidade**, √© crucial para entender a otimalidade da proje√ß√£o linear [^2].

> üí° **MSE da Proje√ß√£o Linear √ìtima:**
> O MSE da proje√ß√£o linear √≥tima √© dado por:
$$MSE[P(Y_{t+1}|X_t)] = E[Y_{t+1} - \alpha'X_t]^2 = E[Y_{t+1}^2] - E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1} E[X_tY_{t+1}]$$
Essa express√£o revela que o MSE da proje√ß√£o linear √© dado pela vari√¢ncia de $Y_{t+1}$ menos um termo que representa a vari√¢ncia explicada pela proje√ß√£o linear sobre $X_t$. Quando a expectativa condicional $E(Y_{t+1}|X_t)$ √© uma fun√ß√£o linear de $X_t$, este MSE √© igual ao MSE da previs√£o com base na expectativa condicional, o que confirma a otimalidade da proje√ß√£o linear.
>
> üí° **Exemplo Num√©rico (MSE):**
>
> Utilizando o primeiro exemplo num√©rico (univariado) com $\alpha=2.1$ , vamos calcular o MSE.
>
> Os valores previstos $\hat{Y}_{t+1}$ s√£o dados por $2.1X_t$:
>
> | t   | $X_t$ | $Y_{t+1}$ | $\hat{Y}_{t+1}$ | $Y_{t+1} - \hat{Y}_{t+1}$ |$(Y_{t+1} - \hat{Y}_{t+1})^2$|
> |-----|-------|-----------|------------------|-------------------------|-----------------------------|
> | 1   | 2     | 5         | 4.2               | 0.8                     | 0.64                        |
> | 2   | 3     | 7         | 6.3               | 0.7                     | 0.49                        |
> | 3   | 4     | 9         | 8.4               | 0.6                     | 0.36                        |
> | 4   | 5     | 10        | 10.5              | -0.5                    | 0.25                        |
> | 5   | 6     | 12        | 12.6              | -0.6                    | 0.36                        |
>
> O MSE √© a m√©dia dos erros ao quadrado:
>
> $$MSE = \frac{0.64 + 0.49 + 0.36 + 0.25 + 0.36}{5} = \frac{2.1}{5} = 0.42$$
>
> Para calcular o MSE usando a f√≥rmula $MSE = E[Y_{t+1}^2] - E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1} E[X_tY_{t+1}]$, temos:
>
> $E[Y_{t+1}^2] = (5^2 + 7^2 + 9^2 + 10^2 + 12^2)/5 = (25 + 49 + 81 + 100 + 144)/5 = 399/5 = 79.8$
>
> Usamos os valores previamente calculados: $E[X_tX_t'] = 18$ e $E[X_tY_{t+1}] = 37.8$. Ent√£o:
>
> $MSE = 79.8 - \frac{37.8^2}{18} = 79.8 - \frac{1428.84}{18} = 79.8 - 79.38 = 0.42$, o que confirma nosso c√°lculo anterior.

> üí° **Compara√ß√£o com a Expectativa Condicional:**
> √â importante notar que a proje√ß√£o linear √© uma ferramenta flex√≠vel e poderosa, mesmo quando a expectativa condicional n√£o √© uma fun√ß√£o linear das vari√°veis explicativas. Em tais casos, a proje√ß√£o linear oferece a melhor previs√£o linear poss√≠vel e se torna uma aproxima√ß√£o √∫til e trat√°vel da expectativa condicional. A diferen√ßa entre a proje√ß√£o linear e a expectativa condicional reside na restri√ß√£o √† linearidade imposta pela primeira.

Em muitas aplica√ß√µes, a proje√ß√£o linear √© estendida para incluir um termo constante, expressa como $\hat{E}(Y_{t+1}|X_t) = P(Y_{t+1}|1, X_t)$. Esta inclus√£o permite que o modelo acomode um n√≠vel base para o valor de $Y_{t+1}$, al√©m da varia√ß√£o linear em fun√ß√£o de $X_t$. A proje√ß√£o linear com um termo constante √© dada por:
$$P(Y_{t+1}|1, X_t) = \beta_0 + \beta'X_t$$
onde
$$\beta' = Cov(Y_{t+1}, X_t) [Var(X_t)]^{-1}$$
e
$$\beta_0 = E(Y_{t+1}) - \beta'E(X_t)$$
Essa formula√ß√£o √© especialmente √∫til em cen√°rios onde o valor m√©dio de $Y_{t+1}$ √© diferente de zero, ou quando a proje√ß√£o linear deve interceptar o eixo vertical em um valor diferente de zero.

> üí° **Exemplo Num√©rico (Proje√ß√£o Linear com Constante):**
>  Vamos recalcular o primeiro exemplo com a adi√ß√£o de uma constante.
> Temos os dados:
>
> | t   | $X_t$ | $Y_{t+1}$ |
> |-----|-------|-----------|
> | 1   | 2     | 5         |
> | 2   | 3     | 7         |
> | 3   | 4     | 9         |
> | 4   | 5     | 10        |
> | 5   | 6     | 12        |
>
> Calculamos as m√©dias:
>
> $\bar{X} = 4$
>
> $\bar{Y} = 8.6$
>
> Calculamos a vari√¢ncia de $X_t$:
>
> $Var(X_t) = E[X_t^2] - (E[X_t])^2 = 18 - 4^2 = 18 - 16 = 2$
>
> Calculamos a covari√¢ncia entre $Y_{t+1}$ e $X_t$:
>
> $Cov(Y_{t+1}, X_t) = E[Y_{t+1}X_t] - E[Y_{t+1}]E[X_t] = 37.8 - 8.6 \times 4 = 37.8 - 34.4 = 3.4$
>
> Calculamos $\beta'$:
>
> $\beta' = \frac{Cov(Y_{t+1}, X_t)}{Var(X_t)} = \frac{3.4}{2} = 1.7$
>
> Calculamos $\beta_0$:
>
> $\beta_0 = E(Y_{t+1}) - \beta'E(X_t) = 8.6 - 1.7 \times 4 = 8.6 - 6.8 = 1.8$
>
> Portanto, a proje√ß√£o linear com constante √© $P(Y_{t+1}|1, X_t) = 1.8 + 1.7 X_t$.
>
> Notemos que o coeficiente angular aqui (1.7) difere do resultado obtido anteriormente (2.1) porque agora estamos permitindo um valor constante (1.8) como intercepto. Isso melhora o ajuste do modelo aos dados, pois agora podemos modelar melhor a rela√ß√£o entre $X_t$ e $Y_{t+1}$.

**Lema 1** (Decomposi√ß√£o do Erro Quadr√°tico M√©dio): O MSE da vari√°vel aleat√≥ria $Y_{t+1}$ pode ser decomposto na soma do MSE da proje√ß√£o linear e da vari√¢ncia do erro da proje√ß√£o.

*Prova*:
I.  Seja $e_{t+1} = Y_{t+1} - P(Y_{t+1}|X_t)$ o erro da proje√ß√£o.
II.  Pela propriedade de ortogonalidade,  $E[e_{t+1}P(Y_{t+1}|X_t)] = 0$.
III. O MSE de $Y_{t+1}$ √© $E[(Y_{t+1} - E(Y_{t+1}))^2] = Var(Y_{t+1})$.
IV.  Podemos escrever $Y_{t+1} - E(Y_{t+1}) =  (Y_{t+1} - P(Y_{t+1}|X_t)) + (P(Y_{t+1}|X_t)-E(Y_{t+1})) = e_{t+1} + (P(Y_{t+1}|X_t)-E(Y_{t+1}))$.
V.  Elevando ao quadrado e tomando a esperan√ßa: $Var(Y_{t+1}) = E[e_{t+1}^2] + E[(P(Y_{t+1}|X_t)-E(Y_{t+1}))^2]  + 2E[e_{t+1}(P(Y_{t+1}|X_t)-E(Y_{t+1}))]$.
VI. O √∫ltimo termo √© zero pela propriedade de ortogonalidade, como explicitado em II, $E[e_{t+1}P(Y_{t+1}|X_t)]=E[e_{t+1}E(P(Y_{t+1}|X_t))]=0$.
VII. Assim, $Var(Y_{t+1}) = E[e_{t+1}^2] + E[(P(Y_{t+1}|X_t)-E(Y_{t+1}))^2] = MSE[P(Y_{t+1}|X_t)] + Var(e_{t+1})$.
VIII. Portanto,  $MSE[Y_{t+1}] = MSE[P(Y_{t+1}|X_t)] + Var(Y_{t+1} - P(Y_{t+1}|X_t))$.‚ñ†

**Lema 2** (MSE √ìtimo e Expectativa Condicional Linear): *Se a expectativa condicional* $E(Y_{t+1}|X_t)$ *√© uma fun√ß√£o linear de* $X_t$, *ent√£o o MSE da proje√ß√£o linear* $P(Y_{t+1}|X_t)$ *√© igual ao MSE da expectativa condicional* $E(Y_{t+1}|X_t)$.

*Prova*:
I.  Seja $E(Y_{t+1}|X_t) = a'X_t$, onde $a$ √© um vetor de coeficientes.
II.  Como a proje√ß√£o linear √© a melhor previs√£o linear, ela deve satisfazer  $E[(Y_{t+1} - \alpha'X_t)X_t]=0$.
III. O MSE da expectativa condicional linear √© $E[(Y_{t+1}-a'X_t)^2]$, e o MSE da proje√ß√£o linear √© $E[(Y_{t+1} - \alpha'X_t)^2]$.
IV.  Podemos escrever $Y_{t+1} = a'X_t + \eta_{t+1}$ onde $\eta_{t+1} = Y_{t+1} - a'X_t$  e $E(\eta_{t+1}|X_t)=0$.
V.  Ent√£o o MSE da expectativa condicional linear √© $E(\eta_{t+1}^2)$.
VI. Por outro lado, podemos escrever $Y_{t+1} = \alpha'X_t + e_{t+1}$ onde $e_{t+1} = Y_{t+1} - \alpha'X_t$.
VII. Substituindo $Y_{t+1}$ na express√£o para $e_{t+1}$, temos: $e_{t+1} =  a'X_t + \eta_{t+1} - \alpha'X_t$.
VIII. Como $E[(Y_{t+1} - \alpha'X_t)X_t]=0$, temos $E[e_{t+1} X_t] = 0$ ou $E[( a'X_t + \eta_{t+1} - \alpha'X_t)X_t] = 0$.
IX. Logo $E[(a'X_t  - \alpha'X_t)X_t] + E[\eta_{t+1}X_t]= 0$, e como $E[\eta_{t+1}X_t]=0$ , temos  $E[(a'-\alpha')X_t X_t']= 0$
X.  Portanto,  $a=\alpha$ e o MSE da proje√ß√£o linear √© $E(e_{t+1}^2)=E(\eta_{t+1}^2)$.  Assim, os MSE s√£o iguais.‚ñ†

**Teorema 1** (Unicidade da Proje√ß√£o Linear): A proje√ß√£o linear $P(Y_{t+1}|X_t)$ que satisfaz $E[(Y_{t+1} - P(Y_{t+1}|X_t))X_t]=0$ √© √∫nica.

*Prova*:
I. Suponha que existam duas proje√ß√µes lineares $P_1(Y_{t+1}|X_t) = \alpha_1'X_t$ e $P_2(Y_{t+1}|X_t) = \alpha_2'X_t$  que satisfazem a condi√ß√£o de ortogonalidade.
II. Ent√£o, temos  $E[(Y_{t+1} - \alpha_1'X_t)X_t] = 0$ e $E[(Y_{t+1} - \alpha_2'X_t)X_t] = 0$.
III. Subtraindo as duas equa√ß√µes,  $E[(\alpha_2'X_t - \alpha_1'X_t)X_t] = 0$.
IV. Isso implica que  $E[(\alpha_2' - \alpha_1')X_tX_t'] = (\alpha_2' - \alpha_1')E[X_tX_t']=0$.
V.  Se $E[X_tX_t']$ √© n√£o singular, ent√£o $(\alpha_2' - \alpha_1')=0$, o que implica  $\alpha_1 = \alpha_2$
VI.  Portanto, $P_1(Y_{t+1}|X_t) = P_2(Y_{t+1}|X_t)$, e a proje√ß√£o linear √© √∫nica.‚ñ†

### Conclus√£o
A nota√ß√£o $P(Y_{t+1}|X_t) = \alpha'X_t$ resume o conceito de proje√ß√£o linear, um m√©todo crucial para construir previs√µes de s√©ries temporais. A proje√ß√£o linear, ao buscar a melhor aproxima√ß√£o linear para $Y_{t+1}$ em termos de $X_t$, garante que o erro da proje√ß√£o seja n√£o correlacionado com $X_t$, uma propriedade de otimalidade dentro da classe linear. Al√©m disso, em modelos onde a expectativa condicional √© linear, a proje√ß√£o linear produz um resultado t√£o bom quanto o da expectativa condicional, simplificando o c√°lculo da previs√£o sem perda de precis√£o. As propriedades do MSE da proje√ß√£o linear e a sua equival√™ncia com o MSE da expectativa condicional linear demonstram a import√¢ncia te√≥rica e pr√°tica desta ferramenta para a an√°lise de s√©ries temporais e previs√£o.
### Refer√™ncias
[^1]: Expression [4.1.1], [4.1.2], [4.1.3], [4.1.4], [4.1.5], [4.1.6], [4.1.7], [4.1.8]
[^2]: [4.1.9], [4.1.10], [4.1.11], [4.1.12], [4.1.13]
[^3]: [4.1.14], [4.1.15]
<!-- END -->
