## Previs√µes Baseadas em Proje√ß√£o Linear
### Introdu√ß√£o
Este cap√≠tulo expande o conceito de previs√£o, introduzindo a **proje√ß√£o linear** como um m√©todo para construir previs√µes que minimizam o erro quadr√°tico m√©dio (MSE). Como vimos anteriormente, a previs√£o √≥tima em termos de MSE √© dada pela expectativa condicional [^1]. No entanto, o c√°lculo da expectativa condicional pode ser complexo e exigir informa√ß√µes que n√£o est√£o sempre dispon√≠veis. A proje√ß√£o linear oferece uma alternativa mais simples e computacionalmente trat√°vel, restringindo a classe de previs√µes a fun√ß√µes lineares das vari√°veis explicativas [^2]. Este m√©todo busca encontrar um conjunto de pesos que minimizem o MSE da previs√£o, garantindo que o erro de previs√£o seja n√£o correlacionado com as vari√°veis explicativas [^2].

### Conceitos Fundamentais
A proje√ß√£o linear restringe a previs√£o de uma vari√°vel $Y_{t+1}$ a uma fun√ß√£o linear das vari√°veis explicativas $X_t$, ou seja, $Y_{t+1}^* = \alpha'X_t$ [^2]. A ideia √© encontrar um vetor $\alpha$ tal que o erro de previs√£o $(Y_{t+1} - \alpha'X_t)$ seja n√£o correlacionado com $X_t$. Formalmente, essa condi√ß√£o √© expressa como:
$$E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$$
Essa condi√ß√£o √© fundamental porque garante que a proje√ß√£o linear $\alpha'X_t$ capture toda a informa√ß√£o linearmente relevante em $X_t$ para prever $Y_{t+1}$ [^2].

> üí° **Exemplo Num√©rico:** Vamos supor que queremos prever o pre√ßo de uma casa ($Y_{t+1}$) com base na sua √°rea em metros quadrados ($X_t$).  Imagine que temos os seguintes dados para um pequeno conjunto de casas:

| Casa | √Årea ($X_t$) | Pre√ßo ($Y_{t+1}$) |
|------|---------------|-----------------|
| 1    | 50            | 150000          |
| 2    | 75            | 210000          |
| 3    | 100           | 280000          |
| 4    | 125           | 350000          |
| 5    | 150           | 410000          |

Podemos representar esses dados como vetores em $\mathbb{R}^5$:
$$ X_t = \begin{bmatrix} 50 \\ 75 \\ 100 \\ 125 \\ 150 \end{bmatrix}, \quad Y_{t+1} = \begin{bmatrix} 150000 \\ 210000 \\ 280000 \\ 350000 \\ 410000 \end{bmatrix} $$

O objetivo da proje√ß√£o linear √© encontrar um escalar $\alpha$ (j√° que $X_t$ √© uma √∫nica vari√°vel) de forma que $Y_{t+1}^* = \alpha X_t$ seja a melhor previs√£o linear.  A condi√ß√£o $E[(Y_{t+1} - \alpha X_t)X_t] = 0$ implica que o erro de previs√£o, $(Y_{t+1} - \alpha X_t)$, n√£o deve ter rela√ß√£o linear com $X_t$.  Na pr√°tica, usamos a vers√£o amostral da esperan√ßa para obter uma estimativa de $\alpha$.

**Lema 1** (Propriedade da Ortogonalidade): *A condi√ß√£o de n√£o correla√ß√£o* $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$ *implica que o erro de previs√£o* $(Y_{t+1} - \alpha'X_t)$ *√© ortogonal a qualquer combina√ß√£o linear das vari√°veis explicativas, ou seja,  para qualquer vetor* $c$, *temos* $E[(Y_{t+1} - \alpha'X_t)c'X_t]=0$.

*Proof:* Seja $c$ um vetor arbitr√°rio.
I.  Come√ßamos com a condi√ß√£o de n√£o correla√ß√£o: $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$.
II. Multiplicamos o termo dentro da expectativa por $c'$:  $E[(Y_{t+1} - \alpha'X_t)c'X_t]$
III. Usando a propriedade da linearidade da esperan√ßa, podemos tirar a constante $c'$: $c'E[(Y_{t+1} - \alpha'X_t)X_t]$
IV. Pela condi√ß√£o inicial, $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$, ent√£o: $c'0' = 0$.
Portanto, $E[(Y_{t+1} - \alpha'X_t)c'X_t] = 0$, o que demonstra a ortogonalidade. ‚ñ†

Para verificar essa afirma√ß√£o, considere qualquer fun√ß√£o $g(X_t)$ como base para a previs√£o: $Y_{t+1}^* = g(X_t)$ [^1]. O erro quadr√°tico m√©dio nesse caso seria:
$$E[Y_{t+1} - g(X_t)]^2 = E[Y_{t+1} - E(Y_{t+1}|X_t) + E(Y_{t+1}|X_t) - g(X_t)]^2$$
$$ = E[Y_{t+1} - E(Y_{t+1}|X_t)]^2 + 2E[(Y_{t+1} - E(Y_{t+1}|X_t))(E(Y_{t+1}|X_t) - g(X_t))] + E[E(Y_{t+1}|X_t) - g(X_t)]^2$$
O termo do meio pode ser escrito como $2E[\eta_{t+1}]$ onde $\eta_{t+1} = [Y_{t+1} - E(Y_{t+1}|X_t)][E(Y_{t+1}|X_t) - g(X_t)]$ [^1]. Condicional a $X_t$,  $E(Y_{t+1}|X_t)$ e $g(X_t)$ s√£o constantes, permitindo fatorar da expectativa:
$$E[\eta_{t+1}|X_t] = [E(Y_{t+1}|X_t) - g(X_t)] \times E([Y_{t+1} - E(Y_{t+1}|X_t)]|X_t) = 0$$
Pela lei da expectativa iterada, $E[\eta_{t+1}] = E_{X_t}[E[\eta_{t+1}|X_t]] = 0$. Substituindo de volta na equa√ß√£o original, temos:
$$E[Y_{t+1} - g(X_t)]^2 = E[Y_{t+1} - E(Y_{t+1}|X_t)]^2 + E[E(Y_{t+1}|X_t) - g(X_t)]^2$$
Como o segundo termo √© sempre n√£o negativo, o MSE ser√° minimizado quando o segundo termo for igual a zero, o que ocorre quando $g(X_t) = E(Y_{t+1}|X_t)$. Portanto, a expectativa condicional √© o preditor que minimiza o MSE [^1].

Expandindo o conceito, se restringirmos a fun√ß√£o $g(X_t)$ a uma fun√ß√£o linear $g(X_t) = \alpha'X_t$, o MSE seria:
$$E[Y_{t+1} - \alpha'X_t]^2 = E[Y_{t+1} - \alpha'X_t]^2 = E[Y_{t+1} - \alpha'X_t + \alpha'X_t - g'X_t]^2$$
$$= E[Y_{t+1} - \alpha'X_t]^2 + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t)] + E[\alpha'X_t - g'X_t]^2$$
O termo do meio tamb√©m se anula, pela condi√ß√£o de n√£o correla√ß√£o $E[(Y_{t+1} - \alpha'X_t)X_t] = 0$. Logo,
$$E[Y_{t+1} - g'X_t]^2 = E[Y_{t+1} - \alpha'X_t]^2 + E[\alpha'X_t - g'X_t]^2$$
O MSE √© minimizado quando o segundo termo √© zero, o que ocorre quando $g'X_t = \alpha'X_t$, confirmando que a proje√ß√£o linear minimiza o erro dentro da classe de previs√µes lineares [^2]. A proje√ß√£o linear, ent√£o, busca encontrar um $\alpha$ que satisfa√ßa a condi√ß√£o de n√£o correla√ß√£o.

Para calcular $\alpha$, partimos da condi√ß√£o:
$$E[(Y_{t+1} - \alpha'X_t)X_t] = E[Y_{t+1}X_t] - \alpha'E[X_tX_t'] = 0'$$
Resolvendo para $\alpha$, temos:
$$\alpha' = E[Y_{t+1}X_t] [E(X_tX_t')]^{-1}$$
Essa f√≥rmula expressa os coeficientes da proje√ß√£o linear em termos dos momentos populacionais de $Y_{t+1}$ e $X_t$ [^2].

> üí° **Exemplo Num√©rico (cont.):** Continuando o exemplo das casas, podemos aproximar os momentos populacionais com os momentos amostrais.  Precisamos calcular $E[Y_{t+1}X_t]$ e $E[X_tX_t']$.
>
> Primeiro, calculamos $X_t X_t'$:
> $$X_t X_t' = \begin{bmatrix} 50 \\ 75 \\ 100 \\ 125 \\ 150 \end{bmatrix} \begin{bmatrix} 50 & 75 & 100 & 125 & 150 \end{bmatrix} = \begin{bmatrix} 2500 & 3750 & 5000 & 6250 & 7500 \\ 3750 & 5625 & 7500 & 9375 & 11250 \\ 5000 & 7500 & 10000 & 12500 & 15000 \\ 6250 & 9375 & 12500 & 15625 & 18750 \\ 7500 & 11250 & 15000 & 18750 & 22500 \end{bmatrix}$$
>
> Calculamos a m√©dia dos elementos de $X_t X_t'$ (que neste caso √© uma matriz 5x5, mas estamos interessados em estimar a vari√¢ncia de X, e precisamos da m√©dia dos quadrados):
>
> $$E[X_tX_t'] \approx \frac{1}{5} \sum_{i=1}^{5} X_{t,i}^2 = \frac{50^2 + 75^2 + 100^2 + 125^2 + 150^2}{5} = \frac{2500 + 5625 + 10000 + 15625 + 22500}{5} = \frac{56250}{5} = 11250$$
>
>
> Agora, calculamos $Y_{t+1}X_t$:
>
> $$Y_{t+1}X_t = \begin{bmatrix} 150000 \\ 210000 \\ 280000 \\ 350000 \\ 410000 \end{bmatrix} \begin{bmatrix} 50 \\ 75 \\ 100 \\ 125 \\ 150 \end{bmatrix} = \begin{bmatrix} 150000 \times 50 \\ 210000 \times 75 \\ 280000 \times 100 \\ 350000 \times 125 \\ 410000 \times 150 \end{bmatrix} = \begin{bmatrix} 7500000 \\ 15750000 \\ 28000000 \\ 43750000 \\ 61500000 \end{bmatrix}$$
>
>
> Calculamos a m√©dia dos elementos de $Y_{t+1}X_t$:
>
> $$E[Y_{t+1}X_t] \approx \frac{1}{5} \sum_{i=1}^{5} Y_{t+1,i}X_{t,i} = \frac{7500000 + 15750000 + 28000000 + 43750000 + 61500000}{5} = \frac{156500000}{5} = 31300000$$
>
>
> Agora, calculamos $\alpha$:
> $$\alpha = \frac{E[Y_{t+1}X_t]}{E[X_tX_t']} = \frac{31300000}{11250} \approx 2782.22$$
>
> Portanto, a proje√ß√£o linear seria $Y_{t+1}^* = 2782.22 X_t$. Isso significa que, para cada metro quadrado a mais na √°rea da casa, o pre√ßo previsto aumenta em aproximadamente 2782.22 unidades monet√°rias.

√â importante observar que se $E(X_tX_t')$ for singular, $\alpha$ n√£o √© unicamente definido pela condi√ß√£o de n√£o correla√ß√£o, embora o produto $\alpha'X_t$ seja unicamente determinado [^3]. Em casos de singularidade, pode-se remover vari√°veis redundantes para obter uma matriz n√£o singular. A proje√ß√£o linear √© frequentemente indicada como $P(Y_{t+1}|X_t) = \alpha'X_t$, ou simplificadamente, $Y_{t+1|t}^* = \alpha'X_t$.

**Proposi√ß√£o 1:** (Decomposi√ß√£o da Vari√¢ncia) *A vari√¢ncia de* $Y_{t+1}$ *pode ser decomposta na soma da vari√¢ncia da proje√ß√£o linear e a vari√¢ncia do erro da proje√ß√£o linear, ou seja:*

$$Var(Y_{t+1}) = Var(\alpha'X_t) + Var(Y_{t+1} - \alpha'X_t)$$

*Proof:*
I. Sabemos que $Y_{t+1}$ pode ser expresso como a soma de sua proje√ß√£o linear e o erro de proje√ß√£o:  $Y_{t+1} = \alpha'X_t + (Y_{t+1} - \alpha'X_t)$.
II. Usando a propriedade da vari√¢ncia de uma soma, temos: $Var(Y_{t+1}) = Var(\alpha'X_t + (Y_{t+1} - \alpha'X_t)) = Var(\alpha'X_t) + Var(Y_{t+1} - \alpha'X_t) + 2Cov(\alpha'X_t, Y_{t+1} - \alpha'X_t)$.
III. Agora, precisamos mostrar que a covari√¢ncia entre a proje√ß√£o e o erro √© zero.  Calculamos a covari√¢ncia: $Cov(\alpha'X_t, Y_{t+1} - \alpha'X_t) = E[\alpha'X_t(Y_{t+1} - \alpha'X_t)'] =  E[\alpha'X_tY_{t+1}' - \alpha'X_tX_t'\alpha]  = \alpha' E[X_tY_{t+1}'] - \alpha' E[X_tX_t']\alpha $.
IV. Pela defini√ß√£o de $\alpha$, temos que $\alpha' E[X_tX_t'] = E[Y_{t+1}X_t]$ , logo $Cov(\alpha'X_t, Y_{t+1} - \alpha'X_t) = \alpha' E[X_tY_{t+1}'] -  E[Y_{t+1}X_t] \alpha = 0$.
V.  Substituindo a covari√¢ncia zero de volta na equa√ß√£o da vari√¢ncia, temos: $Var(Y_{t+1}) = Var(\alpha'X_t) + Var(Y_{t+1} - \alpha'X_t)$.
Portanto, a vari√¢ncia de $Y_{t+1}$ √© decomposta na soma da vari√¢ncia da proje√ß√£o linear e a vari√¢ncia do erro da proje√ß√£o linear. ‚ñ†

> üí° **Exemplo Num√©rico (Decomposi√ß√£o da Vari√¢ncia):**  Vamos usar os dados do exemplo anterior para ilustrar a decomposi√ß√£o da vari√¢ncia. Primeiro, calculamos a vari√¢ncia de $Y_{t+1}$:
>
>   $\text{Var}(Y_{t+1}) = \frac{1}{5}\sum_{i=1}^{5}(Y_{t+1,i} - \bar{Y}_{t+1})^2$
>
>    Onde $\bar{Y}_{t+1} = \frac{150000+210000+280000+350000+410000}{5} = 280000$
>
>    $\text{Var}(Y_{t+1}) = \frac{(150000-280000)^2 + (210000-280000)^2 + (280000-280000)^2 + (350000-280000)^2 + (410000-280000)^2}{5} = \frac{16900000000 + 4900000000 + 0 + 4900000000 + 16900000000}{5} = \frac{43600000000}{5} = 8720000000$
>
> Agora, calculamos a vari√¢ncia da proje√ß√£o linear, $\alpha'X_t$, onde $\alpha = 2782.22$.
>
>  $\text{Var}(\alpha'X_t) = \frac{1}{5}\sum_{i=1}^{5}(\alpha X_{t,i} - \overline{\alpha X_t})^2$
>
>  Onde $\overline{\alpha X_t} = \frac{2782.22 * 50 + 2782.22 * 75 + 2782.22 * 100 + 2782.22 * 125 + 2782.22 * 150}{5} = 2782.22 * \frac{50+75+100+125+150}{5} = 2782.22 * 100 = 278222$
>
>  $\text{Var}(\alpha'X_t) = \frac{(139111-278222)^2 + (208666.5-278222)^2 + (278222-278222)^2 + (347777.5-278222)^2 + (417333-278222)^2}{5} = \frac{19347322776.84 + 4836830694.24 + 0 + 4836830694.24 + 19347322776.84}{5} = \frac{48368306942.16}{5} = 9673661388.43$
>
>  Finalmente, calculamos a vari√¢ncia do erro de proje√ß√£o:
>  $\text{Var}(Y_{t+1} - \alpha'X_t)$.  Precisamos calcular os erros primeiro:
>
>  $e_1 = 150000 - 2782.22*50 = 150000 - 139111 = 10889$
>  $e_2 = 210000 - 2782.22*75 = 210000 - 208666.5 = 1333.5$
>  $e_3 = 280000 - 2782.22*100 = 280000 - 278222 = 1778$
>  $e_4 = 350000 - 2782.22*125 = 350000 - 347777.5 = 2222.5$
>  $e_5 = 410000 - 2782.22*150 = 410000 - 417333 = -7333$
>
>  $\bar{e} = \frac{10889 + 1333.5 + 1778 + 2222.5 - 7333}{5} = 1778$
>
>  $\text{Var}(Y_{t+1} - \alpha'X_t) = \frac{(10889 - 1778)^2 + (1333.5 - 1778)^2 + (1778-1778)^2 + (2222.5-1778)^2 + (-7333-1778)^2}{5} = \frac{82985496.21 + 199384.25 + 0 + 199384.25 + 82985496.21}{5} =  33296292.18$
>
>  Note que:
>  $\text{Var}(Y_{t+1}) \approx 8720000000$
>  $\text{Var}(\alpha'X_t) \approx 9673661388.43$
>  $\text{Var}(Y_{t+1} - \alpha'X_t) \approx 33296292.18$
>
>  A soma de $\text{Var}(\alpha'X_t)$ e  $\text{Var}(Y_{t+1} - \alpha'X_t)$ n√£o resulta exatamente em  $\text{Var}(Y_{t+1})$ devido aos erros de arredondamento nos c√°lculos.  Contudo, a rela√ß√£o da decomposi√ß√£o da vari√¢ncia √© ilustrada: a vari√¢ncia da vari√°vel original √© aproximadamente a soma da vari√¢ncia da proje√ß√£o linear e a vari√¢ncia do erro da proje√ß√£o linear. Em modelos de regress√£o bem ajustados, a vari√¢ncia do erro deve ser pequena, indicando que a maior parte da vari√¢ncia de Y √© explicada pela proje√ß√£o linear.

O MSE da proje√ß√£o linear √© dado por:
$$E(Y_{t+1} - \alpha'X_t)^2 = E(Y_{t+1}^2) - E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} E(X_tY_{t+1})$$
Em muitos casos pr√°ticos, um termo constante √© inclu√≠do na proje√ß√£o. Nesse caso, podemos expressar a proje√ß√£o linear com um termo constante como:
$$\hat{E}(Y_{t+1}|X_t) = P(Y_{t+1}|1, X_t)$$

**Teorema 1:** (Proje√ß√£o Linear com Constante) *A proje√ß√£o linear de* $Y_{t+1}$ *em* $(1, X_t)$ *pode ser expressa como:*
$$P(Y_{t+1}|1, X_t) = \beta_0 + \beta'X_t$$
*onde*
$$\beta' = Cov(Y_{t+1},X_t) [Var(X_t)]^{-1}$$
*e*
$$\beta_0 = E(Y_{t+1}) - \beta'E(X_t)$$
*Proof*:
I. Seja $Z_t = [1, X_t]'$. A proje√ß√£o linear de $Y_{t+1}$ em $Z_t$ √© dada por $P(Y_{t+1}|Z_t) = \beta'Z_t$, onde $\beta' = E[Y_{t+1}Z_t'](E[Z_tZ_t'])^{-1}$.
II. Particionando $\beta' = [\beta_0, \beta]$ e notando que
$E[Z_tZ_t'] = \begin{bmatrix} E[1] & E[X_t'] \\ E[X_t] & E[X_tX_t'] \end{bmatrix} = \begin{bmatrix} 1 & E[X_t'] \\ E[X_t] & E[X_tX_t'] \end{bmatrix}$ e $E[Y_{t+1}Z_t'] = \begin{bmatrix} E[Y_{t+1}] & E[Y_{t+1}X_t'] \end{bmatrix}$.
III. Usando a condi√ß√£o de ortogonalidade $E[(Y_{t+1} - \beta_0 - \beta'X_t)(1)]=0$ e $E[(Y_{t+1} - \beta_0 - \beta'X_t)(X_t)]=0$, temos:
$E[Y_{t+1}] - \beta_0 - \beta'E[X_t]=0$ e $E[Y_{t+1}X_t] - \beta_0E[X_t] - \beta'E[X_tX_t'] = 0$.
IV. Da primeira equa√ß√£o, temos $\beta_0 = E[Y_{t+1}] - \beta'E[X_t]$.
V. Substituindo $\beta_0$ na segunda equa√ß√£o, temos:
$E[Y_{t+1}X_t] - (E[Y_{t+1}] - \beta'E[X_t])E[X_t] - \beta'E[X_tX_t'] = 0$
$E[Y_{t+1}X_t] - E[Y_{t+1}]E[X_t] = \beta'(E[X_tX_t'] - E[X_t]E[X_t'])$
$Cov(Y_{t+1}, X_t) = \beta'Var(X_t)$
VI. Resolvendo para $\beta'$, temos: $\beta' = Cov(Y_{t+1},X_t) [Var(X_t)]^{-1}$.
VII. Substituindo $\beta'$ de volta na equa√ß√£o para $\beta_0$, temos: $\beta_0 = E(Y_{t+1}) - \beta'E(X_t)$.
Portanto, a proje√ß√£o linear com constante √© expressa como $P(Y_{t+1}|1, X_t) = \beta_0 + \beta'X_t$, onde $\beta' = Cov(Y_{t+1},X_t) [Var(X_t)]^{-1}$ e $\beta_0 = E(Y_{t+1}) - \beta'E(X_t)$. ‚ñ†

> üí° **Exemplo Num√©rico (Proje√ß√£o Linear com Constante):** Vamos usar novamente os dados das casas. J√° calculamos $\alpha$ anteriormente, que era aproximadamente 2782.22. Agora, vamos calcular $\beta_0$ e $\beta$ para a proje√ß√£o linear com constante.
>
> Primeiro, calculamos a m√©dia de $X_t$:
>
>   $$E[X_t] = \frac{50 + 75 + 100 + 125 + 150}{5} = 100$$
>
> E a m√©dia de $Y_{t+1}$:
>
>   $$E[Y_{t+1}] = \frac{150000 + 210000 + 280000 + 350000 + 410000}{5} = 280000$$
>
>  J√° calculamos que $Cov(Y_{t+1}, X_t)$ √© aproximadamente $31300000 - (280000 * 100) = 31300000 - 28000000 = 3300000$ e $Var(X_t) \approx 11250 - 100^2 = 1250$
>
> Agora, calculamos $\beta$:
>
> $$\beta = \frac{Cov(Y_{t+1}, X_t)}{Var(X_t)} = \frac{3300000}{1250} = 2640$$
>
>  E calculamos $\beta_0$:
>  $$\beta_0 = E[Y_{t+1}] - \beta E[X_t] = 280000 - (2640 \times 100) = 280000 - 264000 = 16000$$
>
>  A proje√ß√£o linear com constante √©, portanto:
>  $$Y_{t+1}^* = 16000 + 2640 X_t$$
>
>   Isso significa que o pre√ßo base de uma casa seria de aproximadamente 16000 e, para cada metro quadrado a mais, o pre√ßo aumenta em 2640 unidades monet√°rias.

### Conclus√£o
A proje√ß√£o linear oferece uma forma eficiente e pr√°tica de construir previs√µes, especialmente quando a expectativa condicional √© dif√≠cil de calcular ou quando se busca uma abordagem mais simples. A condi√ß√£o fundamental para a proje√ß√£o linear √© que o erro de previs√£o seja n√£o correlacionado com as vari√°veis explicativas, garantindo que toda a informa√ß√£o relevante seja capturada de forma linear. Este m√©todo se baseia nos momentos populacionais, que podem ser estimados a partir de dados de amostra, levando √† t√©cnica de regress√£o de m√≠nimos quadrados ordin√°rios (OLS) [^3]. A conex√£o entre proje√ß√£o linear e OLS ser√° explorada em mais detalhes adiante.
### Refer√™ncias
[^1]:  Expression [4.1.1], [4.1.2], [4.1.3], [4.1.4], [4.1.5], [4.1.6], [4.1.7], [4.1.8]
[^2]: [4.1.9], [4.1.10], [4.1.11], [4.1.12], [4.1.13]
[^3]: [4.1.14], [4.1.15]
<!-- END -->
