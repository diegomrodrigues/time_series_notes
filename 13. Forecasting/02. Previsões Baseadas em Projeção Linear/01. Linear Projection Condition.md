## Previsões Baseadas em Projeção Linear
### Introdução
Este capítulo expande o conceito de previsão, introduzindo a **projeção linear** como um método para construir previsões que minimizam o erro quadrático médio (MSE). Como vimos anteriormente, a previsão ótima em termos de MSE é dada pela expectativa condicional [^1]. No entanto, o cálculo da expectativa condicional pode ser complexo e exigir informações que não estão sempre disponíveis. A projeção linear oferece uma alternativa mais simples e computacionalmente tratável, restringindo a classe de previsões a funções lineares das variáveis explicativas [^2]. Este método busca encontrar um conjunto de pesos que minimizem o MSE da previsão, garantindo que o erro de previsão seja não correlacionado com as variáveis explicativas [^2].

### Conceitos Fundamentais
A projeção linear restringe a previsão de uma variável $Y_{t+1}$ a uma função linear das variáveis explicativas $X_t$, ou seja, $Y_{t+1}^* = \alpha'X_t$ [^2]. A ideia é encontrar um vetor $\alpha$ tal que o erro de previsão $(Y_{t+1} - \alpha'X_t)$ seja não correlacionado com $X_t$. Formalmente, essa condição é expressa como:
$$E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$$
Essa condição é fundamental porque garante que a projeção linear $\alpha'X_t$ capture toda a informação linearmente relevante em $X_t$ para prever $Y_{t+1}$ [^2].

> 💡 **Exemplo Numérico:** Vamos supor que queremos prever o preço de uma casa ($Y_{t+1}$) com base na sua área em metros quadrados ($X_t$).  Imagine que temos os seguintes dados para um pequeno conjunto de casas:

| Casa | Área ($X_t$) | Preço ($Y_{t+1}$) |
|------|---------------|-----------------|
| 1    | 50            | 150000          |
| 2    | 75            | 210000          |
| 3    | 100           | 280000          |
| 4    | 125           | 350000          |
| 5    | 150           | 410000          |

Podemos representar esses dados como vetores em $\mathbb{R}^5$:
$$ X_t = \begin{bmatrix} 50 \\ 75 \\ 100 \\ 125 \\ 150 \end{bmatrix}, \quad Y_{t+1} = \begin{bmatrix} 150000 \\ 210000 \\ 280000 \\ 350000 \\ 410000 \end{bmatrix} $$

O objetivo da projeção linear é encontrar um escalar $\alpha$ (já que $X_t$ é uma única variável) de forma que $Y_{t+1}^* = \alpha X_t$ seja a melhor previsão linear.  A condição $E[(Y_{t+1} - \alpha X_t)X_t] = 0$ implica que o erro de previsão, $(Y_{t+1} - \alpha X_t)$, não deve ter relação linear com $X_t$.  Na prática, usamos a versão amostral da esperança para obter uma estimativa de $\alpha$.

**Lema 1** (Propriedade da Ortogonalidade): *A condição de não correlação* $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$ *implica que o erro de previsão* $(Y_{t+1} - \alpha'X_t)$ *é ortogonal a qualquer combinação linear das variáveis explicativas, ou seja,  para qualquer vetor* $c$, *temos* $E[(Y_{t+1} - \alpha'X_t)c'X_t]=0$.

*Proof:* Seja $c$ um vetor arbitrário.
I.  Começamos com a condição de não correlação: $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$.
II. Multiplicamos o termo dentro da expectativa por $c'$:  $E[(Y_{t+1} - \alpha'X_t)c'X_t]$
III. Usando a propriedade da linearidade da esperança, podemos tirar a constante $c'$: $c'E[(Y_{t+1} - \alpha'X_t)X_t]$
IV. Pela condição inicial, $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$, então: $c'0' = 0$.
Portanto, $E[(Y_{t+1} - \alpha'X_t)c'X_t] = 0$, o que demonstra a ortogonalidade. ■

Para verificar essa afirmação, considere qualquer função $g(X_t)$ como base para a previsão: $Y_{t+1}^* = g(X_t)$ [^1]. O erro quadrático médio nesse caso seria:
$$E[Y_{t+1} - g(X_t)]^2 = E[Y_{t+1} - E(Y_{t+1}|X_t) + E(Y_{t+1}|X_t) - g(X_t)]^2$$
$$ = E[Y_{t+1} - E(Y_{t+1}|X_t)]^2 + 2E[(Y_{t+1} - E(Y_{t+1}|X_t))(E(Y_{t+1}|X_t) - g(X_t))] + E[E(Y_{t+1}|X_t) - g(X_t)]^2$$
O termo do meio pode ser escrito como $2E[\eta_{t+1}]$ onde $\eta_{t+1} = [Y_{t+1} - E(Y_{t+1}|X_t)][E(Y_{t+1}|X_t) - g(X_t)]$ [^1]. Condicional a $X_t$,  $E(Y_{t+1}|X_t)$ e $g(X_t)$ são constantes, permitindo fatorar da expectativa:
$$E[\eta_{t+1}|X_t] = [E(Y_{t+1}|X_t) - g(X_t)] \times E([Y_{t+1} - E(Y_{t+1}|X_t)]|X_t) = 0$$
Pela lei da expectativa iterada, $E[\eta_{t+1}] = E_{X_t}[E[\eta_{t+1}|X_t]] = 0$. Substituindo de volta na equação original, temos:
$$E[Y_{t+1} - g(X_t)]^2 = E[Y_{t+1} - E(Y_{t+1}|X_t)]^2 + E[E(Y_{t+1}|X_t) - g(X_t)]^2$$
Como o segundo termo é sempre não negativo, o MSE será minimizado quando o segundo termo for igual a zero, o que ocorre quando $g(X_t) = E(Y_{t+1}|X_t)$. Portanto, a expectativa condicional é o preditor que minimiza o MSE [^1].

Expandindo o conceito, se restringirmos a função $g(X_t)$ a uma função linear $g(X_t) = \alpha'X_t$, o MSE seria:
$$E[Y_{t+1} - \alpha'X_t]^2 = E[Y_{t+1} - \alpha'X_t]^2 = E[Y_{t+1} - \alpha'X_t + \alpha'X_t - g'X_t]^2$$
$$= E[Y_{t+1} - \alpha'X_t]^2 + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t)] + E[\alpha'X_t - g'X_t]^2$$
O termo do meio também se anula, pela condição de não correlação $E[(Y_{t+1} - \alpha'X_t)X_t] = 0$. Logo,
$$E[Y_{t+1} - g'X_t]^2 = E[Y_{t+1} - \alpha'X_t]^2 + E[\alpha'X_t - g'X_t]^2$$
O MSE é minimizado quando o segundo termo é zero, o que ocorre quando $g'X_t = \alpha'X_t$, confirmando que a projeção linear minimiza o erro dentro da classe de previsões lineares [^2]. A projeção linear, então, busca encontrar um $\alpha$ que satisfaça a condição de não correlação.

Para calcular $\alpha$, partimos da condição:
$$E[(Y_{t+1} - \alpha'X_t)X_t] = E[Y_{t+1}X_t] - \alpha'E[X_tX_t'] = 0'$$
Resolvendo para $\alpha$, temos:
$$\alpha' = E[Y_{t+1}X_t] [E(X_tX_t')]^{-1}$$
Essa fórmula expressa os coeficientes da projeção linear em termos dos momentos populacionais de $Y_{t+1}$ e $X_t$ [^2].

> 💡 **Exemplo Numérico (cont.):** Continuando o exemplo das casas, podemos aproximar os momentos populacionais com os momentos amostrais.  Precisamos calcular $E[Y_{t+1}X_t]$ e $E[X_tX_t']$.
>
> Primeiro, calculamos $X_t X_t'$:
> $$X_t X_t' = \begin{bmatrix} 50 \\ 75 \\ 100 \\ 125 \\ 150 \end{bmatrix} \begin{bmatrix} 50 & 75 & 100 & 125 & 150 \end{bmatrix} = \begin{bmatrix} 2500 & 3750 & 5000 & 6250 & 7500 \\ 3750 & 5625 & 7500 & 9375 & 11250 \\ 5000 & 7500 & 10000 & 12500 & 15000 \\ 6250 & 9375 & 12500 & 15625 & 18750 \\ 7500 & 11250 & 15000 & 18750 & 22500 \end{bmatrix}$$
>
> Calculamos a média dos elementos de $X_t X_t'$ (que neste caso é uma matriz 5x5, mas estamos interessados em estimar a variância de X, e precisamos da média dos quadrados):
>
> $$E[X_tX_t'] \approx \frac{1}{5} \sum_{i=1}^{5} X_{t,i}^2 = \frac{50^2 + 75^2 + 100^2 + 125^2 + 150^2}{5} = \frac{2500 + 5625 + 10000 + 15625 + 22500}{5} = \frac{56250}{5} = 11250$$
>
>
> Agora, calculamos $Y_{t+1}X_t$:
>
> $$Y_{t+1}X_t = \begin{bmatrix} 150000 \\ 210000 \\ 280000 \\ 350000 \\ 410000 \end{bmatrix} \begin{bmatrix} 50 \\ 75 \\ 100 \\ 125 \\ 150 \end{bmatrix} = \begin{bmatrix} 150000 \times 50 \\ 210000 \times 75 \\ 280000 \times 100 \\ 350000 \times 125 \\ 410000 \times 150 \end{bmatrix} = \begin{bmatrix} 7500000 \\ 15750000 \\ 28000000 \\ 43750000 \\ 61500000 \end{bmatrix}$$
>
>
> Calculamos a média dos elementos de $Y_{t+1}X_t$:
>
> $$E[Y_{t+1}X_t] \approx \frac{1}{5} \sum_{i=1}^{5} Y_{t+1,i}X_{t,i} = \frac{7500000 + 15750000 + 28000000 + 43750000 + 61500000}{5} = \frac{156500000}{5} = 31300000$$
>
>
> Agora, calculamos $\alpha$:
> $$\alpha = \frac{E[Y_{t+1}X_t]}{E[X_tX_t']} = \frac{31300000}{11250} \approx 2782.22$$
>
> Portanto, a projeção linear seria $Y_{t+1}^* = 2782.22 X_t$. Isso significa que, para cada metro quadrado a mais na área da casa, o preço previsto aumenta em aproximadamente 2782.22 unidades monetárias.

É importante observar que se $E(X_tX_t')$ for singular, $\alpha$ não é unicamente definido pela condição de não correlação, embora o produto $\alpha'X_t$ seja unicamente determinado [^3]. Em casos de singularidade, pode-se remover variáveis redundantes para obter uma matriz não singular. A projeção linear é frequentemente indicada como $P(Y_{t+1}|X_t) = \alpha'X_t$, ou simplificadamente, $Y_{t+1|t}^* = \alpha'X_t$.

**Proposição 1:** (Decomposição da Variância) *A variância de* $Y_{t+1}$ *pode ser decomposta na soma da variância da projeção linear e a variância do erro da projeção linear, ou seja:*

$$Var(Y_{t+1}) = Var(\alpha'X_t) + Var(Y_{t+1} - \alpha'X_t)$$

*Proof:*
I. Sabemos que $Y_{t+1}$ pode ser expresso como a soma de sua projeção linear e o erro de projeção:  $Y_{t+1} = \alpha'X_t + (Y_{t+1} - \alpha'X_t)$.
II. Usando a propriedade da variância de uma soma, temos: $Var(Y_{t+1}) = Var(\alpha'X_t + (Y_{t+1} - \alpha'X_t)) = Var(\alpha'X_t) + Var(Y_{t+1} - \alpha'X_t) + 2Cov(\alpha'X_t, Y_{t+1} - \alpha'X_t)$.
III. Agora, precisamos mostrar que a covariância entre a projeção e o erro é zero.  Calculamos a covariância: $Cov(\alpha'X_t, Y_{t+1} - \alpha'X_t) = E[\alpha'X_t(Y_{t+1} - \alpha'X_t)'] =  E[\alpha'X_tY_{t+1}' - \alpha'X_tX_t'\alpha]  = \alpha' E[X_tY_{t+1}'] - \alpha' E[X_tX_t']\alpha $.
IV. Pela definição de $\alpha$, temos que $\alpha' E[X_tX_t'] = E[Y_{t+1}X_t]$ , logo $Cov(\alpha'X_t, Y_{t+1} - \alpha'X_t) = \alpha' E[X_tY_{t+1}'] -  E[Y_{t+1}X_t] \alpha = 0$.
V.  Substituindo a covariância zero de volta na equação da variância, temos: $Var(Y_{t+1}) = Var(\alpha'X_t) + Var(Y_{t+1} - \alpha'X_t)$.
Portanto, a variância de $Y_{t+1}$ é decomposta na soma da variância da projeção linear e a variância do erro da projeção linear. ■

> 💡 **Exemplo Numérico (Decomposição da Variância):**  Vamos usar os dados do exemplo anterior para ilustrar a decomposição da variância. Primeiro, calculamos a variância de $Y_{t+1}$:
>
>   $\text{Var}(Y_{t+1}) = \frac{1}{5}\sum_{i=1}^{5}(Y_{t+1,i} - \bar{Y}_{t+1})^2$
>
>    Onde $\bar{Y}_{t+1} = \frac{150000+210000+280000+350000+410000}{5} = 280000$
>
>    $\text{Var}(Y_{t+1}) = \frac{(150000-280000)^2 + (210000-280000)^2 + (280000-280000)^2 + (350000-280000)^2 + (410000-280000)^2}{5} = \frac{16900000000 + 4900000000 + 0 + 4900000000 + 16900000000}{5} = \frac{43600000000}{5} = 8720000000$
>
> Agora, calculamos a variância da projeção linear, $\alpha'X_t$, onde $\alpha = 2782.22$.
>
>  $\text{Var}(\alpha'X_t) = \frac{1}{5}\sum_{i=1}^{5}(\alpha X_{t,i} - \overline{\alpha X_t})^2$
>
>  Onde $\overline{\alpha X_t} = \frac{2782.22 * 50 + 2782.22 * 75 + 2782.22 * 100 + 2782.22 * 125 + 2782.22 * 150}{5} = 2782.22 * \frac{50+75+100+125+150}{5} = 2782.22 * 100 = 278222$
>
>  $\text{Var}(\alpha'X_t) = \frac{(139111-278222)^2 + (208666.5-278222)^2 + (278222-278222)^2 + (347777.5-278222)^2 + (417333-278222)^2}{5} = \frac{19347322776.84 + 4836830694.24 + 0 + 4836830694.24 + 19347322776.84}{5} = \frac{48368306942.16}{5} = 9673661388.43$
>
>  Finalmente, calculamos a variância do erro de projeção:
>  $\text{Var}(Y_{t+1} - \alpha'X_t)$.  Precisamos calcular os erros primeiro:
>
>  $e_1 = 150000 - 2782.22*50 = 150000 - 139111 = 10889$
>  $e_2 = 210000 - 2782.22*75 = 210000 - 208666.5 = 1333.5$
>  $e_3 = 280000 - 2782.22*100 = 280000 - 278222 = 1778$
>  $e_4 = 350000 - 2782.22*125 = 350000 - 347777.5 = 2222.5$
>  $e_5 = 410000 - 2782.22*150 = 410000 - 417333 = -7333$
>
>  $\bar{e} = \frac{10889 + 1333.5 + 1778 + 2222.5 - 7333}{5} = 1778$
>
>  $\text{Var}(Y_{t+1} - \alpha'X_t) = \frac{(10889 - 1778)^2 + (1333.5 - 1778)^2 + (1778-1778)^2 + (2222.5-1778)^2 + (-7333-1778)^2}{5} = \frac{82985496.21 + 199384.25 + 0 + 199384.25 + 82985496.21}{5} =  33296292.18$
>
>  Note que:
>  $\text{Var}(Y_{t+1}) \approx 8720000000$
>  $\text{Var}(\alpha'X_t) \approx 9673661388.43$
>  $\text{Var}(Y_{t+1} - \alpha'X_t) \approx 33296292.18$
>
>  A soma de $\text{Var}(\alpha'X_t)$ e  $\text{Var}(Y_{t+1} - \alpha'X_t)$ não resulta exatamente em  $\text{Var}(Y_{t+1})$ devido aos erros de arredondamento nos cálculos.  Contudo, a relação da decomposição da variância é ilustrada: a variância da variável original é aproximadamente a soma da variância da projeção linear e a variância do erro da projeção linear. Em modelos de regressão bem ajustados, a variância do erro deve ser pequena, indicando que a maior parte da variância de Y é explicada pela projeção linear.

O MSE da projeção linear é dado por:
$$E(Y_{t+1} - \alpha'X_t)^2 = E(Y_{t+1}^2) - E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} E(X_tY_{t+1})$$
Em muitos casos práticos, um termo constante é incluído na projeção. Nesse caso, podemos expressar a projeção linear com um termo constante como:
$$\hat{E}(Y_{t+1}|X_t) = P(Y_{t+1}|1, X_t)$$

**Teorema 1:** (Projeção Linear com Constante) *A projeção linear de* $Y_{t+1}$ *em* $(1, X_t)$ *pode ser expressa como:*
$$P(Y_{t+1}|1, X_t) = \beta_0 + \beta'X_t$$
*onde*
$$\beta' = Cov(Y_{t+1},X_t) [Var(X_t)]^{-1}$$
*e*
$$\beta_0 = E(Y_{t+1}) - \beta'E(X_t)$$
*Proof*:
I. Seja $Z_t = [1, X_t]'$. A projeção linear de $Y_{t+1}$ em $Z_t$ é dada por $P(Y_{t+1}|Z_t) = \beta'Z_t$, onde $\beta' = E[Y_{t+1}Z_t'](E[Z_tZ_t'])^{-1}$.
II. Particionando $\beta' = [\beta_0, \beta]$ e notando que
$E[Z_tZ_t'] = \begin{bmatrix} E[1] & E[X_t'] \\ E[X_t] & E[X_tX_t'] \end{bmatrix} = \begin{bmatrix} 1 & E[X_t'] \\ E[X_t] & E[X_tX_t'] \end{bmatrix}$ e $E[Y_{t+1}Z_t'] = \begin{bmatrix} E[Y_{t+1}] & E[Y_{t+1}X_t'] \end{bmatrix}$.
III. Usando a condição de ortogonalidade $E[(Y_{t+1} - \beta_0 - \beta'X_t)(1)]=0$ e $E[(Y_{t+1} - \beta_0 - \beta'X_t)(X_t)]=0$, temos:
$E[Y_{t+1}] - \beta_0 - \beta'E[X_t]=0$ e $E[Y_{t+1}X_t] - \beta_0E[X_t] - \beta'E[X_tX_t'] = 0$.
IV. Da primeira equação, temos $\beta_0 = E[Y_{t+1}] - \beta'E[X_t]$.
V. Substituindo $\beta_0$ na segunda equação, temos:
$E[Y_{t+1}X_t] - (E[Y_{t+1}] - \beta'E[X_t])E[X_t] - \beta'E[X_tX_t'] = 0$
$E[Y_{t+1}X_t] - E[Y_{t+1}]E[X_t] = \beta'(E[X_tX_t'] - E[X_t]E[X_t'])$
$Cov(Y_{t+1}, X_t) = \beta'Var(X_t)$
VI. Resolvendo para $\beta'$, temos: $\beta' = Cov(Y_{t+1},X_t) [Var(X_t)]^{-1}$.
VII. Substituindo $\beta'$ de volta na equação para $\beta_0$, temos: $\beta_0 = E(Y_{t+1}) - \beta'E(X_t)$.
Portanto, a projeção linear com constante é expressa como $P(Y_{t+1}|1, X_t) = \beta_0 + \beta'X_t$, onde $\beta' = Cov(Y_{t+1},X_t) [Var(X_t)]^{-1}$ e $\beta_0 = E(Y_{t+1}) - \beta'E(X_t)$. ■

> 💡 **Exemplo Numérico (Projeção Linear com Constante):** Vamos usar novamente os dados das casas. Já calculamos $\alpha$ anteriormente, que era aproximadamente 2782.22. Agora, vamos calcular $\beta_0$ e $\beta$ para a projeção linear com constante.
>
> Primeiro, calculamos a média de $X_t$:
>
>   $$E[X_t] = \frac{50 + 75 + 100 + 125 + 150}{5} = 100$$
>
> E a média de $Y_{t+1}$:
>
>   $$E[Y_{t+1}] = \frac{150000 + 210000 + 280000 + 350000 + 410000}{5} = 280000$$
>
>  Já calculamos que $Cov(Y_{t+1}, X_t)$ é aproximadamente $31300000 - (280000 * 100) = 31300000 - 28000000 = 3300000$ e $Var(X_t) \approx 11250 - 100^2 = 1250$
>
> Agora, calculamos $\beta$:
>
> $$\beta = \frac{Cov(Y_{t+1}, X_t)}{Var(X_t)} = \frac{3300000}{1250} = 2640$$
>
>  E calculamos $\beta_0$:
>  $$\beta_0 = E[Y_{t+1}] - \beta E[X_t] = 280000 - (2640 \times 100) = 280000 - 264000 = 16000$$
>
>  A projeção linear com constante é, portanto:
>  $$Y_{t+1}^* = 16000 + 2640 X_t$$
>
>   Isso significa que o preço base de uma casa seria de aproximadamente 16000 e, para cada metro quadrado a mais, o preço aumenta em 2640 unidades monetárias.

### Conclusão
A projeção linear oferece uma forma eficiente e prática de construir previsões, especialmente quando a expectativa condicional é difícil de calcular ou quando se busca uma abordagem mais simples. A condição fundamental para a projeção linear é que o erro de previsão seja não correlacionado com as variáveis explicativas, garantindo que toda a informação relevante seja capturada de forma linear. Este método se baseia nos momentos populacionais, que podem ser estimados a partir de dados de amostra, levando à técnica de regressão de mínimos quadrados ordinários (OLS) [^3]. A conexão entre projeção linear e OLS será explorada em mais detalhes adiante.
### Referências
[^1]:  Expression [4.1.1], [4.1.2], [4.1.3], [4.1.4], [4.1.5], [4.1.6], [4.1.7], [4.1.8]
[^2]: [4.1.9], [4.1.10], [4.1.11], [4.1.12], [4.1.13]
[^3]: [4.1.14], [4.1.15]
<!-- END -->
