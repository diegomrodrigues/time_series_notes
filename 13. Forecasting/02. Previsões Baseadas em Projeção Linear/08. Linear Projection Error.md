## A Proje√ß√£o Linear e a Minimiza√ß√£o do Erro Quadr√°tico M√©dio Dentro da Classe de Previs√µes Lineares

### Introdu√ß√£o

Este cap√≠tulo explora a fundo a **otimalidade da proje√ß√£o linear** no contexto de previs√µes de s√©ries temporais. Como discutido anteriormente, a proje√ß√£o linear, expressa como $P(Y_{t+1}|X_t) = \alpha'X_t$, busca a melhor aproxima√ß√£o linear para $Y_{t+1}$ com base nas vari√°veis explicativas $X_t$ [^2]. A condi√ß√£o fundamental para a proje√ß√£o linear, $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$, garante que o erro de previs√£o seja n√£o correlacionado com as vari√°veis explicativas [^2]. Este cap√≠tulo oferece uma prova matem√°tica detalhada de que essa proje√ß√£o linear minimiza o erro quadr√°tico m√©dio (MSE) dentro da classe de previs√µes lineares, demonstrando uma otimalidade an√°loga √† da expectativa condicional em rela√ß√£o a todas as previs√µes poss√≠veis [^1].

### A Otimidade da Proje√ß√£o Linear: Prova Formal

A otimalidade da proje√ß√£o linear reside no fato de que ela minimiza o MSE dentro da classe de previs√µes lineares. Para provar essa afirma√ß√£o, vamos considerar qualquer fun√ß√£o linear $g(X_t)$ como uma candidata para prever $Y_{t+1}$, ou seja, $Y_{t+1}^* = g(X_t)$. O erro quadr√°tico m√©dio nesse caso seria:
$$E[Y_{t+1} - g(X_t)]^2$$
Podemos reescrever esse MSE adicionando e subtraindo a proje√ß√£o linear $\alpha'X_t$:
$$E[(Y_{t+1} - \alpha'X_t + \alpha'X_t - g(X_t))^2]$$
Expandindo o termo dentro da esperan√ßa, obtemos:
$$E[(Y_{t+1} - \alpha'X_t)^2 + 2(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g(X_t)) + (\alpha'X_t - g(X_t))^2]$$
Pela propriedade da linearidade da esperan√ßa, podemos escrever:
$$E[(Y_{t+1} - \alpha'X_t)^2] + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g(X_t))] + E[(\alpha'X_t - g(X_t))^2]$$
O termo central √© crucial para demonstrar a otimalidade. Definimos $\eta_{t+1} = (Y_{t+1} - \alpha'X_t)(\alpha'X_t - g(X_t))$. Pela condi√ß√£o de n√£o correla√ß√£o, sabemos que $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$, e podemos mostrar que a esperan√ßa do termo do meio se anula:
$$E[\eta_{t+1}] = E[E[\eta_{t+1}|X_t]] = E[(Y_{t+1} - \alpha'X_t)E[\alpha'X_t - g(X_t)|X_t]] = 0$$
Logo, o MSE se torna:
$$E[(Y_{t+1} - g(X_t))^2] = E[(Y_{t+1} - \alpha'X_t)^2] + E[(\alpha'X_t - g(X_t))^2]$$
O segundo termo √© sempre n√£o negativo, ent√£o, o MSE √© minimizado quando esse termo √© igual a zero, o que ocorre apenas quando $g(X_t) = \alpha'X_t$. Portanto, a proje√ß√£o linear $\alpha'X_t$ minimiza o MSE dentro da classe das previs√µes lineares [^1].

> üí° **Exemplo Num√©rico:**
> Suponha que desejamos prever o n√∫mero de vendas de um produto ($Y_{t+1}$) com base nos gastos com marketing digital ($X_t$) em milhares de unidades monet√°rias. Temos alguns dados:
>
> | t  | Gasto com Marketing ($X_t$) | Vendas ($Y_{t+1}$) |
> |----|-----------------------------|-------------------|
> | 1  | 2                           | 5                |
> | 2  | 3                           | 8                |
> | 3  | 4                           | 10               |
> | 4  | 5                           | 13               |
> | 5  | 6                           | 15               |
>
> Para calcular $\alpha$  usando os dados da amostra, seguimos os seguintes passos:
>
> 1.  **Calculamos a m√©dia de $X_t$ e $Y_{t+1}$:**
>     $$\bar{X} = \frac{2 + 3 + 4 + 5 + 6}{5} = 4$$
>     $$\bar{Y} = \frac{5 + 8 + 10 + 13 + 15}{5} = 10.2$$
>
> 2. **Calculamos $E(X_t^2)$:**
>    $$E[X_t^2] = \frac{2^2 + 3^2 + 4^2 + 5^2 + 6^2}{5} = \frac{4+9+16+25+36}{5} = 18$$
>
> 3. **Calculamos $E(Y_{t+1}X_t)$:**
>     $$E[Y_{t+1}X_t] = \frac{2\cdot5 + 3\cdot8 + 4\cdot10 + 5\cdot13 + 6\cdot15}{5} = \frac{10+24+40+65+90}{5} = 45.8$$
>
> 4.  **Calculamos $\alpha$:**
>    $$\alpha = \frac{E[Y_{t+1}X_t]}{E[X_t^2]} = \frac{45.8}{18} \approx 2.54$$
>
> Portanto, a proje√ß√£o linear √© dada por:
> $$\hat{Y}_{t+1} = 2.54 X_t$$
> Para demonstrar a otimalidade, vamos assumir uma fun√ß√£o de previs√£o linear diferente como $g(X_t) = 2 X_t$, e analisar o MSE de cada proje√ß√£o.
> Primeiro, com nossa proje√ß√£o $\hat{Y}_{t+1} = 2.54 X_t$, temos:
>
> | t  | $X_t$ | $Y_{t+1}$ | $\hat{Y}_{t+1} = 2.54 X_t$ | $Y_{t+1} - \hat{Y}_{t+1}$ | $(Y_{t+1} - \hat{Y}_{t+1})^2$ |
> |----|-------|-----------|--------------------------|-----------------------------|-----------------------------|
> | 1  | 2     | 5         | 5.08                      | -0.08                         | 0.0064                       |
> | 2  | 3     | 8         | 7.62                      | 0.38                         | 0.1444                       |
> | 3  | 4     | 10        | 10.16                      | -0.16                         | 0.0256                       |
> | 4  | 5     | 13        | 12.70                      | 0.30                         | 0.09                         |
> | 5  | 6     | 15        | 15.24                      | -0.24                         | 0.0576                       |
>
> $$MSE_{proje√ß√£o} = \frac{0.0064 + 0.1444 + 0.0256 + 0.09 + 0.0576}{5} \approx 0.06$$
>
> Agora, calculamos os valores com $g(X_t) = 2X_t$:
>
> | t  | $X_t$ | $Y_{t+1}$ | $g(X_t) = 2X_t$ | $Y_{t+1} - g(X_t)$ | $(Y_{t+1} - g(X_t))^2$ |
> |----|-------|-----------|--------------------|--------------------|-----------------------|
> | 1  | 2     | 5         | 4                  | 1                  | 1                     |
> | 2  | 3     | 8         | 6                  | 2                  | 4                     |
> | 3  | 4     | 10        | 8                  | 2                  | 4                     |
> | 4  | 5     | 13        | 10                 | 3                  | 9                     |
> | 5  | 6     | 15        | 12                 | 3                  | 9                     |
> $$MSE_{g} = \frac{1+4+4+9+9}{5} = \frac{27}{5} = 5.4$$
>
> Conforme demonstrado, $MSE_{proje√ß√£o} < MSE_{g}$, o que ilustra que a proje√ß√£o linear, com o $\alpha$ calculado usando a condi√ß√£o de n√£o correla√ß√£o, minimiza o MSE dentro da classe das previs√µes lineares.

A condi√ß√£o $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$ pode ser interpretada como uma condi√ß√£o de ortogonalidade entre o erro de previs√£o e as vari√°veis explicativas. Ou seja, o erro de previs√£o √© ortogonal ao espa√ßo vetorial gerado pelas vari√°veis explicativas, o que √© essencial para a otimalidade da proje√ß√£o linear [^2].

**Teorema 1:** (Proje√ß√£o Linear e Minimiza√ß√£o do MSE) *A proje√ß√£o linear* $P(Y_{t+1}|X_t) = \alpha'X_t$ *com*  $\alpha' = E[Y_{t+1}X_t] [E(X_tX_t')]^{-1}$ *minimiza o erro quadr√°tico m√©dio (MSE) da previs√£o dentro da classe de todas as fun√ß√µes lineares de* $X_t$.
*Proof:*
I. Seja $g(X_t)$ uma fun√ß√£o linear qualquer, representando uma previs√£o de $Y_{t+1}$ com base em $X_t$.
II. O erro da previs√£o √© dado por $e_{t+1} = Y_{t+1} - g(X_t)$, e o MSE √© dado por $E[e_{t+1}^2] = E[(Y_{t+1} - g(X_t))^2]$.
III.  Queremos mostrar que o MSE √© minimizado quando $g(X_t) = \alpha'X_t$, onde  $\alpha' = E[Y_{t+1}X_t] [E(X_tX_t')]^{-1}$ √© o vetor de coeficientes da proje√ß√£o linear.
IV. Podemos reescrever o erro da previs√£o como:
$$Y_{t+1} - g(X_t) = Y_{t+1} - \alpha'X_t + \alpha'X_t - g(X_t)$$
V.  Elevando ao quadrado e tomando a esperan√ßa:
$$E[(Y_{t+1} - g(X_t))^2] = E[(Y_{t+1} - \alpha'X_t + \alpha'X_t - g(X_t))^2]$$
VI. Expandindo o quadrado:
$$E[(Y_{t+1} - g(X_t))^2] = E[(Y_{t+1} - \alpha'X_t)^2] + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g(X_t))] + E[(\alpha'X_t - g(X_t))^2]$$
VII. O termo do meio pode ser reescrito como $2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g(X_t))] = 2E[(Y_{t+1} - \alpha'X_t)h(X_t)]$ onde $h(X_t)$ √© uma combina√ß√£o linear de $X_t$. Pela condi√ß√£o de ortogonalidade, o termo do meio se anula.
VIII. Assim, temos:
$$E[(Y_{t+1} - g(X_t))^2] = E[(Y_{t+1} - \alpha'X_t)^2] + E[(\alpha'X_t - g(X_t))^2]$$
IX. O primeiro termo √© o MSE da proje√ß√£o linear $MSE_{\alpha} = E[(Y_{t+1} - \alpha'X_t)^2]$ e o segundo termo, $E[(\alpha'X_t - g(X_t))^2]$, √© sempre n√£o negativo.
X. O MSE da fun√ß√£o $g(X_t)$ √© minimizado quando $E[(\alpha'X_t - g(X_t))^2]=0$, o que ocorre quando $\alpha'X_t = g(X_t)$.
XI. Portanto, a proje√ß√£o linear $P(Y_{t+1}|X_t) = \alpha'X_t$ minimiza o MSE dentro da classe de fun√ß√µes lineares de $X_t$. $\blacksquare$

**Lema 1.1** (Equival√™ncia do MSE da Proje√ß√£o Linear e da Expectativa Condicional): *Se a expectativa condicional* $E(Y_{t+1}|X_t)$ *√© uma fun√ß√£o linear de* $X_t$, *ent√£o o MSE da proje√ß√£o linear* $P(Y_{t+1}|X_t)$ *√© igual ao MSE da expectativa condicional.*

*Proof:*
I. Seja $E(Y_{t+1}|X_t) = \gamma'X_t$ uma fun√ß√£o linear de $X_t$, e  $\gamma' = E(Y_{t+1}X_t)[E(X_tX_t')]^{-1}$ o vetor de coeficientes da expectativa condicional.
II. Se a expectativa condicional √© linear, sabemos que a proje√ß√£o linear ser√° igual √† expectativa condicional: $P(Y_{t+1}|X_t) = E(Y_{t+1}|X_t) = \gamma'X_t$, ou seja, $\gamma = \alpha$.
III.  O MSE da proje√ß√£o linear √© dado por: $MSE_{P} = E[(Y_{t+1} - \alpha'X_t)^2]$
IV. O MSE da expectativa condicional √© dado por: $MSE_{EC} = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] = E[(Y_{t+1} - \gamma'X_t)^2]$
V. Como $\alpha' = \gamma'$, temos: $MSE_{P} = MSE_{EC}$, e os MSEs s√£o iguais. $\blacksquare$

Esse resultado not√°vel implica que, quando a expectativa condicional √© linear, a proje√ß√£o linear, que √© computacionalmente mais simples, oferece o mesmo desempenho da expectativa condicional em termos de MSE, o que refor√ßa sua relev√¢ncia pr√°tica.

### O Significado do MSE M√≠nimo
A propriedade de minimizar o MSE dentro da classe de previs√µes lineares, significa que a proje√ß√£o linear captura toda a informa√ß√£o linearmente relevante em $X_t$ para prever $Y_{t+1}$. N√£o h√° outra fun√ß√£o linear de $X_t$ que possa fornecer uma previs√£o com MSE menor. Isso n√£o implica que a proje√ß√£o linear seja a melhor previs√£o poss√≠vel dentro de todas as previs√µes n√£o lineares (essa seria dada pela expectativa condicional), mas sim a melhor dentro do conjunto de previs√µes lineares.

> üí° **Exemplo Num√©rico:**
> Vamos considerar um caso em que a rela√ß√£o entre $Y_{t+1}$ e $X_t$ √© n√£o linear.  Suponha que $Y_{t+1} = X_t^2$ e que temos os seguintes dados:
>
> | t | $X_t$ | $Y_{t+1}$ |
> |---|-------|-----------|
> | 1 | 1     | 1         |
> | 2 | 2     | 4         |
> | 3 | 3     | 9         |
> | 4 | 4     | 16        |
> | 5 | 5     | 25        |
>
> Vamos calcular a proje√ß√£o linear de $Y_{t+1}$ sobre $X_t$, ou seja, queremos encontrar $\alpha$ tal que $P(Y_{t+1}|X_t) = \alpha X_t$.
>
> 1. **Calculamos $E(X_t^2)$:**
>    $$E[X_t^2] = \frac{1^2 + 2^2 + 3^2 + 4^2 + 5^2}{5} = \frac{1 + 4 + 9 + 16 + 25}{5} = \frac{55}{5} = 11$$
>
> 2. **Calculamos $E(Y_{t+1}X_t)$:**
>    $$E[Y_{t+1}X_t] = \frac{1\cdot1 + 2\cdot4 + 3\cdot9 + 4\cdot16 + 5\cdot25}{5} = \frac{1 + 8 + 27 + 64 + 125}{5} = \frac{225}{5} = 45$$
>
> 3.  **Calculamos $\alpha$:**
> $$\alpha = \frac{E[Y_{t+1}X_t]}{E[X_t^2]} = \frac{45}{11} \approx 4.09$$
>
> A proje√ß√£o linear √© ent√£o $\hat{Y}_{t+1} = 4.09X_t$.
>
> Note que a proje√ß√£o linear n√£o consegue capturar perfeitamente a rela√ß√£o n√£o linear entre $Y_{t+1}$ e $X_t$, pois a verdadeira rela√ß√£o √© dada por $Y_{t+1} = X_t^2$. No entanto, a proje√ß√£o linear oferece a melhor aproxima√ß√£o linear poss√≠vel, minimizando o MSE dentro da classe de fun√ß√µes lineares.
>
> Para verificar o MSE, podemos calcular a soma dos erros ao quadrado:
>
> | t | $X_t$ | $Y_{t+1}$ | $\hat{Y}_{t+1} = 4.09 X_t$ | $Y_{t+1} - \hat{Y}_{t+1}$ | $(Y_{t+1} - \hat{Y}_{t+1})^2$ |
> |---|-------|-----------|---------------------------|---------------------------|-----------------------------|
> | 1 | 1     | 1         | 4.09                      | -3.09                     | 9.54                        |
> | 2 | 2     | 4         | 8.18                      | -4.18                     | 17.47                       |
> | 3 | 3     | 9         | 12.27                     | -3.27                     | 10.69                       |
> | 4 | 4     | 16        | 16.36                     | -0.36                     | 0.13                        |
> | 5 | 5     | 25        | 20.45                     | 4.55                      | 20.70                       |
> $$MSE = \frac{9.54 + 17.47 + 10.69 + 0.13 + 20.70}{5} \approx 11.71$$
> Se utilizarmos outra fun√ß√£o linear, como $g(X_t) = 5X_t$ (uma proje√ß√£o linear diferente de $\alpha'X_t$), o MSE ser√°:
> | t | $X_t$ | $Y_{t+1}$ | $g(X_t) = 5 X_t$ | $Y_{t+1} - g(X_t)$ | $(Y_{t+1} - g(X_t))^2$ |
> |---|-------|-----------|-------------------|--------------------|-----------------------|
> | 1 | 1     | 1         | 5                 | -4                 | 16                    |
> | 2 | 2     | 4         | 10                | -6                 | 36                    |
> | 3 | 3     | 9         | 15                | -6                 | 36                    |
> | 4 | 4     | 16        | 20                | -4                 | 16                    |
> | 5 | 5     | 25        | 25                | 0                  | 0                     |
>
> $$MSE_{g} = \frac{16 + 36 + 36 + 16 + 0}{5} = \frac{104}{5} = 20.8$$
>
> O MSE da proje√ß√£o linear (11.71) √© menor que o MSE de uma proje√ß√£o linear diferente (20.8), confirmando que a proje√ß√£o linear minimiza o erro dentro da classe linear. Embora ambas sejam proje√ß√µes lineares, apenas aquela derivada da condi√ß√£o de ortogonalidade garante o menor erro poss√≠vel dentro de sua classe, e √© equivalente ao estimador de m√≠nimos quadrados (com intercepto nulo).

### Formaliza√ß√£o Matem√°tica da Minimiza√ß√£o do MSE

A prova matem√°tica da minimiza√ß√£o do MSE pela proje√ß√£o linear √© baseada na condi√ß√£o de ortogonalidade:
$$E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$$
Essa condi√ß√£o implica que o erro de previs√£o $e_{t+1} = Y_{t+1} - \alpha'X_t$ √© n√£o correlacionado com as vari√°veis explicativas $X_t$. Para ver isso, podemos expandir a equa√ß√£o acima para:
$$E[Y_{t+1}X_t] - \alpha'E[X_tX_t'] = 0'$$
Resolvendo para $\alpha'$, obtemos:
$$\alpha' = E[Y_{t+1}X_t] [E(X_tX_t')]^{-1}$$
Essa equa√ß√£o expressa os coeficientes da proje√ß√£o linear em termos dos momentos populacionais.

  **Teorema 2:** (Decomposi√ß√£o da Vari√¢ncia)  *A vari√¢ncia de Y_{t+1} pode ser expressa como a soma da vari√¢ncia de sua proje√ß√£o linear sobre X_t e a vari√¢ncia do erro de proje√ß√£o:*
   $$Var(Y_{t+1}) = Var(\alpha'X_t) + Var(Y_{t+1} - \alpha'X_t)$$
  *Proof:*
  I.  Pela defini√ß√£o de proje√ß√£o linear, $Y_{t+1} = \alpha'X_t + e_{t+1}$ , onde $e_{t+1} = Y_{t+1} - \alpha'X_t$ √© o erro de proje√ß√£o.
  II.  Aplicando a propriedade da vari√¢ncia de uma soma:
   $Var(Y_{t+1}) = Var(\alpha'X_t + e_{t+1}) = Var(\alpha'X_t) + Var(e_{t+1}) + 2Cov(\alpha'X_t, e_{t+1})$.
 III. Pelo lema 1.1, sabemos que $Cov(\alpha'X_t, e_{t+1})=0$.
 IV. Portanto, $Var(Y_{t+1}) = Var(\alpha'X_t) + Var(e_{t+1})$, onde $Var(e_{t+1})$ √© o MSE da proje√ß√£o linear.  $\blacksquare$

  Essa decomposi√ß√£o demonstra que o MSE da proje√ß√£o linear √© uma medida da vari√¢ncia de $Y_{t+1}$ que n√£o √© explicada pela proje√ß√£o linear.

**Corol√°rio 2.1:** (Rela√ß√£o entre $R^2$ e MSE) *O coeficiente de determina√ß√£o* $R^2$ *da proje√ß√£o linear, que mede a propor√ß√£o da vari√¢ncia de* $Y_{t+1}$ *explicada pela proje√ß√£o, √© dado por:*

 $$ R^2 = \frac{Var(\alpha'X_t)}{Var(Y_{t+1})} = 1 - \frac{Var(Y_{t+1}-\alpha'X_t)}{Var(Y_{t+1})}$$

 *Proof:*
I. Do Teorema 2, temos $Var(Y_{t+1}) = Var(\alpha'X_t) + Var(Y_{t+1}-\alpha'X_t)$.
II. Reorganizando, temos $Var(\alpha'X_t) = Var(Y_{t+1}) - Var(Y_{t+1}-\alpha'X_t)$.
III. Dividindo ambos os lados por $Var(Y_{t+1})$, obtemos $\frac{Var(\alpha'X_t)}{Var(Y_{t+1})} = 1 - \frac{Var(Y_{t+1}-\alpha'X_t)}{Var(Y_{t+1})}$.
IV. Pela defini√ß√£o de $R^2$, temos o resultado desejado. $\blacksquare$

Este resultado conecta o $R^2$, uma m√©trica comum para avaliar o ajuste de modelos lineares, com o MSE da proje√ß√£o linear, mostrando que um maior $R^2$ corresponde a um menor MSE relativo em rela√ß√£o √† vari√¢ncia de $Y_{t+1}$.

> üí° **Exemplo Num√©rico:**
> Suponha que temos os seguintes dados para a vari√°vel dependente $Y_{t+1}$ e a vari√°vel independente $X_t$:
>
> | t | $X_t$ | $Y_{t+1}$ |
> |---|-------|-----------|
> | 1 | 1     | 2         |
> | 2 | 2     | 3         |
> | 3 | 3     | 5         |
> | 4 | 4     | 6         |
> | 5 | 5     | 8         |
>
> Primeiro, calculamos a proje√ß√£o linear $\hat{Y}_{t+1} = \alpha X_t$. J√° calculamos que $\alpha =  \frac{E[Y_{t+1}X_t]}{E[X_t^2]}$.
>
> 1. **Calculamos $E(X_t^2)$:**
>    $$E[X_t^2] = \frac{1^2 + 2^2 + 3^2 + 4^2 + 5^2}{5} = \frac{1 + 4 + 9 + 16 + 25}{5} = \frac{55}{5} = 11$$
>
> 2. **Calculamos $E(Y_{t+1}X_t)$:**
>    $$E[Y_{t+1}X_t] = \frac{1\cdot2 + 2\cdot3 + 3\cdot5 + 4\cdot6 + 5\cdot8}{5} = \frac{2 + 6 + 15 + 24 + 40}{5} = \frac{87}{5} = 17.4$$
>
> 3. **Calculamos $\alpha$:**
>    $$\alpha = \frac{E[Y_{t+1}X_t]}{E[X_t^2]} = \frac{17.4}{11} \approx 1.58$$
>
>  Assim, a proje√ß√£o linear √©  $\hat{Y}_{t+1} = 1.58X_t$.
>
>  Agora, vamos calcular o $R^2$  utilizando a defini√ß√£o  $ R^2 = \frac{Var(\alpha'X_t)}{Var(Y_{t+1})}$.
>
> 1. **Calculamos $E[Y_{t+1}]$:**
>   $$E[Y_{t+1}] = \frac{2 + 3 + 5 + 6 + 8}{5} = \frac{24}{5} = 4.8$$
> 2. **Calculamos $Var(Y_{t+1})$:**
>    $$Var(Y_{t+1}) = \frac{(2-4.8)^2 + (3-4.8)^2 + (5-4.8)^2 + (6-4.8)^2 + (8-4.8)^2}{5} = \frac{7.84 + 3.24 + 0.04 + 1.44 + 10.24}{5} = \frac{22.8}{5} = 4.56$$
> 3. **Calculamos $E[\alpha'X_t]$:**
>    $$E[\alpha'X_t] = 1.58 \cdot E[X_t] = 1.58 \cdot  \frac{1 + 2 + 3 + 4 + 5}{5} = 1.58 \cdot 3 = 4.74$$
> 4. **Calculamos $Var(\alpha'X_t)$:**
>  $$Var(\alpha'X_t) = \frac{(1.58\cdot1 - 4.74)^2 + (1.58\cdot2 - 4.74)^2 + (1.58\cdot3 - 4.74)^2 + (1.58\cdot4 - 4.74)^2 + (1.58\cdot5 - 4.74)^2}{5} $$
>  $$Var(\alpha'X_t) = \frac{(-3.16)^2 + (-1.58)^2 + (0)^2 + (1.58)^2 + (3.16)^2}{5} = \frac{9.9856 + 2.4964 + 0 + 2.4964 + 9.9856}{5} = \frac{24.964}{5} \approx 4.99$$
>
> 5. **Calculamos $R^2$:**
>    $$R^2 = \frac{Var(\alpha'X_t)}{Var(Y_{t+1})} = \frac{4.99}{4.56} \approx 1.09$$
>
>  Perceba que o $R^2$ resultou maior do que 1, o que indica que houve um erro nos c√°lculos. Isso acontece quando utilizamos dados amostrais para aproximar momentos populacionais. Para garantir que o $R^2$ seja entre 0 e 1, √© necess√°rio calcular as vari√¢ncias e covari√¢ncias utilizando os graus de liberdade.
>
>  Usando os graus de liberdade para o c√°lculo das vari√¢ncias e a covari√¢ncia, obtemos:
>  $$Var(Y_{t+1}) =  \frac{22.8}{4} = 5.7$$
>  $$Var(\alpha'X_t) = \frac{24.964}{4} = 6.24$$
>
>  A covari√¢ncia entre $X_t$ e $Y_{t+1}$ √© dada por:
>  $$Cov(X_t, Y_{t+1}) = \frac{(1-3)(2-4.8) + (2-3)(3-4.8) + (3-3)(5-4.8) + (4-3)(6-4.8) + (5-3)(8-4.8)}{4} = \frac{5.6 + 1.8 + 0 + 1.2 + 6.4}{4} = \frac{15}{4} = 3.75 $$
>
> Finalmente, o $R^2$ √© dado por:
> $$R^2 = \frac{Cov(X_t, Y_{t+1})^2}{Var(X_t)Var(Y_{t+1})} = \frac{(3.75)^2}{ \frac{10}{4} \cdot \frac{22.8}{4}} = \frac{14.0625}{ \frac{228}{16}} \approx 0.987$$
>
> O valor de $R^2$ de aproximadamente 0.987 indica que aproximadamente 98.7% da vari√¢ncia de $Y_{t+1}$ pode ser explicada pela proje√ß√£o linear usando $X_t$.

**Proposi√ß√£o 3:** (Unicidade da Proje√ß√£o Linear) *A proje√ß√£o linear*  $P(Y_{t+1}|X_t) = \alpha'X_t$ *√© √∫nica, dada a condi√ß√£o de ortogonalidade*  $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$.
 *Proof:*
 I. Suponha que existam duas proje√ß√µes lineares, $P_1(Y_{t+1}|X_t) = \alpha_1'X_t$ e $P_2(Y_{t+1}|X_t) = \alpha_2'X_t$, que satisfazem a condi√ß√£o de ortogonalidade.
 II. Ent√£o, temos:
     $E[(Y_{t+1} - \alpha_1'X_t)X_t] = 0'$ e $E[(Y_{t+1} - \alpha_2'X_t)X_t] = 0'$.
 III. Subtraindo as duas equa√ß√µes, obtemos:
     $E[(\alpha_2'X_t - \alpha_1'X_t)X_t] = E[(\alpha_2' - \alpha_1')X_tX_t'] = 0'$.
 IV.  Como $E[X_tX_t']$ √© a matriz de covari√¢ncia de $X_t$, que √© positiva definida (ou positiva semi-definida se h√° colinearidade), ent√£o  $\alpha_2' - \alpha_1' = 0'$, o que implica que $\alpha_1' = \alpha_2'$.
 V.  Portanto, a proje√ß√£o linear √© √∫nica. $\blacksquare$

Essa proposi√ß√£o assegura que, dentro da classe das fun√ß√µes lineares, a proje√ß√£o obtida pela condi√ß√£o de ortogonalidade √© a √∫nica que minimiza o MSE, justificando a unicidade do estimador.

### Conclus√£o
A proje√ß√£o linear, definida formalmente como $P(Y_{t+1}|X_t) = \alpha'X_t$, busca a fun√ß√£o linear de $X_t$ que minimiza o MSE para prever $Y_{t+1}$. A condi√ß√£o de n√£o correla√ß√£o, $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$, √© fundamental para garantir a otimalidade dessa proje√ß√£o dentro da classe de previs√µes lineares. Embora n√£o seja a melhor previs√£o poss√≠vel para todas asfun√ß√µes, ela garante que n√£o h√° nenhum componente de $X_t$ que possa ser usado para melhorar a previs√£o linear de $Y_{t+1}$.

**O Modelo de Vetor Autorregressivo (VAR)**

O modelo VAR √© uma extens√£o do modelo AR para sistemas multivariados. Em vez de modelar uma √∫nica vari√°vel ao longo do tempo, o VAR modela um vetor de vari√°veis. Um modelo VAR(p) √© dado por:

$$Y_t = A_1Y_{t-1} + A_2Y_{t-2} + ... + A_pY_{t-p} + \epsilon_t$$

onde:
*   $Y_t$ √© um vetor de vari√°veis no tempo $t$.
*   $A_i$ s√£o matrizes de coeficientes.
*   $\epsilon_t$ √© um vetor de erros, que geralmente se assume ser ru√≠do branco multivariado.

O modelo VAR √© √∫til para analisar a din√¢mica entre m√∫ltiplas s√©ries temporais, onde cada vari√°vel √© influenciada por seus pr√≥prios valores passados e pelos valores passados das outras vari√°veis no sistema. O modelo VAR permite examinar as interdepend√™ncias e as rela√ß√µes de causalidade entre essas vari√°veis.

**Fun√ß√£o de Resposta ao Impulso (IRF)**

Uma das ferramentas mais importantes para analisar modelos VAR √© a Fun√ß√£o de Resposta ao Impulso (IRF). A IRF mostra o efeito de um choque em uma vari√°vel sobre si mesma e sobre as outras vari√°veis no sistema ao longo do tempo. Formalmente, o IRF √© calculado como a resposta de $Y$ a um choque unit√°rio no erro $\epsilon$.

Um exemplo pr√°tico √©: dado um modelo VAR que inclui infla√ß√£o e taxa de juros, um choque na taxa de juros pode ter efeitos na infla√ß√£o ao longo do tempo, tanto positivos quanto negativos dependendo dos coeficientes no modelo. A IRF visualiza esses efeitos, permitindo aos analistas entender as din√¢micas entre vari√°veis em um contexto macroecon√¥mico, por exemplo.

**Causalidade de Granger**

A causalidade de Granger √© um conceito que se refere a previsibilidade e n√£o √† causalidade no sentido estrito. Uma vari√°vel $X$ causa no sentido de Granger uma vari√°vel $Y$ se os valores passados de $X$ contribuem para uma melhor previs√£o de $Y$. Formalmente, dizemos que $X$ causa Granger $Y$ se o modelo que usa os valores passados de $Y$ e $X$ para prever $Y$ tem um desempenho superior ao modelo que usa apenas os valores passados de $Y$.

Essa ferramenta √© frequentemente usada em estudos emp√≠ricos para investigar a dire√ß√£o das rela√ß√µes entre vari√°veis e como o conhecimento de uma vari√°vel pode auxiliar na previs√£o de outra.

**Modelos de M√©dia M√≥vel (MA)**

Os modelos de M√©dia M√≥vel (MA) s√£o outro tipo importante de modelo de s√©ries temporais. Um modelo MA(q) assume que o valor da s√©rie em um dado momento √© uma fun√ß√£o linear dos erros passados:

$$Y_t = \mu + \epsilon_t + \theta_1\epsilon_{t-1} + \theta_2\epsilon_{t-2} + ... + \theta_q\epsilon_{t-q}$$

onde:
*   $Y_t$ √© a s√©rie temporal no tempo $t$.
*   $\mu$ √© a m√©dia da s√©rie.
*   $\epsilon_t$ √© o erro no tempo $t$ (ru√≠do branco).
*   $\theta_i$ s√£o os coeficientes do modelo MA.

A ideia central de um modelo MA √© que as perturba√ß√µes ou choques passados t√™m um efeito sobre o valor atual da s√©rie. Os modelos MA s√£o usados para modelar depend√™ncias de curto prazo e correla√ß√µes em s√©ries temporais. Eles s√£o complementares aos modelos AR e podem ser combinados em modelos ARMA ou ARIMA para capturar melhor as caracter√≠sticas de uma dada s√©rie temporal.

**Modelos ARMA e ARIMA**

Os modelos ARMA combinam os modelos AR e MA, capturando tanto a autocorrela√ß√£o quanto os efeitos dos erros passados. Um modelo ARMA(p,q) √© dado por:

$$Y_t = \alpha_1Y_{t-1} + ... + \alpha_pY_{t-p} + \epsilon_t + \theta_1\epsilon_{t-1} + ... + \theta_q\epsilon_{t-q}$$

onde $p$ e $q$ s√£o as ordens dos componentes AR e MA, respectivamente.

Os modelos ARIMA estendem os modelos ARMA ao incluir a diferencia√ß√£o, que √© uma t√©cnica usada para tornar as s√©ries temporais estacion√°rias. Um modelo ARIMA(p,d,q) inclui p termos autorregressivos, q termos de m√©dia m√≥vel, e a s√©rie temporal √© diferenciada 'd' vezes. A diferencia√ß√£o pode ajudar a eliminar tend√™ncias e sazonalidades, tornando o modelo mais adequado para previs√£o e an√°lise.

<!-- END -->
