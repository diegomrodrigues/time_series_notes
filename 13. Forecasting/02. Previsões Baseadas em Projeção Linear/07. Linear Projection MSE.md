## A ProjeÃ§Ã£o Linear e a MinimizaÃ§Ã£o do Erro QuadrÃ¡tico MÃ©dio: Uma AnÃ¡lise Detalhada

### IntroduÃ§Ã£o
Este capÃ­tulo mergulha na essÃªncia da **projeÃ§Ã£o linear**, explorando sua conexÃ£o com a minimizaÃ§Ã£o do erro quadrÃ¡tico mÃ©dio (MSE) e sua analogia com a esperanÃ§a condicional no contexto linear. Como discutido anteriormente, a projeÃ§Ã£o linear, definida como $P(Y_{t+1}|X_t) = \alpha'X_t$, busca a melhor aproximaÃ§Ã£o linear de $Y_{t+1}$ usando as variÃ¡veis explicativas $X_t$ [^1]. O ponto crucial Ã© que o vetor $\alpha$ Ã© determinado de forma a minimizar o MSE, com a condiÃ§Ã£o fundamental de que o erro de previsÃ£o seja nÃ£o correlacionado com $X_t$ [^2]. Vamos desvendar a matemÃ¡tica por trÃ¡s dessa condiÃ§Ã£o e seu papel na otimalidade da projeÃ§Ã£o linear.

### A Busca pelo Vetor $\alpha$ Ã“timo
A projeÃ§Ã£o linear de $Y_{t+1}$ sobre $X_t$ Ã© expressa como $P(Y_{t+1}|X_t) = \alpha'X_t$. O vetor $\alpha$ Ã© escolhido de forma a minimizar o MSE da previsÃ£o, definido como $E[(Y_{t+1} - \alpha'X_t)^2]$. A condiÃ§Ã£o fundamental para a otimalidade da projeÃ§Ã£o linear Ã© que o erro de previsÃ£o, $e_{t+1} = Y_{t+1} - \alpha'X_t$, seja nÃ£o correlacionado com as variÃ¡veis explicativas $X_t$, ou seja:
$$E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$$
Essa condiÃ§Ã£o de ortogonalidade garante que a projeÃ§Ã£o linear capture toda a informaÃ§Ã£o linearmente relevante em $X_t$ para prever $Y_{t+1}$ [^2].

Para demonstrar a relaÃ§Ã£o entre essa condiÃ§Ã£o e a minimizaÃ§Ã£o do MSE, vamos considerar uma previsÃ£o linear arbitrÃ¡ria $g(X_t)$. O MSE associado a essa previsÃ£o Ã© dado por:
$$E[(Y_{t+1} - g(X_t))^2]$$
Podemos reescrever essa expressÃ£o adicionando e subtraindo o termo da projeÃ§Ã£o linear $\alpha'X_t$:
$$E[(Y_{t+1} - \alpha'X_t + \alpha'X_t - g(X_t))^2]$$
Expandindo o quadrado, obtemos:
$$E[(Y_{t+1} - \alpha'X_t)^2] + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g(X_t))] + E[(\alpha'X_t - g(X_t))^2]$$
O termo crucial para demonstrar a otimalidade Ã© o termo do meio. Vamos analisar este termo mais a fundo. Definimos $\eta_{t+1} = (Y_{t+1} - \alpha'X_t)(\alpha'X_t - g(X_t))$. Condicional a $X_t$, os termos $\alpha'X_t$ e $g(X_t)$ sÃ£o constantes, permitindo que sejam fatorados da esperanÃ§a [^1]. AlÃ©m disso, sabemos que $E[(Y_{t+1} - \alpha'X_t)|X_t] = 0$ pela condiÃ§Ã£o de ortogonalidade:
$$E[\eta_{t+1}|X_t] = E[(Y_{t+1} - \alpha'X_t)|X_t](\alpha'X_t - g(X_t)) = 0 \cdot [\alpha'X_t - g(X_t)] = 0$$
Aplicando a lei da esperanÃ§a iterada, temos:
$$E[\eta_{t+1}] = E_{X_t}[E[\eta_{t+1}|X_t]] = E_{X_t}[0] = 0$$
Substituindo este resultado de volta na expressÃ£o do MSE, obtemos:
$$E[(Y_{t+1} - g(X_t))^2] = E[(Y_{t+1} - \alpha'X_t)^2] + E[(\alpha'X_t - g(X_t))^2]$$
O segundo termo Ã© sempre nÃ£o negativo. Portanto, o MSE Ã© minimizado quando este termo Ã© igual a zero, o que ocorre se e somente se $g(X_t) = \alpha'X_t$. Isso demonstra que a projeÃ§Ã£o linear $\alpha'X_t$ minimiza o MSE dentro da classe de funÃ§Ãµes lineares de $X_t$ [^1].

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Vamos ilustrar a otimalidade da projeÃ§Ã£o linear com um exemplo concreto. Suponha que estejamos tentando prever o desempenho de um estudante em um exame ($Y_{t+1}$) com base no nÃºmero de horas de estudo ($X_t$). Temos alguns dados:
>
> | t | Horas de Estudo ($X_t$) | Desempenho ($Y_{t+1}$) |
> |---|-----------------------|------------------------|
> | 1 | 2                     | 60                     |
> | 2 | 3                     | 70                     |
> | 3 | 4                     | 80                     |
> | 4 | 5                     | 90                     |
> | 5 | 6                     | 95                     |
>
> Primeiro, calculamos o valor de $\alpha$ da projeÃ§Ã£o linear $\hat{Y}_{t+1} = \alpha X_t$:
>
> 1. **Calculamos $E[X_t^2]$:**
>    $$E[X_t^2] = \frac{2^2 + 3^2 + 4^2 + 5^2 + 6^2}{5} = \frac{4 + 9 + 16 + 25 + 36}{5} = \frac{90}{5} = 18$$
>
> 2. **Calculamos $E[Y_{t+1}X_t]$:**
>    $$E[Y_{t+1}X_t] = \frac{2\cdot60 + 3\cdot70 + 4\cdot80 + 5\cdot90 + 6\cdot95}{5} = \frac{120 + 210 + 320 + 450 + 570}{5} = \frac{1670}{5} = 334$$
>
> 3. **Calculamos $\alpha$:**
>    $$\alpha = \frac{E[Y_{t+1}X_t]}{E[X_t^2]} = \frac{334}{18} \approx 18.56$$
>
>  A projeÃ§Ã£o linear Ã© entÃ£o $\hat{Y}_{t+1} = 18.56X_t$. Para demonstrar a otimalidade, vamos comparar a projeÃ§Ã£o linear com outra funÃ§Ã£o linear, por exemplo, $g(X_t) = 15X_t$:
>
> | t  | $X_t$ | $Y_{t+1}$ | $\hat{Y}_{t+1}=18.56X_t$ | $Y_{t+1} - \hat{Y}_{t+1}$ | $(Y_{t+1} - \hat{Y}_{t+1})^2$ | $g(X_t)=15X_t$ | $Y_{t+1} - g(X_t)$ | $(Y_{t+1} - g(X_t))^2$ |
> |----|-------|-----------|---------------------------|---------------------------|-------------------------------|---------------|--------------------|------------------------|
> | 1  | 2     | 60        | 37.12                   | 22.88                      | 523.50                        | 30           | 30                 | 900                   |
> | 2  | 3     | 70        | 55.68                   | 14.32                      | 205.06                        | 45           | 25                 | 625                   |
> | 3  | 4     | 80        | 74.24                   | 5.76                       | 33.18                        | 60           | 20                 | 400                  |
> | 4  | 5     | 90        | 92.80                   | -2.80                      | 7.84                         | 75           | 15                 | 225                  |
> | 5  | 6     | 95        | 111.36                  | -16.36                     | 267.65                       | 90           | 5                  | 25                  |
>
> $$MSE_{\alpha} = \frac{523.50 + 205.06 + 33.18 + 7.84 + 267.65}{5} = \frac{1037.23}{5} \approx 207.45$$
>
> $$MSE_{g} = \frac{900 + 625 + 400 + 225 + 25}{5} = \frac{2175}{5} = 435$$
>
> O MSE da projeÃ§Ã£o linear (207.45) Ã© menor do que o MSE da funÃ§Ã£o $g(X_t)$ (435), confirmando que a projeÃ§Ã£o linear minimiza o MSE dentro da classe de previsÃµes lineares.
  
Para solidificar o entendimento da minimizaÃ§Ã£o do MSE, podemos expressar o MSE como:
 $$MSE = E[(Y_{t+1} - \alpha'X_t)^2]$$
Expandindo o termo quadrÃ¡tico:
$$MSE = E[Y_{t+1}^2 - 2Y_{t+1}\alpha'X_t + (\alpha'X_t)^2]$$
Aplicando a linearidade da esperanÃ§a, obtemos:
$$MSE = E[Y_{t+1}^2] - 2E[Y_{t+1}\alpha'X_t] + E[(\alpha'X_t)^2]$$
Como $\alpha' = E[Y_{t+1}X_t][E(X_tX_t')]^{-1}$, substituÃ­mos $\alpha'$ na expressÃ£o acima e chegamos em:
$$MSE = E[Y_{t+1}^2] - E[Y_{t+1}X_t][E(X_tX_t')]^{-1}E[X_tY_{t+1}]$$
Esta expressÃ£o mostra que o MSE da projeÃ§Ã£o linear Ã© igual Ã  variÃ¢ncia de $Y_{t+1}$ menos um termo que reflete a variÃ¢ncia explicada pela projeÃ§Ã£o linear. O MSE representa a parte da variÃ¢ncia de $Y_{t+1}$ que nÃ£o pode ser explicada linearmente por $X_t$, e a projeÃ§Ã£o linear, dentro da classe linear, minimiza este valor.

### A Analogia com a Expectativa Condicional
A projeÃ§Ã£o linear, embora restrita ao espaÃ§o de funÃ§Ãµes lineares, tem uma forte analogia com a expectativa condicional $E(Y_{t+1}|X_t)$, que Ã© a previsÃ£o que minimiza o MSE dentro de todas as previsÃµes possÃ­veis. Em particular, se a expectativa condicional $E(Y_{t+1}|X_t)$ for uma funÃ§Ã£o linear de $X_t$, entÃ£o a projeÃ§Ã£o linear $P(Y_{t+1}|X_t)$ coincidirÃ¡ com a expectativa condicional. Ou seja, se $E(Y_{t+1}|X_t) = \beta'X_t$, entÃ£o $\alpha' = \beta'$, e o MSE da projeÃ§Ã£o linear Ã© igual ao MSE da expectativa condicional.

O ponto crucial Ã© que a projeÃ§Ã£o linear Ã© a melhor aproximaÃ§Ã£o *linear* da expectativa condicional, o que a torna uma ferramenta computacionalmente Ãºtil e relevante quando a expectativa condicional nÃ£o Ã© linear ou Ã© desconhecida.

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Imagine que a relaÃ§Ã£o real entre o nÃºmero de horas de estudo ($X_t$) e a nota no exame ($Y_{t+1}$) seja dada por $E(Y_{t+1}|X_t) = 50 + 10X_t + 0.5X_t^2$. Contudo, o analista acredita que existe uma relaÃ§Ã£o linear. Vamos gerar um exemplo para ver como a projeÃ§Ã£o linear se comporta nessa situaÃ§Ã£o. Primeiro, criaremos dados simulados utilizando a relaÃ§Ã£o nÃ£o-linear:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LinearRegression
>
> np.random.seed(42)
> X_t = np.linspace(2, 10, 100)
> Y_t_plus_1 = 50 + 10 * X_t + 0.5 * X_t**2 + np.random.normal(0, 15, 100)
>
> # Ajustando a projeÃ§Ã£o linear
> X_t_reshaped = X_t.reshape(-1, 1)
> model = LinearRegression()
> model.fit(X_t_reshaped, Y_t_plus_1)
> alpha = model.coef_[0]
> intercept = model.intercept_
>
> # Valores preditos pela projeÃ§Ã£o linear
> Y_hat = intercept + alpha * X_t
>
> # Plotando os resultados
> plt.figure(figsize=(8, 6))
> plt.scatter(X_t, Y_t_plus_1, label='Dados Simulados', alpha=0.7)
> plt.plot(X_t, Y_hat, color='red', label=f'ProjeÃ§Ã£o Linear ($\\alpha$={alpha:.2f})')
> plt.xlabel('Horas de Estudo ($X_t$)')
> plt.ylabel('Nota no Exame ($Y_{t+1}$)')
> plt.title('ComparaÃ§Ã£o entre a RelaÃ§Ã£o Real NÃ£o Linear e a ProjeÃ§Ã£o Linear')
> plt.legend()
> plt.grid(True)
> plt.show()
>
> print(f'Intercepto da ProjeÃ§Ã£o Linear: {intercept:.2f}')
> print(f'Coeficiente da ProjeÃ§Ã£o Linear ($\\alpha$): {alpha:.2f}')
> ```
>
> Neste exemplo, a relaÃ§Ã£o entre as variÃ¡veis Ã© nÃ£o linear, mas a projeÃ§Ã£o linear tenta aproximar essa relaÃ§Ã£o usando uma linha reta.  O grÃ¡fico mostra que a projeÃ§Ã£o linear nÃ£o captura toda a relaÃ§Ã£o, mas Ã© a melhor aproximaÃ§Ã£o linear possÃ­vel (menor MSE) para os dados. Podemos calcular o MSE para a projeÃ§Ã£o linear, para comparar com outros modelos.  
>
> ```python
> # CÃ¡lculo do MSE da projeÃ§Ã£o linear
> mse_linear = np.mean((Y_t_plus_1 - Y_hat)**2)
> print(f'MSE da ProjeÃ§Ã£o Linear: {mse_linear:.2f}')
> ```
>
> Como a relaÃ§Ã£o verdadeira nÃ£o Ã© linear, o MSE da projeÃ§Ã£o linear nÃ£o serÃ¡ zero.  Entretanto, se tentarmos outras funÃ§Ãµes lineares, o MSE serÃ¡ sempre superior a este. A projeÃ§Ã£o linear ainda Ã© Ãºtil pois provÃª a melhor aproximaÃ§Ã£o linear da relaÃ§Ã£o verdadeira.

  
  **Teorema 1:** (ProjeÃ§Ã£o Linear e MinimizaÃ§Ã£o do MSE) *A projeÃ§Ã£o linear* $P(Y_{t+1}|X_t) = \alpha'X_t$, *com* $\alpha' = E[Y_{t+1}X_t][E(X_tX_t')]^{-1}$, *minimiza o erro quadrÃ¡tico mÃ©dio (MSE) da previsÃ£o dentro da classe de todas as previsÃµes lineares* $g(X_t)$.
  
  *Proof:*
  
  I. Queremos mostrar que o MSE da projeÃ§Ã£o linear $MSE = E[(Y_{t+1} - \alpha'X_t)^2]$ Ã© menor do que qualquer outro MSE $MSE_g= E[(Y_{t+1} - g(X_t))^2]$ onde $g(X_t)$ Ã© uma funÃ§Ã£o linear arbitrÃ¡ria.
  II. ComeÃ§amos expressando $g(X_t) = \beta'X_t$, onde $\beta$ Ã© um vetor de coeficientes diferente de $\alpha$.
  III. O MSE associado a $g(X_t)$ Ã©:
     $$MSE_g = E[(Y_{t+1} - \beta'X_t)^2]$$
    
   IV. Adicionando e subtraindo $\alpha'X_t$, temos:
    $$MSE_g = E[(Y_{t+1} - \alpha'X_t + \alpha'X_t - \beta'X_t)^2]$$
    
   V. Expandindo o termo ao quadrado:
   $$MSE_g = E[(Y_{t+1} - \alpha'X_t)^2] + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - \beta'X_t)] + E[(\alpha'X_t - \beta'X_t)^2]$$
    
  VI. O termo do meio Ã© zero pela condiÃ§Ã£o de ortogonalidade:
      $E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - \beta'X_t)] = E[(Y_{t+1} - \alpha'X_t)h(X_t)] = 0$
       onde $h(X_t)$ Ã© uma combinaÃ§Ã£o linear de $X_t$.
  VII. Portanto, temos:
    $$MSE_g = E[(Y_{t+1} - \alpha'X_t)^2] + E[(\alpha'X_t - \beta'X_t)^2]$$
    
 VIII. O segundo termo Ã© sempre nÃ£o negativo, portanto o MSE Ã© minimizado quando ele Ã© igual a zero.  Este valor Ã© zero quando $\alpha' = \beta'$, o que prova que a projeÃ§Ã£o linear $\alpha'X_t$ minimiza o MSE dentro da classe de previsÃµes lineares. $\blacksquare$

**Lema 1:** (MSE da ProjeÃ§Ã£o Linear e Expectativa Condicional) *Se a expectativa condicional* $E(Y_{t+1}|X_t)$ *Ã© uma funÃ§Ã£o linear de* $X_t$, *entÃ£o o erro quadrÃ¡tico mÃ©dio (MSE) da projeÃ§Ã£o linear* $P(Y_{t+1}|X_t)$ *Ã© igual ao MSE da expectativa condicional.*

*Proof:*
I.  Assumimos que a expectativa condicional Ã© linear: $E(Y_{t+1}|X_t) = \gamma'X_t$.
II. O MSE da expectativa condicional Ã© $MSE_{EC} = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] = E[(Y_{t+1} - \gamma'X_t)^2]$.
III.  A projeÃ§Ã£o linear Ã© dada por $P(Y_{t+1}|X_t) = \alpha'X_t$.
IV. Se  $E(Y_{t+1}|X_t)$ Ã© linear em $X_t$, sabemos que $\alpha$ minimiza o MSE e que $\alpha = \gamma$.  Neste caso, a projeÃ§Ã£o linear coincide com a expectativa condicional:  $P(Y_{t+1}|X_t) =  \gamma'X_t$ .
V.  O MSE da projeÃ§Ã£o linear Ã© $MSE_{PL} = E[(Y_{t+1} - \alpha'X_t)^2]$.
VI.  Substituindo $\gamma$ por $\alpha$, temos: $MSE_{PL} = E[(Y_{t+1} - \gamma'X_t)^2]$.
VII. Portanto, $MSE_{PL} = MSE_{EC}$ quando a expectativa condicional Ã© linear em $X_t$. $\blacksquare$

**Lema 1.1** (MSE da ProjeÃ§Ã£o Linear): *O erro quadrÃ¡tico mÃ©dio da projeÃ§Ã£o linear* $P(Y_{t+1}|X_t) = \alpha'X_t$ *pode ser expresso como:*
$$MSE = E(Y_{t+1}^2) - E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}E(X_tY_{t+1})$$
*Proof:*
I. ComeÃ§amos com a definiÃ§Ã£o do MSE: $MSE = E[(Y_{t+1} - \alpha'X_t)^2]$.
II. Expandindo o quadrado: $MSE = E[Y_{t+1}^2 - 2Y_{t+1}\alpha'X_t + (\alpha'X_t)^2]$.
III. Aplicando a linearidade da esperanÃ§a: $MSE = E[Y_{t+1}^2] - 2E[Y_{t+1}\alpha'X_t] + E[\alpha'X_tX_t'\alpha]$.
IV. Sabemos que $\alpha' = E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}$. SubstituÃ­mos na expressÃ£o do MSE:
$$MSE = E[Y_{t+1}^2] - 2E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}] + E[E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}X_tX_t'E[X_tY_{t+1}][E(X_tX_t')]^{-1}]$$
V.  Simplificando:
$$MSE = E[Y_{t+1}^2] - 2E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}] + E[Y_{t+1}X_t'][E(X_tX_t')]^{-1}E[X_tX_t'][E(X_tX_t')]^{-1}E[X_tY_{t+1}]$$
VI. Simplificando os termos e agrupando:
 $$MSE = E[Y_{t+1}^2] - E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}]$$
  $\blacksquare$
Esta decomposiÃ§Ã£o do MSE mostra que a projeÃ§Ã£o linear busca capturar a mÃ¡xima variÃ¢ncia possÃ­vel de $Y_{t+1}$ atravÃ©s de uma combinaÃ§Ã£o linear de $X_t$.

**ProposiÃ§Ã£o 1:** (Unicidade da ProjeÃ§Ã£o Linear) *Se a matriz* $E[X_tX_t']$ *Ã© nÃ£o singular, entÃ£o o vetor* $\alpha$ *que minimiza o MSE na projeÃ§Ã£o linear* $P(Y_{t+1}|X_t) = \alpha'X_t$ *Ã© Ãºnico.*

*Proof:*
I.  O vetor $\alpha$ Ã© definido como $\alpha' = E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}$.
II. Para que $\alpha$ seja Ãºnico, a inversa $[E(X_tX_t')]^{-1}$ precisa ser Ãºnica.
III. A condiÃ§Ã£o para que uma matriz tenha inversa Ãºnica Ã© que ela seja nÃ£o singular.
IV. Portanto, se $E[X_tX_t']$ Ã© nÃ£o singular, entÃ£o sua inversa $[E(X_tX_t')]^{-1}$ Ã© Ãºnica.
V. Consequentemente, o vetor $\alpha$ Ã© Ãºnico, garantindo a unicidade da projeÃ§Ã£o linear. $\blacksquare$

**CorolÃ¡rio 1:** (InterpretaÃ§Ã£o do MSE mÃ­nimo) *O erro quadrÃ¡tico mÃ©dio (MSE) mÃ­nimo da projeÃ§Ã£o linear, dado por:*
$$MSE = E(Y_{t+1}^2) - E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}E(X_tY_{t+1})$$
*representa a variÃ¢ncia de* $Y_{t+1}$ *que nÃ£o Ã© explicada linearmente pelas variÃ¡veis explicativas* $X_t$.

*Proof:*
I. Do Lema 1.1, sabemos que $MSE = E[Y_{t+1}^2] - E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}]$.
II. O termo $E[Y_{t+1}^2]$ representa a variÃ¢ncia de $Y_{t+1}$, denotada por $Var[Y_{t+1}]$.
III. O termo $E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}]$ representa a variÃ¢ncia de $Y_{t+1}$ explicada pela projeÃ§Ã£o linear.
IV. Portanto, o MSE Ã© a diferenÃ§a entre a variÃ¢ncia total de $Y_{t+1}$ e a variÃ¢ncia explicada pela projeÃ§Ã£o linear.
V. Isso implica que o MSE mÃ­nimo representa a variÃ¢ncia de $Y_{t+1}$ que nÃ£o pode ser explicada linearmente pelas variÃ¡veis explicativas $X_t$. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Vamos usar os dados do exemplo anterior para calcular o MSE da projeÃ§Ã£o linear usando a fÃ³rmula do CorolÃ¡rio 1.
>
> Primeiro, precisamos calcular $E[Y_{t+1}^2]$, $E[Y_{t+1}X_t]$, e $E[X_t^2]$. JÃ¡ calculamos $E[Y_{t+1}X_t] = 334$ e $E[X_t^2] = 18$.
>
> Agora, calculamos $E[Y_{t+1}^2]$:
> $$E[Y_{t+1}^2] = \frac{60^2 + 70^2 + 80^2 + 90^2 + 95^2}{5} = \frac{3600 + 4900 + 6400 + 8100 + 9025}{5} = \frac{32025}{5} = 6405$$
>
> Agora podemos calcular o MSE usando a fÃ³rmula do CorolÃ¡rio 1:
> $$MSE = E[Y_{t+1}^2] - E[Y_{t+1}X_t][E(X_t^2)]^{-1}E[X_tY_{t+1}]$$
> $$MSE = 6405 - 334 \cdot (18)^{-1} \cdot 334 = 6405 - \frac{334^2}{18} = 6405 - \frac{111556}{18} = 6405 - 6197.56 \approx 207.44$$
>
> Esse valor Ã© muito prÃ³ximo do MSE calculado anteriormente (207.45), confirmando a validade da fÃ³rmula.
>
> A variÃ¢ncia total de $Y_{t+1}$ Ã© dada por:
> $$Var(Y_{t+1}) = E[Y_{t+1}^2] - (E[Y_{t+1}])^2$$
>
> Primeiro calculamos $E[Y_{t+1}]$:
> $$E[Y_{t+1}] = \frac{60+70+80+90+95}{5} = \frac{395}{5} = 79$$
>
> Agora podemos calcular a variÃ¢ncia:
> $$Var(Y_{t+1}) = 6405 - 79^2 = 6405 - 6241 = 164$$
>
> O MSE da projeÃ§Ã£o linear (207.44) Ã© a porÃ§Ã£o da variÃ¢ncia de $Y_{t+1}$ que a projeÃ§Ã£o linear nÃ£o consegue explicar.

### ConclusÃ£o
A projeÃ§Ã£o linear, definida como $P(Y_{t+1}|X_t) = \alpha'X_t$, encontra o vetor $\alpha$ que minimiza o erro quadrÃ¡tico mÃ©dio dentro da classe das previsÃµes lineares. A condiÃ§Ã£o de ortogonalidade, $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$, garante que o erro de previsÃ£o seja nÃ£o correlacionado com as variÃ¡veis explicativas, o que Ã© fundamental para a otimalidade da projeÃ§Ã£o linear dentro de sua classe. A projeÃ§Ã£o linear Ã© um conceito fundamental na modelagem de sÃ©ries temporais e serve como base para uma vasta gama de outros modelos e ferramentas. Apesar de estar limitada a relaÃ§Ãµes lineares, a projeÃ§Ã£o linear oferece uma excelente alternativa quando a expectativa condicional Ã© desconhecida ou computacionalmente inviÃ¡vel.
  
### ReferÃªncias
[^1]: Expression [4.1.1], [4.1.2], [4.1.3], [4.1.4], [4.1.5], [4.1.6], [4.1.7], [4.1.8]
[^2]: [4.1.9], [4.1.10], [4.1.11], [4.1.12], [4.1.13]
<!-- END -->
