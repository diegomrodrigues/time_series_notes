## A Proje√ß√£o Linear e a Minimiza√ß√£o do Erro Quadr√°tico M√©dio: Uma An√°lise Detalhada

### Introdu√ß√£o
Este cap√≠tulo mergulha na ess√™ncia da **proje√ß√£o linear**, explorando sua conex√£o com a minimiza√ß√£o do erro quadr√°tico m√©dio (MSE) e sua analogia com a esperan√ßa condicional no contexto linear. Como discutido anteriormente, a proje√ß√£o linear, definida como $P(Y_{t+1}|X_t) = \alpha'X_t$, busca a melhor aproxima√ß√£o linear de $Y_{t+1}$ usando as vari√°veis explicativas $X_t$ [^1]. O ponto crucial √© que o vetor $\alpha$ √© determinado de forma a minimizar o MSE, com a condi√ß√£o fundamental de que o erro de previs√£o seja n√£o correlacionado com $X_t$ [^2]. Vamos desvendar a matem√°tica por tr√°s dessa condi√ß√£o e seu papel na otimalidade da proje√ß√£o linear.

### A Busca pelo Vetor $\alpha$ √ìtimo
A proje√ß√£o linear de $Y_{t+1}$ sobre $X_t$ √© expressa como $P(Y_{t+1}|X_t) = \alpha'X_t$. O vetor $\alpha$ √© escolhido de forma a minimizar o MSE da previs√£o, definido como $E[(Y_{t+1} - \alpha'X_t)^2]$. A condi√ß√£o fundamental para a otimalidade da proje√ß√£o linear √© que o erro de previs√£o, $e_{t+1} = Y_{t+1} - \alpha'X_t$, seja n√£o correlacionado com as vari√°veis explicativas $X_t$, ou seja:
$$E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$$
Essa condi√ß√£o de ortogonalidade garante que a proje√ß√£o linear capture toda a informa√ß√£o linearmente relevante em $X_t$ para prever $Y_{t+1}$ [^2].

Para demonstrar a rela√ß√£o entre essa condi√ß√£o e a minimiza√ß√£o do MSE, vamos considerar uma previs√£o linear arbitr√°ria $g(X_t)$. O MSE associado a essa previs√£o √© dado por:
$$E[(Y_{t+1} - g(X_t))^2]$$
Podemos reescrever essa express√£o adicionando e subtraindo o termo da proje√ß√£o linear $\alpha'X_t$:
$$E[(Y_{t+1} - \alpha'X_t + \alpha'X_t - g(X_t))^2]$$
Expandindo o quadrado, obtemos:
$$E[(Y_{t+1} - \alpha'X_t)^2] + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g(X_t))] + E[(\alpha'X_t - g(X_t))^2]$$
O termo crucial para demonstrar a otimalidade √© o termo do meio. Vamos analisar este termo mais a fundo. Definimos $\eta_{t+1} = (Y_{t+1} - \alpha'X_t)(\alpha'X_t - g(X_t))$. Condicional a $X_t$, os termos $\alpha'X_t$ e $g(X_t)$ s√£o constantes, permitindo que sejam fatorados da esperan√ßa [^1]. Al√©m disso, sabemos que $E[(Y_{t+1} - \alpha'X_t)|X_t] = 0$ pela condi√ß√£o de ortogonalidade:
$$E[\eta_{t+1}|X_t] = E[(Y_{t+1} - \alpha'X_t)|X_t](\alpha'X_t - g(X_t)) = 0 \cdot [\alpha'X_t - g(X_t)] = 0$$
Aplicando a lei da esperan√ßa iterada, temos:
$$E[\eta_{t+1}] = E_{X_t}[E[\eta_{t+1}|X_t]] = E_{X_t}[0] = 0$$
Substituindo este resultado de volta na express√£o do MSE, obtemos:
$$E[(Y_{t+1} - g(X_t))^2] = E[(Y_{t+1} - \alpha'X_t)^2] + E[(\alpha'X_t - g(X_t))^2]$$
O segundo termo √© sempre n√£o negativo. Portanto, o MSE √© minimizado quando este termo √© igual a zero, o que ocorre se e somente se $g(X_t) = \alpha'X_t$. Isso demonstra que a proje√ß√£o linear $\alpha'X_t$ minimiza o MSE dentro da classe de fun√ß√µes lineares de $X_t$ [^1].

> üí° **Exemplo Num√©rico:**
> Vamos ilustrar a otimalidade da proje√ß√£o linear com um exemplo concreto. Suponha que estejamos tentando prever o desempenho de um estudante em um exame ($Y_{t+1}$) com base no n√∫mero de horas de estudo ($X_t$). Temos alguns dados:
>
> | t | Horas de Estudo ($X_t$) | Desempenho ($Y_{t+1}$) |
> |---|-----------------------|------------------------|
> | 1 | 2                     | 60                     |
> | 2 | 3                     | 70                     |
> | 3 | 4                     | 80                     |
> | 4 | 5                     | 90                     |
> | 5 | 6                     | 95                     |
>
> Primeiro, calculamos o valor de $\alpha$ da proje√ß√£o linear $\hat{Y}_{t+1} = \alpha X_t$:
>
> 1. **Calculamos $E[X_t^2]$:**
>    $$E[X_t^2] = \frac{2^2 + 3^2 + 4^2 + 5^2 + 6^2}{5} = \frac{4 + 9 + 16 + 25 + 36}{5} = \frac{90}{5} = 18$$
>
> 2. **Calculamos $E[Y_{t+1}X_t]$:**
>    $$E[Y_{t+1}X_t] = \frac{2\cdot60 + 3\cdot70 + 4\cdot80 + 5\cdot90 + 6\cdot95}{5} = \frac{120 + 210 + 320 + 450 + 570}{5} = \frac{1670}{5} = 334$$
>
> 3. **Calculamos $\alpha$:**
>    $$\alpha = \frac{E[Y_{t+1}X_t]}{E[X_t^2]} = \frac{334}{18} \approx 18.56$$
>
>  A proje√ß√£o linear √© ent√£o $\hat{Y}_{t+1} = 18.56X_t$. Para demonstrar a otimalidade, vamos comparar a proje√ß√£o linear com outra fun√ß√£o linear, por exemplo, $g(X_t) = 15X_t$:
>
> | t  | $X_t$ | $Y_{t+1}$ | $\hat{Y}_{t+1}=18.56X_t$ | $Y_{t+1} - \hat{Y}_{t+1}$ | $(Y_{t+1} - \hat{Y}_{t+1})^2$ | $g(X_t)=15X_t$ | $Y_{t+1} - g(X_t)$ | $(Y_{t+1} - g(X_t))^2$ |
> |----|-------|-----------|---------------------------|---------------------------|-------------------------------|---------------|--------------------|------------------------|
> | 1  | 2     | 60        | 37.12                   | 22.88                      | 523.50                        | 30           | 30                 | 900                   |
> | 2  | 3     | 70        | 55.68                   | 14.32                      | 205.06                        | 45           | 25                 | 625                   |
> | 3  | 4     | 80        | 74.24                   | 5.76                       | 33.18                        | 60           | 20                 | 400                  |
> | 4  | 5     | 90        | 92.80                   | -2.80                      | 7.84                         | 75           | 15                 | 225                  |
> | 5  | 6     | 95        | 111.36                  | -16.36                     | 267.65                       | 90           | 5                  | 25                  |
>
> $$MSE_{\alpha} = \frac{523.50 + 205.06 + 33.18 + 7.84 + 267.65}{5} = \frac{1037.23}{5} \approx 207.45$$
>
> $$MSE_{g} = \frac{900 + 625 + 400 + 225 + 25}{5} = \frac{2175}{5} = 435$$
>
> O MSE da proje√ß√£o linear (207.45) √© menor do que o MSE da fun√ß√£o $g(X_t)$ (435), confirmando que a proje√ß√£o linear minimiza o MSE dentro da classe de previs√µes lineares.
  
Para solidificar o entendimento da minimiza√ß√£o do MSE, podemos expressar o MSE como:
 $$MSE = E[(Y_{t+1} - \alpha'X_t)^2]$$
Expandindo o termo quadr√°tico:
$$MSE = E[Y_{t+1}^2 - 2Y_{t+1}\alpha'X_t + (\alpha'X_t)^2]$$
Aplicando a linearidade da esperan√ßa, obtemos:
$$MSE = E[Y_{t+1}^2] - 2E[Y_{t+1}\alpha'X_t] + E[(\alpha'X_t)^2]$$
Como $\alpha' = E[Y_{t+1}X_t][E(X_tX_t')]^{-1}$, substitu√≠mos $\alpha'$ na express√£o acima e chegamos em:
$$MSE = E[Y_{t+1}^2] - E[Y_{t+1}X_t][E(X_tX_t')]^{-1}E[X_tY_{t+1}]$$
Esta express√£o mostra que o MSE da proje√ß√£o linear √© igual √† vari√¢ncia de $Y_{t+1}$ menos um termo que reflete a vari√¢ncia explicada pela proje√ß√£o linear. O MSE representa a parte da vari√¢ncia de $Y_{t+1}$ que n√£o pode ser explicada linearmente por $X_t$, e a proje√ß√£o linear, dentro da classe linear, minimiza este valor.

### A Analogia com a Expectativa Condicional
A proje√ß√£o linear, embora restrita ao espa√ßo de fun√ß√µes lineares, tem uma forte analogia com a expectativa condicional $E(Y_{t+1}|X_t)$, que √© a previs√£o que minimiza o MSE dentro de todas as previs√µes poss√≠veis. Em particular, se a expectativa condicional $E(Y_{t+1}|X_t)$ for uma fun√ß√£o linear de $X_t$, ent√£o a proje√ß√£o linear $P(Y_{t+1}|X_t)$ coincidir√° com a expectativa condicional. Ou seja, se $E(Y_{t+1}|X_t) = \beta'X_t$, ent√£o $\alpha' = \beta'$, e o MSE da proje√ß√£o linear √© igual ao MSE da expectativa condicional.

O ponto crucial √© que a proje√ß√£o linear √© a melhor aproxima√ß√£o *linear* da expectativa condicional, o que a torna uma ferramenta computacionalmente √∫til e relevante quando a expectativa condicional n√£o √© linear ou √© desconhecida.

> üí° **Exemplo Num√©rico:**
> Imagine que a rela√ß√£o real entre o n√∫mero de horas de estudo ($X_t$) e a nota no exame ($Y_{t+1}$) seja dada por $E(Y_{t+1}|X_t) = 50 + 10X_t + 0.5X_t^2$. Contudo, o analista acredita que existe uma rela√ß√£o linear. Vamos gerar um exemplo para ver como a proje√ß√£o linear se comporta nessa situa√ß√£o. Primeiro, criaremos dados simulados utilizando a rela√ß√£o n√£o-linear:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LinearRegression
>
> np.random.seed(42)
> X_t = np.linspace(2, 10, 100)
> Y_t_plus_1 = 50 + 10 * X_t + 0.5 * X_t**2 + np.random.normal(0, 15, 100)
>
> # Ajustando a proje√ß√£o linear
> X_t_reshaped = X_t.reshape(-1, 1)
> model = LinearRegression()
> model.fit(X_t_reshaped, Y_t_plus_1)
> alpha = model.coef_[0]
> intercept = model.intercept_
>
> # Valores preditos pela proje√ß√£o linear
> Y_hat = intercept + alpha * X_t
>
> # Plotando os resultados
> plt.figure(figsize=(8, 6))
> plt.scatter(X_t, Y_t_plus_1, label='Dados Simulados', alpha=0.7)
> plt.plot(X_t, Y_hat, color='red', label=f'Proje√ß√£o Linear ($\\alpha$={alpha:.2f})')
> plt.xlabel('Horas de Estudo ($X_t$)')
> plt.ylabel('Nota no Exame ($Y_{t+1}$)')
> plt.title('Compara√ß√£o entre a Rela√ß√£o Real N√£o Linear e a Proje√ß√£o Linear')
> plt.legend()
> plt.grid(True)
> plt.show()
>
> print(f'Intercepto da Proje√ß√£o Linear: {intercept:.2f}')
> print(f'Coeficiente da Proje√ß√£o Linear ($\\alpha$): {alpha:.2f}')
> ```
>
> Neste exemplo, a rela√ß√£o entre as vari√°veis √© n√£o linear, mas a proje√ß√£o linear tenta aproximar essa rela√ß√£o usando uma linha reta.  O gr√°fico mostra que a proje√ß√£o linear n√£o captura toda a rela√ß√£o, mas √© a melhor aproxima√ß√£o linear poss√≠vel (menor MSE) para os dados. Podemos calcular o MSE para a proje√ß√£o linear, para comparar com outros modelos.  
>
> ```python
> # C√°lculo do MSE da proje√ß√£o linear
> mse_linear = np.mean((Y_t_plus_1 - Y_hat)**2)
> print(f'MSE da Proje√ß√£o Linear: {mse_linear:.2f}')
> ```
>
> Como a rela√ß√£o verdadeira n√£o √© linear, o MSE da proje√ß√£o linear n√£o ser√° zero.  Entretanto, se tentarmos outras fun√ß√µes lineares, o MSE ser√° sempre superior a este. A proje√ß√£o linear ainda √© √∫til pois prov√™ a melhor aproxima√ß√£o linear da rela√ß√£o verdadeira.

  
  **Teorema 1:** (Proje√ß√£o Linear e Minimiza√ß√£o do MSE) *A proje√ß√£o linear* $P(Y_{t+1}|X_t) = \alpha'X_t$, *com* $\alpha' = E[Y_{t+1}X_t][E(X_tX_t')]^{-1}$, *minimiza o erro quadr√°tico m√©dio (MSE) da previs√£o dentro da classe de todas as previs√µes lineares* $g(X_t)$.
  
  *Proof:*
  
  I. Queremos mostrar que o MSE da proje√ß√£o linear $MSE = E[(Y_{t+1} - \alpha'X_t)^2]$ √© menor do que qualquer outro MSE $MSE_g= E[(Y_{t+1} - g(X_t))^2]$ onde $g(X_t)$ √© uma fun√ß√£o linear arbitr√°ria.
  II. Come√ßamos expressando $g(X_t) = \beta'X_t$, onde $\beta$ √© um vetor de coeficientes diferente de $\alpha$.
  III. O MSE associado a $g(X_t)$ √©:
     $$MSE_g = E[(Y_{t+1} - \beta'X_t)^2]$$
    
   IV. Adicionando e subtraindo $\alpha'X_t$, temos:
    $$MSE_g = E[(Y_{t+1} - \alpha'X_t + \alpha'X_t - \beta'X_t)^2]$$
    
   V. Expandindo o termo ao quadrado:
   $$MSE_g = E[(Y_{t+1} - \alpha'X_t)^2] + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - \beta'X_t)] + E[(\alpha'X_t - \beta'X_t)^2]$$
    
  VI. O termo do meio √© zero pela condi√ß√£o de ortogonalidade:
      $E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - \beta'X_t)] = E[(Y_{t+1} - \alpha'X_t)h(X_t)] = 0$
       onde $h(X_t)$ √© uma combina√ß√£o linear de $X_t$.
  VII. Portanto, temos:
    $$MSE_g = E[(Y_{t+1} - \alpha'X_t)^2] + E[(\alpha'X_t - \beta'X_t)^2]$$
    
 VIII. O segundo termo √© sempre n√£o negativo, portanto o MSE √© minimizado quando ele √© igual a zero.  Este valor √© zero quando $\alpha' = \beta'$, o que prova que a proje√ß√£o linear $\alpha'X_t$ minimiza o MSE dentro da classe de previs√µes lineares. $\blacksquare$

**Lema 1:** (MSE da Proje√ß√£o Linear e Expectativa Condicional) *Se a expectativa condicional* $E(Y_{t+1}|X_t)$ *√© uma fun√ß√£o linear de* $X_t$, *ent√£o o erro quadr√°tico m√©dio (MSE) da proje√ß√£o linear* $P(Y_{t+1}|X_t)$ *√© igual ao MSE da expectativa condicional.*

*Proof:*
I.  Assumimos que a expectativa condicional √© linear: $E(Y_{t+1}|X_t) = \gamma'X_t$.
II. O MSE da expectativa condicional √© $MSE_{EC} = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] = E[(Y_{t+1} - \gamma'X_t)^2]$.
III.  A proje√ß√£o linear √© dada por $P(Y_{t+1}|X_t) = \alpha'X_t$.
IV. Se  $E(Y_{t+1}|X_t)$ √© linear em $X_t$, sabemos que $\alpha$ minimiza o MSE e que $\alpha = \gamma$.  Neste caso, a proje√ß√£o linear coincide com a expectativa condicional:  $P(Y_{t+1}|X_t) =  \gamma'X_t$ .
V.  O MSE da proje√ß√£o linear √© $MSE_{PL} = E[(Y_{t+1} - \alpha'X_t)^2]$.
VI.  Substituindo $\gamma$ por $\alpha$, temos: $MSE_{PL} = E[(Y_{t+1} - \gamma'X_t)^2]$.
VII. Portanto, $MSE_{PL} = MSE_{EC}$ quando a expectativa condicional √© linear em $X_t$. $\blacksquare$

**Lema 1.1** (MSE da Proje√ß√£o Linear): *O erro quadr√°tico m√©dio da proje√ß√£o linear* $P(Y_{t+1}|X_t) = \alpha'X_t$ *pode ser expresso como:*
$$MSE = E(Y_{t+1}^2) - E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}E(X_tY_{t+1})$$
*Proof:*
I. Come√ßamos com a defini√ß√£o do MSE: $MSE = E[(Y_{t+1} - \alpha'X_t)^2]$.
II. Expandindo o quadrado: $MSE = E[Y_{t+1}^2 - 2Y_{t+1}\alpha'X_t + (\alpha'X_t)^2]$.
III. Aplicando a linearidade da esperan√ßa: $MSE = E[Y_{t+1}^2] - 2E[Y_{t+1}\alpha'X_t] + E[\alpha'X_tX_t'\alpha]$.
IV. Sabemos que $\alpha' = E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}$. Substitu√≠mos na express√£o do MSE:
$$MSE = E[Y_{t+1}^2] - 2E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}] + E[E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}X_tX_t'E[X_tY_{t+1}][E(X_tX_t')]^{-1}]$$
V.  Simplificando:
$$MSE = E[Y_{t+1}^2] - 2E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}] + E[Y_{t+1}X_t'][E(X_tX_t')]^{-1}E[X_tX_t'][E(X_tX_t')]^{-1}E[X_tY_{t+1}]$$
VI. Simplificando os termos e agrupando:
 $$MSE = E[Y_{t+1}^2] - E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}]$$
  $\blacksquare$
Esta decomposi√ß√£o do MSE mostra que a proje√ß√£o linear busca capturar a m√°xima vari√¢ncia poss√≠vel de $Y_{t+1}$ atrav√©s de uma combina√ß√£o linear de $X_t$.

**Proposi√ß√£o 1:** (Unicidade da Proje√ß√£o Linear) *Se a matriz* $E[X_tX_t']$ *√© n√£o singular, ent√£o o vetor* $\alpha$ *que minimiza o MSE na proje√ß√£o linear* $P(Y_{t+1}|X_t) = \alpha'X_t$ *√© √∫nico.*

*Proof:*
I.  O vetor $\alpha$ √© definido como $\alpha' = E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}$.
II. Para que $\alpha$ seja √∫nico, a inversa $[E(X_tX_t')]^{-1}$ precisa ser √∫nica.
III. A condi√ß√£o para que uma matriz tenha inversa √∫nica √© que ela seja n√£o singular.
IV. Portanto, se $E[X_tX_t']$ √© n√£o singular, ent√£o sua inversa $[E(X_tX_t')]^{-1}$ √© √∫nica.
V. Consequentemente, o vetor $\alpha$ √© √∫nico, garantindo a unicidade da proje√ß√£o linear. $\blacksquare$

**Corol√°rio 1:** (Interpreta√ß√£o do MSE m√≠nimo) *O erro quadr√°tico m√©dio (MSE) m√≠nimo da proje√ß√£o linear, dado por:*
$$MSE = E(Y_{t+1}^2) - E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}E(X_tY_{t+1})$$
*representa a vari√¢ncia de* $Y_{t+1}$ *que n√£o √© explicada linearmente pelas vari√°veis explicativas* $X_t$.

*Proof:*
I. Do Lema 1.1, sabemos que $MSE = E[Y_{t+1}^2] - E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}]$.
II. O termo $E[Y_{t+1}^2]$ representa a vari√¢ncia de $Y_{t+1}$, denotada por $Var[Y_{t+1}]$.
III. O termo $E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}]$ representa a vari√¢ncia de $Y_{t+1}$ explicada pela proje√ß√£o linear.
IV. Portanto, o MSE √© a diferen√ßa entre a vari√¢ncia total de $Y_{t+1}$ e a vari√¢ncia explicada pela proje√ß√£o linear.
V. Isso implica que o MSE m√≠nimo representa a vari√¢ncia de $Y_{t+1}$ que n√£o pode ser explicada linearmente pelas vari√°veis explicativas $X_t$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Vamos usar os dados do exemplo anterior para calcular o MSE da proje√ß√£o linear usando a f√≥rmula do Corol√°rio 1.
>
> Primeiro, precisamos calcular $E[Y_{t+1}^2]$, $E[Y_{t+1}X_t]$, e $E[X_t^2]$. J√° calculamos $E[Y_{t+1}X_t] = 334$ e $E[X_t^2] = 18$.
>
> Agora, calculamos $E[Y_{t+1}^2]$:
> $$E[Y_{t+1}^2] = \frac{60^2 + 70^2 + 80^2 + 90^2 + 95^2}{5} = \frac{3600 + 4900 + 6400 + 8100 + 9025}{5} = \frac{32025}{5} = 6405$$
>
> Agora podemos calcular o MSE usando a f√≥rmula do Corol√°rio 1:
> $$MSE = E[Y_{t+1}^2] - E[Y_{t+1}X_t][E(X_t^2)]^{-1}E[X_tY_{t+1}]$$
> $$MSE = 6405 - 334 \cdot (18)^{-1} \cdot 334 = 6405 - \frac{334^2}{18} = 6405 - \frac{111556}{18} = 6405 - 6197.56 \approx 207.44$$
>
> Esse valor √© muito pr√≥ximo do MSE calculado anteriormente (207.45), confirmando a validade da f√≥rmula.
>
> A vari√¢ncia total de $Y_{t+1}$ √© dada por:
> $$Var(Y_{t+1}) = E[Y_{t+1}^2] - (E[Y_{t+1}])^2$$
>
> Primeiro calculamos $E[Y_{t+1}]$:
> $$E[Y_{t+1}] = \frac{60+70+80+90+95}{5} = \frac{395}{5} = 79$$
>
> Agora podemos calcular a vari√¢ncia:
> $$Var(Y_{t+1}) = 6405 - 79^2 = 6405 - 6241 = 164$$
>
> O MSE da proje√ß√£o linear (207.44) √© a por√ß√£o da vari√¢ncia de $Y_{t+1}$ que a proje√ß√£o linear n√£o consegue explicar.

### Conclus√£o
A proje√ß√£o linear, definida como $P(Y_{t+1}|X_t) = \alpha'X_t$, encontra o vetor $\alpha$ que minimiza o erro quadr√°tico m√©dio dentro da classe das previs√µes lineares. A condi√ß√£o de ortogonalidade, $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$, garante que o erro de previs√£o seja n√£o correlacionado com as vari√°veis explicativas, o que √© fundamental para a otimalidade da proje√ß√£o linear dentro de sua classe. A proje√ß√£o linear √© um conceito fundamental na modelagem de s√©ries temporais e serve como base para uma vasta gama de outros modelos e ferramentas. Apesar de estar limitada a rela√ß√µes lineares, a proje√ß√£o linear oferece uma excelente alternativa quando a expectativa condicional √© desconhecida ou computacionalmente invi√°vel.
  
### Refer√™ncias
[^1]: Expression [4.1.1], [4.1.2], [4.1.3], [4.1.4], [4.1.5], [4.1.6], [4.1.7], [4.1.8]
[^2]: [4.1.9], [4.1.10], [4.1.11], [4.1.12], [4.1.13]
<!-- END -->
