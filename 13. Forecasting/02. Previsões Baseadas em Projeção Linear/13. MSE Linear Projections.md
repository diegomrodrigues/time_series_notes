## An√°lise Detalhada do Erro Quadr√°tico M√©dio em Proje√ß√µes Lineares

### Introdu√ß√£o
Este cap√≠tulo investiga o erro quadr√°tico m√©dio (MSE) associado √†s **proje√ß√µes lineares**, detalhando como a express√£o $E(Y_{t+1} - \alpha'X_t)^2$ pode ser expandida e simplificada para revelar a influ√™ncia do vetor de coeficientes $\alpha$ na vari√¢ncia do erro de previs√£o. Como explorado anteriormente, a proje√ß√£o linear busca minimizar esse MSE, encontrando o vetor $\alpha$ que resulta na melhor aproxima√ß√£o linear de $Y_{t+1}$ a partir de $X_t$ [^1]. A an√°lise a seguir, utilizando propriedades da esperan√ßa e substituindo o coeficiente $\alpha$, nos permitir√° compreender em profundidade como o MSE √© afetado e o seu significado no contexto da proje√ß√£o linear.

### Expans√£o e Simplifica√ß√£o do MSE
O erro quadr√°tico m√©dio associado a uma proje√ß√£o linear $P(Y_{t+1}|X_t) = \alpha'X_t$ √© definido como:
$$MSE = E[(Y_{t+1} - \alpha'X_t)^2]$$
Para entender como a escolha de $\alpha$ afeta o MSE, podemos expandir essa express√£o:
$$MSE = E[Y_{t+1}^2 - 2Y_{t+1}\alpha'X_t + (\alpha'X_t)^2]$$
Aplicando a propriedade da linearidade da esperan√ßa, temos:
$$MSE = E[Y_{t+1}^2] - 2E[Y_{t+1}\alpha'X_t] + E[(\alpha'X_t)^2]$$
Reorganizando a express√£o do MSE para evidenciar sua depend√™ncia em rela√ß√£o a $\alpha$, temos:
$$MSE = E[Y_{t+1}^2] - 2E[Y_{t+1}X_t']\alpha + E[X_tX_t']\alpha\alpha'$$
Agora, sabemos que o vetor $\alpha$ na proje√ß√£o linear √© dado por $\alpha' = E[Y_{t+1}X_t] [E(X_tX_t')]^{-1}$ [^2]. Substituindo essa express√£o no MSE, obtemos:
$$MSE = E[Y_{t+1}^2] - 2E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}] + E[X_tX_t']E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1} [E(X_tX_t')]^{-1}E[X_tY_{t+1}]$$
Simplificando os termos, temos que:
$$MSE = E[Y_{t+1}^2] - 2E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}] + E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1} E[X_tY_{t+1}]$$
$$MSE = E[Y_{t+1}^2] - E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}]$$
Essa express√£o final revela que o MSE pode ser decomposto em duas partes: a vari√¢ncia de $Y_{t+1}$ ($E[Y_{t+1}^2]$) e um termo que depende dos momentos cruzados de $Y_{t+1}$ e $X_t$, e da inversa da matriz de covari√¢ncia de $X_t$. Este termo captura a varia√ß√£o de $Y_{t+1}$ que √© explicada linearmente por $X_t$. Portanto, ao minimizar o MSE, a proje√ß√£o linear busca maximizar a vari√¢ncia explicada por $X_t$.

> üí° **Exemplo Num√©rico:**
> Vamos considerar um exemplo pr√°tico para ilustrar a expans√£o e simplifica√ß√£o do MSE. Suponha que estamos prevendo o n√∫mero de clientes em uma loja ($Y_{t+1}$) com base na quantidade de dinheiro gasto em publicidade ($X_t$). Temos alguns dados simulados:
>
> | t | Publicidade ($X_t$) | Clientes ($Y_{t+1}$) |
> |---|---------------------|----------------------|
> | 1 | 2                   | 5                    |
> | 2 | 3                   | 7                    |
> | 3 | 4                   | 9                    |
> | 4 | 5                   | 11                   |
> | 5 | 6                   | 12                   |
>
>  Primeiro, calculamos os momentos populacionais necess√°rios:
>
> 1. **Calculamos** $E[X_t^2]$:
>  $$E[X_t^2] = \frac{2^2 + 3^2 + 4^2 + 5^2 + 6^2}{5} = \frac{4+9+16+25+36}{5} = \frac{90}{5} = 18$$
>
> 2.  **Calculamos** $E[Y_{t+1}X_t]$:
>  $$E[Y_{t+1}X_t] = \frac{2\cdot5 + 3\cdot7 + 4\cdot9 + 5\cdot11 + 6\cdot12}{5} = \frac{10 + 21 + 36 + 55 + 72}{5} = \frac{194}{5} = 38.8$$
>
> 3. **Calculamos** $E[Y_{t+1}^2]$:
>  $$E[Y_{t+1}^2] = \frac{5^2 + 7^2 + 9^2 + 11^2 + 12^2}{5} = \frac{25+49+81+121+144}{5} = \frac{420}{5} = 84$$
>
> Agora, podemos calcular o vetor $\alpha$ e o MSE:
>
> $$\alpha = \frac{E[Y_{t+1}X_t]}{E[X_t^2]} = \frac{38.8}{18} \approx 2.156$$
>
> A proje√ß√£o linear √© $\hat{Y}_{t+1} = 2.156X_t$.
>
> O MSE utilizando os momentos calculados √©:
> $$MSE = E[Y_{t+1}^2] - E[Y_{t+1}X_t] [E(X_tX_t')]^{-1}E[X_tY_{t+1}]$$
> $$MSE = 84 - 38.8 \cdot \frac{1}{18} \cdot 38.8 = 84 - \frac{38.8^2}{18} = 84 - 83.47555 \approx 0.52$$
>
> Para verificar, vamos calcular o MSE diretamente:
> | t   | $X_t$ | $Y_{t+1}$ | $\hat{Y}_{t+1}=2.156X_t$ | $Y_{t+1}-\hat{Y}_{t+1}$ | $(Y_{t+1} - \hat{Y}_{t+1})^2$ |
> |-----|-------|-----------|---------------------------|-------------------------|-----------------------------|
> | 1   | 2     | 5         | 4.312                      | 0.688                   | 0.473                       |
> | 2   | 3     | 7         | 6.468                      | 0.532                   | 0.283                       |
> | 3   | 4     | 9         | 8.624                      | 0.376                   | 0.141                       |
> | 4   | 5     | 11        | 10.78                     | 0.220                   | 0.048                       |
> | 5   | 6     | 12        | 12.936                     | -0.936                  | 0.876                       |
> $$MSE = \frac{0.473+0.283+0.141+0.048+0.876}{5} = \frac{1.821}{5} = 0.3642$$
>
> A pequena diferen√ßa nos resultados surge pois nos c√°lculos acima usamos dados amostrais para aproximar os momentos populacionais. A f√≥rmula, na realidade, precisa dos momentos populacionais. No entanto, ambos os resultados demonstram que o MSE √© afetado pelos momentos de $Y_{t+1}$ e $X_t$ e da forma como eles interagem na proje√ß√£o linear.
>
> Al√©m disso, podemos calcular a vari√¢ncia de $Y_{t+1}$ para mostrar que a proje√ß√£o linear est√° capturando uma parte da vari√¢ncia total:
>  $$E(Y_{t+1}) = \frac{5+7+9+11+12}{5} = \frac{44}{5} = 8.8$$
> $$Var(Y_{t+1}) = \frac{(5-8.8)^2 + (7-8.8)^2 + (9-8.8)^2 + (11-8.8)^2 + (12-8.8)^2}{5} = \frac{14.44 + 3.24 + 0.04 + 4.84 + 10.24}{5} = \frac{32.8}{5} = 6.56$$
>
> A vari√¢ncia da proje√ß√£o linear $\hat{Y}_{t+1} = 2.156 X_t$ √© dada por:
> $$Var(\hat{Y}_{t+1}) = \frac{(2.156\cdot2 - 8.8)^2 + (2.156\cdot3 - 8.8)^2 + (2.156\cdot4 - 8.8)^2 + (2.156\cdot5 - 8.8)^2 + (2.156\cdot6 - 8.8)^2}{5} $$
> $$Var(\hat{Y}_{t+1}) = \frac{(-4.48)^2 + (-2.32)^2 + (-0.16)^2 + (2)^2 + (4.16)^2}{5} = \frac{20.07 + 5.38 + 0.025 + 4 + 17.31}{5} = \frac{46.785}{5} = 9.357$$
>
> Observe que $Var(Y_{t+1})$ √© igual a soma do MSE e a vari√¢ncia da proje√ß√£o linear (considerando os erros de amostra): $6.56 \approx 0.364 + 9.357$, demonstrando a decomposi√ß√£o da vari√¢ncia.
>
> Esse exemplo ilustra como o MSE √© influenciado pela vari√¢ncia total de $Y_{t+1}$ e pela capacidade da proje√ß√£o linear em capturar parte dessa varia√ß√£o atrav√©s de $X_t$.

### Interpreta√ß√£o do MSE
A express√£o final do MSE demonstra que o objetivo da proje√ß√£o linear √© encontrar um vetor $\alpha$ que minimize a diferen√ßa entre a vari√¢ncia de $Y_{t+1}$ e a vari√¢ncia explicada por $X_t$ atrav√©s da proje√ß√£o linear. O termo $E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}]$ representa a parte da vari√¢ncia de $Y_{t+1}$ que √© capturada pela proje√ß√£o linear, e o MSE representa a por√ß√£o da vari√¢ncia de $Y_{t+1}$ que n√£o √© explicada linearmente por $X_t$.
$$MSE = E[Y_{t+1}^2] - E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}]$$
O MSE da proje√ß√£o linear tem o mesmo papel da vari√¢ncia do erro na regress√£o linear por m√≠nimos quadrados. De fato, se substituirmos os momentos populacionais na express√£o do MSE por seus estimadores amostrais, obtemos a soma dos erros ao quadrado da regress√£o linear.

**Lema 1:** (Decomposi√ß√£o do MSE da Proje√ß√£o Linear) *O erro quadr√°tico m√©dio (MSE) da proje√ß√£o linear* $P(Y_{t+1}|X_t) = \alpha'X_t$, *dado por*  $MSE = E[(Y_{t+1} - \alpha'X_t)^2]$, *pode ser expresso como:*
$$MSE = E[Y_{t+1}^2] - E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}]$$
*Proof:*
I. Partimos da defini√ß√£o do MSE:
$$MSE = E[(Y_{t+1} - \alpha'X_t)^2]$$
II. Expandindo o termo quadr√°tico, obtemos:
$$MSE = E[Y_{t+1}^2 - 2Y_{t+1}\alpha'X_t + (\alpha'X_t)^2]$$
III. Aplicando a propriedade da linearidade da esperan√ßa:
$$MSE = E[Y_{t+1}^2] - 2E[Y_{t+1}\alpha'X_t] + E[(\alpha'X_t)^2]$$
IV. Reorganizando a express√£o do MSE:
$$MSE = E[Y_{t+1}^2] - 2E[Y_{t+1}X_t']\alpha + E[X_tX_t']\alpha\alpha'$$
V. Substituindo a express√£o para $\alpha$:
$$MSE = E[Y_{t+1}^2] - 2E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}] + E[X_tX_t']E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}  [E(X_tX_t')]^{-1}E[X_tY_{t+1}]$$
VI.  Simplificando os termos, obtemos a forma final do MSE:
$$MSE = E[Y_{t+1}^2] - 2E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}] + E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1} E[X_tY_{t+1}]$$
VII.  Portanto:
$$MSE = E[Y_{t+1}^2] - E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}]$$  $\blacksquare$
 
  **Teorema 1:** (Decomposi√ß√£o da Vari√¢ncia e o MSE) *A vari√¢ncia de* $Y_{t+1}$ *pode ser decomposta como a soma da vari√¢ncia da proje√ß√£o linear* $\alpha'X_t$ *e do MSE da proje√ß√£o, que representa a vari√¢ncia do erro* $e_{t+1}$:
   $$Var(Y_{t+1}) = Var(\alpha'X_t) + MSE(Y_{t+1}|X_t)$$
  *Proof:*
 I.   Partimos do fato de que  $Y_{t+1} = \alpha'X_t + e_{t+1}$, onde $e_{t+1} = Y_{t+1} - \alpha'X_t$ √© o erro de previs√£o.
 II.   Calculando a vari√¢ncia de ambos os lados:
    $$Var(Y_{t+1}) = Var(\alpha'X_t + e_{t+1})$$
 III. Expandindo:
     $$Var(Y_{t+1}) = Var(\alpha'X_t) + Var(e_{t+1}) + 2Cov(\alpha'X_t, e_{t+1})$$
 IV. Pela propriedade da ortogonalidade entre o erro da proje√ß√£o linear e a proje√ß√£o linear (Lema 1.1 da se√ß√£o anterior),  temos que $Cov(\alpha'X_t, e_{t+1})=0$.
 V. Logo,
  $$Var(Y_{t+1}) = Var(\alpha'X_t) + Var(e_{t+1})$$
 VI. Reconhecendo que $Var(e_{t+1})$ √© o MSE da proje√ß√£o linear $P(Y_{t+1}|X_t)$:
   $$Var(Y_{t+1}) = Var(\alpha'X_t) + MSE(Y_{t+1}|X_t)$$
$\blacksquare$

Este teorema demonstra que a vari√¢ncia total de $Y_{t+1}$ √© dividida em duas partes: a parte explicada pela proje√ß√£o linear e a parte que n√£o pode ser explicada pela proje√ß√£o linear (o MSE). Ao minimizar o MSE, a proje√ß√£o linear busca maximizar a parte da vari√¢ncia explicada e, por consequ√™ncia, a capacidade preditiva do modelo.

**Proposi√ß√£o 1** (MSE Condicional e Incondicional): *O MSE calculado at√© o momento √© um MSE incondicional. Podemos ainda definir o MSE condicional dado* $X_t$:
$$MSE(Y_{t+1}|X_t) = E[(Y_{t+1} - \alpha'X_t)^2|X_t]$$
*e o MSE incondicional ser√°:*
$$MSE = E[MSE(Y_{t+1}|X_t)]$$
*Proof:*
I.  Partimos da defini√ß√£o do MSE incondicional:
$$MSE = E[(Y_{t+1} - \alpha'X_t)^2]$$
II. Aplicando a lei das expectativas totais, temos:
$$MSE = E[E[(Y_{t+1} - \alpha'X_t)^2|X_t]]$$
III.  Reconhecendo que o termo interno √© o MSE condicional, definimos:
$$MSE(Y_{t+1}|X_t) = E[(Y_{t+1} - \alpha'X_t)^2|X_t]$$
IV.  Portanto, o MSE incondicional √© a esperan√ßa do MSE condicional:
$$MSE = E[MSE(Y_{t+1}|X_t)]$$ $\blacksquare$

A Proposi√ß√£o 1 esclarece a rela√ß√£o entre o MSE condicional e incondicional, mostrando que o MSE que temos trabalhado at√© aqui √© o valor esperado do MSE condicionado em $X_t$. Esta diferencia√ß√£o √© importante em alguns cen√°rios onde a distribui√ß√£o condicional de $Y_{t+1}$ dado $X_t$ √© relevante.

> üí° **Exemplo Num√©rico:**
> Para entender a diferen√ßa entre MSE condicional e incondicional, vamos expandir o exemplo anterior. Suponha que os clientes ($Y_{t+1}$) tamb√©m dependam de um fator aleat√≥rio (por exemplo, o dia da semana), que √© desconhecido para o modelador. Vamos gerar dados adicionais:
>
> | t  | Publicidade ($X_t$) | Fator Aleat√≥rio | Clientes ($Y_{t+1}$) |
> |----|---------------------|-----------------|----------------------|
> | 1  | 2                   | 1               | 6                   |
> | 2  | 3                   | 2               | 9                   |
> | 3  | 4                   | -1              | 8                   |
> | 4  | 5                   | 0               | 11                  |
> | 5  | 6                   | 1               | 13                  |
>
>  Note que o n√∫mero de clientes $Y_{t+1}$ √© dado por $Y_{t+1} = 2.156X_t + \epsilon_t$, onde $\epsilon_t$ √© um termo de erro que incorpora o "fator aleat√≥rio". Vamos calcular o MSE condicional para cada valor de $X_t$, usando a mesma proje√ß√£o linear $\hat{Y}_{t+1} = 2.156X_t$ obtida anteriormente:
>
> | t | $X_t$ | $Y_{t+1}$ | $\hat{Y}_{t+1}=2.156X_t$ | $Y_{t+1}-\hat{Y}_{t+1}$ | $(Y_{t+1} - \hat{Y}_{t+1})^2$ |
> |---|-------|-----------|---------------------------|-------------------------|-----------------------------|
> | 1 | 2     | 6         | 4.312                      | 1.688                   | 2.85                     |
> | 2 | 3     | 9         | 6.468                      | 2.532                   | 6.41                     |
> | 3 | 4     | 8         | 8.624                      | -0.624                  | 0.39                     |
> | 4 | 5     | 11        | 10.78                      | 0.220                   | 0.048                    |
> | 5 | 6     | 13        | 12.936                     | 0.064                   | 0.004                    |
>
> O MSE condicional para cada $X_t$ √© simplesmente $(Y_{t+1} - \hat{Y}_{t+1})^2$. O MSE incondicional √© a m√©dia desses valores:
> $$MSE = E[MSE(Y_{t+1}|X_t)] = \frac{2.85 + 6.41 + 0.39 + 0.048 + 0.004}{5} = \frac{9.702}{5} \approx 1.94$$
>
> Observe que este MSE incondicional (1.94) √© maior do que o MSE no exemplo anterior (0.364), refletindo a adi√ß√£o de um erro aleat√≥rio na gera√ß√£o de $Y_{t+1}$. O MSE condicional captura a varia√ß√£o do erro para cada valor espec√≠fico de $X_t$. Por exemplo, quando $X_t=3$, o erro quadr√°tico $(Y_{t+1} - \hat{Y}_{t+1})^2=6.41$, enquanto quando $X_t=6$, o erro √© apenas 0.004. O MSE incondicional √© uma m√©dia desses erros ao longo de todos os valores de $X_t$.
>
> Este exemplo demonstra que o MSE condicional avalia a precis√£o da previs√£o para um dado valor de $X_t$, enquanto o MSE incondicional avalia a precis√£o da previs√£o no geral, em todos os valores de $X_t$. A diferen√ßa surge da esperan√ßa de um termo de erro aleat√≥rio que adiciona incerteza √† previs√£o.

**Corol√°rio 1.1** (MSE Condicional): *O MSE condicional da proje√ß√£o linear pode ser expresso como:*
$$MSE(Y_{t+1}|X_t) = Var(Y_{t+1}|X_t) -  E[(Y_{t+1}|X_t)X_t'] [E(X_tX_t'|X_t)]^{-1}E[X_tY_{t+1}|X_t]$$
*Proof:*
I. O MSE condicional √© dado por:
$$MSE(Y_{t+1}|X_t) = E[(Y_{t+1} - \alpha'X_t)^2|X_t]$$
II. Expandindo e aplicando a linearidade da esperan√ßa condicional, de forma an√°loga ao Lema 1, obtemos:
$$MSE(Y_{t+1}|X_t) = E[Y_{t+1}^2|X_t] - E[Y_{t+1}X_t'|X_t] [E(X_tX_t'|X_t)]^{-1}E[X_tY_{t+1}|X_t]$$
III.  Reconhecendo que $Var(Y_{t+1}|X_t) = E[Y_{t+1}^2|X_t] - (E[Y_{t+1}|X_t])^2$, temos:
 $$MSE(Y_{t+1}|X_t) = Var(Y_{t+1}|X_t) + (E[Y_{t+1}|X_t])^2 -  E[Y_{t+1}X_t'|X_t] [E(X_tX_t'|X_t)]^{-1}E[X_tY_{t+1}|X_t]$$
 IV. Observando que a proje√ß√£o linear $\alpha'X_t =  E[Y_{t+1}X_t'|X_t] [E(X_tX_t'|X_t)]^{-1}X_t$ √© igual a esperan√ßa condicional $E(Y_{t+1}|X_t)$ quando $E[Y_{t+1}|X_t]$ √© uma fun√ß√£o linear de $X_t$. Em geral, a express√£o  do MSE condicional √©:
$$MSE(Y_{t+1}|X_t) = Var(Y_{t+1}|X_t) -  E[(Y_{t+1}|X_t)X_t'] [E(X_tX_t'|X_t)]^{-1}E[X_tY_{t+1}|X_t]$$
$\blacksquare$

Este corol√°rio apresenta a forma do MSE condicional, que √© an√°loga ao MSE incondicional, por√©m utilizando esperan√ßas e vari√¢ncias condicionais. Essa express√£o √© √∫til quando queremos analisar a qualidade da previs√£o em fun√ß√£o de valores espec√≠ficos de $X_t$.

### Conclus√£o
O MSE associado √†s proje√ß√µes lineares, expresso como $E(Y_{t+1} - \alpha'X_t)^2$, pode ser decomposto de forma a mostrar como a escolha de $\alpha$ afeta a vari√¢ncia do erro de previs√£o. O vetor $\alpha$ √© determinado a partir da condi√ß√£o de ortogonalidade, e, ao substituir este valor na equa√ß√£o do MSE, percebemos que a proje√ß√£o linear busca capturar a m√°xima vari√¢ncia poss√≠vel de $Y_{t+1}$ usando $X_t$. A an√°lise realizada neste cap√≠tulo, utilizando as propriedades da esperan√ßa e simplificando o MSE, fornece uma vis√£o clara do funcionamento da proje√ß√£o linear e de sua rela√ß√£o com a minimiza√ß√£o da vari√¢ncia do erro de previs√£o. A equival√™ncia entre a proje√ß√£o linear e a regress√£o OLS fornece um framework √∫til para an√°lise de dados e constru√ß√£o de previs√µes.

### Refer√™ncias
[^1]: Expression [4.1.1], [4.1.2], [4.1.3], [4.1.4], [4.1.5], [4.1.6], [4.1.7], [4.1.8]
[^2]: [4.1.9], [4.1.10], [4.1.11], [4.1.12], [4.1.13]
<!-- END -->
