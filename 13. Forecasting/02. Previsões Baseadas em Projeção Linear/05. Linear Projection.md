## A Proje√ß√£o Linear Formalizada: Coeficientes e Ortogonalidade
### Introdu√ß√£o

Neste cap√≠tulo, formalizamos a defini√ß√£o da **proje√ß√£o linear** de $Y_{t+1}$ sobre $X_t$, focando na sua representa√ß√£o matem√°tica e na condi√ß√£o essencial de n√£o correla√ß√£o entre o erro de previs√£o e as vari√°veis explicativas. Como explorado em cap√≠tulos anteriores [^1], a proje√ß√£o linear busca a melhor aproxima√ß√£o linear de $Y_{t+1}$ em termos de $X_t$, minimizando o erro quadr√°tico m√©dio (MSE). A representa√ß√£o formal dessa proje√ß√£o como $\alpha'X_t$, onde $\alpha'$ √© um vetor de coeficientes, √© crucial para entender como construir previs√µes √≥timas dentro do espa√ßo linear [^2].

### Defini√ß√£o Formal da Proje√ß√£o Linear

A proje√ß√£o linear de $Y_{t+1}$ sobre $X_t$ √© formalmente definida como $\alpha'X_t$, onde $\alpha$ √© um vetor de coeficientes que garante que o erro de previs√£o, $Y_{t+1} - \alpha'X_t$, seja n√£o correlacionado com as vari√°veis explicativas $X_t$ [^2]. A condi√ß√£o de n√£o correla√ß√£o, expressa matematicamente como:
$$E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$$
√© o pilar central da proje√ß√£o linear. Essa condi√ß√£o significa que n√£o h√° nenhuma informa√ß√£o linearmente relevante em $X_t$ que possa ser usada para reduzir o erro de previs√£o [^2].

> üí° **Exemplo Num√©rico:**
> Vamos considerar a previs√£o do pre√ßo de um ativo financeiro ($Y_{t+1}$) com base em um √≠ndice de mercado ($X_t$).  Suponha que tenhamos um conjunto de dados simulados:
>
> | t | √çndice de Mercado ($X_t$) | Pre√ßo do Ativo ($Y_{t+1}$) |
> |---|-------------------------|-------------------------|
> | 1 | 100                     | 120                     |
> | 2 | 105                     | 130                     |
> | 3 | 110                     | 140                     |
> | 4 | 115                     | 155                     |
> | 5 | 120                     | 170                     |
>
> Expressando em termos vetoriais:
> $$ X_t = \begin{bmatrix} 100 \\ 105 \\ 110 \\ 115 \\ 120 \end{bmatrix}, \quad Y_{t+1} = \begin{bmatrix} 120 \\ 130 \\ 140 \\ 155 \\ 170 \end{bmatrix} $$
>
> Queremos encontrar um valor de $\alpha$ tal que $P(Y_{t+1}|X_t) = \alpha X_t$, ou seja,  a proje√ß√£o linear √© uma fun√ß√£o linear da vari√°vel explicativa $X_t$. A condi√ß√£o de n√£o correla√ß√£o,  $E[(Y_{t+1} - \alpha X_t)X_t] = 0$,  implica que qualquer erro de previs√£o n√£o tenha nenhuma rela√ß√£o linear com o √≠ndice de mercado.
>
> Para calcular $\alpha$, podemos usar a f√≥rmula dos m√≠nimos quadrados, dado que a proje√ß√£o linear √© equivalente √† regress√£o por m√≠nimos quadrados quando o intercepto √© for√ßado a ser zero. Calculamos $\alpha$ usando o seguinte c√≥digo Python:
>
> ```python
> import numpy as np
>
> X = np.array([[100], [105], [110], [115], [120]])
> Y = np.array([120, 130, 140, 155, 170])
>
> XtX = X.T @ X
> XtY = X.T @ Y
> alpha = np.linalg.solve(XtX, XtY)
> print(f"O valor de alpha √©: {alpha[0]:.4f}")
> ```
>
>  O c√≥digo acima produzir√° um valor de $\alpha \approx 1.3169$. Isso significa que, para cada unidade de aumento no √≠ndice de mercado, o pre√ßo do ativo aumenta em aproximadamente 1.3169 unidades. Assim, a proje√ß√£o linear de $Y_{t+1}$ sobre $X_t$ √© dada por $\hat{Y}_{t+1} = 1.3169 X_t$.

Matematicamente, a condi√ß√£o de n√£o correla√ß√£o implica que a esperan√ßa do produto entre o erro de previs√£o e o vetor de vari√°veis explicativas seja igual a um vetor nulo, ou seja, $0'$. Em termos de momentos populacionais, essa condi√ß√£o se traduz em:
$$E[Y_{t+1}X_t] - \alpha'E[X_tX_t'] = 0'$$
Resolvendo para $\alpha'$, temos:
$$\alpha' = E[Y_{t+1}X_t] [E(X_tX_t')]^{-1}$$
Essa f√≥rmula mostra como os coeficientes da proje√ß√£o linear s√£o determinados pelos momentos populacionais de $Y_{t+1}$ e $X_t$ [^2]. Se $E[X_tX_t']$ for singular, o vetor $\alpha$ n√£o √© unicamente definido, embora o produto $\alpha'X_t$ seja unicamente determinado [^3].

> üí° **O Significado da N√£o Correla√ß√£o:**
> A condi√ß√£o $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$ n√£o √© uma restri√ß√£o imposta arbitrariamente, mas sim uma consequ√™ncia direta da minimiza√ß√£o do erro quadr√°tico m√©dio (MSE). Quando buscamos a melhor aproxima√ß√£o linear de $Y_{t+1}$ em termos de $X_t$, estamos automaticamente exigindo que o erro de previs√£o seja ortogonal ao espa√ßo gerado por $X_t$.
>
> Para demonstrar a rela√ß√£o entre ortogonalidade e minimiza√ß√£o do MSE, vamos expressar o erro de previs√£o como $e_{t+1} = Y_{t+1} - \alpha'X_t$. O MSE √© definido como $E[e_{t+1}^2] = E[(Y_{t+1} - \alpha'X_t)^2]$.  Queremos minimizar essa express√£o. Para isso, calculamos a derivada do MSE com rela√ß√£o a $\alpha'$ e igualamos a zero:
>  $$\frac{\partial MSE}{\partial \alpha'} = \frac{\partial}{\partial \alpha'} E[(Y_{t+1} - \alpha'X_t)^2] = 0'$$
>
> Usando a propriedade da linearidade da esperan√ßa:
>
>  $$\frac{\partial}{\partial \alpha'} E[Y_{t+1}^2 - 2Y_{t+1}\alpha'X_t + (\alpha'X_t)^2] = -2E[Y_{t+1}X_t] + 2E[X_tX_t']\alpha' = 0'$$
>
> Resolvendo para $\alpha'$, obtemos:
>
> $$\alpha' = E[Y_{t+1}X_t][E[X_tX_t']]^{-1}$$
>
> Observe que essa √© exatamente a mesma express√£o para os coeficientes da proje√ß√£o linear obtida anteriormente. Ou seja, a condi√ß√£o de n√£o correla√ß√£o √© uma consequ√™ncia da minimiza√ß√£o do MSE dentro da classe de previs√µes lineares. Portanto, se $\alpha$ minimizar o MSE dentro do espa√ßo linear, o erro de previs√£o precisa ser n√£o correlacionado com $X_t$, e vice versa.
>
> üí° **Exemplo Num√©rico:**
> Suponha que tenhamos um modelo onde $Y_{t+1}$ √© o consumo de energia de uma casa e $X_t$ √© a temperatura m√©dia no dia anterior.  Vamos considerar um conjunto de dados hipot√©ticos para exemplificar o processo de minimiza√ß√£o do MSE e o c√°lculo de $\alpha$.
>
> | t  | Temperatura ($X_t$) | Consumo ($Y_{t+1}$) |
> |----|---------------------|--------------------|
> | 1  | 20                  | 150                |
> | 2  | 22                  | 160                |
> | 3  | 24                  | 175                |
> | 4  | 26                  | 190                |
> | 5  | 28                  | 200                |
>
> Usando os dados acima, podemos calcular $E[Y_{t+1}X_t]$ e $E[X_tX_t']$. Primeiro, convertemos em arrays numpy:
>
> ```python
> import numpy as np
>
> X = np.array([20, 22, 24, 26, 28])
> Y = np.array([150, 160, 175, 190, 200])
>
> E_XtY = np.mean(X * Y)
> E_XtXt = np.mean(X * X)
>
> alpha = E_XtY / E_XtXt
> print(f"O valor de alpha √©: {alpha:.4f}")
>
> ```
>
> O c√≥digo produzir√° $\alpha \approx 6.2273$. Isso significa que, em m√©dia, para cada grau Celsius de aumento na temperatura, o consumo de energia aumenta em aproximadamente 6.2273 unidades. A proje√ß√£o linear do consumo de energia $Y_{t+1}$ √© dada por $\hat{Y}_{t+1} = 6.2273X_t$.
>
> Para verificar a minimiza√ß√£o do MSE, podemos calcular o MSE usando este valor de $\alpha$. Primeiro, calculamos os valores previstos $\hat{Y}_{t+1}$:
>
> | t  | Temperatura ($X_t$) | Consumo ($Y_{t+1}$) | $\hat{Y}_{t+1} = 6.2273 X_t$ | $e_{t+1} = Y_{t+1} - \hat{Y}_{t+1}$ | $e_{t+1}^2$ |
> |----|---------------------|--------------------|------------------------------|------------------------------------|-------------|
> | 1  | 20                  | 150                | 124.546                        | 25.454                            | 647.90        |
> | 2  | 22                  | 160                | 137.0006                       | 22.9994                            | 528.97        |
> | 3  | 24                  | 175                | 149.4552                       | 25.5448                            | 652.53        |
> | 4  | 26                  | 190                | 161.9098                       | 28.0902                            | 789.06        |
> | 5  | 28                  | 200                | 174.3644                       | 25.6356                            | 657.18        |
>
> O MSE √© a m√©dia dos erros ao quadrado:
>
>  $$MSE = \frac{647.90 + 528.97 + 652.53 + 789.06 + 657.18}{5} \approx 655.13$$
>
>  O valor de $\alpha$ que encontramos minimiza o MSE, garantindo que o erro de previs√£o n√£o tenha correla√ß√£o linear com a vari√°vel explicativa, temperatura.

**Lema 1** (Propriedade da Ortogonalidade): *Se o erro de previs√£o* $e_{t+1} = Y_{t+1} - \alpha'X_t$ *√© ortogonal a* $X_t$, *ent√£o ele √© ortogonal a qualquer combina√ß√£o linear de* $X_t$, ou seja, para qualquer vetor constante $b$, temos:
$$E[e_{t+1}b'X_t] = 0$$
*Proof:*
I.  Sabemos que  $E[e_{t+1}X_t]=0'$ pela defini√ß√£o da proje√ß√£o linear.
II. Para qualquer vetor constante $b$, considere $b'X_t$.
III. Pela propriedade da linearidade da esperan√ßa, temos que: $E[e_{t+1}(b'X_t)] = E[b'e_{t+1}X_t] = b'E[e_{t+1}X_t]$
IV. Como $E[e_{t+1}X_t]=0$, temos: $E[e_{t+1}(b'X_t)] = b'\cdot 0' = 0$.
Portanto, o erro de previs√£o √© ortogonal a qualquer combina√ß√£o linear de $X_t$. $\blacksquare$

**Lema 1.1** (Ortogonalidade do Erro e da Proje√ß√£o): *O erro de previs√£o $e_{t+1}$ √© ortogonal √† pr√≥pria proje√ß√£o linear $\hat{Y}_{t+1} = \alpha'X_t$, ou seja, $E[e_{t+1}\hat{Y}_{t+1}] = 0$.*

*Proof:*
I. Temos que $e_{t+1} = Y_{t+1} - \alpha'X_t$ e $\hat{Y}_{t+1} = \alpha'X_t$.
II.  Ent√£o, $E[e_{t+1}\hat{Y}_{t+1}] = E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t)]$.
III.  Pela linearidade da esperan√ßa, $E[e_{t+1}\hat{Y}_{t+1}] = E[Y_{t+1}\alpha'X_t] - E[\alpha'X_t\alpha'X_t]$.
IV. Reorganizando termos, $E[e_{t+1}\hat{Y}_{t+1}] = \alpha'E[Y_{t+1}X_t] - \alpha'E[X_tX_t']\alpha$.
V.  Sabemos que $\alpha' = E[Y_{t+1}X_t] [E(X_tX_t')]^{-1}$. Substituindo na express√£o anterior:
$E[e_{t+1}\hat{Y}_{t+1}] = \alpha'E[Y_{t+1}X_t] - \alpha'E[X_tX_t'](E[X_tX_t']^{-1}E[Y_{t+1}X_t])' = \alpha'E[Y_{t+1}X_t] - \alpha'E[X_tX_t']E[X_tX_t']^{-1}E[X_tY_{t+1}]$
VI. Dado que $E[X_tY_{t+1}] = (E[Y_{t+1}X_t])'$,
$E[e_{t+1}\hat{Y}_{t+1}] = \alpha'E[Y_{t+1}X_t] - \alpha'E[Y_{t+1}X_t] = 0$
Portanto, o erro de previs√£o √© ortogonal √† sua proje√ß√£o linear. $\blacksquare$

### Propriedades do Vetor de Coeficientes Œ±

O vetor de coeficientes $\alpha$ na proje√ß√£o linear, $P(Y_{t+1}|X_t) = \alpha'X_t$, possui algumas propriedades importantes:
   - **Unicidade:** Se a matriz $E(X_tX_t')$ √© n√£o singular, ent√£o o vetor $\alpha$ √© √∫nico, o que significa que existe apenas uma proje√ß√£o linear que satisfaz a condi√ß√£o de n√£o correla√ß√£o [^2].
    - **Depend√™ncia dos Momentos:** Os coeficientes $\alpha$ dependem exclusivamente dos momentos populacionais $E[Y_{t+1}X_t]$ e $E[X_tX_t']$. Isso significa que, se esses momentos mudarem, o vetor $\alpha$ tamb√©m mudar√°.
  - **N√£o Depend√™ncia da Distribui√ß√£o:** A defini√ß√£o da proje√ß√£o linear e o c√°lculo de $\alpha$ n√£o exigem que uma distribui√ß√£o espec√≠fica para $Y_{t+1}$ e $X_t$ seja assumida. A √∫nica condi√ß√£o necess√°ria √© a exist√™ncia e estabilidade dos momentos populacionais [^2].
    
**Proposi√ß√£o 1** (Linearidade da Proje√ß√£o): *Se* $P(Y_{t+1}|X_t) = \alpha'X_t$ *e* $P(Z_{t+1}|X_t) = \beta'X_t$, *ent√£o, para quaisquer constantes* $c_1$ *e* $c_2$, *temos que* $P(c_1Y_{t+1} + c_2Z_{t+1}|X_t) = c_1\alpha'X_t + c_2\beta'X_t$.

*Proof:*
I. Seja $W_{t+1} = c_1Y_{t+1} + c_2Z_{t+1}$. Queremos mostrar que $P(W_{t+1}|X_t) = c_1\alpha'X_t + c_2\beta'X_t$.
II.  A proje√ß√£o linear de $W_{t+1}$ sobre $X_t$ √© dada por $P(W_{t+1}|X_t) = \gamma'X_t$, onde $\gamma' = E[W_{t+1}X_t][E(X_tX_t')]^{-1}$.
III. Substituindo $W_{t+1}$, temos $\gamma' = E[(c_1Y_{t+1} + c_2Z_{t+1})X_t][E(X_tX_t')]^{-1}$.
IV. Pela linearidade da esperan√ßa, $\gamma' = (c_1E[Y_{t+1}X_t] + c_2E[Z_{t+1}X_t])[E(X_tX_t')]^{-1}$.
V.  Usando a defini√ß√£o de $\alpha$ e $\beta$,  $\gamma' = c_1E[Y_{t+1}X_t][E(X_tX_t')]^{-1} + c_2E[Z_{t+1}X_t][E(X_tX_t')]^{-1} = c_1\alpha' + c_2\beta'$.
VI. Portanto, $P(W_{t+1}|X_t) = (c_1\alpha' + c_2\beta')X_t = c_1\alpha'X_t + c_2\beta'X_t$. $\blacksquare$
> üí° **Exemplo Num√©rico:**
> Imagine que queremos prever o consumo total de energia de uma cidade ($W_{t+1}$), e esse consumo √© a soma ponderada do consumo residencial ($Y_{t+1}$) e do consumo industrial ($Z_{t+1}$), ou seja, $W_{t+1} = 0.6Y_{t+1} + 0.4Z_{t+1}$.  Suponha que j√° temos as proje√ß√µes lineares de $Y_{t+1}$ e $Z_{t+1}$ sobre a temperatura m√©dia do dia anterior ($X_t$):
>
> $$P(Y_{t+1}|X_t) = 5X_t$$
> $$P(Z_{t+1}|X_t) = 10X_t$$
>
>  De acordo com a proposi√ß√£o 1, a proje√ß√£o linear do consumo total $W_{t+1}$ sobre $X_t$ √©:
>
> $$P(W_{t+1}|X_t) = P(0.6Y_{t+1} + 0.4Z_{t+1}|X_t) = 0.6P(Y_{t+1}|X_t) + 0.4P(Z_{t+1}|X_t)$$
> $$P(W_{t+1}|X_t) = 0.6(5X_t) + 0.4(10X_t) = 3X_t + 4X_t = 7X_t$$
>
> Isso significa que a proje√ß√£o linear do consumo total de energia √© simplesmente a combina√ß√£o linear das proje√ß√µes dos consumos individuais, ponderada pelos seus respectivos pesos.

### O Erro da Proje√ß√£o Linear
O erro da proje√ß√£o linear √© definido como a diferen√ßa entre o valor real de $Y_{t+1}$ e sua proje√ß√£o linear, ou seja, $e_{t+1} = Y_{t+1} - \alpha'X_t$. Como j√° vimos, esse erro √© n√£o correlacionado com $X_t$, o que √© fundamental para a otimalidade da proje√ß√£o linear. O MSE da proje√ß√£o linear √© a vari√¢ncia desse erro, dada por:
$$MSE[P(Y_{t+1}|X_t)] = E[e_{t+1}^2] = E[(Y_{t+1} - \alpha'X_t)^2]$$
A condi√ß√£o de n√£o correla√ß√£o garante que a proje√ß√£o linear capture toda a varia√ß√£o linearmente explic√°vel de $Y_{t+1}$ pelas vari√°veis em $X_t$. A parte do erro de previs√£o que n√£o √© correlacionada com as vari√°veis explicativas, que comp√µem o MSE da proje√ß√£o linear, representa uma informa√ß√£o que n√£o pode ser prevista de forma linear por $X_t$.
A proje√ß√£o linear tamb√©m possui uma propriedade importante que relaciona o MSE da proje√ß√£o linear √† vari√¢ncia de $Y_{t+1}$. A vari√¢ncia de $Y_{t+1}$ pode ser decomposta da seguinte forma [^3]:
$$Var(Y_{t+1}) = Var(\alpha'X_t) + E(Y_{t+1} - \alpha'X_t)^2$$

> üí° **Exemplo Num√©rico:**
> Voltando ao exemplo dos pre√ßos dos ativos financeiros e √≠ndices de mercado, vamos analisar o MSE da nossa proje√ß√£o linear.
> A proje√ß√£o linear calculada foi $\hat{Y}_{t+1} = 1.3169 X_t$. Podemos calcular os erros de previs√£o como $e_{t+1} = Y_{t+1} - \hat{Y}_{t+1}$:
>
> | t   | $X_t$ | $Y_{t+1}$ | $\hat{Y}_{t+1}$ | $e_{t+1}$ | $e_{t+1}^2$ |
> |-----|-------|-----------|-----------------|-----------|-------------|
> | 1   | 100   | 120       | 131.69          | -11.69     | 136.65      |
> | 2   | 105   | 130       | 138.28          | -8.28     | 68.55       |
> | 3   | 110   | 140       | 144.85          | -4.85     | 23.52       |
> | 4   | 115   | 155       | 151.44          | 3.56      | 12.67       |
> | 5   | 120   | 170       | 158.03          | 11.97     | 143.28      |
>
> Agora, calculamos o MSE como a m√©dia dos erros ao quadrado:
>
> $$MSE = \frac{136.65 + 68.55 + 23.52 + 12.67 + 143.28}{5} \approx 76.93$$
>
>  Usando a decomposi√ß√£o da vari√¢ncia,  podemos expressar o MSE como:
>
>  $$E(Y_{t+1} - \alpha'X_t)^2 = E(Y_{t+1}^2) - E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} E(X_tY_{t+1})$$
>
>
>  Podemos verificar essa decomposi√ß√£o utilizando o Python:
>
> ```python
> import numpy as np
>
> X = np.array([[100], [105], [110], [115], [120]])
> Y = np.array([120, 130, 140, 155, 170])
>
> XtX = X.T @ X
> XtY = X.T @ Y
> alpha = np.linalg.solve(XtX, XtY)
>
> # C√°lculo da m√©dia e vari√¢ncia de Y
> mean_Y = np.mean(Y)
> var_Y = np.mean((Y - mean_Y)**2)
>
> # C√°lculo dos termos da decomposi√ß√£o do MSE
> E_Y2 = np.mean(Y**2)
> E_XY = np.mean(X.flatten() * Y)
> E_X2 = np.mean(X**2)
>
> mse_calc = E_Y2 - (E_XY**2)/E_X2
>
> # C√°lculo do MSE direto
> predicted_Y = alpha * X
> residuals = Y - predicted_Y.flatten()
> mse = np.mean(residuals**2)
>
> print(f"MSE calculado diretamente: {mse:.4f}")
> print(f"MSE calculado com decomposi√ß√£o da vari√¢ncia: {mse_calc:.4f}")
> print(f"Vari√¢ncia de Y: {var_Y:.4f}")
>
> ```
>
> O c√≥digo acima calcular√° o MSE diretamente e usando a decomposi√ß√£o da vari√¢ncia, confirmando que ambos os c√°lculos produzem o mesmo resultado (aproximadamente 76.93), e tamb√©m calcular√° a vari√¢ncia total de $Y_{t+1}$, que √© maior que o MSE.
>
> Al√©m disso, a vari√¢ncia da proje√ß√£o linear $\hat{Y}_{t+1}$ pode ser calculada como $Var(\hat{Y}_{t+1}) = \alpha^2 Var(X_t)$.  Neste exemplo:
>
> ```python
> var_X = np.var(X)
> var_predicted_Y = alpha**2 * var_X
> print(f"Vari√¢ncia da proje√ß√£o linear: {var_predicted_Y[0]:.4f}")
> ```
> A vari√¢ncia da proje√ß√£o linear √© aproximadamente 228.80, e podemos verificar que a vari√¢ncia total de Y (280.6) √© igual √† soma da vari√¢ncia da proje√ß√£o linear e o MSE: 228.80 + 76.93 = 305.73 (com um pouco de erro devido a arredondamento)

**Teorema 1:** (Minimiza√ß√£o do MSE pela Proje√ß√£o Linear) *A proje√ß√£o linear* $P(Y_{t+1}|X_t) = \alpha'X_t$ *com* $\alpha' = E[Y_{t+1}X_t] [E(X_tX_t')]^{-1}$ *minimiza o erro quadr√°tico m√©dio (MSE) entre todas as fun√ß√µes lineares de* $X_t$.
    
   *Proof:*
   
   I. Seja $g(X_t)$ uma fun√ß√£o linear qualquer de $X_t$, e o MSE da proje√ß√£o com essa fun√ß√£o dado por $MSE_g = E[(Y_{t+1} - g(X_t))^2]$ e o MSE da proje√ß√£o linear por $MSE_{\alpha} =  E[(Y_{t+1} - \alpha'X_t)^2]$.
    
  II.  Podemos reescrever o MSE com $g(X_t)$ como:
     $$MSE_g = E[(Y_{t+1} - \alpha'X_t + \alpha'X_t - g(X_t))^2]$$
     
   III. Expandindo o quadrado:
    $$MSE_g = E[(Y_{t+1} - \alpha'X_t)^2] + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g(X_t))] + E[(\alpha'X_t - g(X_t))^2]$$
    
   IV. Defina $\eta_{t+1} = (Y_{t+1} - \alpha'X_t)(\alpha'X_t - g(X_t))$. Condicional em $X_t$, temos que $E(Y_{t+1} - \alpha'X_t|X_t)=0$, portanto:
   $$E[\eta_{t+1}|X_t] = E[(Y_{t+1} - \alpha'X_t)|X_t](\alpha'X_t - g(X_t)) = 0$$
   
  V.  Aplicando a lei da expectativa iterada, temos: $E[\eta_{t+1}]= E[E[\eta_{t+1}|X_t]] = 0$, e o MSE torna-se:
     $$MSE_g = MSE_{\alpha} + E[(\alpha'X_t - g(X_t))^2]$$
     
 VI. Como o termo $E[(\alpha'X_t - g(X_t))^2]$ √© sempre n√£o-negativo, o MSE √© minimizado quando este termo for igual a zero, o que ocorre apenas quando $g(X_t) = \alpha'X_t$.
  
 VII. Portanto, a proje√ß√£o linear  $P(Y_{t+1}|X_t) = \alpha'X_t$ minimiza o erro quadr√°tico m√©dio entre todas as fun√ß√µes lineares de $X_t$. $\blacksquare$
 
 **Teorema 1.1** (Decomposi√ß√£o da Vari√¢ncia): *A vari√¢ncia de* $Y_{t+1}$ *pode ser decomposta em duas partes: a vari√¢ncia da proje√ß√£o linear* $\alpha'X_t$ *e o MSE da proje√ß√£o linear, isto √©:*
    $$Var(Y_{t+1}) = Var(\alpha'X_t) + MSE[P(Y_{t+1}|X_t)]$$

*Proof:*
I. Sabemos que $Var(Y_{t+1}) = E[(Y_{t+1} - E[Y_{t+1}])^2]$.
II. Adicionando e subtraindo a proje√ß√£o linear, $Var(Y_{t+1}) = E[(Y_{t+1} - \alpha'X_t + \alpha'X_t - E[Y_{t+1}])^2]$.
III. Expandindo o quadrado:
$Var(Y_{t+1}) = E[(Y_{t+1} - \alpha'X_t)^2] + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - E[Y_{t+1}])] + E[(\alpha'X_t - E[Y_{t+1}])^2]$.
IV. O termo $E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - E[Y_{t+1}])]]$ √© igual a zero, dado que $E[(Y_{t+1} - \alpha'X_t)X_t] = 0$. Para ver isso, note que: $E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - E[Y_{t+1}])] = E[Y_{t+1}\alpha'X_t] - E[Y_{t+1}E[Y_{t+1}]] - E[\alpha'X_t\alpha'X_t] + E[\alpha'X_tE[Y_{t+1}]]$. Dado que $E[Y_{t+1} - \alpha'X_t] = E[Y_{t+1}] - E[\alpha'X_t]$, usando a propriedade da ortogonalidade (Lema 1), temos que $E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - E[Y_{t+1}])] = 0$.
V. O primeiro termo √© o MSE da proje√ß√£o linear: $E[(Y_{t+1} - \alpha'X_t)^2] = MSE[P(Y_{t+1}|X_t)]$.
VI. O terceiro termo √© a vari√¢ncia da proje√ß√£o linear: $E[(\alpha'X_t - E[Y_{t+1}])^2] = Var(\alpha'X_t)$.
VII. Portanto, $Var(Y_{t+1}) = Var(\alpha'X_t) + MSE[P(Y_{t+1}|X_t)]$. $\blacksquare$

### Conclus√£o
A proje√ß√£o linear de $Y_{t+1}$ sobre $X_t$ √© formalmente definida como $\alpha'X_t$, onde $\alpha'$ √© um vetor de coeficientes que garante a n√£o correla√ß√£o entre o erro de previs√£o e as vari√°veis explicativas. A condi√ß√£o $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$ √© uma consequ√™ncia da busca pela melhor aproxima√ß√£o linear de $Y_{t+1}$ em termos de $X_t$, no sentido de minimizar o MSE.  Essa formula√ß√£o permite derivar os coeficientes da proje√ß√£o linear e analisar suas propriedades, como a unicidade e a depend√™ncia dos momentos populacionais. Al√©m disso, a decomposi√ß√£o do MSE em termos da vari√¢ncia de $Y_{t+1}$ e da vari√¢ncia explicada pela proje√ß√£o linear destaca o papel central deste m√©todo na constru√ß√£o de previs√µes. A equival√™ncia do MSE da proje√ß√£o linear com o MSE da expectativa condicional linear demonstra a import√¢ncia te√≥rica e pr√°tica desta ferramenta para a an√°lise de s√©ries temporais e previs√£o.

### Refer√™ncias
[^1]: Expression [4.1.1], [4.1.2], [4.1.3], [4.1.4], [4.1.5], [4.1.6], [4.1.7], [4.1.8]
[^2]: [4.1.9], [4.1.10], [4.1.11], [4.1.12], [4.1.13]
[^3]: [4.1.14], [4.1.15]
<!-- END -->
