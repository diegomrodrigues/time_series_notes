## ProjeÃ§Ã£o Linear e o Erro QuadrÃ¡tico MÃ©dio Ã“timo
### IntroduÃ§Ã£o
Neste capÃ­tulo, exploramos a **projeÃ§Ã£o linear** como uma ferramenta fundamental na anÃ¡lise de sÃ©ries temporais e na construÃ§Ã£o de previsÃµes. A projeÃ§Ã£o linear de $Y_{t+1}$ sobre $X_t$, denotada como $P(Y_{t+1}|X_t) = \alpha'X_t$, busca minimizar o erro quadrÃ¡tico mÃ©dio (MSE) dentro da classe de previsÃµes lineares [^2]. A condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o entre o erro de previsÃ£o e as variÃ¡veis explicativas, $E[(Y_{t+1} - \alpha'X_t)X_t] = 0$, garante a otimalidade desta projeÃ§Ã£o dentro do espaÃ§o linear [^2]. Este mÃ©todo, embora mais restritivo do que usar a expectativa condicional, apresenta uma soluÃ§Ã£o prÃ¡tica e computacionalmente eficiente.

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Suponha que temos dados de vendas ($Y_{t+1}$) e gastos com publicidade ($X_t$) ao longo de vÃ¡rios perÃ­odos. Para simplificar, vamos considerar que $Y_{t+1}$ Ã© o total de vendas no perÃ­odo $t+1$ e $X_t$ Ã© o gasto com publicidade no perÃ­odo $t$ (em milhares de unidades monetÃ¡rias). Temos os seguintes dados simulados:
>
> | t   | $X_t$ (Publicidade) | $Y_{t+1}$ (Vendas) |
> |-----|---------------------|-------------------|
> | 1   | 2                   | 5                 |
> | 2   | 3                   | 7                 |
> | 3   | 4                   | 9                 |
> | 4   | 5                   | 10                |
> | 5   | 6                   | 12                |
>
>
> Nosso objetivo Ã© encontrar a melhor projeÃ§Ã£o linear de $Y_{t+1}$ em $X_t$, ou seja, encontrar $\alpha$ tal que $P(Y_{t+1}|X_t) = \alpha X_t$. Para isso, vamos calcular $\alpha$ usando a fÃ³rmula:
> $$\alpha = \frac{E[Y_{t+1}X_t]}{E[X_t^2]}$$
>
> Primeiro, calculamos as mÃ©dias e os momentos:
>
> $\text{MÃ©dia de } X_t$: $\bar{X} = \frac{2 + 3 + 4 + 5 + 6}{5} = 4$
>
> $\text{MÃ©dia de } Y_{t+1}$: $\bar{Y} = \frac{5 + 7 + 9 + 10 + 12}{5} = 8.6$
>
> $E[X_t^2] = \frac{2^2 + 3^2 + 4^2 + 5^2 + 6^2}{5} = \frac{4 + 9 + 16 + 25 + 36}{5} = \frac{90}{5} = 18$
>
> $E[Y_{t+1}X_t] = \frac{(2 \times 5) + (3 \times 7) + (4 \times 9) + (5 \times 10) + (6 \times 12)}{5} = \frac{10 + 21 + 36 + 50 + 72}{5} = \frac{189}{5} = 37.8$
>
> Agora, podemos calcular $\alpha$:
>
> $$\alpha = \frac{37.8}{18} = 2.1$$
>
> Portanto, a projeÃ§Ã£o linear de $Y_{t+1}$ em $X_t$ Ã© dada por $P(Y_{t+1}|X_t) = 2.1 X_t$.
>
> Isso significa que, para cada unidade de gasto com publicidade ($X_t$), esperamos um aumento de 2.1 unidades nas vendas ($Y_{t+1}$).

### Conceitos Fundamentais
A notaÃ§Ã£o $P(Y_{t+1}|X_t) = \alpha'X_t$ representa a **projeÃ§Ã£o linear de $Y_{t+1}$ em $X_t$**, onde $\alpha$ Ã© um vetor de coeficientes que minimiza o MSE da previsÃ£o [^2]. Como vimos na seÃ§Ã£o anterior, o erro de previsÃ£o Ã© definido como $Y_{t+1} - \alpha'X_t$, e a condiÃ§Ã£o $E[(Y_{t+1} - \alpha'X_t)X_t] = 0$ implica que este erro Ã© nÃ£o correlacionado com as variÃ¡veis explicativas $X_t$. Ã‰ essencial notar que essa condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o Ã© fundamental para a otimalidade da projeÃ§Ã£o linear [^2].

> ğŸ’¡ **RelaÃ§Ã£o com a Expectativa Condicional:**
> Embora a expectativa condicional $E(Y_{t+1}|X_t)$ represente a previsÃ£o que minimiza o MSE para todas as funÃ§Ãµes de $X_t$, a projeÃ§Ã£o linear restringe a previsÃ£o a uma funÃ§Ã£o linear de $X_t$, ou seja, $\alpha'X_t$. No entanto, Ã© um resultado notÃ¡vel que *o MSE da projeÃ§Ã£o linear Ã³tima* $P(Y_{t+1}|X_t)$ *Ã© igual ao MSE da expectativa condicional* $E(Y_{t+1}|X_t)$ *quando a expectativa condicional Ã© uma funÃ§Ã£o linear de $X_t$*, o que geralmente Ã© o caso em modelos gaussianos.

A condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o  $E[(Y_{t+1} - \alpha'X_t)X_t] = 0$ pode ser reescrita como:
$$E[Y_{t+1}X_t] = \alpha' E[X_tX_t']$$
Resolvendo para $\alpha'$, obtemos os coeficientes da projeÃ§Ã£o linear:
$$\alpha' = E[Y_{t+1}X_t] [E(X_tX_t')]^{-1}$$
Essa fÃ³rmula expressa os coeficientes da projeÃ§Ã£o linear em termos dos momentos populacionais de $Y_{t+1}$ e $X_t$ [^2]. A projeÃ§Ã£o linear Ã© a melhor previsÃ£o linear possÃ­vel no sentido de que ela minimiza o MSE dentro da classe de funÃ§Ãµes lineares das variÃ¡veis explicativas [^2].

> ğŸ’¡ **Exemplo NumÃ©rico (Multivariado):**
>  Vamos expandir o exemplo anterior para incluir outra variÃ¡vel, digamos, o Ã­ndice de confianÃ§a do consumidor ($Z_t$). Agora temos duas variÃ¡veis explicativas $X_t$ (publicidade) e $Z_t$ (confianÃ§a do consumidor).
>
> | t   | $X_t$ | $Z_t$ | $Y_{t+1}$ |
> |-----|-------|-------|-----------|
> | 1   | 2     | 10    | 5         |
> | 2   | 3     | 12    | 7         |
> | 3   | 4     | 11    | 9         |
> | 4   | 5     | 13    | 10        |
> | 5   | 6     | 15    | 12        |
>
> Queremos encontrar $\alpha = [\alpha_1, \alpha_2]$ tal que $P(Y_{t+1}|X_t, Z_t) = \alpha_1 X_t + \alpha_2 Z_t$.
>
> Primeiro, vamos construir as matrizes $X$ (variÃ¡veis explicativas) e $Y$ (variÃ¡vel dependente):
> $$ X = \begin{bmatrix} 2 & 10 \\ 3 & 12 \\ 4 & 11 \\ 5 & 13 \\ 6 & 15 \end{bmatrix} \quad Y = \begin{bmatrix} 5 \\ 7 \\ 9 \\ 10 \\ 12 \end{bmatrix}$$
>
> Usamos a fÃ³rmula $\alpha = (X^T X)^{-1} X^T Y$
>
> Calculamos $X^T X$ :
>$$ X^T X = \begin{bmatrix} 2 & 3 & 4 & 5 & 6 \\ 10 & 12 & 11 & 13 & 15 \end{bmatrix} \begin{bmatrix} 2 & 10 \\ 3 & 12 \\ 4 & 11 \\ 5 & 13 \\ 6 & 15 \end{bmatrix} = \begin{bmatrix} 90 & 483 \\ 483 & 2679 \end{bmatrix} $$
>
> Calculamos $(X^T X)^{-1}$:
>
> $$ (X^T X)^{-1} = \frac{1}{(90*2679 - 483*483)} \begin{bmatrix} 2679 & -483 \\ -483 & 90 \end{bmatrix} \approx \begin{bmatrix} 0.469 & -0.085 \\ -0.085 & 0.016 \end{bmatrix} $$
>
> Calculamos $X^T Y$:
>$$ X^T Y = \begin{bmatrix} 2 & 3 & 4 & 5 & 6 \\ 10 & 12 & 11 & 13 & 15 \end{bmatrix} \begin{bmatrix} 5 \\ 7 \\ 9 \\ 10 \\ 12 \end{bmatrix} = \begin{bmatrix} 189 \\ 1047 \end{bmatrix} $$
>
> Agora, calculamos $\alpha$:
> $$\alpha = (X^T X)^{-1} X^T Y = \begin{bmatrix} 0.469 & -0.085 \\ -0.085 & 0.016 \end{bmatrix} \begin{bmatrix} 189 \\ 1047 \end{bmatrix} \approx \begin{bmatrix} 0.032 \\ 0.028 \end{bmatrix}$$
>
> Assim, a projeÃ§Ã£o linear Ã© aproximadamente $P(Y_{t+1}|X_t, Z_t) = 0.032 X_t + 0.028 Z_t$. Isso indica que tanto a publicidade quanto a confianÃ§a do consumidor tÃªm um efeito positivo nas vendas, mas a publicidade tem um efeito ligeiramente maior.

Uma propriedade fundamental da projeÃ§Ã£o linear Ã© que o erro da projeÃ§Ã£o, $Y_{t+1} - \alpha'X_t$, Ã© nÃ£o correlacionado com qualquer combinaÃ§Ã£o linear de $X_t$. Ou seja, para qualquer vetor $c$, temos:
$$E[(Y_{t+1} - \alpha'X_t)c'X_t] = 0$$
Esta propriedade, conhecida como **propriedade de ortogonalidade**, Ã© crucial para entender a otimalidade da projeÃ§Ã£o linear [^2].

> ğŸ’¡ **MSE da ProjeÃ§Ã£o Linear Ã“tima:**
> O MSE da projeÃ§Ã£o linear Ã³tima Ã© dado por:
$$MSE[P(Y_{t+1}|X_t)] = E[Y_{t+1} - \alpha'X_t]^2 = E[Y_{t+1}^2] - E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1} E[X_tY_{t+1}]$$
Essa expressÃ£o revela que o MSE da projeÃ§Ã£o linear Ã© dado pela variÃ¢ncia de $Y_{t+1}$ menos um termo que representa a variÃ¢ncia explicada pela projeÃ§Ã£o linear sobre $X_t$. Quando a expectativa condicional $E(Y_{t+1}|X_t)$ Ã© uma funÃ§Ã£o linear de $X_t$, este MSE Ã© igual ao MSE da previsÃ£o com base na expectativa condicional, o que confirma a otimalidade da projeÃ§Ã£o linear.
>
> ğŸ’¡ **Exemplo NumÃ©rico (MSE):**
>
> Utilizando o primeiro exemplo numÃ©rico (univariado) com $\alpha=2.1$ , vamos calcular o MSE.
>
> Os valores previstos $\hat{Y}_{t+1}$ sÃ£o dados por $2.1X_t$:
>
> | t   | $X_t$ | $Y_{t+1}$ | $\hat{Y}_{t+1}$ | $Y_{t+1} - \hat{Y}_{t+1}$ |$(Y_{t+1} - \hat{Y}_{t+1})^2$|
> |-----|-------|-----------|------------------|-------------------------|-----------------------------|
> | 1   | 2     | 5         | 4.2               | 0.8                     | 0.64                        |
> | 2   | 3     | 7         | 6.3               | 0.7                     | 0.49                        |
> | 3   | 4     | 9         | 8.4               | 0.6                     | 0.36                        |
> | 4   | 5     | 10        | 10.5              | -0.5                    | 0.25                        |
> | 5   | 6     | 12        | 12.6              | -0.6                    | 0.36                        |
>
> O MSE Ã© a mÃ©dia dos erros ao quadrado:
>
> $$MSE = \frac{0.64 + 0.49 + 0.36 + 0.25 + 0.36}{5} = \frac{2.1}{5} = 0.42$$
>
> Para calcular o MSE usando a fÃ³rmula $MSE = E[Y_{t+1}^2] - E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1} E[X_tY_{t+1}]$, temos:
>
> $E[Y_{t+1}^2] = (5^2 + 7^2 + 9^2 + 10^2 + 12^2)/5 = (25 + 49 + 81 + 100 + 144)/5 = 399/5 = 79.8$
>
> Usamos os valores previamente calculados: $E[X_tX_t'] = 18$ e $E[X_tY_{t+1}] = 37.8$. EntÃ£o:
>
> $MSE = 79.8 - \frac{37.8^2}{18} = 79.8 - \frac{1428.84}{18} = 79.8 - 79.38 = 0.42$, o que confirma nosso cÃ¡lculo anterior.

> ğŸ’¡ **ComparaÃ§Ã£o com a Expectativa Condicional:**
> Ã‰ importante notar que a projeÃ§Ã£o linear Ã© uma ferramenta flexÃ­vel e poderosa, mesmo quando a expectativa condicional nÃ£o Ã© uma funÃ§Ã£o linear das variÃ¡veis explicativas. Em tais casos, a projeÃ§Ã£o linear oferece a melhor previsÃ£o linear possÃ­vel e se torna uma aproximaÃ§Ã£o Ãºtil e tratÃ¡vel da expectativa condicional. A diferenÃ§a entre a projeÃ§Ã£o linear e a expectativa condicional reside na restriÃ§Ã£o Ã  linearidade imposta pela primeira.

Em muitas aplicaÃ§Ãµes, a projeÃ§Ã£o linear Ã© estendida para incluir um termo constante, expressa como $\hat{E}(Y_{t+1}|X_t) = P(Y_{t+1}|1, X_t)$. Esta inclusÃ£o permite que o modelo acomode um nÃ­vel base para o valor de $Y_{t+1}$, alÃ©m da variaÃ§Ã£o linear em funÃ§Ã£o de $X_t$. A projeÃ§Ã£o linear com um termo constante Ã© dada por:
$$P(Y_{t+1}|1, X_t) = \beta_0 + \beta'X_t$$
onde
$$\beta' = Cov(Y_{t+1}, X_t) [Var(X_t)]^{-1}$$
e
$$\beta_0 = E(Y_{t+1}) - \beta'E(X_t)$$
Essa formulaÃ§Ã£o Ã© especialmente Ãºtil em cenÃ¡rios onde o valor mÃ©dio de $Y_{t+1}$ Ã© diferente de zero, ou quando a projeÃ§Ã£o linear deve interceptar o eixo vertical em um valor diferente de zero.

> ğŸ’¡ **Exemplo NumÃ©rico (ProjeÃ§Ã£o Linear com Constante):**
>  Vamos recalcular o primeiro exemplo com a adiÃ§Ã£o de uma constante.
> Temos os dados:
>
> | t   | $X_t$ | $Y_{t+1}$ |
> |-----|-------|-----------|
> | 1   | 2     | 5         |
> | 2   | 3     | 7         |
> | 3   | 4     | 9         |
> | 4   | 5     | 10        |
> | 5   | 6     | 12        |
>
> Calculamos as mÃ©dias:
>
> $\bar{X} = 4$
>
> $\bar{Y} = 8.6$
>
> Calculamos a variÃ¢ncia de $X_t$:
>
> $Var(X_t) = E[X_t^2] - (E[X_t])^2 = 18 - 4^2 = 18 - 16 = 2$
>
> Calculamos a covariÃ¢ncia entre $Y_{t+1}$ e $X_t$:
>
> $Cov(Y_{t+1}, X_t) = E[Y_{t+1}X_t] - E[Y_{t+1}]E[X_t] = 37.8 - 8.6 \times 4 = 37.8 - 34.4 = 3.4$
>
> Calculamos $\beta'$:
>
> $\beta' = \frac{Cov(Y_{t+1}, X_t)}{Var(X_t)} = \frac{3.4}{2} = 1.7$
>
> Calculamos $\beta_0$:
>
> $\beta_0 = E(Y_{t+1}) - \beta'E(X_t) = 8.6 - 1.7 \times 4 = 8.6 - 6.8 = 1.8$
>
> Portanto, a projeÃ§Ã£o linear com constante Ã© $P(Y_{t+1}|1, X_t) = 1.8 + 1.7 X_t$.
>
> Notemos que o coeficiente angular aqui (1.7) difere do resultado obtido anteriormente (2.1) porque agora estamos permitindo um valor constante (1.8) como intercepto. Isso melhora o ajuste do modelo aos dados, pois agora podemos modelar melhor a relaÃ§Ã£o entre $X_t$ e $Y_{t+1}$.

**Lema 1** (DecomposiÃ§Ã£o do Erro QuadrÃ¡tico MÃ©dio): O MSE da variÃ¡vel aleatÃ³ria $Y_{t+1}$ pode ser decomposto na soma do MSE da projeÃ§Ã£o linear e da variÃ¢ncia do erro da projeÃ§Ã£o.

*Prova*:
I.  Seja $e_{t+1} = Y_{t+1} - P(Y_{t+1}|X_t)$ o erro da projeÃ§Ã£o.
II.  Pela propriedade de ortogonalidade,  $E[e_{t+1}P(Y_{t+1}|X_t)] = 0$.
III. O MSE de $Y_{t+1}$ Ã© $E[(Y_{t+1} - E(Y_{t+1}))^2] = Var(Y_{t+1})$.
IV.  Podemos escrever $Y_{t+1} - E(Y_{t+1}) =  (Y_{t+1} - P(Y_{t+1}|X_t)) + (P(Y_{t+1}|X_t)-E(Y_{t+1})) = e_{t+1} + (P(Y_{t+1}|X_t)-E(Y_{t+1}))$.
V.  Elevando ao quadrado e tomando a esperanÃ§a: $Var(Y_{t+1}) = E[e_{t+1}^2] + E[(P(Y_{t+1}|X_t)-E(Y_{t+1}))^2]  + 2E[e_{t+1}(P(Y_{t+1}|X_t)-E(Y_{t+1}))]$.
VI. O Ãºltimo termo Ã© zero pela propriedade de ortogonalidade, como explicitado em II, $E[e_{t+1}P(Y_{t+1}|X_t)]=E[e_{t+1}E(P(Y_{t+1}|X_t))]=0$.
VII. Assim, $Var(Y_{t+1}) = E[e_{t+1}^2] + E[(P(Y_{t+1}|X_t)-E(Y_{t+1}))^2] = MSE[P(Y_{t+1}|X_t)] + Var(e_{t+1})$.
VIII. Portanto,  $MSE[Y_{t+1}] = MSE[P(Y_{t+1}|X_t)] + Var(Y_{t+1} - P(Y_{t+1}|X_t))$.â– 

**Lema 2** (MSE Ã“timo e Expectativa Condicional Linear): *Se a expectativa condicional* $E(Y_{t+1}|X_t)$ *Ã© uma funÃ§Ã£o linear de* $X_t$, *entÃ£o o MSE da projeÃ§Ã£o linear* $P(Y_{t+1}|X_t)$ *Ã© igual ao MSE da expectativa condicional* $E(Y_{t+1}|X_t)$.

*Prova*:
I.  Seja $E(Y_{t+1}|X_t) = a'X_t$, onde $a$ Ã© um vetor de coeficientes.
II.  Como a projeÃ§Ã£o linear Ã© a melhor previsÃ£o linear, ela deve satisfazer  $E[(Y_{t+1} - \alpha'X_t)X_t]=0$.
III. O MSE da expectativa condicional linear Ã© $E[(Y_{t+1}-a'X_t)^2]$, e o MSE da projeÃ§Ã£o linear Ã© $E[(Y_{t+1} - \alpha'X_t)^2]$.
IV.  Podemos escrever $Y_{t+1} = a'X_t + \eta_{t+1}$ onde $\eta_{t+1} = Y_{t+1} - a'X_t$  e $E(\eta_{t+1}|X_t)=0$.
V.  EntÃ£o o MSE da expectativa condicional linear Ã© $E(\eta_{t+1}^2)$.
VI. Por outro lado, podemos escrever $Y_{t+1} = \alpha'X_t + e_{t+1}$ onde $e_{t+1} = Y_{t+1} - \alpha'X_t$.
VII. Substituindo $Y_{t+1}$ na expressÃ£o para $e_{t+1}$, temos: $e_{t+1} =  a'X_t + \eta_{t+1} - \alpha'X_t$.
VIII. Como $E[(Y_{t+1} - \alpha'X_t)X_t]=0$, temos $E[e_{t+1} X_t] = 0$ ou $E[( a'X_t + \eta_{t+1} - \alpha'X_t)X_t] = 0$.
IX. Logo $E[(a'X_t  - \alpha'X_t)X_t] + E[\eta_{t+1}X_t]= 0$, e como $E[\eta_{t+1}X_t]=0$ , temos  $E[(a'-\alpha')X_t X_t']= 0$
X.  Portanto,  $a=\alpha$ e o MSE da projeÃ§Ã£o linear Ã© $E(e_{t+1}^2)=E(\eta_{t+1}^2)$.  Assim, os MSE sÃ£o iguais.â– 

**Teorema 1** (Unicidade da ProjeÃ§Ã£o Linear): A projeÃ§Ã£o linear $P(Y_{t+1}|X_t)$ que satisfaz $E[(Y_{t+1} - P(Y_{t+1}|X_t))X_t]=0$ Ã© Ãºnica.

*Prova*:
I. Suponha que existam duas projeÃ§Ãµes lineares $P_1(Y_{t+1}|X_t) = \alpha_1'X_t$ e $P_2(Y_{t+1}|X_t) = \alpha_2'X_t$  que satisfazem a condiÃ§Ã£o de ortogonalidade.
II. EntÃ£o, temos  $E[(Y_{t+1} - \alpha_1'X_t)X_t] = 0$ e $E[(Y_{t+1} - \alpha_2'X_t)X_t] = 0$.
III. Subtraindo as duas equaÃ§Ãµes,  $E[(\alpha_2'X_t - \alpha_1'X_t)X_t] = 0$.
IV. Isso implica que  $E[(\alpha_2' - \alpha_1')X_tX_t'] = (\alpha_2' - \alpha_1')E[X_tX_t']=0$.
V.  Se $E[X_tX_t']$ Ã© nÃ£o singular, entÃ£o $(\alpha_2' - \alpha_1')=0$, o que implica  $\alpha_1 = \alpha_2$
VI.  Portanto, $P_1(Y_{t+1}|X_t) = P_2(Y_{t+1}|X_t)$, e a projeÃ§Ã£o linear Ã© Ãºnica.â– 

### ConclusÃ£o
A notaÃ§Ã£o $P(Y_{t+1}|X_t) = \alpha'X_t$ resume o conceito de projeÃ§Ã£o linear, um mÃ©todo crucial para construir previsÃµes de sÃ©ries temporais. A projeÃ§Ã£o linear, ao buscar a melhor aproximaÃ§Ã£o linear para $Y_{t+1}$ em termos de $X_t$, garante que o erro da projeÃ§Ã£o seja nÃ£o correlacionado com $X_t$, uma propriedade de otimalidade dentro da classe linear. AlÃ©m disso, em modelos onde a expectativa condicional Ã© linear, a projeÃ§Ã£o linear produz um resultado tÃ£o bom quanto o da expectativa condicional, simplificando o cÃ¡lculo da previsÃ£o sem perda de precisÃ£o. As propriedades do MSE da projeÃ§Ã£o linear e a sua equivalÃªncia com o MSE da expectativa condicional linear demonstram a importÃ¢ncia teÃ³rica e prÃ¡tica desta ferramenta para a anÃ¡lise de sÃ©ries temporais e previsÃ£o.
### ReferÃªncias
[^1]: Expression [4.1.1], [4.1.2], [4.1.3], [4.1.4], [4.1.5], [4.1.6], [4.1.7], [4.1.8]
[^2]: [4.1.9], [4.1.10], [4.1.11], [4.1.12], [4.1.13]
[^3]: [4.1.14], [4.1.15]
<!-- END -->
