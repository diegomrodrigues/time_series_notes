## A Proje√ß√£o Linear e a Minimiza√ß√£o do Erro Quadr√°tico M√©dio
### Introdu√ß√£o
Este cap√≠tulo explora em profundidade a **proje√ß√£o linear**, um m√©todo fundamental para a constru√ß√£o de previs√µes em s√©ries temporais. Construindo sobre os conceitos introduzidos anteriormente [^1], a proje√ß√£o linear visa encontrar uma fun√ß√£o linear das vari√°veis explicativas $X_t$ que melhor se aproxima da vari√°vel dependente $Y_{t+1}$, no sentido de minimizar o erro quadr√°tico m√©dio (MSE). Uma caracter√≠stica central deste m√©todo √© a garantia de que o erro de previs√£o seja n√£o correlacionado com as vari√°veis explicativas [^2]. Vamos aprofundar o significado dessa condi√ß√£o e sua rela√ß√£o com a otimalidade da proje√ß√£o linear.

### Conceitos Fundamentais
A proje√ß√£o linear de $Y_{t+1}$ sobre $X_t$ √© expressa como $P(Y_{t+1}|X_t) = \alpha'X_t$, onde $\alpha$ √© um vetor de coeficientes a serem determinados. O objetivo √© encontrar $\alpha$ que minimize o MSE da previs√£o, que √© dado por $E[(Y_{t+1} - \alpha'X_t)^2]$ [^1]. A condi√ß√£o fundamental para que $\alpha'X_t$ seja a melhor previs√£o linear √© que o erro de previs√£o, $Y_{t+1} - \alpha'X_t$, seja n√£o correlacionado com as vari√°veis explicativas $X_t$ [^2]. Essa condi√ß√£o √© expressa formalmente como:
$$E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$$
Essa rela√ß√£o, tamb√©m conhecida como **condi√ß√£o de ortogonalidade**, garante que a proje√ß√£o linear capture toda a informa√ß√£o linearmente relevante presente em $X_t$ para prever $Y_{t+1}$. Ela implica que o erro de previs√£o n√£o possui qualquer componente que possa ser sistematicamente explicado pelas vari√°veis explicativas $X_t$ [^2].

> üí° **Exemplo Num√©rico:**
> Imagine que estamos prevendo o consumo de energia el√©trica ($Y_{t+1}$) com base na temperatura m√©dia di√°ria ($X_t$).  Temos alguns dados de um per√≠odo curto:
>
> | Dia  | Temperatura ($X_t$) | Consumo ($Y_{t+1}$) |
> |------|---------------------|--------------------|
> | 1    | 20                  | 100                |
> | 2    | 25                  | 120                |
> | 3    | 30                  | 150                |
> | 4    | 35                  | 180                |
> | 5    | 28                  | 135                |
>
> Podemos expressar isso como vetores:
> $$ X_t = \begin{bmatrix} 20 \\ 25 \\ 30 \\ 35 \\ 28 \end{bmatrix}, \quad Y_{t+1} = \begin{bmatrix} 100 \\ 120 \\ 150 \\ 180 \\ 135 \end{bmatrix} $$
> Neste cen√°rio, a proje√ß√£o linear seria $P(Y_{t+1}|X_t) = \alpha X_t$. Para determinar $\alpha$ , precisamos da condi√ß√£o de n√£o correla√ß√£o $E[(Y_{t+1} - \alpha X_t)X_t] = 0$. Isso garante que o erro, $Y_{t+1} - \alpha X_t$, n√£o tenha nenhuma rela√ß√£o linear com a temperatura $X_t$.
>
> Vamos calcular o valor de $\alpha$ usando os dados amostrais. Primeiro, criamos uma matriz $X$ que cont√©m as temperaturas e um vetor $Y$ com os consumos:
>
> ```python
> import numpy as np
>
> X = np.array([[20], [25], [30], [35], [28]])
> Y = np.array([100, 120, 150, 180, 135])
> ```
>
> Agora, calculamos $\alpha$ usando a f√≥rmula $\alpha = (X^T X)^{-1} X^T Y$:
>
> ```python
> XtX = X.T @ X
> XtY = X.T @ Y
> alpha = np.linalg.solve(XtX, XtY)
> print(f"O valor de alpha √©: {alpha[0]:.2f}")
> ```
>
> O c√≥digo acima calcular√° um valor de $\alpha$ de aproximadamente 4.97. Isso significa que a cada grau de aumento na temperatura, o consumo de energia aumenta em cerca de 4.97 unidades.
>
> A proje√ß√£o linear √©, ent√£o, expressa como $\hat{Y}_{t+1} = 4.97 X_t$. Por exemplo, se a temperatura em um novo dia for 32 graus, o consumo de energia previsto seria $\hat{Y}_{t+1} = 4.97 \times 32 \approx 159.04$.
>
>  Visualizando essa rela√ß√£o, temos o seguinte gr√°fico:
>
> ```mermaid
> graph LR
>     A[Temperatura (X_t)] --> B(Consumo Previsto (Y_hat));
>     style A fill:#f9f,stroke:#333,stroke-width:2px
>     style B fill:#ccf,stroke:#333,stroke-width:2px
> ```
>
> Onde a temperatura $X_t$ influencia linearmente o consumo previsto $\hat{Y}_{t+1}$. A condi√ß√£o de n√£o correla√ß√£o garante que qualquer erro entre o consumo real e o consumo previsto n√£o seja explicado pela temperatura.

Para demonstrar a otimalidade da proje√ß√£o linear, suponha que utilizemos uma fun√ß√£o arbitr√°ria $g(X_t)$ para prever $Y_{t+1}$, resultando em um erro de previs√£o $Y_{t+1} - g(X_t)$.  O MSE desta previs√£o seria [^1]:
$$E[(Y_{t+1} - g(X_t))^2] = E[(Y_{t+1} - \alpha'X_t + \alpha'X_t - g(X_t))^2]$$
Expandindo o termo do lado direito, temos:
$$E[(Y_{t+1} - \alpha'X_t)^2] + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g(X_t))] + E[(\alpha'X_t - g(X_t))^2]$$
A chave para demonstrar a otimalidade √© que o termo do meio √© zero.  Vamos definir $\eta_{t+1} = [Y_{t+1} - \alpha'X_t][\alpha'X_t - g(X_t)]$. Condicional em $X_t$, os termos $\alpha'X_t$ e $g(X_t)$ s√£o constantes, permitindo que sejam fatorados da expectativa [^1].  Al√©m disso, a condi√ß√£o de ortogonalidade implica que $E(Y_{t+1}-\alpha'X_t|X_t)=0$. Assim:
$$E[\eta_{t+1}|X_t] = E[(Y_{t+1} - \alpha'X_t)|X_t][\alpha'X_t - g(X_t)] = 0$$
Utilizando a lei da expectativa iterada, temos que $E[\eta_{t+1}] = E_{X_t}[E[\eta_{t+1}|X_t]] = 0$.  Substituindo de volta na equa√ß√£o do MSE, obtemos:
$$E[(Y_{t+1} - g(X_t))^2] = E[(Y_{t+1} - \alpha'X_t)^2] + E[(\alpha'X_t - g(X_t))^2]$$
Como o segundo termo √© sempre n√£o negativo, o MSE √© minimizado quando $E[(\alpha'X_t - g(X_t))^2]=0$, que ocorre quando $g(X_t) = \alpha'X_t$.  Isso demonstra que a proje√ß√£o linear minimiza o erro dentro da classe de previs√µes lineares.
  
  *Prova da Otimidade da Proje√ß√£o Linear:*
  
  I. Come√ßamos com o erro quadr√°tico m√©dio (MSE) da previs√£o utilizando uma fun√ß√£o arbitr√°ria $g(X_t)$:
     $$MSE = E[(Y_{t+1} - g(X_t))^2]$$
     
  II. Adicionamos e subtra√≠mos $\alpha'X_t$ dentro do par√™nteses:
    $$MSE = E[(Y_{t+1} - \alpha'X_t + \alpha'X_t - g(X_t))^2]$$

 III. Expandindo o quadrado, obtemos:
     $$MSE = E[(Y_{t+1} - \alpha'X_t)^2] + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g(X_t))] + E[(\alpha'X_t - g(X_t))^2]$$

 IV. Definimos $\eta_{t+1} = [Y_{t+1} - \alpha'X_t][\alpha'X_t - g(X_t)]$ e analisamos o segundo termo da equa√ß√£o do MSE, utilizando a expectativa condicional em rela√ß√£o a $X_t$:
  $$E[\eta_{t+1}|X_t] = E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g(X_t))|X_t] = E[(Y_{t+1} - \alpha'X_t)|X_t](\alpha'X_t - g(X_t))$$
  
  V. Pela condi√ß√£o de ortogonalidade, $E[(Y_{t+1} - \alpha'X_t)|X_t] = 0$, ent√£o:
    $$E[\eta_{t+1}|X_t] = 0 \cdot (\alpha'X_t - g(X_t)) = 0$$
  
  VI. Aplicando a lei da expectativa iterada, temos que:
    $$E[\eta_{t+1}] = E_{X_t}[E[\eta_{t+1}|X_t]] = E_{X_t}[0] = 0$$
  
 VII. Substituindo de volta na equa√ß√£o do MSE:
    $$MSE = E[(Y_{t+1} - \alpha'X_t)^2] + E[(\alpha'X_t - g(X_t))^2]$$
  
  VIII. O termo $E[(\alpha'X_t - g(X_t))^2]$ √© sempre n√£o-negativo. Assim, o MSE √© minimizado quando esse termo √© igual a zero, o que ocorre quando $\alpha'X_t = g(X_t)$, portanto a proje√ß√£o linear minimiza o erro quadr√°tico m√©dio dentro da classe de previs√µes lineares. $\blacksquare$

> üí° **Observa√ß√£o:** √â fundamental notar que, embora a proje√ß√£o linear minimize o MSE dentro da classe de fun√ß√µes lineares de $X_t$, a proje√ß√£o linear n√£o √© a melhor previs√£o poss√≠vel quando avaliamos todas as fun√ß√µes de $X_t$. Em geral, a previs√£o √≥tima em termos de MSE √© dada pela expectativa condicional $E(Y_{t+1}|X_t)$ [^1]. No entanto, o c√°lculo da expectativa condicional pode ser complexo ou exigir informa√ß√µes que n√£o est√£o dispon√≠veis. A proje√ß√£o linear oferece uma alternativa mais simples e pr√°tica, embora restrita a fun√ß√µes lineares. √â um resultado not√°vel que, quando a expectativa condicional √© linear, a proje√ß√£o linear oferece o mesmo MSE da previs√£o da expectativa condicional [^2].

Para encontrar $\alpha$, come√ßamos com a condi√ß√£o de n√£o correla√ß√£o:
$$E[(Y_{t+1} - \alpha'X_t)X_t] = E[Y_{t+1}X_t] - \alpha'E[X_tX_t'] = 0'$$
Resolvendo para $\alpha'$, obtemos a f√≥rmula dos coeficientes da proje√ß√£o linear:
$$\alpha' = E[Y_{t+1}X_t] [E(X_tX_t')]^{-1}$$
Essa express√£o, como vimos anteriormente, relaciona os coeficientes da proje√ß√£o linear com os momentos populacionais de $Y_{t+1}$ e $X_t$ [^2].

> üí° **Exemplo Num√©rico (Continua√ß√£o):**
> Retomando o exemplo do consumo de energia e temperatura, vamos calcular $\alpha$ usando os momentos amostrais:
>
> Primeiro, calculamos $E[X_tX_t']$:
> $$E[X_tX_t'] \approx \frac{20^2 + 25^2 + 30^2 + 35^2 + 28^2}{5} = \frac{400 + 625 + 900 + 1225 + 784}{5} = \frac{3934}{5} = 786.8$$
>
> E calculamos $E[Y_{t+1}X_t]$:
> $$E[Y_{t+1}X_t] \approx \frac{20\cdot100 + 25\cdot120 + 30\cdot150 + 35\cdot180 + 28\cdot135}{5} = \frac{2000 + 3000 + 4500 + 6300 + 3780}{5} = \frac{19580}{5} = 3916$$
>
> Agora, calculamos $\alpha$:
>
> $$\alpha = \frac{3916}{786.8} \approx 4.97$$
>
> Portanto, a proje√ß√£o linear seria $P(Y_{t+1}|X_t) = 4.97X_t$, indicando que, a cada grau a mais na temperatura, esperamos um aumento de 4.97 unidades no consumo de energia.
>
> Vamos analisar os erros da nossa proje√ß√£o. Primeiro, calculamos os valores previstos:
>
> ```python
> predicted_Y = alpha * X
> print("Valores previstos:", predicted_Y)
> ```
>
> Depois, calculamos os res√≠duos (erros):
>
> ```python
> residuals = Y - predicted_Y.flatten()
> print("Res√≠duos:", residuals)
> ```
>
> Podemos verificar a condi√ß√£o de ortogonalidade calculando a correla√ß√£o entre os res√≠duos e as temperaturas:
>
> ```python
> correlation = np.corrcoef(residuals, X.flatten())[0, 1]
> print(f"Correla√ß√£o entre res√≠duos e X: {correlation:.4f}")
> ```
>
> O valor da correla√ß√£o deve ser pr√≥ximo de zero, indicando que os res√≠duos n√£o est√£o correlacionados com a temperatura, confirmando a condi√ß√£o de ortogonalidade. Al√©m disso, √© poss√≠vel analisar o MSE:
>
> ```python
> mse = np.mean(residuals**2)
> print(f"Erro Quadr√°tico M√©dio (MSE): {mse:.2f}")
> ```
>
> O MSE representa a m√©dia dos quadrados dos res√≠duos e √© uma m√©trica importante para avaliar o desempenho da proje√ß√£o linear.

  
  **Proposi√ß√£o 1**
   *Se a vari√°vel explicativa $X_t$ tiver m√©dia n√£o nula, a inclus√£o de uma constante na proje√ß√£o linear pode melhorar o ajuste. Ou seja, considerar $\hat{E}(Y_{t+1}|X_t) = \beta_0 + \beta'X_t$ em vez de $\hat{E}(Y_{t+1}|X_t) = \alpha'X_t$ pode levar a um erro quadr√°tico m√©dio menor.*
 
  *Prova:*
  
  A inclus√£o de um termo constante permite que a proje√ß√£o capture um n√≠vel m√©dio de $Y_{t+1}$ que n√£o √© explicado pela varia√ß√£o de $X_t$. Se $E[X_t] \neq 0$, a proje√ß√£o linear sem constante pode ser viesada, pois ela for√ßa a linha de proje√ß√£o a passar pela origem. Ao adicionar uma constante, a proje√ß√£o se torna mais flex√≠vel e pode se ajustar melhor aos dados. Formalmente, podemos mostrar que o MSE da proje√ß√£o linear com intercepto √© menor ou igual ao MSE da proje√ß√£o linear sem intercepto, j√° que a proje√ß√£o com intercepto √© uma forma mais geral da proje√ß√£o linear.
  
  A propriedade de linearidade da proje√ß√£o tamb√©m pode ser expandida para incluir um termo constante. Se a proje√ß√£o linear incluir um termo constante, ela √© escrita como $\hat{E}(Y_{t+1}|X_t) = P(Y_{t+1}|1, X_t) = \beta_0 + \beta'X_t$. O vetor $\beta$ √© calculado usando os momentos populacionais ou amostrais, similar √† equa√ß√£o para $\alpha$.
  
   *Prova da Proposi√ß√£o 1:*
   
   I. Seja $P(Y_{t+1}|X_t) = \alpha'X_t$ a proje√ß√£o linear sem intercepto e $P(Y_{t+1}|1, X_t) = \beta_0 + \beta'X_t$ a proje√ß√£o linear com intercepto.
   
   II. O erro da proje√ß√£o sem intercepto √© $e_{t+1} = Y_{t+1} - \alpha'X_t$, e o erro da proje√ß√£o com intercepto √© $u_{t+1} = Y_{t+1} - \beta_0 - \beta'X_t$.
   
   III. O MSE da proje√ß√£o sem intercepto √© $MSE_{sem} = E[(Y_{t+1} - \alpha'X_t)^2]$ e o MSE da proje√ß√£o com intercepto √© $MSE_{com} = E[(Y_{t+1} - \beta_0 - \beta'X_t)^2]$.
   
   IV. Podemos escrever $MSE_{sem}$ como:
      $$MSE_{sem} = E[(Y_{t+1} - \beta_0 - \beta'X_t + \beta_0 + (\beta' - \alpha')X_t)^2]$$
    
   V. Expandindo o quadrado:
      $$MSE_{sem} = E[(Y_{t+1} - \beta_0 - \beta'X_t)^2] + 2E[(Y_{t+1} - \beta_0 - \beta'X_t)(\beta_0 + (\beta' - \alpha')X_t)] + E[(\beta_0 + (\beta' - \alpha')X_t)^2]$$
    
   VI. Pela condi√ß√£o de ortogonalidade da proje√ß√£o com intercepto, o termo $2E[(Y_{t+1} - \beta_0 - \beta'X_t)(\beta_0 + (\beta' - \alpha')X_t)]$ √© zero.
   
   VII. Assim, a equa√ß√£o se reduz a:
      $$MSE_{sem} = MSE_{com} + E[(\beta_0 + (\beta' - \alpha')X_t)^2]$$
      
   VIII. Como $E[(\beta_0 + (\beta' - \alpha')X_t)^2]$ √© n√£o-negativo, segue que $MSE_{sem} \geq MSE_{com}$, o que demonstra que a inclus√£o do intercepto resulta em um MSE menor ou igual ao da proje√ß√£o linear sem intercepto, e ser√° estritamente menor quando $X_t$ tiver m√©dia n√£o nula, melhorando o ajuste. $\blacksquare$

### O MSE da Proje√ß√£o Linear √ìtima
O MSE da proje√ß√£o linear √≥tima √© dado pela esperan√ßa do erro quadr√°tico da proje√ß√£o linear:
$$MSE[P(Y_{t+1}|X_t)] = E[Y_{t+1} - \alpha'X_t]^2$$
Uma importante propriedade √© que o MSE da proje√ß√£o linear pode ser decomposto da seguinte maneira:
$$E(Y_{t+1} - \alpha'X_t)^2 = E(Y_{t+1}^2) - E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} E(X_tY_{t+1})$$
Essa express√£o mostra que o MSE da proje√ß√£o linear √© igual √† vari√¢ncia de $Y_{t+1}$ menos um termo que reflete a vari√¢ncia explicada pela proje√ß√£o linear. Isso demonstra como a proje√ß√£o linear busca capturar o m√°ximo poss√≠vel da vari√¢ncia de $Y_{t+1}$ por meio de uma fun√ß√£o linear de $X_t$ [^3].
  
   *Prova da Decomposi√ß√£o do MSE da Proje√ß√£o Linear √ìtima:*
  
  I. Come√ßamos com a defini√ß√£o do MSE da proje√ß√£o linear:
  $$MSE = E[(Y_{t+1} - \alpha'X_t)^2]$$
  
  II. Expandindo o quadrado:
  $$MSE = E[Y_{t+1}^2 - 2Y_{t+1}\alpha'X_t + (\alpha'X_t)^2]$$
  
  III. Aplicando a linearidade da esperan√ßa:
  $$MSE = E[Y_{t+1}^2] - 2E[Y_{t+1}\alpha'X_t] + E[\alpha'X_tX_t'\alpha]$$
  
  IV. Substitu√≠mos $\alpha'$ pela express√£o encontrada anteriormente: $\alpha' = E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}$:
   $$MSE = E[Y_{t+1}^2] - 2E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}] + E[E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}X_tX_t'E[X_tY_{t+1}][E(X_tX_t')]^{-1}]$$
   
  V. Simplificando a express√£o:
   $$MSE = E[Y_{t+1}^2] - 2E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}] + E[Y_{t+1}X_t'][E(X_tX_t')]^{-1}E[X_tX_t'][E(X_tX_t')]^{-1}E[X_tY_{t+1}]$$
  
  VI. Simplificando os termos:
  $$MSE = E[Y_{t+1}^2] - 2E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}] + E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}E[X_tY_{t+1}]$$

 VII. Combinando os termos do meio:
 $$MSE = E[Y_{t+1}^2] - E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1} E[X_tY_{t+1}]$$
  
  VIII. O que demonstra a decomposi√ß√£o do MSE da proje√ß√£o linear em termos da vari√¢ncia de $Y_{t+1}$ e a vari√¢ncia explicada pela proje√ß√£o. $\blacksquare$
  
  **Lema 1**
  *Se o erro de previs√£o $e_{t+1} = Y_{t+1} - \alpha'X_t$ √© ortogonal a $X_t$, ent√£o a covari√¢ncia entre $e_{t+1}$ e qualquer fun√ß√£o linear de $X_t$ tamb√©m √© zero.*

  *Prova:*
  
  Se $E[e_{t+1}X_t] = 0$, ent√£o para qualquer vetor constante $b$, temos que $E[e_{t+1}(b'X_t)] = b'E[e_{t+1}X_t] = b' \cdot 0 = 0$. Isso implica que o erro √© ortogonal a qualquer combina√ß√£o linear de $X_t$, refor√ßando o conceito de que a proje√ß√£o linear captura toda a informa√ß√£o linearmente relevante em $X_t$. Essa propriedade √© crucial para derivar outras propriedades da proje√ß√£o linear.
  
  *Prova do Lema 1:*
  
    I. Sabemos que $e_{t+1} = Y_{t+1} - \alpha'X_t$ e $E[e_{t+1}X_t] = 0$, pela condi√ß√£o de ortogonalidade.
    
   II. Queremos mostrar que a covari√¢ncia entre $e_{t+1}$ e qualquer fun√ß√£o linear de $X_t$, digamos $b'X_t$, √© zero: $E[e_{t+1}(b'X_t)] = 0$
   
    III. Usando a propriedade da linearidade da esperan√ßa:
    $$E[e_{t+1}(b'X_t)] = E[b'e_{t+1}X_t] = b'E[e_{t+1}X_t]$$
  
  IV. J√° sabemos que $E[e_{t+1}X_t] = 0$, portanto:
  $$E[e_{t+1}(b'X_t)] = b' \cdot 0 = 0$$
  
 V.  O que demonstra que o erro de previs√£o $e_{t+1}$ √© ortogonal a qualquer combina√ß√£o linear de $X_t$. $\blacksquare$
### A Otimidade da Proje√ß√£o Linear
√â crucial entender que a proje√ß√£o linear, embora possa ser expressa em termos dos momentos da distribui√ß√£o conjunta de $Y_{t+1}$ e $X_t$, *n√£o exige que a distribui√ß√£o seja especificada para ser v√°lida*.  A condi√ß√£o de n√£o correla√ß√£o $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$ √© suficiente para garantir que a proje√ß√£o linear minimize o erro quadr√°tico m√©dio dentro da classe de previs√µes lineares. Al√©m disso, o MSE da proje√ß√£o linear √© igual ao MSE da expectativa condicional, caso esta seja linear, o que garante que a proje√ß√£o linear √© a melhor previs√£o poss√≠vel dentro de sua classe, e que, em casos onde a expectativa condicional for linear, a proje√ß√£o linear recupera a mesma informa√ß√£o contida na expectativa condicional, tornando-a uma alternativa computacionalmente mais eficiente.
  
  **Teorema 1**
   *Se a expectativa condicional $E(Y_{t+1}|X_t)$ √© uma fun√ß√£o linear de $X_t$, ent√£o a proje√ß√£o linear $P(Y_{t+1}|X_t)$ √© igual √† expectativa condicional.*
  
   *Prova:*
  
   Se $E(Y_{t+1}|X_t) = \gamma'X_t$, ent√£o o erro de previs√£o da expectativa condicional √© dado por $e_{t+1} = Y_{t+1} - \gamma'X_t$. Para que a expectativa condicional seja uma proje√ß√£o linear, o erro de previs√£o deve satisfazer a condi√ß√£o de ortogonalidade: $E[(Y_{t+1} - \gamma'X_t)X_t] = 0$. Substituindo $E(Y_{t+1}|X_t)$ por $\gamma'X_t$ na equa√ß√£o da proje√ß√£o linear, obtemos $\alpha' = E[Y_{t+1}X_t][E(X_tX_t')]^{-1} = E[E(Y_{t+1}|X_t)X_t][E(X_tX_t')]^{-1} = E[\gamma'X_tX_t][E(X_tX_t')]^{-1} = \gamma'E[X_tX_t'][E(X_tX_t')]^{-1} = \gamma'$. Portanto, $\alpha' = \gamma'$ e a proje√ß√£o linear √© igual √† expectativa condicional.
   
   *Prova do Teorema 1:*
  
   I. Assumimos que a expectativa condicional √© linear em $X_t$, ou seja, $E(Y_{t+1}|X_t) = \gamma'X_t$.
   
  II. O erro da expectativa condicional √© $e_{t+1} = Y_{t+1} - E(Y_{t+1}|X_t) = Y_{t+1} - \gamma'X_t$.
   
  III. A proje√ß√£o linear √© definida como $P(Y_{t+1}|X_t) = \alpha'X_t$, onde $\alpha' = E[Y_{t+1}X_t'][E(X_tX_t')]^{-1}$.
   
  IV. Para que a expectativa condicional seja igual √† proje√ß√£o linear, o erro de previs√£o $e_{t+1}$ deve satisfazer a condi√ß√£o de ortogonalidade: $E[(Y_{t+1} - \gamma'X_t)X_t] = 0$.
   
  V.  Substituindo $E(Y_{t+1}|X_t)$ por $\gamma'X_t$ na equa√ß√£o de $\alpha'$:
   $$ \alpha' = E[Y_{t+1}X_t'][E(X_tX_t')]^{-1} = E[E(Y_{t+1}|X_t)X_t'][E(X_tX_t')]^{-1}$$
   
  VI. Como $E(Y_{t+1}|X_t) = \gamma'X_t$:
  $$\alpha' = E[\gamma'X_tX_t'][E(X_tX_t')]^{-1} = \gamma'E[X_tX_t'][E(X_tX_t')]^{-1}$$
   
 VII. Simplificando a express√£o:
    $$\alpha' = \gamma'$$
    
 VIII. Portanto, se a expectativa condicional for linear, ent√£o $\alpha' = \gamma'$ e a proje√ß√£o linear √© igual √† expectativa condicional: $P(Y_{t+1}|X_t) = E(Y_{t+1}|X_t)$. $\blacksquare$

> üí° **Rela√ß√£o com OLS:**
> A proje√ß√£o linear est√° intimamente relacionada com a regress√£o de m√≠nimos quadrados ordin√°rios (OLS). Em OLS, procuramos encontrar os coeficientes que minimizam a soma dos erros quadr√°ticos amostrais, usando um modelo da forma $Y_{t+1} = \beta'X_t + e_t$.  Utilizando os momentos amostrais na f√≥rmula da proje√ß√£o linear, obtemos os mesmos coeficientes que minimizam a soma dos erros quadr√°ticos amostrais na regress√£o OLS, quando o intercepto √© igual a zero. Assim, a proje√ß√£o linear fornece um framework te√≥rico para entender os fundamentos da regress√£o OLS. Este ponto √© explorado em mais detalhe nas se√ß√µes seguintes.
>
> Para ilustrar esta rela√ß√£o, vamos usar os mesmos dados do exemplo anterior:
>
> ```python
> from sklearn.linear_model import LinearRegression
>
> # Adiciona uma coluna de 1s para o intercepto (se necess√°rio)
> X_with_intercept = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)
>
> # Cria um modelo de regress√£o linear com intercepto
> model_ols = LinearRegression(fit_intercept=True)
> model_ols.fit(X_with_intercept, Y)
>
> # Imprime os coeficientes do OLS
> print(f"Coeficiente (OLS com intercepto): {model_ols.coef_}")
> print(f"Intercepto (OLS com intercepto): {model_ols.intercept_}")
>
> # Cria um modelo de regress√£o linear sem intercepto
> model_ols_no_intercept = LinearRegression(fit_intercept=False)
> model_ols_no_intercept.fit(X, Y)
> print(f"Coeficiente (OLS sem intercepto): {model_ols_no_intercept.coef_}")
> ```
>
> Comparando os coeficientes do OLS sem intercepto com o valor de $\alpha$ calculado anteriormente, vemos que eles s√£o id√™nticos. O coeficiente do OLS com intercepto nos d√° um intercepto de aproximadamente 1.45 e uma inclina√ß√£o de 4.90, mostrando como a inclus√£o de um intercepto melhora o ajuste.

  
  **Corol√°rio 1.1**
  *Se $E(Y_{t+1}|X_t)$ √© linear em $X_t$, o erro quadr√°tico m√©dio da proje√ß√£o linear √© igual ao erro quadr√°tico m√©dio da expectativa condicional.*
  
  *Prova:*
  
   Se $E(Y_{t+1}|X_t)$ √© linear, ent√£o, pelo Teorema 1, a proje√ß√£o linear √© igual √† expectativa condicional.  Consequentemente, o MSE da proje√ß√£o linear √© dado por $E[Y_{t+1} - P(Y_{t+1}|X_t)]^2$ e o MSE da expectativa condicional √© dado por $E[Y_{t+1} - E(Y_{t+1}|X_t)]^2$. Como $P(Y_{t+1}|X_t) = E(Y_{t+1}|X_t)$ neste caso, os dois MSEs s√£o iguais, refor√ßando a ideia de que em modelos lineares a proje√ß√£o linear √© uma alternativa computacionalmente mais simples para o c√°lculo da expectativa condicional.
  
 *Prova do Corol√°rio 1.1:*
  
  I.  Se $E(Y_{t+1}|X_t)$ √© linear em $X_t$, ent√£o, pelo Teorema 1, temos que $P(Y_{t+1}|X_t) = E(Y_{t+1}|X_t)$.
  
  II. O MSE da proje√ß√£o linear √© dado por $MSE_{PL} = E[(Y_{t+1} - P(Y_{t+1}|X_t))^2]$.
  
  III. O MSE da expectativa condicional √© dado por $MSE_{EC} = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2]$.
  
  IV. Como $P(Y_{t+1}|X_t) = E(Y_{t+1}|X_t)$, substitu√≠mos a proje√ß√£o linear na express√£o do MSE:
      $$MSE_{PL} = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2]$$
      
  V.  Como $MSE_{EC} = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2]$, vemos que:
       $$MSE_{PL} = MSE_{EC}$$
       
   VI. Portanto, o erro quadr√°tico m√©dio da proje√ß√£o linear √© igual ao erro quadr√°tico m√©dio da expectativa condicional quando a expectativa condicional √© linear em $X_t$. $\blacksquare$

### Conclus√£o
A proje√ß√£o linear de $Y_{t+1}$ em $X_t$, expressa como $P(Y_{t+1}|X_t) = \alpha'X_t$, busca a fun√ß√£o linear que melhor se aproxima de $Y_{t+1}$ no sentido de minimizar o erro quadr√°tico m√©dio, garantindo que o erro de previs√£o seja n√£o correlacionado com as vari√°veis explicativas. Esta condi√ß√£o de ortogonalidade √© fundamental para a otimalidade da projeidade das estimativas.

### Deriva√ß√£o Matem√°tica da Solu√ß√£o de M√≠nimos Quadrados

A busca pela solu√ß√£o de m√≠nimos quadrados envolve a minimiza√ß√£o da fun√ß√£o de custo, que √© a soma dos quadrados dos erros. Em termos matem√°ticos, se temos um modelo linear dado por:

$$
y = X\beta + \epsilon
$$

onde $y$ √© o vetor de observa√ß√µes, $X$ √© a matriz de vari√°veis explicativas, $\beta$ √© o vetor de par√¢metros a serem estimados e $\epsilon$ √© o vetor de erros, a fun√ß√£o de custo $J(\beta)$ √© dada por:

$$
J(\beta) = \epsilon^T \epsilon = (y - X\beta)^T (y - X\beta)
$$

Para minimizar $J(\beta)$, derivamos em rela√ß√£o a $\beta$ e igualamos a zero:

$$
\frac{\partial J(\beta)}{\partial \beta} = -2X^T(y - X\beta) = 0
$$

Resolvendo para $\beta$, obtemos o estimador de m√≠nimos quadrados:

$$
\hat{\beta} = (X^TX)^{-1}X^Ty
$$

Essa solu√ß√£o √© v√°lida quando a matriz $X^TX$ √© invert√≠vel, o que requer que as vari√°veis explicativas n√£o sejam linearmente dependentes, ou seja, n√£o haja multicolinearidade perfeita.

### Propriedades do Estimador de M√≠nimos Quadrados

O estimador de m√≠nimos quadrados possui propriedades estat√≠sticas desej√°veis, sob certas condi√ß√µes:

1.  **N√£o-viesado:** O estimador $\hat{\beta}$ √© n√£o-viesado, ou seja, $E[\hat{\beta}] = \beta$, o que significa que, em m√©dia, o estimador converge para o verdadeiro valor do par√¢metro.
2.  **Efici√™ncia:** Sob as hip√≥teses de homocedasticidade e n√£o-correla√ß√£o dos erros, o estimador de m√≠nimos quadrados √© o estimador linear n√£o-viesado de menor vari√¢ncia (BLUE - Best Linear Unbiased Estimator). Este resultado √© conhecido como Teorema de Gauss-Markov.
3. **Consist√™ncia:** Quando o n√∫mero de amostras aumenta, o estimador $\hat{\beta}$ converge em probabilidade para o valor verdadeiro do par√¢metro.

### Considera√ß√µes Pr√°ticas

Na pr√°tica, √© importante verificar as hip√≥teses do modelo de m√≠nimos quadrados para garantir a validade das estimativas. Isso inclui:

*   **Linearidade:** A rela√ß√£o entre as vari√°veis dependentes e independentes deve ser linear.
*   **Independ√™ncia dos erros:** Os erros devem ser independentes entre si.
*   **Homocedasticidade:** A vari√¢ncia dos erros deve ser constante para todas as observa√ß√µes.
*   **Normalidade dos erros:** Os erros devem seguir uma distribui√ß√£o normal (especialmente importante para testes de hip√≥teses e intervalos de confian√ßa).

A viola√ß√£o dessas hip√≥teses pode levar a estimativas viesadas, ineficientes e a conclus√µes incorretas. T√©cnicas como transforma√ß√£o de vari√°veis, pondera√ß√£o de m√≠nimos quadrados e m√©todos robustos podem ser utilizados para tratar essas viola√ß√µes.

### Aplica√ß√µes e Extens√µes

A regress√£o linear e o m√©todo de m√≠nimos quadrados t√™m uma vasta gama de aplica√ß√µes em diversas √°reas, desde economia e finan√ßas at√© engenharia e ci√™ncias sociais. Al√©m disso, o m√©todo de m√≠nimos quadrados serve de base para m√©todos mais avan√ßados, como:

*   **Regress√£o n√£o linear:** Adapta o conceito de m√≠nimos quadrados para modelos n√£o lineares.
*   **Regress√£o log√≠stica:** Utilizada para modelar vari√°veis dependentes bin√°rias ou categ√≥ricas.
*   **Modelos de efeitos mistos:** Incorporam efeitos fixos e aleat√≥rios.

Em todos esses casos, o princ√≠pio fundamental de minimizar a soma dos quadrados dos erros √© central para a estima√ß√£o de par√¢metros.

### Exemplo Pr√°tico

Para ilustrar, considere um modelo simples onde queremos prever o pre√ßo de uma casa ($y$) com base no seu tamanho em metros quadrados ($x$):

$$
y = \beta_0 + \beta_1 x + \epsilon
$$

Os dados podem ser representados por um conjunto de pares $(x_i, y_i)$, para $i=1, \dots, n$. Utilizando o m√©todo de m√≠nimos quadrados, podemos estimar $\beta_0$ e $\beta_1$ de forma que a soma dos quadrados dos erros seja minimizada. Este exemplo simples demonstra como a teoria se aplica na pr√°tica.
<!-- END -->
