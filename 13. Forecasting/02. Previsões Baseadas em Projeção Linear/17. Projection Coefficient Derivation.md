## O C√°lculo do Coeficiente de Proje√ß√£o Linear: Uma An√°lise Baseada em Momentos Populacionais

### Introdu√ß√£o
Este cap√≠tulo foca na dedu√ß√£o e interpreta√ß√£o do **coeficiente de proje√ß√£o linear** $\alpha'$, mostrando como ele √© calculado usando os momentos populacionais de $Y_{t+1}$ e $X_t$. Conforme discutido em cap√≠tulos anteriores, a proje√ß√£o linear, definida como $P(Y_{t+1}|X_t) = \alpha'X_t$, busca a melhor aproxima√ß√£o linear de $Y_{t+1}$ utilizando as vari√°veis explicativas $X_t$ [^1]. O ponto central √© que o vetor de coeficientes $\alpha$ √© determinado de forma a minimizar o erro quadr√°tico m√©dio (MSE), com a condi√ß√£o crucial de que o erro de previs√£o seja n√£o correlacionado com as vari√°veis explicativas [^2]. Vamos detalhar como essa condi√ß√£o leva √† formula√ß√£o do coeficiente $\alpha$ e explorar suas implica√ß√µes.

### A Deriva√ß√£o do Coeficiente de Proje√ß√£o Linear
A proje√ß√£o linear busca o vetor $\alpha$ que minimiza o MSE, sujeito √† restri√ß√£o de que o erro de previs√£o, $e_{t+1} = Y_{t+1} - \alpha'X_t$, seja ortogonal (n√£o correlacionado) ao espa√ßo gerado pelas vari√°veis explicativas $X_t$. Essa condi√ß√£o de ortogonalidade √© expressa como:
$$E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$$
onde $0'$ denota um vetor nulo de dimens√£o apropriada. Essa equa√ß√£o implica que a esperan√ßa do produto do erro de previs√£o com cada uma das vari√°veis explicativas (e, portanto, com qualquer combina√ß√£o linear dessas vari√°veis) deve ser igual a zero.

Aplicando a propriedade da linearidade da esperan√ßa, podemos reescrever essa condi√ß√£o como:
$$E[Y_{t+1}X_t] - E[\alpha'X_tX_t] = 0'$$
ou equivalentemente:
$$E[Y_{t+1}X_t] - \alpha'E[X_tX_t'] = 0'$$
Agora, isolamos $\alpha'$:
$$\alpha'E[X_tX_t'] = E[Y_{t+1}X_t]$$
Para encontrar $\alpha'$, precisamos multiplicar ambos os lados da equa√ß√£o pela inversa da matriz $E[X_tX_t']$, assumindo que essa matriz seja n√£o singular:
$$\alpha' = E[Y_{t+1}X_t] [E(X_tX_t')]^{-1}$$
Essa √© a formula√ß√£o do coeficiente de proje√ß√£o linear $\alpha'$, que conecta os momentos populacionais de $Y_{t+1}$ e $X_t$ com o vetor que minimiza o MSE dentro do espa√ßo linear.

A matriz $E[X_tX_t']$ √© uma matriz de covari√¢ncia (ou de segunda ordem, no caso geral, quando a m√©dia de X n√£o √© nula) das vari√°veis explicativas. A condi√ß√£o de n√£o singularidade dessa matriz √© crucial para garantir a exist√™ncia da inversa e, consequentemente, a exist√™ncia e unicidade do vetor $\alpha$ [^2]. Quando essa condi√ß√£o n√£o √© satisfeita, significa que algumas vari√°veis em $X_t$ s√£o linearmente dependentes de outras, e a proje√ß√£o linear n√£o √© √∫nica.

> üí° **Exemplo Num√©rico:**
> Para ilustrar o c√°lculo do coeficiente $\alpha'$, consideremos um exemplo pr√°tico onde desejamos prever as vendas de um produto ($Y_{t+1}$) com base nos gastos com marketing digital ($X_t$). Suponha que tenhamos alguns dados simulados:
>
> | t   | Marketing ($X_t$) | Vendas ($Y_{t+1}$) |
> |-----|-------------------|-------------------|
> | 1   | 1                 | 4                 |
> | 2   | 2                 | 6                 |
> | 3   | 3                 | 8                 |
> | 4   | 4                 | 11                |
> | 5   | 5                 | 13                |
>
> Para calcular $\alpha$, precisamos calcular os momentos populacionais $E[X_tX_t']$ e $E[Y_{t+1}X_t]$. Em primeiro lugar, calculamos os momentos:
>
> 1. **Calculamos** $E[X_tX_t']$:
>   $$E[X_tX_t'] = E[X_t^2] = \frac{1^2+2^2+3^2+4^2+5^2}{5} = \frac{1+4+9+16+25}{5} = \frac{55}{5} = 11$$
>
> 2. **Calculamos** $E[Y_{t+1}X_t]$:
>   $$E[Y_{t+1}X_t] = \frac{1\cdot4 + 2\cdot6 + 3\cdot8 + 4\cdot11 + 5\cdot13}{5} = \frac{4+12+24+44+65}{5} = \frac{147}{5} = 29.4$$
>
> Agora, podemos calcular o coeficiente $\alpha$:
> $$\alpha = E[Y_{t+1}X_t] [E(X_tX_t')]^{-1} = \frac{29.4}{11} \approx 2.67$$
>
> Portanto, a proje√ß√£o linear √© $\hat{Y}_{t+1} = 2.67 X_t$. Isso significa que, para cada unidade monet√°ria gasta em marketing digital, espera-se um aumento de aproximadamente 2.67 unidades nas vendas do produto.
>
> Para verificar se esta escolha de $\alpha$ satisfaz a condi√ß√£o de ortogonalidade, calculamos o erro de previs√£o e o produto desse erro com $X_t$:
>
> | t   | $X_t$ | $Y_{t+1}$ | $\hat{Y}_{t+1} = 2.67X_t$ | $e_{t+1} = Y_{t+1} - \hat{Y}_{t+1}$ | $e_{t+1}X_t$ |
> |-----|-------|-----------|-------------------------|----------------------------|------------------|
> | 1   | 1     | 4         | 2.67                    | 1.33                       | 1.33             |
> | 2   | 2     | 6         | 5.34                    | 0.66                       | 1.32             |
> | 3   | 3     | 8         | 8.01                    | -0.01                      | -0.03            |
> | 4   | 4     | 11        | 10.68                   | 0.32                       | 1.28             |
> | 5   | 5     | 13        | 13.35                   | -0.35                      | -1.75            |
>
> Calculando a m√©dia de $e_{t+1}X_t$, temos que o produto amostral √© $\approx 0.43$, que √© uma aproxima√ß√£o de 0, com o erro surgindo por causa dos erros amostrais ao inv√©s dos momentos populacionais.

### O Papel da N√£o Singularidade de $E(X_tX_t')$
A n√£o singularidade da matriz $E(X_tX_t')$ √© essencial para garantir que a inversa $[E(X_tX_t')]^{-1}$ exista, o que √© necess√°rio para calcular o coeficiente $\alpha'$. Matematicamente, uma matriz √© n√£o singular (ou invers√≠vel) se seu determinante for diferente de zero. No contexto de proje√ß√µes lineares, essa condi√ß√£o garante que as vari√°veis explicativas em $X_t$ n√£o sejam linearmente dependentes entre si, ou seja, que nenhuma vari√°vel possa ser escrita como uma combina√ß√£o linear das outras.

Quando a matriz $E(X_tX_t')$ √© singular, significa que h√° multicolinearidade perfeita entre as vari√°veis explicativas, e a proje√ß√£o linear n√£o pode ser determinada de forma √∫nica. Nesses casos, uma poss√≠vel solu√ß√£o √© remover as vari√°veis redundantes ou usar m√©todos de regulariza√ß√£o que adicionam um pequeno termo na diagonal da matriz.

> üí° **Exemplo Num√©rico:**
> Para ilustrar o que acontece quando a matriz $E(X_tX_t')$ √© singular, vamos considerar um exemplo onde temos duas vari√°veis explicativas linearmente dependentes: $X_{1t}$ e $X_{2t} = 2X_{1t}$.
>
> | t  | $X_{1t}$ | $X_{2t} = 2X_{1t}$  | $Y_{t+1}$ |
> |----|----------|-------------------|-----------|
> | 1  | 1        | 2                 | 4         |
> | 2  | 2        | 4                 | 6         |
> | 3  | 3        | 6                 | 8         |
>
> Vamos criar um vetor $X_t = [X_{1t}, X_{2t}]'$ com as vari√°veis explicativas e calcular a matriz $E(X_tX_t')$:
>
> 1.  **Calculamos** $E[X_tX_t']$:
>
>  $X_tX_t' =  \begin{bmatrix} X_{1t}^2 & X_{1t}X_{2t} \\ X_{2t}X_{1t} & X_{2t}^2 \end{bmatrix}$
>
> $$E[X_tX_t'] = \frac{1}{3} \sum_{t=1}^{3} X_tX_t' = \frac{1}{3} \begin{bmatrix} 1^2+2^2+3^2 & 1\cdot2+2\cdot4+3\cdot6 \\ 2\cdot1+4\cdot2+6\cdot3 & 2^2+4^2+6^2 \end{bmatrix} = \frac{1}{3} \begin{bmatrix} 14 & 28 \\ 28 & 56 \end{bmatrix} = \begin{bmatrix} 14/3 & 28/3 \\ 28/3 & 56/3 \end{bmatrix}$$
>
> 2. **Calculamos** $E[Y_{t+1}X_t]$:
>  $$E[Y_{t+1}X_t] = \frac{1}{3} \sum_{t=1}^{3} Y_{t+1}X_t =  \frac{1}{3} \begin{bmatrix} 1\cdot4 + 2\cdot6 + 3\cdot8 \\ 2\cdot4+4\cdot6+6\cdot8 \end{bmatrix} = \frac{1}{3} \begin{bmatrix} 40 \\ 80 \end{bmatrix} = \begin{bmatrix} 40/3 \\ 80/3 \end{bmatrix}$$
>
> Para demonstrar que a matriz $E(X_tX_t')$ √© singular, calculamos o determinante:
>  $$\text{det}(E[X_tX_t']) = \frac{14}{3}\frac{56}{3} - \frac{28}{3}\frac{28}{3} = \frac{784}{9} - \frac{784}{9} = 0$$
>
> Como o determinante √© igual a zero, a matriz n√£o √© invers√≠vel. Se tentarmos calcular $\alpha$ utilizando a f√≥rmula, obteremos um erro. Isso ocorre por que a rela√ß√£o linear entre $X_{1t}$ e $X_{2t}$ faz com que a proje√ß√£o linear n√£o seja √∫nica.

### A Condi√ß√£o de Ortogonalidade e a Minimiza√ß√£o do MSE
A condi√ß√£o de ortogonalidade, $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$, √© uma consequ√™ncia direta da minimiza√ß√£o do MSE.  Ela garante que o erro de previs√£o seja n√£o correlacionado com as vari√°veis explicativas. Como demonstrado em cap√≠tulos anteriores, a proje√ß√£o linear busca o vetor $\alpha$ que minimiza o MSE:
$$MSE = E[(Y_{t+1} - \alpha'X_t)^2]$$
Expandindo o MSE e utilizando a condi√ß√£o de ortogonalidade, como j√° demonstrado, obtemos:
$$MSE = E[Y_{t+1}^2] - E[Y_{t+1}X_t'][E(X_tX_t')]^{-1}E[X_tY_{t+1}]$$
Essa express√£o explicita como a escolha do vetor $\alpha$ influencia o valor do MSE, minimizando-o ao capturar a m√°xima vari√¢ncia poss√≠vel de $Y_{t+1}$ a partir de uma combina√ß√£o linear de $X_t$. A condi√ß√£o de ortogonalidade √©, portanto, o princ√≠pio fundamental que conecta a minimiza√ß√£o do MSE com a obten√ß√£o do coeficiente de proje√ß√£o linear [^2].

**Lema 1:** (A condi√ß√£o de ortogonalidade implica na minimiza√ß√£o do MSE) *Se o erro de previs√£o* $e_{t+1} = Y_{t+1} - \alpha'X_t$ *satisfaz a condi√ß√£o de ortogonalidade* $E[e_{t+1}X_t] = 0'$, *ent√£o a proje√ß√£o linear* $\alpha'X_t$ *minimiza o MSE dentro da classe das previs√µes lineares.*

*Proof:*
I. Seja $g'X_t$ uma previs√£o linear qualquer de $Y_{t+1}$.  Queremos mostrar que $MSE(Y_{t+1}|\alpha'X_t) \le MSE(Y_{t+1}|g'X_t)$.
II.  Considerando o MSE da previs√£o linear arbitr√°ria $g'X_t$: $MSE = E[(Y_{t+1} - g'X_t)^2]$.
III. Adicionando e subtraindo $\alpha'X_t$ na express√£o do MSE:
$MSE = E[(Y_{t+1} - \alpha'X_t + \alpha'X_t - g'X_t)^2]$.
IV. Expandindo o quadrado:
$MSE = E[(Y_{t+1} - \alpha'X_t)^2] + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t)] + E[(\alpha'X_t - g'X_t)^2]$.
V. O termo intermedi√°rio √© zero pela condi√ß√£o de ortogonalidade:
  $E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t)] = E[(Y_{t+1} - \alpha'X_t)X_t'](\alpha - g) = 0 (\alpha - g) = 0$
VI.  Logo:
$MSE = E[(Y_{t+1} - \alpha'X_t)^2] + E[(\alpha'X_t - g'X_t)^2]$
VII. O segundo termo √© n√£o negativo, logo o MSE √© minimizado quando ele for zero, ou seja quando $\alpha = g$.
VIII. Portanto, a proje√ß√£o linear $\alpha'X_t$ minimiza o MSE dentro da classe de previs√µes lineares. $\blacksquare$
 
**Lema 2:** (Unicidade do Coeficiente de Proje√ß√£o Linear) *Se a matriz* $E(X_tX_t')$ *for n√£o singular, ent√£o o coeficiente de proje√ß√£o linear* $\alpha$ *√© √∫nico.*

*Proof:*
I.  Sabemos que $\alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}$.
II. Se  $E(X_tX_t')$ √© n√£o singular, ent√£o sua inversa $[E(X_tX_t')]^{-1}$ √© √∫nica.
III. A unicidade da inversa garante que o valor de $\alpha'$ √© √∫nico.
IV.  Portanto, se $E(X_tX_t')$ √© n√£o singular, o coeficiente $\alpha$ √© √∫nico. $\blacksquare$

**Proposi√ß√£o 1:** *Se* $X_t$ *incluir um termo constante (intercepto), a condi√ß√£o de ortogonalidade  √©* $E[(Y_{t+1} - (\beta_0 + \beta'X_t))(1,X_t)] = 0$. *Dessa forma, os coeficientes da proje√ß√£o linear com intercepto* $\hat{E}(Y_{t+1}|X_t) = \beta_0 + \beta'X_t$ *s√£o obtidos com*
$$[\beta_0, \beta] = E(Y_{t+1}X_t^*)[E(X_t^*X_t^{*'})]^{-1}$$
 *onde*  $X_t^* = [1, X_t]'$.

*Proof:*
I.  A proje√ß√£o linear com intercepto √© dada por $\hat{Y}_{t+1} =  \beta_0 + \beta'X_t$.
II.  A condi√ß√£o de ortogonalidade √© $E[(Y_{t+1} - (\beta_0 + \beta'X_t))(1, X_t)] = 0$.
III. Definindo $X_t^* = [1, X_t]'$, podemos reescrever a proje√ß√£o linear como $\hat{Y}_{t+1} =  \beta^{*'}X_t^*$, onde $\beta^{*'} = [\beta_0, \beta']$.
IV.  Dessa forma, a condi√ß√£o de ortogonalidade pode ser escrita como $E[(Y_{t+1} - \beta^{*'}X_t^*)X_t^*] = 0$.
V.  Utilizando o resultado anterior, o vetor  $\beta^*$ √© dado por $\beta^* = E[Y_{t+1}X_t^*] [E(X_t^*X_t^{*'})]^{-1}$.  $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Vamos considerar o mesmo exemplo de vendas e marketing, mas adicionando um intercepto para modelar uma venda base que n√£o depende de marketing.  Adicionamos uma coluna de 1s para o intercepto.  A nossa matriz $X_t^*$ agora √©  $X_t^* = [1, X_t]'$. Os dados simulados agora s√£o:
>
> | t   | Intercepto | Marketing ($X_t$) | Vendas ($Y_{t+1}$) |
> |-----|------------|-------------------|-------------------|
> | 1   | 1          | 1                 | 4                 |
> | 2   | 1          | 2                 | 6                 |
> | 3   | 1          | 3                 | 8                 |
> | 4   | 1          | 4                 | 11                |
> | 5   | 1          | 5                 | 13                |
>
> 1. **Calculamos** $E[X_t^*X_t^{*'}]$:
>   $$E[X_t^*X_t^{*'}] = \frac{1}{5} \sum_{t=1}^{5} \begin{bmatrix} 1 & X_t \\ 1 & X_t \end{bmatrix} \begin{bmatrix} 1 & 1 \\ X_t & X_t \end{bmatrix} = \frac{1}{5} \sum_{t=1}^{5} \begin{bmatrix} 1 & X_t \\ X_t & X_t^2 \end{bmatrix}$$
>   $$E[X_t^*X_t^{*'}] = \frac{1}{5} \begin{bmatrix} 5 & 15 \\ 15 & 55 \end{bmatrix} = \begin{bmatrix} 1 & 3 \\ 3 & 11 \end{bmatrix}$$
>
> 2. **Calculamos** $E[Y_{t+1}X_t^*]$:
>
>   $$E[Y_{t+1}X_t^*] = \frac{1}{5} \sum_{t=1}^{5} \begin{bmatrix} Y_{t+1} \\ Y_{t+1}X_t \end{bmatrix}  = \frac{1}{5} \begin{bmatrix} 4+6+8+11+13 \\ 4+12+24+44+65 \end{bmatrix} = \frac{1}{5} \begin{bmatrix} 42 \\ 147 \end{bmatrix} = \begin{bmatrix} 8.4 \\ 29.4 \end{bmatrix}$$
>
>
> 3. **Calculamos** $[E(X_t^*X_t^{*'})]^{-1}$:
>
>   O determinante de $E[X_t^*X_t^{*'}]$ √© $11-9=2$.  A inversa √©:
>   $$[E(X_t^*X_t^{*'})]^{-1} = \frac{1}{2} \begin{bmatrix} 11 & -3 \\ -3 & 1 \end{bmatrix} = \begin{bmatrix} 5.5 & -1.5 \\ -1.5 & 0.5 \end{bmatrix}$$
>
> 4. **Calculamos** $[\beta_0, \beta] = E(Y_{t+1}X_t^*)[E(X_t^*X_t^{*'})]^{-1}$:
>
>   $$[\beta_0, \beta] = \begin{bmatrix} 8.4 & 29.4 \end{bmatrix} \begin{bmatrix} 5.5 & -1.5 \\ -1.5 & 0.5 \end{bmatrix} = \begin{bmatrix} 8.4\cdot 5.5 - 29.4 \cdot 1.5 & -8.4\cdot 1.5 + 29.4 \cdot 0.5 \end{bmatrix} = \begin{bmatrix} 1.5 & 2.7 \end{bmatrix}$$
>
> Portanto, a proje√ß√£o linear com intercepto √© $\hat{Y}_{t+1} = 1.5 + 2.7 X_t$. O intercepto $\beta_0$ = 1.5 indica uma venda base de 1.5 unidades quando n√£o h√° gastos com marketing e $\beta$ = 2.7 indica que cada unidade de gasto em marketing aumentar√° as vendas em 2.7 unidades, mantendo um intercepto de 1.5 unidades.

**Teorema 1:** (Decomposi√ß√£o da Vari√¢ncia) *A vari√¢ncia de* $Y_{t+1}$ *pode ser decomposta em duas partes, a vari√¢ncia da proje√ß√£o linear e a vari√¢ncia do erro:*
  $$Var(Y_{t+1}) = Var(\alpha'X_t) + Var(e_{t+1})$$
*onde* $e_{t+1} = Y_{t+1} - \alpha'X_t$ *√© o erro de proje√ß√£o linear*

*Proof:*
I. A vari√¢ncia de $Y_{t+1}$ √© $Var(Y_{t+1}) = E[(Y_{t+1}-E[Y_{t+1}])^2]$.
II. Sabemos que $Y_{t+1} = \alpha'X_t + e_{t+1}$ e que $E[e_{t+1}X_t]=0$.
III.  Vamos adicionar e subtrair $E[\alpha'X_t]$:
$Var(Y_{t+1}) = E[(\alpha'X_t + e_{t+1} - E[Y_{t+1}])^2] = E[(\alpha'X_t - E[\alpha'X_t] + e_{t+1} - E[e_{t+1}])^2]$
IV.  Expandindo o quadrado:
$Var(Y_{t+1}) = E[(\alpha'X_t - E[\alpha'X_t])^2] + E[(e_{t+1} - E[e_{t+1}])^2] + 2E[(\alpha'X_t - E[\alpha'X_t])(e_{t+1} - E[e_{t+1}])]$
V. O √∫ltimo termo √© nulo pois
$E[(\alpha'X_t - E[\alpha'X_t])(e_{t+1} - E[e_{t+1}])] = E[\alpha'X_te_{t+1}] - E[\alpha'X_t]E[e_{t+1}] -E[E[\alpha'X_t]e_{t+1}] + E[\alpha'X_t]E[e_{t+1}] =  E[\alpha'X_te_{t+1}] - E[\alpha'X_t]E[e_{t+1}] = 0$
  Pois $E[\alpha'X_te_{t+1}] = \alpha'E[X_te_{t+1}] = 0$ (pela condi√ß√£o de ortogonalidade)
VI. Ent√£o
$Var(Y_{t+1}) = E[(\alpha'X_t - E[\alpha'X_t])^2] + E[(e_{t+1} - E[e_{t+1}])^2] = Var(\alpha'X_t) + Var(e_{t+1}) $.  $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Usando o primeiro exemplo num√©rico (sem intercepto), podemos ilustrar a decomposi√ß√£o da vari√¢ncia.  Lembrando, a proje√ß√£o linear foi $\hat{Y}_{t+1} = 2.67X_t$.
>
> 1.  **Calculamos** a vari√¢ncia de $Y_{t+1}$:
>
>  $E[Y_{t+1}] = \frac{4+6+8+11+13}{5} = 8.4$
>
>   $Var(Y_{t+1}) = E[(Y_{t+1} - E[Y_{t+1}])^2] = \frac{(4-8.4)^2 + (6-8.4)^2 + (8-8.4)^2 + (11-8.4)^2 + (13-8.4)^2}{5} = \frac{19.36 + 5.76 + 0.16 + 6.76 + 21.16}{5} = 10.64$
>
>
> 2. **Calculamos** a vari√¢ncia da proje√ß√£o linear $\hat{Y}_{t+1} = 2.67X_t$:
>
>    $E[\hat{Y}_{t+1}] = 2.67E[X_t] = 2.67 * \frac{1+2+3+4+5}{5} = 2.67*3 = 8.01$
>
>    $Var(\hat{Y}_{t+1}) = E[(\hat{Y}_{t+1} - E[\hat{Y}_{t+1}])^2] = \frac{(2.67-8.01)^2 + (5.34-8.01)^2 + (8.01-8.01)^2 + (10.68-8.01)^2 + (13.35-8.01)^2}{5} = \frac{28.40+7.13 + 0 + 7.13 + 28.40}{5} = 14.21$
>
>
> 3. **Calculamos** a vari√¢ncia do erro $e_{t+1}$:
>
>    $e_{t+1}$ = [1.33, 0.66, -0.01, 0.32, -0.35]
>
>  $E[e_{t+1}] = \frac{1.33 + 0.66 - 0.01 + 0.32 - 0.35}{5} = 0.39$
>
>    $Var(e_{t+1}) = E[(e_{t+1} - E[e_{t+1}])^2] =  \frac{(1.33-0.39)^2+(0.66-0.39)^2+(-0.01-0.39)^2+(0.32-0.39)^2+(-0.35-0.39)^2}{5} = \frac{0.8836+0.0729+0.16+0.0049+0.5476}{5} = \frac{1.669}{5} = 0.33$
>
>
>  Note que $Var(\hat{Y}_{t+1}) + Var(e_{t+1}) = 14.21 + 0.33 = 14.54$, o que √© diferente de $Var(Y_{t+1}) = 10.64$ por conta de erros amostrais, visto que calculamos momentos amostrais e n√£o populacionais. Entretanto, em um caso populacional, a igualdade √© mantida, como demonstrado no teorema.

**Corol√°rio 1.1:** *A propor√ß√£o da vari√¢ncia de* $Y_{t+1}$ *explicada pela proje√ß√£o linear √© dada por*
$$\frac{Var(\alpha'X_t)}{Var(Y_{t+1})} = 1 - \frac{Var(e_{t+1})}{Var(Y_{t+1})}$$

*Proof:*
I. Pelo Teorema 1 temos que $Var(Y_{t+1}) = Var(\alpha'X_t) + Var(e_{t+1})$.
II. Dividindo ambos os lados por $Var(Y_{t+1})$:
$1 = \frac{Var(\alpha'X_t)}{Var(Y_{t+1})} + \frac{Var(e_{t+1})}{Var(Y_{t+1})}$
III. Isolando $\frac{Var(\alpha'X_t)}{Var(Y_{t+1})}$:
$\frac{Var(\alpha'X_t)}{Var(Y_{t+1})} = 1 - \frac{Var(e_{t+1})}{Var(Y_{t+1})}$
IV. Portanto, a propor√ß√£o da vari√¢ncia de $Y_{t+1}$ explicada pela proje√ß√£o linear √© $1 - \frac{Var(e_{t+1})}{Var(Y_{t+1})}$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Utilizando os resultados do exemplo anterior, podemos calcular a propor√ß√£o da vari√¢ncia de $Y_{t+1}$ explicada pela proje√ß√£o linear:
>
> $$\frac{Var(\alpha'X_t)}{Var(Y_{t+1})} = \frac{14.21}{10.64} \approx 1.335$$
>
> e
>
> $$1 - \frac{Var(e_{t+1})}{Var(Y_{t+1})} = 1 - \frac{0.33}{10.64} = 1 - 0.031 = 0.969$$
>
> Note que os valores s√£o diferentes por erros amostrais. No caso populacional, como demonstrado no corol√°rio, as duas quantidades ser√£o id√™nticas.  Em nosso caso, vemos que o modelo de regress√£o linear explica aproximadamente 96.9% da varia√ß√£o nas vendas em nosso exemplo.

### Conclus√£o
O coeficiente de proje√ß√£o linear $\alpha'$ √© calculado como $\alpha' = E(Y_{t+1}X_t')[E(X_tX_t')]^{-1}$, uma f√≥rmula que conecta os momentos populacionais de $Y_{t+1}$ e $X_t$ com o vetor que minimiza o MSE. A condi√ß√£o de n√£o singularidade da matriz $E(X_tX_t')$ √© crucial para garantir a exist√™ncia e unicidade do vetor $\alpha$ e a condi√ß√£o de ortogonalidade entre o erro de previs√£o e as vari√°veis explicativas garante que essa escolha de $\alpha$ minimize o MSE. A an√°lise detalhada neste cap√≠tulo fornece uma base s√≥lida para a compreens√£o do c√°lculo e da interpreta√ß√£o do coeficiente de proje√ß√£o linear, bem como das condi√ß√µes necess√°rias para sua aplica√ß√£o.

### Refer√™ncias
[^1]: Expression [4.1.1], [4.1.2], [4.1.3], [4.1.4], [4.1.5], [4.1.6], [4.1.7], [4.1.8]
[^2]: [4.1.9], [4.1.10], [4.1.11], [4.1.12], [4.1.13]
<!-- END -->
