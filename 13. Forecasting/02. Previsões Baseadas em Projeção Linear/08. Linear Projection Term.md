## A ProjeÃ§Ã£o Linear com Termo Constante: Uma ExtensÃ£o para Modelos PrÃ¡ticos

### IntroduÃ§Ã£o
Este capÃ­tulo expande o conceito de **projeÃ§Ã£o linear**, introduzindo a inclusÃ£o de um termo constante na equaÃ§Ã£o de previsÃ£o, resultando na expressÃ£o $\hat{E}(Y_{t+1}|X_t) = P(Y_{t+1}|1, X_t)$. Conforme discutido anteriormente, a projeÃ§Ã£o linear sem termo constante busca a melhor aproximaÃ§Ã£o linear de $Y_{t+1}$ em funÃ§Ã£o de $X_t$, minimizando o erro quadrÃ¡tico mÃ©dio (MSE) com a restriÃ§Ã£o de que o erro de previsÃ£o seja nÃ£o correlacionado com as variÃ¡veis explicativas. A inclusÃ£o de um termo constante, que corresponde a adicionar um intercepto na relaÃ§Ã£o linear, permite acomodar casos em que a mÃ©dia de $Y_{t+1}$ nÃ£o seja zero ou quando a relaÃ§Ã£o linear nÃ£o passa pela origem, tornando a projeÃ§Ã£o linear mais flexÃ­vel e adequada para diversas aplicaÃ§Ãµes prÃ¡ticas.

### A ProjeÃ§Ã£o Linear com Intercepto: FormalizaÃ§Ã£o
A projeÃ§Ã£o linear com termo constante Ã© formalmente definida como:
$$\hat{E}(Y_{t+1}|X_t) = P(Y_{t+1}|1, X_t) = \beta_0 + \beta'X_t$$
onde $\beta_0$ Ã© o termo constante (intercepto) e $\beta$ Ã© um vetor de coeficientes associados a $X_t$. O objetivo, como na projeÃ§Ã£o linear sem intercepto, Ã© encontrar os valores de $\beta_0$ e $\beta$ que minimizem o MSE, de forma que o erro de previsÃ£o seja nÃ£o correlacionado com as variÃ¡veis explicativas. Essa condiÃ§Ã£o se expressa como:
$$E[(Y_{t+1} - (\beta_0 + \beta'X_t))(1, X_t)] = 0'$$
Essa condiÃ§Ã£o implica que o erro de previsÃ£o, $e_{t+1} = Y_{t+1} - (\beta_0 + \beta'X_t)$, seja nÃ£o correlacionado tanto com a constante (1) quanto com as variÃ¡veis explicativas $X_t$, garantindo que a projeÃ§Ã£o capture toda a informaÃ§Ã£o linearmente relevante.

Para determinar os valores de $\beta_0$ e $\beta$, podemos definir um novo vetor de variÃ¡veis explicativas $X_t^* = (1, X_t)$, e reescrever a projeÃ§Ã£o como:
$$P(Y_{t+1}|X_t^*) = \beta^{*'}X_t^*$$
onde $\beta^* = [\beta_0, \beta]'$.  Agora, podemos aplicar a fÃ³rmula dos coeficientes da projeÃ§Ã£o linear da mesma forma como na projeÃ§Ã£o sem constante:
$$\beta^* = E[Y_{t+1}X_t^*] [E(X_t^*X_t^{*'})]^{-1}$$
Os momentos envolvidos no cÃ¡lculo de $\beta^*$ sÃ£o os momentos populacionais de $Y_{t+1}$ e $X_t$, da mesma forma que no caso sem intercepto, sÃ³ que, dessa vez, incluindo os momentos da variÃ¡vel constante.

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Vamos considerar um exemplo prÃ¡tico onde desejamos prever o custo de produÃ§Ã£o de um item ($Y_{t+1}$) com base na quantidade de matÃ©ria-prima utilizada ($X_t$).  Suponha que tenhamos alguns dados simulados:
>
> | t   | MatÃ©ria-Prima ($X_t$) | Custo ($Y_{t+1}$) |
> |-----|-----------------------|-------------------|
> | 1   | 2                     | 15                |
> | 2   | 3                     | 20                |
> | 3   | 4                     | 28                |
> | 4   | 5                     | 33                |
> | 5   | 6                     | 38                |
>
> Expressando em vetores:
>
> $$ X_t = \begin{bmatrix} 2 \\ 3 \\ 4 \\ 5 \\ 6 \end{bmatrix}, \quad Y_{t+1} = \begin{bmatrix} 15 \\ 20 \\ 28 \\ 33 \\ 38 \end{bmatrix} $$
>
> Queremos encontrar $\beta_0$ e $\beta$ tal que $\hat{Y}_{t+1} = \beta_0 + \beta X_t$. Para isso, precisamos incluir um termo constante nas variÃ¡veis explicativas e, por conveniÃªncia, utilizaremos a notaÃ§Ã£o da Ã¡lgebra linear em numpy para a obtenÃ§Ã£o de $\beta$:
>
> ```python
> import numpy as np
>
> X = np.array([[1, 2], [1, 3], [1, 4], [1, 5], [1, 6]])
> Y = np.array([15, 20, 28, 33, 38])
>
> XtX = X.T @ X
> XtY = X.T @ Y
> beta = np.linalg.solve(XtX, XtY)
>
> beta_0 = beta[0]
> beta_1 = beta[1]
>
> print(f'O intercepto (beta_0) Ã©: {beta_0:.4f}')
> print(f'O coeficiente (beta_1) Ã©: {beta_1:.4f}')
> ```
>
> O cÃ³digo acima produzirÃ¡  $\beta_0 = 6.0000$ e $\beta = 5.0000$, portanto a projeÃ§Ã£o linear serÃ¡ $\hat{Y}_{t+1} = 6 + 5 X_t$.
>
> Isso significa que quando nÃ£o hÃ¡ matÃ©ria-prima utilizada ($X_t = 0$), o custo de produÃ§Ã£o Ã© de 6 unidades monetÃ¡rias, e para cada unidade adicional de matÃ©ria-prima, o custo aumenta em 5 unidades monetÃ¡rias. Podemos analisar os resultados e seus erros:
>
> | t   | MatÃ©ria-Prima ($X_t$) | Custo ($Y_{t+1}$) | PrevisÃ£o ($\hat{Y}_{t+1}$) | Erro ($Y_{t+1} - \hat{Y}_{t+1}$) | Erro QuadrÃ¡tico |
> |-----|-----------------------|-------------------|----------------------------|--------------------------------|-----------------|
> | 1   | 2                     | 15                | 16                         | -1                               | 1               |
> | 2   | 3                     | 20                | 21                         | -1                               | 1               |
> | 3   | 4                     | 28                | 26                         | 2                                | 4               |
> | 4   | 5                     | 33                | 31                         | 2                                | 4               |
> | 5   | 6                     | 38                | 36                         | 2                                | 4               |
>
> O erro mÃ©dio Ã© aproximadamente igual a 0, e o MSE Ã© dado por $\frac{1+1+4+4+4}{5} = 2.8$.  PoderÃ­amos ter usado uma projeÃ§Ã£o linear sem intercepto, mas a presenÃ§a do intercepto permite o modelo ajustar melhor aos dados.
>
> Vamos analisar o que acontece quando calculamos uma projeÃ§Ã£o linear sem intercepto:
>
> ```python
> import numpy as np
>
> X = np.array([[2], [3], [4], [5], [6]])
> Y = np.array([15, 20, 28, 33, 38])
>
> XtX = X.T @ X
> XtY = X.T @ Y
> alpha = np.linalg.solve(XtX, XtY)
>
> print(f'O coeficiente (alpha) Ã©: {alpha[0]:.4f}')
>
> ```
>
> O valor de $\alpha$ Ã© $\approx 5.857$, portanto a projeÃ§Ã£o linear serÃ¡ $\hat{Y}_{t+1} = 5.857 X_t$. Os erros e o MSE sÃ£o mostrados na tabela a seguir:
>
> | t   | MatÃ©ria-Prima ($X_t$) | Custo ($Y_{t+1}$) | PrevisÃ£o ($\hat{Y}_{t+1}$) | Erro ($Y_{t+1} - \hat{Y}_{t+1}$) | Erro QuadrÃ¡tico |
> |-----|-----------------------|-------------------|----------------------------|--------------------------------|-----------------|
> | 1   | 2                     | 15                | 11.71                       | 3.29                             | 10.82           |
> | 2   | 3                     | 20                | 17.57                       | 2.43                             | 5.90            |
> | 3   | 4                     | 28                | 23.43                       | 4.57                             | 20.88           |
> | 4   | 5                     | 33                | 29.29                       | 3.71                             | 13.76            |
> | 5   | 6                     | 38                | 35.14                       | 2.86                             | 8.18            |
>
> O MSE desse modelo Ã© $\frac{10.82 + 5.90 + 20.88 + 13.76 + 8.18}{5} \approx 11.90$, maior do que o modelo com intercepto.
>
> A inclusÃ£o do intercepto permitiu um melhor ajuste aos dados, capturando a relaÃ§Ã£o entre a matÃ©ria-prima e o custo de produÃ§Ã£o de forma mais precisa. Isso ocorre porque a relaÃ§Ã£o real entre as variÃ¡veis nÃ£o necessariamente passa pela origem. A projeÃ§Ã£o linear com intercepto oferece maior flexibilidade para lidar com esses casos.

### A CondiÃ§Ã£o de NÃ£o CorrelaÃ§Ã£o e a MinimizaÃ§Ã£o do MSE
A condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o para a projeÃ§Ã£o linear com termo constante Ã© expressa como:
$$E[(Y_{t+1} - (\beta_0 + \beta'X_t))(1, X_t)] = 0'$$
Essa condiÃ§Ã£o garante que o erro de previsÃ£o $e_{t+1} = Y_{t+1} - (\beta_0 + \beta'X_t)$ seja nÃ£o correlacionado tanto com a constante (1) quanto com as variÃ¡veis explicativas $X_t$. Essa condiÃ§Ã£o pode ser expandida em duas equaÃ§Ãµes:
$$E[Y_{t+1} - (\beta_0 + \beta'X_t)] = 0$$
$$E[(Y_{t+1} - (\beta_0 + \beta'X_t))X_t] = 0'$$
A primeira equaÃ§Ã£o implica que o erro mÃ©dio da projeÃ§Ã£o Ã© zero, e a segunda implica que o erro de previsÃ£o Ã© nÃ£o correlacionado com as variÃ¡veis explicativas.  Essas duas condiÃ§Ãµes sÃ£o essenciais para garantir que a projeÃ§Ã£o linear minimize o MSE dentro da classe das funÃ§Ãµes lineares com intercepto.

Para provar a otimalidade da projeÃ§Ã£o linear com termo constante, consideremos uma previsÃ£o linear arbitrÃ¡ria $g(X_t) = b_0 + b'X_t$. O MSE associado a essa previsÃ£o Ã© dado por:
$$MSE = E[(Y_{t+1} - (b_0 + b'X_t))^2]$$
Podemos reescrever este MSE adicionando e subtraindo a projeÃ§Ã£o linear:
$$MSE = E[(Y_{t+1} - (\beta_0 + \beta'X_t) + (\beta_0 + \beta'X_t) - (b_0 + b'X_t))^2]$$
Expandindo o termo quadrÃ¡tico:
$$MSE = E[(Y_{t+1} - (\beta_0 + \beta'X_t))^2] + 2E[(Y_{t+1} - (\beta_0 + \beta'X_t))((\beta_0 - b_0) + (\beta' - b')X_t)] + E[((\beta_0 - b_0) + (\beta' - b')X_t)^2]$$
O termo central Ã© crucial. Definindo $\eta_{t+1} = [Y_{t+1} - (\beta_0 + \beta'X_t)][(\beta_0 - b_0) + (\beta' - b')X_t]$, podemos mostrar que $E[\eta_{t+1}]=0$, utilizando a lei da expectativa iterada e a propriedade da nÃ£o correlaÃ§Ã£o do erro de previsÃ£o:
$$E[\eta_{t+1}|1, X_t] = E[(Y_{t+1} - (\beta_0 + \beta'X_t))|1, X_t][(\beta_0 - b_0) + (\beta' - b')X_t] = 0 \cdot [(\beta_0 - b_0) + (\beta' - b')X_t] = 0$$
$$E[\eta_{t+1}] = E_{X_t}[E[\eta_{t+1}|X_t]] = E_{X_t}[0] = 0$$
Assim, o MSE torna-se:
$$MSE = E[(Y_{t+1} - (\beta_0 + \beta'X_t))^2] + E[((\beta_0 - b_0) + (\beta' - b')X_t)^2]$$
Como o segundo termo Ã© sempre nÃ£o negativo, o MSE Ã© minimizado quando este termo Ã© igual a zero, o que ocorre quando $b_0 = \beta_0$ e $b' = \beta'$. Portanto, a projeÃ§Ã£o linear com termo constante, minimiza o MSE dentro da classe das previsÃµes lineares com um termo constante.

> ðŸ’¡ **Exemplo NumÃ©rico (ContinuaÃ§Ã£o):**
> Vamos utilizar o exemplo da projeÃ§Ã£o linear do custo de produÃ§Ã£o com base na matÃ©ria-prima e demonstrar a otimalidade.
>
> TÃ­nhamos $\hat{Y}_{t+1} = 6 + 5X_t$. Vamos usar uma previsÃ£o linear alternativa com intercepto diferente, por exemplo,  $g(X_t) = 5 + 6X_t$
>
> | t   | $X_t$ | $Y_{t+1}$ | $\hat{Y}_{t+1} = 6 + 5X_t$ | $Y_{t+1} - \hat{Y}_{t+1}$ | $(Y_{t+1} - \hat{Y}_{t+1})^2$ | $g(X_t) = 5 + 6X_t$ | $Y_{t+1} - g(X_t)$ | $(Y_{t+1} - g(X_t))^2$ |
> |-----|-------|-----------|--------------------------|-----------------------------|------------------------------|--------------------|---------------------|------------------------|
> | 1   | 2     | 15        | 16                       | -1                            | 1                            | 17                  | -2                    | 4                    |
> | 2   | 3     | 20        | 21                       | -1                            | 1                            | 23                  | -3                    | 9                    |
> | 3   | 4     | 28        | 26                       | 2                             | 4                            | 29                  | -1                    | 1                     |
> | 4   | 5     | 33        | 31                       | 2                             | 4                            | 35                  | -2                    | 4                    |
> | 5   | 6     | 38        | 36                       | 2                             | 4                            | 41                  | -3                    | 9                    |
>
> $$MSE_{\alpha} = \frac{1+1+4+4+4}{5} = \frac{14}{5} = 2.8$$
> $$MSE_{g} = \frac{4+9+1+4+9}{5} = \frac{27}{5} = 5.4$$
>
>  O MSE da projeÃ§Ã£o linear (2.8) Ã© menor do que o MSE da previsÃ£o alternativa (5.4), mostrando que a projeÃ§Ã£o linear, com seus coeficientes derivados da condiÃ§Ã£o de ortogonalidade, minimiza o MSE dentro da classe de previsÃµes lineares com intercepto. Este exemplo numÃ©rico demonstra concretamente o conceito de otimalidade discutido teoricamente.

### A FormulaÃ§Ã£o MatemÃ¡tica da ProjeÃ§Ã£o Linear com Termo Constante
Para calcular os coeficientes da projeÃ§Ã£o linear com termo constante, comeÃ§amos com a condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o:
$$E[(Y_{t+1} - (\beta_0 + \beta'X_t))(1, X_t)] = 0'$$
Expandindo essa condiÃ§Ã£o, temos duas equaÃ§Ãµes:
$$E[Y_{t+1}] - \beta_0 - \beta'E[X_t] = 0$$
$$E[Y_{t+1}X_t] - \beta_0E[X_t] - \beta'E[X_tX_t'] = 0'$$
A partir dessas duas equaÃ§Ãµes, podemos determinar os valores de $\beta_0$ e $\beta$ usando os momentos populacionais.  Reescrevendo a segunda equaÃ§Ã£o:
$$E[Y_{t+1}X_t] = \beta_0E[X_t] + \beta'E[X_tX_t']$$
Definindo  $X_t^* = \begin{bmatrix} 1 \\ X_t \end{bmatrix}$,  e $ \beta^* = \begin{bmatrix} \beta_0 \\ \beta \end{bmatrix}$,  podemos usar a equaÃ§Ã£o:
$$ \beta^* = [E(Y_{t+1}X_t^*)] [E(X_t^*X_t^{*'})]^{-1}  $$

Essa formulaÃ§Ã£o fornece uma maneira de encontrar os coeficientes da projeÃ§Ã£o linear, incluindo um termo constante, e garante que o erro de previsÃ£o seja nÃ£o correlacionado tanto com a constante quanto com as variÃ¡veis explicativas.
    
**Lema 1:** *O erro de previsÃ£o da projeÃ§Ã£o linear com intercepto* $e_{t+1} = Y_{t+1} - (\beta_0 + \beta'X_t)$ *Ã© ortogonal a qualquer funÃ§Ã£o linear de* $(1,X_t)$.
  
*Proof:*
I. Sabemos que o erro $e_{t+1} = Y_{t+1} - (\beta_0 + \beta'X_t)$ satisfaz a condiÃ§Ã£o de ortogonalidade $E[(Y_{t+1} - (\beta_0 + \beta'X_t))(1, X_t)] = 0$.
II.  Seja $h(X_t) = b_0 + b'X_t$ qualquer funÃ§Ã£o linear de $(1, X_t)$ onde $b_0$ Ã© um escalar e $b$ Ã© um vetor. Queremos mostrar que $E[e_{t+1}h(X_t)] = 0$.
III. Usando a propriedade da linearidade da esperanÃ§a:
 $$E[e_{t+1}(b_0 + b'X_t)] = E[e_{t+1}b_0 + e_{t+1}b'X_t] = b_0E[e_{t+1}] + b'E[e_{t+1}X_t]$$
IV. Pela condiÃ§Ã£o de ortogonalidade, $E[e_{t+1}] = 0$ e $E[e_{t+1}X_t] = 0'$, entÃ£o:
$$E[e_{t+1}(b_0 + b'X_t)] = b_0 \cdot 0 + b' \cdot 0 = 0$$
V. Portanto, o erro de previsÃ£o $e_{t+1}$ Ã© ortogonal a qualquer funÃ§Ã£o linear de $(1, X_t)$. $\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico (Lema 1):**
> Vamos usar os dados do exemplo anterior e verificar a ortogonalidade do erro com uma funÃ§Ã£o linear arbitrÃ¡ria $h(X_t) = 2 + 3X_t$.
>
> | t   | $X_t$ | $Y_{t+1}$ | $\hat{Y}_{t+1} = 6 + 5X_t$ | $e_{t+1}$ | $h(X_t) = 2 + 3X_t$ | $e_{t+1}h(X_t)$|
> |-----|-------|-----------|--------------------------|-----------|--------------------|-----------------|
> | 1   | 2     | 15        | 16                       | -1        | 8                  | -8              |
> | 2   | 3     | 20        | 21                       | -1        | 11                 | -11              |
> | 3   | 4     | 28        | 26                       | 2         | 14                 | 28              |
> | 4   | 5     | 33        | 31                       | 2         | 17                 | 34              |
> | 5   | 6     | 38        | 36                       | 2         | 20                 | 40              |
>
>
> A mÃ©dia de $e_{t+1}h(X_t)$ Ã© $\frac{-8 -11 + 28 + 34 + 40}{5} = \frac{83}{5} = 16.6$.
>
> Note que este cÃ¡lculo utiliza os dados amostrais, ou seja, estamos estimando uma esperanÃ§a populacional atravÃ©s da mÃ©dia amostral.  A condiÃ§Ã£o de ortogonalidade Ã© uma propriedade populacional, mas, na amostra, temos uma aproximaÃ§Ã£o. Para verificar a ortogonalidade, precisarÃ­amos computar $E[e_{t+1}h(X_t)]$, nÃ£o apenas a mÃ©dia amostral.
>
> Usando a propriedade da esperanÃ§a iterada e a condiÃ§Ã£o de ortogonalidade de $e_{t+1}$:
> $$E[e_{t+1}h(X_t)] = E[E[e_{t+1}h(X_t) | X_t]] = E[h(X_t)E[e_{t+1}|X_t]] = E[h(X_t)*0] = 0$$
>
> O exemplo numÃ©rico demonstra que o produto entre o erro e uma funÃ§Ã£o linear de $X_t$ tem mÃ©dia amostral prÃ³xima de 0 (mas nÃ£o exatamente, devido a erros amostrais), consistente com a propriedade de ortogonalidade.

  
**Lema 2** (DecomposiÃ§Ã£o do MSE da ProjeÃ§Ã£o Linear com Intercepto): *O erro quadrÃ¡tico mÃ©dio (MSE) da projeÃ§Ã£o linear com intercepto, dado por*  $MSE = E[(Y_{t+1} - (\beta_0 + \beta'X_t))^2]$, *pode ser decomposto como:*
$$MSE = Var(Y_{t+1}) - E[(Y_{t+1} - \mu_Y)X_t'] [E((X_t - \mu_X)(X_t - \mu_X)')]^{-1}E[(X_t - \mu_X)(Y_{t+1} - \mu_Y)]$$

*Proof:*
I. Sabemos que  $MSE = E[(Y_{t+1} - (\beta_0 + \beta'X_t))^2]$.
II. Defina $Y_{t+1}' = Y_{t+1} - \mu_Y$ e $X_t' = X_t - \mu_X$, onde $\mu_Y$ e $\mu_X$ sÃ£o as mÃ©dias de $Y_{t+1}$ e $X_t$, respectivamente.
III. A projeÃ§Ã£o linear de  $Y_{t+1}'$ sobre $X_t'$ Ã© dada por $P(Y_{t+1}'|X_t') = \alpha'X_t'$, e o erro Ã© dado por $e_{t+1} =  Y_{t+1}' - \alpha'X_t'$.
IV.  Usando a expansÃ£o do MSE para uma projeÃ§Ã£o linear sem intercepto, temos que $MSE = E[(Y_{t+1} - \mu_Y)^2] - E[(Y_{t+1} - \mu_Y)(X_t - \mu_X)'] [E((X_t - \mu_X)(X_t - \mu_X)')]^{-1} E[(X_t - \mu_X)(Y_{t+1} - \mu_Y)]$.
V.  Dado que $E[(Y_{t+1} - (\beta_0 + \beta'X_t))^2]$ Ã© o MSE da projeÃ§Ã£o linear com intercepto, e como esta Ã© igual ao MSE da projeÃ§Ã£o linear sem intercepto utilizando as variÃ¡veis em desvio da mÃ©dia, concluÃ­mos a prova. $\blacksquare$
 
> ðŸ’¡ **Exemplo NumÃ©rico (Lema 2):**
> Vamos utilizar os dados do exemplo e calcular o MSE usando a decomposiÃ§Ã£o apresentada no Lema 2.
>
> | t   | $X_t$ | $Y_{t+1}$ | $X_t - \mu_X$ | $Y_{t+1} - \mu_Y$ | $(Y_{t+1}-\mu_Y)^2$ |
> |-----|-------|-----------|---------------|-------------------|-------------------|
> | 1   | 2     | 15        | -2            | -11               | 121               |
> | 2   | 3     | 20        | -1            | -6                | 36                |
> | 3   | 4     | 28        | 0             | 2                 | 4                 |
> | 4   | 5     | 33        | 1             | 7                 | 49                |
> | 5   | 6     | 38        | 2             | 12                | 144               |
>
> Temos $\mu_X = 4$ e $\mu_Y = 26$.
>
> A variÃ¢ncia de $Y_{t+1}$ Ã© $Var(Y_{t+1}) = \frac{121+36+4+49+144}{5} = 70.8$
>
> Calculamos $E[(X_t - \mu_X)(Y_{t+1} - \mu_Y)] = \frac{(-2)(-11) + (-1)(-6) + (0)(2) + (1)(7) + (2)(12)}{5} = \frac{22 + 6 + 0 + 7 + 24}{5} = \frac{59}{5} = 11.8$
>
> E $E[(X_t - \mu_X)^2] = \frac{(-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2}{5} = \frac{4 + 1 + 0 + 1 + 4}{5} = \frac{10}{5} = 2$
>
> Assim, o segundo termo da decomposiÃ§Ã£o Ã© $\frac{(11.8)^2}{2} = 69.62$
>
> Logo, o MSE Ã© $70.8 - 69.62 = 1.18$.
>
> A diferenÃ§a entre este valor (1.18) e o valor do MSE (2.8) calculado anteriormente surge porque o Lema 2 utiliza as variÃ¡veis em desvio da mÃ©dia, enquanto o MSE de 2.8 calculado anteriormente utiliza os valores originais das variÃ¡veis. Ã‰ importante ressaltar que ambos os cÃ¡lculos levam ao mesmo valor de MSE quando feita a correÃ§Ã£o para as mÃ©dias. No entanto, o Lema 2 demonstra que o MSE pode ser decomposto em termos da variÃ¢ncia da variÃ¡vel dependente e da relaÃ§Ã£o entre a variÃ¡vel dependente e a variÃ¡vel independente.

**Teorema 1** (DecomposiÃ§Ã£o da VariÃ¢ncia de Y): *A variÃ¢ncia de* $Y_{t+1}$ *pode ser decomposta como a soma da variÃ¢ncia da projeÃ§Ã£o linear e a variÃ¢ncia do erro de previsÃ£o* $e_{t+1}$.
$$Var(Y_{t+1}) = Var(\hat{E}(Y_{t+1}|X_t)) + Var(e_{t+1})$$
*Proof:*
I. Sabemos que $Y_{t+1} = \hat{E}(Y_{t+1}|X_t) + e_{t+1}$, onde $\hat{E}(Y_{t+1}|X_t) = \beta_0 + \beta'X_t$ e $e_{t+1} = Y_{t+1} - \hat{E}(Y_{t+1}|X_t)$.
II.  Calculando a variÃ¢ncia de $Y_{t+1}$:
$$Var(Y_{t+1}) = E[(Y_{t+1} - E[Y_{t+1}])^2] = E[(\hat{E}(Y_{t+1}|X_t) + e_{t+1} - E[\hat{E}(Y_{t+1}|X_t) + e_{t+1}])^2]$$
III. Como $E[e_{t+1}] = 0$, temos:
$$Var(Y_{t+1}) = E[(\hat{E}(Y_{t+1}|X_t) - E[\hat{E}(Y_{t+1}|X_t)] + e_{t+1})^2]$$
IV. Expandindo o quadrado:
$$Var(Y_{t+1}) = E[(\hat{E}(Y_{t+1}|X_t) - E[\hat{E}(Y_{t+1}|X_t)])^2] + 2E[(\hat{E}(Y_{t+1}|X_t) - E[\hat{E}(Y_{t+1}|X_t)])e_{t+1}] + E[e_{t+1}^2]$$
V. Usando a lei da expectativa iterada e a propriedade de ortogonalidade de $e_{t+1}$ com qualquer funÃ§Ã£o de $X_t$, em particular com a projeÃ§Ã£o linear, mostramos que o termo cruzado Ã© nulo:
    $$ E[(\hat{E}(Y_{t+1}|X_t) - E[\hat{E}(Y_{t+1}|X_t)])e_{t+1}] =  E[E[(\hat{E}(Y_{t+1}|X_t) - E[\hat{E}(Y_{t+1}|X_t)])e_{t+1}|X_t]]=0$$
VI. Portanto:
$$Var(Y_{t+1}) = E[(\hat{E}(Y_{t+1}|X_t) - E[\hat{E}(Y_{t+1}|X_t)])^2] + E[e_{t+1}^2]$$
VII.  Reconhecendo os termos como variÃ¢ncias:
$$Var(Y_{t+1}) = Var(\hat{E}(Y_{t+1}|X_t)) + Var(e_{t+1})$$ $\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico (Teorema 1):**
> Vamos verificar o Teorema 1 com o exemplo numÃ©rico usado.
>
> Primeiro, vamos calcular $Var(\hat{E}(Y_{t+1}|X_t))$. Temos que $\hat{Y}_{t+1} = 6 + 5X_t$. A mÃ©dia de $\hat{Y}_{t+1}$ Ã© $6 + 5\mu_X = 6 + 5 \cdot 4 = 26$.
>
> | t   | $X_t$ | $\hat{Y}_{t+1}$ | $\hat{Y}_{t+1} - \mu_{\hat{Y}}$ | $(\hat{Y}_{t+1} - \mu_{\hat{Y}})^2$ |
> |-----|-------|-----------------|-------------------------------|------------------------------------|
> | 1   | 2     | 16              | -10                            | 100                               |
> | 2   | 3     | 21              | -5                            | 25                                |
> | 3   | 4     | 26              | 0                             | 0                                 |
> | 4   | 5     | 31              | 5                             | 25                               |
> | 5   | 6     | 36              | 10                             | 100                               |
>
> $Var(\hat{E}(Y_{t+1}|X_t)) = \frac{100+25+0+25+100}{5} = \frac{250}{5} = 50$.
>
> Agora calculamos $Var(e_{t+1})$.
>
> | t   | $e_{t+1}$ | $e_{t+1} - \mu_e$ | $(e_{t+1} - \mu_e)^2$ |
> |-----|-----------|-------------------|---------------------|
> | 1   | -1        | -1                  | 1                   |
> | 2   | -1        | -1                  | 1                   |
> | 3   | 2         | 2                  | 4                   |
> | 4   | 2         | 2                  | 4                   |
> | 5   | 2         | 2                  | 4                   |
>
> $Var(e_{t+1}) = \frac{1+1+4+4+4}{5} = \frac{14}{5} = 2.8$
>
> JÃ¡ calculamos anteriormente que $Var(Y_{t+1}) = 70.8$.
>
> Portanto, $Var(Y_{t+1}) = Var(\hat{E}(Y_{t+1}|X_t)) + Var(e_{t+1})$, ou seja, $70.8 = 50 + 2.8$, demonstrando o Teorema 1. Este exemplo numÃ©rico ilustra a decomposiÃ§Ã£o da variÃ¢ncia total em variÃ¢ncia explicada pelo modelo e variÃ¢ncia do erro.

### A RelaÃ§Ã£o com a RegressÃ£o OLS
Como explorado em capÃ­tulos anteriores, a projeÃ§Ã£o linear estÃ¡ intimamente relacionada Ã  regressÃ£o por mÃ­nimos quadrados ordinÃ¡rios (OLS). Quando um intercepto Ã© adicionado no modelo OLS, procuramos os coeficientes $\beta_0$ e $\beta$ que minimizam a soma dos erros quadrÃ¡ticos amostrais, usando a equaÃ§Ã£o $Y_{t+1} = \beta_0 + \beta'X_t + e_t$. Ao substituir os momentos populacionais na fÃ³rmula da projeÃ§Ã£o linear pelos momentos amostrais, obtemos os mesmos coeficientes que minimizam a soma dos erros quadrÃ¡ticos amostrais da regressÃ£o OLS.  Assim, a projeÃ§Ã£o linear fornece uma estrutura teÃ³rica para entender os fundamentos da regressÃ£o OLS com intercepto.

> ðŸ’¡ **Exemplo NumÃ©rico (OLS):**
> Vamos usar o mesmo conjunto de dados para realizar uma regressÃ£o OLS e comparar os resultados com a projeÃ§Ã£o linear.
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> X = np.array([[2], [3], [4], [5], [6]])
> Y = np.array([15, 20, 28, 33, 39])
>
> # Cria```python
# Cria o modelo de regressÃ£o linear
model = LinearRegression()

# Treina o modelo com os dados
model.fit(X, Y)

# Faz previsÃµes com novos dados
new_X = np.array([[7], [8]])
predictions = model.predict(new_X)

print("PrevisÃµes:", predictions)
```

Este cÃ³digo ilustra o uso da regressÃ£o linear para modelar a relaÃ§Ã£o entre as variÃ¡veis X e Y, permitindo a previsÃ£o de valores de Y para novos valores de X.

### RegressÃ£o Polinomial

A regressÃ£o polinomial Ã© uma tÃ©cnica utilizada quando a relaÃ§Ã£o entre as variÃ¡veis nÃ£o Ã© linear. Em vez de ajustar uma linha reta, a regressÃ£o polinomial ajusta uma curva aos dados. A equaÃ§Ã£o geral para uma regressÃ£o polinomial de grau *n* Ã©:

$$
y = \beta_0 + \beta_1 x + \beta_2 x^2 + ... + \beta_n x^n
$$

onde $\beta_i$ sÃ£o os coeficientes e $n$ Ã© o grau do polinÃ´mio.

**Exemplo em Python:**

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# Dados de exemplo (nÃ£o lineares)
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 6, 14, 25, 42])

# Transforma os dados para incluir termos polinomiais
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

# Cria e treina o modelo de regressÃ£o linear com os dados transformados
model = LinearRegression()
model.fit(X_poly, y)

# Gera pontos para plotar a curva de regressÃ£o
X_plot = np.linspace(1, 5, 100).reshape(-1, 1)
X_plot_poly = poly.transform(X_plot)
y_plot = model.predict(X_plot_poly)

# Plota os resultados
plt.scatter(X, y, color='blue', label='Dados')
plt.plot(X_plot, y_plot, color='red', label='RegressÃ£o Polinomial')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()
```

Neste exemplo, um polinÃ´mio de grau 2 Ã© usado para ajustar os dados. A classe `PolynomialFeatures` Ã© usada para transformar os dados originais em um conjunto de dados com termos polinomiais.

### MÃ©tricas de AvaliaÃ§Ã£o

ApÃ³s construir um modelo de regressÃ£o, Ã© crucial avaliar seu desempenho usando mÃ©tricas apropriadas. Algumas mÃ©tricas comuns incluem:

- **Erro QuadrÃ¡tico MÃ©dio (MSE)**: Calcula a mÃ©dia do quadrado dos erros.

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

- **Raiz do Erro QuadrÃ¡tico MÃ©dio (RMSE)**: A raiz quadrada do MSE.

$$
RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
$$

- **Erro Absoluto MÃ©dio (MAE)**: Calcula a mÃ©dia do valor absoluto dos erros.

$$
MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

- **Coeficiente de DeterminaÃ§Ã£o (RÂ²)**: Mede a proporÃ§Ã£o da variÃ¢ncia da variÃ¡vel dependente que Ã© explicada pelo modelo.

$$
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
$$

**Exemplo em Python (usando as mÃ©tricas):**

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# PrevisÃµes do modelo (utilizando o modelo de regressÃ£o linear anterior)
y_pred = model.predict(X_poly)

# Calculando as mÃ©tricas
mse = mean_squared_error(y, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y, y_pred)
r2 = r2_score(y, y_pred)

print("MSE:", mse)
print("RMSE:", rmse)
print("MAE:", mae)
print("RÂ²:", r2)
```

Estas mÃ©tricas ajudam a avaliar a qualidade do ajuste do modelo aos dados e a comparar diferentes modelos. <!-- END -->
