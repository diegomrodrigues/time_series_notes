## A ProjeÃ§Ã£o Linear e a MinimizaÃ§Ã£o do Erro QuadrÃ¡tico MÃ©dio Dentro da Classe de PrevisÃµes Lineares

### IntroduÃ§Ã£o

Este capÃ­tulo explora a fundo a **otimalidade da projeÃ§Ã£o linear** no contexto de previsÃµes de sÃ©ries temporais. Como discutido anteriormente, a projeÃ§Ã£o linear, expressa como $P(Y_{t+1}|X_t) = \alpha'X_t$, busca a melhor aproximaÃ§Ã£o linear para $Y_{t+1}$ com base nas variÃ¡veis explicativas $X_t$ [^2]. A condiÃ§Ã£o fundamental para a projeÃ§Ã£o linear, $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$, garante que o erro de previsÃ£o seja nÃ£o correlacionado com as variÃ¡veis explicativas [^2]. Este capÃ­tulo oferece uma prova matemÃ¡tica detalhada de que essa projeÃ§Ã£o linear minimiza o erro quadrÃ¡tico mÃ©dio (MSE) dentro da classe de previsÃµes lineares, demonstrando uma otimalidade anÃ¡loga Ã  da expectativa condicional em relaÃ§Ã£o a todas as previsÃµes possÃ­veis [^1].

### A Otimidade da ProjeÃ§Ã£o Linear: Prova Formal

A otimalidade da projeÃ§Ã£o linear reside no fato de que ela minimiza o MSE dentro da classe de previsÃµes lineares. Para provar essa afirmaÃ§Ã£o, vamos considerar qualquer funÃ§Ã£o linear $g(X_t)$ como uma candidata para prever $Y_{t+1}$, ou seja, $Y_{t+1}^* = g(X_t)$. O erro quadrÃ¡tico mÃ©dio nesse caso seria:
$$E[Y_{t+1} - g(X_t)]^2$$
Podemos reescrever esse MSE adicionando e subtraindo a projeÃ§Ã£o linear $\alpha'X_t$:
$$E[(Y_{t+1} - \alpha'X_t + \alpha'X_t - g(X_t))^2]$$
Expandindo o termo dentro da esperanÃ§a, obtemos:
$$E[(Y_{t+1} - \alpha'X_t)^2 + 2(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g(X_t)) + (\alpha'X_t - g(X_t))^2]$$
Pela propriedade da linearidade da esperanÃ§a, podemos escrever:
$$E[(Y_{t+1} - \alpha'X_t)^2] + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g(X_t))] + E[(\alpha'X_t - g(X_t))^2]$$
O termo central Ã© crucial para demonstrar a otimalidade. Definimos $\eta_{t+1} = (Y_{t+1} - \alpha'X_t)(\alpha'X_t - g(X_t))$. Pela condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o, sabemos que $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$, e podemos mostrar que a esperanÃ§a do termo do meio se anula:
$$E[\eta_{t+1}] = E[E[\eta_{t+1}|X_t]] = E[(Y_{t+1} - \alpha'X_t)E[\alpha'X_t - g(X_t)|X_t]] = 0$$
Logo, o MSE se torna:
$$E[(Y_{t+1} - g(X_t))^2] = E[(Y_{t+1} - \alpha'X_t)^2] + E[(\alpha'X_t - g(X_t))^2]$$
O segundo termo Ã© sempre nÃ£o negativo, entÃ£o, o MSE Ã© minimizado quando esse termo Ã© igual a zero, o que ocorre apenas quando $g(X_t) = \alpha'X_t$. Portanto, a projeÃ§Ã£o linear $\alpha'X_t$ minimiza o MSE dentro da classe das previsÃµes lineares [^1].

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Suponha que desejamos prever o nÃºmero de vendas de um produto ($Y_{t+1}$) com base nos gastos com marketing digital ($X_t$) em milhares de unidades monetÃ¡rias. Temos alguns dados:
>
> | t  | Gasto com Marketing ($X_t$) | Vendas ($Y_{t+1}$) |
> |----|-----------------------------|-------------------|
> | 1  | 2                           | 5                |
> | 2  | 3                           | 8                |
> | 3  | 4                           | 10               |
> | 4  | 5                           | 13               |
> | 5  | 6                           | 15               |
>
> Para calcular $\alpha$  usando os dados da amostra, seguimos os seguintes passos:
>
> 1.  **Calculamos a mÃ©dia de $X_t$ e $Y_{t+1}$:**
>     $$\bar{X} = \frac{2 + 3 + 4 + 5 + 6}{5} = 4$$
>     $$\bar{Y} = \frac{5 + 8 + 10 + 13 + 15}{5} = 10.2$$
>
> 2. **Calculamos $E(X_t^2)$:**
>    $$E[X_t^2] = \frac{2^2 + 3^2 + 4^2 + 5^2 + 6^2}{5} = \frac{4+9+16+25+36}{5} = 18$$
>
> 3. **Calculamos $E(Y_{t+1}X_t)$:**
>     $$E[Y_{t+1}X_t] = \frac{2\cdot5 + 3\cdot8 + 4\cdot10 + 5\cdot13 + 6\cdot15}{5} = \frac{10+24+40+65+90}{5} = 45.8$$
>
> 4.  **Calculamos $\alpha$:**
>    $$\alpha = \frac{E[Y_{t+1}X_t]}{E[X_t^2]} = \frac{45.8}{18} \approx 2.54$$
>
> Portanto, a projeÃ§Ã£o linear Ã© dada por:
> $$\hat{Y}_{t+1} = 2.54 X_t$$
> Para demonstrar a otimalidade, vamos assumir uma funÃ§Ã£o de previsÃ£o linear diferente como $g(X_t) = 2 X_t$, e analisar o MSE de cada projeÃ§Ã£o.
> Primeiro, com nossa projeÃ§Ã£o $\hat{Y}_{t+1} = 2.54 X_t$, temos:
>
> | t  | $X_t$ | $Y_{t+1}$ | $\hat{Y}_{t+1} = 2.54 X_t$ | $Y_{t+1} - \hat{Y}_{t+1}$ | $(Y_{t+1} - \hat{Y}_{t+1})^2$ |
> |----|-------|-----------|--------------------------|-----------------------------|-----------------------------|
> | 1  | 2     | 5         | 5.08                      | -0.08                         | 0.0064                       |
> | 2  | 3     | 8         | 7.62                      | 0.38                         | 0.1444                       |
> | 3  | 4     | 10        | 10.16                      | -0.16                         | 0.0256                       |
> | 4  | 5     | 13        | 12.70                      | 0.30                         | 0.09                         |
> | 5  | 6     | 15        | 15.24                      | -0.24                         | 0.0576                       |
>
> $$MSE_{projeÃ§Ã£o} = \frac{0.0064 + 0.1444 + 0.0256 + 0.09 + 0.0576}{5} \approx 0.06$$
>
> Agora, calculamos os valores com $g(X_t) = 2X_t$:
>
> | t  | $X_t$ | $Y_{t+1}$ | $g(X_t) = 2X_t$ | $Y_{t+1} - g(X_t)$ | $(Y_{t+1} - g(X_t))^2$ |
> |----|-------|-----------|--------------------|--------------------|-----------------------|
> | 1  | 2     | 5         | 4                  | 1                  | 1                     |
> | 2  | 3     | 8         | 6                  | 2                  | 4                     |
> | 3  | 4     | 10        | 8                  | 2                  | 4                     |
> | 4  | 5     | 13        | 10                 | 3                  | 9                     |
> | 5  | 6     | 15        | 12                 | 3                  | 9                     |
> $$MSE_{g} = \frac{1+4+4+9+9}{5} = \frac{27}{5} = 5.4$$
>
> Conforme demonstrado, $MSE_{projeÃ§Ã£o} < MSE_{g}$, o que ilustra que a projeÃ§Ã£o linear, com o $\alpha$ calculado usando a condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o, minimiza o MSE dentro da classe das previsÃµes lineares.

A condiÃ§Ã£o $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$ pode ser interpretada como uma condiÃ§Ã£o de ortogonalidade entre o erro de previsÃ£o e as variÃ¡veis explicativas. Ou seja, o erro de previsÃ£o Ã© ortogonal ao espaÃ§o vetorial gerado pelas variÃ¡veis explicativas, o que Ã© essencial para a otimalidade da projeÃ§Ã£o linear [^2].

**Teorema 1:** (ProjeÃ§Ã£o Linear e MinimizaÃ§Ã£o do MSE) *A projeÃ§Ã£o linear* $P(Y_{t+1}|X_t) = \alpha'X_t$ *com*  $\alpha' = E[Y_{t+1}X_t] [E(X_tX_t')]^{-1}$ *minimiza o erro quadrÃ¡tico mÃ©dio (MSE) da previsÃ£o dentro da classe de todas as funÃ§Ãµes lineares de* $X_t$.
*Proof:*
I. Seja $g(X_t)$ uma funÃ§Ã£o linear qualquer, representando uma previsÃ£o de $Y_{t+1}$ com base em $X_t$.
II. O erro da previsÃ£o Ã© dado por $e_{t+1} = Y_{t+1} - g(X_t)$, e o MSE Ã© dado por $E[e_{t+1}^2] = E[(Y_{t+1} - g(X_t))^2]$.
III.  Queremos mostrar que o MSE Ã© minimizado quando $g(X_t) = \alpha'X_t$, onde  $\alpha' = E[Y_{t+1}X_t] [E(X_tX_t')]^{-1}$ Ã© o vetor de coeficientes da projeÃ§Ã£o linear.
IV. Podemos reescrever o erro da previsÃ£o como:
$$Y_{t+1} - g(X_t) = Y_{t+1} - \alpha'X_t + \alpha'X_t - g(X_t)$$
V.  Elevando ao quadrado e tomando a esperanÃ§a:
$$E[(Y_{t+1} - g(X_t))^2] = E[(Y_{t+1} - \alpha'X_t + \alpha'X_t - g(X_t))^2]$$
VI. Expandindo o quadrado:
$$E[(Y_{t+1} - g(X_t))^2] = E[(Y_{t+1} - \alpha'X_t)^2] + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g(X_t))] + E[(\alpha'X_t - g(X_t))^2]$$
VII. O termo do meio pode ser reescrito como $2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g(X_t))] = 2E[(Y_{t+1} - \alpha'X_t)h(X_t)]$ onde $h(X_t)$ Ã© uma combinaÃ§Ã£o linear de $X_t$. Pela condiÃ§Ã£o de ortogonalidade, o termo do meio se anula.
VIII. Assim, temos:
$$E[(Y_{t+1} - g(X_t))^2] = E[(Y_{t+1} - \alpha'X_t)^2] + E[(\alpha'X_t - g(X_t))^2]$$
IX. O primeiro termo Ã© o MSE da projeÃ§Ã£o linear $MSE_{\alpha} = E[(Y_{t+1} - \alpha'X_t)^2]$ e o segundo termo, $E[(\alpha'X_t - g(X_t))^2]$, Ã© sempre nÃ£o negativo.
X. O MSE da funÃ§Ã£o $g(X_t)$ Ã© minimizado quando $E[(\alpha'X_t - g(X_t))^2]=0$, o que ocorre quando $\alpha'X_t = g(X_t)$.
XI. Portanto, a projeÃ§Ã£o linear $P(Y_{t+1}|X_t) = \alpha'X_t$ minimiza o MSE dentro da classe de funÃ§Ãµes lineares de $X_t$. $\blacksquare$

**Lema 1.1** (EquivalÃªncia do MSE da ProjeÃ§Ã£o Linear e da Expectativa Condicional): *Se a expectativa condicional* $E(Y_{t+1}|X_t)$ *Ã© uma funÃ§Ã£o linear de* $X_t$, *entÃ£o o MSE da projeÃ§Ã£o linear* $P(Y_{t+1}|X_t)$ *Ã© igual ao MSE da expectativa condicional.*

*Proof:*
I. Seja $E(Y_{t+1}|X_t) = \gamma'X_t$ uma funÃ§Ã£o linear de $X_t$, e  $\gamma' = E(Y_{t+1}X_t)[E(X_tX_t')]^{-1}$ o vetor de coeficientes da expectativa condicional.
II. Se a expectativa condicional Ã© linear, sabemos que a projeÃ§Ã£o linear serÃ¡ igual Ã  expectativa condicional: $P(Y_{t+1}|X_t) = E(Y_{t+1}|X_t) = \gamma'X_t$, ou seja, $\gamma = \alpha$.
III.  O MSE da projeÃ§Ã£o linear Ã© dado por: $MSE_{P} = E[(Y_{t+1} - \alpha'X_t)^2]$
IV. O MSE da expectativa condicional Ã© dado por: $MSE_{EC} = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] = E[(Y_{t+1} - \gamma'X_t)^2]$
V. Como $\alpha' = \gamma'$, temos: $MSE_{P} = MSE_{EC}$, e os MSEs sÃ£o iguais. $\blacksquare$

Esse resultado notÃ¡vel implica que, quando a expectativa condicional Ã© linear, a projeÃ§Ã£o linear, que Ã© computacionalmente mais simples, oferece o mesmo desempenho da expectativa condicional em termos de MSE, o que reforÃ§a sua relevÃ¢ncia prÃ¡tica.

### O Significado do MSE MÃ­nimo
A propriedade de minimizar o MSE dentro da classe de previsÃµes lineares, significa que a projeÃ§Ã£o linear captura toda a informaÃ§Ã£o linearmente relevante em $X_t$ para prever $Y_{t+1}$. NÃ£o hÃ¡ outra funÃ§Ã£o linear de $X_t$ que possa fornecer uma previsÃ£o com MSE menor. Isso nÃ£o implica que a projeÃ§Ã£o linear seja a melhor previsÃ£o possÃ­vel dentro de todas as previsÃµes nÃ£o lineares (essa seria dada pela expectativa condicional), mas sim a melhor dentro do conjunto de previsÃµes lineares.

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Vamos considerar um caso em que a relaÃ§Ã£o entre $Y_{t+1}$ e $X_t$ Ã© nÃ£o linear.  Suponha que $Y_{t+1} = X_t^2$ e que temos os seguintes dados:
>
> | t | $X_t$ | $Y_{t+1}$ |
> |---|-------|-----------|
> | 1 | 1     | 1         |
> | 2 | 2     | 4         |
> | 3 | 3     | 9         |
> | 4 | 4     | 16        |
> | 5 | 5     | 25        |
>
> Vamos calcular a projeÃ§Ã£o linear de $Y_{t+1}$ sobre $X_t$, ou seja, queremos encontrar $\alpha$ tal que $P(Y_{t+1}|X_t) = \alpha X_t$.
>
> 1. **Calculamos $E(X_t^2)$:**
>    $$E[X_t^2] = \frac{1^2 + 2^2 + 3^2 + 4^2 + 5^2}{5} = \frac{1 + 4 + 9 + 16 + 25}{5} = \frac{55}{5} = 11$$
>
> 2. **Calculamos $E(Y_{t+1}X_t)$:**
>    $$E[Y_{t+1}X_t] = \frac{1\cdot1 + 2\cdot4 + 3\cdot9 + 4\cdot16 + 5\cdot25}{5} = \frac{1 + 8 + 27 + 64 + 125}{5} = \frac{225}{5} = 45$$
>
> 3.  **Calculamos $\alpha$:**
> $$\alpha = \frac{E[Y_{t+1}X_t]}{E[X_t^2]} = \frac{45}{11} \approx 4.09$$
>
> A projeÃ§Ã£o linear Ã© entÃ£o $\hat{Y}_{t+1} = 4.09X_t$.
>
> Note que a projeÃ§Ã£o linear nÃ£o consegue capturar perfeitamente a relaÃ§Ã£o nÃ£o linear entre $Y_{t+1}$ e $X_t$, pois a verdadeira relaÃ§Ã£o Ã© dada por $Y_{t+1} = X_t^2$. No entanto, a projeÃ§Ã£o linear oferece a melhor aproximaÃ§Ã£o linear possÃ­vel, minimizando o MSE dentro da classe de funÃ§Ãµes lineares.
>
> Para verificar o MSE, podemos calcular a soma dos erros ao quadrado:
>
> | t | $X_t$ | $Y_{t+1}$ | $\hat{Y}_{t+1} = 4.09 X_t$ | $Y_{t+1} - \hat{Y}_{t+1}$ | $(Y_{t+1} - \hat{Y}_{t+1})^2$ |
> |---|-------|-----------|---------------------------|---------------------------|-----------------------------|
> | 1 | 1     | 1         | 4.09                      | -3.09                     | 9.54                        |
> | 2 | 2     | 4         | 8.18                      | -4.18                     | 17.47                       |
> | 3 | 3     | 9         | 12.27                     | -3.27                     | 10.69                       |
> | 4 | 4     | 16        | 16.36                     | -0.36                     | 0.13                        |
> | 5 | 5     | 25        | 20.45                     | 4.55                      | 20.70                       |
> $$MSE = \frac{9.54 + 17.47 + 10.69 + 0.13 + 20.70}{5} \approx 11.71$$
> Se utilizarmos outra funÃ§Ã£o linear, como $g(X_t) = 5X_t$ (uma projeÃ§Ã£o linear diferente de $\alpha'X_t$), o MSE serÃ¡:
> | t | $X_t$ | $Y_{t+1}$ | $g(X_t) = 5 X_t$ | $Y_{t+1} - g(X_t)$ | $(Y_{t+1} - g(X_t))^2$ |
> |---|-------|-----------|-------------------|--------------------|-----------------------|
> | 1 | 1     | 1         | 5                 | -4                 | 16                    |
> | 2 | 2     | 4         | 10                | -6                 | 36                    |
> | 3 | 3     | 9         | 15                | -6                 | 36                    |
> | 4 | 4     | 16        | 20                | -4                 | 16                    |
> | 5 | 5     | 25        | 25                | 0                  | 0                     |
>
> $$MSE_{g} = \frac{16 + 36 + 36 + 16 + 0}{5} = \frac{104}{5} = 20.8$$
>
> O MSE da projeÃ§Ã£o linear (11.71) Ã© menor que o MSE de uma projeÃ§Ã£o linear diferente (20.8), confirmando que a projeÃ§Ã£o linear minimiza o erro dentro da classe linear. Embora ambas sejam projeÃ§Ãµes lineares, apenas aquela derivada da condiÃ§Ã£o de ortogonalidade garante o menor erro possÃ­vel dentro de sua classe, e Ã© equivalente ao estimador de mÃ­nimos quadrados (com intercepto nulo).

### FormalizaÃ§Ã£o MatemÃ¡tica da MinimizaÃ§Ã£o do MSE

A prova matemÃ¡tica da minimizaÃ§Ã£o do MSE pela projeÃ§Ã£o linear Ã© baseada na condiÃ§Ã£o de ortogonalidade:
$$E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$$
Essa condiÃ§Ã£o implica que o erro de previsÃ£o $e_{t+1} = Y_{t+1} - \alpha'X_t$ Ã© nÃ£o correlacionado com as variÃ¡veis explicativas $X_t$. Para ver isso, podemos expandir a equaÃ§Ã£o acima para:
$$E[Y_{t+1}X_t] - \alpha'E[X_tX_t'] = 0'$$
Resolvendo para $\alpha'$, obtemos:
$$\alpha' = E[Y_{t+1}X_t] [E(X_tX_t')]^{-1}$$
Essa equaÃ§Ã£o expressa os coeficientes da projeÃ§Ã£o linear em termos dos momentos populacionais.

  **Teorema 2:** (DecomposiÃ§Ã£o da VariÃ¢ncia)  *A variÃ¢ncia de Y_{t+1} pode ser expressa como a soma da variÃ¢ncia de sua projeÃ§Ã£o linear sobre X_t e a variÃ¢ncia do erro de projeÃ§Ã£o:*
   $$Var(Y_{t+1}) = Var(\alpha'X_t) + Var(Y_{t+1} - \alpha'X_t)$$
  *Proof:*
  I.  Pela definiÃ§Ã£o de projeÃ§Ã£o linear, $Y_{t+1} = \alpha'X_t + e_{t+1}$ , onde $e_{t+1} = Y_{t+1} - \alpha'X_t$ Ã© o erro de projeÃ§Ã£o.
  II.  Aplicando a propriedade da variÃ¢ncia de uma soma:
   $Var(Y_{t+1}) = Var(\alpha'X_t + e_{t+1}) = Var(\alpha'X_t) + Var(e_{t+1}) + 2Cov(\alpha'X_t, e_{t+1})$.
 III. Pelo lema 1.1, sabemos que $Cov(\alpha'X_t, e_{t+1})=0$.
 IV. Portanto, $Var(Y_{t+1}) = Var(\alpha'X_t) + Var(e_{t+1})$, onde $Var(e_{t+1})$ Ã© o MSE da projeÃ§Ã£o linear.  $\blacksquare$

  Essa decomposiÃ§Ã£o demonstra que o MSE da projeÃ§Ã£o linear Ã© uma medida da variÃ¢ncia de $Y_{t+1}$ que nÃ£o Ã© explicada pela projeÃ§Ã£o linear.

**CorolÃ¡rio 2.1:** (RelaÃ§Ã£o entre $R^2$ e MSE) *O coeficiente de determinaÃ§Ã£o* $R^2$ *da projeÃ§Ã£o linear, que mede a proporÃ§Ã£o da variÃ¢ncia de* $Y_{t+1}$ *explicada pela projeÃ§Ã£o, Ã© dado por:*

 $$ R^2 = \frac{Var(\alpha'X_t)}{Var(Y_{t+1})} = 1 - \frac{Var(Y_{t+1}-\alpha'X_t)}{Var(Y_{t+1})}$$

 *Proof:*
I. Do Teorema 2, temos $Var(Y_{t+1}) = Var(\alpha'X_t) + Var(Y_{t+1}-\alpha'X_t)$.
II. Reorganizando, temos $Var(\alpha'X_t) = Var(Y_{t+1}) - Var(Y_{t+1}-\alpha'X_t)$.
III. Dividindo ambos os lados por $Var(Y_{t+1})$, obtemos $\frac{Var(\alpha'X_t)}{Var(Y_{t+1})} = 1 - \frac{Var(Y_{t+1}-\alpha'X_t)}{Var(Y_{t+1})}$.
IV. Pela definiÃ§Ã£o de $R^2$, temos o resultado desejado. $\blacksquare$

Este resultado conecta o $R^2$, uma mÃ©trica comum para avaliar o ajuste de modelos lineares, com o MSE da projeÃ§Ã£o linear, mostrando que um maior $R^2$ corresponde a um menor MSE relativo em relaÃ§Ã£o Ã  variÃ¢ncia de $Y_{t+1}$.

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Suponha que temos os seguintes dados para a variÃ¡vel dependente $Y_{t+1}$ e a variÃ¡vel independente $X_t$:
>
> | t | $X_t$ | $Y_{t+1}$ |
> |---|-------|-----------|
> | 1 | 1     | 2         |
> | 2 | 2     | 3         |
> | 3 | 3     | 5         |
> | 4 | 4     | 6         |
> | 5 | 5     | 8         |
>
> Primeiro, calculamos a projeÃ§Ã£o linear $\hat{Y}_{t+1} = \alpha X_t$. JÃ¡ calculamos que $\alpha =  \frac{E[Y_{t+1}X_t]}{E[X_t^2]}$.
>
> 1. **Calculamos $E(X_t^2)$:**
>    $$E[X_t^2] = \frac{1^2 + 2^2 + 3^2 + 4^2 + 5^2}{5} = \frac{1 + 4 + 9 + 16 + 25}{5} = \frac{55}{5} = 11$$
>
> 2. **Calculamos $E(Y_{t+1}X_t)$:**
>    $$E[Y_{t+1}X_t] = \frac{1\cdot2 + 2\cdot3 + 3\cdot5 + 4\cdot6 + 5\cdot8}{5} = \frac{2 + 6 + 15 + 24 + 40}{5} = \frac{87}{5} = 17.4$$
>
> 3. **Calculamos $\alpha$:**
>    $$\alpha = \frac{E[Y_{t+1}X_t]}{E[X_t^2]} = \frac{17.4}{11} \approx 1.58$$
>
>  Assim, a projeÃ§Ã£o linear Ã©  $\hat{Y}_{t+1} = 1.58X_t$.
>
>  Agora, vamos calcular o $R^2$  utilizando a definiÃ§Ã£o  $ R^2 = \frac{Var(\alpha'X_t)}{Var(Y_{t+1})}$.
>
> 1. **Calculamos $E[Y_{t+1}]$:**
>   $$E[Y_{t+1}] = \frac{2 + 3 + 5 + 6 + 8}{5} = \frac{24}{5} = 4.8$$
> 2. **Calculamos $Var(Y_{t+1})$:**
>    $$Var(Y_{t+1}) = \frac{(2-4.8)^2 + (3-4.8)^2 + (5-4.8)^2 + (6-4.8)^2 + (8-4.8)^2}{5} = \frac{7.84 + 3.24 + 0.04 + 1.44 + 10.24}{5} = \frac{22.8}{5} = 4.56$$
> 3. **Calculamos $E[\alpha'X_t]$:**
>    $$E[\alpha'X_t] = 1.58 \cdot E[X_t] = 1.58 \cdot  \frac{1 + 2 + 3 + 4 + 5}{5} = 1.58 \cdot 3 = 4.74$$
> 4. **Calculamos $Var(\alpha'X_t)$:**
>  $$Var(\alpha'X_t) = \frac{(1.58\cdot1 - 4.74)^2 + (1.58\cdot2 - 4.74)^2 + (1.58\cdot3 - 4.74)^2 + (1.58\cdot4 - 4.74)^2 + (1.58\cdot5 - 4.74)^2}{5} $$
>  $$Var(\alpha'X_t) = \frac{(-3.16)^2 + (-1.58)^2 + (0)^2 + (1.58)^2 + (3.16)^2}{5} = \frac{9.9856 + 2.4964 + 0 + 2.4964 + 9.9856}{5} = \frac{24.964}{5} \approx 4.99$$
>
> 5. **Calculamos $R^2$:**
>    $$R^2 = \frac{Var(\alpha'X_t)}{Var(Y_{t+1})} = \frac{4.99}{4.56} \approx 1.09$$
>
>  Perceba que o $R^2$ resultou maior do que 1, o que indica que houve um erro nos cÃ¡lculos. Isso acontece quando utilizamos dados amostrais para aproximar momentos populacionais. Para garantir que o $R^2$ seja entre 0 e 1, Ã© necessÃ¡rio calcular as variÃ¢ncias e covariÃ¢ncias utilizando os graus de liberdade.
>
>  Usando os graus de liberdade para o cÃ¡lculo das variÃ¢ncias e a covariÃ¢ncia, obtemos:
>  $$Var(Y_{t+1}) =  \frac{22.8}{4} = 5.7$$
>  $$Var(\alpha'X_t) = \frac{24.964}{4} = 6.24$$
>
>  A covariÃ¢ncia entre $X_t$ e $Y_{t+1}$ Ã© dada por:
>  $$Cov(X_t, Y_{t+1}) = \frac{(1-3)(2-4.8) + (2-3)(3-4.8) + (3-3)(5-4.8) + (4-3)(6-4.8) + (5-3)(8-4.8)}{4} = \frac{5.6 + 1.8 + 0 + 1.2 + 6.4}{4} = \frac{15}{4} = 3.75 $$
>
> Finalmente, o $R^2$ Ã© dado por:
> $$R^2 = \frac{Cov(X_t, Y_{t+1})^2}{Var(X_t)Var(Y_{t+1})} = \frac{(3.75)^2}{ \frac{10}{4} \cdot \frac{22.8}{4}} = \frac{14.0625}{ \frac{228}{16}} \approx 0.987$$
>
> O valor de $R^2$ de aproximadamente 0.987 indica que aproximadamente 98.7% da variÃ¢ncia de $Y_{t+1}$ pode ser explicada pela projeÃ§Ã£o linear usando $X_t$.

**ProposiÃ§Ã£o 3:** (Unicidade da ProjeÃ§Ã£o Linear) *A projeÃ§Ã£o linear*  $P(Y_{t+1}|X_t) = \alpha'X_t$ *Ã© Ãºnica, dada a condiÃ§Ã£o de ortogonalidade*  $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$.
 *Proof:*
 I. Suponha que existam duas projeÃ§Ãµes lineares, $P_1(Y_{t+1}|X_t) = \alpha_1'X_t$ e $P_2(Y_{t+1}|X_t) = \alpha_2'X_t$, que satisfazem a condiÃ§Ã£o de ortogonalidade.
 II. EntÃ£o, temos:
     $E[(Y_{t+1} - \alpha_1'X_t)X_t] = 0'$ e $E[(Y_{t+1} - \alpha_2'X_t)X_t] = 0'$.
 III. Subtraindo as duas equaÃ§Ãµes, obtemos:
     $E[(\alpha_2'X_t - \alpha_1'X_t)X_t] = E[(\alpha_2' - \alpha_1')X_tX_t'] = 0'$.
 IV.  Como $E[X_tX_t']$ Ã© a matriz de covariÃ¢ncia de $X_t$, que Ã© positiva definida (ou positiva semi-definida se hÃ¡ colinearidade), entÃ£o  $\alpha_2' - \alpha_1' = 0'$, o que implica que $\alpha_1' = \alpha_2'$.
 V.  Portanto, a projeÃ§Ã£o linear Ã© Ãºnica. $\blacksquare$

Essa proposiÃ§Ã£o assegura que, dentro da classe das funÃ§Ãµes lineares, a projeÃ§Ã£o obtida pela condiÃ§Ã£o de ortogonalidade Ã© a Ãºnica que minimiza o MSE, justificando a unicidade do estimador.

### ConclusÃ£o
A projeÃ§Ã£o linear, definida formalmente como $P(Y_{t+1}|X_t) = \alpha'X_t$, busca a funÃ§Ã£o linear de $X_t$ que minimiza o MSE para prever $Y_{t+1}$. A condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o, $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$, Ã© fundamental para garantir a otimalidade dessa projeÃ§Ã£o dentro da classe de previsÃµes lineares. Embora nÃ£o seja a melhor previsÃ£o possÃ­vel para todas asfunÃ§Ãµes, ela garante que nÃ£o hÃ¡ nenhum componente de $X_t$ que possa ser usado para melhorar a previsÃ£o linear de $Y_{t+1}$.

**O Modelo de Vetor Autorregressivo (VAR)**

O modelo VAR Ã© uma extensÃ£o do modelo AR para sistemas multivariados. Em vez de modelar uma Ãºnica variÃ¡vel ao longo do tempo, o VAR modela um vetor de variÃ¡veis. Um modelo VAR(p) Ã© dado por:

$$Y_t = A_1Y_{t-1} + A_2Y_{t-2} + ... + A_pY_{t-p} + \epsilon_t$$

onde:
*   $Y_t$ Ã© um vetor de variÃ¡veis no tempo $t$.
*   $A_i$ sÃ£o matrizes de coeficientes.
*   $\epsilon_t$ Ã© um vetor de erros, que geralmente se assume ser ruÃ­do branco multivariado.

O modelo VAR Ã© Ãºtil para analisar a dinÃ¢mica entre mÃºltiplas sÃ©ries temporais, onde cada variÃ¡vel Ã© influenciada por seus prÃ³prios valores passados e pelos valores passados das outras variÃ¡veis no sistema. O modelo VAR permite examinar as interdependÃªncias e as relaÃ§Ãµes de causalidade entre essas variÃ¡veis.

**FunÃ§Ã£o de Resposta ao Impulso (IRF)**

Uma das ferramentas mais importantes para analisar modelos VAR Ã© a FunÃ§Ã£o de Resposta ao Impulso (IRF). A IRF mostra o efeito de um choque em uma variÃ¡vel sobre si mesma e sobre as outras variÃ¡veis no sistema ao longo do tempo. Formalmente, o IRF Ã© calculado como a resposta de $Y$ a um choque unitÃ¡rio no erro $\epsilon$.

Um exemplo prÃ¡tico Ã©: dado um modelo VAR que inclui inflaÃ§Ã£o e taxa de juros, um choque na taxa de juros pode ter efeitos na inflaÃ§Ã£o ao longo do tempo, tanto positivos quanto negativos dependendo dos coeficientes no modelo. A IRF visualiza esses efeitos, permitindo aos analistas entender as dinÃ¢micas entre variÃ¡veis em um contexto macroeconÃ´mico, por exemplo.

**Causalidade de Granger**

A causalidade de Granger Ã© um conceito que se refere a previsibilidade e nÃ£o Ã  causalidade no sentido estrito. Uma variÃ¡vel $X$ causa no sentido de Granger uma variÃ¡vel $Y$ se os valores passados de $X$ contribuem para uma melhor previsÃ£o de $Y$. Formalmente, dizemos que $X$ causa Granger $Y$ se o modelo que usa os valores passados de $Y$ e $X$ para prever $Y$ tem um desempenho superior ao modelo que usa apenas os valores passados de $Y$.

Essa ferramenta Ã© frequentemente usada em estudos empÃ­ricos para investigar a direÃ§Ã£o das relaÃ§Ãµes entre variÃ¡veis e como o conhecimento de uma variÃ¡vel pode auxiliar na previsÃ£o de outra.

**Modelos de MÃ©dia MÃ³vel (MA)**

Os modelos de MÃ©dia MÃ³vel (MA) sÃ£o outro tipo importante de modelo de sÃ©ries temporais. Um modelo MA(q) assume que o valor da sÃ©rie em um dado momento Ã© uma funÃ§Ã£o linear dos erros passados:

$$Y_t = \mu + \epsilon_t + \theta_1\epsilon_{t-1} + \theta_2\epsilon_{t-2} + ... + \theta_q\epsilon_{t-q}$$

onde:
*   $Y_t$ Ã© a sÃ©rie temporal no tempo $t$.
*   $\mu$ Ã© a mÃ©dia da sÃ©rie.
*   $\epsilon_t$ Ã© o erro no tempo $t$ (ruÃ­do branco).
*   $\theta_i$ sÃ£o os coeficientes do modelo MA.

A ideia central de um modelo MA Ã© que as perturbaÃ§Ãµes ou choques passados tÃªm um efeito sobre o valor atual da sÃ©rie. Os modelos MA sÃ£o usados para modelar dependÃªncias de curto prazo e correlaÃ§Ãµes em sÃ©ries temporais. Eles sÃ£o complementares aos modelos AR e podem ser combinados em modelos ARMA ou ARIMA para capturar melhor as caracterÃ­sticas de uma dada sÃ©rie temporal.

**Modelos ARMA e ARIMA**

Os modelos ARMA combinam os modelos AR e MA, capturando tanto a autocorrelaÃ§Ã£o quanto os efeitos dos erros passados. Um modelo ARMA(p,q) Ã© dado por:

$$Y_t = \alpha_1Y_{t-1} + ... + \alpha_pY_{t-p} + \epsilon_t + \theta_1\epsilon_{t-1} + ... + \theta_q\epsilon_{t-q}$$

onde $p$ e $q$ sÃ£o as ordens dos componentes AR e MA, respectivamente.

Os modelos ARIMA estendem os modelos ARMA ao incluir a diferenciaÃ§Ã£o, que Ã© uma tÃ©cnica usada para tornar as sÃ©ries temporais estacionÃ¡rias. Um modelo ARIMA(p,d,q) inclui p termos autorregressivos, q termos de mÃ©dia mÃ³vel, e a sÃ©rie temporal Ã© diferenciada 'd' vezes. A diferenciaÃ§Ã£o pode ajudar a eliminar tendÃªncias e sazonalidades, tornando o modelo mais adequado para previsÃ£o e anÃ¡lise.

<!-- END -->
