## O Fator de AtualizaÃ§Ã£o e sua InterpretaÃ§Ã£o em ProjeÃ§Ãµes Lineares

### IntroduÃ§Ã£o

Este capÃ­tulo aprofunda a anÃ¡lise da atualizaÃ§Ã£o de projeÃ§Ãµes lineares, focando especificamente no fator de atualizaÃ§Ã£o $h_{32}h_{22}^{-1}$, que surge ao incorporar novas informaÃ§Ãµes em uma projeÃ§Ã£o linear existente. Em continuidade aos capÃ­tulos anteriores, que estabeleceram os fundamentos das projeÃ§Ãµes lineares, da fatoraÃ§Ã£o triangular da matriz de momentos e da decomposiÃ§Ã£o do erro de previsÃ£o, este capÃ­tulo visa fornecer uma interpretaÃ§Ã£o geomÃ©trica e intuitiva do fator de atualizaÃ§Ã£o, mostrando como ele relaciona os resÃ­duos das projeÃ§Ãµes e o efeito da inclusÃ£o de novas variÃ¡veis na precisÃ£o da projeÃ§Ã£o [^4.5.14].

### Conceitos Fundamentais

A atualizaÃ§Ã£o de uma projeÃ§Ã£o linear envolve adicionar Ã  projeÃ§Ã£o inicial um termo de correÃ§Ã£o que Ã© proporcional Ã  nova informaÃ§Ã£o, isto Ã©, ao componente nÃ£o antecipado da nova variÃ¡vel [^4.5.12]. Este termo de correÃ§Ã£o Ã© o produto do componente nÃ£o antecipado da nova variÃ¡vel ($Y_2 - P(Y_2|Y_1)$) com o fator de atualizaÃ§Ã£o $h_{32}h_{22}^{-1}$. A fatoraÃ§Ã£o triangular da matriz de covariÃ¢ncia $\Omega$, expressa como $\Omega = ADA'$, nos permite decompor o espaÃ§o vetorial em componentes nÃ£o correlacionadas e facilita o cÃ¡lculo eficiente desse fator de atualizaÃ§Ã£o [^4.4.1].

#### O Fator de AtualizaÃ§Ã£o e a CovariÃ¢ncia dos ResÃ­duos

O fator de atualizaÃ§Ã£o $h_{32}h_{22}^{-1}$ Ã© um elemento essencial na atualizaÃ§Ã£o de projeÃ§Ãµes lineares. Como estabelecido anteriormente [^4.5.14], esse fator surge da seguinte expressÃ£o:
$$P(Y_3|Y_2,Y_1) = P(Y_3|Y_1) + h_{32}h_{22}^{-1} [Y_2 - P(Y_2|Y_1)].$$
onde $h_{32}$ e $h_{22}$ sÃ£o definidos como:
$$ h_{32} = E\{[Y_3 - P(Y_3|Y_1)][Y_2 - P(Y_2|Y_1)]\}$$
e
$$h_{22} = E[Y_2 - P(Y_2|Y_1)]^2$$

Essas quantidades representam covariÃ¢ncias e variÃ¢ncias dos resÃ­duos das projeÃ§Ãµes. Especificamente:
- $h_{22}$ representa a variÃ¢ncia do resÃ­duo da projeÃ§Ã£o de $Y_2$ em $Y_1$.
- $h_{32}$ representa a covariÃ¢ncia entre o resÃ­duo da projeÃ§Ã£o de $Y_3$ em $Y_1$ e o resÃ­duo da projeÃ§Ã£o de $Y_2$ em $Y_1$.

Portanto, o fator de atualizaÃ§Ã£o $h_{32}h_{22}^{-1}$ pode ser interpretado como a razÃ£o entre a covariÃ¢ncia entre os resÃ­duos das projeÃ§Ãµes de $Y_3$ e $Y_2$ sobre $Y_1$ e a variÃ¢ncia do resÃ­duo da projeÃ§Ã£o de $Y_2$ sobre $Y_1$.

**Lema 3:** *O fator de atualizaÃ§Ã£o $h_{32}h_{22}^{-1}$ Ã© igual ao coeficiente da projeÃ§Ã£o linear do resÃ­duo de $Y_3$ na projeÃ§Ã£o de $Y_1$ sobre o resÃ­duo de $Y_2$ na projeÃ§Ã£o de $Y_1$.*
*DemonstraÃ§Ã£o:*
I. Definimos os resÃ­duos como:
   $$e_{3|1} = Y_3 - P(Y_3|Y_1)$$
   $$e_{2|1} = Y_2 - P(Y_2|Y_1)$$
II. O fator de atualizaÃ§Ã£o Ã© dado por:
$$h_{32}h_{22}^{-1} = \frac{E(e_{3|1}e_{2|1})}{E(e_{2|1}^2)}$$
III. A expressÃ£o acima Ã© exatamente o coeficiente da projeÃ§Ã£o de $e_{3|1}$ sobre $e_{2|1}$. Logo, o fator de atualizaÃ§Ã£o Ã© igual ao coeficiente da projeÃ§Ã£o linear do resÃ­duo de $Y_3$ em relaÃ§Ã£o a $Y_1$ sobre o resÃ­duo de $Y_2$ em relaÃ§Ã£o a $Y_1$. â– 

**ProposiÃ§Ã£o 2:** *O fator de atualizaÃ§Ã£o $h_{32}h_{22}^{-1}$ pode ser expresso em termos da matriz de covariÃ¢ncia $\Omega$ como:*
$$h_{32}h_{22}^{-1} = \frac{\Omega_{32} - \Omega_{31}\Omega_{11}^{-1}\Omega_{12}}{\Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}}$$
*DemonstraÃ§Ã£o:*
I. Da definiÃ§Ã£o, $h_{32} = E\{[Y_3 - P(Y_3|Y_1)][Y_2 - P(Y_2|Y_1)]\}$ e $h_{22} = E[Y_2 - P(Y_2|Y_1)]^2$.
II. Sabemos que $P(Y_3|Y_1) = \Omega_{31}\Omega_{11}^{-1}Y_1$ e $P(Y_2|Y_1) = \Omega_{21}\Omega_{11}^{-1}Y_1$.
III. Substituindo as projeÃ§Ãµes em $h_{32}$, temos:
    $$h_{32} = E\{[Y_3 - \Omega_{31}\Omega_{11}^{-1}Y_1][Y_2 - \Omega_{21}\Omega_{11}^{-1}Y_1]\}$$
IV. Expandindo, obtemos:
  $$h_{32} = E(Y_3Y_2) - E(Y_3\Omega_{21}\Omega_{11}^{-1}Y_1) - E(\Omega_{31}\Omega_{11}^{-1}Y_1Y_2) + E(\Omega_{31}\Omega_{11}^{-1}Y_1\Omega_{21}\Omega_{11}^{-1}Y_1)$$
V. Usando as propriedades da esperanÃ§a e da matriz de covariÃ¢ncia:
    $$h_{32} = \Omega_{32} - \Omega_{31}\Omega_{11}^{-1}\Omega_{12} - \Omega_{31}\Omega_{11}^{-1}\Omega_{12} + \Omega_{31}\Omega_{11}^{-1}\Omega_{11}\Omega_{11}^{-1}\Omega_{12}$$
VI. Simplificando, temos:
    $$h_{32} = \Omega_{32} - \Omega_{31}\Omega_{11}^{-1}\Omega_{12}$$
VII. De forma anÃ¡loga, $h_{22} = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}$.
VIII. Portanto, o fator de atualizaÃ§Ã£o Ã©:
    $$h_{32}h_{22}^{-1} = \frac{\Omega_{32} - \Omega_{31}\Omega_{11}^{-1}\Omega_{12}}{\Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}}$$
Assim, o fator de atualizaÃ§Ã£o pode ser expresso diretamente em termos das covariÃ¢ncias e das projeÃ§Ãµes das variÃ¡veis originais. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Considere a matriz de covariÃ¢ncia
> $$\Omega = \begin{bmatrix}
> 4 & 1 & 2 \\
> 1 & 2 & 0.5 \\
> 2 & 0.5 & 5
> \end{bmatrix}$$
>
> Temos $\Omega_{11} = 4$, $\Omega_{12} = 1$, $\Omega_{21} = 1$, $\Omega_{22} = 2$, $\Omega_{31} = 2$, $\Omega_{32} = 0.5$. EntÃ£o, usando a proposiÃ§Ã£o 2, o fator de atualizaÃ§Ã£o Ã©
> $$h_{32}h_{22}^{-1} = \frac{0.5 - 2 \cdot 4^{-1} \cdot 1}{2 - 1 \cdot 4^{-1} \cdot 1} = \frac{0.5 - 0.5}{2 - 0.25} = 0$$
>
> Nesse caso, o fator de atualizaÃ§Ã£o Ã© zero, indicando que a informaÃ§Ã£o adicional de $Y_2$ nÃ£o contribui para a projeÃ§Ã£o de $Y_3$ alÃ©m do que jÃ¡ Ã© provido por $Y_1$.
>
> Agora, vamos considerar uma matriz de covariÃ¢ncia diferente, onde a relaÃ§Ã£o entre $Y_2$ e $Y_3$ Ã© mais forte apÃ³s remover o efeito de $Y_1$:
>
> $$\Omega' = \begin{bmatrix}
> 4 & 1 & 1 \\
> 1 & 2 & 1.5 \\
> 1 & 1.5 & 5
> \end{bmatrix}$$
>
> Usando os mesmos passos: $\Omega'_{11} = 4$, $\Omega'_{12} = 1$, $\Omega'_{21} = 1$, $\Omega'_{22} = 2$, $\Omega'_{31} = 1$, $\Omega'_{32} = 1.5$.
>
> EntÃ£o, o fator de atualizaÃ§Ã£o Ã©:
> $$h_{32}h_{22}^{-1} = \frac{1.5 - 1 \cdot 4^{-1} \cdot 1}{2 - 1 \cdot 4^{-1} \cdot 1} = \frac{1.5 - 0.25}{2 - 0.25} = \frac{1.25}{1.75} \approx 0.714$$
>
> Neste caso, o fator de atualizaÃ§Ã£o Ã© aproximadamente 0.714, indicando que a informaÃ§Ã£o adicional de $Y_2$ contribui positivamente para a projeÃ§Ã£o de $Y_3$ apÃ³s remover o efeito de $Y_1$. Um aumento de uma unidade no resÃ­duo de $Y_2$ em relaÃ§Ã£o a $Y_1$ leva a um aumento de aproximadamente 0.714 unidades na projeÃ§Ã£o de $Y_3$.

#### InterpretaÃ§Ã£o GeomÃ©trica do Fator de AtualizaÃ§Ã£o

O fator de atualizaÃ§Ã£o $h_{32}h_{22}^{-1}$ pode ser interpretado geometricamente como a inclinaÃ§Ã£o da projeÃ§Ã£o linear do resÃ­duo de $Y_3$ apÃ³s a projeÃ§Ã£o em $Y_1$ sobre o resÃ­duo de $Y_2$ apÃ³s a projeÃ§Ã£o em $Y_1$.
Em outras palavras, ele mede o quanto a nova informaÃ§Ã£o (o componente nÃ£o antecipado de $Y_2$) afeta a previsÃ£o de $Y_3$.
   - Se $h_{32}h_{22}^{-1}$ for positivo, um valor acima da mÃ©dia de $Y_2$ (apÃ³s remover o efeito de $Y_1$) indica que $Y_3$ tambÃ©m deve ser maior que o previsto com base apenas em $Y_1$.
   - Se $h_{32}h_{22}^{-1}$ for negativo, o oposto ocorre.
   - Se $h_{32}h_{22}^{-1}$ for zero, a nova informaÃ§Ã£o de $Y_2$ nÃ£o contribui para prever $Y_3$.

AlÃ©m disso, $h_{22}$ Ã© a variÃ¢ncia do resÃ­duo de $Y_2$ apÃ³s projetar em $Y_1$. Assim, o fator de atualizaÃ§Ã£o tambÃ©m reflete o grau de incerteza associado Ã  projeÃ§Ã£o de $Y_2$ em $Y_1$. Se o erro ao prever $Y_2$ com base em $Y_1$ for pequeno ($h_{22}$ pequeno), entÃ£o o fator de atualizaÃ§Ã£o pode ter um valor maior e, por conseguinte, a influÃªncia da nova informaÃ§Ã£o sobre a projeÃ§Ã£o de $Y_3$ serÃ¡ maior.

#### O Fator de AtualizaÃ§Ã£o e a EficiÃªncia da ProjeÃ§Ã£o
O fator de atualizaÃ§Ã£o tambÃ©m estÃ¡ ligado Ã  eficiÃªncia da atualizaÃ§Ã£o da projeÃ§Ã£o linear. Se $h_{32}$ for grande em relaÃ§Ã£o a $h_{22}$, significa que o resÃ­duo de $Y_2$ contÃ©m muita informaÃ§Ã£o relevante para prever o resÃ­duo de $Y_3$, e a atualizaÃ§Ã£o trarÃ¡ uma melhora substancial na projeÃ§Ã£o. Caso contrÃ¡rio, a contribuiÃ§Ã£o da nova informaÃ§Ã£o de $Y_2$ serÃ¡ menor.
  * Caso $h_{32} = 0$, o fator de atualizaÃ§Ã£o serÃ¡ zero, e a adiÃ§Ã£o de $Y_2$ Ã  projeÃ§Ã£o nÃ£o diminui o erro de previsÃ£o, o que sugere que a informaÃ§Ã£o de $Y_2$ Ã© redundante para a projeÃ§Ã£o de $Y_3$ dados os dados de $Y_1$ jÃ¡ utilizados.

**ProposiÃ§Ã£o 2.1:** *O fator de atualizaÃ§Ã£o pode ser expresso em termos de projeÃ§Ãµes lineares como:*
$$h_{32}h_{22}^{-1} = \frac{Cov(Y_3 - P(Y_3|Y_1), Y_2 - P(Y_2|Y_1))}{Var(Y_2 - P(Y_2|Y_1))}$$
*DemonstraÃ§Ã£o:*
I. Definimos os resÃ­duos $e_{3|1} = Y_3 - P(Y_3|Y_1)$ e $e_{2|1} = Y_2 - P(Y_2|Y_1)$.
II. Pela definiÃ§Ã£o de covariÃ¢ncia, $Cov(e_{3|1}, e_{2|1}) = E[(e_{3|1} - E[e_{3|1}])(e_{2|1} - E[e_{2|1}])]$.
III. Como $E[e_{3|1}] = 0$ e $E[e_{2|1}] = 0$ (propriedade dos resÃ­duos), $Cov(e_{3|1}, e_{2|1}) = E[e_{3|1}e_{2|1}] = h_{32}$.
IV. De maneira semelhante, $Var(e_{2|1}) = E[(e_{2|1} - E[e_{2|1}])^2] = E[e_{2|1}^2] = h_{22}$.
V. Portanto, $h_{32}h_{22}^{-1} = \frac{Cov(e_{3|1}, e_{2|1})}{Var(e_{2|1})}= \frac{Cov(Y_3 - P(Y_3|Y_1), Y_2 - P(Y_2|Y_1))}{Var(Y_2 - P(Y_2|Y_1))}$.â– 

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> No exemplo anterior em que a matriz de covariÃ¢ncia era
> $$\Omega = \begin{bmatrix}
> 4 & 1 & 2 \\
> 1 & 2 & 0.5 \\
> 2 & 0.5 & 5
> \end{bmatrix}$$
>
> O fator de atualizaÃ§Ã£o era 0.  Isto significa que, a projeÃ§Ã£o de $Y_3$ com base em $Y_1$ jÃ¡ incorpora toda informaÃ§Ã£o relevante da projeÃ§Ã£o de $Y_2$ com base em $Y_1$. Ao analisar a matriz $\Omega$ vemos que $\Omega_{32}$ Ã© pequeno, indicando que $Y_2$ nÃ£o adiciona muita informaÃ§Ã£o sobre $Y_3$.
>
>  Se o termo $\Omega_{32}$ fosse maior, a atualizaÃ§Ã£o teria um maior impacto na projeÃ§Ã£o, ou seja, ao projetarmos $Y_3$ sobre $Y_1$, a informaÃ§Ã£o de $Y_2$ nÃ£o estaria sendo totalmente capturada e por isso o termo de atualizaÃ§Ã£o aumentaria o ajuste do modelo.
>
> Se o termo  $\Omega_{21}$ fosse zero, as variÃ¡veis $Y_1$ e $Y_2$ seriam ortogonais, e o fator de atualizaÃ§Ã£o se simplificaria para $\frac{\Omega_{32}}{\Omega_{22}}$, que Ã© o coeficiente da projeÃ§Ã£o de $Y_3$ em $Y_2$.
>
> Para demonstrar isto, vamos usar uma matriz de covariÃ¢ncia com $\Omega_{21} = 0$.
>
> $$\Omega'' = \begin{bmatrix}
> 4 & 0 & 2 \\
> 0 & 2 & 1 \\
> 2 & 1 & 5
> \end{bmatrix}$$
>
> Neste caso, $\Omega''_{11} = 4$, $\Omega''_{12} = 0$, $\Omega''_{21} = 0$, $\Omega''_{22} = 2$, $\Omega''_{31} = 2$, $\Omega''_{32} = 1$.
>
> O fator de atualizaÃ§Ã£o Ã©:
>
> $$h_{32}h_{22}^{-1} = \frac{1 - 2 \cdot 4^{-1} \cdot 0}{2 - 0 \cdot 4^{-1} \cdot 0} = \frac{1}{2} = 0.5$$
>
> De fato, como $\Omega_{21} = 0$, temos que $Y_1$ e $Y_2$ sÃ£o ortogonais e o fator de atualizaÃ§Ã£o se reduz a $\frac{\Omega_{32}}{\Omega_{22}} = \frac{1}{2}$. Isso indica que a influÃªncia da nova informaÃ§Ã£o de $Y_2$ na projeÃ§Ã£o de $Y_3$ Ã© dada diretamente pela covariÃ¢ncia entre $Y_2$ e $Y_3$, ajustada pela variÃ¢ncia de $Y_2$.

### ConclusÃ£o

O fator de atualizaÃ§Ã£o $h_{32}h_{22}^{-1}$ Ã© um componente fundamental na atualizaÃ§Ã£o de projeÃ§Ãµes lineares, representando a inclinaÃ§Ã£o da relaÃ§Ã£o linear entre o resÃ­duo de $Y_3$ e o resÃ­duo de $Y_2$ apÃ³s a projeÃ§Ã£o em $Y_1$. Ele incorpora a covariÃ¢ncia entre esses resÃ­duos e a incerteza na previsÃ£o de $Y_2$ com base em $Y_1$. A compreensÃ£o deste fator permite uma melhor interpretaÃ§Ã£o do mecanismo de atualizaÃ§Ã£o de projeÃ§Ãµes lineares e sua relaÃ§Ã£o com a fatoraÃ§Ã£o triangular da matriz de momentos. A anÃ¡lise apresentada fornece uma base para entender como novas informaÃ§Ãµes modificam projeÃ§Ãµes existentes e como a fatoraÃ§Ã£o triangular contribui para otimizar este processo.

### ReferÃªncias
[^4.1.10]: *SeÃ§Ã£o 4.1, pÃ¡gina 73*
[^4.4.1]:  *SeÃ§Ã£o 4.4, pÃ¡gina 87*
[^4.5.2]:  *SeÃ§Ã£o 4.5, pÃ¡gina 92*
[^4.5.12]: *SeÃ§Ã£o 4.5, pÃ¡gina 93*
[^4.5.14]: *SeÃ§Ã£o 4.5, pÃ¡gina 94*

### 5.2. Likelihood Function for an AR(1) Process
**Lema 1:** *The conditional expectation of $Y_t$ given the past is $E[Y_t|Y_{t-1},Y_{t-2},...] = c + \phi Y_{t-1}$ for an AR(1) process.*

*Proof:* Given the AR(1) model, $Y_t = c + \phi Y_{t-1} + \epsilon_t$, we know that $E[\epsilon_t | Y_{t-1}, Y_{t-2}, ... ] = 0$, since the errors are assumed to be i.i.d. and thus independent of the past. Hence, taking conditional expectations, we get:
$$E[Y_t|Y_{t-1},Y_{t-2},...] = E[c + \phi Y_{t-1} + \epsilon_t|Y_{t-1},Y_{t-2},...] = c + \phi E[Y_{t-1}|Y_{t-1},Y_{t-2},...] + E[\epsilon_t|Y_{t-1},Y_{t-2},...]=c+\phi Y_{t-1}+0$$
Thus, $E[Y_t|Y_{t-1},Y_{t-2},...] = c + \phi Y_{t-1}$ .â– 

We begin by deriving the likelihood function for an AR(1) process. Consider the model:

$$Y_t = c + \phi Y_{t-1} + \epsilon_t$$ [5.2.1]

where $\epsilon_t \sim i.i.d. N(0, \sigma^2)$.  To calculate the likelihood, we need the joint density of the observed data $Y_1, Y_2, \ldots, Y_T$.  Since the $\epsilon_t$ are independent and normally distributed, the conditional distribution of $Y_t$ given $Y_{t-1}$ is also normal, with mean $c + \phi Y_{t-1}$ and variance $\sigma^2$:

$$Y_t | Y_{t-1} \sim N(c + \phi Y_{t-1}, \sigma^2)$$

The joint density can be written as the product of conditional densities:

$$f_{Y_1, Y_2, \ldots, Y_T}(y_1, y_2, \ldots, y_T; c, \phi, \sigma^2) = f_{Y_1}(y_1; c, \phi, \sigma^2) \prod_{t=2}^{T} f_{Y_t|Y_{t-1}}(y_t | y_{t-1}; c, \phi, \sigma^2)$$  [5.2.2]

The conditional densities are straightforward to compute from the Gaussian assumption:

$$f_{Y_t|Y_{t-1}}(y_t | y_{t-1}; c, \phi, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_t - c - \phi y_{t-1})^2}{2\sigma^2}\right)$$ [5.2.3]

However, the initial value $Y_1$ does not have a preceeding value with which to condition upon.  To proceed, we must make some assumption about its distribution.  A common, albeit not perfectly accurate assumption is that $Y_1$ is drawn from the unconditional distribution of $Y_t$.  For a stationary AR(1), we have that the unconditional mean is given by $\mu = \frac{c}{1 - \phi}$ and the unconditional variance is given by $\frac{\sigma^2}{1 - \phi^2}$.  Thus, our assumption about the initial condition is

$$Y_1 \sim N\left(\frac{c}{1 - \phi}, \frac{\sigma^2}{1 - \phi^2}\right)$$
and therefore

$$f_{Y_1}(y_1; c, \phi, \sigma^2) = \frac{1}{\sqrt{2\pi\frac{\sigma^2}{1 - \phi^2}}} \exp\left(-\frac{(y_1 - \frac{c}{1 - \phi})^2}{2\frac{\sigma^2}{1-\phi^2}}\right)$$ [5.2.4]

Plugging [5.2.3] and [5.2.4] into [5.2.2], the likelihood function for the AR(1) process is given by:

$$L(c, \phi, \sigma^2 | Y_1, \ldots, Y_T) = \left(\frac{1}{\sqrt{2\pi\frac{\sigma^2}{1-\phi^2}}}\right) \exp\left(-\frac{(y_1 - \frac{c}{1-\phi})^2}{2\frac{\sigma^2}{1-\phi^2}}\right) \prod_{t=2}^{T} \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) \exp\left(-\frac{(y_t - c - \phi y_{t-1})^2}{2\sigma^2}\right)$$  [5.2.5]

It is more convenient to work with the log-likelihood:

$$\mathcal{L}(c, \phi, \sigma^2 | Y_1, \ldots, Y_T) = -\frac{1}{2} \log(2\pi) - \frac{1}{2}\log\left(\frac{\sigma^2}{1-\phi^2}\right)  -\frac{(y_1 - \frac{c}{1-\phi})^2}{2\frac{\sigma^2}{1-\phi^2}} - \frac{T-1}{2} \log(2\pi) - \frac{T-1}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=2}^{T} (y_t - c - \phi y_{t-1})^2$$
[5.2.6]

The maximum likelihood estimates of the parameters are those that maximize [5.2.6], or equivalently, minimize the negative of the log-likelihood function.  In general, there is no closed-form solution for the parameters, requiring numerical optimization techniques. However, if $\phi$ is known, then it is straightforward to minimize the negative of the likelihood function with respect to $c$ and $\sigma^2$ using OLS. In particular, conditional on $\phi$, we can interpret $Y_t - \phi Y_{t-1}$ as the dependent variable in an OLS regression with constant c.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que temos uma sÃ©rie temporal de 5 observaÃ§Ãµes de um processo AR(1):
> $Y = [1.2, 1.8, 2.1, 2.5, 2.9]$
>
> Assumindo um valor de $\phi = 0.5$, podemos calcular os parÃ¢metros $c$ e $\sigma^2$ usando OLS.
> Primeiro, construÃ­mos uma nova sÃ©rie temporal $Y^*_t = Y_t - \phi Y_{t-1}$. Como $Y_0$ nÃ£o Ã© observado, assumimos que $Y^*_1 = Y_1 = 1.2$.
>
> $Y^*_1 = 1.2$
> $Y^*_2 = 1.8 - 0.5 * 1.2 = 1.2$
> $Y^*_3 = 2.1 - 0.5 * 1.8 = 1.2$
> $Y^*_4 = 2.5 - 0.5 * 2.1 = 1.45$
> $Y^*_5 = 2.9 - 0.5 * 2.5 = 1.65$
>
> EntÃ£o, $Y^* = [1.2, 1.2, 1.2, 1.45, 1.65]$. Usando OLS em $Y^*_t = c + \epsilon_t$, obtemos
>
> $$c = \frac{1}{5}\sum_{t=1}^{5} Y^*_t = \frac{1.2 + 1.2 + 1.2 + 1.45 + 1.65}{5} = 1.3$$
>
> E para $\sigma^2$, calculamos os resÃ­duos:
> $\epsilon_1 = 1.2 - 1.3 = -0.1$
> $\epsilon_2 = 1.2 - 1.3 = -0.1$
> $\epsilon_3 = 1.2 - 1.3 = -0.1$
> $\epsilon_4 = 1.45 - 1.3 = 0.15$
> $\epsilon_5 = 1.65 - 1.3 = 0.35$
>
> E
> $$\sigma^2 = \frac{1}{5}\sum_{t=1}^{5} \epsilon_t^2 = \frac{(-0.1)^2 + (-0.1)^2 + (-0.1)^2 + (0.15)^2 + (0.35)^2}{5} = 0.031$$
>
> Assim, os parÃ¢metros estimados condicionalmente a $\phi = 0.5$ sÃ£o: $c \approx 1.3$ e $\sigma^2 \approx 0.031$. Esses resultados minimizam a soma dos erros quadrÃ¡ticos condicional a um dado $\phi$.  Para obter os parÃ¢metros de mÃ¡xima verossimilhanÃ§a, seria necessÃ¡rio realizar uma otimizaÃ§Ã£o numÃ©rica que variasse $\phi$ e recalcularia $c$ e $\sigma^2$ recursivamente.

### 5.3. Likelihood Function for an MA(1) Process

Now, consider the moving average process of order one:

$$Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$$ [5.3.1]

where $\epsilon_t \sim i.i.d. N(0, \sigma^2)$. In this case, the likelihood function cannot be written in terms of simple conditional densities of $Y_t$ given $Y_{t-1}$. The difficulty here arises because the errors, $\epsilon_t$, are not directly observed. We can not simply condition on $\epsilon_{t-1}$ or, likewise, on $Y_{t-1}$ (as we did in the AR(1) case), because those errors are unobserved. To proceed, we need to derive the joint distribution of $(Y_1, Y_2, \ldots, Y_T)$. This can be accomplished by using the fact that the joint distribution of the $\epsilon_t$ is known, and then using the transformations implied by equation [5.3.1] to get the joint distribution of the $Y_t$. In practice, the calculation is easier to carry out if we treat the unknown $\epsilon$ from before the sample period ($\epsilon_0$, $\epsilon_{-1}$, etc.) as fixed at zero. With that assumption, we can calculate the values of the $\epsilon$ by rearranging [5.3.1]:
$$\epsilon_t = Y_t - \mu - \theta \epsilon_{t-1}$$ [5.3.2]
$$\epsilon_1 = Y_1 - \mu$$
$$\epsilon_2 = Y_2 - \mu - \theta \epsilon_1 = Y_2 - \mu - \theta (Y_1 - \mu)$$
$$\epsilon_3 = Y_3 - \mu - \theta \epsilon_2 = Y_3 - \mu - \theta(Y_2 - \mu - \theta (Y_1 - \mu))$$
$$\vdots$$

We can continue this substitution to rewrite $\epsilon_t$ in terms of past values of $Y_t$.  With the assumption that  $\epsilon_0=0$, the vector $\epsilon = (\epsilon_1, \ldots, \epsilon_T)$ can be considered a linear transformation of the vector $Y = (Y_1, \ldots, Y_T)$. As such, the vector $Y$ will have a multivariate normal distribution if the vector of errors has a multivariate normal distribution. Let us denote the sample mean as $\bar{y}$.  Then, we have,
$$ \text{vec}(Y-\mu) = A \text{vec}(\epsilon),$$
where A is a lower triangular matrix.  To see this, we can write the system of equation for the first 3 values of $Y$ and $\epsilon$ from [5.3.2]:
$$
\begin{bmatrix}
Y_1 - \mu \\
Y_2 - \mu \\
Y_3 - \mu
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & 0 \\
\theta & 1 & 0 \\
\theta^2 & \theta & 1
\end{bmatrix}
\begin{bmatrix}
\epsilon_1 \\
\epsilon_2 \\
\epsilon_3
\end{bmatrix}
$$
The determinant of this transformation is 1. Therefore, the joint distribution of $Y$ can be written as:

$$f_{Y_1, Y_2, \ldots, Y_T}(y_1, y_2, \ldots, y_T; \mu, \theta, \sigma^2) = \frac{1}{(2\pi\sigma^2)^{T/2} |A|} \exp\left(-\frac{1}{2\sigma^2} \epsilon' \epsilon\right)$$ [5.3.3]
Where $\epsilon' \epsilon = \sum_t \epsilon_t^2$. Because the determinant is 1, the log-likelihood is

$$\mathcal{L}(\mu, \theta, \sigma^2 | Y_1, \ldots, Y_T) = -\frac{T}{2} \log(2\pi) -\frac{T}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=1}^T \epsilon_t^2 $$
[5.3.4]

Again, the maximum likelihood estimates of the parameters are those that maximize [5.3.4], or equivalently, minimize the negative of the log-likelihood. These values can only be found using numerical optimization. However, if $\theta$ is known, the parameters $\mu$ and $\sigma^2$ can be found using a procedure very similar to OLS. To do this, note that the sum of the squared errors depends on previous errors, $\epsilon_{t-1}$. To perform a standard OLS regression we will therefore need to transform the data such that the parameters of the MA model are the error terms. By rewriting [5.3.1], we have that
$$ \epsilon_t = Y_t - \mu - \theta\epsilon_{t-1} $$
We thus transform the data as $Y_t^* = Y_t - \theta \epsilon_{t-1}$. Note that since $\epsilon_0=0$, we have that $Y_1^* = Y_1$.  Thus, once we have an estimate for $\theta$, the parameters $\mu$ and $\sigma^2$ can be found by performing OLS on the transformed data $Y^*$, for which
$$ Y_t^* = \mu + \epsilon_t $$.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que temos a seguinte sÃ©rie temporal de 4 observaÃ§Ãµes de um processo MA(1):
> $Y = [1.5, 2.0, 1.8, 2.2]$
>
> E assumindo que $\theta = 0.3$, podemos calcular $\mu$ e $\sigma^2$ utilizando OLS.
>
> Primeiro, precisamos calcular os valores dos erros $\epsilon_t$ recursivamente, lembrando que $\epsilon_0 = 0$.
>
> $\epsilon_1 = Y_1 - \mu = 1.5 - \mu$
> $\epsilon_2 = Y_2 - \mu - \theta\epsilon_1 = 2.0 - \mu - 0.3(1.5 - \mu) = 2.0 - \mu - 0.45 + 0.3\mu = 1.55 - 0.7\mu$
> $\epsilon_3 = Y_3 - \mu - \theta\epsilon_2 = 1.8 - \mu - 0.3(1.55 - 0.7\mu) = 1.8 - \mu - 0.465 + 0.21\mu = 1.335 - 0.79\mu$
> $\epsilon_4 = Y_4 - \mu - \theta\epsilon_3 = 2.2 - \mu - 0.3(1.335 - 0.79\mu) = 2.2 - \mu - 0.4005 + 0.237\mu = 1.7$1.7995 - 0.763\mu$
>$\epsilon_5 = Y_5 - \mu - \theta\epsilon_4 = 2.5 - \mu - 0.3(1.7995 - 0.763\mu) = 2.5 - \mu - 0.53985 + 0.2289\mu = 1.96015 - 0.7711\mu$

Now, to find the estimate of $\mu$, we need to minimize the sum of squared errors:

$S(\mu) = \epsilon_1^2 + \epsilon_2^2 + \epsilon_3^2 + \epsilon_4^2 + \epsilon_5^2$
$S(\mu) = (1.0 - \mu)^2 + (1.25 - 0.7\mu)^2 + (1.335 - 0.79\mu)^2 + (1.7995 - 0.763\mu)^2 + (1.96015 - 0.7711\mu)^2$

Taking the derivative with respect to $\mu$ and setting it equal to zero:

$\frac{dS(\mu)}{d\mu} = 2(1.0 - \mu)(-1) + 2(1.25 - 0.7\mu)(-0.7) + 2(1.335 - 0.79\mu)(-0.79) + 2(1.7995 - 0.763\mu)(-0.763) + 2(1.96015 - 0.7711\mu)(-0.7711) = 0$

$-2(1.0 - \mu) - 1.4(1.25 - 0.7\mu) - 1.58(1.335 - 0.79\mu) - 1.526(1.7995 - 0.763\mu) - 1.5422(1.96015 - 0.7711\mu) = 0$

$-2 + 2\mu - 1.75 + 0.98\mu - 2.1093 + 1.2482\mu - 2.7461 + 1.1643\mu - 3.0231 + 1.1892\mu = 0$

$(-2 - 1.75 - 2.1093 - 2.7461 - 3.0231) + (2 + 0.98 + 1.2482 + 1.1643 + 1.1892)\mu = 0$

$-11.6285 + 6.5817\mu = 0$

$6.5817\mu = 11.6285$

$\mu = \frac{11.6285}{6.5817} \approx 1.7668$

Therefore, the estimate of $\mu$ is approximately $1.7668$.

Now let's calculate the estimates for $\epsilon_1, \epsilon_2, \epsilon_3, \epsilon_4, \epsilon_5$ with $\hat{\mu} \approx 1.7668$:

$\epsilon_1 = 1.0 - 1.7668 = -0.7668$
$\epsilon_2 = 1.25 - 0.7(1.7668) = 1.25 - 1.23676 = 0.01324$
$\epsilon_3 = 1.335 - 0.79(1.7668) = 1.335 - 1.395772 = -0.060772$
$\epsilon_4 = 1.7995 - 0.763(1.7668) = 1.7995 - 1.35197 = 0.44753$
$\epsilon_5 = 1.96015 - 0.7711(1.7668) = 1.96015 - 1.36156 = 0.59859$

And here are the values of $Y_i$ based on this estimate of $\mu$:
$Y_1 = 1.0$
$Y_2 = 1.25$
$Y_3 = 1.55$
$Y_4 = 2.2$
$Y_5 = 2.5$
<!-- END -->
