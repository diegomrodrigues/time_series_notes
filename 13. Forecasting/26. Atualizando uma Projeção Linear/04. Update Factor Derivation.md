## O Fator de Atualiza√ß√£o e sua Interpreta√ß√£o em Proje√ß√µes Lineares

### Introdu√ß√£o

Este cap√≠tulo aprofunda a an√°lise da atualiza√ß√£o de proje√ß√µes lineares, focando especificamente no fator de atualiza√ß√£o $h_{32}h_{22}^{-1}$, que surge ao incorporar novas informa√ß√µes em uma proje√ß√£o linear existente. Em continuidade aos cap√≠tulos anteriores, que estabeleceram os fundamentos das proje√ß√µes lineares, da fatora√ß√£o triangular da matriz de momentos e da decomposi√ß√£o do erro de previs√£o, este cap√≠tulo visa fornecer uma interpreta√ß√£o geom√©trica e intuitiva do fator de atualiza√ß√£o, mostrando como ele relaciona os res√≠duos das proje√ß√µes e o efeito da inclus√£o de novas vari√°veis na precis√£o da proje√ß√£o [^4.5.14].

### Conceitos Fundamentais

A atualiza√ß√£o de uma proje√ß√£o linear envolve adicionar √† proje√ß√£o inicial um termo de corre√ß√£o que √© proporcional √† nova informa√ß√£o, isto √©, ao componente n√£o antecipado da nova vari√°vel [^4.5.12]. Este termo de corre√ß√£o √© o produto do componente n√£o antecipado da nova vari√°vel ($Y_2 - P(Y_2|Y_1)$) com o fator de atualiza√ß√£o $h_{32}h_{22}^{-1}$. A fatora√ß√£o triangular da matriz de covari√¢ncia $\Omega$, expressa como $\Omega = ADA'$, nos permite decompor o espa√ßo vetorial em componentes n√£o correlacionadas e facilita o c√°lculo eficiente desse fator de atualiza√ß√£o [^4.4.1].

#### O Fator de Atualiza√ß√£o e a Covari√¢ncia dos Res√≠duos

O fator de atualiza√ß√£o $h_{32}h_{22}^{-1}$ √© um elemento essencial na atualiza√ß√£o de proje√ß√µes lineares. Como estabelecido anteriormente [^4.5.14], esse fator surge da seguinte express√£o:
$$P(Y_3|Y_2,Y_1) = P(Y_3|Y_1) + h_{32}h_{22}^{-1} [Y_2 - P(Y_2|Y_1)].$$
onde $h_{32}$ e $h_{22}$ s√£o definidos como:
$$ h_{32} = E\{[Y_3 - P(Y_3|Y_1)][Y_2 - P(Y_2|Y_1)]\}$$
e
$$h_{22} = E[Y_2 - P(Y_2|Y_1)]^2$$

Essas quantidades representam covari√¢ncias e vari√¢ncias dos res√≠duos das proje√ß√µes. Especificamente:
- $h_{22}$ representa a vari√¢ncia do res√≠duo da proje√ß√£o de $Y_2$ em $Y_1$.
- $h_{32}$ representa a covari√¢ncia entre o res√≠duo da proje√ß√£o de $Y_3$ em $Y_1$ e o res√≠duo da proje√ß√£o de $Y_2$ em $Y_1$.

Portanto, o fator de atualiza√ß√£o $h_{32}h_{22}^{-1}$ pode ser interpretado como a raz√£o entre a covari√¢ncia entre os res√≠duos das proje√ß√µes de $Y_3$ e $Y_2$ sobre $Y_1$ e a vari√¢ncia do res√≠duo da proje√ß√£o de $Y_2$ sobre $Y_1$.

**Lema 3:** *O fator de atualiza√ß√£o $h_{32}h_{22}^{-1}$ √© igual ao coeficiente da proje√ß√£o linear do res√≠duo de $Y_3$ na proje√ß√£o de $Y_1$ sobre o res√≠duo de $Y_2$ na proje√ß√£o de $Y_1$.*
*Demonstra√ß√£o:*
I. Definimos os res√≠duos como:
   $$e_{3|1} = Y_3 - P(Y_3|Y_1)$$
   $$e_{2|1} = Y_2 - P(Y_2|Y_1)$$
II. O fator de atualiza√ß√£o √© dado por:
$$h_{32}h_{22}^{-1} = \frac{E(e_{3|1}e_{2|1})}{E(e_{2|1}^2)}$$
III. A express√£o acima √© exatamente o coeficiente da proje√ß√£o de $e_{3|1}$ sobre $e_{2|1}$. Logo, o fator de atualiza√ß√£o √© igual ao coeficiente da proje√ß√£o linear do res√≠duo de $Y_3$ em rela√ß√£o a $Y_1$ sobre o res√≠duo de $Y_2$ em rela√ß√£o a $Y_1$. ‚ñ†

**Proposi√ß√£o 2:** *O fator de atualiza√ß√£o $h_{32}h_{22}^{-1}$ pode ser expresso em termos da matriz de covari√¢ncia $\Omega$ como:*
$$h_{32}h_{22}^{-1} = \frac{\Omega_{32} - \Omega_{31}\Omega_{11}^{-1}\Omega_{12}}{\Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}}$$
*Demonstra√ß√£o:*
I. Da defini√ß√£o, $h_{32} = E\{[Y_3 - P(Y_3|Y_1)][Y_2 - P(Y_2|Y_1)]\}$ e $h_{22} = E[Y_2 - P(Y_2|Y_1)]^2$.
II. Sabemos que $P(Y_3|Y_1) = \Omega_{31}\Omega_{11}^{-1}Y_1$ e $P(Y_2|Y_1) = \Omega_{21}\Omega_{11}^{-1}Y_1$.
III. Substituindo as proje√ß√µes em $h_{32}$, temos:
    $$h_{32} = E\{[Y_3 - \Omega_{31}\Omega_{11}^{-1}Y_1][Y_2 - \Omega_{21}\Omega_{11}^{-1}Y_1]\}$$
IV. Expandindo, obtemos:
  $$h_{32} = E(Y_3Y_2) - E(Y_3\Omega_{21}\Omega_{11}^{-1}Y_1) - E(\Omega_{31}\Omega_{11}^{-1}Y_1Y_2) + E(\Omega_{31}\Omega_{11}^{-1}Y_1\Omega_{21}\Omega_{11}^{-1}Y_1)$$
V. Usando as propriedades da esperan√ßa e da matriz de covari√¢ncia:
    $$h_{32} = \Omega_{32} - \Omega_{31}\Omega_{11}^{-1}\Omega_{12} - \Omega_{31}\Omega_{11}^{-1}\Omega_{12} + \Omega_{31}\Omega_{11}^{-1}\Omega_{11}\Omega_{11}^{-1}\Omega_{12}$$
VI. Simplificando, temos:
    $$h_{32} = \Omega_{32} - \Omega_{31}\Omega_{11}^{-1}\Omega_{12}$$
VII. De forma an√°loga, $h_{22} = \Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}$.
VIII. Portanto, o fator de atualiza√ß√£o √©:
    $$h_{32}h_{22}^{-1} = \frac{\Omega_{32} - \Omega_{31}\Omega_{11}^{-1}\Omega_{12}}{\Omega_{22} - \Omega_{21}\Omega_{11}^{-1}\Omega_{12}}$$
Assim, o fator de atualiza√ß√£o pode ser expresso diretamente em termos das covari√¢ncias e das proje√ß√µes das vari√°veis originais. ‚ñ†

> üí° **Exemplo Num√©rico:**
> Considere a matriz de covari√¢ncia
> $$\Omega = \begin{bmatrix}
> 4 & 1 & 2 \\
> 1 & 2 & 0.5 \\
> 2 & 0.5 & 5
> \end{bmatrix}$$
>
> Temos $\Omega_{11} = 4$, $\Omega_{12} = 1$, $\Omega_{21} = 1$, $\Omega_{22} = 2$, $\Omega_{31} = 2$, $\Omega_{32} = 0.5$. Ent√£o, usando a proposi√ß√£o 2, o fator de atualiza√ß√£o √©
> $$h_{32}h_{22}^{-1} = \frac{0.5 - 2 \cdot 4^{-1} \cdot 1}{2 - 1 \cdot 4^{-1} \cdot 1} = \frac{0.5 - 0.5}{2 - 0.25} = 0$$
>
> Nesse caso, o fator de atualiza√ß√£o √© zero, indicando que a informa√ß√£o adicional de $Y_2$ n√£o contribui para a proje√ß√£o de $Y_3$ al√©m do que j√° √© provido por $Y_1$.
>
> Agora, vamos considerar uma matriz de covari√¢ncia diferente, onde a rela√ß√£o entre $Y_2$ e $Y_3$ √© mais forte ap√≥s remover o efeito de $Y_1$:
>
> $$\Omega' = \begin{bmatrix}
> 4 & 1 & 1 \\
> 1 & 2 & 1.5 \\
> 1 & 1.5 & 5
> \end{bmatrix}$$
>
> Usando os mesmos passos: $\Omega'_{11} = 4$, $\Omega'_{12} = 1$, $\Omega'_{21} = 1$, $\Omega'_{22} = 2$, $\Omega'_{31} = 1$, $\Omega'_{32} = 1.5$.
>
> Ent√£o, o fator de atualiza√ß√£o √©:
> $$h_{32}h_{22}^{-1} = \frac{1.5 - 1 \cdot 4^{-1} \cdot 1}{2 - 1 \cdot 4^{-1} \cdot 1} = \frac{1.5 - 0.25}{2 - 0.25} = \frac{1.25}{1.75} \approx 0.714$$
>
> Neste caso, o fator de atualiza√ß√£o √© aproximadamente 0.714, indicando que a informa√ß√£o adicional de $Y_2$ contribui positivamente para a proje√ß√£o de $Y_3$ ap√≥s remover o efeito de $Y_1$. Um aumento de uma unidade no res√≠duo de $Y_2$ em rela√ß√£o a $Y_1$ leva a um aumento de aproximadamente 0.714 unidades na proje√ß√£o de $Y_3$.

#### Interpreta√ß√£o Geom√©trica do Fator de Atualiza√ß√£o

O fator de atualiza√ß√£o $h_{32}h_{22}^{-1}$ pode ser interpretado geometricamente como a inclina√ß√£o da proje√ß√£o linear do res√≠duo de $Y_3$ ap√≥s a proje√ß√£o em $Y_1$ sobre o res√≠duo de $Y_2$ ap√≥s a proje√ß√£o em $Y_1$.
Em outras palavras, ele mede o quanto a nova informa√ß√£o (o componente n√£o antecipado de $Y_2$) afeta a previs√£o de $Y_3$.
   - Se $h_{32}h_{22}^{-1}$ for positivo, um valor acima da m√©dia de $Y_2$ (ap√≥s remover o efeito de $Y_1$) indica que $Y_3$ tamb√©m deve ser maior que o previsto com base apenas em $Y_1$.
   - Se $h_{32}h_{22}^{-1}$ for negativo, o oposto ocorre.
   - Se $h_{32}h_{22}^{-1}$ for zero, a nova informa√ß√£o de $Y_2$ n√£o contribui para prever $Y_3$.

Al√©m disso, $h_{22}$ √© a vari√¢ncia do res√≠duo de $Y_2$ ap√≥s projetar em $Y_1$. Assim, o fator de atualiza√ß√£o tamb√©m reflete o grau de incerteza associado √† proje√ß√£o de $Y_2$ em $Y_1$. Se o erro ao prever $Y_2$ com base em $Y_1$ for pequeno ($h_{22}$ pequeno), ent√£o o fator de atualiza√ß√£o pode ter um valor maior e, por conseguinte, a influ√™ncia da nova informa√ß√£o sobre a proje√ß√£o de $Y_3$ ser√° maior.

#### O Fator de Atualiza√ß√£o e a Efici√™ncia da Proje√ß√£o
O fator de atualiza√ß√£o tamb√©m est√° ligado √† efici√™ncia da atualiza√ß√£o da proje√ß√£o linear. Se $h_{32}$ for grande em rela√ß√£o a $h_{22}$, significa que o res√≠duo de $Y_2$ cont√©m muita informa√ß√£o relevante para prever o res√≠duo de $Y_3$, e a atualiza√ß√£o trar√° uma melhora substancial na proje√ß√£o. Caso contr√°rio, a contribui√ß√£o da nova informa√ß√£o de $Y_2$ ser√° menor.
  * Caso $h_{32} = 0$, o fator de atualiza√ß√£o ser√° zero, e a adi√ß√£o de $Y_2$ √† proje√ß√£o n√£o diminui o erro de previs√£o, o que sugere que a informa√ß√£o de $Y_2$ √© redundante para a proje√ß√£o de $Y_3$ dados os dados de $Y_1$ j√° utilizados.

**Proposi√ß√£o 2.1:** *O fator de atualiza√ß√£o pode ser expresso em termos de proje√ß√µes lineares como:*
$$h_{32}h_{22}^{-1} = \frac{Cov(Y_3 - P(Y_3|Y_1), Y_2 - P(Y_2|Y_1))}{Var(Y_2 - P(Y_2|Y_1))}$$
*Demonstra√ß√£o:*
I. Definimos os res√≠duos $e_{3|1} = Y_3 - P(Y_3|Y_1)$ e $e_{2|1} = Y_2 - P(Y_2|Y_1)$.
II. Pela defini√ß√£o de covari√¢ncia, $Cov(e_{3|1}, e_{2|1}) = E[(e_{3|1} - E[e_{3|1}])(e_{2|1} - E[e_{2|1}])]$.
III. Como $E[e_{3|1}] = 0$ e $E[e_{2|1}] = 0$ (propriedade dos res√≠duos), $Cov(e_{3|1}, e_{2|1}) = E[e_{3|1}e_{2|1}] = h_{32}$.
IV. De maneira semelhante, $Var(e_{2|1}) = E[(e_{2|1} - E[e_{2|1}])^2] = E[e_{2|1}^2] = h_{22}$.
V. Portanto, $h_{32}h_{22}^{-1} = \frac{Cov(e_{3|1}, e_{2|1})}{Var(e_{2|1})}= \frac{Cov(Y_3 - P(Y_3|Y_1), Y_2 - P(Y_2|Y_1))}{Var(Y_2 - P(Y_2|Y_1))}$.‚ñ†

> üí° **Exemplo Num√©rico:**
>
> No exemplo anterior em que a matriz de covari√¢ncia era
> $$\Omega = \begin{bmatrix}
> 4 & 1 & 2 \\
> 1 & 2 & 0.5 \\
> 2 & 0.5 & 5
> \end{bmatrix}$$
>
> O fator de atualiza√ß√£o era 0.  Isto significa que, a proje√ß√£o de $Y_3$ com base em $Y_1$ j√° incorpora toda informa√ß√£o relevante da proje√ß√£o de $Y_2$ com base em $Y_1$. Ao analisar a matriz $\Omega$ vemos que $\Omega_{32}$ √© pequeno, indicando que $Y_2$ n√£o adiciona muita informa√ß√£o sobre $Y_3$.
>
>  Se o termo $\Omega_{32}$ fosse maior, a atualiza√ß√£o teria um maior impacto na proje√ß√£o, ou seja, ao projetarmos $Y_3$ sobre $Y_1$, a informa√ß√£o de $Y_2$ n√£o estaria sendo totalmente capturada e por isso o termo de atualiza√ß√£o aumentaria o ajuste do modelo.
>
> Se o termo  $\Omega_{21}$ fosse zero, as vari√°veis $Y_1$ e $Y_2$ seriam ortogonais, e o fator de atualiza√ß√£o se simplificaria para $\frac{\Omega_{32}}{\Omega_{22}}$, que √© o coeficiente da proje√ß√£o de $Y_3$ em $Y_2$.
>
> Para demonstrar isto, vamos usar uma matriz de covari√¢ncia com $\Omega_{21} = 0$.
>
> $$\Omega'' = \begin{bmatrix}
> 4 & 0 & 2 \\
> 0 & 2 & 1 \\
> 2 & 1 & 5
> \end{bmatrix}$$
>
> Neste caso, $\Omega''_{11} = 4$, $\Omega''_{12} = 0$, $\Omega''_{21} = 0$, $\Omega''_{22} = 2$, $\Omega''_{31} = 2$, $\Omega''_{32} = 1$.
>
> O fator de atualiza√ß√£o √©:
>
> $$h_{32}h_{22}^{-1} = \frac{1 - 2 \cdot 4^{-1} \cdot 0}{2 - 0 \cdot 4^{-1} \cdot 0} = \frac{1}{2} = 0.5$$
>
> De fato, como $\Omega_{21} = 0$, temos que $Y_1$ e $Y_2$ s√£o ortogonais e o fator de atualiza√ß√£o se reduz a $\frac{\Omega_{32}}{\Omega_{22}} = \frac{1}{2}$. Isso indica que a influ√™ncia da nova informa√ß√£o de $Y_2$ na proje√ß√£o de $Y_3$ √© dada diretamente pela covari√¢ncia entre $Y_2$ e $Y_3$, ajustada pela vari√¢ncia de $Y_2$.

### Conclus√£o

O fator de atualiza√ß√£o $h_{32}h_{22}^{-1}$ √© um componente fundamental na atualiza√ß√£o de proje√ß√µes lineares, representando a inclina√ß√£o da rela√ß√£o linear entre o res√≠duo de $Y_3$ e o res√≠duo de $Y_2$ ap√≥s a proje√ß√£o em $Y_1$. Ele incorpora a covari√¢ncia entre esses res√≠duos e a incerteza na previs√£o de $Y_2$ com base em $Y_1$. A compreens√£o deste fator permite uma melhor interpreta√ß√£o do mecanismo de atualiza√ß√£o de proje√ß√µes lineares e sua rela√ß√£o com a fatora√ß√£o triangular da matriz de momentos. A an√°lise apresentada fornece uma base para entender como novas informa√ß√µes modificam proje√ß√µes existentes e como a fatora√ß√£o triangular contribui para otimizar este processo.

### Refer√™ncias
[^4.1.10]: *Se√ß√£o 4.1, p√°gina 73*
[^4.4.1]:  *Se√ß√£o 4.4, p√°gina 87*
[^4.5.2]:  *Se√ß√£o 4.5, p√°gina 92*
[^4.5.12]: *Se√ß√£o 4.5, p√°gina 93*
[^4.5.14]: *Se√ß√£o 4.5, p√°gina 94*

### 5.2. Likelihood Function for an AR(1) Process
**Lema 1:** *The conditional expectation of $Y_t$ given the past is $E[Y_t|Y_{t-1},Y_{t-2},...] = c + \phi Y_{t-1}$ for an AR(1) process.*

*Proof:* Given the AR(1) model, $Y_t = c + \phi Y_{t-1} + \epsilon_t$, we know that $E[\epsilon_t | Y_{t-1}, Y_{t-2}, ... ] = 0$, since the errors are assumed to be i.i.d. and thus independent of the past. Hence, taking conditional expectations, we get:
$$E[Y_t|Y_{t-1},Y_{t-2},...] = E[c + \phi Y_{t-1} + \epsilon_t|Y_{t-1},Y_{t-2},...] = c + \phi E[Y_{t-1}|Y_{t-1},Y_{t-2},...] + E[\epsilon_t|Y_{t-1},Y_{t-2},...]=c+\phi Y_{t-1}+0$$
Thus, $E[Y_t|Y_{t-1},Y_{t-2},...] = c + \phi Y_{t-1}$ .‚ñ†

We begin by deriving the likelihood function for an AR(1) process. Consider the model:

$$Y_t = c + \phi Y_{t-1} + \epsilon_t$$ [5.2.1]

where $\epsilon_t \sim i.i.d. N(0, \sigma^2)$.  To calculate the likelihood, we need the joint density of the observed data $Y_1, Y_2, \ldots, Y_T$.  Since the $\epsilon_t$ are independent and normally distributed, the conditional distribution of $Y_t$ given $Y_{t-1}$ is also normal, with mean $c + \phi Y_{t-1}$ and variance $\sigma^2$:

$$Y_t | Y_{t-1} \sim N(c + \phi Y_{t-1}, \sigma^2)$$

The joint density can be written as the product of conditional densities:

$$f_{Y_1, Y_2, \ldots, Y_T}(y_1, y_2, \ldots, y_T; c, \phi, \sigma^2) = f_{Y_1}(y_1; c, \phi, \sigma^2) \prod_{t=2}^{T} f_{Y_t|Y_{t-1}}(y_t | y_{t-1}; c, \phi, \sigma^2)$$  [5.2.2]

The conditional densities are straightforward to compute from the Gaussian assumption:

$$f_{Y_t|Y_{t-1}}(y_t | y_{t-1}; c, \phi, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_t - c - \phi y_{t-1})^2}{2\sigma^2}\right)$$ [5.2.3]

However, the initial value $Y_1$ does not have a preceeding value with which to condition upon.  To proceed, we must make some assumption about its distribution.  A common, albeit not perfectly accurate assumption is that $Y_1$ is drawn from the unconditional distribution of $Y_t$.  For a stationary AR(1), we have that the unconditional mean is given by $\mu = \frac{c}{1 - \phi}$ and the unconditional variance is given by $\frac{\sigma^2}{1 - \phi^2}$.  Thus, our assumption about the initial condition is

$$Y_1 \sim N\left(\frac{c}{1 - \phi}, \frac{\sigma^2}{1 - \phi^2}\right)$$
and therefore

$$f_{Y_1}(y_1; c, \phi, \sigma^2) = \frac{1}{\sqrt{2\pi\frac{\sigma^2}{1 - \phi^2}}} \exp\left(-\frac{(y_1 - \frac{c}{1 - \phi})^2}{2\frac{\sigma^2}{1-\phi^2}}\right)$$ [5.2.4]

Plugging [5.2.3] and [5.2.4] into [5.2.2], the likelihood function for the AR(1) process is given by:

$$L(c, \phi, \sigma^2 | Y_1, \ldots, Y_T) = \left(\frac{1}{\sqrt{2\pi\frac{\sigma^2}{1-\phi^2}}}\right) \exp\left(-\frac{(y_1 - \frac{c}{1-\phi})^2}{2\frac{\sigma^2}{1-\phi^2}}\right) \prod_{t=2}^{T} \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) \exp\left(-\frac{(y_t - c - \phi y_{t-1})^2}{2\sigma^2}\right)$$  [5.2.5]

It is more convenient to work with the log-likelihood:

$$\mathcal{L}(c, \phi, \sigma^2 | Y_1, \ldots, Y_T) = -\frac{1}{2} \log(2\pi) - \frac{1}{2}\log\left(\frac{\sigma^2}{1-\phi^2}\right)  -\frac{(y_1 - \frac{c}{1-\phi})^2}{2\frac{\sigma^2}{1-\phi^2}} - \frac{T-1}{2} \log(2\pi) - \frac{T-1}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=2}^{T} (y_t - c - \phi y_{t-1})^2$$
[5.2.6]

The maximum likelihood estimates of the parameters are those that maximize [5.2.6], or equivalently, minimize the negative of the log-likelihood function.  In general, there is no closed-form solution for the parameters, requiring numerical optimization techniques. However, if $\phi$ is known, then it is straightforward to minimize the negative of the likelihood function with respect to $c$ and $\sigma^2$ using OLS. In particular, conditional on $\phi$, we can interpret $Y_t - \phi Y_{t-1}$ as the dependent variable in an OLS regression with constant c.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos uma s√©rie temporal de 5 observa√ß√µes de um processo AR(1):
> $Y = [1.2, 1.8, 2.1, 2.5, 2.9]$
>
> Assumindo um valor de $\phi = 0.5$, podemos calcular os par√¢metros $c$ e $\sigma^2$ usando OLS.
> Primeiro, constru√≠mos uma nova s√©rie temporal $Y^*_t = Y_t - \phi Y_{t-1}$. Como $Y_0$ n√£o √© observado, assumimos que $Y^*_1 = Y_1 = 1.2$.
>
> $Y^*_1 = 1.2$
> $Y^*_2 = 1.8 - 0.5 * 1.2 = 1.2$
> $Y^*_3 = 2.1 - 0.5 * 1.8 = 1.2$
> $Y^*_4 = 2.5 - 0.5 * 2.1 = 1.45$
> $Y^*_5 = 2.9 - 0.5 * 2.5 = 1.65$
>
> Ent√£o, $Y^* = [1.2, 1.2, 1.2, 1.45, 1.65]$. Usando OLS em $Y^*_t = c + \epsilon_t$, obtemos
>
> $$c = \frac{1}{5}\sum_{t=1}^{5} Y^*_t = \frac{1.2 + 1.2 + 1.2 + 1.45 + 1.65}{5} = 1.3$$
>
> E para $\sigma^2$, calculamos os res√≠duos:
> $\epsilon_1 = 1.2 - 1.3 = -0.1$
> $\epsilon_2 = 1.2 - 1.3 = -0.1$
> $\epsilon_3 = 1.2 - 1.3 = -0.1$
> $\epsilon_4 = 1.45 - 1.3 = 0.15$
> $\epsilon_5 = 1.65 - 1.3 = 0.35$
>
> E
> $$\sigma^2 = \frac{1}{5}\sum_{t=1}^{5} \epsilon_t^2 = \frac{(-0.1)^2 + (-0.1)^2 + (-0.1)^2 + (0.15)^2 + (0.35)^2}{5} = 0.031$$
>
> Assim, os par√¢metros estimados condicionalmente a $\phi = 0.5$ s√£o: $c \approx 1.3$ e $\sigma^2 \approx 0.031$. Esses resultados minimizam a soma dos erros quadr√°ticos condicional a um dado $\phi$.  Para obter os par√¢metros de m√°xima verossimilhan√ßa, seria necess√°rio realizar uma otimiza√ß√£o num√©rica que variasse $\phi$ e recalcularia $c$ e $\sigma^2$ recursivamente.

### 5.3. Likelihood Function for an MA(1) Process

Now, consider the moving average process of order one:

$$Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$$ [5.3.1]

where $\epsilon_t \sim i.i.d. N(0, \sigma^2)$. In this case, the likelihood function cannot be written in terms of simple conditional densities of $Y_t$ given $Y_{t-1}$. The difficulty here arises because the errors, $\epsilon_t$, are not directly observed. We can not simply condition on $\epsilon_{t-1}$ or, likewise, on $Y_{t-1}$ (as we did in the AR(1) case), because those errors are unobserved. To proceed, we need to derive the joint distribution of $(Y_1, Y_2, \ldots, Y_T)$. This can be accomplished by using the fact that the joint distribution of the $\epsilon_t$ is known, and then using the transformations implied by equation [5.3.1] to get the joint distribution of the $Y_t$. In practice, the calculation is easier to carry out if we treat the unknown $\epsilon$ from before the sample period ($\epsilon_0$, $\epsilon_{-1}$, etc.) as fixed at zero. With that assumption, we can calculate the values of the $\epsilon$ by rearranging [5.3.1]:
$$\epsilon_t = Y_t - \mu - \theta \epsilon_{t-1}$$ [5.3.2]
$$\epsilon_1 = Y_1 - \mu$$
$$\epsilon_2 = Y_2 - \mu - \theta \epsilon_1 = Y_2 - \mu - \theta (Y_1 - \mu)$$
$$\epsilon_3 = Y_3 - \mu - \theta \epsilon_2 = Y_3 - \mu - \theta(Y_2 - \mu - \theta (Y_1 - \mu))$$
$$\vdots$$

We can continue this substitution to rewrite $\epsilon_t$ in terms of past values of $Y_t$.  With the assumption that  $\epsilon_0=0$, the vector $\epsilon = (\epsilon_1, \ldots, \epsilon_T)$ can be considered a linear transformation of the vector $Y = (Y_1, \ldots, Y_T)$. As such, the vector $Y$ will have a multivariate normal distribution if the vector of errors has a multivariate normal distribution. Let us denote the sample mean as $\bar{y}$.  Then, we have,
$$ \text{vec}(Y-\mu) = A \text{vec}(\epsilon),$$
where A is a lower triangular matrix.  To see this, we can write the system of equation for the first 3 values of $Y$ and $\epsilon$ from [5.3.2]:
$$
\begin{bmatrix}
Y_1 - \mu \\
Y_2 - \mu \\
Y_3 - \mu
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & 0 \\
\theta & 1 & 0 \\
\theta^2 & \theta & 1
\end{bmatrix}
\begin{bmatrix}
\epsilon_1 \\
\epsilon_2 \\
\epsilon_3
\end{bmatrix}
$$
The determinant of this transformation is 1. Therefore, the joint distribution of $Y$ can be written as:

$$f_{Y_1, Y_2, \ldots, Y_T}(y_1, y_2, \ldots, y_T; \mu, \theta, \sigma^2) = \frac{1}{(2\pi\sigma^2)^{T/2} |A|} \exp\left(-\frac{1}{2\sigma^2} \epsilon' \epsilon\right)$$ [5.3.3]
Where $\epsilon' \epsilon = \sum_t \epsilon_t^2$. Because the determinant is 1, the log-likelihood is

$$\mathcal{L}(\mu, \theta, \sigma^2 | Y_1, \ldots, Y_T) = -\frac{T}{2} \log(2\pi) -\frac{T}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=1}^T \epsilon_t^2 $$
[5.3.4]

Again, the maximum likelihood estimates of the parameters are those that maximize [5.3.4], or equivalently, minimize the negative of the log-likelihood. These values can only be found using numerical optimization. However, if $\theta$ is known, the parameters $\mu$ and $\sigma^2$ can be found using a procedure very similar to OLS. To do this, note that the sum of the squared errors depends on previous errors, $\epsilon_{t-1}$. To perform a standard OLS regression we will therefore need to transform the data such that the parameters of the MA model are the error terms. By rewriting [5.3.1], we have that
$$ \epsilon_t = Y_t - \mu - \theta\epsilon_{t-1} $$
We thus transform the data as $Y_t^* = Y_t - \theta \epsilon_{t-1}$. Note that since $\epsilon_0=0$, we have that $Y_1^* = Y_1$.  Thus, once we have an estimate for $\theta$, the parameters $\mu$ and $\sigma^2$ can be found by performing OLS on the transformed data $Y^*$, for which
$$ Y_t^* = \mu + \epsilon_t $$.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos a seguinte s√©rie temporal de 4 observa√ß√µes de um processo MA(1):
> $Y = [1.5, 2.0, 1.8, 2.2]$
>
> E assumindo que $\theta = 0.3$, podemos calcular $\mu$ e $\sigma^2$ utilizando OLS.
>
> Primeiro, precisamos calcular os valores dos erros $\epsilon_t$ recursivamente, lembrando que $\epsilon_0 = 0$.
>
> $\epsilon_1 = Y_1 - \mu = 1.5 - \mu$
> $\epsilon_2 = Y_2 - \mu - \theta\epsilon_1 = 2.0 - \mu - 0.3(1.5 - \mu) = 2.0 - \mu - 0.45 + 0.3\mu = 1.55 - 0.7\mu$
> $\epsilon_3 = Y_3 - \mu - \theta\epsilon_2 = 1.8 - \mu - 0.3(1.55 - 0.7\mu) = 1.8 - \mu - 0.465 + 0.21\mu = 1.335 - 0.79\mu$
> $\epsilon_4 = Y_4 - \mu - \theta\epsilon_3 = 2.2 - \mu - 0.3(1.335 - 0.79\mu) = 2.2 - \mu - 0.4005 + 0.237\mu = 1.7$1.7995 - 0.763\mu$
>$\epsilon_5 = Y_5 - \mu - \theta\epsilon_4 = 2.5 - \mu - 0.3(1.7995 - 0.763\mu) = 2.5 - \mu - 0.53985 + 0.2289\mu = 1.96015 - 0.7711\mu$

Now, to find the estimate of $\mu$, we need to minimize the sum of squared errors:

$S(\mu) = \epsilon_1^2 + \epsilon_2^2 + \epsilon_3^2 + \epsilon_4^2 + \epsilon_5^2$
$S(\mu) = (1.0 - \mu)^2 + (1.25 - 0.7\mu)^2 + (1.335 - 0.79\mu)^2 + (1.7995 - 0.763\mu)^2 + (1.96015 - 0.7711\mu)^2$

Taking the derivative with respect to $\mu$ and setting it equal to zero:

$\frac{dS(\mu)}{d\mu} = 2(1.0 - \mu)(-1) + 2(1.25 - 0.7\mu)(-0.7) + 2(1.335 - 0.79\mu)(-0.79) + 2(1.7995 - 0.763\mu)(-0.763) + 2(1.96015 - 0.7711\mu)(-0.7711) = 0$

$-2(1.0 - \mu) - 1.4(1.25 - 0.7\mu) - 1.58(1.335 - 0.79\mu) - 1.526(1.7995 - 0.763\mu) - 1.5422(1.96015 - 0.7711\mu) = 0$

$-2 + 2\mu - 1.75 + 0.98\mu - 2.1093 + 1.2482\mu - 2.7461 + 1.1643\mu - 3.0231 + 1.1892\mu = 0$

$(-2 - 1.75 - 2.1093 - 2.7461 - 3.0231) + (2 + 0.98 + 1.2482 + 1.1643 + 1.1892)\mu = 0$

$-11.6285 + 6.5817\mu = 0$

$6.5817\mu = 11.6285$

$\mu = \frac{11.6285}{6.5817} \approx 1.7668$

Therefore, the estimate of $\mu$ is approximately $1.7668$.

Now let's calculate the estimates for $\epsilon_1, \epsilon_2, \epsilon_3, \epsilon_4, \epsilon_5$ with $\hat{\mu} \approx 1.7668$:

$\epsilon_1 = 1.0 - 1.7668 = -0.7668$
$\epsilon_2 = 1.25 - 0.7(1.7668) = 1.25 - 1.23676 = 0.01324$
$\epsilon_3 = 1.335 - 0.79(1.7668) = 1.335 - 1.395772 = -0.060772$
$\epsilon_4 = 1.7995 - 0.763(1.7668) = 1.7995 - 1.35197 = 0.44753$
$\epsilon_5 = 1.96015 - 0.7711(1.7668) = 1.96015 - 1.36156 = 0.59859$

And here are the values of $Y_i$ based on this estimate of $\mu$:
$Y_1 = 1.0$
$Y_2 = 1.25$
$Y_3 = 1.55$
$Y_4 = 2.2$
$Y_5 = 2.5$
<!-- END -->
