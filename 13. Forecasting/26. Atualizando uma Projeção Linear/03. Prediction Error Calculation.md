## DecomposiÃ§Ã£o do Erro de PrevisÃ£o e a AtualizaÃ§Ã£o da ProjeÃ§Ã£o Linear

### IntroduÃ§Ã£o
Este capÃ­tulo continua a anÃ¡lise das projeÃ§Ãµes lineares, com foco na decomposiÃ§Ã£o do erro de previsÃ£o quando se utiliza informaÃ§Ãµes adicionais para atualizar a projeÃ§Ã£o inicial. Em continuidade aos capÃ­tulos anteriores, que discutiram a atualizaÃ§Ã£o de projeÃ§Ãµes lineares e a decomposiÃ§Ã£o dessas projeÃ§Ãµes, este capÃ­tulo explora como o erro de previsÃ£o Ã© afetado pela inclusÃ£o de novas informaÃ§Ãµes e como a fatoraÃ§Ã£o triangular da matriz de momentos contribui para a compreensÃ£o desse processo [^4.5.13].

### Conceitos Fundamentais
Como vimos, a fatoraÃ§Ã£o triangular da matriz de covariÃ¢ncia $\Omega$, definida por $\Omega = ADA'$, desempenha um papel fundamental na anÃ¡lise de projeÃ§Ãµes lineares [^4.4.1]. Essa decomposiÃ§Ã£o nos permite transformar variÃ¡veis originais $Y$ em variÃ¡veis nÃ£o correlacionadas $\tilde{Y}$, onde $\tilde{Y} = A^{-1}Y$ [^4.5.2]. Essa transformaÃ§Ã£o facilita a anÃ¡lise da atualizaÃ§Ã£o das projeÃ§Ãµes e tambÃ©m permite que decomponhamos o erro de previsÃ£o de forma a entender os efeitos da inclusÃ£o de novas variÃ¡veis [^4.5.4], [^4.5.6].

#### DecomposiÃ§Ã£o do Erro de PrevisÃ£o

Quando projetamos $Y_3$ com base em $Y_1$, o erro de previsÃ£o Ã© dado por [^4.5.13]:
$$E[Y_3 - P(Y_3|Y_1)]^2 = d_{33}$$
onde $d_{33}$ Ã© o terceiro elemento diagonal da matriz $D$ na fatoraÃ§Ã£o triangular de $\Omega$. Se, alÃ©m de $Y_1$, tivermos tambÃ©m $Y_2$, a projeÃ§Ã£o de $Y_3$ Ã© dada por:
$$P(Y_3|Y_2, Y_1) = P(Y_3|Y_1) + h_{32}h_{22}^{-1} [Y_2 - P(Y_2|Y_1)]$$
E o erro de previsÃ£o correspondente Ã©:
$$E[Y_3 - P(Y_3|Y_2, Y_1)]^2 = d_{33} - h_{32}h_{22}^{-1}h_{23}$$
ou, de maneira equivalente,
$$ E[Y_3 - P(Y_3|Y_2, Y_1)]^2 = h_{33} - h_{32}h_{22}^{-1}h_{23} $$
> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que temos a seguinte matriz de covariÃ¢ncia (para simplificar, considere $h_{23} = h_{32}$):
> $$\Omega = \begin{bmatrix}
> 4 & 1 & 2 \\
> 1 & 2 & 0.5 \\
> 2 & 0.5 & 5
> \end{bmatrix}$$
>
> A projeÃ§Ã£o de $Y_3$ sobre $Y_1$ Ã©: $P(Y_3|Y_1) = \frac{2}{4}Y_1 = 0.5Y_1$. O erro de previsÃ£o Ã© $d_{33}$ que Ã© a variÃ¢ncia do resÃ­duo de $Y_3$ quando projetado sobre $Y_1$. Usando a fatoraÃ§Ã£o triangular da seÃ§Ã£o anterior, assumimos que $d_{33} = 4.929$, entÃ£o o erro de previsÃ£o Ã© 4.929.
>
> A projeÃ§Ã£o de $Y_3$ sobre $Y_1$ e $Y_2$ Ã© $P(Y_3|Y_2, Y_1) = 0.5Y_1 + 0.214[Y_2 - 0.25Y_1]$. O erro de previsÃ£o Ã© $d_{33} - h_{32}h_{22}^{-1}h_{23} = d_{33} - (h_{32}^2)/h_{22}$. Temos que $h_{22} = 1.75$ e $h_{32} = 0.375$, assim, o erro de previsÃ£o Ã© $4.929 - 0.375^2/1.75 = 4.929 - 0.08 = 4.849$. O erro de previsÃ£o diminuiu quando a informaÃ§Ã£o de $Y_2$ Ã© incluÃ­da na projeÃ§Ã£o de $Y_3$.

**Teorema 3:** *O erro de previsÃ£o de $Y_3$ com base em $Y_1$ e $Y_2$ pode ser decomposto em duas partes: o erro de previsÃ£o de $Y_3$ com base em $Y_1$ e o componente nÃ£o antecipado de $Y_2$ multiplicado pelo fator de atualizaÃ§Ã£o $h_{32}h_{22}^{-1}$, de acordo com a seguinte relaÃ§Ã£o:*
$$
E[Y_3 - P(Y_3|Y_2,Y_1)]^2 = E[Y_3 - P(Y_3|Y_1)]^2 - h_{32}h_{22}^{-1} h_{23}
$$
*DemonstraÃ§Ã£o:*
I.  Definimos o erro da projeÃ§Ã£o de $Y_3$ sobre $Y_1$ como $e_1 = Y_3 - P(Y_3|Y_1)$, que possui variÃ¢ncia $d_{33}$.
II.  Sabemos tambÃ©m que a projeÃ§Ã£o de $Y_3$ em $Y_1$ e $Y_2$ pode ser expressa como:
$$P(Y_3|Y_2,Y_1) = P(Y_3|Y_1) + h_{32}h_{22}^{-1} [Y_2 - P(Y_2|Y_1)].$$
III.  O erro da projeÃ§Ã£o de $Y_3$ em $Y_1$ e $Y_2$ Ã© dado por:
$$e_2 = Y_3 - P(Y_3|Y_2, Y_1) = Y_3 - P(Y_3|Y_1) - h_{32}h_{22}^{-1} [Y_2 - P(Y_2|Y_1)]$$
IV. Substituindo $e_1$ na equaÃ§Ã£o anterior, temos:
$$e_2 = e_1 -  h_{32}h_{22}^{-1} [Y_2 - P(Y_2|Y_1)]$$
V.  A variÃ¢ncia do erro de projeÃ§Ã£o, que Ã© o erro quadrÃ¡tico mÃ©dio, Ã© entÃ£o:
$$E[e_2^2] = E[e_1^2] - 2h_{32}h_{22}^{-1}E[e_1(Y_2 - P(Y_2|Y_1))] + E[h_{32}h_{22}^{-1}(Y_2 - P(Y_2|Y_1))]^2$$
VI. Como $E[e_1(Y_2 - P(Y_2|Y_1))]$ Ã© igual a $h_{32}$ e, $E[Y_2 - P(Y_2|Y_1)]^2 = h_{22}$
$$E[e_2^2] = E[e_1^2] - 2h_{32}h_{22}^{-1}h_{32} + (h_{32}h_{22}^{-1})^2 h_{22} = E[e_1^2] - 2h_{32}^2 h_{22}^{-1} + h_{32}^2 h_{22}^{-1}$$
VII. Simplificando, temos
$$E[e_2^2] = E[e_1^2] - h_{32}h_{22}^{-1}h_{32} $$
VIII.  Reconhecendo que $E[e_1^2] = d_{33}$ e que $h_{23} = h_{32}$, temos:
$$E[Y_3 - P(Y_3|Y_2, Y_1)]^2 = E[Y_3 - P(Y_3|Y_1)]^2 - h_{32}h_{22}^{-1}h_{23}$$
Assim, o erro de projeÃ§Ã£o de Y3 com base em Y1 e Y2 Ã© igual ao erro de projeÃ§Ã£o usando apenas Y1 menos o componente nÃ£o antecipado de Y2 multiplicado por um fator de atualizaÃ§Ã£o. â– 

Este teorema formaliza como o erro de previsÃ£o Ã© reduzido ao adicionar novas informaÃ§Ãµes, e como a fatoraÃ§Ã£o triangular nos permite calcular esses ajustes de forma eficiente.

#### O Papel do Fator de AtualizaÃ§Ã£o no Erro de PrevisÃ£o

O fator de atualizaÃ§Ã£o $h_{32}h_{22}^{-1}$ desempenha um papel fundamental tanto na atualizaÃ§Ã£o da projeÃ§Ã£o linear quanto na reduÃ§Ã£o do erro de previsÃ£o. Como discutido anteriormente, $h_{32}$ representa a covariÃ¢ncia entre os resÃ­duos de $Y_3$ e $Y_2$ apÃ³s a projeÃ§Ã£o em $Y_1$, e $h_{22}$ representa a variÃ¢ncia do resÃ­duo de $Y_2$ apÃ³s a projeÃ§Ã£o em $Y_1$. Assim, o fator de atualizaÃ§Ã£o modula o efeito da nova informaÃ§Ã£o (o componente nÃ£o antecipado de $Y_2$) sobre a projeÃ§Ã£o de $Y_3$.

> ğŸ’¡ **Exemplo NumÃ©rico:**
> No exemplo anterior, o fator de atualizaÃ§Ã£o $h_{32}h_{22}^{-1} = 0.375 / 1.75 = 0.214$ ajusta a projeÃ§Ã£o inicial de $Y_3$ com base em $Y_1$.  O erro inicial foi 4.929, mas ao incluir $Y_2$ o erro Ã© reduzido em  $h_{32}h_{22}^{-1}h_{23} = 0.214 * 0.375 = 0.08$, e assim o novo erro de projeÃ§Ã£o passa a ser $4.929 - 0.08 = 4.849$
>
> No caso em que  $h_{32} = 0$ (como visto em um exemplo anterior), o fator de atualizaÃ§Ã£o seria 0, e a projeÃ§Ã£o de $Y_3$ nÃ£o Ã© atualizada com a informaÃ§Ã£o de $Y_2$. AlÃ©m disso, o erro de projeÃ§Ã£o nÃ£o diminui, visto que o termo de correÃ§Ã£o do erro Ã© 0.

**CorolÃ¡rio 3.1:** *O erro de previsÃ£o ao usar $Y_1$ e $Y_2$ Ã© sempre menor ou igual ao erro de previsÃ£o usando apenas $Y_1$.*
*DemonstraÃ§Ã£o:*
I. Do Teorema 3, o erro de previsÃ£o ao usar $Y_1$ e $Y_2$ Ã©
$$E[Y_3 - P(Y_3|Y_2,Y_1)]^2 = E[Y_3 - P(Y_3|Y_1)]^2 - h_{32}h_{22}^{-1} h_{23}$$
II. O termo $h_{32}h_{22}^{-1} h_{23}$ Ã© nÃ£o negativo.
III. Visto que $h_{23} = h_{32}$ entÃ£o $h_{32}h_{22}^{-1} h_{23} = \frac{h_{32}^2}{h_{22}}$
IV. Sabemos que $h_{22} = E[Y_2 - P(Y_2|Y_1)]^2$ que Ã© uma variÃ¢ncia e por isso Ã© nÃ£o negativa.
V. Logo $\frac{h_{32}^2}{h_{22}} \geq 0$
VI. Portanto, $E[Y_3 - P(Y_3|Y_2,Y_1)]^2 \leq E[Y_3 - P(Y_3|Y_1)]^2$. â– 
> ğŸ’¡ **Exemplo NumÃ©rico:** No exemplo anterior, o erro de previsÃ£o usando apenas $Y_1$ era 4.929, e ao incluir $Y_2$, o erro de previsÃ£o diminui para 4.849, ilustrando o CorolÃ¡rio 3.1.
>
> Note que caso $h_{32}=0$, o termo de correÃ§Ã£o Ã© 0 e o erro permanece igual, ou seja, em geral, a adiÃ§Ã£o de uma variÃ¡vel ao modelo pode reduzir, mas nunca aumentar, o erro de previsÃ£o.

**ProposiÃ§Ã£o 1:** *A decomposiÃ§Ã£o do erro de previsÃ£o pode ser generalizada para um nÃºmero arbitrÃ¡rio de variÃ¡veis.*

*DemonstraÃ§Ã£o:*
I. Considere o caso geral onde desejamos projetar $Y_n$ dado $Y_1, Y_2, ..., Y_{n-1}$.
II. Podemos aplicar o mesmo raciocÃ­nio iterativamente. ComeÃ§amos projetando $Y_n$ em $Y_1$, obtendo o erro inicial $e_1 = Y_n - P(Y_n|Y_1)$.
III. Em seguida, projetamos $Y_n$ em $Y_1$ e $Y_2$. O erro Ã© dado por $e_2 = Y_n - P(Y_n|Y_1, Y_2) = e_1 - h_{n2}h_{22}^{-1}[Y_2 - P(Y_2|Y_1)]$.
IV. Prosseguindo, projetamos $Y_n$ em $Y_1, Y_2, Y_3$. O erro serÃ¡ $e_3 = e_2 - h_{n3}h_{33}^{-1}[Y_3 - P(Y_3|Y_1, Y_2)]$.
V. Iterando atÃ© projetar $Y_n$ em $Y_1, Y_2, ..., Y_{n-1}$, o erro final serÃ¡ da forma:
   $$e_{n-1} = Y_n - P(Y_n|Y_1, ..., Y_{n-1}) = e_{n-2} - h_{n,n-1}h_{n-1,n-1}^{-1}[Y_{n-1} - P(Y_{n-1}|Y_1, ..., Y_{n-2})]$$
VI. A variÃ¢ncia do erro de previsÃ£o Ã© entÃ£o
  $$E[e_{n-1}^2] = E[e_1^2] - \sum_{i=2}^{n-1} h_{ni} h_{ii}^{-1} h_{in}$$
VII.  Portanto, o erro de previsÃ£o ao usar $Y_1$ atÃ© $Y_{n-1}$ pode ser expresso recursivamente como o erro usando $Y_1$ atÃ© $Y_{n-2}$ menos um termo de correÃ§Ã£o que envolve o fator de atualizaÃ§Ã£o e o componente nÃ£o antecipado de $Y_{n-1}$.
VIII. O erro de previsÃ£o Ã© decomposto em termos das variÃ¢ncias dos resÃ­duos em cada passo da projeÃ§Ã£o. â– 

> ğŸ’¡ **Exemplo NumÃ©rico:** Vamos considerar a matriz de covariÃ¢ncia $\Omega$ abaixo e calcular os erros de previsÃ£o para diferentes conjuntos de variÃ¡veis.
> $$\Omega = \begin{bmatrix}
> 4 & 1 & 2 & 0.5\\
> 1 & 2 & 0.5 & 0.8\\
> 2 & 0.5 & 5 & 1.2\\
> 0.5 & 0.8 & 1.2 & 3
> \end{bmatrix}$$
>
> Primeiro, vamos calcular o erro ao projetar $Y_4$ sobre $Y_1$.  A projeÃ§Ã£o Ã© $P(Y_4|Y_1) = \frac{0.5}{4}Y_1 = 0.125Y_1$.  Assumindo que $d_{44} = 2.9375$ apÃ³s a fatoraÃ§Ã£o triangular (o cÃ¡lculo detalhado estÃ¡ fora do escopo deste exemplo, mas segue a mesma lÃ³gica usada no primeiro exemplo numÃ©rico), temos o erro de previsÃ£o $E[Y_4 - P(Y_4|Y_1)]^2 = 2.9375$.
>
> Agora, vamos calcular o erro ao projetar $Y_4$ sobre $Y_1$ e $Y_2$. Precisamos de $h_{42}$, $h_{22}$. $h_{22}$ Ã© a variÃ¢ncia do resÃ­duo de $Y_2$ quando projetado sobre $Y_1$ e, como visto no exemplo anterior, Ã© $1.75$.
> Precisamos tambÃ©m calcular $h_{42}$ que Ã© o componente da matriz de decomposiÃ§Ã£o triangular associado aos resÃ­duos de $Y_4$ e $Y_2$.  Assumiremos que $h_{42}=0.725$ apÃ³s a decomposiÃ§Ã£o triangular de $\Omega$.
> O erro de projeÃ§Ã£o Ã© entÃ£o $E[Y_4 - P(Y_4|Y_1, Y_2)]^2 = 2.9375 - (0.725^2)/1.75 = 2.9375 - 0.2997 = 2.6378$.
>
> Agora, vamos calcular o erro ao projetar $Y_4$ sobre $Y_1$, $Y_2$ e $Y_3$.  Precisamos de $h_{43}$ e $h_{33}$. Assumindo que $h_{33} = 4.849$ e $h_{43} = 0.987$ apÃ³s a decomposiÃ§Ã£o triangular de $\Omega$, o erro de projeÃ§Ã£o Ã© $E[Y_4 - P(Y_4|Y_1, Y_2, Y_3)]^2 = 2.6378 - (0.987^2)/4.849 = 2.6378 - 0.2007 = 2.437$.
>
> Assim, conforme adicionamos variÃ¡veis, o erro de previsÃ£o decresce:
> | VariÃ¡veis Usadas | Erro de PrevisÃ£o |
> |-----------------|-------------------|
> | $Y_1$            | 2.9375            |
> | $Y_1, Y_2$      | 2.6378            |
> | $Y_1, Y_2, Y_3$  | 2.437             |

**ObservaÃ§Ã£o 1:** A proposiÃ§Ã£o acima demonstra que a lÃ³gica de decomposiÃ§Ã£o do erro de previsÃ£o se estende naturalmente a um nÃºmero maior de variÃ¡veis. A cada nova variÃ¡vel incluÃ­da, o erro de previsÃ£o Ã© reduzido (ou mantido igual) devido Ã  adiÃ§Ã£o de um termo nÃ£o negativo. Esta observaÃ§Ã£o Ã© uma extensÃ£o natural do CorolÃ¡rio 3.1 e do Teorema 3.

**Lema 1:** *Se $h_{ij} = 0$ para algum $i$, onde $j<i$, entÃ£o a inclusÃ£o da variÃ¡vel $Y_j$ na projeÃ§Ã£o de $Y_i$ nÃ£o altera o erro de previsÃ£o de $Y_i$ projetado sobre $Y_1, \dots, Y_{j-1}$.*

*DemonstraÃ§Ã£o:*
I.  O erro de previsÃ£o de $Y_i$ projetado sobre $Y_1, \dots, Y_{j-1}$ Ã© $E[Y_i - P(Y_i|Y_1, ..., Y_{j-1})]^2$.
II.  O erro de previsÃ£o de $Y_i$ projetado sobre $Y_1, \dots, Y_j$ Ã© $E[Y_i - P(Y_i|Y_1, ..., Y_{j})]^2$.
III.  Pelo Teorema 3 e pela ProposiÃ§Ã£o 1, sabemos que:
    $$E[Y_i - P(Y_i|Y_1, ..., Y_{j})]^2 = E[Y_i - P(Y_i|Y_1, ..., Y_{j-1})]^2 - h_{ij} h_{jj}^{-1} h_{ji}$$
IV. Se $h_{ij} = 0$, entÃ£o  $h_{ij} h_{jj}^{-1} h_{ji} = 0$
V. Assim,  $E[Y_i - P(Y_i|Y_1, ..., Y_{j})]^2 = E[Y_i - P(Y_i|Y_1, ..., Y_{j-1})]^2$.
VI.  Portanto, quando $h_{ij} = 0$, a inclusÃ£o de $Y_j$ na projeÃ§Ã£o de $Y_i$ nÃ£o altera o erro de previsÃ£o. â– 

> ğŸ’¡ **Exemplo NumÃ©rico:** Se no exemplo dado, $h_{32}=0$, entÃ£o o erro de previsÃ£o de $Y_3$ quando projetado sobre $Y_1$ e $Y_2$ seria o mesmo erro de previsÃ£o de $Y_3$ projetado apenas sobre $Y_1$.
>
> Para ilustrar isto, vamos modificar a matriz $\Omega$ do primeiro exemplo de forma que $h_{32} = 0$, tal que a matriz seja
> $$\Omega = \begin{bmatrix}
> 4 & 1 & 2 \\
> 1 & 2 & 0 \\
> 2 & 0 & 5
> \end{bmatrix}$$
>
> Nesse caso, $Y_2$ Ã© ortogonal ao resÃ­duo da projeÃ§Ã£o de $Y_3$ em $Y_1$. A projeÃ§Ã£o de $Y_3$ sobre $Y_1$ Ã© $P(Y_3|Y_1) = 0.5Y_1$ e o erro de previsÃ£o seria $d_{33}$, que nesse caso (apÃ³s a decomposiÃ§Ã£o triangular) Ã© igual a $4.900$. Ao incluir $Y_2$, a projeÃ§Ã£o seria $P(Y_3|Y_2, Y_1) = 0.5Y_1 + 0[Y_2 - 0.25Y_1]$. Note que o fator de atualizaÃ§Ã£o Ã© zero. O erro de previsÃ£o passa a ser $4.900 - 0 = 4.900$, ou seja, nÃ£o hÃ¡ mudanÃ§a no erro, confirmando o lema.

### ConclusÃ£o

Este capÃ­tulo demonstrou como o erro de previsÃ£o Ã© decomposto quando novas informaÃ§Ãµes sÃ£o usadas para atualizar as projeÃ§Ãµes lineares. Vimos que o erro de previsÃ£o ao usar $Y_1$ e $Y_2$ pode ser expresso em termos do erro da projeÃ§Ã£o em $Y_1$ e um termo corretivo, que por sua vez envolve o fator de atualizaÃ§Ã£o ($h_{32}h_{22}^{-1}$) e o componente nÃ£o antecipado de $Y_2$. O teorema 3 e o seu corolÃ¡rio mostram que a adiÃ§Ã£o de novas variÃ¡veis reduz ou mantÃ©m o erro da previsÃ£o. A fatoraÃ§Ã£o triangular da matriz de momentos emerge como uma ferramenta essencial para calcular esses ajustes de maneira eficiente, facilitando a atualizaÃ§Ã£o contÃ­nua de projeÃ§Ãµes em situaÃ§Ãµes onde novas informaÃ§Ãµes estÃ£o continuamente a surgir. Essa compreensÃ£o mais profunda do erro de previsÃ£o forma um alicerce essencial para a anÃ¡lise de sÃ©ries temporais.

### ReferÃªncias
[^4.1.10]: *SeÃ§Ã£o 4.1, pÃ¡gina 73*
[^4.4.1]: *SeÃ§Ã£o 4.4, pÃ¡gina 87*
[^4.5.2]: *SeÃ§Ã£o 4.5, pÃ¡gina 92*
[^4.5.4]: *SeÃ§Ã£o 4.5, pÃ¡gina 92*
[^4.5.6]: *SeÃ§Ã£o 4.5, pÃ¡gina 92*
[^4.5.11]: *SeÃ§Ã£o 4.5, pÃ¡gina 93*
[^4.5.12]: *SeÃ§Ã£o 4.5, pÃ¡gina 93*
[^4.5.13]: *SeÃ§Ã£o 4.5, pÃ¡gina 93*
[^4.5.14]: *SeÃ§Ã£o 4.5, pÃ¡gina 94*
<!-- END -->
