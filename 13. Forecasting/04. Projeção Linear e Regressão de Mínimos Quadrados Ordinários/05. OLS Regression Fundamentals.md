## Proje√ß√£o Linear, Regress√£o OLS e Suposi√ß√µes de Estacionariedade e Ergodicidade

### Introdu√ß√£o

Este cap√≠tulo explora o papel fundamental da **estacionariedade** e **ergodicidade** na aplica√ß√£o da **regress√£o de m√≠nimos quadrados ordin√°rios (OLS)** como base para previs√£o, especialmente em compara√ß√£o com a **econometria estrutural**. Como visto anteriormente [^4, 4.1.20], a **proje√ß√£o linear** utiliza momentos populacionais para modelar rela√ß√µes entre vari√°veis aleat√≥rias, enquanto a **regress√£o OLS** estima essas rela√ß√µes com base em dados amostrais [^4, 4.1.17]. A liga√ß√£o entre ambas se d√° atrav√©s da converg√™ncia dos momentos amostrais para os momentos populacionais sob certas condi√ß√µes, o que permite que o estimador OLS seja uma aproxima√ß√£o consistente para a proje√ß√£o linear, dado que o processo de dados seja estacion√°rio e erg√≥dico para os segundos momentos. Este cap√≠tulo aprofunda a discuss√£o, contrastando a abordagem da **proje√ß√£o linear** e **regress√£o OLS** com a **econometria estrutural**, que exige suposi√ß√µes mais fortes sobre rela√ß√µes causais.

### Suposi√ß√µes de Estacionariedade e Ergodicidade

A **estacionariedade** e a **ergodicidade** s√£o suposi√ß√µes cruciais para a validade da regress√£o OLS como base para previs√£o. Um processo **estacion√°rio** √© caracterizado por suas propriedades estat√≠sticas (como m√©dia, vari√¢ncia e autocovari√¢ncia) n√£o se alterarem ao longo do tempo. Ou seja, se observarmos um peda√ßo de um processo estacion√°rio hoje, amanh√£ ou daqui a um ano, suas propriedades ser√£o as mesmas. Formalmente, um processo $\{Y_t\}$ √© **estritamente estacion√°rio** se a distribui√ß√£o conjunta de $(Y_{t_1}, Y_{t_2}, ..., Y_{t_n})$ √© igual √† distribui√ß√£o de $(Y_{t_1 + k}, Y_{t_2 + k}, ..., Y_{t_n + k})$ para qualquer $t_1, t_2, ..., t_n$, $k$ e $n$. Um processo √© **fracamente estacion√°rio** (ou estacion√°rio em segunda ordem) se sua m√©dia $E[Y_t]$ √© constante e sua autocovari√¢ncia $Cov[Y_t, Y_{t-j}]$ depende apenas de $j$.

A **ergodicidade**, por sua vez, garante que as m√©dias amostrais convergem para as m√©dias populacionais √† medida que o tamanho da amostra aumenta. Ou seja, a ergodicidade permite que o pesquisador use dados amostrais para obter informa√ß√µes sobre a distribui√ß√£o populacional. Formalmente, um processo $\{Y_t\}$ √© **erg√≥dico** se a m√©dia amostral $\bar{Y}_T = \frac{1}{T} \sum_{t=1}^T Y_t$ converge em probabilidade para a m√©dia populacional $E[Y_t]$ quando $T \to \infty$. Da mesma forma, um processo √© **erg√≥dico para segundos momentos** se os momentos amostrais de segunda ordem convergem em probabilidade para seus correspondentes momentos populacionais.

> üí° **Exemplo Pr√°tico (Estacionariedade):** Considere um processo como o lan√ßamento de uma moeda honesta. Em cada lan√ßamento, o resultado (cara ou coroa) √© uma vari√°vel aleat√≥ria. A probabilidade de cara √© sempre 0,5, independentemente do lan√ßamento. Esse processo √© estacion√°rio. Agora considere um processo como o crescimento da popula√ß√£o de uma cidade, o qual n√£o √© estacion√°rio, pois a sua m√©dia (tamanho da popula√ß√£o) se altera com o passar do tempo.

> üí° **Exemplo Pr√°tico (Ergodicidade):** Imagine que voc√™ tem um saco com bolas coloridas e quer determinar a propor√ß√£o de bolas vermelhas. Se voc√™ retirar um n√∫mero grande de bolas (com reposi√ß√£o), a propor√ß√£o de bolas vermelhas na sua amostra ir√° se aproximar da propor√ß√£o de bolas vermelhas no saco. Este √© um exemplo de ergodicidade. Se, no entanto, o saco fosse din√¢mico, com a composi√ß√£o alterada ao longo do tempo, a m√©dia amostral n√£o convergiria para a m√©dia populacional, e ter√≠amos um processo n√£o erg√≥dico.

Em termos de regress√£o OLS, a suposi√ß√£o de **estacionariedade** garante que os par√¢metros populacionais, como a rela√ß√£o linear entre as vari√°veis explicativas e a vari√°vel dependente, sejam constantes ao longo do tempo. A suposi√ß√£o de **ergodicidade**, por sua vez, garante que os momentos amostrais utilizados no c√°lculo do estimador OLS convergem para seus correspondentes momentos populacionais quando o tamanho da amostra tende ao infinito.

Como vimos anteriormente, as m√©dias amostrais e as somas de produtos que aparecem nos c√°lculos da regress√£o OLS convergem em probabilidade para as suas contrapartes populacionais quando o processo de dados √© erg√≥dico. Isto √©:

**Lema 1** (Converg√™ncia das M√©dias Amostrais)  Dado um processo estacion√°rio e erg√≥dico $\{X_t\}$, a m√©dia amostral $\frac{1}{T} \sum_{t=1}^T X_t$ converge em probabilidade para a m√©dia populacional $E[X_t]$ quando $T$ tende ao infinito. Ou seja,
$\frac{1}{T} \sum_{t=1}^T X_t \xrightarrow{p} E[X_t]$.

**Lema 1.1** (Converg√™ncia dos Momentos Amostrais)
Dado um processo estacion√°rio e erg√≥dico $\{X_t\}$, o momento amostral $\frac{1}{T} \sum_{t=1}^T X_t X_t'$ converge em probabilidade para o momento populacional $E[X_t X_t']$ quando $T$ tende ao infinito. Ou seja,
$\frac{1}{T} \sum_{t=1}^T X_t X_t' \xrightarrow{p} E[X_t X_t']$.

Com base nesses lemas e na continuidade da opera√ß√£o de invers√£o de matrizes, √© poss√≠vel mostrar a consist√™ncia do estimador OLS:

**Proposi√ß√£o 1** (Consist√™ncia do Estimador OLS) Se o processo $(X_t, Y_{t+1})$ √© estacion√°rio e erg√≥dico para os segundos momentos, e se a matriz $E(X_tX_t')$ √© n√£o-singular, ent√£o o estimador OLS $b = \left(\sum_{t=1}^T x_t x_t'\right)^{-1} \left(\sum_{t=1}^T x_t y_{t+1}\right)$ converge em probabilidade para o coeficiente da proje√ß√£o linear $\alpha = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}$ quando $T$ tende ao infinito.
*Prova*:
I. Seja $(X_t, Y_{t+1})$ um processo estacion√°rio e erg√≥dico para os segundos momentos.
II. Pelo Lema 1 (Converg√™ncia das M√©dias Amostrais) e o Lema 1.1 (Converg√™ncia dos Momentos Amostrais) e a propriedade da ergodicidade, temos que:
$\frac{1}{T} \sum_{t=1}^T x_t x_t' \xrightarrow{p} E(X_t X_t')$ quando $T \to \infty$, e
$\frac{1}{T} \sum_{t=1}^T x_t y_{t+1} \xrightarrow{p} E(Y_{t+1} X_t')$ quando $T \to \infty$.
III. Podemos reescrever o estimador OLS $b$ como: $b = \left(\frac{1}{T}\sum_{t=1}^T x_t x_t'\right)^{-1} \left(\frac{1}{T}\sum_{t=1}^T x_t y_{t+1}\right)$.
IV. Dado que $E(X_t X_t')$ √© n√£o-singular (invert√≠vel), sua inversa existe, e pela continuidade da opera√ß√£o de invers√£o:
$\left(\frac{1}{T} \sum_{t=1}^T x_t x_t'\right)^{-1} \xrightarrow{p} \left(E(X_t X_t')\right)^{-1}$ quando $T \to \infty$.
V. Pela propriedade de converg√™ncia de produtos em probabilidade, se $A_T \xrightarrow{p} A$ e $B_T \xrightarrow{p} B$, ent√£o $A_T B_T \xrightarrow{p} AB$. Aplicando essa propriedade, temos que:
$b \xrightarrow{p} (E(X_t X_t'))^{-1} E(Y_{t+1} X_t') = \alpha$ quando $T \to \infty$.
VI. Portanto, o estimador OLS $b$ converge em probabilidade para o coeficiente da proje√ß√£o linear $\alpha$ quando $T \to \infty$. $\blacksquare$

**Lema 1.2** (Converg√™ncia dos Momentos Amostrais Centrados)
Dado um processo estacion√°rio e erg√≥dico $\{X_t\}$ com m√©dia $E[X_t] = \mu_X$, o momento amostral centrado $\frac{1}{T} \sum_{t=1}^T (X_t - \mu_X)(X_t - \mu_X)'$ converge em probabilidade para o momento populacional centrado $E[(X_t - \mu_X)(X_t - \mu_X)'] = Cov[X_t, X_t]$ quando $T$ tende ao infinito. Ou seja,
$\frac{1}{T} \sum_{t=1}^T (X_t - \mu_X)(X_t - \mu_X)' \xrightarrow{p} Cov[X_t, X_t]$.
*Prova*:
I. Seja $\{X_t\}$ um processo estacion√°rio e erg√≥dico com m√©dia $E[X_t] = \mu_X$.
II. Defina $Z_t = X_t - \mu_X$.
III. O momento amostral centrado pode ser reescrito como $\frac{1}{T}\sum_{t=1}^T Z_t Z_t'$.
IV. Pelo Lema 1.1 (Converg√™ncia dos Momentos Amostrais), temos que $\frac{1}{T}\sum_{t=1}^T Z_t Z_t' \xrightarrow{p} E[Z_t Z_t']$.
V. Como $E[Z_t Z_t'] = E[(X_t - \mu_X)(X_t - \mu_X)'] = Cov[X_t, X_t]$, segue que $\frac{1}{T}\sum_{t=1}^T (X_t - \mu_X)(X_t - \mu_X)' \xrightarrow{p} Cov[X_t, X_t]$. $\blacksquare$

> üí° **Exemplo Num√©rico (Simula√ß√£o):** Para ilustrar a import√¢ncia da estacionaridade e ergodicidade, vamos simular um processo n√£o estacion√°rio e n√£o erg√≥dico. Seja $Y_{t+1} = \beta X_t + \epsilon_t$, onde $X_t$ √© um processo com uma tend√™ncia crescente, e $\epsilon_t$ √© ru√≠do branco. Este processo n√£o √© estacion√°rio porque a m√©dia e a vari√¢ncia de $X_t$ mudam com o tempo. Em um cen√°rio erg√≥dico, a m√©dia amostral convergiria para a m√©dia populacional, mas nesse caso, como o processo n√£o √© erg√≥dico, a m√©dia amostral n√£o representar√° a m√©dia da popula√ß√£o e a estimativa da regress√£o OLS ser√° inconsistente.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def generate_non_stationary_data(T, true_beta=2, std_dev=5):
>    X = np.arange(1, T+1) + np.random.normal(0, 10, T) #Non-stationary process for X
>    epsilon = np.random.normal(0, std_dev, T)
>    Y = true_beta * X + epsilon
>    return X.reshape(-1,1), Y.reshape(-1,1)
>
> def estimate_ols(X, Y):
>    X_transpose = X.T
>    XTX = np.dot(X_transpose, X)
>    XTY = np.dot(X_transpose, Y)
>    XTX_inv = np.linalg.inv(XTX)
>    beta = np.dot(XTX_inv, XTY)
>    return beta
>
> sample_sizes = [100, 500, 1000]
> estimated_betas = []
>
> for T in sample_sizes:
>   X, Y = generate_non_stationary_data(T)
>   beta_hat = estimate_ols(X, Y)
>   estimated_betas.append(beta_hat[0][0])
>
> plt.figure(figsize=(8,6))
> plt.plot(sample_sizes, estimated_betas, marker='o')
> plt.axhline(y=2, color='r', linestyle='--', label='Beta Verdadeiro')
> plt.xlabel('Tamanho da Amostra (T)')
> plt.ylabel('Estimativa do Beta')
> plt.title('Converg√™ncia do Estimador OLS em Dados N√£o Estacion√°rios')
> plt.legend()
> plt.grid(True)
> plt.show()
>
> print(f"Estimativa para T=100: {estimated_betas[0]:.2f}")
> print(f"Estimativa para T=500: {estimated_betas[1]:.2f}")
> print(f"Estimativa para T=1000: {estimated_betas[2]:.2f}")
> ```
> Este exemplo ilustra que, na presen√ßa de n√£o-estacionariedade, o estimador OLS n√£o converge para o valor verdadeiro do par√¢metro, mesmo com o aumento do tamanho da amostra. Isso demonstra que a estacionaridade e a ergodicidade s√£o condi√ß√µes importantes para que o OLS seja uma estimativa consistente da proje√ß√£o linear e para predi√ß√£o, e que a sua viola√ß√£o leva a resultados pouco confi√°veis.

> üí° **Exemplo Num√©rico (Regress√£o OLS):** Suponha que temos dados sobre o gasto com publicidade ($X_t$) e as vendas ($Y_{t+1}$) de uma empresa, com $T=100$ observa√ß√µes. Assumimos que o processo $(X_t, Y_{t+1})$ √© estacion√°rio e erg√≥dico para os segundos momentos. Os dados s√£o gerados com $Y_{t+1} = 0.5 X_t + \epsilon_t$, onde $\epsilon_t \sim N(0, 10)$. Para ilustrar a converg√™ncia do estimador OLS, vamos comparar a estimativa com o verdadeiro valor do par√¢metro $\beta=0.5$:
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> np.random.seed(42)
>
> def generate_data(T, true_beta=0.5, std_dev=10):
>    X = np.random.uniform(10, 100, T)
>    epsilon = np.random.normal(0, std_dev, T)
>    Y = true_beta * X + epsilon
>    return X.reshape(-1,1), Y.reshape(-1,1)
>
> def estimate_ols(X, Y):
>    X_transpose = X.T
>    XTX = np.dot(X_transpose, X)
>    XTY = np.dot(X_transpose, Y)
>    XTX_inv = np.linalg.inv(XTX)
>    beta = np.dot(XTX_inv, XTY)
>    return beta
>
> T = 100
> X, Y = generate_data(T)
> beta_hat = estimate_ols(X, Y)
> print(f"Estimativa do beta (OLS): {beta_hat[0][0]:.4f}")
>
> # Plotting the data and the regression line
> plt.figure(figsize=(8,6))
> plt.scatter(X, Y, label='Dados Observados')
> x_range = np.linspace(X.min(), X.max(), 100)
> y_predicted = beta_hat[0][0] * x_range
> plt.plot(x_range, y_predicted, color='red', label='Linha de Regress√£o OLS')
> plt.xlabel('Gasto com Publicidade (X)')
> plt.ylabel('Vendas (Y)')
> plt.title('Regress√£o OLS: Vendas vs. Gasto com Publicidade')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
> Neste exemplo, a estimativa do par√¢metro $\beta$ obtida pela regress√£o OLS √© pr√≥xima do valor verdadeiro (0.5), demonstrando a consist√™ncia do estimador OLS sob as suposi√ß√µes de estacionariedade e ergodicidade. O gr√°fico ilustra os dados e a linha de regress√£o ajustada.

### Regress√£o OLS vs. Econometria Estrutural

A **regress√£o OLS**, quando utilizada como ferramenta de previs√£o, se baseia nas propriedades estat√≠sticas do processo gerador de dados, principalmente nas suposi√ß√µes de **estacionariedade** e **ergodicidade**. A econometria estrutural, por outro lado, procura modelar rela√ß√µes causais entre as vari√°veis, usando teorias econ√¥micas como guia. A **econometria estrutural** busca identificar e quantificar o efeito causal de uma vari√°vel sobre outra.

> üí° **Exemplo:** Considere que queremos estudar a rela√ß√£o entre pol√≠ticas fiscais ($X_t$) e crescimento econ√¥mico ($Y_{t+1}$). A regress√£o OLS busca simplesmente encontrar o melhor ajuste linear entre essas duas vari√°veis, dado que o processo √© estacion√°rio e erg√≥dico. A econometria estrutural, por outro lado, tenta especificar um modelo que explique *como* as pol√≠ticas fiscais afetam o crescimento econ√¥mico, usando teorias econ√¥micas como base e buscando identificar rela√ß√µes de causa e efeito. O modelo estrutural, por exemplo, pode levar em considera√ß√£o diversas outras vari√°veis que influenciam ambas, e determinar que parte da rela√ß√£o entre elas √© causal.

A **principal diferen√ßa** entre as duas abordagens reside nas suposi√ß√µes: a econometria estrutural exige suposi√ß√µes mais fortes sobre a natureza da rela√ß√£o entre as vari√°veis, a ordem de causalidade, e a aus√™ncia de vari√°veis omitidas que possam confundir a rela√ß√£o de interesse. A regress√£o OLS, por sua vez, requer apenas as suposi√ß√µes de estacionariedade e ergodicidade, e que a rela√ß√£o entre as vari√°veis seja linear (ou pelo menos aproximada por um modelo linear), para fornecer uma estimativa consistente para o coeficiente da proje√ß√£o linear. A **econometria estrutural** est√° interessada em descobrir o *porqu√™* e o *como*, enquanto a regress√£o OLS est√° interessada no *quanto*, quando o foco √© predi√ß√£o [^4, 4.1.20]. Ou seja, a **econometria estrutural**  tem uma preocupa√ß√£o com a modelagem causal, enquanto a regress√£o OLS tem uma preocupa√ß√£o com a modelagem preditiva.

**Teorema 2** (Teorema de Representa√ß√£o de Wold) Qualquer processo fracamente estacion√°rio $\{Y_t\}$ pode ser representado como uma m√©dia m√≥vel infinita de choques n√£o correlacionados, ou seja, $Y_t = \mu + \sum_{j=0}^{\infty} \psi_j \epsilon_{t-j}$, onde $\mu = E[Y_t]$, $\{\epsilon_t\}$ √© um processo de ru√≠do branco com $E[\epsilon_t] = 0$ e $Var[\epsilon_t] = \sigma^2_{\epsilon}$, e os coeficientes $\psi_j$ satisfazem $\sum_{j=0}^{\infty} |\psi_j| < \infty$. Este teorema √© central para modelos de s√©ries temporais e fundamenta a ideia de que a din√¢mica de um processo estacion√°rio pode ser expressa por choques passados.

Para prever, a regress√£o OLS se torna vantajosa devido √† suavidade das suas suposi√ß√µes. Se o objetivo √© somente previs√£o, e n√£o se pretende estabelecer uma rela√ß√£o causal entre as vari√°veis, a regress√£o OLS √© uma ferramenta mais robusta e parcimoniosa. Por outro lado, se o objetivo √© entender o mecanismo subjacente e estabelecer rela√ß√µes causais, a abordagem da econometria estrutural √© mais apropriada.

Em termos pr√°ticos, o uso da regress√£o OLS √© justificado quando nosso principal objetivo √© a previs√£o, e n√£o a identifica√ß√£o de rela√ß√µes causais. No entanto, √© crucial que a suposi√ß√£o de estacionariedade e ergodicidade seja razo√°vel para que os resultados sejam confi√°veis. Quando essas suposi√ß√µes n√£o se sustentam, alternativas, como o uso de m√©todos de diferencia√ß√£o ou modelos de mudan√ßa no tempo (time-varying), podem ser consideradas.

### Conclus√£o

A regress√£o OLS, sob suposi√ß√µes suaves de **estacionariedade** e **ergodicidade** para segundos momentos, fornece uma base s√≥lida para previs√£o, com o estimador OLS convergindo para o coeficiente da proje√ß√£o linear populacional. Essa abordagem contrasta com a **econometria estrutural**, que exige suposi√ß√µes mais fortes sobre a rela√ß√£o causal entre as vari√°veis. A escolha entre os m√©todos depender√° do objetivo da an√°lise: se o foco √© previs√£o, a regress√£o OLS √© mais adequada devido √†s suas suposi√ß√µes mais brandas; se o foco √© a identifica√ß√£o de rela√ß√µes causais e compreens√£o do mecanismo subjacente, a econometria estrutural √© mais apropriada. Em ambas as abordagens, entretanto, os pressupostos devem ser adequadamente testados e validados para que os resultados sejam confi√°veis e relevantes.

### Refer√™ncias

[^1]:
[^2]:
[^3]: *Trechos do texto onde o conceito √© discutido ou mencionado*
[^4]: *Trechos do texto onde o conceito √© discutido ou mencionado*
<!-- END -->
