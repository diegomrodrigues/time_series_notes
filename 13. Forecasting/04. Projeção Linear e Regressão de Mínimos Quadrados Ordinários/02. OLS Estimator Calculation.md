## Proje√ß√£o Linear e Regress√£o de M√≠nimos Quadrados Ordin√°rios: C√°lculo do Estimador OLS e Singularidade da Matriz

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre **proje√ß√µes lineares** e sua rela√ß√£o com a **regress√£o de m√≠nimos quadrados ordin√°rios (OLS)**, este cap√≠tulo detalha a f√≥rmula para o c√°lculo do estimador OLS, abordando os desafios impostos pela singularidade da matriz de covari√¢ncia. Como explorado anteriormente, a **proje√ß√£o linear** busca encontrar a melhor aproxima√ß√£o linear de uma vari√°vel aleat√≥ria em termos de outras vari√°veis, e a **regress√£o OLS** emerge como uma ferramenta para estimar essa rela√ß√£o linear em amostras finitas [^4]. O presente t√≥pico aprofunda o aspecto computacional, destacando como obter o estimador OLS e lidar com poss√≠veis problemas de **singularidade da matriz**.

### Conceitos Fundamentais
Como vimos anteriormente [^2], a proje√ß√£o linear de $Y_{t+1}$ sobre $X_t$ √© dada por $Y_{t+1|t} = \alpha'X_t$, onde $\alpha$ √© um vetor de coeficientes. A condi√ß√£o de ortogonalidade do erro de previs√£o com as vari√°veis explicativas, $E[(Y_{t+1} - \alpha'X_t)X_t] = 0$, leva √† solu√ß√£o para $\alpha$ como $\alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}$ [^4, 4.1.13]. Ao introduzir o conceito de **regress√£o OLS**, procuramos encontrar um estimador $\beta$ que minimize a soma dos quadrados dos res√≠duos, ou seja, $\sum_{t=1}^T (y_{t+1} - \beta'x_t)^2$ [^3, 4.1.17]. O estimador $\beta$ que atende a esta condi√ß√£o, denotado por $b$, √© dado por

$$
b = \left(\sum_{t=1}^T x_t x_t'\right)^{-1} \left(\sum_{t=1}^T x_t y_{t+1}\right).
$$

Esta express√£o, como observamos anteriormente [^4, 4.1.18], envolve a **invers√£o da matriz** $\left(\sum_{t=1}^T x_t x_t'\right)$ e sua **multiplica√ß√£o** pelo vetor $\left(\sum_{t=1}^T x_t y_{t+1}\right)$. Em termos computacionais, o c√°lculo de $b$ requer a computa√ß√£o das somas $\sum_{t=1}^T x_t x_t'$ e $\sum_{t=1}^T x_t y_{t+1}$, a **invers√£o da matriz** $\left(\sum_{t=1}^T x_t x_t'\right)$, e finalmente a **multiplica√ß√£o** resultante pelo vetor $\left(\sum_{t=1}^T x_t y_{t+1}\right)$.

> üí° **Exemplo Num√©rico:**  Vamos considerar um exemplo simplificado com $T=3$ e duas vari√°veis explicativas. Suponha que temos os seguintes dados:
> $X = \begin{bmatrix} 1 & 2 \\ 2 & 1 \\ 3 & 3 \end{bmatrix}$ e $Y = \begin{bmatrix} 5 \\ 6 \\ 8 \end{bmatrix}$.
>
> Primeiro, calculamos $X'X$:
> $$
> X'X = \begin{bmatrix} 1 & 2 & 3 \\ 2 & 1 & 3 \end{bmatrix} \begin{bmatrix} 1 & 2 \\ 2 & 1 \\ 3 & 3 \end{bmatrix} = \begin{bmatrix} 14 & 13 \\ 13 & 14 \end{bmatrix}
> $$
>
> Em seguida, calculamos $X'Y$:
> $$
> X'Y = \begin{bmatrix} 1 & 2 & 3 \\ 2 & 1 & 3 \end{bmatrix} \begin{bmatrix} 5 \\ 6 \\ 8 \end{bmatrix} = \begin{bmatrix} 41 \\ 40 \end{bmatrix}
> $$
>
> Agora, precisamos encontrar a inversa de $X'X$. Para uma matriz 2x2, $\begin{bmatrix} a & b \\ c & d \end{bmatrix}^{-1} = \frac{1}{ad-bc}\begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$. Portanto,
>
> $(X'X)^{-1} = \frac{1}{14*14 - 13*13}\begin{bmatrix} 14 & -13 \\ -13 & 14 \end{bmatrix} = \begin{bmatrix} 14 & -13 \\ -13 & 14 \end{bmatrix}$
>
> Finalmente, calculamos o estimador $\beta$ (denotado por $b$):
> $$
> b = (X'X)^{-1} X'Y = \begin{bmatrix} 14 & -13 \\ -13 & 14 \end{bmatrix} \begin{bmatrix} 41 \\ 40 \end{bmatrix} = \begin{bmatrix} 14*41 -13*40 \\ -13*41 + 14*40 \end{bmatrix} = \begin{bmatrix} 574 - 520 \\ -533 + 560 \end{bmatrix} = \begin{bmatrix} 54 \\ 27 \end{bmatrix}
> $$
>  Portanto, os coeficientes estimados s√£o $\beta_1 = 54$ e $\beta_2 = 27$. A equa√ß√£o de regress√£o estimada √© $\hat{y} = 54x_1 + 27x_2$.
>
> ```python
> import numpy as np
>
> X = np.array([[1, 2], [2, 1], [3, 3]])
> Y = np.array([5, 6, 8])
>
> X_transpose = X.T
>
> XTX = np.dot(X_transpose, X)
>
> XTY = np.dot(X_transpose, Y)
>
> XTX_inv = np.linalg.inv(XTX)
>
> beta = np.dot(XTX_inv, XTY)
>
> print("Estimated Beta:", beta)
> ```
>
> Este exemplo ilustra como a f√≥rmula do estimador OLS √© aplicada na pr√°tica, envolvendo c√°lculos de produto de matrizes e invers√£o.

**Lema 1** A matriz $\sum_{t=1}^T x_t x_t'$ √© sim√©trica e semidefinida positiva.
*Prova*: A simetria segue diretamente da defini√ß√£o, pois $(x_t x_t')' = x_t x_t'$. Para provar que √© semidefinida positiva, observe que para qualquer vetor $v$, temos $v'\left(\sum_{t=1}^T x_t x_t'\right)v = \sum_{t=1}^T v'x_t x_t'v = \sum_{t=1}^T (x_t'v)^2 \geq 0$.

I. Seja $A = \sum_{t=1}^T x_t x_t'$.
II. A transposta de $A$ √© dada por $A' = \left(\sum_{t=1}^T x_t x_t'\right)' = \sum_{t=1}^T (x_t x_t')'$.
III. Usando a propriedade da transposta de um produto, $(x_t x_t')' = (x_t')' x_t' = x_t x_t'$.
IV. Assim, $A' = \sum_{t=1}^T x_t x_t' = A$, o que prova que $A$ √© sim√©trica.
V. Seja $v$ um vetor arbitr√°rio. Considere a forma quadr√°tica $v'Av = v'\left(\sum_{t=1}^T x_t x_t'\right)v$.
VI. Pela propriedade distributiva da multiplica√ß√£o por um escalar, temos $v'\left(\sum_{t=1}^T x_t x_t'\right)v = \sum_{t=1}^T v' x_t x_t' v$.
VII. Reorganizando os termos, temos $\sum_{t=1}^T v' x_t x_t' v = \sum_{t=1}^T (x_t' v)' (x_t' v) = \sum_{t=1}^T (x_t' v)^2$.
VIII. Dado que o quadrado de qualquer n√∫mero real √© n√£o negativo, $(x_t' v)^2 \geq 0$.
IX. Consequentemente, a soma de n√∫meros n√£o negativos √© n√£o negativa, ou seja, $\sum_{t=1}^T (x_t' v)^2 \geq 0$.
X. Portanto, $v'Av \geq 0$ para qualquer vetor $v$, o que mostra que $A$ √© semidefinida positiva.‚ñ†

Entretanto, um problema surge quando a matriz $\sum_{t=1}^T x_t x_t'$ √© **singular**. A **singularidade** implica que a matriz n√£o possui inversa, tornando imposs√≠vel calcular o estimador $b$ diretamente atrav√©s da f√≥rmula apresentada [^3]. Geometricamente, a singularidade ocorre quando existe uma **combina√ß√£o linear** entre as colunas da matriz, ou seja, quando as vari√°veis explicativas n√£o s√£o linearmente independentes. Como discutido anteriormente, este problema surge quando uma vari√°vel em $X_t$ √© uma vers√£o reescalonada de outra vari√°vel, tornando parte da informa√ß√£o redundante [^3].

> üí° **Exemplo Num√©rico (Singularidade):** Considere o seguinte conjunto de dados:
>
> $X = \begin{bmatrix} 1 & 2 & 2 \\ 2 & 4 & 4 \\ 3 & 6 & 6 \end{bmatrix}$ e $Y = \begin{bmatrix} 5 \\ 6 \\ 8 \end{bmatrix}$.
>
> Observe que a terceira coluna de $X$ √© simplesmente a segunda coluna multiplicada por 1. Isso significa que existe uma combina√ß√£o linear entre as colunas, e a matriz $X'X$ ser√° singular. Calculando $X'X$ obtemos:
>
> $$
> X'X = \begin{bmatrix} 1 & 2 & 3 \\ 2 & 4 & 6 \\ 2 & 4 & 6 \end{bmatrix} \begin{bmatrix} 1 & 2 & 2 \\ 2 & 4 & 4 \\ 3 & 6 & 6 \end{bmatrix} = \begin{bmatrix} 14 & 28 & 28 \\ 28 & 56 & 56 \\ 28 & 56 & 56 \end{bmatrix}
> $$
>
> Esta matriz $X'X$ √© singular (o determinante √© zero), pois a segunda e a terceira linhas s√£o proporcionais √† primeira. Ao tentar calcular a inversa, um erro ser√° gerado no programa ou ser√° retornado um valor "inf", impossibilitando o c√°lculo de $\beta$.
>
> ```python
> import numpy as np
>
> X = np.array([[1, 2, 2], [2, 4, 4], [3, 6, 6]])
> Y = np.array([5, 6, 8])
>
> X_transpose = X.T
> XTX = np.dot(X_transpose, X)
> try:
>     XTX_inv = np.linalg.inv(XTX) #This line will cause an error
>     print("Inversa de XTX:", XTX_inv)
> except np.linalg.LinAlgError:
>     print("A matriz XTX √© singular, n√£o √© poss√≠vel calcular a inversa.")
> ```
>
> Este exemplo demonstra como a depend√™ncia linear entre as vari√°veis explicativas leva a uma matriz singular, impossibilitando o c√°lculo direto do estimador OLS pela invers√£o.

Nesse cen√°rio, a **proje√ß√£o linear** $\alpha'X_t$ n√£o √© unicamente determinada pela condi√ß√£o de ortogonalidade [^3, 4.1.10]. Uma solu√ß√£o para esse problema √© a **redu√ß√£o de dimensionalidade**. Podemos eliminar as vari√°veis redundantes em $X_t$ e trabalhar com uma matriz $X_t^*$ contendo apenas as vari√°veis linearmente independentes. Em termos matem√°ticos, se a matriz $E(X_tX_t')$ √© singular, existe um vetor n√£o nulo $c$ tal que $c'E(X_tX_t')c = E(c'X_t)^2 = 0$, indicando que a combina√ß√£o linear $c'X_t$ √© igual a zero para todas as realiza√ß√µes [^3, 4.1.18]. Nestes casos, calcula-se a proje√ß√£o linear de $Y_{t+1}$ em $X_t^*$, onde $X_t^*$ √© composto pelas vari√°veis n√£o redundantes de $X_t$.

**Teorema 1** (Decomposi√ß√£o em Valores Singulares) Qualquer matriz $A_{m \times n}$ pode ser decomposta em $A = U \Sigma V'$, onde $U$ √© uma matriz ortogonal $m \times m$, $\Sigma$ √© uma matriz diagonal $m \times n$ com entradas n√£o negativas (valores singulares) e $V$ √© uma matriz ortogonal $n \times n$.

> üí° **Exemplo Num√©rico (SVD):** Utilizando a matriz $X$ do exemplo anterior com singularidade:
> $$
> X = \begin{bmatrix} 1 & 2 & 2 \\ 2 & 4 & 4 \\ 3 & 6 & 6 \end{bmatrix}
> $$
>
> Aplicando a Decomposi√ß√£o em Valores Singulares (SVD), temos:
> ```python
> import numpy as np
> from numpy.linalg import svd
>
> X = np.array([[1, 2, 2], [2, 4, 4], [3, 6, 6]])
> U, S, VT = svd(X)
> print("Matriz U:\n", U)
> print("Valores Singulares (S):\n", S)
> print("Matriz VT:\n", VT)
> ```
>  Os valores singulares em `S` indicam a import√¢ncia de cada componente. Observamos que, neste caso, o segundo e terceiro valores singulares s√£o zero (ou muito pr√≥ximos de zero devido a erros num√©ricos), confirmando a singularidade da matriz $X$. A SVD permite calcular uma pseudo-inversa para lidar com esse problema.
>
> Para ilustrar a remo√ß√£o de redund√¢ncia, neste exemplo removeriamos a terceira coluna de X, resultando em uma matriz com duas colunas linearmente independentes.

**Observa√ß√£o:** A decomposi√ß√£o em valores singulares (SVD) oferece uma abordagem alternativa para lidar com a singularidade. Embora a matriz $\left(\sum_{t=1}^T x_t x_t'\right)$ possa n√£o ser invert√≠vel, a SVD permite calcular uma pseudo-inversa que pode ser utilizada para obter uma solu√ß√£o para o problema de regress√£o, mesmo em casos de singularidade.

Em outras palavras, a proje√ß√£o linear $\alpha'X_t$, apesar de n√£o ter um vetor $\alpha$ √∫nico, representa a mesma vari√°vel aleat√≥ria, ou seja, $\alpha'X_t = \alpha^*'X_t^*$ para todas as combina√ß√µes lineares consistentes com a condi√ß√£o de ortogonalidade [^3, 4.1.18]. Em termos pr√°ticos, esta redu√ß√£o de dimensionalidade garante que a matriz $E(X_t^*X_t^{*'})$ seja n√£o singular e, portanto, invert√≠vel, permitindo calcular a proje√ß√£o linear e o estimador OLS.

### Conclus√£o
O c√°lculo do estimador OLS $b$ envolve a invers√£o de uma matriz e multiplica√ß√£o por um vetor. A singularidade da matriz de covari√¢ncia $\left(\sum_{t=1}^T x_t x_t'\right)$ impossibilita a aplica√ß√£o direta da f√≥rmula e exige t√©cnicas de redu√ß√£o de dimensionalidade. Ao remover as vari√°veis linearmente dependentes, garantimos que a proje√ß√£o linear seja unicamente determinada e que o estimador OLS possa ser calculado de maneira precisa. Esta discuss√£o evidencia a import√¢ncia de verificar a condi√ß√£o de singularidade antes da aplica√ß√£o de m√©todos de estima√ß√£o baseados em **invers√£o de matriz**, garantindo a validade e a interpretabilidade dos resultados. Al√©m disso, a decomposi√ß√£o em valores singulares fornece uma ferramenta alternativa para o c√°lculo do estimador OLS, mesmo quando a matriz de covari√¢ncia √© singular, por meio da computa√ß√£o da pseudo-inversa.

### Refer√™ncias
[^1]: 
[^2]: 
[^3]: *Trechos do texto onde o conceito √© discutido ou mencionado*
[^4]: *Trechos do texto onde o conceito √© discutido ou mencionado*
<!-- END -->
