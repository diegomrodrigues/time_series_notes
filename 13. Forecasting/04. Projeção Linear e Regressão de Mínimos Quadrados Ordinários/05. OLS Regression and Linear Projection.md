## Proje√ß√£o Linear e Regress√£o OLS: Implementa√ß√£o Computacional e Converg√™ncia

### Introdu√ß√£o

Este cap√≠tulo explora a rela√ß√£o entre **proje√ß√£o linear** e **regress√£o de m√≠nimos quadrados ordin√°rios (OLS)**, com um foco particular em como a **regress√£o OLS** implementa computacionalmente a proje√ß√£o linear. Como estabelecido em se√ß√µes anteriores [^4], a proje√ß√£o linear busca descrever rela√ß√µes entre vari√°veis utilizando momentos populacionais, enquanto a regress√£o OLS estima essas rela√ß√µes a partir de dados amostrais, usando momentos amostrais [^4, 4.1.17, 4.1.19]. A **equival√™ncia formal matem√°tica** entre as duas abordagens foi demonstrada, especialmente quando o tamanho da amostra tende ao infinito [^4, Ap√™ndice 4.A]. Este cap√≠tulo aprofunda essa discuss√£o, enfatizando a implementa√ß√£o computacional da regress√£o OLS e como o vetor de coeficientes $b$ converge para o vetor de coeficientes $\alpha$ da proje√ß√£o linear sob certas condi√ß√µes [^4, 4.1.20].

### Implementa√ß√£o Computacional da Proje√ß√£o Linear via Regress√£o OLS

A proje√ß√£o linear de $Y_{t+1}$ sobre $X_t$ √© definida como $\hat{Y}_{t+1} = \alpha'X_t$, onde $\alpha$ √© o vetor de coeficientes derivado dos momentos populacionais: $\alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}$ [^4, 4.1.13]. Em cen√°rios pr√°ticos, os momentos populacionais, $E(Y_{t+1}X_t')$ e $E(X_tX_t')$, s√£o geralmente desconhecidos. A **regress√£o OLS**, nesse contexto, emerge como uma abordagem computacional para estimar a proje√ß√£o linear usando dados amostrais. Como visto anteriormente [^4, 4.1.18], o estimador OLS $b$ √© calculado como:
$$
b = \left(\sum_{t=1}^T x_t x_t'\right)^{-1} \left(\sum_{t=1}^T x_t y_{t+1}\right).
$$
A implementa√ß√£o computacional da **regress√£o OLS** envolve as seguintes etapas:

1.  **C√°lculo dos Momentos Amostrais:** Computar $\sum_{t=1}^T x_t x_t'$ e $\sum_{t=1}^T x_t y_{t+1}$, que s√£o os an√°logos amostrais dos momentos populacionais $E(X_tX_t')$ e $E(Y_{t+1}X_t')$.
2.  **Invers√£o da Matriz:** Calcular a inversa da matriz $\left(\sum_{t=1}^T x_t x_t'\right)$, que √© crucial para determinar os coeficientes da regress√£o. √â importante lembrar que a **singularidade** desta matriz impede o c√°lculo direto, requerendo t√©cnicas de redu√ß√£o de dimensionalidade [^4, 4.1.18].
3.  **Multiplica√ß√£o de Matrizes:** Multiplicar a matriz inversa calculada no passo anterior pelo vetor $\left(\sum_{t=1}^T x_t y_{t+1}\right)$, resultando no vetor de coeficientes $b$.

Este processo computacional, embora baseado em opera√ß√µes lineares, implementa a proje√ß√£o linear de forma pr√°tica, usando dados observados para estimar os coeficientes da rela√ß√£o linear. Os coeficientes obtidos atrav√©s da **regress√£o OLS** (vetor $b$) s√£o, nesse sentido, uma *estimativa* do verdadeiro coeficiente populacional $\alpha$.

> üí° **Exemplo Num√©rico:** Vamos considerar um exemplo com $T=4$ observa√ß√µes e duas vari√°veis preditoras. Suponha que temos os seguintes dados:
>
> $X = \begin{bmatrix} 1 & 2 \\ 2 & 3 \\ 3 & 4 \\ 4 & 5 \end{bmatrix}$, $Y = \begin{bmatrix} 6 \\ 9 \\ 12 \\ 15 \end{bmatrix}$
>
> 1.  **C√°lculo dos Momentos Amostrais:** Primeiro, calculamos $X'X$ e $X'Y$:
> $$
> X'X = \begin{bmatrix} 1 & 2 & 3 & 4 \\ 2 & 3 & 4 & 5 \end{bmatrix} \begin{bmatrix} 1 & 2 \\ 2 & 3 \\ 3 & 4 \\ 4 & 5 \end{bmatrix} = \begin{bmatrix} 1\cdot1+2\cdot2+3\cdot3+4\cdot4 & 1\cdot2+2\cdot3+3\cdot4+4\cdot5 \\ 2\cdot1+3\cdot2+4\cdot3+5\cdot4 & 2\cdot2+3\cdot3+4\cdot4+5\cdot5 \end{bmatrix} = \begin{bmatrix} 30 & 40 \\ 40 & 54 \end{bmatrix}
> $$
> $$
> X'Y = \begin{bmatrix} 1 & 2 & 3 & 4 \\ 2 & 3 & 4 & 5 \end{bmatrix} \begin{bmatrix} 6 \\ 9 \\ 12 \\ 15 \end{bmatrix} = \begin{bmatrix} 1\cdot6+2\cdot9+3\cdot12+4\cdot15 \\ 2\cdot6+3\cdot9+4\cdot12+5\cdot15 \end{bmatrix} = \begin{bmatrix} 120 \\ 162 \end{bmatrix}
> $$
>
> 2.  **Invers√£o da Matriz:** Calculamos a inversa de $X'X$. Para uma matriz 2x2, a inversa √© dada por:
> $$
> A = \begin{bmatrix} a & b \\ c & d \end{bmatrix} \Rightarrow A^{-1} = \frac{1}{ad-bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}
> $$
>
> Aplicando isso, temos:
> $$
> (X'X)^{-1} = \frac{1}{(30 \times 54) - (40 \times 40)}\begin{bmatrix} 54 & -40 \\ -40 & 30 \end{bmatrix} = \frac{1}{1620 - 1600}\begin{bmatrix} 54 & -40 \\ -40 & 30 \end{bmatrix} = \frac{1}{20}\begin{bmatrix} 54 & -40 \\ -40 & 30 \end{bmatrix} = \begin{bmatrix} 2.7 & -2 \\ -2 & 1.5 \end{bmatrix}
> $$
>
> 3. **Multiplica√ß√£o de Matrizes:** Agora, multiplicamos $(X'X)^{-1}$ por $X'Y$:
> $$
> b = (X'X)^{-1} X'Y = \begin{bmatrix} 2.7 & -2 \\ -2 & 1.5 \end{bmatrix} \begin{bmatrix} 120 \\ 162 \end{bmatrix} = \begin{bmatrix} 2.7 \cdot 120 + (-2) \cdot 162 \\ -2 \cdot 120 + 1.5 \cdot 162 \end{bmatrix} = \begin{bmatrix} 324 - 324 \\ -240 + 243 \end{bmatrix} = \begin{bmatrix} 0 \\ 3 \end{bmatrix}
> $$
> Portanto, a equa√ß√£o de regress√£o OLS √© $\hat{Y}_{t+1} = 0 X_{t,1} + 3 X_{t,2}$, ou simplificando,  $\hat{Y}_{t+1} =  3 X_{t,2}$.
>
> ```python
> import numpy as np
>
> X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])
> Y = np.array([6, 9, 12, 15])
>
> X_transpose = X.T
>
> XTX = np.dot(X_transpose, X)
> XTY = np.dot(X_transpose, Y)
>
> XTX_inv = np.linalg.inv(XTX)
> beta = np.dot(XTX_inv, XTY)
>
> print("Estimated Beta:", beta)
> ```
>
> **Interpreta√ß√£o:** O coeficiente da vari√°vel $X_1$ √© 0 e o coeficiente da vari√°vel $X_2$ √© 3. Isso significa que, neste modelo, a vari√°vel $X_1$ n√£o contribui para a previs√£o de $Y$, enquanto um aumento de uma unidade em $X_2$ leva a um aumento de 3 unidades em $Y$.

**Observa√ß√£o 1:** √â importante notar que a regress√£o OLS, como implementada computacionalmente, n√£o apenas estima o coeficiente da proje√ß√£o linear, mas tamb√©m fornece um modelo preditivo. O vetor $\hat{Y}_{t+1} = b'X_t$ representa os valores preditos de $Y_{t+1}$ com base nos dados observados $X_t$. A diferen√ßa entre os valores observados $Y_{t+1}$ e os valores preditos $\hat{Y}_{t+1}$ s√£o os res√≠duos da regress√£o.

**Lema 2** (Propriedades dos Res√≠duos OLS):
Sejam $\hat{Y}_{t+1}$ os valores preditos pela regress√£o OLS e $e_t = Y_{t+1} - \hat{Y}_{t+1}$ os res√≠duos, ent√£o:
1.  $\sum_{t=1}^T e_t = 0$, se o modelo inclui uma constante.
2.  $\sum_{t=1}^T e_t x_t = 0$

*Prova*:
1. Se o modelo inclui uma constante (i.e., a primeira coluna de $X$ √© um vetor de 1's), ent√£o a primeira linha de $X'e =0$ implica que $\sum_{t=1}^T e_t = 0$.
2. Por defini√ß√£o, o estimador OLS $b$ minimiza a soma dos quadrados dos res√≠duos. Isto √©, $b$ √© escolhido de forma que $ \frac{\partial}{\partial b} \sum_{t=1}^{T} (y_{t+1} - x_t'b)^2 = 0 $. Calculando a derivada, obtemos $-2 \sum_{t=1}^{T} x_t (y_{t+1} - x_t'b) = -2 \sum_{t=1}^{T} x_t e_t = 0$, que implica $\sum_{t=1}^T e_t x_t = 0$. $\blacksquare$

> üí° **Exemplo Num√©rico (Res√≠duos):** Usando os dados do exemplo anterior, vamos calcular os res√≠duos e verificar as propriedades mencionadas no Lema 2.
>
> Os valores preditos s√£o $\hat{Y} = 3 X_2$. Portanto, $\hat{Y} = \begin{bmatrix} 3\times2 \\ 3\times3 \\ 3\times4 \\ 3\times5 \end{bmatrix} = \begin{bmatrix} 6 \\ 9 \\ 12 \\ 15 \end{bmatrix}$
>
> Os res√≠duos s√£o: $e = Y - \hat{Y} = \begin{bmatrix} 6-6 \\ 9-9 \\ 12-12 \\ 15-15 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \end{bmatrix}$
>
> 1.  A soma dos res√≠duos √© $\sum e_t = 0 + 0 + 0 + 0 = 0$.
> 2.  O produto dos res√≠duos com os regressores √© $\sum e_t x_t = 0 \cdot \begin{bmatrix} 1 \\ 2 \end{bmatrix} +  0 \cdot \begin{bmatrix} 2 \\ 3 \end{bmatrix} + 0 \cdot \begin{bmatrix} 3 \\ 4 \end{bmatrix} +  0 \cdot \begin{bmatrix} 4 \\ 5 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix} $.
>
> ```python
> import numpy as np
>
> X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])
> Y = np.array([6, 9, 12, 15])
>
> X_transpose = X.T
>
> XTX = np.dot(X_transpose, X)
> XTY = np.dot(X_transpose, Y)
>
> XTX_inv = np.linalg.inv(XTX)
> beta = np.dot(XTX_inv, XTY)
>
> Y_hat = np.dot(X,beta)
> residuals = Y- Y_hat
>
> print("Residuals:", residuals)
> print("Sum of Residuals:", np.sum(residuals))
> print("Sum of Residuals * X:", np.dot(X_transpose, residuals))
> ```
>
> Como esperado, a soma dos res√≠duos e a soma dos res√≠duos multiplicados pelos regressores s√£o zero, confirmando as propriedades dos res√≠duos OLS.

### Converg√™ncia do Estimador OLS para a Proje√ß√£o Linear

Sob condi√ß√µes de estacionaridade e ergodicidade, como discutido anteriormente [^4], os momentos amostrais convergem para seus equivalentes populacionais quando o tamanho da amostra $T$ tende ao infinito. Especificamente, como visto anteriormente:

**Lema 1:** (Converg√™ncia da M√©dia Amostral)
$\frac{1}{T} \sum_{t=1}^T X_t \xrightarrow{p} E[X_t]$ quando $T \to \infty$.

**Lema 1.1:** (Converg√™ncia dos Momentos Amostrais)
$\frac{1}{T} \sum_{t=1}^T X_t X_t' \xrightarrow{p} E[X_t X_t']$ quando $T \to \infty$.

Com a converg√™ncia dos momentos amostrais, o estimador OLS $b$ tamb√©m converge para o coeficiente da proje√ß√£o linear $\alpha$. Isso significa que, √† medida que aumentamos o n√∫mero de observa√ß√µes, a regress√£o OLS passa a ser uma aproxima√ß√£o cada vez mais precisa da proje√ß√£o linear, que se baseia nos par√¢metros populacionais [^4, 4.1.20].

Formalmente, como visto anteriormente, o estimador OLS √© dado por:
$$
b = \left(\frac{1}{T}\sum_{t=1}^T x_t x_t'\right)^{-1} \left(\frac{1}{T}\sum_{t=1}^T x_t y_{t+1}\right)
$$
E o coeficiente da proje√ß√£o linear √© dado por:
$$
\alpha = (E[X_tX_t'])^{-1} E[Y_{t+1}X_t']
$$
A proposi√ß√£o abaixo estabelece a **converg√™ncia** do estimador OLS para a proje√ß√£o linear sob condi√ß√µes de estacionariedade e ergodicidade:

**Proposi√ß√£o 1:** (Consist√™ncia do Estimador OLS)
Se o processo $(X_t, Y_{t+1})$ √© estacion√°rio e erg√≥dico, e se a matriz $E(X_tX_t')$ √© n√£o-singular, ent√£o o estimador OLS $b = \left(\sum_{t=1}^T x_t x_t'\right)^{-1} \left(\sum_{t=1}^T x_t y_{t+1}\right)$ converge em probabilidade para o coeficiente da proje√ß√£o linear $\alpha = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}$ quando $T$ tende ao infinito.
*Prova*:
I. Seja $(X_t, Y_{t+1})$ um processo estacion√°rio e erg√≥dico.
II. Pelo Lema 1 (Converg√™ncia das M√©dias Amostrais) e o Lema 1.1 (Converg√™ncia dos Momentos Amostrais), e pela propriedade da ergodicidade, temos que:
$\frac{1}{T} \sum_{t=1}^T x_t x_t' \xrightarrow{p} E(X_t X_t')$ quando $T \to \infty$, e
$\frac{1}{T} \sum_{t=1}^T x_t y_{t+1} \xrightarrow{p} E(Y_{t+1} X_t')$ quando $T \to \infty$.
III. Podemos reescrever o estimador OLS $b$ como: $b = \left(\frac{1}{T}\sum_{t=1}^T x_t x_t'\right)^{-1} \left(\frac{1}{T}\sum_{t=1}^T x_t y_{t+1}\right)$.
IV. Dado que $E(X_t X_t')$ √© n√£o-singular (invert√≠vel), sua inversa existe, e pela continuidade da opera√ß√£o de invers√£o:
$\left(\frac{1}{T} \sum_{t=1}^T x_t x_t'\right)^{-1} \xrightarrow{p} \left(E(X_t X_t')\right)^{-1}$ quando $T \to \infty$.
V. Pela propriedade de converg√™ncia de produtos em probabilidade, se $A_T \xrightarrow{p} A$ e $B_T \xrightarrow{p} B$, ent√£o $A_T B_T \xrightarrow{p} AB$.  Aplicando essa propriedade, temos que:
$b \xrightarrow{p}  (E(X_t X_t'))^{-1} E(Y_{t+1} X_t') = \alpha$ quando $T \to \infty$.
VI. Portanto, o estimador OLS $b$ converge em probabilidade para o coeficiente da proje√ß√£o linear $\alpha$. $\blacksquare$

**Proposi√ß√£o 1.1:** (Consist√™ncia do Estimador OLS com um Termo Constante)
Se o processo $(X_t, Y_{t+1})$ √© estacion√°rio e erg√≥dico, e se a matriz $E(Z_tZ_t')$ √© n√£o-singular, onde $Z_t = [1, X_t']'$, ent√£o o estimador OLS $b = \left(\sum_{t=1}^T z_t z_t'\right)^{-1} \left(\sum_{t=1}^T z_t y_{t+1}\right)$ converge em probabilidade para o coeficiente da proje√ß√£o linear com intercepto $\alpha = E(Y_{t+1}Z_t') [E(Z_tZ_t')]^{-1}$ quando $T$ tende ao infinito.
*Prova*:
I. Seja $(X_t, Y_{t+1})$ um processo estacion√°rio e erg√≥dico. Defina $Z_t = [1, X_t']'$, onde o 1 representa o termo constante.
II. Pelo Lema 1 (Converg√™ncia das M√©dias Amostrais) e o Lema 1.1 (Converg√™ncia dos Momentos Amostrais), e pela propriedade da ergodicidade, temos que:
$\frac{1}{T} \sum_{t=1}^T z_t z_t' \xrightarrow{p} E(Z_t Z_t')$ quando $T \to \infty$, e
$\frac{1}{T} \sum_{t=1}^T z_t y_{t+1} \xrightarrow{p} E(Y_{t+1} Z_t')$ quando $T \to \infty$.
III. Podemos reescrever o estimador OLS $b$ com termo constante como: $b = \left(\frac{1}{T}\sum_{t=1}^T z_t z_t'\right)^{-1} \left(\frac{1}{T}\sum_{t=1}^T z_t y_{t+1}\right)$.
IV. Dado que $E(Z_t Z_t')$ √© n√£o-singular (invert√≠vel), sua inversa existe, e pela continuidade da opera√ß√£o de invers√£o:
$\left(\frac{1}{T} \sum_{t=1}^T z_t z_t'\right)^{-1} \xrightarrow{p} \left(E(Z_t Z_t')\right)^{-1}$ quando $T \to \infty$.
V. Pela propriedade de converg√™ncia de produtos em probabilidade, se $A_T \xrightarrow{p} A$ e $B_T \xrightarrow{p} B$, ent√£o $A_T B_T \xrightarrow{p} AB$. Aplicando essa propriedade, temos que:
$b \xrightarrow{p} (E(Z_t Z_t'))^{-1} E(Y_{t+1} Z_t') = \alpha$ quando $T \to \infty$.
VI. Portanto, o estimador OLS $b$ com termo constante converge em probabilidade para o coeficiente da proje√ß√£o linear com intercepto $\alpha$. $\blacksquare$

> üí° **Exemplo Num√©rico (Converg√™ncia):** Para ilustrar a converg√™ncia na pr√°tica, suponha que a rela√ß√£o *verdadeira* seja  $Y_{t+1} = 2X_t + 3 + \epsilon_t$, onde $\epsilon_t$ √© um erro com m√©dia zero e desvio padr√£o igual a 5. Vamos simular dados com diferentes tamanhos de amostra ($T = 5, 50, 500$) e verificar como o estimador OLS se aproxima do valor verdadeiro (coeficiente 2 e intercepto 3).
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def generate_data(T, true_beta=2, true_intercept = 3, std_dev=5, x_mean = 50, x_std = 20):
>   X = np.random.normal(x_mean, x_std, T).reshape(-1, 1)
>   epsilon = np.random.normal(0, std_dev, T).reshape(-1, 1)
>   Y = true_beta * X + true_intercept + epsilon
>   Z = np.concatenate((np.ones((T,1)), X), axis=1)
>   return Z, Y
>
> def estimate_ols(Z, Y):
>    Z_transpose = Z.T
>    ZTZ = np.dot(Z_transpose, Z)
>    ZTY = np.dot(Z_transpose, Y)
>    ZTZ_inv = np.linalg.inv(ZTZ)
>    beta = np.dot(ZTZ_inv, ZTY)
>    return beta
>
> sample_sizes = [5, 50, 500]
> estimated_betas = []
> estimated_intercepts = []
>
> for T in sample_sizes:
>   Z, Y = generate_data(T)
>   beta_hat = estimate_ols(Z, Y)
>   estimated_intercepts.append(beta_hat[0][0])
>   estimated_betas.append(beta_hat[1][0])
>
> plt.figure(figsize=(8, 6))
> plt.subplot(2,1,1)
> plt.plot(sample_sizes, estimated_betas, marker='o')
> plt.axhline(y=2, color='r', linestyle='--', label='Beta Verdadeiro')
> plt.xlabel('Tamanho da Amostra (T)')
> plt.ylabel('Estimativa do Beta')
> plt.title('Converg√™ncia do Estimador OLS - Coeficiente Beta')
> plt.legend()
> plt.grid(True)
> plt.subplot(2,1,2)
> plt.plot(sample_sizes, estimated_intercepts, marker='o')
> plt.axhline(y=3, color='r', linestyle='--', label='Intercepto Verdadeiro')
> plt.xlabel('Tamanho da Amostra (T)')
> plt.ylabel('Estimativa do Intercepto')
> plt.title('Converg√™ncia do Estimador OLS - Intercepto')
> plt.legend()
> plt.tight_layout()
> plt.show()
>
> print(f"Estimativa Beta para T=5: {estimated_betas[0]:.2f}")
> print(f"Estimativa Intercepto para T=5: {estimated_intercepts[0]:.2f}")
> print(f"Estimativa Beta para T=50: {estimated_betas[1]:.2f}")
> print(f"Estimativa Intercepto para T=50: {estimated_intercepts[1]:.2f}")
> print(f"Estimativa Beta para T=500: {estimated_betas[2]:.2f}")
> print(f"Estimativa Intercepto para T=500: {estimated_intercepts[2]:.2f}")
> ```
>
> **Interpreta√ß√£o:** As estimativas para o coeficiente beta e para o intercepto se aproximam dos valores verdadeiros de 2 e 3, respectivamente, conforme o tamanho da amostra aumenta. Esta √© uma demonstra√ß√£o pr√°tica da converg√™ncia do estimador OLS para os verdadeiros par√¢metros populacionais. Observe que, com amostras pequenas ($T=5$), a estimativa pode estar bem distante do verdadeiro valor, demonstrando que a consist√™ncia √© uma propriedade assint√≥tica (i.e., v√°lida quando $T \to \infty$).

A **implementa√ß√£o computacional** da regress√£o OLS, portanto, permite que obtenhamos uma estimativa pr√°tica para a proje√ß√£o linear, e com amostras suficientemente grandes, tal estimativa converge para os verdadeiros par√¢metros da popula√ß√£o.

### Conclus√£o

A regress√£o OLS oferece uma **implementa√ß√£o computacional** da proje√ß√£o linear, onde o vetor de coeficientes $b$ √© derivado a partir de momentos amostrais. A **equival√™ncia formal matem√°tica** entre os dois m√©todos √© estabelecida quando o tamanho da amostra tende ao infinito, sob condi√ß√µes de estacionaridade e ergodicidade, onde o estimador OLS converge em probabilidade para o coeficiente da proje√ß√£o linear. Este cap√≠tulo enfatiza como a regress√£o OLS, atrav√©s de opera√ß√µes de **invers√£o de matriz** e **multiplica√ß√£o**, busca encontrar um ajuste que minimiza a soma dos quadrados dos res√≠duos, e sob certas condi√ß√µes, este ajuste se torna uma aproxima√ß√£o cada vez mais precisa da rela√ß√£o populacional expressa pela proje√ß√£o linear.

### Refer√™ncias
[^1]:
[^2]:
[^3]: *Trechos do texto onde o conceito √© discutido ou mencionado*
[^4]: *Trechos do texto onde o conceito √© discutido ou mencionado*
<!-- END -->
