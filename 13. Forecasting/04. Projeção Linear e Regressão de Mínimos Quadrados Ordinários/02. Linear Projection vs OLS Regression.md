## Proje√ß√£o Linear vs. Regress√£o OLS: Interpreta√ß√µes e Mecanismos Alg√©bricos Compartilhados

### Introdu√ß√£o
Este cap√≠tulo aprofunda a distin√ß√£o entre **proje√ß√£o linear** e **regress√£o de m√≠nimos quadrados ordin√°rios (OLS)**, focando nas suas diferentes interpreta√ß√µes, enquanto reconhece os mecanismos alg√©bricos compartilhados. Como vimos anteriormente [^4], a proje√ß√£o linear busca a melhor aproxima√ß√£o linear de uma vari√°vel aleat√≥ria com base em outras vari√°veis, usando momentos populacionais, enquanto a regress√£o OLS busca minimizar a soma dos quadrados dos res√≠duos em uma amostra finita [^4, 4.1.17]. Embora as f√≥rmulas para o c√°lculo dos coeficientes sejam semelhantes, a diferen√ßa fundamental reside na interpreta√ß√£o dos resultados [^4, 4.1.19]. Este t√≥pico explora essa dualidade, elucidando como a mesma estrutura matem√°tica pode ser utilizada para prop√≥sitos distintos: descrever rela√ß√µes populacionais e resumir dados amostrais [^4, 4.1.20].

### Conceitos Fundamentais
Como vimos em se√ß√µes anteriores, a **proje√ß√£o linear** de $Y_{t+1}$ sobre $X_t$ √© expressa como $\hat{Y}_{t+1} = \alpha' X_t$ [^4, 4.1.21]. O vetor de coeficientes $\alpha$ √© determinado a partir de momentos populacionais, ou seja, $\alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}$ [^4, 4.1.13]. Esta abordagem tem como objetivo capturar a rela√ß√£o linear *verdadeira* entre $Y_{t+1}$ e $X_t$, que √© uma caracter√≠stica da popula√ß√£o [^4, 4.1.20]. J√° a **regress√£o OLS**, por outro lado, busca encontrar os coeficientes $b$ que minimizem a soma dos quadrados dos res√≠duos em uma amostra finita [^4, 4.1.17]:
$$
b = \left(\sum_{t=1}^T x_t x_t'\right)^{-1} \left(\sum_{t=1}^T x_t y_{t+1}\right).
$$
Como vimos anteriormente, esta f√≥rmula envolve a **invers√£o da matriz** $\left(\sum_{t=1}^T x_t x_t'\right)$ e sua **multiplica√ß√£o** pelo vetor $\left(\sum_{t=1}^T x_t y_{t+1}\right)$.

> üí° **Exemplo Num√©rico:** Considere um cen√°rio onde estamos modelando o consumo ($Y_{t+1}$) como uma fun√ß√£o da renda ($X_t$). Em um contexto de **proje√ß√£o linear**, n√≥s usar√≠amos os momentos populacionais (que em geral desconhecemos) para calcular $\alpha$. Por exemplo, se $E(Y_{t+1}X_t) = 10$ e $E(X_tX_t') = 4$, ent√£o $\alpha = 10/4 = 2.5$, e a rela√ß√£o de proje√ß√£o seria $Y_{t+1} = 2.5 X_t$. No entanto, na **regress√£o OLS**, temos dados amostrais de consumo e renda. Digamos que temos tr√™s observa√ß√µes $(x_1,y_2)=(1,3)$, $(x_2,y_3)=(2,6)$, e $(x_3,y_4)=(3,7)$. Ent√£o, o estimador $b$ seria obtido atrav√©s da f√≥rmula da regress√£o OLS:
>
> $$
> X = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} ; Y = \begin{bmatrix} 3 \\ 6 \\ 7 \end{bmatrix}
> $$
>
> Calculando $X'X$ e $X'Y$:
>
> $$
> X'X = \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} = 14
> $$
> $$
> X'Y = \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 3 \\ 6 \\ 7 \end{bmatrix} = 36
> $$
>
> Assim,
>
> $b = (X'X)^{-1} X'Y = 14^{-1} \times 36 = 36/14 \approx 2.57$.
>
> A rela√ß√£o estimada pela regress√£o OLS seria $\hat{Y}_{t+1} = 2.57X_t$. Note que os coeficientes obtidos com proje√ß√£o linear (usando momentos populacionais) e regress√£o OLS (usando dados amostrais) s√£o diferentes, o que ilustra a distin√ß√£o fundamental nas suas interpreta√ß√µes.
>
> ```python
> import numpy as np
>
> X = np.array([[1], [2], [3]])
> Y = np.array([[3], [6], [7]])
>
> X_transpose = X.T
>
> XTX = np.dot(X_transpose, X)
>
> XTY = np.dot(X_transpose, Y)
>
> XTX_inv = np.linalg.inv(XTX)
>
> beta = np.dot(XTX_inv, XTY)
>
> print("Estimated Beta:", beta)
> ```
>
> Nesse exemplo, a proje√ß√£o linear se basearia em $E(Y_{t+1}X_t)$ e $E(X_tX_t')$ (que seriam momentos populacionais e n√£o as amostras), enquanto a regress√£o OLS usa os dados amostrais para encontrar o melhor ajuste para esses dados.

> üí° **Exemplo Num√©rico:** Para ilustrar melhor, vamos adicionar um exemplo com mais dados e calcular os res√≠duos. Suponha que temos as seguintes 5 observa√ß√µes de renda ($X_t$) e consumo ($Y_{t+1}$):
>
> | t | $X_t$ | $Y_{t+1}$ |
> |---|-------|----------|
> | 1 | 10    | 22       |
> | 2 | 15    | 34       |
> | 3 | 20    | 40       |
> | 4 | 25    | 53       |
> | 5 | 30    | 60       |
>
>  A matriz $X$ agora ser√°:
>
> $$
> X = \begin{bmatrix} 10 \\ 15 \\ 20 \\ 25 \\ 30 \end{bmatrix} ; Y = \begin{bmatrix} 22 \\ 34 \\ 40 \\ 53 \\ 60 \end{bmatrix}
> $$
> Calculando $X'X$ e $X'Y$:
>
> $$
> X'X = \begin{bmatrix} 10 & 15 & 20 & 25 & 30 \end{bmatrix} \begin{bmatrix} 10 \\ 15 \\ 20 \\ 25 \\ 30 \end{bmatrix} = 2750
> $$
> $$
> X'Y = \begin{bmatrix} 10 & 15 & 20 & 25 & 30 \end{bmatrix} \begin{bmatrix} 22 \\ 34 \\ 40 \\ 53 \\ 60 \end{bmatrix} = 6900
> $$
> Assim,
>
> $b = (X'X)^{-1} X'Y = 2750^{-1} \times 6900 \approx 2.509$.
>
> A rela√ß√£o estimada pela regress√£o OLS seria $\hat{Y}_{t+1} = 2.509X_t$.
>
> Os valores previstos $\hat{Y}$ s√£o:
>
> | t | $X_t$ | $Y_{t+1}$ | $\hat{Y}_{t+1}$ | Res√≠duos ($Y_{t+1} - \hat{Y}_{t+1}$) |
> |---|-------|----------|-----------------|----------------------------------|
> | 1 | 10    | 22       | 25.09           | -3.09                             |
> | 2 | 15    | 34       | 37.64           | -3.64                             |
> | 3 | 20    | 40       | 50.18           | -10.18                             |
> | 4 | 25    | 53       | 62.73           | -9.73                             |
> | 5 | 30    | 60       | 75.27           | -15.27                             |
>
> ```python
> import numpy as np
>
> X = np.array([[10], [15], [20], [25], [30]])
> Y = np.array([[22], [34], [40], [53], [60]])
>
> X_transpose = X.T
>
> XTX = np.dot(X_transpose, X)
> XTY = np.dot(X_transpose, Y)
>
> XTX_inv = np.linalg.inv(XTX)
>
> beta = np.dot(XTX_inv, XTY)
>
> Y_hat = np.dot(X, beta)
> residuals = Y - Y_hat
>
> print("Estimated Beta:", beta)
> print("Predicted Y:", Y_hat)
> print("Residuals:", residuals)
> ```
> Este exemplo mostra como o OLS encontra um ajuste que minimiza a soma dos quadrados dos res√≠duos, ilustrando a diferen√ßa entre os valores observados e os valores previstos pelo modelo. √â importante analisar os res√≠duos para avaliar a qualidade do ajuste.

A **principal distin√ß√£o** reside no foco: enquanto a **proje√ß√£o linear** busca descrever uma rela√ß√£o linear *verdadeira* na popula√ß√£o (que geralmente n√£o √© diretamente observ√°vel), a **regress√£o OLS** busca resumir uma rela√ß√£o linear em uma amostra espec√≠fica. O estimador OLS $b$ √© um resumo dos dados amostrais, enquanto $\alpha$ √© um par√¢metro populacional [^4, 4.1.20]. Em outras palavras, o estimador $b$ √© uma *estimativa* do par√¢metro populacional $\alpha$, e a qualidade desta estimativa depender√° do tamanho da amostra e das propriedades estat√≠sticas do processo gerador de dados. Em termos computacionais, ambos os c√°lculos envolvem **invers√£o de matriz** e **multiplica√ß√£o de matrizes**, mas a motiva√ß√£o e a interpreta√ß√£o dos resultados diferem.

> üí° **Analogia:** Imagine que voc√™ est√° tentando descrever a altura m√©dia de todos os indiv√≠duos de um pa√≠s.
> - A **proje√ß√£o linear** seria an√°loga a usar um modelo matem√°tico que tenta capturar a rela√ß√£o *verdadeira* entre a altura e vari√°veis gen√©ticas, buscando um par√¢metro que descreva essa rela√ß√£o em toda a popula√ß√£o.
> - A **regress√£o OLS** seria an√°loga a medir a altura de algumas pessoas e calcular a m√©dia desses dados amostrais. A m√©dia calculada √© um resumo da amostra, n√£o necessariamente a m√©dia *verdadeira* da popula√ß√£o, e sua precis√£o depender√° do tamanho e da representatividade da amostra.

Apesar das diferentes interpreta√ß√µes, as opera√ß√µes alg√©bricas para encontrar os coeficientes em ambas as abordagens s√£o as mesmas. A f√≥rmula para o estimador OLS $b$ [^4, 4.1.18] √© algebricamente id√™ntica √† express√£o para o coeficiente da proje√ß√£o linear $\alpha$, quando os momentos populacionais s√£o substitu√≠dos por seus equivalentes amostrais. Esta equival√™ncia matem√°tica √© formalizada atrav√©s da introdu√ß√£o de uma vari√°vel aleat√≥ria constru√≠da a partir dos dados amostrais [^4, Ap√™ndice 4.A], garantindo que os momentos da vari√°vel artificial sejam iguais aos momentos amostrais observados. Desta forma, a regress√£o OLS pode ser vista como um caso especial da proje√ß√£o linear, onde a distribui√ß√£o populacional √© substitu√≠da pela distribui√ß√£o emp√≠rica amostral.

> üí° **Formaliza√ß√£o:** Formalmente, podemos criar uma vari√°vel aleat√≥ria $\xi$ que toma os valores de $x_t$ com probabilidade $1/T$. Assim, $E(\xi) = \frac{1}{T}\sum_{t=1}^T x_t$ e $E(\xi\xi') = \frac{1}{T}\sum_{t=1}^T x_t x_t'$. Analogamente, podemos criar uma vari√°vel aleat√≥ria $\omega$ que toma os valores de $y_{t+1}$ com probabilidade $1/T$. Assim, $E(\omega) = \frac{1}{T}\sum_{t=1}^T y_{t+1}$. Se calcularmos a proje√ß√£o linear de $\omega$ em $\xi$, obtemos o estimador de m√≠nimos quadrados $b$:
> $$
> \alpha' = E(\omega\xi') [E(\xi\xi')]^{-1} = \left( \frac{1}{T} \sum_{t=1}^T y_{t+1}x_t' \right) \left( \frac{1}{T} \sum_{t=1}^T x_t x_t' \right)^{-1} = \left(\sum_{t=1}^T x_t x_t'\right)^{-1} \left(\sum_{t=1}^T x_t y_{t+1}\right) = b'
> $$
> Isso demonstra que o estimador OLS pode ser obtido aplicando a f√≥rmula da proje√ß√£o linear em vari√°veis aleat√≥rias artificiais que replicam os momentos amostrais.

**Teorema 1** (Converg√™ncia da M√©dia Amostral) Se um processo {X_t} √© estacion√°rio e erg√≥dico, ent√£o a m√©dia amostral $\bar{X} = \frac{1}{T} \sum_{t=1}^T X_t$ converge em probabilidade para a m√©dia populacional $\mu$ quando T tende ao infinito.
*Prova*: 
I. Seja $\{X_t\}$ um processo estacion√°rio e erg√≥dico.
II. Pela defini√ß√£o de estacionariedade, a m√©dia populacional $\mu = E[X_t]$ √© constante para todo $t$.
III. Pela defini√ß√£o de ergodicidade, a m√©dia amostral $\bar{X} = \frac{1}{T} \sum_{t=1}^T X_t$ converge em probabilidade para a m√©dia populacional $\mu$ quando $T \to \infty$.
IV.  Portanto, $\bar{X} \xrightarrow{p} \mu$ quando $T \to \infty$. $\blacksquare$

**Lema 1.1** (Converg√™ncia dos Momentos Amostrais) Se um processo $\{X_t\}$ √© estacion√°rio e erg√≥dico, ent√£o o momento amostral $\frac{1}{T}\sum_{t=1}^T X_t X_t'$ converge em probabilidade para o momento populacional $E(X_t X_t')$ quando $T$ tende ao infinito.
*Prova*:
I. Seja $\{X_t\}$ um processo estacion√°rio e erg√≥dico.
II. Defina $Z_t = X_t X_t'$.
III. Como $\{X_t\}$ √© estacion√°rio e erg√≥dico, $\{Z_t\}$ tamb√©m o √©.
IV. Pelo Teorema 1 (Converg√™ncia da M√©dia Amostral), a m√©dia amostral $\frac{1}{T}\sum_{t=1}^T Z_t = \frac{1}{T}\sum_{t=1}^T X_t X_t'$ converge em probabilidade para a m√©dia populacional $E[Z_t] = E(X_t X_t')$ quando $T \to \infty$.
V. Portanto, $\frac{1}{T}\sum_{t=1}^T X_t X_t' \xrightarrow{p} E(X_t X_t')$ quando $T \to \infty$. $\blacksquare$

Conforme o teorema e lema acima, sob condi√ß√µes de estacionaridade e ergodicidade, os momentos amostrais convergem para os seus equivalentes populacionais quando o tamanho da amostra tende ao infinito [^4, 4.1.20]. Isso significa que, com um n√∫mero suficientemente grande de observa√ß√µes, a regress√£o OLS fornece estimativas consistentes da proje√ß√£o linear da popula√ß√£o. No entanto, quando as suposi√ß√µes de estacionaridade ou ergodicidade n√£o s√£o v√°lidas, a regress√£o OLS pode gerar resultados inconsistentes para as proje√ß√µes lineares populacionais, como mencionado anteriormente [^4, 4.1.20].

> üí° **Exemplo Num√©rico:** Para ilustrar a converg√™ncia, vamos supor que a verdadeira rela√ß√£o populacional entre a renda ($X_t$) e o consumo ($Y_{t+1}$) seja $Y_{t+1} = 2.5X_t + \epsilon_t$, onde $\epsilon_t$ √© um erro aleat√≥rio com m√©dia zero. Vamos gerar conjuntos de dados simulados com diferentes tamanhos de amostra (T=20, T=100, T=1000) e verificar como o estimador OLS converge para o valor verdadeiro de 2.5.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def generate_data(T, beta_true=2.5, std_dev=5):
>     X = np.random.uniform(10, 100, T).reshape(-1, 1)
>     epsilon = np.random.normal(0, std_dev, T).reshape(-1, 1)
>     Y = beta_true * X + epsilon
>     return X, Y
>
> def estimate_ols(X, Y):
>    X_transpose = X.T
>    XTX = np.dot(X_transpose, X)
>    XTY = np.dot(X_transpose, Y)
>    XTX_inv = np.linalg.inv(XTX)
>    beta = np.dot(XTX_inv, XTY)
>    return beta
>
> sample_sizes = [20, 100, 1000]
> estimated_betas = []
>
> for T in sample_sizes:
>    X, Y = generate_data(T)
>    beta_hat = estimate_ols(X, Y)
>    estimated_betas.append(beta_hat[0][0])
>
>
> plt.figure(figsize=(8, 6))
> plt.plot(sample_sizes, estimated_betas, marker='o')
> plt.axhline(y=2.5, color='r', linestyle='--', label='Beta Verdadeiro')
> plt.xlabel('Tamanho da Amostra (T)')
> plt.ylabel('Estimativa do Beta')
> plt.title('Converg√™ncia do Estimador OLS')
> plt.legend()
> plt.grid(True)
> plt.show()
>
> print(f"Estimativa para T=20: {estimated_betas[0]}")
> print(f"Estimativa para T=100: {estimated_betas[1]}")
> print(f"Estimativa para T=1000: {estimated_betas[2]}")
> ```
> Este exemplo mostra como, √† medida que o tamanho da amostra aumenta, a estimativa do OLS se aproxima do valor verdadeiro de $\beta$, ilustrando a consist√™ncia do estimador sob certas condi√ß√µes.

**Proposi√ß√£o 2** (Consist√™ncia do Estimador OLS) Se o processo $(X_t, Y_{t+1})$ √© estacion√°rio e erg√≥dico, e se a matriz $E(X_tX_t')$ √© n√£o-singular, ent√£o o estimador OLS $b = \left(\sum_{t=1}^T x_t x_t'\right)^{-1} \left(\sum_{t=1}^T x_t y_{t+1}\right)$ converge em probabilidade para o coeficiente da proje√ß√£o linear $\alpha = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}$ quando $T$ tende ao infinito.
*Prova*:
I. Seja o processo $(X_t, Y_{t+1})$ estacion√°rio e erg√≥dico.
II. Pelo Lema 1.1, temos que $\frac{1}{T}\sum_{t=1}^T x_t x_t' \xrightarrow{p} E(X_t X_t')$ quando $T \to \infty$.
III. Similarmente, $\frac{1}{T}\sum_{t=1}^T x_t y_{t+1} \xrightarrow{p} E(Y_{t+1}X_t')$ quando $T \to \infty$.
IV. O estimador OLS √© dado por $b = \left(\sum_{t=1}^T x_t x_t'\right)^{-1} \left(\sum_{t=1}^T x_t y_{t+1}\right)$.
V. Podemos reescrever o estimador OLS como $b = \left(\frac{1}{T}\sum_{t=1}^T x_t x_t'\right)^{-1} \left(\frac{1}{T}\sum_{t=1}^T x_t y_{t+1}\right)$.
VI. Dado que a matriz $E(X_tX_t')$ √© n√£o-singular, sua inversa existe. A fun√ß√£o de invers√£o de matrizes √© cont√≠nua em matrizes n√£o-singulares.
VII. Como a converg√™ncia em probabilidade preserva continuidade, temos que $\left(\frac{1}{T}\sum_{t=1}^T x_t x_t'\right)^{-1} \xrightarrow{p} \left(E(X_t X_t')\right)^{-1}$.
VIII. Pela propriedade de converg√™ncia em probabilidade, o produto de converg√™ncias tamb√©m converge em probabilidade, ent√£o:
$b = \left(\frac{1}{T}\sum_{t=1}^T x_t x_t'\right)^{-1} \left(\frac{1}{T}\sum_{t=1}^T x_t y_{t+1}\right) \xrightarrow{p} \left(E(X_t X_t')\right)^{-1} E(Y_{t+1}X_t') = \alpha$.
IX. Portanto, o estimador OLS $b$ converge em probabilidade para o coeficiente da proje√ß√£o linear $\alpha$ quando $T \to \infty$. $\blacksquare$

### Conclus√£o
A **proje√ß√£o linear** e a **regress√£o OLS** compartilham mecanismos alg√©bricos semelhantes, mas diferem em sua interpreta√ß√£o fundamental. A proje√ß√£o linear foca em rela√ß√µes populacionais, enquanto a regress√£o OLS resume dados amostrais. Apesar dessa distin√ß√£o, a regress√£o OLS pode ser vista como um caso particular da proje√ß√£o linear, onde a distribui√ß√£o populacional √© substitu√≠da pela distribui√ß√£o emp√≠rica. Sob condi√ß√µes de estacionariedade e ergodicidade, os momentos amostrais convergem para os populacionais, tornando a regress√£o OLS uma ferramenta √∫til para estimar as proje√ß√µes lineares da popula√ß√£o, o que evidencia o mecanismo de converg√™ncia para as proje√ß√µes populacionais quando as amostras tendem a infinito.

### Refer√™ncias
[^1]: 
[^2]: 
[^3]: *Trechos do texto onde o conceito √© discutido ou mencionado*
[^4]: *Trechos do texto onde o conceito √© discutido ou mencionado*
<!-- END -->
