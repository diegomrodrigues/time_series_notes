## Proje√ß√£o Linear e Regress√£o OLS: Uma Perspectiva Comparativa
### Introdu√ß√£o
Este cap√≠tulo visa consolidar a compreens√£o sobre a **proje√ß√£o linear** e a **regress√£o de m√≠nimos quadrados ordin√°rios (OLS)**, enfatizando sua inter-rela√ß√£o e as nuances de suas aplica√ß√µes. Conforme discutido em cap√≠tulos anteriores [^4], a proje√ß√£o linear aborda a modelagem de rela√ß√µes entre vari√°veis aleat√≥rias utilizando momentos populacionais, enquanto a regress√£o OLS estima essa rela√ß√£o com base em dados amostrais, utilizando momentos amostrais [^4, 4.1.17]. Este cap√≠tulo explora essa rela√ß√£o mais a fundo, destacando como a **regress√£o OLS** pode ser interpretada como uma realiza√ß√£o emp√≠rica da proje√ß√£o linear, na qual os momentos populacionais s√£o substitu√≠dos por seus equivalentes amostrais [^4, 4.1.19].

### A Rela√ß√£o entre Proje√ß√£o Linear e Regress√£o OLS
Como vimos anteriormente, a **proje√ß√£o linear** de $Y_{t+1}$ sobre $X_t$ busca determinar o melhor preditor linear de $Y_{t+1}$ dado $X_t$, onde o vetor de coeficientes $\alpha$ √© calculado a partir de momentos populacionais: $\alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}$ [^4, 4.1.13]. Este vetor $\alpha$ captura uma rela√ß√£o *verdadeira* e te√≥rica entre as vari√°veis, v√°lida para toda a popula√ß√£o [^4, 4.1.20]. Por outro lado, a **regress√£o OLS** estima um vetor de coeficientes $b$ usando dados amostrais de $Y_{t+1}$ e $X_t$, minimizando a soma dos quadrados dos res√≠duos:
$$
b = \left(\sum_{t=1}^T x_t x_t'\right)^{-1} \left(\sum_{t=1}^T x_t y_{t+1}\right).
$$
Esta abordagem, conforme discutimos anteriormente, envolve opera√ß√µes de **invers√£o de matriz** e **multiplica√ß√£o** que aplicamos aos dados amostrais [^4, 4.1.18].

> üí° **Exemplo Num√©rico:** Suponha que queremos analisar a rela√ß√£o entre o investimento em publicidade ($X_t$) e o volume de vendas ($Y_{t+1}$). Na **proje√ß√£o linear**, n√≥s usar√≠amos $E(Y_{t+1}X_t)$ e $E(X_tX_t')$ para determinar os coeficientes ideais para *toda a popula√ß√£o*. Por exemplo, suponha que $E(Y_{t+1}X_t) = 100$ e $E(X_tX_t') = 25$, ent√£o o coeficiente $\alpha$ seria $\alpha = 100 / 25 = 4$, levando √† proje√ß√£o $Y_{t+1} = 4 X_t$. J√° na **regress√£o OLS**, usar√≠amos os dados amostrais. Suponha que temos as seguintes observa√ß√µes: $(x_1,y_2) = (5, 19)$, $(x_2,y_3) = (10, 38)$, e $(x_3,y_4) = (15, 62)$. Para calcular $b$, fazemos os c√°lculos necess√°rios:
>
> $$
> X = \begin{bmatrix} 5 \\ 10 \\ 15 \end{bmatrix}; Y = \begin{bmatrix} 19 \\ 38 \\ 62 \end{bmatrix}
> $$
> $$
> X'X = \begin{bmatrix} 5 & 10 & 15 \end{bmatrix} \begin{bmatrix} 5 \\ 10 \\ 15 \end{bmatrix} = 25 + 100 + 225 = 350
> $$
> $$
> X'Y = \begin{bmatrix} 5 & 10 & 15 \end{bmatrix} \begin{bmatrix} 19 \\ 38 \\ 62 \end{bmatrix} = 95 + 380 + 930 = 1405
> $$
> $$
> b = (X'X)^{-1} X'Y = \frac{1}{350} \times 1405 \approx 4.014
> $$
> A equa√ß√£o de regress√£o OLS estimada seria $\hat{Y}_{t+1} = 4.014 X_t$.
>
> A proje√ß√£o linear usa momentos populacionais, enquanto a regress√£o OLS usa momentos amostrais para estimar a rela√ß√£o entre as vari√°veis. A diferen√ßa nas respostas num√©ricas ilustra que a OLS est√° focada em resumir a rela√ß√£o em uma amostra, e a proje√ß√£o linear est√° focada na rela√ß√£o da popula√ß√£o.
>
> ```python
> import numpy as np
>
> X = np.array([[5], [10], [15]])
> Y = np.array([[19], [38], [62]])
>
> X_transpose = X.T
> XTX = np.dot(X_transpose, X)
> XTY = np.dot(X_transpose, Y)
>
> XTX_inv = np.linalg.inv(XTX)
> beta = np.dot(XTX_inv, XTY)
>
> print("Estimated Beta:", beta)
> ```
>
>
> üí° **Exemplo Num√©rico (Residual Analysis):** Continuando o exemplo anterior, podemos analisar os res√≠duos para avaliar a qualidade do ajuste da regress√£o OLS. Os res√≠duos s√£o as diferen√ßas entre os valores observados e os valores previstos ($\hat{Y}_{t+1} = 4.014X_t$).
>
> Para nossa amostra:
>
> $\hat{y}_2 = 4.014 \times 5 = 20.07$
>
> $\hat{y}_3 = 4.014 \times 10 = 40.14$
>
> $\hat{y}_4 = 4.014 \times 15 = 60.21$
>
> Os res√≠duos ($e_t = y_{t+1} - \hat{y}_{t+1}$) s√£o:
>
> $e_1 = 19 - 20.07 = -1.07$
>
> $e_2 = 38 - 40.14 = -2.14$
>
> $e_3 = 62 - 60.21 = 1.79$
>
> A soma dos quadrados dos res√≠duos √© $(-1.07)^2 + (-2.14)^2 + (1.79)^2 \approx 7.85$. Idealmente, os res√≠duos devem ser aleat√≥rios e centrados em zero. Aqui, a an√°lise de res√≠duos permite avaliar a adequa√ß√£o do modelo. A magnitude dos res√≠duos nos da uma ideia de como o modelo captura a variabilidade dos dados.
> ```python
> import numpy as np
>
> X = np.array([[5], [10], [15]])
> Y = np.array([[19], [38], [62]])
>
> X_transpose = X.T
> XTX = np.dot(X_transpose, X)
> XTY = np.dot(X_transpose, Y)
>
> XTX_inv = np.linalg.inv(XTX)
> beta = np.dot(XTX_inv, XTY)
>
> y_hat = beta * X
> residuals = Y - y_hat
>
> print("Estimated Beta:", beta)
> print("Residuals:", residuals)
>
> sse = np.sum(residuals**2)
> print("Sum of Squared Errors:", sse)
> ```

A liga√ß√£o entre as duas abordagens reside na observa√ß√£o de que, quando o tamanho da amostra $T$ tende ao infinito, sob condi√ß√µes de estacionaridade e ergodicidade, os momentos amostrais convergem para seus correspondentes momentos populacionais [^4, 4.1.20]. Em outras palavras, √† medida que aumentamos o n√∫mero de observa√ß√µes, a regress√£o OLS, baseada em momentos amostrais, se torna uma aproxima√ß√£o cada vez mais precisa da proje√ß√£o linear, que se baseia em momentos populacionais.

**Lema 1** (Converg√™ncia das M√©dias Amostrais) Dado um processo estacion√°rio e erg√≥dico $\{X_t\}$, a m√©dia amostral $\frac{1}{T} \sum_{t=1}^T X_t$ converge em probabilidade para a m√©dia populacional $E[X_t]$ quando $T$ tende ao infinito, ou seja, $\frac{1}{T} \sum_{t=1}^T X_t \xrightarrow{p} E[X_t]$.
*Prova*:
I. Seja $\{X_t\}$ um processo estacion√°rio e erg√≥dico.
II. Por defini√ß√£o, um processo estacion√°rio tem m√©dia constante ao longo do tempo, ou seja, $E[X_t] = \mu$ para todo $t$.
III. Por defini√ß√£o, um processo erg√≥dico tem a propriedade que as m√©dias amostrais convergem para a m√©dia populacional quando $T \to \infty$. Portanto,
$\frac{1}{T} \sum_{t=1}^T X_t \xrightarrow{p} \mu = E[X_t]$.  $\blacksquare$
Esse lema e seus resultados, como discutido anteriormente, fundamentam a rela√ß√£o entre momentos populacionais e amostrais [^4].

**Lema 1.1** (Converg√™ncia dos Momentos Amostrais): Dado um processo estacion√°rio e erg√≥dico $\{X_t\}$, o momento amostral $\frac{1}{T} \sum_{t=1}^T X_t X_t'$ converge em probabilidade para o momento populacional $E[X_t X_t']$ quando $T$ tende ao infinito, ou seja, $\frac{1}{T} \sum_{t=1}^T X_t X_t' \xrightarrow{p} E[X_t X_t']$.
*Prova*:
I. Seja $\{X_t\}$ um processo estacion√°rio e erg√≥dico.
II.  Pela defini√ß√£o de estacionaridade, os momentos de segunda ordem $E[X_t X_t']$ s√£o constantes ao longo do tempo.
III. Pela ergodicidade, as m√©dias amostrais convergem para as m√©dias populacionais, ou seja, $\frac{1}{T} \sum_{t=1}^T (X_t X_t') \xrightarrow{p} E[X_t X_t']$. $\blacksquare$

A converg√™ncia dos momentos amostrais para os momentos populacionais implica que, em amostras suficientemente grandes, o estimador OLS $b$ se aproxima do coeficiente da proje√ß√£o linear $\alpha$, ou seja, $b \approx \alpha$. Formalmente, podemos mostrar que o estimador OLS $b$ √© uma estimativa consistente do par√¢metro populacional $\alpha$, quando o processo de dados √© estacion√°rio e erg√≥dico, conforme a proposi√ß√£o a seguir:

**Proposi√ß√£o 1** (Consist√™ncia do Estimador OLS): Se o processo $(X_t, Y_{t+1})$ √© estacion√°rio e erg√≥dico, e se a matriz $E(X_tX_t')$ √© n√£o-singular, ent√£o o estimador OLS $b = \left(\sum_{t=1}^T x_t x_t'\right)^{-1} \left(\sum_{t=1}^T x_t y_{t+1}\right)$ converge em probabilidade para o coeficiente da proje√ß√£o linear $\alpha = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}$ quando $T$ tende ao infinito.
*Prova*:
I. Seja $(X_t, Y_{t+1})$ um processo estacion√°rio e erg√≥dico.
II. Pelo Lema 1 (Converg√™ncia das M√©dias Amostrais) e o Lema 1.1 (Converg√™ncia dos Momentos Amostrais) e a propriedade de ergodicidade, temos que:
$\frac{1}{T} \sum_{t=1}^T x_t x_t' \xrightarrow{p} E(X_t X_t')$ quando $T \to \infty$, e
$\frac{1}{T} \sum_{t=1}^T x_t y_{t+1} \xrightarrow{p} E(Y_{t+1} X_t')$ quando $T \to \infty$.
III. Podemos reescrever o estimador OLS $b$ como: $b = \left(\frac{1}{T}\sum_{t=1}^T x_t x_t'\right)^{-1} \left(\frac{1}{T}\sum_{t=1}^T x_t y_{t+1}\right)$.
IV. Dado que $E(X_t X_t')$ √© n√£o-singular (invert√≠vel), sua inversa existe, e pela continuidade da opera√ß√£o de invers√£o:
$\left(\frac{1}{T} \sum_{t=1}^T x_t x_t'\right)^{-1} \xrightarrow{p} \left(E(X_t X_t')\right)^{-1}$ quando $T \to \infty$.
V. Pela propriedade de converg√™ncia de produtos em probabilidade, se $A_T \xrightarrow{p} A$ e $B_T \xrightarrow{p} B$, ent√£o $A_T B_T \xrightarrow{p} AB$.  Aplicando essa propriedade, temos que:
$b \xrightarrow{p}  (E(X_t X_t'))^{-1} E(Y_{t+1} X_t') = \alpha$ quando $T \to \infty$.
VI. Portanto, o estimador OLS $b$ converge em probabilidade para o coeficiente da proje√ß√£o linear $\alpha$. $\blacksquare$

**Proposi√ß√£o 1.1** (Consist√™ncia do Estimador OLS para intercepto): Se o processo $(X_t, Y_{t+1})$ √© estacion√°rio e erg√≥dico, e se a matriz $E(Z_tZ_t')$ √© n√£o-singular, onde $Z_t = [1, X_t']'$, ent√£o o estimador OLS $\beta = \left(\sum_{t=1}^T z_t z_t'\right)^{-1} \left(\sum_{t=1}^T z_t y_{t+1}\right)$ converge em probabilidade para o coeficiente da proje√ß√£o linear $\gamma = E(Y_{t+1}Z_t') [E(Z_tZ_t')]^{-1}$ quando $T$ tende ao infinito, onde $z_t = [1, x_t']'$.
*Prova*:
I. Seja $(X_t, Y_{t+1})$ um processo estacion√°rio e erg√≥dico.
II. Defina $Z_t = [1, X_t']'$,  ent√£o  $\beta = \left(\sum_{t=1}^T z_t z_t'\right)^{-1} \left(\sum_{t=1}^T z_t y_{t+1}\right)$ e $\gamma = E(Y_{t+1}Z_t') [E(Z_tZ_t')]^{-1}$.
III. Pelo Lema 1 (Converg√™ncia das M√©dias Amostrais) e o Lema 1.1 (Converg√™ncia dos Momentos Amostrais), e pela propriedade da ergodicidade, segue que
$\frac{1}{T} \sum_{t=1}^T z_t z_t' \xrightarrow{p} E(Z_t Z_t')$ quando $T \to \infty$, e
$\frac{1}{T} \sum_{t=1}^T z_t y_{t+1} \xrightarrow{p} E(Y_{t+1} Z_t')$ quando $T \to \infty$.
IV. Podemos reescrever o estimador OLS $\beta$ como: $\beta = \left(\frac{1}{T}\sum_{t=1}^T z_t z_t'\right)^{-1} \left(\frac{1}{T}\sum_{t=1}^T z_t y_{t+1}\right)$.
V. Dado que $E(Z_t Z_t')$ √© n√£o-singular (invert√≠vel), sua inversa existe, e pela continuidade da opera√ß√£o de invers√£o:
$\left(\frac{1}{T} \sum_{t=1}^T z_t z_t'\right)^{-1} \xrightarrow{p} \left(E(Z_t Z_t')\right)^{-1}$ quando $T \to \infty$.
VI. Pela propriedade de converg√™ncia de produtos em probabilidade, se $A_T \xrightarrow{p} A$ e $B_T \xrightarrow{p} B$, ent√£o $A_T B_T \xrightarrow{p} AB$.  Aplicando essa propriedade, temos que:
$\beta \xrightarrow{p}  (E(Z_t Z_t'))^{-1} E(Y_{t+1} Z_t') = \gamma$ quando $T \to \infty$.
VII. Portanto, o estimador OLS $\beta$ converge em probabilidade para o coeficiente da proje√ß√£o linear $\gamma$. $\blacksquare$

> üí° **Exemplo Num√©rico (Consist√™ncia):** Para ilustrar a consist√™ncia do estimador OLS na pr√°tica, vamos simular dados para diferentes tamanhos de amostra, e observar o comportamento do estimador com base nas m√©dias populacionais. Seja a rela√ß√£o verdadeira $Y_{t+1} = 2X_t + \epsilon_t$, onde $\epsilon_t \sim N(0, 1)$ √© ru√≠do branco. Vamos gerar conjuntos de dados simulados para diferentes valores de $T$ (e.g., $T = 10, 100, 1000, 10000$), calcular o estimador OLS para cada amostra, e verificar como ele se aproxima do valor verdadeiro ($\beta = 2$) com o aumento de $T$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def generate_data(T, true_beta=2, std_dev=1, x_mean = 50, x_std = 20):
>    X = np.random.normal(x_mean, x_std, T).reshape(-1, 1)
>    epsilon = np.random.normal(0, std_dev, T).reshape(-1, 1)
>    Y = true_beta * X + epsilon
>    return X, Y
>
> def estimate_ols(X, Y):
>    X_transpose = X.T
>    XTX = np.dot(X_transpose, X)
>    XTY = np.dot(X_transpose, Y)
>    XTX_inv = np.linalg.inv(XTX)
>    beta = np.dot(XTX_inv, XTY)
>    return beta
>
> sample_sizes = [10, 100, 1000, 10000]
> estimated_betas = []
>
> for T in sample_sizes:
>    X, Y = generate_data(T)
>    beta_hat = estimate_ols(X, Y)
>    estimated_betas.append(beta_hat[0][0])
>
> plt.figure(figsize=(8, 6))
> plt.plot(sample_sizes, estimated_betas, marker='o')
> plt.axhline(y=2, color='r', linestyle='--', label='Beta Verdadeiro')
> plt.xlabel('Tamanho da Amostra (T)')
> plt.ylabel('Estimativa do Beta')
> plt.title('Converg√™ncia do Estimador OLS')
> plt.legend()
> plt.grid(True)
> plt.show()
>
> print(f"Estimativa para T=10: {estimated_betas[0]}")
> print(f"Estimativa para T=100: {estimated_betas[1]}")
> print(f"Estimativa para T=1000: {estimated_betas[2]}")
> print(f"Estimativa para T=10000: {estimated_betas[3]}")
> ```
> Os resultados demonstram como a estimativa da regress√£o OLS se aproxima do valor verdadeiro do par√¢metro √† medida que o tamanho da amostra aumenta, ilustrando a ideia de que, sob condi√ß√µes de ergodicidade, o estimador OLS se torna uma aproxima√ß√£o cada vez melhor da proje√ß√£o linear.
>
> üí° **Exemplo Num√©rico (Intervalo de Confian√ßa):** Vamos gerar dados similares ao exemplo anterior, mas com T=100 para estimar o intervalo de confian√ßa do estimador. Vamos simular 1000 amostras, calcular o estimador OLS para cada amostra, e calcular o intervalo de confian√ßa de 95%.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy import stats
>
> def generate_data(T, true_beta=2, std_dev=1, x_mean = 50, x_std = 20):
>    X = np.random.normal(x_mean, x_std, T).reshape(-1, 1)
>    epsilon = np.random.normal(0, std_dev, T).reshape(-1, 1)
>    Y = true_beta * X + epsilon
>    return X, Y
>
> def estimate_ols(X, Y):
>    X_transpose = X.T
>    XTX = np.dot(X_transpose, X)
>    XTY = np.dot(X_transpose, Y)
>    XTX_inv = np.linalg.inv(XTX)
>    beta = np.dot(XTX_inv, XTY)
>    return beta[0][0]
>
> num_simulations = 1000
> T = 100
> estimated_betas = []
>
> for _ in range(num_simulations):
>    X, Y = generate_data(T)
>    beta_hat = estimate_ols(X, Y)
>    estimated_betas.append(beta_hat)
>
> mean_beta = np.mean(estimated_betas)
> std_err_beta = np.std(estimated_betas, ddof=1) / np.sqrt(num_simulations)
> confidence_interval = stats.t.interval(0.95, num_simulations-1, loc=mean_beta, scale=std_err_beta)
>
> plt.figure(figsize=(8,6))
> plt.hist(estimated_betas, bins=30, density=True, alpha=0.6, color='g', label='Distribui√ß√£o das Estimativas')
> plt.axvline(x=mean_beta, color='r', linestyle='-', label=f'M√©dia: {mean_beta:.2f}')
> plt.axvline(x=confidence_interval[0], color='b', linestyle='--', label=f'Limite Inferior: {confidence_interval[0]:.2f}')
> plt.axvline(x=confidence_interval[1], color='b', linestyle='--', label=f'Limite Superior: {confidence_interval[1]:.2f}')
> plt.axvline(x=2, color='black', linestyle='-.', label=f'Beta Verdadeiro: 2')
> plt.xlabel('Estimativa do Beta')
> plt.ylabel('Densidade')
> plt.title('Distribui√ß√£o das Estimativas OLS e Intervalo de Confian√ßa')
> plt.legend()
> plt.show()
>
> print(f"M√©dia do Beta Estimado: {mean_beta}")
> print(f"Intervalo de Confian√ßa de 95%: {confidence_interval}")
> ```
> O intervalo de confian√ßa nos d√° uma ideia da precis√£o de nossa estimativa. O gr√°fico tamb√©m mostra a distribui√ß√£o das estimativas de $\beta$ para diferentes amostras.

Apesar da equival√™ncia assint√≥tica, √© importante destacar que em amostras finitas, a regress√£o OLS gera estimativas que s√£o espec√≠ficas para a amostra observada. Isso implica que o estimador OLS $b$ pode variar de amostra para amostra, e n√£o representa necessariamente a rela√ß√£o *verdadeira* na popula√ß√£o. Por sua vez, a proje√ß√£o linear, utilizando momentos populacionais, representa uma rela√ß√£o *verdadeira* e te√≥rica, embora em geral desconhecida na pr√°tica.

### Conclus√£o
Em s√≠ntese, a **proje√ß√£o linear** e a **regress√£o OLS** compartilham uma base matem√°tica comum, mas diferem em sua interpreta√ß√£o e aplica√ß√£o. A proje√ß√£o linear busca descrever rela√ß√µes populacionais, enquanto a regress√£o OLS resume rela√ß√µes amostrais. No entanto, sob condi√ß√µes de estacionaridade e ergodicidade, a regress√£o OLS se torna uma aproxima√ß√£o consistente da proje√ß√£o linear quando o tamanho da amostra aumenta, o que demonstra como os dados amostrais podem ser usados para estimar par√¢metros populacionais. Essa dualidade, junto com as diferen√ßas nas aplica√ß√µes, ilustra a import√¢ncia de entender quando usar cada m√©todo e como interpretar os resultados.

### Refer√™ncias
[^1]:
[^2]:
[^3]: *Trechos do texto onde o conceito √© discutido ou mencionado*
[^4]: *Trechos do texto onde o conceito √© discutido ou mencionado*
<!-- END -->
