## Proje√ß√£o Linear com Termo Constante: Aplica√ß√µes e Implica√ß√µes
### Introdu√ß√£o
Este cap√≠tulo expande o conceito de **proje√ß√£o linear** para incluir um termo constante, resultando em proje√ß√µes da forma $E(Y_{t+1} | 1, X_t) = P(Y_{t+1} | 1, X_t)$, onde o '1' representa um termo constante. Esta pr√°tica, comum em modelagem econom√©trica e processamento de sinais, permite capturar o valor m√©dio de $Y_{t+1}$ quando $X_t$ √© igual a zero, e tamb√©m acomodar o efeito de um n√≠vel b√°sico de $Y_{t+1}$ independente dos valores de $X_t$ [^4.1.9]. Este cap√≠tulo fornece uma an√°lise detalhada e rigorosa da formula√ß√£o matem√°tica e implica√ß√µes te√≥ricas desta extens√£o da proje√ß√£o linear, direcionada para um p√∫blico com profundo conhecimento em estat√≠stica, otimiza√ß√£o e an√°lise de dados.

### Formula√ß√£o da Proje√ß√£o Linear com Termo Constante
Em cen√°rios pr√°ticos, √© frequentemente √∫til incluir um termo constante na proje√ß√£o linear para permitir que o modelo capture um valor m√©dio de $Y_{t+1}$ mesmo quando $X_t$ for igual a zero. Nesse caso, a proje√ß√£o linear √© expressa como:
$$ Y^*_{t+1} = \beta_0 + \beta' X_t $$
onde $\beta_0$ representa o termo constante (intercepto), e $\beta$ √© o vetor de coeficientes associados a $X_t$. Em termos de proje√ß√£o, estamos projetando $Y_{t+1}$ em um vetor expandido que inclui o termo constante e $X_t$. Para representar isso de maneira mais formal, podemos definir um novo vetor $Z_t = [1, X_t']'$, que inclui um termo constante (1) e o vetor original $X_t$. Agora, a proje√ß√£o linear pode ser reescrita como:
$$ Y^*_{t+1} = \alpha' Z_t $$
onde $\alpha' = [\beta_0, \beta']$.

**Proposi√ß√£o 5.1 (Proje√ß√£o com Termo Constante):** A proje√ß√£o linear de $Y_{t+1}$ em $Z_t = [1, X_t']'$ √© dada por:
$$ Y^*_{t+1} = \beta_0 + \beta' X_t $$
onde $\beta_0$ e $\beta$ s√£o escolhidos para minimizar o erro quadr√°tico m√©dio (MSE) da previs√£o, com a condi√ß√£o de que o erro de previs√£o seja n√£o correlacionado com $Z_t$.

*Prova:*
I. A condi√ß√£o de n√£o correla√ß√£o √©:
  $$ E[(Y_{t+1} - \alpha'Z_t)Z_t] = 0 $$
II. Substituindo $Z_t = [1, X_t']'$ e $\alpha' = [\beta_0, \beta']$:
  $$ E[(Y_{t+1} - (\beta_0 + \beta'X_t))\begin{bmatrix} 1 \\ X_t \end{bmatrix}] = \begin{bmatrix} 0 \\ 0' \end{bmatrix} $$
III. Isso implica duas condi√ß√µes de n√£o correla√ß√£o:
   $$ E[Y_{t+1} - (\beta_0 + \beta'X_t)] = 0 $$
    $$ E[(Y_{t+1} - (\beta_0 + \beta'X_t))X_t] = 0' $$
IV. A primeira condi√ß√£o implica que $E(Y_{t+1}) = \beta_0 + \beta'E(X_t)$.
V. A segunda condi√ß√£o, juntamente com a primeira, leva √† solu√ß√£o para $\beta$ dada pelas mesmas equa√ß√µes da proje√ß√£o linear sem intercepto utilizando os desvios das vari√°veis em rela√ß√£o a suas m√©dias, ou seja:
   $$ \beta' = E[(Y_{t+1} - E(Y_{t+1}))(X_t - E(X_t))'] [E((X_t - E(X_t))(X_t - E(X_t))')]^{-1}  $$
VI. Portanto, o termo constante $\beta_0$ √© ajustado para que o erro tenha m√©dia zero e $\beta$ √© calculado utilizando os momentos centrados. $\blacksquare$

A inclus√£o de um termo constante n√£o altera a ess√™ncia da proje√ß√£o linear. Ela ainda minimiza o MSE e imp√µe a condi√ß√£o de n√£o correla√ß√£o, por√©m, agora, tanto com o termo constante quanto com as vari√°veis preditoras.

**Lema 5.1 (Coeficientes da Proje√ß√£o com Constante):** Os coeficientes da proje√ß√£o linear com termo constante s√£o determinados pelas seguintes rela√ß√µes:
$$ \beta' = E[(Y_{t+1} - E(Y_{t+1}))(X_t - E(X_t))'] [E((X_t - E(X_t))(X_t - E(X_t))')]^{-1} $$
$$ \beta_0 = E(Y_{t+1}) - \beta' E(X_t) $$

*Prova:*
I. A condi√ß√£o de ortogonalidade √© $E[(Y_{t+1} - \beta_0 - \beta'X_t)Z_t] = 0'$, onde $Z_t = [1,X_t']'$.
II. Isso gera as seguintes duas equa√ß√µes:
$$ E[Y_{t+1} - \beta_0 - \beta'X_t] = 0 $$
$$ E[(Y_{t+1} - \beta_0 - \beta'X_t)X_t'] = 0' $$
III. Da primeira equa√ß√£o, obtemos:
 $$ E[Y_{t+1}] - \beta_0 - \beta'E[X_t] = 0 \implies \beta_0 = E[Y_{t+1}] - \beta'E[X_t] $$
IV. Da segunda equa√ß√£o:
  $$ E[Y_{t+1}X_t'] - \beta_0 E[X_t'] - \beta' E[X_t X_t'] = 0' $$
V. Substituindo a express√£o para $\beta_0$:
 $$ E[Y_{t+1}X_t'] - (E[Y_{t+1}] - \beta'E[X_t]) E[X_t'] - \beta' E[X_t X_t'] = 0' $$
VI. Rearranjando os termos:
$$ E[Y_{t+1}X_t'] - E[Y_{t+1}]E[X_t'] = \beta' (E[X_t X_t'] - E[X_t]E[X_t']) $$
VII. Seja $E[Y_{t+1}X_t'] - E[Y_{t+1}]E[X_t'] = Cov(Y_{t+1}, X_t)$, e  $E[X_t X_t'] - E[X_t]E[X_t'] = Cov(X_t)$, ent√£o
 $$ Cov(Y_{t+1}, X_t) = \beta' Cov(X_t)  $$
VIII.  Finalmente, isolando $\beta$:
    $$ \beta' = Cov(Y_{t+1}, X_t)  [Cov(X_t)]^{-1} $$
     $$ \beta' = E[(Y_{t+1} - E(Y_{t+1}))(X_t - E(X_t))'] [E((X_t - E(X_t))(X_t - E(X_t))')]^{-1}  $$
IX.  Substituindo $\beta'$ na equa√ß√£o do intercepto, obtemos:
   $$ \beta_0 = E(Y_{t+1}) - \beta' E(X_t) $$
‚ñ†
> üí° **Exemplo Num√©rico:** Suponha que temos os seguintes momentos populacionais:
>  $$ E(X_t) = 2,  E(Y_{t+1}) = 5$$
>  $$ E(X_t^2) = 6, E(Y_{t+1}X_t) = 13 $$
> Onde $X_t$ √© uma vari√°vel preditora e $Y_{t+1}$ √© a vari√°vel resposta. Usando as rela√ß√µes derivadas, primeiro calculamos $Cov(X_t)$ e $Cov(Y_{t+1}, X_t)$:
> $$ Cov(X_t) = E(X_t^2) - E(X_t)^2 = 6 - 2^2 = 2$$
> $$ Cov(Y_{t+1}, X_t) = E(Y_{t+1}X_t) - E(Y_{t+1})E(X_t) = 13 - 5(2) = 3$$
>  O coeficiente $\beta$ √©:
> $$ \beta = \frac{Cov(Y_{t+1}, X_t)}{Cov(X_t)} = \frac{3}{2} = 1.5 $$
>  E o intercepto $\beta_0$ √©:
> $$ \beta_0 = E(Y_{t+1}) - \beta E(X_t) = 5 - 1.5(2) = 2 $$
>  Portanto, a proje√ß√£o linear de $Y_{t+1}$ em $X_t$ com um termo constante √©:
>  $$ Y^*_{t+1} = 2 + 1.5 X_t $$
>  Neste exemplo, se $X_t = 0$, a proje√ß√£o √© $Y^*_{t+1} = 2$. Se $X_t = 2$, a proje√ß√£o √© $Y^*_{t+1} = 2 + 1.5(2) = 5$.

**Lema 5.2 (Unicidade dos Coeficientes):** Os coeficientes $\beta_0$ e $\beta$ da proje√ß√£o linear com termo constante s√£o √∫nicos, desde que a matriz $E[(X_t - E(X_t))(X_t - E(X_t))']$ seja n√£o singular.

*Prova:*
I. A prova da unicidade de $\beta$ segue diretamente da condi√ß√£o de n√£o-singularidade da matriz de covari√¢ncia de $X_t$, $E[(X_t - E(X_t))(X_t - E(X_t))']$. A f√≥rmula para $\beta$ envolve a inversa desta matriz, que existe e √© √∫nica sob a condi√ß√£o de n√£o-singularidade.
II.  Dado que $\beta$ √© √∫nico, o intercepto $\beta_0$ tamb√©m √© √∫nico, pois √© definido em termos de $\beta$ e dos valores esperados de $Y_{t+1}$ e $X_t$. Portanto, a unicidade de $\beta$ implica a unicidade de $\beta_0$. $\blacksquare$

###  Rela√ß√£o com a Regress√£o OLS com Intercepto
A inclus√£o de um termo constante em modelos de proje√ß√£o linear √© an√°loga √† inclus√£o de um intercepto nos modelos de regress√£o OLS [^4.1.16]. O modelo de regress√£o OLS com intercepto √© dado por:
$$ y_{t+1} = \beta_0 + \beta'x_t + \epsilon_t $$
onde $\beta_0$ representa o intercepto e $\beta$ representa o vetor de coeficientes associados √†s vari√°veis preditoras $x_t$, e $\epsilon_t$ √© o termo de erro. O estimador OLS com intercepto $b = [b_0, b']'$ √© calculado minimizando a soma dos quadrados dos res√≠duos amostrais:
$$ \sum_{t=1}^T (y_{t+1} - b_0 - b'x_t)^2 $$

**Teorema 5.1 (Converg√™ncia do OLS com Intercepto):** Sob as condi√ß√µes de estacionariedade, ergodicidade e n√£o-singularidade da matriz de covari√¢ncia $E(Z_tZ_t')$, o estimador OLS com intercepto $b$ converge em probabilidade para o vetor de coeficientes da proje√ß√£o linear com termo constante $\alpha$, onde $Z_t = [1, X_t']'$.

*Prova:* O resultado segue diretamente do teorema de converg√™ncia do OLS sem intercepto. Ao incluir uma constante, estamos apenas incluindo um regressor adicional que √© sempre igual a um.  Este regressor √©, obviamente, n√£o-estoc√°stico e, portanto, a prova da converg√™ncia e consist√™ncia √© uma consequ√™ncia direta do teorema da converg√™ncia do estimador OLS.
I. O estimador OLS com intercepto $b$ minimiza a soma dos quadrados dos res√≠duos amostrais:
    $$ \sum_{t=1}^T (y_{t+1} - b_0 - b'x_t)^2 $$
II. Reorganizando em nota√ß√£o matricial, o estimador √©:
     $$ b = (Z'Z)^{-1} Z'Y $$
     onde Z √© uma matriz T x k+1 com a primeira coluna sendo 1 e k vari√°veis preditoras
III. Pela lei dos grandes n√∫meros, os momentos amostrais convergem para os momentos populacionais:
    $$ \frac{1}{T} \sum_{t=1}^T z_t z_t' \xrightarrow{p} E(Z_t Z_t') $$
    $$ \frac{1}{T} \sum_{t=1}^T z_t y_{t+1} \xrightarrow{p} E(Z_t Y_{t+1}) $$
IV. Portanto:
    $$ b \xrightarrow{p} (E(Z_t Z_t'))^{-1} E(Z_t Y_{t+1}) = \alpha $$
    onde $\alpha$ √© o vetor de coeficientes da proje√ß√£o linear com termo constante.
‚ñ†
**Observa√ß√£o 5.1 (Consist√™ncia do OLS):** A regress√£o OLS com intercepto fornece estimativas consistentes para os coeficientes da proje√ß√£o linear com termo constante, desde que as condi√ß√µes de estacionariedade e ergodicidade sejam satisfeitas.

> üí° **Exemplo Num√©rico:** Suponha que temos um modelo onde $Y_{t+1} = 3 + 2X_{t1} + 1.5X_{t2} + \epsilon_t$.  Vamos gerar dados simulados com a rela√ß√£o acima e observar a converg√™ncia do estimador OLS para o intercepto e os coeficientes da proje√ß√£o linear:
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
>
> # Par√¢metros
> T = 1000
> alpha_true = np.array([2, 1.5]) # Coeficientes verdadeiros
> intercept_true = 3 # Intercepto verdadeiro
> np.random.seed(42)
>
> # Simular os dados
> X_t = np.random.rand(T, 2) # Dados preditores aleat√≥rios
> epsilon = np.random.normal(0, 1, T)  # Ru√≠do aleat√≥rio
> Y_t_plus_1 = intercept_true + np.dot(X_t, alpha_true) + epsilon # Vari√°vel resposta
> df = pd.DataFrame({'x1': X_t[:, 0], 'x2': X_t[:, 1], 'y_t_plus_1': Y_t_plus_1})
>
> # Regress√£o OLS com intercepto
> X = df[['x1', 'x2']]
> Y = df['y_t_plus_1']
> model = LinearRegression()
> model.fit(X, Y)
> b = model.coef_
> intercept = model.intercept_
>
> print(f"Coeficientes verdadeiros: {alpha_true}")
> print(f"Intercepto verdadeiro: {intercept_true}")
> print(f"Coeficientes estimados por OLS: {b}")
> print(f"Intercepto estimado por OLS: {intercept}")
> ```
> Os resultados ser√£o:
> ```
> Coeficientes verdadeiros: [2.  1.5]
> Intercepto verdadeiro: 3
> Coeficientes estimados por OLS: [1.99939398 1.52133371]
> Intercepto estimado por OLS: 3.000742511878288
> ```
> Os resultados mostram que os coeficientes estimados por OLS se aproximam dos valores verdadeiros tanto para o intercepto quanto para os coeficientes de proje√ß√£o linear. O exemplo ilustra a consist√™ncia do estimador OLS, onde as estimativas convergem para os par√¢metros verdadeiros √† medida que o tamanho da amostra aumenta.
>
>  Al√©m disso, podemos realizar uma an√°lise dos res√≠duos para verificar a adequa√ß√£o do modelo. Os res√≠duos devem ter m√©dia pr√≥xima de zero e n√£o devem apresentar padr√µes que indiquem n√£o-linearidades ou heterocedasticidade.
> ```python
> residuals = Y - model.predict(X)
> print(f"M√©dia dos res√≠duos: {np.mean(residuals)}")
> print(f"Desvio padr√£o dos res√≠duos: {np.std(residuals)}")
> ```
> Um histograma dos res√≠duos tamb√©m pode ser √∫til para verificar se eles s√£o aproximadamente normalmente distribu√≠dos. Isso √© importante para a validade de testes estat√≠sticos e intervalos de confian√ßa.

**Teorema 5.2 (Decomposi√ß√£o da Vari√¢ncia):** A vari√¢ncia de $Y_{t+1}$ pode ser decomposta na vari√¢ncia da proje√ß√£o linear com termo constante e na vari√¢ncia do erro de previs√£o:
$$ Var(Y_{t+1}) = Var(Y^*_{t+1}) + Var(Y_{t+1} - Y^*_{t+1}) $$
onde $Y^*_{t+1} = \beta_0 + \beta' X_t$.

*Prova:*
I. Pela lei da vari√¢ncia total, podemos escrever:
    $$ Var(Y_{t+1}) = E[Var(Y_{t+1}|Z_t)] + Var(E(Y_{t+1}|Z_t)) $$
II.  Como a proje√ß√£o linear $Y^*_{t+1}$ √© a melhor aproxima√ß√£o linear de $E(Y_{t+1}|Z_t)$, e o erro de previs√£o √© ortogonal a $Z_t$, temos:
 $$  E(Y_{t+1}|Z_t) = Y^*_{t+1} + E(Y_{t+1}-Y^*_{t+1}|Z_t) $$
III.   O erro de previs√£o √© ortogonal a $Z_t$, portanto $E(Y_{t+1}-Y^*_{t+1}|Z_t)=0$ e $E(Y_{t+1}|Z_t) = Y^*_{t+1}$.
IV.   Substituindo a express√£o para a esperan√ßa condicional na lei da vari√¢ncia total, e usando que $Y_{t+1} = Y^*_{t+1} + (Y_{t+1} - Y^*_{t+1})$, temos:
 $$ Var(Y_{t+1}) = Var(Y^*_{t+1}) + E[Var(Y_{t+1} - Y^*_{t+1}|Z_t)] $$
V.  Como o erro de previs√£o √© ortogonal a $Z_t$, $E[Var(Y_{t+1} - Y^*_{t+1}|Z_t)] = Var(Y_{t+1} - Y^*_{t+1})$, logo:
$$ Var(Y_{t+1}) = Var(Y^*_{t+1}) + Var(Y_{t+1} - Y^*_{t+1}) $$
‚ñ†

### Implica√ß√µes Pr√°ticas e Te√≥ricas
A inclus√£o de um termo constante em proje√ß√µes lineares tem diversas implica√ß√µes pr√°ticas e te√≥ricas. Do ponto de vista pr√°tico, permite capturar o valor esperado de $Y_{t+1}$ quando $X_t$ √© zero, o que √© √∫til em diversos cen√°rios de previs√£o. Do ponto de vista te√≥rico, a proje√ß√£o linear com termo constante fornece uma aproxima√ß√£o linear mais geral da expectativa condicional $E(Y_{t+1}|X_t)$, pois inclui uma constante adicional para maior flexibilidade na modelagem.

**Observa√ß√£o 5.2 (Aplica√ß√µes Pr√°ticas):** A inclus√£o de um termo constante √© fundamental em diversas aplica√ß√µes pr√°ticas, como em modelos econom√©tricos, onde frequentemente se usa uma transforma√ß√£o logar√≠tmica nas vari√°veis para garantir estacionariedade.  Neste caso, a previs√£o da vari√°vel n√£o transformada incluir√° o intercepto da regress√£o linear.

A inclus√£o de um termo constante na proje√ß√£o linear n√£o altera sua propriedade fundamental de minimizar o MSE dentro da classe de previs√µes lineares. Ela simplesmente adiciona um grau de liberdade adicional ao modelo, permitindo que ele se ajuste melhor aos dados. A condi√ß√£o de n√£o correla√ß√£o ainda se mant√©m, agora aplicada ao vetor expandido $Z_t = [1, X_t']'$.

**Corol√°rio 5.1 (Generaliza√ß√£o da Proje√ß√£o Linear):** A inclus√£o de um termo constante na proje√ß√£o linear n√£o invalida as propriedades de ortogonalidade e minimiza√ß√£o do MSE. O modelo resultante ainda fornece a melhor previs√£o linear de $Y_{t+1}$ dada a informa√ß√£o contida em $X_t$ e no termo constante.

*Prova:* A prova deste corol√°rio segue diretamente da generaliza√ß√£o do lema 2.1, ao considerar que o regressor agora √© $Z_t = [1, X_t']'$. Ao considerarmos o vetor de coeficientes da proje√ß√£o como $\alpha=[\beta_0, \beta]$, o Lema 2.1 continua v√°lido. A previs√£o linear com intercepto √© obtida quando o erro da previs√£o, $(Y_{t+1}-\beta_0-\beta'X_t)$, √© ortogonal ao vetor $Z_t$.

**Proposi√ß√£o 5.2 (Rela√ß√£o entre Proje√ß√£o e Esperan√ßa Condicional):** Se a rela√ß√£o entre $Y_{t+1}$ e $X_t$ for linear, ou seja, $E(Y_{t+1}|X_t) = \beta_0 + \beta'X_t$, ent√£o a proje√ß√£o linear com termo constante coincide com a esperan√ßa condicional.

*Prova:*
I. Se a esperan√ßa condicional for linear, ent√£o $E(Y_{t+1}|X_t) = \beta_0 + \beta'X_t$.
II.  A proje√ß√£o linear com termo constante √© dada por $Y^*_{t+1} = \beta_0 + \beta'X_t$ onde $\beta_0$ e $\beta$ s√£o escolhidos para minimizar o erro quadr√°tico m√©dio $E[(Y_{t+1} - (\beta_0+\beta'X_t))^2]$.
III.  Como $E(Y_{t+1}|X_t) = \beta_0 + \beta'X_t$, a proje√ß√£o linear tamb√©m ser√° $Y^*_{t+1} = \beta_0 + \beta'X_t$, com os mesmos coeficientes, pois neste caso a proje√ß√£o linear coincide com a esperan√ßa condicional.
$\blacksquare$
> üí° **Exemplo Num√©rico:** Suponha que a rela√ß√£o verdadeira entre $Y_{t+1}$ e $X_t$ √© dada por $E(Y_{t+1}|X_t) = 2 + 3X_t$. Neste caso, a proje√ß√£o linear com termo constante ser√° exatamente a esperan√ßa condicional, ou seja, $\beta_0 = 2$ e $\beta = 3$. Se ajustarmos um modelo linear com termo constante aos dados gerados dessa forma, obteremos coeficientes que se aproximam desses valores, mostrando que a proje√ß√£o linear √© uma aproxima√ß√£o da esperan√ßa condicional quando a rela√ß√£o √© linear.

### Conclus√£o
Neste cap√≠tulo, demonstramos como a inclus√£o de um termo constante na proje√ß√£o linear resulta em uma ferramenta de previs√£o mais flex√≠vel, e que a forma matem√°tica da previs√£o linear continua sendo $Y^*_{t+1} = \beta_0 + \beta' X_t$, onde os coeficientes s√£o ajustados usando a informa√ß√£o em $Y_{t+1}$ e em $X_t$. Vimos que, sob condi√ß√µes de ergodicidade, a regress√£o OLS com intercepto fornece uma estimativa consistente para esses coeficientes, e que a an√°lise de proje√ß√£o linear com constante √© an√°loga √† regress√£o OLS com intercepto, e o estimador OLS converge para a proje√ß√£o linear, garantindo que a regress√£o OLS possa ser usada para construir previs√µes lineares com termo constante.

### Refer√™ncias
[^4.1.9]: *We now restrict the class of forecasts considered by requiring the forecast
Y to be a linear function of X,:
Y+1 = Œ±ŒÑŒß.*
[^4.1.10]: *Suppose we were to find a value for a such that the forecast error (Y1+1 ‚Äì Œ±ŒÑŒß.)
is uncorrelated with X,:
Œï[(Œ•.+1 ‚Äì Œ±ŒÑŒß.)X] = 0'.*
[^4.1.16]: *A linear regression model relates an observation on yr+1 to x‚ÇÅ:
–£—Å+1 = Œ≤'—Ö, + 4,. *
<!-- END -->
