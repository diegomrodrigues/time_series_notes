## Projeção Linear com Termo Constante: Aplicações e Implicações
### Introdução
Este capítulo expande o conceito de **projeção linear** para incluir um termo constante, resultando em projeções da forma $E(Y_{t+1} | 1, X_t) = P(Y_{t+1} | 1, X_t)$, onde o '1' representa um termo constante. Esta prática, comum em modelagem econométrica e processamento de sinais, permite capturar o valor médio de $Y_{t+1}$ quando $X_t$ é igual a zero, e também acomodar o efeito de um nível básico de $Y_{t+1}$ independente dos valores de $X_t$ [^4.1.9]. Este capítulo fornece uma análise detalhada e rigorosa da formulação matemática e implicações teóricas desta extensão da projeção linear, direcionada para um público com profundo conhecimento em estatística, otimização e análise de dados.

### Formulação da Projeção Linear com Termo Constante
Em cenários práticos, é frequentemente útil incluir um termo constante na projeção linear para permitir que o modelo capture um valor médio de $Y_{t+1}$ mesmo quando $X_t$ for igual a zero. Nesse caso, a projeção linear é expressa como:
$$ Y^*_{t+1} = \beta_0 + \beta' X_t $$
onde $\beta_0$ representa o termo constante (intercepto), e $\beta$ é o vetor de coeficientes associados a $X_t$. Em termos de projeção, estamos projetando $Y_{t+1}$ em um vetor expandido que inclui o termo constante e $X_t$. Para representar isso de maneira mais formal, podemos definir um novo vetor $Z_t = [1, X_t']'$, que inclui um termo constante (1) e o vetor original $X_t$. Agora, a projeção linear pode ser reescrita como:
$$ Y^*_{t+1} = \alpha' Z_t $$
onde $\alpha' = [\beta_0, \beta']$.

**Proposição 5.1 (Projeção com Termo Constante):** A projeção linear de $Y_{t+1}$ em $Z_t = [1, X_t']'$ é dada por:
$$ Y^*_{t+1} = \beta_0 + \beta' X_t $$
onde $\beta_0$ e $\beta$ são escolhidos para minimizar o erro quadrático médio (MSE) da previsão, com a condição de que o erro de previsão seja não correlacionado com $Z_t$.

*Prova:*
I. A condição de não correlação é:
  $$ E[(Y_{t+1} - \alpha'Z_t)Z_t] = 0 $$
II. Substituindo $Z_t = [1, X_t']'$ e $\alpha' = [\beta_0, \beta']$:
  $$ E[(Y_{t+1} - (\beta_0 + \beta'X_t))\begin{bmatrix} 1 \\ X_t \end{bmatrix}] = \begin{bmatrix} 0 \\ 0' \end{bmatrix} $$
III. Isso implica duas condições de não correlação:
   $$ E[Y_{t+1} - (\beta_0 + \beta'X_t)] = 0 $$
    $$ E[(Y_{t+1} - (\beta_0 + \beta'X_t))X_t] = 0' $$
IV. A primeira condição implica que $E(Y_{t+1}) = \beta_0 + \beta'E(X_t)$.
V. A segunda condição, juntamente com a primeira, leva à solução para $\beta$ dada pelas mesmas equações da projeção linear sem intercepto utilizando os desvios das variáveis em relação a suas médias, ou seja:
   $$ \beta' = E[(Y_{t+1} - E(Y_{t+1}))(X_t - E(X_t))'] [E((X_t - E(X_t))(X_t - E(X_t))')]^{-1}  $$
VI. Portanto, o termo constante $\beta_0$ é ajustado para que o erro tenha média zero e $\beta$ é calculado utilizando os momentos centrados. $\blacksquare$

A inclusão de um termo constante não altera a essência da projeção linear. Ela ainda minimiza o MSE e impõe a condição de não correlação, porém, agora, tanto com o termo constante quanto com as variáveis preditoras.

**Lema 5.1 (Coeficientes da Projeção com Constante):** Os coeficientes da projeção linear com termo constante são determinados pelas seguintes relações:
$$ \beta' = E[(Y_{t+1} - E(Y_{t+1}))(X_t - E(X_t))'] [E((X_t - E(X_t))(X_t - E(X_t))')]^{-1} $$
$$ \beta_0 = E(Y_{t+1}) - \beta' E(X_t) $$

*Prova:*
I. A condição de ortogonalidade é $E[(Y_{t+1} - \beta_0 - \beta'X_t)Z_t] = 0'$, onde $Z_t = [1,X_t']'$.
II. Isso gera as seguintes duas equações:
$$ E[Y_{t+1} - \beta_0 - \beta'X_t] = 0 $$
$$ E[(Y_{t+1} - \beta_0 - \beta'X_t)X_t'] = 0' $$
III. Da primeira equação, obtemos:
 $$ E[Y_{t+1}] - \beta_0 - \beta'E[X_t] = 0 \implies \beta_0 = E[Y_{t+1}] - \beta'E[X_t] $$
IV. Da segunda equação:
  $$ E[Y_{t+1}X_t'] - \beta_0 E[X_t'] - \beta' E[X_t X_t'] = 0' $$
V. Substituindo a expressão para $\beta_0$:
 $$ E[Y_{t+1}X_t'] - (E[Y_{t+1}] - \beta'E[X_t]) E[X_t'] - \beta' E[X_t X_t'] = 0' $$
VI. Rearranjando os termos:
$$ E[Y_{t+1}X_t'] - E[Y_{t+1}]E[X_t'] = \beta' (E[X_t X_t'] - E[X_t]E[X_t']) $$
VII. Seja $E[Y_{t+1}X_t'] - E[Y_{t+1}]E[X_t'] = Cov(Y_{t+1}, X_t)$, e  $E[X_t X_t'] - E[X_t]E[X_t'] = Cov(X_t)$, então
 $$ Cov(Y_{t+1}, X_t) = \beta' Cov(X_t)  $$
VIII.  Finalmente, isolando $\beta$:
    $$ \beta' = Cov(Y_{t+1}, X_t)  [Cov(X_t)]^{-1} $$
     $$ \beta' = E[(Y_{t+1} - E(Y_{t+1}))(X_t - E(X_t))'] [E((X_t - E(X_t))(X_t - E(X_t))')]^{-1}  $$
IX.  Substituindo $\beta'$ na equação do intercepto, obtemos:
   $$ \beta_0 = E(Y_{t+1}) - \beta' E(X_t) $$
■
> 💡 **Exemplo Numérico:** Suponha que temos os seguintes momentos populacionais:
>  $$ E(X_t) = 2,  E(Y_{t+1}) = 5$$
>  $$ E(X_t^2) = 6, E(Y_{t+1}X_t) = 13 $$
> Onde $X_t$ é uma variável preditora e $Y_{t+1}$ é a variável resposta. Usando as relações derivadas, primeiro calculamos $Cov(X_t)$ e $Cov(Y_{t+1}, X_t)$:
> $$ Cov(X_t) = E(X_t^2) - E(X_t)^2 = 6 - 2^2 = 2$$
> $$ Cov(Y_{t+1}, X_t) = E(Y_{t+1}X_t) - E(Y_{t+1})E(X_t) = 13 - 5(2) = 3$$
>  O coeficiente $\beta$ é:
> $$ \beta = \frac{Cov(Y_{t+1}, X_t)}{Cov(X_t)} = \frac{3}{2} = 1.5 $$
>  E o intercepto $\beta_0$ é:
> $$ \beta_0 = E(Y_{t+1}) - \beta E(X_t) = 5 - 1.5(2) = 2 $$
>  Portanto, a projeção linear de $Y_{t+1}$ em $X_t$ com um termo constante é:
>  $$ Y^*_{t+1} = 2 + 1.5 X_t $$
>  Neste exemplo, se $X_t = 0$, a projeção é $Y^*_{t+1} = 2$. Se $X_t = 2$, a projeção é $Y^*_{t+1} = 2 + 1.5(2) = 5$.

**Lema 5.2 (Unicidade dos Coeficientes):** Os coeficientes $\beta_0$ e $\beta$ da projeção linear com termo constante são únicos, desde que a matriz $E[(X_t - E(X_t))(X_t - E(X_t))']$ seja não singular.

*Prova:*
I. A prova da unicidade de $\beta$ segue diretamente da condição de não-singularidade da matriz de covariância de $X_t$, $E[(X_t - E(X_t))(X_t - E(X_t))']$. A fórmula para $\beta$ envolve a inversa desta matriz, que existe e é única sob a condição de não-singularidade.
II.  Dado que $\beta$ é único, o intercepto $\beta_0$ também é único, pois é definido em termos de $\beta$ e dos valores esperados de $Y_{t+1}$ e $X_t$. Portanto, a unicidade de $\beta$ implica a unicidade de $\beta_0$. $\blacksquare$

###  Relação com a Regressão OLS com Intercepto
A inclusão de um termo constante em modelos de projeção linear é análoga à inclusão de um intercepto nos modelos de regressão OLS [^4.1.16]. O modelo de regressão OLS com intercepto é dado por:
$$ y_{t+1} = \beta_0 + \beta'x_t + \epsilon_t $$
onde $\beta_0$ representa o intercepto e $\beta$ representa o vetor de coeficientes associados às variáveis preditoras $x_t$, e $\epsilon_t$ é o termo de erro. O estimador OLS com intercepto $b = [b_0, b']'$ é calculado minimizando a soma dos quadrados dos resíduos amostrais:
$$ \sum_{t=1}^T (y_{t+1} - b_0 - b'x_t)^2 $$

**Teorema 5.1 (Convergência do OLS com Intercepto):** Sob as condições de estacionariedade, ergodicidade e não-singularidade da matriz de covariância $E(Z_tZ_t')$, o estimador OLS com intercepto $b$ converge em probabilidade para o vetor de coeficientes da projeção linear com termo constante $\alpha$, onde $Z_t = [1, X_t']'$.

*Prova:* O resultado segue diretamente do teorema de convergência do OLS sem intercepto. Ao incluir uma constante, estamos apenas incluindo um regressor adicional que é sempre igual a um.  Este regressor é, obviamente, não-estocástico e, portanto, a prova da convergência e consistência é uma consequência direta do teorema da convergência do estimador OLS.
I. O estimador OLS com intercepto $b$ minimiza a soma dos quadrados dos resíduos amostrais:
    $$ \sum_{t=1}^T (y_{t+1} - b_0 - b'x_t)^2 $$
II. Reorganizando em notação matricial, o estimador é:
     $$ b = (Z'Z)^{-1} Z'Y $$
     onde Z é uma matriz T x k+1 com a primeira coluna sendo 1 e k variáveis preditoras
III. Pela lei dos grandes números, os momentos amostrais convergem para os momentos populacionais:
    $$ \frac{1}{T} \sum_{t=1}^T z_t z_t' \xrightarrow{p} E(Z_t Z_t') $$
    $$ \frac{1}{T} \sum_{t=1}^T z_t y_{t+1} \xrightarrow{p} E(Z_t Y_{t+1}) $$
IV. Portanto:
    $$ b \xrightarrow{p} (E(Z_t Z_t'))^{-1} E(Z_t Y_{t+1}) = \alpha $$
    onde $\alpha$ é o vetor de coeficientes da projeção linear com termo constante.
■
**Observação 5.1 (Consistência do OLS):** A regressão OLS com intercepto fornece estimativas consistentes para os coeficientes da projeção linear com termo constante, desde que as condições de estacionariedade e ergodicidade sejam satisfeitas.

> 💡 **Exemplo Numérico:** Suponha que temos um modelo onde $Y_{t+1} = 3 + 2X_{t1} + 1.5X_{t2} + \epsilon_t$.  Vamos gerar dados simulados com a relação acima e observar a convergência do estimador OLS para o intercepto e os coeficientes da projeção linear:
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
>
> # Parâmetros
> T = 1000
> alpha_true = np.array([2, 1.5]) # Coeficientes verdadeiros
> intercept_true = 3 # Intercepto verdadeiro
> np.random.seed(42)
>
> # Simular os dados
> X_t = np.random.rand(T, 2) # Dados preditores aleatórios
> epsilon = np.random.normal(0, 1, T)  # Ruído aleatório
> Y_t_plus_1 = intercept_true + np.dot(X_t, alpha_true) + epsilon # Variável resposta
> df = pd.DataFrame({'x1': X_t[:, 0], 'x2': X_t[:, 1], 'y_t_plus_1': Y_t_plus_1})
>
> # Regressão OLS com intercepto
> X = df[['x1', 'x2']]
> Y = df['y_t_plus_1']
> model = LinearRegression()
> model.fit(X, Y)
> b = model.coef_
> intercept = model.intercept_
>
> print(f"Coeficientes verdadeiros: {alpha_true}")
> print(f"Intercepto verdadeiro: {intercept_true}")
> print(f"Coeficientes estimados por OLS: {b}")
> print(f"Intercepto estimado por OLS: {intercept}")
> ```
> Os resultados serão:
> ```
> Coeficientes verdadeiros: [2.  1.5]
> Intercepto verdadeiro: 3
> Coeficientes estimados por OLS: [1.99939398 1.52133371]
> Intercepto estimado por OLS: 3.000742511878288
> ```
> Os resultados mostram que os coeficientes estimados por OLS se aproximam dos valores verdadeiros tanto para o intercepto quanto para os coeficientes de projeção linear. O exemplo ilustra a consistência do estimador OLS, onde as estimativas convergem para os parâmetros verdadeiros à medida que o tamanho da amostra aumenta.
>
>  Além disso, podemos realizar uma análise dos resíduos para verificar a adequação do modelo. Os resíduos devem ter média próxima de zero e não devem apresentar padrões que indiquem não-linearidades ou heterocedasticidade.
> ```python
> residuals = Y - model.predict(X)
> print(f"Média dos resíduos: {np.mean(residuals)}")
> print(f"Desvio padrão dos resíduos: {np.std(residuals)}")
> ```
> Um histograma dos resíduos também pode ser útil para verificar se eles são aproximadamente normalmente distribuídos. Isso é importante para a validade de testes estatísticos e intervalos de confiança.

**Teorema 5.2 (Decomposição da Variância):** A variância de $Y_{t+1}$ pode ser decomposta na variância da projeção linear com termo constante e na variância do erro de previsão:
$$ Var(Y_{t+1}) = Var(Y^*_{t+1}) + Var(Y_{t+1} - Y^*_{t+1}) $$
onde $Y^*_{t+1} = \beta_0 + \beta' X_t$.

*Prova:*
I. Pela lei da variância total, podemos escrever:
    $$ Var(Y_{t+1}) = E[Var(Y_{t+1}|Z_t)] + Var(E(Y_{t+1}|Z_t)) $$
II.  Como a projeção linear $Y^*_{t+1}$ é a melhor aproximação linear de $E(Y_{t+1}|Z_t)$, e o erro de previsão é ortogonal a $Z_t$, temos:
 $$  E(Y_{t+1}|Z_t) = Y^*_{t+1} + E(Y_{t+1}-Y^*_{t+1}|Z_t) $$
III.   O erro de previsão é ortogonal a $Z_t$, portanto $E(Y_{t+1}-Y^*_{t+1}|Z_t)=0$ e $E(Y_{t+1}|Z_t) = Y^*_{t+1}$.
IV.   Substituindo a expressão para a esperança condicional na lei da variância total, e usando que $Y_{t+1} = Y^*_{t+1} + (Y_{t+1} - Y^*_{t+1})$, temos:
 $$ Var(Y_{t+1}) = Var(Y^*_{t+1}) + E[Var(Y_{t+1} - Y^*_{t+1}|Z_t)] $$
V.  Como o erro de previsão é ortogonal a $Z_t$, $E[Var(Y_{t+1} - Y^*_{t+1}|Z_t)] = Var(Y_{t+1} - Y^*_{t+1})$, logo:
$$ Var(Y_{t+1}) = Var(Y^*_{t+1}) + Var(Y_{t+1} - Y^*_{t+1}) $$
■

### Implicações Práticas e Teóricas
A inclusão de um termo constante em projeções lineares tem diversas implicações práticas e teóricas. Do ponto de vista prático, permite capturar o valor esperado de $Y_{t+1}$ quando $X_t$ é zero, o que é útil em diversos cenários de previsão. Do ponto de vista teórico, a projeção linear com termo constante fornece uma aproximação linear mais geral da expectativa condicional $E(Y_{t+1}|X_t)$, pois inclui uma constante adicional para maior flexibilidade na modelagem.

**Observação 5.2 (Aplicações Práticas):** A inclusão de um termo constante é fundamental em diversas aplicações práticas, como em modelos econométricos, onde frequentemente se usa uma transformação logarítmica nas variáveis para garantir estacionariedade.  Neste caso, a previsão da variável não transformada incluirá o intercepto da regressão linear.

A inclusão de um termo constante na projeção linear não altera sua propriedade fundamental de minimizar o MSE dentro da classe de previsões lineares. Ela simplesmente adiciona um grau de liberdade adicional ao modelo, permitindo que ele se ajuste melhor aos dados. A condição de não correlação ainda se mantém, agora aplicada ao vetor expandido $Z_t = [1, X_t']'$.

**Corolário 5.1 (Generalização da Projeção Linear):** A inclusão de um termo constante na projeção linear não invalida as propriedades de ortogonalidade e minimização do MSE. O modelo resultante ainda fornece a melhor previsão linear de $Y_{t+1}$ dada a informação contida em $X_t$ e no termo constante.

*Prova:* A prova deste corolário segue diretamente da generalização do lema 2.1, ao considerar que o regressor agora é $Z_t = [1, X_t']'$. Ao considerarmos o vetor de coeficientes da projeção como $\alpha=[\beta_0, \beta]$, o Lema 2.1 continua válido. A previsão linear com intercepto é obtida quando o erro da previsão, $(Y_{t+1}-\beta_0-\beta'X_t)$, é ortogonal ao vetor $Z_t$.

**Proposição 5.2 (Relação entre Projeção e Esperança Condicional):** Se a relação entre $Y_{t+1}$ e $X_t$ for linear, ou seja, $E(Y_{t+1}|X_t) = \beta_0 + \beta'X_t$, então a projeção linear com termo constante coincide com a esperança condicional.

*Prova:*
I. Se a esperança condicional for linear, então $E(Y_{t+1}|X_t) = \beta_0 + \beta'X_t$.
II.  A projeção linear com termo constante é dada por $Y^*_{t+1} = \beta_0 + \beta'X_t$ onde $\beta_0$ e $\beta$ são escolhidos para minimizar o erro quadrático médio $E[(Y_{t+1} - (\beta_0+\beta'X_t))^2]$.
III.  Como $E(Y_{t+1}|X_t) = \beta_0 + \beta'X_t$, a projeção linear também será $Y^*_{t+1} = \beta_0 + \beta'X_t$, com os mesmos coeficientes, pois neste caso a projeção linear coincide com a esperança condicional.
$\blacksquare$
> 💡 **Exemplo Numérico:** Suponha que a relação verdadeira entre $Y_{t+1}$ e $X_t$ é dada por $E(Y_{t+1}|X_t) = 2 + 3X_t$. Neste caso, a projeção linear com termo constante será exatamente a esperança condicional, ou seja, $\beta_0 = 2$ e $\beta = 3$. Se ajustarmos um modelo linear com termo constante aos dados gerados dessa forma, obteremos coeficientes que se aproximam desses valores, mostrando que a projeção linear é uma aproximação da esperança condicional quando a relação é linear.

### Conclusão
Neste capítulo, demonstramos como a inclusão de um termo constante na projeção linear resulta em uma ferramenta de previsão mais flexível, e que a forma matemática da previsão linear continua sendo $Y^*_{t+1} = \beta_0 + \beta' X_t$, onde os coeficientes são ajustados usando a informação em $Y_{t+1}$ e em $X_t$. Vimos que, sob condições de ergodicidade, a regressão OLS com intercepto fornece uma estimativa consistente para esses coeficientes, e que a análise de projeção linear com constante é análoga à regressão OLS com intercepto, e o estimador OLS converge para a projeção linear, garantindo que a regressão OLS possa ser usada para construir previsões lineares com termo constante.

### Referências
[^4.1.9]: *We now restrict the class of forecasts considered by requiring the forecast
Y to be a linear function of X,:
Y+1 = α΄Χ.*
[^4.1.10]: *Suppose we were to find a value for a such that the forecast error (Y1+1 – α΄Χ.)
is uncorrelated with X,:
Ε[(Υ.+1 – α΄Χ.)X] = 0'.*
[^4.1.16]: *A linear regression model relates an observation on yr+1 to x₁:
Ус+1 = β'х, + 4,. *
<!-- END -->
