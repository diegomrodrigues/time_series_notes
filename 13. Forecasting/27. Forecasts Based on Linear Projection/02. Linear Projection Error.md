## Proje√ß√£o Linear: Minimiza√ß√£o do Erro Quadr√°tico M√©dio e N√£o Correla√ß√£o
### Introdu√ß√£o
Dando continuidade ao estudo da **proje√ß√£o linear** [^4.1.9], este cap√≠tulo aprofunda a an√°lise do seu processo de otimiza√ß√£o e da condi√ß√£o de n√£o correla√ß√£o que define o coeficiente de proje√ß√£o. O foco principal √© demonstrar como a proje√ß√£o linear, denotada como $\alpha'X_t$, minimiza o erro quadr√°tico m√©dio (MSE) dentro da classe de previs√µes lineares e como essa minimiza√ß√£o √© obtida atrav√©s da condi√ß√£o de que o erro de previs√£o ($Y_{t+1} - \alpha'X_t$) seja n√£o correlacionado com $X_t$ [^4.1.10]. O objetivo √© fornecer um tratamento matem√°tico detalhado e rigoroso deste tema para acad√™micos com conhecimento avan√ßado em estat√≠stica e otimiza√ß√£o.

### Minimiza√ß√£o do Erro Quadr√°tico M√©dio e a Condi√ß√£o de N√£o Correla√ß√£o
Como estabelecido anteriormente, a proje√ß√£o linear de $Y_{t+1}$ em $X_t$ √© dada por:
$$ Y^*_{t+1} = \alpha' X_t $$
onde $\alpha$ √© o vetor de coeficientes que buscamos determinar. O objetivo da proje√ß√£o linear √© encontrar o vetor $\alpha$ que minimize o erro quadr√°tico m√©dio (MSE) da previs√£o, definido como [^4.1.1]:
$$ MSE(Y^*_{t+1}) = E[(Y_{t+1} - Y^*_{t+1})^2] = E[(Y_{t+1} - \alpha' X_t)^2] $$
Para alcan√ßar esta minimiza√ß√£o, √© crucial que o erro de previs√£o $(Y_{t+1} - \alpha'X_t)$ seja n√£o correlacionado com $X_t$. Esta condi√ß√£o √© expressa matematicamente como [^4.1.10]:
$$ E[(Y_{t+1} - \alpha' X_t)X_t] = 0' $$
Esta condi√ß√£o de n√£o correla√ß√£o tem uma interpreta√ß√£o geom√©trica: o vetor erro $(Y_{t+1} - \alpha'X_t)$ √© ortogonal ao espa√ßo vetorial gerado por $X_t$. Em outras palavras, a proje√ß√£o linear $\alpha'X_t$ captura toda a informa√ß√£o linearmente dispon√≠vel em $X_t$ para prever $Y_{t+1}$, e qualquer parte de $Y_{t+1}$ que n√£o possa ser explicada por $\alpha'X_t$ √© n√£o correlacionada com $X_t$.

**Proposi√ß√£o 2.1 (Ortogonalidade do Erro):** A condi√ß√£o de n√£o correla√ß√£o $E[(Y_{t+1} - \alpha' X_t)X_t] = 0'$ √© equivalente a dizer que o erro de previs√£o √© ortogonal ao espa√ßo gerado por $X_t$.

*Prova:* A condi√ß√£o $E[(Y_{t+1} - \alpha' X_t)X_t] = 0'$ indica que a covari√¢ncia entre o erro de previs√£o $(Y_{t+1} - \alpha' X_t)$ e qualquer elemento do vetor $X_t$ √© zero. Geometricamente, isso significa que o vetor erro √© ortogonal a cada um dos vetores componentes de $X_t$, e, por conseguinte, a todo o espa√ßo vetorial gerado por $X_t$.

**Lema 2.1 (Minimiza√ß√£o do MSE):** A escolha de $\alpha$ que satisfaz a condi√ß√£o de n√£o correla√ß√£o $E[(Y_{t+1} - \alpha' X_t)X_t] = 0'$ √© a mesma que minimiza o MSE.

*Prova:*
I. Come√ßamos com a express√£o do erro quadr√°tico m√©dio de uma previs√£o linear arbitr√°ria $g'X_t$:
   $$ MSE(g'X_t) = E[Y_{t+1} - g'X_t]^2 $$
II. Adicionamos e subtra√≠mos a proje√ß√£o linear $\alpha'X_t$ dentro da express√£o:
    $$ E[Y_{t+1} - \alpha'X_t + \alpha'X_t - g'X_t]^2 $$
III.  Reorganizamos os termos para separar o erro da proje√ß√£o linear e o erro adicional:
   $$ E[(Y_{t+1} - \alpha'X_t) + (\alpha'X_t - g'X_t)]^2 $$
IV. Expandimos o quadrado:
    $$ E[(Y_{t+1} - \alpha'X_t)^2 + 2(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t) + (\alpha'X_t - g'X_t)^2] $$
V. Pela linearidade do operador de esperan√ßa:
    $$ E[Y_{t+1} - \alpha'X_t]^2 + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t)] + E[\alpha'X_t - g'X_t]^2 $$
VI. O termo do meio pode ser reescrito como:
  $$  2E[(Y_{t+1} - \alpha'X_t)X_t'(\alpha - g)]$$
VII.  Usando a condi√ß√£o de n√£o correla√ß√£o, $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$, o termo do meio se anula:
   $$  2(\alpha-g)E[(Y_{t+1} - \alpha'X_t)X_t] = 0$$
VIII. O MSE se reduz a:
    $$ MSE(g'X_t) = E[Y_{t+1} - \alpha'X_t]^2 + E[\alpha'X_t - g'X_t]^2 $$
IX.  O primeiro termo √© o MSE da proje√ß√£o linear $\alpha'X_t$. O segundo termo √© sempre n√£o-negativo, e √© zero somente quando $g'X_t = \alpha'X_t$. Portanto, o MSE √© minimizado quando $g = \alpha$.$\blacksquare$

> üí° **Exemplo Num√©rico:** Para ilustrar, considere um modelo simples onde $Y_{t+1} = 2X_t + \epsilon_t$, e $\epsilon_t$ √© um erro com m√©dia zero e n√£o correlacionado com $X_t$. Suponha que os momentos s√£o:
> $E(X_t^2) = 4$, $E(Y_{t+1}X_t) = 8$. O coeficiente de proje√ß√£o linear $\alpha$ √©:
> $$\alpha = \frac{E(Y_{t+1}X_t)}{E(X_t^2)} = \frac{8}{4} = 2$$
> Portanto, $Y^*_{t+1} = 2 X_t$. Se usarmos uma previs√£o linear arbitr√°ria, por exemplo, $Y^*_{t+1} = 1.5 X_t$ (ou seja $g=1.5$), podemos mostrar que o erro ser√° maior do que a proje√ß√£o linear √≥tima, pois a parte do erro que √© correlacionada com $X_t$ n√£o foi capturada.
> Seja $E(\epsilon_t^2) = 1$ , e portanto $E[(Y_{t+1}-2X_t)^2] = 1 $. O MSE com $g=1.5$ √©:
> $$ E[Y_{t+1} - 1.5X_t]^2 = E[Y_{t+1} - 2X_t + 0.5X_t]^2 $$
> $$ = E[(Y_{t+1} - 2X_t)^2] + E[0.5X_t]^2 = E[\epsilon_t^2] + 0.25 E[X_t^2] $$
> $$  = 1 + 0.25(4) = 2 $$
> O MSE da proje√ß√£o linear √© 1, mostrando que qualquer escolha diferente do coeficiente de proje√ß√£o linear aumenta o erro quadr√°tico m√©dio.

Em ess√™ncia, a condi√ß√£o de n√£o correla√ß√£o √© crucial porque garante que a proje√ß√£o linear $\alpha'X_t$ capta toda a informa√ß√£o linear em $X_t$ para prever $Y_{t+1}$. Se essa condi√ß√£o n√£o fosse satisfeita, o erro de previs√£o estaria correlacionado com $X_t$, o que indicaria que ainda h√° alguma informa√ß√£o √∫til em $X_t$ que n√£o foi utilizada pela previs√£o.

**Lema 2.2 (Unicidade da Proje√ß√£o Linear):** Se a matriz $E[X_tX_t']$ √© n√£o-singular, ent√£o o vetor $\alpha$ que minimiza o MSE √© √∫nico.

*Prova:*
I. J√° demonstramos que qualquer previs√£o linear $g'X_t$ pode ser decomposta como:
   $$MSE(g'X_t) = MSE(\alpha'X_t) + E[(\alpha'X_t - g'X_t)^2]$$
II. Se $E[X_tX_t']$ √© n√£o-singular, ent√£o:
    $$E[(\alpha'X_t - g'X_t)^2] = E[(\alpha - g)'X_t X_t'(\alpha - g)]$$
   √© estritamente positivo para $\alpha \neq g$, porque $X_t$ n√£o √© identicamente zero com probabilidade 1.
III. Isso implica que a proje√ß√£o linear $\alpha'X_t$ √© a √∫nica que minimiza o MSE.$\blacksquare$
> üí° **Exemplo Num√©rico:**
> Para ilustrar a unicidade, vamos considerar um caso com dois preditores: $X_t = [X_{t1}, X_{t2}]'$. Suponha que temos a matriz de covari√¢ncia de $X_t$ como:
>
> $$ E[X_t X_t'] = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix} $$
>
> e o vetor de covari√¢ncia entre $Y_{t+1}$ e $X_t$ como:
>
> $$ E[Y_{t+1} X_t'] = \begin{bmatrix} 5 \\ 7 \end{bmatrix} $$
>
> O vetor de coeficientes $\alpha$ √© dado por:
>
> $$ \alpha' = E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1} $$
>
> Primeiro, calculamos a inversa da matriz de covari√¢ncia de $X_t$:
>
> $$ [E(X_tX_t')]^{-1} = \frac{1}{(2)(3) - (1)(1)} \begin{bmatrix} 3 & -1 \\ -1 & 2 \end{bmatrix} = \frac{1}{5} \begin{bmatrix} 3 & -1 \\ -1 & 2 \end{bmatrix} = \begin{bmatrix} 0.6 & -0.2 \\ -0.2 & 0.4 \end{bmatrix} $$
>
> Agora, multiplicamos a matriz inversa pela covari√¢ncia entre $Y_{t+1}$ e $X_t$:
>
> $$ \alpha' = \begin{bmatrix} 5 & 7 \end{bmatrix} \begin{bmatrix} 0.6 & -0.2 \\ -0.2 & 0.4 \end{bmatrix} = \begin{bmatrix} 5(0.6) + 7(-0.2) & 5(-0.2) + 7(0.4) \end{bmatrix} = \begin{bmatrix} 3 - 1.4 & -1 + 2.8 \end{bmatrix} = \begin{bmatrix} 1.6 & 1.8 \end{bmatrix} $$
>
> Portanto, $\alpha = \begin{bmatrix} 1.6 \\ 1.8 \end{bmatrix}$.  Este vetor $\alpha$ √© √∫nico dado que $E[X_tX_t']$ √© n√£o-singular (determinante √© 5). Qualquer outro vetor $g$ resultaria em um MSE maior, ilustrando a unicidade da proje√ß√£o linear.

### Deriva√ß√£o do Coeficiente de Proje√ß√£o Linear
O coeficiente $\alpha$ pode ser derivado a partir da condi√ß√£o de n√£o correla√ß√£o [^4.1.13]:
$$ E[(Y_{t+1} - \alpha' X_t)X_t'] = 0' $$
Expandindo esta equa√ß√£o:
$$ E[Y_{t+1}X_t'] - E[\alpha' X_t X_t'] = 0' $$
$$ E[Y_{t+1}X_t'] - \alpha' E[X_t X_t'] = 0' $$
Reorganizando os termos para isolar $\alpha'$:
$$ \alpha' E[X_t X_t'] = E[Y_{t+1}X_t'] $$
Se a matriz $E[X_tX_t']$ for n√£o-singular, podemos multiplicar ambos os lados pela sua inversa √† direita:
$$ \alpha' = E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1} $$
Esta equa√ß√£o demonstra explicitamente como o coeficiente de proje√ß√£o linear $\alpha$ √© calculado em termos dos momentos populacionais de $Y_{t+1}$ e $X_t$.
> üí° **Exemplo Num√©rico:**
> Vamos supor que temos um conjunto de dados onde:
> $$ E[X_t X_t'] = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 2 \end{bmatrix} $$
> $$ E[Y_{t+1} X_t'] = \begin{bmatrix} 3 & 4 \end{bmatrix} $$
> Usando a f√≥rmula derivada, podemos calcular $\alpha'$:
> 1. Calcule a inversa da matriz $E[X_t X_t']$:
> $$ [E(X_t X_t')]^{-1} = \frac{1}{(1)(2) - (0.5)(0.5)} \begin{bmatrix} 2 & -0.5 \\ -0.5 & 1 \end{bmatrix} = \frac{1}{1.75} \begin{bmatrix} 2 & -0.5 \\ -0.5 & 1 \end{bmatrix} \approx \begin{bmatrix} 1.14 & -0.29 \\ -0.29 & 0.57 \end{bmatrix} $$
> 2. Multiplique $E[Y_{t+1} X_t']$ pela inversa:
> $$ \alpha' = \begin{bmatrix} 3 & 4 \end{bmatrix} \begin{bmatrix} 1.14 & -0.29 \\ -0.29 & 0.57 \end{bmatrix} = \begin{bmatrix} 3(1.14) + 4(-0.29) & 3(-0.29) + 4(0.57) \end{bmatrix} = \begin{bmatrix} 2.26 & 1.41 \end{bmatrix} $$
> Assim, $\alpha \approx \begin{bmatrix} 2.26 \\ 1.41 \end{bmatrix}$. Isso significa que a previs√£o linear de $Y_{t+1}$ usando $X_t$ √© dada por $Y_{t+1}^* = 2.26X_{t1} + 1.41X_{t2}$.

**Observa√ß√£o 2.1 (Singularidade):** Se a matriz $E[X_tX_t']$ for singular, o vetor $\alpha$ n√£o ser√° unicamente determinado. No entanto, a previs√£o $\alpha'X_t$ ainda √© unicamente definida [^4.1.13].

**Proposi√ß√£o 2.2 (Proje√ß√£o e Melhor Previs√£o Linear):** A proje√ß√£o linear $\alpha'X_t$ √© a melhor previs√£o linear de $Y_{t+1}$ dado $X_t$ no sentido de que ela minimiza o MSE dentre todas as previs√µes lineares, ou seja, para qualquer outro vetor $g$, $MSE(\alpha'X_t) \le MSE(g'X_t)$.

*Prova:*
I. Esta proposi√ß√£o √© uma reformula√ß√£o do Lema 2.1, com a diferen√ßa de explicitar o conceito de melhor previs√£o.
II. Pelo Lema 2.1, temos que:
$$MSE(g'X_t) = E[Y_{t+1} - \alpha'X_t]^2 + E[(\alpha'X_t - g'X_t)^2]$$
III. Como $E[(\alpha'X_t - g'X_t)^2]$ √© sempre n√£o-negativo,
$$MSE(g'X_t) \ge E[Y_{t+1} - \alpha'X_t]^2 = MSE(\alpha'X_t)$$
IV. Portanto, a proje√ß√£o linear $\alpha'X_t$ √© a melhor previs√£o linear de $Y_{t+1}$ dado $X_t$.$\blacksquare$

### Implica√ß√µes Pr√°ticas e Te√≥ricas
A proje√ß√£o linear √© uma ferramenta valiosa para a previs√£o, pois ela oferece uma previs√£o linear √≥tima em termos de erro quadr√°tico m√©dio. A condi√ß√£o de n√£o correla√ß√£o √© a base matem√°tica que garante essa otimalidade. Ao usar momentos populacionais ou amostrais (no caso da regress√£o OLS), podemos estimar os coeficientes de proje√ß√£o e, assim, obter previs√µes precisas.

A proje√ß√£o linear, ao contr√°rio da an√°lise estrutural, n√£o requer nenhuma restri√ß√£o sobre a causalidade entre $Y_{t+1}$ e $X_t$. Isso a torna aplic√°vel em uma variedade maior de cen√°rios onde o foco √© a previs√£o. A sua simplicidade e robustez a tornam uma t√©cnica popular em diversas √°reas.
Expandindo sobre a conex√£o entre proje√ß√£o linear e regress√£o OLS, podemos dizer que a proje√ß√£o linear define o vetor $\alpha$ usando momentos populacionais, enquanto que a regress√£o OLS calcula o vetor $b$ usando momentos amostrais. Pelo teorema da converg√™ncia dos momentos (que requer ergodicidade), o estimador OLS $b$ converge para $\alpha$ conforme a amostra aumenta para o infinito.

**Corol√°rio 2.1 (Converg√™ncia do OLS):** Se a amostra de $X_t$ e $Y_{t+1}$ √© tal que os momentos amostrais convergem para os momentos populacionais, ent√£o o estimador OLS converge para o coeficiente de proje√ß√£o linear $\alpha$.

*Prova:*
I. O estimador OLS √© definido como:
  $$b = (\sum_{t=1}^T X_t X_t')^{-1} (\sum_{t=1}^T X_t Y_{t+1})$$
   onde a soma √© sobre as amostras observadas.
II. Se a amostra √© tal que:
  $$\frac{1}{T}\sum_{t=1}^T X_t X_t' \rightarrow E(X_tX_t')$$
  e
   $$\frac{1}{T}\sum_{t=1}^T X_t Y_{t+1} \rightarrow E(Y_{t+1}X_t')$$
   quando $T \rightarrow \infty$,
III. Ent√£o, pelo teorema da converg√™ncia de momentos,
  $$b \rightarrow  [E(X_tX_t')]^{-1}E(Y_{t+1}X_t') = \alpha$$ quando $T \rightarrow \infty$.$\blacksquare$

> üí° **Exemplo Num√©rico:** Para demonstrar a converg√™ncia do OLS, vamos gerar dados simulados com uma rela√ß√£o linear entre $X_t$ e $Y_{t+1}$, adicionar um ru√≠do e mostrar que o OLS se aproxima do valor te√≥rico da proje√ß√£o linear quando o tamanho da amostra aumenta.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LinearRegression
>
> # Par√¢metros
> alpha_true = np.array([2, 1.5]) # Coeficientes verdadeiros
> n_samples = [10, 100, 1000, 10000]  # Tamanhos de amostra
> n_simulations = 100
>
> results = {n: [] for n in n_samples}
>
> for n in n_samples:
>     for sim in range(n_simulations):
>         X_t = np.random.rand(n, 2) # Dados preditores aleat√≥rios
>         epsilon = np.random.normal(0, 0.5, n)  # Ru√≠do aleat√≥rio
>         Y_t1 = np.dot(X_t, alpha_true) + epsilon # Vari√°vel resposta
>
>         # Regress√£o OLS
>         model = LinearRegression()
>         model.fit(X_t, Y_t1)
>         results[n].append(model.coef_)
>
> # Calcula a m√©dia dos coeficientes para cada tamanho de amostra
> mean_coeffs = {n: np.mean(np.array(results[n]), axis=0) for n in n_samples}
>
> # Imprime os resultados
> print("Coeficientes verdadeiros:", alpha_true)
> for n, mean_coef in mean_coeffs.items():
>     print(f"Tamanho da amostra: {n}, Coeficientes OLS: {mean_coef}")
>
> # Visualiza√ß√£o (opcional)
> plt.figure(figsize=(10,6))
> for i,n in enumerate(n_samples):
>     plt.plot(results[n], label=f"OLS Coefs (N={n})", marker = 'o', linestyle='dashed', alpha=0.5)
> plt.axhline(y=alpha_true[0], color='r', linestyle='-', label='True Coef 1')
> plt.axhline(y=alpha_true[1], color='b', linestyle='-', label='True Coef 2')
> plt.xlabel("Simula√ß√£o")
> plt.ylabel("Valor do Coeficiente")
> plt.title("Converg√™ncia OLS: Simula√ß√µes x Coeficientes Estimados")
> plt.legend()
> plt.show()
> ```
> Este c√≥digo gera dados simulados e executa a regress√£o OLS para v√°rios tamanhos de amostra. Os resultados mostram que, √† medida que o tamanho da amostra aumenta, os coeficientes estimados pelo OLS se aproximam dos coeficientes verdadeiros usados para gerar os dados, ilustrando a converg√™ncia do estimador OLS para o coeficiente de proje√ß√£o linear.

### Conclus√£o
Neste cap√≠tulo, examinamos em detalhes a proje√ß√£o linear $\alpha'X_t$, enfatizando sua capacidade de minimizar o erro quadr√°tico m√©dio da previs√£o. Atrav√©s da condi√ß√£o de n√£o correla√ß√£o $E[(Y_{t+1} - \alpha' X_t)X_t] = 0'$, mostramos que o erro da proje√ß√£o linear √© ortogonal ao espa√ßo gerado por $X_t$, capturando toda a informa√ß√£o linear relevante em $X_t$. Exploramos tamb√©m como a proje√ß√£o linear se conecta com a regress√£o OLS, destacando como ambas compartilham a mesma base matem√°tica, embora com diferentes fontes de momentos (populacional vs amostral). A compreens√£o destes conceitos √© fundamental para qualquer estudo avan√ßado em modelagem e previs√£o de s√©ries temporais.
### Refer√™ncias
[^4.1.1]: *Expression [4.1.1] is known as the mean squared error associated with the forecast
Y*+ 11, denoted
MSE(Y*+1/2) = E(Y1+1 - Y+1)2.*
[^4.1.9]: *We now restrict the class of forecasts considered by requiring the forecast
Y to be a linear function of X,:
Y+1 = Œ±ŒÑŒß.*
[^4.1.10]: *Suppose we were to find a value for a such that the forecast error (Y1+1 ‚Äì Œ±ŒÑŒß.)
is uncorrelated with X,:
Œï[(Œ•.+1 ‚Äì Œ±ŒÑŒß.)X] = 0'.*
[^4.1.13]: *Œ±' = E(Y+1X)[E(X,X;)]¬Ø¬π, assuming that E(X,X) is a nonsingular matrix.*
<!-- END -->
