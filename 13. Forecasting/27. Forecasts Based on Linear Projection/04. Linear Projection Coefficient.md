## C√°lculo do Coeficiente de Proje√ß√£o Linear: Momentos e Otimiza√ß√£o
### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise do **coeficiente de proje√ß√£o linear** $\alpha$, focando na sua computa√ß√£o atrav√©s de momentos populacionais e na sua import√¢ncia para garantir a otimalidade da previs√£o linear. Expandindo os conceitos anteriormente introduzidos, investigaremos detalhadamente como o coeficiente $\alpha$ √© determinado pela rela√ß√£o entre os momentos de $Y_{t+1}$ e $X_t$, especificamente $E(Y_{t+1}X_t')$ e $E(X_tX_t')$, e como essa formula√ß√£o garante que a previs√£o $\alpha'X_t$ seja a melhor aproxima√ß√£o linear de $Y_{t+1}$ baseada em $X_t$ [^4.1.9]. O objetivo deste cap√≠tulo √© oferecer um tratamento matem√°tico rigoroso e completo sobre o tema para acad√™micos com forte base em estat√≠stica, otimiza√ß√£o e an√°lise de dados.

### Formula√ß√£o do Coeficiente de Proje√ß√£o Linear
Como estabelecido, a proje√ß√£o linear de $Y_{t+1}$ sobre $X_t$ √© expressa como:
$$ Y^*_{t+1} = \alpha' X_t $$
Onde $\alpha$ √© o vetor de coeficientes que buscamos determinar. A condi√ß√£o de n√£o correla√ß√£o entre o erro de previs√£o $(Y_{t+1} - \alpha' X_t)$ e $X_t$, dada por $E[(Y_{t+1} - \alpha' X_t)X_t] = 0'$ [^4.1.10], √© fundamental para garantir que a proje√ß√£o linear minimize o erro quadr√°tico m√©dio (MSE) [^4.1.2]. Esta condi√ß√£o nos leva √† f√≥rmula do coeficiente de proje√ß√£o linear:
$$ \alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} $$
Esta f√≥rmula envolve o c√°lculo de duas matrizes de momentos populacionais:
1.  $E(Y_{t+1}X_t')$: A matriz de covari√¢ncia entre $Y_{t+1}$ e $X_t$. Esta matriz capta como as vari√°veis em $X_t$ comovimentam com $Y_{t+1}$.
2.  $E(X_tX_t')$: A matriz de covari√¢ncia de $X_t$. Esta matriz descreve as rela√ß√µes entre as vari√°veis em $X_t$.

**Proposi√ß√£o 4.1 (C√°lculo do Coeficiente de Proje√ß√£o):** O coeficiente de proje√ß√£o linear $\alpha$ √© obtido atrav√©s da seguinte express√£o:
$$ \alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} $$
onde $E(Y_{t+1}X_t')$ representa a matriz de covari√¢ncia entre $Y_{t+1}$ e $X_t$, e $E(X_tX_t')$ representa a matriz de covari√¢ncia de $X_t$.

*Prova:*
I. Come√ßamos com a condi√ß√£o de n√£o correla√ß√£o:
$$ E[(Y_{t+1} - \alpha'X_t)X_t'] = 0' $$
II. Expandindo a express√£o, obtemos:
$$ E[Y_{t+1}X_t'] - E[\alpha'X_tX_t'] = 0' $$
III. Usando a propriedade de linearidade da esperan√ßa:
$$ E[Y_{t+1}X_t'] - \alpha'E[X_tX_t'] = 0' $$
IV. Isolando $\alpha'$, temos:
$$ \alpha'E[X_tX_t'] = E[Y_{t+1}X_t'] $$
V. Multiplicando ambos os lados por $[E(X_tX_t')]^{-1}$ √† direita, assumindo que essa inversa existe:
$$ \alpha' = E[Y_{t+1}X_t'][E(X_tX_t')]^{-1} $$
Este resultado deriva explicitamente a express√£o para o coeficiente de proje√ß√£o linear em termos dos momentos populacionais.  ‚ñ†

**Lema 4.1 (Interpreta√ß√£o da F√≥rmula):** A f√≥rmula $\alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}$ expressa o coeficiente $\alpha$ como a rela√ß√£o entre a comovimenta√ß√£o de $Y_{t+1}$ e $X_t$, medida por $E(Y_{t+1}X_t')$, e a variabilidade de $X_t$, medida por $E(X_tX_t')$.

*Prova:*
I. A matriz $E(Y_{t+1}X_t')$ representa a covari√¢ncia entre a vari√°vel que queremos prever ($Y_{t+1}$) e as vari√°veis preditoras ($X_t$). Cada elemento desta matriz indica como uma vari√°vel em $X_t$ comovaria linearmente com $Y_{t+1}$.
II. A matriz $E(X_tX_t')$ representa a matriz de covari√¢ncia das vari√°veis preditoras ($X_t$). Ela descreve a variabilidade e a rela√ß√£o entre as vari√°veis em $X_t$.
III. A inversa de $E(X_tX_t')$ ajusta a rela√ß√£o com base nas correla√ß√µes e vari√¢ncias entre os preditores.
IV. A multiplica√ß√£o de $E(Y_{t+1}X_t')$ pela inversa de $E(X_tX_t')$ efetivamente pondera as rela√ß√µes entre $Y_{t+1}$ e $X_t$ com base na variabilidade de $X_t$, resultando no melhor coeficiente linear para a proje√ß√£o.
V. Portanto, o coeficiente $\alpha$ capta a rela√ß√£o linear √≥tima entre as vari√°veis em termos de previs√£o. $\blacksquare$

**Lema 4.2 (Propriedade de Ortogonalidade):** O erro de previs√£o, $Y_{t+1} - \alpha'X_t$, √© ortogonal a qualquer combina√ß√£o linear das vari√°veis em $X_t$.

*Prova:*
I. J√° sabemos que $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0'$.
II. Seja $Z_t$ uma combina√ß√£o linear das vari√°veis em $X_t$, ou seja, $Z_t = B'X_t$, onde $B$ √© um vetor de constantes.
III. Ent√£o, $E[(Y_{t+1} - \alpha'X_t)Z_t'] = E[(Y_{t+1} - \alpha'X_t)(B'X_t)'] = E[(Y_{t+1} - \alpha'X_t)X_t'B] =  E[(Y_{t+1} - \alpha'X_t)X_t']B$.
IV. Como $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0'$, ent√£o $E[(Y_{t+1} - \alpha'X_t)Z_t'] = 0'B=0'$.
V. Portanto, o erro de previs√£o √© ortogonal a qualquer combina√ß√£o linear das vari√°veis em $X_t$, incluindo as pr√≥prias vari√°veis de $X_t$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que temos as seguintes matrizes de momentos populacionais:
> $$ E(X_tX_t') = \begin{bmatrix} 2 & 0.5 \\ 0.5 & 3 \end{bmatrix} $$
> $$ E(Y_{t+1}X_t') = \begin{bmatrix} 4 & 7 \end{bmatrix} $$
> Onde $X_t$ √© um vetor de duas vari√°veis preditoras, $X_{t1}$ e $X_{t2}$, e $Y_{t+1}$ √© a vari√°vel que queremos prever. O coeficiente de proje√ß√£o linear √© dado por:
> $$ \alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} $$
> Primeiro calculamos a inversa de $E(X_tX_t')$:
> $$ [E(X_tX_t')]^{-1} = \frac{1}{(2)(3) - (0.5)(0.5)} \begin{bmatrix} 3 & -0.5 \\ -0.5 & 2 \end{bmatrix} = \frac{1}{5.75} \begin{bmatrix} 3 & -0.5 \\ -0.5 & 2 \end{bmatrix} $$
> $$ \approx \begin{bmatrix} 0.5217 & -0.0870 \\ -0.0870 & 0.3478 \end{bmatrix} $$
> Agora multiplicamos $E(Y_{t+1}X_t')$ pela inversa:
> $$ \alpha' = \begin{bmatrix} 4 & 7 \end{bmatrix} \begin{bmatrix} 0.5217 & -0.0870 \\ -0.0870 & 0.3478 \end{bmatrix} = \begin{bmatrix} 4(0.5217) + 7(-0.0870) & 4(-0.0870) + 7(0.3478) \end{bmatrix} $$
> $$ \alpha' \approx \begin{bmatrix} 1.4568 & 2.0966 \end{bmatrix} $$
> Portanto, o vetor de coeficientes de proje√ß√£o linear √© $\alpha \approx \begin{bmatrix} 1.4568 \\ 2.0966 \end{bmatrix}$. A previs√£o linear √©, aproximadamente, $Y^*_{t+1} \approx 1.4568X_{t1} + 2.0966X_{t2}$.
> Intuitivamente, o coeficiente 1.4568 indica que, para cada unidade de aumento em $X_{t1}$, $Y_{t+1}$ aumenta em 1.4568 unidades, mantendo $X_{t2}$ constante. Similarmente, o coeficiente 2.0966 indica o efeito de $X_{t2}$ sobre $Y_{t+1}$.

### C√°lculo Pr√°tico dos Momentos
Na pr√°tica, os momentos populacionais $E(Y_{t+1}X_t')$ e $E(X_tX_t')$ s√£o desconhecidos e precisam ser estimados a partir dos dados amostrais dispon√≠veis. A forma mais comum de estimar esses momentos √© usando as m√©dias amostrais. As estimativas s√£o:
$$ \hat{E}(Y_{t+1}X_t') = \frac{1}{T} \sum_{t=1}^T y_{t+1} x_t' $$
$$ \hat{E}(X_tX_t') = \frac{1}{T} \sum_{t=1}^T x_t x_t' $$
Onde $T$ √© o tamanho da amostra. Ao substituir os momentos populacionais por seus equivalentes amostrais, podemos obter uma estimativa do coeficiente de proje√ß√£o linear:
$$ \hat{\alpha}' = \left( \frac{1}{T} \sum_{t=1}^T y_{t+1} x_t' \right) \left( \frac{1}{T} \sum_{t=1}^T x_t x_t' \right)^{-1} $$
Esta formula√ß√£o est√° relacionada ao estimador OLS, conforme j√° vimos anteriormente [^4.1.18], e que converge para o verdadeiro valor da proje√ß√£o linear sob as condi√ß√µes de ergodicidade [^4.1.20].

**Observa√ß√£o 4.1 (Estima√ß√£o por OLS):** Quando aplicado a dados estacion√°rios, o estimador OLS $b$ fornece uma estimativa consistente do coeficiente de proje√ß√£o linear $\alpha$. Isso √© porque o estimador OLS √© baseado em momentos amostrais, que sob ergodicidade, convergem em probabilidade para os momentos populacionais.

**Proposi√ß√£o 4.3 (Consist√™ncia do Estimador OLS):** Sob condi√ß√µes de estacionariedade e ergodicidade, o estimador OLS converge em probabilidade para o coeficiente de proje√ß√£o linear $\alpha$ quando o tamanho da amostra $T$ tende para o infinito.

*Prova:*
I. O estimador OLS minimiza a soma dos quadrados dos erros, que pode ser expressa como $\hat{\alpha} = (\sum_{t=1}^{T} x_t x_t')^{-1} (\sum_{t=1}^{T} y_{t+1} x_t')$.
II. Sob ergodicidade, as m√©dias amostrais convergem para os momentos populacionais, ou seja, $\frac{1}{T} \sum_{t=1}^{T} x_t x_t' \xrightarrow{p} E(X_tX_t')$ e $\frac{1}{T} \sum_{t=1}^{T} y_{t+1} x_t' \xrightarrow{p} E(Y_{t+1}X_t')$.
III. Portanto, o estimador OLS $\hat{\alpha} = (\frac{1}{T}\sum_{t=1}^{T} x_t x_t')^{-1} (\frac{1}{T}\sum_{t=1}^{T} y_{t+1} x_t')$ converge em probabilidade para $E(X_tX_t')^{-1} E(Y_{t+1}X_t')$, que √© exatamente o coeficiente de proje√ß√£o linear $\alpha$. $\blacksquare$

A regress√£o por m√≠nimos quadrados (OLS) pode ser usada para estimar $\alpha$, uma vez que a regress√£o OLS busca o vetor $b$ que minimiza a soma dos quadrados dos erros, $ \sum_{t=1}^{T} (y_{t+1} - b'x_{t})^2 $. Sob certas condi√ß√µes (estacionariedade, ergodicidade), $b$ converge em probabilidade para $\alpha$ quando o tamanho da amostra $T$ tende para o infinito.
> üí° **Exemplo Num√©rico:** Suponha que temos uma s√©rie temporal onde $Y_{t+1} = 2X_{t1} + 3X_{t2} + \epsilon_t$. Vamos simular um conjunto de dados para demonstrar a converg√™ncia do estimador OLS para os verdadeiros coeficientes da proje√ß√£o linear.
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
>
> # Par√¢metros
> T = 1000
> alpha_true = np.array([2, 3])
> np.random.seed(42)
>
> # Simular os dados
> X_t = np.random.rand(T, 2)
> epsilon = np.random.normal(0, 1, T)
> Y_t_plus_1 = np.dot(X_t, alpha_true) + epsilon
>
> # Criar dataframe
> df = pd.DataFrame({'x1': X_t[:, 0], 'x2': X_t[:, 1], 'y_t_plus_1': Y_t_plus_1})
>
> # Regress√£o OLS
> X = df[['x1', 'x2']]
> Y = df['y_t_plus_1']
> model = LinearRegression()
> model.fit(X, Y)
> b = model.coef_
>
> print(f"Coeficientes verdadeiros: {alpha_true}")
> print(f"Coeficientes estimados por OLS: {b}")
> ```
> O c√≥digo produzir√° resultados como:
> ```
> Coeficientes verdadeiros: [2 3]
> Coeficientes estimados por OLS: [1.97965237 2.99914365]
> ```
>
> Este resultado demonstra como o estimador OLS $b$ se aproxima dos verdadeiros coeficientes de proje√ß√£o linear, conforme o tamanho da amostra aumenta. Os coeficientes estimados 1.979 e 2.999 s√£o muito pr√≥ximos dos coeficientes verdadeiros 2 e 3, demonstrando a consist√™ncia do estimador OLS.
>
> Vamos analisar o res√≠duo do modelo:
> ```python
> # Calcular os res√≠duos
> residuals = Y - model.predict(X)
>
> # Calcular a m√©dia dos res√≠duos
> mean_residual = np.mean(residuals)
> print(f"M√©dia dos res√≠duos: {mean_residual}")
>
> # Plotar os res√≠duos
> import matplotlib.pyplot as plt
> plt.scatter(model.predict(X), residuals)
> plt.axhline(y=0, color='r', linestyle='--')
> plt.xlabel("Valores Ajustados")
> plt.ylabel("Res√≠duos")
> plt.title("Gr√°fico de Res√≠duos")
> plt.show()
> ```
> A m√©dia dos res√≠duos √© muito pr√≥xima de zero, confirmando que o modelo OLS n√£o introduz vi√©s. O gr√°fico de res√≠duos mostra uma dispers√£o aleat√≥ria sem padr√£o claro, indicando que o modelo linear se ajusta bem aos dados e os pressupostos da regress√£o OLS s√£o v√°lidos.

### Condi√ß√µes para a Exist√™ncia e Unicidade do Coeficiente
A exist√™ncia e a unicidade do coeficiente de proje√ß√£o linear $\alpha$ dependem da n√£o-singularidade da matriz de covari√¢ncia $E(X_tX_t')$. Se $E(X_tX_t')$ √© singular, o vetor $\alpha$ n√£o √© unicamente determinado, embora a previs√£o $\alpha'X_t$ ainda seja [^4.1.13].

**Proposi√ß√£o 4.2 (Singularidade da Matriz de Covari√¢ncia):** Se a matriz de covari√¢ncia $E(X_tX_t')$ for singular, ent√£o a inversa $[E(X_tX_t')]^{-1}$ n√£o existe, e o coeficiente $\alpha$ n√£o ser√° √∫nico, embora a previs√£o $\alpha'X_t$ ainda seja.

*Prova:*
I. A matriz $E(X_tX_t')$ √© singular se o seu determinante for zero. Uma matriz singular n√£o possui inversa.
II. A f√≥rmula para o coeficiente de proje√ß√£o linear √©:
    $$ \alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} $$
III. Se $E(X_tX_t')$ √© singular, a inversa $[E(X_tX_t')]^{-1}$ n√£o existe, e o coeficiente $\alpha$ n√£o pode ser calculado usando esta f√≥rmula.
IV. No entanto, mesmo que o coeficiente $\alpha$ n√£o seja √∫nico, a previs√£o linear $\alpha'X_t$ ainda pode ser bem definida se ela se anular para todos os valores de $X_t$ que formam a singularidade.
‚ñ†

**Observa√ß√£o 4.2 (Multicolinearidade e Singularidade):** A singularidade da matriz de covari√¢ncia $E(X_tX_t')$ geralmente indica a presen√ßa de multicolinearidade entre as vari√°veis em $X_t$. A multicolinearidade ocorre quando uma ou mais vari√°veis em $X_t$ s√£o aproximadamente combina√ß√µes lineares de outras vari√°veis em $X_t$. Isso torna a matriz de covari√¢ncia quase singular e a estimativa do coeficiente $\alpha$ inst√°vel.

> üí° **Exemplo Num√©rico:** Vamos considerar um caso onde $X_t$ cont√©m duas vari√°veis, $X_{t1}$ e $X_{t2}$, e $X_{t2}=2X_{t1}$. Ent√£o a matriz de covari√¢ncia $E(X_tX_t')$ √© singular:
>
> $$ E(X_tX_t') = E\begin{bmatrix} X_{t1}^2 & X_{t1}X_{t2} \\ X_{t2}X_{t1} & X_{t2}^2 \end{bmatrix} = E\begin{bmatrix} X_{t1}^2 & 2X_{t1}^2 \\ 2X_{t1}^2 & 4X_{t1}^2 \end{bmatrix} $$
>
>  O determinante desta matriz √© zero, indicando que ela √© singular, e n√£o existe uma solu√ß√£o √∫nica para $\alpha$.
>  No entanto, se a singularidade √© causada por uma combina√ß√£o linear entre os preditores, como no exemplo acima, a previs√£o $\alpha' X_t$ ainda pode ser bem definida, mesmo que $\alpha$ n√£o seja unicamente determinado.
> Suponha que $Y_{t+1} = a X_{t1} + b X_{t2} + \epsilon_t$.
> Como $X_{t2} = 2X_{t1}$, podemos reescrever a equa√ß√£o como $Y_{t+1} = (a+2b)X_{t1} + \epsilon_t$ . Portanto, apesar de n√£o ser poss√≠vel obter um $\alpha$ √∫nico, a previs√£o $Y^*_{t+1}$ √© √∫nica.
>
> Para demonstrar isso numericamente, vamos gerar dados com multicolinearidade:
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
>
> # Par√¢metros
> T = 100
> np.random.seed(42)
>
> # Simular os dados
> X_t1 = np.random.rand(T)
> X_t2 = 2 * X_t1 # multicolinearidade perfeita
> X_t = np.column_stack((X_t1, X_t2))
> epsilon = np.random.normal(0, 1, T)
> Y_t_plus_1 = 2 * X_t1 + 3 * X_t2 + epsilon
>
> # Criar dataframe
> df = pd.DataFrame({'x1': X_t1, 'x2': X_t2, 'y_t_plus_1': Y_t_plus_1})
>
> # Regress√£o OLS
> X = df[['x1', 'x2']]
> Y = df['y_t_plus_1']
> model = LinearRegression()
> try:
>   model.fit(X, Y)
>   b = model.coef_
>   print(f"Coeficientes estimados por OLS: {b}")
> except Exception as e:
>   print(f"Erro ao ajustar o modelo: {e}")
>
> # Regress√£o OLS com uma √∫nica vari√°vel (x1)
> X_single = df[['x1']]
> model_single = LinearRegression()
> model_single.fit(X_single, Y)
> b_single = model_single.coef_
> print(f"Coeficientes estimados por OLS (x1 apenas): {b_single}")
> ```
>  O c√≥digo acima demonstra que quando h√° multicolinearidade perfeita, a regress√£o OLS com ambas as vari√°veis resulta em um erro, pois a matriz de covari√¢ncia √© singular. No entanto, a regress√£o OLS com apenas uma das vari√°veis (x1) funciona e estima o coeficiente combinado (2 + 2 * 3 = 8). Isso mostra que, embora o coeficiente $\alpha$ n√£o seja √∫nico, a previs√£o ainda √© consistente.
>
>  Os resultados do c√≥digo acima ser√£o semelhantes a:
>  ```
> Erro ao ajustar o modelo: singular matrix
> Coeficientes estimados por OLS (x1 apenas): [7.99439366]
>  ```
>  Este exemplo mostra numericamente como a multicolinearidade impede o c√°lculo de coeficientes √∫nicos, mas a previs√£o linear pode ser feita usando uma combina√ß√£o linear das vari√°veis.

Quando a matriz $E(X_tX_t')$ √© singular, isso indica que existe multicolinearidade entre as vari√°veis em $X_t$. Ou seja, uma vari√°vel em $X_t$ √© combina√ß√£o linear de outra, e isso torna o coeficiente n√£o √∫nico e a matriz de covari√¢ncia n√£o invert√≠vel.

### Conclus√£o
Neste cap√≠tulo, analisamos em detalhes como o coeficiente de proje√ß√£o linear $\alpha$ √© calculado, mostrando como ele √© definido pela rela√ß√£o entre as matrizes de momentos $E(Y_{t+1}X_t')$ e $E(X_tX_t')$ [^4.1.13]. Exploramos tamb√©m como a regress√£o OLS fornece uma estimativa consistente de $\alpha$ utilizando momentos amostrais, e discutimos as condi√ß√µes para a exist√™ncia e unicidade do coeficiente de proje√ß√£o linear. A compreens√£o destes conceitos √© fundamental para a aplica√ß√£o pr√°tica de modelos de proje√ß√£o linear em diversos contextos, e para entender as propriedades dos estimadores OLS quando usados para esse prop√≥sito. A pr√≥xima etapa ser√° aprofundar o uso do operador de defasagem, e da sua rela√ß√£o com os modelos de proje√ß√£o linear, para prever s√©ries temporais.
### Refer√™ncias
[^4.1.9]: *We now restrict the class of forecasts considered by requiring the forecast
Y to be a linear function of X,:
Y+1 = Œ±ŒÑŒß.*
[^4.1.10]: *Suppose we were to find a value for a such that the forecast error (Y1+1 ‚Äì Œ±ŒÑŒß.)
is uncorrelated with X,:
Œï[(Œ•.+1 ‚Äì Œ±ŒÑŒß.)X] = 0'.*
[^4.1.13]: *Œ±' = E(Y+1X)[E(X,X;)]¬Ø¬π, assuming that E(X,X) is a nonsingular matrix. When E(XX) is singular, the coefficient vector a is not uniquely determined by [4.1.10], though the product of this vector with the explanatory variables, a'X,, is uniquely determined by [4.1.10].*
[^4.1.18]: *The value of Œ≤ that minimizes [4.1.17], denoted b, is the ordinary least squares (OLS) estimate of Œ≤. The formula for b turns out to be
b =  [Œ£ x,x;] [Œ£ x‚ÇÅy‚ÇÅ+] . *
[^4.1.19]: *Comparing the OLS coefficient estimate b in equation [4.1.19] with the linear
projection coefficient a in equation [4.1.13], we see that b is constructed from the
sample moments (1/T)Œ£x,x; and (1/T)Œ£x+1 while a is constructed from population moments E(X,X) and E(X,Y,+1).*
[^4.1.20]: *Thus OLS regression of y‚ÇÅ+1 on x, yields a consistent estimate of the linear projection coefficient.*
<!-- END -->
