## Proje√ß√£o Linear como Aproxima√ß√£o da Esperan√ßa Condicional: C√°lculo de Coeficientes e Condi√ß√µes de Ortogonalidade
### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise da **proje√ß√£o linear** $\alpha'X_t$ como uma alternativa √† esperan√ßa condicional $E(Y_{t+1}|X_t)$, particularmente quando esta √∫ltima √© dif√≠cil de calcular diretamente. O foco central reside no c√°lculo preciso dos coeficientes de proje√ß√£o $\alpha'$ e na explora√ß√£o da condi√ß√£o de ortogonalidade $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$, [^4.1.10] que garante a minimiza√ß√£o do Erro Quadr√°tico M√©dio (MSE).  Este cap√≠tulo tem como objetivo consolidar a compreens√£o matem√°tica e estat√≠stica da proje√ß√£o linear.

###  Proje√ß√£o Linear e a Condi√ß√£o de Ortogonalidade
A proje√ß√£o linear busca encontrar a melhor aproxima√ß√£o linear de $Y_{t+1}$ com base em $X_t$, dada por:
$$ Y^*_{t+1} = \alpha'X_t $$
O vetor de coeficientes $\alpha$ √© determinado pela condi√ß√£o de ortogonalidade, que estabelece que o erro de previs√£o $(Y_{t+1} - \alpha'X_t)$ deve ser n√£o correlacionado com as vari√°veis preditoras $X_t$. Matematicamente, essa condi√ß√£o √© expressa como:
$$ E[(Y_{t+1} - \alpha'X_t)X_t] = 0' $$
Esta condi√ß√£o √© crucial, pois ela garante que a proje√ß√£o linear capture toda a rela√ß√£o linear entre $Y_{t+1}$ e $X_t$, minimizando o MSE dentro da classe de previs√µes lineares [^4.1.2].

**Lema 9.1 (Condi√ß√£o de Ortogonalidade e MSE):** A condi√ß√£o de ortogonalidade $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$ √© equivalente √† minimiza√ß√£o do MSE da proje√ß√£o linear.

*Prova:*
I. O MSE da proje√ß√£o linear √© definido como:
   $$ MSE = E[(Y_{t+1} - \alpha'X_t)^2] $$
II. Expande o quadrado, obtemos:
   $$ MSE = E[Y_{t+1}^2 - 2Y_{t+1}\alpha'X_t + (\alpha'X_t)^2] $$
   $$ MSE = E[Y_{t+1}^2] - 2\alpha'E[Y_{t+1}X_t] + \alpha'E[X_tX_t']\alpha $$
III. Para encontrar o vetor $\alpha$ que minimiza o MSE, calculamos a derivada do MSE em rela√ß√£o a $\alpha$ e igualamos a zero:
    $$ \frac{\partial MSE}{\partial \alpha} = -2E[Y_{t+1}X_t] + 2E[X_tX_t']\alpha = 0'$$
IV. Isolando $\alpha$, encontramos o mesmo resultado que a condi√ß√£o de ortogonalidade:
    $$ E[X_tX_t']\alpha = E[Y_{t+1}X_t] $$
    $$ \alpha' = E[Y_{t+1}X_t'] [E[X_tX_t']]^{-1} $$
V. Portanto, a condi√ß√£o de ortogonalidade garante que o MSE seja minimizado, resultando no melhor coeficiente de proje√ß√£o linear.  $\blacksquare$

Este lema demonstra que a condi√ß√£o de ortogonalidade √© fundamental para garantir que a proje√ß√£o linear seja a melhor poss√≠vel dentro da classe de previs√µes lineares. Ela estabelece a equival√™ncia entre a n√£o correla√ß√£o do erro e a minimiza√ß√£o do MSE.

**Lema 9.1.1 (Unicidade da Solu√ß√£o):** Se a matriz $E[X_tX_t']$ for n√£o singular, ent√£o o vetor de coeficientes $\alpha$ que satisfaz a condi√ß√£o de ortogonalidade e minimiza o MSE √© √∫nico.

*Prova:*
I.  Do Lema 9.1, sabemos que a condi√ß√£o de ortogonalidade leva √† equa√ß√£o $E[X_tX_t']\alpha = E[Y_{t+1}X_t]$.
II.  Se a matriz $E[X_tX_t']$ for n√£o singular (ou seja, seu determinante √© diferente de zero), ent√£o ela possui uma inversa, denotada por $[E[X_tX_t']]^{-1}$.
III. Multiplicando ambos os lados da equa√ß√£o $E[X_tX_t']\alpha = E[Y_{t+1}X_t]$ pela inversa de $E[X_tX_t']$, obtemos:
    $$ [E[X_tX_t']]^{-1}E[X_tX_t']\alpha = [E[X_tX_t']]^{-1}E[Y_{t+1}X_t] $$
IV. Como $[E[X_tX_t']]^{-1}E[X_tX_t']$ √© a matriz identidade, temos:
    $$ \alpha = [E[X_tX_t']]^{-1}E[Y_{t+1}X_t] $$
V. Esta express√£o define um √∫nico valor para $\alpha$ dado que a inversa de $E[X_tX_t']$ √© √∫nica. Portanto, se $E[X_tX_t']$ √© n√£o singular, a solu√ß√£o para $\alpha$ √© √∫nica. $\blacksquare$

Este lema garante que, sob uma condi√ß√£o razo√°vel (a n√£o singularidade da matriz de covari√¢ncia das vari√°veis preditoras), o problema de minimiza√ß√£o do MSE tem uma solu√ß√£o √∫nica, o que √© essencial para a aplica√ß√£o da proje√ß√£o linear.

###  C√°lculo dos Coeficientes de Proje√ß√£o Linear
Como j√° vimos, o vetor de coeficientes $\alpha'$ √© calculado atrav√©s da seguinte f√≥rmula:
$$ \alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} $$
Essa f√≥rmula envolve o c√°lculo de dois momentos populacionais: a matriz de covari√¢ncia cruzada $E(Y_{t+1}X_t')$ e a matriz de covari√¢ncia de $X_t$, $E(X_tX_t')$.

**Lema 9.2 (Express√£o para $\alpha$):** O vetor de coeficientes $\alpha$ pode ser expresso como:
$$ \alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} $$
onde $E(Y_{t+1}X_t')$ representa a matriz de covari√¢ncia entre $Y_{t+1}$ e $X_t$, e $E(X_tX_t')$ √© a matriz de covari√¢ncia de $X_t$.

*Prova:*
I. Partimos da condi√ß√£o de ortogonalidade:
   $$ E[(Y_{t+1} - \alpha'X_t)X_t'] = 0' $$
II. Expandindo a express√£o:
   $$ E[Y_{t+1}X_t'] - E[\alpha'X_tX_t'] = 0' $$
III. Utilizando a propriedade de linearidade da esperan√ßa:
   $$ E[Y_{t+1}X_t'] - \alpha'E[X_tX_t'] = 0' $$
IV. Isolando $\alpha'$, obtemos a f√≥rmula para o vetor de coeficientes da proje√ß√£o linear:
    $$  \alpha'E[X_tX_t'] = E[Y_{t+1}X_t'] $$
    $$  \alpha' = E[Y_{t+1}X_t'][E[X_tX_t']]^{-1} $$
$\blacksquare$

Na pr√°tica, como os momentos populacionais n√£o s√£o conhecidos, utilizamos suas estimativas amostrais para calcular uma estimativa do coeficiente $\alpha$.

**Observa√ß√£o 9.1 (Estimativa de $\alpha$):** Na pr√°tica, o vetor $\alpha$ √© estimado utilizando os momentos amostrais:
$$ \hat{\alpha}' = \left(\frac{1}{T} \sum_{t=1}^{T} Y_{t+1}X_t'\right) \left(\frac{1}{T}\sum_{t=1}^{T} X_tX_t'\right)^{-1} $$

> üí° **Exemplo Num√©rico:** Suponha que temos um modelo onde $Y_{t+1} = 2X_{t1} + 3X_{t2} + \epsilon_t$, e as matrizes de momentos amostrais s√£o:
>
> $$\frac{1}{T}\sum_{t=1}^{T}X_tX_t' = \begin{bmatrix} 4 & 1 \\ 1 & 5 \end{bmatrix}$$
>
> $$\frac{1}{T}\sum_{t=1}^{T} Y_{t+1}X_t' = \begin{bmatrix} 11 & 17 \end{bmatrix}$$
>
> Usando a f√≥rmula, calculamos $\hat{\alpha}'$:
>
> $$\hat{\alpha}' = \begin{bmatrix} 11 & 17 \end{bmatrix} \begin{bmatrix} 4 & 1 \\ 1 & 5 \end{bmatrix}^{-1} = \begin{bmatrix} 11 & 17 \end{bmatrix}  \frac{1}{19} \begin{bmatrix} 5 & -1 \\ -1 & 4 \end{bmatrix} $$
> $$\hat{\alpha}' = \frac{1}{19} \begin{bmatrix} 55 -17 & -11 + 68 \end{bmatrix} = \frac{1}{19} \begin{bmatrix} 38 & 57 \end{bmatrix} = \begin{bmatrix} 2 & 3 \end{bmatrix}$$
> Portanto, $\hat{\alpha} = \begin{bmatrix} 2 \\ 3 \end{bmatrix}$. Nesse caso,  $\hat{\alpha}$ coincide com os coeficientes do modelo verdadeiro, como esperado.

### Proje√ß√£o Linear e Esperan√ßa Condicional
Em casos especiais, quando a esperan√ßa condicional $E(Y_{t+1}|X_t)$ √© uma fun√ß√£o linear de $X_t$, ou seja, $E(Y_{t+1}|X_t) = \beta'X_t$, a proje√ß√£o linear coincide com a esperan√ßa condicional [^4.6.7]. Isso significa que a proje√ß√£o linear fornece a melhor previs√£o poss√≠vel dentro da classe de previs√µes lineares, atingindo o mesmo resultado da esperan√ßa condicional.

**Proposi√ß√£o 9.1 (Proje√ß√£o e Esperan√ßa Condicional):** Se a esperan√ßa condicional de $Y_{t+1}$ dado $X_t$ for linear em $X_t$, ou seja, $E(Y_{t+1}|X_t) = \beta'X_t$, ent√£o a proje√ß√£o linear $\alpha'X_t$ coincide com a esperan√ßa condicional, e $\alpha = \beta$.
*Prova:*
I.  Se $E(Y_{t+1}|X_t)$ √© linear em $X_t$, ent√£o $E(Y_{t+1}|X_t) = \beta'X_t$ para algum vetor $\beta$.
II.   A condi√ß√£o para a proje√ß√£o linear √© que $E[(Y_{t+1} - \alpha'X_t)X_t']=0'$.
III. Tomando a esperan√ßa condicional em rela√ß√£o a $X_t$ temos que  $E[(Y_{t+1} - \beta'X_t)X_t'|X_t] = 0'$.
IV.  Tomando a esperan√ßa incondicional, temos:  $E[E[(Y_{t+1} - \beta'X_t)X_t'|X_t]] =  E[(Y_{t+1} - \beta'X_t)X_t'] = 0'$.
V.  Esta √© a mesma condi√ß√£o da proje√ß√£o linear, o que implica que o coeficiente da proje√ß√£o linear, $\alpha$, coincide com o vetor $\beta$ da esperan√ßa condicional. $\blacksquare$

Essa propriedade √© importante porque garante que, quando a rela√ß√£o entre $Y_{t+1}$ e $X_t$ √© linear na esperan√ßa condicional, a proje√ß√£o linear n√£o perde nenhuma informa√ß√£o relevante para a previs√£o.

**Corol√°rio 9.1 (MSE da Proje√ß√£o Linear em Modelos Lineares):** Se $E(Y_{t+1}|X_t) = \beta'X_t$, ent√£o o MSE da proje√ß√£o linear √© igual ao MSE da esperan√ßa condicional.

*Prova:*
I. Pela Proposi√ß√£o 9.1, sabemos que se $E(Y_{t+1}|X_t) = \beta'X_t$, ent√£o a proje√ß√£o linear $\alpha'X_t$ coincide com a esperan√ßa condicional, ou seja, $\alpha = \beta$.
II.  O MSE da proje√ß√£o linear √© $E[(Y_{t+1} - \alpha'X_t)^2]$.
III. O MSE da esperan√ßa condicional √© $E[(Y_{t+1} - E(Y_{t+1}|X_t))^2]$.
IV. Como $\alpha = \beta$, temos que $E[(Y_{t+1} - \alpha'X_t)^2] = E[(Y_{t+1} - \beta'X_t)^2]$.
V. Substituindo $E(Y_{t+1}|X_t)$ por $\beta'X_t$ em $E[(Y_{t+1} - E(Y_{t+1}|X_t))^2]$, obtemos  $E[(Y_{t+1} - \beta'X_t)^2]$.
VI. Portanto, o MSE da proje√ß√£o linear √© igual ao MSE da esperan√ßa condicional quando a esperan√ßa condicional √© uma fun√ß√£o linear de $X_t$. $\blacksquare$

Este corol√°rio estabelece que quando a rela√ß√£o entre as vari√°veis √© linear, a proje√ß√£o linear √© t√£o eficiente quanto a esperan√ßa condicional, em termos de MSE.

###  Proje√ß√µes Lineares com Termo Constante
Em contextos pr√°ticos, como j√° vimos, a proje√ß√£o linear geralmente inclui um termo constante, de forma que a proje√ß√£o linear √© da forma:
$$ Y^*_{t+1} = \beta_0 + \beta'X_t $$
Para obter os coeficientes, o vetor de vari√°veis preditoras √© ampliado, de forma que a proje√ß√£o possa ser escrita como
$$ Y^*_{t+1} = \alpha'Z_t $$
onde $Z_t = \begin{bmatrix} 1 \\ X_t \end{bmatrix}$, e $\alpha = \begin{bmatrix} \beta_0 \\ \beta \end{bmatrix}$. A condi√ß√£o de ortogonalidade neste caso, √© dada por:
$$ E[(Y_{t+1} - \alpha'Z_t)Z_t] = 0' $$
onde o erro de previs√£o deve ser ortogonal ao vetor $Z_t$, que inclui o termo constante e $X_t$.

**Lema 9.3 (Proje√ß√£o com Termo Constante):** Quando inclu√≠mos um termo constante na proje√ß√£o linear, o coeficiente $\alpha'$ pode ser calculado usando o vetor $Z_t = [1, X_t']'$, utilizando a condi√ß√£o de ortogonalidade $E[(Y_{t+1} - \alpha'Z_t)Z_t] = 0'$.

*Prova:*
I. Definindo a proje√ß√£o linear com um termo constante como $Y^*_{t+1} = \beta_0 + \beta'X_t = \alpha'Z_t$ onde $\alpha' = [\beta_0, \beta']$ e $Z_t = [1, X_t']'$.
II. A condi√ß√£o de ortogonalidade implica que
$$ E[(Y_{t+1} - \alpha'Z_t)Z_t] = 0' $$
III. Substituindo $Z_t$ e $\alpha$:
   $$ E\left[\left(Y_{t+1} - (\beta_0 + \beta'X_t)\right) \begin{bmatrix} 1 \\ X_t \end{bmatrix}\right] = \begin{bmatrix} 0 \\ 0' \end{bmatrix} $$
IV. Isso resulta em duas equa√ß√µes:
  $$ E[Y_{t+1} - (\beta_0 + \beta'X_t)] = 0 $$
   $$ E[(Y_{t+1} - (\beta_0 + \beta'X_t))X_t'] = 0' $$
V. A primeira equa√ß√£o implica que $E[Y_{t+1}] - \beta_0 - \beta'E[X_t] = 0$.  Logo,  $\beta_0 = E[Y_{t+1}] - \beta'E[X_t]$.
VI.  A segunda equa√ß√£o, em conjunto com a primeira, implica que $E[(Y_{t+1} - E[Y_{t+1}] - \beta'(X_t - E[X_t]))X_t']=0'$.
VII. Portanto,
   $$ \beta' =  E[(Y_{t+1}- E[Y_{t+1}]) (X_t - E[X_t])'] [E[(X_t- E[X_t]) (X_t - E[X_t])']]^{-1} $$
VIII. Portanto, os coeficientes $\beta_0$ e $\beta$ podem ser obtidos usando os momentos populacionais e a condi√ß√£o de ortogonalidade. $\blacksquare$

Esta generaliza√ß√£o permite que a proje√ß√£o linear capture um n√≠vel b√°sico de $Y_{t+1}$ que n√£o depende de $X_t$, aumentando a flexibilidade do modelo.

**Observa√ß√£o 9.2 (Estimativa dos coeficientes com constante):** Na pr√°tica, os coeficientes com termo constante s√£o estimados usando regress√£o OLS, que encontra o intercepto e o vetor de coeficientes que minimizam o erro quadr√°tico m√©dio da proje√ß√£o.

> üí° **Exemplo Num√©rico:** Considere um cen√°rio onde $Y_{t+1}$ √© o pre√ßo de uma a√ß√£o no dia $t+1$, e $X_t$ √© o pre√ßo da mesma a√ß√£o no dia $t$. Suponha que temos dados amostrais para 5 dias. As seguintes matrizes de momentos amostrais s√£o calculadas:
>
> $$ \frac{1}{T}\sum_{t=1}^{T} Z_tZ_t' = \begin{bmatrix} 5 & 15 \\ 15 & 50 \end{bmatrix} $$
>
> $$ \frac{1}{T}\sum_{t=1}^{T} Y_{t+1}Z_t' = \begin{bmatrix} 30 & 100 \end{bmatrix} $$
>
> Aqui, $Z_t = \begin{bmatrix} 1 \\ X_t \end{bmatrix}$. Vamos calcular os coeficientes $\hat{\alpha} = \begin{bmatrix} \hat{\beta_0} \\ \hat{\beta} \end{bmatrix}$:
>
> $$\hat{\alpha}' = \begin{bmatrix} 30 & 100 \end{bmatrix} \begin{bmatrix} 5 & 15 \\ 15 & 50 \end{bmatrix}^{-1} = \begin{bmatrix} 30 & 100 \end{bmatrix} \frac{1}{25}\begin{bmatrix} 50 & -15 \\ -15 & 5 \end{bmatrix} $$
>
> $$\hat{\alpha}' = \frac{1}{25} \begin{bmatrix} 1500 - 1500 & -450 + 500 \end{bmatrix} = \frac{1}{25} \begin{bmatrix} 0 & 50 \end{bmatrix} = \begin{bmatrix} 0 & 2 \end{bmatrix} $$
>
> Assim, $\hat{\beta_0} = 0$ e $\hat{\beta} = 2$. Isso sugere que o pre√ßo da a√ß√£o no dia $t+1$ √© aproximadamente duas vezes o pre√ßo do dia anterior.

**Lema 9.4 (Decomposi√ß√£o do MSE):** O MSE da proje√ß√£o linear com termo constante pode ser decomposto em uma parte explicada pela proje√ß√£o e uma parte n√£o explicada.

*Prova:*
I. Seja $Y_{t+1}^* = \beta_0 + \beta'X_t$ a proje√ß√£o linear com termo constante.
II. O MSE da proje√ß√£o linear √© dado por $MSE = E[(Y_{t+1} - Y_{t+1}^*)^2]$.
III. Adicionando e subtraindo a m√©dia de $Y_{t+1}$, $E[Y_{t+1}]$, obtemos:
   $$ MSE = E[(Y_{t+1} - E[Y_{t+1}] + E[Y_{t+1}] - Y_{t+1}^*)^2] $$
IV. Expandindo o quadrado, temos:
$$ MSE = E[(Y_{t+1} - E[Y_{t+1}])^2] + E[(E[Y_{t+1}] - Y_{t+1}^*)^2] + 2E[(Y_{t+1} - E[Y_{t+1}])(E[Y_{t+1}] - Y_{t+1}^*)] $$
V. O √∫ltimo termo da expans√£o √© nulo, pois $E[Y_{t+1}] - Y_{t+1}^* = E[Y_{t+1}] - (\beta_0 + \beta'X_t)$ √© uma fun√ß√£o de $X_t$, e pelo Lema 9.3 sabemos que  $E[(Y_{t+1} - (\beta_0 + \beta'X_t))X_t']=0'$, e $E[(Y_{t+1} - (\beta_0 + \beta'X_t))] = 0$, o que implica que $E[(Y_{t+1} - E[Y_{t+1}])(E[Y_{t+1}] - Y_{t+1}^*)] = 0$.
VI. Portanto, temos que
    $$ MSE = E[(Y_{t+1} - E[Y_{t+1}])^2] + E[(E[Y_{t+1}] - Y_{t+1}^*)^2] $$
VII. O termo $E[(Y_{t+1} - E[Y_{t+1}])^2]$ representa a vari√¢ncia total de $Y_{t+1}$, e o termo $E[(E[Y_{t+1}] - Y_{t+1}^*)^2]$ representa a vari√¢ncia da proje√ß√£o linear. O termo que representa a vari√¢ncia da proje√ß√£o linear, quando igual a zero, implica que a proje√ß√£o √© uma constante, ou seja, que o vetor $\beta$ √© nulo.  $\blacksquare$

> üí° **Exemplo Num√©rico:**  Vamos usar o exemplo num√©rico anterior com a proje√ß√£o com termo constante, e os valores calculados para $\hat{\beta_0} = 0$ e $\hat{\beta} = 2$ , $Y^*_{t+1} = 0 + 2X_t$. Suponha que a vari√¢ncia total de $Y_{t+1}$, $E[(Y_{t+1} - E[Y_{t+1}])^2]$, seja 20. E que $E[(E[Y_{t+1}] - Y_{t+1}^*)^2]$ seja igual a 15. Ent√£o, o MSE da proje√ß√£o linear √© $MSE = 20 - 15 = 5$. Isso significa que a proje√ß√£o linear explica 15 unidades da variabilidade total de $Y_{t+1}$ e a parte n√£o explicada √© de 5 unidades.

Esta decomposi√ß√£o √© √∫til para entender a parcela da variabilidade de $Y_{t+1}$ que √© capturada pela proje√ß√£o linear, e a parcela que n√£o √©.

### Conclus√£o
Neste cap√≠tulo, aprofundamos nossa compreens√£o da proje√ß√£o linear como uma ferramenta poderosa e vers√°til, focando no c√°lculo preciso dos coeficientes $\alpha$ e explorando a condi√ß√£o de ortogonalidade que garante a minimiza√ß√£o do MSE. Vimos como a proje√ß√£o linear oferece uma aproxima√ß√£o computacionalmente trat√°vel da esperan√ßa condicional, e como ela coincide com a esperan√ßa condicional quando esta √© linear. As propriedades e conceitos discutidos neste cap√≠tulo s√£o cruciais para a aplica√ß√£o de modelos de proje√ß√£o linear em diversas √°reas, incluindo a modelagem de s√©ries temporais e modelagem econom√©trica.  A pr√≥xima se√ß√£o explorar√° como esses conceitos se generalizam para a previs√£o de vetores.
### Refer√™ncias
[^4.1.2]: *The forecast with the smallest mean squared error turns out to be the expectation of Y.+1 conditional on X‚ÇÅ:
Y*+1 = E(Y1+1/Œß.).*
[^4.1.9]: *We now restrict the class of forecasts considered by requiring the forecast
Y to be a linear function of X,:
Y+1 = Œ±ŒÑŒß.*
[^4.1.10]: *Suppose we were to find a value for a such that the forecast error (Y1+1 ‚Äì Œ±ŒÑŒß.)
is uncorrelated with X,:
Œï[(Œ•.+1 ‚Äì Œ±ŒÑŒß.)X] = 0'.*
<!-- END -->
