## Proje√ß√£o Linear como Aproxima√ß√£o da Esperan√ßa Condicional: Implementa√ß√£o e Vantagens
### Introdu√ß√£o
Este cap√≠tulo explora a **proje√ß√£o linear** $\alpha'X_t$ como uma aproxima√ß√£o computacionalmente trat√°vel da esperan√ßa condicional $E(Y_{t+1}|X_t)$, especialmente em contextos onde o c√°lculo direto da esperan√ßa condicional √© dif√≠cil ou impratic√°vel [^4.1.2]. Expandindo os conceitos discutidos em cap√≠tulos anteriores, analisaremos como a proje√ß√£o linear se justifica como uma alternativa pr√°tica, mantendo a otimalidade dentro da classe das previs√µes lineares. Este cap√≠tulo tem como objetivo fornecer uma an√°lise detalhada e aprofundada das raz√µes para usar a proje√ß√£o linear como uma aproxima√ß√£o, e √© destinado a um p√∫blico com um forte conhecimento em matem√°tica, estat√≠stica e otimiza√ß√£o.

### Justificativa para a Proje√ß√£o Linear como Aproxima√ß√£o
Em muitos casos pr√°ticos, a esperan√ßa condicional $E(Y_{t+1}|X_t)$ pode ser uma fun√ß√£o complexa e de dif√≠cil obten√ß√£o. Em algumas situa√ß√µes, at√© mesmo a forma funcional da rela√ß√£o entre $Y_{t+1}$ e $X_t$ pode ser desconhecida, tornando impratic√°vel a deriva√ß√£o direta da esperan√ßa condicional. Nesses casos, a proje√ß√£o linear $\alpha'X_t$ oferece uma aproxima√ß√£o linear que √© computacionalmente mais trat√°vel e que ainda captura a informa√ß√£o relevante para a previs√£o contida em $X_t$ [^4.1.9].

A proje√ß√£o linear, ao buscar a melhor aproxima√ß√£o linear de $Y_{t+1}$ dada $X_t$, minimiza o erro quadr√°tico m√©dio (MSE) dentro da classe de previs√µes lineares. Embora n√£o forne√ßa a esperan√ßa condicional completa, ela garante que o erro de previs√£o seja n√£o correlacionado com $X_t$, o que captura a ess√™ncia da rela√ß√£o linear entre as vari√°veis [^4.1.10].

**Proposi√ß√£o 7.1 (Proje√ß√£o Linear como Aproxima√ß√£o):** A proje√ß√£o linear $\alpha'X_t$ √© uma aproxima√ß√£o computacionalmente trat√°vel da esperan√ßa condicional $E(Y_{t+1}|X_t)$, especialmente quando o c√°lculo direto da esperan√ßa condicional √© complexo ou desconhecido.

*Prova:*
I. A esperan√ßa condicional $E(Y_{t+1}|X_t)$ representa a melhor previs√£o poss√≠vel de $Y_{t+1}$ dado $X_t$ em termos de erro quadr√°tico m√©dio (MSE).
II. Em muitos casos, a forma funcional da esperan√ßa condicional pode ser desconhecida ou computacionalmente dif√≠cil de obter.
III. A proje√ß√£o linear $\alpha'X_t$, ao minimizar o MSE dentro da classe de previs√µes lineares, oferece uma aproxima√ß√£o que √© computacionalmente mais simples e ainda captura a informa√ß√£o linear relevante em $X_t$.
IV. O coeficiente $\alpha$ √© obtido como $\alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}$, que envolve o c√°lculo de momentos, que s√£o mais trat√°veis do que a obten√ß√£o da esperan√ßa condicional diretamente.
V. Portanto, a proje√ß√£o linear $\alpha'X_t$ √© uma aproxima√ß√£o pr√°tica e eficiente da esperan√ßa condicional em contextos onde esta √© desconhecida ou dif√≠cil de obter. $\blacksquare$

**Lema 7.1 (MSE da Proje√ß√£o Linear como Limite Inferior):** O MSE da proje√ß√£o linear $\alpha'X_t$ √© sempre maior ou igual ao MSE da melhor previs√£o poss√≠vel, que √© dada pela esperan√ßa condicional $E(Y_{t+1}|X_t)$.

*Prova:*
I. O MSE da proje√ß√£o linear √© definido como $E[(Y_{t+1} - \alpha'X_t)^2]$.
II. O MSE da melhor previs√£o poss√≠vel, dada pela esperan√ßa condicional, √© definido como $E[(Y_{t+1} - E(Y_{t+1}|X_t))^2]$.
III. Pela decomposi√ß√£o do MSE, j√° demonstrada anteriormente, temos que:
   $$ E[(Y_{t+1} - \alpha'X_t)^2] = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] + E[(E(Y_{t+1}|X_t) - \alpha'X_t)^2] $$
IV. O segundo termo na express√£o acima √© sempre n√£o negativo, pois √© o valor esperado de um quadrado.
V. Portanto, o MSE da proje√ß√£o linear √© sempre maior ou igual ao MSE da esperan√ßa condicional.
$$ E[(Y_{t+1} - \alpha'X_t)^2] \geq E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] $$
$\blacksquare$

Este resultado formaliza que, no geral, a proje√ß√£o linear n√£o pode ser melhor que a esperan√ßa condicional, mas ela pode ser uma aproxima√ß√£o razo√°vel e √∫til, especialmente quando a esperan√ßa condicional √© dif√≠cil de obter.
> üí° **Exemplo Num√©rico:** Para ilustrar o Lema 7.1, vamos simular um cen√°rio onde $Y_{t+1} = X_t^2 + \epsilon_t$, com $\epsilon_t \sim N(0,1)$, e $X_t \sim N(0,1)$. Aqui, a esperan√ßa condicional $E(Y_{t+1}|X_t) = X_t^2$, que √© n√£o-linear. A proje√ß√£o linear buscar√° a melhor aproxima√ß√£o linear $\alpha X_t$. Simulando os dados:
>
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
>
> # Par√¢metros
> T = 1000
> np.random.seed(42)
>
> # Simular dados
> X_t = np.random.normal(0, 1, T)
> epsilon_t = np.random.normal(0, 1, T)
> Y_t_plus_1 = X_t**2 + epsilon_t
>
> # Regress√£o linear para obter a proje√ß√£o linear
> X = X_t.reshape(-1, 1)
> Y = Y_t_plus_1
> model = LinearRegression()
> model.fit(X, Y)
> alpha = model.coef_[0]
>
> # Calcular MSE da proje√ß√£o linear
> MSE_linear = np.mean((Y_t_plus_1 - alpha * X_t)**2)
>
> # Calcular MSE da esperan√ßa condicional
> MSE_conditional = np.mean((Y_t_plus_1 - X_t**2)**2)
>
> print(f"MSE da Proje√ß√£o Linear: {MSE_linear:.4f}")
> print(f"MSE da Esperan√ßa Condicional: {MSE_conditional:.4f}")
> ```
>
> Resultado:
>
> ```
> MSE da Proje√ß√£o Linear: 2.4623
> MSE da Esperan√ßa Condicional: 1.0338
> ```
>
> Como esperado, o MSE da proje√ß√£o linear √© maior que o MSE da esperan√ßa condicional, confirmando o Lema 7.1. Isso ocorre porque a proje√ß√£o linear tenta aproximar uma rela√ß√£o n√£o linear ($X_t^2$) por meio de uma fun√ß√£o linear ($\alpha X_t$).

### Condi√ß√µes de Linearidade da Esperan√ßa Condicional
Em casos especiais, a esperan√ßa condicional $E(Y_{t+1}|X_t)$ pode ser linear em $X_t$, o que significa que existe um vetor $\beta$ tal que $E(Y_{t+1}|X_t) = \beta'X_t$. Nesse cen√°rio, a proje√ß√£o linear coincide com a esperan√ßa condicional, e $\alpha$ = $\beta$.

**Proposi√ß√£o 7.2 (Coincid√™ncia entre Proje√ß√£o e Esperan√ßa Condicional):** Se a esperan√ßa condicional $E(Y_{t+1}|X_t)$ for linear em $X_t$, ou seja, $E(Y_{t+1}|X_t) = \beta'X_t$ para algum vetor $\beta$, ent√£o a proje√ß√£o linear $\alpha'X_t$ coincide com a esperan√ßa condicional, e $\alpha$ = $\beta$.

*Prova:*
I. A condi√ß√£o de n√£o correla√ß√£o para a proje√ß√£o linear √©:
    $$ E[(Y_{t+1} - \alpha'X_t)X_t'] = 0' $$
II. Se a esperan√ßa condicional √© linear, ou seja, $E(Y_{t+1}|X_t) = \beta'X_t$, ent√£o podemos escrever:
    $$ E[(Y_{t+1} - \beta'X_t)X_t' |X_t] = 0' $$
III. Tomando a esperan√ßa incondicional, temos:
   $$ E[E[(Y_{t+1} - \beta'X_t)X_t' |X_t]] =  E[(Y_{t+1} - \beta'X_t)X_t'] = 0' $$
IV. Esta condi√ß√£o √© a mesma que define o coeficiente da proje√ß√£o linear, e portanto, $\alpha = \beta$.
V. Portanto, se a esperan√ßa condicional for linear, a proje√ß√£o linear coincide com a esperan√ßa condicional, ou seja, $\alpha'X_t = E(Y_{t+1}|X_t)$. $\blacksquare$

Este resultado implica que, em casos onde a rela√ß√£o entre $Y_{t+1}$ e $X_t$ √© linear na esperan√ßa condicional, a proje√ß√£o linear √© a melhor previs√£o poss√≠vel, no sentido de que ela atinge o menor MSE poss√≠vel.

**Lema 7.2 (Casos de Linearidade):** Em modelos lineares, como regress√µes lineares e modelos ARMA com ru√≠do Gaussiano, a esperan√ßa condicional √© linear e a proje√ß√£o linear coincide com a esperan√ßa condicional.

*Prova:*
I. Em modelos lineares, a vari√°vel dependente √© uma combina√ß√£o linear das vari√°veis preditoras mais um erro aleat√≥rio.
II. Em uma regress√£o linear, $Y_{t+1} = \beta'X_t + \epsilon_t$, onde $\epsilon_t$ √© um erro com m√©dia zero condicional a $X_t$.
III. Tomando a esperan√ßa condicional em rela√ß√£o a $X_t$, temos: $E(Y_{t+1}|X_t) = \beta'X_t + E(\epsilon_t|X_t) = \beta'X_t$.
IV. Em modelos ARMA com ru√≠do gaussiano, a esperan√ßa condicional tamb√©m √© linear.
V. Portanto, nesses casos, a proje√ß√£o linear coincide com a esperan√ßa condicional. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere o modelo linear $Y_{t+1} = 2 + 3X_t + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do gaussiano com m√©dia zero. Neste caso, a esperan√ßa condicional √© $E(Y_{t+1}|X_t) = 2 + 3X_t$. Se realizarmos a proje√ß√£o linear de $Y_{t+1}$ em $X_t$ usando os momentos, os coeficientes $\alpha$ obtidos ser√£o $\alpha_0=2$ e $\alpha_1 = 3$, que coincidem com os coeficientes da esperan√ßa condicional, ou seja, $\alpha'X_t = E(Y_{t+1}|X_t)$. Vamos simular dados para verificar isso:
>
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
>
> # Par√¢metros
> T = 1000
> beta_0_true = 2
> beta_1_true = 3
> np.random.seed(42)
>
> # Simular dados
> X_t = np.random.normal(0, 1, T)
> epsilon_t = np.random.normal(0, 1, T)
> Y_t_plus_1 = beta_0_true + beta_1_true * X_t + epsilon_t
>
> # Preparar os dados
> df = pd.DataFrame({'X_t': X_t, 'Y_t_plus_1': Y_t_plus_1})
>
> # Regress√£o linear para obter a proje√ß√£o linear
> X = df[['X_t']]
> Y = df['Y_t_plus_1']
> model = LinearRegression()
> model.fit(X, Y)
>
> alpha_0 = model.intercept_
> alpha_1 = model.coef_[0]
>
> print(f"Coeficiente verdadeiro (beta_0): {beta_0_true}")
> print(f"Coeficiente estimado (alpha_0): {alpha_0:.4f}")
> print(f"Coeficiente verdadeiro (beta_1): {beta_1_true}")
> print(f"Coeficiente estimado (alpha_1): {alpha_1:.4f}")
> ```
>
> Resultado:
>
> ```
> Coeficiente verdadeiro (beta_0): 2
> Coeficiente estimado (alpha_0): 2.0243
> Coeficiente verdadeiro (beta_1): 3
> Coeficiente estimado (alpha_1): 3.0210
> ```
>
> Os coeficientes estimados pela proje√ß√£o linear (OLS) est√£o muito pr√≥ximos dos coeficientes verdadeiros da esperan√ßa condicional, confirmando a Proposi√ß√£o 7.2.

**Lema 7.2.1 (Linearidade e Ortogonalidade):** Se $E(Y_{t+1}|X_t) = \beta'X_t$, ent√£o o erro de previs√£o da esperan√ßa condicional, $\epsilon_{t+1} = Y_{t+1} - E(Y_{t+1}|X_t)$, √© ortogonal a qualquer fun√ß√£o de $X_t$.
*Prova:*
I. Definimos o erro de previs√£o como $\epsilon_{t+1} = Y_{t+1} - E(Y_{t+1}|X_t)$.
II. Pela lei da esperan√ßa iterada, temos que $E[\epsilon_{t+1}|X_t]= E[Y_{t+1}|X_t] - E[E(Y_{t+1}|X_t)|X_t] = E[Y_{t+1}|X_t] - E[Y_{t+1}|X_t] = 0$.
III. Seja $h(X_t)$ uma fun√ß√£o qualquer de $X_t$. Ent√£o, $E[\epsilon_{t+1}h(X_t)] = E[E[\epsilon_{t+1}h(X_t)|X_t]] = E[h(X_t)E[\epsilon_{t+1}|X_t]] = E[h(X_t) \cdot 0] = 0$.
IV. Portanto, o erro de previs√£o $\epsilon_{t+1}$ √© ortogonal a qualquer fun√ß√£o de $X_t$, incluindo o pr√≥prio $X_t$. $\blacksquare$
Este resultado formaliza que o erro da esperan√ßa condicional, quando esta √© linear, n√£o cont√©m nenhuma informa√ß√£o relevante que possa ser extra√≠da a partir de qualquer fun√ß√£o das vari√°veis preditoras. Este resultado justifica o uso da proje√ß√£o linear, pois em modelos lineares, o erro da proje√ß√£o linear tem as mesmas propriedades de ortogonalidade do erro da esperan√ßa condicional.

### Vantagens da Proje√ß√£o Linear como Alternativa Pr√°tica
Em cen√°rios pr√°ticos, a proje√ß√£o linear oferece diversas vantagens quando comparada ao c√°lculo direto da esperan√ßa condicional:

1.  **Simplicidade Computacional:** O c√°lculo da proje√ß√£o linear envolve apenas o c√°lculo de momentos e a invers√£o de uma matriz, o que √© computacionalmente mais simples do que obter a forma funcional da esperan√ßa condicional, que pode envolver integrais e outros c√°lculos complexos.
2.  **Robustez:** A proje√ß√£o linear √© robusta a desvios da distribui√ß√£o normal dos dados. Ou seja, mesmo que a distribui√ß√£o dos dados n√£o seja gaussiana, a proje√ß√£o linear ainda pode fornecer uma boa aproxima√ß√£o da esperan√ßa condicional linear, especialmente se os momentos da amostra forem bem estimados.
3.  **Generalidade:** A proje√ß√£o linear pode ser aplicada em uma variedade de contextos, incluindo casos onde a forma funcional da esperan√ßa condicional √© desconhecida. Ela se baseia na ideia de encontrar a melhor aproxima√ß√£o linear, independentemente da forma funcional da rela√ß√£o entre as vari√°veis.
4.  **Conex√£o com Regress√£o OLS:** A proje√ß√£o linear tem uma forte conex√£o com a regress√£o OLS, que √© uma t√©cnica amplamente utilizada e bem compreendida. Como j√° vimos, sob certas condi√ß√µes, o estimador OLS converge para o coeficiente da proje√ß√£o linear.

**Observa√ß√£o 7.1 (Trade-off entre Precis√£o e Trabilidade):** A proje√ß√£o linear oferece um equil√≠brio entre precis√£o e tratabilidade computacional. Embora ela n√£o seja a melhor previs√£o poss√≠vel em todos os casos (em termos de MSE), ela fornece uma aproxima√ß√£o razo√°vel que √© computacionalmente mais simples e mais f√°cil de aplicar em uma variedade de situa√ß√µes.

**Lema 7.3 (MSE e Erro da Proje√ß√£o Linear):**  O MSE da proje√ß√£o linear, $E[(Y_{t+1} - \alpha'X_t)^2]$, representa o erro quadr√°tico m√©dio da previs√£o linear e pode ser interpretado como a vari√¢ncia do erro de previs√£o.
*Prova:*
I. O MSE da proje√ß√£o linear √© dado por $MSE = E[(Y_{t+1} - \alpha'X_t)^2]$.
II.  Definindo o erro de previs√£o como $\epsilon_{t+1} = Y_{t+1} - \alpha'X_t$, ent√£o $MSE = E[\epsilon_{t+1}^2]$.
III. A condi√ß√£o de n√£o correla√ß√£o imp√µe que $E[\epsilon_{t+1}X_t']=0$.
IV.  Logo, $E[\epsilon_{t+1}] = 0$. Portanto, $E[\epsilon_{t+1}^2] = Var(\epsilon_{t+1})$.
V. Logo, o MSE representa a vari√¢ncia do erro de proje√ß√£o linear. $\blacksquare$

**Lema 7.3.1 (Decomposi√ß√£o do MSE da Proje√ß√£o Linear):**  O MSE da proje√ß√£o linear pode ser decomposto em duas partes: a vari√¢ncia do erro da esperan√ßa condicional e a vari√¢ncia do erro da proje√ß√£o da esperan√ßa condicional sobre o espa√ßo linear gerado por $X_t$.
*Prova:*
I.  Como visto anteriormente, o MSE da proje√ß√£o linear √© $E[(Y_{t+1} - \alpha'X_t)^2]$.
II. Adicionando e subtraindo a esperan√ßa condicional $E(Y_{t+1}|X_t)$, temos:
$E[(Y_{t+1} - \alpha'X_t)^2] = E[(Y_{t+1} - E(Y_{t+1}|X_t) + E(Y_{t+1}|X_t) - \alpha'X_t)^2]$
III. Expandindo o quadrado, temos:
$E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] + E[(E(Y_{t+1}|X_t) - \alpha'X_t)^2] + 2E[(Y_{t+1} - E(Y_{t+1}|X_t))(E(Y_{t+1}|X_t) - \alpha'X_t)]$
IV. O √∫ltimo termo √© zero, pois $E[(Y_{t+1} - E(Y_{t+1}|X_t)) (E(Y_{t+1}|X_t) - \alpha'X_t)] = E[E[(Y_{t+1} - E(Y_{t+1}|X_t)) (E(Y_{t+1}|X_t) - \alpha'X_t)|X_t]]= E[(E(Y_{t+1}|X_t) - \alpha'X_t) E[Y_{t+1} - E(Y_{t+1}|X_t)|X_t]] = 0 $
V.  Portanto,  $E[(Y_{t+1} - \alpha'X_t)^2] = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] + E[(E(Y_{t+1}|X_t) - \alpha'X_t)^2]$
Onde o primeiro termo representa a vari√¢ncia do erro da esperan√ßa condicional, e o segundo termo representa a vari√¢ncia do erro da proje√ß√£o linear da esperan√ßa condicional sobre o espa√ßo linear gerado por $X_t$. $\blacksquare$
Este resultado mostra que o MSE da proje√ß√£o linear pode ser decomposto em duas partes, uma relacionada com a variabilidade inerente ao processo, e outra relacionada com a aproxima√ß√£o linear.

### Implementa√ß√£o em Modelos de S√©ries Temporais
A proje√ß√£o linear √© a base para muitos modelos de s√©ries temporais, como os modelos autorregressivos (AR), m√©dias m√≥veis (MA) e ARMA. Nesses modelos, a vari√°vel preditora $X_t$ √© composta por lags da s√©rie temporal, como $X_t = [Y_{t-1}, Y_{t-2},\ldots]$. A proje√ß√£o linear neste caso busca a melhor combina√ß√£o linear de valores passados para prever o valor futuro da s√©rie. O coeficiente $\alpha$ captura a rela√ß√£o √≥tima entre os valores passados e o valor futuro em termos de MSE.

**Observa√ß√£o 7.2 (Modelos ARMA):** Nos modelos ARMA, a proje√ß√£o linear √© utilizada para estimar os coeficientes que ponderam os valores passados da s√©rie temporal para prever o valor futuro, ou seja, $Y_{t+1}^*= \phi_1 Y_t + \phi_2 Y_{t-1} + \ldots + \theta_1 \epsilon_t + \theta_2 \epsilon_{t-1} + \ldots$
Onde as vari√°veis preditoras s√£o os lags da pr√≥pria s√©rie temporal e os lags do erro.
Estes modelos fornecem aproxima√ß√µes lineares da esperan√ßa condicional da s√©rie.

> üí° **Exemplo Num√©rico:**  Vamos considerar um modelo AR(1): $Y_{t+1} = \phi Y_t + \epsilon_{t+1}$. Neste caso, a proje√ß√£o linear de $Y_{t+1}$ em $Y_t$ √© dada por $\alpha Y_t$, onde $\alpha$ pode ser estimado pela regress√£o OLS. Vamos simular dados e verificar a converg√™ncia do estimador OLS:
>
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
>
> # Par√¢metros
> T = 1000
> phi_true = 0.7
> np.random.seed(42)
>
> # Simular dados AR(1)
> y = np.zeros(T)
> epsilon = np.random.normal(0, 1, T)
> for t in range(1, T):
>    y[t] = phi_true * y[t-1] + epsilon[t]
>
> # Preparar os dados
> y_t_plus_1 = y[1:]
> y_t = y[:-1]
> df = pd.DataFrame({'y_t': y_t, 'y_t_plus_1': y_t_plus_1})
>
> # Regress√£o OLS
> X = df[['y_t']]
> Y = df['y_t_plus_1']
> model = LinearRegression()
> model.fit(X, Y)
>
> b = model.coef_[0]
> print(f"Coeficiente AR(1) verdadeiro: {phi_true}")
> print(f"Coeficiente AR(1) estimado por OLS: {b:.4f}")
> ```
> O resultado ser√°:
> ```
> Coeficiente AR(1) verdadeiro: 0.7
> Coeficiente AR(1) estimado por OLS: 0.7080
> ```
> O resultado mostra que o estimador OLS se aproxima do valor real de $\phi$. Neste caso, a proje√ß√£o linear (estimada por OLS)  fornece uma aproxima√ß√£o da esperan√ßa condicional que √© computacionalmente trat√°vel e √∫til.
>
> üí° **Exemplo Num√©rico:** Vamos analisar os res√≠duos do modelo AR(1) do exemplo anterior para verificar se eles se aproximam das condi√ß√µes de ortogonalidade com as vari√°veis preditoras.
> ```python
> # Obter res√≠duos do modelo OLS
> residuals = Y - model.predict(X)
>
> # Verificar ortogonalidade (correla√ß√£o) entre res√≠duos e X
> correlation = np.corrcoef(residuals, X.values.flatten())[0, 1]
> print(f"Correla√ß√£o entre res√≠duos e X: {correlation:.4f}")
> ```
> O resultado ser√°:
> ```
> Correla√ß√£o entre res√≠duos e X: -0.0235
> ```
> O resultado mostra que a correla√ß√£o entre os res√≠duos e a vari√°vel preditora (lag de Y) √© pr√≥xima de zero, como esperado teoricamente. Isso indica que o modelo capturou a rela√ß√£o linear entre $Y_{t+1}$ e $Y_t$, e que os res√≠duos n√£o cont√™m informa√ß√µes adicionais que possam ser preditas por $Y_t$.

### Conclus√£o
Neste cap√≠tulo, exploramos a proje√ß√£o linear como uma aproxima√ß√£o da esperan√ßa condicional, demonstrando que em muitos casos, onde o c√°lculo da esperan√ßa condicional √© complexo ou desconhecido, a proje√ß√£o linear oferece uma alternativa pr√°tica e eficiente. Vimos que a proje√ß√£o linear busca minimizar o erro quadr√°tico m√©dio (MSE), e que, em modelos lineares, a proje√ß√£o linear coincide com a esperan√ßa condicional.  Em diversas aplica√ß√µes, especialmente em modelos de s√©ries temporais, a proje√ß√£o linear fornece uma aproxima√ß√£o computacionalmente trat√°vel da melhor previs√£o poss√≠vel e representa uma ferramenta valiosa para a an√°lise e previs√£o de dados.
### Refer√™ncias
[^4.1.2]: *The forecast with the smallest mean squared error turns out to be the expectation of Y.+1 conditional on X‚ÇÅ:
Y*+1 = E(Y1+1/Œß.).*
[^4.1.8]: *The MSE of this optimal forecast is
E[Y1+1-g(X,)]¬≤ = E[Y1+1 - E(Y1+1|X.)]¬≤. *
[^4.1.9]: *We now restrict the class of forecasts considered by requiring the forecast
Y to be a linear function of X,:
Y+1 = Œ±ŒÑŒß.*
[^4.1.10]: *Suppose we were to find a value for a such that the forecast error (Y1+1 ‚Äì Œ±ŒÑŒß.)
is uncorrelated with X,:
Œï[(Œ•.+1 ‚Äì Œ±ŒÑŒß.)X] = 0'.*
[^4.1.12]: *The optimal linear forecast g'X, is the value that sets the second term in [4.1.12] equal to zero.*
<!-- END -->
