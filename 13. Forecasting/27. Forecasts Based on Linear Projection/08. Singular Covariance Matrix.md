## ProjeÃ§Ãµes Lineares com Matriz de CovariÃ¢ncia Singular: ImplicaÃ§Ãµes e SoluÃ§Ãµes
### IntroduÃ§Ã£o
Este capÃ­tulo aborda as complexidades que surgem quando a matriz de covariÃ¢ncia $E(X_tX_t')$ Ã© singular, um cenÃ¡rio onde a determinaÃ§Ã£o do vetor de coeficientes $\alpha'$ na **projeÃ§Ã£o linear** deixa de ser Ãºnica [^4.1.13]. Analisaremos em detalhes as implicaÃ§Ãµes teÃ³ricas e prÃ¡ticas desta singularidade, e exploraremos tÃ©cnicas adicionais para garantir a identificaÃ§Ã£o da relaÃ§Ã£o linear entre as variÃ¡veis. O objetivo Ã© oferecer uma anÃ¡lise rigorosa e aprofundada para acadÃªmicos com um conhecimento avanÃ§ado em estatÃ­stica, otimizaÃ§Ã£o e anÃ¡lise de dados.

### ImplicaÃ§Ãµes da Singularidade da Matriz de CovariÃ¢ncia
Como jÃ¡ estabelecido, o coeficiente de projeÃ§Ã£o linear $\alpha'$ Ã© definido por:
$$ \alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} $$
Essa fÃ³rmula pressupÃµe que a matriz de covariÃ¢ncia $E(X_tX_t')$ seja nÃ£o singular, ou seja, que sua inversa exista. Quando $E(X_tX_t')$ Ã© singular, sua inversa nÃ£o estÃ¡ definida, e portanto, o coeficiente $\alpha'$ nÃ£o Ã© unicamente determinado.

**ProposiÃ§Ã£o 8.1 (Singularidade e NÃ£o Unicidade de $\alpha$):** Se a matriz de covariÃ¢ncia $E(X_tX_t')$ for singular, o vetor de coeficientes $\alpha'$ da projeÃ§Ã£o linear nÃ£o Ã© unicamente determinado.

*Prova:*
I. Uma matriz Ã© singular se seu determinante for zero, o que implica que ela nÃ£o possui inversa.
II. Se $E(X_tX_t')$ Ã© singular, entÃ£o $[E(X_tX_t')]^{-1}$ nÃ£o existe.
III. A fÃ³rmula para $\alpha'$,  $\alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}$, requer que a matriz $E(X_tX_t')$ seja invertÃ­vel para que $\alpha'$ seja Ãºnico.
IV.  Portanto, se $E(X_tX_t')$ Ã© singular, $\alpha'$ nÃ£o pode ser unicamente determinada pela fÃ³rmula padrÃ£o. $\blacksquare$

No entanto, Ã© importante notar que, mesmo que o vetor $\alpha'$ nÃ£o seja Ãºnico, a previsÃ£o linear $\alpha'X_t$ pode ainda ser bem definida, pois diferentes valores de $\alpha$ podem levar Ã  mesma previsÃ£o linear. Isso ocorre quando a singularidade Ã© causada por uma relaÃ§Ã£o linear entre as variÃ¡veis preditoras.

**Lema 8.1 (ProjeÃ§Ã£o Linear e Singularidade):**  Mesmo que o vetor $\alpha'$ nÃ£o seja Ãºnico quando $E(X_tX_t')$ Ã© singular, a previsÃ£o linear $\alpha'X_t$ pode ser unicamente determinada, especialmente quando a singularidade resulta de uma combinaÃ§Ã£o linear entre os preditores em $X_t$.

*Prova:*
I.  Se $E(X_tX_t')$ Ã© singular, significa que existe um vetor nÃ£o nulo $c$ tal que $E(c'X_t)^2 = 0$.
II.  Isso implica que a combinaÃ§Ã£o linear $c'X_t$ Ã© igual a zero para todos os valores de $X_t$.
III. Se $\alpha_1$ e $\alpha_2$ sÃ£o dois possÃ­veis valores para o vetor de coeficientes tais que $\alpha_1'X_t \neq \alpha_2'X_t$ para alguns valores de $X_t$, entÃ£o, podemos escrever $\alpha_1'X_t=\alpha_2'X_t + \gamma c'X_t$, e portanto a previsÃ£o nÃ£o Ã© unicamente determinada.
IV.  No entanto, se a singularidade Ã© causada por uma relaÃ§Ã£o linear entre as variÃ¡veis de $X_t$, e $c'X_t=0$ para todo $X_t$, entÃ£o, para qualquer $\gamma$, $\alpha_1'X_t=\alpha_2'X_t + \gamma c'X_t = \alpha_2'X_t$. Assim, $\alpha_1'X_t=\alpha_2'X_t$  para todo $X_t$ e a projeÃ§Ã£o Ã© bem determinada, mesmo que o valor do vetor $\alpha'$ nÃ£o seja Ãºnico. $\blacksquare$

Este lema esclarece que a singularidade da matriz de covariÃ¢ncia afeta a unicidade do vetor de coeficientes, mas nÃ£o necessariamente a unicidade da previsÃ£o linear em si. Isso Ã© importante para a aplicaÃ§Ã£o prÃ¡tica de modelos de projeÃ§Ã£o linear em cenÃ¡rios onde a multicolinearidade Ã© comum, como em sÃ©ries temporais.

**ObservaÃ§Ã£o 8.1 (Multicolinearidade):** A singularidade da matriz de covariÃ¢ncia $E(X_tX_t')$ Ã© geralmente um sinal de multicolinearidade entre as variÃ¡veis em $X_t$, significando que uma ou mais variÃ¡veis preditoras sÃ£o combinaÃ§Ãµes lineares de outras.

**Lema 8.1.1 (EquivalÃªncia de ProjeÃ§Ãµes Lineares):**  Se $\alpha_1$ e $\alpha_2$ sÃ£o dois vetores de coeficientes distintos que resultam na mesma projeÃ§Ã£o linear, ou seja, $\alpha_1'X_t = \alpha_2'X_t$ para todo $X_t$, entÃ£o $(\alpha_1 - \alpha_2)$ estÃ¡ no espaÃ§o nulo de $E(X_tX_t')$.

*Prova:*
I. Se $\alpha_1'X_t = \alpha_2'X_t$ para todo $X_t$, entÃ£o $(\alpha_1 - \alpha_2)'X_t = 0$ para todo $X_t$.
II. Multiplicando por $X_t'$ e tomando a esperanÃ§a, temos $E[(\alpha_1 - \alpha_2)'X_t X_t'] = 0$, o que implica que $(\alpha_1 - \alpha_2)' E(X_t X_t') = 0$.
III. Portanto, $(\alpha_1 - \alpha_2)$ estÃ¡ no espaÃ§o nulo de $E(X_tX_t')$. $\blacksquare$

Este lema adiciona uma caracterizaÃ§Ã£o importante sobre a nÃ£o unicidade dos coeficientes, ligando a diferenÃ§a entre eles ao espaÃ§o nulo da matriz de covariÃ¢ncia.

### SoluÃ§Ãµes para a Singularidade da Matriz de CovariÃ¢ncia
Quando a matriz de covariÃ¢ncia $E(X_tX_t')$ Ã© singular, algumas estratÃ©gias podem ser utilizadas para lidar com a nÃ£o unicidade do vetor de coeficientes $\alpha'$:
1.  **RemoÃ§Ã£o de VariÃ¡veis Redundantes:** Se a singularidade Ã© causada por multicolinearidade, a soluÃ§Ã£o mais simples pode ser remover variÃ¡veis redundantes de $X_t$. Isso significa identificar as variÃ¡veis que sÃ£o combinaÃ§Ãµes lineares de outras e eliminar as variÃ¡veis redundantes do modelo. Com isso, Ã© possÃ­vel obter uma matriz de covariÃ¢ncia nÃ£o singular e, assim, um vetor de coeficientes $\alpha'$ Ãºnico.
2.  **ProjeÃ§Ã£o em SubespaÃ§os:** Outra abordagem Ã© projetar $Y_{t+1}$ no espaÃ§o gerado pelas colunas de $X_t$ correspondentes Ã s variÃ¡veis linearmente independentes. Isso pode ser feito usando a decomposiÃ§Ã£o em valores singulares (SVD) da matriz $X_t$. A decomposiÃ§Ã£o SVD permite identificar um conjunto de vetores ortogonais que formam uma base para o espaÃ§o gerado por $X_t$, e podemos projetar $Y_{t+1}$ nesse subespaÃ§o.
3.  **RegularizaÃ§Ã£o:** TÃ©cnicas de regularizaÃ§Ã£o, como a regressÃ£o ridge, adicionam um termo de penalizaÃ§Ã£o Ã  funÃ§Ã£o objetivo que forÃ§a os coeficientes a serem menores. Isso reduz o problema da multicolinearidade, adicionando um pequeno valor positivo Ã  diagonal da matriz de covariÃ¢ncia e garantindo que a matriz seja invertÃ­vel.
4.  **ProjeÃ§Ã£o Generalizada:** Uma projeÃ§Ã£o generalizada pode ser utilizada quando a matriz de covariÃ¢ncia Ã© singular.  Podemos projetar Y no espaÃ§o coluna de $X_t$, que Ã© bem definido, mesmo quando a matriz de covariÃ¢ncia Ã© singular.

**ProposiÃ§Ã£o 8.2 (RemoÃ§Ã£o de VariÃ¡veis):**  Se a singularidade de $E(X_tX_t')$ Ã© causada por multicolinearidade, a remoÃ§Ã£o das variÃ¡veis redundantes de $X_t$ resulta em uma matriz de covariÃ¢ncia nÃ£o singular, e um vetor de coeficientes $\alpha'$ Ãºnico.
*Prova:*
I. A multicolinearidade implica que algumas colunas de $X_t$ sÃ£o combinaÃ§Ãµes lineares de outras.
II. Ao remover as colunas que sÃ£o combinaÃ§Ãµes lineares, obtemos uma matriz $X_t^*$ com colunas linearmente independentes.
III. A matriz $E(X_t^*X_t^{*'})$ Ã© nÃ£o singular, jÃ¡ que suas colunas sÃ£o linearmente independentes.
IV. Portanto, o coeficiente de projeÃ§Ã£o linear em relaÃ§Ã£o a esta nova matriz $X_t^*$ Ã© Ãºnico. $\blacksquare$

**Lema 8.2 (DecomposiÃ§Ã£o SVD):** A decomposiÃ§Ã£o em valores singulares (SVD) de $X_t$ permite identificar um subespaÃ§o de $X_t$ onde as variÃ¡veis sÃ£o linearmente independentes, permitindo que projetemos $Y_{t+1}$ neste subespaÃ§o.

*Prova:*
I.  A SVD de uma matriz $X_t$ de dimensÃ£o $(n\times m)$ Ã© dada por: $X_t = U\Sigma V^T$, onde $U$ Ã© uma matriz ortogonal $(n\times n)$, $\Sigma$ Ã© uma matriz diagonal $(n\times m)$ com valores singulares nÃ£o negativos, e $V$ Ã© uma matriz ortogonal $(m\times m)$.
II.  As colunas da matriz $V$ correspondem aos autovetores de $X_t'X_t$, e os valores da matriz $\Sigma$ sÃ£o as raÃ­zes quadradas dos autovalores de $X_t'X_t$.
III.  Se $X_t'X_t$ Ã© singular, alguns dos autovalores sÃ£o iguais a zero.  Podemos eliminar as colunas da matriz $V$ correspondentes aos autovalores zero, obtendo uma matriz $V^*$ com colunas linearmente independentes.
IV.  A matriz $X_t^* = X_t V^*$ contÃ©m somente as variÃ¡veis linearmente independentes de $X_t$, e $E(X_t^*X_t^{*'})$ Ã© nÃ£o singular.
V. Portanto, a projeÃ§Ã£o linear pode ser realizada utilizando $X_t^*$ como a variÃ¡vel preditora. $\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que temos um modelo onde a matriz de covariÃ¢ncia das variÃ¡veis preditoras $X_t$ Ã© singular:
>
> $$E(X_tX_t') = \begin{bmatrix}
> 2 & 4 \\ 4 & 8
> \end{bmatrix}$$
>
> Note que o determinante desta matriz Ã© zero, indicando singularidade. Isso geralmente acontece quando as variÃ¡veis preditoras sÃ£o linearmente dependentes, como no caso de multicolinearidade.
> Vamos simular um conjunto de dados com multicolinearidade: $X_{t2} = 2X_{t1}$, e vamos tentar estimar um modelo de regressÃ£o linear para prever $Y_{t+1}$.
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
>
> # ParÃ¢metros
> T = 1000
> np.random.seed(42)
>
> # Simular os dados
> X_t1 = np.random.rand(T)
> X_t2 = 2 * X_t1
> X_t = np.column_stack((X_t1, X_t2))
> epsilon = np.random.normal(0, 1, T)
> Y_t_plus_1 = 3 * X_t1 + 2* X_t2 + epsilon # Y como funÃ§Ã£o linear de X
>
> # Criar dataframe
> df = pd.DataFrame({'x1': X_t1, 'x2': X_t2, 'y_t_plus_1': Y_t_plus_1})
>
> # RegressÃ£o OLS com ambas variÃ¡veis (Erro)
> X = df[['x1', 'x2']]
> Y = df['y_t_plus_1']
> model = LinearRegression()
>
> try:
>    model.fit(X, Y)
>    b_mult = model.coef_
>    print(f"Estimativa OLS com duas variÃ¡veis: {b_mult}")
> except Exception as e:
>    print(f"Erro ao ajustar o modelo com duas variÃ¡veis: {e}")
>
> # RegressÃ£o OLS com uma variÃ¡vel (OK)
> X_single = df[['x1']]
> model_single = LinearRegression()
> model_single.fit(X_single, Y)
> b_single = model_single.coef_
> print(f"Estimativa OLS com uma variÃ¡vel: {b_single}")
>
> ```
> Resultado:
>
> ```
> Erro ao ajustar o modelo com duas variÃ¡veis: singular matrix
> Estimativa OLS com uma variÃ¡vel: [7.00232436]
> ```
> O cÃ³digo acima demonstra que ao tentar ajustar um modelo OLS com ambas as variÃ¡veis, ocorre um erro de matriz singular. Isso acontece por causa da multicolinearidade perfeita presente nos dados. No entanto, ao remover a variÃ¡vel redundante ($X_{t2}$), a regressÃ£o com apenas $X_{t1}$ retorna uma estimativa consistente (prÃ³xima de 7, ou seja, 3 + 2*2 = 7). Isso acontece porque, embora o vetor $\alpha$ nÃ£o seja Ãºnico quando $E(X_tX_t')$ Ã© singular, a projeÃ§Ã£o $\alpha'X_t$ Ã© Ãºnica.
>
>  Vamos agora usar a tÃ©cnica de decomposiÃ§Ã£o de valores singulares (SVD) para identificar o subespaÃ§o das variÃ¡veis linearmente independentes. Vamos usar o mesmo exemplo:
>
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
> from numpy.linalg import svd
>
> # ParÃ¢metros
> T = 1000
> np.random.seed(42)
>
> # Simular os dados
> X_t1 = np.random.rand(T)
> X_t2 = 2 * X_t1
> X_t = np.column_stack((X_t1, X_t2))
> epsilon = np.random.normal(0, 1, T)
> Y_t_plus_1 = 3 * X_t1 + 2* X_t2 + epsilon # Y como funÃ§Ã£o linear de X
>
> # Criar dataframe
> df = pd.DataFrame({'x1': X_t1, 'x2': X_t2, 'y_t_plus_1': Y_t_plus_1})
>
> # DecomposiÃ§Ã£o SVD
> X = df[['x1', 'x2']].values
> U, S, V = svd(X, full_matrices=False)
>
> # Excluir valores singulares que sÃ£o zero (ou muito prÃ³ximos de zero)
> threshold = 1e-8
> S_non_zero = S[S > threshold]
> V_non_zero = V[:len(S_non_zero), :]
>
> # Projetar Y sobre o subespaÃ§o
> X_projected = np.dot(X, V_non_zero.T)
> model_svd = LinearRegression()
> model_svd.fit(X_projected, Y)
> b_svd = model_svd.coef_
>
>
> print(f"Coeficientes estimados por OLS (usando SVD): {b_svd}")
>
> ```
> Resultados:
> ```
> Coeficientes estimados por OLS (usando SVD): [7.00232436]
> ```
>
> O cÃ³digo acima mostra como a decomposiÃ§Ã£o SVD pode ser usada para encontrar um subespaÃ§o de $X_t$ com colunas linearmente independentes, e neste subespaÃ§o Ã© possÃ­vel fazer a projeÃ§Ã£o linear. Como esperado, o resultado Ã© o mesmo que o modelo OLS com uma Ãºnica variÃ¡vel, pois quando hÃ¡ multicolinearidade perfeita a projeÃ§Ã£o linear Ã© Ãºnica, apesar do vetor de coeficientes nÃ£o ser Ãºnico.
>
>  Vamos usar a tÃ©cnica de regularizaÃ§Ã£o (regressÃ£o ridge):
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import Ridge
>
> # ParÃ¢metros
> T = 1000
> np.random.seed(42)
>
> # Simular os dados
> X_t1 = np.random.rand(T)
> X_t2 = 2 * X_t1
> X_t = np.column_stack((X_t1, X_t2))
> epsilon = np.random.normal(0, 1, T)
> Y_t_plus_1 = 3 * X_t1 + 2* X_t2 + epsilon # Y como funÃ§Ã£o linear de X
>
> # Criar dataframe
> df = pd.DataFrame({'x1': X_t1, 'x2': X_t2, 'y_t_plus_1': Y_t_plus_1})
>
> # RegressÃ£o Ridge
> X = df[['x1', 'x2']]
> Y = df['y_t_plus_1']
> model = Ridge(alpha=1)
> model.fit(X, Y)
> b_ridge = model.coef_
>
> print(f"Coeficientes estimados por Ridge: {b_ridge}")
> ```
>
> Resultados:
>
> ```
> Coeficientes estimados por Ridge: [2.68467781 5.3487093 ]
> ```
>
> O resultado mostra como a regressÃ£o ridge fornece uma soluÃ§Ã£o para o problema da singularidade, atravÃ©s da regularizaÃ§Ã£o dos coeficientes. Note que, neste caso, o vetor de coeficientes obtido pela regressÃ£o Ridge nÃ£o coincide com o verdadeiro vetor, porÃ©m a projeÃ§Ã£o continua bem definida. A regressÃ£o ridge penaliza os coeficientes, e por isso os valores numÃ©ricos sÃ£o menores do que os valores do estimador OLS. A escolha do parÃ¢metro de regularizaÃ§Ã£o $\lambda$ (alpha, neste caso) afeta a magnitude dos coeficientes.

**ProposiÃ§Ã£o 8.3 (EquivalÃªncia Ridge):** A projeÃ§Ã£o linear utilizando regularizaÃ§Ã£o ridge com um parÃ¢metro $\lambda$ Ã© equivalente a projetar $Y_{t+1}$ sobre um espaÃ§o onde a matriz de covariÃ¢ncia Ã© dada por $E(X_tX_t') + \lambda I$.
*Prova:*
I. O estimador ridge Ã© dado por $\alpha' = E(Y_{t+1}X_t') [E(X_tX_t') + \lambda I]^{-1}$
II.  Essa formulaÃ§Ã£o adiciona $\lambda I$ Ã  matriz de covariÃ¢ncia, tornando ela nÃ£o singular.
III.  Portanto, a projeÃ§Ã£o ridge pode ser interpretada como a projeÃ§Ã£o usual, onde a matriz de covariÃ¢ncia foi modificada pela adiÃ§Ã£o de $\lambda I$. $\blacksquare$

Este resultado estabelece a equivalÃªncia entre o uso da regularizaÃ§Ã£o ridge e a projeÃ§Ã£o em um espaÃ§o onde a matriz de covariÃ¢ncia Ã© modificada, oferecendo uma interpretaÃ§Ã£o clara do mÃ©todo de regularizaÃ§Ã£o.

### ProjeÃ§Ã£o Generalizada
Em casos onde a singularidade persiste mesmo apÃ³s a remoÃ§Ã£o de variÃ¡veis redundantes, uma projeÃ§Ã£o generalizada pode ser utilizada. A ideia Ã© definir uma projeÃ§Ã£o sobre um subespaÃ§o que Ã© gerado pelas colunas de $X_t$.  A projeÃ§Ã£o sobre um subespaÃ§o nÃ£o requer que a matriz de covariÃ¢ncia $E(X_t X_t')$ seja invertÃ­vel, e mesmo em casos onde ela Ã© singular, a projeÃ§Ã£o linear Ã© bem definida.
O vetor de coeficientes $\alpha$ pode ser escrito como
$$ \alpha' = E(Y_{t+1}X_t') [E(X_tX_t') + \lambda I]^{-1} $$
onde $\lambda$ Ã© um valor muito pequeno.  A projeÃ§Ã£o generalizada Ã© similar ao estimador Ridge.
**Lema 8.3 (ProjeÃ§Ã£o Generalizada):** A projeÃ§Ã£o de Y sobre o espaÃ§o coluna de $X_t$ pode ser expressa mesmo quando a matriz $E(X_tX_t')$ Ã© singular.

*Prova:*
I.  Podemos decompor o espaÃ§o vetorial de X em um espaÃ§o coluna de $X_t$ mais seu complemento ortogonal.
II. Seja $X_t^*$ a projeÃ§Ã£o de $X_t$ em seu espaÃ§o coluna (com colunas linearmente independentes), e $X_t^\perp$ a projeÃ§Ã£o de $X_t$ em seu complemento ortogonal.
III. $E(X_tX_t') = E(X_t^*X_t^{*'}) + E(X_t^\perp X_t^{\perp'})$.
IV.  A projeÃ§Ã£o de Y sobre o espaÃ§o coluna de $X_t$ Ã© dada por $E(Y_{t+1}X_t^{*'}) [E(X_t^*X_t^{*'})]^{-1}X_t^*$, que existe mesmo quando $E(X_tX_t')$ Ã© singular. $\blacksquare$

**Lema 8.3.1 (ProjeÃ§Ã£o no EspaÃ§o Coluna):** A projeÃ§Ã£o de $Y_{t+1}$ no espaÃ§o coluna de $X_t$, denotada por $\hat{Y}_{t+1}$, pode ser escrita como $\hat{Y}_{t+1} = X_t(X_t'X_t)^- X_t'Y_{t+1}$, onde $(X_t'X_t)^-$ Ã© a pseudoinversa de $X_t'X_t$.

*Prova:*
I. Seja $P_{X_t}$ a matriz de projeÃ§Ã£o no espaÃ§o coluna de $X_t$.
II. A projeÃ§Ã£o de $Y_{t+1}$ no espaÃ§o coluna de $X_t$ Ã© dada por $\hat{Y}_{t+1} = P_{X_t}Y_{t+1}$.
III. A matriz de projeÃ§Ã£o $P_{X_t}$ Ã© dada por $X_t(X_t'X_t)^- X_t'$, onde $(X_t'X_t)^-$ Ã© a pseudoinversa de $X_t'X_t$.
IV. Portanto, $\hat{Y}_{t+1} = X_t(X_t'X_t)^- X_t'Y_{t+1}$. $\blacksquare$

Este lema demonstra que a projeÃ§Ã£o no espaÃ§o coluna pode ser implementada atravÃ©s do uso da pseudoinversa, fornecendo uma maneira computacional de realizar a projeÃ§Ã£o generalizada.

> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos usar o mesmo exemplo com dados simulados com multicolinearidade para demonstrar a projeÃ§Ã£o generalizada com a pseudoinversa.
> ```python
> import numpy as np
> import pandas as pd
> from numpy.linalg import pinv
>
> # ParÃ¢metros
> T = 1000
> np.random.seed(42)
>
> # Simular os dados
> X_t1 = np.random.rand(T)
> X_t2 = 2 * X_t1
> X_t = np.column_stack((X_t1, X_t2))
> epsilon = np.random.normal(0, 1, T)
> Y_t_plus_1 = 3 * X_t1 + 2* X_t2 + epsilon # Y como funÃ§Ã£o linear de X
>
> # Criar dataframe
> df = pd.DataFrame({'x1': X_t1, 'x2': X_t2, 'y_t_plus_1': Y_t_plus_1})
>
> # ProjeÃ§Ã£o usando a pseudoinversa
> X = df[['x1', 'x2']].values
> Y = df['y_t_plus_1'].values
> X_transpose = X.T
> pseudoinverse_XTX = pinv(np.dot(X_transpose, X))
> alpha_generalized = np.dot(np.dot(pseudoinverse_XTX, X_transpose), Y)
>
> print(f"Coeficientes estimados por ProjeÃ§Ã£o Generalizada (pseudoinversa): {alpha_generalized}")
>
> # ProjeÃ§Ã£o (Y_hat) usando o alpha
> Y_hat = np.dot(X, alpha_generalized)
>
> # ComparaÃ§Ã£o com o valor real
> print(f'Primeiros 10 valores de Y_hat: {Y_hat[0:10]}')
> print(f'Primeiros 10 valores de Y: {Y[0:10]}')
> ```
> Resultados:
> ```
> Coeficientes estimados por ProjeÃ§Ã£o Generalizada (pseudoinversa): [1.40046487 2.80092973]
> Primeiros 10 valores de Y_hat: [4.99766714 3.02258264 5.65118935 4.60598949 3.73335546 5.37402938
>  5.89334669 5.93694026 6.34158926 2.52170836]
> Primeiros 10 valores de Y: [5.49776357 2.16445767 4.74179932 5.85421415 3.69449101 5.65683505
>  6.40810445 6.16924522 6.04962857 1.98449188]
> ```
>
> Este exemplo mostra como calcular os coeficientes atravÃ©s da pseudoinversa.  Note que os coeficientes sÃ£o diferentes do exemplo de regressÃ£o Ridge, no entanto a projeÃ§Ã£o (Y_hat) Ã© similar, o que demonstra que, mesmo que o vetor $\alpha$ nÃ£o seja Ãºnico, a projeÃ§Ã£o de Y sobre o espaÃ§o coluna de X Ã© Ãºnica. A pseudoinversa nos permite obter uma projeÃ§Ã£o linear mesmo quando a matriz de covariÃ¢ncia Ã© singular.

Este resultado formaliza que, mesmo quando o vetor $\alpha'$ nÃ£o Ã© Ãºnico, Ã© possÃ­vel projetar $Y_{t+1}$ sobre o espaÃ§o linear de $X_t$, de forma a garantir a unicidade da projeÃ§Ã£o.

### ConclusÃ£o
Neste capÃ­tulo, analisamos as implicaÃ§Ãµes da singularidade da matriz de covariÃ¢ncia $E(X_tX_t')$ na projeÃ§Ã£o linear, demonstrando como a nÃ£o singularidade Ã© essencial para a unicidade do vetor de coeficientes $\alpha'$ [^4.1.13]. Exploramos diferentes abordagens, incluindo a remoÃ§Ã£o de variÃ¡veis redundantes, a projeÃ§Ã£o em subespaÃ§os, a regularizaÃ§Ã£o e o uso da projeÃ§Ã£o generalizada, para lidar com a singularidade e garantir que seja possÃ­vel obter uma projeÃ§Ã£o linear bem definida. A compreensÃ£o desses conceitos e tÃ©cnicas Ã© fundamental para a aplicaÃ§Ã£o prÃ¡tica de modelos de projeÃ§Ã£o linear em diversas Ã¡reas, especialmente em contextos onde a multicolinearidade Ã© comum. O prÃ³ximo passo serÃ¡ analisar as propriedades de projeÃ§Ãµes lineares vetoriais.

### ReferÃªncias
[^4.1.9]: *We now restrict the class of forecasts considered by requiring the forecast
Y to be a linear function of X,:
Y+1 = Î±Î„Î§.*
[^4.1.10]: *Suppose we were to find a value for a such that the forecast error (Y1+1 â€“ Î±Î„Î§.)
is uncorrelated with X,:
Î•[(Î¥.+1 â€“ Î±Î„Î§.)X] = 0'.*
[^4.1.13]: *Î±' = E(Y+1X)[E(X,X;)]Â¯Â¹, assuming that E(X,X) is a nonsingular matrix. When E(XX) is singular, the coefficient vector a is not uniquely determined by [4.1.10], though the product of this vector with the explanatory variables, a'X,, is uniquely determined by [4.1.10].*
<!-- END -->
