## Avalia√ß√£o da Precis√£o da Proje√ß√£o Linear: An√°lise do Erro Quadr√°tico M√©dio (MSE)

### Introdu√ß√£o
Este cap√≠tulo foca na an√°lise do **Erro Quadr√°tico M√©dio (MSE)** como uma m√©trica fundamental para avaliar a precis√£o das previs√µes geradas pela **proje√ß√£o linear** [^4.1.1]. Expandindo os conceitos discutidos nos cap√≠tulos anteriores, exploraremos como o MSE, expresso como $E[(Y_{t+1} - \alpha'X_t)^2]$, quantifica a qualidade da previs√£o e como sua minimiza√ß√£o guia a otimiza√ß√£o dos par√¢metros do modelo. O objetivo √© apresentar um tratamento detalhado e rigoroso do MSE, crucial para a avalia√ß√£o de modelos de previs√£o em s√©ries temporais, direcionado para um p√∫blico com conhecimento avan√ßado em estat√≠stica, otimiza√ß√£o e an√°lise de dados.

### Defini√ß√£o e Import√¢ncia do Erro Quadr√°tico M√©dio (MSE)
O Erro Quadr√°tico M√©dio (MSE) √© uma m√©trica que quantifica a discrep√¢ncia entre os valores previstos e os valores reais de uma vari√°vel. No contexto da proje√ß√£o linear, o MSE √© definido como o valor esperado do quadrado da diferen√ßa entre o valor real $Y_{t+1}$ e a previs√£o linear $\alpha'X_t$:
$$ MSE = E[(Y_{t+1} - \alpha'X_t)^2] $$
O MSE √© uma m√©trica popular para avaliar modelos de previs√£o devido √† sua interpretabilidade e √†s suas propriedades matem√°ticas. Ele penaliza tanto erros positivos quanto negativos, e penaliza erros maiores mais fortemente que erros menores. O objetivo principal da proje√ß√£o linear √©, portanto, minimizar o MSE, encontrando os melhores par√¢metros $\alpha$ que garantem a previs√£o mais precisa. O MSE √© essencialmente a vari√¢ncia do erro de proje√ß√£o.

**Proposi√ß√£o 6.1 (MSE e Qualidade da Previs√£o):** O MSE quantifica a qualidade da previs√£o da proje√ß√£o linear, onde um valor menor de MSE indica uma previs√£o mais precisa.
*Prova:*
I. O MSE √© definido como $E[(Y_{t+1} - \alpha'X_t)^2]$, que √© o valor esperado do quadrado do erro de previs√£o.
II. Quanto menor a magnitude dos erros de previs√£o, menor ser√° o valor do MSE.
III. Portanto, um MSE menor indica que a previs√£o linear est√° mais pr√≥xima dos valores reais, ou seja, a qualidade da previs√£o √© melhor. $\blacksquare$

O MSE n√£o apenas quantifica o erro, mas tamb√©m serve como uma fun√ß√£o objetivo para a otimiza√ß√£o. Ao buscar o vetor $\alpha$ que minimiza o MSE, estamos simultaneamente encontrando a melhor proje√ß√£o linear de $Y_{t+1}$ em $X_t$.

###  C√°lculo e Interpreta√ß√£o do MSE da Proje√ß√£o Linear
O MSE da proje√ß√£o linear √© dado por [^4.1.8]:
$$ MSE = E[(Y_{t+1} - \alpha'X_t)^2] $$
Como discutido anteriormente, o MSE representa a varia√ß√£o em $Y_{t+1}$ que n√£o √© explicada pela proje√ß√£o linear $\alpha'X_t$.  Podemos decompor o MSE usando a condi√ß√£o de ortogonalidade:
$$ E[(Y_{t+1} - \alpha'X_t)^2] =  E[(Y_{t+1} - E(Y_{t+1}|X_t) + E(Y_{t+1}|X_t) - \alpha'X_t)^2] $$
Usando a ortogonalidade do erro de proje√ß√£o:
$$  = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] + E[(E(Y_{t+1}|X_t) - \alpha'X_t)^2] $$
Essa decomposi√ß√£o mostra que o MSE √© a soma do erro da melhor previs√£o poss√≠vel, a esperan√ßa condicional, e o erro ao aproximar essa esperan√ßa condicional pela proje√ß√£o linear $\alpha'X_t$. Se $E(Y_{t+1}|X_t)$ for linear, ent√£o o segundo termo ser√° zero e o MSE ser√° igual ao erro da esperan√ßa condicional, que √© o m√≠nimo poss√≠vel.

**Lema 6.1 (Decomposi√ß√£o do MSE):** O MSE da proje√ß√£o linear pode ser decomposto como:
$$ MSE = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] + E[(E(Y_{t+1}|X_t) - \alpha'X_t)^2] $$
onde o primeiro termo representa o erro m√≠nimo poss√≠vel, dado pela esperan√ßa condicional, e o segundo termo representa o erro ao aproximar a esperan√ßa condicional pela proje√ß√£o linear.

*Prova:*
I. Expandimos o termo $Y_{t+1} - \alpha'X_t$ adicionando e subtraindo a esperan√ßa condicional $E(Y_{t+1}|X_t)$:
  $$ (Y_{t+1} - \alpha'X_t) = (Y_{t+1} - E(Y_{t+1}|X_t)) + (E(Y_{t+1}|X_t) - \alpha'X_t) $$
II. Elevando ao quadrado e tomando a esperan√ßa:
    $$ E[(Y_{t+1} - \alpha'X_t)^2] = E[((Y_{t+1} - E(Y_{t+1}|X_t)) + (E(Y_{t+1}|X_t) - \alpha'X_t))^2] $$
    $$ = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] + 2E[(Y_{t+1} - E(Y_{t+1}|X_t))(E(Y_{t+1}|X_t) - \alpha'X_t)] + E[(E(Y_{t+1}|X_t) - \alpha'X_t)^2] $$
III. O termo do meio se anula devido √† ortogonalidade entre o erro condicional e a esperan√ßa condicional. Ou seja, $E[(Y_{t+1} - E(Y_{t+1}|X_t))(E(Y_{t+1}|X_t) - \alpha'X_t)] = 0$
    
    *Nota: Para provar que  $E[(Y_{t+1} - E(Y_{t+1}|X_t))(E(Y_{t+1}|X_t) - \alpha'X_t)]=0$, expandimos o termo:*
    
    $$ E[(Y_{t+1} - E(Y_{t+1}|X_t))(E(Y_{t+1}|X_t) - \alpha'X_t)]  = E[Y_{t+1}E(Y_{t+1}|X_t) - Y_{t+1}\alpha'X_t - E(Y_{t+1}|X_t)^2 + E(Y_{t+1}|X_t)\alpha'X_t]$$
    
    *Usando a lei da esperan√ßa iterada temos $E[Y_{t+1}E(Y_{t+1}|X_t)]=E[E(Y_{t+1}E(Y_{t+1}|X_t)|X_t)]=E[E(Y_{t+1}|X_t)^2]$, e tamb√©m, $E[Y_{t+1}\alpha'X_t]=E[E(Y_{t+1}\alpha'X_t|X_t)]=E[\alpha'X_tE(Y_{t+1}|X_t)]$. Substituindo na equa√ß√£o acima:*
   
    $$ E[Y_{t+1}E(Y_{t+1}|X_t) - Y_{t+1}\alpha'X_t - E(Y_{t+1}|X_t)^2 + E(Y_{t+1}|X_t)\alpha'X_t]= E[E(Y_{t+1}|X_t)^2] - E[\alpha'X_tE(Y_{t+1}|X_t)] - E[E(Y_{t+1}|X_t)^2] + E[E(Y_{t+1}|X_t)\alpha'X_t] = 0$$
IV.  Portanto, o MSE √© dado por:
$$ MSE = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] + E[(E(Y_{t+1}|X_t) - \alpha'X_t)^2] $$
Onde o primeiro termo √© a vari√¢ncia do erro condicional, e o segundo termo representa o erro de aproximar a esperan√ßa condicional pela proje√ß√£o linear.$\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um modelo onde $Y_{t+1} = 3 + 2X_t + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do com m√©dia zero e vari√¢ncia $\sigma^2 = 1$. Vamos supor que temos um conjunto de dados onde $X_t$ varia entre 1 e 5, e queremos avaliar o MSE de duas proje√ß√µes lineares diferentes.
>
> **Cen√°rio 1: Proje√ß√£o Linear Correta**
>   Neste caso, a proje√ß√£o linear ideal √© $\alpha'X_t = 3+2X_t$.  O MSE ser√°:
>   $$ MSE_1 = E[(Y_{t+1} - (3+2X_t))^2] = E[\epsilon_t^2] = 1 $$
>
> **Cen√°rio 2: Proje√ß√£o Linear com Coeficiente Incorreto**
>  Agora, considere uma proje√ß√£o linear incorreta: $\alpha'X_t = 2 + 1.5X_t$. O MSE ser√°:
> $$ MSE_2 = E[(Y_{t+1} - (2+1.5X_t))^2] = E[(3 + 2X_t + \epsilon_t - 2 - 1.5X_t)^2] $$
> $$ = E[(1 + 0.5X_t + \epsilon_t)^2] = E[1 + 0.25X_t^2 + \epsilon_t^2 + X_t + 2\epsilon_t + X_t\epsilon_t] $$
> Assumindo que $X_t$ e $\epsilon_t$ s√£o independentes, $E[X_t\epsilon_t] = E[X_t]E[\epsilon_t]=0$ e $E[\epsilon_t] = 0$.
> Ent√£o,
> $$MSE_2 = 1 + 0.25E[X_t^2] + E[X_t] + 1$$
> Se a m√©dia de $X_t$ √© $3$ e a vari√¢ncia √© $2$, ent√£o $E[X_t^2]=Var[X_t] + E[X_t]^2 = 2 + 3^2 = 11$.
> $$ MSE_2 = 1 + 0.25*11 + 3 + 1 = 6.75$$
> Como esperado, o MSE da proje√ß√£o linear correta (1) √© menor do que o da proje√ß√£o linear incorreta (6.75). Isto demonstra como o MSE reflete a qualidade da aproxima√ß√£o linear.

O primeiro termo na decomposi√ß√£o do MSE, $E[(Y_{t+1} - E(Y_{t+1}|X_t))^2]$, representa o erro m√≠nimo poss√≠vel da previs√£o, dado pela esperan√ßa condicional. O segundo termo, $E[(E(Y_{t+1}|X_t) - \alpha'X_t)^2]$, representa o erro ao aproximar essa esperan√ßa condicional pela proje√ß√£o linear. Se a esperan√ßa condicional $E(Y_{t+1}|X_t)$ for linear em $X_t$, o segundo termo se anula, e a proje√ß√£o linear atinge a melhor previs√£o poss√≠vel.

**Observa√ß√£o 6.1 (Linearidade da Esperan√ßa Condicional):** Se a esperan√ßa condicional $E(Y_{t+1}|X_t)$ √© linear em $X_t$, ent√£o a proje√ß√£o linear fornece a melhor previs√£o poss√≠vel e o MSE √© igual √† vari√¢ncia do erro condicional, dado por $E[(Y_{t+1} - E(Y_{t+1}|X_t))^2]$.

**Lema 6.2 (Rela√ß√£o entre MSE e Vari√¢ncia Condicional):** Quando a esperan√ßa condicional $E(Y_{t+1}|X_t)$ √© linear, o MSE √© igual √† vari√¢ncia condicional do erro, ou seja, $MSE = Var(Y_{t+1}|X_t)$.
*Prova:*
I. Se $E(Y_{t+1}|X_t)$ √© linear em $X_t$, ent√£o $E(Y_{t+1}|X_t) = \alpha'X_t$ para algum vetor $\alpha$.
II. Nesse caso, a decomposi√ß√£o do MSE (Lema 6.1) simplifica-se para:
   $$ MSE = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] + E[(E(Y_{t+1}|X_t) - \alpha'X_t)^2] = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2] + 0 $$
III. O primeiro termo √© a defini√ß√£o da vari√¢ncia condicional $Var(Y_{t+1}|X_t) = E[(Y_{t+1} - E(Y_{t+1}|X_t))^2]$.
IV. Portanto, se a esperan√ßa condicional √© linear, ent√£o $MSE = Var(Y_{t+1}|X_t)$. $\blacksquare$

### Minimizar o MSE
O objetivo da proje√ß√£o linear √© minimizar o MSE, e essa minimiza√ß√£o √© alcan√ßada atrav√©s da escolha apropriada do vetor de coeficientes $\alpha$ [^4.1.12]. Como j√° vimos, a condi√ß√£o de que o erro de previs√£o $Y_{t+1}-\alpha'X_t$ seja n√£o correlacionado com $X_t$ resulta na solu√ß√£o para $\alpha$ que minimiza o MSE [^4.1.10].
Na pr√°tica, como os momentos populacionais s√£o desconhecidos, o MSE √© estimado usando momentos amostrais, e a minimiza√ß√£o do MSE amostral leva ao estimador OLS, discutido em cap√≠tulos anteriores.

**Proposi√ß√£o 6.2 (MSE e Otimiza√ß√£o):** A proje√ß√£o linear $\alpha'X_t$ minimiza o MSE entre todas as previs√µes lineares, e essa minimiza√ß√£o √© equivalente √† condi√ß√£o de que o erro de previs√£o seja n√£o correlacionado com $X_t$, ou seja, $E[(Y_{t+1} - \alpha'X_t)X_t]=0'$.
*Prova:*
I. Seja $g'X_t$ uma previs√£o linear arbitr√°ria de $Y_{t+1}$. Ent√£o, o erro quadr√°tico m√©dio de $g'X_t$ pode ser decomposto como:
   $$ E[(Y_{t+1} - g'X_t)^2] = E[((Y_{t+1} - \alpha'X_t) + (\alpha'X_t - g'X_t))^2]$$
   $$ = E[(Y_{t+1} - \alpha'X_t)^2] + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t)] + E[(\alpha'X_t - g'X_t)^2] $$
II.  O termo do meio $2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t)]$ √© zero, pois $E[(Y_{t+1} - \alpha'X_t)X_t]=0'$. Isso pode ser provado da seguinte forma:
    $E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t)] = E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t)] - E[(Y_{t+1} - \alpha'X_t)(g'X_t)]$
    $= \alpha' E[(Y_{t+1} - \alpha'X_t)X_t] - g'E[(Y_{t+1} - \alpha'X_t)X_t] = 0$ pois $E[(Y_{t+1} - \alpha'X_t)X_t]=0'$
III. Logo, temos: $$ E[(Y_{t+1} - g'X_t)^2] = E[(Y_{t+1} - \alpha'X_t)^2] + E[(\alpha'X_t - g'X_t)^2] $$
IV. Como o segundo termo na express√£o acima √© n√£o negativo e √© zero quando $g'X_t = \alpha'X_t$, ent√£o o MSE √© minimizado quando $g'X_t = \alpha'X_t$.
V. Como vimos em cap√≠tulos anteriores, a condi√ß√£o de ortogonalidade $E[(Y_{t+1} - \alpha'X_t)X_t] = 0'$ garante que o erro quadr√°tico m√©dio seja minimizado.
VI. Portanto, a proje√ß√£o linear $\alpha'X_t$ √© a melhor previs√£o linear de $Y_{t+1}$ dado $X_t$. $\blacksquare$
O MSE √© uma fun√ß√£o convexa em rela√ß√£o aos coeficientes do modelo de proje√ß√£o linear. Isto implica que existe um m√≠nimo global para o MSE, que √© encontrado atrav√©s da condi√ß√£o de n√£o correla√ß√£o.

**Teorema 6.1 (Convexidade do MSE):** O MSE, como fun√ß√£o do vetor de coeficientes $\alpha$, √© uma fun√ß√£o convexa.
*Prova:*
I. O MSE pode ser expresso como $MSE(\alpha) = E[(Y_{t+1} - \alpha'X_t)^2]$.
II. Expandindo, temos $MSE(\alpha) = E[Y_{t+1}^2 - 2Y_{t+1}\alpha'X_t + \alpha'X_tX_t'\alpha]$.
III. A segunda derivada de $MSE(\alpha)$ em rela√ß√£o a $\alpha$ (o Hessiano) √© $2E[X_tX_t']$.
IV. A matriz $X_tX_t'$ √© sempre semi-definida positiva pois, para qualquer vetor n√£o nulo $z$, $z'X_tX_t'z = (X_t'z)'(X_t'z) \ge 0$.
V. Portanto, $E[X_tX_t']$ tamb√©m √© semi-definida positiva, pois √© a esperan√ßa de uma matriz semi-definida positiva.
VI. Uma fun√ß√£o cuja segunda derivada (o Hessiano) √© semi-definida positiva √© convexa. Portanto, o MSE √© uma fun√ß√£o convexa de $\alpha$. $\blacksquare$

### Uso do MSE na Pr√°tica
Na pr√°tica, o MSE √© usado como m√©trica para comparar diferentes modelos de previs√£o. O modelo com o menor MSE √© considerado o melhor em termos de precis√£o.  No entanto, √© importante ressaltar que o MSE √© uma m√©trica que quantifica apenas a precis√£o das previs√µes, e n√£o fornece nenhuma informa√ß√£o sobre a adequa√ß√£o do modelo aos dados. Em alguns casos, pode ser mais apropriado usar outras m√©tricas, como o erro absoluto m√©dio (MAE) ou outras m√©tricas, para realizar uma avalia√ß√£o mais completa dos modelos de previs√£o.

> üí° **Exemplo Num√©rico:** Vamos usar dados simulados para comparar o MSE de dois modelos de proje√ß√£o linear. Suponha que a rela√ß√£o verdadeira seja $Y_{t+1} = 5 + 2X_t + \epsilon_t$, onde $\epsilon_t$ tem distribui√ß√£o normal com m√©dia 0 e desvio padr√£o 1. Vamos gerar dados para 100 per√≠odos e comparar o MSE de dois modelos:
>
> *   Modelo 1: Proje√ß√£o Linear com coeficientes estimados por OLS
> *   Modelo 2: Proje√ß√£o Linear com coeficientes incorretos
>
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
> from sklearn.metrics import mean_squared_error
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> T = 100
> alpha_true = 2
> beta_true = 5
> np.random.seed(42)
>
> # Simula√ß√£o de dados
> X_t = np.random.rand(T) * 10  # Valores de X entre 0 e 10
> epsilon_t = np.random.normal(0, 1, T) # Erro com m√©dia 0 e desvio padr√£o 1
> Y_t_plus_1 = beta_true + alpha_true * X_t + epsilon_t
>
> # Modelo 1: Proje√ß√£o Linear com OLS
> X_t_reshaped = X_t.reshape(-1, 1)
> model_1 = LinearRegression()
> model_1.fit(X_t_reshaped, Y_t_plus_1)
> Y_t_plus_1_pred_1 = model_1.predict(X_t_reshaped)
> mse_1 = mean_squared_error(Y_t_plus_1, Y_t_plus_1_pred_1)
>
> # Modelo 2: Proje√ß√£o Linear com coeficientes incorretos
> alpha_wrong = 1.5
> beta_wrong = 4
> Y_t_plus_1_pred_2 = beta_wrong + alpha_wrong * X_t
> mse_2 = mean_squared_error(Y_t_plus_1, Y_t_plus_1_pred_2)
>
> # Cria um DataFrame para facilitar a visualiza√ß√£o
> df_results = pd.DataFrame({
>    'X_t': X_t,
>    'Y_real': Y_t_plus_1,
>    'Y_pred_OLS': Y_t_plus_1_pred_1,
>    'Y_pred_Wrong': Y_t_plus_1_pred_2
> })
>
> # Plot dos resultados
> plt.figure(figsize=(10, 6))
> plt.scatter(df_results['X_t'], df_results['Y_real'], label='Valores Reais', color='blue')
> plt.plot(df_results['X_t'], df_results['Y_pred_OLS'], label='Previs√£o OLS', color='red')
> plt.plot(df_results['X_t'], df_results['Y_pred_Wrong'], label='Previs√£o Incorreta', color='green')
> plt.xlabel('X_t')
> plt.ylabel('Y_{t+1}')
> plt.title('Compara√ß√£o de Proje√ß√µes Lineares')
> plt.legend()
> plt.grid(True)
> plt.show()
>
> print(f"MSE do Modelo 1 (OLS): {mse_1:.4f}")
> print(f"MSE do Modelo 2 (Incorreto): {mse_2:.4f}")
> ```
>
> Este c√≥digo simula dados com uma rela√ß√£o linear, ajusta uma proje√ß√£o linear usando OLS e calcula o MSE. Tamb√©m calcula o MSE com coeficientes incorretos. O resultado mostrar√° que o MSE do modelo com OLS √© menor, pois este minimiza o MSE amostral.
>
> Os resultados ser√£o algo como:
>
> ```
> MSE do Modelo 1 (OLS): 0.9598
> MSE do Modelo 2 (Incorreto): 2.9867
> ```
>
> Conforme esperado, o MSE do modelo ajustado por OLS √© significativamente menor, mostrando que √© a melhor aproxima√ß√£o linear dentro do conjunto de dados.

### Limita√ß√µes do MSE
Embora o MSE seja uma m√©trica √∫til, ele tem algumas limita√ß√µes. O MSE √© sens√≠vel a outliers, o que pode levar a uma avalia√ß√£o distorcida da precis√£o dos modelos de previs√£o. Em alguns casos, onde os outliers s√£o comuns, pode ser mais apropriado utilizar outras m√©tricas como o MAE (Erro M√©dio Absoluto). Al√©m disso, o MSE n√£o fornece nenhuma informa√ß√£o sobre o vi√©s do modelo, ou seja, se o modelo est√° sistematicamente subestimando ou superestimando as previs√µes.

**Observa√ß√£o 6.2 (Sensibilidade a Outliers):** O MSE penaliza os erros maiores mais fortemente que os erros menores, tornando-o sens√≠vel a outliers.

**Proposi√ß√£o 6.3 (MSE e Vi√©s):** O MSE n√£o fornece informa√ß√µes sobre o vi√©s do modelo de previs√£o. Um modelo com MSE baixo pode ainda ter um vi√©s substancial.
*Prova:*
I. O MSE √© calculado como a m√©dia do quadrado dos erros.
II. Um modelo pode ter um vi√©s sistem√°tico, ou seja, subestimar ou superestimar consistentemente os valores reais.
III. Mesmo com um vi√©s presente, os erros quadr√°ticos podem ser relativamente pequenos, resultando em um MSE baixo.
IV. Portanto, o MSE n√£o captura o vi√©s do modelo de previs√£o. $\blacksquare$

### Conclus√£o
Neste cap√≠tulo, exploramos o Erro Quadr√°tico M√©dio (MSE) como uma m√©trica chave para avaliar a precis√£o das proje√ß√µes lineares. Vimos como o MSE √© calculado e como ele representa a varia√ß√£o na s√©rie temporal que n√£o √© explicada pela proje√ß√£o linear.  Mostramos como a minimiza√ß√£o do MSE √© fundamental para a otimiza√ß√£o do modelo de proje√ß√£o linear. Apresentamos tamb√©m a conex√£o do MSE com as propriedades estat√≠sticas da proje√ß√£o linear, e como o estimador OLS pode ser usado para obter uma estimativa consistente dos coeficientes do modelo de proje√ß√£o linear. Compreender o papel do MSE √© fundamental para a an√°lise, modelagem e previs√£o de s√©ries temporais com proje√ß√£o linear, pois ele guia a busca pela melhor aproxima√ß√£o linear dos dados.
### Refer√™ncias
[^4.1.1]: *Expression [4.1.1] is known as the mean squared error associated with the forecast
Y*+ 11, denoted
MSE(Y*+1/2) = E(Y1+1 - Y+1)2.*
[^4.1.2]: *The forecast with the smallest mean squared error turns out to be the expectation of Y.+1 conditional on X‚ÇÅ:
Y*+1 = E(Y1+1/Œß.).*
[^4.1.8]: *The MSE of this optimal forecast is
E[Y1+1-g(X,)]¬≤ = E[Y1+1 - E(Y1+1|X.)]¬≤. *
[^4.1.9]: *We now restrict the class of forecasts considered by requiring the forecast
Y to be a linear function of X,:
Y+1 = Œ±ŒÑŒß.*
[^4.1.10]: *Suppose we were to find a value for a such that the forecast error (Y1+1 ‚Äì Œ±ŒÑŒß.)
is uncorrelated with X,:
Œï[(Œ•.+1 ‚Äì Œ±ŒÑŒß.)X] = 0'.*
[^4.1.12]: *The optimal linear forecast g'X, is the value that sets the second term in [4.1.12] equal to zero.*
<!-- END -->
