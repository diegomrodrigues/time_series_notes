## Projeção Linear e Previsão: Uma Análise Detalhada
### Introdução
Este capítulo aprofunda o conceito de **projeção linear** como uma ferramenta poderosa para previsão, contrastando-a com abordagens estruturais e ressaltando sua aplicabilidade em cenários onde o foco primário é a acurácia da previsão, em vez da compreensão das relações causais subjacentes. Expandindo o conceito apresentado anteriormente [^4.1.9], exploraremos as nuances da projeção linear e como ela se compara à regressão de mínimos quadrados ordinários (OLS), fornecendo uma base teórica sólida para acadêmicos com conhecimento avançado em matemática, estatística e otimização.

### Conceitos Fundamentais
A **projeção linear** de uma variável $Y_{t+1}$ em um vetor de variáveis $X_t$ é definida como uma função linear de $X_t$ que minimiza o erro quadrático médio de previsão. Formalmente, buscamos um vetor $\alpha$ tal que [^4.1.9]:
$$ Y^*_{t+1} = \alpha' X_t $$
onde $Y^*_{t+1}$ denota a previsão linear de $Y_{t+1}$. O vetor $\alpha$ é determinado de forma que o erro de previsão ($Y_{t+1} - \alpha' X_t$) seja não correlacionado com $X_t$ [^4.1.10]:
$$ E[(Y_{t+1} - \alpha' X_t)X_t] = 0' $$
Este conceito contrasta com a *expectativa condicional* $E(Y_{t+1}|X_t)$, que é a melhor previsão possível em termos de erro quadrático médio [^4.1.2], mas que pode não ser linear em $X_t$. A projeção linear, por sua vez, fornece a melhor aproximação linear para esta expectativa condicional.

**Proposição 1** (Unicidade da Projeção Linear): Se a matriz $E(X_t X_t')$ for não singular, então o vetor $\alpha$ que define a projeção linear é único.

*Prova:* A condição $E[(Y_{t+1} - \alpha' X_t)X_t] = 0'$ pode ser reescrita como $E[Y_{t+1}X_t'] - \alpha' E[X_tX_t'] = 0'$. Se $E[X_tX_t']$ é não singular, podemos multiplicar ambos os lados da equação por $[E(X_tX_t')]^{-1}$ à direita, obtendo $\alpha' = E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}$, que é a expressão para $\alpha'$ dada anteriormente [^4.1.13]. Portanto, se $E(X_t X_t')$ é não singular, $\alpha'$ é único.*

*Prova:*
I. Começamos com a condição de ortogonalidade: $E[(Y_{t+1} - \alpha' X_t)X_t] = 0'$.
II. Expandindo a expressão, obtemos: $E[Y_{t+1}X_t'] - E[\alpha' X_t X_t'] = 0'$.
III. Usando a propriedade de linearidade do operador de esperança, temos: $E[Y_{t+1}X_t'] - \alpha' E[X_tX_t'] = 0'$.
IV. Reorganizando os termos para isolar $\alpha'$, temos: $\alpha' E[X_tX_t'] = E[Y_{t+1}X_t']$.
V.  Se a matriz $E[X_tX_t']$ for não-singular, podemos multiplicar ambos os lados por sua inversa:
  $$\alpha' = E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}$$
VI. Essa expressão define um único vetor $\alpha'$, provando a unicidade.■

> 💡 **Exemplo Numérico:** Suponha que temos uma variável dependente $Y_{t+1}$ e uma variável preditora $X_t$. Assumamos que temos os seguintes momentos populacionais:
> $E(X_t X_t') = 2$ (variância de $X_t$) e $E(Y_{t+1}X_t') = 3$ (covariância entre $Y_{t+1}$ e $X_t$).
>  O coeficiente de projeção linear $\alpha$ é calculado como:
> $$\alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} = 3 \cdot 2^{-1} = 1.5$$
>  Portanto, a projeção linear de $Y_{t+1}$ sobre $X_t$ é $Y^*_{t+1} = 1.5 X_t$. Isto significa que, em média, para cada unidade de aumento em $X_t$, prevemos um aumento de 1.5 unidades em $Y_{t+1}$.

Como vimos anteriormente [^4.1.11], o erro quadrático médio de uma regra de previsão linear arbitrária $g'X_t$ é dado por
$$ E[Y_{t+1} - g'X_t]^2 = E[Y_{t+1} - \alpha'X_t]^2 + E[\alpha'X_t - g'X_t]^2 $$
O termo do meio se anula devido à condição de não correlação [^4.1.10]. Isso mostra que a projeção linear $\alpha'X_t$ é a função linear que minimiza o erro quadrático médio, já que o segundo termo é não-negativo e é zero quando $g'X_t = \alpha'X_t$ [^4.1.12].

O coeficiente de projeção $\alpha$ pode ser calculado usando momentos populacionais [^4.1.13]:
$$ \alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} $$
assumindo que a matriz $E(X_tX_t')$ seja não-singular. Se $E(X_tX_t')$ for singular, então $\alpha$ não será determinado de forma única, embora a previsão $\alpha'X_t$ ainda seja [^4.1.13]. A projeção linear não exige uma relação causal entre $X_t$ e $Y_{t+1}$, mas sim que os erros sejam ortogonais ao regressor [^4.1.10].

**Lema 1.1** (Decomposição do Erro): O erro de previsão de qualquer função linear $g'X_t$ pode ser decomposto em duas componentes: o erro da projeção linear $\alpha'X_t$ e um termo adicional que quantifica o desvio de $g'X_t$ em relação à $\alpha'X_t$.
$$ E[Y_{t+1} - g'X_t]^2 = E[Y_{t+1} - \alpha'X_t]^2 + E[(\alpha' - g')X_t]^2 $$
*Prova: Esta é uma reformulação da expansão do erro quadrático médio já apresentada anteriormente. A chave está na ortogonalidade entre o erro da projeção linear e o regressor, i.e. $E[(Y_{t+1}-\alpha'X_t)X_t] = 0$. O resultado segue diretamente da expansão e da aplicação da propriedade de ortogonalidade.*

*Prova:*
I. Começamos com o erro quadrático médio de uma previsão linear arbitrária $g'X_t$:
    $$ E[Y_{t+1} - g'X_t]^2 $$
II. Adicionamos e subtraímos $\alpha'X_t$ dentro dos colchetes:
    $$ E[Y_{t+1} - \alpha'X_t + \alpha'X_t - g'X_t]^2 $$
III. Reorganizamos os termos:
    $$ E[(Y_{t+1} - \alpha'X_t) + (\alpha'X_t - g'X_t)]^2 $$
IV. Expandimos o quadrado:
    $$ E[(Y_{t+1} - \alpha'X_t)^2 + 2(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t) + (\alpha'X_t - g'X_t)^2] $$
V. Pela linearidade da esperança, obtemos:
    $$ E[Y_{t+1} - \alpha'X_t]^2 + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t)] + E[\alpha'X_t - g'X_t]^2 $$
VI. O termo do meio pode ser reescrito como:
    $$ 2E[(Y_{t+1} - \alpha'X_t)X_t'(\alpha - g)] $$
VII. Pela condição de ortogonalidade $E[(Y_{t+1} - \alpha'X_t)X_t]=0'$, o termo do meio é zero:
$$ 2(\alpha - g)E[(Y_{t+1} - \alpha'X_t)X_t]=0 $$
VIII. O erro quadrático médio se reduz a:
    $$ E[Y_{t+1} - g'X_t]^2 = E[Y_{t+1} - \alpha'X_t]^2 + E[(\alpha' - g')X_t]^2 $$
■

> 💡 **Exemplo Numérico:** Continuando o exemplo anterior, suponha que, em vez de usar o $\alpha=1.5$, usamos um vetor $g=1$. O erro quadrático médio da projeção linear ($\alpha'X_t$) é $E[Y_{t+1} - 1.5X_t]^2$ e o erro quadrático médio da previsão linear arbitrária ($g'X_t$) é $E[Y_{t+1} - 1X_t]^2$. A decomposição do erro afirma que:
> $$ E[Y_{t+1} - 1X_t]^2 = E[Y_{t+1} - 1.5X_t]^2 + E[(1.5-1)X_t]^2 $$
> Assumindo que $E[Y_{t+1} - 1.5X_t]^2 = 1$ (o erro da projeção linear), e que $E[X_t^2] = 2$ (a variância de $X_t$), então:
> $$ E[Y_{t+1} - 1X_t]^2 = 1 + E[0.5 X_t]^2 = 1 + 0.25 \cdot 2 = 1.5 $$
> Isso ilustra que qualquer escolha diferente de $\alpha=1.5$ irá aumentar o erro quadrático médio da previsão.

Expandindo sobre a conexão entre projeção linear e regressão OLS [^4.1.16], notamos que a regressão OLS minimiza a soma dos quadrados dos resíduos amostrais [^4.1.17]:
$$ \sum_{t=1}^T (y_{t+1} - \beta'x_t)^2 $$
onde $y_{t+1}$ e $x_t$ representam observações específicas de $Y_{t+1}$ e $X_t$, respectivamente. O estimador OLS, $b$, é dado por [^4.1.18]:
$$ b = [\sum_{t=1}^T x_t x_t']^{-1} \sum_{t=1}^T x_t y_{t+1} $$
De acordo com [^4.1.19], esta fórmula pode ser reescrita como:
$$ b = [(1/T)\sum_{t=1}^T x_t x_t']^{-1} (1/T)\sum_{t=1}^T x_t y_{t+1} $$
Comparando os estimadores OLS com os coeficientes de projeção linear, vemos que ambos são construídos de formas similares: o estimador OLS usa momentos amostrais, enquanto a projeção linear usa momentos populacionais [^4.1.19]. O teorema de convergência de momentos [^4.1.20] estabelece que sob a condição de ergodicidade, os momentos amostrais convergem para os momentos populacionais conforme o tamanho da amostra aumenta para o infinito.

**Teorema 2.1** (Convergência do Estimador OLS para a Projeção Linear): Sob condições de ergodicidade e a condição de que $E(X_t X_t')$ seja não-singular, o estimador OLS $b$ converge em probabilidade para o coeficiente da projeção linear $\alpha$ conforme o tamanho da amostra $T$ tende ao infinito.
 *Prova: O estimador OLS $b$ é construído a partir dos momentos amostrais, enquanto o coeficiente da projeção linear $\alpha$ é construído a partir dos momentos populacionais. Pelo teorema de convergência dos momentos, sob ergodicidade, os momentos amostrais convergem para os momentos populacionais. Assim,  $ \frac{1}{T}\sum_{t=1}^T x_t x_t' \xrightarrow{p} E(X_t X_t')$ e $\frac{1}{T}\sum_{t=1}^T x_t y_{t+1} \xrightarrow{p} E(X_t Y_{t+1})$.  Portanto, $b = (\frac{1}{T}\sum_{t=1}^T x_t x_t')^{-1}  (\frac{1}{T}\sum_{t=1}^T x_t y_{t+1}) \xrightarrow{p} [E(X_t X_t')]^{-1} E(X_t Y_{t+1}) = \alpha$ .*

*Prova:*
I. O estimador OLS $b$ é dado por:
    $$ b = \left( \frac{1}{T}\sum_{t=1}^T x_t x_t' \right)^{-1} \left( \frac{1}{T}\sum_{t=1}^T x_t y_{t+1} \right) $$
II. O coeficiente de projeção linear $\alpha$ é dado por:
    $$ \alpha = \left( E(X_t X_t') \right)^{-1} E(X_t Y_{t+1}) $$
III. Pela lei dos grandes números (ou teorema de convergência de momentos), sob ergodicidade, os momentos amostrais convergem em probabilidade para os momentos populacionais:
$$ \frac{1}{T}\sum_{t=1}^T x_t x_t' \xrightarrow{p} E(X_t X_t') $$
$$ \frac{1}{T}\sum_{t=1}^T x_t y_{t+1} \xrightarrow{p} E(X_t Y_{t+1}) $$
IV. Aplicando o resultado acima ao estimador OLS:
$$ b = \left( \frac{1}{T}\sum_{t=1}^T x_t x_t' \right)^{-1} \left( \frac{1}{T}\sum_{t=1}^T x_t y_{t+1} \right) \xrightarrow{p} \left( E(X_t X_t') \right)^{-1} E(X_t Y_{t+1}) $$
V. Portanto:
$$ b \xrightarrow{p} \alpha $$
■

> 💡 **Exemplo Numérico:** Suponha que temos um conjunto de dados com 100 observações de $x_t$ e $y_{t+1}$.
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
>
> # Generate synthetic data
> np.random.seed(42)
> T = 100
> x_t = np.random.rand(T) * 10
> y_t_plus_1 = 2 + 1.5 * x_t + np.random.randn(T) * 2 # add random error
>
> df = pd.DataFrame({'x_t': x_t, 'y_t_plus_1': y_t_plus_1})
>
> # OLS Regression
> X = df[['x_t']]
> y = df['y_t_plus_1']
> model = LinearRegression()
> model.fit(X, y)
>
> b = model.coef_[0]
> intercept = model.intercept_
>
> # Calculate sample moments
> sample_x_variance = np.mean(x_t**2) - np.mean(x_t)**2
> sample_covariance = np.mean(x_t * y_t_plus_1) - np.mean(x_t) * np.mean(y_t_plus_1)
>
>
> print(f"OLS Coefficient (b): {b:.4f}")
> print(f"Sample Covariance: {sample_covariance:.4f}")
> print(f"Sample Variance of X: {sample_x_variance:.4f}")
>
> #Compare with the population values used in previous example
> print(f"Population Covariance: 3")
> print(f"Population Variance of X: 2")
> ```
> O código calcula o coeficiente OLS e também os momentos amostrais. Os resultados são:
> ```
> OLS Coefficient (b): 1.4894
> Sample Covariance: 14.9998
> Sample Variance of X: 8.5128
> Population Covariance: 3
> Population Variance of X: 2
> ```
> O estimador OLS (1.4894) aproxima-se do coeficiente da projeção linear (1.5), e os momentos amostrais aproximam-se dos momentos populacionais, ilustrando a convergência em probabilidade quando a amostra aumenta.
> A projeção linear seria $Y^*_{t+1} = 1.5 X_t$ e a previsão OLS seria $y^*_{t+1} = 2.0739 + 1.4894 x_t$.

Em termos matemáticos, há uma clara semelhança entre a projeção linear e a regressão OLS, e é possível mostrar que os resultados da regressão OLS podem ser vistos como um caso especial de projeção linear [^4.1.19].

### Considerações sobre a Relação Causal e Aplicações
A principal diferença entre projeção linear e análise estrutural reside nos pressupostos sobre a relação entre as variáveis. A análise estrutural busca modelar as relações causais subjacentes entre variáveis, com o objetivo de entender o impacto de mudanças em uma variável sobre outra. Isso requer um conjunto de pressupostos muito mais fortes sobre o processo que gera os dados [^4.1.20]. Em contrapartida, a projeção linear é um método de previsão. Ou seja, como demonstrado, a projeção linear não faz nenhuma afirmação sobre como ou por que $X_t$ e $Y_{t+1}$ se relacionam. Ela requer apenas o conhecimento de sua comovimentação histórica, sem levar em conta se X causa Y ou vice-versa.

**Observação 1:** A projeção linear, ao se basear em comovimentação histórica, captura relações estatísticas robustas para previsão, mesmo quando as relações causais são desconhecidas ou complexas. Ela se aproveita de padrões empíricos sem exigir a identificação de mecanismos causais, tornando-a uma ferramenta prática e flexível para previsão em uma variedade de contextos.

A análise estrutural é valiosa quando se pretende avaliar o impacto de intervenções específicas, por exemplo, analisar os efeitos de uma alteração na política econômica. Nesses casos, o entendimento da relação causal é imprescindível para avaliar o resultado de tais intervenções. No entanto, quando o foco é a previsão, é suficiente usar a projeção linear, uma vez que ela se baseia em momentos e padrões históricos sem nenhuma preocupação com a estrutura causal subjacente.

O ponto essencial a ser lembrado é que, para o objetivo de previsão, o importante são as comovimentações históricas entre as variáveis, e não a estrutura causal. A regressão OLS serve como uma base sólida para previsões, especialmente sob condições moderadas [^4.1.20].

### Conclusão
Neste capítulo, exploramos detalhadamente a projeção linear como uma ferramenta para previsão em séries temporais, comparando-a com a regressão OLS e enfatizando que ambas minimizam o erro quadrático médio de acordo com um conjunto específico de restrições. A principal diferença reside no escopo: a projeção linear é um método de previsão e a análise estrutural é um método de modelagem que exige mais pressupostos sobre o processo gerador de dados. A escolha entre as duas abordagens depende do objetivo: se o foco é a previsão, a projeção linear é mais apropriada, enquanto se o foco é entender a estrutura causal, a análise estrutural é preferível. Em essência, a projeção linear emerge como uma ferramenta eficaz e robusta para previsão em uma ampla variedade de aplicações práticas, particularmente quando a ênfase é na acurácia da previsão e não na interpretação causal das relações entre as variáveis.

### Referências
[^4.1.2]: *The forecast with the smallest mean squared error turns out to be the expectation of Y.+1 conditional on X₁:
Y*+1 = E(Y1+1|Χ.).*
[^4.1.9]: *We now restrict the class of forecasts considered by requiring the forecast
Y to be a linear function of X,:
Y+1 = α΄Χ.*
[^4.1.10]: *Suppose we were to find a value for a such that the forecast error (Y1+1 – α΄Χ.) is uncorrelated with X,:
Ε[(Υ.+1 – α΄Χ.)X] = 0'.*
[^4.1.11]: *E[Y1+1 - g'X]² = E[Y,+1 – α΄Χ, + α΄Χ, – g'X,]2
= E[Y,+1 – α΄Χ,]² + 2E{[Y,+1 – α΄Χ.] [α'X, – g'X,]}
+ E[α'X, g'X,]2.*
[^4.1.12]: *The optimal linear forecast g'X, is the value that sets the second term in [4.1.12] equal to zero.*
[^4.1.13]: *α' = E(Y+1X)[E(X,X;)]¯¹, assuming that E(X,X) is a nonsingular matrix. When E(XX) is singular, the coefficient vector a is not uniquely determined by [4.1.10], though the product of this vector with the explanatory variables, a'X,, is uniquely determined by [4.1.10].*
[^4.1.16]: *A linear regression model relates an observation on yr+1 to x₁:
Ус+1 = β'х, + 4,. *
[^4.1.17]: *Given a sample of T observations on y and x, the sample sum of squared residuals is defined as
Σ(9+1 - β΄Χ.).*
[^4.1.18]: *The value of ẞ that minimizes [4.1.17], denoted b, is the ordinary least squares (OLS) estimate of β. The formula for b turns out to be
b =  [Σ x,x;] [Σ x₁y₁+] . *
[^4.1.19]: *Comparing the OLS coefficient estimate b in equation [4.1.19] with the linear
projection coefficient a in equation [4.1.13], we see that b is constructed from the
sample moments (1/T)Σx,x; and (1/T)Σx+1 while a is constructed from population moments E(X,X) and E(X,Y,+1).*
[^4.1.20]: *Thus OLS regression of y₁+1 on x, yields a consistent estimate of the linear projection coefficient.*
<!-- END -->
