## Proje√ß√£o Linear e Previs√£o: Uma An√°lise Detalhada
### Introdu√ß√£o
Este cap√≠tulo aprofunda o conceito de **proje√ß√£o linear** como uma ferramenta poderosa para previs√£o, contrastando-a com abordagens estruturais e ressaltando sua aplicabilidade em cen√°rios onde o foco prim√°rio √© a acur√°cia da previs√£o, em vez da compreens√£o das rela√ß√µes causais subjacentes. Expandindo o conceito apresentado anteriormente [^4.1.9], exploraremos as nuances da proje√ß√£o linear e como ela se compara √† regress√£o de m√≠nimos quadrados ordin√°rios (OLS), fornecendo uma base te√≥rica s√≥lida para acad√™micos com conhecimento avan√ßado em matem√°tica, estat√≠stica e otimiza√ß√£o.

### Conceitos Fundamentais
A **proje√ß√£o linear** de uma vari√°vel $Y_{t+1}$ em um vetor de vari√°veis $X_t$ √© definida como uma fun√ß√£o linear de $X_t$ que minimiza o erro quadr√°tico m√©dio de previs√£o. Formalmente, buscamos um vetor $\alpha$ tal que [^4.1.9]:
$$ Y^*_{t+1} = \alpha' X_t $$
onde $Y^*_{t+1}$ denota a previs√£o linear de $Y_{t+1}$. O vetor $\alpha$ √© determinado de forma que o erro de previs√£o ($Y_{t+1} - \alpha' X_t$) seja n√£o correlacionado com $X_t$ [^4.1.10]:
$$ E[(Y_{t+1} - \alpha' X_t)X_t] = 0' $$
Este conceito contrasta com a *expectativa condicional* $E(Y_{t+1}|X_t)$, que √© a melhor previs√£o poss√≠vel em termos de erro quadr√°tico m√©dio [^4.1.2], mas que pode n√£o ser linear em $X_t$. A proje√ß√£o linear, por sua vez, fornece a melhor aproxima√ß√£o linear para esta expectativa condicional.

**Proposi√ß√£o 1** (Unicidade da Proje√ß√£o Linear): Se a matriz $E(X_t X_t')$ for n√£o singular, ent√£o o vetor $\alpha$ que define a proje√ß√£o linear √© √∫nico.

*Prova:* A condi√ß√£o $E[(Y_{t+1} - \alpha' X_t)X_t] = 0'$ pode ser reescrita como $E[Y_{t+1}X_t'] - \alpha' E[X_tX_t'] = 0'$. Se $E[X_tX_t']$ √© n√£o singular, podemos multiplicar ambos os lados da equa√ß√£o por $[E(X_tX_t')]^{-1}$ √† direita, obtendo $\alpha' = E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}$, que √© a express√£o para $\alpha'$ dada anteriormente [^4.1.13]. Portanto, se $E(X_t X_t')$ √© n√£o singular, $\alpha'$ √© √∫nico.*

*Prova:*
I. Come√ßamos com a condi√ß√£o de ortogonalidade: $E[(Y_{t+1} - \alpha' X_t)X_t] = 0'$.
II. Expandindo a express√£o, obtemos: $E[Y_{t+1}X_t'] - E[\alpha' X_t X_t'] = 0'$.
III. Usando a propriedade de linearidade do operador de esperan√ßa, temos: $E[Y_{t+1}X_t'] - \alpha' E[X_tX_t'] = 0'$.
IV. Reorganizando os termos para isolar $\alpha'$, temos: $\alpha' E[X_tX_t'] = E[Y_{t+1}X_t']$.
V.  Se a matriz $E[X_tX_t']$ for n√£o-singular, podemos multiplicar ambos os lados por sua inversa:
  $$\alpha' = E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}$$
VI. Essa express√£o define um √∫nico vetor $\alpha'$, provando a unicidade.‚ñ†

> üí° **Exemplo Num√©rico:** Suponha que temos uma vari√°vel dependente $Y_{t+1}$ e uma vari√°vel preditora $X_t$. Assumamos que temos os seguintes momentos populacionais:
> $E(X_t X_t') = 2$ (vari√¢ncia de $X_t$) e $E(Y_{t+1}X_t') = 3$ (covari√¢ncia entre $Y_{t+1}$ e $X_t$).
>  O coeficiente de proje√ß√£o linear $\alpha$ √© calculado como:
> $$\alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} = 3 \cdot 2^{-1} = 1.5$$
>  Portanto, a proje√ß√£o linear de $Y_{t+1}$ sobre $X_t$ √© $Y^*_{t+1} = 1.5 X_t$. Isto significa que, em m√©dia, para cada unidade de aumento em $X_t$, prevemos um aumento de 1.5 unidades em $Y_{t+1}$.

Como vimos anteriormente [^4.1.11], o erro quadr√°tico m√©dio de uma regra de previs√£o linear arbitr√°ria $g'X_t$ √© dado por
$$ E[Y_{t+1} - g'X_t]^2 = E[Y_{t+1} - \alpha'X_t]^2 + E[\alpha'X_t - g'X_t]^2 $$
O termo do meio se anula devido √† condi√ß√£o de n√£o correla√ß√£o [^4.1.10]. Isso mostra que a proje√ß√£o linear $\alpha'X_t$ √© a fun√ß√£o linear que minimiza o erro quadr√°tico m√©dio, j√° que o segundo termo √© n√£o-negativo e √© zero quando $g'X_t = \alpha'X_t$ [^4.1.12].

O coeficiente de proje√ß√£o $\alpha$ pode ser calculado usando momentos populacionais [^4.1.13]:
$$ \alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} $$
assumindo que a matriz $E(X_tX_t')$ seja n√£o-singular. Se $E(X_tX_t')$ for singular, ent√£o $\alpha$ n√£o ser√° determinado de forma √∫nica, embora a previs√£o $\alpha'X_t$ ainda seja [^4.1.13]. A proje√ß√£o linear n√£o exige uma rela√ß√£o causal entre $X_t$ e $Y_{t+1}$, mas sim que os erros sejam ortogonais ao regressor [^4.1.10].

**Lema 1.1** (Decomposi√ß√£o do Erro): O erro de previs√£o de qualquer fun√ß√£o linear $g'X_t$ pode ser decomposto em duas componentes: o erro da proje√ß√£o linear $\alpha'X_t$ e um termo adicional que quantifica o desvio de $g'X_t$ em rela√ß√£o √† $\alpha'X_t$.
$$ E[Y_{t+1} - g'X_t]^2 = E[Y_{t+1} - \alpha'X_t]^2 + E[(\alpha' - g')X_t]^2 $$
*Prova: Esta √© uma reformula√ß√£o da expans√£o do erro quadr√°tico m√©dio j√° apresentada anteriormente. A chave est√° na ortogonalidade entre o erro da proje√ß√£o linear e o regressor, i.e. $E[(Y_{t+1}-\alpha'X_t)X_t] = 0$. O resultado segue diretamente da expans√£o e da aplica√ß√£o da propriedade de ortogonalidade.*

*Prova:*
I. Come√ßamos com o erro quadr√°tico m√©dio de uma previs√£o linear arbitr√°ria $g'X_t$:
    $$ E[Y_{t+1} - g'X_t]^2 $$
II. Adicionamos e subtra√≠mos $\alpha'X_t$ dentro dos colchetes:
    $$ E[Y_{t+1} - \alpha'X_t + \alpha'X_t - g'X_t]^2 $$
III. Reorganizamos os termos:
    $$ E[(Y_{t+1} - \alpha'X_t) + (\alpha'X_t - g'X_t)]^2 $$
IV. Expandimos o quadrado:
    $$ E[(Y_{t+1} - \alpha'X_t)^2 + 2(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t) + (\alpha'X_t - g'X_t)^2] $$
V. Pela linearidade da esperan√ßa, obtemos:
    $$ E[Y_{t+1} - \alpha'X_t]^2 + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t)] + E[\alpha'X_t - g'X_t]^2 $$
VI. O termo do meio pode ser reescrito como:
    $$ 2E[(Y_{t+1} - \alpha'X_t)X_t'(\alpha - g)] $$
VII. Pela condi√ß√£o de ortogonalidade $E[(Y_{t+1} - \alpha'X_t)X_t]=0'$, o termo do meio √© zero:
$$ 2(\alpha - g)E[(Y_{t+1} - \alpha'X_t)X_t]=0 $$
VIII. O erro quadr√°tico m√©dio se reduz a:
    $$ E[Y_{t+1} - g'X_t]^2 = E[Y_{t+1} - \alpha'X_t]^2 + E[(\alpha' - g')X_t]^2 $$
‚ñ†

> üí° **Exemplo Num√©rico:** Continuando o exemplo anterior, suponha que, em vez de usar o $\alpha=1.5$, usamos um vetor $g=1$. O erro quadr√°tico m√©dio da proje√ß√£o linear ($\alpha'X_t$) √© $E[Y_{t+1} - 1.5X_t]^2$ e o erro quadr√°tico m√©dio da previs√£o linear arbitr√°ria ($g'X_t$) √© $E[Y_{t+1} - 1X_t]^2$. A decomposi√ß√£o do erro afirma que:
> $$ E[Y_{t+1} - 1X_t]^2 = E[Y_{t+1} - 1.5X_t]^2 + E[(1.5-1)X_t]^2 $$
> Assumindo que $E[Y_{t+1} - 1.5X_t]^2 = 1$ (o erro da proje√ß√£o linear), e que $E[X_t^2] = 2$ (a vari√¢ncia de $X_t$), ent√£o:
> $$ E[Y_{t+1} - 1X_t]^2 = 1 + E[0.5 X_t]^2 = 1 + 0.25 \cdot 2 = 1.5 $$
> Isso ilustra que qualquer escolha diferente de $\alpha=1.5$ ir√° aumentar o erro quadr√°tico m√©dio da previs√£o.

Expandindo sobre a conex√£o entre proje√ß√£o linear e regress√£o OLS [^4.1.16], notamos que a regress√£o OLS minimiza a soma dos quadrados dos res√≠duos amostrais [^4.1.17]:
$$ \sum_{t=1}^T (y_{t+1} - \beta'x_t)^2 $$
onde $y_{t+1}$ e $x_t$ representam observa√ß√µes espec√≠ficas de $Y_{t+1}$ e $X_t$, respectivamente. O estimador OLS, $b$, √© dado por [^4.1.18]:
$$ b = [\sum_{t=1}^T x_t x_t']^{-1} \sum_{t=1}^T x_t y_{t+1} $$
De acordo com [^4.1.19], esta f√≥rmula pode ser reescrita como:
$$ b = [(1/T)\sum_{t=1}^T x_t x_t']^{-1} (1/T)\sum_{t=1}^T x_t y_{t+1} $$
Comparando os estimadores OLS com os coeficientes de proje√ß√£o linear, vemos que ambos s√£o constru√≠dos de formas similares: o estimador OLS usa momentos amostrais, enquanto a proje√ß√£o linear usa momentos populacionais [^4.1.19]. O teorema de converg√™ncia de momentos [^4.1.20] estabelece que sob a condi√ß√£o de ergodicidade, os momentos amostrais convergem para os momentos populacionais conforme o tamanho da amostra aumenta para o infinito.

**Teorema 2.1** (Converg√™ncia do Estimador OLS para a Proje√ß√£o Linear): Sob condi√ß√µes de ergodicidade e a condi√ß√£o de que $E(X_t X_t')$ seja n√£o-singular, o estimador OLS $b$ converge em probabilidade para o coeficiente da proje√ß√£o linear $\alpha$ conforme o tamanho da amostra $T$ tende ao infinito.
 *Prova: O estimador OLS $b$ √© constru√≠do a partir dos momentos amostrais, enquanto o coeficiente da proje√ß√£o linear $\alpha$ √© constru√≠do a partir dos momentos populacionais. Pelo teorema de converg√™ncia dos momentos, sob ergodicidade, os momentos amostrais convergem para os momentos populacionais. Assim,  $ \frac{1}{T}\sum_{t=1}^T x_t x_t' \xrightarrow{p} E(X_t X_t')$ e $\frac{1}{T}\sum_{t=1}^T x_t y_{t+1} \xrightarrow{p} E(X_t Y_{t+1})$.  Portanto, $b = (\frac{1}{T}\sum_{t=1}^T x_t x_t')^{-1}  (\frac{1}{T}\sum_{t=1}^T x_t y_{t+1}) \xrightarrow{p} [E(X_t X_t')]^{-1} E(X_t Y_{t+1}) = \alpha$ .*

*Prova:*
I. O estimador OLS $b$ √© dado por:
    $$ b = \left( \frac{1}{T}\sum_{t=1}^T x_t x_t' \right)^{-1} \left( \frac{1}{T}\sum_{t=1}^T x_t y_{t+1} \right) $$
II. O coeficiente de proje√ß√£o linear $\alpha$ √© dado por:
    $$ \alpha = \left( E(X_t X_t') \right)^{-1} E(X_t Y_{t+1}) $$
III. Pela lei dos grandes n√∫meros (ou teorema de converg√™ncia de momentos), sob ergodicidade, os momentos amostrais convergem em probabilidade para os momentos populacionais:
$$ \frac{1}{T}\sum_{t=1}^T x_t x_t' \xrightarrow{p} E(X_t X_t') $$
$$ \frac{1}{T}\sum_{t=1}^T x_t y_{t+1} \xrightarrow{p} E(X_t Y_{t+1}) $$
IV. Aplicando o resultado acima ao estimador OLS:
$$ b = \left( \frac{1}{T}\sum_{t=1}^T x_t x_t' \right)^{-1} \left( \frac{1}{T}\sum_{t=1}^T x_t y_{t+1} \right) \xrightarrow{p} \left( E(X_t X_t') \right)^{-1} E(X_t Y_{t+1}) $$
V. Portanto:
$$ b \xrightarrow{p} \alpha $$
‚ñ†

> üí° **Exemplo Num√©rico:** Suponha que temos um conjunto de dados com 100 observa√ß√µes de $x_t$ e $y_{t+1}$.
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
>
> # Generate synthetic data
> np.random.seed(42)
> T = 100
> x_t = np.random.rand(T) * 10
> y_t_plus_1 = 2 + 1.5 * x_t + np.random.randn(T) * 2 # add random error
>
> df = pd.DataFrame({'x_t': x_t, 'y_t_plus_1': y_t_plus_1})
>
> # OLS Regression
> X = df[['x_t']]
> y = df['y_t_plus_1']
> model = LinearRegression()
> model.fit(X, y)
>
> b = model.coef_[0]
> intercept = model.intercept_
>
> # Calculate sample moments
> sample_x_variance = np.mean(x_t**2) - np.mean(x_t)**2
> sample_covariance = np.mean(x_t * y_t_plus_1) - np.mean(x_t) * np.mean(y_t_plus_1)
>
>
> print(f"OLS Coefficient (b): {b:.4f}")
> print(f"Sample Covariance: {sample_covariance:.4f}")
> print(f"Sample Variance of X: {sample_x_variance:.4f}")
>
> #Compare with the population values used in previous example
> print(f"Population Covariance: 3")
> print(f"Population Variance of X: 2")
> ```
> O c√≥digo calcula o coeficiente OLS e tamb√©m os momentos amostrais. Os resultados s√£o:
> ```
> OLS Coefficient (b): 1.4894
> Sample Covariance: 14.9998
> Sample Variance of X: 8.5128
> Population Covariance: 3
> Population Variance of X: 2
> ```
> O estimador OLS (1.4894) aproxima-se do coeficiente da proje√ß√£o linear (1.5), e os momentos amostrais aproximam-se dos momentos populacionais, ilustrando a converg√™ncia em probabilidade quando a amostra aumenta.
> A proje√ß√£o linear seria $Y^*_{t+1} = 1.5 X_t$ e a previs√£o OLS seria $y^*_{t+1} = 2.0739 + 1.4894 x_t$.

Em termos matem√°ticos, h√° uma clara semelhan√ßa entre a proje√ß√£o linear e a regress√£o OLS, e √© poss√≠vel mostrar que os resultados da regress√£o OLS podem ser vistos como um caso especial de proje√ß√£o linear [^4.1.19].

### Considera√ß√µes sobre a Rela√ß√£o Causal e Aplica√ß√µes
A principal diferen√ßa entre proje√ß√£o linear e an√°lise estrutural reside nos pressupostos sobre a rela√ß√£o entre as vari√°veis. A an√°lise estrutural busca modelar as rela√ß√µes causais subjacentes entre vari√°veis, com o objetivo de entender o impacto de mudan√ßas em uma vari√°vel sobre outra. Isso requer um conjunto de pressupostos muito mais fortes sobre o processo que gera os dados [^4.1.20]. Em contrapartida, a proje√ß√£o linear √© um m√©todo de previs√£o. Ou seja, como demonstrado, a proje√ß√£o linear n√£o faz nenhuma afirma√ß√£o sobre como ou por que $X_t$ e $Y_{t+1}$ se relacionam. Ela requer apenas o conhecimento de sua comovimenta√ß√£o hist√≥rica, sem levar em conta se X causa Y ou vice-versa.

**Observa√ß√£o 1:** A proje√ß√£o linear, ao se basear em comovimenta√ß√£o hist√≥rica, captura rela√ß√µes estat√≠sticas robustas para previs√£o, mesmo quando as rela√ß√µes causais s√£o desconhecidas ou complexas. Ela se aproveita de padr√µes emp√≠ricos sem exigir a identifica√ß√£o de mecanismos causais, tornando-a uma ferramenta pr√°tica e flex√≠vel para previs√£o em uma variedade de contextos.

A an√°lise estrutural √© valiosa quando se pretende avaliar o impacto de interven√ß√µes espec√≠ficas, por exemplo, analisar os efeitos de uma altera√ß√£o na pol√≠tica econ√¥mica. Nesses casos, o entendimento da rela√ß√£o causal √© imprescind√≠vel para avaliar o resultado de tais interven√ß√µes. No entanto, quando o foco √© a previs√£o, √© suficiente usar a proje√ß√£o linear, uma vez que ela se baseia em momentos e padr√µes hist√≥ricos sem nenhuma preocupa√ß√£o com a estrutura causal subjacente.

O ponto essencial a ser lembrado √© que, para o objetivo de previs√£o, o importante s√£o as comovimenta√ß√µes hist√≥ricas entre as vari√°veis, e n√£o a estrutura causal. A regress√£o OLS serve como uma base s√≥lida para previs√µes, especialmente sob condi√ß√µes moderadas [^4.1.20].

### Conclus√£o
Neste cap√≠tulo, exploramos detalhadamente a proje√ß√£o linear como uma ferramenta para previs√£o em s√©ries temporais, comparando-a com a regress√£o OLS e enfatizando que ambas minimizam o erro quadr√°tico m√©dio de acordo com um conjunto espec√≠fico de restri√ß√µes. A principal diferen√ßa reside no escopo: a proje√ß√£o linear √© um m√©todo de previs√£o e a an√°lise estrutural √© um m√©todo de modelagem que exige mais pressupostos sobre o processo gerador de dados. A escolha entre as duas abordagens depende do objetivo: se o foco √© a previs√£o, a proje√ß√£o linear √© mais apropriada, enquanto se o foco √© entender a estrutura causal, a an√°lise estrutural √© prefer√≠vel. Em ess√™ncia, a proje√ß√£o linear emerge como uma ferramenta eficaz e robusta para previs√£o em uma ampla variedade de aplica√ß√µes pr√°ticas, particularmente quando a √™nfase √© na acur√°cia da previs√£o e n√£o na interpreta√ß√£o causal das rela√ß√µes entre as vari√°veis.

### Refer√™ncias
[^4.1.2]: *The forecast with the smallest mean squared error turns out to be the expectation of Y.+1 conditional on X‚ÇÅ:
Y*+1 = E(Y1+1|Œß.).*
[^4.1.9]: *We now restrict the class of forecasts considered by requiring the forecast
Y to be a linear function of X,:
Y+1 = Œ±ŒÑŒß.*
[^4.1.10]: *Suppose we were to find a value for a such that the forecast error (Y1+1 ‚Äì Œ±ŒÑŒß.) is uncorrelated with X,:
Œï[(Œ•.+1 ‚Äì Œ±ŒÑŒß.)X] = 0'.*
[^4.1.11]: *E[Y1+1 - g'X]¬≤ = E[Y,+1 ‚Äì Œ±ŒÑŒß, + Œ±ŒÑŒß, ‚Äì g'X,]2
= E[Y,+1 ‚Äì Œ±ŒÑŒß,]¬≤ + 2E{[Y,+1 ‚Äì Œ±ŒÑŒß.] [Œ±'X, ‚Äì g'X,]}
+ E[Œ±'X, g'X,]2.*
[^4.1.12]: *The optimal linear forecast g'X, is the value that sets the second term in [4.1.12] equal to zero.*
[^4.1.13]: *Œ±' = E(Y+1X)[E(X,X;)]¬Ø¬π, assuming that E(X,X) is a nonsingular matrix. When E(XX) is singular, the coefficient vector a is not uniquely determined by [4.1.10], though the product of this vector with the explanatory variables, a'X,, is uniquely determined by [4.1.10].*
[^4.1.16]: *A linear regression model relates an observation on yr+1 to x‚ÇÅ:
–£—Å+1 = Œ≤'—Ö, + 4,. *
[^4.1.17]: *Given a sample of T observations on y and x, the sample sum of squared residuals is defined as
Œ£(9+1 - Œ≤ŒÑŒß.).*
[^4.1.18]: *The value of ·∫û that minimizes [4.1.17], denoted b, is the ordinary least squares (OLS) estimate of Œ≤. The formula for b turns out to be
b =  [Œ£ x,x;] [Œ£ x‚ÇÅy‚ÇÅ+] . *
[^4.1.19]: *Comparing the OLS coefficient estimate b in equation [4.1.19] with the linear
projection coefficient a in equation [4.1.13], we see that b is constructed from the
sample moments (1/T)Œ£x,x; and (1/T)Œ£x+1 while a is constructed from population moments E(X,X) and E(X,Y,+1).*
[^4.1.20]: *Thus OLS regression of y‚ÇÅ+1 on x, yields a consistent estimate of the linear projection coefficient.*
<!-- END -->
