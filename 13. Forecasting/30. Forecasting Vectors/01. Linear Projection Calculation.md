## Proje√ß√µes Lineares Vetoriais e o C√°lculo da Matriz de Coeficientes

### Introdu√ß√£o
Em continuidade √† an√°lise de *previs√µes baseadas em proje√ß√£o*, este cap√≠tulo explora o cen√°rio onde o que se busca prever √© um vetor de vari√°veis, e n√£o apenas uma vari√°vel escalar. Como foi visto anteriormente, a proje√ß√£o linear √© uma ferramenta poderosa para construir previs√µes, e aqui vamos generalizar esse conceito para o contexto multivariado.  Especificamente, o foco ser√° em como calcular a matriz de coeficientes que minimiza o erro quadr√°tico m√©dio (MSE) para cada elemento do vetor de vari√°veis dependentes.

### Conceitos Fundamentais
A generaliza√ß√£o das proje√ß√µes lineares para o contexto vetorial envolve considerar um vetor $Y_{t+1}$ de dimens√£o $n \times 1$, que se pretende prever, com base em um vetor $X_t$ de dimens√£o $m \times 1$. A proje√ß√£o linear de $Y_{t+1}$ em $X_t$ √© dada por:

$$
\hat{Y}_{t+1} = P(Y_{t+1} | X_t) = \alpha' X_t,
$$
onde $\alpha'$ √© uma matriz de coeficientes de dimens√£o $n \times m$ [^4.1.21].

O objetivo √© determinar essa matriz $\alpha'$ de modo que cada elemento do vetor $\hat{Y}_{t+1}$ seja a melhor previs√£o de seu correspondente em $Y_{t+1}$, no sentido de minimizar o erro quadr√°tico m√©dio (MSE). Essa condi√ß√£o de minimiza√ß√£o do MSE √© equivalente a exigir que o erro de previs√£o $(Y_{t+1} - \hat{Y}_{t+1})$ seja n√£o correlacionado com cada um dos elementos de $X_t$, conforme expressado em:

$$
E[(Y_{t+1} - \alpha'X_t)X_t'] = 0,
$$
onde $0$ representa uma matriz nula de dimens√£o $n \times m$ [^4.1.22]. Esta condi√ß√£o garante que a proje√ß√£o linear $\alpha'X_t$ esgota toda a informa√ß√£o de $X_t$ √∫til para prever $Y_{t+1}$.

**Proposi√ß√£o 1:** A condi√ß√£o $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$ implica que cada elemento do vetor erro $Y_{t+1} - \hat{Y}_{t+1}$ √© ortogonal a cada elemento do vetor $X_t$.

*Prova:* 
I. A igualdade $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$ pode ser escrita como $E[(Y_{t+1} - \hat{Y}_{t+1})X_t'] = 0$, dado que $\hat{Y}_{t+1} = \alpha'X_t$.
II. Expandindo a express√£o, temos que $E[(Y_{t+1} - \hat{Y}_{t+1})X_t'] = \begin{bmatrix} E[(Y_{t+1,1} - \hat{Y}_{t+1,1})X_{t,1}] & \ldots & E[(Y_{t+1,1} - \hat{Y}_{t+1,1})X_{t,m}] \\ \vdots & \ddots & \vdots \\ E[(Y_{t+1,n} - \hat{Y}_{t+1,n})X_{t,1}] & \ldots & E[(Y_{t+1,n} - \hat{Y}_{t+1,n})X_{t,m}] \end{bmatrix} = 0$, onde 0 √© a matriz nula de dimens√£o $n \times m$.
III. Cada elemento da matriz resultante √© da forma $E[(Y_{t+1,i} - \hat{Y}_{t+1,i})X_{t,j}] = 0$ para todo $i = 1, \ldots, n$ e $j = 1, \ldots, m$.
IV.  Esta igualdade implica que a covari√¢ncia entre o erro de previs√£o para a *i*-√©sima componente de $Y$ e a *j*-√©sima componente de $X$ √© nula.
V. Uma covari√¢ncia nula implica ortogonalidade. Portanto, cada elemento do vetor erro $Y_{t+1} - \hat{Y}_{t+1}$ √© ortogonal a cada elemento do vetor $X_t$. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere o caso em que $Y_{t+1}$ √© um vetor de dimens√£o 2x1 (e.g., pre√ßos de duas a√ß√µes) e $X_t$ √© um vetor de dimens√£o 2x1 (e.g., dois indicadores econ√¥micos). Suponha que temos as seguintes realiza√ß√µes para $Y_{t+1}$ e $X_t$:
>
> $$Y_{t+1} = \begin{bmatrix} 5 \\ 7 \end{bmatrix}, X_t = \begin{bmatrix} 2 \\ 3 \end{bmatrix}$$
>
> A proje√ß√£o linear de $Y_{t+1}$ em $X_t$ √© dada por $\hat{Y}_{t+1} = \alpha'X_t$, onde $\alpha'$ √© uma matriz 2x2.
> Se $\alpha' = \begin{bmatrix} 1 & 0.5 \\ 0.2 & 1 \end{bmatrix}$, ent√£o $\hat{Y}_{t+1} = \begin{bmatrix} 1 & 0.5 \\ 0.2 & 1 \end{bmatrix} \begin{bmatrix} 2 \\ 3 \end{bmatrix} = \begin{bmatrix} 3.5 \\ 3.4 \end{bmatrix}$.
>
> O vetor erro √© ent√£o $Y_{t+1} - \hat{Y}_{t+1} = \begin{bmatrix} 5 \\ 7 \end{bmatrix} - \begin{bmatrix} 3.5 \\ 3.4 \end{bmatrix} = \begin{bmatrix} 1.5 \\ 3.6 \end{bmatrix}$.
>
> A condi√ß√£o de ortogonalidade exige que a covari√¢ncia entre o erro e cada elemento de $X_t$ seja zero. No nosso exemplo, usando amostras para aproximar o valor esperado, isto √©:
>
> $E[(Y_{t+1,1} - \hat{Y}_{t+1,1})X_{t,1}] = 1.5 \times 2 = 3$
> $E[(Y_{t+1,1} - \hat{Y}_{t+1,1})X_{t,2}] = 1.5 \times 3 = 4.5$
> $E[(Y_{t+1,2} - \hat{Y}_{t+1,2})X_{t,1}] = 3.6 \times 2 = 7.2$
> $E[(Y_{t+1,2} - \hat{Y}_{t+1,2})X_{t,2}] = 3.6 \times 3 = 10.8$
>
>  A matriz $E[(Y_{t+1} - \hat{Y}_{t+1})X_t']$ √© $\begin{bmatrix} 3 & 4.5 \\ 7.2 & 10.8 \end{bmatrix}$ que √© diferente da matriz nula. Portanto, $\alpha'$ precisa ser calculado de maneira a satisfazer a condi√ß√£o de ortogonalidade.

Para obter a matriz de coeficientes $\alpha'$, resolvemos a equa√ß√£o acima, e obtemos [^4.1.23]:
$$
\alpha' = [E(Y_{t+1}X_t')] [E(X_t X_t')]^{-1}.
$$

Esta equa√ß√£o fornece a matriz de coeficientes que minimiza o MSE para cada um dos elementos de $Y_{t+1}$.

> üí° **Exemplo Num√©rico:** Continuando o exemplo anterior, suponha que temos um conjunto de dados maior que nos permite estimar $E(Y_{t+1}X_t')$ e $E(X_t X_t')$. Suponha que ap√≥s calcular as m√©dias amostrais, obtivemos:
>
> $E(Y_{t+1}X_t') = \begin{bmatrix} 10 & 15 \\ 12 & 18 \end{bmatrix}$
>
> $E(X_t X_t') = \begin{bmatrix} 5 & 6 \\ 6 & 10 \end{bmatrix}$
>
> Para calcular $\alpha'$, primeiro precisamos inverter $E(X_t X_t')$:
>
> $(E(X_t X_t'))^{-1} = \frac{1}{(5 \times 10) - (6 \times 6)} \begin{bmatrix} 10 & -6 \\ -6 & 5 \end{bmatrix} = \frac{1}{14} \begin{bmatrix} 10 & -6 \\ -6 & 5 \end{bmatrix} = \begin{bmatrix} 10/14 & -6/14 \\ -6/14 & 5/14 \end{bmatrix} \approx \begin{bmatrix} 0.714 & -0.429 \\ -0.429 & 0.357 \end{bmatrix}$
>
> Ent√£o,
>
> $\alpha' = E(Y_{t+1}X_t') [E(X_t X_t')]^{-1} = \begin{bmatrix} 10 & 15 \\ 12 & 18 \end{bmatrix} \begin{bmatrix} 0.714 & -0.429 \\ -0.429 & 0.357 \end{bmatrix} = \begin{bmatrix} (10*0.714 - 15*0.429) & (-10*0.429+15*0.357) \\ (12*0.714-18*0.429) & (-12*0.429 + 18*0.357)\end{bmatrix} = \begin{bmatrix} 0.7155 & 1.041 \\ 0.858 & 1.116 \end{bmatrix}$
>
> A matriz $\alpha'$ resultante fornece os coeficientes para prever $Y_{t+1}$ usando $X_t$.

**Lema 1:** A matriz $E(X_t X_t')$ √© invert√≠vel se e somente se n√£o existe nenhuma combina√ß√£o linear dos elementos de $X_t$ que seja igual a zero com probabilidade 1.

*Prova:*
I. A matriz $E(X_t X_t')$ √© a matriz de covari√¢ncia de $X_t$ (assumindo que $E(X_t) = 0$). 
II. Uma matriz de covari√¢ncia √© invert√≠vel se e somente se seus vetores de coluna (ou linha) s√£o linearmente independentes. 
III. Se existe uma combina√ß√£o linear dos elementos de $X_t$ igual a zero com probabilidade 1, ent√£o existe um vetor $c \ne 0$ tal que $c'X_t = 0$ com probabilidade 1.
IV. Isso implica que $E(c'X_t X_t'c) = E((c'X_t)^2) = 0$.  
V. Desde que $(c'X_t)^2$ √© um quadrado, $E((c'X_t)^2) = 0$ implica que $c'X_t = 0$ com probabilidade 1.
VI. Se $c'X_t=0$, ent√£o as colunas da matriz de covari√¢ncia de $X_t$ s√£o linearmente dependentes, e, portanto, a matriz n√£o √© invert√≠vel.
VII. Reciprocamente, se as colunas de $E(X_t X_t')$ s√£o linearmente dependentes, ent√£o existe um vetor $c \ne 0$ tal que $E(X_t X_t')c = 0$, que implica $c'E(X_t X_t')c = 0$, ou seja, $E(c'X_t X_t'c) = E((c'X_t)^2)=0$, o que implica $c'X_t=0$ com probabilidade 1.  ‚ñ†

Expandindo um pouco mais, o MSE desta previs√£o √≥tima √© dado por [^4.1.24]:

$$
MSE(\alpha'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)'] = E[Y_{t+1}Y_{t+1}'] - E[Y_{t+1}X_t'] [E(X_t X_t')]^{-1} E[X_tY_{t+1}'].
$$
Este resultado √© uma generaliza√ß√£o da f√≥rmula de MSE para o caso escalar. √â importante notar que, cada elemento $j$ do vetor $\hat{Y}_{t+1}$  fornece a previs√£o de $Y_{j, t+1}$ com o menor MSE poss√≠vel, e, al√©m disso, qualquer combina√ß√£o linear $h'\hat{Y}_{t+1}$ fornece a previs√£o com menor MSE de $h'Y_{t+1}$ para qualquer vetor $h$.

> üí° **Exemplo Num√©rico:** Usando os valores do exemplo anterior, suponha que $E(Y_{t+1}Y_{t+1}') = \begin{bmatrix} 30 & 35 \\ 35 & 45 \end{bmatrix}$.
>
> Ent√£o,
>
> $E[X_tY_{t+1}'] = (E[Y_{t+1}X_t'])' = \begin{bmatrix} 10 & 12 \\ 15 & 18 \end{bmatrix}$.
>
> $MSE(\alpha'X_t) = E[Y_{t+1}Y_{t+1}'] - E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1} E[X_tY_{t+1}'] = \begin{bmatrix} 30 & 35 \\ 35 & 45 \end{bmatrix} - \begin{bmatrix} 10 & 15 \\ 12 & 18 \end{bmatrix} \begin{bmatrix} 0.714 & -0.429 \\ -0.429 & 0.357 \end{bmatrix} \begin{bmatrix} 10 & 12 \\ 15 & 18 \end{bmatrix} = \begin{bmatrix} 30 & 35 \\ 35 & 45 \end{bmatrix} - \begin{bmatrix} 0.7155 & 1.041 \\ 0.858 & 1.116 \end{bmatrix} \begin{bmatrix} 10 & 12 \\ 15 & 18 \end{bmatrix} = \begin{bmatrix} 30 & 35 \\ 35 & 45 \end{bmatrix} - \begin{bmatrix} 22.77 & 27.53 \\ 25.62 & 31.00 \end{bmatrix} = \begin{bmatrix} 7.23 & 7.47 \\ 9.38 & 14.00 \end{bmatrix}$
>
> A matriz MSE resultante representa o erro quadr√°tico m√©dio da proje√ß√£o linear vetorial. O tra√ßo da matriz MSE √© $7.23+14=21.23$ e representa a soma dos erros quadr√°ticos m√©dios de cada vari√°vel em $Y_{t+1}$.

**Teorema 1.1:** A proje√ß√£o linear $\hat{Y}_{t+1} = \alpha'X_t$, com $\alpha'$ dado por $\alpha' = [E(Y_{t+1}X_t')] [E(X_t X_t')]^{-1}$, √© a melhor previs√£o linear de $Y_{t+1}$ no sentido de minimizar o tra√ßo da matriz $MSE(\alpha'X_t)$.

*Prova:*
I. A matriz $MSE(\alpha'X_t)$ √© definida como $E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)']$.
II. Essa matriz √© semidefinida positiva, o que significa que todos os seus autovalores s√£o n√£o negativos. 
III. O tra√ßo de uma matriz √© a soma de seus autovalores, e portanto, o tra√ßo de $MSE(\alpha'X_t)$ √© a soma das vari√¢ncias dos elementos do vetor erro $Y_{t+1}-\hat{Y}_{t+1}$ e de suas covari√¢ncias.
IV. Minimizar o tra√ßo de $MSE(\alpha'X_t)$ equivale a minimizar a soma das vari√¢ncias de cada elemento do vetor de erro, $Y_{t+1} - \hat{Y}_{t+1}$, mais as covari√¢ncias.
V. J√° sabemos que a proje√ß√£o $\alpha'X_t$ minimiza o MSE para cada elemento de $Y_{t+1}$, ou seja, minimiza a vari√¢ncia de cada elemento do vetor erro.
VI. Portanto, a matriz $\alpha'$ encontrada minimiza a soma das vari√¢ncias e covari√¢ncias dos erros, que √© o tra√ßo da matriz $MSE(\alpha'X_t)$. ‚ñ†

### Conclus√£o
O desenvolvimento da proje√ß√£o linear vetorial fornece uma base te√≥rica s√≥lida para a previs√£o de s√©ries temporais multivariadas. O c√°lculo da matriz de coeficientes $\alpha'$ usando momentos populacionais, que vimos neste cap√≠tulo, √© crucial para obter previs√µes √≥timas no sentido do MSE. Este t√≥pico √© uma extens√£o natural dos conceitos de proje√ß√µes lineares para o contexto multivariado, que abre caminho para o estudo de modelos econom√©tricos mais avan√ßados.

### Refer√™ncias
[^4.1.21]:  *$P(Y_{t+1}|X_t) = \alpha'X_t = \hat{Y}_{t+1}$*.
[^4.1.22]: *$E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$*.
[^4.1.23]: *$\alpha' = [E(Y_{t+1}X_t')] \cdot [E(X_tX_t')]^{-1}$*.
[^4.1.24]: *$MSE(\alpha'X_t) = E{[Y_{t+1} - \alpha'X_t] \cdot [Y_{t+1} - \alpha'X_t]'} = E(Y_{t+1}Y_{t+1}') - [E(Y_{t+1}X_t')] \cdot [E(X_tX_t')]^{-1} \cdot [E(X_tY_{t+1}')]$*.
<!-- END -->
