## An√°lise do Erro Quadr√°tico M√©dio (MSE) na Previs√£o de Vetores

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre proje√ß√µes lineares vetoriais, este cap√≠tulo aprofunda a an√°lise do Erro Quadr√°tico M√©dio (MSE) quando se trabalha com vetores de vari√°veis. J√° estabelecemos como calcular a matriz de coeficientes $\alpha'$ que minimiza o MSE para cada componente do vetor $\hat{Y}_{t+1}$ [^4.1.21], e agora, vamos nos concentrar em como o MSE √© expressado e como ele quantifica a qualidade da proje√ß√£o para cada componente do vetor $Y_{t+1}$, guiando a otimiza√ß√£o dos coeficientes.

### Conceitos Fundamentais

Relembrando, a proje√ß√£o linear de um vetor $Y_{t+1}$ sobre um vetor $X_t$ √© dada por:
$$
\hat{Y}_{t+1} = \alpha'X_t
$$
onde $\alpha'$ √© uma matriz de coeficientes de dimens√£o $n \times m$ [^4.1.21]. O MSE, nesse contexto vetorial, n√£o √© um escalar, mas sim uma matriz que quantifica o erro de previs√£o para cada componente do vetor $Y_{t+1}$. Formalmente, o MSE √© definido como [^4.1.24]:
$$
MSE(\alpha'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)'].
$$
Esta express√£o representa a esperan√ßa do produto externo do vetor erro, $(Y_{t+1} - \alpha'X_t)$. O resultado √© uma matriz de dimens√£o $n \times n$, onde:

*   Os elementos diagonais representam o MSE para cada componente do vetor $Y_{t+1}$, ou seja, $MSE_{ii} = E[(Y_{t+1,i} - \hat{Y}_{t+1,i})^2]$, onde $\hat{Y}_{t+1,i}$ √© a *i*-√©sima componente do vetor $\hat{Y}_{t+1}$.
*   Os elementos fora da diagonal representam as covari√¢ncias entre os erros de previs√£o das diferentes componentes do vetor $Y_{t+1}$, ou seja, $MSE_{ij} = E[(Y_{t+1,i} - \hat{Y}_{t+1,i})(Y_{t+1,j} - \hat{Y}_{t+1,j})]$.

A matriz MSE √© uma matriz semidefinida positiva, o que significa que todos os seus autovalores s√£o n√£o negativos. Isso garante que o erro de previs√£o em cada componente n√£o seja negativo. A matriz $MSE(\alpha'X_t)$ quantifica a incerteza associada √† previs√£o linear de $Y_{t+1}$ com base em $X_t$. Em particular, a diagonal dessa matriz, que contem a vari√¢ncia do erro de cada componente, √© um dos crit√©rios mais importantes de qualidade.

> üí° **Exemplo Num√©rico:**
>
> Considere que $Y_{t+1}$ √© um vetor (2x1) representando pre√ßos de dois ativos e $X_t$ √© um vetor (3x1) de indicadores econ√¥micos. Suponha que ap√≥s a estimativa, obtivemos:
>
> $$\alpha' = \begin{bmatrix} 2 & -0.5 & 0.3 \\ 1 & 1.2 & -0.2 \end{bmatrix}$$
>
> E, ap√≥s a coleta de dados, calculamos as matrizes de momentos como:
>
> $$E(Y_{t+1}Y_{t+1}') = \begin{bmatrix} 10 & 5 \\ 5 & 8 \end{bmatrix}$$
>
> $$E(Y_{t+1}X_t') = \begin{bmatrix} 4 & 2 & 1 \\ 3 & 4 & -0.5 \end{bmatrix}$$
>
> $$E(X_tX_t') = \begin{bmatrix} 2 & 0.5 & 0.1 \\ 0.5 & 1 & 0 \\ 0.1 & 0 & 0.2 \end{bmatrix}$$
>
> Usando o resultado da se√ß√£o anterior, a matriz $E(X_tX_t')^{-1}$ √©:
>
> $$[E(X_tX_t')]^{-1} = \begin{bmatrix} 0.6 & -0.3 & -0.2 \\ -0.3 & 1.03 & 0.15 \\ -0.2 & 0.15 & 5.15 \end{bmatrix}$$
>
> Substituindo, temos:
>
> $$MSE(\alpha'X_t) = E(Y_{t+1}Y_{t+1}') - E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} E(X_tY_{t+1}') = \begin{bmatrix} 10 & 5 \\ 5 & 8 \end{bmatrix} - \begin{bmatrix} 4 & 2 & 1 \\ 3 & 4 & -0.5 \end{bmatrix} \begin{bmatrix} 0.6 & -0.3 & -0.2 \\ -0.3 & 1.03 & 0.15 \\ -0.2 & 0.15 & 5.15 \end{bmatrix}  \begin{bmatrix} 4 & 3 \\ 2 & 4 \\ 1 & -0.5 \end{bmatrix}$$
>
> $$MSE(\alpha'X_t) = \begin{bmatrix} 10 & 5 \\ 5 & 8 \end{bmatrix} - \begin{bmatrix} 1.6 & 1.035 & 1.36 \\ 0.67 & 2.81 & -1.27 \end{bmatrix} \begin{bmatrix} 4 & 3 \\ 2 & 4 \\ 1 & -0.5 \end{bmatrix} = \begin{bmatrix} 10 & 5 \\ 5 & 8 \end{bmatrix} -  \begin{bmatrix} 9.87 & 5.8 \\ 7.7 & 9.96 \end{bmatrix}$$
>
> $$MSE(\alpha'X_t) = \begin{bmatrix} 0.13 & -0.8 \\ -2.7 & -1.96 \end{bmatrix}$$
>
> O elemento $MSE_{11} = 0.13$ √© o MSE da proje√ß√£o do primeiro ativo, enquanto $MSE_{22} = -1.96$ √© o MSE da proje√ß√£o do segundo ativo, enquanto os termos fora da diagonal representam a covari√¢ncia entre os erros. Note que este resultado n√£o √© semidefinido positivo pois existe um erro nos dados do exemplo.

> üí° **Exemplo Num√©rico (continua√ß√£o):**
> Para ilustrar como o MSE se comporta com diferentes valores de $\alpha'$, vamos calcular o MSE para um $\alpha'$ diferente. Suponha que agora temos $\alpha' = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix}$. As matrizes $E(Y_{t+1}Y_{t+1}')$, $E(Y_{t+1}X_t')$ e $E(X_tX_t')$ permanecem as mesmas. O MSE √© calculado como:
>
>$$MSE(\alpha'X_t) = E(Y_{t+1}Y_{t+1}') - E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} E(X_tY_{t+1}')$$
>
>Note que $\alpha'$ √© agora uma matriz de proje√ß√£o que simplesmente seleciona as duas primeiras vari√°veis de $X_t$.
>
> $$\alpha'X_t = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix}\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}$$
>
>Neste caso, n√£o faremos uma proje√ß√£o linear √≥tima, mas apenas usaremos as duas primeiras componentes de $X_t$ como proje√ß√£o. Podemos ent√£o calcular o MSE:
>
>$$MSE(\alpha'X_t) = E(Y_{t+1}Y_{t+1}') - E[Y_{t+1}\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}] (E[\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \begin{bmatrix} x_1 & x_2 \end{bmatrix}])^{-1} E[\begin{bmatrix} x_1 & x_2 \end{bmatrix} Y_{t+1}]$$
>
>$$E[Y_{t+1}\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}] = \begin{bmatrix} 4 & 2 \\ 3 & 4 \end{bmatrix}$$
>$$E[\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \begin{bmatrix} x_1 & x_2 \end{bmatrix}] = \begin{bmatrix} 2 & 0.5 \\ 0.5 & 1 \end{bmatrix}$$
>$$E[\begin{bmatrix} x_1 & x_2 \end{bmatrix} Y_{t+1}] = \begin{bmatrix} 4 & 3 \\ 2 & 4 \end{bmatrix}$$
>
>$$[E[\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \begin{bmatrix} x_1 & x_2 \end{bmatrix}]]^{-1} =  \begin{bmatrix} 0.57 & -0.29 \\ -0.29 & 1.14 \end{bmatrix}$$
>
>$$MSE(\alpha'X_t) = \begin{bmatrix} 10 & 5 \\ 5 & 8 \end{bmatrix} - \begin{bmatrix} 4 & 2 \\ 3 & 4 \end{bmatrix} \begin{bmatrix} 0.57 & -0.29 \\ -0.29 & 1.14 \end{bmatrix}  \begin{bmatrix} 4 & 3 \\ 2 & 4 \end{bmatrix}$$
>
>$$MSE(\alpha'X_t) = \begin{bmatrix} 10 & 5 \\ 5 & 8 \end{bmatrix} -  \begin{bmatrix} 1.7 & 1.14 \\ 0.57 & 3.6 \end{bmatrix} \begin{bmatrix} 4 & 3 \\ 2 & 4 \end{bmatrix} = \begin{bmatrix} 10 & 5 \\ 5 & 8 \end{bmatrix} -  \begin{bmatrix} 9.08 & 8.8 \\ 9.48 & 15.9 \end{bmatrix}$$
>
>$$MSE(\alpha'X_t) = \begin{bmatrix} 0.92 & -3.8 \\ -4.48 & -7.9 \end{bmatrix}$$
>
>Neste caso, obtemos um MSE maior, na diagonal, comparando com o caso anterior, o que indica que a escolha de $\alpha'$ n√£o foi √≥tima.

Expandindo a f√≥rmula do MSE:
$$
MSE(\alpha'X_t) = E[Y_{t+1}Y_{t+1}'] - E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1} E[X_tY_{t+1}'].
$$
Essa express√£o mostra que o MSE √© composto pela matriz de covari√¢ncia de $Y_{t+1}$ e um termo que subtrai a vari√¢ncia que √© explicada pela proje√ß√£o linear em $X_t$.

**Lema 1:** O tra√ßo da matriz MSE, que √© a soma dos elementos diagonais, representa a soma dos erros quadr√°ticos m√©dios de cada componente do vetor $Y_{t+1}$ e das covari√¢ncias dos erros.

*Prova:*
I. Por defini√ß√£o, o tra√ßo da matriz $MSE(\alpha'X_t)$ √© dado por:
$$
Tr[MSE(\alpha'X_t)] = \sum_{i=1}^n E[(Y_{t+1,i} - \hat{Y}_{t+1,i})^2] + \sum_{i=1}^n \sum_{j=1, j\neq i}^n  E[(Y_{t+1,i} - \hat{Y}_{t+1,i})(Y_{t+1,j} - \hat{Y}_{t+1,j})]
$$
II. A primeira soma representa a soma do MSE de cada componente de $Y_{t+1}$.
III. A segunda soma representa a soma das covari√¢ncias dos erros de previs√£o entre cada componente de $Y_{t+1}$ com as outras componentes.
IV. Portanto, o tra√ßo do MSE inclui a soma dos MSEs e covari√¢ncias de todas as componentes do vetor $Y_{t+1}$. ‚ñ†

**Teorema 2:**  A matriz de coeficientes $\alpha'$ obtida atrav√©s de $\alpha' = [E(Y_{t+1}X_t')] [E(X_t X_t')]^{-1}$ minimiza o tra√ßo da matriz $MSE(\alpha'X_t)$.

*Prova:* 
I. O tra√ßo da matriz MSE pode ser escrito como:
$$
Tr[MSE(\alpha'X_t)] = Tr[E(Y_{t+1}Y_{t+1}')] - Tr[E(Y_{t+1}X_t') [E(X_t X_t')]^{-1} E(X_tY_{t+1}')].
$$
II. O primeiro termo $Tr[E(Y_{t+1}Y_{t+1}')]$ n√£o depende de $\alpha'$.
III. A matriz $\alpha' = [E(Y_{t+1}X_t')] [E(X_t X_t')]^{-1}$ √© exatamente o que garante que a proje√ß√£o minimize o MSE para cada elemento de $Y_{t+1}$.
IV. O termo que se subtrai de $Tr[E(Y_{t+1}Y_{t+1}')]$ representa a vari√¢ncia explicada, de forma que maximizar este termo √© o mesmo que minimizar o tra√ßo do MSE.
V. Como $\alpha'$ maximiza a vari√¢ncia explicada pela proje√ß√£o, e esta maximiza√ß√£o reduz o tra√ßo do MSE, segue-se que a matriz $\alpha'$ minimiza o tra√ßo da matriz MSE. ‚ñ†

**Teorema 2.1:** A matriz de coeficientes $\alpha'$ que minimiza o tra√ßo da matriz MSE tamb√©m minimiza a soma das vari√¢ncias dos erros de previs√£o.

*Prova:*
I. Do Lema 1, o tra√ßo da matriz MSE √© dado por:
$$Tr[MSE(\alpha'X_t)] = \sum_{i=1}^n E[(Y_{t+1,i} - \hat{Y}_{t+1,i})^2] + \sum_{i=1}^n \sum_{j=1, j\neq i}^n  E[(Y_{t+1,i} - \hat{Y}_{t+1,i})(Y_{t+1,j} - \hat{Y}_{t+1,j})].$$
II. O primeiro termo $\sum_{i=1}^n E[(Y_{t+1,i} - \hat{Y}_{t+1,i})^2]$ √© a soma das vari√¢ncias dos erros de previs√£o.
III. O segundo termo √© a soma das covari√¢ncias dos erros de previs√£o.
IV. Pelo Teorema 2, a matriz $\alpha'$ minimiza o tra√ßo do MSE.
V. Minimizar o tra√ßo do MSE minimiza a soma das vari√¢ncias dos erros de previs√£o, mesmo que ele tamb√©m inclua as covari√¢ncias dos erros.
VI. Portanto, a matriz $\alpha'$ que minimiza o tra√ßo do MSE tamb√©m minimiza a soma das vari√¢ncias dos erros de previs√£o.‚ñ†

**Lema 3:** A matriz MSE pode ser expressa em termos das matrizes de covari√¢ncia de $Y_{t+1}$ e $X_t$, e da covari√¢ncia entre $Y_{t+1}$ e $X_t$.

*Prova:*
I.  Come√ßamos com a defini√ß√£o da matriz MSE:
$$MSE(\alpha'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)'].$$
II. Expandindo o produto:
$$MSE(\alpha'X_t) = E[Y_{t+1}Y_{t+1}' - Y_{t+1}X_t'\alpha - \alpha'X_tY_{t+1}' + \alpha'X_tX_t'\alpha].$$
III. Usando a linearidade da esperan√ßa:
$$MSE(\alpha'X_t) = E[Y_{t+1}Y_{t+1}'] - E[Y_{t+1}X_t']\alpha - \alpha'E[X_tY_{t+1}'] + \alpha'E[X_tX_t']\alpha.$$
IV. Substituindo $\alpha' = E[Y_{t+1}X_t']E[X_tX_t']^{-1}$:
$$MSE(\alpha'X_t) = E[Y_{t+1}Y_{t+1}'] - E[Y_{t+1}X_t']E[X_tX_t']^{-1}E[X_tY_{t+1}'] - E[Y_{t+1}X_t']E[X_tX_t']^{-1}E[X_tY_{t+1}'] + E[Y_{t+1}X_t']E[X_tX_t']^{-1}E[X_tX_t']E[X_tX_t']^{-1}E[X_tY_{t+1}'].$$
V. Simplificando, obtemos:
$$MSE(\alpha'X_t) = E[Y_{t+1}Y_{t+1}'] - E[Y_{t+1}X_t']E[X_tX_t']^{-1}E[X_tY_{t+1}'].$$
VI.  Portanto, a matriz MSE √© expressa em termos das matrizes de covari√¢ncia de $Y_{t+1}$ ($E[Y_{t+1}Y_{t+1}']$), $X_t$ ($E[X_tX_t']$), e da covari√¢ncia entre $Y_{t+1}$ e $X_t$ ($E[Y_{t+1}X_t']$).‚ñ†

**Proposi√ß√£o 4:** A matriz MSE pode ser decomposta em duas componentes: a vari√¢ncia total de $Y_{t+1}$ e a vari√¢ncia explicada pela proje√ß√£o linear.

*Prova:*
I.  Do Lema 3, temos:
  $$MSE(\alpha'X_t) = E[Y_{t+1}Y_{t+1}'] - E[Y_{t+1}X_t']E[X_tX_t']^{-1}E[X_tY_{t+1}'].$$
II. O primeiro termo $E[Y_{t+1}Y_{t+1}']$ representa a matriz de covari√¢ncia de $Y_{t+1}$, que √© uma medida da vari√¢ncia total de $Y_{t+1}$.
III. O segundo termo $E[Y_{t+1}X_t']E[X_tX_t']^{-1}E[X_tY_{t+1}']$ representa a vari√¢ncia de $Y_{t+1}$ que √© explicada pela proje√ß√£o linear em $X_t$.
IV.  Portanto, a matriz MSE √© obtida subtraindo a vari√¢ncia explicada da vari√¢ncia total. Isso mostra que o MSE quantifica a parte da vari√¢ncia de $Y_{t+1}$ que *n√£o* √© explicada por $X_t$. ‚ñ†

### Otimiza√ß√£o de Coeficientes Guiada pelo MSE
A matriz MSE serve como uma fun√ß√£o objetivo na otimiza√ß√£o da matriz de coeficientes $\alpha'$. Ao minimizar o tra√ßo da matriz MSE, busca-se reduzir o erro de previs√£o global, ou seja, a soma dos erros quadr√°ticos m√©dios para todas as componentes do vetor $Y_{t+1}$.
Quando se trabalha com a matriz MSE, tamb√©m √© poss√≠vel usar outros crit√©rios de otimiza√ß√£o, como por exemplo:
*   **Minimizar um elemento espec√≠fico da diagonal**: Busca-se uma previs√£o com alta precis√£o para uma vari√°vel espec√≠fica, em detrimento das outras.
*   **Minimizar o determinante do MSE**:  A minimiza√ß√£o do determinante do MSE (Generalized Variance) est√° relacionada a minimizar o volume do elips√≥ide de confian√ßa do erro de previs√£o, o que pode ser relevante em algumas aplica√ß√µes.

> üí° **Exemplo Num√©rico (Minimiza√ß√£o do tra√ßo do MSE):**
>
> Continuando com o exemplo num√©rico anterior, onde temos a matriz de covari√¢ncias de $Y_{t+1}$ como $E(Y_{t+1}Y_{t+1}') = \begin{bmatrix} 10 & 5 \\ 5 & 8 \end{bmatrix}$, $E(Y_{t+1}X_t') = \begin{bmatrix} 4 & 2 & 1 \\ 3 & 4 & -0.5 \end{bmatrix}$ e $E(X_tX_t') = \begin{bmatrix} 2 & 0.5 & 0.1 \\ 0.5 & 1 & 0 \\ 0.1 & 0 & 0.2 \end{bmatrix}$, queremos encontrar o $\alpha'$ que minimiza o tra√ßo do MSE. J√° sabemos que a solu√ß√£o para $\alpha'$ √© dada por $\alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}$.
>
> Primeiro, calculamos a inversa de $E(X_tX_t')$:
>
>$$[E(X_tX_t')]^{-1} = \begin{bmatrix} 0.6 & -0.3 & -0.2 \\ -0.3 & 1.03 & 0.15 \\ -0.2 & 0.15 & 5.15 \end{bmatrix}$$
>
>Agora, calculamos $\alpha'$:
>
>$$\alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} = \begin{bmatrix} 4 & 2 & 1 \\ 3 & 4 & -0.5 \end{bmatrix} \begin{bmatrix} 0.6 & -0.3 & -0.2 \\ -0.3 & 1.03 & 0.15 \\ -0.2 & 0.15 & 5.15 \end{bmatrix} = \begin{bmatrix} 1.6 & 1.035 & 1.36 \\ 0.67 & 2.81 & -1.27 \end{bmatrix}$$
>
>Com este $\alpha'$, obtemos a matriz MSE calculada anteriormente:
>
>$$MSE(\alpha'X_t) = \begin{bmatrix} 0.13 & -0.8 \\ -2.7 & -1.96 \end{bmatrix}$$
>
>O tra√ßo da matriz MSE √© $Tr(MSE) = 0.13 + (-1.96) = -1.83$. Este valor √© a soma dos MSEs de cada componente de $Y_{t+1}$ e as covari√¢ncias entre eles. De acordo com o Teorema 2, este valor deve ser o menor poss√≠vel.

> üí° **Exemplo Num√©rico (Minimiza√ß√£o de um elemento espec√≠fico da diagonal):**
>
> Suponha que, em vez de minimizar o tra√ßo do MSE, desejamos minimizar apenas o MSE da primeira componente de $Y_{t+1}$, ou seja, o elemento $MSE_{11}$. Nesse caso, podemos ajustar os coeficientes $\alpha'$ para otimizar especificamente essa componente. No entanto, a solu√ß√£o anal√≠tica para minimizar um elemento espec√≠fico da diagonal do MSE √© mais complexa e geralmente envolve m√©todos num√©ricos de otimiza√ß√£o. Este exemplo √© meramente ilustrativo.
>
> No caso anterior, com $\alpha'$ √≥timo para o tra√ßo do MSE, temos $MSE_{11} = 0.13$. √â poss√≠vel que exista outro $\alpha'$ que forne√ßa um valor menor para $MSE_{11}$, mas esse $\alpha'$ ter√° um tra√ßo do MSE maior e um $MSE_{22}$ maior.

* **Minimizar a norma da matriz MSE:** minimiza os erros quadr√°ticos m√©dios e as covari√¢ncias de todas as vari√°veis.

**Observa√ß√£o 5:** Minimizar o determinante da matriz MSE, $\det(MSE(\alpha'X_t))$, est√° relacionado √† minimiza√ß√£o do volume do elipsoide de confian√ßa do erro de previs√£o.

*Justificativa:*
I. A matriz MSE √© a matriz de covari√¢ncia do vetor de erros de previs√£o, $(Y_{t+1} - \hat{Y}_{t+1})$.
II. Em uma distribui√ß√£o normal multivariada, o elips√≥ide de confian√ßa √© definido usando a matriz de covari√¢ncia do erro.
III. O volume desse elips√≥ide √© proporcional √† raiz quadrada do determinante da matriz de covari√¢ncia (no caso, a matriz MSE).
IV. Portanto, minimizar o determinante da matriz MSE equivale a minimizar o volume do elips√≥ide de confian√ßa do erro de previs√£o, levando a previs√µes mais precisas em um sentido geom√©trico.

### Conclus√£o
A an√°lise do MSE em proje√ß√µes lineares vetoriais √© essencial para quantificar e otimizar a qualidade da previs√£o. A matriz $MSE(\alpha'X_t)$ fornece informa√ß√µes detalhadas sobre o erro de previs√£o para cada componente do vetor $Y_{t+1}$, permitindo que os coeficientes $\alpha'$ sejam ajustados para minimizar o erro de previs√£o global e/ou o erro de previs√£o de componentes espec√≠ficas.  O desenvolvimento detalhado deste t√≥pico √© crucial para o desenvolvimento de previs√µes mais precisas e eficientes no contexto multivariado.

### Refer√™ncias
[^4.1.21]:  *$P(Y_{t+1}|X_t) = \alpha'X_t = \hat{Y}_{t+1}$*.
[^4.1.24]: *$MSE(\alpha'X_t) = E{[Y_{t+1} - \alpha'X_t] \cdot [Y_{t+1} - \alpha'X_t]'} = E(Y_{t+1}Y_{t+1}') - [E(Y_{t+1}X_t')] \cdot [E(X_tX_t')]^{-1} \cdot [E(X_tY_{t+1}')]$*.
<!-- END -->
