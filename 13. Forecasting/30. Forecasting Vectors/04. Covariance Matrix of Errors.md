## A Matriz de Covari√¢ncia dos Erros de Previs√£o em Proje√ß√µes Lineares Vetoriais

### Introdu√ß√£o
Este cap√≠tulo se aprofunda na an√°lise da *matriz de covari√¢ncia dos erros de previs√£o* em proje√ß√µes lineares vetoriais. Como vimos anteriormente, a proje√ß√£o linear de um vetor $Y_{t+1}$ sobre um vetor $X_t$ resulta em um vetor de previs√£o $\hat{Y}_{t+1}$. Este cap√≠tulo foca em como a variabilidade dos res√≠duos √© quantificada pela matriz de covari√¢ncia dos erros, $E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)']$, e como essa matriz desempenha um papel crucial na an√°lise da qualidade da proje√ß√£o e dos limites de previsibilidade. O objetivo √© entender como esta matriz se relaciona com o MSE e como ela fornece informa√ß√µes detalhadas sobre a dispers√£o e correla√ß√£o dos erros de previs√£o.

### Conceitos Fundamentais
Na proje√ß√£o linear vetorial, buscamos prever um vetor $Y_{t+1}$ (de dimens√£o $n \times 1$) utilizando um vetor $X_t$ (de dimens√£o $m \times 1$), atrav√©s da rela√ß√£o:
$$
\hat{Y}_{t+1} = \alpha' X_t,
$$
onde $\alpha'$ √© a matriz de coeficientes de dimens√£o $n \times m$ que minimiza o erro quadr√°tico m√©dio (MSE) para cada componente do vetor $Y_{t+1}$ [^4.1.21].

O erro de previs√£o √© dado pelo vetor:
$$
e_{t+1} = Y_{t+1} - \hat{Y}_{t+1} = Y_{t+1} - \alpha'X_t
$$
A *matriz de covari√¢ncia dos erros de previs√£o*, denotada por $\Sigma_e$, √© definida como:
$$
\Sigma_e = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)'] = MSE(\alpha'X_t)
$$
Esta matriz √© de dimens√£o $n \times n$ e possui as seguintes propriedades:

*   Os elementos diagonais, $\Sigma_{e_{ii}}$, representam a vari√¢ncia do erro de previs√£o para a *i*-√©sima componente de $Y_{t+1}$:
$$
\Sigma_{e_{ii}} = E[(Y_{t+1,i} - \hat{Y}_{t+1,i})^2]
$$
*   Os elementos fora da diagonal, $\Sigma_{e_{ij}}$, representam a covari√¢ncia entre os erros de previs√£o das componentes *i* e *j* de $Y_{t+1}$:
$$
\Sigma_{e_{ij}} = E[(Y_{t+1,i} - \hat{Y}_{t+1,i})(Y_{t+1,j} - \hat{Y}_{t+1,j})]
$$

A matriz $\Sigma_e$ quantifica a variabilidade dos erros de previs√£o, revelando n√£o apenas a dispers√£o de cada componente do erro, mas tamb√©m as rela√ß√µes de depend√™ncia linear entre os erros de diferentes componentes de $Y_{t+1}$. Uma matriz de covari√¢ncia dos erros com elementos diagonais pequenos indica que a proje√ß√£o linear √© precisa para cada componente de $Y_{t+1}$, enquanto elementos fora da diagonal pr√≥ximos de zero indicam que os erros de previs√£o para componentes diferentes n√£o s√£o correlacionados.

> üí° **Exemplo Num√©rico:**
>
> Considere novamente um exemplo onde $Y_{t+1}$ (2x1) representa pre√ßos de dois ativos e $X_t$ (3x1) representa indicadores econ√¥micos. Suponha que, ap√≥s a proje√ß√£o linear, obtivemos a matriz de coeficientes $\alpha'$ e as matrizes de momentos de tal forma que a matriz de covari√¢ncia dos erros de previs√£o  $E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)']$ √© dada por:
>
> $$
> \Sigma_e = \begin{bmatrix} 0.5 & 0.2 \\ 0.2 & 0.8 \end{bmatrix}
> $$
>
> Nesse caso:
>
> *   A vari√¢ncia do erro de previs√£o para o primeiro ativo √© 0.5.
> *   A vari√¢ncia do erro de previs√£o para o segundo ativo √© 0.8.
> *   A covari√¢ncia entre os erros de previs√£o dos dois ativos √© 0.2.
>
> A vari√¢ncia dos erros de previs√£o para cada componente do vetor $Y_{t+1}$ √© dada pelos elementos da diagonal de $\Sigma_e$, e a covari√¢ncia entre os erros de previs√£o √© dada pelos elementos fora da diagonal.
>
> Intuitivamente, isso significa que ao tentar prever o pre√ßo do primeiro ativo, nosso modelo tem um erro com vari√¢ncia de 0.5. O mesmo para o segundo ativo, com uma vari√¢ncia de 0.8. A covari√¢ncia de 0.2 indica que os erros de previs√£o entre os dois ativos s√£o ligeiramente correlacionados. Um erro positivo ao prever o pre√ßo do primeiro ativo, tende a estar associado a um erro tamb√©m positivo ao prever o pre√ßo do segundo ativo.

**Lema 1:**  A matriz de covari√¢ncia dos erros de previs√£o $\Sigma_e$ √© semidefinida positiva.

*Prova:*
I.  Para provar que $\Sigma_e$ √© semidefinida positiva, devemos mostrar que para qualquer vetor $z$ de dimens√£o $n \times 1$, temos $z'\Sigma_e z \ge 0$.
II. Usamos a defini√ß√£o da matriz $\Sigma_e$:
    $$z'\Sigma_e z = z'E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)']z.$$
III. Pela propriedade de linearidade da esperan√ßa, temos:
    $$z'\Sigma_e z = E[z'(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)'z].$$
IV. Reagrupando os termos:
    $$z'\Sigma_e z = E[(z'(Y_{t+1} - \alpha'X_t))((Y_{t+1} - \alpha'X_t)'z)].$$
V.  Definimos o vetor $v' = (Y_{t+1} - \alpha'X_t)'z$, de forma que:
    $$z'\Sigma_e z = E[v'v] = E[v^2].$$
VI.  Como $v^2$ √© sempre n√£o negativo, a esperan√ßa de um valor n√£o negativo tamb√©m √© n√£o negativa:
   $$ E[v^2] \ge 0$$
VII.  Portanto, $z'\Sigma_e z \ge 0$ para qualquer vetor $z$, e a matriz $\Sigma_e$ √© semidefinida positiva. ‚ñ†

**Lema 1.1:** A matriz de covari√¢ncia dos erros de previs√£o $\Sigma_e$ √© definida positiva se e somente se nenhum componente do vetor $Y_{t+1} - \alpha'X_t$ √© identicamente zero com probabilidade 1.

*Prova:*
I.  J√° sabemos que $\Sigma_e$ √© semidefinida positiva. Para $\Sigma_e$ ser definida positiva, precisamos mostrar que $z'\Sigma_e z > 0$ para todo vetor $z \neq 0$.
II. Da prova do Lema 1, temos $z'\Sigma_e z = E[v^2]$, onde $v = (Y_{t+1} - \alpha'X_t)'z$.
III. Se algum componente de $(Y_{t+1} - \alpha'X_t)$ √© identicamente zero com probabilidade 1, digamos o i-√©simo componente, ent√£o podemos construir um vetor $z$ tal que apenas o i-√©simo componente de $z$ seja diferente de zero. Nesse caso, $v=0$ com probabilidade 1, e $E[v^2]=0$, o que implica que $\Sigma_e$ n√£o √© definida positiva.
IV. Reciprocamente, se nenhum componente de $(Y_{t+1} - \alpha'X_t)$ √© identicamente zero com probabilidade 1, ent√£o para qualquer vetor $z\neq 0$, temos que $(Y_{t+1} - \alpha'X_t)'z$ n√£o √© identicamente zero com probabilidade 1. Logo, $v^2>0$ com probabilidade positiva e, portanto, $E[v^2]>0$, o que implica que $\Sigma_e$ √© definida positiva. ‚ñ†

A matriz de covari√¢ncia dos erros de previs√£o $\Sigma_e$ est√° diretamente relacionada com a matriz MSE, como explicitado anteriormente:
$$
\Sigma_e = MSE(\alpha'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)'].
$$
Relembrando, a matriz MSE √© dada por [^4.1.24]:
$$
MSE(\alpha'X_t) = E[Y_{t+1}Y_{t+1}'] - E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1} E[X_tY_{t+1}'].
$$
Portanto, a matriz de covari√¢ncia dos erros de previs√£o pode ser calculada utilizando os momentos de $Y_{t+1}$ e $X_t$.

> üí° **Exemplo Num√©rico:**
>
> Utilizando as matrizes de momentos do exemplo anterior,  podemos calcular a matriz $\Sigma_e$ (ou MSE) atrav√©s da rela√ß√£o:
>
> $$
> MSE(\alpha'X_t) = E[Y_{t+1}Y_{t+1}'] - E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1} E[X_tY_{t+1}'].
> $$
>
> Usando as matrizes:
>
> $$E(Y_{t+1}Y_{t+1}') = \begin{bmatrix} 10 & 5 \\ 5 & 8 \end{bmatrix}$$
>
> $$E(Y_{t+1}X_t') = \begin{bmatrix} 4 & 2 & 1 \\ 3 & 4 & -0.5 \end{bmatrix}$$
>
> $$E(X_tX_t') = \begin{bmatrix} 2 & 0.5 & 0.1 \\ 0.5 & 1 & 0 \\ 0.1 & 0 & 0.2 \end{bmatrix}$$
>
> Calculando a inversa de $E(X_tX_t')$:
>
> ```python
> import numpy as np
>
> # Define a matriz E(X_tX_t')
> EXXt = np.array([[2, 0.5, 0.1],
>                 [0.5, 1, 0],
>                 [0.1, 0, 0.2]])
>
> # Calcula a inversa
> EXXt_inv = np.linalg.inv(EXXt)
> print(EXXt_inv)
> # Output:
#[[ 0.60273973 -0.30136986 -0.2739726 ]
# [-0.30136986  1.03424658  0.1369863 ]
# [-0.2739726   0.1369863   5.15753425]]
> ```
>
> $$[E(X_tX_t')]^{-1} = \begin{bmatrix} 0.6027 & -0.3014 & -0.2740 \\ -0.3014 & 1.0342 & 0.1370 \\ -0.2740 & 0.1370 & 5.1575 \end{bmatrix}$$
>
> Ent√£o, o MSE (ou a matriz $\Sigma_e$) √©:
>
> ```python
> # Define as matrizes
> EYt1Yt1 = np.array([[10, 5],
>                     [5, 8]])
> EYt1Xt = np.array([[4, 2, 1],
>                    [3, 4, -0.5]])
>
> # Calcula o MSE
> MSE = EYt1Yt1 - np.dot(np.dot(EYt1Xt, EXXt_inv), EYt1Xt.T)
> print(MSE)
> # Output:
# [[0.57945205 0.01369863]
# [0.01369863 3.0630137 ]]
> ```
>
> $$MSE(\alpha'X_t) = \begin{bmatrix} 10 & 5 \\ 5 & 8 \end{bmatrix} - \begin{bmatrix} 4 & 2 & 1 \\ 3 & 4 & -0.5 \end{bmatrix} \begin{bmatrix} 0.6027 & -0.3014 & -0.2740 \\ -0.3014 & 1.0342 & 0.1370 \\ -0.2740 & 0.1370 & 5.1575 \end{bmatrix} \begin{bmatrix} 4 & 3 \\ 2 & 4 \\ 1 & -0.5 \end{bmatrix} = \begin{bmatrix} 0.5795 & 0.0137 \\ 0.0137 & 3.0630 \end{bmatrix}$$
>
> Os elementos diagonais da matriz MSE representam os erros quadr√°ticos m√©dios das proje√ß√µes para cada componente de $Y_{t+1}$, enquanto os elementos fora da diagonal representam as covari√¢ncias dos erros de previs√£o.  Note que a matriz resultante √© sim√©trica, como esperado para uma matriz de covari√¢ncia.
>
> Neste caso, a vari√¢ncia do erro na previs√£o do primeiro ativo √© aproximadamente 0.58, e a vari√¢ncia do erro na previs√£o do segundo ativo √© aproximadamente 3.06. A covari√¢ncia entre os erros de previs√£o √© aproximadamente 0.0137, indicando que os erros s√£o pouco correlacionados.

**Teorema 1:** A matriz $\Sigma_e$ representa a vari√¢ncia e covari√¢ncia dos erros de previs√£o, quantificando a incerteza inerente √† proje√ß√£o linear vetorial.

*Prova:*
I. A matriz $\Sigma_e$ √© definida como $\Sigma_e = E[(Y_{t+1} - \hat{Y}_{t+1})(Y_{t+1} - \hat{Y}_{t+1})']$, onde $\hat{Y}_{t+1} = \alpha'X_t$
II. O elemento $(i,i)$ da matriz $\Sigma_e$ √© dado por $E[(Y_{t+1,i} - \hat{Y}_{t+1,i})^2]$, que √© a vari√¢ncia do erro de previs√£o para a i-√©sima componente do vetor $Y_{t+1}$.
III. O elemento $(i,j)$ da matriz $\Sigma_e$ √© dado por $E[(Y_{t+1,i} - \hat{Y}_{t+1,i})(Y_{t+1,j} - \hat{Y}_{t+1,j})]$, que √© a covari√¢ncia entre os erros de previs√£o das i-√©sima e j-√©sima componentes do vetor $Y_{t+1}$.
IV. Portanto, a matriz $\Sigma_e$ cont√©m as vari√¢ncias e covari√¢ncias dos erros de previs√£o de cada elemento do vetor $Y_{t+1}$, e, portanto, quantifica a incerteza inerente √† proje√ß√£o linear vetorial. ‚ñ†

### Rela√ß√£o com a Qualidade da Proje√ß√£o
A matriz $\Sigma_e$ √© crucial para avaliar a qualidade da proje√ß√£o linear. Uma matriz $\Sigma_e$ com elementos diagonais pequenos indica que as previs√µes para cada componente de $Y_{t+1}$ s√£o precisas, com pouca variabilidade no erro de previs√£o. Elementos fora da diagonal pequenos indicam que os erros de previs√£o para diferentes componentes de $Y_{t+1}$ n√£o s√£o fortemente correlacionados, o que seria desej√°vel, pois implica que os erros de uma componente n√£o s√£o um bom preditor dos erros de outra componente.

Em termos de an√°lise dos limites da previsibilidade, a matriz $\Sigma_e$ nos informa a variabilidade m√≠nima que pode ser esperada na proje√ß√£o linear, considerando a informa√ß√£o contida em $X_t$. Mesmo que a proje√ß√£o linear seja √≥tima, uma matriz $\Sigma_e$ com valores elevados na diagonal indica que a proje√ß√£o linear deixa uma grande parte da variabilidade de $Y_{t+1}$ inexplicada. Nesse caso, ou se adiciona mais vari√°veis em $X_t$, ou a proje√ß√£o linear n√£o √© um bom modelo para prever $Y_{t+1}$.

**Proposi√ß√£o 1:** Minimizar o tra√ßo da matriz $\Sigma_e$ equivale a minimizar a soma das vari√¢ncias dos erros de previs√£o de cada componente de $Y_{t+1}$.

*Prova:*
I.  O tra√ßo da matriz $\Sigma_e$ √© a soma dos elementos diagonais, isto √©, as vari√¢ncias dos erros de previs√£o de cada componente de $Y_{t+1}$:
$$Tr(\Sigma_e) = \sum_{i=1}^n E[(Y_{t+1,i} - \hat{Y}_{t+1,i})^2].$$
II.  Minimizar o tra√ßo da matriz $\Sigma_e$ equivale a minimizar a soma das vari√¢ncias dos erros de previs√£o para todas as componentes de $Y_{t+1}$. ‚ñ†

**Proposi√ß√£o 1.1:**  Se a matriz $\Sigma_e$ √© diagonal, ent√£o os erros de previs√£o de cada componente de $Y_{t+1}$ s√£o n√£o correlacionados.

*Prova:*
I. Se a matriz $\Sigma_e$ √© diagonal, todos os elementos fora da diagonal s√£o zero.
II. Os elementos fora da diagonal de $\Sigma_e$, $\Sigma_{e_{ij}}$ para $i \neq j$, representam a covari√¢ncia entre os erros de previs√£o das componentes $i$ e $j$ de $Y_{t+1}$.
III. Se $\Sigma_{e_{ij}} = 0$ para todo $i \neq j$, ent√£o os erros de previs√£o das diferentes componentes de $Y_{t+1}$ s√£o n√£o correlacionados. ‚ñ†

### Implica√ß√µes Pr√°ticas
A matriz de covari√¢ncia dos erros de previs√£o $\Sigma_e$ √© uma ferramenta essencial na an√°lise de proje√ß√µes lineares vetoriais, permitindo:

*   **Quantificar a precis√£o da previs√£o**:  As vari√¢ncias (elementos diagonais) na matriz $\Sigma_e$ indicam o grau de incerteza associado √† previs√£o de cada vari√°vel em $Y_{t+1}$. Vari√¢ncias menores implicam previs√µes mais precisas.
*   **Avaliar a depend√™ncia entre os erros**: As covari√¢ncias (elementos fora da diagonal) na matriz $\Sigma_e$ indicam se os erros de previs√£o de diferentes vari√°veis em $Y_{t+1}$ est√£o correlacionados. Uma forte correla√ß√£o pode indicar que ainda h√° informa√ß√µes nos erros que podem ser utilizadas para aprimorar a previs√£o.
*   **Comparar diferentes modelos de proje√ß√£o**:  Ao calcular a matriz $\Sigma_e$ para diferentes escolhas de $\alpha'$, √© poss√≠vel comparar a qualidade das proje√ß√µes, escolhendo o modelo com o menor tra√ßo do MSE (que equivale a minimizar a soma das vari√¢ncias).
*   **Analisar os limites de previsibilidade**:  A matriz $\Sigma_e$ permite verificar a variabilidade m√≠nima que pode ser esperada na proje√ß√£o, dados os dados usados para construir a proje√ß√£o linear.

> üí° **Exemplo Num√©rico:**
>
>  Para ilustrar a compara√ß√£o entre modelos de proje√ß√£o, considere dois modelos diferentes para prever $Y_{t+1}$ (pre√ßos de dois ativos), baseados em diferentes conjuntos de vari√°veis em $X_t$.
>
>   **Modelo 1:** Utiliza $X_{t1}$ com 3 indicadores econ√¥micos, resultando em uma matriz de covari√¢ncia dos erros $\Sigma_{e1}$:
>
>   $$\Sigma_{e1} = \begin{bmatrix} 0.6 & 0.2 \\ 0.2 & 0.9 \end{bmatrix}$$
>
>   **Modelo 2:** Utiliza $X_{t2}$ com 5 indicadores econ√¥micos, resultando em uma matriz de covari√¢ncia dos erros $\Sigma_{e2}$:
>
>   $$\Sigma_{e2} = \begin{bmatrix} 0.4 & 0.1 \\ 0.1 & 0.7 \end{bmatrix}$$
>
>   Calculando o tra√ßo de cada matriz:
>
>   $$Tr(\Sigma_{e1}) = 0.6 + 0.9 = 1.5$$
>
>   $$Tr(\Sigma_{e2}) = 0.4 + 0.7 = 1.1$$
>
>   Como $Tr(\Sigma_{e2}) < Tr(\Sigma_{e1})$, o Modelo 2, que utiliza mais indicadores econ√¥micos, apresenta menor soma das vari√¢ncias dos erros de previs√£o, indicando uma melhor qualidade na proje√ß√£o. Isso demonstra como a an√°lise do tra√ßo da matriz de covari√¢ncia dos erros pode ser usada para comparar e selecionar o melhor modelo.

**Teorema 2:** A matriz de coeficientes $\alpha'$, dada por $\alpha' = [E(Y_{t+1}X_t')] [E(X_t X_t')]^{-1}$, garante que o vetor erro $(Y_{t+1} - \hat{Y}_{t+1})$ seja ortogonal a qualquer combina√ß√£o linear das vari√°veis em $X_t$.
*Prova:* Seja $Z_t = c'X_t$ uma combina√ß√£o linear qualquer das vari√°veis em $X_t$. J√° vimos que $E[(Y_{t+1} - \alpha'X_t)X_t']=0$.
I.  Multiplicamos a equa√ß√£o da ortogonalidade √† direita por $c$:
    $E[(Y_{t+1} - \alpha'X_t)X_t']c = 0$
II. Note que $X_t'c = Z_t$:
    $E[(Y_{t+1} - \alpha'X_t)Z_t'] = 0$
III. Portanto, o erro √© ortogonal a qualquer combina√ß√£o linear de $X_t$.‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere um cen√°rio simplificado onde $Y_{t+1}$ √© o pre√ßo de um ativo e $X_t$ √© um √∫nico indicador econ√¥mico. Suponha que tenhamos os seguintes valores:
>
> $$E[Y_{t+1}X_t] = 5$$
> $$E[X_tX_t] = 2$$
>
> Usando a f√≥rmula para $\alpha'$, temos:
> $$\alpha' = E[Y_{t+1}X_t] [E[X_tX_t]]^{-1} = 5 \cdot 2^{-1} = 2.5$$
>
> Portanto, $\hat{Y}_{t+1} = 2.5 X_t$.
>
> Agora, vamos verificar a ortogonalidade do erro com $X_t$. Suponha que $E[Y_{t+1}X_t] = 5$ e $E[X_t^2] = 2$. Usamos a f√≥rmula de $\alpha'$: $\alpha = E[Y_{t+1}X_t] E[X_t^2]^{-1} = 5 \cdot (2)^{-1} = 2.5$.
>
> Assim, o erro √© $e_{t+1} = Y_{t+1} - 2.5X_t$.
>
> Para verificar a ortogonalidade, calculamos $E[e_{t+1}X_t] = E[(Y_{t+1} - 2.5X_t)X_t] = E[Y_{t+1}X_t] - 2.5E[X_t^2] = 5 - 2.5 \cdot 2 = 0$.
>
> Isso demonstra que o erro √© ortogonal a $X_t$, confirmando o Teorema 2.

**Teorema 2.1:**  Se a matriz $\Sigma_e$ √© diagonal, ent√£o os erros de previs√£o $e_{t+1,i}$ para cada componente i de $Y_{t+1}$ s√£o mutuamente n√£o correlacionados e n√£o podem ser usados para melhorar a previs√£o de outras componentes de $Y_{t+1}$ atrav√©s de uma proje√ß√£o linear.

*Prova:*
I. Do Teorema 1, sabemos que se a matriz $\Sigma_e$ √© diagonal, ent√£o as covari√¢ncias entre os erros de previs√£o de diferentes componentes s√£o todas zero.
II.  Pela Proposi√ß√£o 1.1, se a matriz $\Sigma_e$ √© diagonal, os erros de previs√£o de cada componente de $Y_{t+1}$ s√£o n√£o correlacionados.
III. Se os erros s√£o n√£o correlacionados, ent√£o a proje√ß√£o linear de um erro em outro √© zero, o que implica que os erros de previs√£o $e_{t+1,i}$ s√£o mutuamente n√£o correlacionados e n√£o podem ser usados para melhorar a previs√£o de outras componentes de $Y_{t+1}$ atrav√©s de uma proje√ß√£o linear. ‚ñ†

### Conclus√£o
A matriz de covari√¢ncia dos erros de previs√£o, $\Sigma_e$, √© uma ferramenta essencial para a an√°lise e otimiza√ß√£o de proje√ß√µes lineares vetoriais. Esta matriz n√£o s√≥ quantifica a variabilidade e correla√ß√£o dos erros de previs√£o, mas tamb√©m fornece informa√ß√µes cruciais sobre a qualidade da proje√ß√£o, ajudando a guiar a sele√ß√£o de modelos e a entender os limites de previsibilidade. A rela√ß√£o intr√≠nseca entre a matriz $\Sigma_e$ e o MSE ressalta a import√¢ncia do c√°lculo correto da matriz de coeficientes $\alpha'$ para a obten√ß√£o de previs√µes √≥timas no sentido do erro quadr√°tico m√©dio. Ao analisar as componentes da matriz $\Sigma_e$, √© poss√≠vel entender melhor as limita√ß√µes de um modelo de proje√ß√£o linear e buscar alternativas, como a adi√ß√£o de novas vari√°veis em $X_t$.

### Refer√™ncias
[^4.1.21]:  *$P(Y_{t+1}|X_t) = \alpha'X_t = \hat{Y}_{t+1}$*.
[^4.1.24]: *$MSE(\alpha'X_t) = E{[Y_{t+1} - \alpha'X_t] \cdot [Y_{t+1} - \alpha'X_t]'} = E(Y_{t+1}Y_{t+1}') - [E(Y_{t+1}X_t')] \cdot [E(X_tX_t')]^{-1} \cdot [E(X_tY_{t+1}')]$*.
<!-- END -->
