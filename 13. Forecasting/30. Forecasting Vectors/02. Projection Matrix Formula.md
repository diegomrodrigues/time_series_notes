## ProjeÃ§Ãµes Lineares Vetoriais e o CÃ¡lculo da Matriz de Coeficientes

### IntroduÃ§Ã£o
Como vimos anteriormente, a projeÃ§Ã£o linear Ã© uma ferramenta poderosa para construir previsÃµes, e agora vamos generalizar esse conceito para o contexto multivariado, explorando como calcular a matriz de coeficientes que minimiza o erro quadrÃ¡tico mÃ©dio (MSE) para cada elemento do vetor de variÃ¡veis dependentes. Em particular, focaremos na obtenÃ§Ã£o da matriz de projeÃ§Ã£o $\alpha'$ usando a fÃ³rmula $\alpha' = [E(Y_{t+1}X_t')] [E(X_tX_t')]^{-1}$, que envolve a inversÃ£o de matrizes. Essa tÃ©cnica Ã© fundamental para o processamento de dados multivariados, permitindo-nos lidar com situaÃ§Ãµes em que mÃºltiplas variÃ¡veis sÃ£o previstas simultaneamente.

### Conceitos Fundamentais
Relembrando, a projeÃ§Ã£o linear de um vetor $Y_{t+1}$ (de dimensÃ£o $n \times 1$) em um vetor $X_t$ (de dimensÃ£o $m \times 1$) Ã© expressa como [^4.1.21]:
$$
\hat{Y}_{t+1} = P(Y_{t+1} | X_t) = \alpha' X_t,
$$
onde $\alpha'$ Ã© uma matriz de coeficientes de dimensÃ£o $n \times m$. O objetivo Ã© encontrar a matriz $\alpha'$ que minimize o MSE para cada elemento do vetor $\hat{Y}_{t+1}$. Isso Ã© equivalente a garantir que o erro de previsÃ£o, $(Y_{t+1} - \hat{Y}_{t+1})$, seja nÃ£o correlacionado com cada elemento de $X_t$ [^4.1.22]:

$$
E[(Y_{t+1} - \alpha'X_t)X_t'] = 0.
$$
Esta condiÃ§Ã£o implica que cada elemento do vetor erro Ã© ortogonal a cada elemento do vetor $X_t$ (ProposiÃ§Ã£o 1 do tÃ³pico anterior).

A soluÃ§Ã£o para $\alpha'$, que minimiza o MSE, Ã© dada por [^4.1.23]:
$$
\alpha' = [E(Y_{t+1}X_t')] [E(X_t X_t')]^{-1}.
$$
Esta fÃ³rmula envolve a inversÃ£o da matriz $E(X_t X_t')$, que Ã© a matriz de covariÃ¢ncia de $X_t$, assumindo que $E(X_t) = 0$ (ou que um termo constante estÃ¡ incluÃ­do em $X_t$). A inversÃ£o dessa matriz Ã© crucial para obter a matriz de coeficientes $\alpha'$ que fornece a melhor projeÃ§Ã£o linear de $Y_{t+1}$ em $X_t$.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
>Vamos supor que temos dois vetores, $Y_{t+1}$ (2x1) e $X_t$ (3x1), representando, por exemplo, preÃ§os de dois ativos e trÃªs indicadores econÃ´micos, respectivamente.
>
> $$ Y_{t+1} = \begin{bmatrix} y_{1,t+1} \\ y_{2,t+1} \end{bmatrix} , \quad X_t = \begin{bmatrix} x_{1,t} \\ x_{2,t} \\ x_{3,t} \end{bmatrix} $$
>
> ApÃ³s coletar dados, estimamos as seguintes matrizes de momentos (usando mÃ©dias amostrais):
>
> $$ E(Y_{t+1}X_t') = \begin{bmatrix} 2 & 1 & 0.5 \\ 1 & 1.5 & 0.2 \end{bmatrix} $$
>
> $$ E(X_t X_t') = \begin{bmatrix} 1 & 0.5 & 0.2 \\ 0.5 & 1 & 0.1 \\ 0.2 & 0.1 & 0.5 \end{bmatrix} $$
>
> Para calcular $\alpha'$, primeiro precisamos inverter $E(X_t X_t')$. Usando o NumPy:
>
> ```python
> import numpy as np
>
> Exx = np.array([[1, 0.5, 0.2],
>                 [0.5, 1, 0.1],
>                 [0.2, 0.1, 0.5]])
>
> Eyx = np.array([[2, 1, 0.5],
>                 [1, 1.5, 0.2]])
>
> Exx_inv = np.linalg.inv(Exx)
>
> alpha_prime = np.dot(Eyx, Exx_inv)
>
> print("Alpha':")
> print(alpha_prime)
> ```
>
> Isso resulta em:
>
> $$\alpha' \approx \begin{bmatrix} 2.28 & -0.35 & 0.09 \\ 0.64 & 1.41 & -0.04 \end{bmatrix}$$
>
> Cada linha de $\alpha'$ representa os coeficientes para prever uma variÃ¡vel em $Y_{t+1}$. Por exemplo, a primeira linha (2.28, -0.35, 0.09) nos diz como usar $x_{1,t}$, $x_{2,t}$ e $x_{3,t}$ para prever $y_{1,t+1}$.

**Lema 1:** (RevisÃ£o) A matriz $E(X_t X_t')$ Ã© invertÃ­vel se e somente se nÃ£o existe nenhuma combinaÃ§Ã£o linear dos elementos de $X_t$ que seja igual a zero com probabilidade 1. (Lema 1 do tÃ³pico anterior)

A condiÃ§Ã£o de invertibilidade Ã© essencial, pois garante que o cÃ¡lculo de $\alpha'$ seja bem definido. Se $E(X_t X_t')$ nÃ£o for invertÃ­vel, significa que hÃ¡ redundÃ¢ncia nas variÃ¡veis de $X_t$, ou seja, algumas variÃ¡veis podem ser expressas como combinaÃ§Ãµes lineares das outras, e isso impede que se encontre uma soluÃ§Ã£o Ãºnica para a matriz $\alpha'$. Nestes casos, Ã© necessÃ¡rio remover as variÃ¡veis redundantes ou usar mÃ©todos de regularizaÃ§Ã£o que permitam obter soluÃ§Ãµes aproximadas.

**Lema 1.1:** Se a matriz $E(X_t X_t')$ nÃ£o for invertÃ­vel, entÃ£o existe um vetor nÃ£o nulo $v$ tal que $X_t'v = 0$ com probabilidade 1.
*Prova:* Se $E(X_t X_t')$ nÃ£o Ã© invertÃ­vel, entÃ£o, pelo Lema 1, existe uma combinaÃ§Ã£o linear dos elementos de $X_t$ que Ã© igual a zero com probabilidade 1. Seja $v$ o vetor de coeficientes desta combinaÃ§Ã£o linear. EntÃ£o $X_t'v=0$ com probabilidade 1, e $v \neq 0$.

A forma expandida do MSE desta previsÃ£o Ã³tima Ã© dada por [^4.1.24]:
$$
MSE(\alpha'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)'] = E[Y_{t+1}Y_{t+1}'] - E[Y_{t+1}X_t'] [E(X_t X_t')]^{-1} E[X_tY_{t+1}'].
$$
Esta fÃ³rmula generaliza o resultado para o caso escalar, e cada elemento diagonal da matriz $MSE(\alpha'X_t)$ representa o erro quadrÃ¡tico mÃ©dio para cada variÃ¡vel em $Y_{t+1}$, enquanto os elementos fora da diagonal representam as covariÃ¢ncias entre os erros de previsÃ£o de diferentes variÃ¡veis em $Y_{t+1}$.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Continuando o exemplo anterior, vamos calcular o MSE. Primeiro, vamos assumir que temos a seguinte matriz:
>
> $$E(Y_{t+1}Y_{t+1}') = \begin{bmatrix} 5 & 2 \\ 2 & 4 \end{bmatrix}$$
>
> Agora vamos calcular o MSE usando a fÃ³rmula:
>
> $$MSE(\alpha'X_t) = E[Y_{t+1}Y_{t+1}'] - E[Y_{t+1}X_t'] [E(X_t X_t')]^{-1} E[X_tY_{t+1}'].$$
>
> JÃ¡ calculamos $\alpha'$ como  $\begin{bmatrix} 2.28 & -0.35 & 0.09 \\ 0.64 & 1.41 & -0.04 \end{bmatrix}$ e temos $E(Y_{t+1}X_t')$ como $\begin{bmatrix} 2 & 1 & 0.5 \\ 1 & 1.5 & 0.2 \end{bmatrix}$. Note que $E[X_tY_{t+1}'] = (E[Y_{t+1}X_t'])'$.
>
> Usando Python:
>
> ```python
> Eyy = np.array([[5, 2],
>                [2, 4]])
>
> MSE = Eyy - np.dot(np.dot(Eyx, Exx_inv), Eyx.T)
> print("MSE:")
> print(MSE)
> ```
>
> O resultado Ã©:
>
> $$MSE \approx \begin{bmatrix} 0.71 & -0.38 \\ -0.38 & 1.26 \end{bmatrix}$$
>
> O elemento (1,1) do MSE, 0.71, Ã© o MSE da projeÃ§Ã£o da primeira variÃ¡vel de $Y_{t+1}$, e o elemento (2,2), 1.26, Ã© o MSE da segunda variÃ¡vel. Os elementos fora da diagonal sÃ£o as covariÃ¢ncias entre os erros de previsÃ£o das duas variÃ¡veis.

**Teorema 1.1:** (RevisÃ£o) A projeÃ§Ã£o linear $\hat{Y}_{t+1} = \alpha'X_t$, com $\alpha'$ dado por $\alpha' = [E(Y_{t+1}X_t')] [E(X_t X_t')]^{-1}$, Ã© a melhor previsÃ£o linear de $Y_{t+1}$ no sentido de minimizar o traÃ§o da matriz $MSE(\alpha'X_t)$. (Teorema 1.1 do tÃ³pico anterior)

O traÃ§o da matriz $MSE$ representa a soma das variÃ¢ncias de cada componente do vetor erro, e minimizar o traÃ§o significa minimizar a soma dos erros quadrÃ¡ticos mÃ©dios das projeÃ§Ãµes para cada variÃ¡vel em $Y_{t+1}$.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> No exemplo anterior, o traÃ§o do MSE Ã© $0.71 + 1.26 = 1.97$.  Este valor representa a soma dos erros quadrÃ¡ticos mÃ©dios da projeÃ§Ã£o. Se compararmos com outras projeÃ§Ãµes lineares, esta projeÃ§Ã£o com o $\alpha'$ calculado minimiza este valor.

**Teorema 1.2:** A projeÃ§Ã£o linear $\hat{Y}_{t+1} = \alpha'X_t$, com $\alpha'$ dado por $\alpha' = [E(Y_{t+1}X_t')] [E(X_t X_t')]^{-1}$, tambÃ©m garante que o erro de previsÃ£o $(Y_{t+1} - \hat{Y}_{t+1})$ seja ortogonal a qualquer combinaÃ§Ã£o linear das variÃ¡veis em $X_t$.
*Prova:* Seja $Z_t = c'X_t$ uma combinaÃ§Ã£o linear qualquer das variÃ¡veis em $X_t$. Devemos provar que $E[(Y_{t+1} - \alpha'X_t)Z_t']=0$.
I.  SubstituÃ­mos $Z_t$ por $c'X_t$:
   $$E[(Y_{t+1} - \alpha'X_t)Z_t'] = E[(Y_{t+1} - \alpha'X_t)(c'X_t)'].$$
II. Expandimos o termo $(c'X_t)'$:
  $$E[(Y_{t+1} - \alpha'X_t)(c'X_t)'] = E[(Y_{t+1} - \alpha'X_t)X_t'c].$$
III. Usamos a propriedade de linearidade do operador de esperanÃ§a:
 $$E[(Y_{t+1} - \alpha'X_t)X_t'c] = E[(Y_{t+1} - \alpha'X_t)X_t']c.$$
IV. Pela condiÃ§Ã£o de ortogonalidade $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$:
 $$E[(Y_{t+1} - \alpha'X_t)X_t']c = 0 \cdot c = 0.$$
V. Portanto, $E[(Y_{t+1} - \alpha'X_t)Z_t']=0$ â– 

### ImplicaÃ§Ãµes PrÃ¡ticas do CÃ¡lculo de Î±'
A fÃ³rmula para calcular $\alpha'$ $\alpha' = [E(Y_{t+1}X_t')] [E(X_t X_t')]^{-1}$, apesar de concisa, envolve algumas etapas cruciais:
 1. **CÃ¡lculo das Matrizes de Momentos**: Ã‰ necessÃ¡rio calcular $E(Y_{t+1}X_t')$ e $E(X_t X_t')$, tipicamente usando as mÃ©dias amostrais. Em aplicaÃ§Ãµes prÃ¡ticas, essas mÃ©dias sÃ£o estimadas usando dados histÃ³ricos.
 2. **InversÃ£o da Matriz de CovariÃ¢ncia**: A inversÃ£o de $E(X_t X_t')$ pode ser computacionalmente custosa, especialmente para matrizes de alta dimensÃ£o. AlÃ©m disso, a matriz deve ser nÃ£o singular para que a inversa exista. Em muitos casos prÃ¡ticos, sÃ£o usadas tÃ©cnicas de regularizaÃ§Ã£o para lidar com problemas de singularidade ou instabilidade numÃ©rica.
 3. **MultiplicaÃ§Ã£o de Matrizes**: A multiplicaÃ§Ã£o das matrizes $E(Y_{t+1}X_t')$ e $[E(X_t X_t')]^{-1}$ Ã© necessÃ¡ria para obter $\alpha'$.

Em cenÃ¡rios de sÃ©ries temporais, os momentos sÃ£o geralmente estimados usando dados histÃ³ricos. A qualidade da estimativa de $\alpha'$ depende diretamente da qualidade da estimativa dos momentos populacionais $E(Y_{t+1}X_t')$ e $E(X_t X_t')$. Portanto, Ã© essencial que a amostra utilizada para a estimaÃ§Ã£o seja representativa e suficientemente grande para que as mÃ©dias amostrais sejam boas aproximaÃ§Ãµes dos momentos populacionais.

**ProposiÃ§Ã£o 1:** Se as variÃ¡veis em $X_t$ sÃ£o linearmente independentes, entÃ£o $E(X_t X_t')$ Ã© definida positiva.
*Prova:* Se as variÃ¡veis em $X_t$ sÃ£o linearmente independentes, entÃ£o, pelo Lema 1, $E(X_t X_t')$ Ã© invertÃ­vel. Para mostrar que $E(X_t X_t')$ Ã© definida positiva, considere um vetor $v \neq 0$.
I.  Definimos a forma quadrÃ¡tica:
    $$ v'E(X_t X_t')v $$
II.  Utilizando a propriedade de esperanÃ§a:
    $$ v'E(X_t X_t')v = E[v'X_t X_t'v].$$
III.  Reagrupando os termos:
     $$E[v'X_t X_t'v] = E[(X_t'v)^2].$$
IV. Como as variÃ¡veis em $X_t$ sÃ£o linearmente independentes, $X_t'v \neq 0$ com probabilidade 1 para qualquer $v\neq 0$.
V. Assim,  $E[(X_t'v)^2] > 0$, para todo vetor $v \neq 0$.
VI. Portanto, $E(X_t X_t')$ Ã© definida positiva. â– 

### ConclusÃ£o
O cÃ¡lculo da matriz de projeÃ§Ã£o $\alpha'$ usando a fÃ³rmula $\alpha' = [E(Y_{t+1}X_t')] [E(X_t X_t')]^{-1}$ Ã© um passo essencial na construÃ§Ã£o de previsÃµes lineares Ã³timas no contexto multivariado. A inversÃ£o da matriz $E(X_t X_t')$ Ã© crucial e pode apresentar desafios computacionais. Este mÃ©todo fornece uma base sÃ³lida para a anÃ¡lise de sÃ©ries temporais multivariadas e permite construir previsÃµes eficientes no sentido de minimizar o MSE, alÃ©m de garantir ortogonalidade do erro da projeÃ§Ã£o em relaÃ§Ã£o a $X_t$. Este resultado Ã© fundamental para a construÃ§Ã£o de modelos economÃ©tricos mais avanÃ§ados e para lidar com dados multivariados em geral.

### ReferÃªncias
[^4.1.21]:  *$P(Y_{t+1}|X_t) = \alpha'X_t = \hat{Y}_{t+1}$*.
[^4.1.22]: *$E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$*.
[^4.1.23]: *$\alpha' = [E(Y_{t+1}X_t')] \cdot [E(X_tX_t')]^{-1}$*.
[^4.1.24]: *$MSE(\alpha'X_t) = E{[Y_{t+1} - \alpha'X_t] \cdot [Y_{t+1} - \alpha'X_t]'} = E(Y_{t+1}Y_{t+1}') - [E(Y_{t+1}X_t')] \cdot [E(X_tX_t')]^{-1} \cdot [E(X_tY_{t+1}')]$*.
<!-- END -->
