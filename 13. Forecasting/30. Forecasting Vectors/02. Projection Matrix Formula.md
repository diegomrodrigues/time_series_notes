## Proje√ß√µes Lineares Vetoriais e o C√°lculo da Matriz de Coeficientes

### Introdu√ß√£o
Como vimos anteriormente, a proje√ß√£o linear √© uma ferramenta poderosa para construir previs√µes, e agora vamos generalizar esse conceito para o contexto multivariado, explorando como calcular a matriz de coeficientes que minimiza o erro quadr√°tico m√©dio (MSE) para cada elemento do vetor de vari√°veis dependentes. Em particular, focaremos na obten√ß√£o da matriz de proje√ß√£o $\alpha'$ usando a f√≥rmula $\alpha' = [E(Y_{t+1}X_t')] [E(X_tX_t')]^{-1}$, que envolve a invers√£o de matrizes. Essa t√©cnica √© fundamental para o processamento de dados multivariados, permitindo-nos lidar com situa√ß√µes em que m√∫ltiplas vari√°veis s√£o previstas simultaneamente.

### Conceitos Fundamentais
Relembrando, a proje√ß√£o linear de um vetor $Y_{t+1}$ (de dimens√£o $n \times 1$) em um vetor $X_t$ (de dimens√£o $m \times 1$) √© expressa como [^4.1.21]:
$$
\hat{Y}_{t+1} = P(Y_{t+1} | X_t) = \alpha' X_t,
$$
onde $\alpha'$ √© uma matriz de coeficientes de dimens√£o $n \times m$. O objetivo √© encontrar a matriz $\alpha'$ que minimize o MSE para cada elemento do vetor $\hat{Y}_{t+1}$. Isso √© equivalente a garantir que o erro de previs√£o, $(Y_{t+1} - \hat{Y}_{t+1})$, seja n√£o correlacionado com cada elemento de $X_t$ [^4.1.22]:

$$
E[(Y_{t+1} - \alpha'X_t)X_t'] = 0.
$$
Esta condi√ß√£o implica que cada elemento do vetor erro √© ortogonal a cada elemento do vetor $X_t$ (Proposi√ß√£o 1 do t√≥pico anterior).

A solu√ß√£o para $\alpha'$, que minimiza o MSE, √© dada por [^4.1.23]:
$$
\alpha' = [E(Y_{t+1}X_t')] [E(X_t X_t')]^{-1}.
$$
Esta f√≥rmula envolve a invers√£o da matriz $E(X_t X_t')$, que √© a matriz de covari√¢ncia de $X_t$, assumindo que $E(X_t) = 0$ (ou que um termo constante est√° inclu√≠do em $X_t$). A invers√£o dessa matriz √© crucial para obter a matriz de coeficientes $\alpha'$ que fornece a melhor proje√ß√£o linear de $Y_{t+1}$ em $X_t$.

> üí° **Exemplo Num√©rico:**
>
>Vamos supor que temos dois vetores, $Y_{t+1}$ (2x1) e $X_t$ (3x1), representando, por exemplo, pre√ßos de dois ativos e tr√™s indicadores econ√¥micos, respectivamente.
>
> $$ Y_{t+1} = \begin{bmatrix} y_{1,t+1} \\ y_{2,t+1} \end{bmatrix} , \quad X_t = \begin{bmatrix} x_{1,t} \\ x_{2,t} \\ x_{3,t} \end{bmatrix} $$
>
> Ap√≥s coletar dados, estimamos as seguintes matrizes de momentos (usando m√©dias amostrais):
>
> $$ E(Y_{t+1}X_t') = \begin{bmatrix} 2 & 1 & 0.5 \\ 1 & 1.5 & 0.2 \end{bmatrix} $$
>
> $$ E(X_t X_t') = \begin{bmatrix} 1 & 0.5 & 0.2 \\ 0.5 & 1 & 0.1 \\ 0.2 & 0.1 & 0.5 \end{bmatrix} $$
>
> Para calcular $\alpha'$, primeiro precisamos inverter $E(X_t X_t')$. Usando o NumPy:
>
> ```python
> import numpy as np
>
> Exx = np.array([[1, 0.5, 0.2],
>                 [0.5, 1, 0.1],
>                 [0.2, 0.1, 0.5]])
>
> Eyx = np.array([[2, 1, 0.5],
>                 [1, 1.5, 0.2]])
>
> Exx_inv = np.linalg.inv(Exx)
>
> alpha_prime = np.dot(Eyx, Exx_inv)
>
> print("Alpha':")
> print(alpha_prime)
> ```
>
> Isso resulta em:
>
> $$\alpha' \approx \begin{bmatrix} 2.28 & -0.35 & 0.09 \\ 0.64 & 1.41 & -0.04 \end{bmatrix}$$
>
> Cada linha de $\alpha'$ representa os coeficientes para prever uma vari√°vel em $Y_{t+1}$. Por exemplo, a primeira linha (2.28, -0.35, 0.09) nos diz como usar $x_{1,t}$, $x_{2,t}$ e $x_{3,t}$ para prever $y_{1,t+1}$.

**Lema 1:** (Revis√£o) A matriz $E(X_t X_t')$ √© invert√≠vel se e somente se n√£o existe nenhuma combina√ß√£o linear dos elementos de $X_t$ que seja igual a zero com probabilidade 1. (Lema 1 do t√≥pico anterior)

A condi√ß√£o de invertibilidade √© essencial, pois garante que o c√°lculo de $\alpha'$ seja bem definido. Se $E(X_t X_t')$ n√£o for invert√≠vel, significa que h√° redund√¢ncia nas vari√°veis de $X_t$, ou seja, algumas vari√°veis podem ser expressas como combina√ß√µes lineares das outras, e isso impede que se encontre uma solu√ß√£o √∫nica para a matriz $\alpha'$. Nestes casos, √© necess√°rio remover as vari√°veis redundantes ou usar m√©todos de regulariza√ß√£o que permitam obter solu√ß√µes aproximadas.

**Lema 1.1:** Se a matriz $E(X_t X_t')$ n√£o for invert√≠vel, ent√£o existe um vetor n√£o nulo $v$ tal que $X_t'v = 0$ com probabilidade 1.
*Prova:* Se $E(X_t X_t')$ n√£o √© invert√≠vel, ent√£o, pelo Lema 1, existe uma combina√ß√£o linear dos elementos de $X_t$ que √© igual a zero com probabilidade 1. Seja $v$ o vetor de coeficientes desta combina√ß√£o linear. Ent√£o $X_t'v=0$ com probabilidade 1, e $v \neq 0$.

A forma expandida do MSE desta previs√£o √≥tima √© dada por [^4.1.24]:
$$
MSE(\alpha'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)'] = E[Y_{t+1}Y_{t+1}'] - E[Y_{t+1}X_t'] [E(X_t X_t')]^{-1} E[X_tY_{t+1}'].
$$
Esta f√≥rmula generaliza o resultado para o caso escalar, e cada elemento diagonal da matriz $MSE(\alpha'X_t)$ representa o erro quadr√°tico m√©dio para cada vari√°vel em $Y_{t+1}$, enquanto os elementos fora da diagonal representam as covari√¢ncias entre os erros de previs√£o de diferentes vari√°veis em $Y_{t+1}$.

> üí° **Exemplo Num√©rico:**
>
> Continuando o exemplo anterior, vamos calcular o MSE. Primeiro, vamos assumir que temos a seguinte matriz:
>
> $$E(Y_{t+1}Y_{t+1}') = \begin{bmatrix} 5 & 2 \\ 2 & 4 \end{bmatrix}$$
>
> Agora vamos calcular o MSE usando a f√≥rmula:
>
> $$MSE(\alpha'X_t) = E[Y_{t+1}Y_{t+1}'] - E[Y_{t+1}X_t'] [E(X_t X_t')]^{-1} E[X_tY_{t+1}'].$$
>
> J√° calculamos $\alpha'$ como  $\begin{bmatrix} 2.28 & -0.35 & 0.09 \\ 0.64 & 1.41 & -0.04 \end{bmatrix}$ e temos $E(Y_{t+1}X_t')$ como $\begin{bmatrix} 2 & 1 & 0.5 \\ 1 & 1.5 & 0.2 \end{bmatrix}$. Note que $E[X_tY_{t+1}'] = (E[Y_{t+1}X_t'])'$.
>
> Usando Python:
>
> ```python
> Eyy = np.array([[5, 2],
>                [2, 4]])
>
> MSE = Eyy - np.dot(np.dot(Eyx, Exx_inv), Eyx.T)
> print("MSE:")
> print(MSE)
> ```
>
> O resultado √©:
>
> $$MSE \approx \begin{bmatrix} 0.71 & -0.38 \\ -0.38 & 1.26 \end{bmatrix}$$
>
> O elemento (1,1) do MSE, 0.71, √© o MSE da proje√ß√£o da primeira vari√°vel de $Y_{t+1}$, e o elemento (2,2), 1.26, √© o MSE da segunda vari√°vel. Os elementos fora da diagonal s√£o as covari√¢ncias entre os erros de previs√£o das duas vari√°veis.

**Teorema 1.1:** (Revis√£o) A proje√ß√£o linear $\hat{Y}_{t+1} = \alpha'X_t$, com $\alpha'$ dado por $\alpha' = [E(Y_{t+1}X_t')] [E(X_t X_t')]^{-1}$, √© a melhor previs√£o linear de $Y_{t+1}$ no sentido de minimizar o tra√ßo da matriz $MSE(\alpha'X_t)$. (Teorema 1.1 do t√≥pico anterior)

O tra√ßo da matriz $MSE$ representa a soma das vari√¢ncias de cada componente do vetor erro, e minimizar o tra√ßo significa minimizar a soma dos erros quadr√°ticos m√©dios das proje√ß√µes para cada vari√°vel em $Y_{t+1}$.

> üí° **Exemplo Num√©rico:**
>
> No exemplo anterior, o tra√ßo do MSE √© $0.71 + 1.26 = 1.97$.  Este valor representa a soma dos erros quadr√°ticos m√©dios da proje√ß√£o. Se compararmos com outras proje√ß√µes lineares, esta proje√ß√£o com o $\alpha'$ calculado minimiza este valor.

**Teorema 1.2:** A proje√ß√£o linear $\hat{Y}_{t+1} = \alpha'X_t$, com $\alpha'$ dado por $\alpha' = [E(Y_{t+1}X_t')] [E(X_t X_t')]^{-1}$, tamb√©m garante que o erro de previs√£o $(Y_{t+1} - \hat{Y}_{t+1})$ seja ortogonal a qualquer combina√ß√£o linear das vari√°veis em $X_t$.
*Prova:* Seja $Z_t = c'X_t$ uma combina√ß√£o linear qualquer das vari√°veis em $X_t$. Devemos provar que $E[(Y_{t+1} - \alpha'X_t)Z_t']=0$.
I.  Substitu√≠mos $Z_t$ por $c'X_t$:
   $$E[(Y_{t+1} - \alpha'X_t)Z_t'] = E[(Y_{t+1} - \alpha'X_t)(c'X_t)'].$$
II. Expandimos o termo $(c'X_t)'$:
  $$E[(Y_{t+1} - \alpha'X_t)(c'X_t)'] = E[(Y_{t+1} - \alpha'X_t)X_t'c].$$
III. Usamos a propriedade de linearidade do operador de esperan√ßa:
 $$E[(Y_{t+1} - \alpha'X_t)X_t'c] = E[(Y_{t+1} - \alpha'X_t)X_t']c.$$
IV. Pela condi√ß√£o de ortogonalidade $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$:
 $$E[(Y_{t+1} - \alpha'X_t)X_t']c = 0 \cdot c = 0.$$
V. Portanto, $E[(Y_{t+1} - \alpha'X_t)Z_t']=0$ ‚ñ†

### Implica√ß√µes Pr√°ticas do C√°lculo de Œ±'
A f√≥rmula para calcular $\alpha'$ $\alpha' = [E(Y_{t+1}X_t')] [E(X_t X_t')]^{-1}$, apesar de concisa, envolve algumas etapas cruciais:
 1. **C√°lculo das Matrizes de Momentos**: √â necess√°rio calcular $E(Y_{t+1}X_t')$ e $E(X_t X_t')$, tipicamente usando as m√©dias amostrais. Em aplica√ß√µes pr√°ticas, essas m√©dias s√£o estimadas usando dados hist√≥ricos.
 2. **Invers√£o da Matriz de Covari√¢ncia**: A invers√£o de $E(X_t X_t')$ pode ser computacionalmente custosa, especialmente para matrizes de alta dimens√£o. Al√©m disso, a matriz deve ser n√£o singular para que a inversa exista. Em muitos casos pr√°ticos, s√£o usadas t√©cnicas de regulariza√ß√£o para lidar com problemas de singularidade ou instabilidade num√©rica.
 3. **Multiplica√ß√£o de Matrizes**: A multiplica√ß√£o das matrizes $E(Y_{t+1}X_t')$ e $[E(X_t X_t')]^{-1}$ √© necess√°ria para obter $\alpha'$.

Em cen√°rios de s√©ries temporais, os momentos s√£o geralmente estimados usando dados hist√≥ricos. A qualidade da estimativa de $\alpha'$ depende diretamente da qualidade da estimativa dos momentos populacionais $E(Y_{t+1}X_t')$ e $E(X_t X_t')$. Portanto, √© essencial que a amostra utilizada para a estima√ß√£o seja representativa e suficientemente grande para que as m√©dias amostrais sejam boas aproxima√ß√µes dos momentos populacionais.

**Proposi√ß√£o 1:** Se as vari√°veis em $X_t$ s√£o linearmente independentes, ent√£o $E(X_t X_t')$ √© definida positiva.
*Prova:* Se as vari√°veis em $X_t$ s√£o linearmente independentes, ent√£o, pelo Lema 1, $E(X_t X_t')$ √© invert√≠vel. Para mostrar que $E(X_t X_t')$ √© definida positiva, considere um vetor $v \neq 0$.
I.  Definimos a forma quadr√°tica:
    $$ v'E(X_t X_t')v $$
II.  Utilizando a propriedade de esperan√ßa:
    $$ v'E(X_t X_t')v = E[v'X_t X_t'v].$$
III.  Reagrupando os termos:
     $$E[v'X_t X_t'v] = E[(X_t'v)^2].$$
IV. Como as vari√°veis em $X_t$ s√£o linearmente independentes, $X_t'v \neq 0$ com probabilidade 1 para qualquer $v\neq 0$.
V. Assim,  $E[(X_t'v)^2] > 0$, para todo vetor $v \neq 0$.
VI. Portanto, $E(X_t X_t')$ √© definida positiva. ‚ñ†

### Conclus√£o
O c√°lculo da matriz de proje√ß√£o $\alpha'$ usando a f√≥rmula $\alpha' = [E(Y_{t+1}X_t')] [E(X_t X_t')]^{-1}$ √© um passo essencial na constru√ß√£o de previs√µes lineares √≥timas no contexto multivariado. A invers√£o da matriz $E(X_t X_t')$ √© crucial e pode apresentar desafios computacionais. Este m√©todo fornece uma base s√≥lida para a an√°lise de s√©ries temporais multivariadas e permite construir previs√µes eficientes no sentido de minimizar o MSE, al√©m de garantir ortogonalidade do erro da proje√ß√£o em rela√ß√£o a $X_t$. Este resultado √© fundamental para a constru√ß√£o de modelos econom√©tricos mais avan√ßados e para lidar com dados multivariados em geral.

### Refer√™ncias
[^4.1.21]:  *$P(Y_{t+1}|X_t) = \alpha'X_t = \hat{Y}_{t+1}$*.
[^4.1.22]: *$E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$*.
[^4.1.23]: *$\alpha' = [E(Y_{t+1}X_t')] \cdot [E(X_tX_t')]^{-1}$*.
[^4.1.24]: *$MSE(\alpha'X_t) = E{[Y_{t+1} - \alpha'X_t] \cdot [Y_{t+1} - \alpha'X_t]'} = E(Y_{t+1}Y_{t+1}') - [E(Y_{t+1}X_t')] \cdot [E(X_tX_t')]^{-1} \cdot [E(X_tY_{t+1}')]$*.
<!-- END -->
