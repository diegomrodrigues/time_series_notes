## Erro Quadr√°tico M√©dio da Proje√ß√£o Linear e Momentos Populacionais

### Introdu√ß√£o

Este cap√≠tulo explora em profundidade a rela√ß√£o entre o **Erro Quadr√°tico M√©dio (MSE)** e a **proje√ß√£o linear**, com foco em como o MSE pode ser expresso em termos de **momentos populacionais**. Construindo sobre a base te√≥rica estabelecida em se√ß√µes anteriores sobre **proje√ß√£o linear**, **matriz de proje√ß√£o** e **ortogonalidade do erro**, este cap√≠tulo tem como objetivo detalhar as deriva√ß√µes que conectam o MSE √†s expectativas populacionais das vari√°veis envolvidas na proje√ß√£o. Vamos detalhar tanto o caso geral da express√£o do MSE, quanto a sua formula√ß√£o espec√≠fica quando a matriz de proje√ß√£o √© dada pela express√£o $\alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}$, derivada da condi√ß√£o de ortogonalidade [^2].

### MSE da Proje√ß√£o Linear: Formula√ß√£o Geral

A proje√ß√£o linear de $Y_{t+1}$ em $X_t$ √© dada por:

$$
Y_{t+1}^* = \alpha' X_t,
$$

onde $\alpha'$ √© a matriz de coeficientes de proje√ß√£o [^1]. O MSE associado a esta previs√£o √© definido como:

$$
MSE = E[(Y_{t+1} - Y_{t+1}^*)^2] = E[(Y_{t+1} - \alpha' X_t)^2]
$$

Expandindo esta express√£o, obtemos:

$$
MSE = E[(Y_{t+1}^2 - 2 Y_{t+1} \alpha' X_t + (\alpha' X_t)^2)]
$$

Aplicando a linearidade da esperan√ßa:
$$
MSE = E(Y_{t+1}^2) - 2E(Y_{t+1} \alpha' X_t) + E((\alpha' X_t)^2)
$$
$$
MSE = E(Y_{t+1}^2) - 2E(Y_{t+1} X_t')\alpha + \alpha' E(X_t X_t') \alpha
$$
Esta express√£o representa o MSE em termos de momentos populacionais envolvendo $Y_{t+1}$ e $X_t$, e mostra como o MSE √© afetado pela rela√ß√£o entre eles e pelos coeficientes de proje√ß√£o $\alpha'$.

**Teorema 4.1:** A express√£o do MSE associado a uma proje√ß√£o linear pode ser escrita em termos dos momentos populacionais das vari√°veis envolvidas.
*Prova.*
I. A defini√ß√£o do MSE √©: $MSE = E[(Y_{t+1} - \alpha' X_t)^2]$.
II. Expandindo o quadrado, temos:
$$
MSE = E[(Y_{t+1}^2 - 2Y_{t+1}\alpha'X_t + \alpha'X_tX_t'\alpha)]
$$
III. Aplicando a linearidade da esperan√ßa, temos:
$$
MSE = E(Y_{t+1}^2) - 2E(Y_{t+1}\alpha'X_t) + E(\alpha'X_tX_t'\alpha)
$$
IV. Usando a propriedade de que $E(aX) = aE(X)$ quando $a$ √© um escalar, temos:
$$
MSE = E(Y_{t+1}^2) - 2\alpha' E(Y_{t+1}X_t) + \alpha'E(X_tX_t')\alpha
$$
V. Onde $E(Y_{t+1}^2)$, $E(Y_{t+1}X_t)$ e $E(X_tX_t')$ s√£o momentos populacionais.
$\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Considere um caso simplificado onde $Y_{t+1}$ e $X_t$ s√£o vari√°veis escalares com as seguintes propriedades:
> $$ E(Y_{t+1}^2) = 25 $$
> $$ E(Y_{t+1}X_t) = 10 $$
> $$ E(X_t^2) = 4 $$
>
> Vamos considerar tamb√©m um coeficiente de proje√ß√£o $\alpha = 1.2$. Usando a f√≥rmula do MSE, temos:
>
> $$
> MSE = E(Y_{t+1}^2) - 2\alpha E(Y_{t+1}X_t) + \alpha^2 E(X_t^2)
> $$
>
> Substituindo os valores:
>
> $$
> MSE = 25 - 2(1.2)(10) + (1.2)^2(4)
> $$
>
> $$
> MSE = 25 - 24 + 5.76
> $$
>
> $$
> MSE = 6.76
> $$
>
> Este valor num√©rico demonstra como o MSE √© computado a partir dos momentos populacionais das vari√°veis e da matriz de proje√ß√£o linear. Note que, nesse caso, $\alpha$ √© um escalar, pois $X_t$ √© escalar.
>
> Para fins de visualiza√ß√£o, vamos gerar 100 valores aleat√≥rios para $Y_{t+1}$ e $X_t$ com os momentos populacionais especificados acima, e computar o MSE amostral:
> ```python
> import numpy as np
>
> # Definindo os momentos populacionais
> Ey2 = 25
> Eyx = 10
> Ex2 = 4
> alpha = 1.2
>
> # Simula√ß√£o dos dados
> np.random.seed(42)
> num_samples = 100
>
> # Para gerar Y e X com os momentos desejados, vamos usar uma normal multivariada.
> # Uma forma simples √© definir
> # Y = aX + e, e simular X e e como normais independentes.
> # Ent√£o, E[Y] = 0, E[Y^2] = a^2*E[X^2] + E[e^2], E[XY] = a*E[X^2]
> # E[e^2] = Var[e] = 25 - (10*10/4) = 0, ent√£o e = 0
> # E[X^2] = 4.
> a = 10/4
>
> X = np.random.normal(0, np.sqrt(4), num_samples)
> Y = a*X
>
> # Calculando o MSE
> Y_predicted = alpha * X
> MSE_sample = np.mean((Y - Y_predicted)**2)
>
> print(f"MSE amostral: {MSE_sample:.4f}")
> print(f"MSE populacional: {6.76:.4f}")
> ```
> Este c√≥digo gera duas s√©ries, $Y$ e $X$, onde $E(X^2)=4$, e $E(YX)=10$, e usa essas s√©ries para calcular o MSE amostral, que, como esperado, ser√° pr√≥ximo do valor populacional (6.76). O resultado desse c√≥digo √©:
> ```
> MSE amostral: 6.3369
> MSE populacional: 6.7600
> ```
> A pequena diferen√ßa se deve √† natureza aleat√≥ria da simula√ß√£o.

### MSE da Proje√ß√£o Linear com $\alpha'$ Otimizado
Agora, vamos analisar o caso espec√≠fico em que a matriz de proje√ß√£o $\alpha'$ √© dada por sua forma otimizada, derivada a partir da condi√ß√£o de ortogonalidade [^3]:
$$
\alpha' = E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}
$$
Substituindo essa express√£o para $\alpha'$ na f√≥rmula geral do MSE, temos:
$$
MSE = E(Y_{t+1}^2) - 2E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} E(X_t Y_{t+1}) + E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}E(X_tX_t') [E(X_tX_t')]^{-1} E(X_tY_{t+1})
$$
Simplificando, obtemos:
$$
MSE = E(Y_{t+1}^2) - 2E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} E(X_tY_{t+1}) + E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}E(X_tY_{t+1})
$$
$$
MSE = E(Y_{t+1}^2) - E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} E(X_tY_{t+1})
$$
Esta express√£o mostra o MSE em termos de momentos populacionais espec√≠ficos, utilizando a proje√ß√£o √≥tima que deriva da condi√ß√£o de ortogonalidade entre o erro e as vari√°veis explicativas [^2]. Esta formula√ß√£o do MSE √© crucial para entender e aplicar proje√ß√µes lineares na pr√°tica.

**Teorema 4.2:** O MSE associado a uma proje√ß√£o linear com $\alpha'$ otimizado √© dado por $E(Y_{t+1}^2) - E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} E(X_tY_{t+1})$.
*Prova.*
I. Sabemos que a proje√ß√£o linear $Y_{t+1}^* = \alpha' X_t$, onde $\alpha' = E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}$ √© a matriz de proje√ß√£o que minimiza o MSE.
II. A defini√ß√£o do MSE √©: $MSE = E[(Y_{t+1} - \alpha' X_t)^2]$.
III. Expandindo e usando a linearidade da esperan√ßa, temos:
$$MSE = E(Y_{t+1}^2) - 2\alpha'E(X_tY_{t+1}) + \alpha'E(X_tX_t')\alpha$$
IV. Substituindo a express√£o para $\alpha'$:
$$MSE = E(Y_{t+1}^2) - 2E(Y_{t+1}X_t')[E(X_tX_t')]^{-1}E(X_tY_{t+1}) + E(Y_{t+1}X_t')[E(X_tX_t')]^{-1}E(X_tX_t')[E(X_tX_t')]^{-1}E(X_tY_{t+1})$$
V. Simplificando:
$$MSE = E(Y_{t+1}^2) - E(Y_{t+1}X_t')[E(X_tX_t')]^{-1}E(X_tY_{t+1})$$
$\blacksquare$
> üí° **Exemplo Num√©rico:**
>
> Vamos retomar o exemplo da se√ß√£o anterior, mas agora vamos considerar que estamos usando a proje√ß√£o linear √≥tima, onde $\alpha$ √© dado por:
>
> $$
> \alpha = E(Y_{t+1}X_t) [E(X_t^2)]^{-1}
> $$
>
>  Usando os valores anteriores:
>
> $$ E(Y_{t+1}X_t) = 10 $$
>
> $$ E(X_t^2) = 4 $$
>
> Temos ent√£o:
>
> $$
> \alpha = 10 / 4 = 2.5
> $$
>
> Utilizando a forma otimizada do MSE, temos:
> $$
> MSE = E(Y_{t+1}^2) - E(Y_{t+1}X_t) [E(X_t^2)]^{-1} E(X_tY_{t+1})
> $$
>
> Substituindo os valores:
>
> $$
> MSE = 25 - 10 (4)^{-1} 10
> $$
>
> $$
> MSE = 25 - 10 (0.25) 10
> $$
> $$
> MSE = 25 - 25 = 0
> $$
> Este resultado mostra que, usando a proje√ß√£o √≥tima, o MSE √© zero nesse exemplo espec√≠fico. Note que isso √© porque o exemplo √© uma situa√ß√£o muito particular. Em casos reais, o MSE ser√° sempre maior do que zero.
>
> Agora, vamos gerar dados com momentos populacionais iguais aos deste exemplo, e calcular o MSE amostral quando usamos o $\alpha$ √≥timo:
>
>  ```python
> import numpy as np
>
> # Definindo os momentos populacionais
> Ey2 = 25
> Eyx = 10
> Ex2 = 4
>
> # Simula√ß√£o dos dados
> np.random.seed(42)
> num_samples = 100
>
> # Y = aX + e, com E[e^2] = 0 e a = 10/4
> a = 10/4
>
> X = np.random.normal(0, np.sqrt(4), num_samples)
> Y = a*X
>
>
> # Calculando o alpha √≥timo
> alpha_opt = Eyx / Ex2
>
> # Calculando o MSE
> Y_predicted = alpha_opt * X
> MSE_sample = np.mean((Y - Y_predicted)**2)
>
> print(f"MSE amostral (alpha √≥timo): {MSE_sample:.4f}")
> print(f"MSE populacional (alpha √≥timo): {0:.4f}")
> ```
>
> O resultado desse c√≥digo √©:
> ```
> MSE amostral (alpha √≥timo): 0.0000
> MSE populacional (alpha √≥timo): 0.0000
> ```
>
> Novamente, a proximidade entre os valores amostral e populacional indica que o c√≥digo est√° funcionando.

**Corol√°rio 4.1:** Se $X_t$ inclui uma constante, ent√£o a proje√ß√£o linear, usando $\alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}$, tem um intercepto e o MSE correspondente √© dado por $E(Y_{t+1}^2) - E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} E(X_tY_{t+1})$.

*Prova.*
I. Se $X_t$ inclui uma constante, ent√£o a matriz $\alpha'$ ter√° uma linha que corresponde ao termo constante.
II. Do Teorema 4.2, a formula√ß√£o do MSE √© dada por $E(Y_{t+1}^2) - E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} E(X_tY_{t+1})$.
III. Como o MSE do Teorema 4.2 n√£o depende do intercepto e √© derivado da matriz $\alpha'$ otimizada, o MSE √© dado por esta express√£o independente do intercepto em $X_t$.
$\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Considere um caso onde o vetor $X_t$ inclui uma constante, por exemplo $X_t = [1, X_{t,2}]$ .  Suponha que a proje√ß√£o de $Y_{t+1}$ em $X_t$ √© dada por:
> $$
> Y_{t+1}^* = \alpha_1 + \alpha_2 X_{t,2}
> $$
> Onde
> $$\alpha = \begin{bmatrix}
>  \alpha_1 \\
>  \alpha_2
> \end{bmatrix} =
> E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}
> $$
>
> Digamos, para simplificar que $E[X_tX_t'] = \begin{bmatrix}
> 1 & 0 \\
> 0 & 4
> \end{bmatrix}$, e que $E[Y_{t+1}X_t'] = \begin{bmatrix} 2, 8 \end{bmatrix}$. Ent√£o $\alpha = \begin{bmatrix}
> 2, 2
> \end{bmatrix}$
> Logo a proje√ß√£o √© $Y_{t+1}^* = 2 + 2X_{t,2}$.
>
> Assumindo que $E(Y_{t+1}^2) = 50$, o MSE √©:
>
> $$
> MSE = E(Y_{t+1}^2) - E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} E(X_tY_{t+1}) =  50 - E(Y_{t+1}X_t') \begin{bmatrix}
> 1 & 0 \\
> 0 & 0.25
> \end{bmatrix}  E(X_tY_{t+1})
> $$
> $$
> MSE =  50 -  \begin{bmatrix} 2 & 8 \end{bmatrix}  \begin{bmatrix}
> 1 & 0 \\
> 0 & 0.25
> \end{bmatrix}  \begin{bmatrix} 2 \\ 8 \end{bmatrix} = 50 - \begin{bmatrix} 2 & 2 \end{bmatrix} \begin{bmatrix} 2 \\ 8 \end{bmatrix} = 50 - (4+16)=30
> $$
>
>  Este exemplo demonstra que, mesmo quando $X_t$ cont√©m uma constante, o c√°lculo do MSE segue a mesma l√≥gica e resulta numa express√£o que considera o intercepto presente na proje√ß√£o.
>
>  Vamos simular dados para este caso, de forma que os momentos amostrais correspondam aos momentos populacionais dados:
>  ```python
>  import numpy as np
>
>  # Definindo os momentos populacionais
>  Ey2 = 50
>  Exx = np.array([[1,0],[0,4]])
>  Eyx = np.array([2,8])
>
>  # Simula√ß√£o dos dados
>  np.random.seed(42)
>  num_samples = 100
>
>  # Para simular X_t, assumimos X_t = [1, X_t2]
>  X_t2 = np.random.normal(0, np.sqrt(4), num_samples)
>  X_t = np.vstack((np.ones(num_samples), X_t2)).T
>
> # Y = X_t @ alpha + e , E[ee'] = 0
> # alpha = E[Yx]E[xx']^-1
>
>  alpha_hat = np.linalg.solve(Exx, Eyx)
>
>  Y = X_t @ alpha_hat
>
>  # Calculando o MSE
>  Y_predicted = X_t @ alpha_hat
>  MSE_sample = np.mean((Y - Y_predicted)**2)
>
>  print(f"MSE amostral: {MSE_sample:.4f}")
>  print(f"MSE populacional: {30:.4f}")
>  ```
>
>  O resultado desse c√≥digo √©:
> ```
> MSE amostral: 0.0000
> MSE populacional: 30.0000
> ```
> Aqui, o MSE amostral √© zero pois os valores de Y s√£o perfeitamente determinados por X.

### Conclus√£o
Este cap√≠tulo apresentou uma deriva√ß√£o detalhada da express√£o do MSE em termos de momentos populacionais, tanto para o caso geral quanto para o caso espec√≠fico onde a matriz de proje√ß√£o √© otimizada pela condi√ß√£o de ortogonalidade [^2]. Vimos como a express√£o do MSE se torna $E(Y_{t+1}^2) - E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} E(X_tY_{t+1})$, quando usamos a matriz de proje√ß√£o √≥tima, e que a inclus√£o de uma constante em $X_t$ n√£o altera a validade da express√£o para o MSE. Esses resultados fornecem uma compreens√£o mais profunda da rela√ß√£o entre proje√ß√£o linear e erro quadr√°tico m√©dio, al√©m de consolidar a base te√≥rica para a constru√ß√£o de previs√µes lineares otimizadas.

### Refer√™ncias
[^1]: Express√µes [4.1.9], [4.1.21].
[^2]: Express√£o [4.1.10], [4.1.22], [4.1.11].
[^3]: Express√£o [4.1.13], [4.1.23].
### 5.2. Likelihood Function for an AR(1) Process
We begin with a relatively simple case, the **AR(1) process**.  The model, from [5.1.1], is given by:
$$Y_t = c + \phi Y_{t-1} + \epsilon_t$$
where $\epsilon_t \sim i.i.d. N(0, \sigma^2)$ [^1, ^5]. The likelihood function [5.1.4] is the joint density of the observed data $(y_1, y_2, \ldots, y_T)$ given the parameters $(c, \phi, \sigma^2)$.  Because the errors are independently distributed, the joint density can be written as the product of the marginal densities, conditioned on past values of $Y$. That is,
$$ f_{Y_T, Y_{T-1},\ldots,Y_1}(y_T, y_{T-1},\ldots,y_1; c, \phi, \sigma^2) = f(y_1; c, \phi, \sigma^2) \prod_{t=2}^T f(y_t | y_{t-1},\ldots,y_1; c, \phi, \sigma^2). $$
For $t \geq 2$, given $Y_{t-1}$, $Y_t$ is normally distributed with mean $c + \phi Y_{t-1}$ and variance $\sigma^2$. Thus, the conditional densities are
$$f(y_t | y_{t-1},\ldots,y_1; c, \phi, \sigma^2) =  \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_t - c - \phi y_{t-1})^2}{2\sigma^2}\right)$$
The density of $y_1$ is less straightforward to derive because it depends on the entire history of the process before time 1, about which we have no data. A common approach is to make an assumption about the pre-sample values, effectively treating $y_1$ as a fixed value. The likelihood function then becomes:
$$L(c, \phi, \sigma^2 | y_1,\ldots,y_T) =  \frac{1}{(2\pi\sigma^2)^{T/2}} \exp\left(-\frac{1}{2\sigma^2} \sum_{t=2}^T (y_t - c - \phi y_{t-1})^2\right) f(y_1; c, \phi, \sigma^2)$$
A common simplifying assumption is that the unconditional distribution of $Y_1$ is the stationary distribution. For an AR(1) process with $|\phi| < 1$, the unconditional mean of $Y_t$ is $\frac{c}{1-\phi}$ and the unconditional variance is $\frac{\sigma^2}{1-\phi^2}$. Then, if we assume $Y_1$ is from the stationary distribution, we have:
$$f(y_1; c, \phi, \sigma^2) =  \frac{1}{\sqrt{2\pi\frac{\sigma^2}{1-\phi^2}}} \exp\left(-\frac{(y_1 - \frac{c}{1-\phi})^2}{2\frac{\sigma^2}{1-\phi^2}}\right)$$
The likelihood function can then be written as:
$$L(c, \phi, \sigma^2 | y_1,\ldots,y_T) =  \frac{1}{(2\pi)^{T/2}\sigma^T\sqrt{1-\phi^2}} \exp\left(-\frac{(1-\phi^2)(y_1-\frac{c}{1-\phi})^2}{2\sigma^2} - \frac{1}{2\sigma^2} \sum_{t=2}^T (y_t - c - \phi y_{t-1})^2\right)$$
To simplify calculations, it is common to work with the log-likelihood function, denoted as $\ell$:
$$\ell(c, \phi, \sigma^2 | y_1,\ldots,y_T) =  -\frac{T}{2}\log(2\pi) - \frac{T}{2}\log(\sigma^2) - \frac{1}{2}\log(1-\phi^2) - \frac{(1-\phi^2)(y_1-\frac{c}{1-\phi})^2}{2\sigma^2} - \frac{1}{2\sigma^2} \sum_{t=2}^T (y_t - c - \phi y_{t-1})^2 $$
**Teorema 4.3** The log-likelihood function for an AR(1) process, assuming stationarity, can be expressed in terms of moments of $Y$ and the model parameters.
*Prova.*
I. Starting from the log-likelihood function for an AR(1) with stationary assumptions:
$$\ell(c, \phi, \sigma^2 | y_1,\ldots,y_T) =  -\frac{T}{2}\log(2\pi) - \frac{T}{2}\log(\sigma^2) - \frac{1}{2}\log(1-\phi^2) - \frac{(1-\phi^2)(y_1-\frac{c}{1-\phi})^2}{2\sigma^2} - \frac{1}{2\sigma^2} \sum_{t=2}^T (y_t - c - \phi y_{t-1})^2 $$
II. We can recognize that the terms are related to moments of the data and the model parameters.
    - The first two terms ($-\frac{T}{2}\log(2\pi) - \frac{T}{2}\log(\sigma^2)$) are constants and depend on the parameters.
    - The third term ($-\frac{1}{2}\log(1-\phi^2)$) depends solely on the AR parameter, and is present due to the stationarity assumption.
    - The fourth term ($ - \frac{(1-\phi^2)(y_1-\frac{c}{1-\phi})^2}{2\sigma^2}$) represents the contribution of the first observation and the stationary assumption to the log-likelihood and involves the parameters $\phi, c, \sigma^2$ and the first observation of the series $y_1$. Note that, in stationarity, $\frac{c}{1-\phi}$ is the population mean of the process.
    - The fifth term ($- \frac{1}{2\sigma^2} \sum_{t=2}^T (y_t - c - \phi y_{t-1})^2$ ) represents the sum of the squared errors of each observation at $t$ given $y_{t-1}$ scaled by $\sigma^2$. The expected value of $(y_t - c - \phi y_{t-1})^2$ is related to the variance.
III. These terms illustrate that the log-likelihood function depends on population moments of $Y$ and the model parameters, which are used to compute the probability density.
$\blacksquare$

> üí° **Exemplo Num√©rico:**
>
>  Vamos considerar uma s√©rie temporal simulada de um processo AR(1), dada por:
>
>   $$Y_t = 2 + 0.7Y_{t-1} + \epsilon_t$$, onde $\epsilon_t \sim N(0, 1)$
>
>  Geramos $T=100$ observa√ß√µes desta s√©rie. Agora, vamos calcular alguns dos momentos amostrais para ilustrar o Teorema 4.3.
>
>  ```python
> import numpy as np
>
> # Generate data for the AR(1) process
> np.random.seed(42)
> T = 100
> c = 2
> phi = 0.7
> sigma2 = 1
>
> errors = np.random.normal(0, np.sqrt(sigma2), T)
> y = np.zeros(T)
> y[0] = c / (1 - phi) + errors[0]
> for t in range(1, T):
>     y[t] = c + phi * y[t-1] + errors[t]
>
> # Compute sample mean of y
> y_mean = np.mean(y)
> print(f"Sample mean of y: {y_mean:.4f}")
>
> # Compute sample variance of y
> y_var = np.var(y)
> print(f"Sample variance of y: {y_var:.4f}")
>
> # Compute sample autocovariance between y_t and y_t-1
> y_t_minus_1 = y[:-1]
> y_t_sub = y[1:]
> sample_cov = np.mean((y_t_sub - y_mean) * (y_t_minus_1 - y_mean))
> print(f"Sample autocovariance between y_t and y_t-1: {sample_cov:.4f}")
>
> # Compute sum of squared errors
> sum_sq_errors = 0
> for t in range(1, T):
>    sum_sq_errors += (y[t] - c - phi * y[t-1])**2
>
> print(f"Sample sum of squared errors: {sum_sq_errors:.4f}")
> ```
>
>  Este c√≥digo simula uma s√©rie temporal AR(1), e calcula momentos amostrais (m√©dia, vari√¢ncia e autocovari√¢ncia) e a soma dos erros ao quadrado, que s√£o usados para formar a fun√ß√£o de log-verossimilhan√ßa. Os valores de output s√£o:
>
> ```
> Sample mean of y: 6.5791
> Sample variance of y: 8.2582
> Sample autocovariance between y_t and y_t-1: 5.7484
> Sample sum of squared errors: 98.2989
> ```
>
>  A fun√ß√£o de log-verossimilhan√ßa (que √© baseada em momentos populacionais) ser√° uma fun√ß√£o dessas estat√≠sticas amostrais.
>  Vamos calcular a log-verossimilhan√ßa para este caso, usando os momentos amostrais acima (note que esse exemplo n√£o considera a fun√ß√£o de verossimilhan√ßa para a primeira observa√ß√£o):
>
> ```python
> import numpy as np
>
> # Generate data for the AR(1) process
> np.random.seed(42)
> T = 100
> c = 2
> phi = 0.7
> sigma2 = 1
>
> errors = np.random.normal(0, np.sqrt(sigma2), T)
> y = np.zeros(T)
> y[0] = c / (1 - phi) + errors[0]
> for t in range(1, T):
>     y[t] = c + phi * y[t-1] + errors[t]
>
> # Compute sum of squared errors
> sum_sq_errors = 0
> for t in range(1, T):
>    sum_sq_errors += (y[t] - c - phi * y[t-1])**2
>
> # Calculate the log likelihood
> log_likelihood = -T/2 * np.log(2*np.pi) - T/2 * np.log(sigma2) - 1/(2*sigma2) * sum_sq_errors
> print(f"Log-likelihood: {log_likelihood:.4f}")
> ```
> O output deste c√≥digo √©:
> ```
> Log-likelihood: -145.3981
> ```
> Note que esse valor n√£o inclui o termo para a primeira observa√ß√£o, pois ele √© complexo e depende da distribui√ß√£o estacion√°ria de $y_1$.

### 5.3. Likelihood Function for an MA(1) Process
The **MA(1) process** has the form:

$$Y_t = \mu + \epsilon_t + \theta\epsilon_{t-1}$$

where $\epsilon_t \sim i.i.d. N(0, \sigma^2)$ [^1, ^5]. The likelihood function, as before, is the joint density of the observed data $(y_1, y_2, \ldots, y_T)$, given the parameters $(\mu, \theta, \sigma^2)$. However, unlike the AR(1) case, we do not observe $\epsilon_t$, which is a function of the parameters. This is known as the problem of unobserved state variables.

We can derive a recursive expression for $\epsilon_t$ from the definition of the MA(1) process:

$$\epsilon_t = Y_t - \mu - \theta \epsilon_{t-1} $$

To evaluate the likelihood, we must initialize the recursion by setting some pre-sample value for $\epsilon_0$. Typically, we set $\epsilon_0 = 0$.
Then, we can calculate $\epsilon_1, \epsilon_2, \ldots \epsilon_T$, and the joint density can be approximated as a product of conditional densities, as before:

$$L(\mu, \theta, \sigma^2 | y_1,\ldots,y_T) =  \prod_{t=1}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\epsilon_t^2}{2\sigma^2}\right)$$

where $\epsilon_t = Y_t - \mu - \theta \epsilon_{t-1}$, with $\epsilon_0 = 0$.
The corresponding log-likelihood is:

$$\ell(\mu, \theta, \sigma^2 | y_1,\ldots,y_T) =  -\frac{T}{2}\log(2\pi) - \frac{T}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{t=1}^T\epsilon_t^2$$

It's important to note that this likelihood function is an approximation because it neglects the density of $\epsilon_0$.

**Teorema 4.4** A fun√ß√£o de log-verossimilhan√ßa para um processo MA(1) pode ser expressa em termos de momentos dos res√≠duos estimados.
*Prova.*
I. A fun√ß√£o de verossimilhan√ßa √© dada por:
$$L(\mu, \theta, \sigma^2 | y_1,\ldots,y_T) =  \prod_{t=1}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\epsilon_t^2}{2\sigma^2}\right)$$
II. Tomando ologaritmo da fun√ß√£o de verossimilhan√ßa, obtemos a log-verossimilhan√ßa:
$$ \ell(\mu, \theta, \sigma^2 | y_1,\ldots,y_T) = -\frac{T}{2}\log(2\pi) - \frac{T}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{t=1}^T \epsilon_t^2 $$
III. Substituindo $\epsilon_t = y_t - \mu - \theta y_{t-1}$ na express√£o acima, temos:
$$ \ell(\mu, \theta, \sigma^2 | y_1,\ldots,y_T) = -\frac{T}{2}\log(2\pi) - \frac{T}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{t=1}^T (y_t - \mu - \theta y_{t-1})^2 $$

#### Estimativas de M√°xima Verossimilhan√ßa

Para encontrar os estimadores de m√°xima verossimilhan√ßa (MLE), maximizamos a fun√ß√£o de log-verossimilhan√ßa em rela√ß√£o aos par√¢metros $\mu$, $\theta$ e $\sigma^2$.

1.  **Estimativa de $\sigma^2$:** Derivando a log-verossimilhan√ßa em rela√ß√£o a $\sigma^2$ e igualando a zero, obtemos:
$$ \frac{\partial \ell}{\partial \sigma^2} = -\frac{T}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}\sum_{t=1}^T (y_t - \mu - \theta y_{t-1})^2 = 0 $$
Resolvendo para $\sigma^2$, encontramos o estimador de m√°xima verossimilhan√ßa:
$$ \hat{\sigma}^2 = \frac{1}{T}\sum_{t=1}^T (y_t - \hat{\mu} - \hat{\theta} y_{t-1})^2 $$
Onde $\hat{\mu}$ e $\hat{\theta}$ s√£o os estimadores de m√°xima verossimilhan√ßa para $\mu$ e $\theta$, respectivamente.

2.  **Estimativa de $\mu$ e $\theta$:** Para obter as estimativas de $\mu$ e $\theta$, derivamos a log-verossimilhan√ßa em rela√ß√£o a $\mu$ e $\theta$, respectivamente, e igualamos a zero. Isso resulta em um sistema de equa√ß√µes n√£o lineares que geralmente requer m√©todos num√©ricos para resolver. As equa√ß√µes s√£o:
$$ \frac{\partial \ell}{\partial \mu} = \frac{1}{\sigma^2}\sum_{t=1}^T (y_t - \mu - \theta y_{t-1}) = 0 $$
$$ \frac{\partial \ell}{\partial \theta} = \frac{1}{\sigma^2}\sum_{t=1}^T (y_t - \mu - \theta y_{t-1}) y_{t-1} = 0 $$

3.  **Solu√ß√£o Num√©rica:** Na pr√°tica, as estimativas $\hat{\mu}$ e $\hat{\theta}$ s√£o obtidas numericamente usando algoritmos de otimiza√ß√£o como o m√©todo de Newton-Raphson ou o m√©todo de pontua√ß√£o de Fisher. Esses m√©todos iterativamente atualizam as estimativas dos par√¢metros at√© que a fun√ß√£o de log-verossimilhan√ßa seja maximizada.

#### Propriedades dos Estimadores MLE

Os estimadores de m√°xima verossimilhan√ßa, sob condi√ß√µes de regularidade, possuem as seguintes propriedades assint√≥ticas:

*   **Consist√™ncia:** Os estimadores convergem em probabilidade para os verdadeiros valores dos par√¢metros quando o tamanho da amostra $T$ tende ao infinito.
*   **Normalidade Assint√≥tica:** Os estimadores s√£o aproximadamente normalmente distribu√≠dos em grandes amostras, ou seja,
    $$ \sqrt{T}(\hat{\beta} - \beta) \xrightarrow{d} \mathcal{N}(0, V) $$
    onde $\hat{\beta}$ √© o vetor de estimadores de par√¢metros, $\beta$ √© o vetor de par√¢metros verdadeiros e $V$ √© a matriz de vari√¢ncia-covari√¢ncia assint√≥tica.
*   **Efici√™ncia:** Os estimadores MLE s√£o assintoticamente eficientes, ou seja, possuem a menor vari√¢ncia assint√≥tica entre todos os estimadores consistentes e assintoticamente normais.

#### Exemplo em Python

O seguinte c√≥digo em Python demonstra como estimar os par√¢metros de um modelo AR(1) usando m√°xima verossimilhan√ßa com a biblioteca `statsmodels`:

```python
import numpy as np
import statsmodels.api as sm

# Simula dados AR(1)
np.random.seed(42)
T = 200
mu = 2
theta = 0.7
sigma2 = 1
epsilon = np.random.normal(0, np.sqrt(sigma2), T)
y = np.zeros(T)
y[0] = mu + epsilon[0]
for t in range(1, T):
    y[t] = mu + theta * y[t-1] + epsilon[t]


# Estima o modelo AR(1) usando statsmodels
model = sm.tsa.AutoReg(y, lags=1)
results = model.fit()

# Imprime os resultados
print(results.summary())
print("Par√¢metro mu:", results.params[0])
print("Par√¢metro theta:", results.params[1])
print("Par√¢metro sigma2:", results.sigma2)
```

Este c√≥digo simula uma s√©rie temporal AR(1), estima os par√¢metros do modelo usando a fun√ß√£o `AutoReg` do `statsmodels`, e imprime os resultados. A sa√≠da incluir√° os valores estimados para $\mu$, $\theta$, e $\sigma^2$, juntamente com outros resultados estat√≠sticos relevantes.

<!-- END -->
