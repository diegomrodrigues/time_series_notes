## A Deriva√ß√£o do Coeficiente de Proje√ß√£o Linear e sua Rela√ß√£o com Momentos

### Introdu√ß√£o
Este cap√≠tulo se dedica a explorar em detalhes a deriva√ß√£o do **coeficiente de proje√ß√£o linear** $\alpha$, estabelecendo sua conex√£o com os **momentos populacionais** de $Y_{t+1}$ e $X_t$. Em particular, vamos demonstrar como a condi√ß√£o de **n√£o correla√ß√£o entre o erro de previs√£o e as vari√°veis explicativas** [^2], $E[(Y_{t+1} - \alpha' X_t) X_t'] = 0$, leva √† formula√ß√£o do coeficiente $\alpha$ como $\alpha' = E(Y_{t+1} X_t') [E(X_t X_t')]^{-1}$, assumindo que $E(X_t X_t')$ √© n√£o singular [^3]. Este cap√≠tulo √© fundamental para entender como as proje√ß√µes lineares s√£o constru√≠das e justificadas, utilizando os momentos populacionais das vari√°veis envolvidas no modelo.

### A Deriva√ß√£o do Coeficiente de Proje√ß√£o Linear

Recordamos que a proje√ß√£o linear de $Y_{t+1}$ sobre $X_t$ √© dada por:

$$
Y_{t+1}^* = \alpha' X_t,
$$

onde $\alpha'$ √© a matriz de coeficientes que minimiza o erro quadr√°tico m√©dio (MSE) dentro da classe das proje√ß√µes lineares [^1]. Para encontrar a express√£o para $\alpha'$, impomos a condi√ß√£o de que o erro de previs√£o $e_{t+1} = Y_{t+1} - Y_{t+1}^* = Y_{t+1} - \alpha' X_t$ seja ortogonal a $X_t$, ou seja, n√£o correlacionado com $X_t$ [^2]:
$$
E[e_{t+1} X_t'] = E[(Y_{t+1} - \alpha' X_t) X_t'] = 0.
$$
Esta condi√ß√£o √© fundamental para derivar a forma espec√≠fica para $\alpha'$ [^2].  Expandindo a equa√ß√£o, temos:
$$
E[Y_{t+1}X_t'] - E[\alpha'X_tX_t'] = 0
$$
$$
E[Y_{t+1}X_t'] = \alpha' E[X_tX_t'].
$$
Assumindo que a matriz de momentos $E[X_t X_t']$ seja **n√£o singular**, o que significa que sua inversa existe, podemos isolar a matriz de coeficientes de proje√ß√£o $\alpha'$:
$$
\alpha' = E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}.
$$
Esta express√£o para $\alpha'$ mostra que os coeficientes de proje√ß√£o linear s√£o calculados a partir dos momentos populacionais de $Y_{t+1}$ e $X_t$ [^3]. Especificamente, $\alpha'$ depende da covari√¢ncia entre $Y_{t+1}$ e $X_t$, dada por $E[Y_{t+1}X_t']$, e da matriz de covari√¢ncia de $X_t$, dada por $E[X_t X_t']$.

**Teorema 5.1:** O coeficiente de proje√ß√£o linear $\alpha'$, derivado da condi√ß√£o de n√£o correla√ß√£o entre o erro de previs√£o e $X_t$, √© dado por  $\alpha' = E(Y_{t+1} X_t') [E(X_t X_t')]^{-1}$, assumindo $E(X_t X_t')$ n√£o singular.
*Prova.*
I. Partimos da condi√ß√£o de n√£o correla√ß√£o $E[(Y_{t+1} - \alpha' X_t)X_t'] = 0$.
II. Expandindo a express√£o, temos: $E[Y_{t+1} X_t'] - E[\alpha' X_t X_t'] = 0$.
III. Rearranjando, obtemos: $E[Y_{t+1} X_t'] = \alpha' E[X_t X_t']$.
IV. Assumindo que $E[X_t X_t']$ √© n√£o singular, ou seja, possui inversa, pr√©-multiplicamos ambos os lados por $[E(X_t X_t')]^{-1}$:
$$
E[Y_{t+1} X_t'] [E(X_t X_t')]^{-1} = \alpha' E[X_t X_t'] [E(X_t X_t')]^{-1}
$$
V. Simplificando, temos:
$$
\alpha' = E[Y_{t+1} X_t'] [E(X_t X_t')]^{-1}.
$$
$\blacksquare$

> üí° **Exemplo Num√©rico:**
>
>  Vamos considerar que temos as seguintes expectativas populacionais:
>
>  $$ E[Y_{t+1} X_t'] = \begin{bmatrix} 10 & 5 \end{bmatrix} $$
>
>  $$ E[X_t X_t'] = \begin{bmatrix} 4 & 1 \\ 1 & 2 \end{bmatrix} $$
>
>  Para calcular o coeficiente de proje√ß√£o linear $\alpha'$, precisamos primeiro calcular a inversa de $E[X_t X_t']$:
>
> $$
>  [E(X_t X_t')]^{-1} =  \begin{bmatrix} 4 & 1 \\ 1 & 2 \end{bmatrix}^{-1}
> $$
>
>  Usando a f√≥rmula para inversa de uma matriz 2x2, que √© dada por $\frac{1}{ad-bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$ , temos:
>
> $$
>  [E(X_t X_t')]^{-1} =  \frac{1}{(4\times 2)-(1\times 1)}\begin{bmatrix} 2 & -1 \\ -1 & 4 \end{bmatrix} = \frac{1}{7}\begin{bmatrix} 2 & -1 \\ -1 & 4 \end{bmatrix}
> $$
>
> Agora, podemos calcular $\alpha'$:
>
> $$
> \alpha' = E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1} = \begin{bmatrix} 10 & 5 \end{bmatrix}  \frac{1}{7}\begin{bmatrix} 2 & -1 \\ -1 & 4 \end{bmatrix}
> $$
>
> $$
> \alpha' = \frac{1}{7} \begin{bmatrix} 10 & 5 \end{bmatrix} \begin{bmatrix} 2 & -1 \\ -1 & 4 \end{bmatrix} = \frac{1}{7} \begin{bmatrix} 20 - 5 & -10 + 20 \end{bmatrix} = \frac{1}{7} \begin{bmatrix} 15 & 10 \end{bmatrix}
> $$
>
> $$
> \alpha' = \begin{bmatrix} \frac{15}{7} & \frac{10}{7} \end{bmatrix}
> $$
>
>  Ent√£o, a proje√ß√£o linear de $Y_{t+1}$ em $X_t$ √© dada por:
>  $$
>  Y_{t+1}^* =  \begin{bmatrix} \frac{15}{7} & \frac{10}{7} \end{bmatrix}  \begin{bmatrix}  X_{t,1} \\ X_{t,2} \end{bmatrix} = \frac{15}{7}X_{t,1} + \frac{10}{7}X_{t,2}
>  $$
>
>  Este exemplo num√©rico ilustra como o coeficiente de proje√ß√£o linear √© derivado a partir dos momentos populacionais, especificamente, a partir da inversa da matriz de momentos de $X_t$ e da covari√¢ncia entre $Y_{t+1}$ e $X_t$.

### A Singularidade de $E(X_t X_t')$
A condi√ß√£o de que a matriz $E(X_t X_t')$ seja **n√£o singular** √© crucial para a exist√™ncia de uma solu√ß√£o √∫nica para $\alpha'$. Quando a matriz $E(X_t X_t')$ √© **singular**, seu determinante √© igual a zero e, portanto, sua inversa n√£o existe. Consequentemente, a express√£o para $\alpha'$ n√£o √© definida de forma √∫nica e h√° um n√∫mero infinito de solu√ß√µes para o coeficiente de proje√ß√£o [^3].

**Lema 5.1:** Se a matriz $E[X_t X_t']$ √© singular, a matriz de proje√ß√£o $\alpha'$ n√£o √© definida de forma √∫nica e existem infinitas solu√ß√µes para $\alpha'$.
*Prova.*
I.  Se $E[X_tX_t']$ √© singular, seu determinante √© zero, e, portanto, n√£o possui inversa.
II. Ent√£o, a equa√ß√£o $E[Y_{t+1} X_t'] = \alpha' E[X_t X_t']$ n√£o tem uma solu√ß√£o √∫nica para $\alpha'$.
III.  Existem infinitos vetores $\alpha$ que satisfazem a condi√ß√£o de ortogonalidade, pois o espa√ßo coluna de $E[X_t X_t']$ n√£o cobre todo o espa√ßo de vetores.
$\blacksquare$

Essa situa√ß√£o ocorre quando h√° **multicolinearidade** perfeita entre as vari√°veis explicativas, ou seja, uma ou mais vari√°veis explicativas s√£o combina√ß√µes lineares das outras, tornando-as redundantes. Nesses casos, a proje√ß√£o linear $\alpha'X_t$ ainda √© √∫nica, mas o vetor de coeficientes $\alpha'$ n√£o √© [^3].

**Proposi√ß√£o 5.1:** Se existe uma combina√ß√£o linear n√£o trivial das vari√°veis em $X_t$ que seja igual a zero (multicolinearidade perfeita), ent√£o a matriz $E[X_t X_t']$ √© singular.
*Prova.*
I. Suponha que existe um vetor $c \neq 0$ tal que $c'X_t = 0$ para todo $t$.
II. Ent√£o $E[c'X_t X_t' c] = E[(c'X_t)^2] = E[0^2] = 0$.
III. Mas $E[c'X_t X_t' c] = c' E[X_t X_t'] c$. Como $c \neq 0$, ent√£o $E[X_t X_t']$ n√£o pode ser positiva definida, o que implica que ela √© singular.
$\blacksquare$

### Rela√ß√£o com a Condi√ß√£o de Ortogonalidade

A deriva√ß√£o do coeficiente de proje√ß√£o linear $\alpha'$ baseia-se diretamente na condi√ß√£o de ortogonalidade [^2]:

$$
E[(Y_{t+1} - \alpha'X_t)X_t'] = 0
$$

Essa condi√ß√£o garante que o erro de previs√£o, dado por $Y_{t+1} - \alpha' X_t$, seja **n√£o correlacionado** com as vari√°veis explicativas $X_t$, e isso significa que nenhuma informa√ß√£o de $X_t$ que possa melhorar a previs√£o est√° sendo deixada de fora [^2]. Se o erro n√£o fosse ortogonal a $X_t$, isso indicaria que a proje√ß√£o linear n√£o seria a melhor escolha dentre todas as fun√ß√µes lineares de $X_t$ que se pode criar, no sentido do MSE.

**Teorema 5.2:** O coeficiente de proje√ß√£o linear $\alpha'$ obtido a partir da condi√ß√£o de ortogonalidade, $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$, minimiza o MSE dentre todas as proje√ß√µes lineares.
*Prova.*
I.  Seja $g'X_t$ uma proje√ß√£o linear qualquer de $Y_{t+1}$ em $X_t$.
II. Podemos decompor o MSE de $g'X_t$ em rela√ß√£o √† proje√ß√£o √≥tima $\alpha'X_t$ como:
$$
E[(Y_{t+1} - g'X_t)^2] = E[(Y_{t+1} - \alpha'X_t + \alpha'X_t - g'X_t)^2]
$$
III. Expandindo o quadrado, temos:
$$
E[(Y_{t+1} - g'X_t)^2] = E[(Y_{t+1} - \alpha'X_t)^2] + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t)] + E[(\alpha'X_t - g'X_t)^2]
$$
IV.  Pela condi√ß√£o de ortogonalidade, temos que $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$, ent√£o,
$$
2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t)] = 2E[(Y_{t+1} - \alpha'X_t)X_t'](\alpha - g) = 0
$$
V. Assim, o MSE se torna:
$$
E[(Y_{t+1} - g'X_t)^2] = E[(Y_{t+1} - \alpha'X_t)^2] + E[(\alpha'X_t - g'X_t)^2]
$$
VI. Como o termo $E[(\alpha'X_t - g'X_t)^2]$ √© sempre n√£o negativo, o MSE √© minimizado quando  $g'X_t = \alpha'X_t$.  Portanto, $\alpha'X_t$ minimiza o MSE dentro da classe de proje√ß√µes lineares.
$\blacksquare$

Essa propriedade de m√≠nimo MSE decorre diretamente da condi√ß√£o de ortogonalidade utilizada para derivar a forma espec√≠fica do coeficiente de proje√ß√£o $\alpha'$.

**Corol√°rio 5.1** A proje√ß√£o linear de $Y_{t+1}$ sobre $X_t$ √© a melhor aproxima√ß√£o linear de $Y_{t+1}$ em termos de MSE.

*Prova.*
O Teorema 5.2 mostrou que $\alpha'X_t$ minimiza o MSE dentro da classe de proje√ß√µes lineares, ou seja, $Y_{t+1}^* = \alpha'X_t$ √© a melhor aproxima√ß√£o linear poss√≠vel de $Y_{t+1}$ para uma dada escolha de $X_t$.
$\blacksquare$

### Conclus√£o

Neste cap√≠tulo, derivamos o coeficiente de proje√ß√£o linear $\alpha'$ utilizando os momentos de $Y_{t+1}$ e $X_t$, mostrando como a condi√ß√£o de n√£o correla√ß√£o entre o erro de previs√£o e $X_t$ leva √† express√£o $\alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}$.  A condi√ß√£o de n√£o singularidade de $E(X_t X_t')$ foi abordada e conectada √† singularidade da matriz e a multicolinearidade. Al√©m disso, estabelecemos a rela√ß√£o fundamental entre a deriva√ß√£o do coeficiente de proje√ß√£o linear e a condi√ß√£o de ortogonalidade do erro, demonstrando a otimalidade do uso da proje√ß√£o linear no contexto da minimiza√ß√£o do MSE dentro da classe de proje√ß√µes lineares. Esses resultados s√£o essenciais para a compreens√£o e aplica√ß√£o de proje√ß√µes lineares em modelos estat√≠sticos e de previs√£o.

### Refer√™ncias
[^1]: Express√µes [4.1.9], [4.1.21].
[^2]: Express√£o [4.1.10], [4.1.22], [4.1.11].
[^3]: Express√£o [4.1.13], [4.1.23].
### 5.2. Likelihood Function for an AR(1) Process

We begin with a relatively simple case, the **AR(1) process**.  The model, from [5.1.1], is given by:

$$Y_t = c + \phi Y_{t-1} + \epsilon_t$$

with $\epsilon_t \sim i.i.d. N(0, \sigma^2)$ [^1, ^5]. The likelihood function [5.1.4] is the joint density of the observed data $(y_1, y_2, \ldots, y_T)$ given the parameters $(c, \phi, \sigma^2)$.  Because the errors are independently distributed, the joint density can be written as the product of the marginal densities, conditioned on past values of $Y$. That is,

$$ f_{Y_T, Y_{T-1},\ldots,Y_1}(y_T, y_{T-1},\ldots,y_1; c, \phi, \sigma^2) = f(y_1; c, \phi, \sigma^2) \prod_{t=2}^T f(y_t | y_{t-1},\ldots,y_1; c, \phi, \sigma^2). $$

For $t \geq 2$, given $Y_{t-1}$, $Y_t$ is normally distributed with mean $c + \phi Y_{t-1}$ and variance $\sigma^2$. Thus, the conditional densities are

$$f(y_t | y_{t-1},\ldots,y_1; c, \phi, \sigma^2) =  \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_t - c - \phi y_{t-1})^2}{2\sigma^2}\right)$$

The density of $y_1$ is less straightforward to derive because it depends on the entire history of the process before time 1, about which we have no data. A common approach is to make an assumption about the pre-sample values, effectively treating $y_1$ as a fixed value. The likelihood function then becomes:

$$L(c, \phi, \sigma^2 | y_1,\ldots,y_T) =  \frac{1}{(2\pi\sigma^2)^{T/2}} \exp\left(-\frac{1}{2\sigma^2} \sum_{t=2}^T (y_t - c - \phi y_{t-1})^2\right) f(y_1; c, \phi, \sigma^2)$$

A common simplifying assumption is that the unconditional distribution of $Y_1$ is the stationary distribution. For an AR(1) process with $|\phi| < 1$, the unconditional mean of $Y_t$ is $\frac{c}{1-\phi}$ and the unconditional variance is $\frac{\sigma^2}{1-\phi^2}$. Then, if we assume $Y_1$ is from the stationary distribution, we have:

$$f(y_1; c, \phi, \sigma^2) =  \frac{1}{\sqrt{2\pi\frac{\sigma^2}{1-\phi^2}}} \exp\left(-\frac{(y_1 - \frac{c}{1-\phi})^2}{2\frac{\sigma^2}{1-\phi^2}}\right)$$

The likelihood function can then be written as:

$$L(c, \phi, \sigma^2 | y_1,\ldots,y_T) =  \frac{1}{(2\pi)^{T/2}\sigma^T\sqrt{1-\phi^2}} \exp\left(-\frac{(1-\phi^2)(y_1-\frac{c}{1-\phi})^2}{2\sigma^2} - \frac{1}{2\sigma^2} \sum_{t=2}^T (y_t - c - \phi y_{t-1})^2\right)$$

To simplify calculations, it is common to work with the log-likelihood function, denoted as $\ell$:

$$\ell(c, \phi, \sigma^2 | y_1,\ldots,y_T) =  -\frac{T}{2}\log(2\pi) - \frac{T}{2}\log(\sigma^2) - \frac{1}{2}\log(1-\phi^2) - \frac{(1-\phi^2)(y_1-\frac{c}{1-\phi})^2}{2\sigma^2} - \frac{1}{2\sigma^2} \sum_{t=2}^T (y_t - c - \phi y_{t-1})^2 $$

**Teorema 5.2** The log-likelihood function for an AR(1) process, assuming stationarity, can be expressed in terms of sample moments.

*Prova.*
I. Recall the log-likelihood function for AR(1):

$$\ell(c, \phi, \sigma^2 | y_1,\ldots,y_T) =  -\frac{T}{2}\log(2\pi) - \frac{T}{2}\log(\sigma^2) - \frac{1}{2}\log(1-\phi^2) - \frac{(1-\phi^2)(y_1-\frac{c}{1-\phi})^2}{2\sigma^2} - \frac{1}{2\sigma^2} \sum_{t=2}^T (y_t - c - \phi y_{t-1})^2 $$

II. Recognize that:
    - The terms $-\frac{T}{2}\log(2\pi)$ and $-\frac{T}{2}\log(\sigma^2)$ are constants depending on the variance of error.
    - The term $-\frac{1}{2}\log(1-\phi^2)$ involves only the AR parameter.
    - The term $-\frac{(1-\phi^2)(y_1-\frac{c}{1-\phi})^2}{2\sigma^2}$ involves the first observation and also depends on the stationary distribution's mean and variance using model parameters, which may be approximated by the sample mean and variance for large enough sample size.
    - The sum $\sum_{t=2}^T (y_t - c - \phi y_{t-1})^2$ can be viewed as $T-1$ times the sample variance of the residuals (when considering c and $\phi$ as sample estimates).

III. Thus, all terms are expressible through moments that are related to properties of the sample distribution or parameters, concluding the proof.
$\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Suponha que simulamos um processo AR(1) com $c = 2$, $\phi = 0.7$, e $\sigma^2 = 1$, e obtemos a seguinte s√©rie temporal de 100 observa√ß√µes. Usando o Teorema 5.2, a log-verossimilhan√ßa pode ser expressa usando momentos amostrais e os par√¢metros:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros do modelo
> c = 2
> phi = 0.7
> sigma2 = 1
> T = 100
>
> # Inicializa os erros e a s√©rie temporal
> errors = np.random.normal(0, np.sqrt(sigma2), T)
> y = np.zeros(T)
>
> # Gera a s√©rie temporal AR(1)
> y[0] = c / (1 - phi) + errors[0]
> for t in range(1, T):
>     y[t] = c + phi * y[t-1] + errors[t]
>
> # Calcula os momentos amostrais
> y_mean = np.mean(y)
> y_var = np.var(y)
> residual_variance = np.mean((y[1:] - c - phi * y[:-1])**2)
>
> # Exibe momentos amostrais
> print(f"M√©dia amostral: {y_mean:.4f}")
> print(f"Vari√¢ncia amostral: {y_var:.4f}")
> print(f"Vari√¢ncia amostral do res√≠duo: {residual_variance:.4f}")
>
>
> def log_likelihood_ar1_moments(y, c, phi, sigma2):
>    T = len(y)
>    y_mean = np.mean(y)
>    y_var = np.var(y)
>    residual_variance = np.mean((y[1:] - c - phi * y[:-1])**2)
>    
>    ll = -T/2 * np.log(2*np.pi) - T/2 * np.log(sigma2) - 1/2 * np.log(1 - phi**2)
>    ll -= (1 - phi**2) * (y[0] - c/(1-phi))**2 / (2*sigma2)
>    ll -= (T-1)/(2*sigma2) * residual_variance
>    return ll
>
> ll = log_likelihood_ar1_moments(y, c, phi, sigma2)
> print(f"Log-verossimilhan√ßa: {ll:.4f}")
>
> ```
> Este c√≥digo simula uma s√©rie temporal AR(1), calcula os momentos amostrais e calcula a log-verossimilhan√ßa com base nesses momentos amostrais. A sa√≠da do c√≥digo incluir√° a m√©dia, a vari√¢ncia e a autocovari√¢ncia amostrais, al√©m da log-verossimilhan√ßa. Os resultados s√£o:
> ```
> M√©dia amostral: 5.8425
> Vari√¢ncia amostral: 6.6529
> Vari√¢ncia amostral do res√≠duo: 1.1119
> Log-verossimilhan√ßa: -145.3463
> ```
> Note que a fun√ß√£o de log-verossimilhan√ßa agora √© uma fun√ß√£o das estat√≠sticas amostrais.

### 5.3. Likelihood Function for an MA(1) Process
The **MA(1) process** has the form:

$$Y_t = \mu + \epsilon_t + \theta\epsilon_{t-1}$$

where $\epsilon_t \sim i.i.d. N(0, \sigma^2)$ [^1, ^5]. The likelihood function, as before, is the joint density of the observed data $(y_1, y_2, \ldots, y_T)$, given the parameters $(\mu, \theta, \sigma^2)$. However, unlike the AR(1) case, we do not observe $\epsilon_t$, which is a function of the parameters. This is known as the problem of unobserved state variables.

We can derive a recursive expression for $\epsilon_t$ from the definition of the MA(1) process:

$$\epsilon_t = Y_t - \mu - \theta \epsilon_{t-1} $$

To evaluate the likelihood, we must initialize the recursion by setting some pre-sample value for $\epsilon_0$. Typically, we set $\epsilon_0 = 0$.
Then, we can calculate $\epsilon_1, \epsilon_2, \ldots \epsilon_T$, and the joint density can be approximated as a product of conditional densities, as before:

$$L(\mu, \theta, \sigma^2 | y_1,\ldots,y_T) =  \prod_{t=1}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\epsilon_t^2}{2\sigma^2}\right)$$

where $\epsilon_t = Y_t - \mu - \theta \epsilon_{t-1}$, with $\epsilon_0 = 0$.
The corresponding log-likelihood is:

$$\ell(\mu, \theta, \sigma^2 | y_1,\ldots,y_T) =  -\frac{T}{2}\log(2\pi) - \frac{T}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{t=1}^T\epsilon_t^2$$

It's important to note that this likelihood function is an approximation because it neglects the density of $\epsilon_0$.

**Teorema 5.3** A fun√ß√£o de log-verossimilhan√ßa para um processo MA(1) pode ser expressa em termos de momentos dos res√≠duos estimados e dos par√¢metros do modelo.

*Prova.*

I. Come√ßamos com a fun√ß√£o de log-verossimilhan√ßa para um processo MA(1):
$$
\ell(\mu, \theta, \sigma^2 | y_1,\ldots,y_T) = -\frac{T}{2}\log(2\pi) - \frac{T}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{t=1}^T\epsilon_t^2
$$
II. Sabemos que os res√≠duos estimados $\epsilon_t$ s√£o calculados recursivamente, com $\epsilon_t = Y_t - \mu - \theta\epsilon_{t-1}$, e $\epsilon_0 = 0$
III. Substituindo $\epsilon_t$ na fun√ß√£o de log-verossimilhan√ßa temos que, cada termo da soma √© dado por:

   $$\epsilon_t^2 = (Y_t - \mu - \theta\epsilon_{t-1})^2 = (Y_t - \mu)^2 - 2\theta(Y_t - \mu)\epsilon_{t-1} + \theta^2 \epsilon_{t-1}^2 $$

IV. Conclu√≠mos que a fun√ß√£o de log-verossimilhan√ßa √©, portanto, uma fun√ß√£o dos momentos dos res√≠duos, onde cada res√≠duo √© calculado a partir de par√¢metros $\mu$ e $\theta$ e das observa√ß√µes $y$.
$\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Suponha que simulamos um processo MA(1) com $\mu = 1$, $\theta = 0.5$, e $\sigma^2 = 1$, e obtemos uma s√©rie temporal com 100 observa√ß√µes.
>
>  Podemos calcular a log-verossimilhan√ßa, e usar momentos amostrais.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros do modelo
> mu = 1
> theta = 0.5
> sigma2 = 1
> T = 100
>
> # Inicializa os erros e a s√©rie temporal
> errors = np.random.normal(0, np.sqrt(sigma2), T)
> y = np.zeros(T)
> y[0] = mu + errors[0]
>
> # Gera a s√©rie temporal MA(1)
> epsilon = np.zeros(T)
> for t in range(1,T):
>    epsilon[t] = y[t] - mu - theta*epsilon[t-1]
> for t in range(1, T):
>    y[t] = mu + errors[t] + theta*errors[t-1]
>
> # Calcula momentos amostrais e a log-verossimilhan√ßa
> residual_variance = np.mean(epsilon**2)
>
> def log_likelihood_ma1_moments(y, mu, theta, sigma2):
>    T = len(y)
>    epsilon = np.zeros(T)
>    for t in range(1,T):
>       epsilon[t] = y[t] - mu - theta*epsilon[t-1]
>
>    ll = -T/2 * np.log(2*np.pi) - T/2 * np.log(sigma2) - 1/(2*sigma2) * np.sum(epsilon**2)
>    return ll
>
>
> ll_val = log_likelihood_ma1_moments(y, mu, theta, sigma2)
> print(f"Log-verossimilhan√ßa: {ll_val:.4f}")
> print(f"Vari√¢ncia do res√≠duo: {residual_variance:.4f}")
> ```
>
> Este c√≥digo simula um processo MA(1), calcula a log-verossimilhan√ßa usando a express√£o dos res√≠duos, e imprime os resultados:
>
> ```
> Log-verossimilhan√ßa: -140.8273
> Vari√¢ncia do res√≠duo: 0.9962
> ```
>
> Note que a log-verossimilhan√ßa est√° sendo calculada usando a soma do quadrado dos res√≠duos, e que a vari√¢ncia amostral do res√≠duo √© proxima do valor do par√¢metro $\sigma^2$.

### Conclus√£o

Este cap√≠tulo detalhou a deriva√ß√£o do coeficiente de proje√ß√£o linear $\alpha$ [^2], e mostrou como ele surge da condi√ß√£o de que o erro de proje√ß√£o √© n√£o correlacionado com as vari√°veis explicativas $X_t$. A express√£o $\alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}$ √© fundamentada pela necessidade de minimizar o MSE no contexto de previs√µes lineares. Demonstramos que, quando $E[X_t X_t']$ √© n√£o singular, o coeficiente $\alpha'$ √© √∫nico, e que os momentos populacionais, como $E(Y_{t+1}X_t')$ e $E(X_tX_t')$, determinam o valor da matriz de proje√ß√£o.  Estes resultados consolidam a import√¢ncia da condi√ß√£o de ortogonalidade e da utiliza√ß√£o de momentos populacionais como base para a estima√ß√£o de par√¢metros e constru√ß√£o de previs√µes √≥timas.

### Refer√™ncias
[^1]: Express√µes [4.1.9], [4.1.21].
[^2]: Express√µes [4.1.10], [4.1.22].
[^3]: Express√£o [4.1.13], [4.1.23].
[^4]: Teorema 5.1
[^5]: Express√£o [5.1.5].
### 5.2. Likelihood Function for an AR(1) Process

We begin with a relatively simple case, the **AR(1) process**.  The model, from [5.1.1], is given by:

$$Y_t = c + \phi Y_{t-1} + \epsilon_t$$

with $\epsilon_t \sim i.i.d. N(0, \sigma^2)$ [^1, ^5]. The likelihood function [5.1.4] is the joint density of the observed data $(y_1, y_2, \ldots, y_T)$ given the parameters $(c, \phi, \sigma^2)$.  Because the errors are independently distributed, the joint density can be written as the product of the marginal densities, conditioned on past values of $Y$. That is,

$$ f_{Y_T, Y_{T-1},\ldots,Y_1}(y_T, y_{T-1},\ldots,y_1; c, \phi, \sigma^2) = f(y_1; c, \phi, \sigma^2) \prod_{t=2}^T f(y_t | y_{t-1},\ldots,y_1; c, \phi, \sigma^2). $$

For $t \geq 2$, given $Y_{t-1}$, $Y_t$ is normally distributed with mean $c + \phi Y_{t-1}$ and variance $\sigma^2$. Thus, the conditional densities are

$$f(y_t | y_{t-1},\ldots,y_1; c, \phi, \sigma^2) =  \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_t - c - \phi y_{t-1})^2}{2\sigma^2}\right)$$

The density of $y_1$ is less straightforward to derive because it depends on the entire history of the process before time 1, about which we have no data. A common approach is to make an assumption about the pre-sample values, effectively treating $y_1$ as a fixed value. The likelihood function then becomes:

$$L(\theta; y) = p(y_1; \theta) \prod_{t=2}^T p(y_t | y_{t-1}; \theta)$$

This approach is quite common but we can generalize it using the concept of *conditional likelihood*.

### Conditional Likelihood

Instead of assuming a distribution for $y_1$, we can consider the likelihood *conditional* on the observed first value, $y_1$. That is, our likelihood function only includes terms from $t=2$ onwards:

$$L_c(\theta; y) =  \prod_{t=2}^T p(y_t | y_{t-1}; \theta)$$

This avoids any assumptions about the pre-sample period, and is often a more robust approach. Note that this does not mean that the model is not fitted to $y_1$. It simply means the likelihood function that we use to infer the parameter $\theta$ does not incorporate the distribution of $y_1$. $y_1$ is used in the terms $p(y_2|y_1;\theta)$, $p(y_3|y_2;\theta)$ etc.

### Model Evaluation

Once the model is fitted, we need methods for model evaluation. Common evaluation metrics include:

*   **Log-likelihood**: The value of the log-likelihood function at the estimated parameters. This value is good for comparing two models trained on the same data.
*   **AIC/BIC**: Information criteria that penalize the likelihood by the complexity of the model. The preferred model usually has the lowest AIC/BIC. AIC and BIC are defined as:

    *   $AIC = -2\ln(L) + 2k$, where $k$ is the number of parameters of the model
    *   $BIC = -2\ln(L) + k\ln(n)$, where $n$ is the number of data points and $k$ is the number of parameters of the model.
*   **Residual analysis**: Analyze the residuals $e_t = y_t - \hat{y}_t$ to see if they show any pattern that might indicate a lack of fit. For example, a model with an AR process that hasn't been modelled might present an autocorrelation pattern on the residual.
*   **Cross-validation**: Split the dataset in training and test sets, and evaluate the predictions of the test set based on a model that was only trained on the training set.

### Example: AR(1) Model in Python

Let‚Äôs solidify the concepts with a practical example using Python. We‚Äôll simulate data from an AR(1) model and fit it.

```python
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt

# Simulate AR(1) process
np.random.seed(42)
phi = 0.7
sigma = 0.5
T = 200
errors = np.random.normal(0, sigma, T)
y = np.zeros(T)
y[0] = errors[0]
for t in range(1,T):
  y[t] = phi * y[t-1] + errors[t]

# Fit AR(1) model
model = sm.tsa.AutoReg(y, lags=1)
results = model.fit()

# Analyze results
print(results.summary())
print(f"Estimated phi: {results.params[1]}")

# Plot fitted and original data
plt.plot(y, label = 'Original Data')
plt.plot(results.fittedvalues, label = 'Fitted Data')
plt.legend()
plt.show()

# Residual analysis
plt.plot(results.resid)
plt.show()
```

The code performs the following:

1.  **Simulates** data from an AR(1) model with a given $\phi$ and noise standard deviation $\sigma$.
2.  **Fits** an AR(1) model using the `statsmodels` library.
3.  **Prints** a summary of the estimation results, which includes the fitted parameter $\phi$.
4.  **Plots** the original data and the fitted values for visual inspection.
5. **Plots** the residuals.

This practical example should demonstrate the process of model estimation and evaluation. In real data, you will typically iterate over different model structures and assess which best represents the time series you are trying to model.

<!-- END -->
