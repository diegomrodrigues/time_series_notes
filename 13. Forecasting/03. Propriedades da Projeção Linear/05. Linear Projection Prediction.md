## ProjeÃ§Ã£o Linear e a Propriedade de MÃ­nimo Erro QuadrÃ¡tico MÃ©dio (MSE) para CombinaÃ§Ãµes Lineares

### IntroduÃ§Ã£o
Este capÃ­tulo aprofunda as propriedades da **projeÃ§Ã£o linear** no contexto de **previsÃµes de sÃ©ries temporais**, com foco especial na demonstraÃ§Ã£o de que a projeÃ§Ã£o linear $Y_{t+1}^*$ nÃ£o sÃ³ minimiza o MSE para $Y_{t+1}$, mas tambÃ©m fornece a previsÃ£o de MSE mÃ­nimo para qualquer combinaÃ§Ã£o linear dos elementos de $Y_{t+1}$, dado $X_t$ [^1]. Este resultado consolida a importÃ¢ncia da projeÃ§Ã£o linear como uma ferramenta fundamental para a previsÃ£o. Ao demonstrar que a projeÃ§Ã£o linear oferece a melhor previsÃ£o, no sentido do MSE, tanto para as variÃ¡veis originais como para qualquer combinaÃ§Ã£o linear dessas variÃ¡veis, estabelecemos um resultado bastante geral e robusto.

### Conceitos Fundamentais
Retomando conceitos anteriores, a projeÃ§Ã£o linear de um vetor $Y_{t+1}$ em um vetor de variÃ¡veis explicativas $X_t$ Ã© dada por:
$$
Y_{t+1}^* = \alpha' X_t,
$$
onde $\alpha'$ Ã© a matriz de coeficientes de projeÃ§Ã£o que minimiza o erro quadrÃ¡tico mÃ©dio (MSE) [^2]. A condiÃ§Ã£o que define $\alpha'$ Ã© que o erro de previsÃ£o $Y_{t+1} - Y_{t+1}^*$ seja **nÃ£o correlacionado** com $X_t$ [^2]:

$$
E[(Y_{t+1} - \alpha' X_t)X_t'] = 0.
$$
Essa condiÃ§Ã£o leva Ã  expressÃ£o para a matriz de projeÃ§Ã£o [^3]:

$$
\alpha' = E[Y_{t+1} X_t'] [E(X_t X_t')]^{-1}.
$$
Agora, vamos generalizar esse resultado para combinaÃ§Ãµes lineares de elementos de $Y_{t+1}$. Considere um vetor $Z_{t+1}$ que Ã© uma combinaÃ§Ã£o linear dos elementos de $Y_{t+1}$ dado por $Z_{t+1} = H'Y_{t+1}$, onde $H$ Ã© uma matriz de constantes. A nossa meta Ã© mostrar que a projeÃ§Ã£o linear de $Z_{t+1}$ em $X_t$, denotada por $Z_{t+1}^*$, Ã© dada por $H'Y_{t+1}^*$ e tambÃ©m minimiza o MSE para $Z_{t+1}$, ou seja, $Z_{t+1}^* =  H' \alpha' X_t$.

**Teorema 3.1:** A projeÃ§Ã£o linear $Y_{t+1}^*$ fornece a previsÃ£o de MSE mÃ­nimo para qualquer combinaÃ§Ã£o linear dos elementos de $Y_{t+1}$ dado $X_t$.
*Prova.*
I. Seja $Z_{t+1} = H'Y_{t+1}$ uma combinaÃ§Ã£o linear dos elementos de $Y_{t+1}$, onde $H$ Ã© uma matriz de constantes.
II. Queremos mostrar que a projeÃ§Ã£o linear de $Z_{t+1}$ em $X_t$, denotada por $Z_{t+1}^*$, Ã© dada por $H'Y_{t+1}^*$, onde $Y_{t+1}^*$ Ã© a projeÃ§Ã£o linear de $Y_{t+1}$ em $X_t$.
III. Primeiro, note que $Z_{t+1}^* = H' Y_{t+1}^*$ Ã© uma funÃ§Ã£o linear de $X_t$.
IV. Para demonstrar que $Z_{t+1}^* = H' \alpha' X_t$ Ã© a projeÃ§Ã£o linear de $Z_{t+1}$ em $X_t$, precisamos provar que o erro de previsÃ£o $(Z_{t+1} - Z_{t+1}^*)$ Ã© nÃ£o correlacionado com $X_t$. Ou seja:
$$
E[(Z_{t+1} - Z_{t+1}^*)X_t'] = 0.
$$
V. Substituindo $Z_{t+1} = H'Y_{t+1}$ e $Z_{t+1}^* = H'Y_{t+1}^* = H' \alpha'X_t$, temos:
$$
E[(H'Y_{t+1} - H'\alpha' X_t)X_t'] = E[H'(Y_{t+1} - \alpha' X_t)X_t'].
$$
VI. Usando a propriedade de linearidade da esperanÃ§a, temos:
$$
H'E[(Y_{t+1} - \alpha' X_t)X_t'] = H' \cdot 0 = 0.
$$
VII. O termo $E[(Y_{t+1} - \alpha' X_t)X_t'] = 0$ porque $\alpha'X_t$ Ã© a projeÃ§Ã£o linear de $Y_{t+1}$ em $X_t$, e, portanto, o erro $(Y_{t+1} - \alpha'X_t)$ Ã© nÃ£o correlacionado com $X_t$.
VIII. Portanto, $Z_{t+1}^* = H' \alpha' X_t$ Ã© a projeÃ§Ã£o linear de $Z_{t+1}$ em $X_t$.
IX. Dado que $Z_{t+1}^* = H'Y_{t+1}^*$, onde $Y_{t+1}^*$ Ã© a projeÃ§Ã£o linear de $Y_{t+1}$ em $X_t$, entÃ£o  $Z_{t+1}^*$ minimiza o MSE de $Z_{t+1}$ dentro da classe das previsÃµes lineares de $X_t$.
$\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos supor que temos uma variÃ¡vel $Y_{t+1}$ representando o preÃ§o de uma aÃ§Ã£o e $X_t$ representando o volume de negociaÃ§Ã£o dessa aÃ§Ã£o no tempo $t$. ApÃ³s a anÃ¡lise, a projeÃ§Ã£o linear de $Y_{t+1}$ em $X_t$ Ã© dada por:
>
> $$Y_{t+1}^* = 2.5 + 0.05X_t$$
>
> Isso significa que, para cada unidade de aumento no volume de negociaÃ§Ã£o ($X_t$), o preÃ§o da aÃ§Ã£o ($Y_{t+1}$) tende a aumentar em 0.05, alÃ©m de um valor base de 2.5. Agora, imagine que estamos interessados em uma nova variÃ¡vel $Z_{t+1}$, que Ã© uma combinaÃ§Ã£o linear de $Y_{t+1}$, dada por $Z_{t+1} = 2Y_{t+1} - 1$. Usando o Teorema 3.1, a projeÃ§Ã£o linear de $Z_{t+1}$ em $X_t$ serÃ¡:
>
> $$Z_{t+1}^* = 2Y_{t+1}^* - 1 = 2(2.5 + 0.05X_t) - 1$$
> $$Z_{t+1}^* = 5 + 0.1X_t - 1 = 4 + 0.1X_t$$
>
> Isso demonstra que, para prever $Z_{t+1}$, aplicamos a mesma combinaÃ§Ã£o linear Ã  projeÃ§Ã£o de $Y_{t+1}$. Por exemplo, se o volume de negociaÃ§Ã£o $X_t$ for 100, entÃ£o:
>
>  $$Y_{t+1}^* = 2.5 + 0.05 \times 100 = 7.5$$
>
>  $$Z_{t+1}^* = 4 + 0.1 \times 100 = 14$$
>
> A projeÃ§Ã£o de $Z_{t+1}$ serÃ¡ 14 quando $X_t=100$. Este exemplo ilustra que a propriedade de mÃ­nimo MSE se mantÃ©m nÃ£o sÃ³ para a variÃ¡vel original, mas tambÃ©m para qualquer combinaÃ§Ã£o linear desta, simplificando a anÃ¡lise de diferentes transformaÃ§Ãµes de variÃ¡veis.

**CorolÃ¡rio 3.1:** Se $Y_{t+1}$ Ã© um escalar, entÃ£o para qualquer constante $a$ e $b$ a projeÃ§Ã£o linear de $(aY_{t+1}+b)$ em $X_t$ Ã© dada por $aP(Y_{t+1}|X_t) + b$, onde $P(Y_{t+1}|X_t)$ Ã© a projeÃ§Ã£o linear de $Y_{t+1}$ em $X_t$.

*Prova.*
I. Se $Y_{t+1}$ Ã© um escalar, a matriz $H$ Ã© um escalar $a$.
II. Seja $Z_{t+1} = aY_{t+1} + b$, usando o Teorema 3.1, sabemos que a projeÃ§Ã£o linear de $Z_{t+1}$ em $X_t$ Ã© dada por
$$Z_{t+1}^* = aP(Y_{t+1}|X_t) + b$$
III. Assim, para qualquer combinaÃ§Ã£o linear de $Y_{t+1}$, o resultado se mantÃ©m.
$\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Continuando o exemplo anterior, suponha que $Y_{t+1}$ representa o preÃ§o de uma aÃ§Ã£o, e a projeÃ§Ã£o linear de $Y_{t+1}$ em $X_t$ (volume de negociaÃ§Ã£o) seja:
>
>  $$P(Y_{t+1}|X_t) =  2.5 + 0.05X_t$$
>
> Agora, considere uma transformaÃ§Ã£o linear de $Y_{t+1}$:
>
> $$Z_{t+1} = 0.5Y_{t+1} + 10$$
>
> Pelo CorolÃ¡rio 3.1, a projeÃ§Ã£o linear de $Z_{t+1}$ em $X_t$ Ã©:
>
> $$Z_{t+1}^* = 0.5P(Y_{t+1}|X_t) + 10$$
> $$Z_{t+1}^* = 0.5(2.5 + 0.05X_t) + 10$$
> $$Z_{t+1}^* = 1.25 + 0.025X_t + 10$$
> $$Z_{t+1}^* = 11.25 + 0.025X_t$$
>
> Se $X_t = 100$, entÃ£o:
>
> $$Z_{t+1}^* = 11.25 + 0.025 \times 100 = 11.25 + 2.5 = 13.75$$
>
> Este exemplo numÃ©rico demonstra que, ao projetar uma combinaÃ§Ã£o linear de $Y_{t+1}$ em $X_t$, podemos simplesmente aplicar a mesma transformaÃ§Ã£o linear Ã  projeÃ§Ã£o de $Y_{t+1}$ em $X_t$. Essa propriedade facilita o cÃ¡lculo e anÃ¡lise de transformaÃ§Ãµes lineares das variÃ¡veis de interesse.

**Teorema 3.2** A projeÃ§Ã£o linear $Y_{t+1}^*$ tambÃ©m minimiza o MSE para qualquer combinaÃ§Ã£o linear dos elementos de $Y_{t+1}$ condicional a $X_t$ .

*Prova*
I.  Do Teorema 3.1, sabemos que $Z_{t+1}^* = H'Y_{t+1}^*$ Ã© a projeÃ§Ã£o linear de $Z_{t+1}$ em $X_t$, onde $Z_{t+1} = H'Y_{t+1}$.
II.  Pela definiÃ§Ã£o de projeÃ§Ã£o linear, o erro $Z_{t+1} - Z_{t+1}^*$ Ã© nÃ£o correlacionado com $X_t$, ou seja, $E[(Z_{t+1} - Z_{t+1}^*)X_t'] = 0$.
III.  A propriedade de nÃ£o correlaÃ§Ã£o implica que a covariÃ¢ncia entre o erro e $X_t$ Ã© zero, ou seja, $Cov(Z_{t+1} - Z_{t+1}^*, X_t) = 0$.
IV.  Para qualquer funÃ§Ã£o $g(X_t)$, tambÃ©m se verifica que $Cov(Z_{t+1} - Z_{t+1}^*, g(X_t)) = 0$, desde que $g(X_t)$ seja uma combinaÃ§Ã£o linear de $X_t$.
V.  A propriedade da projeÃ§Ã£o linear implica que o erro da projeÃ§Ã£o Ã© ortogonal a qualquer funÃ§Ã£o linear de $X_t$. Portanto, $E[(Z_{t+1} - Z_{t+1}^*)g(X_t)'] = 0$.
VI. Considere agora o MSE condicional $E[(Z_{t+1} - \hat{Z}_{t+1})^2 | X_t]$, onde $\hat{Z}_{t+1}$ Ã© um preditor condicional qualquer de $Z_{t+1}$ dado $X_t$. Podemos decompor o erro em relaÃ§Ã£o Ã  projeÃ§Ã£o linear $Z_{t+1}^*$ como:

$E[(Z_{t+1} - \hat{Z}_{t+1})^2 | X_t] = E[(Z_{t+1} - Z_{t+1}^* + Z_{t+1}^* - \hat{Z}_{t+1})^2 | X_t] $

$ = E[(Z_{t+1} - Z_{t+1}^*)^2|X_t] + E[(Z_{t+1}^* - \hat{Z}_{t+1})^2|X_t] + 2E[(Z_{t+1} - Z_{t+1}^*)(Z_{t+1}^* - \hat{Z}_{t+1})|X_t]$

VII. O Ãºltimo termo Ã© zero pois  $Z_{t+1}^* - \hat{Z}_{t+1}$ Ã© uma funÃ§Ã£o linear de $X_t$ e o erro da projeÃ§Ã£o $Z_{t+1} - Z_{t+1}^*$ Ã© ortogonal a qualquer funÃ§Ã£o linear de $X_t$. EntÃ£o:

$E[(Z_{t+1} - \hat{Z}_{t+1})^2 | X_t] =  E[(Z_{t+1} - Z_{t+1}^*)^2|X_t] + E[(Z_{t+1}^* - \hat{Z}_{t+1})^2|X_t]$

VIII. Como o termo $E[(Z_{t+1}^* - \hat{Z}_{t+1})^2|X_t]$ Ã© sempre nÃ£o negativo, o MSE condicional Ã© minimizado quando $Z_{t+1}^* = \hat{Z}_{t+1}$. Portanto, a projeÃ§Ã£o linear $Z_{t+1}^*$ minimiza o MSE condicional de $Z_{t+1}$ dado $X_t$.
IX. Como $Z_{t+1}^* = H'Y_{t+1}^*$ e  $Y_{t+1}^*$ Ã© a projeÃ§Ã£o linear de $Y_{t+1}$ em $X_t$, entÃ£o a projeÃ§Ã£o linear $Y_{t+1}^*$ minimiza o MSE condicional para qualquer combinaÃ§Ã£o linear de $Y_{t+1}$.

$\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que a projeÃ§Ã£o linear de $Y_{t+1}$ em $X_t$ Ã© dada por $Y_{t+1}^* = 10 + 0.5X_t$. Queremos avaliar o MSE condicional para a previsÃ£o de uma variÃ¡vel transformada $Z_{t+1} = 2Y_{t+1} + 5$.
> Pelo Teorema 3.2, a projeÃ§Ã£o linear de $Z_{t+1}$ Ã© $Z_{t+1}^* = 2Y_{t+1}^* + 5 = 2(10 + 0.5X_t) + 5 = 25 + X_t$.
>
> Vamos calcular o MSE condicional para uma observaÃ§Ã£o onde $X_t = 20$:
>
> 1.  **ProjeÃ§Ã£o de** $Y_{t+1}$: $Y_{t+1}^* = 10 + 0.5 \times 20 = 20$
> 2.  **ProjeÃ§Ã£o de** $Z_{t+1}$: $Z_{t+1}^* = 25 + 20 = 45$
>
> Suponha que o valor real de $Y_{t+1}$ Ã© 22 e o valor real de $Z_{t+1} = 2 \times 22 + 5 = 49$.
>
> *   O erro na projeÃ§Ã£o de $Y_{t+1}$ Ã© $Y_{t+1} - Y_{t+1}^* = 22 - 20 = 2$.
> *   O erro na projeÃ§Ã£o de $Z_{t+1}$ Ã© $Z_{t+1} - Z_{t+1}^* = 49 - 45 = 4$.
>
> O MSE condicional Ã© dado por $E[(Z_{t+1} - Z_{t+1}^*)^2 | X_t]$. Em um caso geral, precisarÃ­amos de um conjunto de dados para calcular essa esperanÃ§a. Para este exemplo, vamos considerar que este seja o Ãºnico ponto de dados disponÃ­vel e usar o erro ao quadrado como uma estimativa do MSE condicional. Para $Z_{t+1}$, o MSE Ã© $4^2 = 16$.
>
> Qualquer outro preditor de $Z_{t+1}$ com base em $X_t$, digamos $\hat{Z}_{t+1} = 40 + 0.8X_t$, levaria a um MSE maior, como demonstrado pelo Teorema 3.2. Por exemplo, $\hat{Z}_{t+1} = 40 + 0.8 \times 20 = 56$. O erro seria $49-56 = -7$ e o MSE seria $(-7)^2=49$.  Este exemplo demonstra que a projeÃ§Ã£o linear $Z_{t+1}^*$ de fato minimiza o MSE condicional.

### ConclusÃ£o
Esta seÃ§Ã£o demonstrou que a projeÃ§Ã£o linear $Y_{t+1}^*$ nÃ£o apenas minimiza o MSE para $Y_{t+1}$, mas tambÃ©m fornece a previsÃ£o de MSE mÃ­nimo para qualquer combinaÃ§Ã£o linear dos elementos de $Y_{t+1}$, dado $X_t$ [^1]. O Teorema 3.1 formaliza este resultado atravÃ©s da demonstraÃ§Ã£o que a projeÃ§Ã£o de uma combinaÃ§Ã£o linear de $Y_{t+1}$ em $X_t$ Ã© dada pela mesma combinaÃ§Ã£o linear da projeÃ§Ã£o de $Y_{t+1}$ em $X_t$, mantendo a propriedade de ortogonalidade do erro. Este resultado Ã© de grande importÃ¢ncia prÃ¡tica e teÃ³rica, reforÃ§ando a aplicabilidade e a robustez da projeÃ§Ã£o linear em diversas situaÃ§Ãµes de previsÃ£o, mostrando que a projeÃ§Ã£o linear oferece previsÃµes Ã³timas para qualquer combinaÃ§Ã£o linear das variÃ¡veis originais com base no mesmo conjunto de variÃ¡veis explicativas. O Teorema 3.2 estende este resultado ao demonstrar que a projeÃ§Ã£o linear tambÃ©m minimiza o MSE condicional, o que reforÃ§a ainda mais a otimalidade do uso da projeÃ§Ã£o linear.

### ReferÃªncias
[^1]: ExpressÃµes [4.1.1], [4.1.4], [4.1.11].
[^2]: ExpressÃµes [4.1.9], [4.1.10], [4.1.13], [4.1.21], [4.1.22].
[^3]: ExpressÃµes [4.1.14], [4.1.23].
[^4]: CorolÃ¡rio 2.1
[^5]: ExpressÃ£o [5.1.5].
### 5.2. Likelihood Function for an AR(1) Process
Consider the AR(1) process:
$$Y_t = c + \phi Y_{t-1} + \epsilon_t$$
where $\epsilon_t \sim i.i.d. N(0, \sigma^2)$ [^5].  Our goal is to derive the likelihood function, given a sample of size $T$, denoted as $(y_1, y_2, \ldots, y_T)$. For simplicity, assume initially that the intercept $c=0$.  The joint density of the errors, given the parameters, is:
$$f(\epsilon_1, \epsilon_2, \ldots, \epsilon_T | \phi, \sigma^2) = \prod_{t=1}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\epsilon_t^2}{2\sigma^2}\right)$$
Since $Y_t = \phi Y_{t-1} + \epsilon_t$, it follows that $\epsilon_t = Y_t - \phi Y_{t-1}$.  Thus, we can write the likelihood function in terms of the observed data $Y_t$, as
$$L(\phi, \sigma^2 | y_1, y_2, \ldots, y_T) = \prod_{t=1}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_t - \phi y_{t-1})^2}{2\sigma^2}\right)$$
It is common practice to work with the log-likelihood function, as it simplifies calculations and does not change the location of the maximum. The log-likelihood is given by
$$log L(\phi, \sigma^2 | y_1, y_2, \ldots, y_T) = -\frac{T}{2}log(2\pi) - \frac{T}{2}log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=1}^T (y_t - \phi y_{t-1})^2$$
The above equation represents the likelihood function conditioned on $y_0$.  We will address the implications of conditioning on $y_0$ later, but for now it suffices. To find the MLE, one would need to maximize the above log-likelihood function with respect to $\phi$ and $\sigma^2$. This optimization is typically done numerically.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos supor que temos uma sÃ©rie temporal com 5 observaÃ§Ãµes: $y = [10, 12, 15, 13, 16]$. Queremos ajustar um modelo AR(1) com $c=0$. Usando a fÃ³rmula da log-verossimilhanÃ§a, temos:
>
>  $$log L(\phi, \sigma^2 | y) = -\frac{5}{2}log(2\pi) - \frac{5}{2}log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=1}^5 (y_t - \phi y_{t-1})^2$$
>
> Expandindo o somatÃ³rio:
>
> $$\sum_{t=1}^5 (y_t - \phi y_{t-1})^2 = (y_1 - \phi y_0)^2 + (y_2 - \phi y_1)^2 + (y_3 - \phi y_2)^2 + (y_4 - \phi y_3)^2 + (y_5 - \phi y_4)^2 $$
>
> Como $y_0$ nÃ£o estÃ¡ disponÃ­vel, usaremos $y_0=0$ para simplificar. Substituindo os valores temos:
>
> $$\sum_{t=1}^5 (y_t - \phi y_{t-1})^2 = (10 - \phi \cdot 0)^2 + (12 - \phi \cdot 10)^2 + (15 - \phi \cdot 12)^2 + (13 - \phi \cdot 15)^2 + (16 - \phi \cdot 13)^2 $$
>
> Para encontrar os valores de $\phi$ e $\sigma^2$ que maximizam essa funÃ§Ã£o de log-verossimilhanÃ§a, precisamos usar mÃ©todos numÃ©ricos. Usando Python com `scipy.optimize.minimize`, podemos fazer:
>
> ```python
> import numpy as np
> from scipy.optimize import minimize
>
> y = np.array([10, 12, 15, 13, 16])
>
> def neg_log_likelihood(params, data):
>     phi, sigma2 = params
>     T = len(data)
>     y_0 = 0 # Assuming y_0 = 0
>     sum_sq_errors = (data[0] - phi*y_0)**2
>     for t in range(1, T):
>        sum_sq_errors += (data[t] - phi*data[t-1])**2
>     return T/2 * np.log(2*np.pi) + T/2 * np.log(sigma2) + 1/(2*sigma2) * sum_sq_errors
>
> # Initial guess for parameters
> initial_params = [0.5, 1]
>
> # Optimization
> results = minimize(neg_log_likelihood, initial_params, args=(y,), method='L-BFGS-B', bounds=((-1, 1), (0.001, None)))
> best_phi, best_sigma2 = results.x
> print(f"MLE of phi: {best_phi:.4f}")
> print(f"MLE of sigma^2: {best_sigma2:.4f}")
> ```
>
> Este cÃ³digo busca os parÃ¢metros $\phi$ e $\sigma^2$ que minimizam o negativo da funÃ§Ã£o de log-verossimilhanÃ§a, que Ã© equivalente a maximizar a funÃ§Ã£o de log-verossimilhanÃ§a. A saÃ­da mostrarÃ¡ os melhores valores para $\phi$ e $\sigma^2$ que se ajustam aos dados. A restriÃ§Ã£o de $\phi$ estar entre -1 e 1 Ã© para garantir a estacionariedade do modelo AR(1).

**Lema 5.1** (Propriedades do Erro no Modelo AR(1))
Para o modelo AR(1) com $Y_t = \phi Y_{t-1} + \epsilon_t$, onde $\epsilon_t \sim i.i.d. N(0, \sigma^2)$, as seguintes propriedades se aplicam:
    I. $E[\epsilon_t] = 0$
    II. $Var(\epsilon_t) = \sigma^2$
    III. $Cov(\epsilon_t, \epsilon_s) = 0$ para $t \neq s$
    IV. $E[Y_t \epsilon_s] = 0$ para $s < t$

*Prova*

I.  A esperanÃ§a de $\epsilon_t$ Ã© zero pela definiÃ§Ã£o $\epsilon_t \sim N(0,\sigma^2)$.
II. A variÃ¢ncia de $\epsilon_t$ Ã© $\sigma^2$ pela definiÃ§Ã£o $\epsilon_t \sim N(0,\sigma^2)$.
III. A covariÃ¢ncia entre $\epsilon_t$ e $\epsilon_s$ Ã© zero para $t \ne s$ porque os erros sÃ£o independentes e identicamente distribuÃ­dos.
IV.  Para demonstrar $E[Y_t \epsilon_s] = 0$ para $s<t$, escrevemos $Y_t$ em funÃ§Ã£o dos erros passados:
$$Y_t = \epsilon_t + \phi \epsilon_{t-1} + \phi^2 \epsilon_{t-2} + \cdots $$
EntÃ£o,
$$E[Y_t \epsilon_s] = E[(\epsilon_t + \phi \epsilon_{t-1} + \phi^2 \epsilon_{t-2} + \cdots) \epsilon_s]$$
Como $\epsilon_t$ Ã© independente de $\epsilon_s$ para $t \ne s$, entÃ£o $E[\epsilon_t \epsilon_s] = 0$ para $t\ne s$. Para $t=s$, temos $E[\epsilon_t^2] = \sigma^2$. Mas no termo $E[Y_t \epsilon_s]$ temos que $s < t$, logo $E[\epsilon_t \epsilon_s]=0$, concluindo que $E[Y_t \epsilon_s]=0$.
$\blacksquare$

### 5.3. Likelihood Function for an AR(p) Process
The analysis for the AR(1) case can be extended to the AR(p) case:
$$Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t$$
where $\epsilon_t \sim i.i.d. N(0, \sigma^2)$[^5].  Again, assuming initially that $c = 0$, the error term can be expressed as:
$$\epsilon_t = Y_t - \phi_1 Y_{t-1} - \phi_2 Y_{t-2} - \ldots - \phi_p Y_{t-p}$$
Following the same approach as in the AR(1) case, the log-likelihood function, conditional on the first $p$ observations $(y_1, y_2, \ldots, y_p)$ is
$$log L(\phi_1, \phi_2, \ldots, \phi_p, \sigma^2 | y_1, y_2, \ldots, y_T) = -\frac{T-p}{2}log(2\pi) - \frac{T-p}{2}log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=p+1}^T (y_t - \phi_1 y_{t-1} - \ldots - \phi_p y_{t-p})^2$$
Note that the summation now starts at $t = p+1$, because the errors $\epsilon_1$ to $\epsilon_p$ are not defined by the model. The likelihood function is conditioned on the first $p$ observations. This difference between $T$ and $T-p$ becomes immaterial as $T$ becomes large. This expression can be maximized to obtain the MLE of the parameters ($\phi_1$, ..., $\phi_p$, $\sigma^2$). The optimization is typically carried out numerically.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que temos uma sÃ©rie temporal de 7 observaÃ§Ãµes: $y = [5, 7, 9, 12, 14, 16, 18]$, e queremos ajustar um modelo AR(2). Aqui, $p = 2$, e a funÃ§Ã£o de log-verossimilhanÃ§a Ã©:
>
>  $$log L(\phi_1, \phi_2, \sigma^2 | y) = -\frac{7-2}{2}log(2\pi) - \frac{7-2}{2}log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=3}^7 (y_t - \phi_1 y_{t-1} - \phi_2 y_{t-2})^2$$
>
> Expandindo o somatÃ³rio:
>  $$\sum_{t=3}^7 (y_t - \phi_1 y_{t-1} - \phi_2 y_{t-2})^2 = (y_3 - \phi_1 y_2 - \phi_2 y_1)^2 + (y_4 - \phi_1 y_3 - \phi_2 y_2)^2 + (y_5 - \phi_1 y_4 - \phi_2 y_3)^2 + (y_6 - \phi_1 y_5 - \phi_2 y_4)^2 + (y_7 - \phi_1 y_6 - \phi_2 y_5)^2 $$
>  Substituindo os valores:
>
> $$\sum_{t=3}^7 (y_t - \phi_1 y_{t-1} - \phi_2 y_{t-2})^2 = (9 - \phi_1 7 - \phi_2 5)^2 + (12 - \phi_1 9 - \phi_2 7)^2 + (14 - \phi_1 12 - \phi_2 9)^2 + (16 - \phi_1 14 - \phi_2 12)^2 + (18 - \phi_1 16 - \phi_2 14)^2 $$
>
> Para encontrar os valores de ($\phi_1, \phi_2, \sigma^2$) que maximizam essa log-verossimilhanÃ§a, podemos usar otimizaÃ§Ã£o numÃ©rica em Python:
>
> ```python
> import numpy as np
> from scipy.optimize import minimize
>
> y = np.array([5, 7, 9, 12, 14, 16, 18])
>
> def neg_log_likelihood(params, data):
>     phi1, phi2, sigma2 = params
>     T = len(data)
>     sum_sq_errors = 0
>     for t in range(2, T):
>         sum_sq_errors += (data[t] - phi1*data[t-1] - phi2*data[t-2])**2
>     return (T-2)/2 * np.log(2*np.pi) + (T-2)/2 * np.log(sigma2) + 1/(2*sigma2) * sum_sq_errors
>
> # Initial guess for parameters
> initial_params = [0.5, 0.2, 1]
>
> # Optimization
> results = minimize(neg_log_likelihood, initial_params, args=(y,), method='L-BFGS-B', bounds=((-1, 1),(-1, 1), (0.001, None)))
> best_phi1, best_phi2, best_sigma2 = results.x
> print(f"MLE of phi_1: {best_phi1:.4f}")
> print(f"MLE of phi_2: {best_phi2:.4f}")
> print(f"MLE of sigma^2: {best_sigma2:.4f}")
> ```
>
> Este cÃ³digo calcula os MLE dos parÃ¢metros do modelo AR(2). A funÃ§Ã£o de log-verossimilhanÃ§a Ã© condicional aos dois primeiros valores da sÃ©rie. As restriÃ§Ãµes para $\phi_1$ e $\phi_2$ sÃ£o para garantir a estacionariedade do modelo AR(2).

**Lema 5.2** (Propriedades do Erro no Modelo AR(p))
Para o modelo AR(p) com $Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t$, onde $\epsilon_t \sim i.i.d. N(0, \sigma^2)$, as seguintes propriedades se aplicam:
   I. $E[\epsilon_t] = 0$
   II. $Var(\epsilon_t) = \sigma^2$
   III. $Cov(\epsilon_t, \epsilon_s) = 0$ para $t \neq s$
   IV. $E[Y_t \epsilon_s] = 0$ para $s<t$.

*Prova*

A prova Ã© anÃ¡loga ao Lema 5.1, pois o termo do erro $\epsilon_t$ Ã© independente de todos os erros passados, e, portanto, todas as propriedades se mantÃ©m.

$\blacksquare$

### 5.4. Likelihood Function for an MA(1) Process
Now consider the moving average MA(1) process:
$$Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$$
where $\epsilon_t \sim i.i.d. N(0, \sigma^2)$[^5].  Again, assuming initially that $\mu = 0$,  the likelihood function for the MA(1) is more challenging due to the fact that theerror terms are autocorrelated and are not directly observed.

**Derivation of the Likelihood Function for MA(1)**

To derive the likelihood function, we express $\epsilon_t$ as a function of the observed $y_t$ and the model parameters. From the model equation we have:

$$\epsilon_t = y_t - \theta \epsilon_{t-1}$$

For $t=1$, we have

$$\epsilon_1 = y_1 - \theta \epsilon_0$$

where we need to determine $\epsilon_0$. We assume that the process has been ongoing for a long time, and thus the initial value for epsilon, $\epsilon_0$, can be handled with one of several methods:

1.  **Setting** $\epsilon_0 = 0$: This is a common simplification, especially for long time series. The effect of this initial assumption diminishes over time.

2.  **Backcasting:** Using iterative computation backward in time to get an estimate for $\epsilon_0$.

3.  **Unconditional Maximum Likelihood:** Choosing $\epsilon_0$ that maximizes the likelihood using the conditional expectation of $E[\epsilon_0|y_1,...]$.

For simplicity and common practice, let's proceed by setting $\epsilon_0 = 0$ which makes the first error term $\epsilon_1=y_1$ and subsequent error terms dependent on past error terms and $y$. The likelihood function can then be constructed by recognizing that the likelihood of the sequence of residuals given the model parameters ($\theta, \sigma^2$) can be expressed as the joint density of $\epsilon_t$. Because the errors are i.i.d. Gaussian, the joint density is just the product of the individual densities, and is given as:

$$L(\theta, \sigma^2|y_1, ..., y_T) = \prod_{t=1}^{T} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\epsilon_t^2}{2\sigma^2}\right)$$

Where $\epsilon_t$ is recursively defined as:

$$\epsilon_t = y_t - \theta \epsilon_{t-1} \text{ with } \epsilon_0 = 0$$

By substituting the expression for $\epsilon_t$ into the likelihood, we have a function of the parameters and data.

**Log-Likelihood Function**

The log-likelihood function simplifies the calculations:

$$l(\theta, \sigma^2|y_1, ..., y_T) = -\frac{T}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{t=1}^{T} \epsilon_t^2$$

Or,

$$l(\theta, \sigma^2|y_1, ..., y_T) = -\frac{T}{2}\log(2\pi) - \frac{T}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{t=1}^{T} \epsilon_t^2$$

This log-likelihood is then maximized with respect to $\theta$ and $\sigma^2$ to find the parameter estimates. Maximizing with respect to $\sigma^2$ by taking the derivative, setting to zero, and solving we get:

$$\hat{\sigma}^2 = \frac{1}{T} \sum_{t=1}^{T} \epsilon_t^2$$

which can then be substituted back to get a concentrated likelihood and used to maximize with respect to theta.  Maximizing with respect to $\theta$ does not have an analytical solution, and is usually done numerically.

**Estimation using Numerical Optimization**
Given the lack of closed-form solutions for the parameters, we typically resort to numerical optimization techniques to estimate $\theta$ and $\sigma^2$. Common methods include gradient descent, Newton-Raphson, and variants thereof. Software packages such as R or Python with libraries like `statsmodels` can be used to carry out this optimization for MA(1) models and report the best fit parameters and likelihoods.

<!-- END -->
