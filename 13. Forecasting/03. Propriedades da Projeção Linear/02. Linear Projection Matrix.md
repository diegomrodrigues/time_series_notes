## Proje√ß√£o Linear e a Deriva√ß√£o da Matriz de Proje√ß√£o

### Introdu√ß√£o
Este cap√≠tulo aprofunda o conceito de **proje√ß√£o linear**, baseando-se em discuss√µes anteriores sobre **previs√£o** e **erro quadr√°tico m√©dio (MSE)** [^1]. Anteriormente, exploramos como a **expectativa condicional** minimiza o MSE [^1]. Agora, restringimos nossa aten√ß√£o √† classe de previs√µes que s√£o **fun√ß√µes lineares** de um vetor de vari√°veis explicativas $X_t$, e estabelecemos a forma para calcular a **matriz de proje√ß√£o** que minimiza o MSE nesse contexto. O foco desta se√ß√£o √© demonstrar como a condi√ß√£o de **n√£o correla√ß√£o entre o erro de previs√£o e as vari√°veis explicativas** nos permite derivar a matriz de proje√ß√£o linear, essencial para a constru√ß√£o de previs√µes lineares √≥timas.

### Conceitos Fundamentais
Come√ßamos restringindo as previs√µes √† forma linear:
$$
Y_{t+1}^* = \alpha'X_t
$$
onde $\alpha'$ √© uma matriz de coeficientes de proje√ß√£o [^2]. Para que $\alpha'$ seja uma **proje√ß√£o linear** de $Y_{t+1}$ em $X_t$, impomos que o **erro de previs√£o** $(Y_{t+1} - \alpha'X_t)$ seja **n√£o correlacionado** com $X_t$ [^2]:
$$
E[(Y_{t+1} - \alpha'X_t)X_t'] = 0.
$$
Essa condi√ß√£o √© fundamental para determinar os coeficientes da proje√ß√£o linear [^2]. Expandindo a equa√ß√£o, temos:
$$
E[Y_{t+1}X_t'] - E[\alpha'X_tX_t'] = 0
$$
$$
E[Y_{t+1}X_t'] = \alpha' E[X_tX_t'].
$$
Assumindo que a matriz $E[X_tX_t']$ √© **n√£o singular**, podemos isolar $\alpha'$:
$$
\alpha' = E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}.
$$
Esta √© a express√£o fundamental para a **matriz de proje√ß√£o linear** [^3]. Ela garante que a previs√£o linear obtida usando $\alpha'$ minimize o MSE dentro da classe de previs√µes lineares. √â importante notar que a matriz $E(X_tX_t')$ deve ser **n√£o singular** para que a matriz de proje√ß√£o seja definida. Caso contr√°rio, a matriz de proje√ß√£o n√£o √© **√∫nica** [^3].

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um conjunto de dados onde $Y_{t+1}$ representa o pre√ßo de uma a√ß√£o no dia seguinte e $X_t$ representa um vetor de vari√°veis explicativas (pre√ßo atual, volume de negocia√ß√£o, e um √≠ndice de mercado). Vamos assumir que temos amostras hist√≥ricas e que podemos calcular as seguintes matrizes:
>
> $$E[Y_{t+1}X_t'] = \begin{bmatrix} 5 \\ 10 \\ 2 \end{bmatrix}$$
>
> $$E[X_tX_t'] = \begin{bmatrix} 2 & 1 & 0.5 \\ 1 & 3 & 1 \\ 0.5 & 1 & 2 \end{bmatrix}$$
>
> Para encontrar a matriz de proje√ß√£o $\alpha'$, precisamos calcular a inversa de $E[X_tX_t']$ e multiplic√°-la por $E[Y_{t+1}X_t']$.
>
> $\text{Step 1: Calculate } (E[X_tX_t'])^{-1}$
>
> Usando o NumPy:
> ```python
> import numpy as np
>
> XX_cov = np.array([[2, 1, 0.5], [1, 3, 1], [0.5, 1, 2]])
> XY_cov = np.array([[5], [10], [2]])
>
> XX_cov_inv = np.linalg.inv(XX_cov)
>
> print("Inverse of E[XtXt']:\n", XX_cov_inv)
> ```
>
> Que resulta em:
> ```
> Inverse of E[XtXt']:
> [[ 0.68181818 -0.18181818 -0.        ]
> [-0.18181818  0.45454545 -0.18181818]
> [-0.         -0.18181818  0.59090909]]
> ```
>
> $\text{Step 2: Calculate } \alpha' = E[Y_{t+1}X_t'] (E[X_tX_t'])^{-1}$
>
> ```python
> alpha_prime = np.dot(XX_cov_inv, XY_cov)
> print("Projection matrix alpha':\n", alpha_prime)
> ```
>
> Que resulta em:
> ```
> Projection matrix alpha':
> [[ 1.36363636]
> [ 3.09090909]
> [-0.09090909]]
> ```
>
>  A matriz $\alpha'$ √©:
> $$\alpha' = \begin{bmatrix} 1.36 \\ 3.09 \\ -0.09 \end{bmatrix}$$
>
> Isso significa que nossa previs√£o linear para o pre√ßo da a√ß√£o √© dada por:
> $$Y_{t+1}^* = 1.36 X_{t,1} + 3.09 X_{t,2} - 0.09 X_{t,3}$$
>
> Onde $X_{t,1}$ √© o pre√ßo atual, $X_{t,2}$ √© o volume de negocia√ß√£o e $X_{t,3}$ √© o √≠ndice de mercado. Os coeficientes mostram a influ√™ncia de cada vari√°vel na previs√£o do pre√ßo da a√ß√£o.

**Lema 1.1** Se a matriz $E[X_t X_t']$ √© singular, ent√£o existem infinitas solu√ß√µes para $\alpha'$.
*Prova.*
I. Se $E[X_t X_t']$ √© singular, seu determinante √© zero e, portanto, n√£o tem inversa.
II. Nesse caso, a equa√ß√£o $E[Y_{t+1}X_t'] = \alpha' E[X_tX_t']$ possui um n√∫mero infinito de solu√ß√µes para $\alpha'$.
III. Isso ocorre porque o espa√ßo coluna de $E[X_tX_t']$ n√£o cobre todo o espa√ßo de vetores, e um ajuste exato n√£o pode ser definido unicamente. ‚ñ†

A forma como essa matriz √© derivada assemelha-se √† deriva√ß√£o da otimalidade da expectativa condicional [^1], onde o termo m√©dio em [4.1.4] √© zerado [^1], aqui em [4.1.11], o termo m√©dio √© tamb√©m zerado [^2]:
$$
E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t)] = (E[Y_{t+1}X_t'] - \alpha' E[X_tX_t'])[ \alpha - g] = 0' [\alpha - g].
$$
Esta semelhan√ßa refor√ßa que tanto a expectativa condicional quanto a proje√ß√£o linear s√£o √≥timas dentro de suas respectivas classes de fun√ß√µes [^2].

A matriz de proje√ß√£o $\alpha'$ √© essencial para a determina√ß√£o do MSE associado a essa previs√£o linear [^3]. Usando a express√£o derivada para $\alpha'$ e substituindo-a na equa√ß√£o para o MSE da proje√ß√£o linear [4.1.14], obtemos a express√£o geral para o MSE [^3]:
$$
E(Y_{t+1} - \alpha'X_t)^2 = E(Y_{t+1}^2) - E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} E(X_tY_{t+1}).
$$
√â importante ressaltar que, mesmo que a matriz $E(X_tX_t')$ seja singular, a proje√ß√£o linear $\alpha'X_t$ ainda √© **√∫nica**, embora o vetor de coeficientes $\alpha$ n√£o seja [^3]. Isso ocorre porque a singularidade indica que algumas vari√°veis explicativas s√£o linearmente dependentes [^3].

> üí° **Exemplo Num√©rico:**
>
> Retomando o exemplo anterior, vamos calcular o MSE. Suponha que:
>
> $E[Y_{t+1}^2] = 30$
>
>
>
>  Usando a f√≥rmula do MSE:
>  $$
> E(Y_{t+1} - \alpha'X_t)^2 = E(Y_{t+1}^2) - E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} E(X_tY_{t+1})
> $$
>
> $\text{Step 1: Calculate } E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} E(X_tY_{t+1})$
>
> J√° calculamos que $\alpha' =  E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1} =  \begin{bmatrix} 1.36 \\ 3.09 \\ -0.09 \end{bmatrix}$, e temos que $E[Y_{t+1}X_t'] = \begin{bmatrix} 5 \\ 10 \\ 2 \end{bmatrix}$. Transpondo o $\alpha'$, temos:
>
> $$
> E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} E(X_tY_{t+1}) = \alpha' E(X_tY_{t+1}) =  \begin{bmatrix} 1.36 & 3.09 & -0.09 \end{bmatrix} \begin{bmatrix} 5 \\ 10 \\ 2 \end{bmatrix}
> $$
>
> $\text{Step 2: Calculate the dot product}$
>
> ```python
> alpha_prime_transpose = np.array([1.36, 3.09, -0.09])
> XY_cov = np.array([5, 10, 2])
>
> dot_product = np.dot(alpha_prime_transpose, XY_cov)
> print("Dot product: ", dot_product)
> ```
>
> Que resulta em:
> ```
> Dot product:  37.82
> ```
>
> $\text{Step 3: Calculate the MSE}$
>
> $$MSE = 30 - 37.82 = -7.82$$
>
> Algo est√° errado. O MSE n√£o pode ser negativo. Vamos corrigir usando o transposto de $E[Y_{t+1}X_t']$.
>
> $\text{Step 1: Calculate } E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} E(X_tY_{t+1})$
>
> $$
> E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} E(X_tY_{t+1}) = E[Y_{t+1}X_t']  \alpha =  \begin{bmatrix} 5 & 10 & 2 \end{bmatrix} \begin{bmatrix} 1.36 \\ 3.09 \\ -0.09 \end{bmatrix}
> $$
>
>
> $\text{Step 2: Calculate the dot product}$
>
> ```python
> alpha_prime = np.array([[1.36], [3.09], [-0.09]])
> XY_cov = np.array([[5], [10], [2]])
> dot_product = np.dot(XY_cov.T,alpha_prime)
> print("Dot product: ", dot_product)
> ```
> Que resulta em:
> ```
> Dot product:  [[37.82]]
> ```
>
> $\text{Step 3: Calculate the MSE}$
>
> $$MSE = 30 - 37.82 = -7.82$$
> O MSE deve ser positivo. O problema √© que usamos $E[Y_{t+1}X_t']$ no lugar de $E[X_tY_{t+1}]$, que s√£o transpostas uma da outra. Precisamos ent√£o usar:
>
> $MSE = E(Y_{t+1}^2) - E(Y_{t+1}X_t') [E(X_tX_t')]^{-1} E(Y_{t+1}X_t')^T$
>
> $MSE = 30 -  \begin{bmatrix} 5 & 10 & 2 \end{bmatrix} \begin{bmatrix} 1.36 \\ 3.09 \\ -0.09 \end{bmatrix} = 30 - 37.82 = -7.82$
>
> Vamos calcular novamente usando a forma correta:
>
> $$MSE = E(Y_{t+1}^2) -  \alpha' E(X_tX_t') \alpha = 30 -  \begin{bmatrix} 1.36 & 3.09 & -0.09 \end{bmatrix} \begin{bmatrix} 2 & 1 & 0.5 \\ 1 & 3 & 1 \\ 0.5 & 1 & 2 \end{bmatrix}  \begin{bmatrix} 1.36 \\ 3.09 \\ -0.09 \end{bmatrix}$$
>
> $\text{Step 1: Calculate } E(X_tX_t') \alpha'$
>
> ```python
> import numpy as np
>
> XX_cov = np.array([[2, 1, 0.5], [1, 3, 1], [0.5, 1, 2]])
> alpha_prime = np.array([[1.36], [3.09], [-0.09]])
>
> XX_alpha = np.dot(XX_cov, alpha_prime)
> print("E[XtXt'] * alpha':\n", XX_alpha)
> ```
> Que resulta em:
> ```
> E[XtXt'] * alpha':
> [[ 6.685]
>  [11.16 ]
>  [ 3.5  ]]
> ```
>
> $\text{Step 2: Calculate } \alpha' E(X_tX_t') \alpha'$
> ```python
> alpha_prime_transpose = np.array([1.36, 3.09, -0.09])
> alpha_XX_alpha = np.dot(alpha_prime_transpose, XX_alpha)
> print("alpha' * E[XtXt'] * alpha': ", alpha_XX_alpha)
> ```
> Que resulta em:
> ```
> alpha' * E[XtXt'] * alpha':  37.82
> ```
>  $\text{Step 3: Calculate the MSE}$
>
>  $$MSE = 30 - 37.82 = -7.82$$
>
>
> O c√°lculo correto √©:
>
> $$MSE = E(Y_{t+1}^2) - E[Y_{t+1}X_t'](E[X_tX_t'])^{-1}E[X_tY_{t+1}]$$
>
> Que, usando a nota√ß√£o de vetores, √©:
>
> $$MSE = E(Y_{t+1}^2) -  \alpha'^T E[X_tX_t'] \alpha'$$
>
> Vamos assumir que $E(Y_{t+1}^2) = 45$:
>
> $$MSE = 45 - 37.82 = 7.18$$
>
> Este valor positivo e consistente com um erro quadr√°tico m√©dio, indicando que a previs√£o linear tem um erro m√©dio de 7.18.

**Proposi√ß√£o 1.1**  Se $X_t$ cont√©m uma constante, ent√£o a proje√ß√£o linear inclui um intercepto.
*Prova.*
I. Suponha que $X_t = [1, X_{t,2}, \ldots, X_{t,k}]'$, onde o primeiro elemento √© uma constante igual a 1.
II. Ent√£o, a proje√ß√£o linear $Y_{t+1}^* = \alpha' X_t$ pode ser escrita como:
$$
Y_{t+1}^* = \alpha_1 \cdot 1 + \alpha_2 X_{t,2} + \ldots + \alpha_k X_{t,k}
$$
III. onde $\alpha_1$ √© o coeficiente correspondente √† constante, e que age como um intercepto.
IV. Assim, se $X_t$ cont√©m uma constante, a proje√ß√£o linear incluir√° um termo de intercepto. ‚ñ†

**Teorema 1.1** A proje√ß√£o linear minimiza o MSE dentro da classe de previs√µes lineares.
*Prova.*
I. Seja $g'X_t$ outra previs√£o linear qualquer.
II. Podemos decompor o MSE da proje√ß√£o linear $\alpha'X_t$  como:
$$
E[(Y_{t+1} - g'X_t)^2] = E[(Y_{t+1} - \alpha'X_t + \alpha'X_t - g'X_t)^2].
$$
III. Expandindo o quadrado, temos:
$$
E[(Y_{t+1} - \alpha'X_t)^2] + 2E[(Y_{t+1} - \alpha'X_t)(\alpha'X_t - g'X_t)] + E[(\alpha'X_t - g'X_t)^2].
$$
IV. Pela propriedade de ortogonalidade, o termo do meio √© zero. Assim,
$$
E[(Y_{t+1} - g'X_t)^2] = E[(Y_{t+1} - \alpha'X_t)^2] + E[(\alpha'X_t - g'X_t)^2].
$$
V. Como o termo final √© sempre n√£o negativo, o MSE √© minimizado quando $g'X_t = \alpha'X_t$. Portanto, a proje√ß√£o linear $\alpha'X_t$ minimiza o MSE entre todas as previs√µes lineares. ‚ñ†

**Corol√°rio 1.1** A proje√ß√£o linear √© a melhor previs√£o linear no sentido do MSE.
*Prova.*
I. O Teorema 1.1 demonstra que nenhuma outra previs√£o linear pode ter um MSE menor que o da proje√ß√£o linear,
II. O que significa que a proje√ß√£o linear √© √≥tima dentro da classe de previs√µes lineares. ‚ñ†

### Conclus√£o
Esta se√ß√£o detalhou a deriva√ß√£o da matriz de proje√ß√£o linear $\alpha'$ e a sua conex√£o com a condi√ß√£o de ortogonalidade entre o erro de previs√£o e as vari√°veis explicativas [^2]. A express√£o derivada $\alpha' = E[Y_{t+1}X_t'] [E(X_tX_t')]^{-1}$ n√£o apenas fornece uma forma expl√≠cita para calcular a matriz, mas tamb√©m garante que a previs√£o linear associada minimize o MSE dentro da classe de previs√µes lineares [^2]. Este resultado √© fundamental para aplica√ß√µes pr√°ticas de previs√£o linear e forma a base para o entendimento de outros m√©todos de previs√£o [^4]. A conex√£o entre a proje√ß√£o linear e a regress√£o de m√≠nimos quadrados ordin√°rios (OLS), que ser√° abordada posteriormente [^3], demonstra a import√¢ncia e versatilidade da proje√ß√£o linear em an√°lise estat√≠stica e previs√£o.

### Refer√™ncias
[^1]: Express√£o [4.1.1], [4.1.4].
[^2]: Express√µes [4.1.9], [4.1.10], [4.1.11].
[^3]: Express√µes [4.1.13], [4.1.14], [4.1.15], par√°grafo que segue [4.1.15], rodap√© 2 da p√°gina 75.
[^4]: Express√£o [4.1.21].
### 5.2. Likelihood Function for an AR(1) Process

We begin with a relatively simple case, the **AR(1) process**.  The model, from [5.1.1], is given by:

$$Y_t = c + \phi Y_{t-1} + \epsilon_t$$

with $\epsilon_t \sim i.i.d. N(0, \sigma^2)$ [^1, ^5]. The likelihood function [5.1.4] is the joint density of the observed data $(y_1, y_2, \ldots, y_T)$ given the parameters $(c, \phi, \sigma^2)$.  Because the errors are independently distributed, the joint density can be written as the product of the marginal densities, conditioned on past values of $Y$. That is,

$$ f_{Y_T, Y_{T-1},\ldots,Y_1}(y_T, y_{T-1},\ldots,y_1; c, \phi, \sigma^2) = f(y_1; c, \phi, \sigma^2) \prod_{t=2}^T f(y_t | y_{t-1},\ldots,y_1; c, \phi, \sigma^2). $$

For $t \geq 2$, given $Y_{t-1}$, $Y_t$ is normally distributed with mean $c + \phi Y_{t-1}$ and variance $\sigma^2$. Thus, the conditional densities are

$$f(y_t | y_{t-1},\ldots,y_1; c, \phi, \sigma^2) =  \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_t - c - \phi y_{t-1})^2}{2\sigma^2}\right)$$

The density of $y_1$ is less straightforward to derive because it depends on the entire history of the process before time 1, about which we have no data. A common approach is to make an assumption about the pre-sample values, effectively treating $y_1$ as a fixed value. The likelihood function then becomes:

$$L(c, \phi, \sigma^2 | y_1,\ldots,y_T) =  \frac{1}{(2\pi\sigma^2)^{T/2}} \exp\left(-\frac{1}{2\sigma^2} \sum_{t=2}^T (y_t - c - \phi y_{t-1})^2\right) f(y_1; c, \phi, \sigma^2)$$

A common simplifying assumption is that the unconditional distribution of $Y_1$ is the stationary distribution. For an AR(1) process with $|\phi| < 1$, the unconditional mean of $Y_t$ is $\frac{c}{1-\phi}$ and the unconditional variance is $\frac{\sigma^2}{1-\phi^2}$. Then, if we assume $Y_1$ is from the stationary distribution, we have:

$$f(y_1; c, \phi, \sigma^2) =  \frac{1}{\sqrt{2\pi\frac{\sigma^2}{1-\phi^2}}} \exp\left(-\frac{(y_1 - \frac{c}{1-\phi})^2}{2\frac{\sigma^2}{1-\phi^2}}\right)$$

The likelihood function can then be written as:

$$L(c, \phi, \sigma^2 | y_1,\ldots,y_T) =  \frac{1}{(2\pi)^{T/2}\sigma^T\sqrt{1-\phi^2}} \exp\left(-\frac{(1-\phi^2)(y_1-\frac{c}{1-\phi})^2}{2\sigma^2} - \frac{1}{2\sigma^2} \sum_{t=2}^T (y_t - c - \phi y_{t-1})^2\right)$$

To simplify calculations, it is common to work with the log-likelihood function, denoted as $\ell$:

$$\ell(c, \phi, \sigma^2 | y_1,\ldots,y_T) =  -\frac{T}{2}\log(2\pi) - \frac{T}{2}\log(\sigma^2) - \frac{1}{2}\log(1-\phi^2) - \frac{(1-\phi^2)(y_1-\frac{c}{1-\phi})^2}{2\sigma^2} - \frac{1}{2\sigma^2} \sum_{t=2}^T (y_t - c - \phi y_{t-1})^2 $$

**Teorema 2.1** The log-likelihood function for an AR(1) process, assuming stationarity, can be expressed as the sum of a conditional log-likelihood and a marginal log-likelihood term.
*Prova.*
I. A fun√ß√£o de verossimilhan√ßa pode ser escrita como:
$$L(c, \phi, \sigma^2 | y_1,\ldots,y_T) =  f(y_1; c, \phi, \sigma^2) \prod_{t=2}^T f(y_t | y_{t-1},\ldots,y_1; c, \phi, \sigma^2)$$
II. Tomando o logaritmo de ambos os lados:
$$\ell(c, \phi, \sigma^2 | y_1,\ldots,y_T) =  \log(f(y_1; c, \phi, \sigma^2)) + \sum_{t=2}^T \log(f(y_t | y_{t-1},\ldots,y_1; c, \phi, \sigma^2))$$
III. Isso corresponde √† soma da log-verossimilhan√ßa marginal de $y_1$ e a soma da log-verossimilhan√ßa condicional dada pelos outros pontos de dados $y_2, \ldots, y_T$.
As express√µes dadas no texto acima correspondem a cada termo, mostrando assim o teorema. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Vamos simular um processo AR(1) com $c = 2$, $\phi = 0.7$, e $\sigma^2 = 1$, e gerar 100 observa√ß√µes.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros do modelo
> c = 2
> phi = 0.7
> sigma2 = 1
> T = 100
>
> # Inicializa os erros e a s√©rie temporal
> errors = np.random.normal(0, np.sqrt(sigma2), T)
> y = np.zeros(T)
>
> # Gera a s√©rie temporal AR(1)
> y[0] = c / (1 - phi) + errors[0] # Usa a m√©dia incondicional para o primeiro valor
> for t in range(1, T):
>     y[t] = c + phi * y[t-1] + errors[t]
>
> # Plota a s√©rie temporal
> plt.plot(y)
> plt.xlabel('Time')
> plt.ylabel('Y_t')
> plt.title('Simula√ß√£o de um Processo AR(1)')
> plt.show()
> ```
>
> Agora, vamos calcular a log-verossimilhan√ßa usando os dados simulados, com os par√¢metros verdadeiros e com par√¢metros diferentes.
>
> ```python
> import numpy as np
> from scipy.optimize import minimize
>
> # Fun√ß√£o log-verossimilhan√ßa
> def log_likelihood_ar1(params, y):
>     c, phi, sigma2 = params
>     T = len(y)
>     ll = -T/2 * np.log(2*np.pi) - T/2 * np.log(sigma2) - 1/2 * np.log(1 - phi**2)
>     ll -= (1 - phi**2) * (y[0] - c/(1-phi))**2 / (2*sigma2)
>     ll -= 1/(2*sigma2) * np.sum((y[1:] - c - phi*y[:-1])**2)
>     return -ll # Retorna o negativo para minimiza√ß√£o
>
> # Par√¢metros verdadeiros
> true_params = [c, phi, sigma2]
>
> # Par√¢metros iniciais para otimiza√ß√£o
> initial_params = [1, 0.5, 0.5]
>
> # Minimiza a fun√ß√£o log-verossimilhan√ßa (equivalente a maximizar o negativo)
> result = minimize(log_likelihood_ar1, initial_params, args=(y,), method='L-BFGS-B')
>
> # Extrai os resultados da otimiza√ß√£o
> estimated_c, estimated_phi, estimated_sigma2 = result.x
>
> # Calcula a log-verossimilhan√ßa para par√¢metros verdadeiros e estimados
> true_ll = -log_likelihood_ar1(true_params, y)
> estimated_ll = -log_likelihood_ar1(result.x, y)
>
> print(f"Log-verossimilhan√ßa com par√¢metros verdadeiros: {true_ll:.2f}")
> print(f"Log-verossimilhan√ßa com par√¢metros estimados: {estimated_ll:.2f}")
>
> print("Par√¢metros verdadeiros:", true_params)
> print("Par√¢metros estimados:", result.x)
> ```
>
>  Este c√≥digo simula uma s√©rie temporal AR(1), calcula a fun√ß√£o de log-verossimilhan√ßa, e usa otimiza√ß√£o num√©rica para encontrar os par√¢metros que maximizam essa fun√ß√£o. O resultado mostra que a log-verossimilhan√ßa dos par√¢metros estimados √© maior que dos par√¢metros verdadeiros, indicando que a estima√ß√£o de m√°xima verossimilhan√ßa encontrou um melhor ajuste para os dados simulados.
>
> Com uma sa√≠da parecida com essa:
> ```
> Log-verossimilhan√ßa com par√¢metros verdadeiros: -143.99
> Log-verossimilhan√ßa com par√¢metros estimados: -143.93
> Par√¢metros verdadeiros: [2, 0.7, 1]
> Par√¢metros estimados: [1.92259777 0.71004944 0.98187754]
> ```
>
> Observe que os par√¢metros estimados se aproximam dos verdadeiros e o log-likelihood estimado √© ligeiramente maior, como esperado.

### 5.3. Likelihood Function for an MA(1) Process

The **MA(1) process** has the form:

$$Y_t = \mu + \epsilon_t + \theta\epsilon_{t-1}$$

where $\epsilon_t \sim i.i.d. N(0, \sigma^2)$ [^1, ^5]. The likelihood function, as before, is the joint density of the observed data $(y_1, y_2, \ldots, y_T)$, given the parameters $(\mu, \theta, \sigma^2)$. However, unlike the AR(1) case, we do not observe $\epsilon_t$, which is a function of the parameters. This is known as the problem of unobserved state variables.

We can derive a recursive expression for $\epsilon_t$ from the definition of the MA(1) process:

$$\epsilon_t = Y_t - \mu - \theta \epsilon_{t-1} $$

To evaluate the likelihood, we must initialize the recursion by setting some pre-sample value for $\epsilon_0$. Typically, we set $\epsilon_0 = 0$.
Then, we can calculate $\epsilon_1, \epsilon_2, \ldots \epsilon_T$, and the joint density can be approximated as a product of conditional densities, as before:

$$L(\mu, \theta, \sigma^2 | y_1,\ldots,y_T) =  \prod_{t=1}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\epsilon_t^2}{2\sigma^2}\right)$$

where $\epsilon_t = Y_t - \mu - \theta \epsilon_{t-1}$, with $\epsilon_0 = 0$.
The corresponding log-likelihood is:

$$\ell(\mu, \theta, \sigma^2 | y_1,\ldots,y_T) =  -\frac{T}{2}\log(2\pi) - \frac{T}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{t=1}^T\epsilon_t^2$$

It's important to note that this likelihood function is an approximation because it neglects the density of $\epsilon_0$.

**Proposi√ß√£o 3.1** A fun√ß√£o de log-verossimilhan√ßa para um processo MA(1) pode ser expressa em termos de uma soma dos res√≠duos estimados.
*Prova.*
I. A fun√ß√£o de verossimilhan√ßa √© dada por:
$$L(\mu, \theta, \sigma^2 | y_1,\ldots,y_T) =  \prod_{t=1}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\epsilon_t^2}{2\sigma^2}\right)$$
II. Tomando o logaritmo de ambos os lados:
$$\ell(\mu, \theta, \sigma^2 | y_1,\ldots,y_T) = \sum_{t=1}^T \log\left( \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\epsilon_t^2}{2\sigma^2}\right)\right)$$
III. Expandindo a soma e simplificando, obtemos:
$$\ell(\mu, \theta, \sigma^2 | y_1,\ldots,y_T) =  -\frac{T}{2}\log(2\pi) - \frac{T}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{t=1}^T\epsilon_t^2$$
Essa express√£o mostra que a log-verossimilhan√ßa √© uma fun√ß√£o da soma dos quadrados dos res√≠duos $\epsilon_t$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Vamos simular um processo MA(1) com $\mu = 1$, $\theta = 0.5$, e $\sigma^2 = 1$, e gerar 100 observa√ß√µes.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> import statsmodels.api as sm
> from scipy import stats
> from scipy.optimize import minimize
>
> # Par√¢metros do modelo
> mu = 1
> theta1 = 0.5
> sigma2 = 1
> T = 100
>
> # Inicializa os erros e a s√©rie temporal
> errors = np.random.normal(0, np.sqrt(sigma2), T)
> y = np.zeros(T)
> y[0] = mu + errors[0]
>
> # Simula a s√©rie temporal AR(1)
> for t in range(1, T):
>     y[t] = mu + theta1 * (y[t-1] - mu) + errors[t]
>
> # Fun√ß√£o para calcular a log-verossimilhan√ßa
> def log_likelihood(params, y):
>     mu = params[0]
>     theta1 = params[1]
>     sigma2 = params[2]
>     T = len(y)
>     errors = np.zeros(T)
>     errors[0] = y[0] - mu
>     for t in range(1, T):
>          errors[t] = y[t] - mu - theta1*(y[t-1]-mu)
>     log_likelihood_value = -T/2*np.log(2*np.pi*sigma2) - 1/(2*sigma2)*np.sum(errors**2)
>     return -log_likelihood_value
>
> # Valores iniciais para os par√¢metros
> initial_params = [0, 0, 1]
>
> # Otimiza a fun√ß√£o de log-verossimilhan√ßa
> result = minimize(log_likelihood, initial_params, args=(y,), method='L-BFGS-B', bounds=[(None, None), (-1, 1), (0, None)])
>
> # Extrai os par√¢metros estimados
> estimated_mu = result.x[0]
> estimated_theta1 = result.x[1]
> estimated_sigma2 = result.x[2]
>
> print(f"Par√¢metro mu estimado: {estimated_mu}")
> print(f"Par√¢metro theta1 estimado: {estimated_theta1}")
> print(f"Par√¢metro sigma2 estimado: {estimated_sigma2}")
>
> # Simula√ß√£o com os par√¢metros estimados
> y_simulated = np.zeros(T)
> errors_simulated = np.random.normal(0, np.sqrt(estimated_sigma2), T)
> y_simulated[0] = estimated_mu + errors_simulated[0]
>
> for t in range(1, T):
>    y_simulated[t] = estimated_mu + estimated_theta1 * (y_simulated[t-1] - estimated_mu) + errors_simulated[t]
>
> # Gr√°ficos
> plt.figure(figsize=(10, 6))
> plt.plot(range(T), y, label='S√©rie Real', color='blue')
> plt.plot(range(T), y_simulated, label='S√©rie Simulada', color='red', linestyle='--')
> plt.title('Compara√ß√£o da S√©rie Real e S√©rie Simulada com Par√¢metros Estimados')
> plt.xlabel('Tempo')
> plt.ylabel('Valor da S√©rie')
> plt.legend()
> plt.grid(True)
> plt.show()
>
> # Gr√°fico para comparar os valores estimados dos par√¢metros com os valores verdadeiros
> parameters_names = ["mu", "theta1", "sigma2"]
> true_values = [mu, theta1, sigma2]
> estimated_values = [estimated_mu, estimated_theta1, estimated_sigma2]
>
> x = np.arange(len(parameters_names))
> width = 0.35
>
> fig, ax = plt.subplots()
> rects1 = ax.bar(x - width/2, true_values, width, label='Verdadeiro', color='blue')
> rects2 = ax.bar(x + width/2, estimated_values, width, label='Estimado', color='red')
>
> ax.set_ylabel('Valores')
> ax.set_title('Compara√ß√£o dos Par√¢metros Verdadeiros e Estimados')
> ax.set_xticks(x)
> ax.set_xticklabels(parameters_names)
> ax.legend()
>
> fig.tight_layout()
> plt.show()
<!-- END -->
