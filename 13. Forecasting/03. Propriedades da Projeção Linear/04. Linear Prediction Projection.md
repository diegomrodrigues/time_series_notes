## Proje√ß√£o Linear de Combina√ß√µes Lineares de Vari√°veis e o Erro de Previs√£o

### Introdu√ß√£o
Este cap√≠tulo continua a explora√ß√£o da **proje√ß√£o linear**, com foco em como ela se comporta quando aplicada a **combina√ß√µes lineares de vari√°veis aleat√≥rias** [^2], especificamente na proje√ß√£o de $aY_{t+1} + b$ em $X_t$, onde $a$ e $b$ s√£o constantes determin√≠sticas. Construindo sobre conceitos anteriores, como a **otimalidade da previs√£o linear** e a **deriva√ß√£o da matriz de proje√ß√£o**, este cap√≠tulo ir√° demonstrar que a proje√ß√£o de uma combina√ß√£o linear de $Y_{t+1}$ mant√©m propriedades importantes de linearidade e ortogonalidade do erro [^1]. Vamos detalhar que a proje√ß√£o de $aY_{t+1} + b$ em $X_t$ √© dada por $aP(Y_{t+1}|X_t) + b$, e que o erro de previs√£o resultante √© ortogonal a $X_t$.

### Conceitos Fundamentais
Inicialmente, recordamos que a proje√ß√£o linear de $Y_{t+1}$ em $X_t$ √© denotada por $P(Y_{t+1}|X_t)$ e √© dada por:

$$
P(Y_{t+1}|X_t) = \alpha' X_t
$$

onde $\alpha'$ √© a matriz de proje√ß√£o [^2]. Queremos agora encontrar a proje√ß√£o linear de uma combina√ß√£o linear de $Y_{t+1}$, ou seja, $aY_{t+1} + b$, onde $a$ e $b$ s√£o constantes determin√≠sticas. Denotemos esta proje√ß√£o por $P(aY_{t+1} + b|X_t)$.

**Proposi√ß√£o 2.1** A proje√ß√£o linear de $aY_{t+1} + b$ em $X_t$ √© dada por $aP(Y_{t+1}|X_t) + b$, onde $a$ e $b$ s√£o constantes determin√≠sticas.

*Prova.*
I. Vamos mostrar que a proje√ß√£o de $aY_{t+1} + b$ em $X_t$, que denotamos por $P(aY_{t+1} + b|X_t)$, √© uma fun√ß√£o linear de $X_t$ e tem a forma $aP(Y_{t+1}|X_t) + b$.
II. Sabemos que $P(Y_{t+1}|X_t)$ √© a proje√ß√£o linear de $Y_{t+1}$ em $X_t$ e que $P(Y_{t+1}|X_t) = \alpha'X_t$ para algum vetor de coeficientes $\alpha'$ [^2].
III. Considere a previs√£o $aP(Y_{t+1}|X_t) + b$, onde $a$ e $b$ s√£o constantes. Essa tamb√©m √© uma fun√ß√£o linear de $X_t$ pois $aP(Y_{t+1}|X_t) + b = a \alpha'X_t + b = (a\alpha') X_t + b$.
IV. Devemos verificar se o erro de previs√£o, $e_t = (aY_{t+1} + b) - (aP(Y_{t+1}|X_t) + b)$ √© n√£o correlacionado com $X_t$.
V. Calculamos a esperan√ßa do produto do erro de previs√£o com $X_t$:
$$
    E[((aY_{t+1} + b) - (aP(Y_{t+1}|X_t) + b))X_t'] = E[a(Y_{t+1} - P(Y_{t+1}|X_t))X_t'] = aE[(Y_{t+1} - P(Y_{t+1}|X_t))X_t'] = 0
$$
VI. Como $P(Y_{t+1}|X_t)$ √© a proje√ß√£o linear de $Y_{t+1}$ em $X_t$, o erro $(Y_{t+1} - P(Y_{t+1}|X_t))$ √© ortogonal a $X_t$ pela propriedade de ortogonalidade.
VII. Portanto, $aP(Y_{t+1}|X_t) + b$ √© a proje√ß√£o linear de $aY_{t+1} + b$ em $X_t$.
$\blacksquare$

Este resultado formaliza que a proje√ß√£o de uma combina√ß√£o linear √© uma combina√ß√£o linear das proje√ß√µes.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um modelo de proje√ß√£o linear onde $Y_{t+1}$ √© o pre√ßo de uma a√ß√£o no tempo $t+1$, e $X_t$ √© um vetor de vari√°veis explicativas no tempo $t$, como o volume de negocia√ß√£o e √≠ndices de mercado. J√° encontramos a proje√ß√£o linear de $Y_{t+1}$ em $X_t$ como:
>
> $$P(Y_{t+1}|X_t) = 0.5X_{t,1} + 0.2X_{t,2} + 10$$
>
> onde $X_{t,1}$ representa o volume de negocia√ß√£o e $X_{t,2}$ representa um √≠ndice de mercado. Agora, vamos considerar que desejamos projetar uma nova vari√°vel, $W_{t+1} = 2Y_{t+1} + 5$, que representa um valor transformado do pre√ßo da a√ß√£o, em fun√ß√£o das mesmas vari√°veis explicativas $X_t$.  De acordo com a Proposi√ß√£o 2.1, a proje√ß√£o de $W_{t+1}$ em $X_t$ √© dada por:
>
> $$P(2Y_{t+1} + 5|X_t) = 2P(Y_{t+1}|X_t) + 5$$
>
> Substituindo a proje√ß√£o de $Y_{t+1}$, temos:
>
> $$P(2Y_{t+1} + 5|X_t) = 2(0.5X_{t,1} + 0.2X_{t,2} + 10) + 5$$
>
> $$P(2Y_{t+1} + 5|X_t) = X_{t,1} + 0.4X_{t,2} + 20 + 5$$
>
> $$P(2Y_{t+1} + 5|X_t) = X_{t,1} + 0.4X_{t,2} + 25$$
>
> Isso significa que para prever a nova vari√°vel $W_{t+1}$, n√≥s simplesmente aplicamos as transforma√ß√µes lineares aos coeficientes da proje√ß√£o original e ao termo constante.  Se por exemplo $X_{t,1} = 100$ e $X_{t,2} = 50$ ent√£o:
> $$P(2Y_{t+1} + 5|X_t) = 100 + 0.4(50) + 25 = 100 + 20 + 25 = 145$$
>  A proje√ß√£o de $2Y_{t+1} + 5$ dada pelos valores de $X_t$ ser√° 145.
>
>  Este exemplo ilustra como a proje√ß√£o de uma combina√ß√£o linear de vari√°veis √© simples e direta, mantendo a linearidade da proje√ß√£o original.

**Teorema 2.1** O erro de previs√£o resultante da proje√ß√£o de $aY_{t+1} + b$ em $X_t$ √© n√£o correlacionado com $X_t$, ou seja:

$$
E[(aY_{t+1} + b - (aP(Y_{t+1}|X_t) + b))X_t'] = 0.
$$

*Prova.*

I. Considere o erro de previs√£o, $e_t$, associado √† proje√ß√£o de $aY_{t+1} + b$ em $X_t$, onde $P(aY_{t+1} + b|X_t) = aP(Y_{t+1}|X_t) + b$:
   $$e_t = aY_{t+1} + b - (aP(Y_{t+1}|X_t) + b) = a(Y_{t+1} - P(Y_{t+1}|X_t)).$$
II. Para mostrar que $e_t$ √© n√£o correlacionado com $X_t$, precisamos provar que $E[e_t X_t'] = 0$.
III. Substituindo $e_t$, temos:
   $$E[e_t X_t'] = E[(a(Y_{t+1} - P(Y_{t+1}|X_t))X_t'] = a E[(Y_{t+1} - P(Y_{t+1}|X_t))X_t'].$$
IV. Pela defini√ß√£o da proje√ß√£o linear, o erro de proje√ß√£o $(Y_{t+1} - P(Y_{t+1}|X_t))$ √© n√£o correlacionado com $X_t$.
V. Portanto,
   $$E[(Y_{t+1} - P(Y_{t+1}|X_t))X_t'] = 0,$$
   e consequentemente,
   $$E[e_t X_t'] = a \cdot 0 = 0.$$
$\blacksquare$

Essa propriedade garante que a proje√ß√£o linear preserva a propriedade de ortogonalidade do erro, mesmo quando aplicada a uma combina√ß√£o linear de vari√°veis aleat√≥rias.

> üí° **Exemplo Num√©rico:**
>
>  Continuando o exemplo anterior, onde $Y_{t+1}$ √© o pre√ßo de uma a√ß√£o e $X_t$ s√£o vari√°veis explicativas e $P(Y_{t+1}|X_t) = 0.5X_{t,1} + 0.2X_{t,2} + 10$, e onde definimos $W_{t+1} = 2Y_{t+1} + 5$. Vimos que $P(W_{t+1}|X_t) = X_{t,1} + 0.4X_{t,2} + 25$. O erro de previs√£o para $W_{t+1}$ √© dado por:
>
> $$e_t = W_{t+1} - P(W_{t+1}|X_t) = (2Y_{t+1} + 5) - (X_{t,1} + 0.4X_{t,2} + 25)$$
>
> $$e_t = 2Y_{t+1} - X_{t,1} - 0.4X_{t,2} - 20$$
>
>  O Teorema 2.1 nos diz que este erro $e_t$ deve ser n√£o correlacionado com $X_t$. Para verificar isso, vamos supor que temos alguns dados simulados.
>
> ```python
> import numpy as np
>
> # Simula√ß√£o de dados
> np.random.seed(42)
> num_obs = 100
> X_t1 = np.random.rand(num_obs) * 100  # Volume de negocia√ß√£o
> X_t2 = np.random.rand(num_obs) * 50   # √çndice de mercado
> Y_t1 = 0.5 * X_t1 + 0.2 * X_t2 + 10 + np.random.randn(num_obs) * 5 # Pre√ßo da a√ß√£o (com algum erro)
>
> W_t1 = 2 * Y_t1 + 5  # Vari√°vel transformada
>
> # Calculando o erro de proje√ß√£o de W_t1
> P_W_t1 = X_t1 + 0.4 * X_t2 + 25
> e_t = W_t1 - P_W_t1
>
> # Verificando a correla√ß√£o
> corr_X1_e = np.corrcoef(X_t1, e_t)[0, 1]
> corr_X2_e = np.corrcoef(X_t2, e_t)[0, 1]
>
> print(f"Correla√ß√£o entre X_t1 e e_t: {corr_X1_e:.4f}")
> print(f"Correla√ß√£o entre X_t2 e e_t: {corr_X2_e:.4f}")
> ```
>
>  Este c√≥digo gera valores aleat√≥rios para $X_{t,1}$, $X_{t,2}$ e $Y_{t+1}$, e ent√£o calcula $W_{t+1}$ e o erro $e_t$ de acordo com o exemplo num√©rico. A sa√≠da do c√≥digo mostra que a correla√ß√£o entre o erro e cada uma das vari√°veis explicativas s√£o muito pr√≥ximas de zero, o que verifica a propriedade de ortogonalidade.
>
> A propriedade de ortogonalidade do erro √© fundamental em proje√ß√£o linear, pois indica que toda a informa√ß√£o de $X_t$ relevante para prever $W_{t+1}$ j√° foi capturada pela proje√ß√£o linear, e o erro resultante √© um ru√≠do aleat√≥rio n√£o correlacionado com as vari√°veis explicativas.

**Corol√°rio 2.1**  Se $X_t$ inclui uma constante, ent√£o a proje√ß√£o linear de $(aY_{t+1} + b)$ em $X_t$ ter√° um termo constante e ser√° dada por $aP(Y_{t+1}|X_t) + b$.
*Prova.*
I. Da Proposi√ß√£o 2.1, sabemos que $P(aY_{t+1}+b|X_t) = aP(Y_{t+1}|X_t) + b$.
II. Se $X_t$ inclui uma constante, ent√£o $P(Y_{t+1}|X_t)$ inclui um intercepto, $P(Y_{t+1}|X_t) = \alpha_1 + \alpha_2 X_{t,2} + \ldots + \alpha_k X_{t,k}$.
III. Assim, a proje√ß√£o linear de $aY_{t+1}+b$ ter√° a forma $a(\alpha_1 + \alpha_2 X_{t,2} + \ldots + \alpha_k X_{t,k}) + b = a\alpha_1 + b + a(\alpha_2 X_{t,2} + \ldots + \alpha_k X_{t,k})$, que inclui um termo constante $a\alpha_1 + b$.
IV. O que demonstra que $P(aY_{t+1} + b|X_t)$ incluir√° um termo constante, se $X_t$ incluir um termo constante.
$\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Suponha que a proje√ß√£o de $Y_{t+1}$ em $X_t$ seja dada por:
> $$P(Y_{t+1}|X_t) = 2 + 0.7X_{t,1} - 0.3X_{t,2}$$
> Aqui, o termo '2' √© o intercepto, que surge porque $X_t$ inclui uma constante (impl√≠cita, por exemplo, uma coluna de '1's no modelo de regress√£o). Agora, se quisermos encontrar a proje√ß√£o de $W_{t+1} = 3Y_{t+1} + 10$ em $X_t$, temos:
> $$P(3Y_{t+1} + 10|X_t) = 3P(Y_{t+1}|X_t) + 10$$
> $$P(3Y_{t+1} + 10|X_t) = 3(2 + 0.7X_{t,1} - 0.3X_{t,2}) + 10$$
> $$P(3Y_{t+1} + 10|X_t) = 6 + 2.1X_{t,1} - 0.9X_{t,2} + 10$$
> $$P(3Y_{t+1} + 10|X_t) = 16 + 2.1X_{t,1} - 0.9X_{t,2}$$
> Note que o novo termo constante √© $16 = 3 \times 2 + 10$, demonstrando que a proje√ß√£o de $aY_{t+1} + b$ inclui um termo constante que √© uma fun√ß√£o linear dos termos constantes da proje√ß√£o original e do $b$.
> O exemplo ilustra como o intercepto na proje√ß√£o original de $Y_{t+1}$ √© afetado e amplificado pela transforma√ß√£o linear.

**Proposi√ß√£o 2.2** Se $X_t$ is orthogonal to a constant term, then $P(aY_{t+1} + b | X_t) = aP(Y_{t+1}|X_t)$.

*Proof.*

I. From Proposition 2.1, we know that $P(aY_{t+1} + b | X_t) = aP(Y_{t+1}|X_t) + b$.

II. If $X_t$ is orthogonal to a constant, meaning $E[X_t] = 0$, then the linear projection $P(c | X_t) = 0$ for any constant c. This implies the projection of $b$ onto $X_t$ is zero since it is a constant.

III. Therefore,  the constant $b$ has no projection onto $X_t$, so the term $b$ will vanish when projecting onto $X_t$ leaving only the $aP(Y_{t+1}|X_t)$ part.

IV. Thus, $P(aY_{t+1} + b | X_t) = aP(Y_{t+1}|X_t)$.
$\blacksquare$

This proposition further clarifies that when $X_t$ is orthogonal to any constant, only the term multiplied by $Y_{t+1}$ is relevant in the projection.

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar a Proposi√ß√£o 2.2, vamos considerar um cen√°rio onde $X_t$ √© uma vari√°vel centrada na m√©dia (i.e., com m√©dia zero), e queremos projetar $2Y_{t+1} + 3$ em $X_t$. Se a proje√ß√£o de $Y_{t+1}$ em $X_t$ fosse:
> $$P(Y_{t+1}|X_t) = 0.8X_t$$
>  Onde $X_t$ tem m√©dia zero, ent√£o a proje√ß√£o de  $2Y_{t+1} + 3$ em $X_t$ seria:
> $$P(2Y_{t+1} + 3|X_t) = 2P(Y_{t+1}|X_t) + 3$$
> Como $X_t$ √© ortogonal a qualquer constante, a proje√ß√£o da constante 3 sobre $X_t$ √© zero:
> $$P(2Y_{t+1} + 3|X_t) = 2(0.8X_t) + 0$$
> $$P(2Y_{t+1} + 3|X_t) = 1.6X_t$$
>
> Aqui, a constante 3 n√£o contribui para a proje√ß√£o, porque a vari√°vel $X_t$ est√° centrada na m√©dia, e como consequ√™ncia a constante n√£o tem parte que se projeta sobre $X_t$. Isso demonstra que, se o vetor de vari√°veis explicativas √© ortogonal a uma constante, a proje√ß√£o da constante adicional (b) √© zero.

### Conclus√£o
Esta se√ß√£o demonstrou que a proje√ß√£o linear de combina√ß√µes lineares de vari√°veis aleat√≥rias $aY_{t+1} + b$ mant√©m propriedades importantes, como a linearidade e a ortogonalidade do erro [^2]. A proje√ß√£o de $aY_{t+1} + b$ em $X_t$ √© dada por $aP(Y_{t+1}|X_t) + b$, e o erro de previs√£o resultante √© n√£o correlacionado com $X_t$. Esses resultados s√£o cruciais para entender como a proje√ß√£o linear pode ser aplicada de forma consistente em diferentes cen√°rios, e expande a sua aplicabilidade para situa√ß√µes em que a vari√°vel a ser prevista √© uma combina√ß√£o linear de outras vari√°veis.

### Refer√™ncias
[^1]: Express√µes [4.1.1], [4.1.4], [4.1.11].
[^2]: Express√µes [4.1.9], [4.1.10], [4.1.13], [4.1.21], [4.1.22].
[^3]: Express√µes [4.1.14].
[^4]: Proposi√ß√£o 1.1.
[^5]: Express√£o [5.1.5].
### 5.2. Likelihood Function for an AR(1) Process
Consider the AR(1) process:
$$Y_t = c + \phi Y_{t-1} + \epsilon_t$$
where $\epsilon_t \sim i.i.d. N(0, \sigma^2)$ [^5].  Our goal is to derive the likelihood function, given a sample of size $T$, denoted as $(y_1, y_2, \ldots, y_T)$. For simplicity, assume initially that the intercept $c=0$.  The joint density of the errors, given the parameters, is:
$$f(\epsilon_1, \epsilon_2, \ldots, \epsilon_T | \phi, \sigma^2) = \prod_{t=1}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\epsilon_t^2}{2\sigma^2}\right)$$
Since $Y_t = \phi Y_{t-1} + \epsilon_t$, it follows that $\epsilon_t = Y_t - \phi Y_{t-1}$.  Thus, we can write the likelihood function in terms of the observed data $Y_t$, as
$$L(\phi, \sigma^2 | y_1, y_2, \ldots, y_T) = \prod_{t=1}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_t - \phi y_{t-1})^2}{2\sigma^2}\right)$$
It is common practice to work with the log-likelihood function, as it simplifies calculations and does not change the location of the maximum. The log-likelihood is given by
$$log L(\phi, \sigma^2 | y_1, y_2, \ldots, y_T) = -\frac{T}{2}log(2\pi) - \frac{T}{2}log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=1}^T (y_t - \phi y_{t-1})^2$$
The above equation represents the likelihood function conditioned on $y_0$.  We will address the implications of conditioning on $y_0$ later, but for now it suffices. To find the MLE, one would need to maximize the above log-likelihood function with respect to $\phi$ and $\sigma^2$. This optimization is typically done numerically.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos a seguinte s√©rie temporal de 5 observa√ß√µes: $y = [2.1, 2.5, 3.1, 2.8, 3.2]$. Queremos ajustar um modelo AR(1) com $c=0$. Usando a f√≥rmula da log-verossimilhan√ßa acima, temos:
>
>  $$log L(\phi, \sigma^2 | y) = -\frac{5}{2}log(2\pi) - \frac{5}{2}log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=1}^5 (y_t - \phi y_{t-1})^2$$
>
>  Vamos explicitar o somat√≥rio:
>
> $$\sum_{t=1}^5 (y_t - \phi y_{t-1})^2 = (y_1 - \phi y_0)^2 + (y_2 - \phi y_1)^2 + (y_3 - \phi y_2)^2 + (y_4 - \phi y_3)^2 + (y_5 - \phi y_4)^2 $$
>
>  Como $y_0$ n√£o est√° dispon√≠vel, para este exemplo usaremos $y_0=0$. Substituindo os valores temos:
>
> $$\sum_{t=1}^5 (y_t - \phi y_{t-1})^2 = (2.1 - \phi \cdot 0)^2 + (2.5 - \phi \cdot 2.1)^2 + (3.1 - \phi \cdot 2.5)^2 + (2.8 - \phi \cdot 3.1)^2 + (3.2 - \phi \cdot 2.8)^2 $$
>
> Para encontrar os valores de $\phi$ e $\sigma^2$ que maximizam essa log-verossimilhan√ßa, precisar√≠amos usar um otimizador num√©rico. Por exemplo, em Python com `scipy.optimize.minimize`, far√≠amos:
>
> ```python
> import numpy as np
> from scipy.optimize import minimize
>
> y = np.array([2.1, 2.5, 3.1, 2.8, 3.2])
>
> def neg_log_likelihood(params, data):
>     phi, sigma2 = params
>     T = len(data)
>     y_0 = 0 # Assuming y_0 = 0
>     sum_sq_errors = (data[0] - phi*y_0)**2
>     for t in range(1, T):
>        sum_sq_errors += (data[t] - phi*data[t-1])**2
>     return T/2 * np.log(2*np.pi) + T/2 * np.log(sigma2) + 1/(2*sigma2) * sum_sq_errors
>
> # Initial guess for parameters
> initial_params = [0.5, 1]
>
> # Optimization
> results = minimize(neg_log_likelihood, initial_params, args=(y,), method='L-BFGS-B', bounds=((-1, 1), (0.001, None)))
> best_phi, best_sigma2 = results.x
> print(f"MLE of phi: {best_phi:.4f}")
> print(f"MLE of sigma^2: {best_sigma2:.4f}")
> ```
>
> A sa√≠da do c√≥digo acima ir√° mostrar os valores de $\phi$ e $\sigma^2$ que minimizam a fun√ß√£o de log-verossimilhan√ßa negativa, que √© equivalente a maximizar a log-verossimilhan√ßa. Este exemplo ilustra como o processo de otimiza√ß√£o √© implementado para encontrar os melhores par√¢metros do modelo.  A restri√ß√£o de $\phi$ estar entre -1 e 1, √© para garantir a estacionariedade do modelo AR(1).

### 5.3. Likelihood Function for an AR(p) Process
The analysis for the AR(1) case can be extended to the AR(p) case:
$$Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t$$
where $\epsilon_t \sim i.i.d. N(0, \sigma^2)$[^5].  Again, assuming initially that $c = 0$, the error term can be expressed as:
$$\epsilon_t = Y_t - \phi_1 Y_{t-1} - \phi_2 Y_{t-2} - \ldots - \phi_p Y_{t-p}$$
Following the same approach as in the AR(1) case, the log-likelihood function, conditional on the first $p$ observations $(y_1, y_2, \ldots, y_p)$ is
$$log L(\phi_1, \phi_2, \ldots, \phi_p, \sigma^2 | y_1, y_2, \ldots, y_T) = -\frac{T-p}{2}log(2\pi) - \frac{T-p}{2}log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=p+1}^T (y_t - \phi_1 y_{t-1} - \ldots - \phi_p y_{t-p})^2$$
Note that the summation now starts at $t = p+1$, because the errors $\epsilon_1$ to $\epsilon_p$ are not defined by the model. The likelihood function is conditioned on the first $p$ observations. This difference between $T$ and $T-p$ becomes immaterial as $T$ becomes large. This expression can be maximized to obtain the MLE of the parameters ($\phi_1$, ..., $\phi_p$, $\sigma^2$). The optimization is typically carried out numerically.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos a seguinte s√©rie temporal de 7 observa√ß√µes e que queremos ajustar um modelo AR(2). Temos $y = [1.2, 1.5, 1.8, 2.0, 2.3, 2.5, 2.7]$.  Nesse caso $p = 2$ e o log-verossimilhan√ßa √©:
>
> $$log L(\phi_1, \phi_2, \sigma^2 | y) = -\frac{7-2}{2}log(2\pi) - \frac{7-2}{2}log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=3}^7 (y_t - \phi_1 y_{t-1} - \phi_2 y_{t-2})^2$$
>
>  Expandindo o somat√≥rio:
>  $$\sum_{t=3}^7 (y_t - \phi_1 y_{t-1} - \phi_2 y_{t-2})^2 = (y_3 - \phi_1 y_2 - \phi_2 y_1)^2 + (y_4 - \phi_1 y_3 - \phi_2 y_2)^2 + (y_5 - \phi_1 y_4 - \phi_2 y_3)^2 + (y_6 - \phi_1 y_5 - \phi_2 y_4)^2 + (y_7 - \phi_1 y_6 - \phi_2 y_5)^2 $$
> Substituindo os valores:
>
> $$\sum_{t=3}^7 (y_t - \phi_1 y_{t-1} - \phi_2 y_{t-2})^2 = (1.8 - \phi_1 1.5 - \phi_2 1.2)^2 + (2.0 - \phi_1 1.8 - \phi_2 1.5)^2 + (2.3 - \phi_1 2.0 - \phi_2 1.8)^2 + (2.5 - \phi_1 2.3 - \phi_2 2.0)^2 + (2.7 - \phi_1 2.5 - \phi_2 2.3)^2 $$
>
>  Para encontrar os valores dos par√¢metros ($\phi_1, \phi_2, \sigma^2$) que maximizam essa fun√ß√£o de log-verossimilhan√ßa, usamos um otimizador num√©rico, similar ao exemplo anterior do AR(1):
>
> ```python
> import numpy as np
> from scipy.optimize import minimize
>
> y = np.array([1.2, 1.5, 1.8, 2.0, 2.3, 2.5, 2.7])
>
> def neg_log_likelihood(params, data):
>     phi1, phi2, sigma2 = params
>     T = len(data)
>     sum_sq_errors = 0
>     for t in range(2, T):
>         sum_sq_errors += (data[t] - phi1*data[t-1] - phi2*data[t-2])**2
>     return (T-2)/2 * np.log(2*np.pi) + (T-2)/2 * np.log(sigma2) + 1/(2*sigma2) * sum_sq_errors
>
> # Initial guess for parameters
> initial_params = [0.5, 0.2, 1]
>
> # Optimization
> results = minimize(neg_log_likelihood, initial_params, args=(y,), method='L-BFGS-B', bounds=((-1, 1),(-1, 1), (0.001, None)))
> best_phi1, best_phi2, best_sigma2 = results.x
> print(f"MLE of phi_1: {best_phi1:.4f}")
> print(f"MLE of phi_2: {best_phi2:.4f}")
> print(f"MLE of sigma^2: {best_sigma2:.4f}")
> ```
> Este c√≥digo exemplifica como encontrar os MLE dos par√¢metros do modelo AR(2). Note que os par√¢metros s√£o estimados usando a fun√ß√£o log-verossimilhan√ßa condicional, que depende dos dois primeiros valores da s√©rie. Novamente as restri√ß√µes de $\phi_1$ e $\phi_2$ entre -1 e 1 s√£o para garantir a estacionariedade do modelo.

### 5.4. Likelihood Function for an MA(1) Process
Now consider the moving average MA(1) process:
$$Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$$
where $\epsilon_t \sim i.i.d. N(0, \sigma^2)$[^5].  Again, assuming initially that $\mu = 0$,  the likelihood function for the MA(1) is more challenging due to the fact that the errors $\epsilon_t$ are not directly observable. Unlike the autoregressive process, the MA model implies an infinite autoregressive representation. Thus, we need to backcast these errors.
We start by recognizing that the errors can be expressed in terms of current and past Y:
$$\epsilon_t = Y_t - \theta \epsilon_{t-1}$$
The above expression defines a recursive relationship which requires starting values for $\epsilon_t$. We use the fact that the model is stationary and hence for sufficiently large negative values of $t$, $\epsilon_t$ is near zero. As such, we can initiate the process with $\epsilon_0 = 0$, which will lead to:
$$\epsilon_1 = Y_1$$
$$\epsilon_2 = Y_2 - \theta \epsilon_1 = Y_2 - \theta Y_1$$
$$\epsilon_3 = Y_3 - \theta \epsilon_2 = Y_3 - \theta (Y_2 - \theta Y_1) = Y_3 - \theta Y_2 + \theta^2 Y_1$$
and so on. In general, we can use the following recurrence relation:
$$\epsilon_t = Y_t - \theta \epsilon_{t-1} $$
Based on the Gaussian error assumption, we can write the likelihood function as:
$$L(\theta, \sigma^2 | y_1, y_2, \ldots, y_T)=$$ \prod_{t=1}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\epsilon_t^2}{2\sigma^2}\right) $$
Substituting the expression for $\epsilon_t$, we get:
$$L(\theta, \sigma^2 | y_1, y_2, \ldots, y_T)= \prod_{t=1}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(Y_t - \theta \epsilon_{t-1})^2}{2\sigma^2}\right)$$

The log-likelihood function is:

$$\ell(\theta, \sigma^2 | y_1, y_2, \ldots, y_T) = \sum_{t=1}^T \left[ -\frac{1}{2} \log(2\pi\sigma^2) - \frac{(Y_t - \theta \epsilon_{t-1})^2}{2\sigma^2} \right]$$

To estimate the parameters $\theta$ and $\sigma^2$, we can maximize the log-likelihood function. First, let's find the maximum likelihood estimator for $\sigma^2$ by taking the derivative of the log-likelihood with respect to $\sigma^2$ and setting it to zero.

$$\frac{\partial \ell}{\partial \sigma^2} = \sum_{t=1}^T \left[ -\frac{1}{2\sigma^2} + \frac{(Y_t - \theta \epsilon_{t-1})^2}{2\sigma^4} \right] = 0$$

Solving for $\sigma^2$, we obtain:

$$\hat{\sigma}^2 = \frac{1}{T} \sum_{t=1}^T (Y_t - \theta \epsilon_{t-1})^2 $$

To find the maximum likelihood estimator for $\theta$, we take the derivative of the log-likelihood with respect to $\theta$ and set it to zero:

$$\frac{\partial \ell}{\partial \theta} = \sum_{t=1}^T \left[  \frac{(Y_t - \theta \epsilon_{t-1}) \epsilon_{t-1}}{\sigma^2} \right] = 0$$

However, note that the previous equation is not solvable analytically, and we need to use numerical optimization methods to find the value of $\theta$ that maximizes the log-likelihood function.

### Implementation in Python

Let's illustrate the estimation procedure with a simulated AR(1) process in Python.
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.optimize import minimize

#Simulate AR(1) process
np.random.seed(42)
T = 500
theta_true = 0.7
sigma_true = 0.5
epsilon = np.random.normal(0,sigma_true,T)
Y = np.zeros(T)
for t in range(1,T):
    Y[t] = theta_true*Y[t-1] + epsilon[t]


# Define the negative log-likelihood function
def neg_log_likelihood(params,Y):
    theta = params[0]
    sigma2 = params[1]
    T = len(Y)
    epsilon = np.zeros(T)

    for t in range(1, T):
      epsilon[t] = Y[t]-theta*epsilon[t-1]
    
    nll = T/2*np.log(2*np.pi*sigma2) + sum(epsilon[1:]**2)/(2*sigma2)
    return nll


#Initial guess for parameters
initial_guess = [0.5, 1.0]

# Minimize the negative log-likelihood function
results = minimize(neg_log_likelihood, initial_guess, args=(Y,), method='L-BFGS-B',bounds=[(-1,1),(0.0001,None)])

# Extract estimated parameters
theta_hat = results.x[0]
sigma2_hat = results.x[1]
sigma_hat = np.sqrt(sigma2_hat)

print(f"Estimated theta: {theta_hat}")
print(f"Estimated sigma: {sigma_hat}")

```
<!-- END -->
