## Proje√ß√£o Linear e a Propriedade de M√≠nimo Erro Quadr√°tico M√©dio (MSE) para Combina√ß√µes Lineares

### Introdu√ß√£o
Este cap√≠tulo aprofunda as propriedades da **proje√ß√£o linear** no contexto de **previs√µes de s√©ries temporais**, com foco especial na demonstra√ß√£o de que a proje√ß√£o linear $Y_{t+1}^*$ n√£o s√≥ minimiza o MSE para $Y_{t+1}$, mas tamb√©m fornece a previs√£o de MSE m√≠nimo para qualquer combina√ß√£o linear dos elementos de $Y_{t+1}$, dado $X_t$ [^1]. Este resultado consolida a import√¢ncia da proje√ß√£o linear como uma ferramenta fundamental para a previs√£o. Ao demonstrar que a proje√ß√£o linear oferece a melhor previs√£o, no sentido do MSE, tanto para as vari√°veis originais como para qualquer combina√ß√£o linear dessas vari√°veis, estabelecemos um resultado bastante geral e robusto.

### Conceitos Fundamentais
Retomando conceitos anteriores, a proje√ß√£o linear de um vetor $Y_{t+1}$ em um vetor de vari√°veis explicativas $X_t$ √© dada por:
$$
Y_{t+1}^* = \alpha' X_t,
$$
onde $\alpha'$ √© a matriz de coeficientes de proje√ß√£o que minimiza o erro quadr√°tico m√©dio (MSE) [^2]. A condi√ß√£o que define $\alpha'$ √© que o erro de previs√£o $Y_{t+1} - Y_{t+1}^*$ seja **n√£o correlacionado** com $X_t$ [^2]:

$$
E[(Y_{t+1} - \alpha' X_t)X_t'] = 0.
$$
Essa condi√ß√£o leva √† express√£o para a matriz de proje√ß√£o [^3]:

$$
\alpha' = E[Y_{t+1} X_t'] [E(X_t X_t')]^{-1}.
$$
Agora, vamos generalizar esse resultado para combina√ß√µes lineares de elementos de $Y_{t+1}$. Considere um vetor $Z_{t+1}$ que √© uma combina√ß√£o linear dos elementos de $Y_{t+1}$ dado por $Z_{t+1} = H'Y_{t+1}$, onde $H$ √© uma matriz de constantes. A nossa meta √© mostrar que a proje√ß√£o linear de $Z_{t+1}$ em $X_t$, denotada por $Z_{t+1}^*$, √© dada por $H'Y_{t+1}^*$ e tamb√©m minimiza o MSE para $Z_{t+1}$, ou seja, $Z_{t+1}^* =  H' \alpha' X_t$.

**Teorema 3.1:** A proje√ß√£o linear $Y_{t+1}^*$ fornece a previs√£o de MSE m√≠nimo para qualquer combina√ß√£o linear dos elementos de $Y_{t+1}$ dado $X_t$.
*Prova.*
I. Seja $Z_{t+1} = H'Y_{t+1}$ uma combina√ß√£o linear dos elementos de $Y_{t+1}$, onde $H$ √© uma matriz de constantes.
II. Queremos mostrar que a proje√ß√£o linear de $Z_{t+1}$ em $X_t$, denotada por $Z_{t+1}^*$, √© dada por $H'Y_{t+1}^*$, onde $Y_{t+1}^*$ √© a proje√ß√£o linear de $Y_{t+1}$ em $X_t$.
III. Primeiro, note que $Z_{t+1}^* = H' Y_{t+1}^*$ √© uma fun√ß√£o linear de $X_t$.
IV. Para demonstrar que $Z_{t+1}^* = H' \alpha' X_t$ √© a proje√ß√£o linear de $Z_{t+1}$ em $X_t$, precisamos provar que o erro de previs√£o $(Z_{t+1} - Z_{t+1}^*)$ √© n√£o correlacionado com $X_t$. Ou seja:
$$
E[(Z_{t+1} - Z_{t+1}^*)X_t'] = 0.
$$
V. Substituindo $Z_{t+1} = H'Y_{t+1}$ e $Z_{t+1}^* = H'Y_{t+1}^* = H' \alpha'X_t$, temos:
$$
E[(H'Y_{t+1} - H'\alpha' X_t)X_t'] = E[H'(Y_{t+1} - \alpha' X_t)X_t'].
$$
VI. Usando a propriedade de linearidade da esperan√ßa, temos:
$$
H'E[(Y_{t+1} - \alpha' X_t)X_t'] = H' \cdot 0 = 0.
$$
VII. O termo $E[(Y_{t+1} - \alpha' X_t)X_t'] = 0$ porque $\alpha'X_t$ √© a proje√ß√£o linear de $Y_{t+1}$ em $X_t$, e, portanto, o erro $(Y_{t+1} - \alpha'X_t)$ √© n√£o correlacionado com $X_t$.
VIII. Portanto, $Z_{t+1}^* = H' \alpha' X_t$ √© a proje√ß√£o linear de $Z_{t+1}$ em $X_t$.
IX. Dado que $Z_{t+1}^* = H'Y_{t+1}^*$, onde $Y_{t+1}^*$ √© a proje√ß√£o linear de $Y_{t+1}$ em $X_t$, ent√£o  $Z_{t+1}^*$ minimiza o MSE de $Z_{t+1}$ dentro da classe das previs√µes lineares de $X_t$.
$\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Vamos supor que temos uma vari√°vel $Y_{t+1}$ representando o pre√ßo de uma a√ß√£o e $X_t$ representando o volume de negocia√ß√£o dessa a√ß√£o no tempo $t$. Ap√≥s a an√°lise, a proje√ß√£o linear de $Y_{t+1}$ em $X_t$ √© dada por:
>
> $$Y_{t+1}^* = 2.5 + 0.05X_t$$
>
> Isso significa que, para cada unidade de aumento no volume de negocia√ß√£o ($X_t$), o pre√ßo da a√ß√£o ($Y_{t+1}$) tende a aumentar em 0.05, al√©m de um valor base de 2.5. Agora, imagine que estamos interessados em uma nova vari√°vel $Z_{t+1}$, que √© uma combina√ß√£o linear de $Y_{t+1}$, dada por $Z_{t+1} = 2Y_{t+1} - 1$. Usando o Teorema 3.1, a proje√ß√£o linear de $Z_{t+1}$ em $X_t$ ser√°:
>
> $$Z_{t+1}^* = 2Y_{t+1}^* - 1 = 2(2.5 + 0.05X_t) - 1$$
> $$Z_{t+1}^* = 5 + 0.1X_t - 1 = 4 + 0.1X_t$$
>
> Isso demonstra que, para prever $Z_{t+1}$, aplicamos a mesma combina√ß√£o linear √† proje√ß√£o de $Y_{t+1}$. Por exemplo, se o volume de negocia√ß√£o $X_t$ for 100, ent√£o:
>
>  $$Y_{t+1}^* = 2.5 + 0.05 \times 100 = 7.5$$
>
>  $$Z_{t+1}^* = 4 + 0.1 \times 100 = 14$$
>
> A proje√ß√£o de $Z_{t+1}$ ser√° 14 quando $X_t=100$. Este exemplo ilustra que a propriedade de m√≠nimo MSE se mant√©m n√£o s√≥ para a vari√°vel original, mas tamb√©m para qualquer combina√ß√£o linear desta, simplificando a an√°lise de diferentes transforma√ß√µes de vari√°veis.

**Corol√°rio 3.1:** Se $Y_{t+1}$ √© um escalar, ent√£o para qualquer constante $a$ e $b$ a proje√ß√£o linear de $(aY_{t+1}+b)$ em $X_t$ √© dada por $aP(Y_{t+1}|X_t) + b$, onde $P(Y_{t+1}|X_t)$ √© a proje√ß√£o linear de $Y_{t+1}$ em $X_t$.

*Prova.*
I. Se $Y_{t+1}$ √© um escalar, a matriz $H$ √© um escalar $a$.
II. Seja $Z_{t+1} = aY_{t+1} + b$, usando o Teorema 3.1, sabemos que a proje√ß√£o linear de $Z_{t+1}$ em $X_t$ √© dada por
$$Z_{t+1}^* = aP(Y_{t+1}|X_t) + b$$
III. Assim, para qualquer combina√ß√£o linear de $Y_{t+1}$, o resultado se mant√©m.
$\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Continuando o exemplo anterior, suponha que $Y_{t+1}$ representa o pre√ßo de uma a√ß√£o, e a proje√ß√£o linear de $Y_{t+1}$ em $X_t$ (volume de negocia√ß√£o) seja:
>
>  $$P(Y_{t+1}|X_t) =  2.5 + 0.05X_t$$
>
> Agora, considere uma transforma√ß√£o linear de $Y_{t+1}$:
>
> $$Z_{t+1} = 0.5Y_{t+1} + 10$$
>
> Pelo Corol√°rio 3.1, a proje√ß√£o linear de $Z_{t+1}$ em $X_t$ √©:
>
> $$Z_{t+1}^* = 0.5P(Y_{t+1}|X_t) + 10$$
> $$Z_{t+1}^* = 0.5(2.5 + 0.05X_t) + 10$$
> $$Z_{t+1}^* = 1.25 + 0.025X_t + 10$$
> $$Z_{t+1}^* = 11.25 + 0.025X_t$$
>
> Se $X_t = 100$, ent√£o:
>
> $$Z_{t+1}^* = 11.25 + 0.025 \times 100 = 11.25 + 2.5 = 13.75$$
>
> Este exemplo num√©rico demonstra que, ao projetar uma combina√ß√£o linear de $Y_{t+1}$ em $X_t$, podemos simplesmente aplicar a mesma transforma√ß√£o linear √† proje√ß√£o de $Y_{t+1}$ em $X_t$. Essa propriedade facilita o c√°lculo e an√°lise de transforma√ß√µes lineares das vari√°veis de interesse.

**Teorema 3.2** A proje√ß√£o linear $Y_{t+1}^*$ tamb√©m minimiza o MSE para qualquer combina√ß√£o linear dos elementos de $Y_{t+1}$ condicional a $X_t$ .

*Prova*
I.  Do Teorema 3.1, sabemos que $Z_{t+1}^* = H'Y_{t+1}^*$ √© a proje√ß√£o linear de $Z_{t+1}$ em $X_t$, onde $Z_{t+1} = H'Y_{t+1}$.
II.  Pela defini√ß√£o de proje√ß√£o linear, o erro $Z_{t+1} - Z_{t+1}^*$ √© n√£o correlacionado com $X_t$, ou seja, $E[(Z_{t+1} - Z_{t+1}^*)X_t'] = 0$.
III.  A propriedade de n√£o correla√ß√£o implica que a covari√¢ncia entre o erro e $X_t$ √© zero, ou seja, $Cov(Z_{t+1} - Z_{t+1}^*, X_t) = 0$.
IV.  Para qualquer fun√ß√£o $g(X_t)$, tamb√©m se verifica que $Cov(Z_{t+1} - Z_{t+1}^*, g(X_t)) = 0$, desde que $g(X_t)$ seja uma combina√ß√£o linear de $X_t$.
V.  A propriedade da proje√ß√£o linear implica que o erro da proje√ß√£o √© ortogonal a qualquer fun√ß√£o linear de $X_t$. Portanto, $E[(Z_{t+1} - Z_{t+1}^*)g(X_t)'] = 0$.
VI. Considere agora o MSE condicional $E[(Z_{t+1} - \hat{Z}_{t+1})^2 | X_t]$, onde $\hat{Z}_{t+1}$ √© um preditor condicional qualquer de $Z_{t+1}$ dado $X_t$. Podemos decompor o erro em rela√ß√£o √† proje√ß√£o linear $Z_{t+1}^*$ como:

$E[(Z_{t+1} - \hat{Z}_{t+1})^2 | X_t] = E[(Z_{t+1} - Z_{t+1}^* + Z_{t+1}^* - \hat{Z}_{t+1})^2 | X_t] $

$ = E[(Z_{t+1} - Z_{t+1}^*)^2|X_t] + E[(Z_{t+1}^* - \hat{Z}_{t+1})^2|X_t] + 2E[(Z_{t+1} - Z_{t+1}^*)(Z_{t+1}^* - \hat{Z}_{t+1})|X_t]$

VII. O √∫ltimo termo √© zero pois  $Z_{t+1}^* - \hat{Z}_{t+1}$ √© uma fun√ß√£o linear de $X_t$ e o erro da proje√ß√£o $Z_{t+1} - Z_{t+1}^*$ √© ortogonal a qualquer fun√ß√£o linear de $X_t$. Ent√£o:

$E[(Z_{t+1} - \hat{Z}_{t+1})^2 | X_t] =  E[(Z_{t+1} - Z_{t+1}^*)^2|X_t] + E[(Z_{t+1}^* - \hat{Z}_{t+1})^2|X_t]$

VIII. Como o termo $E[(Z_{t+1}^* - \hat{Z}_{t+1})^2|X_t]$ √© sempre n√£o negativo, o MSE condicional √© minimizado quando $Z_{t+1}^* = \hat{Z}_{t+1}$. Portanto, a proje√ß√£o linear $Z_{t+1}^*$ minimiza o MSE condicional de $Z_{t+1}$ dado $X_t$.
IX. Como $Z_{t+1}^* = H'Y_{t+1}^*$ e  $Y_{t+1}^*$ √© a proje√ß√£o linear de $Y_{t+1}$ em $X_t$, ent√£o a proje√ß√£o linear $Y_{t+1}^*$ minimiza o MSE condicional para qualquer combina√ß√£o linear de $Y_{t+1}$.

$\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Suponha que a proje√ß√£o linear de $Y_{t+1}$ em $X_t$ √© dada por $Y_{t+1}^* = 10 + 0.5X_t$. Queremos avaliar o MSE condicional para a previs√£o de uma vari√°vel transformada $Z_{t+1} = 2Y_{t+1} + 5$.
> Pelo Teorema 3.2, a proje√ß√£o linear de $Z_{t+1}$ √© $Z_{t+1}^* = 2Y_{t+1}^* + 5 = 2(10 + 0.5X_t) + 5 = 25 + X_t$.
>
> Vamos calcular o MSE condicional para uma observa√ß√£o onde $X_t = 20$:
>
> 1.  **Proje√ß√£o de** $Y_{t+1}$: $Y_{t+1}^* = 10 + 0.5 \times 20 = 20$
> 2.  **Proje√ß√£o de** $Z_{t+1}$: $Z_{t+1}^* = 25 + 20 = 45$
>
> Suponha que o valor real de $Y_{t+1}$ √© 22 e o valor real de $Z_{t+1} = 2 \times 22 + 5 = 49$.
>
> *   O erro na proje√ß√£o de $Y_{t+1}$ √© $Y_{t+1} - Y_{t+1}^* = 22 - 20 = 2$.
> *   O erro na proje√ß√£o de $Z_{t+1}$ √© $Z_{t+1} - Z_{t+1}^* = 49 - 45 = 4$.
>
> O MSE condicional √© dado por $E[(Z_{t+1} - Z_{t+1}^*)^2 | X_t]$. Em um caso geral, precisar√≠amos de um conjunto de dados para calcular essa esperan√ßa. Para este exemplo, vamos considerar que este seja o √∫nico ponto de dados dispon√≠vel e usar o erro ao quadrado como uma estimativa do MSE condicional. Para $Z_{t+1}$, o MSE √© $4^2 = 16$.
>
> Qualquer outro preditor de $Z_{t+1}$ com base em $X_t$, digamos $\hat{Z}_{t+1} = 40 + 0.8X_t$, levaria a um MSE maior, como demonstrado pelo Teorema 3.2. Por exemplo, $\hat{Z}_{t+1} = 40 + 0.8 \times 20 = 56$. O erro seria $49-56 = -7$ e o MSE seria $(-7)^2=49$.  Este exemplo demonstra que a proje√ß√£o linear $Z_{t+1}^*$ de fato minimiza o MSE condicional.

### Conclus√£o
Esta se√ß√£o demonstrou que a proje√ß√£o linear $Y_{t+1}^*$ n√£o apenas minimiza o MSE para $Y_{t+1}$, mas tamb√©m fornece a previs√£o de MSE m√≠nimo para qualquer combina√ß√£o linear dos elementos de $Y_{t+1}$, dado $X_t$ [^1]. O Teorema 3.1 formaliza este resultado atrav√©s da demonstra√ß√£o que a proje√ß√£o de uma combina√ß√£o linear de $Y_{t+1}$ em $X_t$ √© dada pela mesma combina√ß√£o linear da proje√ß√£o de $Y_{t+1}$ em $X_t$, mantendo a propriedade de ortogonalidade do erro. Este resultado √© de grande import√¢ncia pr√°tica e te√≥rica, refor√ßando a aplicabilidade e a robustez da proje√ß√£o linear em diversas situa√ß√µes de previs√£o, mostrando que a proje√ß√£o linear oferece previs√µes √≥timas para qualquer combina√ß√£o linear das vari√°veis originais com base no mesmo conjunto de vari√°veis explicativas. O Teorema 3.2 estende este resultado ao demonstrar que a proje√ß√£o linear tamb√©m minimiza o MSE condicional, o que refor√ßa ainda mais a otimalidade do uso da proje√ß√£o linear.

### Refer√™ncias
[^1]: Express√µes [4.1.1], [4.1.4], [4.1.11].
[^2]: Express√µes [4.1.9], [4.1.10], [4.1.13], [4.1.21], [4.1.22].
[^3]: Express√µes [4.1.14], [4.1.23].
[^4]: Corol√°rio 2.1
[^5]: Express√£o [5.1.5].
### 5.2. Likelihood Function for an AR(1) Process
Consider the AR(1) process:
$$Y_t = c + \phi Y_{t-1} + \epsilon_t$$
where $\epsilon_t \sim i.i.d. N(0, \sigma^2)$ [^5].  Our goal is to derive the likelihood function, given a sample of size $T$, denoted as $(y_1, y_2, \ldots, y_T)$. For simplicity, assume initially that the intercept $c=0$.  The joint density of the errors, given the parameters, is:
$$f(\epsilon_1, \epsilon_2, \ldots, \epsilon_T | \phi, \sigma^2) = \prod_{t=1}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\epsilon_t^2}{2\sigma^2}\right)$$
Since $Y_t = \phi Y_{t-1} + \epsilon_t$, it follows that $\epsilon_t = Y_t - \phi Y_{t-1}$.  Thus, we can write the likelihood function in terms of the observed data $Y_t$, as
$$L(\phi, \sigma^2 | y_1, y_2, \ldots, y_T) = \prod_{t=1}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_t - \phi y_{t-1})^2}{2\sigma^2}\right)$$
It is common practice to work with the log-likelihood function, as it simplifies calculations and does not change the location of the maximum. The log-likelihood is given by
$$log L(\phi, \sigma^2 | y_1, y_2, \ldots, y_T) = -\frac{T}{2}log(2\pi) - \frac{T}{2}log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=1}^T (y_t - \phi y_{t-1})^2$$
The above equation represents the likelihood function conditioned on $y_0$.  We will address the implications of conditioning on $y_0$ later, but for now it suffices. To find the MLE, one would need to maximize the above log-likelihood function with respect to $\phi$ and $\sigma^2$. This optimization is typically done numerically.

> üí° **Exemplo Num√©rico:**
>
> Vamos supor que temos uma s√©rie temporal com 5 observa√ß√µes: $y = [10, 12, 15, 13, 16]$. Queremos ajustar um modelo AR(1) com $c=0$. Usando a f√≥rmula da log-verossimilhan√ßa, temos:
>
>  $$log L(\phi, \sigma^2 | y) = -\frac{5}{2}log(2\pi) - \frac{5}{2}log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=1}^5 (y_t - \phi y_{t-1})^2$$
>
> Expandindo o somat√≥rio:
>
> $$\sum_{t=1}^5 (y_t - \phi y_{t-1})^2 = (y_1 - \phi y_0)^2 + (y_2 - \phi y_1)^2 + (y_3 - \phi y_2)^2 + (y_4 - \phi y_3)^2 + (y_5 - \phi y_4)^2 $$
>
> Como $y_0$ n√£o est√° dispon√≠vel, usaremos $y_0=0$ para simplificar. Substituindo os valores temos:
>
> $$\sum_{t=1}^5 (y_t - \phi y_{t-1})^2 = (10 - \phi \cdot 0)^2 + (12 - \phi \cdot 10)^2 + (15 - \phi \cdot 12)^2 + (13 - \phi \cdot 15)^2 + (16 - \phi \cdot 13)^2 $$
>
> Para encontrar os valores de $\phi$ e $\sigma^2$ que maximizam essa fun√ß√£o de log-verossimilhan√ßa, precisamos usar m√©todos num√©ricos. Usando Python com `scipy.optimize.minimize`, podemos fazer:
>
> ```python
> import numpy as np
> from scipy.optimize import minimize
>
> y = np.array([10, 12, 15, 13, 16])
>
> def neg_log_likelihood(params, data):
>     phi, sigma2 = params
>     T = len(data)
>     y_0 = 0 # Assuming y_0 = 0
>     sum_sq_errors = (data[0] - phi*y_0)**2
>     for t in range(1, T):
>        sum_sq_errors += (data[t] - phi*data[t-1])**2
>     return T/2 * np.log(2*np.pi) + T/2 * np.log(sigma2) + 1/(2*sigma2) * sum_sq_errors
>
> # Initial guess for parameters
> initial_params = [0.5, 1]
>
> # Optimization
> results = minimize(neg_log_likelihood, initial_params, args=(y,), method='L-BFGS-B', bounds=((-1, 1), (0.001, None)))
> best_phi, best_sigma2 = results.x
> print(f"MLE of phi: {best_phi:.4f}")
> print(f"MLE of sigma^2: {best_sigma2:.4f}")
> ```
>
> Este c√≥digo busca os par√¢metros $\phi$ e $\sigma^2$ que minimizam o negativo da fun√ß√£o de log-verossimilhan√ßa, que √© equivalente a maximizar a fun√ß√£o de log-verossimilhan√ßa. A sa√≠da mostrar√° os melhores valores para $\phi$ e $\sigma^2$ que se ajustam aos dados. A restri√ß√£o de $\phi$ estar entre -1 e 1 √© para garantir a estacionariedade do modelo AR(1).

**Lema 5.1** (Propriedades do Erro no Modelo AR(1))
Para o modelo AR(1) com $Y_t = \phi Y_{t-1} + \epsilon_t$, onde $\epsilon_t \sim i.i.d. N(0, \sigma^2)$, as seguintes propriedades se aplicam:
    I. $E[\epsilon_t] = 0$
    II. $Var(\epsilon_t) = \sigma^2$
    III. $Cov(\epsilon_t, \epsilon_s) = 0$ para $t \neq s$
    IV. $E[Y_t \epsilon_s] = 0$ para $s < t$

*Prova*

I.  A esperan√ßa de $\epsilon_t$ √© zero pela defini√ß√£o $\epsilon_t \sim N(0,\sigma^2)$.
II. A vari√¢ncia de $\epsilon_t$ √© $\sigma^2$ pela defini√ß√£o $\epsilon_t \sim N(0,\sigma^2)$.
III. A covari√¢ncia entre $\epsilon_t$ e $\epsilon_s$ √© zero para $t \ne s$ porque os erros s√£o independentes e identicamente distribu√≠dos.
IV.  Para demonstrar $E[Y_t \epsilon_s] = 0$ para $s<t$, escrevemos $Y_t$ em fun√ß√£o dos erros passados:
$$Y_t = \epsilon_t + \phi \epsilon_{t-1} + \phi^2 \epsilon_{t-2} + \cdots $$
Ent√£o,
$$E[Y_t \epsilon_s] = E[(\epsilon_t + \phi \epsilon_{t-1} + \phi^2 \epsilon_{t-2} + \cdots) \epsilon_s]$$
Como $\epsilon_t$ √© independente de $\epsilon_s$ para $t \ne s$, ent√£o $E[\epsilon_t \epsilon_s] = 0$ para $t\ne s$. Para $t=s$, temos $E[\epsilon_t^2] = \sigma^2$. Mas no termo $E[Y_t \epsilon_s]$ temos que $s < t$, logo $E[\epsilon_t \epsilon_s]=0$, concluindo que $E[Y_t \epsilon_s]=0$.
$\blacksquare$

### 5.3. Likelihood Function for an AR(p) Process
The analysis for the AR(1) case can be extended to the AR(p) case:
$$Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t$$
where $\epsilon_t \sim i.i.d. N(0, \sigma^2)$[^5].  Again, assuming initially that $c = 0$, the error term can be expressed as:
$$\epsilon_t = Y_t - \phi_1 Y_{t-1} - \phi_2 Y_{t-2} - \ldots - \phi_p Y_{t-p}$$
Following the same approach as in the AR(1) case, the log-likelihood function, conditional on the first $p$ observations $(y_1, y_2, \ldots, y_p)$ is
$$log L(\phi_1, \phi_2, \ldots, \phi_p, \sigma^2 | y_1, y_2, \ldots, y_T) = -\frac{T-p}{2}log(2\pi) - \frac{T-p}{2}log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=p+1}^T (y_t - \phi_1 y_{t-1} - \ldots - \phi_p y_{t-p})^2$$
Note that the summation now starts at $t = p+1$, because the errors $\epsilon_1$ to $\epsilon_p$ are not defined by the model. The likelihood function is conditioned on the first $p$ observations. This difference between $T$ and $T-p$ becomes immaterial as $T$ becomes large. This expression can be maximized to obtain the MLE of the parameters ($\phi_1$, ..., $\phi_p$, $\sigma^2$). The optimization is typically carried out numerically.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos uma s√©rie temporal de 7 observa√ß√µes: $y = [5, 7, 9, 12, 14, 16, 18]$, e queremos ajustar um modelo AR(2). Aqui, $p = 2$, e a fun√ß√£o de log-verossimilhan√ßa √©:
>
>  $$log L(\phi_1, \phi_2, \sigma^2 | y) = -\frac{7-2}{2}log(2\pi) - \frac{7-2}{2}log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=3}^7 (y_t - \phi_1 y_{t-1} - \phi_2 y_{t-2})^2$$
>
> Expandindo o somat√≥rio:
>  $$\sum_{t=3}^7 (y_t - \phi_1 y_{t-1} - \phi_2 y_{t-2})^2 = (y_3 - \phi_1 y_2 - \phi_2 y_1)^2 + (y_4 - \phi_1 y_3 - \phi_2 y_2)^2 + (y_5 - \phi_1 y_4 - \phi_2 y_3)^2 + (y_6 - \phi_1 y_5 - \phi_2 y_4)^2 + (y_7 - \phi_1 y_6 - \phi_2 y_5)^2 $$
>  Substituindo os valores:
>
> $$\sum_{t=3}^7 (y_t - \phi_1 y_{t-1} - \phi_2 y_{t-2})^2 = (9 - \phi_1 7 - \phi_2 5)^2 + (12 - \phi_1 9 - \phi_2 7)^2 + (14 - \phi_1 12 - \phi_2 9)^2 + (16 - \phi_1 14 - \phi_2 12)^2 + (18 - \phi_1 16 - \phi_2 14)^2 $$
>
> Para encontrar os valores de ($\phi_1, \phi_2, \sigma^2$) que maximizam essa log-verossimilhan√ßa, podemos usar otimiza√ß√£o num√©rica em Python:
>
> ```python
> import numpy as np
> from scipy.optimize import minimize
>
> y = np.array([5, 7, 9, 12, 14, 16, 18])
>
> def neg_log_likelihood(params, data):
>     phi1, phi2, sigma2 = params
>     T = len(data)
>     sum_sq_errors = 0
>     for t in range(2, T):
>         sum_sq_errors += (data[t] - phi1*data[t-1] - phi2*data[t-2])**2
>     return (T-2)/2 * np.log(2*np.pi) + (T-2)/2 * np.log(sigma2) + 1/(2*sigma2) * sum_sq_errors
>
> # Initial guess for parameters
> initial_params = [0.5, 0.2, 1]
>
> # Optimization
> results = minimize(neg_log_likelihood, initial_params, args=(y,), method='L-BFGS-B', bounds=((-1, 1),(-1, 1), (0.001, None)))
> best_phi1, best_phi2, best_sigma2 = results.x
> print(f"MLE of phi_1: {best_phi1:.4f}")
> print(f"MLE of phi_2: {best_phi2:.4f}")
> print(f"MLE of sigma^2: {best_sigma2:.4f}")
> ```
>
> Este c√≥digo calcula os MLE dos par√¢metros do modelo AR(2). A fun√ß√£o de log-verossimilhan√ßa √© condicional aos dois primeiros valores da s√©rie. As restri√ß√µes para $\phi_1$ e $\phi_2$ s√£o para garantir a estacionariedade do modelo AR(2).

**Lema 5.2** (Propriedades do Erro no Modelo AR(p))
Para o modelo AR(p) com $Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t$, onde $\epsilon_t \sim i.i.d. N(0, \sigma^2)$, as seguintes propriedades se aplicam:
   I. $E[\epsilon_t] = 0$
   II. $Var(\epsilon_t) = \sigma^2$
   III. $Cov(\epsilon_t, \epsilon_s) = 0$ para $t \neq s$
   IV. $E[Y_t \epsilon_s] = 0$ para $s<t$.

*Prova*

A prova √© an√°loga ao Lema 5.1, pois o termo do erro $\epsilon_t$ √© independente de todos os erros passados, e, portanto, todas as propriedades se mant√©m.

$\blacksquare$

### 5.4. Likelihood Function for an MA(1) Process
Now consider the moving average MA(1) process:
$$Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$$
where $\epsilon_t \sim i.i.d. N(0, \sigma^2)$[^5].  Again, assuming initially that $\mu = 0$,  the likelihood function for the MA(1) is more challenging due to the fact that theerror terms are autocorrelated and are not directly observed.

**Derivation of the Likelihood Function for MA(1)**

To derive the likelihood function, we express $\epsilon_t$ as a function of the observed $y_t$ and the model parameters. From the model equation we have:

$$\epsilon_t = y_t - \theta \epsilon_{t-1}$$

For $t=1$, we have

$$\epsilon_1 = y_1 - \theta \epsilon_0$$

where we need to determine $\epsilon_0$. We assume that the process has been ongoing for a long time, and thus the initial value for epsilon, $\epsilon_0$, can be handled with one of several methods:

1.  **Setting** $\epsilon_0 = 0$: This is a common simplification, especially for long time series. The effect of this initial assumption diminishes over time.

2.  **Backcasting:** Using iterative computation backward in time to get an estimate for $\epsilon_0$.

3.  **Unconditional Maximum Likelihood:** Choosing $\epsilon_0$ that maximizes the likelihood using the conditional expectation of $E[\epsilon_0|y_1,...]$.

For simplicity and common practice, let's proceed by setting $\epsilon_0 = 0$ which makes the first error term $\epsilon_1=y_1$ and subsequent error terms dependent on past error terms and $y$. The likelihood function can then be constructed by recognizing that the likelihood of the sequence of residuals given the model parameters ($\theta, \sigma^2$) can be expressed as the joint density of $\epsilon_t$. Because the errors are i.i.d. Gaussian, the joint density is just the product of the individual densities, and is given as:

$$L(\theta, \sigma^2|y_1, ..., y_T) = \prod_{t=1}^{T} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\epsilon_t^2}{2\sigma^2}\right)$$

Where $\epsilon_t$ is recursively defined as:

$$\epsilon_t = y_t - \theta \epsilon_{t-1} \text{ with } \epsilon_0 = 0$$

By substituting the expression for $\epsilon_t$ into the likelihood, we have a function of the parameters and data.

**Log-Likelihood Function**

The log-likelihood function simplifies the calculations:

$$l(\theta, \sigma^2|y_1, ..., y_T) = -\frac{T}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{t=1}^{T} \epsilon_t^2$$

Or,

$$l(\theta, \sigma^2|y_1, ..., y_T) = -\frac{T}{2}\log(2\pi) - \frac{T}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{t=1}^{T} \epsilon_t^2$$

This log-likelihood is then maximized with respect to $\theta$ and $\sigma^2$ to find the parameter estimates. Maximizing with respect to $\sigma^2$ by taking the derivative, setting to zero, and solving we get:

$$\hat{\sigma}^2 = \frac{1}{T} \sum_{t=1}^{T} \epsilon_t^2$$

which can then be substituted back to get a concentrated likelihood and used to maximize with respect to theta.  Maximizing with respect to $\theta$ does not have an analytical solution, and is usually done numerically.

**Estimation using Numerical Optimization**
Given the lack of closed-form solutions for the parameters, we typically resort to numerical optimization techniques to estimate $\theta$ and $\sigma^2$. Common methods include gradient descent, Newton-Raphson, and variants thereof. Software packages such as R or Python with libraries like `statsmodels` can be used to carry out this optimization for MA(1) models and report the best fit parameters and likelihoods.

<!-- END -->
