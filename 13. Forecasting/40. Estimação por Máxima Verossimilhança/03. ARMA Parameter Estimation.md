## Estima√ß√£o de Par√¢metros ARMA por M√°xima Verossimilhan√ßa: Otimiza√ß√£o e Casos Especiais

### Introdu√ß√£o
Nos cap√≠tulos anteriores, estabelecemos os fundamentos da estima√ß√£o por m√°xima verossimilhan√ßa (MLE) e a constru√ß√£o da fun√ß√£o de verossimilhan√ßa para modelos ARMA (Autoregressive Moving Average) com erros Gaussianos [^5.1], [^Lema 1], [^Teorema 1], [^Teorema 1.1]. Agora, neste cap√≠tulo, vamos nos aprofundar nos detalhes da estima√ß√£o de par√¢metros ARMA por meio da MLE, abordando especificamente as t√©cnicas de otimiza√ß√£o, como Newton-Raphson, e explorando algoritmos alternativos, como o Expectation-Maximization (EM) para lidar com dados faltantes. Al√©m disso, examinaremos o processo computacional necess√°rio para obter estimativas precisas e confi√°veis dos par√¢metros do modelo ARMA, enfatizando a import√¢ncia do controle da converg√™ncia e da escolha de valores iniciais adequados. As discuss√µes anteriores forneceram as ferramentas te√≥ricas e matem√°ticas [^Lema 2], [^Lema 2.1], e este cap√≠tulo tem como objetivo aplicar esse conhecimento em situa√ß√µes pr√°ticas, comumente encontradas na an√°lise de s√©ries temporais.

### Otimiza√ß√£o da Fun√ß√£o de Verossimilhan√ßa: M√©todos de Newton-Raphson e Varia√ß√µes
Como mencionado anteriormente [^Proposi√ß√£o 1], o m√©todo de Newton-Raphson √© uma t√©cnica iterativa de segunda ordem que pode ser usada para maximizar a fun√ß√£o de log-verossimilhan√ßa para modelos ARMA. Este m√©todo requer o c√°lculo da primeira e segunda derivadas da fun√ß√£o de log-verossimilhan√ßa em rela√ß√£o aos par√¢metros do modelo.

> üí° **Exemplo Num√©rico:** Suponha que temos um modelo AR(1) com par√¢metro $\phi$ e vari√¢ncia do erro $\sigma^2$.  A cada itera√ß√£o $i$, o m√©todo de Newton-Raphson atualiza as estimativas dos par√¢metros $\phi_i$ e $\sigma^2_i$ usando as seguintes equa√ß√µes:
>  $$ \begin{bmatrix} \phi_{i+1} \\ \sigma^2_{i+1} \end{bmatrix} = \begin{bmatrix} \phi_{i} \\ \sigma^2_{i} \end{bmatrix} - [H(\theta_i)]^{-1} \nabla L(\theta_i) $$
>  Onde $\nabla L(\theta_i)$ √© o vetor gradiente da fun√ß√£o de log-verossimilhan√ßa avaliada nos par√¢metros atuais $\phi_i$ e $\sigma^2_i$, e  $H(\theta_i)$ √© a matriz Hessiana (matriz das segundas derivadas parciais) tamb√©m avaliada nos par√¢metros atuais. A cada itera√ß√£o, o vetor de par√¢metros √© atualizado na dire√ß√£o que maximiza a fun√ß√£o de verossimilhan√ßa. A converg√™ncia ocorre quando a atualiza√ß√£o dos par√¢metros se torna desprez√≠vel ou a verossimilhan√ßa atinge um valor m√°ximo.
>  
>  Vamos considerar um exemplo num√©rico com dados simulados. Suponha que temos uma s√©rie temporal simulada de um modelo AR(1) com $\phi = 0.7$ e $\sigma^2 = 1$.
>  
> ```python
>  import numpy as np
>  import pandas as pd
>  from scipy.optimize import minimize
> 
>  np.random.seed(42)
>  n = 100
>  phi_true = 0.7
>  sigma2_true = 1
>  errors = np.random.normal(0, np.sqrt(sigma2_true), n)
>  y = np.zeros(n)
>  y[0] = errors[0]
>  for t in range(1, n):
>      y[t] = phi_true * y[t-1] + errors[t]
> 
>  # Fun√ß√£o de log-verossimilhan√ßa para AR(1)
>  def log_likelihood_ar1(params, data):
>      phi, sigma2 = params
>      n = len(data)
>      errors = np.zeros(n)
>      errors[0] = data[0]
>      for t in range(1,n):
>        errors[t] = data[t] - phi * data[t-1]
>      ll = -0.5 * n * np.log(2*np.pi*sigma2) - 0.5 * np.sum(errors**2)/sigma2
>      return -ll # Negativo para minimizar
> 
>  # Derivada (gradiente) da log-verossimilhan√ßa (calculada analiticamente para simplificar)
>  def gradient_log_likelihood_ar1(params, data):
>      phi, sigma2 = params
>      n = len(data)
>      errors = np.zeros(n)
>      errors[0] = data[0]
>      for t in range(1,n):
>        errors[t] = data[t] - phi * data[t-1]
>      
>      d_phi = np.sum(errors[1:] * data[:-1])/sigma2
>      d_sigma2 = -n/(2*sigma2) + np.sum(errors**2)/(2*sigma2**2)
>      return -np.array([d_phi, d_sigma2])
> 
>  # Matriz Hessiana da log-verossimilhan√ßa (calculada analiticamente para simplificar)
>  def hessian_log_likelihood_ar1(params, data):
>    phi, sigma2 = params
>    n = len(data)
>    d2_phi2 = -np.sum(data[:-1]**2) / sigma2
>    d2_sigma2_2 = n/(2*sigma2**2) - np.sum(errors**2)/sigma2**3
>    d2_phi_sigma2 = 0
>    
>    return -np.array([[d2_phi2, d2_phi_sigma2], [d2_phi_sigma2, d2_sigma2_2]])
> 
>  # Inicializa√ß√£o
>  initial_params = np.array([0.1, 0.5])  # Valores iniciais para phi e sigma2
> 
>  # Otimiza√ß√£o com Newton-Raphson (usando minimize com m√©todo trust-ncg para simular Newton-Raphson)
>  result = minimize(log_likelihood_ar1, initial_params, method='trust-ncg',jac=gradient_log_likelihood_ar1, hess=hessian_log_likelihood_ar1, args=(y,))
>  phi_mle, sigma2_mle = result.x
>  
>  print(f"Estimativas MLE:\n  phi = {phi_mle:.4f}\n  sigma2 = {sigma2_mle:.4f}")
> ```
>
>  Neste exemplo, a fun√ß√£o `log_likelihood_ar1` calcula o valor negativo da log-verossimilhan√ßa (j√° que o `minimize` busca o m√≠nimo), e o `gradient_log_likelihood_ar1` calcula o gradiente analiticamente, e `hessian_log_likelihood_ar1` calcula a Hessiana. Utilizamos `scipy.optimize.minimize` com o m√©todo `trust-ncg` para simular o m√©todo de Newton-Raphson usando o gradiente e a Hessiana. As estimativas obtidas por MLE s√£o $\hat{\phi} = 0.686$ e $\hat{\sigma}^2 = 0.975$, que s√£o pr√≥ximas dos valores reais usados na simula√ß√£o.

O m√©todo de Newton-Raphson, conforme [^Proposi√ß√£o 2], utiliza uma atualiza√ß√£o que leva em conta tanto a inclina√ß√£o quanto a curvatura da fun√ß√£o de verossimilhan√ßa, geralmente proporcionando uma converg√™ncia mais r√°pida do que m√©todos de primeira ordem [^Proposi√ß√£o 2.1]. No entanto, este m√©todo exige o c√°lculo da matriz Hessiana, que pode ser computacionalmente custoso e complexo para modelos ARMA de alta ordem.

**Corol√°rio 1:** *Em situa√ß√µes onde o c√°lculo da matriz Hessiana √© computacionalmente proibitivo, m√©todos quase-Newton, como o BFGS (Broyden-Fletcher-Goldfarb-Shanno), oferecem uma alternativa vi√°vel. Esses m√©todos aproximam a matriz Hessiana usando informa√ß√µes das itera√ß√µes anteriores e, portanto, evitam o c√°lculo direto das segundas derivadas, com boa performance.*

> üí° **Exemplo Num√©rico:** O algoritmo BFGS realiza a mesma itera√ß√£o do m√©todo de Newton-Raphson, mas em vez de calcular a Hessiana $H(\theta_i)$ a cada itera√ß√£o, o m√©todo atualiza uma aproxima√ß√£o da Hessiana usando as informa√ß√µes dos gradientes das itera√ß√µes anteriores. Suponha que temos $\theta_i$ como a estimativa dos par√¢metros e que o gradiente da log-verossimilhan√ßa seja $\nabla L(\theta_i)$  e uma aproxima√ß√£o da inversa da Hessiana seja $H^{-1}_i$. A atualiza√ß√£o dos par√¢metros para a pr√≥xima itera√ß√£o $i+1$ se daria da seguinte forma:
>
> 1.  Calculamos a dire√ß√£o de busca $p_i = -H^{-1}_i \nabla L(\theta_i)$.
>
> 2.  Atualizamos os par√¢metros $\theta_{i+1} = \theta_i + \alpha_i p_i$, onde $\alpha_i$ √© o tamanho do passo.
>
> 3.  Atualizamos a aproxima√ß√£o da inversa da Hessiana $H^{-1}_{i+1}$ usando a informa√ß√£o dos gradientes $\nabla L(\theta_{i+1})$ e $\nabla L(\theta_{i})$.
>
>
> Essa aproxima√ß√£o da Hessiana reduz o custo computacional de cada itera√ß√£o, sem comprometer muito a performance da otimiza√ß√£o.
>
>  Vamos usar o mesmo conjunto de dados simulados do exemplo anterior e aplicar o m√©todo BFGS:
>  
>  ```python
>  # Otimiza√ß√£o com BFGS
>  result_bfgs = minimize(log_likelihood_ar1, initial_params, method='BFGS', jac=gradient_log_likelihood_ar1, args=(y,))
>  phi_mle_bfgs, sigma2_mle_bfgs = result_bfgs.x
>  
>  print(f"Estimativas MLE (BFGS):\n  phi = {phi_mle_bfgs:.4f}\n  sigma2 = {sigma2_mle_bfgs:.4f}")
>  ```
>
>  Neste caso, utilizamos o mesmo gradiente e a mesma fun√ß√£o de log-verossimilhan√ßa do exemplo anterior, mas o m√©todo de otimiza√ß√£o foi alterado para `BFGS`. As estimativas obtidas com BFGS s√£o $\hat{\phi} = 0.686$ e $\hat{\sigma}^2 = 0.975$, que s√£o muito pr√≥ximas das estimativas de Newton-Raphson e tamb√©m pr√≥ximas dos valores reais dos par√¢metros. Em geral, m√©todos como BFGS s√£o mais eficientes computacionalmente que Newton-Raphson, especialmente quando o n√∫mero de par√¢metros √© grande.

**Lema 3:** *A converg√™ncia do m√©todo de Newton-Raphson, e suas varia√ß√µes, depende das propriedades da fun√ß√£o de log-verossimilhan√ßa, especificamente sua concavidade e diferenciabilidade. Se a fun√ß√£o de log-verossimilhan√ßa for estritamente c√¥ncava e as derivadas forem cont√≠nuas, e o valor inicial estiver suficientemente perto do m√°ximo global, o m√©todo converge quadraticamente.*

*Prova do Lema 3:*
I.  O m√©todo de Newton-Raphson busca iterativamente a raiz da derivada da fun√ß√£o, que corresponde ao m√°ximo (ou m√≠nimo) da fun√ß√£o original. A atualiza√ß√£o em cada itera√ß√£o √© dada por:
$$ \theta_{i+1} = \theta_i - [H(\theta_i)]^{-1} \nabla L(\theta_i) $$
onde $\theta_i$ √© o vetor de par√¢metros na itera√ß√£o $i$, $\nabla L(\theta_i)$ √© o gradiente da fun√ß√£o de log-verossimilhan√ßa em $\theta_i$, e $H(\theta_i)$ √© a matriz Hessiana.
II.  Se a fun√ß√£o de log-verossimilhan√ßa for estritamente c√¥ncava, ent√£o a matriz Hessiana √© definida negativa, garantindo que a itera√ß√£o do m√©todo se mova na dire√ß√£o do m√°ximo.
III. Se a fun√ß√£o de log-verossimilhan√ßa for duas vezes diferenci√°vel com derivadas cont√≠nuas, ent√£o o Teorema de Taylor nos diz que podemos aproximar a fun√ß√£o de log-verossimilhan√ßa em torno do m√°ximo local $\theta^*$ como:
$$L(\theta) \approx L(\theta^*) + \nabla L(\theta^*) (\theta-\theta^*) + \frac{1}{2} (\theta-\theta^*)^T H(\theta^*) (\theta-\theta^*)$$
Como $\theta^*$ √© um m√°ximo,  $\nabla L(\theta^*) = 0$.
IV. Quando o ponto inicial $\theta_0$ estiver suficientemente perto do m√°ximo global $\theta^*$, e as condi√ß√µes acima forem satisfeitas, a sequ√™ncia de itera√ß√µes $\theta_i$ converge quadraticamente para $\theta^*$. Isso significa que o n√∫mero de d√≠gitos corretos das estimativas dobra a cada itera√ß√£o, levando a uma converg√™ncia r√°pida.
Portanto, provamos que sob as condi√ß√µes de concavidade, diferenciabilidade e um bom valor inicial, o m√©todo de Newton-Raphson converge quadraticamente para o m√°ximo global.‚ñ†

Al√©m dos m√©todos de Newton-Raphson e BFGS, outro m√©todo popular para otimiza√ß√£o da fun√ß√£o de log-verossimilhan√ßa √© o m√©todo do Gradiente Descendente, que utiliza apenas as primeiras derivadas.

**Proposi√ß√£o 3:** *O m√©todo do Gradiente Descendente atualiza os par√¢metros iterativamente na dire√ß√£o oposta ao gradiente da fun√ß√£o de log-verossimilhan√ßa. Em cada itera√ß√£o, os par√¢metros s√£o atualizados como:*
$$ \theta_{i+1} = \theta_i - \alpha \nabla L(\theta_i) $$
*onde $\alpha$ √© um fator de ajuste do tamanho do passo. O m√©todo converge para o m√°ximo local (ou global, se a fun√ß√£o for convexa) quando o gradiente se aproxima de zero.*

> üí° **Exemplo Num√©rico:** Considere novamente o modelo AR(1). O m√©todo do Gradiente Descendente atualiza os par√¢metros $\phi_i$ e $\sigma^2_i$ com base no gradiente da fun√ß√£o de log-verossimilhan√ßa. O tamanho do passo $\alpha$ controla a velocidade de converg√™ncia. Um valor muito pequeno pode levar a uma converg√™ncia lenta, enquanto um valor muito grande pode levar a oscila√ß√µes em torno do √≥timo e impedir a converg√™ncia. A principal vantagem do Gradiente Descendente √© a simplicidade de implementa√ß√£o, j√° que n√£o requer o c√°lculo de derivadas de segunda ordem. No entanto, a converg√™ncia √© normalmente mais lenta que a do Newton-Raphson.
>
>  Usando os mesmos dados simulados e a mesma fun√ß√£o de log-verossimilhan√ßa:
>  ```python
>  # M√©todo do Gradiente Descendente
>  def gradient_descent(initial_params, data, learning_rate, n_iter):
>      params = initial_params.copy()
>      for i in range(n_iter):
>          grad = gradient_log_likelihood_ar1(params, data)
>          params = params - learning_rate * grad
>      return params
> 
>  learning_rate = 0.01
>  n_iter = 500
>  params_gd = gradient_descent(initial_params, y, learning_rate, n_iter)
>  phi_mle_gd, sigma2_mle_gd = params_gd
>  print(f"Estimativas MLE (Gradiente Descendente):\n  phi = {phi_mle_gd:.4f}\n  sigma2 = {sigma2_mle_gd:.4f}")
> ```
>  
>  Neste exemplo, implementamos o Gradiente Descendente de forma iterativa, onde `learning_rate` √© o tamanho do passo, e `n_iter` o n√∫mero de itera√ß√µes.  As estimativas obtidas por gradiente descendente foram $\hat{\phi} = 0.685$ e $\hat{\sigma}^2 = 0.976$ (ap√≥s 500 itera√ß√µes), o que mostra que o Gradiente Descendente tamb√©m converge para valores pr√≥ximos dos valores reais, mas pode precisar de mais itera√ß√µes para convergir.

### Algoritmo Expectation-Maximization (EM) para Dados Faltantes
Em muitos cen√°rios do mundo real, dados faltantes s√£o um problema comum. O algoritmo Expectation-Maximization (EM) √© uma t√©cnica iterativa projetada para encontrar estimativas de m√°xima verossimilhan√ßa em situa√ß√µes com dados incompletos ou dados faltantes. No contexto de modelos ARMA, dados faltantes podem ocorrer de forma aleat√≥ria, ou podem estar associados com determinadas caracter√≠sticas da s√©rie temporal.

**Teorema 2:** *O algoritmo Expectation-Maximization (EM) √© um m√©todo iterativo que alterna entre duas etapas: a etapa de Expecta√ß√£o (E) e a etapa de Maximiza√ß√£o (M). Na etapa E, a esperan√ßa da fun√ß√£o de log-verossimilhan√ßa (completa), dados os dados observados e os par√¢metros atuais, √© calculada. Na etapa M, os par√¢metros do modelo s√£o atualizados para maximizar a esperan√ßa calculada na etapa E.*

> üí° **Exemplo Num√©rico:** Suponha que temos um modelo AR(1) $y_t = \phi y_{t-1} + \epsilon_t$ e dados faltantes em alguns pontos na s√©rie temporal. O algoritmo EM procede da seguinte forma:
>
> 1. **Inicializa√ß√£o**:  Come√ßamos com um palpite inicial para os par√¢metros, por exemplo, $\phi_0$ e $\sigma^2_0$ .
>
> 2. **Passo E (Expecta√ß√£o)**: Dado o palpite inicial, preenchemos os dados faltantes com seus valores esperados usando o modelo AR(1), e calculamos a fun√ß√£o de log-verossimilhan√ßa completa, incluindo os dados observados e "preenchidos".
>
>    a.  Calculamos $\hat{y}_{t|t-1} = \phi_0 y_{t-1}$, e substitu√≠mos os valores faltantes.
>    b.  Calculamos os erros $\epsilon_t$, usando os valores observados e os valores "preenchidos".
>    c.  Calculamos a fun√ß√£o de log-verossimilhan√ßa, usando os erros calculados.
>
> 3. **Passo M (Maximiza√ß√£o)**:  Calculamos a derivada parcial da fun√ß√£o de log-verossimilhan√ßa e encontramos os novos valores dos par√¢metros que maximizam a fun√ß√£o.
>
>    a. Encontramos os novos par√¢metros que maximizam a fun√ß√£o de log-verossimilhan√ßa: $\phi_1$ e $\sigma^2_1$.
>
> 4. **Itera√ß√£o**: Repetimos os passos E e M usando os novos valores dos par√¢metros $\phi_1$ e $\sigma^2_1$, e continuamos iterando at√© a converg√™ncia.
>
>  Para demonstrar o algoritmo EM, vamos simular uma s√©rie temporal AR(1) com alguns dados faltantes.
>  
>  ```python
>  # Simular dados com dados faltantes
>  missing_indices = np.random.choice(n, size=int(0.2*n), replace=False)
>  y_missing = y.copy()
>  y_missing[missing_indices] = np.nan # Substitui alguns valores por NaN
>  
>  # Implementa√ß√£o do algoritmo EM para AR(1)
>  def em_algorithm_ar1(y_missing, initial_params, n_iter):
>     phi, sigma2 = initial_params
>     y_filled = y_missing.copy()
> 
>     for i in range(n_iter):
>        # Passo E: Imputar os dados faltantes
>        for t in range(1, len(y_missing)):
>           if np.isnan(y_filled[t]):
>              y_filled[t] = phi * y_filled[t-1] # Imputa com o valor predito
>
>        # Passo M: Estima os par√¢metros
>        errors = np.zeros(len(y_filled))
>        errors[0] = y_filled[0]
>        for t in range(1,len(y_filled)):
>           errors[t] = y_filled[t] - phi*y_filled[t-1]
>       
>        phi = np.sum(errors[1:]*y_filled[:-1]) / np.sum(y_filled[:-1]**2)
>        sigma2 = np.mean(errors**2)
> 
>     return phi, sigma2
>  
>  initial_params = np.array([0.1, 0.5])
>  n_iter = 100
>  phi_em, sigma2_em = em_algorithm_ar1(y_missing,initial_params,n_iter)
>  print(f"Estimativas MLE (EM):\n  phi = {phi_em:.4f}\n  sigma2 = {sigma2_em:.4f}")
> ```
>
>  Neste exemplo, a fun√ß√£o `em_algorithm_ar1` implementa os passos E e M de forma iterativa.  No passo E, os dados faltantes s√£o preenchidos com os valores preditos pelo modelo AR(1) com os par√¢metros atuais. No passo M, os par√¢metros s√£o reestimados usando os dados "preenchidos". As estimativas obtidas por EM foram $\hat{\phi} = 0.695$ e $\hat{\sigma}^2 = 1.026$, valores pr√≥ximos dos valores reais. O algoritmo EM converge de forma eficiente com poucas itera√ß√µes.

**Prova do Teorema 2:**
I. A fun√ß√£o de verossimilhan√ßa completa, que inclui tanto os dados observados como os dados faltantes, √© denotada por $L_c(\theta; Y, Z)$, onde Y s√£o os dados observados e Z s√£o os dados faltantes. O objetivo √© encontrar os par√¢metros $\theta$ que maximizam a verossimilhan√ßa marginal $L(\theta; Y) = \int L_c(\theta; Y, Z) dZ$, onde a integral √© tomada sobre todos os valores poss√≠veis de $Z$.
II. O algoritmo EM n√£o maximiza diretamente $L(\theta; Y)$ , mas sim maximiza a esperan√ßa da fun√ß√£o de log-verossimilhan√ßa completa, dado os dados observados Y e o valor atual dos par√¢metros $\theta^{(i)}$:
$$ Q(\theta | \theta^{(i)}) = E_{Z|Y, \theta^{(i)}}[\log L_c(\theta; Y, Z)] $$
III. No passo E, calculamos a esperan√ßa $Q(\theta | \theta^{(i)})$. No passo M, atualizamos os par√¢metros:
$$ \theta^{(i+1)} = \arg \max_{\theta} Q(\theta | \theta^{(i)}) $$
O algoritmo EM itera entre os passos E e M at√© que um crit√©rio de converg√™ncia seja satisfeito, como a diferen√ßa entre as estimativas dos par√¢metros em itera√ß√µes consecutivas ser menor que um limite definido.
IV.  O algoritmo EM garante que a verossimilhan√ßa marginal dos dados observados $L(\theta; Y)$ n√£o diminua a cada itera√ß√£o. Isto √©, $L(\theta^{(i+1)}; Y) \geq L(\theta^{(i)}; Y)$. Isso √© uma consequ√™ncia do fato que a cada passo estamos maximizando uma fun√ß√£o menor do que a fun√ß√£o de verossimilhan√ßa.
Portanto, provamos que o algoritmo EM itera entre os passos E (Expecta√ß√£o) e M (Maximiza√ß√£o) para estimar os par√¢metros com dados faltantes. ‚ñ†

O algoritmo EM √© particularmente √∫til quando a fun√ß√£o de verossimilhan√ßa completa, dados os dados observados e os dados faltantes, √© mais simples de manipular do que a fun√ß√£o de verossimilhan√ßa marginal, que envolve dados observados apenas. A utiliza√ß√£o do algoritmo EM para modelos ARMA com dados faltantes envolve  a utiliza√ß√£o da estrutura do modelo para imputar os dados faltantes, usando a m√©dia condicional dos dados faltantes, dado o modelo e os par√¢metros atuais, e ent√£o maximizar a fun√ß√£o de log-verossimilhan√ßa, como se os dados estivessem completos. √â crucial, contudo, garantir a correta implementa√ß√£o do algoritmo, especialmente na imputa√ß√£o dos dados faltantes, que depende da estrutura do modelo e da distribui√ß√£o dos erros.

**Lema 4:** *A etapa E do algoritmo EM para modelos ARMA com dados faltantes envolve a utiliza√ß√£o de filtros de Kalman para imputar os dados faltantes com base nas estimativas atuais dos par√¢metros e na estrutura do modelo.*

*Prova do Lema 4:*
I.  Modelos ARMA podem ser representados como modelos de espa√ßo de estados. Um modelo de espa√ßo de estados √© definido por duas equa√ß√µes: a equa√ß√£o de estado e a equa√ß√£o de observa√ß√£o. Por exemplo, um modelo AR(1) pode ser escrito como:
$$ y_t = \phi y_{t-1} + \epsilon_t$$
    que √© a equa√ß√£o de estado, e
$$ z_t = y_t $$
que √© a equa√ß√£o de observa√ß√£o (onde $z_t$ √© a observa√ß√£o, que pode estar faltando).
II.  O filtro de Kalman √© um algoritmo recursivo que estima o estado de um sistema din√¢mico a partir de uma s√©rie de medi√ß√µes ruidosas. No contexto de modelos ARMA com dados faltantes, o filtro de Kalman estima os valores dos dados faltantes ($y_t$ na equa√ß√£o de estado) com base nos dados observados ($z_t$ na equa√ß√£o de observa√ß√£o), no modelo e nas estimativas atuais dos par√¢metros.
III. A etapa E do algoritmo EM utiliza o filtro de Kalman para calcular o valor esperado dos dados faltantes, dado os dados observados e as estimativas atuais dos par√¢metros. O filtro de Kalman realiza duas etapas:
    a.  **Predi√ß√£o**: Ele prediz o valor atual do estado, baseado nos valores dos estados anteriores.
    b.  **Atualiza√ß√£o**: Ele atualiza a predi√ß√£o usando a nova observa√ß√£o (se dispon√≠vel).
IV.  No caso de dados faltantes, a etapa de atualiza√ß√£o do filtro de Kalman n√£o √© utilizada, e a predi√ß√£o √© usada como a melhor estimativa do valor faltante. O filtro de Kalman calcula n√£o apenas o valor predito dos dados faltantes, mas tamb√©m a vari√¢ncia da estimativa.
Portanto, provamos que a etapa E do algoritmo EM utiliza o filtro de Kalman para imputar os dados faltantes em modelos ARMA. ‚ñ†

#### Converg√™ncia e Escolha de Valores Iniciais
A implementa√ß√£o computacional dos m√©todos de otimiza√ß√£o, como Newton-Raphson ou EM, exige um monitoramento cuidadoso da converg√™ncia. Uma converg√™ncia inadequada pode resultar em estimativas de par√¢metros incorretas e prejudicar a an√°lise de s√©ries temporais. √â fundamental verificar que a fun√ß√£o de log-verossimilhan√ßa esteja convergindo para um m√°ximo local e que os par√¢metros estejam se estabilizando, isto √©, as altera√ß√µes nos par√¢metros sejam muito pequenas de itera√ß√£o em itera√ß√£o.

**Corol√°rio 2:** *A escolha de valores iniciais adequados para os par√¢metros √© crucial para garantir a converg√™ncia dos m√©todos de otimiza√ß√£o para o m√°ximo global da fun√ß√£o de verossimilhan√ßa. M√©todos como o uso de um "grid search" inicial e outras estrat√©gias podem melhorar as chances de encontrar estimativas √≥timas. Em outras palavras, √© crucial testar v√°rios valores iniciais para garantir a converg√™ncia ao √≥timo global.*

> üí° **Exemplo Num√©rico:** Imagine que estamos ajustando um modelo ARMA(1,1) e come√ßamos com valores iniciais para $\phi_1$ e $\theta_1$ que est√£o longe do √≥timo global. O m√©todo de Newton-Raphson pode levar v√°rias itera√ß√µes para encontrar um valor razo√°vel ou pode at√© mesmo convergir para um m√°ximo local em vez do m√°ximo global. Por outro lado, se come√ßarmos com valores iniciais mais pr√≥ximos do √≥timo, como obtidos de uma an√°lise explorat√≥ria dos dados, √© prov√°vel que o algoritmo convirja mais rapidamente para uma solu√ß√£o melhor. Usar uma busca em grade em uma regi√£o plaus√≠vel dos par√¢metros √© muitas vezes uma boa estrat√©gia.
>
> Para ilustrar a import√¢ncia dos valores iniciais, vamos simular um ARMA(1,1) e estim√°-lo com Newton-Raphson usando diferentes valores iniciais:
>
> ```python
> import numpy as np
> import pandas as pd
> from scipy.optimize import minimize
> 
> np.random.seed(42)
> n = 200
> phi_true = 0.7
> theta_true = 0.4
> sigma2_true = 1
> errors = np.random.normal(0, np.sqrt(sigma2_true), n)
> y = np.zeros(n)
> y[0] = errors[0]
> y[1] = phi_true*y[0] + errors[1] + theta_true*errors[0]
> for t in range(2, n):
>    y[t] = phi_true * y[t-1] + errors[t] + theta_true*errors[t-1]
>
> # Fun√ß√£o de log-verossimilhan√ßa para ARMA(1,1) (para simplificar usaremos a fun√ß√£o do statsmodels)
> from statsmodels.tsa.arima.model import ARIMA
>
> def log_likelihood_arma11(params, data):
>     phi, theta, sigma2 = params
>     model = ARIMA(data, order=(1,0,1))
>     results = model.fit(method='innovations_mle', maxiter=0, start_params = [phi, theta, np.sqrt(sigma2)] )
>     return -results.llf # Negativo para minimizar
> 
> # Derivada (gradiente) da log-verossimilhan√ßa (calculada numericamente para simplificar)
> def gradient_log_likelihood_arma11(params, data):
>   from scipy.optimize import approx_fprime
>   eps = 1e-8
>   return approx_fprime(params, lambda x: log_likelihood_arma11(x, data), eps)
> 
> # Matriz Hessiana da log-verossimilhan√ßa (calculada numericamente para simplificar)
> def hessian_log_likelihood_arma11(params, data):
>     from scipy.optimize import approx_fprime
>     eps = 1e-8
>     return approx_fprime(params,lambda x: gradient_log_likelihood_arma11(x, data),eps).T
> 
> # Valores iniciais ruins
> initial_params_bad = np.array([-0.5, -0.5, 0.2])
> # Valores iniciais bons
> initial_params_good = np.array([0.2, 0.1, 0.8])
>
> # Otimiza√ß√£o com Newton-Raphson (usando minimize com m√©todo trust-ncg para simular Newton-Raphson)
> result_bad = minimize(log_likelihood_arma11, initial_params_bad, method='trust-ncg',jac=gradient_log_likelihood_arma11, hess=hessian_log_likelihood_arma11, args=(y,))
> phi_mle_bad, theta_mle_bad, sigma2_mle_bad = result_bad.x
> 
> result_good = minimize(log_likelihood_arma11, initial_params_good, method='trust-ncg',jac=gradient_log_likelihood_arma11, hess=hessian_log_likelihood_arma11, args=(y,))
> phi_mle_good, theta_mle_good, sigma2_mle_good = result_good.x
> 
> print("Estimativas com valores iniciais ruins:")
> print(f"  phi: {phi_mle_bad:.4f}, theta: {theta_mle_bad:.4f}, sigma2: {sigma2_mle_bad:.4f}")
> print("Estimativas com valores iniciais bons:")
> print(f"  phi: {phi_mle_good:.4f}, theta: {theta_mle_good:.4f}, sigma2: {sigma2_mle_good:.4f}")
> ```
>
> Neste exemplo, usamos valores iniciais ruins ([-0.5, -0.5, 0.2]) e valores iniciais bons ([0.2, 0.1, 0.8]). Os resultados mostram que com valores iniciais ruins, as estimativas s√£o piores em compara√ß√£o com os valores reais ($\phi=0.7$, $\theta=0.4$, $\sigma^2=1$) e com os resultados obtidos com os valores iniciais bons. Este exemplo ilustra a import√¢ncia de escolher valores iniciais adequados.

Al√©m disso, √© importante realizar an√°lises de sensibilidade para avaliar o impacto da escolha de valores iniciais nas estimativas finais. Uma alta sensibilidade sugere que os resultados podem n√£o ser robustos a diferentes especifica√ß√µes. Em particular, se os par√¢metros estimados forem utilizados em predi√ß√µes fora da amostra, essa sensibilidade pode levar a resultados pouco confi√°veis.

**Teorema 3:** *A converg√™ncia do algoritmo EM √© garantida sob certas condi√ß√µes, mas a velocidade de converg√™ncia pode ser lenta. Al√©m disso, o algoritmo EM converge para um ponto estacion√°rio da fun√ß√£o de verossimilhan√ßa, que pode ser um m√°ximo local e n√£o necessariamente o m√°ximo global.*

*Prova do Teorema 3:*
I. O algoritmo EM garante que a verossimilhan√ßa dos dados observados n√£o diminua a cada itera√ß√£o. Formalmente, $L(\theta^{(i+1)}; Y) \geq L(\theta^{(i)}; Y)$, onde $\theta^{(i)}$ s√£o os par√¢metros na itera√ß√£o $i$, e $Y$ s√£o os dados observados.
II. A cada itera√ß√£o do algoritmo EM, atualizamos os par√¢metros $\theta$ para maximizar a fun√ß√£o Q:
$$Q(\theta|\theta^{(i)}) = E_{Z|Y,\theta^{(i)}}[\log L_c(\theta; Y,Z)]$$
onde $L_c$ √© a fun√ß√£o de verossimilhan√ßa completa dos dados. Como $Q$ √© uma aproxima√ß√£o da log-verossimilhan√ßa e √© maximizada a cada itera√ß√£o, a log-verossimilhan√ßa marginal tamb√©m n√£o diminui.
III. A converg√™ncia do algoritmo EM para um ponto estacion√°rio segue do fato de que a sequ√™ncia de valores da verossimilhan√ßa marginal $L(\theta; Y)$ √© mon√≥tona n√£o-decrescente e limitada superiormente (j√° que √© uma probabilidade).
IV. A converg√™ncia a um ponto estacion√°rio n√£o implica que seja um m√°ximo global, mas sim um ponto onde as derivadas s√£o zero. Assim, a converg√™ncia a um m√°ximo local pode acontecer. A velocidade de converg√™ncia pode ser afetada por muitos fatores, como o n√∫mero de dados faltantes, a "achatamento" da fun√ß√£o de verossimilhan√ßa, ou a proximidade do ponto inicial com o m√°ximo global.
Portanto, provamos que o algoritmo EM converge para um ponto estacion√°rio da fun√ß√£o de verossimilhan√ßa, que pode ser um m√°ximo local, e que a velocidade de converg√™ncia pode ser lenta. ‚ñ†

### Conclus√£o
Este cap√≠tulo detalhou os aspectos pr√°ticos da estima√ß√£o de par√¢metros ARMA por m√°xima verossimilhan√ßa. Exploramos t√©cnicas de otimiza√ß√£o, como o m√©todo de Newton-Raphson e o algoritmo EM, e discutimos a import√¢ncia do controle da converg√™ncia e da escolha de valores iniciais adequados. A aplica√ß√£o bem-sucedida da MLE requer uma compreens√£o profunda tanto da teoriasubjacente quanto das nuances pr√°ticas do modelo probabil√≠stico espec√≠fico.

### Abordagens Bayesianas para Estima√ß√£o de Par√¢metros

Enquanto a MLE busca encontrar o √∫nico conjunto de par√¢metros que maximiza a probabilidade dos dados observados, a abordagem Bayesiana adota uma perspectiva diferente. Em vez de tratar os par√¢metros como valores fixos e desconhecidos, a infer√™ncia Bayesiana os considera como vari√°veis aleat√≥rias com distribui√ß√µes de probabilidade. Isso permite incorporar conhecimento pr√©vio ou cren√ßas sobre os par√¢metros atrav√©s de uma distribui√ß√£o *a priori*. A infer√™ncia ent√£o atualiza essa cren√ßa usando os dados observados, resultando em uma distribui√ß√£o *a posteriori* sobre os par√¢metros.

A f√≥rmula fundamental da infer√™ncia Bayesiana √© dada pelo Teorema de Bayes:

$$ P(\theta | D) = \frac{P(D | \theta) P(\theta)}{P(D)} $$

onde:

*   $P(\theta | D)$ √© a distribui√ß√£o *a posteriori* dos par√¢metros $\theta$ dado os dados $D$.
*   $P(D | \theta)$ √© a fun√ß√£o de verossimilhan√ßa, que representa a probabilidade dos dados dado os par√¢metros.
*   $P(\theta)$ √© a distribui√ß√£o *a priori* dos par√¢metros $\theta$, que codifica nosso conhecimento pr√©vio sobre os par√¢metros.
*   $P(D)$ √© a evid√™ncia ou probabilidade marginal dos dados, que atua como um fator de normaliza√ß√£o.

A distribui√ß√£o *a posteriori* $P(\theta | D)$ cont√©m toda a informa√ß√£o sobre os par√¢metros ap√≥s observar os dados. Em geral, a probabilidade marginal $P(D)$ √© dif√≠cil de calcular, e t√©cnicas de aproxima√ß√£o como *Markov Chain Monte Carlo* (MCMC) s√£o frequentemente usadas para amostrar da distribui√ß√£o *a posteriori*.

#### Compara√ß√£o com a MLE

A infer√™ncia Bayesiana oferece v√°rias vantagens em rela√ß√£o √† MLE:

*   **Incerteza nos par√¢metros:** A infer√™ncia Bayesiana quantifica a incerteza nos par√¢metros por meio da distribui√ß√£o *a posteriori*, enquanto a MLE fornece apenas uma estimativa pontual.
*   **Incorpora√ß√£o de conhecimento pr√©vio:** A infer√™ncia Bayesiana permite incorporar conhecimento pr√©vio ou cren√ßas sobre os par√¢metros por meio da distribui√ß√£o *a priori*.
*   **Robustez:** A infer√™ncia Bayesiana tende a ser mais robusta a *outliers* e pequenas amostras de dados do que a MLE.

No entanto, a infer√™ncia Bayesiana tamb√©m possui desafios:

*   **Escolha da distribui√ß√£o a priori:** A escolha da distribui√ß√£o *a priori* pode ter um impacto significativo na distribui√ß√£o *a posteriori*.
*   **Custo computacional:** A amostragem da distribui√ß√£o *a posteriori* pode ser computacionalmente custosa, especialmente para modelos complexos.

### T√©cnicas de Otimiza√ß√£o para MLE e MAP

Tanto a MLE quanto a infer√™ncia Bayesiana (usando a estimativa de m√°xima *a posteriori* ou MAP) geralmente envolvem problemas de otimiza√ß√£o. A MLE busca maximizar a fun√ß√£o de verossimilhan√ßa, enquanto a MAP busca maximizar a verossimilhan√ßa multiplicada pela *a priori* (que corresponde a maximizar a distribui√ß√£o *a posteriori*).

V√°rias t√©cnicas de otimiza√ß√£o podem ser usadas, incluindo:

*   **Descida do gradiente:** Um m√©todo iterativo que move os par√¢metros na dire√ß√£o do gradiente da fun√ß√£o objetivo.
*   **M√©todos de segunda ordem:** M√©todos como o m√©todo de Newton que usam a segunda derivada (Hessiana) para obter uma converg√™ncia mais r√°pida.
*   **Algoritmos de busca:** Algoritmos como a busca em grade ou algoritmos gen√©ticos que podem ser √∫teis quando a fun√ß√£o objetivo n√£o √© diferenci√°vel.

A escolha da t√©cnica de otimiza√ß√£o depende do modelo espec√≠fico e da complexidade da fun√ß√£o objetivo.

<!-- END -->
