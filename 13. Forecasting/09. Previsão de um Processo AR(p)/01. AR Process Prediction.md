## Previs√£o de um Processo AR(p)
### Introdu√ß√£o
Este cap√≠tulo continua a nossa explora√ß√£o de t√©cnicas de previs√£o, focando agora especificamente em processos Autorregressivos de ordem *p* (AR(p)). Anteriormente, estabelecemos as bases para a previs√£o linear, tanto atrav√©s da proje√ß√£o linear quanto das expectativas condicionais, e exploramos cen√°rios com um n√∫mero infinito de observa√ß√µes e tamb√©m com um n√∫mero finito. Vimos que a previs√£o √≥tima, em termos de erro quadr√°tico m√©dio, √© dada pela esperan√ßa condicional da vari√°vel futura dado o conjunto de informa√ß√µes dispon√≠veis [^1]. No caso de modelos lineares, essa esperan√ßa condicional se traduz na proje√ß√£o linear da vari√°vel no conjunto de informa√ß√µes [^1]. Expandindo o conceito apresentado no contexto, este cap√≠tulo aborda como aplicar esses princ√≠pios a um modelo AR(p), que √© um tipo fundamental de modelo de s√©ries temporais [^1]. Especificamente, vamos explorar como derivar e implementar previs√µes *s*-per√≠odos √† frente, utilizando a estrutura autoregressiva.

### Conceitos Fundamentais
A previs√£o para um processo AR(p) envolve a manipula√ß√£o da rela√ß√£o autoregressiva para expressar o valor futuro $Y_{t+s}$ em termos de valores passados e choques da s√©rie temporal [^1]. Considere um processo AR(p) estacion√°rio, dado por [^1]:
$$ (1 - \phi_1 L - \phi_2 L^2 - \ldots - \phi_p L^p)(Y_t - \mu) = \epsilon_t, $$
onde $L$ √© o operador de defasagem, $\phi_i$ s√£o os coeficientes autoregressivos, $\mu$ √© a m√©dia do processo e $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia constante. Expandindo esta equa√ß√£o, podemos expressar $Y_t$ em fun√ß√£o dos valores defasados e do choque corrente:
$$ Y_t = \mu + \phi_1 (Y_{t-1} - \mu) + \phi_2 (Y_{t-2} - \mu) + \ldots + \phi_p (Y_{t-p} - \mu) + \epsilon_t $$
**Observa√ß√£o 1:**  √â importante notar que a estacionariedade do processo AR(p) √© crucial para a aplica√ß√£o dessas t√©cnicas de previs√£o. A estacionariedade garante que os par√¢metros do modelo, $\phi_i$, e a m√©dia $\mu$ sejam constantes ao longo do tempo, permitindo que as previs√µes geradas com base em dados passados sejam relevantes para o futuro.

Para gerar previs√µes para $Y_{t+s}$, o objetivo √© expressar $Y_{t+s}$ em termos de valores passados e choques, removendo qualquer depend√™ncia de choques futuros. Este processo envolve o uso da lei das proje√ß√µes iteradas [^1]. Come√ßando com a previs√£o de um per√≠odo √† frente ($s=1$), podemos expressar $Y_{t+1}$ como [^1]:
$$ E[Y_{t+1}|Y_t, Y_{t-1},\ldots] = \mu + \phi_1 (Y_t - \mu) + \phi_2 (Y_{t-1} - \mu) + \ldots + \phi_p (Y_{t-p+1} - \mu) $$
Onde $E[\epsilon_{t+1}|Y_t, Y_{t-1},\ldots]=0$, dado que $\epsilon_{t+1}$ √© um choque futuro e, por defini√ß√£o, √© n√£o correlacionado com a informa√ß√£o presente [^1]. Para previs√µes al√©m de um per√≠odo, por exemplo, dois per√≠odos √† frente ($s=2$), aplicamos novamente a l√≥gica da expectativa condicional, substituindo os valores futuros de $Y$ pela sua respectiva previs√£o, obtida na etapa anterior [^1]:
$$ E[Y_{t+2}|Y_t, Y_{t-1},\ldots] = \mu + \phi_1 E[Y_{t+1}|Y_t, Y_{t-1},\ldots] + \phi_2 (Y_t - \mu) + \ldots + \phi_p (Y_{t-p+2} - \mu) $$
Aqui, $E[Y_{t+1}|Y_t, Y_{t-1},\ldots]$ √© a previs√£o de um per√≠odo √† frente calculada anteriormente. Em outras palavras, as previs√µes s√£o geradas de forma recursiva utilizando as previs√µes previamente geradas [^1].

> üí° **Exemplo Num√©rico:** Vamos considerar um processo AR(2) com $\mu = 10$, $\phi_1 = 0.7$, e $\phi_2 = 0.2$. Suponha que observamos $Y_t = 12$ e $Y_{t-1} = 11$. A previs√£o de um per√≠odo √† frente, $\hat{Y}_{t+1|t}$, √© calculada como:
>  
> $\hat{Y}_{t+1|t} = 10 + 0.7 * (12 - 10) + 0.2 * (11 - 10) = 10 + 0.7 * 2 + 0.2 * 1 = 10 + 1.4 + 0.2 = 11.6$.
>
> Para a previs√£o de dois per√≠odos √† frente, $\hat{Y}_{t+2|t}$, usamos a previs√£o de um per√≠odo √† frente:
>
> $\hat{Y}_{t+2|t} = 10 + 0.7 * (11.6 - 10) + 0.2 * (12 - 10) = 10 + 0.7 * 1.6 + 0.2 * 2 = 10 + 1.12 + 0.4 = 11.52$.
>
> Este exemplo ilustra como a previs√£o √© feita recursivamente, usando previs√µes anteriores para construir previs√µes futuras.
>
> ```python
> import numpy as np
>
> # Par√¢metros do modelo
> mu = 10
> phi1 = 0.7
> phi2 = 0.2
>
> # Valores observados
> Y_t = 12
> Y_t_minus_1 = 11
>
> # Previs√£o de 1 per√≠odo √† frente
> Y_hat_t_plus_1 = mu + phi1 * (Y_t - mu) + phi2 * (Y_t_minus_1 - mu)
> print(f"Previs√£o de 1 per√≠odo √† frente: {Y_hat_t_plus_1}")
>
> # Previs√£o de 2 per√≠odos √† frente
> Y_hat_t_plus_2 = mu + phi1 * (Y_hat_t_plus_1 - mu) + phi2 * (Y_t - mu)
> print(f"Previs√£o de 2 per√≠odos √† frente: {Y_hat_t_plus_2}")
> ```

Em geral, para uma previs√£o de *s* per√≠odos √† frente, a express√£o que relaciona $Y_{t+s}$ a valores iniciais e choques subsequentes (mas sem choques futuros) √© dada por [^1]:
$$ Y_{t+s} - \mu = f_1^{(s)}(Y_t - \mu) + f_2^{(s)}(Y_{t-1} - \mu) + \ldots + f_p^{(s)}(Y_{t-p+1} - \mu) + \epsilon_{t+s} + \psi_1 \epsilon_{t+s-1} + \ldots + \psi_{s-1} \epsilon_{t+1} $$
onde $f_i^{(s)}$ s√£o os coeficientes que descrevem como os valores passados do processo afetam a previs√£o de $s$ per√≠odos √† frente e $\psi_i$ s√£o os coeficientes do ru√≠do branco. A equa√ß√£o acima expressa $Y_{t+s}$ em termos dos valores iniciais de $Y$ e de um conjunto de ru√≠dos brancos subsequentes, eliminando a necessidade de prever os valores futuros do ru√≠do. Assim, para previs√µes maiores que um per√≠odo √† frente, o componente do choque √© uma m√©dia ponderada dos choques subsequentes [^1].
A previs√£o √≥tima para um processo AR(p) *s*-per√≠odos √† frente, ent√£o, √© dada por [^1]:
$$  \hat{Y}_{t+s|t} = \mu + f_1^{(s)}(Y_t - \mu) + f_2^{(s)}(Y_{t-1} - \mu) + \ldots + f_p^{(s)}(Y_{t-p+1} - \mu) $$
Onde os valores de $Y$ com defasagens menores que zero, s√£o os valores observados, e os valores de $Y$ com defasagens maiores ou iguais a zero, s√£o as previs√µes obtidas na etapa anterior do processo iterativo.

Como notado no contexto [^1], o c√°lculo recursivo da previs√£o √© a maneira mais f√°cil de implementar este processo, e √© baseado na lei das proje√ß√µes iteradas. Al√©m disso, as matrizes $F$ e $f^{(s)}$ mencionadas no contexto s√£o relacionadas aos coeficientes do modelo AR(p), e podem ser utilizadas para calcular a evolu√ß√£o da proje√ß√£o [^1].

**Lema 1:** *Rela√ß√£o entre os Coeficientes $f_i^{(s)}$ e os Par√¢metros do AR(p)*
Os coeficientes $f_i^{(s)}$ nas equa√ß√µes de previs√£o *s*-per√≠odos √† frente s√£o determinados recursivamente pelos coeficientes $\phi_i$ do processo AR(p). Para $s=1$, temos $f_i^{(1)} = \phi_i$. Para $s > 1$, a rela√ß√£o √© dada por:
$f_i^{(s)} = \sum_{j=1}^p \phi_j f_{i-j}^{(s-1)}$
onde definimos $f_i^{(s)}=0$ se $i\leq 0$ ou $i > p$.

*Prova*: A prova decorre da aplica√ß√£o recursiva da equa√ß√£o do AR(p) e da defini√ß√£o de proje√ß√£o linear. Quando $s=1$, a express√£o se reduz √† defini√ß√£o do processo AR(p). Para $s > 1$, a aplica√ß√£o da lei das proje√ß√µes iteradas leva √† recorr√™ncia apresentada.

I. Para $s = 1$, a equa√ß√£o de previs√£o √© dada por:
   $$ E[Y_{t+1}|Y_t, Y_{t-1},\ldots] = \mu + \phi_1 (Y_t - \mu) + \phi_2 (Y_{t-1} - \mu) + \ldots + \phi_p (Y_{t-p+1} - \mu) $$
   Comparando com a defini√ß√£o da equa√ß√£o de previs√£o geral, vemos que $f_i^{(1)} = \phi_i$.

II. Para $s > 1$, come√ßamos com a equa√ß√£o AR(p):
    $$ Y_{t+s} = \mu + \sum_{j=1}^{p} \phi_j(Y_{t+s-j}-\mu) + \epsilon_{t+s} $$
   Tomando a expectativa condicional em $t$, temos:
    $$E[Y_{t+s}|Y_t, Y_{t-1},\ldots] = \mu + \sum_{j=1}^{p} \phi_j E[Y_{t+s-j}|Y_t, Y_{t-1},\ldots] $$

III.  Usando a defini√ß√£o da equa√ß√£o de previs√£o *s*-per√≠odos √† frente:
     $$E[Y_{t+s-j}|Y_t, Y_{t-1},\ldots] = \mu + \sum_{i=1}^p f_i^{(s-j)}(Y_{t-i+1}-\mu)$$
     Substitu√≠mos na express√£o anterior:
     $$ E[Y_{t+s}|Y_t, Y_{t-1},\ldots] = \mu + \sum_{j=1}^{p} \phi_j \left( \mu + \sum_{i=1}^p f_i^{(s-j)}(Y_{t-i+1}-\mu) \right) $$
   
IV.  Expandindo a express√£o e reorganizando os termos, podemos igualar os coeficientes do termo $Y_{t-i+1} - \mu$  na forma:
     $$ E[Y_{t+s}|Y_t, Y_{t-1},\ldots] = \mu + \sum_{i=1}^p \left( \sum_{j=1}^p \phi_j f_{i-j}^{(s-1)} \right) (Y_{t-i+1} - \mu) $$
     Comparando esta express√£o com a defini√ß√£o da equa√ß√£o de previs√£o *s*-per√≠odos √† frente, conclu√≠mos que:
    $$f_i^{(s)} = \sum_{j=1}^p \phi_j f_{i-j}^{(s-1)}$$
    
V. Portanto, provamos que $f_i^{(s)} = \sum_{j=1}^p \phi_j f_{i-j}^{(s-1)}$ e que para $s=1$, $f_i^{(1)} = \phi_i$. ‚ñ†

> üí° **Exemplo Num√©rico:** Para ilustrar o Lema 1, vamos usar o mesmo processo AR(2) anterior ($\phi_1 = 0.7$, $\phi_2 = 0.2$).
>
> Para $s = 1$: $f_1^{(1)} = \phi_1 = 0.7$ e $f_2^{(1)} = \phi_2 = 0.2$.
>
> Para $s = 2$:
>   $f_1^{(2)} = \phi_1 f_1^{(1)} + \phi_2 f_0^{(1)} = 0.7 * 0.7 + 0.2 * 0 = 0.49$ (Lembrando que $f_0^{(1)} = 0$ por defini√ß√£o)
>  $f_2^{(2)} = \phi_1 f_2^{(1)} + \phi_2 f_1^{(1)} = 0.7 * 0.2 + 0.2 * 0.7 = 0.14 + 0.14 = 0.28$
>
>  Para $s=3$:
>  $f_1^{(3)} = \phi_1 f_1^{(2)} + \phi_2 f_0^{(2)} = 0.7 * 0.49 + 0.2 * 0 = 0.343$
>  $f_2^{(3)} = \phi_1 f_2^{(2)} + \phi_2 f_1^{(2)} = 0.7 * 0.28 + 0.2 * 0.49 = 0.196 + 0.098 = 0.294$
>
>  Este exemplo demonstra como os coeficientes $f_i^{(s)}$ s√£o calculados recursivamente, utilizando os par√¢metros do AR(p) e os coeficientes do passo de previs√£o anterior.
>
>```python
>import numpy as np
>
># Par√¢metros do modelo AR(2)
>phi1 = 0.7
>phi2 = 0.2
>
>def calculate_f_coefficients(phi1, phi2, s):
>    if s == 1:
>        return np.array([phi1, phi2])
>
>    f_prev = calculate_f_coefficients(phi1, phi2, s - 1)
>    f_current = np.zeros(2)
>    f_current[0] = phi1 * f_prev[0]
>    f_current[1] = phi1 * f_prev[1] + phi2 * f_prev[0]
>    return f_current
>
>f1 = calculate_f_coefficients(phi1, phi2, 1)
>f2 = calculate_f_coefficients(phi1, phi2, 2)
>f3 = calculate_f_coefficients(phi1, phi2, 3)
>print(f"f(1): {f1}")
>print(f"f(2): {f2}")
>print(f"f(3): {f3}")
>```

**Corol√°rio 1:** *Propriedade da Converg√™ncia da Previs√£o*
Para um processo AR(p) estacion√°rio, a previs√£o *s*-per√≠odos √† frente, $\hat{Y}_{t+s|t}$, converge para a m√©dia do processo, $\mu$, √† medida que *s* aumenta. Formalmente,
$$\lim_{s \to \infty} \hat{Y}_{t+s|t} = \mu $$

*Prova*: Como o processo √© estacion√°rio, os coeficientes $f_i^{(s)}$ tendem a zero quando $s \rightarrow \infty$. Isso √© uma consequ√™ncia da estabilidade do processo AR(p), garantindo que o impacto dos valores defasados do processo em previs√µes mais distantes se dissipe ao longo do tempo. Portanto, quando $s$ tende ao infinito, a previs√£o $\hat{Y}_{t+s|t}$ se torna igual √† m√©dia do processo $\mu$.

I.  A previs√£o *s*-per√≠odos √† frente √© dada por:
    $$\hat{Y}_{t+s|t} = \mu + \sum_{i=1}^p f_i^{(s)}(Y_{t-i+1}-\mu)$$
    
II.  Sabemos do Lema 1 que $f_i^{(s)}$ s√£o definidos recursivamente como:
    $f_i^{(s)} = \sum_{j=1}^p \phi_j f_{i-j}^{(s-1)}$
    
III. Como o processo AR(p) √© estacion√°rio, as ra√≠zes do polin√¥mio caracter√≠stico est√£o fora do c√≠rculo unit√°rio. Isso implica que os coeficientes $f_i^{(s)}$ convergem para 0 √† medida que $s$ tende ao infinito, ou seja:
    $$\lim_{s \to \infty} f_i^{(s)} = 0$$
    
IV. Aplicando esse limite na equa√ß√£o de previs√£o:
    $$\lim_{s \to \infty} \hat{Y}_{t+s|t} = \mu + \lim_{s \to \infty} \sum_{i=1}^p f_i^{(s)}(Y_{t-i+1}-\mu) = \mu + \sum_{i=1}^p  \left( \lim_{s \to \infty} f_i^{(s)} \right)(Y_{t-i+1}-\mu) = \mu + \sum_{i=1}^p 0 \cdot (Y_{t-i+1}-\mu) $$

V.  Assim, temos:
    $$\lim_{s \to \infty} \hat{Y}_{t+s|t} = \mu$$
    Portanto, a previs√£o *s*-per√≠odos √† frente converge para a m√©dia do processo $\mu$ quando $s$ tende ao infinito. ‚ñ†

> üí° **Exemplo Num√©rico:** Usando o exemplo AR(2) anterior, podemos ver que os coeficientes $f_i^{(s)}$ calculados tendem a diminuir √† medida que 's' aumenta. Isso ilustra como a influ√™ncia de observa√ß√µes passadas na previs√£o diminui com o aumento do horizonte de previs√£o, fazendo com que a previs√£o eventualmente convirja para a m√©dia do processo, que √© $\mu=10$.
>
>```python
>import numpy as np
>import matplotlib.pyplot as plt
>
>
>phi1 = 0.7
>phi2 = 0.2
>
>def calculate_f_coefficients(phi1, phi2, s):
>    if s == 1:
>        return np.array([phi1, phi2])
>
>    f_prev = calculate_f_coefficients(phi1, phi2, s - 1)
>    f_current = np.zeros(2)
>    f_current[0] = phi1 * f_prev[0]
>    f_current[1] = phi1 * f_prev[1] + phi2 * f_prev[0]
>    return f_current
>
>
>s_values = np.arange(1, 11)
>f1_values = [calculate_f_coefficients(phi1, phi2, s)[0] for s in s_values]
>f2_values = [calculate_f_coefficients(phi1, phi2, s)[1] for s in s_values]
>
>plt.figure(figsize=(10, 6))
>plt.plot(s_values, f1_values, label='$f_1^{(s)}$')
>plt.plot(s_values, f2_values, label='$f_2^{(s)}$')
>plt.xlabel('Horizonte de Previs√£o (s)')
>plt.ylabel('Coeficientes $f_i^{(s)}$')
>plt.title('Converg√™ncia dos Coeficientes $f_i^{(s)}$')
>plt.legend()
>plt.grid(True)
>plt.show()
>```
>
>Este gr√°fico mostra como os coeficientes $f_1^{(s)}$ e $f_2^{(s)}$ diminuem em magnitude √† medida que o horizonte de previs√£o (s) aumenta, ilustrando a converg√™ncia da previs√£o para a m√©dia do processo a longo prazo.

### Conclus√£o
Este cap√≠tulo descreveu um m√©todo para derivar e implementar previs√µes *s*-per√≠odos √† frente para processos AR(p). O m√©todo envolve a reescrita da equa√ß√£o autoregressiva para expressar os valores futuros em termos de valores passados e choques presentes e passados, iterando o processo de proje√ß√£o linear [^1]. Com essa abordagem, conseguimos derivar previs√µes √≥timas em termos de erro quadr√°tico m√©dio e tamb√©m conseguimos entender a rela√ß√£o entre previs√µes em diferentes horizontes. Esta discuss√£o fornece a base para modelos de previs√£o mais complexos envolvendo componentes de m√©dia m√≥vel [^1]. Em continuidade ao que foi apresentado, o pr√≥ximo passo √© explorar como aplicar esses conceitos em um contexto pr√°tico utilizando dados reais, e para isso √© preciso abordar os m√©todos de estima√ß√£o desses par√¢metros.

### Refer√™ncias
[^1]: Texto fornecido.
<!-- END -->
