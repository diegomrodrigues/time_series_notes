## PrevisÃ£o de um Processo AR(p)
### IntroduÃ§Ã£o
Este capÃ­tulo continua a nossa exploraÃ§Ã£o de tÃ©cnicas de previsÃ£o, focando agora especificamente em processos Autorregressivos de ordem *p* (AR(p)). Anteriormente, estabelecemos as bases para a previsÃ£o linear, tanto atravÃ©s da projeÃ§Ã£o linear quanto das expectativas condicionais, e exploramos cenÃ¡rios com um nÃºmero infinito de observaÃ§Ãµes e tambÃ©m com um nÃºmero finito. Vimos que a previsÃ£o Ã³tima, em termos de erro quadrÃ¡tico mÃ©dio, Ã© dada pela esperanÃ§a condicional da variÃ¡vel futura dado o conjunto de informaÃ§Ãµes disponÃ­veis [^1]. No caso de modelos lineares, essa esperanÃ§a condicional se traduz na projeÃ§Ã£o linear da variÃ¡vel no conjunto de informaÃ§Ãµes [^1]. Expandindo o conceito apresentado no contexto, este capÃ­tulo aborda como aplicar esses princÃ­pios a um modelo AR(p), que Ã© um tipo fundamental de modelo de sÃ©ries temporais [^1]. Especificamente, vamos explorar como derivar e implementar previsÃµes *s*-perÃ­odos Ã  frente, utilizando a estrutura autoregressiva.

### Conceitos Fundamentais
A previsÃ£o para um processo AR(p) envolve a manipulaÃ§Ã£o da relaÃ§Ã£o autoregressiva para expressar o valor futuro $Y_{t+s}$ em termos de valores passados e choques da sÃ©rie temporal [^1]. Considere um processo AR(p) estacionÃ¡rio, dado por [^1]:
$$ (1 - \phi_1 L - \phi_2 L^2 - \ldots - \phi_p L^p)(Y_t - \mu) = \epsilon_t, $$
onde $L$ Ã© o operador de defasagem, $\phi_i$ sÃ£o os coeficientes autoregressivos, $\mu$ Ã© a mÃ©dia do processo e $\epsilon_t$ Ã© um ruÃ­do branco com mÃ©dia zero e variÃ¢ncia constante. Expandindo esta equaÃ§Ã£o, podemos expressar $Y_t$ em funÃ§Ã£o dos valores defasados e do choque corrente:
$$ Y_t = \mu + \phi_1 (Y_{t-1} - \mu) + \phi_2 (Y_{t-2} - \mu) + \ldots + \phi_p (Y_{t-p} - \mu) + \epsilon_t $$
**ObservaÃ§Ã£o 1:**  Ã‰ importante notar que a estacionariedade do processo AR(p) Ã© crucial para a aplicaÃ§Ã£o dessas tÃ©cnicas de previsÃ£o. A estacionariedade garante que os parÃ¢metros do modelo, $\phi_i$, e a mÃ©dia $\mu$ sejam constantes ao longo do tempo, permitindo que as previsÃµes geradas com base em dados passados sejam relevantes para o futuro.

Para gerar previsÃµes para $Y_{t+s}$, o objetivo Ã© expressar $Y_{t+s}$ em termos de valores passados e choques, removendo qualquer dependÃªncia de choques futuros. Este processo envolve o uso da lei das projeÃ§Ãµes iteradas [^1]. ComeÃ§ando com a previsÃ£o de um perÃ­odo Ã  frente ($s=1$), podemos expressar $Y_{t+1}$ como [^1]:
$$ E[Y_{t+1}|Y_t, Y_{t-1},\ldots] = \mu + \phi_1 (Y_t - \mu) + \phi_2 (Y_{t-1} - \mu) + \ldots + \phi_p (Y_{t-p+1} - \mu) $$
Onde $E[\epsilon_{t+1}|Y_t, Y_{t-1},\ldots]=0$, dado que $\epsilon_{t+1}$ Ã© um choque futuro e, por definiÃ§Ã£o, Ã© nÃ£o correlacionado com a informaÃ§Ã£o presente [^1]. Para previsÃµes alÃ©m de um perÃ­odo, por exemplo, dois perÃ­odos Ã  frente ($s=2$), aplicamos novamente a lÃ³gica da expectativa condicional, substituindo os valores futuros de $Y$ pela sua respectiva previsÃ£o, obtida na etapa anterior [^1]:
$$ E[Y_{t+2}|Y_t, Y_{t-1},\ldots] = \mu + \phi_1 E[Y_{t+1}|Y_t, Y_{t-1},\ldots] + \phi_2 (Y_t - \mu) + \ldots + \phi_p (Y_{t-p+2} - \mu) $$
Aqui, $E[Y_{t+1}|Y_t, Y_{t-1},\ldots]$ Ã© a previsÃ£o de um perÃ­odo Ã  frente calculada anteriormente. Em outras palavras, as previsÃµes sÃ£o geradas de forma recursiva utilizando as previsÃµes previamente geradas [^1].

> ğŸ’¡ **Exemplo NumÃ©rico:** Vamos considerar um processo AR(2) com $\mu = 10$, $\phi_1 = 0.7$, e $\phi_2 = 0.2$. Suponha que observamos $Y_t = 12$ e $Y_{t-1} = 11$. A previsÃ£o de um perÃ­odo Ã  frente, $\hat{Y}_{t+1|t}$, Ã© calculada como:
>  
> $\hat{Y}_{t+1|t} = 10 + 0.7 * (12 - 10) + 0.2 * (11 - 10) = 10 + 0.7 * 2 + 0.2 * 1 = 10 + 1.4 + 0.2 = 11.6$.
>
> Para a previsÃ£o de dois perÃ­odos Ã  frente, $\hat{Y}_{t+2|t}$, usamos a previsÃ£o de um perÃ­odo Ã  frente:
>
> $\hat{Y}_{t+2|t} = 10 + 0.7 * (11.6 - 10) + 0.2 * (12 - 10) = 10 + 0.7 * 1.6 + 0.2 * 2 = 10 + 1.12 + 0.4 = 11.52$.
>
> Este exemplo ilustra como a previsÃ£o Ã© feita recursivamente, usando previsÃµes anteriores para construir previsÃµes futuras.
>
> ```python
> import numpy as np
>
> # ParÃ¢metros do modelo
> mu = 10
> phi1 = 0.7
> phi2 = 0.2
>
> # Valores observados
> Y_t = 12
> Y_t_minus_1 = 11
>
> # PrevisÃ£o de 1 perÃ­odo Ã  frente
> Y_hat_t_plus_1 = mu + phi1 * (Y_t - mu) + phi2 * (Y_t_minus_1 - mu)
> print(f"PrevisÃ£o de 1 perÃ­odo Ã  frente: {Y_hat_t_plus_1}")
>
> # PrevisÃ£o de 2 perÃ­odos Ã  frente
> Y_hat_t_plus_2 = mu + phi1 * (Y_hat_t_plus_1 - mu) + phi2 * (Y_t - mu)
> print(f"PrevisÃ£o de 2 perÃ­odos Ã  frente: {Y_hat_t_plus_2}")
> ```

Em geral, para uma previsÃ£o de *s* perÃ­odos Ã  frente, a expressÃ£o que relaciona $Y_{t+s}$ a valores iniciais e choques subsequentes (mas sem choques futuros) Ã© dada por [^1]:
$$ Y_{t+s} - \mu = f_1^{(s)}(Y_t - \mu) + f_2^{(s)}(Y_{t-1} - \mu) + \ldots + f_p^{(s)}(Y_{t-p+1} - \mu) + \epsilon_{t+s} + \psi_1 \epsilon_{t+s-1} + \ldots + \psi_{s-1} \epsilon_{t+1} $$
onde $f_i^{(s)}$ sÃ£o os coeficientes que descrevem como os valores passados do processo afetam a previsÃ£o de $s$ perÃ­odos Ã  frente e $\psi_i$ sÃ£o os coeficientes do ruÃ­do branco. A equaÃ§Ã£o acima expressa $Y_{t+s}$ em termos dos valores iniciais de $Y$ e de um conjunto de ruÃ­dos brancos subsequentes, eliminando a necessidade de prever os valores futuros do ruÃ­do. Assim, para previsÃµes maiores que um perÃ­odo Ã  frente, o componente do choque Ã© uma mÃ©dia ponderada dos choques subsequentes [^1].
A previsÃ£o Ã³tima para um processo AR(p) *s*-perÃ­odos Ã  frente, entÃ£o, Ã© dada por [^1]:
$$  \hat{Y}_{t+s|t} = \mu + f_1^{(s)}(Y_t - \mu) + f_2^{(s)}(Y_{t-1} - \mu) + \ldots + f_p^{(s)}(Y_{t-p+1} - \mu) $$
Onde os valores de $Y$ com defasagens menores que zero, sÃ£o os valores observados, e os valores de $Y$ com defasagens maiores ou iguais a zero, sÃ£o as previsÃµes obtidas na etapa anterior do processo iterativo.

Como notado no contexto [^1], o cÃ¡lculo recursivo da previsÃ£o Ã© a maneira mais fÃ¡cil de implementar este processo, e Ã© baseado na lei das projeÃ§Ãµes iteradas. AlÃ©m disso, as matrizes $F$ e $f^{(s)}$ mencionadas no contexto sÃ£o relacionadas aos coeficientes do modelo AR(p), e podem ser utilizadas para calcular a evoluÃ§Ã£o da projeÃ§Ã£o [^1].

**Lema 1:** *RelaÃ§Ã£o entre os Coeficientes $f_i^{(s)}$ e os ParÃ¢metros do AR(p)*
Os coeficientes $f_i^{(s)}$ nas equaÃ§Ãµes de previsÃ£o *s*-perÃ­odos Ã  frente sÃ£o determinados recursivamente pelos coeficientes $\phi_i$ do processo AR(p). Para $s=1$, temos $f_i^{(1)} = \phi_i$. Para $s > 1$, a relaÃ§Ã£o Ã© dada por:
$f_i^{(s)} = \sum_{j=1}^p \phi_j f_{i-j}^{(s-1)}$
onde definimos $f_i^{(s)}=0$ se $i\leq 0$ ou $i > p$.

*Prova*: A prova decorre da aplicaÃ§Ã£o recursiva da equaÃ§Ã£o do AR(p) e da definiÃ§Ã£o de projeÃ§Ã£o linear. Quando $s=1$, a expressÃ£o se reduz Ã  definiÃ§Ã£o do processo AR(p). Para $s > 1$, a aplicaÃ§Ã£o da lei das projeÃ§Ãµes iteradas leva Ã  recorrÃªncia apresentada.

I. Para $s = 1$, a equaÃ§Ã£o de previsÃ£o Ã© dada por:
   $$ E[Y_{t+1}|Y_t, Y_{t-1},\ldots] = \mu + \phi_1 (Y_t - \mu) + \phi_2 (Y_{t-1} - \mu) + \ldots + \phi_p (Y_{t-p+1} - \mu) $$
   Comparando com a definiÃ§Ã£o da equaÃ§Ã£o de previsÃ£o geral, vemos que $f_i^{(1)} = \phi_i$.

II. Para $s > 1$, comeÃ§amos com a equaÃ§Ã£o AR(p):
    $$ Y_{t+s} = \mu + \sum_{j=1}^{p} \phi_j(Y_{t+s-j}-\mu) + \epsilon_{t+s} $$
   Tomando a expectativa condicional em $t$, temos:
    $$E[Y_{t+s}|Y_t, Y_{t-1},\ldots] = \mu + \sum_{j=1}^{p} \phi_j E[Y_{t+s-j}|Y_t, Y_{t-1},\ldots] $$

III.  Usando a definiÃ§Ã£o da equaÃ§Ã£o de previsÃ£o *s*-perÃ­odos Ã  frente:
     $$E[Y_{t+s-j}|Y_t, Y_{t-1},\ldots] = \mu + \sum_{i=1}^p f_i^{(s-j)}(Y_{t-i+1}-\mu)$$
     SubstituÃ­mos na expressÃ£o anterior:
     $$ E[Y_{t+s}|Y_t, Y_{t-1},\ldots] = \mu + \sum_{j=1}^{p} \phi_j \left( \mu + \sum_{i=1}^p f_i^{(s-j)}(Y_{t-i+1}-\mu) \right) $$
   
IV.  Expandindo a expressÃ£o e reorganizando os termos, podemos igualar os coeficientes do termo $Y_{t-i+1} - \mu$  na forma:
     $$ E[Y_{t+s}|Y_t, Y_{t-1},\ldots] = \mu + \sum_{i=1}^p \left( \sum_{j=1}^p \phi_j f_{i-j}^{(s-1)} \right) (Y_{t-i+1} - \mu) $$
     Comparando esta expressÃ£o com a definiÃ§Ã£o da equaÃ§Ã£o de previsÃ£o *s*-perÃ­odos Ã  frente, concluÃ­mos que:
    $$f_i^{(s)} = \sum_{j=1}^p \phi_j f_{i-j}^{(s-1)}$$
    
V. Portanto, provamos que $f_i^{(s)} = \sum_{j=1}^p \phi_j f_{i-j}^{(s-1)}$ e que para $s=1$, $f_i^{(1)} = \phi_i$. â– 

> ğŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar o Lema 1, vamos usar o mesmo processo AR(2) anterior ($\phi_1 = 0.7$, $\phi_2 = 0.2$).
>
> Para $s = 1$: $f_1^{(1)} = \phi_1 = 0.7$ e $f_2^{(1)} = \phi_2 = 0.2$.
>
> Para $s = 2$:
>   $f_1^{(2)} = \phi_1 f_1^{(1)} + \phi_2 f_0^{(1)} = 0.7 * 0.7 + 0.2 * 0 = 0.49$ (Lembrando que $f_0^{(1)} = 0$ por definiÃ§Ã£o)
>  $f_2^{(2)} = \phi_1 f_2^{(1)} + \phi_2 f_1^{(1)} = 0.7 * 0.2 + 0.2 * 0.7 = 0.14 + 0.14 = 0.28$
>
>  Para $s=3$:
>  $f_1^{(3)} = \phi_1 f_1^{(2)} + \phi_2 f_0^{(2)} = 0.7 * 0.49 + 0.2 * 0 = 0.343$
>  $f_2^{(3)} = \phi_1 f_2^{(2)} + \phi_2 f_1^{(2)} = 0.7 * 0.28 + 0.2 * 0.49 = 0.196 + 0.098 = 0.294$
>
>  Este exemplo demonstra como os coeficientes $f_i^{(s)}$ sÃ£o calculados recursivamente, utilizando os parÃ¢metros do AR(p) e os coeficientes do passo de previsÃ£o anterior.
>
>```python
>import numpy as np
>
># ParÃ¢metros do modelo AR(2)
>phi1 = 0.7
>phi2 = 0.2
>
>def calculate_f_coefficients(phi1, phi2, s):
>    if s == 1:
>        return np.array([phi1, phi2])
>
>    f_prev = calculate_f_coefficients(phi1, phi2, s - 1)
>    f_current = np.zeros(2)
>    f_current[0] = phi1 * f_prev[0]
>    f_current[1] = phi1 * f_prev[1] + phi2 * f_prev[0]
>    return f_current
>
>f1 = calculate_f_coefficients(phi1, phi2, 1)
>f2 = calculate_f_coefficients(phi1, phi2, 2)
>f3 = calculate_f_coefficients(phi1, phi2, 3)
>print(f"f(1): {f1}")
>print(f"f(2): {f2}")
>print(f"f(3): {f3}")
>```

**CorolÃ¡rio 1:** *Propriedade da ConvergÃªncia da PrevisÃ£o*
Para um processo AR(p) estacionÃ¡rio, a previsÃ£o *s*-perÃ­odos Ã  frente, $\hat{Y}_{t+s|t}$, converge para a mÃ©dia do processo, $\mu$, Ã  medida que *s* aumenta. Formalmente,
$$\lim_{s \to \infty} \hat{Y}_{t+s|t} = \mu $$

*Prova*: Como o processo Ã© estacionÃ¡rio, os coeficientes $f_i^{(s)}$ tendem a zero quando $s \rightarrow \infty$. Isso Ã© uma consequÃªncia da estabilidade do processo AR(p), garantindo que o impacto dos valores defasados do processo em previsÃµes mais distantes se dissipe ao longo do tempo. Portanto, quando $s$ tende ao infinito, a previsÃ£o $\hat{Y}_{t+s|t}$ se torna igual Ã  mÃ©dia do processo $\mu$.

I.  A previsÃ£o *s*-perÃ­odos Ã  frente Ã© dada por:
    $$\hat{Y}_{t+s|t} = \mu + \sum_{i=1}^p f_i^{(s)}(Y_{t-i+1}-\mu)$$
    
II.  Sabemos do Lema 1 que $f_i^{(s)}$ sÃ£o definidos recursivamente como:
    $f_i^{(s)} = \sum_{j=1}^p \phi_j f_{i-j}^{(s-1)}$
    
III. Como o processo AR(p) Ã© estacionÃ¡rio, as raÃ­zes do polinÃ´mio caracterÃ­stico estÃ£o fora do cÃ­rculo unitÃ¡rio. Isso implica que os coeficientes $f_i^{(s)}$ convergem para 0 Ã  medida que $s$ tende ao infinito, ou seja:
    $$\lim_{s \to \infty} f_i^{(s)} = 0$$
    
IV. Aplicando esse limite na equaÃ§Ã£o de previsÃ£o:
    $$\lim_{s \to \infty} \hat{Y}_{t+s|t} = \mu + \lim_{s \to \infty} \sum_{i=1}^p f_i^{(s)}(Y_{t-i+1}-\mu) = \mu + \sum_{i=1}^p  \left( \lim_{s \to \infty} f_i^{(s)} \right)(Y_{t-i+1}-\mu) = \mu + \sum_{i=1}^p 0 \cdot (Y_{t-i+1}-\mu) $$

V.  Assim, temos:
    $$\lim_{s \to \infty} \hat{Y}_{t+s|t} = \mu$$
    Portanto, a previsÃ£o *s*-perÃ­odos Ã  frente converge para a mÃ©dia do processo $\mu$ quando $s$ tende ao infinito. â– 

> ğŸ’¡ **Exemplo NumÃ©rico:** Usando o exemplo AR(2) anterior, podemos ver que os coeficientes $f_i^{(s)}$ calculados tendem a diminuir Ã  medida que 's' aumenta. Isso ilustra como a influÃªncia de observaÃ§Ãµes passadas na previsÃ£o diminui com o aumento do horizonte de previsÃ£o, fazendo com que a previsÃ£o eventualmente convirja para a mÃ©dia do processo, que Ã© $\mu=10$.
>
>```python
>import numpy as np
>import matplotlib.pyplot as plt
>
>
>phi1 = 0.7
>phi2 = 0.2
>
>def calculate_f_coefficients(phi1, phi2, s):
>    if s == 1:
>        return np.array([phi1, phi2])
>
>    f_prev = calculate_f_coefficients(phi1, phi2, s - 1)
>    f_current = np.zeros(2)
>    f_current[0] = phi1 * f_prev[0]
>    f_current[1] = phi1 * f_prev[1] + phi2 * f_prev[0]
>    return f_current
>
>
>s_values = np.arange(1, 11)
>f1_values = [calculate_f_coefficients(phi1, phi2, s)[0] for s in s_values]
>f2_values = [calculate_f_coefficients(phi1, phi2, s)[1] for s in s_values]
>
>plt.figure(figsize=(10, 6))
>plt.plot(s_values, f1_values, label='$f_1^{(s)}$')
>plt.plot(s_values, f2_values, label='$f_2^{(s)}$')
>plt.xlabel('Horizonte de PrevisÃ£o (s)')
>plt.ylabel('Coeficientes $f_i^{(s)}$')
>plt.title('ConvergÃªncia dos Coeficientes $f_i^{(s)}$')
>plt.legend()
>plt.grid(True)
>plt.show()
>```
>
>Este grÃ¡fico mostra como os coeficientes $f_1^{(s)}$ e $f_2^{(s)}$ diminuem em magnitude Ã  medida que o horizonte de previsÃ£o (s) aumenta, ilustrando a convergÃªncia da previsÃ£o para a mÃ©dia do processo a longo prazo.

### ConclusÃ£o
Este capÃ­tulo descreveu um mÃ©todo para derivar e implementar previsÃµes *s*-perÃ­odos Ã  frente para processos AR(p). O mÃ©todo envolve a reescrita da equaÃ§Ã£o autoregressiva para expressar os valores futuros em termos de valores passados e choques presentes e passados, iterando o processo de projeÃ§Ã£o linear [^1]. Com essa abordagem, conseguimos derivar previsÃµes Ã³timas em termos de erro quadrÃ¡tico mÃ©dio e tambÃ©m conseguimos entender a relaÃ§Ã£o entre previsÃµes em diferentes horizontes. Esta discussÃ£o fornece a base para modelos de previsÃ£o mais complexos envolvendo componentes de mÃ©dia mÃ³vel [^1]. Em continuidade ao que foi apresentado, o prÃ³ximo passo Ã© explorar como aplicar esses conceitos em um contexto prÃ¡tico utilizando dados reais, e para isso Ã© preciso abordar os mÃ©todos de estimaÃ§Ã£o desses parÃ¢metros.

### ReferÃªncias
[^1]: Texto fornecido.
<!-- END -->
