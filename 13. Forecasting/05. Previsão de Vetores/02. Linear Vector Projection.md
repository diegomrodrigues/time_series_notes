## Previs√£o de Vetores: Generaliza√ß√£o da Proje√ß√£o Linear para M√∫ltiplas Vari√°veis
### Introdu√ß√£o
Em continuidade √† discuss√£o sobre proje√ß√µes lineares, este cap√≠tulo expande o conceito para o contexto multivariado, onde estamos interessados em prever um vetor de vari√°veis, $Y_{t+1}$, usando outro vetor de vari√°veis, $X_t$. Esta generaliza√ß√£o √© crucial para lidar com sistemas complexos onde m√∫ltiplas s√©ries temporais interagem. A adapta√ß√£o do conceito de proje√ß√£o linear para vetores envolve a introdu√ß√£o de um coeficiente de proje√ß√£o que agora se torna uma matriz, e as condi√ß√µes de n√£o correla√ß√£o tamb√©m se aplicam de forma multivariada.

### Conceitos Fundamentais
Como vimos anteriormente, a proje√ß√£o linear √© uma t√©cnica que visa encontrar a melhor aproxima√ß√£o linear de uma vari√°vel em termos de outras vari√°veis. A express√£o [4.1.9] define a proje√ß√£o linear de $Y_{t+1}$ em $X_t$ como $Y^*_{t+1} = \alpha'X_t$ [^2]. Agora, vamos considerar o caso em que $Y_{t+1}$ √© um vetor de dimens√£o (n x 1) e $X_t$ √© um vetor de dimens√£o (m x 1). Neste cen√°rio, a proje√ß√£o linear assume a forma:

$$
\hat{Y}_{t+1} = P(Y_{t+1}|X_t) = \alpha'X_t, \quad [4.1.21]
$$

onde $\alpha'$ agora representa uma matriz de coeficientes de proje√ß√£o de dimens√£o (n x m). Esta matriz $\alpha'$ √© fundamental para estabelecer a rela√ß√£o linear entre os vetores $Y_{t+1}$ e $X_t$.

**Lema 1** *A matriz de coeficientes de proje√ß√£o $\alpha'$ √© √∫nica.*

*Prova:* Suponha que existam duas matrizes de coeficientes de proje√ß√£o, $\alpha_1'$ e $\alpha_2'$, que satisfazem a condi√ß√£o de n√£o correla√ß√£o (4.1.22). Ent√£o, temos:

$E[(Y_{t+1} - \alpha_1'X_t)X_t'] = 0$ e $E[(Y_{t+1} - \alpha_2'X_t)X_t'] = 0$.

I. Subtraindo as duas equa√ß√µes, obtemos:
$$E[(\alpha_2'X_t - \alpha_1'X_t)X_t'] = E[((\alpha_2' - \alpha_1')X_t)X_t'] = 0$$

II. Isso implica que:
$$(\alpha_2' - \alpha_1')E(X_tX_t') = 0$$

III. Como $E(X_tX_t')$ √© invers√≠vel, podemos multiplicar √† direita por $[E(X_tX_t')]^{-1}$, o que resulta em:
$$(\alpha_2' - \alpha_1') = 0$$

IV. Portanto,
$$\alpha_1' = \alpha_2'$$
A matriz de coeficientes de proje√ß√£o √© √∫nica. ‚ñ†

A condi√ß√£o fundamental para que $\alpha'X_t$ seja considerada a proje√ß√£o linear de $Y_{t+1}$ em $X_t$ √© que o erro de previs√£o, dado por $Y_{t+1} - \hat{Y}_{t+1}$, seja n√£o correlacionado com $X_t$. Matematicamente, isso √© expresso como:

$$
E[(Y_{t+1} - \alpha'X_t)X_t'] = 0, \quad [4.1.22]
$$

Esta condi√ß√£o implica que cada um dos $n$ elementos do vetor de erro $(Y_{t+1} - \hat{Y}_{t+1})$ deve ser n√£o correlacionado com cada um dos $m$ elementos do vetor $X_t$. Essa √© a extens√£o da condi√ß√£o de n√£o correla√ß√£o para o caso multivariado.
> üí° **Exemplo Num√©rico:** Imagine que temos um modelo onde queremos prever duas vari√°veis, $Y_{t+1} = \begin{bmatrix} y_{1,t+1} \\ y_{2,t+1} \end{bmatrix}$, usando duas vari√°veis preditoras, $X_t = \begin{bmatrix} x_{1,t} \\ x_{2,t} \end{bmatrix}$. A condi√ß√£o de n√£o correla√ß√£o implica que o erro de previs√£o, $e_{t+1} = Y_{t+1} - \hat{Y}_{t+1} = \begin{bmatrix} y_{1,t+1} - \hat{y}_{1,t+1} \\ y_{2,t+1} - \hat{y}_{2,t+1} \end{bmatrix}$, deve ser n√£o correlacionado com ambas $x_{1,t}$ e $x_{2,t}$. Ou seja, $E[(y_{1,t+1} - \hat{y}_{1,t+1})x_{1,t}] = 0$, $E[(y_{1,t+1} - \hat{y}_{1,t+1})x_{2,t}] = 0$, $E[(y_{2,t+1} - \hat{y}_{2,t+1})x_{1,t}] = 0$ e $E[(y_{2,t+1} - \hat{y}_{2,t+1})x_{2,t}] = 0$.

A matriz de coeficientes de proje√ß√£o, $\alpha'$, pode ser calculada usando momentos populacionais, de forma similar ao caso escalar:

$$
\alpha' = [E(Y_{t+1}X_t')] [E(X_tX_t')]^{-1}, \quad [4.1.23]
$$

√â crucial observar que $E(Y_{t+1}X_t')$ √© uma matriz (n x m), e $E(X_tX_t')$ √© uma matriz (m x m), cuja inversa deve existir para que a matriz $\alpha'$ seja definida.
> üí° **Exemplo Num√©rico:** Suponha que temos as seguintes matrizes de momentos populacionais:
>
> $E(Y_{t+1}X_t') = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}$ e $E(X_tX_t') = \begin{bmatrix} 4 & 0 \\ 0 & 2 \end{bmatrix}$.
>
> Primeiro, calculamos a inversa de $E(X_tX_t')$:
>
> $[E(X_tX_t')]^{-1} = \begin{bmatrix} 1/4 & 0 \\ 0 & 1/2 \end{bmatrix}$.
>
> Em seguida, calculamos a matriz $\alpha'$:
>
> $\alpha' = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix} \begin{bmatrix} 1/4 & 0 \\ 0 & 1/2 \end{bmatrix} = \begin{bmatrix} 2*(1/4) + 1*0 & 2*0 + 1*(1/2) \\ 1*(1/4) + 3*0 & 1*0 + 3*(1/2) \end{bmatrix} = \begin{bmatrix} 1/2 & 1/2 \\ 1/4 & 3/2 \end{bmatrix}$
>
> Assim, a matriz de coeficientes de proje√ß√£o $\alpha'$ √© $\begin{bmatrix} 1/2 & 1/2 \\ 1/4 & 3/2 \end{bmatrix}$. Isto significa que $\hat{Y}_{t+1} = \begin{bmatrix} 1/2 & 1/2 \\ 1/4 & 3/2 \end{bmatrix} \begin{bmatrix} x_{1,t} \\ x_{2,t} \end{bmatrix} = \begin{bmatrix} 0.5x_{1,t} + 0.5x_{2,t} \\ 0.25x_{1,t} + 1.5x_{2,t} \end{bmatrix}$.

A generaliza√ß√£o da express√£o do erro quadr√°tico m√©dio (MSE) para o caso multivariado √© dada por:
$$
MSE(\alpha'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)'], \quad [4.1.24]
$$

que se expande para:
$$
MSE(\alpha'X_t) = E(Y_{t+1}Y_{t+1}') - E(Y_{t+1}X_t')[E(X_tX_t')]^{-1}E(X_tY_{t+1}').
$$
A express√£o [4.1.24] mostra que o MSE, no caso multivariado, √© uma matriz que representa a vari√¢ncia e covari√¢ncia dos erros de previs√£o.
> üí° **Exemplo Num√©rico:** Continuando o exemplo anterior, suponha que $E(Y_{t+1}Y_{t+1}') = \begin{bmatrix} 5 & 2 \\ 2 & 6 \end{bmatrix}$. Utilizando os resultados anteriores, temos:
>
> $MSE(\alpha'X_t) = E(Y_{t+1}Y_{t+1}') - E(Y_{t+1}X_t')[E(X_tX_t')]^{-1}E(X_tY_{t+1}')$
>
> $MSE(\alpha'X_t) = \begin{bmatrix} 5 & 2 \\ 2 & 6 \end{bmatrix} - \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix} \begin{bmatrix} 1/4 & 0 \\ 0 & 1/2 \end{bmatrix} \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}$
>
> $MSE(\alpha'X_t) = \begin{bmatrix} 5 & 2 \\ 2 & 6 \end{bmatrix} - \begin{bmatrix} 1/2 & 1/2 \\ 1/4 & 3/2 \end{bmatrix} \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}$
>
> $MSE(\alpha'X_t) = \begin{bmatrix} 5 & 2 \\ 2 & 6 \end{bmatrix} - \begin{bmatrix} 1.5 & 2 \\ 3.25 & 4.75 \end{bmatrix} = \begin{bmatrix} 3.5 & 0 \\ 0 & 1.25 \end{bmatrix} $.
>
> A matriz MSE resultante √© $\begin{bmatrix} 3.5 & 0 \\ 0 & 1.25 \end{bmatrix}$, onde 3.5 representa a vari√¢ncia do erro de previs√£o para $y_{1,t+1}$ e 1.25 a vari√¢ncia do erro de previs√£o para $y_{2,t+1}$. A aus√™ncia de covari√¢ncia (elementos fora da diagonal principal iguais a zero) indica que os erros de previs√£o para $y_{1,t+1}$ e $y_{2,t+1}$ n√£o s√£o correlacionados neste exemplo particular.

**Teorema 1** *A matriz de coeficientes de proje√ß√£o $\alpha'$ minimiza o MSE no sentido de que para qualquer outra matriz $B$ (n x m), $MSE(\alpha'X_t) \leq MSE(B'X_t)$, onde a desigualdade √© no sentido de matrizes semidefinidas positivas.*

*Prova:* Considere uma matriz $B$ qualquer (n x m). Podemos escrever o MSE associado a $B'X_t$ como:
$$MSE(B'X_t) = E[(Y_{t+1}-B'X_t)(Y_{t+1}-B'X_t)']$$

I. Adicionando e subtraindo $\alpha'X_t$, temos:
$$MSE(B'X_t) = E[(Y_{t+1}-\alpha'X_t + (\alpha'-B')X_t)(Y_{t+1}-\alpha'X_t + (\alpha'-B')X_t)']$$

II. Expandindo, temos:
$$MSE(B'X_t) = E[(Y_{t+1}-\alpha'X_t)(Y_{t+1}-\alpha'X_t)'] + E[(\alpha'-B')X_tX_t'(\alpha-B)] + E[(Y_{t+1}-\alpha'X_t)X_t'(\alpha-B)] + E[(\alpha'-B)X_t(Y_{t+1}-\alpha'X_t)']$$

III.  O √∫ltimo e o pen√∫ltimo termo s√£o iguais a zero pela condi√ß√£o de n√£o correla√ß√£o, pois $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$  e o termo transposto tamb√©m ser√° zero. Assim:
$$MSE(B'X_t) = E[(Y_{t+1}-\alpha'X_t)(Y_{t+1}-\alpha'X_t)'] + E[(\alpha'-B')X_tX_t'(\alpha-B)]$$
$$MSE(B'X_t) = MSE(\alpha'X_t) + E[(\alpha'-B')X_tX_t'(\alpha-B)]$$

IV. Como $E[(\alpha'-B')X_tX_t'(\alpha-B)]$ √© sempre uma matriz semidefinida positiva, temos que:
$$MSE(B'X_t) \geq MSE(\alpha'X_t)$$

V. Portanto $\alpha'$ minimiza o MSE. ‚ñ†

Um ponto importante a se destacar √© que a proje√ß√£o linear de qualquer combina√ß√£o linear dos elementos de $Y_{t+1}$, digamos $h'Y_{t+1}$, onde $h$ √© um vetor de constantes, √© dada por $h'\hat{Y}_{t+1} = h'\alpha'X_t$. Isso decorre diretamente da propriedade da proje√ß√£o linear e da condi√ß√£o de n√£o correla√ß√£o, garantindo que $h'(Y_{t+1} - \hat{Y}_{t+1})$ seja n√£o correlacionado com $X_t$ [^5].
> üí° **Exemplo Num√©rico:** Se $Y_{t+1} = \begin{bmatrix} y_{1,t+1} \\ y_{2,t+1} \end{bmatrix}$ e temos $h = \begin{bmatrix} 2 \\ -1 \end{bmatrix}$, ent√£o $h'Y_{t+1} = 2y_{1,t+1} - y_{2,t+1}$.  A proje√ß√£o linear de $h'Y_{t+1}$ ser√° $h'\hat{Y}_{t+1} = 2\hat{y}_{1,t+1} - \hat{y}_{2,t+1}$.  Usando o $\alpha'$ do exemplo anterior, se $X_t = \begin{bmatrix} 1 \\ 1\end{bmatrix}$,  $\hat{Y}_{t+1} = \begin{bmatrix} 1/2 & 1/2 \\ 1/4 & 3/2 \end{bmatrix} \begin{bmatrix} 1 \\ 1\end{bmatrix} =  \begin{bmatrix} 1 \\ 1.75 \end{bmatrix}$.  Portanto,  $h'\hat{Y}_{t+1} = 2*1 - 1.75 = 0.25$. A condi√ß√£o de n√£o correla√ß√£o garante que o erro de previs√£o $h'(Y_{t+1}-\hat{Y}_{t+1})$ seja n√£o correlacionado com $X_t$.

### Conclus√£o
A extens√£o da proje√ß√£o linear para o caso de vetores $Y_{t+1}$ e $X_t$ introduz uma matriz de coeficientes de proje√ß√£o, $\alpha'$, e imp√µe que cada componente do erro de previs√£o seja n√£o correlacionado com cada componente de $X_t$. Esta generaliza√ß√£o √© vital para modelos econom√©tricos e de s√©ries temporais que lidam com m√∫ltiplas vari√°veis. A capacidade de projetar uma combina√ß√£o linear de componentes de $Y_{t+1}$ tamb√©m √© preservada nessa generaliza√ß√£o. As express√µes para o coeficiente de proje√ß√£o e o MSE s√£o adaptadas para o contexto matricial, mantendo a estrutura conceitual das proje√ß√µes lineares.

### Refer√™ncias
[^2]: *[4.1.9] We now restrict the class of forecasts considered by requiring the forecast $Y^*_{t+1}$ to be a linear function of $X_t$: $Y^*_{t+1} = \alpha'X_t$*
[^5]: *[4.1.22] ...that is, each of the n elements of ($Y_{t+1} - \hat{Y}_{t+1}$) is uncorrelated with each of the m elements of $X_t$. ...to forecast any linear combination of the elements of $Y_{t+1}$, say, $z_{t+1} = h'Y_{t+1}$, the minimum MSE forecast of $z_{t+1}$ requires $(z_{t+1} - \hat{z}_{t+1})$ to be uncorrelated with $X_t$. But since each of the elements of $(Y_{t+1} - \hat{Y}_{t+1})$ is uncorrelated with $X_t$, clearly $h'(Y_{t+1} - \hat{Y}_{t+1})$ is also uncorrelated with $X_t$.*
[^1]: *[4.1.21] The preceding results can be extended to forecast an (n √ó 1) vector $Y_{t+1}$ on the basis of a linear function of an (m x 1) vector $X_t$: $P(Y_{t+1}|X_t) = \alpha'X_t = \hat{Y}_{t+1}$*
[^3]: *[4.1.23] From [4.1.22], the matrix of projection coefficients is given by $\alpha' = [E(Y_{t+1}X_t')] \cdot [E(X_tX_t')]^{-1}$*
[^4]: *[4.1.24] The matrix generalization of the formula for the mean squared error [4.1.15] is $MSE(\alpha'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)'] = E(Y_{t+1}Y_{t+1}') - [E(Y_{t+1}X_t')] \cdot [E(X_tX_t')]^{-1} \cdot [E(X_tY_{t+1})']$*
<!-- END -->
