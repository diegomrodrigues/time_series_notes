## Previs√£o de Vetores: Matriz de Coeficientes de Proje√ß√£o e Erro Quadr√°tico M√©dio Generalizado
### Introdu√ß√£o
Este cap√≠tulo aprofunda a discuss√£o sobre a previs√£o de vetores, concentrando-se especificamente na obten√ß√£o da matriz de coeficientes de proje√ß√£o e na generaliza√ß√£o da f√≥rmula do erro quadr√°tico m√©dio (MSE) para o contexto multivariado. Como vimos anteriormente, quando se trata de prever um vetor de vari√°veis $Y_{t+1}$ com base em um vetor de vari√°veis $X_t$, a proje√ß√£o linear envolve uma matriz de coeficientes $\alpha'$ [^1]. Este cap√≠tulo detalha como essa matriz √© calculada e como a no√ß√£o de MSE √© adaptada para refletir a incerteza associada √† previs√£o vetorial.

### Conceitos Fundamentais

A matriz de coeficientes de proje√ß√£o, $\alpha'$, √© fundamental na previs√£o linear de vetores, pois estabelece a rela√ß√£o linear entre os vetores de vari√°veis $Y_{t+1}$ e $X_t$. A express√£o [4.1.21] define a proje√ß√£o linear de $Y_{t+1}$ em $X_t$ como $\hat{Y}_{t+1} = \alpha'X_t$ [^1]. No contexto de vetores, a condi√ß√£o de n√£o correla√ß√£o entre o erro de previs√£o e os preditores $X_t$ √© dada por $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$ [^2].

Expandindo o conceito apresentado,  a matriz $\alpha'$, que minimiza o erro de previs√£o, √© dada pela express√£o [4.1.23]:
$$
\alpha' = [E(Y_{t+1}X_t')] [E(X_tX_t')]^{-1}, \quad [4.1.23]
$$

Essa f√≥rmula generaliza a express√£o escalar e destaca a import√¢ncia dos momentos populacionais na determina√ß√£o da melhor rela√ß√£o linear entre os vetores. Como demonstrado no Lema 1, a matriz $\alpha'$ que satisfaz a condi√ß√£o de n√£o correla√ß√£o e minimiza o MSE √© √∫nica [^Lema1].

**Lema 1.1** *A matriz $\alpha'$ dada por [4.1.23] √© a √∫nica matriz que satisfaz a condi√ß√£o de ortogonalidade $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$.*

*Prova:*
I. Suponha que exista uma outra matriz $\beta'$ tal que $E[(Y_{t+1} - \beta'X_t)X_t'] = 0$.
II. Expandindo a equa√ß√£o, temos: $E[Y_{t+1}X_t'] - \beta'E[X_tX_t'] = 0$.
III. Isolando $\beta'$, temos: $\beta'E[X_tX_t'] = E[Y_{t+1}X_t']$, que implica $\beta' = E[Y_{t+1}X_t']E[X_tX_t']^{-1}$.
IV. Comparando com a express√£o para $\alpha'$, vemos que $\beta' = \alpha'$.
V. Portanto, $\alpha'$ √© a √∫nica matriz que satisfaz a condi√ß√£o de ortogonalidade.  ‚ñ†

> üí° **Exemplo Num√©rico:** Imagine que queremos prever um vetor $Y_{t+1}$ representando as vendas de dois produtos (digamos, produto A e produto B) com base em um vetor $X_t$ que cont√©m informa√ß√µes sobre tr√™s vari√°veis de marketing (gasto em publicidade online, gasto em publicidade em r√°dio e o n√∫mero de visitas ao site). Assim, $Y_{t+1}$ √© um vetor (2x1) e $X_t$ √© um vetor (3x1). Para calcular $\alpha'$, precisamos das matrizes de momentos populacionais $E(Y_{t+1}X_t')$ (2x3) e $E(X_tX_t')$ (3x3). Suponhamos que, ap√≥s analisar os dados hist√≥ricos, estimamos que
>
> $E(Y_{t+1}X_t') = \begin{bmatrix} 10 & 5 & 2 \\ 8 & 4 & 3 \end{bmatrix}$ e $E(X_tX_t') = \begin{bmatrix} 5 & 1 & 0 \\ 1 & 3 & 1 \\ 0 & 1 & 2 \end{bmatrix}$.
>
> Primeiro, calculamos a inversa de $E(X_tX_t')$. Usando m√©todos de c√°lculo matricial ou uma ferramenta computacional, encontramos que:
>
> $[E(X_tX_t')]^{-1} \approx \begin{bmatrix} 0.25 & -0.083 & 0.042 \\ -0.083 & 0.333 & -0.167 \\ 0.042 & -0.167 & 0.583 \end{bmatrix}$.
>
> Em seguida, calculamos a matriz $\alpha'$:
>
> $\alpha' = \begin{bmatrix} 10 & 5 & 2 \\ 8 & 4 & 3 \end{bmatrix} \begin{bmatrix} 0.25 & -0.083 & 0.042 \\ -0.083 & 0.333 & -0.167 \\ 0.042 & -0.167 & 0.583 \end{bmatrix} \approx \begin{bmatrix} 2.375 & 1.25 & 0.125 \\ 1.833 & 0.917 & 0.333 \end{bmatrix}$.
>
> Portanto, a matriz de coeficientes de proje√ß√£o √© aproximadamente $\begin{bmatrix} 2.375 & 1.25 & 0.125 \\ 1.833 & 0.917 & 0.333 \end{bmatrix}$. Isso significa que a previs√£o linear √© dada por $\hat{Y}_{t+1} = \begin{bmatrix} 2.375 & 1.25 & 0.125 \\ 1.833 & 0.917 & 0.333 \end{bmatrix} X_t$. Por exemplo, o primeiro elemento da primeira linha (2.375) indica como o gasto com publicidade online afeta a venda do produto A, enquanto o segundo elemento da segunda linha (0.917) indica como o gasto com publicidade em r√°dio afeta a venda do produto B.

Em continuidade ao desenvolvimento do conceito, a f√≥rmula do erro quadr√°tico m√©dio (MSE) para o caso multivariado √© dada pela express√£o [4.1.24]:
$$
MSE(\alpha'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)']
$$
e se expande para:
$$
MSE(\alpha'X_t) = E(Y_{t+1}Y_{t+1}') - E(Y_{t+1}X_t')[E(X_tX_t')]^{-1}E(X_tY_{t+1}') \quad [4.1.24].
$$

Essa generaliza√ß√£o √© crucial porque o MSE no caso multivariado n√£o √© um escalar, mas uma matriz que captura tanto a variabilidade dos erros de previs√£o para cada componente de $Y_{t+1}$ quanto as rela√ß√µes de covari√¢ncia entre esses erros.

**Proposi√ß√£o 1** *A matriz MSE definida em [4.1.24] √© sempre semidefinida positiva.*
*Prova:*
I. A express√£o para MSE √© dada por $MSE(\alpha'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)']$.
II. Seja $Z_t = Y_{t+1} - \alpha'X_t$. Ent√£o $MSE(\alpha'X_t) = E[Z_tZ_t']$.
III. Para qualquer vetor $v$, temos $v'E[Z_tZ_t']v = E[v'Z_tZ_t'v]$.
IV. Reorganizando os termos, temos: $E[v'Z_tZ_t'v] = E[(Z_t'v)'(Z_t'v)] = E[\|Z_t'v\|^2]$.
V. Como o quadrado de qualquer norma √© n√£o negativo, $E[\|Z_t'v\|^2] \geq 0$.
VI. Portanto, $MSE(\alpha'X_t)$ √© semidefinida positiva.  ‚ñ†

> üí° **Exemplo Num√©rico:** Continuando com o exemplo anterior, e supondo que $E(Y_{t+1}Y_{t+1}') = \begin{bmatrix} 25 & 15 \\ 15 & 20 \end{bmatrix}$, podemos calcular a matriz MSE:
>
> Primeiro, notamos que  $E(X_tY_{t+1}') = [E(Y_{t+1}X_t')]' =  \begin{bmatrix} 10 & 8 \\ 5 & 4 \\ 2 & 3 \end{bmatrix}$.
>
> Agora, usando os resultados anteriores para $\alpha'$ e $E(X_tX_t')^{-1}$:
>
> $E(Y_{t+1}X_t')[E(X_tX_t')]^{-1}E(X_tY_{t+1}') =  \begin{bmatrix} 10 & 5 & 2 \\ 8 & 4 & 3 \end{bmatrix}  \begin{bmatrix} 0.25 & -0.083 & 0.042 \\ -0.083 & 0.333 & -0.167 \\ 0.042 & -0.167 & 0.583 \end{bmatrix}  \begin{bmatrix} 10 & 8 \\ 5 & 4 \\ 2 & 3 \end{bmatrix}$
>
> $E(Y_{t+1}X_t')[E(X_tX_t')]^{-1}E(X_tY_{t+1}') \approx  \begin{bmatrix} 2.375 & 1.25 & 0.125 \\ 1.833 & 0.917 & 0.333 \end{bmatrix}  \begin{bmatrix} 10 & 8 \\ 5 & 4 \\ 2 & 3 \end{bmatrix} \approx \begin{bmatrix} 30.25 & 24.166 \\ 23.333 & 18.667 \end{bmatrix}$
>
> Agora, calculamos o MSE:
>
> $MSE(\alpha'X_t) = \begin{bmatrix} 25 & 15 \\ 15 & 20 \end{bmatrix} -  \begin{bmatrix} 30.25 & 24.166 \\ 23.333 & 18.667 \end{bmatrix} \approx \begin{bmatrix} -5.25 & -9.166 \\ -8.333 & 1.333 \end{bmatrix}$
>
> **Observa√ß√£o:** Os valores obtidos como exemplo nesse caso levam a um MSE com alguns valores negativos, o que n√£o faz sentido. √â crucial destacar que $MSE$ √© uma matriz semidefinida positiva e, portanto, os valores da diagonal n√£o podem ser negativos. O c√°lculo acima tem intuito did√°tico de apresentar como a f√≥rmula √© aplicada. Em um contexto real, os valores das matrizes de momentos seriam obtidos de dados amostrais, resultando em um $MSE$ semidefinida positiva. Em outras palavras, este exemplo num√©rico est√° usando valores irreais para fins ilustrativos.
>
> A matriz MSE resultante nos fornece informa√ß√µes sobre a vari√¢ncia dos erros de previs√£o para cada componente (diagonal da matriz), bem como sobre as covari√¢ncias entre esses erros (elementos fora da diagonal). Por exemplo, a vari√¢ncia do erro de previs√£o para o primeiro produto (produto A) √© -5.25 (valor irreal para um exemplo did√°tico), enquanto a covari√¢ncia entre o erro de previs√£o do produto A e o produto B √© -9.166. Uma matriz MSE com valores negativos na diagonal indica que os dados de entrada foram utilizados para fins ilustrativos e n√£o correspondem a um cen√°rio real.

O Teorema 1 garante que a matriz de coeficientes de proje√ß√£o $\alpha'$, dada por [4.1.23], minimiza o MSE no sentido de matrizes semidefinidas positivas [^Teorema1]. A prova desse teorema demonstra que para qualquer outra matriz B, o MSE associado a $B'X_t$ √© sempre maior ou igual ao MSE associado a $\alpha'X_t$, garantindo a otimalidade da proje√ß√£o linear.
> üí° **Teorema 1** *A matriz de coeficientes de proje√ß√£o $\alpha'$ minimiza o MSE no sentido de que para qualquer outra matriz $B$ (n x m), $MSE(\alpha'X_t) \leq MSE(B'X_t)$, onde a desigualdade √© no sentido de matrizes semidefinidas positivas.*
>
> *Prova:*
I.  Como apresentado anteriormente, considere uma matriz $B$ qualquer (n x m). O MSE associado a $B'X_t$ pode ser escrito como:
$$MSE(B'X_t) = E[(Y_{t+1}-B'X_t)(Y_{t+1}-B'X_t)']$$
II. Adicionando e subtraindo $\alpha'X_t$, temos:
$$MSE(B'X_t) = E[(Y_{t+1}-\alpha'X_t + (\alpha'-B')X_t)(Y_{t+1}-\alpha'X_t + (\alpha'-B')X_t)']$$
III. Expandindo, temos:
$$MSE(B'X_t) = E[(Y_{t+1}-\alpha'X_t)(Y_{t+1}-\alpha'X_t)'] + E[(\alpha'-B')X_tX_t'(\alpha-B)] + E[(Y_{t+1}-\alpha'X_t)X_t'(\alpha-B)] + E[(\alpha'-B)X_t(Y_{t+1}-\alpha'X_t)']$$
IV. O √∫ltimo e o pen√∫ltimo termo s√£o iguais a zero pela condi√ß√£o de n√£o correla√ß√£o, pois $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$ e o termo transposto tamb√©m ser√° zero. Assim:
$$MSE(B'X_t) = E[(Y_{t+1}-\alpha'X_t)(Y_{t+1}-\alpha'X_t)'] + E[(\alpha'-B')X_tX_t'(\alpha-B)]$$
$$MSE(B'X_t) = MSE(\alpha'X_t) + E[(\alpha'-B')X_tX_t'(\alpha-B)]$$
V. Como $E[(\alpha'-B')X_tX_t'(\alpha-B)]$ √© sempre uma matriz semidefinida positiva, temos que:
$$MSE(B'X_t) \geq MSE(\alpha'X_t)$$
VI. Portanto, $\alpha'$ minimiza o MSE. ‚ñ†
>
> Este teorema formaliza a otimalidade da matriz $\alpha'$ para previs√µes lineares de vetores.

**Teorema 1.1** *Se a matriz $E[X_tX_t']$ for definida positiva, ent√£o a matriz $E[(\alpha' - B')X_tX_t'(\alpha - B)]$ √© definida positiva se $B \neq \alpha$.*
*Prova:*
I. Como $E[X_tX_t']$ √© definida positiva, ent√£o para qualquer vetor $v \neq 0$, $v'E[X_tX_t']v > 0$.
II. Seja $M = E[(\alpha' - B')X_tX_t'(\alpha - B)]$.
III. Considere um vetor $z \neq 0$. Ent√£o, $z'Mz = z'E[(\alpha' - B')X_tX_t'(\alpha - B)]z = E[z'(\alpha' - B')X_tX_t'(\alpha - B)z]$.
IV. Seja $w = (\alpha - B)z$. Ent√£o $z'Mz = E[w'X_tX_t'w]$.
V. Se $B \neq \alpha$, ent√£o existe um $z \neq 0$ tal que $w \neq 0$. Assim, como $E[X_tX_t']$ √© definida positiva, $E[w'X_tX_t'w] > 0$ e $M$ √© definida positiva.‚ñ†

### Conclus√£o
Este cap√≠tulo explorou em detalhes a matriz de coeficientes de proje√ß√£o $\alpha'$ e a generaliza√ß√£o do MSE para o contexto de previs√£o vetorial. A express√£o $\alpha' = [E(Y_{t+1}X_t')] [E(X_tX_t')]^{-1}$ fornece a matriz que estabelece a melhor rela√ß√£o linear entre os vetores $Y_{t+1}$ e $X_t$. A express√£o do MSE, $E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)']$, quantifica a variabilidade e covariabilidade dos erros de previs√£o. Esses resultados s√£o fundamentais para a constru√ß√£o e avalia√ß√£o de modelos de previs√£o em econometria e an√°lise de s√©ries temporais que envolvam m√∫ltiplas vari√°veis.

### Refer√™ncias
[^1]: *[4.1.21] The preceding results can be extended to forecast an (n √ó 1) vector $Y_{t+1}$ on the basis of a linear function of an (m x 1) vector $X_t$: $P(Y_{t+1}|X_t) = \alpha'X_t = \hat{Y}_{t+1}$*
[^2]: *[4.1.22] ...that is, each of the n elements of ($Y_{t+1} - \hat{Y}_{t+1}$) is uncorrelated with each of the m elements of $X_t$ ...*
[^3]: *[4.1.23] From [4.1.22], the matrix of projection coefficients is given by $\alpha' = [E(Y_{t+1}X_t')] \cdot [E(X_tX_t')]^{-1}$*
[^4]: *[4.1.24] The matrix generalization of the formula for the mean squared error [4.1.15] is $MSE(\alpha'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)'] = E(Y_{t+1}Y_{t+1}') - [E(Y_{t+1}X_t')] \cdot [E(X_tX_t')]^{-1} \cdot [E(X_tY_{t+1})']$*
[^Teorema1]: *Teorema 1: A matriz de coeficientes de proje√ß√£o Œ±' minimiza o MSE no sentido de que para qualquer outra matriz B (n x m), $MSE(\alpha'X_t) \leq MSE(B'X_t)$, onde a desigualdade √© no sentido de matrizes semidefinidas positivas.*
[^Lema1]: *Lema 1: A matriz de coeficientes de proje√ß√£o Œ±' √© √∫nica.*
<!-- END -->
