## Previs√£o de Vetores: Extens√£o da Proje√ß√£o Linear para Vetores e An√°lise da Matriz de Coeficientes
### Introdu√ß√£o
Este cap√≠tulo explora a extens√£o da proje√ß√£o linear para cen√°rios onde tanto a vari√°vel a ser prevista quanto as vari√°veis preditoras s√£o vetores, um avan√ßo crucial para a an√°lise de sistemas complexos com m√∫ltiplas s√©ries temporais. A proje√ß√£o linear, nesse contexto, torna-se uma ferramenta poderosa, fornecendo previs√µes √≥timas com base em rela√ß√µes lineares entre conjuntos de vari√°veis. A discuss√£o se aprofunda na interpreta√ß√£o e no c√°lculo da matriz de coeficientes de proje√ß√£o, destacando sua import√¢ncia na previs√£o multivariada e sua conex√£o com a condi√ß√£o de n√£o correla√ß√£o [^1, ^2]. Este cap√≠tulo se baseia nos conceitos previamente introduzidos, utilizando os resultados obtidos no caso escalar e generalizando-os para o caso vetorial.

### Conceitos Fundamentais

A extens√£o da proje√ß√£o linear para vetores envolve a considera√ß√£o de que tanto a vari√°vel a ser prevista, $Y_{t+1}$, quanto as vari√°veis preditoras, $X_t$, s√£o vetores. Formalmente, essa proje√ß√£o linear √© expressa como:

$$
P(Y_{t+1}|X_t) = \alpha'X_t = \hat{Y}_{t+1} \quad [4.1.21]
$$

onde $Y_{t+1}$ √© um vetor (n x 1), $X_t$ √© um vetor (m x 1), e $\alpha'$ √© uma matriz de coeficientes de proje√ß√£o de dimens√£o (n x m) [^1]. Esta matriz $\alpha'$ √© essencial para capturar as rela√ß√µes lineares entre os componentes de $Y_{t+1}$ e $X_t$.

A condi√ß√£o de n√£o correla√ß√£o, crucial para garantir que $\hat{Y}_{t+1}$ seja a melhor aproxima√ß√£o linear de $Y_{t+1}$, √© dada por [4.1.22]:
$$
E[(Y_{t+1} - \alpha'X_t)X_t'] = 0 \quad [4.1.22]
$$

Esta condi√ß√£o implica que cada elemento do erro de previs√£o, $Y_{t+1} - \alpha'X_t$, deve ser n√£o correlacionado com cada elemento de $X_t$. Essa √© uma extens√£o natural da condi√ß√£o de n√£o correla√ß√£o do caso escalar e √© fundamental para garantir a otimalidade da proje√ß√£o linear no caso multivariado [^2].

Como vimos em cap√≠tulos anteriores, a matriz de coeficientes de proje√ß√£o $\alpha'$ √© obtida atrav√©s da seguinte express√£o [4.1.23]:

$$
\alpha' = [E(Y_{t+1}X_t')] [E(X_tX_t')]^{-1} \quad [4.1.23]
$$

Esta f√≥rmula generaliza o caso escalar, utilizando momentos populacionais dos vetores $Y_{t+1}$ e $X_t$. A matriz $E(Y_{t+1}X_t')$ √© de dimens√£o (n x m), enquanto $E(X_tX_t')$ √© de dimens√£o (m x m), e a inversa desta √∫ltima deve existir para que a matriz $\alpha'$ seja definida.

**Lema 3.1** *A matriz $\alpha'$ definida em [4.1.23] satisfaz a condi√ß√£o de n√£o correla√ß√£o $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$.*

*Prova:*
I. Substituindo a express√£o de $\alpha'$ na condi√ß√£o de n√£o correla√ß√£o, obtemos:
   $E[(Y_{t+1} - [E(Y_{t+1}X_t')] [E(X_tX_t')]^{-1}X_t)X_t'] = E[Y_{t+1}X_t'] - [E(Y_{t+1}X_t')] [E(X_tX_t')]^{-1}E[X_tX_t']$.
II. Simplificando, obtemos: $E[Y_{t+1}X_t'] - E[Y_{t+1}X_t'] = 0$.
III. Portanto, a matriz $\alpha'$ satisfaz a condi√ß√£o de n√£o correla√ß√£o.  $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um cen√°rio onde queremos prever o consumo de energia de duas regi√µes ($Y_{t+1}$) com base em tr√™s vari√°veis preditoras ($X_t$): temperatura m√©dia, n√≠vel de atividade industrial e dia da semana (codificado como 1 para dias √∫teis e 0 para fim de semana).  Seja $Y_{t+1} = \begin{bmatrix} y_{1,t+1} \\ y_{2,t+1} \end{bmatrix}$ representando o consumo de energia das regi√µes 1 e 2, respectivamente, e $X_t = \begin{bmatrix} x_{1,t} \\ x_{2,t} \\ x_{3,t} \end{bmatrix}$ representando a temperatura, atividade industrial e dia da semana. Suponha que os momentos populacionais sejam estimados como:
>
>  $E(Y_{t+1}X_t') = \begin{bmatrix} 10 & 7 & 2 \\ 8 & 12 & 5 \end{bmatrix}$ e $E(X_tX_t') = \begin{bmatrix} 25 & 5 & 2 \\ 5 & 10 & 1 \\ 2 & 1 & 2 \end{bmatrix}$.
>
> Primeiro, calculamos a inversa de $E(X_tX_t')$:
> ```python
> import numpy as np
>
> E_XX = np.array([[25, 5, 2], [5, 10, 1], [2, 1, 2]])
> E_XX_inv = np.linalg.inv(E_XX)
> print("Inversa de E(X_tX_t'):\n", E_XX_inv)
> ```
>  Resultando em:
>  $(E(X_tX_t'))^{-1} = \begin{bmatrix} 0.044 & -0.023 & -0.004 \\ -0.023 & 0.109 & -0.027 \\ -0.004 & -0.027 & 0.533 \end{bmatrix}$ (aproximadamente).
>
> Em seguida, calculamos a matriz $\alpha'$:
> ```python
> E_YX = np.array([[10, 7, 2], [8, 12, 5]])
> alpha_prime = np.dot(E_YX, E_XX_inv)
> print("Matriz alpha':\n", alpha_prime)
> ```
>
> $\alpha' = \begin{bmatrix} 10 & 7 & 2 \\ 8 & 12 & 5 \end{bmatrix} \begin{bmatrix} 0.044 & -0.023 & -0.004 \\ -0.023 & 0.109 & -0.027 \\ -0.004 & -0.027 & 0.533 \end{bmatrix} = \begin{bmatrix} 0.277 & 0.644 & 0.840 \\ 0.469 & 1.106 & 2.273 \end{bmatrix}$ (aproximadamente).
>
> A matriz $\alpha'$ resultante estabelece a rela√ß√£o linear entre os vetores $Y_{t+1}$ e $X_t$. Por exemplo, o elemento  $(\alpha')_{11} = 0.277$ indica que um aumento de uma unidade na temperatura m√©dia ($x_1$) est√° associado a um aumento de 0.277 unidades no consumo de energia da regi√£o 1 ($y_1$), mantendo os outros preditores constantes. O elemento  $(\alpha')_{23} = 2.273$ indica que, em dias √∫teis ($x_3=1$), o consumo de energia da regi√£o 2 ($y_2$) tende a ser 2.273 unidades maior do que em fins de semana ($x_3=0$).

A matriz de coeficientes de proje√ß√£o, $\alpha'$, tem uma interpreta√ß√£o crucial. Cada elemento $(\alpha')_{ij}$ representa o efeito da vari√°vel $x_j$ (o j-√©simo componente de $X_t$) na previs√£o da vari√°vel $y_i$ (o i-√©simo componente de $Y_{t+1}$). Assim, $\alpha'$ n√£o apenas define a rela√ß√£o linear entre os vetores, mas tamb√©m quantifica a influ√™ncia de cada componente preditor em cada componente previsto.

**Proposi√ß√£o 3.1** *A proje√ß√£o linear $\hat{Y}_{t+1} = \alpha'X_t$ minimiza a matriz de erro quadr√°tico m√©dio (MSE), definida como $MSE(\alpha'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)']$, ou seja, para qualquer matriz $B$ (n x m), $MSE(\alpha'X_t) \leq MSE(B'X_t)$.*

*Prova:*
I. Seja $e_{t+1} = Y_{t+1} - \alpha'X_t$ o erro de previs√£o da proje√ß√£o linear $\hat{Y}_{t+1} = \alpha'X_t$. Seja $B$ uma matriz de coeficientes de dimens√£o (n x m), e considere um preditor linear alternativo dado por $\tilde{Y}_{t+1} = B'X_t$.
II. O erro de previs√£o associado a esse preditor √© dado por $\tilde{e}_{t+1} = Y_{t+1} - B'X_t$.
III. Podemos escrever o MSE de $\tilde{Y}_{t+1}$ como:
$MSE(B'X_t) = E[(Y_{t+1} - B'X_t)(Y_{t+1} - B'X_t)']$.
IV. Adicionando e subtraindo $\alpha'X_t$, temos:
$MSE(B'X_t) = E[(Y_{t+1} - \alpha'X_t + (\alpha' - B')X_t)(Y_{t+1} - \alpha'X_t + (\alpha' - B')X_t)']$.
V. Expandindo, temos:
$MSE(B'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)'] + E[(\alpha' - B')X_tX_t'(\alpha - B)'] + E[(Y_{t+1} - \alpha'X_t)X_t'(\alpha - B)'] + E[(\alpha - B)'X_t(Y_{t+1} - \alpha'X_t)']$.
VI. Pela condi√ß√£o de n√£o correla√ß√£o $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$, os dois √∫ltimos termos s√£o nulos, e temos:
$MSE(B'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)'] + E[(\alpha' - B')X_tX_t'(\alpha - B)'] = MSE(\alpha'X_t) + E[(\alpha' - B')X_tX_t'(\alpha - B)']$.
VII. O termo $E[(\alpha' - B')X_tX_t'(\alpha - B)']$ √© uma matriz semidefinida positiva, pois √© uma esperan√ßa do produto de uma matriz por sua transposta. Portanto, $MSE(B'X_t) \geq MSE(\alpha'X_t)$, onde a desigualdade √© no sentido de matrizes semidefinidas positivas.
VIII. Assim, a proje√ß√£o linear $\hat{Y}_{t+1} = \alpha'X_t$ minimiza a matriz MSE.  $\blacksquare$

> üí° **Exemplo Num√©rico:** Para ilustrar essa propriedade, considere o caso anterior com $\alpha' = \begin{bmatrix} 0.277 & 0.644 & 0.840 \\ 0.469 & 1.106 & 2.273 \end{bmatrix}$ e suponha que usemos um preditor linear alternativo, $\tilde{Y}_{t+1} = B'X_t$ com $B' = \begin{bmatrix} 0.3 & 0.7 & 0.5 \\ 0.5 & 1.2 & 2.0 \end{bmatrix}$.  Suponha que $E(Y_{t+1}Y_{t+1}') = \begin{bmatrix} 40 & 30 \\ 30 & 60 \end{bmatrix}$. Para simplificar o exemplo, vamos considerar um caso onde $X_t$ √© um vetor de valores fixos, por exemplo  $X_t = \begin{bmatrix} 10 \\ 5 \\ 1 \end{bmatrix}$.
>
> Primeiro, calculemos  $\hat{Y}_{t+1} = \alpha'X_t$ e $\tilde{Y}_{t+1} = B'X_t$:
>  $\hat{Y}_{t+1} = \begin{bmatrix} 0.277 & 0.644 & 0.840 \\ 0.469 & 1.106 & 2.273 \end{bmatrix} \begin{bmatrix} 10 \\ 5 \\ 1 \end{bmatrix} = \begin{bmatrix} 7.83 \\ 13.54 \end{bmatrix}$
>  $\tilde{Y}_{t+1} = \begin{bmatrix} 0.3 & 0.7 & 0.5 \\ 0.5 & 1.2 & 2.0 \end{bmatrix} \begin{bmatrix} 10 \\ 5 \\ 1 \end{bmatrix} = \begin{bmatrix} 7 \\ 16 \end{bmatrix}$
>
> Agora, vamos simular um valor de $Y_{t+1}$, digamos $Y_{t+1} = \begin{bmatrix} 8 \\ 15 \end{bmatrix}$, para calcularmos o MSE.
>
> O erro da proje√ß√£o linear √©: $e_{t+1} = Y_{t+1} - \hat{Y}_{t+1} = \begin{bmatrix} 8 \\ 15 \end{bmatrix} - \begin{bmatrix} 7.83 \\ 13.54 \end{bmatrix} = \begin{bmatrix} 0.17 \\ 1.46 \end{bmatrix}$
>
> O erro do preditor alternativo √©:  $\tilde{e}_{t+1} = Y_{t+1} - \tilde{Y}_{t+1} = \begin{bmatrix} 8 \\ 15 \end{bmatrix} - \begin{bmatrix} 7 \\ 16 \end{bmatrix} = \begin{bmatrix} 1 \\ -1 \end{bmatrix}$
>
> Agora, calculemos as matrizes MSEs:
>
> $MSE(\alpha'X_t) = E[(Y_{t+1} - \hat{Y}_{t+1})(Y_{t+1} - \hat{Y}_{t+1})'] = \begin{bmatrix} 0.17 \\ 1.46 \end{bmatrix}\begin{bmatrix} 0.17 & 1.46 \end{bmatrix} = \begin{bmatrix} 0.0289 & 0.2482 \\ 0.2482 & 2.1316 \end{bmatrix}$
>
> $MSE(B'X_t) = E[(Y_{t+1} - \tilde{Y}_{t+1})(Y_{t+1} - \tilde{Y}_{t+1})'] = \begin{bmatrix} 1 \\ -1 \end{bmatrix}\begin{bmatrix} 1 & -1 \end{bmatrix} = \begin{bmatrix} 1 & -1 \\ -1 & 1 \end{bmatrix}$
>
> Observamos que a matriz $MSE(B'X_t) - MSE(\alpha'X_t) =  \begin{bmatrix} 0.9711 & -1.2482 \\ -1.2482 & -1.1316 \end{bmatrix}$ n√£o √© semidefinida positiva, o que √© esperado j√° que estamos usando um √∫nico ponto para fazer o c√°lculo e estamos aproximando a esperan√ßa. Em um cen√°rio real com um conjunto grande de dados, a desigualdade $MSE(B'X_t) \geq MSE(\alpha'X_t)$  seria verificada em termos de matrizes semidefinidas positivas.

A propriedade de otimalidade tamb√©m se estende a combina√ß√µes lineares dos elementos de $Y_{t+1}$. Se estamos interessados em prever $h'Y_{t+1}$, onde $h$ √© um vetor de constantes, a melhor proje√ß√£o linear √© dada por $h'\hat{Y}_{t+1}$, como demonstrado em cap√≠tulos anteriores [^5]. Isso decorre diretamente da condi√ß√£o de n√£o correla√ß√£o e das propriedades da proje√ß√£o linear.

**Lema 3.2** *Se a condi√ß√£o de n√£o correla√ß√£o $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$ √© satisfeita, ent√£o, para qualquer vetor constante $h$, a proje√ß√£o linear de $h'Y_{t+1}$ √© dada por $h'\hat{Y}_{t+1}$.*

*Prova:*
I. Considere a combina√ß√£o linear $h'Y_{t+1}$, onde $h$ √© um vetor constante.
II. A proje√ß√£o linear de $h'Y_{t+1}$ sobre $X_t$ √© dada por $P(h'Y_{t+1}|X_t) = E(h'Y_{t+1}X_t')E(X_tX_t')^{-1}X_t$.
III. Usando a propriedade da esperan√ßa, $E(h'Y_{t+1}X_t') = h'E(Y_{t+1}X_t')$.
IV. Substituindo, obtemos $P(h'Y_{t+1}|X_t) = h'E(Y_{t+1}X_t')E(X_tX_t')^{-1}X_t$.
V. Como $\alpha' = E(Y_{t+1}X_t')E(X_tX_t')^{-1}$ e $\hat{Y}_{t+1} = \alpha'X_t$, temos $P(h'Y_{t+1}|X_t) = h'\alpha'X_t = h'\hat{Y}_{t+1}$.
VI. Portanto, a proje√ß√£o linear de $h'Y_{t+1}$ √© $h'\hat{Y}_{t+1}$, que √© a mesma combina√ß√£o linear da proje√ß√£o linear de $Y_{t+1}$.  $\blacksquare$

> üí° **Exemplo Num√©rico:** Utilizando a matriz $\alpha'$ calculada anteriormente,  $\alpha' = \begin{bmatrix} 0.277 & 0.644 & 0.840 \\ 0.469 & 1.106 & 2.273 \end{bmatrix}$, considere $h = \begin{bmatrix} 1 \\ -0.5 \end{bmatrix}$. Ent√£o $h'Y_{t+1} = y_{1,t+1} - 0.5y_{2,t+1}$. Se $X_t = \begin{bmatrix} 10 \\ 5 \\ 1 \end{bmatrix}$, ent√£o $\hat{Y}_{t+1} = \begin{bmatrix} 0.277(10)+0.644(5)+0.840(1) \\ 0.469(10)+1.106(5)+2.273(1) \end{bmatrix} =  \begin{bmatrix} 7.83 \\ 13.54 \end{bmatrix}$. A proje√ß√£o linear de $h'Y_{t+1}$ √© dada por $h'\hat{Y}_{t+1} = 1(7.83) - 0.5(13.54)= 1.06$. Este resultado demonstra que n√£o precisamos recalcular a matriz de coeficientes de proje√ß√£o para obter a melhor previs√£o linear de qualquer combina√ß√£o linear de $Y_{t+1}$, pois basta utilizar a proje√ß√£o de $Y_{t+1}$. A proje√ß√£o de $h'Y_{t+1}$ usando a mesma combina√ß√£o linear √© $1(y_{1,t+1})-0.5(y_{2,t+1})$.

**Lema 3.3** *A condi√ß√£o de n√£o correla√ß√£o $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$ implica que os res√≠duos da proje√ß√£o linear, $Y_{t+1} - \alpha'X_t$, s√£o ortogonais ao espa√ßo gerado pelas vari√°veis preditoras $X_t$.*
*Prova:*
I. Seja $e_{t+1} = Y_{t+1} - \alpha'X_t$ o vetor de erros da proje√ß√£o linear.
II. A condi√ß√£o de n√£o correla√ß√£o dada por $E[e_{t+1}X_t'] = 0$ implica que cada elemento do vetor $e_{t+1}$ √© n√£o correlacionado com cada elemento do vetor $X_t$.
III. Geometricamente, isto significa que o vetor de erros $e_{t+1}$ √© ortogonal ao espa√ßo vetorial gerado pelos vetores $X_t$. Mais especificamente, o produto interno entre $e_{t+1}$ e qualquer combina√ß√£o linear dos elementos de $X_t$ √© zero, pois $E[e_{t+1}(c'X_t)] = c'E[e_{t+1}X_t']=0$ para qualquer vetor constante $c$.
IV. Portanto, os res√≠duos da proje√ß√£o linear s√£o ortogonais ao espa√ßo gerado pelas vari√°veis preditoras $X_t$. $\blacksquare$

A condi√ß√£o de n√£o correla√ß√£o tamb√©m tem implica√ß√µes importantes para a decomposi√ß√£o da vari√¢ncia de $Y_{t+1}$. Podemos expressar a matriz de covari√¢ncia de $Y_{t+1}$ como a soma da matriz de covari√¢ncia da proje√ß√£o linear $\hat{Y}_{t+1}$ e a matriz de covari√¢ncia do erro $e_{t+1} = Y_{t+1} - \hat{Y}_{t+1}$.

**Proposi√ß√£o 3.2** *A matriz de covari√¢ncia de $Y_{t+1}$ pode ser decomposta da seguinte forma:*
$$
E(Y_{t+1}Y_{t+1}') = E(\hat{Y}_{t+1}\hat{Y}_{t+1}') + E(e_{t+1}e_{t+1}')
$$
*onde $e_{t+1} = Y_{t+1} - \hat{Y}_{t+1}$ √© o erro da proje√ß√£o linear.*
*Prova:*
I. Sabemos que $Y_{t+1} = \hat{Y}_{t+1} + e_{t+1}$.
II. Calculando a matriz de covari√¢ncia de $Y_{t+1}$, obtemos:
    $E(Y_{t+1}Y_{t+1}') = E[(\hat{Y}_{t+1} + e_{t+1})(\hat{Y}_{t+1} + e_{t+1})']$.
III. Expandindo, temos:
    $E(Y_{t+1}Y_{t+1}') = E(\hat{Y}_{t+1}\hat{Y}_{t+1}') + E(e_{t+1}e_{t+1}') + E(\hat{Y}_{t+1}e_{t+1}') + E(e_{t+1}\hat{Y}_{t+1}')$.
IV. Como $\hat{Y}_{t+1} = \alpha'X_t$ e $E(e_{t+1}X_t')=0$, segue que $E(\hat{Y}_{t+1}e_{t+1}') = E(\alpha'X_te_{t+1}') = \alpha'E(X_te_{t+1}') = 0$ e $E(e_{t+1}\hat{Y}_{t+1}') = E(e_{t+1}X_t'\alpha) = E(e_{t+1}X_t')\alpha = 0$.
V. Portanto, $E(Y_{t+1}Y_{t+1}') = E(\hat{Y}_{t+1}\hat{Y}_{t+1}') + E(e_{t+1}e_{t+1}')$. $\blacksquare$
> üí° **Exemplo Num√©rico:** Usando os dados do exemplo anterior, onde $Y_{t+1} = \begin{bmatrix} y_{1,t+1} \\ y_{2,t+1} \end{bmatrix}$, $\hat{Y}_{t+1} = \begin{bmatrix} \hat{y}_{1,t+1} \\ \hat{y}_{2,t+1} \end{bmatrix}$, e $e_{t+1} = \begin{bmatrix} e_{1,t+1} \\ e_{2,t+1} \end{bmatrix}$, vamos ilustrar essa decomposi√ß√£o. Suponha que $E(Y_{t+1}Y_{t+1}') = \begin{bmatrix} 40 & 30 \\ 30 & 60 \end{bmatrix}$, $E(X_tX_t') = \begin{bmatrix} 25 & 5 & 2 \\ 5 & 10 & 1 \\ 2 & 1 & 2 \end{bmatrix}$, $E(Y_{t+1}X_t') = \begin{bmatrix} 10 & 7 & 2 \\ 8 & 12 & 5 \end{bmatrix}$, e usando $X_t = \begin{bmatrix} 10 \\ 5 \\ 1 \end{bmatrix}$,  calculamos $\hat{Y}_{t+1} = \begin{bmatrix} 7.83 \\ 13.54 \end{bmatrix}$.
>
> $E(\hat{Y}_{t+1}\hat{Y}_{t+1}') = \begin{bmatrix} 7.83 \\ 13.54 \end{bmatrix} \begin{bmatrix} 7.83 & 13.54 \end{bmatrix} = \begin{bmatrix} 61.3089 & 105.9182 \\ 105.9182 & 183.3316 \end{bmatrix}$. Note que isso √© um c√°lculo usando um √∫nico ponto e, portanto, n√£o representa a verdadeira matriz de covari√¢ncia. Para calcular o valor real precisamos de uma amostra grande.
>
>  Assumindo que $E(e_{t+1}e_{t+1}') = \begin{bmatrix} 40 & 30 \\ 30 & 60 \end{bmatrix} - \begin{bmatrix} 61.3089 & 105.9182 \\ 105.9182 & 183.3316 \end{bmatrix} = \begin{bmatrix} -21.3089 & -75.9182 \\ -75.9182 & -123.3316 \end{bmatrix}$, observamos que essa matriz n√£o √© definida positiva, o que √© esperado, pois estamos usando um √∫nico ponto para aproximar os valores.
>
> A decomposi√ß√£o te√≥rica mostra que a vari√¢ncia total de $Y_{t+1}$ √© a soma da vari√¢ncia explicada pelo modelo linear e da vari√¢ncia do erro. Em um cen√°rio real com um grande n√∫mero de dados, essa decomposi√ß√£o seria confirmada.

Essa decomposi√ß√£o mostra que a variabilidade total de $Y_{t+1}$ √© composta pela variabilidade explicada pela proje√ß√£o linear ($\hat{Y}_{t+1}$) e pela variabilidade do erro n√£o explicado pela proje√ß√£o ($e_{t+1}$). Essa rela√ß√£o √© uma extens√£o natural da decomposi√ß√£o da vari√¢ncia no caso escalar.

### Conclus√£o
Este cap√≠tulo forneceu uma an√°lise aprofundada da proje√ß√£o linear no contexto multivariado. A matriz de coeficientes de proje√ß√£o $\alpha'$ √© uma ferramenta essencial para modelar rela√ß√µes lineares entre vetores de vari√°veis, e sua estrutura √© intimamente ligada √† condi√ß√£o de n√£o correla√ß√£o. A proje√ß√£o linear gera previs√µes de m√≠nimo MSE n√£o apenas para os componentes individuais do vetor $Y_{t+1}$, mas tamb√©m para suas combina√ß√µes lineares. Este resultado destaca a flexibilidade e a robustez da proje√ß√£o linear como ferramenta de previs√£o em contextos onde m√∫ltiplas vari√°veis interagem. As propriedades de otimalidade, ortogonalidade e a decomposi√ß√£o da vari√¢ncia fornecem uma estrutura s√≥lida para a an√°lise de sistemas complexos e a constru√ß√£o de modelos de previs√£o eficazes.
### Refer√™ncias
[^1]: *[4.1.21] The preceding results can be extended to forecast an (n √ó 1) vector $Y_{t+1}$ on the basis of a linear function of an (m x 1) vector $X_t$: $P(Y_{t+1}|X_t) = \alpha'X_t = \hat{Y}_{t+1}$*
[^2]: *[4.1.22] ...that is, each of the n elements of ($Y_{t+1} - \hat{Y}_{t+1}$) is uncorrelated with each of the m elements of $X_t$.*
[^3]: *[4.1.23] From [4.1.22], the matrix of projection coefficients is given by $\alpha' = [E(Y_{t+1}X_t')] \cdot [E(X_tX_t')]^{-1}$*
[^4]: *[4.1.24] The matrix generalization of the formula for the mean squared error [4.1.15] is $MSE(\alpha'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)'] = E(Y_{t+1}Y_{t+1}') - [E(Y_{t+1}X_t')] \cdot [E(X_tX_t')]^{-1} \cdot [E(X_tY_{t+1})']$*
[^5]: *[4.1.22] ...to forecast any linear combination of the elements of $Y_{t+1}$, say, $z_{t+1} = h'Y_{t+1}$, the minimum MSE forecast of $z_{t+1}$ requires $(z_{t+1} - \hat{z}_{t+1})$ to be uncorrelated with $X_t$. But since each of the elements of $(Y_{t+1} - \hat{Y}_{t+1})$ is uncorrelated with $X_t$, clearly $h'(Y_{t+1} - \hat{Y}_{t+1})$ is also uncorrelated with $X_t$.*
[^Teorema1]: *Teorema 1: A matriz de coeficientes de proje√ß√£o Œ±' minimiza o MSE no sentido de que para qualquer outra matriz B (n x m), $MSE(\alpha'X_t) \leq MSE(B'X_t)$, onde a desigualdade √© no sentido de matrizes semidefinidas positivas.*
[^Lema2.1]: *Lema 2.1: A condi√ß√£o de n√£o correla√ß√£o $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$ implica que os res√≠duos da proje√ß√£o linear, $Y_{t+1} - \alpha'X_t$, s√£o ortogonais ao espa√ßo gerado pelas vari√°veis preditoras $X_t$.*
<!-- END -->
