## PrevisÃ£o de Vetores: A Matriz de Coeficientes de ProjeÃ§Ã£o e a CondiÃ§Ã£o de NÃ£o CorrelaÃ§Ã£o Multivariada

### IntroduÃ§Ã£o
Este capÃ­tulo continua a exploraÃ§Ã£o da previsÃ£o de vetores, focando na interpretaÃ§Ã£o e nas implicaÃ§Ãµes da matriz de coeficientes de projeÃ§Ã£o, $\alpha'$, e na condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o multivariada, $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$ [^2]. Como estabelecido em capÃ­tulos anteriores, essa condiÃ§Ã£o Ã© fundamental para garantir a otimalidade da projeÃ§Ã£o linear no contexto multivariado. A matriz $\alpha'$ estabelece a relaÃ§Ã£o linear entre o vetor de variÃ¡veis a ser previsto, $Y_{t+1}$, e o vetor de variÃ¡veis preditoras, $X_t$ [^1]. Esta seÃ§Ã£o examina em detalhes o que essa condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o implica e como ela afeta as propriedades da projeÃ§Ã£o linear.

### Conceitos Fundamentais
A projeÃ§Ã£o linear de um vetor $Y_{t+1}$ sobre um vetor $X_t$ Ã© dada por $\hat{Y}_{t+1} = \alpha'X_t$, onde $\alpha'$ Ã© uma matriz de coeficientes de dimensÃ£o $n \times m$ [^1], onde $n$ Ã© a dimensÃ£o de $Y_{t+1}$ e $m$ Ã© a dimensÃ£o de $X_t$. A condiÃ§Ã£o crucial para que $\alpha'X_t$ seja considerada a projeÃ§Ã£o linear de $Y_{t+1}$ em $X_t$ Ã© que o erro de previsÃ£o, dado por $Y_{t+1} - \hat{Y}_{t+1} = Y_{t+1} - \alpha'X_t$, seja nÃ£o correlacionado com $X_t$, como expresso por:
$$
E[(Y_{t+1} - \alpha'X_t)X_t'] = 0. \quad [4.1.22]
$$
Esta expressÃ£o matricial implica um conjunto de $n \times m$ condiÃ§Ãµes de nÃ£o correlaÃ§Ã£o: cada elemento do vetor de erro $Y_{t+1} - \alpha'X_t$ deve ser nÃ£o correlacionado com cada elemento do vetor $X_t$.
> ðŸ’¡ **Exemplo NumÃ©rico:** Considere um cenÃ¡rio onde queremos prever duas variÃ¡veis, $Y_{t+1} = \begin{bmatrix} y_{1,t+1} \\ y_{2,t+1} \end{bmatrix}$, utilizando duas variÃ¡veis preditoras, $X_t = \begin{bmatrix} x_{1,t} \\ x_{2,t} \end{bmatrix}$. A condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o, $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$, neste caso, se expande para quatro condiÃ§Ãµes de nÃ£o correlaÃ§Ã£o:
>
>  $E[(y_{1,t+1} - (\alpha_{11}x_{1,t} + \alpha_{12}x_{2,t}))x_{1,t}] = 0$
>
>  $E[(y_{1,t+1} - (\alpha_{11}x_{1,t} + \alpha_{12}x_{2,t}))x_{2,t}] = 0$
>
>  $E[(y_{2,t+1} - (\alpha_{21}x_{1,t} + \alpha_{22}x_{2,t}))x_{1,t}] = 0$
>
>  $E[(y_{2,t+1} - (\alpha_{21}x_{1,t} + \alpha_{22}x_{2,t}))x_{2,t}] = 0$.
>
> Cada uma dessas condiÃ§Ãµes afirma que o erro de previsÃ£o de cada componente de $Y_{t+1}$ Ã© nÃ£o correlacionado com cada componente de $X_t$.
>
> Suponha que tenhamos os seguintes valores amostrais para as variÃ¡veis:
>
> $Y_{t+1} = \begin{bmatrix} 5 \\ 8 \end{bmatrix}$, $X_t = \begin{bmatrix} 2 \\ 3 \end{bmatrix}$, e $\alpha' = \begin{bmatrix} 0.5 & 1 \\ 1 & 2 \end{bmatrix}$
>
> EntÃ£o, a previsÃ£o seria $\hat{Y}_{t+1} = \begin{bmatrix} 0.5 & 1 \\ 1 & 2 \end{bmatrix} \begin{bmatrix} 2 \\ 3 \end{bmatrix} = \begin{bmatrix} 4 \\ 8 \end{bmatrix}$.
>
> O erro de previsÃ£o Ã© $e_{t+1} = Y_{t+1} - \hat{Y}_{t+1} = \begin{bmatrix} 5 \\ 8 \end{bmatrix} - \begin{bmatrix} 4 \\ 8 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$.
>
> As condiÃ§Ãµes de nÃ£o correlaÃ§Ã£o seriam verificadas no nÃ­vel populacional. No nÃ­vel amostral, podemos calcular o produto do erro com o preditor.  A condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o Ã© que a esperanÃ§a (mÃ©dia amostral) desses produtos seja zero. Note que a matriz $\alpha'$ Ã© obtida atravÃ©s dos momentos amostrais, de forma que estas condiÃ§Ãµes seriam satisfeitas (aproximadamente) na amostra usada para construir $\alpha'$.

A matriz $\alpha'$ Ã© determinada a partir dos momentos populacionais dos vetores $Y_{t+1}$ e $X_t$ atravÃ©s da seguinte fÃ³rmula [4.1.23]:
$$
\alpha' = [E(Y_{t+1}X_t')] [E(X_tX_t')]^{-1}, \quad [4.1.23]
$$
Essa fÃ³rmula generaliza a expressÃ£o escalar e destaca a importÃ¢ncia dos momentos populacionais na determinaÃ§Ã£o da melhor relaÃ§Ã£o linear entre os vetores. A condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o Ã© crucial para a derivaÃ§Ã£o da fÃ³rmula para $\alpha'$, garantindo que $\alpha'X_t$ fornece o melhor preditor linear de $Y_{t+1}$ no sentido de minimizar o erro quadrÃ¡tico mÃ©dio (MSE) [^Teorema1].

**Lema 2.1** *A condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$ implica que os resÃ­duos da projeÃ§Ã£o linear, $Y_{t+1} - \alpha'X_t$, sÃ£o ortogonais ao espaÃ§o gerado pelas variÃ¡veis preditoras $X_t$.*

*Prova:*
I. A condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o Ã© $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$.
II. Expandindo esta expressÃ£o, temos que $E[Y_{t+1}X_t'] - \alpha'E[X_tX_t'] = 0$.
III. PrÃ©-multiplicando a expressÃ£o acima pela inversa de $E[X_tX_t']$:
$$
\alpha' = E[Y_{t+1}X_t'] E[X_tX_t']^{-1}
$$
IV. Substituindo a equaÃ§Ã£o de $\alpha'$ na condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o:
$$
E[Y_{t+1}X_t'] - E[Y_{t+1}X_t']E[X_tX_t']^{-1} E[X_tX_t'] = E[Y_{t+1}X_t'] - E[Y_{t+1}X_t'] = 0
$$
V. Isso implica que o termo de erro $Y_{t+1} - \alpha'X_t$ Ã© ortogonal Ã s variÃ¡veis preditoras $X_t$ no sentido de que a esperanÃ§a do produto entre eles Ã© zero.  â– 

> ðŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar a ortogonalidade, suponha que temos a projeÃ§Ã£o linear $\hat{Y}_{t+1} = \begin{bmatrix} 0.5 & 0.2 \\ 0.3 & 0.7 \end{bmatrix}X_t$, onde $X_t = \begin{bmatrix} x_{1,t} \\ x_{2,t} \end{bmatrix}$. A condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o implica que o erro de previsÃ£o $e_{t+1} = Y_{t+1} - \hat{Y}_{t+1}$ seja ortogonal a $X_t$. Isso significa que $E[e_{1,t+1}x_{1,t}] = 0$, $E[e_{1,t+1}x_{2,t}] = 0$, $E[e_{2,t+1}x_{1,t}] = 0$, e $E[e_{2,t+1}x_{2,t}] = 0$, onde $e_{1,t+1}$ e $e_{2,t+1}$ sÃ£o os componentes do vetor de erro $e_{t+1}$. Isso implica que os resÃ­duos nÃ£o fornecem nenhuma informaÃ§Ã£o que possa ser extraÃ­da por uma projeÃ§Ã£o linear em $X_t$.
>
> Para uma amostra com valores especÃ­ficos:
> $Y_{t+1} = \begin{bmatrix} 3 \\ 5 \end{bmatrix}$, $X_t = \begin{bmatrix} 2 \\ 1 \end{bmatrix}$
> $\hat{Y}_{t+1} =  \begin{bmatrix} 0.5 & 0.2 \\ 0.3 & 0.7 \end{bmatrix} \begin{bmatrix} 2 \\ 1 \end{bmatrix} = \begin{bmatrix} 1.2 \\ 1.3 \end{bmatrix}$
>
> $e_{t+1} = Y_{t+1} - \hat{Y}_{t+1} = \begin{bmatrix} 3 \\ 5 \end{bmatrix} - \begin{bmatrix} 1.2 \\ 1.3 \end{bmatrix} = \begin{bmatrix} 1.8 \\ 3.7 \end{bmatrix}$
>
> Verificando a ortogonalidade (no nÃ­vel amostral):
> $e_{t+1}'X_t =  \begin{bmatrix} 1.8 & 3.7 \end{bmatrix} \begin{bmatrix} 2 \\ 1 \end{bmatrix} = (1.8 * 2) + (3.7*1) = 3.6 + 3.7 = 7.3 $
>
>  A condiÃ§Ã£o de ortogonalidade implica que $E[e_{t+1}'X_t] = 0$ no nÃ­vel populacional.  No nÃ­vel amostral, essa condiÃ§Ã£o serÃ¡ verificada aproximadamente, dependendo do tamanho da amostra e da variabilidade dos dados. Para que a condiÃ§Ã£o fosse exatamente satisfeita na amostra,  $\alpha'$ teria que ser estimado usando os dados da amostra. Se $\alpha'$ foi estimado na amostra, o valor seria  zero.

O MSE da projeÃ§Ã£o linear, no caso multivariado, Ã© dado por [4.1.24]:
$$
MSE(\alpha'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)'] = E(Y_{t+1}Y_{t+1}') - E(Y_{t+1}X_t')[E(X_tX_t')]^{-1}E(X_tY_{t+1}')
$$

A expressÃ£o do MSE representa a variÃ¢ncia e covariÃ¢ncia dos erros de previsÃ£o. Especificamente, os elementos diagonais representam a variÃ¢ncia do erro de previsÃ£o para cada componente de $Y_{t+1}$, e os elementos fora da diagonal representam as covariÃ¢ncias entre esses erros.

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que tenhamos os seguintes momentos populacionais:
> $E(Y_{t+1}Y_{t+1}') = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix}$, $E(Y_{t+1}X_t') = \begin{bmatrix} 6 & 3 \\ 5 & 8 \end{bmatrix}$, e $E(X_tX_t') = \begin{bmatrix} 4 & 1 \\ 1 & 5 \end{bmatrix}$.
>
> Primeiro, calculamos a inversa de $E(X_tX_t')$:
> $(E(X_tX_t'))^{-1} = \frac{1}{(4*5 - 1*1)} \begin{bmatrix} 5 & -1 \\ -1 & 4 \end{bmatrix} = \frac{1}{19} \begin{bmatrix} 5 & -1 \\ -1 & 4 \end{bmatrix} = \begin{bmatrix} 5/19 & -1/19 \\ -1/19 & 4/19 \end{bmatrix}$
>
> Em seguida, calculamos $\alpha'$:
> $\alpha' = E(Y_{t+1}X_t')[E(X_tX_t')]^{-1} = \begin{bmatrix} 6 & 3 \\ 5 & 8 \end{bmatrix} \begin{bmatrix} 5/19 & -1/19 \\ -1/19 & 4/19 \end{bmatrix} = \begin{bmatrix} 27/19 & 6/19 \\ -7/19 & 27/19 \end{bmatrix}  \approx \begin{bmatrix} 1.42 & 0.32 \\ -0.37 & 1.42 \end{bmatrix}$
>
> Agora, calculamos o MSE:
> $MSE(\alpha'X_t) = E(Y_{t+1}Y_{t+1}') - E(Y_{t+1}X_t')[E(X_tX_t')]^{-1}E(X_tY_{t+1}')$
> $MSE(\alpha'X_t) = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 6 & 3 \\ 5 & 8 \end{bmatrix} \begin{bmatrix} 5/19 & -1/19 \\ -1/19 & 4/19 \end{bmatrix} \begin{bmatrix} 6 & 5 \\ 3 & 8 \end{bmatrix} =  \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 27/19 & 6/19 \\ -7/19 & 27/19 \end{bmatrix} \begin{bmatrix} 6 & 5 \\ 3 & 8 \end{bmatrix} = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} -  \begin{bmatrix} 17.10 & 13.52 \\ 2.16 & 10.26 \end{bmatrix}  =  \begin{bmatrix} -7.10 & -9.52 \\ 1.84 & 9.74 \end{bmatrix}$.
>
>  Notamos que o resultado acima estÃ¡ incorreto, pois a matriz resultante deveria ser positiva semidefinida. O erro estÃ¡ nos cÃ¡lculos acima:
>
> $MSE(\alpha'X_t) = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 6 & 3 \\ 5 & 8 \end{bmatrix} \begin{bmatrix} 5/19 & -1/19 \\ -1/19 & 4/19 \end{bmatrix} \begin{bmatrix} 6 & 5 \\ 3 & 8 \end{bmatrix} =  \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 1.42 & 0.32 \\ -0.37 & 1.42 \end{bmatrix} \begin{bmatrix} 6 & 5 \\ 3 & 8 \end{bmatrix} = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 9.50 & 9.46 \\ 1.94 & 9.99 \end{bmatrix} = \begin{bmatrix} 0.50 & -5.46 \\ 2.06 & 10.01 \end{bmatrix} $.
>
>  O cÃ¡lculo correto Ã©:
>
> $MSE(\alpha'X_t) = E(Y_{t+1}Y_{t+1}') - E(Y_{t+1}X_t')[E(X_tX_t')]^{-1}E(X_tY_{t+1}')$
>
> $MSE(\alpha'X_t) = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 6 & 3 \\ 5 & 8 \end{bmatrix} \begin{bmatrix} 5/19 & -1/19 \\ -1/19 & 4/19 \end{bmatrix} \begin{bmatrix} 6 & 5 \\ 3 & 8 \end{bmatrix} = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 6 & 3 \\ 5 & 8 \end{bmatrix} \begin{bmatrix} 27/19 & 31/19 \\ -7/19 & 27/19 \end{bmatrix} = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 6*27/19 -3*7/19  &  6*31/19 + 3*27/19 \\ 5*27/19 - 8*7/19 &  5*31/19 + 8*27/19 \end{bmatrix} = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 153/19 & 267/19 \\ 79/19 & 371/19 \end{bmatrix} = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 8.05 & 14.05 \\ 4.16 & 19.52 \end{bmatrix} = \begin{bmatrix} 1.95 & -10.05 \\ -0.16 & 0.48 \end{bmatrix}$.
>
> O cÃ¡lculo correto Ã©:
> $MSE(\alpha'X_t) = E(Y_{t+1}Y_{t+1}') - E(\hat{Y}_{t+1} \hat{Y}_{t+1}')$
> $E(\hat{Y}_{t+1} \hat{Y}_{t+1}') = E( \alpha' X_t X_t' \alpha) = \alpha'E(X_t X_t') \alpha =   \begin{bmatrix} 27/19 & 6/19 \\ -7/19 & 27/19 \end{bmatrix} \begin{bmatrix} 4 & 1 \\ 1 & 5 \end{bmatrix} \begin{bmatrix} 27/19 & -7/19 \\ 6/19 & 27/19 \end{bmatrix} = \begin{bmatrix} 114/19 & 57/19 \\ -11/19 & 134/19 \end{bmatrix} \begin{bmatrix} 27/19 & -7/19 \\ 6/19 & 27/19 \end{bmatrix} = \begin{bmatrix} 3078/361 + 342/361 & -798/361+1539/361 \\ -297/361+804/361 & 77/361+3618/361 \end{bmatrix} = \begin{bmatrix} 3420/361 & 741/361 \\ 507/361 & 3695/361 \end{bmatrix} = \begin{bmatrix}  9.47 & 2.05 \\ 1.40 & 10.23 \end{bmatrix}  $
> $MSE = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix}  9.47 & 2.05 \\ 1.40 & 10.23 \end{bmatrix}  = \begin{bmatrix} 0.53 & 1.95 \\ 2.6 & 9.77 \end{bmatrix}$
>
> Os elementos diagonais (0.53 e 9.77) representam as variÃ¢ncias do erro de previsÃ£o para a primeira e segunda variÃ¡veis, respectivamente. Os elementos fora da diagonal representam a covariÃ¢ncia entre os erros de previsÃ£o. Note que o MSE representa o erro de projeÃ§Ã£o no nÃ­vel populacional.

A matriz $\alpha'$, como demonstrado anteriormente, Ã© tal que minimiza o MSE [^Teorema1] no sentido de que qualquer outra matriz de coeficientes $B'$ resultarÃ¡ em um MSE maior ou igual.

**Lema 2.2** *A condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$ Ã© equivalente a afirmar que a projeÃ§Ã£o linear $\hat{Y}_{t+1}$ Ã© o preditor linear de $Y_{t+1}$ mais prÃ³ximo de $Y_{t+1}$ no sentido do MSE.*
*Prova:*
I. Seja $B'X_t$ um preditor linear qualquer de $Y_{t+1}$.
II. Podemos escrever o erro de previsÃ£o de $B'X_t$ como $Y_{t+1} - B'X_t$.
III. O MSE do preditor $B'X_t$ Ã© dado por $E[(Y_{t+1} - B'X_t)(Y_{t+1} - B'X_t)']$.
IV. Para provar que $\alpha'X_t$ minimiza o MSE, podemos mostrar que $MSE(B'X_t) \ge MSE(\alpha'X_t)$.
V. Usando o resultado anterior $MSE(B'X_t) = E[(Y_{t+1} - \alpha'X_t + (\alpha' - B')X_t)(Y_{t+1} - \alpha'X_t + (\alpha' - B')X_t)']$.
VI. Expandindo: $MSE(B'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)'] + E[(\alpha'-B')X_t(Y_{t+1} - \alpha'X_t)'] + E[(Y_{t+1} - \alpha'X_t)X_t'(\alpha-B)] + E[(\alpha'-B')X_tX_t'(\alpha-B)]$.
VII. Pela condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o, sabemos que $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$, entÃ£o os termos mistos sÃ£o iguais a zero.
VIII. Portanto $MSE(B'X_t) = MSE(\alpha'X_t) + E[(\alpha'-B')X_tX_t'(\alpha-B)]$.
IX. Como $E[(\alpha'-B')X_tX_t'(\alpha-B)]$ Ã© sempre uma matriz semidefinida positiva, temos que $MSE(B'X_t) \ge MSE(\alpha'X_t)$, o que comprova que $\hat{Y}_{t+1}$ Ã© o preditor mais prÃ³ximo de $Y_{t+1}$ no sentido do MSE. â– 
> ðŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar o Lema 2.2, vamos considerar que $\alpha'$ Ã© a matriz de coeficientes Ã³tima que jÃ¡ calculamos no exemplo anterior e vamos definir uma matriz $B'$ que difere de $\alpha'$.
>
>  Seja $B' = \begin{bmatrix} 1.5 & 0.4 \\ -0.2 & 1.3 \end{bmatrix}$.
>
>  Usando os mesmos momentos populacionais do exemplo anterior, e sabendo que $MSE(\alpha'X_t) = \begin{bmatrix} 0.53 & 1.95 \\ 2.6 & 9.77 \end{bmatrix}$, podemos calcular o $MSE(B'X_t)$.
>
> $MSE(B'X_t) = E(Y_{t+1}Y_{t+1}') - E(B'X_tX_t'B) = E(Y_{t+1}Y_{t+1}') - B'E(X_tX_t')B$
> $E(B'X_tX_t'B) = \begin{bmatrix} 1.5 & 0.4 \\ -0.2 & 1.3 \end{bmatrix} \begin{bmatrix} 4 & 1 \\ 1 & 5 \end{bmatrix} \begin{bmatrix} 1.5 & -0.2 \\ 0.4 & 1.3 \end{bmatrix} = \begin{bmatrix} 6.4 & 3.5 \\ 0.5 & 6.3 \end{bmatrix} \begin{bmatrix} 1.5 & -0.2 \\ 0.4 & 1.3 \end{bmatrix} = \begin{bmatrix} 11 & 3.27 \\ 3.27 & 8.09 \end{bmatrix}$
>
> $MSE(B'X_t) = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 11 & 3.27 \\ 3.27 & 8.09 \end{bmatrix}  = \begin{bmatrix} -1 & 0.73 \\ 0.73 & 11.91 \end{bmatrix}$.
>
> O cÃ¡lculo correto Ã©:
> $MSE(B'X_t) = E[(Y_{t+1} - B'X_t)(Y_{t+1} - B'X_t)'] = E(Y_{t+1}Y_{t+1}') - E(Y_{t+1}X_t')B - B'E(X_tY_{t+1}') + B'E(X_tX_t')B$
>
> $MSE(B'X_t) = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 6 & 3 \\ 5 & 8 \end{bmatrix}  \begin{bmatrix} 1.5 & -0.2 \\ 0.4 & 1.3 \end{bmatrix} - \begin{bmatrix} 1.5 & 0.4 \\ -0.2 & 1.3 \end{bmatrix} \begin{bmatrix} 6 & 5 \\ 3 & 8 \end{bmatrix}  + \begin{bmatrix} 1.5 & 0.4 \\ -0.2 & 1.3 \end{bmatrix} \begin{bmatrix} 4 & 1 \\ 1 & 5 \end{bmatrix}  \begin{bmatrix} 1.5 & -0.2 \\ 0.4 & 1.3 \end{bmatrix}$
>
> $MSE(B'X_t) = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 10.2 & 2.7 \\ 6.2 & 9.9 \end{bmatrix} - \begin{bmatrix} 10.2 & 6.5 \\ 2.7 & 9.4 \end{bmatrix} + \begin{bmatrix} 11 & 3.27 \\ 3.27 & 8.09 \end{bmatrix}  = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 10.2+10.2-11 & 2.7+6.5-3.27 \\ 6.2+2.7-3.27 & 9.9+9.4-8.09 \end{bmatrix} = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 9.4 & 6 \\ 5.63 & 11.21 \end{bmatrix} =  \begin{bmatrix} 0.6 & -2 \\ -1.63 & 8.79 \end{bmatrix} $
>
>  Note que a matriz acima tambÃ©m nÃ£o parece positiva semidefinida, indicando que um erro no cÃ¡lculo.
>
> $MSE(B'X_t) = MSE(\alpha'X_t) + E[(\alpha'-B')X_tX_t'(\alpha-B)]$
>
> $\alpha' - B' = \begin{bmatrix} 1.42 & 0.32 \\ -0.37 & 1.42 \end{bmatrix} - \begin{bmatrix} 1.5 & 0.4 \\ -0.2 & 1.3 \end{bmatrix} = \begin{bmatrix} -0.08 & -0.08 \\ -0.17 & 0.12 \end{bmatrix}$
>
> $E[(\alpha'-B')X_tX_t'(\alpha-B)] = \begin{bmatrix} -0.08 & -0.08 \\ -0.17 & 0.12 \end{bmatrix} \begin{bmatrix} 4 & 1 \\ 1 & 5 \end{bmatrix}  \begin{bmatrix} -0.08 & -0.17 \\ -0.08 & 0.12 \end{bmatrix} = \begin{bmatrix} -0.4 & -0.48 \\ -0.56 & 0.43 \end{bmatrix}  \begin{bmatrix} -0.08 & -0.17 \\ -0.08 & 0.12 \end{bmatrix}  = \begin{bmatrix} 0.0672 & -0.0176 \\ -0.0056 & 0.14 \end{bmatrix} $
>
> $MSE(B'X_t) =  \begin{bmatrix} 0.53 & 1.95 \\ 2.6 & 9.77 \end{bmatrix} + \begin{bmatrix} 0.0672 & -0.0176 \\ -0.0056 & 0.14 \end{bmatrix} = \begin{bmatrix} 0.5972 & 1.9324 \\ 2.5944 & 9.91 \end{bmatrix}$
>
>  Note que o  $MSE(B'X_t)$ Ã© maior (no sentido de matrizes semidefinidas positivas) que $MSE(\alpha'X_t)$.
>
> A ideia principal Ã© que qualquer outro preditor linear $B'X_t$ terÃ¡ um MSE maior ou igual ao preditor $\alpha'X_t$, demonstrando a otimalidade da projeÃ§Ã£o linear sob a condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o.

A condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o Ã© fundamental nÃ£o apenas para garantir a otimalidade da projeÃ§Ã£o linear, mas tambÃ©m para facilitar a interpretaÃ§Ã£o dos resultados. Ao assegurar que o erro de previsÃ£o seja ortogonal Ã s variÃ¡veis preditoras, podemos afirmar que o modelo linear capturou toda a informaÃ§Ã£o relevante de $X_t$ para prever $Y_{t+1}$, e que nenhum padrÃ£o restante no erro pode ser explicado por uma funÃ§Ã£o linear de $X_t$.

**Lema 2.3** *Se a condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$ Ã© satisfeita, entÃ£o a matriz de covariÃ¢ncia do erro de previsÃ£o, $E[(Y_{t+1} - \hat{Y}_{t+1})(Y_{t+1} - \hat{Y}_{t+1})']$, Ã© igual Ã  matriz de covariÃ¢ncia de $Y_{t+1}$ menos a matriz de covariÃ¢ncia da projeÃ§Ã£o linear $\hat{Y}_{t+1}$, ou seja, $E[(Y_{t+1} - \hat{Y}_{t+1})(Y_{t+1} - \hat{Y}_{t+1})'] = E(Y_{t+1}Y_{t+1}') - E(\hat{Y}_{t+1}\hat{Y}_{t+1}')$.*

*Prova:*
I. ComeÃ§amos com a definiÃ§Ã£o do erro de previsÃ£o: $e_{t+1} = Y_{t+1} - \hat{Y}_{t+1} = Y_{t+1} - \alpha'X_t$.
II. Queremos calcular a matriz de covariÃ¢ncia do erro, $E[e_{t+1}e_{t+1}'] = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)']$.
III. Expandindo a expressÃ£o, temos: $E[e_{t+1}e_{t+1}'] = E[Y_{t+1}Y_{t+1}'] - E[Y_{t+1}X_t']\alpha - \alpha'E[X_tY_{t+1}'] + \alpha'E[X_tX_t']\alpha$.
IV. Substituindo a expressÃ£o de $\alpha'$ em $E[Y_{t+1}X_t']\alpha + \alpha'E[X_tY_{t+1}']$, temos: $E[Y_{t+1}X_t'](E[X_tX_t']^{-1}E[X_tY_{t+1}'])' + E[Y_{t+1}X_t'](E[X_tX_t']^{-1}E[X_tY_{t+1}']) = 2E[Y_{t+1}X_t']E[X_tX_t']^{-1}E[X_tY_{t+1}'] $.
V. Simplificando, obtemos $2E[Y_{t+1}X_t']E[X_tX_t']^{-1}E[X_tY_{t+1}'] = 2E[\hat{Y}_{t+1}\hat{Y}_{t+1}']$.
VI. Substituindo em III e notando que $\alpha'E[X_tX_t']\alpha= E[\hat{Y}_{t+1} \hat{Y}_{t+1}']$, obtemos:  $E[e_{t+1}e_{t+1}'] = E[Y_{t+1}Y_{t+1}'] - 2E[\hat{Y}_{t+1}\hat{Y}_{t+1}'] + E[\hat{Y}_{t+1}\hat{Y}_{t+1}'] = E[Y_{t+1}Y_{t+1}'] - E[\hat{Y}_{t+1}\hat{Y}_{t+1}']$.
VII. Finalmente, lembrando que  $E[Y_{t+1}Y_{t+1}'] = E[Y_{t+1}^2]$  e  $E[\hat{Y}_{t+1}\hat{Y}_{t+1}'] = E[\hat{Y}_{t+1}^2]$, obtemos:
$E[e_{t+1}e_{t+1}'] = E[Y_{t+1}^2] - E[\hat{Y}_{t+1}^2]$.

**ConclusÃ£o**

A variÃ¢ncia do erro de previsÃ£o Ã© igual Ã  diferenÃ§a entre a variÃ¢ncia da variÃ¡vel dependente e a variÃ¢ncia da previsÃ£o. Isso indica que, quanto maior a variÃ¢ncia da previsÃ£o, menor serÃ¡ a variÃ¢ncia do erro.

**InterpretaÃ§Ã£o geomÃ©trica**

Em um espaÃ§o vetorial, podemos interpretar a previsÃ£o $\hat{Y}_{t+1}$ como a projeÃ§Ã£o ortogonal de $Y_{t+1}$ sobre o subespaÃ§o gerado pelas variÃ¡veis explicativas $X_t$. O erro de previsÃ£o $e_{t+1}$ Ã© o vetor que liga $Y_{t+1}$ Ã  sua projeÃ§Ã£o $\hat{Y}_{t+1}$. A decomposiÃ§Ã£o da variÃ¢ncia do erro que derivamos acima reflete o Teorema de PitÃ¡goras nesse espaÃ§o.

**IlustraÃ§Ã£o com um exemplo simples**

Consideremos o caso de uma regressÃ£o linear simples, onde $Y_{t+1} = \alpha X_t + e_{t+1}$.

1.  A previsÃ£o Ã© dada por $\hat{Y}_{t+1} = \hat{\alpha} X_t$, onde $\hat{\alpha}$ Ã© o estimador de mÃ­nimos quadrados de $\alpha$.
2.  O erro Ã©  $e_{t+1} = Y_{t+1} - \hat{Y}_{t+1}$.
3.  Podemos mostrar que $E[e_{t+1}X_t'] = 0$, o que significa que o erro Ã© ortogonal Ã s variÃ¡veis explicativas.
4.  A variÃ¢ncia de $Y_{t+1}$ Ã©  $Var[Y_{t+1}] = \alpha^2 Var[X_t] + Var[e_{t+1}]$.
5.  A variÃ¢ncia da previsÃ£o Ã©  $Var[\hat{Y}_{t+1}] = \hat{\alpha}^2 Var[X_t]$.
6.  A variÃ¢ncia do erro de previsÃ£o Ã© $Var[e_{t+1}] = Var[Y_{t+1}] - Var[\hat{Y}_{t+1}]$.

Este exemplo simples ilustra a validade da decomposiÃ§Ã£o da variÃ¢ncia do erro, mesmo em um caso menos geral.

<!-- END -->
