## Previs√£o de Vetores: A Matriz de Coeficientes de Proje√ß√£o e a Condi√ß√£o de N√£o Correla√ß√£o Multivariada

### Introdu√ß√£o
Este cap√≠tulo continua a explora√ß√£o da previs√£o de vetores, focando na interpreta√ß√£o e nas implica√ß√µes da matriz de coeficientes de proje√ß√£o, $\alpha'$, e na condi√ß√£o de n√£o correla√ß√£o multivariada, $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$ [^2]. Como estabelecido em cap√≠tulos anteriores, essa condi√ß√£o √© fundamental para garantir a otimalidade da proje√ß√£o linear no contexto multivariado. A matriz $\alpha'$ estabelece a rela√ß√£o linear entre o vetor de vari√°veis a ser previsto, $Y_{t+1}$, e o vetor de vari√°veis preditoras, $X_t$ [^1]. Esta se√ß√£o examina em detalhes o que essa condi√ß√£o de n√£o correla√ß√£o implica e como ela afeta as propriedades da proje√ß√£o linear.

### Conceitos Fundamentais
A proje√ß√£o linear de um vetor $Y_{t+1}$ sobre um vetor $X_t$ √© dada por $\hat{Y}_{t+1} = \alpha'X_t$, onde $\alpha'$ √© uma matriz de coeficientes de dimens√£o $n \times m$ [^1], onde $n$ √© a dimens√£o de $Y_{t+1}$ e $m$ √© a dimens√£o de $X_t$. A condi√ß√£o crucial para que $\alpha'X_t$ seja considerada a proje√ß√£o linear de $Y_{t+1}$ em $X_t$ √© que o erro de previs√£o, dado por $Y_{t+1} - \hat{Y}_{t+1} = Y_{t+1} - \alpha'X_t$, seja n√£o correlacionado com $X_t$, como expresso por:
$$
E[(Y_{t+1} - \alpha'X_t)X_t'] = 0. \quad [4.1.22]
$$
Esta express√£o matricial implica um conjunto de $n \times m$ condi√ß√µes de n√£o correla√ß√£o: cada elemento do vetor de erro $Y_{t+1} - \alpha'X_t$ deve ser n√£o correlacionado com cada elemento do vetor $X_t$.
> üí° **Exemplo Num√©rico:** Considere um cen√°rio onde queremos prever duas vari√°veis, $Y_{t+1} = \begin{bmatrix} y_{1,t+1} \\ y_{2,t+1} \end{bmatrix}$, utilizando duas vari√°veis preditoras, $X_t = \begin{bmatrix} x_{1,t} \\ x_{2,t} \end{bmatrix}$. A condi√ß√£o de n√£o correla√ß√£o, $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$, neste caso, se expande para quatro condi√ß√µes de n√£o correla√ß√£o:
>
>  $E[(y_{1,t+1} - (\alpha_{11}x_{1,t} + \alpha_{12}x_{2,t}))x_{1,t}] = 0$
>
>  $E[(y_{1,t+1} - (\alpha_{11}x_{1,t} + \alpha_{12}x_{2,t}))x_{2,t}] = 0$
>
>  $E[(y_{2,t+1} - (\alpha_{21}x_{1,t} + \alpha_{22}x_{2,t}))x_{1,t}] = 0$
>
>  $E[(y_{2,t+1} - (\alpha_{21}x_{1,t} + \alpha_{22}x_{2,t}))x_{2,t}] = 0$.
>
> Cada uma dessas condi√ß√µes afirma que o erro de previs√£o de cada componente de $Y_{t+1}$ √© n√£o correlacionado com cada componente de $X_t$.
>
> Suponha que tenhamos os seguintes valores amostrais para as vari√°veis:
>
> $Y_{t+1} = \begin{bmatrix} 5 \\ 8 \end{bmatrix}$, $X_t = \begin{bmatrix} 2 \\ 3 \end{bmatrix}$, e $\alpha' = \begin{bmatrix} 0.5 & 1 \\ 1 & 2 \end{bmatrix}$
>
> Ent√£o, a previs√£o seria $\hat{Y}_{t+1} = \begin{bmatrix} 0.5 & 1 \\ 1 & 2 \end{bmatrix} \begin{bmatrix} 2 \\ 3 \end{bmatrix} = \begin{bmatrix} 4 \\ 8 \end{bmatrix}$.
>
> O erro de previs√£o √© $e_{t+1} = Y_{t+1} - \hat{Y}_{t+1} = \begin{bmatrix} 5 \\ 8 \end{bmatrix} - \begin{bmatrix} 4 \\ 8 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$.
>
> As condi√ß√µes de n√£o correla√ß√£o seriam verificadas no n√≠vel populacional. No n√≠vel amostral, podemos calcular o produto do erro com o preditor.  A condi√ß√£o de n√£o correla√ß√£o √© que a esperan√ßa (m√©dia amostral) desses produtos seja zero. Note que a matriz $\alpha'$ √© obtida atrav√©s dos momentos amostrais, de forma que estas condi√ß√µes seriam satisfeitas (aproximadamente) na amostra usada para construir $\alpha'$.

A matriz $\alpha'$ √© determinada a partir dos momentos populacionais dos vetores $Y_{t+1}$ e $X_t$ atrav√©s da seguinte f√≥rmula [4.1.23]:
$$
\alpha' = [E(Y_{t+1}X_t')] [E(X_tX_t')]^{-1}, \quad [4.1.23]
$$
Essa f√≥rmula generaliza a express√£o escalar e destaca a import√¢ncia dos momentos populacionais na determina√ß√£o da melhor rela√ß√£o linear entre os vetores. A condi√ß√£o de n√£o correla√ß√£o √© crucial para a deriva√ß√£o da f√≥rmula para $\alpha'$, garantindo que $\alpha'X_t$ fornece o melhor preditor linear de $Y_{t+1}$ no sentido de minimizar o erro quadr√°tico m√©dio (MSE) [^Teorema1].

**Lema 2.1** *A condi√ß√£o de n√£o correla√ß√£o $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$ implica que os res√≠duos da proje√ß√£o linear, $Y_{t+1} - \alpha'X_t$, s√£o ortogonais ao espa√ßo gerado pelas vari√°veis preditoras $X_t$.*

*Prova:*
I. A condi√ß√£o de n√£o correla√ß√£o √© $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$.
II. Expandindo esta express√£o, temos que $E[Y_{t+1}X_t'] - \alpha'E[X_tX_t'] = 0$.
III. Pr√©-multiplicando a express√£o acima pela inversa de $E[X_tX_t']$:
$$
\alpha' = E[Y_{t+1}X_t'] E[X_tX_t']^{-1}
$$
IV. Substituindo a equa√ß√£o de $\alpha'$ na condi√ß√£o de n√£o correla√ß√£o:
$$
E[Y_{t+1}X_t'] - E[Y_{t+1}X_t']E[X_tX_t']^{-1} E[X_tX_t'] = E[Y_{t+1}X_t'] - E[Y_{t+1}X_t'] = 0
$$
V. Isso implica que o termo de erro $Y_{t+1} - \alpha'X_t$ √© ortogonal √†s vari√°veis preditoras $X_t$ no sentido de que a esperan√ßa do produto entre eles √© zero.  ‚ñ†

> üí° **Exemplo Num√©rico:** Para ilustrar a ortogonalidade, suponha que temos a proje√ß√£o linear $\hat{Y}_{t+1} = \begin{bmatrix} 0.5 & 0.2 \\ 0.3 & 0.7 \end{bmatrix}X_t$, onde $X_t = \begin{bmatrix} x_{1,t} \\ x_{2,t} \end{bmatrix}$. A condi√ß√£o de n√£o correla√ß√£o implica que o erro de previs√£o $e_{t+1} = Y_{t+1} - \hat{Y}_{t+1}$ seja ortogonal a $X_t$. Isso significa que $E[e_{1,t+1}x_{1,t}] = 0$, $E[e_{1,t+1}x_{2,t}] = 0$, $E[e_{2,t+1}x_{1,t}] = 0$, e $E[e_{2,t+1}x_{2,t}] = 0$, onde $e_{1,t+1}$ e $e_{2,t+1}$ s√£o os componentes do vetor de erro $e_{t+1}$. Isso implica que os res√≠duos n√£o fornecem nenhuma informa√ß√£o que possa ser extra√≠da por uma proje√ß√£o linear em $X_t$.
>
> Para uma amostra com valores espec√≠ficos:
> $Y_{t+1} = \begin{bmatrix} 3 \\ 5 \end{bmatrix}$, $X_t = \begin{bmatrix} 2 \\ 1 \end{bmatrix}$
> $\hat{Y}_{t+1} =  \begin{bmatrix} 0.5 & 0.2 \\ 0.3 & 0.7 \end{bmatrix} \begin{bmatrix} 2 \\ 1 \end{bmatrix} = \begin{bmatrix} 1.2 \\ 1.3 \end{bmatrix}$
>
> $e_{t+1} = Y_{t+1} - \hat{Y}_{t+1} = \begin{bmatrix} 3 \\ 5 \end{bmatrix} - \begin{bmatrix} 1.2 \\ 1.3 \end{bmatrix} = \begin{bmatrix} 1.8 \\ 3.7 \end{bmatrix}$
>
> Verificando a ortogonalidade (no n√≠vel amostral):
> $e_{t+1}'X_t =  \begin{bmatrix} 1.8 & 3.7 \end{bmatrix} \begin{bmatrix} 2 \\ 1 \end{bmatrix} = (1.8 * 2) + (3.7*1) = 3.6 + 3.7 = 7.3 $
>
>  A condi√ß√£o de ortogonalidade implica que $E[e_{t+1}'X_t] = 0$ no n√≠vel populacional.  No n√≠vel amostral, essa condi√ß√£o ser√° verificada aproximadamente, dependendo do tamanho da amostra e da variabilidade dos dados. Para que a condi√ß√£o fosse exatamente satisfeita na amostra,  $\alpha'$ teria que ser estimado usando os dados da amostra. Se $\alpha'$ foi estimado na amostra, o valor seria  zero.

O MSE da proje√ß√£o linear, no caso multivariado, √© dado por [4.1.24]:
$$
MSE(\alpha'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)'] = E(Y_{t+1}Y_{t+1}') - E(Y_{t+1}X_t')[E(X_tX_t')]^{-1}E(X_tY_{t+1}')
$$

A express√£o do MSE representa a vari√¢ncia e covari√¢ncia dos erros de previs√£o. Especificamente, os elementos diagonais representam a vari√¢ncia do erro de previs√£o para cada componente de $Y_{t+1}$, e os elementos fora da diagonal representam as covari√¢ncias entre esses erros.

> üí° **Exemplo Num√©rico:** Suponha que tenhamos os seguintes momentos populacionais:
> $E(Y_{t+1}Y_{t+1}') = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix}$, $E(Y_{t+1}X_t') = \begin{bmatrix} 6 & 3 \\ 5 & 8 \end{bmatrix}$, e $E(X_tX_t') = \begin{bmatrix} 4 & 1 \\ 1 & 5 \end{bmatrix}$.
>
> Primeiro, calculamos a inversa de $E(X_tX_t')$:
> $(E(X_tX_t'))^{-1} = \frac{1}{(4*5 - 1*1)} \begin{bmatrix} 5 & -1 \\ -1 & 4 \end{bmatrix} = \frac{1}{19} \begin{bmatrix} 5 & -1 \\ -1 & 4 \end{bmatrix} = \begin{bmatrix} 5/19 & -1/19 \\ -1/19 & 4/19 \end{bmatrix}$
>
> Em seguida, calculamos $\alpha'$:
> $\alpha' = E(Y_{t+1}X_t')[E(X_tX_t')]^{-1} = \begin{bmatrix} 6 & 3 \\ 5 & 8 \end{bmatrix} \begin{bmatrix} 5/19 & -1/19 \\ -1/19 & 4/19 \end{bmatrix} = \begin{bmatrix} 27/19 & 6/19 \\ -7/19 & 27/19 \end{bmatrix}  \approx \begin{bmatrix} 1.42 & 0.32 \\ -0.37 & 1.42 \end{bmatrix}$
>
> Agora, calculamos o MSE:
> $MSE(\alpha'X_t) = E(Y_{t+1}Y_{t+1}') - E(Y_{t+1}X_t')[E(X_tX_t')]^{-1}E(X_tY_{t+1}')$
> $MSE(\alpha'X_t) = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 6 & 3 \\ 5 & 8 \end{bmatrix} \begin{bmatrix} 5/19 & -1/19 \\ -1/19 & 4/19 \end{bmatrix} \begin{bmatrix} 6 & 5 \\ 3 & 8 \end{bmatrix} =  \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 27/19 & 6/19 \\ -7/19 & 27/19 \end{bmatrix} \begin{bmatrix} 6 & 5 \\ 3 & 8 \end{bmatrix} = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} -  \begin{bmatrix} 17.10 & 13.52 \\ 2.16 & 10.26 \end{bmatrix}  =  \begin{bmatrix} -7.10 & -9.52 \\ 1.84 & 9.74 \end{bmatrix}$.
>
>  Notamos que o resultado acima est√° incorreto, pois a matriz resultante deveria ser positiva semidefinida. O erro est√° nos c√°lculos acima:
>
> $MSE(\alpha'X_t) = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 6 & 3 \\ 5 & 8 \end{bmatrix} \begin{bmatrix} 5/19 & -1/19 \\ -1/19 & 4/19 \end{bmatrix} \begin{bmatrix} 6 & 5 \\ 3 & 8 \end{bmatrix} =  \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 1.42 & 0.32 \\ -0.37 & 1.42 \end{bmatrix} \begin{bmatrix} 6 & 5 \\ 3 & 8 \end{bmatrix} = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 9.50 & 9.46 \\ 1.94 & 9.99 \end{bmatrix} = \begin{bmatrix} 0.50 & -5.46 \\ 2.06 & 10.01 \end{bmatrix} $.
>
>  O c√°lculo correto √©:
>
> $MSE(\alpha'X_t) = E(Y_{t+1}Y_{t+1}') - E(Y_{t+1}X_t')[E(X_tX_t')]^{-1}E(X_tY_{t+1}')$
>
> $MSE(\alpha'X_t) = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 6 & 3 \\ 5 & 8 \end{bmatrix} \begin{bmatrix} 5/19 & -1/19 \\ -1/19 & 4/19 \end{bmatrix} \begin{bmatrix} 6 & 5 \\ 3 & 8 \end{bmatrix} = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 6 & 3 \\ 5 & 8 \end{bmatrix} \begin{bmatrix} 27/19 & 31/19 \\ -7/19 & 27/19 \end{bmatrix} = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 6*27/19 -3*7/19  &  6*31/19 + 3*27/19 \\ 5*27/19 - 8*7/19 &  5*31/19 + 8*27/19 \end{bmatrix} = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 153/19 & 267/19 \\ 79/19 & 371/19 \end{bmatrix} = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 8.05 & 14.05 \\ 4.16 & 19.52 \end{bmatrix} = \begin{bmatrix} 1.95 & -10.05 \\ -0.16 & 0.48 \end{bmatrix}$.
>
> O c√°lculo correto √©:
> $MSE(\alpha'X_t) = E(Y_{t+1}Y_{t+1}') - E(\hat{Y}_{t+1} \hat{Y}_{t+1}')$
> $E(\hat{Y}_{t+1} \hat{Y}_{t+1}') = E( \alpha' X_t X_t' \alpha) = \alpha'E(X_t X_t') \alpha =   \begin{bmatrix} 27/19 & 6/19 \\ -7/19 & 27/19 \end{bmatrix} \begin{bmatrix} 4 & 1 \\ 1 & 5 \end{bmatrix} \begin{bmatrix} 27/19 & -7/19 \\ 6/19 & 27/19 \end{bmatrix} = \begin{bmatrix} 114/19 & 57/19 \\ -11/19 & 134/19 \end{bmatrix} \begin{bmatrix} 27/19 & -7/19 \\ 6/19 & 27/19 \end{bmatrix} = \begin{bmatrix} 3078/361 + 342/361 & -798/361+1539/361 \\ -297/361+804/361 & 77/361+3618/361 \end{bmatrix} = \begin{bmatrix} 3420/361 & 741/361 \\ 507/361 & 3695/361 \end{bmatrix} = \begin{bmatrix}  9.47 & 2.05 \\ 1.40 & 10.23 \end{bmatrix}  $
> $MSE = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix}  9.47 & 2.05 \\ 1.40 & 10.23 \end{bmatrix}  = \begin{bmatrix} 0.53 & 1.95 \\ 2.6 & 9.77 \end{bmatrix}$
>
> Os elementos diagonais (0.53 e 9.77) representam as vari√¢ncias do erro de previs√£o para a primeira e segunda vari√°veis, respectivamente. Os elementos fora da diagonal representam a covari√¢ncia entre os erros de previs√£o. Note que o MSE representa o erro de proje√ß√£o no n√≠vel populacional.

A matriz $\alpha'$, como demonstrado anteriormente, √© tal que minimiza o MSE [^Teorema1] no sentido de que qualquer outra matriz de coeficientes $B'$ resultar√° em um MSE maior ou igual.

**Lema 2.2** *A condi√ß√£o de n√£o correla√ß√£o $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$ √© equivalente a afirmar que a proje√ß√£o linear $\hat{Y}_{t+1}$ √© o preditor linear de $Y_{t+1}$ mais pr√≥ximo de $Y_{t+1}$ no sentido do MSE.*
*Prova:*
I. Seja $B'X_t$ um preditor linear qualquer de $Y_{t+1}$.
II. Podemos escrever o erro de previs√£o de $B'X_t$ como $Y_{t+1} - B'X_t$.
III. O MSE do preditor $B'X_t$ √© dado por $E[(Y_{t+1} - B'X_t)(Y_{t+1} - B'X_t)']$.
IV. Para provar que $\alpha'X_t$ minimiza o MSE, podemos mostrar que $MSE(B'X_t) \ge MSE(\alpha'X_t)$.
V. Usando o resultado anterior $MSE(B'X_t) = E[(Y_{t+1} - \alpha'X_t + (\alpha' - B')X_t)(Y_{t+1} - \alpha'X_t + (\alpha' - B')X_t)']$.
VI. Expandindo: $MSE(B'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)'] + E[(\alpha'-B')X_t(Y_{t+1} - \alpha'X_t)'] + E[(Y_{t+1} - \alpha'X_t)X_t'(\alpha-B)] + E[(\alpha'-B')X_tX_t'(\alpha-B)]$.
VII. Pela condi√ß√£o de n√£o correla√ß√£o, sabemos que $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$, ent√£o os termos mistos s√£o iguais a zero.
VIII. Portanto $MSE(B'X_t) = MSE(\alpha'X_t) + E[(\alpha'-B')X_tX_t'(\alpha-B)]$.
IX. Como $E[(\alpha'-B')X_tX_t'(\alpha-B)]$ √© sempre uma matriz semidefinida positiva, temos que $MSE(B'X_t) \ge MSE(\alpha'X_t)$, o que comprova que $\hat{Y}_{t+1}$ √© o preditor mais pr√≥ximo de $Y_{t+1}$ no sentido do MSE. ‚ñ†
> üí° **Exemplo Num√©rico:** Para ilustrar o Lema 2.2, vamos considerar que $\alpha'$ √© a matriz de coeficientes √≥tima que j√° calculamos no exemplo anterior e vamos definir uma matriz $B'$ que difere de $\alpha'$.
>
>  Seja $B' = \begin{bmatrix} 1.5 & 0.4 \\ -0.2 & 1.3 \end{bmatrix}$.
>
>  Usando os mesmos momentos populacionais do exemplo anterior, e sabendo que $MSE(\alpha'X_t) = \begin{bmatrix} 0.53 & 1.95 \\ 2.6 & 9.77 \end{bmatrix}$, podemos calcular o $MSE(B'X_t)$.
>
> $MSE(B'X_t) = E(Y_{t+1}Y_{t+1}') - E(B'X_tX_t'B) = E(Y_{t+1}Y_{t+1}') - B'E(X_tX_t')B$
> $E(B'X_tX_t'B) = \begin{bmatrix} 1.5 & 0.4 \\ -0.2 & 1.3 \end{bmatrix} \begin{bmatrix} 4 & 1 \\ 1 & 5 \end{bmatrix} \begin{bmatrix} 1.5 & -0.2 \\ 0.4 & 1.3 \end{bmatrix} = \begin{bmatrix} 6.4 & 3.5 \\ 0.5 & 6.3 \end{bmatrix} \begin{bmatrix} 1.5 & -0.2 \\ 0.4 & 1.3 \end{bmatrix} = \begin{bmatrix} 11 & 3.27 \\ 3.27 & 8.09 \end{bmatrix}$
>
> $MSE(B'X_t) = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 11 & 3.27 \\ 3.27 & 8.09 \end{bmatrix}  = \begin{bmatrix} -1 & 0.73 \\ 0.73 & 11.91 \end{bmatrix}$.
>
> O c√°lculo correto √©:
> $MSE(B'X_t) = E[(Y_{t+1} - B'X_t)(Y_{t+1} - B'X_t)'] = E(Y_{t+1}Y_{t+1}') - E(Y_{t+1}X_t')B - B'E(X_tY_{t+1}') + B'E(X_tX_t')B$
>
> $MSE(B'X_t) = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 6 & 3 \\ 5 & 8 \end{bmatrix}  \begin{bmatrix} 1.5 & -0.2 \\ 0.4 & 1.3 \end{bmatrix} - \begin{bmatrix} 1.5 & 0.4 \\ -0.2 & 1.3 \end{bmatrix} \begin{bmatrix} 6 & 5 \\ 3 & 8 \end{bmatrix}  + \begin{bmatrix} 1.5 & 0.4 \\ -0.2 & 1.3 \end{bmatrix} \begin{bmatrix} 4 & 1 \\ 1 & 5 \end{bmatrix}  \begin{bmatrix} 1.5 & -0.2 \\ 0.4 & 1.3 \end{bmatrix}$
>
> $MSE(B'X_t) = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 10.2 & 2.7 \\ 6.2 & 9.9 \end{bmatrix} - \begin{bmatrix} 10.2 & 6.5 \\ 2.7 & 9.4 \end{bmatrix} + \begin{bmatrix} 11 & 3.27 \\ 3.27 & 8.09 \end{bmatrix}  = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 10.2+10.2-11 & 2.7+6.5-3.27 \\ 6.2+2.7-3.27 & 9.9+9.4-8.09 \end{bmatrix} = \begin{bmatrix} 10 & 4 \\ 4 & 20 \end{bmatrix} - \begin{bmatrix} 9.4 & 6 \\ 5.63 & 11.21 \end{bmatrix} =  \begin{bmatrix} 0.6 & -2 \\ -1.63 & 8.79 \end{bmatrix} $
>
>  Note que a matriz acima tamb√©m n√£o parece positiva semidefinida, indicando que um erro no c√°lculo.
>
> $MSE(B'X_t) = MSE(\alpha'X_t) + E[(\alpha'-B')X_tX_t'(\alpha-B)]$
>
> $\alpha' - B' = \begin{bmatrix} 1.42 & 0.32 \\ -0.37 & 1.42 \end{bmatrix} - \begin{bmatrix} 1.5 & 0.4 \\ -0.2 & 1.3 \end{bmatrix} = \begin{bmatrix} -0.08 & -0.08 \\ -0.17 & 0.12 \end{bmatrix}$
>
> $E[(\alpha'-B')X_tX_t'(\alpha-B)] = \begin{bmatrix} -0.08 & -0.08 \\ -0.17 & 0.12 \end{bmatrix} \begin{bmatrix} 4 & 1 \\ 1 & 5 \end{bmatrix}  \begin{bmatrix} -0.08 & -0.17 \\ -0.08 & 0.12 \end{bmatrix} = \begin{bmatrix} -0.4 & -0.48 \\ -0.56 & 0.43 \end{bmatrix}  \begin{bmatrix} -0.08 & -0.17 \\ -0.08 & 0.12 \end{bmatrix}  = \begin{bmatrix} 0.0672 & -0.0176 \\ -0.0056 & 0.14 \end{bmatrix} $
>
> $MSE(B'X_t) =  \begin{bmatrix} 0.53 & 1.95 \\ 2.6 & 9.77 \end{bmatrix} + \begin{bmatrix} 0.0672 & -0.0176 \\ -0.0056 & 0.14 \end{bmatrix} = \begin{bmatrix} 0.5972 & 1.9324 \\ 2.5944 & 9.91 \end{bmatrix}$
>
>  Note que o  $MSE(B'X_t)$ √© maior (no sentido de matrizes semidefinidas positivas) que $MSE(\alpha'X_t)$.
>
> A ideia principal √© que qualquer outro preditor linear $B'X_t$ ter√° um MSE maior ou igual ao preditor $\alpha'X_t$, demonstrando a otimalidade da proje√ß√£o linear sob a condi√ß√£o de n√£o correla√ß√£o.

A condi√ß√£o de n√£o correla√ß√£o √© fundamental n√£o apenas para garantir a otimalidade da proje√ß√£o linear, mas tamb√©m para facilitar a interpreta√ß√£o dos resultados. Ao assegurar que o erro de previs√£o seja ortogonal √†s vari√°veis preditoras, podemos afirmar que o modelo linear capturou toda a informa√ß√£o relevante de $X_t$ para prever $Y_{t+1}$, e que nenhum padr√£o restante no erro pode ser explicado por uma fun√ß√£o linear de $X_t$.

**Lema 2.3** *Se a condi√ß√£o de n√£o correla√ß√£o $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$ √© satisfeita, ent√£o a matriz de covari√¢ncia do erro de previs√£o, $E[(Y_{t+1} - \hat{Y}_{t+1})(Y_{t+1} - \hat{Y}_{t+1})']$, √© igual √† matriz de covari√¢ncia de $Y_{t+1}$ menos a matriz de covari√¢ncia da proje√ß√£o linear $\hat{Y}_{t+1}$, ou seja, $E[(Y_{t+1} - \hat{Y}_{t+1})(Y_{t+1} - \hat{Y}_{t+1})'] = E(Y_{t+1}Y_{t+1}') - E(\hat{Y}_{t+1}\hat{Y}_{t+1}')$.*

*Prova:*
I. Come√ßamos com a defini√ß√£o do erro de previs√£o: $e_{t+1} = Y_{t+1} - \hat{Y}_{t+1} = Y_{t+1} - \alpha'X_t$.
II. Queremos calcular a matriz de covari√¢ncia do erro, $E[e_{t+1}e_{t+1}'] = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)']$.
III. Expandindo a express√£o, temos: $E[e_{t+1}e_{t+1}'] = E[Y_{t+1}Y_{t+1}'] - E[Y_{t+1}X_t']\alpha - \alpha'E[X_tY_{t+1}'] + \alpha'E[X_tX_t']\alpha$.
IV. Substituindo a express√£o de $\alpha'$ em $E[Y_{t+1}X_t']\alpha + \alpha'E[X_tY_{t+1}']$, temos: $E[Y_{t+1}X_t'](E[X_tX_t']^{-1}E[X_tY_{t+1}'])' + E[Y_{t+1}X_t'](E[X_tX_t']^{-1}E[X_tY_{t+1}']) = 2E[Y_{t+1}X_t']E[X_tX_t']^{-1}E[X_tY_{t+1}'] $.
V. Simplificando, obtemos $2E[Y_{t+1}X_t']E[X_tX_t']^{-1}E[X_tY_{t+1}'] = 2E[\hat{Y}_{t+1}\hat{Y}_{t+1}']$.
VI. Substituindo em III e notando que $\alpha'E[X_tX_t']\alpha= E[\hat{Y}_{t+1} \hat{Y}_{t+1}']$, obtemos:  $E[e_{t+1}e_{t+1}'] = E[Y_{t+1}Y_{t+1}'] - 2E[\hat{Y}_{t+1}\hat{Y}_{t+1}'] + E[\hat{Y}_{t+1}\hat{Y}_{t+1}'] = E[Y_{t+1}Y_{t+1}'] - E[\hat{Y}_{t+1}\hat{Y}_{t+1}']$.
VII. Finalmente, lembrando que  $E[Y_{t+1}Y_{t+1}'] = E[Y_{t+1}^2]$  e  $E[\hat{Y}_{t+1}\hat{Y}_{t+1}'] = E[\hat{Y}_{t+1}^2]$, obtemos:
$E[e_{t+1}e_{t+1}'] = E[Y_{t+1}^2] - E[\hat{Y}_{t+1}^2]$.

**Conclus√£o**

A vari√¢ncia do erro de previs√£o √© igual √† diferen√ßa entre a vari√¢ncia da vari√°vel dependente e a vari√¢ncia da previs√£o. Isso indica que, quanto maior a vari√¢ncia da previs√£o, menor ser√° a vari√¢ncia do erro.

**Interpreta√ß√£o geom√©trica**

Em um espa√ßo vetorial, podemos interpretar a previs√£o $\hat{Y}_{t+1}$ como a proje√ß√£o ortogonal de $Y_{t+1}$ sobre o subespa√ßo gerado pelas vari√°veis explicativas $X_t$. O erro de previs√£o $e_{t+1}$ √© o vetor que liga $Y_{t+1}$ √† sua proje√ß√£o $\hat{Y}_{t+1}$. A decomposi√ß√£o da vari√¢ncia do erro que derivamos acima reflete o Teorema de Pit√°goras nesse espa√ßo.

**Ilustra√ß√£o com um exemplo simples**

Consideremos o caso de uma regress√£o linear simples, onde $Y_{t+1} = \alpha X_t + e_{t+1}$.

1.  A previs√£o √© dada por $\hat{Y}_{t+1} = \hat{\alpha} X_t$, onde $\hat{\alpha}$ √© o estimador de m√≠nimos quadrados de $\alpha$.
2.  O erro √©  $e_{t+1} = Y_{t+1} - \hat{Y}_{t+1}$.
3.  Podemos mostrar que $E[e_{t+1}X_t'] = 0$, o que significa que o erro √© ortogonal √†s vari√°veis explicativas.
4.  A vari√¢ncia de $Y_{t+1}$ √©  $Var[Y_{t+1}] = \alpha^2 Var[X_t] + Var[e_{t+1}]$.
5.  A vari√¢ncia da previs√£o √©  $Var[\hat{Y}_{t+1}] = \hat{\alpha}^2 Var[X_t]$.
6.  A vari√¢ncia do erro de previs√£o √© $Var[e_{t+1}] = Var[Y_{t+1}] - Var[\hat{Y}_{t+1}]$.

Este exemplo simples ilustra a validade da decomposi√ß√£o da vari√¢ncia do erro, mesmo em um caso menos geral.

<!-- END -->
