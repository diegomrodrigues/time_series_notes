## PrevisÃ£o de Vetores: OtimizaÃ§Ã£o e Propriedades da ProjeÃ§Ã£o Linear Multivariada
### IntroduÃ§Ã£o
Este capÃ­tulo aprofunda a anÃ¡lise da previsÃ£o de vetores, abordando a otimalidade da projeÃ§Ã£o linear no contexto multivariado e as propriedades decorrentes dessa otimizaÃ§Ã£o. Como estabelecido anteriormente, a projeÃ§Ã£o linear visa encontrar a melhor aproximaÃ§Ã£o linear de um vetor de variÃ¡veis, $Y_{t+1}$, em termos de um vetor de variÃ¡veis preditoras, $X_t$. Este capÃ­tulo explora como essa aproximaÃ§Ã£o garante previsÃµes de mÃ­nimo erro quadrÃ¡tico mÃ©dio (MSE) para os elementos de $Y_{t+1}$ e suas combinaÃ§Ãµes lineares.

### Conceitos Fundamentais
A projeÃ§Ã£o linear de um vetor $Y_{t+1}$ sobre um vetor $X_t$ Ã© definida como $\hat{Y}_{t+1} = \alpha'X_t$ [^1], onde $\alpha'$ Ã© uma matriz de coeficientes de projeÃ§Ã£o de dimensÃ£o $n \times m$. A condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o, $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$ [^2], Ã© fundamental para garantir que a projeÃ§Ã£o linear seja Ã³tima no sentido do MSE. A expressÃ£o [4.1.23] para a matriz de coeficientes de projeÃ§Ã£o, $\alpha'$, Ã© dada por:
$$
\alpha' = [E(Y_{t+1}X_t')] [E(X_tX_t')]^{-1}. \quad [4.1.23]
$$
Essa matriz, como demonstrado anteriormente, minimiza o erro quadrÃ¡tico mÃ©dio (MSE) [^Teorema1], que no contexto multivariado Ã© dado por [4.1.24]:
$$
MSE(\alpha'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)'] \quad [4.1.24].
$$
A interpretaÃ§Ã£o da condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o Ã© crucial: ela garante que o erro de previsÃ£o, $Y_{t+1} - \hat{Y}_{t+1}$, seja ortogonal ao espaÃ§o gerado pelas variÃ¡veis preditoras $X_t$ [^Lema2.1].

**Teorema 2.1** *A projeÃ§Ã£o linear $\hat{Y}_{t+1} = \alpha'X_t$ gera previsÃµes de mÃ­nimo erro quadrÃ¡tico mÃ©dio (MSE) para os elementos do vetor $Y_{t+1}$.*

*Prova:*
I. Seja $e_{t+1} = Y_{t+1} - \hat{Y}_{t+1} = Y_{t+1} - \alpha'X_t$ o erro de previsÃ£o.
II. A condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o, $E[e_{t+1}X_t']=0$, implica que cada elemento do vetor de erro $e_{t+1}$ Ã© nÃ£o correlacionado com cada elemento do vetor $X_t$.
III. Considere a projeÃ§Ã£o linear de um elemento especÃ­fico de $Y_{t+1}$, digamos $y_{i,t+1}$, que Ã© o $i$-Ã©simo elemento do vetor $Y_{t+1}$. A projeÃ§Ã£o linear de $y_{i,t+1}$ Ã© dada por $\hat{y}_{i,t+1}$, que Ã© o $i$-Ã©simo elemento do vetor $\hat{Y}_{t+1}$.
IV. Como $\hat{Y}_{t+1} = \alpha'X_t$ minimiza o $MSE(\alpha'X_t)$,  segue que $\hat{y}_{i,t+1}$ tambÃ©m minimiza o $MSE$ do seu erro de previsÃ£o $E[(y_{i,t+1} - \hat{y}_{i,t+1})^2]$. Isso ocorre porque $E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)']$ representa a matriz de covariÃ¢ncia do erro de previsÃ£o, onde os elementos diagonais sÃ£o exatamente os $MSE$s dos elementos individuais.
V. Portanto, cada elemento de $\hat{Y}_{t+1}$ gera previsÃµes de mÃ­nimo $MSE$ para cada elemento correspondente de $Y_{t+1}$. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que queremos prever $Y_{t+1} = \begin{bmatrix} y_{1,t+1} \\ y_{2,t+1} \end{bmatrix}$ com $X_t = \begin{bmatrix} x_{1,t} \\ x_{2,t} \end{bmatrix}$ e temos que $\alpha' = \begin{bmatrix} 1/2 & 1/2 \\ 1/4 & 3/2 \end{bmatrix}$. A previsÃ£o Ã© $\hat{Y}_{t+1} = \begin{bmatrix} 1/2 & 1/2 \\ 1/4 & 3/2 \end{bmatrix} X_t = \begin{bmatrix} 0.5x_{1,t} + 0.5x_{2,t} \\ 0.25x_{1,t} + 1.5x_{2,t} \end{bmatrix}$.  O Teorema 2.1 garante que $0.5x_{1,t} + 0.5x_{2,t}$ Ã© o preditor linear de mÃ­nimo MSE para $y_{1,t+1}$, e $0.25x_{1,t} + 1.5x_{2,t}$ Ã© o preditor linear de mÃ­nimo MSE para $y_{2,t+1}$.
>
> Usando momentos populacionais hipotÃ©ticos $E(Y_{t+1}X_t') = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}$, $E(X_tX_t') = \begin{bmatrix} 4 & 0 \\ 0 & 2 \end{bmatrix}$ e $E(Y_{t+1}Y_{t+1}') = \begin{bmatrix} 5 & 2 \\ 2 & 6 \end{bmatrix}$, temos que $\alpha' = E(Y_{t+1}X_t')E(X_tX_t')^{-1} =  \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix} \begin{bmatrix} 1/4 & 0 \\ 0 & 1/2 \end{bmatrix} = \begin{bmatrix} 1/2 & 1/2 \\ 1/4 & 3/2 \end{bmatrix}$  e $MSE(\alpha'X_t) =  E(Y_{t+1}Y_{t+1}') - E(Y_{t+1}X_t')E(X_tX_t')^{-1}E(X_tY_{t+1}') = \begin{bmatrix} 5 & 2 \\ 2 & 6 \end{bmatrix} - \begin{bmatrix} 1/2 & 1/2 \\ 1/4 & 3/2 \end{bmatrix} \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix} =  \begin{bmatrix} 5 & 2 \\ 2 & 6 \end{bmatrix} - \begin{bmatrix} 1.5 & 2 \\ 1.75 & 4.75 \end{bmatrix} =  \begin{bmatrix} 3.5 & 0 \\ 0.25 & 1.25 \end{bmatrix}$.  Assim, 3.5 Ã© o mÃ­nimo MSE alcanÃ§Ã¡vel para a previsÃ£o de $y_{1,t+1}$ e 1.25 Ã© o mÃ­nimo MSE alcanÃ§Ã¡vel para a previsÃ£o de $y_{2,t+1}$.
>
> Observe que o Teorema 2.1 garante a otimalidade para cada elemento de Y separadamente, nÃ£o apenas para o vetor. Em outras palavras, nÃ£o Ã© apenas o MSE da previsÃ£o vetorial que Ã© minimizado, mas tambÃ©m os MSEs das previsÃµes de cada componente individual.
>
> Em termos geomÃ©tricos, isso significa que a projeÃ§Ã£o de cada componente de $Y_{t+1}$ Ã© a "sombra" ortogonal daquele componente sobre o espaÃ§o gerado pelas variÃ¡veis preditoras, e essa "sombra" Ã© a melhor possÃ­vel no sentido de minimizar a distÃ¢ncia ao ponto de origem.

Uma propriedade adicional, e crucial para aplicaÃ§Ãµes prÃ¡ticas, Ã© que a projeÃ§Ã£o linear preserva a otimalidade para qualquer combinaÃ§Ã£o linear dos elementos do vetor $Y_{t+1}$. Ou seja, se estivermos interessados em prever uma combinaÃ§Ã£o linear especÃ­fica dos elementos de $Y_{t+1}$, digamos $h'Y_{t+1}$, onde $h$ Ã© um vetor de constantes, a melhor previsÃ£o linear de $h'Y_{t+1}$ Ã© dada por $h'\hat{Y}_{t+1}$, que Ã© a mesma combinaÃ§Ã£o linear da projeÃ§Ã£o linear de $Y_{t+1}$.

**Teorema 2.2** *A projeÃ§Ã£o linear de qualquer combinaÃ§Ã£o linear dos elementos de $Y_{t+1}$, digamos $h'Y_{t+1}$, onde $h$ Ã© um vetor de constantes, Ã© dada por $h'\hat{Y}_{t+1}$.*

*Prova:*
I. Seja $z_{t+1} = h'Y_{t+1}$ uma combinaÃ§Ã£o linear dos elementos do vetor $Y_{t+1}$.
II. A projeÃ§Ã£o linear de $z_{t+1}$ Ã© dada por $\hat{z}_{t+1} = E(z_{t+1}X_t')E(X_tX_t')^{-1}X_t$, que tambÃ©m pode ser expressa como  $\hat{z}_{t+1} = E(h'Y_{t+1}X_t')E(X_tX_t')^{-1}X_t = h'E(Y_{t+1}X_t')E(X_tX_t')^{-1}X_t$.
III. Como $\alpha' = E(Y_{t+1}X_t')E(X_tX_t')^{-1}$  e $\hat{Y}_{t+1} = \alpha'X_t$, temos $\hat{z}_{t+1} = h'\alpha'X_t = h'\hat{Y}_{t+1}$.
IV. Portanto, a projeÃ§Ã£o linear de $h'Y_{t+1}$ Ã© $h'\hat{Y}_{t+1}$, que Ã© a mesma combinaÃ§Ã£o linear da projeÃ§Ã£o linear de $Y_{t+1}$. â– 
> ðŸ’¡ **Exemplo NumÃ©rico:** Considere $Y_{t+1} = \begin{bmatrix} y_{1,t+1} \\ y_{2,t+1} \end{bmatrix}$ e seja $h = \begin{bmatrix} 2 \\ -1 \end{bmatrix}$. EntÃ£o, $h'Y_{t+1} = 2y_{1,t+1} - y_{2,t+1}$. A projeÃ§Ã£o linear de $Y_{t+1}$ Ã© $\hat{Y}_{t+1} = \alpha' X_t$. O Teorema 2.2 garante que a projeÃ§Ã£o linear de $h'Y_{t+1}$ Ã© dada por $h'\hat{Y}_{t+1} = 2\hat{y}_{1,t+1} - \hat{y}_{2,t+1}$, que Ã© a mesma combinaÃ§Ã£o linear da projeÃ§Ã£o de $Y_{t+1}$. Isso implica que nÃ£o precisamos recalcular as projeÃ§Ãµes lineares para combinaÃ§Ãµes lineares de $Y_{t+1}$; basta aplicar as mesmas combinaÃ§Ãµes lineares Ã s projeÃ§Ãµes de $Y_{t+1}$.
>
>  Se tivermos $\hat{Y}_{t+1} = \begin{bmatrix} 0.8 \\ 1.2 \end{bmatrix}$, entÃ£o a melhor previsÃ£o para $h'Y_{t+1}$ Ã© $2(0.8)-1.2=0.4$, sem a necessidade de um novo cÃ¡lculo. A previsÃ£o de $h'Y_{t+1}$ Ã© o preditor linear de mÃ­nimo MSE para $h'Y_{t+1}$.
>
> ```mermaid
> graph LR
>     A[Y_{t+1}] -->|ProjeÃ§Ã£o Linear| B[hat{Y}_{t+1}]
>     C[h'Y_{t+1}] -->|ProjeÃ§Ã£o Linear| D[h'hat{Y}_{t+1}]
>     B --> |h'| D
> ```
> O diagrama ilustra como a projeÃ§Ã£o linear de $h'Y_{t+1}$ Ã© equivalente a aplicar $h'$ na projeÃ§Ã£o linear de $Y_{t+1}$, sem a necessidade de recalcular a projeÃ§Ã£o linear.

**Teorema 2.3** *A projeÃ§Ã£o linear $\hat{Y}_{t+1} = \alpha'X_t$ Ã© a melhor aproximaÃ§Ã£o linear no sentido de minimizar a matriz de erro quadrÃ¡tico mÃ©dio (MSE) para todo vetor $B'X_t$, onde B Ã© uma matriz de coeficientes de dimensÃ£o $n \times m$, isto Ã©, $MSE(\alpha'X_t) \leq MSE(B'X_t)$, onde a desigualdade Ã© no sentido de matrizes semidefinidas positivas.*

*Prova:*
I. Seja $B$ uma matriz de coeficientes $n \times m$, e considere a previsÃ£o linear $\tilde{Y}_{t+1} = B'X_t$.
II. O MSE dessa previsÃ£o Ã© dado por $MSE(B'X_t) = E[(Y_{t+1} - B'X_t)(Y_{t+1} - B'X_t)']$.
III. Podemos expressar o erro dessa previsÃ£o como $Y_{t+1} - B'X_t = (Y_{t+1} - \alpha'X_t) + (\alpha'X_t - B'X_t) = e_{t+1} + (\alpha' - B')X_t$.
IV. Substituindo isso na expressÃ£o do MSE, temos:
$MSE(B'X_t) = E[(e_{t+1} + (\alpha' - B')X_t)(e_{t+1} + (\alpha' - B')X_t)'] = E[e_{t+1}e_{t+1}'] + E[(\alpha' - B')X_tX_t'(\alpha' - B')'] + E[e_{t+1}X_t'(\alpha' - B')'] + E[(\alpha' - B')X_te_{t+1}']$.
V. Pela condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o $E[e_{t+1}X_t']=0$, os dois Ãºltimos termos sÃ£o nulos.
VI. Portanto, $MSE(B'X_t) = E[e_{t+1}e_{t+1}'] + E[(\alpha' - B')X_tX_t'(\alpha' - B')'] = MSE(\alpha'X_t) + E[(\alpha' - B')X_tX_t'(\alpha' - B')']$.
VII. Como $E[(\alpha' - B')X_tX_t'(\alpha' - B')']$ Ã© uma matriz semidefinida positiva, segue que $MSE(B'X_t) \geq MSE(\alpha'X_t)$, onde a desigualdade Ã© no sentido de matrizes semidefinidas positivas.
VIII. Isso demonstra que a projeÃ§Ã£o linear $\hat{Y}_{t+1} = \alpha'X_t$ Ã© a melhor aproximaÃ§Ã£o linear no sentido de minimizar a matriz MSE. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar o Teorema 2.3, vamos usar os momentos populacionais do exemplo anterior e comparar o MSE da projeÃ§Ã£o Ã³tima com uma projeÃ§Ã£o nÃ£o-Ã³tima. Assumimos que a projeÃ§Ã£o Ã³tima Ã© $\alpha' = \begin{bmatrix} 1/2 & 1/2 \\ 1/4 & 3/2 \end{bmatrix}$. Agora, considere uma matriz de coeficientes nÃ£o-Ã³tima, $B' = \begin{bmatrix} 1/3 & 1/3 \\ 1/3 & 1 \end{bmatrix}$.
>
> Temos $MSE(\alpha'X_t) = \begin{bmatrix} 3.5 & 0 \\ 0 & 1.25 \end{bmatrix}$, como calculado anteriormente.  Agora, vamos calcular $MSE(B'X_t)$. Primeiro, vamos calcular $E[(Y_{t+1}-B'X_t)(Y_{t+1}-B'X_t)']$.
>
> $MSE(B'X_t) = E(Y_{t+1}Y_{t+1}') - E(Y_{t+1}X_t')B - B'E(X_tY_{t+1}') + B'E(X_tX_t')B$
>
> $MSE(B'X_t) = \begin{bmatrix} 5 & 2 \\ 2 & 6 \end{bmatrix} - \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix} \begin{bmatrix} 1/3 & 1/3 \\ 1/3 & 1 \end{bmatrix} - \begin{bmatrix} 1/3 & 1/3 \\ 1/3 & 1 \end{bmatrix} \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix} + \begin{bmatrix} 1/3 & 1/3 \\ 1/3 & 1 \end{bmatrix} \begin{bmatrix} 4 & 0 \\ 0 & 2 \end{bmatrix} \begin{bmatrix} 1/3 & 1/3 \\ 1/3 & 1 \end{bmatrix}$
>
> $MSE(B'X_t) = \begin{bmatrix} 5 & 2 \\ 2 & 6 \end{bmatrix} - \begin{bmatrix} 1 & 5/3 \\ 4/3 & 10/3 \end{bmatrix} - \begin{bmatrix} 1 & 4/3 \\ 5/3 & 10/3 \end{bmatrix} + \begin{bmatrix} 4/9 + 2/9 & 4/9+2/3 \\ 4/9+2/3 & 4/9+2 \end{bmatrix} \begin{bmatrix} 1/3 & 1/3 \\ 1/3 & 1 \end{bmatrix} = \begin{bmatrix} 5 & 2 \\ 2 & 6 \end{bmatrix} - \begin{bmatrix} 2 & 3 \\ 3 & 20/3 \end{bmatrix} + \begin{bmatrix} 2/3 & 10/9 \\ 10/9 & 22/9 \end{bmatrix} = \begin{bmatrix} 3.667 & -1 \\ -1 & 2.556 \end{bmatrix}$
>
> Note que a diagonal de $MSE(B'X_t)$ Ã© maior que a diagonal de $MSE(\alpha'X_t)$. $3.667 > 3.5$ e $2.556>1.25$. Isso ilustra que a projeÃ§Ã£o linear Ã³tima $\alpha'$ minimiza o MSE, como garantido pelo Teorema 2.3. A diferenÃ§a $MSE(B'X_t) - MSE(\alpha'X_t)$ Ã© uma matriz semidefinida positiva.

AlÃ©m disso, a condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o e a expressÃ£o para a matriz de coeficientes de projeÃ§Ã£o, $\alpha'$, garantem que a projeÃ§Ã£o linear Ã© a melhor aproximaÃ§Ã£o no sentido de minimizar o erro quadrÃ¡tico mÃ©dio (MSE) da previsÃ£o, tanto para as variÃ¡veis individuais quanto para suas combinaÃ§Ãµes lineares.

### ConclusÃ£o
Este capÃ­tulo reforÃ§ou a compreensÃ£o da projeÃ§Ã£o linear para vetores, demonstrando que a projeÃ§Ã£o $\hat{Y}_{t+1} = \alpha'X_t$ nÃ£o apenas fornece a melhor aproximaÃ§Ã£o linear de $Y_{t+1}$ como um todo, mas tambÃ©m gera previsÃµes de mÃ­nimo MSE para cada um dos seus componentes individuais. Adicionalmente, a propriedade de preservar a otimalidade para combinaÃ§Ãµes lineares dos elementos de $Y_{t+1}$ demonstra a robustez e a aplicabilidade da projeÃ§Ã£o linear no contexto multivariado. A condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o e a fÃ³rmula para $\alpha'$ sÃ£o cruciais para a derivar estas propriedades de otimalidade e sÃ£o fundamentos para os modelos economÃ©tricos e de sÃ©ries temporais que usam previsÃµes multivariadas.
### ReferÃªncias
[^1]: *[4.1.21] The preceding results can be extended to forecast an (n Ã— 1) vector $Y_{t+1}$ on the basis of a linear function of an (m x 1) vector $X_t$: $P(Y_{t+1}|X_t) = \alpha'X_t = \hat{Y}_{t+1}$*
[^2]: *[4.1.22] ...that is, each of the n elements of ($Y_{t+1} - \hat{Y}_{t+1}$) is uncorrelated with each of the m elements of $X_t$.*
[^3]: *[4.1.23] From [4.1.22], the matrix of projection coefficients is given by $\alpha' = [E(Y_{t+1}X_t')] \cdot [E(X_tX_t')]^{-1}$*
[^4]: *[4.1.24] The matrix generalization of the formula for the mean squared error [4.1.15] is $MSE(\alpha'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)'] = E(Y_{t+1}Y_{t+1}') - [E(Y_{t+1}X_t')] \cdot [E(X_tX_t')]^{-1} \cdot [E(X_tY_{t+1})']$*
[^Teorema1]: *Teorema 1: A matriz de coeficientes de projeÃ§Ã£o Î±' minimiza o MSE no sentido de que para qualquer outra matriz B (n x m), $MSE(\alpha'X_t) \leq MSE(B'X_t)$, onde a desigualdade Ã© no sentido de matrizes semidefinidas positivas.*
[^Lema2.1]: *Lema 2.1: A condiÃ§Ã£o de nÃ£o correlaÃ§Ã£o $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$ implica que os resÃ­duos da projeÃ§Ã£o linear, $Y_{t+1} - \alpha'X_t$, sÃ£o ortogonais ao espaÃ§o gerado pelas variÃ¡veis preditoras $X_t$.*
<!-- END -->
