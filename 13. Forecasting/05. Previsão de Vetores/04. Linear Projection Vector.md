## Previs√£o de Vetores: Otimiza√ß√£o e Propriedades da Proje√ß√£o Linear Multivariada
### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise da previs√£o de vetores, abordando a otimalidade da proje√ß√£o linear no contexto multivariado e as propriedades decorrentes dessa otimiza√ß√£o. Como estabelecido anteriormente, a proje√ß√£o linear visa encontrar a melhor aproxima√ß√£o linear de um vetor de vari√°veis, $Y_{t+1}$, em termos de um vetor de vari√°veis preditoras, $X_t$. Este cap√≠tulo explora como essa aproxima√ß√£o garante previs√µes de m√≠nimo erro quadr√°tico m√©dio (MSE) para os elementos de $Y_{t+1}$ e suas combina√ß√µes lineares.

### Conceitos Fundamentais
A proje√ß√£o linear de um vetor $Y_{t+1}$ sobre um vetor $X_t$ √© definida como $\hat{Y}_{t+1} = \alpha'X_t$ [^1], onde $\alpha'$ √© uma matriz de coeficientes de proje√ß√£o de dimens√£o $n \times m$. A condi√ß√£o de n√£o correla√ß√£o, $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$ [^2], √© fundamental para garantir que a proje√ß√£o linear seja √≥tima no sentido do MSE. A express√£o [4.1.23] para a matriz de coeficientes de proje√ß√£o, $\alpha'$, √© dada por:
$$
\alpha' = [E(Y_{t+1}X_t')] [E(X_tX_t')]^{-1}. \quad [4.1.23]
$$
Essa matriz, como demonstrado anteriormente, minimiza o erro quadr√°tico m√©dio (MSE) [^Teorema1], que no contexto multivariado √© dado por [4.1.24]:
$$
MSE(\alpha'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)'] \quad [4.1.24].
$$
A interpreta√ß√£o da condi√ß√£o de n√£o correla√ß√£o √© crucial: ela garante que o erro de previs√£o, $Y_{t+1} - \hat{Y}_{t+1}$, seja ortogonal ao espa√ßo gerado pelas vari√°veis preditoras $X_t$ [^Lema2.1].

**Teorema 2.1** *A proje√ß√£o linear $\hat{Y}_{t+1} = \alpha'X_t$ gera previs√µes de m√≠nimo erro quadr√°tico m√©dio (MSE) para os elementos do vetor $Y_{t+1}$.*

*Prova:*
I. Seja $e_{t+1} = Y_{t+1} - \hat{Y}_{t+1} = Y_{t+1} - \alpha'X_t$ o erro de previs√£o.
II. A condi√ß√£o de n√£o correla√ß√£o, $E[e_{t+1}X_t']=0$, implica que cada elemento do vetor de erro $e_{t+1}$ √© n√£o correlacionado com cada elemento do vetor $X_t$.
III. Considere a proje√ß√£o linear de um elemento espec√≠fico de $Y_{t+1}$, digamos $y_{i,t+1}$, que √© o $i$-√©simo elemento do vetor $Y_{t+1}$. A proje√ß√£o linear de $y_{i,t+1}$ √© dada por $\hat{y}_{i,t+1}$, que √© o $i$-√©simo elemento do vetor $\hat{Y}_{t+1}$.
IV. Como $\hat{Y}_{t+1} = \alpha'X_t$ minimiza o $MSE(\alpha'X_t)$,  segue que $\hat{y}_{i,t+1}$ tamb√©m minimiza o $MSE$ do seu erro de previs√£o $E[(y_{i,t+1} - \hat{y}_{i,t+1})^2]$. Isso ocorre porque $E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)']$ representa a matriz de covari√¢ncia do erro de previs√£o, onde os elementos diagonais s√£o exatamente os $MSE$s dos elementos individuais.
V. Portanto, cada elemento de $\hat{Y}_{t+1}$ gera previs√µes de m√≠nimo $MSE$ para cada elemento correspondente de $Y_{t+1}$. ‚ñ†

> üí° **Exemplo Num√©rico:** Suponha que queremos prever $Y_{t+1} = \begin{bmatrix} y_{1,t+1} \\ y_{2,t+1} \end{bmatrix}$ com $X_t = \begin{bmatrix} x_{1,t} \\ x_{2,t} \end{bmatrix}$ e temos que $\alpha' = \begin{bmatrix} 1/2 & 1/2 \\ 1/4 & 3/2 \end{bmatrix}$. A previs√£o √© $\hat{Y}_{t+1} = \begin{bmatrix} 1/2 & 1/2 \\ 1/4 & 3/2 \end{bmatrix} X_t = \begin{bmatrix} 0.5x_{1,t} + 0.5x_{2,t} \\ 0.25x_{1,t} + 1.5x_{2,t} \end{bmatrix}$.  O Teorema 2.1 garante que $0.5x_{1,t} + 0.5x_{2,t}$ √© o preditor linear de m√≠nimo MSE para $y_{1,t+1}$, e $0.25x_{1,t} + 1.5x_{2,t}$ √© o preditor linear de m√≠nimo MSE para $y_{2,t+1}$.
>
> Usando momentos populacionais hipot√©ticos $E(Y_{t+1}X_t') = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}$, $E(X_tX_t') = \begin{bmatrix} 4 & 0 \\ 0 & 2 \end{bmatrix}$ e $E(Y_{t+1}Y_{t+1}') = \begin{bmatrix} 5 & 2 \\ 2 & 6 \end{bmatrix}$, temos que $\alpha' = E(Y_{t+1}X_t')E(X_tX_t')^{-1} =  \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix} \begin{bmatrix} 1/4 & 0 \\ 0 & 1/2 \end{bmatrix} = \begin{bmatrix} 1/2 & 1/2 \\ 1/4 & 3/2 \end{bmatrix}$  e $MSE(\alpha'X_t) =  E(Y_{t+1}Y_{t+1}') - E(Y_{t+1}X_t')E(X_tX_t')^{-1}E(X_tY_{t+1}') = \begin{bmatrix} 5 & 2 \\ 2 & 6 \end{bmatrix} - \begin{bmatrix} 1/2 & 1/2 \\ 1/4 & 3/2 \end{bmatrix} \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix} =  \begin{bmatrix} 5 & 2 \\ 2 & 6 \end{bmatrix} - \begin{bmatrix} 1.5 & 2 \\ 1.75 & 4.75 \end{bmatrix} =  \begin{bmatrix} 3.5 & 0 \\ 0.25 & 1.25 \end{bmatrix}$.  Assim, 3.5 √© o m√≠nimo MSE alcan√ß√°vel para a previs√£o de $y_{1,t+1}$ e 1.25 √© o m√≠nimo MSE alcan√ß√°vel para a previs√£o de $y_{2,t+1}$.
>
> Observe que o Teorema 2.1 garante a otimalidade para cada elemento de Y separadamente, n√£o apenas para o vetor. Em outras palavras, n√£o √© apenas o MSE da previs√£o vetorial que √© minimizado, mas tamb√©m os MSEs das previs√µes de cada componente individual.
>
> Em termos geom√©tricos, isso significa que a proje√ß√£o de cada componente de $Y_{t+1}$ √© a "sombra" ortogonal daquele componente sobre o espa√ßo gerado pelas vari√°veis preditoras, e essa "sombra" √© a melhor poss√≠vel no sentido de minimizar a dist√¢ncia ao ponto de origem.

Uma propriedade adicional, e crucial para aplica√ß√µes pr√°ticas, √© que a proje√ß√£o linear preserva a otimalidade para qualquer combina√ß√£o linear dos elementos do vetor $Y_{t+1}$. Ou seja, se estivermos interessados em prever uma combina√ß√£o linear espec√≠fica dos elementos de $Y_{t+1}$, digamos $h'Y_{t+1}$, onde $h$ √© um vetor de constantes, a melhor previs√£o linear de $h'Y_{t+1}$ √© dada por $h'\hat{Y}_{t+1}$, que √© a mesma combina√ß√£o linear da proje√ß√£o linear de $Y_{t+1}$.

**Teorema 2.2** *A proje√ß√£o linear de qualquer combina√ß√£o linear dos elementos de $Y_{t+1}$, digamos $h'Y_{t+1}$, onde $h$ √© um vetor de constantes, √© dada por $h'\hat{Y}_{t+1}$.*

*Prova:*
I. Seja $z_{t+1} = h'Y_{t+1}$ uma combina√ß√£o linear dos elementos do vetor $Y_{t+1}$.
II. A proje√ß√£o linear de $z_{t+1}$ √© dada por $\hat{z}_{t+1} = E(z_{t+1}X_t')E(X_tX_t')^{-1}X_t$, que tamb√©m pode ser expressa como  $\hat{z}_{t+1} = E(h'Y_{t+1}X_t')E(X_tX_t')^{-1}X_t = h'E(Y_{t+1}X_t')E(X_tX_t')^{-1}X_t$.
III. Como $\alpha' = E(Y_{t+1}X_t')E(X_tX_t')^{-1}$  e $\hat{Y}_{t+1} = \alpha'X_t$, temos $\hat{z}_{t+1} = h'\alpha'X_t = h'\hat{Y}_{t+1}$.
IV. Portanto, a proje√ß√£o linear de $h'Y_{t+1}$ √© $h'\hat{Y}_{t+1}$, que √© a mesma combina√ß√£o linear da proje√ß√£o linear de $Y_{t+1}$. ‚ñ†
> üí° **Exemplo Num√©rico:** Considere $Y_{t+1} = \begin{bmatrix} y_{1,t+1} \\ y_{2,t+1} \end{bmatrix}$ e seja $h = \begin{bmatrix} 2 \\ -1 \end{bmatrix}$. Ent√£o, $h'Y_{t+1} = 2y_{1,t+1} - y_{2,t+1}$. A proje√ß√£o linear de $Y_{t+1}$ √© $\hat{Y}_{t+1} = \alpha' X_t$. O Teorema 2.2 garante que a proje√ß√£o linear de $h'Y_{t+1}$ √© dada por $h'\hat{Y}_{t+1} = 2\hat{y}_{1,t+1} - \hat{y}_{2,t+1}$, que √© a mesma combina√ß√£o linear da proje√ß√£o de $Y_{t+1}$. Isso implica que n√£o precisamos recalcular as proje√ß√µes lineares para combina√ß√µes lineares de $Y_{t+1}$; basta aplicar as mesmas combina√ß√µes lineares √†s proje√ß√µes de $Y_{t+1}$.
>
>  Se tivermos $\hat{Y}_{t+1} = \begin{bmatrix} 0.8 \\ 1.2 \end{bmatrix}$, ent√£o a melhor previs√£o para $h'Y_{t+1}$ √© $2(0.8)-1.2=0.4$, sem a necessidade de um novo c√°lculo. A previs√£o de $h'Y_{t+1}$ √© o preditor linear de m√≠nimo MSE para $h'Y_{t+1}$.
>
> ```mermaid
> graph LR
>     A[Y_{t+1}] -->|Proje√ß√£o Linear| B[hat{Y}_{t+1}]
>     C[h'Y_{t+1}] -->|Proje√ß√£o Linear| D[h'hat{Y}_{t+1}]
>     B --> |h'| D
> ```
> O diagrama ilustra como a proje√ß√£o linear de $h'Y_{t+1}$ √© equivalente a aplicar $h'$ na proje√ß√£o linear de $Y_{t+1}$, sem a necessidade de recalcular a proje√ß√£o linear.

**Teorema 2.3** *A proje√ß√£o linear $\hat{Y}_{t+1} = \alpha'X_t$ √© a melhor aproxima√ß√£o linear no sentido de minimizar a matriz de erro quadr√°tico m√©dio (MSE) para todo vetor $B'X_t$, onde B √© uma matriz de coeficientes de dimens√£o $n \times m$, isto √©, $MSE(\alpha'X_t) \leq MSE(B'X_t)$, onde a desigualdade √© no sentido de matrizes semidefinidas positivas.*

*Prova:*
I. Seja $B$ uma matriz de coeficientes $n \times m$, e considere a previs√£o linear $\tilde{Y}_{t+1} = B'X_t$.
II. O MSE dessa previs√£o √© dado por $MSE(B'X_t) = E[(Y_{t+1} - B'X_t)(Y_{t+1} - B'X_t)']$.
III. Podemos expressar o erro dessa previs√£o como $Y_{t+1} - B'X_t = (Y_{t+1} - \alpha'X_t) + (\alpha'X_t - B'X_t) = e_{t+1} + (\alpha' - B')X_t$.
IV. Substituindo isso na express√£o do MSE, temos:
$MSE(B'X_t) = E[(e_{t+1} + (\alpha' - B')X_t)(e_{t+1} + (\alpha' - B')X_t)'] = E[e_{t+1}e_{t+1}'] + E[(\alpha' - B')X_tX_t'(\alpha' - B')'] + E[e_{t+1}X_t'(\alpha' - B')'] + E[(\alpha' - B')X_te_{t+1}']$.
V. Pela condi√ß√£o de n√£o correla√ß√£o $E[e_{t+1}X_t']=0$, os dois √∫ltimos termos s√£o nulos.
VI. Portanto, $MSE(B'X_t) = E[e_{t+1}e_{t+1}'] + E[(\alpha' - B')X_tX_t'(\alpha' - B')'] = MSE(\alpha'X_t) + E[(\alpha' - B')X_tX_t'(\alpha' - B')']$.
VII. Como $E[(\alpha' - B')X_tX_t'(\alpha' - B')']$ √© uma matriz semidefinida positiva, segue que $MSE(B'X_t) \geq MSE(\alpha'X_t)$, onde a desigualdade √© no sentido de matrizes semidefinidas positivas.
VIII. Isso demonstra que a proje√ß√£o linear $\hat{Y}_{t+1} = \alpha'X_t$ √© a melhor aproxima√ß√£o linear no sentido de minimizar a matriz MSE. ‚ñ†

> üí° **Exemplo Num√©rico:** Para ilustrar o Teorema 2.3, vamos usar os momentos populacionais do exemplo anterior e comparar o MSE da proje√ß√£o √≥tima com uma proje√ß√£o n√£o-√≥tima. Assumimos que a proje√ß√£o √≥tima √© $\alpha' = \begin{bmatrix} 1/2 & 1/2 \\ 1/4 & 3/2 \end{bmatrix}$. Agora, considere uma matriz de coeficientes n√£o-√≥tima, $B' = \begin{bmatrix} 1/3 & 1/3 \\ 1/3 & 1 \end{bmatrix}$.
>
> Temos $MSE(\alpha'X_t) = \begin{bmatrix} 3.5 & 0 \\ 0 & 1.25 \end{bmatrix}$, como calculado anteriormente.  Agora, vamos calcular $MSE(B'X_t)$. Primeiro, vamos calcular $E[(Y_{t+1}-B'X_t)(Y_{t+1}-B'X_t)']$.
>
> $MSE(B'X_t) = E(Y_{t+1}Y_{t+1}') - E(Y_{t+1}X_t')B - B'E(X_tY_{t+1}') + B'E(X_tX_t')B$
>
> $MSE(B'X_t) = \begin{bmatrix} 5 & 2 \\ 2 & 6 \end{bmatrix} - \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix} \begin{bmatrix} 1/3 & 1/3 \\ 1/3 & 1 \end{bmatrix} - \begin{bmatrix} 1/3 & 1/3 \\ 1/3 & 1 \end{bmatrix} \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix} + \begin{bmatrix} 1/3 & 1/3 \\ 1/3 & 1 \end{bmatrix} \begin{bmatrix} 4 & 0 \\ 0 & 2 \end{bmatrix} \begin{bmatrix} 1/3 & 1/3 \\ 1/3 & 1 \end{bmatrix}$
>
> $MSE(B'X_t) = \begin{bmatrix} 5 & 2 \\ 2 & 6 \end{bmatrix} - \begin{bmatrix} 1 & 5/3 \\ 4/3 & 10/3 \end{bmatrix} - \begin{bmatrix} 1 & 4/3 \\ 5/3 & 10/3 \end{bmatrix} + \begin{bmatrix} 4/9 + 2/9 & 4/9+2/3 \\ 4/9+2/3 & 4/9+2 \end{bmatrix} \begin{bmatrix} 1/3 & 1/3 \\ 1/3 & 1 \end{bmatrix} = \begin{bmatrix} 5 & 2 \\ 2 & 6 \end{bmatrix} - \begin{bmatrix} 2 & 3 \\ 3 & 20/3 \end{bmatrix} + \begin{bmatrix} 2/3 & 10/9 \\ 10/9 & 22/9 \end{bmatrix} = \begin{bmatrix} 3.667 & -1 \\ -1 & 2.556 \end{bmatrix}$
>
> Note que a diagonal de $MSE(B'X_t)$ √© maior que a diagonal de $MSE(\alpha'X_t)$. $3.667 > 3.5$ e $2.556>1.25$. Isso ilustra que a proje√ß√£o linear √≥tima $\alpha'$ minimiza o MSE, como garantido pelo Teorema 2.3. A diferen√ßa $MSE(B'X_t) - MSE(\alpha'X_t)$ √© uma matriz semidefinida positiva.

Al√©m disso, a condi√ß√£o de n√£o correla√ß√£o e a express√£o para a matriz de coeficientes de proje√ß√£o, $\alpha'$, garantem que a proje√ß√£o linear √© a melhor aproxima√ß√£o no sentido de minimizar o erro quadr√°tico m√©dio (MSE) da previs√£o, tanto para as vari√°veis individuais quanto para suas combina√ß√µes lineares.

### Conclus√£o
Este cap√≠tulo refor√ßou a compreens√£o da proje√ß√£o linear para vetores, demonstrando que a proje√ß√£o $\hat{Y}_{t+1} = \alpha'X_t$ n√£o apenas fornece a melhor aproxima√ß√£o linear de $Y_{t+1}$ como um todo, mas tamb√©m gera previs√µes de m√≠nimo MSE para cada um dos seus componentes individuais. Adicionalmente, a propriedade de preservar a otimalidade para combina√ß√µes lineares dos elementos de $Y_{t+1}$ demonstra a robustez e a aplicabilidade da proje√ß√£o linear no contexto multivariado. A condi√ß√£o de n√£o correla√ß√£o e a f√≥rmula para $\alpha'$ s√£o cruciais para a derivar estas propriedades de otimalidade e s√£o fundamentos para os modelos econom√©tricos e de s√©ries temporais que usam previs√µes multivariadas.
### Refer√™ncias
[^1]: *[4.1.21] The preceding results can be extended to forecast an (n √ó 1) vector $Y_{t+1}$ on the basis of a linear function of an (m x 1) vector $X_t$: $P(Y_{t+1}|X_t) = \alpha'X_t = \hat{Y}_{t+1}$*
[^2]: *[4.1.22] ...that is, each of the n elements of ($Y_{t+1} - \hat{Y}_{t+1}$) is uncorrelated with each of the m elements of $X_t$.*
[^3]: *[4.1.23] From [4.1.22], the matrix of projection coefficients is given by $\alpha' = [E(Y_{t+1}X_t')] \cdot [E(X_tX_t')]^{-1}$*
[^4]: *[4.1.24] The matrix generalization of the formula for the mean squared error [4.1.15] is $MSE(\alpha'X_t) = E[(Y_{t+1} - \alpha'X_t)(Y_{t+1} - \alpha'X_t)'] = E(Y_{t+1}Y_{t+1}') - [E(Y_{t+1}X_t')] \cdot [E(X_tX_t')]^{-1} \cdot [E(X_tY_{t+1})']$*
[^Teorema1]: *Teorema 1: A matriz de coeficientes de proje√ß√£o Œ±' minimiza o MSE no sentido de que para qualquer outra matriz B (n x m), $MSE(\alpha'X_t) \leq MSE(B'X_t)$, onde a desigualdade √© no sentido de matrizes semidefinidas positivas.*
[^Lema2.1]: *Lema 2.1: A condi√ß√£o de n√£o correla√ß√£o $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$ implica que os res√≠duos da proje√ß√£o linear, $Y_{t+1} - \alpha'X_t$, s√£o ortogonais ao espa√ßo gerado pelas vari√°veis preditoras $X_t$.*
<!-- END -->
