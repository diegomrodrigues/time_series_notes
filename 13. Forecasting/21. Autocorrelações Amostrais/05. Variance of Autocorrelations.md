## Vari√¢ncia dos Estimadores de Autocorrela√ß√µes Amostrais e Testes de Signific√¢ncia

### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise das autocorrela√ß√µes amostrais, focando na deriva√ß√£o de suas vari√¢ncias e no uso dessas vari√¢ncias para realizar testes de signific√¢ncia. Como explorado em cap√≠tulos anteriores, as autocorrela√ß√µes amostrais ($\hat{p}_j$) s√£o estimativas das autocorrela√ß√µes populacionais ($\rho_j$) e s√£o instrumentos importantes na identifica√ß√£o de propriedades de s√©ries temporais [^1, ^4]. No entanto, as autocorrela√ß√µes amostrais s√£o estimativas, e, portanto, sujeitas a variabilidade amostral. Este cap√≠tulo busca formalizar esse conceito, derivando uma express√£o para a vari√¢ncia dessas estimativas e apresentando como essa express√£o pode ser usada para realizar testes de hip√≥teses sobre as autocorrela√ß√µes populacionais. Este conhecimento √© fundamental para a aplica√ß√£o da metodologia de Box-Jenkins e na sele√ß√£o de modelos para s√©ries temporais.

### Deriva√ß√£o da Vari√¢ncia dos Estimadores de Autocorrela√ß√µes Amostrais

Como estabelecido anteriormente, as autocorrela√ß√µes amostrais ($\hat{p}_j$) s√£o estimativas das autocorrela√ß√µes populacionais ($\rho_j$), onde [^4]:

$$
\hat{p}_j = \frac{\hat{\gamma}_j}{\hat{\gamma}_0}
$$

e $\hat{\gamma}_j$ representa a *covari√¢ncia amostral* no lag $j$, que √© dada por [^4]:

$$
\hat{\gamma}_j = \frac{1}{T} \sum_{t=j+1}^{T} (y_t - \bar{y})(y_{t-j} - \bar{y})
$$

onde $\bar{y}$ √© a m√©dia amostral. O objetivo agora √© obter uma express√£o para a vari√¢ncia de $\hat{p}_j$, que nos permitir√° quantificar a incerteza associada a essa estimativa e realizar testes de hip√≥tese. A deriva√ß√£o formal da vari√¢ncia de $\hat{p}_j$ √© complexa e envolve aproxima√ß√µes. Assumindo que a s√©rie temporal segue um processo estacion√°rio, podemos utilizar um resultado assint√≥tico que diz que a vari√¢ncia de $\hat{p}_j$ pode ser aproximada como [^4]:

$$
Var(\hat{p}_j) \approx \frac{1}{T}\left(1 + 2\sum_{i=1}^{j-1} \rho_i^2\right)
$$

Essa express√£o mostra que a vari√¢ncia de $\hat{p}_j$ depende do tamanho da amostra $T$ e das autocorrela√ß√µes populacionais $\rho_i$ para lags menores que $j$. No entanto, na pr√°tica, as autocorrela√ß√µes populacionais s√£o desconhecidas, e a vari√¢ncia √© aproximada usando as autocorrela√ß√µes amostrais como uma substitui√ß√£o. Esta aproxima√ß√£o √© v√°lida sob condi√ß√µes de regularidade, principalmente quando o tamanho da amostra √© grande.

Para o caso especial onde a s√©rie temporal √© um ru√≠do branco, a vari√¢ncia de $\hat{p}_j$ √© dada por:
$$
Var(\hat{p}_j) \approx \frac{1}{T}
$$
Este resultado √© uma simplifica√ß√£o da express√£o geral, considerando que todas as autocorrela√ß√µes populacionais $\rho_i$ s√£o iguais a zero para ru√≠do branco quando $i \ne 0$. No caso em que $j=0$, $\hat{p}_0=1$ e, portanto, a sua vari√¢ncia √© zero.

> üí° **Exemplo Num√©rico:** Suponha que temos uma s√©rie temporal de ru√≠do branco com $T=100$ observa√ß√µes. Usando a express√£o aproximada, temos que a vari√¢ncia da autocorrela√ß√£o amostral para qualquer lag $j \ne 0$ √© dada por:
> $$Var(\hat{p}_j) \approx \frac{1}{100} = 0.01$$
> O desvio padr√£o seria a raiz quadrada da vari√¢ncia, ou seja $\sqrt{0.01} = 0.1$. Para um intervalo de confian√ßa de 95%, o intervalo para $\hat{p}_j$ seria aproximadamente $\pm 2 \times 0.1 = \pm 0.2$. Isso significa que, sob a hip√≥tese nula de que a s√©rie temporal √© um ru√≠do branco, esperamos que 95% das autocorrela√ß√µes amostrais estejam entre -0.2 e 0.2. √â importante ressaltar que a deriva√ß√£o da vari√¢ncia √© uma aproxima√ß√£o, e sua validade se torna maior conforme o tamanho da amostra cresce.
>
> Suponha agora que a s√©rie temporal n√£o √© ru√≠do branco, e que a autocorrela√ß√£o populacional no lag 1 √© $\rho_1 = 0.5$.  Para o lag 2, a vari√¢ncia de $\hat{p}_2$ seria aproximadamente:
>
> $$Var(\hat{p}_2) \approx \frac{1}{100}\left(1 + 2\rho_1^2\right) = \frac{1}{100}\left(1 + 2(0.5)^2\right) = \frac{1}{100}(1 + 0.5) = 0.015$$
>
> O desvio padr√£o de $\hat{p}_2$ seria $\sqrt{0.015} \approx 0.122$. Para um intervalo de confian√ßa de 95%, ter√≠amos $\pm 2 \times 0.122 \approx \pm 0.244$. Ou seja, para um processo que n√£o √© ru√≠do branco, a vari√¢ncia das autocorrela√ß√µes amostrais pode ser maior, dependendo das autocorrela√ß√µes populacionais.

**Lema 2:** *Sob condi√ß√µes de regularidade, as autocorrela√ß√µes amostrais $\hat{p}_j$ s√£o assintoticamente n√£o enviesadas.*
*Proof:* Como visto em [^Proposi√ß√£o 1] e [^Proposi√ß√£o 1.1], os estimadores de autocovari√¢ncias $\hat{\gamma}_j$ s√£o enviesados, mas para amostras grandes, este vi√©s se torna desprez√≠vel. Dado que a autocorrela√ß√£o amostral $\hat{p}_j$ √© uma fun√ß√£o da autocovari√¢ncia amostral $\hat{\gamma}_j$, o vi√©s de $\hat{p}_j$ tamb√©m tende a zero quando o tamanho da amostra tende ao infinito. Uma demonstra√ß√£o formal deste resultado requer a aplica√ß√£o de resultados de converg√™ncia assint√≥tica e est√° al√©m do escopo deste texto. Contudo, o resultado principal √© que para amostras grandes, a m√©dia amostral $\bar{y}$ converge para a m√©dia populacional $\mu$ e a autocovari√¢ncia amostral $\hat{\gamma}_j$ converge para a autocovari√¢ncia populacional $\gamma_j$, e, portanto, as autocorrela√ß√µes amostrais $\hat{p}_j$ convergem para as autocorrela√ß√µes populacionais $\rho_j$.

**Lema 2.1:** *Sob condi√ß√µes de regularidade, e se o processo for um ru√≠do branco, a autocorrela√ß√£o amostral $\hat{p}_j$ para $j \neq 0$  √© assintoticamente n√£o correlacionada com $\hat{p}_k$ para $k \neq j$ e $k \neq 0$.*

*Proof:*
I.  Pela defini√ß√£o, as autocorrela√ß√µes amostrais $\hat{p}_j$ e $\hat{p}_k$ s√£o dadas por $\hat{p}_j = \frac{\hat{\gamma}_j}{\hat{\gamma}_0}$ e $\hat{p}_k = \frac{\hat{\gamma}_k}{\hat{\gamma}_0}$, respectivamente.
II. Para um processo de ru√≠do branco, as autocovari√¢ncias populacionais $\gamma_j$ s√£o zero para $j \neq 0$.
III. As autocovari√¢ncias amostrais $\hat{\gamma}_j$ e $\hat{\gamma}_k$ s√£o estimativas das autocovari√¢ncias populacionais correspondentes.
IV.  Sob condi√ß√µes de regularidade e para grandes amostras, as autocovari√¢ncias amostrais de um processo de ru√≠do branco s√£o assintoticamente n√£o correlacionadas, ou seja, $Cov(\hat{\gamma}_j, \hat{\gamma}_k) \approx 0$ para $j \neq k$.
V. Dado que $\hat{p}_j$ e $\hat{p}_k$ s√£o fun√ß√µes de $\hat{\gamma}_j$ e $\hat{\gamma}_k$, respectivamente, e que $\hat{\gamma}_0$ √© comum, e como as autocovari√¢ncias amostrais s√£o assintoticamente n√£o correlacionadas, as autocorrela√ß√µes amostrais $\hat{p}_j$ e $\hat{p}_k$ s√£o assintoticamente n√£o correlacionadas, i.e.,  $Cov(\hat{p}_j, \hat{p}_k) \approx 0$ para $j \neq k$.
VI. Portanto, para um processo de ru√≠do branco, as autocorrela√ß√µes amostrais em lags diferentes s√£o assintoticamente n√£o correlacionadas.
‚ñ†

### Testes de Signific√¢ncia para Autocorrela√ß√µes Amostrais

A vari√¢ncia dos estimadores das autocorrela√ß√µes amostrais desempenha um papel importante na realiza√ß√£o de testes de signific√¢ncia. O objetivo principal desses testes √© determinar se os valores das autocorrela√ß√µes amostrais s√£o estatisticamente diferentes de zero. Para isso, formulamos a hip√≥tese nula de que a autocorrela√ß√£o populacional √© igual a zero ($\rho_j = 0$) e verificamos se h√° evid√™ncias, baseadas na autocorrela√ß√£o amostral $\hat{p}_j$, para rejeitar essa hip√≥tese nula.
Em geral, assumindo que a s√©rie temporal √© um ru√≠do branco gaussiano, os testes de hip√≥tese s√£o realizados usando o resultado da distribui√ß√£o assint√≥tica para as autocorrela√ß√µes amostrais, onde a estat√≠stica de teste para a hip√≥tese nula $\rho_j=0$ para qualquer $j \neq 0$ √© dada por:

$$
Z = \frac{\hat{p}_j}{\sqrt{\frac{1}{T}}} = \hat{p}_j\sqrt{T}
$$

Onde Z segue uma distribui√ß√£o normal padr√£o assintoticamente. A hip√≥tese nula ser√° rejeitada se $|Z| > Z_{\alpha/2}$ onde $Z_{\alpha/2}$ √© o valor cr√≠tico para um teste bicaudal com n√≠vel de signific√¢ncia $\alpha$. Por exemplo, para um teste com n√≠vel de signific√¢ncia de 5%, $Z_{0.025} \approx 1.96$.

> üí° **Exemplo Num√©rico:** Vamos considerar a mesma s√©rie temporal de ru√≠do branco com $T=100$ observa√ß√µes. Suponha que calculamos a autocorrela√ß√£o amostral para o lag $j=1$ e obtivemos $\hat{p}_1 = 0.18$. Para realizar o teste de hip√≥tese, calculamos a estat√≠stica $Z$:
>
> $$Z = 0.18 \times \sqrt{100} = 1.8$$
>
> Como $|Z| = 1.8 < 1.96$, n√£o podemos rejeitar a hip√≥tese nula de que a autocorrela√ß√£o populacional no lag 1 √© zero com n√≠vel de signific√¢ncia de 5%. Isso significa que, apesar de $\hat{p}_1$ n√£o ser igual a zero, o desvio da hip√≥tese nula √© pequeno o suficiente para n√£o ser estatisticamente significante. Se, por outro lado, tiv√©ssemos observado um valor de $\hat{p}_1 = 0.3$, o valor da estat√≠stica de teste seria $Z = 0.3 \times \sqrt{100} = 3$, o que nos levaria a rejeitar a hip√≥tese nula, dado que o valor absoluto √© maior que 1.96, indicando que o valor de $\hat{p}_1$ √© estatisticamente significante.
>
> Vamos usar um exemplo diferente, onde temos uma s√©rie temporal de $T=200$ e observamos $\hat{p}_1 = 0.05$ e $\hat{p}_2 = -0.15$. Para $\hat{p}_1$, temos:
> $$Z_1 = 0.05 \times \sqrt{200} \approx 0.707$$
> Como $|Z_1| < 1.96$, n√£o rejeitamos a hip√≥tese nula de que $\rho_1 = 0$. Para $\hat{p}_2$:
> $$Z_2 = -0.15 \times \sqrt{200} \approx -2.121$$
> Como $|Z_2| > 1.96$, rejeitamos a hip√≥tese nula de que $\rho_2 = 0$. Isso significa que, com n√≠vel de signific√¢ncia de 5%, a autocorrela√ß√£o no lag 2 √© estatisticamente diferente de zero.

√â importante notar que o uso do intervalo de confian√ßa e testes de hip√≥teses com as autocorrela√ß√µes amostrais devem ser feitos com cautela, uma vez que os estimadores para diferentes lags n√£o s√£o independentes. Caso a amostra n√£o seja suficientemente grande, ou o processo n√£o seja bem comportado, os resultados desses testes podem ser enganosos.

**Teorema 3:** *Sob as condi√ß√µes de regularidade e se a s√©rie temporal for um ru√≠do branco gaussiano, as autocorrela√ß√µes amostrais $\hat{p}_j$ para $j=1,2,...$ s√£o assintoticamente independentes e distribu√≠das como $N(0, 1/T)$.*

*Proof:*
I. Pelo Lema 2.1, para um processo de ru√≠do branco, as autocorrela√ß√µes amostrais $\hat{p}_j$ e $\hat{p}_k$ s√£o assintoticamente n√£o correlacionadas para $j \neq k$.
II. Para um processo de ru√≠do branco gaussiano, cada observa√ß√£o da s√©rie temporal √© independente e normalmente distribu√≠da.
III. A autocorrela√ß√£o amostral $\hat{p}_j$ √© uma fun√ß√£o da autocovari√¢ncia amostral $\hat{\gamma}_j$, que √© uma soma de termos independentes.
IV.  Pelo Teorema Central do Limite, a distribui√ß√£o da autocorrela√ß√£o amostral $\hat{p}_j$ converge para uma distribui√ß√£o normal conforme o tamanho da amostra $T$ tende ao infinito.
V. Dado que a esperan√ßa da autocorrela√ß√£o amostral √© aproximadamente zero (Lema 2) e sua vari√¢ncia √© aproximadamente $1/T$ para ru√≠do branco, a distribui√ß√£o assint√≥tica de $\hat{p}_j$ √© normal com m√©dia 0 e vari√¢ncia $1/T$, i.e., $\hat{p}_j \sim N(0, 1/T)$.
VI. Como as autocorrela√ß√µes amostrais em diferentes lags s√£o assintoticamente n√£o correlacionadas e a distribui√ß√£o de cada uma √© assintoticamente normal, as autocorrela√ß√µes amostrais $\hat{p}_j$ para $j=1,2,...$ s√£o assintoticamente independentes e distribu√≠das como $N(0, 1/T)$.
‚ñ†

**Corol√°rio 3.1:** *Sob as condi√ß√µes do Teorema 3, um teste conjunto de hip√≥tese de que as primeiras $k$ autocorrela√ß√µes s√£o simultaneamente zero pode ser realizado utilizando a estat√≠stica de Ljung-Box, que √© definida como:*

$$Q = T(T+2)\sum_{j=1}^{k} \frac{\hat{p}_j^2}{T-j}$$

*E que, assintoticamente, segue uma distribui√ß√£o $\chi^2$ com $k$ graus de liberdade.*
*Proof:*
I. Do Teorema 3, sabemos que sob a hip√≥tese nula de que a s√©rie temporal √© um ru√≠do branco gaussiano, as autocorrela√ß√µes amostrais $\hat{p}_j$ s√£o assintoticamente independentes e distribu√≠das como $N(0, 1/T)$.
II. A estat√≠stica $Z_j = \hat{p}_j \sqrt{T}$ segue uma distribui√ß√£o normal padr√£o assintoticamente, ou seja, $Z_j \sim N(0,1)$.
III.  O quadrado de uma vari√°vel aleat√≥ria com distribui√ß√£o normal padr√£o, $Z_j^2 = T\hat{p}_j^2$, segue uma distribui√ß√£o qui-quadrado com 1 grau de liberdade, i.e., $T\hat{p}_j^2 \sim \chi^2(1)$.
IV. A soma de $k$ vari√°veis aleat√≥rias independentes que seguem uma distribui√ß√£o qui-quadrado com 1 grau de liberdade, segue uma distribui√ß√£o qui-quadrado com $k$ graus de liberdade, i.e., $\sum_{j=1}^{k} T\hat{p}_j^2 \sim \chi^2(k)$.
V.  A estat√≠stica de Ljung-Box √© dada por $Q = T(T+2)\sum_{j=1}^{k} \frac{\hat{p}_j^2}{T-j}$. Para grandes amostras, $\frac{T+2}{T-j} \approx 1$, ent√£o podemos aproximar $Q$ por $Q \approx T\sum_{j=1}^{k} \hat{p}_j^2$.
VI.  Portanto, sob a hip√≥tese nula, a estat√≠stica de Ljung-Box, $Q$, √© assintoticamente distribu√≠da como uma qui-quadrado com $k$ graus de liberdade.
‚ñ†
> üí° **Exemplo Num√©rico:** Vamos supor que temos uma s√©rie temporal de $T=150$ e calculamos as autocorrela√ß√µes amostrais para os primeiros 3 lags: $\hat{p}_1 = 0.12$, $\hat{p}_2 = 0.08$ e $\hat{p}_3 = -0.10$. Queremos testar a hip√≥tese conjunta de que todas as tr√™s autocorrela√ß√µes s√£o zero. Usamos a estat√≠stica de Ljung-Box com $k=3$:
>
> $$Q = 150(150+2)\left(\frac{0.12^2}{150-1} + \frac{0.08^2}{150-2} + \frac{(-0.10)^2}{150-3}\right)$$
> $$Q = 150(152)\left(\frac{0.0144}{149} + \frac{0.0064}{148} + \frac{0.01}{147}\right)$$
> $$Q \approx 22800(0.0000966 + 0.0000432 + 0.000068) = 22800 \times 0.0002078 \approx 4.74$$
>
> Com $k=3$ graus de liberdade, o valor cr√≠tico para um teste com 5% de signific√¢ncia √© $\chi^2_{0.05,3} \approx 7.81$. Como $4.74 < 7.81$, n√£o rejeitamos a hip√≥tese nula de que as primeiras 3 autocorrela√ß√µes s√£o conjuntamente iguais a zero.  Se o valor de $Q$ fosse maior que 7.81, rejeitar√≠amos a hip√≥tese nula, indicando que pelo menos uma das tr√™s autocorrela√ß√µes √© estatisticamente diferente de zero.

### Conclus√£o

A deriva√ß√£o da vari√¢ncia dos estimadores de autocorrela√ß√µes amostrais e o uso de testes de signific√¢ncia s√£o passos importantes na an√°lise de s√©ries temporais. Esses conceitos nos permitem quantificar a incerteza associada √†s estimativas amostrais e determinar se elas s√£o estatisticamente diferentes de zero, o que auxilia na identifica√ß√£o de padr√µes em s√©ries temporais e na escolha de modelos adequados. √â fundamental reconhecer as limita√ß√µes dessas ferramentas, principalmente a influ√™ncia do tamanho da amostra e as aproxima√ß√µes envolvidas na deriva√ß√£o das vari√¢ncias, e interpretar os resultados de forma cautelosa. Os conceitos desenvolvidos nesta se√ß√£o ser√£o usados para realizar a identifica√ß√£o do modelo nas pr√≥ximas se√ß√µes da metodologia de Box-Jenkins.
### Refer√™ncias
[^1]: Express√£o [4.1.1] √© conhecida como o erro quadr√°tico m√©dio associado √† previs√£o.
[^4]: Express√£o [4.8.6]
[^Proposi√ß√£o 1]: Proposi√ß√£o 1 no cap√≠tulo anterior
[^Proposi√ß√£o 1.1]: Proposi√ß√£o 1.1 no cap√≠tulo anterior
<!-- END -->
