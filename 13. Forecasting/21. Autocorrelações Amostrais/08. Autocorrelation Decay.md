## Decaimento das Autocorrelações Amostrais: Distinção entre Processos MA e AR

### Introdução

Este capítulo aprofunda a discussão sobre a análise de autocorrelações amostrais, com foco na distinção entre processos de *Moving Average* (MA) e *Autoregressive* (AR) através do padrão de decaimento das autocorrelações amostrais. Como visto anteriormente, as autocorrelações amostrais ($ \hat{p}_j$) são estimativas das autocorrelações populacionais ($ \rho_j$) e desempenham um papel crucial na análise de séries temporais [^1, ^4]. Em capítulos anteriores, foi introduzido como as autocorrelações amostrais podem ser usadas para verificar se uma série temporal é um ruído branco. Em seguida, demonstramos como a variância das autocorrelações amostrais pode ser calculada, e como usar testes de hipótese para verificar se os valores das autocorrelações são estatisticamente diferentes de zero. Agora, vamos focar em como os padrões de decaimento dessas autocorrelações amostrais podem nos guiar na identificação da ordem de um processo MA ou AR. Este conhecimento é fundamental para a aplicação da metodologia Box-Jenkins.

### Decaimento das Autocorrelações Amostrais em Processos MA

Em um processo *Moving Average* de ordem $q$ (MA(q)), o valor atual da série temporal é uma combinação linear de *shocks* (ruídos brancos) atuais e passados, até um atraso $q$. Matematicamente, um processo MA(q) pode ser representado como:

$$
Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \ldots + \theta_q \epsilon_{t-q}
$$

onde $\mu$ é a média da série, $\epsilon_t$ é um ruído branco com média zero e variância $\sigma^2$, e $\theta_i$ são os parâmetros do modelo.

Uma característica fundamental dos processos MA(q) é que a autocorrelação populacional $\rho_j$ é zero para todos os lags $j > q$. Isso ocorre porque, no modelo MA(q), o valor atual $Y_t$ não é afetado por *shocks* ocorridos mais de $q$ períodos atrás. Consequentemente, as autocorrelações amostrais $\hat{p}_j$ para $j > q$ devem ser aproximadamente zero (dentro do erro amostral). O gráfico das autocorrelações amostrais ($\hat{p}_j$ em função de $j$) mostrará, portanto, um corte abrupto, ou seja, as autocorrelações serão significativamente diferentes de zero para os lags $j \leq q$, e aproximadamente zero para $j > q$.

> 💡 **Exemplo Numérico:** Suponha que temos um processo MA(2) definido por $Y_t = 0.5 + \epsilon_t + 0.8\epsilon_{t-1} -0.4\epsilon_{t-2}$, onde $\epsilon_t$ é um ruído branco com $\sigma^2 = 1$. As autocorrelações populacionais $\rho_j$ são: $\rho_0=1$, $\rho_1 = \frac{\theta_1 + \theta_2 \theta_1}{1 + \theta_1^2 + \theta_2^2} = \frac{0.8 + (0.8)(-0.4)}{1 + 0.8^2 + (-0.4)^2} = \frac{0.48}{1.8} \approx 0.2667$, $\rho_2 = \frac{\theta_2}{1 + \theta_1^2 + \theta_2^2} = \frac{-0.4}{1.8} \approx -0.2222$, e $\rho_j = 0$ para $j > 2$. Ao gerar uma série temporal com este modelo e calcular as autocorrelações amostrais $\hat{p}_j$, esperamos observar valores significativos para $\hat{p}_1$ e $\hat{p}_2$, e valores próximos de zero para lags maiores que 2. Para uma amostra de tamanho T=100, simulamos uma série temporal usando Python e calculamos as autocorrelações amostrais:
>
> ```python
> import numpy as np
> import statsmodels.api as sm
>
> np.random.seed(42)
> T = 100
> errors = np.random.normal(0, 1, T+2) #Generate errors for T+2 time periods for lags
> y = 0.5 + errors[2:] + 0.8 * errors[1:-1] - 0.4 * errors[:-2] #Generate Y_t values for the MA(2) process
> acf_values = sm.tsa.acf(y, nlags=5) #Calculate ACF up to lag 5
> print(f"Sample ACF: {acf_values}")
> ```
>
> A saída do código acima será algo próximo de: `Sample ACF: [1.         0.21253674 -0.20418247 -0.13000312  0.08636215 -0.03606612]`. Note que os valores são diferentes dos valores populacionais, o que é esperado devido ao erro amostral. Para verificar se os valores são estatisticamente significativos, podemos calcular os intervalos de confiança. Usando a fórmula para a variância de $\hat{p}_j$ para $j > q$, e assumindo que para $j>2$ a variância é aproximadamente $1/T$, temos que o desvio padrão é $\sqrt{1/100} = 0.1$. Considerando um nível de significância de 5%, o intervalo de confiança para $\hat{p}_j$ é dado por $\pm 2*0.1 = \pm 0.2$. Portanto, os valores amostrais para lags maiores que 2 não são estatisticamente significativos, como esperado para um processo MA(2).
>

É importante notar que, na prática, o “corte” nas autocorrelações amostrais pode não ser tão abrupto como na teoria, devido ao erro amostral. Como foi visto em capítulos anteriores, as autocorrelações amostrais são estimativas, e estão sujeitas a erro amostral [^1, ^4, ^Lema 2.1]. Portanto, mesmo que a autocorrelação populacional seja zero, as amostras podem mostrar valores diferentes de zero. A variância de $\hat{p}_j$ é dada aproximadamente por [^4]:

$$
Var(\hat{p}_j) \approx \frac{1}{T}\left(1 + 2\sum_{i=1}^{j-1} \rho_i^2\right)
$$
Para o caso específico do MA(q), esta variância, para $j > q$, torna-se:
$$
Var(\hat{p}_j) \approx \frac{1}{T}\left(1 + 2\sum_{i=1}^{q} \rho_i^2\right)
$$

Onde $\rho_i$ são as autocorrelações populacionais para $i \leq q$ [^Lema 1]. Este resultado mostra que a variância dos estimadores de autocorrelação amostral depende do tamanho da amostra e das autocorrelações populacionais, e é usado para calcular os intervalos de confiança em torno da estimativa $\hat{p}_j$.
**Lema 1.1** Para um processo MA(q), as autocorrelações populacionais $\rho_j$ podem ser calculadas usando os parâmetros $\theta_i$ do modelo.

*Proof*:
I. Considere um processo MA(q): $Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \ldots + \theta_q \epsilon_{t-q}$.
II.  A autocovariância no lag $j$ é definida como $\gamma_j = Cov(Y_t, Y_{t-j}) = E[(Y_t - \mu)(Y_{t-j} - \mu)]$.
III.  Substituindo a expressão de $Y_t$ na autocovariância, temos:

$\gamma_j = E\left[ \left( \sum_{i=0}^{q} \theta_i \epsilon_{t-i} \right) \left( \sum_{k=0}^{q} \theta_k \epsilon_{t-j-k} \right) \right]$, onde $\theta_0 = 1$.

IV.  Expandindo a expressão, notamos que $E[\epsilon_{t-i}\epsilon_{t-j-k}] = 0$ para $i \neq j+k$ e $E[\epsilon_{t-i}^2] = \sigma^2$. Portanto, a autocovariância $\gamma_j$ é dada por:

$\gamma_j = \sigma^2 \sum_{i=0}^{q-j} \theta_i \theta_{i+j}$ para $0 \leq j \leq q$, e $\gamma_j = 0$ para $j>q$.

V. A autocorrelação é $\rho_j = \frac{\gamma_j}{\gamma_0}$. Portanto, $\rho_j = \frac{\sum_{i=0}^{q-j} \theta_i \theta_{i+j}}{\sum_{i=0}^q \theta_i^2}$ para $0 \leq j \leq q$, e $\rho_j = 0$ para $j>q$.
■
Este lema estabelece uma ligação direta entre os parâmetros do modelo MA(q) e as suas autocorrelações populacionais. Isso é útil para calcular as autocorrelações teóricas de um processo MA(q) e comparar com as estimativas amostrais.

### Decaimento das Autocorrelações Amostrais em Processos AR

Em um processo *Autoregressive* de ordem $p$ (AR(p)), o valor atual da série temporal é uma combinação linear de seus valores passados, até um atraso $p$, mais um *shock* aleatório. Matematicamente, um processo AR(p) pode ser representado como:

$$
Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t
$$

onde $c$ é uma constante, $\epsilon_t$ é um ruído branco com média zero e variância $\sigma^2$, e $\phi_i$ são os parâmetros do modelo.

Ao contrário dos processos MA(q), as autocorrelações populacionais $\rho_j$ em um processo AR(p) não são zero após um lag específico. Em vez disso, as autocorrelações amostrais $\hat{p}_j$ em um processo AR(p) tendem a decair gradualmente em direção a zero, como uma mistura de exponenciais ou senoides amortecidas [^4]. Essa propriedade decorre da natureza recursiva dos processos AR, em que cada valor da série depende dos valores anteriores. Em outras palavras, o valor atual da série é influenciado por valores passados e, por consequência, os lags também estão relacionados. O padrão de decaimento das autocorrelações depende dos valores dos parâmetros $\phi_i$, e pode assumir diversas formas, desde um decaimento rápido a um decaimento mais lento e ondulatório.

> 💡 **Exemplo Numérico:** Considere um processo AR(1) definido por $Y_t = 0.8Y_{t-1} + \epsilon_t$. As autocorrelações populacionais $\rho_j$ decairão exponencialmente, onde $\rho_1 = 0.8$, $\rho_2 = (0.8)^2 = 0.64$, $\rho_3 = (0.8)^3 = 0.512$, e assim por diante. As autocorrelações amostrais $\hat{p}_j$ seguirão esse mesmo padrão, mostrando um decaimento gradual em direção a zero conforme o lag $j$ aumenta.
>
> Para uma série simulada com T = 100, podemos gerar os dados e calcular a ACF utilizando o seguinte código:
> ```python
> import numpy as np
> import statsmodels.api as sm
>
> np.random.seed(42)
> T = 100
> errors = np.random.normal(0, 1, T+100) #Generate errors for more periods to avoid initial conditions effect
> y = np.zeros(T+100) #Initialize vector
> for t in range(1,T+100):
>    y[t] = 0.8 * y[t-1] + errors[t] #Generate Y_t values for the AR(1) process
> acf_values = sm.tsa.acf(y[100:], nlags=5) #Calculate ACF up to lag 5, discarding the initial 100 periods
> print(f"Sample ACF: {acf_values}")
> ```
> A saída deste código será algo como: `Sample ACF: [1.         0.79944224 0.64965565 0.51488569 0.43755624 0.34490762]`.
>
> Para um processo AR(2) definido por $Y_t = 0.8Y_{t-1} - 0.2Y_{t-2} + \epsilon_t$, as autocorrelações populacionais podem ter um decaimento ondulatório, refletindo a interação dos dois lags no processo. Nesse caso, podemos observar valores negativos e positivos para autocorrelações de diferentes lags. As autocorrelações amostrais $\hat{p}_j$ devem refletir esse padrão de decaimento ondulatório. Podemos simular uma série temporal usando o seguinte código:
>
> ```python
> import numpy as np
> import statsmodels.api as sm
>
> np.random.seed(42)
> T = 100
> errors = np.random.normal(0, 1, T+100) #Generate errors
> y = np.zeros(T+100) #Initialize vector
> for t in range(2,T+100):
>    y[t] = 0.8 * y[t-1] - 0.2 * y[t-2] + errors[t] #Generate Y_t values for the AR(2) process
> acf_values = sm.tsa.acf(y[100:], nlags=5) #Calculate ACF up to lag 5, discarding the initial 100 periods
> print(f"Sample ACF: {acf_values}")
> ```
>
> A saída desse código será algo como: `Sample ACF: [ 1.          0.74557114  0.42945176  0.13994168 -0.09896664 -0.24811966]`.  Notamos que o padrão de decaimento é mais complexo do que o decaimento exponencial do AR(1), e contém valores negativos como esperado.

Como foi discutido nos capítulos anteriores, para o caso especial de ruído branco, as autocorrelações populacionais são zero para todos os lags diferentes de zero, e, portanto, tanto a autocorrelação amostral quanto a sua variância são iguais a zero. Os intervalos de confiança das autocorrelações amostrais são então calculados como $\pm 2/\sqrt{T}$, para um nível de significância de 5% [^4].
### Distinção entre Processos MA e AR com Autocorrelações Amostrais
O padrão de decaimento das autocorrelações amostrais fornece um meio prático para distinguir entre processos MA e AR:
*   **Processos MA(q):** Apresentam um corte abrupto nas autocorrelações amostrais, ou seja, as autocorrelações são significativamente diferentes de zero para lags $j \leq q$, e aproximadamente zero para $j > q$.
*   **Processos AR(p):** As autocorrelações amostrais decaem gradualmente em direção a zero, seguindo um padrão exponencial ou sinusoidal amortecido, dependendo dos parâmetros do modelo.

Essa diferença nos padrões de decaimento pode ser usada como um guia para a identificação da ordem de processos MA ou AR, no contexto da metodologia de Box-Jenkins.

É importante enfatizar que a análise do decaimento das autocorrelações amostrais não é um processo exato e está sujeita a interpretação. O erro amostral pode fazer com que autocorrelações amostrais pareçam ser significativamente diferentes de zero quando não são, ou que pareçam decair de forma mais rápida ou mais lenta do que o esperado. Além disso, a forma com que os intervalos de confiança são calculados pode influenciar a decisão sobre quais valores devem ser considerados significativos.

Em particular, um problema comum é a dificuldade de distinguir entre um processo AR e um processo MA de alta ordem. Por exemplo, as autocorrelações de um processo MA(q) podem não ser iguais a zero para lags maiores do que $q$ devido ao erro amostral. Nesse cenário, a análise das autocorrelações parciais pode auxiliar na identificação da ordem do modelo, uma vez que a autocorrelação parcial tem como objetivo medir a correlação entre duas variáveis após remover a influência de variáveis intermediárias.

**Proposição 1**: O decaimento exponencial das autocorrelações amostrais para um processo AR(1) é diretamente proporcional ao parâmetro autoregressivo $\phi$, onde a autocorrelação no lag $j$ é $\rho_j = \phi^j$.

*Prova*:
I. Um processo AR(1) é definido como $Y_t = \phi Y_{t-1} + \epsilon_t$. A autocorrelação no lag 1 é definida como $\rho_1 = \frac{Cov(Y_t, Y_{t-1})}{Var(Y_t)}$.
II. Multiplicando a definição do processo AR(1) por $Y_{t-1}$ e tomando a esperança, temos $E[Y_t Y_{t-1}] = \phi E[Y_{t-1}^2] + E[\epsilon_t Y_{t-1}]$.
III. Como $\epsilon_t$ é um ruído branco e não correlacionado com os valores passados de $Y_t$, temos $E[\epsilon_t Y_{t-1}] = 0$. Assim, $Cov(Y_t, Y_{t-1}) = E[Y_t Y_{t-1}] = \phi E[Y_{t-1}^2] = \phi Var(Y_t)$.
IV. Portanto, $\rho_1 = \frac{Cov(Y_t, Y_{t-1})}{Var(Y_t)} = \frac{\phi Var(Y_t)}{Var(Y_t)} = \phi$.
V. De maneira similar, multiplicando a equação do AR(1) por $Y_{t-j}$ e tomando a esperança, temos $E[Y_t Y_{t-j}] = \phi E[Y_{t-1} Y_{t-j}] + E[\epsilon_t Y_{t-j}]$. Para $j>1$, temos que $E[\epsilon_t Y_{t-j}] = 0$ e $Cov(Y_t, Y_{t-j}) = \phi Cov(Y_{t-1}, Y_{t-j}) = \phi \gamma_{j-1}$, onde $\gamma_k$ é a autocovariância no lag $k$.
VI. Dividindo pelo a variância de $Y_t$ temos que $\rho_j = \phi \rho_{j-1}$. Portanto, $\rho_j = \phi^j$.
VII. O resultado para a autocorrelação amostral segue que, para grandes amostras, $\hat{\rho}_j \approx \rho_j$.
■
**Teorema 1.1** A autocorrelação amostral $\hat{p}_j$ de um processo AR(1) converge em probabilidade para $\phi^j$ quando o tamanho da amostra tende ao infinito.

*Proof:*
I. Pela proposição 1, sabemos que a autocorrelação populacional de um processo AR(1) é dada por $\rho_j = \phi^j$.
II. A autocorrelação amostral $\hat{p}_j$ é um estimador consistente da autocorrelação populacional $\rho_j$.
III. Por definição, um estimador consistente converge em probabilidade para o valor do parâmetro que ele está estimando.
IV. Portanto, $\hat{p}_j \xrightarrow{p} \rho_j$ quando o tamanho da amostra tende ao infinito, onde $\xrightarrow{p}$ indica convergência em probabilidade.
V.  Assim, $\hat{p}_j \xrightarrow{p} \phi^j$ quando o tamanho da amostra tende ao infinito.
■
Este teorema formaliza a ideia de que, para amostras grandes, as autocorrelações amostrais de um processo AR(1) seguem o padrão de decaimento exponencial da autocorrelação populacional.

**Proposição 2**: Para um processo MA(q), a autocorrelação amostral $\hat{p}_j$ é assintoticamente zero para $j>q$.

*Proof:*
I. Um processo MA(q) é definido como $Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \ldots + \theta_q \epsilon_{t-q}$
II. As autocorrelações populacionais $\rho_j$ de um processo MA(q) são iguais a zero para lags $j>q$, dado que não há dependência entre valores separados por mais de $q$ lags.
III. As autocorrelações amostrais são estimativas das autocorrelações populacionais. Portanto, $\hat{p}_j \rightarrow \rho_j$ quando o tamanho da amostra tende ao infinito.
IV. Dado que $\rho_j = 0$ para $j>q$, temos que $\hat{p}_j \rightarrow 0$ quando o tamanho da amostra tende ao infinito.
■
### Conclusão

O decaimento das autocorrelações amostrais, com o corte abrupto para modelos MA(q) e decaimento gradual para modelos AR(p), é uma ferramenta fundamental para a análise de séries temporais. Embora as autocorrelações amostrais sejam estimativas sujeitas a erros, a análise do padrão do seu decaimento juntamente com as autocorrelações parciais, o uso de testes de hipóteses, o conhecimento da variância, e a parcimônia são ferramentas valiosas na seleção de modelos adequados para previsão e análise de séries temporais. No próximo capítulo, usaremos esse conhecimento no contexto da função de verossimilhança para estimar os parâmetros de processos ARMA.
### Referências
[^1]: Expressão [4.1.1] é conhecida como o erro quadrático médio associado à previsão.
[^4]: Expressão [4.8.6]
[^Lema 1]: Lema 1 no capítulo anterior
[^Lema 2.1]: Lema 2.1 no capítulo anterior
<!-- END -->
