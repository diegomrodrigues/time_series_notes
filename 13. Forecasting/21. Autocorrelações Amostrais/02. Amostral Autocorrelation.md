## Autocorrela√ß√µes Amostrais e Identifica√ß√£o de Modelos
### Introdu√ß√£o
Este cap√≠tulo explora o conceito de autocorrela√ß√µes amostrais como estimativas das autocorrela√ß√µes populacionais, destacando seu papel crucial na identifica√ß√£o de propriedades de s√©ries temporais, especificamente na distin√ß√£o entre processos de *Moving Average* (MA) e *Autoregressive* (AR). Expandindo o conceito apresentado em se√ß√µes anteriores sobre a import√¢ncia das autocorrela√ß√µes populacionais na caracteriza√ß√£o de processos estoc√°sticos, focaremos agora em como estim√°-las a partir de dados observados e como estas estimativas podem nos guiar na sele√ß√£o de modelos adequados para a s√©rie temporal em an√°lise [^1].

### Conceitos Fundamentais
As autocorrela√ß√µes amostrais s√£o, em ess√™ncia, estimativas das autocorrela√ß√µes populacionais calculadas a partir de um conjunto de dados finito. Em outras palavras, enquanto as autocorrela√ß√µes populacionais ($p_j$) descrevem as rela√ß√µes de depend√™ncia te√≥rica entre observa√ß√µes em diferentes momentos no tempo, as autocorrela√ß√µes amostrais ($ \hat{p}_j$) s√£o uma aproxima√ß√£o dessas rela√ß√µes baseadas em dados observados [^1]. A formula para o c√°lculo da autocorrela√ß√£o amostral  $\hat{p}_j$ no lag $j$ √© dada por [^4]:

$$
\hat{p}_j = \frac{\hat{\gamma}_j}{\hat{\gamma}_0}
$$

onde $\hat{\gamma}_j$ representa a *covari√¢ncia amostral* no lag $j$, calculada como [^4]:

$$
\hat{\gamma}_j = \frac{1}{T} \sum_{t=j+1}^{T} (y_t - \bar{y})(y_{t-j} - \bar{y})
$$

e $\bar{y}$ √© a m√©dia amostral da s√©rie temporal, dada por [^4]:
$$
\bar{y} = \frac{1}{T} \sum_{t=1}^{T} y_t.
$$
√â importante notar que, embora apenas $T - j$ observa√ß√µes sejam usadas para calcular $\hat{\gamma}_j$, o denominador na f√≥rmula [4.8.6] √© $T$ em vez de $T-j$ [^4]. Essa escolha garante que as estimativas se aproximem de zero conforme $j$ aumenta, consistindo com a hip√≥tese de estacionariedade de covari√¢ncia da s√©rie temporal.  Al√©m disso, o conjunto completo de observa√ß√µes √© utilizado para computar $\bar{y}$.

> üí° **Exemplo Num√©rico:** Vamos considerar uma s√©rie temporal com $T=10$ observa√ß√µes: $y = [2, 4, 6, 5, 8, 10, 9, 12, 14, 11]$.
>
> Primeiro, calculamos a m√©dia amostral:
>
> $\bar{y} = \frac{2+4+6+5+8+10+9+12+14+11}{10} = \frac{81}{10} = 8.1$
>
> Agora, vamos calcular a autocovari√¢ncia amostral para o lag $j=1$, $\hat{\gamma}_1$:
>
> $\hat{\gamma}_1 = \frac{1}{10} [(4-8.1)(2-8.1) + (6-8.1)(4-8.1) + (5-8.1)(6-8.1) + (8-8.1)(5-8.1) + (10-8.1)(8-8.1) + (9-8.1)(10-8.1) + (12-8.1)(9-8.1) + (14-8.1)(12-8.1) + (11-8.1)(14-8.1)]$
>
> $\hat{\gamma}_1 = \frac{1}{10} [(-4.1)(-6.1) + (-2.1)(-4.1) + (-3.1)(-2.1) + (-0.1)(-3.1) + (1.9)(-0.1) + (0.9)(1.9) + (3.9)(0.9) + (5.9)(3.9) + (2.9)(5.9)]$
>
> $\hat{\gamma}_1 = \frac{1}{10} [25.01 + 8.61 + 6.51 + 0.31 - 0.19 + 1.71 + 3.51 + 23.01 + 17.11]$
>
> $\hat{\gamma}_1 = \frac{85.58}{10} = 8.558$
>
> Para calcular $\hat{\gamma}_0$:
>
> $\hat{\gamma}_0 = \frac{1}{10} [(2-8.1)^2 + (4-8.1)^2 + (6-8.1)^2 + (5-8.1)^2 + (8-8.1)^2 + (10-8.1)^2 + (9-8.1)^2 + (12-8.1)^2 + (14-8.1)^2 + (11-8.1)^2]$
>
> $\hat{\gamma}_0 = \frac{1}{10} [37.21 + 16.81 + 4.41 + 9.61 + 0.01 + 3.61 + 0.81 + 15.21 + 34.81 + 8.41] = \frac{130.9}{10} = 13.09$
>
> Finalmente, calculamos a autocorrela√ß√£o amostral no lag 1:
>
> $\hat{p}_1 = \frac{\hat{\gamma}_1}{\hat{\gamma}_0} = \frac{8.558}{13.09} \approx 0.6538$
>
> Este valor representa a correla√ß√£o entre os valores da s√©rie temporal em instantes consecutivos.

O comportamento das autocorrela√ß√µes amostrais ao longo de diferentes lags fornece *insights* valiosos sobre a natureza da depend√™ncia temporal na s√©rie em quest√£o. Especificamente, a forma como as autocorrela√ß√µes amostrais decaem com o aumento de $j$ pode nos auxiliar na identifica√ß√£o da ordem de processos MA ou AR. Em um processo MA(q), por exemplo, espera-se que as autocorrela√ß√µes amostrais sejam essencialmente zero para lags maiores que $q$. Isso ocorre porque, por defini√ß√£o, em um processo MA(q) o valor presente √© apenas afetado por *shocks* (ru√≠dos brancos) das √∫ltimas $q$ per√≠odos [^4]. Matematicamente, isso significa que para um processo MA(q), a autocorrela√ß√£o populacional $\rho_j$ ser√° zero para $j>q$.  A partir do momento que a autocorrela√ß√£o amostral $\hat{p}_j$ √© uma estimativa da autocorrela√ß√£o populacional $\rho_j$, √© esperado que $\hat{p}_j$ seja aproximadamente zero para $j>q$, caso a s√©rie temporal seja origin√°ria de um processo MA(q).

> üí° **Exemplo Num√©rico:** Suponha que temos uma s√©rie temporal gerada por um processo MA(2). As autocorrela√ß√µes populacionais $\rho_j$ ser√£o: $\rho_1 \ne 0$, $\rho_2 \ne 0$, e $\rho_j = 0$ para $j > 2$. As autocorrela√ß√µes amostrais correspondentes $\hat{p}_j$, calculadas a partir de uma amostra dessa s√©rie, devem refletir esse comportamento. Se calcularmos $\hat{p}_j$ para v√°rios lags, esperamos observar valores significativos para $\hat{p}_1$ e $\hat{p}_2$ (e possivelmente para um lag muito pequeno com $j=3$ devido √† aleatoriedade da amostra), e valores pr√≥ximos de zero para lags maiores.

Por outro lado, em um processo AR(p), as autocorrela√ß√µes amostrais tendem a decair gradualmente em dire√ß√£o a zero como uma mistura de exponenciais ou senoides amortecidas [^4]. Essa propriedade decorre do fato de que, em um processo AR(p), o valor presente √© afetado por valores da s√©rie em per√≠odos anteriores, e esse efeito se propaga ao longo do tempo.  Assim, a autocorrela√ß√£o populacional $\rho_j$ de um processo AR(p) n√£o √© zero para $j>p$ mas decai gradativamente para zero √† medida que $j$ aumenta. Consequentemente, as autocorrela√ß√µes amostrais $\hat{p}_j$ tamb√©m seguem esse padr√£o.

> üí° **Exemplo Num√©rico:** Para um processo AR(1), a autocorrela√ß√£o populacional $\rho_j$ decai exponencialmente para zero conforme $j$ aumenta. Se o par√¢metro do AR(1) √©, por exemplo, $\phi_1 = 0.7$, ent√£o esperamos que $\rho_1 = 0.7$, $\rho_2 = (0.7)^2 = 0.49$, $\rho_3 = (0.7)^3 = 0.343$, e assim por diante. As autocorrela√ß√µes amostrais $\hat{p}_j$ devem seguir esse mesmo padr√£o de decaimento gradual.

Al√©m da an√°lise do padr√£o de decaimento, tamb√©m podemos avaliar a signific√¢ncia das autocorrela√ß√µes amostrais para identificar se a s√©rie temporal √© um ru√≠do branco, onde a autocorrela√ß√£o amostral para qualquer lag maior que zero deve ser igual a zero. Se a s√©rie temporal for um ru√≠do branco gaussiano, a vari√¢ncia da estimativa $\hat{p}_j$ para qualquer $j \neq 0$ pode ser aproximada por $Var(\hat{p}_j) \approx \frac{1}{T}$  [^4].  O intervalo de confian√ßa de 95% para $\hat{p}_j$  neste caso seria aproximadamente  $\pm 2/\sqrt{T}$ , ent√£o, caso a estimativa $\hat{p}_j$ esteja dentro deste intervalo podemos assumir que a s√©rie temporal n√£o possui autocorrela√ß√£o. No entanto, √© importante ressaltar que se houver autocorrela√ß√£o na s√©rie temporal que gerou os dados originais $\{Y_t\}$, ent√£o a estimativa $\hat{p}_i$ ser√° correlacionada com $\hat{p}_j$ quando $i \neq j$  [^4].  Portanto, os padr√µes que surgem da an√°lise das autocorrela√ß√µes amostrais podem representar erros de amostragem em vez de padr√µes da autocorrela√ß√£o populacional da s√©rie temporal.

> üí° **Exemplo Num√©rico:** Se temos uma s√©rie temporal de tamanho $T = 100$ que se comporta como um ru√≠do branco, a vari√¢ncia da autocorrela√ß√£o amostral √© aproximadamente $Var(\hat{p}_j) \approx \frac{1}{100} = 0.01$. O desvio padr√£o seria $\sqrt{0.01} = 0.1$.  Assim, o intervalo de confian√ßa de 95% √© aproximadamente $\pm 2 \times 0.1 = \pm 0.2$. Portanto, se estimarmos $\hat{p}_j$ para v√°rios lags e todos os valores estiverem dentro desse intervalo (por exemplo, entre -0.2 e 0.2), podemos concluir que a s√©rie temporal √© um ru√≠do branco. Caso algum valor de $\hat{p}_j$ esteja fora desse intervalo, isso seria uma evid√™ncia de que a s√©rie temporal possui autocorrela√ß√£o.

**Proposi√ß√£o 1**
O estimador $\hat{\gamma}_j$ da autocovari√¢ncia $\gamma_j$ √© um estimador enviesado.

*Prova*:
Vamos analisar o valor esperado de $\hat{\gamma}_j$:
$$E[\hat{\gamma}_j] = E\left[\frac{1}{T} \sum_{t=j+1}^{T} (y_t - \bar{y})(y_{t-j} - \bar{y})\right]$$
I. Expandindo a express√£o dentro do somat√≥rio:
    $$E[\hat{\gamma}_j] = \frac{1}{T} E\left[\sum_{t=j+1}^{T} (y_t y_{t-j} - y_t \bar{y} - \bar{y} y_{t-j} + \bar{y}^2)\right]$$
II. Aplicando a linearidade da esperan√ßa:
$$E[\hat{\gamma}_j] = \frac{1}{T} \sum_{t=j+1}^{T} E[y_t y_{t-j}] - \frac{1}{T} \sum_{t=j+1}^{T}E[y_t \bar{y}] - \frac{1}{T} \sum_{t=j+1}^{T}E[\bar{y} y_{t-j}] + \frac{1}{T} \sum_{t=j+1}^{T}E[\bar{y}^2]$$
III. Sabemos que $E[y_t y_{t-j}] = \gamma_j + \mu^2$, onde $\mu$ √© a m√©dia da s√©rie e $E[\bar{y}] = \mu$. Adicionalmente, para processos estacion√°rios, $E[y_t y_{t-j}] = \gamma_j + \mu^2$ para todo $t$.
$$E[\hat{\gamma}_j] = \frac{1}{T} \sum_{t=j+1}^{T} (\gamma_j + \mu^2) - \frac{1}{T} \sum_{t=j+1}^{T} E[y_t \bar{y}] - \frac{1}{T} \sum_{t=j+1}^{T} E[\bar{y} y_{t-j}] + \frac{1}{T} \sum_{t=j+1}^{T}E[\bar{y}^2]$$

IV. Note que $E[y_t \bar{y}] = E[y_t \frac{1}{T} \sum_{k=1}^{T} y_k] = \frac{1}{T} \sum_{k=1}^{T} E[y_t y_k] = \frac{1}{T}\sum_{k=1}^{T} \gamma_{t-k} + \mu^2$. 
Portanto $E[y_t \bar{y}] \ne \gamma_0 + \mu^2$, ent√£o  $E[\hat{\gamma}_j] \ne \gamma_j$.
V. √â claro que, para $j=0$, $E[\hat{\gamma}_0] = E[\frac{1}{T} \sum_{t=1}^{T} (y_t - \bar{y})^2] = \frac{T-1}{T}\gamma_0$. Portanto, o estimador $\hat{\gamma}_0$ √© enviesado. Uma vez que $\hat{\gamma}_j$ √© enviesado para todo $j$, o mesmo ocorre com $\hat{p}_j$, uma vez que este √© uma fun√ß√£o de $\hat{\gamma}_j$. ‚ñ†

**Proposi√ß√£o 1.1**
Um estimador n√£o viesado da autocovari√¢ncia $\gamma_j$ √© dado por:
$$ \tilde{\gamma}_j = \frac{1}{T-j} \sum_{t=j+1}^{T} (y_t - \bar{y})(y_{t-j} - \bar{y}) $$

*Prova:*
O valor esperado de $\tilde{\gamma}_j$ √©:
$$E[\tilde{\gamma}_j] = E\left[\frac{1}{T-j} \sum_{t=j+1}^{T} (y_t - \bar{y})(y_{t-j} - \bar{y})\right]$$
I. Expandindo a express√£o dentro do somat√≥rio:
$$E[\tilde{\gamma}_j] = \frac{1}{T-j} E\left[\sum_{t=j+1}^{T} (y_t y_{t-j} - y_t \bar{y} - \bar{y} y_{t-j} + \bar{y}^2)\right]$$
II. Aplicando a linearidade da esperan√ßa:
$$E[\tilde{\gamma}_j] = \frac{1}{T-j} \sum_{t=j+1}^{T} E[y_t y_{t-j}] - \frac{1}{T-j} \sum_{t=j+1}^{T}E[y_t \bar{y}] - \frac{1}{T-j} \sum_{t=j+1}^{T}E[\bar{y} y_{t-j}] + \frac{1}{T-j} \sum_{t=j+1}^{T}E[\bar{y}^2]$$
III. No caso em que a m√©dia populacional $\mu$ √© conhecida, ou no caso em que $\bar{y}$ pode ser considerada uma boa aproxima√ß√£o da m√©dia populacional, √© poss√≠vel demonstrar que:
$$E[\tilde{\gamma}_j] \approx \frac{1}{T-j} \sum_{t=j+1}^{T} \gamma_j = \gamma_j $$
IV. Para $j=0$, temos $\tilde{\gamma}_0 = \frac{1}{T} \sum_{t=1}^{T} (y_t - \bar{y})^2 = \frac{T-1}{T} \gamma_0$, cujo valor esperado √© $\frac{T-1}{T} \gamma_0$, ou seja, $\tilde{\gamma}_0$ ainda √© enviesado, e para que ele seja n√£o enviesado, devemos multiplicar por $\frac{T}{T-1}$. Portanto, para $j=0$ temos
$$\tilde{\gamma}_0 = \frac{1}{T-1} \sum_{t=1}^{T} (y_t - \bar{y})^2$$
V. Note que, mesmo que o estimador $\tilde{\gamma}_j$ seja n√£o viesado, a sua vari√¢ncia pode ser alta. Em geral, usa-se o estimador $\hat{\gamma}_j$ por ser mais adequado quando $T$ √© grande. ‚ñ†

**Lema 1**
Se uma s√©rie temporal $\{Y_t\}$ √© um ru√≠do branco gaussiano com m√©dia zero e vari√¢ncia $\sigma^2$, ent√£o a autocovari√¢ncia amostral $\hat{\gamma}_j$ tem m√©dia aproximadamente zero e vari√¢ncia aproximadamente $\frac{\sigma^4}{T}$ para $j \neq 0$.

*Prova:*
I. Sabemos que para um ru√≠do branco gaussiano, as autocovari√¢ncias populacionais $\gamma_j$ s√£o zero para $j\neq0$.  A autocovari√¢ncia amostral √© dada por $\hat{\gamma}_j = \frac{1}{T} \sum_{t=j+1}^{T} (y_t - \bar{y})(y_{t-j} - \bar{y})$. Uma vez que $E[y_t] = 0$, ent√£o $E[\hat{\gamma}_j]$ tamb√©m ser√° aproximadamente zero para $T$ grande.
II. A vari√¢ncia de $\hat{\gamma}_j$ √© dada por $Var(\hat{\gamma}_j) = E[\hat{\gamma}_j^2] - (E[\hat{\gamma}_j])^2 \approx E[\hat{\gamma}_j^2]$ uma vez que a m√©dia de $\hat{\gamma}_j$ √© aproximadamente zero.
III. Assim,
$$Var(\hat{\gamma}_j) = E\left[\left(\frac{1}{T} \sum_{t=j+1}^{T} (y_t - \bar{y})(y_{t-j} - \bar{y})\right)^2\right]$$
IV. Dado que $\bar{y}$ tende a zero quando $T$ cresce, podemos aproximar como:
$$Var(\hat{\gamma}_j) \approx \frac{1}{T^2} E\left[\left(\sum_{t=j+1}^{T} y_t y_{t-j}\right)^2\right] $$
V. Expandindo o quadrado e usando a propriedade de que, para ru√≠do branco, $E[y_t y_{t-j}y_k y_{k-l}]$ ser√° zero a n√£o ser que $t=k$ e $t-j = k-l$. No caso em que $t=k$ e $t-j=k-l$, temos $l=j$.
$$Var(\hat{\gamma}_j) \approx \frac{1}{T^2} E\left[\sum_{t=j+1}^{T} (y_t y_{t-j})(y_t y_{t-j}) \right] $$
VI. Portanto:
$$Var(\hat{\gamma}_j) \approx \frac{1}{T^2} \sum_{t=j+1}^{T} E[y_t^2 y_{t-j}^2]$$
VII. Como $y_t$ s√£o independentes e identicamente distribu√≠das, $E[y_t^2 y_{t-j}^2] = E[y_t^2]E[y_{t-j}^2] = \sigma^2 \sigma^2 = \sigma^4$.
$$Var(\hat{\gamma}_j) \approx \frac{1}{T^2} \sum_{t=j+1}^{T} \sigma^4 = \frac{T-j}{T^2} \sigma^4 \approx \frac{\sigma^4}{T}$$‚ñ†

**Lema 1.1**
Se uma s√©rie temporal $\{Y_t\}$ √© um ru√≠do branco gaussiano com m√©dia zero e vari√¢ncia $\sigma^2$, ent√£o a autocorrela√ß√£o amostral $\hat{p}_j$ tem m√©dia aproximadamente zero e vari√¢ncia aproximadamente $\frac{1}{T}$ para $j \neq 0$.

*Prova:*
I. A autocorrela√ß√£o amostral √© dada por $\hat{p}_j = \frac{\hat{\gamma}_j}{\hat{\gamma}_0}$. Uma vez que, para o ru√≠do branco gaussiano, $E[\hat{\gamma}_j] \approx 0$ para $j\neq0$, ent√£o $E[\hat{p}_j] \approx 0$. Para $j=0$ temos $\hat{p}_0 = 1$.
II. A vari√¢ncia de $\hat{p}_j$ √© dada por $Var(\hat{p}_j) \approx \frac{Var(\hat{\gamma}_j)}{\hat{\gamma}_0^2}$.
III. Como $Var(\hat{\gamma}_j) \approx \frac{\sigma^4}{T}$, e $\hat{\gamma}_0 \approx \sigma^2$, temos:
$$Var(\hat{p}_j) \approx \frac{\sigma^4/T}{\sigma^4} = \frac{1}{T}$$
IV. Este resultado justifica a regra do intervalo de confian√ßa de 95% para autocorrela√ß√µes amostrais de ru√≠do branco como $\pm 2/\sqrt{T}$. ‚ñ†

### Conclus√£o
Em resumo, as autocorrela√ß√µes amostrais desempenham um papel fundamental na an√°lise de s√©ries temporais, fornecendo uma estimativa tang√≠vel das rela√ß√µes de depend√™ncia temporal que podem orientar a escolha do modelo adequado. A an√°lise dos padr√µes de decaimento das autocorrela√ß√µes amostrais nos permite distinguir entre processos MA e AR, e com o uso de testes de hip√≥teses na signific√¢ncia das autocorrela√ß√µes amostrais, podemos inferir se o processo √© um ru√≠do branco ou n√£o. Embora as autocorrela√ß√µes amostrais sejam estimativas sujeitas a erros, seu uso cuidadoso, com aten√ß√£o √†s propriedades do processo sob investiga√ß√£o, constitui uma ferramenta poderosa para a modelagem e previs√£o de s√©ries temporais. Em se√ß√µes seguintes exploraremos como esses conceitos se inserem em metodologias como a de Box-Jenkins, al√©m de como usar outras t√©cnicas como as autocorrela√ß√µes parciais para auxiliar na an√°lise de s√©ries temporais.

### Refer√™ncias
[^1]: Express√£o [4.1.1] √© conhecida como o erro quadr√°tico m√©dio associado √† previs√£o.
[^4]: Express√£o [4.8.6]
<!-- END -->
