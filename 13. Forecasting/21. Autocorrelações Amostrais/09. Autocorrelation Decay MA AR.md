## Decaimento das Autocorrela√ß√µes Amostrais: Distin√ß√£o entre Processos MA e AR

### Introdu√ß√£o

Este cap√≠tulo aprofunda a discuss√£o sobre a an√°lise de autocorrela√ß√µes amostrais, com foco na distin√ß√£o entre processos de *Moving Average* (MA) e *Autoregressive* (AR) atrav√©s do padr√£o de decaimento das autocorrela√ß√µes amostrais. Como visto anteriormente, as autocorrela√ß√µes amostrais ($ \hat{p}_j$) s√£o estimativas das autocorrela√ß√µes populacionais ($ \rho_j$) e desempenham um papel crucial na an√°lise de s√©ries temporais [^1, ^4]. Em cap√≠tulos anteriores, foi introduzido como as autocorrela√ß√µes amostrais podem ser usadas para verificar se uma s√©rie temporal √© um ru√≠do branco. Em seguida, demonstramos como a vari√¢ncia das autocorrela√ß√µes amostrais pode ser calculada, e como usar testes de hip√≥tese para verificar se os valores das autocorrela√ß√µes s√£o estatisticamente diferentes de zero. Agora, vamos focar em como os padr√µes de decaimento dessas autocorrela√ß√µes amostrais podem nos guiar na identifica√ß√£o da ordem de um processo MA ou AR. Este conhecimento √© fundamental para a aplica√ß√£o da metodologia Box-Jenkins.

### Decaimento das Autocorrela√ß√µes Amostrais em Processos MA

Em um processo *Moving Average* de ordem $q$ (MA(q)), o valor atual da s√©rie temporal √© uma combina√ß√£o linear de *shocks* (ru√≠dos brancos) atuais e passados, at√© um atraso $q$. Matematicamente, um processo MA(q) pode ser representado como:

$$
Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \ldots + \theta_q \epsilon_{t-q}
$$

onde $\mu$ √© a m√©dia da s√©rie, $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, e $\theta_i$ s√£o os par√¢metros do modelo.

Uma caracter√≠stica fundamental dos processos MA(q) √© que a autocorrela√ß√£o populacional $\rho_j$ √© zero para todos os lags $j > q$. Isso ocorre porque, no modelo MA(q), o valor atual $Y_t$ n√£o √© afetado por *shocks* ocorridos mais de $q$ per√≠odos atr√°s. Consequentemente, as autocorrela√ß√µes amostrais $\hat{p}_j$ para $j > q$ devem ser aproximadamente zero (dentro do erro amostral). O gr√°fico das autocorrela√ß√µes amostrais ($\hat{p}_j$ em fun√ß√£o de $j$) mostrar√°, portanto, um corte abrupto, ou seja, as autocorrela√ß√µes ser√£o significativamente diferentes de zero para os lags $j \leq q$, e aproximadamente zero para $j > q$.

> üí° **Exemplo Num√©rico:** Suponha que temos um processo MA(2) definido por $Y_t = 0.5 + \epsilon_t + 0.8\epsilon_{t-1} -0.4\epsilon_{t-2}$, onde $\epsilon_t$ √© um ru√≠do branco com $\sigma^2 = 1$. As autocorrela√ß√µes populacionais $\rho_j$ s√£o: $\rho_0=1$, $\rho_1 = \frac{\theta_1 + \theta_2 \theta_1}{1 + \theta_1^2 + \theta_2^2} = \frac{0.8 + (0.8)(-0.4)}{1 + 0.8^2 + (-0.4)^2} = \frac{0.48}{1.8} \approx 0.2667$, $\rho_2 = \frac{\theta_2}{1 + \theta_1^2 + \theta_2^2} = \frac{-0.4}{1.8} \approx -0.2222$, e $\rho_j = 0$ para $j > 2$. Ao gerar uma s√©rie temporal com este modelo e calcular as autocorrela√ß√µes amostrais $\hat{p}_j$, esperamos observar valores significativos para $\hat{p}_1$ e $\hat{p}_2$, e valores pr√≥ximos de zero para lags maiores que 2. Para uma amostra de tamanho T=100, simulamos uma s√©rie temporal usando Python e calculamos as autocorrela√ß√µes amostrais:
>
> ```python
> import numpy as np
> import statsmodels.api as sm
>
> np.random.seed(42)
> T = 100
> errors = np.random.normal(0, 1, T+2) #Generate errors for T+2 time periods for lags
> y = 0.5 + errors[2:] + 0.8 * errors[1:-1] - 0.4 * errors[:-2] #Generate Y_t values for the MA(2) process
> acf_values = sm.tsa.acf(y, nlags=5) #Calculate ACF up to lag 5
> print(f"Sample ACF: {acf_values}")
> ```
>
> A sa√≠da do c√≥digo acima ser√° algo pr√≥ximo de: `Sample ACF: [1.         0.21253674 -0.20418247 -0.13000312  0.08636215 -0.03606612]`. Note que os valores s√£o diferentes dos valores populacionais, o que √© esperado devido ao erro amostral. Para verificar se os valores s√£o estatisticamente significativos, podemos calcular os intervalos de confian√ßa. Usando a f√≥rmula para a vari√¢ncia de $\hat{p}_j$ para $j > q$, e assumindo que para $j>2$ a vari√¢ncia √© aproximadamente $1/T$, temos que o desvio padr√£o √© $\sqrt{1/100} = 0.1$. Considerando um n√≠vel de signific√¢ncia de 5%, o intervalo de confian√ßa para $\hat{p}_j$ √© dado por $\pm 2*0.1 = \pm 0.2$. Portanto, os valores amostrais para lags maiores que 2 n√£o s√£o estatisticamente significativos, como esperado para um processo MA(2).
>

√â importante notar que, na pr√°tica, o ‚Äúcorte‚Äù nas autocorrela√ß√µes amostrais pode n√£o ser t√£o abrupto como na teoria, devido ao erro amostral. Como foi visto em cap√≠tulos anteriores, as autocorrela√ß√µes amostrais s√£o estimativas, e est√£o sujeitas a erro amostral [^1, ^4, ^Lema 2.1]. Portanto, mesmo que a autocorrela√ß√£o populacional seja zero, as amostras podem mostrar valores diferentes de zero. A vari√¢ncia de $\hat{p}_j$ √© dada aproximadamente por [^4]:

$$
Var(\hat{p}_j) \approx \frac{1}{T}\left(1 + 2\sum_{i=1}^{j-1} \rho_i^2\right)
$$
Para o caso espec√≠fico do MA(q), esta vari√¢ncia, para $j > q$, torna-se:
$$
Var(\hat{p}_j) \approx \frac{1}{T}\left(1 + 2\sum_{i=1}^{q} \rho_i^2\right)
$$

Onde $\rho_i$ s√£o as autocorrela√ß√µes populacionais para $i \leq q$ [^Lema 1]. Este resultado mostra que a vari√¢ncia dos estimadores de autocorrela√ß√£o amostral depende do tamanho da amostra e das autocorrela√ß√µes populacionais, e √© usado para calcular os intervalos de confian√ßa em torno da estimativa $\hat{p}_j$.
**Lema 1.1** Para um processo MA(q), as autocorrela√ß√µes populacionais $\rho_j$ podem ser calculadas usando os par√¢metros $\theta_i$ do modelo.

*Proof*:
I. Considere um processo MA(q): $Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \ldots + \theta_q \epsilon_{t-q}$.
II.  A autocovari√¢ncia no lag $j$ √© definida como $\gamma_j = Cov(Y_t, Y_{t-j}) = E[(Y_t - \mu)(Y_{t-j} - \mu)]$.
III.  Substituindo a express√£o de $Y_t$ na autocovari√¢ncia, temos:

$\gamma_j = E\left[ \left( \sum_{i=0}^{q} \theta_i \epsilon_{t-i} \right) \left( \sum_{k=0}^{q} \theta_k \epsilon_{t-j-k} \right) \right]$, onde $\theta_0 = 1$.

IV.  Expandindo a express√£o, notamos que $E[\epsilon_{t-i}\epsilon_{t-j-k}] = 0$ para $i \neq j+k$ e $E[\epsilon_{t-i}^2] = \sigma^2$. Portanto, a autocovari√¢ncia $\gamma_j$ √© dada por:

$\gamma_j = \sigma^2 \sum_{i=0}^{q-j} \theta_i \theta_{i+j}$ para $0 \leq j \leq q$, e $\gamma_j = 0$ para $j>q$.

V. A autocorrela√ß√£o √© $\rho_j = \frac{\gamma_j}{\gamma_0}$. Portanto, $\rho_j = \frac{\sum_{i=0}^{q-j} \theta_i \theta_{i+j}}{\sum_{i=0}^q \theta_i^2}$ para $0 \leq j \leq q$, e $\rho_j = 0$ para $j>q$.
‚ñ†
Este lema estabelece uma liga√ß√£o direta entre os par√¢metros do modelo MA(q) e as suas autocorrela√ß√µes populacionais. Isso √© √∫til para calcular as autocorrela√ß√µes te√≥ricas de um processo MA(q) e comparar com as estimativas amostrais.

### Decaimento das Autocorrela√ß√µes Amostrais em Processos AR

Em um processo *Autoregressive* de ordem $p$ (AR(p)), o valor atual da s√©rie temporal √© uma combina√ß√£o linear de seus valores passados, at√© um atraso $p$, mais um *shock* aleat√≥rio. Matematicamente, um processo AR(p) pode ser representado como:

$$
Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t
$$

onde $c$ √© uma constante, $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, e $\phi_i$ s√£o os par√¢metros do modelo.

Ao contr√°rio dos processos MA(q), as autocorrela√ß√µes populacionais $\rho_j$ em um processo AR(p) n√£o s√£o zero ap√≥s um lag espec√≠fico. Em vez disso, as autocorrela√ß√µes amostrais $\hat{p}_j$ em um processo AR(p) tendem a decair gradualmente em dire√ß√£o a zero, como uma mistura de exponenciais ou senoides amortecidas [^4]. Essa propriedade decorre da natureza recursiva dos processos AR, em que cada valor da s√©rie depende dos valores anteriores. Em outras palavras, o valor atual da s√©rie √© influenciado por valores passados e, por consequ√™ncia, os lags tamb√©m est√£o relacionados. O padr√£o de decaimento das autocorrela√ß√µes depende dos valores dos par√¢metros $\phi_i$, e pode assumir diversas formas, desde um decaimento r√°pido a um decaimento mais lento e ondulat√≥rio.

> üí° **Exemplo Num√©rico:** Considere um processo AR(1) definido por $Y_t = 0.8Y_{t-1} + \epsilon_t$. As autocorrela√ß√µes populacionais $\rho_j$ decair√£o exponencialmente, onde $\rho_1 = 0.8$, $\rho_2 = (0.8)^2 = 0.64$, $\rho_3 = (0.8)^3 = 0.512$, e assim por diante. As autocorrela√ß√µes amostrais $\hat{p}_j$ seguir√£o esse mesmo padr√£o, mostrando um decaimento gradual em dire√ß√£o a zero conforme o lag $j$ aumenta.
>
> Para uma s√©rie simulada com T = 100, podemos gerar os dados e calcular a ACF utilizando o seguinte c√≥digo:
> ```python
> import numpy as np
> import statsmodels.api as sm
>
> np.random.seed(42)
> T = 100
> errors = np.random.normal(0, 1, T+100) #Generate errors for more periods to avoid initial conditions effect
> y = np.zeros(T+100) #Initialize vector
> for t in range(1,T+100):
>    y[t] = 0.8 * y[t-1] + errors[t] #Generate Y_t values for the AR(1) process
> acf_values = sm.tsa.acf(y[100:], nlags=5) #Calculate ACF up to lag 5, discarding the initial 100 periods
> print(f"Sample ACF: {acf_values}")
> ```
> A sa√≠da deste c√≥digo ser√° algo como: `Sample ACF: [1.         0.79944224 0.64965565 0.51488569 0.43755624 0.34490762]`.
>
> Para um processo AR(2) definido por $Y_t = 0.8Y_{t-1} - 0.2Y_{t-2} + \epsilon_t$, as autocorrela√ß√µes populacionais podem ter um decaimento ondulat√≥rio, refletindo a intera√ß√£o dos dois lags no processo. Nesse caso, podemos observar valores negativos e positivos para autocorrela√ß√µes de diferentes lags. As autocorrela√ß√µes amostrais $\hat{p}_j$ devem refletir esse padr√£o de decaimento ondulat√≥rio. Podemos simular uma s√©rie temporal usando o seguinte c√≥digo:
>
> ```python
> import numpy as np
> import statsmodels.api as sm
>
> np.random.seed(42)
> T = 100
> errors = np.random.normal(0, 1, T+100) #Generate errors
> y = np.zeros(T+100) #Initialize vector
> for t in range(2,T+100):
>    y[t] = 0.8 * y[t-1] - 0.2 * y[t-2] + errors[t] #Generate Y_t values for the AR(2) process
> acf_values = sm.tsa.acf(y[100:], nlags=5) #Calculate ACF up to lag 5, discarding the initial 100 periods
> print(f"Sample ACF: {acf_values}")
> ```
>
> A sa√≠da desse c√≥digo ser√° algo como: `Sample ACF: [ 1.          0.74557114  0.42945176  0.13994168 -0.09896664 -0.24811966]`.  Notamos que o padr√£o de decaimento √© mais complexo do que o decaimento exponencial do AR(1), e cont√©m valores negativos como esperado.

Como foi discutido nos cap√≠tulos anteriores, para o caso especial de ru√≠do branco, as autocorrela√ß√µes populacionais s√£o zero para todos os lags diferentes de zero, e, portanto, tanto a autocorrela√ß√£o amostral quanto a sua vari√¢ncia s√£o iguais a zero. Os intervalos de confian√ßa das autocorrela√ß√µes amostrais s√£o ent√£o calculados como $\pm 2/\sqrt{T}$, para um n√≠vel de signific√¢ncia de 5% [^4].
### Distin√ß√£o entre Processos MA e AR com Autocorrela√ß√µes Amostrais
O padr√£o de decaimento das autocorrela√ß√µes amostrais fornece um meio pr√°tico para distinguir entre processos MA e AR:
*   **Processos MA(q):** Apresentam um corte abrupto nas autocorrela√ß√µes amostrais, ou seja, as autocorrela√ß√µes s√£o significativamente diferentes de zero para lags $j \leq q$, e aproximadamente zero para $j > q$.
*   **Processos AR(p):** As autocorrela√ß√µes amostrais decaem gradualmente em dire√ß√£o a zero, seguindo um padr√£o exponencial ou sinusoidal amortecido, dependendo dos par√¢metros do modelo.

Essa diferen√ßa nos padr√µes de decaimento pode ser usada como um guia para a identifica√ß√£o da ordem de processos MA ou AR, no contexto da metodologia de Box-Jenkins.

√â importante enfatizar que a an√°lise do decaimento das autocorrela√ß√µes amostrais n√£o √© um processo exato e est√° sujeita a interpreta√ß√£o. O erro amostral pode fazer com que autocorrela√ß√µes amostrais pare√ßam ser significativamente diferentes de zero quando n√£o s√£o, ou que pare√ßam decair de forma mais r√°pida ou mais lenta do que o esperado. Al√©m disso, a forma com que os intervalos de confian√ßa s√£o calculados pode influenciar a decis√£o sobre quais valores devem ser considerados significativos.

Em particular, um problema comum √© a dificuldade de distinguir entre um processo AR e um processo MA de alta ordem. Por exemplo, as autocorrela√ß√µes de um processo MA(q) podem n√£o ser iguais a zero para lags maiores do que $q$ devido ao erro amostral. Nesse cen√°rio, a an√°lise das autocorrela√ß√µes parciais pode auxiliar na identifica√ß√£o da ordem do modelo, uma vez que a autocorrela√ß√£o parcial tem como objetivo medir a correla√ß√£o entre duas vari√°veis ap√≥s remover a influ√™ncia de vari√°veis intermedi√°rias.

**Proposi√ß√£o 1**: O decaimento exponencial das autocorrela√ß√µes amostrais para um processo AR(1) √© diretamente proporcional ao par√¢metro autoregressivo $\phi$, onde a autocorrela√ß√£o no lag $j$ √© $\rho_j = \phi^j$.

*Prova*:
I. Um processo AR(1) √© definido como $Y_t = \phi Y_{t-1} + \epsilon_t$. A autocorrela√ß√£o no lag 1 √© definida como $\rho_1 = \frac{Cov(Y_t, Y_{t-1})}{Var(Y_t)}$.
II. Multiplicando a defini√ß√£o do processo AR(1) por $Y_{t-1}$ e tomando a esperan√ßa, temos $E[Y_t Y_{t-1}] = \phi E[Y_{t-1}^2] + E[\epsilon_t Y_{t-1}]$.
III. Como $\epsilon_t$ √© um ru√≠do branco e n√£o correlacionado com os valores passados de $Y_t$, temos $E[\epsilon_t Y_{t-1}] = 0$. Assim, $Cov(Y_t, Y_{t-1}) = E[Y_t Y_{t-1}] = \phi E[Y_{t-1}^2] = \phi Var(Y_t)$.
IV. Portanto, $\rho_1 = \frac{Cov(Y_t, Y_{t-1})}{Var(Y_t)} = \frac{\phi Var(Y_t)}{Var(Y_t)} = \phi$.
V. De maneira similar, multiplicando a equa√ß√£o do AR(1) por $Y_{t-j}$ e tomando a esperan√ßa, temos $E[Y_t Y_{t-j}] = \phi E[Y_{t-1} Y_{t-j}] + E[\epsilon_t Y_{t-j}]$. Para $j>1$, temos que $E[\epsilon_t Y_{t-j}] = 0$ e $Cov(Y_t, Y_{t-j}) = \phi Cov(Y_{t-1}, Y_{t-j}) = \phi \gamma_{j-1}$, onde $\gamma_k$ √© a autocovari√¢ncia no lag $k$.
VI. Dividindo pelo a vari√¢ncia de $Y_t$ temos que $\rho_j = \phi \rho_{j-1}$. Portanto, $\rho_j = \phi^j$.
VII. O resultado para a autocorrela√ß√£o amostral segue que, para grandes amostras, $\hat{\rho}_j \approx \rho_j$.
‚ñ†
**Teorema 1.1** A autocorrela√ß√£o amostral $\hat{p}_j$ de um processo AR(1) converge em probabilidade para $\phi^j$ quando o tamanho da amostra tende ao infinito.

*Proof:*
I. Pela proposi√ß√£o 1, sabemos que a autocorrela√ß√£o populacional de um processo AR(1) √© dada por $\rho_j = \phi^j$.
II. A autocorrela√ß√£o amostral $\hat{p}_j$ √© um estimador consistente da autocorrela√ß√£o populacional $\rho_j$.
III. Por defini√ß√£o, um estimador consistente converge em probabilidade para o valor do par√¢metro que ele est√° estimando.
IV. Portanto, $\hat{p}_j \xrightarrow{p} \rho_j$ quando o tamanho da amostra tende ao infinito, onde $\xrightarrow{p}$ indica converg√™ncia em probabilidade.
V.  Assim, $\hat{p}_j \xrightarrow{p} \phi^j$ quando o tamanho da amostra tende ao infinito.
‚ñ†
Este teorema formaliza a ideia de que, para amostras grandes, as autocorrela√ß√µes amostrais de um processo AR(1) seguem o padr√£o de decaimento exponencial da autocorrela√ß√£o populacional.

**Proposi√ß√£o 2**: Para um processo MA(q), a autocorrela√ß√£o amostral $\hat{p}_j$ √© assintoticamente zero para $j>q$.

*Proof:*
I. Um processo MA(q) √© definido como $Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \ldots + \theta_q \epsilon_{t-q}$
II. As autocorrela√ß√µes populacionais $\rho_j$ de um processo MA(q) s√£o iguais a zero para lags $j>q$, dado que n√£o h√° depend√™ncia entre valores separados por mais de $q$ lags.
III. As autocorrela√ß√µes amostrais s√£o estimativas das autocorrela√ß√µes populacionais. Portanto, $\hat{p}_j \rightarrow \rho_j$ quando o tamanho da amostra tende ao infinito.
IV. Dado que $\rho_j = 0$ para $j>q$, temos que $\hat{p}_j \rightarrow 0$ quando o tamanho da amostra tende ao infinito.
‚ñ†
### Conclus√£o

O decaimento das autocorrela√ß√µes amostrais, com o corte abrupto para modelos MA(q) e decaimento gradual para modelos AR(p), √© uma ferramenta fundamental para a an√°lise de s√©ries temporais. Embora as autocorrela√ß√µes amostrais sejam estimativas sujeitas a erros, a an√°lise do padr√£o do seu decaimento juntamente com as autocorrela√ß√µes parciais, o uso de testes de hip√≥teses, o conhecimento da vari√¢ncia, e a parcim√¥nia s√£o ferramentas valiosas na sele√ß√£o de modelos adequados para previs√£o e an√°lise de s√©ries temporais. No pr√≥ximo cap√≠tulo, usaremos esse conhecimento no contexto da fun√ß√£o de verossimilhan√ßa para estimar os par√¢metros de processos ARMA.
### Refer√™ncias
[^1]: Express√£o [4.1.1] √© conhecida como o erro quadr√°tico m√©dio associado √† previs√£o.
[^4]: Express√£o [4.8.6]
[^Lema 1]: Lema 1 no cap√≠tulo anterior
[^Lema 2.1]: Lema 2.1 no cap√≠tulo anterior
<!-- END -->
