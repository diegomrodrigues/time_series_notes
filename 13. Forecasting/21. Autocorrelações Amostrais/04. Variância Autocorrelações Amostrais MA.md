## Vari√¢ncia das Autocorrela√ß√µes Amostrais para Processos MA(q)

### Introdu√ß√£o
Este cap√≠tulo expande a an√°lise da vari√¢ncia das autocorrela√ß√µes amostrais, concentrando-se especificamente em processos *Moving Average* (MA(q)). Como estabelecido em cap√≠tulos anteriores, as autocorrela√ß√µes amostrais ($\hat{p}_j$) s√£o estimativas das autocorrela√ß√µes populacionais ($\rho_j$) e s√£o ferramentas fundamentais na an√°lise de s√©ries temporais [^1, ^4, ^Proposi√ß√£o 2.1]. Para processos MA(q), as autocorrela√ß√µes populacionais $\rho_j$ s√£o zero para lags $j > q$. Entretanto, devido ao erro amostral, as autocorrela√ß√µes amostrais $\hat{p}_j$ para $j > q$ podem n√£o ser exatamente zero. O objetivo deste cap√≠tulo √© formalizar a vari√¢ncia dessas estimativas para processos MA(q), apresentando t√©cnicas para o seu c√°lculo, o qual envolve o uso de somat√≥rios de autocorrela√ß√µes amostrais elevadas ao quadrado. Al√©m disso, discutiremos como m√©todos num√©ricos podem ser empregados para evitar o problema de matrizes mal condicionadas que podem surgir nesse processo. Este entendimento √© crucial para a aplica√ß√£o robusta de testes de hip√≥tese na modelagem de s√©ries temporais utilizando a metodologia Box-Jenkins, e na interpreta√ß√£o dos resultados.

### C√°lculo da Vari√¢ncia das Autocorrela√ß√µes Amostrais para MA(q)

Como discutido anteriormente, a vari√¢ncia da autocorrela√ß√£o amostral $\hat{p}_j$ √© dada por [^4]:

$$
Var(\hat{p}_j) \approx \frac{1}{T}\left(1 + 2\sum_{i=1}^{j-1} \rho_i^2\right)
$$

Para processos MA(q), temos que $\rho_i = 0$ para todo $i > q$. Isso simplifica a express√£o acima para $j > q$:

$$
Var(\hat{p}_j) \approx \frac{1}{T}\left(1 + 2\sum_{i=1}^{q} \rho_i^2\right)
$$

Essa express√£o mostra que, para $j > q$, a vari√¢ncia da autocorrela√ß√£o amostral $\hat{p}_j$ depende apenas das autocorrela√ß√µes populacionais dos $q$ primeiros lags. Entretanto, na pr√°tica, as autocorrela√ß√µes populacionais $\rho_i$ s√£o desconhecidas. Portanto, a vari√¢ncia de $\hat{p}_j$ √© geralmente aproximada usando as autocorrela√ß√µes amostrais como uma substitui√ß√£o:

$$
\widehat{Var}(\hat{p}_j) \approx \frac{1}{T}\left(1 + 2\sum_{i=1}^{q} \hat{p}_i^2\right)
$$

Essa aproxima√ß√£o √© v√°lida sob condi√ß√µes de regularidade e se o tamanho da amostra $T$ for suficientemente grande. Para os casos em que a amostra n√£o √© grande o suficiente, √© importante salientar que o uso dessa aproxima√ß√£o pode gerar testes com baixa capacidade de detec√ß√£o (poder) da hip√≥tese nula.

√â importante ressaltar que o uso das autocorrela√ß√µes amostrais dentro do somat√≥rio acima faz com que os estimadores de $\widehat{Var}(\hat{p}_j)$ tamb√©m sejam estimativas sujeitas √† variabilidade amostral. Assim, √© essencial usar os resultados deste cap√≠tulo com cautela.

> üí° **Exemplo Num√©rico:** Suponha que temos um processo MA(2) com par√¢metros $\theta_1=0.8$ e $\theta_2 = -0.4$. As autocorrela√ß√µes populacionais $\rho_1$ e $\rho_2$ podem ser obtidas em fun√ß√£o dos par√¢metros do processo, e s√£o dadas por:
>
> $$\rho_1 = \frac{\theta_1 + \theta_1\theta_2}{1 + \theta_1^2 + \theta_2^2} = \frac{0.8 + 0.8(-0.4)}{1 + 0.8^2 + (-0.4)^2} = \frac{0.8 - 0.32}{1 + 0.64 + 0.16} = \frac{0.48}{1.8} = 0.267$$
>
> $$\rho_2 = \frac{\theta_2}{1 + \theta_1^2 + \theta_2^2} = \frac{-0.4}{1 + 0.8^2 + (-0.4)^2} = \frac{-0.4}{1.8} = -0.222$$
>
> E, para $j > 2$, $\rho_j=0$. Suponha que temos uma amostra com $T=200$ observa√ß√µes desse processo e que as autocorrela√ß√µes amostrais sejam $\hat{p}_1=0.25$ e $\hat{p}_2=-0.20$.  Para o lag $j=3$, a vari√¢ncia estimada de $\hat{p}_3$ √©:
>
>  $$
> \widehat{Var}(\hat{p}_3) \approx \frac{1}{200}\left(1 + 2(0.25)^2 + 2(-0.20)^2\right)
> $$
>
>  $$
> \widehat{Var}(\hat{p}_3) \approx \frac{1}{200}(1 + 2(0.0625) + 2(0.04)) = \frac{1}{200}(1 + 0.125 + 0.08) = \frac{1.205}{200} = 0.006025
> $$
>  O desvio padr√£o seria a raiz quadrada de 0.006025, ou seja, $\approx 0.0776$. Isso significa que o intervalo de confian√ßa de 95% para a autocorrela√ß√£o amostral $\hat{p}_3$ seria aproximadamente $\pm 2 \times 0.0776 \approx \pm 0.155$. Portanto, se $\hat{p}_3$ for maior que $0.155$ ou menor que $-0.155$, rejeitamos a hip√≥tese nula de que a autocorrela√ß√£o populacional para o lag 3 seja zero.
**Lema 1:** A vari√¢ncia da autocorrela√ß√£o amostral $\hat{p}_j$ para um processo MA(q), quando $j > q$, pode ser expressa usando a fun√ß√£o de autocovari√¢ncia $\gamma_i$ do processo. Especificamente,
    $$
    Var(\hat{p}_j) \approx \frac{1}{T} \left( \sum_{i=-\infty}^{\infty} \rho_i^2 \right) = \frac{1}{T} \left( \sum_{i=-\infty}^{\infty} \frac{\gamma_i^2}{\gamma_0^2} \right)
    $$
    *Prova:*
    I. A vari√¢ncia da autocorrela√ß√£o amostral $\hat{p}_j$ √© dada por $Var(\hat{p}_j) \approx \frac{1}{T}\left(1 + 2\sum_{i=1}^{j-1} \rho_i^2\right)$ [^4].
    II. Para um processo MA(q), $\rho_i = 0$ para $|i| > q$. Portanto, a soma se torna finita, e pode ser reescrita como $\sum_{i=-\infty}^{\infty} \rho_i^2 = 1 + 2\sum_{i=1}^{q} \rho_i^2 $.
    III.  A autocorrela√ß√£o $\rho_i$ √© definida como $\rho_i = \frac{\gamma_i}{\gamma_0}$, onde $\gamma_i$ √© a autocovari√¢ncia no lag $i$.
    IV. Substituindo a defini√ß√£o de autocorrela√ß√£o na express√£o da vari√¢ncia, obtemos $Var(\hat{p}_j) \approx \frac{1}{T} \left( \sum_{i=-\infty}^{\infty} \frac{\gamma_i^2}{\gamma_0^2} \right)$
    V. Portanto, $Var(\hat{p}_j) \approx \frac{1}{T} \left( \sum_{i=-\infty}^{\infty} \rho_i^2 \right) = \frac{1}{T} \left( \sum_{i=-\infty}^{\infty} \frac{\gamma_i^2}{\gamma_0^2} \right)$ ‚ñ†

Para testar a hip√≥tese conjunta de que as autocorrela√ß√µes amostrais s√£o iguais a zero em v√°rios lags, podemos usar a estat√≠stica de Ljung-Box, que, como vimos anteriormente, √© dada por [^Corol√°rio 3.1]:

$$Q = T(T+2)\sum_{j=1}^{k} \frac{\hat{p}_j^2}{T-j}$$
E que, sob a hip√≥tese nula, segue uma distribui√ß√£o $\chi^2$ com $k$ graus de liberdade.

### Problemas de Matrizes Mal Condicionadas e Solu√ß√µes Num√©ricas

Em alguns casos, ao calcular a vari√¢ncia das autocorrela√ß√µes amostrais, podemos enfrentar problemas de instabilidade num√©rica associados a matrizes mal condicionadas. Um problema de matriz mal condicionada ocorre quando a matriz a ser invertida tem autovalores muito pequenos ou pr√≥ximos de zero, o que pode levar a instabilidade nos resultados da computa√ß√£o. Em geral, isso ocorre quando o modelo da s√©rie temporal que estamos analisando tem par√¢metros que est√£o perto da n√£o invertibilidade, o que causa alta correla√ß√£o entre os lags da s√©rie.

No caso espec√≠fico do c√°lculo da vari√¢ncia das autocorrela√ß√µes amostrais para modelos MA(q), este problema pode surgir ao tentar calcular as autocorrela√ß√µes populacionais $\rho_i$ ou ao computar diretamente a vari√¢ncia $\widehat{Var}(\hat{p}_j)$ usando somat√≥rios de amostras em casos em que as autocorrela√ß√µes amostrais $\hat{p}_j$ s√£o grandes, ou quando o tamanho da amostra $T$ n√£o √© suficientemente grande.
**Proposi√ß√£o 1:** A condi√ß√£o de invertibilidade de um processo MA(q) est√° diretamente relacionada ao condicionamento da matriz de autocovari√¢ncias. Par√¢metros pr√≥ximos ao limite de n√£o invertibilidade levam a matrizes de autocovari√¢ncias mal condicionadas, o que dificulta a estimativa precisa das autocorrela√ß√µes amostrais e suas vari√¢ncias.
*Prova:*
   I. Um processo MA(q) √© invert√≠vel se as ra√≠zes do polin√¥mio MA, $1 + \theta_1 z + \theta_2 z^2 + \ldots + \theta_q z^q = 0$, estiverem fora do c√≠rculo unit√°rio no plano complexo. Isso garante que o processo possa ser expresso como um processo AR(‚àû) convergente.
   II. Quando os par√¢metros $\theta_i$ se aproximam do limite de n√£o invertibilidade, as ra√≠zes do polin√¥mio se aproximam do c√≠rculo unit√°rio.
   III. As autocovari√¢ncias de um processo MA(q) s√£o dadas por $\gamma_k = \mathbb{E}[(X_t - \mu)(X_{t-k} - \mu)]$, onde $X_t$ √© o processo MA(q) e $\mu$ √© sua m√©dia.
   IV. A matriz de autocovari√¢ncias $\Gamma$ √© formada por essas autocovari√¢ncias, com elementos $\Gamma_{ij} = \gamma_{|i-j|}$.
   V.  Quando as ra√≠zes do polin√¥mio MA se aproximam do c√≠rculo unit√°rio, as autocovari√¢ncias se tornam altamente correlacionadas. Isso significa que a matriz de autocovari√¢ncias se torna quase singular, ou seja, tem um determinante pr√≥ximo de zero.
   VI. Uma matriz com determinante pr√≥ximo de zero √© mal condicionada, pois pequenas mudan√ßas nos seus elementos podem levar a grandes varia√ß√µes na sua inversa, o que dificulta o c√°lculo preciso das autocorrela√ß√µes e suas vari√¢ncias.
   VII. Portanto, a condi√ß√£o de invertibilidade est√° ligada ao condicionamento da matriz de autocovari√¢ncias: par√¢metros pr√≥ximos ao limite de n√£o invertibilidade levam a matrizes mal condicionadas. ‚ñ†

Para mitigar os efeitos desses problemas, podemos adotar algumas t√©cnicas num√©ricas:

1.  **Regulariza√ß√£o:** A regulariza√ß√£o √© uma t√©cnica que adiciona um pequeno valor √† diagonal da matriz a ser invertida, o que impede que os autovalores sejam muito pr√≥ximos de zero. No nosso caso, isso se traduz em adicionar um pequeno valor $\epsilon$ √†s autocorrela√ß√µes, ou seja, usar $\rho_i + \epsilon$ no lugar de $\rho_i$. A escolha do valor de $\epsilon$ √© um problema que deve ser levado em conta ao analisar os resultados, mas o m√©todo garante a estabilidade da solu√ß√£o.
2.  **Decomposi√ß√£o de Valores Singulares (SVD):** A SVD √© uma t√©cnica de √°lgebra linear que decomp√µe uma matriz em tr√™s outras matrizes com propriedades espec√≠ficas, onde os valores singulares podem ser analisados. Ao usar a SVD, podemos ignorar autovalores muito pequenos, evitando problemas de instabilidade.
3.  **Algoritmos Iterativos:** O uso de algoritmos iterativos podem ser uma solu√ß√£o robusta para os casos em que o problema da invers√£o de matrizes √© inst√°vel. Esses algoritmos encontram aproxima√ß√µes das inversas sem a necessidade de realizar a invers√£o direta.
4.  **Software:** Existem pacotes computacionais que j√° implementam essas t√©cnicas, o que pode facilitar o c√°lculo das vari√¢ncias das autocorrela√ß√µes amostrais, como o pacote `statsmodels` no Python.
5.  **Aumentar o tamanho da amostra:** Aumento do tamanho amostral, sempre que poss√≠vel, pode mitigar os problemas causados por matrizes mal condicionadas, uma vez que o erro amostral diminui com o aumento do tamanho amostral.

> üí° **Exemplo Num√©rico:** Vamos considerar um processo MA(1) com $\theta = 0.99$ e uma amostra de $T=50$. Nesse cen√°rio, o par√¢metro $\theta$ est√° pr√≥ximo do limite de n√£o invertibilidade, o que torna a matriz da autocovari√¢ncia mal condicionada. Ao calcular as autocorrela√ß√µes amostrais e suas vari√¢ncias usando a f√≥rmula padr√£o, podemos observar resultados inst√°veis. No entanto, ao usar um m√©todo de regulariza√ß√£o (adicionar $\epsilon = 0.01$ √†s autocorrela√ß√µes), ou usar m√©todos de √°lgebra linear como o SVD, podemos obter resultados mais est√°veis.
>
> Um exemplo de como fazer isso usando o pacote `statsmodels` no Python √© dado a seguir:
> ```python
> import numpy as np
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
>
> # Define parameters
> theta = 0.99
> T = 50
>
> # Simulate a MA(1) series
> np.random.seed(42)  # For reproducibility
> errors = np.random.normal(0, 1, T)
> ma1_series = [errors[0]]
> for t in range(1, T):
>   ma1_series.append(errors[t] + theta * errors[t-1])
>
> # Calculate ACF
> acf_values = sm.tsa.acf(ma1_series, nlags=10)
>
> # Calculate variance of sample autocorrelations
> # Method 1: Using the approximation for white noise
> variance1 = np.full(len(acf_values), 1/T)
>
> # Method 2: Using approximation for MA(q) with regularized rho
>
> # Calculate rho
> rho1 = theta / (1 + theta**2)
> rho = [1, rho1]
> variance2 = np.array([0, 1/T, 1/T * (1+2*rho[1]**2)])
> variance2 = np.concatenate([variance2, np.full(len(acf_values) - len(variance2), variance2[-1])])
>
> # Plotting the ACF with the two approximations
> plt.figure(figsize=(10, 6))
> plt.stem(range(len(acf_values)), acf_values, label='ACF Values')
> plt.axhline(y=0, color='black', linestyle='--')
> plt.axhspan(y=-2 * np.sqrt(variance1[1]), y=2 * np.sqrt(variance1[1]), alpha=0.2, color='blue', label='White Noise Confidence Interval (Approx.)')
> plt.axhspan(y=-2 * np.sqrt(variance2[1]), y=2 * np.sqrt(variance2[1]), alpha=0.2, color='red', label='MA(q) Confidence Interval (Approx.)')
> plt.xlabel('Lag')
> plt.ylabel('Autocorrelation')
> plt.title('Sample ACF with Confidence Intervals (MA(1) near non-invertible region)')
> plt.legend()
> plt.show()
> ```
>
> O c√≥digo acima demonstra como as fun√ß√µes do `statsmodels` podem ser usadas para gerar uma s√©rie MA(1) e calcular as autocorrela√ß√µes amostrais.  Al√©m disso, s√£o calculados os intervalos de confian√ßa das autocorrela√ß√µes usando as vari√¢ncias estimadas pelos dois m√©todos. O primeiro m√©todo assume que a s√©rie temporal √© um ru√≠do branco (vari√¢ncia de $\frac{1}{T}$), e o segundo m√©todo √© uma aproxima√ß√£o para um MA(q).  Ao observar o resultado, podemos ver que para as primeiras autocorrela√ß√µes, a aproxima√ß√£o MA(q) gera um intervalo de confian√ßa maior, uma vez que ela leva em conta que o processo pode n√£o ser um ru√≠do branco.
>
> üí° **Exemplo Num√©rico:** Para ilustrar o problema de matrizes mal condicionadas, vamos considerar um processo MA(1) com $\theta = 0.999$. Este valor de $\theta$ est√° extremamente pr√≥ximo do limite de n√£o invertibilidade. Vamos supor que temos uma amostra de tamanho $T = 100$. A autocorrela√ß√£o te√≥rica no lag 1 √© dada por $\rho_1 = \frac{\theta}{1+\theta^2} = \frac{0.999}{1+0.999^2} \approx 0.5$.
>
> Vamos calcular a matriz de autocovari√¢ncias para este processo. A matriz de autocovari√¢ncias para um MA(1) tem a forma:
>
> $$
> \Gamma = \begin{bmatrix}
> \gamma_0 & \gamma_1 & 0 & 0 & \cdots & 0 \\
> \gamma_1 & \gamma_0 & \gamma_1 & 0 & \cdots & 0 \\
> 0 & \gamma_1 & \gamma_0 & \gamma_1 & \cdots & 0 \\
> \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
> 0 & 0 & 0 & 0 & \cdots & \gamma_0
> \end{bmatrix}
> $$
>
> Onde $\gamma_0 = \sigma^2 (1 + \theta^2)$ e $\gamma_1 = \sigma^2 \theta$ (e $\gamma_k = 0$ para $k > 1$). Assumindo $\sigma^2 = 1$ temos $\gamma_0 = 1 + 0.999^2 \approx 1.998$ e $\gamma_1 = 0.999$.
>
> Se tentarmos calcular a inversa desta matriz (que √© necess√°ria para obter a vari√¢ncia das autocorrela√ß√µes amostrais) usando m√©todos num√©ricos padr√£o, podemos observar que os resultados podem ser muito sens√≠veis a pequenas mudan√ßas nos elementos da matriz, especialmente para valores de $\theta$ pr√≥ximos a 1 ou -1. Isto se deve a que os autovalores da matriz est√£o pr√≥ximos a zero, tornando a matriz mal condicionada. Uma solu√ß√£o √© usar a t√©cnica de regulariza√ß√£o, onde adicionamos um valor pequeno ($\epsilon$) √† diagonal principal da matriz, ou usar m√©todos mais est√°veis como a SVD para inverter a matriz.
>
> Usando regulariza√ß√£o, a matriz $\Gamma$ √© transformada em $\Gamma + \epsilon I$, onde $I$ √© a matriz identidade, e $\epsilon$ √© um pequeno valor (ex: 0.01). Isto desloca os autovalores para longe de zero, tornando a matriz mais est√°vel para invers√£o.
>
> Outra forma de contornar o problema √© usando SVD, que decomp√µe a matriz em tr√™s outras com propriedades interessantes. Ao realizar a invers√£o usando a decomposi√ß√£o em valores singulares, podemos ignorar autovalores que est√£o pr√≥ximos de zero.

### Conclus√£o

Este cap√≠tulo detalhou como calcular a vari√¢ncia das autocorrela√ß√µes amostrais para modelos MA(q) e apresentou solu√ß√µes para problemas num√©ricos que podem surgir neste processo, como o uso de somat√≥rios de autocorrela√ß√µes amostrais elevadas ao quadrado, e como usar m√©todos num√©ricos para evitar o problema de matrizes mal condicionadas. O uso de autocorrela√ß√µes amostrais em testes de hip√≥teses deve ser feito com cautela, e √© preciso ter aten√ß√£o aos seus vieses e aproxima√ß√µes, bem como os limites de validade dos testes e aproxima√ß√µes. Compreender a variabilidade amostral e adotar t√©cnicas num√©ricas robustas s√£o passos fundamentais para a constru√ß√£o de modelos precisos e para a valida√ß√£o dos resultados da modelagem de s√©ries temporais. O pr√≥ximo passo √© usar esse conhecimento para realizar a identifica√ß√£o e escolha do modelo adequado, como na metodologia de Box-Jenkins.
### Refer√™ncias
[^1]: Express√£o [4.1.1] √© conhecida como o erro quadr√°tico m√©dio associado √† previs√£o.
[^4]: Express√£o [4.8.6]
[^Proposi√ß√£o 2.1]: Proposi√ß√£o 2.1 no cap√≠tulo anterior
[^Corol√°rio 3.1]: Corol√°rio 3.1 no cap√≠tulo anterior
<!-- END -->
