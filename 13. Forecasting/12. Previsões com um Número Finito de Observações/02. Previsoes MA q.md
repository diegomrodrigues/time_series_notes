## Previs√µes com um N√∫mero Finito de Observa√ß√µes: Aproxima√ß√µes e Recurs√£o
### Introdu√ß√£o
Este cap√≠tulo explora m√©todos para gerar previs√µes quando temos um n√∫mero finito de observa√ß√µes, construindo sobre as bases te√≥ricas estabelecidas anteriormente. Em vez de assumir um n√∫mero infinito de observa√ß√µes, como em [^4.2], agora abordamos cen√°rios pr√°ticos onde os dados passados s√£o limitados. Os m√©todos de previs√£o √≥timos, conforme discutido em se√ß√µes anteriores, usam informa√ß√µes do passado at√© o ponto em que os dados est√£o dispon√≠veis [^4.2]. No entanto, para modelos MA e ARMA, isso implicaria um n√∫mero infinito de valores passados de Y, levando √† necessidade de aproxima√ß√µes pr√°ticas quando o n√∫mero de observa√ß√µes √© finito. Assim, este cap√≠tulo explora como aproximar as previs√µes √≥timas quando o n√∫mero de observa√ß√µes √© limitado [^4.3].

### Conceitos Fundamentais
#### Aproxima√ß√µes para Previs√µes √ìtimas
Uma abordagem para prever com um n√∫mero finito de observa√ß√µes √© tratar os erros de pr√©-amostra como todos iguais a zero [^4.3]. A ideia √© usar a seguinte aproxima√ß√£o:

$$
\hat{E}(Y_{t+s} | Y_t, Y_{t-1}, \dots) \approx \hat{E}(Y_{t+s} | Y_t, Y_{t-1}, \dots, Y_{t-m+1}, \epsilon_{t-m}=0, \epsilon_{t-m-1}=0, \dots)
$$

onde $m$ denota o n√∫mero de observa√ß√µes passadas consideradas. Este m√©todo tenta imitar a previs√£o ideal, mas com a condi√ß√£o de que os erros passados que est√£o fora do escopo da amostra s√£o definidos como zero [^4.3].

Para exemplificar, considere a previs√£o de um processo MA(q) que, como vimos na se√ß√£o [^4.2], √© descrito pela seguinte equa√ß√£o:
$$
Y_t = \mu + \epsilon_t + \theta_1\epsilon_{t-1} + \theta_2\epsilon_{t-2} + \dots + \theta_q\epsilon_{t-q}
$$

A previs√£o √≥tima linear, conforme derivado anteriormente em [^4.2.4], √©:

$$
\hat{E}[Y_{t+s} | \epsilon_t, \epsilon_{t-1}, \dots] = \mu + \psi_s\epsilon_t + \psi_{s+1}\epsilon_{t-1} + \psi_{s+2}\epsilon_{t-2} + \dots
$$

Para realizar esta previs√£o na pr√°tica, quando temos um n√∫mero finito de observa√ß√µes, precisamos aproximar os valores passados de $\epsilon_t$ , que podem ser gerados usando uma itera√ß√£o recursiva [^4.3]:
$$\epsilon_{t-m} = \epsilon_{t-m-1} = \dots = \epsilon_{t-m-q+1} = 0$$
e ent√£o, iterando em [^4.2.36]
$$
\hat{\epsilon}_{t-m+1} = (Y_{t-m+1} - \mu), \\
\hat{\epsilon}_{t-m+2} = (Y_{t-m+2} - \mu) - \theta_1\hat{\epsilon}_{t-m+1} \\
\hat{\epsilon}_{t-m+3} = (Y_{t-m+3} - \mu) - \theta_1\hat{\epsilon}_{t-m+2} - \theta_2\hat{\epsilon}_{t-m+1}
$$

e assim por diante, at√© obter os valores atuais, $\hat{\epsilon}_t$. Esses erros estimados s√£o usados na previs√£o [^4.3.5].

> üí° **Exemplo Num√©rico:** Considere um modelo MA(2) com $\mu = 10$, $\theta_1 = 0.5$, $\theta_2 = 0.3$, e temos as seguintes observa√ß√µes: $Y_{t-2} = 10.5$, $Y_{t-1} = 11.2$, $Y_t = 10.8$. Queremos calcular os erros estimados recursivamente. Assumimos que $\hat{\epsilon}_{t-3} = \hat{\epsilon}_{t-4} = 0$.
>
> $\text{Step 1: } \hat{\epsilon}_{t-2} = Y_{t-2} - \mu = 10.5 - 10 = 0.5$
>
> $\text{Step 2: } \hat{\epsilon}_{t-1} = (Y_{t-1} - \mu) - \theta_1\hat{\epsilon}_{t-2} = (11.2 - 10) - 0.5 \times 0.5 = 1.2 - 0.25 = 0.95$
>
> $\text{Step 3: } \hat{\epsilon}_{t} = (Y_{t} - \mu) - \theta_1\hat{\epsilon}_{t-1} - \theta_2\hat{\epsilon}_{t-2} = (10.8 - 10) - 0.5 \times 0.95 - 0.3 \times 0.5 = 0.8 - 0.475 - 0.15 = 0.175$
>
> Esses valores de $\hat{\epsilon}_{t-2}$, $\hat{\epsilon}_{t-1}$, e $\hat{\epsilon}_{t}$ seriam ent√£o usados para calcular a previs√£o de $Y_{t+s}$ para algum $s$.

**Proposi√ß√£o 1** *A recurs√£o para estimar os erros passados $\hat{\epsilon}_t$ pode ser generalizada para qualquer instante de tempo, usando a seguinte f√≥rmula:*

$$
\hat{\epsilon}_{t} = (Y_{t} - \mu) - \sum_{j=1}^{q} \theta_j \hat{\epsilon}_{t-j}
$$

*onde $\hat{\epsilon}_{t-j}$ s√£o os erros estimados recursivamente, e $\hat{\epsilon}_{k} = 0$ para $k<t-m+1$.*

*Prova:*
I. Come√ßamos com a defini√ß√£o do processo MA(q):
$$Y_t = \mu + \epsilon_t + \theta_1\epsilon_{t-1} + \theta_2\epsilon_{t-2} + \dots + \theta_q\epsilon_{t-q}$$

II. Reorganizando a equa√ß√£o acima para isolar o termo de erro $\epsilon_t$, obtemos:
$$\epsilon_t = Y_t - \mu - \theta_1\epsilon_{t-1} - \theta_2\epsilon_{t-2} - \dots - \theta_q\epsilon_{t-q}$$

III. Substituindo $\epsilon_t$ por $\hat{\epsilon}_t$, e os erros passados $\epsilon_{t-j}$ por suas estimativas recursivas $\hat{\epsilon}_{t-j}$  e usando a nota√ß√£o de somat√≥rio, obtemos:
$$\hat{\epsilon}_t = (Y_t - \mu) - \sum_{j=1}^{q} \theta_j \hat{\epsilon}_{t-j}$$

IV. Definimos $\hat{\epsilon}_k = 0$ para $k < t-m+1$, onde m √© o n√∫mero de observa√ß√µes usadas para a previs√£o, estabelecendo a condi√ß√£o inicial para o processo recursivo.
   Portanto, a f√≥rmula recursiva para calcular os erros estimados $\hat{\epsilon}_t$ para qualquer instante de tempo $t$ √©:
   $$\hat{\epsilon}_{t} = (Y_{t} - \mu) - \sum_{j=1}^{q} \theta_j \hat{\epsilon}_{t-j}$$
‚ñ†

Essa proposi√ß√£o estabelece uma forma geral para o c√°lculo recursivo dos erros, facilitando a implementa√ß√£o computacional das previs√µes.

#### Previs√£o de um Modelo MA(q) com Amostra Finita
Para obter uma intui√ß√£o mais clara, considere o caso da previs√£o para o modelo MA(q). Conforme mencionado anteriormente, a previs√£o do modelo MA(q), como vimos em [^4.2], √© dada por:

$$
\hat{Y}_{t+s|t} = \mu + \theta_s\epsilon_t + \theta_{s+1}\epsilon_{t-1} + \dots + \theta_q\epsilon_{t-q+s}
$$

Quando temos um n√∫mero finito de observa√ß√µes, a previs√£o √≥tima √© aproximada definindo todos os erros de pr√©-amostra como zero [^4.3].

Portanto, para obter a previs√£o aproximada de um modelo MA(q) com um n√∫mero finito de observa√ß√µes, tratamos os erros iniciais como zero e usamos as itera√ß√µes recursivas como mencionado anteriormente, para obter o valor presente de $\epsilon$. Como demonstrado em [^4.3],  para $s=1$, a previs√£o aproximada ser√°:

$$
\hat{Y}_{t+1|t} = \mu + \theta(Y_t - \mu) - \theta^2(Y_{t-1} - \mu) + \dots + (-1)^{m-1}\theta^m(Y_{t-m+1} - \mu)
$$

onde m denota o n√∫mero de observa√ß√µes passadas que estamos usando. Esta aproxima√ß√£o usa itera√ß√µes recursivas para gerar os erros passados a partir de $Y_t, Y_{t-1}, \dots, Y_{t-m+1}$, com um erro inicial $\hat{\epsilon}_{t-m}=0$.  Em ess√™ncia, o m√©todo de aproxima√ß√£o usa itera√ß√£o recursiva para estimar erros que n√£o est√£o diretamente dispon√≠veis, usando os dados que temos [^4.3].

> üí° **Exemplo Num√©rico:** Vamos considerar um modelo MA(1) para simplificar, onde $q=1$. Seja $\mu = 5$ e $\theta = 0.6$. Suponha que temos as observa√ß√µes $Y_{t-2} = 5.3$, $Y_{t-1} = 5.8$ e $Y_t = 6.1$. Queremos prever $Y_{t+1}$, utilizando as √∫ltimas $m=3$ observa√ß√µes. Inicializamos com $\hat{\epsilon}_{t-3} = 0$.
>
> $\text{Step 1: Calculate } \hat{\epsilon}_{t-2} = Y_{t-2} - \mu = 5.3 - 5 = 0.3$
>
> $\text{Step 2: Calculate } \hat{\epsilon}_{t-1} = (Y_{t-1} - \mu) - \theta\hat{\epsilon}_{t-2} = (5.8 - 5) - 0.6 \times 0.3 = 0.8 - 0.18 = 0.62$
>
> $\text{Step 3: Calculate } \hat{\epsilon}_{t} = (Y_{t} - \mu) - \theta\hat{\epsilon}_{t-1} = (6.1 - 5) - 0.6 \times 0.62 = 1.1 - 0.372 = 0.728$
>
> $\text{Step 4: The prediction is given by:} \hat{Y}_{t+1|t} = \mu + \theta\hat{\epsilon}_t = 5 + 0.6 \times 0.728 = 5 + 0.4368 = 5.4368$.
>
> Usando a formula aproximada para $m=3$:
> $\hat{Y}_{t+1|t} = \mu + \theta(Y_t - \mu) - \theta^2(Y_{t-1} - \mu) + \theta^3(Y_{t-2} - \mu) = 5 + 0.6(6.1 - 5) - 0.6^2(5.8 - 5) + 0.6^3(5.3-5) = 5 + 0.66 - 0.288 + 0.0648 = 5.4368$.
> Ambas abordagens levam √† mesma previs√£o.

**Lema 1** *A express√£o para $\hat{Y}_{t+1|t}$ pode ser escrita de forma compacta como:*

$$
\hat{Y}_{t+1|t} = \mu + \sum_{k=1}^{m} (-\theta)^{k-1} \theta (Y_{t-k+1} - \mu)
$$

*Prova:*
I.  Come√ßamos com a express√£o para a previs√£o aproximada $\hat{Y}_{t+1|t}$:
$$\hat{Y}_{t+1|t} = \mu + \theta(Y_t - \mu) - \theta^2(Y_{t-1} - \mu) + \dots + (-1)^{m-1}\theta^m(Y_{t-m+1} - \mu)$$

II. Observamos que cada termo ap√≥s $\mu$ tem a forma de  $(-1)^{k-1}\theta^k(Y_{t-k+1} - \mu)$, onde $k$ varia de 1 a $m$.

III. Fatorando $\theta$ de cada termo, exceto o termo $\mu$, obtemos $(-1)^{k-1} \theta^{k} = \theta (-\theta)^{k-1}$.

IV. Substituindo na express√£o de previs√£o e expressando em forma de somat√≥rio obtemos:
$$\hat{Y}_{t+1|t} = \mu + \sum_{k=1}^{m} (-\theta)^{k-1} \theta (Y_{t-k+1} - \mu)$$
‚ñ†

Essa reformula√ß√£o do lema simplifica a nota√ß√£o da previs√£o aproximada e facilita a an√°lise de suas propriedades.

Note tamb√©m que, para m grande e $|\theta| <1$, esta aproxima√ß√£o √© uma boa aproxima√ß√£o. No entanto, quando $ \theta $ est√° pr√≥ximo da unidade, pode ser uma aproxima√ß√£o ruim [^4.3]. Al√©m disso, quando o operador da m√©dia m√≥vel √© n√£o invert√≠vel, a previs√£o [^4.3.1] torna-se inadequada.

**Observa√ß√£o 1.** *Uma considera√ß√£o importante √© que a qualidade da aproxima√ß√£o depende tanto de $m$ quanto de $\theta$. Para valores de $\theta$ pr√≥ximos de 1,  √© crucial escolher um valor de $m$ suficientemente grande para garantir a precis√£o da previs√£o. Em outras palavras, a converg√™ncia da previs√£o aproximada para a previs√£o √≥tima √© mais lenta quando $\theta$ se aproxima de 1.*

Essa observa√ß√£o destaca a necessidade de considerar o valor dos par√¢metros do modelo ao aplicar a t√©cnica de previs√£o aproximada.

> üí° **Exemplo Num√©rico:** Vamos analisar o impacto de $\theta$ no desempenho da aproxima√ß√£o. Consideremos dois modelos MA(1) com diferentes valores de $\theta$. No primeiro caso, $\theta=0.9$, e no segundo, $\theta=0.2$. Assumimos $\mu = 0$ e temos uma amostra finita de tamanho $m=10$. Geramos dados sint√©ticos de um MA(1) com os par√¢metros dados e calculamos as previs√µes para $t+1$ usando a formula de aproxima√ß√£o com diferentes valores de $m$: $m = \{1,3,5,10\}$.

```python
import numpy as np
import matplotlib.pyplot as plt

def generate_ma1(n, theta, mu=0, sigma=1):
    epsilon = np.random.normal(0, sigma, n+100)
    y = np.zeros(n+100)
    for i in range(1, n+100):
       y[i] = mu + epsilon[i] + theta * epsilon[i-1]
    return y[100:]

def ma1_prediction_approx(y, theta, mu, m):
  n = len(y)
  y_hat = np.zeros(n)
  for t in range(m-1, n):
    y_hat[t] = mu + np.sum( [((-theta)**(k-1)) * theta * (y[t-k+1]-mu)  for k in range(1,min(m, t+1)+1)])
  return y_hat

# Example data generation and prediction for theta = 0.9
np.random.seed(42)
n = 200
theta_1 = 0.9
y_1 = generate_ma1(n, theta_1)
mu = 0
m_values = [1, 3, 5, 10]

predictions_1 = [ma1_prediction_approx(y_1, theta_1, mu, m) for m in m_values]

# Example data generation and prediction for theta = 0.2
theta_2 = 0.2
y_2 = generate_ma1(n, theta_2)
predictions_2 = [ma1_prediction_approx(y_2, theta_2, mu, m) for m in m_values]

# Visualization
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,6))
axes = axes.flatten()

titles = [f'm = {m}' for m in m_values]
for idx, ax in enumerate(axes):
  ax.plot(y_1[m_values[idx]-1:], label='Data', color='blue')
  ax.plot(predictions_1[idx][m_values[idx]-1:], label=f'Prediction (Œ∏=0.9)', color='red')
  ax.plot(y_2[m_values[idx]-1:], label='Data', color='green')
  ax.plot(predictions_2[idx][m_values[idx]-1:], label=f'Prediction (Œ∏=0.2)', color='orange')
  ax.set_title(titles[idx])
  ax.legend()

plt.tight_layout()
plt.show()

```

> Podemos observar que quando $\theta$ √© pr√≥ximo de 1 ($\theta=0.9$), a previs√£o melhora com o aumento de $m$, como previsto na Observa√ß√£o 1. Quando $\theta$ √© menor ($\theta=0.2$), a previs√£o converge mais rapidamente, pois o termo $\theta^k$ decai mais rapidamente, e portanto os valores passados tem menor peso na previs√£o.

### Conclus√£o
Este cap√≠tulo focou em estrat√©gias para realizar previs√µes quando o n√∫mero de observa√ß√µes √© finito. Abordamos a constru√ß√£o de aproxima√ß√µes para previs√µes √≥timas, especialmente considerando os erros pr√©-amostrais como zero.  Aproxima√ß√µes como essa permitem prever com um n√∫mero finito de dados, o que √© essencial em aplica√ß√µes pr√°ticas de previs√£o.  Ao derivar previs√µes aproximadas para um modelo MA(q) utilizando itera√ß√£o recursiva, ilustramos a implementa√ß√£o dessas estrat√©gias. O principal benef√≠cio desta abordagem √© que, em geral, a previs√£o se aproxima da previs√£o √≥tima quando o n√∫mero de dados aumenta [^4.3], fornecendo uma ferramenta pr√°tica para previs√µes em s√©ries temporais.

### Refer√™ncias
[^4.2]: Se√ß√£o 4.2 do documento original.
[^4.3]: Se√ß√£o 4.3 do documento original.
<!-- END -->
