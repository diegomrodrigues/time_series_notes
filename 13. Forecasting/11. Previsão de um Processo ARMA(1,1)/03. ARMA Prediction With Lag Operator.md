## Previs√£o para um Processo ARMA(1,1) Estacion√°rio e Invert√≠vel com o Operador de Retardo

### Introdu√ß√£o
Este cap√≠tulo explora a previs√£o de um processo **ARMA(1,1)** estacion√°rio e invert√≠vel, com um foco particular em como o operador de retardo, $L$, √© utilizado para expressar a rela√ß√£o entre as observa√ß√µes passadas e futuras. O objetivo √© derivar uma express√£o que relacione $Y_{t+s}$ com $Y_t$ e o erro $\epsilon_t$, onde $Y_t$ √© o valor da s√©rie temporal no tempo $t$, e $\epsilon_t$ √© o erro de ru√≠do branco. Construindo sobre os fundamentos estabelecidos no cap√≠tulo anterior sobre a previs√£o de um processo **ARMA(1,1)** com horizonte de *s* per√≠odos [^Anterior], expandiremos nossa an√°lise detalhando a import√¢ncia da estacionariedade e invertibilidade no contexto da proje√ß√£o linear.

### Conceitos Fundamentais
Um processo **ARMA(1,1)** √© caracterizado pela seguinte equa√ß√£o [^4.2.13]:
$$(1 - \phi L)(Y_t - \mu) = (1 + \theta L) \epsilon_t$$
onde $\phi$ √© o par√¢metro autorregressivo, $\theta$ √© o par√¢metro da m√©dia m√≥vel, $Y_t$ √© a s√©rie temporal, $\mu$ √© a m√©dia, $\epsilon_t$ √© o ru√≠do branco, e $L$ √© o operador de retardo.
Para que o processo **ARMA(1,1)** seja estacion√°rio e invert√≠vel, os seguintes requisitos devem ser atendidos [^4.2.37]:
- Estacionariedade: $|\phi| < 1$. Isso garante que as ra√≠zes do polin√¥mio autorregressivo estejam fora do c√≠rculo unit√°rio, o que significa que a s√©rie n√£o cresce indefinidamente ao longo do tempo e tem m√©dia e vari√¢ncia constantes.
- Invertibilidade: $|\theta| < 1$. Isso garante que o processo possa ser expresso em termos de seus erros passados, e √© fundamental para a previs√£o baseada em dados passados.

**Proposi√ß√£o 1** A condi√ß√£o de estacionariedade $|\phi|<1$ implica que a fun√ß√£o de autocorrela√ß√£o do processo ARMA(1,1) decai exponencialmente.
*Proof:*
   I. A fun√ß√£o de autocorrela√ß√£o $\rho(k)$ de um processo estacion√°rio descreve a correla√ß√£o entre as observa√ß√µes em diferentes tempos $k$.
   II. Para um processo AR(1), a fun√ß√£o de autocorrela√ß√£o √© dada por $\rho(k) = \phi^{|k|}$. A condi√ß√£o $|\phi| < 1$ implica que $\rho(k)$ decai exponencialmente para zero quando $|k|$ aumenta.
    III. Em um processo ARMA(1,1), a autocorrela√ß√£o inicial se comporta como um AR(1), e, portanto, sua fun√ß√£o tamb√©m exibir√° decaimento exponencial para valores de |k| suficientemente grandes.
    ‚ñ†
    
> üí° **Exemplo Num√©rico:**
> Suponha que temos um processo ARMA(1,1) com $\phi = 0.7$ e $\theta = 0.4$. A fun√ß√£o de autocorrela√ß√£o para um processo AR(1) √© dada por $\rho(k) = \phi^{|k|}$. Para os primeiros lags, temos:
>
> * $\rho(0) = 0.7^0 = 1$
> * $\rho(1) = 0.7^1 = 0.7$
> * $\rho(2) = 0.7^2 = 0.49$
> * $\rho(3) = 0.7^3 = 0.343$
>
> Como podemos ver, a autocorrela√ß√£o decai exponencialmente para zero, confirmando a proposi√ß√£o.

A previs√£o √≥tima para $Y_{t+s}$, dado o conjunto de informa√ß√µes em $t$, denotada por $\hat{Y}_{t+s|t}$, √© dada pela expectativa condicional $E(Y_{t+s}|Y_t, Y_{t-1}, \ldots)$ [^4.1.2].  Para um processo **ARMA(1,1)**, o processo de previs√£o √© linear e pode ser expresso usando o operador de retardo e os valores passados da s√©rie temporal. A express√£o geral para proje√ß√£o linear de $Y_{t+s}$ em $Y_t$ √© dada por [^4.2.16]:
$$ \hat{Y}_{t+s|t} = \mu + \left[ \frac{\psi(L)}{L^s} \frac{1}{\phi(L)} \right]_+ (Y_t - \mu) $$
onde $\psi(L) = 1 + \theta L$ e $\phi(L) = 1 - \phi L$ para o **ARMA(1,1)**. O termo $[...]_+$ significa que todos os termos envolvendo pot√™ncias negativas de $L$ s√£o zerados (annihilation operator).

Expandindo a equa√ß√£o acima para o caso de um processo **ARMA(1,1)** [^Anterior]:
$$ \hat{Y}_{t+s|t} = \mu + \left[ \frac{1+\theta L}{L^s} \right]_+ \frac{1}{1 - \phi L} (Y_t - \mu) $$

### Desenvolvimento
A chave para calcular a previs√£o em processos **ARMA** reside na manipula√ß√£o dos polin√¥mios de retardo. Expandindo o termo $\frac{1}{1-\phi L}$ como uma s√©rie geom√©trica [^Anterior]:
$$ \frac{1}{1 - \phi L} = \sum_{k=0}^{\infty} \phi^k L^k = 1 + \phi L + \phi^2 L^2 + \phi^3 L^3 + \ldots $$

O operador de retardo, $L$, quando aplicado em uma s√©rie temporal, desloca cada observa√ß√£o um per√≠odo para tr√°s. Ou seja, $L^kY_t = Y_{t-k}$ e $L^k\epsilon_t = \epsilon_{t-k}$.  Substituindo a s√©rie geom√©trica na equa√ß√£o de previs√£o [^4.2.16]:
$$ \hat{Y}_{t+s|t} = \mu + \left[ \frac{1+\theta L}{L^s} \right]_+ \left( \sum_{k=0}^{\infty} \phi^k L^k \right) (Y_t - \mu) $$
Para entender como a previs√£o √© constru√≠da, vamos considerar o caso em que $s=1$:
$$ \hat{Y}_{t+1|t} = \mu +  \left[ \frac{1+\theta L}{L} \right]_+ \left( \sum_{k=0}^{\infty} \phi^k L^k \right) (Y_t - \mu) $$
Aplicando o operador de aniquila√ß√£o, $\left[ \frac{1+\theta L}{L} \right]_+$:
$$ \left[ \frac{1+\theta L}{L} \right]_+ = \theta + L^{-1} $$
Considerando que $L^{-1}$ √© anulado pelo operador $+$, ent√£o $\left[ \frac{1+\theta L}{L} \right]_+= \theta $. Logo:
$$ \hat{Y}_{t+1|t} = \mu +  \theta \left( \sum_{k=0}^{\infty} \phi^k L^k \right) (Y_t - \mu) $$
Aplicando a propriedade de distributividade do operador $\sum_{k=0}^{\infty} \phi^k L^k$:
$$ \hat{Y}_{t+1|t} = \mu + \left( \sum_{k=0}^{\infty} \theta\phi^k L^k \right) (Y_t - \mu) $$
Aplicando o operador de retardo em $(Y_t-\mu)$:
$$ \hat{Y}_{t+1|t} = \mu +  \sum_{k=0}^{\infty} \theta\phi^k (Y_{t-k} - \mu) $$

> üí° **Exemplo Num√©rico:**
> Vamos supor que temos um processo ARMA(1,1) com $\phi = 0.5$, $\theta = 0.3$, e $\mu = 10$. Assumindo que observamos $Y_t = 12$, e o erro no instante $t$ √© $\epsilon_t = 0.5$. Vamos calcular a previs√£o para $Y_{t+1}$.
>
> Usando a f√≥rmula derivada, temos:
>
> $$\hat{Y}_{t+1|t} = \mu +  \sum_{k=0}^{\infty} \theta\phi^k (Y_{t-k} - \mu)$$
>
> Expandindo os primeiros termos da soma para fins ilustrativos:
>
> $$\hat{Y}_{t+1|t} = 10 + \theta\phi^0(Y_t - 10) + \theta\phi^1(Y_{t-1} - 10) + \theta\phi^2(Y_{t-2} - 10) + \ldots$$
>
> $$\hat{Y}_{t+1|t} = 10 + 0.3 * 0.5^0 (12 - 10) + 0.3 * 0.5^1 (Y_{t-1} - 10) +  0.3 * 0.5^2 (Y_{t-2} - 10) + \ldots$$
>
> $$\hat{Y}_{t+1|t} = 10 + 0.3 * 1 * 2 + 0.3 * 0.5 (Y_{t-1} - 10) + 0.3 * 0.25 (Y_{t-2} - 10) + \ldots$$
>
> $$\hat{Y}_{t+1|t} = 10 + 0.6 + 0.15 (Y_{t-1} - 10) + 0.075 (Y_{t-2} - 10) + \ldots$$
>
> Se assumirmos que o impacto dos termos mais antigos √© muito pequeno e podemos truncar a s√©rie, a previs√£o para um per√≠odo √† frente seria aproximadamente:
>
> $$\hat{Y}_{t+1|t} \approx 10.6$$
>
> Note que esse √© o resultado usando a expans√£o da s√©rie geom√©trica. Usando o Corol√°rio 2.1, temos:
>
> $$\hat{Y}_{t+1|t} = \mu + \phi (Y_{t} - \mu) + \theta \epsilon_{t}$$
>
> $$\hat{Y}_{t+1|t} = 10 + 0.5 * (12 - 10) + 0.3 * 0.5$$
>
> $$\hat{Y}_{t+1|t} = 10 + 0.5 * 2 + 0.15 = 11.15$$
>
> Essa diferen√ßa ocorre porque a expans√£o da s√©rie geom√©trica inclui todos os termos passados, enquanto o Corol√°rio 2.1 incorpora o efeito desses valores por meio do termo $\phi(Y_t - \mu)$.

O termo $\sum_{k=0}^{\infty} \phi^k (Y_{t-k} - \mu)$ √© uma combina√ß√£o linear ponderada das observa√ß√µes passadas, onde os pesos $\phi^k$ diminuem geometricamente no tempo.  Em termos pr√°ticos, a previs√£o de um per√≠odo √† frente, $\hat{Y}_{t+1|t}$, √© uma combina√ß√£o linear da m√©dia $\mu$ e das observa√ß√µes passadas, com pesos que dependem de $\phi$ e $\theta$.

Para $s > 1$, a express√£o  $\frac{1 + \theta L}{L^s}$ resulta em $\theta L^{1-s} + L^{-s}$. Ao aplicarmos o operador de aniquila√ß√£o, o termo $\theta L^{1-s}$ √© anulado para $s > 1$, pois $1-s<0$.  Dessa forma, temos:
$$ \hat{Y}_{t+s|t} = \mu + \left[ \frac{1}{L^{s-1}} \right]_+ \frac{1}{1 - \phi L} (Y_t - \mu) = \mu + \frac{\phi^{s-1}}{1-\phi L} (Y_t - \mu) $$
que expandindo com a expans√£o da s√©rie geom√©trica, temos:
$$ \hat{Y}_{t+s|t} = \mu + \phi^{s-1}\sum_{k=0}^{\infty} \phi^k (Y_{t-k}-\mu) $$
> üí° **Exemplo Num√©rico:**
>
> Retomando o exemplo anterior com $\phi = 0.5$, $\theta = 0.3$ e $\mu = 10$, vamos calcular a previs√£o para $Y_{t+2}$.
>
> Usando a f√≥rmula:
>
> $$\hat{Y}_{t+s|t} = \mu + \phi^{s-1}\sum_{k=0}^{\infty} \phi^k (Y_{t-k}-\mu)$$
>
> Para $s = 2$:
>
> $$\hat{Y}_{t+2|t} = 10 + 0.5^{2-1}\sum_{k=0}^{\infty} 0.5^k (Y_{t-k}-10)$$
>
> $$\hat{Y}_{t+2|t} = 10 + 0.5\sum_{k=0}^{\infty} 0.5^k (Y_{t-k}-10)$$
>
> Expandindo os primeiros termos:
>
> $$\hat{Y}_{t+2|t} = 10 + 0.5[1*(Y_t - 10) + 0.5*(Y_{t-1} - 10) + 0.25*(Y_{t-2} - 10) + \ldots]$$
>
> Usando a representa√ß√£o iterativa, temos:
>
> $$\hat{Y}_{t+2|t} = \mu + \phi(\hat{Y}_{t+1|t}-\mu)$$
>
> Lembrando que $\hat{Y}_{t+1|t} = 11.15$ e $\mu = 10$, temos:
>
> $$\hat{Y}_{t+2|t} = 10 + 0.5(11.15-10) = 10 + 0.5 * 1.15 = 10.575$$
>
> Isso demonstra como a previs√£o para $Y_{t+2}$ √© baseada na previs√£o de $Y_{t+1}$, juntamente com o par√¢metro $\phi$.

**Teorema 1** A previs√£o $\hat{Y}_{t+s|t}$ de um processo ARMA(1,1) converge para a m√©dia $\mu$ quando o horizonte de previs√£o $s$ aumenta para infinito.

*Proof:*

I. A express√£o para a previs√£o de um processo ARMA(1,1) com horizonte s √©:
$$\hat{Y}_{t+s|t} = \mu + \phi^{s-1}\sum_{k=0}^{\infty} \phi^k (Y_{t-k}-\mu)$$

II. Como $|\phi| < 1$ (condi√ß√£o de estacionariedade), $\lim_{s \to \infty} \phi^{s-1} = 0$.

III. Assim, a medida que $s \to \infty$, o termo $\phi^{s-1}$ multiplica a soma de pondera√ß√µes passadas $(Y_{t-k}-\mu)$, que √© finita quando $|\phi| < 1$.

IV. Portanto, $\lim_{s \to \infty} \hat{Y}_{t+s|t} = \mu$.

‚ñ†

> üí° **Exemplo Num√©rico:**
> Vamos usar o mesmo modelo com $\phi = 0.5$ e $\mu = 10$. Se calcularmos a previs√£o para v√°rios passos √† frente, vemos como $\phi^{s-1}$ se aproxima de zero e, consequentemente, a previs√£o se aproxima da m√©dia.
>
> *   Para $s=1$: $\hat{Y}_{t+1|t} = 10 + 0.5^0 \sum_{k=0}^{\infty} 0.5^k(Y_{t-k} - 10)$
> *   Para $s=5$: $\hat{Y}_{t+5|t} = 10 + 0.5^4 \sum_{k=0}^{\infty} 0.5^k(Y_{t-k} - 10) $
> *   Para $s=10$: $\hat{Y}_{t+10|t} = 10 + 0.5^9 \sum_{k=0}^{\infty} 0.5^k(Y_{t-k} - 10)$
>
> Como $0.5^4 = 0.0625$ e $0.5^9 \approx 0.00195$, o peso das observa√ß√µes passadas diminui rapidamente para zero, fazendo com que a previs√£o convirja para $\mu = 10$ quando *s* aumenta.

Note que a intui√ß√£o para horizonte $s>1$ √© a mesma, ou seja, a previs√£o converge para a m√©dia $\mu$ √† medida que $s$ cresce.

**Lema 2**
A representa√ß√£o iterativa da previs√£o $ \hat{Y}_{t+s|t} $ √© expressa em termos de previs√µes anteriores e erros [^4.2.41]:
$$ \hat{Y}_{t+s|t} = \mu + \phi (\hat{Y}_{t+s-1|t} - \mu) + \theta \epsilon_{t+s-1} $$
*Proof:*
I.  Partimos da equa√ß√£o do processo ARMA(1,1):
    $$ (1 - \phi L)(Y_t - \mu) = (1 + \theta L)\epsilon_t $$
    
II. Expandimos a equa√ß√£o:
    $$ Y_t - \mu - \phi(Y_{t-1} - \mu) = \epsilon_t + \theta \epsilon_{t-1} $$

III. Isolamos $\epsilon_t$:
    $$ \epsilon_t = (Y_t - \mu) - \phi(Y_{t-1} - \mu) - \theta \epsilon_{t-1} $$
    
IV. Aplicamos a expectativa condicional em $Y_{t+s}$:
    $$ E(Y_{t+s}|Y_t, Y_{t-1},\ldots) = E(\mu + \phi(Y_{t+s-1} - \mu) + \epsilon_{t+s} + \theta \epsilon_{t+s-1}) | Y_t, Y_{t-1},\ldots)$$
    
V. Utilizando a propriedade de linearidade da expectativa e o fato de que $E(\epsilon_{t+s}|Y_t, Y_{t-1},\ldots) = 0$ para $s > 0$ e $E(\epsilon_{t}|Y_t, Y_{t-1},\ldots) = \epsilon_{t}$, obtemos:
    $$ \hat{Y}_{t+s|t} = \mu + \phi E(Y_{t+s-1}|Y_t, Y_{t-1},\ldots)  + \theta E(\epsilon_{t+s-1}|Y_t, Y_{t-1},\ldots)$$
    $$ \hat{Y}_{t+s|t} = \mu + \phi (\hat{Y}_{t+s-1|t} - \mu) + \theta \epsilon_{t+s-1} $$

    ‚ñ†

Notavelmente, o erro $\epsilon_{t+s-1}$ √© igual a 0 para $s>1$ pois por defini√ß√£o √© um choque n√£o antecip√°vel. Assim, para s > 1, a previs√£o iterativa se torna:
$$ \hat{Y}_{t+s|t} = \mu + \phi (\hat{Y}_{t+s-1|t} - \mu)  $$
Este resultado tamb√©m demonstra como as previs√µes futuras s√£o constru√≠das iterativamente usando a previs√£o do per√≠odo anterior e os par√¢metros do modelo.

**Corol√°rio 2.1**  Quando $s=1$,  a representa√ß√£o iterativa de previs√£o √© expressa por:
$$ \hat{Y}_{t+1|t} = \mu + \phi (Y_{t} - \mu) + \theta \epsilon_{t} $$

*Proof:*
I. Do Lema 2, temos que $\hat{Y}_{t+s|t} = \mu + \phi (\hat{Y}_{t+s-1|t} - \mu) + \theta \epsilon_{t+s-1} $.

II. Fazendo s=1, temos $\hat{Y}_{t+1|t} = \mu + \phi (\hat{Y}_{t|t} - \mu) + \theta \epsilon_{t}$.
  
III. Por defini√ß√£o $\hat{Y}_{t|t} = Y_t$. Logo, $\hat{Y}_{t+1|t} = \mu + \phi (Y_{t} - \mu) + \theta \epsilon_{t}$.

‚ñ†

### Conclus√£o
Este cap√≠tulo demonstrou como derivar a previs√£o para um processo **ARMA(1,1)** estacion√°rio e invert√≠vel utilizando o operador de retardo, detalhando a rela√ß√£o entre os valores passados e futuros da s√©rie temporal. A estacionariedade e invertibilidade garantem que a s√©rie tenha momentos constantes ao longo do tempo, o que permite previs√µes confi√°veis e que o processo seja express√≠vel em termos de seus erros passados. A express√£o final para a previs√£o √© uma combina√ß√£o de valores passados ponderados que converge para a m√©dia da s√©rie temporal √† medida que o horizonte de previs√£o aumenta, o que representa um resultado intuitivo. Este resultado √© fundamental para entender a din√¢mica de previs√£o em modelos **ARMA** e para aplicar esses modelos na pr√°tica.

### Refer√™ncias
[^Anterior]:  Cap√≠tulo anterior sobre previs√£o de um processo ARMA(1,1) com horizonte de s per√≠odos.
[^4.1.2]:   Defini√ß√£o da previs√£o com base na expectativa condicional.
[^4.2.13]:  Equa√ß√£o que define o processo ARMA(1,1).
[^4.2.16]: F√≥rmula geral para proje√ß√£o linear.
[^4.2.37]: Defini√ß√£o da estacionariedade e invertibilidade de um processo ARMA(1,1).
[^4.2.41]: Representa√ß√£o iterativa da previs√£o.
<!-- END -->
