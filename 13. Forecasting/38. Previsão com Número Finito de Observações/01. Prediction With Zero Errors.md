## Previs√µes com um N√∫mero Finito de Observa√ß√µes: Aproxima√ß√µes para Previs√µes √ìtimas

### Introdu√ß√£o
Como vimos anteriormente, a teoria da previs√£o em s√©ries temporais assume frequentemente um n√∫mero infinito de observa√ß√µes. No entanto, em cen√°rios pr√°ticos, lidamos com um conjunto finito de dados. Este cap√≠tulo foca em como adaptar os m√©todos de previs√£o para lidar com um n√∫mero limitado de observa√ß√µes, um problema crucial para aplica√ß√µes pr√°ticas. Construindo sobre os conceitos de proje√ß√£o linear e esperan√ßa condicional apresentados anteriormente, exploraremos uma abordagem para lidar com amostras finitas.

### Conceitos Fundamentais
A se√ß√£o anterior apresentou m√©todos de previs√£o ideais para modelos ARMA, assumindo um n√∫mero infinito de observa√ß√µes, e que os par√¢metros da popula√ß√£o fossem conhecidos com certeza. No entanto, em situa√ß√µes reais, temos apenas um n√∫mero finito de dados, $\{Y_t, Y_{t-1}, ..., Y_{t-m+1}\}$, e tamb√©m enfrentamos incerteza sobre os par√¢metros da popula√ß√£o. Esta se√ß√£o continua assumindo que os par√¢metros da popula√ß√£o s√£o conhecidos com certeza, mas ir√° desenvolver previs√µes com base em um n√∫mero finito de observa√ß√µes.
Em modelos ARMA, precisamos de todos os valores hist√≥ricos de Y para implementar as f√≥rmulas das se√ß√µes anteriores. Em contraste, para um processo AR(p), a previs√£o linear √≥tima usa apenas os p valores mais recentes, $\{Y_t, Y_{t-1}, ..., Y_{t-p+1}\}$.

Uma forma de lidar com essa quest√£o √© tratar os erros pr√©-amostra como se todos fossem iguais a zero. A ideia √© usar a seguinte aproxima√ß√£o [^4.3.1]:
$$ \hat{E}(Y_{t+s} | Y_t, Y_{t-1},...) \approx \hat{E}(Y_{t+s} | Y_t, Y_{t-1},..., Y_{t-m+1}, \epsilon_{t-m} = 0, \epsilon_{t-m-1} = 0, ...) $$
onde $\epsilon_t$ representa o erro no instante t.

> üí° **Exemplo Num√©rico:** Considere um processo MA(1) onde $Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$, com $\mu = 5$, $\theta = 0.7$, e os erros $\epsilon_t$ seguem uma distribui√ß√£o normal com m√©dia 0 e desvio padr√£o 1. Suponha que tenhamos 3 observa√ß√µes: $Y_1 = 5.5$, $Y_2 = 6.2$, $Y_3 = 4.8$. Para prever $Y_4$, precisamos de $\epsilon_3$. Se inicializarmos $\epsilon_0 = \epsilon_{-1} = ... = 0$, podemos calcular $\hat{\epsilon}_1 = Y_1 - \mu = 5.5 - 5 = 0.5$. Seguindo, $\hat{\epsilon}_2 = Y_2 - \mu - \theta \hat{\epsilon}_1 = 6.2 - 5 - 0.7 * 0.5 = 0.85$. E finalmente,  $\hat{\epsilon}_3 = Y_3 - \mu - \theta \hat{\epsilon}_2 = 4.8 - 5 - 0.7 * 0.85 = -1.195$. Usando a aproxima√ß√£o, $\hat{Y}_{4|3} = \mu + \theta \hat{\epsilon}_3 = 5 + 0.7 * (-1.195) = 4.1665$. Esta aproxima√ß√£o trata os erros anteriores como zero, permitindo a previs√£o mesmo com um n√∫mero limitado de dados.

Essa abordagem permite a constru√ß√£o de previs√µes de forma recursiva. Especificamente, para um processo MA(q) , podemos inicializar a recurs√£o [^4.2.36] com:
$$ \hat{\epsilon}_{-m} = \hat{\epsilon}_{-m-1} = ... = \hat{\epsilon}_{t-m-q+1} = 0 $$
[4.3.2]
e ent√£o iterar para frente no tempo para gerar os valores $\hat{\epsilon}_{t-m+1}, \hat{\epsilon}_{t-m+2},..., \hat{\epsilon}_{t}$. Esses valores resultantes s√£o ent√£o inseridos diretamente em [^4.2.35] para criar a previs√£o. Por exemplo, para s = q = 1, a previs√£o seria:
$$ \hat{Y}_{t+1|t} = \mu + \theta(Y_t - \mu) - \theta^2(Y_{t-1} - \mu) + \theta^3(Y_{t-2} - \mu) + ... + (-1)^{m-1} \theta^m (Y_{t-m+1} - \mu) $$
[^4.3.3]

> üí° **Exemplo Num√©rico:** Considere um processo MA(1) com $\mu = 10$ e $\theta = 0.6$. Suponha que tenhamos as observa√ß√µes: $Y_1 = 10.5$, $Y_2 = 11.2$, $Y_3 = 9.8$. Para prever $Y_4$ usando a recurs√£o, com $m = 3$:
>
> $\hat{Y}_{4|3} = 10 + 0.6(9.8 - 10) - 0.6^2(11.2 - 10) + 0.6^3(10.5 - 10) = 10 + 0.6(-0.2) - 0.36(1.2) + 0.216(0.5) = 10 - 0.12 - 0.432 + 0.108 = 9.556$.
>
> Note que o impacto de observa√ß√µes mais antigas √© reduzido por $\theta^k$.

Essa express√£o √© usada como uma aproxima√ß√£o para a previs√£o de um processo AR($\infty$). Similarmente, para a representa√ß√£o AR($\infty$), a previs√£o seria:
$$ \mu + \theta(Y_t - \mu) - \theta^2(Y_{t-1} - \mu) + \theta^3(Y_{t-2} - \mu) \ldots $$
[^4.3.4]

Para m grande e $|\theta|$ pequeno, a aproxima√ß√£o de [^4.3.3] √© excelente. No entanto, para $|\theta|$ pr√≥ximo de 1, a aproxima√ß√£o pode n√£o ser t√£o precisa. √â importante notar que, se o operador de m√©dia m√≥vel for n√£o invert√≠vel, a previs√£o [^4.3.1] n√£o √© apropriada e n√£o deve ser usada [^4.3].

**Proposi√ß√£o 1** (Efeito do Tamanho da Amostra em MA(1)): Para um processo MA(1) com par√¢metro $\theta$, a aproxima√ß√£o recursiva para a previs√£o, dada por [^4.3.3], converge para a previs√£o √≥tima √† medida que o tamanho da amostra *m* aumenta, dado que $|\theta| < 1$.

*Prova:*
I. A previs√£o √≥tima para um MA(1) com um n√∫mero infinito de observa√ß√µes √© dada por:
    $$\hat{Y}_{t+1|t} = \mu + \sum_{j=0}^{\infty} \theta^{j+1} (Y_{t-j} - \mu)$$

II. A aproxima√ß√£o dada por [^4.3.3] com um n√∫mero finito *m* de observa√ß√µes √©:
    $$\hat{Y}_{t+1|t}^{(m)} = \mu + \sum_{j=0}^{m-1} (-1)^j \theta^{j+1} (Y_{t-j} - \mu)$$

III. Dado que o processo √© invert√≠vel, temos $|\theta| < 1$. Isso implica que a soma infinita na etapa I converge.

IV. √Ä medida que *m* tende ao infinito, a soma parcial na etapa II se aproxima da soma infinita na etapa I:
    $$\lim_{m \to \infty} \hat{Y}_{t+1|t}^{(m)} = \mu + \sum_{j=0}^{\infty} \theta^{j+1} (Y_{t-j} - \mu) = \hat{Y}_{t+1|t}$$
    Portanto, a aproxima√ß√£o recursiva converge para a previs√£o √≥tima √† medida que o tamanho da amostra *m* aumenta.
‚ñ†

Uma abordagem alternativa √© calcular a proje√ß√£o exata de $Y_{t+1}$ em seus m valores mais recentes. Seja $X_t$ definido como:
$$
X_t =
\begin{bmatrix}
1 \\ Y_t \\ Y_{t-1} \\ \vdots \\ Y_{t-m+1}
\end{bmatrix}
$$
Buscamos ent√£o uma previs√£o linear da forma [^4.3.5]:
$$ \alpha^{(m)'}X_t = \alpha_0^{(m)} + \alpha_1^{(m)}Y_t + \alpha_2^{(m)}Y_{t-1} + ... + \alpha_m^{(m)}Y_{t-m+1} $$
O coeficiente $\alpha_i^{(m)}$ relaciona $Y_{t+1}$ a $Y_{t-i}$ na proje√ß√£o de $Y_{t+1}$ nos m valores mais recentes de Y, e em geral, $\alpha_i^{(m)}$ n√£o √© igual a $\alpha_i^{(m+1)}$. Se $Y_t$ √© estacion√°ria, ent√£o $E(Y_tY_{t-j}) = \gamma_j + \mu^2$. Definindo $X_t$ = (1, $Y_t$, $Y_{t-1}$,..., $Y_{t-m+1}$)', o resultado [^4.1.13] implica que [^4.3.6]:
$$
    \alpha^{(m)} = \begin{bmatrix} \alpha_0^{(m)} & \alpha_1^{(m)} & \alpha_2^{(m)} & \ldots & \alpha_m^{(m)}  \end{bmatrix} =
     \begin{bmatrix} \mu & (\gamma_1 + \mu^2) & (\gamma_2 + \mu^2) & \ldots & (\gamma_m + \mu^2)  \end{bmatrix}
    \begin{bmatrix}
    \mu & \mu & \mu & \cdots & \mu \\
    \gamma_0 + \mu^2 & \gamma_1 + \mu^2 & \gamma_{m-1} + \mu^2 & \cdots & \gamma_m + \mu^2 \\
    \mu & \gamma_0 + \mu^2 & \gamma_{m-2} + \mu^2 & \cdots & \gamma_{m-1} + \mu^2 \\
     \vdots & \vdots & \vdots & \vdots \\
      \mu & \gamma_{m-1} + \mu^2 & \gamma_{m-2} + \mu^2 & \cdots & \gamma_0 + \mu^2
    \end{bmatrix}^{-1}
$$
Quando um termo constante est√° inclu√≠do em $X_t$, √© mais conveniente expressar as vari√°veis em desvios da m√©dia. Ent√£o, podemos calcular a proje√ß√£o de ($Y_{t+1}$ - $\mu$) em $X_t$ = [($Y_t$ - $\mu$), ($Y_{t-1}$ - $\mu$), ..., ($Y_{t-m+1}$ - $\mu$)]',
$$ \hat{Y}_{t+1|t} - \mu = \alpha_1^{(m)}(Y_t - \mu) + \alpha_2^{(m)}(Y_{t-1} - \mu) + ... + \alpha_m^{(m)}(Y_{t-m+1} - \mu) $$ [^4.3.7]
Para esta defini√ß√£o de $X_t$, os coeficientes podem ser calculados diretamente de [^4.1.13]:

$$
    \begin{bmatrix} \alpha_1^{(m)} \\ \alpha_2^{(m)} \\ \vdots \\ \alpha_m^{(m)}  \end{bmatrix} =
    \begin{bmatrix}
    \gamma_0 & \gamma_1 & \cdots & \gamma_{m-1} \\
    \gamma_1 & \gamma_0 & \cdots & \gamma_{m-2} \\
    \vdots & \vdots & \ddots & \vdots \\
    \gamma_{m-1} & \gamma_{m-2} & \cdots & \gamma_0
    \end{bmatrix}^{-1}
        \begin{bmatrix} \gamma_1 \\ \gamma_2 \\ \vdots \\ \gamma_m \end{bmatrix}
$$
[^4.3.8]
> üí° **Exemplo Num√©rico:** Considere um processo AR(1) com $\mu = 2$, $\phi = 0.8$, e $\sigma^2 = 1$. As autocovari√¢ncias s√£o dadas por $\gamma_k = \frac{\phi^k \sigma^2}{1-\phi^2}$. Ent√£o, $\gamma_0 = \frac{1}{1-0.8^2} = 2.778$, $\gamma_1 = 0.8 * \gamma_0 = 2.222$, $\gamma_2 = 0.8^2 * \gamma_0 = 1.778$. Para m=2, a matriz de autocovari√¢ncias √©:
>
>  $\Gamma_2 = \begin{bmatrix} \gamma_0 & \gamma_1 \\ \gamma_1 & \gamma_0 \end{bmatrix} = \begin{bmatrix} 2.778 & 2.222 \\ 2.222 & 2.778 \end{bmatrix}$.
>
> E o vetor de autocovari√¢ncias √©:
>
>  $\begin{bmatrix} \gamma_1 \\ \gamma_2 \end{bmatrix} = \begin{bmatrix} 2.222 \\ 1.778 \end{bmatrix}$.
>
>  Para calcular os coeficientes $\alpha_1^{(2)}$ e $\alpha_2^{(2)}$, precisamos inverter $\Gamma_2$.
>
>  $\Gamma_2^{-1} = \frac{1}{(2.778^2 - 2.222^2)}\begin{bmatrix} 2.778 & -2.222 \\ -2.222 & 2.778 \end{bmatrix} = \frac{1}{2.549} \begin{bmatrix} 2.778 & -2.222 \\ -2.222 & 2.778 \end{bmatrix} \approx \begin{bmatrix} 1.089 & -0.872 \\ -0.872 & 1.089 \end{bmatrix}$.
>
>  $\begin{bmatrix} \alpha_1^{(2)} \\ \alpha_2^{(2)} \end{bmatrix} = \begin{bmatrix} 1.089 & -0.872 \\ -0.872 & 1.089 \end{bmatrix} \begin{bmatrix} 2.222 \\ 1.778 \end{bmatrix} = \begin{bmatrix} 2.419 - 1.551 \\ -1.938 + 1.938 \end{bmatrix} = \begin{bmatrix} 0.868 \\ 0 \end{bmatrix}$
>
>  Ent√£o, $\hat{Y}_{t+1|t} - 2 = 0.868(Y_t - 2)$. Note que $\alpha_2$ √© 0, o que √© consistente com o processo AR(1), onde apenas o lag 1 √© relevante.

O uso de express√µes como [^4.3.8] requer a invers√£o de uma matriz ($m \times m$). Em geral, para gerar previs√µes s-per√≠odo-√†-frente, podemos usar:

$$ \hat{Y}_{t+s|t} = \mu + \alpha_1^{(m,s)}(Y_t-\mu) + \alpha_2^{(m,s)}(Y_{t-1}-\mu) + ... + \alpha_m^{(m,s)}(Y_{t-m+1} - \mu) $$
[^4.3.9]

onde
$$
\begin{bmatrix} \alpha_1^{(m,s)} \\ \alpha_2^{(m,s)} \\ \vdots \\ \alpha_m^{(m,s)}  \end{bmatrix} =
    \begin{bmatrix}
    \gamma_0 & \gamma_1 & \cdots & \gamma_{m-1} \\
    \gamma_1 & \gamma_0 & \cdots & \gamma_{m-2} \\
    \vdots & \vdots & \ddots & \vdots \\
    \gamma_{m-1} & \gamma_{m-2} & \cdots & \gamma_0
    \end{bmatrix}^{-1}
        \begin{bmatrix} \gamma_s \\ \gamma_{s+1} \\ \vdots \\ \gamma_{s+m-1} \end{bmatrix}
$$
V√°rios algoritmos podem ser usados para avaliar [^4.3.8] usando c√°lculos relativamente simples. Uma abordagem baseia-se no filtro de Kalman, discutido no Cap√≠tulo 13, que pode gerar previs√µes exatas em amostras finitas para uma ampla gama de processos, incluindo qualquer especifica√ß√£o ARMA. Uma segunda abordagem baseia-se na fatora√ß√£o triangular da matriz em [^4.3.8]. Essa segunda abordagem ser√° desenvolvida nas pr√≥ximas se√ß√µes.

**Lema 1** (Matriz de Covari√¢ncias Toeplitz): A matriz de covari√¢ncias $\Gamma_m = [\gamma_{|i-j|}]_{i,j=0}^{m-1}$  usada em [^4.3.8] e [^4.3.9] √© uma matriz de Toeplitz.

*Prova:*
I. A matriz $\Gamma_m$ √© definida como $[\gamma_{|i-j|}]_{i,j=0}^{m-1}$, onde $\gamma_{|i-j|}$ √© a autocovari√¢ncia entre $Y_{t-i}$ e $Y_{t-j}$.

II. Para um processo estacion√°rio, a autocovari√¢ncia $\gamma_{|i-j|}$ depende apenas da diferen√ßa absoluta entre os √≠ndices $|i-j|$.

III. Portanto, todos os elementos da matriz que t√™m a mesma diferen√ßa absoluta entre seus √≠ndices s√£o iguais, ou seja, $\gamma_{ij} = \gamma_{|i-j|}$.

IV. Uma matriz com essa propriedade, em que os elementos ao longo de cada diagonal s√£o constantes, √© definida como uma matriz de Toeplitz.
V. Logo, $\Gamma_m$ √© uma matriz de Toeplitz.
‚ñ†

**Teorema 1** (Propriedades da Proje√ß√£o Linear): A proje√ß√£o linear de $Y_{t+1}$ sobre os valores passados $\{Y_t, Y_{t-1}, ..., Y_{t-m+1}\}$ √© a melhor previs√£o linear de $Y_{t+1}$ no sentido de minimizar o erro quadr√°tico m√©dio. Al√©m disso, as aproxima√ß√µes recursivas fornecidas no in√≠cio desta se√ß√£o convergem para esta proje√ß√£o linear √† medida que o n√∫mero de observa√ß√µes consideradas (m) aumenta e sob certas condi√ß√µes de invertibilidade para modelos MA.

*Prova:*
I. Seja $L(Y_{t+1}|Y_t, Y_{t-1}, ..., Y_{t-m+1})$ a proje√ß√£o linear de $Y_{t+1}$ sobre os valores passados $\{Y_t, Y_{t-1}, ..., Y_{t-m+1}\}$.

II. Por defini√ß√£o, a proje√ß√£o linear minimiza o erro quadr√°tico m√©dio, ou seja, $E[(Y_{t+1} - L(Y_{t+1}|Y_t, Y_{t-1}, ..., Y_{t-m+1}))^2]$ √© minimizado.

III. A converg√™ncia das aproxima√ß√µes recursivas para a proje√ß√£o linear segue da Proposi√ß√£o 1 para o caso MA(1). Argumentos similares podem ser utilizados para processos ARMA mais gerais sob certas condi√ß√µes de invertibilidade. Especificamente, a aproxima√ß√£o recursiva, ao considerar mais valores hist√≥ricos (aumentando *m*), se aproxima da proje√ß√£o linear que usa todos os valores passados dispon√≠veis.

IV. Em resumo, a proje√ß√£o linear √© a melhor previs√£o linear em termos de erro quadr√°tico m√©dio, e as aproxima√ß√µes recursivas convergem para essa previs√£o √† medida que *m* aumenta, desde que as condi√ß√µes de invertibilidade sejam atendidas para modelos MA.
‚ñ†

### Conclus√£o
Este cap√≠tulo abordou a quest√£o de previs√µes com um n√∫mero finito de observa√ß√µes, com foco em uma aproxima√ß√£o que utiliza recurs√£o, o que computacionalmente significa inicializar o processo de previs√£o com zeros e iterar no tempo. Foram tamb√©m introduzidas aproxima√ß√µes para calcular a proje√ß√£o linear exata de $Y_{t+1}$ sobre seus $m$ valores mais recentes, onde os coeficientes podem ser calculados atrav√©s de invers√£o matricial. O conceito de fatoriza√ß√£o triangular de matrizes foi introduzido, e nos pr√≥ximos cap√≠tulos, vamos ver como essa t√©cnica pode ser usada para calcular previs√µes mais eficientemente. Estas abordagens s√£o cruciais quando se trabalha com dados reais, fornecendo meios pr√°ticos para lidar com conjuntos de dados finitos, e se baseiam e complementam os conceitos de proje√ß√£o linear e esperan√ßa condicional discutidos nos cap√≠tulos anteriores.

### Refer√™ncias
[^4.3.1]:  *‚ÄúOne approach to forecasting based on a finite number of observations is to act as if presample e's were all equal to zero.‚Äù*
[^4.3.2]: *‚ÄúThe recursion [4.2.36] can be started by setting ...‚Äù*
[^4.2.35]: *‚Äú...are then substituted directly into [4.2.35] to produce the forecast [4.3.1]‚Äù*
[^4.3.3]: *‚ÄúFor example, for s = q = 1, the forecast would be...‚Äù*
[^4.3.4]:  *‚Äú...as an approximation to the AR(‚àû) forecast...‚Äù*
[^4.3]:  *‚ÄúNote that if the moving average operator is noninvertible, the forecast [4.3.1] is inappropriate and should not be used.‚Äù*
[^4.3.5]: *‚ÄúWe thus seek a linear forecast of the form ...‚Äù*
[^4.1.13]: *‚Äú...implies‚Äù*
[^4.3.6]: *‚ÄúŒ±(m) = [Œ±(m)0 Œ±(m)1 ... Œ±(m)m] ...‚Äù*
[^4.3.7]: *‚ÄúFor this definition of X, the coefficients can be calculated directly from [4.1.13] to be ...‚Äù*
[^4.1.13]: *‚Äú...the coefficient vector Œ± is not uniquely determined by [4.1.10], though the product of this vector with the explanatory variables, Œ±‚Ä≤X_t, is uniquely determined by [4.1.10].‚Äù*
[^4.3.8]: *‚Äú...can be calculated directly from [4.1.13] to be‚Äù*
[^4.3.9]: *‚ÄúTo generate an s-period-ahead forecast ≈∂_t+s|t, we would use‚Äù*
[^4.2.36]: *‚Äú...where ≈ù, can be characterized by the recursion...‚Äù*
<!-- END -->
