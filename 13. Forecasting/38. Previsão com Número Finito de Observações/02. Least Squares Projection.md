## Proje√ß√£o Linear Exata com Amostras Finitas e Fatora√ß√£o Triangular

### Introdu√ß√£o

Como vimos no cap√≠tulo anterior, a obten√ß√£o de proje√ß√µes lineares exatas em amostras finitas envolve a manipula√ß√£o e invers√£o de matrizes de autocovari√¢ncias. Construindo sobre os conceitos de aproxima√ß√µes para previs√µes √≥timas e as proje√ß√µes lineares com amostras finitas, este cap√≠tulo explora em detalhe os m√©todos para calcular a proje√ß√£o exata em amostras finitas, com foco na formula√ß√£o de m√≠nimos quadrados implementada atrav√©s da invers√£o de matrizes, e como algoritmos de fatora√ß√£o triangular, como a fatora√ß√£o de Cholesky, podem tornar a opera√ß√£o computacionalmente mais eficiente para matrizes sim√©tricas e positivas definidas. Este t√≥pico √© crucial para o entendimento de m√©todos eficientes e pr√°ticos em previs√µes de s√©ries temporais.

### Formula√ß√£o de M√≠nimos Quadrados e Invers√£o de Matrizes

A abordagem para calcular a proje√ß√£o linear exata, conforme discutido no cap√≠tulo anterior, envolve a minimiza√ß√£o do erro quadr√°tico m√©dio (MSE) [^4.3.7], o que leva √† necessidade de inverter matrizes de autocovari√¢ncias [^4.3.8], [^4.3.9]. Essa invers√£o matricial √© um passo computacionalmente intensivo, especialmente para grandes conjuntos de dados. Especificamente, dados m valores passados de Y, o c√°lculo dos coeficientes $\alpha^{(m)}$ na proje√ß√£o linear exige a invers√£o da matriz de Toeplitz, como mostrado no Lema 1 do cap√≠tulo anterior, que cont√©m os momentos de segunda ordem do processo [^4.3.8].

Formalmente, seja $\Gamma_m$ a matriz de autocovari√¢ncias de dimens√£o $m \times m$:
$$
\Gamma_m = \begin{bmatrix}
\gamma_0 & \gamma_1 & \cdots & \gamma_{m-1} \\
\gamma_1 & \gamma_0 & \cdots & \gamma_{m-2} \\
\vdots & \vdots & \ddots & \vdots \\
\gamma_{m-1} & \gamma_{m-2} & \cdots & \gamma_0
\end{bmatrix}
$$

O vetor de coeficientes para a proje√ß√£o linear de $Y_{t+1}$ sobre os m valores passados de $Y$, denotado por $\alpha^{(m)}$, √© dado por:
$$
\begin{bmatrix} \alpha_1^{(m)} \\ \alpha_2^{(m)} \\ \vdots \\ \alpha_m^{(m)}  \end{bmatrix} =
    \Gamma_m^{-1}
        \begin{bmatrix} \gamma_1 \\ \gamma_2 \\ \vdots \\ \gamma_m \end{bmatrix}
$$

Onde $\gamma_i$ s√£o as autocovari√¢ncias do processo Y, e $\Gamma_m^{-1}$ √© a inversa da matriz de Toeplitz $\Gamma_m$. Em problemas pr√°ticos, a invers√£o direta de grandes matrizes pode ser computacionalmente custosa. Abordagens para calcular essas proje√ß√µes de forma mais eficiente s√£o cruciais para a aplicabilidade dos m√©todos apresentados neste curso.

**Lema 1.1** (Propriedades da Matriz de Autocovari√¢ncias): A matriz de autocovari√¢ncias $\Gamma_m$ √© sempre sim√©trica e positiva semi-definida. Se o processo Y for estacion√°rio e o espectro do processo for diferente de zero, ent√£o $\Gamma_m$ √© positiva definida.

*Prova:*

I.  **Simetria:** A simetria de $\Gamma_m$ segue diretamente da defini√ß√£o de autocovari√¢ncia: $\gamma_i = Cov(Y_t, Y_{t-i}) = Cov(Y_{t-i}, Y_t) = \gamma_{-i}$. Para processos estacion√°rios, $\gamma_{-i} = \gamma_i$.
II.  **Positiva Semi-Definida:** Para qualquer vetor $x \in \mathbb{R}^m$, temos $x'\Gamma_m x = \sum_{i=1}^m \sum_{j=1}^m x_i x_j \gamma_{i-j}$. Utilizando a defini√ß√£o de covari√¢ncia e assumindo a linearidade da covari√¢ncia, podemos reescrever como $x'\Gamma_m x = Cov(\sum_{i=1}^m x_i Y_{t-i}, \sum_{j=1}^m x_j Y_{t-j})$. Como a covari√¢ncia de qualquer vari√°vel aleat√≥ria com ela mesma √© sempre maior ou igual a zero, ent√£o $x'\Gamma_m x \ge 0$.
III. **Positiva Definida:** Se o processo Y for estacion√°rio e o espectro do processo for diferente de zero, ent√£o a matriz $\Gamma_m$ √© positiva definida. Isso significa que $x'\Gamma_m x > 0$ para qualquer $x \neq 0$. A condi√ß√£o espectral garante que n√£o exista vetor n√£o nulo que fa√ßa a vari√¢ncia da combina√ß√£o linear de Y seja zero.
‚ñ†

Este lema garante que a matriz $\Gamma_m$ tem as propriedades necess√°rias para a aplica√ß√£o da fatora√ß√£o de Cholesky, como veremos a seguir.

### Fatora√ß√£o Triangular e Algoritmo de Cholesky

Uma alternativa para a invers√£o direta de matrizes √© a decomposi√ß√£o ou fatora√ß√£o triangular, onde a matriz original √© decomposta em matrizes triangulares mais simples [^4.4.1]. Em particular, para matrizes sim√©tricas e positivas definidas, a fatora√ß√£o de Cholesky fornece uma maneira eficiente de realizar essa decomposi√ß√£o.

A **fatora√ß√£o de Cholesky** de uma matriz sim√©trica e positiva definida $\Omega$ (como as matrizes de autocovari√¢ncia em nossos problemas) decomp√µe $\Omega$ no produto de uma matriz triangular inferior $A$ com 1s na diagonal principal, uma matriz diagonal $D$ e a transposta de $A$, $A'$.
$$ \Omega = ADA' $$
onde:
*   $A$ √© uma matriz triangular inferior com 1s na diagonal principal [^4.4.1].
*   $D$ √© uma matriz diagonal com entradas positivas na diagonal principal [^4.4.1].

A decomposi√ß√£o de Cholesky √© √∫nica e fornece uma alternativa para a invers√£o direta de $\Omega$ [^4.4.14]. Uma vantagem computacional not√°vel √© que, ao inv√©s de calcular $\Omega^{-1}$ diretamente, resolve-se um sistema triangular mais simples, o que √© computacionalmente mais r√°pido.

No contexto do c√°lculo dos coeficientes da proje√ß√£o linear, podemos aplicar a fatora√ß√£o de Cholesky na matriz de autocovari√¢ncias $\Gamma_m$. Seja $\Gamma_m = A_m D_m A_m'$. Podemos ent√£o obter a inversa de $\Gamma_m$, dado que:
$$
\Gamma_m^{-1} = (A_m')^{-1}D_m^{-1}A_m^{-1}
$$
Dessa forma, ao inv√©s de realizar uma invers√£o matricial direta, realiza-se as opera√ß√µes de solu√ß√£o de sistemas lineares utilizando as matrizes triangulares ($A_m$ e $A_m'$) e uma divis√£o por uma matriz diagonal ($D_m^{-1}$).

O processo de fatora√ß√£o de Cholesky envolve os seguintes passos:

1. **Inicializa√ß√£o:**  Decompor a matriz de autocovari√¢ncias, $\Gamma_m$, em uma matriz triangular inferior $A$, uma matriz diagonal $D$ e a transposta de $A$, $A'$.
2. **Itera√ß√£o:** Usando o m√©todo descrito em [^4.4.3] at√© [^4.4.8], os elementos de $A$ e $D$ s√£o calculados recursivamente, coluna por coluna.
3. **Solu√ß√£o:** A inversa da matriz de autocovari√¢ncias √© ent√£o calculada usando as matrizes triangulares e diagonal: $\Gamma_m^{-1} = (A_m')^{-1}D_m^{-1}A_m^{-1}$.

> üí° **Exemplo Num√©rico:** Retomando o exemplo do cap√≠tulo anterior, com um processo AR(1) com $\mu = 2$, $\phi = 0.8$, e $\sigma^2 = 1$, a matriz de autocovari√¢ncias para m=2 √©:
> $\Gamma_2 = \begin{bmatrix} 2.778 & 2.222 \\ 2.222 & 2.778 \end{bmatrix}$.
>
> Fatorando $\Gamma_2$ utilizando Cholesky, podemos encontrar as matrizes $A$ e $D$:
>
> $d_{11} = 2.778$, $a_{21} = \frac{2.222}{2.778} = 0.8$, e $d_{22} = 2.778 - \frac{2.222^2}{2.778} = 2.778 - 1.778 = 1.000$, ent√£o:
>
> $A = \begin{bmatrix} 1 & 0 \\ 0.8 & 1 \end{bmatrix}$,  $D = \begin{bmatrix} 2.778 & 0 \\ 0 & 1 \end{bmatrix}$.
>
> Para obter a inversa, usamos $ \Gamma_2^{-1} = (A')^{-1} D^{-1} A^{-1} $ .
>
> $ A^{-1} =  \begin{bmatrix} 1 & 0 \\ -0.8 & 1 \end{bmatrix} $, $(A')^{-1} = \begin{bmatrix} 1 & -0.8 \\ 0 & 1 \end{bmatrix} $ e  $ D^{-1} = \begin{bmatrix} 0.36 & 0 \\ 0 & 1 \end{bmatrix} $.
>
> Portanto,
>
> $\Gamma_2^{-1} = \begin{bmatrix} 1 & -0.8 \\ 0 & 1 \end{bmatrix}  \begin{bmatrix} 0.36 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ -0.8 & 1 \end{bmatrix} = \begin{bmatrix} 1.089 & -0.872 \\ -0.872 & 1.089 \end{bmatrix} $.
>
> Os coeficientes s√£o calculados como antes:
>
> $\begin{bmatrix} \alpha_1^{(2)} \\ \alpha_2^{(2)} \end{bmatrix} = \begin{bmatrix} 1.089 & -0.872 \\ -0.872 & 1.089 \end{bmatrix} \begin{bmatrix} 2.222 \\ 1.778 \end{bmatrix} = \begin{bmatrix} 0.868 \\ 0 \end{bmatrix}$

> üí° **Exemplo Num√©rico:** Vamos considerar um processo AR(2) com $\phi_1=0.7$, $\phi_2=0.2$, e $\sigma^2=1$, e calcular os coeficientes da proje√ß√£o linear usando fatora√ß√£o de Cholesky para $m=3$.  Primeiro, precisamos calcular a matriz de autocovari√¢ncias $\Gamma_3$ e o vetor de autocovari√¢ncias $\gamma$:
>
>  $\gamma_0 = \frac{\sigma^2}{1 - \phi_1^2 - \phi_2^2 - 2\phi_1\phi_2^2 / (1 - \phi_2)} = \frac{1}{1 - 0.7^2 - 0.2^2 - 2*0.7*0.2^2/(1-0.2)} \approx 2.00 $
>  $\gamma_1 = \phi_1\gamma_0 + \phi_2\gamma_1 = 0.7 * 2 + 0.2 * 0 = 1.4$
>  $\gamma_2 = \phi_1\gamma_1 + \phi_2\gamma_0 = 0.7 * 1.4 + 0.2 * 2 = 1.38$
>  $\gamma_3 = \phi_1\gamma_2 + \phi_2\gamma_1 = 0.7 * 1.38 + 0.2 * 1.4 = 1.246$
>
> $\Gamma_3 = \begin{bmatrix} 2.00 & 1.4 & 1.38 \\ 1.4 & 2.00 & 1.4 \\ 1.38 & 1.4 & 2.00 \end{bmatrix}$  e  $ \begin{bmatrix} \gamma_1 \\ \gamma_2 \\ \gamma_3 \end{bmatrix} = \begin{bmatrix} 1.4 \\ 1.38 \\ 1.246 \end{bmatrix} $
>
> Agora, vamos realizar a fatora√ß√£o de Cholesky:
>
> $\text{Step 1: } d_{11} = 2.00$
> $\text{Step 2: } a_{21} = \frac{1.4}{2.0} = 0.7, a_{31} = \frac{1.38}{2.0} = 0.69$
> $\text{Step 3: } d_{22} = 2.00 - (0.7)^2*2.0 = 1.02$
> $\text{Step 4: } a_{32} = \frac{1.4 - (0.7)*(1.38)}{1.02} = 0.36$
> $\text{Step 5: } d_{33} = 2.0 - (0.69)^2*2.0 - (0.36)^2*1.02 = 0.92$
>
> $A = \begin{bmatrix} 1 & 0 & 0 \\ 0.7 & 1 & 0 \\ 0.69 & 0.36 & 1 \end{bmatrix}$,  $D = \begin{bmatrix} 2.0 & 0 & 0 \\ 0 & 1.02 & 0 \\ 0 & 0 & 0.92 \end{bmatrix}$.
>
> Calculando as inversas:
>
> $A^{-1} = \begin{bmatrix} 1 & 0 & 0 \\ -0.7 & 1 & 0 \\ -0.43  & -0.36 & 1 \end{bmatrix}$,
> $(A')^{-1} = \begin{bmatrix} 1 & -0.7 & -0.43  \\ 0 & 1 & -0.36 \\ 0 & 0 & 1 \end{bmatrix}$,
> $D^{-1} = \begin{bmatrix} 0.5 & 0 & 0 \\ 0 & 0.98 & 0 \\ 0 & 0 & 1.09 \end{bmatrix}$.
>
> $\Gamma_3^{-1} = (A')^{-1} D^{-1} A^{-1} = \begin{bmatrix} 0.757 & -0.397 & -0.165 \\ -0.397 & 1.124 & -0.395 \\ -0.165 & -0.395 & 1.21 \end{bmatrix}$
>
>  Finalmente, os coeficientes da proje√ß√£o linear:
>
> $\begin{bmatrix} \alpha_1^{(3)} \\ \alpha_2^{(3)} \\ \alpha_3^{(3)} \end{bmatrix} = \begin{bmatrix} 0.757 & -0.397 & -0.165 \\ -0.397 & 1.124 & -0.395 \\ -0.165 & -0.395 & 1.21 \end{bmatrix}  \begin{bmatrix} 1.4 \\ 1.38 \\ 1.246 \end{bmatrix} = \begin{bmatrix} 0.694 \\ 0.202 \\ 0.003 \end{bmatrix}$
>
> Os coeficientes da proje√ß√£o linear para $m=3$ s√£o aproximadamente $\alpha_1 = 0.694$, $\alpha_2 = 0.202$, e $\alpha_3 = 0.003$.

A fatora√ß√£o de Cholesky, portanto, proporciona um meio computacionalmente mais eficiente para calcular a inversa da matriz de autocovari√¢ncias, o que, por sua vez, permite uma avalia√ß√£o mais eficiente dos coeficientes da proje√ß√£o linear.

**Proposi√ß√£o 2** (Efici√™ncia Computacional da Fatora√ß√£o de Cholesky):  Para uma matriz sim√©trica e positiva definida de dimens√£o $n \times n$, a fatora√ß√£o de Cholesky √© mais eficiente computacionalmente do que a invers√£o direta, tendo uma complexidade computacional de $O(n^3/3)$ em compara√ß√£o com $O(n^3)$ da invers√£o direta.

*Prova:*

I. **Invers√£o Direta:** A invers√£o direta de uma matriz $n \times n$ geralmente envolve m√©todos como elimina√ß√£o gaussiana ou decomposi√ß√£o LU, que t√™m uma complexidade computacional de $O(n^3)$. Isso ocorre porque, em cada etapa da elimina√ß√£o gaussiana, √© necess√°rio realizar opera√ß√µes em todas as linhas e colunas da matriz, o que leva a uma complexidade de ordem c√∫bica em rela√ß√£o √† dimens√£o da matriz.

II. **Fatora√ß√£o de Cholesky:**
    a. A fatora√ß√£o de Cholesky, para calcular $A$ e $D$, envolve opera√ß√µes que podem ser realizadas coluna por coluna da matriz original.
    b. Para cada coluna $k$, o c√°lculo dos elementos de $A$ e $D$ envolve somat√≥rios e divis√µes que dependem dos elementos das colunas anteriores. A complexidade total para uma matriz $n \times n$ √© dada por:
     $$\sum_{k=1}^{n}  \left(  \sum_{i=k}^{n} (i-k+1)  \right) =  \frac{n^3}{3} + O(n^2)$$
    c. A solu√ß√£o de sistemas triangulares ap√≥s a decomposi√ß√£o tem complexidade $O(n^2)$, significativamente menor, uma vez que √© necess√°rio apenas realizar substitui√ß√µes para encontrar a solu√ß√£o.

III. **Compara√ß√£o:** Portanto, a fatora√ß√£o de Cholesky tem uma complexidade computacional menor em compara√ß√£o com a invers√£o direta, o que a torna mais eficiente para grandes matrizes. A economia computacional √© significativa √† medida que $n$ aumenta.
‚ñ†

**Observa√ß√£o 2.1:** Al√©m da efici√™ncia computacional, a fatora√ß√£o de Cholesky tamb√©m possui vantagens em termos de estabilidade num√©rica, especialmente quando comparada com a invers√£o direta de matrizes que podem ser mal condicionadas. A natureza triangular das matrizes resultantes da fatora√ß√£o facilita a solu√ß√£o de sistemas lineares, o que √© menos propenso a erros de arredondamento.

**Teorema 2.1** (Unicidade da Fatora√ß√£o de Cholesky): A fatora√ß√£o de Cholesky, se existir, √© √∫nica. Ou seja, para uma matriz sim√©trica e positiva definida $\Omega$, existe uma √∫nica matriz triangular inferior $A$ com 1s na diagonal e uma √∫nica matriz diagonal $D$ com entradas positivas tal que $\Omega = ADA'$.

*Prova:*
I. Assume-se que existem duas fatora√ß√µes de Cholesky para $\Omega$: $\Omega = A_1 D_1 A_1'$ e $\Omega = A_2 D_2 A_2'$.
II. Multiplicando a primeira equa√ß√£o por $A_2'^{-1}$ √† direita e a segunda equa√ß√£o por $A_2^{-1}$ √† esquerda, temos:
    $$ \Omega A_2'^{-1} = A_1 D_1 A_1' A_2'^{-1} $$
    $$ A_2^{-1} \Omega = A_2^{-1} A_1 D_1 A_1' $$
III. Substituindo o valor de $\Omega$ da primeira equa√ß√£o na segunda, obtemos:
$$ A_2^{-1} A_1 D_1 A_1' = A_2^{-1} A_1 D_1 A_1' A_2'^{-1} A_2' $$
$$ A_2^{-1} A_1 D_1 = D_2 A_2' A_1'^{-1} $$
IV. Definindo $X = A_2^{-1} A_1$ e $Y = A_2' A_1'^{-1}$, a equa√ß√£o acima se torna $X D_1 = D_2 Y$. Notavelmente, como $A_1$ e $A_2$ s√£o matrizes triangulares inferiores com 1s na diagonal, $X = A_2^{-1} A_1$ tamb√©m √© triangular inferior com 1s na diagonal, e $Y = A_2' A_1'^{-1} = (A_1 A_2^{-1})'$ √© triangular superior com 1s na diagonal.
V. Transpondo a equa√ß√£o, temos $D_1 X' = Y' D_2$. Dado que $D_1$ e $D_2$ s√£o diagonais e $X'$ √© triangular superior com 1s na diagonal, temos que $D_1 X'$ √© uma matriz triangular superior.  Similarmente $Y'D_2$ √© triangular inferior, e portanto, a √∫nica solu√ß√£o poss√≠vel √© que $X$ e $Y$ s√£o matrizes diagonais com 1s na diagonal, ou seja, matrizes identidades.
VI. Assim, $X=I$, implicando em $A_2^{-1} A_1 = I$ ou $A_1 = A_2$.
VII. Substituindo $A_1 = A_2$ em $\Omega = A_1 D_1 A_1'$ e $\Omega = A_2 D_2 A_2'$, temos $A_1 D_1 A_1' = A_1 D_2 A_1'$. Multiplicando por $A_1^{-1}$ √† esquerda e $(A_1')^{-1}$ √† direita, obtemos $D_1 = D_2$. Portanto, as decomposi√ß√µes s√£o iguais e a fatora√ß√£o de Cholesky √© √∫nica.
‚ñ†

### Conclus√£o

Este cap√≠tulo aprofundou o entendimento do c√°lculo da proje√ß√£o linear exata, com amostras finitas. Foi detalhado como a formula√ß√£o de m√≠nimos quadrados leva a opera√ß√µes de invers√£o de matrizes de autocovari√¢ncias e como a fatora√ß√£o triangular, especialmente a fatora√ß√£o de Cholesky, se apresenta como uma alternativa computacionalmente eficiente, por simplificar a opera√ß√£o em sistemas lineares triangulares, em vez da invers√£o matricial direta. A compreens√£o desses m√©todos n√£o apenas fornece uma base te√≥rica s√≥lida, mas tamb√©m instrumentaliza a aplica√ß√£o pr√°tica de previs√µes de s√©ries temporais de maneira eficiente. Essa combina√ß√£o de teoria e m√©todos pr√°ticos √© crucial para o avan√ßo do estudo da an√°lise de s√©ries temporais e de suas aplica√ß√µes.

### Refer√™ncias

[^4.3.7]: *‚Äú...Then we could calculate the projection of ...‚Äù*
[^4.3.8]: *‚ÄúFor this definition of X, the coefficients can be calculated directly from [4.1.13] to be ...‚Äù*
[^4.3.9]: *‚ÄúTo generate an s-period-ahead forecast ≈∂_t+s|t, we would use‚Äù*
[^4.4.1]: *‚ÄúAny positive definite symmetric (n √ó n) matrix Œ© has a unique representation of the form ...‚Äù*
[^4.4.14]: *‚ÄúWe next establish that the triangular factorization is unique.‚Äù*
[^4.4.3]: *‚ÄúThe matrix Œ© can be transformed into a matrix with zero in the (2, 1) position by multiplying the first row of Œ© by Œ©_21Œ©_11^-1 and subtracting the resulting row from the second.‚Äù*
[^4.4.8]:  *‚ÄúThus each E_j is lower triangular with unit determinant. Hence E_j^-1 exists, and the following matrix exists:‚Äù*
<!-- END -->
