## Projeção Linear Exata com Amostras Finitas e Fatoração Triangular

### Introdução

Como vimos no capítulo anterior, a obtenção de projeções lineares exatas em amostras finitas envolve a manipulação e inversão de matrizes de autocovariâncias. Construindo sobre os conceitos de aproximações para previsões ótimas e as projeções lineares com amostras finitas, este capítulo explora em detalhe os métodos para calcular a projeção exata em amostras finitas, com foco na formulação de mínimos quadrados implementada através da inversão de matrizes, e como algoritmos de fatoração triangular, como a fatoração de Cholesky, podem tornar a operação computacionalmente mais eficiente para matrizes simétricas e positivas definidas. Este tópico é crucial para o entendimento de métodos eficientes e práticos em previsões de séries temporais.

### Formulação de Mínimos Quadrados e Inversão de Matrizes

A abordagem para calcular a projeção linear exata, conforme discutido no capítulo anterior, envolve a minimização do erro quadrático médio (MSE) [^4.3.7], o que leva à necessidade de inverter matrizes de autocovariâncias [^4.3.8], [^4.3.9]. Essa inversão matricial é um passo computacionalmente intensivo, especialmente para grandes conjuntos de dados. Especificamente, dados m valores passados de Y, o cálculo dos coeficientes $\alpha^{(m)}$ na projeção linear exige a inversão da matriz de Toeplitz, como mostrado no Lema 1 do capítulo anterior, que contém os momentos de segunda ordem do processo [^4.3.8].

Formalmente, seja $\Gamma_m$ a matriz de autocovariâncias de dimensão $m \times m$:
$$
\Gamma_m = \begin{bmatrix}
\gamma_0 & \gamma_1 & \cdots & \gamma_{m-1} \\
\gamma_1 & \gamma_0 & \cdots & \gamma_{m-2} \\
\vdots & \vdots & \ddots & \vdots \\
\gamma_{m-1} & \gamma_{m-2} & \cdots & \gamma_0
\end{bmatrix}
$$

O vetor de coeficientes para a projeção linear de $Y_{t+1}$ sobre os m valores passados de $Y$, denotado por $\alpha^{(m)}$, é dado por:
$$
\begin{bmatrix} \alpha_1^{(m)} \\ \alpha_2^{(m)} \\ \vdots \\ \alpha_m^{(m)}  \end{bmatrix} =
    \Gamma_m^{-1}
        \begin{bmatrix} \gamma_1 \\ \gamma_2 \\ \vdots \\ \gamma_m \end{bmatrix}
$$

Onde $\gamma_i$ são as autocovariâncias do processo Y, e $\Gamma_m^{-1}$ é a inversa da matriz de Toeplitz $\Gamma_m$. Em problemas práticos, a inversão direta de grandes matrizes pode ser computacionalmente custosa. Abordagens para calcular essas projeções de forma mais eficiente são cruciais para a aplicabilidade dos métodos apresentados neste curso.

**Lema 1.1** (Propriedades da Matriz de Autocovariâncias): A matriz de autocovariâncias $\Gamma_m$ é sempre simétrica e positiva semi-definida. Se o processo Y for estacionário e o espectro do processo for diferente de zero, então $\Gamma_m$ é positiva definida.

*Prova:*

I.  **Simetria:** A simetria de $\Gamma_m$ segue diretamente da definição de autocovariância: $\gamma_i = Cov(Y_t, Y_{t-i}) = Cov(Y_{t-i}, Y_t) = \gamma_{-i}$. Para processos estacionários, $\gamma_{-i} = \gamma_i$.
II.  **Positiva Semi-Definida:** Para qualquer vetor $x \in \mathbb{R}^m$, temos $x'\Gamma_m x = \sum_{i=1}^m \sum_{j=1}^m x_i x_j \gamma_{i-j}$. Utilizando a definição de covariância e assumindo a linearidade da covariância, podemos reescrever como $x'\Gamma_m x = Cov(\sum_{i=1}^m x_i Y_{t-i}, \sum_{j=1}^m x_j Y_{t-j})$. Como a covariância de qualquer variável aleatória com ela mesma é sempre maior ou igual a zero, então $x'\Gamma_m x \ge 0$.
III. **Positiva Definida:** Se o processo Y for estacionário e o espectro do processo for diferente de zero, então a matriz $\Gamma_m$ é positiva definida. Isso significa que $x'\Gamma_m x > 0$ para qualquer $x \neq 0$. A condição espectral garante que não exista vetor não nulo que faça a variância da combinação linear de Y seja zero.
■

Este lema garante que a matriz $\Gamma_m$ tem as propriedades necessárias para a aplicação da fatoração de Cholesky, como veremos a seguir.

### Fatoração Triangular e Algoritmo de Cholesky

Uma alternativa para a inversão direta de matrizes é a decomposição ou fatoração triangular, onde a matriz original é decomposta em matrizes triangulares mais simples [^4.4.1]. Em particular, para matrizes simétricas e positivas definidas, a fatoração de Cholesky fornece uma maneira eficiente de realizar essa decomposição.

A **fatoração de Cholesky** de uma matriz simétrica e positiva definida $\Omega$ (como as matrizes de autocovariância em nossos problemas) decompõe $\Omega$ no produto de uma matriz triangular inferior $A$ com 1s na diagonal principal, uma matriz diagonal $D$ e a transposta de $A$, $A'$.
$$ \Omega = ADA' $$
onde:
*   $A$ é uma matriz triangular inferior com 1s na diagonal principal [^4.4.1].
*   $D$ é uma matriz diagonal com entradas positivas na diagonal principal [^4.4.1].

A decomposição de Cholesky é única e fornece uma alternativa para a inversão direta de $\Omega$ [^4.4.14]. Uma vantagem computacional notável é que, ao invés de calcular $\Omega^{-1}$ diretamente, resolve-se um sistema triangular mais simples, o que é computacionalmente mais rápido.

No contexto do cálculo dos coeficientes da projeção linear, podemos aplicar a fatoração de Cholesky na matriz de autocovariâncias $\Gamma_m$. Seja $\Gamma_m = A_m D_m A_m'$. Podemos então obter a inversa de $\Gamma_m$, dado que:
$$
\Gamma_m^{-1} = (A_m')^{-1}D_m^{-1}A_m^{-1}
$$
Dessa forma, ao invés de realizar uma inversão matricial direta, realiza-se as operações de solução de sistemas lineares utilizando as matrizes triangulares ($A_m$ e $A_m'$) e uma divisão por uma matriz diagonal ($D_m^{-1}$).

O processo de fatoração de Cholesky envolve os seguintes passos:

1. **Inicialização:**  Decompor a matriz de autocovariâncias, $\Gamma_m$, em uma matriz triangular inferior $A$, uma matriz diagonal $D$ e a transposta de $A$, $A'$.
2. **Iteração:** Usando o método descrito em [^4.4.3] até [^4.4.8], os elementos de $A$ e $D$ são calculados recursivamente, coluna por coluna.
3. **Solução:** A inversa da matriz de autocovariâncias é então calculada usando as matrizes triangulares e diagonal: $\Gamma_m^{-1} = (A_m')^{-1}D_m^{-1}A_m^{-1}$.

> 💡 **Exemplo Numérico:** Retomando o exemplo do capítulo anterior, com um processo AR(1) com $\mu = 2$, $\phi = 0.8$, e $\sigma^2 = 1$, a matriz de autocovariâncias para m=2 é:
> $\Gamma_2 = \begin{bmatrix} 2.778 & 2.222 \\ 2.222 & 2.778 \end{bmatrix}$.
>
> Fatorando $\Gamma_2$ utilizando Cholesky, podemos encontrar as matrizes $A$ e $D$:
>
> $d_{11} = 2.778$, $a_{21} = \frac{2.222}{2.778} = 0.8$, e $d_{22} = 2.778 - \frac{2.222^2}{2.778} = 2.778 - 1.778 = 1.000$, então:
>
> $A = \begin{bmatrix} 1 & 0 \\ 0.8 & 1 \end{bmatrix}$,  $D = \begin{bmatrix} 2.778 & 0 \\ 0 & 1 \end{bmatrix}$.
>
> Para obter a inversa, usamos $ \Gamma_2^{-1} = (A')^{-1} D^{-1} A^{-1} $ .
>
> $ A^{-1} =  \begin{bmatrix} 1 & 0 \\ -0.8 & 1 \end{bmatrix} $, $(A')^{-1} = \begin{bmatrix} 1 & -0.8 \\ 0 & 1 \end{bmatrix} $ e  $ D^{-1} = \begin{bmatrix} 0.36 & 0 \\ 0 & 1 \end{bmatrix} $.
>
> Portanto,
>
> $\Gamma_2^{-1} = \begin{bmatrix} 1 & -0.8 \\ 0 & 1 \end{bmatrix}  \begin{bmatrix} 0.36 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ -0.8 & 1 \end{bmatrix} = \begin{bmatrix} 1.089 & -0.872 \\ -0.872 & 1.089 \end{bmatrix} $.
>
> Os coeficientes são calculados como antes:
>
> $\begin{bmatrix} \alpha_1^{(2)} \\ \alpha_2^{(2)} \end{bmatrix} = \begin{bmatrix} 1.089 & -0.872 \\ -0.872 & 1.089 \end{bmatrix} \begin{bmatrix} 2.222 \\ 1.778 \end{bmatrix} = \begin{bmatrix} 0.868 \\ 0 \end{bmatrix}$

> 💡 **Exemplo Numérico:** Vamos considerar um processo AR(2) com $\phi_1=0.7$, $\phi_2=0.2$, e $\sigma^2=1$, e calcular os coeficientes da projeção linear usando fatoração de Cholesky para $m=3$.  Primeiro, precisamos calcular a matriz de autocovariâncias $\Gamma_3$ e o vetor de autocovariâncias $\gamma$:
>
>  $\gamma_0 = \frac{\sigma^2}{1 - \phi_1^2 - \phi_2^2 - 2\phi_1\phi_2^2 / (1 - \phi_2)} = \frac{1}{1 - 0.7^2 - 0.2^2 - 2*0.7*0.2^2/(1-0.2)} \approx 2.00 $
>  $\gamma_1 = \phi_1\gamma_0 + \phi_2\gamma_1 = 0.7 * 2 + 0.2 * 0 = 1.4$
>  $\gamma_2 = \phi_1\gamma_1 + \phi_2\gamma_0 = 0.7 * 1.4 + 0.2 * 2 = 1.38$
>  $\gamma_3 = \phi_1\gamma_2 + \phi_2\gamma_1 = 0.7 * 1.38 + 0.2 * 1.4 = 1.246$
>
> $\Gamma_3 = \begin{bmatrix} 2.00 & 1.4 & 1.38 \\ 1.4 & 2.00 & 1.4 \\ 1.38 & 1.4 & 2.00 \end{bmatrix}$  e  $ \begin{bmatrix} \gamma_1 \\ \gamma_2 \\ \gamma_3 \end{bmatrix} = \begin{bmatrix} 1.4 \\ 1.38 \\ 1.246 \end{bmatrix} $
>
> Agora, vamos realizar a fatoração de Cholesky:
>
> $\text{Step 1: } d_{11} = 2.00$
> $\text{Step 2: } a_{21} = \frac{1.4}{2.0} = 0.7, a_{31} = \frac{1.38}{2.0} = 0.69$
> $\text{Step 3: } d_{22} = 2.00 - (0.7)^2*2.0 = 1.02$
> $\text{Step 4: } a_{32} = \frac{1.4 - (0.7)*(1.38)}{1.02} = 0.36$
> $\text{Step 5: } d_{33} = 2.0 - (0.69)^2*2.0 - (0.36)^2*1.02 = 0.92$
>
> $A = \begin{bmatrix} 1 & 0 & 0 \\ 0.7 & 1 & 0 \\ 0.69 & 0.36 & 1 \end{bmatrix}$,  $D = \begin{bmatrix} 2.0 & 0 & 0 \\ 0 & 1.02 & 0 \\ 0 & 0 & 0.92 \end{bmatrix}$.
>
> Calculando as inversas:
>
> $A^{-1} = \begin{bmatrix} 1 & 0 & 0 \\ -0.7 & 1 & 0 \\ -0.43  & -0.36 & 1 \end{bmatrix}$,
> $(A')^{-1} = \begin{bmatrix} 1 & -0.7 & -0.43  \\ 0 & 1 & -0.36 \\ 0 & 0 & 1 \end{bmatrix}$,
> $D^{-1} = \begin{bmatrix} 0.5 & 0 & 0 \\ 0 & 0.98 & 0 \\ 0 & 0 & 1.09 \end{bmatrix}$.
>
> $\Gamma_3^{-1} = (A')^{-1} D^{-1} A^{-1} = \begin{bmatrix} 0.757 & -0.397 & -0.165 \\ -0.397 & 1.124 & -0.395 \\ -0.165 & -0.395 & 1.21 \end{bmatrix}$
>
>  Finalmente, os coeficientes da projeção linear:
>
> $\begin{bmatrix} \alpha_1^{(3)} \\ \alpha_2^{(3)} \\ \alpha_3^{(3)} \end{bmatrix} = \begin{bmatrix} 0.757 & -0.397 & -0.165 \\ -0.397 & 1.124 & -0.395 \\ -0.165 & -0.395 & 1.21 \end{bmatrix}  \begin{bmatrix} 1.4 \\ 1.38 \\ 1.246 \end{bmatrix} = \begin{bmatrix} 0.694 \\ 0.202 \\ 0.003 \end{bmatrix}$
>
> Os coeficientes da projeção linear para $m=3$ são aproximadamente $\alpha_1 = 0.694$, $\alpha_2 = 0.202$, e $\alpha_3 = 0.003$.

A fatoração de Cholesky, portanto, proporciona um meio computacionalmente mais eficiente para calcular a inversa da matriz de autocovariâncias, o que, por sua vez, permite uma avaliação mais eficiente dos coeficientes da projeção linear.

**Proposição 2** (Eficiência Computacional da Fatoração de Cholesky):  Para uma matriz simétrica e positiva definida de dimensão $n \times n$, a fatoração de Cholesky é mais eficiente computacionalmente do que a inversão direta, tendo uma complexidade computacional de $O(n^3/3)$ em comparação com $O(n^3)$ da inversão direta.

*Prova:*

I. **Inversão Direta:** A inversão direta de uma matriz $n \times n$ geralmente envolve métodos como eliminação gaussiana ou decomposição LU, que têm uma complexidade computacional de $O(n^3)$. Isso ocorre porque, em cada etapa da eliminação gaussiana, é necessário realizar operações em todas as linhas e colunas da matriz, o que leva a uma complexidade de ordem cúbica em relação à dimensão da matriz.

II. **Fatoração de Cholesky:**
    a. A fatoração de Cholesky, para calcular $A$ e $D$, envolve operações que podem ser realizadas coluna por coluna da matriz original.
    b. Para cada coluna $k$, o cálculo dos elementos de $A$ e $D$ envolve somatórios e divisões que dependem dos elementos das colunas anteriores. A complexidade total para uma matriz $n \times n$ é dada por:
     $$\sum_{k=1}^{n}  \left(  \sum_{i=k}^{n} (i-k+1)  \right) =  \frac{n^3}{3} + O(n^2)$$
    c. A solução de sistemas triangulares após a decomposição tem complexidade $O(n^2)$, significativamente menor, uma vez que é necessário apenas realizar substituições para encontrar a solução.

III. **Comparação:** Portanto, a fatoração de Cholesky tem uma complexidade computacional menor em comparação com a inversão direta, o que a torna mais eficiente para grandes matrizes. A economia computacional é significativa à medida que $n$ aumenta.
■

**Observação 2.1:** Além da eficiência computacional, a fatoração de Cholesky também possui vantagens em termos de estabilidade numérica, especialmente quando comparada com a inversão direta de matrizes que podem ser mal condicionadas. A natureza triangular das matrizes resultantes da fatoração facilita a solução de sistemas lineares, o que é menos propenso a erros de arredondamento.

**Teorema 2.1** (Unicidade da Fatoração de Cholesky): A fatoração de Cholesky, se existir, é única. Ou seja, para uma matriz simétrica e positiva definida $\Omega$, existe uma única matriz triangular inferior $A$ com 1s na diagonal e uma única matriz diagonal $D$ com entradas positivas tal que $\Omega = ADA'$.

*Prova:*
I. Assume-se que existem duas fatorações de Cholesky para $\Omega$: $\Omega = A_1 D_1 A_1'$ e $\Omega = A_2 D_2 A_2'$.
II. Multiplicando a primeira equação por $A_2'^{-1}$ à direita e a segunda equação por $A_2^{-1}$ à esquerda, temos:
    $$ \Omega A_2'^{-1} = A_1 D_1 A_1' A_2'^{-1} $$
    $$ A_2^{-1} \Omega = A_2^{-1} A_1 D_1 A_1' $$
III. Substituindo o valor de $\Omega$ da primeira equação na segunda, obtemos:
$$ A_2^{-1} A_1 D_1 A_1' = A_2^{-1} A_1 D_1 A_1' A_2'^{-1} A_2' $$
$$ A_2^{-1} A_1 D_1 = D_2 A_2' A_1'^{-1} $$
IV. Definindo $X = A_2^{-1} A_1$ e $Y = A_2' A_1'^{-1}$, a equação acima se torna $X D_1 = D_2 Y$. Notavelmente, como $A_1$ e $A_2$ são matrizes triangulares inferiores com 1s na diagonal, $X = A_2^{-1} A_1$ também é triangular inferior com 1s na diagonal, e $Y = A_2' A_1'^{-1} = (A_1 A_2^{-1})'$ é triangular superior com 1s na diagonal.
V. Transpondo a equação, temos $D_1 X' = Y' D_2$. Dado que $D_1$ e $D_2$ são diagonais e $X'$ é triangular superior com 1s na diagonal, temos que $D_1 X'$ é uma matriz triangular superior.  Similarmente $Y'D_2$ é triangular inferior, e portanto, a única solução possível é que $X$ e $Y$ são matrizes diagonais com 1s na diagonal, ou seja, matrizes identidades.
VI. Assim, $X=I$, implicando em $A_2^{-1} A_1 = I$ ou $A_1 = A_2$.
VII. Substituindo $A_1 = A_2$ em $\Omega = A_1 D_1 A_1'$ e $\Omega = A_2 D_2 A_2'$, temos $A_1 D_1 A_1' = A_1 D_2 A_1'$. Multiplicando por $A_1^{-1}$ à esquerda e $(A_1')^{-1}$ à direita, obtemos $D_1 = D_2$. Portanto, as decomposições são iguais e a fatoração de Cholesky é única.
■

### Conclusão

Este capítulo aprofundou o entendimento do cálculo da projeção linear exata, com amostras finitas. Foi detalhado como a formulação de mínimos quadrados leva a operações de inversão de matrizes de autocovariâncias e como a fatoração triangular, especialmente a fatoração de Cholesky, se apresenta como uma alternativa computacionalmente eficiente, por simplificar a operação em sistemas lineares triangulares, em vez da inversão matricial direta. A compreensão desses métodos não apenas fornece uma base teórica sólida, mas também instrumentaliza a aplicação prática de previsões de séries temporais de maneira eficiente. Essa combinação de teoria e métodos práticos é crucial para o avanço do estudo da análise de séries temporais e de suas aplicações.

### Referências

[^4.3.7]: *“...Then we could calculate the projection of ...”*
[^4.3.8]: *“For this definition of X, the coefficients can be calculated directly from [4.1.13] to be ...”*
[^4.3.9]: *“To generate an s-period-ahead forecast Ŷ_t+s|t, we would use”*
[^4.4.1]: *“Any positive definite symmetric (n × n) matrix Ω has a unique representation of the form ...”*
[^4.4.14]: *“We next establish that the triangular factorization is unique.”*
[^4.4.3]: *“The matrix Ω can be transformed into a matrix with zero in the (2, 1) position by multiplying the first row of Ω by Ω_21Ω_11^-1 and subtracting the resulting row from the second.”*
[^4.4.8]:  *“Thus each E_j is lower triangular with unit determinant. Hence E_j^-1 exists, and the following matrix exists:”*
<!-- END -->
