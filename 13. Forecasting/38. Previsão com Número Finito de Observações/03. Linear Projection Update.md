## Atualiza√ß√£o da Proje√ß√£o Linear e Fatora√ß√£o Triangular

### Introdu√ß√£o

Este cap√≠tulo aprofunda a discuss√£o sobre previs√£o em s√©ries temporais, explorando o processo de atualiza√ß√£o da proje√ß√£o linear. Construindo sobre os conceitos de proje√ß√£o linear exata com amostras finitas e fatora√ß√£o triangular, este cap√≠tulo explora como informa√ß√µes adicionais podem ser utilizadas para refinar previs√µes iniciais, e como o m√©todo de fatora√ß√£o triangular pode ser empregado de forma recursiva neste processo. Al√©m disso, aborda-se os requisitos computacionais e de armazenamento para a implementa√ß√£o deste processo de atualiza√ß√£o. Este t√≥pico √© fundamental para entender como integrar novos dados em previs√µes existentes de forma eficiente.

### Mecanismos de Atualiza√ß√£o da Proje√ß√£o Linear

No cap√≠tulo anterior, foi demonstrado como obter proje√ß√µes lineares exatas em amostras finitas utilizando a fatora√ß√£o de Cholesky. No entanto, em muitas aplica√ß√µes pr√°ticas, novas observa√ß√µes tornam-se dispon√≠veis ao longo do tempo e estas informa√ß√µes adicionais podem ser usadas para refinar as previs√µes existentes. Esta se√ß√£o explora como realizar a atualiza√ß√£o da proje√ß√£o linear, um processo chave na an√°lise de s√©ries temporais.

A ideia principal da atualiza√ß√£o da proje√ß√£o linear √© incorporar novas informa√ß√µes, e os erros de previs√£o resultantes, para refinar as previs√µes anteriores. Considere uma previs√£o inicial de $Y_3$ baseada em $Y_1$:
$$ P(Y_3 | Y_1) = \Omega_{31} \Omega_{11}^{-1} Y_1 $$
[4.5.12]

Suponha que uma nova observa√ß√£o $Y_2$ se torne dispon√≠vel, e buscamos atualizar a previs√£o de $Y_3$ utilizando tanto $Y_1$ quanto $Y_2$. A atualiza√ß√£o da proje√ß√£o linear √© dada por [^4.5.14]:
$$ P(Y_3 | Y_2, Y_1) = P(Y_3 | Y_1) + h_{32} h_{22}^{-1} [Y_2 - P(Y_2 | Y_1)] $$
onde:
*   $P(Y_3 | Y_1)$ √© a proje√ß√£o linear inicial de $Y_3$ sobre $Y_1$.
*   $P(Y_2 | Y_1)$ √© a proje√ß√£o linear de $Y_2$ sobre $Y_1$.
*   $h_{32}$ √© o produto esperado dos erros de previs√£o de $Y_3$ e $Y_2$, quando ambos s√£o previstos com base em $Y_1$.
*   $h_{22}$ √© a vari√¢ncia do erro de previs√£o de $Y_2$ com base em $Y_1$.

Essa express√£o captura a ess√™ncia da atualiza√ß√£o da proje√ß√£o linear: a previs√£o inicial de $Y_3$ com base em $Y_1$ √© ajustada pelo termo $h_{32} h_{22}^{-1}$, o qual √© o produto da raz√£o entre o erro de previs√£o dos dois elementos vezes o erro de previs√£o de $Y_2$ com base em $Y_1$. Em ess√™ncia, o termo $h_{32} h_{22}^{-1}$ corrige a proje√ß√£o inicial, adicionando o novo componente de informa√ß√£o contida em $Y_2$.

O termo $h_{32} h_{22}^{-1}$ pode ser interpretado como a magnitude da corre√ß√£o que deve ser aplicada √† previs√£o inicial devido √† nova informa√ß√£o de $Y_2$. Como $h_{22}$ √© o MSE de prever $Y_2$ com base em $Y_1$, a raz√£o $h_{32} h_{22}^{-1}$ quantifica o quanto a informa√ß√£o em $Y_2$ ajuda a melhorar a previs√£o de $Y_3$, dado que ambas s√£o correlacionadas com $Y_1$.

> üí° **Exemplo Num√©rico:** Vamos considerar um exemplo pr√°tico onde temos observa√ß√µes sequenciais de temperatura em uma determinada cidade. Suponha que $Y_1 = 20^\circ C$, e usando um modelo simples, prevemos $Y_3$ como $P(Y_3|Y_1) = 22^\circ C$. Agora, uma nova observa√ß√£o $Y_2 = 23^\circ C$ torna-se dispon√≠vel. Assumindo que $P(Y_2|Y_1) = 21^\circ C$, e que, calculando a covari√¢ncia entre os erros de previs√£o, temos $h_{32} = 0.8$ e $h_{22} = 1$, podemos atualizar a previs√£o de $Y_3$ usando a f√≥rmula:
>
> $P(Y_3 | Y_2, Y_1) = 22 + \frac{0.8}{1} [23 - 21] = 22 + 0.8 * 2 = 23.6$.
>
> O resultado √© a nova previs√£o $P(Y_3 | Y_2, Y_1) = 23.6^\circ C$, onde a informa√ß√£o adicional de $Y_2$ elevou a previs√£o de $Y_3$ em $1.6^\circ C$.

**Lema 1** (Ortogonalidade dos Erros de Previs√£o): Os erros de previs√£o utilizados na atualiza√ß√£o da proje√ß√£o linear s√£o ortogonais √†s vari√°veis utilizadas para a proje√ß√£o. Especificamente, $E[(Y_k - P(Y_k|Y_1,\ldots,Y_{k-1}))Y_j] = 0$ para $j < k$.

*Prova*:
I.  A proje√ß√£o linear $P(Y_k|Y_1,\ldots,Y_{k-1})$ √© definida de forma que o erro de previs√£o $Y_k - P(Y_k|Y_1,\ldots,Y_{k-1})$ seja ortogonal ao espa√ßo gerado por $Y_1, \ldots, Y_{k-1}$.
II.  Isso significa que a covari√¢ncia entre o erro de previs√£o e qualquer combina√ß√£o linear das vari√°veis $Y_1, \ldots, Y_{k-1}$ √© zero.
III. Como $Y_j$ √© uma combina√ß√£o linear das vari√°veis $Y_1, \ldots, Y_{k-1}$ para $j<k$, segue-se que $E[(Y_k - P(Y_k|Y_1,\ldots,Y_{k-1}))Y_j] = 0$.
‚ñ†

### Interpreta√ß√£o da Atualiza√ß√£o e Rela√ß√£o com a Fatora√ß√£o Triangular

A express√£o para atualiza√ß√£o da proje√ß√£o linear [^4.5.14] pode ser relacionada com a fatora√ß√£o triangular de matrizes, como apresentado no cap√≠tulo anterior. No processo de fatora√ß√£o triangular, vimos que a matriz de autocovari√¢ncias $\Omega$ pode ser fatorada como $\Omega = ADA'$ [^4.4.1], onde A √© uma matriz triangular inferior com 1s na diagonal e D √© uma matriz diagonal.

A rela√ß√£o entre a fatora√ß√£o triangular e a atualiza√ß√£o da proje√ß√£o linear pode ser demonstrada definindo um vetor $\hat{Y}^{(1)} = E_1Y$, onde $E_1$ √© uma matriz que cria os res√≠duos da proje√ß√£o de cada $Y_i$ sobre $Y_1$. Notavelmente, a matriz de covari√¢ncias de $\hat{Y}^{(1)}$, denotada por $H$, tem as seguintes caracter√≠sticas:
$$ H = E(\hat{Y}^{(1)} \hat{Y}^{(1)'}) = E_1 \Omega E_1' $$
onde $H$ √© a matriz obtida no primeiro passo da fatora√ß√£o triangular.

Os elementos de $H$ est√£o diretamente relacionados aos componentes da atualiza√ß√£o da proje√ß√£o linear:
*   $h_{22}$ √© a vari√¢ncia do erro de previs√£o de $Y_2$ com base em $Y_1$ e, tamb√©m, o elemento (2,2) de $H$ (ou seja, o MSE de proje√ß√£o de $Y_2$ em $Y_1$).
*   $h_{32}$ √© o produto esperado do erro de previs√£o de $Y_3$ e $Y_2$, quando ambos s√£o previstos com base em $Y_1$, e √©, similarmente, um dos elementos de $H$ (especificamente o elemento (3,2) de $H$).

A equa√ß√£o de atualiza√ß√£o da proje√ß√£o linear [^4.5.16] pode ser expressa em termos da matriz $H$:
$$ P(Y_3| Y_2, Y_1) = P(Y_3| Y_1) + \{E[Y_3-P(Y_3| Y_1)][Y_2-P(Y_2| Y_1)]\}  \{E[Y_2-P(Y_2| Y_1)]^2\}^{-1} [Y_2-P(Y_2| Y_1)]$$
onde $P(Y_3|Y_1) = \Omega_{31}\Omega_{11}^{-1}Y_1$.

A matriz $H$ tamb√©m desempenha um papel crucial no processo recursivo de atualiza√ß√£o. Se uma terceira vari√°vel $Y_3$ estiver dispon√≠vel, a proje√ß√£o de $Y_4$ sobre $Y_3$, $Y_2$ e $Y_1$ √© dada por:
$$P(Y_4 | Y_3, Y_2, Y_1) = P(Y_4 | Y_1) + h_{42} h_{22}^{-1} [Y_2 - P(Y_2 | Y_1)] + h_{43.2} h_{33.2}^{-1} [Y_3 - P(Y_3 | Y_2, Y_1)] $$
onde:
*   $h_{42}$ √© o produto esperado do erro de previs√£o de $Y_4$ e $Y_2$ com base em $Y_1$.
*   $h_{43.2}$ √© o produto esperado do erro de previs√£o de $Y_4$ e $Y_3$ com base em $Y_1$ e $Y_2$.
*   $h_{33.2}$ √© a vari√¢ncia do erro de previs√£o de $Y_3$ com base em $Y_1$ e $Y_2$.

> üí° **Exemplo Num√©rico:** Suponha que temos uma s√©rie temporal de pre√ßos de a√ß√µes, e j√° calculamos que $P(Y_4 | Y_1) = 150$. Ap√≥s a observa√ß√£o de $Y_2$, determinamos que $P(Y_2 | Y_1) = 152$, e que $h_{42} = 2$ e $h_{22} = 1$. A nova observa√ß√£o $Y_2 = 153$ nos leva a atualizar a previs√£o para:
> $P(Y_4 | Y_2, Y_1) = 150 + 2 * (153-152) = 152$.
> Posteriormente, observamos $Y_3 = 155$, com $P(Y_3 | Y_2, Y_1) = 154$, e descobrimos que $h_{43.2} = 1.5$ e $h_{33.2} = 0.8$. A previs√£o atualizada para $Y_4$ com a inclus√£o de $Y_3$ √© ent√£o:
> $P(Y_4 | Y_3, Y_2, Y_1) = 152 + \frac{1.5}{0.8} (155 - 154) = 152 + 1.875 = 153.875$.
> Este exemplo demonstra como cada nova observa√ß√£o sequencial permite refinar a previs√£o de $Y_4$.

**Teorema 1** (Decomposi√ß√£o Recursiva da Proje√ß√£o Linear): A proje√ß√£o linear de $Y_k$ sobre $Y_1, \ldots, Y_{k-1}$ pode ser expressa recursivamente utilizando os erros de previs√£o sucessivos.

*Prova:*
I.  A proje√ß√£o linear $P(Y_k|Y_1,\ldots,Y_{k-1})$ pode ser expressa como uma soma de proje√ß√µes, onde cada termo utiliza um novo erro de proje√ß√£o obtido recursivamente:

   $$P(Y_k|Y_1,\ldots,Y_{k-1}) = P(Y_k|Y_1) + \sum_{j=2}^{k-1} h_{k,j.1,\ldots,j-1}h_{j,j.1,\ldots,j-1}^{-1} [Y_j - P(Y_j|Y_1,\ldots,Y_{j-1})]$$

    onde $h_{k,j.1,\ldots,j-1}$ √© o produto esperado entre os erros de previs√£o de $Y_k$ e $Y_j$ quando ambos s√£o previstos com base em $Y_1,\ldots,Y_{j-1}$, e $h_{j,j.1,\ldots,j-1}$ √© a vari√¢ncia do erro de previs√£o de $Y_j$ baseado em $Y_1,\ldots,Y_{j-1}$.
II. Cada termo na soma adiciona a corre√ß√£o para a previs√£o inicial, utilizando a nova informa√ß√£o contida no erro de previs√£o mais recente, de forma an√°loga a [^4.5.14].
III. Este processo √© recursivo porque cada $P(Y_j|Y_1,\ldots,Y_{j-1})$ tamb√©m pode ser expandido da mesma forma, demonstrando que a atualiza√ß√£o √© uma composi√ß√£o de sucessivas proje√ß√µes e corre√ß√µes, utilizando erros sucessivamente ortogonalizados.
‚ñ†

Este processo pode ser repetido recursivamente para incorporar sucessivamente novas informa√ß√µes e atualizar as previs√µes existentes. A cada etapa, os coeficientes do processo de atualiza√ß√£o (como $h_{42} h_{22}^{-1}$ e $h_{43.2} h_{33.2}^{-1}$) s√£o derivados de elementos da matriz $H$.

### Implica√ß√µes Computacionais e de Armazenamento

A implementa√ß√£o da atualiza√ß√£o da proje√ß√£o linear requer a manuten√ß√£o e o c√°lculo de diversos componentes:

1.  **Coeficientes de Proje√ß√£o:** Os coeficientes da proje√ß√£o linear inicial ($ \Omega_{31} \Omega_{11}^{-1} $) e os coeficientes de atualiza√ß√£o ($h_{32}h_{22}^{-1}, h_{43.2} h_{33.2}^{-1}$), que podem ser derivados dos elementos da matriz $H$.
2.  **Res√≠duos:** Os termos residuais, como $[Y_2 - P(Y_2 | Y_1)]$ e $[Y_3 - P(Y_3 | Y_2, Y_1)]$ para poder aplicar a proje√ß√£o linear.
3.  **Matriz de Covari√¢ncias:** A matriz de covari√¢ncias ($\Omega$ ou suas transforma√ß√µes) √© utilizada para realizar os c√°lculos recursivos necess√°rios.

A fatora√ß√£o triangular pode simplificar o processo de atualiza√ß√£o ao fornecer uma maneira eficiente de computar os coeficientes e vari√¢ncias necess√°rios. Especificamente, ao inv√©s de calcular uma matriz de autocovari√¢ncia completa a cada etapa, pode-se atualizar iterativamente a matriz triangular $A$ e a matriz diagonal $D$.

> üí° **Exemplo Num√©rico:** Vamos ilustrar como a fatora√ß√£o triangular simplifica a atualiza√ß√£o. Imagine que temos uma s√©rie temporal com tr√™s observa√ß√µes e a matriz de covari√¢ncia correspondente, e que ap√≥s a fatora√ß√£o de Cholesky encontramos as matrizes A e D.
>
> Inicialmente, vamos considerar uma matriz de covari√¢ncia $\Omega$:
>
> $\Omega = \begin{bmatrix} 1 & 0.8 & 0.6 \\ 0.8 & 1 & 0.8 \\ 0.6 & 0.8 & 1 \end{bmatrix}$
>
> Atrav√©s da fatora√ß√£o de Cholesky $\Omega = ADA'$, obtemos as matrizes $A$ e $D$:
>
> $A = \begin{bmatrix} 1 & 0 & 0 \\ 0.8 & 1 & 0 \\ 0.6 & 0.4 & 1 \end{bmatrix}$ e $D = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0.36 & 0 \\ 0 & 0 & 0.2 \end{bmatrix}$
>
> Agora, se uma nova observa√ß√£o $Y_4$ torna-se dispon√≠vel, e desejamos atualizar a proje√ß√£o de $Y_5$ em fun√ß√£o de $Y_1, Y_2, Y_3, Y_4$, podemos utilizar as matrizes $A$ e $D$, e a nova linha/coluna da matriz de covari√¢ncia, para realizar a atualiza√ß√£o da proje√ß√£o e atualizar as matrizes $A$ e $D$, sem precisar recalcular toda a matriz $\Omega$.

Em termos de armazenamento, √© necess√°rio manter os valores de $A$ e $D$ e atualizar estas matrizes, √† medida que novas informa√ß√µes se tornam dispon√≠veis. A natureza triangular da matriz $A$ tamb√©m reduz a quantidade de mem√≥ria necess√°ria para armazenamento em rela√ß√£o a uma matriz de autocovari√¢ncias completa.

**Proposi√ß√£o 3** (Atualiza√ß√£o Recursiva com Fatora√ß√£o Triangular):  A fatora√ß√£o triangular e o m√©todo de Cholesky fornecem um meio para implementar recursivamente a atualiza√ß√£o da proje√ß√£o linear. Os coeficientes da proje√ß√£o linear podem ser atualizados eficientemente a cada nova observa√ß√£o.

*Prova:*

I.  A fatora√ß√£o de Cholesky decomp√µe uma matriz de covari√¢ncias $\Omega$ em $\Omega = ADA'$, onde $A$ √© uma matriz triangular inferior com 1s na diagonal principal, e $D$ √© uma matriz diagonal.
II. A atualiza√ß√£o da proje√ß√£o linear envolve a corre√ß√£o da previs√£o inicial por um termo proporcional √† diferen√ßa entre o valor observado e a sua previs√£o, e esse termo √© dado por $h_{j,k} h_{k,k}^{-1}$ [^4.5.16].
III. A matriz $H$ obtida no processo de fatora√ß√£o triangular tem seus elementos diretamente ligados aos coeficientes $h_{j,k}$ e $h_{k,k}$. Especificamente, os elementos da matriz $H$ podem ser obtidos com o processo recursivo descrito em [^4.4.4] e seguintes.
IV. A atualiza√ß√£o da proje√ß√£o linear pode ser realizada iterativamente, aproveitando o processo recursivo para atualizar tanto os termos da proje√ß√£o inicial, como os erros e vari√¢ncias com cada nova observa√ß√£o e a sua rela√ß√£o com a decomposi√ß√£o triangular.
V. Os c√°lculos da fatora√ß√£o triangular (matrizes $A$ e $D$) podem ser atualizados recursivamente, sem a necessidade de recalcular a matriz $\Omega$ inteira a cada nova informa√ß√£o. Isso economiza tempo e recursos computacionais, e permite que a atualiza√ß√£o da proje√ß√£o seja computacionalmente eficiente.
‚ñ†

**Corol√°rio 3.1** (Efici√™ncia Computacional): A atualiza√ß√£o recursiva da fatora√ß√£o triangular reduz a complexidade computacional de $O(n^3)$ para $O(n^2)$ por itera√ß√£o na atualiza√ß√£o da proje√ß√£o linear, em rela√ß√£o a calcular novamente a fatora√ß√£o a cada passo.

*Prova:*
I. O c√°lculo direto da fatora√ß√£o de Cholesky de uma matriz de covari√¢ncias $\Omega$ de dimens√£o $n \times n$ tem complexidade de $O(n^3)$.
II. A atualiza√ß√£o recursiva da fatora√ß√£o triangular envolve a atualiza√ß√£o da matriz $A$ e $D$, um processo que tem complexidade $O(n^2)$ para cada atualiza√ß√£o.
III. Logo, o uso da atualiza√ß√£o recursiva da fatora√ß√£o triangular reduz significativamente o custo computacional quando √© necess√°rio realizar m√∫ltiplas atualiza√ß√µes da proje√ß√£o linear, em especial para s√©ries temporais longas.
‚ñ†

### Conclus√£o

Este cap√≠tulo detalhou como a proje√ß√£o linear pode ser atualizada ao incorporar novas informa√ß√µes, e explorou como a fatora√ß√£o triangular auxilia este processo.  Al√©m disso, detalhou-se os requisitos computacionais e de armazenamento envolvidos na implementa√ß√£o deste processo de atualiza√ß√£o. A combina√ß√£o dessas t√©cnicas fornece uma base te√≥rica e pr√°tica para o desenvolvimento de m√©todos de previs√£o eficientes e adaptativos em an√°lise de s√©ries temporais, onde √© comum que informa√ß√µes adicionais se tornem dispon√≠veis ao longo do tempo. Essas ideias s√£o cruciais para aplica√ß√µes reais e ajudam a fundamentar a discuss√£o nos pr√≥ximos cap√≠tulos.

### Refer√™ncias

[^4.5.14]: *‚ÄúEquation [4.5.12] states that ...‚Äù*
[^4.4.1]: *‚ÄúAny positive definite symmetric (n √ó n) matrix Œ© has a unique representation of the form ...‚Äù*
[^4.5.16]: *‚ÄúThus equation [4.5.14] states that a linear projection can be updated using the following formula:‚Äù*
<!-- END -->
