## Previs√£o Baseada em Y Defasados: A Lei das Proje√ß√µes Iteradas
### Introdu√ß√£o
No cap√≠tulo anterior, examinamos as previs√µes baseadas na premissa de que os erros ($\epsilon_t$) eram observados diretamente. No entanto, em cen√°rios de previs√£o t√≠picos, temos observa√ß√µes dos valores defasados de Y, e n√£o dos erros defasados. Para abordar esta situa√ß√£o, exploramos as previs√µes com base em valores defasados de Y. Como visto anteriormente [^4.2.10], um processo pode ter uma representa√ß√£o AR(‚àû) expressa como $\eta(L)(Y_t - \mu) = \epsilon_t$, onde $\eta(L)$ √© um polin√¥mio em termos do operador de defasagem, e o polin√¥mio AR $\eta(L)$ e o polin√¥mio MA $\psi(L)$ est√£o relacionados por $\eta(L) = [\psi(L)]^{-1}$ [^4.2.11]. Agora, vamos aprofundar a an√°lise para entender como a lei das proje√ß√µes iteradas nos permite gerar previs√µes de m√∫ltiplos per√≠odos de forma recursiva, adaptando previs√µes anteriores √† medida que novos dados se tornam dispon√≠veis.

**Observa√ß√£o 1:** √â importante notar que a representa√ß√£o AR(‚àû) mencionada aqui √© uma consequ√™ncia da invertibilidade de processos ARMA, onde um processo MA pode ser expresso como um processo AR de ordem infinita. Essa conex√£o √© crucial para entender a rela√ß√£o entre as diferentes representa√ß√µes de um mesmo processo estoc√°stico.

### Conceitos Fundamentais
Inicialmente, derivamos uma previs√£o de um per√≠odo √† frente utilizando a representa√ß√£o AR(p) do processo, expressa como
$$(Y_{t+1} - \mu) = \phi_1(Y_t - \mu) + \phi_2(Y_{t-1} - \mu) + \dots + \phi_p(Y_{t-p+1} - \mu)$$ [^4.2.24].
Em seguida, para obter uma previs√£o de dois per√≠odos √† frente, substitu√≠mos t por t+1 na equa√ß√£o acima, obtendo:
$$(Y_{t+2} - \mu) = \phi_1(Y_{t+1} - \mu) + \phi_2(Y_t - \mu) + \dots + \phi_p(Y_{t-p+2} - \mu)$$
A lei das proje√ß√µes iteradas estabelece que a proje√ß√£o da previs√£o para t+2 em rela√ß√£o √† informa√ß√£o em t, √© equivalente √† previs√£o para t+2 no instante t. Matematicamente, isso significa que  
$$ \mathbb{E}[Y_{t+2}|Y_t, Y_{t-1}, \ldots ] = \mathbb{E}[ \mathbb{E}[Y_{t+2}|Y_{t+1}, Y_t, Y_{t-1}, \ldots ]|Y_t, Y_{t-1}, \ldots ] $$.
Assim, a previs√£o de dois per√≠odos √† frente no instante t, pode ser calculada utilizando a previs√£o de um per√≠odo √† frente como:
$$(\hat{Y}_{t+2|t} - \mu) = \phi_1(\hat{Y}_{t+1|t} - \mu) + \phi_2(Y_t - \mu) + \dots + \phi_p(Y_{t-p+2} - \mu)$$ [^4.2.26]

> üí° **Exemplo Num√©rico:** Considere um processo AR(2) com $\phi_1 = 0.7$, $\phi_2 = 0.2$, e $\mu = 10$. Suponha que observamos $Y_t = 12$ e $Y_{t-1} = 11$.
>
>  **Passo 1: Previs√£o de um per√≠odo √† frente ($\hat{Y}_{t+1|t}$):**
>
>  $\hat{Y}_{t+1|t} - 10 = 0.7(12 - 10) + 0.2(11 - 10)$
>
>  $\hat{Y}_{t+1|t} - 10 = 0.7(2) + 0.2(1) = 1.4 + 0.2 = 1.6$
>
>  $\hat{Y}_{t+1|t} = 11.6$
>
>  **Passo 2: Previs√£o de dois per√≠odos √† frente ($\hat{Y}_{t+2|t}$):**
>
> $\hat{Y}_{t+2|t} - 10 = 0.7(\hat{Y}_{t+1|t} - 10) + 0.2(Y_t - 10)$
>
> $\hat{Y}_{t+2|t} - 10 = 0.7(11.6 - 10) + 0.2(12 - 10)$
>
> $\hat{Y}_{t+2|t} - 10 = 0.7(1.6) + 0.2(2) = 1.12 + 0.4 = 1.52$
>
> $\hat{Y}_{t+2|t} = 11.52$
>
> Assim, a previs√£o de um per√≠odo √† frente √© 11.6 e a previs√£o de dois per√≠odos √† frente √© 11.52. Note como a previs√£o para t+2 usa a previs√£o para t+1, e n√£o o valor observado em t+1 (que n√£o temos).

Para um processo AR(p), obtemos a previs√£o de dois per√≠odos √† frente substituindo a previs√£o de um per√≠odo √† frente em sua pr√≥pria representa√ß√£o autorregressiva. Expandindo a previs√£o de dois per√≠odos √† frente,
$$ (\hat{Y}_{t+2|t} - \mu) = \phi_1[\phi_1(Y_t - \mu) + \phi_2(Y_{t-1} - \mu) + \dots + \phi_p(Y_{t-p+1} - \mu)] + \phi_2(Y_t - \mu) + \dots + \phi_p(Y_{t-p+2} - \mu)$$.
Essa abordagem recursiva demonstra como uma previs√£o de m√∫ltiplos per√≠odos √† frente pode ser obtida utilizando a lei das proje√ß√µes iteradas [^4.2.27].

> üí° **Exemplo Num√©rico (Expans√£o):** Usando o mesmo processo AR(2) do exemplo anterior, vamos expandir a previs√£o de $\hat{Y}_{t+2|t}$:
>
> $\hat{Y}_{t+2|t} - 10 = 0.7[0.7(12-10) + 0.2(11-10)] + 0.2(12-10)$
>
> $\hat{Y}_{t+2|t} - 10 = 0.7[0.7(2) + 0.2(1)] + 0.2(2)$
>
> $\hat{Y}_{t+2|t} - 10 = 0.7[1.4 + 0.2] + 0.4$
>
> $\hat{Y}_{t+2|t} - 10 = 0.7[1.6] + 0.4$
>
> $\hat{Y}_{t+2|t} - 10 = 1.12 + 0.4 = 1.52$
>
> $\hat{Y}_{t+2|t} = 11.52$
>
>  Este resultado √© id√™ntico ao obtido atrav√©s do m√©todo iterativo, confirmando a equival√™ncia entre as duas abordagens.

**Lema 1:** A lei das proje√ß√µes iteradas pode ser generalizada para qualquer n√∫mero de passos √† frente. Formalmente, para qualquer $k > 0$, temos:
$$ \mathbb{E}[Y_{t+k}|Y_t, Y_{t-1}, \ldots ] = \mathbb{E}[ \mathbb{E}[Y_{t+k}|Y_{t+k-1}, Y_{t+k-2}, \ldots,Y_t, Y_{t-1}, \ldots ]|Y_t, Y_{t-1}, \ldots ] $$

*Proof:* Esta propriedade √© uma consequ√™ncia direta da propriedade de torre da esperan√ßa condicional. Aplicando repetidamente essa propriedade, podemos decompor a expectativa de $Y_{t+k}$ dada a informa√ß√£o em $t$ em uma sequ√™ncia de expectativas condicionais, comprovando o lema.

I.  Vamos definir $\mathcal{F}_t = \{Y_t, Y_{t-1}, \ldots\}$ como o conjunto de informa√ß√µes dispon√≠veis no tempo $t$.

II.  Pela propriedade da torre da esperan√ßa condicional, sabemos que para quaisquer vari√°veis aleat√≥rias $X$, $Y$, e $\mathcal{G}$, onde $\mathcal{G}$ √© uma sigma-√°lgebra,
    $$\mathbb{E}[X | \mathcal{G}] = \mathbb{E}[\mathbb{E}[X|Y, \mathcal{G}]|\mathcal{G}]$$

III. Aplicando a propriedade da torre, temos:
    $$\mathbb{E}[Y_{t+k}|\mathcal{F}_t] = \mathbb{E}[\mathbb{E}[Y_{t+k}|\mathcal{F}_{t+k-1}]|\mathcal{F}_t]$$
    onde $\mathcal{F}_{t+k-1} = \{Y_{t+k-1}, Y_{t+k-2},\ldots, Y_t, Y_{t-1},\ldots\}$.

IV. Expandindo iterativamente a esperan√ßa condicional, podemos escrever:
    $$ \mathbb{E}[Y_{t+k}|\mathcal{F}_t] = \mathbb{E}[ \mathbb{E}[Y_{t+k}|Y_{t+k-1}, Y_{t+k-2}, \ldots,Y_t, Y_{t-1}, \ldots ]|Y_t, Y_{t-1}, \ldots ] $$
    
V. Portanto, a lei das proje√ß√µes iteradas √© provada.
‚ñ†

Este processo iterativo √© formalizado da seguinte forma, para um processo AR(p):

$$(\hat{Y}_{t+j|t} - \mu) = \phi_1(\hat{Y}_{t+j-1|t} - \mu) + \phi_2(\hat{Y}_{t+j-2|t} - \mu) + \dots + \phi_p(\hat{Y}_{t+j-p|t} - \mu)$$

onde $\hat{Y}_{t+i|t} = Y_{t+i}$ para $i \leq 0$. Em outras palavras, a previs√£o para o per√≠odo $t+j$, no instante t, √© uma fun√ß√£o linear das previs√µes dos $p$ per√≠odos anteriores e dos par√¢metros $\phi_i$, com base na informa√ß√£o dispon√≠vel no per√≠odo t.

√â importante notar que este resultado √© v√°lido para qualquer horizonte de tempo. Assim, se quisermos gerar uma previs√£o para o instante $t+s$, a melhor previs√£o ser√° obtida utilizando os par√¢metros do modelo AR(p) e as previs√µes de instantes anteriores.

**Teorema 1.1:** Para um processo AR(p), a previs√£o de *j* per√≠odos √† frente, $\hat{Y}_{t+j|t}$,  converge para a m√©dia incondicional $\mu$ √† medida que *j* tende ao infinito, assumindo que o processo √© estacion√°rio.

*Proof:* Para um processo AR(p) estacion√°rio, as ra√≠zes do polin√¥mio caracter√≠stico associado est√£o fora do c√≠rculo unit√°rio. Isso implica que os coeficientes $\phi_i$ s√£o tais que a influ√™ncia de observa√ß√µes mais antigas decresce exponencialmente. Portanto, √† medida que *j* aumenta, a depend√™ncia de $\hat{Y}_{t+j|t}$ em rela√ß√£o a $Y_t, Y_{t-1}, \ldots$ torna-se cada vez menor. No limite, a previs√£o converge para a m√©dia incondicional do processo, $\mu$.

I.  Considere um processo AR(p) estacion√°rio:
    $$Y_t = \mu + \phi_1(Y_{t-1}-\mu) + \phi_2(Y_{t-2}-\mu) + \ldots + \phi_p(Y_{t-p}-\mu) + \epsilon_t$$
   onde $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia finita.

II.  A previs√£o *j* passos √† frente pode ser escrita como:
    $$ \hat{Y}_{t+j|t} - \mu = \sum_{i=1}^p \phi_i(\hat{Y}_{t+j-i|t} - \mu) $$
    com $\hat{Y}_{t+i|t}=Y_{t+i}$ para $i\le0$.

III.  Se o processo √© estacion√°rio, as ra√≠zes do polin√¥mio caracter√≠stico $1 - \phi_1z - \phi_2z^2 - \ldots - \phi_pz^p = 0$ est√£o fora do c√≠rculo unit√°rio no plano complexo. Isso implica que os coeficientes $\phi_i$ satisfazem certas condi√ß√µes, garantindo que a influ√™ncia de observa√ß√µes defasadas decres√ßa com o tempo.

IV.  √Ä medida que *j* aumenta, o efeito das condi√ß√µes iniciais $Y_t, Y_{t-1}, \ldots$ na previs√£o $\hat{Y}_{t+j|t}$ diminui, pois as previs√µes de passos anteriores que dependem dessas observa√ß√µes se tornam cada vez menos relevantes no c√°lculo da previs√£o atual.

V. No limite quando *j* tende ao infinito, as previs√µes se aproximam da m√©dia incondicional do processo, $\mu$, pois o efeito das observa√ß√µes passadas torna-se desprez√≠vel. Portanto,
    $$ \lim_{j \to \infty} \hat{Y}_{t+j|t} = \mu $$
‚ñ†
> üí° **Exemplo Num√©rico (Converg√™ncia):** Considerando o mesmo processo AR(2) com $\phi_1=0.7$, $\phi_2=0.2$ e $\mu=10$, vamos calcular previs√µes para v√°rios passos √† frente, partindo de $Y_t=12$ e $Y_{t-1}=11$, e observar a converg√™ncia para a m√©dia.
>
>  - $\hat{Y}_{t+1|t} = 11.6$ (j√° calculado)
>  - $\hat{Y}_{t+2|t} = 11.52$ (j√° calculado)
>  - $\hat{Y}_{t+3|t} = 0.7(11.52 - 10) + 0.2(11.6-10) + 10= 0.7(1.52) + 0.2(1.6) + 10 = 1.064 + 0.32 + 10 = 11.384$
>  - $\hat{Y}_{t+4|t} = 0.7(11.384-10) + 0.2(11.52-10) + 10= 0.7(1.384) + 0.2(1.52) + 10 = 0.9688 + 0.304 + 10 = 11.2728$
>
>  Continuando esse processo, as previs√µes se aproximar√£o cada vez mais de 10, a m√©dia do processo.
>
>   | Passo √† Frente (j) | Previs√£o $\hat{Y}_{t+j|t}$ |
>   |-------------------|------------------------|
>   | 1                 | 11.6                   |
>   | 2                 | 11.52                  |
>   | 3                 | 11.384                 |
>   | 4                 | 11.2728                |
>   | 5                 | 11.1918                |
>   | 10                | 10.524                 |
>   | 20                | 10.102                 |
>   | 50                | 10.003                 |
>
> √â evidente que √† medida que *j* aumenta, as previs√µes se aproximam da m√©dia $\mu = 10$, ilustrando o Teorema 1.1.

Essa metodologia recursiva e iterativa √© muito valiosa na pr√°tica, pois permite que se ajustem as previs√µes √† medida que novos dados se tornam dispon√≠veis e o horizonte da previs√£o se aproxima.  Note que, para previs√µes com um horizonte de tempo maior do que o n√∫mero de defasagens (p), utiliza-se as previs√µes anteriores e n√£o os valores observados para calcular a previs√£o atual.

### Conclus√£o
A lei das proje√ß√µes iteradas fornece uma base s√≥lida para a constru√ß√£o de previs√µes de m√∫ltiplos per√≠odos √† frente de forma recursiva, utilizando as informa√ß√µes dispon√≠veis em cada per√≠odo. Esta abordagem,  desenvolvida para os processos AR(p), permite atualizar as previs√µes √† medida que novas observa√ß√µes se tornam dispon√≠veis, adaptando as previs√µes anteriores. Como observado anteriormente, para modelos ARMA, o resultado √© um modelo autoregressivo com choques correspondentes [^4.2.43]. Este m√©todo recursivo e iterativo √© crucial para aplica√ß√µes pr√°ticas, pois reflete a realidade de que as previs√µes s√£o frequentemente atualizadas √† medida que os dados se acumulam. Ao usar as previs√µes dos instantes anteriores em vez dos valores observados, garantimos a efici√™ncia e a coer√™ncia das previs√µes ao longo do tempo.

**Corol√°rio 1:** A lei das proje√ß√µes iteradas pode ser usada para derivar os erros de previs√£o para diferentes horizontes temporais. Estes erros s√£o cruciais para quantificar a incerteza associada a cada previs√£o e para construir intervalos de confian√ßa. A an√°lise do comportamento dos erros de previs√£o com o aumento do horizonte de tempo √© fundamental para determinar a confiabilidade das previs√µes de longo prazo.

### Refer√™ncias
[^4.2.10]:  *Se√ß√£o 4.2 do texto base*
[^4.2.11]:  *Se√ß√£o 4.2 do texto base*
[^4.2.24]:  *Se√ß√£o 4.2 do texto base*
[^4.2.26]:  *Se√ß√£o 4.2 do texto base*
[^4.2.27]:  *Se√ß√£o 4.2 do texto base*
[^4.2.43]: *Se√ß√£o 4.2 do texto base*
<!-- END -->
