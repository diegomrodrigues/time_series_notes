## PrevisÃ£o em Processos AR(p) EstacionÃ¡rios: DecomposiÃ§Ã£o e ConvergÃªncia

### IntroduÃ§Ã£o
Este capÃ­tulo aprofunda a anÃ¡lise da previsÃ£o em processos AR(p) estacionÃ¡rios, com foco na representaÃ§Ã£o da variÃ¡vel em termos de suas defasagens e erros futuros, explorando a forma da soluÃ§Ã£o de uma equaÃ§Ã£o de diferenÃ§as de ordem *p* para calcular a previsÃ£o Ã³tima. Em continuidade aos tÃ³picos anteriores [^SECTION_PLACEHOLDER] e [^PREV_TOPIC], que abordaram a lei das projeÃ§Ãµes iteradas e o decaimento geomÃ©trico em processos AR(1), serÃ¡ demonstrado como um processo AR(p) pode ser decomposto em termos de seus valores passados e futuros erros, e como essa decomposiÃ§Ã£o permite uma melhor compreensÃ£o da estrutura das previsÃµes e da convergÃªncia para a mÃ©dia de um processo estacionÃ¡rio.

### Conceitos Fundamentais
Conforme estabelecido em [^PREV_TOPIC], a previsÃ£o Ã³tima de um processo AR(p) Ã© calculada de forma iterativa, utilizando a lei das projeÃ§Ãµes iteradas.  Em resumo, um processo AR(p) estacionÃ¡rio Ã© definido como:
$$(Y_t - \mu) = \phi_1(Y_{t-1} - \mu) + \phi_2(Y_{t-2} - \mu) + \ldots + \phi_p(Y_{t-p} - \mu) + \epsilon_t$$
Onde $\mu$ Ã© a mÃ©dia do processo, $\phi_1, \phi_2, ..., \phi_p$ sÃ£o os coeficientes autorregressivos, e $\epsilon_t$ Ã© um erro de ruÃ­do branco com mÃ©dia zero e variÃ¢ncia constante $\sigma^2$. A previsÃ£o de *s* perÃ­odos Ã  frente, $\hat{Y}_{t+s|t}$, baseada nas informaÃ§Ãµes disponÃ­veis no instante *t*, Ã© dada pela iteraÃ§Ã£o da equaÃ§Ã£o acima.
A principal dificuldade surge quando se tenta expressar $Y_{t+s}$ em termos de $Y_t$, $Y_{t-1}$, ... e dos $\epsilon$ futuros ($\epsilon_{t+1}$, $\epsilon_{t+2}$, ...).  Uma abordagem Ã© basear a previsÃ£o na soluÃ§Ã£o da equaÃ§Ã£o de diferenÃ§as de ordem *p* associada ao modelo AR(p), que, conforme a equaÃ§Ã£o [^4.2.20], se expressa como:
$$Y_{t+s} - \mu = f_1^s(Y_t - \mu) + f_2^s(Y_{t-1} - \mu) + \ldots + f_p^s(Y_{t-p+1} - \mu) + \epsilon_{t+s} + \psi_1 \epsilon_{t+s-1} + \psi_2 \epsilon_{t+s-2} + \ldots + \psi_{s-1} \epsilon_{t+1}$$
onde os termos $f_i^s$ sÃ£o funÃ§Ãµes dos coeficientes $\phi_i$ e do horizonte de previsÃ£o *s*. Esta equaÃ§Ã£o revela a estrutura da previsÃ£o de *s* perÃ­odos Ã  frente, mostrando como ela se decompÃµe em funÃ§Ã£o dos valores passados de $Y$ e dos erros futuros.

A previsÃ£o Ã³tima $\hat{Y}_{t+s|t}$  Ã© obtida ao tomar a esperanÃ§a condicional da equaÃ§Ã£o anterior em relaÃ§Ã£o as informaÃ§Ãµes em *t*:
$$ \hat{Y}_{t+s|t} - \mu = f_1^s(Y_t - \mu) + f_2^s(Y_{t-1} - \mu) + \ldots + f_p^s(Y_{t-p+1} - \mu) $$
Note que os erros futuros, por serem independentes do presente e do passado, desaparecem da equaÃ§Ã£o da previsÃ£o. Assim, a previsÃ£o Ã© expressa em termos de valores passados de $Y$ atravÃ©s da funÃ§Ã£o $f_i^s$. Para encontrar os valores das funÃ§Ãµes $f_i^s$ Ã© necessÃ¡rio encontrar a soluÃ§Ã£o geral da equaÃ§Ã£o de diferenÃ§as, e usar as condiÃ§Ãµes iniciais para determinar os coeficientes especÃ­ficos.

> ðŸ’¡ **Exemplo NumÃ©rico:** Considere um processo AR(2) com $\mu = 10$, $\phi_1 = 0.7$, e $\phi_2 = 0.2$. A equaÃ§Ã£o do processo Ã©
>
> $$ (Y_t - 10) = 0.7(Y_{t-1} - 10) + 0.2(Y_{t-2} - 10) + \epsilon_t $$
>
>  ou
>
> $$ Y_t = 10 + 0.7(Y_{t-1} - 10) + 0.2(Y_{t-2} - 10) + \epsilon_t $$
> $$ Y_t = 0.7Y_{t-1} + 0.2Y_{t-2} + 10 - 7 - 2 + \epsilon_t $$
> $$ Y_t = 0.7Y_{t-1} + 0.2Y_{t-2} + 1 + \epsilon_t $$
>
>  Para encontrar a previsÃ£o de 2 perÃ­odos Ã  frente a partir de valores em *t*, precisamos expressar $Y_{t+2}$ em termos de valores em *t* e dos erros futuros. Primeiro, podemos escrever:
>
>  $$ Y_{t+1} = 0.7Y_t + 0.2Y_{t-1} + 1 + \epsilon_{t+1} $$
>  $$ Y_{t+2} = 0.7Y_{t+1} + 0.2Y_t + 1 + \epsilon_{t+2} $$
>  Substituindo a primeira equaÃ§Ã£o na segunda:
>
>  $$ Y_{t+2} = 0.7 (0.7Y_t + 0.2Y_{t-1} + 1 + \epsilon_{t+1}) + 0.2Y_t + 1 + \epsilon_{t+2} $$
>  $$ Y_{t+2} = (0.7^2 + 0.2)Y_t + (0.7 \times 0.2)Y_{t-1} + 0.7 + 1 + 0.7\epsilon_{t+1} + \epsilon_{t+2} $$
>  $$ Y_{t+2} = 0.69Y_t + 0.14Y_{t-1} + 1.7 + 0.7\epsilon_{t+1} + \epsilon_{t+2} $$
>
>  Assim, a previsÃ£o de 2 perÃ­odos Ã  frente Ã©:
>
>  $$\hat{Y}_{t+2|t} = 0.69Y_t + 0.14Y_{t-1} + 1.7$$
>  ou, usando a forma centrada:
>  $$\hat{Y}_{t+2|t} - 10 = 0.69(Y_t-10) + 0.14(Y_{t-1}-10)$$
>  Observe que os coeficientes de $Y_t$ e $Y_{t-1}$ sÃ£o obtidos a partir da forma da soluÃ§Ã£o da equaÃ§Ã£o de diferenÃ§as. Em geral, os coeficientes $f_i^s$ sÃ£o encontrados atravÃ©s da soluÃ§Ã£o da equaÃ§Ã£o de diferenÃ§as de ordem *p* associada ao processo AR(p). Esta soluÃ§Ã£o depende dos parÃ¢metros do modelo AR(p) ($\phi_1, \phi_2, ..., \phi_p$) e do horizonte de previsÃ£o *s*.

**ObservaÃ§Ã£o 1:** A soluÃ§Ã£o da equaÃ§Ã£o de diferenÃ§as para um processo AR(p) pode ser obtida atravÃ©s da anÃ¡lise das raÃ­zes do polinÃ´mio caracterÃ­stico associado, como explorado em [^4.2.20]. A estacionariedade do processo AR(p) garante que estas raÃ­zes estejam fora do cÃ­rculo unitÃ¡rio, o que, por sua vez, assegura o decaimento dos coeficientes $f_i^s$ Ã  medida que o horizonte de previsÃ£o *s* aumenta. Essa anÃ¡lise das raÃ­zes Ã© crucial para entender a dinÃ¢mica de longo prazo e a convergÃªncia das previsÃµes para a mÃ©dia.

**Lema 4:** A previsÃ£o Ã³tima $\hat{Y}_{t+s|t}$ para um processo AR(p) estacionÃ¡rio converge para a mÃ©dia incondicional $\mu$ Ã  medida que o horizonte de previsÃ£o *s* tende ao infinito.

*Proof:*

I. A previsÃ£o de *s* perÃ­odos Ã  frente para um processo AR(p) Ã© dada por:
$$\hat{Y}_{t+s|t} - \mu = f_1^s(Y_t - \mu) + f_2^s(Y_{t-1} - \mu) + \ldots + f_p^s(Y_{t-p+1} - \mu)$$
onde $f_i^s$ sÃ£o funÃ§Ãµes que dependem dos parÃ¢metros do modelo AR(p) e de *s*.

II. Como o processo AR(p) Ã© estacionÃ¡rio, as raÃ­zes do polinÃ´mio caracterÃ­stico associado ($1 - \phi_1z - \phi_2z^2 - \ldots - \phi_pz^p = 0$) estÃ£o fora do cÃ­rculo unitÃ¡rio no plano complexo.

III. A estacionariedade implica que o impacto de observaÃ§Ãµes passadas sobre as previsÃµes futuras decresce exponencialmente com o aumento do horizonte de tempo *s*.  Isso garante que os coeficientes $f_i^s$ tendam a zero quando *s* tende ao infinito.

IV. Portanto:
$$\lim_{s \to \infty} f_i^s = 0$$

V. Como os termos $(Y_{t-i+1} - \mu)$ sÃ£o finitos, temos:
$$\lim_{s \to \infty} (\hat{Y}_{t+s|t} - \mu) = 0$$
VI. Portanto, a previsÃ£o converge para a mÃ©dia incondicional:
$$\lim_{s \to \infty} \hat{Y}_{t+s|t} = \mu$$
â– 

**Lema 4.1:** Para um processo AR(p) estacionÃ¡rio, os coeficientes $\psi_j$ na representaÃ§Ã£o de $Y_{t+s}$ em funÃ§Ã£o dos erros futuros, ou seja,
$$Y_{t+s} - \mu = f_1^s(Y_t - \mu) + f_2^s(Y_{t-1} - \mu) + \ldots + f_p^s(Y_{t-p+1} - \mu) + \epsilon_{t+s} + \psi_1 \epsilon_{t+s-1} + \psi_2 \epsilon_{t+s-2} + \ldots + \psi_{s-1} \epsilon_{t+1}$$
tambÃ©m decrescem Ã  medida que o Ã­ndice *j* aumenta.

*Proof:*  A representaÃ§Ã£o de $Y_{t+s}$ em termos de erros futuros Ã© obtida pela iteraÃ§Ã£o da equaÃ§Ã£o do processo AR(p), como demonstrado no exemplo numÃ©rico para o caso AR(2). Como o processo Ã© estacionÃ¡rio, os coeficientes $\psi_j$ sÃ£o determinados pela estrutura do modelo AR(p), e refletem o impacto decrescente de um choque no processo em diferentes perÃ­odos no futuro.  A estacionariedade garante que o efeito de choques passados decresÃ§a no tempo, logo o impacto dos erros futuros decresce Ã  medida que o Ã­ndice *j* aumenta.
â– 

**CorolÃ¡rio 4:** A convergÃªncia para a mÃ©dia incondicional $\mu$ para um processo AR(p) estacionÃ¡rio Ã© garantida pela estacionariedade do processo, que implica um decaimento da influÃªncia de valores passados e dos erros futuros sobre a previsÃ£o, fazendo com que previsÃµes de longo prazo se aproximem da mÃ©dia do processo.

*Proof:*  Segue diretamente do Lema 4 e da definiÃ§Ã£o de um processo estacionÃ¡rio. A condiÃ§Ã£o de estacionariedade restringe os parÃ¢metros do modelo (os $\phi_i$), e essas restriÃ§Ãµes garantem que o impacto de condiÃ§Ãµes iniciais na previsÃ£o se tornem cada vez menos relevantes Ã  medida que o horizonte de previsÃ£o aumenta.
â– 

> ðŸ’¡ **Exemplo NumÃ©rico:**  Utilizando o mesmo processo AR(2) do exemplo anterior, com $\mu = 10$, $\phi_1 = 0.7$, e $\phi_2 = 0.2$, podemos analisar a convergÃªncia para a mÃ©dia.
>
>   - $\hat{Y}_{t+1|t} = 0.7 Y_t + 0.2 Y_{t-1} + 1$ (jÃ¡ calculado no exemplo anterior, usando iteraÃ§Ã£o do modelo)
>   - $\hat{Y}_{t+2|t} = 0.69 Y_t + 0.14 Y_{t-1} + 1.7$ (jÃ¡ calculado no exemplo anterior, usando a forma da soluÃ§Ã£o da equaÃ§Ã£o de diferenÃ§as)
>
>  Para encontrar $\hat{Y}_{t+3|t}$ e observar a tendÃªncia, precisamos expressar $Y_{t+3}$ em termos de $Y_t$, $Y_{t-1}$,...
>
> $$Y_{t+3} = 0.7Y_{t+2} + 0.2Y_{t+1} + 1 + \epsilon_{t+3}$$
>
>  Substituindo:
>
> $$Y_{t+3} = 0.7(0.69Y_t + 0.14Y_{t-1} + 1.7 + 0.7\epsilon_{t+1} + \epsilon_{t+2}) + 0.2(0.7Y_t + 0.2Y_{t-1} + 1 + \epsilon_{t+1}) + 1 + \epsilon_{t+3}$$
>
> $$Y_{t+3} = (0.7 \times 0.69 + 0.2 \times 0.7)Y_t + (0.7 \times 0.14 + 0.2 \times 0.2)Y_{t-1}  + 0.7 \times 1.7 + 0.2 \times 1 + 1 + 0.7^2\epsilon_{t+1} + 0.2\epsilon_{t+1} + 0.7\epsilon_{t+2}+ \epsilon_{t+3}$$
>
> $$Y_{t+3} = 0.623Y_t + 0.138Y_{t-1}  + 3.39 + 0.69\epsilon_{t+1} + 0.7\epsilon_{t+2}+ \epsilon_{t+3}$$
>
>  E portanto:
>
>  $$\hat{Y}_{t+3|t} =  0.623Y_t + 0.138Y_{t-1} + 3.39$$
>  ou, na forma centrada:
>  $$\hat{Y}_{t+3|t} - 10 = 0.623(Y_t-10) + 0.138(Y_{t-1}-10)$$
>
>  Note como os coeficientes dos valores de $Y$ defasados estÃ£o ficando cada vez menores, ilustrando a convergÃªncia para a mÃ©dia (10) com o aumento de *s*. Os valores dos coeficientes podem ser obtidos da soluÃ§Ã£o da equaÃ§Ã£o de diferenÃ§as correspondente.
>
>  Suponha que  $Y_t = 15$ e $Y_{t-1} = 12$:
>
>  - $\hat{Y}_{t+1|t} = 0.7 \times 15 + 0.2 \times 12 + 1= 13.9$
>  - $\hat{Y}_{t+2|t} = 0.69 \times 15 + 0.14 \times 12 + 1.7 = 13.53$
>  - $\hat{Y}_{t+3|t} = 0.623 \times 15 + 0.138 \times 12 + 3.39 = 13.369$
>
>  Como podemos ver, o impacto das observaÃ§Ãµes passadas vai diminuindo em cada passo a frente e as previsÃµes se aproximam da mÃ©dia de 10.
>
>   | Passo Ã  Frente (s) | Coeficiente de $Y_t$ | Coeficiente de $Y_{t-1}$ | Termo Constante | PrevisÃ£o $\hat{Y}_{t+s|t}$ |
>   |-------------------|------------------------|---------------------------|----------------|---------------------------|
>   | 1                 | 0.7                    | 0.2                      | 1              | 13.9                    |
>   | 2                 | 0.69                   | 0.14                      | 1.7            | 13.53                    |
>   | 3                 | 0.623                 | 0.138                       | 3.39             | 13.369                      |
>   | ...                 | ... | ... | ...            | ...|
>
>  Este exemplo demonstra que, para um processo AR(2) estacionÃ¡rio, a previsÃ£o Ã³tima converge para a mÃ©dia incondicional $\mu = 10$ Ã  medida que o horizonte de previsÃ£o *s* aumenta. Observe como a magnitude dos coeficientes $f_i^s$ de $Y_t$ e $Y_{t-1}$ diminui com o aumento de *s*, refletindo a perda de informaÃ§Ã£o e a convergÃªncia para a mÃ©dia.

A forma geral da previsÃ£o de *s* perÃ­odos Ã  frente pode ser expressa como uma combinaÃ§Ã£o linear das observaÃ§Ãµes passadas, com pesos que dependem do horizonte de previsÃ£o e dos parÃ¢metros do modelo AR(p).
**ProposiÃ§Ã£o 2.1:** A representaÃ§Ã£o de um processo AR(p) em termos de seus valores passados e erros futuros pode ser obtida de forma iterativa, seguindo o processo de substituiÃ§Ã£o usado no exemplo numÃ©rico para o caso AR(2), e a soluÃ§Ã£o da equaÃ§Ã£o de diferenÃ§as fornece uma forma geral e explÃ­cita para expressar essa representaÃ§Ã£o.
*Proof:*
I. A equaÃ§Ã£o de um processo AR(p) Ã© definida como:
$$Y_t - \mu = \phi_1(Y_{t-1} - \mu) + \phi_2(Y_{t-2} - \mu) + \ldots + \phi_p(Y_{t-p} - \mu) + \epsilon_t$$
II. Para expressar $Y_{t+s}$ em funÃ§Ã£o dos valores passados e dos erros futuros, pode-se realizar substituiÃ§Ãµes sucessivas, de forma similar ao exemplo numÃ©rico para o caso AR(2).
III. Cada substituiÃ§Ã£o de $Y_{t+k}$  em termos de seus valores defasados adiciona um termo $\epsilon_{t+k}$  e modifica os coeficientes dos valores passados de $Y$.
IV. Esse processo iterativo gera uma expressÃ£o para $Y_{t+s}$ como uma combinaÃ§Ã£o linear dos valores passados $Y_t, Y_{t-1}, \ldots$ e dos erros futuros $\epsilon_{t+1}, \epsilon_{t+2}, \ldots, \epsilon_{t+s}$, cujos coeficientes dependem da estrutura do modelo AR(p) e do horizonte de previsÃ£o *s*.
V. A soluÃ§Ã£o da equaÃ§Ã£o de diferenÃ§as correspondente ao processo AR(p) fornece uma forma geral para essa representaÃ§Ã£o, permitindo obter os coeficientes $f_i^s$ e $\psi_j$ em funÃ§Ã£o dos parÃ¢metros do modelo AR(p) e do horizonte de previsÃ£o *s*.
â– 

**ProposiÃ§Ã£o 2:** A soluÃ§Ã£o geral da equaÃ§Ã£o de diferenÃ§as para um processo AR(p), expressa como a representaÃ§Ã£o da variÃ¡vel em termos de suas defasagens e erros futuros, permite calcular a previsÃ£o Ã³tima de *s* perÃ­odos Ã  frente como uma combinaÃ§Ã£o linear dos valores defasados e dos parÃ¢metros do modelo, que converge para a mÃ©dia do processo quando *s* tende para o infinito.

*Proof:*
I.  A soluÃ§Ã£o geral da equaÃ§Ã£o de diferenÃ§as associada ao modelo AR(p) expressa $Y_{t+s}$ como uma funÃ§Ã£o dos valores defasados $Y_t, Y_{t-1}, ...$ e dos erros futuros $\epsilon_{t+1}, \epsilon_{t+2},...$:
$$Y_{t+s} - \mu = f_1^s(Y_t - \mu) + f_2^s(Y_{t-1} - \mu) + \ldots + f_p^s(Y_{t-p+1} - \mu) + \epsilon_{t+s} + \psi_1 \epsilon_{t+s-1} + \psi_2 \epsilon_{t+s-2} + \ldots + \psi_{s-1} \epsilon_{t+1}$$
II. A previsÃ£o Ã³tima $\hat{Y}_{t+s|t}$ Ã© obtida tomando a esperanÃ§a condicional de $Y_{t+s}$ no tempo $t$, o que elimina os termos com erros futuros, pois $E[\epsilon_{t+k}|Y_t, Y_{t-1}, ...] = 0$ para $k > 0$:
$$\hat{Y}_{t+s|t} - \mu = E[Y_{t+s} - \mu|Y_t, Y_{t-1}, ...] = f_1^s(Y_t - \mu) + f_2^s(Y_{t-1} - \mu) + \ldots + f_p^s(Y_{t-p+1} - \mu)$$
III. De acordo com o Lema 4, quando o processo Ã© estacionÃ¡rio, os coeficientes $f_i^s$ tendem a zero Ã  medida que *s* tende ao infinito, ou seja:
$$ \lim_{s \to \infty} f_i^s = 0 $$
IV. Portanto, a previsÃ£o Ã³tima converge para a mÃ©dia $\mu$ quando *s* tende ao infinito:
$$ \lim_{s \to \infty} \hat{Y}_{t+s|t} = \mu $$
V. ConcluÃ­mos que a soluÃ§Ã£o geral da equaÃ§Ã£o de diferenÃ§as permite obter uma representaÃ§Ã£o da variÃ¡vel em termos de suas defasagens e erros futuros, e esse mÃ©todo possibilita calcular a previsÃ£o Ã³tima e verificar sua convergÃªncia para a mÃ©dia.
â– 

### ConclusÃ£o
Este capÃ­tulo explorou a previsÃ£o em processos AR(p) estacionÃ¡rios, com foco na representaÃ§Ã£o da variÃ¡vel em termos de suas defasagens e erros futuros, utilizando a soluÃ§Ã£o da equaÃ§Ã£o de diferenÃ§as associada ao modelo AR(p). A anÃ¡lise demonstrou como a previsÃ£o Ã³tima para mÃºltiplos perÃ­odos Ã  frente Ã© obtida iterativamente e como o decaimento da influÃªncia das observaÃ§Ãµes passadas e erros futuros leva a uma convergÃªncia para a mÃ©dia do processo quando o horizonte de previsÃ£o tende ao infinito. Os resultados apresentados sÃ£o fundamentais para a compreensÃ£o da estrutura das previsÃµes em modelos AR(p) e para o desenvolvimento de ferramentas de anÃ¡lise de sÃ©ries temporais. A soluÃ§Ã£o da equaÃ§Ã£o de diferenÃ§as fornece uma maneira geral de calcular a previsÃ£o, e de explicitar como as informaÃ§Ãµes passadas influenciam a previsÃ£o futura.

### ReferÃªncias
[^SECTION_PLACEHOLDER]: *TÃ³pico anterior do texto base*
[^PREV_TOPIC]: *TÃ³pico anterior do texto base*
[^4.2.20]:  *SeÃ§Ã£o 4.2 do texto base*
<!-- END -->
