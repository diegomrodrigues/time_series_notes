## Previs√£o Iterativa em Processos AR(1): Decaimento Geom√©trico e Converg√™ncia para a M√©dia

### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise de previs√µes iterativas em modelos AR(1), explorando como a previs√£o √≥tima para m√∫ltiplos per√≠odos √† frente, $\hat{Y}_{t+s|t}$, se comporta √† medida que o horizonte de previs√£o *s* aumenta. Partindo dos fundamentos da lei das proje√ß√µes iteradas [^SECTION_PLACEHOLDER], e em continuidade ao t√≥pico anterior [^PREV_TOPIC], ser√° demonstrado que a previs√£o √≥tima para um processo AR(1) exibe um decaimento geom√©trico do termo $(Y_t - \mu)$ em dire√ß√£o √† m√©dia $\mu$ com o aumento de *s*. Este comportamento √© crucial para a compreens√£o da natureza das previs√µes de longo prazo em modelos AR(1) e suas implica√ß√µes para a modelagem de s√©ries temporais.

### Conceitos Fundamentais
Como introduzido em [^PREV_TOPIC], a lei das proje√ß√µes iteradas permite a obten√ß√£o de previs√µes de m√∫ltiplos per√≠odos atrav√©s da aplica√ß√£o recursiva da mesma estrutura de pondera√ß√£o que comp√µe o modelo autorregressivo. Para o caso espec√≠fico de um processo AR(1) estacion√°rio, definido como:
$$(Y_t - \mu) = \phi(Y_{t-1} - \mu) + \epsilon_t$$ [^4.2.14]
onde $\mu$ √© a m√©dia do processo, $\phi$ √© o coeficiente autorregressivo, e $\epsilon_t$ √© um erro de ru√≠do branco.  A previs√£o √≥tima de *s* per√≠odos √† frente para o instante *t*, $\hat{Y}_{t+s|t}$,  √© obtida iterando-se a equa√ß√£o do modelo AR(1), resultando em:
$$ \hat{Y}_{t+s|t} - \mu = \phi^s(Y_t - \mu) $$ [^4.2.19]
Esta equa√ß√£o demonstra que a previs√£o de *s* per√≠odos √† frente √© obtida pela aplica√ß√£o recursiva do coeficiente autorregressivo $\phi$, elevado √† pot√™ncia *s*, sobre a defasagem $(Y_t - \mu)$. Este resultado √© uma aplica√ß√£o direta da lei das proje√ß√µes iteradas, onde a previs√£o de um per√≠odo √† frente no tempo t+1 √© utilizada recursivamente para obter a previs√£o de dois per√≠odos √† frente e assim por diante.

> üí° **Exemplo Num√©rico:** Considere um processo AR(1) com m√©dia $\mu = 10$ e coeficiente autorregressivo $\phi = 0.8$. Suponha que, no instante *t*, observamos $Y_t = 15$.
>
>   **Previs√£o de um per√≠odo √† frente (s=1):**
>   $$ \hat{Y}_{t+1|t} - 10 = 0.8^1 (15 - 10) = 0.8 \times 5 = 4 $$
>   $$ \hat{Y}_{t+1|t} = 14 $$
>
>   **Previs√£o de dois per√≠odos √† frente (s=2):**
>    $$ \hat{Y}_{t+2|t} - 10 = 0.8^2 (15 - 10) = 0.64 \times 5 = 3.2 $$
>    $$ \hat{Y}_{t+2|t} = 13.2 $$
>
>  **Previs√£o de tr√™s per√≠odos √† frente (s=3):**
>    $$ \hat{Y}_{t+3|t} - 10 = 0.8^3 (15 - 10) = 0.512 \times 5 = 2.56 $$
>    $$ \hat{Y}_{t+3|t} = 12.56 $$
>
>   Este exemplo ilustra como o impacto da observa√ß√£o inicial $(Y_t - \mu)$ decresce geometricamente com o aumento do horizonte de previs√£o *s*, tendendo √† m√©dia do processo, que nesse caso √© 10.

O termo $\phi^s$ √© um fator de decaimento geom√©trico, que determina como a informa√ß√£o inicial $(Y_t - \mu)$ √© amortecida √† medida que o horizonte de previs√£o aumenta. A taxa de decaimento √© governada pela magnitude do coeficiente autorregressivo $\phi$. Para um processo AR(1) estacion√°rio, $|\phi|<1$,  o que garante que a previs√£o convirja para a m√©dia $\mu$ √† medida que o horizonte de previs√£o *s* se torna grande.
O valor de $\phi$ est√° diretamente ligado √† velocidade de converg√™ncia da previs√£o para a m√©dia do processo. Quanto menor o valor absoluto de $\phi$, mais r√°pido a previs√£o se aproxima da m√©dia do processo com o aumento de *s*.

**Lema 3:** Para um processo AR(1) estacion√°rio, a previs√£o √≥tima de *s* per√≠odos √† frente, $\hat{Y}_{t+s|t}$, converge para a m√©dia incondicional $\mu$ √† medida que *s* tende ao infinito.

*Proof:*
I.  Partimos da equa√ß√£o da previs√£o √≥tima de *s* per√≠odos √† frente para um processo AR(1):
$$\hat{Y}_{t+s|t} - \mu = \phi^s(Y_t - \mu)$$

II. Para que um processo AR(1) seja estacion√°rio, o coeficiente autorregressivo $\phi$ deve satisfazer a condi√ß√£o $|\phi| < 1$.

III.  √Ä medida que *s* tende ao infinito, temos:
$$\lim_{s \to \infty} \phi^s = 0$$

IV. Aplicando este limite na equa√ß√£o de previs√£o, obtemos:
$$ \lim_{s \to \infty} (\hat{Y}_{t+s|t} - \mu) = \lim_{s \to \infty} \phi^s(Y_t - \mu) = 0 \times (Y_t - \mu) = 0$$

V.  Portanto,
$$ \lim_{s \to \infty} \hat{Y}_{t+s|t} = \mu $$
‚ñ†

**Lema 3.1:** O erro de previs√£o para um horizonte *s* per√≠odos √† frente, definido como $e_{t+s|t} = Y_{t+s} - \hat{Y}_{t+s|t}$, tem m√©dia zero e vari√¢ncia que aumenta com *s*, aproximando-se da vari√¢ncia incondicional do processo √† medida que *s* tende ao infinito.

*Proof:*
I.  A partir da defini√ß√£o do processo AR(1), temos:
$$Y_{t+s} - \mu = \phi^s(Y_t - \mu) + \sum_{j=0}^{s-1} \phi^j \epsilon_{t+s-j}$$

II. A previs√£o √≥tima para *s* per√≠odos √† frente √©:
$$\hat{Y}_{t+s|t} - \mu = \phi^s(Y_t - \mu)$$

III. O erro de previs√£o √© dado por:
$$e_{t+s|t} = Y_{t+s} - \hat{Y}_{t+s|t} = \sum_{j=0}^{s-1} \phi^j \epsilon_{t+s-j}$$

IV.  A m√©dia do erro de previs√£o √©:
$$E[e_{t+s|t}] = E[\sum_{j=0}^{s-1} \phi^j \epsilon_{t+s-j}] = \sum_{j=0}^{s-1} \phi^j E[\epsilon_{t+s-j}] = 0$$
j√° que $E[\epsilon_t] = 0$.

V. A vari√¢ncia do erro de previs√£o √©:
$$Var[e_{t+s|t}] = Var[\sum_{j=0}^{s-1} \phi^j \epsilon_{t+s-j}] = \sum_{j=0}^{s-1} \phi^{2j} Var[\epsilon_{t+s-j}] = \sigma^2 \sum_{j=0}^{s-1} \phi^{2j} $$
onde $\sigma^2$ √© a vari√¢ncia de $\epsilon_t$, e usamos o fato que os erros s√£o independentes e identicamente distribu√≠dos.

VI. √Ä medida que *s* tende ao infinito, a vari√¢ncia do erro converge para:
$$\lim_{s \to \infty} Var[e_{t+s|t}] = \sigma^2 \sum_{j=0}^{\infty} \phi^{2j} = \frac{\sigma^2}{1-\phi^2}$$
que corresponde √† vari√¢ncia incondicional do processo AR(1).
‚ñ†
> üí° **Exemplo Num√©rico (Erro de Previs√£o):** Usando o mesmo processo AR(1) do exemplo anterior, com $\mu=10$, $\phi=0.8$ e $\sigma^2=1$, podemos verificar o comportamento da vari√¢ncia do erro de previs√£o com o aumento de *s*.
>   - $Var[e_{t+1|t}] = \sigma^2=1$
>   - $Var[e_{t+2|t}] = \sigma^2 (1 + \phi^2) = 1+0.64=1.64$
>   - $Var[e_{t+3|t}] = \sigma^2 (1 + \phi^2 + \phi^4) = 1+0.64+0.4096=2.0496$
>   -  $\lim_{s \to \infty} Var[e_{t+s|t}] = \frac{\sigma^2}{1-\phi^2} = \frac{1}{1-0.64} = 2.777...$

> Este exemplo mostra que √† medida que *s* aumenta, a vari√¢ncia do erro de previs√£o se aproxima da vari√¢ncia incondicional do processo AR(1).

> **Observa√ß√£o:** O Lema 3.1 refor√ßa a ideia de que a previsibilidade diminui com o aumento do horizonte de previs√£o.  A vari√¢ncia do erro de previs√£o aumenta, refletindo a incerteza sobre os valores futuros da s√©rie temporal.

> Al√©m disso, podemos expressar o erro de previs√£o $e_{t+s|t}$ como uma combina√ß√£o linear dos erros de ru√≠do branco $\epsilon_{t+s-j}$, o que √© consistente com o fato de que os choques aleat√≥rios s√£o os √∫nicos elementos que impedem a previs√£o perfeita.

>
>  | Passo √† Frente (s) | Vari√¢ncia do erro de Previs√£o $Var[e_{t+s|t}]$ |
>  |-------------------|--------------------------------------------|
>  | 1                 | 1                                          |
>  | 2                 | 1.64                                       |
>  | 3                 | 2.0496                                     |
>  | 4                 | 2.3297                                     |
>  | 5                 | 2.543                                    |
>  | $\infty$          | 2.777...                                    |

> **Proposi√ß√£o 1:** A previs√£o √≥tima de s-per√≠odos √† frente minimiza o erro quadr√°tico m√©dio de previs√£o.

*Proof:* O erro quadr√°tico m√©dio de previs√£o √© dado por $E[(Y_{t+s} - \hat{Y}_{t+s|t})^2]$.  Como $\hat{Y}_{t+s|t}$ √© a proje√ß√£o ortogonal de $Y_{t+s}$ no espa√ßo gerado pelas informa√ß√µes dispon√≠veis no instante *t*, ent√£o o erro de previs√£o √© ortogonal a qualquer fun√ß√£o dessas informa√ß√µes, em particular, ao pr√≥prio $\hat{Y}_{t+s|t}$. Assim:
$$E[(Y_{t+s} - \hat{Y}_{t+s|t})\hat{Y}_{t+s|t}] = 0$$
Qualquer outra previs√£o $\tilde{Y}_{t+s|t}$ pode ser escrita como $\tilde{Y}_{t+s|t} = \hat{Y}_{t+s|t} + \delta$, onde $\delta$ √© uma fun√ß√£o das informa√ß√µes em *t*. O erro quadr√°tico m√©dio de $\tilde{Y}_{t+s|t}$ √©:
$$E[(Y_{t+s} - \tilde{Y}_{t+s|t})^2] = E[(Y_{t+s} - \hat{Y}_{t+s|t} - \delta)^2]$$
Expandindo:
$$E[(Y_{t+s} - \hat{Y}_{t+s|t})^2] + E[\delta^2] - 2E[(Y_{t+s} - \hat{Y}_{t+s|t})\delta]$$
O √∫ltimo termo √© zero por conta da ortogonalidade. Assim:
$$E[(Y_{t+s} - \tilde{Y}_{t+s|t})^2] = E[(Y_{t+s} - \hat{Y}_{t+s|t})^2] + E[\delta^2] \ge E[(Y_{t+s} - \hat{Y}_{t+s|t})^2]$$
Portanto, a previs√£o √≥tima $\hat{Y}_{t+s|t}$ minimiza o erro quadr√°tico m√©dio de previs√£o.
‚ñ†

> **Corol√°rio 1:** Como a previs√£o √≥tima minimiza o erro quadr√°tico m√©dio, temos que a vari√¢ncia do erro de previs√£o calculada no Lema 3.1 representa o m√≠nimo poss√≠vel para qualquer previs√£o baseada nas informa√ß√µes dispon√≠veis no instante *t*.

> *Proof:* Segue diretamente da Proposi√ß√£o 1, j√° que a vari√¢ncia do erro de previs√£o √© uma medida do erro quadr√°tico m√©dio.

> **Lema 3.2:** Se o ru√≠do branco $\epsilon_t$ for gaussiano, o erro de previs√£o $e_{t+s|t}$ tamb√©m ser√° gaussiano, com m√©dia zero e a vari√¢ncia calculada no Lema 3.1.
>
>*Proof:* O erro de previs√£o √© uma combina√ß√£o linear de vari√°veis aleat√≥rias gaussianas independentes, $\epsilon_{t+s-j}$. Portanto, por propriedades da distribui√ß√£o normal, o erro de previs√£o tamb√©m ser√° gaussiano. A m√©dia e a vari√¢ncia s√£o dadas pelos resultados do Lema 3.1.
>
> Esta propriedade √© particularmente √∫til para a constru√ß√£o de intervalos de confian√ßa para as previs√µes.

> **Observa√ß√£o:** Os resultados apresentados neste cap√≠tulo s√£o v√°lidos para processos AR(1) estacion√°rios, ou seja, para $|\phi| < 1$. Caso $|\phi| \ge 1$, o processo n√£o ser√° estacion√°rio e os resultados apresentados n√£o se aplicam. O comportamento das previs√µes para processos n√£o estacion√°rios √© mais complexo e foge do escopo deste cap√≠tulo.

>
>  **Teorema 1** Para o processo AR(1) estacion√°rio $(Y_t - \mu) = \phi(Y_{t-1} - \mu) + \epsilon_t$, a autocovari√¢ncia $\gamma_k = Cov(Y_t,Y_{t-k})$ √© dada por:
>
> $$\gamma_k = \phi^k \frac{\sigma^2}{1-\phi^2}$$
>
> *Proof:*
>
> I.  Sabemos que $Y_t - \mu = \phi(Y_{t-1} - \mu) + \epsilon_t$ e que a vari√¢ncia de $Y_t$ √© $\gamma_0 = \frac{\sigma^2}{1-\phi^2}$
>
> II.  Multiplicando a equa√ß√£o do processo por $Y_{t-k}-\mu$ e tomando a esperan√ßa, temos:
>
> $$ E[(Y_t - \mu)(Y_{t-k} - \mu)] = E[\phi(Y_{t-1} - \mu)(Y_{t-k} - \mu)] + E[\epsilon_t(Y_{t-k} - \mu)] $$
>
> III. O termo  $E[\epsilon_t(Y_{t-k} - \mu)] = 0$ para $k>0$, j√° que o erro √© independente de realiza√ß√µes passadas do processo.
>
> IV. Assim, temos que $\gamma_k = \phi \gamma_{k-1}$.
>
> V.  Aplicando iterativamente esta express√£o, obtemos:
>
> $$\gamma_k = \phi^k \gamma_0 = \phi^k \frac{\sigma^2}{1-\phi^2}$$
>
> O resultado demonstra que a autocovari√¢ncia decai geometricamente com o aumento do lag k, com uma taxa de decaimento dada por $\phi$.
‚ñ†

> **Corol√°rio 1.1:** A autocorrela√ß√£o $\rho_k$ para um processo AR(1) estacion√°rio √© dada por $\rho_k = \phi^k$.
>
>*Proof:* A autocorrela√ß√£o √© definida como $\rho_k = \frac{\gamma_k}{\gamma_0}$. Substituindo os valores de $\gamma_k$ e $\gamma_0$ obtemos o resultado.
>
>Este resultado demonstra que a autocorrela√ß√£o tamb√©m decai geometricamente com o aumento do lag k.

O valor de $\phi$ est√° diretamente ligado √† velocidade de converg√™ncia da previs√£o para a m√©dia do processo. Quanto menor o valor absoluto de $\phi$, mais r√°pido a previs√£o se aproxima da m√©dia do processo com o aumento de *s*.

**Corol√°rio 3:** A taxa de converg√™ncia da previs√£o $\hat{Y}_{t+s|t}$ para a m√©dia $\mu$ √© diretamente proporcional ao valor absoluto de $\phi$. Quanto menor $|\phi|$, mais r√°pida a converg√™ncia, enquanto valores de $|\phi|$ mais pr√≥ximos de 1 levam a uma converg√™ncia mais lenta.

*Proof:* A velocidade com que a previs√£o converge para a m√©dia √© determinada pela forma como o termo $\phi^s$ se aproxima de zero com o aumento de *s*. O valor absoluto de $\phi$ controla a taxa de decaimento geom√©trico, onde:
$$ \lim_{s \to \infty} \phi^s = 0, \text{ quando } |\phi| < 1 $$
Quanto mais pr√≥ximo de zero for o valor absoluto de $\phi$, mais rapidamente $\phi^s$ se aproxima de zero, levando a uma converg√™ncia mais r√°pida da previs√£o para a m√©dia do processo.
‚ñ†
> üí° **Exemplo Num√©rico (Compara√ß√£o):** Para comparar a taxa de converg√™ncia, vamos considerar dois processos AR(1), um com $\phi = 0.5$ e outro com $\phi = 0.9$, ambos com $\mu=0$. Supondo $Y_t = 5$ e comparando as previs√µes com diferentes valores de *s*, temos:
>
>   **Processo 1: $\phi = 0.5$**
>    - $\hat{Y}_{t+1|t} = 0.5 \times 5 = 2.5$
>    - $\hat{Y}_{t+2|t} = 0.5^2 \times 5 = 1.25$
>    - $\hat{Y}_{t+3|t} = 0.5^3 \times 5 = 0.625$
>    - $\hat{Y}_{t+10|t} = 0.5^{10} \times 5 = 0.0049$
>
>  **Processo 2: $\phi = 0.9$**
>   - $\hat{Y}_{t+1|t} = 0.9 \times 5 = 4.5$
>   - $\hat{Y}_{t+2|t} = 0.9^2 \times 5 = 4.05$
>   - $\hat{Y}_{t+3|t} = 0.9^3 \times 5 = 3.645$
>   - $\hat{Y}_{t+10|t} = 0.9^{10} \times 5 = 1.74$
>
>   As previs√µes do processo com $\phi = 0.5$ se aproximam mais rapidamente de 0 (a m√©dia) do que as previs√µes do processo com $\phi = 0.9$, ilustrando o Corol√°rio 3.

### Conclus√£o
Este cap√≠tulo demonstrou como a lei das proje√ß√µes iteradas se aplica em um processo AR(1), com foco no decaimento geom√©trico do termo $(Y_t - \mu)$ na previs√£o √≥tima de *s* per√≠odos √† frente, $\hat{Y}_{t+s|t}$. A an√°lise mostrou que, √† medida que o horizonte de previs√£o aumenta, o efeito da observa√ß√£o corrente $Y_t$ diminui, e as previs√µes convergem para a m√©dia do processo $\mu$, com a velocidade da converg√™ncia sendo determinada pelo coeficiente autorregressivo $\phi$. Este resultado fornece insights importantes para a compreens√£o do comportamento de previs√µes de longo prazo, especialmente em processos AR(1) estacion√°rios, onde a previsibilidade se reduz com o aumento do horizonte de previs√£o. A compreens√£o destes conceitos √© crucial para a an√°lise e modelagem de s√©ries temporais, bem como para a avalia√ß√£o da confiabilidade das previs√µes geradas.

### Refer√™ncias
[^SECTION_PLACEHOLDER]: *T√≥pico anterior do texto base*
[^PREV_TOPIC]: *T√≥pico anterior do texto base*
[^4.2.14]:  *Se√ß√£o 4.2 do texto base*
[^4.2.19]:  *Se√ß√£o 4.2 do texto base*
<!-- END -->
