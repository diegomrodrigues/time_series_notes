## Previs√£o de M√∫ltiplos Per√≠odos e a Lei das Proje√ß√µes Iteradas em Modelos AR(p)

### Introdu√ß√£o
Em continuidade ao t√≥pico anterior, que abordou a lei das proje√ß√µes iteradas como um m√©todo para gerar previs√µes de m√∫ltiplos per√≠odos a partir de previs√µes de um per√≠odo [^SECTION_PLACEHOLDER], este cap√≠tulo aprofundar√° a an√°lise da aplica√ß√£o desse conceito em modelos AR(p), com foco espec√≠fico na estrutura das previs√µes de um e dois per√≠odos √† frente. Abordaremos como a previs√£o de um per√≠odo √† frente para um processo AR(p) √© constru√≠da a partir de uma combina√ß√£o linear dos seus valores defasados e como a previs√£o de dois per√≠odos √† frente se baseia, de forma iterativa, nas previs√µes de um per√≠odo √† frente utilizando a mesma estrutura de pesos, demonstrando a consist√™ncia e coer√™ncia do m√©todo.

### Conceitos Fundamentais
Como vimos anteriormente [^SECTION_PLACEHOLDER], a previs√£o de um per√≠odo √† frente para um processo AR(p) √© dada por
$$(\hat{Y}_{t+1|t} - \mu) = \phi_1(Y_t - \mu) + \phi_2(Y_{t-1} - \mu) + \ldots + \phi_p(Y_{t-p+1} - \mu)$$ [^4.2.24],
onde $\mu$ √© a m√©dia do processo, $Y_t$ s√£o os valores observados no tempo $t$, e $\phi_1, \phi_2, \ldots, \phi_p$ s√£o os coeficientes autorregressivos. Esta equa√ß√£o expressa $\hat{Y}_{t+1|t}$, a previs√£o de $Y_{t+1}$ feita no instante $t$, como uma combina√ß√£o linear dos valores defasados do processo Y, ponderados pelos coeficientes $\phi_i$.

Em seguida, a previs√£o de dois per√≠odos √† frente, $\hat{Y}_{t+2|t}$, √© obtida utilizando a mesma l√≥gica de pondera√ß√£o, mas aplicando-a ao valor previsto para o instante t+1, $\hat{Y}_{t+1|t}$, bem como a valores observados defasados. Substituindo t por t+1 na equa√ß√£o do processo AR(p), e aplicando o conceito de proje√ß√µes iteradas, obtemos:
$$(\hat{Y}_{t+2|t} - \mu) = \phi_1(\hat{Y}_{t+1|t} - \mu) + \phi_2(Y_t - \mu) + \ldots + \phi_p(Y_{t-p+2} - \mu)$$ [^4.2.26]
Note que, enquanto a previs√£o de um per√≠odo √† frente utiliza diretamente os valores observados de Y, a previs√£o de dois per√≠odos √† frente utiliza a previs√£o de um per√≠odo √† frente como um de seus inputs. Isso √© crucial, pois demonstra como a lei das proje√ß√µes iteradas funciona de maneira recursiva, aproveitando previs√µes anteriores.
Em outras palavras, as previs√µes de dois per√≠odos √† frente, para um processo AR(p), reutilizam a mesma estrutura de pesos, mas aplicam-na sobre as previs√µes de um per√≠odo √† frente, e n√£o sobre os valores observados.

O conceito fundamental aqui √© que tanto a previs√£o de um per√≠odo quanto a de dois per√≠odos √† frente s√£o essencialmente combina√ß√µes lineares de valores (observados ou previstos), ponderadas pelos mesmos coeficientes do modelo AR(p). Essa consist√™ncia na aplica√ß√£o dos coeficientes garante que o modelo, no processo de previs√£o, se comporta de forma consistente e previs√≠vel ao longo do tempo, e  permite a extens√£o para m√∫ltiplos per√≠odos.

> üí° **Exemplo:** Considere um processo AR(2) com $\phi_1 = 0.5$, $\phi_2 = 0.3$ e $\mu = 5$.
>
>   **Passo 1: Previs√£o de um per√≠odo √† frente**
>   
>   Suponha que observamos $Y_t = 8$ e $Y_{t-1} = 7$. Ent√£o, a previs√£o de um per√≠odo √† frente √©:
>   $$\hat{Y}_{t+1|t} - 5 = 0.5(8 - 5) + 0.3(7 - 5)$$
>   $$\hat{Y}_{t+1|t} - 5 = 0.5(3) + 0.3(2) = 1.5 + 0.6 = 2.1$$
>   $$\hat{Y}_{t+1|t} = 7.1$$
>  
>   **Passo 2: Previs√£o de dois per√≠odos √† frente**
>   
>   Para obter a previs√£o de dois per√≠odos √† frente, utilizamos $\hat{Y}_{t+1|t} = 7.1$:
>   $$\hat{Y}_{t+2|t} - 5 = 0.5(\hat{Y}_{t+1|t} - 5) + 0.3(Y_t - 5)$$
>   $$\hat{Y}_{t+2|t} - 5 = 0.5(7.1 - 5) + 0.3(8 - 5)$$
>   $$\hat{Y}_{t+2|t} - 5 = 0.5(2.1) + 0.3(3) = 1.05 + 0.9 = 1.95$$
>   $$\hat{Y}_{t+2|t} = 6.95$$
>
>   Observe como a estrutura de coeficientes (0.5 e 0.3) foi usada em ambos os passos. Na previs√£o de um per√≠odo √† frente, eles s√£o aplicados sobre os valores observados, enquanto na previs√£o de dois per√≠odos, eles s√£o aplicados sobre a previs√£o de um per√≠odo e sobre valores observados.

A lei das proje√ß√µes iteradas garante que a previs√£o de dois per√≠odos √† frente, $\hat{Y}_{t+2|t}$, √© a melhor proje√ß√£o linear de $Y_{t+2}$ no espa√ßo de informa√ß√µes dispon√≠veis no tempo $t$, ou seja,  $Y_t, Y_{t-1}, \ldots$, [^SECTION_PLACEHOLDER].

**Lema 2:** A previs√£o de *j* per√≠odos √† frente, $\hat{Y}_{t+j|t}$, para um processo AR(p), pode ser escrita de forma geral como:
$$(\hat{Y}_{t+j|t} - \mu) = \phi_1(\hat{Y}_{t+j-1|t} - \mu) + \phi_2(\hat{Y}_{t+j-2|t} - \mu) + \ldots + \phi_p(\hat{Y}_{t+j-p|t} - \mu)$$
onde $\hat{Y}_{t+i|t} = Y_{t+i}$ para $i \leq 0$.

*Proof:* Esta equa√ß√£o √© uma generaliza√ß√£o da lei das proje√ß√µes iteradas [^SECTION_PLACEHOLDER] para um horizonte de previs√£o *j*. Ela representa a proje√ß√£o de $Y_{t+j}$ no espa√ßo de informa√ß√£o no tempo *t* como uma combina√ß√£o linear das proje√ß√µes anteriores, garantindo que a previs√£o de m√∫ltiplos passos √† frente seja obtida de forma iterativa, usando o mesmo conjunto de par√¢metros do modelo AR(p), onde a cada itera√ß√£o utiliza-se as previs√µes dos instantes anteriores e n√£o os valores observados.
‚ñ†
> üí° **Exemplo Num√©rico:** Vamos considerar um processo AR(1) simples onde $Y_t = 0.7Y_{t-1} + \epsilon_t$, com m√©dia $\mu = 0$.  Suponha que temos $Y_{10}=5$, e que queremos fazer uma previs√£o para 3 per√≠odos √† frente.
>
>   **Passo 1: Previs√£o de um per√≠odo √† frente (t+1)**
>   $$\hat{Y}_{11|10} = 0.7 Y_{10} = 0.7 \times 5 = 3.5$$
>
>   **Passo 2: Previs√£o de dois per√≠odos √† frente (t+2)**
>   $$\hat{Y}_{12|10} = 0.7 \hat{Y}_{11|10} = 0.7 \times 3.5 = 2.45$$
>
>   **Passo 3: Previs√£o de tr√™s per√≠odos √† frente (t+3)**
>  $$\hat{Y}_{13|10} = 0.7 \hat{Y}_{12|10} = 0.7 \times 2.45 = 1.715$$
>
>   Este exemplo num√©rico ilustra como a previs√£o de m√∫ltiplos per√≠odos se constr√≥i iterativamente, usando o resultado da previs√£o anterior. Note como a previs√£o vai se aproximando da m√©dia do processo, que nesse caso √© 0, √† medida que o horizonte temporal se alonga. Isso √© uma caracter√≠stica comum em processos AR est√°veis.

O lema 2 mostra a consist√™ncia da aplica√ß√£o dos coeficientes autorregressivos para diferentes horizontes de tempo. Ou seja, a cada passo de previs√£o √† frente, a estrutura do modelo AR(p) se mant√©m a mesma, com a √∫nica diferen√ßa sendo a aplica√ß√£o dos coeficientes sobre as previs√µes de instantes anteriores e n√£o sobre os valores observados.

**Corol√°rio 2:** As equa√ß√µes de previs√£o de um e dois per√≠odos √† frente para um modelo AR(p) s√£o casos particulares da equa√ß√£o mais geral estabelecida no Lema 2, onde no caso de um per√≠odo √† frente, $\hat{Y}_{t+1|t}$ utiliza diretamente os valores observados, e no caso de dois per√≠odos √† frente, $\hat{Y}_{t+2|t}$ utiliza a previs√£o de um per√≠odo √† frente ($\hat{Y}_{t+1|t}$), al√©m de valores observados defasados.

*Proof:*
I. Para $j=1$ (previs√£o de um per√≠odo √† frente):
   $$(\hat{Y}_{t+1|t} - \mu) = \phi_1(\hat{Y}_{t|t} - \mu) + \phi_2(\hat{Y}_{t-1|t} - \mu) + \ldots + \phi_p(\hat{Y}_{t-p+1|t} - \mu)$$
   Como $\hat{Y}_{t|t} = Y_t$, $\hat{Y}_{t-1|t} = Y_{t-1}$, ..., $\hat{Y}_{t-p+1|t} = Y_{t-p+1}$, recuperamos a equa√ß√£o da previs√£o de um per√≠odo √† frente:
   $$(\hat{Y}_{t+1|t} - \mu) = \phi_1(Y_t - \mu) + \phi_2(Y_{t-1} - \mu) + \ldots + \phi_p(Y_{t-p+1} - \mu)$$

II. Para $j=2$ (previs√£o de dois per√≠odos √† frente):
   $$(\hat{Y}_{t+2|t} - \mu) = \phi_1(\hat{Y}_{t+1|t} - \mu) + \phi_2(\hat{Y}_{t|t} - \mu) + \ldots + \phi_p(\hat{Y}_{t-p+2|t} - \mu)$$
   Substituindo $\hat{Y}_{t|t} = Y_t$, $\hat{Y}_{t-1|t} = Y_{t-1}$, ..., $\hat{Y}_{t-p+2|t} = Y_{t-p+2}$, obtemos a equa√ß√£o da previs√£o de dois per√≠odos √† frente:
   $$(\hat{Y}_{t+2|t} - \mu) = \phi_1(\hat{Y}_{t+1|t} - \mu) + \phi_2(Y_t - \mu) + \ldots + \phi_p(Y_{t-p+2} - \mu)$$
‚ñ†
**Proposi√ß√£o 1:** A previs√£o de *j* per√≠odos √† frente, $\hat{Y}_{t+j|t}$, para um processo AR(p), pode ser expressa como uma fun√ß√£o linear dos valores observados at√© o tempo *t*, $Y_t, Y_{t-1}, Y_{t-2},...$.

*Proof*:
O Lema 2 estabelece que  $$(\hat{Y}_{t+j|t} - \mu) = \phi_1(\hat{Y}_{t+j-1|t} - \mu) + \phi_2(\hat{Y}_{t+j-2|t} - \mu) + \ldots + \phi_p(\hat{Y}_{t+j-p|t} - \mu)$$.
Note that, for $i \leq 0$, $\hat{Y}_{t+i|t}=Y_{t+i}$ are the actual observed values.
For $i >0$ we can iteratively substitute the predicted values. Specifically, we can write,
$$ \hat{Y}_{t+1|t} = \mu + \phi_1 (Y_t - \mu) + \phi_2(Y_{t-1} - \mu) + \ldots + \phi_p (Y_{t-p+1} - \mu) $$
and then
$$ \hat{Y}_{t+2|t} = \mu + \phi_1 (\hat{Y}_{t+1|t} - \mu) + \phi_2(Y_t - \mu) + \ldots + \phi_p (Y_{t-p+2} - \mu) $$
Substituting the expression for $\hat{Y}_{t+1|t}$ into the expression for $\hat{Y}_{t+2|t}$ we obtain an expression for $\hat{Y}_{t+2|t}$ that depends only on the observed values $Y_t, Y_{t-1},...$ and the model parameters. We can continue this substitution process iteratively, resulting in an expression of $\hat{Y}_{t+j|t}$ as a function of the parameters of the AR(p) process and the observed values $Y_t, Y_{t-1}, Y_{t-2},...$. This shows that the prediction is a linear function of these observed values.
‚ñ†
This proposition highlights that despite the iterative nature of multi-period forecasting, the final prediction can always be expressed in terms of the observed data. This can be useful for understanding the overall impact of past observations on future predictions.

> üí° **Exemplo Num√©rico:** Para ilustrar a Proposi√ß√£o 1, vamos usar o exemplo anterior do processo AR(1) com $Y_t = 0.7Y_{t-1} + \epsilon_t$ e $\mu = 0$.  Vamos analisar a previs√£o de 2 per√≠odos √† frente. Sabemos que $\hat{Y}_{t+1|t} = 0.7 Y_t$. A previs√£o de dois per√≠odos √† frente √© $\hat{Y}_{t+2|t} = 0.7 \hat{Y}_{t+1|t}$. Substituindo $\hat{Y}_{t+1|t}$ em $\hat{Y}_{t+2|t}$:
> $$\hat{Y}_{t+2|t} = 0.7 (0.7 Y_t) = 0.49 Y_t$$
>
>  Desta forma,  $\hat{Y}_{t+2|t}$ √© expressa como uma fun√ß√£o linear dos valores observados no instante *t*, $Y_t$. Se tivessemos que fazer a previs√£o para 3 periodos, teriamos:
>   $$\hat{Y}_{t+3|t} = 0.7 \hat{Y}_{t+2|t} = 0.7 (0.49 Y_t) = 0.343 Y_t$$
>  
>  Este exemplo num√©rico ilustra como podemos expressar previs√µes de m√∫ltiplos per√≠odos √† frente em termos dos valores observados no tempo *t* e dos par√¢metros do modelo, atrav√©s de substitui√ß√µes iterativas.

### Conclus√£o
Este cap√≠tulo explorou em detalhe como a lei das proje√ß√µes iteradas √© aplicada em modelos AR(p) para derivar as previs√µes de um e dois per√≠odos √† frente, demonstrando a consist√™ncia e coer√™ncia da estrutura utilizada. Foi demonstrado como a previs√£o de dois per√≠odos √† frente reutiliza os coeficientes autorregressivos, aplicados de forma iterativa, sobre as previs√µes de um per√≠odo, bem como sobre valores defasados. Essa abordagem garante que a previs√£o de m√∫ltiplos per√≠odos √† frente seja consistente com as propriedades do modelo AR(p), utilizando a informa√ß√£o dispon√≠vel no momento de cada previs√£o,  e estabelece as bases para a extens√£o para outros modelos mais gerais de s√©ries temporais. O desenvolvimento formal da lei das proje√ß√µes iteradas garante a coer√™ncia te√≥rica, enquanto os exemplos num√©ricos ilustram sua aplica√ß√£o pr√°tica.

### Refer√™ncias
[^SECTION_PLACEHOLDER]: *T√≥pico anterior do texto base*
[^4.2.24]:  *Se√ß√£o 4.2 do texto base*
[^4.2.26]:  *Se√ß√£o 4.2 do texto base*
## 5.2. Likelihood Function for an AR(1) Model
### 5.2.1. Conditional Likelihood
We will begin with the simplest possible ARMA specification, an AR(1) model, where
$$Y_t = c + \phi Y_{t-1} + \epsilon_t$$. [^5.1.1]
Assuming that the errors $\epsilon_t$ are *i.i.d.* $N(0, \sigma^2)$, as in [^5.1.5], and that $Y_0$ is given, the joint density of the sample $(Y_1, Y_2, ..., Y_T)$ conditional on $Y_0$ can be written as
$$ f(y_1, y_2, ..., y_T | Y_0; c, \phi, \sigma^2) = \prod_{t=1}^T f(\epsilon_t) = \prod_{t=1}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{\epsilon_t^2}{2\sigma^2} \right), $$
where $\epsilon_t = Y_t - c - \phi Y_{t-1}$. This is the conditional likelihood function, conditional on the value of $Y_0$.
The log of this likelihood function, the conditional log-likelihood function, is
$$ \mathcal{L}(c, \phi, \sigma^2 | Y_0) = - \frac{T}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=1}^T (Y_t - c - \phi Y_{t-1})^2. $$
The parameters that maximize the likelihood can also be found by maximizing the log-likelihood. In this case, we will often simply refer to the log-likelihood, without the "conditional" qualification.
> üí° **Exemplo Num√©rico:** Considere um conjunto de dados simulados de um modelo AR(1) com $c = 2$, $\phi = 0.7$, $\sigma^2 = 1$, e um tamanho de amostra T=100. Os valores observados de $Y_t$ s√£o gerados de acordo com a equa√ß√£o do modelo. O objetivo √© estimar os par√¢metros usando a fun√ß√£o de log-verossimilhan√ßa condicional.  Abaixo, uma implementa√ß√£o em Python para ilustrar o conceito:
>
> ```python
> import numpy as np
> import scipy.optimize as optimize
>
> # Simula√ß√£o dos dados
> np.random.seed(42)
> T = 100
> c_true = 2
> phi_true = 0.7
> sigma2_true = 1
> errors = np.random.normal(0, np.sqrt(sigma2_true), T)
> Y = np.zeros(T)
> Y[0] = np.random.normal(0, np.sqrt(sigma2_true/(1-phi_true**2))) # Inicializa√ß√£o para estabilidade
> for t in range(1, T):
>    Y[t] = c_true + phi_true * Y[t-1] + errors[t]
>
> # Fun√ß√£o de log-verossimilhan√ßa condicional
> def log_likelihood_ar1(params, Y):
>    c, phi, sigma2 = params
>    T = len(Y)
>    epsilon = Y[1:] - c - phi * Y[:-1]
>    loglik = -T/2 * np.log(2*np.pi*sigma2) - 1/(2*sigma2) * np.sum(epsilon**2)
>    return -loglik # Retorna o negativo para otimiza√ß√£o (minimiza√ß√£o)
>
> # Otimiza√ß√£o
> initial_params = [0, 0, 0.5]
> result = optimize.minimize(log_likelihood_ar1, initial_params, args=(Y,), method='BFGS')
> estimated_c, estimated_phi, estimated_sigma2 = result.x
>
> print(f"True c: {c_true}, Estimated c: {estimated_c:.3f}")
> print(f"True phi: {phi_true}, Estimated phi: {estimated_phi:.3f}")
> print(f"True sigma2: {sigma2_true}, Estimated sigma2: {estimated_sigma2:.3f}")
> ```
>
>  Este c√≥digo simula dados de um processo AR(1), define a fun√ß√£o de log-verossimilhan√ßa e usa o m√©todo BFGS para otimizar e encontrar os par√¢metros que maximizam a verossimilhan√ßa.  A sa√≠da desse c√≥digo apresentaria os valores dos par√¢metros verdadeiros utilizados na simula√ß√£o e os valores estimados, permitindo a avalia√ß√£o da efici√™ncia da estima√ß√£o por m√°xima verossimilhan√ßa. √â importante notar que os valores estimados variam para cada simula√ß√£o devido a aleatoriedade presente no processo.

### 5.2.2. Concentrated Likelihood
The log-likelihood function can be maximized with respect to $\sigma^2$ analytically. Differentiating $\mathcal{L}$ with respect to $\sigma^2$ and setting it to zero gives
$$ \frac{\partial \mathcal{L}}{\partial \sigma^2} = - \frac{T}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{t=1}^T (Y_t - c - \phi Y_{t-1})^2 = 0. $$
Solving for $\sigma^2$, we obtain
$$ \hat{\sigma}^2 = \frac{1}{T} \sum_{t=1}^T (Y_t - c - \phi Y_{t-1})^2. $$
Substituting this back into the log-likelihood, we obtain the *concentrated log-likelihood*, which depends only on $c$ and $\phi$:
$$ \mathcal{L}_c(c, \phi) = -\frac{T}{2} \log \left( 2\pi \frac{1}{T} \sum_{t=1}^T (Y_t - c - \phi Y_{t-1})^2 \right) - \frac{T}{2} = -\frac{T}{2} \log(2\pi) - \frac{T}{2} \log \left( \frac{1}{T} \sum_{t=1}^T (Y_t - c - \phi Y_{t-1})^2 \right) - \frac{T}{2}. $$
Maximizing the concentrated log-likelihood is equivalent to minimizing the sum of squared errors, the term inside the logarithm. Numerical algorithms are then used to find the values of $c$ and $\phi$ that maximize this concentrated likelihood, or equivalently, minimize the sum of squared errors.

> üí° **Exemplo Num√©rico:** Utilizando os dados simulados do exemplo anterior, podemos agora calcular a fun√ß√£o de log-verossimilhan√ßa concentrada.  O c√≥digo abaixo mostra a implementa√ß√£o:
>
> ```python
> import numpy as np
> import scipy.optimize as optimize
>
> # Simula√ß√£o dos dados (mesmo c√≥digo do exemplo anterior)
> np.random.seed(42)
> T = 100
> c_true = 2
> phi_true = 0.7
> sigma2_true = 1
> errors = np.random.normal(0, np.sqrt(sigma2_true), T)
> Y = np.zeros(T)
> Y[0] = np.random.normal(0, np.sqrt(sigma2_true/(1-phi_true**2)))
> for t in range(1, T):
>    Y[t] = c_true + phi_true * Y[t-1] + errors[t]
>
> # Fun√ß√£o de log-verossimilhan√ßa concentrada
> def concentrated_log_likelihood_ar1(params, Y):
>    c, phi = params
>    T = len(Y)
>    sigma2_hat = 1/T * np.sum((Y[1:] - c - phi * Y[:-1])**2)
>    loglik = -T/2 * np.log(2*np.pi*sigma2_hat) - T/2
>    return -loglik # Retorna o negativo para otimiza√ß√£o (minimiza√ß√£o)
>
> # Otimiza√ß√£o
> initial_params = [0, 0]
> result = optimize.minimize(concentrated_log_likelihood_ar1, initial_params, args=(Y,), method='BFGS')
> estimated_c, estimated_phi = result.x
>
> # Estimativa de sigma2
> estimated_sigma2 = 1/T * np.sum((Y[1:] - estimated_c - estimated_phi * Y[:-1])**2)
>
> print(f"True c: {c_true}, Estimated c: {estimated_c:.3f}")
> print(f"True phi: {phi_true}, Estimated phi: {estimated_phi:.3f}")
> print(f"Estimated sigma2: {estimated_sigma2:.3f}")
> ```
>
> Este c√≥digo calcula a fun√ß√£o de verossimilhan√ßa concentrada e a usa para estimar os par√¢metros *c* e $\phi$. O par√¢metro $\sigma^2$ √© estimado posteriormente, ap√≥s a otimiza√ß√£o da fun√ß√£o de verossimilhan√ßa concentrada. Os resultados mostram que a estima√ß√£o via verossimilhan√ßa concentrada leva a resultados similares aos obtidos via verossimilhan√ßa condicional. A vantagem deste m√©todo √© que ele reduz o n√∫mero de par√¢metros a serem otimizados, o que pode acelerar o processo de otimiza√ß√£o.

## 5.3. Likelihood Function for an MA(1) Model
Now consider the MA(1) model
$$ Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1} $$ [^5.1.1]
with $\epsilon_t$ being *i.i.d.* $N(0, \sigma^2)$ as in [^5.1.5].
Unlike the AR(1) model, where the error term $\epsilon_t$ is explicitly defined in terms of current and lagged values of $Y_t$, the current error $\epsilon_t$ in an MA(1) depends implicitly on current and lagged $Y_t$ values. In fact, to express $\epsilon_t$ in terms of $Y_t$ requires an infinite number of past $Y_t$ values,
$$ \epsilon_t = (1 + \theta L)^{-1} (Y_t - \mu) = (Y_t - \mu) - \theta (Y_{t-1} - \mu) + \theta^2 (Y_{t-2} - \mu) - \ldots $$
Thus, a key problem in calculating the likelihood function is how to get started. We have to confront the fact that the series $\{Y_t\}$ does not have a convenient initial value, $Y_0$, like the AR(1) model. In the case of the AR(1) model we simply treated $Y_0$ as given, and calculated a conditional likelihood function.
### 5.3.1. Approximate Likelihood
One strategy is to set the presample errors, $\{\epsilon_0, \epsilon_{-1}, ...\}$, to their expected value of zero as suggested in the approximations discussed in [^4.3]. Thus, we can approximate the current error $\epsilon_t$ by
$$ \tilde{\epsilon}_t = (Y_t - \mu) - \theta \tilde{\epsilon}_{t-1}, $$
with $\tilde{\epsilon}_0 = 0$. Then, we can write an approximate likelihood function as
$$ f(y_1, y_2, ..., y_T ; \mu, \theta, \sigma^2) \approx \prod_{t=1}^T \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{\tilde{\epsilon}_t^2}{2\sigma^2} \right). $$
Taking the log, we obtain an approximate log-likelihood function that can then be concentrated with respect to $\sigma^2$ and then maximized numerically with respect to $\mu$ and $\theta$. While this method is often used, we can do better by recognizing how the errors at the beginning of the sample contribute to the joint density.
> üí° **Exemplo Num√©rico:** Vamos simular dados de um processo MA(1) com $\mu = 1$, $\theta = 0.5$, e $\sigma^2 = 0.25$, com tamanho de amostra $T=100$. O objetivo √© ilustrar a constru√ß√£o da fun√ß√£o de log-verossimilhan√ßa aproximada e a otimiza√ß√£o num√©rica dos par√¢metros.
>
>  ```python
> import numpy as np
> import scipy.optimize as optimize
>
> # Simula√ß√£o dos dados
> np.random.seed(42)
> T = 100
> mu_true = 1
> theta_true = 0.5
> sigma2_true = 0.25
> errors = np.random.normal(0, np.sqrt(sigma2_true), T+100) # Amostra um pouco maior para remover impacto de valores iniciais
> Y = np.zeros(T)
> epsilon_hat = np.zeros(T+100)
> for t in range(1, T+100):
>   epsilon_hat[t] = errors[t] + theta_true*epsilon_hat[t-1]
> for t in range(0,T):
>   Y[t] = mu_true + epsilon_hat[t+100]
> Y=Y
>
> # Fun√ß√£o de log-verossimilhan√ßa aproximada
> def approximate_log_likelihood_ma1(params, Y):
>    mu, theta, sigma2 = params
>    T = len(Y)
>    epsilon_hat = np.zeros(T)
>    for t in range(1, T):
>        epsilon_hat[t] = (Y[t] - mu) - theta * epsilon_hat[t-1]
>    loglik = -T/2 * np.log(2*np.pi*sigma2) - 1/(2*sigma2) * np.sum(epsilon_hat[1:]**2)
>    return -loglik
>
> # Otimiza√ß√£o
> initial_params = [0, 0, 0.1]
> result = optimize.minimize(approximate_log_likelihood_ma1, initial_params, args=(Y,), method='BFGS')
> estimated_mu, estimated_theta, estimated_sigma2 = result.x
>
> print(f"True mu: {mu_true}, Estimated mu: {estimated_mu:.3f}")
> print(f"True theta: {theta_true}, Estimated theta: {estimated_theta:.3f}")
> print(f"True sigma2: {sigma2_true}, Estimated sigma2: {estimated_sigma2:.3f}")
> ```
>
>  Este c√≥digo simula dados de um processo MA(1) e calcula a log-verossimilhan√ßa aproximada, otimizando-a para encontrar os par√¢metros que a maximizam. A diferen√ßa entre a log-verossimilhan√ßa aproximada e a log-verossimilhan√ßa exata reside na forma como tratamos os erros iniciais do processo. Os resultados obtidos s√£o pr√≥ximos aos par√¢metros verdadeiros utilizados na simula√ß√£o.

### 5.3.2. Exact Likelihood
To construct an exact likelihood function for the MA(1) model, we must recognize that the series $\{Y_t\}$ is, in fact, jointly Gaussian. This means that its joint density is characterized by the mean vector and the variance-covariance matrix. This also means that we can use the results from Section 4.6.
The mean vector of the observations is given by
$$ E(Y) = E \begin{bmatrix} Y_1 \\ Y_2 \\ \vdots \\ Y_T \end{bmatrix} = \mu \mathbf{1}, $$
where $\mathbf{1}$ is a $T \times 1$ vector of ones.
The variance-covariance matrix $\Omega$ is
$$ \Omega = E((Y - \mu\mathbf{1})(Y-\mu\mathbf{1})') $$
where the $(i,j)$th element of the matrix is given by
$$ E[(Y_i-\mu)(Y_j-\mu)] = \begin{cases} (1+\theta^2)\sigma^2 & \text{if } i = j \\ \theta\sigma^2 & \text{if } |i-j| = 1 \\ 0 & \text{otherwise}. \end{cases} $$
We can then write the exact log-likelihood for the MA(1) model as
$$ \mathcal{L}(\mu, \theta, \sigma^2) = -\frac{T}{2} \log(2\pi) - \frac{1}{2} \log |\Omega| - \frac{1}{2} (Y - \mu \mathbf{1})'\Omega^{-1}(Y - \mu \mathbf{1}). $$
The term $|\Omega|$ is the determinant of the variance-covariance matrix, and the term $\Omega^{-1}$ is the inverse of this matrix. Computing these terms directly can be extremely demanding, especially when $T$ is large. However, we can utilize the Cholesky factorization techniques described in sections 4.4 and 4.5 to simplify our task.

> üí° **Exemplo Num√©rico:** Usando os mesmos dados simulados do exemplo anterior, vamos calcular a fun√ß√£o de log-verossimilhan√ßa exata. Para isso, precisamos construir a matriz de vari√¢ncia-covari√¢ncia e utilizar o conceito de fatora√ß√£o de Cholesky para calcular o determinante e a inversa da matriz.
>
> ```python
> import numpy as np
> import scipy.optimize as optimize
> from scipy.linalg import cholesky, solve_triangular
>
> # Simula√ß√£o dos dados (mesmo c√≥digo do exemplo anterior)
> np.random.seed(42)
> T = 100
> mu_true = 1
> theta_true = 0.5
> sigma2_true = 0.25
> errors = np.random.normal(0, np.sqrt(sigma2_true), T+100) # Amostra um pouco maior para remover impacto de valores iniciais
> Y = np.zeros(T)
> epsilon_hat = np.zeros(T+100)
> for t in range(1, T+100):
>   epsilon_hat[t] = errors[t] + theta_true*epsilon_hat[t-1]
> for t in range(0,T):
>   Y[t] = mu_true + epsilon_hat[t+100]
>
> # Fun√ß√£o para construir a matriz de covari√¢ncia
> def covariance_matrix_ma1(theta, sigma2, T):
>    Omega = np.zeros((T, T))
>    for i in range```python
        for j in range(T):
            if i == j:
                Omega[i, j] = sigma2 * (1 + theta**2)
            elif abs(i - j) == 1:
                Omega[i, j] = sigma2 * theta
    return Omega

# Calculando a matriz de covari√¢ncia para o modelo MA(1)
Omega_hat = covariance_matrix_ma1(theta_hat, sigma2_hat, T)

# Gerando os valores de y_hat
y_hat = np.zeros(T)
for t in range(T):
    y_hat[t] = mu_hat + np.dot(Omega_hat[t,:], np.linalg.solve(Omega_hat, (Y - mu_hat * np.ones(T))))

# Plotando os valores simulados de Y e os valores preditos
plt.figure(figsize=(12, 6))
plt.plot(range(T), Y, label='Y')
plt.plot(range(T), y_hat, label='y_hat', linestyle='--')
plt.xlabel('Time')
plt.ylabel('Value')
plt.title('Simulated Y vs Predicted y_hat')
plt.legend()
plt.show()

# Imprimindo os par√¢metros estimados
print(f"mu_hat = {mu_hat}")
print(f"theta_hat = {theta_hat}")
print(f"sigma2_hat = {sigma2_hat}")
```

### An√°lise dos Resultados

O c√≥digo acima realiza uma simula√ß√£o de um modelo MA(1) e estima seus par√¢metros utilizando o m√©todo de m√°xima verossimilhan√ßa. Em seguida, ele utiliza a matriz de covari√¢ncia estimada para realizar previs√µes dos valores de *Y*. Os resultados s√£o ent√£o visualizados em um gr√°fico comparando os valores simulados de *Y* com as previs√µes *y_hat*. Os par√¢metros estimados (mu_hat, theta_hat e sigma2_hat) s√£o impressos para an√°lise.

√â importante notar que a qualidade das previs√µes *y_hat* depende da qualidade da estima√ß√£o dos par√¢metros do modelo MA(1). Uma boa estima√ß√£o deve levar a previs√µes que se aproximam dos valores reais de *Y*.

O gr√°fico gerado permite uma avalia√ß√£o visual r√°pida da performance do modelo. Espera-se que as previs√µes sigam a tend√™ncia dos valores simulados, com as diferen√ßas sendo atribu√≠das ao ru√≠do aleat√≥rio modelado pelo modelo MA(1).

<!-- END -->
