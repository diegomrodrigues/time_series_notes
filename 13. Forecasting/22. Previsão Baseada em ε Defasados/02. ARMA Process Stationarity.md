## Previsão Baseada em Modelos ARMA(p,q) Utilizando Representação AR(∞)

### Introdução
Em continuidade à discussão sobre a representação AR(∞) de processos temporais [^7], exploraremos neste capítulo como um processo ARMA(p,q) também se enquadra nessa categoria quando suas partes autorregressiva (AR) e média móvel (MA) satisfazem certas condições [^7]. Especificamente, demonstraremos que, sob as condições de estacionariedade para o componente AR e invertibilidade para o componente MA, um modelo ARMA(p,q) pode ser reescrito em termos de um modelo AR(∞) [^7]. Este capítulo aprofunda o entendimento sobre a relação entre essas representações e consolida a base para previsões em cenários práticos.

### ARMA(p,q) como um caso especial de AR(∞)
Conforme mencionado no capítulo anterior [^7], um modelo ARMA(p,q) é definido como:
$$
(1 - \phi_1 L - \phi_2 L^2 - \ldots - \phi_p L^p) (Y_t - \mu) = (1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q)\epsilon_t
$$
que pode ser reescrito de forma compacta como:
$$
\phi(L)(Y_t - \mu) = \theta(L)\epsilon_t
$$
onde $\phi(L) = (1 - \phi_1 L - \phi_2 L^2 - \ldots - \phi_p L^p)$ e $\theta(L) = (1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q)$ [^7]. Para que um modelo ARMA(p,q) possa ser expressado em termos de um modelo AR(∞), precisamos garantir que o operador autoregressivo $\phi(L)$ satisfaça a condição de estacionariedade (i.e., as raízes do polinômio $\phi(z) = 0$ estejam fora do círculo unitário) e que o operador da média móvel $\theta(L)$ satisfaça a condição de invertibilidade (i.e., as raízes do polinômio $\theta(z) = 0$ também estejam fora do círculo unitário) [^7].

Sob essas condições, podemos expressar o modelo ARMA(p,q) em termos de um modelo AR(∞) da seguinte forma:
$$
(Y_t - \mu) = \frac{\theta(L)}{\phi(L)} \epsilon_t
$$
Definindo $\eta(L) = [\frac{\theta(L)}{\phi(L)}]^{-1} = \frac{\phi(L)}{\theta(L)}$, a equação acima torna-se:
$$
\eta(L)(Y_t - \mu) = \epsilon_t
$$
que é a forma do modelo AR(∞) apresentado anteriormente [^7].
Onde $\eta(L)$ é dado por:
$$
\eta(L) = \sum_{j=0}^{\infty} \eta_j L^j
$$
Em termos práticos, $\eta(L)$ pode ser obtido através da divisão polinomial longa entre $\phi(L)$ e $\theta(L)$.

**Lema 2:**
Se um processo ARMA(p, q) é definido por $\phi(L)(Y_t - \mu) = \theta(L)\epsilon_t$ e os polinômios $\phi(L)$ e $\theta(L)$ satisfazem as condições de estacionariedade e invertibilidade, respectivamente, então o processo pode ser escrito na forma AR(∞):
$$
\eta(L)(Y_t-\mu)=\epsilon_t,
$$
onde $\eta(L) = \frac{\phi(L)}{\theta(L)}$.
*Prova:*
I. Partimos da definição do modelo ARMA(p,q):
$$
\phi(L)(Y_t-\mu) = \theta(L)\epsilon_t
$$
II. Dividindo ambos os lados por $\theta(L)$ (que é invertível por hipótese) temos:
$$
\frac{\phi(L)}{\theta(L)}(Y_t-\mu) = \epsilon_t
$$
III. Definindo $\eta(L) = \frac{\phi(L)}{\theta(L)}$, obtemos:
$$
\eta(L)(Y_t-\mu) = \epsilon_t
$$
Essa última equação é a definição de um modelo AR(∞).
■

#### Expansão em Série de $\eta(L)$
Para realizar previsões usando a representação AR(∞), é útil ter uma forma explícita para o operador $\eta(L)$ [^7]. Isso pode ser feito através da expansão em série de $\eta(L) = \frac{\phi(L)}{\theta(L)}$. Existem algumas técnicas para calcular essa expansão, incluindo a divisão polinomial longa, que pode ser laboriosa, ou usando a expansão geométrica para os operadores $\theta(L)$ e $\phi(L)$. No caso mais geral, precisamos obter os coeficientes $\eta_j$ da seguinte forma:
$$
\eta(L) = \frac{\phi(L)}{\theta(L)} = \sum_{j=0}^{\infty} \eta_j L^j
$$
Na prática, essa expansão em série é truncada em algum ponto para fins computacionais.

**Lema 2.1**
A expansão em série de $\eta(L)$ pode ser escrita como:
$$\eta(L) = \frac{\phi(L)}{\theta(L)} = \frac{1 - \sum_{i=1}^{p} \phi_i L^i}{1 + \sum_{j=1}^{q} \theta_j L^j} = 1 + \sum_{k=1}^{\infty} \eta_k L^k$$
*Prova:*
I. A partir da definição de $\eta(L)$, temos:
$$\eta(L) = \frac{\phi(L)}{\theta(L)}$$
II. Substituindo os polinômios $\phi(L)$ e $\theta(L)$ pelas suas definições:
$$\eta(L) = \frac{1 - \sum_{i=1}^{p} \phi_i L^i}{1 + \sum_{j=1}^{q} \theta_j L^j}$$
III. Assumindo que a divisão polinomial resulta em uma série infinita, podemos escrever:
$$\eta(L) = 1 + \sum_{k=1}^{\infty} \eta_k L^k$$
IV. A igualdade entre as duas expressões para $\eta(L)$ mostra que podemos encontrar os coeficientes $\eta_k$ a partir dos coeficientes $\phi_i$ e $\theta_j$.
■

**Corolário 1**
Se $\eta(L) = \frac{\phi(L)}{\theta(L)}$, então o erro $\epsilon_t$ pode ser expresso em função dos valores defasados de $Y$ através da expansão em série do operador $\eta(L)$:
$$\epsilon_t = \eta(L)(Y_t-\mu) = (1 + \eta_1L + \eta_2L^2 + \ldots)(Y_t-\mu) $$
*Prova:*
I. Partimos da definição de $\eta(L)$:
$$
\eta(L) = \frac{\phi(L)}{\theta(L)}
$$
II. Da definição do modelo ARMA, temos:
$$
\frac{\phi(L)}{\theta(L)}(Y_t-\mu) = \epsilon_t
$$
III. Substituindo $\eta(L)$ na equação anterior, temos:
$$
\eta(L)(Y_t-\mu) = \epsilon_t
$$
IV. Expandindo a série $\eta(L)$ obtemos o resultado desejado:
$$
\epsilon_t = (1 + \eta_1L + \eta_2L^2 + \ldots)(Y_t-\mu)
$$
■
> 💡 **Exemplo Numérico:** Considere um modelo ARMA(1,1) dado por $(1 - 0.5L)(Y_t - \mu) = (1 + 0.2L)\epsilon_t$. Temos que $\phi(L) = 1 - 0.5L$ e $\theta(L) = 1 + 0.2L$. Queremos encontrar a representação AR(∞), então:
> $$\eta(L) = \frac{1 - 0.5L}{1 + 0.2L}$$
> Usando expansão geométrica temos $(1 + 0.2L)^{-1} = 1 - 0.2L + (0.2L)^2 - (0.2L)^3 + \ldots = 1 - 0.2L + 0.04L^2 - 0.008L^3 + \ldots$. Assim:
>  $$\begin{aligned}
>  \eta(L) &= (1 - 0.5L)(1 - 0.2L + 0.04L^2 - 0.008L^3 + \ldots) \\
>  &= 1 - 0.2L + 0.04L^2 - 0.008L^3 -0.5L + 0.1L^2 - 0.02L^3 + \ldots \\
>  &= 1 - 0.7L + 0.14L^2 - 0.028L^3 + \ldots
>  \end{aligned}$$
>  Portanto, o processo AR(∞) seria:
>  $$Y_t - \mu = 0.7(Y_{t-1} - \mu) - 0.14(Y_{t-2} - \mu) + 0.028(Y_{t-3} - \mu) + \ldots + \epsilon_t $$
>  E o erro $\epsilon_t$ seria expresso por:
>  $$ \epsilon_t = (Y_t - \mu) - 0.7(Y_{t-1} - \mu) + 0.14(Y_{t-2} - \mu) - 0.028(Y_{t-3} - \mu) + \ldots $$
>  Note que os coeficientes de $\eta(L)$ são os coeficientes da representação AR(∞)
>  .

### Implicações para a Previsão
A capacidade de reescrever um processo ARMA(p,q) como um processo AR(∞) tem implicações diretas para a previsão. Como vimos no capítulo anterior, podemos usar a fórmula de previsão de Wiener-Kolmogorov [^8]:
$$
\hat{Y}_{t+s|t} = \mu + \left[ \frac{\theta(L)}{L^s\phi(L)} \right]_+  \phi(L)(Y_t - \mu)
$$
Essa fórmula enfatiza que, em termos práticos, podemos utilizar a representação AR(∞) para derivar previsões, uma vez que a representação ARMA(p,q) pode ser expressa nesse formato [^7]. O uso da representação AR(∞) nos permite calcular os previsores usando os valores defasados da série temporal, sem precisar dos erros defasados, que em geral não são observáveis [^7].

**Teorema 2:**
Se um processo é descrito por um modelo ARMA(p, q) dado por  $\phi(L)(Y_t - \mu) = \theta(L)\epsilon_t$, onde  $\phi(L)$ e $\theta(L)$ satisfazem as condições de estacionariedade e invertibilidade, respectivamente, então o melhor preditor linear de  $Y_{t+s}$ dado  $Y_t, Y_{t-1}, Y_{t-2}, \ldots$ é dado por
$$
\hat{Y}_{t+s|t} = \mu + \left[ \frac{\theta(L)}{L^s \phi(L)} \right]_+  \phi(L)(Y_t - \mu)
$$
*Prova:*
I. Começamos com a representação do modelo ARMA(p,q):
$$
\phi(L)(Y_t - \mu) = \theta(L)\epsilon_t
$$
II.  Dividindo ambos os lados por $\phi(L)$ e multiplicando por $L^{-s}$:
$$
L^{-s}(Y_{t+s} - \mu) = L^{-s} \frac{\theta(L)}{\phi(L)} \epsilon_{t+s}
$$
III. Aplicando o operador de aniquilação $[ \cdot ]_+$ e sabendo que $\hat{\epsilon}_{t+j|t} = 0$ para $j>0$ obtemos:
$$
\hat{Y}_{t+s|t} - \mu = \left[ L^{-s} \frac{\theta(L)}{\phi(L)} \right]_+ \epsilon_{t+s}
$$
IV. Substituindo $\epsilon_t$ por $\frac{\phi(L)}{\theta(L)}(Y_t-\mu)$:
$$
\hat{Y}_{t+s|t} = \mu + \left[ L^{-s} \frac{\theta(L)}{\phi(L)} \right]_+ \frac{\phi(L)}{\theta(L)} (Y_t - \mu)
$$
V. Sabendo que   $\frac{\phi(L)}{\theta(L)}  = \frac{1}{\eta(L)}$, temos:
$$
\hat{Y}_{t+s|t} = \mu + \left[ \frac{\theta(L)}{L^s\phi(L)} \right]_+   \phi(L)(Y_t - \mu)
$$
■

**Teorema 2.1:**
A previsão de $Y_{t+s}$ pode ser expressa usando os coeficientes da expansão em série de $\eta(L)$ , definidos no Lema 2.1, e a representação AR($\infty$):
$$
\hat{Y}_{t+s|t} = \mu + \sum_{j=s}^{\infty} \eta_j(Y_{t+s-j} - \mu)
$$
*Prova:*
I. Da representação AR(∞) temos:
$$
\epsilon_t = \eta(L)(Y_t - \mu)
$$
II.  A partir do Teorema 2, sabemos que:
$$\hat{Y}_{t+s|t} = \mu + \left[ \frac{\theta(L)}{L^s\phi(L)} \right]_+ \phi(L)(Y_t-\mu)$$
III. Substituindo $\frac{\phi(L)}{\theta(L)}$ por $\eta(L)$:
$$
\hat{Y}_{t+s|t} = \mu + \left[ L^{-s} \frac{1}{\eta(L)} \right]_+ \epsilon_{t+s}
$$
IV. Usando a expansão em série de $\eta(L)$ e aplicando o operador $[\cdot]_+$:
$$
\hat{Y}_{t+s|t} = \mu + \left[ L^{-s} \frac{1}{1 + \sum_{k=1}^{\infty} \eta_k L^k} \right]_+ \epsilon_{t+s} = \mu + \sum_{j=s}^{\infty} \eta_j (Y_{t+s-j}-\mu)
$$
■

> 💡 **Exemplo Numérico:** Considere o modelo ARMA(1,1)  $(1 - 0.5L)(Y_t - \mu) = (1 + 0.2L)\epsilon_t$  e desejamos prever $Y_{t+1}$, então $s = 1$:
> $$
> \hat{Y}_{t+1|t} = \mu + \left[ \frac{1 + 0.2L}{L(1 - 0.5L)} \right]_+ (1 - 0.5L)(Y_t - \mu)
> $$
> Precisamos expandir a parte fracionária:
> $$\frac{1 + 0.2L}{L(1 - 0.5L)} = \frac{1 + 0.2L}{L} (1 + 0.5L + 0.25L^2 + \ldots) =  \left(\frac{1}{L} + 0.2\right)(1 + 0.5L + 0.25L^2 + 0.125L^3+\ldots) $$
>  $$ = \frac{1}{L} + 0.5 + 0.25L + 0.125L^2 + 0.2 + 0.1L + 0.05L^2 + \ldots =  \frac{1}{L} + 0.7 + 0.35L + 0.175L^2+\ldots $$
> Aplicando o operador de aniquilação $[ \cdot ]_+$:
>  $$ \left[ \frac{1 + 0.2L}{L(1 - 0.5L)} \right]_+ = 0.7 + 0.35L + 0.175L^2 + \ldots  $$
> Então, a previsão de um passo a frente é:
> $$
> \hat{Y}_{t+1|t} = \mu + (0.7 + 0.35L + 0.175L^2 + \ldots)(1 - 0.5L)(Y_t - \mu)
> $$
> Na prática, podemos truncar a série e usar os primeiros termos ou usar a representação ARMA para derivar previsões.  Aplicando a distributiva e truncando a série, teremos:
>  $$\begin{aligned}
>   \hat{Y}_{t+1|t} &\approx  \mu + (0.7 + 0.35L + 0.175L^2 + \ldots)(Y_t - \mu - 0.5(Y_{t-1}-\mu)) \\
>   &\approx  \mu + 0.7(Y_t - \mu) + 0.35(Y_{t-1}-\mu) - 0.5\times 0.7(Y_{t-1} - \mu) + \ldots \\
>   &\approx \mu + 0.7(Y_t - \mu) + 0(Y_{t-1}-\mu)+\ldots
>   \end{aligned}$$
>  Note que, usando o modelo ARMA(1,1) diretamente,
>  $$ Y_{t+1} - \mu = 0.5(Y_t - \mu) + \epsilon_{t+1} + 0.2\epsilon_t $$
>  A melhor estimativa de $\epsilon_{t+1}$ é zero, então:
> $$ \hat{Y}_{t+1|t} = \mu + 0.5(Y_t - \mu) + 0.2\epsilon_t  $$
>  e $\epsilon_t = (Y_t - \mu) - 0.5(Y_{t-1} - \mu) - 0.2\epsilon_{t-1} $. Usando essa representação, precisamos estimar $\epsilon_t$ recursivamente, o que pode ser computacionalmente mais complexo do que usar a representação AR($\infty$).
>   
>  Considerando a representação AR($\infty$) obtida no exemplo anterior, e truncando em 3 lags, podemos obter o mesmo resultado:
>  $$\hat{Y}_{t+1|t} \approx  \mu +  0.7(Y_t-\mu) - 0.14(Y_{t-1} - \mu) + 0.028(Y_{t-2} - \mu)$$
>  Note que os coeficientes do modelo AR($\infty$) determinam como os valores passados influenciam o valor futuro, e que os valores numéricos dos dois métodos não serão exatamente iguais, pois dependem de truncamentos e aproximações.

### Conclusão
Este capítulo demonstrou que um processo ARMA(p,q) que satisfaz as condições de estacionariedade e invertibilidade pode ser expresso como um processo AR(∞) [^7]. Essa equivalência é fundamental para construir previsões usando a fórmula de Wiener-Kolmogorov. A capacidade de alternar entre as representações ARMA e AR(∞) fornece flexibilidade na modelagem e previsão de séries temporais, além de destacar a relação entre diferentes formas de representar o mesmo processo. Ao entender como modelos ARMA(p,q) podem ser representados como modelos AR(∞), temos uma visão mais completa e robusta das ferramentas para previsão em séries temporais, usando os valores passados das séries temporais, como descrito em [^7].

### Referências
[^7]: Seção 4.2 - *Forecasts Based on Lagged Y's*
[^8]: Seção 4.2 - *Forecasting Based on Lagged e's*
<!-- END -->
