## Previs√£o Baseada em Modelos ARMA(p,q) Utilizando Representa√ß√£o AR(‚àû)

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre a representa√ß√£o AR(‚àû) de processos temporais [^7], exploraremos neste cap√≠tulo como um processo ARMA(p,q) tamb√©m se enquadra nessa categoria quando suas partes autorregressiva (AR) e m√©dia m√≥vel (MA) satisfazem certas condi√ß√µes [^7]. Especificamente, demonstraremos que, sob as condi√ß√µes de estacionariedade para o componente AR e invertibilidade para o componente MA, um modelo ARMA(p,q) pode ser reescrito em termos de um modelo AR(‚àû) [^7]. Este cap√≠tulo aprofunda o entendimento sobre a rela√ß√£o entre essas representa√ß√µes e consolida a base para previs√µes em cen√°rios pr√°ticos.

### ARMA(p,q) como um caso especial de AR(‚àû)
Conforme mencionado no cap√≠tulo anterior [^7], um modelo ARMA(p,q) √© definido como:
$$
(1 - \phi_1 L - \phi_2 L^2 - \ldots - \phi_p L^p) (Y_t - \mu) = (1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q)\epsilon_t
$$
que pode ser reescrito de forma compacta como:
$$
\phi(L)(Y_t - \mu) = \theta(L)\epsilon_t
$$
onde $\phi(L) = (1 - \phi_1 L - \phi_2 L^2 - \ldots - \phi_p L^p)$ e $\theta(L) = (1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q)$ [^7]. Para que um modelo ARMA(p,q) possa ser expressado em termos de um modelo AR(‚àû), precisamos garantir que o operador autoregressivo $\phi(L)$ satisfa√ßa a condi√ß√£o de estacionariedade (i.e., as ra√≠zes do polin√¥mio $\phi(z) = 0$ estejam fora do c√≠rculo unit√°rio) e que o operador da m√©dia m√≥vel $\theta(L)$ satisfa√ßa a condi√ß√£o de invertibilidade (i.e., as ra√≠zes do polin√¥mio $\theta(z) = 0$ tamb√©m estejam fora do c√≠rculo unit√°rio) [^7].

Sob essas condi√ß√µes, podemos expressar o modelo ARMA(p,q) em termos de um modelo AR(‚àû) da seguinte forma:
$$
(Y_t - \mu) = \frac{\theta(L)}{\phi(L)} \epsilon_t
$$
Definindo $\eta(L) = [\frac{\theta(L)}{\phi(L)}]^{-1} = \frac{\phi(L)}{\theta(L)}$, a equa√ß√£o acima torna-se:
$$
\eta(L)(Y_t - \mu) = \epsilon_t
$$
que √© a forma do modelo AR(‚àû) apresentado anteriormente [^7].
Onde $\eta(L)$ √© dado por:
$$
\eta(L) = \sum_{j=0}^{\infty} \eta_j L^j
$$
Em termos pr√°ticos, $\eta(L)$ pode ser obtido atrav√©s da divis√£o polinomial longa entre $\phi(L)$ e $\theta(L)$.

**Lema 2:**
Se um processo ARMA(p, q) √© definido por $\phi(L)(Y_t - \mu) = \theta(L)\epsilon_t$ e os polin√¥mios $\phi(L)$ e $\theta(L)$ satisfazem as condi√ß√µes de estacionariedade e invertibilidade, respectivamente, ent√£o o processo pode ser escrito na forma AR(‚àû):
$$
\eta(L)(Y_t-\mu)=\epsilon_t,
$$
onde $\eta(L) = \frac{\phi(L)}{\theta(L)}$.
*Prova:*
I. Partimos da defini√ß√£o do modelo ARMA(p,q):
$$
\phi(L)(Y_t-\mu) = \theta(L)\epsilon_t
$$
II. Dividindo ambos os lados por $\theta(L)$ (que √© invert√≠vel por hip√≥tese) temos:
$$
\frac{\phi(L)}{\theta(L)}(Y_t-\mu) = \epsilon_t
$$
III. Definindo $\eta(L) = \frac{\phi(L)}{\theta(L)}$, obtemos:
$$
\eta(L)(Y_t-\mu) = \epsilon_t
$$
Essa √∫ltima equa√ß√£o √© a defini√ß√£o de um modelo AR(‚àû).
‚ñ†

#### Expans√£o em S√©rie de $\eta(L)$
Para realizar previs√µes usando a representa√ß√£o AR(‚àû), √© √∫til ter uma forma expl√≠cita para o operador $\eta(L)$ [^7]. Isso pode ser feito atrav√©s da expans√£o em s√©rie de $\eta(L) = \frac{\phi(L)}{\theta(L)}$. Existem algumas t√©cnicas para calcular essa expans√£o, incluindo a divis√£o polinomial longa, que pode ser laboriosa, ou usando a expans√£o geom√©trica para os operadores $\theta(L)$ e $\phi(L)$. No caso mais geral, precisamos obter os coeficientes $\eta_j$ da seguinte forma:
$$
\eta(L) = \frac{\phi(L)}{\theta(L)} = \sum_{j=0}^{\infty} \eta_j L^j
$$
Na pr√°tica, essa expans√£o em s√©rie √© truncada em algum ponto para fins computacionais.

**Lema 2.1**
A expans√£o em s√©rie de $\eta(L)$ pode ser escrita como:
$$\eta(L) = \frac{\phi(L)}{\theta(L)} = \frac{1 - \sum_{i=1}^{p} \phi_i L^i}{1 + \sum_{j=1}^{q} \theta_j L^j} = 1 + \sum_{k=1}^{\infty} \eta_k L^k$$
*Prova:*
I. A partir da defini√ß√£o de $\eta(L)$, temos:
$$\eta(L) = \frac{\phi(L)}{\theta(L)}$$
II. Substituindo os polin√¥mios $\phi(L)$ e $\theta(L)$ pelas suas defini√ß√µes:
$$\eta(L) = \frac{1 - \sum_{i=1}^{p} \phi_i L^i}{1 + \sum_{j=1}^{q} \theta_j L^j}$$
III. Assumindo que a divis√£o polinomial resulta em uma s√©rie infinita, podemos escrever:
$$\eta(L) = 1 + \sum_{k=1}^{\infty} \eta_k L^k$$
IV. A igualdade entre as duas express√µes para $\eta(L)$ mostra que podemos encontrar os coeficientes $\eta_k$ a partir dos coeficientes $\phi_i$ e $\theta_j$.
‚ñ†

**Corol√°rio 1**
Se $\eta(L) = \frac{\phi(L)}{\theta(L)}$, ent√£o o erro $\epsilon_t$ pode ser expresso em fun√ß√£o dos valores defasados de $Y$ atrav√©s da expans√£o em s√©rie do operador $\eta(L)$:
$$\epsilon_t = \eta(L)(Y_t-\mu) = (1 + \eta_1L + \eta_2L^2 + \ldots)(Y_t-\mu) $$
*Prova:*
I. Partimos da defini√ß√£o de $\eta(L)$:
$$
\eta(L) = \frac{\phi(L)}{\theta(L)}
$$
II. Da defini√ß√£o do modelo ARMA, temos:
$$
\frac{\phi(L)}{\theta(L)}(Y_t-\mu) = \epsilon_t
$$
III. Substituindo $\eta(L)$ na equa√ß√£o anterior, temos:
$$
\eta(L)(Y_t-\mu) = \epsilon_t
$$
IV. Expandindo a s√©rie $\eta(L)$ obtemos o resultado desejado:
$$
\epsilon_t = (1 + \eta_1L + \eta_2L^2 + \ldots)(Y_t-\mu)
$$
‚ñ†
> üí° **Exemplo Num√©rico:** Considere um modelo ARMA(1,1) dado por $(1 - 0.5L)(Y_t - \mu) = (1 + 0.2L)\epsilon_t$. Temos que $\phi(L) = 1 - 0.5L$ e $\theta(L) = 1 + 0.2L$. Queremos encontrar a representa√ß√£o AR(‚àû), ent√£o:
> $$\eta(L) = \frac{1 - 0.5L}{1 + 0.2L}$$
> Usando expans√£o geom√©trica temos $(1 + 0.2L)^{-1} = 1 - 0.2L + (0.2L)^2 - (0.2L)^3 + \ldots = 1 - 0.2L + 0.04L^2 - 0.008L^3 + \ldots$. Assim:
>  $$\begin{aligned}
>  \eta(L) &= (1 - 0.5L)(1 - 0.2L + 0.04L^2 - 0.008L^3 + \ldots) \\
>  &= 1 - 0.2L + 0.04L^2 - 0.008L^3 -0.5L + 0.1L^2 - 0.02L^3 + \ldots \\
>  &= 1 - 0.7L + 0.14L^2 - 0.028L^3 + \ldots
>  \end{aligned}$$
>  Portanto, o processo AR(‚àû) seria:
>  $$Y_t - \mu = 0.7(Y_{t-1} - \mu) - 0.14(Y_{t-2} - \mu) + 0.028(Y_{t-3} - \mu) + \ldots + \epsilon_t $$
>  E o erro $\epsilon_t$ seria expresso por:
>  $$ \epsilon_t = (Y_t - \mu) - 0.7(Y_{t-1} - \mu) + 0.14(Y_{t-2} - \mu) - 0.028(Y_{t-3} - \mu) + \ldots $$
>  Note que os coeficientes de $\eta(L)$ s√£o os coeficientes da representa√ß√£o AR(‚àû)
>  .

### Implica√ß√µes para a Previs√£o
A capacidade de reescrever um processo ARMA(p,q) como um processo AR(‚àû) tem implica√ß√µes diretas para a previs√£o. Como vimos no cap√≠tulo anterior, podemos usar a f√≥rmula de previs√£o de Wiener-Kolmogorov [^8]:
$$
\hat{Y}_{t+s|t} = \mu + \left[ \frac{\theta(L)}{L^s\phi(L)} \right]_+  \phi(L)(Y_t - \mu)
$$
Essa f√≥rmula enfatiza que, em termos pr√°ticos, podemos utilizar a representa√ß√£o AR(‚àû) para derivar previs√µes, uma vez que a representa√ß√£o ARMA(p,q) pode ser expressa nesse formato [^7]. O uso da representa√ß√£o AR(‚àû) nos permite calcular os previsores usando os valores defasados da s√©rie temporal, sem precisar dos erros defasados, que em geral n√£o s√£o observ√°veis [^7].

**Teorema 2:**
Se um processo √© descrito por um modelo ARMA(p, q) dado por  $\phi(L)(Y_t - \mu) = \theta(L)\epsilon_t$, onde  $\phi(L)$ e $\theta(L)$ satisfazem as condi√ß√µes de estacionariedade e invertibilidade, respectivamente, ent√£o o melhor preditor linear de  $Y_{t+s}$ dado  $Y_t, Y_{t-1}, Y_{t-2}, \ldots$ √© dado por
$$
\hat{Y}_{t+s|t} = \mu + \left[ \frac{\theta(L)}{L^s \phi(L)} \right]_+  \phi(L)(Y_t - \mu)
$$
*Prova:*
I. Come√ßamos com a representa√ß√£o do modelo ARMA(p,q):
$$
\phi(L)(Y_t - \mu) = \theta(L)\epsilon_t
$$
II.  Dividindo ambos os lados por $\phi(L)$ e multiplicando por $L^{-s}$:
$$
L^{-s}(Y_{t+s} - \mu) = L^{-s} \frac{\theta(L)}{\phi(L)} \epsilon_{t+s}
$$
III. Aplicando o operador de aniquila√ß√£o $[ \cdot ]_+$ e sabendo que $\hat{\epsilon}_{t+j|t} = 0$ para $j>0$ obtemos:
$$
\hat{Y}_{t+s|t} - \mu = \left[ L^{-s} \frac{\theta(L)}{\phi(L)} \right]_+ \epsilon_{t+s}
$$
IV. Substituindo $\epsilon_t$ por $\frac{\phi(L)}{\theta(L)}(Y_t-\mu)$:
$$
\hat{Y}_{t+s|t} = \mu + \left[ L^{-s} \frac{\theta(L)}{\phi(L)} \right]_+ \frac{\phi(L)}{\theta(L)} (Y_t - \mu)
$$
V. Sabendo que   $\frac{\phi(L)}{\theta(L)}  = \frac{1}{\eta(L)}$, temos:
$$
\hat{Y}_{t+s|t} = \mu + \left[ \frac{\theta(L)}{L^s\phi(L)} \right]_+   \phi(L)(Y_t - \mu)
$$
‚ñ†

**Teorema 2.1:**
A previs√£o de $Y_{t+s}$ pode ser expressa usando os coeficientes da expans√£o em s√©rie de $\eta(L)$ , definidos no Lema 2.1, e a representa√ß√£o AR($\infty$):
$$
\hat{Y}_{t+s|t} = \mu + \sum_{j=s}^{\infty} \eta_j(Y_{t+s-j} - \mu)
$$
*Prova:*
I. Da representa√ß√£o AR(‚àû) temos:
$$
\epsilon_t = \eta(L)(Y_t - \mu)
$$
II.  A partir do Teorema 2, sabemos que:
$$\hat{Y}_{t+s|t} = \mu + \left[ \frac{\theta(L)}{L^s\phi(L)} \right]_+ \phi(L)(Y_t-\mu)$$
III. Substituindo $\frac{\phi(L)}{\theta(L)}$ por $\eta(L)$:
$$
\hat{Y}_{t+s|t} = \mu + \left[ L^{-s} \frac{1}{\eta(L)} \right]_+ \epsilon_{t+s}
$$
IV. Usando a expans√£o em s√©rie de $\eta(L)$ e aplicando o operador $[\cdot]_+$:
$$
\hat{Y}_{t+s|t} = \mu + \left[ L^{-s} \frac{1}{1 + \sum_{k=1}^{\infty} \eta_k L^k} \right]_+ \epsilon_{t+s} = \mu + \sum_{j=s}^{\infty} \eta_j (Y_{t+s-j}-\mu)
$$
‚ñ†

> üí° **Exemplo Num√©rico:** Considere o modelo ARMA(1,1)  $(1 - 0.5L)(Y_t - \mu) = (1 + 0.2L)\epsilon_t$  e desejamos prever $Y_{t+1}$, ent√£o $s = 1$:
> $$
> \hat{Y}_{t+1|t} = \mu + \left[ \frac{1 + 0.2L}{L(1 - 0.5L)} \right]_+ (1 - 0.5L)(Y_t - \mu)
> $$
> Precisamos expandir a parte fracion√°ria:
> $$\frac{1 + 0.2L}{L(1 - 0.5L)} = \frac{1 + 0.2L}{L} (1 + 0.5L + 0.25L^2 + \ldots) =  \left(\frac{1}{L} + 0.2\right)(1 + 0.5L + 0.25L^2 + 0.125L^3+\ldots) $$
>  $$ = \frac{1}{L} + 0.5 + 0.25L + 0.125L^2 + 0.2 + 0.1L + 0.05L^2 + \ldots =  \frac{1}{L} + 0.7 + 0.35L + 0.175L^2+\ldots $$
> Aplicando o operador de aniquila√ß√£o $[ \cdot ]_+$:
>  $$ \left[ \frac{1 + 0.2L}{L(1 - 0.5L)} \right]_+ = 0.7 + 0.35L + 0.175L^2 + \ldots  $$
> Ent√£o, a previs√£o de um passo a frente √©:
> $$
> \hat{Y}_{t+1|t} = \mu + (0.7 + 0.35L + 0.175L^2 + \ldots)(1 - 0.5L)(Y_t - \mu)
> $$
> Na pr√°tica, podemos truncar a s√©rie e usar os primeiros termos ou usar a representa√ß√£o ARMA para derivar previs√µes.  Aplicando a distributiva e truncando a s√©rie, teremos:
>  $$\begin{aligned}
>   \hat{Y}_{t+1|t} &\approx  \mu + (0.7 + 0.35L + 0.175L^2 + \ldots)(Y_t - \mu - 0.5(Y_{t-1}-\mu)) \\
>   &\approx  \mu + 0.7(Y_t - \mu) + 0.35(Y_{t-1}-\mu) - 0.5\times 0.7(Y_{t-1} - \mu) + \ldots \\
>   &\approx \mu + 0.7(Y_t - \mu) + 0(Y_{t-1}-\mu)+\ldots
>   \end{aligned}$$
>  Note que, usando o modelo ARMA(1,1) diretamente,
>  $$ Y_{t+1} - \mu = 0.5(Y_t - \mu) + \epsilon_{t+1} + 0.2\epsilon_t $$
>  A melhor estimativa de $\epsilon_{t+1}$ √© zero, ent√£o:
> $$ \hat{Y}_{t+1|t} = \mu + 0.5(Y_t - \mu) + 0.2\epsilon_t  $$
>  e $\epsilon_t = (Y_t - \mu) - 0.5(Y_{t-1} - \mu) - 0.2\epsilon_{t-1} $. Usando essa representa√ß√£o, precisamos estimar $\epsilon_t$ recursivamente, o que pode ser computacionalmente mais complexo do que usar a representa√ß√£o AR($\infty$).
>   
>  Considerando a representa√ß√£o AR($\infty$) obtida no exemplo anterior, e truncando em 3 lags, podemos obter o mesmo resultado:
>  $$\hat{Y}_{t+1|t} \approx  \mu +  0.7(Y_t-\mu) - 0.14(Y_{t-1} - \mu) + 0.028(Y_{t-2} - \mu)$$
>  Note que os coeficientes do modelo AR($\infty$) determinam como os valores passados influenciam o valor futuro, e que os valores num√©ricos dos dois m√©todos n√£o ser√£o exatamente iguais, pois dependem de truncamentos e aproxima√ß√µes.

### Conclus√£o
Este cap√≠tulo demonstrou que um processo ARMA(p,q) que satisfaz as condi√ß√µes de estacionariedade e invertibilidade pode ser expresso como um processo AR(‚àû) [^7]. Essa equival√™ncia √© fundamental para construir previs√µes usando a f√≥rmula de Wiener-Kolmogorov. A capacidade de alternar entre as representa√ß√µes ARMA e AR(‚àû) fornece flexibilidade na modelagem e previs√£o de s√©ries temporais, al√©m de destacar a rela√ß√£o entre diferentes formas de representar o mesmo processo. Ao entender como modelos ARMA(p,q) podem ser representados como modelos AR(‚àû), temos uma vis√£o mais completa e robusta das ferramentas para previs√£o em s√©ries temporais, usando os valores passados das s√©ries temporais, como descrito em [^7].

### Refer√™ncias
[^7]: Se√ß√£o 4.2 - *Forecasts Based on Lagged Y's*
[^8]: Se√ß√£o 4.2 - *Forecasting Based on Lagged e's*
<!-- END -->
