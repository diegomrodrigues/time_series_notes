## Previs√£o Baseada em Ys Defasados Utilizando um Modelo AR(‚àû)

### Introdu√ß√£o
Como vimos anteriormente, os modelos de previs√£o apresentados at√© ent√£o se baseavam na premissa de que o erro $\epsilon_t$ era observado diretamente [^7]. No entanto, em cen√°rios reais, usualmente temos acesso apenas a observa√ß√µes de Y defasados e n√£o aos erros $\epsilon_t$ [^7]. Para lidar com essa situa√ß√£o, √© necess√°rio recorrer a uma representa√ß√£o alternativa para o processo, especificamente um modelo AR(‚àû). Este cap√≠tulo ir√° explorar essa abordagem, detalhando como essa representa√ß√£o se conecta com o modelo MA(‚àû) e como ela pode ser usada para derivar previs√µes.

### Conceitos Fundamentais
A principal ideia por tr√°s do uso de um modelo **AR(‚àû)** √© expressar a s√©rie temporal $Y_t$ como uma fun√ß√£o linear de seus pr√≥prios valores defasados, ao inv√©s de uma fun√ß√£o linear dos erros defasados. Formalmente, podemos representar um processo com uma representa√ß√£o **AR(‚àû)** da seguinte maneira [^7]:
$$
\eta(L)(Y_t - \mu) = \epsilon_t,
$$
onde
$$
\eta(L) = \sum_{j=0}^{\infty} \eta_j L^j,
$$
com $\eta_0 = 1$ e $\sum_{j=0}^{\infty} |\eta_j| < \infty$ [^7]. Aqui, $\eta(L)$ √© um polin√¥mio de defasagem infinito, $\mu$ representa a m√©dia da s√©rie temporal, e $\epsilon_t$ √© um ru√≠do branco [^7]. Al√©m disso, assumimos que o polin√¥mio AR $\eta(L)$ e o polin√¥mio MA $\psi(L)$ s√£o relacionados por $\eta(L) = [\psi(L)]^{-1}$ [^7].

√â importante notar que essa representa√ß√£o est√° intrinsecamente ligada ao modelo **MA(‚àû)** que vimos anteriormente [^7]. De fato, um processo **MA(‚àû)** dado por $(Y_t - \mu) = \psi(L)\epsilon_t$ pode ser reescrito em termos de um modelo AR(‚àû) se $\psi(L)$ for invert√≠vel. A condi√ß√£o de invertibilidade requer que as ra√≠zes do polin√¥mio $\psi(z) = 0$ estejam fora do c√≠rculo unit√°rio [^7].

A representa√ß√£o **AR(‚àû)** nos permite expressar a s√©rie temporal $Y_t$ em termos de seus pr√≥prios valores defasados [^7]:
$$
Y_t - \mu = \sum_{j=1}^{\infty} (-\eta_j)(Y_{t-j} - \mu) + \epsilon_t,
$$
Essa representa√ß√£o √© crucial para derivar previs√µes quando apenas as observa√ß√µes defasadas de Y est√£o dispon√≠veis.

**Lema 1**
Se um processo √© representado por um modelo AR(‚àû) com $\eta(L) = \sum_{j=0}^{\infty} \eta_j L^j$ onde $\eta_0=1$, ent√£o, para todo *t*,
$$
Y_t = \mu + \sum_{j=1}^{\infty} (-\eta_j)(Y_{t-j}-\mu) + \epsilon_t
$$
*Prova:*
I. Come√ßamos com a defini√ß√£o do modelo AR(‚àû):
$$
\eta(L)(Y_t-\mu)=\epsilon_t
$$
II. Expandindo $\eta(L)$, temos:
$$
(1 + \sum_{j=1}^{\infty} \eta_j L^j)(Y_t - \mu) = \epsilon_t
$$
III. Distribuindo $(Y_t - \mu)$ dentro da soma:
$$
(Y_t-\mu) + \sum_{j=1}^{\infty} \eta_j(Y_{t-j} - \mu) = \epsilon_t
$$
IV. Isolando $Y_t$, obtemos o resultado desejado:
$$
Y_t = \mu + \sum_{j=1}^{\infty} (-\eta_j)(Y_{t-j}-\mu) + \epsilon_t
$$
‚ñ†

#### Conex√£o com Modelos ARMA(p,q)
A rela√ß√£o entre os modelos AR(‚àû) e ARMA(p,q) tamb√©m merece aten√ß√£o. Um modelo ARMA(p,q) pode ser expresso como:
$$(1 - \phi_1 L - \phi_2 L^2 - \ldots - \phi_p L^p) (Y_t - \mu) = (1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q)\epsilon_t$$
que √© equivalente a
$$\phi(L)(Y_t - \mu) = \theta(L)\epsilon_t$$
onde $\phi(L) = (1 - \phi_1 L - \phi_2 L^2 - \ldots - \phi_p L^p)$ e $\theta(L) = (1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q)$  [^7].  Neste contexto,  $\eta(L) = \phi(L)/\theta(L)$ , desde que o operador autorregressivo $\phi(L)$ satisfa√ßa a condi√ß√£o de estacionariedade (ra√≠zes de $\phi(z)$ = 0 fora do c√≠rculo unit√°rio) e que o operador da m√©dia m√≥vel $\theta(L)$ satisfa√ßa a condi√ß√£o de invertibilidade (ra√≠zes de $\theta(z)$ = 0 fora do c√≠rculo unit√°rio) [^7].

Para obter a representa√ß√£o AR(‚àû) a partir de um modelo ARMA, podemos reescrever:
$$ Y_t - \mu =  \frac{\theta(L)}{\phi(L)} \epsilon_t $$
e, se $\phi(L)$ for invert√≠vel, temos:
$$
\eta(L) = [\theta(L)]^{-1}\phi(L) = [\psi(L)]^{-1}
$$

> üí° **Exemplo Num√©rico:** Considere um modelo ARMA(1,1) onde $\phi(L) = 1 - 0.7L$ e $\theta(L) = 1 + 0.5L$. Ent√£o, temos
$$
(1 - 0.7L)(Y_t - \mu) = (1 + 0.5L)\epsilon_t
$$
Para encontrar a representa√ß√£o AR(‚àû), precisamos encontrar $\eta(L) = \frac{1 - 0.7L}{1 + 0.5L}$. Podemos fazer isso usando divis√£o polinomial longa ou expans√£o em s√©rie. Uma maneira pr√°tica √© notar que $(1+0.5L)^{-1} = 1 - 0.5L + 0.25L^2 - 0.125L^3 + \ldots$  (expans√£o geom√©trica) , ent√£o:
$$ \eta(L) = (1 - 0.7L)(1 - 0.5L + 0.25L^2 - 0.125L^3 + \ldots) = 1 - 1.2L + 0.6L^2 - 0.35L^3 + \ldots$$
Portanto, a representa√ß√£o AR(‚àû) ser√° dada por:
$$Y_t - \mu = 1.2(Y_{t-1} - \mu) - 0.6(Y_{t-2} - \mu) + 0.35(Y_{t-3} - \mu) + \ldots + \epsilon_t $$
Aqui, podemos ver como os coeficientes $\eta_j$ decaem, como esperado, para que a condi√ß√£o de $\sum_{j=0}^{\infty} |\eta_j| < \infty$ seja atendida.

**Observa√ß√£o 1:** √â importante notar que a representa√ß√£o AR(‚àû) existe se o polin√¥mio $\phi(L)$ for invert√≠vel, o que significa que suas ra√≠zes est√£o fora do c√≠rculo unit√°rio. No entanto, mesmo que $\phi(L)$ n√£o seja invert√≠vel, √© poss√≠vel obter uma representa√ß√£o AR(‚àû) aproximada truncando a s√©rie infinita, o que √© √∫til na pr√°tica, para modelos n√£o estacion√°rios.

### Aplica√ß√µes e Implica√ß√µes
Uma das principais aplica√ß√µes dessa representa√ß√£o √© a capacidade de expressar a previs√£o de $Y_{t+s}$ como uma fun√ß√£o de $Y_s$ defasados [^7]. A f√≥rmula de predi√ß√£o de Wiener-Kolmogorov, expressa em termos de operadores de defasagem, pode ser usada para obter essas previs√µes [^8]:
$$
\hat{Y}_{t+s|t} = \mu + \left[ \frac{\psi(L)}{L^s} \right]_+ \frac{1}{\eta(L)} (Y_t - \mu),
$$
ou, de forma equivalente,
$$
\hat{Y}_{t+s|t} = \mu + \left[ \frac{\psi(L)}{L^s} \right]_+ \eta(L)(Y_t-\mu)
$$
onde o operador de aniquila√ß√£o $[ \cdot ]_+$ substitui as pot√™ncias negativas de L por zero [^8].  Essa formula√ß√£o permite calcular o melhor previsor linear para $Y_{t+s}$, dado o hist√≥rico de $Y_s$ at√© o tempo t.

**Teorema 1** Se um processo √© dado por $Y_t - \mu = \psi(L) \epsilon_t$, onde $\psi(L)$ √© um operador MA invert√≠vel e $\epsilon_t$ √© ru√≠do branco, ent√£o o melhor preditor linear de $Y_{t+s}$ dado $Y_t, Y_{t-1}, Y_{t-2}, \ldots$ √© dado por
$$
\hat{Y}_{t+s|t} = \mu + \left[ \frac{\psi(L)}{L^s} \right]_+ \frac{1}{\eta(L)} (Y_t - \mu)
$$
*Prova:*
I. Come√ßamos com a representa√ß√£o do processo:
$$
Y_t - \mu = \psi(L) \epsilon_t
$$
II. Multiplicamos ambos os lados por $\eta(L) = [\psi(L)]^{-1}$:
$$
\eta(L)(Y_t - \mu) = \epsilon_t
$$
III.  A previs√£o de $Y_{t+s}$ dado $Y_t, Y_{t-1},\ldots$ √© dada por $\hat{Y}_{t+s|t}$. Sabemos que:
$$
Y_{t+s} - \mu = \psi(L) \epsilon_{t+s}
$$
IV.  Multiplicando ambos os lados por $L^{-s}$:
$$
L^{-s}(Y_{t+s} - \mu) = L^{-s}\psi(L) \epsilon_{t+s}
$$
V.  Aplicando o operador de aniquila√ß√£o $[ \cdot ]_+$ (que elimina as pot√™ncias negativas de L) e usando o fato de que $\hat{\epsilon}_{t+j|t} = 0$ para $j>0$ , temos:
$$
\hat{Y}_{t+s|t} - \mu =  \left[ \frac{\psi(L)}{L^s} \right]_+  \epsilon_{t+s}
$$
VI. Substituindo $\epsilon_t$ por $\eta(L)(Y_t - \mu)$ e usando o fato de que $\epsilon_{t+j}=0$ para $j>0$ quando usado no preditor, obtemos o resultado desejado:
$$
\hat{Y}_{t+s|t} = \mu + \left[ \frac{\psi(L)}{L^s} \right]_+ \frac{1}{\eta(L)} (Y_t - \mu)
$$
‚ñ†
> üí° **Exemplo Num√©rico:** Vamos supor que temos um processo que pode ser modelado como  $Y_t - \mu = \psi(L)\epsilon_t$, e que $\psi(L) = 1 + 0.8L$. Queremos prever $Y_{t+1}$ dado $Y_t, Y_{t-1}, \ldots$. Sabemos que $\eta(L) = [\psi(L)]^{-1} = (1+0.8L)^{-1} = 1 - 0.8L + 0.64L^2 - 0.512L^3 + \ldots$. Para prever $Y_{t+1}$, temos $s=1$, ent√£o:
> $$ \hat{Y}_{t+1|t} = \mu + \left[ \frac{1 + 0.8L}{L} \right]_+ \frac{1}{\eta(L)} (Y_t - \mu) = \mu + [0.8 + L^{-1}]_+ \frac{1}{\eta(L)}(Y_t - \mu) = \mu + 0.8  \frac{1}{\eta(L)}(Y_t - \mu) $$
>  Como $1/\eta(L)=\psi(L)$, ent√£o  $\hat{Y}_{t+1|t} = \mu + 0.8(1 + 0.8L)(Y_t - \mu)$.  Expandindo, obtemos:
>  $$ \hat{Y}_{t+1|t} = \mu + 0.8(Y_t - \mu) + 0.64(Y_{t-1} - \mu) $$
>  Isso mostra como a previs√£o de $Y_{t+1}$ √© uma fun√ß√£o dos valores passados de Y. Os coeficientes indicam a influ√™ncia de cada valor defasado na previs√£o.

#### Constru√ß√£o do Erro
Como os erros $\epsilon_t$ n√£o s√£o diretamente observ√°veis, eles devem ser constru√≠dos a partir das observa√ß√µes de Y [^7]. Se a representa√ß√£o AR(‚àû) for dada por $\eta(L)(Y_t - \mu) = \epsilon_t$ , ent√£o o erro $\epsilon_t$ pode ser obtido da seguinte forma:
$$ \epsilon_t = \eta(L)(Y_t - \mu). $$
Na pr√°tica, como o operador $\eta(L)$ √© infinito, essa constru√ß√£o envolve uma aproxima√ß√£o com base em um n√∫mero finito de termos do operador de defasagem ou ent√£o a considera√ß√£o de um modelo ARMA invert√≠vel, que permite computar o erro atrav√©s da sua representa√ß√£o MA invert√≠vel.

**Proposi√ß√£o 1**
Para um modelo ARMA(p,q) onde $\phi(L)(Y_t - \mu) = \theta(L)\epsilon_t$ e $\phi(L)$ √© invert√≠vel, o erro $\epsilon_t$ pode ser calculado como:
$$
\epsilon_t = \frac{\phi(L)}{\theta(L)} (Y_t-\mu)
$$
*Prova:*
I.  Come√ßamos com a equa√ß√£o do modelo ARMA(p,q):
$$
\phi(L)(Y_t - \mu) = \theta(L)\epsilon_t
$$
II. Dividindo ambos os lados por $\theta(L)$ (que assumimos ser invert√≠vel), obtemos o resultado desejado:
$$
\epsilon_t = \frac{\phi(L)}{\theta(L)} (Y_t-\mu)
$$
‚ñ†

> üí° **Exemplo Num√©rico:** Considerando o mesmo modelo ARMA(1,1) do exemplo anterior, $(1 - 0.7L)(Y_t - \mu) = (1 + 0.5L)\epsilon_t$, temos:
>  $$ \epsilon_t = \frac{1 - 0.7L}{1 + 0.5L}(Y_t - \mu) $$
>  Como vimos, $\frac{1 - 0.7L}{1 + 0.5L} = 1 - 1.2L + 0.6L^2 - 0.35L^3 + \ldots$, ent√£o
>  $$ \epsilon_t = (Y_t - \mu) - 1.2(Y_{t-1} - \mu) + 0.6(Y_{t-2} - \mu) - 0.35(Y_{t-3} - \mu) + \ldots $$
>  Na pr√°tica, para calcular  $\epsilon_t$, podemos truncar a representa√ß√£o AR(‚àû) usando um n√∫mero finito de termos ou usar a representa√ß√£o ARMA  para estimar os erros. Se tivermos um modelo ARMA estimado com os coeficientes $\phi_1 = 0.7$ e $\theta_1 = 0.5$, e observa√ß√µes de $Y$ (por exemplo, $\mu = 0$,  $Y_t = 10$, $Y_{t-1} = 8$, $Y_{t-2}=6$), o erro $\epsilon_t$ √© calculado usando $\epsilon_t  = (Y_t - 0.7Y_{t-1}) - 0.5\epsilon_{t-1} $, e precisamos de um valor inicial para $\epsilon_{t-1}$. Se assumirmos $\epsilon_{t-1} = 0$, ent√£o  $\epsilon_t = (10 - 0.7 \times 8) = 4.4 $. Se tivermos um valor de $\epsilon_{t-1}$, usamos a recurs√£o:  $\epsilon_t =  10 - 0.7 * 8 - 0.5\epsilon_{t-1}$. Note que  $\epsilon_t$  depende dos valores passados de  $Y$ e tamb√©m dos valores passados de $\epsilon$. Esta forma recursiva de calcular os erros √© essencial quando usamos um modelo ARMA(p,q) para obter os res√≠duos.

### Conclus√£o
O uso de um modelo AR(‚àû) oferece uma abordagem pr√°tica para lidar com a situa√ß√£o em que apenas os valores defasados de Y s√£o observ√°veis [^7]. Ao expressar o processo em termos de seus pr√≥prios valores defasados, √© poss√≠vel construir previs√µes e analisar as propriedades estat√≠sticas do processo, usando a representa√ß√£o de Wiener-Kolmogorov [^8]. Essa formula√ß√£o conecta as representa√ß√µes MA e AR e demonstra a import√¢ncia de escolher a representa√ß√£o mais adequada ao cen√°rio. Al√©m disso, essa representa√ß√£o, atrav√©s da manipula√ß√£o dos operadores, permite derivar previs√µes mais precisas em situa√ß√µes pr√°ticas onde os erros $\epsilon_t$ n√£o s√£o diretamente observados.

### Refer√™ncias
[^7]:  Se√ß√£o 4.2 - *Forecasts Based on Lagged Y's*
[^8]: Se√ß√£o 4.2 - *Forecasting Based on Lagged e's*
<!-- END -->
