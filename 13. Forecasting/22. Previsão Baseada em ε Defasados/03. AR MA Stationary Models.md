## Previs√£o para Modelos AR(p) e MA(q) Estacion√°rios Usando a Invers√£o de Œ∑(L)

### Introdu√ß√£o
Em continuidade √† an√°lise das representa√ß√µes AR(‚àû) e suas aplica√ß√µes para previs√µes, este cap√≠tulo aborda a aplica√ß√£o dessas t√©cnicas a modelos AR(p) e MA(q) estacion√°rios. Especificamente, focaremos em como a propriedade $\eta(L) = 1/\psi(L)$, v√°lida para esses modelos, permite aplicar as mesmas t√©cnicas de previs√£o desenvolvidas para os casos em que o erro $\epsilon_t$ √© conhecido [^7]. A discuss√£o ir√° elucidar como a condi√ß√£o de estacionariedade simplifica o processo de previs√£o, permitindo que modelos AR(p) e MA(q) sejam tratados de forma similar aos casos em que o erro √© observado diretamente.

### A Propriedade Œ∑(L) = 1/œà(L) em Modelos Estacion√°rios
Em modelos AR(p) e MA(q) estacion√°rios, a representa√ß√£o AR(‚àû) assume uma forma mais simples e manej√°vel. Conforme discutido anteriormente, a representa√ß√£o AR(‚àû) √© dada por:
$$
\eta(L)(Y_t - \mu) = \epsilon_t
$$
onde $\eta(L)$ √© um polin√¥mio de defasagem infinito. Nos modelos AR(p) e MA(q), $\eta(L)$ assume a forma $1/\psi(L)$, onde $\psi(L)$ √© o polin√¥mio da representa√ß√£o MA [^7]. Especificamente, para um modelo AR(p), onde $\psi(L) = 1$, temos $\eta(L) = 1/\phi(L)$. Para um modelo MA(q), temos $\eta(L) = 1/\theta(L)$. Essa propriedade surge da condi√ß√£o de que um modelo ARMA(p, q), estacion√°rio e invert√≠vel, pode ser expresso tanto em termos de um polin√¥mio AR infinito como de um polin√¥mio MA infinito, e esses polin√¥mios s√£o inversos um do outro [^7].

**Lema 3:** Para modelos AR(p) e MA(q) estacion√°rios, a rela√ß√£o entre $\eta(L)$ e $\psi(L)$ √© dada por
$$ \eta(L) = \frac{1}{\psi(L)} $$
*Prova:*
I. Para um modelo AR(p), a representa√ß√£o do modelo √© dada por $\phi(L)(Y_t - \mu) = \epsilon_t$. A representa√ß√£o MA($\infty$) para modelos AR(p) √© $Y_t - \mu = \frac{1}{\phi(L)}\epsilon_t$, que √© igual a $Y_t - \mu = \frac{1}{\psi(L)}\epsilon_t$ porque $\psi(L) = 1$ para modelos AR(p).  Portanto, $\eta(L) = \phi(L)$ que √© equivalente a $\eta(L) = 1/\psi(L)$.
II. Para um modelo MA(q), a representa√ß√£o √© dada por $(Y_t - \mu) = \theta(L)\epsilon_t$. Comparando isso com $Y_t - \mu = \frac{1}{\eta(L)}\epsilon_t$ temos $\eta(L) = \frac{1}{\theta(L)}$ que √© igual a $\eta(L) = \frac{1}{\psi(L)}$ porque $\psi(L)=\theta(L)$ em modelos MA(q).
III. Para um modelo ARMA(p, q) estacion√°rio e invert√≠vel, temos que  $\phi(L)(Y_t - \mu) = \theta(L)\epsilon_t$. Reorganizando,  $(Y_t - \mu) = \frac{\theta(L)}{\phi(L)}\epsilon_t$.  Comparando isso com $(Y_t - \mu) = \frac{1}{\eta(L)}\epsilon_t$, vemos que $\eta(L) = \frac{\phi(L)}{\theta(L)}$. Como sabemos que $\psi(L) = \frac{\theta(L)}{\phi(L)}$ para um modelo ARMA, temos $\eta(L) = \frac{1}{\psi(L)}$.
‚ñ†

Essa propriedade √© crucial, pois ela simplifica a an√°lise da previs√£o, permitindo que abordagens desenvolvidas para modelos MA(‚àû) possam ser aplicadas de forma direta.

**Lema 3.1:** Para um modelo ARMA(p, q) estacion√°rio e invert√≠vel, $\eta(L)$ tamb√©m pode ser expresso como uma raz√£o entre dois polin√¥mios de defasagem, especificamente $\eta(L) = \frac{\phi(L)}{\theta(L)}$, onde $\phi(L)$ √© o polin√¥mio AR e $\theta(L)$ √© o polin√¥mio MA.
*Prova:*
A representa√ß√£o geral de um modelo ARMA(p, q) √© dada por $\phi(L)(Y_t - \mu) = \theta(L)\epsilon_t$. Reorganizando, temos $(Y_t - \mu) = \frac{\theta(L)}{\phi(L)}\epsilon_t$. Comparando com a representa√ß√£o AR(‚àû), $(Y_t - \mu) = \frac{1}{\eta(L)}\epsilon_t$, conclu√≠mos que $\frac{1}{\eta(L)} = \frac{\theta(L)}{\phi(L)}$, e portanto, $\eta(L) = \frac{\phi(L)}{\theta(L)}$.
‚ñ†
> üí° **Exemplo Num√©rico:**  Considere um modelo ARMA(1,1) com $\phi(L) = 1 - 0.7L$ e $\theta(L) = 1 + 0.5L$.  Ent√£o $\eta(L) = \frac{1-0.7L}{1+0.5L}$. Se expandirmos $\eta(L)$ em pot√™ncias de $L$, podemos obter:
>$$ \eta(L) = (1 - 0.7L)(1 - 0.5L + 0.25L^2 - 0.125L^3 + \ldots) = 1 - 1.2L + 0.65L^2 - 0.3125L^3 + \ldots$$
> Isso nos permite expressar o erro $\epsilon_t$ em termos de valores defasados de $Y_t$ como:
> $$\epsilon_t = (Y_t - \mu) - 1.2(Y_{t-1} - \mu) + 0.65(Y_{t-2} - \mu) - 0.3125(Y_{t-3} - \mu) + \ldots$$
> Embora essa representa√ß√£o seja te√≥rica e √∫til para entender a estrutura do modelo, na pr√°tica, as previs√µes s√£o geralmente feitas usando a forma original do modelo ARMA.

Este resultado complementa o Lema 3, detalhando a forma espec√≠fica de $\eta(L)$ para modelos ARMA, demonstrando sua rela√ß√£o com os polin√¥mios AR e MA.

### Previs√£o em Modelos AR(p)
Em um modelo **AR(p)**, temos que $\psi(L) = 1$, e portanto, $\eta(L) = 1/\phi(L)$ [^7]. Assim, o modelo pode ser representado como:
$$
\phi(L)(Y_t - \mu) = \epsilon_t
$$
onde $\phi(L) = 1 - \phi_1 L - \phi_2 L^2 - \ldots - \phi_p L^p$. A previs√£o de s passos √† frente pode ser obtida utilizando a mesma abordagem de Wiener-Kolmogorov, com o operador de aniquila√ß√£o [^8]:
$$
\hat{Y}_{t+s|t} = \mu + \left[ \frac{1}{L^s} \right]_+ \frac{1}{\eta(L)} (Y_t - \mu) = \mu + \left[ \frac{1}{L^s} \right]_+ \phi(L)(Y_t - \mu)
$$
Note que o operador de aniquila√ß√£o $[ \cdot ]_+$  imp√µe que apenas as pot√™ncias positivas de L sejam mantidas [^8]. No caso do modelo AR(p), a representa√ß√£o AR(‚àû) se torna equivalente √† representa√ß√£o original, de forma que a previs√£o pode ser obtida diretamente da forma AR(p).
De forma recursiva, podemos expressar a previs√£o para s per√≠odos a frente como:
$$ \hat{Y}_{t+s|t}  = \phi_1 \hat{Y}_{t+s-1|t}  + \phi_2 \hat{Y}_{t+s-2|t}  + \ldots + \phi_p \hat{Y}_{t+s-p|t} + \mu $$
onde $\hat{Y}_{t+j|t}  =  Y_{t+j}$ se $j \leq 0$.
> üí° **Exemplo Num√©rico:** Considere um modelo AR(1) estacion√°rio dado por $(1 - 0.8L)(Y_t - \mu) = \epsilon_t$.  Para prever $Y_{t+1}$ dado $Y_t, Y_{t-1}, \ldots$, temos $\phi(L) = 1 - 0.8L$, $s=1$, e ent√£o:
>  $$
> \hat{Y}_{t+1|t} = \mu + \left[ \frac{1}{L} \right]_+ (1 - 0.8L)(Y_t - \mu) = \mu + 0.8(Y_t-\mu)
>  $$
>  Se assumirmos $\mu = 10$ e $Y_t = 15$, ent√£o:
>   $$
>  \hat{Y}_{t+1|t} = 10 + 0.8(15-10) = 10 + 0.8(5) = 14
>  $$
>  Para prever $Y_{t+2}$, temos:
>  $$
> \hat{Y}_{t+2|t} = \mu + \left[ \frac{1}{L^2} \right]_+ (1 - 0.8L)(Y_t - \mu)  = \mu +  0.8 \hat{Y}_{t+1|t} - \mu = \mu + 0.8(0.8(Y_t-\mu)) = \mu + 0.64(Y_t - \mu)
>  $$
>  Usando $\hat{Y}_{t+1|t} = 14$:
>   $$
>  \hat{Y}_{t+2|t} =  10 + 0.8(14 - 10) = 10 + 0.8(4) = 13.2
>  $$
> Ou equivalentemente:
>    $$
>  \hat{Y}_{t+2|t} =  10 + 0.64(15 - 10) = 10 + 0.64(5) = 13.2
>  $$
>  A representa√ß√£o AR(‚àû) n√£o √© necess√°ria, pois a representa√ß√£o original j√° descreve a rela√ß√£o dos valores defasados com o valor corrente.
>  Se continuarmos com essa itera√ß√£o, por exemplo para $s=5$, teremos
>     $$
>  \hat{Y}_{t+5|t} =  10 + 0.8^5(15 - 10) \approx 10 + 0.32768(5) \approx 11.64
>  $$
> Como podemos observar, a previs√£o se aproxima da m√©dia $\mu=10$ a medida que $s$ aumenta.

**Proposi√ß√£o 1:** As previs√µes para modelos AR(p) convergem para a m√©dia $\mu$ √† medida que o horizonte de previs√£o $s$ aumenta.
*Prova:*
I. A previs√£o recursiva de um modelo AR(p) √© dada por $\hat{Y}_{t+s|t} = \phi_1 \hat{Y}_{t+s-1|t} + \phi_2 \hat{Y}_{t+s-2|t} + \ldots + \phi_p \hat{Y}_{t+s-p|t} + \mu$.
II. √Ä medida que $s$ cresce, os valores de $\hat{Y}_{t+j|t}$ para $j > 0$ s√£o previs√µes e n√£o valores observados.
III. Sob a condi√ß√£o de estacionaridade do modelo AR(p), sabemos que as previs√µes convergem para a m√©dia $\mu$ conforme $s$ aumenta, ou seja, $\lim_{s \to \infty} \hat{Y}_{t+s|t} = \mu$.
IV. Portanto, para $s$ suficientemente grande, $\hat{Y}_{t+s|t} \approx \mu$.
‚ñ†

Essa proposi√ß√£o formaliza o comportamento das previs√µes em modelos AR(p) a longo prazo, mostrando que elas se aproximam da m√©dia da s√©rie temporal.

### Previs√£o em Modelos MA(q)
Em modelos **MA(q)**, temos que $\eta(L) = 1/\theta(L)$, onde $\theta(L) = 1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q$ [^7]. A representa√ß√£o do modelo √©:
$$
(Y_t - \mu) = \theta(L) \epsilon_t
$$
Para obter a previs√£o de $Y_{t+s}$ com base em $Y_t, Y_{t-1}, \ldots$  utilizamos a mesma f√≥rmula de Wiener-Kolmogorov [^8]:
$$
\hat{Y}_{t+s|t} = \mu + \left[ \frac{\theta(L)}{L^s} \right]_+ \frac{1}{1/\theta(L)}(Y_t - \mu) =  \mu + \left[ \frac{\theta(L)}{L^s} \right]_+ \theta(L) (Y_t - \mu)
$$
onde  $[ \cdot ]_+$ √© o operador de aniquila√ß√£o, que descarta pot√™ncias negativas de L [^8].  Essa equa√ß√£o √© semelhante √†quela obtida para o modelo MA($\infty$), com a particularidade de que o operador $\theta(L)$ √© de ordem finita.
De forma equivalente, podemos usar a expans√£o em s√©rie de $\eta(L) = 1/\theta(L)$ para obter o erro como uma fun√ß√£o dos valores defasados de Y, mas n√£o √© necess√°rio para computar a previs√£o de $Y_{t+s}$.
> üí° **Exemplo Num√©rico:** Considere um modelo MA(1) dado por $(Y_t - \mu) = (1 + 0.6L)\epsilon_t$. Ent√£o $\theta(L) = 1 + 0.6L$.  Para prever $Y_{t+1}$ dado $Y_t, Y_{t-1}, \ldots$, temos $s=1$, e portanto:
> $$\hat{Y}_{t+1|t} = \mu + \left[ \frac{1 + 0.6L}{L} \right]_+  \epsilon_t = \mu + 0.6\epsilon_t $$
> como $\epsilon_t  = (Y_t - \mu) - 0.6\epsilon_{t-1}$. Obtemos:
>  $$ \hat{Y}_{t+1|t} = \mu + 0.6[(Y_t - \mu) - 0.6\epsilon_{t-1}] = \mu + 0.6(Y_t-\mu)  - 0.36 \epsilon_{t-1}$$
>  Note que, assim como na representa√ß√£o AR($\infty$) geral, √© preciso obter $\epsilon_t$  recursivamente. No entanto, quando   $s>q$, a melhor previs√£o √© simplesmente a m√©dia $\mu$. Se quisermos prever $Y_{t+2}$, ent√£o $s=2$ e
>  $$\hat{Y}_{t+2|t} = \mu + \left[ \frac{1 + 0.6L}{L^2} \right]_+ (1+0.6L) (Y_t - \mu) = \mu + [0](Y_t - \mu) = \mu $$
>  pois o operador de aniquila√ß√£o descarta pot√™ncias negativas.
>  Vamos assumir $\mu=20$. Se tivermos um valor observado $Y_t = 25$ e soubermos que $\epsilon_{t-1} = 2$, ent√£o:
>  $$ \hat{Y}_{t+1|t} = 20 + 0.6(25-20) - 0.36(2) = 20 + 3 - 0.72 = 22.28$$
>  E a previs√£o para $s=2$ ser√°:
> $$ \hat{Y}_{t+2|t} = 20 $$
> Como podemos observar, a previs√£o se iguala a media $\mu$ quando $s > q = 1$.

**Proposi√ß√£o 2:** Para modelos MA(q), a previs√£o $\hat{Y}_{t+s|t}$ √© igual √† m√©dia $\mu$ para horizontes de previs√£o $s > q$.
*Prova:*
I. Em um modelo MA(q), a previs√£o de $s$ passos √† frente √© dada por $\hat{Y}_{t+s|t} = \mu + \left[ \frac{\theta(L)}{L^s} \right]_+ \theta(L) (Y_t - \mu)$.
II. Quando $s > q$, o operador de aniquila√ß√£o $[ \frac{\theta(L)}{L^s} ]_+$ resultar√° em zero, pois todas as pot√™ncias de $L$ em $\frac{\theta(L)}{L^s}$ ser√£o negativas.
III. Portanto, $\hat{Y}_{t+s|t} = \mu + [0] (Y_t - \mu) = \mu$.
‚ñ†

Esta proposi√ß√£o estabelece um resultado importante sobre o comportamento das previs√µes em modelos MA(q), indicando que, a partir de um certo horizonte temporal, a previs√£o se torna constante e igual √† m√©dia do processo.

### Conclus√£o
A propriedade $\eta(L) = 1/\psi(L)$, v√°lida para modelos AR(p) e MA(q) estacion√°rios, simplifica a previs√£o ao permitir o uso das mesmas t√©cnicas derivadas para os casos em que o erro √© conhecido [^7]. Essa propriedade mostra que a representa√ß√£o AR(‚àû) serve como uma estrutura unificadora, permitindo a aplica√ß√£o das mesmas ferramentas de previs√£o a diversos modelos de s√©ries temporais [^7]. A previs√£o √© feita usando os valores defasados, construindo recursivamente os erros, ou usando a forma original do modelo e a intui√ß√£o sobre o efeito dos operadores de defasagem. Ao explorar essas conex√µes, este cap√≠tulo consolida a base para uma abordagem pr√°tica e robusta na previs√£o de s√©ries temporais com modelos AR(p) e MA(q) estacion√°rios.

### Refer√™ncias
[^7]: Se√ß√£o 4.2 - *Forecasts Based on Lagged Y's*
[^8]: Se√ß√£o 4.2 - *Forecasting Based on Lagged e's*
<!-- END -->
