## Regress√£o OLS e a Minimiza√ß√£o do Erro Quadr√°tico M√©dio na Proje√ß√£o Linear

### Introdu√ß√£o

Este cap√≠tulo expande a discuss√£o anterior sobre a rela√ß√£o entre **proje√ß√£o linear** e **regress√£o de m√≠nimos quadrados ordin√°rios (OLS)**, focando especificamente em como o m√©todo OLS minimiza a soma dos quadrados dos res√≠duos, um processo intimamente ligado √† minimiza√ß√£o do **erro quadr√°tico m√©dio (MSE)** na proje√ß√£o linear. Como vimos anteriormente [^4.1.1, ^4.1.9], a proje√ß√£o linear busca a melhor aproxima√ß√£o linear de uma vari√°vel aleat√≥ria $Y_{t+1}$ em fun√ß√£o de outras vari√°veis $X_t$, e esta aproxima√ß√£o √© definida pela minimiza√ß√£o do MSE. Em contrapartida, a regress√£o OLS busca ajustar um modelo linear aos dados, minimizando a soma dos quadrados dos res√≠duos [^4.1.17]. O objetivo desta se√ß√£o √© demonstrar como essas duas abordagens est√£o fundamentalmente conectadas, e como a minimiza√ß√£o da soma dos quadrados dos res√≠duos na regress√£o OLS pode ser entendida como uma aproxima√ß√£o da minimiza√ß√£o do MSE na proje√ß√£o linear.

### Conex√£o entre OLS e Minimiza√ß√£o do MSE na Proje√ß√£o Linear

Na **proje√ß√£o linear**, o objetivo √© encontrar o vetor $\alpha$ que minimiza o erro quadr√°tico m√©dio:
$$ MSE(Y_{t+1}|X_t) = E[(Y_{t+1} - \alpha'X_t)^2]. $$
A solu√ß√£o para este problema √© dada por [^4.1.13]:
$$ \alpha' = E(Y_{t+1}X_t')[E(X_tX_t')]^{-1}. $$
**Lema 1:** A solu√ß√£o $\alpha'$ da proje√ß√£o linear tamb√©m satisfaz a seguinte rela√ß√£o de ortogonalidade:
$E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$.
*Prova:*
I. Expandindo o MSE, temos
$MSE(Y_{t+1}|X_t) = E[Y_{t+1}^2 - 2\alpha'X_tY_{t+1} + (\alpha'X_t)^2]$.
II. Tomando a derivada em rela√ß√£o a $\alpha$ e igualando a zero para encontrar o m√≠nimo, temos:
$-2E(X_tY_{t+1}) + 2E(X_tX_t')\alpha=0$.
III. Isso nos leva a $\alpha' = E(Y_{t+1}X_t')[E(X_tX_t')]^{-1}$.
IV. Multiplicando ambos os lados por $E(X_tX_t')$ e rearranjando, obtemos $E(X_tY_{t+1}) - \alpha'E(X_tX_t')=0$, o que implica que $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$.
‚ñ†
> üí° **Exemplo Num√©rico:**
>
> Suponha que temos as seguintes esperan√ßas para as vari√°veis $Y_{t+1}$ e $X_t$ (onde $X_t$ √© um vetor de duas vari√°veis) :
>
> $E(Y_{t+1}X_t') = \begin{bmatrix} 1 & 0.8 \end{bmatrix}$
> $E(X_tX_t') = \begin{bmatrix} 2 & 0.5 \\ 0.5 & 1 \end{bmatrix}$
>
>
> Podemos calcular $\alpha'$ usando a f√≥rmula:
>
> $\alpha' = E(Y_{t+1}X_t')[E(X_tX_t')]^{-1}$
>
> Primeiro, calculamos a inversa de $E(X_tX_t')$:
>
> $ [E(X_tX_t')]^{-1} = \frac{1}{(2)(1) - (0.5)(0.5)} \begin{bmatrix} 1 & -0.5 \\ -0.5 & 2 \end{bmatrix} = \frac{1}{1.75} \begin{bmatrix} 1 & -0.5 \\ -0.5 & 2 \end{bmatrix} \approx \begin{bmatrix} 0.571 & -0.286 \\ -0.286 & 1.143 \end{bmatrix} $
>
> Agora, multiplicamos $E(Y_{t+1}X_t')$ pela inversa de $E(X_tX_t')$:
>
> $\alpha' = \begin{bmatrix} 1 & 0.8 \end{bmatrix} \begin{bmatrix} 0.571 & -0.286 \\ -0.286 & 1.143 \end{bmatrix} = \begin{bmatrix} 0.571 - 0.229 & -0.286 + 0.914 \end{bmatrix} = \begin{bmatrix} 0.342 & 0.628 \end{bmatrix}$
>
> Portanto, o vetor $\alpha'$ que minimiza o MSE na proje√ß√£o linear √© aproximadamente $\begin{bmatrix} 0.342 & 0.628 \end{bmatrix}$. A condi√ß√£o de ortogonalidade do lema 1 implica que $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$, o que significa que o erro da proje√ß√£o n√£o tem correla√ß√£o com os regressores.

Por outro lado, a **regress√£o OLS** tem como objetivo minimizar a soma dos quadrados dos res√≠duos, que √© dada por:
$$ \sum_{t=1}^T (y_{t+1} - \beta'x_t)^2. $$
O estimador OLS, denotado por $b$, √© obtido minimizando esta soma:
$$ b = \left(\sum_{t=1}^{T} x_tx_t'\right)^{-1} \sum_{t=1}^{T} x_ty_{t+1}. $$
Como discutido anteriormente [^4.1.20] e formalizado no ap√™ndice 4.A, se definirmos vari√°veis aleat√≥rias artificiais $\xi$ e $\omega$ cujos momentos populacionais correspondem aos momentos amostrais, podemos interpretar o estimador OLS $b$ como a solu√ß√£o da minimiza√ß√£o do MSE para a proje√ß√£o linear dessas vari√°veis artificiais. Ou seja, o problema de minimizar a soma dos quadrados dos res√≠duos, que √© uma opera√ß√£o com momentos de amostra, torna-se equivalente √† minimiza√ß√£o de um erro quadr√°tico m√©dio com momentos populacionais constru√≠dos a partir da amostra.
$$E[(\omega - \alpha'\xi)^2] = \frac{1}{T} \sum_{t=1}^T(y_{t+1} - \alpha'x_t)^2$$
**Lema 1.1:** O estimador OLS $b$ satisfaz a seguinte condi√ß√£o de ortogonalidade amostral:
$\sum_{t=1}^T (y_{t+1} - b'x_t)x_t' = 0$.
*Prova:*
I. O estimador OLS $b$ √© obtido minimizando $\sum_{t=1}^T (y_{t+1} - \beta'x_t)^2$.
II. Derivando essa express√£o em rela√ß√£o a $\beta$ e igualando a zero, obtemos:
$-2\sum_{t=1}^T x_t(y_{t+1} - \beta'x_t) = 0$.
III. Resolvendo para $\beta$, obtemos a solu√ß√£o OLS $b = \left(\sum_{t=1}^T x_t x_t'\right)^{-1} \sum_{t=1}^T x_t y_{t+1}$.
IV. Rearranjando a express√£o da derivada, obtemos $\sum_{t=1}^T x_t y_{t+1} - \sum_{t=1}^T x_t x_t'b = 0$, que implica $\sum_{t=1}^T (y_{t+1} - b'x_t)x_t' = 0$.
‚ñ†
> üí° **Exemplo Num√©rico:**
>
> Suponha que temos uma amostra de 5 observa√ß√µes para $y_{t+1}$ e $x_t$ (onde $x_t$ √© um vetor de duas vari√°veis):
>
> $ y = \begin{bmatrix} 2 \\ 3 \\ 4 \\ 5 \\ 6 \end{bmatrix},  X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 2 & 2 \\ 2 & 3 \\ 3 & 4 \end{bmatrix} $
>
> Podemos calcular o estimador OLS $b$ usando a f√≥rmula:
>
> $ b = \left(\sum_{t=1}^{T} x_tx_t'\right)^{-1} \sum_{t=1}^{T} x_ty_{t+1}$
>
> Primeiro, vamos calcular $\sum_{t=1}^{T} x_tx_t'$:
>
> $\sum_{t=1}^{T} x_tx_t' =  \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 2 & 2 \\ 2 & 3 \\ 3 & 4 \end{bmatrix}^T \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 2 & 2 \\ 2 & 3 \\ 3 & 4 \end{bmatrix} = \begin{bmatrix} 19 & 23 \\ 23 & 34 \end{bmatrix}$
>
> Agora, calculamos a inversa de $\sum_{t=1}^{T} x_tx_t'$:
>
> $\left(\sum_{t=1}^{T} x_tx_t'\right)^{-1} = \frac{1}{(19)(34)-(23)(23)} \begin{bmatrix} 34 & -23 \\ -23 & 19 \end{bmatrix} = \frac{1}{119} \begin{bmatrix} 34 & -23 \\ -23 & 19 \end{bmatrix} \approx \begin{bmatrix} 0.286 & -0.193 \\ -0.193 & 0.160 \end{bmatrix}$
>
> Em seguida, calculamos $\sum_{t=1}^{T} x_ty_{t+1}$:
>
> $\sum_{t=1}^{T} x_ty_{t+1} = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 2 & 2 \\ 2 & 3 \\ 3 & 4 \end{bmatrix}^T \begin{bmatrix} 2 \\ 3 \\ 4 \\ 5 \\ 6 \end{bmatrix} = \begin{bmatrix} 47 \\ 66 \end{bmatrix}$
>
>
> Finalmente, calculamos $b$:
>
> $b = \begin{bmatrix} 0.286 & -0.193 \\ -0.193 & 0.160 \end{bmatrix} \begin{bmatrix} 47 \\ 66 \end{bmatrix} = \begin{bmatrix} 0.286*47 - 0.193*66 \\ -0.193*47 + 0.160*66 \end{bmatrix} = \begin{bmatrix} 13.442 - 12.738 \\ -9.071 + 10.56 \end{bmatrix} = \begin{bmatrix} 0.704 \\ 1.489 \end{bmatrix}$
>
> Portanto, o estimador OLS $b$ √© aproximadamente $\begin{bmatrix} 0.704 \\ 1.489 \end{bmatrix}$.
>
> A condi√ß√£o de ortogonalidade amostral implica que $\sum_{t=1}^T (y_{t+1} - b'x_t)x_t' = 0$, ou seja, os res√≠duos amostrais s√£o ortogonais aos regressores.

A conex√£o crucial entre esses dois m√©todos reside na rela√ß√£o entre momentos amostrais e momentos populacionais. Sob as condi√ß√µes de *covari√¢ncia-estacionariedade* e *ergodicidade* [^4.1.20], temos que:

$$ \frac{1}{T} \sum_{t=1}^T x_t x_t' \xrightarrow{p} E(X_t X_t'), $$
$$ \frac{1}{T} \sum_{t=1}^T x_t y_{t+1} \xrightarrow{p} E(Y_{t+1} X_t'). $$

Com isso, o estimador OLS $b$ converge em probabilidade para o coeficiente $\alpha$ da proje√ß√£o linear:
$$ b \xrightarrow{p} \alpha. $$
**Observa√ß√£o 1:** A condi√ß√£o de ortogonalidade amostral para OLS, $\sum_{t=1}^T (y_{t+1} - b'x_t)x_t' = 0$ implica que a amostra dos res√≠duos de OLS s√£o ortogonais aos regressores, o que √© uma condi√ß√£o an√°loga √† condi√ß√£o de ortogonalidade populacional para a proje√ß√£o linear, $E[(Y_{t+1} - \alpha'X_t)X_t'] = 0$.

Isso significa que, em amostras grandes, a regress√£o OLS fornece uma aproxima√ß√£o consistente do coeficiente da proje√ß√£o linear, e a minimiza√ß√£o da soma dos quadrados dos res√≠duos na regress√£o OLS √© uma aproxima√ß√£o da minimiza√ß√£o do MSE na proje√ß√£o linear.

**Teorema 2:** Dado que a regress√£o OLS minimiza $\frac{1}{T}\sum_{t=1}^T(y_{t+1}-\beta'x_t)^2$, e que sob estacionariedade e ergodicidade $\frac{1}{T}\sum_{t=1}^T(y_{t+1}-\beta'x_t)^2 \xrightarrow{p} E[(Y_{t+1}-\beta'X_t)^2]$, ent√£o o estimador OLS $b$ converge para o valor que minimiza o MSE na proje√ß√£o linear, quando $T\rightarrow \infty$.

*Prova:*
I. O estimador OLS $b$ √© o valor que minimiza a soma dos quadrados dos res√≠duos: $\frac{1}{T}\sum_{t=1}^T(y_{t+1}-\beta'x_t)^2$ [^4.1.18].
II. Sob estacionariedade e ergodicidade, $\frac{1}{T}\sum_{t=1}^T(y_{t+1}-\beta'x_t)^2$ converge em probabilidade para o valor esperado $E[(Y_{t+1}-\beta'X_t)^2]$.
III.  A proje√ß√£o linear busca o valor de $\alpha$ que minimiza o MSE $E[(Y_{t+1}-\alpha'X_t)^2]$ [^4.1.1].
IV.  Portanto, quando $T\rightarrow \infty$, o estimador OLS $b$ converge para o valor $\alpha$ que minimiza o MSE na proje√ß√£o linear.
‚ñ†
**Teorema 2.1:** Se o processo $(Y_t, X_t)$ √© estacion√°rio e erg√≥dico, e $E(X_tX_t')$ √© n√£o singular, ent√£o a matriz de covari√¢ncia do estimador OLS converge para a matriz de covari√¢ncia assint√≥tica da proje√ß√£o linear.
*Prova:*
I. Sabemos que o estimador OLS √© dado por $b = \left(\sum_{t=1}^{T} x_tx_t'\right)^{-1} \sum_{t=1}^{T} x_ty_{t+1}$.
II. Da mesma forma, o estimador da proje√ß√£o linear √© dado por $\alpha' = E(Y_{t+1}X_t')[E(X_tX_t')]^{-1}$.
III. Substituindo $y_{t+1}$ por $\alpha'x_t + u_t$, onde $u_t$ √© o res√≠duo da proje√ß√£o linear, temos
$b = \left(\sum_{t=1}^{T} x_tx_t'\right)^{-1} \sum_{t=1}^{T} x_t(\alpha'x_t + u_t) = \alpha' + \left(\sum_{t=1}^{T} x_tx_t'\right)^{-1} \sum_{t=1}^{T} x_tu_t$.
IV. Portanto, $b - \alpha' = \left(\frac{1}{T}\sum_{t=1}^{T} x_tx_t'\right)^{-1} \frac{1}{T}\sum_{t=1}^{T} x_tu_t$.
V. A matriz de covari√¢ncia amostral de $b$ √© dada por $\text{Var}(b) = E[(b-E(b))(b-E(b))']$.
VI. Quando $T \rightarrow \infty$, $\frac{1}{T}\sum_{t=1}^{T} x_tx_t' \xrightarrow{p} E(X_tX_t')$ e $\frac{1}{T}\sum_{t=1}^{T} x_tu_t \xrightarrow{p} E(X_t U_t) = 0$, pela condi√ß√£o de ortogonalidade.
VII. A vari√¢ncia assint√≥tica de $b$ √© dada por
$\text{Avar}(b) =  E(X_tX_t')^{-1} E(X_t U_t^2 X_t')E(X_tX_t')^{-1} = \frac{1}{T} (E(X_tX_t'))^{-1} E(X_tX_t' \sigma_u^2) (E(X_tX_t'))^{-1}$ onde $\sigma_u^2$ √© a vari√¢ncia do erro da proje√ß√£o.
VIII. Como os momentos amostrais convergem para os momentos populacionais, a matriz de covari√¢ncia do estimador OLS converge para a matriz de covari√¢ncia da proje√ß√£o linear quando $T\rightarrow \infty$.
‚ñ†
> üí° **Exemplo Num√©rico:**
>
> Retomando o exemplo anterior, vamos verificar a converg√™ncia do estimador OLS para o estimador da proje√ß√£o linear com um conjunto maior de dados.
>
> Suponha que tenhamos um conjunto de dados simulados com 1000 observa√ß√µes geradas usando a rela√ß√£o: $y_{t+1} = 0.342x_{1t} + 0.628x_{2t} + u_t$ onde $x_{1t}$ e $x_{2t}$ s√£o vari√°veis independentes normalmente distribu√≠das e $u_t$ √© um termo de erro com distribui√ß√£o normal com m√©dia zero e desvio padr√£o 0.5.
>
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
>
> np.random.seed(42)
> n_obs = 1000
> x1 = np.random.normal(0, 1, n_obs)
> x2 = np.random.normal(0, 1, n_obs)
> u = np.random.normal(0, 0.5, n_obs)
>
> y = 0.342*x1 + 0.628*x2 + u
>
> X = pd.DataFrame({'x1': x1, 'x2': x2})
>
> model = LinearRegression()
> model.fit(X, y)
>
> print(f"Coeficientes OLS: {model.coef_}")
> print(f"Intercepto OLS: {model.intercept_}")
> ```
>
> A sa√≠da do c√≥digo ser√° algo pr√≥ximo de:
>
> ```
> Coeficientes OLS: [0.3392, 0.6295]
> Intercepto OLS: -0.0013
> ```
>
> Note que os coeficientes estimados pelo OLS (0.3392 e 0.6295) est√£o muito pr√≥ximos dos valores usados na simula√ß√£o (0.342 e 0.628), que s√£o os coeficientes da proje√ß√£o linear. Isso ilustra a converg√™ncia do estimador OLS para o estimador da proje√ß√£o linear em amostras grandes.
>
> Al√©m disso, podemos calcular o MSE da regress√£o OLS:
> ```python
> from sklearn.metrics import mean_squared_error
> y_pred = model.predict(X)
> mse = mean_squared_error(y, y_pred)
> print(f"MSE OLS: {mse}")
> ```
>
> O MSE obtido no c√≥digo acima ser√° algo pr√≥ximo de 0.249, que √© pr√≥ximo do MSE esperado para a proje√ß√£o linear quando o tamanho da amostra √© grande.

Este teorema estabelece formalmente a liga√ß√£o entre o processo de minimiza√ß√£o de res√≠duos na regress√£o OLS e a minimiza√ß√£o do erro quadr√°tico m√©dio na proje√ß√£o linear. A regress√£o OLS, ao minimizar a soma dos quadrados dos res√≠duos, est√° efetivamente aproximando a solu√ß√£o para o problema da proje√ß√£o linear quando o tamanho da amostra tende ao infinito.

**Observa√ß√£o:** Embora a regress√£o OLS seja uma ferramenta poderosa para estimar coeficientes de proje√ß√£o linear, √© crucial reconhecer suas limita√ß√µes. Em particular, o resultado de que os res√≠duos de OLS s√£o ortogonais aos regressores, demonstrado pela Proposi√ß√£o 1, implica que qualquer componente da rela√ß√£o entre as vari√°veis que n√£o seja capturada pelo modelo linear (e esteja presente no termo de erro), n√£o pode estar correlacionada com os regressores $X$. Isso requer suposi√ß√µes fortes sobre a rela√ß√£o entre os regressores e o termo de erro, e a viola√ß√£o dessas premissas pode levar a estimativas viesadas, mesmo assintoticamente.

> üí° **Exemplo Num√©rico:**
>
> Vamos usar o exemplo anterior onde encontramos $b = [0.3245, 0.6945]'$ como o estimador OLS. Usando os mesmos momentos populacionais dos exemplos anteriores podemos calcular o MSE da proje√ß√£o linear:
>
> $$ E[(Y_{t+1} - \alpha'X_t)^2] = E[Y_{t+1}^2] - \alpha'E(Y_{t+1}X_t') = E[Y_{t+1}^2] - \begin{bmatrix}0.342 & 0.628\end{bmatrix}\begin{bmatrix} 1 \\ 0.8 \end{bmatrix} $$
>
> Assumindo que $E[Y_{t+1}^2]=2$, o MSE seria:
>
> $$ MSE = 2 - (0.342 + 0.628*0.8) = 2 - 0.342 - 0.5024 = 1.1556$$
>
>
> Agora, usando o estimador OLS $b$ e assumindo que a amostra utilizada no c√°lculo √© representativa da popula√ß√£o (o que faria com que os momentos amostrais fossem pr√≥ximos aos populacionais), podemos calcular uma aproxima√ß√£o para o MSE usando a soma dos res√≠duos quadrados:
>
> $$\text{MSE}_{\text{OLS}} \approx \frac{1}{T}\sum_{t=1}^T (y_{t+1} - b'x_t)^2$$
>
> O valor que o OLS ir√° obter para o $\text{MSE}_{\text{OLS}}$ √© aproximadamente o mesmo que o MSE da proje√ß√£o linear quando $T\rightarrow \infty$ .

### Conclus√£o

Este cap√≠tulo demonstrou que a minimiza√ß√£o da soma dos quadrados dos res√≠duos na regress√£o OLS est√° diretamente ligada √† minimiza√ß√£o do MSE na proje√ß√£o linear. Quando as condi√ß√µes de estacionariedade e ergodicidade s√£o satisfeitas, o estimador OLS $b$ converge para o coeficiente $\alpha$ da proje√ß√£o linear, proporcionando uma base s√≥lida para a utiliza√ß√£o da regress√£o OLS para a estima√ß√£o dos coeficientes da proje√ß√£o linear. Al√©m disso, formalizamos a rela√ß√£o entre a minimiza√ß√£o do MSE e a ortogonalidade dos res√≠duos, refor√ßando a liga√ß√£o entre as duas abordagens. Assim, conclu√≠mos que a regress√£o OLS √© um m√©todo eficiente para estimar os par√¢metros de uma proje√ß√£o linear, principalmente quando o tamanho da amostra √© grande, ou quando, sob determinadas condi√ß√µes, os momentos da amostra se aproximam dos momentos populacionais. No entanto, √© importante manter presente as limita√ß√µes associadas a modelos lineares, que podem gerar estimadores viesados caso as condi√ß√µes necess√°rias n√£o sejam satisfeitas.

### Refer√™ncias

[^4.1.1]:  Express√£o [4.1.1] √© conhecida como o erro quadr√°tico m√©dio associado √† previs√£o $Y_{t+1|t}$, denotado  $MSE(Y_{t+1|t}) = E(Y_{t+1} - Y_{t+1|t})^2$.
[^4.1.9]: Agora restringimos a classe de previs√µes consideradas exigindo que a previs√£o $Y_{t+1|t}^*$ seja uma fun√ß√£o linear de $X_t$: $Y_{t+1|t}^* = \alpha'X_t$.
[^4.1.13]:  $\alpha' = E(Y_{t+1}X_t')[E(X_tX_t')]^{-1}$, assumindo que $E(X_tX_t')$ √© uma matriz n√£o singular.
[^4.1.17]: Dado um conjunto de $T$ observa√ß√µes em $y$ e $x$, a soma da amostra dos res√≠duos quadrados √© definida como $\sum_{t=1}^T (y_{t+1} - \beta'x_t)^2$.
[^4.1.18]:  O valor de $\beta$ que minimiza [4.1.17], denotado por $b$, √© a estimativa de m√≠nimos quadrados ordin√°rios (OLS) de $\beta$.  A f√≥rmula para $b$ √© $b = \left(\sum_{t=1}^{T} x_tx_t'\right)^{-1} \sum_{t=1}^{T} x_ty_{t+1}$.
[^4.1.20]: Assim, a regress√£o OLS de $y_{t+1}$ em $x_t$ produz uma estimativa consistente do coeficiente da proje√ß√£o linear. Observe que este resultado requer apenas que o processo seja erg√≥dico para segundos momentos. Em contraste, a an√°lise econom√©trica estrutural requer suposi√ß√µes muito mais fortes sobre a rela√ß√£o entre $X$ e $Y$.
[^4.A]: O ap√™ndice 4.A deste cap√≠tulo discute este paralelo e mostra como as f√≥rmulas para uma regress√£o OLS podem ser vistas como um caso especial das f√≥rmulas para uma proje√ß√£o linear.
<!-- END -->
