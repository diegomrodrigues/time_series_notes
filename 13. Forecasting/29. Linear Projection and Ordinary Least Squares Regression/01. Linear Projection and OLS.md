## Proje√ß√£o Linear e Regress√£o de M√≠nimos Quadrados Ordin√°rios: Uma Perspectiva Unificada

### Introdu√ß√£o
Neste cap√≠tulo, exploramos a rela√ß√£o intr√≠nseca entre a **proje√ß√£o linear** e a **regress√£o de m√≠nimos quadrados ordin√°rios (OLS)**. Como vimos anteriormente [^4.1.9], a proje√ß√£o linear busca encontrar a melhor aproxima√ß√£o linear de uma vari√°vel aleat√≥ria em fun√ß√£o de outra, minimizando o erro quadr√°tico m√©dio. A regress√£o OLS, por outro lado, visa ajustar um modelo linear aos dados observados, minimizando a soma dos quadrados dos res√≠duos [^4.1.17]. Embora esses dois m√©todos pare√ßam distintos √† primeira vista, eles compartilham princ√≠pios fundamentais e podem ser entendidos como casos especiais um do outro. Este cap√≠tulo visa elucidar essa conex√£o, mostrando como a regress√£o OLS pode ser vista como uma aplica√ß√£o da proje√ß√£o linear, particularmente quando os momentos da amostra convergem para os momentos da popula√ß√£o [^4.1.20].

### Conceitos Fundamentais
A **proje√ß√£o linear** de $Y_{t+1}$ em $X_t$ √© definida como a fun√ß√£o linear $\alpha'X_t$ que minimiza o erro quadr√°tico m√©dio $E[(Y_{t+1} - \alpha'X_t)^2]$ [^4.1.1]. A solu√ß√£o para $\alpha$ √© dada por $\alpha' = E(Y_{t+1}X_t') [E(X_tX_t')]^{-1}$ [^4.1.13], assumindo que $E(X_tX_t')$ √© n√£o singular.

**Observa√ß√£o:** √â importante ressaltar que a matriz $E(X_tX_t')$ representa a matriz de covari√¢ncia das vari√°veis em $X_t$, e sua n√£o singularidade √© crucial para a exist√™ncia e unicidade da solu√ß√£o para $\alpha$. Esta condi√ß√£o implica que as vari√°veis em $X_t$ n√£o podem ser perfeitamente linearmente dependentes.

> üí° **Exemplo Num√©rico:** Vamos considerar um cen√°rio onde $X_t$ √© um vetor de duas vari√°veis, $X_t = [X_{1t}, X_{2t}]'$, e $Y_{t+1}$ √© a vari√°vel dependente. Suponha que a matriz de covari√¢ncia $E(X_tX_t')$ e o vetor de covari√¢ncia $E(Y_{t+1}X_t')$ s√£o dados por:
> $$
E(X_tX_t') = \begin{bmatrix} 2 & 0.5 \\ 0.5 & 1 \end{bmatrix}, \quad E(Y_{t+1}X_t') = \begin{bmatrix} 1 \\ 0.8 \end{bmatrix}
$$
>
> Para calcular $\alpha$, precisamos calcular a inversa de $E(X_tX_t')$:
>
> $\text{det}(E(X_tX_t')) = (2)(1) - (0.5)(0.5) = 2 - 0.25 = 1.75$
>
> $$
[E(X_tX_t')]^{-1} = \frac{1}{1.75} \begin{bmatrix} 1 & -0.5 \\ -0.5 & 2 \end{bmatrix} \approx \begin{bmatrix} 0.571 & -0.286 \\ -0.286 & 1.143 \end{bmatrix}
$$
>
> Agora, podemos calcular $\alpha$:
>
> $$
\alpha = [E(X_tX_t')]^{-1} E(Y_{t+1}X_t') =  \begin{bmatrix} 0.571 & -0.286 \\ -0.286 & 1.143 \end{bmatrix} \begin{bmatrix} 1 \\ 0.8 \end{bmatrix} = \begin{bmatrix} 0.571 - 0.229 \\ -0.286 + 0.914 \end{bmatrix} = \begin{bmatrix} 0.342 \\ 0.628 \end{bmatrix}
$$
>
> Portanto, a proje√ß√£o linear de $Y_{t+1}$ em $X_t$ √© aproximadamente $0.342X_{1t} + 0.628X_{2t}$.

Em contraste, a **regress√£o OLS** busca encontrar o vetor $\beta$ que minimiza a soma dos quadrados dos res√≠duos em um modelo linear $y_{t+1} = \beta'x_t + u_t$ [^4.1.16]. A solu√ß√£o para $\beta$, denotada por $b$, √© dada por $b = \left(\sum_{t=1}^{T} x_tx_t'\right)^{-1} \sum_{t=1}^{T} x_ty_{t+1}$ [^4.1.18].

> üí° **Exemplo Num√©rico:** Continuando o exemplo anterior, vamos supor que temos dados amostrais para $T=100$. As matrizes $\sum_{t=1}^{T} x_tx_t'$ e $\sum_{t=1}^{T} x_ty_{t+1}$ foram calculadas a partir dos dados:
>
> $$
\sum_{t=1}^{T} x_tx_t' = \begin{bmatrix} 210 & 55 \\ 55 & 98 \end{bmatrix}, \quad \sum_{t=1}^{T} x_ty_{t+1} = \begin{bmatrix} 105 \\ 85 \end{bmatrix}
$$
>
> Calculamos o estimador OLS $b$:
>
> $\text{det}(\sum_{t=1}^{T} x_tx_t') = (210)(98) - (55)(55) = 20580 - 3025 = 17555$
>
> $$
(\sum_{t=1}^{T} x_tx_t')^{-1} = \frac{1}{17555} \begin{bmatrix} 98 & -55 \\ -55 & 210 \end{bmatrix} \approx  \begin{bmatrix} 0.0056 & -0.0031 \\ -0.0031 & 0.0120 \end{bmatrix}
$$
>
> $$
b = (\sum_{t=1}^{T} x_tx_t')^{-1} \sum_{t=1}^{T} x_ty_{t+1} = \begin{bmatrix} 0.0056 & -0.0031 \\ -0.0031 & 0.0120 \end{bmatrix} \begin{bmatrix} 105 \\ 85 \end{bmatrix} = \begin{bmatrix} 0.588 - 0.2635 \\ -0.3255 + 1.02 \end{bmatrix} = \begin{bmatrix} 0.3245 \\ 0.6945 \end{bmatrix}
$$
>
> O estimador OLS $b$ √© aproximadamente $0.3245x_{1t} + 0.6945x_{2t}$. Observe como os resultados da regress√£o OLS, com dados amostrais, se aproximam dos resultados da proje√ß√£o linear, que utilizam momentos populacionais.

A conex√£o entre esses dois m√©todos se torna clara quando observamos que a proje√ß√£o linear se baseia nos momentos da *popula√ß√£o* ($E(X_tX_t')$ e $E(Y_{t+1}X_t')$), enquanto a regress√£o OLS usa os momentos da *amostra* ($\frac{1}{T}\sum_{t=1}^{T}x_tx_t'$ e $\frac{1}{T}\sum_{t=1}^{T}x_ty_{t+1}$). No entanto, sob condi√ß√µes de *covari√¢ncia-estacionariedade* e *ergodicidade* [^4.1.20], os momentos da amostra convergem para os momentos da popula√ß√£o quando o tamanho da amostra $T$ tende ao infinito:

$$
\frac{1}{T}\sum_{t=1}^{T}x_tx_t' \xrightarrow{p} E(X_tX_t')
$$
$$
\frac{1}{T}\sum_{t=1}^{T}x_ty_{t+1} \xrightarrow{p} E(Y_{t+1}X_t')
$$
onde $\xrightarrow{p}$ denota converg√™ncia em probabilidade.

**Lema 1:** Sob as condi√ß√µes de covari√¢ncia-estacionariedade e ergodicidade, a matriz $\frac{1}{T}\sum_{t=1}^{T}x_tx_t'$ converge em probabilidade para uma matriz n√£o singular, desde que $E(X_tX_t')$ seja n√£o singular.

*Prova:*
I. A n√£o singularidade de $E(X_tX_t')$ implica que seus autovalores s√£o todos positivos.
II.  Pela converg√™ncia em probabilidade, para $T$ suficientemente grande, os autovalores de $\frac{1}{T}\sum_{t=1}^{T}x_tx_t'$ ser√£o arbitrariamente pr√≥ximos dos autovalores de $E(X_tX_t')$.
III. Assim, $\frac{1}{T}\sum_{t=1}^{T}x_tx_t'$ tamb√©m ser√° n√£o singular para $T$ grande o suficiente.
‚ñ†

Isso implica que, conforme $T \rightarrow \infty$, o estimador OLS $b$ converge para o coeficiente da proje√ß√£o linear $\alpha$ [^4.1.20]:

$$
b \xrightarrow{p} \alpha
$$
Este resultado fundamental estabelece que a regress√£o OLS pode ser vista como uma forma de estimar o coeficiente da proje√ß√£o linear.

**Teorema 1:** Se $E(X_tX_t')$ for n√£o singular e as condi√ß√µes de covari√¢ncia-estacionariedade e ergodicidade forem satisfeitas, ent√£o o estimador OLS $b$ √© um estimador consistente para $\alpha$.

*Prova:*
I. Pelo Lema 1, $\left(\frac{1}{T}\sum_{t=1}^{T}x_tx_t'\right)^{-1} \xrightarrow{p} [E(X_tX_t')]^{-1}$.
II.  Sabemos que $\frac{1}{T}\sum_{t=1}^{T}x_ty_{t+1} \xrightarrow{p} E(Y_{t+1}X_t')$.
III. Usando a propriedade da converg√™ncia em probabilidade, o produto de duas sequ√™ncias convergentes em probabilidade tamb√©m converge em probabilidade para o produto dos seus limites.
IV. Portanto,  $b = \left(\sum_{t=1}^{T} x_tx_t'\right)^{-1} \sum_{t=1}^{T} x_ty_{t+1} \xrightarrow{p} [E(X_tX_t')]^{-1}E(Y_{t+1}X_t')=\alpha$.
‚ñ†

A rela√ß√£o entre os dois m√©todos √© ainda mais profunda, conforme explorado no Ap√™ndice 4.A [^4.A], onde √© demonstrado que a regress√£o OLS pode ser interpretada como um caso especial da proje√ß√£o linear, utilizando vari√°veis aleat√≥rias artificiais constru√≠das para ter momentos populacionais que coincidem com os momentos amostrais do problema de regress√£o. Isto √©, ao criar uma vari√°vel aleat√≥ria $\xi$ que toma os valores $x_t$ com probabilidade $\frac{1}{T}$, e $\omega$ que toma os valores $y_{t+1}$ com a mesma probabilidade, os momentos populacionais de $\xi$ e $\omega$ coincidem com os momentos amostrais dos dados. Assim, o problema de minimizar $E[(\omega - \alpha' \xi)^2]$ corresponde ao problema de minimizar a soma dos quadrados dos res√≠duos, revelando uma unidade estrutural entre os dois m√©todos.

**Observa√ß√£o Importante:** A discuss√£o acima enfatiza que a proje√ß√£o linear foca nos momentos da popula√ß√£o e que a regress√£o OLS, por sua vez, se baseia nos momentos da amostra. No entanto, sob condi√ß√µes de estacionariedade e ergodicidade, os momentos amostrais convergem para os momentos populacionais, fornecendo uma base s√≥lida para a regress√£o OLS como uma maneira de estimar os coeficientes de proje√ß√£o linear [^4.1.20].

**Proposi√ß√£o 1:** A consist√™ncia do estimador OLS tamb√©m pode ser avaliada por meio do conceito de ortogonalidade dos res√≠duos. Se os res√≠duos estimados $u_t = y_{t+1} - b'x_t$ forem assintoticamente n√£o correlacionados com $x_t$ (i.e., $\frac{1}{T}\sum_{t=1}^T u_t x_t' \xrightarrow{p} 0$), ent√£o o estimador $b$ converge em probabilidade para o coeficiente da proje√ß√£o linear $\alpha$.

> üí° **Exemplo Num√©rico:** Vamos usar os dados do exemplo anterior com $b = [0.3245, 0.6945]'$. Considere um ponto de dado $x_t = [2, 1]'$ e $y_{t+1} = 1.5$. O res√≠duo √© $u_t = y_{t+1} - b'x_t = 1.5 - (0.3245 * 2 + 0.6945 * 1) = 1.5 - 0.649 - 0.6945 = 0.1565$.
>
> Para verificar a ortogonalidade, precisamos calcular a soma de $\frac{1}{T}\sum_{t=1}^T u_t x_t'$:
>
> Dado que o estimador OLS busca minimizar a soma dos quadrados dos res√≠duos, o termo $\frac{1}{T}\sum_{t=1}^T u_t x_t'$ √© igual a zero. De forma mais geral, para uma amostra com $T$ dados, ter√≠amos:
> $$
\frac{1}{T}\sum_{t=1}^T u_t x_t' = \frac{1}{T}\sum_{t=1}^T (y_{t+1} - b'x_t)x_t' \approx 0
$$
> Isso demonstra que os res√≠duos s√£o ortogonais aos regressores, um requisito fundamental para a consist√™ncia do OLS.

*Prova:*
I. Se $\frac{1}{T}\sum_{t=1}^T u_t x_t' \xrightarrow{p} 0$,  ent√£o $\frac{1}{T}\sum_{t=1}^T (y_{t+1} - b'x_t)x_t' \xrightarrow{p} 0$.
II. Isso implica que $\frac{1}{T}\sum_{t=1}^T y_{t+1}x_t' - b' \frac{1}{T}\sum_{t=1}^T x_tx_t' \xrightarrow{p} 0$.
III.  Rearranjando temos que $b' \frac{1}{T}\sum_{t=1}^T x_tx_t' \xrightarrow{p} \frac{1}{T}\sum_{t=1}^T y_{t+1}x_t'$.
IV.  Como $\frac{1}{T}\sum_{t=1}^T x_tx_t' \xrightarrow{p} E(X_tX_t')$ e $\frac{1}{T}\sum_{t=1}^T y_{t+1}x_t' \xrightarrow{p} E(Y_{t+1}X_t')$, e utilizando a converg√™ncia em probabilidade de matrizes e seus inversos, temos que $b' \xrightarrow{p} E(Y_{t+1}X_t')[E(X_tX_t')]^{-1} = \alpha'$.
‚ñ†

### Conclus√£o
A **proje√ß√£o linear** e a **regress√£o de m√≠nimos quadrados ordin√°rios** s√£o m√©todos intimamente relacionados, ambos buscando a melhor aproxima√ß√£o linear em fun√ß√£o de um crit√©rio de minimiza√ß√£o de erro. Enquanto a proje√ß√£o linear se baseia nos momentos da popula√ß√£o, a regress√£o OLS utiliza os momentos da amostra. No entanto, sob condi√ß√µes apropriadas, os momentos da amostra convergem para os momentos da popula√ß√£o, estabelecendo uma liga√ß√£o te√≥rica profunda entre os dois m√©todos. Essa conex√£o permite uma interpreta√ß√£o unificada e promove a compreens√£o de como a regress√£o OLS pode ser vista como uma aplica√ß√£o da proje√ß√£o linear para dados amostrais. O Ap√™ndice 4.A formaliza ainda mais esta liga√ß√£o, mostrando que os dois m√©todos compartilham uma base matem√°tica comum na minimiza√ß√£o de erros quadr√°ticos.

### Refer√™ncias
[^4.1.1]:  Express√£o [4.1.1] √© conhecida como o erro quadr√°tico m√©dio associado √† previs√£o $Y_{t+1|t}$, denotado  $MSE(Y_{t+1|t}) = E(Y_{t+1} - Y_{t+1|t})^2$.

[^4.1.9]: Agora restringimos a classe de previs√µes consideradas exigindo que a previs√£o $Y_{t+1|t}^*$ seja uma fun√ß√£o linear de $X_t$: $Y_{t+1|t}^* = \alpha'X_t$.

[^4.1.13]:  $\alpha' = E(Y_{t+1}X_t')[E(X_tX_t')]^{-1}$, assumindo que $E(X_tX_t')$ √© uma matriz n√£o singular.

[^4.1.17]: Dado um conjunto de $T$ observa√ß√µes em $y$ e $x$, a soma da amostra dos res√≠duos quadrados √© definida como $\sum_{t=1}^T (y_{t+1} - \beta'x_t)^2$.

[^4.1.18]:  O valor de $\beta$ que minimiza [4.1.17], denotado por $b$, √© a estimativa de m√≠nimos quadrados ordin√°rios (OLS) de $\beta$.  A f√≥rmula para $b$ √© $b = \left(\sum_{t=1}^{T} x_tx_t'\right)^{-1} \sum_{t=1}^{T} x_ty_{t+1}$.

[^4.1.20]: Assim, a regress√£o OLS de $y_{t+1}$ em $x_t$ produz uma estimativa consistente do coeficiente da proje√ß√£o linear. Observe que este resultado requer apenas que o processo seja erg√≥dico para segundos momentos. Em contraste, a an√°lise econom√©trica estrutural requer suposi√ß√µes muito mais fortes sobre a rela√ß√£o entre $X$ e $Y$.

[^4.A]: O ap√™ndice 4.A deste cap√≠tulo discute este paralelo e mostra como as f√≥rmulas para uma regress√£o OLS podem ser vistas como um caso especial das f√≥rmulas para uma proje√ß√£o linear.
<!-- END -->
