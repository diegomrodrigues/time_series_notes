## A F√≥rmula do Estimador OLS: Implica√ß√µes e Computa√ß√£o

### Introdu√ß√£o
Como vimos nos cap√≠tulos anteriores [^4.1.18], o estimador de m√≠nimos quadrados ordin√°rios (OLS), denotado por $b$, desempenha um papel central na regress√£o linear. Este estimador √© obtido atrav√©s da minimiza√ß√£o da soma dos quadrados dos res√≠duos, e sua f√≥rmula, $b = \left(\sum_{t=1}^{T} x_tx_t'\right)^{-1} \sum_{t=1}^{T} x_ty_{t+1}$, envolve opera√ß√µes matem√°ticas cruciais, como a invers√£o de matrizes e produtos vetoriais. Este cap√≠tulo tem como objetivo explorar em detalhes a f√≥rmula do estimador OLS, suas implica√ß√µes te√≥ricas e sua import√¢ncia em aplica√ß√µes computacionais de regress√£o linear. Analisaremos a estrutura matem√°tica da f√≥rmula, os requisitos para sua aplica√ß√£o, e os algoritmos computacionais utilizados para sua implementa√ß√£o eficiente.

### Decompondo a F√≥rmula do Estimador OLS
A f√≥rmula do estimador OLS $b$, expressa como:
$$ b = \left(\sum_{t=1}^{T} x_tx_t'\right)^{-1} \sum_{t=1}^{T} x_ty_{t+1} $$
pode ser dividida em tr√™s componentes principais para uma melhor compreens√£o:
1. **Matriz de Covari√¢ncia Amostral dos Regressores:** $\sum_{t=1}^{T} x_tx_t'$ representa a soma dos produtos externos dos vetores de regressores $x_t$ ao longo do tempo. Esta matriz √© de dimens√£o $(k \times k)$, onde $k$ √© o n√∫mero de regressores, e captura as rela√ß√µes de covari√¢ncia entre eles.
2. **Inversa da Matriz de Covari√¢ncia Amostral:** $\left(\sum_{t=1}^{T} x_tx_t'\right)^{-1}$ denota a inversa da matriz de covari√¢ncia amostral dos regressores. A exist√™ncia desta inversa √© crucial para a aplica√ß√£o da f√≥rmula do estimador OLS e requer que a matriz $\sum_{t=1}^{T} x_tx_t'$ seja n√£o singular.
   **Lema 1:** Se os regressores em $x_t$ forem linearmente independentes, ent√£o $\sum_{t=1}^T x_tx_t'$ √© n√£o singular.
    *Prova:*
    I. $\sum_{t=1}^T x_tx_t'$ √© uma matriz sim√©trica. Se os regressores em $x_t$ s√£o linearmente independentes, isso significa que n√£o existe nenhum vetor n√£o nulo $c$ tal que $c'x_t=0$ para todo $t$.
    II. Se $\sum_{t=1}^T x_tx_t'$ fosse singular, existiria um vetor n√£o nulo $v$ tal que $v'\left(\sum_{t=1}^T x_tx_t'\right)v=0$. Isso implica que $\sum_{t=1}^T (v'x_t)^2 = 0$, que significa que $v'x_t=0$ para todo $t$.
    III. Isso contraria a suposi√ß√£o de que os regressores s√£o linearmente independentes, e portanto,  $\sum_{t=1}^T x_tx_t'$ √© n√£o singular.
   ‚ñ†
    **Lema 1.1:** Uma condi√ß√£o equivalente para a n√£o singularidade de $\sum_{t=1}^T x_tx_t'$ √© que a matriz de regressores $X = [x_1, x_2, \ldots, x_T]'$ tenha posto coluna completo, isto √©, posto igual a $k$, onde $k$ √© o n√∫mero de regressores.
     *Prova:*
     I. A matriz $\sum_{t=1}^T x_tx_t'$ pode ser reescrita como $X'X$, onde $X$ √© a matriz de regressores.
     II.  A matriz $X'X$ √© n√£o singular se e somente se a matriz $X$ tiver posto coluna completo. Isso ocorre quando todas as colunas de $X$ s√£o linearmente independentes.
     III. Se as colunas de $X$ s√£o linearmente independentes, ent√£o as colunas de $X$ formam uma base para o subespa√ßo gerado pelos regressores, e, portanto, o posto de $X$ √© igual ao n√∫mero de regressores $k$.
     IV. Se o posto de $X$ √© $k$, ent√£o $X'X$ √© n√£o singular, o que implica que $\sum_{t=1}^T x_tx_t'$ √© n√£o singular.
   ‚ñ†
3. **Produto Vetorial entre Regressores e Vari√°vel Dependente:** $\sum_{t=1}^{T} x_ty_{t+1}$ representa a soma dos produtos entre os vetores de regressores $x_t$ e a vari√°vel dependente $y_{t+1}$ ao longo do tempo. Este vetor de dimens√£o $(k \times 1)$ captura as rela√ß√µes de covari√¢ncia entre a vari√°vel dependente e cada um dos regressores.

**Observa√ß√£o 1:** Para que o estimador OLS $b$ seja bem definido, a matriz $\sum_{t=1}^{T} x_tx_t'$ deve ser invers√≠vel, ou seja, n√£o singular. Esta condi√ß√£o √© satisfeita se os regressores em $x_t$ forem linearmente independentes. Quando essa condi√ß√£o n√£o se verifica, o problema de regress√£o n√£o tem uma solu√ß√£o √∫nica para $b$.

> üí° **Exemplo Num√©rico:**
>
> Considere um modelo de regress√£o linear simples com um intercepto, onde $y_{t+1}$ √© a vari√°vel dependente e $x_t$ √© o √∫nico regressor. Adicionamos uma coluna de 1s para o intercepto. Com um conjunto de dados $T=4$, podemos representar $x_t$ e $y_{t+1}$ como vetores:
> $$ X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix} , \quad y = \begin{bmatrix} 2 \\ 4 \\ 5 \\ 4 \end{bmatrix} $$
>
> Primeiro, calculamos $X'X$:
> $$X'X = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix} = \begin{bmatrix} 4 & 10 \\ 10 & 30 \end{bmatrix}$$
>
> Em seguida, calculamos a inversa de $X'X$:
> $$(X'X)^{-1} = \frac{1}{(4)(30) - (10)(10)} \begin{bmatrix} 30 & -10 \\ -10 & 4 \end{bmatrix} = \frac{1}{20} \begin{bmatrix} 30 & -10 \\ -10 & 4 \end{bmatrix} = \begin{bmatrix} 1.5 & -0.5 \\ -0.5 & 0.2 \end{bmatrix}$$
>
> Agora, calculamos $X'y$:
> $$X'y = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 \end{bmatrix} \begin{bmatrix} 2 \\ 4 \\ 5 \\ 4 \end{bmatrix} = \begin{bmatrix} 15 \\ 41 \end{bmatrix}$$
>
> O estimador OLS $b$ √© dado por:
>
> $$ b = (X'X)^{-1} X'y = \begin{bmatrix} 1.5 & -0.5 \\ -0.5 & 0.2 \end{bmatrix} \begin{bmatrix} 15 \\ 41 \end{bmatrix} = \begin{bmatrix} 1.5*15 - 0.5*41 \\ -0.5*15 + 0.2*41 \end{bmatrix} = \begin{bmatrix} 22.5 - 20.5 \\ -7.5 + 8.2 \end{bmatrix} = \begin{bmatrix} 2 \\ 0.7 \end{bmatrix}$$
>
> Neste caso, o estimador $b = [2, 0.7]'$, onde 2 √© o intercepto e 0.7 √© a inclina√ß√£o da reta de regress√£o.
>
> ```python
> import numpy as np
>
> X = np.array([[1, 1], [1, 2], [1, 3], [1, 4]])
> y = np.array([2, 4, 5, 4])
>
> XtX = X.T @ X
> XtX_inv = np.linalg.inv(XtX)
> Xty = X.T @ y
> b = XtX_inv @ Xty
> print(b)
> ```
>
> O c√≥digo acima confirma o resultado:
> ```
> [2.  0.7]
> ```

### Implica√ß√µes Te√≥ricas da F√≥rmula do Estimador OLS

A f√≥rmula do estimador OLS tem v√°rias implica√ß√µes te√≥ricas importantes:
1. **Minimiza√ß√£o da Soma dos Quadrados dos Res√≠duos:** A f√≥rmula para $b$ √© derivada da condi√ß√£o de primeira ordem da minimiza√ß√£o da soma dos quadrados dos res√≠duos, como demonstrado em [^4.1.18]. Isso garante que o estimador OLS fornece os coeficientes que melhor ajustam o modelo linear aos dados, no sentido de minimizar o erro total quadr√°tico.
2. **Ortogonalidade dos Res√≠duos:** Como visto anteriormente [^Lema 1.1 no capitulo anterior], o estimador OLS satisfaz a condi√ß√£o de ortogonalidade dos res√≠duos, i.e., $\sum_{t=1}^{T} (y_{t+1} - b'x_t)x_t' = 0$. Isto significa que os res√≠duos da regress√£o s√£o n√£o correlacionados com os regressores, indicando que a parte linear da rela√ß√£o entre as vari√°veis foi capturada pelo modelo.
   **Lema 2:**  Se o modelo inclui um intercepto, ent√£o os res√≠duos t√™m m√©dia amostral zero.
   *Prova:*
   I. O modelo de regress√£o com intercepto pode ser escrito como $y_{t+1} = b_0 + b'x_t + u_t$, onde $b_0$ √© o intercepto.
   II.  O estimador OLS $b$ √© obtido da minimiza√ß√£o da soma dos quadrados dos res√≠duos, que leva √† condi√ß√£o de primeira ordem $\sum_{t=1}^{T} (y_{t+1} - b_0 - b'x_t)x_t' = 0$.
   III.  Se adicionarmos uma coluna de 1's ao vetor de regressores, ent√£o a primeira linha dessa condi√ß√£o de primeira ordem seria $\sum_{t=1}^{T} (y_{t+1} - b_0 - b'x_t) = 0$, ou seja, $\sum_{t=1}^{T} u_t = 0$, o que implica que $\frac{1}{T}\sum_{t=1}^{T} u_t = 0$.
   IV. Isso significa que a m√©dia amostral dos res√≠duos √© zero.
  ‚ñ†
   **Lema 2.1:** A condi√ß√£o de ortogonalidade dos res√≠duos $\sum_{t=1}^{T} (y_{t+1} - b'x_t)x_t' = 0$ implica que o vetor de res√≠duos $u = y - Xb$ √© ortogonal ao espa√ßo coluna da matriz de regressores $X$.
    *Prova:*
    I. A condi√ß√£o de ortogonalidade pode ser escrita em forma matricial como $X'(y - Xb) = 0$.
    II.  O termo $Xb$ representa a proje√ß√£o de $y$ no espa√ßo coluna de $X$.
    III. Portanto, o vetor de res√≠duos $u = y - Xb$ √© ortogonal a qualquer vetor no espa√ßo coluna de $X$.
    IV. Isso significa que os res√≠duos s√£o n√£o correlacionados com qualquer combina√ß√£o linear dos regressores.
   ‚ñ†
> üí° **Exemplo Num√©rico:**
>
> Usando os dados do exemplo anterior, podemos calcular os res√≠duos e verificar sua ortogonalidade aos regressores:
>
> Os valores preditos $\hat{y}$ s√£o:
> $$\hat{y} = Xb = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix} \begin{bmatrix} 2 \\ 0.7 \end{bmatrix} = \begin{bmatrix} 2.7 \\ 3.4 \\ 4.1 \\ 4.8 \end{bmatrix}$$
>
> Os res√≠duos s√£o:
> $$u = y - \hat{y} = \begin{bmatrix} 2 \\ 4 \\ 5 \\ 4 \end{bmatrix} - \begin{bmatrix} 2.7 \\ 3.4 \\ 4.1 \\ 4.8 \end{bmatrix} = \begin{bmatrix} -0.7 \\ 0.6 \\ 0.9 \\ -0.8 \end{bmatrix}$$
>
> A soma dos res√≠duos √©: $-0.7 + 0.6 + 0.9 - 0.8 = 0$. Portanto, a m√©dia amostral dos res√≠duos √© zero.
>
> Vamos verificar a ortogonalidade:
> $$X'u = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 \end{bmatrix} \begin{bmatrix} -0.7 \\ 0.6 \\ 0.9 \\ -0.8 \end{bmatrix} = \begin{bmatrix} -0.7 + 0.6 + 0.9 - 0.8 \\ -0.7 + 1.2 + 2.7 - 3.2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$$
>
> A ortogonalidade √© confirmada.
> ```python
> import numpy as np
>
> X = np.array([[1, 1], [1, 2], [1, 3], [1, 4]])
> y = np.array([2, 4, 5, 4])
>
> XtX = X.T @ X
> XtX_inv = np.linalg.inv(XtX)
> Xty = X.T @ y
> b = XtX_inv @ Xty
>
> y_hat = X @ b
> u = y - y_hat
>
> print("Res√≠duos:", u)
> print("X'u:", X.T @ u)
> print("M√©dia dos res√≠duos:", np.mean(u))
>
> ```
>
> A sa√≠da do c√≥digo confirma os c√°lculos:
> ```
> Res√≠duos: [-0.7  0.6  0.9 -0.8]
> X'u: [ 0.0000000e+00 -3.5527137e-15]
> M√©dia dos res√≠duos: 0.0
> ```
> Note que o resultado de $X'u$ √© pr√≥ximo de zero, devido a pequenos erros de arredondamento da opera√ß√£o matricial no computador.

3. **Consist√™ncia e Efici√™ncia:** Sob condi√ß√µes de estacionariedade e ergodicidade, o estimador OLS √© consistente, i.e., ele converge em probabilidade para o verdadeiro valor do par√¢metro populacional quando o tamanho da amostra $T$ tende ao infinito [^4.1.20]. Al√©m disso, quando os erros s√£o homoced√°sticos e n√£o correlacionados, o estimador OLS √© o mais eficiente dentro da classe de estimadores lineares e n√£o viesados.
  **Teorema 1:** Se as condi√ß√µes do teorema de Gauss-Markov forem satisfeitas, o estimador OLS √© o melhor estimador linear n√£o viesado.
    *Prova:*
    I. Suponha que temos o modelo $y_{t+1} = \beta'x_t + u_t$, com as condi√ß√µes cl√°ssicas de erros homoced√°sticos e n√£o correlacionados com os regressores.
    II. Seja $b$ o estimador OLS, e $b_a$ um outro estimador linear n√£o viesado qualquer, de forma que $E(b_a) = \beta$.
    III. A vari√¢ncia do estimador $b_a$ pode ser decomposta como  $Var(b_a) = Var(b) + Var(b_a - b)$.
    IV. Como o estimador OLS tem a menor vari√¢ncia dentre os estimadores n√£o viesados, ent√£o $Var(b_a - b) \geq 0$, e portanto $Var(b_a) \geq Var(b)$.
    V. Isto demonstra que o estimador OLS √© o melhor estimador linear n√£o viesado, no sentido de que ele possui a menor vari√¢ncia dentro da classe de estimadores lineares n√£o viesados.
    ‚ñ†
  **Teorema 1.1:** Uma condi√ß√£o suficiente para a consist√™ncia do estimador OLS √© que a matriz $\frac{1}{T}\sum_{t=1}^T x_tx_t'$ convirja para uma matriz n√£o singular e que $\frac{1}{T}\sum_{t=1}^T x_t u_t$ convirja para um vetor zero.
   *Prova:*
    I. A f√≥rmula do estimador OLS √© $b = \left(\sum_{t=1}^{T} x_tx_t'\right)^{-1} \sum_{t=1}^{T} x_ty_{t+1}$.
    II. Substituindo $y_{t+1} = \beta'x_t + u_t$, temos $b = \left(\sum_{t=1}^{T} x_tx_t'\right)^{-1} \sum_{t=1}^{T} x_t(\beta'x_t + u_t) = \beta + \left(\sum_{t=1}^{T} x_tx_t'\right)^{-1} \sum_{t=1}^{T} x_tu_t$.
    III. Multiplicando e dividindo por $T$, obtemos $b = \beta + \left(\frac{1}{T}\sum_{t=1}^{T} x_tx_t'\right)^{-1} \frac{1}{T}\sum_{t=1}^{T} x_tu_t$.
    IV. Para que o estimador OLS $b$ seja consistente, $b$ deve convergir em probabilidade para $\beta$ quando $T$ tende ao infinito. Isso ocorrer√° se o termo $\left(\frac{1}{T}\sum_{t=1}^{T} x_tx_t'\right)^{-1} \frac{1}{T}\sum_{t=1}^{T} x_tu_t$ convergir para zero.
    V. Se $\frac{1}{T}\sum_{t=1}^T x_tx_t'$ converge para uma matriz n√£o singular e $\frac{1}{T}\sum_{t=1}^T x_t u_t$ converge para um vetor zero, ent√£o a express√£o $\left(\frac{1}{T}\sum_{t=1}^{T} x_tx_t'\right)^{-1} \frac{1}{T}\sum_{t=1}^{T} x_tu_t$ converge para zero, e, portanto, $b$ √© um estimador consistente de $\beta$.
    ‚ñ†
### Aplica√ß√µes Computacionais e Implementa√ß√£o da F√≥rmula OLS
Em aplica√ß√µes computacionais, a f√≥rmula do estimador OLS √© frequentemente implementada utilizando algoritmos eficientes de √°lgebra linear. Algumas considera√ß√µes importantes na implementa√ß√£o incluem:
1. **Invers√£o de Matrizes:** O c√°lculo da inversa da matriz $\sum_{t=1}^{T} x_tx_t'$ pode ser computacionalmente custoso para grandes conjuntos de dados e matrizes de alta dimens√£o. Algoritmos como a decomposi√ß√£o LU ou a decomposi√ß√£o de Cholesky s√£o frequentemente utilizados para calcular a inversa de forma mais eficiente.
  **Exemplo Num√©rico:**
  Para exemplificar a decomposi√ß√£o de Cholesky, vamos utilizar a matriz $E(X_t X_t')$ vista no exemplo anterior, dada por:
  $$E(X_tX_t') = \begin{bmatrix} 2 & 0.5 \\ 0.5 & 1 \end{bmatrix}$$
   Esta matriz √© sim√©trica e definida positiva, tornando poss√≠vel a aplica√ß√£o da decomposi√ß√£o de Cholesky $A = LL'$, onde $L$ √© uma matriz triangular inferior.
   Para encontrar $L$, calculamos seus elementos:
    - $L_{11} = \sqrt{A_{11}} = \sqrt{2} \approx 1.414$
    - $L_{21} = \frac{A_{21}}{L_{11}} = \frac{0.5}{\sqrt{2}} \approx 0.354$
    - $L_{22} = \sqrt{A_{22} - L_{21}^2} = \sqrt{1 - (0.354)^2} = \sqrt{1 - 0.125} = \sqrt{0.875} \approx 0.935$
  Dessa forma, obtemos:
   $$L = \begin{bmatrix} 1.414 & 0 \\ 0.354 & 0.935 \end{bmatrix}$$
  E pode-se calcular a inversa da matriz original por meio da inversa de L e L':
  $$A^{-1} = (LL')^{-1} = (L')^{-1}L^{-1}$$
2. **Estabilidade Num√©rica:** Em ambientes de computa√ß√£o, opera√ß√µes envolvendo n√∫meros muito grandes ou muito pequenos podem levar a erros de arredondamento e problemas de instabilidade num√©rica. O uso de algoritmos numericamente est√°veis √© essencial para obter resultados precisos, especialmente em modelos de regress√£o com um grande n√∫mero de regressores ou com dados mal condicionados.
3. **Computa√ß√£o Paralela:** Em alguns casos com grandes conjuntos de dados, √© vantajoso utilizar computa√ß√£o paralela para dividir o c√°lculo da f√≥rmula do estimador OLS em m√∫ltiplas unidades de processamento, acelerando a execu√ß√£o da an√°lise.

> üí° **Exemplo Num√©rico:**
>
> Implementando a regress√£o OLS no Python utilizando o pacote NumPy e Scikit-learn:
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> # Dados de exemplo
> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3], [3, 4]])
> y = np.array([2, 4, 5, 4, 6])
>
> # Adicionar intercepto (coluna de 1s) √† matriz de regressores
> X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)
>
> # C√°lculo do estimador OLS utilizando √°lgebra linear
> XtX = X.T @ X
> XtX_inv = np.linalg.inv(XtX)
> Xty = X.T @ y
> b_manual = XtX_inv @ Xty
>
> # C√°lculo do estimador OLS utilizando Scikit-learn
> model = LinearRegression(fit_intercept=False) # N√£o ajustamos o intercepto no fit, pois j√° o adicionamos na matriz X
> model.fit(X, y)
>
> # Imprimir resultados
> print("Estimador OLS calculado manualmente:")
> print(b_manual)
>
> print("\nEstimador OLS utilizando Scikit-learn:")
> print(model.coef_)
> ```
> A sa√≠da desse c√≥digo demonstra a equival√™ncia entre a implementa√ß√£o manual e o uso da biblioteca Scikit-learn:
>
> ```
> Estimador OLS calculado manualmente:
> [ 0.48305085  0.10169492  1.25423729]
>
> Estimador OLS utilizando Scikit-learn:
> [ 0.48305085  0.10169492  1.25423729]
> ```
 **Proposi√ß√£o 1:** A decomposi√ß√£o QR tamb√©m pode ser usada para resolver o problema de m√≠nimos quadrados e obter o estimador OLS.
    *Prova:*
    I. A decomposi√ß√£o QR de X √© dada por $X = QR$, onde $Q$ √© uma matriz ortogonal e $R$ √© uma matriz triangular superior.
    II. O estimador OLS pode ser expresso como $b = (X'X)^{-1}X'y$.
    III. Substituindo a decomposi√ß√£o QR em $X'X$, temos: $X'X = (QR)'(QR) = R'Q'QR = R'R$, pois $Q'Q = I$, sendo $I$ a matriz identidade.
    IV.  O estimador OLS passa a ser: $b = (R'R)^{-1}R'Q'y = R^{-1}(R')^{-1}R'Q'y = R^{-1}Q'y$.
    V. Portanto, o estimador OLS pode ser calculado atrav√©s da resolu√ß√£o do sistema $Rb = Q'y$.
    ‚ñ†

### Conclus√£o
A f√≥rmula do estimador OLS $b = \left(\sum_{t=1}^{T} x_tx_t'\right)^{-1} \sum_{t=1}^{T} x_ty_{t+1}$ √© uma express√£o concisa que engloba v√°rias opera√ß√µes matem√°ticas e tem profundas implica√ß√µes te√≥ricas na regress√£o linear. A sua computa√ß√£o exige a invers√£o de matrizes, produtos vetoriais e o uso de algoritmos que garantam precis√£o e estabilidade num√©rica. Compreender a estrutura, os requisitos e as implica√ß√µes desta f√≥rmula √© fundamental para uma aplica√ß√£o correta e eficiente da regress√£o OLS em an√°lise de dados e modelagem. Atrav√©s da rela√ß√£o da regress√£o OLS com a proje√ß√£o linear, a f√≥rmula do estimador $b$ estabelece uma ponte entre conceitos te√≥ricos e aplica√ß√µes pr√°ticas, e a sua correta implementa√ß√£o computacional garante a precis√£o e robustez dos resultados de modelos estat√≠sticos.
### Refer√™ncias
[^4.1.18]:  O valor de $\beta$ que minimiza [4.1.17], denotado por $b$, √© a estimativa de m√≠nimos quadrados ordin√°rios (OLS) de $\beta$.  A f√≥rmula para $b$ √© $b = \left(\sum_{t=1}^{T} x_tx_t'\right)^{-1} \sum_{t=1}^{T} x_ty_{t+1}$.

[^4.1.20]: Assim, a regress√£o OLS de $y_{t+1}$ em $x_t$ produz uma estimativa consistente do coeficiente da proje√ß√£o linear. Observe que este resultado requer apenas que o processo seja erg√≥dico para segundos momentos. Em contraste, a an√°lise econom√©trica estrutural requer suposi√ß√µes muito mais fortes sobre a rela√ß√£o entre $X$ e $Y$.

[^Lema 1.1 no capitulo anterior]: O estimador OLS $b$ satisfaz a seguinte condi√ß√£o de ortogonalidade amostral:
$\sum_{t=1}^T (y_{t+1} - b'x_t)x_t' = 0$.
<!-- END -->
