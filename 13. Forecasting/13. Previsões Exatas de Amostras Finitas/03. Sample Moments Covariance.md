## Fatora√ß√£o Triangular para Proje√ß√µes Lineares em Amostras Finitas

### Introdu√ß√£o
Como vimos na se√ß√£o anterior, o c√°lculo dos coeficientes para proje√ß√µes lineares em amostras finitas requer a invers√£o de matrizes de covari√¢ncia [^4.3.8]. Esta se√ß√£o tem como objetivo introduzir uma ferramenta matem√°tica poderosa e eficiente para lidar com o problema da invers√£o de matrizes de covari√¢ncia: a fatora√ß√£o triangular de uma matriz definida positiva [^4.4]. Ao explorar essa t√©cnica, tornaremos o processo de c√°lculo de proje√ß√µes lineares mais pr√°tico e eficiente, al√©m de criar uma base para outros resultados te√≥ricos.

### Fatora√ß√£o Triangular de uma Matriz Definida Positiva
Uma matriz sim√©trica definida positiva $\Omega$ pode ser expressa de forma √∫nica como o produto de tr√™s matrizes [^4.4.1]:
$$
\Omega = ADA',
$$
onde $A$ √© uma matriz triangular inferior com 1s na diagonal principal, $D$ √© uma matriz diagonal com entradas estritamente positivas e $A'$ √© a transposta de $A$. A fatora√ß√£o triangular √© um m√©todo de decomposi√ß√£o de matrizes que possui vantagens computacionais, pois permite resolver sistemas lineares e calcular inversas de maneira mais eficiente. O m√©todo para realizar esta fatora√ß√£o pode ser resumido como [^4.4]:

1.  **Transforma√ß√£o da Matriz:** Dada uma matriz $\Omega$, uma sequ√™ncia de opera√ß√µes √© realizada para transform√°-la em uma matriz triangular superior. Esta transforma√ß√£o envolve pr√©-multiplicar $\Omega$ por matrizes triangulares inferiores $E_i$ com 1s na diagonal principal e p√≥s-multiplicar por suas transpostas $E_i'$.

2.  **Elimina√ß√£o de Elementos:** No primeiro passo, matrizes $E_1$ s√£o utilizadas para zerar os elementos na primeira coluna abaixo da diagonal, de forma que,  $E_1 \Omega E_1' = H$, onde $H$ tem zeros na primeira coluna abaixo da diagonal [^4.4.3]. Analogamente, $E_2$ √© utilizada para zerar os elementos na segunda coluna abaixo da diagonal de $H$, formando $E_2HE_2' = K$, e assim por diante. Este processo continua at√© que a matriz seja transformada em uma matriz diagonal $D$ [^4.4.7].

3.  **Constru√ß√£o da Matriz A:** A matriz $A$ √© constru√≠da como o produto das inversas das matrizes de transforma√ß√£o, ou seja, $A = (E_{n-1} \dots E_2 E_1)^{-1}$ [^4.4.8]. Devido √† estrutura particular das matrizes $E_i$, a inversa $E_i^{-1}$ possui a mesma estrutura, com os elementos da coluna $i$ abaixo da diagonal tendo o sinal oposto, como definido em [^4.4.10]. Al√©m disso, o produto das matrizes $E_i^{-1}$ √© trivial, de forma que a $j$-√©sima coluna de $A$ √© igual a $j$-√©sima coluna de $E_j^{-1}$ [^4.4.11].

A fatora√ß√£o triangular nos permite expressar a matriz de covari√¢ncia de forma mais simples. Al√©m disso, ela revela informa√ß√µes importantes sobre a estrutura de depend√™ncia da s√©rie temporal, que ser√£o √∫teis em previs√µes.

> üí° **Exemplo Num√©rico:**
> Vamos usar a matriz $\Omega$ do exemplo num√©rico anterior, em que $\gamma_0 = 4$, $\gamma_1 = 2$ e $\gamma_2 = 1$:
>
> $$
> \Omega =
> \begin{bmatrix}
> 4 & 2 & 1 \\
> 2 & 4 & 2 \\
> 1 & 2 & 4
> \end{bmatrix}
> $$
>
> $\text{Step 1: }$ Definir $E_1$ para eliminar os elementos na primeira coluna abaixo da diagonal:
>
> $$
> E_1 =
> \begin{bmatrix}
> 1 & 0 & 0 \\
> -2/4 & 1 & 0 \\
> -1/4 & 0 & 1
> \end{bmatrix}
> =
> \begin{bmatrix}
> 1 & 0 & 0 \\
> -0.5 & 1 & 0 \\
> -0.25 & 0 & 1
> \end{bmatrix}
> $$
>
>
> $\text{Step 2: }$ Calcular $H = E_1 \Omega E_1'$:
>
> $$
> H =
> \begin{bmatrix}
> 1 & -0.5 & -0.25 \\
> 0 & 1 & 0 \\
> 0 & 0 & 1
> \end{bmatrix}
> \begin{bmatrix}
> 4 & 2 & 1 \\
> 2 & 4 & 2 \\
> 1 & 2 & 4
> \end{bmatrix}
> \begin{bmatrix}
> 1 & -0.5 & -0.25 \\
> 0 & 1 & 0 \\
> 0 & 0 & 1
> \end{bmatrix}'
> =
> \begin{bmatrix}
> 3 & 3 & 3/2 \\
> 0 & 3 & 3/2 \\
> 0 & 3/2 & 15/4
> \end{bmatrix}
> \begin{bmatrix}
> 1 & -0.5 & -0.25 \\
> 0 & 1 & 0 \\
> 0 & 0 & 1
> \end{bmatrix}
> =
> \begin{bmatrix}
> 4 & 0 & 0 \\
> 0 & 3 & 3/2 \\
> 0 & 3/2 & 15/4
> \end{bmatrix}
> $$
>
> $\text{Step 3: }$ Definir $E_2$ para eliminar o elemento na segunda coluna abaixo da diagonal:
>
> $$
> E_2 =
> \begin{bmatrix}
> 1 & 0 & 0 \\
> 0 & 1 & 0 \\
> 0 & - (3/2)/3 & 1
> \end{bmatrix}
> =
> \begin{bmatrix}
> 1 & 0 & 0 \\
> 0 & 1 & 0 \\
> 0 & -1/2 & 1
> \end{bmatrix}
> $$
>
>
> $\text{Step 4: }$ Calcular $K = E_2HE_2'$:
> $$
> K =
> \begin{bmatrix}
> 1 & 0 & 0 \\
> 0 & 1 & -0.5 \\
> 0 & 0 & 1
> \end{bmatrix}
> \begin{bmatrix}
> 4 & 0 & 0 \\
> 0 & 3 & 3/2 \\
> 0 & 3/2 & 15/4
> \end{bmatrix}
> \begin{bmatrix}
> 1 & 0 & 0 \\
> 0 & 1 & -0.5 \\
> 0 & 0 & 1
> \end{bmatrix}'
> =
> \begin{bmatrix}
> 4 & 0 & 0 \\
> 0 & 3 & 0 \\
> 0 & 0 & 3
> \end{bmatrix}
> $$
>
> $\text{Step 5: }$ Definir a matriz diagonal $D$:
> $$
> D =
> \begin{bmatrix}
> 4 & 0 & 0 \\
> 0 & 3 & 0 \\
> 0 & 0 & 3
> \end{bmatrix}
> $$
>
>
> $\text{Step 6: }$ Calcular a matriz $A$:
>
> $$
> A = (E_2 E_1)^{-1} = E_1^{-1} E_2^{-1} =
> \begin{bmatrix}
> 1 & 0 & 0 \\
> 1/2 & 1 & 0 \\
> 1/4 & 1/2 & 1
> \end{bmatrix}
> $$
>
> A matriz $A$ consiste nos elementos de $E_i^{-1}$ que foram utilizados para eliminar os elementos abaixo da diagonal. A fatora√ß√£o de $\Omega$ √© ent√£o $\Omega = ADA'$.
>
>
> ```python
> import numpy as np
>
> # Define the matrix Omega
> Omega = np.array([[4, 2, 1],
>                   [2, 4, 2],
>                   [1, 2, 4]])
>
> # Step 1: Define E1
> E1 = np.array([[1, 0, 0],
>                [-2/4, 1, 0],
>                [-1/4, 0, 1]])
>
> # Step 2: Calculate H
> H = E1 @ Omega @ E1.T
>
> # Step 3: Define E2
> E2 = np.array([[1, 0, 0],
>                [0, 1, 0],
>                [0, -(3/2)/3, 1]])
>
> # Step 4: Calculate K
> K = E2 @ H @ E2.T
>
> # Step 5: Define D (which is K in this case)
> D = K
>
> # Step 6: Calculate A
> E1_inv = np.array([[1, 0, 0],
>                   [2/4, 1, 0],
>                   [1/4, 0, 1]])
>
> E2_inv = np.array([[1, 0, 0],
>                   [0, 1, 0],
>                   [0, (3/2)/3, 1]])
>
> A = E1_inv @ E2_inv
>
> print("Matrix A:\n", A)
> print("Matrix D:\n", D)
> print("Check ADA':\n", A @ D @ A.T)
> print("Original Omega:\n", Omega)
> ```

**Lema 2**
A fatora√ß√£o triangular de uma matriz definida positiva $\Omega$ √© √∫nica.

*Proof Outline:* A prova da unicidade da fatora√ß√£o triangular √© feita por contradi√ß√£o, mostrando que se duas fatora√ß√µes distintas existirem, ent√£o elas s√£o, na realidade, a mesma fatora√ß√£o. Isso implica na unicidade dos componentes da fatora√ß√£o.

*Prova:*
I.  Suponha que $\Omega$ pode ser fatorada de duas maneiras distintas: $\Omega = A_1 D_1 A_1'$ e $\Omega = A_2 D_2 A_2'$, onde $A_1$ e $A_2$ s√£o matrizes triangulares inferiores com 1s na diagonal, e $D_1$ e $D_2$ s√£o matrizes diagonais com elementos estritamente positivos.

II.  Multiplicando ambos os lados da equa√ß√£o por $A_1^{-1}$ e $(A_1')^{-1}$, temos $A_1^{-1} \Omega (A_1')^{-1} = D_1$ e analogamente,  $A_2^{-1} \Omega (A_2')^{-1} = D_2$.
Isso implica que $D_1$ e $D_2$ s√£o diagonais.

III. Definindo $B = A_2^{-1}A_1$, temos $B D_1 B' = D_2$. Como $A_1$ e $A_2$ s√£o matrizes triangulares inferiores com 1s na diagonal principal, ent√£o $B$ tamb√©m √© uma matriz triangular inferior com 1s na diagonal. A transposta $B'$ √© uma matriz triangular superior com 1s na diagonal.

IV. Multiplicando por $B^{-1}$ √† esquerda e $(B')^{-1}$ √† direita, obtemos  $D_1 = B^{-1} D_2 (B')^{-1}$. O produto de duas matrizes triangulares inferiores √© uma matriz triangular inferior. A inversa de uma matriz triangular inferior tamb√©m √© triangular inferior. Analogamente para matrizes triangulares superiores. Logo $B^{-1}$ √© triangular inferior com 1s na diagonal e $(B')^{-1}$ √© triangular superior com 1s na diagonal.

V. Como $D_1$ √© diagonal, a igualdade $D_1 = B^{-1} D_2 (B')^{-1}$ s√≥ √© poss√≠vel se $B$ for a matriz identidade. Portanto, $B = A_2^{-1}A_1 = I$, o que implica que $A_1 = A_2$.

VI. Substituindo $A_1 = A_2$ na equa√ß√£o original, temos $A_1 D_1 A_1' = A_1 D_2 A_1'$. Multiplicando por $A_1^{-1}$ √† esquerda e $(A_1')^{-1}$ √† direita, obtemos $D_1 = D_2$.

Portanto, a fatora√ß√£o triangular de uma matriz definida positiva √© √∫nica. ‚ñ†

**Lema 2.1**
As matrizes $E_i$ utilizadas na fatora√ß√£o triangular s√£o unicamente definidas para cada etapa do processo de decomposi√ß√£o.

*Proof Outline:* A prova da unicidade das matrizes $E_i$ se baseia no fato de que elas s√£o constru√≠das para zerar elementos espec√≠ficos da matriz, e que essa constru√ß√£o √© un√≠voca.

*Prova:*
I. Suponha que, em uma etapa da fatora√ß√£o, existam duas matrizes distintas $E_i$ e $E_i^*$ que zeram os elementos abaixo da diagonal na $i$-√©sima coluna.

II. Isso implica que $E_i \Omega_i E_i' = H_i$ e $E_i^* \Omega_i (E_i^*)' = H_i^*$, onde $\Omega_i$ √© a matriz na etapa $i$, e $H_i$ e $H_i^*$ t√™m zeros na $i$-√©sima coluna abaixo da diagonal.

III. A constru√ß√£o de $E_i$ envolve dividir os elementos da $i$-√©sima coluna de $\Omega_i$ pelos elementos da diagonal correspondente. Essa opera√ß√£o √© un√≠voca, j√° que os elementos s√£o predeterminados.

IV. Portanto, os elementos de $E_i$ e $E_i^*$ devem ser iguais, caso contr√°rio os zeros abaixo da diagonal n√£o seriam gerados na $i$-√©sima coluna.

V. Consequentemente, $E_i = E_i^*$.

Portanto, as matrizes $E_i$ s√£o unicamente definidas em cada etapa da fatora√ß√£o triangular. ‚ñ†

### Conex√£o com Proje√ß√µes Lineares
A fatora√ß√£o triangular oferece uma forma eficiente de calcular as proje√ß√µes lineares e o erro quadr√°tico m√©dio (MSE) associado. Como vimos anteriormente [^4.5], a matriz $A$ que surge na fatora√ß√£o triangular $\Omega=ADA'$ permite calcular o coeficiente de uma proje√ß√£o linear.

Relembrando [^4.5.6], podemos escrever:

$$AY = \hat{Y}$$
Essa transforma√ß√£o gera um novo vetor de vari√°veis,  $\hat{Y}$, que s√£o n√£o correlacionadas entre si (isto √©, $E(\hat{Y}\hat{Y}')=D$) e relacionadas com $Y$ de forma triangular, onde os elementos da matriz $A$ s√£o precisamente os coeficientes de proje√ß√£o linear de um determinado $Y_i$ sobre os valores $Y_{j}$ para $j < i$. Em particular, a $i$-√©sima linha de $\hat{Y}$ √© igual ao valor de $Y_i$ menos a sua proje√ß√£o linear nos valores $Y_j$ para $j < i$, como ilustrado em [^4.5.7]

Al√©m disso, o MSE do erro de proje√ß√£o, correspondente √† vari√¢ncia de cada elemento do vetor $\hat{Y}$, √© dado pela matriz diagonal $D$.  Em outras palavras, $D$ cont√©m o MSE do erro de previs√£o dos elementos correspondentes do vetor $Y$.

> üí° **Exemplo Num√©rico:**
> Utilizando a matriz $A$ e $D$ calculadas no exemplo anterior, vamos ilustrar a conex√£o com proje√ß√µes lineares.
> Sabemos que $AY = \hat{Y}$, ou:
> $$
> \begin{bmatrix}
> 1 & 0 & 0 \\
> 1/2 & 1 & 0 \\
> 1/4 & 1/2 & 1
> \end{bmatrix}
> \begin{bmatrix}
> Y_1 \\
> Y_2 \\
> Y_3
> \end{bmatrix} =
> \begin{bmatrix}
> Y_1 \\
> Y_2 - \frac{1}{2}Y_1 \\
> Y_3 - \frac{1}{4}Y_1 - \frac{1}{2}(Y_2 - \frac{1}{2}Y_1)
> \end{bmatrix}
> $$
>
> A primeira linha de $\hat{Y}$ √© igual a $Y_1$, j√° que n√£o h√° nenhum outro $Y$ anterior.
> A segunda linha de $\hat{Y}$ √© igual a $Y_2 - \frac{1}{2}Y_1$, que √© o res√≠duo da proje√ß√£o de $Y_2$ em $Y_1$, onde $1/2$ √© o coeficiente da proje√ß√£o, que √© o mesmo valor de $a_{21}$ na matriz A.
> A terceira linha de $\hat{Y}$ √© igual a $Y_3 - \frac{1}{4}Y_1 - \frac{1}{2}(Y_2 - \frac{1}{2}Y_1)$, que √© o res√≠duo da proje√ß√£o de $Y_3$ em $Y_1$ e $Y_2$, onde $1/4$ e $1/2$ s√£o os coeficientes da proje√ß√£o, os mesmos valores da terceira linha de $A$.
>
> Os MSEs das proje√ß√µes correspondentes, as vari√¢ncias dos res√≠duos, s√£o dadas pela matriz D:
> $$
> D = \begin{bmatrix}
> 4 & 0 & 0 \\
> 0 & 3 & 0 \\
> 0 & 0 & 3
> \end{bmatrix}
> $$
> Ou seja, o MSE de prever $Y_1$ √© 4, de prever $Y_2$ dados $Y_1$ √© 3, e de prever $Y_3$ dado $Y_1$ e $Y_2$ √© 3.
>
>
> ```python
> import numpy as np
>
> # Matrix A from the previous example
> A = np.array([[1, 0, 0],
>               [1/2, 1, 0],
>               [1/4, 1/2, 1]])
>
> # Matrix D from the previous example
> D = np.array([[4, 0, 0],
>               [0, 3, 0],
>               [0, 0, 3]])
>
> # Example Y vector
> Y = np.array([10, 15, 20])
>
> # Calculate Y_hat
> Y_hat = A @ Y
>
> print("Y_hat:\n", Y_hat)
>
> # Calculate error terms
> Y_error = Y_hat - Y
> print("Y_error: ",Y_error)
>
> # Verify the relationship E(Y_hat @ Y_hat.T) = D
> cov_Y_hat = np.outer(Y_hat, Y_hat)
> print("Covariance of Y_hat:\n", cov_Y_hat)
>
>
> ```
>
> Em vez de calcular a vari√¢ncia de $\hat{Y}$, o c√≥digo acima mostra um exemplo de c√°lculo de  $\hat{Y}$ e como ele se relaciona com $Y$. Em termos gerais, o c√≥digo ilustra a transforma√ß√£o $A Y = \hat{Y}$, onde $\hat{Y}$ representa as vari√°veis originais transformadas de modo que os erros de proje√ß√£o sejam n√£o correlacionados. A matriz $D$ representa a vari√¢ncia destes erros de proje√ß√£o, mas em casos reais, a vari√¢ncia √© calculada usando um n√∫mero maior de amostras do vetor $Y$.

**Teorema 2**
O MSE associado √† proje√ß√£o linear de $Y_{t+1}$ nos seus $m$ valores passados √© igual √† diagonal da matriz D resultante da fatora√ß√£o triangular da matriz de covari√¢ncia dos dados, e pode ser calculado como [^4.5.13]
$$
    E[Y_{t+1} - P(Y_{t+1}|Y_t, Y_{t-1},\dots Y_{t-m+1})]^2 = d_{m+1,m+1},
$$

*Proof Outline:* A prova deste resultado se baseia no fato que a matriz D resultante da fatora√ß√£o triangular representa a matriz de covari√¢ncia dos erros de proje√ß√£o, ou seja, as vari√¢ncias dos res√≠duos de cada proje√ß√£o.

*Prova:*
I. Seja $\Omega$ a matriz de covari√¢ncia de $Y_t, Y_{t-1}, \dots, Y_{t-m+1}$.

II. A fatora√ß√£o triangular de $\Omega$ √© dada por $\Omega = ADA'$.

III. Definimos o vetor de res√≠duos $\hat{Y} = A^{-1} Y$, onde o $i$-√©simo elemento de $\hat{Y}$ √© o res√≠duo da proje√ß√£o de $Y_i$ sobre seus valores anteriores.

IV. Sabemos que  $E(\hat{Y}\hat{Y}')=D$. Isso significa que os elementos da diagonal de $D$ s√£o as vari√¢ncias dos res√≠duos da proje√ß√£o, ou seja $d_{ii} = E[\hat{Y_i}^2]$.

V. O res√≠duo da proje√ß√£o de $Y_{t+1}$ sobre  $Y_t, Y_{t-1},\dots Y_{t-m+1}$ √© dado pelo elemento $m+1$ do vetor $\hat{Y}$.

VI. Portanto, o MSE da proje√ß√£o de $Y_{t+1}$ sobre $m$ valores passados √© dado por $E[Y_{t+1} - P(Y_{t+1}|Y_t, Y_{t-1},\dots Y_{t-m+1})]^2 = E[\hat{Y}_{m+1}^2] = d_{m+1,m+1}$.

Portanto, o MSE da proje√ß√£o linear √© dado pela diagonal da matriz $D$. ‚ñ†

**Teorema 2.1**
A matriz $A$ resultante da fatora√ß√£o triangular $\Omega = ADA'$ permite expressar a proje√ß√£o linear de um vetor $Y$ em seus valores passados. Especificamente, o produto $A^{-1}Y$ gera um vetor cujos elementos representam os erros de proje√ß√£o, e esses erros s√£o n√£o correlacionados e suas vari√¢ncias s√£o dadas pela matriz $D$.

*Proof Outline:* A prova se baseia no fato que a fatora√ß√£o triangular decomp√µe a matriz de covari√¢ncia em uma matriz triangular inferior que relaciona as vari√°veis originais com os erros de proje√ß√£o, que s√£o n√£o correlacionados.

*Prova:*
I. Seja $\Omega$ a matriz de covari√¢ncia de $Y$, com fatora√ß√£o triangular $\Omega = ADA'$.

II. Definimos o vetor de res√≠duos $\hat{Y}$ como $\hat{Y} = A^{-1}Y$.

III. A matriz de covari√¢ncia de $\hat{Y}$ √© dada por $E[\hat{Y}\hat{Y}'] = E[A^{-1}YY'(A^{-1})'] = A^{-1}E[YY'](A^{-1})' = A^{-1}\Omega(A^{-1})' = A^{-1}(ADA')(A^{-1})' = A^{-1}AD(A'A^{-1}') = D$.

IV. Como $E[\hat{Y}\hat{Y}'] = D$ e $D$ √© uma matriz diagonal, os elementos de $\hat{Y}$ s√£o n√£o correlacionados, e suas vari√¢ncias est√£o na diagonal de $D$.

V. A transforma√ß√£o $A^{-1}Y = \hat{Y}$ expressa $Y$ como uma combina√ß√£o linear dos res√≠duos $\hat{Y}$. A estrutura triangular de $A$ garante que cada elemento de $\hat{Y}$ seja o res√≠duo da proje√ß√£o de um componente de $Y$ em seus valores passados.

VI. Portanto, a matriz $A$ expressa as proje√ß√µes lineares de $Y$ em seus valores passados, e o vetor $A^{-1}Y$ representa os erros de previs√£o correspondentes, com vari√¢ncia dada por D. ‚ñ†

### Conclus√£o
A fatora√ß√£o triangular surge como uma ferramenta eficiente para calcular proje√ß√µes lineares, fornecendo uma alternativa para o problema da invers√£o de matrizes, que aparece no contexto de previs√µes em amostras finitas. A decomposi√ß√£o da matriz de covari√¢ncia $\Omega$ em $ADA'$ permite calcular n√£o apenas os coeficientes da proje√ß√£o, mas tamb√©m o MSE associado. Essa t√©cnica se torna essencial para a implementa√ß√£o de previs√µes exatas em modelos ARMA, abrindo o caminho para abordagens mais pr√°ticas e robustas. Ao usar a fatora√ß√£o triangular, obtemos tamb√©m insights sobre a estrutura da s√©rie temporal, incluindo os erros de previs√£o correspondentes. No pr√≥ximo cap√≠tulo, veremos como podemos usar os momentos amostrais para estimar os par√¢metros populacionais para que as proje√ß√µes lineares sejam ainda mais precisas.

### Refer√™ncias
[^4.3.8]: ...*A equa√ß√£o para os coeficientes da proje√ß√£o enfatiza a depend√™ncia direta das proje√ß√µes lineares nas autocovari√¢ncias da s√©rie temporal.*
[^4.4]: ...*A fatora√ß√£o triangular de uma matriz definida positiva  √© uma decomposi√ß√£o √∫nica que expressa a matriz como um produto de tr√™s matrizes com caracter√≠sticas espec√≠ficas.*
[^4.4.1]: ...*A fatora√ß√£o triangular de uma matriz definida positiva $\Omega$ expressa $\Omega$ como $ADA'$, onde $A$ √© uma matriz triangular inferior com 1s na diagonal e $D$ √© uma matriz diagonal com entradas positivas.*
[^4.4.3]: ...*O processo de fatora√ß√£o triangular envolve transforma√ß√µes da matriz original com o objetivo de zerar elementos abaixo da diagonal, usando matrizes triangulares inferiores como multiplicadores.*
[^4.4.7]: ...*O processo de transforma√ß√£o continua at√© que a matriz original seja convertida em uma matriz diagonal, onde cada transforma√ß√£o envolve pr√© e p√≥s-multiplica√ß√£o por matrizes com estruturas espec√≠ficas.*
[^4.4.8]: ...*A matriz A, um componente da fatora√ß√£o, √© calculada como o produto das matrizes inversas de transforma√ß√£o, que possuem uma estrutura triangular inferior.*
[^4.4.10]: ...*As matrizes triangulares inferiores $E_i$ t√™m inversas que tamb√©m s√£o triangulares inferiores, com elementos abaixo da diagonal com sinais invertidos.*
[^4.4.11]: ...*Devido √† estrutura das matrizes $E_i$ e suas inversas, o c√°lculo da matriz $A$ pode ser feito de forma eficiente, sem a necessidade de realizar invers√µes de matrizes.*
[^4.5]: ... *Na fatora√ß√£o triangular, as matrizes obtidas possuem uma interpreta√ß√£o intuitiva, e os elementos da matriz A representam proje√ß√µes lineares em diferentes vari√°veis.*
[^4.5.6]: ...*A rela√ß√£o $AY = \hat{Y}$ expressa a transforma√ß√£o das vari√°veis originais em um conjunto de res√≠duos n√£o correlacionados.*
[^4.5.7]: ...*Cada componente do vetor transformado $\hat{Y}$ corresponde ao res√≠duo de uma proje√ß√£o linear de uma das vari√°veis nas anteriores.*
[^4.5.13]: ...*A matriz $D$, resultante da fatora√ß√£o triangular, cont√©m o MSE das proje√ß√µes, ou seja, a vari√¢ncia de cada elemento do vetor de res√≠duos transformado.*
<!-- END -->
