## Previs√µes Exatas de Amostras Finitas
### Introdu√ß√£o
Este cap√≠tulo visa explorar m√©todos de previs√£o para s√©ries temporais, focando especificamente em abordagens para amostras finitas. Como vimos anteriormente, as previs√µes √≥timas, baseadas em uma quantidade infinita de observa√ß√µes, dependem de par√¢metros populacionais e, para modelos ARMA, de todo o hist√≥rico da s√©rie [^4.2]. Em situa√ß√µes pr√°ticas, contudo, temos apenas um n√∫mero finito de observa√ß√µes, o que exige adapta√ß√µes nas t√©cnicas de previs√£o. Uma abordagem natural √© truncar a representa√ß√£o do modelo, como na se√ß√£o anterior, definindo valores para o ru√≠do branco inicial [^4.3]. No entanto, esta se√ß√£o se concentra em um m√©todo alternativo, o c√°lculo da proje√ß√£o exata de $Y_{t+1}$ nos seus $m$ valores mais recentes, explorando as propriedades da matriz de covari√¢ncia dos dados e as t√©cnicas de fatora√ß√£o triangular.

### Conceitos Fundamentais
Como definido previamente, a proje√ß√£o linear de $Y_{t+1}$ em uma combina√ß√£o linear dos seus valores mais recentes pode ser expressa como [^4.3.5]:

$$
\alpha^{(m)'}X_t = \alpha_0^{(m)} + \alpha_1^{(m)}Y_t + \alpha_2^{(m)}Y_{t-1} + \dots + \alpha_m^{(m)}Y_{t-m+1}
$$

Onde $X_t$ √© um vetor que cont√©m os valores passados de $Y$. O coeficiente $\alpha_i^{(m)}$ representa o peso de $Y_{t-i+1}$ na proje√ß√£o de $Y_{t+1}$ sobre os m valores mais recentes, este coeficiente, em geral, difere do coeficiente quando usamos $m+1$ valores mais recentes. Se a s√©rie temporal $Y_t$ for estacion√°ria, a esperan√ßa de $Y_t Y_{t-j}$ ser√° igual a  $\gamma_j + \mu^2$, onde $\gamma_j$ √© a autocovari√¢ncia no lag j e $\mu$ √© a m√©dia de $Y_t$. Se definirmos $X_t = (1, Y_t, Y_{t-1}, \dots, Y_{t-m+1})'$, ent√£o podemos expressar os coeficientes da proje√ß√£o como [^4.3.6]:

$$
\begin{aligned}
\alpha^{(m)'} &= [\alpha_0^{(m)}, \alpha_1^{(m)}, \alpha_2^{(m)} \dots \alpha_m^{(m)}] \\
&= [\mu, \gamma_1 + \mu^2, \gamma_2 + \mu^2, \dots, \gamma_m + \mu^2] \begin{bmatrix} \mu & \gamma_0 + \mu^2 & \gamma_1 + \mu^2 & \dots & \gamma_{m-1} + \mu^2 \\
\mu & \gamma_1 + \mu^2 & \gamma_0 + \mu^2 & \dots & \gamma_{m-2} + \mu^2 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\mu & \gamma_{m-1} + \mu^2 & \gamma_{m-2} + \mu^2 & \dots & \gamma_0 + \mu^2
\end{bmatrix}^{-1}
\end{aligned}
$$

Esta express√£o pode ser simplificada ao se trabalhar com as vari√°veis centradas na m√©dia. Definindo $\hat{Y}_{t+1|t}$ como a proje√ß√£o de $Y_{t+1}$ em uma combina√ß√£o linear dos valores centrados na m√©dia, ent√£o podemos escrever [^4.3.7]:

$$
\hat{Y}_{t+1|t} - \mu = \alpha_1^{(m)}(Y_t - \mu) + \alpha_2^{(m)}(Y_{t-1} - \mu) + \dots + \alpha_m^{(m)}(Y_{t-m+1} - \mu)
$$

Neste caso, os coeficientes $\alpha_i^{(m)}$ podem ser calculados diretamente como [^4.3.8]:
$$
\begin{bmatrix} \alpha_1^{(m)} \\ \alpha_2^{(m)} \\ \vdots \\ \alpha_m^{(m)} \end{bmatrix} = \begin{bmatrix} \gamma_0 & \gamma_1 & \dots & \gamma_{m-1} \\
\gamma_1 & \gamma_0 & \dots & \gamma_{m-2} \\
\vdots & \vdots & \ddots & \vdots \\
\gamma_{m-1} & \gamma_{m-2} & \dots & \gamma_0 \end{bmatrix}^{-1} \begin{bmatrix} \gamma_1 \\ \gamma_2 \\ \vdots \\ \gamma_m \end{bmatrix}
$$
Essa formula√ß√£o oferece uma abordagem alternativa para previs√µes exatas com amostras finitas. Uma vez que ela elimina a necessidade de suposi√ß√µes sobre o ru√≠do branco inicial, e ao focar na proje√ß√£o direta de $Y_{t+1}$ em seus valores mais recentes, essa abordagem  captura as depend√™ncias lineares nos dados de forma precisa.

> üí° **Exemplo Num√©rico:**
> Suponha que temos uma s√©rie temporal com autocovari√¢ncias $\gamma_0 = 4$, $\gamma_1 = 2$, $\gamma_2 = 1$, e vamos usar $m=2$ valores passados para prever o pr√≥ximo valor.  Queremos calcular os coeficientes $\alpha_1^{(2)}$ e $\alpha_2^{(2)}$ para prever $Y_{t+1}$ a partir de $Y_t$ e $Y_{t-1}$.
>
> A matriz de autocovari√¢ncia $\Gamma_2$ e o vetor de autocovari√¢ncias s√£o dados por:
>
> $$ \Gamma_2 = \begin{bmatrix} \gamma_0 & \gamma_1 \\ \gamma_1 & \gamma_0 \end{bmatrix} = \begin{bmatrix} 4 & 2 \\ 2 & 4 \end{bmatrix} \quad \text{e} \quad \begin{bmatrix} \gamma_1 \\ \gamma_2 \end{bmatrix} = \begin{bmatrix} 2 \\ 1 \end{bmatrix} $$
>
> Para encontrar os coeficientes $\alpha^{(2)} = [\alpha_1^{(2)}, \alpha_2^{(2)}]^T$, precisamos calcular a inversa da matriz $\Gamma_2$:
>
> $\text{Step 1: }$ Calcular o determinante de $\Gamma_2$: $\det(\Gamma_2) = (4 \times 4) - (2 \times 2) = 16 - 4 = 12$.
>
> $\text{Step 2: }$ Encontrar a matriz adjunta de $\Gamma_2$: $\text{adj}(\Gamma_2) = \begin{bmatrix} 4 & -2 \\ -2 & 4 \end{bmatrix}$.
>
> $\text{Step 3: }$ Calcular a inversa de $\Gamma_2$: $\Gamma_2^{-1} = \frac{1}{\det(\Gamma_2)} \text{adj}(\Gamma_2) = \frac{1}{12} \begin{bmatrix} 4 & -2 \\ -2 & 4 \end{bmatrix} = \begin{bmatrix} 1/3 & -1/6 \\ -1/6 & 1/3 \end{bmatrix}$.
>
> $\text{Step 4: }$ Calcular os coeficientes $\alpha^{(2)}$:
>
> $$
> \begin{bmatrix} \alpha_1^{(2)} \\ \alpha_2^{(2)} \end{bmatrix} = \Gamma_2^{-1} \begin{bmatrix} \gamma_1 \\ \gamma_2 \end{bmatrix} = \begin{bmatrix} 1/3 & -1/6 \\ -1/6 & 1/3 \end{bmatrix} \begin{bmatrix} 2 \\ 1 \end{bmatrix} = \begin{bmatrix} (1/3)\times 2 + (-1/6)\times 1 \\ (-1/6)\times 2 + (1/3)\times 1 \end{bmatrix} = \begin{bmatrix} 1/2 \\ -1/6 \end{bmatrix}
> $$
>
> Portanto, $\alpha_1^{(2)} = 0.5$ e $\alpha_2^{(2)} = -0.1667$.  A proje√ß√£o de $Y_{t+1}$ √© ent√£o: $\hat{Y}_{t+1|t} - \mu = 0.5(Y_t - \mu) -0.1667(Y_{t-1} - \mu)$. Isso significa que $Y_t$ tem um peso positivo e $Y_{t-1}$ um peso negativo na previs√£o de $Y_{t+1}$.

**Lema 1**
Se a matriz de autocovari√¢ncia $\Gamma_m = [\gamma_{|i-j|}]_{i,j=1}^m$ √© definida positiva, ent√£o ela √© invert√≠vel e a solu√ß√£o para os coeficientes $\alpha^{(m)}$ existe e √© √∫nica. Al√©m disso, a matriz inversa $\Gamma_m^{-1}$ tamb√©m √© definida positiva.

*Proof Outline:* A defini√ß√£o de matriz definida positiva garante que todos os seus autovalores s√£o estritamente positivos, e consequentemente, o determinante da matriz √© n√£o nulo, o que implica sua inversibilidade. A inversa de uma matriz definida positiva tamb√©m √© definida positiva.

*Prova:*
I.  Uma matriz $\Gamma_m$ √© definida positiva se $x'\Gamma_m x > 0$ para todo vetor n√£o nulo $x \in \mathbb{R}^m$.

II.  Uma matriz definida positiva tem todos os seus autovalores estritamente positivos.

III. Se todos os autovalores s√£o estritamente positivos, ent√£o o determinante da matriz, que √© o produto dos autovalores, √© tamb√©m estritamente positivo.
    $$\det(\Gamma_m) = \prod_{i=1}^m \lambda_i > 0$$

IV. Uma matriz √© invert√≠vel se, e somente se, seu determinante √© diferente de zero. Como $\det(\Gamma_m) > 0$,  $\Gamma_m$ √© invert√≠vel.

V. Se $\Gamma_m$ √© invert√≠vel, ent√£o a equa√ß√£o que define os coeficientes $\alpha^{(m)}$ tem solu√ß√£o √∫nica.

VI. A inversa de uma matriz definida positiva √© tamb√©m definida positiva. Para ver isto, se $\Gamma_m$ √© definida positiva, ent√£o $x'\Gamma_m x > 0$. Substituindo $x = \Gamma_m^{-1}y$, temos $y'\Gamma_m^{-1}\Gamma_m\Gamma_m^{-1}y = y'\Gamma_m^{-1}y > 0$, provando que $\Gamma_m^{-1}$ √© definida positiva.

Portanto, se $\Gamma_m$ √© definida positiva, ela √© invert√≠vel, a solu√ß√£o para os coeficientes $\alpha^{(m)}$ existe e √© √∫nica e $\Gamma_m^{-1}$ √© definida positiva. ‚ñ†

Para gerar uma previs√£o $s$-per√≠odos √† frente, $\hat{Y}_{t+s|t}$, a proje√ß√£o linear pode ser escrita como [^4.3.9]:
$$
\hat{Y}_{t+s|t} = \mu + \alpha_1^{(m,s)}(Y_t - \mu) + \alpha_2^{(m,s)}(Y_{t-1} - \mu) + \dots + \alpha_m^{(m,s)}(Y_{t-m+1} - \mu),
$$
Onde os coeficientes $\alpha_i^{(m,s)}$ s√£o determinados atrav√©s da rela√ß√£o:
$$
\begin{bmatrix} \alpha_1^{(m,s)} \\ \alpha_2^{(m,s)} \\ \vdots \\ \alpha_m^{(m,s)} \end{bmatrix} = \begin{bmatrix} \gamma_0 & \gamma_1 & \dots & \gamma_{m-1} \\
\gamma_1 & \gamma_0 & \dots & \gamma_{m-2} \\
\vdots & \vdots & \ddots & \vdots \\
\gamma_{m-1} & \gamma_{m-2} & \dots & \gamma_0 \end{bmatrix}^{-1} \begin{bmatrix} \gamma_s \\ \gamma_{s+1} \\ \vdots \\ \gamma_{s+m-1} \end{bmatrix}
$$
> üí° **Exemplo Num√©rico:**
> Usando o exemplo anterior, vamos calcular os coeficientes $\alpha^{(2,2)}$ para prever $Y_{t+2}$ usando $Y_t$ e $Y_{t-1}$. J√° calculamos a matriz inversa $\Gamma_2^{-1}$.
> Precisamos agora do vetor de autocovari√¢ncias para o horizonte de 2 passos:
> $$\begin{bmatrix} \gamma_2 \\ \gamma_3 \end{bmatrix}$$
> Supondo que $\gamma_3 = 0.5$, temos:
> $$\begin{bmatrix} \gamma_2 \\ \gamma_3 \end{bmatrix} = \begin{bmatrix} 1 \\ 0.5 \end{bmatrix}$$
>
> Calculamos ent√£o $\alpha^{(2,2)}$:
>$$
> \begin{bmatrix} \alpha_1^{(2,2)} \\ \alpha_2^{(2,2)} \end{bmatrix} = \begin{bmatrix} 1/3 & -1/6 \\ -1/6 & 1/3 \end{bmatrix} \begin{bmatrix} 1 \\ 0.5 \end{bmatrix} = \begin{bmatrix} (1/3)\times 1 + (-1/6)\times 0.5 \\ (-1/6)\times 1 + (1/3)\times 0.5 \end{bmatrix} = \begin{bmatrix} 7/24 \\ -1/12 \end{bmatrix}
> $$
>
> Logo, $\alpha_1^{(2,2)} \approx 0.29$ e $\alpha_2^{(2,2)} \approx -0.083$. A previs√£o de $Y_{t+2}$ √© dada por $\hat{Y}_{t+2|t} - \mu = 0.29(Y_t - \mu) -0.083(Y_{t-1} - \mu)$. Observe que os coeficientes s√£o diferentes dos de previs√£o de um passo √† frente e que os pesos dos valores passados diminuem com o horizonte de previs√£o.

A dificuldade computacional deste m√©todo reside na necessidade de invers√£o de matrizes de dimens√£o $m \times m$. V√°rias estrat√©gias podem ser adotadas para lidar com esse problema. Uma abordagem √© o uso do filtro de Kalman, discutido no cap√≠tulo 13, que permite gerar previs√µes exatas para modelos ARMA. Outra abordagem √© baseada na fatora√ß√£o triangular da matriz de covari√¢ncia, que exploraremos na pr√≥xima se√ß√£o.
**Teorema 1**
Para uma s√©rie temporal estacion√°ria, os coeficientes de proje√ß√£o $\alpha^{(m)}$ convergem para os coeficientes de proje√ß√£o √≥timos da s√©rie temporal com hist√≥rico infinito quando $m$ tende ao infinito, assumindo que a matriz de autocovari√¢ncia √© definida positiva para todo $m$.

*Proof Outline:* Este resultado segue do fato que a proje√ß√£o linear de $Y_{t+1}$ sobre um espa√ßo cada vez maior de valores passados converge para a proje√ß√£o sobre todo o passado da s√©rie. Formalmente, isso pode ser mostrado usando o conceito de operadores de proje√ß√£o em espa√ßos de Hilbert.

*Prova:*
I. Seja $H_t^{(m)}$ o espa√ßo linear gerado por $\{Y_t, Y_{t-1}, \dots, Y_{t-m+1}\}$. Este espa√ßo √© um subespa√ßo de Hilbert do espa√ßo de todas as vari√°veis aleat√≥rias de segunda ordem.

II. O espa√ßo $H_t^{(\infty)}$ √© o espa√ßo linear gerado por todo o passado $\{Y_t, Y_{t-1}, Y_{t-2}, \dots \}$, e  $H_t^{(m)} \subseteq H_t^{(\infty)}$ para qualquer $m$.

III. A proje√ß√£o linear de $Y_{t+1}$ em $H_t^{(m)}$ √© denotada por $\hat{Y}_{t+1|t}^{(m)} = \sum_{i=1}^m \alpha_i^{(m)} (Y_{t-i+1} - \mu)$, onde os $\alpha_i^{(m)}$ s√£o os coeficientes de proje√ß√£o que minimizam o erro quadr√°tico m√©dio.

IV. √Ä medida que $m$ aumenta, o espa√ßo $H_t^{(m)}$ se expande e converge para $H_t^{(\infty)}$.

V.  Como $\hat{Y}_{t+1|t}^{(m)}$ √© a proje√ß√£o de $Y_{t+1}$ em $H_t^{(m)}$,  o erro de proje√ß√£o, $e_t^{(m)} = Y_{t+1} - \hat{Y}_{t+1|t}^{(m)}$ √© ortogonal a $H_t^{(m)}$. Isso significa que $\mathbb{E}[(Y_{t+1} - \hat{Y}_{t+1|t}^{(m)}) (Y_{t-i+1} - \mu)] = 0$ para $i=1, 2, \dots, m$.

VI. Quando $m \to \infty$,  $\hat{Y}_{t+1|t}^{(m)}$ converge para a proje√ß√£o de $Y_{t+1}$ em $H_t^{(\infty)}$, que √© $\hat{Y}_{t+1|t}^{(\infty)} = \sum_{i=1}^{\infty} \alpha_i Y_{t-i+1}$. Os coeficientes $\alpha_i^{(m)}$ convergem para os coeficientes de proje√ß√£o √≥timos da s√©rie temporal com hist√≥rico infinito $\alpha_i$.

VII. A converg√™ncia dos coeficientes de proje√ß√£o √© garantida pela estacionariedade da s√©rie e o fato de que o espa√ßo $H_t^{(m)}$ converge para $H_t^{(\infty)}$ e que a matriz de autocovari√¢ncia √© definida positiva para todo $m$.

Portanto, os coeficientes de proje√ß√£o $\alpha^{(m)}$ convergem para os coeficientes de proje√ß√£o √≥timos da s√©rie temporal com hist√≥rico infinito quando $m$ tende ao infinito. ‚ñ†

### Conclus√£o
Este m√©todo de proje√ß√£o linear com amostras finitas oferece um contraponto √† abordagem tradicional de truncamento de s√©ries infinitas.  Ao trabalhar diretamente com os valores observados de $Y_t$ e suas autocovari√¢ncias, a proje√ß√£o exata em seus $m$ valores mais recentes fornece previs√µes que consideram as particularidades dos dados dispon√≠veis. Este m√©todo √© particularmente √∫til em situa√ß√µes onde o tamanho da amostra √© limitado e onde se deseja evitar suposi√ß√µes sobre valores iniciais do processo de ru√≠do branco. A efici√™ncia computacional desse m√©todo, contudo, reside em como a matriz de covari√¢ncia √© tratada. A pr√≥xima se√ß√£o  explorar√° a fatora√ß√£o triangular, uma ferramenta chave para o c√°lculo de proje√ß√µes lineares exatas, o que abrir√° caminho para a implementa√ß√£o de m√©todos robustos de previs√£o com amostras finitas.

### Refer√™ncias
[^4.2]:  ...*A discuss√£o anterior sobre previs√µes de s√©ries temporais abordou modelos baseados em uma quantidade infinita de observa√ß√µes e fez uso de par√¢metros populacionais conhecidos, o que permite a aplica√ß√£o direta das f√≥rmulas de proje√ß√£o.*
[^4.3]: ...*No entanto, uma dificuldade surge quando temos apenas um n√∫mero finito de observa√ß√µes, j√° que precisamos de suposi√ß√µes sobre o ru√≠do branco inicial, que pode influenciar as previs√µes.*
[^4.3.5]: ...*A busca pela melhor previs√£o linear dos valores mais recentes √© feita por meio dos coeficientes que minimizam o erro quadr√°tico m√©dio, em uma abordagem alternativa √† truncagem de modelos.*
[^4.3.6]: ...*Os coeficientes da proje√ß√£o linear s√£o definidos pelas autocovari√¢ncias da s√©rie temporal e podem ser obtidos por meio da invers√£o de uma matriz.*
[^4.3.7]: ...*O m√©todo de proje√ß√£o linear em vari√°veis centradas na m√©dia √© uma alternativa para simplificar os c√°lculos e focar na estrutura de depend√™ncia da s√©rie.*
[^4.3.8]: ...*A equa√ß√£o para os coeficientes da proje√ß√£o enfatiza a depend√™ncia direta das proje√ß√µes lineares nas autocovari√¢ncias da s√©rie temporal.*
[^4.3.9]: ...*Para previs√µes com v√°rios per√≠odos √† frente, a f√≥rmula da proje√ß√£o √© adaptada para considerar o intervalo de previs√£o.*
[^4.1.10]: ... *O ponto chave na deriva√ß√£o da proje√ß√£o linear reside na escolha dos coeficientes que minimizam o erro quadr√°tico m√©dio, o que √© assegurado ao se fazer o erro de proje√ß√£o n√£o correlacionado com o regressor.*
<!-- END -->
