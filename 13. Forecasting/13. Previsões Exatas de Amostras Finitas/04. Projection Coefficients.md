## Os Coeficientes da Proje√ß√£o Exata de Amostras Finitas s√£o os Mesmos que os Coeficientes da Proje√ß√£o em Desvios da M√©dia.
### Introdu√ß√£o
Neste cap√≠tulo, temos explorado abordagens para previs√µes de s√©ries temporais com amostras finitas. Vimos que a proje√ß√£o linear de $Y_{t+1}$ pode ser calculada diretamente sobre os $m$ valores mais recentes, e que a fatora√ß√£o triangular de matrizes de covari√¢ncia pode auxiliar nesse processo [^4.4], [^4.5]. Uma quest√£o que surge √© se os coeficientes obtidos atrav√©s da proje√ß√£o linear sobre os valores observados de $Y_t$ s√£o diferentes dos coeficientes calculados sobre as vari√°veis em desvios da m√©dia. Esta se√ß√£o visa a demonstrar que, na verdade, esses coeficientes s√£o id√™nticos [^4.3.8], e que trabalhar com desvios da m√©dia √© uma simplifica√ß√£o que n√£o altera os resultados.
### Equival√™ncia entre Coeficientes
Na se√ß√£o anterior, apresentamos duas formas de calcular os coeficientes de proje√ß√£o linear. A primeira envolve a proje√ß√£o de $Y_{t+1}$ diretamente sobre os valores observados de $Y_t$, utilizando um vetor $X_t$ que inclui um intercepto, enquanto a segunda utiliza os desvios da m√©dia de $Y_t$.

Come√ßamos com a forma da proje√ß√£o em valores observados:
$$
\alpha^{(m)'}X_t = \alpha_0^{(m)} + \alpha_1^{(m)}Y_t + \alpha_2^{(m)}Y_{t-1} + \dots + \alpha_m^{(m)}Y_{t-m+1}
$$
O vetor de coeficientes $\alpha^{(m)}$ √© calculado como [^4.3.6]:
$$
\alpha^{(m)} = [\mu, \gamma_1 + \mu^2, \gamma_2 + \mu^2, \dots, \gamma_m + \mu^2] \Gamma_m^{-1},
$$
onde $\Gamma_m$ √© a matriz de autocovari√¢ncia com dimens√£o $m \times m$ e cada elemento √© dado por $\Gamma_m[i,j] = \gamma_{|i-j|} + \mu^2$.

Alternativamente, podemos expressar a proje√ß√£o utilizando vari√°veis em desvios da m√©dia:
$$
\hat{Y}_{t+1|t} - \mu = \alpha_1^{(m)}(Y_t - \mu) + \alpha_2^{(m)}(Y_{t-1} - \mu) + \dots + \alpha_m^{(m)}(Y_{t-m+1} - \mu).
$$
Os coeficientes $\alpha_i^{(m)}$ podem ser calculados utilizando apenas a matriz de autocovari√¢ncias, $\Gamma_m$, com cada elemento dado por $\Gamma_m[i,j] = \gamma_{|i-j|}$ e o vetor  $[\gamma_1, \gamma_2, \dots, \gamma_m]'$ [^4.3.8]:
$$
\begin{bmatrix} \alpha_1^{(m)} \\ \alpha_2^{(m)} \\ \vdots \\ \alpha_m^{(m)} \end{bmatrix} = \begin{bmatrix} \gamma_0 & \gamma_1 & \dots & \gamma_{m-1} \\
\gamma_1 & \gamma_0 & \dots & \gamma_{m-2} \\
\vdots & \vdots & \ddots & \vdots \\
\gamma_{m-1} & \gamma_{m-2} & \dots & \gamma_0 \end{bmatrix}^{-1} \begin{bmatrix} \gamma_1 \\ \gamma_2 \\ \vdots \\ \gamma_m \end{bmatrix}
$$
A equival√™ncia desses coeficientes √© expressa no seguinte teorema:

**Teorema 3**
Os coeficientes $\alpha_i^{(m)}$ obtidos na proje√ß√£o de $Y_{t+1}$ em seus $m$ valores mais recentes, seja trabalhando diretamente com os valores observados ou com os desvios da m√©dia, s√£o id√™nticos, exceto pelo coeficiente do intercepto $\alpha_0^{(m)}$. Formalmente,
$$
\alpha_i^{(m)} \text{ em  }  \hat{Y}_{t+1|t} = \mu +  \alpha_1^{(m)}(Y_t - \mu) + \ldots = \alpha_i^{(m)}  \text{ em  }  \hat{Y}_{t+1|t} = \alpha_0^{(m)} +  \alpha_1^{(m)}Y_t + \ldots, \text{ para } i = 1, 2, \ldots,m
$$
*Proof Outline:* A prova da equival√™ncia dos coeficientes √© feita atrav√©s da an√°lise das propriedades das matrizes de covari√¢ncia e dos vetores utilizados para o c√°lculo, e pelo reconhecimento que os coeficientes obtidos na proje√ß√£o linear em vari√°veis centradas na m√©dia n√£o s√£o afetados pela presen√ßa da m√©dia.

*Prova:*
I. Considere a proje√ß√£o de $Y_{t+1}$ sobre um vetor $X_t$ que inclui uma constante e $m$ valores passados: $X_t = [1, Y_t, Y_{t-1}, \dots, Y_{t-m+1}]'$. O vetor de coeficientes de proje√ß√£o √© $\alpha^{(m)} = [\alpha_0^{(m)}, \alpha_1^{(m)}, \alpha_2^{(m)}, \dots, \alpha_m^{(m)}]$.

II. A matriz de covari√¢ncia associada a $X_t$ √©:

$$
\Gamma_m^{ext} =
\begin{bmatrix}
1 & \mu & \mu & \dots & \mu \\
\mu & \gamma_0+\mu^2 & \gamma_1+\mu^2 & \dots & \gamma_{m-1}+\mu^2 \\
\mu & \gamma_1+\mu^2 & \gamma_0+\mu^2 & \dots & \gamma_{m-2}+\mu^2 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\mu & \gamma_{m-1}+\mu^2 & \gamma_{m-2}+\mu^2 & \dots & \gamma_0+\mu^2
\end{bmatrix}
$$

III.  O vetor de covari√¢ncias entre $Y_{t+1}$ e $X_t$ √© $C = [\mu, \gamma_1 + \mu^2, \gamma_2 + \mu^2, \dots, \gamma_m + \mu^2]'$.
Os coeficientes de proje√ß√£o s√£o dados por:
$\alpha^{(m)} = (\Gamma_m^{ext})^{-1} C'$.

IV. Considere agora a proje√ß√£o de $Y_{t+1} - \mu$ sobre os desvios da m√©dia de $Y_t$,  $Y_{t-1}$, ..., $Y_{t-m+1}$. O vetor de regressores √© $Z_t = [Y_t-\mu, Y_{t-1}-\mu, \dots, Y_{t-m+1}-\mu]$. A matriz de covari√¢ncia associada √©:

$$
\Gamma_m = \begin{bmatrix}
\gamma_0 & \gamma_1 & \dots & \gamma_{m-1} \\
\gamma_1 & \gamma_0 & \dots & \gamma_{m-2} \\
\vdots & \vdots & \ddots & \vdots \\
\gamma_{m-1} & \gamma_{m-2} & \dots & \gamma_0
\end{bmatrix}.
$$

V.  O vetor de covari√¢ncias entre $Y_{t+1} - \mu$ e $Z_t$ √© $C_Z = [\gamma_1, \gamma_2, \dots, \gamma_m]'$.
Os coeficientes de proje√ß√£o s√£o dados por:
$\alpha_Z^{(m)} = (\Gamma_m)^{-1} C_Z$.

VI. A rela√ß√£o entre as duas proje√ß√µes √© expressa como:
$$\hat{Y}_{t+1|t} - \mu = \sum_{i=1}^m \alpha_i (Y_{t-i+1} - \mu) \text{ e } \hat{Y}_{t+1|t} = \alpha_0 + \sum_{i=1}^m \alpha_i Y_{t-i+1}$$
onde $\alpha_i$ s√£o os mesmos coeficientes nos dois casos, com $\alpha_0 = \mu(1-\sum_{i=1}^m\alpha_i)$. A m√©dia $\mu$ n√£o afeta os coeficientes de proje√ß√£o linear que medem a rela√ß√£o entre as vari√°veis, apenas o coeficiente do intercepto.

Portanto, os coeficientes $\alpha_i^{(m)}$ para $i = 1, 2, ..., m$ obtidos em ambas as proje√ß√µes s√£o id√™nticos. A √∫nica diferen√ßa reside no intercepto $\alpha_0^{(m)}$ que aparece na proje√ß√£o usando os valores observados, mas n√£o na proje√ß√£o que usa os desvios da m√©dia. ‚ñ†

**Lema 3.1**
O intercepto $\alpha_0^{(m)}$ na proje√ß√£o com valores observados pode ser calculado diretamente a partir da m√©dia $\mu$ e dos coeficientes $\alpha_i^{(m)}$ obtidos na proje√ß√£o com desvios da m√©dia, especificamente:
$$
\alpha_0^{(m)} = \mu \left( 1 - \sum_{i=1}^m \alpha_i^{(m)} \right).
$$
*Proof:*
I. A partir do Teorema 3, sabemos que:
$$
\hat{Y}_{t+1|t} = \mu +  \sum_{i=1}^m \alpha_i^{(m)}(Y_{t-i+1} - \mu) = \alpha_0^{(m)} +  \sum_{i=1}^m \alpha_i^{(m)}Y_{t-i+1}.
$$
II. Expandindo a primeira express√£o, temos:
$$
\hat{Y}_{t+1|t} = \mu +  \sum_{i=1}^m \alpha_i^{(m)}Y_{t-i+1} - \mu\sum_{i=1}^m \alpha_i^{(m)} = \mu \left( 1 - \sum_{i=1}^m \alpha_i^{(m)} \right) + \sum_{i=1}^m \alpha_i^{(m)}Y_{t-i+1}.
$$
III. Comparando com a segunda express√£o, $\hat{Y}_{t+1|t} = \alpha_0^{(m)} +  \sum_{i=1}^m \alpha_i^{(m)}Y_{t-i+1}$,  podemos concluir que:
$$
\alpha_0^{(m)} = \mu \left( 1 - \sum_{i=1}^m \alpha_i^{(m)} \right).
$$
Este resultado estabelece uma rela√ß√£o expl√≠cita entre o intercepto da proje√ß√£o com valores observados e os coeficientes da proje√ß√£o com desvios da m√©dia. ‚ñ†

### Implica√ß√µes Pr√°ticas e Simplifica√ß√µes
A equival√™ncia entre os coeficientes de proje√ß√£o demonstra que, na pr√°tica, podemos trabalhar diretamente com os desvios da m√©dia sem perda de generalidade [^4.3.7]. Isso simplifica o c√°lculo dos coeficientes, pois n√£o necessitamos incluir uma constante no vetor $X_t$ nem lidar com as m√©dias e covari√¢ncias da matriz $\Gamma_m^{ext}$. Trabalhar com os desvios da m√©dia reduz o n√∫mero de par√¢metros na proje√ß√£o e torna os c√°lculos mais simples. A proje√ß√£o com vari√°veis centradas na m√©dia √© dada por:
$$
\hat{Y}_{t+1|t} - \mu = \alpha_1^{(m)}(Y_t - \mu) + \alpha_2^{(m)}(Y_{t-1} - \mu) + \dots + \alpha_m^{(m)}(Y_{t-m+1} - \mu).
$$
Onde os coeficientes $\alpha_i$ s√£o dados por:
$$
\begin{bmatrix} \alpha_1^{(m)} \\ \alpha_2^{(m)} \\ \vdots \\ \alpha_m^{(m)} \end{bmatrix} = \begin{bmatrix} \gamma_0 & \gamma_1 & \dots & \gamma_{m-1} \\
\gamma_1 & \gamma_0 & \dots & \gamma_{m-2} \\
\vdots & \vdots & \ddots & \vdots \\
\gamma_{m-1} & \gamma_{m-2} & \dots & \gamma_0 \end{bmatrix}^{-1} \begin{bmatrix} \gamma_1 \\ \gamma_2 \\ \vdots \\ \gamma_m \end{bmatrix}
$$
Esse resultado √© fundamental pois mostra que a escolha de centrar ou n√£o a vari√°vel n√£o afeta os coeficientes da rela√ß√£o linear entre as vari√°veis, mas apenas o intercepto, que pode ser obtido atrav√©s da m√©dia.

> üí° **Exemplo Num√©rico:**
> Vamos considerar uma s√©rie temporal com m√©dia $\mu=10$ e as autocovari√¢ncias $\gamma_0=9$, $\gamma_1=6$ e $\gamma_2=3$. Utilizando $m=2$, vamos calcular a proje√ß√£o de $Y_{t+1}$ em $Y_t$ e $Y_{t-1}$ utilizando as duas abordagens (com as vari√°veis observadas e com os desvios da m√©dia).
>
> $\text{Step 1: }$ Calcular a proje√ß√£o com os valores observados:
>
> $$
> \alpha^{(2)} = [\mu, \gamma_1 + \mu^2, \gamma_2 + \mu^2] (\Gamma_2^{ext})^{-1}
> $$
>
> Onde $\Gamma_2^{ext}$ √©:
> $$
> \Gamma_2^{ext} =
> \begin{bmatrix}
> 1 & 10 & 10 \\
> 10 & 109 & 106 \\
> 10 & 106 & 109
> \end{bmatrix}
> $$
> E seu inverso √©:
> $$
> (\Gamma_2^{ext})^{-1} =
> \begin{bmatrix}
> 1.098 & -0.099 & -0.099 \\
> -0.099 & 0.0198 & 0.010 \\
> -0.099 & 0.010 & 0.0198
> \end{bmatrix}
> $$
>
> Logo:
> $$
> \alpha^{(2)} = [10, 6 + 10^2, 3 + 10^2] (\Gamma_2^{ext})^{-1} = [10, 106, 103]\begin{bmatrix}
> 1.098 & -0.099 & -0.099 \\
> -0.099 & 0.0198 & 0.010 \\
> -0.099 & 0.010 & 0.0198
> \end{bmatrix} = [-0.029, 0.66, 0.33]
> $$
>
> Portanto, a proje√ß√£o √©:
> $$
> \hat{Y}_{t+1|t} = -0.029 + 0.66Y_t + 0.33Y_{t-1}
> $$
>
>
> $\text{Step 2: }$ Calcular a proje√ß√£o com os desvios da m√©dia:
>
> $$
> \begin{bmatrix} \alpha_1^{(2)} \\ \alpha_2^{(2)} \end{bmatrix} = \begin{bmatrix} 9 & 6 \\ 6 & 9 \end{bmatrix}^{-1} \begin{bmatrix} 6 \\ 3 \end{bmatrix} = \begin{bmatrix} 0.222 & -0.148 \\ -0.148 & 0.222 \end{bmatrix} \begin{bmatrix} 6 \\ 3 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}
> $$
>
> Logo, a proje√ß√£o √©:
> $$
> \hat{Y}_{t+1|t} - \mu = 0.66(Y_t - \mu) + 0.33(Y_{t-1} - \mu).
> $$
>
> Observe que os coeficientes da proje√ß√£o de $Y_{t+1}$ sobre $Y_t$ e $Y_{t-1}$ s√£o os mesmos em ambas as abordagens. Al√©m disso, podemos verificar que o termo constante da segunda abordagem √© o mesmo da primeira:
> $$
> \hat{Y}_{t+1|t} = 10 + 0.66(Y_t - 10) + 0.33(Y_{t-1} - 10) = -0.029 + 0.66Y_t + 0.33Y_{t-1}
> $$
>
> Este exemplo ilustra a equival√™ncia entre os dois m√©todos de c√°lculo dos coeficientes de proje√ß√£o, e o m√©todo com desvio da m√©dia √© mais simples pois dispensa o c√°lculo da matriz $\Gamma_2^{ext}$.
>
> ```python
> import numpy as np
>
> # Define parameters
> mu = 10
> gamma_0 = 9
> gamma_1 = 6
> gamma_2 = 3
>
> # Step 1: Calculate projection with observed values
> Gamma_ext = np.array([[1, mu, mu],
>                          [mu, gamma_0 + mu**2, gamma_1 + mu**2],
>                          [mu, gamma_1 + mu**2, gamma_0 + mu**2]])
>
> C_ext = np.array([mu, gamma_1 + mu**2, gamma_2 + mu**2])
>
> Gamma_ext_inv = np.linalg.inv(Gamma_ext)
> alpha_ext = C_ext @ Gamma_ext_inv
>
> print("Coefficients with observed values:", alpha_ext)
>
> # Step 2: Calculate projection with mean deviations
> Gamma = np.array([[gamma_0, gamma_1],
>                  [gamma_1, gamma_0]])
>
> C_z = np.array([gamma_1, gamma_2])
> Gamma_inv = np.linalg.inv(Gamma)
> alpha_z = Gamma_inv @ C_z
> print("Coefficients with mean deviations:", alpha_z)
>
> # Calculate constant for projection with observed values
> alpha_0 = mu * (1 - np.sum(alpha_z))
> print("Alpha_0 for projection with observed values:", alpha_0)
> ```
>
> üí° **Exemplo Num√©rico com Diferentes Autocovari√¢ncias**
>
> Para refor√ßar o entendimento, considere um segundo exemplo onde as autocovari√¢ncias s√£o diferentes e a m√©dia √© $\mu = 2$. Sejam $\gamma_0 = 5$, $\gamma_1 = 3$, e $\gamma_2 = 1$. Vamos recalcular a proje√ß√£o usando $m=2$.
>
> $\text{Step 1: }$ Proje√ß√£o com valores observados:
>
> $$
> \Gamma_2^{ext} = \begin{bmatrix}
> 1 & 2 & 2 \\
> 2 & 9 & 7 \\
> 2 & 7 & 9
> \end{bmatrix}
> $$
>
> $$
> (\Gamma_2^{ext})^{-1} = \begin{bmatrix}
> 2.0625 & -0.25 & -0.25 \\
> -0.25 & 0.1875 & 0.0625 \\
> -0.25 & 0.0625 & 0.1875
> \end{bmatrix}
> $$
>
> $$
> \alpha^{(2)} = [2, 3+4, 1+4] (\Gamma_2^{ext})^{-1} = [2, 7, 5] \begin{bmatrix}
> 2.0625 & -0.25 & -0.25 \\
> -0.25 & 0.1875 & 0.0625 \\
> -0.25 & 0.0625 & 0.1875
> \end{bmatrix} = [0.0625, 0.9375, -0.25]
> $$
>
> Proje√ß√£o: $\hat{Y}_{t+1|t} = 0.0625 + 0.9375Y_t - 0.25Y_{t-1}$
>
> $\text{Step 2: }$ Proje√ß√£o com desvios da m√©dia:
>
> $$
> \begin{bmatrix} \alpha_1^{(2)} \\ \alpha_2^{(2)} \end{bmatrix} = \begin{bmatrix} 5 & 3 \\ 3 & 5 \end{bmatrix}^{-1} \begin{bmatrix} 3 \\ 1 \end{bmatrix} = \begin{bmatrix} 0.3125 & -0.1875 \\ -0.1875 & 0.3125 \end{bmatrix} \begin{bmatrix} 3 \\ 1 \end{bmatrix} = \begin{bmatrix} 0.75 \\ -0.25 \end{bmatrix}
> $$
>
> Proje√ß√£o: $\hat{Y}_{t+1|t} - \mu = 0.75(Y_t - \mu) - 0.25(Y_{t-1} - \mu)$
>
> Verificando a igualdade das proje√ß√µes:
>
> $\hat{Y}_{t+1|t} = 2 + 0.75(Y_t - 2) - 0.25(Y_{t-1} - 2) = 0.0625 + 0.9375Y_t - 0.25Y_{t-1}$
>
> Note que, novamente, os coeficientes para $Y_t$ e $Y_{t-1}$ s√£o os mesmos, e o intercepto √© calculado corretamente usando a m√©dia e os coeficientes obtidos da proje√ß√£o com desvio da m√©dia.
> ```python
> import numpy as np
>
> # Define parameters
> mu = 2
> gamma_0 = 5
> gamma_1 = 3
> gamma_2 = 1
>
> # Step 1: Calculate projection with observed values
> Gamma_ext = np.array([[1, mu, mu],
>                          [mu, gamma_0 + mu**2, gamma_1 + mu**2],
>                          [mu, gamma_1 + mu**2, gamma_0 + mu**2]])
>
> C_ext = np.array([mu, gamma_1 + mu**2, gamma_2 + mu**2])
>
> Gamma_ext_inv = np.linalg.inv(Gamma_ext)
> alpha_ext = C_ext @ Gamma_ext_inv
>
> print("Coefficients with observed values:", alpha_ext)
>
> # Step 2: Calculate projection with mean deviations
> Gamma = np.array([[gamma_0, gamma_1],
>                  [gamma_1, gamma_0]])
>
> C_z = np.array([gamma_1, gamma_2])
> Gamma_inv = np.linalg.inv(Gamma)
> alpha_z = Gamma_inv @ C_z
> print("Coefficients with mean deviations:", alpha_z)
>
> # Calculate constant for projection with observed values
> alpha_0 = mu * (1 - np.sum(alpha_z))
> print("Alpha_0 for projection with observed values:", alpha_0)
> ```

### Conclus√£o
Nesta se√ß√£o, demonstramos que os coeficientes da proje√ß√£o linear, quando calculados diretamente sobre os valores observados ou sobre os desvios da m√©dia, s√£o id√™nticos, exceto pelo termo do intercepto. Esse resultado √© de extrema import√¢ncia, pois permite simplificar os c√°lculos das proje√ß√µes e focar na estrutura de depend√™ncia dos dados, sem a necessidade de incluir uma constante no vetor regressor. O resultado apresentado simplifica o c√°lculo das proje√ß√µes e tamb√©m oferece uma vis√£o mais clara da rela√ß√£o entre os valores passados de uma s√©rie temporal e sua evolu√ß√£o futura. O desenvolvimento deste conceito √© um passo importante para a compreens√£o e aplica√ß√£o eficaz de t√©cnicas de previs√£o.

### Refer√™ncias
[^4.3.8]: ...*A equa√ß√£o para os coeficientes da proje√ß√£o enfatiza a depend√™ncia direta das proje√ß√µes lineares nas autocovari√¢ncias da s√©rie temporal.*
[^4.4]:  ... *A fatora√ß√£o triangular de uma matriz definida positiva  √© uma decomposi√ß√£o √∫nica que expressa a matriz como um produto de tr√™s matrizes com caracter√≠sticas espec√≠ficas.*
[^4.5]:  ... *Na fatora√ß√£o triangular, as matrizes obtidas possuem uma interpreta√ß√£o intuitiva, e os elementos da matriz A representam proje√ß√µes lineares em diferentes vari√°veis.*
[^4.3.6]: ...*Os coeficientes da proje√ß√£o linear s√£o definidos pelas autocovari√¢ncias da s√©rie temporal e podem ser obtidos por meio da invers√£o de uma matriz.*
[^4.3.7]: ...*O m√©todo de proje√ß√£o linear em vari√°veis centradas na m√©dia √© uma alternativa para simplificar os c√°lculos e focar na estrutura de depend√™ncia da s√©rie.*
<!-- END -->
