## Trade-off entre Computa√ß√£o e Previsibilidade no Processo AR(1)

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre o processo **Autorregressivo de ordem 1 (AR(1))** e suas propriedades de previs√£o [^1], assim como sua efici√™ncia computacional [^2] e implementa√ß√£o recursiva [^3], este cap√≠tulo aborda um aspecto crucial na modelagem: o trade-off entre computa√ß√£o e previsibilidade. O modelo AR(1), com sua estrutura simples e eficiente, oferece uma excelente plataforma para ilustrar esse compromisso. Exploraremos como essa simplicidade, enquanto vantajosa em termos computacionais, pode levar a uma menor precis√£o nas previs√µes de longo prazo, particularmente em compara√ß√£o com modelos mais complexos que incorporam mais informa√ß√µes sobre os valores passados da s√©rie.

### A Simplicidade Computacional do AR(1)
O modelo AR(1), descrito pela equa√ß√£o [4.2.14]:
$$ (1 - \phi L)(Y_t - \mu) = \epsilon_t, $$
possui uma estrutura matem√°tica que resulta em uma implementa√ß√£o computacional extremamente simples. Como demonstrado [^3], a previs√£o para qualquer horizonte *s* pode ser calculada recursivamente:
$$ \hat{Y}_{t+s|t} = \mu + \phi (\hat{Y}_{t+s-1|t} - \mu). $$
A implementa√ß√£o recursiva requer apenas opera√ß√µes aritm√©ticas b√°sicas, tais como subtra√ß√£o, multiplica√ß√£o e adi√ß√£o. Essa simplicidade leva a um baixo custo computacional, o que √© altamente vantajoso em cen√°rios onde os recursos computacionais s√£o limitados ou onde √© necess√°rio realizar previs√µes em tempo real [^2]. Em contraste, modelos mais complexos, como os modelos ARMA(p,q) com p > 1 ou q > 1,  possuem um n√∫mero maior de par√¢metros, aumentando a complexidade de estima√ß√£o e o custo computacional da previs√£o.

> üí° **Exemplo Num√©rico:** Imagine um cen√°rio onde √© necess√°rio prever a demanda por um produto em um pequeno dispositivo embarcado com recursos computacionais limitados. Um modelo AR(1), com seu baixo custo computacional, seria muito mais f√°cil de implementar e executar do que um modelo ARMA(2,2). O modelo AR(1) requer apenas o armazenamento dos par√¢metros $\mu$ e $\phi$, a observa√ß√£o atual $Y_t$, e um loop recursivo simples, enquanto um ARMA(2,2) demandaria armazenar os coeficientes $\phi_1, \phi_2, \theta_1, \theta_2$  e usar um esquema de c√°lculo mais elaborado envolvendo termos passados tanto de Y quanto de $\epsilon$. O menor custo computacional do AR(1) torna sua execu√ß√£o r√°pida e com menor consumo de energia.
>
> üí° **Exemplo Num√©rico:** Suponha que temos a seguinte s√©rie temporal representando a temperatura di√°ria (em graus Celsius) em uma cidade:
> ```
> Y = [25, 27, 26, 28, 29, 30, 28, 27, 26, 25]
> ```
> Para estimar os par√¢metros de um modelo AR(1), primeiro calculamos a m√©dia $\mu$ dos valores. $\mu \approx 27.1$. Usando o Lema 1.1, podemos estimar $\phi$ usando:
> $$ \hat{\phi} = \frac{\sum_{t=2}^T (Y_t - \mu)(Y_{t-1} - \mu)}{\sum_{t=2}^T (Y_{t-1} - \mu)^2} $$
> Substituindo os valores, temos:
>
> $\text{Numerador} = (27-27.1)(25-27.1) + (26-27.1)(27-27.1) + \ldots + (25-27.1)(26-27.1) \approx 16.5$
> $\text{Denominador} = (25-27.1)^2 + (27-27.1)^2 + \ldots + (26-27.1)^2 \approx 20.3$
>
> $\hat{\phi} \approx \frac{16.5}{20.3} \approx 0.81$
>
> A previs√£o para o dia seguinte ($t+1$), usando o valor mais recente $Y_{10}=25$, seria:
> $\hat{Y}_{11|10} = \mu + \phi (Y_{10} - \mu) = 27.1 + 0.81 * (25 - 27.1) \approx 25.39$.
>
> Este exemplo mostra como os par√¢metros s√£o estimados e como a previs√£o √© feita usando uma simples opera√ß√£o recursiva.

**Observa√ß√£o 1:** A efici√™ncia computacional do AR(1) tamb√©m se manifesta no processo de estima√ß√£o dos par√¢metros. M√©todos como m√≠nimos quadrados ou m√°xima verossimilhan√ßa, quando aplicados ao AR(1), levam a computa√ß√µes diretas e eficientes. Em contrapartida, modelos ARMA(p,q) com p > 1 ou q > 1 podem exigir m√©todos de estima√ß√£o iterativos com maior custo computacional e maior demanda por recursos.

**Lema 1.1:** O estimador de m√≠nimos quadrados para $\phi$ no modelo AR(1) pode ser obtido de forma anal√≠tica, sem a necessidade de itera√ß√µes.
*Prova:*
I. O estimador de m√≠nimos quadrados para $\phi$ no modelo AR(1) √© obtido minimizando a soma dos erros quadr√°ticos:
$$ \sum_{t=2}^T (Y_t - \mu - \phi(Y_{t-1} - \mu))^2 $$
II. Derivando a express√£o acima em rela√ß√£o a $\phi$ e igualando a zero, obtemos:
$$ \frac{\partial}{\partial \phi} \sum_{t=2}^T (Y_t - \mu - \phi(Y_{t-1} - \mu))^2 = -2 \sum_{t=2}^T (Y_t - \mu - \phi(Y_{t-1} - \mu))(Y_{t-1} - \mu) = 0 $$
III. Simplificando a express√£o:
$$ \sum_{t=2}^T (Y_t - \mu)(Y_{t-1} - \mu) = \phi \sum_{t=2}^T (Y_{t-1} - \mu)^2 $$
IV. Resolvendo para $\phi$, temos o estimador de m√≠nimos quadrados:
$$ \hat{\phi} = \frac{\sum_{t=2}^T (Y_t - \mu)(Y_{t-1} - \mu)}{\sum_{t=2}^T (Y_{t-1} - \mu)^2} $$
V. Este resultado mostra que $\hat{\phi}$ pode ser calculado diretamente a partir dos dados, sem necessidade de m√©todos iterativos.
‚ñ†

This result reinforces the computational simplicity of the AR(1) model, as its parameters can be estimated directly.

### Limita√ß√µes na Precis√£o das Previs√µes de Longo Prazo
A simplicidade do modelo AR(1) traz um pre√ßo: uma menor precis√£o nas previs√µes de longo prazo. A previs√£o no AR(1) √© dada por [4.2.19]:
$$ \hat{Y}_{t+s|t} = \mu + \phi^s (Y_t - \mu). $$
O fator $\phi^s$ demonstra que a influ√™ncia da observa√ß√£o atual $Y_t$ na previs√£o decai geometricamente com o aumento do horizonte de previs√£o *s*. Para um processo estacion√°rio, $|\phi| < 1$, e o termo $\phi^s$ tende a zero quando *s* tende ao infinito, o que significa que a previs√£o converge para a m√©dia incondicional do processo, $\mu$. A converg√™ncia para a m√©dia implica que, para horizontes de longo prazo, a previs√£o n√£o considera nenhuma informa√ß√£o sobre a din√¢mica do processo al√©m de sua m√©dia.

> üí° **Exemplo Num√©rico:** Imagine um processo AR(1) com $\mu = 10$ e $\phi = 0.9$. Se o valor atual $Y_t = 20$, ent√£o a previs√£o para um per√≠odo a frente √© $\hat{Y}_{t+1|t} = 10 + 0.9 (20 - 10) = 19$. Para dois per√≠odos a frente a previs√£o √© $\hat{Y}_{t+2|t} = 10 + 0.9^2 (20 - 10) = 18.1$, e para tr√™s per√≠odos a frente a previs√£o √© $\hat{Y}_{t+3|t} = 10 + 0.9^3 (20 - 10) = 17.29$.  √Ä medida que o horizonte de previs√£o aumenta, a previs√£o se aproxima cada vez mais da m√©dia, convergindo para 10. Isto ilustra que o modelo perde a capacidade de fornecer previs√µes com maior detalhe, e apenas replica a m√©dia do processo. Isso acontece porque o modelo AR(1) se restringe a usar apenas o valor anterior, $Y_t$ para realizar a previs√£o, e n√£o √© capaz de modelar padr√µes de depend√™ncia complexos, que poderiam ser capturados por modelos com mais termos defasados.
>
> üí° **Exemplo Num√©rico:**  Considere o mesmo processo AR(1) com $\mu = 10$ e $\phi = 0.9$, e com o valor atual $Y_t = 20$. Vamos calcular as previs√µes para v√°rios passos √† frente e observar a converg√™ncia para $\mu$:
>
> | s (Passos √† frente) | $\hat{Y}_{t+s|t}$ |
> |---|---|
> | 1 | $10 + 0.9^1 (20 - 10) = 19.0$ |
> | 5 | $10 + 0.9^5 (20 - 10) \approx 15.90$ |
> | 10 | $10 + 0.9^{10} (20 - 10) \approx 13.49$ |
> | 20 | $10 + 0.9^{20} (20 - 10) \approx 11.21$ |
> | 50 | $10 + 0.9^{50} (20 - 10) \approx 10.05$ |
> | 100 | $10 + 0.9^{100} (20 - 10) \approx 10.00$ |
>
> Note que √† medida que *s* aumenta, as previs√µes se aproximam de $\mu = 10$. Isso demonstra como a influ√™ncia do valor atual $Y_t$ diminui ao longo do tempo, e a previs√£o tende para a m√©dia incondicional do processo.

A limita√ß√£o na precis√£o das previs√µes de longo prazo surge da incapacidade do modelo AR(1) de capturar depend√™ncias temporais de ordem superior. Um modelo AR(p) com *p > 1*, por exemplo, seria capaz de levar em conta informa√ß√µes sobre v√°rios valores defasados da vari√°vel, proporcionando uma previs√£o que potencialmente se ajusta melhor √† din√¢mica da s√©rie temporal. Por outro lado, a complexidade do modelo aumenta e, portanto, √© necess√°rio um trade-off entre precis√£o e custo computacional.

**Observa√ß√£o 2:** O erro quadr√°tico m√©dio de previs√£o para um processo AR(1), como j√° visto [^1], √© dado por:
$$ E[(Y_{t+s} - \hat{Y}_{t+s|t})^2] = \sigma^2 \frac{1 - \phi^{2s}}{1 - \phi^2}.$$
Este resultado demonstra que o erro de previs√£o aumenta com o horizonte *s*. No limite, quando *s* tende ao infinito, o erro converge para a vari√¢ncia incondicional do processo:
$$ \lim_{s \to \infty} E[(Y_{t+s} - \hat{Y}_{t+s|t})^2] = \frac{\sigma^2}{1 - \phi^2}.$$
Este limite indica que para horizontes longos, o erro de previs√£o se estabiliza em um valor que √© proporcional √† vari√¢ncia do ru√≠do branco e inversamente proporcional √† estabilidade do processo, medido pelo par√¢metro $\phi$.

> üí° **Exemplo Num√©rico:**  Suponha um modelo AR(1) com $\phi=0.8$ e $\sigma^2 = 4$. Vamos calcular o erro quadr√°tico m√©dio de previs√£o para diferentes horizontes *s*:
>
> | s (Passos √† frente) | $E[(Y_{t+s} - \hat{Y}_{t+s|t})^2]$ |
> |---|---|
> | 1 | $4 \cdot \frac{1 - 0.8^2}{1 - 0.8^2} = 4$ |
> | 2 | $4 \cdot \frac{1 - 0.8^4}{1 - 0.8^2} \approx 6.88$ |
> | 5 | $4 \cdot \frac{1 - 0.8^{10}}{1 - 0.8^2} \approx 10.72$ |
> | 10 | $4 \cdot \frac{1 - 0.8^{20}}{1 - 0.8^2} \approx 10.98$ |
> | $\infty$ | $4 / (1 - 0.8^2) = 4 / 0.36 \approx 11.11$ |
>
> Como podemos observar, o erro de previs√£o aumenta √† medida que o horizonte de previs√£o aumenta e se estabiliza em torno de 11.11. Este exemplo ilustra que a vari√¢ncia do erro de previs√£o de longo prazo para o modelo AR(1) √© sempre maior do que a vari√¢ncia do ru√≠do branco (4 neste caso), confirmando que as previs√µes de longo prazo s√£o menos precisas.

**Corol√°rio 2.1:**  A vari√¢ncia do erro de previs√£o de longo prazo para o modelo AR(1) √© sempre maior ou igual √† vari√¢ncia do ru√≠do branco, $\sigma^2$.

*Prova:*
I. Como demonstrado na Observa√ß√£o 2, o erro quadr√°tico m√©dio de previs√£o no longo prazo √© dado por $\frac{\sigma^2}{1-\phi^2}$.
II. Para que o processo AR(1) seja estacion√°rio, devemos ter $|\phi| < 1$. Isso implica que $-1 < \phi < 1$, e consequentemente $0 \leq \phi^2 < 1$.
III.  Subtraindo de 1, temos que $0 < 1-\phi^2 \leq 1$.
IV. Invertendo, obtemos $\frac{1}{1-\phi^2} \geq 1$.
V. Multiplicando por $\sigma^2$ (que √© sempre positivo), obtemos $\frac{\sigma^2}{1-\phi^2} \geq \sigma^2$.
VI. Portanto, a vari√¢ncia do erro de previs√£o de longo prazo √© sempre maior ou igual √† vari√¢ncia do ru√≠do branco, $\sigma^2$.
‚ñ†

This corollary directly follows from the previous observation and highlights the lower bound on the long-term forecast error variance. It also reinforces that the model is unable to get a long-term forecast better than simply using the mean and the noise of the process.

### O Trade-off entre Computa√ß√£o e Precis√£o
O processo AR(1) exemplifica claramente o trade-off entre computa√ß√£o e precis√£o na modelagem de s√©ries temporais. Ao adotar um modelo mais simples como o AR(1), obtemos vantagens computacionais significativas, o que √© crucial para aplica√ß√µes de tempo real e sistemas de baixo consumo de energia. No entanto, essa simplicidade vem com uma limita√ß√£o na capacidade de capturar padr√µes de depend√™ncia mais complexos e, portanto, na precis√£o das previs√µes de longo prazo. Modelos mais sofisticados, como ARMA(p,q) ou modelos n√£o lineares, podem oferecer maior precis√£o, mas com um custo computacional maior, tornando-os menos adequados para ambientes com restri√ß√µes computacionais.

A escolha do modelo apropriado deve ser guiada pelos requisitos espec√≠ficos da aplica√ß√£o. Se o foco √© em previs√µes de curto prazo e o tempo de computa√ß√£o √© crucial, o AR(1) pode ser a melhor op√ß√£o. Se, por outro lado, a precis√£o das previs√µes de longo prazo √© essencial, e o custo computacional √© secund√°rio, modelos mais complexos podem ser mais apropriados.

> üí° **Exemplo Num√©rico:** Em um sistema de monitoramento de temperatura com previs√µes de curto prazo, um modelo AR(1) pode ser adequado porque o tempo de resposta √© fundamental. J√° em um sistema de previs√£o de demanda por energia el√©trica em longo prazo, um modelo mais complexo, como um ARMA(p, q) com componentes sazonais, pode oferecer uma previs√£o mais precisa, apesar do maior custo computacional. A escolha do modelo deve ser uma an√°lise cuidadosa do cen√°rio de aplica√ß√£o.
>
> üí° **Exemplo Num√©rico:** Considere o desenvolvimento de um algoritmo de trading. Se o foco √© em transa√ß√µes de alta frequ√™ncia, o AR(1) pode ser utilizado como um primeiro passo, realizando previs√µes r√°pidas a partir do pre√ßo atual para os pr√≥ximos segundos. J√° para decis√µes de investimento de longo prazo, um modelo ARMA(p,q) que incorpore diferentes fatores de mercado seria mais apropriado, mesmo que a velocidade da previs√£o seja menor.
>
> üí° **Exemplo Num√©rico:** Para ilustrar o trade-off, vamos comparar o uso do AR(1) com um AR(2) usando um conjunto de dados simulados. Suponha que temos a seguinte s√©rie temporal:
>
> ```python
> import numpy as np
> import pandas as pd
> from statsmodels.tsa.ar_model import AutoReg
> from sklearn.metrics import mean_squared_error
>
> np.random.seed(42)
>
> # Simula um processo AR(2)
> def generate_ar2_data(n, phi1, phi2, sigma):
>     errors = np.random.normal(0, sigma, n)
>     y = np.zeros(n)
>     y[0] = errors[0]
>     y[1] = phi1 * y[0] + errors[1]
>     for t in range(2,n):
>         y[t] = phi1 * y[t-1] + phi2 * y[t-2] + errors[t]
>     return y
>
> # Generate data for testing
> n = 100
> phi1 = 0.8
> phi2 = -0.3
> sigma = 1
> data = generate_ar2_data(n, phi1, phi2, sigma)
>
> # Split data in train and test
> train_size = int(n * 0.8)
> train_data = data[:train_size]
> test_data = data[train_size:]
>
> # Fit AR(1) model
> model_ar1 = AutoReg(train_data, lags=1).fit()
>
> # Fit AR(2) model
> model_ar2 = AutoReg(train_data, lags=2).fit()
>
> # Generate predictions
> pred_ar1 = model_ar1.predict(start=len(train_data), end=len(data)-1)
> pred_ar2 = model_ar2.predict(start=len(train_data), end=len(data)-1)
>
> # Calculate MSE
> mse_ar1 = mean_squared_error(test_data, pred_ar1)
> mse_ar2 = mean_squared_error(test_data, pred_ar2)
>
> print(f"MSE AR(1): {mse_ar1:.4f}")
> print(f"MSE AR(2): {mse_ar2:.4f}")
> ```
>
> Este c√≥digo simula uma s√©rie temporal usando um processo AR(2), divide os dados em treino e teste, ajusta modelos AR(1) e AR(2) aos dados de treino e calcula o erro quadr√°tico m√©dio (MSE) nos dados de teste. Uma poss√≠vel sa√≠da seria:
>
> ```
> MSE AR(1): 2.4431
> MSE AR(2): 1.4756
> ```
>
> Este exemplo demonstra que o modelo AR(2) (mais complexo) pode ter uma precis√£o maior nos dados de teste (menor MSE) em rela√ß√£o ao AR(1) (mais simples), mas com um custo computacional maior, j√° que precisa estimar mais par√¢metros.

A escolha do modelo n√£o √©, portanto, um processo bin√°rio. Muitas vezes, a melhor estrat√©gia √© come√ßar com um modelo simples como o AR(1) e, em seguida, aumentar a complexidade do modelo √† medida que as necessidades espec√≠ficas da aplica√ß√£o s√£o melhor compreendidas. A valida√ß√£o do modelo com dados reais √© essencial para garantir que o compromisso entre computa√ß√£o e precis√£o esteja alinhado com os objetivos do projeto.

**Proposi√ß√£o 1:** Em condi√ß√µes estacion√°rias, o erro quadr√°tico m√©dio de previs√£o de longo prazo para o modelo AR(1) √© sempre maior ou igual ao erro quadr√°tico m√©dio do ru√≠do branco do processo original.

*Prova:*
I. O erro quadr√°tico m√©dio de previs√£o para um processo AR(1) √© dado por:
    $E[(Y_{t+s} - \hat{Y}_{t+s|t})^2] = \sigma^2 \frac{1 - \phi^{2s}}{1 - \phi^2}$.
II. No limite, quando $s \to \infty$, o termo $\phi^{2s}$ tende a zero (para $|\phi|<1$), e o erro de previs√£o converge para a vari√¢ncia incondicional do processo:
    $\lim_{s \to \infty} E[(Y_{t+s} - \hat{Y}_{t+s|t})^2] = \frac{\sigma^2}{1 - \phi^2}$.
III. Para que o processo AR(1) seja estacion√°rio, devemos ter $|\phi| < 1$, o que implica que $0 \leq \phi^2 < 1$.
IV. Portanto, $0 < 1 - \phi^2 \leq 1$.
V.  Invertendo, obtemos $\frac{1}{1-\phi^2} \geq 1$.
VI. Multiplicando por $\sigma^2$ (que √© sempre positivo), temos $\frac{\sigma^2}{1-\phi^2} \geq \sigma^2$.
VII. Assim, o erro de previs√£o de longo prazo para o modelo AR(1) √© sempre maior ou igual √† vari√¢ncia do ru√≠do branco.
‚ñ†

This proposition demonstrates that there's a fundamental limit to the accuracy achievable by the AR(1) model in the long run; its prediction will always have an error greater or equal than the noise from the original data.

**Proposi√ß√£o 1.1:** O modelo AR(1) √© um caso especial do modelo AR(p) onde p=1.
*Prova:*
I. Um modelo AR(p) √© definido como:
$Y_t = \mu + \phi_1(Y_{t-1} - \mu) + \phi_2(Y_{t-2} - \mu) + \ldots + \phi_p(Y_{t-p} - \mu) + \epsilon_t$
II. Se fizermos p=1, ent√£o todos os coeficientes $\phi_i$ para $i>1$ se tornam 0, e temos:
$Y_t = \mu + \phi_1(Y_{t-1} - \mu) + \epsilon_t$
III. Esta √© a defini√ß√£o do modelo AR(1), onde $\phi = \phi_1$.
IV. Portanto, o modelo AR(1) √© um caso particular do modelo AR(p).
‚ñ†

This new proposition explicitly states a fundamental connection between the AR(1) and the general AR(p) models, showing that the AR(1) is simply an AR(p) model with p=1. This reinforces that AR(1) is a simpler case of a more general family of models.

### Conclus√£o
O modelo AR(1) ilustra o trade-off fundamental entre a simplicidade computacional e a precis√£o das previs√µes, especialmente em horizontes de longo prazo. Sua estrutura recursiva e seu baixo custo computacional o tornam uma ferramenta valiosa em sistemas de alta performance e aplica√ß√µes de tempo real. Entretanto, a converg√™ncia da previs√£o para a m√©dia e a incapacidade de capturar depend√™ncias de ordem superior resultam em menor precis√£o em horizontes de longo prazo. A escolha entre o AR(1) e modelos mais complexos deve ser feita considerando os requisitos espec√≠ficos da aplica√ß√£o, com uma an√°lise cuidadosa do compromisso entre custo computacional e n√≠vel de precis√£o desejado, e a valida√ß√£o do modelo com dados reais. O AR(1) se revela um modelo √∫til para o estudo e compreens√£o dos conceitos b√°sicos de s√©ries temporais, e serve como um importante ponto de partida para a an√°lise de modelos mais complexos e sofisticados.

### Refer√™ncias
[^1]:  Refer√™ncia ao contexto onde o modelo AR(1) e suas propriedades de previs√£o foram detalhadas.
[^2]: Refer√™ncia ao contexto onde foi discutida a efici√™ncia computacional do modelo AR(1) e suas aplica√ß√µes.
[^3]: Refer√™ncia ao contexto onde a implementa√ß√£o recursiva do modelo AR(1) foi explorada e sua adequa√ß√£o a sistemas de alta performance.
<!-- END -->
