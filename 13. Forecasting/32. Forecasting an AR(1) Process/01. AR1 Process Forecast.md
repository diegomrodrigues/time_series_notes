## Previs√£o no Processo AR(1): Decaimento Geom√©trico e Converg√™ncia para a M√©dia
### Introdu√ß√£o
Em continuidade √† explora√ß√£o dos modelos de previs√£o, este cap√≠tulo se aprofunda nas caracter√≠sticas espec√≠ficas da previs√£o em um processo **Autorregressivo de ordem 1 (AR(1))**. Como vimos anteriormente [^1], a previs√£o linear √≥tima busca minimizar o erro quadr√°tico m√©dio, e, no contexto de modelos AR, essa busca se traduz em formula√ß√µes que exploram a depend√™ncia da vari√°vel em rela√ß√£o a seus valores passados. No processo AR(1), essa depend√™ncia √© simplificada, envolvendo apenas o valor da vari√°vel no per√≠odo anterior.

### Conceitos Fundamentais
O modelo AR(1) √© definido pela equa√ß√£o [4.2.14]:
$$ (1 - \phi L)(Y_t - \mu) = \epsilon_t, $$
onde $Y_t$ √© a vari√°vel no tempo *t*, $\mu$ √© a m√©dia do processo, $\phi$ √© o **coeficiente autoregressivo**, $L$ √© o operador de defasagem, e $\epsilon_t$ √© o ru√≠do branco. A partir da equa√ß√£o acima, temos que a previs√£o linear √≥tima de $Y_{t+s}$ baseada em $Y_t$, $Y_{t-1}$, ... , denotada por $\hat{Y}_{t+s|t}$, para um processo AR(1) estacion√°rio, √© obtida atrav√©s da aplica√ß√£o da f√≥rmula de Wiener-Kolmogorov [4.2.16]:
$$ \hat{Y}_{t+s|t} = \mu + \frac{\psi(L)}{L^s} \frac{1}{\psi(L)} (Y_t - \mu). $$
Para o processo AR(1), sabemos que $\psi(L) = \frac{1}{1-\phi L}$ [^1]. Expandindo essa express√£o em uma s√©rie de pot√™ncias, obtemos $\psi(L) = 1 + \phi L + \phi^2 L^2 + \phi^3 L^3 + \dots$ [4.2.17]. Aplicando o operador de aniquila√ß√£o [4.2.8], temos [4.2.18]:
$$ \left[\frac{\psi(L)}{L^s}\right]_+ = \phi^s + \phi^{s+1}L + \phi^{s+2}L^2 + \dots = \frac{\phi^s}{1-\phi L}. $$

**Prova da Express√£o para  $\left[\frac{\psi(L)}{L^s}\right]_+$**
I. Temos que $\psi(L) = \frac{1}{1 - \phi L}$. Ent√£o, $\frac{\psi(L)}{L^s} = \frac{1}{L^s(1-\phi L)}$.
II. Expans√£o da s√©rie de pot√™ncia: $\psi(L) = 1 + \phi L + \phi^2 L^2 + \phi^3 L^3 + \ldots$.
III. Multiplicando por $L^{-s}$, obtemos: $\frac{\psi(L)}{L^s} = L^{-s} + \phi L^{1-s} + \phi^2 L^{2-s} + \phi^3 L^{3-s} + \ldots$.
IV. Aplicando o operador $[\cdot]_+$ que remove termos com expoentes negativos de L,  temos $\left[\frac{\psi(L)}{L^s}\right]_+ =  \phi^s + \phi^{s+1}L + \phi^{s+2}L^2 + \ldots$.
V. Fatorando $\phi^s$, obtemos $\left[\frac{\psi(L)}{L^s}\right]_+ = \phi^s(1 + \phi L + \phi^2 L^2 + \ldots) = \phi^s \frac{1}{1 - \phi L} = \frac{\phi^s}{1 - \phi L}$.
‚ñ†

Substituindo na f√≥rmula de Wiener-Kolmogorov, obtemos [4.2.19]:
$$\hat{Y}_{t+s|t} = \mu + \frac{\phi^s}{1-\phi L}(1-\phi L)(Y_t - \mu) = \mu + \phi^s(Y_t - \mu).$$

**Prova da Express√£o para $\hat{Y}_{t+s|t}$**
I. Substituindo $\frac{\psi(L)}{L^s} = \frac{\phi^s}{1-\phi L}$ e $\psi(L) = \frac{1}{1-\phi L}$ na f√≥rmula de Wiener-Kolmogorov:
   $$\hat{Y}_{t+s|t} = \mu + \frac{\phi^s}{1-\phi L} (1-\phi L) (Y_t - \mu)$$
II. Simplificando os termos $(1-\phi L)$ no numerador e denominador:
    $$\hat{Y}_{t+s|t} = \mu + \phi^s (Y_t - \mu)$$
‚ñ†

> üí° **Exemplo Num√©rico:** Suponha que temos um processo AR(1) com m√©dia $\mu = 10$, coeficiente autoregressivo $\phi = 0.7$, e um valor atual observado de $Y_t = 20$. Vamos calcular as previs√µes para $s=1, 2, 3$ e analisar como a previs√£o converge para a m√©dia:
>
> - Para $s=1$:
>  $\hat{Y}_{t+1|t} = 10 + 0.7^1(20 - 10) = 10 + 0.7(10) = 17$
> - Para $s=2$:
>  $\hat{Y}_{t+2|t} = 10 + 0.7^2(20 - 10) = 10 + 0.49(10) = 14.9$
> - Para $s=3$:
>  $\hat{Y}_{t+3|t} = 10 + 0.7^3(20 - 10) = 10 + 0.343(10) = 13.43$
>
> Observamos que, √† medida que $s$ aumenta, a previs√£o se aproxima cada vez mais da m√©dia $\mu = 10$. O fator $\phi^s$ reduz o peso da observa√ß√£o $Y_t$ na previs√£o, demonstrando o decaimento geom√©trico.

A partir dessa equa√ß√£o, observamos que a previs√£o de $Y_{t+s}$ decai geometricamente para a m√©dia $\mu$ √† medida que o horizonte de previs√£o *s* aumenta, devido ao fator $\phi^s$. Este √© um resultado central para a compreens√£o da din√¢mica da previs√£o em modelos AR(1).

Este decaimento geom√©trico √© uma caracter√≠stica intr√≠nseca do processo AR(1), e sua taxa √© diretamente controlada pelo coeficiente autoregressivo $\phi$. Se $|\phi| < 1$, o processo √© estacion√°rio, e o decaimento para a m√©dia √© garantido. Quando o horizonte de previs√£o *s* aumenta, o peso dos valores passados de $Y$ na previs√£o se torna cada vez menor, e a previs√£o converge para a m√©dia incondicional do processo.

**Observa√ß√£o 1:** Uma forma alternativa de expressar a previs√£o $\hat{Y}_{t+s|t}$ √© como uma combina√ß√£o linear da m√©dia $\mu$ e do desvio de $Y_t$ em rela√ß√£o a essa m√©dia. Especificamente, podemos reescrever a equa√ß√£o da previs√£o como $\hat{Y}_{t+s|t} = (1-\phi^s)\mu + \phi^s Y_t$. Essa formula√ß√£o destaca que, √† medida que $s$ aumenta e $\phi^s$ se aproxima de zero (dado que $|\phi|<1$ para estacionariedade), a previs√£o tende √† m√©dia do processo, $\mu$.

> üí° **Exemplo Num√©rico:** Usando o mesmo exemplo anterior com $\mu=10$, $\phi=0.7$ e $Y_t = 20$, vamos recalcular as previs√µes usando a forma alternativa:
> - Para $s=1$:
> $\hat{Y}_{t+1|t} = (1 - 0.7^1)10 + 0.7^1(20) = 0.3 \times 10 + 0.7 \times 20 = 3 + 14 = 17$
> - Para $s=2$:
> $\hat{Y}_{t+2|t} = (1 - 0.7^2)10 + 0.7^2(20) = 0.51 \times 10 + 0.49 \times 20 = 5.1 + 9.8 = 14.9$
> - Para $s=3$:
> $\hat{Y}_{t+3|t} = (1 - 0.7^3)10 + 0.7^3(20) = 0.657 \times 10 + 0.343 \times 20 = 6.57 + 6.86 = 13.43$
>
> Como esperado, os resultados s√£o id√™nticos √† forma anterior, mas esta formula√ß√£o refor√ßa a ideia de que a previs√£o √© uma combina√ß√£o ponderada entre a m√©dia e o valor atual, com o peso do valor atual diminuindo √† medida que *s* aumenta.

Al√©m disso, a partir de [4.2.17], sabemos que os pesos $\psi_i$ s√£o dados por $\phi^i$. Portanto, o erro quadr√°tico m√©dio de previs√£o para um horizonte *s* √© dado por [4.2.6]:
$$ E[(Y_{t+s} - \hat{Y}_{t+s|t})^2] = (1 + \phi^2 + \phi^4 + \dots + \phi^{2(s-1)})\sigma^2. $$
O erro de previs√£o aumenta com o horizonte *s* e, √† medida que *s* tende a infinito, converge para $\frac{\sigma^2}{1-\phi^2}$, que √© a vari√¢ncia incondicional de *Y* para o processo AR(1).

**Lema 1:** O erro quadr√°tico m√©dio de previs√£o para um horizonte *s* em um processo AR(1) pode ser expresso em termos da vari√¢ncia do ru√≠do branco, $\sigma^2$, e o coeficiente autoregressivo, $\phi$, como:
$$ E[(Y_{t+s} - \hat{Y}_{t+s|t})^2] = \sigma^2 \frac{1 - \phi^{2s}}{1 - \phi^2}.$$
*Prova:*  A express√£o dada para o erro quadr√°tico m√©dio de previs√£o √© uma soma geom√©trica. Utilizando a f√≥rmula para a soma de uma progress√£o geom√©trica finita, $S_n = a_1\frac{1-r^n}{1-r}$, onde $a_1 = 1$ e $r=\phi^2$ e o n√∫mero de termos √© $s$, temos: $1 + \phi^2 + \phi^4 + \dots + \phi^{2(s-1)} = \frac{1-(\phi^2)^s}{1-\phi^2} = \frac{1-\phi^{2s}}{1-\phi^2}$. Multiplicando por $\sigma^2$ obtemos o resultado desejado.
‚ñ†

> üí° **Exemplo Num√©rico:** Consideremos o mesmo processo AR(1) com $\phi = 0.7$ e vamos supor que a vari√¢ncia do ru√≠do branco √© $\sigma^2 = 4$. Vamos calcular o erro quadr√°tico m√©dio de previs√£o para diferentes horizontes *s*:
> - Para $s=1$:
>  $E[(Y_{t+1} - \hat{Y}_{t+1|t})^2] = 4 \times \frac{1 - 0.7^{2\times 1}}{1 - 0.7^2} = 4 \times \frac{1 - 0.49}{1 - 0.49} = 4 \times 1 = 4$
> - Para $s=2$:
>  $E[(Y_{t+2} - \hat{Y}_{t+2|t})^2] = 4 \times \frac{1 - 0.7^{2\times 2}}{1 - 0.7^2} = 4 \times \frac{1 - 0.2401}{1 - 0.49} = 4 \times \frac{0.7599}{0.51} \approx 5.96$
> - Para $s=3$:
>  $E[(Y_{t+3} - \hat{Y}_{t+3|t})^2] = 4 \times \frac{1 - 0.7^{2\times 3}}{1 - 0.7^2} = 4 \times \frac{1 - 0.117649}{1 - 0.49} = 4 \times \frac{0.882351}{0.51} \approx 6.92$
>
> Observamos que o erro de previs√£o aumenta √† medida que o horizonte *s* cresce.

**Corol√°rio 1:** O limite do erro quadr√°tico m√©dio de previs√£o, quando o horizonte de previs√£o $s$ tende ao infinito, para um processo AR(1) estacion√°rio, √© dado pela vari√¢ncia incondicional do processo:
$$ \lim_{s \to \infty} E[(Y_{t+s} - \hat{Y}_{t+s|t})^2] = \frac{\sigma^2}{1-\phi^2}. $$
*Prova:* Tomando o limite da express√£o do Lema 1 quando $s$ tende a infinito e sabendo que $|\phi| < 1$ para estacionariedade, temos que $\phi^{2s}$ tende a zero. Portanto, $\lim_{s \to \infty} \frac{1-\phi^{2s}}{1-\phi^2} = \frac{1}{1-\phi^2}$. Multiplicando por $\sigma^2$, obtemos o resultado.
‚ñ†

> üí° **Exemplo Num√©rico:** Usando novamente $\phi = 0.7$ e $\sigma^2 = 4$, o limite do erro quadr√°tico m√©dio quando $s \rightarrow \infty$ √©:
> $$ \lim_{s \to \infty} E[(Y_{t+s} - \hat{Y}_{t+s|t})^2] = \frac{4}{1 - 0.7^2} = \frac{4}{1 - 0.49} = \frac{4}{0.51} \approx 7.84 $$
>
> Este valor representa a vari√¢ncia incondicional do processo AR(1) e indica que o erro de previs√£o se estabiliza neste patamar quando o horizonte de previs√£o √© muito grande.

### Conclus√£o
Em resumo, a previs√£o em um modelo AR(1) √© caracterizada por um decaimento geom√©trico para a m√©dia $\mu$ do processo. Este decaimento √© regido pelo coeficiente autoregressivo $\phi$, e a previs√£o converge para a m√©dia √† medida que o horizonte de previs√£o aumenta. Este resultado √© uma consequ√™ncia da natureza autorregressiva do modelo, onde o valor atual da vari√°vel depende do seu valor no per√≠odo anterior. Portanto, o modelo AR(1) oferece um framework de previs√£o que, apesar de simples, consegue capturar uma din√¢mica fundamental das s√©ries temporais em rela√ß√£o ao seu comportamento em rela√ß√£o √† m√©dia e √† sua varia√ß√£o com o horizonte de previs√£o.

### Refer√™ncias
[^1]:  Refer√™ncia ao contexto onde o modelo AR(1) e as f√≥rmulas de Wiener-Kolmogorov foram introduzidas.
<!-- END -->
