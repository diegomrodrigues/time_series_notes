## Previs√£o Linear √ìtima com Representa√ß√µes AR($\infty$)

### Introdu√ß√£o
Expandindo o conceito de **previs√£o** com base em valores defasados de Y, apresentado anteriormente [^4.2.1], este cap√≠tulo se concentra em como calcular previs√µes √≥timas, assumindo que o processo pode ser expresso como uma representa√ß√£o AR($\infty$). Como exploramos, a representa√ß√£o AR($\infty$) permite expressar o erro de previs√£o, $\epsilon_t$, em termos de valores passados da s√©rie temporal $Y$ [^4.2.10, 4.2.11]. Sob as condi√ß√µes de estacionariedade e invertibilidade, √© poss√≠vel derivar f√≥rmulas de previs√£o que dependem apenas dos valores passados de Y. Este cap√≠tulo visa detalhar como as previs√µes podem ser calculadas substituindo a representa√ß√£o AR($\infty$) na f√≥rmula de previs√£o linear, explorando o uso da f√≥rmula de Wiener-Kolmogorov e suas implica√ß√µes pr√°ticas.

### Conceitos Fundamentais

Para processos que s√£o estacion√°rios e invert√≠veis, a representa√ß√£o AR($\infty$) permite expressar o processo $Y_t$ como uma combina√ß√£o linear de seus pr√≥prios valores defasados e um termo de erro $\epsilon_t$. A representa√ß√£o √© dada por:

$$
\eta(L)(Y_t - \mu) = \epsilon_t,
$$

onde $\eta(L) = \sum_{j=0}^{\infty} \eta_j L^j$, $\eta_0 = 1$ e $\sum_{j=0}^{\infty} |\eta_j| < \infty$. Como discutido [^4.2.11], o polin√¥mio AR $\eta(L)$ est√° relacionado ao polin√¥mio MA $\psi(L)$ atrav√©s da rela√ß√£o $\eta(L) = [\psi(L)]^{-1}$. Esta rela√ß√£o √© fundamental porque permite que processos MA(q) e ARMA(p,q) sejam expressos como AR($\infty$) quando eles satisfazem as condi√ß√µes de estacionariedade e invertibilidade.

**Proposi√ß√£o 1:** *A condi√ß√£o de invertibilidade para um processo MA(q) garante a exist√™ncia de uma representa√ß√£o AR($\infty$), permitindo expressar o processo em termos de seus valores defasados.*

*Prova:* A invertibilidade de um processo MA(q) implica que o polin√¥mio $\psi(L)$ possui todas as suas ra√≠zes fora do c√≠rculo unit√°rio. Isso permite que a fun√ß√£o $\frac{1}{\psi(L)}$ seja expressa como uma s√©rie convergente $\eta(L) = \sum_{j=0}^{\infty} \eta_j L^j$, com $\sum_{j=0}^{\infty} |\eta_j| < \infty$.  A representa√ß√£o $\eta(L)$ define os pesos da combina√ß√£o linear infinita de valores defasados de Y, o que garante a representa√ß√£o AR($\infty$). ‚ñ†

A representa√ß√£o AR($\infty$) tem uma rela√ß√£o direta com a previs√£o linear. Se o erro $\epsilon_t$ puder ser expresso como fun√ß√£o dos valores passados de $Y_t$, o erro de previs√£o ser√° ortogonal a esses valores passados, conforme a condi√ß√£o [^4.1.10]:

$$
E[(Y_{t+1} - \alpha'X_t)X_t] = 0
$$

Esta condi√ß√£o garante que $\alpha'X_t$ seja a melhor previs√£o linear de $Y_{t+1}$ baseado nas informa√ß√µes de $X_t$, um vetor que cont√©m os valores defasados de $Y$. Como vimos, a representa√ß√£o AR(p) satisfaz essa condi√ß√£o [^4.2.14], e o mesmo ocorre com MA(q) invert√≠veis e ARMA(p,q) [^4.2.13].

**Lema 1.1:** *A ortogonalidade do erro de previs√£o em rela√ß√£o aos valores passados de Y √© uma condi√ß√£o necess√°ria para que a previs√£o linear seja √≥tima.*

*Prova:* A condi√ß√£o $E[(Y_{t+1} - \alpha'X_t)X_t] = 0$ estabelece que o erro de previs√£o $(Y_{t+1} - \alpha'X_t)$ √© ortogonal aos valores defasados de $Y$ contidos em $X_t$. Se o erro n√£o fosse ortogonal a esses valores, ent√£o existiria uma combina√ß√£o linear dos valores defasados que reduziria o erro de previs√£o, contradizendo a otimalidade da previs√£o $\alpha'X_t$. Portanto, a ortogonalidade √© uma condi√ß√£o necess√°ria para que a previs√£o linear seja √≥tima no sentido de minimizar o erro quadr√°tico m√©dio. ‚ñ†

Uma vez que tenhamos a representa√ß√£o AR($\infty$) do processo, podemos utilizar a f√≥rmula de previs√£o de Wiener-Kolmogorov para expressar a previs√£o √≥tima linear de $Y_{t+s}$ com base nos valores passados de $Y$:

$$
E[Y_{t+s}|Y_t, Y_{t-1}, \ldots] = \mu + \left[ \frac{\psi(L)}{L^s} \right]_+ \frac{1}{\psi(L)}(Y_t - \mu)
$$
Essa formula [^4.2.16] indica que a previs√£o √≥tima √© dada pela m√©dia $\mu$ mais uma parte que depende dos valores passados de $Y$, filtrada pelos polin√¥mios $\psi(L)$ e sua inversa. O operador $[\cdot]_+$ elimina os termos com pot√™ncias negativas de $L$, indicando que apenas os valores presentes e passados de $Y$ s√£o considerados na previs√£o. Para aplicar essa formula na pr√°tica, √© necess√°rio entender como a representa√ß√£o $\psi(L)$ e sua inversa $\eta(L)$ podem ser utilizadas.

**Lema 2:** *A substitui√ß√£o da representa√ß√£o AR(‚àû) na f√≥rmula de proje√ß√£o linear resulta na f√≥rmula de Wiener-Kolmogorov, que expressa a melhor previs√£o linear de $Y_{t+s}$ com base no hist√≥rico de valores passados de Y sob condi√ß√µes de estacionariedade e invertibilidade.*

*Prova:* A prova deste lema envolve a manipula√ß√£o da representa√ß√£o AR(‚àû) e sua substitui√ß√£o na f√≥rmula de previs√£o linear. Inicialmente, a f√≥rmula de previs√£o linear geral √© dada pela proje√ß√£o ortogonal, como visto anteriormente.

I. A representa√ß√£o AR(‚àû) do processo √© dada por $\eta(L)(Y_t - \mu) = \epsilon_t$, onde $\eta(L) = [\psi(L)]^{-1}$
II. Substituindo $Y_t$ por $\psi(L)\epsilon_t + \mu$, e utilizando o resultado de [^4.2.9] , temos a previs√£o de $Y_{t+s}$ como fun√ß√£o de $\epsilon$ passado
     $E[Y_{t+s}|\epsilon_t, \epsilon_{t-1},...] = \mu + \left[ \frac{\psi(L)}{L^s} \right]_+ \epsilon_t$.
III. Agora, substituindo $\epsilon_t$ pela sua representa√ß√£o em fun√ß√£o de $Y$ na f√≥rmula acima, temos
     $ E[Y_{t+s}|Y_t, Y_{t-1},...] =  \mu + \left[ \frac{\psi(L)}{L^s} \right]_+ \eta(L)(Y_t-\mu)$
IV. Como $\eta(L) = [\psi(L)]^{-1}$, a express√£o acima se torna
$ E[Y_{t+s}|Y_t, Y_{t-1},...] = \mu + \left[ \frac{\psi(L)}{L^s} \right]_+ \frac{1}{\psi(L)}(Y_t - \mu)$.
V. Essa √∫ltima express√£o √© exatamente a formula de Wiener-Kolmogorov, mostrando que a substitui√ß√£o da representa√ß√£o AR(‚àû) na formula de proje√ß√£o linear resulta na f√≥rmula de Wiener-Kolmogorov. ‚ñ†

Para demonstrar como a f√≥rmula de Wiener-Kolmogorov pode ser aplicada na pr√°tica, consideremos alguns casos espec√≠ficos.

**Exemplo 1: Processo AR(1)**

Para um processo AR(1), temos $\psi(L) = \frac{1}{1-\phi L}$. A previs√£o de um passo √† frente (s=1) √© dada por [^4.2.19]:

$$
E[Y_{t+1}|Y_t, Y_{t-1}, \ldots] = \mu + \phi(Y_t - \mu).
$$
Esta express√£o mostra que a melhor previs√£o linear de $Y_{t+1}$ √© a m√©dia $\mu$ mais um fator que depende da diferen√ßa entre o valor atual $Y_t$ e a m√©dia, ponderado pelo coeficiente $\phi$.

> üí° **Exemplo Num√©rico:** Suponha que temos um processo AR(1) com $\mu = 5$ e $\phi = 0.7$. Se o valor atual da s√©rie temporal √© $Y_t = 12$, ent√£o a previs√£o de um passo √† frente √©:
>
> $E[Y_{t+1}|Y_t, Y_{t-1}, \ldots] = 5 + 0.7(12 - 5) = 5 + 0.7(7) = 5 + 4.9 = 9.9$.
>
> Isso significa que, com base no modelo AR(1) e no valor atual de 12, a melhor previs√£o para o pr√≥ximo per√≠odo √© 9.9. Se $Y_t$ fosse 6, a previs√£o seria $5 + 0.7(6-5) = 5.7$. Este exemplo ilustra como a previs√£o √© influenciada pelo valor atual e pela m√©dia do processo, ponderados pelo par√¢metro $\phi$.

**Exemplo 2: Processo MA(1)**

Para um processo MA(1) na forma invert√≠vel, temos $\psi(L) = 1 + \theta L$. A previs√£o de um passo √† frente (s=1) √© dada por [^4.2.30]:
$$
E[Y_{t+1}|Y_t, Y_{t-1}, \ldots] = \mu + \theta(Y_t - \mu) - \theta^2(Y_{t-1} - \mu) + \theta^3(Y_{t-2} - \mu) - \ldots.
$$
Na pratica, truncamos a s√©rie no n√∫mero de valores que observamos.  Observe que, para derivar este resultado,  o termo $[(1+\theta L)L^{-1}]_+$ elimina o termo com pot√™ncia negativa de L, e a expans√£o da fra√ß√£o $\frac{1}{1+\theta L}$ resulta na representa√ß√£o invert√≠vel. A previs√£o, neste caso, depende de uma combina√ß√£o linear decrescente dos valores defasados da s√©rie temporal, ponderados por pot√™ncias do par√¢metro $\theta$.

**Lema 2.1** *A converg√™ncia da representa√ß√£o AR($\infty$) para processos MA(q) invert√≠veis garante que a previs√£o √≥tima pode ser aproximada com um n√∫mero finito de termos defasados.*

*Prova:* Para um processo MA(q) invert√≠vel, $\eta(L) = [\psi(L)]^{-1}$ √© um polin√¥mio de grau infinito, onde os coeficientes $\eta_j$ decaem geometricamente em magnitude. Devido √† condi√ß√£o de invertibilidade, $|\theta|<1$,  portanto, as pot√™ncias de $\theta$  convergem para zero, conforme $j$ aumenta. Assim, a contribui√ß√£o dos valores defasados mais distantes para a previs√£o se torna cada vez menor. Na pr√°tica, podemos truncar a expans√£o da representa√ß√£o AR($\infty$) em um n√∫mero finito de termos, obtendo uma aproxima√ß√£o da previs√£o √≥tima, com um erro que diminui com o n√∫mero de termos inclu√≠dos. ‚ñ†
> üí° **Exemplo Num√©rico:** Considere o processo MA(1) com $\theta = 0.6$ e $\mu= 10$. Se temos $Y_t= 15, Y_{t-1}= 12$ e $Y_{t-2}=10$, uma aproxima√ß√£o da previs√£o √© $ E[Y_{t+1}|Y_t, Y_{t-1}, \ldots] =  10 + 0.6(15 - 10) - 0.6^2(12 - 10) + 0.6^3(10-10) = 10 + 0.6(5) - 0.36(2) = 10 + 3 - 0.72 = 12.28.$
>
> Nesta previs√£o, demos maior peso ao valor mais recente $Y_t=15$, com um peso de 0.6, e o valor $Y_{t-1}=12$ tem um peso menor de -0.36. Valores mais defasados como $Y_{t-2}$ teriam um peso ainda menor. A previs√£o se aproxima da m√©dia, mas considera os valores recentes da s√©rie temporal.

**Exemplo 3: Processo ARMA(1,1)**

Para um processo ARMA(1,1) dado por $(1 - \phi L)(Y_t - \mu) = (1 + \theta L)\epsilon_t$, temos $\psi(L) = \frac{1 + \theta L}{1 - \phi L}$. A previs√£o de um passo √† frente √© dada por [^4.2.39]:

$$
E[Y_{t+1}|Y_t, Y_{t-1}, \ldots] = \mu + \frac{\phi + \theta}{1 + \theta L} (Y_t - \mu).
$$
Expandindo, temos:
$$
E[Y_{t+1}|Y_t, Y_{t-1}, \ldots] =  \mu + \frac{(\phi + \theta)}{1 + \theta L}(Y_t - \mu)  = \mu + (\phi + \theta)(Y_t - \mu) - \theta(\phi + \theta)(Y_{t-1} - \mu) + \theta^2(\phi + \theta)(Y_{t-2} - \mu)\ldots
$$

**Corol√°rio 3.1:** *A previs√£o para o processo ARMA(1,1) combina caracter√≠sticas dos modelos AR(1) e MA(1), com um efeito de mem√≥ria estendida que envolve tanto o par√¢metro autoregressivo ($\phi$) quanto o par√¢metro da m√©dia m√≥vel ($\theta$).*

*Prova:* A f√≥rmula de previs√£o para ARMA(1,1) mostra que a previs√£o depende n√£o apenas do valor atual ($Y_t$), mas tamb√©m de valores defasados ($Y_{t-1}, Y_{t-2},\ldots$), cada um ponderado por termos que envolvem tanto $\phi$ quanto $\theta$. A influ√™ncia de cada valor defasado √© controlada tanto pelo termo $\phi$ que aparece implicitamente na expans√£o de $\frac{1}{1+\theta L}$ quanto pelo termo $\theta$ que multiplica a diferen√ßa $(Y_t-\mu)$. Isso caracteriza o processo ARMA(1,1) como uma combina√ß√£o dos modelos AR(1) e MA(1), capturando um espectro maior de depend√™ncias temporais. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere o processo ARMA(1,1) com $\phi=0.4$, $\theta=0.5$ e $\mu=5$. Para $Y_t=8$ e $Y_{t-1}=6$, a previs√£o √© $E[Y_{t+1}|Y_t, Y_{t-1}, \ldots] = 5 + \frac{0.4+0.5}{1+0.5L}(8-5) = 5 + \frac{0.9}{1+0.5L}(3) = 5 + (0.9)(3) -0.5(0.9)(3) + \ldots = 5 + 2.7 -1.35 + \ldots$
> Na pr√°tica aproximamos a soma infinita pelos valores dispon√≠veis de Y. Uma aproxima√ß√£o pode ser $5 + 2.7 -1.35 = 6.35$.
>
> Aqui, a previs√£o de 6.35  √© resultado da m√©dia do processo (5), ajustada pelo valor atual ($Y_t=8$) e pelo valor defasado ($Y_{t-1}=6$), com pesos que dependem de $\phi$ e $\theta$. O peso do valor atual √© 0.9 e do valor defasado √© -0.45. Este exemplo mostra como o modelo ARMA(1,1) considera tanto a autoregress√£o quanto a m√©dia m√≥vel para gerar a previs√£o.

Esses exemplos demonstram que, sob condi√ß√µes de estacionariedade e invertibilidade, a previs√£o √≥tima linear pode ser expressa atrav√©s da substitui√ß√£o da representa√ß√£o AR($\infty$) na f√≥rmula de previs√£o linear, resultando na f√≥rmula de Wiener-Kolmogorov.

### Conclus√£o

Este cap√≠tulo aprofundou a metodologia de previs√£o linear √≥tima utilizando representa√ß√µes AR($\infty$). Ao substituir a representa√ß√£o AR($\infty$) na f√≥rmula de previs√£o linear, obtemos a f√≥rmula de Wiener-Kolmogorov. Esta f√≥rmula possibilita derivar as previs√µes √≥timas baseadas nos valores defasados de $Y$, desde que os processos sejam estacion√°rios e invert√≠veis. Casos espec√≠ficos, como modelos AR(1), MA(1) e ARMA(1,1) foram utilizados para demonstrar como essa f√≥rmula pode ser aplicada na pr√°tica. A compreens√£o da rela√ß√£o entre a representa√ß√£o AR($\infty$) e a f√≥rmula de Wiener-Kolmogorov √© fundamental para a modelagem e previs√£o de s√©ries temporais, onde a informa√ß√£o relevante para a previs√£o est√° contida nos valores passados da pr√≥pria s√©rie.

### Refer√™ncias
[^4.2.1]:  Expression [4.1.1] is known as the mean squared error associated with the forecast
$Y^*_{t+1|t}$, denoted
$MSE(Y^*_{t+1|t}) = E(Y_{t+1} - Y^*_{t+1|t})^2$.

[^4.2.10]: We now restrict the class of forecasts considered by requiring the forecast
$Y^*_{t+1|t}$ to be a linear function of $X_t$:
$Y^*_{t+1|t} = \alpha'X_t.$

[^4.2.11]: Suppose further that the AR polynomial $\eta(L)$ and the MA polynomial $\psi(L)$ are related by
$\eta(L) = [\psi(L)]^{-1}.$

[^4.2.13]: An MA(q) process
$Y_t - \mu = (1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q)\epsilon_t$
or
$Y_t - \mu = \theta(L)\epsilon_t,$
is also of this form, with $\psi(L) = \theta(L)$ and $\eta(L) = [\theta(L)]^{-1}$, provided that [4.2.13]
is based on the invertible representation. With a noninvertible MA(q), the roots
must first be flipped as described in Section 3.7 before applying the formulas given
in this section. An ARMA(p, q) also satisfies [4.2.10] and [4.2.11] with $\phi(L) =
\theta(L)/\phi(L)$, provided that the autoregressive operator $\phi(L)$ satisfies the stationarity
condition (roots of $\phi(z) = 0$ lie outside the unit circle) and that the moving average
operator $\theta(L)$ satisfies the invertibility condition (roots of $\theta(z) = 0$ lie outside the
unit circle).

[^4.1.10]: If [4.1.10] holds, then the forecast $\alpha'X_t$ is called the linear projection of $Y_{t+1}$ on $X_t.$
$E[(Y_{t+1} - \alpha'X_t)X_t] = 0.$

[^4.2.14]: For example, for an AR(1) process [4.2.10] would be
$(1 - \phi L)(Y_t - \mu) = \epsilon_t.$

[^4.2.16]: or, using [4.2.11],
$E[Y_{t+s}|Y_t, Y_{t-1},...] = \mu +  \left[ \frac{\psi(L)}{L^s} \right]_+ \frac{1}{\psi(L)}(Y_t - \mu).$
[^4.2.19]: For the covariance-stationary AR(1) process [4.2.14], we have
$E[Y_{t+s}|Y_t, Y_{t-1},...] = \mu +  \phi^s(Y_t - \mu)$.

[^4.2.30]: To forecast an MA(1) process one period into the future (s = 1),
$E[Y_{t+1}|Y_t, Y_{t-1},...] = \mu + \frac{\theta}{1 + \theta L}(Y_t-\mu)$.

[^4.2.39]:  For an ARMA(1, 1) process
$E[Y_{t+1}|Y_t, Y_{t-1},...] = \mu + \frac{\phi + \theta}{1 + \theta L} (Y_t - \mu)$.
<!-- END -->
