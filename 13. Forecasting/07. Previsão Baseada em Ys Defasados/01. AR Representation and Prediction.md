## Previs√£o Baseada em Ys Defasados

### Introdu√ß√£o

Em continuidade √† discuss√£o sobre **previs√£o** em s√©ries temporais, este cap√≠tulo se aprofunda na metodologia de previs√£o baseada em valores defasados da pr√≥pria s√©rie temporal, Y. Como vimos anteriormente [^4.2.1], a previs√£o √≥tima linear de uma vari√°vel $Y_{t+s}$ condicional a informa√ß√£o dispon√≠vel no tempo $t$ pode ser expressa em termos de valores passados do ru√≠do branco $\epsilon$. No entanto, na pr√°tica, observamos os valores passados da s√©rie temporal $Y$ e n√£o os valores passados de $\epsilon$. Este cap√≠tulo explora o cen√°rio onde a previs√£o √© realizada usando os valores defasados de $Y$, assumindo que o processo tem uma representa√ß√£o AR($\infty$) [^4.2.10]. Expandindo o conhecimento apresentado anteriormente, vamos analisar as particularidades de modelos AR(p) e como eles satisfazem as condi√ß√µes necess√°rias para a previs√£o com base em valores defasados de Y.

### Conceitos Fundamentais

Como mencionado, em situa√ß√µes reais, os valores observados s√£o os de $Y$ e n√£o de $\epsilon$ [^4.2.10]. Assim, para realizar a previs√£o com base nos valores defasados de $Y$, assumimos que o processo possui uma representa√ß√£o AR($\infty$):

$$
\eta(L)(Y_t - \mu) = \epsilon_t,
$$
onde $\eta(L) = \sum_{j=0}^{\infty} \eta_j L^j$, $\eta_0 = 1$ e $\sum_{j=0}^{\infty} |\eta_j| < \infty$. Al√©m disso, assumimos que o polin√¥mio AR $\eta(L)$ e o polin√¥mio MA $\psi(L)$ est√£o relacionados por $\eta(L) = [\psi(L)]^{-1}$ [^4.2.11].

Uma representa√ß√£o AR(p) estacion√°ria [^4.2.12] da forma:

$$
(1 - \phi_1 L - \phi_2 L^2 - \ldots - \phi_p L^p)(Y_t - \mu) = \epsilon_t
$$
ou, de forma mais compacta,
$$
\phi(L)(Y_t - \mu) = \epsilon_t
$$
satisfaz os requisitos acima, com $\eta(L) = \phi(L)$ e $\psi(L) = [\phi(L)]^{-1}$. Um processo MA(q) tamb√©m se encaixa nessa forma, com $\phi(L) = \theta(L)$ e $\eta(L) = [\theta(L)]^{-1}$, desde que seja utilizada a representa√ß√£o invert√≠vel [^4.2.13]. A representa√ß√£o invert√≠vel garante que o processo MA(q) possa ser expresso como um AR($\infty$). Similarmente, um processo ARMA(p,q) tamb√©m satisfaz as condi√ß√µes, com $\phi(L) = \theta(L)/\phi(L)$, desde que o operador autorregressivo $\phi(L)$ satisfa√ßa a condi√ß√£o de estacionariedade e o operador de m√©dia m√≥vel $\theta(L)$ satisfa√ßa a condi√ß√£o de invertibilidade [^4.2.13].

**Proposi√ß√£o 1:** *A condi√ß√£o de estacionariedade para um processo AR(p) implica que as ra√≠zes do polin√¥mio $\phi(z) = 1 - \phi_1 z - \phi_2 z^2 - \ldots - \phi_p z^p$ devem estar fora do c√≠rculo unit√°rio no plano complexo.*

*Prova:* A condi√ß√£o de estacionariedade garante que o processo AR(p) n√£o ir√° divergir ao longo do tempo. Matematicamente, isso significa que os momentos do processo (m√©dia, vari√¢ncia, autocovari√¢ncias) devem ser finitos e constantes no tempo. Essa propriedade √© diretamente ligada √† localiza√ß√£o das ra√≠zes do polin√¥mio caracter√≠stico $\phi(z)$.

I.  A estacionariedade de um processo AR(p) requer que suas autocovari√¢ncias sejam finitas e n√£o variem com o tempo.
II.  O polin√¥mio caracter√≠stico do processo AR(p) √© dado por $\phi(z) = 1 - \phi_1 z - \phi_2 z^2 - \ldots - \phi_p z^p$. As ra√≠zes deste polin√¥mio, $z_i$, s√£o encontradas resolvendo $\phi(z) = 0$.
III.  A condi√ß√£o de estacionariedade para um processo AR(p) pode ser expressa em termos das ra√≠zes do polin√¥mio caracter√≠stico, onde todas as ra√≠zes $z_i$ devem estar fora do c√≠rculo unit√°rio, isto √©, $|z_i| > 1$ para todo $i = 1, 2, \ldots, p$.
IV. Se alguma raiz estivesse dentro ou no c√≠rculo unit√°rio, a s√©rie temporal divergiria e n√£o seria estacion√°ria.
V. Portanto, a condi√ß√£o de estacionariedade para um processo AR(p) implica que as ra√≠zes do polin√¥mio $\phi(z)$ devem estar fora do c√≠rculo unit√°rio no plano complexo. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere um processo AR(2) com $\phi_1 = 0.5$ e $\phi_2 = 0.3$. O polin√¥mio caracter√≠stico √© $\phi(z) = 1 - 0.5z - 0.3z^2$. Para verificar a estacionariedade, encontramos as ra√≠zes de $\phi(z) = 0$. Usando a f√≥rmula quadr√°tica, as ra√≠zes s√£o $z_1 \approx 1.95$ e $z_2 \approx -3.62$. Como $|z_1| > 1$ e $|z_2| > 1$, o processo AR(2) com esses par√¢metros √© estacion√°rio. Agora, considere um processo AR(1) com $\phi_1 = 1.2$. O polin√¥mio √© $\phi(z) = 1 - 1.2z$. A raiz √© $z = 1/1.2 \approx 0.83$. Como $|z| < 1$, este processo n√£o √© estacion√°rio.

A condi√ß√£o para que a previs√£o linear √≥tima seja expressa em termos de valores defasados de Y √© dada por:

$$
E[(Y_{t+1} - \alpha'X_t)X_t] = 0
$$
onde $X_t$ √© um vetor de valores defasados de $Y$, e $\alpha$ √© o vetor de coeficientes de proje√ß√£o [^4.1.10].

No caso de modelos AR, a representa√ß√£o em termos de valores defasados de Y √© direta [^4.2.14]. Por exemplo, para um processo AR(1):

$$
(1 - \phi L)(Y_t - \mu) = \epsilon_t
$$
O erro $\epsilon_t$ pode ser expresso como:
$$
\epsilon_t = (Y_t - \mu) - \phi(Y_{t-1} - \mu).
$$
Assim, dado $\phi$ e $\mu$, e observa√ß√µes de $Y_t$ e $Y_{t-1}$, o valor de $\epsilon_t$ pode ser constru√≠do. De forma an√°loga, para um processo MA(1) na forma invert√≠vel [^4.2.15]:

$$
(1 + \theta L)^{-1}(Y_t - \mu) = \epsilon_t
$$
Com um n√∫mero infinito de observa√ß√µes de $Y$, $\epsilon_t$ pode ser constru√≠do a partir de:
$$
\epsilon_t = (Y_t - \mu) - \theta(Y_{t-1} - \mu) + \theta^2(Y_{t-2} - \mu) - \theta^3(Y_{t-3} - \mu) + \ldots
$$

> üí° **Exemplo Num√©rico:** Considere um processo AR(1) com $\phi = 0.7$ e $\mu = 10$. Se $Y_t = 15$ e $Y_{t-1} = 12$, ent√£o $\epsilon_t = (15 - 10) - 0.7(12 - 10) = 5 - 0.7(2) = 5 - 1.4 = 3.6$. Para um processo MA(1) com $\theta = 0.5$ e $\mu= 5$, e valores de $Y_t=8, Y_{t-1}=7, Y_{t-2}=6, Y_{t-3}=5$,  $\epsilon_t = (8-5) - 0.5(7-5) + 0.5^2(6-5) -0.5^3(5-5) = 3 - 0.5(2) + 0.25(1) - 0.125(0) = 3 - 1 + 0.25 = 2.25$. Note que para um MA(1),  calculamos o erro usando uma soma infinita, mas na pr√°tica, aproximamos com os valores dispon√≠veis.

Sob essas condi√ß√µes, a previs√£o de $Y_{t+s}$ pode ser expressa como uma fun√ß√£o dos valores defasados de $Y$:

$$
E[Y_{t+s}|Y_t, Y_{t-1}, \ldots] = \mu + \left[ \frac{\psi(L)}{L^s} \right]_+ \eta(L)(Y_t - \mu)
$$
onde o operador $[.]_+$ elimina pot√™ncias negativas de $L$ [^4.2.9]. Essa express√£o implica que a previs√£o √≥tima de $Y_{t+s}$ √© uma fun√ß√£o linear de valores presentes e passados de $Y$, ponderados pelos coeficientes apropriados da representa√ß√£o AR($\infty$) ou MA($\infty$) do processo.

**Lema 1:** *A representa√ß√£o de um processo MA(q) na forma invert√≠vel √© crucial para express√°-lo como um AR($\infty$). A condi√ß√£o de invertibilidade garante que os coeficientes da representa√ß√£o AR($\infty$) convirjam.*

*Prova:* A representa√ß√£o invert√≠vel de um processo MA(q) permite expressar o erro $\epsilon_t$ como uma fun√ß√£o linear dos valores passados de $Y_t$. Isso √© essencial para que o processo MA(q) possa ser expresso como um processo AR($\infty$). A condi√ß√£o de invertibilidade garante que os coeficientes da representa√ß√£o AR($\infty$) diminuam √† medida que as defasagens aumentam, assegurando que a soma infinita seja finita.

I.  A representa√ß√£o de um processo MA(q) √© dada por $Y_t - \mu = \theta(L)\epsilon_t$, onde $\theta(L) = 1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q$.
II.  Para que o processo MA(q) tenha uma representa√ß√£o AR($\infty$), o operador $\theta(L)$ deve ser invert√≠vel, o que significa que existe um operador $\eta(L)$ tal que $\eta(L) = [\theta(L)]^{-1}$.
III.  A representa√ß√£o invert√≠vel expressa $\epsilon_t$ como uma fun√ß√£o dos valores passados de $Y_t$: $\epsilon_t = \eta(L)(Y_t - \mu)$, onde $\eta(L) = \sum_{j=0}^\infty \eta_j L^j$.
IV.  A condi√ß√£o de invertibilidade assegura que a soma infinita $\sum_{j=0}^\infty |\eta_j|$ convirja, garantindo que os coeficientes da representa√ß√£o AR($\infty$) diminuam √† medida que as defasagens aumentam.
V.  Sem a condi√ß√£o de invertibilidade, a representa√ß√£o AR($\infty$) pode n√£o convergir, tornando a representa√ß√£o do processo inadequada para previs√£o com base em valores defasados de Y.
VI. Portanto, a representa√ß√£o de um processo MA(q) na forma invert√≠vel √© crucial para express√°-lo como um AR($\infty$), e a condi√ß√£o de invertibilidade garante que os coeficientes da representa√ß√£o AR($\infty$) convirjam. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere um processo MA(1) com $\theta = 0.8$.  Para a invertibilidade, $|\theta| < 1$, o que √© v√°lido. Sua representa√ß√£o invert√≠vel √© dada por $\epsilon_t = (Y_t - \mu) - 0.8(Y_{t-1} - \mu) + 0.8^2(Y_{t-2} - \mu) - \ldots$. Se $\theta = 1.2$, o processo MA(1) n√£o √© invert√≠vel e sua representa√ß√£o AR($\infty$) n√£o converge. Os coeficientes n√£o diminuem a zero e a soma infinita diverge, logo, n√£o podemos expressar o processo como um AR($\infty$).

Al√©m disso, a f√≥rmula de previs√£o de Wiener-Kolmogorov √© derivada quando a previs√£o √© baseada em $Y's$ passados [^4.2.16]:

$$
E[Y_{t+s}|Y_t, Y_{t-1}, \ldots] = \mu + \left[ \frac{\psi(L)}{L^s} \right]_+ \frac{1}{\psi(L)}(Y_t - \mu).
$$
**Teorema 1:** *A f√≥rmula de previs√£o de Wiener-Kolmogorov, expressa como
$E[Y_{t+s}|Y_t, Y_{t-1}, \ldots] = \mu + \left[ \frac{\psi(L)}{L^s} \right]_+ \frac{1}{\psi(L)}(Y_t - \mu)$,
representa a melhor previs√£o linear de $Y_{t+s}$ dado o hist√≥rico de valores passados de $Y$,
sob a condi√ß√£o de que o processo possa ser expresso como uma representa√ß√£o AR($\infty$).*

*Prova:* A demonstra√ß√£o deste teorema se apoia na propriedade de proje√ß√£o ortogonal, conforme descrito na condi√ß√£o $E[(Y_{t+1} - \alpha'X_t)X_t] = 0$, que define a melhor previs√£o linear. A express√£o dada pela f√≥rmula de Wiener-Kolmogorov √© resultado da aplica√ß√£o dessa propriedade, combinada com a hip√≥tese de que o processo tem uma representa√ß√£o AR($\infty$). Em termos pr√°ticos, a demonstra√ß√£o pode ser realizada expandindo a representa√ß√£o do processo e comparando-a com a f√≥rmula de proje√ß√£o, mostrando que a f√≥rmula dada de fato minimiza o erro quadr√°tico m√©dio.

I. A melhor previs√£o linear de $Y_{t+s}$ dado o hist√≥rico de valores passados de $Y$, $Y_t, Y_{t-1}, \ldots$, √© aquela que minimiza o erro quadr√°tico m√©dio $E[(Y_{t+s} - \hat{Y}_{t+s|t})^2]$, onde $\hat{Y}_{t+s|t}$ √© a previs√£o de $Y_{t+s}$ no instante $t$.
II. De acordo com o princ√≠pio da proje√ß√£o ortogonal, a melhor previs√£o linear $\hat{Y}_{t+s|t}$ √© dada pela proje√ß√£o de $Y_{t+s}$ no espa√ßo linear gerado por $Y_t, Y_{t-1}, \ldots$. Isso significa que o erro de previs√£o $(Y_{t+s} - \hat{Y}_{t+s|t})$ √© ortogonal aos valores defasados de $Y$, ou seja, $E[(Y_{t+s} - \hat{Y}_{t+s|t})Y_{t-j}] = 0$ para $j \geq 0$.
III. A representa√ß√£o AR($\infty$) do processo √© dada por $\eta(L)(Y_t - \mu) = \epsilon_t$, onde $\eta(L) = [\psi(L)]^{-1}$. Assim, podemos expressar $(Y_t - \mu)$ como $\psi(L)\epsilon_t$.
IV. Substituindo a representa√ß√£o AR($\infty$) na express√£o da previs√£o √≥tima, temos:
    $$E[Y_{t+s}|Y_t, Y_{t-1}, \ldots] = \mu + E[\psi(L)\epsilon_{t+s}|Y_t, Y_{t-1}, \ldots]$$
V.  Usando a express√£o  $E[Y_{t+s}|\epsilon_t, \epsilon_{t-1},...] = \mu + \left[ \frac{\psi(L)}{L^s} \right]_+ \epsilon_t$ e substituindo $\epsilon_t$ por $\eta(L)(Y_t-\mu)$ ou $\frac{1}{\psi(L)}(Y_t-\mu)$, resulta em:
$$ E[Y_{t+s}|Y_t, Y_{t-1}, \ldots] = \mu + \left[ \frac{\psi(L)}{L^s} \right]_+ \frac{1}{\psi(L)}(Y_t - \mu).$$
VI. Portanto, a f√≥rmula de previs√£o de Wiener-Kolmogorov representa a melhor previs√£o linear de $Y_{t+s}$ dado o hist√≥rico de valores passados de $Y$, sob a condi√ß√£o de que o processo possa ser expresso como uma representa√ß√£o AR($\infty$). ‚ñ†
> üí° **Exemplo Num√©rico:** Vamos aplicar a f√≥rmula de previs√£o de Wiener-Kolmogorov para um processo AR(1) com $\phi = 0.6$ e $\mu = 20$ para prever $Y_{t+1}$ usando $Y_t$. Aqui $\psi(L) = \frac{1}{1-0.6L}$. A previs√£o para $s=1$ √© $E[Y_{t+1}|Y_t, Y_{t-1}, \ldots] = \mu + \left[ \frac{\psi(L)}{L} \right]_+ \frac{1}{\psi(L)}(Y_t - \mu) = \mu + \left[ \frac{1}{L(1-0.6L)} \right]_+ (1-0.6L)(Y_t - \mu) = 20 + 0.6(Y_t - 20)$. Se $Y_t = 25$, a previs√£o de $Y_{t+1}$ √© $20 + 0.6(25 - 20) = 20 + 0.6(5) = 23$. Note que o termo $\left[ \frac{\psi(L)}{L} \right]_+$ elimina pot√™ncias negativas de L, neste caso ficando com o coeficiente de L, que √© $\phi=0.6$.

### Conclus√£o

Este cap√≠tulo demonstrou que a previs√£o utilizando valores defasados de $Y$ √© vi√°vel sob certas condi√ß√µes, notavelmente, quando o processo pode ser representado por um modelo AR(‚àû) [^4.2.10]. Modelos AR(p), MA(q) (na forma invert√≠vel), e ARMA(p,q), satisfazem os requisitos necess√°rios para que isso seja feito. A chave √© a capacidade de expressar o termo de erro ($\epsilon_t$) como uma fun√ß√£o dos valores passados de $Y$, permitindo que as previs√µes sejam feitas diretamente com base nas observa√ß√µes da s√©rie temporal. As f√≥rmulas de previs√£o apresentadas nesse cap√≠tulo estabelecem uma base s√≥lida para a modelagem e previs√£o de s√©ries temporais, onde a informa√ß√£o relevante para a previs√£o √© expressa atrav√©s dos valores defasados da pr√≥pria s√©rie. O conceito de **representa√ß√£o invert√≠vel** desempenha um papel fundamental na garantia da viabilidade da previs√£o baseada em valores defasados de Y. A formula de previs√£o de Wiener-Kolmogorov tamb√©m foi derivada utilizando esses conceitos.

### Refer√™ncias
[^4.2.1]:  Expression [4.1.1] is known as the mean squared error associated with the forecast
$Y^*_{t+1|t}$, denoted
$MSE(Y^*_{t+1|t}) = E(Y_{t+1} - Y^*_{t+1|t})^2$.

[^4.2.10]: We now restrict the class of forecasts considered by requiring the forecast
$Y^*_{t+1|t}$ to be a linear function of $X_t$:
$Y^*_{t+1|t} = \alpha'X_t.$

[^4.2.11]: Suppose further that the AR polynomial $\eta(L)$ and the MA polynomial $\psi(L)$ are related by
$\eta(L) = [\psi(L)]^{-1}.$

[^4.2.12]: A covariance-stationary AR(p) model of the form
$(1 - \phi_1 L - \phi_2 L^2 \ldots - \phi_p L^p)(Y_t - \mu) = \epsilon_t,$
or, more compactly,
$\phi(L)(Y_t - \mu) = \epsilon_t,$

[^4.2.13]: An MA(q) process
$Y_t - \mu = (1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q)\epsilon_t$
or
$Y_t - \mu = \theta(L)\epsilon_t,$
is also of this form, with $\psi(L) = \theta(L)$ and $\eta(L) = [\theta(L)]^{-1}$, provided that [4.2.13]
is based on the invertible representation. With a noninvertible MA(q), the roots
must first be flipped as described in Section 3.7 before applying the formulas given
in this section. An ARMA(p, q) also satisfies [4.2.10] and [4.2.11] with $\phi(L) =
\theta(L)/\phi(L)$, provided that the autoregressive operator $\phi(L)$ satisfies the stationarity
condition (roots of $\phi(z) = 0$ lie outside the unit circle) and that the moving average
operator $\theta(L)$ satisfies the invertibility condition (roots of $\theta(z) = 0$ lie outside the
unit circle).

[^4.1.10]: If [4.1.10] holds, then the forecast $\alpha'X_t$ is called the linear projection of $Y_{t+1}$ on $X_t.$
$E[(Y_{t+1} - \alpha'X_t)X_t] = 0.$

[^4.2.14]: For example, for an AR(1) process [4.2.10] would be
$(1 - \phi L)(Y_t - \mu) = \epsilon_t.$

[^4.2.15]: For an MA(1) process written in invertible form, [4.2.10] would be
$(1 + \theta L)^{-1}(Y_t - \mu) = \epsilon_t.$

[^4.2.9]: Comparing [4.2.8] with [4.2.4], the optimal forecast could be written in lag operator
notation as
$E[Y_{t+s}|\epsilon_t, \epsilon_{t-1},...] = \mu + \left[ \frac{\psi(L)}{L^s} \right]_+ \epsilon_t$
[^4.2.16]: or, using [4.2.11],
$E[Y_{t+s}|Y_t, Y_{t-1},...] = \mu +  \left[ \frac{\psi(L)}{L^s} \right]_+ \frac{1}{\psi(L)}(Y_t - \mu).$
<!-- END -->
