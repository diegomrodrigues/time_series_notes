## Fatora√ß√£o Triangular: C√°lculo e Aplica√ß√µes em Proje√ß√µes Lineares
### Introdu√ß√£o
Neste cap√≠tulo, aprofundamos a aplica√ß√£o da fatora√ß√£o triangular de matrizes sim√©tricas definidas positivas, com um foco particular em como as matrizes $A$ e $D$ podem ser calculadas por meio de opera√ß√µes de pr√© e p√≥s-multiplica√ß√£o, garantindo que a matriz resultante, $H$, possua zeros nas posi√ß√µes desejadas [^4]. Exploraremos tamb√©m como essa fatora√ß√£o √© utilizada em proje√ß√µes lineares e outros contextos estat√≠sticos, oferecendo um m√©todo eficiente para c√°lculos e an√°lises [^4]. A manipula√ß√£o de matrizes por meio de pr√© e p√≥s-multiplica√ß√£o √© um recurso fundamental para garantir a obten√ß√£o da forma desejada das matrizes resultantes.

### C√°lculo das Matrizes A e D via Pr√© e P√≥s-Multiplica√ß√£o
Como discutido anteriormente, a fatora√ß√£o triangular de uma matriz sim√©trica definida positiva $\Omega$ √© expressa como $\Omega = ADA'$, onde $A$ √© uma matriz triangular inferior com 1s na diagonal e $D$ √© uma matriz diagonal com entradas positivas. O c√°lculo das matrizes $A$ e $D$ envolve um processo iterativo de transforma√ß√µes sobre $\Omega$.

**Transforma√ß√£o de Œ© via Matrizes Elementares**
O processo inicia com a aplica√ß√£o de opera√ß√µes elementares √† matriz $\Omega$, que podem ser representadas por meio de matrizes elementares, denotadas por $E_k$. Essas matrizes s√£o triangulares inferiores com 1s na diagonal e um √∫nico elemento diferente de zero abaixo da diagonal principal. O objetivo √© transformar $\Omega$ em uma matriz diagonal $D$.

1.  **Pr√©-multiplica√ß√£o por E‚ÇÅ:** A matriz $\Omega$ √© pr√©-multiplicada pela matriz $E_1$ da seguinte forma:
  $$
  E_1 = \begin{bmatrix}
  1 & 0 & 0 & \cdots & 0 \\
  -\Omega_{21}\Omega_{11}^{-1} & 1 & 0 & \cdots & 0 \\
  -\Omega_{31}\Omega_{11}^{-1} & 0 & 1 & \cdots & 0 \\
  \vdots & \vdots & \vdots & \ddots & \vdots \\
  -\Omega_{n1}\Omega_{11}^{-1} & 0 & 0 & \cdots & 1
  \end{bmatrix}
  $$
   onde $-\Omega_{i1}\Omega_{11}^{-1}$ √© o elemento da posi√ß√£o $(i,1)$ para zerar a primeira coluna de $\Omega$ abaixo da diagonal principal.

2.  **P√≥s-multiplica√ß√£o por E‚ÇÅ':**  A matriz resultante √© ent√£o p√≥s-multiplicada pela transposta de $E_1$, $E_1'$, resultando em $H$:

$$H = E_1 \Omega E_1'$$

Essa opera√ß√£o garante que $H$ tenha zeros abaixo da diagonal na primeira coluna.

3.  **Itera√ß√£o do Processo:** As pr√≥ximas matrizes elementares, $E_2$, $E_3$ e assim por diante, s√£o constru√≠das para zerar os elementos abaixo da diagonal principal nas colunas subsequentes. Cada $E_k$ √© criada para zerar os elementos da $k$-√©sima coluna, operando sobre a matriz resultante da etapa anterior, que denotamos por $H$. Assim, ap√≥s $n-1$ transforma√ß√µes:

$$D = E_{n-1} \cdots E_2 E_1 \Omega E_1' E_2' \cdots E_{n-1}'$$

A matriz resultante, $D$, √© uma matriz diagonal.

> üí° **Exemplo Num√©rico:** Considere a matriz $\Omega$ do exemplo anterior:
>
> $$
\Omega = \begin{bmatrix}
4 & 2 & 2 \\
2 & 5 & 3 \\
2 & 3 & 6
\end{bmatrix}
$$
>
> Inicialmente, calculamos $E_1$:
>
>$$
E_1 = \begin{bmatrix}
1 & 0 & 0 \\
-0.5 & 1 & 0 \\
-0.5 & 0 & 1
\end{bmatrix}
$$
>
> Em seguida, calculamos a matriz $H$:
>
>$$
H = E_1\Omega E_1' =  \begin{bmatrix}
4 & 0 & 0 \\
0 & 4 & 2 \\
0 & 2 & 5
\end{bmatrix}
$$
>
> Calculamos $E_2$ para zerar o elemento (3,2) de $H$:
>$$
E_2 = \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & -0.5 & 1
\end{bmatrix}
$$
> E finalmente, obtemos a matriz $D$:
>$$
D = E_2HE_2' = \begin{bmatrix}
4 & 0 & 0 \\
0 & 4 & 0 \\
0 & 0 & 4
\end{bmatrix}
$$
>
>  Observe que cada etapa da transforma√ß√£o $E_k$ busca zerar os elementos fora da diagonal principal. No primeiro passo, $E_1$ zera os elementos abaixo da diagonal na primeira coluna. No segundo passo, $E_2$ zera o elemento abaixo da diagonal na segunda coluna. Ao final, a matriz $D$ resultante √© diagonal.

**Lema 1.1** A inversa de uma matriz elementar $E_k$ √© obtida trocando o sinal dos elementos fora da diagonal principal.
*Prova:*
Seja $E_k$ uma matriz elementar que difere da matriz identidade apenas na sua $k$-√©sima coluna abaixo da diagonal principal, com elementos $-\Omega_{ik} \Omega_{kk}^{-1}$, para $i > k$. A matriz inversa $E_k^{-1}$ deve satisfazer $E_k E_k^{-1} = I$, onde $I$ √© a matriz identidade.  Ao multiplicarmos $E_k$ por uma matriz que tem os mesmos elementos, mas com sinal oposto, i.e., $\Omega_{ik} \Omega_{kk}^{-1}$, a matriz resultante ser√° a identidade, provando o lema. $\blacksquare$
I. Seja $E_k$ uma matriz elementar como definido, com elementos $-\Omega_{ik} \Omega_{kk}^{-1}$ abaixo da diagonal principal na $k$-√©sima coluna.
II. Seja $E_k^{-1}$ uma matriz id√™ntica a $E_k$, exceto pelos sinais opostos dos elementos abaixo da diagonal principal, i.e., $\Omega_{ik} \Omega_{kk}^{-1}$.
III.  O produto $E_k E_k^{-1}$ resulta em uma matriz onde a $k$-√©sima coluna √© multiplicada por 1, exceto pelos elementos abaixo da diagonal, que se anulam devido √† multiplica√ß√£o por $-\Omega_{ik} \Omega_{kk}^{-1}$ e $\Omega_{ik} \Omega_{kk}^{-1}$.
IV. Portanto, o produto $E_k E_k^{-1}$ resulta na matriz identidade $I$.
V. Por defini√ß√£o, se $E_k E_k^{-1} = I$, ent√£o $E_k^{-1}$ √© a matriz inversa de $E_k$. $\blacksquare$

**C√°lculo de A usando as Inversas de E**

A matriz $A$ √© obtida pela combina√ß√£o das inversas das matrizes elementares.
$A = (E_{n-1} \cdots E_2E_1)^{-1}$  [^4.4.8]

Como vimos anteriormente, a inversa de uma matriz elementar $E_k$ √© obtida simplesmente trocando o sinal dos elementos fora da diagonal principal.
Cada matriz $E_k^{-1}$ adiciona uma coluna √† matriz $A$, e as colunas s√£o adicionadas sequencialmente, de forma que a $j$-√©sima coluna da matriz $A$ corresponde a $j$-√©sima coluna da matriz $E_j^{-1}$.

> üí° **Exemplo Num√©rico (Continua√ß√£o):** Usando as matrizes $E_1$ e $E_2$ do exemplo anterior, calculamos as inversas $E_1^{-1}$ e $E_2^{-1}$:
>
> $$
E_1^{-1} = \begin{bmatrix}
1 & 0 & 0 \\
0.5 & 1 & 0 \\
0.5 & 0 & 1
\end{bmatrix}
$$
>
>$$
E_2^{-1} = \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0.5 & 1
\end{bmatrix}
$$
>
> E, portanto, a matriz $A$ √© dada por:
> $$
A = E_1^{-1}E_2^{-1} = \begin{bmatrix}
1 & 0 & 0 \\
0.5 & 1 & 0 \\
0.5 & 0.5 & 1
\end{bmatrix}
$$
> Observe que a matriz $A$ √© constru√≠da multiplicando as inversas de $E_1$ e $E_2$. Os elementos abaixo da diagonal em $A$ correspondem aos multiplicadores utilizados nas opera√ß√µes de elimina√ß√£o gaussiana (ou seja, os valores que foram usados para zerar os elementos abaixo da diagonal em $\Omega$).

**Teorema 1** O produto das inversas das matrizes elementares $E_k$ resulta na matriz $A$, isto √©, $A = (E_{n-1} \cdots E_2 E_1)^{-1} = E_1^{-1}E_2^{-1} \cdots E_{n-1}^{-1}$.
*Prova:*
A partir da defini√ß√£o de matriz inversa, temos que $(E_{n-1} \cdots E_2 E_1)(E_1^{-1}E_2^{-1} \cdots E_{n-1}^{-1}) = I$. Portanto, a matriz inversa do produto das matrizes elementares √© o produto das inversas na ordem reversa. Pela defini√ß√£o de $A$, o resultado segue diretamente. $\blacksquare$
I. Seja $E = E_{n-1} \cdots E_2 E_1$ o produto das matrizes elementares.
II. A inversa de um produto de matrizes √© o produto das inversas na ordem reversa, ou seja, $(E_{n-1} \cdots E_2 E_1)^{-1} = E_1^{-1}E_2^{-1} \cdots E_{n-1}^{-1}$.
III. Por defini√ß√£o, $A = (E_{n-1} \cdots E_2 E_1)^{-1}$.
IV. Substituindo (II) em (III), temos $A = E_1^{-1}E_2^{-1} \cdots E_{n-1}^{-1}$.
V. Portanto, o produto das inversas das matrizes elementares $E_k$ resulta na matriz $A$. $\blacksquare$

**Proposi√ß√£o 2:** O processo de pr√© e p√≥s-multiplica√ß√£o com as matrizes elementares $E_k$ transforma $\Omega$ em uma matriz diagonal $D$, onde as opera√ß√µes s√£o equivalentes a eliminar os elementos abaixo da diagonal principal sequencialmente em cada coluna.

*Prova:*
I.  A matriz $E_k$ √© projetada para eliminar os elementos na $k$-√©sima coluna de $\Omega$ abaixo da diagonal principal.
II.  A pr√©-multiplica√ß√£o por $E_k$ opera sobre as linhas da matriz $\Omega$, subtraindo m√∫ltiplos da $k$-√©sima linha das linhas abaixo dela.
III. A p√≥s-multiplica√ß√£o por $E_k'$ opera sobre as colunas da matriz, adicionando m√∫ltiplos das colunas da esquerda √† $k$-√©sima coluna.
IV. O produto $E_k \Omega E_k'$ resulta numa matriz com zeros nas posi√ß√µes $(i,k)$ e $(k,i)$ para $i>k$.
V.  Ao iterar esse processo de pr√© e p√≥s-multiplica√ß√£o, todos os elementos abaixo da diagonal principal s√£o eliminados de forma sequencial em cada coluna, resultando em uma matriz diagonal $D$. $\blacksquare$
I. A matriz elementar $E_k$ √© constru√≠da de forma que, ao pr√©-multiplicar a matriz $\Omega$, subtrai-se um m√∫ltiplo da linha $k$ das linhas $i > k$, eliminando assim o elemento na posi√ß√£o $(i, k)$.
II. A p√≥s-multiplica√ß√£o pela transposta de $E_k$, $E_k'$, realiza opera√ß√µes correspondentes nas colunas, eliminando os elementos nas posi√ß√µes $(k, i)$ para $i > k$.
III. Portanto, o produto $E_k \Omega E_k'$ resulta em uma matriz com zeros nas posi√ß√µes $(i, k)$ e $(k, i)$ para $i > k$.
IV. Aplicando este processo iterativamente para $k = 1, 2, \dots, n-1$, eliminamos todos os elementos abaixo da diagonal principal.
V. O resultado final √© uma matriz diagonal $D$. $\blacksquare$

**Corol√°rio 2:** O processo iterativo de pr√© e p√≥s-multiplica√ß√£o usando as matrizes elementares $E_k$ resulta na fatora√ß√£o triangular $\Omega=ADA'$.

*Prova:*
I. Pelo Teorema 1, sabemos que o processo iterativo resulta na fatora√ß√£o $\Omega = E^{-1}DE'^{-1}$.
II. Pelo Lema 1.1, sabemos que $E^{-1}=A$.
III.  Portanto, $E'^{-1}=A'$.
IV.  Substituindo, obtemos $\Omega = ADA'$. $\blacksquare$
I. Seja $E = E_{n-1} \cdots E_2 E_1$. Ent√£o, pela Proposi√ß√£o 2, temos que $D = E\Omega E'$.
II. Multiplicando √† esquerda por $E^{-1}$ e √† direita por $E'^{-1}$, temos $E^{-1}DE'^{-1} = \Omega$.
III. Pelo Teorema 1, $A = E^{-1}$.
IV. Portanto, $A' = (E^{-1})' = E'^{-1}$.
V. Substituindo na equa√ß√£o de (II), obtemos $\Omega = ADA'$. $\blacksquare$

**Observa√ß√£o 1:** Uma consequ√™ncia direta do processo de fatora√ß√£o √© que a matriz $\Omega$ pode ser expressa como um produto de matrizes triangulares e diagonais. Este resultado tem implica√ß√µes diretas para o c√°lculo de determinantes e a resolu√ß√£o de sistemas lineares, conforme ser√° discutido na se√ß√£o seguinte.

### Aplica√ß√µes da Fatora√ß√£o Triangular
A fatora√ß√£o triangular, obtida atrav√©s de opera√ß√µes de pr√© e p√≥s-multiplica√ß√£o, √© uma ferramenta fundamental em diversos contextos estat√≠sticos:

1.  **Proje√ß√µes Lineares:** A fatora√ß√£o triangular √© utilizada para calcular os coeficientes de proje√ß√£o linear, como demonstrado no cap√≠tulo anterior. As matrizes $A$ e $D$ permitem encontrar os coeficientes de uma proje√ß√£o linear de uma vari√°vel sobre outras. Especificamente, o elemento $a_{ij}$ da matriz $A$ representa o coeficiente da proje√ß√£o da vari√°vel $Y_i$ sobre a vari√°vel $Y_j$, ajustado pelas vari√°veis $Y_1$ at√© $Y_{j-1}$.

2.  **C√°lculo de Vari√¢ncias e Covari√¢ncias Condicionais:** Os elementos da matriz diagonal $D$ representam as vari√¢ncias condicionais dos res√≠duos ap√≥s cada etapa de proje√ß√£o linear.
     O elemento $d_{ii}$ da matriz diagonal $D$ corresponde √† vari√¢ncia condicional de $Y_i$, dado $Y_{i-1}, \dots , Y_1$. Este √© o erro de previs√£o de $Y_i$ ao se usar todos os valores anteriores como preditores lineares.

> üí° **Exemplo Num√©rico:** Usando a matriz $D$ obtida anteriormente,
>$$ D = \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 4 \end{bmatrix} $$
> temos que:
>   - $d_{11} = 4$ √© a vari√¢ncia de $Y_1$ sem condicionamento.
>   - $d_{22} = 4$ √© a vari√¢ncia de $Y_2$ dado $Y_1$, representando o erro de previs√£o de $Y_2$ ao se usar $Y_1$.
>   - $d_{33} = 4$ √© a vari√¢ncia de $Y_3$ dado $Y_1$ e $Y_2$, representando o erro de previs√£o de $Y_3$ ao se usar $Y_1$ e $Y_2$.
>  Observamos que como a matriz de covari√¢ncia $\Omega$ utilizada neste exemplo era sim√©trica e bem-comportada, as vari√¢ncias condicionais s√£o id√™nticas. Em cen√°rios mais gerais, cada $d_{ii}$ representar√° a variabilidade do res√≠duo, ap√≥s a remo√ß√£o da parte que pode ser linearmente explicada pelas vari√°veis $Y_1$ at√© $Y_{i-1}$.

3. **Solu√ß√£o de Sistemas Lineares:** Em problemas que envolvem a resolu√ß√£o de sistemas de equa√ß√µes lineares, a fatora√ß√£o triangular permite uma solu√ß√£o mais eficiente, transformando o problema em dois sistemas triangulares mais f√°ceis de resolver. Especificamente, dado o sistema $\Omega x = b$, podemos usar a fatora√ß√£o $\Omega = ADA'$ para escrever $ADA'x=b$.  Fazendo $y=A'x$ e $z=Dy$, resolvemos primeiro $Az=b$ (sistema triangular inferior), depois $Dy=z$ (sistema diagonal) e finalmente $A'x=y$ (sistema triangular superior).

> üí° **Exemplo Num√©rico:** Considere o sistema $\Omega x = b$, onde
>
>$$\Omega = \begin{bmatrix} 4 & 2 & 2 \\ 2 & 5 & 3 \\ 2 & 3 & 6 \end{bmatrix} \quad \text{e} \quad b = \begin{bmatrix} 12 \\ 15 \\ 20 \end{bmatrix}$$
>
>Usando a fatora√ß√£o $\Omega = ADA'$ com
>
>$$A = \begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.5 & 0.5 & 1 \end{bmatrix} \quad \text{e} \quad D = \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 4 \end{bmatrix}$$
>
>Primeiro, resolvemos $Az = b$:
>
>$$\begin{bmatrix} 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.5 & 0.5 & 1 \end{bmatrix} \begin{bmatrix} z_1 \\ z_2 \\ z_3 \end{bmatrix} = \begin{bmatrix} 12 \\ 15 \\ 20 \end{bmatrix}$$
>
>Resolvendo o sistema triangular inferior, obtemos $z_1 = 12$, $z_2 = 15 - 0.5 \times 12 = 9$, e $z_3 = 20 - 0.5 \times 12 - 0.5 \times 9 = 20 - 6 - 4.5 = 9.5$. Assim, $z = \begin{bmatrix} 12 \\ 9 \\ 9.5 \end{bmatrix}$
>
>Segundo, resolvemos $Dy=z$:
>
>$$\begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 4 \end{bmatrix} \begin{bmatrix} y_1 \\ y_2 \\ y_3 \end{bmatrix} = \begin{bmatrix} 12 \\ 9 \\ 9.5 \end{bmatrix}$$
>
>Como $D$ √© diagonal, temos $y_1 = 12/4 = 3$, $y_2 = 9/4 = 2.25$, e $y_3 = 9.5/4 = 2.375$. Assim, $y = \begin{bmatrix} 3 \\ 2.25 \\ 2.375 \end{bmatrix}$
>
>Finalmente, resolvemos $A'x=y$:
>
>$$\begin{bmatrix} 1 & 0.5 & 0.5 \\ 0 & 1 & 0.5 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} 3 \\ 2.25 \\ 2.375 \end{bmatrix}$$
>
>Resolvendo o sistema triangular superior, obtemos $x_3 = 2.375$, $x_2 = 2.25 - 0.5 \times 2.375 = 1.0625$, e $x_1 = 3 - 0.5 \times 1.0625 - 0.5 \times 2.375 = 3 - 0.53125 - 1.1875 = 1.28125$. Assim, $x = \begin{bmatrix} 1.28125 \\ 1.0625 \\ 2.375 \end{bmatrix}$.
>
>Este m√©todo de resolu√ß√£o, ao inv√©s de diretamente calcular $\Omega^{-1}$ e multiplicar por $b$, decomp√µe o sistema em etapas que s√£o computacionalmente mais eficientes, especialmente para matrizes grandes.

4.  **Computa√ß√£o de Determinantes e Inversas:** A fatora√ß√£o triangular simplifica o c√°lculo de determinantes e inversas de matrizes, o que √© crucial para diversos algoritmos estat√≠sticos e econom√©tricos. Como a matriz $D$ √© diagonal, seu determinante √© o produto dos elementos da diagonal. O determinante de $\Omega$ √© ent√£o igual ao determinante de $D$.

> üí° **Exemplo Num√©rico:** Usando a matriz $\Omega$ do exemplo anterior e sua fatora√ß√£o $D$, temos:
>
> $$ \Omega = \begin{bmatrix} 4 & 2 & 2 \\ 2 & 5 & 3 \\ 2 & 3 & 6 \end{bmatrix} \quad \text{e} \quad D = \begin{bmatrix} 4 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 4 \end{bmatrix} $$
>
>O determinante de $D$ √© $\det(D) = 4 \times 4 \times 4 = 64$. Portanto, o determinante de $\Omega$ tamb√©m √© 64. Esse m√©todo evita o c√°lculo direto do determinante de $\Omega$, que pode ser mais complexo. A fatora√ß√£o triangular simplifica o c√°lculo do determinante, especialmente para matrizes grandes, j√° que o determinante de uma matriz diagonal √© simplesmente o produto dos elementos da diagonal.

> üí° **Exemplo Num√©rico (Continua√ß√£o):** Usando a matriz $\Omega$ do exemplo anterior, e suas fatora√ß√µes $A$ e $D$, podemos verificar o c√°lculo da proje√ß√£o linear de $Y_3$ em fun√ß√£o de $Y_1$ e $Y_2$. Pela fatora√ß√£o triangular:
>$$A = \begin{bmatrix}
1 & 0 & 0 \\
0.5 & 1 & 0 \\
0.5 & 0.5 & 1
\end{bmatrix}$$
> Os coeficientes da proje√ß√£o linear de $Y_3$ em fun√ß√£o de $Y_1$ e $Y_2$ s√£o dados pela terceira linha de $A$:
>
> $$P(Y_3 | Y_1, Y_2) = 0.5Y_1 + 0.5(Y_2 - 0.5Y_1) = 0.25Y_1 + 0.5Y_2$$
>
> O erro de previs√£o ao utilizar os preditores lineares $Y_1$ e $Y_2$ √© dado por $d_{33}$, o terceiro elemento da matriz $D$.
>
> No exemplo num√©rico anterior, podemos verificar explicitamente o c√°lculo da proje√ß√£o. O coeficiente de $Y_1$ ao projetar $Y_3$ √© 0.25, e o coeficiente de $Y_2$ ao projetar $Y_3$ √© 0.5. O erro de previs√£o, obtido de $d_{33}$, √© 4.

### Conclus√£o
A fatora√ß√£o triangular, quando calculada atrav√©s de opera√ß√µes de pr√© e p√≥s-multiplica√ß√£o, resulta em matrizes que permitem uma manipula√ß√£o eficiente e compreens√£o das rela√ß√µes entre vari√°veis. A capacidade de transformar uma matriz $\Omega$ em um produto de matrizes triangulares inferiores e diagonais, com o aux√≠lio de transforma√ß√µes elementares, oferece um m√©todo poderoso para diversos c√°lculos estat√≠sticos, incluindo proje√ß√µes lineares e a an√°lise de vari√¢ncias e covari√¢ncias condicionais. Esta abordagem, fundamental na an√°lise de s√©ries temporais, possibilita a constru√ß√£o de modelos mais precisos e a extra√ß√£o de insights valiosos a partir dos dados.

### Refer√™ncias
[^4]: *A refer√™ncia geral ao cap√≠tulo 4 do livro √© utilizada para contextualizar as informa√ß√µes apresentadas.*
[^4.4.3]: *Apresenta a matriz E‚ÇÅ utilizada na primeira transforma√ß√£o de linhas.*
[^4.4.6]: *Apresenta a matriz E‚ÇÇ utilizada na transforma√ß√£o de linhas subsequente.*
[^4.4.8]: *Apresenta a rela√ß√£o entre as matrizes E e a matriz A.*
<!-- END -->
