## Obtenção dos Estimadores de Parâmetros via Condições de Primeira e Segunda Ordem no FIML

### Introdução

Dando sequência à nossa exploração do método de **Full-Information Maximum Likelihood (FIML)**, este capítulo se aprofunda no processo de obtenção dos estimadores de parâmetros. Como já estabelecido, o FIML envolve a maximização da função de log-verossimilhança conjunta do sistema de equações simultâneas [^9.4]. A maximização desta função é alcançada através da resolução de um sistema de equações não lineares, conhecidas como condições de primeira ordem, seguida pela verificação das condições de segunda ordem para garantir a natureza do ponto crítico encontrado [^previous_section]. Este capítulo detalha como estas condições são derivadas e resolvidas, focando na natureza iterativa dos métodos de otimização numérica envolvidos e nas suas implicações práticas.

### Derivação e Interpretação das Condições de Primeira Ordem

As condições de primeira ordem são obtidas derivando-se a função de log-verossimilhança em relação a cada parâmetro do modelo e igualando-se estas derivadas a zero. No contexto de um modelo de equações simultâneas do tipo $By_t + \Gamma x_t = u_t$, e assumindo uma distribuição normal multivariada para os termos de erro $u_t$, a função de log-verossimilhança é dada por [^9.4.4]:

$$ \mathcal{L}(B, \Gamma, D) = -\frac{Tn}{2} \log(2\pi) + T \log|B| - \frac{T}{2} \log|D| - \frac{1}{2} \sum_{t=1}^T (By_t + \Gamma x_t)'D^{-1}(By_t + \Gamma x_t) $$

onde:
-   $T$ é o número de observações,
-   $n$ é a dimensão do vetor de variáveis endógenas $y_t$,
-   $B$ é a matriz de coeficientes das variáveis endógenas,
-   $\Gamma$ é a matriz de coeficientes das variáveis predeterminadas, e
-   $D$ é a matriz de covariância dos erros.

A maximização desta função requer que as derivadas parciais em relação a cada elemento das matrizes $B$, $\Gamma$ e $D$ sejam iguais a zero. Por exemplo, a derivada parcial em relação a um elemento $b_{ij}$ da matriz $B$ é dada por:

$$ \frac{\partial \mathcal{L}}{\partial b_{ij}} = 0 $$

Similarmente, derivadas parciais são calculadas para cada elemento de $\Gamma$ e para os elementos únicos da matriz de covariância $D$.

Em geral, estas condições de primeira ordem resultam em um sistema de equações não lineares que não possuem soluções analíticas diretas. Esta não-linearidade exige a utilização de métodos numéricos de otimização para encontrar os valores dos parâmetros que maximizam a verossimilhança.

### Resolução Numérica das Condições de Primeira Ordem

O sistema de equações não lineares obtido das condições de primeira ordem é tipicamente resolvido por meio de algoritmos de otimização iterativos. Estes algoritmos iniciam com uma estimativa inicial dos parâmetros e, iterativamente, refinam essas estimativas, seguindo na direção que leva ao máximo da função de log-verossimilhança.

Entre os métodos mais comumente utilizados para essa otimização, destacam-se:

1. **Método de Newton-Raphson e suas variações:** Este é um método de otimização iterativo que utiliza as primeiras e segundas derivadas da função objetivo (neste caso, a função de log-verossimilhança) para atualizar os parâmetros a cada iteração. Dada uma iteração $k$, o parâmetro $\theta$ (que pode representar todos os parâmetros do modelo) é atualizado de acordo com:

    $$ \theta_{k+1} = \theta_k - H^{-1}(\theta_k)g(\theta_k) $$

    onde $g(\theta_k)$ é o vetor gradiente (derivadas de primeira ordem) e $H(\theta_k)$ é a matriz Hessiana (matriz das derivadas de segunda ordem) da função de log-verossimilhança, ambos avaliados na iteração $k$. Apesar de eficiente, o método de Newton-Raphson pode ser computacionalmente caro, especialmente em modelos com muitos parâmetros, devido à necessidade de calcular a matriz Hessiana e sua inversa em cada iteração.
    A necessidade de inversão da matriz Hessiana é uma limitação em sistemas de alta dimensão. Métodos quase-Newton como BFGS (Broyden-Fletcher-Goldfarb-Shanno) são frequentemente usados, pois utilizam uma aproximação da matriz Hessiana, evitando o custo computacional elevado da inversão da matriz a cada iteração [^previous_section].

2. **Método do Gradiente:** Em situações onde o cálculo da matriz Hessiana é muito custoso, o método do gradiente pode ser usado como alternativa. Este método utiliza apenas as derivadas de primeira ordem para atualizar os parâmetros, movendo-se na direção do gradiente. A atualização é feita de acordo com:

    $$ \theta_{k+1} = \theta_k + \alpha g(\theta_k) $$

    onde $\alpha$ é a taxa de aprendizado. Embora o método do gradiente seja mais simples computacionalmente, sua convergência pode ser mais lenta em comparação com o método de Newton-Raphson.

3.  **Algoritmos de Otimização Paralelos:** Para lidar com a complexidade computacional em modelos de grande escala, algoritmos de otimização paralelos podem ser empregados [^previous_section]. Estes algoritmos dividem o problema de otimização em subproblemas que podem ser resolvidos simultaneamente em múltiplos processadores, reduzindo o tempo total de computação.

### Verificação das Condições de Segunda Ordem

Após encontrar um ponto crítico por meio dos métodos de otimização numérica, a etapa seguinte e fundamental é verificar se esse ponto crítico corresponde a um máximo local ou global da função de log-verossimilhança e não a um ponto de sela ou um mínimo local.  Isso é feito através da avaliação das condições de segunda ordem, que envolvem a matriz Hessiana da função de log-verossimilhança.

A matriz Hessiana é uma matriz quadrada de derivadas parciais de segunda ordem da função de log-verossimilhança, avaliada no ponto crítico encontrado:

$$H(\theta) = \begin{bmatrix}
\frac{\partial^2 \mathcal{L}}{\partial \theta_1^2} & \frac{\partial^2 \mathcal{L}}{\partial \theta_1 \partial \theta_2} & \cdots & \frac{\partial^2 \mathcal{L}}{\partial \theta_1 \partial \theta_k} \\
\frac{\partial^2 \mathcal{L}}{\partial \theta_2 \partial \theta_1} & \frac{\partial^2 \mathcal{L}}{\partial \theta_2^2} & \cdots & \frac{\partial^2 \mathcal{L}}{\partial \theta_2 \partial \theta_k} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 \mathcal{L}}{\partial \theta_k \partial \theta_1} & \frac{\partial^2 \mathcal{L}}{\partial \theta_k \partial \theta_2} & \cdots & \frac{\partial^2 \mathcal{L}}{\partial \theta_k^2}
\end{bmatrix}$$

onde $\theta$ representa o vetor de todos os parâmetros do modelo e $\frac{\partial^2 \mathcal{L}}{\partial \theta_i \partial \theta_j}$ é a derivada parcial de segunda ordem de $\mathcal{L}$ em relação aos parâmetros $\theta_i$ e $\theta_j$.

Para que o ponto crítico seja um máximo local, é necessário que a matriz Hessiana seja **negativa definida** nesse ponto. Em outras palavras, todos os seus autovalores devem ser negativos. Na prática, essa condição pode ser difícil de verificar computacionalmente para grandes modelos, sendo geralmente avaliada a negatividade da matriz Hessiana ou através da verificação da negatividade da matriz Hessiana ou de uma matriz Hessiana aproximada para os parâmetros do modelo. Se a matriz Hessiana não for negativa definida, pode ser necessário reconsiderar as condições iniciais ou o modelo propriamente dito.

### Desafios e Considerações Práticas

A obtenção dos estimadores de parâmetros no FIML envolve vários desafios práticos:

1.  **Convergência:** Os métodos iterativos podem não convergir, ou podem convergir para um máximo local em vez de um máximo global. Métodos de busca global ou a utilização de diferentes condições iniciais podem ser necessários para mitigar esse problema.
2.  **Custo Computacional:** O cálculo das derivadas e da matriz Hessiana, especialmente em modelos de grande dimensão, pode ser computacionalmente muito custoso. Isso exige a utilização de métodos de otimização eficientes e, em alguns casos, de computação paralela.
3.  **Condições Iniciais:** A escolha das condições iniciais pode ter um impacto significativo no resultado do processo de otimização. É importante experimentar diferentes condições iniciais para verificar a robustez dos resultados.
4.  **Complexidade do Modelo:** A complexidade do modelo (número de equações e de parâmetros) afeta diretamente a dificuldade de implementação do FIML. Modelos mais complexos podem exigir técnicas de redução de dimensionalidade para diminuir o custo computacional.

### Conclusão

A obtenção dos estimadores de parâmetros no método FIML é um processo iterativo que envolve a resolução de um sistema de equações não lineares (condições de primeira ordem) através de métodos de otimização numérica e a verificação da natureza do ponto crítico encontrado através da avaliação das condições de segunda ordem. Embora o método FIML seja teoricamente poderoso, sua implementação prática apresenta desafios significativos, incluindo a complexidade computacional, a dificuldade de garantir a convergência para o máximo global, e a necessidade de escolher condições iniciais apropriadas. O balanço entre custo computacional e qualidade da estimativa deve ser levado em consideração. A verificação das condições de segunda ordem é um passo crucial para assegurar que os estimadores obtidos correspondam a uma maximização da função de verossimilhança.

### Referências
[^9.4]: Seção 9.4 do contexto original
[^9.4.4]: Equação 9.4.4 do contexto original
[^previous_section]: Seção anterior sobre a implementação computacional do método FIML.
<!-- END -->
