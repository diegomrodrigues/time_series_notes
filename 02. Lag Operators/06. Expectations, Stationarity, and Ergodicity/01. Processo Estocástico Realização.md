## Processos Estoc√°sticos e Realiza√ß√µes
### Introdu√ß√£o
Este cap√≠tulo introduz os conceitos fundamentais de processos estoc√°sticos, que s√£o sequ√™ncias de vari√°veis aleat√≥rias indexadas pelo tempo, e discute as suas realiza√ß√µes [^1]. Em continuidade ao estudo dos operadores de atraso, que foram explorados no cap√≠tulo anterior, este cap√≠tulo se dedica a construir uma base s√≥lida para a an√°lise de s√©ries temporais, focando em expectativas, estacionaridade e ergodicidade. A compreens√£o desses conceitos √© crucial para modelar e analisar dados que evoluem ao longo do tempo.

### Conceitos Fundamentais

Um **processo estoc√°stico** (ou processo aleat√≥rio) √© definido como uma sequ√™ncia de vari√°veis aleat√≥rias indexadas pelo tempo [^1]. √â importante frisar que, ao observarmos dados de s√©ries temporais, estamos a lidar com uma **realiza√ß√£o espec√≠fica** de um processo estoc√°stico, isto √©, uma amostra de valores que o processo gera [^1]. Assim, √© crucial entender que uma amostra observada representa apenas um resultado poss√≠vel de todo o conjunto de resultados que o processo poderia gerar.

Para ilustrar, consideremos uma amostra de tamanho $T$ de uma vari√°vel aleat√≥ria $Y_t$:
$$ \{Y_1, Y_2, \ldots, Y_T\} $$
Essa amostra representa um conjunto de $T$ n√∫meros particulares, mas ela √© apenas uma entre as infinitas realiza√ß√µes poss√≠veis do processo estoc√°stico subjacente [^1].

Podemos pensar num cen√°rio onde $T$ vari√°veis aleat√≥rias independentes e identicamente distribu√≠das (i.i.d.) $\epsilon_t$ s√£o geradas:
$$ \{\epsilon_1, \epsilon_2, \ldots, \epsilon_T\} $$
onde
$$ \epsilon_t \sim N(0, \sigma^2) $$
Este conjunto √© referido como uma amostra de tamanho $T$ de um **processo de ru√≠do branco gaussiano** [^1].
> üí° **Exemplo Num√©rico:** Vamos gerar uma amostra de tamanho $T=100$ de um processo de ru√≠do branco gaussiano com $\sigma^2 = 1$. Podemos usar Python com a biblioteca `numpy`:
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> T = 100
> sigma = 1
> epsilon = np.random.normal(0, np.sqrt(sigma), T)
>
> plt.plot(epsilon)
> plt.title("Realiza√ß√£o de um Processo de Ru√≠do Branco Gaussiano")
> plt.xlabel("Tempo (t)")
> plt.ylabel("Œµ_t")
> plt.grid(True)
> plt.show()
> ```
> Este c√≥digo gera 100 valores aleat√≥rios de uma distribui√ß√£o normal com m√©dia 0 e desvio padr√£o 1. O gr√°fico resultante mostra uma poss√≠vel realiza√ß√£o deste processo. Cada vez que rodamos este c√≥digo, uma nova realiza√ß√£o diferente ser√° gerada.

**Defini√ß√£o 1** Um processo de ru√≠do branco √© um processo estoc√°stico em que as vari√°veis aleat√≥rias $\epsilon_t$ t√™m m√©dia zero, vari√¢ncia constante e n√£o s√£o correlacionadas entre si. Formalmente,
$$E[\epsilon_t] = 0$$
$$Var[\epsilon_t] = \sigma^2$$
$$Cov[\epsilon_t, \epsilon_s] = 0, \forall t \neq s$$
O ru√≠do branco gaussiano √© um caso particular de ru√≠do branco onde as vari√°veis $\epsilon_t$ seguem uma distribui√ß√£o normal.

**Defini√ß√£o 1.1** (Ru√≠do Branco com Vari√¢ncia Vari√°vel)
Um processo de ru√≠do branco com vari√¢ncia vari√°vel √© um processo estoc√°stico em que as vari√°veis aleat√≥rias $\epsilon_t$ t√™m m√©dia zero, e n√£o s√£o correlacionadas entre si, mas a vari√¢ncia pode variar no tempo. Formalmente:
$$E[\epsilon_t] = 0$$
$$Var[\epsilon_t] = \sigma_t^2$$
$$Cov[\epsilon_t, \epsilon_s] = 0, \forall t \neq s$$
Este processo generaliza o ru√≠do branco tradicional, permitindo que a dispers√£o dos dados varie ao longo do tempo. Este tipo de ru√≠do √© √∫til para modelar situa√ß√µes onde a volatilidade do processo n√£o √© constante.

> üí° **Exemplo Num√©rico:**  Vamos criar um processo de ru√≠do branco com vari√¢ncia vari√°vel. Vamos definir que a vari√¢ncia cresce linearmente com o tempo, ou seja, $\sigma_t^2 = 0.5 + 0.01t$.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> T = 100
> variance = np.array([0.5 + 0.01*t for t in range(T)])
> epsilon = np.random.normal(0, np.sqrt(variance), T)
>
> plt.plot(epsilon)
> plt.title("Realiza√ß√£o de Ru√≠do Branco com Vari√¢ncia Vari√°vel")
> plt.xlabel("Tempo (t)")
> plt.ylabel("Œµ_t")
> plt.grid(True)
> plt.show()
> ```
> Neste caso, podemos observar que a dispers√£o dos pontos aumenta ao longo do tempo, refletindo o aumento da vari√¢ncia.

**Defini√ß√£o 1.2** (Processo de Ru√≠do Branco Generalizado)
Um processo de ru√≠do branco generalizado √© um processo estoc√°stico em que as vari√°veis aleat√≥rias $\epsilon_t$ t√™m m√©dia zero, e n√£o s√£o correlacionadas entre si. Formalmente:
$$E[\epsilon_t] = 0$$
$$Cov[\epsilon_t, \epsilon_s] = 0, \forall t \neq s$$
Note que o processo de ru√≠do branco com vari√¢ncia vari√°vel (Defini√ß√£o 1.1) √© um caso particular de ru√≠do branco generalizado. Este processo √© mais geral pois n√£o imp√µe restri√ß√µes sobre a vari√¢ncia.

Mesmo que imaginemos observar um processo por um per√≠odo infinito de tempo, obtendo a sequ√™ncia:
$$ \{y_t\}_{-\infty}^{\infty} = \{\ldots, y_{-1}, y_0, y_1, y_2, \ldots, y_T, y_{T+1}, y_{T+2}, \ldots\} $$
essa sequ√™ncia infinita ainda ser√° considerada uma √∫nica realiza√ß√£o do processo estoc√°stico [^1].

Podemos imaginar, por exemplo, dois computadores gerando, independentemente, sequ√™ncias infinitas de vari√°veis i.i.d. com distribui√ß√£o $N(0, \sigma^2)$,  $\{\epsilon_t^{(1)}\}_{-\infty}^{\infty}$ e $\{\epsilon_t^{(2)}\}_{-\infty}^{\infty}$.  Cada sequ√™ncia constituir√° uma realiza√ß√£o distinta do mesmo processo de ru√≠do branco gaussiano [^1]. O processo estoc√°stico √©, portanto, definido pelas leis de probabilidade que governam a gera√ß√£o dessas realiza√ß√µes, e a an√°lise de s√©ries temporais envolve o estudo dessas leis a partir de uma ou mais realiza√ß√µes observadas.

**Proposi√ß√£o 1**
Seja $\{X_t\}_{t=-\infty}^{\infty}$ um processo estoc√°stico. Para qualquer instante de tempo $t$, $X_t$ √© uma vari√°vel aleat√≥ria com uma fun√ß√£o de distribui√ß√£o $F_{X_t}(x) = P(X_t \leq x)$. A cole√ß√£o de todas as fun√ß√µes de distribui√ß√£o $F_{X_t}$ define o processo.
*Prova:*
I.  Um processo estoc√°stico $\{X_t\}_{t=-\infty}^{\infty}$ √© uma sequ√™ncia de vari√°veis aleat√≥rias indexadas pelo tempo $t$.
II. Para cada instante de tempo $t$, $X_t$ √© uma vari√°vel aleat√≥ria.
III. Uma vari√°vel aleat√≥ria $X_t$ tem uma fun√ß√£o de distribui√ß√£o $F_{X_t}(x)$ que especifica a probabilidade de $X_t$ ser menor ou igual a um valor $x$, ou seja, $F_{X_t}(x) = P(X_t \leq x)$.
IV.  Como o processo estoc√°stico √© definido como uma sequ√™ncia de vari√°veis aleat√≥rias, a cole√ß√£o de todas as fun√ß√µes de distribui√ß√£o $F_{X_t}$ (uma para cada $t$) define completamente o processo estoc√°stico.
Portanto, a cole√ß√£o de todas as fun√ß√µes de distribui√ß√£o $F_{X_t}$ define o processo. ‚ñ†

**Lema 1.1** (Fun√ß√£o de Distribui√ß√£o Conjunta)
A distribui√ß√£o conjunta de um processo estoc√°stico $\{X_t\}_{t=-\infty}^{\infty}$ para qualquer conjunto de instantes $t_1, t_2, \ldots, t_n$ √© dada por uma fun√ß√£o de distribui√ß√£o conjunta
$F_{X_{t_1}, X_{t_2},\ldots,X_{t_n}}(x_1, x_2, \ldots, x_n) = P(X_{t_1} \leq x_1, X_{t_2} \leq x_2, \ldots, X_{t_n} \leq x_n)$.  Essa fun√ß√£o de distribui√ß√£o conjunta fornece uma descri√ß√£o completa das probabilidades de observa√ß√£o dos valores de um processo em m√∫ltiplos instantes de tempo.
*Prova:*
I.   Um processo estoc√°stico $\{X_t\}_{t=-\infty}^{\infty}$ √© uma sequ√™ncia de vari√°veis aleat√≥rias indexadas pelo tempo $t$.
II. Para um conjunto de instantes de tempo $t_1, t_2, \ldots, t_n$, temos um conjunto correspondente de vari√°veis aleat√≥rias $X_{t_1}, X_{t_2}, \ldots, X_{t_n}$.
III. A fun√ß√£o de distribui√ß√£o conjunta $F_{X_{t_1}, X_{t_2},\ldots,X_{t_n}}(x_1, x_2, \ldots, x_n)$  define a probabilidade de que, simultaneamente, $X_{t_1}$ seja menor ou igual a $x_1$, $X_{t_2}$ seja menor ou igual a $x_2$, e assim por diante, at√© $X_{t_n}$ ser menor ou igual a $x_n$.
IV.  Esta fun√ß√£o descreve completamente a distribui√ß√£o de probabilidade conjunta das vari√°veis $X_{t_1}, X_{t_2}, \ldots, X_{t_n}$, fornecendo a probabilidade de observa√ß√£o dos valores do processo em m√∫ltiplos instantes de tempo.
Portanto, a fun√ß√£o de distribui√ß√£o conjunta define as probabilidades de observa√ß√£o dos valores de um processo em m√∫ltiplos instantes de tempo. ‚ñ†

**Lema 1.2** (Fun√ß√£o de Densidade Conjunta)
Se as vari√°veis aleat√≥rias $X_{t_1}, X_{t_2}, \ldots, X_{t_n}$ forem cont√≠nuas, a distribui√ß√£o conjunta pode ser representada pela fun√ß√£o de densidade conjunta
$f_{X_{t_1}, X_{t_2},\ldots,X_{t_n}}(x_1, x_2, \ldots, x_n)$. Esta fun√ß√£o, quando integrada em qualquer regi√£o do espa√ßo amostral, fornece a probabilidade do vetor aleat√≥rio $(X_{t_1}, X_{t_2}, \ldots, X_{t_n})$ cair nessa regi√£o.
*Prova:*
I.   Se as vari√°veis aleat√≥rias $X_{t_1}, X_{t_2}, \ldots, X_{t_n}$ de um processo estoc√°stico s√£o cont√≠nuas.
II. A distribui√ß√£o conjunta dessas vari√°veis pode ser descrita por uma fun√ß√£o de densidade conjunta $f_{X_{t_1}, X_{t_2},\ldots,X_{t_n}}(x_1, x_2, \ldots, x_n)$.
III. A integral da fun√ß√£o de densidade conjunta sobre uma regi√£o espec√≠fica do espa√ßo amostral fornece a probabilidade do vetor aleat√≥rio $(X_{t_1}, X_{t_2}, \ldots, X_{t_n})$ cair dentro dessa regi√£o.
Portanto, a fun√ß√£o de densidade conjunta, quando integrada, fornece as probabilidades de um vetor aleat√≥rio cair em uma regi√£o espec√≠fica do espa√ßo amostral. ‚ñ†

**Lema 1.3** (Covari√¢ncia entre Vari√°veis em um Processo Estoc√°stico)
Para um processo estoc√°stico $\{X_t\}_{t=-\infty}^{\infty}$, a covari√¢ncia entre duas vari√°veis $X_t$ e $X_s$ √© definida como $Cov(X_t, X_s) = E[(X_t - \mu_t)(X_s - \mu_s)]$, onde $\mu_t = E[X_t]$ e $\mu_s = E[X_s]$. Esta fun√ß√£o, que pode variar em fun√ß√£o de $t$ e $s$, descreve como duas vari√°veis no processo se relacionam linearmente.
*Prova:*
I.   Seja $\{X_t\}_{t=-\infty}^{\infty}$ um processo estoc√°stico.
II.  Para quaisquer dois instantes de tempo $t$ e $s$, temos vari√°veis aleat√≥rias $X_t$ e $X_s$.
III. A m√©dia de $X_t$ √© definida como $\mu_t = E[X_t]$, e a m√©dia de $X_s$ √© definida como $\mu_s = E[X_s]$.
IV.  A covari√¢ncia entre $X_t$ e $X_s$ √© definida como $Cov(X_t, X_s) = E[(X_t - \mu_t)(X_s - \mu_s)]$.
V. Esta medida descreve o quanto as vari√°veis $X_t$ e $X_s$ variam juntas linearmente em rela√ß√£o √†s suas respectivas m√©dias.
Portanto, a covari√¢ncia entre vari√°veis em um processo estoc√°stico √© definida como $Cov(X_t, X_s) = E[(X_t - \mu_t)(X_s - \mu_s)]$. ‚ñ†
> üí° **Exemplo Num√©rico:** Vamos calcular a covari√¢ncia entre $X_t$ e $X_{t+1}$ para o nosso processo de ru√≠do branco gaussiano com $\sigma^2 = 1$ do primeiro exemplo. Como o ru√≠do branco n√£o tem correla√ß√£o, a covari√¢ncia te√≥rica deve ser zero. Vamos verificar isso empiricamente.
> ```python
> import numpy as np
>
> T = 100
> sigma = 1
> epsilon = np.random.normal(0, np.sqrt(sigma), T)
>
> # Calcula a covari√¢ncia entre epsilon_t e epsilon_{t+1}
> cov_empirical = np.cov(epsilon[:-1], epsilon[1:])[0,1]
> print(f"Covari√¢ncia Emp√≠rica entre epsilon_t e epsilon_{t+1}: {cov_empirical}")
> ```
> O valor da covari√¢ncia emp√≠rica ser√° um valor pr√≥ximo a zero (mas n√£o exatamente zero devido ao tamanho finito da amostra), confirmando que as vari√°veis n√£o s√£o correlacionadas.

**Proposi√ß√£o 1.1** (Esperan√ßa de um Processo Estoc√°stico)
Seja $\{X_t\}_{t=-\infty}^{\infty}$ um processo estoc√°stico. A fun√ß√£o de esperan√ßa do processo √© dada por $\mu_t = E[X_t]$, que pode variar com o tempo $t$. Se $\mu_t = \mu$ para todo $t$, ent√£o o processo tem m√©dia constante.
*Prova:*
I. Seja $\{X_t\}_{t=-\infty}^{\infty}$ um processo estoc√°stico.
II.  Para cada instante de tempo $t$, $X_t$ √© uma vari√°vel aleat√≥ria.
III. A esperan√ßa de $X_t$ √© definida como $\mu_t = E[X_t]$.
IV.  A cole√ß√£o de esperan√ßas $\mu_t$ para todos os instantes de tempo $t$ forma a fun√ß√£o de esperan√ßa do processo.
V.  Se a fun√ß√£o de esperan√ßa $\mu_t$ √© constante para todo $t$, ou seja, $\mu_t = \mu$, ent√£o o processo tem m√©dia constante.
Portanto, a fun√ß√£o de esperan√ßa de um processo estoc√°stico √© dada por $\mu_t = E[X_t]$, e se ela for constante, o processo tem m√©dia constante. ‚ñ†

> üí° **Exemplo Num√©rico:** Para o nosso processo de ru√≠do branco gaussiano com $\epsilon_t \sim N(0, \sigma^2)$, a esperan√ßa te√≥rica √© $E[\epsilon_t] = 0$ para todos os tempos. Podemos estimar a m√©dia emp√≠rica de uma realiza√ß√£o:
> ```python
> import numpy as np
>
> T = 100
> sigma = 1
> epsilon = np.random.normal(0, np.sqrt(sigma), T)
>
> mean_empirical = np.mean(epsilon)
> print(f"M√©dia Emp√≠rica do processo: {mean_empirical}")
> ```
> Este valor ser√° pr√≥ximo de 0 para uma amostra suficientemente grande, confirmando a esperan√ßa te√≥rica.

**Proposi√ß√£o 1.2** (Vari√¢ncia de um Processo Estoc√°stico)
Seja $\{X_t\}_{t=-\infty}^{\infty}$ um processo estoc√°stico. A fun√ß√£o de vari√¢ncia do processo √© dada por $\sigma_t^2 = Var[X_t] = E[(X_t-\mu_t)^2]$, que tamb√©m pode variar com o tempo $t$. Se $\sigma_t^2 = \sigma^2$ para todo $t$, ent√£o o processo tem vari√¢ncia constante.
*Prova:*
I. Seja $\{X_t\}_{t=-\infty}^{\infty}$ um processo estoc√°stico.
II. Para cada instante de tempo $t$, $X_t$ √© uma vari√°vel aleat√≥ria com m√©dia $\mu_t$.
III.  A vari√¢ncia de $X_t$ √© definida como $\sigma_t^2 = Var[X_t] = E[(X_t-\mu_t)^2]$.
IV. A cole√ß√£o de vari√¢ncias $\sigma_t^2$ para todos os instantes de tempo $t$ forma a fun√ß√£o de vari√¢ncia do processo.
V. Se a fun√ß√£o de vari√¢ncia $\sigma_t^2$ √© constante para todo $t$, ou seja, $\sigma_t^2 = \sigma^2$, ent√£o o processo tem vari√¢ncia constante.
Portanto, a fun√ß√£o de vari√¢ncia de um processo estoc√°stico √© dada por $\sigma_t^2 = Var[X_t] = E[(X_t-\mu_t)^2]$, e se ela for constante, o processo tem vari√¢ncia constante. ‚ñ†

> üí° **Exemplo Num√©rico:** Para o nosso processo de ru√≠do branco gaussiano, a vari√¢ncia te√≥rica √© $\sigma^2$. Podemos estimar a vari√¢ncia empiricamente de uma realiza√ß√£o:
> ```python
> import numpy as np
>
> T = 100
> sigma = 1
> epsilon = np.random.normal(0, np.sqrt(sigma), T)
>
> variance_empirical = np.var(epsilon)
> print(f"Vari√¢ncia Emp√≠rica do processo: {variance_empirical}")
> ```
> O valor ser√° pr√≥ximo de 1, confirmando que a vari√¢ncia amostral aproxima a vari√¢ncia te√≥rica.

**Proposi√ß√£o 1.3** (Covari√¢ncia de um Processo Estoc√°stico)
Seja $\{X_t\}_{t=-\infty}^{\infty}$ um processo estoc√°stico. A fun√ß√£o de covari√¢ncia do processo √© dada por $Cov(X_t, X_s) = E[(X_t - \mu_t)(X_s - \mu_s)]$, que pode variar com o tempo $t$ e $s$. Se $Cov(X_t, X_s) = \gamma(t-s)$ para todo $t$ e $s$, ent√£o a covari√¢ncia do processo depende apenas do *lag* ($t-s$).
*Prova:*
I. Seja $\{X_t\}_{t=-\infty}^{\infty}$ um processo estoc√°stico.
II. Para quaisquer dois instantes de tempo $t$ e $s$, temos vari√°veis aleat√≥rias $X_t$ e $X_s$ com m√©dias $\mu_t$ e $\mu_s$, respectivamente.
III.  A covari√¢ncia entre $X_t$ e $X_s$ √© definida como $Cov(X_t, X_s) = E[(X_t - \mu_t)(X_s - \mu_s)]$.
IV. A cole√ß√£o de covari√¢ncias $Cov(X_t, X_s)$ para todos os pares de instantes de tempo $t$ e $s$ forma a fun√ß√£o de covari√¢ncia do processo.
V. Se a fun√ß√£o de covari√¢ncia depender apenas da diferen√ßa entre os instantes de tempo, ou seja, $Cov(X_t, X_s) = \gamma(t-s)$, ent√£o a covari√¢ncia do processo depende apenas do lag ($t-s$).
Portanto, a fun√ß√£o de covari√¢ncia de um processo estoc√°stico √© dada por $Cov(X_t, X_s) = E[(X_t - \mu_t)(X_s - \mu_s)]$, e se ela depender apenas de $t-s$, a covari√¢ncia depende apenas do lag. ‚ñ†

**Observa√ß√£o 1** (Autocovari√¢ncia)
Quando $t=s$, a covari√¢ncia $Cov(X_t,X_s)$ se reduz √† vari√¢ncia, ou seja $Cov(X_t,X_t)=Var(X_t)=\sigma_t^2$.

### Conclus√£o

Este cap√≠tulo iniciou a explora√ß√£o de processos estoc√°sticos, enfatizando a distin√ß√£o entre o processo em si e as suas realiza√ß√µes, destacando que os dados observados representam apenas uma amostra espec√≠fica de todas as poss√≠veis sequ√™ncias que o processo aleat√≥rio poderia gerar. Essa distin√ß√£o √© fundamental para o entendimento e modelagem de s√©ries temporais, pois nos permite perceber a natureza aleat√≥ria e probabil√≠stica dos dados que analisamos. Os conceitos de expectativas, estacionaridade e ergodicidade, que ser√£o abordados nas pr√≥ximas se√ß√µes, s√£o constru√≠dos sobre esta base, permitindo-nos fazer infer√™ncias sobre o processo subjacente a partir das realiza√ß√µes observadas.

### Refer√™ncias
[^1]:  *This chapter introduces univariate ARMA processes, which provide a very useful class of models for describing the dynamics of an individual time series. The chapter begins with definitions of some of the key concepts used in time series analysis.*
[^1]:  *Suppose we have observed a sample of size T of some random variable Yt: {Y1, Y2, ..., YT}.*
[^1]:  *For example, consider a collection of T independent and identically distributed (i.i.d.) variables Œµt, {Œµ1, Œµ2, ..., ŒµT}, with Œµt ~ N(0, œÉ2). This is referred to as a sample of size T from a Gaussian white noise process.*
[^1]:  *The observed sample [3.1.1] represents T particular numbers, but this set of T numbers is only one possible outcome of the underlying stochastic process that generated the data.*
[^1]: *Indeed, even if we were to imagine having observed the process for an infinite period of time, arriving at the sequence {yt}‚àû‚àí‚àû = {..., y‚àí1, y0, y1, y2, ..., yT, yT+1, yT+2, ...}, the infinite sequence {yt}‚àû‚àí‚àû would still be viewed as a single realization from a time series process.*
[^1]: *For example, we might set one computer to work generating an infinite sequence of i.i.d. N(0, œÉ2) variates, {Œµ(1)t}‚àû‚àí‚àû, and a second computer generating a separate sequence, {Œµ(2)t}‚àû‚àí‚àû. We would then view these as two independent realizations of a Gaussian white noise process.*
<!-- END -->
