## Modelagem AR(m) da Heteroskedasticidade Condicional

### Introdu√ß√£o

Este cap√≠tulo explora a modelagem da heteroskedasticidade condicional em s√©ries temporais, focando especificamente na abordagem de modelar o quadrado do termo de erro $u_t$ como um processo Autorregressivo (AR). Expandindo o conceito de **Heteroskedasticidade Condicional Autoregressiva (ARCH)** introduzido anteriormente, examinaremos como um modelo AR pode ser utilizado para capturar a depend√™ncia temporal da vari√¢ncia condicional. A an√°lise detalhada desta abordagem permite uma compreens√£o mais profunda da din√¢mica da vari√¢ncia em s√©ries temporais financeiras e econ√¥micas.

### Modelando $u_t^2$ como um Processo AR(m)

Uma forma de modelar a heteroskedasticidade condicional, ou seja, a vari√¢ncia condicional vari√°vel no tempo do termo de erro $u_t$, √© descrever o quadrado de $u_t$ como seguindo um processo autorregressivo de ordem *m* [^1]. Formalmente, isso √© expresso como:

$$
u_t^2 = \zeta + \alpha_1 u_{t-1}^2 + \alpha_2 u_{t-2}^2 + \dots + \alpha_m u_{t-m}^2 + w_t
$$

onde:

*   $u_t$ √© o termo de erro no tempo $t$.
*   $\zeta$ √© uma constante.
*   $\alpha_i$ s√£o os coeficientes autorregressivos para $i = 1, 2, \dots, m$.
*   $u_{t-i}^2$ s√£o os quadrados dos termos de erro nos tempos passados.
*   $w_t$ √© um processo de ru√≠do branco, significando que tem m√©dia zero e √© n√£o correlacionado serialmente [^1].

Esta equa√ß√£o [^1] descreve como o quadrado do termo de erro no tempo *t* ($u_t^2$) √© uma fun√ß√£o linear dos *m* valores passados de $u^2$ ($u_{t-1}^2, u_{t-2}^2,...,u_{t-m}^2$), mais um termo de ru√≠do branco ($w_t$). Este processo captura a ideia de que grandes (ou pequenos) erros quadr√°ticos passados tendem a ser seguidos por grandes (ou pequenos) erros quadr√°ticos no presente, refletindo a volatilidade agrupada observada em muitas s√©ries temporais financeiras [^1].

> üí° **Exemplo Num√©rico:** Suponha que estamos modelando a volatilidade di√°ria de um ativo financeiro usando um modelo AR(1) para $u_t^2$. Seja $\zeta = 0.01$ e $\alpha_1 = 0.6$. Se $u_{t-1}^2 = 0.04$ (o quadrado do erro no dia anterior foi 0.04), ent√£o a previs√£o para $u_t^2$ √©:
>
> $u_t^2 = 0.01 + 0.6 \times 0.04 + w_t = 0.01 + 0.024 + w_t = 0.034 + w_t$
>
> Se assumirmos que $w_t$ tem uma m√©dia de 0, a previs√£o para a volatilidade ao quadrado no dia *t* √© 0.034. Este exemplo demonstra como o modelo AR(1) usa a volatilidade passada para prever a volatilidade atual. Se, em vez disso, $\alpha_1$ fosse 0.9, a previs√£o seria $u_t^2 = 0.01 + 0.9 \times 0.04 = 0.046$, indicando uma maior persist√™ncia da volatilidade.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros do modelo
> zeta = 0.01
> alpha1 = 0.6
>
> # Valor de u_{t-1}^2
> u_tm1_squared = 0.04
>
> # Calculando u_t^2
> u_t_squared = zeta + alpha1 * u_tm1_squared
>
> print(f"Previs√£o de u_t^2: {u_t_squared}")
>
> # Visualiza√ß√£o
> plt.figure(figsize=(8, 6))
> plt.bar(['Previs√£o de u_t^2'], [u_t_squared], color='skyblue')
> plt.ylabel('Valor de u_t^2')
> plt.title('Previs√£o da Volatilidade ao Quadrado usando Modelo AR(1)')
> plt.ylim(0, 0.05)  # Definindo limites do eixo y para melhor visualiza√ß√£o
> plt.grid(axis='y')
> plt.show()
> ```

Para complementar essa modelagem, podemos considerar a distribui√ß√£o do termo de erro $w_t$. Tipicamente, assume-se que $w_t$ segue uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia constante, ou seja, $w_t \sim N(0, \sigma_w^2)$. No entanto, outras distribui√ß√µes, como a distribui√ß√£o t de Student, podem ser mais adequadas em algumas aplica√ß√µes, especialmente quando se lida com dados financeiros que apresentam caudas pesadas.

**Lema 1** Se $w_t \sim N(0, \sigma_w^2)$, ent√£o $E(w_t) = 0$ e $Var(w_t) = \sigma_w^2$. Al√©m disso, $E(w_t w_s) = 0$ para $t \neq s$, o que confirma a propriedade de ru√≠do branco.

*Prova do Lema 1:*
I. Dado que $w_t$ segue uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\sigma_w^2$, denotado como $w_t \sim N(0, \sigma_w^2)$, a fun√ß√£o de densidade de probabilidade (pdf) de $w_t$ √© sim√©trica em torno de zero.

II. A esperan√ßa de $w_t$, denotada $E(w_t)$, √© definida como a integral de $w_t$ multiplicado pela sua pdf sobre todo o seu suporte. Devido √† simetria da distribui√ß√£o normal em torno de zero, o valor esperado √© zero:
    $$E(w_t) = \int_{-\infty}^{\infty} w_t \cdot \frac{1}{\sqrt{2\pi\sigma_w^2}} e^{-\frac{w_t^2}{2\sigma_w^2}} dw_t = 0$$

III. A vari√¢ncia de $w_t$, denotada $Var(w_t)$, √© definida como $E[(w_t - E(w_t))^2]$. Uma vez que $E(w_t) = 0$, temos:
     $$Var(w_t) = E(w_t^2) = \int_{-\infty}^{\infty} w_t^2 \cdot \frac{1}{\sqrt{2\pi\sigma_w^2}} e^{-\frac{w_t^2}{2\sigma_w^2}} dw_t = \sigma_w^2$$
     Esta √© a vari√¢ncia definida da distribui√ß√£o normal.

IV. Para mostrar que $E(w_t w_s) = 0$ para $t \neq s$, usamos a propriedade de que $w_t$ √© um processo de ru√≠do branco, o que significa que √© serialmente n√£o correlacionado. Portanto, a covari√¢ncia entre $w_t$ e $w_s$ √© zero quando $t \neq s$:
    $$Cov(w_t, w_s) = E[(w_t - E(w_t))(w_s - E(w_s))] = E(w_t w_s) - E(w_t)E(w_s) = E(w_t w_s) = 0$$
    j√° que $E(w_t) = E(w_s) = 0$.

V. Assim, $E(w_t) = 0$, $Var(w_t) = \sigma_w^2$, e $E(w_t w_s) = 0$ para $t \neq s$, confirmando que $w_t$ √© um processo de ru√≠do branco. ‚ñ†

### Proje√ß√£o Linear e o Modelo ARCH(m)

A proje√ß√£o linear do erro quadr√°tico da previs√£o de $y_t$ nos *m* erros quadr√°ticos anteriores √© dada por [^1]:

$$
E(u_t^2 | u_{t-1}^2, u_{t-2}^2, \dots, u_{t-m}^2) = \zeta + \alpha_1 u_{t-1}^2 + \alpha_2 u_{t-2}^2 + \dots + \alpha_m u_{t-m}^2
$$

Este resultado [^1] expressa o valor esperado de $u_t^2$ dado o conhecimento dos *m* valores passados de $u^2$. Se o processo $u_t$ satisfaz esta equa√ß√£o e as condi√ß√µes estabelecidas acima, ent√£o dizemos que $u_t$ segue um processo ARCH de ordem *m*, denotado como $u_t \sim \text{ARCH}(m)$ [^1].

**Condi√ß√µes para uma Representa√ß√£o Sens√≠vel**

Como $u_t$ √© aleat√≥rio e $u_t^2$ n√£o pode ser negativo, a representa√ß√£o [^1] √© sens√≠vel somente se $E(u_t^2 | u_{t-1}^2, u_{t-2}^2, \dots, u_{t-m}^2)$ for positivo e a equa√ß√£o $u_t^2 = \zeta + \alpha_1 u_{t-1}^2 + \alpha_2 u_{t-2}^2 + \dots + \alpha_m u_{t-m}^2 + w_t$ for n√£o-negativa para todas as realiza√ß√µes de {u_t} [^1]. Isto pode ser assegurado se $w_t$ for limitado inferiormente por $-\zeta$ com $\zeta > 0$ e se $\alpha_j \geq 0$ para $j = 1, 2, \dots, m$ [^1].

> üí° **Exemplo Num√©rico:** Considere um modelo ARCH(2) com $\zeta = 0.02$, $\alpha_1 = 0.3$ e $\alpha_2 = 0.4$. Para garantir que $u_t^2$ seja n√£o-negativo, precisamos que $w_t$ seja limitado inferiormente por -0.02. Al√©m disso, todos os coeficientes $\alpha_j$ s√£o n√£o-negativos, como exigido. Se observarmos $u_{t-1}^2 = 0.05$ e $u_{t-2}^2 = 0.03$, ent√£o:
>
> $E(u_t^2 | u_{t-1}^2, u_{t-2}^2) = 0.02 + 0.3 \times 0.05 + 0.4 \times 0.03 = 0.02 + 0.015 + 0.012 = 0.047$
>
> Este valor √© positivo, indicando que a vari√¢ncia condicional prevista √© 0.047.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros do modelo ARCH(2)
> zeta = 0.02
> alpha1 = 0.3
> alpha2 = 0.4
>
> # Valores passados de u^2
> u_tm1_squared = 0.05
> u_tm2_squared = 0.03
>
> # Calculando a proje√ß√£o linear
> E_u_t_squared = zeta + alpha1 * u_tm1_squared + alpha2 * u_tm2_squared
>
> print(f"Proje√ß√£o linear de E(u_t^2): {E_u_t_squared}")
>
> # Visualiza√ß√£o
> plt.figure(figsize=(8, 6))
> plt.bar(['E(u_t^2)'], [E_u_t_squared], color='lightgreen')
> plt.ylabel('Valor Esperado de u_t^2')
> plt.title('Proje√ß√£o Linear de u_t^2 em um Modelo ARCH(2)')
> plt.ylim(0, 0.06)  # Ajustando os limites do eixo y
> plt.grid(axis='y')
> plt.show()
> ```

Para que $u_t^2$ seja covariance-stationary, requer-se adicionalmente que as ra√≠zes de $1 - \alpha_1 z - \alpha_2 z^2 - \dots - \alpha_m z^m = 0$ estejam fora do c√≠rculo unit√°rio [^1]. Se todos os $\alpha_j$ s√£o n√£o negativos, isto √© equivalente ao requisito de que $\alpha_1 + \alpha_2 + \dots + \alpha_m < 1$ [^1]. Quando estas condi√ß√µes s√£o satisfeitas, a vari√¢ncia incondicional de $u_t$ √© dada por $\sigma^2 = E(u_t^2) = \frac{\zeta}{1 - \alpha_1 - \alpha_2 - \dots - \alpha_m}$ [^1].

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior do modelo ARCH(2) com $\zeta = 0.02$, $\alpha_1 = 0.3$ e $\alpha_2 = 0.4$, verificamos a condi√ß√£o de estacionariedade:
>
> $\alpha_1 + \alpha_2 = 0.3 + 0.4 = 0.7 < 1$
>
> A condi√ß√£o de estacionariedade √© satisfeita. A vari√¢ncia incondicional de $u_t$ √©:
>
> $\sigma^2 = \frac{0.02}{1 - 0.3 - 0.4} = \frac{0.02}{0.3} \approx 0.0667$
>
> Isso significa que, a longo prazo, a vari√¢ncia do termo de erro se estabilizar√° em torno de 0.0667.
>
> ```python
> import numpy as np
>
> # Par√¢metros do modelo ARCH(2)
> zeta = 0.02
> alpha1 = 0.3
> alpha2 = 0.4
>
> # Verificando a condi√ß√£o de estacionariedade
> estacionaridade = alpha1 + alpha2 < 1
>
> # Calculando a vari√¢ncia incondicional
> if estacionaridade:
>     variancia_incondicional = zeta / (1 - alpha1 - alpha2)
>     print(f"A vari√¢ncia incondicional de u_t √©: {variancia_incondicional}")
> else:
>     print("O processo n√£o √© estacion√°rio.")
> ```

*Prova da Vari√¢ncia Incondicional:*
I. Assumimos que o processo $u_t^2$ √© covariance-stationary. Isso implica que $E(u_t^2)$ √© constante ao longo do tempo e igual √† vari√¢ncia incondicional $\sigma^2$. Assim, $E(u_t^2) = \sigma^2$ para todo $t$.

II. Tomando a esperan√ßa da equa√ß√£o $u_t^2 = \zeta + \alpha_1 u_{t-1}^2 + \alpha_2 u_{t-2}^2 + \dots + \alpha_m u_{t-m}^2 + w_t$, temos:
    $$E(u_t^2) = E(\zeta + \alpha_1 u_{t-1}^2 + \alpha_2 u_{t-2}^2 + \dots + \alpha_m u_{t-m}^2 + w_t)$$

III. Usando a linearidade do operador de esperan√ßa:
     $$E(u_t^2) = E(\zeta) + \alpha_1 E(u_{t-1}^2) + \alpha_2 E(u_{t-2}^2) + \dots + \alpha_m E(u_{t-m}^2) + E(w_t)$$

IV. Como o processo √© covariance-stationary, $E(u_t^2) = E(u_{t-1}^2) = \dots = E(u_{t-m}^2) = \sigma^2$. Al√©m disso, $E(w_t) = 0$ porque $w_t$ √© um ru√≠do branco. Substituindo estas igualdades, obtemos:
    $$\sigma^2 = \zeta + \alpha_1 \sigma^2 + \alpha_2 \sigma^2 + \dots + \alpha_m \sigma^2 + 0$$

V. Fatorando $\sigma^2$ do lado direito:
   $$\sigma^2 = \zeta + \sigma^2 (\alpha_1 + \alpha_2 + \dots + \alpha_m)$$

VI. Resolvendo para $\sigma^2$:
    $$\sigma^2 - \sigma^2 (\alpha_1 + \alpha_2 + \dots + \alpha_m) = \zeta$$
    $$\sigma^2 (1 - \alpha_1 - \alpha_2 - \dots - \alpha_m) = \zeta$$
    $$\sigma^2 = \frac{\zeta}{1 - \alpha_1 - \alpha_2 - \dots - \alpha_m}$$

VII. Portanto, a vari√¢ncia incondicional de $u_t$ √© dada por:
     $$\sigma^2 = E(u_t^2) = \frac{\zeta}{1 - \alpha_1 - \alpha_2 - \dots - \alpha_m}$$ ‚ñ†

Al√©m da estacionariedade de covari√¢ncia, √© importante considerar a exist√™ncia de momentos de ordem superior. Em particular, a exist√™ncia do quarto momento, $E(u_t^4)$, √© crucial para a validade de testes estat√≠sticos e infer√™ncias.

**Teorema 1** Para um processo ARCH(m), a exist√™ncia do quarto momento $E(u_t^4)$ requer condi√ß√µes mais restritivas do que a estacionariedade de covari√¢ncia.

*Prova (Esbo√ßo)* A prova envolve expressar $E(u_t^4)$ em termos dos coeficientes $\alpha_i$ e de momentos de $w_t$. A condi√ß√£o para a exist√™ncia de $E(u_t^4)$ envolve uma desigualdade que restringe ainda mais a magnitude dos $\alpha_i$ em compara√ß√£o com a condi√ß√£o para a estacionariedade de covari√¢ncia. Detalhes completos podem ser encontrados em Bollerslev (1988).

**Corol√°rio 1.1** Se um processo ARCH(m) satisfaz a condi√ß√£o $E(u_t^4) < \infty$, ent√£o ele tamb√©m satisfaz a condi√ß√£o de estacionariedade de covari√¢ncia. A rec√≠proca, no entanto, n√£o √© sempre verdadeira.

*Prova do Corol√°rio 1.1:*

I. Para um processo ARCH(m), a exist√™ncia do quarto momento $E(u_t^4) < \infty$ implica que a cauda da distribui√ß√£o de $u_t$ decai suficientemente r√°pido. Isso significa que valores extremos de $u_t$ s√£o menos prov√°veis em compara√ß√£o com um processo que n√£o tem um quarto momento finito.

II. A condi√ß√£o para a estacionariedade de covari√¢ncia, $\alpha_1 + \alpha_2 + \dots + \alpha_m < 1$, garante que a vari√¢ncia incondicional de $u_t$ seja finita. No entanto, ela n√£o restringe diretamente o comportamento da cauda da distribui√ß√£o.

III. A exist√™ncia do quarto momento finito imp√µe restri√ß√µes adicionais sobre os coeficientes $\alpha_i$ al√©m daquelas impostas pela condi√ß√£o de estacionariedade de covari√¢ncia. Essas restri√ß√µes garantem que os valores extremos de $u_t$ n√£o sejam t√£o frequentes a ponto de tornar o quarto momento infinito.

IV. Portanto, se $E(u_t^4) < \infty$, as condi√ß√µes para a estacionariedade de covari√¢ncia s√£o automaticamente satisfeitas porque as restri√ß√µes adicionais sobre os $\alpha_i$ implicam que $\alpha_1 + \alpha_2 + \dots + \alpha_m < 1$.

V. No entanto, o inverso n√£o √© verdadeiro. Um processo ARCH(m) pode satisfazer a condi√ß√£o de estacionariedade de covari√¢ncia sem ter um quarto momento finito. Isso ocorre quando os $\alpha_i$ s√£o tais que a vari√¢ncia incondicional √© finita, mas as caudas da distribui√ß√£o decaem muito lentamente, levando a um quarto momento infinito. ‚ñ†

### Conclus√£o

A modelagem da heteroskedasticidade condicional atrav√©s da descri√ß√£o do quadrado do termo de erro como um processo AR(m) oferece uma estrutura poderosa para capturar a din√¢mica da vari√¢ncia em s√©ries temporais. A proje√ß√£o linear do erro quadr√°tico da previs√£o fornece insights sobre como a informa√ß√£o passada influencia a volatilidade presente. As condi√ß√µes para uma representa√ß√£o sens√≠vel e para a estacionariedade da covari√¢ncia garantem que o modelo seja bem comportado e forne√ßa resultados interpret√°veis. A abordagem ARCH(m) √© um componente fundamental na an√°lise de s√©ries temporais financeiras, permitindo a modelagem e previs√£o da volatilidade com precis√£o aprimorada.

### Refer√™ncias

[^1]: Cap√≠tulo 21 do texto fornecido.
<!-- END -->