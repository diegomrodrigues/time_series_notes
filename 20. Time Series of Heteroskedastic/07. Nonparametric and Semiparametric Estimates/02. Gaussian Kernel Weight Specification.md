## Kernel Gaussiano na Estimativa N√£o Param√©trica da Heteroskedasticidade

### Introdu√ß√£o

Em continuidade ao cap√≠tulo anterior, que introduziu as estimativas n√£o param√©tricas da heteroskedasticidade condicional, este cap√≠tulo se aprofunda na especifica√ß√£o do kernel Gaussiano para o c√°lculo dos pesos na estimativa de kernel. O kernel Gaussiano √© amplamente utilizado devido √†s suas propriedades de suaviza√ß√£o e facilidade de implementa√ß√£o. Exploraremos a formula√ß√£o, interpreta√ß√£o e impacto do par√¢metro de *bandwidth* $\lambda$ na estimativa da vari√¢ncia condicional.

### Especifica√ß√£o do Kernel Gaussiano

Conforme introduzido anteriormente [^1], a estimativa de kernel n√£o param√©trica para a vari√¢ncia condicional $h_t$ √© dada por:

$$
h_t = \sum_{\tau=1}^{T} w_t(\tau) u_\tau^2
$$

onde os pesos $w_t(\tau)$ s√£o calculados usando uma fun√ß√£o kernel $\kappa_t(\tau)$. Uma **especifica√ß√£o popular para o peso** $w_t(\tau)$ √© o **kernel Gaussiano** [^1]:

$$
\kappa_t(\tau) = \prod_{j=1}^{m} (2\pi \lambda_j^2)^{-1/2} \exp\left[ -\frac{(u_{t-j} - u_{\tau-j})^2}{2\lambda_j^2} \right]
$$

Nesta formula√ß√£o, $m$ representa o n√∫mero de *lags* considerados para a estimativa, $u_{t-j}$ e $u_{\tau-j}$ s√£o os valores do processo no tempo $t-j$ e $\tau-j$, respectivamente, e $\lambda_j$ √© o par√¢metro de *bandwidth* para o $j$-√©simo *lag* [^1]. √â importante notar a corre√ß√£o na nota√ß√£o: no contexto, foi apresentado $\lambda_j$, mas o correto √© $\lambda_j^2$ dentro do termo exponencial, para que o par√¢metro represente a vari√¢ncia do kernel Gaussiano.

> üí° **Exemplo Num√©rico:** Considere um modelo com um √∫nico lag ($m=1$). Temos os seguintes dados: $u_{t-1} = 0.02$, $u_{\tau-1} = 0.015$, e um *bandwidth* $\lambda_1 = 0.005$. Vamos calcular $\kappa_t(\tau)$:
>
> $$ \kappa_t(\tau) = (2\pi \lambda_1^2)^{-1/2} \exp\left[ -\frac{(u_{t-1} - u_{\tau-1})^2}{2\lambda_1^2} \right] $$
>
> $$ \kappa_t(\tau) = (2\pi (0.005)^2)^{-1/2} \exp\left[ -\frac{(0.02 - 0.015)^2}{2(0.005)^2} \right] $$
>
> $$ \kappa_t(\tau) = (0.000157)^{-1/2} \exp\left[ -\frac{(0.005)^2}{2(0.000025)} \right] $$
>
> $$ \kappa_t(\tau) = 79.788 \cdot \exp\left[ -\frac{0.000025}{0.00005} \right] $$
>
> $$ \kappa_t(\tau) = 79.788 \cdot \exp(-0.5) $$
>
> $$ \kappa_t(\tau) = 79.788 \cdot 0.6065 \approx 48.40 $$
>
> Agora, suponha que tenhamos 5 observa√ß√µes, e calculamos os seguintes valores para $\kappa_t(\tau)$: $\kappa_t(1) = 48.40$, $\kappa_t(2) = 55.2$, $\kappa_t(3) = 60.1$, $\kappa_t(4) = 52.8$, $\kappa_t(5) = 45.9$. Para calcular os pesos normalizados, somamos os $\kappa_t(\tau)$ e dividimos cada $\kappa_t(\tau)$ pela soma. A soma √© $48.40 + 55.2 + 60.1 + 52.8 + 45.9 = 262.4$. Os pesos normalizados $w_t(\tau)$ s√£o ent√£o:
>
> $w_t(1) = 48.40/262.4 \approx 0.184$
> $w_t(2) = 55.2/262.4 \approx 0.210$
> $w_t(3) = 60.1/262.4 \approx 0.229$
> $w_t(4) = 52.8/262.4 \approx 0.201$
> $w_t(5) = 45.9/262.4 \approx 0.175$
>
> Note que a soma dos pesos √© aproximadamente 1 (0.184 + 0.210 + 0.229 + 0.201 + 0.175 = 0.999).  Esses pesos s√£o ent√£o usados para ponderar os valores passados de $u_\tau^2$ para obter a estimativa da vari√¢ncia condicional $h_t$.

> üí° **Exemplo Num√©rico:** Considere $m = 2$, $u_{t-1} = 0.01$, $u_{t-2} = -0.005$, $u_{\tau-1} = 0.012$, $u_{\tau-2} = -0.004$, $\lambda_1 = 0.003$ e $\lambda_2 = 0.002$. Ent√£o,
>
> $$ \kappa_t(\tau) = (2\pi \cdot 0.003^2)^{-1/2} \exp\left[ -\frac{(0.01 - 0.012)^2}{2 \cdot 0.003^2} \right] \cdot (2\pi \cdot 0.002^2)^{-1/2} \exp\left[ -\frac{(-0.005 - (-0.004))^2}{2 \cdot 0.002^2} \right] $$
>
> $$ \kappa_t(\tau) \approx 72.70 \cdot \exp\left[ -\frac{0.000004}{0.000018} \right] \cdot 199.47 \cdot \exp\left[ -\frac{0.000001}{0.000008} \right] $$
>
> $$ \kappa_t(\tau) \approx 72.70 \cdot 0.801 \cdot 199.47 \cdot 0.882 \approx 10256.4 $$
>
> Este valor de $\kappa_t(\tau)$ ser√° ent√£o usado para calcular o peso $w_t(\tau)$, dividindo-o pela soma de todos os $\kappa_t(\tau)$ para diferentes $\tau$.

O **par√¢metro de *bandwidth*** $\lambda_j$ desempenha um papel crucial na determina√ß√£o da suavidade da estimativa [^1]. Ele controla a taxa de decaimento do peso atribu√≠do aos valores passados de $u_\tau$ √† medida que se distanciam dos valores atuais $u_t$. Um valor pequeno de $\lambda_j$ resulta em um decaimento r√°pido, dando mais peso aos valores passados muito pr√≥ximos aos valores atuais, enquanto um valor grande de $\lambda_j$ produz um decaimento mais lento, permitindo que valores passados mais distantes exer√ßam influ√™ncia na estimativa.

> üí° **Exemplo Num√©rico:** Para ilustrar o efeito do *bandwidth*, consideremos dois cen√°rios. Em ambos os cen√°rios, $u_{t-1} = 0$. No primeiro cen√°rio, $\lambda_1 = 0.01$. No segundo cen√°rio, $\lambda_1 = 0.1$.  Vamos comparar os pesos atribu√≠dos a $u_{\tau-1} = 0.02$ nesses dois cen√°rios.
>
> **Cen√°rio 1: $\lambda_1 = 0.01$**
>
> $$ \kappa_t(\tau) = (2\pi (0.01)^2)^{-1/2} \exp\left[ -\frac{(0 - 0.02)^2}{2(0.01)^2} \right] $$
> $$ \kappa_t(\tau) = 39.89 \cdot \exp\left[ -\frac{0.0004}{0.0002} \right] = 39.89 \cdot e^{-2} \approx 39.89 \cdot 0.1353 \approx 5.40 $$
>
> **Cen√°rio 2: $\lambda_1 = 0.1$**
>
> $$ \kappa_t(\tau) = (2\pi (0.1)^2)^{-1/2} \exp\left[ -\frac{(0 - 0.02)^2}{2(0.1)^2} \right] $$
> $$ \kappa_t(\tau) = 3.989 \cdot \exp\left[ -\frac{0.0004}{0.02} \right] = 3.989 \cdot e^{-0.02} \approx 3.989 \cdot 0.9802 \approx 3.91 $$
>
> No primeiro cen√°rio, com um *bandwidth* menor, a fun√ß√£o kernel decai mais rapidamente.  Se considerarmos outro valor, $u_{\tau-1} = 0.001$, no cen√°rio 1, ter√≠amos:
> $$ \kappa_t(\tau) = 39.89 \cdot \exp\left[ -\frac{(0 - 0.001)^2}{2(0.01)^2} \right] = 39.89 \cdot e^{-0.005} \approx 39.89 \cdot 0.995 \approx 39.69 $$
> Isso demonstra que um *bandwidth* pequeno atribui um peso muito maior a valores pr√≥ximos (0.001) do que a valores mais distantes (0.02).  No segundo cen√°rio, a diferen√ßa no peso seria muito menor.

> üí° **Exemplo Num√©rico:** Se $\lambda_j$ √© muito pequeno, a estimativa $h_t$ ser√° muito sens√≠vel a pequenas flutua√ß√µes nos dados e pode resultar em uma estimativa ruidosa. Por outro lado, se $\lambda_j$ √© muito grande, a estimativa $h_t$ ser√° muito suave e pode n√£o capturar as mudan√ßas r√°pidas na vari√¢ncia condicional.

A **interpreta√ß√£o do kernel Gaussiano** reside na sua capacidade de aproximar a densidade de probabilidade dos valores passados em torno dos valores atuais. A fun√ß√£o exponencial no kernel Gaussiano atribui um peso maior aos valores passados que est√£o dentro de uma vizinhan√ßa definida pelo *bandwidth* $\lambda_j$ [^1]. Essa vizinhan√ßa define a regi√£o de influ√™ncia dos valores passados na estimativa da vari√¢ncia condicional no tempo $t$.

**Lema 1:** [Normaliza√ß√£o dos Pesos]
Os pesos $w_t(\tau)$ devem ser normalizados de forma que $\sum_{\tau=1}^{T} w_t(\tau) = 1$.

*Proof strategy:* A normaliza√ß√£o garante que a estimativa $h_t$ seja uma m√©dia ponderada dos valores passados de $u_\tau^2$, o que √© uma propriedade desej√°vel para uma estimativa da vari√¢ncia condicional.

*Proof:*

Para garantir que $h_t$ seja uma m√©dia ponderada dos valores passados de $u_\tau^2$, os pesos $w_t(\tau)$ devem satisfazer a condi√ß√£o:

$$
\sum_{\tau=1}^{T} w_t(\tau) = 1
$$

Os pesos s√£o calculados a partir do kernel Gaussiano $\kappa_t(\tau)$ da seguinte forma:

$$
w_t(\tau) = \frac{\kappa_t(\tau)}{\sum_{s=1}^{T} \kappa_t(s)}
$$

Substituindo esta express√£o na condi√ß√£o de normaliza√ß√£o, temos:

$$
\sum_{\tau=1}^{T} w_t(\tau) = \sum_{\tau=1}^{T} \frac{\kappa_t(\tau)}{\sum_{s=1}^{T} \kappa_t(s)}
$$

I. Fatorando o termo $\frac{1}{\sum_{s=1}^{T} \kappa_t(s)}$ da soma:
$$
\sum_{\tau=1}^{T} \frac{\kappa_t(\tau)}{\sum_{s=1}^{T} \kappa_t(s)} = \frac{1}{\sum_{s=1}^{T} \kappa_t(s)} \sum_{\tau=1}^{T} \kappa_t(\tau)
$$

II. Reorganizando a express√£o:
$$
\frac{1}{\sum_{s=1}^{T} \kappa_t(s)} \sum_{\tau=1}^{T} \kappa_t(\tau) = \frac{\sum_{\tau=1}^{T} \kappa_t(\tau)}{\sum_{s=1}^{T} \kappa_t(s)}
$$

III. Como a soma no numerador √© id√™ntica √† soma no denominador, a raz√£o √© igual a 1:
$$
\frac{\sum_{\tau=1}^{T} \kappa_t(\tau)}{\sum_{s=1}^{T} \kappa_t(s)} = 1
$$

Portanto, os pesos $w_t(\tau)$ calculados desta forma s√£o normalizados e satisfazem a condi√ß√£o de que sua soma seja igual a 1. ‚ñ†

### Sele√ß√£o do *Bandwidth*

A **sele√ß√£o apropriada do par√¢metro de *bandwidth*** $\lambda_j$ √© fundamental para o desempenho da estimativa de kernel. Um *bandwidth* inadequado pode levar a um *under smoothing* (vi√©s alto) ou *over smoothing* (vari√¢ncia alta) da estimativa. Conforme mencionado anteriormente [^1], a **valida√ß√£o cruzada** √© uma t√©cnica comum para selecionar o *bandwidth* ideal.

> üí° **Exemplo Num√©rico:** Suponha que estejamos usando valida√ß√£o cruzada para escolher o melhor *bandwidth*. Dividimos nossos dados em 5 folds. Para cada valor de $\lambda$ em um conjunto de valores candidatos (por exemplo, $\lambda = [0.001, 0.005, 0.01, 0.05, 0.1]$), iteramos sobre os folds. Em cada itera√ß√£o, usamos 4 folds para treinar o modelo e o fold restante para validar. Calculamos o Erro Quadr√°tico M√©dio (MSE) no fold de valida√ß√£o.  Ap√≥s iterar sobre todos os folds, calculamos o MSE m√©dio para cada valor de $\lambda$. O valor de $\lambda$ que minimiza o MSE m√©dio √© selecionado como o *bandwidth* ideal.
>
> Por exemplo, os resultados podem ser resumidos na seguinte tabela:
>
> | Bandwidth ($\lambda$) | MSE M√©dio |
> |-----------------------|------------|
> | 0.001                | 0.0008     |
> | 0.005                | 0.0005     |
> | 0.01                 | 0.0004     |
> | 0.05                 | 0.0006     |
> | 0.1                  | 0.0009     |
>
> Neste caso, $\lambda = 0.01$ seria selecionado como o *bandwidth* ideal, pois minimiza o MSE m√©dio.

Al√©m da valida√ß√£o cruzada, outros m√©todos de sele√ß√£o de *bandwidth* incluem:

1.  **Regras de bolso (*Rule-of-thumb*):** Essas regras fornecem estimativas iniciais para $\lambda_j$ com base em caracter√≠sticas dos dados, como o desvio padr√£o da amostra. Uma regra de bolso comum √© a regra de Silverman, que sugere [^1]:

$$
\lambda_j = 1.06 \cdot \hat{\sigma}_j \cdot T^{-1/5}
$$

onde $\hat{\sigma}_j$ √© o desvio padr√£o da amostra do $j$-√©simo *lag* e $T$ √© o tamanho da amostra.

> üí° **Exemplo Num√©rico:** Suponha que temos uma s√©rie temporal de retornos di√°rios com 2500 observa√ß√µes ($T = 2500$).  Calculamos o desvio padr√£o dos retornos ($\hat{\sigma}_j$) como 0.015.  Usando a regra de Silverman, temos:
>
> $$ \lambda_j = 1.06 \cdot 0.015 \cdot (2500)^{-1/5} $$
> $$ \lambda_j = 1.06 \cdot 0.015 \cdot (2500)^{-0.2} $$
> $$ \lambda_j = 1.06 \cdot 0.015 \cdot 0.177 $$
> $$ \lambda_j \approx 0.0028 $$
>
> Este valor de $\lambda_j = 0.0028$ pode ser usado como uma estimativa inicial para o *bandwidth*.

2.  **M√©todos Plug-in:** Esses m√©todos estimam os par√¢metros necess√°rios para calcular o *bandwidth* ideal diretamente dos dados. Eles envolvem a estimativa de derivadas da fun√ß√£o de densidade subjacente e podem ser computacionalmente intensivos.

A **escolha do m√©todo de sele√ß√£o de *bandwidth*** depende das caracter√≠sticas dos dados e dos objetivos da an√°lise. A valida√ß√£o cruzada √© geralmente prefer√≠vel quando a precis√£o da estimativa √© crucial, enquanto as regras de bolso podem ser √∫teis para obter uma estimativa inicial r√°pida do *bandwidth*.

**Corol√°rio 1** [Sensibilidade ao *Bandwidth*]
A precis√£o da estimativa de kernel √© sens√≠vel √† escolha do par√¢metro de *bandwidth* $\lambda$. Um valor de $\lambda$ muito pequeno resulta em uma estimativa ruidosa, enquanto um valor muito grande leva a um *over smoothing*, obscurecendo caracter√≠sticas importantes da vari√¢ncia condicional.

*Proof strategy:* A prova se baseia na an√°lise do vi√©s e da vari√¢ncia da estimativa de kernel em fun√ß√£o de $\lambda$. Um valor pequeno de $\lambda$ reduz o vi√©s, mas aumenta a vari√¢ncia, enquanto um valor grande de $\lambda$ reduz a vari√¢ncia, mas aumenta o vi√©s. A escolha ideal de $\lambda$ equilibra esses dois efeitos.

**Prova do Corol√°rio 1**

Para demonstrar a sensibilidade da estimativa de kernel √† escolha do *bandwidth* $\lambda$, analisamos o efeito de $\lambda$ no vi√©s e na vari√¢ncia da estimativa.

I. **Defini√ß√£o de Vi√©s:** O vi√©s de um estimador $\hat{h}_t$ √© definido como a diferen√ßa entre o valor esperado do estimador e o valor verdadeiro $h_t$:
   $$Bias(\hat{h}_t) = E[\hat{h}_t] - h_t$$

II. **Vi√©s e *Bandwidth* ($\lambda$):** Em estimadores de kernel, um *bandwidth* menor permite que o estimador capture varia√ß√µes mais r√°pidas nos dados, reduzindo o vi√©s. No entanto, isso tamb√©m torna o estimador mais sens√≠vel ao ru√≠do. Um *bandwidth* maior suaviza os dados, reduzindo a vari√¢ncia, mas pode levar a um vi√©s maior ao n√£o capturar varia√ß√µes importantes. Em geral, o vi√©s √© proporcional a $\lambda^2$.

III. **Defini√ß√£o de Vari√¢ncia:** A vari√¢ncia de um estimador $\hat{h}_t$ mede a dispers√£o das estimativas em torno de seu valor esperado:
    $$Var(\hat{h}_t) = E[(\hat{h}_t - E[\hat{h}_t])^2]$$

IV. **Vari√¢ncia e *Bandwidth* ($\lambda$):** A vari√¢ncia de um estimador de kernel √© inversamente proporcional a $\lambda$. Um *bandwidth* menor aumenta a vari√¢ncia, pois o estimador se torna mais sens√≠vel a flutua√ß√µes aleat√≥rias. Um *bandwidth* maior diminui a vari√¢ncia, pois o estimador suaviza essas flutua√ß√µes.

V. **Conclus√£o:** Portanto, a precis√£o da estimativa de kernel √© sens√≠vel √† escolha de $\lambda$. Um $\lambda$ muito pequeno causa alta vari√¢ncia (estimativa ruidosa), enquanto um $\lambda$ muito grande causa alto vi√©s (*over smoothing*). A escolha ideal de $\lambda$ equilibra esse *trade-off*. ‚ñ†

**Teorema 1:** [Converg√™ncia da Estimativa de Kernel]
Sob certas condi√ß√µes de regularidade sobre o processo $u_t$ e a fun√ß√£o kernel $\kappa_t(\tau)$, a estimativa de kernel $\hat{h}_t$ converge em probabilidade para a vari√¢ncia condicional verdadeira $h_t$ quando $T \rightarrow \infty$ e $\lambda \rightarrow 0$ tal que $T\lambda \rightarrow \infty$.

*Proof strategy:* A prova envolve mostrar que o vi√©s e a vari√¢ncia da estimativa de kernel convergem para zero sob as condi√ß√µes dadas.

**Prova do Teorema 1**
Para demonstrar a converg√™ncia da estimativa de kernel $\hat{h}_t$ para a vari√¢ncia condicional verdadeira $h_t$, precisamos mostrar que, sob certas condi√ß√µes, tanto o vi√©s quanto a vari√¢ncia da estimativa convergem para zero quando o tamanho da amostra $T$ tende ao infinito e o *bandwidth* $\lambda$ tende a zero de forma controlada.

I. **Defini√ß√£o de Converg√™ncia em Probabilidade:** Um estimador $\hat{h}_t$ converge em probabilidade para $h_t$ se, para todo $\epsilon > 0$:
   $$
   \lim_{T \to \infty} P(|\hat{h}_t - h_t| > \epsilon) = 0
   $$

II. **Rela√ß√£o entre MSE, Vi√©s e Vari√¢ncia:** O Erro Quadr√°tico M√©dio (MSE) de um estimador √© a soma do quadrado do vi√©s e da vari√¢ncia:
   $$
   MSE(\hat{h}_t) = E[(\hat{h}_t - h_t)^2] = Bias(\hat{h}_t)^2 + Var(\hat{h}_t)
   $$

III. **Condi√ß√µes para Converg√™ncia:** Para que $\hat{h}_t$ convirja em probabilidade para $h_t$, o MSE deve convergir para zero quando $T \to \infty$. Isso significa que tanto o vi√©s quanto a vari√¢ncia devem convergir para zero.

IV. **Comportamento do Vi√©s:** O vi√©s da estimativa de kernel √© geralmente proporcional a $\lambda^2$. Portanto, para que o vi√©s convirja para zero, precisamos que $\lambda \to 0$ quando $T \to \infty$:
    $$
    \lim_{T \to \infty} Bias(\hat{h}_t) = 0 \text{ se } \lambda \to 0
    $$

V. **Comportamento da Vari√¢ncia:** A vari√¢ncia da estimativa de kernel √© geralmente inversamente proporcional a $T\lambda$. Portanto, para que a vari√¢ncia convirja para zero, precisamos que $T\lambda \to \infty$ quando $T \to \infty$:
   $$
   \lim_{T \to \infty} Var(\hat{h}_t) = 0 \text{ se } T\lambda \to \infty
   $$

VI. **Conclus√£o:** Sob as condi√ß√µes de regularidade dadas (processo $u_t$ e fun√ß√£o kernel $\kappa_t(\tau)$), e se $\lambda \to 0$ e $T\lambda \to \infty$ quando $T \to \infty$, ent√£o tanto o vi√©s quanto a vari√¢ncia da estimativa de kernel convergem para zero, o que implica que o MSE converge para zero. Portanto, a estimativa de kernel $\hat{h}_t$ converge em probabilidade para a vari√¢ncia condicional verdadeira $h_t$. ‚ñ†

### Conclus√£o

O kernel Gaussiano oferece uma abordagem flex√≠vel e amplamente utilizada para a estimativa n√£o param√©trica da heteroskedasticidade condicional. A especifica√ß√£o do kernel Gaussiano permite a atribui√ß√£o de pesos diferenciados aos valores passados, com base em sua proximidade aos valores atuais. No entanto, a escolha do *bandwidth* $\lambda$ √© cr√≠tica para o desempenho da estimativa, exigindo t√©cnicas de sele√ß√£o apropriadas para equilibrar o vi√©s e a vari√¢ncia.

### Refer√™ncias

[^1]: Cap√≠tulo 21, "Time Series Models of Heteroskedasticity".
<!-- END -->