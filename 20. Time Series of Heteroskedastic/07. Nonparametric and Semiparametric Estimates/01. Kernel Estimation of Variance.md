## Estimativas N√£o Param√©tricas da Heteroskedasticidade Condicional

### Introdu√ß√£o

Em modelos de s√©ries temporais com heteroskedasticidade condicional, uma abordagem comum √© estimar a vari√¢ncia condicional utilizando modelos param√©tricos como ARCH e GARCH. No entanto, tamb√©m √© poss√≠vel empregar m√©todos n√£o param√©tricos e semiparam√©tricos para estimar a vari√¢ncia condicional, que oferecem flexibilidade ao evitar a imposi√ß√£o de formas funcionais espec√≠ficas. Este cap√≠tulo explora as estimativas n√£o param√©tricas, com foco na estimativa de kernel, para modelar a heteroskedasticidade condicional.

### Estimativas N√£o Param√©tricas com Kernel

As **estimativas n√£o param√©tricas** oferecem uma abordagem flex√≠vel para modelar a vari√¢ncia condicional, permitindo que os dados determinem a forma da rela√ß√£o entre os valores passados e a vari√¢ncia condicional atual, sem a imposi√ß√£o de uma estrutura param√©trica r√≠gida. Uma t√©cnica comum √© a **estimativa de kernel**, que calcula a vari√¢ncia condicional como uma m√©dia ponderada dos valores passados de $u_\tau^2$, onde os pesos s√£o determinados pela proximidade dos valores passados de $u_\tau$ aos valores atuais $u_t$ [^1].

> üí° **Exemplo Num√©rico:** Imagine que voc√™ est√° modelando a volatilidade di√°ria de uma a√ß√£o. Em vez de impor um modelo GARCH, voc√™ usa uma estimativa de kernel. Se a a√ß√£o teve retornos de $u_{t-1} = 0.01$, $u_{t-2} = -0.005$, e $u_{t-3} = 0.015$, e hoje o retorno √© $u_t = 0.008$, a estimativa de kernel dar√° mais peso aos dias em que os retornos foram pr√≥ximos a 0.008.

A **estimativa de kernel n√£o param√©trica** para a vari√¢ncia condicional $h_t$ √© dada por [^1]:

$$
h_t = \sum_{\tau=1}^{T} w_t(\tau) u_\tau^2
$$

onde $w_t(\tau)$ s√£o os pesos que medem a influ√™ncia do valor passado $u_\tau^2$ na estimativa da vari√¢ncia condicional no tempo $t$. Esses pesos s√£o calculados usando uma **fun√ß√£o kernel** $\kappa_t(\tau)$ [^1]:

$$
w_t(\tau) = \frac{\kappa_t(\tau)}{\sum_{\tau=1}^{T} \kappa_t(\tau)}
$$

A fun√ß√£o kernel $\kappa_t(\tau)$ atribui pesos maiores aos valores passados de $u_\tau$ que s√£o "pr√≥ximos" aos valores atuais $u_t$, e pesos menores aos valores passados que s√£o mais distantes [^1]. A escolha da fun√ß√£o kernel e do par√¢metro de suaviza√ß√£o (bandwidth) √© crucial para o desempenho da estimativa.

> üí° **Exemplo Num√©rico:** Se $\kappa_t(\tau)$ √© uma fun√ß√£o que vale 1 quando $|u_t - u_\tau| < 0.003$ e 0 caso contr√°rio, ent√£o apenas os retornos passados que est√£o dentro de 0.3% do retorno atual ter√£o algum peso na estimativa da vari√¢ncia condicional.

Uma especifica√ß√£o popular para o peso $w_t(\tau)$ √© usar um **kernel Gaussiano**:

$$
\kappa_t(\tau) = \prod_{j=1}^{m} (2\pi \lambda_j)^{-1/2} \exp\left[ -\frac{(u_{t-j} - u_{\tau-j})^2}{2\lambda_j} \right]
$$

onde $\lambda_j$ √© o par√¢metro de suaviza√ß√£o (bandwidth) para o $j$-√©simo *lag* [^1]. O par√¢metro $\lambda_j$ controla a largura da janela do kernel, determinando o qu√£o "pr√≥ximos" os valores passados devem estar para ter uma influ√™ncia significativa na estimativa de $h_t$ [^1]. Um valor menor de $\lambda_j$ implica que apenas os valores passados muito pr√≥ximos aos valores atuais ter√£o um peso substancial, enquanto um valor maior de $\lambda_j$ permite que valores passados mais distantes influenciem a estimativa.

> üí° **Exemplo Num√©rico:** Suponha que $m = 1$ e $\lambda_1 = 0.001$. Se $u_t = 0.01$ e $u_{\tau} = 0.011$, ent√£o $\kappa_t(\tau) = (2\pi \cdot 0.001)^{-1/2} \exp\left[ -\frac{(0.01 - 0.011)^2}{2 \cdot 0.001} \right] \approx 19.947 \cdot \exp(-0.005) \approx 19.847$. Se $u_{\tau} = 0.02$, ent√£o $\kappa_t(\tau) \approx 19.947 \cdot \exp(-5) \approx 0.134$. Note como o kernel Gaussiano penaliza rapidamente os valores mais distantes.

A **sele√ß√£o do par√¢metro de suaviza√ß√£o** $\lambda_j$ √© um desafio fundamental na estimativa de kernel. Uma abordagem comum √© a **valida√ß√£o cruzada** (*cross-validation*), que envolve a escolha do valor de $\lambda_j$ que minimiza o erro quadr√°tico m√©dio da previs√£o da vari√¢ncia condicional [^1]. Especificamente, se o mesmo *bandwidth* √© selecionado para todos os *lags* ($\lambda_j = \lambda$ para todo $j = 1, 2, ..., m$), a estimativa n√£o param√©trica de $h_t$ pode ser denotada como $h_t(\lambda)$, e $\lambda$ pode ser escolhido para minimizar [^1]:

$$
\sum_{t=1}^{T} [u_t^2 - h_t(\lambda)]^2
$$

> üí° **Exemplo Num√©rico:** Imagine que voc√™ tem uma s√©rie temporal de 1000 retornos di√°rios. Voc√™ tenta diferentes valores de $\lambda$ (e.g., 0.0001, 0.001, 0.01). Para cada $\lambda$, voc√™ calcula $h_t(\lambda)$ e, em seguida, calcula o erro quadr√°tico m√©dio. O $\lambda$ que resulta no menor erro quadr√°tico m√©dio √© o escolhido.

Al√©m da valida√ß√£o cruzada, outras t√©cnicas de sele√ß√£o de *bandwidth* incluem m√©todos baseados em regras de bolso (*rule-of-thumb*), que fornecem estimativas iniciais para $\lambda$ com base em caracter√≠sticas dos dados, e m√©todos plug-in, que estimam os par√¢metros necess√°rios para calcular o *bandwidth* ideal diretamente dos dados. A escolha do m√©todo de sele√ß√£o de *bandwidth* pode ter um impacto significativo no desempenho da estimativa de kernel.

**Teorema 1** [Consist√™ncia da Estimativa de Kernel]
Sob condi√ß√µes de regularidade apropriadas sobre a fun√ß√£o kernel, o processo estoc√°stico $u_t$, e a taxa de decaimento do *bandwidth* $\lambda$, a estimativa de kernel $\hat{h}_t$ converge em probabilidade para a verdadeira vari√¢ncia condicional $h_t$ quando $T \to \infty$.

*Proof strategy:* A prova envolve demonstrar que o vi√©s e a vari√¢ncia da estimativa de kernel convergem para zero quando o tamanho da amostra aumenta. Isso requer impor condi√ß√µes sobre a suavidade da fun√ß√£o kernel, o comportamento de depend√™ncia dos dados e a taxa em que o *bandwidth* converge para zero.

**Prova do Teorema 1**

Para provar a consist√™ncia da estimativa de kernel $\hat{h}_t$, devemos mostrar que $\hat{h}_t$ converge em probabilidade para $h_t$ quando $T \rightarrow \infty$, ou seja, para qualquer $\epsilon > 0$:

$$
\lim_{T \to \infty} P(|\hat{h}_t - h_t| > \epsilon) = 0
$$

I. **Defini√ß√£o da Estimativa e Erro:**

   A estimativa de kernel √© definida como $\hat{h}_t = \sum_{\tau=1}^{T} w_t(\tau) u_\tau^2$, onde $w_t(\tau) = \frac{\kappa_t(\tau)}{\sum_{\tau=1}^{T} \kappa_t(\tau)}$. O erro √© ent√£o dado por $\hat{h}_t - h_t$.

II. **Decomposi√ß√£o do Erro:**

   Podemos decompor o erro em dois componentes: o vi√©s e o erro estoc√°stico.  Primeiro, expressamos $h_t$ como a esperan√ßa condicional de $u_t^2$ dado o passado: $h_t = E[u_t^2 | u_{t-1}, u_{t-2}, \ldots]$. Ent√£o,

   $$\hat{h}_t - h_t = \left( \sum_{\tau=1}^{T} w_t(\tau) u_\tau^2 - E[u_t^2 | u_{t-1}, u_{t-2}, \ldots] \right)$$

III. **Condi√ß√µes de Regularidade:**

   Assumimos as seguintes condi√ß√µes de regularidade:

   a) A fun√ß√£o kernel $\kappa_t(\tau)$ √© sim√©trica e integra para 1, ou seja, $\int \kappa_t(\tau) d\tau = 1$.
   b) O processo estoc√°stico $u_t$ √© estacion√°rio e fracamente dependente (e.g., $\alpha$-mixing).
   c) O *bandwidth* $\lambda$ satisfaz $\lambda \to 0$ e $T\lambda \to \infty$ quando $T \to \infty$.

IV. **An√°lise do Vi√©s:**

   O vi√©s da estimativa √© dado por:
   $$Bias(\hat{h}_t) = E[\hat{h}_t] - h_t = E\left[ \sum_{\tau=1}^{T} w_t(\tau) u_\tau^2 \right] - h_t$$
   Sob as condi√ß√µes de regularidade mencionadas, o vi√©s converge para zero quando $T \to \infty$.  Isso ocorre porque a fun√ß√£o kernel concentra os pesos em torno dos valores passados pr√≥ximos a $u_t$, e, √† medida que $\lambda$ diminui, a m√©dia ponderada se aproxima da esperan√ßa condicional local.

V. **An√°lise da Vari√¢ncia:**

   A vari√¢ncia da estimativa √© dada por:
   $$Var(\hat{h}_t) = Var\left( \sum_{\tau=1}^{T} w_t(\tau) u_\tau^2 \right)$$
   Sob as condi√ß√µes de regularidade e a condi√ß√£o de depend√™ncia fraca de $u_t$, a vari√¢ncia converge para zero quando $T \to \infty$. Isso ocorre porque o n√∫mero efetivo de observa√ß√µes utilizadas na estimativa aumenta com $T$, enquanto os pesos $w_t(\tau)$ diminuem.

VI. **Converg√™ncia em Probabilidade:**

   Usando a desigualdade de Chebyshev:
   $$P(|\hat{h}_t - h_t| > \epsilon) \leq \frac{E[(\hat{h}_t - h_t)^2]}{\epsilon^2} = \frac{Bias(\hat{h}_t)^2 + Var(\hat{h}_t)}{\epsilon^2}$$
   Como $Bias(\hat{h}_t) \to 0$ e $Var(\hat{h}_t) \to 0$ quando $T \to \infty$, temos:
   $$\lim_{T \to \infty} P(|\hat{h}_t - h_t| > \epsilon) = 0$$
   Portanto, $\hat{h}_t$ converge em probabilidade para $h_t$. ‚ñ†

### Considera√ß√µes e Desafios

Embora as estimativas n√£o param√©tricas ofere√ßam flexibilidade, elas tamb√©m apresentam desafios. A escolha do *bandwidth* √© crucial, pois um *bandwidth* muito pequeno pode resultar em uma estimativa ruidosa, enquanto um *bandwidth* muito grande pode levar a um *over smoothing*, obscurecendo caracter√≠sticas importantes da vari√¢ncia condicional [^1]. Al√©m disso, a estimativa de kernel pode sofrer da **maldi√ß√£o da dimensionalidade** quando o n√∫mero de *lags* ($m$) √© grande, pois a quantidade de dados necess√°ria para obter uma estimativa precisa aumenta exponencialmente com o n√∫mero de vari√°veis [^1].

> üí° **Exemplo Num√©rico:** Se voc√™ est√° usando 10 lags ($m=10$) para estimar a vari√¢ncia condicional, o n√∫mero de combina√ß√µes poss√≠veis de valores passados de $u_t$ cresce exponencialmente. Isso significa que voc√™ precisa de uma quantidade enorme de dados para cobrir todas as combina√ß√µes poss√≠veis e obter uma estimativa precisa.

Para mitigar a maldi√ß√£o da dimensionalidade, t√©cnicas de redu√ß√£o de dimensionalidade, como a an√°lise de componentes principais (PCA), podem ser aplicadas aos *lags* de $u_t$ antes da estimativa de kernel. Al√©m disso, modelos aditivos generalizados (GAMs) oferecem uma alternativa flex√≠vel para modelar a vari√¢ncia condicional n√£o parametricamente, permitindo que cada *lag* de $u_t$ contribua para a estimativa de $h_t$ de forma aditiva, o que pode reduzir o problema da dimensionalidade.

> üí° **Exemplo Num√©rico:** Suponha que voc√™ tem retornos di√°rios de uma a√ß√£o nos √∫ltimos 5 anos e deseja usar os √∫ltimos 10 dias para prever a vari√¢ncia condicional de hoje. Usando PCA, voc√™ pode reduzir esses 10 *lags* para 2 ou 3 componentes principais, que capturam a maior parte da varia√ß√£o nos dados. Ent√£o, voc√™ pode usar esses componentes principais como entradas para a sua estimativa de kernel, reduzindo a dimensionalidade do problema.

**Lema 1** [Converg√™ncia em M√©dia Quadr√°tica]
Se a fun√ß√£o kernel $\kappa_t(\tau)$ satisfaz as condi√ß√µes de Lipschitz e os dados $u_t$ s√£o estacion√°rios e fracamente dependentes, ent√£o, com a escolha apropriada do *bandwidth* $\lambda$, a estimativa de kernel converge em m√©dia quadr√°tica para a verdadeira vari√¢ncia condicional $h_t$.

*Proof strategy:* A prova se baseia na demonstra√ß√£o de que o erro quadr√°tico m√©dio da estimativa de kernel, $E[(\hat{h}_t - h_t)^2]$, converge para zero quando $T \to \infty$. Isso requer impor condi√ß√µes sobre a suavidade da fun√ß√£o kernel, o comportamento de depend√™ncia dos dados e a taxa em que o *bandwidth* converge para zero.

**Prova do Lema 1**

Para provar a converg√™ncia em m√©dia quadr√°tica, devemos mostrar que:

$$
\lim_{T \to \infty} E[(\hat{h}_t - h_t)^2] = 0
$$

I. **Defini√ß√£o do Erro Quadr√°tico M√©dio (MSE):**

   O erro quadr√°tico m√©dio √© definido como $MSE = E[(\hat{h}_t - h_t)^2]$. Ele pode ser decomposto em vi√©s ao quadrado e vari√¢ncia:

   $$
   MSE = E[(\hat{h}_t - h_t)^2] = [Bias(\hat{h}_t)]^2 + Var(\hat{h}_t)
   $$
   onde $Bias(\hat{h}_t) = E[\hat{h}_t] - h_t$ e $Var(\hat{h}_t) = E[(\hat{h}_t - E[\hat{h}_t])^2]$.

II. **Condi√ß√µes de Lipschitz:**

   Assumimos que a fun√ß√£o kernel $\kappa_t(\tau)$ satisfaz a condi√ß√£o de Lipschitz, ou seja, existe uma constante $L > 0$ tal que:

   $$
   |\kappa_t(\tau_1) - \kappa_t(\tau_2)| \leq L |\tau_1 - \tau_2|
   $$

   Isso garante que a fun√ß√£o kernel seja suficientemente suave.

III. **Dados Estacion√°rios e Fracamente Dependentes:**

   Assumimos que os dados $u_t$ s√£o estacion√°rios e fracamente dependentes (e.g., $\alpha$-mixing). Isso implica que as autocovari√¢ncias decaem rapidamente com o aumento da dist√¢ncia no tempo.

IV. **An√°lise do Vi√©s:**

   Conforme mostrado na prova do Teorema 1, sob as condi√ß√µes de regularidade e a escolha apropriada do *bandwidth* $\lambda$, o vi√©s converge para zero quando $T \to \infty$:

   $$
   \lim_{T \to \infty} Bias(\hat{h}_t) = 0
   $$

V. **An√°lise da Vari√¢ncia:**

   A vari√¢ncia da estimativa de kernel pode ser expressa como:

   $$
   Var(\hat{h}_t) = Var\left(\sum_{\tau=1}^{T} w_t(\tau) u_\tau^2\right) = \sum_{\tau_1=1}^{T} \sum_{\tau_2=1}^{T} w_t(\tau_1) w_t(\tau_2) Cov(u_{\tau_1}^2, u_{\tau_2}^2)
   $$

   Devido √† estacionariedade e √† depend√™ncia fraca dos dados, as autocovari√¢ncias $Cov(u_{\tau_1}^2, u_{\tau_2}^2)$ decaem rapidamente quando $|\tau_1 - \tau_2|$ aumenta. Al√©m disso, os pesos $w_t(\tau)$ s√£o determinados pela fun√ß√£o kernel e pelo *bandwidth* $\lambda$.  Com a escolha apropriada de $\lambda$ tal que $\lambda \to 0$ e $T\lambda \to \infty$ quando $T \to \infty$, a vari√¢ncia tamb√©m converge para zero quando $T \to \infty$:

   $$
   \lim_{T \to \infty} Var(\hat{h}_t) = 0
   $$

VI. **Converg√™ncia em M√©dia Quadr√°tica:**

   Como o vi√©s ao quadrado e a vari√¢ncia convergem para zero quando $T \to \infty$, o erro quadr√°tico m√©dio tamb√©m converge para zero:

   $$
   \lim_{T \to \infty} E[(\hat{h}_t - h_t)^2] = \lim_{T \to \infty} \left([Bias(\hat{h}_t)]^2 + Var(\hat{h}_t)\right) = 0
   $$

   Portanto, a estimativa de kernel $\hat{h}_t$ converge em m√©dia quadr√°tica para a verdadeira vari√¢ncia condicional $h_t$. ‚ñ†

### Conclus√£o

As estimativas n√£o param√©tricas, como a estimativa de kernel, oferecem uma alternativa flex√≠vel aos modelos param√©tricos tradicionais para modelar a heteroskedasticidade condicional. Ao permitir que os dados determinem a forma da rela√ß√£o entre os valores passados e a vari√¢ncia condicional atual, esses m√©todos podem capturar padr√µes complexos que os modelos param√©tricos podem n√£o conseguir identificar. No entanto, a escolha do *bandwidth* e a maldi√ß√£o da dimensionalidade s√£o desafios importantes que devem ser considerados ao aplicar essas t√©cnicas. As estimativas n√£o param√©tricas podem ser combinadas com abordagens param√©tricas em modelos semiparam√©tricos, o que pode proporcionar um equil√≠brio entre flexibilidade e interpretabilidade.

**Proposi√ß√£o 1** [Intervalos de Confian√ßa Assint√≥ticos]
Sob certas condi√ß√µes de regularidade, um intervalo de confian√ßa assint√≥tico para a vari√¢ncia condicional $h_t$ pode ser constru√≠do usando a distribui√ß√£o normal assint√≥tica da estimativa de kernel $\hat{h}_t$.

*Proof strategy:* A prova envolve derivar a distribui√ß√£o assint√≥tica da estimativa de kernel usando o teorema do limite central. Isso requer impor condi√ß√µes sobre a suavidade da fun√ß√£o kernel, o comportamento de depend√™ncia dos dados e a taxa em que o *bandwidth* converge para zero. O intervalo de confian√ßa √© ent√£o constru√≠do usando a estimativa da vari√¢ncia assint√≥tica.

**Prova da Proposi√ß√£o 1**

Para construir um intervalo de confian√ßa assint√≥tico para $h_t$, precisamos estabelecer a distribui√ß√£o assint√≥tica de $\hat{h}_t$.

I. **Defini√ß√£o da Estimativa e Condi√ß√µes:**

   Seja $\hat{h}_t = \sum_{\tau=1}^{T} w_t(\tau) u_\tau^2$ a estimativa de kernel da vari√¢ncia condicional $h_t$. Assumimos as seguintes condi√ß√µes de regularidade:

   a) A fun√ß√£o kernel $\kappa_t(\tau)$ √© sim√©trica e de ordem superior (i.e., $\int u\kappa(u)du = 0$).
   b) O processo $u_t^2$ √© estacion√°rio, $\alpha$-mixing com coeficientes de mixing que decaem suficientemente r√°pido.
   c) O *bandwidth* $\lambda$ satisfaz $\lambda \to 0$, $T\lambda \to \infty$, e $T\lambda^4 \to 0$ quando $T \to \infty$.

II. **Teorema do Limite Central (TLC):**

   Sob as condi√ß√µes acima, podemos aplicar um Teorema do Limite Central para processos fracamente dependentes.  Especificamente, podemos mostrar que:
   $$\sqrt{T\lambda} (\hat{h}_t - h_t) \xrightarrow{d} N(0, \sigma^2)$$
   onde $\sigma^2$ √© a vari√¢ncia assint√≥tica da estimativa de kernel e $\xrightarrow{d}$ denota converg√™ncia em distribui√ß√£o.

III. **Express√£o da Vari√¢ncia Assint√≥tica:**

   A vari√¢ncia assint√≥tica $\sigma^2$ √© dada por:
    $$\sigma^2 = \lim_{T \to \infty} T \lambda Var(\hat{h}_t) = \sigma_k^2 E[u_t^4 | u_{t-1}, u_{t-2}, \ldots ] = \sigma_k^2 E[u_t^4 | \mathcal{F}_{t-1}]$$
    Onde $\sigma_k^2 = \int \kappa^2(u) du$ e $\mathcal{F}_{t-1}$ denota a informa√ß√£o dispon√≠vel at√© o tempo $t-1$.

IV. **Estimativa da Vari√¢ncia Assint√≥tica:**

    Para tornar o resultado do TLC utiliz√°vel, precisamos estimar $\sigma^2$. Uma estimativa consistente √© dada por:

    $$\hat{\sigma}^2 = \hat{\sigma}_k^2 \frac{1}{T} \sum_{t=1}^{T} u_t^4$$

    Onde $\hat{\sigma}_k^2$ √© uma estimativa de $\sigma_k^2$.

V. **Intervalo de Confian√ßa Assint√≥tico:**

   Com a distribui√ß√£o assint√≥tica e uma estimativa consistente da vari√¢ncia, podemos construir um intervalo de confian√ßa assint√≥tico para $h_t$ com n√≠vel de confian√ßa $1-\alpha$:

   $$P\left( \hat{h}_t - z_{\alpha/2} \frac{\hat{\sigma}}{\sqrt{T\lambda}} \leq h_t \leq \hat{h}_t + z_{\alpha/2} \frac{\hat{\sigma}}{\sqrt{T\lambda}} \right) \approx 1 - \alpha$$

   onde $z_{\alpha/2}$ √© o quantil $(1 - \alpha/2)$-√©simo da distribui√ß√£o normal padr√£o. Portanto, o intervalo de confian√ßa assint√≥tico √© dado por:

   $$\left[ \hat{h}_t - z_{\alpha/2} \frac{\hat{\sigma}}{\sqrt{T\lambda}}, \hat{h}_t + z_{\alpha/2} \frac{\hat{\sigma}}{\sqrt{T\lambda}} \right]$$

   Esse intervalo de confian√ßa fornece uma medida da incerteza associada √† estimativa de kernel da vari√¢ncia condicional. ‚ñ†

> üí° **Exemplo Num√©rico:** Usando os dados de retornos di√°rios da a√ß√£o, voc√™ estima $\hat{h}_t = 0.00015$ (ou seja, 0.015%). Voc√™ tamb√©m estima $\hat{\sigma}^2 = 0.0000001$. Se voc√™ tem 500 dias de dados e escolheu $\lambda = 0.001$, e quer um intervalo de confian√ßa de 95% (ent√£o $z_{\alpha/2} \approx 1.96$), ent√£o o intervalo de confian√ßa √©:
>
> $$ \left[ 0.00015 - 1.96 \cdot \frac{\sqrt{0.0000001}}{\sqrt{500 \cdot 0.001}}, 0.00015 + 1.96 \cdot \frac{\sqrt{0.0000001}}{\sqrt{500 \cdot 0.001}} \right] $$
>
> $$ \left[ 0.00015 - 1.96 \cdot \frac{0.000316}{0.707}, 0.00015 + 1.96 \cdot \frac{0.000316}{0.707} \right] $$
>
> $$ \left[ 0.00015 - 0.000875, 0.00015 + 0.000875 \right] $$
>
> $$ \left[ -0.000725, 0.001025 \right] $$
>
>  Note que o limite inferior √© negativo, o que n√£o faz sentido para uma vari√¢ncia. Isso √© comum em pr√°tica, especialmente quando a volatilidade √© baixa, e geralmente indica que um modelo mais complexo ou um per√≠odo de tempo maior √© necess√°rio.

### Refer√™ncias

[^1]: Cap√≠tulo 21, "Time Series Models of Heteroskedasticity".

<!-- END -->