## Estimativas NÃ£o ParamÃ©tricas da Heteroskedasticidade Condicional

### IntroduÃ§Ã£o

Em modelos de sÃ©ries temporais com heteroskedasticidade condicional, uma abordagem comum Ã© estimar a variÃ¢ncia condicional utilizando modelos paramÃ©tricos como ARCH e GARCH. No entanto, tambÃ©m Ã© possÃ­vel empregar mÃ©todos nÃ£o paramÃ©tricos e semiparamÃ©tricos para estimar a variÃ¢ncia condicional, que oferecem flexibilidade ao evitar a imposiÃ§Ã£o de formas funcionais especÃ­ficas. Este capÃ­tulo explora as estimativas nÃ£o paramÃ©tricas, com foco na estimativa de kernel, para modelar a heteroskedasticidade condicional.

### Estimativas NÃ£o ParamÃ©tricas com Kernel

As **estimativas nÃ£o paramÃ©tricas** oferecem uma abordagem flexÃ­vel para modelar a variÃ¢ncia condicional, permitindo que os dados determinem a forma da relaÃ§Ã£o entre os valores passados e a variÃ¢ncia condicional atual, sem a imposiÃ§Ã£o de uma estrutura paramÃ©trica rÃ­gida. Uma tÃ©cnica comum Ã© a **estimativa de kernel**, que calcula a variÃ¢ncia condicional como uma mÃ©dia ponderada dos valores passados de $u_\tau^2$, onde os pesos sÃ£o determinados pela proximidade dos valores passados de $u_\tau$ aos valores atuais $u_t$ [^1].

> ğŸ’¡ **Exemplo NumÃ©rico:** Imagine que vocÃª estÃ¡ modelando a volatilidade diÃ¡ria de uma aÃ§Ã£o. Em vez de impor um modelo GARCH, vocÃª usa uma estimativa de kernel. Se a aÃ§Ã£o teve retornos de $u_{t-1} = 0.01$, $u_{t-2} = -0.005$, e $u_{t-3} = 0.015$, e hoje o retorno Ã© $u_t = 0.008$, a estimativa de kernel darÃ¡ mais peso aos dias em que os retornos foram prÃ³ximos a 0.008.

A **estimativa de kernel nÃ£o paramÃ©trica** para a variÃ¢ncia condicional $h_t$ Ã© dada por [^1]:

$$
h_t = \sum_{\tau=1}^{T} w_t(\tau) u_\tau^2
$$

onde $w_t(\tau)$ sÃ£o os pesos que medem a influÃªncia do valor passado $u_\tau^2$ na estimativa da variÃ¢ncia condicional no tempo $t$. Esses pesos sÃ£o calculados usando uma **funÃ§Ã£o kernel** $\kappa_t(\tau)$ [^1]:

$$
w_t(\tau) = \frac{\kappa_t(\tau)}{\sum_{\tau=1}^{T} \kappa_t(\tau)}
$$

A funÃ§Ã£o kernel $\kappa_t(\tau)$ atribui pesos maiores aos valores passados de $u_\tau$ que sÃ£o "prÃ³ximos" aos valores atuais $u_t$, e pesos menores aos valores passados que sÃ£o mais distantes [^1]. A escolha da funÃ§Ã£o kernel e do parÃ¢metro de suavizaÃ§Ã£o (bandwidth) Ã© crucial para o desempenho da estimativa.

> ğŸ’¡ **Exemplo NumÃ©rico:** Se $\kappa_t(\tau)$ Ã© uma funÃ§Ã£o que vale 1 quando $|u_t - u_\tau| < 0.003$ e 0 caso contrÃ¡rio, entÃ£o apenas os retornos passados que estÃ£o dentro de 0.3% do retorno atual terÃ£o algum peso na estimativa da variÃ¢ncia condicional.

Uma especificaÃ§Ã£o popular para o peso $w_t(\tau)$ Ã© usar um **kernel Gaussiano**:

$$
\kappa_t(\tau) = \prod_{j=1}^{m} (2\pi \lambda_j)^{-1/2} \exp\left[ -\frac{(u_{t-j} - u_{\tau-j})^2}{2\lambda_j} \right]
$$

onde $\lambda_j$ Ã© o parÃ¢metro de suavizaÃ§Ã£o (bandwidth) para o $j$-Ã©simo *lag* [^1]. O parÃ¢metro $\lambda_j$ controla a largura da janela do kernel, determinando o quÃ£o "prÃ³ximos" os valores passados devem estar para ter uma influÃªncia significativa na estimativa de $h_t$ [^1]. Um valor menor de $\lambda_j$ implica que apenas os valores passados muito prÃ³ximos aos valores atuais terÃ£o um peso substancial, enquanto um valor maior de $\lambda_j$ permite que valores passados mais distantes influenciem a estimativa.

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que $m = 1$ e $\lambda_1 = 0.001$. Se $u_t = 0.01$ e $u_{\tau} = 0.011$, entÃ£o $\kappa_t(\tau) = (2\pi \cdot 0.001)^{-1/2} \exp\left[ -\frac{(0.01 - 0.011)^2}{2 \cdot 0.001} \right] \approx 19.947 \cdot \exp(-0.005) \approx 19.847$. Se $u_{\tau} = 0.02$, entÃ£o $\kappa_t(\tau) \approx 19.947 \cdot \exp(-5) \approx 0.134$. Note como o kernel Gaussiano penaliza rapidamente os valores mais distantes.

A **seleÃ§Ã£o do parÃ¢metro de suavizaÃ§Ã£o** $\lambda_j$ Ã© um desafio fundamental na estimativa de kernel. Uma abordagem comum Ã© a **validaÃ§Ã£o cruzada** (*cross-validation*), que envolve a escolha do valor de $\lambda_j$ que minimiza o erro quadrÃ¡tico mÃ©dio da previsÃ£o da variÃ¢ncia condicional [^1]. Especificamente, se o mesmo *bandwidth* Ã© selecionado para todos os *lags* ($\lambda_j = \lambda$ para todo $j = 1, 2, ..., m$), a estimativa nÃ£o paramÃ©trica de $h_t$ pode ser denotada como $h_t(\lambda)$, e $\lambda$ pode ser escolhido para minimizar [^1]:

$$
\sum_{t=1}^{T} [u_t^2 - h_t(\lambda)]^2
$$

> ğŸ’¡ **Exemplo NumÃ©rico:** Imagine que vocÃª tem uma sÃ©rie temporal de 1000 retornos diÃ¡rios. VocÃª tenta diferentes valores de $\lambda$ (e.g., 0.0001, 0.001, 0.01). Para cada $\lambda$, vocÃª calcula $h_t(\lambda)$ e, em seguida, calcula o erro quadrÃ¡tico mÃ©dio. O $\lambda$ que resulta no menor erro quadrÃ¡tico mÃ©dio Ã© o escolhido.

AlÃ©m da validaÃ§Ã£o cruzada, outras tÃ©cnicas de seleÃ§Ã£o de *bandwidth* incluem mÃ©todos baseados em regras de bolso (*rule-of-thumb*), que fornecem estimativas iniciais para $\lambda$ com base em caracterÃ­sticas dos dados, e mÃ©todos plug-in, que estimam os parÃ¢metros necessÃ¡rios para calcular o *bandwidth* ideal diretamente dos dados. A escolha do mÃ©todo de seleÃ§Ã£o de *bandwidth* pode ter um impacto significativo no desempenho da estimativa de kernel.

**Teorema 1** [ConsistÃªncia da Estimativa de Kernel]
Sob condiÃ§Ãµes de regularidade apropriadas sobre a funÃ§Ã£o kernel, o processo estocÃ¡stico $u_t$, e a taxa de decaimento do *bandwidth* $\lambda$, a estimativa de kernel $\hat{h}_t$ converge em probabilidade para a verdadeira variÃ¢ncia condicional $h_t$ quando $T \to \infty$.

*Proof strategy:* A prova envolve demonstrar que o viÃ©s e a variÃ¢ncia da estimativa de kernel convergem para zero quando o tamanho da amostra aumenta. Isso requer impor condiÃ§Ãµes sobre a suavidade da funÃ§Ã£o kernel, o comportamento de dependÃªncia dos dados e a taxa em que o *bandwidth* converge para zero.

**Prova do Teorema 1**

Para provar a consistÃªncia da estimativa de kernel $\hat{h}_t$, devemos mostrar que $\hat{h}_t$ converge em probabilidade para $h_t$ quando $T \rightarrow \infty$, ou seja, para qualquer $\epsilon > 0$:

$$
\lim_{T \to \infty} P(|\hat{h}_t - h_t| > \epsilon) = 0
$$

I. **DefiniÃ§Ã£o da Estimativa e Erro:**

   A estimativa de kernel Ã© definida como $\hat{h}_t = \sum_{\tau=1}^{T} w_t(\tau) u_\tau^2$, onde $w_t(\tau) = \frac{\kappa_t(\tau)}{\sum_{\tau=1}^{T} \kappa_t(\tau)}$. O erro Ã© entÃ£o dado por $\hat{h}_t - h_t$.

II. **DecomposiÃ§Ã£o do Erro:**

   Podemos decompor o erro em dois componentes: o viÃ©s e o erro estocÃ¡stico.  Primeiro, expressamos $h_t$ como a esperanÃ§a condicional de $u_t^2$ dado o passado: $h_t = E[u_t^2 | u_{t-1}, u_{t-2}, \ldots]$. EntÃ£o,

   $$\hat{h}_t - h_t = \left( \sum_{\tau=1}^{T} w_t(\tau) u_\tau^2 - E[u_t^2 | u_{t-1}, u_{t-2}, \ldots] \right)$$

III. **CondiÃ§Ãµes de Regularidade:**

   Assumimos as seguintes condiÃ§Ãµes de regularidade:

   a) A funÃ§Ã£o kernel $\kappa_t(\tau)$ Ã© simÃ©trica e integra para 1, ou seja, $\int \kappa_t(\tau) d\tau = 1$.
   b) O processo estocÃ¡stico $u_t$ Ã© estacionÃ¡rio e fracamente dependente (e.g., $\alpha$-mixing).
   c) O *bandwidth* $\lambda$ satisfaz $\lambda \to 0$ e $T\lambda \to \infty$ quando $T \to \infty$.

IV. **AnÃ¡lise do ViÃ©s:**

   O viÃ©s da estimativa Ã© dado por:
   $$Bias(\hat{h}_t) = E[\hat{h}_t] - h_t = E\left[ \sum_{\tau=1}^{T} w_t(\tau) u_\tau^2 \right] - h_t$$
   Sob as condiÃ§Ãµes de regularidade mencionadas, o viÃ©s converge para zero quando $T \to \infty$.  Isso ocorre porque a funÃ§Ã£o kernel concentra os pesos em torno dos valores passados prÃ³ximos a $u_t$, e, Ã  medida que $\lambda$ diminui, a mÃ©dia ponderada se aproxima da esperanÃ§a condicional local.

V. **AnÃ¡lise da VariÃ¢ncia:**

   A variÃ¢ncia da estimativa Ã© dada por:
   $$Var(\hat{h}_t) = Var\left( \sum_{\tau=1}^{T} w_t(\tau) u_\tau^2 \right)$$
   Sob as condiÃ§Ãµes de regularidade e a condiÃ§Ã£o de dependÃªncia fraca de $u_t$, a variÃ¢ncia converge para zero quando $T \to \infty$. Isso ocorre porque o nÃºmero efetivo de observaÃ§Ãµes utilizadas na estimativa aumenta com $T$, enquanto os pesos $w_t(\tau)$ diminuem.

VI. **ConvergÃªncia em Probabilidade:**

   Usando a desigualdade de Chebyshev:
   $$P(|\hat{h}_t - h_t| > \epsilon) \leq \frac{E[(\hat{h}_t - h_t)^2]}{\epsilon^2} = \frac{Bias(\hat{h}_t)^2 + Var(\hat{h}_t)}{\epsilon^2}$$
   Como $Bias(\hat{h}_t) \to 0$ e $Var(\hat{h}_t) \to 0$ quando $T \to \infty$, temos:
   $$\lim_{T \to \infty} P(|\hat{h}_t - h_t| > \epsilon) = 0$$
   Portanto, $\hat{h}_t$ converge em probabilidade para $h_t$. â– 

### ConsideraÃ§Ãµes e Desafios

Embora as estimativas nÃ£o paramÃ©tricas ofereÃ§am flexibilidade, elas tambÃ©m apresentam desafios. A escolha do *bandwidth* Ã© crucial, pois um *bandwidth* muito pequeno pode resultar em uma estimativa ruidosa, enquanto um *bandwidth* muito grande pode levar a um *over smoothing*, obscurecendo caracterÃ­sticas importantes da variÃ¢ncia condicional [^1]. AlÃ©m disso, a estimativa de kernel pode sofrer da **maldiÃ§Ã£o da dimensionalidade** quando o nÃºmero de *lags* ($m$) Ã© grande, pois a quantidade de dados necessÃ¡ria para obter uma estimativa precisa aumenta exponencialmente com o nÃºmero de variÃ¡veis [^1].

> ğŸ’¡ **Exemplo NumÃ©rico:** Se vocÃª estÃ¡ usando 10 lags ($m=10$) para estimar a variÃ¢ncia condicional, o nÃºmero de combinaÃ§Ãµes possÃ­veis de valores passados de $u_t$ cresce exponencialmente. Isso significa que vocÃª precisa de uma quantidade enorme de dados para cobrir todas as combinaÃ§Ãµes possÃ­veis e obter uma estimativa precisa.

Para mitigar a maldiÃ§Ã£o da dimensionalidade, tÃ©cnicas de reduÃ§Ã£o de dimensionalidade, como a anÃ¡lise de componentes principais (PCA), podem ser aplicadas aos *lags* de $u_t$ antes da estimativa de kernel. AlÃ©m disso, modelos aditivos generalizados (GAMs) oferecem uma alternativa flexÃ­vel para modelar a variÃ¢ncia condicional nÃ£o parametricamente, permitindo que cada *lag* de $u_t$ contribua para a estimativa de $h_t$ de forma aditiva, o que pode reduzir o problema da dimensionalidade.

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que vocÃª tem retornos diÃ¡rios de uma aÃ§Ã£o nos Ãºltimos 5 anos e deseja usar os Ãºltimos 10 dias para prever a variÃ¢ncia condicional de hoje. Usando PCA, vocÃª pode reduzir esses 10 *lags* para 2 ou 3 componentes principais, que capturam a maior parte da variaÃ§Ã£o nos dados. EntÃ£o, vocÃª pode usar esses componentes principais como entradas para a sua estimativa de kernel, reduzindo a dimensionalidade do problema.

**Lema 1** [ConvergÃªncia em MÃ©dia QuadrÃ¡tica]
Se a funÃ§Ã£o kernel $\kappa_t(\tau)$ satisfaz as condiÃ§Ãµes de Lipschitz e os dados $u_t$ sÃ£o estacionÃ¡rios e fracamente dependentes, entÃ£o, com a escolha apropriada do *bandwidth* $\lambda$, a estimativa de kernel converge em mÃ©dia quadrÃ¡tica para a verdadeira variÃ¢ncia condicional $h_t$.

*Proof strategy:* A prova se baseia na demonstraÃ§Ã£o de que o erro quadrÃ¡tico mÃ©dio da estimativa de kernel, $E[(\hat{h}_t - h_t)^2]$, converge para zero quando $T \to \infty$. Isso requer impor condiÃ§Ãµes sobre a suavidade da funÃ§Ã£o kernel, o comportamento de dependÃªncia dos dados e a taxa em que o *bandwidth* converge para zero.

**Prova do Lema 1**

Para provar a convergÃªncia em mÃ©dia quadrÃ¡tica, devemos mostrar que:

$$
\lim_{T \to \infty} E[(\hat{h}_t - h_t)^2] = 0
$$

I. **DefiniÃ§Ã£o do Erro QuadrÃ¡tico MÃ©dio (MSE):**

   O erro quadrÃ¡tico mÃ©dio Ã© definido como $MSE = E[(\hat{h}_t - h_t)^2]$. Ele pode ser decomposto em viÃ©s ao quadrado e variÃ¢ncia:

   $$
   MSE = E[(\hat{h}_t - h_t)^2] = [Bias(\hat{h}_t)]^2 + Var(\hat{h}_t)
   $$
   onde $Bias(\hat{h}_t) = E[\hat{h}_t] - h_t$ e $Var(\hat{h}_t) = E[(\hat{h}_t - E[\hat{h}_t])^2]$.

II. **CondiÃ§Ãµes de Lipschitz:**

   Assumimos que a funÃ§Ã£o kernel $\kappa_t(\tau)$ satisfaz a condiÃ§Ã£o de Lipschitz, ou seja, existe uma constante $L > 0$ tal que:

   $$
   |\kappa_t(\tau_1) - \kappa_t(\tau_2)| \leq L |\tau_1 - \tau_2|
   $$

   Isso garante que a funÃ§Ã£o kernel seja suficientemente suave.

III. **Dados EstacionÃ¡rios e Fracamente Dependentes:**

   Assumimos que os dados $u_t$ sÃ£o estacionÃ¡rios e fracamente dependentes (e.g., $\alpha$-mixing). Isso implica que as autocovariÃ¢ncias decaem rapidamente com o aumento da distÃ¢ncia no tempo.

IV. **AnÃ¡lise do ViÃ©s:**

   Conforme mostrado na prova do Teorema 1, sob as condiÃ§Ãµes de regularidade e a escolha apropriada do *bandwidth* $\lambda$, o viÃ©s converge para zero quando $T \to \infty$:

   $$
   \lim_{T \to \infty} Bias(\hat{h}_t) = 0
   $$

V. **AnÃ¡lise da VariÃ¢ncia:**

   A variÃ¢ncia da estimativa de kernel pode ser expressa como:

   $$
   Var(\hat{h}_t) = Var\left(\sum_{\tau=1}^{T} w_t(\tau) u_\tau^2\right) = \sum_{\tau_1=1}^{T} \sum_{\tau_2=1}^{T} w_t(\tau_1) w_t(\tau_2) Cov(u_{\tau_1}^2, u_{\tau_2}^2)
   $$

   Devido Ã  estacionariedade e Ã  dependÃªncia fraca dos dados, as autocovariÃ¢ncias $Cov(u_{\tau_1}^2, u_{\tau_2}^2)$ decaem rapidamente quando $|\tau_1 - \tau_2|$ aumenta. AlÃ©m disso, os pesos $w_t(\tau)$ sÃ£o determinados pela funÃ§Ã£o kernel e pelo *bandwidth* $\lambda$.  Com a escolha apropriada de $\lambda$ tal que $\lambda \to 0$ e $T\lambda \to \infty$ quando $T \to \infty$, a variÃ¢ncia tambÃ©m converge para zero quando $T \to \infty$:

   $$
   \lim_{T \to \infty} Var(\hat{h}_t) = 0
   $$

VI. **ConvergÃªncia em MÃ©dia QuadrÃ¡tica:**

   Como o viÃ©s ao quadrado e a variÃ¢ncia convergem para zero quando $T \to \infty$, o erro quadrÃ¡tico mÃ©dio tambÃ©m converge para zero:

   $$
   \lim_{T \to \infty} E[(\hat{h}_t - h_t)^2] = \lim_{T \to \infty} \left([Bias(\hat{h}_t)]^2 + Var(\hat{h}_t)\right) = 0
   $$

   Portanto, a estimativa de kernel $\hat{h}_t$ converge em mÃ©dia quadrÃ¡tica para a verdadeira variÃ¢ncia condicional $h_t$. â– 

### ConclusÃ£o

As estimativas nÃ£o paramÃ©tricas, como a estimativa de kernel, oferecem uma alternativa flexÃ­vel aos modelos paramÃ©tricos tradicionais para modelar a heteroskedasticidade condicional. Ao permitir que os dados determinem a forma da relaÃ§Ã£o entre os valores passados e a variÃ¢ncia condicional atual, esses mÃ©todos podem capturar padrÃµes complexos que os modelos paramÃ©tricos podem nÃ£o conseguir identificar. No entanto, a escolha do *bandwidth* e a maldiÃ§Ã£o da dimensionalidade sÃ£o desafios importantes que devem ser considerados ao aplicar essas tÃ©cnicas. As estimativas nÃ£o paramÃ©tricas podem ser combinadas com abordagens paramÃ©tricas em modelos semiparamÃ©tricos, o que pode proporcionar um equilÃ­brio entre flexibilidade e interpretabilidade.

**ProposiÃ§Ã£o 1** [Intervalos de ConfianÃ§a AssintÃ³ticos]
Sob certas condiÃ§Ãµes de regularidade, um intervalo de confianÃ§a assintÃ³tico para a variÃ¢ncia condicional $h_t$ pode ser construÃ­do usando a distribuiÃ§Ã£o normal assintÃ³tica da estimativa de kernel $\hat{h}_t$.

*Proof strategy:* A prova envolve derivar a distribuiÃ§Ã£o assintÃ³tica da estimativa de kernel usando o teorema do limite central. Isso requer impor condiÃ§Ãµes sobre a suavidade da funÃ§Ã£o kernel, o comportamento de dependÃªncia dos dados e a taxa em que o *bandwidth* converge para zero. O intervalo de confianÃ§a Ã© entÃ£o construÃ­do usando a estimativa da variÃ¢ncia assintÃ³tica.

**Prova da ProposiÃ§Ã£o 1**

Para construir um intervalo de confianÃ§a assintÃ³tico para $h_t$, precisamos estabelecer a distribuiÃ§Ã£o assintÃ³tica de $\hat{h}_t$.

I. **DefiniÃ§Ã£o da Estimativa e CondiÃ§Ãµes:**

   Seja $\hat{h}_t = \sum_{\tau=1}^{T} w_t(\tau) u_\tau^2$ a estimativa de kernel da variÃ¢ncia condicional $h_t$. Assumimos as seguintes condiÃ§Ãµes de regularidade:

   a) A funÃ§Ã£o kernel $\kappa_t(\tau)$ Ã© simÃ©trica e de ordem superior (i.e., $\int u\kappa(u)du = 0$).
   b) O processo $u_t^2$ Ã© estacionÃ¡rio, $\alpha$-mixing com coeficientes de mixing que decaem suficientemente rÃ¡pido.
   c) O *bandwidth* $\lambda$ satisfaz $\lambda \to 0$, $T\lambda \to \infty$, e $T\lambda^4 \to 0$ quando $T \to \infty$.

II. **Teorema do Limite Central (TLC):**

   Sob as condiÃ§Ãµes acima, podemos aplicar um Teorema do Limite Central para processos fracamente dependentes.  Especificamente, podemos mostrar que:
   $$\sqrt{T\lambda} (\hat{h}_t - h_t) \xrightarrow{d} N(0, \sigma^2)$$
   onde $\sigma^2$ Ã© a variÃ¢ncia assintÃ³tica da estimativa de kernel e $\xrightarrow{d}$ denota convergÃªncia em distribuiÃ§Ã£o.

III. **ExpressÃ£o da VariÃ¢ncia AssintÃ³tica:**

   A variÃ¢ncia assintÃ³tica $\sigma^2$ Ã© dada por:
    $$\sigma^2 = \lim_{T \to \infty} T \lambda Var(\hat{h}_t) = \sigma_k^2 E[u_t^4 | u_{t-1}, u_{t-2}, \ldots ] = \sigma_k^2 E[u_t^4 | \mathcal{F}_{t-1}]$$
    Onde $\sigma_k^2 = \int \kappa^2(u) du$ e $\mathcal{F}_{t-1}$ denota a informaÃ§Ã£o disponÃ­vel atÃ© o tempo $t-1$.

IV. **Estimativa da VariÃ¢ncia AssintÃ³tica:**

    Para tornar o resultado do TLC utilizÃ¡vel, precisamos estimar $\sigma^2$. Uma estimativa consistente Ã© dada por:

    $$\hat{\sigma}^2 = \hat{\sigma}_k^2 \frac{1}{T} \sum_{t=1}^{T} u_t^4$$

    Onde $\hat{\sigma}_k^2$ Ã© uma estimativa de $\sigma_k^2$.

V. **Intervalo de ConfianÃ§a AssintÃ³tico:**

   Com a distribuiÃ§Ã£o assintÃ³tica e uma estimativa consistente da variÃ¢ncia, podemos construir um intervalo de confianÃ§a assintÃ³tico para $h_t$ com nÃ­vel de confianÃ§a $1-\alpha$:

   $$P\left( \hat{h}_t - z_{\alpha/2} \frac{\hat{\sigma}}{\sqrt{T\lambda}} \leq h_t \leq \hat{h}_t + z_{\alpha/2} \frac{\hat{\sigma}}{\sqrt{T\lambda}} \right) \approx 1 - \alpha$$

   onde $z_{\alpha/2}$ Ã© o quantil $(1 - \alpha/2)$-Ã©simo da distribuiÃ§Ã£o normal padrÃ£o. Portanto, o intervalo de confianÃ§a assintÃ³tico Ã© dado por:

   $$\left[ \hat{h}_t - z_{\alpha/2} \frac{\hat{\sigma}}{\sqrt{T\lambda}}, \hat{h}_t + z_{\alpha/2} \frac{\hat{\sigma}}{\sqrt{T\lambda}} \right]$$

   Esse intervalo de confianÃ§a fornece uma medida da incerteza associada Ã  estimativa de kernel da variÃ¢ncia condicional. â– 

> ğŸ’¡ **Exemplo NumÃ©rico:** Usando os dados de retornos diÃ¡rios da aÃ§Ã£o, vocÃª estima $\hat{h}_t = 0.00015$ (ou seja, 0.015%). VocÃª tambÃ©m estima $\hat{\sigma}^2 = 0.0000001$. Se vocÃª tem 500 dias de dados e escolheu $\lambda = 0.001$, e quer um intervalo de confianÃ§a de 95% (entÃ£o $z_{\alpha/2} \approx 1.96$), entÃ£o o intervalo de confianÃ§a Ã©:
>
> $$ \left[ 0.00015 - 1.96 \cdot \frac{\sqrt{0.0000001}}{\sqrt{500 \cdot 0.001}}, 0.00015 + 1.96 \cdot \frac{\sqrt{0.0000001}}{\sqrt{500 \cdot 0.001}} \right] $$
>
> $$ \left[ 0.00015 - 1.96 \cdot \frac{0.000316}{0.707}, 0.00015 + 1.96 \cdot \frac{0.000316}{0.707} \right] $$
>
> $$ \left[ 0.00015 - 0.000875, 0.00015 + 0.000875 \right] $$
>
> $$ \left[ -0.000725, 0.001025 \right] $$
>
>  Note que o limite inferior Ã© negativo, o que nÃ£o faz sentido para uma variÃ¢ncia. Isso Ã© comum em prÃ¡tica, especialmente quando a volatilidade Ã© baixa, e geralmente indica que um modelo mais complexo ou um perÃ­odo de tempo maior Ã© necessÃ¡rio.

### ReferÃªncias

[^1]: CapÃ­tulo 21, "Time Series Models of Heteroskedasticity".

<!-- END -->