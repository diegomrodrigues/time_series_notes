## Valida√ß√£o Cruzada na Sele√ß√£o do *Bandwidth* em Estimativas N√£o Param√©tricas

### Introdu√ß√£o

Em continuidade aos cap√≠tulos anteriores sobre estimativas n√£o param√©tricas da heteroskedasticidade condicional e a especifica√ß√£o do kernel Gaussiano, este cap√≠tulo aborda em profundidade o m√©todo de **valida√ß√£o cruzada** (*cross-validation*) para a sele√ß√£o do par√¢metro de *bandwidth* $\lambda$. A escolha apropriada do *bandwidth* √© crucial para o desempenho das estimativas n√£o param√©tricas, equilibrando o *trade-off* entre vi√©s e vari√¢ncia. A valida√ß√£o cruzada oferece uma abordagem baseada em dados para determinar o valor ideal de $\lambda$ que minimiza o erro de previs√£o.

### Valida√ß√£o Cruzada para Sele√ß√£o do *Bandwidth*

Conforme mencionado anteriormente [^1], a escolha do *bandwidth* $\lambda$ √© um desafio fundamental na estimativa de kernel. Um *bandwidth* muito pequeno resulta em uma estimativa ruidosa, enquanto um *bandwidth* muito grande leva a um *over smoothing*, obscurecendo caracter√≠sticas importantes da vari√¢ncia condicional. Para lidar com esse desafio, a **valida√ß√£o cruzada** oferece um m√©todo para selecionar o *bandwidth* ideal, que minimiza o erro de previs√£o da vari√¢ncia condicional [^1].

A valida√ß√£o cruzada envolve a divis√£o dos dados em $k$ subconjuntos ou *folds* de aproximadamente igual tamanho. Para cada valor candidato de $\lambda$, o modelo √© treinado em $k-1$ *folds* e validado no *fold* restante. O processo √© repetido $k$ vezes, de modo que cada *fold* seja usado como conjunto de valida√ß√£o uma vez. O desempenho do modelo √© ent√£o avaliado calculando o erro m√©dio sobre todos os *folds*. O valor de $\lambda$ que minimiza esse erro m√©dio √© selecionado como o *bandwidth* ideal.

Para a estimativa n√£o param√©trica da heteroskedasticidade condicional, o objetivo √© minimizar o erro de previs√£o da vari√¢ncia condicional $h_t$. Portanto, o erro a ser minimizado na valida√ß√£o cruzada √© o **erro quadr√°tico m√©dio** (*mean squared error*, MSE) da previs√£o de $u_t^2$. Especificamente, se denotarmos a estimativa de kernel com *bandwidth* $\lambda$ como $h_t(\lambda)$, o objetivo √© minimizar [^1]:

$$
\sum_{t=1}^{T} [u_t^2 - h_t(\lambda)]^2
$$

na escolha de $\lambda$.

> üí° **Exemplo Num√©rico:** Suponha que temos uma s√©rie temporal de 1000 observa√ß√µes de retornos di√°rios e decidimos usar a valida√ß√£o cruzada com $k = 5$ *folds*. Isso significa que dividimos os dados em 5 subconjuntos de 200 observa√ß√µes cada. Definimos um conjunto de valores candidatos para $\lambda$: $\lambda = [0.0001, 0.0005, 0.001, 0.005, 0.01]$. Para cada valor de $\lambda$, seguimos os seguintes passos:
>
> 1.  **Iterar sobre os *folds*:** Para cada *fold* $i$ de 1 a 5:
>
>     a. **Treinar o modelo:** Usamos as 800 observa√ß√µes dos outros 4 *folds* para treinar o modelo de estimativa de kernel. Isso significa que, para cada $t$ no conjunto de treinamento, calculamos $h_t(\lambda)$ usando os valores passados de $u_\tau^2$ e os pesos derivados do kernel Gaussiano com o *bandwidth* $\lambda$.
>
>     b. **Validar o modelo:** Usamos as 200 observa√ß√µes do *fold* $i$ para validar o modelo. Para cada $t$ no conjunto de valida√ß√£o, calculamos o erro quadr√°tico $[u_t^2 - h_t(\lambda)]^2$.
>
>     c. **Calcular o MSE:** Calculamos o MSE para o *fold* $i$ como a m√©dia dos erros quadr√°ticos:
>     $$
>     MSE_i(\lambda) = \frac{1}{200} \sum_{t \in fold\, i} [u_t^2 - h_t(\lambda)]^2
>     $$
>
> 2.  **Calcular o MSE m√©dio:** Ap√≥s iterar sobre todos os *folds*, calculamos o MSE m√©dio para o valor de $\lambda$ como a m√©dia dos MSEs de cada *fold*:
>
>     $$
>     MSE(\lambda) = \frac{1}{5} \sum_{i=1}^{5} MSE_i(\lambda)
>     $$
>
> 3.  **Selecionar o melhor $\lambda$:** Repetimos os passos 1 e 2 para todos os valores candidatos de $\lambda$. Selecionamos o valor de $\lambda$ que minimiza o MSE m√©dio:
>
>     $$
>     \lambda^* = \arg\min_{\lambda \in [0.0001, 0.0005, 0.001, 0.005, 0.01]} MSE(\lambda)
>     $$
>
> O valor $\lambda^*$ √© ent√£o usado como o *bandwidth* ideal para a estimativa de kernel.

A escolha do n√∫mero de *folds* $k$ √© um aspecto importante da valida√ß√£o cruzada. Valores comuns para $k$ s√£o 5 e 10. Um valor maior de $k$ resulta em uma estimativa mais precisa do erro de generaliza√ß√£o, mas tamb√©m aumenta o custo computacional. Em casos extremos, como a **valida√ß√£o cruzada *leave-one-out*** (*leave-one-out cross-validation*, LOOCV), $k$ √© igual ao tamanho da amostra $T$, de modo que cada observa√ß√£o √© usada como conjunto de valida√ß√£o uma vez. Embora o LOOCV forne√ßa uma estimativa quase n√£o viesada do erro de generaliza√ß√£o, ele pode ter alta vari√¢ncia e ser computacionalmente proibitivo para grandes conjuntos de dados.

> üí° **Exemplo Num√©rico:** Na valida√ß√£o cruzada *leave-one-out*, para cada observa√ß√£o $t$ na s√©rie temporal, removemos essa observa√ß√£o e treinamos o modelo com as $T-1$ observa√ß√µes restantes. Em seguida, usamos o modelo treinado para prever o valor de $u_t^2$ e calculamos o erro quadr√°tico $[u_t^2 - h_t(\lambda)]^2$. Repetimos esse processo para cada observa√ß√£o na s√©rie temporal. O MSE √© ent√£o calculado como a m√©dia dos erros quadr√°ticos:
>
> $$
> MSE(\lambda) = \frac{1}{T} \sum_{t=1}^{T} [u_t^2 - h_t(\lambda)]^2
> $$
>
> Este processo √© repetido para cada valor candidato de $\lambda$, e o valor que minimiza o MSE √© selecionado como o *bandwidth* ideal.
>
> üí° **Exemplo Num√©rico:** Para visualizar o efeito do bandwidth escolhido por valida√ß√£o cruzada, considere um cen√°rio simulado onde a verdadeira vari√¢ncia condicional ($h_t$) segue um padr√£o espec√≠fico ao longo do tempo. Podemos comparar o desempenho de tr√™s valores diferentes de $\lambda$ (um muito pequeno, um ideal selecionado por valida√ß√£o cruzada, e um muito grande).
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Gerar dados simulados
> np.random.seed(42)
> T = 200
> h_true = np.sin(np.linspace(0, 10, T)) + 1.5  # Vari√¢ncia condicional verdadeira (sempre positiva)
> u = np.random.normal(0, np.sqrt(h_true), T)  # Inova√ß√£o
> u_squared = u**2  # u_t^2
>
> # Fun√ß√£o para estimar a vari√¢ncia condicional usando um kernel gaussiano
> def kernel_estimator(u_squared, lambda_val):
>     h_est = np.zeros_like(u_squared)
>     for t in range(T):
>         weights = np.exp(-((np.arange(T) - t)**2) / (2 * lambda_val**2))
>         h_est[t] = np.sum(weights * u_squared) / np.sum(weights)
>     return h_est
>
> # Escolha de valores de lambda
> lambda_small = 1
> lambda_optimal = 5 # Este valor seria encontrado via cross-validation em dados reais
> lambda_large = 20
>
> # Estimar a vari√¢ncia condicional com diferentes lambdas
> h_est_small = kernel_estimator(u_squared, lambda_small)
> h_est_optimal = kernel_estimator(u_squared, lambda_optimal)
> h_est_large = kernel_estimator(u_squared, lambda_large)
>
> # Plotar os resultados
> plt.figure(figsize=(12, 8))
> plt.plot(h_true, label='Vari√¢ncia Condicional Verdadeira', linewidth=2)
> plt.plot(h_est_small, label=f'Estimativa com $\\lambda$ = {lambda_small}', linestyle='--')
> plt.plot(h_est_optimal, label=f'Estimativa com $\\lambda$ = {lambda_optimal} (√ìtimo)', linestyle='--')
> plt.plot(h_est_large, label=f'Estimativa com $\\lambda$ = {lambda_large}', linestyle='--')
> plt.xlabel('Tempo (t)')
> plt.ylabel('Vari√¢ncia Condicional ($h_t$)')
> plt.title('Compara√ß√£o de Estimativas de Vari√¢ncia Condicional com Diferentes $\\lambda$')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> Neste exemplo, $\lambda_{small}$ produz uma estimativa ruidosa que segue de perto as flutua√ß√µes dos dados, refletindo alta vari√¢ncia e baixo vi√©s. $\lambda_{large}$ produz uma estimativa suavizada, com baixo vari√¢ncia, mas com maior vi√©s, perdendo detalhes importantes da vari√¢ncia condicional verdadeira. $\lambda_{optimal}$ (encontrado atrav√©s de valida√ß√£o cruzada em um cen√°rio real) busca um equil√≠brio entre vi√©s e vari√¢ncia, fornecendo uma estimativa mais precisa da vari√¢ncia condicional verdadeira.

Para complementar a discuss√£o sobre as diferentes abordagens de valida√ß√£o cruzada, podemos definir formalmente o MSE utilizado no contexto da *k*-fold cross-validation e, em seguida, derivar algumas de suas propriedades estat√≠sticas.

**Defini√ß√£o 1** [*k*-fold Cross-Validation MSE]

Seja $MSE_{CV}(\lambda, k)$ o erro quadr√°tico m√©dio estimado pela *k*-fold cross-validation para um dado *bandwidth* $\lambda$. Este √© definido como:

$$
MSE_{CV}(\lambda, k) = \frac{1}{k} \sum_{i=1}^{k} MSE_i(\lambda)
$$

onde $MSE_i(\lambda)$ √© o erro quadr√°tico m√©dio no *fold* $i$, calculado como:

$$
MSE_i(\lambda) = \frac{1}{n_i} \sum_{t \in fold\, i} [u_t^2 - h_{(-i)t}(\lambda)]^2
$$

e $h_{(-i)t}(\lambda)$ √© a estimativa da vari√¢ncia condicional no ponto $t$ utilizando o modelo treinado nos dados excluindo o *fold* $i$, e $n_i$ √© o n√∫mero de observa√ß√µes no *fold* $i$.

**Proposi√ß√£o 1** [Decomposi√ß√£o do MSE da Valida√ß√£o Cruzada]
O erro quadr√°tico m√©dio da valida√ß√£o cruzada pode ser decomposto da seguinte forma:
$$
E[MSE_{CV}(\lambda, k)] = Bias^2 + Variance + Noise
$$
onde $Bias^2$ representa o vi√©s quadr√°tico da estimativa, $Variance$ representa a vari√¢ncia da estimativa e $Noise$ representa o erro irredut√≠vel.

*Proof strategy:* A prova envolve expandir a express√£o para $E[MSE_{CV}(\lambda, k)]$ e identificar os termos correspondentes ao vi√©s, vari√¢ncia e ru√≠do.

**Prova da Proposi√ß√£o 1**

Para decompor o MSE da valida√ß√£o cruzada, come√ßamos expandindo $MSE_{CV}(\lambda, k)$:

$$
E[MSE_{CV}(\lambda, k)] = E\left[\frac{1}{k} \sum_{i=1}^{k} \frac{1}{n_i} \sum_{t \in fold\, i} [u_t^2 - h_{(-i)t}(\lambda)]^2\right]
$$

Podemos reescrever essa express√£o em termos do valor verdadeiro da vari√¢ncia condicional $h_t$ e do erro de estimativa:

$$
E[MSE_{CV}(\lambda, k)] = E\left[\frac{1}{k} \sum_{i=1}^{k} \frac{1}{n_i} \sum_{t \in fold\, i} [(u_t^2 - h_t) + (h_t - h_{(-i)t}(\lambda))]^2\right]
$$

Expandindo o quadrado, obtemos:

$$
E[MSE_{CV}(\lambda, k)] = E\left[\frac{1}{k} \sum_{i=1}^{k} \frac{1}{n_i} \sum_{t \in fold\, i} (u_t^2 - h_t)^2 + 2(u_t^2 - h_t)(h_t - h_{(-i)t}(\lambda)) + (h_t - h_{(-i)t}(\lambda))^2\right]
$$

Agora, analisamos cada termo individualmente:

I. **Termo de Ru√≠do (Irredut√≠vel):**

    $$
    E\left[\frac{1}{k} \sum_{i=1}^{k} \frac{1}{n_i} \sum_{t \in fold\, i} (u_t^2 - h_t)^2\right] = E[(u_t^2 - h_t)^2] = Noise
    $$

    Este termo representa a vari√¢ncia do erro, que √© independente do estimador $h_{(-i)t}(\lambda)$ e, portanto, irredut√≠vel.

II. **Termo de Vi√©s:**

    $$
    E\left[\frac{1}{k} \sum_{i=1}^{k} \frac{1}{n_i} \sum_{t \in fold\, i} (h_t - h_{(-i)t}(\lambda))^2\right] \approx \left(E[h_{(-i)t}(\lambda)] - h_t\right)^2 = Bias^2
    $$

    Este termo representa o vi√©s quadr√°tico do estimador.

III. **Termo de Vari√¢ncia e Covari√¢ncia:**

    O termo cruzado $2(u_t^2 - h_t)(h_t - h_{(-i)t}(\lambda))$ pode ser decomposto em termos de vari√¢ncia e covari√¢ncia. Assumindo que o erro $(u_t^2 - h_t)$ tem m√©dia zero e √© independente do estimador $h_{(-i)t}(\lambda)$, este termo converge para zero. No entanto, a vari√¢ncia de $h_{(-i)t}(\lambda)$ contribui para o MSE.

    Portanto, o MSE pode ser expresso como:

    $$
    E[MSE_{CV}(\lambda, k)] = Noise + Bias^2 + Variance
    $$

    Onde $Variance$ representa a vari√¢ncia do estimador $h_{(-i)t}(\lambda)$.

Em conclus√£o, o MSE da valida√ß√£o cruzada √© decomposto em tr√™s componentes principais: o ru√≠do irredut√≠vel, o vi√©s quadr√°tico e a vari√¢ncia. Esta decomposi√ß√£o √© fundamental para entender como a valida√ß√£o cruzada ajuda a selecionar um *bandwidth* que equilibra o *trade-off* entre vi√©s e vari√¢ncia, minimizando assim o erro total de previs√£o. ‚ñ†

**Teorema 1** [Consist√™ncia da Valida√ß√£o Cruzada]
Sob condi√ß√µes de regularidade apropriadas sobre o processo estoc√°stico $u_t$, a fun√ß√£o kernel $\kappa_t(\tau)$ e o conjunto de valores candidatos para o *bandwidth* $\Lambda$, o valor de $\lambda$ selecionado pela valida√ß√£o cruzada converge em probabilidade para o valor de $\lambda$ que minimiza o erro de previs√£o assintoticamente.

*Proof strategy:* A prova envolve mostrar que o erro estimado pela valida√ß√£o cruzada converge para o erro verdadeiro √† medida que o tamanho da amostra aumenta. Isso requer impor condi√ß√µes sobre a suavidade da fun√ß√£o kernel, o comportamento de depend√™ncia dos dados e a densidade do conjunto de valores candidatos para o *bandwidth*.

**Prova do Teorema 1**
Para demonstrar a consist√™ncia da valida√ß√£o cruzada, devemos mostrar que o *bandwidth* $\lambda^*$ selecionado pela valida√ß√£o cruzada converge em probabilidade para o *bandwidth* $\lambda_{opt}$ que minimiza o erro de previs√£o assintoticamente. Ou seja, para qualquer $\epsilon > 0$:

$$
\lim_{T \to \infty} P(|\lambda^* - \lambda_{opt}| > \epsilon) = 0
$$

I. **Defini√ß√£o de $\lambda^*$ e $\lambda_{opt}$:**

   a) Seja $\lambda^* = \arg\min_{\lambda \in \Lambda} MSE_{CV}(\lambda)$ o *bandwidth* selecionado pela valida√ß√£o cruzada, onde $\Lambda$ √© o conjunto de valores candidatos para $\lambda$ e $MSE_{CV}(\lambda)$ √© o erro quadr√°tico m√©dio estimado pela valida√ß√£o cruzada.
   b) Seja $\lambda_{opt} = \arg\min_{\lambda \in \Lambda} MSE(\lambda)$ o *bandwidth* que minimiza o erro quadr√°tico m√©dio assintoticamente, onde $MSE(\lambda) = \lim_{T \to \infty} E\left[\frac{1}{T} \sum_{t=1}^{T} [u_t^2 - h_t(\lambda)]^2\right]$.

II. **Condi√ß√µes de Regularidade:**

   Assumimos as seguintes condi√ß√µes de regularidade:

   a) O processo estoc√°stico $u_t$ √© estacion√°rio, $\alpha$-mixing e tem momentos finitos de ordem superior.
   b) A fun√ß√£o kernel $\kappa_t(\tau)$ √© sim√©trica, de ordem superior e satisfaz a condi√ß√£o de Lipschitz.
   c) O conjunto de valores candidatos para o *bandwidth* $\Lambda$ √© compacto e denso em torno de $\lambda_{opt}$.

III. **Consist√™ncia do MSE estimado pela Valida√ß√£o Cruzada:**

   Sob as condi√ß√µes acima, o erro quadr√°tico m√©dio estimado pela valida√ß√£o cruzada converge em probabilidade para o erro quadr√°tico m√©dio assint√≥tico:

   $$
   \lim_{T \to \infty} P(|MSE_{CV}(\lambda) - MSE(\lambda)| > \epsilon) = 0
   $$

   Essa converg√™ncia √© garantida pela lei dos grandes n√∫meros aplicada aos dados de valida√ß√£o cruzada.

IV. **Converg√™ncia do *Bandwidth* Selecionado:**

   Dado que $MSE_{CV}(\lambda)$ converge para $MSE(\lambda)$ e que $\Lambda$ √© compacto e denso em torno de $\lambda_{opt}$, podemos aplicar o teorema de converg√™ncia do argmin. Isso implica que o *bandwidth* $\lambda^*$ selecionado pela valida√ß√£o cruzada converge em probabilidade para o *bandwidth* $\lambda_{opt}$ que minimiza o erro quadr√°tico m√©dio assintoticamente:

   $$
   \lim_{T \to \infty} P(|\lambda^* - \lambda_{opt}| > \epsilon) = 0
   $$

V. **Conclus√£o:**

   Portanto, sob as condi√ß√µes de regularidade apropriadas, o valor de $\lambda$ selecionado pela valida√ß√£o cruzada converge em probabilidade para o valor de $\lambda$ que minimiza o erro de previs√£o assintoticamente. ‚ñ†

### Considera√ß√µes Pr√°ticas

Embora a valida√ß√£o cruzada seja um m√©todo poderoso para selecionar o *bandwidth*, existem algumas considera√ß√µes pr√°ticas a serem levadas em conta:

1.  **Custo Computacional:** A valida√ß√£o cruzada pode ser computacionalmente intensiva, especialmente para grandes conjuntos de dados e um grande conjunto de valores candidatos para $\lambda$.
2.  **Escolha do N√∫mero de *Folds*:** A escolha do n√∫mero de *folds* $k$ pode afetar o desempenho da valida√ß√£o cruzada. Valores t√≠picos para $k$ s√£o 5 e 10, mas a escolha ideal pode depender das caracter√≠sticas dos dados.
3.  **Paraleliza√ß√£o:** O processo de valida√ß√£o cruzada pode ser facilmente paralelizado, o que pode reduzir significativamente o tempo de computa√ß√£o.
4. **Regulariza√ß√£o:** A valida√ß√£o cruzada pode ser vista como uma forma de regulariza√ß√£o. Ela ajuda a evitar o *overfitting* ao selecionar um modelo que generaliza bem para dados n√£o vistos.

**Lema 1** [Tradeoff Vi√©s-Vari√¢ncia]
A valida√ß√£o cruzada efetivamente equilibra o *trade-off* entre o vi√©s e a vari√¢ncia na escolha do *bandwidth*.

*Proof strategy:* A prova se baseia na observa√ß√£o de que a valida√ß√£o cruzada minimiza o erro de previs√£o, que √© a soma do vi√©s ao quadrado e da vari√¢ncia.

**Prova do Lema 1**

O objetivo da valida√ß√£o cruzada √© minimizar o Erro Quadr√°tico M√©dio (MSE) de previs√£o, que √© definido como:

$$
MSE = E[(h_t - \hat{h}_t)^2]
$$

O MSE pode ser decomposto em duas componentes: o vi√©s ao quadrado e a vari√¢ncia:

$$
MSE = Bias(\hat{h}_t)^2 + Var(\hat{h}_t)
$$

Onde:
   *  $Bias(\hat{h}_t) = E[\hat{h}_t] - h_t$ √© o vi√©s do estimador.
   *  $Var(\hat{h}_t) = E[(\hat{h}_t - E[\hat{h}_t])^2]$ √© a vari√¢ncia do estimador.

A valida√ß√£o cruzada busca um *bandwidth* $\lambda$ que minimize o MSE estimado por meio da divis√£o dos dados em *folds* de treinamento e valida√ß√£o. Ao minimizar o MSE na valida√ß√£o cruzada, estamos implicitamente equilibrando o *trade-off* entre o vi√©s e a vari√¢ncia. Um *bandwidth* muito pequeno resulta em um modelo com baixo vi√©s, mas alta vari√¢ncia, enquanto um *bandwidth* muito grande resulta em um modelo com alto vi√©s, mas baixa vari√¢ncia. A valida√ß√£o cruzada seleciona um *bandwidth* que proporciona o melhor equil√≠brio entre esses dois efeitos, minimizando o erro de generaliza√ß√£o. Portanto, a valida√ß√£o cruzada efetivamente equilibra o *trade-off* entre o vi√©s e a vari√¢ncia na escolha do *bandwidth*. ‚ñ†

**Lema 1.1** [Converg√™ncia da Valida√ß√£o Cruzada Leave-One-Out]
Sob as mesmas condi√ß√µes de regularidade do Teorema 1, a valida√ß√£o cruzada *leave-one-out* (LOOCV) fornece uma estimativa assintoticamente n√£o viesada do erro de generaliza√ß√£o.

*Proof strategy:* A prova envolve demonstrar que, √† medida que o tamanho da amostra tende ao infinito, o vi√©s da estimativa LOOCV converge para zero.

**Prova do Lema 1.1**

Para demonstrar que a valida√ß√£o cruzada *leave-one-out* (LOOCV) fornece uma estimativa assintoticamente n√£o viesada do erro de generaliza√ß√£o, precisamos mostrar que o vi√©s da estimativa LOOCV converge para zero √† medida que o tamanho da amostra $T$ tende ao infinito.

I. **Defini√ß√£o do MSE da LOOCV:**

   O MSE da LOOCV √© definido como:

   $$
   MSE_{LOOCV}(\lambda) = \frac{1}{T} \sum_{t=1}^{T} [u_t^2 - h_{(-t)}(\lambda)]^2
   $$

   onde $h_{(-t)}(\lambda)$ √© a estimativa da vari√¢ncia condicional no ponto $t$ utilizando o modelo treinado nos dados excluindo a observa√ß√£o $t$.

II. **Vi√©s da Estimativa LOOCV:**

   O vi√©s da estimativa LOOCV √© dado por:

   $$
   Bias_{LOOCV}(\lambda) = E[MSE_{LOOCV}(\lambda)] - MSE(\lambda)
   $$

   onde $MSE(\lambda)$ √© o erro quadr√°tico m√©dio verdadeiro.

III. **An√°lise do Vi√©s:**

   Para analisar o vi√©s, expandimos $E[MSE_{LOOCV}(\lambda)]$:

   $$
   E[MSE_{LOOCV}(\lambda)] = E\left[\frac{1}{T} \sum_{t=1}^{T} [u_t^2 - h_{(-t)}(\lambda)]^2\right]
   $$

   Como cada observa√ß√£o √© removida uma de cada vez, o modelo √© treinado em $T-1$ observa√ß√µes. √Ä medida que $T$ se torna grande, a diferen√ßa entre treinar em $T-1$ observa√ß√µes e treinar em $T$ observa√ß√µes diminui. Portanto, para $T$ grande, $h_{(-t)}(\lambda)$ se aproxima de $h_t(\lambda)$, onde $h_t(\lambda)$ √© a estimativa usando todos os dados.

IV. **Converg√™ncia para Zero:**

   √Ä medida que $T \to \infty$, $h_{(-t)}(\lambda) \to h_t(\lambda)$, e portanto:

   $$
   E[MSE_{LOOCV}(\lambda)] \to E\left[\frac{1}{T} \sum_{t=1}^{T} [u_t^2 - h_{t}(\lambda)]^2\right] = MSE(\lambda)
   $$

   Isso implica que:

   $$
   Bias_{LOOCV}(\lambda) = E[MSE_{LOOCV}(\lambda)] - MSE(\lambda) \to 0
   $$

V. **Conclus√£o:**

   Portanto, sob as condi√ß√µes de regularidade do Teorema 1, a valida√ß√£o cruzada *leave-one-out* (LOOCV) fornece uma estimativa assintoticamente n√£o viesada do erro de generaliza√ß√£o. O vi√©s da estimativa LOOCV converge para zero √† medida que o tamanho da amostra tende ao infinito. ‚ñ†

### Conclus√£o

A valida√ß√£o cruzada √© uma t√©cnica poderosa para a sele√ß√£o do par√¢metro de *bandwidth* $\lambda$ em estimativas n√£o param√©tricas da heteroskedasticidade condicional. Ao minimizar o erro de previs√£o da vari√¢ncia condicional, a valida√ß√£o cruzada ajuda a equilibrar o *trade-off* entre vi√©s e vari√¢ncia, resultando em uma estimativa mais precisa e robusta. Apesar do custo computacional, a valida√ß√£o cruzada √© geralmente prefer√≠vel a outras t√©cnicas de sele√ß√£o de *bandwidth* devido √† sua capacidade de se adaptar √†s caracter√≠sticas espec√≠ficas dos dados.

### Refer√™ncias

[^1]: Cap√≠tulo 21, "Time Series Models of Heteroskedasticity".
<!-- END -->