## Cálculo e Avaliação da Variância Condicional na MLE de Modelos ARCH

### Introdução

Este capítulo detalha o processo de cálculo e avaliação da **variância condicional** ($h_t$) para um valor numérico dado do vetor de parâmetros ($\theta$) no contexto da **Estimação de Máxima Verossimilhança (MLE)** de modelos **ARCH**. Como estabelecido nos capítulos anteriores, a variância condicional desempenha um papel crucial na formulação e maximização da função de log-verossimilhança. Este capítulo visa fornecer uma compreensão detalhada de como essa sequência de variâncias condicionais é computada e como ela é utilizada para avaliar a função de log-verossimilhança, um passo essencial no processo de MLE.

### Cálculo da Sequência de Variâncias Condicionais

Dado um vetor de parâmetros $\theta$, o primeiro passo é calcular a sequência de variâncias condicionais $h_t$ para $t = 1, 2, ..., T$. A forma funcional de $h_t$ depende da especificação do modelo ARCH. Em geral, para um modelo ARCH(m), a variância condicional é expressa como [^21.1.10, 21.1.19]:

$$h_t = \zeta + \alpha_1 u_{t-1}^2 + \alpha_2 u_{t-2}^2 + \ldots + \alpha_m u_{t-m}^2$$

onde $\zeta > 0$ e $\alpha_i \geq 0$ para $i = 1, 2, ..., m$ são os parâmetros do modelo, e $u_t = y_t - x_t'\beta$ são os resíduos da equação de regressão [^21.1.17]. Note que o termo $u_t^2$ é por vezes referido como o *squared residual*, e representa a informação chave do período $t$ que influencia a volatilidade no período $t+1$.

> 💡 **Exemplo Numérico:** Considere um modelo ARCH(2): $h_t = \zeta + \alpha_1 u_{t-1}^2 + \alpha_2 u_{t-2}^2$. Se $\zeta = 0.01$, $\alpha_1 = 0.3$, e $\alpha_2 = 0.2$, e tivermos $u_{t-1}^2 = 0.004$ e $u_{t-2}^2 = 0.009$, então $h_t = 0.01 + 0.3(0.004) + 0.2(0.009) = 0.01 + 0.0012 + 0.0018 = 0.013$. A variância condicional no período *t* é influenciada pelos *squared residuals* dos dois períodos anteriores.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Parâmetros
> zeta = 0.01
> alpha1 = 0.3
> alpha2 = 0.2
> u_t_minus_1_squared = 0.004
> u_t_minus_2_squared = 0.009
>
> # Cálculo da variância condicional
> h_t = zeta + alpha1 * u_t_minus_1_squared + alpha2 * u_t_minus_2_squared
>
> print(f"Variância condicional (h_t): {h_t}")
>
> # Visualização (opcional)
> plt.figure(figsize=(8, 5))
> valores = [zeta, alpha1 * u_t_minus_1_squared, alpha2 * u_t_minus_2_squared]
> labels = ['zeta', 'alpha1 * u_{t-1}^2', 'alpha2 * u_{t-2}^2']
> plt.bar(labels, valores, color=['blue', 'green', 'red'])
> plt.ylabel('Contribuição para h_t')
> plt.title('Decomposição da Variância Condicional ARCH(2)')
> plt.grid(axis='y', linestyle='--')
> plt.show()
> ```

Para iniciar a sequência de variâncias condicionais, é necessário estabelecer valores iniciais para $u_t^2$ para $t = -m+1, ..., 0$. Uma abordagem comum é utilizar a variância amostral dos resíduos como valor inicial [^21.2]. Especificamente, sugere-se definir:

$$h_j = u_j^2 = \hat{\sigma}^2 \quad \text{for } j = -m+1, \ldots, 0$$

onde [^21.2]:
$$\hat{\sigma}^2 = \frac{1}{T} \sum_{t=1}^T (y_t - x_t'\hat{\beta})^2$$
e $\hat{\beta}$ é o estimador de mínimos quadrados ordinários (OLS) para $\beta$ [^21.2].

> 💡 **Exemplo Numérico:** Suponha que temos uma amostra de $T = 100$ observações e estimamos $\beta$ por OLS. Calculamos $\hat{\sigma}^2 = 0.02$. Para um modelo ARCH(1), inicializamos $h_0 = u_0^2 = 0.02$.
>
> ```python
> import numpy as np
>
> # Dados de exemplo (substitua pelos seus dados reais)
> T = 100
> y = np.random.rand(T)  # Dados aleatórios para y
> x = np.random.rand(T, 2)  # Dados aleatórios para x (2 variáveis explicativas)
> beta_hat = np.array([0.5, 0.3])  # Estimativas OLS para beta (apenas para demonstração)
>
> # Calcular os resíduos
> u = y - np.dot(x, beta_hat)
>
> # Calcular a variância amostral dos resíduos
> sigma_hat_squared = np.mean(u**2)
>
> print(f"Variância amostral dos resíduos (sigma_hat_squared): {sigma_hat_squared}")
>
> # Inicializar h_0 para um modelo ARCH(1)
> h_0 = sigma_hat_squared
>
> print(f"Valor inicial para h_0: {h_0}")
> ```

**Observação 1:** A escolha dos valores iniciais pode ter um impacto nas estimativas dos parâmetros, especialmente para amostras pequenas. Alternativas para inicializar a sequência de variâncias incluem utilizar um valor constante (e.g., a variância incondicional estimada) ou utilizar a média dos *squared residuals* das primeiras *m* observações.

**Observação 2:** Em alguns casos, a especificação do modelo pode envolver transformações dos resíduos ou da variância condicional. Por exemplo, em modelos **EGARCH** (Exponential GARCH), o logaritmo da variância condicional é modelado [^21.2]:

$$\log h_t = \zeta + \sum_{j=1}^{\infty} \pi_j \{\theta v_{t-j} + \gamma |v_{t-j}| - E|v_{t-j}| \}$$

onde $v_t = u_t/\sqrt{h_t}$ é um choque padronizado, e $\theta$ e $\gamma$ são parâmetros que capturam efeitos assimétricos. Nesses casos, o cálculo da variância condicional envolve a exponenciação da equação acima.

**Proposição 1:** Para modelos com dependência temporal da variância condicional, a escolha de valores iniciais para a sequência $h_t$ é assintoticamente irrelevante sob condições de estacionariedade e ergodicidade.

*Prova:* Sob condições de estacionariedade e ergodicidade, a influência dos valores iniciais decresce exponencialmente à medida que *t* aumenta. Formalmente, para um modelo ARCH(m) estacionário, a dependência de $h_t$ em relação aos valores iniciais $h_j$ com $j \leq 0$ diminui geometricamente rápido. Portanto, para *T* suficientemente grande, o efeito dos valores iniciais na estimação de $\theta$ torna-se desprezível.

Para formalizar a prova da proposição 1:

I.  Seja $h_t = f(u_{t-1}^2, u_{t-2}^2, ..., u_{t-m}^2; \theta)$ a variância condicional em um modelo ARCH(m), onde $f$ é uma função contínua e diferenciável, e $\theta$ é o vetor de parâmetros.

II. Sob estacionariedade e ergodicidade, o processo $\{u_t^2\}$ é fracamente dependente, o que significa que a correlação entre $u_t^2$ e $u_{t-k}^2$ tende a zero quando $k \rightarrow \infty$.

III. Considere duas sequências de variâncias condicionais, $\{h_t\}$ e $\{h_t^*\}$, iniciadas com valores diferentes, $h_j$ e $h_j^*$, para $j = -m+1, ..., 0$.

IV. Seja $d_t = |h_t - h_t^*|$ a diferença absoluta entre as duas sequências no tempo $t$. Queremos mostrar que $d_t \rightarrow 0$ quando $t \rightarrow \infty$.

V. Pela expansão em série de Taylor de primeira ordem de $f$ em torno de $h_t^*$, temos:
    $$h_t \approx h_t^* + \sum_{i=1}^m \frac{\partial f}{\partial u_{t-i}^2} (u_{t-i}^2 - u_{t-i}^{2*})$$
    onde $u_{t-i}^{2*}$ são os resíduos quadrados correspondentes à sequência $\{h_t^*\}$.

VI. Tomando o valor absoluto,
    $$d_t = |h_t - h_t^*| \approx \left| \sum_{i=1}^m \frac{\partial f}{\partial u_{t-i}^2} (u_{t-i}^2 - u_{t-i}^{2*}) \right|$$

VII. Devido à estacionariedade e ergodicidade, os coeficientes $\frac{\partial f}{\partial u_{t-i}^2}$ são limitados e a influência dos valores iniciais diminui exponencialmente. Assim, à medida que $t$ aumenta, a diferença entre $u_{t-i}^2$ e $u_{t-i}^{2*}$ torna-se menor, e a dependência dos valores iniciais desaparece.

VIII. Portanto, $d_t \rightarrow 0$ quando $t \rightarrow \infty$, o que implica que a escolha dos valores iniciais é assintoticamente irrelevante. $\blacksquare$

**Algoritmo para Cálculo da Sequência de Variâncias Condicionais:**

1.  Estimar os parâmetros $\beta$ da equação de regressão por OLS e calcular os resíduos $u_t = y_t - x_t'\hat{\beta}$ para $t = 1, 2, ..., T$.

2.  Calcular a variância amostral dos resíduos, $\hat{\sigma}^2$, e utilizar este valor para inicializar $h_j = u_j^2 = \hat{\sigma}^2$ para $j = -m+1, ..., 0$.

3.  Para $t = 1, 2, ..., T$, calcular a variância condicional $h_t$ utilizando a especificação do modelo ARCH e os valores de $u_{t-1}^2, u_{t-2}^2, ..., u_{t-m}^2$ e os parâmetros $\zeta, \alpha_1, \alpha_2, ..., \alpha_m$ do modelo.

### Avaliação da Função de Log-Verossimilhança

Uma vez que a sequência de variâncias condicionais $h_t$ tenha sido calculada, o próximo passo é avaliar a função de log-verossimilhança [^21.1.20]:

$$L(\theta) = -\frac{T}{2}\log(2\pi) - \frac{1}{2}\sum_{t=1}^{T} \log(h_t) - \frac{1}{2}\sum_{t=1}^{T} \frac{(y_t - x_t'\beta)^2}{h_t}$$

Esta avaliação fornece um valor numérico para a função de log-verossimilhança, dado o vetor de parâmetros $\theta$. O objetivo da MLE é encontrar o vetor de parâmetros $\theta$ que maximize esta função.

> 💡 **Exemplo Numérico:** Suponha que temos $T = 5$ observações e, após calcular a sequência de variâncias condicionais, obtemos os seguintes valores: $h_1 = 0.012$, $h_2 = 0.015$, $h_3 = 0.018$, $h_4 = 0.020$, $h_5 = 0.022$. Suponha também que os *squared residuals* são: $(y_1 - x_1'\beta)^2 = 0.010$, $(y_2 - x_2'\beta)^2 = 0.013$, $(y_3 - x_3'\beta)^2 = 0.016$, $(y_4 - x_4'\beta)^2 = 0.018$, $(y_5 - x_5'\beta)^2 = 0.020$. Então, a função de log-verossimilhança é:
>
> $$L(\theta) = -\frac{5}{2}\log(2\pi) - \frac{1}{2} \left[ \log(0.012) + \log(0.015) + \log(0.018) + \log(0.020) + \log(0.022) \right] - \frac{1}{2} \left[ \frac{0.010}{0.012} + \frac{0.013}{0.015} + \frac{0.016}{0.018} + \frac{0.018}{0.020} + \frac{0.020}{0.022} \right]$$
>
> $$L(\theta) \approx -4.581 - \frac{1}{2} (-19.95) - \frac{1}{2}(4.38) \approx -4.581 + 9.975 - 2.19 = 3.204$$
>
> Este valor representa a verossimilhança dos dados dado o vetor de parâmetros $\theta$.
>
> ```python
> import numpy as np
> from scipy.stats import norm
>
> # Dados de exemplo
> T = 5
> h_t = np.array([0.012, 0.015, 0.018, 0.020, 0.022])
> squared_residuals = np.array([0.010, 0.013, 0.016, 0.018, 0.020])
>
> # Função de log-verossimilhança
> def log_likelihood(h_t, squared_residuals, T):
>     log_likelihood = - (T/2) * np.log(2*np.pi) - (1/2) * np.sum(np.log(h_t)) - (1/2) * np.sum(squared_residuals / h_t)
>     return log_likelihood
>
> # Avaliar a função de log-verossimilhança
> L = log_likelihood(h_t, squared_residuals, T)
>
> print(f"Valor da função de log-verossimilhança: {L}")
> ```

**Teorema 1:** A função de log-verossimilhança $L(\theta)$ para modelos ARCH, sob condições de normalidade condicional, é contínua e diferenciável em $\theta$ se a variância condicional $h_t(\theta)$ for contínua e diferenciável em $\theta$ para todo $t$.

*Prova:* A continuidade e diferenciabilidade de $L(\theta)$ seguem diretamente da continuidade e diferenciabilidade das funções $\log(h_t(\theta))$ e $\frac{(y_t - x_t'\beta)^2}{h_t(\theta)}$ em relação a $\theta$, desde que $h_t(\theta) > 0$ para todo $t$.

I. Assumimos que a variância condicional $h_t(\theta)$ é contínua e diferenciável em $\theta$ para todo $t$.

II. A função de log-verossimilhança é dada por:
    $$L(\theta) = -\frac{T}{2}\log(2\pi) - \frac{1}{2}\sum_{t=1}^{T} \log(h_t(\theta)) - \frac{1}{2}\sum_{t=1}^{T} \frac{(y_t - x_t'\beta)^2}{h_t(\theta)}$$

III. Cada termo na soma, $\log(h_t(\theta))$ e $\frac{(y_t - x_t'\beta)^2}{h_t(\theta)}$, é uma função contínua e diferenciável de $\theta$ se $h_t(\theta)$ for contínua, diferenciável e estritamente positiva para todo $t$. A função $\log(x)$ é contínua e diferenciável para $x>0$, e a função $\frac{1}{x}$ é contínua e diferenciável para $x \neq 0$.

IV. A soma de funções contínuas e diferenciáveis é também contínua e diferenciável. Portanto, $\sum_{t=1}^{T} \log(h_t(\theta))$ e $\sum_{t=1}^{T} \frac{(y_t - x_t'\beta)^2}{h_t(\theta)}$ são contínuas e diferenciáveis em $\theta$.

V. Consequentemente, $L(\theta)$ é contínua e diferenciável em $\theta$. $\blacksquare$

### Otimização Numérica

O processo de MLE envolve a repetição dos passos descritos acima para diferentes valores de $\theta$, utilizando um algoritmo de otimização numérica para encontrar o valor de $\theta$ que maximize $L(\theta)$ [^21.1.20]. Algoritmos comuns incluem o método de Newton-Raphson, o algoritmo BFGS e o método de *scoring* [^21.1.21].

> 💡 **Exemplo Numérico:** Para ilustrar a otimização numérica, vamos usar o algoritmo BFGS para encontrar os parâmetros $\zeta$ e $\alpha_1$ de um modelo ARCH(1).
>
> Suponha que temos os seguintes dados simulados:
> ```python
> import numpy as np
> from scipy.optimize import minimize
>
> # Seed para reproducibilidade
> np.random.seed(42)
>
> # Dados simulados
> T = 200
> zeta_true = 0.01
> alpha_true = 0.3
>
> # Gerar dados ARCH(1)
> u = np.zeros(T)
> h = np.zeros(T)
> h[0] = zeta_true / (1 - alpha_true)  # Variância incondicional
> u[0] = np.sqrt(h[0]) * np.random.normal(0, 1)
>
> for t in range(1, T):
>     h[t] = zeta_true + alpha_true * u[t-1]**2
>     u[t] = np.sqrt(h[t]) * np.random.normal(0, 1)
>
> # Função de log-verossimilhança (negativa para minimizar)
> def neg_log_likelihood(params, u):
>     zeta, alpha = params
>     T = len(u)
>     h = np.zeros(T)
>     h[0] = np.var(u)  # Inicialização
>     for t in range(1, T):
>         h[t] = zeta + alpha * u[t-1]**2
>     if np.any(h <= 0): # Garantir que a variância seja positiva
>         return np.inf
>     log_likelihood = - (T/2) * np.log(2*np.pi) - (1/2) * np.sum(np.log(h)) - (1/2) * np.sum(u**2 / h)
>     return -log_likelihood
>
> # Valores iniciais para os parâmetros
> initial_params = np.array([0.005, 0.2])
>
> # Restrições para garantir zeta > 0 e 0 < alpha < 1 (estacionariedade)
> bounds = ((1e-6, None), (1e-6, 0.999)) # Pequeno valor para garantir positividade
>
> # Otimização usando BFGS
> result = minimize(neg_log_likelihood, initial_params, args=(u,), method='L-BFGS-B', bounds=bounds)
>
> # Extrair resultados
> zeta_hat, alpha_hat = result.x
>
> print("Resultados da otimização:")
> print(f"Estimativa de zeta: {zeta_hat}")
> print(f"Estimativa de alpha: {alpha_hat}")
> print(f"Valor da função de log-verossimilhança no ponto ótimo: {-result.fun}")
>
> # Análise dos resíduos (opcional)
> residuos_padronizados = u / np.sqrt(h) # h calculado com os parâmetros estimados
>
> plt.figure(figsize=(10, 6))
> plt.plot(residuos_padronizados, label='Resíduos Padronizados')
> plt.title('Resíduos Padronizados do Modelo ARCH(1)')
> plt.xlabel('Tempo')
> plt.ylabel('Valor')
> plt.grid(True)
> plt.legend()
> plt.show()
>
> plt.figure(figsize=(10, 6))
> plt.hist(residuos_padronizados, bins=30, density=True, alpha=0.6, color='skyblue', label='Resíduos Padronizados')
>
> # Adicionar a curva normal padrão para comparação
> x = np.linspace(-4, 4, 100)
> plt.plot(x, norm.pdf(x, 0, 1), 'r-', label='Normal Padrão', linewidth=2)
>
> plt.title('Histograma dos Resíduos Padronizados vs Normal Padrão')
> plt.xlabel('Valor')
> plt.ylabel('Densidade')
> plt.grid(True)
> plt.legend()
> plt.show()
> ```

**Considerações Importantes na Otimização:**

*   **Restrições:** É importante impor as restrições de positividade e estacionariedade durante o processo de otimização. Isso pode ser feito utilizando transformações dos parâmetros (e.g., exponenciando os parâmetros para garantir positividade) ou utilizando algoritmos de otimização que permitam restrições.
*   **Convergência:** É fundamental monitorar a convergência do algoritmo e garantir que o gradiente da função de log-verossimilhança se aproxime de zero no ponto de convergência. A não convergência pode indicar problemas com a especificação do modelo ou com a implementação do algoritmo de otimização.
*   **Múltiplos Máximos:** A função de log-verossimilhança pode ter múltiplos máximos locais. É importante utilizar diferentes valores iniciais para o vetor de parâmetros $\theta$ para verificar se o algoritmo converge para o mesmo máximo global.

**Lema 1:** Para modelos ARCH(m) com restrições de não-negatividade nos parâmetros (i.e., $\zeta > 0$ e $\alpha_i \geq 0$), uma transformação logarítmica dos parâmetros pode ser utilizada para garantir que as restrições sejam satisfeitas durante a otimização.

*Prova:* Defina $\zeta = \exp(\zeta')$ e $\alpha_i = \exp(\alpha_i')$. Ao otimizar em relação a $\zeta'$ e $\alpha_i'$, as restrições de não-negatividade são automaticamente satisfeitas, pois a função exponencial sempre produz valores positivos. Após a otimização, os parâmetros originais podem ser obtidos tomando a exponencial dos parâmetros transformados.

Para formalizar a prova do Lema 1:

I. Queremos provar que a transformação $\zeta = \exp(\zeta')$ e $\alpha_i = \exp(\alpha_i')$ garante que $\zeta > 0$ e $\alpha_i \geq 0$ para todo $i$.

II. A função exponencial, $\exp(x)$, é definida como $e^x$, onde $e$ é a constante de Euler (aproximadamente 2.71828).

III. Uma propriedade fundamental da função exponencial é que $e^x > 0$ para todo $x \in \mathbb{R}$. Isto significa que, independentemente do valor de $x$, a função exponencial sempre retorna um valor positivo.

IV. Portanto, $\zeta = \exp(\zeta') > 0$ para qualquer valor de $\zeta' \in \mathbb{R}$, e $\alpha_i = \exp(\alpha_i') > 0$ para qualquer valor de $\alpha_i' \in \mathbb{R}$.

V. Concluímos que a transformação logarítmica garante as restrições de não-negatividade nos parâmetros $\zeta$ e $\alpha_i$. $\blacksquare$

### Conclusão

O cálculo e avaliação da variância condicional são etapas cruciais no processo de MLE para modelos ARCH. A precisão e eficiência dessas etapas têm um impacto direto na qualidade das estimativas dos parâmetros e na validade da inferência estatística. Ao entender os detalhes do cálculo da variância condicional e da avaliação da função de log-verossimilhança, os pesquisadores e profissionais podem aplicar modelos ARCH com maior confiança e obter *insights* mais precisos sobre a dinâmica da volatilidade em séries temporais.

### Referências

[^21.1.10]: If $h_t$ evolves according to $h_t = \zeta + \alpha_1 u_{t-1}^2 + \alpha_2 u_{t-2}^2 + \ldots + \alpha_m u_{t-m}^2$
[^21.1.17]: $y_t = x_t\beta + u_t$.
[^21.1.19]: $h_t = \zeta + \alpha_1(y_{t-1} - x_{t-1}\beta)^2 + \alpha_2(y_{t-2} - x_{t-2}\beta)^2 + \ldots + \alpha_m(y_{t-m}\beta)^2 = [z(\beta)]'\delta$
[^21.1.20]: $L(\theta) = \sum_{t=1}^{T} \log f(y_t|x_t, Y_{t-1}; \theta) = -\frac{T}{2}\log(2\pi) - \frac{1}{2}\sum_{t=1}^{T} \log(h_t) - \frac{1}{2}\sum_{t=1}^{T} \frac{(y_t - x_t\beta)^2}{h_t}$
[^21.2]: See Baillie and Bollerslev (1992) for further discussion of forecasts and mean squared errors for GARCH processes.
[^21.2]: Calculation of the sequence of conditional variances {hₜ}ₜ=₁ᵀ from [21.2.3] requires presample values for h₋ₚ₊₁, . . . , h₀ and u²₋ₚ₊₁, . . . , u₀². If we have observations on yₜ and xₜ for t = 1, 2, . . . , T, Bollerslev (1986, p. 316) suggested setting hⱼ = uⱼ² = σ² for j = −p + 1, . . . , 0, where σ² = T⁻¹ Σₜ=₁ᵀ (yₜ − xₜ′β)².
<!-- END -->