## C√°lculo e Avalia√ß√£o da Vari√¢ncia Condicional na MLE de Modelos ARCH

### Introdu√ß√£o

Este cap√≠tulo detalha o processo de c√°lculo e avalia√ß√£o da **vari√¢ncia condicional** ($h_t$) para um valor num√©rico dado do vetor de par√¢metros ($\theta$) no contexto da **Estima√ß√£o de M√°xima Verossimilhan√ßa (MLE)** de modelos **ARCH**. Como estabelecido nos cap√≠tulos anteriores, a vari√¢ncia condicional desempenha um papel crucial na formula√ß√£o e maximiza√ß√£o da fun√ß√£o de log-verossimilhan√ßa. Este cap√≠tulo visa fornecer uma compreens√£o detalhada de como essa sequ√™ncia de vari√¢ncias condicionais √© computada e como ela √© utilizada para avaliar a fun√ß√£o de log-verossimilhan√ßa, um passo essencial no processo de MLE.

### C√°lculo da Sequ√™ncia de Vari√¢ncias Condicionais

Dado um vetor de par√¢metros $\theta$, o primeiro passo √© calcular a sequ√™ncia de vari√¢ncias condicionais $h_t$ para $t = 1, 2, ..., T$. A forma funcional de $h_t$ depende da especifica√ß√£o do modelo ARCH. Em geral, para um modelo ARCH(m), a vari√¢ncia condicional √© expressa como [^21.1.10, 21.1.19]:

$$h_t = \zeta + \alpha_1 u_{t-1}^2 + \alpha_2 u_{t-2}^2 + \ldots + \alpha_m u_{t-m}^2$$

onde $\zeta > 0$ e $\alpha_i \geq 0$ para $i = 1, 2, ..., m$ s√£o os par√¢metros do modelo, e $u_t = y_t - x_t'\beta$ s√£o os res√≠duos da equa√ß√£o de regress√£o [^21.1.17]. Note que o termo $u_t^2$ √© por vezes referido como o *squared residual*, e representa a informa√ß√£o chave do per√≠odo $t$ que influencia a volatilidade no per√≠odo $t+1$.

> üí° **Exemplo Num√©rico:** Considere um modelo ARCH(2): $h_t = \zeta + \alpha_1 u_{t-1}^2 + \alpha_2 u_{t-2}^2$. Se $\zeta = 0.01$, $\alpha_1 = 0.3$, e $\alpha_2 = 0.2$, e tivermos $u_{t-1}^2 = 0.004$ e $u_{t-2}^2 = 0.009$, ent√£o $h_t = 0.01 + 0.3(0.004) + 0.2(0.009) = 0.01 + 0.0012 + 0.0018 = 0.013$. A vari√¢ncia condicional no per√≠odo *t* √© influenciada pelos *squared residuals* dos dois per√≠odos anteriores.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> zeta = 0.01
> alpha1 = 0.3
> alpha2 = 0.2
> u_t_minus_1_squared = 0.004
> u_t_minus_2_squared = 0.009
>
> # C√°lculo da vari√¢ncia condicional
> h_t = zeta + alpha1 * u_t_minus_1_squared + alpha2 * u_t_minus_2_squared
>
> print(f"Vari√¢ncia condicional (h_t): {h_t}")
>
> # Visualiza√ß√£o (opcional)
> plt.figure(figsize=(8, 5))
> valores = [zeta, alpha1 * u_t_minus_1_squared, alpha2 * u_t_minus_2_squared]
> labels = ['zeta', 'alpha1 * u_{t-1}^2', 'alpha2 * u_{t-2}^2']
> plt.bar(labels, valores, color=['blue', 'green', 'red'])
> plt.ylabel('Contribui√ß√£o para h_t')
> plt.title('Decomposi√ß√£o da Vari√¢ncia Condicional ARCH(2)')
> plt.grid(axis='y', linestyle='--')
> plt.show()
> ```

Para iniciar a sequ√™ncia de vari√¢ncias condicionais, √© necess√°rio estabelecer valores iniciais para $u_t^2$ para $t = -m+1, ..., 0$. Uma abordagem comum √© utilizar a vari√¢ncia amostral dos res√≠duos como valor inicial [^21.2]. Especificamente, sugere-se definir:

$$h_j = u_j^2 = \hat{\sigma}^2 \quad \text{for } j = -m+1, \ldots, 0$$

onde [^21.2]:
$$\hat{\sigma}^2 = \frac{1}{T} \sum_{t=1}^T (y_t - x_t'\hat{\beta})^2$$
e $\hat{\beta}$ √© o estimador de m√≠nimos quadrados ordin√°rios (OLS) para $\beta$ [^21.2].

> üí° **Exemplo Num√©rico:** Suponha que temos uma amostra de $T = 100$ observa√ß√µes e estimamos $\beta$ por OLS. Calculamos $\hat{\sigma}^2 = 0.02$. Para um modelo ARCH(1), inicializamos $h_0 = u_0^2 = 0.02$.
>
> ```python
> import numpy as np
>
> # Dados de exemplo (substitua pelos seus dados reais)
> T = 100
> y = np.random.rand(T)  # Dados aleat√≥rios para y
> x = np.random.rand(T, 2)  # Dados aleat√≥rios para x (2 vari√°veis explicativas)
> beta_hat = np.array([0.5, 0.3])  # Estimativas OLS para beta (apenas para demonstra√ß√£o)
>
> # Calcular os res√≠duos
> u = y - np.dot(x, beta_hat)
>
> # Calcular a vari√¢ncia amostral dos res√≠duos
> sigma_hat_squared = np.mean(u**2)
>
> print(f"Vari√¢ncia amostral dos res√≠duos (sigma_hat_squared): {sigma_hat_squared}")
>
> # Inicializar h_0 para um modelo ARCH(1)
> h_0 = sigma_hat_squared
>
> print(f"Valor inicial para h_0: {h_0}")
> ```

**Observa√ß√£o 1:** A escolha dos valores iniciais pode ter um impacto nas estimativas dos par√¢metros, especialmente para amostras pequenas. Alternativas para inicializar a sequ√™ncia de vari√¢ncias incluem utilizar um valor constante (e.g., a vari√¢ncia incondicional estimada) ou utilizar a m√©dia dos *squared residuals* das primeiras *m* observa√ß√µes.

**Observa√ß√£o 2:** Em alguns casos, a especifica√ß√£o do modelo pode envolver transforma√ß√µes dos res√≠duos ou da vari√¢ncia condicional. Por exemplo, em modelos **EGARCH** (Exponential GARCH), o logaritmo da vari√¢ncia condicional √© modelado [^21.2]:

$$\log h_t = \zeta + \sum_{j=1}^{\infty} \pi_j \{\theta v_{t-j} + \gamma |v_{t-j}| - E|v_{t-j}| \}$$

onde $v_t = u_t/\sqrt{h_t}$ √© um choque padronizado, e $\theta$ e $\gamma$ s√£o par√¢metros que capturam efeitos assim√©tricos. Nesses casos, o c√°lculo da vari√¢ncia condicional envolve a exponencia√ß√£o da equa√ß√£o acima.

**Proposi√ß√£o 1:** Para modelos com depend√™ncia temporal da vari√¢ncia condicional, a escolha de valores iniciais para a sequ√™ncia $h_t$ √© assintoticamente irrelevante sob condi√ß√µes de estacionariedade e ergodicidade.

*Prova:* Sob condi√ß√µes de estacionariedade e ergodicidade, a influ√™ncia dos valores iniciais decresce exponencialmente √† medida que *t* aumenta. Formalmente, para um modelo ARCH(m) estacion√°rio, a depend√™ncia de $h_t$ em rela√ß√£o aos valores iniciais $h_j$ com $j \leq 0$ diminui geometricamente r√°pido. Portanto, para *T* suficientemente grande, o efeito dos valores iniciais na estima√ß√£o de $\theta$ torna-se desprez√≠vel.

Para formalizar a prova da proposi√ß√£o 1:

I.  Seja $h_t = f(u_{t-1}^2, u_{t-2}^2, ..., u_{t-m}^2; \theta)$ a vari√¢ncia condicional em um modelo ARCH(m), onde $f$ √© uma fun√ß√£o cont√≠nua e diferenci√°vel, e $\theta$ √© o vetor de par√¢metros.

II. Sob estacionariedade e ergodicidade, o processo $\{u_t^2\}$ √© fracamente dependente, o que significa que a correla√ß√£o entre $u_t^2$ e $u_{t-k}^2$ tende a zero quando $k \rightarrow \infty$.

III. Considere duas sequ√™ncias de vari√¢ncias condicionais, $\{h_t\}$ e $\{h_t^*\}$, iniciadas com valores diferentes, $h_j$ e $h_j^*$, para $j = -m+1, ..., 0$.

IV. Seja $d_t = |h_t - h_t^*|$ a diferen√ßa absoluta entre as duas sequ√™ncias no tempo $t$. Queremos mostrar que $d_t \rightarrow 0$ quando $t \rightarrow \infty$.

V. Pela expans√£o em s√©rie de Taylor de primeira ordem de $f$ em torno de $h_t^*$, temos:
    $$h_t \approx h_t^* + \sum_{i=1}^m \frac{\partial f}{\partial u_{t-i}^2} (u_{t-i}^2 - u_{t-i}^{2*})$$
    onde $u_{t-i}^{2*}$ s√£o os res√≠duos quadrados correspondentes √† sequ√™ncia $\{h_t^*\}$.

VI. Tomando o valor absoluto,
    $$d_t = |h_t - h_t^*| \approx \left| \sum_{i=1}^m \frac{\partial f}{\partial u_{t-i}^2} (u_{t-i}^2 - u_{t-i}^{2*}) \right|$$

VII. Devido √† estacionariedade e ergodicidade, os coeficientes $\frac{\partial f}{\partial u_{t-i}^2}$ s√£o limitados e a influ√™ncia dos valores iniciais diminui exponencialmente. Assim, √† medida que $t$ aumenta, a diferen√ßa entre $u_{t-i}^2$ e $u_{t-i}^{2*}$ torna-se menor, e a depend√™ncia dos valores iniciais desaparece.

VIII. Portanto, $d_t \rightarrow 0$ quando $t \rightarrow \infty$, o que implica que a escolha dos valores iniciais √© assintoticamente irrelevante. $\blacksquare$

**Algoritmo para C√°lculo da Sequ√™ncia de Vari√¢ncias Condicionais:**

1.  Estimar os par√¢metros $\beta$ da equa√ß√£o de regress√£o por OLS e calcular os res√≠duos $u_t = y_t - x_t'\hat{\beta}$ para $t = 1, 2, ..., T$.

2.  Calcular a vari√¢ncia amostral dos res√≠duos, $\hat{\sigma}^2$, e utilizar este valor para inicializar $h_j = u_j^2 = \hat{\sigma}^2$ para $j = -m+1, ..., 0$.

3.  Para $t = 1, 2, ..., T$, calcular a vari√¢ncia condicional $h_t$ utilizando a especifica√ß√£o do modelo ARCH e os valores de $u_{t-1}^2, u_{t-2}^2, ..., u_{t-m}^2$ e os par√¢metros $\zeta, \alpha_1, \alpha_2, ..., \alpha_m$ do modelo.

### Avalia√ß√£o da Fun√ß√£o de Log-Verossimilhan√ßa

Uma vez que a sequ√™ncia de vari√¢ncias condicionais $h_t$ tenha sido calculada, o pr√≥ximo passo √© avaliar a fun√ß√£o de log-verossimilhan√ßa [^21.1.20]:

$$L(\theta) = -\frac{T}{2}\log(2\pi) - \frac{1}{2}\sum_{t=1}^{T} \log(h_t) - \frac{1}{2}\sum_{t=1}^{T} \frac{(y_t - x_t'\beta)^2}{h_t}$$

Esta avalia√ß√£o fornece um valor num√©rico para a fun√ß√£o de log-verossimilhan√ßa, dado o vetor de par√¢metros $\theta$. O objetivo da MLE √© encontrar o vetor de par√¢metros $\theta$ que maximize esta fun√ß√£o.

> üí° **Exemplo Num√©rico:** Suponha que temos $T = 5$ observa√ß√µes e, ap√≥s calcular a sequ√™ncia de vari√¢ncias condicionais, obtemos os seguintes valores: $h_1 = 0.012$, $h_2 = 0.015$, $h_3 = 0.018$, $h_4 = 0.020$, $h_5 = 0.022$. Suponha tamb√©m que os *squared residuals* s√£o: $(y_1 - x_1'\beta)^2 = 0.010$, $(y_2 - x_2'\beta)^2 = 0.013$, $(y_3 - x_3'\beta)^2 = 0.016$, $(y_4 - x_4'\beta)^2 = 0.018$, $(y_5 - x_5'\beta)^2 = 0.020$. Ent√£o, a fun√ß√£o de log-verossimilhan√ßa √©:
>
> $$L(\theta) = -\frac{5}{2}\log(2\pi) - \frac{1}{2} \left[ \log(0.012) + \log(0.015) + \log(0.018) + \log(0.020) + \log(0.022) \right] - \frac{1}{2} \left[ \frac{0.010}{0.012} + \frac{0.013}{0.015} + \frac{0.016}{0.018} + \frac{0.018}{0.020} + \frac{0.020}{0.022} \right]$$
>
> $$L(\theta) \approx -4.581 - \frac{1}{2} (-19.95) - \frac{1}{2}(4.38) \approx -4.581 + 9.975 - 2.19 = 3.204$$
>
> Este valor representa a verossimilhan√ßa dos dados dado o vetor de par√¢metros $\theta$.
>
> ```python
> import numpy as np
> from scipy.stats import norm
>
> # Dados de exemplo
> T = 5
> h_t = np.array([0.012, 0.015, 0.018, 0.020, 0.022])
> squared_residuals = np.array([0.010, 0.013, 0.016, 0.018, 0.020])
>
> # Fun√ß√£o de log-verossimilhan√ßa
> def log_likelihood(h_t, squared_residuals, T):
>     log_likelihood = - (T/2) * np.log(2*np.pi) - (1/2) * np.sum(np.log(h_t)) - (1/2) * np.sum(squared_residuals / h_t)
>     return log_likelihood
>
> # Avaliar a fun√ß√£o de log-verossimilhan√ßa
> L = log_likelihood(h_t, squared_residuals, T)
>
> print(f"Valor da fun√ß√£o de log-verossimilhan√ßa: {L}")
> ```

**Teorema 1:** A fun√ß√£o de log-verossimilhan√ßa $L(\theta)$ para modelos ARCH, sob condi√ß√µes de normalidade condicional, √© cont√≠nua e diferenci√°vel em $\theta$ se a vari√¢ncia condicional $h_t(\theta)$ for cont√≠nua e diferenci√°vel em $\theta$ para todo $t$.

*Prova:* A continuidade e diferenciabilidade de $L(\theta)$ seguem diretamente da continuidade e diferenciabilidade das fun√ß√µes $\log(h_t(\theta))$ e $\frac{(y_t - x_t'\beta)^2}{h_t(\theta)}$ em rela√ß√£o a $\theta$, desde que $h_t(\theta) > 0$ para todo $t$.

I. Assumimos que a vari√¢ncia condicional $h_t(\theta)$ √© cont√≠nua e diferenci√°vel em $\theta$ para todo $t$.

II. A fun√ß√£o de log-verossimilhan√ßa √© dada por:
    $$L(\theta) = -\frac{T}{2}\log(2\pi) - \frac{1}{2}\sum_{t=1}^{T} \log(h_t(\theta)) - \frac{1}{2}\sum_{t=1}^{T} \frac{(y_t - x_t'\beta)^2}{h_t(\theta)}$$

III. Cada termo na soma, $\log(h_t(\theta))$ e $\frac{(y_t - x_t'\beta)^2}{h_t(\theta)}$, √© uma fun√ß√£o cont√≠nua e diferenci√°vel de $\theta$ se $h_t(\theta)$ for cont√≠nua, diferenci√°vel e estritamente positiva para todo $t$. A fun√ß√£o $\log(x)$ √© cont√≠nua e diferenci√°vel para $x>0$, e a fun√ß√£o $\frac{1}{x}$ √© cont√≠nua e diferenci√°vel para $x \neq 0$.

IV. A soma de fun√ß√µes cont√≠nuas e diferenci√°veis √© tamb√©m cont√≠nua e diferenci√°vel. Portanto, $\sum_{t=1}^{T} \log(h_t(\theta))$ e $\sum_{t=1}^{T} \frac{(y_t - x_t'\beta)^2}{h_t(\theta)}$ s√£o cont√≠nuas e diferenci√°veis em $\theta$.

V. Consequentemente, $L(\theta)$ √© cont√≠nua e diferenci√°vel em $\theta$. $\blacksquare$

### Otimiza√ß√£o Num√©rica

O processo de MLE envolve a repeti√ß√£o dos passos descritos acima para diferentes valores de $\theta$, utilizando um algoritmo de otimiza√ß√£o num√©rica para encontrar o valor de $\theta$ que maximize $L(\theta)$ [^21.1.20]. Algoritmos comuns incluem o m√©todo de Newton-Raphson, o algoritmo BFGS e o m√©todo de *scoring* [^21.1.21].

> üí° **Exemplo Num√©rico:** Para ilustrar a otimiza√ß√£o num√©rica, vamos usar o algoritmo BFGS para encontrar os par√¢metros $\zeta$ e $\alpha_1$ de um modelo ARCH(1).
>
> Suponha que temos os seguintes dados simulados:
> ```python
> import numpy as np
> from scipy.optimize import minimize
>
> # Seed para reproducibilidade
> np.random.seed(42)
>
> # Dados simulados
> T = 200
> zeta_true = 0.01
> alpha_true = 0.3
>
> # Gerar dados ARCH(1)
> u = np.zeros(T)
> h = np.zeros(T)
> h[0] = zeta_true / (1 - alpha_true)  # Vari√¢ncia incondicional
> u[0] = np.sqrt(h[0]) * np.random.normal(0, 1)
>
> for t in range(1, T):
>     h[t] = zeta_true + alpha_true * u[t-1]**2
>     u[t] = np.sqrt(h[t]) * np.random.normal(0, 1)
>
> # Fun√ß√£o de log-verossimilhan√ßa (negativa para minimizar)
> def neg_log_likelihood(params, u):
>     zeta, alpha = params
>     T = len(u)
>     h = np.zeros(T)
>     h[0] = np.var(u)  # Inicializa√ß√£o
>     for t in range(1, T):
>         h[t] = zeta + alpha * u[t-1]**2
>     if np.any(h <= 0): # Garantir que a vari√¢ncia seja positiva
>         return np.inf
>     log_likelihood = - (T/2) * np.log(2*np.pi) - (1/2) * np.sum(np.log(h)) - (1/2) * np.sum(u**2 / h)
>     return -log_likelihood
>
> # Valores iniciais para os par√¢metros
> initial_params = np.array([0.005, 0.2])
>
> # Restri√ß√µes para garantir zeta > 0 e 0 < alpha < 1 (estacionariedade)
> bounds = ((1e-6, None), (1e-6, 0.999)) # Pequeno valor para garantir positividade
>
> # Otimiza√ß√£o usando BFGS
> result = minimize(neg_log_likelihood, initial_params, args=(u,), method='L-BFGS-B', bounds=bounds)
>
> # Extrair resultados
> zeta_hat, alpha_hat = result.x
>
> print("Resultados da otimiza√ß√£o:")
> print(f"Estimativa de zeta: {zeta_hat}")
> print(f"Estimativa de alpha: {alpha_hat}")
> print(f"Valor da fun√ß√£o de log-verossimilhan√ßa no ponto √≥timo: {-result.fun}")
>
> # An√°lise dos res√≠duos (opcional)
> residuos_padronizados = u / np.sqrt(h) # h calculado com os par√¢metros estimados
>
> plt.figure(figsize=(10, 6))
> plt.plot(residuos_padronizados, label='Res√≠duos Padronizados')
> plt.title('Res√≠duos Padronizados do Modelo ARCH(1)')
> plt.xlabel('Tempo')
> plt.ylabel('Valor')
> plt.grid(True)
> plt.legend()
> plt.show()
>
> plt.figure(figsize=(10, 6))
> plt.hist(residuos_padronizados, bins=30, density=True, alpha=0.6, color='skyblue', label='Res√≠duos Padronizados')
>
> # Adicionar a curva normal padr√£o para compara√ß√£o
> x = np.linspace(-4, 4, 100)
> plt.plot(x, norm.pdf(x, 0, 1), 'r-', label='Normal Padr√£o', linewidth=2)
>
> plt.title('Histograma dos Res√≠duos Padronizados vs Normal Padr√£o')
> plt.xlabel('Valor')
> plt.ylabel('Densidade')
> plt.grid(True)
> plt.legend()
> plt.show()
> ```

**Considera√ß√µes Importantes na Otimiza√ß√£o:**

*   **Restri√ß√µes:** √â importante impor as restri√ß√µes de positividade e estacionariedade durante o processo de otimiza√ß√£o. Isso pode ser feito utilizando transforma√ß√µes dos par√¢metros (e.g., exponenciando os par√¢metros para garantir positividade) ou utilizando algoritmos de otimiza√ß√£o que permitam restri√ß√µes.
*   **Converg√™ncia:** √â fundamental monitorar a converg√™ncia do algoritmo e garantir que o gradiente da fun√ß√£o de log-verossimilhan√ßa se aproxime de zero no ponto de converg√™ncia. A n√£o converg√™ncia pode indicar problemas com a especifica√ß√£o do modelo ou com a implementa√ß√£o do algoritmo de otimiza√ß√£o.
*   **M√∫ltiplos M√°ximos:** A fun√ß√£o de log-verossimilhan√ßa pode ter m√∫ltiplos m√°ximos locais. √â importante utilizar diferentes valores iniciais para o vetor de par√¢metros $\theta$ para verificar se o algoritmo converge para o mesmo m√°ximo global.

**Lema 1:** Para modelos ARCH(m) com restri√ß√µes de n√£o-negatividade nos par√¢metros (i.e., $\zeta > 0$ e $\alpha_i \geq 0$), uma transforma√ß√£o logar√≠tmica dos par√¢metros pode ser utilizada para garantir que as restri√ß√µes sejam satisfeitas durante a otimiza√ß√£o.

*Prova:* Defina $\zeta = \exp(\zeta')$ e $\alpha_i = \exp(\alpha_i')$. Ao otimizar em rela√ß√£o a $\zeta'$ e $\alpha_i'$, as restri√ß√µes de n√£o-negatividade s√£o automaticamente satisfeitas, pois a fun√ß√£o exponencial sempre produz valores positivos. Ap√≥s a otimiza√ß√£o, os par√¢metros originais podem ser obtidos tomando a exponencial dos par√¢metros transformados.

Para formalizar a prova do Lema 1:

I. Queremos provar que a transforma√ß√£o $\zeta = \exp(\zeta')$ e $\alpha_i = \exp(\alpha_i')$ garante que $\zeta > 0$ e $\alpha_i \geq 0$ para todo $i$.

II. A fun√ß√£o exponencial, $\exp(x)$, √© definida como $e^x$, onde $e$ √© a constante de Euler (aproximadamente 2.71828).

III. Uma propriedade fundamental da fun√ß√£o exponencial √© que $e^x > 0$ para todo $x \in \mathbb{R}$. Isto significa que, independentemente do valor de $x$, a fun√ß√£o exponencial sempre retorna um valor positivo.

IV. Portanto, $\zeta = \exp(\zeta') > 0$ para qualquer valor de $\zeta' \in \mathbb{R}$, e $\alpha_i = \exp(\alpha_i') > 0$ para qualquer valor de $\alpha_i' \in \mathbb{R}$.

V. Conclu√≠mos que a transforma√ß√£o logar√≠tmica garante as restri√ß√µes de n√£o-negatividade nos par√¢metros $\zeta$ e $\alpha_i$. $\blacksquare$

### Conclus√£o

O c√°lculo e avalia√ß√£o da vari√¢ncia condicional s√£o etapas cruciais no processo de MLE para modelos ARCH. A precis√£o e efici√™ncia dessas etapas t√™m um impacto direto na qualidade das estimativas dos par√¢metros e na validade da infer√™ncia estat√≠stica. Ao entender os detalhes do c√°lculo da vari√¢ncia condicional e da avalia√ß√£o da fun√ß√£o de log-verossimilhan√ßa, os pesquisadores e profissionais podem aplicar modelos ARCH com maior confian√ßa e obter *insights* mais precisos sobre a din√¢mica da volatilidade em s√©ries temporais.

### Refer√™ncias

[^21.1.10]: If $h_t$ evolves according to $h_t = \zeta + \alpha_1 u_{t-1}^2 + \alpha_2 u_{t-2}^2 + \ldots + \alpha_m u_{t-m}^2$
[^21.1.17]: $y_t = x_t\beta + u_t$.
[^21.1.19]: $h_t = \zeta + \alpha_1(y_{t-1} - x_{t-1}\beta)^2 + \alpha_2(y_{t-2} - x_{t-2}\beta)^2 + \ldots + \alpha_m(y_{t-m}\beta)^2 = [z(\beta)]'\delta$
[^21.1.20]: $L(\theta) = \sum_{t=1}^{T} \log f(y_t|x_t, Y_{t-1}; \theta) = -\frac{T}{2}\log(2\pi) - \frac{1}{2}\sum_{t=1}^{T} \log(h_t) - \frac{1}{2}\sum_{t=1}^{T} \frac{(y_t - x_t\beta)^2}{h_t}$
[^21.2]: See Baillie and Bollerslev (1992) for further discussion of forecasts and mean squared errors for GARCH processes.
[^21.2]: Calculation of the sequence of conditional variances {h‚Çú}‚Çú=‚ÇÅ·µÄ from [21.2.3] requires presample values for h‚Çã‚Çö‚Çä‚ÇÅ, . . . , h‚ÇÄ and u¬≤‚Çã‚Çö‚Çä‚ÇÅ, . . . , u‚ÇÄ¬≤. If we have observations on y‚Çú and x‚Çú for t = 1, 2, . . . , T, Bollerslev (1986, p. 316) suggested setting h‚±º = u‚±º¬≤ = œÉ¬≤ for j = ‚àíp + 1, . . . , 0, where œÉ¬≤ = T‚Åª¬π Œ£‚Çú=‚ÇÅ·µÄ (y‚Çú ‚àí x‚Çú‚Ä≤Œ≤)¬≤.
<!-- END -->