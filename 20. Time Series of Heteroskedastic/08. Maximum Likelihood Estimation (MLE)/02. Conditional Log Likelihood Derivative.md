## An√°lise Detalhada do Score no Modelo ARCH

### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise do *score*, ou a derivada do logaritmo da fun√ß√£o de verossimilhan√ßa condicional com rela√ß√£o ao vetor de par√¢metros, em modelos **ARCH**. O *score* desempenha um papel crucial na estima√ß√£o de m√°xima verossimilhan√ßa (MLE), fornecendo a dire√ß√£o de maior aumento da fun√ß√£o de verossimilhan√ßa. Compreender a estrutura e as propriedades do *score* √© essencial para desenvolver algoritmos de otimiza√ß√£o eficientes e realizar infer√™ncia estat√≠stica v√°lida. Este cap√≠tulo expande o conceito introduzido anteriormente [anterior section], detalhando a sua deriva√ß√£o e interpreta√ß√£o no contexto dos modelos ARCH.

### Deriva√ß√£o e Estrutura Detalhada do Score

Recordando a fun√ß√£o de log-verossimilhan√ßa condicional apresentada anteriormente [previous section]:

$$L(\theta) = \sum_{t=1}^{T} \log f(y_t|x_t, Y_{t-1}; \theta) = -\frac{T}{2}\log(2\pi) - \frac{1}{2}\sum_{t=1}^{T} \log(h_t) - \frac{1}{2}\sum_{t=1}^{T} \frac{(y_t - x_t'\beta)^2}{h_t}$$

onde $f(y_t|x_t, Y_{t-1}; \theta)$ √© a densidade condicional de $y_t$ dado o hist√≥rico $Y_{t-1}$ e o vetor de par√¢metros $\theta$, e $h_t$ representa a vari√¢ncia condicional. O *score* para a *t*-√©sima observa√ß√£o √© definido como a derivada parcial de $\log f(y_t|x_t, Y_{t-1}; \theta)$ com rela√ß√£o a $\theta$, denotado por $s_t(\theta) = \frac{\partial \log f(y_t|x_t, Y_{t-1}; \theta)}{\partial \theta}$ [^21.1.21].

Dado que $u_t = y_t - x_t'\beta$, podemos reescrever o *score* como [^21.1.21]:

$$s_t(\theta) = \frac{\partial \log f(y_t|x_t, Y_{t-1}; \theta)}{\partial \theta} = \frac{\partial}{\partial \theta} \left[ -\frac{1}{2}\log(2\pi) - \frac{1}{2}\log(h_t) - \frac{1}{2} \frac{u_t^2}{h_t} \right]$$

Aplicando a regra da cadeia, obtemos [^21.1.21]:

$$s_t(\theta) = -\frac{1}{2} \frac{1}{h_t} \frac{\partial h_t}{\partial \theta} + \frac{1}{2} \frac{u_t^2}{h_t^2} \frac{\partial h_t}{\partial \theta} + \frac{u_t}{h_t} \frac{\partial u_t}{\partial \theta}$$

Reorganizando os termos, temos:

$$s_t(\theta) = \frac{u_t^2 - h_t}{2h_t^2} \frac{\partial h_t}{\partial \theta} + \frac{u_t}{h_t} \frac{\partial u_t}{\partial \theta}$$

Considerando agora que $u_t = y_t - x_t'\beta$, ent√£o $\frac{\partial u_t}{\partial \beta} = -x_t$. Assumindo que os par√¢metros na vari√¢ncia condicional (e.g., $\zeta, \alpha_i$ em um modelo ARCH) s√£o distintos de $\beta$, a derivada parcial da fun√ß√£o de log-verossimilhan√ßa com respeito a esses par√¢metros de vari√¢ncia ter√° o segundo termo igual a zero, uma vez que $u_t$ n√£o depende diretamente desses par√¢metros. Portanto, podemos decompor o *score* em dois componentes [^21.1.21]:

$$s_t(\theta) = \begin{bmatrix} \frac{u_t^2 - h_t}{2h_t^2} \frac{\partial h_t}{\partial \theta} \\ \frac{u_t}{h_t} (-x_t) \end{bmatrix}$$

O primeiro componente est√° relacionado com a especifica√ß√£o do modelo de vari√¢ncia condicional, e o segundo componente est√° relacionado com a especifica√ß√£o da m√©dia condicional.

> üí° **Exemplo Num√©rico:**
>
> Suponha um modelo simples onde $y_t = \beta x_t + u_t$ e $h_t = \alpha_0 + \alpha_1 u_{t-1}^2$. Queremos calcular o *score* para uma observa√ß√£o espec√≠fica $t$ dados os par√¢metros e dados anteriores. Sejam os valores:
>
> -   $y_t = 1.5$
> -   $x_t = 1.0$
> -   $\beta = 1.0$
> -   $\alpha_0 = 0.1$
> -   $\alpha_1 = 0.5$
> -   $u_{t-1} = 0.5$ (residuo do periodo anterior)
>
> Ent√£o, $u_t = y_t - \beta x_t = 1.5 - 1.0 * 1.0 = 0.5$.  E $h_t = \alpha_0 + \alpha_1 u_{t-1}^2 = 0.1 + 0.5 * (0.5)^2 = 0.1 + 0.125 = 0.225$.
>
> Agora, calculamos as derivadas parciais:
>
> -   $\frac{\partial h_t}{\partial \alpha_0} = 1$
> -   $\frac{\partial h_t}{\partial \alpha_1} = u_{t-1}^2 = 0.25$
> -   $\frac{\partial u_t}{\partial \beta} = -x_t = -1$
>
> O *score* para essa observa√ß√£o √©, portanto:
>
> $$s_t(\theta) = \begin{bmatrix} \frac{u_t^2 - h_t}{2h_t^2} \frac{\partial h_t}{\partial \theta} \\ \frac{u_t}{h_t} (-x_t) \end{bmatrix} = \begin{bmatrix} \frac{(0.5)^2 - 0.225}{2(0.225)^2} \begin{bmatrix} 1 \\ 0.25 \end{bmatrix} \\ \frac{0.5}{0.225} (-1) \end{bmatrix}$$
>
> $$s_t(\theta) = \begin{bmatrix} \frac{0.025}{0.10125} \begin{bmatrix} 1 \\ 0.25 \end{bmatrix} \\ -2.22 \end{bmatrix} = \begin{bmatrix} 0.247 \begin{bmatrix} 1 \\ 0.25 \end{bmatrix} \\ -2.22 \end{bmatrix} = \begin{bmatrix} 0.247 \\ 0.062 \\ -2.22 \end{bmatrix}$$
>
> Este vetor *score* indica a dire√ß√£o de maior aumento na log-verossimilhan√ßa em rela√ß√£o a $\alpha_0$, $\alpha_1$ e $\beta$ para a observa√ß√£o *t*.

Notando que a equa√ß√£o fornecida no contexto (Subtopic) parece ter uma estrutura diferente para o √∫ltimo componente, reescrevemos o *score* separando os par√¢metros:

$$s_t(\theta) = \begin{bmatrix} \frac{(u_t^2 - h_t)}{2h_t} \frac{\partial \log h_t}{\partial \theta} \\ \frac{(x_t u_t)}{h_t} \end{bmatrix}$$

Essa forma √© obtida ao notar que $\frac{\partial h_t}{\partial \theta} = h_t \frac{\partial \log h_t}{\partial \theta}$ e reorganizar os termos. O primeiro bloco do *score* captura a contribui√ß√£o da vari√¢ncia condicional, enquanto o segundo bloco captura a contribui√ß√£o da m√©dia condicional, como expresso em [^21.1.21].

**Lema 1:** Se o modelo for corretamente especificado, o valor esperado condicional do *score* √© zero, i.e., $E[s_t(\theta)|Y_{t-1}] = 0$.

*Proof:*
Tomando a esperan√ßa condicional do *score*, obtemos:

$E[s_t(\theta)|Y_{t-1}] = E\left[\frac{u_t^2 - h_t}{2h_t^2} \frac{\partial h_t}{\partial \theta} + \frac{u_t}{h_t} \frac{\partial u_t}{\partial \theta} | Y_{t-1}\right]$

Sabemos que $E[u_t|Y_{t-1}] = 0$ e $E[u_t^2|Y_{t-1}] = h_t$. Portanto,

$E[s_t(\theta)|Y_{t-1}] = \frac{E[u_t^2|Y_{t-1}] - h_t}{2h_t^2} \frac{\partial h_t}{\partial \theta} + \frac{E[u_t|Y_{t-1}]}{h_t} \frac{\partial u_t}{\partial \theta} = \frac{h_t - h_t}{2h_t^2} \frac{\partial h_t}{\partial \theta} + \frac{0}{h_t} \frac{\partial u_t}{\partial \theta} = 0$

$\blacksquare$

A propriedade $E[s_t(\theta)|Y_{t-1}] = 0$ √© fundamental para garantir a consist√™ncia dos estimadores de m√°xima verossimilhan√ßa.

**Teorema 1:** Sob condi√ß√µes de regularidade, o estimador de m√°xima verossimilhan√ßa $\hat{\theta}$ √© consistente e assintoticamente normal, com distribui√ß√£o dada por:

$$\sqrt{T}(\hat{\theta} - \theta) \xrightarrow{d} N(0, I(\theta)^{-1})$$

onde $I(\theta)$ √© a matriz de informa√ß√£o de Fisher.

*Proof:* (Sketch)
A prova envolve demonstrar que, sob condi√ß√µes de regularidade (como diferenciabilidade da fun√ß√£o de log-verossimilhan√ßa, exist√™ncia de momentos finitos, e identificabilidade dos par√¢metros), as condi√ß√µes para a consist√™ncia e normalidade assint√≥tica dos estimadores de MLE s√£o satisfeitas. A matriz de informa√ß√£o de Fisher √© definida como $I(\theta) = -E\left[\frac{\partial^2 L(\theta)}{\partial \theta \partial \theta'}\right]$, e sob as condi√ß√µes de regularidade, ela tamb√©m pode ser expressa como $I(\theta) = E[s_t(\theta)s_t(\theta)']$. A consist√™ncia segue da lei dos grandes n√∫meros aplicada ao *score*, e a normalidade assint√≥tica segue do teorema do limite central aplicado ao *score*.

$\blacksquare$

**Lema 1.1:** A matriz de informa√ß√£o de Fisher pode ser expressa como a esperan√ßa do produto externo do score: $I(\theta) = E[s_t(\theta)s_t(\theta)']$.

*Proof:*
Come√ßamos com a identidade do score: $E[s_t(\theta)] = 0$. Diferenciando essa identidade em rela√ß√£o a $\theta$, obtemos:
$\frac{\partial}{\partial \theta} E[s_t(\theta)] = E\left[\frac{\partial s_t(\theta)}{\partial \theta}\right] = 0$.

Agora, consideremos a derivada da fun√ß√£o de log-verossimilhan√ßa:
$L(\theta) = \sum_{t=1}^T \log f(y_t|x_t, Y_{t-1}; \theta)$.
A primeira derivada √© o score: $s_t(\theta) = \frac{\partial \log f(y_t|x_t, Y_{t-1}; \theta)}{\partial \theta}$.
A segunda derivada √© a Hessiana: $H(\theta) = \frac{\partial^2 L(\theta)}{\partial \theta \partial \theta'} = \sum_{t=1}^T \frac{\partial^2 \log f(y_t|x_t, Y_{t-1}; \theta)}{\partial \theta \partial \theta'}$.

A matriz de informa√ß√£o de Fisher √© definida como $I(\theta) = -E[H(\theta)] = -E\left[\frac{\partial^2 L(\theta)}{\partial \theta \partial \theta'}\right]$.
Usando a identidade do score, podemos reescrever isso como:
$I(\theta) = E[s_t(\theta)s_t(\theta)']$.
Essa igualdade decorre da diferenciabilidade da fun√ß√£o de log-verossimilhan√ßa e da propriedade do score de ter m√©dia zero sob a verdadeira especifica√ß√£o do modelo.

$\blacksquare$

**Proposi√ß√£o 1:** O teste LM (Lagrange Multiplier) para testar restri√ß√µes sobre os par√¢metros pode ser constru√≠do usando o score. Seja $H_0: r(\theta) = 0$ a hip√≥tese nula, onde $r(\theta)$ √© um vetor de fun√ß√µes diferenci√°veis dos par√¢metros. O estat√≠stico LM √© dado por:

$$LM = s(\hat{\theta}_R)'I(\hat{\theta}_R)^{-1}s(\hat{\theta}_R)$$

onde $\hat{\theta}_R$ √© o estimador MLE sob a restri√ß√£o $H_0$, e $s(\hat{\theta}_R)$ √© o score avaliado em $\hat{\theta}_R$, e $I(\hat{\theta}_R)$ √© a matriz de informa√ß√£o de Fisher avaliada em $\hat{\theta}_R$. Sob $H_0$, $LM$ segue uma distribui√ß√£o $\chi^2$ com graus de liberdade iguais ao n√∫mero de restri√ß√µes impostas.

*Proof:*

I. Considere a fun√ß√£o Lagrangeana:
   $$ \mathcal{L}(\theta, \lambda) = L(\theta) + \lambda'r(\theta) $$
   onde $L(\theta)$ √© a fun√ß√£o de log-verossimilhan√ßa, $r(\theta)$ √© o vetor de restri√ß√µes, e $\lambda$ √© o vetor de multiplicadores de Lagrange.

II. As condi√ß√µes de primeira ordem para otimiza√ß√£o restrita s√£o:
    $$ \frac{\partial \mathcal{L}}{\partial \theta} = \frac{\partial L(\theta)}{\partial \theta} + \frac{\partial r(\theta)'}{\partial \theta}\lambda = 0 $$
    $$ \frac{\partial \mathcal{L}}{\partial \lambda} = r(\theta) = 0 $$

III. Sob a hip√≥tese nula $H_0: r(\theta) = 0$, o estimador restrito $\hat{\theta}_R$ satisfaz $r(\hat{\theta}_R) = 0$. Portanto, o score avaliado em $\hat{\theta}_R$ √©:
     $$ s(\hat{\theta}_R) = -\left[ \frac{\partial r(\hat{\theta}_R)'}{\partial \theta} \right] \lambda $$

IV. Podemos aproximar $s(\hat{\theta}_R)$ usando uma expans√£o de Taylor em torno do estimador irrestrito $\hat{\theta}$:
    $$ s(\hat{\theta}_R) \approx s(\hat{\theta}) + H(\hat{\theta})(\hat{\theta}_R - \hat{\theta}) $$
    onde $H(\hat{\theta})$ √© a matriz Hessiana da fun√ß√£o de log-verossimilhan√ßa avaliada em $\hat{\theta}$.  Como $\hat{\theta}$ √© o estimador irrestrito, $s(\hat{\theta}) \approx 0$, ent√£o:
    $$ s(\hat{\theta}_R) \approx H(\hat{\theta})(\hat{\theta}_R - \hat{\theta}) $$

V.  O estat√≠stico LM pode ser derivado usando a matriz de informa√ß√£o de Fisher $I(\theta) = -E[H(\theta)]$ e as condi√ß√µes de primeira ordem da otimiza√ß√£o restrita.  Sob a hip√≥tese nula, o estat√≠stico LM tem uma distribui√ß√£o assint√≥tica $\chi^2$ com o n√∫mero de graus de liberdade igual ao n√∫mero de restri√ß√µes impostas.

VI. Portanto, o estat√≠stico LM √© dado por:
$$LM = s(\hat{\theta}_R)'I(\hat{\theta}_R)^{-1}s(\hat{\theta}_R)$$
Sob $H_0$, $LM$ segue uma distribui√ß√£o $\chi^2$ com graus de liberdade iguais ao n√∫mero de restri√ß√µes impostas.

$\blacksquare$

### Aplica√ß√µes e Utiliza√ß√£o do Score

O *score* √© amplamente utilizado em:

1.  **Otimiza√ß√£o:** O *score* √© o gradiente da fun√ß√£o de log-verossimilhan√ßa, e √© utilizado em algoritmos de otimiza√ß√£o, como o m√©todo de Newton-Raphson ou m√©todos quasi-Newton (e.g., BFGS), para encontrar o m√°ximo da fun√ß√£o de verossimilhan√ßa.

2.  **Testes de Hip√≥teses:** O *score* pode ser usado para construir testes de hip√≥teses sobre os par√¢metros do modelo. O teste de *score* (ou teste de Lagrange Multiplier) compara o valor do *score* sob a hip√≥tese nula com sua distribui√ß√£o assint√≥tica.

> üí° **Exemplo Num√©rico:**
>
> Suponha que queremos testar a hip√≥tese nula de que $\alpha_1 = 0$ em um modelo ARCH(1) usando o teste LM. Sob a hip√≥tese nula, estimamos o modelo restrito e obtemos os seguintes resultados:
>
> -   $\hat{\alpha}_0 = 0.2$
> -   $\hat{\beta} = 1.1$
>
> Avaliamos o *score* no estimador restrito $\hat{\theta}_R = [\hat{\alpha}_0, \hat{\alpha}_1=0, \hat{\beta}]'$ usando os dados dispon√≠veis. Suponha que o *score* avaliado no estimador restrito seja:
>
> $$s(\hat{\theta}_R) = \begin{bmatrix} 0.1 \\ 0.05 \\ -0.2 \end{bmatrix}$$
>
> Tamb√©m precisamos da matriz de informa√ß√£o de Fisher avaliada no estimador restrito, $I(\hat{\theta}_R)$. Suponha que, ap√≥s o c√°lculo, obtemos:
>
> $$I(\hat{\theta}_R) = \begin{bmatrix} 0.5 & 0.1 & 0 \\ 0.1 & 0.3 & 0 \\ 0 & 0 & 0.4 \end{bmatrix}$$
>
> O estat√≠stico LM √© ent√£o calculado como:
>
> $$LM = s(\hat{\theta}_R)'I(\hat{\theta}_R)^{-1}s(\hat{\theta}_R)$$
>
> Primeiro, calculamos a inversa da matriz de informa√ß√£o de Fisher:
>
> $$I(\hat{\theta}_R)^{-1} = \begin{bmatrix} 2.14 & -0.71 & 0 \\ -0.71 & 3.57 & 0 \\ 0 & 0 & 2.5 \end{bmatrix}$$
>
> Agora, calculamos o estat√≠stico LM:
>
> $$LM = \begin{bmatrix} 0.1 & 0.05 & -0.2 \end{bmatrix} \begin{bmatrix} 2.14 & -0.71 & 0 \\ -0.71 & 3.57 & 0 \\ 0 & 0 & 2.5 \end{bmatrix} \begin{bmatrix} 0.1 \\ 0.05 \\ -0.2 \end{bmatrix}$$
>
> $$LM = \begin{bmatrix} 0.1785 & 0.107 & -0.5 \end{bmatrix} \begin{bmatrix} 0.1 \\ 0.05 \\ -0.2 \end{bmatrix} = 0.01785 + 0.00535 + 0.1 = 0.1232$$
>
> Sob a hip√≥tese nula, este estat√≠stico LM segue uma distribui√ß√£o $\chi^2$ com 1 grau de liberdade (j√° que estamos testando uma restri√ß√£o). Se o valor cr√≠tico da distribui√ß√£o $\chi^2$ com 1 grau de liberdade a um n√≠vel de signific√¢ncia de 5% √© 3.84, rejeitamos a hip√≥tese nula se $LM > 3.84$. Neste caso, $0.1232 < 3.84$, portanto, n√£o rejeitamos a hip√≥tese nula de que $\alpha_1 = 0$.

3.  **Diagn√≥stico do Modelo:** A an√°lise dos *scores* individuais pode revelar informa√ß√µes sobre o ajuste do modelo a diferentes partes dos dados. Grandes *scores* podem indicar observa√ß√µes influentes ou regi√µes onde o modelo se ajusta mal.

> üí° **Exemplo Num√©rico:**
>
> Ap√≥s estimar um modelo ARCH, voc√™ analisa os *scores* individuais para cada observa√ß√£o. Voc√™ percebe que para a observa√ß√£o *t* = 150, o *score* para o par√¢metro $\alpha_1$ √© muito grande em magnitude (e.g., 5.0), enquanto para outras observa√ß√µes os *scores* est√£o geralmente pr√≥ximos de zero. Isso pode indicar que a observa√ß√£o 150 tem um impacto desproporcional na estimativa de $\alpha_1$. Uma an√°lise mais aprofundada desta observa√ß√£o pode revelar que ela corresponde a um per√≠odo de alta volatilidade n√£o capturado adequadamente pelo modelo, ou que h√° um erro nos dados para essa observa√ß√£o.

4.  **Estima√ß√£o por GMM:** Como $E[s_t(\theta)|Y_{t-1}] = 0$, o *score* fornece um conjunto de condi√ß√µes de ortogonalidade que podem ser utilizadas na estima√ß√£o por M√©todo dos Momentos Generalizado (GMM).

### Considera√ß√µes Num√©ricas

No c√°lculo do *score* em modelos **ARCH**, √© essencial garantir a estabilidade num√©rica, especialmente ao calcular as derivadas de $h_t$. Em modelos complexos, a diferencia√ß√£o num√©rica pode ser necess√°ria, e t√©cnicas como diferen√ßas finitas ou diferencia√ß√£o autom√°tica podem ser empregadas. √â importante escolher um tamanho de passo apropriado para a diferencia√ß√£o num√©rica para equilibrar o vi√©s e a vari√¢ncia.

Al√©m disso, ao utilizar algoritmos de otimiza√ß√£o baseados no gradiente, √© importante monitorar a converg√™ncia do algoritmo e verificar se o *score* se aproxima de zero no ponto de converg√™ncia.

### Conclus√£o

O *score* √© uma ferramenta fundamental na estima√ß√£o e infer√™ncia em modelos **ARCH**. Sua estrutura e propriedades, especialmente a condi√ß√£o de ortogonalidade, fornecem uma base s√≥lida para a estima√ß√£o por m√°xima verossimilhan√ßa e testes de hip√≥teses. A compreens√£o detalhada do *score* √© essencial para a aplica√ß√£o bem-sucedida de modelos **ARCH** em diversas √°reas, como finan√ßas, economia e engenharia.

### Refer√™ncias
[^21.1.21]: $s_t(\theta) = \frac{\partial \log f(y_t|x_t, Y_{t-1}; \theta)}{\partial \theta} $
<!-- END -->