## Imposi√ß√£o de Restri√ß√µes de Estacionariedade e N√£o-Negatividade em Modelos ARCH

### Introdu√ß√£o

Este cap√≠tulo aborda os desafios e t√©cnicas para impor as restri√ß√µes de **estacionariedade** e **n√£o-negatividade** durante a estima√ß√£o de modelos **ARCH**. Como discutido em cap√≠tulos anteriores, essas restri√ß√µes s√£o cruciais para garantir a validade te√≥rica e a interpretabilidade dos resultados. A imposi√ß√£o dessas restri√ß√µes pode ser complexa na pr√°tica, e este cap√≠tulo oferece uma vis√£o abrangente das abordagens comuns e avan√ßadas utilizadas para lidar com esses desafios. √â essencial abordar este t√≥pico, pois como discutido anteriormente [previous section], os algoritmos de otimiza√ß√£o num√©rica requerem que certas condi√ß√µes sejam satisfeitas, e um modelo mal especificado ou estimado fora das restri√ß√µes te√≥ricas pode levar a conclus√µes err√¥neas.

### Desafios na Imposi√ß√£o das Restri√ß√µes

Na estima√ß√£o de modelos **ARCH(m)**, dois conjuntos de restri√ß√µes precisam ser satisfeitos [^21.1.21]:

1.  **N√£o-Negatividade:** Os par√¢metros $\zeta$ e $\alpha_i$ devem ser n√£o-negativos para $i = 1, 2, ..., m$. Isso garante que a vari√¢ncia condicional $h_t$ seja sempre positiva [^21.1.10, 21.1.19].
2.  **Estacionariedade:** A soma dos coeficientes $\alpha_i$ deve ser menor que 1, ou seja, $\sum_{i=1}^m \alpha_i < 1$. Isso garante que o processo seja fracamente estacion√°rio e que a vari√¢ncia incondicional seja finita [^21.1.7, 21.1.21].

> üí° **Exemplo Num√©rico:** Considere um modelo ARCH(1): $h_t = \zeta + \alpha_1 u_{t-1}^2$. Se $\zeta < 0$ ou $\alpha_1 < 0$, ent√£o $h_t$ pode se tornar negativo para alguns valores de $u_{t-1}^2$, o que n√£o faz sentido, pois $h_t$ √© uma vari√¢ncia. Similarmente, se $\alpha_1 > 1$, o processo se torna explosivo, e a vari√¢ncia incondicional tende ao infinito.

A imposi√ß√£o dessas restri√ß√µes pode ser desafiadora porque os algoritmos de otimiza√ß√£o num√©rica padr√£o n√£o garantem automaticamente que essas restri√ß√µes ser√£o satisfeitas. Al√©m disso, impor as restri√ß√µes diretamente pode complicar a otimiza√ß√£o e aumentar o tempo computacional.

**Proposi√ß√£o 1:** Sem a imposi√ß√£o das restri√ß√µes de n√£o-negatividade e estacionariedade, o estimador de m√°xima verossimilhan√ßa pode ser inconsistente e as infer√™ncias baseadas no modelo podem ser inv√°lidas.

*Prova:*

I.  Se as restri√ß√µes de n√£o-negatividade n√£o forem impostas e qualquer um dos par√¢metros ($\zeta$ ou $\alpha_i$) assumir valores negativos, ent√£o $h_t$ poder√° assumir valores negativos. Como $h_t$ √© uma vari√¢ncia condicional, essa situa√ß√£o √© teoricamente inconsistente.

II. Se a vari√¢ncia condicional for negativa, a fun√ß√£o de log-verossimilhan√ßa n√£o estar√° bem definida para a distribui√ß√£o normal, uma vez que o termo logar√≠tmico ($\log(h_t)$) se torna indefinido.

III. Se a restri√ß√£o de estacionariedade n√£o for imposta, e a soma dos coeficientes $\alpha_i$ for maior ou igual a 1, ent√£o o processo ARCH se tornar√° n√£o-estacion√°rio. Isso significa que a vari√¢ncia incondicional do processo √© infinita, e os momentos do processo n√£o ser√£o constantes ao longo do tempo.

IV. Se as restri√ß√µes n√£o forem impostas e a fun√ß√£o de log-verossimilhan√ßa n√£o estiver bem definida ou as propriedades estat√≠sticas do processo forem violadas, ent√£o os estimadores de m√°xima verossimilhan√ßa n√£o convergir√£o para os verdadeiros valores dos par√¢metros.

V. Assim, a infer√™ncia estat√≠stica (testes de hip√≥teses e intervalos de confian√ßa) baseada em modelos estimados sem impor estas restri√ß√µes pode levar a conclus√µes err√¥neas. $\blacksquare$

**Proposi√ß√£o 1.1:** A viola√ß√£o das restri√ß√µes de n√£o-negatividade e estacionariedade pode levar a previs√µes de vari√¢ncia condicional negativas ou explosivas, comprometendo a utilidade do modelo para fins de previs√£o e gerenciamento de risco.

*Prova:*

I. Se $\zeta < 0$ ou $\alpha_i < 0$ para algum $i$, ent√£o para certos valores de $u_{t-i}^2$, a vari√¢ncia condicional $h_t = \zeta + \sum_{i=1}^m \alpha_i u_{t-i}^2$ pode ser negativa, o que √© inconsistente com a defini√ß√£o de vari√¢ncia. Isso inviabiliza o uso do modelo para prever a volatilidade.

II. Se $\sum_{i=1}^m \alpha_i \geq 1$, ent√£o a vari√¢ncia condicional pode crescer indefinidamente ao longo do tempo, especialmente em per√≠odos de alta volatilidade passada. Isso leva a previs√µes de volatilidade irrealisticamente altas, comprometendo o gerenciamento de risco, pois as medidas de risco (como Value at Risk) ser√£o superestimadas.

III. Portanto, a viola√ß√£o dessas restri√ß√µes compromete a validade das previs√µes e das an√°lises de risco baseadas no modelo. $\blacksquare$

### T√©cnicas para Impor as Restri√ß√µes

Diversas t√©cnicas podem ser utilizadas para impor as restri√ß√µes de n√£o-negatividade e estacionariedade:

1.  **Restri√ß√£o Direta:** A forma mais simples √© impor as restri√ß√µes diretamente no algoritmo de otimiza√ß√£o [^21.1.21]. Isso pode ser feito utilizando algoritmos que permitam restri√ß√µes de desigualdade, como o m√©todo de programa√ß√£o quadr√°tica sequencial (SQP).
    > üí° **Exemplo Num√©rico:** Ao utilizar a fun√ß√£o `minimize` do `scipy.optimize` em Python, √© poss√≠vel definir as restri√ß√µes utilizando a op√ß√£o `bounds`. Considere um modelo ARCH(1) com $\zeta$ e $\alpha_1$ como par√¢metros. Queremos estimar esses par√¢metros usando dados simulados.
    >
    > ```python
    > import numpy as np
    > from scipy.optimize import minimize
    >
    > # Simula√ß√£o de dados ARCH(1)
    > np.random.seed(42)
    > T = 200  # Tamanho da amostra
    > zeta_true = 0.1
    > alpha_true = 0.5
    > u = np.random.normal(0, 1, T)
    > h = np.zeros(T)
    > y = np.zeros(T)
    >
    > h[0] = zeta_true / (1 - alpha_true)  # Vari√¢ncia incondicional inicial
    > y[0] = u[0] * np.sqrt(h[0])
    >
    > for t in range(1, T):
    >     h[t] = zeta_true + alpha_true * y[t-1]**2
    >     y[t] = u[t] * np.sqrt(h[t])
    >
    > data = y
    >
    > # Defini√ß√£o da fun√ß√£o objetivo (negativa da log-verossimilhan√ßa)
    > def objective(params, data):
    >     zeta, alpha = params
    >     T = len(data)
    >     h = np.zeros(T)
    >     h[0] = zeta / (1 - alpha) if alpha < 1 else zeta # Vari√¢ncia incondicional inicial
    >     loglik = 0
    >     for t in range(1, T):
    >         h[t] = zeta + alpha * data[t-1]**2
    >         loglik += -0.5 * np.log(h[t]) - 0.5 * (data[t]**2) / h[t]
    >     return -loglik # Negativo porque queremos minimizar
    >
    > # Defini√ß√£o das restri√ß√µes
    > # N√£o-negatividade: zeta >= 0 e alpha >= 0
    > # Estacionariedade: alpha < 1
    > bounds = ((0.0001, None), (0.0001, 0.9999)) # (min, max) para zeta e alpha.  Adicionado um valor m√≠nimo pequeno para evitar problemas num√©ricos
    >
    > # Chute inicial
    > initial_guess = [0.2, 0.3]
    >
    > # Otimiza√ß√£o com restri√ß√µes
    > result = minimize(objective, initial_guess, args=(data,), method='L-BFGS-B', bounds=bounds)
    >
    > # Resultados
    > print(result)
    > print(f"Estimativa de zeta: {result.x[0]:.4f}")
    > print(f"Estimativa de alpha: {result.x[1]:.4f}")
    > ```

2.  **Transforma√ß√£o de Par√¢metros:** Uma t√©cnica comum √© transformar os par√¢metros para que as restri√ß√µes sejam automaticamente satisfeitas [previous section, Lema 1]. Por exemplo, pode-se utilizar a transforma√ß√£o exponencial para garantir a n√£o-negatividade [previous section, Lema 1]:

    $$\zeta = \exp(\zeta')$$

    $$\alpha_i = \exp(\alpha_i')$$

    onde $\zeta'$ e $\alpha_i'$ s√£o os par√¢metros a serem otimizados. Ap√≥s a otimiza√ß√£o, os par√¢metros originais s√£o obtidos tomando a exponencial dos par√¢metros transformados [previous section, Lema 1].

    > üí° **Exemplo Num√©rico:** Se $\zeta' = 0$, ent√£o $\zeta = \exp(0) = 1 > 0$. Se $\alpha_1' = -1$, ent√£o $\alpha_1 = \exp(-1) \approx 0.368 > 0$.
    >
    > Para impor a restri√ß√£o de estacionariedade, pode-se utilizar a transforma√ß√£o log√≠stica:
    >$$\alpha_i = \frac{\exp(\alpha_i')}{1 + \exp(\alpha_i')}$$
    >
    >para garantir que o parametro fique entre 0 e 1. O ponto fraco dessa abordagem √© que a soma dos par√¢metros ainda tem que ser menor do que 1. Uma aproxima√ß√£o alternativa seria normalizar os par√¢metros $\alpha_i$, e restringir um outro par√¢metro $\gamma$, para que a soma dos $\alpha_i$ normalized seja menor que $\gamma$:
    >
    >$$ \alpha_{i, \text{normalized}} = \frac{\exp(\alpha_i')}{\sum_{j=1}^{m} \exp(\alpha_j')} \gamma$$
    >
    >sendo $\gamma$ um par√¢metro que estimamos, com $0 \leq \gamma < 1$.
    >
    > > üí° **Exemplo Num√©rico:** Vamos aplicar a transforma√ß√£o exponencial e log√≠stica em um modelo ARCH(1) para garantir n√£o-negatividade e estacionariedade:
    > >
    > > ```python
    > > import numpy as np
    > > from scipy.optimize import minimize
    > >
    > > # Simula√ß√£o de dados ARCH(1) (como no exemplo anterior)
    > > np.random.seed(42)
    > > T = 200  # Tamanho da amostra
    > > zeta_true = 0.1
    > > alpha_true = 0.5
    > > u = np.random.normal(0, 1, T)
    > > h = np.zeros(T)
    > > y = np.zeros(T)
    > >
    > > h[0] = zeta_true / (1 - alpha_true)  # Vari√¢ncia incondicional inicial
    > > y[0] = u[0] * np.sqrt(h[0])
    > >
    > > for t in range(1, T):
    > >     h[t] = zeta_true + alpha_true * y[t-1]**2
    > >     y[t] = u[t] * np.sqrt(h[t])
    > >
    > > data = y
    > >
    > > # Defini√ß√£o da fun√ß√£o objetivo (negativa da log-verossimilhan√ßa) com transforma√ß√£o
    > > def objective_transformed(params, data):
    > >     zeta_prime, alpha_prime = params
    > >     zeta = np.exp(zeta_prime)
    > >     alpha = np.exp(alpha_prime) / (1 + np.exp(alpha_prime))  # Transforma√ß√£o log√≠stica
    > >     T = len(data)
    > >     h = np.zeros(T)
    > >     h[0] = zeta / (1 - alpha) if alpha < 1 else zeta  # Vari√¢ncia incondicional inicial
    > >     loglik = 0
    > >     for t in range(1, T):
    > >         h[t] = zeta + alpha * data[t-1]**2
    > >         loglik += -0.5 * np.log(h[t]) - 0.5 * (data[t]**2) / h[t]
    > >     return -loglik
    > >
    > > # Chute inicial para os par√¢metros transformados
    > > initial_guess_transformed = [np.log(0.2), np.log(0.3) ]
    > >
    > > # Otimiza√ß√£o sem restri√ß√µes expl√≠citas
    > > result_transformed = minimize(objective_transformed, initial_guess_transformed, args=(data,), method='L-BFGS-B')
    > >
    > > # Recuperar os par√¢metros originais
    > > zeta_estimated = np.exp(result_transformed.x[0])
    > > alpha_estimated = np.exp(result_transformed.x[1]) / (1 + np.exp(result_transformed.x[1]))
    > >
    > > # Resultados
    > > print(result_transformed)
    > > print(f"Estimativa de zeta: {zeta_estimated:.4f}")
    > > print(f"Estimativa de alpha: {alpha_estimated:.4f}")
    > > ```

**Lema 2:** A transforma√ß√£o exponencial garante a n√£o-negatividade dos par√¢metros, mas pode dificultar a otimiza√ß√£o se os valores √≥timos de $\zeta$ ou $\alpha_i$ forem pr√≥ximos de zero.

*Prova:*
I. A fun√ß√£o exponencial $\exp(x)$ √© sempre positiva para qualquer valor real de $x$. Portanto, ao transformar os par√¢metros $\zeta$ e $\alpha_i$ usando a fun√ß√£o exponencial, garantimos que eles sejam sempre n√£o-negativos.

II. No entanto, a derivada da fun√ß√£o exponencial se aproxima de zero quando $x$ tende a $-\infty$. Isso significa que pequenas mudan√ßas em $\zeta'$ ou $\alpha_i'$ podem levar a mudan√ßas muito pequenas em $\zeta$ ou $\alpha_i$ quando esses par√¢metros s√£o pr√≥ximos de zero. Isso pode dificultar a converg√™ncia do algoritmo de otimiza√ß√£o, especialmente se os valores √≥timos de $\zeta$ ou $\alpha_i$ forem pr√≥ximos de zero. $\blacksquare$

3.  **Reparametriza√ß√£o:** Outra abordagem √© reparametrizar o modelo de forma que as restri√ß√µes sejam inerentes √† especifica√ß√£o. Por exemplo, em vez de estimar os par√¢metros $\alpha_i$ diretamente, pode-se estimar par√¢metros que determinam a decomposi√ß√£o da vari√¢ncia condicional em componentes positivos.

4.  **Truncamento:** Se o algoritmo de otimiza√ß√£o produzir valores que violem as restri√ß√µes, pode-se truncar os valores para que fiquem dentro dos limites permitidos [^21.1.21]. No entanto, essa abordagem pode introduzir vi√©s e deve ser utilizada com cautela.

> üí° **Exemplo Num√©rico:** Se, em um modelo ARCH(1), o algoritmo de otimiza√ß√£o estimar $\alpha_1 = 1.2$, pode-se truncar o valor para $\alpha_1 = 0.999$ para garantir a estacionariedade.

**Lema 3:** O truncamento de par√¢metros para satisfazer as restri√ß√µes pode introduzir vi√©s nas estimativas, especialmente se os verdadeiros valores dos par√¢metros estiverem pr√≥ximos dos limites das restri√ß√µes.

*Prova:*

I. O truncamento for√ßa os par√¢metros a assumirem valores dentro dos limites especificados, mesmo que o estimador irrestrito (sem truncamento) esteja fora desses limites.

II. Se o verdadeiro valor de um par√¢metro (digamos, $\alpha_1$) estiver pr√≥ximo de 1 (o limite superior para estacionariedade em um ARCH(1)), ent√£o o truncamento frequentemente for√ßar√° $\hat{\alpha}_1$ a ser menor do que seu valor verdadeiro. Isso sistematicamente subestima o par√¢metro, introduzindo um vi√©s negativo.

III. O vi√©s introduzido pelo truncamento pode afetar a precis√£o das previs√µes e a validade das infer√™ncias estat√≠sticas baseadas no modelo. $\blacksquare$

5.  **Penaliza√ß√£o:** Uma abordagem alternativa √© adicionar um termo de penalidade √† fun√ß√£o de log-verossimilhan√ßa que penalize valores dos par√¢metros que violem as restri√ß√µes. Por exemplo, pode-se adicionar um termo da forma:

$$P(\theta) = -M \cdot \mathbb{I}(\sum_{i=1}^m \alpha_i > 1) - M \sum_{i=1}^m \mathbb{I}(\alpha_i < 0)$$

onde $\mathbb{I}(\cdot)$ √© a fun√ß√£o indicadora e $M$ √© uma constante positiva grande. Ao maximizar a fun√ß√£o de log-verossimilhan√ßa com a penalidade, o algoritmo tender√° a evitar valores dos par√¢metros que violem as restri√ß√µes.

> üí° **Exemplo Num√©rico:** Vamos criar uma fun√ß√£o de penalidade para um modelo ARCH(1):
> ```python
> import numpy as np
> from scipy.optimize import minimize
>
> # Simula√ß√£o de dados ARCH(1) (como no exemplo anterior)
> np.random.seed(42)
> T = 200  # Tamanho da amostra
> zeta_true = 0.1
> alpha_true = 0.5
> u = np.random.normal(0, 1, T)
> h = np.zeros(T)
> y = np.zeros(T)
>
> h[0] = zeta_true / (1 - alpha_true)  # Vari√¢ncia incondicional inicial
> y[0] = u[0] * np.sqrt(h[0])
>
> for t in range(1, T):
>     h[t] = zeta_true + alpha_true * y[t-1]**2
>     y[t] = u[t] * np.sqrt(h[t])
>
> data = y
>
> # Defini√ß√£o da fun√ß√£o objetivo (negativa da log-verossimilhan√ßa)
> def objective(params, data):
>     zeta, alpha = params
>     T = len(data)
>     h = np.zeros(T)
>     h[0] = zeta / (1 - alpha) if alpha < 1 else zeta  # Vari√¢ncia incondicional inicial
>     loglik = 0
>     for t in range(1, T):
>         h[t] = zeta + alpha * data[t-1]**2
>         loglik += -0.5 * np.log(h[t]) - 0.5 * (data[t]**2) / h[t]
>     return -loglik
>
> # Defini√ß√£o da fun√ß√£o de penalidade
> def penalty(params, M):
>     zeta, alpha = params
>     penalty_value = 0
>     if alpha >= 1:
>         penalty_value += M
>     if zeta < 0:
>         penalty_value += M
>     if alpha < 0:
>         penalty_value += M
>     return penalty_value
>
> # Fun√ß√£o objetivo com penalidade
> def objective_penalized(params, data, M):
>     return objective(params, data) + penalty(params, M)
>
> # Chute inicial
> initial_guess = [0.2, 0.3]
>
> # Constante de penalidade
> M = 1000
>
> # Otimiza√ß√£o com penalidade
> result_penalized = minimize(objective_penalized, initial_guess, args=(data, M), method='L-BFGS-B')
>
> # Resultados
> print(result_penalized)
> print(f"Estimativa de zeta: {result_penalized.x[0]:.4f}")
> print(f"Estimativa de alpha: {result_penalized.x[1]:.4f}")
> ```

6.  **Pequeno *m* ou Estrutura *ad hoc*:** Frequentemente, m √© pequeno ou uma estrutura ad hoc √© imposta em {Œ±‚±º} [^21.1.21]. Por exemplo, pode-se considerar apenas modelos ARCH(1) ou ARCH(2) devido √† sua simplicidade. Uma estrutura *ad hoc* pode ser impor uma forma funcional espec√≠fica nos $\alpha_i$.

### Considera√ß√µes Pr√°ticas

Ao escolher uma t√©cnica para impor as restri√ß√µes de estacionariedade e n√£o-negatividade, √© importante considerar as seguintes quest√µes:

*   **Complexidade:** A t√©cnica deve ser relativamente simples de implementar e n√£o deve aumentar significativamente o tempo computacional.
*   **Flexibilidade:** A t√©cnica deve ser flex√≠vel o suficiente para lidar com diferentes especifica√ß√µes de modelos ARCH.
*   **Vi√©s:** A t√©cnica n√£o deve introduzir vi√©s significativo nas estimativas dos par√¢metros.
*   **Interpretabilidade:** A t√©cnica n√£o deve dificultar a interpreta√ß√£o dos resultados.

> üí° **Exemplo Num√©rico:** A transforma√ß√£o de par√¢metros √© uma t√©cnica amplamente utilizada devido √† sua simplicidade e flexibilidade. No entanto, √© importante ter cuidado ao interpretar os resultados, pois os par√¢metros transformados podem n√£o ter uma interpreta√ß√£o direta.

**Teorema 1:** Para modelos ARCH(m) estacion√°rios e erg√≥dicos, a imposi√ß√£o das restri√ß√µes de n√£o-negatividade e estacionariedade garante que o estimador de m√°xima verossimilhan√ßa seja consistente e assintoticamente normal.

*Prova:*

A prova desse teorema √© geralmente baseada nas seguintes etapas:

I.  **Condi√ß√µes de Identifica√ß√£o:** As restri√ß√µes garantem que os par√¢metros do modelo sejam identific√°veis. Sem essas restri√ß√µes, m√∫ltiplos conjuntos de par√¢metros poderiam levar √† mesma fun√ß√£o de log-verossimilhan√ßa, tornando a estima√ß√£o inconsistente.

II. **Regularidade Assint√≥tica:** Impor as restri√ß√µes de n√£o-negatividade e estacionariedade geralmente envolve transformar os par√¢metros do modelo (por exemplo, usando fun√ß√µes exponenciais ou log√≠sticas) para garantir que as restri√ß√µes sejam satisfeitas automaticamente. Essas transforma√ß√µes preservam a diferenciabilidade e a continuidade da fun√ß√£o de log-verossimilhan√ßa, desde que as transforma√ß√µes em si sejam diferenci√°veis e cont√≠nuas.  Isto assegura que podemos aplicar as propriedades assint√≥ticas para demonstrar a normalidade assint√≥tica do estimador MLE.

III. **Validade da Matriz de Informa√ß√£o:** As restri√ß√µes de n√£o-negatividade e estacionariedade garantem que a matriz de informa√ß√£o de Fisher seja positiva definida. Isso √© crucial para a validade da infer√™ncia assint√≥tica.

IV. **Consist√™ncia e Normalidade Assint√≥tica:** Sob estacionariedade e ergodicidade, o processo satisfaz as condi√ß√µes para a consist√™ncia e normalidade assint√≥tica dos estimadores de m√°xima verossimilhan√ßa, dado que a fun√ß√£o de log-verossimilhan√ßa √© bem comportada e as restri√ß√µes s√£o impostas.

$\blacksquare$

**Teorema 1.1:** Sob as mesmas condi√ß√µes do Teorema 1, a efici√™ncia assint√≥tica do estimador de m√°xima verossimilhan√ßa restrito √© igual ou superior √† do estimador irrestrito.

*Prova:*

I. A imposi√ß√£o de restri√ß√µes corretas (isto √©, restri√ß√µes que s√£o v√°lidas na popula√ß√£o) nunca diminui a efici√™ncia assint√≥tica do estimador de m√°xima verossimilhan√ßa.

II. Intuitivamente, a imposi√ß√£o das restri√ß√µes reduz o espa√ßo de par√¢metros, concentrando a estima√ß√£o em uma regi√£o menor e mais relevante. Isso pode levar a estimativas mais precisas, especialmente em amostras finitas.

III. Formalmente, a efici√™ncia assint√≥tica √© medida pela matriz de informa√ß√£o de Fisher. A imposi√ß√£o de restri√ß√µes (quando verdadeiras) geralmente aumenta a matriz de informa√ß√£o, resultando em uma menor vari√¢ncia assint√≥tica e, portanto, maior efici√™ncia. $\blacksquare$

**Lema 1:** Seja $\hat{\theta}$ o estimador de m√°xima verossimilhan√ßa restrito, ou seja, o estimador obtido ap√≥s impor as restri√ß√µes de n√£o-negatividade e estacionariedade. Ent√£o, sob condi√ß√µes de regularidade, $\hat{\theta}$ converge em probabilidade para o verdadeiro valor do par√¢metro $\theta_0$, ou seja, $\hat{\theta} \xrightarrow{p} \theta_0$.

*Proof:*
A prova desse lema depende das seguintes propriedades:
I. **Identificabilidade:** Como mencionado anteriormente, a identificabilidade dos par√¢metros √© crucial para a consist√™ncia.
II.  **Consist√™ncia do Estimador Irrestrito:** Sob as condi√ß√µes de regularidade usuais, o estimador de m√°xima verossimilhan√ßa irrestrito √© consistente.
III.  **Continuidade da Proje√ß√£o:** A imposi√ß√£o das restri√ß√µes pode ser vista como uma proje√ß√£o do estimador irrestrito no espa√ßo de par√¢metros restrito. Se a proje√ß√£o √© cont√≠nua, ent√£o a consist√™ncia √© preservada.
IV.  **Domin√¢ncia:** Se a fun√ß√£o de log-verossimilhan√ßa √© dominada, ent√£o a consist√™ncia segue.
Com base nessas propriedades, podemos concluir que o estimador de m√°xima verossimilhan√ßa restrito √© consistente. $\blacksquare$

### Conclus√£o

A imposi√ß√£o das restri√ß√µes de estacionariedade e n√£o-negatividade √© uma etapa essencial na estima√ß√£o de modelos ARCH. T√©cnicas como a transforma√ß√£o de par√¢metros e a restri√ß√£o direta s√£o amplamente utilizadas na pr√°tica e garantem que as estimativas dos par√¢metros sejam consistentes com a teoria e com a realidade econ√¥mica. Ao considerar cuidadosamente os desafios e t√©cnicas associados √† imposi√ß√£o dessas restri√ß√µes, os pesquisadores e profissionais podem aplicar modelos ARCH com maior confian√ßa e obter resultados mais confi√°veis e interpret√°veis.

### Refer√™ncias
[^21.1.7]: $\alpha_1 + \alpha_2 + \ldots + \alpha_m < 1$.
[^21.1.10]: If $h_t$ evolves according to $h_t = \zeta + \alpha_1 u_{t-1}^2 + \alpha_2 u_{t-2}^2 + \ldots + \alpha_m u_{t-m}^2$
[^21.1.19]: $h_t = \zeta + \alpha_1(y_{t-1} - x_{t-1}\beta)^2 + \alpha_2(y_{t-2} - x_{t-2}\beta)^2 + \ldots + \alpha_m(y_{t-m}\beta)^2 = [z(\beta)]'\delta$
[^21.1.21]: A imposi√ß√£o da estacionariedade (Œ£Œ±‚±º < 1) e da n√£o negatividade (Œ±‚±º ‚â• 0) pode ser dif√≠cil. Frequentemente, m √© pequeno ou uma estrutura ad hoc √© imposta em {Œ±‚±º}.
<!-- END -->