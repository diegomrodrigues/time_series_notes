## EstimaÃ§Ã£o de MÃ¡xima VerossimilhanÃ§a com InovaÃ§Ãµes NÃ£o Gaussianas

### IntroduÃ§Ã£o

Este capÃ­tulo expande a discussÃ£o anterior sobre EstimaÃ§Ã£o de MÃ¡xima VerossimilhanÃ§a (MLE) em modelos **ARCH** para o caso em que a distribuiÃ§Ã£o das inovaÃ§Ãµes ($v_t$) nÃ£o Ã© Gaussiana. Embora a normalidade condicional seja uma suposiÃ§Ã£o comum, muitas sÃ©ries temporais financeiras exibem caudas mais pesadas do que as permitidas pela distribuiÃ§Ã£o normal. A modelagem correta dessas caudas pesadas pode melhorar significativamente a precisÃ£o das previsÃµes e a avaliaÃ§Ã£o de risco. Este capÃ­tulo explora como o procedimento bÃ¡sico de MLE pode ser adaptado para acomodar outras distribuiÃ§Ãµes, com foco particular na distribuiÃ§Ã£o *t* de Student.

### Modelagem com a DistribuiÃ§Ã£o *t* de Student

Uma escolha popular para modelar as inovaÃ§Ãµes nÃ£o-Gaussianas Ã© a distribuiÃ§Ã£o *t* de Student [^21.1.22]. A distribuiÃ§Ã£o *t* de Student, com $\nu$ graus de liberdade, permite modelar caudas mais pesadas do que a distribuiÃ§Ã£o normal, tornando-a mais robusta Ã  presenÃ§a de outliers. A densidade da distribuiÃ§Ã£o *t* de Student Ã© dada por [^21.1.22]:

$$f(u_t) = \frac{\Gamma[(\nu + 1)/2]}{\Gamma(\nu/2) \sqrt{\pi \nu}} (\nu - 2)^{-1/2} h_t^{-1/2} \left[ 1 + \frac{u_t^2}{h_t (\nu - 2)} \right]^{-(\nu+1)/2}$$

onde $\Gamma(\cdot)$ Ã© a funÃ§Ã£o gama. Para $\nu > 2$, a distribuiÃ§Ã£o *t* tem mÃ©dia zero e variÃ¢ncia $\frac{\nu}{\nu-2} h_t$, onde $h_t$ Ã© o parÃ¢metro de escala [^21.1.22].

> ğŸ’¡ **Exemplo NumÃ©rico:** Uma distribuiÃ§Ã£o *t* de Student com 3 graus de liberdade ($\nu=3$) tem caudas muito mais pesadas que uma Normal. Para $\nu \to \infty$, a distribuiÃ§Ã£o *t* se aproxima da Normal. O parÃ¢metro $\nu$ controla o grau de *kurtosis* da distribuiÃ§Ã£o, e, por consequÃªncia, as caudas. Um baixo valor de $\nu$ implica maior probabilidade de eventos extremos, e uma maior magnitude desses eventos.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy.stats import t, norm
>
> # Defina os graus de liberdade
> degrees_of_freedom = [3, 5, 10, 30, 100]
>
> # Gere valores x para o plot
> x = np.linspace(-5, 5, 400)
>
> # Crie a figura e os eixos
> plt.figure(figsize=(12, 8))
>
> # Plote a distribuiÃ§Ã£o t de Student para cada grau de liberdade
> for df in degrees_of_freedom:
>     plt.plot(x, t.pdf(x, df), label=f't-distribution (df={df})')
>
> # Plote a distribuiÃ§Ã£o normal padrÃ£o
> plt.plot(x, norm.pdf(x, 0, 1), label='Normal distribution', linestyle='--')
>
> # Adicione tÃ­tulo e rÃ³tulos
> plt.title('ComparaÃ§Ã£o da DistribuiÃ§Ã£o t de Student com a DistribuiÃ§Ã£o Normal')
> plt.xlabel('x')
> plt.ylabel('Densidade')
>
> # Adicione legenda
> plt.legend()
>
> # Adicione grade
> plt.grid(True)
>
> # Mostre o plot
> plt.show()
> ```
> O grÃ¡fico mostra como a distribuiÃ§Ã£o *t* se aproxima da distribuiÃ§Ã£o normal Ã  medida que os graus de liberdade aumentam. Para baixos graus de liberdade (e.g., 3), as caudas da distribuiÃ§Ã£o *t* sÃ£o significativamente mais pesadas, indicando uma maior probabilidade de valores extremos.

Para utilizar a distribuiÃ§Ã£o *t* de Student em modelos **ARCH**, substituÃ­mos a densidade normal na funÃ§Ã£o de log-verossimilhanÃ§a pela densidade *t*. A funÃ§Ã£o de log-verossimilhanÃ§a amostral condicional torna-se:

$$L(\theta) = \sum_{t=1}^{T} \log f(y_t|x_t, Y_{t-1}; \theta) = T \log \left\{ \frac{\Gamma[(\nu + 1)/2]}{\Gamma(\nu/2) \sqrt{\pi \nu}} \right\} - \frac{T}{2} \log(\nu - 2) - \frac{1}{2} \sum_{t=1}^{T} \log(h_t) - \frac{(\nu + 1)}{2} \sum_{t=1}^{T} \log \left[ 1 + \frac{(y_t - x_t'\beta)^2}{h_t (\nu - 2)} \right]$$

onde $\theta$ inclui agora o parÃ¢metro $\nu$, alÃ©m de $\beta$ e os parÃ¢metros do modelo **ARCH** ($\zeta, \alpha_i$). Essa funÃ§Ã£o Ã© maximizada numericamente em relaÃ§Ã£o a $\nu$, $\beta$ e $\delta$, sujeita Ã  restriÃ§Ã£o $\nu > 2$ [previous section].

> ğŸ’¡ **Exemplo NumÃ©rico:** Para demonstrar como a escolha da distribuiÃ§Ã£o afeta o valor da funÃ§Ã£o de log-verossimilhanÃ§a, vamos comparar os valores para uma observaÃ§Ã£o especÃ­fica sob uma distribuiÃ§Ã£o normal e uma distribuiÃ§Ã£o *t* com $\nu = 5$.
>
> Suponha que tenhamos $y_t - x_t'\beta = u_t = 0.1$, $h_t = 0.01$, e $\nu = 5$.
>
> Para a distribuiÃ§Ã£o normal, a contribuiÃ§Ã£o para a log-verossimilhanÃ§a Ã©:
> $$ \log f_{Normal}(u_t) = -\frac{1}{2} \log(2\pi) - \frac{1}{2} \log(h_t) - \frac{1}{2} \frac{u_t^2}{h_t} = -\frac{1}{2} \log(2\pi) - \frac{1}{2} \log(0.01) - \frac{1}{2} \frac{(0.1)^2}{0.01} \approx 0.9788 $$
>
> Para a distribuiÃ§Ã£o *t* com $\nu = 5$, a contribuiÃ§Ã£o Ã©:
>
> ```python
> import numpy as np
> from scipy.stats import t
> from scipy.special import gamma
>
> # Valores dos parÃ¢metros
> nu = 5
> u_t = 0.1
> h_t = 0.01
>
> # CÃ¡lculo da constante
> constant = gamma((nu + 1) / 2) / (gamma(nu / 2) * np.sqrt(np.pi * nu))
>
> # Densidade t
> density_t = constant * (nu - 2)**(-0.5) * h_t**(-0.5) * (1 + (u_t**2) / (h_t * (nu - 2)))**(-(nu + 1) / 2)
>
> # Log da densidade t
> log_density_t = np.log(density_t)
>
> print(f"Valor da log-densidade t: {log_density_t}")
> # Resultado: Valor da log-densidade t: 1.0497
> ```
>
> Neste caso, a distribuiÃ§Ã£o *t* atribui uma probabilidade ligeiramente maior a esse valor do que a distribuiÃ§Ã£o normal. Este efeito Ã© mais pronunciado para valores de $u_t$ maiores (caudas mais pesadas).
>
> Agora, considere um valor mais extremo, $u_t = 0.5$. Mantendo $h_t = 0.01$:
>
> Para a distribuiÃ§Ã£o normal:
> $$ \log f_{Normal}(u_t) = -\frac{1}{2} \log(2\pi) - \frac{1}{2} \log(h_t) - \frac{1}{2} \frac{(0.5)^2}{h_t} \approx -11.207 $$
>
> Para a distribuiÃ§Ã£o *t* com $\nu = 5$:
>
> ```python
> import numpy as np
> from scipy.stats import t
> from scipy.special import gamma
>
> # Valores dos parÃ¢metros
> nu = 5
> u_t = 0.5
> h_t = 0.01
>
> # CÃ¡lculo da constante
> constant = gamma((nu + 1) / 2) / (gamma(nu / 2) * np.sqrt(np.pi * nu))
>
> # Densidade t
> density_t = constant * (nu - 2)**(-0.5) * h_t**(-0.5) * (1 + (u_t**2) / (h_t * (nu - 2)))**(-(nu + 1) / 2)
>
> # Log da densidade t
> log_density_t = np.log(density_t)
>
> print(f"Valor da log-densidade t: {log_density_t}")
> # Resultado: Valor da log-densidade t: -2.764
> ```
>
> A distribuiÃ§Ã£o *t* atribui uma probabilidade muito maior a esse valor extremo do que a distribuiÃ§Ã£o normal.

**Lema 1:** Se $u_t \sim t(\nu)$, entÃ£o $E[u_t]=0$ para $\nu > 1$ e $Var[u_t] = \frac{\nu}{\nu-2}h_t$ para $\nu > 2$. AlÃ©m disso, o quarto momento existe se $\nu > 4$, e Ã© dado por $E[u_t^4] = 3h_t^2 \frac{\nu^2}{(\nu-2)(\nu-4)}$.

*Prova:* A prova segue diretamente das propriedades da distribuiÃ§Ã£o t de Student e pode ser encontrada em textos padrÃ£o de estatÃ­stica. $\blacksquare$

### ImposiÃ§Ã£o da RestriÃ§Ã£o $\nu > 2$

A restriÃ§Ã£o $\nu > 2$ Ã© crucial porque garante a existÃªncia da variÃ¢ncia da distribuiÃ§Ã£o *t* [^21.1.22]. Para impor essa restriÃ§Ã£o durante a otimizaÃ§Ã£o, podemos utilizar uma transformaÃ§Ã£o dos parÃ¢metros. Uma transformaÃ§Ã£o comum Ã©:

$$\nu = 2 + \exp(\nu')$$

onde $\nu'$ Ã© o parÃ¢metro a ser otimizado. Essa transformaÃ§Ã£o garante que $\nu$ seja sempre maior que 2.

**ProposiÃ§Ã£o 1:** A transformaÃ§Ã£o $\nu = 2 + \exp(\nu')$ mapeia o espaÃ§o de parÃ¢metros irrestrito $\nu' \in \mathbb{R}$ para o espaÃ§o de parÃ¢metros restrito $\nu \in (2, \infty)$.

*Prova:* Dado que a funÃ§Ã£o exponencial $\exp(\nu')$ Ã© sempre positiva para qualquer valor de $\nu' \in \mathbb{R}$, temos que $\exp(\nu') > 0$. Adicionando 2 a ambos os lados, obtemos $2 + \exp(\nu') > 2$. Portanto, $\nu > 2$. AlÃ©m disso, para qualquer $\nu > 2$, existe um $\nu' = \ln(\nu - 2)$ tal que $\nu = 2 + \exp(\nu')$. Isto demonstra que a transformaÃ§Ã£o Ã© uma bijeÃ§Ã£o entre $\mathbb{R}$ e $(2, \infty)$. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:** Se $\nu' = 0$, entÃ£o $\nu = 2 + \exp(0) = 2 + 1 = 3$. Se $\nu' = 1$, entÃ£o $\nu = 2 + \exp(1) \approx 2 + 2.718 = 4.718$. Esta transformaÃ§Ã£o garante que o valor de $\nu$ seja sempre maior que 2, independentemente do valor de $\nu'$ utilizado na otimizaÃ§Ã£o.
>
> ```python
> import numpy as np
>
> # FunÃ§Ã£o para transformar nu' em nu
> def transform_nu_prime_to_nu(nu_prime):
>   return 2 + np.exp(nu_prime)
>
> # Testando a funÃ§Ã£o com diferentes valores de nu_prime
> nu_prime_values = [-1, 0, 1, 2, 5]
>
> print("nu'   |   nu")
> print("-------|-------")
> for nu_prime in nu_prime_values:
>   nu = transform_nu_prime_to_nu(nu_prime)
>   print(f"{nu_prime:5.2f} | {nu:5.2f}")
> ```
> Este cÃ³digo demonstra como diferentes valores de $\nu'$ sÃ£o transformados em valores de $\nu$ maiores que 2.

### OtimizaÃ§Ã£o NumÃ©rica

A maximizaÃ§Ã£o da funÃ§Ã£o de log-verossimilhanÃ§a com a distribuiÃ§Ã£o *t* de Student Ã© geralmente realizada numericamente. Os mesmos algoritmos de otimizaÃ§Ã£o utilizados para a distribuiÃ§Ã£o normal podem ser aplicados, como o mÃ©todo de Newton-Raphson, o algoritmo BFGS e o mÃ©todo de *scoring* [previous section, 21.1.21]. No entanto, Ã© importante verificar a convergÃªncia do algoritmo e utilizar diferentes valores iniciais para garantir que o mÃ¡ximo global seja encontrado.

**Lema 2:** A funÃ§Ã£o de log-verossimilhanÃ§a com distribuiÃ§Ã£o t de Student Ã© diferenciÃ¡vel em relaÃ§Ã£o a todos os seus parÃ¢metros, incluindo $\nu$, desde que $\nu > 2$.

*Prova:* A diferenciabilidade da funÃ§Ã£o de log-verossimilhanÃ§a segue da diferenciabilidade da funÃ§Ã£o gama e da funÃ§Ã£o logaritmo. A derivada em relaÃ§Ã£o a $\nu$ envolve a funÃ§Ã£o digamma (derivada da funÃ§Ã£o gama), que Ã© bem definida para $\nu > 2$. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Considere que estamos otimizando a log-verossimilhanÃ§a utilizando o algoritmo BFGS. Iniciamos a otimizaÃ§Ã£o com um valor inicial de $\nu' = 0$, o que corresponde a $\nu = 3$. ApÃ³s algumas iteraÃ§Ãµes, o algoritmo converge para $\nu' = 1.0986$, o que corresponde a $\nu \approx 5$. Ã‰ importante verificar se a funÃ§Ã£o de log-verossimilhanÃ§a aumenta a cada iteraÃ§Ã£o e se o algoritmo converge para um valor estÃ¡vel.

### InterpretaÃ§Ã£o dos Resultados

Ao estimar um modelo **ARCH** com a distribuiÃ§Ã£o *t* de Student, Ã© importante interpretar o parÃ¢metro $\nu$. Valores baixos de $\nu$ indicam caudas mais pesadas e maior probabilidade de eventos extremos. A comparaÃ§Ã£o dos resultados com os obtidos sob a suposiÃ§Ã£o de normalidade pode revelar se a modelagem das caudas pesadas melhora significativamente o ajuste do modelo.

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Considere que ao estimar um modelo ARCH(1) sob a suposiÃ§Ã£o de normalidade, obtemos uma log-verossimilhanÃ§a de 1000. Ao estimar o mesmo modelo com a distribuiÃ§Ã£o *t* de Student, obtemos uma log-verossimilhanÃ§a de 1050 e um valor estimado de $\nu = 4$. Isso sugere que a modelagem das caudas pesadas melhora significativamente o ajuste do modelo aos dados.
>
> ```python
> # Exemplo de valores de log-verossimilhanÃ§a
> log_likelihood_normal = 1000
> log_likelihood_t = 1050
> degrees_of_freedom = 1 # ParÃ¢metro adicional nu na distribuiÃ§Ã£o t
>
> # CÃ¡lculo da estatÃ­stica LR
> lr_statistic = 2 * (log_likelihood_t - log_likelihood_normal)
>
> print(f"EstatÃ­stica LR: {lr_statistic}")
>
> # Comparando com o valor crÃ­tico de uma distribuiÃ§Ã£o qui-quadrado
> from scipy.stats import chi2
> alpha = 0.05 # NÃ­vel de significÃ¢ncia
> critical_value = chi2.ppf(1 - alpha, degrees_of_freedom)
> print(f"Valor crÃ­tico (alpha={alpha}): {critical_value}")
>
> # Teste de hipÃ³teses
> if lr_statistic > critical_value:
>     print("Rejeitamos a hipÃ³tese nula: A distribuiÃ§Ã£o t de Student se ajusta significativamente melhor que a distribuiÃ§Ã£o normal.")
> else:
>     print("NÃ£o rejeitamos a hipÃ³tese nula: NÃ£o hÃ¡ evidÃªncias suficientes para afirmar que a distribuiÃ§Ã£o t de Student se ajusta significativamente melhor que a distribuiÃ§Ã£o normal.")
> ```
> Este cÃ³digo compara as log-verossimilhanÃ§as e realiza um teste de razÃ£o de verossimilhanÃ§as (LR) para determinar se a distribuiÃ§Ã£o *t* de Student fornece um ajuste significativamente melhor aos dados do que a distribuiÃ§Ã£o normal.

AlÃ©m disso, pode-se utilizar testes de hipÃ³teses para verificar se a distribuiÃ§Ã£o *t* de Student Ã© uma escolha melhor do que a distribuiÃ§Ã£o normal. O teste de razÃ£o de verossimilhanÃ§as (LR) pode ser utilizado para comparar os dois modelos:

$$LR = 2 (L_{t} - L_{N})$$

onde $L_{t}$ Ã© a log-verossimilhanÃ§a do modelo com a distribuiÃ§Ã£o *t* e $L_{N}$ Ã© a log-verossimilhanÃ§a do modelo com a distribuiÃ§Ã£o normal. O estatÃ­stico LR segue uma distribuiÃ§Ã£o $\chi^2$ com um grau de liberdade sob a hipÃ³tese nula de que a distribuiÃ§Ã£o normal Ã© a correta.

**Teorema 1.1:** Sob as condiÃ§Ãµes de regularidade usuais, o teste da razÃ£o de verossimilhanÃ§as (LR) para comparar um modelo ARCH com distribuiÃ§Ã£o normal e um modelo ARCH com distribuiÃ§Ã£o t de Student Ã© assintoticamente $\chi^2(1)$ sob a hipÃ³tese nula de que a distribuiÃ§Ã£o normal Ã© a verdadeira distribuiÃ§Ã£o.

*Prova:* A prova segue da teoria assintÃ³tica do teste da razÃ£o de verossimilhanÃ§as. Sob a hipÃ³tese nula de que a distribuiÃ§Ã£o normal Ã© a verdadeira distribuiÃ§Ã£o, o modelo ARCH com distribuiÃ§Ã£o normal Ã© um caso especial do modelo ARCH com distribuiÃ§Ã£o t de Student (quando $\nu \to \infty$). A estatÃ­stica LR compara a log-verossimilhanÃ§a maximizada dos dois modelos, e sob a hipÃ³tese nula, converge para uma distribuiÃ§Ã£o $\chi^2$ com graus de liberdade iguais Ã  diferenÃ§a no nÃºmero de parÃ¢metros entre os dois modelos (neste caso, 1, correspondente ao parÃ¢metro $\nu$). $\blacksquare$

### Outras DistribuiÃ§Ãµes NÃ£o Gaussianas

Embora a distribuiÃ§Ã£o *t* de Student seja uma escolha popular, outras distribuiÃ§Ãµes nÃ£o Gaussianas podem ser utilizadas para modelar as inovaÃ§Ãµes, como a distribuiÃ§Ã£o generalizada de erro (GED), a distribuiÃ§Ã£o de Skewed *t* e misturas de distribuiÃ§Ãµes normais [^21.2]. A escolha da distribuiÃ§Ã£o depende das caracterÃ­sticas especÃ­ficas dos dados e dos objetivos da modelagem.

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere a utilizaÃ§Ã£o da distribuiÃ§Ã£o GED. Ela Ã© dada por:
> $f(x; \nu) = \frac{\nu \exp(-0.5 |x/\lambda|^\nu)}{\lambda 2^{(1 + 1/\nu)} \Gamma(1/\nu)}$
> onde $\lambda = \sqrt{\frac{2^{-2/\nu}\Gamma(1/\nu)}{\Gamma(3/\nu)}}$.
> Para $\nu = 2$, a distribuiÃ§Ã£o GED se reduz Ã  distribuiÃ§Ã£o normal. Para $\nu < 2$, as caudas sÃ£o mais pesadas que a normal, e para $\nu > 2$, as caudas sÃ£o mais leves. A escolha entre a distribuiÃ§Ã£o t de Student e a GED pode depender da simetria dos dados e do quÃ£o rapidamente as caudas decaem.

**Teorema 1:** Para modelos ARCH(m) estacionÃ¡rios e ergÃ³dicos, o estimador de mÃ¡xima verossimilhanÃ§a obtido utilizando uma distribuiÃ§Ã£o nÃ£o-gaussiana para modelar as inovaÃ§Ãµes Ã© consistente e assintoticamente normal, desde que a distribuiÃ§Ã£o verdadeira pertenÃ§a Ã  famÃ­lia de distribuiÃ§Ãµes consideradas e que as condiÃ§Ãµes de regularidade usuais sejam satisfeitas.

*Prova:*
A prova segue os passos usuais da teoria de MLE.

I. **IdentificaÃ§Ã£o:** A identificaÃ§Ã£o dos parÃ¢metros depende da especificaÃ§Ã£o da distribuiÃ§Ã£o e da estrutura do modelo. Ã‰ fundamental garantir que diferentes valores dos parÃ¢metros levem a diferentes distribuiÃ§Ãµes.
II. **CondiÃ§Ãµes de Regularidade:** As condiÃ§Ãµes de regularidade incluem a continuidade e diferenciabilidade da funÃ§Ã£o de log-verossimilhanÃ§a, a existÃªncia de momentos finitos e a dominÃ¢ncia da funÃ§Ã£o.
III. **ConsistÃªncia:** Sob estas condiÃ§Ãµes, o estimador de mÃ¡xima verossimilhanÃ§a Ã© consistente, ou seja, converge em probabilidade para o verdadeiro valor do parÃ¢metro Ã  medida que o tamanho da amostra aumenta.
IV. **Normalidade AssintÃ³tica:** AlÃ©m da consistÃªncia, o estimador Ã© assintoticamente normal, com uma matriz de covariÃ¢ncia que pode ser estimada utilizando a matriz de informaÃ§Ã£o de Fisher.

A verificaÃ§Ã£o de que a distribuiÃ§Ã£o verdadeira estÃ¡ contida na famÃ­lia de distribuiÃ§Ãµes consideradas Ã© fundamental para a validade dos resultados. $\blacksquare$

**ConsideraÃ§Ãµes Importantes:**

*   **Robustez:** A escolha de uma distribuiÃ§Ã£o com caudas mais pesadas pode aumentar a robustez das estimativas em relaÃ§Ã£o a outliers.
*   **Flexibilidade:** Algumas distribuiÃ§Ãµes, como a GED, possuem um parÃ¢metro adicional que controla a espessura das caudas, permitindo uma maior flexibilidade na modelagem.
*   **Complexidade:** A utilizaÃ§Ã£o de distribuiÃ§Ãµes nÃ£o Gaussianas pode aumentar a complexidade da otimizaÃ§Ã£o numÃ©rica e exigir mais tempo computacional.

### ConclusÃ£o

A utilizaÃ§Ã£o de distribuiÃ§Ãµes nÃ£o Gaussianas, como a distribuiÃ§Ã£o *t* de Student, permite modelar as caudas pesadas observadas em muitas sÃ©ries temporais financeiras, melhorando a precisÃ£o das previsÃµes e a avaliaÃ§Ã£o de risco. A adaptaÃ§Ã£o do procedimento bÃ¡sico de MLE para acomodar essas distribuiÃ§Ãµes envolve a substituiÃ§Ã£o da densidade normal na funÃ§Ã£o de log-verossimilhanÃ§a pela densidade apropriada e a imposiÃ§Ã£o de restriÃ§Ãµes sobre os parÃ¢metros adicionais.

### ReferÃªncias

[^21.1.22]: $f(u_t) = \frac{\Gamma[(\nu + 1)/2]}{\Gamma(\nu/2) \sqrt{\pi \nu}} (\nu - 2)^{-1/2} h_t^{-1/2} \left[ 1 + \frac{u_t^2}{h_t (\nu - 2)} \right]^{-(\nu+1)/2}$. If $\nu > 2$, then $v_t$ has mean zero and varianceÂ².
[^21.2]: See Pagan and Schwert (1990), Engle and Ng (1991), and the studies cited in Bollerslev, Chou, and Kroner (1992, p. 24).
<!-- END -->