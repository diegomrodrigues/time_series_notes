## Estima√ß√£o de M√°xima Verossimilhan√ßa com Inova√ß√µes N√£o Gaussianas

### Introdu√ß√£o

Este cap√≠tulo expande a discuss√£o anterior sobre Estima√ß√£o de M√°xima Verossimilhan√ßa (MLE) em modelos **ARCH** para o caso em que a distribui√ß√£o das inova√ß√µes ($v_t$) n√£o √© Gaussiana. Embora a normalidade condicional seja uma suposi√ß√£o comum, muitas s√©ries temporais financeiras exibem caudas mais pesadas do que as permitidas pela distribui√ß√£o normal. A modelagem correta dessas caudas pesadas pode melhorar significativamente a precis√£o das previs√µes e a avalia√ß√£o de risco. Este cap√≠tulo explora como o procedimento b√°sico de MLE pode ser adaptado para acomodar outras distribui√ß√µes, com foco particular na distribui√ß√£o *t* de Student.

### Modelagem com a Distribui√ß√£o *t* de Student

Uma escolha popular para modelar as inova√ß√µes n√£o-Gaussianas √© a distribui√ß√£o *t* de Student [^21.1.22]. A distribui√ß√£o *t* de Student, com $\nu$ graus de liberdade, permite modelar caudas mais pesadas do que a distribui√ß√£o normal, tornando-a mais robusta √† presen√ßa de outliers. A densidade da distribui√ß√£o *t* de Student √© dada por [^21.1.22]:

$$f(u_t) = \frac{\Gamma[(\nu + 1)/2]}{\Gamma(\nu/2) \sqrt{\pi \nu}} (\nu - 2)^{-1/2} h_t^{-1/2} \left[ 1 + \frac{u_t^2}{h_t (\nu - 2)} \right]^{-(\nu+1)/2}$$

onde $\Gamma(\cdot)$ √© a fun√ß√£o gama. Para $\nu > 2$, a distribui√ß√£o *t* tem m√©dia zero e vari√¢ncia $\frac{\nu}{\nu-2} h_t$, onde $h_t$ √© o par√¢metro de escala [^21.1.22].

> üí° **Exemplo Num√©rico:** Uma distribui√ß√£o *t* de Student com 3 graus de liberdade ($\nu=3$) tem caudas muito mais pesadas que uma Normal. Para $\nu \to \infty$, a distribui√ß√£o *t* se aproxima da Normal. O par√¢metro $\nu$ controla o grau de *kurtosis* da distribui√ß√£o, e, por consequ√™ncia, as caudas. Um baixo valor de $\nu$ implica maior probabilidade de eventos extremos, e uma maior magnitude desses eventos.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy.stats import t, norm
>
> # Defina os graus de liberdade
> degrees_of_freedom = [3, 5, 10, 30, 100]
>
> # Gere valores x para o plot
> x = np.linspace(-5, 5, 400)
>
> # Crie a figura e os eixos
> plt.figure(figsize=(12, 8))
>
> # Plote a distribui√ß√£o t de Student para cada grau de liberdade
> for df in degrees_of_freedom:
>     plt.plot(x, t.pdf(x, df), label=f't-distribution (df={df})')
>
> # Plote a distribui√ß√£o normal padr√£o
> plt.plot(x, norm.pdf(x, 0, 1), label='Normal distribution', linestyle='--')
>
> # Adicione t√≠tulo e r√≥tulos
> plt.title('Compara√ß√£o da Distribui√ß√£o t de Student com a Distribui√ß√£o Normal')
> plt.xlabel('x')
> plt.ylabel('Densidade')
>
> # Adicione legenda
> plt.legend()
>
> # Adicione grade
> plt.grid(True)
>
> # Mostre o plot
> plt.show()
> ```
> O gr√°fico mostra como a distribui√ß√£o *t* se aproxima da distribui√ß√£o normal √† medida que os graus de liberdade aumentam. Para baixos graus de liberdade (e.g., 3), as caudas da distribui√ß√£o *t* s√£o significativamente mais pesadas, indicando uma maior probabilidade de valores extremos.

Para utilizar a distribui√ß√£o *t* de Student em modelos **ARCH**, substitu√≠mos a densidade normal na fun√ß√£o de log-verossimilhan√ßa pela densidade *t*. A fun√ß√£o de log-verossimilhan√ßa amostral condicional torna-se:

$$L(\theta) = \sum_{t=1}^{T} \log f(y_t|x_t, Y_{t-1}; \theta) = T \log \left\{ \frac{\Gamma[(\nu + 1)/2]}{\Gamma(\nu/2) \sqrt{\pi \nu}} \right\} - \frac{T}{2} \log(\nu - 2) - \frac{1}{2} \sum_{t=1}^{T} \log(h_t) - \frac{(\nu + 1)}{2} \sum_{t=1}^{T} \log \left[ 1 + \frac{(y_t - x_t'\beta)^2}{h_t (\nu - 2)} \right]$$

onde $\theta$ inclui agora o par√¢metro $\nu$, al√©m de $\beta$ e os par√¢metros do modelo **ARCH** ($\zeta, \alpha_i$). Essa fun√ß√£o √© maximizada numericamente em rela√ß√£o a $\nu$, $\beta$ e $\delta$, sujeita √† restri√ß√£o $\nu > 2$ [previous section].

> üí° **Exemplo Num√©rico:** Para demonstrar como a escolha da distribui√ß√£o afeta o valor da fun√ß√£o de log-verossimilhan√ßa, vamos comparar os valores para uma observa√ß√£o espec√≠fica sob uma distribui√ß√£o normal e uma distribui√ß√£o *t* com $\nu = 5$.
>
> Suponha que tenhamos $y_t - x_t'\beta = u_t = 0.1$, $h_t = 0.01$, e $\nu = 5$.
>
> Para a distribui√ß√£o normal, a contribui√ß√£o para a log-verossimilhan√ßa √©:
> $$ \log f_{Normal}(u_t) = -\frac{1}{2} \log(2\pi) - \frac{1}{2} \log(h_t) - \frac{1}{2} \frac{u_t^2}{h_t} = -\frac{1}{2} \log(2\pi) - \frac{1}{2} \log(0.01) - \frac{1}{2} \frac{(0.1)^2}{0.01} \approx 0.9788 $$
>
> Para a distribui√ß√£o *t* com $\nu = 5$, a contribui√ß√£o √©:
>
> ```python
> import numpy as np
> from scipy.stats import t
> from scipy.special import gamma
>
> # Valores dos par√¢metros
> nu = 5
> u_t = 0.1
> h_t = 0.01
>
> # C√°lculo da constante
> constant = gamma((nu + 1) / 2) / (gamma(nu / 2) * np.sqrt(np.pi * nu))
>
> # Densidade t
> density_t = constant * (nu - 2)**(-0.5) * h_t**(-0.5) * (1 + (u_t**2) / (h_t * (nu - 2)))**(-(nu + 1) / 2)
>
> # Log da densidade t
> log_density_t = np.log(density_t)
>
> print(f"Valor da log-densidade t: {log_density_t}")
> # Resultado: Valor da log-densidade t: 1.0497
> ```
>
> Neste caso, a distribui√ß√£o *t* atribui uma probabilidade ligeiramente maior a esse valor do que a distribui√ß√£o normal. Este efeito √© mais pronunciado para valores de $u_t$ maiores (caudas mais pesadas).
>
> Agora, considere um valor mais extremo, $u_t = 0.5$. Mantendo $h_t = 0.01$:
>
> Para a distribui√ß√£o normal:
> $$ \log f_{Normal}(u_t) = -\frac{1}{2} \log(2\pi) - \frac{1}{2} \log(h_t) - \frac{1}{2} \frac{(0.5)^2}{h_t} \approx -11.207 $$
>
> Para a distribui√ß√£o *t* com $\nu = 5$:
>
> ```python
> import numpy as np
> from scipy.stats import t
> from scipy.special import gamma
>
> # Valores dos par√¢metros
> nu = 5
> u_t = 0.5
> h_t = 0.01
>
> # C√°lculo da constante
> constant = gamma((nu + 1) / 2) / (gamma(nu / 2) * np.sqrt(np.pi * nu))
>
> # Densidade t
> density_t = constant * (nu - 2)**(-0.5) * h_t**(-0.5) * (1 + (u_t**2) / (h_t * (nu - 2)))**(-(nu + 1) / 2)
>
> # Log da densidade t
> log_density_t = np.log(density_t)
>
> print(f"Valor da log-densidade t: {log_density_t}")
> # Resultado: Valor da log-densidade t: -2.764
> ```
>
> A distribui√ß√£o *t* atribui uma probabilidade muito maior a esse valor extremo do que a distribui√ß√£o normal.

**Lema 1:** Se $u_t \sim t(\nu)$, ent√£o $E[u_t]=0$ para $\nu > 1$ e $Var[u_t] = \frac{\nu}{\nu-2}h_t$ para $\nu > 2$. Al√©m disso, o quarto momento existe se $\nu > 4$, e √© dado por $E[u_t^4] = 3h_t^2 \frac{\nu^2}{(\nu-2)(\nu-4)}$.

*Prova:* A prova segue diretamente das propriedades da distribui√ß√£o t de Student e pode ser encontrada em textos padr√£o de estat√≠stica. $\blacksquare$

### Imposi√ß√£o da Restri√ß√£o $\nu > 2$

A restri√ß√£o $\nu > 2$ √© crucial porque garante a exist√™ncia da vari√¢ncia da distribui√ß√£o *t* [^21.1.22]. Para impor essa restri√ß√£o durante a otimiza√ß√£o, podemos utilizar uma transforma√ß√£o dos par√¢metros. Uma transforma√ß√£o comum √©:

$$\nu = 2 + \exp(\nu')$$

onde $\nu'$ √© o par√¢metro a ser otimizado. Essa transforma√ß√£o garante que $\nu$ seja sempre maior que 2.

**Proposi√ß√£o 1:** A transforma√ß√£o $\nu = 2 + \exp(\nu')$ mapeia o espa√ßo de par√¢metros irrestrito $\nu' \in \mathbb{R}$ para o espa√ßo de par√¢metros restrito $\nu \in (2, \infty)$.

*Prova:* Dado que a fun√ß√£o exponencial $\exp(\nu')$ √© sempre positiva para qualquer valor de $\nu' \in \mathbb{R}$, temos que $\exp(\nu') > 0$. Adicionando 2 a ambos os lados, obtemos $2 + \exp(\nu') > 2$. Portanto, $\nu > 2$. Al√©m disso, para qualquer $\nu > 2$, existe um $\nu' = \ln(\nu - 2)$ tal que $\nu = 2 + \exp(\nu')$. Isto demonstra que a transforma√ß√£o √© uma bije√ß√£o entre $\mathbb{R}$ e $(2, \infty)$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Se $\nu' = 0$, ent√£o $\nu = 2 + \exp(0) = 2 + 1 = 3$. Se $\nu' = 1$, ent√£o $\nu = 2 + \exp(1) \approx 2 + 2.718 = 4.718$. Esta transforma√ß√£o garante que o valor de $\nu$ seja sempre maior que 2, independentemente do valor de $\nu'$ utilizado na otimiza√ß√£o.
>
> ```python
> import numpy as np
>
> # Fun√ß√£o para transformar nu' em nu
> def transform_nu_prime_to_nu(nu_prime):
>   return 2 + np.exp(nu_prime)
>
> # Testando a fun√ß√£o com diferentes valores de nu_prime
> nu_prime_values = [-1, 0, 1, 2, 5]
>
> print("nu'   |   nu")
> print("-------|-------")
> for nu_prime in nu_prime_values:
>   nu = transform_nu_prime_to_nu(nu_prime)
>   print(f"{nu_prime:5.2f} | {nu:5.2f}")
> ```
> Este c√≥digo demonstra como diferentes valores de $\nu'$ s√£o transformados em valores de $\nu$ maiores que 2.

### Otimiza√ß√£o Num√©rica

A maximiza√ß√£o da fun√ß√£o de log-verossimilhan√ßa com a distribui√ß√£o *t* de Student √© geralmente realizada numericamente. Os mesmos algoritmos de otimiza√ß√£o utilizados para a distribui√ß√£o normal podem ser aplicados, como o m√©todo de Newton-Raphson, o algoritmo BFGS e o m√©todo de *scoring* [previous section, 21.1.21]. No entanto, √© importante verificar a converg√™ncia do algoritmo e utilizar diferentes valores iniciais para garantir que o m√°ximo global seja encontrado.

**Lema 2:** A fun√ß√£o de log-verossimilhan√ßa com distribui√ß√£o t de Student √© diferenci√°vel em rela√ß√£o a todos os seus par√¢metros, incluindo $\nu$, desde que $\nu > 2$.

*Prova:* A diferenciabilidade da fun√ß√£o de log-verossimilhan√ßa segue da diferenciabilidade da fun√ß√£o gama e da fun√ß√£o logaritmo. A derivada em rela√ß√£o a $\nu$ envolve a fun√ß√£o digamma (derivada da fun√ß√£o gama), que √© bem definida para $\nu > 2$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Considere que estamos otimizando a log-verossimilhan√ßa utilizando o algoritmo BFGS. Iniciamos a otimiza√ß√£o com um valor inicial de $\nu' = 0$, o que corresponde a $\nu = 3$. Ap√≥s algumas itera√ß√µes, o algoritmo converge para $\nu' = 1.0986$, o que corresponde a $\nu \approx 5$. √â importante verificar se a fun√ß√£o de log-verossimilhan√ßa aumenta a cada itera√ß√£o e se o algoritmo converge para um valor est√°vel.

### Interpreta√ß√£o dos Resultados

Ao estimar um modelo **ARCH** com a distribui√ß√£o *t* de Student, √© importante interpretar o par√¢metro $\nu$. Valores baixos de $\nu$ indicam caudas mais pesadas e maior probabilidade de eventos extremos. A compara√ß√£o dos resultados com os obtidos sob a suposi√ß√£o de normalidade pode revelar se a modelagem das caudas pesadas melhora significativamente o ajuste do modelo.

> üí° **Exemplo Num√©rico:**
> Considere que ao estimar um modelo ARCH(1) sob a suposi√ß√£o de normalidade, obtemos uma log-verossimilhan√ßa de 1000. Ao estimar o mesmo modelo com a distribui√ß√£o *t* de Student, obtemos uma log-verossimilhan√ßa de 1050 e um valor estimado de $\nu = 4$. Isso sugere que a modelagem das caudas pesadas melhora significativamente o ajuste do modelo aos dados.
>
> ```python
> # Exemplo de valores de log-verossimilhan√ßa
> log_likelihood_normal = 1000
> log_likelihood_t = 1050
> degrees_of_freedom = 1 # Par√¢metro adicional nu na distribui√ß√£o t
>
> # C√°lculo da estat√≠stica LR
> lr_statistic = 2 * (log_likelihood_t - log_likelihood_normal)
>
> print(f"Estat√≠stica LR: {lr_statistic}")
>
> # Comparando com o valor cr√≠tico de uma distribui√ß√£o qui-quadrado
> from scipy.stats import chi2
> alpha = 0.05 # N√≠vel de signific√¢ncia
> critical_value = chi2.ppf(1 - alpha, degrees_of_freedom)
> print(f"Valor cr√≠tico (alpha={alpha}): {critical_value}")
>
> # Teste de hip√≥teses
> if lr_statistic > critical_value:
>     print("Rejeitamos a hip√≥tese nula: A distribui√ß√£o t de Student se ajusta significativamente melhor que a distribui√ß√£o normal.")
> else:
>     print("N√£o rejeitamos a hip√≥tese nula: N√£o h√° evid√™ncias suficientes para afirmar que a distribui√ß√£o t de Student se ajusta significativamente melhor que a distribui√ß√£o normal.")
> ```
> Este c√≥digo compara as log-verossimilhan√ßas e realiza um teste de raz√£o de verossimilhan√ßas (LR) para determinar se a distribui√ß√£o *t* de Student fornece um ajuste significativamente melhor aos dados do que a distribui√ß√£o normal.

Al√©m disso, pode-se utilizar testes de hip√≥teses para verificar se a distribui√ß√£o *t* de Student √© uma escolha melhor do que a distribui√ß√£o normal. O teste de raz√£o de verossimilhan√ßas (LR) pode ser utilizado para comparar os dois modelos:

$$LR = 2 (L_{t} - L_{N})$$

onde $L_{t}$ √© a log-verossimilhan√ßa do modelo com a distribui√ß√£o *t* e $L_{N}$ √© a log-verossimilhan√ßa do modelo com a distribui√ß√£o normal. O estat√≠stico LR segue uma distribui√ß√£o $\chi^2$ com um grau de liberdade sob a hip√≥tese nula de que a distribui√ß√£o normal √© a correta.

**Teorema 1.1:** Sob as condi√ß√µes de regularidade usuais, o teste da raz√£o de verossimilhan√ßas (LR) para comparar um modelo ARCH com distribui√ß√£o normal e um modelo ARCH com distribui√ß√£o t de Student √© assintoticamente $\chi^2(1)$ sob a hip√≥tese nula de que a distribui√ß√£o normal √© a verdadeira distribui√ß√£o.

*Prova:* A prova segue da teoria assint√≥tica do teste da raz√£o de verossimilhan√ßas. Sob a hip√≥tese nula de que a distribui√ß√£o normal √© a verdadeira distribui√ß√£o, o modelo ARCH com distribui√ß√£o normal √© um caso especial do modelo ARCH com distribui√ß√£o t de Student (quando $\nu \to \infty$). A estat√≠stica LR compara a log-verossimilhan√ßa maximizada dos dois modelos, e sob a hip√≥tese nula, converge para uma distribui√ß√£o $\chi^2$ com graus de liberdade iguais √† diferen√ßa no n√∫mero de par√¢metros entre os dois modelos (neste caso, 1, correspondente ao par√¢metro $\nu$). $\blacksquare$

### Outras Distribui√ß√µes N√£o Gaussianas

Embora a distribui√ß√£o *t* de Student seja uma escolha popular, outras distribui√ß√µes n√£o Gaussianas podem ser utilizadas para modelar as inova√ß√µes, como a distribui√ß√£o generalizada de erro (GED), a distribui√ß√£o de Skewed *t* e misturas de distribui√ß√µes normais [^21.2]. A escolha da distribui√ß√£o depende das caracter√≠sticas espec√≠ficas dos dados e dos objetivos da modelagem.

> üí° **Exemplo Num√©rico:** Considere a utiliza√ß√£o da distribui√ß√£o GED. Ela √© dada por:
> $f(x; \nu) = \frac{\nu \exp(-0.5 |x/\lambda|^\nu)}{\lambda 2^{(1 + 1/\nu)} \Gamma(1/\nu)}$
> onde $\lambda = \sqrt{\frac{2^{-2/\nu}\Gamma(1/\nu)}{\Gamma(3/\nu)}}$.
> Para $\nu = 2$, a distribui√ß√£o GED se reduz √† distribui√ß√£o normal. Para $\nu < 2$, as caudas s√£o mais pesadas que a normal, e para $\nu > 2$, as caudas s√£o mais leves. A escolha entre a distribui√ß√£o t de Student e a GED pode depender da simetria dos dados e do qu√£o rapidamente as caudas decaem.

**Teorema 1:** Para modelos ARCH(m) estacion√°rios e erg√≥dicos, o estimador de m√°xima verossimilhan√ßa obtido utilizando uma distribui√ß√£o n√£o-gaussiana para modelar as inova√ß√µes √© consistente e assintoticamente normal, desde que a distribui√ß√£o verdadeira perten√ßa √† fam√≠lia de distribui√ß√µes consideradas e que as condi√ß√µes de regularidade usuais sejam satisfeitas.

*Prova:*
A prova segue os passos usuais da teoria de MLE.

I. **Identifica√ß√£o:** A identifica√ß√£o dos par√¢metros depende da especifica√ß√£o da distribui√ß√£o e da estrutura do modelo. √â fundamental garantir que diferentes valores dos par√¢metros levem a diferentes distribui√ß√µes.
II. **Condi√ß√µes de Regularidade:** As condi√ß√µes de regularidade incluem a continuidade e diferenciabilidade da fun√ß√£o de log-verossimilhan√ßa, a exist√™ncia de momentos finitos e a domin√¢ncia da fun√ß√£o.
III. **Consist√™ncia:** Sob estas condi√ß√µes, o estimador de m√°xima verossimilhan√ßa √© consistente, ou seja, converge em probabilidade para o verdadeiro valor do par√¢metro √† medida que o tamanho da amostra aumenta.
IV. **Normalidade Assint√≥tica:** Al√©m da consist√™ncia, o estimador √© assintoticamente normal, com uma matriz de covari√¢ncia que pode ser estimada utilizando a matriz de informa√ß√£o de Fisher.

A verifica√ß√£o de que a distribui√ß√£o verdadeira est√° contida na fam√≠lia de distribui√ß√µes consideradas √© fundamental para a validade dos resultados. $\blacksquare$

**Considera√ß√µes Importantes:**

*   **Robustez:** A escolha de uma distribui√ß√£o com caudas mais pesadas pode aumentar a robustez das estimativas em rela√ß√£o a outliers.
*   **Flexibilidade:** Algumas distribui√ß√µes, como a GED, possuem um par√¢metro adicional que controla a espessura das caudas, permitindo uma maior flexibilidade na modelagem.
*   **Complexidade:** A utiliza√ß√£o de distribui√ß√µes n√£o Gaussianas pode aumentar a complexidade da otimiza√ß√£o num√©rica e exigir mais tempo computacional.

### Conclus√£o

A utiliza√ß√£o de distribui√ß√µes n√£o Gaussianas, como a distribui√ß√£o *t* de Student, permite modelar as caudas pesadas observadas em muitas s√©ries temporais financeiras, melhorando a precis√£o das previs√µes e a avalia√ß√£o de risco. A adapta√ß√£o do procedimento b√°sico de MLE para acomodar essas distribui√ß√µes envolve a substitui√ß√£o da densidade normal na fun√ß√£o de log-verossimilhan√ßa pela densidade apropriada e a imposi√ß√£o de restri√ß√µes sobre os par√¢metros adicionais.

### Refer√™ncias

[^21.1.22]: $f(u_t) = \frac{\Gamma[(\nu + 1)/2]}{\Gamma(\nu/2) \sqrt{\pi \nu}} (\nu - 2)^{-1/2} h_t^{-1/2} \left[ 1 + \frac{u_t^2}{h_t (\nu - 2)} \right]^{-(\nu+1)/2}$. If $\nu > 2$, then $v_t$ has mean zero and variance¬≤.
[^21.2]: See Pagan and Schwert (1990), Engle and Ng (1991), and the studies cited in Bollerslev, Chou, and Kroner (1992, p. 24).
<!-- END -->