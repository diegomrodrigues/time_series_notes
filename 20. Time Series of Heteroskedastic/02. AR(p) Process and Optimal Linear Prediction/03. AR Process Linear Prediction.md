## Previs√£o Linear √ìtima em Modelos AR(p)

### Introdu√ß√£o

Este cap√≠tulo aprofunda a an√°lise da **previs√£o linear √≥tima** em **modelos Autoregressivos de ordem *p* (AR(*p*))**, com √™nfase na sua deriva√ß√£o e propriedades, complementando os conceitos previamente apresentados [^1]. Exploraremos em detalhes como a proje√ß√£o linear de $y_t$ sobre seus valores passados minimiza o erro quadr√°tico m√©dio da previs√£o e como a heteroscedasticidade afeta essa proje√ß√£o.

### Deriva√ß√£o da Previs√£o Linear √ìtima

Como estabelecido no cap√≠tulo anterior, a melhor previs√£o linear do n√≠vel de $y_t$ para um processo AR(*p*) √© dada por [^1]:

$$ \hat{E}(y_t | y_{t-1}, y_{t-2}, \dots) = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} $$

Essa express√£o representa a proje√ß√£o linear de $y_t$ sobre uma constante e os valores passados de $y$, minimizando o erro quadr√°tico m√©dio da previs√£o. Para demonstrar formalmente que essa proje√ß√£o √© √≥tima, consideramos a seguinte abordagem:

Seja $\hat{y}_t$ uma previs√£o linear gen√©rica de $y_t$ baseada nos valores passados, definida como:

$$ \hat{y}_t = a_0 + a_1 y_{t-1} + a_2 y_{t-2} + \dots + a_p y_{t-p} $$

Nosso objetivo √© encontrar os coeficientes $a_0, a_1, a_2, \dots, a_p$ que minimizem o erro quadr√°tico m√©dio (MSE) da previs√£o, dado por:

$$ MSE = E[(y_t - \hat{y}_t)^2] $$

Para minimizar o MSE, calculamos as derivadas parciais em rela√ß√£o a cada coeficiente $a_i$ e igualamos a zero:

$$ \frac{\partial MSE}{\partial a_0} = -2 E[y_t - (a_0 + a_1 y_{t-1} + a_2 y_{t-2} + \dots + a_p y_{t-p})] = 0 $$

$$ \frac{\partial MSE}{\partial a_i} = -2 E[(y_t - (a_0 + a_1 y_{t-1} + a_2 y_{t-2} + \dots + a_p y_{t-p}))y_{t-i}] = 0, \quad i = 1, 2, \dots, p $$

Essas equa√ß√µes formam um sistema de *p* + 1 equa√ß√µes lineares nas *p* + 1 inc√≥gnitas $a_0, a_1, a_2, \dots, a_p$. Resolvendo este sistema, encontramos os valores dos coeficientes que minimizam o MSE.

> üí° **Exemplo Num√©rico:** Vamos considerar um modelo AR(1) simples: $y_t = a_0 + a_1 y_{t-1} + u_t$. Queremos encontrar os valores de $a_0$ e $a_1$ que minimizem o MSE. As equa√ß√µes de otimalidade s√£o:
>
> $\frac{\partial MSE}{\partial a_0} = -2 E[y_t - (a_0 + a_1 y_{t-1})] = 0$
>
> $\frac{\partial MSE}{\partial a_1} = -2 E[(y_t - (a_0 + a_1 y_{t-1}))y_{t-1}] = 0$
>
> Resolvendo este sistema, obtemos:
>
> $E[y_t] = a_0 + a_1 E[y_{t-1}]$
>
> $E[y_t y_{t-1}] = a_0 E[y_{t-1}] + a_1 E[y_{t-1}^2]$
>
> Suponha que temos os seguintes momentos amostrais (estimados a partir dos dados):
>
> $E[y_t] = 10$
>
> $E[y_t y_{t-1}] = 120$
>
> $E[y_{t-1}] = 10$
>
> $E[y_{t-1}^2] = 110$
>
> Substituindo esses valores nas equa√ß√µes, obtemos:
>
> $10 = a_0 + 10 a_1$
>
> $120 = 10 a_0 + 110 a_1$
>
> Resolvendo este sistema de equa√ß√µes lineares, encontramos:
>
> $a_0 = -1$
>
> $a_1 = 1.1$
>
> Portanto, a previs√£o linear √≥tima para este modelo AR(1) √©: $\hat{y}_t = -1 + 1.1 y_{t-1}$

Agora, substitu√≠mos $y_t$ pela sua express√£o no modelo AR(*p*):

$$ y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + u_t $$

Reescrevemos as equa√ß√µes de otimalidade:

$$ E[c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + u_t - (a_0 + a_1 y_{t-1} + a_2 y_{t-2} + \dots + a_p y_{t-p})] = 0 $$

$$ E[(c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + u_t - (a_0 + a_1 y_{t-1} + a_2 y_{t-2} + \dots + a_p y_{t-p}))y_{t-i}] = 0, \quad i = 1, 2, \dots, p $$

Usando a propriedade de que $E[u_t] = 0$ e $E[u_t y_{t-i}] = 0$ para $i > 0$ (j√° que $u_t$ √© ru√≠do branco e n√£o correlacionado com os valores passados de $y$), simplificamos as equa√ß√µes:

$$ c - a_0 + (\phi_1 - a_1) E[y_{t-1}] + (\phi_2 - a_2) E[y_{t-2}] + \dots + (\phi_p - a_p) E[y_{t-p}] = 0 $$

$$ E[(c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} - (a_0 + a_1 y_{t-1} + a_2 y_{t-2} + \dots + a_p y_{t-p}))y_{t-i}] = 0, \quad i = 1, 2, \dots, p $$

Para que estas equa√ß√µes sejam satisfeitas, devemos ter:

$$ a_0 = c $$

$$ a_i = \phi_i, \quad i = 1, 2, \dots, p $$

Portanto, a previs√£o linear √≥tima √© dada por:

$$ \hat{y}_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} $$

que √© exatamente a proje√ß√£o linear de $y_t$ sobre seus valores passados, como apresentado inicialmente [^1]. Este resultado demonstra formalmente que a proje√ß√£o linear minimiza o MSE e, portanto, √© a melhor previs√£o linear.

**Teorema 5**: A proje√ß√£o linear $\hat{E}(y_t | y_{t-1}, y_{t-2}, \dots)$ √© a melhor previs√£o linear no sentido de que minimiza o erro quadr√°tico m√©dio (MSE) entre $y_t$ e qualquer outra previs√£o linear baseada em $y_{t-1}, y_{t-2}, \dots$.

*Proof:* Seja $g(y_{t-1}, y_{t-2}, \dots)$ qualquer outra previs√£o linear de $y_t$. Precisamos mostrar que $E[(y_t - \hat{E}(y_t | y_{t-1}, y_{t-2}, \dots))^2] \le E[(y_t - g(y_{t-1}, y_{t-2}, \dots))^2]$.

I.  Defina $e_t = y_t - \hat{E}(y_t | y_{t-1}, y_{t-2}, \dots)$ como o erro da proje√ß√£o linear √≥tima.
II.  Por defini√ß√£o da proje√ß√£o linear, $E[e_t | y_{t-1}, y_{t-2}, \dots] = 0$ e $E[e_t y_{t-i}] = 0$ para todo $i > 0$.
III.  Agora, considere qualquer outra previs√£o $g(y_{t-1}, y_{t-2}, \dots)$. Podemos escrever:
    $E[(y_t - g(y_{t-1}, y_{t-2}, \dots))^2] = E[(y_t - \hat{E}(y_t | y_{t-1}, y_{t-2}, \dots) + \hat{E}(y_t | y_{t-1}, y_{t-2}, \dots) - g(y_{t-1}, y_{t-2}, \dots))^2]$
IV. Expandindo o quadrado:
    $E[(y_t - g(y_{t-1}, y_{t-2}, \dots))^2] = E[e_t^2] + E[(\hat{E}(y_t | y_{t-1}, y_{t-2}, \dots) - g(y_{t-1}, y_{t-2}, \dots))^2] + 2E[e_t(\hat{E}(y_t | y_{t-1}, y_{t-2}, \dots) - g(y_{t-1}, y_{t-2}, \dots))]$
V. O termo cruzado √© zero, porque:
   $E[e_t(\hat{E}(y_t | y_{t-1}, y_{t-2}, \dots) - g(y_{t-1}, y_{t-2}, \dots))] = E[E[e_t(\hat{E}(y_t | y_{t-1}, y_{t-2}, \dots) - g(y_{t-1}, y_{t-2}, \dots)) | y_{t-1}, y_{t-2}, \dots]] = E[(\hat{E}(y_t | y_{t-1}, y_{t-2}, \dots) - g(y_{t-1}, y_{t-2}, \dots))E[e_t | y_{t-1}, y_{t-2}, \dots]] = 0$
VI. Portanto:
    $E[(y_t - g(y_{t-1}, y_{t-2}, \dots))^2] = E[e_t^2] + E[(\hat{E}(y_t | y_{t-1}, y_{t-2}, \dots) - g(y_{t-1}, y_{t-2}, \dots))^2]$
VII. Como $E[(\hat{E}(y_t | y_{t-1}, y_{t-2}, \dots) - g(y_{t-1}, y_{t-2}, \dots))^2] \geq 0$, temos:
     $E[(y_t - g(y_{t-1}, y_{t-2}, \dots))^2] \geq E[e_t^2] = E[(y_t - \hat{E}(y_t | y_{t-1}, y_{t-2}, \dots))^2]$
Isto demonstra que o MSE da proje√ß√£o linear √≥tima √© menor ou igual ao MSE de qualquer outra previs√£o linear, provando o teorema. $\blacksquare$

**Teorema 5.1:** A proje√ß√£o linear $\hat{E}(y_t | y_{t-1}, y_{t-2}, \dots, y_{t-p})$ coincide com a esperan√ßa condicional $E(y_t | y_{t-1}, y_{t-2}, \dots, y_{t-p})$ se e somente se o processo $\{y_t\}$ √© linear e Gaussiano.

*Proof:*

$(\Rightarrow)$ Se o processo $\{y_t\}$ √© linear e Gaussiano, ent√£o a esperan√ßa condicional $E(y_t | y_{t-1}, y_{t-2}, \dots, y_{t-p})$ √© uma fun√ß√£o linear dos valores passados $y_{t-1}, y_{t-2}, \dots, y_{t-p}$. Portanto, a proje√ß√£o linear $\hat{E}(y_t | y_{t-1}, y_{t-2}, \dots, y_{t-p})$ coincide com a esperan√ßa condicional.

$(\Leftarrow)$ Reciprocamente, se a proje√ß√£o linear $\hat{E}(y_t | y_{t-1}, y_{t-2}, \dots, y_{t-p})$ coincide com a esperan√ßa condicional $E(y_t | y_{t-1}, y_{t-2}, \dots, y_{t-p})$, ent√£o a rela√ß√£o entre $y_t$ e seus valores passados √© linear. Al√©m disso, se o processo for Gaussiano, ent√£o a distribui√ß√£o condicional de $y_t$ dado $y_{t-1}, y_{t-2}, \dots, y_{t-p}$ √© tamb√©m Gaussiana, e a esperan√ßa condicional √© a melhor previs√£o no sentido de minimizar o MSE.

Portanto, a proje√ß√£o linear coincide com a esperan√ßa condicional se e somente se o processo √© linear e Gaussiano. $\blacksquare$

### Heteroscedasticidade e a Previs√£o Linear √ìtima

Embora a proje√ß√£o linear seja √≥tima no sentido de minimizar o MSE, a presen√ßa de heteroscedasticidade no termo de erro $u_t$ pode afetar a efici√™ncia da previs√£o.  Em um modelo com heteroscedasticidade condicional, a vari√¢ncia de $u_t$ depende dos valores passados da s√©rie temporal:

$$ Var(u_t | y_{t-1}, y_{t-2}, \dots) = \sigma_t^2 $$

Nesse caso, a proje√ß√£o linear ainda √© a melhor previs√£o linear, mas n√£o √© a melhor previs√£o no sentido geral, ou seja, n√£o minimiza o MSE entre todas as poss√≠veis fun√ß√µes de $y_{t-1}, y_{t-2}, ...$. Modelos que levam em conta a heteroscedasticidade condicional, como os modelos ARCH e GARCH (que ser√£o explorados em cap√≠tulos subsequentes), podem fornecer previs√µes mais precisas, capturando a vari√¢ncia vari√°vel ao longo do tempo.

**Teorema 6**: Se $Var(u_t | y_{t-1}, y_{t-2}, \dots) = \sigma_t^2$ (heteroscedasticidade condicional), ent√£o a proje√ß√£o linear $\hat{E}(y_t | y_{t-1}, y_{t-2}, \dots)$ continua sendo a melhor previs√£o linear, mas n√£o necessariamente a melhor previs√£o (n√£o linear).

**Prova:** A demonstra√ß√£o segue os passos do Teorema 5, mas reconhecendo que a vari√¢ncia condicional do erro $e_t = u_t$ agora varia com os valores passados: $Var(e_t | y_{t-1}, y_{t-2}, \dots) = \sigma_t^2$. A proje√ß√£o linear ainda garante ortogonalidade do erro aos regressores, minimizando o MSE dentro da classe de previs√µes lineares. No entanto, uma previs√£o n√£o linear que modele a vari√¢ncia condicional pode reduzir o MSE ainda mais.

Suponha que tenhamos uma previs√£o n√£o linear $g(y_{t-1}, y_{t-2}, \dots)$. Podemos escrever:

$$E[(y_t - g(y_{t-1}, y_{t-2}, \dots))^2] = E[E[(y_t - g(y_{t-1}, y_{t-2}, \dots))^2 | y_{t-1}, y_{t-2}, \dots]]$$

Para minimizar essa express√£o, √© necess√°rio minimizar a esperan√ßa condicional $E[(y_t - g(y_{t-1}, y_{t-2}, \dots))^2 | y_{t-1}, y_{t-2}, \dots]$.

Se $g(y_{t-1}, y_{t-2}, \dots)$ for igual a $E[y_t | y_{t-1}, y_{t-2}, \dots]$, que √© a esperan√ßa condicional completa, ent√£o o MSE ser√° minimizado. No entanto, a proje√ß√£o linear $\hat{E}(y_t | y_{t-1}, y_{t-2}, \dots)$ √© uma aproxima√ß√£o linear da esperan√ßa condicional completa, e s√≥ coincide com ela se a rela√ß√£o entre $y_t$ e seus valores passados for estritamente linear.

Se a rela√ß√£o n√£o for estritamente linear (devido √† heteroscedasticidade condicional), ent√£o uma previs√£o n√£o linear pode ser melhor do que a proje√ß√£o linear. $\blacksquare$

**Corol√°rio 6.1**: Em um modelo AR(p) com heteroscedasticidade condicional, a vari√¢ncia do erro de previs√£o √© vari√°vel no tempo e pode ser modelada usando modelos ARCH ou GARCH.

> üí° **Exemplo Num√©rico:** Suponha que os retornos de uma a√ß√£o sigam um modelo AR(1) com heteroscedasticidade condicional, definido como:
>
> $y_t = 0.01 + 0.5 y_{t-1} + u_t$
>
> $u_t = \sigma_t \epsilon_t$, onde $\epsilon_t \sim N(0, 1)$ e $\sigma_t^2 = 0.0001 + 0.8 u_{t-1}^2$ (ARCH(1))
>
> Durante um per√≠odo de baixa volatilidade, $u_{t-1}$ pode ser pequeno (por exemplo, 0.01), ent√£o $\sigma_t^2 \approx 0.0001 + 0.8 * (0.01)^2 = 0.0001008$.
>
> Durante um per√≠odo de alta volatilidade, $u_{t-1}$ pode ser grande (por exemplo, 0.1), ent√£o $\sigma_t^2 \approx 0.0001 + 0.8 * (0.1)^2 = 0.0081$.
>
> A proje√ß√£o linear $\hat{E}(y_t | y_{t-1}) = 0.01 + 0.5 y_{t-1}$ fornecer√° uma previs√£o do retorno m√©dio, mas n√£o capturar√° a mudan√ßa na vari√¢ncia. Um modelo ARCH(1) capturaria a vari√¢ncia vari√°vel, fornecendo uma melhor medida de risco.

### Implementa√ß√£o Pr√°tica da Previs√£o Linear √ìtima

Na pr√°tica, a implementa√ß√£o da previs√£o linear √≥tima envolve estimar os par√¢metros do modelo AR(*p*) (os coeficientes $\phi_i$ e a constante *c*) usando dados hist√≥ricos e, em seguida, usar esses par√¢metros estimados para calcular a previs√£o. A escolha do m√©todo de estima√ß√£o (m√≠nimos quadrados ordin√°rios, m√°xima verossimilhan√ßa) pode afetar a precis√£o da previs√£o, especialmente em amostras pequenas ou na presen√ßa de outliers.

> üí° **Exemplo:** Suponha que voc√™ tenha estimado um modelo AR(2) para prever as vendas de uma empresa com base nos dados dos √∫ltimos 5 anos.  Com os coeficientes estimados ($\hat{c}, \hat{\phi_1}, \hat{\phi_2}$) e os dois valores mais recentes das vendas ($y_{t-1}, y_{t-2}$), voc√™ pode calcular a previs√£o para o pr√≥ximo per√≠odo: $\hat{y}_t = \hat{c} + \hat{\phi_1} y_{t-1} + \hat{\phi_2} y_{t-2}$.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> from statsmodels.tsa.arima.model import ARIMA
> import matplotlib.pyplot as plt
>
> # Sample Sales Data (replace with your actual data)
> np.random.seed(42)
> n = 100
> trend = np.linspace(100, 150, n)
> noise = np.random.randn(n) * 5  # Add some noise
> sales = trend + noise
>
> # Fit an AR(2) model
> model = ARIMA(sales, order=(2, 0, 0))  # AR(2) - (p, d, q)
> results = model.fit()
>
> # Print model summary
> print(results.summary())
>
> # Extract parameters
> c = results.params[0]  # constant (intercept)
> phi1 = results.params[1] # AR(1) coefficient
> phi2 = results.params[2] # AR(2) coefficient
>
> # Obtain the last two values of sales
> y_t_minus_1 = sales[-1]  # Most recent value
> y_t_minus_2 = sales[-2]  # Second most recent value
>
> # Calculate the forecast
> y_hat = c + phi1 * y_t_minus_1 + phi2 * y_t_minus_2
>
> print("Parameters: c={}, phi1={}, phi2={}".format(c, phi1, phi2))
> print("Last two sales values: y_t_minus_1={}, y_t_minus_2={}".format(y_t_minus_1, y_t_minus_2))
> print("Forecasted sales:", y_hat)
>
> # Plot the historical data and the forecast
> plt.figure(figsize=(10, 6))
> plt.plot(sales, label="Historical Sales Data")
> plt.scatter(n, y_hat, color='red', marker='o', label="Forecast")
> plt.title("AR(2) Forecast")
> plt.xlabel("Time")
> plt.ylabel("Sales")
> plt.legend()
> plt.grid(True)
> plt.show()
> ```

Al√©m disso, a escolha da ordem *p* do modelo AR(*p*) √© crucial para o desempenho da previs√£o.  A sele√ß√£o da ordem pode ser baseada em crit√©rios de informa√ß√£o como AIC (Akaike Information Criterion) ou BIC (Bayesian Information Criterion), que penalizam a complexidade do modelo e ajudam a evitar o overfitting.

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal simulada. Ajustamos modelos AR(1), AR(2) e AR(3) e comparamos seus AIC e BIC.
>
> ```python
> import numpy as np
> import pandas as pd
> from statsmodels.tsa.arima.model import ARIMA
> import matplotlib.pyplot as plt
>
> # Generate AR(2) data
> np.random.seed(42)
> n = 200
> errors = np.random.randn(n)
> y = np.zeros(n)
> phi1 = 0.6
> phi2 = 0.3
>
> for t in range(2, n):
>     y[t] = phi1 * y[t-1] + phi2 * y[t-2] + errors[t]
>
> # Fit different AR models
> models = {}
> aic = {}
> bic = {}
>
> for p in range(1, 4):
>     model = ARIMA(y, order=(p, 0, 0))
>     results = model.fit()
>     models[p] = results
>     aic[p] = results.aic
>     bic[p] = results.bic
>
> # Print AIC and BIC
> print("AIC values:", aic)
> print("BIC values:", bic)
>
> # Plot AIC and BIC
> p_values = list(aic.keys())
> aic_values = list(aic.values())
> bic_values = list(bic.values())
>
> plt.figure(figsize=(10, 6))
> plt.plot(p_values, aic_values, marker='o', label='AIC')
> plt.plot(p_values, bic_values, marker='x', label='BIC')
> plt.xticks(p_values)
> plt.xlabel('Order (p)')
> plt.ylabel('Information Criterion Value')
> plt.title('AIC and BIC for Different AR Orders')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> Ao analisar os valores de AIC e BIC, escolher√≠amos o modelo com o menor valor. Neste caso, o BIC pode favorecer um modelo mais simples (AR(1) ou AR(2)) em compara√ß√£o com o AIC, dependendo dos valores num√©ricos obtidos.

**Proposi√ß√£o 7:** Minimizar AIC ou BIC √© assintoticamente equivalente a minimizar o erro de previs√£o fora da amostra, sob certas condi√ß√µes de regularidade.

*Discuss√£o:* Crit√©rios como AIC e BIC adicionam uma penalidade √† verossimilhan√ßa do modelo com base no n√∫mero de par√¢metros. Essa penalidade visa aproximar a complexidade do modelo √† sua capacidade de generaliza√ß√£o para dados n√£o vistos. A minimiza√ß√£o desses crit√©rios busca um equil√≠brio entre o ajuste aos dados da amostra e a capacidade de prever dados futuros. Em geral, BIC tende a selecionar modelos mais simples do que AIC, especialmente em amostras grandes.

### Conclus√£o

Este cap√≠tulo forneceu uma an√°lise detalhada da previs√£o linear √≥tima em modelos AR(*p*), demonstrando formalmente a sua propriedade de minimizar o MSE. Tamb√©m discutimos como a heteroscedasticidade condicional pode limitar a efici√™ncia da previs√£o linear, motivando o uso de modelos mais sofisticados que capturam a vari√¢ncia vari√°vel no tempo. A combina√ß√£o dos conceitos discutidos neste cap√≠tulo com os apresentados nos cap√≠tulos anteriores fornece uma base s√≥lida para a an√°lise e modelagem de s√©ries temporais heterosced√°sticas.

### Refer√™ncias
[^1]: P√°gina 657.
<!-- END -->