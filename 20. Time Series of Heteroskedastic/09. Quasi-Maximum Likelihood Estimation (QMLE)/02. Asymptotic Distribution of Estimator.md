## Normalidade Assint√≥tica e Infer√™ncia em QMLE

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre a Quase M√°xima Verossimilhan√ßa (QMLE) e a consist√™ncia dos estimadores sob condi√ß√µes de heteroscedasticidade, este cap√≠tulo se aprofunda na distribui√ß√£o assint√≥tica dos estimadores QMLE e nas implica√ß√µes para a infer√™ncia estat√≠stica. Em particular, exploraremos como, sob certas condi√ß√µes de regularidade, o estimador QMLE converge para uma distribui√ß√£o normal, permitindo a constru√ß√£o de intervalos de confian√ßa e testes de hip√≥teses [^663].

### Conceitos Fundamentais
Como vimos anteriormente, a QMLE √© um m√©todo de estima√ß√£o que maximiza a fun√ß√£o de log-verossimilhan√ßa gaussiana, mesmo quando a distribui√ß√£o verdadeira dos erros n√£o √© normal. Sob certas condi√ß√µes, como a correta especifica√ß√£o do modelo de vari√¢ncia condicional e as condi√ß√µes sobre os res√≠duos padronizados ($E(v_t|x_t, \mathcal{Y}_{t-1}) = 0$ e $E(v_t^2|x_t, \mathcal{Y}_{t-1}) = 1$), a QMLE fornece estimativas consistentes dos par√¢metros [^663].

No entanto, a consist√™ncia √© apenas uma propriedade assint√≥tica, e para realizar infer√™ncia estat√≠stica (por exemplo, construir intervalos de confian√ßa ou realizar testes de hip√≥teses), precisamos conhecer a distribui√ß√£o assint√≥tica dos estimadores. O resultado fundamental neste contexto √© que, sob condi√ß√µes de regularidade adicionais, o estimador QMLE √© assintoticamente normal [^663].

**Teorema da Normalidade Assint√≥tica da QMLE:**

Sob condi√ß√µes de regularidade apropriadas e supondo que $E(|v_t|^{2+\delta}|x_t, \mathcal{Y}_{t-1}) < \infty$ para algum $\delta > 0$, temos que [^663]:

$$\sqrt{T}(\hat{\theta} - \theta) \xrightarrow{d} N(0, D^{-1}SD^{-1})$$

onde:

*   $\hat{\theta}$ √© o estimador QMLE dos par√¢metros $\theta$.
*   $\theta$ √© o verdadeiro valor dos par√¢metros.
*   $T$ √© o tamanho da amostra.
*   $\xrightarrow{d}$ denota converg√™ncia em distribui√ß√£o.
*   $N(0, D^{-1}SD^{-1})$ representa uma distribui√ß√£o normal com m√©dia zero e matriz de covari√¢ncia $D^{-1}SD^{-1}$.
*   $S = \text{plim } T^{-1} \sum_{t=1}^T [s_t(\theta)][s_t(\theta)]'$ √© a matriz de escore.
*   $D = \text{plim } T^{-1} \sum_{t=1}^T -E[\frac{\partial s_t(\theta)}{\partial \theta}]$ √© a matriz de informa√ß√£o [^663].

*Prova (Resumo):*

Este teorema pode ser provado usando o Teorema do Limite Central para martingales. O vetor de escore, quando normalizado por $\sqrt{T}$, converge em distribui√ß√£o para uma distribui√ß√£o normal com m√©dia zero e matriz de covari√¢ncia $D^{-1}SD^{-1}$. A condi√ß√£o de momento finito garante que as condi√ß√µes de Lindeberg para o Teorema do Limite Central sejam satisfeitas. $\blacksquare$

Para fornecer uma prova mais detalhada, podemos expandir o resumo fornecido:

*Prova (Detalhada):*

Para provar o Teorema da Normalidade Assint√≥tica da QMLE, seguiremos os seguintes passos:

I. **Definir o vetor de escore:** O vetor de escore √© definido como a derivada da fun√ß√£o de log-verossimilhan√ßa em rela√ß√£o aos par√¢metros $\theta$:

   $$s_t(\theta) = \frac{\partial q_t(\theta)}{\partial \theta}$$

   Onde $q_t(\theta)$ √© a fun√ß√£o de log-verossimilhan√ßa para a observa√ß√£o $t$.  O vetor de escore total √© a soma dos escores individuais:

   $$S_T(\theta) = \sum_{t=1}^T s_t(\theta)$$

II. **Teorema do Limite Central para Martingales:** Sob as condi√ß√µes de regularidade e a condi√ß√£o de momento finito $E(|v_t|^{2+\delta}|x_t, \mathcal{Y}_{t-1}) < \infty$, o Teorema do Limite Central para Martingales implica que:

    $$\frac{1}{\sqrt{T}} \sum_{t=1}^T s_t(\theta) \xrightarrow{d} N(0, S)$$

    onde $S = \text{plim } T^{-1} \sum_{t=1}^T [s_t(\theta)][s_t(\theta)]'$ √© a matriz de covari√¢ncia do vetor de escore.  A condi√ß√£o de momento finito garante que as condi√ß√µes de Lindeberg para o Teorema do Limite Central sejam satisfeitas, garantindo a converg√™ncia para a distribui√ß√£o normal.

III. **Expans√£o de Taylor:** Expandimos o vetor de escore em torno do verdadeiro valor dos par√¢metros $\theta$:

     $$0 = \frac{1}{\sqrt{T}} S_T(\hat{\theta}) = \frac{1}{\sqrt{T}} S_T(\theta) + \left[ \frac{1}{T} \sum_{t=1}^T \frac{\partial s_t(\theta)}{\partial \theta'} \right] \sqrt{T}(\hat{\theta} - \theta) + o_p(1)$$

     Onde $\hat{\theta}$ √© o estimador QMLE.

IV. **Converg√™ncia em Probabilidade:** Pela Lei dos Grandes N√∫meros, temos:

    $$\frac{1}{T} \sum_{t=1}^T \frac{\partial s_t(\theta)}{\partial \theta'} \xrightarrow{p} -D$$

    Onde $D = \text{plim } T^{-1} \sum_{t=1}^T -E[\frac{\partial s_t(\theta)}{\partial \theta}]$ √© a matriz de informa√ß√£o.

V. **Reorganizando os Termos:** Reorganizando a expans√£o de Taylor e usando os resultados anteriores, obtemos:

   $$\sqrt{T}(\hat{\theta} - \theta) = D^{-1} \left[ \frac{1}{\sqrt{T}} \sum_{t=1}^T s_t(\theta) \right] + o_p(1)$$

VI. **Converg√™ncia Assint√≥tica:** Aplicando o Teorema do Limite Central para Martingales, conclu√≠mos que:

    $$\sqrt{T}(\hat{\theta} - \theta) \xrightarrow{d} N(0, D^{-1}SD^{-1})$$

    Isto estabelece a normalidade assint√≥tica do estimador QMLE. ‚ñ†

**Observa√ß√£o:** A matriz de covari√¢ncia $D^{-1}SD^{-1}$ √© frequentemente referida como a matriz de covari√¢ncia robusta, pois ela √© consistente mesmo quando a fun√ß√£o de log-verossimilhan√ßa √© mal especificada. A matriz $D^{-1}$ seria a matriz de covari√¢ncia eficiente se a fun√ß√£o de log-verossimilhan√ßa fosse corretamente especificada.

> üí° **Exemplo Num√©rico:** Suponha que estamos estimando um modelo linear simples usando QMLE. O modelo √© dado por $y_t = x_t\beta + \epsilon_t$, onde $y_t$ √© a vari√°vel dependente, $x_t$ √© a vari√°vel independente e $\epsilon_t$ √© o termo de erro. Assumimos que $\epsilon_t$ segue uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia condicional $h_t$, que pode ser heterosced√°stica.
>
> Ap√≥s estimar o modelo usando QMLE, obtemos $\hat{\beta} = 2.5$. Suponha que o tamanho da amostra seja $T = 500$. Atrav√©s do c√°lculo da matriz de covari√¢ncia robusta $D^{-1}SD^{-1}$, estimamos o erro padr√£o de $\hat{\beta}$ como $\text{SE}(\hat{\beta}) = 0.2$.
>
> Podemos ent√£o construir um intervalo de confian√ßa de 95% para $\beta$ usando a distribui√ß√£o normal assint√≥tica:
>
> $IC_{95\%}(\beta) = \hat{\beta} \pm 1.96 \cdot \text{SE}(\hat{\beta}) = 2.5 \pm 1.96 \cdot 0.2 = [2.108, 2.892]$
>
> Isso significa que estamos 95% confiantes de que o verdadeiro valor do par√¢metro $\beta$ est√° entre 2.108 e 2.892.
>
> ```python
> import numpy as np
> from scipy.stats import norm
>
> # Estimativa do par√¢metro beta
> beta_hat = 2.5
>
> # Erro padr√£o robusto de beta_hat
> se_beta_hat = 0.2
>
> # N√≠vel de confian√ßa
> confidence_level = 0.95
>
> # Valor cr√≠tico da distribui√ß√£o normal padr√£o (Z-score)
> z_critical = norm.ppf((1 + confidence_level) / 2)
>
> # Calculando o intervalo de confian√ßa
> lower_bound = beta_hat - z_critical * se_beta_hat
> upper_bound = beta_hat + z_critical * se_beta_hat
>
> print(f"Intervalo de Confian√ßa de {confidence_level*100}\% para beta: [{lower_bound:.3f}, {upper_bound:.3f}]")
> ```

**Teorema 1:** (Consist√™ncia da Matriz de Covari√¢ncia Robusta)

Sob as mesmas condi√ß√µes do Teorema da Normalidade Assint√≥tica da QMLE, o estimador da matriz de covari√¢ncia robusta $\hat{D}_T^{-1}\hat{S}_T\hat{D}_T^{-1}$ converge em probabilidade para $D^{-1}SD^{-1}$.

*Prova (Resumo):*

A prova segue diretamente das Leis dos Grandes N√∫meros e do Teorema da Converg√™ncia Cont√≠nua. Uma vez que $\hat{D}_T$ e $\hat{S}_T$ s√£o estimadores consistentes de $D$ e $S$, respectivamente, e a fun√ß√£o $A^{-1}BA^{-1}$ √© cont√≠nua, ent√£o $\hat{D}_T^{-1}\hat{S}_T\hat{D}_T^{-1}$ converge em probabilidade para $D^{-1}SD^{-1}$. $\blacksquare$

A prova detalhada da consist√™ncia da matriz de covari√¢ncia robusta pode ser estruturada como:

*Prova (Detalhada):*

Para provar que o estimador da matriz de covari√¢ncia robusta converge em probabilidade para a verdadeira matriz de covari√¢ncia, seguiremos os seguintes passos:

I. **Definir os Estimadores:** Definimos os estimadores amostrais das matrizes $S$ e $D$ como:

    $$\hat{S}_T = T^{-1} \sum_{t=1}^T [s_t(\hat{\theta})][s_t(\hat{\theta})]'$$

    $$\hat{D}_T = T^{-1} \sum_{t=1}^T -E\left[\frac{\partial^2 q_t(\theta)}{\partial \theta \partial \theta'}\right]_{\theta=\hat{\theta}}$$

II. **Consist√™ncia dos Estimadores:** Pela consist√™ncia do estimador QMLE $\hat{\theta}$ e pelas Leis dos Grandes N√∫meros, temos que:

    $$\hat{S}_T \xrightarrow{p} S$$

    $$\hat{D}_T \xrightarrow{p} D$$

    Isto significa que, √† medida que o tamanho da amostra $T$ aumenta, os estimadores amostrais $\hat{S}_T$ e $\hat{D}_T$ convergem em probabilidade para as verdadeiras matrizes $S$ e $D$, respectivamente.

III. **Continuidade da Fun√ß√£o:** A fun√ß√£o que mapeia as matrizes $\hat{D}_T$ e $\hat{S}_T$ para a matriz de covari√¢ncia robusta $\hat{D}_T^{-1}\hat{S}_T\hat{D}_T^{-1}$ √© cont√≠nua. Especificamente, a fun√ß√£o $f(A, B) = A^{-1}BA^{-1}$ √© cont√≠nua para matrizes $A$ e $B$ onde $A$ √© invert√≠vel.

IV. **Teorema da Converg√™ncia Cont√≠nua:** Aplicando o Teorema da Converg√™ncia Cont√≠nua, que afirma que se $X_n \xrightarrow{p} X$ e $g(\cdot)$ √© uma fun√ß√£o cont√≠nua, ent√£o $g(X_n) \xrightarrow{p} g(X)$, temos:

    $$\hat{D}_T^{-1}\hat{S}_T\hat{D}_T^{-1} \xrightarrow{p} D^{-1}SD^{-1}$$

    Isto implica que o estimador da matriz de covari√¢ncia robusta $\hat{D}_T^{-1}\hat{S}_T\hat{D}_T^{-1}$ converge em probabilidade para a verdadeira matriz de covari√¢ncia $D^{-1}SD^{-1}$. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere a estima√ß√£o de um modelo linear $y_t = \beta x_t + \epsilon_t$ usando QMLE, onde $\epsilon_t$ √© heterosced√°stico. Suponha que ap√≥s a estima√ß√£o com uma amostra de tamanho $T=200$, obtivemos as seguintes matrizes:
>
> $\hat{S}_T = \begin{bmatrix} 0.8 \end{bmatrix}$ e $\hat{D}_T = \begin{bmatrix} 0.5 \end{bmatrix}$. (Note que, neste caso univariado, as matrizes se reduzem a escalares).
>
> O estimador da matriz de covari√¢ncia robusta √© ent√£o calculado como:
>
> $\hat{D}_T^{-1}\hat{S}_T\hat{D}_T^{-1} = (0.5)^{-1} \cdot 0.8 \cdot (0.5)^{-1} = 2 \cdot 0.8 \cdot 2 = 3.2$.
>
> Portanto, a vari√¢ncia robusta do estimador $\hat{\beta}$ √© 3.2/T = 3.2/200 = 0.016, e o erro padr√£o robusto √© $\sqrt{0.016} \approx 0.1265$.  Este erro padr√£o robusto pode ser usado para realizar infer√™ncia sobre $\beta$.
>
> ```python
> import numpy as np
>
> # Matrizes estimadas
> S_hat = np.array([[0.8]])
> D_hat = np.array([[0.5]])
> T = 200
>
> # Calculando a matriz de covari√¢ncia robusta
> cov_robust_hat = np.linalg.inv(D_hat) @ S_hat @ np.linalg.inv(D_hat)
>
> # Vari√¢ncia robusta
> robust_variance = cov_robust_hat[0, 0] / T
>
> # Erro padr√£o robusto
> robust_se = np.sqrt(robust_variance)
>
> print(f"Matriz de Covari√¢ncia Robusta Estimada: \n{cov_robust_hat}")
> print(f"Vari√¢ncia Robusta Estimada: {robust_variance:.4f}")
> print(f"Erro Padr√£o Robusto Estimado: {robust_se:.4f}")
> ```

**Ajuste dos Erros Padr√£o (Revis√£o):**

Conforme mencionado anteriormente, √© crucial ajustar os erros padr√£o para levar em conta a n√£o normalidade dos res√≠duos. Os erros padr√£o para $\hat{\theta}$ que s√£o robustos √† m√° especifica√ß√£o da fam√≠lia de densidades podem, portanto, ser obtidos a partir da raiz quadrada dos elementos diagonais de [^663]:

$$T^{-1}\hat{D}_T^{-1}\hat{S}_T\hat{D}_T^{-1}$$

Onde [^663]:

*   $\hat{S}_T = T^{-1} \sum_{t=1}^T [s_t(\hat{\theta}_T)][s_t(\hat{\theta}_T)]'$
*   $\hat{D}_T = T^{-1} \sum_{t=1}^T -E[\frac{\partial^2 q_t(\theta)}{\partial \theta \partial \theta'}]$

Essas matrizes s√£o estimadas utilizando os dados amostrais e as estimativas QMLE dos par√¢metros.

> üí° **Exemplo Num√©rico:**
> Revisitando o exemplo anterior, suponha que estimamos o seguinte modelo para a vari√¢ncia condicional:
> $h_t = \alpha_0 + \alpha_1 \epsilon_{t-1}^2$.  Al√©m disso, suponha que $y_t = \sqrt{h_t} * v_t$, onde $v_t \sim N(0,1)$.
> Se a verdadeira distribui√ß√£o de $v_t$ √©, de fato, diferente de uma normal, ent√£o os erros padr√£o que calculamos usando a QMLE precisam ser ajustados.
>
> ```python
> import numpy as np
>
> # Estimativas de par√¢metros
> alpha_0_hat = 0.05
> alpha_1_hat = 0.2
>
> # Erros padr√£o (ing√™nuos, n√£o ajustados)
> se_alpha_0_naive = 0.02
> se_alpha_1_naive = 0.08
>
> # Elementos da matriz de covari√¢ncia robusta (calculados externamente)
> cov_alpha_0_robust = 0.0005
> cov_alpha_1_robust = 0.008
>
> # Erros padr√£o robustos (ajustados)
> se_alpha_0_robust = np.sqrt(cov_alpha_0_robust)
> se_alpha_1_robust = np.sqrt(cov_alpha_1_robust)
>
> # Imprimindo os resultados
> print("Resultados da Estimativa QMLE:")
> print(f"alpha_0_hat = {alpha_0_hat:.4f}, Erro Padr√£o Ing√™nuo = {se_alpha_0_naive:.4f}, Erro Padr√£o Robusto = {se_alpha_0_robust:.4f}")
> print(f"alpha_1_hat = {alpha_1_hat:.4f}, Erro Padr√£o Ing√™nuo = {se_alpha_1_naive:.4f}, Erro Padr√£o Robusto = {se_alpha_1_robust:.4f}")
> ```

> üí° **Exemplo Num√©rico:** Imagine que voc√™ est√° modelando a volatilidade de retornos de a√ß√µes usando um modelo GARCH(1,1). Voc√™ estimou os par√¢metros usando QMLE e obteve um erro padr√£o para o par√¢metro $\alpha_1$ (que mede a persist√™ncia de choques na volatilidade) de 0.05 usando o m√©todo padr√£o (n√£o ajustado). No entanto, voc√™ suspeita que os res√≠duos n√£o s√£o normalmente distribu√≠dos. Ap√≥s calcular a matriz de covari√¢ncia robusta, voc√™ descobre que o erro padr√£o ajustado para $\alpha_1$ √©, na verdade, 0.07. Este erro padr√£o maior reflete a incerteza adicional causada pela n√£o-normalidade dos res√≠duos e levar√° a intervalos de confian√ßa mais amplos e testes de hip√≥teses mais conservadores.
>
> ```python
> import numpy as np
>
> # Erro padr√£o n√£o ajustado
> se_alpha_1_unadjusted = 0.05
>
> # Erro padr√£o ajustado (robusto)
> se_alpha_1_adjusted = 0.07
>
> # Estimativa do par√¢metro (assumindo um valor para exemplo)
> alpha_1_hat = 0.3
>
> # N√≠vel de confian√ßa
> confidence_level = 0.95
>
> # Valor cr√≠tico da distribui√ß√£o normal padr√£o
> z_critical = np.abs(np.random.normal(0, 1, 100000)).mean() # approximated critical value of Z distribution (abs value)
>
> # Intervalo de confian√ßa n√£o ajustado
> ic_unadjusted = (alpha_1_hat - z_critical * se_alpha_1_unadjusted, alpha_1_hat + z_critical * se_alpha_1_unadjusted)
>
> # Intervalo de confian√ßa ajustado
> ic_adjusted = (alpha_1_hat - z_critical * se_alpha_1_adjusted, alpha_1_hat + z_critical * se_alpha_1_adjusted)
>
> print(f"Intervalo de Confian√ßa N√£o Ajustado (95\%): {ic_unadjusted}")
> print(f"Intervalo de Confian√ßa Ajustado (95\%): {ic_adjusted}")
> ```

**Lema 1:** (Consist√™ncia do Estimador da Matriz de Escore)
Sob condi√ß√µes de regularidade apropriadas, o estimador $\hat{S}_T$ da matriz de escore $S$ √© consistente, isto √©, $\hat{S}_T \xrightarrow{p} S$.

*Prova (Resumo):*
A prova baseia-se na Lei dos Grandes N√∫meros. Sob as condi√ß√µes de regularidade, $s_t(\hat{\theta})s_t(\hat{\theta})'$ √© assintoticamente n√£o correlacionado, e sua m√©dia converge para $S$. Portanto, a m√©dia amostral $\hat{S}_T$ converge em probabilidade para $S$. $\blacksquare$

Podemos detalhar a prova da consist√™ncia do estimador da matriz de escore da seguinte forma:

*Prova (Detalhada):*

Para provar a consist√™ncia do estimador $\hat{S}_T$ da matriz de escore $S$, seguiremos os seguintes passos:

I. **Defini√ß√£o do Estimador:** Definimos o estimador amostral da matriz de escore como:

   $$\hat{S}_T = T^{-1} \sum_{t=1}^T [s_t(\hat{\theta})][s_t(\hat{\theta})]'$$

   Onde $s_t(\hat{\theta})$ √© o vetor de escore avaliado no estimador QMLE $\hat{\theta}$.

II. **Lei dos Grandes N√∫meros (LLN):** Sob as condi√ß√µes de regularidade apropriadas, podemos aplicar a Lei dos Grandes N√∫meros para processos estacion√°rios e erg√≥dicos.  Especificamente, assumimos que:

    *   $E[s_t(\theta)s_t(\theta)'] = S$ existe e √© finito.
    *   $\{s_t(\hat{\theta})s_t(\hat{\theta})'\}$ √© assintoticamente n√£o correlacionado.

III. **Converg√™ncia em M√©dia:** Devido √† consist√™ncia de $\hat{\theta}$, temos que $\hat{\theta} \xrightarrow{p} \theta$. Portanto, $s_t(\hat{\theta}) \xrightarrow{p} s_t(\theta)$.

IV. **Aplica√ß√£o da LLN:** Aplicando a Lei dos Grandes N√∫meros, temos:

    $$\hat{S}_T = T^{-1} \sum_{t=1}^T [s_t(\hat{\theta})][s_t(\hat{\theta})]' \xrightarrow{p} E[s_t(\theta)s_t(\theta)'] = S$$

    Isto implica que o estimador amostral $\hat{S}_T$ converge em probabilidade para a verdadeira matriz de escore $S$. ‚ñ†

> üí° **Exemplo Num√©rico:**  Suponha que estejamos estimando um modelo QMLE e temos um tamanho de amostra de T = 300. Ap√≥s a estima√ß√£o, calculamos o vetor de escore para cada observa√ß√£o $t$, $s_t(\hat{\theta})$. Ent√£o, calculamos $\hat{S}_T$ como a m√©dia das somas dos produtos dos vetores de escore:
>
> $\hat{S}_T = \frac{1}{300} \sum_{t=1}^{300} [s_t(\hat{\theta})][s_t(\hat{\theta})]'$
>
> Se, ap√≥s os c√°lculos, obtivermos $\hat{S}_T = \begin{bmatrix} 1.2 \end{bmatrix}$ (novamente, no caso univariado), isso significa que nossa estimativa amostral da matriz de escore converge para o valor 1.2 √† medida que o tamanho da amostra aumenta. A consist√™ncia garante que este valor seja uma boa aproxima√ß√£o da verdadeira matriz de escore $S$.
>
> ```python
> import numpy as np
>
> # Tamanho da amostra
> T = 300
>
> # Simulando vetores de escore (para fins ilustrativos)
> np.random.seed(42)  # para reproducibilidade
> score_vectors = np.random.normal(0, 1, size=(T, 1))  # simulando vetores de escore
>
> # Calculando a matriz S_hat
> S_hat = (1/T) * np.sum([s @ s.T for s in score_vectors], axis=0)
>
> print(f"Estimativa da Matriz de Escore (S_hat): \n{S_hat}")
> ```

### Testes de Hip√≥teses
A normalidade assint√≥tica dos estimadores QMLE permite realizar testes de hip√≥teses sobre os par√¢metros do modelo.  As hip√≥teses nula e alternativa t√≠picas s√£o expressas como:

*   $H_0: \theta = \theta_0$
*   $H_1: \theta \neq \theta_0$

A estat√≠stica de teste √© constru√≠da como:

$$z = \frac{\hat{\theta} - \theta_0}{\text{SE}(\hat{\theta})}$$

Onde $\text{SE}(\hat{\theta})$ √© o erro padr√£o robusto do estimador $\hat{\theta}$. Sob a hip√≥tese nula, a estat√≠stica de teste $z$ segue aproximadamente uma distribui√ß√£o normal padr√£o. Podemos ent√£o calcular o p-valor associado √† estat√≠stica de teste e rejeitar ou n√£o a hip√≥tese nula com base no n√≠vel de signific√¢ncia escolhido.

**Observa√ß√£o:** Al√©m do teste z, outros testes assintoticamente equivalentes podem ser utilizados, como o teste de Wald, o teste do Multiplicador de Lagrange (LM) e o teste da Raz√£o de Verossimilhan√ßas (LR). Esses testes tamb√©m se baseiam na distribui√ß√£o normal assint√≥tica dos estimadores QMLE.

> üí° **Exemplo Num√©rico:**
> Suponha que estimamos um modelo ARCH(1) e queremos testar a hip√≥tese de que $\alpha_1 = 0$. Isso corresponderia a testar se a vari√¢ncia condicional depende dos choques passados.
>
> *   Hip√≥teses:
>     *   $H_0: \alpha_1 = 0$
>     *   $H_1: \alpha_1 \neq 0$
> *   Estimativa: $\hat{\alpha}_1 = 0.25$
> *   Erro padr√£o robusto: $\text{SE}(\hat{\alpha}_1) = 0.1$
> *   Estat√≠stica de teste: $z = \frac{0.25 - 0}{0.1} = 2.5$
> *   P-valor: Usando uma distribui√ß√£o normal padr√£o, o p-valor para um teste bicaudal √© $2 * (1 - \Phi(2.5)) \approx 0.0124$, onde $\Phi$ √© a fun√ß√£o de distribui√ß√£o cumulativa da normal padr√£o.
>
> Se escolhermos um n√≠vel de signific√¢ncia de 5%, rejeitamos a hip√≥tese nula de que $\alpha_1 = 0$, indicando que h√° evid√™ncias de que a vari√¢ncia condicional depende dos choques passados.
>
> ```python
> import numpy as np
> from scipy.stats import norm
>
> # Valor estimado do par√¢metro
> alpha_1_hat = 0.25
>
> # Erro padr√£o robusto do par√¢metro
> se_alpha_1 = 0.1
>
> # Valor do par√¢metro sob a hip√≥tese nula
> alpha_1_null = 0
>
> # Calculando a estat√≠stica z
> z_stat = (alpha_1_hat - alpha_1_null) / se_alpha_1
>
> # Calculando o p-valor (teste bicaudal)
> p_value = 2 * (1 - norm.cdf(np.abs(z_stat)))
>
> # N√≠vel de signific√¢ncia
> alpha = 0.05
>
> # Imprimindo os resultados
> print(f"Estat√≠stica de teste z: {z_stat:.4f}")
> print(f"P-valor: {p_value:.4f}")
>
> # Verificando se rejeitamos a hip√≥tese nula
> if p_value < alpha:
>     print("Rejeitamos a hip√≥tese nula ao n√≠vel de signific√¢ncia de 5\%.")
> else:
>     print("N√£o rejeitamos a hip√≥tese nula ao n√≠vel de signific√¢ncia de 5\%.")
> ```

> üí° **Exemplo Num√©rico:** Vamos supor que voc√™ esteja estimando um modelo de regress√£o linear e queira testar se o coeficiente de uma vari√°vel explicativa ($x_1$) √© igual a 1. As hip√≥teses s√£o:
>
> *   $H_0: \beta_1 = 1$
> *   $H_1: \beta_1 \neq 1$
>
> Ap√≥s a estima√ß√£o QMLE, voc√™ obt√©m:
>
> *   $\hat{\beta}_1 = 1.2$ (estimativa do coeficiente)
> *   $\text{SE}(\hat{\beta}_1) = 0.15$ (erro padr√£o robusto)
>
> A estat√≠stica de teste √©:
>
> $z = \frac{1.2 - 1}{0.15} = \frac{0.2}{0.15} \approx 1.333$
>
> O p-valor (teste bicaudal) √©:
>
> $p = 2 * (1 - \Phi(|1.333|)) \approx 2 * (1 - 0.9088) \approx 0.1824$
>
> Se o n√≠vel de signific√¢ncia for 5% (0.05), como o p-valor (0.1824) √© maior que 0.05, n√£o rejeitamos a hip√≥tese nula. Isso significa que n√£o h√° evid√™ncias estat√≠sticas suficientes para afirmar que o coeficiente de $x_1$ √© diferente de 1.
>
> ```python
> import numpy as np
> from scipy.stats import norm
>
> # Estimativa do coeficiente
> beta_1_hat = 1.2
>
> # Erro padr√£o robusto
> se_beta_1 = 0.15
>
> # Valor sob a hip√≥tese nula
> beta_1_null = 1
>
> # Calcula a estat√≠stica z
> z_statistic = (beta_1_hat - beta_1_null) / se_beta_1
>
> # Calcula o p-valor (two-sided test)
> p_value = 2 * (1 - norm.cdf(abs(z_statistic)))
>
> # N√≠vel de signific√¢ncia
> alpha = 0.05
>
> print(f"Estat√≠stica z: {z_statistic:.3f}")
> print(f"P-valor: {p_value:.3f}")
>
> # Verifica se rejeita a hip√≥tese nula
> if p_value < alpha:
>     print("Rejeita a hip√≥tese nula.")
> else:
>     print("N√£o rejeita a hip√≥tese nula.")
> ```

### Conclus√£o

Este cap√≠tulo destacou a import√¢ncia da normalidade assint√≥tica dos estimadores QMLE para a realiza√ß√£o de infer√™ncia estat√≠stica v√°lida em modelos de s√©ries temporais com heteroscedasticidade [^663]. Embora a QMLE ofere√ßa uma abordagem robusta para a estima√ß√£o, √© crucial ajustar os erros padr√£o para levar em conta a n√£o normalidade dos res√≠duos [^663]. Ao fazer isso, podemos construir intervalos de confian√ßa e realizar testes de hip√≥teses sobre os par√¢metros do modelo, permitindo uma an√°lise mais completa e precisa das propriedades din√¢micas da s√©rie temporal.

### Refer√™ncias

[^663]: Cap√≠tulo 21, "Time Series Models of Heteroskedasticity", Se√ß√£o 21.1. Autoregressive Conditional Heteroskedasticity (ARCH).
<!-- END -->