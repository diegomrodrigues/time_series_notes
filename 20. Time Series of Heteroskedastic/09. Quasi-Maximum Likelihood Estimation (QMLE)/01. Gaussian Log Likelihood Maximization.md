## Estimação de Quase Máxima Verossimilhança em Modelos Heteroscedásticos

### Introdução
Este capítulo aprofunda a análise de modelos de séries temporais com heteroscedasticidade, focando na estimação de parâmetros através do método de **Quase Máxima Verossimilhança (QMLE)**. Em particular, exploraremos as condições sob as quais a QMLE fornece estimativas consistentes mesmo quando as suposições distribucionais do modelo são violadas.

### Conceitos Fundamentais

Em modelagem de séries temporais, frequentemente assume-se que os erros seguem uma distribuição normal independente e identicamente distribuída (i.i.d.). No entanto, em muitos casos práticos, essa suposição é violada. Por exemplo, em dados financeiros, a volatilidade (variância condicional) pode variar ao longo do tempo, caracterizando um comportamento heteroscedástico.

Neste contexto, o método de Quase Máxima Verossimilhança (QMLE) oferece uma alternativa robusta para a estimação de parâmetros. O QMLE consiste em maximizar a função de log-verossimilhança gaussiana, mesmo quando a distribuição verdadeira dos erros não é normal. A grande vantagem desse método é que, sob certas condições, ele ainda pode fornecer estimativas consistentes dos parâmetros do modelo [^663].

> 💡 **Exemplo Numérico:** Imagine que estamos modelando o retorno diário de uma ação. Usamos um modelo GARCH(1,1) onde a variância condicional no tempo $t$ depende da variância condicional no tempo $t-1$ e do erro ao quadrado no tempo $t-1$. Formalmente:
>
> $y_t = \mu + \epsilon_t$
>
> $\epsilon_t = v_t \sqrt{h_t}$
>
> $h_t = \alpha_0 + \alpha_1 \epsilon_{t-1}^2 + \beta_1 h_{t-1}$
>
> Aqui, $y_t$ é o retorno diário da ação, $\mu$ é o retorno médio, $\epsilon_t$ é o erro, $v_t$ é um choque i.i.d. com média 0 e variância 1, e $h_t$ é a variância condicional. $\alpha_0$, $\alpha_1$ e $\beta_1$ são os parâmetros do modelo GARCH(1,1) que precisamos estimar. Suponha que, após aplicar QMLE, obtivemos as seguintes estimativas: $\hat{\mu} = 0.001$, $\hat{\alpha}_0 = 0.01$, $\hat{\alpha}_1 = 0.1$, $\hat{\beta}_1 = 0.8$. Isso significa que o retorno médio diário estimado é de 0.1%, e a variância condicional atual é influenciada pelo choque do dia anterior (com peso 0.1) e pela variância condicional do dia anterior (com peso 0.8).
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Parâmetros estimados
> mu_hat = 0.001
> alpha_0_hat = 0.01
> alpha_1_hat = 0.1
> beta_1_hat = 0.8
>
> # Número de simulações
> num_simulations = 300
>
> # Inicialização
> returns = np.zeros(num_simulations)
> conditional_variance = np.zeros(num_simulations)
> errors = np.zeros(num_simulations)
>
> # Inicializando a variância condicional com um valor arbitrário
> conditional_variance[0] = 0.05
>
> # Gerando os retornos simulados
> for t in range(1, num_simulations):
>     errors[t] = np.random.normal(0, np.sqrt(conditional_variance[t-1]))  # Gerando o erro com base na variância condicional anterior
>     conditional_variance[t] = alpha_0_hat + alpha_1_hat * errors[t-1]**2 + beta_1_hat * conditional_variance[t-1]
>     returns[t] = mu_hat + errors[t]
>
> # Plotando a variância condicional
> plt.figure(figsize=(10, 6))
> plt.plot(conditional_variance)
> plt.title("Variância Condicional Simulada (GARCH(1,1))")
> plt.xlabel("Tempo")
> plt.ylabel("Variância Condicional")
> plt.grid(True)
> plt.show()
>
> # Plotando os retornos simulados
> plt.figure(figsize=(10, 6))
> plt.plot(returns)
> plt.title("Retornos Diários Simulados (GARCH(1,1))")
> plt.xlabel("Tempo")
> plt.ylabel("Retornos")
> plt.grid(True)
> plt.show()
> ```

Considerando que a suposição de que $v_t$ é i.i.d. N(0, 1) seja inválida, vimos em [21.1.6] que a especificação ARCH ainda pode oferecer um modelo razoável sobre o qual basear uma previsão linear do valor quadrado de $v_t$ [^663].

**Proposição 1** Sob condições de regularidade apropriadas, a QMLE estimator $\hat{\theta}_T$ converge em probabilidade para um valor $\theta_0$ que minimiza a distância de Kullback-Leibler entre a verdadeira densidade e a densidade gaussiana utilizada na QMLE.

*Prova (Esboço):* A prova segue a linha de argumentação padrão para a consistência de estimadores de máxima verossimilhança, adaptada para o caso da QMLE. Envolve mostrar que a função objetivo (log-verossimilhança gaussiana) converge uniformemente em probabilidade para sua esperança, e que essa esperança é minimizada no verdadeiro valor do parâmetro sob as condições dadas. As condições de regularidade asseguram a existência e unicidade do mínimo, bem como a diferenciabilidade da função objetivo.

Para detalhar um pouco mais a prova da Proposição 1, podemos adicionar os seguintes passos:

*Prova (Detalhada):*

Para provar a consistência do estimador QMLE $\hat{\theta}_T$, mostraremos que ele converge em probabilidade para o verdadeiro valor $\theta_0$ que minimiza a distância de Kullback-Leibler.

I. **Definindo a Função Objetivo:** A função de log-verossimilhança gaussiana para a QMLE é dada por:
$$Q_T(\theta) = \frac{1}{T} \sum_{t=1}^{T} q_t(\theta) =  \frac{1}{T} \sum_{t=1}^{T} \left[ -\frac{1}{2} \log(2\pi h_t(\theta)) - \frac{(y_t - x_t'\beta)^2}{2h_t(\theta)} \right]$$
onde $h_t(\theta)$ é a variância condicional modelada e $\theta$ é o vetor de parâmetros.

II. **Convergência Uniforme em Probabilidade:** Assumimos que $Q_T(\theta)$ converge uniformemente em probabilidade para sua esperança, ou seja:
$$\sup_{\theta \in \Theta} |Q_T(\theta) - E[Q_T(\theta)]| \xrightarrow{p} 0$$
Isto requer condições de regularidade sobre a função $q_t(\theta)$ e o espaço de parâmetros $\Theta$, garantindo que a lei dos grandes números se aplica uniformemente.

III. **Identificação:** Assumimos que o verdadeiro valor do parâmetro, $\theta_0$, é unicamente identificado, ou seja, $\theta_0 = \arg\max_{\theta \in \Theta} E[Q_T(\theta)]$. Isto significa que a função de log-verossimilhança esperada é maximizada unicamente no verdadeiro valor do parâmetro.  Equivalentemente, $\theta_0$ minimiza a distância de Kullback-Leibler entre a verdadeira densidade e a densidade gaussiana utilizada na QMLE.

IV. **Otimizador:** O estimador QMLE $\hat{\theta}_T$ é definido como o otimizador da função de log-verossimilhança amostral:
$$\hat{\theta}_T = \arg\max_{\theta \in \Theta} Q_T(\theta)$$

V. **Convergência:** Usando os resultados de convergência uniforme em probabilidade e identificação, podemos mostrar que $\hat{\theta}_T$ converge em probabilidade para $\theta_0$:
$$\hat{\theta}_T \xrightarrow{p} \theta_0$$
Isto significa que, à medida que o tamanho da amostra aumenta, o estimador QMLE se aproxima do verdadeiro valor do parâmetro.

VI. **Condições de Regularidade:** As condições de regularidade incluem:

   *   O espaço de parâmetros $\Theta$ é compacto.
   *   A função $q_t(\theta)$ é contínua em $\theta$ para quase todo $t$.
   *   A função $E[q_t(\theta)]$ tem um máximo único em $\theta_0$.
   *   $q_t(\theta)$ é dominada por uma função integrável.

Sob estas condições, a QMLE $\hat{\theta}_T$ é um estimador consistente de $\theta_0$. ■

**Condições para Consistência da QMLE:**

A consistência da QMLE requer que as seguintes condições sejam satisfeitas [^663]:

1.  A especificação do modelo para a variância condicional seja correta.
2.  Os resíduos padronizados ($v_t$) satisfaçam:
    *   $E(v_t|x_t, \mathcal{Y}_{t-1}) = 0$
    *   $E(v_t^2|x_t, \mathcal{Y}_{t-1}) = 1$

Onde $x_t$ representa variáveis exógenas e $\mathcal{Y}_{t-1}$ o conjunto de informação disponível no tempo $t-1$ [^663]. Essencialmente, essas condições garantem que o modelo capture corretamente a dinâmica da variância condicional, mesmo que a distribuição incondicional dos erros não seja normal [^663].

> 💡 **Exemplo Numérico:** Suponha que temos dados de retornos diários de ações e modelamos a variância condicional usando um modelo ARCH(1). Se a verdadeira variância condicional seguir um processo diferente, como um GARCH(1,1), a condição 1 não será satisfeita. Se, após ajustar o modelo ARCH(1), observarmos que os resíduos padronizados apresentam autocorrelação significativa em seus quadrados, isso indica que o modelo não capturou toda a dinâmica da variância condicional. Além disso, se calcularmos a média condicional e a variância condicional dos resíduos padronizados e encontrarmos valores significativamente diferentes de 0 e 1, respectivamente, isso indica uma violação da condição 2. Por exemplo, poderíamos realizar um teste de Ljung-Box nos quadrados dos resíduos padronizados para verificar a presença de autocorrelação.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from statsmodels.stats.diagnostic import acorr_ljungbox
>
> # Gerando dados simulados com um modelo GARCH(1,1)
> np.random.seed(42)
> T = 500  # Tamanho da amostra
> alpha_0 = 0.1
> alpha_1 = 0.3
> beta_1 = 0.5
>
> # Inicializando a variância condicional
> h = np.zeros(T)
> h[0] = 1
>
> # Gerando os resíduos
> epsilon = np.zeros(T)
> for t in range(1, T):
>     epsilon[t] = np.random.normal(0, np.sqrt(h[t-1]))
>     h[t] = alpha_0 + alpha_1 * epsilon[t-1]**2 + beta_1 * h[t-1]
>
> # Simulando os retornos
> returns = np.random.normal(0, np.sqrt(h))
>
> # Ajustando um modelo ARCH(1) aos dados simulados (para fins de ilustração)
> # Aqui, para simplificar, vamos simular os resíduos de um modelo ARCH(1)
> # com parâmetros diferentes do GARCH(1,1) original
> alpha_0_arch = 0.2
> alpha_1_arch = 0.4
> h_arch = np.zeros(T)
> h_arch[0] = 1
>
> # Simulando os resíduos do modelo ARCH(1)
> residuals_arch = np.zeros(T)
> for t in range(1, T):
>     residuals_arch[t] = np.random.normal(0, np.sqrt(h_arch[t-1]))
>     h_arch[t] = alpha_0_arch + alpha_1_arch * residuals_arch[t-1]**2
>
> # Calculando os resíduos padronizados
> standardized_residuals = residuals_arch / np.sqrt(h_arch)
>
> # Teste de Ljung-Box nos quadrados dos resíduos padronizados
> lb_test, p_values = acorr_ljungbox(standardized_residuals**2, lags=[10])
>
> # Imprimindo os resultados do teste de Ljung-Box
> print("Teste de Ljung-Box para autocorrelação nos quadrados dos resíduos padronizados:")
> print("Estatística de teste:", lb_test)
> print("P-valor:", p_values)
>
> # Interpretando os resultados
> if p_values[0] < 0.05:
>     print("Rejeitamos a hipótese nula de ausência de autocorrelação. O modelo ARCH(1) pode não ser adequado.")
> else:
>     print("Não rejeitamos a hipótese nula de ausência de autocorrelação. O modelo ARCH(1) pode ser adequado.")
>
> # Plotando os resíduos padronizados
> plt.figure(figsize=(10, 6))
> plt.plot(standardized_residuals)
> plt.title("Resíduos Padronizados do Modelo ARCH(1)")
> plt.xlabel("Tempo")
> plt.ylabel("Resíduos Padronizados")
> plt.grid(True)
> plt.show()
>
> # Plotando os quadrados dos resíduos padronizados
> plt.figure(figsize=(10, 6))
> plt.plot(standardized_residuals**2)
> plt.title("Quadrados dos Resíduos Padronizados do Modelo ARCH(1)")
> plt.xlabel("Tempo")
> plt.ylabel("Quadrados dos Resíduos Padronizados")
> plt.grid(True)
> plt.show()
> ```

**Teorema 1.1** (Extensão da Condição 2)
Se, além das condições originais, existir um $\delta > 0$ tal que $E(|v_t|^{2+\delta}|x_t, \mathcal{Y}_{t-1}) < \infty$, então o estimador QMLE será assintoticamente normal.

*Prova (Esboço):* A prova envolve a aplicação do Teorema do Limite Central para martingales. Sob as condições dadas, o vetor de escore, quando normalizado por $\sqrt{T}$, converge em distribuição para uma distribuição normal com média zero e matriz de covariância dada por $D^{-1}SD^{-1}$, onde $S$ e $D$ são as matrizes definidas posteriormente. A condição de momento finito garante que as condições de Lindeberg para o Teorema do Limite Central sejam satisfeitas.

Para fornecer uma prova mais detalhada do Teorema 1.1, considere o seguinte:

*Prova (Detalhada):*

Para provar a normalidade assintótica do estimador QMLE $\hat{\theta}_T$, demonstraremos que, sob as condições fornecidas, o vetor de escore normalizado converge em distribuição para uma distribuição normal.

I. **Definindo o Vetor de Escore:** O vetor de escore é a derivada da função de log-verossimilhança gaussiana em relação aos parâmetros $\theta$:
$$s_t(\theta) = \frac{\partial q_t(\theta)}{\partial \theta} = \frac{\partial}{\partial \theta} \left[ -\frac{1}{2} \log(2\pi h_t(\theta)) - \frac{(y_t - x_t'\beta)^2}{2h_t(\theta)} \right]$$

II. **Teorema do Limite Central para Martingales:** Queremos aplicar o Teorema do Limite Central (TLC) para martingales ao vetor de escore normalizado. Para isso, precisamos verificar as condições do TLC. Seja $S_T(\theta) = \frac{1}{\sqrt{T}} \sum_{t=1}^{T} s_t(\theta)$

III. **Condições de Martingale:** Sob a condição $E(v_t|x_t, \mathcal{Y}_{t-1}) = 0$ e $E(v_t^2|x_t, \mathcal{Y}_{t-1}) = 1$, o vetor de escore $s_t(\theta)$ é uma diferença de martingale, ou seja, $E[s_t(\theta)|\mathcal{Y}_{t-1}] = 0$.

IV. **Matriz de Covariância:** Definimos as matrizes $S$ e $D$ como:
$$S = \text{plim } T^{-1} \sum_{t=1}^T [s_t(\theta)] [s_t(\theta)]'$$
$$D = \text{plim } T^{-1} \sum_{t=1}^T -E\left[\frac{\partial^2 q_t(\theta)}{\partial \theta \partial \theta'}\right]$$
Sob as condições do modelo, $D = \text{plim } T^{-1} \sum_{t=1}^T E[s_t(\theta) s_t(\theta)']$, que seria igual a $S$ se o modelo fosse corretamente especificado.

V. **Condição de Momento Finito:** A condição $E(|v_t|^{2+\delta}|x_t, \mathcal{Y}_{t-1}) < \infty$ para algum $\delta > 0$ garante que as condições de Lindeberg para o TLC para martingales são satisfeitas. Isto implica que os momentos de ordem $2 + \delta$ de $s_t(\theta)$ são finitos e que o efeito de qualquer termo individual $s_t(\theta)$ na soma $\sum_{t=1}^{T} s_t(\theta)$ é assintoticamente desprezível.

VI. **Normalidade Assintótica:** Aplicando o TLC para martingales, temos que:
$$\sqrt{T}(\hat{\theta}_T - \theta_0) \xrightarrow{d} N(0, D^{-1}SD^{-1})$$
onde $\xrightarrow{d}$ denota convergência em distribuição.

VII. **Interpretação:** Este resultado mostra que o estimador QMLE $\hat{\theta}_T$ é assintoticamente normal com média $\theta_0$ e matriz de covariância $D^{-1}SD^{-1}$. A matriz $D^{-1}SD^{-1}$ representa a variância assintótica do estimador, que precisa ser estimada de forma consistente para inferência estatística válida. ■

> 💡 **Exemplo Numérico:** Considere um modelo onde $v_t$ segue uma distribuição t de Student com graus de liberdade baixos (por exemplo, 3). Neste caso, os momentos de ordem superior de $v_t$ podem não existir, ou podem ser muito grandes. Para $\delta = 1$, $2 + \delta = 3$. Se $E(|v_t|^3|x_t, \mathcal{Y}_{t-1}) = \infty$, então a condição do Teorema 1.1 não é satisfeita diretamente. No entanto, para distribuições t com graus de liberdade maiores que 4, a condição geralmente se mantém para um $\delta$ suficientemente pequeno. Neste caso, podemos usar a QMLE, mas devemos estar cientes de que os erros padrão podem ser subestimados se não forem ajustados corretamente, pois a normalidade assintótica pode se estabelecer mais lentamente devido às caudas pesadas da distribuição t.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy.stats import t
>
> # Definindo os graus de liberdade da distribuição t
> df = 5  # Graus de liberdade
>
> # Tamanho da amostra
> T = 500
>
> # Gerando dados simulados de uma distribuição t
> v = t.rvs(df, size=T)
>
> # Calculando E(|v_t|^{2+\delta}) para diferentes valores de delta
> delta_values = [0.5, 1, 1.5]
> expected_values = []
>
> for delta in delta_values:
>     expected_value = np.mean(np.abs(v)**(2 + delta))
>     expected_values.append(expected_value)
>     print(f"E(|v_t|^{{2 + {delta}}}) = {expected_value}")
>
> # Plotando os dados simulados
> plt.figure(figsize=(10, 6))
> plt.plot(v)
> plt.title(f"Dados Simulados da Distribuição t com {df} Graus de Liberdade")
> plt.xlabel("Tempo")
> plt.ylabel("Valor")
> plt.grid(True)
> plt.show()
>
> # Plotando o histograma dos dados simulados
> plt.figure(figsize=(10, 6))
> plt.hist(v, bins=30, density=True, alpha=0.7, label="Histograma")
>
> # Plotando a distribuição normal para comparação
> from scipy.stats import norm
> x = np.linspace(min(v), max(v), 100)
> plt.plot(x, norm.pdf(x, 0, 1), label="Distribuição Normal (0, 1)")
>
> plt.title(f"Histograma dos Dados Simulados da Distribuição t com {df} Graus de Liberdade")
> plt.xlabel("Valor")
> plt.ylabel("Densidade")
> plt.legend()
> plt.grid(True)
> plt.show()
> ```

**Ajuste dos Erros Padrão:**

Mesmo que a QMLE forneça estimativas consistentes, os erros padrão obtidos diretamente da função de log-verossimilhança gaussiana não são válidos quando a distribuição verdadeira dos erros não é normal [^663]. Portanto, é necessário ajustar os erros padrão para levar em conta a não normalidade.

Os erros padrão para $\hat{\theta}_T$ que são robustos à má especificação da família de densidades podem, portanto, ser obtidos a partir da raiz quadrada dos elementos diagonais de [^663]:
$$T^{-1}\hat{D}_T^{-1}\hat{S}_T\hat{D}_T^{-1}$$

Lembre-se de que, se o modelo for especificado corretamente, de modo que os dados sejam realmente gerados por um modelo gaussiano, então $S = D$, e isso simplifica para a matriz de variância assintótica usual para estimativa de máxima verossimilhança [^664].

Para construir estimativas consistentes dos erros padrão, necessitamos estimar as matrizes $S$ e $D$ [^663]. Definimos as matrizes S e D como:

$S = \text{plim } T^{-1} \sum_{t=1}^T [s_t(\theta)] [s_t(\theta)]'$

$D = \text{plim } T^{-1} \sum_{t=1}^T -E[\frac{\partial s_t(\theta)}{\partial \theta}]$

Onde $s_t(\theta)$ é o vetor de escore calculado em [21.1.21] [^663].

**Estimando as Matrizes S e D**
A matriz S pode ser estimada de forma consistente por [^663]:
$$\hat{S}_T = T^{-1} \sum_{t=1}^T [s_t(\hat{\theta}_T)] [s_t(\hat{\theta}_T)]'$$
onde $s_t(\hat{\theta}_T)$ indica o vetor dado em [21.1.21] avaliado em $\hat{\theta}_T$ [^663].

Similarmente, a matriz D pode ser estimada de forma consistente por [^663]:

$$\hat{D}_T = T^{-1} \sum_{t=1}^T \left\{ \frac{1}{2h_t^2} \left[ \frac{-2 \partial h_t}{\partial \theta} \frac{(y_t - x_t'\beta)^2}{h_t} \right] + \left( \frac{1}{h_t} \right) \left[ \frac{\partial x_t' x_t}{\partial \theta} \right] \right\}$$

> 💡 **Exemplo Numérico:** Suponha que, após estimar um modelo ARCH(1) para os retornos de uma ação, obtivemos os seguintes resultados:
>
> $\hat{\alpha}_0 = 0.02$
> $\hat{\alpha}_1 = 0.3$
>
> e o tamanho da amostra é $T=500$.
>
> Para calcular os erros padrão robustos, precisamos estimar as matrizes $\hat{S}_T$ e $\hat{D}_T$. A matriz $\hat{S}_T$ é a média amostral dos produtos externos dos vetores de escore, e a matriz $\hat{D}_T$ é a média amostral das derivadas segundas da função de log-verossimilhança gaussiana.
>
> Suponha que, após realizar os cálculos (que são computacionalmente intensivos e geralmente feitos por software estatístico), obtivemos as seguintes estimativas:
>
> $\hat{S}_T = \begin{bmatrix} 0.0001 & 0.00002 \\ 0.00002 & 0.000009 \end{bmatrix}$
>
> $\hat{D}_T = \begin{bmatrix} 0.0002 & 0 \\ 0 & 0.0001 \end{bmatrix}$
>
> Então, a matriz de covariância robusta é:
>
> $\text{Cov}(\hat{\theta}_T) = T^{-1}\hat{D}_T^{-1}\hat{S}_T\hat{D}_T^{-1} = \frac{1}{500} \begin{bmatrix} 5000 & 0 \\ 0 & 10000 \end{bmatrix} \begin{bmatrix} 0.0001 & 0.00002 \\ 0.00002 & 0.000009 \end{bmatrix} \begin{bmatrix} 5000 & 0 \\ 0 & 10000 \end{bmatrix}$
>
> $\text{Cov}(\hat{\theta}_T) = \frac{1}{500} \begin{bmatrix} 2.5 & 0.1 \\ 0.1 & 0.9 \end{bmatrix} \begin{bmatrix} 5000 & 0 \\ 0 & 10000 \end{bmatrix} = \frac{1}{500} \begin{bmatrix} 12500 & 1000 \\ 1000 & 9000 \end{bmatrix} = \begin{bmatrix} 25 & 2 \\ 2 & 18 \end{bmatrix} / 500 = \begin{bmatrix} 0.05 & 0.004 \\ 0.004 & 0.036 \end{bmatrix}$
>
> Os erros padrão robustos são as raízes quadradas dos elementos diagonais:
>
> $\text{SE}(\hat{\alpha}_0) = \sqrt{0.05} \approx 0.2236$
> $\text{SE}(\hat{\alpha}_1) = \sqrt{0.036} \approx 0.1897$
>
> Sem o ajuste, os erros padrão seriam calculados usando a inversa de $\hat{D}_T$ diretamente, que pode ser menor se a distribuição verdadeira tiver caudas mais pesadas do que a normal, levando a uma subestimação da incerteza.
>
> ```python
> import numpy as np
>
> # Estimativas dos parâmetros
> alpha_0_hat = 0.02
> alpha_1_hat = 0.3
>
> # Tamanho da amostra
> T = 500
>
> # Matrizes S e D (estimativas)
> S_hat = np.array([[0.0001, 0.00002],
>                   [0.00002, 0.000009]])
>
> D_hat = np.array([[0.0002, 0],
>                   [0, 0.0001]])
>
> # Calculando a inversa da matriz D
> D_hat_inv = np.linalg.inv(D_hat)
>
> # Calculando a matriz de covariância robusta
> cov_matrix_robust = (1/T) * D_hat_inv @ S_hat @ D_hat_inv
>
> # Calculando os erros padrão robustos
> se_alpha_0_robust = np.sqrt(cov_matrix_robust[0, 0])
> se_alpha_1_robust = np.sqrt(cov_matrix_robust[1, 1])
>
> print("Erros Padrão Robustos:")
> print(f"SE(alpha_0): {se_alpha_0_robust}")
> print(f"SE(alpha_1): {se_alpha_1_robust}")
>
> # Calculando a matriz de covariância sem correção (usando a inversa de D)
> cov_matrix_naive = (1/T) * D_hat_inv
>
> # Calculando os erros padrão sem correção
> se_alpha_0_naive = np.sqrt(cov_matrix_naive[0, 0])
> se_alpha_1_naive = np.sqrt(cov_matrix_naive[1, 1])
>
> print("\nErros Padrão Sem Correção:")
> print(f"SE(alpha_0): {se_alpha_0_naive}")
> print(f"SE(alpha_1): {se_alpha_1_naive}")
> ```

**Lema 2** A matriz $D$ é definida positiva se e somente se o modelo para a variância condicional está corretamente especificado e os parâmetros são identificáveis.

*Prova (Esboço):* Se o modelo está corretamente especificado, então a função de log-verossimilhança gaussiana tem um máximo bem definido no verdadeiro valor dos parâmetros. Isso implica que a matriz Hessiana (que está relacionada com $D$) é definida negativa no máximo, e portanto $D$ é definida positiva. A identificabilidade dos parâmetros garante que não existem múltiplos valores de parâmetros que produzem a mesma variância condicional, o que também é necessário para a positividade de $D$.

Para formalizar a prova do Lema 2, podemos adicionar os seguintes passos:

*Prova (Detalhada):*

Mostraremos que a matriz $D$ é definida positiva se e somente se o modelo para a variância condicional está corretamente especificado e os parâmetros são identificáveis.

I. **Definição de Matriz Definida Positiva:** Uma matriz $D$ é definida positiva se $x'Dx > 0$ para todo vetor não nulo $x$.

II. **Ligação com a Hessiana:** A matriz $D$ é definida como o limite em probabilidade da média amostral do negativo do valor esperado da Hessiana da função de log-verossimilhança gaussiana:
$$D = \text{plim } T^{-1} \sum_{t=1}^T -E\left[\frac{\partial^2 q_t(\theta)}{\partial \theta \partial \theta'}\right]$$

III. **Modelo Corretamente Especificado:** Se o modelo para a variância condicional está corretamente especificado, então a função de log-verossimilhança gaussiana $Q_T(\theta)$ tem um máximo bem definido no verdadeiro valor dos parâmetros $\theta_0$. Portanto, a Hessiana da função de log-verossimilhança avaliada em $\theta_0$ é definida negativa:
$$\frac{\partial^2 Q_T(\theta_0)}{\partial \theta \partial \theta'} < 0$$
Isto implica que $-E\left[\frac{\partial^2 q_t(\theta)}{\partial \theta \partial \theta'}\right]$ é definida positiva em $\theta_0$.  Como $D$ é o limite em probabilidade da média amostral destes termos, $D$ também é definida positiva.

IV. **Identificabilidade dos Parâmetros:** A identificabilidade dos parâmetros significa que não existem dois valores distintos dos parâmetros $\theta_1$ e $\theta_2$ que produzem a mesma variância condicional, ou seja, $h_t(\theta_1) \neq h_t(\theta_2)$ para algum $t$. Se os parâmetros não fossem identificáveis, então a função de log-verossimilhança poderia ter múltiplos máximos, e a Hessiana não seria definida negativa em um único ponto. Isto contradiz a condição de que a Hessiana é definida negativa no máximo e, portanto, $D$ é definida positiva.

V. **Reciprocidade:** Agora, mostramos a recíproca. Se $D$ é definida positiva, então a função de log-verossimilhança tem um máximo bem definido. Isto implica que o modelo está corretamente especificado e os parâmetros são identificáveis. Se o modelo não estivesse corretamente especificado, então a função de log-verossimilhança não teria um máximo bem definido, e $D$ não seria definida positiva. Se os parâmetros não fossem identificáveis, então a função de log-verossimilhança teria múltiplos máximos, e $D$ não seria definida positiva.

VI. **Conclusão:** Portanto, a matriz $D$ é definida positiva se e somente se o modelo para a variância condicional está corretamente especificado e os parâmetros são identificáveis. ■

### Conclusão

A Quase Máxima Verossimilhança (QMLE) é uma ferramenta poderosa para a estimação de modelos de séries temporais com heteroscedasticidade [^663]. Embora a QMLE se baseie na função de log-verossimilhança gaussiana, ela pode fornecer estimativas consistentes mesmo quando a distribuição verdadeira dos erros não é normal, desde que as condições especificadas sejam satisfeitas [^663]. No entanto, é crucial ajustar os erros padrão para levar em conta a não normalidade, utilizando estimadores robustos baseados nas matrizes $S$ e $D$ [^663].

### Referências

[^663]: Capítulo 21, "Time Series Models of Heteroskedasticity", Seção 21## Capítulo 22: Modelos de Estado-Espaço

Modelos de espaço de estados fornecem uma estrutura flexível para modelar séries temporais que evoluem ao longo do tempo, onde o estado do sistema em um determinado momento depende do estado no momento anterior, bem como de quaisquer entradas externas ou ruído. Eles são particularmente úteis para modelar sistemas dinâmicos com componentes não observáveis.

### Representação Geral

Um modelo de espaço de estados é geralmente definido por duas equações:

1.  **Equação de Estado (ou Equação de Transição):**

    $$
    x_t = F x_{t-1} + G u_t + w_t
    $$

    onde:

    *   $x_t$ é o vetor de estado no tempo $t$.
    *   $F$ é a matriz de transição de estado.
    *   $u_t$ é o vetor de entrada no tempo $t$.
    *   $G$ é a matriz de controle de entrada.
    *   $w_t$ é o ruído do processo (ou ruído do sistema).
2.  **Equação de Medição (ou Equação de Observação):**

    $$
    y_t = H x_t + v_t
    $$

    onde:

    *   $y_t$ é o vetor de medição (observação) no tempo $t$.
    *   $H$ é a matriz de medição.
    *   $v_t$ é o ruído de medição.

### Componentes do Modelo

*   **Vetor de Estado ($x_t$):** Representa o estado do sistema no tempo $t$. Pode incluir variáveis como nível, tendência, sazonalidade e outros componentes dinâmicos.

*   **Matriz de Transição de Estado ($F$):** Descreve como o estado evolui de um período para o próximo.

*   **Vetor de Entrada ($u_t$):** Representa as entradas externas ou variáveis explicativas que influenciam o estado do sistema.

*   **Matriz de Controle de Entrada ($G$):** Descreve como as entradas externas afetam o estado do sistema.

*   **Vetor de Medição ($y_t$):** Representa as observações ou medidas disponíveis no tempo $t$.

*   **Matriz de Medição ($H$):** Descreve como o estado se relaciona com as observações.

*   **Ruído do Processo ($w_t$):** Representa o ruído ou incerteza na evolução do estado. Assume-se geralmente que $w_t \sim N(0, Q)$, onde $Q$ é a matriz de covariância do ruído do processo.

*   **Ruído de Medição ($v_t$):** Representa o ruído ou incerteza nas medições. Assume-se geralmente que $v_t \sim N(0, R)$, onde $R$ é a matriz de covariância do ruído de medição.

### Exemplo Simples: Modelo de Caminhada Aleatória com Deriva

Considere um modelo de caminhada aleatória com deriva, onde a posição no tempo $t$ é igual à posição no tempo $t-1$ mais uma deriva constante ($\delta$) e um ruído aleatório.

*   **Equação de Estado:**

    $$
    x_t = x_{t-1} + \delta + w_t
    $$

    Aqui, $x_t$ representa a posição no tempo $t$, e $w_t$ é o ruído do processo.

*   **Equação de Medição:**

    $$
    y_t = x_t + v_t
    $$

    Aqui, $y_t$ é a medição da posição no tempo $t$, e $v_t$ é o ruído de medição.

Neste caso:

*   $F = 1$
*   $G = 1$ (multiplicando a deriva $\delta$)
*   $u_t = \delta$ (a deriva constante)
*   $H = 1$

### Filtragem de Kalman

O Filtro de Kalman é um algoritmo recursivo que estima o estado de um sistema dinâmico ao longo do tempo, utilizando uma série de medições ruidosas. Ele é amplamente utilizado em modelos de espaço de estados para inferir o estado não observável do sistema.

O Filtro de Kalman opera em duas etapas:

1.  **Predição:** Estima o estado e a covariância do erro do estado no próximo período, com base no estado e na covariância do erro do estado no período atual.

    *   Estado Predito:

        $$
        \hat{x}_{t|t-1} = F \hat{x}_{t-1|t-1} + G u_t
        $$

    *   Covariância do Erro do Estado Predito:

        $$
        P_{t|t-1} = F P_{t-1|t-1} F^T + Q
        $$
2.  **Atualização:** Atualiza a estimativa do estado e a covariância do erro do estado, com base na medição mais recente.

    *   Ganho de Kalman:

        $$
        K_t = P_{t|t-1} H^T (H P_{t|t-1} H^T + R)^{-1}
        $$

    *   Estado Atualizado:

        $$
        \hat{x}_{t|t} = \hat{x}_{t|t-1} + K_t (y_t - H \hat{x}_{t|t-1})
        $$

    *   Covariância do Erro do Estado Atualizado:

        $$
        P_{t|t} = (I - K_t H) P_{t|t-1}
        $$

Onde:

*   $\hat{x}_{t|t-1}$ é a estimativa do estado no tempo $t$ dado as informações até o tempo $t-1$.
*   $P_{t|t-1}$ é a covariância do erro do estado no tempo $t$ dado as informações até o tempo $t-1$.
*   $K_t$ é o ganho de Kalman no tempo $t$.
*   $\hat{x}_{t|t}$ é a estimativa do estado no tempo $t$ dado as informações até o tempo $t$.
*   $P_{t|t}$ é a covariância do erro do estado no tempo $t$ dado as informações até o tempo $t$.

### Aplicações

Modelos de espaço de estados e o Filtro de Kalman têm diversas aplicações, incluindo:

*   **Navegação e Rastreamento:** Estimativa da posição e velocidade de um objeto em movimento.
*   **Controle de Processos:** Otimização e controle de processos industriais.
*   **Economia e Finanças:** Modelagem de séries temporais financeiras, previsão de preços de ações e modelagem macroeconômica.
*   **Meteorologia:** Previsão do tempo e modelagem climática.
*   **Engenharia Aeroespacial:** Controle de voo e navegação de satélites.

### Vantagens e Desvantagens

**Vantagens:**

*   Flexibilidade para modelar sistemas dinâmicos complexos.
*   Capacidade de lidar com componentes não observáveis.
*   Incorporação de ruído e incerteza de forma explícita.
*   Estimativa ótima do estado do sistema, dadas as medições disponíveis.

**Desvantagens:**

*   Complexidade na formulação e implementação do modelo.
*   Necessidade de especificar as matrizes do modelo ($F$, $G$, $H$, $Q$, $R$).
*   Sensibilidade à especificação correta das distribuições de ruído.
*   Custos computacionais, especialmente para sistemas de alta dimensão.

### Referências Adicionais

*   **"Time Series Analysis by State Space Methods"** de James Durbin e Siem Jan Koopman.
*   **"Kalman Filtering and Smoothing: With MATLAB Exercises"** de Simo Särkkä.
*   **"State-Space Models with Regime Switching: Econometric Analysis of Financial and Economic Time Series"** de Marcelle Chauvet e Ilyascheff Mauricio.

<!-- END -->