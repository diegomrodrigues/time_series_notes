## Expectativa Condicional e Incondicional em SÃ©ries Temporais

### IntroduÃ§Ã£o

Este capÃ­tulo aprofunda a discussÃ£o sobre a **expectativa** de uma sÃ©rie temporal, diferenciando entre **expectativa incondicional** e **condicional**, e explorando como a mÃ©dia $E(Y_t)$ ou $\mu_t$ pode variar em funÃ§Ã£o do tempo *t* [^1]. Expandindo os conceitos introduzidos no capÃ­tulo anterior, exploraremos exemplos onde a mÃ©dia Ã© constante e onde ela depende explicitamente do tempo, como em processos com tendÃªncia. TambÃ©m, serÃ¡ explorado os diferentes modelos, como o $MA(1)$, $AR(1)$, $ARMA(p,q)$, *White Noise*, RuÃ­do Branco Gaussiano e as aplicaÃ§Ãµes de cada um.

### Expectativa Incondicional vs. Condicional

Conforme vimos anteriormente, a **expectativa incondicional** de uma sÃ©rie temporal $Y_t$, denotada como $E(Y_t)$ ou $\mu_t$, representa o valor mÃ©dio de $Y_t$ sem considerar qualquer informaÃ§Ã£o adicional sobre o estado do processo. Ela Ã© calculada integrando sobre toda a distribuiÃ§Ã£o de probabilidade de $Y_t$ [^1]:

$$E(Y_t) = \int_{-\infty}^{\infty} y_t f_{Y_t}(y_t) \, dy_t$$

Em contraste, a **expectativa condicional** de $Y_t$, denotada como $E(Y_t | \mathcal{F}_{t-1})$, representa o valor mÃ©dio de $Y_t$ dado o conhecimento do histÃ³rico do processo atÃ© o instante $t-1$, representado por $\mathcal{F}_{t-1}$. Aqui, $\mathcal{F}_{t-1}$ Ã© uma sigma-Ã¡lgebra que formaliza a noÃ§Ã£o de "informaÃ§Ã£o disponÃ­vel atÃ© o tempo $t-1$". Essa informaÃ§Ã£o pode incluir valores passados da sÃ©rie temporal, outras variÃ¡veis relevantes, ou ambos.

Para ilustrar, consideremos um modelo AR(1) com uma constante, apresentado no capÃ­tulo anterior:

$$Y_t = c + \phi Y_{t-1} + \epsilon_t$$

A **expectativa incondicional**, como demonstrado, Ã©:

$$E(Y_t) = \frac{c}{1 - \phi}$$

*Prova:*
I. Tomamos a esperanÃ§a incondicional de ambos os lados da equaÃ§Ã£o: $E(Y_t) = E(c + \phi Y_{t-1} + \epsilon_t)$.

II. Usamos a propriedade da linearidade da esperanÃ§a: $E(Y_t) = E(c) + E(\phi Y_{t-1}) + E(\epsilon_t)$.

III. Como $c$ Ã© uma constante, $E(c) = c$. TambÃ©m, $\phi$ Ã© uma constante, entÃ£o $E(\phi Y_{t-1}) = \phi E(Y_{t-1})$.  Assumimos que $E(\epsilon_t) = 0$, pois $\epsilon_t$ Ã© ruÃ­do branco.

IV. Assumimos que o processo Ã© estacionÃ¡rio, o que implica que $E(Y_t) = E(Y_{t-1})$.  Denotamos essa esperanÃ§a comum por $\mu$, entÃ£o $\mu = c + \phi \mu$.

V. Resolvemos para $\mu$: $\mu - \phi \mu = c \Rightarrow \mu(1 - \phi) = c \Rightarrow \mu = \frac{c}{1 - \phi}$. Portanto, $E(Y_t) = \frac{c}{1 - \phi}$. â– 

No entanto, a **expectativa condicional** de $Y_t$ dado $\mathcal{F}_{t-1}$ Ã©:

$$E(Y_t | \mathcal{F}_{t-1}) = E(c + \phi Y_{t-1} + \epsilon_t | \mathcal{F}_{t-1}) = c + \phi Y_{t-1} + E(\epsilon_t | \mathcal{F}_{t-1}) = c + \phi Y_{t-1}$$

Uma vez que $Y_{t-1}$ Ã© conhecido em $\mathcal{F}_{t-1}$, ele se comporta como uma constante ao tomar a expectativa condicional. Assumimos tambÃ©m que $E(\epsilon_t | \mathcal{F}_{t-1}) = 0$, o que Ã© razoÃ¡vel se $\epsilon_t$ Ã© ruÃ­do branco independente do passado.

> ðŸ’¡ **Exemplo NumÃ©rico (Expectativa Condicional):**
>
> Suponha que temos um modelo AR(1) definido por $Y_t = 1 + 0.7Y_{t-1} + \epsilon_t$, onde $\epsilon_t$ Ã© ruÃ­do branco com mÃ©dia zero e desvio padrÃ£o 1. Vamos gerar uma sÃ©rie temporal de 100 pontos e calcular a expectativa condicional para o ponto 100, dado o valor do ponto 99.
>
> Definimos $c = 1$ e $\phi = 0.7$. Se observarmos que $Y_{99} = 3$, entÃ£o a expectativa condicional de $Y_{100}$ dado $Y_{99}$ Ã©:
>
> $E(Y_{100} | Y_{99} = 3) = 1 + 0.7 \times 3 = 1 + 2.1 = 3.1$.
>
> Simularemos a sÃ©rie para verificar este resultado:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # ParÃ¢metros do modelo
> c = 1
> phi = 0.7
> sigma = 1  # Desvio padrÃ£o do ruÃ­do branco
>
> # InicializaÃ§Ã£o
> np.random.seed(42)  # Para reprodutibilidade
> n = 100
> epsilon = np.random.normal(0, sigma, n)  # RuÃ­do branco
> Y = np.zeros(n)
> Y[0] = np.random.normal(c / (1 - phi), sigma)  # InicializaÃ§Ã£o do primeiro valor
>
> # GeraÃ§Ã£o da sÃ©rie temporal
> for t in range(1, n):
>     Y[t] = c + phi * Y[t-1] + epsilon[t]
>
> # Valor de Y[99] (o Ã­ndice em Python Ã© 98)
> Y99 = Y[98]
>
> # Expectativa condicional de Y[100]
> E_Y100_cond = c + phi * Y99
>
> # Expectativa incondicional de Y
> E_Y_incond = c / (1 - phi)
>
> print(f"Y[99] = {Y99:.2f}")
> print(f"Expectativa Condicional E[Y_100 | Y_99] = {E_Y100_cond:.2f}")
> print(f"Expectativa Incondicional E[Y] = {E_Y_incond:.2f}")
>
> # Plot da sÃ©rie temporal
> plt.figure(figsize=(10, 6))
> plt.plot(Y, label='SÃ©rie Temporal Y_t')
> plt.scatter(98, Y99, color='red', label=f'Y_99 = {Y99:.2f}')
> plt.scatter(99, E_Y100_cond, color='green', label=f'E[Y_100 | Y_99] = {E_Y100_cond:.2f}')
> plt.title('SÃ©rie Temporal AR(1) com Expectativa Condicional')
> plt.xlabel('Tempo (t)')
> plt.ylabel('Valor Y_t')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> No exemplo simulado, observamos o valor de $Y_{99}$ e calculamos a expectativa condicional de $Y_{100}$ com base nesse valor. A visualizaÃ§Ã£o permite comparar a expectativa condicional com os valores reais da sÃ©rie.

> ðŸ’¡ **Exemplo NumÃ©rico (Expectativa Incondicional):**
>
> Usando o mesmo modelo AR(1) $Y_t = 1 + 0.7Y_{t-1} + \epsilon_t$, calculamos a expectativa incondicional:
>
> $E(Y_t) = \frac{c}{1 - \phi} = \frac{1}{1 - 0.7} = \frac{1}{0.3} \approx 3.33$.
>
> Isso representa o valor mÃ©dio da sÃ©rie temporal a longo prazo, independentemente de qualquer valor especÃ­fico observado em um determinado momento.

> ðŸ’¡ **Exemplo NumÃ©rico (Expectativa Condicional):**
>
> Suponha que temos um modelo AR(1) definido por $Y_t = 1 + 0.7Y_{t-1} + \epsilon_t$, e sabemos que $Y_9 = 5$. Vamos calcular a expectativa condicional de $Y_{10}$ dado $Y_9$.
>
> Usando a fÃ³rmula $E(Y_{10} | Y_9) = c + \phi Y_9$, temos:
> $E(Y_{10} | Y_9 = 5) = 1 + 0.7 \times 5 = 1 + 3.5 = 4.5$.
>
> Portanto, a expectativa condicional de $Y_{10}$ dado que $Y_9 = 5$ Ã© 4.5. Note que esta Ã© diferente da expectativa incondicional, que seria $\frac{1}{1 - 0.7} = \frac{1}{0.3} \approx 3.33$.
>
> ```python
> import numpy as np
>
> # ParÃ¢metros
> c = 1
> phi = 0.7
> sigma = 1
>
> # Valor conhecido de Y[9]
> Y9 = 5
>
> # CÃ¡lculo da expectativa condicional
> E_Y10_cond = c + phi * Y9
>
> # Expectativa incondicional
> E_Y_incond = c / (1 - phi)
>
> print(f"Expectativa Condicional E[Y_10 | Y_9 = {Y9}]: {E_Y10_cond}")
> print(f"Expectativa Incondicional E[Y_t]: {E_Y_incond}")
> ```

*Prova:*
I. ComeÃ§amos com o modelo AR(1): $Y_t = c + \phi Y_{t-1} + \epsilon_t$.

II. Tomamos a expectativa condicional de ambos os lados, dado $\mathcal{F}_{t-1}$: $E(Y_t | \mathcal{F}_{t-1}) = E(c + \phi Y_{t-1} + \epsilon_t | \mathcal{F}_{t-1})$.

III. Usamos a propriedade da linearidade da expectativa condicional: $E(Y_t | \mathcal{F}_{t-1}) = E(c | \mathcal{F}_{t-1}) + E(\phi Y_{t-1} | \mathcal{F}_{t-1}) + E(\epsilon_t | \mathcal{F}_{t-1})$.

IV. Como $c$ e $\phi$ sÃ£o constantes, $E(c | \mathcal{F}_{t-1}) = c$ e $E(\phi Y_{t-1} | \mathcal{F}_{t-1}) = \phi E(Y_{t-1} | \mathcal{F}_{t-1}) = \phi Y_{t-1}$, jÃ¡ que $Y_{t-1}$ Ã© conhecido em $\mathcal{F}_{t-1}$.

V. Assumimos que o ruÃ­do branco Ã© independente do passado, entÃ£o $E(\epsilon_t | \mathcal{F}_{t-1}) = 0$.

VI. Portanto, $E(Y_t | \mathcal{F}_{t-1}) = c + \phi Y_{t-1}$. â– 

**Teorema 1** (Lei da Expectativa Iterada): Para quaisquer variÃ¡veis aleatÃ³rias $X$ e $Y$, vale a seguinte identidade:

$$E[X] = E[E[X|Y]]$$

Esta lei Ã© fundamental para relacionar expectativas condicionais e incondicionais. Ela afirma que a expectativa incondicional de uma variÃ¡vel aleatÃ³ria pode ser obtida tomando a expectativa da expectativa condicional dessa variÃ¡vel, dada outra variÃ¡vel aleatÃ³ria.

*Prova:* A prova formal da Lei da Expectativa Iterada envolve argumentos de teoria da medida. Informalmente, ela segue da definiÃ§Ã£o de esperanÃ§a condicional como a melhor previsÃ£o de $X$ dado $Y$. Ao tomar a esperanÃ§a dessa melhor previsÃ£o, obtemos a esperanÃ§a incondicional de $X$. â– 

**Teorema 1.1** Dado um modelo AR(1) definido como $Y_t = c + \phi Y_{t-1} + \epsilon_t$, onde $\epsilon_t$ Ã© ruÃ­do branco com mÃ©dia zero e variÃ¢ncia $\sigma^2$, podemos derivar a esperanÃ§a incondicional utilizando a Lei da Expectativa Iterada.

*Prova:*
I. Aplicando a Lei da Expectativa Iterada: $E[Y_t] = E[E[Y_t | \mathcal{F}_{t-1}]]$.

II. Substituindo $E[Y_t | \mathcal{F}_{t-1}]$ pela sua expressÃ£o no modelo AR(1): $E[Y_t] = E[c + \phi Y_{t-1}]$.

III. Utilizando a linearidade da esperanÃ§a: $E[Y_t] = E[c] + E[\phi Y_{t-1}] = c + \phi E[Y_{t-1}]$.

IV. Assumindo estacionariedade (i.e., $E[Y_t] = E[Y_{t-1}] = \mu$): $\mu = c + \phi \mu$.

V. Resolvendo para $\mu$: $\mu = \frac{c}{1 - \phi}$, que Ã© a esperanÃ§a incondicional. â– 

### Expectativa como FunÃ§Ã£o do Tempo

A notaÃ§Ã£o $\mu_t$ para a expectativa incondicional $E(Y_t)$ reconhece explicitamente que a mÃ©dia pode ser uma funÃ§Ã£o do tempo *t* [^1]. Isso Ã© particularmente relevante em processos nÃ£o estacionÃ¡rios, como aqueles com uma tendÃªncia determinÃ­stica.

Considere novamente o processo com tendÃªncia linear:

$$Y_t = \beta t + \epsilon_t$$

Neste caso, a mÃ©dia incondicional Ã©:

$$E(Y_t) = \mu_t = \beta t$$

A mÃ©dia varia linearmente com o tempo, refletindo a tendÃªncia do processo. Este Ã© um exemplo onde a expectativa incondicional nÃ£o Ã© constante [^1].

> ðŸ’¡ **Exemplo NumÃ©rico (TendÃªncia Linear):**
>
> Suponha que temos uma sÃ©rie temporal que representa o nÃºmero de passageiros de uma companhia aÃ©rea ao longo dos meses. Observamos que hÃ¡ um crescimento linear ao longo do tempo. Podemos modelar isso como $Y_t = 100 + 5t + \epsilon_t$, onde $Y_t$ Ã© o nÃºmero de passageiros no mÃªs *t*, 100 Ã© o nÃºmero inicial de passageiros, 5 Ã© o incremento mÃ©dio mensal, e $\epsilon_t$ Ã© um erro aleatÃ³rio com mÃ©dia zero.
>
> Neste caso, a expectativa incondicional no tempo *t* Ã©:
>
> $E(Y_t) = 100 + 5t$
>
> Isso significa que, em mÃ©dia, esperamos que o nÃºmero de passageiros aumente 5 por mÃªs, comeÃ§ando com uma base de 100 passageiros. No mÃªs 20, por exemplo, a expectativa seria $E(Y_{20}) = 100 + 5 \times 20 = 200$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # ParÃ¢metros
> beta0 = 100
> beta1 = 5
> sigma = 10  # Desvio padrÃ£o do ruÃ­do branco
>
> # GeraÃ§Ã£o da sÃ©rie temporal
> np.random.seed(42)
> n = 60  # 5 anos de dados mensais
> t = np.arange(1, n + 1)
> epsilon = np.random.normal(0, sigma, n)
> Y = beta0 + beta1 * t + epsilon
>
> # Expectativa incondicional
> E_Y = beta0 + beta1 * t
>
> # Plot da sÃ©rie temporal
> plt.figure(figsize=(10, 6))
> plt.plot(t, Y, label='SÃ©rie Temporal Y_t')
> plt.plot(t, E_Y, color='red', linestyle='--', label='Expectativa Incondicional E[Y_t]')
> plt.title('SÃ©rie Temporal com TendÃªncia Linear')
> plt.xlabel('Tempo (t)')
> plt.ylabel('Valor Y_t')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```

> ðŸ’¡ **Exemplo NumÃ©rico (TendÃªncia NÃ£o-Linear):**
>
> Suponha que uma sÃ©rie temporal segue uma tendÃªncia quadrÃ¡tica: $Y_t = \alpha + \beta t + \gamma t^2 + \epsilon_t$, onde $\alpha$, $\beta$, e $\gamma$ sÃ£o constantes, e $\epsilon_t$ Ã© ruÃ­do branco. EntÃ£o, a expectativa incondicional Ã©:
>
> $$E(Y_t) = E(\alpha + \beta t + \gamma t^2 + \epsilon_t) = \alpha + \beta t + \gamma t^2$$
>
> A mÃ©dia varia quadraticamente com o tempo.

> ðŸ’¡ **Exemplo NumÃ©rico (TendÃªncia NÃ£o-Linear):**
>
> Considere uma sÃ©rie temporal que modela o crescimento de uma populaÃ§Ã£o de bactÃ©rias em um ambiente controlado. O crescimento pode ser modelado como uma funÃ§Ã£o quadrÃ¡tica do tempo: $Y_t = 10 + 2t + 0.5t^2 + \epsilon_t$, onde $Y_t$ Ã© o nÃºmero de bactÃ©rias no tempo *t*, 10 Ã© o nÃºmero inicial de bactÃ©rias, 2 e 0.5 sÃ£o os coeficientes que determinam a taxa de crescimento, e $\epsilon_t$ Ã© um termo de erro aleatÃ³rio.
>
> Neste caso, a expectativa incondicional Ã©:
>
> $E(Y_t) = 10 + 2t + 0.5t^2$
>
> Isso significa que, em mÃ©dia, esperamos que a populaÃ§Ã£o de bactÃ©rias cresÃ§a de forma acelerada ao longo do tempo. No tempo 10, por exemplo, a expectativa seria $E(Y_{10}) = 10 + 2 \times 10 + 0.5 \times 10^2 = 10 + 20 + 50 = 80$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # ParÃ¢metros
> alpha = 10
> beta = 2
> gamma = 0.5
> sigma = 5  # Desvio padrÃ£o do ruÃ­do branco
>
> # GeraÃ§Ã£o da sÃ©rie temporal
> np.random.seed(42)
> n = 20
> t = np.arange(1, n + 1)
> epsilon = np.random.normal(0, sigma, n)
> Y = alpha + beta * t + gamma * t**2 + epsilon
>
> # Expectativa incondicional
> E_Y = alpha + beta * t + gamma * t**2
>
> # Plot da sÃ©rie temporal
> plt.figure(figsize=(10, 6))
> plt.plot(t, Y, label='SÃ©rie Temporal Y_t')
> plt.plot(t, E_Y, color='red', linestyle='--', label='Expectativa Incondicional E[Y_t]')
> plt.title('SÃ©rie Temporal com TendÃªncia QuadrÃ¡tica')
> plt.xlabel('Tempo (t)')
> plt.ylabel('Valor Y_t')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```

**ProposiÃ§Ã£o 1:** Para um processo com tendÃªncia polinomial de grau *n*, dado por:

$$Y_t = \sum_{i=0}^{n} \beta_i t^i + \epsilon_t$$

Onde $\beta_i$ sÃ£o constantes e $\epsilon_t$ Ã© ruÃ­do branco com $E[\epsilon_t] = 0$, a expectativa incondicional no tempo *t* Ã©:

$$E[Y_t] = \sum_{i=0}^{n} \beta_i t^i$$

*Prova:*
I. Tomamos a expectativa de ambos os lados da equaÃ§Ã£o: $E[Y_t] = E[\sum_{i=0}^{n} \beta_i t^i + \epsilon_t]$.

II. Aplicamos a linearidade da expectativa: $E[Y_t] = \sum_{i=0}^{n} E[\beta_i t^i] + E[\epsilon_t]$.

III. Como $\beta_i$ sÃ£o constantes e *t* Ã© determinÃ­stico, $E[\beta_i t^i] = \beta_i t^i$.

IV. Dado que $E[\epsilon_t] = 0$, temos: $E[Y_t] = \sum_{i=0}^{n} \beta_i t^i$. â– 

### ImplicaÃ§Ãµes para Modelagem e PrevisÃ£o

A distinÃ§Ã£o entre expectativa incondicional e condicional, e a possibilidade de a mÃ©dia variar com o tempo, tÃªm implicaÃ§Ãµes significativas para a modelagem e previsÃ£o de sÃ©ries temporais.

*   **Estacionariedade:** Processos estacionÃ¡rios tÃªm mÃ©dia e variÃ¢ncia constantes ao longo do tempo. Modelos que impÃµem estacionariedade (e.g., ARMA com raÃ­zes dentro do cÃ­rculo unitÃ¡rio) implicam que a expectativa incondicional Ã© constante.
*   **PrevisÃ£o:** A expectativa condicional $E(Y_{t+h} | \mathcal{F}_t)$ Ã© frequentemente usada como a melhor previsÃ£o de $Y_{t+h}$ com base na informaÃ§Ã£o disponÃ­vel no tempo *t*. Em modelos ARMA, a previsÃ£o Ã³tima pode ser expressa recursivamente em termos de valores passados da sÃ©rie e dos resÃ­duos.
*   **RemoÃ§Ã£o de TendÃªncia:** Em sÃ©ries temporais com tendÃªncia, Ã© comum remover a tendÃªncia antes de aplicar modelos estacionÃ¡rios como ARMA. Isso pode ser feito modelando a tendÃªncia determinÃ­stica (e.g., com uma regressÃ£o em *t*) e subtraindo-a da sÃ©rie original. A sÃ©rie resultante, livre da tendÃªncia, pode entÃ£o ser modelada com um modelo estacionÃ¡rio.

> ðŸ’¡ **Exemplo NumÃ©rico (RemoÃ§Ã£o de TendÃªncia):**
>
> Suponha que temos uma sÃ©rie temporal de vendas mensais de um produto, que mostra uma tendÃªncia de crescimento linear ao longo do tempo. Modelamos a tendÃªncia como $T_t = 100 + 2t$, onde $T_t$ Ã© a tendÃªncia no mÃªs *t*. Estimamos a tendÃªncia usando uma regressÃ£o linear e obtemos $\hat{T}_t = 98 + 2.1t$. Para remover a tendÃªncia, subtraÃ­mos a tendÃªncia estimada da sÃ©rie original: $Y_t - \hat{T}_t$. A sÃ©rie resultante deve ser aproximadamente estacionÃ¡ria, permitindo a aplicaÃ§Ã£o de modelos ARMA.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LinearRegression
>
> # Dados de exemplo (vendas mensais)
> np.random.seed(42)
> n = 60  # 5 anos de dados mensais
> t = np.arange(1, n + 1)
> T = 100 + 2 * t  # TendÃªncia real
> epsilon = np.random.normal(0, 10, n)  # RuÃ­do branco
> Y = T + epsilon  # SÃ©rie temporal original
>
> # EstimaÃ§Ã£o da tendÃªncia usando regressÃ£o linear
> model = LinearRegression()
> model.fit(t.reshape(-1, 1), Y)
> T_hat = model.predict(t.reshape(-1, 1))  # TendÃªncia estimada
>
> # RemoÃ§Ã£o da tendÃªncia
> Y_detrended = Y - T_hat
>
> # Plot da sÃ©rie temporal original e da sÃ©rie detrended
> plt.figure(figsize=(12, 6))
> plt.subplot(1, 2, 1)
> plt.plot(t, Y, label='SÃ©rie Temporal Original Y_t')
> plt.plot(t, T_hat, color='red', linestyle='--', label='TendÃªncia Estimada T_hat')
> plt.title('SÃ©rie Temporal Original com TendÃªncia')
> plt.xlabel('Tempo (t)')
> plt.ylabel('Vendas')
> plt.legend()
> plt.grid(True)
>
> plt.subplot(1, 2, 2)
> plt.plot(t, Y_detrended, label='SÃ©rie Temporal Detrended')
> plt.title('SÃ©rie Temporal Detrended')
> plt.xlabel('Tempo (t)')
> plt.ylabel('Vendas Detrended')
> plt.legend()
> plt.grid(True)
>
> plt.tight_layout()
> plt.show()
> ```
>
> A visualizaÃ§Ã£o mostra a sÃ©rie temporal original com a tendÃªncia estimada e a sÃ©rie detrended resultante. A sÃ©rie detrended deve apresentar um comportamento mais estacionÃ¡rio, facilitando a modelagem com modelos ARMA.

**Lema 1:** Seja $Y_t$ uma sÃ©rie temporal com tendÃªncia determinÃ­stica $T_t$ e um componente estacionÃ¡rio $S_t$, tal que $Y_t = T_t + S_t$. Se modelarmos a tendÃªncia determinÃ­stica $T_t$ corretamente, entÃ£o a sÃ©rie $Y_t - \hat{T}_t$, onde $\hat{T}_t$ Ã© a estimativa de $T_t$, resultarÃ¡ em uma sÃ©rie estacionÃ¡ria com mÃ©dia aproximadamente zero.

*Prova:*
I. Definimos $Y_t = T_t + S_t$, onde $T_t$ Ã© a tendÃªncia determinÃ­stica e $S_t$ Ã© o componente estacionÃ¡rio.

II. Estimamos a tendÃªncia determinÃ­stica $T_t$ usando algum mÃ©todo de modelagem, obtendo $\hat{T}_t$.

III. SubtraÃ­mos a tendÃªncia estimada da sÃ©rie original: $Y_t - \hat{T}_t = (T_t + S_t) - \hat{T}_t = (T_t - \hat{T}_t) + S_t$.

IV. Se o modelo para a tendÃªncia for correto, entÃ£o $\hat{T}_t$ serÃ¡ uma boa aproximaÃ§Ã£o de $T_t$, e $T_t - \hat{T}_t$ serÃ¡ prÃ³ximo de zero.

V. Portanto, $Y_t - \hat{T}_t \approx S_t$, que Ã© uma sÃ©rie estacionÃ¡ria. AlÃ©m disso, se a mÃ©dia de $S_t$ Ã© zero, entÃ£o a mÃ©dia de $Y_t - \hat{T}_t$ tambÃ©m serÃ¡ aproximadamente zero. â– 

### Modelos e EsperanÃ§a MatemÃ¡tica

A esperanÃ§a matemÃ¡tica varia consideravelmente de acordo com o modelo estatÃ­stico aplicado aos dados, apresentaremos alguns modelos e suas esperanÃ§as:

*   **White Noise:** $E[Y_t] = 0$
*   **RuÃ­do Branco Gaussiano:** $E[Y_t] = \mu$
*   **MA(1):** $Y_t = \mu + \epsilon_t + \theta\epsilon_{t-1}$; $E[Y_t] = \mu$
*   **AR(1):** $Y_t = c + \phi Y_{t-1} + \epsilon_t$; $E[Y_t] = c/(1 - \phi)$
*   **ARMA(p,q):** $Y_t = c + \phi_1Y_{t-1} + \phi_2Y_{t-2} + \ldots + \phi_pY_{t-p} + \epsilon_t + \theta_1\epsilon_{t-1} + \theta_2\epsilon_{t-2} + \ldots + \theta_q\epsilon_{t-q}$

**Teorema 2:** (EsperanÃ§a para Modelos MA(q)) Dado um modelo MA(q) definido por:

$$Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \ldots + \theta_q \epsilon_{t-q}$$

Onde $\mu$ Ã© uma constante e $\epsilon_t$ Ã© ruÃ­do branco com $E[\epsilon_t] = 0$, entÃ£o a esperanÃ§a incondicional de $Y_t$ Ã© $E[Y_t] = \mu$.

*Prova:*
I. Tomamos a expectativa de ambos os lados da equaÃ§Ã£o: $E[Y_t] = E[\mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \ldots + \theta_q \epsilon_{t-q}]$.

II. Aplicamos a linearidade da expectativa: $E[Y_t] = E[\mu] + E[\epsilon_t] + \theta_1 E[\epsilon_{t-1}] + \theta_2 E[\epsilon_{t-2}] + \ldots + \theta_q E[\epsilon_{t-q}]$.

III. Como $\mu$ Ã© uma constante, $E[\mu] = \mu$.

IV. Dado que $E[\epsilon_t] = 0$ para todo *t*, todos os termos $E[\epsilon_{t-i}]$ sÃ£o zero.

V. Portanto, $E[Y_t] = \mu + 0 + 0 + \ldots + 0 = \mu$. â– 

> ðŸ’¡ **Exemplo NumÃ©rico (Modelo MA(1)):**
>
> Considere um modelo MA(1) definido por $Y_t = 5 + \epsilon_t + 0.8\epsilon_{t-1}$, onde $\mu = 5$ e $\theta_1 = 0.8$. A esperanÃ§a incondicional de $Y_t$ Ã© simplesmente $E[Y_t] = \mu = 5$. Isso significa que, em mÃ©dia, esperamos que a sÃ©rie temporal oscile em torno do valor 5.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # ParÃ¢metros
> mu = 5
> theta = 0.8
> sigma = 1  # Desvio padrÃ£o do ruÃ­do branco
>
> # GeraÃ§Ã£o da sÃ©rie temporal
> np.random.seed(42)
> n = 100
> epsilon = np.random.normal(0, sigma, n)
> Y = np.zeros(n)
> Y[0] = mu + epsilon[0]
> for t in range(1, n):
>     Y[t] = mu + epsilon[t] + theta * epsilon[t-1]
>
> # Expectativa incondicional
> E_Y = mu
>
> # Plot da sÃ©rie temporal
> plt.figure(figsize=(10, 6))
> plt.plot(Y, label='SÃ©rie Temporal MA(1)')
> plt.axhline(y=E_Y, color='red', linestyle='--', label='Expectativa Incondicional E[Y_t]')
> plt.title('SÃ©rie Temporal MA(1) com Expectativa Incondicional')
> plt.xlabel('Tempo (t)')
> plt.ylabel('Valor Y_t')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```

### ConclusÃ£o

Este capÃ­tulo detalhou a importÃ¢ncia da **expectativa** em sÃ©ries temporais, distinguindo entre as versÃµes **incondicional** e **condicional**. A capacidade de a mÃ©dia variar com o tempo foi ilustrada com exemplos de processos com tendÃªncia. A correta compreensÃ£o e modelagem da expectativa sÃ£o cruciais para a anÃ¡lise, previsÃ£o e interpretaÃ§Ã£o de sÃ©ries temporais. A escolha entre modelos estacionÃ¡rios e nÃ£o estacionÃ¡rios, e o tratamento adequado de tendÃªncias, dependem fundamentalmente da compreensÃ£o do comportamento da expectativa da sÃ©rie.

### ReferÃªncias

[^1]: PÃ¡gina 44 do texto original.
<!-- END -->