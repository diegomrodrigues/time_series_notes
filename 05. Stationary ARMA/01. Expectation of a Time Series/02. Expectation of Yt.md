## Expectativa Condicional e Incondicional em S√©ries Temporais

### Introdu√ß√£o

Este cap√≠tulo aprofunda a discuss√£o sobre a **expectativa** de uma s√©rie temporal, diferenciando entre **expectativa incondicional** e **condicional**, e explorando como a m√©dia $E(Y_t)$ ou $\mu_t$ pode variar em fun√ß√£o do tempo *t* [^1]. Expandindo os conceitos introduzidos no cap√≠tulo anterior, exploraremos exemplos onde a m√©dia √© constante e onde ela depende explicitamente do tempo, como em processos com tend√™ncia. Tamb√©m, ser√° explorado os diferentes modelos, como o $MA(1)$, $AR(1)$, $ARMA(p,q)$, *White Noise*, Ru√≠do Branco Gaussiano e as aplica√ß√µes de cada um.

### Expectativa Incondicional vs. Condicional

Conforme vimos anteriormente, a **expectativa incondicional** de uma s√©rie temporal $Y_t$, denotada como $E(Y_t)$ ou $\mu_t$, representa o valor m√©dio de $Y_t$ sem considerar qualquer informa√ß√£o adicional sobre o estado do processo. Ela √© calculada integrando sobre toda a distribui√ß√£o de probabilidade de $Y_t$ [^1]:

$$E(Y_t) = \int_{-\infty}^{\infty} y_t f_{Y_t}(y_t) \, dy_t$$

Em contraste, a **expectativa condicional** de $Y_t$, denotada como $E(Y_t | \mathcal{F}_{t-1})$, representa o valor m√©dio de $Y_t$ dado o conhecimento do hist√≥rico do processo at√© o instante $t-1$, representado por $\mathcal{F}_{t-1}$. Aqui, $\mathcal{F}_{t-1}$ √© uma sigma-√°lgebra que formaliza a no√ß√£o de "informa√ß√£o dispon√≠vel at√© o tempo $t-1$". Essa informa√ß√£o pode incluir valores passados da s√©rie temporal, outras vari√°veis relevantes, ou ambos.

Para ilustrar, consideremos um modelo AR(1) com uma constante, apresentado no cap√≠tulo anterior:

$$Y_t = c + \phi Y_{t-1} + \epsilon_t$$

A **expectativa incondicional**, como demonstrado, √©:

$$E(Y_t) = \frac{c}{1 - \phi}$$

*Prova:*
I. Tomamos a esperan√ßa incondicional de ambos os lados da equa√ß√£o: $E(Y_t) = E(c + \phi Y_{t-1} + \epsilon_t)$.

II. Usamos a propriedade da linearidade da esperan√ßa: $E(Y_t) = E(c) + E(\phi Y_{t-1}) + E(\epsilon_t)$.

III. Como $c$ √© uma constante, $E(c) = c$. Tamb√©m, $\phi$ √© uma constante, ent√£o $E(\phi Y_{t-1}) = \phi E(Y_{t-1})$.  Assumimos que $E(\epsilon_t) = 0$, pois $\epsilon_t$ √© ru√≠do branco.

IV. Assumimos que o processo √© estacion√°rio, o que implica que $E(Y_t) = E(Y_{t-1})$.  Denotamos essa esperan√ßa comum por $\mu$, ent√£o $\mu = c + \phi \mu$.

V. Resolvemos para $\mu$: $\mu - \phi \mu = c \Rightarrow \mu(1 - \phi) = c \Rightarrow \mu = \frac{c}{1 - \phi}$. Portanto, $E(Y_t) = \frac{c}{1 - \phi}$. ‚ñ†

No entanto, a **expectativa condicional** de $Y_t$ dado $\mathcal{F}_{t-1}$ √©:

$$E(Y_t | \mathcal{F}_{t-1}) = E(c + \phi Y_{t-1} + \epsilon_t | \mathcal{F}_{t-1}) = c + \phi Y_{t-1} + E(\epsilon_t | \mathcal{F}_{t-1}) = c + \phi Y_{t-1}$$

Uma vez que $Y_{t-1}$ √© conhecido em $\mathcal{F}_{t-1}$, ele se comporta como uma constante ao tomar a expectativa condicional. Assumimos tamb√©m que $E(\epsilon_t | \mathcal{F}_{t-1}) = 0$, o que √© razo√°vel se $\epsilon_t$ √© ru√≠do branco independente do passado.

> üí° **Exemplo Num√©rico (Expectativa Condicional):**
>
> Suponha que temos um modelo AR(1) definido por $Y_t = 1 + 0.7Y_{t-1} + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco com m√©dia zero e desvio padr√£o 1. Vamos gerar uma s√©rie temporal de 100 pontos e calcular a expectativa condicional para o ponto 100, dado o valor do ponto 99.
>
> Definimos $c = 1$ e $\phi = 0.7$. Se observarmos que $Y_{99} = 3$, ent√£o a expectativa condicional de $Y_{100}$ dado $Y_{99}$ √©:
>
> $E(Y_{100} | Y_{99} = 3) = 1 + 0.7 \times 3 = 1 + 2.1 = 3.1$.
>
> Simularemos a s√©rie para verificar este resultado:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros do modelo
> c = 1
> phi = 0.7
> sigma = 1  # Desvio padr√£o do ru√≠do branco
>
> # Inicializa√ß√£o
> np.random.seed(42)  # Para reprodutibilidade
> n = 100
> epsilon = np.random.normal(0, sigma, n)  # Ru√≠do branco
> Y = np.zeros(n)
> Y[0] = np.random.normal(c / (1 - phi), sigma)  # Inicializa√ß√£o do primeiro valor
>
> # Gera√ß√£o da s√©rie temporal
> for t in range(1, n):
>     Y[t] = c + phi * Y[t-1] + epsilon[t]
>
> # Valor de Y[99] (o √≠ndice em Python √© 98)
> Y99 = Y[98]
>
> # Expectativa condicional de Y[100]
> E_Y100_cond = c + phi * Y99
>
> # Expectativa incondicional de Y
> E_Y_incond = c / (1 - phi)
>
> print(f"Y[99] = {Y99:.2f}")
> print(f"Expectativa Condicional E[Y_100 | Y_99] = {E_Y100_cond:.2f}")
> print(f"Expectativa Incondicional E[Y] = {E_Y_incond:.2f}")
>
> # Plot da s√©rie temporal
> plt.figure(figsize=(10, 6))
> plt.plot(Y, label='S√©rie Temporal Y_t')
> plt.scatter(98, Y99, color='red', label=f'Y_99 = {Y99:.2f}')
> plt.scatter(99, E_Y100_cond, color='green', label=f'E[Y_100 | Y_99] = {E_Y100_cond:.2f}')
> plt.title('S√©rie Temporal AR(1) com Expectativa Condicional')
> plt.xlabel('Tempo (t)')
> plt.ylabel('Valor Y_t')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> No exemplo simulado, observamos o valor de $Y_{99}$ e calculamos a expectativa condicional de $Y_{100}$ com base nesse valor. A visualiza√ß√£o permite comparar a expectativa condicional com os valores reais da s√©rie.

> üí° **Exemplo Num√©rico (Expectativa Incondicional):**
>
> Usando o mesmo modelo AR(1) $Y_t = 1 + 0.7Y_{t-1} + \epsilon_t$, calculamos a expectativa incondicional:
>
> $E(Y_t) = \frac{c}{1 - \phi} = \frac{1}{1 - 0.7} = \frac{1}{0.3} \approx 3.33$.
>
> Isso representa o valor m√©dio da s√©rie temporal a longo prazo, independentemente de qualquer valor espec√≠fico observado em um determinado momento.

> üí° **Exemplo Num√©rico (Expectativa Condicional):**
>
> Suponha que temos um modelo AR(1) definido por $Y_t = 1 + 0.7Y_{t-1} + \epsilon_t$, e sabemos que $Y_9 = 5$. Vamos calcular a expectativa condicional de $Y_{10}$ dado $Y_9$.
>
> Usando a f√≥rmula $E(Y_{10} | Y_9) = c + \phi Y_9$, temos:
> $E(Y_{10} | Y_9 = 5) = 1 + 0.7 \times 5 = 1 + 3.5 = 4.5$.
>
> Portanto, a expectativa condicional de $Y_{10}$ dado que $Y_9 = 5$ √© 4.5. Note que esta √© diferente da expectativa incondicional, que seria $\frac{1}{1 - 0.7} = \frac{1}{0.3} \approx 3.33$.
>
> ```python
> import numpy as np
>
> # Par√¢metros
> c = 1
> phi = 0.7
> sigma = 1
>
> # Valor conhecido de Y[9]
> Y9 = 5
>
> # C√°lculo da expectativa condicional
> E_Y10_cond = c + phi * Y9
>
> # Expectativa incondicional
> E_Y_incond = c / (1 - phi)
>
> print(f"Expectativa Condicional E[Y_10 | Y_9 = {Y9}]: {E_Y10_cond}")
> print(f"Expectativa Incondicional E[Y_t]: {E_Y_incond}")
> ```

*Prova:*
I. Come√ßamos com o modelo AR(1): $Y_t = c + \phi Y_{t-1} + \epsilon_t$.

II. Tomamos a expectativa condicional de ambos os lados, dado $\mathcal{F}_{t-1}$: $E(Y_t | \mathcal{F}_{t-1}) = E(c + \phi Y_{t-1} + \epsilon_t | \mathcal{F}_{t-1})$.

III. Usamos a propriedade da linearidade da expectativa condicional: $E(Y_t | \mathcal{F}_{t-1}) = E(c | \mathcal{F}_{t-1}) + E(\phi Y_{t-1} | \mathcal{F}_{t-1}) + E(\epsilon_t | \mathcal{F}_{t-1})$.

IV. Como $c$ e $\phi$ s√£o constantes, $E(c | \mathcal{F}_{t-1}) = c$ e $E(\phi Y_{t-1} | \mathcal{F}_{t-1}) = \phi E(Y_{t-1} | \mathcal{F}_{t-1}) = \phi Y_{t-1}$, j√° que $Y_{t-1}$ √© conhecido em $\mathcal{F}_{t-1}$.

V. Assumimos que o ru√≠do branco √© independente do passado, ent√£o $E(\epsilon_t | \mathcal{F}_{t-1}) = 0$.

VI. Portanto, $E(Y_t | \mathcal{F}_{t-1}) = c + \phi Y_{t-1}$. ‚ñ†

**Teorema 1** (Lei da Expectativa Iterada): Para quaisquer vari√°veis aleat√≥rias $X$ e $Y$, vale a seguinte identidade:

$$E[X] = E[E[X|Y]]$$

Esta lei √© fundamental para relacionar expectativas condicionais e incondicionais. Ela afirma que a expectativa incondicional de uma vari√°vel aleat√≥ria pode ser obtida tomando a expectativa da expectativa condicional dessa vari√°vel, dada outra vari√°vel aleat√≥ria.

*Prova:* A prova formal da Lei da Expectativa Iterada envolve argumentos de teoria da medida. Informalmente, ela segue da defini√ß√£o de esperan√ßa condicional como a melhor previs√£o de $X$ dado $Y$. Ao tomar a esperan√ßa dessa melhor previs√£o, obtemos a esperan√ßa incondicional de $X$. ‚ñ†

**Teorema 1.1** Dado um modelo AR(1) definido como $Y_t = c + \phi Y_{t-1} + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, podemos derivar a esperan√ßa incondicional utilizando a Lei da Expectativa Iterada.

*Prova:*
I. Aplicando a Lei da Expectativa Iterada: $E[Y_t] = E[E[Y_t | \mathcal{F}_{t-1}]]$.

II. Substituindo $E[Y_t | \mathcal{F}_{t-1}]$ pela sua express√£o no modelo AR(1): $E[Y_t] = E[c + \phi Y_{t-1}]$.

III. Utilizando a linearidade da esperan√ßa: $E[Y_t] = E[c] + E[\phi Y_{t-1}] = c + \phi E[Y_{t-1}]$.

IV. Assumindo estacionariedade (i.e., $E[Y_t] = E[Y_{t-1}] = \mu$): $\mu = c + \phi \mu$.

V. Resolvendo para $\mu$: $\mu = \frac{c}{1 - \phi}$, que √© a esperan√ßa incondicional. ‚ñ†

### Expectativa como Fun√ß√£o do Tempo

A nota√ß√£o $\mu_t$ para a expectativa incondicional $E(Y_t)$ reconhece explicitamente que a m√©dia pode ser uma fun√ß√£o do tempo *t* [^1]. Isso √© particularmente relevante em processos n√£o estacion√°rios, como aqueles com uma tend√™ncia determin√≠stica.

Considere novamente o processo com tend√™ncia linear:

$$Y_t = \beta t + \epsilon_t$$

Neste caso, a m√©dia incondicional √©:

$$E(Y_t) = \mu_t = \beta t$$

A m√©dia varia linearmente com o tempo, refletindo a tend√™ncia do processo. Este √© um exemplo onde a expectativa incondicional n√£o √© constante [^1].

> üí° **Exemplo Num√©rico (Tend√™ncia Linear):**
>
> Suponha que temos uma s√©rie temporal que representa o n√∫mero de passageiros de uma companhia a√©rea ao longo dos meses. Observamos que h√° um crescimento linear ao longo do tempo. Podemos modelar isso como $Y_t = 100 + 5t + \epsilon_t$, onde $Y_t$ √© o n√∫mero de passageiros no m√™s *t*, 100 √© o n√∫mero inicial de passageiros, 5 √© o incremento m√©dio mensal, e $\epsilon_t$ √© um erro aleat√≥rio com m√©dia zero.
>
> Neste caso, a expectativa incondicional no tempo *t* √©:
>
> $E(Y_t) = 100 + 5t$
>
> Isso significa que, em m√©dia, esperamos que o n√∫mero de passageiros aumente 5 por m√™s, come√ßando com uma base de 100 passageiros. No m√™s 20, por exemplo, a expectativa seria $E(Y_{20}) = 100 + 5 \times 20 = 200$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> beta0 = 100
> beta1 = 5
> sigma = 10  # Desvio padr√£o do ru√≠do branco
>
> # Gera√ß√£o da s√©rie temporal
> np.random.seed(42)
> n = 60  # 5 anos de dados mensais
> t = np.arange(1, n + 1)
> epsilon = np.random.normal(0, sigma, n)
> Y = beta0 + beta1 * t + epsilon
>
> # Expectativa incondicional
> E_Y = beta0 + beta1 * t
>
> # Plot da s√©rie temporal
> plt.figure(figsize=(10, 6))
> plt.plot(t, Y, label='S√©rie Temporal Y_t')
> plt.plot(t, E_Y, color='red', linestyle='--', label='Expectativa Incondicional E[Y_t]')
> plt.title('S√©rie Temporal com Tend√™ncia Linear')
> plt.xlabel('Tempo (t)')
> plt.ylabel('Valor Y_t')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```

> üí° **Exemplo Num√©rico (Tend√™ncia N√£o-Linear):**
>
> Suponha que uma s√©rie temporal segue uma tend√™ncia quadr√°tica: $Y_t = \alpha + \beta t + \gamma t^2 + \epsilon_t$, onde $\alpha$, $\beta$, e $\gamma$ s√£o constantes, e $\epsilon_t$ √© ru√≠do branco. Ent√£o, a expectativa incondicional √©:
>
> $$E(Y_t) = E(\alpha + \beta t + \gamma t^2 + \epsilon_t) = \alpha + \beta t + \gamma t^2$$
>
> A m√©dia varia quadraticamente com o tempo.

> üí° **Exemplo Num√©rico (Tend√™ncia N√£o-Linear):**
>
> Considere uma s√©rie temporal que modela o crescimento de uma popula√ß√£o de bact√©rias em um ambiente controlado. O crescimento pode ser modelado como uma fun√ß√£o quadr√°tica do tempo: $Y_t = 10 + 2t + 0.5t^2 + \epsilon_t$, onde $Y_t$ √© o n√∫mero de bact√©rias no tempo *t*, 10 √© o n√∫mero inicial de bact√©rias, 2 e 0.5 s√£o os coeficientes que determinam a taxa de crescimento, e $\epsilon_t$ √© um termo de erro aleat√≥rio.
>
> Neste caso, a expectativa incondicional √©:
>
> $E(Y_t) = 10 + 2t + 0.5t^2$
>
> Isso significa que, em m√©dia, esperamos que a popula√ß√£o de bact√©rias cres√ßa de forma acelerada ao longo do tempo. No tempo 10, por exemplo, a expectativa seria $E(Y_{10}) = 10 + 2 \times 10 + 0.5 \times 10^2 = 10 + 20 + 50 = 80$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> alpha = 10
> beta = 2
> gamma = 0.5
> sigma = 5  # Desvio padr√£o do ru√≠do branco
>
> # Gera√ß√£o da s√©rie temporal
> np.random.seed(42)
> n = 20
> t = np.arange(1, n + 1)
> epsilon = np.random.normal(0, sigma, n)
> Y = alpha + beta * t + gamma * t**2 + epsilon
>
> # Expectativa incondicional
> E_Y = alpha + beta * t + gamma * t**2
>
> # Plot da s√©rie temporal
> plt.figure(figsize=(10, 6))
> plt.plot(t, Y, label='S√©rie Temporal Y_t')
> plt.plot(t, E_Y, color='red', linestyle='--', label='Expectativa Incondicional E[Y_t]')
> plt.title('S√©rie Temporal com Tend√™ncia Quadr√°tica')
> plt.xlabel('Tempo (t)')
> plt.ylabel('Valor Y_t')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```

**Proposi√ß√£o 1:** Para um processo com tend√™ncia polinomial de grau *n*, dado por:

$$Y_t = \sum_{i=0}^{n} \beta_i t^i + \epsilon_t$$

Onde $\beta_i$ s√£o constantes e $\epsilon_t$ √© ru√≠do branco com $E[\epsilon_t] = 0$, a expectativa incondicional no tempo *t* √©:

$$E[Y_t] = \sum_{i=0}^{n} \beta_i t^i$$

*Prova:*
I. Tomamos a expectativa de ambos os lados da equa√ß√£o: $E[Y_t] = E[\sum_{i=0}^{n} \beta_i t^i + \epsilon_t]$.

II. Aplicamos a linearidade da expectativa: $E[Y_t] = \sum_{i=0}^{n} E[\beta_i t^i] + E[\epsilon_t]$.

III. Como $\beta_i$ s√£o constantes e *t* √© determin√≠stico, $E[\beta_i t^i] = \beta_i t^i$.

IV. Dado que $E[\epsilon_t] = 0$, temos: $E[Y_t] = \sum_{i=0}^{n} \beta_i t^i$. ‚ñ†

### Implica√ß√µes para Modelagem e Previs√£o

A distin√ß√£o entre expectativa incondicional e condicional, e a possibilidade de a m√©dia variar com o tempo, t√™m implica√ß√µes significativas para a modelagem e previs√£o de s√©ries temporais.

*   **Estacionariedade:** Processos estacion√°rios t√™m m√©dia e vari√¢ncia constantes ao longo do tempo. Modelos que imp√µem estacionariedade (e.g., ARMA com ra√≠zes dentro do c√≠rculo unit√°rio) implicam que a expectativa incondicional √© constante.
*   **Previs√£o:** A expectativa condicional $E(Y_{t+h} | \mathcal{F}_t)$ √© frequentemente usada como a melhor previs√£o de $Y_{t+h}$ com base na informa√ß√£o dispon√≠vel no tempo *t*. Em modelos ARMA, a previs√£o √≥tima pode ser expressa recursivamente em termos de valores passados da s√©rie e dos res√≠duos.
*   **Remo√ß√£o de Tend√™ncia:** Em s√©ries temporais com tend√™ncia, √© comum remover a tend√™ncia antes de aplicar modelos estacion√°rios como ARMA. Isso pode ser feito modelando a tend√™ncia determin√≠stica (e.g., com uma regress√£o em *t*) e subtraindo-a da s√©rie original. A s√©rie resultante, livre da tend√™ncia, pode ent√£o ser modelada com um modelo estacion√°rio.

> üí° **Exemplo Num√©rico (Remo√ß√£o de Tend√™ncia):**
>
> Suponha que temos uma s√©rie temporal de vendas mensais de um produto, que mostra uma tend√™ncia de crescimento linear ao longo do tempo. Modelamos a tend√™ncia como $T_t = 100 + 2t$, onde $T_t$ √© a tend√™ncia no m√™s *t*. Estimamos a tend√™ncia usando uma regress√£o linear e obtemos $\hat{T}_t = 98 + 2.1t$. Para remover a tend√™ncia, subtra√≠mos a tend√™ncia estimada da s√©rie original: $Y_t - \hat{T}_t$. A s√©rie resultante deve ser aproximadamente estacion√°ria, permitindo a aplica√ß√£o de modelos ARMA.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LinearRegression
>
> # Dados de exemplo (vendas mensais)
> np.random.seed(42)
> n = 60  # 5 anos de dados mensais
> t = np.arange(1, n + 1)
> T = 100 + 2 * t  # Tend√™ncia real
> epsilon = np.random.normal(0, 10, n)  # Ru√≠do branco
> Y = T + epsilon  # S√©rie temporal original
>
> # Estima√ß√£o da tend√™ncia usando regress√£o linear
> model = LinearRegression()
> model.fit(t.reshape(-1, 1), Y)
> T_hat = model.predict(t.reshape(-1, 1))  # Tend√™ncia estimada
>
> # Remo√ß√£o da tend√™ncia
> Y_detrended = Y - T_hat
>
> # Plot da s√©rie temporal original e da s√©rie detrended
> plt.figure(figsize=(12, 6))
> plt.subplot(1, 2, 1)
> plt.plot(t, Y, label='S√©rie Temporal Original Y_t')
> plt.plot(t, T_hat, color='red', linestyle='--', label='Tend√™ncia Estimada T_hat')
> plt.title('S√©rie Temporal Original com Tend√™ncia')
> plt.xlabel('Tempo (t)')
> plt.ylabel('Vendas')
> plt.legend()
> plt.grid(True)
>
> plt.subplot(1, 2, 2)
> plt.plot(t, Y_detrended, label='S√©rie Temporal Detrended')
> plt.title('S√©rie Temporal Detrended')
> plt.xlabel('Tempo (t)')
> plt.ylabel('Vendas Detrended')
> plt.legend()
> plt.grid(True)
>
> plt.tight_layout()
> plt.show()
> ```
>
> A visualiza√ß√£o mostra a s√©rie temporal original com a tend√™ncia estimada e a s√©rie detrended resultante. A s√©rie detrended deve apresentar um comportamento mais estacion√°rio, facilitando a modelagem com modelos ARMA.

**Lema 1:** Seja $Y_t$ uma s√©rie temporal com tend√™ncia determin√≠stica $T_t$ e um componente estacion√°rio $S_t$, tal que $Y_t = T_t + S_t$. Se modelarmos a tend√™ncia determin√≠stica $T_t$ corretamente, ent√£o a s√©rie $Y_t - \hat{T}_t$, onde $\hat{T}_t$ √© a estimativa de $T_t$, resultar√° em uma s√©rie estacion√°ria com m√©dia aproximadamente zero.

*Prova:*
I. Definimos $Y_t = T_t + S_t$, onde $T_t$ √© a tend√™ncia determin√≠stica e $S_t$ √© o componente estacion√°rio.

II. Estimamos a tend√™ncia determin√≠stica $T_t$ usando algum m√©todo de modelagem, obtendo $\hat{T}_t$.

III. Subtra√≠mos a tend√™ncia estimada da s√©rie original: $Y_t - \hat{T}_t = (T_t + S_t) - \hat{T}_t = (T_t - \hat{T}_t) + S_t$.

IV. Se o modelo para a tend√™ncia for correto, ent√£o $\hat{T}_t$ ser√° uma boa aproxima√ß√£o de $T_t$, e $T_t - \hat{T}_t$ ser√° pr√≥ximo de zero.

V. Portanto, $Y_t - \hat{T}_t \approx S_t$, que √© uma s√©rie estacion√°ria. Al√©m disso, se a m√©dia de $S_t$ √© zero, ent√£o a m√©dia de $Y_t - \hat{T}_t$ tamb√©m ser√° aproximadamente zero. ‚ñ†

### Modelos e Esperan√ßa Matem√°tica

A esperan√ßa matem√°tica varia consideravelmente de acordo com o modelo estat√≠stico aplicado aos dados, apresentaremos alguns modelos e suas esperan√ßas:

*   **White Noise:** $E[Y_t] = 0$
*   **Ru√≠do Branco Gaussiano:** $E[Y_t] = \mu$
*   **MA(1):** $Y_t = \mu + \epsilon_t + \theta\epsilon_{t-1}$; $E[Y_t] = \mu$
*   **AR(1):** $Y_t = c + \phi Y_{t-1} + \epsilon_t$; $E[Y_t] = c/(1 - \phi)$
*   **ARMA(p,q):** $Y_t = c + \phi_1Y_{t-1} + \phi_2Y_{t-2} + \ldots + \phi_pY_{t-p} + \epsilon_t + \theta_1\epsilon_{t-1} + \theta_2\epsilon_{t-2} + \ldots + \theta_q\epsilon_{t-q}$

**Teorema 2:** (Esperan√ßa para Modelos MA(q)) Dado um modelo MA(q) definido por:

$$Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \ldots + \theta_q \epsilon_{t-q}$$

Onde $\mu$ √© uma constante e $\epsilon_t$ √© ru√≠do branco com $E[\epsilon_t] = 0$, ent√£o a esperan√ßa incondicional de $Y_t$ √© $E[Y_t] = \mu$.

*Prova:*
I. Tomamos a expectativa de ambos os lados da equa√ß√£o: $E[Y_t] = E[\mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \ldots + \theta_q \epsilon_{t-q}]$.

II. Aplicamos a linearidade da expectativa: $E[Y_t] = E[\mu] + E[\epsilon_t] + \theta_1 E[\epsilon_{t-1}] + \theta_2 E[\epsilon_{t-2}] + \ldots + \theta_q E[\epsilon_{t-q}]$.

III. Como $\mu$ √© uma constante, $E[\mu] = \mu$.

IV. Dado que $E[\epsilon_t] = 0$ para todo *t*, todos os termos $E[\epsilon_{t-i}]$ s√£o zero.

V. Portanto, $E[Y_t] = \mu + 0 + 0 + \ldots + 0 = \mu$. ‚ñ†

> üí° **Exemplo Num√©rico (Modelo MA(1)):**
>
> Considere um modelo MA(1) definido por $Y_t = 5 + \epsilon_t + 0.8\epsilon_{t-1}$, onde $\mu = 5$ e $\theta_1 = 0.8$. A esperan√ßa incondicional de $Y_t$ √© simplesmente $E[Y_t] = \mu = 5$. Isso significa que, em m√©dia, esperamos que a s√©rie temporal oscile em torno do valor 5.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> mu = 5
> theta = 0.8
> sigma = 1  # Desvio padr√£o do ru√≠do branco
>
> # Gera√ß√£o da s√©rie temporal
> np.random.seed(42)
> n = 100
> epsilon = np.random.normal(0, sigma, n)
> Y = np.zeros(n)
> Y[0] = mu + epsilon[0]
> for t in range(1, n):
>     Y[t] = mu + epsilon[t] + theta * epsilon[t-1]
>
> # Expectativa incondicional
> E_Y = mu
>
> # Plot da s√©rie temporal
> plt.figure(figsize=(10, 6))
> plt.plot(Y, label='S√©rie Temporal MA(1)')
> plt.axhline(y=E_Y, color='red', linestyle='--', label='Expectativa Incondicional E[Y_t]')
> plt.title('S√©rie Temporal MA(1) com Expectativa Incondicional')
> plt.xlabel('Tempo (t)')
> plt.ylabel('Valor Y_t')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```

### Conclus√£o

Este cap√≠tulo detalhou a import√¢ncia da **expectativa** em s√©ries temporais, distinguindo entre as vers√µes **incondicional** e **condicional**. A capacidade de a m√©dia variar com o tempo foi ilustrada com exemplos de processos com tend√™ncia. A correta compreens√£o e modelagem da expectativa s√£o cruciais para a an√°lise, previs√£o e interpreta√ß√£o de s√©ries temporais. A escolha entre modelos estacion√°rios e n√£o estacion√°rios, e o tratamento adequado de tend√™ncias, dependem fundamentalmente da compreens√£o do comportamento da expectativa da s√©rie.

### Refer√™ncias

[^1]: P√°gina 44 do texto original.
<!-- END -->