## Expectativa de uma S√©rie Temporal

### Introdu√ß√£o
Este cap√≠tulo explora em profundidade o conceito de **expectativa** de uma s√©rie temporal, focando na interpreta√ß√£o e c√°lculo da m√©dia da distribui√ß√£o de probabilidade da *t*-√©sima observa√ß√£o. Conforme definido, a expectativa $E(Y_t)$ representa o valor m√©dio que se espera observar no instante *t*, considerando a distribui√ß√£o de probabilidade da vari√°vel aleat√≥ria $Y_t$ [^1].

### Conceitos Fundamentais

A **expectativa** $E(Y_t)$ da *t*-√©sima observa√ß√£o de uma s√©rie temporal √© definida como a m√©dia da distribui√ß√£o de probabilidade dessa observa√ß√£o, desde que essa m√©dia exista. Matematicamente, ela √© expressa como:

$$E(Y_t) = \int_{-\infty}^{\infty} y_t f_{Y_t}(y_t) \, dy_t$$

onde $f_{Y_t}(y_t)$ representa a **fun√ß√£o densidade de probabilidade (PDF)** da vari√°vel aleat√≥ria $Y_t$ [^1]. Esta integral calcula a m√©dia ponderada de todos os valores poss√≠veis de $y_t$, ponderados pela sua probabilidade de ocorr√™ncia.

Uma interpreta√ß√£o para a expectativa √© o **limite de probabilidade da m√©dia do ensemble**. Ou seja, se tivermos $I$ realiza√ß√µes independentes da s√©rie temporal, a m√©dia do ensemble no instante *t* √© dada por $\frac{1}{I} \sum_{i=1}^{I} Y_t^{(i)}$, e a expectativa pode ser vista como:

$$E(Y_t) = \text{plim}_{I \to \infty} \left( \frac{1}{I} \sum_{i=1}^{I} Y_t^{(i)} \right)$$

[^1]. Esta formula√ß√£o conecta a defini√ß√£o te√≥rica da expectativa com uma interpreta√ß√£o pr√°tica baseada em m√∫ltiplas realiza√ß√µes da s√©rie temporal.

**Teorema 1** (Propriedades da Expectativa). A expectativa √© um operador linear. Ou seja, para vari√°veis aleat√≥rias $X$ e $Y$ e constantes $a$ e $b$:

$$E(aX + bY) = aE(X) + bE(Y)$$

Adicionalmente, se $X$ e $Y$ s√£o independentes, ent√£o:

$$E(XY) = E(X)E(Y)$$

*Prova:* A prova da linearidade segue diretamente das propriedades da integral. A prova da propriedade do produto para vari√°veis independentes tamb√©m decorre da defini√ß√£o da integral, usando a propriedade que a PDF conjunta de vari√°veis independentes √© o produto das PDFs marginais.

*Prova da Linearidade:*
I.  Considere a defini√ß√£o da expectativa para vari√°veis cont√≠nuas: $E(X) = \int_{-\infty}^{\infty} x f_X(x) \, dx$ e $E(Y) = \int_{-\infty}^{\infty} y f_Y(y) \, dy$, onde $f_X(x)$ e $f_Y(y)$ s√£o as PDFs de $X$ e $Y$, respectivamente.

II. Agora, considere a expectativa de uma combina√ß√£o linear de $X$ e $Y$: $E(aX + bY) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (ax + by) f_{X,Y}(x, y) \, dx \, dy$, onde $f_{X,Y}(x, y)$ √© a PDF conjunta de $X$ e $Y$.

III. Usando a propriedade da integral, separamos a integral em duas: $E(aX + bY) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} ax f_{X,Y}(x, y) \, dx \, dy + \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} by f_{X,Y}(x, y) \, dx \, dy$

IV. As constantes $a$ e $b$ podem ser retiradas das integrais: $E(aX + bY) = a \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x f_{X,Y}(x, y) \, dx \, dy + b \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y f_{X,Y}(x, y) \, dx \, dy$

V.  Reconhecemos que $\int_{-\infty}^{\infty} f_{X,Y}(x, y) \, dy = f_X(x)$ e $\int_{-\infty}^{\infty} f_{X,Y}(x, y) \, dx = f_Y(y)$, ent√£o podemos reescrever a express√£o como:  $E(aX + bY) = a \int_{-\infty}^{\infty} x f_X(x) \, dx + b \int_{-\infty}^{\infty} y f_Y(y) \, dy$

VI. Finalmente, substitu√≠mos as integrais pelas expectativas de $X$ e $Y$, respectivamente, obtendo: $E(aX + bY) = aE(X) + bE(Y)$.  Portanto, a propriedade da linearidade da expectativa est√° provada. ‚ñ†

*Prova da Expectativa do Produto de Vari√°veis Aleat√≥rias Independentes:*
I.  Come√ßamos com a defini√ß√£o da expectativa do produto de duas vari√°veis aleat√≥rias independentes $X$ e $Y$: $E(XY) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} xy f_{X,Y}(x, y) \, dx \, dy$.

II. Como $X$ e $Y$ s√£o independentes, sua PDF conjunta √© o produto de suas PDFs marginais: $f_{X,Y}(x, y) = f_X(x) f_Y(y)$.

III. Substitu√≠mos a PDF conjunta na integral: $E(XY) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} xy f_X(x) f_Y(y) \, dx \, dy$.

IV.  Reescrevemos a integral dupla como o produto de duas integrais simples: $E(XY) = \int_{-\infty}^{\infty} x f_X(x) \, dx \cdot \int_{-\infty}^{\infty} y f_Y(y) \, dy$.

V.  Reconhecemos que cada integral representa a expectativa de $X$ e $Y$, respectivamente: $E(XY) = E(X) E(Y)$. Portanto, a propriedade do produto para vari√°veis aleat√≥rias independentes est√° provada. ‚ñ†

Para ilustrar, considere o caso em que a s√©rie temporal $Y_t$ √© composta por um **n√≠vel constante** $\mu$ mais um **ru√≠do branco gaussiano** $\epsilon_t$ [^1]:

$$Y_t = \mu + \epsilon_t$$

Neste caso, a expectativa de $Y_t$ √© simplesmente:

$$E(Y_t) = E(\mu + \epsilon_t) = \mu + E(\epsilon_t) = \mu$$

j√° que a expectativa do ru√≠do branco gaussiano √© zero [^1].

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos uma s√©rie temporal definida por $Y_t = 5 + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco gaussiano com m√©dia 0 e desvio padr√£o 1. Ent√£o, a expectativa de $Y_t$ √©:
>
> $E(Y_t) = E(5 + \epsilon_t) = E(5) + E(\epsilon_t) = 5 + 0 = 5$.
>
> Isso significa que, em m√©dia, esperamos observar valores pr√≥ximos de 5 para esta s√©rie temporal.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> mu = 5
> sigma = 1
> n_samples = 100
>
> # Gera√ß√£o de dados
> np.random.seed(42)  # Para reprodutibilidade
> epsilon = np.random.normal(0, sigma, n_samples)
> Y = mu + epsilon
>
> # Plotagem
> plt.figure(figsize=(10, 6))
> plt.plot(Y, label='Y_t = 5 + epsilon_t')
> plt.axhline(y=mu, color='r', linestyle='--', label='E[Y_t] = 5')
> plt.title('S√©rie Temporal com N√≠vel Constante e Ru√≠do Branco')
> plt.xlabel('Tempo (t)')
> plt.ylabel('Valor de Y_t')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```

*Prova:*
I. Dado o modelo $Y_t = \mu + \epsilon_t$, queremos encontrar $E(Y_t)$.

II. Aplicamos o operador de expectativa em ambos os lados da equa√ß√£o: $E(Y_t) = E(\mu + \epsilon_t)$.

III. Usando a propriedade da linearidade da expectativa (Teorema 1), temos: $E(Y_t) = E(\mu) + E(\epsilon_t)$.

IV. Como $\mu$ √© uma constante, $E(\mu) = \mu$.

V. Por defini√ß√£o, o ru√≠do branco gaussiano tem m√©dia zero, ent√£o $E(\epsilon_t) = 0$.

VI. Portanto, $E(Y_t) = \mu + 0 = \mu$. ‚ñ†

Alternativamente, se $Y_t$ representa uma **tend√™ncia linear no tempo** $\beta t$ mais um ru√≠do branco gaussiano $\epsilon_t$ [^1]:

$$Y_t = \beta t + \epsilon_t$$

Ent√£o a expectativa de $Y_t$ √©:

$$E(Y_t) = E(\beta t + \epsilon_t) = \beta t + E(\epsilon_t) = \beta t$$

Neste caso, a expectativa √© uma fun√ß√£o do tempo, refletindo a tend√™ncia linear [^1].

> üí° **Exemplo Num√©rico:**
>
> Considere uma s√©rie temporal $Y_t = 0.5t + \epsilon_t$, onde $\beta = 0.5$ e $\epsilon_t$ √© ru√≠do branco gaussiano com m√©dia 0 e desvio padr√£o 2.  Vamos calcular a expectativa para $t = 10$ e $t = 20$.
>
> Para $t = 10$: $E(Y_{10}) = 0.5 \times 10 + E(\epsilon_{10}) = 5 + 0 = 5$.
> Para $t = 20$: $E(Y_{20}) = 0.5 \times 20 + E(\epsilon_{20}) = 10 + 0 = 10$.
>
> Isso mostra que a expectativa aumenta linearmente com o tempo.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> beta = 0.5
> sigma = 2
> n_samples = 100
>
> # Gera√ß√£o de dados
> np.random.seed(42)  # Para reprodutibilidade
> t = np.arange(1, n_samples + 1)
> epsilon = np.random.normal(0, sigma, n_samples)
> Y = beta * t + epsilon
>
> # C√°lculo da expectativa
> E_Y = beta * t
>
> # Plotagem
> plt.figure(figsize=(10, 6))
> plt.plot(t, Y, label='Y_t = 0.5t + epsilon_t')
> plt.plot(t, E_Y, color='r', linestyle='--', label='E[Y_t] = 0.5t')
> plt.title('S√©rie Temporal com Tend√™ncia Linear e Ru√≠do Branco')
> plt.xlabel('Tempo (t)')
> plt.ylabel('Valor de Y_t')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```

*Prova:*
I. Dado o modelo $Y_t = \beta t + \epsilon_t$, queremos encontrar $E(Y_t)$.

II. Aplicamos o operador de expectativa em ambos os lados da equa√ß√£o: $E(Y_t) = E(\beta t + \epsilon_t)$.

III. Usando a propriedade da linearidade da expectativa (Teorema 1), temos: $E(Y_t) = E(\beta t) + E(\epsilon_t)$.

IV. Como $\beta$ √© uma constante e $t$ √© uma vari√°vel determin√≠stica, $E(\beta t) = \beta t$.

V. Por defini√ß√£o, o ru√≠do branco gaussiano tem m√©dia zero, ent√£o $E(\epsilon_t) = 0$.

VI. Portanto, $E(Y_t) = \beta t + 0 = \beta t$. ‚ñ†

Agora, vamos considerar um modelo um pouco mais complexo, um **modelo auto-regressivo de ordem 1 (AR(1))** com uma constante:

$$Y_t = c + \phi Y_{t-1} + \epsilon_t$$

onde $|\phi| < 1$ para garantir a estacionariedade, e $\epsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$.  Podemos calcular a expectativa neste caso.

**Teorema 1.1** (Expectativa de um modelo AR(1) com constante).  Para o modelo AR(1) dado por $Y_t = c + \phi Y_{t-1} + \epsilon_t$, com $|\phi| < 1$ e $E(\epsilon_t) = 0$, a expectativa √© constante e dada por:

$$E(Y_t) = \frac{c}{1 - \phi}$$

> üí° **Exemplo Num√©rico:**
>
> Suponha um modelo AR(1) definido por $Y_t = 2 + 0.8Y_{t-1} + \epsilon_t$, onde $c = 2$ e $\phi = 0.8$.  Vamos calcular a expectativa de $Y_t$.
>
> Usando a f√≥rmula $E(Y_t) = \frac{c}{1 - \phi}$, temos:
> $E(Y_t) = \frac{2}{1 - 0.8} = \frac{2}{0.2} = 10$.
>
> Portanto, a expectativa da s√©rie temporal √© 10.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> c = 2
> phi = 0.8
> sigma = 1
> n_samples = 100
>
> # Gera√ß√£o de dados (simula√ß√£o do AR(1))
> np.random.seed(42)
> epsilon = np.random.normal(0, sigma, n_samples)
> Y = np.zeros(n_samples)
> Y[0] = np.random.normal(c / (1 - phi), sigma / np.sqrt(1 - phi**2)) # Inicializa√ß√£o
> for t in range(1, n_samples):
>     Y[t] = c + phi * Y[t-1] + epsilon[t]
>
> # C√°lculo da expectativa te√≥rica
> E_Y = c / (1 - phi)
>
> # Plotagem
> plt.figure(figsize=(10, 6))
> plt.plot(Y, label='Y_t = 2 + 0.8Y_{t-1} + epsilon_t')
> plt.axhline(y=E_Y, color='r', linestyle='--', label='E[Y_t] = 10')
> plt.title('S√©rie Temporal AR(1) com Constante')
> plt.xlabel('Tempo (t)')
> plt.ylabel('Valor de Y_t')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```

*Prova:* Tomando a expectativa de ambos os lados da equa√ß√£o do modelo AR(1), obtemos:

$$E(Y_t) = E(c + \phi Y_{t-1} + \epsilon_t) = c + \phi E(Y_{t-1}) + E(\epsilon_t)$$

Como estamos assumindo que a s√©rie √© estacion√°ria (ou que estamos olhando para a m√©dia incondicional), $E(Y_t) = E(Y_{t-1}) = \mu$.  Portanto:

$$\mu = c + \phi \mu + 0$$

Resolvendo para $\mu$, obtemos:

$$\mu = \frac{c}{1 - \phi}$$

Em certas situa√ß√µes, para enfatizar, a expectativa $E(Y_t)$ √© referida como a **m√©dia incondicional** de $Y_t$ e denotada por $\mu_t$ [^1]. Esta nota√ß√£o permite a generaliza√ß√£o onde a m√©dia pode ser uma fun√ß√£o do tempo *t*, como no exemplo da tend√™ncia linear [^1].

### Conclus√£o

A expectativa $E(Y_t)$ √© uma medida fundamental para caracterizar o comportamento m√©dio de uma s√©rie temporal no instante *t*. Sua interpreta√ß√£o como m√©dia da distribui√ß√£o de probabilidade e como limite de probabilidade da m√©dia do ensemble oferece perspectivas complementares. A capacidade de calcular a expectativa para diferentes modelos, como a soma de uma constante com ru√≠do branco ou uma tend√™ncia linear com ru√≠do branco, demonstra sua versatilidade e import√¢ncia na an√°lise de s√©ries temporais. A nota√ß√£o $\mu_t$ permite expressar a depend√™ncia temporal da m√©dia, tornando o conceito ainda mais geral e aplic√°vel [^1].

### Refer√™ncias
[^1]: P√°gina 44 do texto original.
<!-- END -->