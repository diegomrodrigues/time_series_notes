## Estacionariedade e Ergodicidade em S√©ries Temporais: Covari√¢ncia-Estacionariedade

### Introdu√ß√£o
A estacionariedade √© uma propriedade fundamental em an√°lise de s√©ries temporais, crucial para a modelagem e previs√£o. Em particular, a **covari√¢ncia-estacionariedade**, tamb√©m conhecida como estacionariedade fraca ou de segunda ordem, imp√µe restri√ß√µes espec√≠ficas sobre os momentos de primeira e segunda ordem da s√©rie temporal [^45]. Este cap√≠tulo explora em profundidade o conceito de covari√¢ncia-estacionariedade, suas implica√ß√µes te√≥ricas e pr√°ticas, e como essa propriedade influencia a constru√ß√£o de modelos estat√≠sticos robustos e confi√°veis. Como veremos, a exig√™ncia de que a s√©rie seja estacion√°ria em covari√¢ncia simplifica significativamente a an√°lise, permitindo o uso de modelos invariantes no tempo e facilitando a estimativa de par√¢metros e algoritmos de previs√£o.

### Conceitos Fundamentais

Um processo estoc√°stico $\{Y_t\}$ √© dito ser **covari√¢ncia-estacion√°rio** (ou fracamente estacion√°rio) se satisfaz duas condi√ß√µes primordiais [^45]:

1.  **M√©dia constante:** O valor esperado de $Y_t$ √© constante e independente do tempo, ou seja,
    $$E(Y_t) = \mu \quad \text{para todo } t$$ [^45]
    onde $\mu$ √© uma constante. Isso implica que a s√©rie temporal n√£o apresenta tend√™ncia determin√≠stica.
    > üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal com os seguintes valores: $Y_1 = 10$, $Y_2 = 12$, $Y_3 = 8$, $Y_4 = 11$, $Y_5 = 9$. A m√©dia amostral √© $\hat{\mu} = \frac{10 + 12 + 8 + 11 + 9}{5} = 10$. Se essa s√©rie √© uma amostra de um processo covari√¢ncia-estacion√°rio, esperamos que a m√©dia populacional $\mu$ seja pr√≥xima de 10 e constante ao longo do tempo.
2.  **Autocovari√¢ncia invariante no tempo:** A autocovari√¢ncia entre $Y_t$ e $Y_{t-j}$ depende apenas da dist√¢ncia *j* entre os dois per√≠odos, e n√£o do tempo *t* [^45]. Formalmente,
    $$E[(Y_t - \mu)(Y_{t-j} - \mu)] = \gamma_j \quad \text{para todo } t \text{ e qualquer } j$$ [^45]
    onde $\gamma_j$ √© a *j*-√©sima autocovari√¢ncia.
    > üí° **Exemplo Num√©rico:** Usando os mesmos dados, e assumindo $\mu = 10$, podemos calcular a autocovari√¢ncia para $j = 1$:
    > $\gamma_1 = \frac{1}{4}[(10-10)(12-10) + (12-10)(8-10) + (8-10)(11-10) + (11-10)(9-10)] = \frac{1}{4}[0 - 4 - 2 - 1] = -1.75$.
    > A autocovari√¢ncia para $j=1$ estimada usando outra janela de tempo deve ser aproximadamente igual se a s√©rie for estacion√°ria.

A primeira condi√ß√£o implica que a m√©dia do processo √© constante ao longo do tempo. A segunda condi√ß√£o implica que a maneira como os valores da s√©rie temporal est√£o relacionados entre si no tempo √© constante.

#### Autocovari√¢ncia e Autocorrela√ß√£o

A fun√ß√£o de **autocovari√¢ncia** $\gamma_j$ mede a depend√™ncia linear entre $Y_t$ e $Y_{t-j}$ [^45]. √â importante notar que $\gamma_0$ representa a vari√¢ncia do processo, pois $\gamma_0 = E[(Y_t - \mu)^2]$ [^45].

A fun√ß√£o de **autocorrela√ß√£o** (ACF), denotada por $\rho_j$, √© definida como a autocovari√¢ncia normalizada pela vari√¢ncia:

$$\rho_j = \frac{\gamma_j}{\gamma_0}$$ [^49]

A ACF fornece uma medida adimensional da depend√™ncia linear entre $Y_t$ e $Y_{t-j}$, variando entre -1 e 1. A estacionariedade em covari√¢ncia implica que tanto a ACF quanto a fun√ß√£o de autocovari√¢ncia dependem apenas do *lag* $j$ e n√£o do tempo *t* [^45].

> üí° **Exemplo Num√©rico:**  Continuando com o exemplo anterior, se a vari√¢ncia da s√©rie for $\gamma_0 = 4$, ent√£o a autocorrela√ß√£o no lag 1 seria $\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{-1.75}{4} = -0.4375$.

#### Exemplo: Ru√≠do Branco Gaussiano

Um exemplo cl√°ssico de um processo estacion√°rio em covari√¢ncia √© o **ru√≠do branco Gaussiano**. Um processo de ru√≠do branco $\{\epsilon_t\}$ possui as seguintes propriedades [^47]:

*   $E(\epsilon_t) = 0$ para todo *t* [^47]
*   $E(\epsilon_t^2) = \sigma^2$ para todo *t* [^47]
*   $E(\epsilon_t \epsilon_\tau) = 0$ para $t \neq \tau$ [^47]

Se, adicionalmente, $\epsilon_t \sim N(0, \sigma^2)$, ent√£o temos um ru√≠do branco Gaussiano [^45]. Claramente, $E(\epsilon_t)$ √© constante e igual a 0, e $E[(\epsilon_t - 0)(\epsilon_{t-j} - 0)] = E(\epsilon_t \epsilon_{t-j})$ depende apenas de *j* (sendo $\sigma^2$ se $j=0$ e 0 caso contr√°rio), satisfazendo as condi√ß√µes de covari√¢ncia-estacionariedade.

> üí° **Exemplo Num√©rico:** Suponha que geramos 100 valores de ru√≠do branco gaussiano com m√©dia 0 e vari√¢ncia 1 usando Python:
> ```python
> import numpy as np
>
> np.random.seed(42)  # for reproducibility
> epsilon = np.random.normal(0, 1, 100)
>
> # Calculate sample mean and variance
> mean_epsilon = np.mean(epsilon)
> variance_epsilon = np.var(epsilon)
>
> print(f"Sample Mean: {mean_epsilon:.4f}")
> print(f"Sample Variance: {variance_epsilon:.4f}")
>
> # Calculate sample autocorrelation for lag 1
> n = len(epsilon)
> autocovariance_1 = np.sum((epsilon[1:] - mean_epsilon) * (epsilon[:-1] - mean_epsilon)) / (n - 1)
> autocorrelation_1 = autocovariance_1 / variance_epsilon
>
> print(f"Sample Autocorrelation (Lag 1): {autocorrelation_1:.4f}")
> ```
> A m√©dia amostral deve estar pr√≥xima de 0, a vari√¢ncia amostral pr√≥xima de 1, e a autocorrela√ß√£o no lag 1 pr√≥xima de 0, confirmando as propriedades do ru√≠do branco.

**Lema 1:** *A soma de dois processos independentes e covari√¢ncia-estacion√°rios tamb√©m √© covari√¢ncia-estacion√°ria.*

*Prova:* Sejam $\{X_t\}$ e $\{Z_t\}$ dois processos independentes e covari√¢ncia-estacion√°rios com m√©dias $\mu_X$ e $\mu_Z$, respectivamente, e autocovari√¢ncias $\gamma_X(j)$ e $\gamma_Z(j)$, respectivamente. Seja $Y_t = X_t + Z_t$. Ent√£o, $E[Y_t] = E[X_t + Z_t] = E[X_t] + E[Z_t] = \mu_X + \mu_Z = \mu$, que √© constante. Al√©m disso,
$E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(X_t + Z_t - \mu_X - \mu_Z)(X_{t-j} + Z_{t-j} - \mu_X - \mu_Z)] = E[(X_t - \mu_X + Z_t - \mu_Z)(X_{t-j} - \mu_X + Z_{t-j} - \mu_Z)] = E[(X_t - \mu_X)(X_{t-j} - \mu_X)] + E[(Z_t - \mu_Z)(Z_{t-j} - \mu_Z)] + E[(X_t - \mu_X)(Z_{t-j} - \mu_Z)] + E[(Z_t - \mu_Z)(X_{t-j} - \mu_X)]$.
Como $\{X_t\}$ e $\{Z_t\}$ s√£o independentes, os dois √∫ltimos termos s√£o zero. Portanto, $E[(Y_t - \mu)(Y_{t-j} - \mu)] = \gamma_X(j) + \gamma_Z(j) = \gamma_j$, que depende apenas de *j*. Assim, $\{Y_t\}$ √© covari√¢ncia-estacion√°rio. ‚ñ†

> üí° **Exemplo Num√©rico:** Seja $X_t$ um ru√≠do branco com m√©dia 2 e vari√¢ncia 1, e $Z_t$ um ru√≠do branco com m√©dia 3 e vari√¢ncia 2.  Ambos s√£o covari√¢ncia-estacion√°rios.  Ent√£o $Y_t = X_t + Z_t$ tem m√©dia $2+3=5$ e vari√¢ncia $1+2=3$.  A autocovari√¢ncia de $Y_t$ no lag 0 √© 3, e a autocovari√¢ncia para lags maiores que 0 √© 0, mostrando que $Y_t$ tamb√©m √© covari√¢ncia-estacion√°rio.

**Lema 1.1:** *Se $\{X_t\}$ √© um processo covari√¢ncia-estacion√°rio e $a$ √© uma constante, ent√£o o processo $\{aX_t\}$ tamb√©m √© covari√¢ncia-estacion√°rio.*

*Prova:* Seja $\{X_t\}$ um processo covari√¢ncia-estacion√°rio com m√©dia $\mu_X$ e autocovari√¢ncia $\gamma_X(j)$. Seja $Y_t = aX_t$. Ent√£o, $E[Y_t] = E[aX_t] = aE[X_t] = a\mu_X = \mu$, que √© constante. Al√©m disso, $E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(aX_t - a\mu_X)(aX_{t-j} - a\mu_X)] = E[a^2(X_t - \mu_X)(X_{t-j} - \mu_X)] = a^2E[(X_t - \mu_X)(X_{t-j} - \mu_X)] = a^2\gamma_X(j) = \gamma_j$, que depende apenas de *j*. Assim, $\{aX_t\}$ √© covari√¢ncia-estacion√°rio. ‚ñ†

> üí° **Exemplo Num√©rico:** Se $X_t$ tem m√©dia 2 e autocovari√¢ncia $\gamma_X(j)$, ent√£o $Y_t = 3X_t$ tem m√©dia $3*2=6$ e autocovari√¢ncia $9\gamma_X(j)$. Se $X_t$ √© covari√¢ncia-estacion√°rio, ent√£o $Y_t$ tamb√©m o √©.

#### Implica√ß√µes da Covari√¢ncia-Estacionariedade

A covari√¢ncia-estacionariedade simplifica enormemente a an√°lise de s√©ries temporais por diversos motivos:

*   **Modelagem:** Permite o uso de modelos que s√£o invariantes no tempo [^45]. Isso significa que os par√¢metros do modelo n√£o precisam ser reestimados a cada novo ponto de dados, reduzindo a complexidade computacional e aumentando a efici√™ncia.
*   **Infer√™ncia:** Facilita a infer√™ncia estat√≠stica, pois a distribui√ß√£o dos estimadores dos par√¢metros do modelo √© mais bem comportada sob a estacionariedade.
*   **Previs√£o:** Simplifica a previs√£o, pois a estrutura de depend√™ncia do processo permanece constante ao longo do tempo.

Sem a estacionariedade, a an√°lise da s√©rie temporal torna-se muito mais complexa, exigindo modelos que se adaptem √†s mudan√ßas nas propriedades estat√≠sticas da s√©rie ao longo do tempo.

#### Processos N√£o Estacion√°rios
Em contraste, um processo n√£o estacion√°rio em covari√¢ncia pode exibir:

*   **Tend√™ncia:** A m√©dia do processo varia ao longo do tempo. Por exemplo, $E(Y_t) = \beta t$, onde $\beta$ √© uma constante [^44].
    > üí° **Exemplo Num√©rico:** Se $E(Y_t) = 0.5t$, ent√£o $E(Y_1) = 0.5$, $E(Y_2) = 1$, $E(Y_3) = 1.5$, e assim por diante. A m√©dia est√° claramente aumentando com o tempo, indicando uma tend√™ncia e n√£o estacionariedade.
*   **Sazonalidade:** Padr√µes que se repetem em intervalos regulares (e.g., anuais).
*   **Heterocedasticidade:** A vari√¢ncia do processo varia ao longo do tempo.

Processos n√£o estacion√°rios requerem transforma√ß√µes (e.g., diferencia√ß√£o, remo√ß√£o da tend√™ncia) antes que modelos estacion√°rios possam ser aplicados [^45].

**Teorema 1:** *Se $\{Y_t\}$ √© um processo covari√¢ncia-estacion√°rio, ent√£o a sua representa√ß√£o espectral existe.*

*Coment√°rio:* Este teorema fundamental relaciona a estacionariedade em covari√¢ncia com a an√°lise no dom√≠nio da frequ√™ncia. A representa√ß√£o espectral decomp√µe a s√©rie temporal em componentes de diferentes frequ√™ncias. A prova formal envolve o Teorema de Herglotz, que estabelece uma correspond√™ncia entre fun√ß√µes de autocovari√¢ncia n√£o negativas definidas positivas e medidas espectrais.

**Teorema 1.1:** *Se $\{Y_t\}$ √© um processo covari√¢ncia-estacion√°rio com fun√ß√£o de autocovari√¢ncia $\gamma_j$, ent√£o $\gamma_j = \gamma_{-j}$ para todo $j$.*

*Prova:* Por defini√ß√£o, $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$.  Substituindo $j$ por $-j$, temos $\gamma_{-j} = E[(Y_t - \mu)(Y_{t+j} - \mu)]$.  Fazendo a substitui√ß√£o $s = t+j$, ent√£o $t = s-j$, e $\gamma_{-j} = E[(Y_{s-j} - \mu)(Y_{s} - \mu)] = E[(Y_{s} - \mu)(Y_{s-j} - \mu)] = \gamma_j$. ‚ñ†

> üí° **Exemplo Num√©rico:** Se $\gamma_2 = 0.5$ para uma s√©rie temporal covari√¢ncia-estacion√°ria, ent√£o $\gamma_{-2}$ tamb√©m deve ser igual a 0.5. Essa simetria √© uma consequ√™ncia da estacionariedade.

### Conclus√£o
A covari√¢ncia-estacionariedade √© uma propriedade essencial para a an√°lise de s√©ries temporais, garantindo que as propriedades estat√≠sticas do processo permane√ßam constantes ao longo do tempo [^45]. Esta condi√ß√£o simplifica a modelagem, infer√™ncia e previs√£o, permitindo o uso de modelos invariantes no tempo. Embora muitos processos do mundo real n√£o sejam estritamente estacion√°rios, transforma√ß√µes adequadas podem, frequentemente, levar a s√©ries aproximadamente estacion√°rias, permitindo a aplica√ß√£o das t√©cnicas e modelos discutidos neste cap√≠tulo. A compreens√£o da covari√¢ncia-estacionariedade e suas implica√ß√µes √© crucial para a constru√ß√£o de modelos de s√©ries temporais robustos e confi√°veis.

### Refer√™ncias
[^44]: Imagine a battery of I such computers generating sequences {y{1},...}.
[^45]: Given a particular realization such as {y{1} on a time series process...
[^47]: The basic building block for all the processes considered in this chapter is a sequence...
[^49]: The jth autocorrelation of a covariance-stationary process (denoted p‚ÇÅ) is defined as...
## T√≠tulo Conciso
### Introdu√ß√£o
Este cap√≠tulo de livro aborda a modelagem de s√©ries temporais estacion√°rias atrav√©s de processos ARMA (Autoregressive Moving Average). Iniciamos com a defini√ß√£o de **ru√≠do branco** [^47], um conceito fundamental na constru√ß√£o desses modelos. Em seguida, exploramos os processos de m√©dias m√≥veis (Moving Average - MA), autorregressivos (Autoregressive - AR) e, finalmente, a combina√ß√£o de ambos em modelos ARMA. O foco principal reside na an√°lise das propriedades estat√≠sticas desses processos, incluindo estacionaridade, invertibilidade e a fun√ß√£o de autocovari√¢ncia. Os modelos de s√©ries temporais estacion√°rias s√£o largamente utilizados em diversas √°reas, como economia, engenharia e finan√ßas, para modelar e prever o comportamento de dados sequenciais ao longo do tempo.

### Conceitos Fundamentais

**3.2 White Noise**

O ru√≠do branco √© uma sequ√™ncia de vari√°veis aleat√≥rias n√£o correlacionadas com m√©dia zero e vari√¢ncia constante [^47]. Formalmente, uma sequ√™ncia $\{\epsilon_t\}_{t=-\infty}^{\infty}$ √© considerada ru√≠do branco se satisfaz as seguintes condi√ß√µes:

*   $E(\epsilon_t) = 0$ [^47]
*   $E(\epsilon_t^2) = \sigma^2$ [^47]
*   $E(\epsilon_t \epsilon_\tau) = 0$ para $t \neq \tau$ [^47]

Onde $\sigma^2$ √© a vari√¢ncia do processo. Uma condi√ß√£o mais forte √© que as vari√°veis aleat√≥rias sejam independentes, e n√£o apenas n√£o correlacionadas [^47]. Se as vari√°veis aleat√≥rias $\epsilon_t$ seguem uma distribui√ß√£o normal (gaussiana), o processo √© denominado **ru√≠do branco gaussiano** [^47].

> üí° **Exemplo Num√©rico:**  Suponha que temos uma s√©rie de ru√≠do branco gaussiano com $\sigma^2 = 4$. Isso significa que, em m√©dia, os valores flutuar√£o em torno de 0, com um desvio padr√£o de 2. Se observarmos a s√©rie, n√£o veremos padr√µes ou correla√ß√µes entre os valores sucessivos.

**3.3 Moving Average Processes**

Um processo de m√©dias m√≥veis de ordem *q*, denotado MA(*q*), √© definido como uma combina√ß√£o linear de *q* valores passados de um ru√≠do branco mais o termo corrente [^47]. A equa√ß√£o geral de um processo MA(*q*) √© dada por:

$$Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q}$$ [^47]

Onde:

*   $Y_t$ √© o valor do processo no instante *t*.
*   $\mu$ √© a m√©dia do processo.
*   $\epsilon_t, \epsilon_{t-1}, \dots, \epsilon_{t-q}$ s√£o os valores do ru√≠do branco nos instantes *t*, *t-1*, ..., *t-q*.
*   $\theta_1, \theta_2, \dots, \theta_q$ s√£o os coeficientes do processo.

A m√©dia de um processo MA(*q*) √© dada por $E(Y_t) = \mu$ [^47]. A vari√¢ncia √©:

$$\gamma_0 = E(Y_t - \mu)^2 = E(\epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q})^2$$ [^47]

$$\gamma_0 = \sigma^2(1 + \theta_1^2 + \theta_2^2 + \dots + \theta_q^2)$$ [^47]

A autocovari√¢ncia no lag *j* √© dada por:

$$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$$ [^47]

Para um MA(*q*), a autocovari√¢ncia √© zero para $j > q$ [^47].

> üí° **Exemplo Num√©rico:** Considere um processo MA(1) com $\mu = 0$, $\theta_1 = 0.8$ e $\sigma^2 = 1$. Ent√£o, $Y_t = \epsilon_t + 0.8\epsilon_{t-1}$.
> *   A m√©dia √© $E(Y_t) = 0$.
> *   A vari√¢ncia √© $\gamma_0 = 1(1 + 0.8^2) = 1.64$.
> *   A autocovari√¢ncia no lag 1 √© $\gamma_1 = E[(\epsilon_t + 0.8\epsilon_{t-1})(\epsilon_{t-1} + 0.8\epsilon_{t-2})] = 0.8$.
> *   A autocovari√¢ncia no lag 2 √© $\gamma_2 = 0$.

**Invertibilidade:** Um processo MA √© dito invert√≠vel se pode ser expresso como um processo autorregressivo de ordem infinita. A invertibilidade √© importante para a estima√ß√£o e previs√£o do modelo [^65].

**3.4 Autoregressive Processes**

Um processo autorregressivo de ordem *p*, denotado AR(*p*), √© definido como uma combina√ß√£o linear de *p* valores passados do pr√≥prio processo mais um termo de ru√≠do branco [^53]. A equa√ß√£o geral de um processo AR(*p*) √© dada por:

$$Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \dots + \phi_p Y_{t-p} + \epsilon_t$$ [^53]

Onde:

*   $Y_t$ √© o valor do processo no instante *t*.
*   $c$ √© uma constante.
*   $\phi_1, \phi_2, \dots, \phi_p$ s√£o os coeficientes do processo.
*   $\epsilon_t$ √© o valor do ru√≠do branco no instante *t*.

A m√©dia de um processo AR(*p*) estacion√°rio √© dada por $\mu = \frac{c}{1 - \phi_1 - \phi_2 - \dots - \phi_p}$ [^53]. As autocovari√¢ncias satisfazem a seguinte equa√ß√£o:

$$\gamma_j = \phi_1 \gamma_{j-1} + \phi_2 \gamma_{j-2} + \dots + \phi_p \gamma_{j-p} \text{ para } j > 0$$ [^57]

> üí° **Exemplo Num√©rico:** Considere um processo AR(1) com $c = 5$, $\phi_1 = 0.7$ e $\epsilon_t \sim N(0, 1)$. Ent√£o, $Y_t = 5 + 0.7Y_{t-1} + \epsilon_t$.
> *   A m√©dia √© $\mu = \frac{5}{1 - 0.7} = \frac{5}{0.3} \approx 16.67$.
> *   Se $\gamma_0$ (vari√¢ncia) = 5, ent√£o $\gamma_1 = 0.7 * 5 = 3.5$, $\gamma_2 = 0.7 * 3.5 = 2.45$, e assim por diante.

**Estacionaridade:** Um processo AR √© estacion√°rio se as ra√≠zes do polin√¥mio caracter√≠stico associado ao processo estiverem fora do c√≠rculo unit√°rio [^57].

**3.5 Mixed Autoregressive Moving Average Processes**

Um processo ARMA(*p*, *q*) combina as caracter√≠sticas dos processos AR(*p*) e MA(*q*) [^59]. A equa√ß√£o geral de um processo ARMA(*p*, *q*) √© dada por:

$$Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \dots + \phi_p Y_{t-p} + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q}$$ [^59]

A an√°lise de estacionaridade e invertibilidade de um processo ARMA(*p*, *q*) envolve a an√°lise das ra√≠zes dos polin√¥mios autorregressivo e de m√©dias m√≥veis, respectivamente [^60].

> üí° **Exemplo Num√©rico:** Considere um processo ARMA(1,1) com $c=0$, $\phi_1 = 0.6$, $\theta_1 = 0.4$, e $\epsilon_t \sim N(0,1)$. A equa√ß√£o √© $Y_t = 0.6Y_{t-1} + \epsilon_t + 0.4\epsilon_{t-1}$. Para entender o comportamento dessa s√©rie, podemos simular alguns passos:
> *   $Y_1 = 0.6Y_0 + \epsilon_1 + 0.4\epsilon_0$.  Assumindo $Y_0=0$ e $\epsilon_0=0$, $Y_1 = \epsilon_1$.
> *   $Y_2 = 0.6Y_1 + \epsilon_2 + 0.4\epsilon_1 = 0.6\epsilon_1 + \epsilon_2 + 0.4\epsilon_1 = \epsilon_2 + \epsilon_1$.
> *   $Y_3 = 0.6Y_2 + \epsilon_3 + 0.4\epsilon_2 = 0.6(\epsilon_2 + \epsilon_1) + \epsilon_3 + 0.4\epsilon_2 = \epsilon_3 + \epsilon_2 + \epsilon_1$
> Vemos que o processo combina as depend√™ncias passadas tanto dos valores da s√©rie quanto dos ru√≠dos brancos.

**3.6 The Autocovariance-Generating Function**

A fun√ß√£o geradora de autocovari√¢ncia (autocovariance-generating function) √© uma ferramenta √∫til para analisar as propriedades de autocovari√¢ncia de um processo estacion√°rio [^61]. Para um processo $Y_t$, a fun√ß√£o geradora de autocovari√¢ncia √© definida como:

$$g_Y(z) = \sum_{j=-\infty}^{\infty} \gamma_j z^j$$ [^61]

onde $\gamma_j$ √© a autocovari√¢ncia no lag $j$ e $z$ √© uma vari√°vel complexa.

**Filters**: A aplica√ß√£o de filtros em s√©ries temporais pode ser analisada atrav√©s da fun√ß√£o geradora de autocovari√¢ncia. Se $X_t = h(L)Y_t$, onde $h(L)$ √© um filtro linear, ent√£o a fun√ß√£o geradora de autocovari√¢ncia de $X_t$ √© dada por $g_X(z) = h(z)h(z^{-1})g_Y(z)$ [^63].

**3.7 Invertibility**
A invertibilidade √© uma propriedade importante para modelos MA e ARMA. Um modelo invert√≠vel permite expressar os ru√≠dos brancos passados em fun√ß√£o dos valores presentes e passados da s√©rie temporal. Isso facilita a estima√ß√£o e a interpreta√ß√£o do modelo [^65].

**Teorema 2.** *Um processo MA(q) √© sempre covari√¢ncia-estacion√°rio.*

*Prova:* A covari√¢ncia-estacionariedade requer m√©dia constante e autocovari√¢ncia dependente apenas do lag. Para um processo MA(q), $Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q}$, onde $\epsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$.
I. A m√©dia de $Y_t$ √© $E[Y_t] = E[\mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q}] = \mu + E[\epsilon_t] + \theta_1E[\epsilon_{t-1}] + \ldots + \theta_qE[\epsilon_{t-q}] = \mu + 0 + 0 + \ldots + 0 = \mu$, que √© constante.

II. A autocovari√¢ncia $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$. Expandindo, temos:
$\gamma_j = E[(\epsilon_t + \theta_1 \epsilon_{t-1} + ... + \theta_q \epsilon_{t-q})(\epsilon_{t-j} + \theta_1 \epsilon_{t-j-1} + ... + \theta_q \epsilon_{t-j-q})]$.  Devido √† propriedade do ru√≠do branco, $E[\epsilon_i \epsilon_k] = 0$ se $i \neq k$ e $E[\epsilon_i \epsilon_k] = \sigma^2$ se $i=k$.
Assim, $\gamma_j$ √© n√£o-nula apenas para $|j| \le q$, e seu valor depende apenas de *j*, dos coeficientes $\theta_i$ e da vari√¢ncia $\sigma^2$, e n√£o de *t*. Especificamente,
$\gamma_0 = \sigma^2(1 + \theta_1^2 + \theta_2^2 + \dots + \theta_q^2)$
$\gamma_1 = \sigma^2(\theta_1 + \theta_2\theta_1 + ... + \theta_q\theta_{q-1})$
$\gamma_2 = \sigma^2(\theta_2 + \theta_3\theta_1 + ... + \theta_q\theta_{q-2})$
...
$\gamma_q = \sigma^2\theta_q$

III. Portanto, o processo MA(q) √© sempre covari√¢ncia-estacion√°rio. ‚ñ†

**Teorema 3:** *Um processo AR(1) dado por $Y_t = c + \phi_1 Y_{t-1} + \epsilon_t$ √© covari√¢ncia-estacion√°rio se $|\phi_1| < 1$.*

*Prova:*
I. Para que o processo seja covari√¢ncia estacion√°rio, a m√©dia e a autocovari√¢ncia devem ser constantes. J√° vimos que a m√©dia de um AR(p) √© $\mu = \frac{c}{1 - \phi_1 - \phi_2 - \dots - \phi_p}$. Para o caso AR(1), $\mu = \frac{c}{1-\phi_1}$.  Para que a m√©dia seja finita, devemos ter $\phi_1 \neq 1$.

II. Analisemos a autocovari√¢ncia.  Para um AR(1), temos $\gamma_j = \phi_1 \gamma_{j-1}$ para $j>0$.  Para $j=1$, $\gamma_1 = \phi_1 \gamma_0$. Como $\gamma_0 = Var(Y_t)$, podemos escrever $Var(Y_t) = \phi_1^2 Var(Y_{t-1}) + \sigma^2$. Se o processo √© estacion√°rio, $Var(Y_t) = Var(Y_{t-1})$, ent√£o $Var(Y_t) = \frac{\sigma^2}{1 - \phi_1^2}$. Para que a vari√¢ncia seja finita e positiva, devemos ter $|\phi_1| < 1$.

III. Se $|\phi_1| < 1$, ent√£o a autocovari√¢ncia $\gamma_j$ decai exponencialmente com o aumento de $j$, dada por $\gamma_j = \gamma_0 \phi_1^{|j|}$.  Portanto, sob a condi√ß√£o $|\phi_1| < 1$, o processo AR(1) √© covari√¢ncia-estacion√°rio. ‚ñ†
> üí° **Exemplo Num√©rico:** Seja $Y_t = 2 + 0.9Y_{t-1} + \epsilon_t$ com $\epsilon_t \sim N(0, 1)$. Como $|\phi_1| = |0.9| < 1$, o processo √© covari√¢ncia-estacion√°rio. A m√©dia √© $\mu = \frac{2}{1 - 0.9} = 20$. A vari√¢ncia √© $\gamma_0 = \frac{1}{1 - 0.9^2} = \frac{1}{0.19} \approx 5.26$. A autocovari√¢ncia para lag 1 √© $\gamma_1 = 0.9 * 5.26 \approx 4.73$.

### Conclus√£o

Este cap√≠tulo forneceu uma vis√£o geral dos processos ARMA, incluindo suas defini√ß√µes, propriedades estat√≠sticas e condi√ß√µes de estacionaridade e invertibilidade. Os processos ARMA s√£o uma ferramenta poderosa para modelar e prever s√©ries temporais estacion√°rias, e sua compreens√£o √© fundamental para diversas aplica√ß√µes em ci√™ncia e engenharia. Modelos de s√©ries temporais, como o ARMA, permitem estimar e verificar a estacionaridade e ergodicidade de um processo [^45].

### Refer√™ncias
[^45]: Cap√≠tulo 3, p√°gina 45
[^47]: Cap√≠tulo 3, p√°gina 47
[^53]: Cap√≠tulo 3, p√°gina 53
[^57]: Cap√≠tulo 3, p√°gina 57
[^59]: Cap√≠tulo 3, p√°gina 59
[^60]: Cap√≠tulo 3, p√°gina 60
[^61]: Cap√≠tulo 3, p√°gina 61
[^63]: Cap√≠tulo 3, p√°gina 63
[^65]: Cap√≠tulo 3, p√°gina 65
<!-- END -->