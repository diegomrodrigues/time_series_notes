## Estacionariedade e Ergodicidade em S√©ries Temporais: Ergodicidade

### Introdu√ß√£o
Em continuidade ao t√≥pico anterior sobre covari√¢ncia-estacionariedade, este cap√≠tulo se aprofunda no conceito de **ergodicidade**, uma propriedade crucial para a infer√™ncia estat√≠stica em s√©ries temporais. Enquanto a estacionariedade garante que as propriedades estat√≠sticas do processo permane√ßam constantes ao longo do tempo, a ergodicidade estabelece uma liga√ß√£o entre as m√©dias temporais (calculadas a partir de uma √∫nica realiza√ß√£o da s√©rie) e as m√©dias de conjunto (calculadas sobre um conjunto de realiza√ß√µes independentes do processo) [^46]. A ergodicidade √© fundamental porque, na pr√°tica, geralmente temos acesso a apenas uma √∫nica realiza√ß√£o da s√©rie temporal, e precisamos ser capazes de inferir as propriedades estat√≠sticas do processo a partir dessa √∫nica amostra.

### Conceitos Fundamentais

A ergodicidade surge como uma resposta √† quest√£o de como as m√©dias temporais calculadas a partir de uma √∫nica realiza√ß√£o de uma s√©rie temporal se relacionam com as m√©dias de conjunto que descrevem o comportamento estat√≠stico do processo subjacente [^46].

#### Defini√ß√£o de Ergodicidade

Um processo estacion√°rio $\{Y_t\}$ √© dito ser **erg√≥dico para a m√©dia** se a m√©dia amostral converge em probabilidade para a m√©dia de conjunto (esperan√ßa matem√°tica) quando o tamanho da amostra tende ao infinito [^46]. Formalmente,

$$\underset{T \rightarrow \infty}{plim} \left(\frac{1}{T} \sum_{t=1}^{T} Y_t\right) = E(Y_t) = \mu$$ [^46]

onde:

*   $plim$ denota o limite em probabilidade.
*   $T$ √© o tamanho da amostra (n√∫mero de observa√ß√µes na s√©rie temporal).
*   $Y_t$ √© o valor da s√©rie temporal no instante *t*.
*   $\mu$ √© a m√©dia de conjunto (esperan√ßa matem√°tica) do processo.

Esta defini√ß√£o implica que, para uma s√©rie temporal suficientemente longa, a m√©dia amostral se aproximar√° da m√©dia te√≥rica do processo.

> üí° **Exemplo Num√©rico:** Imagine que voc√™ tem uma s√©rie temporal de retornos di√°rios de a√ß√µes. Se essa s√©rie for erg√≥dica para a m√©dia, a m√©dia dos retornos di√°rios calculada ao longo de muitos dias (a m√©dia amostral) ser√° uma boa aproxima√ß√£o do retorno m√©dio "verdadeiro" (a m√©dia de conjunto) que voc√™ obteria se pudesse observar retornos di√°rios de a√ß√µes em universos paralelos simultaneamente.

#### Condi√ß√£o Suficiente para Ergodicidade para a M√©dia
Uma condi√ß√£o suficiente para a ergodicidade para a m√©dia em processos covari√¢ncia-estacion√°rios √© que a autocovari√¢ncia $\gamma_j$ convirja para zero suficientemente r√°pido quando o *lag* $j$ aumenta [^46]. Matematicamente, isso pode ser expresso como:

$$\sum_{j=0}^{\infty} |\gamma_j| < \infty$$ [^46]

Para entender melhor a condi√ß√£o suficiente para a ergodicidade para a m√©dia, apresentamos a seguinte prova:

**Prova da Condi√ß√£o Suficiente para Ergodicidade para a M√©dia:**
Provaremos que se $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ para um processo covari√¢ncia-estacion√°rio $\{Y_t\}$, ent√£o o processo √© erg√≥dico para a m√©dia.

I. Seja $\bar{Y} = \frac{1}{T} \sum_{t=1}^{T} Y_t$ a m√©dia amostral. Queremos mostrar que $\underset{T \rightarrow \infty}{plim} \bar{Y} = \mu = E(Y_t)$.

II. Primeiro, calculamos a esperan√ßa da m√©dia amostral:
    $$E(\bar{Y}) = E\left(\frac{1}{T} \sum_{t=1}^{T} Y_t\right) = \frac{1}{T} \sum_{t=1}^{T} E(Y_t) = \frac{1}{T} \sum_{t=1}^{T} \mu = \mu$$

III. Agora, calculamos a vari√¢ncia da m√©dia amostral:
     $$Var(\bar{Y}) = Var\left(\frac{1}{T} \sum_{t=1}^{T} Y_t\right) = \frac{1}{T^2} Var\left(\sum_{t=1}^{T} Y_t\right) = \frac{1}{T^2} \sum_{t=1}^{T} \sum_{s=1}^{T} Cov(Y_t, Y_s)$$

IV.  Podemos expressar a covari√¢ncia em termos da fun√ß√£o de autocovari√¢ncia $\gamma_{|t-s|}$:
      $$Var(\bar{Y}) = \frac{1}{T^2} \sum_{t=1}^{T} \sum_{s=1}^{T} \gamma_{|t-s|}$$

V.  Reorganizando a soma, podemos escrever:
     $$Var(\bar{Y}) = \frac{1}{T^2} \sum_{j=-(T-1)}^{T-1} (T-|j|) \gamma_j = \frac{1}{T} \sum_{j=-(T-1)}^{T-1} \left(1 - \frac{|j|}{T}\right) \gamma_j$$

VI.  Dado que $\sum_{j=0}^{\infty} |\gamma_j| < \infty$, a s√©rie $\sum_{j=-\infty}^{\infty} |\gamma_j|$ converge absolutamente.  Portanto, quando $T \rightarrow \infty$:
      $$\lim_{T \to \infty} Var(\bar{Y}) = \lim_{T \to \infty} \frac{1}{T} \sum_{j=-(T-1)}^{T-1} \left(1 - \frac{|j|}{T}\right) \gamma_j = 0$$
      Porque a soma $\sum_{j=-(T-1)}^{T-1} \left(1 - \frac{|j|}{T}\right) \gamma_j$ converge para um valor finito enquanto √© dividida por $T$, o limite tende a zero.

VII. Como $E(\bar{Y}) = \mu$ e $\lim_{T \to \infty} Var(\bar{Y}) = 0$, segue-se que $\underset{T \rightarrow \infty}{plim} \bar{Y} = \mu$. Isso significa que a m√©dia amostral converge em probabilidade para a m√©dia de conjunto, o que √© a defini√ß√£o de ergodicidade para a m√©dia. ‚ñ†

Esta condi√ß√£o garante que as depend√™ncias entre observa√ß√µes distantes no tempo sejam fracas o suficiente para que a m√©dia amostral convirja para a m√©dia de conjunto.

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal com as seguintes autocovari√¢ncias: $\gamma_0 = 1$, $\gamma_1 = 0.5$, $\gamma_2 = 0.25$, $\gamma_3 = 0.125$, e assim por diante, onde $\gamma_j = (0.5)^j$. A soma dos valores absolutos das autocovari√¢ncias √© $\sum_{j=0}^{\infty} |(0.5)^j| = \frac{1}{1-0.5} = 2 < \infty$. Portanto, esta s√©rie satisfaz a condi√ß√£o suficiente para ergodicidade para a m√©dia.
>
> üí° **Exemplo Num√©rico:** Suponha que temos uma s√©rie temporal de temperatura di√°ria. Se a autocovari√¢ncia entre as temperaturas de hoje e as de dias cada vez mais distantes diminui rapidamente, a s√©rie provavelmente ser√° erg√≥dica para a m√©dia. Isso significaria que a m√©dia da temperatura di√°ria calculada ao longo de um longo per√≠odo de tempo √© uma boa estimativa da temperatura m√©dia de longo prazo.

#### Ergodicidade para Momentos de Ordem Superior
Similarmente, um processo estacion√°rio √© dito ser **erg√≥dico para os momentos de segunda ordem** (ou ergodicidade para a autocovari√¢ncia) se a m√©dia temporal do produto das flutua√ß√µes em rela√ß√£o √† m√©dia converge para a autocovari√¢ncia te√≥rica [^46]. Ou seja,

$$\underset{T \rightarrow \infty}{plim} \left[\frac{1}{T-j} \sum_{t=j+1}^{T} (Y_t - \mu)(Y_{t-j} - \mu)\right] = \gamma_j$$ [^46]

Condi√ß√µes suficientes para a ergodicidade de momentos de ordem superior s√£o mais complexas e geralmente envolvem restri√ß√µes sobre as cumulantes do processo. Para processos Gaussianos estacion√°rios, a condi√ß√£o para ergodicidade para a m√©dia √© suficiente para garantir a ergodicidade para todos os momentos [^46].

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal que representa o consumo de eletricidade. Se essa s√©rie √© erg√≥dica para a autocovari√¢ncia, ent√£o a autocovari√¢ncia amostral (calculada a partir de dados hist√≥ricos) ser√° uma boa estimativa da autocovari√¢ncia te√≥rica do processo. Isso √© √∫til para modelar e prever o consumo de eletricidade.

<!-- NEW ADDITION START -->
**Teorema 1:** *Se um processo √© erg√≥dico para a m√©dia, ent√£o a m√©dia amostral √© um estimador consistente da m√©dia populacional.*

*Prova:* A ergodicidade para a m√©dia implica que:
$$\underset{T \rightarrow \infty}{plim} \left(\frac{1}{T} \sum_{t=1}^{T} Y_t\right) = E(Y_t) = \mu$$
Seja $\hat{\mu} = \frac{1}{T} \sum_{t=1}^{T} Y_t$ a m√©dia amostral. Para que $\hat{\mu}$ seja um estimador consistente de $\mu$, devemos mostrar que $\underset{T \rightarrow \infty}{plim} (\hat{\mu}) = \mu$. Pela defini√ß√£o de ergodicidade para a m√©dia, isso √© diretamente satisfeito. Portanto, a m√©dia amostral √© um estimador consistente da m√©dia populacional. ‚ñ†

**Lema 1:** *Se $X_t$ e $Y_t$ s√£o processos erg√≥dicos para a m√©dia, ent√£o $Z_t = aX_t + bY_t$ √© tamb√©m erg√≥dico para a m√©dia, onde a e b s√£o constantes.*

*Prova:* Seja $\hat{\mu}_X = \frac{1}{T} \sum_{t=1}^{T} X_t$ e $\hat{\mu}_Y = \frac{1}{T} \sum_{t=1}^{T} Y_t$ as m√©dias amostrais de $X_t$ e $Y_t$, respectivamente. Como $X_t$ e $Y_t$ s√£o erg√≥dicos para a m√©dia, temos:
$$\underset{T \rightarrow \infty}{plim} (\hat{\mu}_X) = E(X_t) = \mu_X$$
$$\underset{T \rightarrow \infty}{plim} (\hat{\mu}_Y) = E(Y_t) = \mu_Y$$
Agora, considere a m√©dia amostral de $Z_t$:
$$\hat{\mu}_Z = \frac{1}{T} \sum_{t=1}^{T} Z_t = \frac{1}{T} \sum_{t=1}^{T} (aX_t + bY_t) = a\left(\frac{1}{T} \sum_{t=1}^{T} X_t\right) + b\left(\frac{1}{T} \sum_{t=1}^{T} Y_t\right) = a\hat{\mu}_X + b\hat{\mu}_Y$$
Aplicando o limite em probabilidade:
$$\underset{T \rightarrow \infty}{plim} (\hat{\mu}_Z) = \underset{T \rightarrow \infty}{plim} (a\hat{\mu}_X + b\hat{\mu}_Y) = a \underset{T \rightarrow \infty}{plim} (\hat{\mu}_X) + b \underset{T \rightarrow \infty}{plim} (\hat{\mu}_Y) = a\mu_X + b\mu_Y$$
Como $E(Z_t) = E(aX_t + bY_t) = aE(X_t) + bE(Y_t) = a\mu_X + b\mu_Y$, temos:
$$\underset{T \rightarrow \infty}{plim} (\hat{\mu}_Z) = E(Z_t)$$
Portanto, $Z_t$ √© erg√≥dico para a m√©dia. ‚ñ†

**Corol√°rio 1:** *Se $X_t$ √© um processo erg√≥dico para a m√©dia com m√©dia $\mu_X$, ent√£o $Y_t = X_t - \mu_X$ √© tamb√©m erg√≥dico para a m√©dia com m√©dia 0.*

*Prova:* Seja $a = 1$ e $b = -1\mu_X$. Ent√£o $Y_t = X_t - \mu_X$ √© uma transforma√ß√£o linear de um processo erg√≥dico e uma constante. Como a m√©dia de uma constante √© a pr√≥pria constante, e $X_t$ √© erg√≥dico para a m√©dia, ent√£o:
$$\underset{T \rightarrow \infty}{plim} \left(\frac{1}{T} \sum_{t=1}^{T} (X_t - \mu_X)\right) = \underset{T \rightarrow \infty}{plim} \left(\frac{1}{T} \sum_{t=1}^{T} X_t\right) - \mu_X = \mu_X - \mu_X = 0$$
Portanto, $Y_t$ √© erg√≥dico para a m√©dia com m√©dia 0. ‚ñ†

<!-- NEW ADDITION END -->

#### Exemplo: Processo N√£o-Erg√≥dico
Para ilustrar a import√¢ncia da ergodicidade, considere um exemplo de um processo estacion√°rio, mas n√£o erg√≥dico [^47]. Suponha que a m√©dia $\mu^{(i)}$ para a *i*-√©sima realiza√ß√£o de uma s√©rie temporal seja gerada a partir de uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\lambda^2$, ou seja, $\mu^{(i)} \sim N(0, \lambda^2)$ [^47]. Considere, agora, o seguinte processo:

$$Y_t^{(i)} = \mu^{(i)} + \epsilon_t$$ [^47]

onde $\epsilon_t$ √© um ru√≠do branco gaussiano com m√©dia zero e vari√¢ncia $\sigma^2$ [^47], independente de $\mu^{(i)}$.

Neste caso:

*   $E(Y_t) = E(\mu^{(i)}) + E(\epsilon_t) = 0 + 0 = 0$ [^47]
*   $\gamma_0 = E(Y_t^2) = E[(\mu^{(i)} + \epsilon_t)^2] = E[(\mu^{(i)})^2] + E[\epsilon_t^2] = \lambda^2 + \sigma^2$ [^47]
*   $\gamma_j = E[(Y_t)(Y_{t-j})] = E[(\mu^{(i)} + \epsilon_t)(\mu^{(i)} + \epsilon_{t-j})] = \lambda^2$ para $j \neq 0$ [^47]

Embora o processo seja estacion√°rio (a m√©dia e a autocovari√¢ncia n√£o dependam de *t*), ele n√£o √© erg√≥dico para a m√©dia [^47]. A m√©dia temporal n√£o converge para a m√©dia de conjunto, mas sim para a m√©dia da realiza√ß√£o espec√≠fica $\mu^{(i)}$ [^47]:

$$\frac{1}{T} \sum_{t=1}^{T} Y_t \rightarrow \mu^{(i)} \neq E(Y_t) = 0$$ [^47]

Isso ocorre porque a depend√™ncia entre as observa√ß√µes √© dominada pela m√©dia aleat√≥ria $\mu^{(i)}$, que √© constante para uma dada realiza√ß√£o, impedindo que a m√©dia temporal convirja para a m√©dia de conjunto [^47].

> üí° **Exemplo Num√©rico:** Suponha que geramos 100 realiza√ß√µes de um processo como esse, cada uma com 1000 pontos, com $\lambda^2 = 1$ e $\sigma^2 = 1$. A m√©dia de cada realiza√ß√£o ser√° diferente, seguindo uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 1. Se calcularmos a m√©dia amostral de cada realiza√ß√£o, cada m√©dia amostral ir√° convergir para o valor de $\mu^{(i)}$ espec√≠fico para essa realiza√ß√£o, em vez de convergir para 0 (a m√©dia de conjunto).
>
> üí° **Exemplo Num√©rico:** Imagine que cada realiza√ß√£o representa o desempenho de um fundo de investimento diferente, e $\mu^{(i)}$ representa a habilidade inerente do gestor do fundo (que permanece constante ao longo do tempo para cada fundo). $\epsilon_t$ representa o ru√≠do aleat√≥rio nos retornos di√°rios. Nesse caso, mesmo que a m√©dia de longo prazo *de todos os gestores de fundos* seja zero, a m√©dia dos retornos de *um √∫nico fundo* n√£o convergir√° para zero, mas sim para a habilidade espec√≠fica daquele gestor.

#### Implica√ß√µes da Ergodicidade
A ergodicidade √© crucial para a an√°lise de s√©ries temporais porque permite inferir propriedades estat√≠sticas do processo a partir de uma √∫nica realiza√ß√£o [^46]. Em particular:
*   **Estimativa de Par√¢metros:** A ergodicidade garante que a m√©dia amostral e a fun√ß√£o de autocovari√¢ncia amostral convirjam para seus valores te√≥ricos, permitindo a estimativa consistente de par√¢metros do modelo.
*   **Previs√£o:** A ergodicidade justifica o uso de modelos estat√≠sticos estimados a partir de dados hist√≥ricos para prever o comportamento futuro da s√©rie temporal.
*   **Interpreta√ß√£o:** A ergodicidade permite interpretar as m√©dias temporais como representativas do comportamento m√©dio do processo a longo prazo.

> üí° **Exemplo Num√©rico:** Se uma s√©rie temporal de vendas mensais de uma empresa √© erg√≥dica, podemos usar os dados de vendas passadas para estimar a m√©dia de vendas mensais e as autocovari√¢ncias, o que nos permite construir um modelo para prever as vendas futuras.

#### Processos ARMA e Ergodicidade
Os processos ARMA (Autoregressive Moving Average) que foram discutidos no cap√≠tulo anterior s√£o, em geral, erg√≥dicos sob certas condi√ß√µes [^46]. Em particular, se as ra√≠zes do polin√¥mio autorregressivo estiverem fora do c√≠rculo unit√°rio (condi√ß√£o de estacionaridade) e a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ for satisfeita, ent√£o o processo ARMA ser√° erg√≥dico para a m√©dia [^46].

**Teorema 4:** *Se $\{\epsilon_t\}$ √© um ru√≠do branco gaussiano, ent√£o o processo MA(1) dado por $Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$ √© erg√≥dico para todos os momentos.*
*Prova:*  Dado que $\{\epsilon_t\}$ √© um ru√≠do branco gaussiano, todos os momentos de $\epsilon_t$ s√£o finitos.
I. Estacionariedade:  Um processo MA(q) √© sempre estacion√°rio em covari√¢ncia (como demonstrado no teorema 2 no cap√≠tulo anterior), portanto o MA(1) √© estacion√°rio em covari√¢ncia.

II. Ergodicidade para a m√©dia:  Para provar a ergodicidade para a m√©dia, devemos mostrar que $\sum_{j=0}^\infty |\gamma_j| < \infty$.  Para o MA(1), $\gamma_0 = \sigma^2(1 + \theta^2)$, $\gamma_1 = \sigma^2 \theta$, e $\gamma_j = 0$ para $j>1$.
Assim, $\sum_{j=0}^\infty |\gamma_j| = |\gamma_0| + |\gamma_1| = \sigma^2(1 + \theta^2) + |\sigma^2 \theta| = \sigma^2 (1 + \theta^2 + |\theta|)$. Dado que $\sigma^2$ e $\theta$ s√£o finitos, a soma √© finita, e o processo √© erg√≥dico para a m√©dia.

III. Ergodicidade para todos os momentos:  Como o processo MA(1) √© uma transforma√ß√£o linear de um ru√≠do branco gaussiano, ele tamb√©m √© gaussiano.  Para processos gaussianos, a ergodicidade para a m√©dia implica a ergodicidade para todos os momentos. Portanto, o processo MA(1) √© erg√≥dico para todos os momentos. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere um processo MA(1) com $\mu = 0$, $\theta = 0.5$, e $\sigma^2 = 1$. Como $\sum_{j=0}^{\infty} |\gamma_j| = 1 + |0.5| = 1.5 < \infty$, o processo √© erg√≥dico para a m√©dia. Dado que o ru√≠do branco √© gaussiano, o processo √© erg√≥dico para todos os momentos.
>
> üí° **Exemplo Num√©rico:** Para o processo MA(1) acima, se simulamos uma longa s√©rie temporal e calculamos a m√©dia amostral, essa m√©dia deve convergir para 0 (a m√©dia de conjunto). Al√©m disso, a autocovari√¢ncia amostral no lag 1 deve convergir para $\theta\sigma^2 = 0.5$.

**Teorema 4.1:** *Se $\{\epsilon_t\}$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia finita, e $\sum_{i=1}^{\infty} |a_i| < \infty$, ent√£o o processo $Y_t = \sum_{i=0}^{\infty} a_i \epsilon_{t-i}$ √© erg√≥dico para a m√©dia.*

*Prova:*
I. Estacionariedade: Sob a condi√ß√£o $\sum_{i=1}^{\infty} |a_i| < \infty$, o processo $Y_t$ √© estacion√°rio em covari√¢ncia.

II. Ergodicidade para a m√©dia: Devemos mostrar que $\sum_{j=0}^{\infty} |\gamma_j| < \infty$. A autocovari√¢ncia $\gamma_j$ √© dada por:
$\gamma_j = Cov(Y_t, Y_{t-j}) = Cov(\sum_{i=0}^{\infty} a_i \epsilon_{t-i}, \sum_{k=0}^{\infty} a_k \epsilon_{t-j-k}) = \sigma^2 \sum_{i=0}^{\infty} a_i a_{i+j}$.
Ent√£o, $\sum_{j=0}^{\infty} |\gamma_j| = \sum_{j=0}^{\infty} |\sigma^2 \sum_{i=0}^{\infty} a_i a_{i+j}| \leq \sigma^2 \sum_{j=0}^{\infty} \sum_{i=0}^{\infty} |a_i| |a_{i+j}| = \sigma^2 (\sum_{i=0}^{\infty} |a_i|)^2 < \infty$, dado que $\sum_{i=0}^{\infty} |a_i| < \infty$.
Portanto, o processo $Y_t$ √© erg√≥dico para a m√©dia. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere o processo $Y_t = \epsilon_t + 0.5\epsilon_{t-1} + 0.25\epsilon_{t-2} + \ldots$, onde $\epsilon_t$ √© ru√≠do branco com vari√¢ncia 1. Aqui, $a_i = (0.5)^i$, e $\sum_{i=0}^{\infty} |a_i| = \sum_{i=0}^{\infty} (0.5)^i = \frac{1}{1-0.5} = 2 < \infty$. Portanto, pelo Teorema 4.1, o processo √© erg√≥dico para a m√©dia.
>
> üí° **Exemplo Num√©rico:** Se simulamos uma longa s√©rie temporal deste processo e calculamos a m√©dia amostral, essa m√©dia deve convergir para 0 (a m√©dia de conjunto).

### Conclus√£o
A ergodicidade √© uma propriedade fundamental que valida o uso de m√©dias temporais para inferir as propriedades estat√≠sticas de s√©ries temporais estacion√°rias [^46]. Em particular, a ergodicidade para a m√©dia garante que a m√©dia amostral convirja para a m√©dia de conjunto quando o tamanho da amostra tende ao infinito. Para processos covari√¢ncia-estacion√°rios, uma condi√ß√£o suficiente para a ergodicidade para a m√©dia √© que a autocovari√¢ncia convirja para zero suficientemente r√°pido quando o *lag* aumenta. A ergodicidade √© crucial para a modelagem, infer√™ncia e previs√£o em s√©ries temporais, permitindo que usemos dados hist√≥ricos para entender e prever o comportamento futuro do processo.
### Refer√™ncias

[^46]: Cap√≠tulo 3, p√°gina 46
[^47]: Cap√≠tulo 3, p√°gina 47
## Modelos de Ru√≠do Branco

### Conceitos Fundamentais
O bloco de constru√ß√£o b√°sico para todos os processos considerados √© uma sequ√™ncia $\{\epsilon_t\}_{t=-\infty}^{\infty}$ cujos elementos t√™m m√©dia zero e vari√¢ncia $\sigma^2$ [^47]:
$$E(\epsilon_t) = 0 \quad [3.2.1]$$
$$E(\epsilon_t^2) = \sigma^2 \quad [3.2.2]$$
e para os quais os $\epsilon$'s n√£o s√£o correlacionados ao longo do tempo [^47]:
$$E(\epsilon_t \epsilon_\tau) = 0 \quad \text{para } t \neq \tau \quad [3.2.3]$$

Um processo que satisfaz [3.2.1] a [3.2.3] √© descrito como um **processo de ru√≠do branco** [^47]. Em algumas ocasi√µes, desejaremos substituir [3.2.3] pela condi√ß√£o ligeiramente mais forte de que os $\epsilon$'s s√£o independentes ao longo do tempo:
$$\epsilon_t, \epsilon_\tau \text{ independentes para } t \neq \tau \quad [3.2.4]$$
Note que [3.2.4] implica [3.2.3], mas [3.2.3] n√£o implica [3.2.4] [^47]. Um processo que satisfaz [3.2.1] a [3.2.4] √© chamado de **processo de ru√≠do branco independente**. Finalmente, se [3.2.1] a [3.2.4] se mantiverem junto com:
$$\epsilon_t \sim N(0, \sigma^2) \quad [3.2.5]$$
ent√£o temos o **processo de ru√≠do branco Gaussiano** [^47].

> üí° **Exemplo Num√©rico:** Gere uma s√©rie temporal de 1000 pontos de ru√≠do branco gaussiano com m√©dia 0 e vari√¢ncia 1 usando Python:
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> np.random.seed(0)  # para reprodutibilidade
> epsilon = np.random.normal(0, 1, 1000)
>
> plt.plot(epsilon)
> plt.title("Ru√≠do Branco Gaussiano")
> plt.xlabel("Tempo")
> plt.ylabel("Valor")
> plt.show()
>
> print(f"M√©dia amostral: {np.mean(epsilon):.4f}")
> print(f"Vari√¢ncia amostral: {np.var(epsilon):.4f}")
> ```
> Este c√≥digo gera uma s√©rie de valores aleat√≥rios que seguem uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 1. A m√©dia e a vari√¢ncia amostrais devem estar pr√≥ximas dos valores te√≥ricos de 0 e 1, respectivamente.

**Proposi√ß√£o 1:** *Um processo de ru√≠do branco gaussiano √© estritamente estacion√°rio.*

*Prova:* Para que um processo seja estritamente estacion√°rio, a distribui√ß√£o conjunta de qualquer conjunto de vari√°veis no tempo $t_1, t_2, \ldots, t_n$ deve ser a mesma da distribui√ß√£o conjunta no tempo $t_1 + h, t_2 + h, \ldots, t_n + h$ para qualquer inteiro $h$.

Dado que $\epsilon_t \sim N(0, \sigma^2)$ e as vari√°veis s√£o independentes, a distribui√ß√£o conjunta de $\epsilon_{t_1}, \epsilon_{t_2}, \ldots, \epsilon_{t_n}$ √© o produto das distribui√ß√µes normais univariadas:
$f(\epsilon_{t_1}, \epsilon_{t_2}, \ldots, \epsilon_{t_n}) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{\epsilon_{t_i}^2}{2\sigma^2}}$.

Da mesma forma, a distribui√ß√£o conjunta de $\epsilon_{t_1+h}, \epsilon_{t_2+h}, \ldots, \epsilon_{t_n+h}$ √©:
$f(\epsilon_{t_1+h}, \epsilon_{t_2+h}, \ldots, \epsilon_{t_n+h}) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{\epsilon_{t_i+h}^2}{2\sigma^2}}$.

Como a distribui√ß√£o de cada $\epsilon_t$ √© id√™ntica (normal com m√©dia 0 e vari√¢ncia $\sigma^2$), as distribui√ß√µes conjuntas s√£o id√™nticas, e o processo √© estritamente estacion√°rio. ‚ñ†

## Processos de M√©dias M√≥veis

### O Processo de M√©dia M√≥vel de Primeira Ordem
Seja $\{\epsilon_t\}$ ru√≠do branco como em [3.2.1] a [3.2.3], e considere o processo [^48]:
$$Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}, \quad [3.3.1]$$
onde $\mu$ e $\theta$ poderiam ser quaisquer constantes. Essa s√©rie temporal √© chamada de **processo de m√©dia m√≥vel de primeira ordem**, denotado MA(1) [^48]. O termo "m√©dia m√≥vel" vem do fato de que $Y_t$ √© constru√≠do a partir de uma soma ponderada, semelhante a uma m√©dia, dos dois valores mais recentes de $\epsilon$ [^48].

> üí° **Exemplo Num√©rico:** Considere o processo MA(1) $Y_t = 5 + \epsilon_t + 0.7 \epsilon_{t-1}$, onde $\epsilon_t$ √© ru√≠do branco com m√©dia 0 e vari√¢ncia 2. Aqui, $\mu = 5$ e $\theta = 0.7$.

A esperan√ßa de $Y_t$ √© dada por [^48]:
$$E(Y_t) = E(\mu + \epsilon_t + \theta \epsilon_{t-1}) = \mu + E(\epsilon_t) + \theta E(\epsilon_{t-1}) = \mu. \quad [3.3.2]$$
Usamos o s√≠mbolo $\mu$ para o termo constante em [3.3.1] em antecipa√ß√£o ao resultado de que esse termo constante acaba sendo a m√©dia do processo [^48]. A vari√¢ncia de $Y_t$ √© [^48]:
$$E(Y_t - \mu)^2 = E(\epsilon_t + \theta \epsilon_{t-1})^2 = E(\epsilon_t^2 + 2 \theta \epsilon_t \epsilon_{t-1} + \theta^2 \epsilon_{t-1}^2) = \sigma^2 + 0 + \theta^2 \sigma^2 = (1 + \theta^2) \sigma^2. \quad [3.3.3]$$
A primeira autocovari√¢ncia √© [^48]:
$$E(Y_t - \mu)(Y_{t-1} - \mu) = E(\epsilon_t + \theta \epsilon_{t-1})(\epsilon_{t-1} + \theta \epsilon_{t-2}) = E(\epsilon_t \epsilon_{t-1} + \theta \epsilon_{t-1}^2 + \theta \epsilon_t \epsilon_{t-2} + \theta^2 \epsilon_{t-1} \epsilon_{t-2}) = 0 + \theta \sigma^2 + 0 + 0 = \theta \sigma^2. \quad [3.3.4]$$
As autocovari√¢ncias superiores s√£o todas zero [^48]:
$$E(Y_t - \mu)(Y_{t-j} - \mu) = E(\epsilon_t + \theta \epsilon_{t-1})(\epsilon_{t-j} + \theta \epsilon_{t-j-1}) = 0 \quad \text{para } j > 1. \quad [3.3.5]$$

Como a m√©dia e as autocovari√¢ncias n√£o s√£o fun√ß√µes do tempo, um processo MA(1) √© estacion√°rio em covari√¢ncia, independentemente do valor de $\theta$ [^48]. Al√©m disso, [3.1.15] √© claramente satisfeito [^48]:
$$\sum_{j=0}^{\infty} |\gamma_j| = (1 + \theta^2)\sigma^2 + |\theta|\sigma^2 < \infty.$$
Assim, se $\{\epsilon_t\}$ √© ru√≠do branco Gaussiano, ent√£o o processo MA(1) [3.3.1] √© erg√≥dico para todos os momentos [^48].

> üí° **Exemplo Num√©rico:** Para o processo MA(1) $Y_t = 5 + \epsilon_t + 0.7 \epsilon_{t-1}$ com $\sigma^2 = 2$, temos:
> *   $E(Y_t) = \mu = 5$
> *   $Var(Y_t) = (1 + \theta^2)\sigma^2 = (1 + 0.7^2) * 2 = 3.98$
> *   $\gamma_1 = \theta\sigma^2 = 0.7 * 2 = 1.4$
> *   $\gamma_j = 0$ para $j > 1$
>
> Podemos simular este processo em Python e verificar se as propriedades amostrais se aproximam dos valores te√≥ricos:
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> np.random.seed(0)
> T = 1000
> mu = 5
> theta = 0.7
> sigma2 = 2
> epsilon = np.random.normal(0, np.sqrt(sigma2), T)
> Y = [mu + epsilon[t] + theta * epsilon[t-1] if t > 0 else mu + epsilon[t] for t in range(T)]
> Y = np.array(Y)
>
> plt.plot(Y)
> plt.title("Processo MA(1)")
> plt.xlabel("Tempo")
> plt.ylabel("Valor")
> plt.show()
>
> print(f"M√©dia amostral: {np.mean(Y):.4f}")
> print(f"Vari√¢ncia amostral: {np.var(Y):.4f}")
>
> # Calcula a autocovari√¢ncia amostral no lag 1
> autocovariance_1 = np.mean((Y[1:] - np.mean(Y[1:])) * (Y[:-1] - np.mean(Y[:-1])))
> print(f"Autocovari√¢ncia amostral no lag 1: {autocovariance_1:.4f}")
> ```

### A Autocorrela√ß√£o j-√©sima de um Processo Estacion√°rio em Covari√¢ncia
A autocorrela√ß√£o j-√©sima de um processo estacion√°rio em covari√¢ncia (denotada $\rho_j$) √© definida como sua autocovari√¢ncia j-√©sima dividida pela vari√¢ncia [^49]:
$$\rho_j = \frac{\gamma_j}{\gamma_0} \quad [3.3.6]$$
Novamente, a terminologia surge do fato de que $\rho_j$ √© a correla√ß√£o entre $Y_t$ e $Y_{t-j}$ [^49]:
$$\text{Corr}(Y_t, Y_{t-j}) = \frac{\text{Cov}(Y_t, Y_{t-j})}{\sqrt{\text{Var}(Y_t) \text{Var}(Y_{t-j})}} = \frac{\gamma_j}{\gamma_0} = \rho_j \quad [3.3.7]$$

### A Fun√ß√£o de Autocorrela√ß√£o Parcial (PACF)

A Fun√ß√£o de Autocorrela√ß√£o Parcial (PACF) √© usada para medir a correla√ß√£o entre $Y_t$ e $Y_{t-j}$ ap√≥s remover os efeitos das autocorrela√ß√µes nos atrasos intermedi√°rios. Em outras palavras, a PACF mede a correla√ß√£o "pura" entre $Y_t$ e $Y_{t-j}$.

Para entender o PACF, considere um processo AR(p):

$$Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \dots + \phi_p Y_{t-p} + w_t$$

O PACF, denotado como $\phi_{jj}$, √© o coeficiente $\phi_j$ no modelo AR(j). Por exemplo:

*   $\phi_{11} = \text{Corr}(Y_t, Y_{t-1})$
*   $\phi_{22}$ √© o coeficiente de $Y_{t-2}$ na regress√£o de $Y_t$ em $Y_{t-1}$ e $Y_{t-2}$.

Formalmente, $\phi_{jj}$ √© o √∫ltimo coeficiente na regress√£o de $Y_t$ em $Y_{t-1}, Y_{t-2}, \dots, Y_{t-j}$.  Isto pode ser encontrado resolvendo as equa√ß√µes de Yule-Walker:

$$\begin{bmatrix}
1 & \rho_1 & \rho_2 & \cdots & \rho_{j-1} \\
\rho_1 & 1 & \rho_1 & \cdots & \rho_{j-2} \\
\rho_2 & \rho_1 & 1 & \cdots & \rho_{j-3} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\rho_{j-1} & \rho_{j-2} & \rho_{j-3} & \cdots & 1
\end{bmatrix}
\begin{bmatrix}
\phi_{j1} \\
\phi_{j2} \\
\phi_{j3} \\
\vdots \\
\phi_{jj}
\end{bmatrix}
=
\begin{bmatrix}
\rho_1 \\
\rho_2 \\
\rho_3 \\
\vdots \\
\rho_j
\end{bmatrix} \quad [3.3.8]$$

Onde $\phi_{jj}$ √© o j-√©simo coeficiente na solu√ß√£o deste sistema de equa√ß√µes.

### Estimando ACF e PACF

Na pr√°tica, ACF e PACF s√£o estimados a partir de dados amostrais. As estimativas s√£o denotadas como $\hat{\rho}_j$ e $\hat{\phi}_{jj}$, respectivamente.

Para uma s√©rie temporal $y_1, y_2, \dots, y_n$, a estimativa da amostra ACF no atraso $j$ √©:

$$\hat{\rho}_j = \frac{\sum_{t=j+1}^{n} (y_t - \bar{y})(y_{t-j} - \bar{y})}{\sum_{t=1}^{n} (y_t - \bar{y})^2} \quad [3.3.9]$$

Onde $\bar{y}$ √© a m√©dia da amostra.

A estimativa da amostra PACF √© obtida resolvendo as equa√ß√µes de Yule-Walker usando as estimativas da amostra ACF.

### Uso de ACF e PACF para Identifica√ß√£o de Modelo

ACF e PACF s√£o ferramentas cruciais para identificar a ordem de modelos ARMA. Aqui est√£o algumas diretrizes gerais:

*   **AR(p):** A ACF diminui exponencialmente ou em forma senoidal. O PACF tem um corte acentuado ap√≥s o atraso p.
*   **MA(q):** A ACF tem um corte acentuado ap√≥s o atraso q. O PACF diminui exponencialmente ou em forma senoidal.
*   **ARMA(p, q):** A ACF e a PACF diminuem exponencialmente.

**Exemplo:**

Suponha que voc√™ tenha uma s√©rie temporal e sua amostra ACF mostre um corte acentuado ap√≥s o atraso 2, enquanto a amostra PACF diminui exponencialmente. Isso sugere um modelo MA(2).

### Testes de Signific√¢ncia

Ao usar ACF e PACF para identifica√ß√£o de modelo, √© importante avaliar a signific√¢ncia estat√≠stica das estimativas. Uma regra pr√°tica comum √© considerar as autocorrela√ß√µes que est√£o fora dos limites $\pm 2/\sqrt{n}$ como estatisticamente significativas, onde $n$ √© o tamanho da amostra.

### Exemplo em Python

Aqui est√° um exemplo de como calcular e plotar ACF e PACF usando Python com a biblioteca `statsmodels`:

```python
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Suponha que voc√™ tenha uma s√©rie temporal chamada 'data' como um objeto pandas Series
# Exemplo: data = pd.Series([1, 2, 3, 4, 5, 4, 3, 2, 1])

# Plota ACF
plot_acf(data, lags=20)
plt.title('Fun√ß√£o de Autocorrela√ß√£o (ACF)')
plt.show()

# Plota PACF
plot_pacf(data, lags=20, method='ywm') # Usar 'ywm' para melhor estimativa
plt.title('Fun√ß√£o de Autocorrela√ß√£o Parcial (PACF)')
plt.show()
```

Neste exemplo, `plot_acf` plota a ACF e `plot_pacf` plota a PACF. O argumento `lags` especifica o n√∫mero de atrasos a serem exibidos. O argumento `method='ywm'` para `plot_pacf` especifica o m√©todo de Yule-Walker para estimar o PACF, que geralmente fornece melhores estimativas.

### Autocorrela√ß√£o e Estacionariedade

A autocorrela√ß√£o est√° intimamente relacionada ao conceito de estacionariedade. Para uma s√©rie temporal ser estacion√°ria, sua ACF deve decair rapidamente. Se a ACF decair lentamente, sugere que a s√©rie temporal n√£o √© estacion√°ria e pode precisar de diferencia√ß√£o para se tornar estacion√°ria.

<!-- END -->