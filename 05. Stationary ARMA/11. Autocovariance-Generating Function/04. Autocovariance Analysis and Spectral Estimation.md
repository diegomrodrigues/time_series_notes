## Efficient Algorithms for Computing and Evaluating the Autocovariance-Generating Function

### Introdu√ß√£o
Este cap√≠tulo dedica-se a explorar algoritmos eficientes para a computa√ß√£o e avalia√ß√£o da **fun√ß√£o geradora de autocovari√¢ncia** (ACGF). Como vimos, a ACGF √© uma ferramenta chave para resumir e analisar autocovari√¢ncias, bem como para a an√°lise no dom√≠nio da frequ√™ncia e estima√ß√£o espectral [^61]. A efici√™ncia computacional torna-se crucial ao lidar com s√©ries temporais extensas ou modelos complexos. Expandindo o conceito de filtros lineares e sua rela√ß√£o com a ACGF [^62], este cap√≠tulo tamb√©m abordar√° algoritmos para calcular a ACGF para processos MA($\infty$) atrav√©s de combina√ß√µes lineares de valores passados. Al√©m disso, exploraremos a rela√ß√£o entre a ACGF e outras fun√ß√µes geradoras, como a fun√ß√£o geradora de probabilidade (FGP), quando aplic√°vel.

### Conceitos Fundamentais
Em muitos casos pr√°ticos, especialmente em cen√°rios de alta dimens√£o ou de tempo real, computar a ACGF diretamente pela defini√ß√£o $g_Y(z) = \sum_{j=-\infty}^{\infty} \gamma_j z^j$ [^61] pode ser computacionalmente proibitivo. Assim, √© necess√°rio desenvolver algoritmos eficientes que explorem as propriedades dos processos estoc√°sticos subjacentes.

> üí° **Exemplo Num√©rico:**
    >
    > Suponha que tenhamos uma s√©rie temporal com autocovari√¢ncias $\gamma_0 = 2$, $\gamma_1 = 1$, $\gamma_2 = 0.5$, e $\gamma_j = 0$ para $|j| > 2$.  A ACGF seria $g_Y(z) = 0.5z^{-2} + 1z^{-1} + 2 + 1z + 0.5z^2$.  Calcular diretamente $g_Y(z)$ para, digamos, 1000 valores diferentes de *z* exigiria 5000 opera√ß√µes (multiplica√ß√µes e adi√ß√µes). Para s√©ries temporais mais longas, isso se torna rapidamente intrat√°vel.
    >
    > ```python
    > import numpy as np
    >
    > # Autocovari√¢ncias
    > gamma = {0: 2, 1: 1, 2: 0.5}
    >
    > # Valores de z para avaliar a ACGF
    > z_values = np.linspace(-2, 2, 1000) # Valores reais apenas para simplificar
    >
    > # Calcular a ACGF diretamente
    > g_y_values = []
    > for z in z_values:
    >     g_y = gamma.get(2,0) * z**2 + gamma.get(1,0) * z + gamma.get(0,0) + gamma.get(1,0) * z**-1 + gamma.get(2,0) * z**-2
    >     g_y_values.append(g_y)
    >
    > # Imprimir os resultados para alguns pontos
    > for i in range(5):
    >     print(f"z={z_values[i]:.4f}, g_Y(z)={g_y_values[i]:.4f}")
    > ```

Consideremos um processo MA(q) [^48], para o qual a ACGF √© dada por [^62]:
$$g_Y(z) = \sigma^2 (1 + \theta_1 z + \theta_2 z^2 + \ldots + \theta_q z^q)(1 + \theta_1 z^{-1} + \theta_2 z^{-2} + \ldots + \theta_q z^{-q})$$
Calcular diretamente os coeficientes de $z^j$ nesta express√£o requer $O(q^2)$ opera√ß√µes. No entanto, podemos explorar t√©cnicas como a **Transformada R√°pida de Fourier (FFT)** para agilizar o c√°lculo, especialmente ao avaliar a ACGF em v√°rios pontos no c√≠rculo unit√°rio complexo para an√°lise espectral.

**Algoritmo 1:** (Avalia√ß√£o da ACGF via FFT)
Este algoritmo usa a FFT para avaliar eficientemente a ACGF para um MA(q).
1.  Dados os coeficientes $\theta_1, \theta_2, ..., \theta_q$ do processo MA(q) e a vari√¢ncia do ru√≠do branco $\sigma^2$, forme o polin√¥mio $P(z) = 1 + \theta_1 z + \theta_2 z^2 + ... + \theta_q z^q$.
2.  Escolha um conjunto de $N$ pontos igualmente espa√ßados no c√≠rculo unit√°rio complexo $z_k = e^{-i 2\pi k/N}$, onde $k = 0, 1, ..., N-1$.
3.  Use a FFT para avaliar $P(z)$ em cada um dos pontos $z_k$, obtendo $P(z_k)$.
4.  Calcule a ACGF em cada ponto $z_k$ como $g_Y(z_k) = \sigma^2 P(z_k)P^*(z_k)$, onde $P^*(z_k)$ √© o conjugado complexo de $P(z_k)$.

**Complexidade Computacional:** A FFT tem complexidade $O(N \log N)$, e o passo final requer $O(N)$ opera√ß√µes. Portanto, a complexidade total √© dominada pela FFT, sendo $O(N \log N)$.

**Prova da Validade do Algoritmo:**
A validade do algoritmo decorre da propriedade da transformada de Fourier que associa um polin√¥mio aos seus valores em pontos igualmente espa√ßados no c√≠rculo unit√°rio complexo. Ao utilizar a FFT, calculamos eficientemente esses valores e, consequentemente, avaliamos a ACGF em cada ponto.

I. Definimos o polin√¥mio MA como $P(z) = 1 + \theta_1 z + \theta_2 z^2 + \ldots + \theta_q z^q$.

II. Avaliamos $P(z)$ nos pontos do c√≠rculo unit√°rio $z_k$ usando a FFT, obtendo $P(z_k)$.

III. Como $g_Y(z) = \sigma^2 P(z)P(z^{-1})$, calculamos $g_Y(z_k) = \sigma^2 P(z_k)P(z_k^{-1})$.

IV. Dado que $z_k = e^{-i 2\pi k/N}$, ent√£o $z_k^{-1} = e^{i 2\pi k/N}$, que √© o conjugado complexo de $z_k$. Portanto, $P(z_k^{-1}) = P^*(z_k)$, o conjugado complexo de $P(z_k)$.

V. Assim, $g_Y(z_k) = \sigma^2 P(z_k)P^*(z_k)$, que √© o resultado desejado, calculado eficientemente via FFT. $\blacksquare$

    > üí° **Exemplo Num√©rico:**
    >
    > Considere um processo MA(2) com coeficientes $\theta_1 = 0.5$, $\theta_2 = 0.2$, $\sigma^2 = 1$, e $N = 64$ pontos no c√≠rculo unit√°rio.
    >
    > 1. Polin√¥mio: $P(z) = 1 + 0.5z + 0.2z^2$.
    >
    > 2. Pontos no c√≠rculo unit√°rio: $z_k = e^{-i 2\pi k/64}$.
    >
    > 3. Usamos a FFT para avaliar $P(z_k)$ para cada $k$.
    >
    > 4. Calculamos $g_Y(z_k) = |P(z_k)|^2$ para cada $k$.
    >
    > ```python
    > import numpy as np
    >
    > # Coeficientes do MA(2)
    > theta = np.array([1, 0.5, 0.2])  # Inclui o termo constante 1
    > sigma2 = 1
    >
    > # N√∫mero de pontos no c√≠rculo unit√°rio
    > N = 64
    >
    > # Pontos no c√≠rculo unit√°rio
    > k = np.arange(N)
    > zk = np.exp(-1j * 2 * np.pi * k / N)
    >
    > # Avaliar o polin√¥mio usando a FFT
    > Pzk = np.fft.fft(theta, N)
    >
    > # Calcular a ACGF
    > gy_zk = sigma2 * np.abs(Pzk)**2
    >
    > # Imprimir os resultados para alguns pontos
    > for i in range(5):
    >     print(f"k={i}, z_k={zk[i]:.4f}, P(z_k)={Pzk[i]:.4f}, g_Y(z_k)={gy_zk[i]:.4f}")
    > ```
    >
    > **Interpreta√ß√£o:** Este exemplo num√©rico demonstra como a FFT pode ser usada para avaliar eficientemente a ACGF em v√°rios pontos no c√≠rculo unit√°rio. Isso √© especialmente √∫til para an√°lise espectral, onde precisamos avaliar a ACGF em muitas frequ√™ncias diferentes para identificar padr√µes no dom√≠nio da frequ√™ncia. A complexidade $O(N \log N)$ da FFT torna este m√©todo muito mais r√°pido do que calcular a ACGF diretamente para grandes valores de $N$.

**Algoritmo 2:** (C√°lculo das Autocovari√¢ncias Diretas)
Este algoritmo foca em calcular diretamente as autocovari√¢ncias $\gamma_j$ do processo MA(q) e ent√£o avaliar a ACGF. Para um MA(q) temos $\gamma_j$ tal que [^48]:
$$\gamma_j = \begin{cases}
\sigma^2 \sum_{i=0}^{q-j} \theta_i \theta_{i+j} & \text{para } 0 \leq j \leq q \\
0 & \text{para } j > q
\end{cases}$$
onde $\theta_0 = 1$.
O algoritmo √© dado por:
1.  Calcular as autocovari√¢ncias $\gamma_j$ para $j = 0, 1, ..., q$ usando a f√≥rmula acima.
2.  A ACGF √© ent√£o $g_Y(z) = \sum_{j=-q}^{q} \gamma_j z^j$.
3.  Para avaliar $g_Y(z)$ em um ponto espec√≠fico $z$, compute a soma $\sum_{j=-q}^{q} \gamma_j z^j$.

**Complexidade Computacional:** O passo 1 requer $O(q^2)$ opera√ß√µes, e o passo 3 requer $O(q)$ opera√ß√µes para cada ponto $z$. Portanto, para avaliar a ACGF em $M$ pontos, a complexidade total √© $O(q^2 + Mq)$.

**Exemplo Num√©rico:**
Usando o mesmo MA(2) com $\theta_1 = 0.5$, $\theta_2 = 0.2$, $\sigma^2 = 1$:
1.  Autocovari√¢ncias:
   $\gamma_0 = 1 + 0.5^2 + 0.2^2 = 1.29$,
   $\gamma_1 = 0.5 + 0.5 \cdot 0.2 = 0.6$,
   $\gamma_2 = 0.2$.
2.  ACGF: $g_Y(z) = 0.2 z^{-2} + 0.6 z^{-1} + 1.29 + 0.6 z + 0.2 z^2$.
3.  Avalia√ß√£o: Para $z = e^{-i\pi/4}$, $g_Y(e^{-i\pi/4}) = 0.2 e^{i\pi/2} + 0.6 e^{i\pi/4} + 1.29 + 0.6 e^{-i\pi/4} + 0.2 e^{-i\pi/2}$.
4.  Agrupar e simplificar termos:
    $g_Y(e^{-i\pi/4}) = 0.2i + 0.6(\frac{\sqrt{2}}{2} + \frac{\sqrt{2}}{2}i) + 1.29 + 0.6(\frac{\sqrt{2}}{2} - \frac{\sqrt{2}}{2}i) - 0.2i = 0.29 + 0.6\sqrt{2} \approx 1.138$.

> üí° **Exemplo Num√©rico Detalhado:**
>
> Vamos calcular a autocovari√¢ncia para um MA(1) com $\theta_1 = 0.8$ e $\sigma^2 = 2$.
>
> 1. Autocovari√¢ncias:
>    *   $\gamma_0 = \sigma^2 (1 + \theta_1^2) = 2 * (1 + 0.8^2) = 2 * 1.64 = 3.28$
>    *   $\gamma_1 = \sigma^2 (\theta_1) = 2 * 0.8 = 1.6$
>    *   $\gamma_j = 0$ para $|j| > 1$
>
> 2.  ACGF: $g_Y(z) = 1.6z^{-1} + 3.28 + 1.6z$
>
> 3.  Avalia√ß√£o: Seja $z = 0.5$, ent√£o:
>    $g_Y(0.5) = 1.6(0.5)^{-1} + 3.28 + 1.6(0.5) = 1.6 * 2 + 3.28 + 0.8 = 3.2 + 3.28 + 0.8 = 7.28$
>
> ```python
> import numpy as np
>
> # Par√¢metros MA(1)
> theta1 = 0.8
> sigma2 = 2
>
> # Calcular autocovari√¢ncias
> gamma0 = sigma2 * (1 + theta1**2)
> gamma1 = sigma2 * theta1
>
> # Definir z
> z = 0.5
>
> # Calcular ACGF
> gy_z = gamma1 * z**(-1) + gamma0 + gamma1 * z
>
> print(f"gamma_0 = {gamma0:.2f}, gamma_1 = {gamma1:.2f}")
> print(f"ACGF(z={z}) = {gy_z:.2f}")
> ```

**Algoritmo 2.1:** (C√°lculo das Autocovari√¢ncias Diretas Utilizando Simetria)
Observando a simetria das autocovari√¢ncias ($\gamma_j = \gamma_{-j}$), podemos otimizar o Algoritmo 2. Em vez de calcular e somar explicitamente os termos com $z^{-j}$, podemos calcular apenas para $j \geq 0$ e usar a simetria para construir a ACGF.

1. Calcular as autocovari√¢ncias $\gamma_j$ para $j = 0, 1, ..., q$ usando a f√≥rmula $\gamma_j = \sigma^2 \sum_{i=0}^{q-j} \theta_i \theta_{i+j}$, onde $\theta_0 = 1$.
2. A ACGF √© ent√£o constru√≠da utilizando a simetria: $g_Y(z) = \gamma_0 + \sum_{j=1}^{q} \gamma_j (z^j + z^{-j})$.
3. Para avaliar $g_Y(z)$ em um ponto espec√≠fico $z$, compute a soma $\gamma_0 + \sum_{j=1}^{q} \gamma_j (z^j + z^{-j})$.

**Complexidade Computacional:** O passo 1 requer $O(q^2)$ opera√ß√µes, e o passo 3 requer $O(q)$ opera√ß√µes para cada ponto $z$. Portanto, para avaliar a ACGF em $M$ pontos, a complexidade total √© $O(q^2 + Mq)$, semelhante ao Algoritmo 2, mas com um fator constante menor devido √† otimiza√ß√£o da soma.

**Extens√£o para o caso MA($\infty$):**

A ACGF para um processo MA($\infty$) [^52] √© dada por:
$$g_Y(z) = \sigma^2 \psi(z) \psi(z^{-1})$$
onde $\psi(L) = \sum_{j=0}^{\infty} \psi_j L^j$ representa uma combina√ß√£o linear de valores passados, e os coeficientes $\psi_j$ s√£o absolutamente som√°veis, isto √©, $\sum_{j=0}^{\infty} |\psi_j| < \infty$ [^52].
Em termos de $z$, temos:
$$\psi(z) = \sum_{j=0}^{\infty} \psi_j z^j \quad \text{e} \quad \psi(z^{-1}) = \sum_{j=0}^{\infty} \psi_j z^{-j}$$
Calcular ou aproximar a ACGF para um MA($\infty$) requer truncar a soma infinita para uma soma finita, dada por [^52]:
$$\psi_T(z) = \sum_{j=0}^{T} \psi_j z^j \quad \text{e} \quad g_{Y,T}(z) = \sigma^2 \psi_T(z) \psi_T(z^{-1})$$
onde *T* √© o truncamento. A escolha de *T* depende da taxa de decaimento dos coeficientes $\psi_j$. Se os coeficientes deca√≠rem rapidamente, um *T* relativamente pequeno pode fornecer uma boa aproxima√ß√£o.

**Algoritmo 3:** (Aproxima√ß√£o da ACGF para MA($\infty$))
Este algoritmo utiliza o truncamento para aproximar a ACGF para um MA($\infty$).
1.  Dado os coeficientes $\psi_j$ para $j = 0, 1, ..., T$ (onde *T* √© o truncamento) e a vari√¢ncia do ru√≠do branco $\sigma^2$, forme o polin√¥mio truncado $\psi_T(z) = \sum_{j=0}^{T} \psi_j z^j$.
2.  Compute a ACGF aproximada como $g_{Y,T}(z) = \sigma^2 \psi_T(z) \psi_T(z^{-1})$.
3.  Para avaliar $g_{Y,T}(z)$ em um ponto espec√≠fico $z$, calcular o valor de $\sigma^2 \psi_T(z) \psi_T(z^{-1})$.

**Complexidade Computacional:**
Calcular os coeficientes do produto $\psi_T(z) \psi_T(z^{-1})$ requer $O(T^2)$ opera√ß√µes. Avaliar $g_{Y,T}(z)$ para $M$ pontos requer $O(MT)$ opera√ß√µes. Portanto, para avaliar a ACGF em $M$ pontos, a complexidade total √© $O(T^2 + MT)$.

**Algoritmo 3.1:** (Aproxima√ß√£o da ACGF para MA($\infty$) usando FFT)
Para acelerar a avalia√ß√£o da ACGF aproximada para MA($\infty$) em m√∫ltiplos pontos, podemos combinar o Algoritmo 3 com a FFT.

1. Dado os coeficientes $\psi_j$ para $j = 0, 1, ..., T$ (onde *T* √© o truncamento) e a vari√¢ncia do ru√≠do branco $\sigma^2$, forme o polin√¥mio truncado $\psi_T(z) = \sum_{j=0}^{T} \psi_j z^j$.
2. Escolha um conjunto de $N$ pontos igualmente espa√ßados no c√≠rculo unit√°rio complexo $z_k = e^{-i 2\pi k/N}$, onde $k = 0, 1, ..., N-1$. Escolha $N \geq T+1$ para evitar aliasing.
3. Use a FFT para avaliar $\psi_T(z)$ em cada um dos pontos $z_k$, obtendo $\psi_T(z_k)$.
4. Calcule a ACGF aproximada em cada ponto $z_k$ como $g_{Y,T}(z_k) = \sigma^2 \psi_T(z_k)\psi_T^*(z_k)$, onde $\psi_T^*(z_k)$ √© o conjugado complexo de $\psi_T(z_k)$.

**Complexidade Computacional:** A FFT tem complexidade $O(N \log N)$, e o passo final requer $O(N)$ opera√ß√µes. Portanto, a complexidade total √© dominada pela FFT, sendo $O(N \log N)$.  Este algoritmo √© mais eficiente que o Algoritmo 3 para avaliar a ACGF em um grande n√∫mero de pontos.

**Exemplo Num√©rico:**

Considere um processo AR(1) [^53] dado por $Y_t = \phi Y_{t-1} + \varepsilon_t$, que pode ser reescrito como um MA($\infty$) com $\psi_j = \phi^j$. Suponha $\phi = 0.7$ e $\sigma^2 = 1$.

Para $T = 5$, temos $\psi_0 = 1$, $\psi_1 = 0.7$, $\psi_2 = 0.49$, $\psi_3 = 0.343$, $\psi_4 = 0.2401$, $\psi_5 = 0.16807$.
$$\psi_5(z) = 1 + 0.7z + 0.49z^2 + 0.343z^3 + 0.2401z^4 + 0.16807z^5$$
Calcular $g_{Y,5}(z)$ requer multiplicar $\psi_5(z)$ por $\psi_5(z^{-1})$. Para um ponto $z$, podemos avaliar diretamente a soma truncada.

> üí° **Exemplo Num√©rico AR(1) como MA(‚àû) com Truncamento:**
>
> Usando o mesmo AR(1) ($Y_t = 0.7Y_{t-1} + \varepsilon_t$, $\sigma^2 = 1$) e truncamento $T=3$, vamos calcular a ACGF aproximada para $z=0.9$.
>
> 1. Coeficientes truncados: $\psi_0 = 1$, $\psi_1 = 0.7$, $\psi_2 = 0.49$, $\psi_3 = 0.343$.
>
> 2. Polin√¥mio truncado: $\psi_3(z) = 1 + 0.7z + 0.49z^2 + 0.343z^3$.
>
> 3. Avaliar $\psi_3(0.9)$:
>    $\psi_3(0.9) = 1 + 0.7(0.9) + 0.49(0.9)^2 + 0.343(0.9)^3 = 1 + 0.63 + 0.3969 + 0.2494 \approx 2.2763$
>
> 4. Avaliar $\psi_3(0.9^{-1})$:
>    $\psi_3(0.9^{-1}) = 1 + 0.7(0.9^{-1}) + 0.49(0.9^{-1})^2 + 0.343(0.9^{-1})^3 = 1 + 0.78 + 0.605 + 0.474 \approx 2.859$
>
> 5. ACGF aproximada: $g_{Y,3}(0.9) = \sigma^2 \psi_3(0.9) \psi_3(0.9^{-1}) = 1 * 2.2763 * 2.859 \approx 6.50$
>
> ```python
> import numpy as np
>
> # Par√¢metros AR(1)
> phi = 0.7
> sigma2 = 1
>
> # Truncamento
> T = 3
>
> # Coeficientes MA(inf)
> psi = np.array([phi**j for j in range(T + 1)])
>
> # Valor de z
> z = 0.9
>
> # Polin√¥mio truncado
> psi_T_z = np.sum(psi * z**np.arange(T + 1))
> psi_T_z_inv = np.sum(psi * z**(-np.arange(T + 1)))
>
> # ACGF aproximada
> gy_T_z = sigma2 * psi_T_z * psi_T_z_inv
>
> print(f"psi_3(z={z}) = {psi_T_z:.3f}")
> print(f"psi_3(z^-1={z**-1:.3f}) = {psi_T_z_inv:.3f}")
> print(f"ACGF_approx(z={z}) = {gy_T_z:.3f}")
> ```
>
> **Interpreta√ß√£o:** Este exemplo mostra como truncar a representa√ß√£o MA(‚àû) de um AR(1) permite calcular uma aproxima√ß√£o da ACGF. O truncamento introduz um erro, mas se $\phi$ for suficientemente pequeno e *T* for grande o suficiente, a aproxima√ß√£o pode ser razo√°vel.

**Scalar-valued Functions for Evaluation:**

Al√©m de calcular a ACGF, algumas fun√ß√µes escalares podem ser utilizadas para resumir as autocovari√¢ncias, fornecendo insights sobre o comportamento do processo. Por exemplo, a soma das autocovari√¢ncias absolutas, $\sum_{j=-\infty}^{\infty} |\gamma_j|$, pode indicar a for√ßa geral da depend√™ncia temporal. Outra fun√ß√£o √© a vari√¢ncia do processo, $\gamma_0$, que representa a magnitude das flutua√ß√µes.

1.  **Soma das Autocovari√¢ncias Absolutas:** $\sum_{j=-\infty}^{\infty} |\gamma_j|$
2.  **Vari√¢ncia do Processo:** $\gamma_0$

**Teorema 1:** (Limitantes para Soma das Autocovari√¢ncias Absolutas de um MA(q))
Para um processo MA(q) com coeficientes $\theta_i$, $i=1, \dots, q$ e vari√¢ncia $\sigma^2$, a soma das autocovari√¢ncias absolutas √© limitada superiormente por:
$$ \sum_{j=-\infty}^{\infty} |\gamma_j| \leq \sigma^2 \left( 1 + \sum_{i=1}^{q} |\theta_i| \right)^2 $$
**Prova:**
Para um MA(q), $\gamma_j = 0$ para $|j| > q$.  Portanto, $\sum_{j=-\infty}^{\infty} |\gamma_j| = \sum_{j=-q}^{q} |\gamma_j|$.  Como $\gamma_j = \gamma_{-j}$, podemos escrever $\sum_{j=-q}^{q} |\gamma_j| = |\gamma_0| + 2\sum_{j=1}^{q} |\gamma_j|$.  Usando a express√£o para $\gamma_j$ dada anteriormente, $|\gamma_j| = |\sigma^2 \sum_{i=0}^{q-j} \theta_i \theta_{i+j}| \le \sigma^2 \sum_{i=0}^{q-j} |\theta_i| |\theta_{i+j}|$. Assim, $\sum_{j=-q}^{q} |\gamma_j| \leq \sigma^2 \sum_{j=-q}^{q} \sum_{i=0}^{q-|j|} |\theta_i| |\theta_{i+|j|}|$, where $\theta_0 = 1$. Note that $\left( \sum_{i=0}^{q} |\theta_i| \right)^2 = \left( 1 + \sum_{i=1}^{q} |\theta_i| \right)^2 = \sum_{i=0}^{q} \sum_{k=0}^{q} |\theta_i||\theta_k|$.  It follows that $\sum_{j=-q}^{q} |\gamma_j| \leq \sigma^2 \left( 1 + \sum_{i=1}^{q} |\theta_i| \right)^2$. $\blacksquare$

> üí° **Exemplo Num√©rico para o Teorema 1:**
>
> Considere um MA(2) com $\theta_1 = 0.6$, $\theta_2 = -0.4$, e $\sigma^2 = 3$.
>
> 1.  Limite superior: $\sum_{j=-\infty}^{\infty} |\gamma_j| \leq 3 * (1 + |0.6| + |-0.4|)^2 = 3 * (1 + 0.6 + 0.4)^2 = 3 * (2)^2 = 12$.
>
> 2.  Autocovari√¢ncias:
>    *   $\gamma_0 = 3 * (1 + 0.6^2 + (-0.4)^2) = 3 * (1 + 0.36 + 0.16) = 3 * 1.52 = 4.56$
>    *   $\gamma_1 = 3 * (0.6 + 0.6 * (-0.4)) = 3 * (0.6 - 0.24) = 3 * 0.36 = 1.08$
>    *   $\gamma_2 = 3 * (-0.4) = -1.2$
>
> 3.  Soma das autocovari√¢ncias absolutas:
>    $|\gamma_0| + 2 * (|\gamma_1| + |\gamma_2|) = 4.56 + 2 * (1.08 + 1.2) = 4.56 + 2 * 2.28 = 4.56 + 4.56 = 9.12$
>
> Observe que a soma real (9.12) est√° abaixo do limite superior te√≥rico (12).
>
> ```python
> import numpy as np
>
> # Par√¢metros MA(2)
> theta1 = 0.6
> theta2 = -0.4
> sigma2 = 3
>
> # Limite superior
> upper_bound = sigma2 * (1 + abs(theta1) + abs(theta2))**2
>
> # Autocovari√¢ncias
> gamma0 = sigma2 * (1 + theta1**2 + theta2**2)
> gamma1 = sigma2 * (theta1 + theta1 * theta2)
> gamma2 = sigma2 * theta2
>
> # Soma das autocovari√¢ncias absolutas
> sum_abs_gamma = abs(gamma0) + 2 * (abs(gamma1) + abs(gamma2))
>
> print(f"Limite superior: {upper_bound:.2f}")
> print(f"Soma das autocovari√¢ncias absolutas: {sum_abs_gamma:.2f}")
> ```

**Teorema 1.1:** (Limitantes para Soma das Autocovari√¢ncias Absolutas de um MA($\infty$))
Para um processo MA($\infty$) com coeficientes $\psi_i$, $i=0, 1, \dots$ e vari√¢ncia $\sigma^2$, onde $\sum_{i=0}^{\infty} |\psi_i| < \infty$, a soma das autocovari√¢ncias absolutas √© limitada superiormente por:
$$ \sum_{j=-\infty}^{\infty} |\gamma_j| \leq \sigma^2 \left( \sum_{i=0}^{\infty} |\psi_i| \right)^2 $$
**Prova:**
A ACGF para um MA($\infty$) √© $g_Y(z) = \sigma^2 \psi(z)\psi(z^{-1})$, onde $\psi(z) = \sum_{i=0}^{\infty} \psi_i z^i$. Os coeficientes $\gamma_j$ da ACGF s√£o dados por $\gamma_j = \sigma^2 \sum_{i=0}^{\infty} \psi_i \psi_{i+j}$ para $j \geq 0$, e $\gamma_j = \gamma_{-j}$. Assim, $\sum_{j=-\infty}^{\infty} |\gamma_j| = |\gamma_0| + 2\sum_{j=1}^{\infty} |\gamma_j|$. Portanto, $\sum_{j=-\infty}^{\infty} |\gamma_j| = \sigma^2 \left( \sum_{i=0}^{\infty} \psi_i^2 + 2 \sum_{j=1}^{\infty} |\sum_{i=0}^{\infty} \psi_i \psi_{i+j}| \right)$. Usando a desigualdade triangular, $|\sum_{i=0}^{\infty} \psi_i \psi_{i+j}| \leq \sum_{i=0}^{\infty} |\psi_i| |\psi_{i+j}|$. Ent√£o, $\sum_{j=-\infty}^{\infty} |\gamma_j| \leq \sigma^2 \sum_{j=-\infty}^{\infty} |\sum_{i=0}^{\infty} \psi_i \psi_{i+|j|}| \leq \sigma^2 \sum_{j=-\infty}^{\infty} \sum_{i=0}^{\infty} |\psi_i| |\psi_{i+|j|}| = \sigma^2 \left( \sum_{i=0}^{\infty} |\psi_i| \right)^2$. $\blacksquare$

**Utiliza√ß√£o para An√°lise e Estima√ß√£o Espectral:**

A ACGF √© diretamente relacionada ao espectro de pot√™ncia do processo [^61]. Uma vez que a ACGF √© avaliada, o espectro de pot√™ncia pode ser estimado como:
$$S_Y(\omega) = \frac{1}{2\pi} g_Y(e^{-i\omega})$$
A estima√ß√£o espectral √© crucial para identificar periodicidades, ciclos e outras caracter√≠sticas no dom√≠nio da frequ√™ncia. Algoritmos eficientes para calcular a ACGF permitem uma estima√ß√£o espectral mais r√°pida e precisa.

**Teorema 2:** (Rela√ß√£o entre ACGF e FGP)
Se $Y_t$ √© uma s√©rie temporal de contagens n√£o negativas com fun√ß√£o geradora de probabilidade (FGP) $G_Y(z) = E[z^{Y_t}]$, e o processo √© fracamente estacion√°rio, ent√£o a ACGF e a FGP est√£o relacionadas atrav√©s dos momentos do processo. Especificamente, se soubermos os momentos de primeira e segunda ordem de $Y_t$, a ACGF pode ser expressa em termos de derivadas da FGP.

**Prova:**
A ACGF √© definida como $g_Y(z) = \sum_{j=-\infty}^{\infty} \gamma_j z^j$, onde $\gamma_j = Cov(Y_t, Y_{t-j}) = E[Y_t Y_{t-j}] - E[Y_t]E[Y_{t-j}]$. Para um processo estacion√°rio, $E[Y_t] = \mu$ para todo $t$, ent√£o $\gamma_j = E[Y_t Y_{t-j}] - \mu^2$.  A FGP √© $G_Y(z) = E[z^{Y_t}]$.  A primeira derivada de $G_Y(z)$ avaliadaem $z=1$ √© $E[Y_t]$. A segunda derivada de $G_Y(z)$ avaliada em $z=1$ √© $E[Y_t(Y_t-1)]$.

### Fun√ß√£o de Autocorrela√ß√£o (FAC)

A fun√ß√£o de autocorrela√ß√£o (FAC) mede a correla√ß√£o entre $Y_t$ e $Y_{t-j}$ como uma fun√ß√£o do atraso $j$. √â definida como:

$\rho_j = \frac{Cov(Y_t, Y_{t-j})}{\sqrt{Var(Y_t)Var(Y_{t-j})}} = \frac{E[(Y_t - \mu)(Y_{t-j} - \mu)]}{\sigma^2} = \frac{\gamma_j}{\gamma_0}$

onde $\gamma_j$ √© a fun√ß√£o de autocovari√¢ncia no atraso $j$ e $\sigma^2$ √© a vari√¢ncia do processo. Para um processo estacion√°rio, a FAC depende apenas do atraso $j$ e n√£o de $t$.  A FAC sempre assume valores entre -1 e 1.

### Processos Lineares

Um processo linear √© definido como:

$Y_t = \mu + \sum_{i=-\infty}^{\infty} \psi_i Z_{t-i}$

onde $\{Z_t\}$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, e $\{\psi_i\}$ s√£o coeficientes. Um processo linear √© estacion√°rio se $\sum_{i=-\infty}^{\infty} |\psi_i| < \infty$.

#### M√©dia e Fun√ß√£o de Autocovari√¢ncia de um Processo Linear

A m√©dia de um processo linear √©:

$E[Y_t] = E[\mu + \sum_{i=-\infty}^{\infty} \psi_i Z_{t-i}] = \mu + \sum_{i=-\infty}^{\infty} \psi_i E[Z_{t-i}] = \mu$

A fun√ß√£o de autocovari√¢ncia de um processo linear √©:

$\gamma_j = Cov(Y_t, Y_{t-j}) = Cov(\sum_{i=-\infty}^{\infty} \psi_i Z_{t-i}, \sum_{k=-\infty}^{\infty} \psi_k Z_{t-j-k})$

$\gamma_j = E[(\sum_{i=-\infty}^{\infty} \psi_i Z_{t-i})(\sum_{k=-\infty}^{\infty} \psi_k Z_{t-j-k})] = \sum_{i=-\infty}^{\infty} \sum_{k=-\infty}^{\infty} \psi_i \psi_k E[Z_{t-i} Z_{t-j-k}]$

Como $E[Z_{t-i} Z_{t-j-k}] = \sigma^2$ se $t-i = t-j-k$ (ou seja, $i = j+k$) e 0 caso contr√°rio:

$\gamma_j = \sigma^2 \sum_{i=-\infty}^{\infty} \psi_i \psi_{i-j}$

#### Exemplo: M√©dia M√≥vel de Primeira Ordem (MA(1))

Um processo MA(1) √© definido como:

$Y_t = \mu + Z_t + \theta Z_{t-1}$

onde $\{Z_t\}$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, e $\theta$ √© um coeficiente.

A m√©dia do processo MA(1) √© $E[Y_t] = \mu$.

A fun√ß√£o de autocovari√¢ncia √©:

$\gamma_0 = Cov(Y_t, Y_t) = Cov(Z_t + \theta Z_{t-1}, Z_t + \theta Z_{t-1}) = Var(Z_t) + \theta^2 Var(Z_{t-1}) = \sigma^2 + \theta^2 \sigma^2 = \sigma^2(1 + \theta^2)$

$\gamma_1 = Cov(Y_t, Y_{t-1}) = Cov(Z_t + \theta Z_{t-1}, Z_{t-1} + \theta Z_{t-2}) = \theta Var(Z_{t-1}) = \theta \sigma^2$

$\gamma_{-1} = Cov(Y_t, Y_{t+1}) = Cov(Z_t + \theta Z_{t-1}, Z_{t+1} + \theta Z_t) = \theta Var(Z_t) = \theta \sigma^2$

$\gamma_j = 0$ para $|j| > 1$

A fun√ß√£o de autocorrela√ß√£o √©:

$\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{\theta \sigma^2}{\sigma^2(1 + \theta^2)} = \frac{\theta}{1 + \theta^2}$

$\rho_j = 0$ para $|j| > 1$

### Modelos ARMA

Os modelos ARMA (Autoregressive Moving Average) combinam componentes autorregressivos (AR) e de m√©dia m√≥vel (MA). Um modelo ARMA(p, q) √© definido como:

$Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + ... + \phi_p Y_{t-p} + Z_t + \theta_1 Z_{t-1} + \theta_2 Z_{t-2} + ... + \theta_q Z_{t-q}$

onde:
*   $Y_t$ √© o valor da s√©rie temporal no tempo $t$.
*   $c$ √© uma constante.
*   $\phi_1, \phi_2, ..., \phi_p$ s√£o os par√¢metros do componente autorregressivo.
*   $\theta_1, \theta_2, ..., \theta_q$ s√£o os par√¢metros do componente de m√©dia m√≥vel.
*   $Z_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia constante.

#### Componente AR(p)

O componente autorregressivo AR(p) regride o valor atual da s√©rie temporal em seus valores passados. A ordem $p$ determina quantos valores passados s√£o usados no modelo.

#### Componente MA(q)

O componente de m√©dia m√≥vel MA(q) regride o valor atual da s√©rie temporal nos erros de ru√≠do branco passados. A ordem $q$ determina quantos erros passados s√£o usados no modelo.

### Modelos ARIMA

Os modelos ARIMA (Autoregressive Integrated Moving Average) s√£o uma extens√£o dos modelos ARMA que incluem um componente de integra√ß√£o (I) para lidar com s√©ries temporais n√£o estacion√°rias. Um modelo ARIMA(p, d, q) √© definido como:

*   p: A ordem do componente autorregressivo (AR).
*   d: O grau de diferencia√ß√£o (I).
*   q: A ordem do componente de m√©dia m√≥vel (MA).

A diferencia√ß√£o envolve subtrair o valor anterior da s√©rie temporal do valor atual. Isso ajuda a remover tend√™ncias e tornar a s√©rie temporal estacion√°ria.

$Y'_t = Y_t - Y_{t-1}$

Se a s√©rie temporal ainda n√£o for estacion√°ria ap√≥s uma diferencia√ß√£o, pode-se aplicar uma segunda diferencia√ß√£o:

$Y''_t = Y'_t - Y'_{t-1}$

E assim por diante.

### Estacionariedade e Invertibilidade

Para que os modelos ARMA e ARIMA sejam √∫teis, √© importante que sejam estacion√°rios e invert√≠veis.

*   **Estacionariedade:** Um processo √© estacion√°rio se sua m√©dia e vari√¢ncia s√£o constantes ao longo do tempo e sua autocovari√¢ncia depende apenas do atraso entre dois pontos no tempo.
*   **Invertibilidade:** Um processo √© invert√≠vel se puder ser expresso como uma fun√ß√£o dos valores presentes e passados da s√©rie temporal. Isso garante que o modelo possa ser usado para fazer previs√µes.

<!-- END -->