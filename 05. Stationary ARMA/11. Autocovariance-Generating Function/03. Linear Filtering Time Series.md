## Linear Filtering and the Autocovariance-Generating Function

### IntroduÃ§Ã£o

Em anÃ¡lise de sÃ©ries temporais, Ã© comum aplicar **filtros lineares** aos dados para realÃ§ar certas caracterÃ­sticas, remover ruÃ­do ou preparar os dados para modelagem. Como vimos anteriormente, a **funÃ§Ã£o geradora de autocovariÃ¢ncia** (ACGF) Ã© uma ferramenta essencial para caracterizar a estrutura de dependÃªncia temporal de um processo estacionÃ¡rio em covariÃ¢ncia [^61, ^62]. Este capÃ­tulo explora o efeito da aplicaÃ§Ã£o de um filtro linear em uma sÃ©rie temporal sobre sua ACGF. Demonstraremos que aplicar um filtro linear $h(L)$ a uma sÃ©rie temporal $Y_t$ resulta na multiplicaÃ§Ã£o da ACGF original por $h(z)h(z^{-1})$, onde $z$ Ã© a variÃ¡vel complexa associada Ã  ACGF. AlÃ©m disso, discutiremos como esse resultado se relaciona com a anÃ¡lise espectral, mostrando como obter o espectro de potÃªncia da sÃ©rie filtrada.

### Conceitos Fundamentais

Seja $\{Y_t\}$ uma sÃ©rie temporal estacionÃ¡ria em covariÃ¢ncia com ACGF $g_Y(z)$. Considere um filtro linear $h(L)$ definido como:

$$h(L) = \sum_{j=-\infty}^{\infty} h_j L^j$$

onde $L$ Ã© o operador de retardo (lag operator) tal que $L^j Y_t = Y_{t-j}$ e $h_j$ sÃ£o os coeficientes do filtro. Ao aplicar o filtro $h(L)$ Ã  sÃ©rie temporal $Y_t$, obtemos uma nova sÃ©rie temporal $X_t$:

$$X_t = h(L)Y_t = \sum_{j=-\infty}^{\infty} h_j Y_{t-j}$$

O objetivo deste capÃ­tulo Ã© determinar como a ACGF de $X_t$, denotada por $g_X(z)$, se relaciona com a ACGF de $Y_t$, $g_Y(z)$.

**Teorema 1:** (Efeito do Filtro Linear na ACGF)
Seja $Y_t$ uma sÃ©rie temporal estacionÃ¡ria em covariÃ¢ncia com ACGF $g_Y(z)$, e seja $X_t = h(L)Y_t$ a sÃ©rie temporal resultante da aplicaÃ§Ã£o do filtro linear $h(L)$ a $Y_t$. EntÃ£o, a ACGF de $X_t$ Ã© dada por:

$$g_X(z) = h(z)h(z^{-1})g_Y(z)$$

**Prova:**

I. Seja $\gamma_X(k) = E[(X_t - \mu_X)(X_{t-k} - \mu_X)]$ a autocovariÃ¢ncia de $X_t$ no lag *k*, onde $\mu_X = E[X_t]$ Ã© a mÃ©dia de $X_t$.

II. Expressamos $X_t$ e $X_{t-k}$ em termos do filtro linear e de $Y_t$:

$$X_t = \sum_{i=-\infty}^{\infty} h_i Y_{t-i}$$
$$X_{t-k} = \sum_{j=-\infty}^{\infty} h_j Y_{t-k-j}$$

III. SubstituÃ­mos essas expressÃµes na definiÃ§Ã£o da autocovariÃ¢ncia de $X_t$:

$$\gamma_X(k) = E\left[\left(\sum_{i=-\infty}^{\infty} h_i Y_{t-i} - \mu_X\right)\left(\sum_{j=-\infty}^{\infty} h_j Y_{t-k-j} - \mu_X\right)\right]$$

IV. Assumindo que $E[Y_t] = \mu_Y$, entÃ£o $\mu_X = h(1)\mu_Y = \mu_Y \sum_{i=-\infty}^{\infty} h_i$. Para simplificar, considere o caso onde $\mu_Y = 0$, entÃ£o $\mu_X = 0$. Assim:

$$\gamma_X(k) = E\left[\sum_{i=-\infty}^{\infty} \sum_{j=-\infty}^{\infty} h_i h_j Y_{t-i} Y_{t-k-j}\right] = \sum_{i=-\infty}^{\infty} \sum_{j=-\infty}^{\infty} h_i h_j E[Y_{t-i} Y_{t-k-j}]$$

V. Usando a propriedade de estacionariedade, $E[Y_{t-i} Y_{t-k-j}] = \gamma_Y(k+j-i)$, onde $\gamma_Y(k+j-i)$ Ã© a autocovariÃ¢ncia de $Y_t$ no lag $k+j-i$:

$$\gamma_X(k) = \sum_{i=-\infty}^{\infty} \sum_{j=-\infty}^{\infty} h_i h_j \gamma_Y(k+j-i)$$

VI. Agora, construÃ­mos a ACGF de $X_t$:

$$g_X(z) = \sum_{k=-\infty}^{\infty} \gamma_X(k) z^k = \sum_{k=-\infty}^{\infty} \left(\sum_{i=-\infty}^{\infty} \sum_{j=-\infty}^{\infty} h_i h_j \gamma_Y(k+j-i)\right) z^k$$

VII. Fazemos uma mudanÃ§a de variÃ¡vel $m = k + j - i$, entÃ£o $k = m - j + i$:

$$g_X(z) = \sum_{i=-\infty}^{\infty} \sum_{j=-\infty}^{\infty} \sum_{m=-\infty}^{\infty} h_i h_j \gamma_Y(m) z^{m-j+i} = \sum_{m=-\infty}^{\infty} \gamma_Y(m) z^m \left(\sum_{i=-\infty}^{\infty} h_i z^i\right) \left(\sum_{j=-\infty}^{\infty} h_j z^{-j}\right)$$

VIII. Reconhecemos que $\sum_{m=-\infty}^{\infty} \gamma_Y(m) z^m = g_Y(z)$, $\sum_{i=-\infty}^{\infty} h_i z^i = h(z)$, e $\sum_{j=-\infty}^{\infty} h_j z^{-j} = h(z^{-1})$:

$$g_X(z) = h(z)h(z^{-1})g_Y(z)$$

Portanto, demonstramos que a ACGF de $X_t$ Ã© o produto da ACGF de $Y_t$ e $h(z)h(z^{-1})$. $\blacksquare$

    > ğŸ’¡ **Exemplo NumÃ©rico:**
    > Considere uma sÃ©rie temporal $Y_t$ que representa ruÃ­do branco com variÃ¢ncia $\sigma^2 = 1$. Portanto, sua ACGF Ã© $g_Y(z) = 1$. Agora, aplicamos um filtro de mÃ©dia mÃ³vel de ordem 2: $h(L) = 0.3 + 0.4L + 0.3L^2$.  Isto significa que $X_t = 0.3Y_t + 0.4Y_{t-1} + 0.3Y_{t-2}$.
    >
    > EntÃ£o, $h(z) = 0.3 + 0.4z + 0.3z^2$ e $h(z^{-1}) = 0.3 + 0.4z^{-1} + 0.3z^{-2}$.  A ACGF da sÃ©rie filtrada $X_t$ Ã©:
    >
    > $g_X(z) = h(z)h(z^{-1})g_Y(z) = (0.3 + 0.4z + 0.3z^2)(0.3 + 0.4z^{-1} + 0.3z^{-2})(1)$
    >
    > Expandindo o produto:
    >
    > $g_X(z) = 0.09 + 0.12z^{-1} + 0.09z^{-2} + 0.12z + 0.16 + 0.12z^{-1} + 0.09z^2 + 0.12z + 0.09$
    >
    > Agrupando termos:
    >
    > $g_X(z) = 0.09z^{-2} + 0.24z^{-1} + 0.34 + 0.24z + 0.09z^2$
    >
    > As autocovariÃ¢ncias de $X_t$ sÃ£o: $\gamma_X(0) = 0.34$, $\gamma_X(1) = \gamma_X(-1) = 0.24$, $\gamma_X(2) = \gamma_X(-2) = 0.09$, e $\gamma_X(k) = 0$ para $|k| > 2$.  Isso indica que $X_t$ Ã© um processo MA(2).
    >
    > ```python
    > import numpy as np
    >
    > # Coeficientes do filtro
    > h = np.array([0.3, 0.4, 0.3])
    >
    > # AutocovariÃ¢ncias calculadas a partir dos coeficientes do filtro (para ruÃ­do branco com variÃ¢ncia 1)
    > autocovariances = np.convolve(h, h[::-1], mode='full')
    > lags = np.arange(-len(h)+1, len(h))
    >
    > # Imprimir as autocovariÃ¢ncias para diferentes lags
    > for lag, autocovariance in zip(lags, autocovariances):
    >     print(f"AutocovariÃ¢ncia no lag {lag}: {autocovariance:.4f}")
    > ```
    > Este exemplo demonstra como um filtro de mÃ©dia mÃ³vel transforma ruÃ­do branco num processo com dependÃªncia temporal.

Antes de prosseguir, Ã© Ãºtil considerar algumas propriedades adicionais dos filtros lineares em relaÃ§Ã£o Ã  estacionariedade.

**Lema 1:** Se $Y_t$ Ã© uma sÃ©rie temporal estacionÃ¡ria em covariÃ¢ncia e $h(L)$ Ã© um filtro linear com coeficientes absolutamente somÃ¡veis (isto Ã©, $\sum_{j=-\infty}^{\infty} |h_j| < \infty$), entÃ£o a sÃ©rie filtrada $X_t = h(L)Y_t$ tambÃ©m Ã© estacionÃ¡ria em covariÃ¢ncia.

**Prova:** Para provar que $X_t$ Ã© estacionÃ¡ria em covariÃ¢ncia, precisamos mostrar que sua mÃ©dia e autocovariÃ¢ncia sÃ£o independentes do tempo.

I. MÃ©dia de $X_t$:
$E[X_t] = E\left[\sum_{j=-\infty}^{\infty} h_j Y_{t-j}\right] = \sum_{j=-\infty}^{\infty} h_j E[Y_{t-j}] = \mu_Y \sum_{j=-\infty}^{\infty} h_j$. Como $Y_t$ Ã© estacionÃ¡ria, $E[Y_{t-j}] = \mu_Y$ Ã© constante. Se $\sum_{j=-\infty}^{\infty} |h_j| < \infty$, entÃ£o $\sum_{j=-\infty}^{\infty} h_j$ Ã© uma constante, e $E[X_t]$ Ã© independente de $t$.

II. AutocovariÃ¢ncia de $X_t$: JÃ¡ mostramos no Teorema 1 que $\gamma_X(k) = \sum_{i=-\infty}^{\infty} \sum_{j=-\infty}^{\infty} h_i h_j \gamma_Y(k+j-i)$.  Como $Y_t$ Ã© estacionÃ¡ria, $\gamma_Y(k+j-i)$ depende apenas de $k+j-i$ e nÃ£o de $t$.  AlÃ©m disso, dado que $\sum_{j=-\infty}^{\infty} |h_j| < \infty$, a dupla soma converge, e $\gamma_X(k)$ depende apenas de $k$ e nÃ£o de $t$.

Portanto, $X_t$ Ã© estacionÃ¡ria em covariÃ¢ncia. $\blacksquare$

AlÃ©m disso, podemos estender o Teorema 1 para o caso de mÃºltiplos filtros aplicados em sequÃªncia.

**Teorema 1.1:** (Efeito de MÃºltiplos Filtros Lineares na ACGF)
Seja $Y_t$ uma sÃ©rie temporal estacionÃ¡ria em covariÃ¢ncia com ACGF $g_Y(z)$. Sejam $h_1(L)$ e $h_2(L)$ dois filtros lineares, e seja $X_t = h_2(L)h_1(L)Y_t$ a sÃ©rie temporal resultante da aplicaÃ§Ã£o dos filtros $h_1(L)$ e $h_2(L)$ em sequÃªncia a $Y_t$. EntÃ£o, a ACGF de $X_t$ Ã© dada por:

$$g_X(z) = h_1(z)h_1(z^{-1})h_2(z)h_2(z^{-1})g_Y(z)$$

**Prova:**

I. Seja $W_t = h_1(L)Y_t$. Pelo Teorema 1, a ACGF de $W_t$ Ã© $g_W(z) = h_1(z)h_1(z^{-1})g_Y(z)$.

II. Agora, $X_t = h_2(L)W_t$. Novamente pelo Teorema 1, a ACGF de $X_t$ Ã© $g_X(z) = h_2(z)h_2(z^{-1})g_W(z)$.

III. Substituindo a expressÃ£o de $g_W(z)$ obtida no passo I:
$g_X(z) = h_2(z)h_2(z^{-1})h_1(z)h_1(z^{-1})g_Y(z)$.

Portanto, a ACGF de $X_t$ Ã© o produto das ACGFs dos filtros e da ACGF de $Y_t$. $\blacksquare$

    > ğŸ’¡ **Exemplo NumÃ©rico:**
    >
    > Seja $Y_t$ ruÃ­do branco com ACGF $g_Y(z) = 1$.  Aplicamos dois filtros em sequÃªncia: $h_1(L) = 0.5 + 0.5L$ e $h_2(L) = 1 - 0.8L$.
    >
    > EntÃ£o, $h_1(z) = 0.5 + 0.5z$, $h_1(z^{-1}) = 0.5 + 0.5z^{-1}$, $h_2(z) = 1 - 0.8z$ e $h_2(z^{-1}) = 1 - 0.8z^{-1}$.
    >
    > $g_X(z) = h_1(z)h_1(z^{-1})h_2(z)h_2(z^{-1})g_Y(z) = (0.5 + 0.5z)(0.5 + 0.5z^{-1})(1 - 0.8z)(1 - 0.8z^{-1})(1)$
    >
    > $g_X(z) = (0.25 + 0.25z + 0.25z^{-1} + 0.25)(1 - 0.8z - 0.8z^{-1} + 0.64)$
    >
    > $g_X(z) = (0.5 + 0.25z + 0.25z^{-1})(1.64 - 0.8z - 0.8z^{-1})$
    >
    > $g_X(z) = 0.82 - 0.4z - 0.4z^{-1} + 0.41z - 0.2z^2 - 0.2z^{-1} + 0.41z^{-1} - 0.2z^{-2} - 0.2z^2$
    >
    > $g_X(z) = -0.2z^{-2} - 0.2z^2 + (-0.4 - 0.2 + 0.41)z^{-1} + (-0.4 + 0.41 - 0.2)z + 0.82$
    >
    > $g_X(z) = -0.2z^{-2} - 0.2z^2 -0.19z^{-1} -0.19z + 0.82$
    >
    > O filtro combinado introduz autocorrelaÃ§Ãµes nos lags 1 e 2.

**CorolÃ¡rio 1.1:** O resultado do Teorema 1.1 pode ser generalizado para um nÃºmero arbitrÃ¡rio de filtros lineares aplicados em sequÃªncia. Se $X_t = h_n(L)h_{n-1}(L)...h_1(L)Y_t$, entÃ£o

$$g_X(z) = \left(\prod_{i=1}^{n} h_i(z)h_i(z^{-1})\right)g_Y(z)$$

**Prova:** A prova segue por induÃ§Ã£o a partir do Teorema 1.1. $\blacksquare$

**Teorema 1.2:** (ConvoluÃ§Ã£o de Filtros Lineares)

Sejam $h_1(L)$ e $h_2(L)$ dois filtros lineares com coeficientes $h_{1,j}$ e $h_{2,j}$, respectivamente. O filtro resultante da aplicaÃ§Ã£o de $h_2(L)$ seguido de $h_1(L)$, denotado por $h(L) = h_1(L)h_2(L)$, tem coeficientes $h_j$ dados pela convoluÃ§Ã£o dos coeficientes dos filtros $h_1(L)$ e $h_2(L)$:

$$h_j = \sum_{k=-\infty}^{\infty} h_{1,k} h_{2,j-k}$$

AlÃ©m disso, $h(z) = h_1(z)h_2(z)$.

**Prova:**

I.  Aplicamos o filtro $h_2(L)$ a $Y_t$ para obter $W_t = h_2(L)Y_t = \sum_{j=-\infty}^{\infty} h_{2,j} Y_{t-j}$.

II. Em seguida, aplicamos o filtro $h_1(L)$ a $W_t$ para obter $X_t = h_1(L)W_t = \sum_{k=-\infty}^{\infty} h_{1,k} W_{t-k}$.

III. SubstituÃ­mos a expressÃ£o de $W_{t-k}$:
    $X_t = \sum_{k=-\infty}^{\infty} h_{1,k} \left(\sum_{j=-\infty}^{\infty} h_{2,j} Y_{t-k-j}\right) = \sum_{k=-\infty}^{\infty} \sum_{j=-\infty}^{\infty} h_{1,k} h_{2,j} Y_{t-k-j}$.

IV. Realizamos uma mudanÃ§a de variÃ¡vel $m = k + j$, entÃ£o $j = m - k$:
$X_t = \sum_{k=-\infty}^{\infty} \sum_{m=-\infty}^{\infty} h_{1,k} h_{2,m-k} Y_{t-m} = \sum_{m=-\infty}^{\infty} \left(\sum_{k=-\infty}^{\infty} h_{1,k} h_{2,m-k}\right) Y_{t-m}$.

V. Definimos $h_m = \sum_{k=-\infty}^{\infty} h_{1,k} h_{2,m-k}$, que Ã© a convoluÃ§Ã£o dos coeficientes de $h_1(L)$ e $h_2(L)$.
Portanto, $X_t = \sum_{m=-\infty}^{\infty} h_m Y_{t-m} = h(L)Y_t$.

VI. Para mostrar que $h(z) = h_1(z)h_2(z)$, consideramos:
$h(z) = \sum_{j=-\infty}^{\infty} h_j z^j = \sum_{j=-\infty}^{\infty} \left(\sum_{k=-\infty}^{\infty} h_{1,k} h_{2,j-k}\right) z^j$.

VII. Fazemos uma mudanÃ§a de variÃ¡vel $m = j - k$, entÃ£o $j = m + k$:
$h(z) = \sum_{k=-\infty}^{\infty} \sum_{m=-\infty}^{\infty} h_{1,k} h_{2,m} z^{m+k} = \left(\sum_{k=-\infty}^{\infty} h_{1,k} z^k\right) \left(\sum_{m=-\infty}^{\infty} h_{2,m} z^m\right) = h_1(z)h_2(z)$.

Portanto, demonstramos que os coeficientes do filtro resultante sÃ£o dados pela convoluÃ§Ã£o dos coeficientes dos filtros individuais, e que $h(z) = h_1(z)h_2(z)$. $\blacksquare$

    > ğŸ’¡ **Exemplo NumÃ©rico:**
    > Seja $h_1(L) = 1 + 0.5L$ e $h_2(L) = 0.7 - 0.3L$. Vamos calcular o filtro resultante $h(L) = h_1(L)h_2(L)$.
    >
    > Os coeficientes de $h_1(L)$ sÃ£o $h_{1,0} = 1$ e $h_{1,1} = 0.5$, e os coeficientes de $h_2(L)$ sÃ£o $h_{2,0} = 0.7$ e $h_{2,1} = -0.3$.
    >
    > Usando a convoluÃ§Ã£o:
    >
    > $h_0 = (1)(0.7) + (0.5)(0) = 0.7$
    > $h_1 = (1)(-0.3) + (0.5)(0.7) = -0.3 + 0.35 = 0.05$
    > $h_2 = (1)(0) + (0.5)(-0.3) = -0.15$
    >
    > Portanto, $h(L) = 0.7 + 0.05L - 0.15L^2$.
    >
    > Alternativamente, podemos multiplicar os polinÃ´mios diretamente:
    >
    > $h(z) = (1 + 0.5z)(0.7 - 0.3z) = 0.7 - 0.3z + 0.35z - 0.15z^2 = 0.7 + 0.05z - 0.15z^2$
    >
    >  Que corresponde a $h(L) = 0.7 + 0.05L - 0.15L^2$, confirmando o resultado da convoluÃ§Ã£o.
    >
    > ```python
    > import numpy as np
    >
    > # Coeficientes dos filtros
    > h1 = np.array([1, 0.5])
    > h2 = np.array([0.7, -0.3])
    >
    > # ConvoluÃ§Ã£o dos coeficientes
    > h = np.convolve(h1, h2)
    >
    > print("Coeficientes do filtro resultante:", h)
    > ```

**Espectro de PotÃªncia e Filtros Lineares:**

Como discutido anteriormente, o espectro de potÃªncia de uma sÃ©rie temporal $Y_t$ pode ser obtido avaliando a ACGF no cÃ­rculo unitÃ¡rio complexo: $S_Y(\omega) = \frac{1}{2\pi} g_Y(e^{-i\omega})$ [^61].

A aplicaÃ§Ã£o de um filtro linear tambÃ©m afeta o espectro de potÃªncia. Seja $S_X(\omega)$ o espectro de potÃªncia da sÃ©rie filtrada $X_t$. EntÃ£o, do Teorema 1, temos:

$$S_X(\omega) = \frac{1}{2\pi} g_X(e^{-i\omega}) = \frac{1}{2\pi} h(e^{-i\omega})h(e^{i\omega})g_Y(e^{-i\omega}) = |h(e^{-i\omega})|^2 S_Y(\omega)$$

onde $|h(e^{-i\omega})|^2$ Ã© a funÃ§Ã£o de transferÃªncia do filtro. Isso mostra que o espectro de potÃªncia da sÃ©rie filtrada Ã© o produto do espectro de potÃªncia da sÃ©rie original e do mÃ³dulo ao quadrado da funÃ§Ã£o de transferÃªncia do filtro.

**Teorema 2:** (Efeito do Filtro Linear no Espectro de PotÃªncia)
Seja $Y_t$ uma sÃ©rie temporal estacionÃ¡ria em covariÃ¢ncia com espectro de potÃªncia $S_Y(\omega)$, e seja $X_t = h(L)Y_t$ a sÃ©rie temporal resultante da aplicaÃ§Ã£o do filtro linear $h(L)$ a $Y_t$. EntÃ£o, o espectro de potÃªncia de $X_t$ Ã© dado por:

$$S_X(\omega) = |h(e^{-i\omega})|^2 S_Y(\omega)$$

**Prova:** A prova segue diretamente do Teorema 1 e da relaÃ§Ã£o entre a ACGF e o espectro de potÃªncia.

I.  ComeÃ§amos com a relaÃ§Ã£o entre a ACGF e o espectro de potÃªncia: $S_Y(\omega) = \frac{1}{2\pi} g_Y(e^{-i\omega})$ e $S_X(\omega) = \frac{1}{2\pi} g_X(e^{-i\omega})$.

II. SubstituÃ­mos a expressÃ£o para $g_X(z)$ do Teorema 1, com $z = e^{-i\omega}$:
    $S_X(\omega) = \frac{1}{2\pi} g_X(e^{-i\omega}) = \frac{1}{2\pi} h(e^{-i\omega})h(e^{i\omega})g_Y(e^{-i\omega})$.

III. Reconhecemos que $h(e^{-i\omega})h(e^{i\omega}) = |h(e^{-i\omega})|^2$, onde $|h(e^{-i\omega})|$ Ã© o mÃ³dulo da funÃ§Ã£o de transferÃªncia do filtro:
     $S_X(\omega) = \frac{1}{2\pi} |h(e^{-i\omega})|^2 g_Y(e^{-i\omega})$.

IV. Usamos a relaÃ§Ã£o entre a ACGF e o espectro de potÃªncia para $Y_t$:
    $S_X(\omega) = |h(e^{-i\omega})|^2 \frac{1}{2\pi} g_Y(e^{-i\omega}) = |h(e^{-i\omega})|^2 S_Y(\omega)$.

Portanto, demonstramos que o espectro de potÃªncia da sÃ©rie filtrada Ã© o produto do espectro de potÃªncia da sÃ©rie original e do mÃ³dulo ao quadrado da funÃ§Ã£o de transferÃªncia do filtro. $\blacksquare$

    > ğŸ’¡ **Exemplo NumÃ©rico:**
    >
    > Seja $Y_t$ ruÃ­do branco com variÃ¢ncia $\sigma^2 = 2$. Portanto, $S_Y(\omega) = \frac{2}{2\pi} = \frac{1}{\pi}$.
    > Aplicamos o filtro $h(L) = 0.8 + 0.2L$. Portanto, $h(e^{-i\omega}) = 0.8 + 0.2e^{-i\omega}$.
    >
    > $|h(e^{-i\omega})|^2 = (0.8 + 0.2e^{-i\omega})(0.8 + 0.2e^{i\omega}) = 0.64 + 0.16e^{i\omega} + 0.16e^{-i\omega} + 0.04 = 0.68 + 0.32\cos(\omega)$.
    >
    > O espectro de potÃªncia da sÃ©rie filtrada Ã©:
    >
    > $S_X(\omega) = |h(e^{-i\omega})|^2 S_Y(\omega) = (0.68 + 0.32\cos(\omega)) \cdot \frac{1}{\pi} = \frac{0.68 + 0.32\cos(\omega)}{\pi}$.
    >
    > Para $\omega = 0$:
    > $S_X(0) = \frac{0.68 + 0.32}{\pi} = \frac{1}{\pi} \approx 0.3183$.  Isso representa a potÃªncia na frequÃªncia 0 (componente DC).
    >
    > Para $\omega = \pi$:
    > $S_X(\pi) = \frac{0.68 - 0.32}{\pi} = \frac{0.36}{\pi} \approx 0.1146$.  Isso representa a potÃªncia na frequÃªncia mÃ¡xima.
    >
    > Este exemplo mostra como o filtro redistribui a energia atravÃ©s das frequÃªncias.

**Exemplo: DiferenciaÃ§Ã£o (Primeira DiferenÃ§a)**

A diferenciaÃ§Ã£o Ã© um filtro linear comum usado para tornar sÃ©ries temporais nÃ£o estacionÃ¡rias em estacionÃ¡rias. O filtro de primeira diferenÃ§a Ã© dado por $h(L) = 1 - L$. Portanto, $h(z) = 1 - z$ e $h(z^{-1}) = 1 - z^{-1}$. A ACGF da sÃ©rie diferenciada $X_t = (1 - L)Y_t$ Ã©:

$$g_X(z) = (1 - z)(1 - z^{-1})g_Y(z)$$

O espectro de potÃªncia da sÃ©rie diferenciada Ã©:

$$S_X(\omega) = |1 - e^{-i\omega}|^2 S_Y(\omega) = (2 - 2\cos(\omega))S_Y(\omega) = 4\sin^2(\omega/2) S_Y(\omega)$$

Este exemplo ilustra como a diferenciaÃ§Ã£o modifica tanto a ACGF quanto o espectro de potÃªncia da sÃ©rie original. O filtro de diferenciaÃ§Ã£o tende a amplificar as frequÃªncias mais altas e atenuar as frequÃªncias mais baixas.

    > ğŸ’¡ **Exemplo NumÃ©rico:**
    >
    > Seja $Y_t$ uma sÃ©rie temporal com espectro de potÃªncia constante $S_Y(\omega) = C$, onde $C$ Ã© uma constante.  Aplicamos o filtro de primeira diferenÃ§a $h(L) = 1 - L$.
    >
    > O espectro de potÃªncia da sÃ©rie diferenciada Ã© $S_X(\omega) = 4\sin^2(\omega/2) S_Y(\omega) = 4C\sin^2(\omega/2)$.
    >
    > Para $\omega = 0$, $S_X(0) = 4C\sin^2(0) = 0$. A energia na frequÃªncia zero Ã© eliminada.
    >
    > Para $\omega = \pi$, $S_X(\pi) = 4C\sin^2(\pi/2) = 4C$. A energia na frequÃªncia mÃ¡xima Ã© amplificada por um fator de 4.
    >
    > Para $\omega = \pi/2$, $S_X(\pi/2) = 4C\sin^2(\pi/4) = 4C(1/\sqrt{2})^2 = 2C$.  A energia na frequÃªncia $\pi/2$ Ã© multiplicada por 2.
    >
    > Este exemplo demonstra que a diferenciaÃ§Ã£o suprime as baixas frequÃªncias e amplifica as altas frequÃªncias, transformando o espectro de potÃªncia original.

**AnÃ¡lise Espectral PrÃ¡tica**

Na prÃ¡tica, para obter o espectro populacional $s_Y(\omega)$, podemos usar a relaÃ§Ã£o [^61]:

$$s_Y(\omega) = \frac{g_Y(e^{-i\omega})}{2\pi}$$

Essa expressÃ£o permite estimar o espectro de potÃªncia de uma sÃ©rie temporal a partir de sua ACGF estimada.

### ConclusÃ£o

A aplicaÃ§Ã£o de filtros lineares Ã© uma tÃ©cnica comum na anÃ¡lise de sÃ©ries temporais, utilizada para modificar as propriedades espectrais dos dados. Este capÃ­tulo demonstrou que aplicar um filtro linear $h(L)$ a uma sÃ©rie temporal $Y_t$ resulta na multiplicaÃ§Ã£o de sua ACGF original por $h(z)h(z^{-1})$ [^62]. Consequentemente, o espectro de potÃªncia da sÃ©rie filtrada Ã© o produto do espectro de potÃªncia da sÃ©rie original e do mÃ³dulo ao quadrado da funÃ§Ã£o de transferÃªncia do filtro. Esses resultados fornecem uma base teÃ³rica para entender e controlar os efeitos dos filtros lineares na estrutura de dependÃªncia temporal das sÃ©ries temporais, sendo cruciais para a anÃ¡lise espectral e modelagem de sÃ©ries temporais.

### ReferÃªncias
[^61]: PÃ¡gina 61 do contexto original
[^62]: PÃ¡gina 62 do contexto original
<!-- END -->