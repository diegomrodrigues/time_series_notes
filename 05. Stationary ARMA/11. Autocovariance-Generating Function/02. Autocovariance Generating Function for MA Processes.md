## The Autocovariance-Generating Function for MA(q) Processes

### Introdu√ß√£o

Em continuidade ao conceito da **fun√ß√£o geradora de autocovari√¢ncia** (ACGF) introduzido anteriormente, este cap√≠tulo focar√° na forma espec√≠fica da ACGF para processos de **m√©dias m√≥veis** (MA) de ordem *q*. A ACGF √© uma ferramenta fundamental para analisar a estrutura de depend√™ncia temporal de um processo estoc√°stico covariance-stationary, condensando todas as autocovari√¢ncias em uma √∫nica fun√ß√£o [^61]. Apresentaremos a forma geral da ACGF para processos MA(q) e exploraremos o caso particular do processo MA(1), demonstrando como os coeficientes da ACGF est√£o relacionados com as autocovari√¢ncias do processo. Al√©m disso, discutiremos a rela√ß√£o entre a ACGF e a fun√ß√£o de autocorrela√ß√£o (ACF), que fornece uma medida normalizada da depend√™ncia temporal.

### Conceitos Fundamentais

Um processo de m√©dias m√≥veis de ordem *q*, denotado MA(q), √© definido como [^50]:

$$Y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \ldots + \theta_q \varepsilon_{t-q}$$

onde $\{\varepsilon_t\}$ √© uma sequ√™ncia de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, $\mu$ √© a m√©dia do processo e $\theta_i$ s√£o os coeficientes do modelo MA. Conforme mencionado anteriormente, o termo "moving average" deriva do fato de que $Y_t$ √© constru√≠do como uma soma ponderada de valores recentes de $\varepsilon_t$ [^48].

A **autocovari√¢ncia-geradora fun√ß√£o** (ACGF) para um processo MA(q) √© dada por [^62]:

$$g_Y(z) = \sigma^2 (1 + \theta_1 z + \theta_2 z^2 + \ldots + \theta_q z^q)(1 + \theta_1 z^{-1} + \theta_2 z^{-2} + \ldots + \theta_q z^{-q})$$

Essa express√£o representa a ACGF como o produto de dois polin√¥mios: um em termos de $z$ e outro em termos de $z^{-1}$. Os coeficientes desses polin√¥mios s√£o os coeficientes do modelo MA(q).

**Obten√ß√£o da ACGF:**

A ACGF para um MA(q) pode ser obtida da seguinte forma [^62]:

I. Primeiramente, escrevemos a express√£o para $Y_t$ em termos dos coeficientes MA e do ru√≠do branco: $Y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \ldots + \theta_q \varepsilon_{t-q}$ [^50].

II. Calculamos as autocovari√¢ncias $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$ para diferentes valores de *j* [^45].

III. Substitu√≠mos as autocovari√¢ncias na defini√ß√£o geral da ACGF: $g_Y(z) = \sum_{j=-\infty}^{\infty} \gamma_j z^j$ [^61].

IV. Simplificamos a express√£o para obter a forma final: $g_Y(z) = \sigma^2 (1 + \theta_1 z + \ldots + \theta_q z^q)(1 + \theta_1 z^{-1} + \ldots + \theta_q z^{-q})$ [^62].

**Interpreta√ß√£o:**

*   A vari√¢ncia do processo, $\gamma_0$, corresponde ao termo constante na ACGF.
*   As autocovari√¢ncias $\gamma_j$ correspondem aos coeficientes de $z^j$ e $z^{-j}$ na ACGF.
*   Para um processo MA(q), a ACGF tem um n√∫mero finito de termos, uma vez que as autocovari√¢ncias s√£o zero para lags maiores que *q* [^48].

    > üí° **Exemplo Num√©rico:**
    > Considere um processo MA(2) com $\theta_1 = 0.6$, $\theta_2 = 0.4$ e $\sigma^2 = 1$. A ACGF √©:
    > $$g_Y(z) = (1 + 0.6z + 0.4z^2)(1 + 0.6z^{-1} + 0.4z^{-2})$$
    > Expandindo:
    > $$g_Y(z) = 1 + 0.6z^{-1} + 0.4z^{-2} + 0.6z + 0.36 + 0.24z^{-1} + 0.4z^2 + 0.24z + 0.16$$
    > Agrupando termos:
    > $$g_Y(z) = 0.4z^2 + (0.6 + 0.24)z + (1 + 0.36 + 0.16) + (0.6 + 0.24)z^{-1} + 0.4z^{-2}$$
    > $$g_Y(z) = 0.4z^2 + 0.84z + 1.52 + 0.84z^{-1} + 0.4z^{-2}$$
    > Portanto, as autocovari√¢ncias s√£o:
    > *   $\gamma_0 = 1.52$
    > *   $\gamma_1 = \gamma_{-1} = 0.84$
    > *   $\gamma_2 = \gamma_{-2} = 0.4$
    > *   $\gamma_j = 0$ para $|j| > 2$
    >
    > Este exemplo demonstra como, a partir da ACGF, podemos extrair as autocovari√¢ncias do processo MA(2).

    Para complementar a interpreta√ß√£o da ACGF, podemos relacion√°-la com a **fun√ß√£o de autocorrela√ß√£o (ACF)**. A ACF, denotada por $\rho_j$, √© definida como a autocovari√¢ncia no lag *j* dividida pela vari√¢ncia do processo: $\rho_j = \frac{\gamma_j}{\gamma_0}$. A ACF fornece uma medida normalizada da correla√ß√£o entre $Y_t$ e $Y_{t-j}$, variando entre -1 e 1.

**Exemplo (MA(1)):**

Para um processo MA(1) [^48]: $Y_t = \mu + \varepsilon_t + \theta \varepsilon_{t-1}$.

A ACGF √© dada por [^62]:

$$g_Y(z) = \sigma^2[\theta z^{-1} + (1 + \theta^2) + \theta z]$$

Alternativamente, a ACGF pode ser escrita como [^62]:

$$g_Y(z) = \sigma^2(1 + \theta z)(1 + \theta z^{-1})$$

**Prova da Equival√™ncia das Formas da ACGF para MA(1):**

Provaremos que $\sigma^2[\theta z^{-1} + (1 + \theta^2) + \theta z] = \sigma^2(1 + \theta z)(1 + \theta z^{-1})$.

I. Expanda o lado direito da equa√ß√£o:
   $$\sigma^2(1 + \theta z)(1 + \theta z^{-1}) = \sigma^2(1 + \theta z^{-1} + \theta z + \theta^2 z z^{-1})$$

II. Simplifique a express√£o:
    $$\sigma^2(1 + \theta z^{-1} + \theta z + \theta^2) = \sigma^2(\theta z^{-1} + \theta z + 1 + \theta^2)$$

III. Reorganize os termos:
     $$\sigma^2(\theta z^{-1} + (1 + \theta^2) + \theta z)$$

IV. Portanto, demonstramos que $\sigma^2[\theta z^{-1} + (1 + \theta^2) + \theta z] = \sigma^2(1 + \theta z)(1 + \theta z^{-1})$ ‚ñ†

**Rela√ß√£o entre a ACGF e as Autocovari√¢ncias no MA(1):**

Para um processo MA(1), podemos extrair as autocovari√¢ncias diretamente da ACGF [^62]:

*   $\gamma_0 = (1 + \theta^2)\sigma^2$:  Este √© o coeficiente do termo constante na ACGF.
*   $\gamma_1 = \gamma_{-1} = \theta \sigma^2$: Este √© o coeficiente de $z$ e $z^{-1}$ na ACGF.
*   $\gamma_j = 0$ para $|j| > 1$:  N√£o h√° termos com pot√™ncias de *z* maiores que 1, indicando que as autocovari√¢ncias s√£o zero para lags maiores que 1.

    > üí° **Exemplo Num√©rico:**
    > Consideremos um processo MA(1) com $\theta = 0.5$ e $\sigma^2 = 2$. Ent√£o, a ACGF √©:
    > $g_Y(z) = 2[0.5z^{-1} + (1 + 0.5^2) + 0.5z] = 2[0.5z^{-1} + 1.25 + 0.5z]$
    >  $g_Y(z) = z^{-1} + 2.5 + z$
    > As autocovari√¢ncias s√£o:
    > $\gamma_0 = 2.5$
    > $\gamma_1 = \gamma_{-1} = 1$
    > $\gamma_j = 0$ para $|j| > 1$
    > Note que podemos ler esses valores diretamente da ACGF.

    Al√©m de obter as autocovari√¢ncias, podemos calcular a ACF para o processo MA(1) do exemplo:
    *   $\rho_0 = \frac{\gamma_0}{\gamma_0} = 1$
    *   $\rho_1 = \rho_{-1} = \frac{\gamma_1}{\gamma_0} = \frac{1}{2.5} = 0.4$
    *   $\rho_j = 0$ para $|j| > 1$
    A ACF do processo MA(1) mostra que a correla√ß√£o √© significativa apenas para o primeiro lag.

    > üí° **Exemplo Num√©rico:**
    > Vamos analisar a influ√™ncia do par√¢metro $\theta$ nos valores da ACF.
    >
    > Caso 1: $\theta = 0.2$
    > *   $\rho_1 = \frac{0.2}{1 + 0.2^2} = \frac{0.2}{1.04} \approx 0.192$
    >
    > Caso 2: $\theta = 0.8$
    > *   $\rho_1 = \frac{0.8}{1 + 0.8^2} = \frac{0.8}{1.64} \approx 0.488$
    >
    > Caso 3: $\theta = -0.5$
    > *   $\rho_1 = \frac{-0.5}{1 + (-0.5)^2} = \frac{-0.5}{1.25} = -0.4$
    >
    > Observa√ß√µes:
    > *   Quando $\theta$ √© pequeno (pr√≥ximo de 0), a correla√ß√£o no lag 1 √© fraca.
    > *   √Ä medida que $|\theta|$ aumenta, a correla√ß√£o no lag 1 se torna mais forte.
    > *   O sinal de $\theta$ determina o sinal da correla√ß√£o no lag 1.

Para formalizar a rela√ß√£o entre a ACGF e a ACF, podemos enunciar o seguinte teorema:

**Teorema 1:**
Seja $g_Y(z)$ a ACGF de um processo estoc√°stico covariance-stationary com vari√¢ncia $\gamma_0$. A fun√ß√£o de autocorrela√ß√£o (ACF) $\rho_j$ no lag *j* √© dada por o coeficiente de $z^j$ em $\frac{g_Y(z)}{\gamma_0}$.

**Prova:**
A prova segue diretamente da defini√ß√£o de ACF como $\rho_j = \frac{\gamma_j}{\gamma_0}$ e da defini√ß√£o da ACGF como $g_Y(z) = \sum_{j=-\infty}^{\infty} \gamma_j z^j$. Dividindo a ACGF por $\gamma_0$, obtemos $\frac{g_Y(z)}{\gamma_0} = \sum_{j=-\infty}^{\infty} \frac{\gamma_j}{\gamma_0} z^j = \sum_{j=-\infty}^{\infty} \rho_j z^j$. Portanto, o coeficiente de $z^j$ em $\frac{g_Y(z)}{\gamma_0}$ √© precisamente $\rho_j$.

Podemos estender o exemplo do processo MA(1) para calcular a ACF em termos do par√¢metro $\theta$ diretamente.

**Teorema 1.1:** Para um processo MA(1) definido por $Y_t = \mu + \varepsilon_t + \theta \varepsilon_{t-1}$, a fun√ß√£o de autocorrela√ß√£o (ACF) √© dada por:

*   $\rho_0 = 1$
*   $\rho_1 = \rho_{-1} = \frac{\theta}{1 + \theta^2}$
*   $\rho_j = 0$ para $|j| > 1$

**Prova:**
J√° sabemos que $\gamma_0 = (1 + \theta^2)\sigma^2$ e $\gamma_1 = \gamma_{-1} = \theta \sigma^2$.  Portanto, $\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{\theta \sigma^2}{(1 + \theta^2)\sigma^2} = \frac{\theta}{1 + \theta^2}$.  Os outros resultados seguem diretamente das defini√ß√µes de ACF e das autocovari√¢ncias do processo MA(1).

Este resultado fornece uma express√£o expl√≠cita para a ACF de um processo MA(1) em fun√ß√£o do par√¢metro $\theta$.

**Prova de que $\gamma_0 = (1 + \theta^2)\sigma^2$ e $\gamma_1 = \gamma_{-1} = \theta \sigma^2$ para um MA(1):**

I. Calcular $\gamma_0 = E[(Y_t - \mu)^2]$:
   $$Y_t - \mu = \varepsilon_t + \theta \varepsilon_{t-1}$$
   $$\gamma_0 = E[(\varepsilon_t + \theta \varepsilon_{t-1})^2] = E[\varepsilon_t^2 + 2\theta\varepsilon_t\varepsilon_{t-1} + \theta^2\varepsilon_{t-1}^2]$$

II. Usando a propriedade de ru√≠do branco $E[\varepsilon_t \varepsilon_{t-j}] = 0$ para $j \neq 0$ e $E[\varepsilon_t^2] = \sigma^2$:
    $$\gamma_0 = E[\varepsilon_t^2] + 2\theta E[\varepsilon_t\varepsilon_{t-1}] + \theta^2 E[\varepsilon_{t-1}^2] = \sigma^2 + 0 + \theta^2 \sigma^2 = (1 + \theta^2)\sigma^2$$

III. Calcular $\gamma_1 = E[(Y_t - \mu)(Y_{t-1} - \mu)]$:
     $$\gamma_1 = E[(\varepsilon_t + \theta \varepsilon_{t-1})(\varepsilon_{t-1} + \theta \varepsilon_{t-2})] = E[\varepsilon_t\varepsilon_{t-1} + \theta\varepsilon_{t-1}^2 + \theta\varepsilon_t\varepsilon_{t-2} + \theta^2\varepsilon_{t-1}\varepsilon_{t-2}]$$

IV. Usando as propriedades de ru√≠do branco:
    $$\gamma_1 = E[\varepsilon_t\varepsilon_{t-1}] + \theta E[\varepsilon_{t-1}^2] + \theta E[\varepsilon_t\varepsilon_{t-2}] + \theta^2 E[\varepsilon_{t-1}\varepsilon_{t-2}] = 0 + \theta\sigma^2 + 0 + 0 = \theta\sigma^2$$

V. Como o processo √© covariance-stationary, $\gamma_{-1} = \gamma_1 = \theta\sigma^2$.

VI. Portanto, demonstramos que $\gamma_0 = (1 + \theta^2)\sigma^2$ e $\gamma_1 = \gamma_{-1} = \theta \sigma^2$ para um processo MA(1) ‚ñ†

### Conclus√£o

A ACGF fornece uma representa√ß√£o compacta e √∫til da estrutura de autocovari√¢ncia de um processo MA(q) [^61]. Ao expressar a ACGF em termos dos coeficientes do modelo MA, podemos facilmente extrair as autocovari√¢ncias e analisar as propriedades do processo [^62]. O caso particular do processo MA(1) ilustra como a ACGF pode ser utilizada para identificar a vari√¢ncia e as autocovari√¢ncias do processo, facilitando a an√°lise e interpreta√ß√£o dos resultados.  Al√©m disso, a conex√£o entre a ACGF e a ACF, como demonstrado no Teorema 1, oferece uma perspectiva adicional sobre a depend√™ncia temporal, expressa em termos de correla√ß√µes normalizadas. A extens√£o para calcular a ACF em termos do par√¢metro $\theta$ no Teorema 1.1 fornece uma f√≥rmula direta para interpretar a for√ßa da depend√™ncia temporal em um processo MA(1). Nos cap√≠tulos subsequentes, veremos como a ACGF se estende a processos ARMA mais gerais e como ela √© utilizada na an√°lise espectral de s√©ries temporais.

### Refer√™ncias
[^45]: P√°gina 45 do contexto original
[^48]: P√°gina 48 do contexto original
[^50]: P√°gina 50 do contexto original
[^61]: P√°gina 61 do contexto original
[^62]: P√°gina 62 do contexto original
<!-- END -->