## T√≠tulo Conciso
An√°lise Detalhada do Processo de M√©dia M√≥vel de Ordem *q* (MA(*q*))

### Introdu√ß√£o
Expandindo os conceitos de **white noise** [^47] e processos de **m√©dia m√≥vel** (MA) explorados anteriormente [^48], este cap√≠tulo aprofunda-se na an√°lise do processo de m√©dia m√≥vel de ordem *q* (MA(*q*)). O MA(*q*) representa uma classe importante de modelos de s√©ries temporais onde o valor atual da s√©rie √© expresso como uma combina√ß√£o linear do ru√≠do branco presente e de *q* realiza√ß√µes passadas desse ru√≠do. Compreender as propriedades e caracter√≠sticas do MA(*q*) √© crucial para a modelagem e previs√£o de dados de s√©ries temporais em diversas √°reas da ci√™ncia e engenharia.

### Conceitos Fundamentais

Um processo de **m√©dia m√≥vel de ordem *q***, denotado por MA(*q*), √© definido como [^50]:

$$Y_t = \mu + \varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \dots + \theta_q\varepsilon_{t-q}$$

onde:

*   $Y_t$ representa o valor da s√©rie temporal no instante *t*.
*   $\mu$ √© a **m√©dia** do processo.
*   $\varepsilon_t$ √© um processo de **ru√≠do branco** com m√©dia zero e vari√¢ncia constante $\sigma^2$ [^47, 48, 50]. Formalmente, $E(\varepsilon_t) = 0$ [^47] e $E(\varepsilon_t \varepsilon_\tau) = 0$ para $t \neq \tau$ [^48].
*   $\theta_1, \theta_2, \dots, \theta_q$ s√£o os **par√¢metros** do modelo, que determinam a influ√™ncia das *q* realiza√ß√µes passadas do ru√≠do branco no valor atual da s√©rie temporal.

> üí° **Exemplo Num√©rico:** Considere um processo MA(1) com $\mu = 10$, $\theta_1 = 0.5$, e $\sigma^2 = 1$. Suponha que temos as seguintes realiza√ß√µes do ru√≠do branco: $\varepsilon_0 = 0.8$, $\varepsilon_1 = -0.2$, $\varepsilon_2 = 0.5$, $\varepsilon_3 = -0.3$. Ent√£o, os valores da s√©rie temporal $Y_t$ seriam:
>
> $Y_1 = 10 + (-0.2) + 0.5(0.8) = 10 - 0.2 + 0.4 = 10.2$
>
> $Y_2 = 10 + (0.5) + 0.5(-0.2) = 10 + 0.5 - 0.1 = 10.4$
>
> $Y_3 = 10 + (-0.3) + 0.5(0.5) = 10 - 0.3 + 0.25 = 9.95$
>
> Este exemplo ilustra como cada valor da s√©rie temporal √© uma combina√ß√£o da m√©dia, o ru√≠do branco atual e uma fra√ß√£o do ru√≠do branco anterior, ponderada pelo par√¢metro $\theta_1$.

A **expectativa** ou m√©dia do processo MA(*q*) √© dada por [^50]:

$$E(Y_t) = E(\mu + \varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \dots + \theta_q\varepsilon_{t-q})$$

Dado que a esperan√ßa √© um operador linear e $E(\varepsilon_t) = 0$ para todo *t*, temos:

$$E(Y_t) = \mu + E(\varepsilon_t) + \theta_1E(\varepsilon_{t-1}) + \theta_2E(\varepsilon_{t-2}) + \dots + \theta_qE(\varepsilon_{t-q}) = \mu$$

Portanto, a **m√©dia** de um processo MA(*q*) √© igual ao par√¢metro $\mu$, que representa um valor esperado constante ao longo do tempo [^50]. Este √© um resultado fundamental que simplifica a an√°lise e interpreta√ß√£o do modelo.

> üí° **Exemplo Num√©rico:** Se tivermos um processo MA(2) com $\mu = 5$, independentemente dos valores de $\theta_1$ e $\theta_2$ e das realiza√ß√µes do ru√≠do branco, a m√©dia da s√©rie temporal sempre ser√° $E(Y_t) = 5$.  Isso porque a esperan√ßa de cada termo $\varepsilon_t$ √© zero.

A **vari√¢ncia** do processo MA(*q*) √© dada por [^50]:

$$ \gamma_0 = E(Y_t - \mu)^2 = E(\varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \dots + \theta_q\varepsilon_{t-q})^2 $$

Expandindo o termo quadr√°tico e utilizando o fato de que as realiza√ß√µes do ru√≠do branco s√£o n√£o correlacionadas (i.e., $E(\varepsilon_t \varepsilon_\tau) = 0$ para $t \neq \tau$), obtemos [^50]:

$$ \gamma_0 = E(\varepsilon_t^2) + \theta_1^2 E(\varepsilon_{t-1}^2) + \theta_2^2 E(\varepsilon_{t-2}^2) + \dots + \theta_q^2 E(\varepsilon_{t-q}^2) = \sigma^2 (1 + \theta_1^2 + \theta_2^2 + \dots + \theta_q^2) $$

**Provaremos detalhadamente a expans√£o da vari√¢ncia.**

I. Come√ßamos com a defini√ß√£o da vari√¢ncia:
   $$\gamma_0 = E(Y_t - \mu)^2 = E(\varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \dots + \theta_q\varepsilon_{t-q})^2$$

II. Expandimos o quadrado:
   $$\gamma_0 = E[(\varepsilon_t + \theta_1\varepsilon_{t-1} + \dots + \theta_q\varepsilon_{t-q})(\varepsilon_t + \theta_1\varepsilon_{t-1} + \dots + \theta_q\varepsilon_{t-q})]$$

III. Distribu√≠mos os termos e aplicamos a esperan√ßa:
   $$\gamma_0 = E[\varepsilon_t^2 + \theta_1^2\varepsilon_{t-1}^2 + \dots + \theta_q^2\varepsilon_{t-q}^2 + 2\theta_1\varepsilon_t\varepsilon_{t-1} + 2\theta_2\varepsilon_t\varepsilon_{t-2} + \dots + 2\theta_q\varepsilon_t\varepsilon_{t-q} + \dots]$$
   $$\gamma_0 = E[\varepsilon_t^2] + \theta_1^2E[\varepsilon_{t-1}^2] + \dots + \theta_q^2E[\varepsilon_{t-q}^2] + 2\theta_1E[\varepsilon_t\varepsilon_{t-1}] + 2\theta_2E[\varepsilon_t\varepsilon_{t-2}] + \dots + 2\theta_qE[\varepsilon_t\varepsilon_{t-q}] + \dots$$

IV. Usamos o fato de que $E[\varepsilon_t \varepsilon_\tau] = 0$ para $t \neq \tau$ (as realiza√ß√µes do ru√≠do branco s√£o n√£o correlacionadas):
   $$\gamma_0 = E[\varepsilon_t^2] + \theta_1^2E[\varepsilon_{t-1}^2] + \dots + \theta_q^2E[\varepsilon_{t-q}^2]$$

V. Dado que $E[\varepsilon_t^2] = \sigma^2$ para todo *t*:
   $$\gamma_0 = \sigma^2 + \theta_1^2\sigma^2 + \dots + \theta_q^2\sigma^2$$

VI. Fatoramos $\sigma^2$:
   $$\gamma_0 = \sigma^2(1 + \theta_1^2 + \dots + \theta_q^2)$$ ‚ñ†

**Observa√ß√£o:** √â importante notar que a vari√¢ncia de um processo MA(*q*) √© sempre finita, uma vez que √© uma soma ponderada de vari√¢ncias do ru√≠do branco, que s√£o finitas.

> üí° **Exemplo Num√©rico:** Para um MA(2) com $\sigma^2 = 4$, $\theta_1 = 0.6$ e $\theta_2 = 0.4$, a vari√¢ncia seria:
>
> $\gamma_0 = 4(1 + 0.6^2 + 0.4^2) = 4(1 + 0.36 + 0.16) = 4(1.52) = 6.08$
>
> Isto demonstra que a vari√¢ncia da s√©rie temporal √© maior do que a vari√¢ncia do ru√≠do branco subjacente, devido √† contribui√ß√£o dos termos de m√©dia m√≥vel.

As **autocovari√¢ncias** s√£o dadas por [^51]:

$$ \gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] $$

Para $j = 1, 2, ..., q$ [^51]:

$$ \gamma_j = E[(\varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \dots + \theta_q\varepsilon_{t-q})(\varepsilon_{t-j} + \theta_1\varepsilon_{t-j-1} + \dots + \theta_q\varepsilon_{t-j-q})] $$

Dado que $E[\varepsilon_t \varepsilon_{t-j}] = 0$ para $j \neq 0$ e $E[\varepsilon_t^2] = \sigma^2$, obtemos [^51]:

$$
\gamma_j = \sigma^2 (\theta_j + \theta_1\theta_{j+1} + \theta_2\theta_{j+2} + \dots + \theta_{q-j}\theta_q)
$$

**Provaremos o c√°lculo das autocovari√¢ncias.**

I. Come√ßamos com a defini√ß√£o da autocovari√¢ncia para $j = 1, 2, ..., q$:
   $$ \gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \dots + \theta_q\varepsilon_{t-q})(\varepsilon_{t-j} + \theta_1\varepsilon_{t-j-1} + \dots + \theta_q\varepsilon_{t-j-q})] $$

II. Expandimos o produto e aplicamos a esperan√ßa:
   $$ \gamma_j = E[\varepsilon_t\varepsilon_{t-j} + \theta_1\varepsilon_{t-1}\varepsilon_{t-j} + \dots + \theta_q\varepsilon_{t-q}\varepsilon_{t-j} + \theta_1\varepsilon_t\varepsilon_{t-j-1} + \theta_1^2\varepsilon_{t-1}\varepsilon_{t-j-1} + \dots + \theta_q^2\varepsilon_{t-q}\varepsilon_{t-j-q}] $$
   $$ \gamma_j = E[\varepsilon_t\varepsilon_{t-j}] + \theta_1E[\varepsilon_{t-1}\varepsilon_{t-j}] + \dots + \theta_qE[\varepsilon_{t-q}\varepsilon_{t-j}] + \theta_1E[\varepsilon_t\varepsilon_{t-j-1}] + \theta_1^2E[\varepsilon_{t-1}\varepsilon_{t-j-1}] + \dots + \theta_q^2E[\varepsilon_{t-q}\varepsilon_{t-j-q}] $$

III. Usamos o fato de que $E[\varepsilon_t \varepsilon_\tau] = 0$ para $t \neq \tau$ e $E[\varepsilon_t^2] = \sigma^2$:
   Os √∫nicos termos que sobrevivem s√£o aqueles onde os √≠ndices de $\varepsilon$ coincidem.  Por exemplo, se $j=1$, ent√£o $E[\varepsilon_{t-1}\varepsilon_{t-j}] = E[\varepsilon_{t-1}^2] = \sigma^2$.

IV. Agrupamos os termos que resultam em $\sigma^2$:
   $$ \gamma_j = \sigma^2(\theta_j + \theta_1\theta_{j+1} + \theta_2\theta_{j+2} + \dots + \theta_{q-j}\theta_q) $$
   Essa express√£o considera todas as combina√ß√µes de termos cujo produto tem esperan√ßa diferente de zero.

V. Portanto:
$$
\gamma_j = \sigma^2 (\theta_j + \theta_1\theta_{j+1} + \theta_2\theta_{j+2} + \dots + \theta_{q-j}\theta_q)
$$ ‚ñ†

e $\gamma_j = 0$ para $j > q$ [^51].

**Provaremos que $\gamma_j = 0$ para $j > q$.**

I. Para $j > q$, a autocovari√¢ncia √©:
    $$ \gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\varepsilon_t + \theta_1\varepsilon_{t-1} + \dots + \theta_q\varepsilon_{t-q})(\varepsilon_{t-j} + \theta_1\varepsilon_{t-j-1} + \dots + \theta_q\varepsilon_{t-j-q})] $$

II. Quando $j > q$, n√£o h√° termos na expans√£o onde os √≠ndices dos $\varepsilon$ coincidam, ou seja, n√£o h√° termos da forma $E[\varepsilon_k^2]$ para nenhum *k*. Isso ocorre porque o maior lag em $Y_t$ √© *q*, e estamos correlacionando com $Y_{t-j}$, onde *j* √© maior que *q*. Portanto, o menor lag em $Y_{t-j}$ √© *j*, que √© maior que *q*.

III. Todos os termos na expans√£o ser√£o da forma $E[\varepsilon_i \varepsilon_k]$ onde $i \neq k$. Como o ru√≠do branco tem a propriedade de que $E[\varepsilon_i \varepsilon_k] = 0$ para $i \neq k$, todos os termos na expans√£o da esperan√ßa ser√£o zero.

IV. Portanto, $\gamma_j = 0$ para $j > q$. ‚ñ†

**Proposi√ß√£o 1:** As autocovari√¢ncias de um processo MA(*q*) s√£o sim√©tricas, i.e., $\gamma_j = \gamma_{-j}$.

*Proof:*
$$\gamma_{-j} = E[(Y_t - \mu)(Y_{t+j} - \mu)] = E[(Y_{t+j} - \mu)(Y_t - \mu)] = \gamma_j$$

> üí° **Exemplo Num√©rico:** Para ilustrar a simetria, se $\gamma_2 = 1.5$, ent√£o $\gamma_{-2}$ tamb√©m ser√° igual a 1.5.  Esta propriedade simplifica a an√°lise, pois s√≥ precisamos calcular as autocovari√¢ncias para lags positivos.

As **autocorrela√ß√µes** s√£o definidas como [^49]:

$$ \rho_j = \frac{\gamma_j}{\gamma_0} $$

Portanto, para um processo MA(*q*), as autocorrela√ß√µes s√£o diferentes de zero apenas para os *q* primeiros lags e zero para lags maiores que *q* [^49, 51]. Esta propriedade √© crucial para identificar a ordem *q* de um processo MA a partir dos dados.

> üí° **Exemplo Num√©rico:** Considere um processo MA(1) com $\theta_1 = 0.6$ e $\sigma^2 = 1$. Primeiro, calculamos $\gamma_0$ e $\gamma_1$:
>
> $\gamma_0 = \sigma^2(1 + \theta_1^2) = 1(1 + 0.6^2) = 1.36$
>
> $\gamma_1 = \sigma^2 \cdot \theta_1 = 1 \cdot 0.6 = 0.6$
>
> Agora, calculamos as autocorrela√ß√µes:
>
> $\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{0.6}{1.36} \approx 0.441$
>
> Para $j > 1$, $\rho_j = 0$. Isso significa que o correlograma (gr√°fico das autocorrela√ß√µes) ter√° um pico significativo em lag 1 e ser√° essencialmente zero para todos os outros lags.

**Teorema 1:** (Invertibilidade) Um processo MA(*q*) √© dito invert√≠vel se ele pode ser reescrito como um processo autorregressivo (AR) de ordem infinita. A condi√ß√£o necess√°ria e suficiente para a invertibilidade √© que as ra√≠zes do polin√¥mio $\Theta(z) = 1 + \theta_1 z + \theta_2 z^2 + \dots + \theta_q z^q$ estejam fora do c√≠rculo unit√°rio, i.e., $|z_i| > 1$ para todas as ra√≠zes $z_i$.

*Proof Strategy:* A demonstra√ß√£o envolve expressar o processo MA(*q*) usando o operador de retardo $B$, obtendo $Y_t = (1 + \theta_1 B + \dots + \theta_q B^q)\varepsilon_t = \Theta(B)\varepsilon_t$.  Para que o processo seja invert√≠vel, $\Theta(B)^{-1}$ deve convergir, o que ocorre se as ra√≠zes de $\Theta(z)$ estiverem fora do c√≠rculo unit√°rio.

**Corol√°rio 1:** Para um processo MA(1), $Y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1}$, a condi√ß√£o de invertibilidade √© $|\theta_1| < 1$.

*Proof:* O polin√¥mio caracter√≠stico √© $\Theta(z) = 1 + \theta_1 z$. A raiz √© $z = -1/\theta_1$. Para a invertibilidade, $|z| > 1$, ent√£o $|-1/\theta_1| > 1$, que implica $|\theta_1| < 1$.

> üí° **Exemplo Num√©rico:** Se $\theta_1 = 0.8$, ent√£o $|\theta_1| = 0.8 < 1$, e o processo MA(1) √© invert√≠vel. No entanto, se $\theta_1 = 1.2$, ent√£o $|\theta_1| = 1.2 > 1$, e o processo n√£o √© invert√≠vel. Invertibilidade garante que o modelo pode ser representado de forma est√°vel como um processo AR infinito.

**Teorema 1.1:** Se um processo MA(*q*) √© invert√≠vel, ent√£o existe uma representa√ß√£o autorregressiva infinita da forma $Y_t - \mu = \sum_{i=1}^{\infty} \pi_i (Y_{t-i} - \mu) + \varepsilon_t$, onde os coeficientes $\pi_i$ decaem √† medida que *i* aumenta.

*Proof Strategy:* Expressar o processo MA(*q*) como $Y_t - \mu = \Theta(B) \varepsilon_t$. Se o processo √© invert√≠vel, ent√£o $\Theta(B)^{-1}$ existe e pode ser representado como uma s√©rie de pot√™ncias em *B*: $\Theta(B)^{-1} = 1 - \sum_{i=1}^{\infty} \pi_i B^i$. Ent√£o, $\varepsilon_t = \Theta(B)^{-1}(Y_t - \mu) = (1 - \sum_{i=1}^{\infty} \pi_i B^i)(Y_t - \mu)$, o que leva √† representa√ß√£o autorregressiva infinita. A condi√ß√£o de invertibilidade garante que a s√©rie $\sum_{i=1}^{\infty} \pi_i B^i$ converge, implicando que os coeficientes $\pi_i$ decaem.

> üí° **Exemplo Num√©rico:** Para um processo MA(1) invert√≠vel com $\theta_1 = 0.5$, podemos express√°-lo como um AR(‚àû) com coeficientes que decaem. A rela√ß√£o √© aproximadamente $Y_t = \mu - \theta_1 (Y_{t-1} - \mu) - \theta_1^2 (Y_{t-2} - \mu) - \dots + \varepsilon_t$. Os coeficientes $\pi_i$ s√£o $-\theta_1^i$, que decaem geometricamente. Isso significa que a influ√™ncia de valores passados de $Y_t$ diminui exponencialmente √† medida que retrocedemos no tempo.

### Conclus√£o

O processo de m√©dia m√≥vel de ordem *q* (MA(*q*)) √© um modelo fundamental para a an√°lise de s√©ries temporais, caracterizado por sua depend√™ncia linear nas *q* realiza√ß√µes passadas de um ru√≠do branco. A m√©dia do processo √© constante e igual ao par√¢metro $\mu$, enquanto a vari√¢ncia e as autocorrela√ß√µes dependem dos par√¢metros $\theta_i$ e da vari√¢ncia do ru√≠do branco. A propriedade de que as autocorrela√ß√µes s√£o zero para lags maiores que *q* √© uma caracter√≠stica distintiva do modelo MA(*q*) e √© amplamente utilizada para identifica√ß√£o e modelagem de s√©ries temporais. O estudo detalhado do MA(*q*) fornece uma base s√≥lida para a compreens√£o de modelos mais complexos, como os modelos ARMA, que combinam componentes autorregressivos e de m√©dia m√≥vel.

### Refer√™ncias

[^47]: The basic building block for all the processes considered in this chapter is a sequence {8} - whose elements have mean zero and variance $\sigma^2$,  Œï(Œµt) = 0 [3.2.1].
[^48]: Œï(ŒµŒµ,) = 0 for t‚â† T. A process satisfying [3.2.1] through [3.2.3] is described as a white noise process.
[^49]: The jth autocorrelation of a covariance-stationary process (denoted p‚ÇÅ) is defined as its jth autocovariance divided by the variance: Pj = Œ•;/Œ≥Œø [3.3.6].
[^50]: A qth-order moving average process, denoted MA(q), is characterized by Y = Œº + Œµ‚ÇÅ + Œ∏ŒπŒµ,-1 + 0281-2 + ... + Œ∏Œ±ŒµŒπœÑŒ±Œπ [3.3.8]
[^51]: For j = 1, 2,..., 9, Œ•‚ÇÅ = Œï[(Œµ, + Œ∏ŒπŒµŒπ-1 + 0281-2 + + Œ∏Œ±ŒµŒπ-Œ±)√ó (ŒµŒπ-; + 0181-j-1 + 0281-j-2 + + @qr-j-q)] [3.3.11]
<!-- END -->