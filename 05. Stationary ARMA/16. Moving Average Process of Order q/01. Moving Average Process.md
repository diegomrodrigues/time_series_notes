## Título Conciso
Análise Detalhada do Processo de Média Móvel de Ordem *q* (MA(*q*))

### Introdução
Expandindo os conceitos de **white noise** [^47] e processos de **média móvel** (MA) explorados anteriormente [^48], este capítulo aprofunda-se na análise do processo de média móvel de ordem *q* (MA(*q*)). O MA(*q*) representa uma classe importante de modelos de séries temporais onde o valor atual da série é expresso como uma combinação linear do ruído branco presente e de *q* realizações passadas desse ruído. Compreender as propriedades e características do MA(*q*) é crucial para a modelagem e previsão de dados de séries temporais em diversas áreas da ciência e engenharia.

### Conceitos Fundamentais

Um processo de **média móvel de ordem *q***, denotado por MA(*q*), é definido como [^50]:

$$Y_t = \mu + \varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \dots + \theta_q\varepsilon_{t-q}$$

onde:

*   $Y_t$ representa o valor da série temporal no instante *t*.
*   $\mu$ é a **média** do processo.
*   $\varepsilon_t$ é um processo de **ruído branco** com média zero e variância constante $\sigma^2$ [^47, 48, 50]. Formalmente, $E(\varepsilon_t) = 0$ [^47] e $E(\varepsilon_t \varepsilon_\tau) = 0$ para $t \neq \tau$ [^48].
*   $\theta_1, \theta_2, \dots, \theta_q$ são os **parâmetros** do modelo, que determinam a influência das *q* realizações passadas do ruído branco no valor atual da série temporal.

> 💡 **Exemplo Numérico:** Considere um processo MA(1) com $\mu = 10$, $\theta_1 = 0.5$, e $\sigma^2 = 1$. Suponha que temos as seguintes realizações do ruído branco: $\varepsilon_0 = 0.8$, $\varepsilon_1 = -0.2$, $\varepsilon_2 = 0.5$, $\varepsilon_3 = -0.3$. Então, os valores da série temporal $Y_t$ seriam:
>
> $Y_1 = 10 + (-0.2) + 0.5(0.8) = 10 - 0.2 + 0.4 = 10.2$
>
> $Y_2 = 10 + (0.5) + 0.5(-0.2) = 10 + 0.5 - 0.1 = 10.4$
>
> $Y_3 = 10 + (-0.3) + 0.5(0.5) = 10 - 0.3 + 0.25 = 9.95$
>
> Este exemplo ilustra como cada valor da série temporal é uma combinação da média, o ruído branco atual e uma fração do ruído branco anterior, ponderada pelo parâmetro $\theta_1$.

A **expectativa** ou média do processo MA(*q*) é dada por [^50]:

$$E(Y_t) = E(\mu + \varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \dots + \theta_q\varepsilon_{t-q})$$

Dado que a esperança é um operador linear e $E(\varepsilon_t) = 0$ para todo *t*, temos:

$$E(Y_t) = \mu + E(\varepsilon_t) + \theta_1E(\varepsilon_{t-1}) + \theta_2E(\varepsilon_{t-2}) + \dots + \theta_qE(\varepsilon_{t-q}) = \mu$$

Portanto, a **média** de um processo MA(*q*) é igual ao parâmetro $\mu$, que representa um valor esperado constante ao longo do tempo [^50]. Este é um resultado fundamental que simplifica a análise e interpretação do modelo.

> 💡 **Exemplo Numérico:** Se tivermos um processo MA(2) com $\mu = 5$, independentemente dos valores de $\theta_1$ e $\theta_2$ e das realizações do ruído branco, a média da série temporal sempre será $E(Y_t) = 5$.  Isso porque a esperança de cada termo $\varepsilon_t$ é zero.

A **variância** do processo MA(*q*) é dada por [^50]:

$$ \gamma_0 = E(Y_t - \mu)^2 = E(\varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \dots + \theta_q\varepsilon_{t-q})^2 $$

Expandindo o termo quadrático e utilizando o fato de que as realizações do ruído branco são não correlacionadas (i.e., $E(\varepsilon_t \varepsilon_\tau) = 0$ para $t \neq \tau$), obtemos [^50]:

$$ \gamma_0 = E(\varepsilon_t^2) + \theta_1^2 E(\varepsilon_{t-1}^2) + \theta_2^2 E(\varepsilon_{t-2}^2) + \dots + \theta_q^2 E(\varepsilon_{t-q}^2) = \sigma^2 (1 + \theta_1^2 + \theta_2^2 + \dots + \theta_q^2) $$

**Provaremos detalhadamente a expansão da variância.**

I. Começamos com a definição da variância:
   $$\gamma_0 = E(Y_t - \mu)^2 = E(\varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \dots + \theta_q\varepsilon_{t-q})^2$$

II. Expandimos o quadrado:
   $$\gamma_0 = E[(\varepsilon_t + \theta_1\varepsilon_{t-1} + \dots + \theta_q\varepsilon_{t-q})(\varepsilon_t + \theta_1\varepsilon_{t-1} + \dots + \theta_q\varepsilon_{t-q})]$$

III. Distribuímos os termos e aplicamos a esperança:
   $$\gamma_0 = E[\varepsilon_t^2 + \theta_1^2\varepsilon_{t-1}^2 + \dots + \theta_q^2\varepsilon_{t-q}^2 + 2\theta_1\varepsilon_t\varepsilon_{t-1} + 2\theta_2\varepsilon_t\varepsilon_{t-2} + \dots + 2\theta_q\varepsilon_t\varepsilon_{t-q} + \dots]$$
   $$\gamma_0 = E[\varepsilon_t^2] + \theta_1^2E[\varepsilon_{t-1}^2] + \dots + \theta_q^2E[\varepsilon_{t-q}^2] + 2\theta_1E[\varepsilon_t\varepsilon_{t-1}] + 2\theta_2E[\varepsilon_t\varepsilon_{t-2}] + \dots + 2\theta_qE[\varepsilon_t\varepsilon_{t-q}] + \dots$$

IV. Usamos o fato de que $E[\varepsilon_t \varepsilon_\tau] = 0$ para $t \neq \tau$ (as realizações do ruído branco são não correlacionadas):
   $$\gamma_0 = E[\varepsilon_t^2] + \theta_1^2E[\varepsilon_{t-1}^2] + \dots + \theta_q^2E[\varepsilon_{t-q}^2]$$

V. Dado que $E[\varepsilon_t^2] = \sigma^2$ para todo *t*:
   $$\gamma_0 = \sigma^2 + \theta_1^2\sigma^2 + \dots + \theta_q^2\sigma^2$$

VI. Fatoramos $\sigma^2$:
   $$\gamma_0 = \sigma^2(1 + \theta_1^2 + \dots + \theta_q^2)$$ ■

**Observação:** É importante notar que a variância de um processo MA(*q*) é sempre finita, uma vez que é uma soma ponderada de variâncias do ruído branco, que são finitas.

> 💡 **Exemplo Numérico:** Para um MA(2) com $\sigma^2 = 4$, $\theta_1 = 0.6$ e $\theta_2 = 0.4$, a variância seria:
>
> $\gamma_0 = 4(1 + 0.6^2 + 0.4^2) = 4(1 + 0.36 + 0.16) = 4(1.52) = 6.08$
>
> Isto demonstra que a variância da série temporal é maior do que a variância do ruído branco subjacente, devido à contribuição dos termos de média móvel.

As **autocovariâncias** são dadas por [^51]:

$$ \gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] $$

Para $j = 1, 2, ..., q$ [^51]:

$$ \gamma_j = E[(\varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \dots + \theta_q\varepsilon_{t-q})(\varepsilon_{t-j} + \theta_1\varepsilon_{t-j-1} + \dots + \theta_q\varepsilon_{t-j-q})] $$

Dado que $E[\varepsilon_t \varepsilon_{t-j}] = 0$ para $j \neq 0$ e $E[\varepsilon_t^2] = \sigma^2$, obtemos [^51]:

$$
\gamma_j = \sigma^2 (\theta_j + \theta_1\theta_{j+1} + \theta_2\theta_{j+2} + \dots + \theta_{q-j}\theta_q)
$$

**Provaremos o cálculo das autocovariâncias.**

I. Começamos com a definição da autocovariância para $j = 1, 2, ..., q$:
   $$ \gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \dots + \theta_q\varepsilon_{t-q})(\varepsilon_{t-j} + \theta_1\varepsilon_{t-j-1} + \dots + \theta_q\varepsilon_{t-j-q})] $$

II. Expandimos o produto e aplicamos a esperança:
   $$ \gamma_j = E[\varepsilon_t\varepsilon_{t-j} + \theta_1\varepsilon_{t-1}\varepsilon_{t-j} + \dots + \theta_q\varepsilon_{t-q}\varepsilon_{t-j} + \theta_1\varepsilon_t\varepsilon_{t-j-1} + \theta_1^2\varepsilon_{t-1}\varepsilon_{t-j-1} + \dots + \theta_q^2\varepsilon_{t-q}\varepsilon_{t-j-q}] $$
   $$ \gamma_j = E[\varepsilon_t\varepsilon_{t-j}] + \theta_1E[\varepsilon_{t-1}\varepsilon_{t-j}] + \dots + \theta_qE[\varepsilon_{t-q}\varepsilon_{t-j}] + \theta_1E[\varepsilon_t\varepsilon_{t-j-1}] + \theta_1^2E[\varepsilon_{t-1}\varepsilon_{t-j-1}] + \dots + \theta_q^2E[\varepsilon_{t-q}\varepsilon_{t-j-q}] $$

III. Usamos o fato de que $E[\varepsilon_t \varepsilon_\tau] = 0$ para $t \neq \tau$ e $E[\varepsilon_t^2] = \sigma^2$:
   Os únicos termos que sobrevivem são aqueles onde os índices de $\varepsilon$ coincidem.  Por exemplo, se $j=1$, então $E[\varepsilon_{t-1}\varepsilon_{t-j}] = E[\varepsilon_{t-1}^2] = \sigma^2$.

IV. Agrupamos os termos que resultam em $\sigma^2$:
   $$ \gamma_j = \sigma^2(\theta_j + \theta_1\theta_{j+1} + \theta_2\theta_{j+2} + \dots + \theta_{q-j}\theta_q) $$
   Essa expressão considera todas as combinações de termos cujo produto tem esperança diferente de zero.

V. Portanto:
$$
\gamma_j = \sigma^2 (\theta_j + \theta_1\theta_{j+1} + \theta_2\theta_{j+2} + \dots + \theta_{q-j}\theta_q)
$$ ■

e $\gamma_j = 0$ para $j > q$ [^51].

**Provaremos que $\gamma_j = 0$ para $j > q$.**

I. Para $j > q$, a autocovariância é:
    $$ \gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\varepsilon_t + \theta_1\varepsilon_{t-1} + \dots + \theta_q\varepsilon_{t-q})(\varepsilon_{t-j} + \theta_1\varepsilon_{t-j-1} + \dots + \theta_q\varepsilon_{t-j-q})] $$

II. Quando $j > q$, não há termos na expansão onde os índices dos $\varepsilon$ coincidam, ou seja, não há termos da forma $E[\varepsilon_k^2]$ para nenhum *k*. Isso ocorre porque o maior lag em $Y_t$ é *q*, e estamos correlacionando com $Y_{t-j}$, onde *j* é maior que *q*. Portanto, o menor lag em $Y_{t-j}$ é *j*, que é maior que *q*.

III. Todos os termos na expansão serão da forma $E[\varepsilon_i \varepsilon_k]$ onde $i \neq k$. Como o ruído branco tem a propriedade de que $E[\varepsilon_i \varepsilon_k] = 0$ para $i \neq k$, todos os termos na expansão da esperança serão zero.

IV. Portanto, $\gamma_j = 0$ para $j > q$. ■

**Proposição 1:** As autocovariâncias de um processo MA(*q*) são simétricas, i.e., $\gamma_j = \gamma_{-j}$.

*Proof:*
$$\gamma_{-j} = E[(Y_t - \mu)(Y_{t+j} - \mu)] = E[(Y_{t+j} - \mu)(Y_t - \mu)] = \gamma_j$$

> 💡 **Exemplo Numérico:** Para ilustrar a simetria, se $\gamma_2 = 1.5$, então $\gamma_{-2}$ também será igual a 1.5.  Esta propriedade simplifica a análise, pois só precisamos calcular as autocovariâncias para lags positivos.

As **autocorrelações** são definidas como [^49]:

$$ \rho_j = \frac{\gamma_j}{\gamma_0} $$

Portanto, para um processo MA(*q*), as autocorrelações são diferentes de zero apenas para os *q* primeiros lags e zero para lags maiores que *q* [^49, 51]. Esta propriedade é crucial para identificar a ordem *q* de um processo MA a partir dos dados.

> 💡 **Exemplo Numérico:** Considere um processo MA(1) com $\theta_1 = 0.6$ e $\sigma^2 = 1$. Primeiro, calculamos $\gamma_0$ e $\gamma_1$:
>
> $\gamma_0 = \sigma^2(1 + \theta_1^2) = 1(1 + 0.6^2) = 1.36$
>
> $\gamma_1 = \sigma^2 \cdot \theta_1 = 1 \cdot 0.6 = 0.6$
>
> Agora, calculamos as autocorrelações:
>
> $\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{0.6}{1.36} \approx 0.441$
>
> Para $j > 1$, $\rho_j = 0$. Isso significa que o correlograma (gráfico das autocorrelações) terá um pico significativo em lag 1 e será essencialmente zero para todos os outros lags.

**Teorema 1:** (Invertibilidade) Um processo MA(*q*) é dito invertível se ele pode ser reescrito como um processo autorregressivo (AR) de ordem infinita. A condição necessária e suficiente para a invertibilidade é que as raízes do polinômio $\Theta(z) = 1 + \theta_1 z + \theta_2 z^2 + \dots + \theta_q z^q$ estejam fora do círculo unitário, i.e., $|z_i| > 1$ para todas as raízes $z_i$.

*Proof Strategy:* A demonstração envolve expressar o processo MA(*q*) usando o operador de retardo $B$, obtendo $Y_t = (1 + \theta_1 B + \dots + \theta_q B^q)\varepsilon_t = \Theta(B)\varepsilon_t$.  Para que o processo seja invertível, $\Theta(B)^{-1}$ deve convergir, o que ocorre se as raízes de $\Theta(z)$ estiverem fora do círculo unitário.

**Corolário 1:** Para um processo MA(1), $Y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1}$, a condição de invertibilidade é $|\theta_1| < 1$.

*Proof:* O polinômio característico é $\Theta(z) = 1 + \theta_1 z$. A raiz é $z = -1/\theta_1$. Para a invertibilidade, $|z| > 1$, então $|-1/\theta_1| > 1$, que implica $|\theta_1| < 1$.

> 💡 **Exemplo Numérico:** Se $\theta_1 = 0.8$, então $|\theta_1| = 0.8 < 1$, e o processo MA(1) é invertível. No entanto, se $\theta_1 = 1.2$, então $|\theta_1| = 1.2 > 1$, e o processo não é invertível. Invertibilidade garante que o modelo pode ser representado de forma estável como um processo AR infinito.

**Teorema 1.1:** Se um processo MA(*q*) é invertível, então existe uma representação autorregressiva infinita da forma $Y_t - \mu = \sum_{i=1}^{\infty} \pi_i (Y_{t-i} - \mu) + \varepsilon_t$, onde os coeficientes $\pi_i$ decaem à medida que *i* aumenta.

*Proof Strategy:* Expressar o processo MA(*q*) como $Y_t - \mu = \Theta(B) \varepsilon_t$. Se o processo é invertível, então $\Theta(B)^{-1}$ existe e pode ser representado como uma série de potências em *B*: $\Theta(B)^{-1} = 1 - \sum_{i=1}^{\infty} \pi_i B^i$. Então, $\varepsilon_t = \Theta(B)^{-1}(Y_t - \mu) = (1 - \sum_{i=1}^{\infty} \pi_i B^i)(Y_t - \mu)$, o que leva à representação autorregressiva infinita. A condição de invertibilidade garante que a série $\sum_{i=1}^{\infty} \pi_i B^i$ converge, implicando que os coeficientes $\pi_i$ decaem.

> 💡 **Exemplo Numérico:** Para um processo MA(1) invertível com $\theta_1 = 0.5$, podemos expressá-lo como um AR(∞) com coeficientes que decaem. A relação é aproximadamente $Y_t = \mu - \theta_1 (Y_{t-1} - \mu) - \theta_1^2 (Y_{t-2} - \mu) - \dots + \varepsilon_t$. Os coeficientes $\pi_i$ são $-\theta_1^i$, que decaem geometricamente. Isso significa que a influência de valores passados de $Y_t$ diminui exponencialmente à medida que retrocedemos no tempo.

### Conclusão

O processo de média móvel de ordem *q* (MA(*q*)) é um modelo fundamental para a análise de séries temporais, caracterizado por sua dependência linear nas *q* realizações passadas de um ruído branco. A média do processo é constante e igual ao parâmetro $\mu$, enquanto a variância e as autocorrelações dependem dos parâmetros $\theta_i$ e da variância do ruído branco. A propriedade de que as autocorrelações são zero para lags maiores que *q* é uma característica distintiva do modelo MA(*q*) e é amplamente utilizada para identificação e modelagem de séries temporais. O estudo detalhado do MA(*q*) fornece uma base sólida para a compreensão de modelos mais complexos, como os modelos ARMA, que combinam componentes autorregressivos e de média móvel.

### Referências

[^47]: The basic building block for all the processes considered in this chapter is a sequence {8} - whose elements have mean zero and variance $\sigma^2$,  Ε(εt) = 0 [3.2.1].
[^48]: Ε(εε,) = 0 for t≠ T. A process satisfying [3.2.1] through [3.2.3] is described as a white noise process.
[^49]: The jth autocorrelation of a covariance-stationary process (denoted p₁) is defined as its jth autocovariance divided by the variance: Pj = Υ;/γο [3.3.6].
[^50]: A qth-order moving average process, denoted MA(q), is characterized by Y = μ + ε₁ + θιε,-1 + 0281-2 + ... + θαειται [3.3.8]
[^51]: For j = 1, 2,..., 9, Υ₁ = Ε[(ε, + θιει-1 + 0281-2 + + θαει-α)× (ει-; + 0181-j-1 + 0281-j-2 + + @qr-j-q)] [3.3.11]
<!-- END -->