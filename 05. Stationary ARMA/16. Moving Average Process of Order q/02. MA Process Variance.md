## An√°lise Detalhada do Processo de M√©dia M√≥vel de Ordem $q$ (MA($q$))

### Introdu√ß√£o
Em continuidade ao conceito de processo de m√©dia m√≥vel (MA) e white noise [^48, 47], este cap√≠tulo visa aprofundar a an√°lise do processo de m√©dia m√≥vel de ordem $q$ (MA($q$)). Este modelo crucial de s√©rie temporal expressa o valor presente como uma combina√ß√£o linear do ru√≠do branco atual e $q$ valores passados do mesmo. A compreens√£o das caracter√≠sticas do MA($q$) √© essencial para modelagem e previs√£o de s√©ries temporais em diversas √°reas da ci√™ncia e engenharia. Como um prel√∫dio, vamos reintroduzir formalmente o conceito j√° mencionado.

### Conceitos Fundamentais

Um processo de **m√©dia m√≥vel de ordem $q$**, denotado por MA($q$), √© definido como [^50]:

$$Y_t = \mu + \varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \dots + \theta_q\varepsilon_{t-q}$$

onde:

*   $Y_t$ representa o valor da s√©rie temporal no instante $t$.
*   $\mu$ √© a **m√©dia** do processo.
*   $\varepsilon_t$ √© um processo de **ru√≠do branco** com m√©dia zero e vari√¢ncia constante $\sigma^2$ [^47, 48, 50]. Formalmente, $E(\varepsilon_t) = 0$ [^47] e $E(\varepsilon_t \varepsilon_\tau) = 0$ para $t \neq \tau$ [^48].
*   $\theta_1, \theta_2, \dots, \theta_q$ s√£o os **par√¢metros** do modelo, que determinam a influ√™ncia das $q$ realiza√ß√µes passadas do ru√≠do branco no valor atual da s√©rie temporal.

Anteriormente, j√° derivamos a m√©dia, vari√¢ncia e autocovari√¢ncia para o MA($q$). Neste cap√≠tulo, enfocaremos a condi√ß√£o de stationarity e, principalmente, mostraremos que o processo MA($q$) √© sempre covariance-stationary, pois sua m√©dia e autocovari√¢ncias n√£o dependem do tempo. Al√©m disso, a vari√¢ncia de um processo MA($q$) √© dada por [^50]:

$$ \gamma_0 = (1 + \theta_1^2 + \theta_2^2 + \dots + \theta_q^2)\sigma^2 $$

que leva em conta as contribui√ß√µes da vari√¢ncia das inova√ß√µes (ru√≠do branco) atuais e de $q$ lags passados.

**Teorema 1:** (M√©dia do Processo MA($q$)) A m√©dia do processo MA($q$) √© dada por $E(Y_t) = \mu$.

*Proof:*
Tomando o valor esperado de ambos os lados da defini√ß√£o do processo MA($q$), temos:

$$ E(Y_t) = E(\mu + \varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \dots + \theta_q\varepsilon_{t-q}) $$

Como o valor esperado de uma soma √© a soma dos valores esperados, e o valor esperado de uma constante √© a pr√≥pria constante, temos:

$$ E(Y_t) = E(\mu) + E(\varepsilon_t) + \theta_1E(\varepsilon_{t-1}) + \theta_2E(\varepsilon_{t-2}) + \dots + \theta_qE(\varepsilon_{t-q}) $$

Sabemos que $E(\mu) = \mu$ e $E(\varepsilon_t) = 0$ para todo $t$. Portanto:

$$ E(Y_t) = \mu + 0 + \theta_1 \cdot 0 + \theta_2 \cdot 0 + \dots + \theta_q \cdot 0 = \mu $$

Assim, a m√©dia do processo MA($q$) √© $\mu$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um processo MA(1) com $\mu = 10$ e $\theta_1 = 0.6$. O ru√≠do branco $\varepsilon_t$ tem m√©dia zero.  Simulamos alguns valores:
>
> *   $Y_1 = 10 + \varepsilon_1$
> *   $Y_2 = 10 + \varepsilon_2 + 0.6\varepsilon_1$
> *   $Y_3 = 10 + \varepsilon_3 + 0.6\varepsilon_2$
>
> Se $\varepsilon_1 = 2$, $\varepsilon_2 = -1$, $\varepsilon_3 = 0.5$, ent√£o:
>
> *   $Y_1 = 10 + 2 = 12$
> *   $Y_2 = 10 - 1 + 0.6(2) = 10 - 1 + 1.2 = 10.2$
> *   $Y_3 = 10 + 0.5 + 0.6(-1) = 10 + 0.5 - 0.6 = 9.9$
>
> O valor esperado de $Y_t$ ao longo de um longo per√≠odo de tempo convergir√° para $\mu = 10$, conforme demonstrado no Teorema 1.
>
> ```python
> import numpy as np
>
> mu = 10
> theta1 = 0.6
> sigma = 1  # Standard deviation of the white noise
>
> num_samples = 1000
> errors = np.random.normal(0, sigma, num_samples)
>
> Y = np.zeros(num_samples)
> Y[0] = mu + errors[0]
> for t in range(1, num_samples):
>     Y[t] = mu + errors[t] + theta1 * errors[t-1]
>
> print(f"Sample mean of Y: {np.mean(Y)}")
> # Expected output is a value close to 10.
> ```

**Teorema 2: Covariance-Stationarity do Processo MA($q$)**
Um processo MA($q$) √© covariance-stationary.

*Proof:*
Para provar que o processo MA($q$) √© covariance-stationary, precisamos mostrar que tanto sua m√©dia quanto suas autocovari√¢ncias n√£o dependem do tempo.

I. **M√©dia:**
J√° mostramos anteriormente que a m√©dia do processo MA($q$) √© dada por $E(Y_t) = \mu$. Como $\mu$ √© uma constante e n√£o depende de $t$, a m√©dia do processo √© independente do tempo.

II. **Autocovari√¢ncias:**
As autocovari√¢ncias s√£o definidas como $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$. Para o processo MA($q$), temos [^51]:
$$ \gamma_j = \sigma^2(\theta_j + \theta_1\theta_{j+1} + \theta_2\theta_{j+2} + \dots + \theta_{q-j}\theta_q)  \text{  para } j \leq q $$
e $\gamma_j = 0$ para $j > q$ [^51].
Observe que a express√£o para $\gamma_j$ depende apenas dos par√¢metros do modelo ($\theta_i$ e $\sigma^2$) e n√£o depende explicitamente de $t$. Portanto, as autocovari√¢ncias do processo MA($q$) tamb√©m n√£o dependem do tempo.

III. **Conclus√£o:**
Como tanto a m√©dia quanto as autocovari√¢ncias do processo MA($q$) s√£o independentes do tempo, o processo √© covariance-stationary. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um processo MA(2) com $\theta_1 = 0.5$, $\theta_2 = 0.3$ e $\sigma^2 = 2$. As autocovari√¢ncias s√£o:
>
> $\gamma_0 = (1 + \theta_1^2 + \theta_2^2)\sigma^2 = (1 + 0.5^2 + 0.3^2) \cdot 2 = (1 + 0.25 + 0.09) \cdot 2 = 2.68$
>
> $\gamma_1 = (\theta_1 + \theta_1\theta_2)\sigma^2 = (0.5 + 0.5 \cdot 0.3) \cdot 2 = (0.5 + 0.15) \cdot 2 = 1.3$
>
> $\gamma_2 = \theta_2 \sigma^2 = 0.3 \cdot 2 = 0.6$
>
> Para $j > 2$, $\gamma_j = 0$. Note que todas essas autocovari√¢ncias s√£o constantes e n√£o variam com o tempo, confirmando a stationarity do MA(2).
>
> ```python
> import numpy as np
>
> sigma_squared = 2
> theta1 = 0.5
> theta2 = 0.3
>
> gamma0 = (1 + theta1**2 + theta2**2) * sigma_squared
> gamma1 = (theta1 + theta1*theta2) * sigma_squared
> gamma2 = theta2 * sigma_squared
>
> print(f"Autocovariance gamma_0: {gamma0}")
> print(f"Autocovariance gamma_1: {gamma1}")
> print(f"Autocovariance gamma_2: {gamma2}")
> ```

**Corol√°rio 1:** (Deriva√ß√£o da Vari√¢ncia do Processo MA($q$)) A vari√¢ncia de um processo MA($q$) √© dada por $\gamma_0 = (1 + \theta_1^2 + \theta_2^2 + \dots + \theta_q^2)\sigma^2$.

*Proof:*
I.  A vari√¢ncia √© definida como $\gamma_0 = E[(Y_t - \mu)^2]$.

II. Substituindo a defini√ß√£o de $Y_t$ do processo MA($q$), temos:

    $$ \gamma_0 = E[(\varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \dots + \theta_q\varepsilon_{t-q})^2] $$

III. Expandindo o quadrado e aplicando o valor esperado:

    $$ \gamma_0 = E[\varepsilon_t^2 + \theta_1^2\varepsilon_{t-1}^2 + \theta_2^2\varepsilon_{t-2}^2 + \dots + \theta_q^2\varepsilon_{t-q}^2 + 2\theta_1\varepsilon_t\varepsilon_{t-1} + 2\theta_2\varepsilon_t\varepsilon_{t-2} + \dots + 2\theta_q\varepsilon_t\varepsilon_{t-q} + \dots] $$

IV. Como $E(\varepsilon_t^2) = \sigma^2$ e $E(\varepsilon_t\varepsilon_{t-j}) = 0$ para $t \neq t-j$ (devido √† propriedade do ru√≠do branco), a express√£o se simplifica para:

    $$ \gamma_0 = \sigma^2 + \theta_1^2\sigma^2 + \theta_2^2\sigma^2 + \dots + \theta_q^2\sigma^2 $$

V. Fatorando $\sigma^2$, obtemos:

    $$ \gamma_0 = (1 + \theta_1^2 + \theta_2^2 + \dots + \theta_q^2)\sigma^2 $$

Assim, a vari√¢ncia do processo MA($q$) √© $\gamma_0 = (1 + \theta_1^2 + \theta_2^2 + \dots + \theta_q^2)\sigma^2$. $\blacksquare$

**Corol√°rio 2:** (Vari√¢ncia Constante) A vari√¢ncia de um processo MA($q$) √© constante e dada por $\gamma_0 = (1 + \theta_1^2 + \theta_2^2 + \dots + \theta_q^2)\sigma^2$.

*Proof:* A vari√¢ncia √© o caso especial da autocovari√¢ncia quando $j = 0$. Como a autocovari√¢ncia n√£o depende do tempo, a vari√¢ncia tamb√©m √© constante.

**Teorema 3:** (Autocorrela√ß√µes do Processo MA($q$)) As autocorrela√ß√µes $\rho_j$ de um processo MA($q$) s√£o dadas por:

$$ \rho_j = \frac{\gamma_j}{\gamma_0} = \begin{cases}
\frac{\theta_j + \theta_1\theta_{j+1} + \theta_2\theta_{j+2} + \dots + \theta_{q-j}\theta_q}{1 + \theta_1^2 + \theta_2^2 + \dots + \theta_q^2} & \text{para } j \leq q \\
0 & \text{para } j > q
\end{cases} $$

*Proof:* As autocorrela√ß√µes s√£o definidas como as autocovari√¢ncias normalizadas pela vari√¢ncia: $\rho_j = \frac{\gamma_j}{\gamma_0}$ [^49]. Usando as express√µes j√° derivadas para $\gamma_j$ e $\gamma_0$, obtemos diretamente o resultado. $\blacksquare$

> üí° **Exemplo Num√©rico:** Usando o mesmo exemplo MA(2) com $\theta_1 = 0.5$, $\theta_2 = 0.3$ e $\sigma^2 = 2$, calculamos as autocorrela√ß√µes:
>
> $\gamma_0 = 2.68$, $\gamma_1 = 1.3$, $\gamma_2 = 0.6$.
>
> $\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{1.3}{2.68} \approx 0.485$
>
> $\rho_2 = \frac{\gamma_2}{\gamma_0} = \frac{0.6}{2.68} \approx 0.224$
>
> Para $j > 2$, $\rho_j = 0$.  Esses valores indicam a correla√ß√£o entre um ponto na s√©rie temporal e seus dois primeiros lags.  O corte abrupto ap√≥s lag 2 √© uma caracter√≠stica distintiva dos processos MA(2).
>
> ```python
> import numpy as np
>
> theta1 = 0.5
> theta2 = 0.3
> sigma_squared = 2
>
> gamma0 = (1 + theta1**2 + theta2**2) * sigma_squared
> gamma1 = (theta1 + theta1*theta2) * sigma_squared
> gamma2 = theta2 * sigma_squared
>
> rho1 = gamma1 / gamma0
> rho2 = gamma2 / gamma0
>
> print(f"Autocorrelation rho_1: {rho1}")
> print(f"Autocorrelation rho_2: {rho2}")
> ```

**Teorema 3.1:** (Bound on Autocorrelations) The autocorrelations $\rho_j$ of a MA($q$) process satisfy $|\rho_j| \leq 1$ for all $j$.

*Proof:* This follows directly from the fact that $\rho_j = \frac{\gamma_j}{\gamma_0}$ and the Cauchy-Schwarz inequality applied to the autocovariance calculation. Since $\gamma_0$ is the variance, it is non-negative.  Moreover, the autocovariances are bounded by the variance, thus the autocorrelations are bounded by 1 in absolute value. $\blacksquare$

**Observa√ß√£o:** O Teorema 3 implica que as autocorrela√ß√µes de um processo MA($q$) s√£o n√£o nulas apenas para os $q$ primeiros lags e s√£o zero para lags maiores que $q$. Esta propriedade √© crucial para identificar a ordem $q$ do processo MA($q$) a partir do seu correlograma amostral.

> üí° **Exemplo Num√©rico:** Simulemos um correlograma para um processo MA(1) com $\theta_1 = 0.7$:
>
> ```python
> import numpy as np
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
>
> # Parameters
> mu = 0
> theta1 = 0.7
> sigma = 1
> num_samples = 200
>
> # Generate white noise
> errors = np.random.normal(mu, sigma, num_samples)
>
> # Generate MA(1) process
> Y = np.zeros(num_samples)
> Y[0] = mu + errors[0]
> for t in range(1, num_samples):
>     Y[t] = mu + errors[t] + theta1 * errors[t-1]
>
> # Calculate and plot the sample correlogram
> fig = sm.graphics.tsa.plot_acf(Y, lags=10)
> plt.title("Correlogram of MA(1) process")
> plt.xlabel("Lag")
> plt.ylabel("Autocorrelation")
> plt.show()
> ```
>
> Este c√≥digo gerar√° um gr√°fico mostrando a fun√ß√£o de autocorrela√ß√£o amostral (ACF). Voc√™ observar√° que a autocorrela√ß√£o no lag 1 ser√° significativa (pr√≥xima de 0.7), enquanto as autocorrela√ß√µes nos lags subsequentes estar√£o pr√≥ximas de zero. Isso confirma o corte no correlograma ap√≥s o lag $q$ = 1, caracter√≠stico de um processo MA(1).  A an√°lise do correlograma √© fundamental para identificar a ordem do processo MA.
>
> <div align="center">
>   <img src="https://i.imgur.com/your_image_url.png" alt="Correlograma MA(1)" width="500"/>
> </div>
>
> *(Substitua "https://i.imgur.com/your_image_url.png" pela URL real do gr√°fico gerado.)*

Al√©m da an√°lise das autocorrela√ß√µes, √© √∫til considerar a fun√ß√£o de autocorrela√ß√£o parcial (PACF). Embora a PACF de um processo MA($q$) n√£o tenha um corte abrupto como a ACF, ela pode fornecer informa√ß√µes adicionais sobre a estrutura do processo.

**Proposi√ß√£o 1:** (Comportamento Assint√≥tico da PACF) Para um processo MA($q$), a fun√ß√£o de autocorrela√ß√£o parcial (PACF) decai assintoticamente.

*Proof:* A PACF de um processo MA($q$) teoricamente decai para zero.  Para lags maiores que $q$, the PACF can be approximated using the inverse of the autocovariance matrix, which will show a decaying pattern. A prova formal envolve a an√°lise das equa√ß√µes de Yule-Walker para um processo AR infinito que se aproxima do MA($q$). $\blacksquare$

> üí° **Exemplo Num√©rico:**  Vamos gerar e plotar a PACF de um processo MA(2) simulado.
>
> ```python
> import numpy as np
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
>
> # Parameters
> mu = 0
> theta1 = 0.6
> theta2 = 0.4
> sigma = 1
> num_samples = 200
>
> # Generate white noise
> errors = np.random.normal(mu, sigma, num_samples)
>
> # Generate MA(2) process
> Y = np.zeros(num_samples)
> Y[0] = mu + errors[0]
> Y[1] = mu + errors[1] + theta1 * errors[0]
> for t in range(2, num_samples):
>     Y[t] = mu + errors[t] + theta1 * errors[t-1] + theta2 * errors[t-2]
>
> # Calculate and plot the sample PACF
> fig = sm.graphics.tsa.plot_pacf(Y, lags=10, method='ywm') # method='ywm' for Yule-Walker equations
> plt.title("Partial Autocorrelation Function (PACF) of MA(2) process")
> plt.xlabel("Lag")
> plt.ylabel("Partial Autocorrelation")
> plt.show()
> ```
>
> Ao executar este c√≥digo, voc√™ notar√° que a PACF n√£o corta abruptamente em lag 2. Em vez disso, ela exibe um padr√£o de decaimento. Embora os dois primeiros lags possam ser relativamente grandes, os lags subsequentes diminuir√£o gradualmente em dire√ß√£o a zero. Isso contrasta com a ACF, que corta ap√≥s o lag $q$. A PACF √©, portanto, uma ferramenta complementar na identifica√ß√£o de modelos MA.
>
> <div align="center">
>   <img src="https://i.imgur.com/your_pacf_image_url.png" alt="PACF MA(2)" width="500"/>
> </div>
>
> *(Substitua "https://i.imgur.com/your_pacf_image_url.png" pela URL real do gr√°fico gerado.)*

### Conclus√£o

O processo de m√©dia m√≥vel de ordem $q$ (MA($q$)) √© um modelo fundamental para a an√°lise de s√©ries temporais, caracterizado por sua depend√™ncia linear nas $q$ realiza√ß√µes passadas de um ru√≠do branco. A covariance-stationarity √© assegurada, pois sua m√©dia e autocovari√¢ncias n√£o s√£o fun√ß√µes do tempo. A vari√¢ncia √© uma soma ponderada das vari√¢ncias do ru√≠do branco, e as autocorrela√ß√µes s√£o n√£o-nulas apenas para os $q$ primeiros lags. Este estudo fornece uma base para a compreens√£o de modelos mais complexos, como os modelos ARMA.

### Refer√™ncias

[^47]: The basic building block for all the processes considered in this chapter is a sequence {8} - whose elements have mean zero and variance $\sigma^2$,  Œï(Œµt) = 0 [3.2.1].
[^48]: Œï(ŒµŒµ,) = 0 for t‚â† T. A process satisfying [3.2.1] through [3.2.3] is described as a white noise process.
[^49]: The jth autocorrelation of a covariance-stationary process (denoted p‚ÇÅ) is defined as its jth autocovariance divided by the variance: Pj = Œ•;/Œ≥Œø [3.3.6].
[^50]: A qth-order moving average process, denoted MA(q), is characterized by Y = Œº + Œµ‚ÇÅ + Œ∏ŒπŒµ,-1 + 0281-2 + ... + Œ∏Œ±ŒµŒπœÑŒ±Œπ [3.3.8]
[^51]: For j = 1, 2,..., 9, Œ•‚ÇÅ = Œï[(Œµ, + Œ∏ŒπŒµŒπ-1 + 0281-2 + + Œ∏Œ±ŒµŒπ-Œ±)√ó (ŒµŒπ-; + 0181-j-1 + 0281-j-2 + + @qr-j-q)] [3.3.11]
<!-- END -->