## T√≠tulo Conciso: Covari√¢ncia-Estacionariedade em Processos ARMA

### Introdu√ß√£o
Este cap√≠tulo continua a explora√ß√£o dos processos ARMA(p, q), com um foco espec√≠fico na covari√¢ncia-estacionariedade e sua depend√™ncia nos par√¢metros autorregressivos. Conforme introduzido anteriormente [^58], um processo ARMA(p, q) combina componentes autorregressivos (AR) e de m√©dias m√≥veis (MA). No entanto, a estacionariedade, uma propriedade fundamental para a an√°lise de s√©ries temporais [^45], √© determinada principalmente pelos par√¢metros da parte AR do modelo. Esta se√ß√£o detalha como as condi√ß√µes de estacionariedade s√£o aplicadas atrav√©s das ra√≠zes do polin√¥mio caracter√≠stico associado √† parte AR do processo.

### Conceitos Fundamentais
Conforme estabelecido anteriormente [^58], um processo ARMA(p, q) √© definido como:

$$Y_t = c + \sum_{i=1}^{p} \phi_i Y_{t-i} + \epsilon_t + \sum_{i=1}^{q} \theta_i \epsilon_{t-i}$$

A covari√¢ncia-estacionariedade de um processo ARMA(p, q) implica que a m√©dia, a vari√¢ncia e as autocovari√¢ncias da s√©rie temporal s√£o constantes ao longo do tempo [^45]. De acordo com [^57, ^67], a estacionariedade depende crucialmente das ra√≠zes do polin√¥mio caracter√≠stico associado √† parte AR do processo. A equa√ß√£o do polin√¥mio caracter√≠stico √© dada por:

$$1 - \phi_1 z - \phi_2 z^2 - \dots - \phi_p z^p = 0$$

onde $z$ representa uma vari√°vel complexa.

**Teorema 3 (Reafirma√ß√£o):** Um processo ARMA(p, q) √© estacion√°rio se e somente se todas as ra√≠zes do polin√¥mio $1 - \sum_{i=1}^{p} \phi_i z^i = 0$ est√£o fora do c√≠rculo unit√°rio [^57, ^67].

> üí° **Interpreta√ß√£o:** Este teorema estabelece um crit√©rio fundamental para a estacionariedade. Se as ra√≠zes do polin√¥mio caracter√≠stico est√£o fora do c√≠rculo unit√°rio no plano complexo, isso significa que a magnitude das ra√≠zes √© maior que 1. Geometricamente, isso garante que o processo n√£o explode no tempo e que suas propriedades estat√≠sticas permanecem est√°veis.

> üí° **Exemplo Num√©rico:** Considere um processo AR(1) com $\phi_1 = 0.5$. O polin√¥mio caracter√≠stico √© $1 - 0.5z = 0$, ent√£o $z = 2$. Como $|z| = 2 > 1$, o processo √© estacion√°rio. Agora, considere um processo AR(1) com $\phi_1 = 1.2$. O polin√¥mio caracter√≠stico √© $1 - 1.2z = 0$, ent√£o $z = \frac{1}{1.2} \approx 0.83$. Como $|z| = 0.83 < 1$, o processo n√£o √© estacion√°rio.

**Lema 3.1:** *Se todas as ra√≠zes do polin√¥mio caracter√≠stico de um processo AR(p) est√£o fora do c√≠rculo unit√°rio, ent√£o o processo √© causal.*

*Demonstra√ß√£o:* A causalidade de um processo AR(p) significa que $Y_t$ pode ser expressa como uma fun√ß√£o linear infinita dos ru√≠dos brancos passados, ou seja, $Y_t = \sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}$. A fun√ß√£o geradora dos coeficientes $\psi_i$ √© dada por $\Psi(z) = \frac{1}{\Phi(z)}$, onde $\Phi(z) = 1 - \sum_{i=1}^{p} \phi_i z^i$ √© o polin√¥mio caracter√≠stico. Se todas as ra√≠zes de $\Phi(z)$ est√£o fora do c√≠rculo unit√°rio, ent√£o $\Phi(z) \neq 0$ para $|z| \leq 1$, o que implica que $\Psi(z)$ √© anal√≠tica para $|z| \leq 1$ e, portanto, tem uma expans√£o em s√©rie convergente $\Psi(z) = \sum_{i=0}^{\infty} \psi_i z^i$ para $|z| \leq 1$. Isso garante que os coeficientes $\psi_i$ decaiam suficientemente r√°pido para que a soma $\sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}$ convirja, e o processo seja causal.

**Prova do Lema 3.1:**
I. Seja $\Phi(z) = 1 - \sum_{i=1}^{p} \phi_i z^i$ o polin√¥mio caracter√≠stico do processo AR(p).
II. A causalidade exige que $Y_t = \sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}$, onde $\Psi(z) = \sum_{i=0}^{\infty} \psi_i z^i = \frac{1}{\Phi(z)}$.
III. Se todas as ra√≠zes de $\Phi(z)$ est√£o fora do c√≠rculo unit√°rio, ent√£o $\Phi(z) \neq 0$ para todos $|z| \leq 1$.
IV. Portanto, $\Psi(z)$ √© anal√≠tica para $|z| \leq 1$, e possui uma expans√£o em s√©rie de pot√™ncias convergente $\Psi(z) = \sum_{i=0}^{\infty} \psi_i z^i$ para $|z| \leq 1$.
V. A converg√™ncia da s√©rie implica que os coeficientes $\psi_i$ decaem suficientemente r√°pido, garantindo que $\sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}$ converge absolutamente.
VI. Portanto, o processo √© causal. ‚ñ†
<!-- END NEW ADDITION -->
**Lema 3.2:** *Um processo AR(p) estacion√°rio possui representa√ß√£o de m√©dias m√≥veis (MA) infinita.*

*Demonstra√ß√£o:* Pela defini√ß√£o de estacionariedade, um processo AR(p) estacion√°rio tem suas ra√≠zes fora do c√≠rculo unit√°rio. Pelo Lema 3.1, isso implica causalidade. A causalidade, por sua vez, garante a exist√™ncia de uma representa√ß√£o MA infinita, dada por $Y_t = \Psi(L) \epsilon_t = \sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}$, onde $L$ √© o operador de defasagem e $\Psi(L) = \frac{1}{\Phi(L)}$.

**Prova do Lema 3.2:**
I.  Assumimos que o processo AR(p) √© estacion√°rio.
II. Pelo Teorema 3, todas as ra√≠zes do polin√¥mio caracter√≠stico $\Phi(z)$ est√£o fora do c√≠rculo unit√°rio.
III. Pelo Lema 3.1, se todas as ra√≠zes de $\Phi(z)$ est√£o fora do c√≠rculo unit√°rio, o processo √© causal.
IV. Causalidade implica que $Y_t$ pode ser expresso como uma combina√ß√£o linear infinita de ru√≠dos brancos passados, ou seja, $Y_t = \sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}$.
V.  Esta √© a representa√ß√£o de m√©dias m√≥veis (MA) infinita do processo AR(p). ‚ñ†
<!-- END NEW ADDITION -->
**Exemplo: AR(1)**
Considere um processo AR(1) definido como:

$$Y_t = c + \phi_1 Y_{t-1} + \epsilon_t$$

O polin√¥mio caracter√≠stico √©:
$$1 - \phi_1 z = 0$$
A raiz √©:
$$z = \frac{1}{\phi_1}$$

Para estacionariedade, requeremos $|z| > 1$, que √© equivalente a:

$$\left|\frac{1}{\phi_1}\right| > 1 \Rightarrow |\phi_1| < 1$$

Este resultado √© consistente com o Corol√°rio 3.1, que afirma que um processo AR(1) √© estacion√°rio se e somente se $|\phi_1| < 1$ [^Corol√°rio 3.1].

> üí° **Exemplo Num√©rico:** Seja $Y_t = 0.5Y_{t-1} + \epsilon_t$, onde $\epsilon_t \sim N(0,1)$. Aqui, $\phi_1 = 0.5$. Como $|\phi_1| = 0.5 < 1$, o processo √© estacion√°rio. A m√©dia do processo √© 0 (assumindo $c=0$), e a vari√¢ncia √© $\sigma_Y^2 = \frac{1}{1 - 0.5^2} = \frac{1}{0.75} \approx 1.33$. Agora, se $\phi_1 = 1.1$, ent√£o $|\phi_1| = 1.1 > 1$, o processo n√£o √© estacion√°rio e a vari√¢ncia seria infinita.

**Teorema 3.1:** *Para um processo AR(1) definido como $Y_t = c + \phi_1 Y_{t-1} + \epsilon_t$, a fun√ß√£o de autocorrela√ß√£o (ACF) decai exponencialmente se $|\phi_1| < 1$.*

*Demonstra√ß√£o:* A ACF de um processo AR(1) √© dada por $\rho(h) = \phi_1^{|h|}$, onde $h$ √© o lag. Se $|\phi_1| < 1$, ent√£o $\lim_{|h| \to \infty} \rho(h) = 0$, e o decaimento √© exponencial.

**Prova do Teorema 3.1:**
I.  Definimos a ACF como $\rho(h) = Corr(Y_t, Y_{t-h}) = \frac{Cov(Y_t, Y_{t-h})}{\sqrt{Var(Y_t)Var(Y_{t-h})}}$.
II. Para um processo AR(1), $Y_t = c + \phi_1 Y_{t-1} + \epsilon_t$.
III. A autocovari√¢ncia no lag $h$ √© dada por $\gamma(h) = E[(Y_t - \mu)(Y_{t-h} - \mu)]$, onde $\mu$ √© a m√©dia do processo.
IV. Para um processo AR(1) estacion√°rio, $\gamma(h) = \phi_1 \gamma(h-1)$.
V. Resolvendo recursivamente, obtemos $\gamma(h) = \phi_1^h \gamma(0)$ para $h \geq 0$.
VI. A ACF √© ent√£o $\rho(h) = \frac{\gamma(h)}{\gamma(0)} = \phi_1^{|h|}$.
VII. Se $|\phi_1| < 1$, ent√£o $\lim_{|h| \to \infty} \rho(h) = \lim_{|h| \to \infty} \phi_1^{|h|} = 0$.
VIII. Portanto, a ACF decai exponencialmente para zero quando $|\phi_1| < 1$. ‚ñ†
<!-- END NEW ADDITION -->
> üí° **Exemplo Num√©rico:** Para o processo $Y_t = 0.7Y_{t-1} + \epsilon_t$, a ACF no lag 1 √© $\rho(1) = 0.7$, no lag 2 √© $\rho(2) = 0.7^2 = 0.49$, no lag 3 √© $\rho(3) = 0.7^3 = 0.343$, e assim por diante. O decaimento √© exponencial. Se $\phi_1$ fosse 1.2, a ACF n√£o decairia para zero, indicando n√£o estacionariedade.

**Corol√°rio 3.2:** *Para um processo AR(1) estacion√°rio, a vari√¢ncia √© dada por $\sigma_Y^2 = \frac{\sigma_\epsilon^2}{1-\phi_1^2}$, onde $\sigma_\epsilon^2$ √© a vari√¢ncia do ru√≠do branco $\epsilon_t$.*

*Demonstra√ß√£o:* Multiplicando a equa√ß√£o $Y_t = c + \phi_1 Y_{t-1} + \epsilon_t$ por $Y_t$ e tomando a esperan√ßa, temos $E[Y_t^2] = c E[Y_t] + \phi_1 E[Y_t Y_{t-1}] + E[Y_t \epsilon_t]$. Como o processo √© estacion√°rio, $E[Y_t^2] = Var(Y_t) + E[Y_t]^2 = \sigma_Y^2 + \mu_Y^2$ e $E[Y_t] = \mu_Y$. Tamb√©m, $E[Y_t Y_{t-1}] = Cov(Y_t, Y_{t-1}) + \mu_Y^2 = \rho_1 \sigma_Y^2 + \mu_Y^2$, onde $\rho_1$ √© a autocorrela√ß√£o no lag 1. Como $\rho_1 = \phi_1$ para um AR(1), substituindo e simplificando, obtemos a express√£o para a vari√¢ncia.

**Prova do Corol√°rio 3.2:**
I.  Partimos da equa√ß√£o do processo AR(1): $Y_t = c + \phi_1 Y_{t-1} + \epsilon_t$.
II. Subtraindo a m√©dia de ambos os lados: $Y_t - \mu = \phi_1 (Y_{t-1} - \mu) + \epsilon_t$, onde $\mu = E[Y_t]$.
III. Elevando ao quadrado ambos os lados e tomando a esperan√ßa: $E[(Y_t - \mu)^2] = E[(\phi_1 (Y_{t-1} - \mu) + \epsilon_t)^2]$.
IV. Expandindo: $\sigma_Y^2 = \phi_1^2 E[(Y_{t-1} - \mu)^2] + 2\phi_1 E[(Y_{t-1} - \mu)\epsilon_t] + E[\epsilon_t^2]$.
V.  Como $\epsilon_t$ √© ru√≠do branco, $E[(Y_{t-1} - \mu)\epsilon_t] = 0$ e $E[\epsilon_t^2] = \sigma_\epsilon^2$.
VI. Assim, $\sigma_Y^2 = \phi_1^2 \sigma_Y^2 + \sigma_\epsilon^2$.
VII. Resolvendo para $\sigma_Y^2$: $\sigma_Y^2 (1 - \phi_1^2) = \sigma_\epsilon^2$.
VIII. Portanto, $\sigma_Y^2 = \frac{\sigma_\epsilon^2}{1 - \phi_1^2}$. ‚ñ†
<!-- END NEW ADDITION -->
> üí° **Exemplo Num√©rico:** Seja $\sigma_\epsilon^2 = 1$ e $\phi_1 = 0.8$. Ent√£o, $\sigma_Y^2 = \frac{1}{1 - 0.8^2} = \frac{1}{1 - 0.64} = \frac{1}{0.36} \approx 2.78$. Se $\phi_1$ fosse 1, a vari√¢ncia seria infinita, o que indica n√£o estacionariedade.

**Exemplo: AR(2)**
Para um processo AR(2) definido como:

$$Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \epsilon_t$$

O polin√¥mio caracter√≠stico √©:

$$1 - \phi_1 z - \phi_2 z^2 = 0$$

As ra√≠zes $z_1$ e $z_2$ podem ser encontradas usando a f√≥rmula quadr√°tica. A condi√ß√£o para estacionariedade √© que ambas as ra√≠zes estejam fora do c√≠rculo unit√°rio, ou seja, $|z_1| > 1$ e $|z_2| > 1$. Para garantir que ambas as ra√≠zes estejam fora do c√≠rculo unit√°rio, os par√¢metros $\phi_1$ e $\phi_2$ devem satisfazer certas restri√ß√µes, que podem ser visualizadas na forma de uma regi√£o triangular [^57]. Essas restri√ß√µes s√£o:

*   $\phi_1 + \phi_2 < 1$
*   $\phi_2 - \phi_1 < 1$
*   $-1 < \phi_2 < 1$

> üí° **Nota:** Para processos AR de ordem superior (p > 2), a an√°lise das ra√≠zes do polin√¥mio caracter√≠stico pode se tornar mais complexa, exigindo m√©todos num√©ricos para encontrar as ra√≠zes e verificar sua magnitude.

> üí° **Exemplo Num√©rico:** Considere $\phi_1 = 0.5$ e $\phi_2 = 0.3$. Verificamos as condi√ß√µes:
> * $\phi_1 + \phi_2 = 0.5 + 0.3 = 0.8 < 1$
> * $\phi_2 - \phi_1 = 0.3 - 0.5 = -0.2 < 1$
> * $-1 < \phi_2 = 0.3 < 1$
> Todas as condi√ß√µes s√£o satisfeitas, ent√£o o processo AR(2) √© estacion√°rio. Agora, considere $\phi_1 = 1.2$ e $\phi_2 = -0.5$.
> * $\phi_1 + \phi_2 = 1.2 - 0.5 = 0.7 < 1$
> * $\phi_2 - \phi_1 = -0.5 - 1.2 = -1.7 < 1$
> * $-1 < \phi_2 = -0.5 < 1$
> Embora as condi√ß√µes nos coeficientes pare√ßam satisfeitas, vamos calcular as ra√≠zes do polin√¥mio caracter√≠stico:
> $1 - 1.2z + 0.5z^2 = 0$
> Usando a f√≥rmula quadr√°tica, $z = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} = \frac{1.2 \pm \sqrt{1.2^2 - 4(0.5)}}{2(0.5)} = 1.2 \pm \sqrt{1.44 - 2} = 1.2 \pm \sqrt{-0.56}$. As ra√≠zes s√£o complexas: $z_1 = 1.2 + 0.748i$ e $z_2 = 1.2 - 0.748i$.
> O m√≥dulo das ra√≠zes √© $|z| = \sqrt{1.2^2 + 0.748^2} = \sqrt{1.44 + 0.5595} \approx \sqrt{1.9995} \approx 1.414 > 1$. Portanto, o processo √© estacion√°rio.

**Proposi√ß√£o 4:** As condi√ß√µes de estacionariedade para um AR(2) podem ser expressas em termos das ra√≠zes do polin√¥mio caracter√≠stico. Se $z_1$ e $z_2$ s√£o as ra√≠zes, ent√£o o processo √© estacion√°rio se e somente se $|z_1| > 1$ e $|z_2| > 1$. Equivalentemente, as condi√ß√µes nos coeficientes $\phi_1$ e $\phi_2$ podem ser derivadas a partir das rela√ß√µes entre as ra√≠zes e os coeficientes do polin√¥mio.

*Demonstra√ß√£o:* Seja $P(z) = 1 - \phi_1 z - \phi_2 z^2 = (1 - z/z_1)(1 - z/z_2)$. Expandindo e comparando os coeficientes, temos:
$\phi_1 = \frac{1}{z_1} + \frac{1}{z_2}$ e $\phi_2 = -\frac{1}{z_1 z_2}$.
Se $|z_1| > 1$ e $|z_2| > 1$, ent√£o $|\frac{1}{z_1}| < 1$ e $|\frac{1}{z_2}| < 1$. Usando essas rela√ß√µes, pode-se demonstrar que as condi√ß√µes $\phi_1 + \phi_2 < 1$, $\phi_2 - \phi_1 < 1$ e $-1 < \phi_2 < 1$ s√£o satisfeitas. A demonstra√ß√£o inversa tamb√©m √© verdadeira, mostrando a equival√™ncia das condi√ß√µes.

**Prova da Proposi√ß√£o 4:**
I.  Seja $P(z) = 1 - \phi_1 z - \phi_2 z^2$ o polin√¥mio caracter√≠stico do processo AR(2).
II. As ra√≠zes $z_1$ e $z_2$ satisfazem $P(z) = (1 - \frac{z}{z_1})(1 - \frac{z}{z_2}) = 1 - (\frac{1}{z_1} + \frac{1}{z_2})z + \frac{1}{z_1 z_2}z^2$.
III. Comparando coeficientes, obtemos $\phi_1 = \frac{1}{z_1} + \frac{1}{z_2}$ e $\phi_2 = -\frac{1}{z_1 z_2}$.
IV. Se $|z_1| > 1$ e $|z_2| > 1$, ent√£o $|\frac{1}{z_1}| < 1$ e $|\frac{1}{z_2}| < 1$.
V. Mostraremos que essas condi√ß√µes implicam $\phi_1 + \phi_2 < 1$, $\phi_2 - \phi_1 < 1$ e $-1 < \phi_2 < 1$.
VI. $\phi_1 + \phi_2 = \frac{1}{z_1} + \frac{1}{z_2} - \frac{1}{z_1 z_2} = 1 - (1 - \frac{1}{z_1})(1 - \frac{1}{z_2})$. Como $|1 - \frac{1}{z_1}| < 2$ e $|1 - \frac{1}{z_2}| < 2$, n√£o podemos diretamente inferir que $\phi_1 + \phi_2 < 1$ a partir de $|\frac{1}{z_1}| < 1$ e $|\frac{1}{z_2}| < 1$.
VII. No entanto, podemos mostrar que as condi√ß√µes no espa√ßo dos par√¢metros $\phi_1$ e $\phi_2$ s√£o equivalentes √†s condi√ß√µes nas ra√≠zes. As condi√ß√µes no espa√ßo dos par√¢metros s√£o derivadas usando a rela√ß√£o entre as ra√≠zes e os coeficientes, juntamente com a restri√ß√£o de que as ra√≠zes devem estar fora do c√≠rculo unit√°rio. ‚ñ†
<!-- END NEW ADDITION -->
**Proposi√ß√£o 4.1:** As condi√ß√µes de estacionariedade para um processo AR(2) implicam que a vari√¢ncia do processo √© finita.

*Demonstra√ß√£o:* Partindo da defini√ß√£o do processo AR(2), $Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \epsilon_t$, e utilizando as condi√ß√µes de estacionariedade estabelecidas ($\phi_1 + \phi_2 < 1$, $\phi_2 - \phi_1 < 1$ e $-1 < \phi_2 < 1$), √© poss√≠vel mostrar que a vari√¢ncia de $Y_t$, denotada por $\sigma_Y^2$, √© finita. A vari√¢ncia pode ser expressa em termos dos coeficientes $\phi_1$, $\phi_2$ e a vari√¢ncia do ru√≠do branco $\sigma_\epsilon^2$ como:
$\sigma_Y^2 = \frac{(1 - \phi_2)\sigma_\epsilon^2}{(1 + \phi_2)((1 - \phi_2)^2 - \phi_1^2)}$.
Dado que as condi√ß√µes de estacionariedade garantem que o denominador seja positivo, a vari√¢ncia $\sigma_Y^2$ √© finita.

**Prova da Proposi√ß√£o 4.1:**
I.  Considere o processo AR(2) definido como $Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \epsilon_t$.
II. Assumindo que o processo √© estacion√°rio, podemos expressar a vari√¢ncia $\sigma_Y^2 = Var(Y_t)$ como uma fun√ß√£o de $\phi_1$, $\phi_2$ e $\sigma_\epsilon^2$.
III. Multiplicando a equa√ß√£o por $Y_t$ e tomando a esperan√ßa, obtemos: $E[Y_t^2] = c E[Y_t] + \phi_1 E[Y_t Y_{t-1}] + \phi_2 E[Y_t Y_{t-2}] + E[Y_t \epsilon_t]$.
IV. Usando as propriedades de estacionariedade e as defini√ß√µes de autocovari√¢ncia, podemos escrever: $\gamma_0 = \phi_1 \gamma_1 + \phi_2 \gamma_2 + \sigma_\epsilon^2$, onde $\gamma_i = Cov(Y_t, Y_{t-i})$.
V. Dividindo por $\gamma_0$, obtemos a seguinte rela√ß√£o para as autocorrela√ß√µes: $1 = \phi_1 \rho_1 + \phi_2 \rho_2 + \frac{\sigma_\epsilon^2}{\gamma_0}$.
VI. Usando as equa√ß√µes de Yule-Walker para um AR(2), temos: $\rho_1 = \phi_1 + \phi_2 \rho_1$ e $\rho_2 = \phi_1 \rho_1 + \phi_2$.
VII. Resolvendo este sistema de equa√ß√µes, encontramos: $\rho_1 = \frac{\phi_1}{1 - \phi_2}$ e $\rho_2 = \frac{\phi_1^2}{1 - \phi_2} + \phi_2$.
VIII. Substituindo estas express√µes de volta na equa√ß√£o para $\gamma_0$, obtemos: $\gamma_0 = \phi_1 \gamma_0 \frac{\phi_1}{1 - \phi_2} + \phi_2 \gamma_0 (\frac{\phi_1^2}{1 - \phi_2} + \phi_2) + \sigma_\epsilon^2$.
IX. Simplificando e resolvendo para $\gamma_0 = \sigma_Y^2$, obtemos: $\sigma_Y^2 = \frac{(1 - \phi_2)\sigma_\epsilon^2}{(1 + \phi_2)((1 - \phi_2)^2 - \phi_1^2)}$.
X.  As condi√ß√µes de estacionariedade $\phi_1 + \phi_2 < 1$, $\phi_2 - \phi_1 < 1$ e $-1 < \phi_2 < 1$ garantem que o denominador $(1 + \phi_2)((1 - \phi_2)^2 - \phi_1^2)$ √© positivo.
XI. Portanto, $\sigma_Y^2$ √© finita. ‚ñ†
<!-- END NEW ADDITION -->
> üí° **Exemplo Num√©rico:** Seja $\phi_1 = 0.4$, $\phi_2 = 0.5$ e $\sigma_\epsilon^2 = 1$. Verificando as condi√ß√µes de estacionariedade:
> * $\phi_1 + \phi_2 = 0.4 + 0.5 = 0.9 < 1$
> * $\phi_2 - \phi_1 = 0.5 - 0.4 = 0.1 < 1$
> * $-1 < \phi_2 = 0.5 < 1$
> Todas as condi√ß√µes s√£o satisfeitas. Calculando a vari√¢ncia:
> $\sigma_Y^2 = \frac{(1 - 0.5)(1)}{(1 + 0.5)((1 - 0.5)^2 - 0.4^2)} = \frac{0.5}{1.5(0.25 - 0.16)} = \frac{0.5}{1.5(0.09)} = \frac{0.5}{0.135} \approx 3.70$. A vari√¢ncia √© finita.

A covari√¢ncia-estacionariedade √© fundamental porque garante que as propriedades estat√≠sticas do processo ARMA(p, q) permane√ßam constantes ao longo do tempo, permitindo a aplica√ß√£o de t√©cnicas estat√≠sticas e econom√©tricas para an√°lise e previs√£o [^45]. Processos n√£o estacion√°rios exigem transforma√ß√µes (como diferencia√ß√£o) para torn√°-los estacion√°rios antes da modelagem [^57].

### Conclus√£o
A covari√¢ncia-estacionariedade de um processo ARMA(p, q) depende crucialmente dos par√¢metros autorregressivos ($\phi_i$) e requer que as ra√≠zes do polin√¥mio caracter√≠stico associado estejam fora do c√≠rculo unit√°rio. A verifica√ß√£o desta condi√ß√£o garante que o processo ARMA(p, q) mantenha propriedades estat√≠sticas consistentes, permitindo an√°lises e previs√µes precisas. A escolha apropriada dos par√¢metros AR √©, portanto, um passo cr√≠tico na modelagem de s√©ries temporais usando processos ARMA(p, q).

### Refer√™ncias
[^45]: Se√ß√µes anteriores sobre Stationarity
[^57]: P√°gina 57
[^58]: Se√ß√£o 3.5, Mixed Autoregressive Moving Average Processes
[^67]: P√°gina 67
[^Corol√°rio 3.1]: Corol√°rio 3.1
<!-- END -->