## T√≠tulo Conciso: Evitando a Redund√¢ncia na Parametriza√ß√£o de Processos ARMA

### Introdu√ß√£o
Este cap√≠tulo aborda um desafio cr√≠tico na modelagem de s√©ries temporais com processos ARMA(p, q): a potencial redund√¢ncia na parametriza√ß√£o. Como vimos anteriormente [^58], processos ARMA(p, q) combinam componentes autorregressivos (AR) e de m√©dias m√≥veis (MA), oferecendo flexibilidade para capturar din√¢micas complexas. No entanto, essa flexibilidade pode levar a modelos com par√¢metros desnecess√°rios, dificultando a interpreta√ß√£o e a precis√£o das previs√µes. Esta se√ß√£o explora as causas da redund√¢ncia de par√¢metros e apresenta t√©cnicas para evit√°-la, garantindo a identifica√ß√£o de modelos ARMA(p, q) parcimoniosos e eficientes.

### Conceitos Fundamentais
Conforme definido anteriormente [^58], um processo ARMA(p, q) √© dado por:

$$Y_t = c + \sum_{i=1}^{p} \phi_i Y_{t-i} + \epsilon_t + \sum_{i=1}^{q} \theta_i \epsilon_{t-i}$$

A redund√¢ncia na parametriza√ß√£o ocorre quando diferentes conjuntos de par√¢metros ($\phi_i$, $\theta_i$) resultam em modelos que produzem essencialmente as mesmas previs√µes e se ajustam aos dados de maneira similar. Essa redund√¢ncia surge principalmente devido a cancelamentos de fatores comuns entre os polin√¥mios AR e MA do modelo [^60].

> üí° **Exemplo Num√©rico:** Considere o exemplo apresentado em [^60], onde um processo de ru√≠do branco simples ($Y_t = \epsilon_t$) √© representado como um ARMA(1,1):

$$(1 - \rho L)Y_t = (1 - \rho L)\epsilon_t$$
$$Y_t = \rho Y_{t-1} + \epsilon_t - \rho \epsilon_{t-1}$$

> Neste caso, os par√¢metros $\phi_1 = \rho$ e $\theta_1 = -\rho$ se cancelam, resultando em um modelo desnecessariamente complexo para um processo de ru√≠do branco. Qualquer valor de $\rho$ descreveria igualmente bem a s√©rie temporal. Tentar estimar $\rho$ por m√°xima verossimilhan√ßa levaria a problemas num√©ricos e dificuldade em obter um estimador preciso.

**Identificabilidade:** A identificabilidade de um modelo ARMA(p, q) refere-se √† capacidade de determinar unicamente os valores dos par√¢metros do modelo a partir das suas propriedades estat√≠sticas (por exemplo, fun√ß√£o de autocorrela√ß√£o). A redund√¢ncia de par√¢metros viola a condi√ß√£o de identificabilidade, pois diferentes conjuntos de par√¢metros podem gerar as mesmas propriedades estat√≠sticas [^60].

**Fatora√ß√£o dos Polin√¥mios:** A chave para detectar e evitar a redund√¢ncia na parametriza√ß√£o √© examinar os polin√¥mios AR e MA para fatores comuns [^60, ^68]. Os polin√¥mios AR e MA s√£o definidos como:

*   Polin√¥mio AR: $\Phi(z) = 1 - \phi_1 z - \phi_2 z^2 - \dots - \phi_p z^p$
*   Polin√¥mio MA: $\Theta(z) = 1 + \theta_1 z + \theta_2 z^2 - \dots - \theta_q z^q$

Se $\Phi(z)$ e $\Theta(z)$ tiverem um fator comum, o modelo ARMA(p, q) pode ser simplificado cancelando esse fator, resultando em um modelo com menos par√¢metros [^60, ^68].

> üí° **Exemplo Num√©rico:** Considere um processo ARMA(2,1) com polin√¥mios:
>
> $$\Phi(z) = 1 - 0.5z + 0.06z^2 = (1-0.2z)(1-0.3z)$$
> $$\Theta(z) = 1 - 0.2z$$
>
> Ambos os polin√¥mios t√™m um fator comum $(1 - 0.2z)$. Se cancelarmos este fator, obtemos um modelo equivalente com:
>
> $$\Phi(z) = 1 - 0.3z$$
> $$\Theta(z) = 1$$
>
> Isso simplifica o modelo ARMA(2,1) original para um AR(1):
>
> $$Y_t = 0.3 Y_{t-1} + \epsilon_t$$
>
> Aqui, o processo ARMA(2,1) original √© redondante e pode ser simplificado para um modelo mais parcimonioso. Estimando o ARMA(2,1) diretamente pode levar a estimativas imprecisas dos par√¢metros e dificuldade em interpretar os resultados.
> ```python
> import numpy as np
> import statsmodels.api as sm
>
> # Simula um processo AR(1)
> np.random.seed(0)
> n = 200
> phi = 0.3
> errors = np.random.randn(n)
> y = np.zeros(n)
> for t in range(1, n):
>     y[t] = phi * y[t-1] + errors[t]
>
> # Estima um ARMA(2,1)
> model = sm.tsa.ARMA(y, order=(2, 1))
> results = model.fit(method='mle')
> print(results.summary())
>
> # Estima um AR(1)
> model_ar1 = sm.tsa.ARMA(y, order=(1, 0))
> results_ar1 = model_ar1.fit(method='mle')
> print(results_ar1.summary())
>
> # Exemplo de sa√≠da (aproximada):
> # ARMA(2,1) - Observar p-valores altos para o AR.L2 e MA.L1, indicando redund√¢ncia
> # AR.L1       0.2998     0.070               4.288      0.000      0.163       0.437
> # AR.L2       0.0100     0.071               0.141      0.888      -0.129       0.149
> # MA.L1       0.0052     0.099               0.052      0.959      -0.190       0.201
>
> # AR(1) - Modelo mais parcimonioso com par√¢metros significativos
> # AR.L1       0.3123     0.069               4.503      0.000      0.176       0.449
> ```
> A sa√≠da do c√≥digo demonstrar√° que o modelo ARMA(2,1) tem par√¢metros n√£o significativos, enquanto o AR(1) tem um √∫nico par√¢metro significativo, capturando a din√¢mica essencial dos dados simulados. Isso ilustra como a redund√¢ncia pode levar √† inclus√£o de par√¢metros desnecess√°rios.

**T√©cnicas para Evitar a Redund√¢ncia:**

1.  **Verifica√ß√£o da Presen√ßa de Ra√≠zes Pr√≥ximas:** Se um processo ARMA(p,q) tiver uma raiz do polin√¥mio AR que √© muito pr√≥xima (em valor absoluto) de uma raiz do polin√¥mio MA, os efeitos desses termos se cancelam aproximadamente. A verifica√ß√£o dessa condi√ß√£o √© uma forma de identificar redund√¢ncia.

    > üí° **Exemplo Num√©rico:** Considere um modelo ARMA(1,1) onde $\phi_1 = 0.9$ e $\theta_1 = -0.89$. As ra√≠zes dos polin√¥mios AR e MA s√£o $1/0.9 \approx 1.11$ e $-1/-0.89 \approx 1.12$, respectivamente. Essas ra√≠zes s√£o muito pr√≥ximas, indicando que os termos AR e MA est√£o quase se cancelando. Este modelo √© praticamente equivalente a um ru√≠do branco com uma pequena perturba√ß√£o.
    >
    > Para quantificar o impacto, podemos comparar a vari√¢ncia do processo ARMA(1,1) com a vari√¢ncia de um ru√≠do branco. A vari√¢ncia do ARMA(1,1) √© dada por:
    >
    > $$Var(Y_t) = \sigma^2 \frac{1 + 2\phi_1\theta_1 + \theta_1^2}{1 - \phi_1^2}$$
    >
    > Assumindo $\sigma^2 = 1$, temos:
    >
    > $$Var(Y_t) = \frac{1 + 2(0.9)(-0.89) + (-0.89)^2}{1 - (0.9)^2} \approx \frac{1 - 1.602 + 0.7921}{1 - 0.81} \approx \frac{0.1901}{0.19} \approx 1.00$$
    >
    > A vari√¢ncia do processo ARMA(1,1) √© muito pr√≥xima de 1, que √© a vari√¢ncia do ru√≠do branco. Isso confirma que o modelo ARMA(1,1) √© quase indistingu√≠vel de um ru√≠do branco devido ao cancelamento dos termos AR e MA.

2.  **Restri√ß√£o de Par√¢metros:** Uma t√©cnica comum para evitar a redund√¢ncia √© impor restri√ß√µes aos par√¢metros [^60]. Por exemplo, podemos restringir os coeficientes AR e MA a um certo intervalo ou impor rela√ß√µes entre eles. No caso do exemplo do ru√≠do branco modelado como um ARMA(1,1), poder√≠amos impor a restri√ß√£o $\phi_1 = -\theta_1$.

    > üí° **Exemplo Num√©rico:**  Se suspectarmos que um processo ARMA(1,1) √©, na verdade, um ru√≠do branco, podemos estimar o modelo impondo a restri√ß√£o $\phi_1 = -\theta_1$.  Isso reduz o n√∫mero de par√¢metros a serem estimados e evita a redund√¢ncia.
    >
    > Podemos implementar isso usando a biblioteca `statsmodels` em Python, definindo uma fun√ß√£o de verossimilhan√ßa customizada que incorpora a restri√ß√£o:
    > ```python
    > import numpy as np
    > import statsmodels.api as sm
    > from scipy.optimize import minimize
    >
    > # Simula um processo de ru√≠do branco
    > np.random.seed(0)
    > n = 100
    > errors = np.random.randn(n)
    >
    > # Define a fun√ß√£o de verossimilhan√ßa com a restri√ß√£o phi = -theta
    > def constrained_likelihood(params, data):
    >     phi = params[0]
    >     theta = -phi  # Restri√ß√£o
    >     sigma2 = params[1]
    >     
    >     # Calcula o res√≠duo (aprox.)
    >     resid = data - phi * np.roll(data, 1) + theta * np.roll(errors, 1)
    >     resid[0] = data[0] # Approximation for the first residual
    >     
    >     log_likelihood = -0.5 * n * np.log(2 * np.pi * \sigma2) - 0.5 * np.sum(resid**2) / sigma2
    >     return -log_likelihood # Negativo para minimiza√ß√£o
    >
    > # Estima o modelo ARMA(1,1) com a restri√ß√£o
    > initial_guess = [0.1, 1] # phi, sigma2
    > results = minimize(constrained_likelihood, initial_guess, args=(errors,), method='L-BFGS-B', bounds=[(-1, 1), (0.01, 10)])
    >
    > estimated_phi = results.x[0]
    > estimated_theta = -estimated_phi
    > estimated_sigma2 = results.x[1]
    >
    > print(f"Estimated phi: {estimated_phi}")
    > print(f"Estimated theta: {estimated_theta}")
    > print(f"Estimated sigma2: {estimated_sigma2}")
    > ```
    > Este c√≥digo demonstra como impor uma restri√ß√£o nos par√¢metros durante a estima√ß√£o, for√ßando o modelo a se ajustar √† condi√ß√£o esperada para um ru√≠do branco.

3.  **Crit√©rios de Sele√ß√£o de Modelo:** Utilizar crit√©rios de sele√ß√£o de modelo, como o Crit√©rio de Informa√ß√£o de Akaike (AIC) ou o Crit√©rio de Informa√ß√£o Bayesiano (BIC), que penalizam modelos com mais par√¢metros [^72]. Esses crit√©rios ajudam a selecionar o modelo ARMA(p, q) mais parcimonioso que melhor se ajusta aos dados.

    > üí° **Exemplo Num√©rico:**  Considere que estamos comparando um modelo ARMA(1,1) e um modelo AR(1) para uma determinada s√©rie temporal.  Ap√≥s estimar ambos os modelos, obtemos os seguintes valores de AIC e BIC:
    >
    > | Modelo    | AIC   | BIC   |
    > | --------- | ----- | ----- |
    > | ARMA(1,1) | 250.5 | 258.2 |
    > | AR(1)     | 248.0 | 253.7 |
    >
    > O modelo AR(1) tem valores de AIC e BIC menores do que o modelo ARMA(1,1). Isso sugere que o modelo AR(1) oferece um melhor trade-off entre o ajuste aos dados e a complexidade do modelo, e deve ser preferido ao ARMA(1,1).
    >
    > A diferen√ßa no BIC √© especialmente importante, pois o BIC penaliza modelos mais complexos de forma mais rigorosa do que o AIC. Neste caso, a diferen√ßa no BIC (258.2 - 253.7 = 4.5) fornece evid√™ncias mais fortes de que o modelo AR(1) √© prefer√≠vel.

4.  **An√°lise da Fun√ß√£o de Autocorrela√ß√£o Parcial (PACF):** Para um processo AR(p), a PACF se torna zero ap√≥s o lag *p*. Se a PACF de um processo ARMA(p,q) exibir um corte abrupto em um lag menor que *p*, isso pode indicar que alguns dos par√¢metros AR s√£o redundantes.

5.  **An√°lise da Fun√ß√£o de Autocorrela√ß√£o (ACF):** Para um processo MA(q), a ACF se torna zero ap√≥s o lag *q*. Se a ACF de um processo ARMA(p,q) exibir um corte abrupto em um lag menor que *q*, isso pode indicar que alguns dos par√¢metros MA s√£o redundantes.

    > üí° **Exemplo Num√©rico:** Suponha que analisamos a ACF e PACF de uma s√©rie temporal e observamos que a ACF decai exponencialmente e a PACF tem um corte abrupto no lag 1. Isso sugere que um modelo AR(1) √© mais apropriado do que um modelo ARMA(p,q) com p > 1 ou q > 0, pois os termos adicionais seriam redundantes. A decad√™ncia exponencial na ACF indica a presen√ßa de depend√™ncia serial de longo prazo, que √© bem capturada por um processo AR(1). O corte na PACF no lag 1 confirma que apenas a primeira defasagem √© importante na modelagem da autocorrela√ß√£o da s√©rie temporal.

6.  **Testes de Signific√¢ncia Estat√≠stica:** Realizar testes de hip√≥teses para avaliar a signific√¢ncia estat√≠stica de cada par√¢metro do modelo [^71]. Par√¢metros n√£o significativos podem ser removidos do modelo sem comprometer significativamente o ajuste aos dados.

    > üí° **Exemplo Num√©rico:**  Suponha que estimemos um ARMA(2,2) e o teste *t* para $\phi_2$ e $\theta_2$ resulta em valores p altos (acima de 0.05), indicando que eles n√£o s√£o estatisticamente diferentes de zero. Isso sugere que um modelo ARMA(1,1) ou um modelo de ordem inferior pode ser mais apropriado.

    > üí° **Exemplo Num√©rico Detalhado:** Suponha que estamos modelando o crescimento trimestral do PIB de um pa√≠s com um ARMA(2,2). Ap√≥s a estima√ß√£o, obtemos os seguintes resultados (hipot√©ticos):
    >
    > | Par√¢metro | Estimativa | Erro Padr√£o | Estat√≠stica t | Valor p |
    > | --------- | ---------- | ----------- | ------------- | ------- |
    > | $\phi_1$  | 0.6        | 0.2         | 3.0           | 0.003   |
    > | $\phi_2$  | 0.2        | 0.15        | 1.33          | 0.185   |
    > | $\theta_1$ | -0.4       | 0.25        | -1.6          | 0.110   |
    > | $\theta_2$ | 0.1        | 0.1         | 1.0           | 0.317   |
    >
    > Os valores p para $\phi_2$, $\theta_1$ e $\theta_2$ s√£o maiores que 0.05, o que indica que esses par√¢metros n√£o s√£o estatisticamente significativos no n√≠vel de signific√¢ncia de 5%. Isso sugere que um modelo mais simples, como um ARMA(1,0) ou AR(1), pode ser mais apropriado.
    >
    > Para confirmar essa suspeita, podemos realizar um teste de raz√£o de verossimilhan√ßas (LRT) para comparar o ARMA(2,2) com o AR(1). Suponha que a log-verossimilhan√ßa do ARMA(2,2) seja -100 e a log-verossimilhan√ßa do AR(1) seja -102. A estat√≠stica do LRT √©:
    >
    > $$LRT = 2 * (log-verossimilhan√ßa_{ARMA(2,2)} - log-verossimilhan√ßa_{AR(1)})$$
    > $$LRT = 2 * (-100 - (-102)) = 4$$
    >
    > O n√∫mero de graus de liberdade √© a diferen√ßa no n√∫mero de par√¢metros entre os dois modelos (4 - 1 = 3). Comparando a estat√≠stica LRT (4) com o valor cr√≠tico de um teste qui-quadrado com 3 graus de liberdade e um n√≠vel de signific√¢ncia de 5% (aproximadamente 7.81), vemos que a estat√≠stica LRT √© menor que o valor cr√≠tico. Isso significa que n√£o h√° evid√™ncia estat√≠stica suficiente para rejeitar a hip√≥tese nula de que o modelo AR(1) √© adequado. Portanto, o AR(1) √© prefer√≠vel ao ARMA(2,2) devido √† sua parcim√¥nia e ajuste estatisticamente compar√°vel.

**Proposi√ß√£o 5:** Se um processo ARMA(p, q) tiver um fator comum entre os polin√¥mios AR e MA, a remo√ß√£o desse fator resultar√° em um modelo equivalente com menos par√¢metros e melhor generaliza√ß√£o.

*Demonstra√ß√£o:* A presen√ßa de um fator comum implica que certos par√¢metros do modelo s√£o desnecess√°rios, pois eles se cancelam mutuamente. Remover esses par√¢metros reduz a complexidade do modelo, tornando-o mais parcimonioso. Modelos mais simples tendem a generalizar melhor para novos dados, pois s√£o menos propensos a overfitting.

**Prova da Proposi√ß√£o 5:**
I.  Seja $\Phi(z)$ o polin√¥mio AR e $\Theta(z)$ o polin√¥mio MA de um processo ARMA(p, q).
II. Assumimos que $\Phi(z)$ e $\Theta(z)$ t√™m um fator comum $F(z)$, de forma que $\Phi(z) = F(z)\Phi'(z)$ e $\Theta(z) = F(z)\Theta'(z)$, onde $\Phi'(z)$ e $\Theta'(z)$ s√£o os polin√¥mios resultantes ap√≥s a remo√ß√£o do fator comum.
III. O processo ARMA(p, q) original √© descrito por $\Phi(L) Y_t = \Theta(L) \epsilon_t$, que pode ser reescrito como $F(L)\Phi'(L) Y_t = F(L)\Theta'(L) \epsilon_t$, onde $L$ √© o operador de defasagem.
IV. Dividindo ambos os lados por $F(L)$ (assumindo que $F(L)$ seja invert√≠vel), obtemos $\Phi'(L) Y_t = \Theta'(L) \epsilon_t$. Este √© um novo processo ARMA com polin√¥mios $\Phi'(z)$ e $\Theta'(z)$, e ordem inferior (menos par√¢metros).
V. Ambos os modelos (o original e o simplificado) geram a mesma fun√ß√£o de autocorrela√ß√£o para Y, dado o cancelamento. A mesma fun√ß√£o de autocorrela√ß√£o implica que o processo gerador √© o mesmo (Teorema da Representa√ß√£o de Wold).
VI. Pelo princ√≠pio da parcim√¥nia, o modelo com menos par√¢metros √© prefer√≠vel, pois ele √© mais simples e menos propenso a overfitting os dados de treinamento. Isso leva a uma melhor generaliza√ß√£o para novos dados.
VII. Portanto, a remo√ß√£o do fator comum resulta em um modelo equivalente com menos par√¢metros e melhor generaliza√ß√£o. $\blacksquare$

**Proposi√ß√£o 5.1:** A equival√™ncia dos modelos pode ser provada atrav√©s da igualdade das fun√ß√µes de autocorrela√ß√£o.

*Demonstra√ß√£o:* Dois modelos ARMA s√£o equivalentes se e somente se suas fun√ß√µes de autocorrela√ß√£o forem id√™nticas. Se um fator comum √© cancelado dos polin√¥mios AR e MA, a fun√ß√£o de autocorrela√ß√£o do modelo resultante permanece a mesma. Isso garante que o modelo simplificado captura as mesmas depend√™ncias temporais dos dados que o modelo original.

**Prova da Proposi√ß√£o 5.1:**
I. Seja $ARMA(p, q)$ um processo com polin√¥mios AR $\Phi(z)$ e MA $\Theta(z)$, e $ARMA(p', q')$ um processo com polin√¥mios AR $\Phi'(z)$ e MA $\Theta'(z)$.
II. As fun√ß√µes de autocorrela√ß√£o de $ARMA(p, q)$ e $ARMA(p', q')$ s√£o denotadas por $\rho(\tau)$ e $\rho'(\tau)$, respectivamente, onde $\tau$ representa o lag temporal.
III. Se $\Phi(z)$ e $\Theta(z)$ t√™m um fator comum $F(z)$, ent√£o $\Phi(z) = F(z)\Phi'(z)$ e $\Theta(z) = F(z)\Theta'(z)$.
IV. A fun√ß√£o de autocorrela√ß√£o $\rho(\tau)$ √© determinada pelos par√¢metros dos polin√¥mios AR e MA. Ao cancelar o fator comum $F(z)$, os par√¢metros dos polin√¥mios AR e MA s√£o alterados, mas a rela√ß√£o entre eles permanece a mesma.
V. Dado que os par√¢metros dos polin√¥mios AR e MA s√£o alterados de forma consistente, a fun√ß√£o de autocorrela√ß√£o resultante $\rho'(\tau)$ √© id√™ntica a $\rho(\tau)$ para todos os lags $\tau$.
VI. Portanto, $ARMA(p, q)$ e $ARMA(p', q')$ s√£o equivalentes, pois suas fun√ß√µes de autocorrela√ß√£o s√£o id√™nticas. $\blacksquare$

<!-- END NEW ADDITION -->
> üí° **Exemplo Num√©rico:** Revisitando o exemplo do ru√≠do branco modelado como ARMA(1,1), a fun√ß√£o de autocorrela√ß√£o para o processo ARMA(1,1) √© a mesma da fun√ß√£o de autocorrela√ß√£o para o ru√≠do branco. Como o modelo de ru√≠do branco tem menos par√¢metros (zero) e se ajusta igualmente bem aos dados, √© prefer√≠vel ao ARMA(1,1).

**Proposi√ß√£o 5.2:** A remo√ß√£o de fatores comuns entre os polin√¥mios AR e MA em um processo ARMA(p,q) n√£o afeta a estacionariedade ou a invertibilidade do processo.

*Demonstra√ß√£o:* A estacionariedade de um processo ARMA √© determinada pelas ra√≠zes do polin√¥mio AR, e a invertibilidade √© determinada pelas ra√≠zes do polin√¥mio MA. Se um fator comum √© removido, as ra√≠zes restantes nos polin√¥mios AR e MA originais permanecem inalteradas. Portanto, as propriedades de estacionariedade e invertibilidade n√£o s√£o afetadas.

**Prova da Proposi√ß√£o 5.2:**
I. A estacionariedade √© determinada pelas ra√≠zes do polin√¥mio AR $\Phi(z)$, e a invertibilidade √© determinada pelas ra√≠zes do polin√¥mio MA $\Theta(z)$.
II. Se $\Phi(z) = F(z)\Phi'(z)$ e $\Theta(z) = F(z)\Theta'(z)$, ent√£o as ra√≠zes de $\Phi(z)$ s√£o as ra√≠zes de $F(z)$ unidas √†s ra√≠zes de $\Phi'(z)$, e similarmente para $\Theta(z)$.
III. Quando um fator comum $F(z)$ √© removido, apenas as ra√≠zes de $F(z)$ s√£o eliminadas dos conjuntos de ra√≠zes originais de $\Phi(z)$ e $\Theta(z)$.
IV. As ra√≠zes restantes, que determinam a estacionariedade e a invertibilidade (as ra√≠zes de $\Phi'(z)$ e $\Theta'(z)$), permanecem inalteradas.
V. Portanto, a remo√ß√£o do fator comum n√£o afeta a estacionariedade ou a invertibilidade do processo. $\blacksquare$

Para complementar a discuss√£o sobre a fatora√ß√£o dos polin√¥mios AR e MA e sua rela√ß√£o com a redund√¢ncia, podemos introduzir o conceito de modelos ARMA can√¥nicos.

**Defini√ß√£o:** Um modelo ARMA(p, q) √© dito can√¥nico (ou irredut√≠vel) se os polin√¥mios AR ($\Phi(z)$) e MA ($\Theta(z)$) n√£o possuem fatores comuns.

**Lema 1:** Para qualquer processo ARMA(p, q) n√£o can√¥nico, existe um processo ARMA(p', q') can√¥nico equivalente, onde p' ‚â§ p e q' ‚â§ q.

*Demonstra√ß√£o:* Se o processo ARMA(p, q) n√£o √© can√¥nico, ent√£o $\Phi(z)$ e $\Theta(z)$ possuem um fator comum, digamos $F(z)$. Dividindo ambos os polin√¥mios por $F(z)$, obtemos polin√¥mios $\Phi'(z)$ e $\Theta'(z)$ de graus p' e q', respectivamente, onde p' < p e q' < q. O processo ARMA(p', q') definido por $\Phi'(L)Y_t = \Theta'(L)\epsilon_t$ √© equivalente ao processo original, mas com menos par√¢metros e sem fatores comuns entre os polin√¥mios AR e MA. Repetindo esse processo at√© que n√£o existam mais fatores comuns, obtemos um processo ARMA can√¥nico equivalente.

**Lema 1.1:** O processo ARMA(p', q') can√¥nico equivalente ao processo ARMA(p, q) n√£o can√¥nico √© √∫nico.

*Demonstra√ß√£o:* Suponha que existam dois processos ARMA can√¥nicos equivalentes, ARMA(p', q') e ARMA(p'', q''), com polin√¥mios $(\Phi'(z), \Theta'(z))$ e $(\Phi''(z), \Theta''(z))$, respectivamente. Se ambos s√£o equivalentes ao processo original ARMA(p, q), ent√£o eles devem ter a mesma fun√ß√£o de autocorrela√ß√£o. A unicidade da representa√ß√£o can√¥nica segue da injetividade do mapeamento entre os par√¢metros ARMA can√¥nicos e a fun√ß√£o de autocorrela√ß√£o. Ou seja, se dois processos ARMA can√¥nicos t√™m a mesma fun√ß√£o de autocorrela√ß√£o, eles devem ter os mesmos par√¢metros.

Al√©m disso, a discuss√£o sobre testes de signific√¢ncia estat√≠stica pode ser estendida com uma an√°lise da matriz de covari√¢ncia dos estimadores dos par√¢metros.

**Observa√ß√£o:** A an√°lise da matriz de covari√¢ncia dos estimadores dos par√¢metros AR e MA pode revelar depend√™ncias entre os estimadores. Se a covari√¢ncia entre dois estimadores, por exemplo, $\hat{\phi_i}$ e $\hat{\theta_j}$, for alta, isso pode indicar que os par√¢metros correspondentes est√£o capturando essencialmente a mesma din√¢mica na s√©rie temporal, sugerindo redund√¢ncia.

> üí° **Exemplo Num√©rico:** Imagine que estimamos um modelo ARMA(1,1) e a matriz de covari√¢ncia dos par√¢metros $\phi_1$ e $\theta_1$ √©:
>
> ```
>        phi1   theta1
> phi1   0.04   -0.038
> theta1 -0.038  0.042
> ```
>
> A correla√ß√£o entre $\hat{\phi_1}$ e $\hat{\theta_1}$ √©:
>
> $$\rho(\hat{\phi_1}, \hat{\theta_1}) = \frac{Cov(\hat{\phi_1}, \hat{\theta_1})}{\sqrt{Var(\hat{\phi_1})Var(\hat{\theta_1})}} = \frac{-0.038}{\sqrt{0.04 * 0.042}} \approx -0.926$$
>
> Uma correla√ß√£o de -0.926 indica uma forte rela√ß√£o negativa entre os estimadores de $\phi_1$ e $\theta_1$, o que sugere que eles est√£o capturando a mesma informa√ß√£o na s√©rie temporal. Isso √© uma forte indica√ß√£o de redund√¢ncia e sugere que um modelo mais simples (como um ru√≠do branco ou um AR(1) ou MA(1)) pode ser mais apropriado.

Para complementar as t√©cnicas para evitar a redund√¢ncia, podemos adicionar uma t√©cnica baseada na decomposi√ß√£o espectral.

7.  **An√°lise Espectral:** Examinar o espectro do processo ARMA estimado. Um espectro "achatado" ou com picos muito amplos pode indicar a presen√ßa de componentes redundantes no modelo. Comparar o espectro do modelo estimado com um modelo de ordem inferior pode ajudar a identificar se a complexidade adicional √© justificada.

    > üí° **Exemplo Num√©rico:** Suponha que estimamos um ARMA(2,1) e a an√°lise espectral revela um espectro quase plano. Isso sugere que o processo √© essencialmente um ru√≠do branco, e os par√¢metros AR e MA adicionais n√£o est√£o contribuindo significativamente para a modelagem da s√©rie temporal. Comparar o espectro do ARMA(2,1) com o espectro de um ru√≠do branco (que √© uma linha horizontal) mostra que eles s√£o muito similares, confirmando a redund√¢ncia dos par√¢metros no ARMA(2,1).
    > ```python
    > import numpy as np
    > import matplotlib.pyplot as plt
    > import statsmodels.api as sm
    > from scipy.signal import periodogram
    >
    > # Simula um ru√≠do branco
    > np.random.seed(0)
    > n = 100
    > white_noise = np.random.randn(n)
    >
    > # Ajusta um ARMA(2,1) aos dados (redundante)
    > model_arma = sm.tsa.ARMA(white_noise, order=(2, 1))
    > results_arma = model_arma.fit(method='mle')
    >
    > # Calcula o espectro do modelo ARMA
    > arma_spectrum = results_arma.spectraldensity(freq_range=(0, 0.5)) # Normalize to 0-0.5 Nyquist
    > frequencies_arma = arma_spectrum.frequencies
    > spectral_density_arma = arma_spectrum.spectral_density
    >
    > # Calcula o espectro do ru√≠do branco
    > frequencies_wn, spectral_density_wn = periodogram(white_noise)
    >
    > # Plot dos espectros
    > plt.figure(figsize=(10, 6))
    > plt.plot(frequencies_arma, spectral_density_arma, label='ARMA(2,1) Espectro')
    > plt.plot(frequencies_wn, spectral_density_wn, label='Ru√≠do Branco Espectro')
    > plt.xlabel('Frequ√™ncia')
    > plt.ylabel('Densidade Espectral')
    > plt.title('Compara√ß√£o dos Espectros')
    > plt.legend()
    > plt.grid(True)
    > plt.show()
    > ```
    > A visualiza√ß√£o mostrar√° que os espectros do ARMA(2,1) e do ru√≠do branco s√£o muito semelhantes, indicando que o modelo ARMA(2,1) √© desnecessariamente complexo para dados que s√£o essencialmente ru√≠do branco.

### Conclus√£o
A redund√¢ncia na parametriza√ß√£o √© um problema potencial ao modelar s√©ries temporais com processos ARMA(p, q). Identificar e evitar essa redund√¢ncia √© crucial para obter modelos parcimoniosos, interpret√°veis e com boa capacidade de generaliza√ß√£o. T√©cnicas como a an√°lise da fun√ß√£o de autocorrela√ß√£o, a fatora√ß√£o dos polin√¥mios AR e MA, a restri√ß√£o de par√¢metros, a an√°lise espectral e o uso de crit√©rios de sele√ß√£o de modelo podem ajudar a garantir a identifica√ß√£o de modelos ARMA(p, q) eficientes e representativos da din√¢mica subjacente da s√©rie temporal. Ao empregar essas t√©cnicas, os analistas podem evitar o overfitting e obter previs√µes mais precisas e confi√°veis.

### Refer√™ncias
[^58]: Se√ß√£o 3.5, Mixed Autoregressive Moving Average Processes
[^60]: P√°gina 60
[^68]: P√°gina 68
[^71]: Se√ß√£o 3.6, Testes Estat√≠sticos
[^72]: P√°gina 72
<!-- END -->