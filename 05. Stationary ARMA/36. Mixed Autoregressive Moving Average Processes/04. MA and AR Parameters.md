## TÃ­tulo Conciso: ParÃ¢metros da RepresentaÃ§Ã£o MA vs. AR em Processos ARMA

### IntroduÃ§Ã£o
Este capÃ­tulo aprofunda a anÃ¡lise de processos mistos autorregressivos de mÃ©dias mÃ³veis (ARMA), explorando a relaÃ§Ã£o entre os parÃ¢metros nas representaÃ§Ãµes de mÃ©dias mÃ³veis (MA) e autorregressivas (AR) de tais processos. Como estabelecido anteriormente [^58], um processo ARMA(p, q) combina componentes AR e MA, e sua anÃ¡lise pode ser realizada atravÃ©s de diferentes perspectivas, incluindo a representaÃ§Ã£o como um processo AR de ordem infinita ou como um processo MA de ordem infinita [^65]. Esta seÃ§Ã£o foca em demonstrar que os parÃ¢metros resultantes dessas representaÃ§Ãµes nÃ£o sÃ£o, em geral, idÃªnticos, e as implicaÃ§Ãµes dessa diferenÃ§a.

### Conceitos Fundamentais
Um processo ARMA(p, q) Ã© definido pela equaÃ§Ã£o:

$$Y_t = c + \sum_{i=1}^{p} \phi_i Y_{t-i} + \epsilon_t + \sum_{i=1}^{q} \theta_i \epsilon_{t-i}$$

onde $Y_t$ Ã© o valor da sÃ©rie temporal no tempo $t$, $c$ Ã© uma constante, $\phi_i$ sÃ£o os coeficientes autorregressivos, $\theta_i$ sÃ£o os coeficientes de mÃ©dias mÃ³veis e $\epsilon_t$ Ã© o termo de erro ou ruÃ­do branco [^47]. A representaÃ§Ã£o em termos do operador de defasagem $L$ Ã© dada por:

$$\Phi(L) Y_t = c + \Theta(L) \epsilon_t$$

onde $\Phi(L) = 1 - \sum_{i=1}^{p} \phi_i L^i$ e $\Theta(L) = 1 + \sum_{i=1}^{q} \theta_i L^i$.

**RepresentaÃ§Ã£o AR Infinita (AR($\infty$))**: Se o processo ARMA(p, q) Ã© estacionÃ¡rio (ou seja, as raÃ­zes do polinÃ´mio $\Phi(L)$ estÃ£o fora do cÃ­rculo unitÃ¡rio [^57, ^67]), ele pode ser representado como um processo AR de ordem infinita [^65]:

$$Y_t = \sum_{j=1}^{\infty} \pi_j Y_{t-j} + v_t $$

ou, equivalentemente,

$$\Pi(L) Y_t = v_t $$

onde $\Pi(L) = 1 - \sum_{j=1}^{\infty} \pi_j L^j$ e $v_t$ Ã© um ruÃ­do branco.

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um processo ARMA(1,0) (equivalente a um AR(1)) definido por $Y_t = 0.7Y_{t-1} + \epsilon_t$.  Aqui, $\phi_1 = 0.7$. A representaÃ§Ã£o AR($\infty$) Ã© trivialmente $Y_t = 0.7Y_{t-1} + \epsilon_t$, assim $\pi_1 = 0.7$ e $\pi_j = 0$ para $j > 1$. Agora, considere um processo ARMA(1,1) definido por $Y_t = 0.7Y_{t-1} + \epsilon_t + 0.3\epsilon_{t-1}$.  Aqui, $\phi_1 = 0.7$ e $\theta_1 = 0.3$. Para obter a representaÃ§Ã£o AR($\infty$), podemos reescrever a equaÃ§Ã£o e expandir:
>
> $\epsilon_t = Y_t - 0.7Y_{t-1} - 0.3\epsilon_{t-1}$
>
> $\epsilon_{t-1} = Y_{t-1} - 0.7Y_{t-2} - 0.3\epsilon_{t-2}$
>
> Substituindo iterativamente, obtemos:
>
> $Y_t = (\phi_1 + \theta_1)Y_{t-1} - \phi_1\theta_1Y_{t-2} + \phi_1\theta_1^2 Y_{t-3} - ... + \epsilon_t$
>
> $Y_t = (0.7 + 0.3)Y_{t-1} - (0.7)(0.3)Y_{t-2} + (0.7)(0.3)^2 Y_{t-3} + ... + \epsilon_t$
>
> $Y_t = 1.0Y_{t-1} - 0.21Y_{t-2} + 0.063 Y_{t-3} + ... + \epsilon_t$
>
> Portanto, $\pi_1 = 1.0$, $\pi_2 = -0.21$, $\pi_3 = 0.063$, etc.  Note que a inclusÃ£o do termo MA altera significativamente os coeficientes na representaÃ§Ã£o AR($\infty$).

**RepresentaÃ§Ã£o MA Infinita (MA($\infty$))**: Se o processo ARMA(p, q) Ã© invertÃ­vel (ou seja, as raÃ­zes do polinÃ´mio $\Theta(L)$ estÃ£o fora do cÃ­rculo unitÃ¡rio [^65]), ele pode ser representado como um processo MA de ordem infinita:

$$Y_t = \sum_{k=0}^{\infty} \psi_k \epsilon_{t-k}$$

ou, equivalentemente,

$$Y_t = \Psi(L) \epsilon_t$$

onde $\Psi(L) = \sum_{k=0}^{\infty} \psi_k L^k$ e $\psi_0 = 1$.

> ğŸ’¡ **Exemplo NumÃ©rico:**  Continuando com o processo ARMA(1,1) do exemplo anterior, $Y_t = 0.7Y_{t-1} + \epsilon_t + 0.3\epsilon_{t-1}$, podemos derivar a representaÃ§Ã£o MA($\infty$). Reescrevendo em termos do operador de defasagem, temos:
>
> $Y_t - 0.7LY_t = \epsilon_t + 0.3L\epsilon_t$
>
> $Y_t = (1 + 0.3L)(1 - 0.7L)^{-1} \epsilon_t$
>
> Expandindo $(1 - 0.7L)^{-1}$ como uma sÃ©rie geomÃ©trica, obtemos:
>
> $Y_t = (1 + 0.3L)(1 + 0.7L + (0.7)^2L^2 + (0.7)^3L^3 + ...) \epsilon_t$
>
> $Y_t = (1 + 0.3L + 0.7L + (0.7)(0.3)L^2 + (0.7)^2L^2 + (0.7)^2(0.3)L^3 + (0.7)^3L^3 + ... ) \epsilon_t$
>
> $Y_t = (1 + 1.0L + 0.49L^2 + 0.343L^3+ 0.147 L^3 + ...) \epsilon_t$
>
> $Y_t = \epsilon_t + 1.0 \epsilon_{t-1} + 0.49 \epsilon_{t-2} + 0.343 \epsilon_{t-3} ...$
>
> Portanto, $\psi_0 = 1$, $\psi_1 = 1.0$, $\psi_2 = 0.49$, $\psi_3 = 0.343$, etc. Comparando com o exemplo da representaÃ§Ã£o AR($\infty$), os coeficientes $\pi_j$ e $\psi_j$ sÃ£o diferentes, demonstrando que as representaÃ§Ãµes AR e MA, embora equivalentes, tÃªm parÃ¢metros distintos.

A autocovariÃ¢ncia para o processo AR(p) derivada da representaÃ§Ã£o MA infinita [^53] Ã© dada por:

$$\gamma_j = h_1 \lambda_1^j + h_2 \lambda_2^j + \ldots + h_p \lambda_p^j,$$
com a condiÃ§Ã£o de que $j > q$ [^53]. A mesma funÃ§Ã£o de autocorrelaÃ§Ã£o para processos AR(p) [^53] Ã© dada por:

$$\gamma_j = g_1 \lambda_1^j + g_2 \lambda_2^j + \ldots + g_p \lambda_p^j,$$
com a condiÃ§Ã£o de que $j > q$ [^53].

**Teorema:** Os parÃ¢metros $h_k$ na representaÃ§Ã£o MA infinita e os parÃ¢metros $g_k$ na representaÃ§Ã£o AR infinita nÃ£o sÃ£o os mesmos.

> ğŸ’¡ **ExplicaÃ§Ã£o:** Esta diferenÃ§a surge porque os coeficientes $h_k$ e $g_k$ sÃ£o determinados pelas condiÃ§Ãµes iniciais utilizadas para resolver as equaÃ§Ãµes de diferenÃ§a que definem as autocovariÃ¢ncias do processo ARMA(p,q). As condiÃ§Ãµes iniciais refletem as diferentes formas como as informaÃ§Ãµes passadas sÃ£o incorporadas nas representaÃ§Ãµes AR e MA, influenciando os pesos dos termos exponenciais $\lambda_i^j$ nas autocovariÃ¢ncias [^53]. Enquanto os autovalores ($\lambda_i$)  sÃ£o determinados pelas raÃ­zes do polinÃ´mio AR, os coeficientes $h_k$ e $g_k$ refletem como cada modo exponencial Ã© "iniciado" a partir das condiÃ§Ãµes iniciais em $t = 0$.

Para complementar a anÃ¡lise da diferenÃ§a entre $h_k$ e $g_k$, podemos formalizar a relaÃ§Ã£o entre os coeficientes da representaÃ§Ã£o ARMA e os coeficientes das representaÃ§Ãµes AR e MA infinitas.

**Lema 6:** Dado um processo ARMA(p, q) estacionÃ¡rio e invertÃ­vel, os coeficientes $\pi_j$ da representaÃ§Ã£o AR($\infty$) e os coeficientes $\psi_k$ da representaÃ§Ã£o MA($\infty$) sÃ£o unicamente determinados pelos coeficientes $\phi_i$ e $\theta_i$ do processo ARMA(p, q).

*DemonstraÃ§Ã£o:*
I.  Da definiÃ§Ã£o, $\Phi(L) Y_t = \Theta(L) \epsilon_t$.
II. Para a representaÃ§Ã£o AR($\infty$), dividimos ambos os lados por $\Theta(L)$, resultando em $\frac{\Phi(L)}{\Theta(L)} Y_t = \epsilon_t$. Dado que o processo Ã© invertÃ­vel, podemos expandir $\frac{\Phi(L)}{\Theta(L)}$ em uma sÃ©rie de potÃªncias do operador de defasagem $L$, obtendo $\Pi(L) = \frac{\Phi(L)}{\Theta(L)} = 1 - \sum_{j=1}^{\infty} \pi_j L^j$.
III. Os coeficientes $\pi_j$ sÃ£o obtidos igualando os coeficientes das potÃªncias correspondentes de $L$ na equaÃ§Ã£o acima. Portanto, os $\pi_j$ sÃ£o funÃ§Ãµes Ãºnicas dos $\phi_i$ e $\theta_i$.
IV. Similarmente, para a representaÃ§Ã£o MA($\infty$), dividimos ambos os lados por $\Phi(L)$, resultando em $Y_t = \frac{\Theta(L)}{\Phi(L)} \epsilon_t$. Dado que o processo Ã© estacionÃ¡rio, podemos expandir $\frac{\Theta(L)}{\Phi(L)}$ em uma sÃ©rie de potÃªncias do operador de defasagem $L$, obtendo $\Psi(L) = \frac{\Theta(L)}{\Phi(L)} = \sum_{k=0}^{\infty} \psi_k L^k$.
V. Os coeficientes $\psi_k$ sÃ£o obtidos igualando os coeficientes das potÃªncias correspondentes de $L$ na equaÃ§Ã£o acima. Portanto, os $\psi_k$ sÃ£o funÃ§Ãµes Ãºnicas dos $\phi_i$ e $\theta_i$.
VI. Isso demonstra que tanto os coeficientes $\pi_j$ quanto os $\psi_k$ sÃ£o unicamente determinados pelos coeficientes $\phi_i$ e $\theta_i$ do processo ARMA(p, q). $\blacksquare$

Para mostrar que, em geral, $\pi_j \neq \psi_j$, podemos considerar um processo ARMA(1,1) e derivar as expressÃµes para $\pi_j$ e $\psi_j$.

*Exemplo:*
Considere o processo ARMA(1,1) dado por $Y_t = \phi Y_{t-1} + \epsilon_t + \theta \epsilon_{t-1}$.

I. **RepresentaÃ§Ã£o AR($\infty$)**: Reescrevemos a equaÃ§Ã£o como $\epsilon_t = Y_t - \phi Y_{t-1} - \theta \epsilon_{t-1}$. Substituindo iterativamente $\epsilon_{t-1}$, $\epsilon_{t-2}$, etc., obtemos:
$\epsilon_t = Y_t - \phi Y_{t-1} - \theta (Y_{t-1} - \phi Y_{t-2} - \theta \epsilon_{t-2}) = Y_t - (\phi + \theta)Y_{t-1} + \phi\theta Y_{t-2} + \theta^2 \epsilon_{t-2}$.

II. Continuando a substituiÃ§Ã£o, chegamos Ã  forma AR($\infty$):
$Y_t = (\phi + \theta)Y_{t-1} - \phi\theta Y_{t-2} + (\phi\theta^2)Y_{t-3} - (\phi\theta^3) Y_{t-4} + \ldots + \epsilon_t$
$Y_t = \sum_{j=1}^{\infty} \pi_j Y_{t-j} + \epsilon_t$, onde $\pi_j = (\phi + \theta)(-\theta)^{j-1}$.

III. **RepresentaÃ§Ã£o MA($\infty$)**: Reescrevemos a equaÃ§Ã£o original como $Y_t - \phi Y_{t-1} = \epsilon_t + \theta \epsilon_{t-1}$, e equivalentemente, $\epsilon_t = Y_t - \phi Y_{t-1} - \theta \epsilon_{t-1}$.
Usamos o operador de defasagem $L$: $Y_t = \phi L Y_t + \epsilon_t + \theta L \epsilon_t$, entÃ£o $Y_t = \frac{1 + \theta L}{1 - \phi L}\epsilon_t$.

IV. Expandindo $\frac{1}{1 - \phi L}$ como uma sÃ©rie geomÃ©trica, temos:
$Y_t = (1 + \theta L)(1 + \phi L + \phi^2 L^2 + \phi^3 L^3 + \ldots) \epsilon_t$
$Y_t = (1 + (\theta + \phi)L + \phi(\theta + \phi)L^2 + \phi^2(\theta + \phi)L^3 + \ldots)\epsilon_t$.

V.  Assim, $Y_t = \sum_{k=0}^{\infty} \psi_k \epsilon_{t-k}$, onde $\psi_0 = 1$ e $\psi_k = \phi^{k-1}(\theta + \phi)$ para $k \geq 1$.

VI. Comparando os coeficientes, temos $\pi_1 = \phi + \theta$ e $\psi_1 = \phi + \theta$. No entanto, $\pi_2 = -\phi \theta$, enquanto $\psi_2 = \phi(\theta + \phi)$. Em geral, $\pi_j \neq \psi_j$ para $j > 1$. Isso demonstra que as representaÃ§Ãµes AR e MA infinitas tÃªm coeficientes diferentes, mesmo que ambos dependam de $\phi$ e $\theta$. $\blacksquare$
<!-- END NEW ADDITION -->
> ğŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar a diferenÃ§a entre as representaÃ§Ãµes AR e MA, considere o processo ARMA(1,1):
> $$Y_t = \phi_1 Y_{t-1} + \epsilon_t + \theta_1 \epsilon_{t-1}$$
> Para obter a representaÃ§Ã£o AR($\infty$), reescrevemos a equaÃ§Ã£o como:
> $$\epsilon_t = Y_t - \phi_1 Y_{t-1} - \theta_1 \epsilon_{t-1}$$
> Substituindo iterativamente $\epsilon_{t-1}$ na equaÃ§Ã£o, obtemos uma representaÃ§Ã£o AR($\infty$) da forma:
> $$Y_t = \sum_{j=1}^{\infty} \pi_j Y_{t-j} + v_t$$
> onde os coeficientes $\pi_j$ dependem tanto de $\phi_1$ quanto de $\theta_1$.
> Para obter a representaÃ§Ã£o MA($\infty$), reescrevemos a equaÃ§Ã£o como:
> $$Y_t = \phi_1 Y_{t-1} + \epsilon_t + \theta_1 \epsilon_{t-1}$$
> Substituindo iterativamente $Y_{t-1}$ na equaÃ§Ã£o, obtemos uma representaÃ§Ã£o MA($\infty$) da forma:
> $$Y_t = \sum_{k=0}^{\infty} \psi_k \epsilon_{t-k}$$
> onde os coeficientes $\psi_k$ tambÃ©m dependem tanto de $\phi_1$ quanto de $\theta_1$, mas de uma maneira diferente dos coeficientes $\pi_j$. A forma funcional distinta das dependÃªncias implica que, em geral, $\pi_j \neq \psi_j$.

A diferenÃ§a entre os parÃ¢metros $h_k$ e $g_k$ implica que, embora ambas as representaÃ§Ãµes AR e MA infinitas capturem a mesma informaÃ§Ã£o sobre as autocovariÃ¢ncias do processo ARMA(p,q), elas o fazem de maneiras distintas, com diferentes pesos nos modos exponenciais subjacentes.

Para conectar essa discussÃ£o com a otimizaÃ§Ã£o e previsÃ£o, podemos analisar como a escolha da representaÃ§Ã£o (AR ou MA) afeta os algoritmos de estimaÃ§Ã£o de parÃ¢metros e os mÃ©todos de previsÃ£o.

**ObservaÃ§Ã£o:** Em cenÃ¡rios prÃ¡ticos, a escolha entre a representaÃ§Ã£o AR e MA pode influenciar o desempenho dos algoritmos de estimaÃ§Ã£o de parÃ¢metros e dos mÃ©todos de previsÃ£o. Em particular:

*   **EstimaÃ§Ã£o:** Se o processo ARMA Ã© mais prÃ³ximo de um processo AR puro, a estimaÃ§Ã£o dos parÃ¢metros $\phi_i$ pode ser mais eficiente do que a estimaÃ§Ã£o conjunta dos parÃ¢metros $\phi_i$ e $\theta_i$. Similarmente, se o processo Ã© mais prÃ³ximo de um processo MA puro, a estimaÃ§Ã£o dos parÃ¢metros $\theta_i$ pode ser mais eficiente.
*   **PrevisÃ£o:** A representaÃ§Ã£o AR Ã© mais adequada para previsÃ£o de curto prazo, pois ela se baseia diretamente nos valores passados da sÃ©rie temporal. A representaÃ§Ã£o MA, por outro lado, Ã© mais adequada para previsÃ£o de longo prazo, pois ela se baseia nos choques passados que afetam a sÃ©rie temporal.

> ğŸ’¡ **Exemplo NumÃ©rico:**  Suponha que temos um ARMA(2,1) com $\phi_1 = 1.5$, $\phi_2 = -0.7$, e $\theta_1 = 0.2$.  Neste caso, as raÃ­zes do polinÃ´mio AR sÃ£o $z_1 \approx 1.07$ e $z_2 \approx 1.07$ (complexas conjugadas). Como as raÃ­zes estÃ£o prÃ³ximas do cÃ­rculo unitÃ¡rio, a representaÃ§Ã£o AR convergirÃ¡ lentamente e a estimaÃ§Ã£o dos parÃ¢metros AR serÃ¡ mais sensÃ­vel aos dados.  A representaÃ§Ã£o MA (se invertÃ­vel) pode ser mais eficiente para previsÃ£o de longo prazo.

Para melhorar ainda mais a compreensÃ£o da estacionariedade e invertibilidade, podemos introduzir um resultado que conecta as representaÃ§Ãµes AR e MA infinitas com a funÃ§Ã£o de autocovariÃ¢ncia do processo ARMA.

**Teorema 7:** Um processo ARMA(p,q) Ã© estacionÃ¡rio e invertÃ­vel se e somente se sua funÃ§Ã£o de autocovariÃ¢ncia pode ser representada como uma combinaÃ§Ã£o linear de exponenciais decrescentes, com pesos determinados pelos coeficientes das representaÃ§Ãµes AR e MA infinitas. Especificamente, existem constantes $c_i$ e $\lambda_i$ tais que $\gamma(h) = \sum_{i=1}^{max(p,q)} c_i \lambda_i^{|h|}$, onde $|\lambda_i| < 1$ para todo $i$.

*EstratÃ©gia da DemonstraÃ§Ã£o:*

A demonstraÃ§Ã£o deste teorema envolve mostrar que a funÃ§Ã£o de autocovariÃ¢ncia de um processo ARMA estacionÃ¡rio e invertÃ­vel satisfaz uma equaÃ§Ã£o de diferenÃ§a linear homogÃªnea de ordem `max(p, q)`. As soluÃ§Ãµes desta equaÃ§Ã£o de diferenÃ§a sÃ£o combinaÃ§Ãµes lineares de exponenciais, onde as taxas de decaimento sÃ£o determinadas pelas raÃ­zes dos polinÃ´mios caracterÃ­sticos associados Ã s partes AR e MA do processo. A estacionariedade e a invertibilidade garantem que todas essas raÃ­zes estejam dentro do cÃ­rculo unitÃ¡rio, resultando em exponenciais decrescentes. Reciprocamente, se a funÃ§Ã£o de autocovariÃ¢ncia tiver essa forma, podemos construir um processo ARMA estacionÃ¡rio e invertÃ­vel que a gere.

AlÃ©m disso, a estacionariedade e invertibilidade de processos ARMA podem ser analisadas utilizando ferramentas da Ã¡lgebra linear.

**Lema:** Um processo ARMA(p,q) Ã© estacionÃ¡rio e invertÃ­vel se e somente se a matriz de Toeplitz associada ao processo Ã© definida positiva.

*DemonstraÃ§Ã£o:*
I. A matriz de Toeplitz $T$ associada a um processo ARMA(p,q) Ã© construÃ­da a partir das autocovariÃ¢ncias do processo:
$$T = \begin{bmatrix}
\gamma_0 & \gamma_1 & \cdots & \gamma_n \\
\gamma_1 & \gamma_0 & \cdots & \gamma_{n-1} \\
\vdots & \vdots & \ddots & \vdots \\
\gamma_n & \gamma_{n-1} & \cdots & \gamma_0
\end{bmatrix}$$
II. Se o processo Ã© estacionÃ¡rio e invertÃ­vel, a funÃ§Ã£o de autocovariÃ¢ncia $\gamma(h)$ decai para zero quando $h$ tende ao infinito.
III. Para que uma matriz de Toeplitz seja definida positiva, todos os seus autovalores devem ser positivos. Isso implica que a energia do processo deve estar distribuÃ­da positivamente em todas as frequÃªncias.
IV. A positividade definida da matriz de Toeplitz estÃ¡ relacionada Ã  condiÃ§Ã£o de que o espectro do processo ARMA seja positivo em todas as frequÃªncias, o que por sua vez estÃ¡ ligado Ã  estacionariedade e invertibilidade.
V. Portanto, a estacionariedade e invertibilidade sÃ£o equivalentes Ã  positividade definida da matriz de Toeplitz associada ao processo.

*Prova (EsboÃ§o)*
Para demonstrar este lema, podemos utilizar a teoria espectral de sÃ©ries temporais. Se o processo for estacionÃ¡rio e invertÃ­vel, o seu espectro Ã© positivo para todas as frequÃªncias, o que implica que a matriz de Toeplitz construÃ­da a partir das autocovariÃ¢ncias Ã© definida positiva. A recÃ­proca tambÃ©m Ã© verdadeira, demonstrando a equivalÃªncia.

Para estender o lema anterior e oferecer uma caracterizaÃ§Ã£o mais completa, podemos relacionar a condiÃ§Ã£o de positividade definida da matriz de Toeplitz com a representaÃ§Ã£o espectral do processo.

**Teorema 8:** Um processo ARMA(p, q) com funÃ§Ã£o espectral $S(f)$ Ã© estacionÃ¡rio e invertÃ­vel se, e somente se, $S(f) > 0$ para todo $f \in [-\pi, \pi]$. AlÃ©m disso, a matriz de Toeplitz associada ao processo Ã© definida positiva se, e somente se, a funÃ§Ã£o espectral correspondente Ã© estritamente positiva.

*EstratÃ©gia da DemonstraÃ§Ã£o:*

A demonstraÃ§Ã£o deste teorema utiliza a representaÃ§Ã£o espectral de processos estacionÃ¡rios. A funÃ§Ã£o espectral $S(f)$ representa a distribuiÃ§Ã£o da variÃ¢ncia do processo em diferentes frequÃªncias. Se o processo Ã© estacionÃ¡rio, a funÃ§Ã£o espectral Ã© real e nÃ£o negativa. A invertibilidade garante que o processo possa ser representado como uma funÃ§Ã£o linear do ruÃ­do branco presente e passado, o que implica que a funÃ§Ã£o espectral nÃ£o pode ser zero em nenhuma frequÃªncia. A positividade definida da matriz de Toeplitz Ã© equivalente Ã  positividade da funÃ§Ã£o espectral. Especificamente, os autovalores da matriz de Toeplitz se aproximam dos valores da funÃ§Ã£o espectral quando a dimensÃ£o da matriz tende ao infinito. Este resultado Ã© uma consequÃªncia do teorema de SzegÅ‘.

Para uma prova completa do teorema, Ã© necessÃ¡rio mergulhar nos detalhes da teoria espectral e das propriedades das matrizes de Toeplitz. Aqui estÃ¡ um esboÃ§o da demonstraÃ§Ã£o que pode ser expandido:

*DemonstraÃ§Ã£o (EsboÃ§o)*

I. **DefiniÃ§Ã£o da FunÃ§Ã£o Espectral:** Seja $Y_t$ um processo ARMA(p,q) estacionÃ¡rio. A funÃ§Ã£o espectral $S(f)$ Ã© definida como a transformada de Fourier da funÃ§Ã£o de autocovariÃ¢ncia $\gamma(h)$:
   $$S(f) = \sum_{h=-\infty}^{\infty} \gamma(h) e^{-i2\pi fh}, \quad f \in [-\pi, \pi]$$

II. **RelaÃ§Ã£o entre Estacionariedade e FunÃ§Ã£o Espectral:** Para um processo estacionÃ¡rio, $S(f)$ Ã© real e nÃ£o negativa: $S(f) \geq 0$.

III. **RelaÃ§Ã£o entre Invertibilidade e FunÃ§Ã£o Espectral:** Se o processo Ã© tambÃ©m invertÃ­vel, entÃ£o $S(f) > 0$ para todo $f$. Isso ocorre porque a invertibilidade garante que o processo possa ser expresso como uma combinaÃ§Ã£o linear do ruÃ­do branco, e, portanto, nÃ£o pode ter "zeros" no espectro. Se $S(f) = 0$ para algum $f$, entÃ£o o processo teria uma componente determinÃ­stica nessa frequÃªncia, o que contradiz a invertibilidade.

IV. **Matriz de Toeplitz e FunÃ§Ã£o Espectral:** A matriz de Toeplitz $T_n$ de dimensÃ£o $(n+1) \times (n+1)$ Ã© definida como:
   $$T_n = [\gamma(i-j)]_{i,j=0}^{n}$$
   onde $\gamma(h)$ Ã© a funÃ§Ã£o de autocovariÃ¢ncia.

V. **Teorema de SzegÅ‘:** O teorema de SzegÅ‘ estabelece uma conexÃ£o entre os autovalores da matriz de Toeplitz e a funÃ§Ã£o espectral. Especificamente, se $\lambda_0^{(n)} \leq \lambda_1^{(n)} \leq \ldots \leq \lambda_n^{(n)}$ sÃ£o os autovalores de $T_n$, entÃ£o para qualquer funÃ§Ã£o contÃ­nua $F$,
   $$\lim_{n \to \infty} \frac{1}{n+1} \sum_{k=0}^{n} F(\lambda_k^{(n)}) = \frac{1}{2\pi} \int_{-\pi}^{\pi} F(S(f)) df$$

VI. **ConexÃ£o com a Positividade Definida:** A matriz de Toeplitz $T_n$ Ã© definida positiva se todos os seus autovalores sÃ£o positivos, ou seja, $\lambda_k^{(n)} > 0$ para todo $k$. Pelo teorema de SzegÅ‘, isso implica que a integral da funÃ§Ã£o espectral Ã© positiva, o que Ã© garantido se $S(f) > 0$ para todo $f$.

VII. **ConclusÃ£o:** Portanto, um processo ARMA(p,q) Ã© estacionÃ¡rio e invertÃ­vel se e somente se sua funÃ§Ã£o espectral Ã© estritamente positiva, $S(f) > 0$, e isso Ã© equivalente Ã  matriz de Toeplitz associada ser definida positiva. $\blacksquare$

### ConclusÃ£o
Os parÃ¢metros $h_k$ e $g_k$ nas representaÃ§Ãµes MA e AR infinitas de um processo ARMA(p, q) nÃ£o sÃ£o, em geral, idÃªnticos. Essa diferenÃ§a decorre das condiÃ§Ãµes iniciais distintas e das diferentes formas como as informaÃ§Ãµes passadas sÃ£o incorporadas em cada representaÃ§Ã£o. Compreender essa distinÃ§Ã£o Ã© crucial para a escolha apropriada de mÃ©todos de estimaÃ§Ã£o e previsÃ£o, bem como para uma interpretaÃ§Ã£o precisa do comportamento da sÃ©rie temporal modelada. A escolha da representaÃ§Ã£o depende das propriedades especÃ­ficas do processo e dos objetivos da anÃ¡lise.

### ReferÃªncias
[^47]: SeÃ§Ã£o 3.2, White Noise
[^53]: SeÃ§Ã£o 3.4, Autoregressive Processes
[^57]: PÃ¡gina 57
[^58]: SeÃ§Ã£o 3.5, Mixed Autoregressive Moving Average Processes
[^65]: SeÃ§Ã£o 3.7, Invertibility
[^67]: PÃ¡gina 67
<!-- END -->