## Estacionariedade em Séries Temporais
### Introdução
Neste capítulo, exploraremos em profundidade o conceito de **estacionariedade** em séries temporais, um pressuposto fundamental para a maioria das técnicas de modelagem e análise de dados temporais. A estacionariedade pode ser abordada sob duas perspectivas principais: **estacionariedade de covariância** (ou fraca) e **estacionariedade estrita**. Compreender as nuances de cada tipo é crucial para a aplicação correta de modelos estatísticos em séries temporais [^4]. Em continuidade ao tópico de Expectativas, Estacionariedade e Ergodicidade, este capítulo aprofunda os critérios de estacionariedade, distinguindo entre os tipos de estacionariedade e sua importância para análise subsequente.

### Conceitos Fundamentais
Um processo é considerado **fracamente estacionário** ou **estacionário de covariância** se sua média e autocovariância não dependem do tempo *t* [^2]. Formalmente, isso significa que a esperança matemática de $Y_t$ é constante para todos os tempos *t*:
$$ E(Y_t) = \mu $$ [^2]
e que a autocovariância entre $Y_t$ e $Y_{t-j}$ depende apenas da defasagem *j*, e não do tempo *t*:
$$ E[(Y_t - \mu)(Y_{t-j} - \mu)] = \gamma_j $$ [^2].

A importância da estacionariedade reside no fato de que ela permite usar dados de diferentes pontos no tempo para fazer inferências estatísticas sobre o processo [^2]. Em outras palavras, se um processo é estacionário, as propriedades estatísticas que observamos no passado são relevantes para o futuro. No entanto, é importante notar que o termo "estacionário" muitas vezes se refere a "estacionário de covariância" na prática [^2]. Em um processo estacionário de covariância, a autocovariância entre dois valores da série depende apenas do tempo que os separa e não do tempo específico t [^2].

Um processo é considerado **estritamente estacionário** se a distribuição conjunta de $(Y_{t_1}, Y_{t_2},..., Y_{t_n})$ depende apenas dos intervalos de tempo entre os pontos $(t_1, t_2,..., t_n)$, e não de seu valor absoluto [^2]. Ou seja, se a distribuição conjunta de $(Y_t, Y_{t+j_1}, ..., Y_{t+j_n})$ depende apenas dos intervalos entre os momentos $(j_1, j_2, ..., j_n)$ e não do momento *t* [^2]. Matematicamente, isso implica que para quaisquer valores de  $j_1, j_2,..., j_n$, a distribuição conjunta de $(Y_t, Y_{t+j_1}, ..., Y_{t+j_n})$ é idêntica à distribuição conjunta de $(Y_s, Y_{s+j_1}, ..., Y_{s+j_n})$, para qualquer *t* e *s* [^2].

A estacionariedade estrita é um conceito mais forte do que a estacionariedade de covariância. Um processo estritamente estacionário com momentos finitos de segunda ordem é também fracamente estacionário [^2]. No entanto, o inverso não é verdadeiro [^2]. Processos podem ser estacionários de covariância sem serem estritamente estacionários, especialmente quando momentos de ordem superior dependem do tempo [^2]. Por exemplo, a média e as autocovariâncias podem ser constantes, mas momentos de ordem superior como $E(Y_t^3)$ podem variar com o tempo [^2].

Um caso especial notável é o de um **processo gaussiano**. Se um processo gaussiano é estacionário de covariância, ele também é estritamente estacionário [^2]. Isso ocorre porque um processo gaussiano é totalmente definido por sua média e variância [^2]. Se essas propriedades são constantes ao longo do tempo, então a distribuição conjunta em qualquer conjunto de pontos no tempo também será constante.

A verificação da estacionariedade estrita é computacionalmente intensiva, e a estacionariedade fraca ou de covariância é frequentemente usada como uma aproximação menos exigente computacionalmente [^2].

Na prática, conjuntos de dados de séries temporais raramente são estacionários [^2]. Muitas vezes, requerem etapas de pré-processamento como remoção de tendências ou diferenciação [^2]. Tais operações são cruciais para algoritmos que assumem a estacionariedade e adicionam uma camada extra de complexidade ao pipeline de dados [^2].

É vital distinguir entre processos estacionários e não estacionários. Por exemplo, um processo é não estacionário se sua média muda com o tempo [^2]. Isso ocorre, por exemplo, em um processo com uma tendência temporal, resultando em covariância não constante entre observações ao longo do tempo [^2]. A tendência temporal faz com que a média varie com o tempo, violando a condição de estacionariedade de covariância [^2].

Em relação à autocovariância, em um processo estacionário, ela depende apenas do atraso *j*. Para processos estacionários, $y_j = y_{-j}$ para todos os inteiros *j* [^3]. Isso significa que a autocovariância entre $Y_t$ e $Y_{t-j}$ é a mesma que entre $Y_t$ e $Y_{t+j}$. Em processos não estacionários, a autocovariância pode variar com o tempo.

### Conclusão
A estacionariedade é um conceito fundamental em análise de séries temporais, crucial para a modelagem estatística e inferência. A distinção entre estacionariedade estrita e de covariância, assim como a compreensão de seus requisitos e implicações, é essencial para aplicar corretamente os modelos e métodos estatísticos. A estacionariedade de covariância, embora seja uma condição mais fraca, serve como uma aproximação prática e útil em muitas aplicações. No entanto, a avaliação da estacionariedade de um conjunto de dados e a aplicação de transformações apropriadas (como diferenciação ou remoção de tendência) para atingir tal estacionariedade são passos cruciais antes da aplicação de modelos de séries temporais.  Em suma, a correta compreensão e aplicação da estacionariedade contribuem para a criação de modelos de séries temporais mais precisos e confiáveis, bem como para a interpretação de resultados de análise de dados temporais. A exploração adicional das autocovariâncias e outros conceitos relacionados em capítulos subsequentes, assim como os exemplos apresentados no contexto deste capítulo, oferecem uma base sólida para modelos mais complexos.

### Referências
[^1]:  
[^2]: *Imagine a battery of I such computers generating sequences {y{1}, {y{2}, ..., {y{I}} and consider selecting the observation associated with date t from each sequence: {y{1), y{2},...,y{I}}. This would be described as a sample of I realizations of the random variable Y.. This random variable has some density, denoted fr(y,), which is called the un-conditional density of Y.. The expectation of the tth observation of a time series refers to the mean of this probability distribution, provided it exists: E(Y) = integral yif(y) dy. The variance of the random variable Y, (denoted you) is similarly defined as Y_or = E(Y_t - mu_t)^2 = integral (y-mu_t)^2 f_Yt(y_t)dy. Given a particular realization such as {y{1} on a time series process, consider constructing a vector x(1) associated with date t. This vector consists of the [j + 1] most recent observations on y as of date t for that realization: x(1) = [y(1)t, y(1)t-1, ..., y(1)t-j]'.  We think of each realization {y{i}t}- as generating one particular value of the vector x, and want to calculate the probability distribution of this vector x across realizations i. This distribution is called the joint distribution of (Yt, Yt-1, ..., Yt-j). From this distribution we can calculate the jth autocovariance of Yt (denoted Yjt):  Yjt = integral ... integral (yt-mu_t)(yt-1-mu_t-1)...(yt-j-mu_t-j) dy_t dy_{t-1}...dy_{t-j}. Thus [3.1.10] could be described as the covariance of Y, with its own lagged value; hence, the term "autocovariance." If neither the mean ut nor the autocovariances Yjt depend on the date t, then the process for Y, is said to be covariance-stationary or weakly stationary:  E(Y) = mu for all t and E(Yt-mu)(Yt-j-mu) = Yj for all t and any j. A different concept is that of strict stationarity. A process is said to be strictly stationary if, for any values of j1, j2,..., jn, the joint distribution of (Yt, Yt+j1, Yt+j2, ... ,Yt+jn) depends only on the intervals separating the dates (j1, j2,..., jn) and not on the date itself (t). Notice that if a process is strictly stationary with finite second moments, then it must be covariance-stationary-if the densities over which we are integrating in [3.1.3] and [3.1.10] do not depend on time, then the moments mu, and yj will not depend on time. However, it is possible to imagine a process that is covariance-stationary but not strictly stationary; the mean and autocovariances could not be functions of time, but perhaps higher moments such as E(Yt3) are. In this text the term "stationary" by itself is taken to mean "covariancestationary." A process {Yt} is said to be Gaussian if the joint density f_Yt,Yt-j1,...Yt-jn(yt, yt-j1,...,yt-jn) is Gaussian for any j1, j2,...,jn. Since the mean and variance are all that are needed to parameterize a multivariate Gaussian distribution completely, a covariance-stationary Gaussian process is strictly stationary.*
[^3]: *If the process is covariance-stationary, then this magnitude is the same for any value of t we might have chosen; for example, we can replace t with t + j: Y_j = E(Y_{t+j}-mu)(Y_t -mu) = E(Y_t-mu)(Y_{t-j}-mu). But referring again to the definition [3.1.12], this last expression is just the definition of y-j. Thus, for any covariance-stationary process, yj = y-j for all integers j.*
[^4]: *The autocovariance yj can be viewed as the (1, j + 1) element of the variance-covariance matrix of the vector xt. For this reason, the autocovariances are described as the second moments of the process for Yt.*
<!-- END -->
