## Aprofundamento no Processo Autoregressivo de Primeira Ordem (AR(1))

### Introdu√ß√£o
Este cap√≠tulo visa aprofundar o entendimento sobre os processos **Autorregressivos de Primeira Ordem (AR(1))**, um dos modelos fundamentais em an√°lise de s√©ries temporais. Construindo sobre os conceitos de **estacionariedade**, **autocovari√¢ncia** e **fun√ß√µes de autocorrela√ß√£o** previamente introduzidos [^45, ^46], exploraremos em detalhes as propriedades, condi√ß√µes de estacionariedade, representa√ß√µes alternativas e aplica√ß√µes do modelo AR(1). Em particular, focaremos na representa√ß√£o do processo AR(1) como um processo de **M√©dia M√≥vel de Ordem Infinita (MA($\infty$))** quando a condi√ß√£o $|\phi|<1$ √© satisfeita [^53, ^54], bem como na an√°lise da fun√ß√£o de autocorrela√ß√£o e suas implica√ß√µes. O objetivo √© fornecer uma base te√≥rica s√≥lida para acad√™micos com conhecimento avan√ßado em matem√°tica e estat√≠stica, permitindo uma an√°lise cr√≠tica e aplica√ß√£o eficaz desses modelos em contextos complexos.

### Conceitos Fundamentais

O modelo **AR(1)** √© definido pela seguinte equa√ß√£o [^53]:

$$ Y_t = c + \phi Y_{t-1} + \epsilon_t $$

onde:
*   $Y_t$ representa o valor da s√©rie temporal no instante *t*.
*   *c* √© uma constante.
*   $\phi$ √© o *coeficiente autoregressivo* de primeira ordem.
*   $\epsilon_t$ √© um termo de *ru√≠do branco* com m√©dia zero e vari√¢ncia constante $\sigma^2$ [^47]. Especificamente, $E(\epsilon_t) = 0$ [^47] e $E(\epsilon_t \epsilon_\tau) = 0$ para $t \neq \tau$ [^48].

Este modelo expressa que o valor atual da s√©rie temporal √© uma fun√ß√£o linear do seu valor anterior, acrescido de um choque aleat√≥rio. Essa depend√™ncia linear de um per√≠odo anterior torna o modelo **AR(1)** o bloco de constru√ß√£o essencial para modelos mais complexos de s√©ries temporais.

> üí° **Exemplo Num√©rico:**
> Suponha que temos uma s√©rie temporal representando o pre√ßo de uma a√ß√£o. Podemos modelar o pre√ßo di√°rio da a√ß√£o usando um modelo AR(1):
> $$Y_t = 0.1 + 0.8Y_{t-1} + \epsilon_t$$
> Aqui, $c = 0.1$ (uma constante), $\phi = 0.8$ (o coeficiente autoregressivo), e $\epsilon_t$ √© o ru√≠do branco com $\sigma^2 = 0.05$. Se $Y_{t-1} = 10$, ent√£o
> $$Y_t = 0.1 + 0.8(10) + \epsilon_t = 8.1 + \epsilon_t$$
> Se $\epsilon_t$ √© um valor aleat√≥rio sorteado de uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia 0.05 (por exemplo, $\epsilon_t = 0.2$), ent√£o $Y_t = 8.3$. Isso significa que o pre√ßo da a√ß√£o no dia *t* √© influenciado pelo pre√ßo do dia anterior (com peso 0.8) mais uma pequena perturba√ß√£o aleat√≥ria.
>
> ```python
> import numpy as np
>
> c = 0.1
> phi = 0.8
> Y_t_minus_1 = 10
> sigma_squared = 0.05
>
> # Simulate epsilon_t from a normal distribution
> epsilon_t = np.random.normal(0, np.sqrt(sigma_squared))
>
> # Calculate Y_t
> Y_t = c + phi * Y_t_minus_1 + epsilon_t
>
> print(f"Y_t-1: {Y_t_minus_1}")
> print(f"Epsilon_t: {epsilon_t:.2f}")
> print(f"Y_t: {Y_t:.2f}")
> ```
>
> Este c√≥digo Python simula um valor para $\epsilon_t$ e calcula $Y_t$ com base na equa√ß√£o AR(1). Note que `epsilon_t:.2f` formata o valor para duas casas decimais.

#### Condi√ß√µes de Estacionariedade
Uma propriedade crucial para a aplicabilidade do modelo **AR(1)** √© a **estacionariedade**. Conforme mencionado anteriormente [^45], um processo √© *covariance-stationary* se sua m√©dia e autocovari√¢ncias n√£o dependem do tempo *t*. No contexto do modelo AR(1), a estacionariedade √© garantida se [^53]:

$$ |\phi| < 1 $$

Esta condi√ß√£o assegura que o impacto de choques passados diminua exponencialmente ao longo do tempo, evitando que a s√©rie temporal divirja. Quando $|\phi| \geq 1$, o processo torna-se n√£o estacion√°rio, e a vari√¢ncia tende ao infinito, invalidando a aplica√ß√£o direta do modelo AR(1) [^53].

> üí° **Exemplo Num√©rico:**
> Se $\phi = 0.5$, a condi√ß√£o de estacionariedade √© satisfeita, pois $|0.5| < 1$. Se $\phi = 1.2$, a condi√ß√£o n√£o √© satisfeita, e o processo AR(1) √© n√£o estacion√°rio. Visualmente, uma s√©rie temporal com $\phi = 0.5$ exibir√° flutua√ß√µes em torno de uma m√©dia constante, enquanto uma s√©rie com $\phi = 1.2$ mostrar√° uma tend√™ncia crescente ou decrescente sem retornar a um n√≠vel m√©dio.
>
> Podemos simular visualmente esse comportamento:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def simulate_ar1(phi, c, sigma_squared, n_samples=100):
>     epsilon = np.random.normal(0, np.sqrt(sigma_squared), n_samples)
>     Y = np.zeros(n_samples)
>     Y[0] = c + epsilon[0]  # Initialize the first value
>     for t in range(1, n_samples):
>         Y[t] = c + phi * Y[t-1] + epsilon[t]
>     return Y
>
> # Simulate stationary AR(1)
> phi_stationary = 0.5
> Y_stationary = simulate_ar1(phi_stationary, 0, 1, 100)
>
> # Simulate non-stationary AR(1)
> phi_non_stationary = 1.2
> Y_non_stationary = simulate_ar1(phi_non_stationary, 0, 1, 100)
>
> # Plotting the time series
> plt.figure(figsize=(12, 6))
> plt.subplot(1, 2, 1)
> plt.plot(Y_stationary)
> plt.title(f'Stationary AR(1) with phi = {phi_stationary}')
> plt.xlabel('Time')
> plt.ylabel('Y_t')
>
> plt.subplot(1, 2, 2)
> plt.plot(Y_non_stationary)
> plt.title(f'Non-Stationary AR(1) with phi = {phi_non_stationary}')
> plt.xlabel('Time')
> plt.ylabel('Y_t')
>
> plt.tight_layout()
> plt.show()
> ```
>
> Este c√≥digo gera duas s√©ries temporais AR(1), uma estacion√°ria e outra n√£o estacion√°ria, e as plota para visualiza√ß√£o.

**Proposi√ß√£o 1** A condi√ß√£o de estacionariedade $|\phi|<1$ implica que o processo AR(1) √© *erg√≥dico para a m√©dia*.

*Demonstra√ß√£o:*
Para um processo ser erg√≥dico para a m√©dia, a m√©dia amostral deve convergir para a m√©dia populacional √† medida que o tamanho da amostra aumenta. Dado que o processo AR(1) √© estacion√°rio quando $|\phi|<1$, sua m√©dia e vari√¢ncia s√£o constantes ao longo do tempo. A m√©dia amostral √© dada por $\bar{Y} = \frac{1}{T}\sum_{t=1}^{T} Y_t$. Sob estacionariedade e a exist√™ncia de momentos de ordem superior finitos (que decorrem das propriedades do ru√≠do branco $\epsilon_t$ e da condi√ß√£o $|\phi|<1$), a Lei Forte dos Grandes N√∫meros implica que $\bar{Y}$ converge para $E[Y_t] = \mu = \frac{c}{1-\phi}$ quando $T \to \infty$. Portanto, o processo AR(1) √© erg√≥dico para a m√©dia quando $|\phi|<1$. $\blacksquare$

Para entender o porqu√™ da condi√ß√£o $|\phi| < 1$ ser necess√°ria para a estacionariedade, podemos reescrever o processo **AR(1)** utilizando substitui√ß√µes sucessivas:

$Y_t = c + \phi Y_{t-1} + \epsilon_t$
$Y_t = c + \phi(c + \phi Y_{t-2} + \epsilon_{t-1}) + \epsilon_t = c + c\phi + \phi^2 Y_{t-2} + \phi \epsilon_{t-1} + \epsilon_t$
$Y_t = c + c\phi + c\phi^2 + \phi^3 Y_{t-3} + \phi^2 \epsilon_{t-2} + \phi \epsilon_{t-1} + \epsilon_t$

Continuando este processo iterativamente:

$Y_t = \epsilon_t + \phi \epsilon_{t-1} + \phi^2 \epsilon_{t-2} + \phi^3 \epsilon_{t-3} + \dots + c(1 + \phi + \phi^2 + \phi^3 + \dots)$

Se $|\phi| < 1$, a soma geom√©trica $1 + \phi + \phi^2 + \phi^3 + \dots$ converge para $\frac{1}{1 - \phi}$, e a equa√ß√£o pode ser reescrita como [^53]:

$$Y_t = \frac{c}{1 - \phi} + \sum_{i=0}^{\infty} \phi^i \epsilon_{t-i}$$

Esta representa√ß√£o demonstra que $Y_t$ √© uma soma ponderada de choques de ru√≠do branco passados ($\epsilon_{t-i}$), e o processo √© estacion√°rio. Se $|\phi| \geq 1$, a soma geom√©trica diverge, e o processo n√£o √© estacion√°rio [^53]. Note que este resultado se alinha com as condi√ß√µes para ergodicity [^47].

> üí° **Exemplo Num√©rico:**
> Vamos ilustrar a converg√™ncia da soma geom√©trica para diferentes valores de $\phi$.
>
> Caso 1: $\phi = 0.5$
> A soma geom√©trica √© $1 + 0.5 + 0.25 + 0.125 + \ldots$ que converge para $\frac{1}{1 - 0.5} = 2$.
>
> Caso 2: $\phi = 0.9$
> A soma geom√©trica √© $1 + 0.9 + 0.81 + 0.729 + \ldots$ que converge para $\frac{1}{1 - 0.9} = 10$.
>
> Caso 3: $\phi = 1.1$
> A soma geom√©trica √© $1 + 1.1 + 1.21 + 1.331 + \ldots$ que diverge.
>
> ```python
> import numpy as np
>
> def geometric_sum(phi, n_terms=10):
>     """Calculates the sum of the first n_terms of a geometric series."""
>     terms = np.array([phi**i for i in range(n_terms)])
>     return np.sum(terms)
>
> # Example usage:
> phi_values = [0.5, 0.9, 1.1]
> for phi in phi_values:
>     sum_result = geometric_sum(phi)
>     print(f"For phi = {phi}, the sum of the first 10 terms is: {sum_result:.2f}")
> ```
>
> Este exemplo demonstra numericamente como a soma geom√©trica converge quando $|\phi| < 1$ e diverge quando $|\phi| \geq 1$.

**Lema 1** Se $|\phi| < 1$, a representa√ß√£o $Y_t = \frac{c}{1 - \phi} + \sum_{i=0}^{\infty} \phi^i \epsilon_{t-i}$ √© convergente em m√©dia quadr√°tica.

*Demonstra√ß√£o:*
Seja $S_n = \sum_{i=0}^{n} \phi^i \epsilon_{t-i}$. Devemos mostrar que $S_n$ converge para algum limite $S$ em m√©dia quadr√°tica, isto √©, $E[(S_n - S)^2] \to 0$ quando $n \to \infty$.
Considere $E[(S_n - S_m)^2]$ para $n > m$. Ent√£o,
I. $E[(S_n - S_m)^2] = E[(\sum_{i=m+1}^{n} \phi^i \epsilon_{t-i})^2]$
II.  Expandindo o quadrado e usando a ortogonalidade dos $\epsilon_t$:
$E[(S_n - S_m)^2] = \sum_{i=m+1}^{n} \phi^{2i} E[\epsilon_{t-i}^2] = \sigma^2 \sum_{i=m+1}^{n} \phi^{2i}$.
III. Como $|\phi| < 1$, a s√©rie $\sum_{i=0}^{\infty} \phi^{2i}$ converge para $\frac{1}{1 - \phi^2}$. Portanto, $\sum_{i=m+1}^{n} \phi^{2i} \to 0$ quando $m, n \to \infty$. Isso implica que $E[(S_n - S_m)^2] \to 0$ quando $m, n \to \infty$. Assim, a sequ√™ncia $S_n$ √© Cauchy em m√©dia quadr√°tica e, portanto, converge em m√©dia quadr√°tica para algum limite $S$. $\blacksquare$

#### Representa√ß√£o MA($\infty$) e Invertibilidade
A representa√ß√£o do processo AR(1) como um processo MA($\infty$) sob a condi√ß√£o de estacionariedade $|\phi|<1$ estabelece uma conex√£o importante com o conceito de *invertibilidade*. Um processo AR √© dito invert√≠vel se ele puder ser expresso como um processo MA. No caso do AR(1) estacion√°rio, essa representa√ß√£o MA($\infty$) explicita a invertibilidade do processo.

**Teorema 1** Um processo AR(1) estacion√°rio √© invert√≠vel.

*Demonstra√ß√£o:*
A representa√ß√£o MA($\infty$) do processo AR(1) estacion√°rio √© dada por $Y_t = \frac{c}{1 - \phi} + \sum_{i=0}^{\infty} \phi^i \epsilon_{t-i}$. Como $|\phi| < 1$, a s√©rie converge absolutamente. A invertibilidade implica que o processo AR(1) pode ser expresso como uma fun√ß√£o linear infinita dos choques passados, o que √© precisamente o que a representa√ß√£o MA($\infty$) demonstra. Portanto, um processo AR(1) estacion√°rio √© invert√≠vel. $\blacksquare$

**Corol√°rio 1** Dado um processo AR(1) estacion√°rio, o operador de retardo $L$ pode ser utilizado para expressar a rela√ß√£o entre $Y_t$ e $\epsilon_t$.

*Demonstra√ß√£o:*
Podemos reescrever a equa√ß√£o do processo AR(1) como $Y_t = c + \phi Y_{t-1} + \epsilon_t$. Usando o operador de retardo $L$, onde $L Y_t = Y_{t-1}$, temos:
$Y_t = c + \phi L Y_t + \epsilon_t$
$Y_t - \phi L Y_t = c + \epsilon_t$
$(1 - \phi L) Y_t = c + \epsilon_t$
$Y_t = \frac{c}{1 - \phi L} + \frac{\epsilon_t}{1 - \phi L}$

Expandindo $\frac{1}{1 - \phi L}$ como uma s√©rie geom√©trica, obtemos $\sum_{i=0}^{\infty} (\phi L)^i$. Portanto,

$Y_t = \frac{c}{1 - \phi} + \sum_{i=0}^{\infty} \phi^i L^i \epsilon_t = \frac{c}{1 - \phi} + \sum_{i=0}^{\infty} \phi^i \epsilon_{t-i}$

Essa representa√ß√£o usando o operador de retardo $L$ √© equivalente √† representa√ß√£o MA($\infty$) e demonstra a invertibilidade do processo AR(1) estacion√°rio. $\blacksquare$

### M√©dia e Vari√¢ncia

Sob a condi√ß√£o de estacionariedade, a **m√©dia** do processo AR(1) √© dada por [^53, ^54]:

$$ \mu = E[Y_t] = \frac{c}{1 - \phi} $$

**Prova da M√©dia:**
I. Tomando a expectativa de ambos os lados da equa√ß√£o $Y_t = c + \phi Y_{t-1} + \epsilon_t$:
$$E[Y_t] = E[c + \phi Y_{t-1} + \epsilon_t]$$

II. Pela linearidade do operador de expectativa:
$$E[Y_t] = E[c] + E[\phi Y_{t-1}] + E[\epsilon_t]$$

III. Como $c$ e $\phi$ s√£o constantes, e $E[\epsilon_t] = 0$:
$$E[Y_t] = c + \phi E[Y_{t-1}]$$

IV. Assumindo estacionariedade, $E[Y_t] = E[Y_{t-1}] = \mu$:
$$\mu = c + \phi \mu$$

V. Resolvendo para $\mu$:
$$\mu - \phi \mu = c$$
$$\mu(1 - \phi) = c$$
$$\mu = \frac{c}{1 - \phi}$$
Portanto, a m√©dia do processo AR(1) √© $\mu = \frac{c}{1 - \phi}$. ‚ñ†

A **vari√¢ncia** do processo AR(1) √© [^53, ^54]:

$$ \gamma_0 = Var[Y_t] = \frac{\sigma^2}{1 - \phi^2} $$

Essas express√µes fornecem informa√ß√µes cruciais sobre o comportamento da s√©rie temporal.

> üí° **Exemplo Num√©rico:**
> Considere o modelo AR(1) com $c = 2$, $\phi = 0.7$ e $\sigma^2 = 4$.
> A m√©dia √©:
> $$\mu = \frac{2}{1 - 0.7} = \frac{2}{0.3} \approx 6.67$$
> A vari√¢ncia √©:
> $$\gamma_0 = \frac{4}{1 - (0.7)^2} = \frac{4}{1 - 0.49} = \frac{4}{0.51} \approx 7.84$$
> Isso significa que, em m√©dia, a s√©rie temporal tem um valor de 6.67 e a dispers√£o dos valores em torno dessa m√©dia √© de aproximadamente 7.84.
>
> Usando Python para calcular:
>
> ```python
> c = 2
> phi = 0.7
> sigma_squared = 4
>
> mu = c / (1 - phi)
> gamma_0 = sigma_squared / (1 - phi**2)
>
> print(f"M√©dia (mu): {mu:.2f}")
> print(f"Vari√¢ncia (gamma_0): {gamma_0:.2f}")
> ```
>
> O c√≥digo calcula a m√©dia e vari√¢ncia do processo AR(1) e as imprime com duas casas decimais.

**Prova da Vari√¢ncia:**
A prova √© dada a partir da equa√ß√£o:
$ Y_t - \mu = \phi(Y_{t-1} - \mu) + \epsilon_t $

I. Elevar ao quadrado ambos os lados:
$ (Y_t - \mu)^2 = (\phi(Y_{t-1} - \mu) + \epsilon_t)^2 $
$ (Y_t - \mu)^2 = \phi^2(Y_{t-1} - \mu)^2 + 2\phi(Y_{t-1} - \mu)\epsilon_t + \epsilon_t^2 $

II. Aplicar o operador de esperan√ßa em ambos os lados:
$ E[(Y_t - \mu)^2] = E[\phi^2(Y_{t-1} - \mu)^2 + 2\phi(Y_{t-1} - \mu)\epsilon_t + \epsilon_t^2] $

III. Usar a propriedade de estacionariedade $E[(Y_t - \mu)^2] = E[(Y_{t-1} - \mu)^2] = \gamma_0$, e a independ√™ncia de $\epsilon_t$ e $Y_{t-1}$, tal que $E[2\phi(Y_{t-1} - \mu)\epsilon_t] = 0$:
$ \gamma_0 = \phi^2\gamma_0 + \sigma^2 $

IV. Resolver para $\gamma_0$:
$ \gamma_0 - \phi^2\gamma_0 = \sigma^2 $
$ \gamma_0(1 - \phi^2) = \sigma^2 $
$ \gamma_0 = \frac{\sigma^2}{1 - \phi^2} $ ‚ñ†

Al√©m da m√©dia e vari√¢ncia, √© importante analisar os momentos de ordem superior para caracterizar completamente a distribui√ß√£o do processo AR(1).

**Lema 2** Para um processo AR(1) estacion√°rio, a curtose √© dada por:

$$Kurt[Y_t] = E\left[\frac{(Y_t - \mu)^4}{\gamma_0^2}\right] - 3 = \frac{6\phi^2}{1 - \phi^2}$$

*Demonstra√ß√£o:*
A curtose mede o grau de cauda pesada da distribui√ß√£o em rela√ß√£o a uma distribui√ß√£o normal. Para calcular a curtose, precisamos calcular o quarto momento central. A deriva√ß√£o completa √© complexa, mas o resultado final mostra que a curtose depende apenas de $\phi$. Quando $\phi$ se aproxima de 0, a curtose se aproxima de 0, indicando que a distribui√ß√£o se aproxima de uma distribui√ß√£o normal. Quando $\phi$ se aproxima de 1 (mantendo $|\phi|<1$ para garantir estacionariedade), a curtose tende ao infinito, indicando caudas mais pesadas do que uma distribui√ß√£o normal. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Suponha $\phi = 0.5$. Ent√£o, a curtose √©:
>
> $$ Kurt[Y_t] = \frac{6(0.5)^2}{1 - (0.5)^2} = \frac{6(0.25)}{1 - 0.25} = \frac{1.5}{0.75} = 2 $$
>
> Isso indica que a distribui√ß√£o tem caudas mais pesadas do que uma distribui√ß√£o normal (curtose de 0).
>
> ```python
> import numpy as np
>
> phi = 0.5
> kurtosis = (6 * phi**2) / (1 - phi**2)
> print(f"Curtose: {kurtosis}")
> ```

#### Fun√ß√£o de Autocorrela√ß√£o (ACF)
A **fun√ß√£o de autocorrela√ß√£o (ACF)** descreve a correla√ß√£o entre os valores da s√©rie temporal em diferentes pontos no tempo. Para o processo AR(1), a autocovari√¢ncia no lag *j* √© dada por [^54]:

$$ \gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = \phi^j \frac{\sigma^2}{1 - \phi^2} $$

**Prova da Autocovari√¢ncia:**
I. Multiplicar ambos os lados da equa√ß√£o $Y_t - \mu = \phi(Y_{t-1} - \mu) + \epsilon_t$ por $(Y_{t-j} - \mu)$:
$$(Y_t - \mu)(Y_{t-j} - \mu) = \phi(Y_{t-1} - \mu)(Y_{t-j} - \mu) + \epsilon_t(Y_{t-j} - \mu)$$

II. Tomar a esperan√ßa de ambos os lados:
$$E[(Y_t - \mu)(Y_{t-j} - \mu)] = \phi E[(Y_{t-1} - \mu)(Y_{t-j} - \mu)] + E[\epsilon_t(Y_{t-j} - \mu)]$$

III. Observar que $E[(Y_t - \mu)(Y_{t-j} - \mu)] = \gamma_j$ e, para $j > 0$, $E[\epsilon_t(Y_{t-j} - \mu)] = 0$ pois $\epsilon_t$ √© ru√≠do branco e independente de $Y_{t-j}$:
$$\gamma_j = \phi \gamma_{j-1}$$

IV. Aplicar recursivamente a rela√ß√£o $\gamma_j = \phi \gamma_{j-1}$:
$$\gamma_1 = \phi \gamma_0$$
$$\gamma_2 = \phi \gamma_1 = \phi^2 \gamma_0$$
$$\gamma_j = \phi^j \gamma_0$$

V. Substituir $\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$:
$$\gamma_j = \phi^j \frac{\sigma^2}{1 - \phi^2}$$
Portanto, a autocovari√¢ncia no lag *j* √© $\gamma_j = \phi^j \frac{\sigma^2}{1 - \phi^2}$. ‚ñ†

A fun√ß√£o de autocorrela√ß√£o (ACF) √© ent√£o calculada como [^49, ^54]:

$$ \rho_j = \frac{\gamma_j}{\gamma_0} = \phi^j $$

Essa fun√ß√£o demonstra um decaimento exponencial das autocorrela√ß√µes √† medida que o lag *j* aumenta, caracter√≠stico dos processos AR(1) [^54]. A forma da ACF, $\rho_j = \phi^j$, demonstra o *decaimento geom√©trico* das autocorrela√ß√µes com o aumento do lag j. A velocidade do decaimento √© determinada pelo valor absoluto de $\phi$; quanto menor o valor absoluto de $\phi$, mais r√°pido o decaimento. O sinal de $\phi$ determina se a autocorrela√ß√£o √© positiva (para $\phi > 0$) ou alternada (para $\phi < 0$).

> üí° **Exemplo Num√©rico:**
> Se $\phi = 0.6$, a ACF para os primeiros lags √©:
> *   $\rho_0 = (0.6)^0 = 1$
> *   $\rho_1 = (0.6)^1 = 0.6$
> *   $\rho_2 = (0.6)^2 = 0.36$
> *   $\rho_3 = (0.6)^3 = 0.216$
> A ACF decai exponencialmente. Isso indica que a correla√ß√£o entre os valores da s√©rie temporal diminui √† medida que o lag aumenta.
>
> ```python
> import numpy as np
>
> phi = 0.6
> lags = np.arange(4)  # Lags 0 to 3
> acf_values = phi ** lags
>
> print("ACF Values:")
> for lag, acf in zip(lags, acf_values):
>     print(f"Lag {lag}: {acf:.3f}")
> ```
>
> Este c√≥digo calcula e imprime os valores da ACF para os primeiros 4 lags.

```mermaid
graph LR
    A[Lag 0] --> B(1.0)
    A --> C[Lag 1]
    C --> D(0.6)
    A --> E[Lag 2]
    E --> F(0.36)
    A --> G[Lag 3]
    G --> H(0.216)
    style B fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#f9f,stroke:#333,stroke-width:2px
    style H fill:#f9f,stroke:#333,stroke-width:2px
```

**Teorema 2** (Yule-Walker Equations for AR(1)) As equa√ß√µes de Yule-Walker para um processo AR(1) fornecem uma rela√ß√£o entre os coeficientes do modelo e as autocorrela√ß√µes.

*Demonstra√ß√£o:*
Multiplicando ambos os lados da equa√ß√£o $Y_t = c + \phi Y_{t-1} + \epsilon_t$ por $Y_{t-j} - \mu$ e tomando a esperan√ßa, obtemos:

$E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(c - \mu + \phi (Y_{t-1} - \mu) + \epsilon_t)(Y_{t-j} - \mu)]$

Para $j = 1$:
$\gamma_1 = \phi \gamma_0$
$\rho_1 = \frac{\gamma_1}{\gamma_0} = \phi$

Para $j > 1$:
$\gamma_j = \phi \gamma_{j-1}$
$\rho_j = \phi \rho_{j-1}$

Estas s√£o as equa√ß√µes de Yule-Walker para o processo AR(1). Elas relacionam diretamente o coeficiente autoregressivo $\phi$ com as autocorrela√ß√µes da s√©rie temporal. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Suponha que temos uma s√©rie temporal com $\gamma_0 = 10$ e $\gamma_1 = 6$. Usando as equa√ß√µes de Yule-Walker, podemos estimar $\phi$ como:
> $\phi = \frac{\gamma_1}{\gamma_0} = \frac{6}{10} = 0.6$
> Isso significa que o coeficiente autoregressivo √© 0.6, indicando uma depend√™ncia positiva entre os valores da s√©rie temporal em lags adjacentes.
>
> ```python
> gamma_0 = 10
> gamma_1 = 6
> phi_estimate = gamma_1 / gamma_0
> print(f"Estimativa de phi usando Yule-Walker: {phi_estimate}")
> ```

Para o c√°lculo da **autocorrela√ß√£o parcial (PACF)** de um processo AR(1), notamos que esta √© diferente de zero somente para o lag 1.
$$ \alpha_{kk} =
\begin{cases}
    \phi, & \text{se } k = 1 \\
    0, & \text{se } k > 1
\end{cases}
$$

Esta propriedade da PACF √© fundamental para identificar a ordem de um modelo AR, demonstrando que um processo AR(1) possui depend√™ncia apenas com o seu primeiro lag.

**Proposi√ß√£o 2** A fun√ß√£o de autocorrela√ß√£o parcial (PACF) de um processo AR(1) estacion√°rio corta ap√≥s o lag 1.

*Demonstra√ß√£o:*
A PACF mede a correla√ß√£o entre $Y_t$ e $Y_{t-k}$ ap√≥s remover o efeito dos lags intermedi√°rios $Y_{t-1}, Y_{t-2}, \ldots, Y_{t-k+1}$. Para um processo AR(1), a depend√™ncia de $Y_t$ em rela√ß√£o ao seu passado est√° totalmente capturada por $Y_{t-1}$. Portanto, a correla√ß√£o parcial entre $Y_t$ e $Y_{t-k}$ para $k > 1$, condicionada aos lags intermedi√°rios, √© zero. Isso implica que a PACF √© n√£o nula apenas para o lag 1 e zero para todos os lags superiores a 1. Portanto, a PACF "corta" ap√≥s o lag 1. $\blacksquare$

**Lema 3** (Rela√ß√£o entre ACF e PACF) A ACF e a PACF s√£o ferramentas complementares na identifica√ß√£o da ordem de um processo AR(1). A ACF decai geometricamente, enquanto a PACF corta ap√≥s o lag 1.

*Demonstra√ß√£o:*
A demonstra√ß√£o decorre diretamente das propriedades da ACF e PACF j√° estabelecidas. A ACF descreve a correla√ß√£o entre valores da s√©rie temporal em diferentes lags, incluindo tanto os efeitos diretos quanto os indiretos dos lags intermedi√°rios. Em um processo AR(1), essa correla√ß√£o decai geometricamente devido √† depend√™ncia de primeira ordem. A PACF, por outro lado, mede a correla√ß√£o direta entre valores em diferentes lags, removendo a influ√™ncia dos lags intermedi√°rios. Para um processo AR(1), essa correla√ß√£o direta √© n√£o nula apenas para o lag 1, pois a depend√™ncia √© apenas de primeira ordem. Portanto, a ACF decai geometricamente e a PACF corta ap√≥s o lag 1, demonstrando sua complementaridade na identifica√ß√£o da ordem do modelo. $\blacksquare$

### Rela√ß√£o com Modelos de M√©dia M√≥vel (MA)
A representa√ß√£o do processo AR(1) como um MA($\infty$) ilustra como a depend√™ncia linear dos valores passados, caracter√≠stica dos modelos AR, pode ser expressa em termos de choques aleat√≥rios passados, caracter√≠stica dos modelos MA. Esta representa√ß√£o fornece insights valiosos para entender a estrutura de depend√™ncia temporal em s√©ries temporais e auxilia na escolha do modelo mais adequado para representar os dados.

**Teorema 3** (Wold Decomposition Theorem) Qualquer processo estoc√°stico estacion√°rio puramente n√£o determin√≠stico pode ser representado como uma soma ponderada infinita de inova√ß√µes (ru√≠do branco), ou seja, como um processo MA($\infty$).

Este teorema, fundamental na teoria de s√©ries temporais, garante que qualquer processo estacion√°rio pode ser expresso em termos de um processo de m√©dia m√≥vel de ordem infinita. A representa√ß√£o MA($\infty$) do AR(1) √© um caso espec√≠fico deste teorema.

**Proposi√ß√£o 3** A representa√ß√£o MA($\infty$) do AR(1) pode ser aproximada por um modelo MA de ordem finita (MA(q)) para fins pr√°ticos.

*Demonstra√ß√£o:*
Dado que $|\phi| < 1$, os coeficientes $\phi^i$ na representa√ß√£o MA($\infty$) $Y_t = \frac{c}{1 - \phi} + \sum_{i=0}^{\infty} \phi^i \epsilon_{t-i}$ decaem geometricamente. Isso significa que, para um *q* suficientemente grande, a contribui√ß√£o dos termos $\phi^i \epsilon_{t-i}$ para $i > q$ torna-se insignificante. Portanto, podemos truncar a s√©rie em *q* termos e obter uma aproxima√ß√£o razo√°vel do processo AR(1) como um modelo MA(q): $Y_t \approx \frac{c}{1 - \phi} + \sum_{i=0}^{q} \phi^i \epsilon_{t-i}$. A escolha de *q* depende da precis√£o desejada e do valor de $\phi$. Quanto menor $|\phi|$, menor o valor de *q* necess√°rio para uma boa aproxima√ß√£o. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Considere um processo AR(1) com$\phi = 0.9$ e $\epsilon \sim N(0,1)$. Desejamos encontrar um valor de *q* tal que $||R_q - R||_F < 0.01$.  Usando a Proposi√ß√£o 1, temos que $||R_q - R||_F \leq \frac{\phi^{q+1}}{\sqrt{1 - \phi^2}}$.  Portanto, precisamos encontrar *q* tal que $\frac{0.9^{q+1}}{\sqrt{1 - 0.9^2}} < 0.01$. Resolvendo para *q*, encontramos que $q > \frac{\log(0.01\sqrt{1 - 0.9^2})}{\log(0.9)} - 1 \approx 41.07$. Assim, *q* = 42 deve ser suficiente.

### Estacionariedade em Modelos ARMA(p, q)

A estacionariedade de um processo ARMA(p, q) depende exclusivamente dos coeficientes da parte AR do modelo. Um processo ARMA(p, q) √© estacion√°rio se as ra√≠zes do polin√¥mio caracter√≠stico da parte AR estiverem fora do c√≠rculo unit√°rio. Formalmente:

Seja o polin√¥mio caracter√≠stico da parte AR dado por:

$A(z) = 1 - \phi_1 z - \phi_2 z^2 - ... - \phi_p z^p$

O processo ARMA(p, q) √© estacion√°rio se todas as ra√≠zes $z_i$ de $A(z) = 0$ satisfazem $|z_i| > 1$ para $i = 1, 2, ..., p$.

> üí° **Exemplo:**
> Considere um modelo ARMA(2, 1) dado por:
> $X_t = \phi_1 X_{t-1} + \phi_2 X_{t-2} + \epsilon_t + \theta_1 \epsilon_{t-1}$
> O polin√¥mio caracter√≠stico √©:
> $A(z) = 1 - \phi_1 z - \phi_2 z^2$
> Para que o processo seja estacion√°rio, as ra√≠zes de $1 - \phi_1 z - \phi_2 z^2 = 0$ devem estar fora do c√≠rculo unit√°rio.

### Invertibilidade em Modelos ARMA(p, q)

A invertibilidade de um processo ARMA(p, q) depende exclusivamente dos coeficientes da parte MA do modelo. Um processo ARMA(p, q) √© invert√≠vel se as ra√≠zes do polin√¥mio caracter√≠stico da parte MA estiverem fora do c√≠rculo unit√°rio. Formalmente:

Seja o polin√¥mio caracter√≠stico da parte MA dado por:

$B(z) = 1 + \theta_1 z + \theta_2 z^2 + ... + \theta_q z^q$

O processo ARMA(p, q) √© invert√≠vel se todas as ra√≠zes $z_i$ de $B(z) = 0$ satisfazem $|z_i| > 1$ para $i = 1, 2, ..., q$.

> üí° **Exemplo:**
> Considere um modelo ARMA(1, 1) dado por:
> $X_t = \phi_1 X_{t-1} + \epsilon_t + \theta_1 \epsilon_{t-1}$
> O polin√¥mio caracter√≠stico da parte MA √©:
> $B(z) = 1 + \theta_1 z$
> Para que o processo seja invert√≠vel, a raiz de $1 + \theta_1 z = 0$ deve estar fora do c√≠rculo unit√°rio. Isso implica que $|\theta_1| < 1$.

### Fun√ß√£o de Autocorrela√ß√£o Parcial (PACF)

A Fun√ß√£o de Autocorrela√ß√£o Parcial (PACF) mede a correla√ß√£o entre $X_t$ e $X_{t-k}$, removendo o efeito das autocorrela√ß√µes nos lags intermedi√°rios. Em outras palavras, a PACF mede a correla√ß√£o adicional explicada por $X_{t-k}$ sobre e acima daquela explicada por $X_{t-1}, X_{t-2}, ..., X_{t-k+1}$.

A PACF √© particularmente √∫til para identificar a ordem *p* de um modelo AR(p). Para um processo AR(p), a PACF ter√° um corte abrupto ap√≥s o lag *p*. Isso significa que $\alpha_k \approx 0$ para $k > p$, onde $\alpha_k$ √© a PACF no lag *k*.

Formalmente, a PACF no lag *k*, denotada por $\alpha_k$, √© o √∫ltimo coeficiente ($\phi_{kk}$) na regress√£o de $X_t$ em $X_{t-1}, X_{t-2}, ..., X_{t-k}$:

$X_t = \phi_{k1} X_{t-1} + \phi_{k2} X_{t-2} + ... + \phi_{kk} X_{t-k} + \epsilon_t$

onde $\alpha_k = \phi_{kk}$.

> üí° **Exemplo:**
> Para um processo AR(2), a PACF ter√° valores significativos em lags 1 e 2, e ser√° aproximadamente zero para lags maiores que 2.

### Rela√ß√£o entre ACF e PACF para Modelos ARMA

A ACF e a PACF fornecem informa√ß√µes complementares sobre a estrutura de depend√™ncia temporal de uma s√©rie temporal. A tabela abaixo resume o comportamento t√≠pico da ACF e PACF para modelos AR, MA e ARMA:

| Modelo   | ACF                                  | PACF                                 |
| :------- | :----------------------------------- | :----------------------------------- |
| AR(p)    | Decai exponencialmente ou sinusoidal | Corte abrupto ap√≥s o lag *p*         |
| MA(q)    | Corte abrupto ap√≥s o lag *q*         | Decai exponencialmente ou sinusoidal |
| ARMA(p,q)| Decai ap√≥s o lag *q*                 | Decai ap√≥s o lag *p*                 |

<!-- END -->