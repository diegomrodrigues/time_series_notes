## Aprofundamento no Processo Autoregressivo de Primeira Ordem (AR(1))

### Introdu√ß√£o
Este cap√≠tulo visa aprofundar o entendimento sobre os processos **Autorregressivos de Primeira Ordem (AR(1))**, um dos modelos fundamentais em an√°lise de s√©ries temporais. Construindo sobre os conceitos de **estacionariedade**, **autocovari√¢ncia** e **fun√ß√µes de autocorrela√ß√£o** previamente introduzidos [^45, ^46], exploraremos em detalhes as propriedades, condi√ß√µes de estacionariedade, representa√ß√µes alternativas e aplica√ß√µes do modelo AR(1).

### Conceitos Fundamentais

O modelo **AR(1)** √© definido pela seguinte equa√ß√£o [^53]:

$$ Y_t = c + \phi Y_{t-1} + \epsilon_t $$

onde:
*   $Y_t$ representa o valor da s√©rie temporal no instante *t*.
*   *c* √© uma constante.
*   $\phi$ √© o *coeficiente autoregressivo* de primeira ordem.
*   $\epsilon_t$ √© um termo de *ru√≠do branco* com m√©dia zero e vari√¢ncia constante $\sigma^2$ [^47]. Especificamente, $E(\epsilon_t) = 0$ [^47] e $E(\epsilon_t \epsilon_\tau) = 0$ para $t \neq \tau$ [^48].

Este modelo expressa que o valor atual da s√©rie temporal √© uma fun√ß√£o linear do seu valor anterior, acrescido de um choque aleat√≥rio. Essa depend√™ncia linear de um per√≠odo anterior torna o modelo **AR(1)** o bloco de constru√ß√£o essencial para modelos mais complexos de s√©ries temporais.

> üí° **Exemplo Num√©rico:**
> Suponha que temos uma s√©rie temporal representando o pre√ßo de uma a√ß√£o. Podemos modelar o pre√ßo di√°rio da a√ß√£o usando um modelo AR(1):
> $$Y_t = 0.1 + 0.8Y_{t-1} + \epsilon_t$$
> Aqui, $c = 0.1$ (uma constante), $\phi = 0.8$ (o coeficiente autoregressivo), e $\epsilon_t$ √© o ru√≠do branco com $\sigma^2 = 0.05$. Se $Y_{t-1} = 10$, ent√£o
> $$Y_t = 0.1 + 0.8(10) + \epsilon_t = 8.1 + \epsilon_t$$
> Se $\epsilon_t$ √© um valor aleat√≥rio sorteado de uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia 0.05 (por exemplo, $\epsilon_t = 0.2$), ent√£o $Y_t = 8.3$. Isso significa que o pre√ßo da a√ß√£o no dia *t* √© influenciado pelo pre√ßo do dia anterior (com peso 0.8) mais uma pequena perturba√ß√£o aleat√≥ria.
>
> ```python
> import numpy as np
>
> c = 0.1
> phi = 0.8
> Y_t_minus_1 = 10
> sigma_squared = 0.05
>
> # Simulate epsilon_t from a normal distribution
> epsilon_t = np.random.normal(0, np.sqrt(sigma_squared))
>
> # Calculate Y_t
> Y_t = c + phi * Y_t_minus_1 + epsilon_t
>
> print(f"Y_t-1: {Y_t_minus_1}")
> print(f"Epsilon_t: {epsilon_t:.2f}")
> print(f"Y_t: {Y_t:.2f}")
> ```
>
> Este c√≥digo Python simula um valor para $\epsilon_t$ e calcula $Y_t$ com base na equa√ß√£o AR(1). Note que `epsilon_t:.2f` formata o valor para duas casas decimais.

#### Condi√ß√µes de Estacionariedade
Uma propriedade crucial para a aplicabilidade do modelo **AR(1)** √© a **estacionariedade**. Conforme mencionado anteriormente [^45], um processo √© *covariance-stationary* se sua m√©dia e autocovari√¢ncias n√£o dependem do tempo *t*. No contexto do modelo AR(1), a estacionariedade √© garantida se [^53]:

$$ |\phi| < 1 $$

Esta condi√ß√£o assegura que o impacto de choques passados diminua exponencialmente ao longo do tempo, evitando que a s√©rie temporal divirja. Quando $|\phi| \geq 1$, o processo torna-se n√£o estacion√°rio, e a vari√¢ncia tende ao infinito, invalidando a aplica√ß√£o direta do modelo AR(1) [^53].

> üí° **Exemplo Num√©rico:**
> Se $\phi = 0.5$, a condi√ß√£o de estacionariedade √© satisfeita, pois $|0.5| < 1$. Se $\phi = 1.2$, a condi√ß√£o n√£o √© satisfeita, e o processo AR(1) √© n√£o estacion√°rio. Visualmente, uma s√©rie temporal com $\phi = 0.5$ exibir√° flutua√ß√µes em torno de uma m√©dia constante, enquanto uma s√©rie com $\phi = 1.2$ mostrar√° uma tend√™ncia crescente ou decrescente sem retornar a um n√≠vel m√©dio.
>
> Podemos simular visualmente esse comportamento:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def simulate_ar1(phi, c, sigma_squared, n_samples=100):
>     epsilon = np.random.normal(0, np.sqrt(sigma_squared), n_samples)
>     Y = np.zeros(n_samples)
>     Y[0] = c + epsilon[0]  # Initialize the first value
>     for t in range(1, n_samples):
>         Y[t] = c + phi * Y[t-1] + epsilon[t]
>     return Y
>
> # Simulate stationary AR(1)
> phi_stationary = 0.5
> Y_stationary = simulate_ar1(phi_stationary, 0, 1, 100)
>
> # Simulate non-stationary AR(1)
> phi_non_stationary = 1.2
> Y_non_stationary = simulate_ar1(phi_non_stationary, 0, 1, 100)
>
> # Plotting the time series
> plt.figure(figsize=(12, 6))
> plt.subplot(1, 2, 1)
> plt.plot(Y_stationary)
> plt.title(f'Stationary AR(1) with phi = {phi_stationary}')
> plt.xlabel('Time')
> plt.ylabel('Y_t')
>
> plt.subplot(1, 2, 2)
> plt.plot(Y_non_stationary)
> plt.title(f'Non-Stationary AR(1) with phi = {phi_non_stationary}')
> plt.xlabel('Time')
> plt.ylabel('Y_t')
>
> plt.tight_layout()
> plt.show()
> ```
>
> Este c√≥digo gera duas s√©ries temporais AR(1), uma estacion√°ria e outra n√£o estacion√°ria, e as plota para visualiza√ß√£o.

**Proposi√ß√£o 1** A condi√ß√£o de estacionariedade $|\phi|<1$ implica que o processo AR(1) √© *erg√≥dico para a m√©dia*.

*Demonstra√ß√£o:*
Para um processo ser erg√≥dico para a m√©dia, a m√©dia amostral deve convergir para a m√©dia populacional √† medida que o tamanho da amostra aumenta. Dado que o processo AR(1) √© estacion√°rio quando $|\phi|<1$, sua m√©dia e vari√¢ncia s√£o constantes ao longo do tempo. A m√©dia amostral √© dada por $\bar{Y} = \frac{1}{T}\sum_{t=1}^{T} Y_t$. Sob estacionariedade e a exist√™ncia de momentos de ordem superior finitos (que decorrem das propriedades do ru√≠do branco $\epsilon_t$ e da condi√ß√£o $|\phi|<1$), a Lei Forte dos Grandes N√∫meros implica que $\bar{Y}$ converge para $E[Y_t] = \mu = \frac{c}{1-\phi}$ quando $T \to \infty$. Portanto, o processo AR(1) √© erg√≥dico para a m√©dia quando $|\phi|<1$. $\blacksquare$

Para entender o porqu√™ da condi√ß√£o $|\phi| < 1$ ser necess√°ria para a estacionariedade, podemos reescrever o processo **AR(1)** utilizando substitui√ß√µes sucessivas:

$Y_t = c + \phi Y_{t-1} + \epsilon_t$
$Y_t = c + \phi(c + \phi Y_{t-2} + \epsilon_{t-1}) + \epsilon_t = c + c\phi + \phi^2 Y_{t-2} + \phi \epsilon_{t-1} + \epsilon_t$
$Y_t = c + c\phi + c\phi^2 + \phi^3 Y_{t-3} + \phi^2 \epsilon_{t-2} + \phi \epsilon_{t-1} + \epsilon_t$

Continuando este processo iterativamente:

$Y_t = \epsilon_t + \phi \epsilon_{t-1} + \phi^2 \epsilon_{t-2} + \phi^3 \epsilon_{t-3} + \dots + c(1 + \phi + \phi^2 + \phi^3 + \dots)$

Se $|\phi| < 1$, a soma geom√©trica $1 + \phi + \phi^2 + \phi^3 + \dots$ converge para $\frac{1}{1 - \phi}$, e a equa√ß√£o pode ser reescrita como:

$$Y_t = \sum_{i=0}^{\infty} \phi^i \epsilon_{t-i} + \frac{c}{1 - \phi}$$

Esta representa√ß√£o demonstra que $Y_t$ √© uma soma ponderada de choques de ru√≠do branco passados ($\epsilon_{t-i}$), e o processo √© estacion√°rio. Se $|\phi| \geq 1$, a soma geom√©trica diverge, e o processo n√£o √© estacion√°rio [^53].

**Lema 1** Se $|\phi| < 1$, a representa√ß√£o $Y_t = \sum_{i=0}^{\infty} \phi^i \epsilon_{t-i} + \frac{c}{1 - \phi}$ √© convergente em m√©dia quadr√°tica.

*Demonstra√ß√£o:*
Seja $S_n = \sum_{i=0}^{n} \phi^i \epsilon_{t-i}$. Devemos mostrar que $S_n$ converge para algum limite $S$ em m√©dia quadr√°tica, isto √©, $E[(S_n - S)^2] \to 0$ quando $n \to \infty$.
Considere $E[(S_n - S_m)^2]$ para $n > m$. Ent√£o,
$E[(S_n - S_m)^2] = E[(\sum_{i=m+1}^{n} \phi^i \epsilon_{t-i})^2] = \sum_{i=m+1}^{n} \phi^{2i} E[\epsilon_{t-i}^2] = \sigma^2 \sum_{i=m+1}^{n} \phi^{2i}$.
Como $|\phi| < 1$, a s√©rie $\sum_{i=0}^{\infty} \phi^{2i}$ converge para $\frac{1}{1 - \phi^2}$. Portanto, $\sum_{i=m+1}^{n} \phi^{2i} \to 0$ quando $m, n \to \infty$. Isso implica que $E[(S_n - S_m)^2] \to 0$ quando $m, n \to \infty$. Assim, a sequ√™ncia $S_n$ √© Cauchy em m√©dia quadr√°tica e, portanto, converge em m√©dia quadr√°tica para algum limite $S$. $\blacksquare$

#### M√©dia e Vari√¢ncia
Sob a condi√ß√£o de estacionariedade, a **m√©dia** do processo AR(1) √© dada por [^53, ^54]:

$$ \mu = E[Y_t] = \frac{c}{1 - \phi} $$

**Prova da M√©dia:**
I. Tomando a expectativa de ambos os lados da equa√ß√£o $Y_t = c + \phi Y_{t-1} + \epsilon_t$:
$$E[Y_t] = E[c + \phi Y_{t-1} + \epsilon_t]$$

II. Pela linearidade do operador de expectativa:
$$E[Y_t] = E[c] + E[\phi Y_{t-1}] + E[\epsilon_t]$$

III. Como $c$ e $\phi$ s√£o constantes, e $E[\epsilon_t] = 0$:
$$E[Y_t] = c + \phi E[Y_{t-1}]$$

IV. Assumindo estacionariedade, $E[Y_t] = E[Y_{t-1}] = \mu$:
$$\mu = c + \phi \mu$$

V. Resolvendo para $\mu$:
$$\mu - \phi \mu = c$$
$$\mu(1 - \phi) = c$$
$$\mu = \frac{c}{1 - \phi}$$
Portanto, a m√©dia do processo AR(1) √© $\mu = \frac{c}{1 - \phi}$. ‚ñ†

A **vari√¢ncia** do processo AR(1) √© [^53, ^54]:

$$ \gamma_0 = Var[Y_t] = \frac{\sigma^2}{1 - \phi^2} $$

Essas express√µes fornecem informa√ß√µes cruciais sobre o comportamento da s√©rie temporal.

> üí° **Exemplo Num√©rico:**
> Considere o modelo AR(1) com $c = 2$, $\phi = 0.7$ e $\sigma^2 = 4$.
> A m√©dia √©:
> $$\mu = \frac{2}{1 - 0.7} = \frac{2}{0.3} \approx 6.67$$
> A vari√¢ncia √©:
> $$\gamma_0 = \frac{4}{1 - (0.7)^2} = \frac{4}{1 - 0.49} = \frac{4}{0.51} \approx 7.84$$
> Isso significa que, em m√©dia, a s√©rie temporal tem um valor de 6.67 e a dispers√£o dos valores em torno dessa m√©dia √© de aproximadamente 7.84.
>
> Usando Python para calcular:
>
> ```python
> c = 2
> phi = 0.7
> sigma_squared = 4
>
> mu = c / (1 - phi)
> gamma_0 = sigma_squared / (1 - phi**2)
>
> print(f"M√©dia (mu): {mu:.2f}")
> print(f"Vari√¢ncia (gamma_0): {gamma_0:.2f}")
> ```
>
> O c√≥digo calcula a m√©dia e vari√¢ncia do processo AR(1) e as imprime com duas casas decimais.

**Prova da Vari√¢ncia:**
A prova √© dada a partir da equa√ß√£o:
$ Y_t - \mu = \phi(Y_{t-1} - \mu) + \epsilon_t $

I. Elevar ao quadrado ambos os lados:
$ (Y_t - \mu)^2 = (\phi(Y_{t-1} - \mu) + \epsilon_t)^2 $
$ (Y_t - \mu)^2 = \phi^2(Y_{t-1} - \mu)^2 + 2\phi(Y_{t-1} - \mu)\epsilon_t + \epsilon_t^2 $

II. Aplicar o operador de esperan√ßa em ambos os lados:
$ E[(Y_t - \mu)^2] = E[\phi^2(Y_{t-1} - \mu)^2 + 2\phi(Y_{t-1} - \mu)\epsilon_t + \epsilon_t^2] $

III. Usar a propriedade de estacionariedade $E[(Y_t - \mu)^2] = E[(Y_{t-1} - \mu)^2] = \gamma_0$, e a independ√™ncia de $\epsilon_t$ e $Y_{t-1}$, tal que $E[2\phi(Y_{t-1} - \mu)\epsilon_t] = 0$:
$ \gamma_0 = \phi^2\gamma_0 + \sigma^2 $

IV. Resolver para $\gamma_0$:
$ \gamma_0 - \phi^2\gamma_0 = \sigma^2 $
$ \gamma_0(1 - \phi^2) = \sigma^2 $
$ \gamma_0 = \frac{\sigma^2}{1 - \phi^2} $ ‚ñ†

Al√©m da m√©dia e vari√¢ncia, √© importante analisar os momentos de ordem superior para caracterizar completamente a distribui√ß√£o do processo AR(1).

**Lema 2** Para um processo AR(1) estacion√°rio, a curtose √© dada por:

$$Kurt[Y_t] = E\left[\frac{(Y_t - \mu)^4}{\gamma_0^2}\right] - 3 = \frac{6\phi^2}{1 - \phi^2}$$

*Demonstra√ß√£o:*
A curtose mede o grau de cauda pesada da distribui√ß√£o em rela√ß√£o a uma distribui√ß√£o normal. Para calcular a curtose, precisamos calcular o quarto momento central. A deriva√ß√£o completa √© complexa, mas o resultado final mostra que a curtose depende apenas de $\phi$. Quando $\phi$ se aproxima de 0, a curtose se aproxima de 0, indicando que a distribui√ß√£o se aproxima de uma distribui√ß√£o normal. Quando $\phi$ se aproxima de 1 (mantendo $|\phi|<1$ para garantir estacionariedade), a curtose tende ao infinito, indicando caudas mais pesadas do que uma distribui√ß√£o normal. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Suponha $\phi = 0.5$. Ent√£o, a curtose √©:
>
> $$ Kurt[Y_t] = \frac{6(0.5)^2}{1 - (0.5)^2} = \frac{6(0.25)}{1 - 0.25} = \frac{1.5}{0.75} = 2 $$
>
> Isso indica que a distribui√ß√£o tem caudas mais pesadas do que uma distribui√ß√£o normal (curtose de 0).
>
> ```python
> import numpy as np
>
> phi = 0.5
> kurtosis = (6 * phi**2) / (1 - phi**2)
> print(f"Curtose: {kurtosis}")
> ```

#### Fun√ß√£o de Autocorrela√ß√£o (ACF)
A **fun√ß√£o de autocorrela√ß√£o (ACF)** descreve a correla√ß√£o entre os valores da s√©rie temporal em diferentes pontos no tempo. Para o processo AR(1), a autocovari√¢ncia no lag *j* √© dada por [^54]:

$$ \gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = \phi^j \frac{\sigma^2}{1 - \phi^2} $$

**Prova da Autocovari√¢ncia:**
I. Multiplicar ambos os lados da equa√ß√£o $Y_t - \mu = \phi(Y_{t-1} - \mu) + \epsilon_t$ por $(Y_{t-j} - \mu)$:
$$(Y_t - \mu)(Y_{t-j} - \mu) = \phi(Y_{t-1} - \mu)(Y_{t-j} - \mu) + \epsilon_t(Y_{t-j} - \mu)$$

II. Tomar a esperan√ßa de ambos os lados:
$$E[(Y_t - \mu)(Y_{t-j} - \mu)] = \phi E[(Y_{t-1} - \mu)(Y_{t-j} - \mu)] + E[\epsilon_t(Y_{t-j} - \mu)]$$

III. Observar que $E[(Y_t - \mu)(Y_{t-j} - \mu)] = \gamma_j$ e, para $j > 0$, $E[\epsilon_t(Y_{t-j} - \mu)] = 0$ pois $\epsilon_t$ √© ru√≠do branco e independente de $Y_{t-j}$:
$$\gamma_j = \phi \gamma_{j-1}$$

IV. Aplicar recursivamente a rela√ß√£o $\gamma_j = \phi \gamma_{j-1}$:
$$\gamma_1 = \phi \gamma_0$$
$$\gamma_2 = \phi \gamma_1 = \phi^2 \gamma_0$$
$$\gamma_j = \phi^j \gamma_0$$

V. Substituir $\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$:
$$\gamma_j = \phi^j \frac{\sigma^2}{1 - \phi^2}$$
Portanto, a autocovari√¢ncia no lag *j* √© $\gamma_j = \phi^j \frac{\sigma^2}{1 - \phi^2}$. ‚ñ†

A fun√ß√£o de autocorrela√ß√£o (ACF) √© ent√£o calculada como [^49, ^54]:

$$ \rho_j = \frac{\gamma_j}{\gamma_0} = \phi^j $$

Essa fun√ß√£o demonstra um decaimento exponencial das autocorrela√ß√µes √† medida que o lag *j* aumenta, caracter√≠stico dos processos AR(1) [^54].

> üí° **Exemplo Num√©rico:**
> Se $\phi = 0.6$, a ACF para os primeiros lags √©:
> *   $\rho_0 = (0.6)^0 = 1$
> *   $\rho_1 = (0.6)^1 = 0.6$
> *   $\rho_2 = (0.6)^2 = 0.36$
> *   $\rho_3 = (0.6)^3 = 0.216$
> A ACF decai exponencialmente. Isso indica que a correla√ß√£o entre os valores da s√©rie temporal diminui √† medida que o lag aumenta.
>
> ```python
> import numpy as np
>
> phi = 0.6
> lags = np.arange(4)  # Lags 0 to 3
> acf_values = phi ** lags
>
> print("ACF Values:")
> for lag, acf in zip(lags, acf_values):
>     print(f"Lag {lag}: {acf:.3f}")
> ```
>
> Este c√≥digo calcula e imprime os valores da ACF para os primeiros 4 lags.

```mermaid
graph LR
    A[Lag 0] --> B(1.0)
    A --> C[Lag 1]
    C --> D(0.6)
    A --> E[Lag 2]
    E --> F(0.36)
    A --> G[Lag 3]
    G --> H(0.216)
    style B fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#f9f,stroke:#333,stroke-width:2px
    style H fill:#f9f,stroke:#333,stroke-width:2px
```

**Teorema 1** (Yule-Walker Equations for AR(1)) As equa√ß√µes de Yule-Walker para um processo AR(1) fornecem uma rela√ß√£o entre os coeficientes do modelo e as autocorrela√ß√µes.

*Demonstra√ß√£o:*
Multiplicando ambos os lados da equa√ß√£o $Y_t = c + \phi Y_{t-1} + \epsilon_t$ por $Y_{t-j} - \mu$ e tomando a esperan√ßa, obtemos:

$E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(c - \mu + \phi (Y_{t-1} - \mu) + \epsilon_t)(Y_{t-j} - \mu)]$

Para $j = 1$:
$\gamma_1 = \phi \gamma_0$
$\rho_1 = \frac{\gamma_1}{\gamma_0} = \phi$

Para $j > 1$:
$\gamma_j = \phi \gamma_{j-1}$
$\rho_j = \phi \rho_{j-1}$

Estas s√£o as equa√ß√µes de Yule-Walker para o processo AR(1). Elas relacionam diretamente o coeficiente autoregressivo $\phi$ com as autocorrela√ß√µes da s√©rie temporal. $\blacksquare$

#### Representa√ß√£o como um Processo de M√©dia M√≥vel de Ordem Infinita (MA($\infty$))
Um processo AR(1) estacion√°rio pode ser expresso como um processo de **M√©dia M√≥vel de Ordem Infinita (MA($\infty$))** [^53]:

$$ Y_t = \mu + \sum_{i=0}^{\infty} \phi^i \epsilon_{t-i} $$

**Prova da Representa√ß√£o MA($\infty$):**
I. Come√ßando com a equa√ß√£o do AR(1): $Y_t = c + \phi Y_{t-1} + \epsilon_t$.

II. Assumindo estacionariedade, podemos escrever $Y_t - \mu = \phi (Y_{t-1} - \mu) + \epsilon_t$.

III. Substituir recursivamente $Y_{t-1} - \mu = \phi (Y_{t-2} - \mu) + \epsilon_{t-1}$ na equa√ß√£o acima:
$Y_t - \mu = \phi [\phi (Y_{t-2} - \mu) + \epsilon_{t-1}] + \epsilon_t = \phi^2 (Y_{t-2} - \mu) + \phi \epsilon_{t-1} + \epsilon_t$.

IV. Continuar a substitui√ß√£o recursiva:
$Y_t - \mu = \phi^2 [\phi (Y_{t-3} - \mu) + \epsilon_{t-2}] + \phi \epsilon_{t-1} + \epsilon_t = \phi^3 (Y_{t-3} - \mu) + \phi^2 \epsilon_{t-2} + \phi \epsilon_{t-1} + \epsilon_t$.

V. Ap√≥s *n* substitui√ß√µes:
$Y_t - \mu = \phi^n (Y_{t-n} - \mu) + \sum_{i=0}^{n-1} \phi^i \epsilon_{t-i}$.

VI. Como $|\phi| < 1$, $\lim_{n \to \infty} \phi^n (Y_{t-n} - \mu) = 0$. Portanto:
$Y_t - \mu = \sum_{i=0}^{\infty} \phi^i \epsilon_{t-i}$.

VII. Finalmente:
$Y_t = \mu + \sum_{i=0}^{\infty} \phi^i \epsilon_{t-i}$. ‚ñ†

Essa representa√ß√£o revela que o valor atual da s√©rie temporal √© uma combina√ß√£o linear ponderada de todos os choques passados, com pesos que decaem exponencialmente.

> üí° **Exemplo Num√©rico:**
> Usando $\phi = 0.4$, e supondo $\mu = 5$, a representa√ß√£o MA($\infty$) seria:
> $$Y_t = 5 + \epsilon_t + 0.4\epsilon_{t-1} + 0.16\epsilon_{t-2} + 0.064\epsilon_{t-3} + ...$$
> Isso mostra que $Y_t$ √© influenciado por todos os choques passados, mas o impacto de choques mais antigos diminui rapidamente.
>
> ```python
> import numpy as np
>
> phi = 0.4
> mu = 5
>
> # Generate some random epsilon values
> n_terms = 5  # Number of MA terms to calculate
> epsilon_values = np.random.normal(0, 1, n_terms)  # Mean 0, std 1
>
> # Calculate the MA coefficients
> ma_coefficients = phi ** np.arange(n_terms)
>
> # Calculate Y_t
> Y_t = mu + np.sum(ma_coefficients * epsilon_values)
>
> print(f"Y_t (MA approximation): {Y_t:.3f}")
> ```
>
> Este c√≥digo Python calcula uma aproxima√ß√£o de $Y_t$ usando os primeiros termos da representa√ß√£o MA($\infty$).

**Corol√°rio 1.1** A representa√ß√£o MA($\infty$) √© √∫til para calcular a resposta a um choque unit√°rio no processo AR(1). Um choque unit√°rio em $\epsilon_{t-i}$ tem um impacto de $\phi^i$ em $Y_t$.

#### Rela√ß√£o com Equa√ß√µes de Diferen√ßa de Primeira Ordem

O modelo AR(1) pode ser visto como uma equa√ß√£o de diferen√ßa de primeira ordem [^53]. A an√°lise de equa√ß√µes de diferen√ßa fornece insights sobre a estabilidade e o comportamento do modelo. Em particular, a condi√ß√£o $|\phi| < 1$ garante a estabilidade da solu√ß√£o da equa√ß√£o de diferen√ßa, correspondendo √† estacionariedade do processo AR(1) [^53].

Al√©m disso, a equa√ß√£o de diferen√ßa pode ser analisada no dom√≠nio da frequ√™ncia utilizando a transformada Z.

**Lema 3** (Fun√ß√£o de Transfer√™ncia) A fun√ß√£o de transfer√™ncia do processo AR(1) √© dada por:

$$H(z) = \frac{1}{1 - \phi z^{-1}}$$

*Demonstra√ß√£o:*
Tomando a transformada Z da equa√ß√£o $Y_t = c + \phi Y_{t-1} + \epsilon_t$, e ignorando a constante *c* (j√° que estamos interessados na rela√ß√£o entre a entrada $\epsilon_t$ e a sa√≠da $Y_t$), temos:
$Y(z) = \phi z^{-1} Y(z) + \epsilon(z)$.
Rearranjando, obtemos a fun√ß√£o de transfer√™ncia $H(z) = \frac{Y(z)}{\epsilon(z)} = \frac{1}{1 - \phi z^{-1}}$. A fun√ß√£o de transfer√™ncia descreve como diferentes frequ√™ncias no sinal de entrada s√£o amplificadas ou atenuadas pelo sistema. $\blacksquare$

**Teorema 1.1** (Fun√ß√£o de Resposta ao Impulso) A fun√ß√£o de resposta ao impulso (IRF) do processo AR(1) √© dada pela sequ√™ncia $\{\phi^i\}_{i=0}^{\infty}$.

*Demonstra√ß√£o:*
A fun√ß√£o de resposta ao impulso (IRF) mede o impacto de um choque unit√°rio em $\epsilon_t$ sobre os valores futuros de $Y_t$. A partir da representa√ß√£o MA($\infty$) do processo AR(1), $Y_t = \mu + \sum_{i=0}^{\infty} \phi^i \epsilon_{t-i}$, vemos que um choque unit√°rio em $\epsilon_t$ (isto √©, $\epsilon_t = 1$) tem um impacto de 1 em $Y_t$, um impacto de $\phi$ em $Y_{t+1}$, um impacto de $\phi^2$ em $Y_{t+2}$, e assim por diante. Portanto, a IRF √© dada pela sequ√™ncia $\{\phi^i\}_{i=0}^{\infty}$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Se $\phi = 0.7$, o impacto de um choque unit√°rio em $\epsilon_t$ sobre os valores futuros de $Y_t$ seria:
>
> - $Y_t$: 1
> - $Y_{t+1}$: 0.7
> - $Y_{t+2}$: 0.49
> - $Y_{t+3}$: 0.343
>
> Isso mostra como o impacto do choque diminui ao longo do tempo.
>
> ```python
> import numpy as np
>
> phi = 0.7
> impulse_response = phi ** np.arange(5)  # Calculate for 5 periods
>
> print("Impulse Response:")
> for i, response in enumerate(impulse_response):
>     print(f"Y_t+{i}: {response:.3f}")
> ```

#### Autocovari√¢ncia e Autocorrela√ß√£o do Processo AR(1)

A autocovari√¢ncia no lag *j* √© dada por [^54]:

$$ \gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = \phi^j \gamma_0 = \phi^j \frac{\sigma^2}{1 - \phi^2} $$

Dividindo a autocovari√¢ncia pela vari√¢ncia, obtemos a fun√ß√£o de autocorrela√ß√£o (ACF) [^54]:

$$ \rho_j = \frac{\gamma_j}{\gamma_0} = \phi^j $$

Isso demonstra que a ACF de um processo AR(1) decai exponencialmente com o aumento do lag *j*.

#### Deriva√ß√£o Alternativa para os Momentos do Processo AR(1)

Podemos obter a m√©dia e vari√¢ncia do processo AR(1) diretamente da equa√ß√£o do processo, assumindo estacionariedade [^54]. Tomando a esperan√ßa de ambos os lados da equa√ß√£o $Y_t = c + \phi Y_{t-1} + \epsilon_t$, temos [^54]:

$$ E[Y_t] = E[c + \phi Y_{t-1} + \epsilon_t] $$
$$ \mu = c + \phi \mu + 0 $$
$$ \mu = \frac{c}{1 - \phi} $$

Para encontrar a vari√¢ncia, subtra√≠mos a m√©dia de ambos os lados da equa√ß√£o do processo [^54]:

$$ Y_t - \mu = \phi(Y_{t-1} - \mu) + \epsilon_t $$

Elevando ao quadrado ambos os lados e tomando a esperan√ßa [^54]:

$$ E[(Y_t - \mu)^2] = E[(\phi(Y_{t-1} - \mu) + \epsilon_t)^2] $$
$$ \gamma_0 = \phi^2 \gamma_0 + \sigma^2 $$
$$ \gamma_0 = \frac{\sigma^2}{1 - \phi^2} $$

#### Interpreta√ß√£o da Fun√ß√£o de Autocorrela√ß√£o (ACF)

A forma da ACF ($\rho_j = \phi^j$) fornece informa√ß√µes sobre a natureza da depend√™ncia temporal no processo AR(1) [^54].
*   Se $0 < \phi < 1$, a ACF decai exponencialmente para zero √† medida que *j* aumenta, indicando uma depend√™ncia positiva e decrescente dos valores passados [^54].
*   Se $-1 < \phi < 0$, a ACF alterna em sinal e decai exponencialmente em magnitude, indicando uma depend√™ncia negativa e decrescente dos valores passados [^54].
*   Se $\phi = 0$, a ACF √© zero para todos os lags diferentes de zero, indicando que o processo √© um ru√≠do branco [^53].

> üí° **Exemplo Num√©rico:**
> Para $\phi = 0.8$ (positivo): A ACF decai gradualmente de 1 para 0.8, 0.64, 0.512...
> Para $\phi = -0.5$ (negativo): A ACF alterna em sinal: 1, -0.5, 0.25, -0.125...
> Para $\phi = 0$: A ACF √© 1 no lag 0 e 0 para todos os outros lags.
> Esses diferentes valores de $\phi$ geram padr√µes distintos na ACF, permitindo identificar o tipode processo AR que melhor se ajusta aos dados.

### Fun√ß√£o de Autocorrela√ß√£o Parcial (PACF)

A Fun√ß√£o de Autocorrela√ß√£o Parcial (PACF) √© uma ferramenta estat√≠stica usada para identificar a ordem de um modelo Autorregressivo (AR). Enquanto a ACF mede a correla√ß√£o entre uma s√©rie temporal e seus lags, a PACF mede a correla√ß√£o entre a s√©rie e seus lags, removendo o efeito das autocorrela√ß√µes dos lags intermedi√°rios.

#### Interpreta√ß√£o da PACF

1.  **Ordem do Modelo AR**: A PACF ajuda a determinar a ordem *p* de um modelo AR(p). O valor de *p* corresponde ao lag onde a PACF corta para zero ou se torna insignificante.
2.  **Correla√ß√£o Direta**: A PACF mostra a correla√ß√£o direta entre um lag e o valor atual da s√©rie, ap√≥s remover as correla√ß√µes explicadas pelos lags intervenientes.

#### Exemplo de PACF

Para um processo AR(1):

*   A PACF ter√° um valor significativo apenas no lag 1.
*   Ap√≥s o lag 1, os valores da PACF ser√£o pr√≥ximos de zero.

Para um processo AR(2):

*   A PACF ter√° valores significativos nos lags 1 e 2.
*   Ap√≥s o lag 2, os valores da PACF ser√£o pr√≥ximos de zero.

#### Utilidade da PACF

A PACF √© √∫til para:

*   **Identifica√ß√£o de Modelos**: Ajudar a identificar a ordem correta de um modelo AR.
*   **Diagn√≥stico de Modelos**: Verificar se a ordem escolhida para o modelo AR √© apropriada, analisando o padr√£o de decaimento da PACF.

#### F√≥rmula Matem√°tica da PACF

A PACF no lag *k*, denotada como $\alpha_{kk}$, √© o coeficiente do √∫ltimo termo em um modelo de regress√£o AR de ordem *k*:

$$
X_t = \alpha_{k1}X_{t-1} + \alpha_{k2}X_{t-2} + \cdots + \alpha_{kk}X_{t-k} + \epsilon_t
$$

Onde:

*   $X_t$ √© o valor da s√©rie temporal no tempo *t*.
*   $\alpha_{ki}$ s√£o os coeficientes de regress√£o.
*   $\epsilon_t$ √© o termo de erro.
*   $\alpha_{kk}$ √© a PACF no lag *k*.

#### C√°lculo da PACF

O c√°lculo da PACF envolve resolver um sistema de equa√ß√µes de Yule-Walker. Para cada lag *k*, a PACF $\alpha_{kk}$ √© calculada resolvendo as seguintes equa√ß√µes:

$$
\begin{aligned}
\rho_1 &= \alpha_{k1} + \alpha_{k2}\rho_1 + \cdots + \alpha_{kk}\rho_{k-1} \\
\rho_2 &= \alpha_{k1}\rho_1 + \alpha_{k2} + \cdots + \alpha_{kk}\rho_{k-2} \\
\vdots \\
\rho_k &= \alpha_{k1}\rho_{k-1} + \alpha_{k2}\rho_{k-2} + \cdots + \alpha_{kk}
\end{aligned}
$$

Onde $\rho_i$ √© a fun√ß√£o de autocorrela√ß√£o no lag *i*.

A PACF $\alpha_{kk}$ √© ent√£o o valor de $\alpha_{kk}$ obtido ao resolver este sistema.

#### Exemplo Pr√°tico de PACF

Considere um processo AR(2) simulado:

```python
import numpy as np
import statsmodels.api as sm
from statsmodels.tsa.arima.model import ARIMA
import matplotlib.pyplot as plt

# Simula um processo AR(2)
np.random.seed(0)
n = 100
phi1 = 0.6
phi2 = 0.3
errors = np.random.normal(0, 1, n)
series = np.zeros(n)

series[0] = errors[0]
series[1] = phi1 * series[0] + errors[1]

for t in range(2, n):
    series[t] = phi1 * series[t-1] + phi2 * series[t-2] + errors[t]

# Calcula a PACF
pacf_values = sm.tsa.pacf(series, nlags=10)

# Plota a PACF
plt.figure(figsize=(10, 6))
plt.stem(range(len(pacf_values)), pacf_values, use_line_collection=True)
plt.title('Fun√ß√£o de Autocorrela√ß√£o Parcial (PACF)')
plt.xlabel('Lag')
plt.ylabel('PACF')
plt.show()
```

Neste exemplo, a PACF mostrar√° valores significativos nos lags 1 e 2, e valores pr√≥ximos de zero para lags maiores.

#### Rela√ß√£o entre ACF e PACF

A ACF e a PACF s√£o complementares na an√°lise de s√©ries temporais:

*   **ACF**: √ötil para identificar a ordem de um modelo MA (M√©dias M√≥veis).
*   **PACF**: √ötil para identificar a ordem de um modelo AR (Autorregressivo).

Ao analisar ambas as fun√ß√µes, √© poss√≠vel determinar a estrutura apropriada para um modelo ARIMA (Autorregressivo Integrado de M√©dias M√≥veis).

### Modelo de M√©dias M√≥veis (MA)

O modelo de M√©dias M√≥veis (MA) √© uma t√©cnica utilizada em an√°lise de s√©ries temporais que modela o valor presente da s√©rie como uma combina√ß√£o linear dos erros passados. Ao contr√°rio dos modelos autorregressivos (AR), que utilizam valores passados da pr√≥pria s√©rie, os modelos MA utilizam os erros (ou choques aleat√≥rios) passados para prever o valor atual.

#### Defini√ß√£o Matem√°tica do Modelo MA

Um modelo de M√©dias M√≥veis de ordem *q*, denotado MA(q), √© definido como:

$$
X_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \cdots + \theta_q \epsilon_{t-q}
$$

Onde:

*   $X_t$ √© o valor da s√©rie temporal no tempo *t*.
*   $\mu$ √© a m√©dia da s√©rie.
*   $\epsilon_t$ √© o termo de erro (ru√≠do branco) no tempo *t*.
*   $\theta_1, \theta_2, \ldots, \theta_q$ s√£o os par√¢metros do modelo MA.
*   $q$ √© a ordem do modelo, representando o n√∫mero de termos de erro passados inclu√≠dos no modelo.

#### Interpreta√ß√£o dos Componentes

*   **$\mu$ (M√©dia)**: Representa o n√≠vel m√©dio da s√©rie temporal.
*   **$\epsilon_t$ (Termo de Erro)**: √â um ru√≠do branco com m√©dia zero e vari√¢ncia constante ($\sigma^2$). Representa a parte imprevis√≠vel da s√©rie.
*   **$\theta_i$ (Par√¢metros)**: Medem a influ√™ncia dos erros passados no valor atual da s√©rie. Um valor maior de $\theta_i$ indica uma maior influ√™ncia do erro no tempo $t-i$.

#### Exemplo de Modelo MA(1)

Um modelo MA(1) √© definido como:

$$
X_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1}
$$

Neste modelo, o valor atual da s√©rie ($X_t$) √© influenciado pelo erro atual ($\epsilon_t$) e pelo erro do per√≠odo anterior ($\epsilon_{t-1}$), ponderado pelo par√¢metro $\theta_1$.

#### Exemplo de Modelo MA(2)

Um modelo MA(2) √© definido como:

$$
X_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2}
$$

Neste modelo, o valor atual da s√©rie √© influenciado pelos erros dos dois per√≠odos anteriores, ponderados pelos par√¢metros $\theta_1$ e $\theta_2$.

#### Caracter√≠sticas dos Modelos MA

1.  **Mem√≥ria Finita**: Os modelos MA t√™m uma mem√≥ria finita, o que significa que apenas os erros dos √∫ltimos *q* per√≠odos afetam o valor atual da s√©rie.
2.  **Estacionariedade**: Os modelos MA s√£o sempre estacion√°rios, desde que os par√¢metros $\theta_i$ sejam constantes ao longo do tempo.
3.  **Fun√ß√£o de Autocorrela√ß√£o (ACF)**: A ACF de um modelo MA(q) corta para zero ap√≥s o lag *q*. Isso significa que a correla√ß√£o entre $X_t$ e $X_{t-k}$ √© zero para $k > q$.

#### Exemplo Pr√°tico de Simula√ß√£o de um Modelo MA(1)

```python
import numpy as np
import matplotlib.pyplot as plt

# Par√¢metros do modelo
mu = 0       # M√©dia da s√©rie
theta = 0.6  # Coeficiente do termo de erro
sigma = 1    # Desvio padr√£o do termo de erro
n = 100      # N√∫mero de observa√ß√µes

# Gera√ß√£o dos termos de erro (ru√≠do branco)
np.random.seed(42)
epsilon = np.random.normal(0, sigma, n)

# Simula√ß√£o do modelo MA(1)
X = np.zeros(n)
X[0] = mu + epsilon[0]  # Valor inicial
for t in range(1, n):
    X[t] = mu + epsilon[t] + theta * epsilon[t-1]

# Visualiza√ß√£o da s√©rie temporal
plt.figure(figsize=(12, 6))
plt.plot(X)
plt.title('Simula√ß√£o de um Modelo MA(1)')
plt.xlabel('Tempo')
plt.ylabel('Valor')
plt.grid(True)
plt.show()
```

Neste exemplo:

*   Geramos um ru√≠do branco ($\epsilon_t$) com m√©dia zero e desvio padr√£o 1.
*   Calculamos os valores da s√©rie temporal $X_t$ usando a f√≥rmula do modelo MA(1).
*   Visualizamos a s√©rie temporal resultante.

#### Estima√ß√£o dos Par√¢metros de um Modelo MA

A estima√ß√£o dos par√¢metros ($\theta_i$) de um modelo MA √© geralmente realizada utilizando m√©todos como:

1.  **M√©todo dos Momentos**: Estima os par√¢metros igualando os momentos te√≥ricos da s√©rie aos momentos amostrais.
2.  **M√°xima Verossimilhan√ßa (MLE)**: Encontra os valores dos par√¢metros que maximizam a fun√ß√£o de verossimilhan√ßa dos dados observados.
3.  **M√≠nimos Quadrados N√£o Lineares**: Minimiza a soma dos quadrados dos erros entre os valores observados e os valores previstos pelo modelo.

O m√©todo de M√°xima Verossimilhan√ßa √© o mais comumente utilizado devido √† sua efici√™ncia e propriedades estat√≠sticas desej√°veis.

#### Utilidade dos Modelos MA

Os modelos MA s√£o √∫teis para:

*   **Modelagem de Curto Prazo**: Capturam depend√™ncias de curto prazo na s√©rie temporal.
*   **Suaviza√ß√£o de S√©ries**: Reduzem o ru√≠do ao modelar a s√©rie como uma m√©dia m√≥vel dos erros.
*   **Previs√£o**: Preveem valores futuros da s√©rie com base nos erros passados.

#### Compara√ß√£o com Modelos AR

*   **Modelos AR (Autorregressivos)**: Usam valores passados da s√©rie temporal para prever o valor atual. S√£o adequados para s√©ries com depend√™ncias de longo prazo.
*   **Modelos MA (M√©dias M√≥veis)**: Usam erros passados para prever o valor atual. S√£o adequados para s√©ries com depend√™ncias de curto prazo.

A escolha entre um modelo AR e um modelo MA depende das caracter√≠sticas espec√≠ficas da s√©rie temporal e dos objetivos da an√°lise. Em muitos casos, uma combina√ß√£o de ambos os modelos (ARIMA) pode ser a mais apropriada.

### Modelo Autorregressivo de M√©dias M√≥veis (ARMA)

O modelo Autorregressivo de M√©dias M√≥veis (ARMA) √© uma combina√ß√£o dos modelos Autorregressivo (AR) e de M√©dias M√≥veis (MA). Ele modela uma s√©rie temporal como uma fun√ß√£o tanto dos seus pr√≥prios valores passados quanto dos erros (ou choques aleat√≥rios) passados. Este modelo √© particularmente √∫til para capturar tanto as depend√™ncias de longo prazo (autorregressivas) quanto as depend√™ncias de curto prazo (m√©dias m√≥veis) em uma s√©rie temporal.

#### Defini√ß√£o Matem√°tica do Modelo ARMA

Um modelo ARMA de ordem (p, q), denotado ARMA(p, q), √© definido como:

$$
X_t = \mu + \phi_1 X_{t-1} + \phi_2 X_{t-2} + \cdots + \phi_p X_{t-p} + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \cdots + \theta_q \epsilon_{t-q}
$$

Onde:

*   $X_t$ √© o valor da s√©rie temporal no tempo *t*.
*   $\mu$ √© a m√©dia da s√©rie.
*   $\phi_1, \phi_2, \ldots, \phi_p$ s√£o os par√¢metros do modelo AR.
*   $\theta_1, \theta_2, \ldots, \theta_q$ s√£o os par√¢metros do modelo MA.
*   $\epsilon_t$ √© o termo de erro (ru√≠do branco) no tempo *t*.
*   $p$ √© a ordem do componente autorregressivo (AR).
*   $q$ √© a ordem do componente de m√©dias m√≥veis (MA).

#### Interpreta√ß√£o dos Componentes

*   **$\mu$ (M√©dia)**: Representa o n√≠vel m√©dio da s√©rie temporal.
*   **$\phi_i$ (Par√¢metros AR)**: Medem a influ√™ncia dos valores passados da s√©rie no valor atual.
*   **$\theta_i$ (Par√¢metros MA)**: Medem a influ√™ncia dos erros passados no valor atual da s√©rie.
*   **$\epsilon_t$ (Termo de Erro)**: √â um ru√≠do branco com m√©dia zero e vari√¢ncia constante ($\sigma^2$).

#### Exemplos de Modelos ARMA

1.  **Modelo ARMA(1, 1)**:
    $$
    X_t = \mu + \phi_1 X_{t-1} + \epsilon_t + \theta_1 \epsilon_{t-1}
    $$
    Este modelo combina um termo autorregressivo de primeira ordem e um termo de m√©dias m√≥veis de primeira ordem.

2.  **Modelo ARMA(2, 1)**:
    $$
    X_t = \mu + \phi_1 X_{t-1} + \phi_2 X_{t-2} + \epsilon_t + \theta_1 \epsilon_{t-1}
    $$
    Este modelo combina um termo autorregressivo de segunda ordem e um termo de m√©dias m√≥veis de primeira ordem.

#### Caracter√≠sticas dos Modelos ARMA

1.  **Flexibilidade**: Os modelos ARMA s√£o flex√≠veis e podem capturar uma ampla gama de padr√µes em s√©ries temporais.
2.  **Estacionariedade e Invertibilidade**:
    *   Para que um modelo ARMA seja estacion√°rio, as ra√≠zes do polin√¥mio AR devem estar fora do c√≠rculo unit√°rio.
    *   Para que um modelo ARMA seja invert√≠vel, as ra√≠zes do polin√¥mio MA devem estar fora do c√≠rculo unit√°rio.
3.  **Fun√ß√µes de Autocorrela√ß√£o (ACF) e Autocorrela√ß√£o Parcial (PACF)**:
    *   A ACF e a PACF de um modelo ARMA decaem gradualmente, tornando dif√≠cil identificar as ordens *p* e *q* diretamente a partir dos gr√°ficos.
    *   Geralmente, a an√°lise combinada da ACF e da PACF √© necess√°ria para identificar a estrutura apropriada do modelo.

#### Identifica√ß√£o das Ordens (p, q)

A identifica√ß√£o das ordens *p* e *q* de um modelo ARMA envolve a an√°lise das fun√ß√µes de autocorrela√ß√£o (ACF) e autocorrela√ß√£o parcial (PACF):

*   **ACF**: A ACF mostra a correla√ß√£o entre a s√©rie temporal e seus lags. Em um modelo ARMA, a ACF geralmente decai gradualmente.
*   **PACF**: A PACF mostra a correla√ß√£o entre a s√©rie temporal e seus lags, removendo o efeito das autocorrela√ß√µes dos lags intermedi√°rios. Em um modelo ARMA, a PACF tamb√©m decai gradualmente.

A tabela a seguir resume como a ACF e a PACF se comportam para modelos AR, MA e ARMA:

| Modelo | ACF                     | PACF                      |
| :----- | :---------------------- | :------------------------ |
| AR(p)  | Decaimento gradual      | Corta ap√≥s o lag *p*     |
| MA(q)  | Corta ap√≥s o lag *q*    | Decaimento gradual      |
| ARMA(p, q) | Decaimento gradual      | Decaimento gradual      |

A an√°lise da ACF e da PACF √© muitas vezes subjetiva e requer experi√™ncia. Em muitos casos, √© necess√°rio experimentar diferentes combina√ß√µes de *p* e *q* para encontrar o modelo que melhor se ajusta aos dados.

#### Exemplo Pr√°tico de Simula√ß√£o e Ajuste de um Modelo ARMA(1, 1)

```python
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.tsa.arima.model import ARIMA

# Par√¢metros do modelo ARMA(1, 1)
mu = 0       # M√©dia da s√©rie
phi = 0.6    # Coeficiente AR
theta = 0.3  # Coeficiente MA
sigma = 1    # Desvio padr√£o do termo de erro
n = 200      # N√∫mero de observa√ß√µes

# Gera√ß√£o dos termos de erro (ru√≠do branco)
np.random.seed(42)
epsilon = np.random.normal(0, sigma, n)

# Simula√ß√£o do modelo ARMA(1, 1)
X = np.zeros(n)
X[0] = mu + epsilon[0]  # Valor inicial
for t in range(1, n):
    X[t] = mu + phi * X[t-1] + epsilon[t] + theta * epsilon[t-1]

# Visualiza√ß√£o da s√©rie temporal
plt.figure(figsize=(12, 6))
plt.plot(X)
plt.title('Simula√ß√£o de um Modelo ARMA(1, 1)')
plt.xlabel('Tempo')
plt.ylabel('Valor')
plt.grid(True)
plt.show()

# Ajuste do modelo ARMA(1, 1) aos dados simulados
model = ARIMA(X, order=(1, 0, 1))  # Ordem (p, d, q) - d=0 para estacionariedade
results = model.fit()

# Imprimir os resultados do ajuste
print(results.summary())

# Diagn√≥stico dos res√≠duos
residuals = results.resid
plt.figure(figsize=(12, 6))
plt.plot(residuals)
plt.title('Res√≠duos do Modelo ARMA(1, 1)')
plt.xlabel('Tempo')
plt.ylabel('Valor')
plt.grid(True)
plt.show()

# An√°lise da ACF dos res√≠duos
acf_residuals = sm.tsa.acf(residuals, nlags=20)
plt.figure(figsize=(12, 6))
plt.stem(acf_residuals, use_line_collection=True)
plt.title('ACF dos Res√≠duos do Modelo ARMA(1, 1)')
plt.xlabel('Lag')
plt.ylabel('Autocorrela√ß√£o')
plt.grid(True)
plt.show()
```

Neste exemplo:

*   Simulamos um modelo ARMA(1, 1) com par√¢metros espec√≠ficos.
*   Ajustamos um modelo ARMA(1, 1) aos dados simulados usando a fun√ß√£o `ARIMA` do `statsmodels`.
*   Imprimimos um resumo dos resultados do ajuste, que inclui os coeficientes estimados, os erros padr√£o, e outras estat√≠sticas.
*   Analisamos os res√≠duos do modelo para verificar se eles se comportam como ru√≠do branco (ou seja, se n√£o h√° padr√µes remanescentes nos res√≠duos).

#### Estima√ß√£o dos Par√¢metros de um Modelo ARMA

A estima√ß√£o dos par√¢metros ($\phi_i$ e $\theta_i$) de um modelo ARMA √© geralmente realizada utilizando m√©todos como:

1.  **M√©todo dos Momentos**: Estima os par√¢metros igualando os momentos te√≥ricos da s√©rie aos momentos amostrais.
2.  **M√°xima Verossimilhan√ßa (MLE)**: Encontra os valores dos par√¢metros que maximizam a fun√ß√£o de verossimilhan√ßa dos dados observados.
3.  **M√≠nimos Quadrados N√£o Lineares**: Minimiza a soma dos quadrados dos erros entre os valores observados e os valores previstos pelo modelo.

O m√©todo de M√°xima Verossimilhan√ßa √© o mais comumente utilizado devido √† sua efici√™ncia e propriedades estat√≠sticas desej√°veis.

#### Utilidade dos Modelos ARMA

Os modelos ARMA s√£o √∫teis para:

*   **Modelagem de S√©ries Temporais Complexas**: Capturam tanto as depend√™ncias de longo prazo quanto as de curto prazo.
*   **Previs√£o**: Preveem valores futuros da s√©rie com base nos valores passados e nos erros passados.
*   **An√°lise de Dados Econ√¥micos e Financeiros**: Modelam e preveem vari√°veis como infla√ß√£o, taxas de juros, e pre√ßos de a√ß√µes.

### Modelo Integrado Autorregressivo de M√©dias M√≥veis (ARIMA)

O modelo Integrado Autorregressivo de M√©dias M√≥veis (ARIMA) √© uma extens√£o do modelo ARMA que inclui uma etapa de diferencia√ß√£o para tornar a s√©rie temporal estacion√°ria. Um modelo ARIMA √© caracterizado por tr√™s par√¢metros: *p*, *d*, e *q*, onde:

*   *p* √© a ordem do componente autorregressivo (AR).
*   *d* √© o grau de diferencia√ß√£o necess√°rio para tornar a s√©rie estacion√°ria.
*   *q* √© a ordem do componente de m√©dias m√≥veis (MA).

#### Defini√ß√£o Matem√°tica do Modelo ARIMA

Um modelo ARIMA de ordem (p, d, q), denotado ARIMA(p, d, q), √© aplicado a uma s√©rie temporal que foi diferenciada *d* vezes para alcan√ßar a estacionariedade. A forma geral do modelo ARIMA √©:

$$
(1 - \sum_{i=1}^{p} \phi_i L^i) (1 - L)^d X_t = (1 + \sum_{j=1}^{q} \theta_j L^j) \epsilon_t
$$

Onde:

*   $X_t$ √© o valor da s√©rie temporal no tempo *t*.
*   $\phi_1, \phi_2, \ldots, \phi_p$ s√£o os par√¢metros do componente autorregressivo (AR).
*   $\theta_1, \theta_2, \ldots, \theta_q$ s√£o os par√¢metros do componente de m√©dias m√≥veis (MA).
*   $\epsilon_t$ √© o termo de erro (ru√≠do branco) no tempo *t*.
*   $L$ √© o operador de defasagem (lag operator), tal que $L X_t = X_{t-1}$.
*   $(1 - L)^d$ representa a opera√ß√£o de diferencia√ß√£o de ordem *d*.

#### Processo de Diferencia√ß√£o

A diferencia√ß√£o √© uma t√©cnica utilizada para remover a tend√™ncia e a sazonalidade de uma s√©rie temporal, tornando-a estacion√°ria. A diferencia√ß√£o de primeira ordem √© definida como:

$$
\Delta X_t = X_t - X_{t-1}
$$

Para uma diferencia√ß√£o de ordem *d*, a s√©rie √© diferenciada *d* vezes at√© que se torne estacion√°ria.

#### Componentes do Modelo ARIMA

1.  **Componente Autorregressivo (AR)**: Utiliza os valores passados da s√©rie diferenciada para prever o valor atual. A ordem *p* determina quantos lags s√£o utilizados.
2.  **Componente Integrado (I)**: Representa o n√∫mero de vezes que a s√©rie precisa ser diferenciada para atingir a estacionariedade.
3.  **Componente de M√©dias M√≥veis (MA)**: Utiliza os erros passados (res√≠duos) para prever o valor atual. A ordem *q* determina quantos lags dos erros s√£o utilizados.

#### Passos para Implementar um Modelo ARIMA

1.  **Verificar a Estacionariedade**:
    *   Utilizar testes estat√≠sticos como o Teste de Dickey-Fuller Aumentado (ADF) para verificar se a s√©rie temporal √© estacion√°ria.
    *   Se a s√©rie n√£o for estacion√°ria, aplicar a diferencia√ß√£o at√© que se torne estacion√°ria.

2.  **Determinar as Ordens (p, d, q)**:
    *   Analisar as fun√ß√µes de autocorrela√ß√£o (ACF) e autocorrela√ß√£o parcial (PACF) para determinar as ordens *p* e *q*.
    *   A ordem *d* √© o n√∫mero de vezes que a s√©rie foi diferenciada.

3.  **Estimar os Par√¢metros do Modelo**:
    *   Utilizar m√©todos como M√°xima Verossimilhan√ßa (MLE) para estimar os par√¢metros do modelo AR e MA.

4.  **Verificar a Validade do Modelo**:
    *   Analisar os res√≠duos do modelo para garantir que eles se comportem como ru√≠do branco (ou seja, que n√£o haja padr√µes remanescentes nos res√≠duos).
    *   Utilizar crit√©rios de informa√ß√£o como o AIC (Akaike Information Criterion) ou o BIC (Bayesian Information Criterion) para comparar diferentes modelos e selecionar o melhor.

#### Exemplo Pr√°tico de Implementa√ß√£o de um Modelo ARIMA

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller

# Gera√ß√£o de uma s√©rie temporal n√£o estacion√°ria (exemplo)
np.random.seed(42)
n = 200
X = np.cumsum(np.random.normal(0, 1, n))  # Caminhada Aleat√≥ria

# Teste de estacionariedade (ADF)
result = adfuller(X)
print('Teste ADF:')
print('Estat√≠stica ADF: %f' % result[0])
print('Valor-p: %f' % result[1])
print('Valores cr√≠ticos:')
for key, value in result[4].items():
    print('\t%s: %.3f' % (key, value))

# Diferencia√ß√£o para tornar a s√©rie estacion√°ria
X_diff = np.diff(X)

# Novo teste de estacionariedade ap√≥s diferencia√ß√£o
result_diff = adfuller(X_diff)
print('\nTeste ADF ap√≥s diferencia√ß√£o:')
print('Estat√≠stica ADF: %f' % result_diff[0])
print('Valor-p: %f' % result_diff[1])
print('Valores cr√≠ticos:')
for key, value in result_diff[4].items():
    print('\t%s: %.3f' % (key, value))

# An√°lise das fun√ß√µes de autocorrela√ß√£o (ACF) e autocorrela√ß√£o parcial (PACF)
fig, axes = plt.subplots(1, 2, figsize=(12, 4))
sm.graphics.tsa.plot_acf(X_diff, lags=20, ax=axes[0])
sm.graphics.tsa.plot_pacf(X_diff, lags=20, ax=axes[1])
plt.show()

# Ajuste do modelo ARIMA
# Exemplo: ARIMA(1, 1, 1) - p=1, d=1, q=1
model = ARIMA(X, order=(1, 1, 1))
results = model.fit()

# Imprimir os resultados do ajuste
print(results.summary())

# An√°lise dos res√≠duos
residuals = results.resid
plt.figure(figsize=(12, 6))
plt.plot(residuals)
plt.title('Res√≠duos do Modelo ARIMA(1, 1, 1)')
plt.xlabel('Tempo')
plt.ylabel('Valor')
plt.grid(True)
plt.show()

# Previs√µes
predictions = results.predict(start=n-30, end=n-1)  # Previs√£o dos √∫ltimos 30 pontos
plt.figure(figsize=(12, 6))
plt.plot(X[n-30:], label='Real')
plt.plot(predictions, color='red', label='Previs√µes')
plt.title('Previs√µes do Modelo ARIMA(1, 1, 1)')
plt.xlabel('Tempo')
plt.ylabel('Valor')
plt.legend()
plt.grid(True)
plt.show()
```

Neste exemplo:

1.  **Gera√ß√£o de Dados**: Criamos uma s√©rie temporal n√£o estacion√°ria (caminhada aleat√≥ria).
2.  **Teste de Estacionariedade**: Utilizamos o Teste ADF para verificar se a s√©rie √© estacion√°ria.
3.  **Diferencia√ß√£o**: Aplicamos a diferencia√ß√£o para tornar a s√©rie estacion√°ria e verificamos novamente com o Teste ADF.
4.  **An√°lise ACF e PACF**: Plotamos as fun√ß√µes de autocorrela√ß√£o para ajudar a determinar as ordens *p* e *q*.
5.  **Ajuste do Modelo**: Ajustamos um modelo ARIMA(1, 1, 1) aos dados.
6.  **An√°lise dos Res√≠duos**: Verificamos se os res√≠duos se comportam como ru√≠do branco.
7.  **Previs√µes**: Geramos previs√µes e comparamos com os valores reais.

#### Crit√©rios de Informa√ß√£o (AIC e BIC)

Os crit√©rios de informa√ß√£o AIC (Akaike Information Criterion) e BIC (Bayesian Information Criterion) s√£o utilizados para comparar diferentes modelos ARIMA e selecionar o que melhor equilibra a complexidade do modelo e o ajuste aos dados.

*   **AIC**:
    $$
    AIC = -2 \log(L) + 2k
    $$
    Onde *L* √© a verossimilhan√ßa m√°xima do modelo e *k* √© o n√∫mero de par√¢metros no modelo.

*   **BIC**:
    $$
    BIC = -2 \log(L) + k \log(n)
    $$
    Onde *L* √© a verossimilhan√ßa m√°xima do modelo, *k* √© o n√∫mero de par√¢metros no modelo, e *n* √© o n√∫mero de observa√ß√µes.

Modelos com menores valores de AIC ou BIC s√£o prefer√≠veis.

#### Utilidade dos Modelos ARIMA

Os modelos ARIMA s√£o amplamente utilizados para:

*   **Previs√£o de S√©ries Temporais**: Preveem valores futuros com base nos padr√µes hist√≥ricos da s√©rie.
*   **An√°lise de Dados Econ√¥micos e Financeiros**: Modelam e preveem vari√°veis como infla√ß√£o, taxas de juros, pre√ßos de a√ß√µes, etc.
*   **Controle de Processos Industriais**: Otimizam e controlam processos com base na an√°lise de s√©ries temporais.

### Modelo Sazonal ARIMA (SARIMA)

O Modelo Sazonal ARIMA (SARIMA) √© uma extens√£o do modelo ARIMA projetada para lidar com s√©ries temporais que exibem padr√µes sazonais. A sazonalidade refere-se a padr√µes regulares e previs√≠veis que se repetem ao longo de um per√≠odo fixo de tempo (por exemplo, trimestral, mensal, semanal). O SARIMA √© particularmente √∫til para modelar dados onde a sazonalidade desempenha um papel significativo, como vendas de varejo, turismo e dados clim√°ticos.

#### Defini√ß√£o Matem√°tica do Modelo SARIMA

Um modelo SARIMA √© caracterizado por dois conjuntos de par√¢metros: os par√¢metros n√£o sazonais (p, d, q) e os par√¢metros sazonais (P, D, Q, s), onde:

*   (p, d, q) s√£o as ordens do componente n√£o sazonal do modelo:
    *   *p* √© a ordem do componente autorregressivo (AR).
    *   *d* √© o grau de diferencia√ß√£o n√£o sazonal.
    *   *q* √© a ordem do componente de m√©dias m√≥veis (MA).
*   (P, D, Q, s) s√£o as ordens do componente sazonal do modelo:
    *   *P* √© a ordem do componente autorregressivo sazonal (SAR).
    *   *D* √© o grau de diferencia√ß√£o sazonal.
    *   *Q* √© a ordem do componente de m√©dias m√≥veis sazonal (SMA).
    *   *s* √© o per√≠odo da sazonalidade (por exemplo, 12 para dados mensais com sazonalidade anual).

A forma geral do modelo SARIMA √©:

$$
\Phi(L^s) \phi(L) (1 - L^s)^D (1 - L)^d X_t = \Theta(L^s) \theta(L) \epsilon_t
$$

Onde:

*   $X_t$ √© o valor da s√©rie temporal no tempo *t*.
*   $\phi(L)$ e $\theta(L)$ s√£o os polin√¥mios AR e MA n√£o sazonais, respectivamente.
*   $\Phi(L^s)$ e $\Theta(L^s)$ s√£o os polin√¥mios SAR e SMA sazonais, respectivamente.
*   $L$ √© o operador de defasagem (lag operator), tal que $L X_t = X_{t-1}$.
*   $(1 - L^s)^D$ representa a opera√ß√£o de diferencia√ß√£o sazonal de ordem *D*.
*   $(1 - L)^d$ representa a opera√ß√£o de diferencia√ß√£o n√£o sazonal de ordem *d*.
*   $\epsilon_t$ √© o termo de erro (ru√≠do branco) no tempo *t*.

#### Componentes do Modelo SARIMA

1.  **Componentes N√£o Sazonais (p, d, q)**:
    *   **AR (Autorregressivo)**: Utiliza os valores passados da s√©rie diferenciada n√£o sazonalmente para prever o valor atual.
    *   **I (Integrado)**: Representa o n√∫mero de vezes que a s√©rie precisa ser diferenciada n√£o sazonalmente para atingir a estacionariedade.
    *   **MA (M√©dias M√≥veis)**: Utiliza os erros passados (res√≠duos) n√£o sazonais para prever o valor atual.

2.  **Componentes Sazonais (P, D, Q, s)**:
    *   **SAR (Autorregressivo Sazonal)**: Utiliza os valores passados sazonalmente da s√©rie diferenciada sazonalmente para prever o valor atual.
    *   **D (Diferencia√ß√£o Sazonal)**: Representa o n√∫mero de vezes que a s√©rie precisa ser diferenciada sazonalmente para atingir a estacionariedade sazonal.
    *   **SMA (M√©dias M√≥veis Sazonal)**: Utiliza os erros passados sazonalmente para prever o valor atual.
    *   **s (Per√≠odo Sazonal)**: Define o n√∫mero de per√≠odos em uma temporada (por exemplo, 12 meses em um ano).

#### Passos para Implementar um Modelo SARIMA

1.  **An√°lise Explorat√≥ria**:
    *   Visualizar a s√©rie temporal para identificar a presen√ßa de sazonalidade e tend√™ncia.
    *   Decompor a s√©rie temporal em seus componentes de tend√™ncia, sazonalidade e res√≠duos usando t√©cnicas como a decomposi√ß√£o sazonal.

2.  **Verificar a Estacionariedade**:
    *   Utilizar testes estat√≠sticos como o Teste de Dickey-Fuller Aumentado (ADF) para verificar se a s√©rie temporal √© estacion√°ria.
    *   Se a s√©rie n√£o for estacion√°ria, aplicar a diferencia√ß√£o n√£o sazonal e/ou sazonal at√© que se torne estacion√°ria.

3.  **Determinar as Ordens (p, d, q) e (P, D, Q, s)**:
    *   Analisar as fun√ß√µes de autocorrela√ß√£o (ACF) e autocorrela√ß√£o parcial (PACF) para determinar as ordens das componentes AR e MA.
    *   A ordem de diferencia√ß√£o (*d*) √© determinada pelo n√∫mero de vezes que a s√©rie precisa ser diferenciada para se tornar estacion√°ria. A ordem de diferencia√ß√£o sazonal (*D*) segue o mesmo princ√≠pio, mas para a componente sazonal.

4.  **Estima√ß√£o dos Par√¢metros**:
    *   Utilizar m√©todos de estima√ß√£o, como m√°xima verossimilhan√ßa, para estimar os coeficientes das componentes AR e MA.

5.  **Verifica√ß√£o Diagn√≥stica**:
    *   Analisar os res√≠duos do modelo para garantir que eles se comportem como ru√≠do branco (ou seja, n√£o apresentem padr√µes de autocorrela√ß√£o).
    *   Utilizar testes estat√≠sticos, como o teste de Ljung-Box, para verificar a aleatoriedade dos res√≠duos.

6.  **Previs√£o**:
    *   Utilizar o modelo estimado para fazer previs√µes futuras, com intervalos de confian√ßa.

### Exemplo Pr√°tico em Python

Utilizando a biblioteca `statsmodels` em Python para ajustar um modelo SARIMA a uma s√©rie temporal.

```python
import pandas as pd
import numpy as np
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import matplotlib.pyplot as plt

# Gerar dados de exemplo (s√©rie temporal com componente sazonal)
np.random.seed(0)
n = 200
t = np.arange(n)
sazonal = np.sin(2 * np.pi * t / 24)  # Sazonalidade com per√≠odo 24
ruido = np.random.randn(n)
data = 50 + 2 * t + sazonal + ruido

# Criar um DataFrame pandas
df = pd.DataFrame({'Data': data})

# Plotar a s√©rie temporal
plt.figure(figsize=(12, 6))
plt.plot(df['Data'])
plt.title('S√©rie Temporal Sint√©tica')
plt.xlabel('Tempo')
plt.ylabel('Valor')
plt.show()

# Analisar ACF e PACF para identificar ordens
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
plot_acf(df['Data'], lags=40, ax=ax1)
plot_pacf(df['Data'], lags=40, ax=ax2)
plt.show()

# Ajustar o modelo SARIMA
# Supondo que as an√°lises ACF e PACF sugerem SARIMA(1, 1, 1)(1, 1, 1, 24)
model = SARIMAX(df['Data'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 24))
model_fit = model.fit()

# Imprimir o resumo do modelo
print(model_fit.summary())

# Diagn√≥stico do modelo: An√°lise dos res√≠duos
residuals = model_fit.resid
plt.figure(figsize=(12, 6))
plt.plot(residuals)
plt.title('Res√≠duos do Modelo SARIMA')
plt.xlabel('Tempo')
plt.ylabel('Valor')
plt.show()

# Plotar ACF dos res√≠duos
plot_acf(residuals, lags=40)
plt.title('ACF dos Res√≠duos')
plt.show()

# Fazer previs√µes
previsoes = model_fit.get_forecast(steps=30)
previsoes_medias = previsoes.predicted_mean
intervalos_confianca = previsoes.conf_int()

# Plotar as previs√µes com intervalos de confian√ßa
plt.figure(figsize=(12, 6))
plt.plot(df['Data'], label='Observado')
plt.plot(np.arange(len(df['Data']), len(df['Data']) + 30), previsoes_medias, label='Previs√µes')
plt.fill_between(np.arange(len(df['Data']), len(df['Data']) + 30),
                 intervalos_confianca[:, 0], intervalos_confianca[:, 1], color='gray', alpha=0.2, label='Intervalo de Confian√ßa')
plt.title('Previs√µes SARIMA')
plt.xlabel('Tempo')
plt.ylabel('Valor')
plt.legend()
plt.show()
```

### Considera√ß√µes Finais

*   **Escolha do Modelo**: A escolha do modelo SARIMA correto √© crucial e depende da an√°lise cuidadosa dos dados e das fun√ß√µes ACF e PACF.
*   **Valida√ß√£o**: Sempre validar o modelo com dados fora da amostra para garantir que ele generalize bem.
*   **Interpreta√ß√£o**: Interpretar os resultados do modelo no contexto do problema de neg√≥cios para fornecer insights acion√°veis.

### Diagrama de Fluxo do Processo SARIMA

```mermaid
graph LR
    A[In√≠cio: S√©rie Temporal] --> B(Teste de Estacionariedade);
    B -- N√£o Estacion√°ria --> C{Diferencia√ß√£o (d, D)};
    C --> B;
    B -- Estacion√°ria --> D{An√°lise ACF/PACF};
    D --> E{Determinar Ordens (p, q, P, Q, s)};
    E --> F(Estima√ß√£o dos Par√¢metros);
    F --> G{Verifica√ß√£o Diagn√≥stica};
    G -- Res√≠duos OK --> H(Previs√£o);
    G -- Res√≠duos N√£o OK --> D;
    H --> I[Fim: Previs√µes e Insights];
```

### Mapa Mental do Modelo SARIMA

```mermaid
mindmap
  root((SARIMA))
    (Componentes)
      AR((AutoRegressivo))
      MA((M√©dia M√≥vel))
      I((Integrado))
      Sazonal((Sazonalidade))
    (Processo)
      Estacionariedade((Estacionariedade))
      ACF_PACF((An√°lise ACF/PACF))
      Estimacao((Estima√ß√£o))
      Diagnostico((Diagn√≥stico))
      Previsao((Previs√£o))
    (Considera√ß√µes)
      Validacao((Valida√ß√£o))
      Interpretacao((Interpreta√ß√£o))
```

### Conclus√£o

O modelo SARIMA √© uma ferramenta poderosa para an√°lise e previs√£o de s√©ries temporais com componentes sazonais. Ao entender e aplicar corretamente este modelo, √© poss√≠vel obter insights valiosos e tomar decis√µes informadas em uma variedade de contextos.
<!-- END -->