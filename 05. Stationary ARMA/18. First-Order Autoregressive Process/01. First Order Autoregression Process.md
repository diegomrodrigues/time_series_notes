## Aprofundamento no Processo Autoregressivo de Primeira Ordem (AR(1))

### Introdu√ß√£o
Este cap√≠tulo visa aprofundar o entendimento sobre os processos **Autorregressivos de Primeira Ordem (AR(1))**, um dos modelos fundamentais em an√°lise de s√©ries temporais. Construindo sobre os conceitos de **estacionariedade**, **autocovari√¢ncia** e **fun√ß√µes de autocorrela√ß√£o** previamente introduzidos [^45, ^46], exploraremos em detalhes as propriedades, condi√ß√µes de estacionariedade, representa√ß√µes alternativas e aplica√ß√µes do modelo AR(1). O objetivo √© fornecer uma base te√≥rica s√≥lida para acad√™micos com conhecimento avan√ßado em matem√°tica e estat√≠stica, permitindo uma an√°lise cr√≠tica e aplica√ß√£o eficaz desses modelos em contextos complexos.

### Conceitos Fundamentais

O modelo **AR(1)** √© definido pela seguinte equa√ß√£o [^53]:

$$ Y_t = c + \phi Y_{t-1} + \epsilon_t $$

onde:
*   $Y_t$ representa o valor da s√©rie temporal no instante *t*.
*   *c* √© uma constante.
*   $\phi$ √© o *coeficiente autoregressivo* de primeira ordem.
*   $\epsilon_t$ √© um termo de *ru√≠do branco* com m√©dia zero e vari√¢ncia constante $\sigma^2$ [^47]. Especificamente, $E(\epsilon_t) = 0$ [^47] e $E(\epsilon_t \epsilon_\tau) = 0$ para $t \neq \tau$ [^48].

Este modelo expressa que o valor atual da s√©rie temporal √© uma fun√ß√£o linear do seu valor anterior, acrescido de um choque aleat√≥rio. Essa depend√™ncia linear de um per√≠odo anterior torna o modelo **AR(1)** o bloco de constru√ß√£o essencial para modelos mais complexos de s√©ries temporais.

> üí° **Exemplo Num√©rico:**
> Suponha que temos uma s√©rie temporal representando o pre√ßo de uma a√ß√£o. Podemos modelar o pre√ßo di√°rio da a√ß√£o usando um modelo AR(1):
> $$Y_t = 0.1 + 0.8Y_{t-1} + \epsilon_t$$
> Aqui, $c = 0.1$ (uma constante), $\phi = 0.8$ (o coeficiente autoregressivo), e $\epsilon_t$ √© o ru√≠do branco com $\sigma^2 = 0.05$. Se $Y_{t-1} = 10$, ent√£o
> $$Y_t = 0.1 + 0.8(10) + \epsilon_t = 8.1 + \epsilon_t$$
> Se $\epsilon_t$ √© um valor aleat√≥rio sorteado de uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia 0.05 (por exemplo, $\epsilon_t = 0.2$), ent√£o $Y_t = 8.3$. Isso significa que o pre√ßo da a√ß√£o no dia *t* √© influenciado pelo pre√ßo do dia anterior (com peso 0.8) mais uma pequena perturba√ß√£o aleat√≥ria.

#### Condi√ß√µes de Estacionariedade
Uma propriedade crucial para a aplicabilidade do modelo **AR(1)** √© a **estacionariedade**. Conforme mencionado anteriormente [^45], um processo √© *covariance-stationary* se sua m√©dia e autocovari√¢ncias n√£o dependem do tempo *t*. No contexto do modelo AR(1), a estacionariedade √© garantida se [^53]:

$$ |\phi| < 1 $$

Esta condi√ß√£o assegura que o impacto de choques passados diminua exponencialmente ao longo do tempo, evitando que a s√©rie temporal divirja. Quando $|\phi| \geq 1$, o processo torna-se n√£o estacion√°rio, e a vari√¢ncia tende ao infinito, invalidando a aplica√ß√£o direta do modelo AR(1) [^53].

> üí° **Exemplo Num√©rico:**
> Se $\phi = 0.5$, a condi√ß√£o de estacionariedade √© satisfeita, pois $|0.5| < 1$. Se $\phi = 1.2$, a condi√ß√£o n√£o √© satisfeita, e o processo AR(1) √© n√£o estacion√°rio. Visualmente, uma s√©rie temporal com $\phi = 0.5$ exibir√° flutua√ß√µes em torno de uma m√©dia constante, enquanto uma s√©rie com $\phi = 1.2$ mostrar√° uma tend√™ncia crescente ou decrescente sem retornar a um n√≠vel m√©dio.

**Proposi√ß√£o 1** A condi√ß√£o de estacionariedade $|\phi|<1$ implica que o processo AR(1) √© *erg√≥dico para a m√©dia*.

*Demonstra√ß√£o:*
Para um processo ser erg√≥dico para a m√©dia, a m√©dia amostral deve convergir para a m√©dia populacional √† medida que o tamanho da amostra aumenta. Dado que o processo AR(1) √© estacion√°rio quando $|\phi|<1$, sua m√©dia e vari√¢ncia s√£o constantes ao longo do tempo. A m√©dia amostral √© dada por $\bar{Y} = \frac{1}{T}\sum_{t=1}^{T} Y_t$. Sob estacionariedade e a exist√™ncia de momentos de ordem superior finitos (que decorrem das propriedades do ru√≠do branco $\epsilon_t$ e da condi√ß√£o $|\phi|<1$), a Lei Forte dos Grandes N√∫meros implica que $\bar{Y}$ converge para $E[Y_t] = \mu = \frac{c}{1-\phi}$ quando $T \to \infty$. Portanto, o processo AR(1) √© erg√≥dico para a m√©dia quando $|\phi|<1$.

Para entender o porqu√™ da condi√ß√£o $|\phi| < 1$ ser necess√°ria para a estacionariedade, podemos reescrever o processo **AR(1)** utilizando substitui√ß√µes sucessivas:

$Y_t = c + \phi Y_{t-1} + \epsilon_t$
$Y_t = c + \phi(c + \phi Y_{t-2} + \epsilon_{t-1}) + \epsilon_t = c + c\phi + \phi^2 Y_{t-2} + \phi \epsilon_{t-1} + \epsilon_t$
$Y_t = c + c\phi + c\phi^2 + \phi^3 Y_{t-3} + \phi^2 \epsilon_{t-2} + \phi \epsilon_{t-1} + \epsilon_t$

Continuando este processo iterativamente:

$Y_t = \epsilon_t + \phi \epsilon_{t-1} + \phi^2 \epsilon_{t-2} + \phi^3 \epsilon_{t-3} + \dots + c(1 + \phi + \phi^2 + \phi^3 + \dots)$

Se $|\phi| < 1$, a soma geom√©trica $1 + \phi + \phi^2 + \phi^3 + \dots$ converge para $\frac{1}{1 - \phi}$, e a equa√ß√£o pode ser reescrita como:

$$Y_t = \sum_{i=0}^{\infty} \phi^i \epsilon_{t-i} + \frac{c}{1 - \phi}$$

Esta representa√ß√£o demonstra que $Y_t$ √© uma soma ponderada de choques de ru√≠do branco passados ($\epsilon_{t-i}$), e o processo √© estacion√°rio. Se $|\phi| \geq 1$, a soma geom√©trica diverge, e o processo n√£o √© estacion√°rio [^53].

**Lema 1** Se $|\phi| < 1$, a representa√ß√£o $Y_t = \sum_{i=0}^{\infty} \phi^i \epsilon_{t-i} + \frac{c}{1 - \phi}$ √© convergente em m√©dia quadr√°tica.

*Demonstra√ß√£o:*
Seja $S_n = \sum_{i=0}^{n} \phi^i \epsilon_{t-i}$. Devemos mostrar que $S_n$ converge para algum limite $S$ em m√©dia quadr√°tica, isto √©, $E[(S_n - S)^2] \to 0$ quando $n \to \infty$.
Considere $E[(S_n - S_m)^2]$ para $n > m$. Ent√£o,
$E[(S_n - S_m)^2] = E[(\sum_{i=m+1}^{n} \phi^i \epsilon_{t-i})^2] = \sum_{i=m+1}^{n} \phi^{2i} E[\epsilon_{t-i}^2] = \sigma^2 \sum_{i=m+1}^{n} \phi^{2i}$.
Como $|\phi| < 1$, a s√©rie $\sum_{i=0}^{\infty} \phi^{2i}$ converge para $\frac{1}{1 - \phi^2}$. Portanto, $\sum_{i=m+1}^{n} \phi^{2i} \to 0$ quando $m, n \to \infty$. Isso implica que $E[(S_n - S_m)^2] \to 0$ quando $m, n \to \infty$. Assim, a sequ√™ncia $S_n$ √© Cauchy em m√©dia quadr√°tica e, portanto, converge em m√©dia quadr√°tica para algum limite $S$.

#### M√©dia e Vari√¢ncia
Sob a condi√ß√£o de estacionariedade, a **m√©dia** do processo AR(1) √© dada por [^53]:

$$ \mu = E[Y_t] = \frac{c}{1 - \phi} $$

A **vari√¢ncia** do processo AR(1) √© [^53]:

$$ \gamma_0 = Var[Y_t] = \frac{\sigma^2}{1 - \phi^2} $$

Essas express√µes fornecem informa√ß√µes cruciais sobre o comportamento da s√©rie temporal.

> üí° **Exemplo Num√©rico:**
> Considere o modelo AR(1) com $c = 2$, $\phi = 0.7$ e $\sigma^2 = 4$.
> A m√©dia √©:
> $$\mu = \frac{2}{1 - 0.7} = \frac{2}{0.3} \approx 6.67$$
> A vari√¢ncia √©:
> $$\gamma_0 = \frac{4}{1 - (0.7)^2} = \frac{4}{1 - 0.49} = \frac{4}{0.51} \approx 7.84$$
> Isso significa que, em m√©dia, a s√©rie temporal tem um valor de 6.67 e a dispers√£o dos valores em torno dessa m√©dia √© de aproximadamente 7.84.

**Prova da Vari√¢ncia:**
A prova √© dada a partir da equa√ß√£o:
$ Y_t - \mu = \phi(Y_{t-1} - \mu) + \epsilon_t $

I. Elevar ao quadrado ambos os lados:
$ (Y_t - \mu)^2 = (\phi(Y_{t-1} - \mu) + \epsilon_t)^2 $
$ (Y_t - \mu)^2 = \phi^2(Y_{t-1} - \mu)^2 + 2\phi(Y_{t-1} - \mu)\epsilon_t + \epsilon_t^2 $

II. Aplicar o operador de esperan√ßa em ambos os lados:
$ E[(Y_t - \mu)^2] = E[\phi^2(Y_{t-1} - \mu)^2 + 2\phi(Y_{t-1} - \mu)\epsilon_t + \epsilon_t^2] $

III. Usar a propriedade de estacionariedade $E[(Y_t - \mu)^2] = E[(Y_{t-1} - \mu)^2] = \gamma_0$, e a independ√™ncia de $\epsilon_t$ e $Y_{t-1}$, tal que $E[2\phi(Y_{t-1} - \mu)\epsilon_t] = 0$:
$ \gamma_0 = \phi^2\gamma_0 + \sigma^2 $

IV. Resolver para $\gamma_0$:
$ \gamma_0 - \phi^2\gamma_0 = \sigma^2 $
$ \gamma_0(1 - \phi^2) = \sigma^2 $
$ \gamma_0 = \frac{\sigma^2}{1 - \phi^2} $ ‚ñ†

#### Fun√ß√£o de Autocorrela√ß√£o (ACF)
A **fun√ß√£o de autocorrela√ß√£o (ACF)** descreve a correla√ß√£o entre os valores da s√©rie temporal em diferentes pontos no tempo. Para o processo AR(1), a autocovari√¢ncia no lag *j* √© dada por [^53]:

$$ \gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = \frac{\phi^j}{1 - \phi^2} \sigma^2 $$

A fun√ß√£o de autocorrela√ß√£o (ACF) √© ent√£o calculada como [^49]:

$$ \rho_j = \frac{\gamma_j}{\gamma_0} = \phi^j $$

Essa fun√ß√£o demonstra um decaimento exponencial das autocorrela√ß√µes √† medida que o lag *j* aumenta, caracter√≠stico dos processos AR(1) [^54].

> üí° **Exemplo Num√©rico:**
> Se $\phi = 0.6$, a ACF para os primeiros lags √©:
> *   $\rho_0 = (0.6)^0 = 1$
> *   $\rho_1 = (0.6)^1 = 0.6$
> *   $\rho_2 = (0.6)^2 = 0.36$
> *   $\rho_3 = (0.6)^3 = 0.216$
> A ACF decai exponencialmente. Isso indica que a correla√ß√£o entre os valores da s√©rie temporal diminui √† medida que o lag aumenta.

```mermaid
graph LR
    A[Lag 0] --> B(1.0)
    A --> C[Lag 1]
    C --> D(0.6)
    A --> E[Lag 2]
    E --> F(0.36)
    A --> G[Lag 3]
    G --> H(0.216)
    style B fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#f9f,stroke:#333,stroke-width:2px
    style H fill:#f9f,stroke:#333,stroke-width:2px
```

**Teorema 1** (Yule-Walker Equations for AR(1)) As equa√ß√µes de Yule-Walker para um processo AR(1) fornecem uma rela√ß√£o entre os coeficientes do modelo e as autocorrela√ß√µes.

*Demonstra√ß√£o:*
Multiplicando ambos os lados da equa√ß√£o $Y_t = c + \phi Y_{t-1} + \epsilon_t$ por $Y_{t-j} - \mu$ e tomando a esperan√ßa, obtemos:

$E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(c - \mu + \phi (Y_{t-1} - \mu) + \epsilon_t)(Y_{t-j} - \mu)]$

Para $j = 1$:
$\gamma_1 = \phi \gamma_0$
$\rho_1 = \frac{\gamma_1}{\gamma_0} = \phi$

Para $j > 1$:
$\gamma_j = \phi \gamma_{j-1}$
$\rho_j = \phi \rho_{j-1}$

Estas s√£o as equa√ß√µes de Yule-Walker para o processo AR(1). Elas relacionam diretamente o coeficiente autoregressivo $\phi$ com as autocorrela√ß√µes da s√©rie temporal.

#### Representa√ß√£o como um Processo de M√©dia M√≥vel de Ordem Infinita (MA($\infty$))
Um processo AR(1) estacion√°rio pode ser expresso como um processo de **M√©dia M√≥vel de Ordem Infinita (MA($\infty$))** [^53]:

$$ Y_t = \mu + \sum_{i=0}^{\infty} \phi^i \epsilon_{t-i} $$

Essa representa√ß√£o revela que o valor atual da s√©rie temporal √© uma combina√ß√£o linear ponderada de todos os choques passados, com pesos que decaem exponencialmente.

> üí° **Exemplo Num√©rico:**
> Usando $\phi = 0.4$, e supondo $\mu = 5$, a representa√ß√£o MA($\infty$) seria:
> $$Y_t = 5 + \epsilon_t + 0.4\epsilon_{t-1} + 0.16\epsilon_{t-2} + 0.064\epsilon_{t-3} + ...$$
> Isso mostra que $Y_t$ √© influenciado por todos os choques passados, mas o impacto de choques mais antigos diminui rapidamente.

**Corol√°rio 1.1** A representa√ß√£o MA($\infty$) √© √∫til para calcular a resposta a um choque unit√°rio no processo AR(1). Um choque unit√°rio em $\epsilon_{t-i}$ tem um impacto de $\phi^i$ em $Y_t$.

#### Rela√ß√£o com Equa√ß√µes de Diferen√ßa de Primeira Ordem

O modelo AR(1) pode ser visto como uma equa√ß√£o de diferen√ßa de primeira ordem [^53]. A an√°lise de equa√ß√µes de diferen√ßa fornece insights sobre a estabilidade e o comportamento do modelo. Em particular, a condi√ß√£o $|\phi| < 1$ garante a estabilidade da solu√ß√£o da equa√ß√£o de diferen√ßa, correspondendo √† estacionariedade do processo AR(1) [^53].

#### Autocovari√¢ncia e Autocorrela√ß√£o do Processo AR(1)

A autocovari√¢ncia no lag *j* √© dada por [^53]:

$$ \gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = \phi^j \gamma_0 = \phi^j \frac{\sigma^2}{1 - \phi^2} $$

Dividindo a autocovari√¢ncia pela vari√¢ncia, obtemos a fun√ß√£o de autocorrela√ß√£o (ACF) [^54]:

$$ \rho_j = \frac{\gamma_j}{\gamma_0} = \phi^j $$

Isso demonstra que a ACF de um processo AR(1) decai exponencialmente com o aumento do lag *j*.

#### Deriva√ß√£o Alternativa para os Momentos do Processo AR(1)

Podemos obter a m√©dia e vari√¢ncia do processo AR(1) diretamente da equa√ß√£o do processo, assumindo estacionariedade [^54]. Tomando a esperan√ßa de ambos os lados da equa√ß√£o $Y_t = c + \phi Y_{t-1} + \epsilon_t$, temos [^54]:

$$ E[Y_t] = E[c + \phi Y_{t-1} + \epsilon_t] $$
$$ \mu = c + \phi \mu + 0 $$
$$ \mu = \frac{c}{1 - \phi} $$

Para encontrar a vari√¢ncia, subtra√≠mos a m√©dia de ambos os lados da equa√ß√£o do processo [^54]:

$$ Y_t - \mu = \phi(Y_{t-1} - \mu) + \epsilon_t $$

Elevando ao quadrado ambos os lados e tomando a esperan√ßa [^54]:

$$ E[(Y_t - \mu)^2] = E[(\phi(Y_{t-1} - \mu) + \epsilon_t)^2] $$
$$ \gamma_0 = \phi^2 \gamma_0 + \sigma^2 $$
$$ \gamma_0 = \frac{\sigma^2}{1 - \phi^2} $$

#### Interpreta√ß√£o da Fun√ß√£o de Autocorrela√ß√£o (ACF)

A forma da ACF ($\rho_j = \phi^j$) fornece informa√ß√µes sobre a natureza da depend√™ncia temporal no processo AR(1) [^54].
*   Se $0 < \phi < 1$, a ACF decai exponencialmente para zero √† medida que *j* aumenta, indicando uma depend√™ncia positiva e decrescente dos valores passados [^54].
*   Se $-1 < \phi < 0$, a ACF alterna em sinal e decai exponencialmente em magnitude, indicando uma depend√™ncia negativa e decrescente dos valores passados [^54].
*   Se $\phi = 0$, a ACF √© zero para todos os lags diferentes de zero, indicando que o processo √© um ru√≠do branco [^53].

> üí° **Exemplo Num√©rico:**
> Para $\phi = 0.8$ (positivo): A ACF decai gradualmente de 1 para 0.8, 0.64, 0.512...
> Para $\phi = -0.5$ (negativo): A ACF alterna em sinal: 1, -0.5, 0.25, -0.125...
> Para $\phi = 0$: A ACF √© 1 no lag 0 e 0 para todos os outros lags.
> Esses diferentes valores de $\phi$ geram padr√µes distintos na ACF, permitindo identificar o tipo de depend√™ncia temporal presente na s√©rie.

**Teorema 2** (Fun√ß√£o de Autocorrela√ß√£o Parcial - PACF) Para um processo AR(1), a fun√ß√£o de autocorrela√ß√£o parcial (PACF) √© diferente de zero apenas no lag 1.

*Demonstra√ß√£o:*
A PACF mede a correla√ß√£o entre $Y_t$ e $Y_{t-j}$ ap√≥s remover a influ√™ncia dos lags intermedi√°rios $Y_{t-1}, Y_{t-2}, \dots, Y_{t-j+1}$. Para um processo AR(1), a depend√™ncia de $Y_t$ em rela√ß√£o ao seu passado √© totalmente capturada por $Y_{t-1}$. Portanto, a correla√ß√£o parcial entre $Y_t$ e $Y_{t-j}$ para $j > 1$, condicionada aos valores intermedi√°rios, √© zero. Formalmente, $\alpha_{11} = \phi$ (onde $\alpha_{11}$ √© o primeiro coeficiente de autocorrela√ß√£o parcial) e $\alpha_{jj} = 0$ para $j > 1$. Isso significa que, ap√≥s remover a influ√™ncia de $Y_{t-1}$, n√£o h√° correla√ß√£o adicional entre $Y_t$ e $Y_{t-j}$ para $j > 1$.

> üí° **Exemplo Num√©rico:**
> Se temos uma s√©rie temporal gerada por um processo AR(1) com $\phi = 0.7$, a PACF ter√° um valor de 0.7 no lag 1 e ser√° aproximadamente zero para todos os outros lags. Isso contrasta com a ACF, que decai exponencialmente. A PACF √© uma ferramenta importante para identificar a ordem de um modelo AR.

**Proposi√ß√£o 2** (Invertibilidade) Um processo AR(1) estacion√°rio √© sempre invert√≠vel.

*Demonstra√ß√£o:*
A invertibilidade de um processo AR(1) significa que ele pode ser representado como um processo MA($\infty$) com coeficientes que decaem para zero √† medida que o lag aumenta. J√° mostramos que um processo AR(1) estacion√°rio ($|\phi| < 1$) tem a representa√ß√£o MA($\infty$) dada por $Y_t = \mu + \sum_{i=0}^{\infty} \phi^i \epsilon_{t-i}$. Os coeficientes nesta representa√ß√£o s√£o $\phi^i$, que decaem exponencialmente para zero √† medida que $i$ aumenta, devido √† condi√ß√£o $|\phi| < 1$. Portanto, um processo AR(1) estacion√°rio √© sempre invert√≠vel.

### Conclus√£o
O processo **AR(1)** representa um modelo fundamental na an√°lise de s√©ries temporais, capturando a depend√™ncia linear de primeira ordem entre valores sucessivos. A condi√ß√£o de estacionariedade $|\phi| < 1$ √© crucial para garantir a estabilidade e interpretabilidade do modelo. A representa√ß√£o como um processo **MA($\infty$)** oferece uma perspectiva alternativa sobre a estrutura de depend√™ncia temporal. O entendimento profundo das propriedades e caracter√≠sticas do modelo AR(1) fornece uma base s√≥lida para a an√°lise e modelagem de s√©ries temporais mais complexas.

### Refer√™ncias
[^45]: Imagine a battery of I such computers generating sequences...
[^46]: Note that [3.1.10] has the form of a covariance between two variables X and Y...
[^47]: The basic building block for all the processes considered in this chapter is a sequence...
[^48]: E(ŒµtŒµœÑ) = 0 for t‚â† œÑ.
[^49]: The jth autocorrelation of a covariance-stationary process (denoted pj) is defined as its jth autocovariance divided by the variance...
[^53]: A first-order autoregression, denoted AR(1), satisfies the following difference equation...
[^54]: It follows from [3.4.4] and [3.4.5] that the autocorrelation function...
<!-- END -->