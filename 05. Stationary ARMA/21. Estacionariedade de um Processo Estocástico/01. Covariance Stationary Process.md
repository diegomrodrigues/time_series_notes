## Estacionariedade em Covariância: Uma Análise Detalhada

### Introdução
Neste capítulo, exploraremos o conceito crucial de **estacionariedade em covariância** (ou estacionariedade fraca) em processos estocásticos, um pilar fundamental para a análise de séries temporais. Construindo sobre a discussão prévia da definição e importância da densidade incondicional de uma variável aleatória [^1], que nos fornece uma base para entender a distribuição de probabilidades de observações individuais, agora nos aprofundaremos nas propriedades temporais que caracterizam um processo estacionário. A estacionariedade em covariância é uma condição que garante a estabilidade estatística de um processo ao longo do tempo, o que simplifica enormemente a análise e modelagem de séries temporais.

### Conceitos Fundamentais

Um processo estocástico $\{Y_t\}$ é considerado **estacionário em covariância** (ou *fracamente estacionário*) se sua média e autocovariâncias não dependem do tempo $t$. Isso implica que as características estatísticas do processo permanecem constantes ao longo do tempo [^2]. A estacionariedade em covariância é definida por duas condições principais:

1.  **Média Constante:** A média do processo, denotada por $E(Y_t)$, é uma constante $\mu$ para todo tempo $t$. Formalmente,
    $$E(Y_t) = \mu, \quad \text{para todo } t.$$ [^2]

2.  **Autocovariância Independente do Tempo:** A autocovariância entre $Y_t$ e $Y_{t-j}$, definida como $E[(Y_t - \mu)(Y_{t-j} - \mu)]$, depende apenas da *defasagem* $j$, e não do tempo específico $t$. Essa autocovariância é denotada por $\gamma_j$. Formalmente,
    $$E[(Y_t - \mu)(Y_{t-j} - \mu)] = \gamma_j, \quad \text{para todo } t \text{ e qualquer } j.$$ [^2]

Em outras palavras, a estacionariedade em covariância significa que a relação entre os valores da série em diferentes momentos é a mesma, independentemente de quando observamos. A autocovariância, que representa a covariância de um processo com seus próprios valores defasados, é fundamental para entender a dependência temporal de uma série. A condição de que a autocovariância $\gamma_j$ dependa somente de $j$, e não de $t$, captura precisamente essa ideia [^2].

**O Processo de Ruído Branco Gaussiano como Exemplo:**

Para ilustrar o conceito de estacionariedade, podemos examinar o processo de *ruído branco gaussiano*, onde $Y_t = \epsilon_t$. Este processo, já apresentado como exemplo da densidade incondicional [^1], tem as seguintes propriedades:

*   **Média:** $E(\epsilon_t) = 0$ [^4].
*   **Variância:** $E(\epsilon_t^2) = \sigma^2$ [^4].
*   **Autocovariância:** $E(\epsilon_t \epsilon_{t-j}) = 0$ para $j \neq 0$ [^4].

Estas condições mostram que o processo de ruído branco gaussiano tem uma média constante (zero) e autocovariâncias que dependem apenas da defasagem (sendo zero para todas as defasagens não nulas), o que o torna um processo estacionário em covariância [^3].

**Contrastando com Processos Não Estacionários:**

Em contraste, um processo *não estacionário em covariância* tem sua média ou autocovariância como função do tempo, o que torna sua análise mais complexa [^2]. Como exemplo, considere o processo com tendência temporal [^1]:
$$Y_t = \beta t + \epsilon_t$$
Neste caso, a média $E(Y_t) = \beta t$ é uma função do tempo, violando a condição de estacionariedade em covariância [^1]. Similarmente, para o processo
$$Y_t = \mu^{(i)} + \epsilon_t$$
onde $\mu^{(i)}$ é a média da *i*-ésima realização gerada a partir de uma distribuição $N(0,\lambda^2)$ [^3], a média $E(Y_t) = E(\mu^{(i)}) = 0$ é estacionária. No entanto, a autocovariância $\gamma_j=E[(Y_t - 0)(Y_{t-j}-0)] = \lambda^2 \neq 0$ para $j \neq 0$  depende do *i*, o que torna o processo não estacionário [^3].

**Relação com Estacionariedade Estrita**

É importante notar a diferença entre a *estacionariedade em covariância* e a *estacionariedade estrita*. Um processo é dito *estritamente estacionário* se a distribuição conjunta de $(Y_{t_1}, Y_{t_2}, \dots, Y_{t_n})$ depende apenas dos intervalos entre os tempos $(t_1, t_2, \dots, t_n)$ e não do tempo absoluto $t$ [^3].  Um processo estritamente estacionário com segundos momentos finitos é necessariamente estacionário em covariância [^3]. No entanto, o inverso não é necessariamente verdadeiro. É possível que um processo seja estacionário em covariância sem ser estritamente estacionário. Por exemplo, os momentos de ordem superior podem variar com o tempo enquanto a média e as autocovariâncias são constantes [^3]. Em muitos casos práticos, a estacionariedade em covariância é suficiente para a aplicação de diversas técnicas de análise de séries temporais.

**A Importância da Estacionariedade na Prática:**

A estacionariedade em covariância é um requisito fundamental para muitas técnicas de análise de séries temporais. Modelos como ARMA (Autoregressive Moving Average) e suas variações dependem dessa condição para garantir que os parâmetros estimados sejam consistentes ao longo do tempo. Em outras palavras, se as propriedades estatísticas da série temporal mudam ao longo do tempo, a aplicação desses modelos pode levar a resultados inválidos e pouco confiáveis.

### Conclusão

A estacionariedade em covariância é um conceito essencial na análise de séries temporais, garantindo que as propriedades estatísticas do processo sejam estáveis ao longo do tempo. Através da imposição de uma média constante e autocovariâncias que dependem apenas da defasagem, a estacionariedade em covariância simplifica a modelagem e análise de séries temporais. Ao contrastar com exemplos de processos não estacionários, a importância da estacionariedade fica clara para o desenvolvimento de modelos robustos e confiáveis. Conforme avançarmos em outros capítulos, veremos como a estacionariedade é crucial para técnicas de previsão, estimação de parâmetros, e outras abordagens avançadas de modelagem.
### Referências
[^1]: Imagine a battery of I such computers generating sequences {y{1},
{y{2}
{y}x, and consider selecting the observation associated with
date t from each sequence:
{y{1), y?),...,y}.
This would be described as a sample of I realizations of the random variable Y..
This random variable has some density, denoted fr(y,), which is called the un-
conditional density of Y.. For example, for the Gaussian white noise process, this
density is given by
$$f_X(y) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left[-\frac{y^2}{2\sigma^2}\right]$$
The expectation of the tth observation of a time series refers to the mean of
this probability distribution, provided it exists:
$$E(Y_t) = \int y_t f(y_t) \, dy_t$$
We might view this as the probability limit of the ensemble average:
$$E(Y_t) = \text{plim} \left(\frac{1}{I}\right) \sum_{i=1}^I Y_{i,t}$$
For example, if $\{Y_t\}$ represents the sum of a constant $\mu$ plus a Gaussian white
noise process $\{\epsilon_t\}$
then its mean is
$$Y_t = \mu + \epsilon_t$$
$$E(Y_t) = \mu + E(\epsilon_t) = \mu$$
If $Y_t$ is a time trend plus Gaussian white noise,
then its mean is
$$Y_t = \beta t + \epsilon_t$$
$$E(Y_t) = \beta t$$
Sometimes for emphasis the expectation $E(Y_t)$ is called the unconditional
mean of $Y_t$. The unconditional mean is denoted $\mu_t$:
$$E(Y_t) = \mu_t$$
Note that this notation allows the general possibility that the mean can be a function
of the date of the observation t. For the process [3.1.7] involving the time trend,
the mean [3.1.8) is a function of time, whereas for the constant plus Gaussian white
noise, the mean [3.1.6] is not a function of time.

[^2]: If neither the mean $\mu_t$, nor the autocovariances $\gamma_{t,j}$ depend on the date t, then
the process for $Y_t$ is said to be covariance-stationary or weakly stationary:
$$E(Y_t) = \mu \quad \text{for all } t$$
$$E[(Y_t - \mu)(Y_{t-j} - \mu)] = \gamma_j \quad \text{for all } t \text{ and any } j.$$

[^3]: For example, the process in [3.1.5] is covariance-stationary:
$$E(Y_t) = \mu$$
$$E[(Y_t - \mu)(Y_{t-j} - \mu)] = \begin{cases} \sigma^2 & \text{for } j = 0 \\ 0 & \text{for } j \neq 0 \end{cases}$$
By contrast, the process of [3.1.7] is not covariance-stationary, because its mean,
$\beta t$, is a function of time.
Notice that if a process is covariance-stationary, the covariance between $Y_t$
and $Y_{t-j}$ depends only on $j$, the length of time separating the observations, and
not on $t$, the date of the observation. It follows that for a covariance-stationary
process, $\gamma_j$ and $\gamma_{-j}$ would represent the same magnitude. To see this, recall the
definition
$$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)].$$
If the process is covariance-stationary, then this magnitude is the same for any
value of t we might have chosen; for example, we can replace t with t + j:
$$\gamma_j = E[(Y_{t+j} - \mu)(Y_{t+j-j} - \mu)] = E[(Y_{t+j} - \mu)(Y_t - \mu)] = E[(Y_t - \mu)(Y_{t+j} - \mu)].$$
But referring again to the definition [3.1.12], this last expression is just the definition
of $\gamma_{-j}$. Thus, for any covariance-stationary process,
$$\gamma_j = \gamma_{-j} \quad \text{for all integers } j.$$
A different concept is that of strict stationarity. A process is said to be strictly
stationary if, for any values of $j_1, j_2, \ldots, j_n$, the joint distribution of $(Y_t, Y_{t+j_1}, Y_{t+j_2}, \ldots, Y_{t+j_n})$ depends only on the intervals separating the dates $(j_1, j_2, \ldots,
j_n)$ and not on the date itself (t). Notice that if a process is strictly stationary with
finite second moments, then it must be covariance-stationary-if the densities over
which we are integrating in [3.1.3] and [3.1.10] do not depend on time, then the
moments $\mu_t$ and $\gamma_j$ will not depend on time. However, it is possible to imagine a
process that is covariance-stationary but not strictly stationary; the mean and au-
tocovariances could not be functions of time, but perhaps higher moments such as
$E(Y_t^3)$ are.
In this text the term "stationary" by itself is taken to mean "covariance-
stationary."

[^4]: The basic building block for all the processes considered in this chapter is a sequence
$\{\epsilon_t\}$ - whose elements have mean zero and variance $\sigma^2$,
$$E(\epsilon_t) = 0$$
$$E(\epsilon_t^2) = \sigma^2,$$
and for which the $\epsilon$'s are uncorrelated across time:
$$E(\epsilon_t \epsilon_{\tau}) = 0$$
for $t \neq \tau$.
A process satisfying [3.2.1] through [3.2.3] is described as a white noise process.
<!-- END -->
