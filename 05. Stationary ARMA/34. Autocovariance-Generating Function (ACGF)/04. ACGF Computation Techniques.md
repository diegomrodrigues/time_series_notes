## T√≠tulo Conciso
Computa√ß√£o Eficiente da ACGF

### Introdu√ß√£o

Em continuidade aos t√≥picos anteriores que estabeleceram a import√¢ncia da fun√ß√£o geradora de autocovari√¢ncia (ACGF) para caracterizar a estrutura de depend√™ncia de s√©ries temporais [^61, ^62], este cap√≠tulo aborda t√©cnicas computacionais eficientes para o c√°lculo da ACGF. A computa√ß√£o eficiente da ACGF √© crucial, especialmente ao lidar com conjuntos de dados extensos e em aplica√ß√µes que exigem an√°lise espectral em tempo real [^63, ^64]. Dada a defini√ß√£o da ACGF como uma soma infinita, a implementa√ß√£o direta pode ser computacionalmente proibitiva.

### Conceitos Fundamentais

A defini√ß√£o formal da ACGF, conforme apresentada anteriormente, √©:

$$
g_Y(z) = \sum_{j=-\infty}^{\infty} \gamma_j z^j
$$

onde $\gamma_j$ s√£o as autocovari√¢ncias da s√©rie temporal $Y_t$ [^61]. A computa√ß√£o direta desta soma para todos os valores de $j$ √© impratic√°vel. Na pr√°tica, a s√©rie temporal √© finita, e aproximamos a ACGF truncando a soma em um n√∫mero finito de termos. Al√©m disso, ao inv√©s de avaliar a ACGF para todos os valores de *z*, geralmente estamos interessados em seu valor no c√≠rculo unit√°rio, $z = e^{-i\omega}$, que nos fornece o espectro de pot√™ncia [^61]:

$$
S_Y(\omega) = \frac{1}{2\pi} g_Y(e^{-i\omega}) = \frac{1}{2\pi} \sum_{j=-\infty}^{\infty} \gamma_j e^{-i\omega j}
$$

Esta aproxima√ß√£o nos permite utilizar a Transformada de Fourier para computar o espectro de pot√™ncia de forma eficiente [^63, ^64].

**Aproxima√ß√£o da ACGF:**

Para uma s√©rie temporal de tamanho *T*, podemos aproximar as autocovari√¢ncias $\gamma_j$ utilizando a seguinte estimativa:

$$
\hat{\gamma}_j = \frac{1}{T} \sum_{t=j+1}^{T} (Y_t - \bar{Y})(Y_{t-j} - \bar{Y})
$$

onde $\bar{Y}$ √© a m√©dia amostral da s√©rie temporal [^44, ^45]. Com esta estimativa, podemos aproximar a ACGF truncando a soma:

$$
\hat{g}_Y(z) = \sum_{j=-M}^{M} \hat{\gamma}_j z^j
$$

onde *M* √© um n√∫mero inteiro positivo que determina o n√∫mero de autocovari√¢ncias inclu√≠das na aproxima√ß√£o. A escolha de *M* √© um compromisso entre precis√£o e custo computacional.

> üí° **Exemplo Num√©rico:** Seja $Y_t = [1, 2, 3, 4, 5]$ e $M = 1$. A m√©dia amostral √© $\bar{Y} = 3$. As autocovari√¢ncias estimadas s√£o: $\hat{\gamma}_0 = \frac{1}{5} \sum_{t=1}^{5} (Y_t - 3)^2 = 2$, $\hat{\gamma}_1 = \frac{1}{5} \sum_{t=2}^{5} (Y_t - 3)(Y_{t-1} - 3) = 1.2$. Ent√£o, a ACGF aproximada √© $\hat{g}_Y(z) = 1.2z^{-1} + 2 + 1.2z$.
>
> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal $Y_t$ representando o n√∫mero di√°rio de vendas de um produto ao longo de um m√™s (T=30). Suponha que a s√©rie temporal tenha uma m√©dia $\bar{Y} = 50$. Para $M = 3$, queremos calcular $\hat{\gamma}_0$, $\hat{\gamma}_1$, $\hat{\gamma}_2$ e $\hat{\gamma}_3$.
> * $\hat{\gamma}_0 = \frac{1}{30} \sum_{t=1}^{30} (Y_t - 50)^2$. Se $\sum_{t=1}^{30} (Y_t - 50)^2 = 600$, ent√£o $\hat{\gamma}_0 = \frac{600}{30} = 20$. Isso representa a vari√¢ncia da s√©rie temporal.
> * $\hat{\gamma}_1 = \frac{1}{30} \sum_{t=2}^{30} (Y_t - 50)(Y_{t-1} - 50)$. Suponha que $\sum_{t=2}^{30} (Y_t - 50)(Y_{t-1} - 50) = 300$, ent√£o $\hat{\gamma}_1 = \frac{300}{30} = 10$. Isso indica a autocovari√¢ncia entre as vendas em dias consecutivos.
> * $\hat{\gamma}_2 = \frac{1}{30} \sum_{t=3}^{30} (Y_t - 50)(Y_{t-2} - 50)$. Suponha que $\sum_{t=3}^{30} (Y_t - 50)(Y_{t-2} - 50) = 150$, ent√£o $\hat{\gamma}_2 = \frac{150}{30} = 5$. Isso indica a autocovari√¢ncia entre as vendas em dias separados por um dia.
> * $\hat{\gamma}_3 = \frac{1}{30} \sum_{t=4}^{30} (Y_t - 50)(Y_{t-3} - 50)$. Suponha que $\sum_{t=4}^{30} (Y_t - 50)(Y_{t-3} - 50) = 75$, ent√£o $\hat{\gamma}_3 = \frac{75}{30} = 2.5$. Isso indica a autocovari√¢ncia entre as vendas em dias separados por dois dias.
>
> A ACGF aproximada seria $\hat{g}_Y(z) = 2.5z^{-3} + 5z^{-2} + 10z^{-1} + 20 + 10z + 5z^{2} + 2.5z^{3}$. Analisando as autocovari√¢ncias, observamos que a depend√™ncia diminui √† medida que o lag aumenta, o que √© comum em muitas s√©ries temporais.

**Proposi√ß√£o 1.** A estimativa $\hat{\gamma}_j$ √© assintoticamente n√£o-viesada para s√©ries temporais estacion√°rias e erg√≥dicas.

*Proof.* (Estrat√©gia) Mostrar que o valor esperado de $\hat{\gamma}_j$ converge para $\gamma_j$ quando $T$ tende ao infinito.

*Proof.*
I.  A defini√ß√£o de $\hat{\gamma}_j$ √©:
    $$\hat{\gamma}_j = \frac{1}{T} \sum_{t=j+1}^{T} (Y_t - \bar{Y})(Y_{t-j} - \bar{Y})$$

II. Tomando o valor esperado de $\hat{\gamma}_j$:
    $$E[\hat{\gamma}_j] = E\left[\frac{1}{T} \sum_{t=j+1}^{T} (Y_t - \bar{Y})(Y_{t-j} - \bar{Y})\right]$$

III. Para s√©ries estacion√°rias e erg√≥dicas, $\bar{Y}$ converge para $E[Y_t] = \mu$ quando $T$ tende ao infinito.

IV. Portanto, quando $T$ √© grande, podemos aproximar:
    $$E[\hat{\gamma}_j] \approx \frac{1}{T} \sum_{t=j+1}^{T} E[(Y_t - \mu)(Y_{t-j} - \mu)] = \frac{1}{T} \sum_{t=j+1}^{T} \gamma_j = \frac{T-j}{T} \gamma_j$$

V.  √Ä medida que $T$ tende ao infinito:
    $$\lim_{T \to \infty} E[\hat{\gamma}_j] = \lim_{T \to \infty} \frac{T-j}{T} \gamma_j = \gamma_j$$

VI. Assim, $\hat{\gamma}_j$ √© assintoticamente n√£o-viesada. $\blacksquare$

**Proposi√ß√£o 1.1.** A vari√¢ncia da estimativa $\hat{\gamma}_j$ diminui com o aumento do tamanho da s√©rie temporal *T*.

*Proof.* (Estrat√©gia) Calcular a vari√¢ncia de $\hat{\gamma}_j$ e mostrar que ela converge para zero quando *T* tende ao infinito.

*Proof.*
I. Assumindo que a s√©rie temporal $Y_t$ √© estacion√°ria e erg√≥dica com m√©dia $\mu$ e autocovari√¢ncia $\gamma_j$, podemos aproximar $\bar{Y}$ por $\mu$ para grandes *T*.

II. A vari√¢ncia de $\hat{\gamma}_j$ pode ser expressa como:
$$Var(\hat{\gamma}_j) = Var\left(\frac{1}{T} \sum_{t=j+1}^{T} (Y_t - \mu)(Y_{t-j} - \mu)\right)$$

III. Como a vari√¢ncia de uma soma de vari√°veis aleat√≥rias independentes √© a soma das vari√¢ncias, e para s√©ries estacion√°rias, a covari√¢ncia entre $(Y_t - \mu)(Y_{t-j} - \mu)$ e $(Y_s - \mu)(Y_{s-j} - \mu)$ decresce √† medida que $|t-s|$ aumenta, podemos aproximar:
$$Var(\hat{\gamma}_j) \approx \frac{1}{T^2} \sum_{t=j+1}^{T} \sum_{s=j+1}^{T} Cov((Y_t - \mu)(Y_{t-j} - \mu), (Y_s - \mu)(Y_{s-j} - \mu))$$

IV. Para s√©ries temporais com depend√™ncia de curto alcance, a covari√¢ncia entre os termos diminui rapidamente com o aumento da dist√¢ncia entre *t* e *s*. Assim, a soma dupla √© limitada por uma constante que depende das autocovari√¢ncias da s√©rie temporal, mas n√£o depende de *T*.

V. Portanto, √† medida que *T* tende ao infinito:
$$\lim_{T \to \infty} Var(\hat{\gamma}_j) \approx \lim_{T \to \infty} \frac{C}{T^2} = 0$$
onde C √© uma constante.

VI. Assim, a vari√¢ncia da estimativa $\hat{\gamma}_j$ diminui com o aumento do tamanho da s√©rie temporal *T*. $\blacksquare$

**Transformada R√°pida de Fourier (FFT):**

A Transformada R√°pida de Fourier (FFT) √© um algoritmo eficiente para computar a Transformada de Fourier Discreta (DFT) [^63, ^64]. A DFT de uma sequ√™ncia de *N* n√∫meros complexos $x_0, x_1, \ldots, x_{N-1}$ √© definida como:

$$
X_k = \sum_{n=0}^{N-1} x_n e^{-i 2\pi k n / N}
$$

para $k = 0, 1, \ldots, N-1$. A computa√ß√£o direta da DFT requer $O(N^2)$ opera√ß√µes, enquanto a FFT reduz este custo para $O(N \log N)$ opera√ß√µes.

> üí° **Exemplo Num√©rico:** Para $N = 1024$, a DFT direta requer aproximadamente $1024^2 = 1,048,576$ opera√ß√µes, enquanto a FFT requer aproximadamente $1024 * \log_2(1024) = 1024 * 10 = 10,240$ opera√ß√µes. A FFT √© significativamente mais r√°pida para grandes *N*.
>
> üí° **Exemplo Num√©rico:** Para uma s√©rie temporal de $N=4096$ pontos, a computa√ß√£o direta da DFT exigiria $4096^2 = 16,777,216$ opera√ß√µes. Usando a FFT, o n√∫mero de opera√ß√µes seria aproximadamente $4096 \times \log_2(4096) = 4096 \times 12 = 49,152$ opera√ß√µes. Este exemplo ilustra como a FFT oferece uma redu√ß√£o significativa na carga computacional para s√©ries temporais maiores.

A rela√ß√£o entre a ACGF e o espectro de pot√™ncia nos permite utilizar a FFT para computar o espectro de pot√™ncia de forma eficiente. Primeiro, calculamos as autocovari√¢ncias estimadas $\hat{\gamma}_j$. Em seguida, aplicamos a FFT √† sequ√™ncia de autocovari√¢ncias para obter uma aproxima√ß√£o do espectro de pot√™ncia [^63, ^64]:

1.  **Estimar as Autocovari√¢ncias:** Calcular $\hat{\gamma}_j$ para $j = 0, 1, \ldots, M$.

2.  **Criar uma Sequ√™ncia Sim√©trica:** Construir uma sequ√™ncia sim√©trica $h_j$ tal que $h_j = \hat{\gamma}_j$ para $j = 0, 1, \ldots, M$ e $h_j = \hat{\gamma}_{-j} = \hat{\gamma}_{|j|}$ para $j = -M, \ldots, -1$. Isto resulta em uma sequ√™ncia de tamanho $2M+1$.

3.  **Aplicar a FFT:** Calcular a DFT da sequ√™ncia $h_j$ utilizando a FFT. O resultado √© uma aproxima√ß√£o do espectro de pot√™ncia [^64].

    $$
    S_Y(\omega_k) \approx \frac{1}{2\pi} \sum_{j=-M}^{M} h_j e^{-i \omega_k j}
    $$

    onde $\omega_k = \frac{2\pi k}{2M+1}$ para $k = 0, 1, \ldots, 2M$.

*Proof.* Mostraremos como o algoritmo de computa√ß√£o do espectro de pot√™ncia utilizando a FFT se relaciona com a defini√ß√£o da ACGF.

I. Come√ßamos com a defini√ß√£o do espectro de pot√™ncia em termos da ACGF:
    $$S_Y(\omega) = \frac{1}{2\pi} \sum_{j=-\infty}^{\infty} \gamma_j e^{-i\omega j}$$

II. Aproximamos a ACGF truncando a soma e estimando as autocovari√¢ncias:
     $$S_Y(\omega) \approx \frac{1}{2\pi} \sum_{j=-M}^{M} \hat{\gamma}_j e^{-i\omega j}$$

III. Para computar numericamente esta soma, discretizamos a frequ√™ncia $\omega$ em $2M+1$ pontos: $\omega_k = \frac{2\pi k}{2M+1}$, para $k = 0, 1, \ldots, 2M$.

IV. Substitu√≠mos a discretiza√ß√£o da frequ√™ncia na express√£o:
     $$S_Y(\omega_k) \approx \frac{1}{2\pi} \sum_{j=-M}^{M} \hat{\gamma}_j e^{-i \frac{2\pi k j}{2M+1}}$$

V. Definimos $h_j = \hat{\gamma}_j$ para $j = 0, 1, \ldots, M$ e $h_j = \hat{\gamma}_{-j} = \hat{\gamma}_{|j|}$ para $j = -M, \ldots, -1$.

VI. Substitu√≠mos $h_j$ na express√£o:
      $$S_Y(\omega_k) \approx \frac{1}{2\pi} \sum_{j=-M}^{M} h_j e^{-i \frac{2\pi k j}{2M+1}}$$

VII. Esta √© a DFT da sequ√™ncia $h_j$, que pode ser computada eficientemente utilizando a FFT. $\blacksquare$

**Lema 1.** A sequ√™ncia sim√©trica $h_j$ garante que o espectro de pot√™ncia estimado $S_Y(\omega_k)$ seja real.

*Proof.* (Estrat√©gia) Mostrar que a transformada de Fourier de uma fun√ß√£o par (sim√©trica) √© sempre real.

*Proof.*
I. A sequ√™ncia $h_j$ √© sim√©trica, ou seja, $h_j = h_{-j}$.

II. O espectro de pot√™ncia estimado √© dado por:
    $$S_Y(\omega_k) \approx \frac{1}{2\pi} \sum_{j=-M}^{M} h_j e^{-i \omega_k j}$$

III. Podemos reescrever a soma como:
    $$S_Y(\omega_k) \approx \frac{1}{2\pi} \left[h_0 + \sum_{j=1}^{M} h_j (e^{-i \omega_k j} + e^{i \omega_k j})\right]$$

IV. Usando a identidade de Euler, $e^{ix} + e^{-ix} = 2\cos(x)$:
    $$S_Y(\omega_k) \approx \frac{1}{2\pi} \left[h_0 + \sum_{j=1}^{M} h_j 2\cos(\omega_k j)\right]$$

V. Como $h_j$ s√£o reais e $\cos(\omega_k j)$ s√£o reais, a soma √© real. Portanto, $S_Y(\omega_k)$ √© real. $\blacksquare$

**Lema 1.1.** Se a sequ√™ncia original $Y_t$ for real, ent√£o as autocovari√¢ncias $\gamma_j$ s√£o reais e sim√©tricas, ou seja, $\gamma_j = \gamma_{-j}$.

*Proof.* (Estrat√©gia) Mostrar que se $Y_t$ √© real, ent√£o a autocovari√¢ncia estimada para um lag positivo *j* √© igual √† autocovari√¢ncia estimada para o lag negativo *-j*.

*Proof.*
I. Dado que $Y_t$ √© real, ent√£o $\bar{Y}$ √© real.

II. A autocovari√¢ncia estimada para um lag *j* √©:
    $$\hat{\gamma}_j = \frac{1}{T} \sum_{t=j+1}^{T} (Y_t - \bar{Y})(Y_{t-j} - \bar{Y})$$

III. A autocovari√¢ncia estimada para um lag *-j* √©:
    $$\hat{\gamma}_{-j} = \frac{1}{T} \sum_{t=-j+1}^{T} (Y_t - \bar{Y})(Y_{t+j} - \bar{Y})$$

IV. Fazendo uma mudan√ßa de vari√°vel $s = t+j$, temos $t = s-j$, e a soma se torna:
    $$\hat{\gamma}_{-j} = \frac{1}{T} \sum_{s=1}^{T+j} (Y_{s-j} - \bar{Y})(Y_{s} - \bar{Y})$$

V. Como estamos lidando com uma s√©rie temporal finita de tamanho *T*, podemos truncar a soma de $\hat{\gamma}_{-j}$ para considerar apenas os termos onde $1 \leq s-j \leq T$. Assim, a soma se torna:
    $$\hat{\gamma}_{-j} = \frac{1}{T} \sum_{s=j+1}^{T} (Y_{s-j} - \bar{Y})(Y_{s} - \bar{Y})$$
   Esta soma √© igual √† soma em $\hat{\gamma}_j$ com os √≠ndices invertidos. Como a multiplica√ß√£o √© comutativa, temos:

VI. $$\hat{\gamma}_{-j} =  \frac{1}{T} \sum_{s=j+1}^{T} (Y_{s} - \bar{Y})(Y_{s-j} - \bar{Y}) = \hat{\gamma}_j$$

VII. Portanto, se a sequ√™ncia original $Y_t$ for real, ent√£o as autocovari√¢ncias $\hat{\gamma}_j$ s√£o reais e sim√©tricas, ou seja, $\hat{\gamma}_j = \hat{\gamma}_{-j}$. $\blacksquare$

**Janelamento (Windowing):**

Para melhorar a precis√£o da estimativa do espectro de pot√™ncia, √© comum aplicar uma janela (window) √† sequ√™ncia de autocovari√¢ncias antes de calcular a FFT [^63]. Uma janela √© uma fun√ß√£o $w_j$ que atenua as autocovari√¢ncias para lags maiores, reduzindo o vazamento espectral (spectral leakage) e melhorando a resolu√ß√£o espectral.

Algumas janelas comuns incluem [^63]:

*   **Janela de Bartlett (triangular):** $w_j = 1 - \frac{|j|}{M}$ para $|j| \leq M$ e $w_j = 0$ caso contr√°rio.
*   **Janela de Hamming:** $w_j = 0.54 + 0.46 \cos\left(\frac{2\pi j}{2M+1}\right)$ para $|j| \leq M$ e $w_j = 0$ caso contr√°rio.
*   **Janela de Hanning:** $w_j = 0.5 + 0.5 \cos\left(\frac{2\pi j}{2M+1}\right)$ para $|j| \leq M$ e $w_j = 0$ caso contr√°rio.

A aplica√ß√£o de uma janela modifica a estimativa do espectro de pot√™ncia:

$$
S_Y(\omega_k) \approx \frac{1}{2\pi} \sum_{j=-M}^{M} w_j h_j e^{-i \omega_k j}
$$

> üí° **Exemplo Num√©rico:** Usando a sequ√™ncia de autocovari√¢ncias do exemplo anterior, aplicamos a janela de Hamming. As autocovari√¢ncias janeladas s√£o $h_0^* = w_0 h_0 = 1 * 2 = 2$, $h_1^* = w_1 h_1 = (0.54 + 0.46 \cos(2\pi / 5)) * 1.2 = 0.762$. A ACGF janelada √© $\hat{g}_Y^*(z) = 0.762z^{-1} + 2 + 0.762z$.
>
> üí° **Exemplo Num√©rico:** Considere uma sequ√™ncia de autocovari√¢ncias $h_j = [4, 2, 1, 0.5, 0.25]$ para $j = 0, 1, 2, 3, 4$ (e seus correspondentes lags negativos). Vamos aplicar a janela de Bartlett com $M=4$ e a janela de Hanning:
> * **Janela de Bartlett:** $w_j = [1, 0.75, 0.5, 0.25, 0]$
> * **Janela de Hanning:** $w_j = [1, 0.9045, 0.6545, 0.3455, 0.0955]$
>
> Agora, vamos calcular as autocovari√¢ncias janeladas:
>
> | Lag (j) | Autocovari√¢ncia ($h_j$) | Bartlett ($w_j$) | Janelamento Bartlett ($w_j h_j$) | Hanning ($w_j$) | Janelamento Hanning ($w_j h_j$) |
> |---|---|---|---|---|---|
> | 0 | 4 | 1 | 4 | 1 | 4 |
> | 1 | 2 | 0.75 | 1.5 | 0.9045 | 1.809 |
> | 2 | 1 | 0.5 | 0.5 | 0.6545 | 0.6545 |
> | 3 | 0.5 | 0.25 | 0.125 | 0.3455 | 0.17275 |
> | 4 | 0.25 | 0 | 0 | 0.0955 | 0.023875 |
>
> Observe como a janela de Bartlett reduz linearmente as autocovari√¢ncias, enquanto a janela de Hanning aplica uma redu√ß√£o mais suave, preservando mais das autocovari√¢ncias em lags menores, mas atenuando mais fortemente em lags maiores.  A escolha entre as janelas impactar√° a resolu√ß√£o e o vazamento espectral na estimativa final do espectro de pot√™ncia.

A escolha da janela depende das caracter√≠sticas da s√©rie temporal e dos objetivos da an√°lise espectral. Janelas com l√≥bulos laterais menores (como a janela de Hamming) reduzem o vazamento espectral, enquanto janelas com l√≥bulos principais mais largos (como a janela retangular) fornecem melhor resolu√ß√£o espectral [^63].

**Lema 2.** O janelamento reduz a vari√¢ncia da estimativa do espectro de pot√™ncia, mas introduz um vi√©s.

*Proof.* A redu√ß√£o da vari√¢ncia segue do fato que o janelamento atenua as autocovari√¢ncias para lags maiores, que s√£o menos precisas. A introdu√ß√£o de um vi√©s segue do fato que a estimativa do espectro de pot√™ncia n√£o √© mais uma estimativa n√£o-viesada do verdadeiro espectro de pot√™ncia. A escolha da janela √© um compromisso entre vari√¢ncia e vi√©s.

**Implementa√ß√£o em Tempo Real:**

Em aplica√ß√µes em tempo real, a s√©rie temporal est√° continuamente sendo atualizada. Para calcular a ACGF e o espectro de pot√™ncia de forma eficiente, podemos utilizar algoritmos recursivos para atualizar as autocovari√¢ncias e o espectro de pot√™ncia √† medida que novos dados chegam [^64].

Seja $Y_t$ a nova observa√ß√£o. Podemos atualizar a m√©dia amostral $\bar{Y}$ recursivamente:

$$
\bar{Y}_{t} = \frac{t-1}{t} \bar{Y}_{t-1} + \frac{1}{t} Y_t
$$

e atualizar as autocovari√¢ncias:

$$
\hat{\gamma}_{j,t} = \frac{t-j-1}{t-j} \hat{\gamma}_{j,t-1} + \frac{1}{t-j} (Y_t - \bar{Y}_t)(Y_{t-j} - \bar{Y}_t)
$$

Com as autocovari√¢ncias atualizadas, podemos aplicar a FFT para obter o espectro de pot√™ncia atualizado [^64].

> üí° **Exemplo Num√©rico:** Seja $Y_t = [1, 2, 3]$. Inicialmente, $\bar{Y}_1 = 1$ e $\hat{\gamma}_{0,1} = 0$. Ap√≥s a segunda observa√ß√£o, $\bar{Y}_2 = (1/2) * 1 + (1/2) * 2 = 1.5$ e $\hat{\gamma}_{0,2} = (0/1) * 0 + (1/1) * (2-1.5)^2 = 0.25$. Ap√≥s a terceira observa√ß√£o, $\bar{Y}_3 = (2/3) * 1.5 + (1/3) * 3 = 2$ e $\hat{\gamma}_{0,3} = (1/2) * 0.25 + (1/2) * (3-2)^2 = 0.625$.
>
> üí° **Exemplo Num√©rico:** Considere um sistema de monitoramento de temperatura que registra a temperatura a cada segundo. Suponha que as primeiras tr√™s leituras sejam $Y_1 = 25$, $Y_2 = 26$, e $Y_3 = 27$ graus Celsius. Vamos calcular recursivamente a m√©dia e a autocovari√¢ncia $\hat{\gamma}_{0,t}$ (vari√¢ncia) para $t=1, 2, 3$:
> * **t = 1:**
>   * $\bar{Y}_1 = Y_1 = 25$
>   * $\hat{\gamma}_{0,1} = 0$ (inicialmente)
> * **t = 2:**
>   * $\bar{Y}_2 = \frac{1}{2} \bar{Y}_1 + \frac{1}{2} Y_2 = \frac{1}{2} (25) + \frac{1}{2} (26) = 25.5$
>   * $\hat{\gamma}_{0,2} = \frac{1}{2-0} (Y_2 - \bar{Y}_2)^2 = \frac{1}{2} (26 - 25.5)^2 = \frac{1}{2} (0.5)^2 = 0.125$
> * **t = 3:**
>   * $\bar{Y}_3 = \frac{2}{3} \bar{Y}_2 + \frac{1}{3} Y_3 = \frac{2}{3} (25.5) + \frac{1}{3} (27) = 26$
>   * $\hat{\gamma}_{0,3} = \frac{2}{3-0} \hat{\gamma}_{0,2} + \frac{1}{3-0} (Y_3 - \bar{Y}_3)^2 = \frac{2}{3} (0.125) + \frac{1}{3} (27 - 26)^2 = \frac{2}{3} (0.125) + \frac{1}{3} (1) = 0.0833 + 0.3333 = 0.4166$
>
> Este exemplo demonstra como a m√©dia e a autocovari√¢ncia s√£o atualizadas a cada nova leitura, permitindo o rastreamento cont√≠nuo das estat√≠sticas da s√©rie temporal em tempo real.

**Teorema 3.** (Teorema de Wiener-Khinchin) Para um processo estacion√°rio de sentido amplo, o espectro de pot√™ncia √© a transformada de Fourier da fun√ß√£o de autocovari√¢ncia.

Este teorema estabelece a rela√ß√£o fundamental entre o dom√≠nio do tempo (autocovari√¢ncia) e o dom√≠nio da frequ√™ncia (espectro de pot√™ncia). A FFT √© uma implementa√ß√£o computacional eficiente dessa transforma√ß√£o.

**Teorema 4.** Para uma s√©rie temporal estacion√°ria com autocovari√¢ncias absolutamente som√°veis, a estimativa do espectro de pot√™ncia obtida via FFT converge para o verdadeiro espectro de pot√™ncia √† medida que o tamanho da s√©rie temporal aumenta.

*Proof.* (Estrat√©gia) A prova envolve demonstrar que as estimativas das autocovari√¢ncias convergem para os verdadeiros valores e que a FFT √© uma transformada linear cont√≠nua.

*Proof.*
I. Se a s√©rie temporal √© estacion√°ria e as autocovari√¢ncias s√£o absolutamente som√°veis, ent√£o a ACGF existe e √© cont√≠nua [^61].

II. As estimativas das autocovari√¢ncias convergem em m√©dia quadr√°tica para os verdadeiros valores √† medida que o tamanho da amostra aumenta.

III. A FFT √© uma transforma√ß√£o linear cont√≠nua, o que significa que pequenas perturba√ß√µes nas autocovari√¢ncias levam a pequenas perturba√ß√µes no espectro de pot√™ncia.

IV. Portanto, √† medida que o tamanho da s√©rie temporal aumenta, as estimativas das autocovari√¢ncias se aproximam dos valores verdadeiros, e a estimativa do espectro de pot√™ncia obtida via FFT converge para o verdadeiro espectro de pot√™ncia. $\blacksquare$

### Conclus√£o

A computa√ß√£o eficiente da ACGF √© essencial para a an√°lise de s√©ries temporais, especialmente quando se lida com grandes conjuntos de dados e em aplica√ß√µes em tempo real [^63, ^64]. A utiliza√ß√£o da FFT, em conjunto com t√©cnicas de janelamento e algoritmos recursivos, permite computar o espectro de pot√™ncia de forma eficiente e precisa. O conhecimento dessas t√©cnicas computacionais √© fundamental para a an√°lise e modelagem de s√©ries temporais em diversas √°reas, incluindo finan√ßas, engenharia e ci√™ncia [^47, ^48].

### Refer√™ncias
[^44]: P√°gina 44, Cap√≠tulo 3, "Stationary ARMA Processes"
[^45]: P√°gina 45, Cap√≠tulo 3, "Stationary ARMA Processes"
[^47]: P√°gina 47, Cap√≠tulo 3, "Stationary ARMA Processes"
[^48]: P√°gina 48, Cap√≠tulo 3, "Stationary ARMA Processes"
[^61]: P√°gina 61, Cap√≠tulo 3, "Stationary ARMA Processes"
[^62]: P√°gina 62, Cap√≠tulo 3, "Stationary ARMA Processes"
[^63]: P√°gina 63, Cap√≠tulo 3, "Stationary ARMA Processes"
[^64]: P√°gina 64, Cap√≠tulo 3, "Stationary ARMA Processes"
<!-- END -->