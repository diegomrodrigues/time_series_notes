## T√≠tulo Conciso
Covariance-Stationarity in Time Series Analysis

### Introdu√ß√£o
Em continuidade aos conceitos de expectativas, estacionaridade e ergodicidade abordados anteriormente, este cap√≠tulo aprofunda a no√ß√£o de **covariance-stationarity** (ou estacionaridade fraca) em s√©ries temporais. A estacionaridade √© uma propriedade fundamental que permite simplificar a an√°lise e modelagem de s√©ries temporais, possibilitando o uso de modelos estat√≠sticos e algoritmos independentes do tempo [^45]. Este cap√≠tulo detalha as condi√ß√µes matem√°ticas que definem a covariance-stationarity, suas implica√ß√µes e exemplos.

### Conceitos Fundamentais

A covariance-stationarity, tamb√©m conhecida como *weak stationarity*, imp√µe restri√ß√µes sobre como as propriedades estat√≠sticas de uma s√©rie temporal podem variar ao longo do tempo. Especificamente, exige que as propriedades estat√≠sticas da s√©rie sejam constantes ao longo do tempo [^45]. Isso se traduz em duas condi√ß√µes principais:

1.  **M√©dia Constante:** A esperan√ßa matem√°tica da s√©rie temporal $Y_t$ deve ser constante e independente do tempo, ou seja, $E(Y_t) = \mu$ para todo $t$ [^45]. Onde $\mu$ √© uma constante. Isso significa que o valor m√©dio da s√©rie n√£o apresenta tend√™ncia ou sazonalidade ao longo do tempo.
> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal $Y_t$ representando o n√∫mero de vendas di√°rias de um produto est√°vel. Se, ao longo de um ano (365 dias), a m√©dia de vendas di√°rias for consistentemente em torno de 100 unidades, ent√£o $E(Y_t) \approx 100$ para todo $t$ (dia). Isso sugere que a condi√ß√£o de m√©dia constante est√° satisfeita. Para confirmar, podemos calcular a m√©dia amostral: $\bar{Y} = \frac{1}{365}\sum_{t=1}^{365} Y_t$. Se $\bar{Y}$ for pr√≥ximo de 100 e n√£o apresentar varia√ß√µes significativas ao longo de subper√≠odos (e.g., trimestres), ent√£o temos evid√™ncias de m√©dia constante.
```python
import numpy as np

# Simula√ß√£o de dados (vendas di√°rias) com m√©dia constante
np.random.seed(42)  # para reproducibilidade
mu = 100
sigma = 15 # desvio padr√£o
y = np.random.normal(mu, sigma, 365) # 365 dias

media_amostral = np.mean(y)
print(f"M√©dia amostral: {media_amostral:.2f}")

# Calcular m√©dias trimestrais para verificar consist√™ncia
trimestre1 = np.mean(y[:90])
trimestre2 = np.mean(y[90:180])
trimestre3 = np.mean(y[180:270])
trimestre4 = np.mean(y[270:])

print(f"M√©dia Trimestre 1: {trimestre1:.2f}")
print(f"M√©dia Trimestre 2: {trimestre2:.2f}")
print(f"M√©dia Trimestre 3: {trimestre3:.2f}")
print(f"M√©dia Trimestre 4: {trimestre4:.2f}")

```

2.  **Autocovari√¢ncia Independente do Tempo:** A autocovari√¢ncia entre $Y_t$ e $Y_{t-j}$ deve depender apenas do *lag* $j$ e n√£o do tempo $t$ [^45]. Matematicamente, $E[(Y_t - \mu)(Y_{t-j} - \mu)] = \gamma_j$, onde $\gamma_j$ √© a autocovari√¢ncia no *lag* $j$. Isso implica que a correla√ß√£o entre dois pontos na s√©rie temporal depende apenas da dist√¢ncia entre eles, e n√£o do momento em que s√£o observados.
> üí° **Exemplo Num√©rico:**  Suponha que analisamos a autocovari√¢ncia de uma s√©rie temporal de temperatura di√°ria em uma cidade ao longo de v√°rios anos. Se a autocovari√¢ncia entre a temperatura de hoje ($Y_t$) e a temperatura de ontem ($Y_{t-1}$) for consistentemente alta (pr√≥xima a um valor fixo, digamos 0.8 $\sigma^2$, onde $\sigma^2$ √© a vari√¢ncia) em todos os anos, ent√£o a condi√ß√£o de autocovari√¢ncia independente do tempo para lag 1 √© aproximadamente satisfeita. Isso indicaria que a depend√™ncia da temperatura de hoje em rela√ß√£o √† temperatura de ontem √© est√°vel ao longo do tempo.
```python
import numpy as np

# Simula√ß√£o de uma s√©rie temporal com autocorrela√ß√£o no lag 1
np.random.seed(42)
n = 365 * 3 # 3 anos de dados
mu = 20      # temperatura m√©dia
sigma = 5    # desvio padr√£o
alpha = 0.8  # coeficiente de autocorrela√ß√£o lag 1

y = np.zeros(n)
y[0] = np.random.normal(mu, sigma)
for t in range(1, n):
    y[t] = mu + alpha * (y[t-1] - mu) + np.random.normal(0, sigma * np.sqrt(1 - alpha**2))

# Estimar autocovari√¢ncia no lag 1
def autocovariance(x, lag):
    n = len(x)
    x_mean = np.mean(x)
    sum = 0
    for i in range(lag, n):
        sum += (x[i] - x_mean) * (x[i-lag] - x_mean)
    return sum / n

gamma_1 = autocovariance(y, 1)
print(f"Autocovari√¢ncia estimada no lag 1: {gamma_1:.2f}")

# Comparar autocovari√¢ncia em diferentes anos
gamma_1_ano1 = autocovariance(y[:365], 1)
gamma_1_ano2 = autocovariance(y[365:730], 1)
gamma_1_ano3 = autocovariance(y[730:], 1)

print(f"Autocovari√¢ncia estimada no lag 1 (Ano 1): {gamma_1_ano1:.2f}")
print(f"Autocovari√¢ncia estimada no lag 1 (Ano 2): {gamma_1_ano2:.2f}")
print(f"Autocovari√¢ncia estimada no lag 1 (Ano 3): {gamma_1_ano3:.2f}")
```

**Em outras palavras:**
> *Se nem a m√©dia $\mu_t$ nem as autocovari√¢ncias $\gamma_{jt}$ dependem da data $t$, ent√£o o processo para $Y_t$ √© dito covariance-stationary ou weakly stationary.* [^45]

Para exemplificar, o processo descrito em [3.1.5], onde $Y_t = \mu + \epsilon_t$ com $\epsilon_t$ sendo ru√≠do branco gaussiano, √© covariance-stationary [^43, 46].  A m√©dia √© $E(Y_t) = \mu$, que √© constante [^43]. As autocovari√¢ncias s√£o $\gamma_{jt} = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E(\epsilon_t \epsilon_{t-j})$ [^45]. Como $\epsilon_t$ √© ru√≠do branco, $E(\epsilon_t \epsilon_{t-j}) = \sigma^2$ se $j = 0$ e $0$ caso contr√°rio [^43]. Portanto, as autocovari√¢ncias dependem apenas de $j$ e n√£o de $t$.
> üí° **Exemplo Num√©rico:** Seja $Y_t = 5 + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco com m√©dia 0 e vari√¢ncia 1. Ent√£o, $E[Y_t] = E[5 + \epsilon_t] = 5 + E[\epsilon_t] = 5$, que √© constante. A autocovari√¢ncia $\gamma_0 = E[(Y_t - 5)(Y_t - 5)] = E[\epsilon_t^2] = 1$. Para $j \neq 0$, $\gamma_j = E[(Y_t - 5)(Y_{t-j} - 5)] = E[\epsilon_t \epsilon_{t-j}] = 0$. Portanto, a s√©rie √© covariance-stationary.
```python
import numpy as np

# Simula√ß√£o de ru√≠do branco
np.random.seed(42)
n = 1000
mu = 5
sigma = 1
epsilon = np.random.normal(0, sigma, n)
y = mu + epsilon

# Calcular m√©dia e autocovari√¢ncia amostral
media_amostral = np.mean(y)
print(f"M√©dia amostral: {media_amostral:.2f}")

def autocovariance(x, lag):
    n = len(x)
    x_mean = np.mean(x)
    sum = 0
    for i in range(lag, n):
        sum += (x[i] - x_mean) * (x[i-lag] - x_mean)
    return sum / n

gamma_0 = autocovariance(y, 0)
gamma_1 = autocovariance(y, 1)

print(f"Autocovari√¢ncia estimada no lag 0: {gamma_0:.2f}")
print(f"Autocovari√¢ncia estimada no lag 1: {gamma_1:.2f}")
```

Em contraste, o processo $Y_t = \beta t + \epsilon_t$, descrito em [3.1.7], *n√£o √© covariance-stationary* [^43, 46]. Embora a vari√¢ncia $E(Y_t - \beta t)^2 = E(\epsilon_t^2) = \sigma^2$ seja constante, a m√©dia $E(Y_t) = \beta t$ depende do tempo [^43].
> üí° **Exemplo Num√©rico:** Considere $Y_t = 2t + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco com m√©dia 0 e vari√¢ncia 1. A m√©dia √© $E[Y_t] = E[2t + \epsilon_t] = 2t + E[\epsilon_t] = 2t$.  A m√©dia varia linearmente com o tempo, violando a condi√ß√£o de m√©dia constante para covariance-stationarity. Se calcularmos a m√©dia para $t = 1$ e $t = 100$, obteremos $E[Y_1] = 2$ e $E[Y_{100}] = 200$, demonstrando que a m√©dia n√£o √© constante.
```python
import numpy as np

# Simula√ß√£o de processo n√£o estacion√°rio
np.random.seed(42)
n = 100
beta = 2
sigma = 1
epsilon = np.random.normal(0, sigma, n)
t = np.arange(1, n + 1)
y = beta * t + epsilon

# Calcular m√©dias em diferentes pontos no tempo
media_t1 = y[0] # y[0] √© para t = 1
media_t100 = y[99] # y[99] √© para t = 100
print(f"M√©dia em t=1: {media_t1:.2f}")
print(f"M√©dia em t=100: {media_t100:.2f}")
```

A covariance-stationarity implica que a vari√¢ncia de $Y_t$ √© constante e igual a $\gamma_0$ [^42, 45]. Matematicamente, $\gamma_0 = E[(Y_t - \mu)^2]$, que √© a vari√¢ncia de $Y_t$.

**Prova:**
Mostraremos que a vari√¢ncia de $Y_t$ √© constante e igual a $\gamma_0$ para um processo covariance-stationary.

I. Por defini√ß√£o, a vari√¢ncia de $Y_t$ √© dada por $Var(Y_t) = E[(Y_t - E[Y_t])^2]$.

II. Como $Y_t$ √© covariance-stationary, $E[Y_t] = \mu$ para todo $t$.

III. Substituindo $E[Y_t]$ por $\mu$ na express√£o da vari√¢ncia, temos $Var(Y_t) = E[(Y_t - \mu)^2]$.

IV. A autocovari√¢ncia no lag 0 √© definida como $\gamma_0 = E[(Y_t - \mu)(Y_{t-0} - \mu)] = E[(Y_t - \mu)^2]$.

V. Portanto, $Var(Y_t) = \gamma_0$, que √© constante e independente de $t$, pois $Y_t$ √© covariance-stationary. $\blacksquare$

Al√©m disso, a j-√©sima autocorrela√ß√£o (denotada por $\rho_j$) de um processo covariance-stationary √© definida como a j-√©sima autocovari√¢ncia dividida pela vari√¢ncia: $\rho_j = \frac{\gamma_j}{\gamma_0}$ [^49]. Como $\gamma_j$ e $\gamma_0$ s√£o independentes de $t$ para um processo covariance-stationary, $\rho_j$ tamb√©m √© independente de $t$ [^45, 49].
> üí° **Exemplo Num√©rico:** Retomando o exemplo da temperatura di√°ria, se $\gamma_1 = 0.8\sigma^2$ e $\gamma_0 = \sigma^2$, ent√£o a autocorrela√ß√£o no lag 1 √© $\rho_1 = \frac{0.8\sigma^2}{\sigma^2} = 0.8$. Se a s√©rie √© covariance-stationary, esse valor de 0.8 deve ser consistente ao longo do tempo. Podemos estimar a autocorrela√ß√£o amostral em diferentes per√≠odos e verificar se permanece aproximadamente constante.
```python
import numpy as np

# Simula√ß√£o da s√©rie temporal com autocorrela√ß√£o no lag 1 (mesmo exemplo anterior)
np.random.seed(42)
n = 365 * 3 # 3 anos de dados
mu = 20      # temperatura m√©dia
sigma = 5    # desvio padr√£o
alpha = 0.8  # coeficiente de autocorrela√ß√£o lag 1

y = np.zeros(n)
y[0] = np.random.normal(mu, sigma)
for t in range(1, n):
    y[t] = mu + alpha * (y[t-1] - mu) + np.random.normal(0, sigma * np.sqrt(1 - alpha**2))

# Estimar autocovari√¢ncia e autocorrela√ß√£o no lag 1
def autocovariance(x, lag):
    n = len(x)
    x_mean = np.mean(x)
    sum = 0
    for i in range(lag, n):
        sum += (x[i] - x_mean) * (x[i-lag] - x_mean)
    return sum / n

def autocorrelation(x, lag):
    gamma_0 = autocovariance(x, 0)
    gamma_lag = autocovariance(x, lag)
    return gamma_lag / gamma_0

rho_1 = autocorrelation(y, 1)
print(f"Autocorrela√ß√£o estimada no lag 1: {rho_1:.2f}")

# Comparar autocorrela√ß√£o em diferentes anos
rho_1_ano1 = autocorrelation(y[:365], 1)
rho_1_ano2 = autocorrelation(y[365:730], 1)
rho_1_ano3 = autocorrelation(y[730:], 1)

print(f"Autocorrela√ß√£o estimada no lag 1 (Ano 1): {rho_1_ano1:.2f}")
print(f"Autocorrela√ß√£o estimada no lag 1 (Ano 2): {rho_1_ano2:.2f}")
print(f"Autocorrela√ß√£o estimada no lag 1 (Ano 3): {rho_1_ano3:.2f}")
```

**Proposi√ß√£o 1**
Se $Y_t$ √© covariance-stationary, ent√£o $\gamma_j = \gamma_{-j}$ para todo $j$.

*Proof:*
Por defini√ß√£o, $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$.  Substituindo $j$ por $-j$, obtemos $\gamma_{-j} = E[(Y_t - \mu)(Y_{t+j} - \mu)]$.  Fazendo a mudan√ßa de vari√°vel $s = t+j$, temos $t = s-j$ e $\gamma_{-j} = E[(Y_{s-j} - \mu)(Y_s - \mu)] = E[(Y_s - \mu)(Y_{s-j} - \mu)] = \gamma_j$.

Al√©m disso, a j-√©sima autocorrela√ß√£o tamb√©m possui a mesma propriedade: $\rho_j = \rho_{-j}$. Isso segue diretamente da proposi√ß√£o acima e da defini√ß√£o de autocorrela√ß√£o.

**Prova:**
Mostraremos que se $Y_t$ √© covariance-stationary, ent√£o $\rho_j = \rho_{-j}$.

I. Por defini√ß√£o, $\rho_j = \frac{\gamma_j}{\gamma_0}$ e $\rho_{-j} = \frac{\gamma_{-j}}{\gamma_0}$.

II. Pela Proposi√ß√£o 1, se $Y_t$ √© covariance-stationary, ent√£o $\gamma_j = \gamma_{-j}$ para todo $j$.

III. Portanto, $\rho_j = \frac{\gamma_j}{\gamma_0} = \frac{\gamma_{-j}}{\gamma_0} = \rho_{-j}$.

IV. Assim, demonstramos que $\rho_j = \rho_{-j}$. $\blacksquare$

√â crucial notar a diferen√ßa entre **covariance-stationarity** e **strict stationarity** [^46]. Um processo √© dito *strictly stationary* se a distribui√ß√£o conjunta de $(Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n})$ √© a mesma que a distribui√ß√£o conjunta de $(Y_{t_1+h}, Y_{t_2+h}, \ldots, Y_{t_n+h})$ para qualquer $h$. Ou seja, as propriedades estat√≠sticas conjuntas s√£o invariantes a transla√ß√µes no tempo. Se um processo √© *strictly stationary* e tem momentos de segunda ordem finitos, ent√£o ele √© *covariance-stationary*, mas o inverso n√£o √© necessariamente verdadeiro. Um processo pode ser covariance-stationary, mas ter momentos superiores que dependem do tempo [^46].
> üí° **Exemplo Num√©rico:** Considere um processo onde $Y_t$ √© o resultado de um lan√ßamento de moeda viciada, com probabilidade $p$ de dar cara e $1-p$ de dar coroa. Se $p$ √© constante ao longo do tempo, o processo √© strictly stationary. No entanto, se a moeda √© trocada a cada 1000 lan√ßamentos, alternando entre $p=0.2$ e $p=0.8$, o processo *pode* ainda ser covariance-stationary (dependendo de como definimos $Y_t$, por exemplo, como a m√©dia dos √∫ltimos *k* lan√ßamentos), mas *n√£o* ser√° strictly stationary, pois a distribui√ß√£o conjunta dos resultados depender√° de quais moedas foram usadas.

**Teorema 2**
Seja $Y_t$ um processo strictly stationary com m√©dia $\mu$ e fun√ß√£o de autocovari√¢ncia $\gamma_j$. Se $E[Y_t^2] < \infty$, ent√£o $Y_t$ √© covariance-stationary.

*Proof:*
Como $Y_t$ √© strictly stationary, a distribui√ß√£o de $Y_t$ √© a mesma para todo $t$. Portanto, $E[Y_t] = \mu$ para todo $t$, onde $\mu$ √© uma constante. Agora, considere a autocovari√¢ncia $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$.  A distribui√ß√£o conjunta de $(Y_t, Y_{t-j})$ √© a mesma que a distribui√ß√£o conjunta de $(Y_{t+h}, Y_{t-j+h})$ para qualquer $h$.  Escolhendo $h = -t+j$, temos que a distribui√ß√£o conjunta de $(Y_t, Y_{t-j})$ √© a mesma que a distribui√ß√£o conjunta de $(Y_j, Y_0)$. Portanto, $E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(Y_j - \mu)(Y_0 - \mu)]$, que √© uma fun√ß√£o apenas de $j$ e n√£o de $t$.  Assim, $Y_t$ √© covariance-stationary.

Para complementar a discuss√£o sobre a rela√ß√£o entre strict stationarity e covariance-stationarity, podemos enunciar o seguinte resultado:

**Teorema 2.1**
Seja $Y_t$ um processo gaussiano strictly stationary. Ent√£o $Y_t$ √© covariance-stationary se e somente se sua m√©dia e autocovari√¢ncia s√£o independentes do tempo.

*Proof:*
A prova da necessidade (se $Y_t$ √© covariance-stationary, ent√£o sua m√©dia e autocovari√¢ncia s√£o independentes do tempo) segue diretamente da defini√ß√£o de covariance-stationarity.

Para a sufici√™ncia, suponha que a m√©dia e autocovari√¢ncia de $Y_t$ s√£o independentes do tempo. Como $Y_t$ √© um processo gaussiano, sua distribui√ß√£o √© completamente determinada por sua m√©dia e fun√ß√£o de autocovari√¢ncia. Uma vez que a m√©dia e a autocovari√¢ncia s√£o invariantes no tempo, segue-se que a distribui√ß√£o conjunta de $(Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n})$ √© a mesma que a distribui√ß√£o conjunta de $(Y_{t_1+h}, Y_{t_2+h}, \ldots, Y_{t_n+h})$ para qualquer $h$ e qualquer conjunto de tempos $t_1, t_2, \ldots, t_n$. Portanto, $Y_t$ √© strictly stationary.  Al√©m disso, se $Y_t$ √© strictly stationary e tem m√©dia e autocovari√¢ncia independentes do tempo, ent√£o pela defini√ß√£o ele √© tamb√©m covariance-stationary.

Al√©m dos exemplos j√° citados, √© √∫til considerar um exemplo de um processo que √© covariance-stationary, mas provavelmente n√£o √© strictly stationary.

**Exemplo:** Considere um processo $Y_t$ definido como segue:
$Y_t = \begin{cases}
    \epsilon_t       & \quad \text{se } t \text{ √© par} \\
    -\epsilon_t      & \quad \text{se } t \text{ √© √≠mpar}
\end{cases}$
onde $\epsilon_t$ √© ru√≠do branco gaussiano com m√©dia 0 e vari√¢ncia $\sigma^2$.

Este processo tem m√©dia $E[Y_t] = 0$ para todo $t$. A autocovari√¢ncia √© $E[Y_t Y_{t-j}] = E[\epsilon_t (-1)^t \epsilon_{t-j} (-1)^{t-j}] = (-1)^j E[\epsilon_t \epsilon_{t-j}]$. Assim, $\gamma_0 = \sigma^2$ e $\gamma_j = 0$ para $j \neq 0$. Portanto, este processo √© covariance-stationary. No entanto, a distribui√ß√£o conjunta de $(Y_t, Y_{t+1})$ √© diferente da distribui√ß√£o conjunta de $(Y_{t+1}, Y_{t+2})$, o que sugere que o processo n√£o √© strictly stationary. Para verificar isto, note que $Y_t$ e $Y_{t+1}$ s√£o sempre n√£o correlacionados e t√™m distribui√ß√µes sim√©tricas em torno de 0, mas $Y_t$ e $Y_{t+1}$ sempre ter√£o sinais opostos se $t$ √© par.  Portanto, este √© um exemplo de um processo covariance-stationary que n√£o √© trivialmente strictly stationary.
> üí° **Exemplo Num√©rico:** Simula√ß√£o do processo $Y_t$ mencionado acima e verifica√ß√£o da autocovari√¢ncia.
```python
import numpy as np

# Simula√ß√£o do processo
np.random.seed(42)
n = 1000
sigma = 1
epsilon = np.random.normal(0, sigma, n)
y = np.zeros(n)

for t in range(n):
    if t % 2 == 0:
        y[t] = epsilon[t]
    else:
        y[t] = -epsilon[t]

# Calcular autocovari√¢ncia amostral
def autocovariance(x, lag):
    n = len(x)
    x_mean = np.mean(x)
    sum = 0
    for i in range(lag, n):
        sum += (x[i] - x_mean) * (x[i-lag] - x_mean)
    return sum / n

gamma_0 = autocovariance(y, 0)
gamma_1 = autocovariance(y, 1)

print(f"Autocovari√¢ncia estimada no lag 0: {gamma_0:.2f}")
print(f"Autocovari√¢ncia estimada no lag 1: {gamma_1:.2f}")

# A autocovari√¢ncia no lag 0 deve ser pr√≥xima da vari√¢ncia de epsilon (1)
# A autocovari√¢ncia no lag 1 deve ser pr√≥xima de zero.
```

### Conclus√£o
A covariance-stationarity √© um conceito central na an√°lise de s√©ries temporais [^45]. Ao garantir que a m√©dia e as autocovari√¢ncias sejam constantes ao longo do tempo, ela simplifica a modelagem e permite a aplica√ß√£o de uma ampla gama de ferramentas estat√≠sticas [^45]. No entanto, √© importante lembrar que a covariance-stationarity √© uma condi√ß√£o mais fraca do que a *strict stationarity*, e nem todas as s√©ries temporais s√£o estacion√°rias. Nos cap√≠tulos subsequentes, exploraremos t√©cnicas para lidar com s√©ries n√£o estacion√°rias e modelar s√©ries temporais mais complexas.

### Refer√™ncias
[^45]: P√°g. 45, Chapter 3, Stationary ARMA Processes
[^43]: P√°g. 43, Chapter 3, Stationary ARMA Processes
[^46]: P√°g. 46, Chapter 3, Stationary ARMA Processes
[^49]: P√°g. 49, Chapter 3, Stationary ARMA Processes
[^42]: P√°g. 42, Chapter 3, Stationary ARMA Processes
<!-- END -->