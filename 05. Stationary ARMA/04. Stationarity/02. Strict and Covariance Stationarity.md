## T√≠tulo Conciso
Strict Stationarity em An√°lise de S√©ries Temporais

### Introdu√ß√£o
Este cap√≠tulo expande a discuss√£o sobre estacionaridade em s√©ries temporais, aprofundando o conceito de **strict stationarity** e contrastando-o com a covariance-stationarity, j√° apresentada no cap√≠tulo anterior. Enquanto a covariance-stationarity imp√µe restri√ß√µes sobre os dois primeiros momentos (m√©dia e autocovari√¢ncia), a strict stationarity exige uma condi√ß√£o mais forte, envolvendo a invari√¢ncia da distribui√ß√£o conjunta das observa√ß√µes ao longo do tempo [^46]. A rela√ß√£o entre essas duas formas de estacionaridade, particularmente no contexto de processos Gaussianos, ser√° explorada em detalhe.

### Conceitos Fundamentais
Um processo √© dito *strictly stationary* se a distribui√ß√£o conjunta de qualquer conjunto de observa√ß√µes $(Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n})$ √© a mesma que a distribui√ß√£o conjunta de $(Y_{t_1+h}, Y_{t_2+h}, \ldots, Y_{t_n+h})$ para qualquer $h$ e qualquer conjunto de tempos $t_1, t_2, \ldots, t_n$ [^46]. Em termos pr√°ticos, isso significa que as caracter√≠sticas estat√≠sticas da s√©rie temporal, capturadas por sua distribui√ß√£o conjunta, n√£o mudam com o tempo. A distribui√ß√£o conjunta depende apenas dos intervalos entre as datas $(j_1, j_2, \dots, j_n)$ e n√£o nas datas em si.

> üí° **Exemplo Num√©rico:** Suponha que temos uma s√©rie temporal com os seguintes valores em tr√™s pontos no tempo: $Y_1 = 2$, $Y_2 = 4$, $Y_3 = 6$. Para ser strictly stationary, a distribui√ß√£o conjunta de $(Y_1, Y_2, Y_3)$ deve ser a mesma que a distribui√ß√£o conjunta de, por exemplo, $(Y_{11}, Y_{12}, Y_{13})$, ou $(Y_{1+h}, Y_{2+h}, Y_{3+h})$ para qualquer *h*. Isso implica que as rela√ß√µes estat√≠sticas entre esses pontos (como correla√ß√µes) devem permanecer constantes ao longo do tempo.

Para clarificar a distin√ß√£o, considere a seguinte formula√ß√£o:

**Defini√ß√£o:**
> *Um processo √© saido ser strictly stationary se, para quaisquer valores de $j_1, j_2, \dots, j_n$, a distribui√ß√£o conjunta de $(Y_t, Y_{t+j_1}, Y_{t+j_2}, \dots, Y_{t+j_n})$ depende apenas dos intervalos separando as datas ($j_1, j_2, \dots, j_n$) e n√£o na data em si ($t$)* [^46].

Em outras palavras, se escolhermos um conjunto de pontos no tempo ($t, t+j_1, \dots, t+j_n$) e analisarmos sua distribui√ß√£o conjunta, essa distribui√ß√£o ser√° id√™ntica √† distribui√ß√£o conjunta de um conjunto de pontos no tempo deslocado ($t+h, t+j_1+h, \dots, t+j_n+h$), independentemente de qu√£o grande seja o deslocamento $h$.

> üí° **Exemplo Num√©rico:** Seja $j_1 = 1$ e $j_2 = 2$.  A defini√ß√£o implica que a distribui√ß√£o conjunta de $(Y_t, Y_{t+1}, Y_{t+2})$ deve ser id√™ntica √† distribui√ß√£o conjunta de $(Y_{t+h}, Y_{t+1+h}, Y_{t+2+h})$ para qualquer *h*. Se, por exemplo, $Y_t$ representa o log do pre√ßo de uma a√ß√£o, strict stationarity implicaria que a forma como o pre√ßo se move entre tr√™s dias consecutivos √© estatisticamente a mesma ao longo de todo o per√≠odo analisado.

**Teorema da Implica√ß√£o:**
> *Um processo strictly stationary com momentos de segunda ordem finitos √© sempre covariance-stationary, but the converse is not necessarily true* [^46].

Para provar a primeira parte, considere um processo $Y_t$ que √© strictly stationary com momentos de segunda ordem finitos. Ent√£o, $E[Y_t] = \mu$ para todo $t$, onde $\mu$ √© uma constante, pois a distribui√ß√£o de $Y_t$ √© a mesma para todo $t$. Al√©m disso, $E[(Y_t - \mu)(Y_{t-j} - \mu)]$ depende apenas de $j$ e n√£o de $t$, pois a distribui√ß√£o conjunta de $(Y_t, Y_{t-j})$ √© a mesma para todo $t$. Portanto, $Y_t$ √© covariance-stationary.

> üí° **Exemplo Num√©rico:** Suponha que $Y_t$ seja um processo estritamente estacion√°rio com $E[Y_t] = 5$. Ent√£o, $E[Y_{t+10}] = 5$ e $E[Y_{t-5}] = 5$. A m√©dia √© constante ao longo do tempo.  Se $Cov(Y_t, Y_{t-2}) = 1.5$, ent√£o $Cov(Y_{t+7}, Y_{t+5}) = 1.5$. A autocovari√¢ncia depende apenas do lag (neste caso, lag 2) e n√£o de *t*.

No entanto, a rec√≠proca n√£o √© sempre verdadeira. √â poss√≠vel que um processo seja covariance-stationary sem ser strictly stationary [^46]. Isso ocorre quando os dois primeiros momentos (m√©dia e autocovari√¢ncia) s√£o constantes, mas momentos de ordem superior variam com o tempo [^46]. Por exemplo, uma s√©rie temporal cuja curtose muda ao longo do tempo pode ser covariance-stationary, mas n√£o strictly stationary.

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal onde a m√©dia e a vari√¢ncia s√£o constantes ao longo do tempo (satisfazendo a covariance-stationarity). No entanto, a probabilidade de observar um valor extremo (por exemplo, 5 desvios padr√£o acima da m√©dia) aumenta significativamente em certos per√≠odos. A curtose, que mede o "tailedness" da distribui√ß√£o, estaria variando ao longo do tempo. Portanto, embora a s√©rie seja covariance-stationary, ela n√£o √© strictly stationary.

**Prova da Implica√ß√£o: Strict Stationarity $\Rightarrow$ Covariance Stationarity**

Provaremos que um processo strictly stationary com momentos de segunda ordem finitos √© tamb√©m covariance-stationary.

I.  Seja $Y_t$ um processo strictly stationary. Por defini√ß√£o, a distribui√ß√£o conjunta de $(Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n})$ √© id√™ntica √† distribui√ß√£o conjunta de $(Y_{t_1+h}, Y_{t_2+h}, \ldots, Y_{t_n+h})$ para todo $h$.

II. Em particular, a distribui√ß√£o de $Y_t$ √© a mesma para todo $t$.  Portanto, $E[Y_t] = \mu$, onde $\mu$ √© uma constante que n√£o depende de $t$.

III. Considere agora a autocovari√¢ncia entre $Y_t$ e $Y_{t-j}$, definida como $Cov(Y_t, Y_{t-j}) = E[(Y_t - \mu)(Y_{t-j} - \mu)]$.

IV. Devido √† strict stationarity, a distribui√ß√£o conjunta de $(Y_t, Y_{t-j})$ √© id√™ntica √† distribui√ß√£o conjunta de $(Y_{t+h}, Y_{t-j+h})$ para qualquer $h$.

V. Escolhendo $h = j$, temos que a distribui√ß√£o conjunta de $(Y_t, Y_{t-j})$ √© a mesma que a distribui√ß√£o conjunta de $(Y_{t+j}, Y_t)$.

VI. Portanto, $E[(Y_t - \mu)(Y_{t-j} - \mu)]$ depende apenas de $j$ e n√£o de $t$.  Isso significa que a autocovari√¢ncia $Cov(Y_t, Y_{t-j})$ √© uma fun√ß√£o apenas do lag $j$.

VII. Conclu√≠mos que $Y_t$ satisfaz as condi√ß√µes de covariance-stationarity: sua m√©dia √© constante e sua autocovari√¢ncia depende apenas do lag.

VIII. Portanto, um processo strictly stationary com momentos de segunda ordem finitos √© sempre covariance-stationary. ‚ñ†

**Lema 1:**
>Seja $Y_t$ um processo estritamente estacion√°rio. Ent√£o, para qualquer fun√ß√£o mensur√°vel $g$, o processo $Z_t = g(Y_t)$ √© tamb√©m estritamente estacion√°rio.

**Prova:**
Considere a distribui√ß√£o conjunta de $(Z_{t_1}, Z_{t_2}, \ldots, Z_{t_n}) = (g(Y_{t_1}), g(Y_{t_2}), \ldots, g(Y_{t_n}))$. Como $Y_t$ √© estritamente estacion√°rio, a distribui√ß√£o conjunta de $(Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n})$ √© a mesma que a distribui√ß√£o conjunta de $(Y_{t_1+h}, Y_{t_2+h}, \ldots, Y_{t_n+h})$ para qualquer $h$. Portanto, a distribui√ß√£o conjunta de $(g(Y_{t_1}), g(Y_{t_2}), \ldots, g(Y_{t_n}))$ √© a mesma que a distribui√ß√£o conjunta de $(g(Y_{t_1+h}), g(Y_{t_2+h}), \ldots, g(Y_{t_n+h}))$, o que implica que $Z_t$ √© estritamente estacion√°rio. $\blacksquare$

> üí° **Exemplo Num√©rico:**  Seja $Y_t$ a temperatura di√°ria em uma cidade (assumindo strict stationarity). Seja $g(x) = x^2$. Ent√£o, $Z_t = Y_t^2$ tamb√©m √© estritamente estacion√°rio. Isso significa que a distribui√ß√£o conjunta dos quadrados das temperaturas em diferentes momentos √© constante ao longo do tempo.

**Lema 1.1:**
> Seja $Y_t$ um processo estritamente estacion√°rio. Ent√£o, para quaisquer fun√ß√µes mensur√°veis $g_1, g_2, \dots, g_n$, o processo $(Z_{1,t}, Z_{2,t}, \dots, Z_{n,t}) = (g_1(Y_t), g_2(Y_t), \dots, g_n(Y_t))$ √© tamb√©m estritamente estacion√°rio.

**Prova:**
A prova √© uma extens√£o direta do Lema 1. Considere a distribui√ß√£o conjunta de $(Z_{1,t_1}, Z_{2,t_1}, \ldots, Z_{n,t_1}, Z_{1,t_2}, Z_{2,t_2}, \ldots, Z_{n,t_2}, \ldots, Z_{1,t_k}, Z_{2,t_k}, \ldots, Z_{n,t_k}) = (g_1(Y_{t_1}), g_2(Y_{t_1}), \ldots, g_n(Y_{t_1}), g_1(Y_{t_2}), g_2(Y_{t_2}), \ldots, g_n(Y_{t_2}), \ldots, g_1(Y_{t_k}), g_2(Y_{t_k}), \ldots, g_n(Y_{t_k}))$. Como $Y_t$ √© estritamente estacion√°rio, a distribui√ß√£o conjunta de $(Y_{t_1}, Y_{t_2}, \ldots, Y_{t_k})$ √© a mesma que a distribui√ß√£o conjunta de $(Y_{t_1+h}, Y_{t_2+h}, \ldots, Y_{t_k+h})$ para qualquer $h$. Portanto, a distribui√ß√£o conjunta de $(g_1(Y_{t_1}), g_2(Y_{t_1}), \ldots, g_n(Y_{t_1}), \ldots, g_1(Y_{t_k}), g_2(Y_{t_k}), \ldots, g_n(Y_{t_k}))$ √© a mesma que a distribui√ß√£o conjunta de $(g_1(Y_{t_1+h}), g_2(Y_{t_1+h}), \ldots, g_n(Y_{t_1+h}), \ldots, g_1(Y_{t_k+h}), g_2(Y_{t_k+h}), \ldots, g_n(Y_{t_k+h}))$, o que implica que $(Z_{1,t}, Z_{2,t}, \dots, Z_{n,t})$ √© estritamente estacion√°rio. $\blacksquare$

> üí° **Exemplo Num√©rico:** Seja $Y_t$ a taxa de c√¢mbio di√°ria entre o Euro e o D√≥lar Americano (assumindo strict stationarity).  Sejam $g_1(x) = log(x)$ e $g_2(x) = x^2$. Ent√£o, o processo bivariado $(Z_{1,t}, Z_{2,t}) = (log(Y_t), Y_t^2)$ tamb√©m √© estritamente estacion√°rio. Isso significa que a distribui√ß√£o conjunta dos logaritmos das taxas de c√¢mbio e dos quadrados das taxas de c√¢mbio √© constante ao longo do tempo.

**Exemplo:**
Considere um processo $Y_t$ definido como:
$Y_t = a_t \epsilon_t$

Onde $\epsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia unit√°ria, e $a_t$ √© uma vari√°vel aleat√≥ria que assume os valores 1 e 2 alternadamente ao longo do tempo, i.e., $a_t = 1$ para $t$ par e $a_t = 2$ para $t$ √≠mpar. A s√©rie $Y_t$ √© covariance-stationary, pois tem m√©dia zero e autocovari√¢ncia que depende apenas do lag:

$E[Y_t] = E[a_t \epsilon_t] = E[a_t]E[\epsilon_t] = 0$
$Cov(Y_t, Y_{t-k}) = E[Y_t Y_{t-k}] = E[a_t \epsilon_t a_{t-k} \epsilon_{t-k}] = E[a_t a_{t-k}] E[\epsilon_t \epsilon_{t-k}]$
Para $k \neq 0$, $E[\epsilon_t \epsilon_{t-k}] = 0$. Para $k = 0$, $E[a_t^2] = \frac{1}{2}(1^2 + 2^2) = \frac{5}{2}$, de forma que $Cov(Y_t, Y_t) = \frac{5}{2}$.

Apesar da estacionaridade fraca, este processo n√£o √© estritamente estacion√°rio, pois a distribui√ß√£o de $Y_t$ depende de $t$.

> üí° **Exemplo Num√©rico Detalhado:** Vamos gerar uma s√©rie temporal com as caracter√≠sticas descritas no exemplo e verificar numericamente a covariance-stationarity e a falta de strict stationarity.

![image-20250207173249808](./image-20250207173249808.png)

**Interpreta√ß√£o:** A m√©dia amostral de Y estar√° pr√≥xima de zero. A autocovari√¢ncia para lag 0 estar√° pr√≥xima de 2.5 (5/2). A autocovari√¢ncia para lag 1 estar√° pr√≥xima de zero.  Os histogramas de $Y_t$ para *t* par e √≠mpar mostrar√£o distribui√ß√µes diferentes, confirmando que a s√©rie n√£o √© estritamente estacion√°ria, mesmo sendo covariance-stationary.

**Estacionariedade para Processos Gaussianos:**
A rela√ß√£o entre covariance-stationarity e strict stationarity se torna mais clara no contexto de processos Gaussianos.

**Teorema:**
> *Para processos Gaussianos, a covariance-stationarity implica strict stationarity* [^46].

Isso ocorre porque a distribui√ß√£o conjunta de um vetor gaussiano √© completamente determinada por seu vetor de m√©dias e matriz de covari√¢ncias. Se a m√©dia e a matriz de covari√¢ncias s√£o independentes do tempo (condi√ß√£o de covariance-stationarity), ent√£o a distribui√ß√£o conjunta tamb√©m √© independente do tempo (condi√ß√£o de strict stationarity) [^46].

**Prova:**
Se $Y_t$ √© um processo gaussiano covariance-stationary, ent√£o $E[Y_t] = \mu$ (constante) e $Cov(Y_t, Y_{t-j}) = \gamma_j$ (dependente apenas de $j$). Considere o vetor $(Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n})$. Este √© um vetor gaussiano, e sua distribui√ß√£o √© completamente determinada por seu vetor de m√©dias e matriz de covari√¢ncias. O vetor de m√©dias tem todos os elementos iguais a $\mu$, e os elementos da matriz de covari√¢ncias s√£o da forma $Cov(Y_{t_i}, Y_{t_j}) = \gamma_{|t_i - t_j|}$. Agora, considere o vetor $(Y_{t_1+h}, Y_{t_2+h}, \ldots, Y_{t_n+h})$. Este tamb√©m √© um vetor gaussiano, e seu vetor de m√©dias tem todos os elementos iguais a $\mu$, e os elementos da matriz de covari√¢ncias s√£o da forma $Cov(Y_{t_i+h}, Y_{t_j+h}) = \gamma_{|(t_i+h) - (t_j+h)|} = \gamma_{|t_i - t_j|}$. Portanto, os dois vetores t√™m a mesma distribui√ß√£o, e $Y_t$ √© strictly stationary. $\blacksquare$

> üí° **Exemplo Num√©rico:** Seja $Y_t$ um processo Gaussiano com m√©dia 0 e vari√¢ncia 1.  Se $Cov(Y_t, Y_{t+1}) = 0.5$ para todo *t*, ent√£o a distribui√ß√£o conjunta de $(Y_1, Y_2, Y_3)$ √© uma normal multivariada com vetor de m√©dias $(0, 0, 0)$ e matriz de covari√¢ncias:
>
> ```
>       | 1.0  0.5  0.0 |
>       | 0.5  1.0  0.5 |
>       | 0.0  0.5  1.0 |
> ```
>
> A distribui√ß√£o conjunta de $(Y_2, Y_3, Y_4)$ tamb√©m ter√° o mesmo vetor de m√©dias e matriz de covari√¢ncias, confirmando a strict stationarity.

**Teorema 1:**
> Se $Y_t$ √© um processo linear da forma $Y_t = \sum_{i=-\infty}^{\infty} a_i \epsilon_{t-i}$, onde $\epsilon_t$ √© um ru√≠do branco estritamente estacion√°rio com $E[\epsilon_t] = 0$ e $\sum_{i=-\infty}^{\infty} |a_i| < \infty$, ent√£o $Y_t$ √© estritamente estacion√°rio.

**Prova:**
Seja $Y_t = \sum_{i=-\infty}^{\infty} a_i \epsilon_{t-i}$. Considere o vetor $(Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n})$. Ent√£o,
$(Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n}) = (\sum_{i=-\infty}^{\infty} a_i \epsilon_{t_1-i}, \sum_{i=-\infty}^{\infty} a_i \epsilon_{t_2-i}, \ldots, \sum_{i=-\infty}^{\infty} a_i \epsilon_{t_n-i})$.
Agora, considere o vetor $(Y_{t_1+h}, Y_{t_2+h}, \ldots, Y_{t_n+h})$. Ent√£o,
$(Y_{t_1+h}, Y_{t_2+h}, \ldots, Y_{t_n+h}) = (\sum_{i=-\infty}^{\infty} a_i \epsilon_{t_1+h-i}, \sum_{i=-\infty}^{\infty} a_i \epsilon_{t_2+h-i}, \ldots, \sum_{i=-\infty}^{\infty} a_i \epsilon_{t_n+h-i})$.
Fazendo a mudan√ßa de vari√°vel $j = i - h$, temos:
$(Y_{t_1+h}, Y_{t_2+h}, \ldots, Y_{t_n+h}) = (\sum_{j=-\infty}^{\infty} a_{j+h} \epsilon_{t_1-j}, \sum_{j=-\infty}^{\infty} a_{j+h} \epsilon_{t_2-j}, \ldots, \sum_{j=-\infty}^{\infty} a_{j+h} \epsilon_{t_n-j})$.
Como $\epsilon_t$ √© estritamente estacion√°rio, a distribui√ß√£o conjunta de $(\epsilon_{t_1-i_1}, \epsilon_{t_2-i_2}, \ldots, \epsilon_{t_n-i_n})$ √© a mesma que a distribui√ß√£o conjunta de $(\epsilon_{t_1+h-i_1}, \epsilon_{t_2+h-i_2}, \ldots, \epsilon_{t_n+h-i_n})$ para quaisquer $i_1, i_2, \dots, i_n$. Portanto, a distribui√ß√£o conjunta de $(Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n})$ √© a mesma que a distribui√ß√£o conjunta de $(Y_{t_1+h}, Y_{t_2+h}, \ldots, Y_{t_n+h})$, o que implica que $Y_t$ √© estritamente estacion√°rio. $\blacksquare$

> üí° **Exemplo Num√©rico:** Seja $\epsilon_t$ um ru√≠do branco estritamente estacion√°rio com m√©dia 0 e vari√¢ncia 1.  Considere o processo $Y_t = 0.5\epsilon_{t-1} + 0.3\epsilon_t + 0.2\epsilon_{t+1}$.  Aqui, $a_{-1} = 0.5$, $a_0 = 0.3$, $a_1 = 0.2$, e todos os outros $a_i$ s√£o zero.  A condi√ß√£o $\sum_{i=-\infty}^{\infty} |a_i| < \infty$ √© satisfeita (0.5 + 0.3 + 0.2 = 1.0).  Portanto, pelo Teorema 1, $Y_t$ √© estritamente estacion√°rio.

**Teorema 2:**
> Seja $Y_t$ um processo estritamente estacion√°rio com m√©dia zero e fun√ß√£o de autocovari√¢ncia $\gamma(k)$. Se $\sum_{k=-\infty}^{\infty} |\gamma(k)| < \infty$, ent√£o a m√©dia amostral $\bar{Y} = \frac{1}{T} \sum_{t=1}^{T} Y_t$ converge em m√©dia quadr√°tica para zero, i.e., $E[(\bar{Y} - 0)^2] \rightarrow 0$ quando $T \rightarrow \infty$.

**Prova:**
Primeiramente, calculamos o valor esperado de $\bar{Y}$:
$E[\bar{Y}] = E[\frac{1}{T} \sum_{t=1}^{T} Y_t] = \frac{1}{T} \sum_{t=1}^{T} E[Y_t] = \frac{1}{T} \sum_{t=1}^{T} 0 = 0$.
Agora, calculamos o segundo momento de $\bar{Y}$:
$E[\bar{Y}^2] = E[(\frac{1}{T} \sum_{t=1}^{T} Y_t)^2] = \frac{1}{T^2} E[\sum_{t=1}^{T} \sum_{s=1}^{T} Y_t Y_s] = \frac{1}{T^2} \sum_{t=1}^{T} \sum_{s=1}^{T} E[Y_t Y_s] = \frac{1}{T^2} \sum_{t=1}^{T} \sum_{s=1}^{T} \gamma(t-s)$.
Fazendo a mudan√ßa de vari√°vel $k = t-s$, temos:
$E[\bar{Y}^2] = \frac{1}{T^2} \sum_{t=1}^{T} \sum_{k=t-T}^{t-1} \gamma(k)$.
Podemos reescrever a soma dupla como:
$E[\bar{Y}^2] = \frac{1}{T^2} \sum_{k=-(T-1)}^{T-1} (T - |k|) \gamma(k) = \frac{1}{T} \sum_{k=-(T-1)}^{T-1} (1 - \frac{|k|}{T}) \gamma(k)$.
Como $\sum_{k=-\infty}^{\infty} |\gamma(k)| < \infty$, podemos aplicar o teorema da converg√™ncia dominada:
$\lim_{T \rightarrow \infty} E[\bar{Y}^2] = \lim_{T \rightarrow \infty} \frac{1}{T} \sum_{k=-(T-1)}^{T-1} (1 - \frac{|k|}{T}) \gamma(k) = 0$.
Portanto, a m√©dia amostral converge em m√©dia quadr√°tica para zero. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que $Y_t$ seja um processo estritamente estacion√°rio com m√©dia zero e fun√ß√£o de autocovari√¢ncia $\gamma(k) = 0.8^{|k|}$.  Ent√£o, $\sum_{k=-\infty}^{\infty} |\gamma(k)| = \sum_{k=-\infty}^{\infty} 0.8^{|k|} = \frac{1 + 0.8}{1 - 0.8} = 9 < \infty$.  Portanto, a m√©dia amostral $\bar{Y}$ converge em m√©dia quadr√°tica para zero.  Isso significa que, √† medida que o tamanho da amostra *T* aumenta, a vari√¢ncia da m√©dia amostral diminui e se aproxima de zero, indicando que $\bar{Y}$ se torna uma estimativa cada vez mais precisa da verdadeira m√©dia (que √© zero).

### Conclus√£o
A strict stationarity √© uma condi√ß√£o mais forte do que a covariance-stationarity, exigindo que a distribui√ß√£o conjunta das observa√ß√µes seja invariante ao longo do tempo [^46]. Embora um processo strictly stationary com momentos de segunda ordem finitos seja sempre covariance-stationary, o inverso n√£o √© necessariamente verdadeiro. No entanto, para processos Gaussianos, a covariance-stationarity √© equivalente √† strict stationarity. A compreens√£o da diferen√ßa entre essas duas formas de estacionaridade √© crucial para a modelagem e an√°lise de s√©ries temporais.

### Refer√™ncias
[^46]: P√°g. 46, Chapter 3, Stationary ARMA Processes
## Modelos de S√©ries Temporais

### Modelos AR (Autorregressivos)

Os modelos autorregressivos (AR) s√£o um tipo de modelo de s√©rie temporal que utiliza os valores passados da s√©rie para prever seus valores futuros. A ordem $p$ de um modelo AR, denotada como AR($p$), indica quantos valores passados s√£o usados na previs√£o. A equa√ß√£o geral para um modelo AR($p$) √©:

$$X_t = c + \sum_{i=1}^{p} \phi_i X_{t-i} + \epsilon_t$$

onde:
*   $X_t$ √© o valor da s√©rie no tempo $t$.
*   $c$ √© uma constante.
*   $\phi_i$ s√£o os coeficientes do modelo.
*   $\epsilon_t$ √© o ru√≠do branco no tempo $t$.

Para garantir a estacionariedade de um modelo AR($p$), as ra√≠zes do polin√¥mio caracter√≠stico associado devem estar fora do c√≠rculo unit√°rio. O polin√¥mio caracter√≠stico √© dado por:

$$1 - \sum_{i=1}^{p} \phi_i z^i = 0$$

onde $z$ representa as ra√≠zes do polin√¥mio.

### Modelos MA (M√©dias M√≥veis)

Os modelos de m√©dias m√≥veis (MA) s√£o outro tipo de modelo de s√©rie temporal que utiliza os erros passados (ru√≠do branco) para prever os valores futuros da s√©rie. A ordem $q$ de um modelo MA, denotada como MA($q$), indica quantos erros passados s√£o usados na previs√£o. A equa√ß√£o geral para um modelo MA($q$) √©:

$$X_t = \mu + \sum_{i=1}^{q} \theta_i \epsilon_{t-i} + \epsilon_t$$

onde:
*   $X_t$ √© o valor da s√©rie no tempo $t$.
*   $\mu$ √© a m√©dia da s√©rie.
*   $\theta_i$ s√£o os coeficientes do modelo.
*   $\epsilon_t$ √© o ru√≠do branco no tempo $t$.

Os modelos MA s√£o sempre estacion√°rios, independentemente dos valores dos coeficientes $\theta_i$. No entanto, para garantir a invertibilidade, as ra√≠zes do polin√¥mio caracter√≠stico associado devem estar fora do c√≠rculo unit√°rio. O polin√¥mio caracter√≠stico para um modelo MA($q$) √© dado por:

$$1 + \sum_{i=1}^{q} \theta_i z^i = 0$$

### Modelos ARMA (Autorregressivos de M√©dias M√≥veis)

Os modelos ARMA combinam as caracter√≠sticas dos modelos AR e MA. Um modelo ARMA($p, q$) utiliza $p$ valores passados da s√©rie e $q$ erros passados para prever os valores futuros da s√©rie. A equa√ß√£o geral para um modelo ARMA($p, q$) √©:

$$X_t = c + \sum_{i=1}^{p} \phi_i X_{t-i} + \sum_{i=1}^{q} \theta_i \epsilon_{t-i} + \epsilon_t$$

onde:

*   $X_t$ √© o valor da s√©rie no tempo $t$.
*   $c$ √© uma constante.
*   $\phi_i$ s√£o os coeficientes autorregressivos.
*   $\theta_i$ s√£o os coeficientes de m√©dias m√≥veis.
*   $\epsilon_t$ √© o ru√≠do branco no tempo $t$.

Para que um modelo ARMA($p, q$) seja estacion√°rio, as ra√≠zes do polin√¥mio autorregressivo devem estar fora do c√≠rculo unit√°rio. Para a invertibilidade, as ra√≠zes do polin√¥mio de m√©dias m√≥veis tamb√©m devem estar fora do c√≠rculo unit√°rio.

### Modelos ARIMA (Autorregressivos Integrados de M√©dias M√≥veis)

Os modelos ARIMA s√£o uma extens√£o dos modelos ARMA que incluem um componente de integra√ß√£o ($I$). Um modelo ARIMA($p, d, q$) envolve $p$ termos autorregressivos, $d$ diferen√ßas para tornar a s√©rie estacion√°ria e $q$ termos de m√©dias m√≥veis. A diferencia√ß√£o √© uma t√©cnica utilizada para remover a n√£o estacionariedade em uma s√©rie temporal, calculando a diferen√ßa entre valores consecutivos.

Seja $Y_t$ a s√©rie temporal diferenciada $d$ vezes. Ent√£o, $Y_t$ pode ser modelada usando um modelo ARMA($p, q$):

$$Y_t = c + \sum_{i=1}^{p} \phi_i Y_{t-i} + \sum_{i=1}^{q} \theta_i \epsilon_{t-i} + \epsilon_t$$

E $X_t$ √© a s√©rie original, ent√£o:

$$Y_t = (1 - B)^d X_t$$

Onde $B$ √© o operador de retrocesso, tal que $BX_t = X_{t-1}$.
<!-- END -->