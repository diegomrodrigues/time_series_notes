## Invertibilidade em Processos MA(1): Autocovari√¢ncia e N√£o-Unicidade

### Introdu√ß√£o
Este cap√≠tulo aprofunda a discuss√£o sobre a **invertibilidade** em processos *Moving Average* de ordem 1 (MA(1)). Em continuidade com o t√≥pico anterior, exploraremos como, para cada representa√ß√£o MA(1) invert√≠vel, existe uma representa√ß√£o n√£o invert√≠vel com $|\theta| > 1$ que compartilha os mesmos primeiros e segundos momentos. Esta equival√™ncia implica que a estrutura de autocovari√¢ncia, por si s√≥, n√£o √© suficiente para identificar unicamente os par√¢metros do modelo, destacando a import√¢ncia da condi√ß√£o de invertibilidade para a estima√ß√£o e interpreta√ß√£o [^3.7.5, ^3.7.6, ^3.7.7]. A an√°lise se ancora nos conceitos de estacionariedade, causalidade, e representa√ß√µes AR($\infty$) j√° apresentadas [^45, ^46, ^47, ^48]. Al√©m disso, vale ressaltar que a invertibilidade est√° diretamente relacionada √† capacidade de expressar o ru√≠do branco como uma combina√ß√£o linear dos valores passados do processo, o que facilita a interpreta√ß√£o e a previs√£o.

### Autocovari√¢ncia e N√£o-Unicidade em Modelos MA(1)
Como vimos, um processo MA(1) √© definido por:

$$Y_t = \mu + (1 + \theta L)\epsilon_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$$ [^3.7.1]

onde $\mu$ √© a m√©dia do processo, $\theta$ √© o coeficiente do termo *moving average*, $L$ √© o operador de *lag* e $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$ [^3.2.1, ^3.2.2, ^3.2.3]. A condi√ß√£o de invertibilidade exige que $|\theta| < 1$.

> üí° **Exemplo Num√©rico:** Considere um processo MA(1) com $\mu = 10$, $\theta = 0.5$, e $\sigma^2 = 1$.  Cada valor $Y_t$ √© gerado como $Y_t = 10 + \epsilon_t + 0.5\epsilon_{t-1}$, onde $\epsilon_t$ √© um valor aleat√≥rio retirado de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 1.  Simulando 100 valores para $\epsilon_t$, podemos calcular uma s√©rie temporal $Y_t$. A condi√ß√£o de invertibilidade $|\theta| < 1$ √© satisfeita, pois $|0.5| < 1$.

A fun√ß√£o de autocovari√¢ncia para este processo √© dada por:
$$ \gamma_0 = \sigma^2 (1 + \theta^2) $$
$$ \gamma_1 = \sigma^2 \theta $$
$$ \gamma_j = 0 \text{ para } j > 1 $$ [^3.7.3, ^3.3.5]

Consideremos agora um processo MA(1) "dual" definido como:
$$\tilde{Y}_t = \mu + (1 + \tilde{\theta} L)\tilde{\epsilon}_t = \mu + \tilde{\epsilon}_t + \tilde{\theta}\tilde{\epsilon}_{t-1}$$ [^3.7.4]
onde $\tilde{\theta}$ √© o coeficiente do termo *moving average* dual, e $\tilde{\epsilon}_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\tilde{\sigma}^2$.

A fun√ß√£o de autocovari√¢ncia para o processo dual √© dada por:
$$ \tilde{\gamma}_0 = \tilde{\sigma}^2 (1 + \tilde{\theta}^2) $$
$$ \tilde{\gamma}_1 = \tilde{\sigma}^2 \tilde{\theta} $$
$$ \tilde{\gamma}_j = 0 \text{ para } j > 1 $$

**Teorema 1:** *Equival√™ncia da Autocovari√¢ncia*

Se os par√¢metros dos dois processos estiverem relacionados por:
$$\tilde{\theta} = \frac{1}{\theta}$$ [^3.7.6]
$$\tilde{\sigma}^2 = \theta^2 \sigma^2$$ [^3.7.7]
ent√£o os dois processos, $Y_t$ e $\tilde{Y}_t$, ter√£o a mesma fun√ß√£o de autocovari√¢ncia.

*Prova:*

Para que os processos $Y_t$ e $\tilde{Y}_t$ tenham a mesma fun√ß√£o de autocovari√¢ncia, devemos ter:
$$ \gamma_0 = \tilde{\gamma}_0 \text{ e } \gamma_1 = \tilde{\gamma}_1 $$
Substituindo as express√µes para $\tilde{\theta}$ e $\tilde{\sigma}^2$, obtemos:
$$ \tilde{\gamma}_0 = \tilde{\sigma}^2 (1 + \tilde{\theta}^2) = \theta^2 \sigma^2 \left(1 + \frac{1}{\theta^2}\right) = \theta^2 \sigma^2 + \sigma^2 = \sigma^2 (1 + \theta^2) = \gamma_0 $$
$$ \tilde{\gamma}_1 = \tilde{\sigma}^2 \tilde{\theta} = \theta^2 \sigma^2 \left(\frac{1}{\theta}\right) = \sigma^2 \theta = \gamma_1 $$

Portanto, as fun√ß√µes de autocovari√¢ncia dos dois processos s√£o id√™nticas. $\blacksquare$

**Teorema 1.1:** *Rela√ß√£o entre as Fun√ß√µes de Autocorrela√ß√£o*
Os processos $Y_t$ e $\tilde{Y}_t$ possuem a mesma fun√ß√£o de autocorrela√ß√£o.

*Prova:*
A fun√ß√£o de autocorrela√ß√£o √© definida como $\rho_j = \frac{\gamma_j}{\gamma_0}$.
Para o processo $Y_t$, temos $\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{\sigma^2 \theta}{\sigma^2 (1 + \theta^2)} = \frac{\theta}{1 + \theta^2}$.
Para o processo $\tilde{Y}_t$, temos $\tilde{\rho}_1 = \frac{\tilde{\gamma}_1}{\tilde{\gamma}_0} = \frac{\tilde{\sigma}^2 \tilde{\theta}}{\tilde{\sigma}^2 (1 + \tilde{\theta}^2)} = \frac{\theta^2 \sigma^2 (\frac{1}{\theta})}{\theta^2 \sigma^2 (1 + (\frac{1}{\theta})^2)} = \frac{\theta}{\theta^2 (1 + \frac{1}{\theta^2})} = \frac{\theta}{\theta^2 + 1} = \frac{\theta}{1 + \theta^2}$.
Como $\rho_1 = \tilde{\rho}_1$ e $\rho_j = \tilde{\rho}_j = 0$ para $j > 1$, as fun√ß√µes de autocorrela√ß√£o s√£o id√™nticas. $\blacksquare$

**Implica√ß√£o Crucial:**

Se $|\theta| < 1$, ent√£o $|\tilde{\theta}| = \left|\frac{1}{\theta}\right| > 1$. Isso significa que um dos processos (com $\theta$) √© invert√≠vel, enquanto o outro (com $\tilde{\theta}$) n√£o √©. No entanto, ambos os processos exibem exatamente a mesma estrutura de autocovari√¢ncia, o que torna imposs√≠vel distingui-los apenas com base na an√°lise dos primeiros e segundos momentos.

> üí° **Exemplo Num√©rico:** Seja $\theta = 0.5$ e $\sigma^2 = 1$. Ent√£o $\tilde{\theta} = \frac{1}{0.5} = 2$ e $\tilde{\sigma}^2 = (0.5)^2 \times 1 = 0.25$.
>
> Para o processo invert√≠vel:
> $\gamma_0 = 1 \times (1 + 0.5^2) = 1.25$
> $\gamma_1 = 1 \times 0.5 = 0.5$
>
> Para o processo n√£o-invert√≠vel:
> $\tilde{\gamma}_0 = 0.25 \times (1 + 2^2) = 0.25 \times 5 = 1.25$
> $\tilde{\gamma}_1 = 0.25 \times 2 = 0.5$
>
> Ambos os processos t√™m a mesma fun√ß√£o de autocovari√¢ncia, mas apenas o primeiro √© invert√≠vel.
>
> ```python
> import numpy as np
>
> theta = 0.5
> sigma_squared = 1
>
> gamma_0 = sigma_squared * (1 + theta**2)
> gamma_1 = sigma_squared * theta
>
> print(f"Invertible MA(1): gamma_0 = {gamma_0}, gamma_1 = {gamma_1}")
>
> theta_tilde = 1 / theta
> sigma_squared_tilde = theta**2 * sigma_squared
>
> gamma_0_tilde = sigma_squared_tilde * (1 + theta_tilde**2)
> gamma_1_tilde = sigma_squared_tilde * theta_tilde
>
> print(f"Non-invertible MA(1): gamma_0 = {gamma_0_tilde}, gamma_1 = {gamma_1_tilde}")
> ```

**Teorema 2:** *N√£o-Unicidade da Identifica√ß√£o*

Para um dado processo MA(1) com autocovari√¢ncias $\gamma_0$ e $\gamma_1$, existem dois modelos MA(1) que geram essas autocovari√¢ncias, um dos quais √© invert√≠vel e o outro n√£o.

*Prova:*
Sejam $\theta$ e $\sigma^2$ os par√¢metros de um processo MA(1). A autocovari√¢ncia no lag 1 √© dada por $\gamma_1 = \sigma^2 \theta$ e a vari√¢ncia √© dada por $\gamma_0 = \sigma^2(1 + \theta^2)$. Se observamos $\gamma_0$ e $\gamma_1$, podemos resolver o seguinte sistema de equa√ß√µes:

$$ \begin{cases}
\gamma_1 = \sigma^2 \theta \\
\gamma_0 = \sigma^2(1 + \theta^2)
\end{cases} $$
Dividindo a primeira equa√ß√£o pela segunda, temos:

$$ \frac{\gamma_1}{\gamma_0} = \frac{\sigma^2 \theta}{\sigma^2(1 + \theta^2)} = \frac{\theta}{1 + \theta^2} $$
Resolvendo para $\theta$:

$$ \theta = \frac{\gamma_1}{\gamma_0}(1 + \theta^2) \Rightarrow \frac{\gamma_1}{\gamma_0}\theta^2 - \theta + \frac{\gamma_1}{\gamma_0} = 0 $$
Esta √© uma equa√ß√£o quadr√°tica em $\theta$. Usando a f√≥rmula quadr√°tica, temos:
$$ \theta = \frac{1 \pm \sqrt{1 - 4(\frac{\gamma_1}{\gamma_0})^2}}{2(\frac{\gamma_1}{\gamma_0})} $$

I. A equa√ß√£o quadr√°tica √© derivada diretamente do sistema de equa√ß√µes da autocovari√¢ncia.
II. A aplica√ß√£o da f√≥rmula quadr√°tica √© um passo alg√©brico padr√£o.
III. A interpreta√ß√£o das duas solu√ß√µes √© que uma corresponder√° ao caso invert√≠vel e a outra ao caso n√£o invert√≠vel.

Esta equa√ß√£o tem duas solu√ß√µes para $\theta$, e a menos que a autocorrela√ß√£o no lag 1 seja exatamente 0.5 (no qual caso teremos uma raiz dupla), teremos um valor menor que 1 em valor absoluto (e, portanto, invert√≠vel) e um valor maior que 1 em valor absoluto (e, portanto, n√£o invert√≠vel). $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Se $\gamma_0 = 1.25$ e $\gamma_1 = 0.5$, ent√£o a equa√ß√£o quadr√°tica se torna:
> $\frac{0.5}{1.25}\theta^2 - \theta + \frac{0.5}{1.25} = 0$, ou $0.4\theta^2 - \theta + 0.4 = 0$
> As solu√ß√µes s√£o:
> $\theta = \frac{1 \pm \sqrt{1 - 4(0.4)^2}}{2(0.4)} = \frac{1 \pm \sqrt{1 - 0.64}}{0.8} = \frac{1 \pm \sqrt{0.36}}{0.8} = \frac{1 \pm 0.6}{0.8}$
>
> $\theta_1 = \frac{1 + 0.6}{0.8} = \frac{1.6}{0.8} = 2$
> $\theta_2 = \frac{1 - 0.6}{0.8} = \frac{0.4}{0.8} = 0.5$
>
> Uma solu√ß√£o √© $\theta = 0.5$, que √© invert√≠vel, e a outra √© $\theta = 2$, que n√£o √© invert√≠vel.
>
> ```python
> import numpy as np
>
> gamma_0 = 1.25
> gamma_1 = 0.5
>
> a = gamma_1 / gamma_0
> b = -1
> c = gamma_1 / gamma_0
>
> discriminant = b**2 - 4 * a * c
>
> if discriminant >= 0:
>     theta_1 = (-b + np.sqrt(discriminant)) / (2 * a)
>     theta_2 = (-b - np.sqrt(discriminant)) / (2 * a)
>
>     print(f"Solutions for theta: theta_1 = {theta_1}, theta_2 = {theta_2}")
> else:
>     print("No real solutions for theta")
> ```

**Teorema 2.1:** *Condi√ß√£o para Ra√≠zes Reais*
A equa√ß√£o quadr√°tica para $\theta$ tem ra√≠zes reais se e somente se $|\rho_1| \leq 0.5$, onde $\rho_1 = \frac{\gamma_1}{\gamma_0}$ √© a autocorrela√ß√£o no lag 1.

*Prova:*
A equa√ß√£o quadr√°tica para $\theta$ √© dada por $\frac{\gamma_1}{\gamma_0}\theta^2 - \theta + \frac{\gamma_1}{\gamma_0} = 0$. O discriminante desta equa√ß√£o √© $\Delta = 1 - 4(\frac{\gamma_1}{\gamma_0})^2 = 1 - 4\rho_1^2$.
Para que as ra√≠zes sejam reais, o discriminante deve ser n√£o negativo, ou seja, $\Delta \geq 0$.
Portanto, $1 - 4\rho_1^2 \geq 0 \Rightarrow 4\rho_1^2 \leq 1 \Rightarrow \rho_1^2 \leq \frac{1}{4} \Rightarrow |\rho_1| \leq \frac{1}{2} = 0.5$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Se $\gamma_0 = 1.25$ e $\gamma_1 = 0.7$, ent√£o $\rho_1 = \frac{0.7}{1.25} = 0.56$. Como $|\rho_1| = 0.56 > 0.5$, a equa√ß√£o quadr√°tica n√£o ter√° ra√≠zes reais.  Isto significa que n√£o existe um processo MA(1) com autocovari√¢ncias $\gamma_0 = 1.25$ e $\gamma_1 = 0.7$.
>
> Se $\gamma_0 = 1.25$ e $\gamma_1 = 0.5$, ent√£o $\rho_1 = \frac{0.5}{1.25} = 0.4$. Como $|\rho_1| = 0.4 < 0.5$, a equa√ß√£o quadr√°tica ter√° ra√≠zes reais.

**Implica√ß√µes Pr√°ticas:**

1.  **Estima√ß√£o e Identifica√ß√£o:** A equival√™ncia na estrutura de autocovari√¢ncia implica que diferentes modelos MA(1) podem se ajustar aos dados de forma semelhante [^3.7.5, ^3.7.6, ^3.7.7]. Isso torna a identifica√ß√£o do modelo correto mais desafiadora. Para garantir a unicidade, a condi√ß√£o de invertibilidade deve ser imposta durante a estima√ß√£o.
2.  **Interpreta√ß√£o:** A escolha entre a representa√ß√£o invert√≠vel e n√£o invert√≠vel afeta a interpreta√ß√£o dos choques $\epsilon_t$. Apenas a representa√ß√£o invert√≠vel permite uma interpreta√ß√£o causal e consistente das inova√ß√µes.
3.  **Previs√£o:** Embora ambos os modelos possam gerar as mesmas autocovari√¢ncias, suas propriedades de previs√£o podem ser diferentes, especialmente em horizontes de longo prazo. Modelos n√£o invert√≠veis podem levar a previs√µes inst√°veis e pouco confi√°veis.

**Estrat√©gias para Lidar com a N√£o-Unicidade:**

1.  **Imposi√ß√£o da Invertibilidade:** A forma mais comum de lidar com a n√£o-unicidade √© impor a condi√ß√£o de invertibilidade ($|\theta| < 1$) durante a estima√ß√£o dos par√¢metros do modelo. Isso garante que o modelo estimado seja causal e que a inova√ß√£o fundamental possa ser interpretada de forma consistente.
2.  **An√°lise da Fun√ß√£o de Autocorrela√ß√£o Parcial (FACP):** A FACP pode ajudar a distinguir entre modelos MA(1) invert√≠veis e n√£o invert√≠veis. Um processo MA(1) invert√≠vel tem uma FACP que decai exponencialmente, enquanto um processo n√£o invert√≠vel n√£o exibe esse padr√£o.
3.  **Crit√©rios de Sele√ß√£o de Modelo:** Crit√©rios como o *Akaike Information Criterion* (AIC) ou o *Bayesian Information Criterion* (BIC) podem ser usados para comparar diferentes modelos e selecionar aquele que melhor se ajusta aos dados, penalizando a complexidade do modelo.

**Lema 1** (Unicidade da Inova√ß√£o): Para um processo MA(1) invert√≠vel, a inova√ß√£o $\epsilon_t$ √© unicamente determinada pelos valores passados e presentes de $Y_t$.

*Prova:*
Dado que a representa√ß√£o AR($\infty$) converge para um processo invert√≠vel, existe uma √∫nica sequ√™ncia de coeficientes que permite expressar $\epsilon_t$ como uma fun√ß√£o linear dos valores passados e presente de $Y_t$:

$$ \epsilon_t = \sum_{i=0}^{\infty} a_i Y_{t-i} $$

onde os $a_i$ s√£o os coeficientes da representa√ß√£o AR($\infty$). Se existisse outra representa√ß√£o com diferentes coeficientes, ela geraria diferentes valores para $\epsilon_t$, contradizendo a unicidade da inova√ß√£o. $\blacksquare$

> üí° **Exemplo Num√©rico:** Se $\gamma_0 = 1.25$ e $\gamma_1 = 0.5$, ent√£o a equa√ß√£o quadr√°tica tem duas ra√≠zes $\theta_1 = 0.5$ e $\theta_2 = 2$.
> A representa√ß√£o invert√≠vel (com $\theta = 0.5$) permite expressar $\epsilon_t$ unicamente como:
> $\epsilon_t = (Y_t - \mu) - 0.5(Y_{t-1} - \mu) + 0.25(Y_{t-2} - \mu) - \dots$
> A representa√ß√£o n√£o invert√≠vel (com $\theta = 2$) n√£o fornece uma forma √∫nica de expressar $\epsilon_t$ com coeficientes que convergem.
>
> Para $\theta = 0.5$, os coeficientes da representa√ß√£o AR($\infty$) s√£o $1, -0.5, 0.25, -0.125, \dots$ que convergem para 0. Para $\theta = 2$, os coeficientes seriam $1, -2, 4, -8, \dots$ que divergem.

**Lema 1.1:** A representa√ß√£o AR($\infty$) de um processo MA(1) n√£o-invert√≠vel n√£o converge.

*Prova:*
A representa√ß√£o AR($\infty$) √© obtida invertendo o operador $(1 + \theta L)$. Para um processo MA(1) invert√≠vel, $|\theta| < 1$, e a expans√£o $(1 + \theta L)^{-1} = 1 - \theta L + \theta^2 L^2 - \theta^3 L^3 + \dots$ converge absolutamente. No entanto, para um processo n√£o-invert√≠vel, $|\theta| > 1$, e a expans√£o n√£o converge, pois os coeficientes $\theta^i$ crescem em magnitude √† medida que $i$ aumenta. Portanto, a representa√ß√£o AR($\infty$) n√£o converge para um processo MA(1) n√£o-invert√≠vel. $\blacksquare$

**Proposi√ß√£o 1:** A representa√ß√£o AR($\infty$) de um processo MA(1) invert√≠vel √© absolutamente convergente.

*Prova:*
Para um processo MA(1) invert√≠vel, temos $|\theta| < 1$. A representa√ß√£o AR($\infty$) √© dada por $(1 + \theta L)^{-1} = \sum_{i=0}^{\infty} (-\theta)^i L^i = 1 - \theta L + \theta^2 L^2 - \theta^3 L^3 + \ldots$.

I. Come√ßamos com a representa√ß√£o AR($\infty$) de um processo MA(1) invert√≠vel, $|\theta| < 1$.
II. Expandimos a express√£o $(1 + \theta L)^{-1}$ como uma s√©rie.
III. Observamos que a converg√™ncia da s√©rie depende da magnitude de $\theta$.

A soma dos valores absolutos dos coeficientes √© $\sum_{i=0}^{\infty} |-\theta|^i = \sum_{i=0}^{\infty} |\theta|^i$.
Como $|\theta| < 1$, esta √© uma s√©rie geom√©trica convergente com soma $\frac{1}{1 - |\theta|}$. Portanto, a representa√ß√£o AR($\infty$) √© absolutamente convergente. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Considere $\theta = 0.8$. A representa√ß√£o AR($\infty$) √© $1 - 0.8L + 0.64L^2 - 0.512L^3 + ...$. A soma dos valores absolutos dos coeficientes √© $1 + 0.8 + 0.64 + 0.512 + ... = \frac{1}{1 - 0.8} = 5$. Portanto, a representa√ß√£o √© absolutamente convergente.

### Conclus√£o
Este cap√≠tulo demonstrou que, embora processos MA(1) invert√≠veis e n√£o invert√≠veis possam compartilhar a mesma estrutura de autocovari√¢ncia, a condi√ß√£o de invertibilidade √© crucial para garantir a unicidade da representa√ß√£o do modelo, a interpretabilidade dos resultados e a estabilidade das previs√µes. Impor a invertibilidade durante a estima√ß√£o √© essencial para evitar ambiguidades e obter resultados consistentes e confi√°veis na an√°lise de s√©ries temporais. A condi√ß√£o $|\theta| < 1$ garante que podemos expressar o ru√≠do branco como uma fun√ß√£o linear dos valores presentes e passados da s√©rie temporal.

### Refer√™ncias
[^3.2.1]: Defini√ß√£o de ru√≠do branco
[^3.2.2]: Defini√ß√£o de ru√≠do branco
[^3.2.3]: Defini√ß√£o de ru√≠do branco
[^3.3.5]: Higher Autocovariances
[^3.7.1]: Processo MA(1)
[^3.7.2]: Representa√ß√£o AR($\infty$)
[^3.7.3]: Fun√ß√£o de autocovari√¢ncia do processo MA(1)
[^3.7.4]: Processo MA(1) com par√¢metros diferentes
[^3.7.5]: Autocovari√¢ncia do processo
[^3.7.6]: Rela√ß√£o entre os par√¢metros do processo MA(1)
[^3.7.7]: Rela√ß√£o entre os par√¢metros do processo MA(1)
[^45]: Cap√≠tulo 3, p√°gina 45
[^46]: Cap√≠tulo 3, p√°gina 46
[^47]: Cap√≠tulo 3, p√°gina 47
[^48]: Cap√≠tulo 3, p√°gina 48
<!-- END -->