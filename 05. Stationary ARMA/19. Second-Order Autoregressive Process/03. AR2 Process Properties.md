## T√≠tulo Conciso
Autocovari√¢ncias e a M√©dia em Processos AR(2) sob Estacionariedade

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre processos AR(2) e suas representa√ß√µes MA($\infty$) [^57], este cap√≠tulo foca na deriva√ß√£o e interpreta√ß√£o da m√©dia e das autocovari√¢ncias sob a suposi√ß√£o de estacionariedade de covari√¢ncia [^56]. A an√°lise das autocovari√¢ncias √© crucial para entender a estrutura de depend√™ncia temporal em processos AR(2) e para fins de estima√ß√£o e previs√£o. As autocovari√¢ncias, juntamente com a m√©dia, fornecem uma descri√ß√£o completa das propriedades de segunda ordem de um processo AR(2) estacion√°rio.

### Conceitos Fundamentais
Como vimos anteriormente [^57], um processo AR(2) estacion√°rio pode ser expresso como:

$$Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \epsilon_t$$

onde $\epsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$ [^47, 48]. Sob a suposi√ß√£o de estacionariedade de covari√¢ncia, a m√©dia do processo AR(2) √© dada por [^57]:

$$\mu = \frac{c}{1 - \phi_1 - \phi_2}$$

*Prova*: Essa deriva√ß√£o foi detalhada no cap√≠tulo anterior, onde aplicamos o operador de valor esperado em ambos os lados da equa√ß√£o AR(2) e utilizamos a condi√ß√£o de estacionariedade para obter uma express√£o para $\mu$ [^56]. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere um processo AR(2) com $c = 5$, $\phi_1 = 0.6$ e $\phi_2 = 0.3$. A m√©dia do processo √© $\mu = \frac{5}{1 - 0.6 - 0.3} = \frac{5}{0.1} = 50$. Isso significa que, em m√©dia, os valores da s√©rie temporal flutuar√£o em torno de 50. Se $c = 0$, a m√©dia seria 0, indicando que a s√©rie temporal flutua em torno de zero.

O foco principal deste cap√≠tulo √© a deriva√ß√£o das **autocovari√¢ncias** [^44], que medem a depend√™ncia linear entre as observa√ß√µes em diferentes pontos no tempo. Definimos a autocovari√¢ncia no lag *j* como:

$$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$$

onde *j* √© um inteiro n√£o negativo. Para um processo AR(2), as autocovari√¢ncias seguem a mesma equa√ß√£o de diferen√ßa de segunda ordem que o processo em si [^57]:

$$\gamma_j = \phi_1 \gamma_{j-1} + \phi_2 \gamma_{j-2} \quad \text{para } j = 1, 2, \ldots$$

Esta equa√ß√£o expressa a autocovari√¢ncia no lag *j* como uma combina√ß√£o linear das autocovari√¢ncias nos lags *j-1* e *j-2*. Para resolver essa equa√ß√£o de diferen√ßa, precisamos de condi√ß√µes iniciais, tipicamente $\gamma_0$ e $\gamma_1$ (ou equivalentemente, $\gamma_0$ e $\rho_1$)[^57].

Para calcular $\gamma_0$, multiplicamos ambos os lados da equa√ß√£o AR(2) por $(Y_t - \mu)$ e tomamos o valor esperado [^57]:

$$E[(Y_t - \mu)^2] = \phi_1 E[(Y_{t-1} - \mu)(Y_t - \mu)] + \phi_2 E[(Y_{t-2} - \mu)(Y_t - \mu)] + E[\epsilon_t (Y_t - \mu)]$$

Isso leva a [^57]:

$$\gamma_0 = \phi_1 \gamma_1 + \phi_2 \gamma_2 + \sigma^2$$

Como $\gamma_j = \gamma_{-j}$ (devido √† estacionariedade), podemos escrever $\gamma_1 = \phi_1 \gamma_0 + \phi_2 \gamma_1$ e $\gamma_2 = \phi_1 \gamma_1 + \phi_2 \gamma_0$, substituindo e resolvendo, teremos um sistema de equa√ß√µes para encontrar $\gamma_0$, $\gamma_1$ e $\gamma_2$.

**Teorema 4** [Express√£o para $\gamma_0$ em Termos de $\phi_1$, $\phi_2$ e $\sigma^2$]

A vari√¢ncia $\gamma_0$ de um processo AR(2) estacion√°rio pode ser expressa como:

$$\gamma_0 = \frac{\sigma^2(1 - \phi_2)}{(1 + \phi_2)[(1 - \phi_2)^2 - \phi_1^2]}$$

*Prova*:

I. **Rela√ß√µes das autocovari√¢ncias**: Partindo das equa√ß√µes:
   $$\gamma_0 = \phi_1 \gamma_1 + \phi_2 \gamma_2 + \sigma^2$$
   $$\gamma_1 = \phi_1 \gamma_0 + \phi_2 \gamma_1$$
   $$\gamma_2 = \phi_1 \gamma_1 + \phi_2 \gamma_0$$

II. **Reorganizando a equa√ß√£o para $\gamma_1$**:
   Reorganizando a equa√ß√£o para $\gamma_1$, temos:
   $$\gamma_1(1-\phi_2) = \phi_1 \gamma_0$$
   $$\gamma_1 = \frac{\phi_1}{1-\phi_2}\gamma_0$$

III. **Substituindo $\gamma_1$ em $\gamma_2$**:
   Substituindo $\gamma_1$ na equa√ß√£o para $\gamma_2$, temos:
   $$\gamma_2 = \phi_1 \left(\frac{\phi_1}{1-\phi_2}\gamma_0\right) + \phi_2 \gamma_0$$
   $$\gamma_2 = \left(\frac{\phi_1^2}{1-\phi_2} + \phi_2\right)\gamma_0 = \left(\frac{\phi_1^2 + \phi_2 - \phi_2^2}{1-\phi_2}\right)\gamma_0$$

IV. **Substituindo $\gamma_1$ e $\gamma_2$ na equa√ß√£o para $\gamma_0$**:
   Agora, substitu√≠mos $\gamma_1$ e $\gamma_2$ na equa√ß√£o para $\gamma_0$:
   $$\gamma_0 = \phi_1 \left(\frac{\phi_1}{1-\phi_2}\gamma_0\right) + \phi_2 \left(\frac{\phi_1^2 + \phi_2 - \phi_2^2}{1-\phi_2}\gamma_0\right) + \sigma^2$$
   Dividindo por $\gamma_0$ (assumindo $\gamma_0 \neq 0$):
   $$1 = \frac{\phi_1^2}{1-\phi_2} + \frac{\phi_2(\phi_1^2 + \phi_2 - \phi_2^2)}{1-\phi_2} + \frac{\sigma^2}{\gamma_0}$$

V. **Isolando $\gamma_0$**:
   Isolando $\frac{\sigma^2}{\gamma_0}$, temos:
   $$\frac{\sigma^2}{\gamma_0} = 1 - \frac{\phi_1^2}{1-\phi_2} - \frac{\phi_2(\phi_1^2 + \phi_2 - \phi_2^2)}{1-\phi_2} = \frac{1-\phi_2 - \phi_1^2 - \phi_2\phi_1^2 - \phi_2^2 + \phi_2^3}{1-\phi_2}$$
   $$\gamma_0 = \frac{\sigma^2(1 - \phi_2)}{1 - \phi_2 - \phi_1^2 - \phi_2^2 + \phi_2^3 - \phi_1^2\phi_2}$$

VI. **Simplificando o Denominador**:
    O denominador pode ser fatorado de forma que:
    $$1-\phi_2 - \phi_1^2 - \phi_2^2 + \phi_2^3 - \phi_1^2\phi_2 = (1 + \phi_2)((1 - \phi_2)^2 - \phi_1^2)$$
    Assim,
    $$\gamma_0 = \frac{\sigma^2(1 - \phi_2)}{(1 + \phi_2)((1 - \phi_2)^2 - \phi_1^2)}$$ ‚ñ†

> üí° **Exemplo Num√©rico:** Sejam $\phi_1 = 0.4$, $\phi_2 = 0.2$ e $\sigma^2 = 2$.  Ent√£o, $\gamma_0 = \frac{2(1 - 0.2)}{(1 + 0.2)[(1 - 0.2)^2 - (0.4)^2]} = \frac{1.6}{1.2(0.64 - 0.16)} = \frac{1.6}{1.2(0.48)} = \frac{1.6}{0.576} \approx 2.778$. A vari√¢ncia do processo AR(2) √© aproximadamente 2.778.

Alternativamente, podemos expressar $\gamma_1$ e $\gamma_2$ em termos de $\gamma_0$ [^57]:

$$\gamma_1 = \rho_1 \gamma_0 = \frac{\phi_1}{1 - \phi_2} \gamma_0$$
$$\gamma_2 = \rho_2 \gamma_0 = \frac{\phi_1^2 + \phi_2 - \phi_2^2}{1 - \phi_2} \gamma_0$$

Substituindo estas express√µes na equa√ß√£o para $\gamma_0$, podemos obter uma express√£o para $\gamma_0$ em termos de $\phi_1$, $\phi_2$ e $\sigma^2$. A solu√ß√£o deste sistema de equa√ß√µes leva a:

$$\gamma_0 = \frac{\sigma^2(1 - \phi_2)}{(1 - \phi_2)^2 - \phi_1^2 - \phi_2(1 - \phi_2)} = \frac{\sigma^2(1 - \phi_2)}{1 - \phi_2 - \phi_1^2 - \phi_2 + \phi_2^2 - \phi_2} = \frac{\sigma^2(1 - \phi_2)}{1 - 2\phi_2 + \phi_2^2 - \phi_1^2}$$

Com alguma manipula√ß√£o alg√©brica, podemos obter a express√£o final:

$$\gamma_0 = \frac{\sigma^2 (1 - \phi_2)}{(1 - \phi_2 - \phi_1)(1 - \phi_2 + \phi_1)}$$

Observe que essa express√£o para $\gamma_0$ √© consistente com as condi√ß√µes de estacionariedade. Para que $\gamma_0$ seja finita e positiva, o denominador deve ser positivo. Isso implica que $(1 - \phi_2 - \phi_1) > 0$ e $(1 - \phi_2 + \phi_1) > 0$, que s√£o equivalentes a $\phi_1 + \phi_2 < 1$ e $\phi_2 - \phi_1 < 1$, respectivamente.

> üí° **Exemplo Num√©rico:** Se $\phi_1 = 0.3$, $\phi_2 = 0.5$ e $\sigma^2 = 1$, ent√£o $\gamma_0 = \frac{1(1 - 0.5)}{(1 - 0.5 - 0.3)(1 - 0.5 + 0.3)} = \frac{0.5}{(0.2)(0.8)} = \frac{0.5}{0.16} = 3.125$. A vari√¢ncia √© 3.125.  Se, por outro lado, $\phi_1 = 0.8$ e $\phi_2 = 0.3$, ent√£o $\phi_1 + \phi_2 = 1.1 > 1$, violando a condi√ß√£o de estacionariedade, e a vari√¢ncia n√£o estaria definida (seria infinita).

Tendo obtido $\gamma_0$, podemos expressar $\gamma_1$ em termos de $\phi_1$, $\phi_2$ e $\sigma^2$ [^57]:

$$\gamma_1 = \rho_1 \gamma_0 = \frac{\phi_1}{1 - \phi_2} \cdot \frac{\sigma^2(1 - \phi_2)}{(1 - \phi_2)^2 - \phi_1^2} = \frac{\phi_1 \sigma^2}{(1 - \phi_2)^2 - \phi_1^2} = \frac{\phi_1 \sigma^2}{(1 - \phi_2 - \phi_1)(1 - \phi_2 + \phi_1)}$$

**Teorema 5** [Express√µes para Autocovari√¢ncias $\gamma_1$ e $\gamma_2$]

As autocovari√¢ncias $\gamma_1$ e $\gamma_2$ de um processo AR(2) estacion√°rio podem ser expressas como:

$$\gamma_1 =  \frac{\phi_1 \sigma^2}{(1 - \phi_2 - \phi_1)(1 - \phi_2 + \phi_1)}$$

$$\gamma_2 =  \frac{\sigma^2(\phi_2 + \phi_1^2)}{(1 - \phi_2 - \phi_1)(1 - \phi_2 + \phi_1)(1-\phi_2)}$$

*Prova*:

I. **Deriva√ß√£o de $\gamma_1$**: Partindo da rela√ß√£o $\gamma_1 = \phi_1 \gamma_0 + \phi_2 \gamma_1$, podemos expressar $\gamma_1$ em termos de $\gamma_0$ e os par√¢metros do modelo:
$$\gamma_1 = \phi_1 \gamma_0 + \phi_2 \gamma_1$$
$$\gamma_1(1 - \phi_2) = \phi_1 \gamma_0$$
$$\gamma_1 = \frac{\phi_1}{1 - \phi_2} \gamma_0$$

II. **Substituindo $\gamma_0$**:
Substituindo a express√£o para $\gamma_0$ do Teorema 4:
$$\gamma_1 = \frac{\phi_1}{1 - \phi_2} \cdot \frac{\sigma^2(1 - \phi_2)}{(1 + \phi_2)[(1 - \phi_2)^2 - \phi_1^2]}$$
$$\gamma_1 = \frac{\phi_1 \sigma^2}{(1 + \phi_2)[(1 - \phi_2)^2 - \phi_1^2]}$$
Expandindo o denominador:
$$\gamma_1 = \frac{\phi_1 \sigma^2}{(1 - \phi_2 - \phi_1)(1 - \phi_2 + \phi_1)}$$

III. **Deriva√ß√£o de $\gamma_2$**: Usamos a rela√ß√£o recursiva $\gamma_2 = \phi_1 \gamma_1 + \phi_2 \gamma_0$:
$$\gamma_2 = \phi_1 \left(\frac{\phi_1 \sigma^2}{(1 - \phi_2 - \phi_1)(1 - \phi_2 + \phi_1)}\right) + \phi_2 \left(\frac{\sigma^2(1 - \phi_2)}{(1 + \phi_2)[(1 - \phi_2)^2 - \phi_1^2]}\right)$$

IV. **Simplificando $\gamma_2$**:
$$\gamma_2 = \frac{\phi_1^2 \sigma^2}{\left((1 - \phi_2)^2 - \phi_1^2 \right)} + \frac{\phi_2 \sigma^2 (1-\phi_2)}{\left((1 - \phi_2)^2 - \phi_1^2\right)} = \frac{\sigma^2(\phi_1^2 + \phi_2 - \phi_2^2)}{((1 - \phi_2)^2 - \phi_1^2)}$$
Com manipula√ß√µes alg√©bricas, teremos:

$$\gamma_2 = \frac{\sigma^2(\phi_2 + \phi_1^2)}{(1 - \phi_2 - \phi_1)(1 - \phi_2 + \phi_1)(1-\phi_2)}$$
‚ñ†

> üí° **Exemplo Num√©rico:** Usando os valores do exemplo anterior, $\phi_1 = 0.4$, $\phi_2 = 0.2$ e $\sigma^2 = 2$, calculemos $\gamma_1$ e $\gamma_2$.
>
> *   $\gamma_1 = \frac{0.4 \cdot 2}{(1 - 0.2 - 0.4)(1 - 0.2 + 0.4)} = \frac{0.8}{(0.4)(1.2)} = \frac{0.8}{0.48} \approx 1.667$
>
> *   $\gamma_2 =  \frac{2(0.2 + 0.4^2)}{(1 - 0.2 - 0.4)(1 - 0.2 + 0.4)(1-0.2)} = \frac{2(0.2 + 0.16)}{(0.4)(1.2)(0.8)} = \frac{2(0.36)}{0.384} = \frac{0.72}{0.384} \approx 1.875$.

Notavelmente, $\gamma_j$ para $j > 1$ pode ser calculado usando a rela√ß√£o recursiva. As autocorrela√ß√µes $\rho_j$ podem ser obtidas dividindo $\gamma_j$ por $\gamma_0$, conforme definido anteriormente.

> üí° **Exemplo Num√©rico:** Seja um processo AR(2) com $c = 0$, $\phi_1 = 0.5$, $\phi_2 = 0.2$ e $\sigma^2 = 1$.
>
> *   A m√©dia √© $\mu = \frac{0}{1 - 0.5 - 0.2} = 0$.
>
> *   A vari√¢ncia √© $\gamma_0 = \frac{1(1 - 0.2)}{(1 + 0.2)[(1 - 0.2)^2 - (0.5)^2]} = \frac{0.8}{1.2(0.64 - 0.25)} = \frac{0.8}{1.2(0.39)} = \frac{0.8}{0.468} \approx 1.709$.
>
> *   A autocovari√¢ncia no lag 1 √© $\gamma_1 = \frac{0.5 \cdot 1}{(1 - 0.2 - 0.5)(1 - 0.2 + 0.5)} = \frac{0.5}{(0.3)(1.3)} = \frac{0.5}{0.39} \approx 1.282$
> *   Podemos calcular $\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{1.282}{1.709} \approx 0.750$.
> *   A autocovari√¢ncia no lag 2 pode ser calculado usando a f√≥rmula:
>$\gamma_2 = \phi_1 \gamma_1 + \phi_2 \gamma_0 = 0.5(1.282) + 0.2(1.709) = 0.641 + 0.3418 = 0.9828$
> *   Podemos calcular $\rho_2 = \frac{\gamma_2}{\gamma_0} = \frac{0.9828}{1.709} \approx 0.575$.

As autocovari√¢ncias (e equivalentemente, as autocorrela√ß√µes) fornecem informa√ß√µes cruciais sobre a estrutura de depend√™ncia temporal do processo AR(2). Por exemplo, o sinal e a magnitude de $\gamma_1$ indicam a for√ßa e dire√ß√£o da depend√™ncia entre observa√ß√µes consecutivas. As condi√ß√µes de estacionariedade garantem que as autocovari√¢ncias decaiam suficientemente r√°pido √† medida que o lag aumenta [^57], o que implica que as observa√ß√µes distantes no tempo t√™m uma depend√™ncia cada vez menor.

**Proposi√ß√£o 1** [Decaimento Exponencial das Autocovari√¢ncias]
Sob condi√ß√µes de estacionariedade, as autocovari√¢ncias $\gamma_j$ de um processo AR(2) decaem exponencialmente √† medida que $j$ aumenta. Este decaimento √© governado pelas ra√≠zes da equa√ß√£o caracter√≠stica do processo AR(2).

*Prova*:
A equa√ß√£o caracter√≠stica do processo AR(2) √© dada por $1 - \phi_1 z - \phi_2 z^2 = 0$. Sejam $z_1$ e $z_2$ as ra√≠zes desta equa√ß√£o. Sob estacionariedade, $|z_1| > 1$ e $|z_2| > 1$. A solu√ß√£o geral para a equa√ß√£o de diferen√ßa $\gamma_j = \phi_1 \gamma_{j-1} + \phi_2 \gamma_{j-2}$ pode ser expressa como $\gamma_j = A z_1^{-j} + B z_2^{-j}$, onde A e B s√£o constantes determinadas pelas condi√ß√µes iniciais $\gamma_0$ e $\gamma_1$. Como $|z_1| > 1$ e $|z_2| > 1$, os termos $z_1^{-j}$ e $z_2^{-j}$ decaem exponencialmente para zero √† medida que $j$ aumenta. Portanto, $\gamma_j$ tamb√©m decai exponencialmente. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere $\phi_1 = 0.6$ e $\phi_2 = 0.3$.  A equa√ß√£o caracter√≠stica √© $1 - 0.6z - 0.3z^2 = 0$.  As ra√≠zes s√£o $z_1 \approx 1.89$ e $z_2 \approx -4$.  Como $|z_1| > 1$ e $|z_2| > 1$, as autocovari√¢ncias decair√£o exponencialmente.  Um decaimento mais r√°pido √© esperado se as ra√≠zes tivessem magnitudes maiores.  Se as ra√≠zes fossem complexas, o decaimento seria oscilat√≥rio.

Al√©m disso, podemos analisar o comportamento das autocorrela√ß√µes parciais (PACF) para um processo AR(2).

**Proposi√ß√£o 2** [Autocorrela√ß√£o Parcial (PACF) de um Processo AR(2)]
Para um processo AR(2), a autocorrela√ß√£o parcial no lag 2 (PACF(2)) √© igual a $\phi_2$, e a autocorrela√ß√£o parcial em lags maiores que 2 √© zero.

*Prova*:
A PACF mede a correla√ß√£o entre $Y_t$ e $Y_{t-k}$ ap√≥s remover o efeito das vari√°veis intermedi√°rias $Y_{t-1}, Y_{t-2}, \dots, Y_{t-k+1}$. Para um processo AR(2), a depend√™ncia de $Y_t$ em $Y_{t-k}$ para $k > 2$ √© completamente explicada pela depend√™ncia em $Y_{t-1}$ e $Y_{t-2}$. Portanto, a PACF(k) √© zero para $k > 2$. A PACF(2) mede a correla√ß√£o direta entre $Y_t$ e $Y_{t-2}$, que √© precisamente $\phi_2$. ‚ñ†

> üí° **Exemplo Num√©rico:** Para um processo AR(2) com $\phi_1 = 0.7$ e $\phi_2 = 0.2$, a PACF no lag 2 ser√° 0.2. Isso significa que, depois de remover o efeito de $Y_{t-1}$ na predi√ß√£o de $Y_t$, a correla√ß√£o restante entre $Y_t$ e $Y_{t-2}$ √© 0.2. A PACF para lags maiores que 2 ser√° zero.

A an√°lise das PACFs complementa a an√°lise das autocorrela√ß√µes e auxilia na identifica√ß√£o da ordem do processo AR.

**Corol√°rio 2.1** [Implica√ß√£o para a Identifica√ß√£o de Modelos AR(2)]
A Proposi√ß√£o 2 implica que, ao analisar a PACF de uma s√©rie temporal, um corte abrupto ap√≥s o lag 2 sugere que um modelo AR(2) pode ser apropriado.

*Prova*:
A prova segue diretamente da Proposi√ß√£o 2. Se a PACF(k) √© essencialmente zero para todos os lags $k > 2$, isso indica que a depend√™ncia da s√©rie temporal se estende apenas at√© o lag 2, consistindo com a estrutura de um processo AR(2). ‚ñ†

Al√©m da an√°lise das autocorrela√ß√µes e autocorrela√ß√µes parciais, √© √∫til considerar as condi√ß√µes sob as quais o processo AR(2) √© invert√≠vel.

**Teorema 6** [Condi√ß√µes de Invertibilidade para um Processo AR(2)]
Um processo AR(2) √© invert√≠vel se as ra√≠zes da sua equa√ß√£o caracter√≠stica estiverem fora do c√≠rculo unit√°rio. Equivalentemente, o processo √© invert√≠vel se os par√¢metros $\phi_1$ e $\phi_2$ satisfazem as seguintes condi√ß√µes:
1. $|\phi_2| < 1$
2. $\phi_2 + \phi_1 < 1$
3. $\phi_2 - \phi_1 < 1$

*Prova*:
A invertibilidade de um processo AR(2) requer que ele possa ser expresso como um processo MA($\infty$) convergente. Isso ocorre se e somente se as ra√≠zes da equa√ß√£o caracter√≠stica $1 - \phi_1 z - \phi_2 z^2 = 0$ estiverem fora do c√≠rculo unit√°rio. As condi√ß√µes dadas para $\phi_1$ e $\phi_2$ s√£o as mesmas condi√ß√µes de estacionariedade e garantem que as ra√≠zes estejam fora do c√≠rculo unit√°rio, assegurando a invertibilidade do processo. ‚ñ†

> üí° **Exemplo Num√©rico:** Seja $\phi_1 = 0.5$ e $\phi_2 = 0.4$.  Verificando as condi√ß√µes de invertibilidade:
>
> 1.  $|\phi_2| = |0.4| < 1$ (OK)
> 2.  $\phi_2 + \phi_1 = 0.4 + 0.5 = 0.9 < 1$ (OK)
> 3.  $\phi_2 - \phi_1 = 0.4 - 0.5 = -0.1 < 1$ (OK)
>
> Portanto, este processo AR(2) √© invert√≠vel. Se, por outro lado, $\phi_2 = 1.2$, a primeira condi√ß√£o seria violada, e o processo n√£o seria invert√≠vel.

A invertibilidade garante que o processo AR(2) possa ser representado como uma combina√ß√£o linear infinita de ru√≠dos brancos passados, o que √© √∫til para previs√£o e an√°lise de s√©ries temporais.

### Conclus√£o
Neste cap√≠tulo, derivamos express√µes para a m√©dia e autocovari√¢ncias de um processo AR(2) sob a suposi√ß√£o de estacionariedade de covari√¢ncia [^57]. A equa√ß√£o de diferen√ßa que rege as autocovari√¢ncias e as condi√ß√µes iniciais derivadas dos par√¢metros do processo AR(2) fornecem uma descri√ß√£o completa das propriedades de segunda ordem do processo. A compreens√£o das autocovari√¢ncias √© essencial para a estima√ß√£o, identifica√ß√£o de modelo e previs√£o em processos AR(2), fornecendo insights valiosos sobre a estrutura de depend√™ncia temporal da s√©rie temporal [^72].

### Refer√™ncias
[^44]: A vari√¢ncia da vari√°vel aleat√≥ria $Y_t$ (denotada $\gamma_{t,0}$) √© similarmente definida como $\gamma_{t,0} = E[(Y_t - \mu_t)^2] = \int_{-\infty}^{\infty} (y_t - \mu_t)^2 f_{Y_t}(y_t) dy$.
[^47]: O bloco de constru√ß√£o b√°sico para todos os processos considerados neste cap√≠tulo √© uma sequ√™ncia $\{\epsilon_t\}_{t=-\infty}^{\infty}$ cujos elementos t√™m m√©dia zero e vari√¢ncia $\sigma^2$: $E(\epsilon_t) = 0$ [3.2.1], $E(\epsilon_t^2) = \sigma^2$ [3.2.2]
[^48]: $E(\epsilon_t \epsilon_\tau) = 0$ para $t \neq \tau$.
[^56]: Uma autoregress√£o de segunda ordem, denotada AR(2), satisfaz $Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \epsilon_t$ [3.4.16].
[^57]: Provided that the roots of $1 - \phi_1 z - \phi_2 z^2 - \dots - \phi_p z^p = 0$ all lie outside the unit circle, it is straightforward to verify that a covariance-stationary representation of the form $Y_t = \mu + \psi(L)\epsilon_t$ [3.4.33].
[^72]: This chapter discusses how to forecast time series.
<!-- END -->