## T√≠tulo Conciso
Second-Order Autoregressive (AR(2)) Processes

### Introdu√ß√£o
Em continuidade ao estudo de processos autorregressivos, exploramos agora o processo autorregressivo de segunda ordem, denotado AR(2). Este processo expande o conceito de AR(1) [^53] ao incluir uma depend√™ncia da vari√°vel em dois per√≠odos anteriores, tornando a an√°lise mais complexa e rica. Compreender o AR(2) √© crucial para modelar s√©ries temporais com depend√™ncias de longo alcance e para servir como base para modelos ARMA mais gerais [^58].

### Conceitos Fundamentais
Um processo autorregressivo de segunda ordem, denotado AR(2), satisfaz a seguinte equa√ß√£o de diferen√ßa [^56]:
$$Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \epsilon_t$$
onde:
*   $Y_t$ √© o valor da s√©rie temporal no tempo *t*.
*   *c* √© uma constante.
*   $\phi_1$ e $\phi_2$ s√£o os par√¢metros autorregressivos, representando a influ√™ncia de $Y_{t-1}$ e $Y_{t-2}$ em $Y_t$, respectivamente.
*   $\epsilon_t$ √© um termo de erro de ru√≠do branco, satisfazendo as propriedades descritas em [^47]: m√©dia zero ($E(\epsilon_t) = 0$ [^47]), vari√¢ncia constante ($E(\epsilon_t^2) = \sigma^2$ [^47]), e n√£o correla√ß√£o entre diferentes pontos no tempo ($E(\epsilon_t \epsilon_\tau) = 0$ para $t \neq \tau$ [^48]).
Esta equa√ß√£o representa uma equa√ß√£o de diferen√ßa de segunda ordem, onde o valor atual da s√©rie temporal depende linearmente de seus dois valores anteriores e de um choque aleat√≥rio [^56].

> üí° **Exemplo Num√©rico:** Suponha que temos um processo AR(2) definido por $Y_t = 5 + 0.6Y_{t-1} - 0.3Y_{t-2} + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco com m√©dia 0 e vari√¢ncia 1. Se $Y_{t-1} = 10$ e $Y_{t-2} = 5$, ent√£o o valor esperado de $Y_t$ √© $Y_t = 5 + 0.6(10) - 0.3(5) + \epsilon_t = 5 + 6 - 1.5 = 9.5 + \epsilon_t$. Portanto, $E[Y_t] = 9.5$.

Para analisar a **estabilidade** do processo AR(2), podemos reescrever a equa√ß√£o usando o operador de defasagem *L* [^57], onde $LY_t = Y_{t-1}$:
$$(1 - \phi_1 L - \phi_2 L^2) Y_t = c + \epsilon_t$$
A estabilidade desta equa√ß√£o de diferen√ßa √© garantida se as ra√≠zes da equa√ß√£o caracter√≠stica [^57]:
$$1 - \phi_1 z - \phi_2 z^2 = 0$$
estiverem fora do c√≠rculo unit√°rio no plano complexo [^57]. Equivalentemente, as ra√≠zes da equa√ß√£o $ \phi_2 z^2 + \phi_1 z - 1 = 0 $ devem ser maiores que 1 em valor absoluto.

**Teorema 1** [Condi√ß√µes de Estacionariedade]
Um processo AR(2) √© estacion√°rio se e somente se os seguintes condi√ß√µes forem satisfeitas:
$$ \phi_1 + \phi_2 < 1 $$
$$ \phi_2 - \phi_1 < 1 $$
$$ -1 < \phi_2 < 1 $$

> üí° **Exemplo Num√©rico:** Consideremos os seguintes casos para $(\phi_1, \phi_2)$:
>
> 1.  $(\phi_1, \phi_2) = (0.5, 0.3)$: $\phi_1 + \phi_2 = 0.8 < 1$, $\phi_2 - \phi_1 = -0.2 < 1$, $-1 < 0.3 < 1$. Todas as condi√ß√µes s√£o satisfeitas, o processo √© estacion√°rio.
> 2.  $(\phi_1, \phi_2) = (1.2, -0.4)$: $\phi_1 + \phi_2 = 0.8 < 1$, $\phi_2 - \phi_1 = -1.6 < 1$, $-1 < -0.4 < 1$. Todas as condi√ß√µes s√£o satisfeitas, o processo √© estacion√°rio.
> 3.  $(\phi_1, \phi_2) = (0.7, 0.5)$: $\phi_1 + \phi_2 = 1.2 > 1$. A primeira condi√ß√£o n√£o √© satisfeita, o processo n√£o √© estacion√°rio.
> 4.  $(\phi_1, \phi_2) = (0.2, 0.9)$: $\phi_1 + \phi_2 = 1.1 > 1$. A primeira condi√ß√£o n√£o √© satisfeita, o processo n√£o √© estacion√°rio.
> 5.  $(\phi_1, \phi_2) = (-0.5, -0.6)$: $\phi_1 + \phi_2 = -1.1 < 1$, $\phi_2 - \phi_1 = -0.1 < 1$, $-1 > -0.6$. A terceira condi√ß√£o n√£o √© satisfeita, o processo n√£o √© estacion√°rio.

*Prova*: As ra√≠zes da equa√ß√£o caracter√≠stica $ \phi_2 z^2 + \phi_1 z - 1 = 0 $ s√£o dadas por:

$$ z_{1,2} = \frac{-\phi_1 \pm \sqrt{\phi_1^2 + 4\phi_2}}{2\phi_2} $$

Para que o processo seja estacion√°rio, devemos ter $|z_1| > 1$ e $|z_2| > 1$. Analisando as condi√ß√µes para que isso ocorra, chegamos √†s restri√ß√µes impostas aos par√¢metros $\phi_1$ e $\phi_2$ enunciadas no teorema.

A prova completa √© um tanto envolvida e depende de considerar diversos casos. Apresentamos aqui uma prova parcial, focando em mostrar que as condi√ß√µes apresentadas s√£o necess√°rias para a estacionariedade.

I. **Reescrevendo as condi√ß√µes**: As condi√ß√µes podem ser reescritas como: $\phi_2 < 1 - \phi_1$, $\phi_2 < 1 + \phi_1$ e $-1 < \phi_2 < 1$.

II. **An√°lise da primeira raiz**: Consideremos o caso em que $\phi_1^2 + 4\phi_2 \geq 0$, ou seja, as ra√≠zes s√£o reais.  Queremos mostrar que $|z_1| > 1$ e $|z_2| > 1$.  Suponha que $\phi_2 > 0$.  Ent√£o, para que $|z_i| > 1$, as ra√≠zes devem satisfazer:
$$ \left| \frac{-\phi_1 \pm \sqrt{\phi_1^2 + 4\phi_2}}{2\phi_2} \right| > 1$$

III. **Considerando uma das ra√≠zes:**  Analisemos a raiz com o sinal positivo: $z_1 = \frac{-\phi_1 + \sqrt{\phi_1^2 + 4\phi_2}}{2\phi_2}$. Para que $|z_1| > 1$, devemos ter:

$$ \frac{-\phi_1 + \sqrt{\phi_1^2 + 4\phi_2}}{2\phi_2} > 1 \text{  ou  } \frac{-\phi_1 + \sqrt{\phi_1^2 + 4\phi_2}}{2\phi_2} < -1$$

IV. **Simplificando a desigualdade:**  A primeira desigualdade implica:

$$ -\phi_1 + \sqrt{\phi_1^2 + 4\phi_2} > 2\phi_2 \implies \sqrt{\phi_1^2 + 4\phi_2} > \phi_1 + 2\phi_2$$
Elevando ao quadrado (sabendo que ambos os lados s√£o positivos, pois estamos assumindo $\phi_2 > 0$ e buscando $|z_1| > 1$):
$$ \phi_1^2 + 4\phi_2 > \phi_1^2 + 4\phi_1\phi_2 + 4\phi_2^2 \implies 4\phi_2 > 4\phi_1\phi_2 + 4\phi_2^2 $$
Dividindo por $4\phi_2$ (j√° que $\phi_2 > 0$):
$$ 1 > \phi_1 + \phi_2 $$
O que corresponde √† primeira condi√ß√£o do teorema.

V. **Repetindo para a segunda raiz**: Uma an√°lise similar pode ser feita para a outra raiz e para o caso em que $\phi_2 < 0$, levando √†s outras duas condi√ß√µes. No entanto, essa an√°lise completa √© extensa. A condi√ß√£o $\phi_2 > -1$ decorre da necessidade de que a vari√¢ncia do processo seja positiva.

VI. **Conclus√£o parcial**: Demonstramos que a condi√ß√£o $\phi_1 + \phi_2 < 1$ √© necess√°ria para a estacionariedade do processo AR(2) quando $\phi_2 > 0$ e as ra√≠zes s√£o reais. Uma an√°lise completa envolveria considerar as outras ra√≠zes e o caso em que as ra√≠zes s√£o complexas, o que est√° al√©m do escopo desta prova simplificada. ‚ñ†

A condi√ß√£o de estabilidade garante que o processo AR(2) seja **estacion√°rio** [^57], ou seja, que sua m√©dia e autocovari√¢ncia n√£o variem ao longo do tempo. Sob condi√ß√µes de estacionariedade, a m√©dia do processo AR(2) pode ser expressa como [^57]:
$$\mu = \frac{c}{1 - \phi_1 - \phi_2}$$
Esta express√£o √© obtida tomando o valor esperado de ambos os lados da equa√ß√£o AR(2) e resolvendo para $E(Y_t) = \mu$ [^57].

> üí° **Exemplo Num√©rico:** Considere um processo AR(2) com $c = 10$, $\phi_1 = 0.3$ e $\phi_2 = 0.2$. A m√©dia do processo √© $\mu = \frac{10}{1 - 0.3 - 0.2} = \frac{10}{0.5} = 20$.
>
> Se $c = 5$, $\phi_1 = 0.6$ e $\phi_2 = -0.3$, ent√£o $\mu = \frac{5}{1 - 0.6 - (-0.3)} = \frac{5}{0.7} \approx 7.14$.

*Prova*: Demonstraremos como a m√©dia do processo AR(2) √© derivada sob as condi√ß√µes de estacionariedade.

I. **Tomando o Valor Esperado**: Aplicamos o operador de valor esperado $E[\cdot]$ a ambos os lados da equa√ß√£o AR(2):
$$E[Y_t] = E[c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \epsilon_t]$$

II. **Usando a Linearidade do Valor Esperado**: O valor esperado de uma soma √© a soma dos valores esperados:
$$E[Y_t] = E[c] + \phi_1 E[Y_{t-1}] + \phi_2 E[Y_{t-2}] + E[\epsilon_t]$$

III. **Aplicando a Estacionariedade**: Assumindo que o processo √© estacion√°rio, a m√©dia √© constante ao longo do tempo, ent√£o $E[Y_t] = E[Y_{t-1}] = E[Y_{t-2}] = \mu$. Al√©m disso, $E[c] = c$ e, como $\epsilon_t$ √© ru√≠do branco, $E[\epsilon_t] = 0$:
$$\mu = c + \phi_1 \mu + \phi_2 \mu + 0$$

IV. **Resolvendo para $\mu$**: Rearranjamos a equa√ß√£o para isolar $\mu$:
$$\mu - \phi_1 \mu - \phi_2 \mu = c$$
$$\mu(1 - \phi_1 - \phi_2) = c$$

V. **Obtendo a M√©dia**: Dividimos ambos os lados por $(1 - \phi_1 - \phi_2)$, que √© diferente de zero devido √† condi√ß√£o de estacionariedade ($\phi_1 + \phi_2 < 1$):
$$\mu = \frac{c}{1 - \phi_1 - \phi_2}$$
Portanto, a m√©dia do processo AR(2) √© $\mu = \frac{c}{1 - \phi_1 - \phi_2}$. ‚ñ†

Para entender melhor o comportamento do processo AR(2), √© essencial analisar suas autocovari√¢ncias ($\gamma_j$) [^44] e autocorrela√ß√µes ($\rho_j$) [^49]. Multiplicando ambos os lados da equa√ß√£o AR(2) por $(Y_{t-j} - \mu)$ e tomando o valor esperado, obtemos a seguinte rela√ß√£o recursiva para as autocovari√¢ncias [^57]:
$$\gamma_j = \phi_1 \gamma_{j-1} + \phi_2 \gamma_{j-2} \quad \text{para } j = 1, 2, \ldots$$
Esta equa√ß√£o mostra que a autocovari√¢ncia no lag *j* √© uma combina√ß√£o linear das autocovari√¢ncias nos lags *j-1* e *j-2* [^57]. Para resolver este sistema, precisamos das condi√ß√µes iniciais $\gamma_0$, $\gamma_1$ e $\gamma_2$ [^57].

**Lema 1** [Rela√ß√£o entre autocovari√¢ncias para j=0]
A autocovari√¢ncia no lag 0, $\gamma_0$, pode ser expressa em termos de $\phi_1$, $\phi_2$ e $\sigma^2$ como:
$$\gamma_0 = \phi_1 \gamma_1 + \phi_2 \gamma_2 + \sigma^2$$

*Prova*: Esta rela√ß√£o √© obtida multiplicando ambos os lados da equa√ß√£o AR(2) por $(Y_t - \mu)$ e tomando o valor esperado, conforme mencionado no texto principal. Vamos detalhar esse processo:

I. **Equa√ß√£o AR(2) Centralizada**: Primeiro, subtra√≠mos a m√©dia $\mu$ de ambos os lados da equa√ß√£o AR(2) original:
$$Y_t - \mu = \phi_1 (Y_{t-1} - \mu) + \phi_2 (Y_{t-2} - \mu) + \epsilon_t$$

II. **Multiplica√ß√£o por $(Y_t - \mu)$**: Multiplicamos ambos os lados da equa√ß√£o por $(Y_t - \mu)$:
$$(Y_t - \mu)^2 = \phi_1 (Y_{t-1} - \mu)(Y_t - \mu) + \phi_2 (Y_{t-2} - \mu)(Y_t - \mu) + \epsilon_t (Y_t - \mu)$$

III. **Tomando o Valor Esperado**: Aplicamos o operador de valor esperado $E[\cdot]$ a ambos os lados:
$$E[(Y_t - \mu)^2] = \phi_1 E[(Y_{t-1} - \mu)(Y_t - \mu)] + \phi_2 E[(Y_{t-2} - \mu)(Y_t - \mu)] + E[\epsilon_t (Y_t - \mu)]$$

IV. **Identificando Autocovari√¢ncias**: Reconhecemos as autocovari√¢ncias:
*   $E[(Y_t - \mu)^2] = \gamma_0$
*   $E[(Y_{t-1} - \mu)(Y_t - \mu)] = \gamma_1$
*   $E[(Y_{t-2} - \mu)(Y_t - \mu)] = \gamma_2$

V. **Simplificando o Termo de Erro**:  Como $\epsilon_t$ √© ru√≠do branco e n√£o correlacionado com os valores passados de $Y_t$, podemos assumir que $E[\epsilon_t (Y_t - \mu)] = E[\epsilon_t^2] = \sigma^2$ (Essa simplifica√ß√£o requer um pouco mais de cuidado, pois $Y_t$ depende de $\epsilon_t$. Uma deriva√ß√£o mais rigorosa mostraria que $E[\epsilon_t (Y_t - \mu)] = \sigma^2$).

VI. **Substitui√ß√£o**: Substitu√≠mos as autocovari√¢ncias na equa√ß√£o:
$$\gamma_0 = \phi_1 \gamma_1 + \phi_2 \gamma_2 + \sigma^2$$

Portanto, demonstramos que $\gamma_0 = \phi_1 \gamma_1 + \phi_2 \gamma_2 + \sigma^2$. ‚ñ†

As **autocorrela√ß√µes** s√£o obtidas dividindo as autocovari√¢ncias pela vari√¢ncia $\gamma_0$:
$$\rho_j = \frac{\gamma_j}{\gamma_0} \quad \text{para } j = 1, 2, \ldots$$
Em particular, para *j* = 1 e *j* = 2 [^57]:
$$\rho_1 = \phi_1 + \phi_2 \rho_1$$
$$\rho_2 = \phi_1 \rho_1 + \phi_2$$
Resolvendo este sistema, encontramos $\rho_1$ e $\rho_2$ em termos de $\phi_1$ e $\phi_2$ [^57]:
$$\rho_1 = \frac{\phi_1}{1 - \phi_2}$$
*Prova*: Para obter $\rho_1$, partimos da equa√ß√£o $\rho_1 = \phi_1 + \phi_2 \rho_1$ e isolamos $\rho_1$:

I. **Isolando** $\rho_1$:
$$\rho_1 - \phi_2 \rho_1 = \phi_1$$
$$\rho_1(1 - \phi_2) = \phi_1$$

II. **Dividindo**:
$$\rho_1 = \frac{\phi_1}{1 - \phi_2}$$
Portanto, $\rho_1 = \frac{\phi_1}{1 - \phi_2}$. ‚ñ†

Para calcular $\rho_2$, podemos substituir $\rho_1$ na segunda equa√ß√£o:

$$\rho_2 = \phi_1 \left(\frac{\phi_1}{1 - \phi_2}\right) + \phi_2 = \frac{\phi_1^2 + \phi_2 - \phi_2^2}{1 - \phi_2}$$
*Prova*: Demonstraremos como $\rho_2$ √© derivado substituindo $\rho_1$ na equa√ß√£o apropriada.

I. **Substitui√ß√£o**: Come√ßamos com a equa√ß√£o para $\rho_2$:
$$\rho_2 = \phi_1 \rho_1 + \phi_2$$
Substitu√≠mos a express√£o para $\rho_1 = \frac{\phi_1}{1 - \phi_2}$:
$$\rho_2 = \phi_1 \left(\frac{\phi_1}{1 - \phi_2}\right) + \phi_2$$

II. **Simplifica√ß√£o**: Encontramos um denominador comum e combinamos os termos:
$$\rho_2 = \frac{\phi_1^2}{1 - \phi_2} + \frac{\phi_2(1 - \phi_2)}{1 - \phi_2}$$
$$\rho_2 = \frac{\phi_1^2 + \phi_2 - \phi_2^2}{1 - \phi_2}$$
Portanto, $\rho_2 = \frac{\phi_1^2 + \phi_2 - \phi_2^2}{1 - \phi_2}$. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere $\phi_1 = 0.4$ e $\phi_2 = 0.3$. Ent√£o,
>
> $$\rho_1 = \frac{0.4}{1 - 0.3} = \frac{0.4}{0.7} \approx 0.571$$
>
> $$\rho_2 = \frac{(0.4)^2 + 0.3 - (0.3)^2}{1 - 0.3} = \frac{0.16 + 0.3 - 0.09}{0.7} = \frac{0.37}{0.7} \approx 0.529$$

Para calcular a vari√¢ncia $\gamma_0$, multiplicamos ambos os lados da equa√ß√£o AR(2) por $(Y_t - \mu)$ e tomamos o valor esperado [^57]:
$$\gamma_0 = \phi_1 \gamma_1 + \phi_2 \gamma_2 + \sigma^2$$
Resolvendo este sistema, encontramos $\gamma_0$ em termos de $\phi_1$, $\phi_2$ e $\sigma^2$ [^57]. Como $\gamma_j = \rho_j \gamma_0$, podemos reescrever a equa√ß√£o acima como:
$$\gamma_0 = \phi_1 \rho_1 \gamma_0 + \phi_2 \rho_2 \gamma_0 + \sigma^2$$
Dividindo ambos os lados por $\gamma_0$, obtemos:
$$1 = \phi_1 \rho_1 + \phi_2 \rho_2 + \frac{\sigma^2}{\gamma_0}$$
Portanto:
$$\gamma_0 = \frac{\sigma^2}{1 - \phi_1 \rho_1 - \phi_2 \rho_2}$$
Substituindo as express√µes para $\rho_1$ e $\rho_2$, temos:
$$\gamma_0 = \frac{\sigma^2}{1 - \frac{\phi_1^2}{1 - \phi_2} - \frac{\phi_1^2 + \phi_2 - \phi_2^2}{1 - \phi_2}} = \frac{\sigma^2(1 - \phi_2)}{1 - \phi_2 - \phi_1^2 - \phi_2 + \phi_2^2} = \frac{\sigma^2(1 - \phi_2)}{1 - \phi_1^2 - 2\phi_2 + \phi_2^2}$$

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior, com $\phi_1 = 0.4$, $\phi_2 = 0.3$ e supondo $\sigma^2 = 1$,
>
> $$\gamma_0 = \frac{1(1 - 0.3)}{1 - (0.4)^2 - 2(0.3) + (0.3)^2} = \frac{0.7}{1 - 0.16 - 0.6 + 0.09} = \frac{0.7}{0.33} \approx 2.121$$

A fun√ß√£o de autocorrela√ß√£o (ACF) [^49] para um processo AR(2) exibe um padr√£o que pode ser uma soma de duas fun√ß√µes exponenciais decrescentes ou uma fun√ß√£o senoidal amortecida [^58], dependendo dos valores de $\phi_1$ e $\phi_2$. Este comportamento √© an√°logo √†s solu√ß√µes da equa√ß√£o de diferen√ßa de segunda ordem, onde as ra√≠zes podem ser reais ou complexas [^58].

> üí° **Exemplo Num√©rico:** Consideremos dois processos AR(2):
>
> 1.  $\phi_1 = 0.5$, $\phi_2 = 0.3$: As ra√≠zes da equa√ß√£o caracter√≠stica s√£o reais e a ACF exibir√° um decaimento exponencial.
> 2.  $\phi_1 = 0.5$, $\phi_2 = -0.8$: As ra√≠zes da equa√ß√£o caracter√≠stica s√£o complexas e a ACF exibir√° um padr√£o senoidal amortecido.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def ar2_acf(phi1, phi2, lags):
>     """Calculates the ACF for an AR(2) process."""
>     acf = np.zeros(lags + 1)
>     acf[0] = 1.0
>     acf[1] = phi1 / (1 - phi2)
>     for k in range(2, lags + 1):
>         acf[k] = phi1 * acf[k-1] + phi2 * acf[k-2]
>     return acf
>
> # Example 1: Real roots
> phi1_1 = 0.5
> phi2_1 = 0.3
> lags = 20
> acf1 = ar2_acf(phi1_1, phi2_1, lags)
>
> # Example 2: Complex roots
> phi1_2 = 0.5
> phi2_2 = -0.8
> acf2 = ar2_acf(phi1_2, phi2_2, lags)
>
> # Plotting the ACFs
> plt.figure(figsize=(12, 6))
>
> plt.subplot(1, 2, 1)
> plt.stem(acf1, use_line_collection=True)
> plt.title(f'ACF for AR(2) with phi1={phi1_1}, phi2={phi2_1} (Real Roots)')
> plt.xlabel('Lag')
> plt.ylabel('Autocorrelation')
>
> plt.subplot(1, 2, 2)
> plt.stem(acf2, use_line_collection=True)
> plt.title(f'ACF for AR(2) with phi1={phi1_2}, phi2={phi2_2} (Complex Roots)')
> plt.xlabel('Lag')
> plt.ylabel('Autocorrelation')
>
> plt.tight_layout()
> plt.show()
> ```
> The code above generates and plots the ACF for two AR(2) processes, one with real roots showing exponential decay and another one with complex roots showing a damped sinusoidal pattern.

**Teorema 2** [Comportamento Assint√≥tico da ACF]
O comportamento assint√≥tico da fun√ß√£o de autocorrela√ß√£o (ACF) de um processo AR(2) √© determinado pelas ra√≠zes da equa√ß√£o caracter√≠stica. Se as ra√≠zes forem reais e distintas, a ACF decair√° exponencialmente. Se as ra√≠zes forem complexas conjugadas, a ACF exibir√° um padr√£o senoidal amortecido.

*Prova*: A ACF de um processo AR(2) satisfaz a equa√ß√£o de diferen√ßa $\rho_j = \phi_1 \rho_{j-1} + \phi_2 \rho_{j-2}$. A solu√ß√£o geral desta equa√ß√£o depende da natureza das ra√≠zes da equa√ß√£o caracter√≠stica $z^2 - \phi_1 z - \phi_2 = 0$. Se as ra√≠zes $z_1$ e $z_2$ forem reais e distintas, ent√£o $\rho_j = A z_1^j + B z_2^j$ para algumas constantes A e B. Como o processo √© estacion√°rio, $|z_1| < 1$ e $|z_2| < 1$, ent√£o $\rho_j$ decai exponencialmente. Se as ra√≠zes forem complexas conjugadas, ent√£o $z_{1,2} = r e^{\pm i\theta}$, e $\rho_j$ ter√° a forma $C r^j \cos(j\theta + \psi)$ para algumas constantes C e $\psi$, representando um padr√£o senoidal amortecido.

### Conclus√£o
O processo AR(2) oferece uma flexibilidade maior na modelagem de depend√™ncias temporais do que o AR(1) [^53], capturando padr√µes mais complexos em s√©ries temporais. A an√°lise das condi√ß√µes de estabilidade, autocovari√¢ncias e autocorrela√ß√µes fornece insights valiosos sobre o comportamento do processo. A compreens√£o detalhada do AR(2) √© essencial para a constru√ß√£o de modelos ARMA mais avan√ßados [^58] e para a aplica√ß√£o de t√©cnicas de previs√£o mais precisas [^72].

### Refer√™ncias
[^44]: A vari√¢ncia da vari√°vel aleat√≥ria $Y_t$ (denotada $\gamma_{t,0}$) √© similarmente definida como $\gamma_{t,0} = E[(Y_t - \mu_t)^2] = \int_{-\infty}^{\infty} (y_t - \mu_t)^2 f_{Y_t}(y_t) dy$.
[^47]: O bloco de constru√ß√£o b√°sico para todos os processos considerados neste cap√≠tulo √© uma sequ√™ncia $\{\epsilon_t\}_{t=-\infty}^{\infty}$ cujos elementos t√™m m√©dia zero e vari√¢ncia $\sigma^2$: $E(\epsilon_t) = 0$ [3.2.1], $E(\epsilon_t^2) = \sigma^2$ [3.2.2]
[^48]: $E(\epsilon_t \epsilon_\tau) = 0$ para $t \neq \tau$.
[^49]: A $j$√©sima autocorrela√ß√£o de um processo estacion√°rio de covari√¢ncia (denotada $\rho_j$) √© definida como sua $j$√©sima autocovari√¢ncia dividida pela vari√¢ncia: $\rho_j = \gamma_j / \gamma_0$ [3.3.6].
[^53]: Um autoregress√£o de primeira ordem, denotado AR(1), satisfaz a seguinte equa√ß√£o de diferen√ßa: $Y_t = c + \phi Y_{t-1} + \epsilon_t$ [3.4.1].
[^56]: Uma autoregress√£o de segunda ordem, denotada AR(2), satisfaz $Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \epsilon_t$ [3.4.16].
[^57]: Provided that the roots of $1 - \phi_1 z - \phi_2 z^2 - \dots - \phi_p z^p = 0$ all lie outside the unit circle, it is straightforward to verify that a covariance-stationary representation of the form $Y_t = \mu + \psi(L)\epsilon_t$ [3.4.33].
[^58]: Um processo ARMA(p, q) inclui ambos os termos autorregressivos e de m√©dia m√≥vel: $Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \dots + \phi_p Y_{t-p} + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q}$ [3.5.1].
[^72]: This chapter discusses how to forecast time series.
<!-- END -->