## An√°lise Detalhada do Processo de M√©dia M√≥vel de Ordem Infinita (MA(‚àû)): Autocovari√¢ncias

### Introdu√ß√£o

Este cap√≠tulo aprofunda o estudo do processo de m√©dia m√≥vel de ordem infinita (MA(‚àû)), focando na deriva√ß√£o e interpreta√ß√£o das autocovari√¢ncias. Em continuidade com a defini√ß√£o e propriedades gerais do MA(‚àû) [^9], exploraremos como as autocovari√¢ncias, que medem a depend√™ncia linear entre os valores da s√©rie temporal em diferentes *lags*, s√£o calculadas e como elas se relacionam com os coeficientes do processo e a vari√¢ncia do ru√≠do branco. Abordaremos tamb√©m como as autocovari√¢ncias do processo MA(‚àû) podem ser obtidas extrapolando os resultados para um processo MA(q) √† medida que $q$ tende ao infinito.

### Conceitos Fundamentais

Relembrando, o processo **MA(‚àû)** √© definido como [^9]:

$$Y_t = \mu + \sum_{j=0}^{\infty} \psi_j \epsilon_{t-j}$$

onde:

*   $Y_t$ representa o valor da s√©rie temporal no instante $t$.
*   $\mu$ √© a m√©dia do processo.
*   $\epsilon_t$ √© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia constante $\sigma^2$ [^4].
*   $\psi_j$ s√£o os coeficientes que ponderam a influ√™ncia dos *shocks* passados $\epsilon_{t-j}$ no valor atual $Y_t$ [^9], com $\psi_0 = 1$.

As **autocovari√¢ncias** do processo MA(‚àû) s√£o dadas por [^9]:

$$\gamma_k = E[(Y_t - \mu)(Y_{t-k} - \mu)]$$

Substituindo a defini√ß√£o do processo MA(‚àû), obtemos:

$$\gamma_k = E\left[\left(\sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}\right)\left(\sum_{j=0}^{\infty} \psi_j \epsilon_{t-k-j}\right)\right]$$

Trocando a ordem da esperan√ßa e das somat√≥rias:

$$\gamma_k = \sum_{i=0}^{\infty} \sum_{j=0}^{\infty} \psi_i \psi_j E[\epsilon_{t-i} \epsilon_{t-k-j}]$$

Como $\epsilon_t$ √© ru√≠do branco, $E[\epsilon_{t-i} \epsilon_{t-k-j}] = \sigma^2$ se $i = k + j$, e $0$ caso contr√°rio.

*Prova:*

I.  Definimos $\epsilon_t$ como um processo de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$. Isso implica que $E[\epsilon_t] = 0$ e $E[\epsilon_t \epsilon_s] = \sigma^2$ se $t=s$, e $E[\epsilon_t \epsilon_s] = 0$ se $t \neq s$.

II. Consideramos o termo $E[\epsilon_{t-i} \epsilon_{t-k-j}]$.

III. Se $i = k+j$, ent√£o $t-i = t - (k+j) = t-k-j$, e $E[\epsilon_{t-i} \epsilon_{t-k-j}] = E[\epsilon_{t-i}^2] = \sigma^2$.

IV. Se $i \neq k+j$, ent√£o $t-i \neq t-k-j$, e $E[\epsilon_{t-i} \epsilon_{t-k-j}] = 0$ porque $\epsilon_t$ √© ru√≠do branco e n√£o correlacionado em diferentes momentos.

V. Portanto, $E[\epsilon_{t-i} \epsilon_{t-k-j}] = \sigma^2$ se $i = k + j$, e $0$ caso contr√°rio. $\blacksquare$

Usando a mesma substitui√ß√£o de √≠ndices realizada anteriormente, obtemos:

$$\gamma_k = \sigma^2 \sum_{j=0}^{\infty} \psi_j \psi_{j+k}$$

*Prova:*

I.  Partimos da express√£o:

$$\gamma_k = \sum_{i=0}^{\infty} \sum_{j=0}^{\infty} \psi_i \psi_j E[\epsilon_{t-i} \epsilon_{t-k-j}]$$

II. Sabemos que $E[\epsilon_{t-i} \epsilon_{t-k-j}] = \sigma^2$ se $i = k + j$, e $0$ caso contr√°rio.  Ent√£o, a dupla somat√≥ria se reduz a casos onde $i = k + j$.

III. Substitu√≠mos $i$ por $k+j$ na somat√≥ria externa:
$$\gamma_k = \sum_{j=0}^{\infty} \psi_{k+j} \psi_j \sigma^2 = \sigma^2 \sum_{j=0}^{\infty} \psi_j \psi_{j+k}$$

IV. Portanto, demonstramos que $\gamma_k = \sigma^2 \sum_{i=0}^{\infty} \psi_i \psi_{i+k}$. $\blacksquare$

Portanto, a autocovari√¢ncia do processo MA(‚àû) no lag $k$ √© dada por:

$$\gamma_k = \sigma^2 \sum_{i=0}^{\infty} \psi_i \psi_{i+k}$$

> üí° **Exemplo Num√©rico:** Seja $\sigma^2 = 1$ e $\psi_i = 0.5^i$. Calcule $\gamma_0$ e $\gamma_1$.
>
> $\gamma_0 = \sigma^2 \sum_{i=0}^{\infty} \psi_i^2 = 1 \cdot \sum_{i=0}^{\infty} (0.5^i)^2 = \sum_{i=0}^{\infty} 0.25^i = \frac{1}{1 - 0.25} = \frac{1}{0.75} = \frac{4}{3} \approx 1.333$
>
> $\gamma_1 = \sigma^2 \sum_{i=0}^{\infty} \psi_i \psi_{i+1} = 1 \cdot \sum_{i=0}^{\infty} (0.5^i)(0.5^{i+1}) = \sum_{i=0}^{\infty} 0.5^{2i+1} = 0.5 \sum_{i=0}^{\infty} 0.25^i = 0.5 \cdot \frac{1}{1 - 0.25} = 0.5 \cdot \frac{4}{3} = \frac{2}{3} \approx 0.667$
>
> A autocovari√¢ncia no lag 0 ($\gamma_0$) √© a vari√¢ncia do processo, e no lag 1 ($\gamma_1$) mede a depend√™ncia linear entre $Y_t$ e $Y_{t-1}$.
>
> ```python
> import numpy as np
>
> sigma_squared = 1
> psi = lambda i: 0.5**i
>
> gamma_0 = sigma_squared * sum([psi(i)**2 for i in range(100)]) # Approximation to infinity
> gamma_1 = sigma_squared * sum([psi(i) * psi(i+1) for i in range(100)]) # Approximation to infinity
>
> print(f"Gamma_0: {gamma_0}")
> print(f"Gamma_1: {gamma_1}")
> ```

Em particular, a vari√¢ncia (autocovari√¢ncia no lag 0) √©:

$$\gamma_0 = \sigma^2 \sum_{i=0}^{\infty} \psi_i^2$$

*Prova:*

I.  Consideramos a f√≥rmula geral da autocovari√¢ncia:

$$\gamma_k = \sigma^2 \sum_{i=0}^{\infty} \psi_i \psi_{i+k}$$

II. Para obter a vari√¢ncia, definimos $k = 0$:

$$\gamma_0 = \sigma^2 \sum_{i=0}^{\infty} \psi_i \psi_{i+0} = \sigma^2 \sum_{i=0}^{\infty} \psi_i \psi_i = \sigma^2 \sum_{i=0}^{\infty} \psi_i^2$$

III. Portanto, a vari√¢ncia √© dada por $\gamma_0 = \sigma^2 \sum_{i=0}^{\infty} \psi_i^2$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que $\sigma^2 = 2$ e $\psi_i = (0.8)^i$. Calcule a vari√¢ncia do processo MA(‚àû).
>
> $\gamma_0 = \sigma^2 \sum_{i=0}^{\infty} \psi_i^2 = 2 \sum_{i=0}^{\infty} (0.8^i)^2 = 2 \sum_{i=0}^{\infty} (0.64)^i = 2 \cdot \frac{1}{1 - 0.64} = 2 \cdot \frac{1}{0.36} = 2 \cdot \frac{100}{36} = \frac{200}{36} = \frac{50}{9} \approx 5.556$
>
> A vari√¢ncia do processo √© $\approx 5.556$. Isso significa que a dispers√£o dos valores $Y_t$ em torno da m√©dia $\mu$ √© maior quando comparada com o exemplo anterior.
>
> ```python
> import numpy as np
>
> sigma_squared = 2
> psi = lambda i: 0.8**i
>
> gamma_0 = sigma_squared * sum([psi(i)**2 for i in range(100)]) # Approximation to infinity
>
> print(f"Gamma_0: {gamma_0}")
> ```

> üí° **Exemplo Num√©rico:** Considere um processo MA(‚àû) com coeficientes $\psi_j = a^j$ onde $|a| < 1$ para garantir a condi√ß√£o de absolute summability e vari√¢ncia do ru√≠do branco $\sigma^2 = 1$. Vamos calcular algumas autocovari√¢ncias:
>
> *   $\gamma_0 = \sum_{i=0}^{\infty} (a^i)^2 = \sum_{i=0}^{\infty} a^{2i} = \frac{1}{1 - a^2}$
> *   $\gamma_1 = \sum_{i=0}^{\infty} a^i a^{i+1} = a \sum_{i=0}^{\infty} a^{2i} = \frac{a}{1 - a^2}$
> *   $\gamma_2 = \sum_{i=0}^{\infty} a^i a^{i+2} = a^2 \sum_{i=0}^{\infty} a^{2i} = \frac{a^2}{1 - a^2}$
>
> Em geral, $\gamma_k = \frac{a^k}{1 - a^2}$.

√â poss√≠vel obter os resultados para o processo MA(‚àû) por meio da **extrapola√ß√£o** dos resultados obtidos para um processo MA(q), fazendo $q$ tender ao infinito [^9].

Considere o processo MA(q):

$$Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q}$$

As autocovari√¢ncias para o processo MA(q) s√£o dadas por:

$$\gamma_k = \begin{cases}
\sigma^2 \sum_{i=0}^{q-k} \theta_i \theta_{i+k} & \text{se } 0 \leq k \leq q \\
0 & \text{se } k > q
\end{cases}$$

onde $\theta_0 = 1$.

*Prova:*

I.  Consideramos a defini√ß√£o do processo MA(q): $Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q}$. Assumimos $\theta_0 = 1$.

II. A autocovari√¢ncia $\gamma_k = E[(Y_t - \mu)(Y_{t-k} - \mu)]$ pode ser escrita como:

$$\gamma_k = E\left[ \left(\sum_{i=0}^{q} \theta_i \epsilon_{t-i}\right) \left(\sum_{j=0}^{q} \theta_j \epsilon_{t-k-j}\right) \right]$$

III. Trocando a ordem da esperan√ßa e da somat√≥ria:

$$\gamma_k = \sum_{i=0}^{q} \sum_{j=0}^{q} \theta_i \theta_j E[\epsilon_{t-i} \epsilon_{t-k-j}]$$

IV. Como $\epsilon_t$ √© ru√≠do branco, $E[\epsilon_{t-i} \epsilon_{t-k-j}] = \sigma^2$ se $i = k + j$, e $0$ caso contr√°rio.  Portanto, a somat√≥ria dupla se reduz aos termos onde $i = k + j$.

V. Substitu√≠mos $i$ por $k+j$:

$$\gamma_k = \sigma^2 \sum_{j=0}^{q} \theta_{k+j} \theta_j$$

VI. Se $k > q$, ent√£o $k+j > q$ para todo $j \geq 0$. Como $\theta_i = 0$ para todo $i > q$, ent√£o $\gamma_k = 0$.

VII. Se $0 \leq k \leq q$, ent√£o a somat√≥ria vai de $j=0$ at√© $q-k$, porque para $j > q-k$, $k+j > q$ e $\theta_{k+j} = 0$. Portanto:

$$\gamma_k = \sigma^2 \sum_{j=0}^{q-k} \theta_j \theta_{j+k}$$

VIII. Mudando o √≠ndice de $j$ para $i$, obtemos:

$$\gamma_k = \sigma^2 \sum_{i=0}^{q-k} \theta_i \theta_{i+k}$$

IX. Combinando os casos, temos:

$$\gamma_k = \begin{cases}
\sigma^2 \sum_{i=0}^{q-k} \theta_i \theta_{i+k} & \text{se } 0 \leq k \leq q \\
0 & \text{se } k > q
\end{cases}$$ $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um processo MA(2) com $\theta_1 = 0.6$, $\theta_2 = 0.4$, e $\sigma^2 = 1$. Calcule $\gamma_0$, $\gamma_1$ e $\gamma_2$.
>
> $\gamma_0 = \sigma^2 \sum_{i=0}^{2} \theta_i^2 = 1 \cdot (1^2 + 0.6^2 + 0.4^2) = 1 + 0.36 + 0.16 = 1.52$
>
> $\gamma_1 = \sigma^2 \sum_{i=0}^{1} \theta_i \theta_{i+1} = 1 \cdot (1 \cdot 0.6 + 0.6 \cdot 0.4) = 0.6 + 0.24 = 0.84$
>
> $\gamma_2 = \sigma^2 \sum_{i=0}^{0} \theta_i \theta_{i+2} = 1 \cdot (1 \cdot 0.4) = 0.4$
>
> Para $k > 2$, $\gamma_k = 0$.
>
> ```python
> import numpy as np
>
> sigma_squared = 1
> theta = [1, 0.6, 0.4] # Coefficients of the MA(2) process
>
> def autocovariance_ma_q(k, theta, sigma_squared):
>     q = len(theta) - 1
>     if k > q:
>         return 0
>     else:
>         return sigma_squared * sum([theta[i] * theta[i+k] for i in range(q-k+1)])
>
> gamma_0 = autocovariance_ma_q(0, theta, sigma_squared)
> gamma_1 = autocovariance_ma_q(1, theta, sigma_squared)
> gamma_2 = autocovariance_ma_q(2, theta, sigma_squared)
> gamma_3 = autocovariance_ma_q(3, theta, sigma_squared)
>
> print(f"Gamma_0: {gamma_0}")
> print(f"Gamma_1: {gamma_1}")
> print(f"Gamma_2: {gamma_2}")
> print(f"Gamma_3: {gamma_3}")
> ```

Para obter as autocovari√¢ncias do processo MA(‚àû), podemos fazer $q \rightarrow \infty$ e $\theta_i \rightarrow \psi_i$:

$$\gamma_k = \lim_{q \to \infty} \sigma^2 \sum_{i=0}^{q-k} \theta_i \theta_{i+k} = \sigma^2 \sum_{i=0}^{\infty} \psi_i \psi_{i+k}$$

*Prova:*
I. Partimos da express√£o da autocovari√¢ncia do MA(q):
$$\gamma_k(q) = \begin{cases}
\sigma^2 \sum_{i=0}^{q-k} \theta_i \theta_{i+k} & \text{se } 0 \leq k \leq q \\
0 & \text{se } k > q
\end{cases}$$
II. Tomamos o limite quando $q \to \infty$:
$$\lim_{q \to \infty} \gamma_k(q) = \lim_{q \to \infty} \sigma^2 \sum_{i=0}^{q-k} \theta_i \theta_{i+k}$$
III. Se assumirmos que $\lim_{q \to \infty} \theta_i = \psi_i$ para todo $i$, ent√£o podemos substituir $\theta_i$ por $\psi_i$:
$$\lim_{q \to \infty} \gamma_k(q) = \sigma^2 \lim_{q \to \infty} \sum_{i=0}^{q-k} \psi_i \psi_{i+k}$$
IV. Para $k$ fixo, quando $q \to \infty$, o limite superior da somat√≥ria tende ao infinito:
$$\lim_{q \to \infty} \gamma_k(q) = \sigma^2 \sum_{i=0}^{\infty} \psi_i \psi_{i+k}$$
V. Portanto, obtemos a autocovari√¢ncia do processo MA(‚àû):
$$\gamma_k = \sigma^2 \sum_{i=0}^{\infty} \psi_i \psi_{i+k}$$
VI. Conclu√≠mos que a abordagem de extrapola√ß√£o √© v√°lida e consistente. $\blacksquare$

Essa abordagem de extrapola√ß√£o √© consistente com os resultados derivados diretamente para o processo MA(‚àû). A seguir, apresentaremos algumas proposi√ß√µes para relacionar as propriedades dos coeficientes $\psi_j$ com o comportamento das autocovari√¢ncias $\gamma_k$.

**Lema 2.1**
Se $\psi_j = 0$ para todo $j > N$, ent√£o $\gamma_k = 0$ para todo $k > N$.

*Prova:*
I.  Assumimos que $\psi_j = 0$ para todo $j > N$.

II. Consideramos a express√£o da autocovari√¢ncia $\gamma_k = \sigma^2 \sum_{i=0}^{\infty} \psi_i \psi_{i+k}$.

III. Se $k > N$, ent√£o para cada $i$ na soma, pelo menos um dos termos $\psi_i$ ou $\psi_{i+k}$ ser√° zero.

IV. Para $0 \leq i \leq N$, temos que $i + k > N$, ent√£o $\psi_{i+k} = 0$.

V. Para $i > N$, temos que $\psi_i = 0$.

VI. Portanto, para $k > N$, cada termo na soma $\sum_{i=0}^{\infty} \psi_i \psi_{i+k}$ √© zero, e assim $\gamma_k = 0$.

VII. Conclu√≠mos que se $\psi_j = 0$ para todo $j > N$, ent√£o $\gamma_k = 0$ para todo $k > N$. $\blacksquare$

**Corol√°rio 2.1**
Se o processo MA(‚àû) se reduz a um processo MA(q) (i.e., $\psi_j = 0$ para todo $j>q$), ent√£o $\gamma_k = 0$ para todo $k > q$.

*Prova:*
Este √© um caso particular do Lema 2.1, onde $N = q$. Portanto, se o processo MA(‚àû) se reduz a um processo MA(q), ent√£o $\gamma_k = 0$ para todo $k > q$. $\blacksquare$

Dado que as autocovari√¢ncias $\gamma_k$ s√£o fun√ß√µes dos coeficientes $\psi_i$, podemos analisar o comportamento assint√≥tico de $\gamma_k$ quando $k$ tende ao infinito. Isso nos permite entender como a depend√™ncia temporal decai √† medida que o *lag* aumenta.

**Lema 2.2** Se $\sum_{i=0}^{\infty} |\psi_i| < \infty$ e $\lim_{i \to \infty} \psi_i = 0$, ent√£o $\lim_{k \to \infty} \gamma_k = 0$.

*Prova:*
I. Assumimos que $\sum_{i=0}^{\infty} |\psi_i| < \infty$ e $\lim_{i \to \infty} \psi_i = 0$.
II. Consideramos a express√£o da autocovari√¢ncia $\gamma_k = \sigma^2 \sum_{i=0}^{\infty} \psi_i \psi_{i+k}$.
III. Dado que $\lim_{i \to \infty} \psi_i = 0$, para qualquer $\epsilon > 0$, existe um $N$ tal que $|\psi_i| < \epsilon$ para todo $i > N$.
IV. Podemos dividir a soma em duas partes: $\gamma_k = \sigma^2 \left( \sum_{i=0}^{N} \psi_i \psi_{i+k} + \sum_{i=N+1}^{\infty} \psi_i \psi_{i+k} \right)$.
V. √Ä medida que $k \to \infty$, para qualquer $i$ fixo no intervalo $0 \leq i \leq N$, temos que $i+k \to \infty$, e portanto $\psi_{i+k} \to 0$. Logo, $\sum_{i=0}^{N} \psi_i \psi_{i+k} \to 0$.
VI. Para a segunda parte da soma, $\left| \sum_{i=N+1}^{\infty} \psi_i \psi_{i+k} \right| \leq \sum_{i=N+1}^{\infty} |\psi_i| |\psi_{i+k}|$. Como $\sum_{i=0}^{\infty} |\psi_i| < \infty$, a cauda da s√©rie converge para zero. Al√©m disso, como $\lim_{i \to \infty} \psi_i = 0$, ent√£o para $k$ suficientemente grande, $|\psi_{i+k}|$ ser√° pequeno.
VII. Portanto, $\lim_{k \to \infty} \gamma_k = 0$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha $\psi_i = (0.9)^i$. Ent√£o $\sum_{i=0}^{\infty} |\psi_i| = \sum_{i=0}^{\infty} (0.9)^i = \frac{1}{1-0.9} = 10 < \infty$ e $\lim_{i \to \infty} \psi_i = \lim_{i \to \infty} (0.9)^i = 0$. Portanto, $\lim_{k \to \infty} \gamma_k = 0$. Isso significa que a depend√™ncia entre os valores da s√©rie temporal diminui √† medida que a dist√¢ncia entre eles (o *lag*) aumenta. Um *lag* muito grande implica uma autocovari√¢ncia pr√≥xima de zero, indicando aus√™ncia de correla√ß√£o linear.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Define the coefficients psi_i
> psi = lambda i: 0.9**i
>
> # Calculate autocovariances for lags 0 to 20
> lags = np.arange(21)
> autocovariances = [sum([psi(i) * psi(i+k) for i in range(100)]) for k in lags] # Approximation
>
> # Plot the autocovariances
> plt.figure(figsize=(10, 6))
> plt.stem(lags, autocovariances, basefmt="b-")
> plt.title("Autocovariances of MA(‚àû) Process")
> plt.xlabel("Lag (k)")
> plt.ylabel("Autocovariance (Œ≥_k)")
> plt.grid(True)
> plt.show()
> ```
> The plot will show that the autocovariances decay towards zero as the lag increases, visually demonstrating Lema 2.2.

Este lema demonstra que, sob condi√ß√µes razo√°veis sobre os coeficientes $\psi_i$, a depend√™ncia temporal medida pelas autocovari√¢ncias decai para zero √† medida que o *lag* aumenta. Isso √© consistente com a ideia de que, em processos estacion√°rios, a influ√™ncia de *shocks* passados diminui com o tempo.

**Teorema 2.1** (Wold Decomposition)
Qualquer processo estoc√°stico estacion√°rio puramente n√£o-determin√≠stico pode ser representado como um processo MA(‚àû).

Este teorema fundamental, conhecido como a Decomposi√ß√£o de Wold, estabelece uma conex√£o profunda entre processos estacion√°rios e processos MA(‚àû). Ele garante que, sob certas condi√ß√µes, qualquer s√©rie temporal estacion√°ria pode ser expressa na forma de uma m√©dia m√≥vel de ordem infinita. Embora a prova completa do Teorema de Wold esteja al√©m do escopo deste texto, sua relev√¢ncia reside em fornecer uma justificativa te√≥rica para o uso extensivo de modelos MA(‚àû) na an√°lise de s√©ries temporais. Uma consequ√™ncia direta deste teorema √© a possibilidade de aproximar processos estacion√°rios complexos por modelos MA de ordem finita, como mencionado na conclus√£o deste cap√≠tulo.

### Conclus√£o

As autocovari√¢ncias do processo MA(‚àû) desempenham um papel fundamental na caracteriza√ß√£o da depend√™ncia temporal da s√©rie. Ao medir a correla√ß√£o entre os valores da s√©rie em diferentes *lags*, as autocovari√¢ncias fornecem *insights* valiosos sobre a estrutura do processo gerador de dados. O c√°lculo das autocovari√¢ncias envolve uma soma infinita, e a converg√™ncia dessa soma √© garantida pelas condi√ß√µes de summability dos coeficientes $\psi_j$. Al√©m disso, a abordagem de extrapola√ß√£o dos resultados do processo MA(q) para o caso MA(‚àû) oferece uma perspectiva √∫til para entender a rela√ß√£o entre esses dois tipos de processos.  Em cen√°rios pr√°ticos, onde a estimativa direta de um n√∫mero infinito de coeficientes √© invi√°vel, aproxima√ß√µes por modelos MA de ordem finita ou ARMA s√£o frequentemente empregadas para modelar as autocovari√¢ncias e, consequentemente, a depend√™ncia temporal de longo alcance. A pr√≥xima se√ß√£o abordar√° t√©cnicas para modelar e estimar os coeficientes de processos MA(‚àû) em situa√ß√µes do mundo real.

### Refer√™ncias

[^4]: "The basic building block for all the processes considered in this chapter is a sequence {8} - whose elements have mean zero and variance œÉ¬≤"
[^9]: "Y, = u + Œ£œàŒ≥ŒµŒπ = Œº + œàŒøŒµŒπ + œàŒπŒµŒπ-1 + œà281-2+.... This could be described as an MA(‚àû) process."
<!-- END -->