## An√°lise Detalhada do Processo de M√©dia M√≥vel de Ordem Infinita (MA(‚àû)): Summabilidade e Ergodicidade

### Introdu√ß√£o

Este cap√≠tulo expande nossa an√°lise do processo de m√©dia m√≥vel de ordem infinita (MA(‚àû)), focando nas condi√ß√µes de summabilidade dos coeficientes e suas implica√ß√µes para a ergodicidade do processo. Em continuidade com a defini√ß√£o, propriedades gerais e autocovari√¢ncias do MA(‚àû) [^9], exploraremos como a absoluta summabilidade dos coeficientes $\psi_j$ garante a absoluta summabilidade das autocovari√¢ncias $\gamma_k$, e como isso, por sua vez, implica que o processo √© erg√≥dico para a m√©dia. Discutiremos tamb√©m a rela√ß√£o entre absoluta summability e square summability, e como essas condi√ß√µes afetam as propriedades estat√≠sticas do MA(‚àû).

### Conceitos Fundamentais

Recordando, o processo **MA(‚àû)** √© definido como [^9]:

$$Y_t = \mu + \sum_{j=0}^{\infty} \psi_j \epsilon_{t-j}$$

onde:

*   $Y_t$ representa o valor da s√©rie temporal no instante $t$.
*   $\mu$ √© a m√©dia do processo.
*   $\epsilon_t$ √© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia constante $\sigma^2$ [^4].
*   $\psi_j$ s√£o os coeficientes que ponderam a influ√™ncia dos *shocks* passados $\epsilon_{t-j}$ no valor atual $Y_t$ [^9], com $\psi_0 = 1$.

A **absoluta summabilidade** dos coeficientes $\psi_j$ √© definida como [^9]:

$$\sum_{j=0}^{\infty} |\psi_j| < \infty$$

> üí° **Exemplo Num√©rico:** Considere $\psi_j = a^j$, onde $|a| < 1$. Ent√£o $\sum_{j=0}^{\infty} |\psi_j| = \sum_{j=0}^{\infty} |a|^j = \frac{1}{1 - |a|} < \infty$. Por exemplo, se $a = 0.5$, ent√£o $\sum_{j=0}^{\infty} (0.5)^j = \frac{1}{1 - 0.5} = 2$. Se $a = 0.9$, ent√£o $\sum_{j=0}^{\infty} (0.9)^j = \frac{1}{1 - 0.9} = 10$. Se $|a| \geq 1$, a soma diverge.

A **square summability** dos coeficientes $\psi_j$ √© definida como [^9]:

$$\sum_{j=0}^{\infty} \psi_j^2 < \infty$$

> üí° **Exemplo Num√©rico:** Novamente, considere $\psi_j = a^j$, onde $|a| < 1$. Ent√£o $\sum_{j=0}^{\infty} \psi_j^2 = \sum_{j=0}^{\infty} (a^j)^2 = \sum_{j=0}^{\infty} (a^2)^j = \frac{1}{1 - a^2} < \infty$. Por exemplo, se $a = 0.5$, ent√£o $\sum_{j=0}^{\infty} (0.5)^{2j} = \frac{1}{1 - 0.25} = \frac{4}{3}$.

A absoluta summabilidade implica square summability, mas o inverso n√£o √© necessariamente verdadeiro [^9].

Antes de prosseguirmos, √© √∫til estabelecer um resultado auxiliar que ser√° usado posteriormente:

**Lema 1**
Se $\sum_{j=0}^{\infty} |\psi_j| < \infty$, ent√£o $\left(\sum_{j=0}^{\infty} |\psi_j|\right)^2 = \sum_{i=0}^{\infty} \sum_{j=0}^{\infty} |\psi_i||\psi_j| < \infty$.

*Prova:*
Como $\sum_{j=0}^{\infty} |\psi_j|$ converge para um valor finito, digamos $S$, ent√£o $S < \infty$.  Portanto, $S^2$ tamb√©m √© finito, ou seja, $S^2 = \left(\sum_{j=0}^{\infty} |\psi_j|\right)^2 < \infty$. Expandindo o quadrado, temos $\left(\sum_{j=0}^{\infty} |\psi_j|\right)^2 = \sum_{i=0}^{\infty} \sum_{j=0}^{\infty} |\psi_i||\psi_j|$, que √©, portanto, finito. $\blacksquare$

> üí° **Exemplo Num√©rico:** Seja $\psi_j = (0.5)^j$.  Ent√£o, $\sum_{j=0}^{\infty} |\psi_j| = 2$, e $\left(\sum_{j=0}^{\infty} |\psi_j|\right)^2 = 2^2 = 4$. Agora, $\sum_{i=0}^{\infty} \sum_{j=0}^{\infty} |\psi_i||\psi_j| = \sum_{i=0}^{\infty} \sum_{j=0}^{\infty} (0.5)^i (0.5)^j = \left(\sum_{i=0}^{\infty} (0.5)^i\right) \left(\sum_{j=0}^{\infty} (0.5)^j\right) = 2 \cdot 2 = 4$.

**Teorema 3**
Se a sequ√™ncia de coeficientes $\{\psi_j\}$ for absolutamente summable, ou seja, $\sum_{j=0}^{\infty} |\psi_j| < \infty$, ent√£o as autocovari√¢ncias $\gamma_k$ tamb√©m s√£o absolutamente summables: $\sum_{k=-\infty}^{\infty} |\gamma_k| < \infty$ [^9].

A prova deste teorema foi apresentada no cap√≠tulo anterior e demonstrada que a condi√ß√£o de absoluta summability dos coeficientes $\psi_j$ implica na absoluta summability das autocovari√¢ncias $\gamma_k$.

> üí° **Exemplo Num√©rico:** Seja $\psi_j = (0.5)^j$ e $\epsilon_t \sim N(0, 1)$. Ent√£o, $\gamma_k = \sigma^2 \sum_{i=0}^{\infty} \psi_i \psi_{i+k} = 1 \cdot \sum_{i=0}^{\infty} (0.5)^i (0.5)^{i+k} = \sum_{i=0}^{\infty} (0.5)^{2i+k} = (0.5)^k \sum_{i=0}^{\infty} (0.25)^i = (0.5)^k \cdot \frac{1}{1-0.25} = \frac{4}{3} (0.5)^k$. Agora, $\sum_{k=-\infty}^{\infty} |\gamma_k| = \sum_{k=-\infty}^{\infty} \frac{4}{3} (0.5)^{|k|} = \frac{4}{3} \left( 1 + 2\sum_{k=1}^{\infty} (0.5)^k \right) = \frac{4}{3} \left( 1 + 2 \cdot \frac{0.5}{1-0.5} \right) = \frac{4}{3} (1 + 2) = 4 < \infty$.

**Teorema 4**
Se um processo √© covariance-stationary e suas autocovari√¢ncias s√£o absolutamente summable, ent√£o o processo √© erg√≥dico para a m√©dia.

Este teorema estabelece uma liga√ß√£o fundamental entre a estrutura de depend√™ncia de um processo (medida pelas autocovari√¢ncias) e sua propriedade de ergodicidade. A ergodicidade para a m√©dia significa que a m√©dia amostral da s√©rie temporal converge em probabilidade para a m√©dia populacional √† medida que o tamanho da amostra aumenta. Isso permite inferir informa√ß√µes sobre a m√©dia do processo a partir de uma √∫nica realiza√ß√£o da s√©rie temporal, o que √© crucial para a an√°lise de dados do mundo real, onde geralmente temos apenas uma amostra observada.

**Lema 4.1:**
Se $\sum_{k=-\infty}^{\infty} |\gamma_k| < \infty$, ent√£o o processo MA(‚àû) √© erg√≥dico para a m√©dia.

*Prova:*
I.  A condi√ß√£o de ergodicidade para a m√©dia √© que a m√©dia amostral $\bar{Y} = \frac{1}{T}\sum_{t=1}^{T} Y_t$ converge em probabilidade para a m√©dia populacional $\mu$.

II. Para demonstrar a ergodicidade, mostramos que $\lim_{T \to \infty} Var(\bar{Y}) = 0$.

III. A vari√¢ncia da m√©dia amostral √© dada por:

$$Var(\bar{Y}) = Var\left(\frac{1}{T}\sum_{t=1}^{T} Y_t\right) = \frac{1}{T^2} Var\left(\sum_{t=1}^{T} Y_t\right)$$

IV. Usando a propriedade da estacionaridade, temos:

$$Var(\bar{Y}) = \frac{1}{T^2} \sum_{t=1}^{T} \sum_{s=1}^{T} Cov(Y_t, Y_s) = \frac{1}{T^2} \sum_{t=1}^{T} \sum_{s=1}^{T} \gamma_{|t-s|}$$

V. Reescrevemos a soma dupla em termos de $k = t-s$:

$$Var(\bar{Y}) = \frac{1}{T^2} \sum_{k=-(T-1)}^{T-1} (T-|k|) \gamma_k$$

VI. Dividimos por T:

$$Var(\bar{Y}) = \frac{1}{T} \sum_{k=-(T-1)}^{T-1} \left(1-\frac{|k|}{T}\right) \gamma_k$$

VII. Tomamos o limite quando $T \to \infty$:

$$\lim_{T \to \infty} Var(\bar{Y}) = \lim_{T \to \infty} \frac{1}{T} \sum_{k=-(T-1)}^{T-1} \left(1-\frac{|k|}{T}\right) \gamma_k$$

VIII. Dado que $\sum_{k=-\infty}^{\infty} |\gamma_k| < \infty$, podemos usar o teorema da converg√™ncia dominada para trocar o limite e a somat√≥ria:

$$\lim_{T \to \infty} Var(\bar{Y}) = \lim_{T \to \infty}  \frac{1}{T}\sum_{k=-\infty}^{\infty}  \gamma_k = 0$$

IX. Isso implica que $\lim_{T \to \infty} Var(\bar{Y}) = 0$.

X. Portanto, o processo MA(‚àû) √© erg√≥dico para a m√©dia. $\blacksquare$

Combinando este resultado com o Teorema 3 do cap√≠tulo anterior, provamos que a absoluta summabilidade dos coeficientes $\psi_j$ implica que o processo MA(‚àû) √© erg√≥dico para a m√©dia [^9].

> üí° **Exemplo Num√©rico:** Suponha que $\gamma_k = \frac{1}{k^2 + 1}$ para $k \neq 0$ e $\gamma_0 = 1$. Ent√£o $\sum_{k=-\infty}^{\infty} |\gamma_k| = 1 + 2\sum_{k=1}^{\infty} \frac{1}{k^2 + 1} < \infty$ (converge). Portanto, o processo √© erg√≥dico para a m√©dia.

Para complementar a an√°lise da ergodicidade, podemos examinar como a autocovari√¢ncia se comporta sob a condi√ß√£o de absoluta summability.

**Teorema 4.1**
Se $\sum_{j=0}^{\infty} |\psi_j| < \infty$, ent√£o $|\gamma_k| \leq \sigma^2 \left(\sum_{j=0}^{\infty} |\psi_j|\right)^2$ para todo $k$.

*Prova:*
I.  Sabemos que $\gamma_k = Cov(Y_t, Y_{t+k}) = E[(Y_t - \mu)(Y_{t+k} - \mu)]$.

II. Substituindo a defini√ß√£o do processo MA(‚àû), temos:

    $\gamma_k = E\left[\left(\sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}\right)\left(\sum_{j=0}^{\infty} \psi_j \epsilon_{t+k-j}\right)\right]$.

III. Expandindo e usando a propriedade de ortogonalidade do ru√≠do branco $E[\epsilon_t \epsilon_s] = \sigma^2 \delta_{ts}$, onde $\delta_{ts}$ √© a fun√ß√£o delta de Kronecker, obtemos:

    $\gamma_k = \sigma^2 \sum_{i=0}^{\infty} \psi_i \psi_{i+k}$.

IV. Tomando o valor absoluto, temos:

    $|\gamma_k| = \sigma^2 \left| \sum_{i=0}^{\infty} \psi_i \psi_{i+k} \right| \leq \sigma^2 \sum_{i=0}^{\infty} |\psi_i| |\psi_{i+k}|$.

V. Reescrevendo a somat√≥ria, temos:

    $|\gamma_k| \leq \sigma^2 \sum_{i=0}^{\infty} |\psi_i| |\psi_{i+k}|  \leq \sigma^2 \sum_{i=0}^{\infty} |\psi_i| \sum_{j=0}^{\infty} |\psi_j|$.

VI. Aplicando o Lema 1:

    $|\gamma_k| \leq \sigma^2 \left(\sum_{j=0}^{\infty} |\psi_j|\right)^2$. $\blacksquare$

Este teorema fornece um limite superior para o valor absoluto da autocovari√¢ncia em termos da vari√¢ncia do ru√≠do branco e da soma dos valores absolutos dos coeficientes $\psi_j$.

> üí° **Exemplo Num√©rico:** Seja $\psi_j = (0.5)^j$ e $\sigma^2 = 1$. Ent√£o, $\sum_{j=0}^{\infty} |\psi_j| = 2$, e $|\gamma_k| \leq 1 \cdot (2)^2 = 4$ para todo $k$. Isso significa que a autocovari√¢ncia √© sempre limitada por 4, independentemente do lag $k$.

#### Implica√ß√µes Pr√°ticas da Ergodicidade

A propriedade de ergodicidade para a m√©dia √© crucial para a an√°lise de s√©ries temporais, pois permite estimar a m√©dia populacional $\mu$ a partir de uma √∫nica realiza√ß√£o da s√©rie temporal. Em particular, se o processo √© erg√≥dico para a m√©dia, ent√£o a m√©dia amostral $\bar{Y}$ converge em probabilidade para $\mu$ √† medida que o tamanho da amostra $T$ aumenta [^9]:

$$\bar{Y} \xrightarrow{p} \mu \text{ quando } T \rightarrow \infty$$

Isso significa que, para amostras suficientemente grandes, a m√©dia amostral $\bar{Y}$ √© uma estimativa precisa da m√©dia populacional $\mu$.

Al√©m disso, se os *shocks* $\epsilon_t$ forem Gaussianos, ent√£o o processo MA(‚àû) √© erg√≥dico para todos os momentos [^9]. Isso implica que n√£o apenas a m√©dia amostral converge para a m√©dia populacional, mas tamb√©m outros momentos amostrais (como a vari√¢ncia amostral) convergem para os momentos populacionais correspondentes. A ergodicidade para todos os momentos √© uma propriedade mais forte que a ergodicidade para a m√©dia e fornece uma base s√≥lida para a infer√™ncia estat√≠stica sobre todas as caracter√≠sticas do processo MA(‚àû).

#### Exemplo Num√©rico

Considere um processo MA(‚àû) com $\psi_j = (0.5)^j$ e $\epsilon_t \sim N(0, 1)$. Como $\sum_{j=0}^{\infty} |\psi_j| = \sum_{j=0}^{\infty} (0.5)^j = 2 < \infty$, o processo √© absolutamente summable e, portanto, erg√≥dico para a m√©dia. Isso significa que, √† medida que aumentamos o tamanho da amostra $T$, a m√©dia amostral $\bar{Y}$ ir√° se aproximar da m√©dia populacional $\mu$. Se gerarmos uma s√©rie temporal longa deste processo e calcularmos a m√©dia amostral, podemos verificar essa converg√™ncia.

```python
import numpy as np
import matplotlib.pyplot as plt

# Parameters of the MA(‚àû) process
mu = 0
sigma = 1
psi = lambda j: 0.5**j

# Generate a long time series
T = 10000
epsilon = np.random.normal(0, sigma, T)
Y = np.zeros(T)

for t in range(T):
    Y[t] = mu + sum([psi(j) * epsilon[t-j] for j in range(min(t+1, 100))]) # Truncate to 100 lags

# Calculate the sample mean
sample_mean = np.mean(Y)

# Print the results
print(f"Population Mean: {mu}")
print(f"Sample Mean: {sample_mean}")

# Check convergence visually
sample_means = [np.mean(Y[:t]) for t in range(100, T)]
plt.figure(figsize=(10, 6))
plt.plot(range(100, T), sample_means)
plt.axhline(y=mu, color='r', linestyle='--', label='Population Mean')
plt.title("Convergence of Sample Mean to Population Mean")
plt.xlabel("Sample Size (T)")
plt.ylabel("Sample Mean")
plt.legend()
plt.grid(True)
plt.show()
```

Este c√≥digo gera uma longa s√©rie temporal de um processo MA(‚àû) com coeficientes decaindo exponencialmente e ru√≠do branco Gaussiano. Ele calcula a m√©dia amostral e verifica visualmente sua converg√™ncia para a m√©dia populacional, demonstrando a propriedade de ergodicidade para a m√©dia.

### Absoluta Summabilidade vs. Square Summability

Como mencionado anteriormente, a absoluta summability implica square summability, mas o inverso n√£o √© necessariamente verdadeiro [^9]. Isso significa que, se a soma dos valores absolutos dos coeficientes for finita, ent√£o a soma dos quadrados dos coeficientes tamb√©m ser√° finita. No entanto, √© poss√≠vel que a soma dos quadrados dos coeficientes seja finita, enquanto a soma dos valores absolutos dos coeficientes seja infinita.

Para ilustrar essa distin√ß√£o, considere a seguinte proposi√ß√£o e seu respectivo corol√°rio:

**Proposi√ß√£o 4.1**
Se $\sum_{j=0}^{\infty} |\psi_j| < \infty$ e $\sigma^2 < \infty$, ent√£o o processo MA(‚àû) tem vari√¢ncia finita e √© covariance-stationary.

*Prova:*
I.  Assumimos que $\sum_{j=0}^{\infty} |\psi_j| < \infty$ e $\sigma^2 < \infty$.

II. Da proposi√ß√£o 1, sabemos que a absoluta summability implica square summability, ou seja, $\sum_{j=0}^{\infty} \psi_j^2 < \infty$.

III. A vari√¢ncia do processo MA(‚àû) √© dada por $\gamma_0 = \sigma^2 \sum_{j=0}^{\infty} \psi_j^2$.

IV. Como $\sum_{j=0}^{\infty} \psi_j^2 < \infty$ e $\sigma^2 < \infty$, o produto $\sigma^2 \sum_{j=0}^{\infty} \psi_j^2$ √© finito.

V. Portanto, a vari√¢ncia $\gamma_0$ √© finita.

VI. As autocovari√¢ncias s√£o dadas por $\gamma_k = \sigma^2 \sum_{i=0}^{\infty} \psi_i \psi_{i+k}$.

VII. Pelo Teorema 3, sabemos que se a sequ√™ncia de coeficientes $\{\psi_j\}$ for absolutamente summable, ent√£o as autocovari√¢ncias $\gamma_k$ tamb√©m s√£o absolutamente summables: $\sum_{k=-\infty}^{\infty} |\gamma_k| < \infty$.

VIII. Como a vari√¢ncia √© finita e as autocovari√¢ncias s√£o absolutamente summables, o processo MA(‚àû) √© covariance-stationary. $\blacksquare$

> üí° **Exemplo Num√©rico:** Seja $\psi_j = (0.8)^j$ e $\sigma^2 = 2$. Ent√£o, $\sum_{j=0}^{\infty} |\psi_j| = \frac{1}{1 - 0.8} = 5 < \infty$. Portanto, $\gamma_0 = 2 \cdot \sum_{j=0}^{\infty} (0.8)^{2j} = 2 \cdot \frac{1}{1 - 0.64} = 2 \cdot \frac{1}{0.36} = \frac{50}{9} < \infty$.

O **Corol√°rio 4.1** explicita os resultados de ergodicidade:

**Corol√°rio 4.1**
Se $\sum_{j=0}^{\infty} |\psi_j| < \infty$ e $\sigma^2 < \infty$, ent√£o o processo MA(‚àû) √© erg√≥dico para a m√©dia.

*Prova:*
Este corol√°rio segue diretamente da Proposi√ß√£o 4.1 e do Teorema 4, que estabelecem que um processo covariance-stationary com autocovari√¢ncias absolutamente summables √© erg√≥dico para a m√©dia. Portanto, se $\sum_{j=0}^{\infty} |\psi_j| < \infty$ e $\sigma^2 < \infty$, o processo MA(‚àû) √© erg√≥dico para a m√©dia. $\blacksquare$

Para complementar a discuss√£o sobre square summability, apresentamos a seguinte proposi√ß√£o:

**Proposi√ß√£o 4.2**
Se $\sum_{j=0}^{\infty} \psi_j^2 < \infty$ e $\sigma^2 < \infty$, ent√£o o processo MA(‚àû) tem vari√¢ncia finita.

*Prova:*
I. Assumimos que $\sum_{j=0}^{\infty} \psi_j^2 < \infty$ e $\sigma^2 < \infty$.
II. A vari√¢ncia do processo MA(‚àû) √© dada por $\gamma_0 = \sigma^2 \sum_{j=0}^{\infty} \psi_j^2$.
III. Como $\sum_{j=0}^{\infty} \psi_j^2 < \infty$ e $\sigma^2 < \infty$, o produto $\sigma^2 \sum_{j=0}^{\infty} \psi_j^2$ √© finito.
IV. Portanto, a vari√¢ncia $\gamma_0$ √© finita. $\blacksquare$

Esta proposi√ß√£o mostra que square summability √© suficiente para garantir que o processo MA(‚àû) tenha vari√¢ncia finita, mesmo que a absoluta summability n√£o seja satisfeita.  Contudo, a estacionariedade e ergodicidade n√£o s√£o garantidas apenas com essa condi√ß√£o.

Para demonstrar que square summability n√£o implica absoluta summability, considere o seguinte exemplo:

**Exemplo:**
Seja $\psi_j = \frac{1}{j+1}$. Ent√£o, $\sum_{j=0}^{\infty} \psi_j^2 = \sum_{j=0}^{\infty} \frac{1}{(j+1)^2} = \frac{\pi^2}{6} < \infty$.  No entanto, $\sum_{j=0}^{\infty} |\psi_j| = \sum_{j=0}^{\infty} \frac{1}{j+1}$, que √© a s√©rie harm√¥nica, e diverge.

Este exemplo ilustra claramente que a condi√ß√£o de square summability n√£o √© suficiente para garantir a absoluta summability dos coeficientes. Em outras palavras, um processo MA(‚àû) pode ter coeficientes que s√£o square summable, garantindo vari√¢ncia finita, mas n√£o absolutamente summable, o que implica que a ergodicidade n√£o est√° garantida.

> üí° **Exemplo Num√©rico:** Considere $\psi_j = \frac{1}{j+1}$ e $\epsilon_t \sim N(0, 1)$.  A vari√¢ncia do processo ser√° finita: $\gamma_0 = \sigma^2 \sum_{j=0}^{\infty} \psi_j^2 = 1 \cdot \sum_{j=0}^{\infty} \frac{1}{(j+1)^2} = \frac{\pi^2}{6} \approx 1.645$. No entanto, como $\sum_{j=0}^{\infty} |\psi_j|$ diverge, a ergodicidade n√£o √© garantida, e a m√©dia amostral pode n√£o convergir para a m√©dia populacional conforme $T \to \infty$.

Al√©m disso, podemos analisar a converg√™ncia em $L^2$:

**Teorema 4.2**
Se $\sum_{j=0}^{\infty} \psi_j^2 < \infty$, ent√£o a s√©rie $\sum_{j=0}^{\infty} \psi_j \epsilon_{t-j}$ converge em $L^2$.

*Prova:*
I.  Seja $S_n = \sum_{j=0}^{n} \psi_j \epsilon_{t-j}$.
II. Queremos mostrar que $S_n$ converge em $L^2$, ou seja, que existe um $S$ tal que $E[(S_n - S)^2] \to 0$ quando $n \to \infty$.
III. Para isso, basta mostrar que $S_n$ √© uma sequ√™ncia de Cauchy em $L^2$, ou seja, que para todo $\varepsilon > 0$, existe $N$ tal que para todo $n, m > N$, $E[(S_n - S_m)^2] < \varepsilon$.
IV. Suponha $n > m$. Ent√£o, $S_n - S_m = \sum_{j=m+1}^{n} \psi_j \epsilon_{t-j}$.
V. Portanto, $E[(S_n - S_m)^2] = E\left[\left(\sum_{j=m+1}^{n} \psi_j \epsilon_{t-j}\right)^2\right] = \sum_{j=m+1}^{n} \psi_j^2 E[\epsilon_{t-j}^2] = \sigma^2 \sum_{j=m+1}^{n} \psi_j^2$.
VI. Como $\sum_{j=0}^{\infty} \psi_j^2 < \infty$, a cauda da s√©rie converge para zero. Ou seja, para todo $\varepsilon > 0$, existe $N$ tal que para todo $n > m > N$, $\sum_{j=m+1}^{n} \psi_j^2 < \frac{\varepsilon}{\sigma^2}$.
VII. Logo, $E[(S_n - S_m)^2] < \varepsilon$ para todo $n > m > N$.
VIII. Portanto, $S_n$ √© uma sequ√™ncia de Cauchy em $L^2$ e converge em $L^2$. $\blacksquare$

Este teorema mostra que, mesmo que a condi√ß√£o de absoluta summability n√£o seja satisfeita, a condi√ß√£o de square summability garante que a s√©rie que define o processo MA(‚àû) converge em $L^2$. Isso significa que o processo MA(‚àû) est√° bem definido como um limite em $L^2$, mesmo que a soma dos valores absolutos dos coeficientes seja infinita.

### Conclus√£o

A absoluta summability dos coeficientes $\psi_j$ √© uma condi√ß√£o crucial para garantir que o processo MA(‚àû) seja bem definido, covariance-stationary e erg√≥dico para a m√©dia. Essa condi√ß√£o garante que a influ√™ncia de *shocks* passados decaia suficientemente r√°pido, de modo que a vari√¢ncia do processo seja finita e a m√©dia amostral convirja para a m√©dia populacional. A ergodicidade para a m√©dia permite inferir informa√ß√µes sobre a m√©dia do processo a partir de uma √∫nica realiza√ß√£o da s√©rie temporal, tornando o MA(‚àû) uma ferramenta poderosa para a an√°lise de dados do mundo real.  Em situa√ß√µes onde os coeficientes n√£o satisfazem a condi√ß√£o de absoluta summability, √© necess√°rio ter cautela na interpreta√ß√£o dos resultados e considerar outras abordagens de modelagem que possam ser mais apropriadas.

### Refer√™ncias

[^4]: "The basic building block for all the processes considered in this chapter is a sequence {8} - whose elements have mean zero and variance œÉ¬≤"
[^9]: "Y, = u + Œ£œàŒ≥ŒµŒπ = Œº + œàŒøŒµŒπ + œàŒπŒµŒπ-1 + œà281-2+.... This could be described as an MA(‚àû) process."
<!-- END -->