## An√°lise Detalhada do Processo de M√©dia M√≥vel de Ordem Infinita (MA(‚àû))

### Introdu√ß√£o
Este cap√≠tulo explora em profundidade o processo de m√©dia m√≥vel de ordem infinita (MA(‚àû)), um conceito fundamental na an√°lise de s√©ries temporais. Construindo sobre os conceitos de processos de m√©dia m√≥vel (MA) de ordem finita, o MA(‚àû) estende a depend√™ncia temporal para um n√∫mero infinito de lags passados. Exploraremos as condi√ß√µes de converg√™ncia, as propriedades estat√≠sticas e o papel crucial deste processo na representa√ß√£o de s√©ries temporais complexas [^9].

### Conceitos Fundamentais

O processo **MA(‚àû)** √© definido como [^9]:
$$Y_t = \mu + \sum_{j=0}^{\infty} \psi_j \epsilon_{t-j}$$

onde:
*   $Y_t$ representa o valor da s√©rie temporal no instante $t$.
*   $\mu$ √© a m√©dia do processo.
*   $\epsilon_t$ √© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia constante $\sigma^2$ [^4].
*   $\psi_j$ s√£o os coeficientes que ponderam a influ√™ncia dos *shocks* passados $\epsilon_{t-j}$ no valor atual $Y_t$ [^9]. Por defini√ß√£o, $\psi_0 = 1$.

> üí° **Exemplo Num√©rico:** Considere um processo MA(‚àû) com $\mu = 10$ e $\psi_j = (0.5)^j$. Suponha que tenhamos os seguintes valores para o ru√≠do branco: $\epsilon_t = 0.2$, $\epsilon_{t-1} = -0.1$, $\epsilon_{t-2} = 0.3$, $\epsilon_{t-3} = -0.2$. Ent√£o,
>
> $Y_t = 10 + (0.5)^0(0.2) + (0.5)^1(-0.1) + (0.5)^2(0.3) + (0.5)^3(-0.2) + \dots$
>
> $Y_t = 10 + (1)(0.2) + (0.5)(-0.1) + (0.25)(0.3) + (0.125)(-0.2) + \dots$
>
> $Y_t = 10 + 0.2 - 0.05 + 0.075 - 0.025 + \dots \approx 10.2$ (considerando apenas os 4 primeiros termos al√©m da m√©dia).
>
> Este exemplo ilustra como os *shocks* passados afetam o valor atual $Y_t$, com a influ√™ncia diminuindo exponencialmente √† medida que o *lag* aumenta.

A **expectativa** do processo MA(‚àû) √© dada por [^9]:
$$E(Y_t) = E\left[\mu + \sum_{j=0}^{\infty} \psi_j \epsilon_{t-j}\right] = \mu + \sum_{j=0}^{\infty} \psi_j E[\epsilon_{t-j}] = \mu$$

Isso se deve √† propriedade do ru√≠do branco ter m√©dia zero: $E[\epsilon_{t-j}] = 0$ para todo $j$ [^4].

Para que o processo MA(‚àû) seja bem definido e covariance-stationary, os coeficientes $\psi_j$ devem satisfazer certas condi√ß√µes de converg√™ncia. A condi√ß√£o mais comum √© a **absoluta summabilidade**, que exige que a soma dos valores absolutos dos coeficientes seja finita [^9]:

$$\sum_{j=0}^{\infty} |\psi_j| < \infty$$

> üí° **Exemplo Num√©rico:**  Considerando o exemplo anterior, onde $\psi_j = (0.5)^j$, vamos verificar a absoluta summability:
>
> $\sum_{j=0}^{\infty} |(0.5)^j| = \sum_{j=0}^{\infty} (0.5)^j = \frac{1}{1 - 0.5} = \frac{1}{0.5} = 2 < \infty$
>
> Neste caso, a condi√ß√£o de absoluta summability √© satisfeita. Isso significa que o processo MA(‚àû) com $\psi_j = (0.5)^j$ √© bem definido e covariance-stationary.

Uma condi√ß√£o mais fraca, mas ainda importante, √© a **square summability**, que requer que a soma dos quadrados dos coeficientes seja finita [^9]:

$$\sum_{j=0}^{\infty} \psi_j^2 < \infty$$

A absoluta summabilidade implica square summability, mas o inverso n√£o √© necessariamente verdadeiro [^9]. A square summability √© suficiente para garantir que a vari√¢ncia do processo MA(‚àû) seja finita.

**Proposi√ß√£o 1**
Se $\sum_{j=0}^{\infty} |\psi_j| < \infty$, ent√£o $\sum_{j=0}^{\infty} \psi_j^2 < \infty$.

*Prova:*
I.  Dado que $\sum_{j=0}^{\infty} |\psi_j|$ converge absolutamente, para qualquer $\epsilon > 0$, existe um inteiro $N$ tal que para todo $j > N$, temos $|\psi_j| < \epsilon$.

II. Escolhendo $\epsilon = 1$, existe um $N$ tal que para $j > N$, $|\psi_j| < 1$.

III. Portanto, para $j > N$, temos $\psi_j^2 = |\psi_j|^2 < |\psi_j|$.

IV. Agora, considere a soma $\sum_{j=0}^{\infty} \psi_j^2$. Podemos dividi-la em duas partes: $\sum_{j=0}^{N} \psi_j^2 + \sum_{j=N+1}^{\infty} \psi_j^2$.

V. A primeira soma, $\sum_{j=0}^{N} \psi_j^2$, √© finita porque √© uma soma finita de termos finitos.

VI. Para a segunda soma, $\sum_{j=N+1}^{\infty} \psi_j^2$, como $\psi_j^2 < |\psi_j|$ para $j > N$, temos $\sum_{j=N+1}^{\infty} \psi_j^2 < \sum_{j=N+1}^{\infty} |\psi_j|$.

VII. Uma vez que $\sum_{j=0}^{\infty} |\psi_j|$ converge, $\sum_{j=N+1}^{\infty} |\psi_j|$ tamb√©m converge (pois √© a cauda de uma s√©rie convergente).

VIII. Portanto, $\sum_{j=N+1}^{\infty} \psi_j^2$ converge.

IX. Assim, $\sum_{j=0}^{\infty} \psi_j^2 = \sum_{j=0}^{N} \psi_j^2 + \sum_{j=N+1}^{\infty} \psi_j^2$ √© a soma de duas s√©ries convergentes, e, portanto, converge.

X. Conclu√≠mos que se $\sum_{j=0}^{\infty} |\psi_j| < \infty$, ent√£o $\sum_{j=0}^{\infty} \psi_j^2 < \infty$. $\blacksquare$

A **vari√¢ncia** do processo MA(‚àû) √© dada por [^9]:

$$\gamma_0 = E[(Y_t - \mu)^2] = E\left[\left(\sum_{j=0}^{\infty} \psi_j \epsilon_{t-j}\right)^2\right] = \sigma^2 \sum_{j=0}^{\infty} \psi_j^2$$

> üí° **Exemplo Num√©rico:** Continuando com o exemplo $\psi_j = (0.5)^j$, e assumindo que $\sigma^2 = 1$, vamos calcular a vari√¢ncia:
>
> $\gamma_0 = (1) \sum_{j=0}^{\infty} ((0.5)^j)^2 = \sum_{j=0}^{\infty} (0.25)^j = \frac{1}{1 - 0.25} = \frac{1}{0.75} = \frac{4}{3} \approx 1.33$
>
> A vari√¢ncia do processo MA(‚àû) neste exemplo √© finita e igual a 4/3, o que est√° de acordo com a condi√ß√£o de square summability.

*Prova:*
I.  Substitu√≠mos $Y_t - \mu$ na express√£o da vari√¢ncia:
    $$ \gamma_0 = E[(Y_t - \mu)^2] = E\left[\left(\sum_{j=0}^{\infty} \psi_j \epsilon_{t-j}\right)^2\right] $$

II. Expandimos o quadrado da soma:
    $$ \gamma_0 = E\left[\left(\sum_{j=0}^{\infty} \psi_j \epsilon_{t-j}\right) \left(\sum_{k=0}^{\infty} \psi_k \epsilon_{t-k}\right)\right] $$

III. Trocamos a ordem da esperan√ßa e das somat√≥rias:
    $$ \gamma_0 = \sum_{j=0}^{\infty} \sum_{k=0}^{\infty} \psi_j \psi_k E[\epsilon_{t-j} \epsilon_{t-k}] $$

IV. Usamos a propriedade do ru√≠do branco de que $E[\epsilon_{t-j} \epsilon_{t-k}] = 0$ se $j \neq k$ e $E[\epsilon_{t-j}^2] = \sigma^2$:
    $$ \gamma_0 = \sum_{j=0}^{\infty} \psi_j^2 E[\epsilon_{t-j}^2] = \sum_{j=0}^{\infty} \psi_j^2 \sigma^2 $$

V. Fatoramos a vari√¢ncia do ru√≠do branco $\sigma^2$:
    $$ \gamma_0 = \sigma^2 \sum_{j=0}^{\infty} \psi_j^2 $$

VI. Portanto, a vari√¢ncia do processo MA(‚àû) √© dada por:
    $$ \gamma_0 = \sigma^2 \sum_{j=0}^{\infty} \psi_j^2 $$ $\blacksquare$

Para complementar a an√°lise da rela√ß√£o entre as condi√ß√µes de converg√™ncia e a finitude da vari√¢ncia, considere a seguinte proposi√ß√£o:

**Proposi√ß√£o 1.1**
Se $\sum_{j=0}^{\infty} \psi_j^2$ diverge, ent√£o a vari√¢ncia $\gamma_0$ √© infinita.

*Prova:*
I.  Dado que a vari√¢ncia do processo MA(‚àû) √© dada por $\gamma_0 = \sigma^2 \sum_{j=0}^{\infty} \psi_j^2$.

II. Assumimos que $\sum_{j=0}^{\infty} \psi_j^2$ diverge, ou seja, $\sum_{j=0}^{\infty} \psi_j^2 = \infty$.

III. A vari√¢ncia do ru√≠do branco, $\sigma^2$, √© uma constante positiva finita.

IV. Multiplicando uma constante positiva finita ($\sigma^2$) por um valor infinito ($\sum_{j=0}^{\infty} \psi_j^2$), obtemos um valor infinito.

V. Portanto, $\gamma_0 = \sigma^2 \cdot \infty = \infty$.

VI. Conclu√≠mos que se $\sum_{j=0}^{\infty} \psi_j^2$ diverge, ent√£o a vari√¢ncia $\gamma_0$ √© infinita. $\blacksquare$

Para complementar nossa an√°lise da vari√¢ncia, √© √∫til considerar a seguinte caracteriza√ß√£o:

**Lema 1.1**
Se o processo MA(‚àû) satisfaz $\sum_{j=0}^{\infty} \psi_j^2 < \infty$, ent√£o a vari√¢ncia $\gamma_0$ √© finita.

*Prova:*
I.  Sabemos que a vari√¢ncia do processo MA(‚àû) √© dada por:
    $$\gamma_0 = \sigma^2 \sum_{j=0}^{\infty} \psi_j^2$$

II. Assumimos que $\sum_{j=0}^{\infty} \psi_j^2 < \infty$, o que significa que a soma dos quadrados dos coeficientes converge para um valor finito.

III. A vari√¢ncia do ru√≠do branco, $\sigma^2$, √© finita por defini√ß√£o.

IV. O produto de dois n√∫meros finitos √© finito. Portanto, $\sigma^2 \sum_{j=0}^{\infty} \psi_j^2$ √© finito.

V. Assim, $\gamma_0$ √© finita.

VI. Portanto, se o processo MA(‚àû) satisfaz $\sum_{j=0}^{\infty} \psi_j^2 < \infty$, ent√£o a vari√¢ncia $\gamma_0$ √© finita. $\blacksquare$

As **autocovari√¢ncias** do processo MA(‚àû) s√£o dadas por [^9]:

$$\gamma_k = E[(Y_t - \mu)(Y_{t-k} - \mu)] = \sigma^2 \sum_{j=0}^{\infty} \psi_j \psi_{j+k}$$

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior $\psi_j = (0.5)^j$ e $\sigma^2 = 1$, calculemos a autocovari√¢ncia para $k=1$:
>
> $\gamma_1 = (1) \sum_{j=0}^{\infty} (0.5)^j (0.5)^{j+1} = \sum_{j=0}^{\infty} (0.5)^{2j+1} = 0.5 \sum_{j=0}^{\infty} (0.25)^j = 0.5 \cdot \frac{1}{1-0.25} = 0.5 \cdot \frac{4}{3} = \frac{2}{3} \approx 0.667$
>
> Isso mostra que a autocovari√¢ncia no lag 1 √© 2/3, indicando uma correla√ß√£o positiva entre $Y_t$ e $Y_{t-1}$.

*Prova:*
I.  Substitu√≠mos as express√µes para $Y_t - \mu$ e $Y_{t-k} - \mu$:
    $$\gamma_k = E\left[\left(\sum_{j=0}^{\infty} \psi_j \epsilon_{t-j}\right)\left(\sum_{i=0}^{\infty} \psi_i \epsilon_{t-k-i}\right)\right]$$

II. Trocamos a ordem da esperan√ßa e das somat√≥rias:
    $$\gamma_k = \sum_{j=0}^{\infty} \sum_{i=0}^{\infty} \psi_j \psi_i E[\epsilon_{t-j} \epsilon_{t-k-i}]$$

III. Usamos a propriedade do ru√≠do branco de que $E[\epsilon_{t-j} \epsilon_{t-k-i}] = 0$ se $t-j \neq t-k-i$, ou seja, se $j \neq k+i$, e $E[\epsilon_{t-j}^2] = \sigma^2$:
    Para que $E[\epsilon_{t-j} \epsilon_{t-k-i}] \neq 0$, devemos ter $j = k+i$.  Substitu√≠mos $i$ por $j-k$:
    $$\gamma_k = \sum_{j=0}^{\infty} \psi_j \psi_{j-k} E[\epsilon_{t-j}^2] \delta(j-k \geq 0)$$
    onde $\delta(j-k \geq 0)$ √© uma fun√ß√£o indicadora que vale 1 se $j \geq k$ e 0 caso contr√°rio.  Alternativamente, podemos fazer uma mudan√ßa de vari√°vel $j' = j-k$, ent√£o $j = j'+k$ e
     $$\gamma_k = \sum_{j'=0}^{\infty} \psi_{j'+k} \psi_{j'} E[\epsilon_{t-j'-k}^2]$$
    Reescrevendo $j'$ como $j$, temos:
    $$\gamma_k = \sum_{j=0}^{\infty} \psi_{j+k} \psi_{j} E[\epsilon_{t-j-k}^2] = \sigma^2 \sum_{j=0}^{\infty} \psi_j \psi_{j+k}$$

IV. Portanto, a autocovari√¢ncia do processo MA(‚àû) no lag $k$ √© dada por:
    $$\gamma_k = \sigma^2 \sum_{j=0}^{\infty} \psi_j \psi_{j+k}$$ $\blacksquare$

Essas autocovari√¢ncias medem a depend√™ncia linear entre os valores da s√©rie temporal em diferentes *lags*. Note que, diferentemente dos processos MA de ordem finita, as autocovari√¢ncias de um processo MA(‚àû) podem ser n√£o nulas para todos os *lags* $k$.

**Teorema 2**
Se $\sum_{j=0}^{\infty} |\psi_j| < \infty$, ent√£o $\sum_{k=-\infty}^{\infty} |\gamma_k| < \infty$.

*Prova:*
I.  Come√ßamos com a defini√ß√£o de autocovari√¢ncia:
    $$ \gamma_k = E[(Y_t - \mu)(Y_{t-k} - \mu)] = \sigma^2 \sum_{j=0}^{\infty} \psi_j \psi_{j+k} $$

II. Tomamos o valor absoluto de $\gamma_k$:
    $$ |\gamma_k| = \left| \sigma^2 \sum_{j=0}^{\infty} \psi_j \psi_{j+k} \right| \leq \sigma^2 \sum_{j=0}^{\infty} |\psi_j| |\psi_{j+k}| $$

III. Somamos os valores absolutos das autocovari√¢ncias sobre todos os lags $k$:
    $$ \sum_{k=-\infty}^{\infty} |\gamma_k| \leq \sum_{k=-\infty}^{\infty} \sigma^2 \sum_{j=0}^{\infty} |\psi_j| |\psi_{j+k}| = \sigma^2 \sum_{k=-\infty}^{\infty} \sum_{j=0}^{\infty} |\psi_j| |\psi_{j+k}|$$

IV. Trocamos a ordem das somat√≥rias:
     $$ \sum_{k=-\infty}^{\infty} |\gamma_k| \leq  \sigma^2 \sum_{j=0}^{\infty} \sum_{k=-\infty}^{\infty} |\psi_j| |\psi_{j+k}|$$

V. Fatoramos $|\psi_j|$ para fora da soma interna (pois n√£o depende de $k$):
    $$ \sum_{k=-\infty}^{\infty} |\gamma_k| \leq \sigma^2 \sum_{j=0}^{\infty} |\psi_j| \sum_{k=-\infty}^{\infty} |\psi_{j+k}| $$

VI. Observamos que $\psi_{j+k} = 0$ para $j+k < 0$, pois os coeficientes s√£o definidos apenas para √≠ndices n√£o negativos. Portanto, a soma $\sum_{k=-\infty}^{\infty} |\psi_{j+k}|$ √© equivalente a $\sum_{k=-j}^{\infty} |\psi_{j+k}|$. Fazendo a mudan√ßa de vari√°vel $i = j+k$, temos $\sum_{i=0}^{\infty} |\psi_{i}|$.

VII. Substitu√≠mos a soma interna pela s√©rie original de coeficientes:
    $$ \sum_{k=-\infty}^{\infty} |\gamma_k| \leq \sigma^2 \sum_{j=0}^{\infty} |\psi_j| \sum_{i=0}^{\infty} |\psi_i| = \sigma^2 \left(\sum_{j=0}^{\infty} |\psi_j|\right) \left(\sum_{i=0}^{\infty} |\psi_i|\right)$$

VIII. Reescrevemos o produto como um quadrado:
    $$ \sum_{k=-\infty}^{\infty} |\gamma_k| \leq \sigma^2 \left(\sum_{j=0}^{\infty} |\psi_j|\right)^2 $$

IX. Dado que $\sum_{j=0}^{\infty} |\psi_j| < \infty$ (absoluta summability), seu quadrado tamb√©m √© finito:
    $$ \left(\sum_{j=0}^{\infty} |\psi_j|\right)^2 < \infty $$

X. Portanto, conclu√≠mos que:
    $$ \sum_{k=-\infty}^{\infty} |\gamma_k| \leq \sigma^2 \left(\sum_{j=0}^{\infty} |\psi_j|\right)^2 < \infty $$
   Assim, $\sum_{k=-\infty}^{\infty} |\gamma_k| < \infty$ $\blacksquare$

Podemos estender esse resultado para caracterizar o comportamento assint√≥tico das autocovari√¢ncias:

**Teorema 2.1**
Se $\sum_{j=0}^{\infty} |\psi_j| < \infty$, ent√£o $\gamma_k \rightarrow 0$ quando $k \rightarrow \infty$.

*Prova:*
I.  Sabemos que a autocovari√¢ncia √© dada por:
    $$ \gamma_k = \sigma^2 \sum_{j=0}^{\infty} \psi_j \psi_{j+k} $$

II. Queremos mostrar que $\lim_{k \to \infty} \gamma_k = 0$.

III. Seja $\epsilon > 0$. Como $\sum_{j=0}^{\infty} |\psi_j|$ converge, existe um $N$ tal que $\sum_{j=N}^{\infty} |\psi_j| < \epsilon$.

IV. Podemos reescrever a autocovari√¢ncia como:
    $$ \gamma_k = \sigma^2 \sum_{j=0}^{N-1} \psi_j \psi_{j+k} + \sigma^2 \sum_{j=N}^{\infty} \psi_j \psi_{j+k} $$

V. Tomamos o valor absoluto:
    $$ |\gamma_k| \leq \sigma^2 \sum_{j=0}^{N-1} |\psi_j| |\psi_{j+k}| + \sigma^2 \sum_{j=N}^{\infty} |\psi_j| |\psi_{j+k}| $$

VI. Analisamos o segundo termo:
    $$ \sigma^2 \sum_{j=N}^{\infty} |\psi_j| |\psi_{j+k}| \leq \sigma^2 \left(\sum_{j=N}^{\infty} |\psi_j|\right) \left(\sup_j |\psi_{j+k}|\right) < \sigma^2 \epsilon \left(\sup_j |\psi_{j+k}|\right)  $$
    Como $\psi_j \rightarrow 0$ quando $j \rightarrow \infty$ (j√° que a s√©rie $\sum_{j=0}^{\infty} |\psi_j|$ converge), existe $K$ tal que para $k > K$, $|\psi_{j+k}| < \epsilon$ para todo $j$.
    Logo, o segundo termo √© menor que $\sigma^2 \epsilon^2$.

VII. Analisamos o primeiro termo:
     $$ \sigma^2 \sum_{j=0}^{N-1} |\psi_j| |\psi_{j+k}| $$
     Para cada $j$ fixo, $\psi_{j+k} \rightarrow 0$ quando $k \rightarrow \infty$. Portanto, para $k$ suficientemente grande, cada termo na soma torna-se arbitrariamente pequeno.

VIII. Combinando os resultados, para $k$ suficientemente grande, $|\gamma_k|$ pode ser tornado arbitrariamente pequeno.

IX. Portanto, $\lim_{k \to \infty} \gamma_k = 0$. $\blacksquare$

Um processo MA(‚àû) que satisfaz a condi√ß√£o de absoluta summability √© **erg√≥dico para a m√©dia** [^9]. Isso significa que a m√©dia amostral da s√©rie temporal converge em probabilidade para a m√©dia populacional $\mu$ √† medida que o tamanho da amostra aumenta. Se os *shocks* $\epsilon_t$ forem Gaussianos, ent√£o o processo MA(‚àû) √© erg√≥dico para todos os momentos [^9].

### Conclus√£o
O processo MA(‚àû) √© uma ferramenta poderosa para modelar s√©ries temporais com depend√™ncia de longo alcance. Ao permitir uma influ√™ncia infinita de *shocks* passados, ele pode capturar padr√µes complexos que n√£o s√£o representados adequadamente por modelos MA de ordem finita. A compreens√£o das condi√ß√µes de converg√™ncia e das propriedades estat√≠sticas do MA(‚àû) √© crucial para sua aplica√ß√£o correta na an√°lise e previs√£o de s√©ries temporais. √â importante notar que, na pr√°tica, os modelos MA(‚àû) s√£o frequentemente aproximados por modelos MA de ordem finita ou por modelos **Autorregressivos de M√©dia M√≥vel (ARMA)**, que oferecem uma representa√ß√£o mais parcimoniosa da depend√™ncia temporal [^15].

### Refer√™ncias
[^4]: "The basic building block for all the processes considered in this chapter is a sequence {8} - whose elements have mean zero and variance œÉ¬≤"
[^9]: "Y, = u + Œ£œàŒ≥ŒµŒπ = Œº + œàŒøŒµŒπ + œàŒπŒµŒπ-1 + œà281-2+.... This could be described as an MA(‚àû) process."
[^15]: "For many applications, stationarity and ergodicity turn out to amount to the same requirements."
<!-- END -->