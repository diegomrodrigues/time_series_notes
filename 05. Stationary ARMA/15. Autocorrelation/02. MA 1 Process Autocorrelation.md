## Autocorrela√ß√£o em S√©ries Temporais Estacion√°rias ARMA

### Introdu√ß√£o
Em continuidade ao conceito de **autocorrela√ß√£o** ($\rho_j$) apresentado anteriormente, este cap√≠tulo aprofunda a an√°lise espec√≠fica da autocorrela√ß√£o em processos Moving Average de ordem 1 (MA(1)) e ru√≠do branco [^3]. A autocorrela√ß√£o, que quantifica a correla√ß√£o linear entre valores da s√©rie temporal em diferentes lags *j*, assume caracter√≠sticas distintas para cada tipo de processo, oferecendo informa√ß√µes cruciais para a identifica√ß√£o e modelagem adequada da s√©rie [^49].

### Conceitos Fundamentais

**Autocorrela√ß√£o em um Processo MA(1)**

Um processo MA(1) √© definido como [^5, 3.3.1]:

$$Y_t = \mu + \varepsilon_t + \theta \varepsilon_{t-1}$$

onde $$\mu$$ √© a m√©dia do processo, $$\varepsilon_t$$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $$\sigma^2$$, e $$\theta$$ √© o coeficiente do termo de m√©dia m√≥vel.

Como vimos anteriormente [^5, 3.3.7], a autocorrela√ß√£o no *lag* 1 ($\rho_1$) para um processo MA(1) √© dada por:

$$\rho_1 = \frac{\theta}{1 + \theta^2}$$

Enquanto todas as autocorrela√ß√µes de ordem superior (j > 1) s√£o iguais a zero [^5, 3.3.5]. Esse comportamento da ACF √© uma caracter√≠stica distintiva de processos MA(1).

> üí° **Exemplo Num√©rico:**
>
> Vamos simular um processo MA(1) com $$\mu = 10$$, $$\theta = 0.6$$, e $$\sigma^2 = 1$$. Geraremos 100 observa√ß√µes.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from statsmodels.graphics.tsaplots import plot_acf
>
> # Par√¢metros do MA(1)
> mu = 10
> theta = 0.6
> sigma = 1
> n = 100
>
> # Gerar ru√≠do branco
> np.random.seed(42) # para reproducibilidade
> epsilon = np.random.normal(0, sigma, n)
>
> # Gerar o processo MA(1)
> Y = mu + epsilon[1:] + theta * epsilon[:-1]
>
> # Plotar a s√©rie temporal
> plt.figure(figsize=(10, 4))
> plt.plot(Y)
> plt.title('Processo MA(1) Simulado')
> plt.xlabel('Tempo')
> plt.ylabel('Y_t')
> plt.show()
>
> # Plotar a ACF
> plot_acf(Y, lags=10)
> plt.title('ACF do Processo MA(1) Simulado')
> plt.xlabel('Lag')
> plt.ylabel('Autocorrela√ß√£o')
> plt.show()
>
> # Calcular rho_1 teoricamente
> rho_1_teorico = theta / (1 + theta**2)
> print(f'Autocorrela√ß√£o te√≥rica no lag 1: {rho_1_teorico:.3f}')
>
> # Calcular rho_1 amostralmente
> def autocorr(x, t=1):
>     return np.corrcoef(x[:-t], x[t:])[0,1]
>
> rho_1_amostral = autocorr(Y, t=1)
> print(f'Autocorrela√ß√£o amostral no lag 1: {rho_1_amostral:.3f}')
> ```
>
> O gr√°fico da s√©rie temporal mostrar√° as flutua√ß√µes ao redor da m√©dia. O gr√°fico da ACF mostrar√° um valor significativo no lag 1, pr√≥ximo ao valor te√≥rico calculado (0.344), e valores pr√≥ximos de zero para os demais lags.  A autocorrela√ß√£o amostral pode variar um pouco devido ao tamanho finito da amostra, mas deve estar relativamente pr√≥xima do valor te√≥rico.

**Prova**:
Podemos verificar que $$\rho_j = 0$$ para j > 1 da seguinte forma:

I. Pela defini√ß√£o, $$\rho_j = \frac{\gamma_j}{\gamma_0}$$, onde $$\gamma_j$$ √© a fun√ß√£o de autocovari√¢ncia no lag *j* e $$\gamma_0$$ √© a vari√¢ncia do processo.
II. Para um processo MA(1), $$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$$
III. Substituindo a defini√ß√£o de $$Y_t$$, temos $$\gamma_j = E[(\varepsilon_t + \theta\varepsilon_{t-1})(\varepsilon_{t-j} + \theta\varepsilon_{t-j-1})]$$
IV. Para j > 1, todos os termos na expans√£o de $$E[(\varepsilon_t + \theta\varepsilon_{t-1})(\varepsilon_{t-j} + \theta\varepsilon_{t-j-1})]$$ envolvem o produto de ru√≠dos brancos em diferentes instantes de tempo, que s√£o n√£o correlacionados por defini√ß√£o. Portanto, $$E[\varepsilon_t \varepsilon_{t-j}] = 0$$ se $$t \neq j$$.  Al√©m disso, $$E[\varepsilon_t] = 0$$ para todo *t*.
V. Logo, para j > 1, $$\gamma_j = 0$$, e consequentemente $$\rho_j = 0$$. ‚ñ†

**Teorema 1**: *A fun√ß√£o de autocorrela√ß√£o (ACF) de um processo MA(1) possui um corte abrupto ap√≥s o lag 1.*

**Prova**:
Este teorema decorre diretamente da prova anterior. Demonstramos que para um processo MA(1), $$\rho_j = 0$$ para todos os $$j > 1$$.  Isso significa que a ACF √© n√£o-nula apenas no lag 1 (e no lag 0, trivialmente), e √© zero para todos os lags subsequentes.  Portanto, a ACF "corta" para zero ap√≥s o lag 1, exibindo um corte abrupto. ‚ñ†

**Teorema 1.1**: *A fun√ß√£o de autocorrela√ß√£o parcial (PACF) de um processo MA(1) decai exponencialmente.*

**Prova (Esbo√ßo)**:
Um processo MA(1) pode ser expresso como um processo autoregressivo de ordem infinita (AR($\infty$)).  A PACF mede a correla√ß√£o entre $$Y_t$$ e $$Y_{t-k}$$ ap√≥s remover o efeito dos lags intermedi√°rios. Como a representa√ß√£o AR($\infty$) tem coeficientes que decaem exponencialmente, a PACF tamb√©m exibir√° um decaimento exponencial. Este decaimento √© uma consequ√™ncia da invertibilidade do processo MA(1).

> üí° **Exemplo Num√©rico:**
>
> Continuando com o processo MA(1) simulado anteriormente ( $$\theta = 0.6$$ ), podemos plotar a PACF para verificar o decaimento exponencial.
>
> ```python
> from statsmodels.graphics.tsaplots import plot_pacf
>
> # Plotar a PACF
> plot_pacf(Y, lags=10, method='ywm') # Usando 'ywm' para evitar aviso
> plt.title('PACF do Processo MA(1) Simulado')
> plt.xlabel('Lag')
> plt.ylabel('Autocorrela√ß√£o Parcial')
> plt.show()
> ```
>
> A PACF mostrar√° um decaimento exponencial. O primeiro lag ter√° um valor significativo, e os lags subsequentes decair√£o em magnitude. A taxa de decaimento est√° relacionada ao valor de $$\theta$$.
>
> ```mermaid
> graph LR
>     A[Y_t] --> B(Y_{t-1})
>     A --> C(Y_{t-2})
>     A --> D(Y_{t-3})
>     style B fill:#f9f,stroke:#333,stroke-width:2px
>     style C fill:#f9f,stroke:#333,stroke-width:2px
>     style D fill:#f9f,stroke:#333,stroke-width:2px
>     B --> E(Decaimento Exponencial)
>     C --> E
>     D --> E
> ```

**Interpreta√ß√£o do Sinal de Œ∏**:

*   **Œ∏ > 0**: Um valor positivo de $$\theta$$ indica que um choque positivo $$\varepsilon_{t-1}$$ influencia positivamente o valor atual $$Y_t$$, resultando em uma correla√ß√£o positiva no *lag* 1. Isso significa que valores acima da m√©dia em $$t-1$$ tendem a ser seguidos por valores acima da m√©dia em *t*.
*   **Œ∏ < 0**: Um valor negativo de $$\theta$$ indica que um choque positivo $$\varepsilon_{t-1}$$ influencia negativamente o valor atual $$Y_t$$, resultando em uma correla√ß√£o negativa no *lag* 1. Isso significa que valores acima da m√©dia em $$t-1$$ tendem a ser seguidos por valores abaixo da m√©dia em *t*.

A magnitude de $$\theta$$ influencia a for√ßa da correla√ß√£o. No entanto, √© importante notar que a autocorrela√ß√£o $$\rho_1$$ √© sempre limitada entre -0.5 e 0.5, independentemente do valor de $$\theta$$.

> üí° **Exemplo Num√©rico:**
>
> *   Se $$\theta = 1$$, ent√£o $$\rho_1 = \frac{1}{1 + 1^2} = 0.5$$.
> *   Se $$\theta = -1$$, ent√£o $$\rho_1 = \frac{-1}{1 + (-1)^2} = -0.5$$.
> *   Se $$\theta = 2$$, ent√£o $$\rho_1 = \frac{2}{1 + 2^2} = 0.4$$.
> *   Se $$\theta = -0.5$$, ent√£o $$\rho_1 = \frac{-0.5}{1 + (-0.5)^2} = -0.4$$.

A propriedade de que a autocorrela√ß√£o sempre se encontra entre -0.5 e 0.5 pode ser utilizada como um crit√©rio de validade de um modelo MA(1) ajustado. Valores de autocorrela√ß√£o amostral fora desse intervalo podem indicar inadequa√ß√£o do modelo.

**Lema 3**: *Para um processo MA(1), a magnitude m√°xima da autocorrela√ß√£o no lag 1 √© 0.5.*

*Prova:*
Para encontrar o valor m√°ximo de $$\rho_1 = \frac{\theta}{1 + \theta^2}$$, podemos derivar em rela√ß√£o a $$\theta$$ e igualar a zero:

I. $$\frac{d\rho_1}{d\theta} = \frac{(1 + \theta^2)(1) - \theta(2\theta)}{(1 + \theta^2)^2} = \frac{1 - \theta^2}{(1 + \theta^2)^2}$$

II. Igualando a zero: $$\frac{1 - \theta^2}{(1 + \theta^2)^2} = 0 \Rightarrow 1 - \theta^2 = 0 \Rightarrow \theta = \pm 1$$

III. Para verificar se s√£o m√°ximos ou m√≠nimos, derivamos a segunda vez: $$\frac{d^2\rho_1}{d\theta^2}=\frac{-2\theta(1+\theta^2)^2 - (1-\theta^2)2(1+\theta^2)2\theta}{(1+\theta^2)^4} = \frac{2\theta^3 - 6\theta}{(1+\theta^2)^3}$$.

IV. Para $$\theta = 1$$, $$\frac{d^2\rho_1}{d\theta^2} = \frac{2 - 6}{8} = -\frac{1}{2} < 0$$, indicando um m√°ximo.  Para $$\theta = -1$$, $$\frac{d^2\rho_1}{d\theta^2} = \frac{-2 + 6}{8} = \frac{1}{2} > 0$$, indicando um m√≠nimo.

V. Substituindo $$\theta = 1$$ e $$\theta = -1$$ em $$\rho_1 = \frac{\theta}{1 + \theta^2}$$, obtemos $$\rho_1 = \frac{1}{2} = 0.5$$ e $$\rho_1 = \frac{-1}{2} = -0.5$$, respectivamente.

VI. Portanto, a magnitude m√°xima da autocorrela√ß√£o no *lag* 1 para um processo MA(1) √© 0.5. ‚ñ†

**Corol√°rio 3.1**: *A autocorrela√ß√£o no lag 1 de um processo MA(1) √© zero se e somente se $$\theta = 0$$.*

*Prova:*
Se $$\theta = 0$$, ent√£o $$\rho_1 = \frac{0}{1 + 0^2} = 0$$. Reciprocamente, se $$\rho_1 = \frac{\theta}{1 + \theta^2} = 0$$, ent√£o $$\theta$$ deve ser igual a 0, uma vez que o denominador √© sempre positivo. ‚ñ†

**Teorema 2**: *A invertibilidade de um processo MA(1) est√° relacionada √† magnitude do par√¢metro $$\theta$$.*

Um processo MA(1) √© dito invert√≠vel se puder ser expresso como um processo autoregressivo (AR) infinito e convergente. A condi√ß√£o de invertibilidade para um MA(1) √© que $$|\theta| < 1$$.

> üí° **Exemplo Num√©rico:**
>
> Se $$\theta = 0.8$$, o processo √© invert√≠vel. Se $$\theta = 1.2$$, o processo n√£o √© invert√≠vel.  A invertibilidade √© importante para a estabilidade do modelo e para garantir que previs√µes futuras n√£o se tornem excessivamente sens√≠veis a choques passados.
>
> Processo invert√≠vel ($$\theta = 0.8$$):
>
> ```mermaid
> graph LR
>     A(Œµ_{t-1}) --> B(Y_t)
>     style A fill:#ccf,stroke:#333,stroke-width:2px
>     B --> C(Decaimento da influ√™ncia de Œµ_{t-1})
>     C --> D(Estabilidade)
> ```
>
> Processo n√£o invert√≠vel ($$\theta = 1.2$$):
>
> ```mermaid
> graph LR
>     A(Œµ_{t-1}) --> B(Y_t)
>     style A fill:#ccf,stroke:#333,stroke-width:2px
>     B --> C(Aumento da influ√™ncia de Œµ_{t-1})
>     C --> D(Instabilidade)
> ```

**Teorema 2.1**: *Se um processo MA(1) √© invert√≠vel ($|\theta| < 1$), ent√£o a influ√™ncia de choques passados diminui exponencialmente ao longo do tempo.*

*Prova:*
A invertibilidade garante que o processo MA(1) possa ser reescrito como um AR($\infty$). Se $$|\theta| < 1$$, os coeficientes do AR($\infty$) decaem exponencialmente, significando que o impacto de $$\varepsilon_{t-j}$$ em $$Y_t$$ diminui √† medida que *j* aumenta. ‚ñ†

**Autocorrela√ß√£o em Ru√≠do Branco**

Um processo de ru√≠do branco √© definido como uma sequ√™ncia de vari√°veis aleat√≥rias n√£o correlacionadas com m√©dia zero e vari√¢ncia constante $$\sigma^2$$ [^5, 3.2.1, 3.2.2, 3.2.3]. Isso significa que cada observa√ß√£o √© independente das demais. Matematicamente,

$$E[\varepsilon_t] = 0$$
$$Var[\varepsilon_t] = \sigma^2$$
$$Cov[\varepsilon_t, \varepsilon_{t-j}] = 0 \text{ para } j \neq 0$$

Consequentemente, a autocorrela√ß√£o para um processo de ru√≠do branco √©:

$$\rho_j = \begin{cases} 1, & \text{se } j = 0 \\ 0, & \text{se } j \neq 0 \end{cases}$$

A ACF de um ru√≠do branco √©, portanto, uma fun√ß√£o delta de Kronecker, com um pico em *lag* 0 e zero para todos os outros *lags*. Isso reflete a aus√™ncia de depend√™ncia temporal em um ru√≠do branco.

> üí° **Exemplo Num√©rico:**
>
> Se gerarmos uma s√©rie temporal de ru√≠do branco, esperar√≠amos observar autocorrela√ß√µes amostrais pr√≥ximas de zero para todos os *lags* diferentes de zero. Devido √† aleatoriedade inerente, as autocorrela√ß√µes amostrais raramente ser√£o exatamente zero, mas devem flutuar em torno de zero, dentro dos limites do intervalo de confian√ßa [^60].
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from statsmodels.graphics.tsaplots import plot_acf
> from scipy import stats
>
> # Gerar ru√≠do branco
> np.random.seed(42)
> white_noise = np.random.randn(100)
> n = len(white_noise)
>
> # Plotar a ACF
> plot_acf(white_noise, lags=20)
> plt.title('ACF de Ru√≠do Branco')
> plt.xlabel('Lag')
> plt.ylabel('Autocorrela√ß√£o')
> plt.show()
>
> # Calcular o intervalo de confian√ßa
> conf_level = 0.95
> alpha = 1 - conf_level
> critical_value = stats.norm.ppf(1 - alpha/2)
> conf_interval = critical_value / np.sqrt(n)
>
> print(f"Intervalo de confian√ßa (n√≠vel de {conf_level*100}%): +/- {conf_interval:.3f}")
> ```
>
> O gr√°fico da ACF resultante dever√° mostrar um pico em lag 0 (autocorrela√ß√£o igual a 1) e valores pr√≥ximos a zero para os demais lags, com algumas flutua√ß√µes aleat√≥rias dentro do intervalo de confian√ßa. Para um intervalo de confian√ßa de 95%, os valores da ACF devem estar entre aproximadamente +/- 0.196.

**Lema 4**: *A soma do quadrado das autocorrela√ß√µes de um ru√≠do branco √© m√≠nima em compara√ß√£o com qualquer outro processo.*

*Prova:*
Para um ru√≠do branco, $$\rho_0 = 1$$ e $$\rho_j = 0$$ para $$j \neq 0$$. Portanto, $$\sum_{j=-\infty}^{\infty} \rho_j^2 = \rho_0^2 + \sum_{j \neq 0} \rho_j^2 = 1^2 + 0 = 1$$. Qualquer outro processo com depend√™ncia temporal ter√° autocorrela√ß√µes n√£o nulas para pelo menos alguns lags diferentes de zero. Como $$\rho_j^2$$ √© sempre n√£o negativo, a soma dos quadrados das autocorrela√ß√µes ser√° maior que 1 para qualquer processo com depend√™ncia temporal. ‚ñ†

**Proposi√ß√£o 5**: *A autocorrela√ß√£o amostral de um ru√≠do branco segue uma distribui√ß√£o assintoticamente normal sob certas condi√ß√µes.*

Para uma amostra de tamanho *n* de um ru√≠do branco, a autocorrela√ß√£o amostral $$\hat{\rho}_j$$ para $$j \neq 0$$ √© assintoticamente normal com m√©dia 0 e vari√¢ncia $$1/n$$.

**Prova (Esbo√ßo)**:
Este resultado √© baseado no Teorema do Limite Central (TLC). Sob certas condi√ß√µes de regularidade, as autocorrela√ß√µes amostrais podem ser expressas como uma m√©dia de vari√°veis aleat√≥rias independentes. Aplicando o TLC, a distribui√ß√£o assint√≥tica da m√©dia (e, portanto, de $$\hat{\rho}_j$$) se aproxima de uma distribui√ß√£o normal com os par√¢metros especificados.

> üí° **Exemplo Num√©rico:**
>
> Se gerarmos 1000 amostras de ru√≠do branco com tamanho 100 e calcularmos a autocorrela√ß√£o no lag 1 para cada amostra, a distribui√ß√£o dessas autocorrela√ß√µes amostrais dever√° se aproximar de uma normal com m√©dia 0 e vari√¢ncia 1/100 = 0.01.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy import stats
>
> # N√∫mero de amostras
> num_samples = 1000
>
> # Tamanho da amostra
> sample_size = 100
>
> # Vetor para armazenar as autocorrela√ß√µes no lag 1
> rho_1_values = np.zeros(num_samples)
>
> # Gerar as amostras e calcular as autocorrela√ß√µes
> for i in range(num_samples):
>     white_noise = np.random.randn(sample_size)
>     rho_1_values[i] = np.corrcoef(white_noise[:-1], white_noise[1:])[0, 1]
>
> # Plotar o histograma das autocorrela√ß√µes
> plt.hist(rho_1_values, bins=30, density=True)
> plt.title('Distribui√ß√£o das Autocorrela√ß√µes Amostrais (Lag 1)')
> plt.xlabel('Autocorrela√ß√£o Amostral (rho_1)')
> plt.ylabel('Densidade')
>
> # Plotar a distribui√ß√£o normal te√≥rica
> x = np.linspace(-0.2, 0.2, 100)
> plt.plot(x, stats.norm.pdf(x, loc=0, scale=np.sqrt(1/sample_size)), 'r-', label='Normal Te√≥rica')
>
> plt.legend()
> plt.show()
>
> # Calcular a m√©dia e o desvio padr√£o das autocorrela√ß√µes amostrais
> mean_rho_1 = np.mean(rho_1_values)
> std_rho_1 = np.std(rho_1_values)
>
> print(f'M√©dia das autocorrela√ß√µes amostrais: {mean_rho_1:.4f}')
> print(f'Desvio padr√£o das autocorrela√ß√µes amostrais: {std_rho_1:.4f}')
> ```
>
> O histograma das autocorrela√ß√µes amostrais dever√° se assemelhar a uma distribui√ß√£o normal centrada em zero. A m√©dia das autocorrela√ß√µes amostrais dever√° estar pr√≥xima de zero, e o desvio padr√£o dever√° ser pr√≥ximo de $$\sqrt{1/100} = 0.1$$.

**Proposi√ß√£o 6**: *Se uma s√©rie temporal √© a soma de dois processos de ru√≠do branco independentes, ent√£o a s√©rie resultante tamb√©m √© um ru√≠do branco.*

*Prova:*
Seja $$Z_t = X_t + Y_t$$, onde $$X_t$$ e $$Y_t$$ s√£o dois processos de ru√≠do branco independentes com m√©dias zero e vari√¢ncias $$\sigma_X^2$$ e $$\sigma_Y^2$$, respectivamente. Ent√£o,

I. $$E[Z_t] = E[X_t + Y_t] = E[X_t] + E[Y_t] = 0 + 0 = 0$$

II. $$Var[Z_t] = Var[X_t + Y_t] = Var[X_t] + Var[Y_t] + 2Cov[X_t, Y_t] = \sigma_X^2 + \sigma_Y^2 + 0 = \sigma_X^2 + \sigma_Y^2$$, que √© uma constante.

III. Para $$j \neq 0$$,  $$Cov[Z_t, Z_{t-j}] = E[(Z_t - E[Z_t])(Z_{t-j} - E[Z_{t-j}])] = E[Z_t Z_{t-j}] = E[(X_t + Y_t)(X_{t-j} + Y_{t-j})] = E[X_t X_{t-j}] + E[X_t Y_{t-j}] + E[Y_t X_{t-j}] + E[Y_t Y_{t-j}]$$. Como $$X_t$$ e $$Y_t$$ s√£o independentes e cada um √© um ru√≠do branco, todos os termos s√£o zero para $$j \neq 0$$.

IV.  Para $$j = 0$$,  $$Cov[Z_t, Z_{t}] = E[(Z_t - E[Z_t])(Z_{t} - E[Z_{t}])] = E[Z_t^2] = E[(X_t + Y_t)^2] = E[X_t^2] + E[Y_t^2] + 2E[X_t Y_t] =  \sigma_X^2 + \sigma_Y^2 + 0 $$.

Portanto, $$Z_t$$ tem m√©dia zero, vari√¢ncia constante e autocorrela√ß√£o zero para todos os lags diferentes de zero, o que satisfaz a defini√ß√£o de ru√≠do branco. ‚ñ†

### Conclus√£o

A an√°lise da autocorrela√ß√£o fornece *insights* valiosos sobre a estrutura temporal de s√©ries temporais estacion√°rias ARMA. Em particular, processos MA(1) exibem autocorrela√ß√£o significativa apenas no *lag* 1, enquanto processos de ru√≠do branco apresentam autocorrela√ß√£o apenas no *lag* 0. As propriedades e teoremas discutidos neste cap√≠tulo fornecem uma base s√≥lida para a identifica√ß√£o, modelagem e previs√£o de s√©ries temporais [^7, 49]. A correta interpreta√ß√£o da ACF √© fundamental para evitar falsos positivos e garantir a adequa√ß√£o do modelo escolhido.

### Refer√™ncias
[^3]: Informa√ß√£o retirada da introdu√ß√£o do cap√≠tulo.
[^49]: Informa√ß√£o retirada da p√°gina 49, se√ß√£o 3.3.6.
[^5]: Informa√ß√£o retirada da p√°gina 48.
[^3.3.1]: Informa√ß√£o retirada da p√°gina 48, se√ß√£o 3.3.1.
[^3.3.5]: Informa√ß√£o retirada da p√°gina 48, se√ß√£o 3.3.5.
[^3.3.7]: Informa√ß√£o retirada da p√°gina 49, se√ß√£o 3.3.7.
[^3.2.1]: Informa√ß√£o retirada da p√°gina 47, se√ß√£o 3.2.1.
[^3.2.2]: Informa√ß√£o retirada da p√°gina 47, se√ß√£o 3.2.2.
[^3.2.3]: Informa√ß√£o retirada da p√°gina 47, se√ß√£o 3.2.3.
[^7]: Informa√ß√£o retirada da p√°gina 47.
[^60]: Estat√≠stica para Economia e Administra√ß√£o, Morettin e Bussab, 7¬™ Edi√ß√£o.
<!-- END -->