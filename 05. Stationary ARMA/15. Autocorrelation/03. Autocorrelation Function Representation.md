## Autocorrela√ß√£o em S√©ries Temporais Estacion√°rias ARMA

### Introdu√ß√£o
Este cap√≠tulo expande nossa compreens√£o da **autocorrela√ß√£o** em s√©ries temporais estacion√°rias ARMA, focando na representa√ß√£o visual da fun√ß√£o de autocorrela√ß√£o (ACF) para processos de ru√≠do branco e Moving Average de ordem 1 (MA(1)) [^3, 5]. A ACF, ao plotar $\rho_j$ em fun√ß√£o de *j*, permite uma an√°lise intuitiva da depend√™ncia temporal, complementando as an√°lises te√≥ricas e matem√°ticas j√° discutidas [^49]. Atrav√©s da visualiza√ß√£o da ACF, podemos identificar caracter√≠sticas distintivas de diferentes processos estoc√°sticos, auxiliando na modelagem e previs√£o de s√©ries temporais.

### Conceitos Fundamentais

**A Fun√ß√£o de Autocorrela√ß√£o (ACF) como Representa√ß√£o Visual**

A fun√ß√£o de autocorrela√ß√£o (ACF) √© uma ferramenta gr√°fica essencial para a an√°lise de s√©ries temporais. Ela representa os valores de $\rho_j$ (autocorrela√ß√£o no *lag* j) como uma fun√ß√£o de *j*, permitindo a visualiza√ß√£o da for√ßa e dire√ß√£o da depend√™ncia temporal na s√©rie [^49]. Cada ponto no gr√°fico da ACF representa a correla√ß√£o entre a s√©rie temporal e sua vers√£o defasada em *j* per√≠odos.

> üí° **Exemplo:**
>
> Imagine a ACF de uma s√©rie temporal de vendas mensais. Se observarmos um pico significativo em *j* = 12, isso indica que as vendas de um m√™s est√£o fortemente correlacionadas com as vendas de 12 meses atr√°s, sugerindo um padr√£o sazonal anual.

**Representa√ß√£o da ACF para Ru√≠do Branco**

Como vimos anteriormente [^5], um processo de ru√≠do branco √© caracterizado pela aus√™ncia de correla√ß√£o entre observa√ß√µes em diferentes pontos no tempo. Portanto, a ACF de um ru√≠do branco exibe um padr√£o muito simples:

$$\rho_j = \begin{cases} 1, & \text{se } j = 0 \\ 0, & \text{se } j \neq 0 \end{cases}$$

Graficamente, a ACF de ru√≠do branco √© representada por uma barra em $\rho_0 = 1$ e valores pr√≥ximos de zero para todos os outros lags [^5]. √â importante notar que, em amostras finitas, as autocorrela√ß√µes amostrais raramente ser√£o exatamente zero, mas flutuar√£o aleatoriamente em torno de zero dentro de um intervalo de confian√ßa.

**Proposi√ß√£o 2** [ACF de Ru√≠do Branco]
Para um processo de ru√≠do branco $Y_t$, onde $E[Y_t] = 0$ e $Cov(Y_t, Y_s) = \begin{cases} \sigma^2, & \text{se } t = s \\ 0, & \text{se } t \neq s \end{cases}$, a fun√ß√£o de autocorrela√ß√£o (ACF) √© dada por:

$$\rho_j = \begin{cases} 1, & \text{se } j = 0 \\ 0, & \text{se } j \neq 0 \end{cases}$$

*Proof:*
I. A autocorrela√ß√£o no lag *j* √© definida como $\rho_j = \frac{Cov(Y_t, Y_{t-j})}{\sqrt{Var(Y_t)Var(Y_{t-j})}}$.

II. Para *j* = 0,  $Cov(Y_t, Y_{t}) = Var(Y_t) = \sigma^2$. Portanto, $\rho_0 = \frac{\sigma^2}{\sqrt{\sigma^2 \sigma^2}} = 1$.

III. Para *j* ‚â† 0, $Cov(Y_t, Y_{t-j}) = 0$, pois as observa√ß√µes em diferentes pontos no tempo s√£o n√£o correlacionadas. Portanto, $\rho_j = \frac{0}{\sqrt{\sigma^2 \sigma^2}} = 0$.

IV. Consequentemente, a ACF de um ru√≠do branco √© 1 no lag 0 e 0 para todos os outros lags. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Vamos gerar um ru√≠do branco com m√©dia zero e desvio padr√£o unit√°rio e plotar sua ACF.  Al√©m disso, calcularemos o intervalo de confian√ßa para verificar se as autocorrela√ß√µes nos lags diferentes de zero s√£o estatisticamente insignificantes.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from statsmodels.graphics.tsaplots import plot_acf
> from scipy import stats
>
> # Gerar ru√≠do branco
> np.random.seed(42) # para reproducibilidade
> white_noise = np.random.randn(100)
> n = len(white_noise)
>
> # Plotar a ACF
> plot_acf(white_noise, lags=20, alpha=0.05) #alpha = 0.05 representa o intervalo de confian√ßa de 95%
> plt.title('ACF de Ru√≠do Branco')
> plt.xlabel('Lag')
> plt.ylabel('Autocorrela√ß√£o')
> plt.show()
>
> # Calcular o intervalo de confian√ßa
> conf_level = 0.95
> alpha = 1 - conf_level
> critical_value = stats.norm.ppf(1 - alpha/2)
> conf_interval = critical_value / np.sqrt(n)
>
> print(f"Intervalo de confian√ßa (n√≠vel de {conf_level*100}%): +/- {conf_interval:.3f}")
> ```
>
> O gr√°fico da ACF exibir√° um pico em lag 0, com as demais autocorrela√ß√µes dentro do intervalo de confian√ßa. Este intervalo de confian√ßa, calculado utilizando o desvio padr√£o assint√≥tico ( $\frac{1}{\sqrt{T}}$ ), auxilia na avalia√ß√£o da signific√¢ncia estat√≠stica das autocorrela√ß√µes amostrais. Por exemplo, com uma amostra de tamanho 100, o intervalo de confian√ßa a 95% √© aproximadamente +/- 0.196. Se alguma autocorrela√ß√£o fora do lag 0 estiver fora desse intervalo, ela pode ser considerada estatisticamente significativa e questionar a hip√≥tese de ru√≠do branco puro.
>
> Se aumentarmos o tamanho da amostra para T = 1000:
> ```python
> import numpy as np
> from scipy import stats
>
> T = 1000
> conf_level = 0.95
> alpha = 1 - conf_level
> critical_value = stats.norm.ppf(1 - alpha/2)
> conf_interval = critical_value / np.sqrt(T)
> print(f"Intervalo de confian√ßa (n√≠vel de {conf_level*100}%): +/- {conf_interval:.3f}")
> ```
> Intervalo de confian√ßa (n√≠vel de 95.0%): +/- 0.062
>
> Observa-se que o intervalo de confian√ßa diminui com o aumento do tamanho da amostra, tornando o teste mais rigoroso.

**Representa√ß√£o da ACF para um Processo MA(1)**

A ACF de um processo MA(1) apresenta uma caracter√≠stica distintiva: autocorrela√ß√£o significativa apenas no *lag* 1 e zero para todos os lags superiores [^5, 3.3.5]. Matematicamente:

$$\rho_j = \begin{cases} \frac{\theta}{1 + \theta^2}, & \text{se } j = 1 \\ 0, & \text{se } j > 1 \end{cases}$$

Graficamente, a ACF de um MA(1) √© representada por uma barra significativa em $\rho_1$ (com a magnitude dependendo do valor de $\theta$) e valores pr√≥ximos de zero para todos os outros lags. O sinal de $\theta$ determina o sinal de $\rho_1$.

**Proposi√ß√£o 3** [ACF de MA(1)]
Para um processo MA(1) definido como $Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$, onde $\epsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, a fun√ß√£o de autocorrela√ß√£o (ACF) √© dada por:
$$\rho_j = \begin{cases} 1, & \text{se } j = 0 \\ \frac{\theta}{1 + \theta^2}, & \text{se } j = 1 \\ 0, & \text{se } j > 1 \end{cases}$$

*Proof:*
I. A autocorrela√ß√£o no lag *j* √© definida como $\rho_j = \frac{Cov(Y_t, Y_{t-j})}{\sqrt{Var(Y_t)Var(Y_{t-j})}}$.

II. Primeiro, calcule a vari√¢ncia de $Y_t$: $Var(Y_t) = Var(\mu + \epsilon_t + \theta \epsilon_{t-1}) = Var(\epsilon_t) + \theta^2 Var(\epsilon_{t-1}) = \sigma^2 + \theta^2 \sigma^2 = \sigma^2(1 + \theta^2)$.

III. Para *j* = 1, calcule a covari√¢ncia $Cov(Y_t, Y_{t-1}) = Cov(\epsilon_t + \theta \epsilon_{t-1}, \epsilon_{t-1} + \theta \epsilon_{t-2}) = Cov(\epsilon_t, \epsilon_{t-1}) + \theta Cov(\epsilon_{t-1}, \epsilon_{t-1}) + \theta Cov(\epsilon_t, \epsilon_{t-2}) + \theta^2 Cov(\epsilon_{t-1}, \epsilon_{t-2}) = 0 + \theta \sigma^2 + 0 + 0 = \theta \sigma^2$.

IV. Portanto, $\rho_1 = \frac{\theta \sigma^2}{\sqrt{\sigma^2(1 + \theta^2) \sigma^2(1 + \theta^2)}} = \frac{\theta \sigma^2}{\sigma^2(1 + \theta^2)} = \frac{\theta}{1 + \theta^2}$.

V. Para *j* > 1, $Cov(Y_t, Y_{t-j}) = Cov(\epsilon_t + \theta \epsilon_{t-1}, \epsilon_{t-j} + \theta \epsilon_{t-j-1}) = 0$, pois n√£o h√° termos comuns entre $Y_t$ e $Y_{t-j}$.

VI. Consequentemente, $\rho_j = 0$ para *j* > 1. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Vamos simular um processo MA(1) com $\theta = 0.7$ e plotar sua ACF.  Tamb√©m variaremos o valor de theta para demonstrar seu impacto na ACF.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from statsmodels.graphics.tsaplots import plot_acf
>
> # Par√¢metros do MA(1)
> mu = 0
> theta = 0.7
> sigma = 1
> n = 100
>
> # Gerar ru√≠do branco
> np.random.seed(42)
> epsilon = np.random.normal(0, sigma, n)
>
> # Gerar o processo MA(1)
> Y = mu + epsilon[1:] + theta * epsilon[:-1]
>
> # Plotar a ACF
> plot_acf(Y, lags=10, alpha=0.05)
> plt.title('ACF de um Processo MA(1) com theta = 0.7')
> plt.xlabel('Lag')
> plt.ylabel('Autocorrela√ß√£o')
> plt.show()
>
> # Calcular o valor te√≥rico de rho_1
> rho_1_theoretical = theta / (1 + theta**2)
> print(f"Valor te√≥rico de rho_1: {rho_1_theoretical:.3f}")
>
> #Simular com theta = -0.7
> theta = -0.7
> Y = mu + epsilon[1:] + theta * epsilon[:-1]
> plot_acf(Y, lags=10, alpha=0.05)
> plt.title('ACF de um Processo MA(1) com theta = -0.7')
> plt.xlabel('Lag')
> plt.ylabel('Autocorrela√ß√£o')
> plt.show()
> rho_1_theoretical = theta / (1 + theta**2)
> print(f"Valor te√≥rico de rho_1: {rho_1_theoretical:.3f}")
>
> #Simular com theta = 0.2
> theta = 0.2
> Y = mu + epsilon[1:] + theta * epsilon[:-1]
> plot_acf(Y, lags=10, alpha=0.05)
> plt.title('ACF de um Processo MA(1) com theta = 0.2')
> plt.xlabel('Lag')
> plt.ylabel('Autocorrela√ß√£o')
> plt.show()
> rho_1_theoretical = theta / (1 + theta**2)
> print(f"Valor te√≥rico de rho_1: {rho_1_theoretical:.3f}")
> ```
>
> O gr√°fico da ACF mostrar√° uma barra significativa no lag 1 (pr√≥ximo ao valor te√≥rico de $\frac{0.7}{1 + 0.7^2} \approx 0.41$) e autocorrela√ß√µes pr√≥ximas de zero para os demais lags. A magnitude da barra em lag 1 reflete a influ√™ncia do par√¢metro $\theta$ na depend√™ncia temporal do processo. Quando theta = -0.7, o rho_1 ser√° negativo, demonstrando a correla√ß√£o negativa no lag 1. Para theta = 0.2, o valor de rho_1 √© menor, demonstrando que para um theta menor, a correla√ß√£o no lag 1 tamb√©m ser√° menor.

**Representa√ß√£o da ACF para um Processo MA(q)**

Podemos generalizar a representa√ß√£o da ACF para um processo MA de ordem *q*, denotado como MA(q). A ACF de um processo MA(q) apresenta autocorrela√ß√µes significativas at√© o *lag* *q*, e autocorrela√ß√µes aproximadamente zero para lags maiores que *q*. Matematicamente:

$$\rho_j = \begin{cases}
\text{Fun√ß√£o de } \theta_1, \theta_2, \ldots, \theta_q, & \text{se } 1 \leq j \leq q \\
0, & \text{se } j > q
\end{cases}$$

**Proposi√ß√£o 1** [ACF de um MA(q)]
Para um processo MA(q) definido como
$$Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \cdots + \theta_q \epsilon_{t-q}$$
onde $\epsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, a fun√ß√£o de autocorrela√ß√£o $\rho_j$ √© dada por:
$$\rho_j = \begin{cases}
1, & \text{se } j = 0 \\
\frac{\sum_{i=0}^{q-j} \theta_i \theta_{i+j}}{1 + \sum_{i=1}^{q} \theta_i^2}, & \text{se } 1 \leq j \leq q \\
0, & \text{se } j > q
\end{cases}$$
onde $\theta_0 = 1$.

*Proof:*
I. A autocorrela√ß√£o no lag *j* √© definida como $\rho_j = \frac{Cov(Y_t, Y_{t-j})}{Var(Y_t)}$.

II. Primeiro, calcule a vari√¢ncia de $Y_t$: $Var(Y_t) = Var(\mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \cdots + \theta_q \epsilon_{t-q}) = \sigma^2(1 + \sum_{i=1}^{q} \theta_i^2)$.

III. Para $1 \leq j \leq q$, calcule a covari√¢ncia $Cov(Y_t, Y_{t-j}) = Cov(\epsilon_t + \theta_1 \epsilon_{t-1} + \cdots + \theta_q \epsilon_{t-q}, \epsilon_{t-j} + \theta_1 \epsilon_{t-j-1} + \cdots + \theta_q \epsilon_{t-j-q})$.  A covari√¢ncia $Cov(Y_t, Y_{t-j})$ √© dada por $\sigma^2 \sum_{i=0}^{q-j} \theta_i \theta_{i+j}$, onde $\theta_0 = 1$.

IV. Para $j > q$, a covari√¢ncia √© zero, pois n√£o h√° termos comuns entre $Y_t$ e $Y_{t-j}$.

V. Dividindo a covari√¢ncia pela vari√¢ncia, obtemos a express√£o para $\rho_j$. ‚ñ†

Graficamente, a ACF de um MA(q) exibir√° barras significativas para os primeiros *q* lags, com as magnitudes dependendo dos valores dos par√¢metros $\theta_1, \theta_2, \ldots, \theta_q$. √â importante notar que, em amostras finitas, as autocorrela√ß√µes amostrais para lags maiores que *q* raramente ser√£o exatamente zero, mas flutuar√£o aleatoriamente em torno de zero dentro de um intervalo de confian√ßa.

**Compara√ß√£o Visual: Ru√≠do Branco vs. MA(1)**

A principal diferen√ßa visual entre a ACF de um ru√≠do branco e a de um processo MA(1) √© a presen√ßa de uma autocorrela√ß√£o significativa no *lag* 1 para o MA(1), enquanto o ru√≠do branco n√£o apresenta autocorrela√ß√£o significativa em nenhum lag diferente de zero [^5]. Esta distin√ß√£o permite identificar visualmente se uma s√©rie temporal exibe caracter√≠sticas de um MA(1) ou se √© mais adequadamente modelada como ru√≠do branco.

> üí° **Exemplo Num√©rico:**
>
> Podemos gerar um ru√≠do branco e um processo MA(1) e plotar suas ACFs lado a lado para compara√ß√£o visual.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from statsmodels.graphics.tsaplots import plot_acf
>
> # Gerar ru√≠do branco
> np.random.seed(42)
> white_noise = np.random.randn(100)
>
> # Par√¢metros do MA(1)
> mu = 0
> theta = 0.7
> sigma = 1
> n = 100
>
> # Gerar ru√≠do branco para o MA(1)
> epsilon = np.random.normal(0, sigma, n)
>
> # Gerar o processo MA(1)
> ma1_process = mu + epsilon[1:] + theta * epsilon[:-1]
>
> # Criar subplots para compara√ß√£o
> fig, axes = plt.subplots(1, 2, figsize=(12, 4))
>
> # Plotar a ACF do ru√≠do branco
> plot_acf(white_noise, lags=10, ax=axes[0], title='ACF de Ru√≠do Branco')
>
> # Plotar a ACF do processo MA(1)
> plot_acf(ma1_process, lags=10, ax=axes[1], title='ACF de um Processo MA(1)')
>
> plt.tight_layout()
> plt.show()
> ```
> Este c√≥digo gerar√° dois gr√°ficos lado a lado, permitindo uma compara√ß√£o visual clara das ACFs.

**Teste de Hip√≥teses para Autocorrela√ß√£o**

Para determinar se os valores da ACF s√£o estatisticamente significativos (diferentes de zero), podemos realizar um teste de hip√≥teses. A hip√≥tese nula √© que a s√©rie temporal √© um ru√≠do branco, e a hip√≥tese alternativa √© que h√° depend√™ncia temporal.

**Estat√≠stica de Teste:**

A estat√≠stica de teste comumente utilizada √© baseada no desvio padr√£o assint√≥tico da autocorrela√ß√£o amostral sob a hip√≥tese nula de ru√≠do branco:

$$z = \frac{\hat{\rho}_j}{\sqrt{\frac{1}{T}}}$$

onde $\hat{\rho}_j$ √© a autocorrela√ß√£o amostral no *lag* *j* e *T* √© o tamanho da amostra. Sob a hip√≥tese nula, *z* segue aproximadamente uma distribui√ß√£o normal padr√£o.

**Regra de Decis√£o:**

Rejeitamos a hip√≥tese nula se o valor absoluto da estat√≠stica de teste for maior que o valor cr√≠tico correspondente a um n√≠vel de signific√¢ncia $\alpha$:

$$|z| > z_{\alpha/2}$$

onde $z_{\alpha/2}$ √© o valor cr√≠tico da distribui√ß√£o normal padr√£o para um teste bicaudal. Para um n√≠vel de signific√¢ncia de 5% ($\alpha = 0.05$), $z_{0.025} \approx 1.96$.

> üí° **Exemplo Num√©rico:**
>
> Em uma s√©rie temporal com *T* = 200 observa√ß√µes, observamos uma autocorrela√ß√£o amostral no *lag* 3 de $\hat{\rho}_3 = 0.15$. A estat√≠stica de teste seria:
>
> $$z = \frac{0.15}{\sqrt{\frac{1}{200}}} = \frac{0.15}{0.0707} \approx 2.12$$
>
> Como $|z| = 2.12 > 1.96$, rejeitamos a hip√≥tese nula ao n√≠vel de signific√¢ncia de 5%, concluindo que h√° evid√™ncias de depend√™ncia temporal no *lag* 3.
>
> ```python
> import numpy as np
> from scipy import stats
>
> rho_j = 0.15
> T = 200
> alpha = 0.05
>
> z = rho_j / np.sqrt(1/T)
> critical_value = stats.norm.ppf(1 - alpha/2)
>
> print(f"Estat√≠stica de teste z: {z:.3f}")
> print(f"Valor cr√≠tico (alpha={alpha}): {critical_value:.3f}")
>
> if abs(z) > critical_value:
>     print("Rejeitar a hip√≥tese nula: H√° depend√™ncia temporal significativa.")
> else:
>     print("N√£o rejeitar a hip√≥tese nula: N√£o h√° evid√™ncia de depend√™ncia temporal significativa.")
> ```
>
> Agora, vamos considerar um exemplo onde a hip√≥tese nula n√£o √© rejeitada. Se, em uma s√©rie temporal com *T* = 500 observa√ß√µes, observarmos uma autocorrela√ß√£o amostral no *lag* 5 de  $\hat{\rho}_5 = 0.05$. A estat√≠stica de teste seria:
>
> $$z = \frac{0.05}{\sqrt{\frac{1}{500}}} = \frac{0.05}{0.0447} \approx 1.12$$
>
> Como $|z| = 1.12 < 1.96$, n√£o rejeitamos a hip√≥tese nula ao n√≠vel de signific√¢ncia de 5%, concluindo que n√£o h√° evid√™ncias de depend√™ncia temporal significativa no *lag* 5.

**Autocorrela√ß√£o Parcial (PACF)**

Embora o foco principal deste cap√≠tulo seja a ACF, √© importante mencionar brevemente a Fun√ß√£o de Autocorrela√ß√£o Parcial (PACF). A PACF mede a correla√ß√£o entre $Y_t$ e $Y_{t-j}$ ap√≥s remover a influ√™ncia dos lags intermedi√°rios $Y_{t-1}, Y_{t-2}, \ldots, Y_{t-j+1}$. A PACF auxilia na identifica√ß√£o da ordem de processos Autoregressivos (AR), que n√£o s√£o o foco deste cap√≠tulo. Para processos MA(1), a PACF geralmente apresenta um decaimento exponencial.

Para fornecer um entendimento mais completo da PACF, podemos apresentar sua defini√ß√£o formal:

**Defini√ß√£o:** A Autocorrela√ß√£o Parcial (PACF) no lag *j*, denotada por $\alpha_j$, √© a correla√ß√£o entre $Y_t$ e $Y_{t-j}$ condicional aos valores intermedi√°rios $Y_{t-1}, Y_{t-2}, \ldots, Y_{t-j+1}$. Matematicamente, $\alpha_j = Corr(Y_t, Y_{t-j} | Y_{t-1}, Y_{t-2}, \ldots, Y_{t-j+1})$. A PACF pode ser obtida resolvendo as equa√ß√µes de Yule-Walker.

**Teorema 1** [Rela√ß√£o entre ACF e PACF]
As fun√ß√µes de autocorrela√ß√£o (ACF) e autocorrela√ß√£o parcial (PACF) s√£o relacionadas pelas equa√ß√µes de Yule-Walker. Para um processo AR(p), a PACF corta ap√≥s o lag p, enquanto a ACF decai gradualmente. Para um processo MA(q), a ACF corta ap√≥s o lag q, enquanto a PACF decai gradualmente.

*Proof:*
I. As equa√ß√µes de Yule-Walker expressam as autocorrela√ß√µes de um processo AR(p) em termos dos seus coeficientes.

II. A PACF, por defini√ß√£o, fornece a correla√ß√£o adicional entre $Y_t$ e $Y_{t-j}$ ap√≥s remover o efeito dos lags intermedi√°rios. Para um AR(p), esta correla√ß√£o adicional √© zero para j > p.

III. Similarmente, a estrutura dual entre AR e MA processos implica que para um MA(q) a ACF √© zero para j > q, enquanto a PACF exibir√° um decaimento. ‚ñ†

**Corol√°rio 1** [Identifica√ß√£o de Modelos ARMA]
A an√°lise combinada da ACF e PACF permite identificar a ordem (p, q) de um processo ARMA(p, q). Se a ACF decai e a PACF corta ap√≥s o lag p, o processo √© AR(p). Se a PACF decai e a ACF corta ap√≥s o lag q, o processo √© MA(q). Se ambas ACF e PACF decaem, o processo √© ARMA(p, q) com p, q > 0.

> üí° **Exemplo Num√©rico:**
>
> Vamos simular um processo MA(1) com $\theta = 0.7$ e plotar sua ACF e PACF para ilustrar o comportamento da PACF em um processo MA(1).
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
>
> # Par√¢metros do MA(1)
> mu = 0
> theta = 0.7
> sigma = 1
> n = 100
>
> # Gerar ru√≠do branco
> np.random.seed(42)
> epsilon = np.random.normal(0, sigma, n)
>
> # Gerar o processo MA(1)
> Y = mu + epsilon[1:] + theta * epsilon[:-1]
>
> # Criar subplots para ACF e PACF
> fig, axes = plt.subplots(1, 2, figsize=(12, 4))
>
> # Plotar a ACF
> plot_acf(Y, lags=10, ax=axes[0], title='ACF de um Processo MA(1)')
>
> # Plotar a PACF
> plot_pacf(Y, lags=10, ax=axes[1], title='PACF de um Processo MA(1)')
>
> plt.tight_layout()
> plt.show()
> ```
> O gr√°fico da PACF mostrar√° um decaimento, enquanto a ACF mostrar√° um corte ap√≥s o lag 1, como esperado para um processo MA(1).

### Conclus√£o
A representa√ß√£o visual da fun√ß√£o de autocorrela√ß√£o (ACF) √© uma ferramenta indispens√°vel na an√°lise de s√©ries temporais estacion√°rias ARMA [^49]. A ACF permite identificar padr√µes de depend√™ncia temporal, distinguindo entre processos de ru√≠do branco e MA(1), e auxiliando na escolha de modelos apropriados. A an√°lise da ACF, combinada com testes de hip√≥teses e a considera√ß√£o do intervalo de confian√ßa, fornece uma base s√≥lida para a modelagem e previs√£o de s√©ries temporais [^7].

### Refer√™ncias
[^3]: Informa√ß√£o retirada da introdu√ß√£o do cap√≠tulo.
[^49]: Informa√ß√£o retirada da p√°gina 49, se√ß√£o 3.3.6.
[^5]: Informa√ß√£o retirada da p√°gina 48.
[^7]: Informa√ß√£o retirada da p√°gina 47.
<!-- END -->