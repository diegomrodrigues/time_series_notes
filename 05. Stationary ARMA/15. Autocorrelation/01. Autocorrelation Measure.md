## Autocorrela√ß√£o em S√©ries Temporais Estacion√°rias ARMA

### Introdu√ß√£o
A autocorrela√ß√£o √© uma ferramenta fundamental na an√°lise de s√©ries temporais, permitindo identificar padr√µes e depend√™ncias temporais nos dados. Este cap√≠tulo aprofunda o conceito de **autocorrela√ß√£o** ($\rho_j$) em s√©ries temporais estacion√°rias ARMA, expandindo a defini√ß√£o e propriedades apresentadas anteriormente [^3]. A autocorrela√ß√£o quantifica a rela√ß√£o linear entre valores da s√©rie temporal em diferentes pontos no tempo, fornecendo *insights* valiosos sobre a estrutura temporal dos dados.

### Conceitos Fundamentais
A autocorrela√ß√£o ($\rho_j$) de uma s√©rie temporal estacion√°ria ARMA √© definida como a correla√ß√£o entre $$Y_t$$ e $$Y_{t-j}$$, onde *j* representa o *lag* ou o n√∫mero de per√≠odos de tempo que separam as duas observa√ß√µes [^49, 3.3.6]. Matematicamente, a autocorrela√ß√£o √© expressa como:

$$\rho_j = \frac{Cov(Y_t, Y_{t-j})}{\sqrt{Var(Y_t)Var(Y_{t-j})}}$$

Como a s√©rie √© estacion√°ria, a vari√¢ncia √© constante ao longo do tempo, ou seja, $$Var(Y_t) = Var(Y_{t-j}) = \gamma_0$$. Al√©m disso, $$Cov(Y_t, Y_{t-j})$$ √© a autocovari√¢ncia $$\gamma_j$$. Portanto, a equa√ß√£o simplifica para:

$$\rho_j = \frac{\gamma_j}{\gamma_0}$$

A autocorrela√ß√£o √© uma medida adimensional, variando entre -1 e 1 [^49]. Um valor de $$\rho_j$$ pr√≥ximo de 1 indica uma forte correla√ß√£o positiva entre $$Y_t$$ e $$Y_{t-j}$$, o que significa que valores altos (ou baixos) em *t* tendem a ser seguidos por valores altos (ou baixos) em *t-j*. Um valor de $$\rho_j$$ pr√≥ximo de -1 indica uma forte correla√ß√£o negativa, onde valores altos em *t* tendem a ser seguidos por valores baixos em *t-j*, e vice-versa. Um valor de $$\rho_j$$ pr√≥ximo de 0 sugere uma correla√ß√£o linear fraca ou inexistente.

> üí° **Exemplo Num√©rico:**
>
> Imagine uma s√©rie temporal representando as vendas mensais de uma loja. Se a autocorrela√ß√£o no *lag* 1 ($\rho_1$) for 0.8, isso sugere que as vendas em um determinado m√™s est√£o fortemente correlacionadas com as vendas do m√™s anterior. Um valor alto de vendas em um m√™s tende a ser seguido por um valor alto no m√™s seguinte. Por outro lado, se $\rho_1$ fosse -0.7, um m√™s com vendas altas tenderia a ser seguido por um m√™s com vendas baixas. Se $\rho_1$ fosse pr√≥ximo de 0 (por exemplo, 0.1), as vendas de um m√™s seriam praticamente independentes das vendas do m√™s anterior.

√â importante notar que $$\rho_0 = \frac{\gamma_0}{\gamma_0} = 1$$ [^49], o que significa que a correla√ß√£o de uma s√©rie consigo mesma (sem *lag*) √© sempre igual a 1. Isso serve como um ponto de refer√™ncia na an√°lise da fun√ß√£o de autocorrela√ß√£o (ACF).

**Prova:**
A prova de que $$\rho_0 = 1$$ √© direta:

I. Pela defini√ß√£o, $$\rho_0 = \frac{Cov(Y_t, Y_{t-0})}{\sqrt{Var(Y_t)Var(Y_{t-0})}}$$
II. Como $$Y_{t-0} = Y_t$$, temos $$\rho_0 = \frac{Cov(Y_t, Y_t)}{\sqrt{Var(Y_t)Var(Y_t)}}$$
III. Sabemos que $$Cov(Y_t, Y_t) = Var(Y_t)$$, logo $$\rho_0 = \frac{Var(Y_t)}{Var(Y_t)}$$
IV. Portanto, $$\rho_0 = 1$$ ‚ñ†

**Fun√ß√£o de Autocorrela√ß√£o (ACF)**: A ACF √© um gr√°fico que plota os valores de $$\rho_j$$ em fun√ß√£o de *j* (o *lag*). A ACF fornece uma representa√ß√£o visual da depend√™ncia temporal na s√©rie temporal, revelando quais *lags* s√£o significativamente correlacionados. A an√°lise da ACF √© crucial para identificar a ordem de modelos ARMA apropriados para a s√©rie temporal [^7].

Para complementar a an√°lise da ACF, √© √∫til considerar tamb√©m os intervalos de confian√ßa para as autocorrela√ß√µes. Isso permite determinar se os valores de $$\rho_j$$ s√£o estatisticamente significativos, ou seja, se s√£o diferentes de zero em um n√≠vel de signific√¢ncia predeterminado.

**Intervalos de Confian√ßa para ACF**:
Sob a hip√≥tese nula de que a s√©rie temporal √© puramente aleat√≥ria (ru√≠do branco), os valores de $$\rho_j$$ s√£o aproximadamente normalmente distribu√≠dos com m√©dia zero e vari√¢ncia $$\frac{1}{T}$$, onde *T* √© o tamanho da amostra [^60]. Portanto, um intervalo de confian√ßa aproximado de 95% para $$\rho_j$$ √© dado por:

$$IC_{95\%}(\rho_j) = \pm \frac{1.96}{\sqrt{T}}$$

Valores de $$\rho_j$$ que caem fora deste intervalo s√£o considerados estatisticamente significativos e indicam uma depend√™ncia temporal n√£o aleat√≥ria na s√©rie.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos uma s√©rie temporal com *T* = 100 observa√ß√µes. O intervalo de confian√ßa de 95% para a ACF seria:
>
> $$IC_{95\%}(\rho_j) = \pm \frac{1.96}{\sqrt{100}} = \pm 0.196$$
>
> Isso significa que, se a s√©rie fosse ru√≠do branco, esperar√≠amos que 95% das autocorrela√ß√µes amostrais estivessem entre -0.196 e 0.196. Se observarmos um valor de $$\rho_3 = 0.3$$, este valor estaria fora do intervalo de confian√ßa e, portanto, seria considerado estatisticamente significativo, sugerindo que h√° uma depend√™ncia temporal no *lag* 3.
>
> ```python
> import numpy as np
>
> T = 100
> ic_95 = 1.96 / np.sqrt(T)
> print(f"Intervalo de Confian√ßa de 95%: +/- {ic_95:.3f}")
> ```

**Lema 1**: *Para uma s√©rie temporal de ru√≠do branco com tamanho amostral T, a probabilidade de encontrar pelo menos um lag j com autocorrela√ß√£o significativamente diferente de zero (fora do intervalo de confian√ßa de 95%) aumenta com o aumento do n√∫mero de lags considerados.*

*Prova:*
A probabilidade de que uma √∫nica autocorrela√ß√£o estimada $$\rho_j$$ esteja dentro do intervalo de confian√ßa de 95% √© de 0.95. Se analisarmos *m* lags, a probabilidade de que todas as *m* autocorrela√ß√µes estejam dentro dos respectivos intervalos de confian√ßa √© $$0.95^m$$. Portanto, a probabilidade de que pelo menos uma autocorrela√ß√£o esteja fora do intervalo de confian√ßa √© $$1 - 0.95^m$$. √Ä medida que *m* aumenta, $$1 - 0.95^m$$ tamb√©m aumenta, demonstrando o lema.

> üí° **Exemplo Num√©rico:**
>
> Vamos calcular a probabilidade de encontrar pelo menos uma autocorrela√ß√£o significativa em uma s√©rie de ru√≠do branco para diferentes n√∫meros de lags (*m*):
>
> *   Para *m* = 1 lag: $$1 - 0.95^1 = 0.05$$ (5% de chance)
> *   Para *m* = 5 lags: $$1 - 0.95^5 \approx 0.226$$ (22.6% de chance)
> *   Para *m* = 10 lags: $$1 - 0.95^{10} \approx 0.401$$ (40.1% de chance)
> *   Para *m* = 20 lags: $$1 - 0.95^{20} \approx 0.642$$ (64.2% de chance)
>
> Este exemplo ilustra como a probabilidade de identificar um falso positivo aumenta consideravelmente com o n√∫mero de lags analisados.
>
> ```python
> import numpy as np
>
> lags = [1, 5, 10, 20]
> probabilities = [1 - (0.95)**m for m in lags]
>
> for m, prob in zip(lags, probabilities):
>     print(f"Probabilidade para m={m} lags: {prob:.3f}")
> ```

Esse lema refor√ßa a import√¢ncia de cautela na interpreta√ß√£o da ACF, especialmente quando se analisam muitos lags, pois a chance de identificar falsos positivos aumenta.

**Propriedades da Autocorrela√ß√£o**:

1.  **Simetria**: Para processos estacion√°rios, a autocorrela√ß√£o √© sim√©trica, ou seja, $$\rho_j = \rho_{-j}$$. Isso significa que a correla√ß√£o entre $$Y_t$$ e $$Y_{t-j}$$ √© a mesma que a correla√ß√£o entre $$Y_t$$ e $$Y_{t+j}$$.

**Prova:**
Para provar a simetria da autocorrela√ß√£o, mostramos que  $$\rho_j = \rho_{-j}$$:
I.  Por defini√ß√£o, $$\rho_j = \frac{\gamma_j}{\gamma_0} = \frac{Cov(Y_t, Y_{t-j})}{Var(Y_t)}$$
II.  Similarmente, $$\rho_{-j} = \frac{\gamma_{-j}}{\gamma_0} = \frac{Cov(Y_t, Y_{t+j})}{Var(Y_t)}$$
III. Como a s√©rie √© estacion√°ria, a autocovari√¢ncia depende apenas da dist√¢ncia entre os pontos no tempo, ent√£o $$Cov(Y_t, Y_{t-j}) = Cov(Y_{t+j}, Y_t)$$
IV. Usando a propriedade de simetria da covari√¢ncia, $$Cov(Y_{t+j}, Y_t) = Cov(Y_t, Y_{t+j})$$
V. Portanto, $$\rho_{-j} = \frac{Cov(Y_t, Y_{t+j})}{Var(Y_t)} = \frac{Cov(Y_t, Y_{t-j})}{Var(Y_t)} = \rho_j$$ ‚ñ†

2.  **Decaimento**: A autocorrela√ß√£o de um processo estacion√°rio tende a decair √† medida que *j* aumenta. A velocidade e o padr√£o desse decaimento fornecem informa√ß√µes importantes sobre a mem√≥ria do processo. Por exemplo, processos AR t√™m um decaimento exponencial ou amortecido, enquanto processos MA t√™m um corte abrupto ap√≥s um certo *lag* [^53, 56].

> üí° **Exemplo Num√©rico:**
>
> Considere um processo AR(1) com $$\phi = 0.5$$. As autocorrela√ß√µes para os primeiros lags seriam:
>
> *   $$\rho_0 = 1$$
> *   $$\rho_1 = 0.5^1 = 0.5$$
> *   $$\rho_2 = 0.5^2 = 0.25$$
> *   $$\rho_3 = 0.5^3 = 0.125$$
> *   $$\rho_4 = 0.5^4 = 0.0625$$
>
> Observe como a autocorrela√ß√£o decai exponencialmente. Ap√≥s alguns lags, a autocorrela√ß√£o se torna muito pequena, indicando que a depend√™ncia temporal diminui com o aumento da dist√¢ncia no tempo. J√° em um processo MA(1), a autocorrela√ß√£o √© zero para lags maiores que 1, indicando uma mem√≥ria curta.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> phi = 0.5
> lags = np.arange(5)
> autocorrelations = phi ** lags
>
> plt.stem(lags, autocorrelations)
> plt.title('Autocorrela√ß√£o do Processo AR(1) com phi=0.5')
> plt.xlabel('Lag (j)')
> plt.ylabel('Autocorrela√ß√£o (rho_j)')
> plt.show()
> ```

Para auxiliar na identifica√ß√£o do decaimento, podemos definir uma medida de "meia-vida" da autocorrela√ß√£o.

**Defini√ß√£o**: A meia-vida da autocorrela√ß√£o √© o n√∫mero de lags (*j*) necess√°rios para que a magnitude da autocorrela√ß√£o ($$|\rho_j|$$) caia abaixo da metade de seu valor inicial ($$|\rho_1|$$).

Essa medida pode ajudar a comparar a persist√™ncia da autocorrela√ß√£o entre diferentes s√©ries temporais ou modelos.

> üí° **Exemplo Num√©rico:**
>
> Para o exemplo anterior do processo AR(1) com $$\phi = 0.5$$, temos $$\rho_1 = 0.5$$. Queremos encontrar o menor *j* tal que $$|\rho_j| < \frac{|\rho_1|}{2} = \frac{0.5}{2} = 0.25$$.
>
> *   $$\rho_1 = 0.5$$
> *   $$\rho_2 = 0.25$$
> *   $$\rho_3 = 0.125$$
>
> Neste caso, a meia-vida √© 3, pois $$\rho_3 = 0.125 < 0.25$$. Isso significa que a autocorrela√ß√£o "perde" metade de sua for√ßa em 3 lags.

3.  **Rela√ß√£o com Autocovari√¢ncia**: Como mencionado anteriormente, a autocorrela√ß√£o √© uma vers√£o normalizada da autocovari√¢ncia. Portanto, as propriedades da autocovari√¢ncia tamb√©m influenciam a autocorrela√ß√£o.

**Teorema 1**: *Se a autocovari√¢ncia $$\gamma_j$$ de uma s√©rie temporal estacion√°ria √© absolutamente som√°vel, ou seja, $$\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$$, ent√£o a s√©rie temporal √© dita ter mem√≥ria curta.*

*Prova:*
A condi√ß√£o $$\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$$ implica que o impacto das observa√ß√µes passadas decai rapidamente, de modo que a depend√™ncia entre observa√ß√µes distantes no tempo torna-se neglig√≠vel. Em outras palavras, a s√©rie temporal "esquece" seu passado rapidamente, caracterizando um comportamento de mem√≥ria curta.

√â importante notar que a condi√ß√£o de absolutamente som√°vel para a autocovari√¢ncia √© uma condi√ß√£o suficiente, mas n√£o necess√°ria, para a mem√≥ria curta.

**Exemplo de Autocorrela√ß√£o em um Processo MA(1)**:
Considere o processo MA(1):

$$Y_t = \mu + \varepsilon_t + \theta \varepsilon_{t-1}$$

A autocorrela√ß√£o no *lag* 1 ($\rho_1$) para este processo √© dada por [^49, 3.3.7]:

$$\rho_1 = \frac{\theta}{1 + \theta^2}$$

E as demais autocorrela√ß√µes s√£o zero ($\rho_j = 0$ para *j > 1*) [^49]. Isso demonstra que o processo MA(1) tem mem√≥ria apenas no *lag* 1.

> üí° **Exemplo Num√©rico:**
>
> Se $$\theta = 0.5$$ no processo MA(1), ent√£o:
>
> $$\rho_1 = \frac{0.5}{1 + 0.5^2} = \frac{0.5}{1.25} = 0.4$$
>
> Isso significa que a correla√ß√£o entre $$Y_t$$ e $$Y_{t-1}$$ √© 0.4. Para todos os outros lags (*j* > 1), $$\rho_j = 0$$.
>
> Se $$\theta = -0.8$$ no processo MA(1), ent√£o:
>
> $$\rho_1 = \frac{-0.8}{1 + (-0.8)^2} = \frac{-0.8}{1.64} \approx -0.488$$
>
> Neste caso, a correla√ß√£o entre $$Y_t$$ e $$Y_{t-1}$$ √© aproximadamente -0.488.

**Lema 2**: *Para o processo MA(1), a autocovari√¢ncia $$\gamma_j$$ √© absolutamente som√°vel.*

*Prova:*
Para o processo MA(1), temos que $$\gamma_0 = (1+\theta^2)\sigma^2$$ , $$\gamma_1 = \theta \sigma^2$$ e $$\gamma_j = 0$$ para $$j > 1$$.
Portanto, $$\sum_{j=-\infty}^{\infty} |\gamma_j| = |\gamma_{-1}| + |\gamma_0| + |\gamma_1| = |\theta \sigma^2| + |(1+\theta^2)\sigma^2| + |\theta \sigma^2| = 2|\theta|\sigma^2 + (1+\theta^2)\sigma^2 < \infty$$, uma vez que $$\theta$$ e $$\sigma^2$$ s√£o par√¢metros finitos. Assim, a autocovari√¢ncia √© absolutamente som√°vel.

Como consequ√™ncia do Teorema 1 e do Lema 2, podemos afirmar que o processo MA(1) √© um processo de mem√≥ria curta.

**Exemplo de Autocorrela√ß√£o em um Processo AR(1)**:
Agora, considere o processo AR(1):

$$Y_t = \phi Y_{t-1} + \varepsilon_t$$

A autocorrela√ß√£o no *lag* j ($\rho_j$) para este processo √© dada por:

$$\rho_j = \phi^j$$

Este resultado mostra que a autocorrela√ß√£o em um processo AR(1) decai exponencialmente com o aumento do lag *j*. A taxa de decaimento √© determinada pelo coeficiente $$\phi$$. Se $$|\phi| < 1$$, a autocorrela√ß√£o converge para zero √† medida que *j* aumenta.

> üí° **Exemplo Num√©rico:**
>
> Considere um processo AR(1) com $$\phi = 0.8$$. As autocorrela√ß√µes para alguns lags seriam:
>
> *   $$\rho_0 = 1$$
> *   $$\rho_1 = 0.8^1 = 0.8$$
> *   $$\rho_2 = 0.8^2 = 0.64$$
> *   $$\rho_3 = 0.8^3 = 0.512$$
> *   $$\rho_4 = 0.8^4 = 0.4096$$
>
> Se $$\phi = -0.6$$, as autocorrela√ß√µes seriam:
>
> *   $$\rho_0 = 1$$
> *   $$\rho_1 = (-0.6)^1 = -0.6$$
> *   $$\rho_2 = (-0.6)^2 = 0.36$$
> *   $$\rho_3 = (-0.6)^3 = -0.216$$
> *   $$\rho_4 = (-0.6)^4 = 0.1296$$
>
> Neste caso, a autocorrela√ß√£o alterna entre valores positivos e negativos, mas ainda decai exponencialmente em magnitude.

**Teorema 2**: *O processo AR(1) √© estacion√°rio se e somente se $$|\phi| < 1$$.*

Este teorema √© fundamental para garantir que as propriedades estat√≠sticas do processo AR(1) sejam bem definidas e que a autocorrela√ß√£o decaia para zero √† medida que o lag aumenta.

**Prova:**
A prova da estacionariedade do AR(1) quando $$|\phi|<1$$ pode ser feita da seguinte forma:

I. Podemos reescrever o processo AR(1) recursivamente:
   $$Y_t = \phi Y_{t-1} + \varepsilon_t = \phi (\phi Y_{t-2} + \varepsilon_{t-1}) + \varepsilon_t = \phi^2 Y_{t-2} + \phi \varepsilon_{t-1} + \varepsilon_t$$
II. Continuando a substitui√ß√£o, obtemos:
   $$Y_t = \sum_{i=0}^{\infty} \phi^i \varepsilon_{t-i}$$
III. Para que $$Y_t$$ seja estacion√°rio, sua m√©dia e vari√¢ncia devem ser constantes ao longo do tempo e finitas.
IV. A m√©dia de $$Y_t$$ √©:
    $$E[Y_t] = E\left[\sum_{i=0}^{\infty} \phi^i \varepsilon_{t-i}\right] = \sum_{i=0}^{\infty} \phi^i E[\varepsilon_{t-i}] = 0$$  (assumindo que $$E[\varepsilon_t] = 0$$)
V. A vari√¢ncia de $$Y_t$$ √©:
   $$Var(Y_t) = Var\left(\sum_{i=0}^{\infty} \phi^i \varepsilon_{t-i}\right) = \sum_{i=0}^{\infty} \phi^{2i} Var(\varepsilon_{t-i}) = \sigma^2 \sum_{i=0}^{\infty} \phi^{2i}$$  (assumindo que $$Var(\varepsilon_t) = \sigma^2$$)
VI. A soma $$\sum_{i=0}^{\infty} \phi^{2i}$$ √© uma s√©rie geom√©trica que converge se $$|\phi^2| < 1$$, ou seja, $$|\phi| < 1$$. Neste caso, a soma √© igual a $$\frac{1}{1 - \phi^2}$$.
VII. Portanto, se $$|\phi| < 1$$, a vari√¢ncia de $$Y_t$$ √©:
    $$Var(Y_t) = \frac{\sigma^2}{1 - \phi^2}$$
VIII. Como a m√©dia e a vari√¢ncia de $$Y_t$$ s√£o constantes e finitas quando $$|\phi| < 1$$, o processo AR(1) √© estacion√°rio sob esta condi√ß√£o. A prova da n√£o estacionariedade quando $$|\phi| \geq 1$$ √© mais complexa e envolve mostrar que a vari√¢ncia diverge. ‚ñ†

**Teorema 2.1**: *Se o processo AR(1) √© estacion√°rio ($$|\phi| < 1$$), ent√£o a autocovari√¢ncia $$\gamma_j$$ √© absolutamente som√°vel, e portanto o processo AR(1) √© de mem√≥ria curta.*

*Prova:*
Se $$|\phi| < 1$$, ent√£o $$\gamma_j = \phi^j \gamma_0$$.  Portanto,
$$\sum_{j=-\infty}^{\infty} |\gamma_j| = \sum_{j=-\infty}^{\infty} |\phi^j \gamma_0| = |\gamma_0| \sum_{j=-\infty}^{\infty} |\phi^j| = |\gamma_0| \left( \sum_{j=-\infty}^{-1} |\phi^j| + 1 + \sum_{j=1}^{\infty} |\phi^j| \right)$$
Como $$|\phi| < 1$$, as somas geom√©tricas convergem:
$$\sum_{j=1}^{\infty} |\phi^j| = \frac{|\phi|}{1-|\phi|}$$ e $$\sum_{j=-\infty}^{-1} |\phi^j| = \sum_{k=1}^{\infty} |\phi^{-k}| = \sum_{k=1}^{\infty} \left(\frac{1}{|\phi|}\right)^k = \frac{\frac{1}{|\phi|}}{1 - \frac{1}{|\phi|}} = \frac{1}{ |\phi| - 1 } = \frac{-1}{1-|\phi|}$$
Assim, $$\sum_{j=-\infty}^{\infty} |\gamma_j| = |\gamma_0| \left( \frac{1}{1-|\phi|} + 1 + \frac{|\phi|}{1-|\phi|} \right) = |\gamma_0| \left( \frac{2-|\phi|}{1-|\phi|} \right) < \infty$$.

Portanto, se o processo AR(1) √© estacion√°rio, sua autocovari√¢ncia √© absolutamente som√°vel, implicando que ele √© de mem√≥ria curta.

### Conclus√£o
A autocorrela√ß√£o ($\rho_j$) √© uma ferramenta essencial na an√°lise de s√©ries temporais estacion√°rias ARMA, permitindo quantificar e visualizar a depend√™ncia temporal nos dados. Ao analisar a fun√ß√£o de autocorrela√ß√£o (ACF), √© poss√≠vel identificar padr√µes, mem√≥ria e a ordem de modelos ARMA adequados para representar a s√©rie temporal. O 0th autocorrela√ß√£o sendo sempre igual a 1 fornece um importante ponto de refer√™ncia. Compreender a autocorrela√ß√£o √© fundamental para modelagem, previs√£o e infer√™ncia estat√≠stica em s√©ries temporais [^7, 49]. A considera√ß√£o de intervalos de confian√ßa e medidas como a meia-vida da autocorrela√ß√£o auxiliam em uma an√°lise mais robusta e informativa.

### Refer√™ncias
[^3]: Informa√ß√£o retirada da introdu√ß√£o do cap√≠tulo.
[^49]: Informa√ß√£o retirada da p√°gina 49, se√ß√£o 3.3.6.
[^7]: Informa√ß√£o retirada da p√°gina 47.
[^53]: Informa√ß√£o retirada da p√°gina 53.
[^56]: Informa√ß√£o retirada da p√°gina 56.
[^60]: Estat√≠stica para Economia e Administra√ß√£o, Morettin e Bussab, 7¬™ Edi√ß√£o.
<!-- END -->