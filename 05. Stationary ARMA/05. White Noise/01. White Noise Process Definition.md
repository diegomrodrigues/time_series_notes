## White Noise: Defini√ß√£o, Propriedades e Import√¢ncia

### Introdu√ß√£o

O **white noise** √© um conceito fundamental na an√°lise de s√©ries temporais, servindo como um bloco de constru√ß√£o b√°sico para modelos mais complexos, como os modelos ARMA (Autoregressive Moving Average). Este cap√≠tulo detalha as propriedades cruciais do white noise e sua relev√¢ncia na modelagem de processos estoc√°sticos.

### Conceitos Fundamentais

Um processo de **white noise** {Œµ‚Çú} √© definido por tr√™s propriedades essenciais [^47]:

1.  **M√©dia Zero:** A esperan√ßa de cada termo Œµ‚Çú √© zero:
    $$E(\varepsilon_t) = 0$$ [^47]
    Esta propriedade garante que o processo n√£o tenha uma tend√™ncia sistem√°tica para valores positivos ou negativos.

    > üí° **Exemplo Num√©rico:** Considere uma sequ√™ncia de 100 n√∫meros aleat√≥rios gerados a partir de uma distribui√ß√£o normal com m√©dia zero e desvio padr√£o 1. Se calcularmos a m√©dia amostral dessa sequ√™ncia, esperamos que ela esteja pr√≥xima de zero.
    ```python
    import numpy as np

    np.random.seed(42) # Seed para reproducibilidade
    epsilon = np.random.normal(0, 1, 100) # Gera 100 n√∫meros aleat√≥rios de uma normal(0,1)
    media_amostral = np.mean(epsilon)
    print(f"M√©dia amostral: {media_amostral:.4f}")
    ```
    O resultado ser√° pr√≥ximo de zero, ilustrando a propriedade de m√©dia zero.

2.  **Vari√¢ncia Constante:** A vari√¢ncia de cada termo Œµ‚Çú √© constante e igual a œÉ¬≤:
    $$E(\varepsilon_t^2) = \sigma^2$$ [^47]
    Isso significa que a dispers√£o dos valores em torno da m√©dia (zero) √© a mesma em todos os pontos no tempo.

    **Prova:**

    Para demonstrar que a vari√¢ncia √© constante e igual a  œÉ¬≤, considere:

    I. Por defini√ß√£o, a vari√¢ncia de Œµ‚Çú √© dada por:
    $$Var(\varepsilon_t) = E[(\varepsilon_t - E[\varepsilon_t])^2]$$

    II. Dado que  $E[\varepsilon_t] = 0$  (m√©dia zero), a express√£o simplifica para:
    $$Var(\varepsilon_t) = E[\varepsilon_t^2]$$

    III. Por defini√ß√£o da propriedade de vari√¢ncia constante,  $E[\varepsilon_t^2] = \sigma^2$.

    IV. Portanto,  $Var(\varepsilon_t) = \sigma^2$.  Isto demonstra que a vari√¢ncia de cada termo Œµ‚Çú √© constante e igual a œÉ¬≤. ‚ñ†

    > üí° **Exemplo Num√©rico:** Usando a mesma sequ√™ncia do exemplo anterior, podemos calcular a vari√¢ncia amostral. Esperamos que ela esteja pr√≥xima de 1 (j√° que o desvio padr√£o √© 1).
    ```python
    import numpy as np

    np.random.seed(42)
    epsilon = np.random.normal(0, 1, 100)
    variancia_amostral = np.var(epsilon)
    print(f"Vari√¢ncia amostral: {variancia_amostral:.4f}")
    ```
    Este resultado confirma a propriedade de vari√¢ncia constante.

3.  **N√£o Correla√ß√£o:** Termos em diferentes pontos no tempo s√£o n√£o correlacionados:
    $$E(\varepsilon_t \varepsilon_\tau) = 0 \quad \text{para } t \neq \tau$$ [^47]
    Esta propriedade crucial implica que n√£o h√° depend√™ncia linear entre os valores do processo em diferentes momentos; ou seja, o valor em um ponto no tempo n√£o fornece nenhuma informa√ß√£o sobre o valor em outro ponto no tempo.

    > üí° **Exemplo Num√©rico:** Para demonstrar a n√£o correla√ß√£o, podemos calcular a correla√ß√£o entre a sequ√™ncia original e uma vers√£o deslocada dela (lagged). Se a sequ√™ncia for white noise, a correla√ß√£o deve ser pr√≥xima de zero.
    ```python
    import numpy as np

    np.random.seed(42)
    epsilon = np.random.normal(0, 1, 100)
    epsilon_lagged = np.concatenate(([0], epsilon[:-1]))  # Desloca a s√©rie em 1 posi√ß√£o
    correlacao = np.corrcoef(epsilon[1:], epsilon_lagged[1:])[0, 1]
    print(f"Correla√ß√£o entre a s√©rie e sua vers√£o deslocada: {correlacao:.4f}")
    ```
    O valor da correla√ß√£o estar√° pr√≥ximo de zero, demonstrando a propriedade de n√£o correla√ß√£o.

√â importante notar que, em alguns casos, uma condi√ß√£o mais forte √© imposta: a independ√™ncia estat√≠stica entre os termos Œµ‚Çú [^48].  Esta condi√ß√£o implica que n√£o h√° *nenhum* tipo de depend√™ncia (linear ou n√£o linear) entre os termos. No entanto, a defini√ß√£o padr√£o de white noise requer apenas que os termos sejam n√£o correlacionados [^48].  Se, al√©m das propriedades acima, cada Œµ‚Çú seguir uma distribui√ß√£o normal (gaussiana) com m√©dia zero e vari√¢ncia œÉ¬≤, o processo √© denominado **Gaussian white noise** [^48].

> üí° **Exemplo Num√©rico:** Para verificar se uma s√©rie de white noise √© gaussiana, podemos plotar um histograma e verificar se ele se aproxima de uma distribui√ß√£o normal.
```python
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

np.random.seed(42)
epsilon = np.random.normal(0, 1, 1000)

plt.figure(figsize=(10, 6))
plt.hist(epsilon, bins=30, density=True, alpha=0.6, color='skyblue')

# Sobrep√µe a curva normal te√≥rica
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = stats.norm.pdf(x, 0, 1)
plt.plot(x, p, 'k', linewidth=2)

plt.title("Histograma do White Noise Gaussiano")
plt.xlabel("Valor")
plt.ylabel("Densidade")
plt.grid(True)
plt.show()
```

Um processo que satisfaz as condi√ß√µes [3.2.1] a [3.2.3] √© descrito como um processo de white noise [^48]. Em algumas situa√ß√µes, podemos substituir [3.2.3] por uma condi√ß√£o um pouco mais forte, de que os Œµ‚Çú s√£o independentes ao longo do tempo [^48]:

$$\varepsilon_t, \varepsilon_\tau \text{ independentes para } t \neq \tau$$ [^48]

√â importante notar que [3.2.4] implica [3.2.3], mas [3.2.3] n√£o implica [3.2.4] [^48]. Um processo que satisfaz [3.2.1] a [3.2.4] √© chamado de processo de white noise independente [^48]. Finalmente, se [3.2.1] a [3.2.4] valem, juntamente com

$$\varepsilon_t \sim N(0, \sigma^2),$$ [^48]

ent√£o temos o processo de white noise gaussiano [^48].

**Observa√ß√£o:** A condi√ß√£o de vari√¢ncia constante (propriedade 2) tamb√©m implica que a vari√¢ncia √© finita.  Processos com m√©dia zero e n√£o correlacionados, mas com vari√¢ncia n√£o constante, s√£o por vezes referidos como *heteroscedastic white noise*.

**Teorema 1**
Se {Œµ‚Çú} √© um processo de white noise com m√©dia zero e vari√¢ncia œÉ¬≤, ent√£o a fun√ß√£o de autocorrela√ß√£o (ACF) œÅ(h) √© zero para todos os lags h ‚â† 0.

*Prova:*
A fun√ß√£o de autocorrela√ß√£o √© definida como:
$$
\rho(h) = \frac{Cov(\varepsilon_t, \varepsilon_{t+h})}{\sqrt{Var(\varepsilon_t)Var(\varepsilon_{t+h})}}
$$
Para h ‚â† 0,  $Cov(\varepsilon_t, \varepsilon_{t+h}) = E[(\varepsilon_t - E[\varepsilon_t])(\varepsilon_{t+h} - E[\varepsilon_{t+h}])] = E[\varepsilon_t \varepsilon_{t+h}] = 0$, devido √† n√£o correla√ß√£o (propriedade 3).
Portanto, œÅ(h) = 0 para h ‚â† 0.

**Prova Detalhada:**
I. Defini√ß√£o da autocorrela√ß√£o:
$$\rho(h) = \frac{Cov(\varepsilon_t, \varepsilon_{t+h})}{\sqrt{Var(\varepsilon_t)Var(\varepsilon_{t+h})}}$$

II. Expans√£o da covari√¢ncia:
$$Cov(\varepsilon_t, \varepsilon_{t+h}) = E[(\varepsilon_t - E[\varepsilon_t])(\varepsilon_{t+h} - E[\varepsilon_{t+h}])]$$

III. Utilizando a propriedade de m√©dia zero, $E[\varepsilon_t] = E[\varepsilon_{t+h}] = 0$:
$$Cov(\varepsilon_t, \varepsilon_{t+h}) = E[\varepsilon_t \varepsilon_{t+h}]$$

IV. Pela propriedade de n√£o correla√ß√£o (propriedade 3), para $h \neq 0$:
$$E[\varepsilon_t \varepsilon_{t+h}] = 0$$

V. Substituindo na defini√ß√£o de autocorrela√ß√£o:
$$\rho(h) = \frac{0}{\sqrt{Var(\varepsilon_t)Var(\varepsilon_{t+h})}} = 0$$

VI. Portanto, a fun√ß√£o de autocorrela√ß√£o œÅ(h) √© zero para todos os lags h ‚â† 0. ‚ñ†

> üí° **Exemplo Num√©rico:** Podemos gerar uma s√©rie de white noise e calcular sua fun√ß√£o de autocorrela√ß√£o (ACF). Esperamos ver picos apenas no lag 0.



```python
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm

# Gerar white noise
np.random.seed(42)
wn = np.random.randn(100)

# Plotar a ACF
sm.graphics.tsa.plot_acf(wn, lags=20)
plt.title("Fun√ß√£o de Autocorrela√ß√£o (ACF) do White Noise")
plt.xlabel("Lag")
plt.ylabel("Autocorrela√ß√£o")
plt.show()

```

O gr√°fico mostrar√° um pico significativo apenas no lag 0, confirmando o Teorema 1.

**Exemplo:**
Um exemplo simples de white noise √© uma sequ√™ncia de n√∫meros aleat√≥rios gerados por um computador, onde cada n√∫mero √© independente dos outros e tem uma m√©dia de zero e uma vari√¢ncia constante.

**Import√¢ncia:**
O white noise √© importante por v√°rias raz√µes:

*   **Base para modelos:** Muitos modelos de s√©ries temporais, como modelos ARMA, decomp√µem um processo complexo em uma combina√ß√£o de white noise e um componente determin√≠stico. O white noise representa a parte imprevis√≠vel ou inova√ß√£o do processo.
*   **Teste de res√≠duos:** Ap√≥s ajustar um modelo a uma s√©rie temporal, os res√≠duos (a diferen√ßa entre os valores observados e os valores previstos) devem se comportar como white noise se o modelo for adequado. Testes estat√≠sticos s√£o frequentemente usados para verificar se os res√≠duos s√£o realmente n√£o correlacionados.

    > üí° **Exemplo Num√©rico:** Suponha que ajustamos um modelo AR(1) a uma s√©rie temporal e obtemos os res√≠duos. Podemos aplicar o teste de Ljung-Box para verificar se os res√≠duos s√£o white noise. Se o p-valor do teste for maior que um n√≠vel de signific√¢ncia (por exemplo, 0.05), falhamos em rejeitar a hip√≥tese nula de que os res√≠duos s√£o n√£o correlacionados, sugerindo que o modelo √© adequado.
    ```python
    import numpy as np
    import statsmodels.api as sm
    from statsmodels.stats.diagnostic import acorr_ljungbox

    # Simula dados AR(1)
    np.random.seed(42)
    n = 100
    phi = 0.7
    errors = np.random.normal(0, 1, n)
    y = np.zeros(n)
    y[0] = errors[0]
    for t in range(1, n):
        y[t] = phi * y[t-1] + errors[t]

    # Ajusta um modelo AR(1) (sabemos que √© o modelo correto)
    model = sm.tsa.AutoReg(y, lags=1)
    results = model.fit()
    residuos = results.resid

    # Aplica o teste de Ljung-Box
    lbvalue, pvalue = acorr_ljungbox(residuos, lags=[10])
    print(f"P-valor do teste de Ljung-Box: {pvalue[0]:.4f}")
    ```
    Se o p-valor for maior que 0.05, os res√≠duos s√£o considerados white noise, indicando um bom ajuste do modelo.
*   **Simula√ß√£o:** O white noise √© usado para simular dados de s√©ries temporais e avaliar o desempenho de diferentes m√©todos de an√°lise.

Al√©m disso, o conceito de *inova√ß√£o* em um processo estoc√°stico est√° intimamente ligado ao white noise.  A inova√ß√£o em um instante *t* representa a nova informa√ß√£o trazida pelo processo nesse instante, que n√£o podia ser prevista com base no passado.

**Defini√ß√£o:** Dado um processo estoc√°stico {X‚Çú}, a inova√ß√£o em *t* √© definida como:
$$
\nu_t = X_t - E[X_t | X_{t-1}, X_{t-2}, \ldots]
$$
Ou seja, a inova√ß√£o √© a diferen√ßa entre o valor atual do processo e a sua previs√£o √≥tima baseada em todo o hist√≥rico passado.

**Teorema 1.1** Se {X‚Çú} √© um processo linear estacion√°rio, ent√£o a sequ√™ncia de inova√ß√µes {ŒΩ‚Çú} √© um processo de white noise.

*Prova (Esbo√ßo):* Para processos lineares estacion√°rios, a previs√£o √≥tima E[X‚Çú | X‚Çú‚Çã‚ÇÅ, X‚Çú‚Çã‚ÇÇ, ...] √© uma fun√ß√£o linear do passado.  As inova√ß√µes, por constru√ß√£o, s√£o n√£o correlacionadas com o passado e t√™m m√©dia zero.  Sob condi√ß√µes de estacionariedade, a vari√¢ncia das inova√ß√µes tamb√©m √© constante. Portanto, {ŒΩ‚Çú} satisfaz as propriedades de white noise.

**Prova Detalhada:**
Para um processo linear estacion√°rio {X‚Çú}, podemos expressar X‚Çú como:
$$X_t = \sum_{i=0}^{\infty} \psi_i \varepsilon_{t-i}$$
onde {Œµ‚Çú} √© um processo de white noise com m√©dia zero e vari√¢ncia œÉ¬≤, e os œà·µ¢ s√£o coeficientes.

I. Defini√ß√£o da Inova√ß√£o:
$$\nu_t = X_t - E[X_t | X_{t-1}, X_{t-2}, \ldots]$$

II. Express√£o da Expectativa Condicional para Processos Lineares:
Para processos lineares estacion√°rios, a melhor previs√£o linear √© dada por:
$$E[X_t | X_{t-1}, X_{t-2}, \ldots] = \sum_{i=1}^{\infty} \psi_i \varepsilon_{t-i}$$

III. Substituindo na Defini√ß√£o da Inova√ß√£o:
$$\nu_t = \sum_{i=0}^{\infty} \psi_i \varepsilon_{t-i} - \sum_{i=1}^{\infty} \psi_i \varepsilon_{t-i} = \psi_0 \varepsilon_t$$
Como convencionalmente $\psi_0 = 1$:
$$\nu_t = \varepsilon_t$$

IV. Conclus√£o:
Como ŒΩ‚Çú = Œµ‚Çú e {Œµ‚Çú} √© um processo de white noise, a sequ√™ncia de inova√ß√µes {ŒΩ‚Çú} √© tamb√©m um processo de white noise. ‚ñ†

**Corol√°rio 1.1** Para um processo linear gaussiano, a sequ√™ncia de inova√ß√µes {ŒΩ‚Çú} √© um processo de white noise gaussiano.

### Conclus√£o

O conceito de white noise √© fundamental na an√°lise de s√©ries temporais. Suas propriedades de m√©dia zero, vari√¢ncia constante e n√£o correla√ß√£o fornecem uma base para modelar e compreender processos estoc√°sticos mais complexos. Compreender as nuances do white noise, incluindo a distin√ß√£o entre n√£o correla√ß√£o e independ√™ncia, √© essencial para a aplica√ß√£o correta de t√©cnicas de an√°lise de s√©ries temporais.

### Refer√™ncias

[^47]: P√°gina 47
[^48]: P√°gina 48
<!-- END -->