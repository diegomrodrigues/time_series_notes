## White Noise Gaussiano: Normalidade e suas Implica√ß√µes

### Introdu√ß√£o

Em continuidade √† discuss√£o sobre **white noise** [^47] e **white noise independente** [^49], este cap√≠tulo foca no **white noise gaussiano**. Exploraremos como a imposi√ß√£o da distribui√ß√£o normal aos elementos do white noise simplifica a an√°lise e quais propriedades emergem dessa condi√ß√£o adicional.

### Conceitos Fundamentais

Como estabelecido anteriormente, um processo de **white noise** {Œµ‚Çú} possui as seguintes propriedades [^47]:

1.  $E(\varepsilon_t) = 0$
2.  $E(\varepsilon_t^2) = \sigma^2$
3.  $E(\varepsilon_t \varepsilon_\tau) = 0 \quad \text{para } t \neq \tau$

O **white noise gaussiano** √© um caso especial onde, al√©m dessas propriedades, assumimos que cada Œµ‚Çú segue uma distribui√ß√£o normal (gaussiana) com m√©dia zero e vari√¢ncia œÉ¬≤ [^48]:

$$ \varepsilon_t \sim N(0, \sigma^2) $$

Isto significa que a fun√ß√£o densidade de probabilidade (PDF) de cada Œµ‚Çú √© dada por:

$$
f(\varepsilon_t) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\varepsilon_t^2}{2\sigma^2}\right)
$$

> üí° **Exemplo Num√©rico:** Se Œµ‚Çú √© um white noise gaussiano com œÉ¬≤ = 4, ent√£o a probabilidade de observar Œµ‚Çú = 1 √© dada por:

```python
import numpy as np
import scipy.stats as stats

sigma = 2  # Desvio padr√£o, raiz quadrada da vari√¢ncia
x = 1      # Valor a ser avaliado

probabilidade = stats.norm.pdf(x, 0, sigma)
print(f"Probabilidade de observar Œµt = 1: {probabilidade:.4f}")
```

**Propriedades Adicionais do White Noise Gaussiano:**

1.  **Independ√™ncia e N√£o Correla√ß√£o Equivalentes:** Para processos gaussianos, a n√£o correla√ß√£o implica independ√™ncia [^49]. Portanto, um white noise gaussiano √© sempre um white noise independente.

    *Prova (Revis√£o):* Vimos na se√ß√£o anterior que se {Œµ‚Çú} √© um processo gaussiano com $E[\varepsilon_t] = 0$ e $E[\varepsilon_t \varepsilon_\tau] = 0$ para $t \neq \tau$, ent√£o a distribui√ß√£o conjunta de qualquer conjunto finito de vari√°veis $\varepsilon_{t_1}, \varepsilon_{t_2}, ..., \varepsilon_{t_n}$ √© uma distribui√ß√£o normal multivariada. Uma matriz de covari√¢ncia diagonal (implicada pela n√£o correla√ß√£o) implica independ√™ncia para uma distribui√ß√£o normal multivariada. Portanto, {Œµ‚Çú} √© um white noise independente [^49].

    *Prova Detalhada:*
    Para provar que n√£o correla√ß√£o implica independ√™ncia para vari√°veis Gaussianas, precisamos mostrar que a fun√ß√£o de densidade de probabilidade conjunta (PDF) pode ser fatorada no produto das PDFs marginais.

    I. Seja $X$ e $Y$ duas vari√°veis aleat√≥rias Gaussianas com m√©dia zero. A fun√ß√£o de densidade de probabilidade conjunta √© dada por:

    $$ f_{X,Y}(x,y) = \frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho^2}} \exp\left(-\frac{1}{2(1-\rho^2)}\left[\frac{x^2}{\sigma_X^2} - \frac{2\rho xy}{\sigma_X\sigma_Y} + \frac{y^2}{\sigma_Y^2}\right]\right) $$

    onde $\rho$ √© o coeficiente de correla√ß√£o entre $X$ e $Y$, $\sigma_X$ e $\sigma_Y$ s√£o os desvios padr√µes de $X$ e $Y$, respectivamente.

    II. Se $X$ e $Y$ s√£o n√£o correlacionadas, ent√£o $\rho = 0$. Substituindo $\rho = 0$ na PDF conjunta, obtemos:

    $$ f_{X,Y}(x,y) = \frac{1}{2\pi\sigma_X\sigma_Y} \exp\left(-\frac{1}{2}\left[\frac{x^2}{\sigma_X^2} + \frac{y^2}{\sigma_Y^2}\right]\right) $$

    III. Podemos reescrever a express√£o acima como o produto de duas PDFs Gaussianas:

    $$ f_{X,Y}(x,y) = \frac{1}{\sqrt{2\pi\sigma_X^2}} \exp\left(-\frac{x^2}{2\sigma_X^2}\right) \cdot \frac{1}{\sqrt{2\pi\sigma_Y^2}} \exp\left(-\frac{y^2}{2\sigma_Y^2}\right) $$

    IV. Reconhecemos cada termo como as PDFs marginais de $X$ e $Y$:

    $$ f_{X,Y}(x,y) = f_X(x) \cdot f_Y(y) $$

    V. Como a PDF conjunta √© o produto das PDFs marginais, $X$ e $Y$ s√£o independentes. Portanto, para vari√°veis aleat√≥rias Gaussianas, n√£o correla√ß√£o implica independ√™ncia. $\blacksquare$

    > üí° **Exemplo Num√©rico:** Considere duas vari√°veis aleat√≥rias Gaussianas, X e Y, ambas com m√©dia 0 e desvio padr√£o 1. Se a covari√¢ncia entre X e Y √© 0, ent√£o X e Y s√£o independentes. Podemos verificar isso gerando amostras aleat√≥rias e calculando a correla√ß√£o:

```python
import numpy as np

np.random.seed(42)
n = 1000
X = np.random.normal(0, 1, n)
Y = np.random.normal(0, 1, n)

correlation = np.corrcoef(X, Y)[0, 1]
print(f"Correla√ß√£o entre X e Y: {correlation:.4f}")

# Diagrama Mermaid mostrando a independ√™ncia
print("""
```mermaid
graph LR
    A[X] -->|N√£o Correla√ß√£o| B(Independ√™ncia)
    Y -->|N√£o Correla√ß√£o| B
```
""")
```

2.  **Transforma√ß√µes Lineares Preservam a Gaussianidade:** Uma transforma√ß√£o linear de um white noise gaussiano resulta em outro processo gaussiano [^49].  Este resultado √© crucial para a an√°lise de modelos ARMA, onde o processo √© expresso como uma combina√ß√£o linear de white noise gaussiano.

    *Prova (Revis√£o):* Vimos anteriormente que se {Œµ‚Çú} √© Gaussian white noise independente, ent√£o qualquer transforma√ß√£o linear de {Œµ‚Çú} tamb√©m √© um processo gaussiano [^49].

    *Prova Detalhada:*
    Para provar que transforma√ß√µes lineares preservam a Gaussianidade, podemos usar a propriedade de que combina√ß√µes lineares de vari√°veis aleat√≥rias Gaussianas s√£o tamb√©m Gaussianas.

    I. Seja $X_1, X_2, ..., X_n$ vari√°veis aleat√≥rias Gaussianas independentes com m√©dias $\mu_i$ e vari√¢ncias $\sigma_i^2$, ou seja, $X_i \sim N(\mu_i, \sigma_i^2)$ para $i = 1, 2, ..., n$.

    II. Considere uma transforma√ß√£o linear dessas vari√°veis:

    $$ Y = a_1X_1 + a_2X_2 + \ldots + a_nX_n + b $$

    onde $a_1, a_2, ..., a_n$ e $b$ s√£o constantes.

    III. A m√©dia de $Y$ √© dada por:

    $$ E[Y] = E[a_1X_1 + a_2X_2 + \ldots + a_nX_n + b] = a_1\mu_1 + a_2\mu_2 + \ldots + a_n\mu_n + b $$

    IV. A vari√¢ncia de $Y$ √© dada por:

    $$ Var[Y] = Var[a_1X_1 + a_2X_2 + \ldots + a_nX_n + b] = a_1^2\sigma_1^2 + a_2^2\sigma_2^2 + \ldots + a_n^2\sigma_n^2 $$

    pois as vari√°veis $X_i$ s√£o independentes.

    V. Como $Y$ √© uma combina√ß√£o linear de vari√°veis aleat√≥rias Gaussianas independentes, $Y$ tamb√©m segue uma distribui√ß√£o normal:

    $$ Y \sim N(E[Y], Var[Y]) = N\left(\sum_{i=1}^{n} a_i\mu_i + b, \sum_{i=1}^{n} a_i^2\sigma_i^2\right) $$

    VI. Portanto, qualquer transforma√ß√£o linear de vari√°veis aleat√≥rias Gaussianas resulta em outra vari√°vel aleat√≥ria Gaussiana. $\blacksquare$

    > üí° **Exemplo Num√©rico:** Seja Œµ‚Çú um white noise gaussiano com m√©dia 0 e vari√¢ncia 1. Considere a transforma√ß√£o linear $X_t = 2\varepsilon_t + 3$. Ent√£o, $X_t$ tamb√©m √© uma vari√°vel gaussiana com m√©dia $2*0 + 3 = 3$ e vari√¢ncia $2^2 * 1 = 4$. Podemos verificar isso simulando os dados:

![Generated plot](./../images/plot_16.png)

3.  **Distribui√ß√£o Conjunta:** A distribui√ß√£o conjunta de um conjunto de vari√°veis {Œµ‚Çú‚ÇÅ, Œµ‚Çú‚ÇÇ, ..., Œµ‚Çú‚Çô} de um white noise gaussiano √© uma distribui√ß√£o normal multivariada com m√©dia zero e matriz de covari√¢ncia diagonal, onde os elementos diagonais s√£o todos iguais a œÉ¬≤.

    $$ f(\varepsilon_{t_1}, \varepsilon_{t_2}, ..., \varepsilon_{t_n}) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\varepsilon_{t_i}^2}{2\sigma^2}\right) $$

    > üí° **Exemplo Num√©rico:** Suponha que temos tr√™s vari√°veis de white noise gaussiano independente: Œµ‚ÇÅ, Œµ‚ÇÇ, e Œµ‚ÇÉ, todas com m√©dia zero e vari√¢ncia 1. A distribui√ß√£o conjunta desses tr√™s pontos √© uma distribui√ß√£o normal multivariada tridimensional com m√©dia zero e matriz de covari√¢ncia identidade. A densidade da probabilidade conjunta em, por exemplo, (1, -1, 0.5), √© dada pelo produto das densidades das probabilidades marginais.

    **Lema 1:** Seja {Œµ‚Çú} um white noise gaussiano. Ent√£o, para qualquer conjunto de √≠ndices distintos $t_1, t_2, ..., t_n$, as vari√°veis aleat√≥rias $\varepsilon_{t_1}, \varepsilon_{t_2}, ..., \varepsilon_{t_n}$ s√£o independentes.

    *Prova:* Pela defini√ß√£o de white noise gaussiano, cada $\varepsilon_t$ √© normalmente distribu√≠do com m√©dia 0 e vari√¢ncia $\sigma^2$. Al√©m disso, para $t \neq \tau$, $E[\varepsilon_t \varepsilon_\tau] = 0$. Como {Œµ‚Çú} √© gaussiano, a n√£o correla√ß√£o implica independ√™ncia. Portanto, $\varepsilon_{t_1}, \varepsilon_{t_2}, ..., \varepsilon_{t_n}$ s√£o independentes. $\blacksquare$

    > üí° **Exemplo Num√©rico:** Seja Œµ‚ÇÅ, Œµ‚ÇÇ, Œµ‚ÇÉ um white noise gaussiano com œÉ¬≤ = 1. Calcule a probabilidade conjunta de observar Œµ‚ÇÅ = 0.5, Œµ‚ÇÇ = -0.2, Œµ‚ÇÉ = 1.0.
```python
import numpy as np
import scipy.stats as stats

sigma = 1
e1 = 0.5
e2 = -0.2
e3 = 1.0

prob_e1 = stats.norm.pdf(e1, 0, sigma)
prob_e2 = stats.norm.pdf(e2, 0, sigma)
prob_e3 = stats.norm.pdf(e3, 0, sigma)

prob_conjunta = prob_e1 * prob_e2 * prob_e3

print(f"Probabilidade conjunta: {prob_conjunta:.4f}")
```

**Import√¢ncia do White Noise Gaussiano:**

1.  **Simplifica√ß√£o da An√°lise:** A suposi√ß√£o de gaussianidade simplifica significativamente a an√°lise de muitos modelos de s√©ries temporais. Por exemplo, em modelos lineares, a distribui√ß√£o gaussiana dos res√≠duos permite o uso de testes estat√≠sticos bem conhecidos e a deriva√ß√£o de propriedades assint√≥ticas.

    > üí° **Exemplo Num√©rico:** Em um modelo de regress√£o linear, se assumirmos que os res√≠duos s√£o white noise gaussiano, podemos usar um teste t para determinar a signific√¢ncia estat√≠stica dos coeficientes.

2.  **Justificativa para M√©todos de Estima√ß√£o:** A suposi√ß√£o de gaussianidade muitas vezes justifica o uso de m√©todos de estima√ß√£o como a m√°xima verossimilhan√ßa (Maximum Likelihood Estimation - MLE). Para um white noise gaussiano, a fun√ß√£o de verossimilhan√ßa tem uma forma anal√≠tica simples, facilitando a otimiza√ß√£o.

    Para ilustrar, considere um amostra de tamanho $n$ de um white noise gaussiano {Œµ‚Çú}. A fun√ß√£o de verossimilhan√ßa √© dada por:

    $$
    L(\sigma^2; \varepsilon_1, ..., \varepsilon_n) = \prod_{t=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\varepsilon_t^2}{2\sigma^2}\right)
    $$

    O log-verossimilhan√ßa √©:

    $$
    \log L(\sigma^2; \varepsilon_1, ..., \varepsilon_n) = -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=1}^{n} \varepsilon_t^2
    $$

    Maximizar o log-verossimilhan√ßa em rela√ß√£o a $\sigma^2$ nos d√° o estimador de m√°xima verossimilhan√ßa:

    $$
    \hat{\sigma}^2_{MLE} = \frac{1}{n} \sum_{t=1}^{n} \varepsilon_t^2
    $$

    *Prova Detalhada:*
    Para derivar o estimador de m√°xima verossimilhan√ßa (MLE) para $\sigma^2$, seguimos estes passos:

    I. A fun√ß√£o de log-verossimilhan√ßa √© dada por:
    $$
    \log L(\sigma^2; \varepsilon_1, ..., \varepsilon_n) = -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=1}^{n} \varepsilon_t^2
    $$

    II. Para encontrar o estimador de m√°xima verossimilhan√ßa, derivamos a fun√ß√£o de log-verossimilhan√ßa em rela√ß√£o a $\sigma^2$ e igualamos a zero:
    $$
    \frac{\partial \log L}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_{t=1}^{n} \varepsilon_t^2
    $$

    III. Igualamos a derivada a zero e resolvemos para $\sigma^2$:
    $$
    -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_{t=1}^{n} \varepsilon_t^2 = 0
    $$
    $$
    \frac{1}{2(\sigma^2)^2} \sum_{t=1}^{n} \varepsilon_t^2 = \frac{n}{2\sigma^2}
    $$
    $$
    \sum_{t=1}^{n} \varepsilon_t^2 = n\sigma^2
    $$

    IV. Resolvemos para $\hat{\sigma}^2_{MLE}$:
    $$
    \hat{\sigma}^2_{MLE} = \frac{1}{n} \sum_{t=1}^{n} \varepsilon_t^2
    $$

    V. Portanto, o estimador de m√°xima verossimilhan√ßa para a vari√¢ncia $\sigma^2$ √© a m√©dia amostral dos quadrados dos erros. $\blacksquare$

    > üí° **Exemplo Num√©rico:** Suponha que temos uma amostra de 10 valores de um white noise gaussiano: Œµ = [0.2, -0.5, 1.1, -0.8, 0.6, -0.3, 0.9, -0.1, 0.4, -0.7]. Calcule o estimador de m√°xima verossimilhan√ßa para œÉ¬≤.

```python
import numpy as np

epsilon = np.array([0.2, -0.5, 1.1, -0.8, 0.6, -0.3, 0.9, -0.1, 0.4, -0.7])
n = len(epsilon)

sigma_squared_mle = np.sum(epsilon**2) / n

print(f"Estimador de m√°xima verossimilhan√ßa para œÉ¬≤: {sigma_squared_mle:.4f}")
```

3.  **Base para Simula√ß√£o:** A facilidade de gerar n√∫meros aleat√≥rios a partir de uma distribui√ß√£o normal torna o white noise gaussiano uma ferramenta fundamental para simular dados de s√©ries temporais e avaliar o desempenho de diferentes m√©todos de an√°lise.

    > üí° **Exemplo Num√©rico (Simula√ß√£o):** Podemos gerar uma s√©rie de white noise gaussiano no Python como segue:

```python
import numpy as np

np.random.seed(42)  # Para reprodutibilidade
n = 100
sigma = 2
epsilon = np.random.normal(0, sigma, n)

print("Primeiros 10 valores do White Noise Gaussiano:", epsilon[:10])
```

Isto cria um array NumPy `epsilon` contendo 100 valores aleat√≥rios amostrados de uma distribui√ß√£o normal com m√©dia 0 e desvio padr√£o 2.

**Teorema 2:** Seja {X‚Çú} um processo ARMA(p,q) dado por:

$$
X_t = c + \sum_{i=1}^{p} \phi_i X_{t-i} + \varepsilon_t + \sum_{j=1}^{q} \theta_j \varepsilon_{t-j}
$$

onde {Œµ‚Çú} √© um white noise gaussiano. Ent√£o, condicionado aos valores iniciais $X_1, ..., X_p$, a distribui√ß√£o condicional de $X_t$ √© normal.

*Prova (Esbo√ßo):* Dado que {Œµ‚Çú} √© white noise gaussiano, cada Œµ‚Çú √© normalmente distribu√≠do e independente dos outros. Para $t > p$, $X_t$ √© uma combina√ß√£o linear de vari√°veis normais (as inova√ß√µes Œµ‚Çú) e dos valores passados $X_{t-1}, ..., X_{t-p}$. Dado $X_{t-1}, ..., X_{t-p}$, $X_t$ √© uma combina√ß√£o linear de vari√°veis normais, e portanto √© normalmente distribu√≠do.

*Prova Detalhada:*

I. Processo ARMA(p,q)
O processo ARMA(p,q) √© definido como:
$$X_t = c + \sum_{i=1}^{p} \phi_i X_{t-i} + \varepsilon_t + \sum_{j=1}^{q} \theta_j \varepsilon_{t-j}$$
onde {Œµ‚Çú} √© white noise gaussiano.

II. A Express√£o Condicional
Dado $X_{t-1}, ..., X_{t-p}$ e assumindo $t > max(p, q)$, a express√£o se torna:
$$X_t = c + \sum_{i=1}^{p} \phi_i X_{t-i} + \varepsilon_t + \sum_{j=1}^{q} \theta_j \varepsilon_{t-j}$$

III. A Linearidade e a Distribui√ß√£o Normal
Como {Œµ‚Çú} √© white noise gaussiano, cada Œµ‚Çú √© normalmente distribu√≠do.  Sob a premissa de termos condicionado em valores iniciais $X_{t-1}, ..., X_{t-p}$ ent√£o $X_t$ √© expresso como uma combina√ß√£o linear de vari√°veis normais (os $\varepsilon_{t-j}$) e constantes ($c$ e os $\phi_i X_{t-i}$), e portanto $X_t$ √© normalmente distribu√≠do.

IV. Formaliza√ß√£o da Distribui√ß√£o Normal Condicional
A distribui√ß√£o condicional de $X_t$, dados $X_{t-1}, ..., X_{t-p}$, √© normal:
$$X_t | X_{t-1}, ..., X_{t-p} \sim N(\mu_t, \sigma_t^2)$$
onde $\mu_t = c + \sum_{i=1}^{p} \phi_i X_{t-i} + \sum_{j=1}^{q} \theta_j E[\varepsilon_{t-j}]$ e $\sigma_t^2 = Var[\varepsilon_t] = \sigma^2$
V. Conclus√£o
Assim, para o processo ARMA(p,q), a distribui√ß√£o condicional de $X_t$, dados os valores anteriores, segue uma distribui√ß√£o normal. $\blacksquare$

    > üí° **Exemplo Num√©rico:** Considere um processo AR(1): $X_t = 0.5X_{t-1} + \varepsilon_t$, onde Œµ‚Çú √© white noise gaussiano com m√©dia 0 e vari√¢ncia 1. Se $X_0 = 2$, ent√£o a distribui√ß√£o condicional de $X_1$ √© normal com m√©dia $0.5 * 2 = 1$ e vari√¢ncia 1. Podemos simular e verificar:

![Generated plot](./../images/plot_17.png)

**Teorema 2.1:** Se {X‚Çú} √© um processo ARMA(p,q) causal com {Œµ‚Çú} sendo um white noise gaussiano com vari√¢ncia $\sigma^2$, ent√£o o processo {X‚Çú} √© estritamente estacion√°rio.

*Prova (Esbo√ßo):* Um processo ARMA(p,q) causal pode ser escrito como uma combina√ß√£o linear infinita de white noise gaussiano. Como o white noise gaussiano √© estritamente estacion√°rio e combina√ß√µes lineares de processos estritamente estacion√°rios s√£o estritamente estacion√°rias, o resultado segue.

*Prova Detalhada:*

I. Representa√ß√£o Causal
Um processo ARMA(p,q) causal pode ser expresso como:
$$X_t = \sum_{j=0}^{\infty} \psi_j \varepsilon_{t-j}$$
onde os coeficientes $\psi_j$ satisfazem $\sum_{j=0}^{\infty} |\psi_j| < \infty$. Esta representa√ß√£o existe porque o processo √© causal.

II. Estacionaridade Estrita do White Noise Gaussiano
O white noise gaussiano {Œµ‚Çú} √© estritamente estacion√°rio, pois sua distribui√ß√£o √© invariante ao tempo (i.e., $\varepsilon_t$ tem a mesma distribui√ß√£o para todo $t$).

III. Combina√ß√£o Linear e Estacionaridade Estrita
Como {X‚Çú} √© uma combina√ß√£o linear de um processo estritamente estacion√°rio (white noise gaussiano), {X‚Çú} tamb√©m √© estritamente estacion√°rio. Isso decorre do fato de que a distribui√ß√£o de qualquer conjunto finito de vari√°veis $X_{t_1}, X_{t_2}, ..., X_{t_n}$ √© a mesma que a distribui√ß√£o de $X_{t_1+h}, X_{t_2+h}, ..., X_{t_n+h}$ para qualquer $h$, pois elas s√£o ambas determinadas pela distribui√ß√£o do white noise gaussiano, que √© invariante ao tempo.

IV. Conclus√£o
Portanto, se {X‚Çú} √© um processo ARMA(p,q) causal com {Œµ‚Çú} sendo um white noise gaussiano, ent√£o {X‚Çú} √© estritamente estacion√°rio. $\blacksquare$

**Limita√ß√µes:**

√â importante notar que a suposi√ß√£o de gaussianidade pode n√£o ser apropriada para todas as s√©ries temporais. Em algumas situa√ß√µes, outras distribui√ß√µes (como distribui√ß√µes t de Student ou distribui√ß√µes mistas) podem fornecer um ajuste melhor aos dados.  A escolha da distribui√ß√£o apropriada deve ser baseada na an√°lise dos dados e no conhecimento do processo gerador de dados.

### Conclus√£o

O **white noise gaussiano** √© um conceito fundamental na an√°lise de s√©ries temporais, combinando as propriedades de white noise com a conveni√™ncia anal√≠tica da distribui√ß√£o normal. A suposi√ß√£o de gaussianidade simplifica a an√°lise, justifica m√©todos de estima√ß√£o comuns e facilita a simula√ß√£o de dados. No entanto, √© crucial reconhecer as limita√ß√µes dessa suposi√ß√£o e considerar distribui√ß√µes alternativas quando apropriado.
<!-- END -->