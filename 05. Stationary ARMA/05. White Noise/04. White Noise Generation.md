## Gera√ß√£o e Teste de Sequ√™ncias de White Noise: M√©todos e Valida√ß√£o

### Introdu√ß√£o

A gera√ß√£o de sequ√™ncias de **white noise** √© uma tarefa essencial em simula√ß√µes, testes de algoritmos e modelagem de s√©ries temporais. Este cap√≠tulo explora os m√©todos computacionais para gerar tais sequ√™ncias e as t√©cnicas estat√≠sticas para validar sua conformidade com as propriedades desejadas. Em particular, vamos explorar em detalhe a gera√ß√£o e valida√ß√£o de **white noise gaussiano**, que como vimos anteriormente [^50], combina a conveni√™ncia do processo de white noise com as propriedades da distribui√ß√£o normal.

### Gera√ß√£o de Sequ√™ncias de White Noise

A gera√ß√£o de sequ√™ncias de white noise no computador √© realizada atrav√©s de **geradores de n√∫meros pseudo-aleat√≥rios (PRNGs)**. Estes algoritmos produzem sequ√™ncias determin√≠sticas que *aproximam* as propriedades de n√∫meros aleat√≥rios verdadeiros. A qualidade de um PRNG √© crucial para garantir que a sequ√™ncia gerada se comporte como white noise.

**Tipos de PRNGs:**

1.  **Linear Congruential Generators (LCGs):** S√£o PRNGs simples e r√°pidos, mas podem apresentar padr√µes percept√≠veis, especialmente em dimens√µes mais altas. A sequ√™ncia √© gerada pela f√≥rmula:

    $$ X_{n+1} = (aX_n + c) \mod m $$

    onde *a*, *c* e *m* s√£o par√¢metros inteiros. LCGs s√£o geralmente desaconselhados para aplica√ß√µes que exigem alta qualidade de aleatoriedade.

    > üí° **Exemplo Num√©rico (LCG):** Para ilustrar o funcionamento de um LCG, implementaremos um exemplo simples em Python.

```python
def lcg(seed, a, c, m, n):
    """
    Implementa√ß√£o de um Linear Congruential Generator.

    Args:
        seed: Valor inicial da sequ√™ncia.
        a, c, m: Par√¢metros do LCG.
        n: N√∫mero de valores a serem gerados.

    Returns:
        Uma lista de n√∫meros pseudo-aleat√≥rios no intervalo [0, 1).
    """
    sequence = []
    X = seed
    for _ in range(n):
        X = (a * X + c) % m
        sequence.append(X / m)  # Normaliza para o intervalo [0, 1)
    return sequence

# Par√¢metros do LCG
seed = 42  # Semente inicial
a = 1664525
c = 1013904223
m = 2**32
n = 100  # N√∫mero de valores a gerar

# Gera a sequ√™ncia
random_numbers = lcg(seed, a, c, m, n)

# Imprime os primeiros 10 n√∫meros
print("Primeiros 10 n√∫meros gerados pelo LCG:", random_numbers[:10])
```

    Este c√≥digo implementa um LCG e gera 100 n√∫meros pseudo-aleat√≥rios. Os n√∫meros s√£o normalizados para o intervalo [0, 1).

    > üí° **Exemplo Num√©rico (LCG com visualiza√ß√£o):** Podemos visualizar a sa√≠da de um LCG para detectar padr√µes. Vamos gerar uma sequ√™ncia maior e plot√°-la.

```python
import matplotlib.pyplot as plt

# Gera uma sequ√™ncia maior (1000 n√∫meros)
random_numbers = lcg(seed, a, c, m, 1000)

# Plota a sequ√™ncia
plt.figure(figsize=(10, 6))
plt.plot(random_numbers)
plt.title("Sequ√™ncia Gerada por um LCG")
plt.xlabel("√çndice")
plt.ylabel("Valor")
plt.grid(True)
plt.show()

```

    Este c√≥digo gera 1000 n√∫meros com o LCG e os plota. A visualiza√ß√£o pode revelar padr√µes, especialmente se os par√¢metros do LCG n√£o forem bem escolhidos.

2.  **Mersenne Twister:** √â um PRNG amplamente utilizado, conhecido por seu longo per√≠odo (2¬π‚Åπ‚Åπ¬≥‚Å∑ - 1) e boas propriedades estat√≠sticas. √â o PRNG padr√£o em muitas linguagens de programa√ß√£o, incluindo Python (m√≥dulo `random`).

    > üí° **Exemplo Num√©rico (Mersenne Twister):** Demonstraremos o uso do Mersenne Twister para gerar n√∫meros aleat√≥rios em Python.

```python
import random

random.seed(42)  # Inicializa o gerador com uma semente
random_numbers = [random.random() for _ in range(100)]

print("Primeiros 10 n√∫meros gerados pelo Mersenne Twister:", random_numbers[:10])
```

    > üí° **Exemplo Num√©rico (Mersenne Twister com distribui√ß√£o):** Podemos verificar a distribui√ß√£o dos n√∫meros gerados pelo Mersenne Twister usando um histograma.

```python
import matplotlib.pyplot as plt

# Gera 1000 n√∫meros com o Mersenne Twister
random_numbers = [random.random() for _ in range(1000)]

# Plota o histograma
plt.figure(figsize=(10, 6))
plt.hist(random_numbers, bins=30, density=True)
plt.title("Distribui√ß√£o dos N√∫meros Gerados pelo Mersenne Twister")
plt.xlabel("Valor")
plt.ylabel("Densidade")
plt.grid(True)
plt.show()
```

    Este c√≥digo gera 1000 n√∫meros com o Mersenne Twister e mostra um histograma. Esperamos ver uma distribui√ß√£o aproximadamente uniforme.

3.  **Cryptographically Secure PRNGs (CSPRNGs):** S√£o PRNGs projetados para aplica√ß√µes de criptografia, oferecendo alta seguran√ßa e imprevisibilidade. Eles s√£o adequados para simula√ß√µes que exigem alta qualidade de aleatoriedade e resist√™ncia a ataques. Exemplos incluem o Fortuna e o ChaCha20.

**Gera√ß√£o de White Noise Gaussiano:**

Para gerar white noise gaussiano, normalmente utiliza-se um PRNG para gerar n√∫meros aleat√≥rios uniformemente distribu√≠dos no intervalo [0, 1), e ent√£o aplica-se uma transforma√ß√£o para obter uma distribui√ß√£o normal. O m√©todo mais comum √© a **transforma√ß√£o Box-Muller:**

1.  Gere dois n√∫meros aleat√≥rios uniformemente distribu√≠dos, $U_1$ e $U_2$, no intervalo (0, 1).

2.  Calcule:

    $$
    Z_1 = \sqrt{-2 \ln(U_1)} \cos(2\pi U_2)
    $$

    $$
    Z_2 = \sqrt{-2 \ln(U_1)} \sin(2\pi U_2)
    $$

    $Z_1$ e $Z_2$ s√£o vari√°veis aleat√≥rias independentes e normalmente distribu√≠das com m√©dia 0 e vari√¢ncia 1 (N(0, 1)).

    > **Prova da Transforma√ß√£o Box-Muller:**
    >
    > Aqui, vamos demonstrar porque a transforma√ß√£o Box-Muller gera vari√°veis aleat√≥rias com distribui√ß√£o normal padr√£o (m√©dia 0 e vari√¢ncia 1).
    >
    > I. Assumimos que $U_1$ e $U_2$ s√£o vari√°veis aleat√≥rias independentes e uniformemente distribu√≠das em (0, 1).
    >
    > II. Definimos as transforma√ß√µes:
    > $$ Z_1 = \sqrt{-2 \ln(U_1)} \cos(2\pi U_2) $$
    > $$ Z_2 = \sqrt{-2 \ln(U_1)} \sin(2\pi U_2) $$
    >
    > III. Mudamos para coordenadas polares: Seja $R = \sqrt{-2 \ln(U_1)}$ e $\Theta = 2\pi U_2$. Ent√£o, $U_1 = e^{-\frac{R^2}{2}}$ e $U_2 = \frac{\Theta}{2\pi}$.
    >
    > IV. Calculamos o Jacobiano da transforma√ß√£o:
    >  O Jacobiano $J$ para a transforma√ß√£o de $(U_1, U_2)$ para $(Z_1, Z_2)$ √© dado por:
    >  $$ J = \begin{vmatrix} \frac{\partial U_1}{\partial Z_1} & \frac{\partial U_1}{\partial Z_2} \\ \frac{\partial U_2}{\partial Z_1} & \frac{\partial U_2}{\partial Z_2} \end{vmatrix} $$
    >  Primeiro, precisamos expressar $Z_1$ e $Z_2$ em termos de $R$ e $\Theta$:
    >  $$ Z_1 = R \cos(\Theta) $$
    >  $$ Z_2 = R \sin(\Theta) $$
    >  Agora, resolvemos para $R$ e $\Theta$ em termos de $Z_1$ e $Z_2$:
    >  $$ R = \sqrt{Z_1^2 + Z_2^2} $$
    >  $$ \Theta = \arctan\left(\frac{Z_2}{Z_1}\right) $$
    >  Ent√£o, $U_1 = e^{-\frac{Z_1^2 + Z_2^2}{2}}$ e $U_2 = \frac{1}{2\pi}\arctan\left(\frac{Z_2}{Z_1}\right)$.
    >  Agora podemos encontrar as derivadas parciais necess√°rias:
    >  $$ \frac{\partial U_1}{\partial Z_1} = -Z_1 e^{-\frac{Z_1^2 + Z_2^2}{2}} $$
    >  $$ \frac{\partial U_1}{\partial Z_2} = -Z_2 e^{-\frac{Z_1^2 + Z_2^2}{2}} $$
    >  $$ \frac{\partial U_2}{\partial Z_1} = \frac{1}{2\pi} \frac{-\frac{Z_2}{Z_1^2}}{1 + \left(\frac{Z_2}{Z_1}\right)^2} = -\frac{1}{2\pi} \frac{Z_2}{Z_1^2 + Z_2^2} $$
    >  $$ \frac{\partial U_2}{\partial Z_2} = \frac{1}{2\pi} \frac{\frac{1}{Z_1}}{1 + \left(\frac{Z_2}{Z_1}\right)^2} = \frac{1}{2\pi} \frac{Z_1}{Z_1^2 + Z_2^2} $$
    >  Calculamos o Jacobiano:
    >  $$ J = \begin{vmatrix} -Z_1 e^{-\frac{Z_1^2 + Z_2^2}{2}} & -Z_2 e^{-\frac{Z_1^2 + Z_2^2}{2}} \\ -\frac{1}{2\pi} \frac{Z_2}{Z_1^2 + Z_2^2} & \frac{1}{2\pi} \frac{Z_1}{Z_1^2 + Z_2^2} \end{vmatrix} $$
    >  $$ J = -\frac{Z_1^2}{2\pi(Z_1^2 + Z_2^2)} e^{-\frac{Z_1^2 + Z_2^2}{2}} - \frac{Z_2^2}{2\pi(Z_1^2 + Z_2^2)} e^{-\frac{Z_1^2 + Z_2^2}{2}} $$
    >  $$ J = -\frac{Z_1^2 + Z_2^2}{2\pi(Z_1^2 + Z_2^2)} e^{-\frac{Z_1^2 + Z_2^2}{2}} $$
    >  $$ J = -\frac{1}{2\pi} e^{-\frac{Z_1^2 + Z_2^2}{2}} $$
    >
    > V. Determinamos a fun√ß√£o de densidade de probabilidade conjunta de $Z_1$ e $Z_2$:
    >
    > Como $U_1$ e $U_2$ s√£o independentes e uniformemente distribu√≠das em (0, 1), sua densidade conjunta √© $f_{U_1, U_2}(u_1, u_2) = 1$ para $0 < u_1, u_2 < 1$. Usando a transforma√ß√£o de vari√°veis, a densidade conjunta de $Z_1$ e $Z_2$ √© dada por:
    >
    > $$ f_{Z_1, Z_2}(z_1, z_2) = f_{U_1, U_2}(u_1(z_1, z_2), u_2(z_1, z_2)) |J| $$
    >
    > $$ f_{Z_1, Z_2}(z_1, z_2) = 1 \cdot \left| -\frac{1}{2\pi} e^{-\frac{z_1^2 + z_2^2}{2}} \right| = \frac{1}{2\pi} e^{-\frac{z_1^2 + z_2^2}{2}} $$
    >
    > VI. Verificamos que $Z_1$ e $Z_2$ s√£o vari√°veis aleat√≥rias normais padr√£o independentes:
    >
    > A densidade conjunta pode ser fatorada como:
    >
    > $$ f_{Z_1, Z_2}(z_1, z_2) = \frac{1}{\sqrt{2\pi}} e^{-\frac{z_1^2}{2}} \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{z_2^2}{2}} $$
    >
    > Isso mostra que $Z_1$ e $Z_2$ s√£o independentes e cada um segue uma distribui√ß√£o normal padr√£o com m√©dia 0 e vari√¢ncia 1.
    >
    > Portanto, a transforma√ß√£o Box-Muller gera vari√°veis aleat√≥rias com distribui√ß√£o normal padr√£o. ‚ñ†

3.  Para obter um white noise gaussiano com m√©dia 0 e vari√¢ncia œÉ¬≤, multiplique $Z_1$ (ou $Z_2$) por œÉ:

    $$
    \varepsilon_t = \sigma Z_1
    $$

    > üí° **Exemplo Num√©rico (Box-Muller):** Implementaremos a transforma√ß√£o Box-Muller em Python para gerar um white noise gaussiano.

```python
import numpy as np

def box_muller(n, sigma=1):
    """
    Gera uma sequ√™ncia de white noise gaussiano usando a transforma√ß√£o Box-Muller.

    Args:
        n: N√∫mero de valores a gerar.
        sigma: Desvio padr√£o da distribui√ß√£o normal.

    Returns:
        Uma lista de n√∫meros aleat√≥rios seguindo uma distribui√ß√£o normal com m√©dia 0 e desvio padr√£o sigma.
    """
    U1 = np.random.uniform(0, 1, n // 2)
    U2 = np.random.uniform(0, 1, n // 2)

    Z1 = np.sqrt(-2 * np.log(U1)) * np.cos(2 * np.pi * U2)
    Z2 = np.sqrt(-2 * np.log(U1)) * np.sin(2 * np.pi * U2)

    # Se n for √≠mpar, adiciona um valor extra
    if n % 2 != 0:
        U1_last = np.random.uniform(0, 1, 1)
        U2_last = np.random.uniform(0, 1, 1)
        Z1_last = np.sqrt(-2 * np.log(U1_last)) * np.cos(2 * np.pi * U2_last)
        Z1 = np.concatenate((Z1, Z1_last))
    return sigma * np.concatenate((Z1, Z2)) if n % 2 == 0 else sigma * Z1

# Gera 1000 valores de white noise gaussiano com desvio padr√£o 2
white_noise = box_muller(1000, sigma=2)

print("Primeiros 10 valores do white noise gaussiano:", white_noise[:10])
```

Este c√≥digo implementa a transforma√ß√£o Box-Muller para gerar uma sequ√™ncia de white noise gaussiano com m√©dia zero e desvio padr√£o especificado. A fun√ß√£o garante que funcione corretamente mesmo para *n* √≠mpar.

    > üí° **Exemplo Num√©rico (Box-Muller com visualiza√ß√£o):** Podemos visualizar a distribui√ß√£o dos n√∫meros gerados pela transforma√ß√£o de Box-Muller.

```python
import matplotlib.pyplot as plt

# Gera 1000 n√∫meros usando a transforma√ß√£o de Box-Muller
white_noise = box_muller(1000, sigma=2)

# Plota o histograma
plt.figure(figsize=(10, 6))
plt.hist(white_noise, bins=30, density=True)
plt.title("Distribui√ß√£o do White Noise Gaussiano (Box-Muller)")
plt.xlabel("Valor")
plt.ylabel("Densidade")
plt.grid(True)
plt.show()
```

    Este c√≥digo gera 1000 n√∫meros usando a transforma√ß√£o de Box-Muller e exibe um histograma. Esperamos uma distribui√ß√£o aproximadamente normal.

Al√©m da transforma√ß√£o Box-Muller, outra alternativa para gerar n√∫meros aleat√≥rios com distribui√ß√£o normal √© a **transforma√ß√£o de invers√£o**. Esta transforma√ß√£o utiliza a fun√ß√£o de distribui√ß√£o cumulativa inversa (quantile function) da distribui√ß√£o normal.

**Teorema 1** (Transforma√ß√£o de Invers√£o): Seja $U$ uma vari√°vel aleat√≥ria uniformemente distribu√≠da no intervalo (0, 1), e seja $F^{-1}(u)$ a fun√ß√£o de distribui√ß√£o cumulativa inversa da distribui√ß√£o normal padr√£o. Ent√£o, $Z = F^{-1}(U)$ √© uma vari√°vel aleat√≥ria com distribui√ß√£o normal padr√£o.

*Proof Strategy:* A demonstra√ß√£o deste teorema decorre diretamente das propriedades da fun√ß√£o de distribui√ß√£o cumulativa e sua inversa.

> **Prova do Teorema 1:**
>
> I. Seja $U$ uma vari√°vel aleat√≥ria uniformemente distribu√≠da em (0, 1).
>
> II. Seja $F(x)$ a fun√ß√£o de distribui√ß√£o cumulativa (CDF) de uma vari√°vel aleat√≥ria $X$.
>
> III. Seja $F^{-1}(u)$ a fun√ß√£o inversa da CDF, tamb√©m conhecida como fun√ß√£o quantil.
>
> IV. Queremos mostrar que $Z = F^{-1}(U)$ tem a mesma distribui√ß√£o que $X$.
>
> V. Calculamos a CDF de $Z$:
> $$ P(Z \le z) = P(F^{-1}(U) \le z) $$
>
> VI. Aplicamos a fun√ß√£o $F$ a ambos os lados da desigualdade:
> $$ P(F(F^{-1}(U)) \le F(z)) = P(U \le F(z)) $$
>
> VII. Como $F(F^{-1}(u)) = u$, temos:
> $$ P(U \le F(z)) $$
>
> VIII. Como $U$ √© uniformemente distribu√≠da em (0, 1), a probabilidade de $U$ ser menor ou igual a $F(z)$ √© simplesmente $F(z)$:
> $$ P(U \le F(z)) = F(z) $$
>
> IX. Portanto, $P(Z \le z) = F(z)$, o que significa que $Z$ tem a mesma distribui√ß√£o que $X$.
>
> X. No caso em que $X$ √© uma vari√°vel aleat√≥ria normal padr√£o, $Z = F^{-1}(U)$ segue uma distribui√ß√£o normal padr√£o. ‚ñ†

> üí° **Exemplo Num√©rico (Transforma√ß√£o de Invers√£o):** Implementaremos a transforma√ß√£o de invers√£o em Python para gerar white noise gaussiano.

```python
import numpy as np
import scipy.stats as stats

def inverse_transform(n, sigma=1):
    """
    Gera uma sequ√™ncia de white noise gaussiano usando a transforma√ß√£o de invers√£o.

    Args:
        n: N√∫mero de valores a gerar.
        sigma: Desvio padr√£o da distribui√ß√£o normal.

    Returns:
        Uma lista de n√∫meros aleat√≥rios seguindo uma distribui√ß√£o normal com m√©dia 0 e desvio padr√£o sigma.
    """
    U = np.random.uniform(0, 1, n)
    Z = stats.norm.ppf(U) # ppf is the inverse of the CDF
    return sigma * Z

# Gera 1000 valores de white noise gaussiano com desvio padr√£o 2
white_noise = inverse_transform(1000, sigma=2)

print("Primeiros 10 valores do white noise gaussiano:", white_noise[:10])
```

A fun√ß√£o `inverse_transform` utiliza a fun√ß√£o `ppf` (percent point function, que √© a inversa da CDF) da biblioteca `scipy.stats` para gerar os n√∫meros aleat√≥rios normalmente distribu√≠dos.

    > üí° **Exemplo Num√©rico (Transforma√ß√£o de Invers√£o com visualiza√ß√£o):** Podemos comparar os histogramas gerados por Box-Muller e Transforma√ß√£o de Invers√£o.

```python
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt

def inverse_transform(n, sigma=1):
    U = np.random.uniform(0, 1, n)
    Z = stats.norm.ppf(U)
    return sigma * Z

white_noise_inv = inverse_transform(1000, sigma=2)

plt.figure(figsize=(12, 6))
plt.hist(white_noise_inv, bins=30, density=True, alpha=0.5, label='Transforma√ß√£o de Invers√£o')
plt.hist(white_noise, bins=30, density=True, alpha=0.5, label='Box-Muller')
plt.title("Compara√ß√£o de Histogramas: Box-Muller vs. Transforma√ß√£o de Invers√£o")
plt.xlabel("Valor")
plt.ylabel("Frequ√™ncia")
plt.legend(loc='upper right')
plt.grid(True)
plt.show()
```

    Este c√≥digo gera 1000 n√∫meros usando ambos os m√©todos e exibe histogramas sobrepostos para compara√ß√£o visual.

### Valida√ß√£o Estat√≠stica

Ap√≥s gerar uma sequ√™ncia de white noise, √© fundamental validar se ela realmente se comporta como tal. V√°rios testes estat√≠sticos podem ser aplicados para verificar as propriedades de m√©dia zero, vari√¢ncia constante e n√£o correla√ß√£o.

**Testes para M√©dia Zero:**

1.  **Teste t:** Testa se a m√©dia amostral da sequ√™ncia √© significativamente diferente de zero.

    *Hip√≥tese Nula (H‚ÇÄ):* A m√©dia da popula√ß√£o √© zero.
    *Estat√≠stica de Teste:*
    $$ t = \frac{\bar{x} - 0}{s / \sqrt{n}} $$
    onde $\bar{x}$ √© a m√©dia amostral, *s* √© o desvio padr√£o amostral e *n* √© o tamanho da amostra.

    > üí° **Exemplo Num√©rico (Teste t):** Vamos realizar um teste t para verificar se a m√©dia amostral de nossa sequ√™ncia de white noise gaussiano √© estatisticamente igual a zero.

```python
import numpy as np
import scipy.stats as stats

# Gerar white noise gaussiano (usando a fun√ß√£o anterior)
white_noise = box_muller(1000, sigma=2)

# Realizar o teste t
t_statistic, p_value = stats.ttest_1samp(white_noise, 0)

print(f"Estat√≠stica t: {t_statistic:.4f}")
print(f"Valor-p: {p_value:.4f}")
```

    Se o valor-p for maior que um n√≠vel de signific√¢ncia (por exemplo, 0.05), falhamos em rejeitar a hip√≥tese nula de que a m√©dia √© zero. Por exemplo, se `p_value = 0.15`, n√£o rejeitamos a hip√≥tese nula. Se `p_value = 0.01`, rejeitamos a hip√≥tese nula.

2.  **Teste de Shapiro-Wilk:** Embora primariamente um teste de normalidade, pode indiretamente dar suporte √† hip√≥tese de m√©dia zero se combinado com outros testes.

**Testes para Vari√¢ncia Constante (Homoscedasticidade):**

1.  **Teste de Bartlett:** Compara a vari√¢ncia de diferentes subgrupos da sequ√™ncia. √â sens√≠vel a desvios da normalidade.

    *Hip√≥tese Nula (H‚ÇÄ):* As vari√¢ncias de todos os grupos s√£o iguais.
    *Estat√≠stica de Teste:*  Envolve o c√°lculo das vari√¢ncias amostrais de *k* grupos e a compara√ß√£o com uma estat√≠stica qui-quadrado.

    > üí° **Exemplo Num√©rico (Teste de Bartlett):** Vamos aplicar o teste de Bartlett √† nossa sequ√™ncia de white noise, dividindo-a em grupos.

```python
import numpy as np
import scipy.stats as stats

# Gerar white noise gaussiano
white_noise = box_muller(1000, sigma=2)

# Dividir a sequ√™ncia em 5 grupos
grupos = np.array_split(white_noise, 5)

# Realizar o teste de Bartlett
statistic, p_value = stats.bartlett(*grupos)

print(f"Estat√≠stica de Bartlett: {statistic:.4f}")
print(f"Valor-p: {p_value:.4f}")
```

    Se o valor-p for maior que um n√≠vel de signific√¢ncia, falhamos em rejeitar a hip√≥tese nula de vari√¢ncias iguais. Por exemplo, se `p_value = 0.20`, n√£o rejeitamos a hip√≥tese nula.

2.  **Teste de Levene:** Similar ao teste de Bartlett, mas mais robusto a desvios da normalidade.

    *Hip√≥tese Nula (H‚ÇÄ):* As vari√¢ncias de todos os grupos s√£o iguais.
    *Estat√≠stica de Teste:* Baseia-se na an√°lise de vari√¢ncia dos desvios absolutos em rela√ß√£o √† mediana.

    > üí° **Exemplo Num√©rico (Teste de Levene):** Vamos aplicar o teste de Levene √† nossa sequ√™ncia de white noise, da mesma forma que fizemos com o teste de Bartlett.

```python
import numpy as np
import scipy.stats as stats

# Gerar white noise gaussiano
white_noise = box_muller(1000, sigma=2)

# Dividir a sequ√™ncia em 5 grupos
grupos = np.array_split(white_noise, 5)

# Realizar o teste de Levene
statistic, p_value = stats.levene(*grupos)

print(f"Estat√≠stica de Levene: {statistic:.4f}")
print(f"Valor-p: {p_value:.4f}")
```

    > üí° **Exemplo Num√©rico (Compara√ß√£o de Bartlett e Levene):** Podemos gerar dados n√£o-normais para observar a robustez do teste de Levene em compara√ß√£o com o teste de Bartlett.

```python
import numpy as np
import scipy.stats as stats

# Gerar dados n√£o-normais (ex: distribui√ß√£o exponencial)
non_normal_data = np.random.exponential(scale=2.0, size=1000)

# Dividir a sequ√™ncia em 5 grupos
grupos_non_normal = np.array_split(non_normal_data, 5)

# Realizar o teste de Bartlett
statistic_bartlett, p_value_bartlett = stats.bartlett(*grupos_non_normal)

# Realizar o teste de Levene
statistic_levene, p_value_levene = stats.levene(*grupos_non_normal)

print("Teste de Bartlett (Dados N√£o-Normais):")
print(f"Estat√≠stica de Bartlett: {statistic_bartlett:.4f}")
print(f"Valor-p: {p_value_bartlett:.4f}")

print("\nTeste de Levene (Dados N√£o-Normais):")
print(f"Estat√≠stica de Levene: {statistic_levene:.4f}")
print(f"Valor-p: {p_value_levene:.4f}")
```

    Geralmente, o teste de Levene ter√° um valor-p maior que o teste de Bartlett quando aplicado a dados n√£o-normais, indicando que √© menos sens√≠vel √† viola√ß√£o da suposi√ß√£o de normalidade.

**Testes para N√£o Correla√ß√£o (Autocorrela√ß√£o):**

1.  **Fun√ß√£o de Autocorrela√ß√£o (ACF):** Calcula a correla√ß√£o entre a sequ√™ncia e suas vers√µes defasadas. Para white noise, a ACF deve ser essencialmente zero para todos os lags diferentes de zero.

    > üí° **Exemplo Num√©rico (ACF):** J√° demonstramos como plotar a ACF de uma s√©rie de white noise na se√ß√£o anterior [^50]. Esperamos ver um pico significativo apenas no lag 0.

2.  **Teste de Ljung-Box:** Testa se h√° autocorrela√ß√£o significativa em um conjunto de lags.

    *Hip√≥tese Nula (H‚ÇÄ):* A s√©rie n√£o apresenta autocorrela√ß√£o at√© a ordem *h*.

    *Estat√≠stica de Teste:*
    $$ Q = n(n+2) \sum_{k=1}^{h} \frac{\hat{\rho}_k^2}{n-k} $$
    onde *n* √© o tamanho da amostra e $\hat{\rho}_k$ √© a autocorrela√ß√£o amostral no lag *k*.

    > üí° **Exemplo Num√©rico (Teste de Ljung-Box):** J√° demonstramos como aplicar o teste de Ljung-Box para verificar a n√£o correla√ß√£o em uma s√©rie de white noise na se√ß√£o anterior [^50]. Se os valores-p forem consistentemente altos (acima de um n√≠vel de signific√¢ncia Œ±, como 0.05), n√£o rejeitamos a hip√≥tese nula de que a s√©rie n√£o apresenta autocorrela√ß√£o at√© a ordem *h*.

**Testes para Normalidade:**

1.  **Teste de Shapiro-Wilk:** Testa se a amostra segue uma distribui√ß√£o normal.

    *Hip√≥tese Nula (H‚ÇÄ):* A amostra segue uma distribui√ß√£o normal.

    > üí° **Exemplo Num√©rico (Teste de Shapiro-Wilk):** Vamos aplicar o teste de Shapiro-Wilk √† nossa sequ√™ncia de white noise gaussiano.

```python
import numpy as np
import scipy.stats as stats

# Gerar white noise gaussiano
white_noise = box_muller(1000, sigma=2)

# Realizar o teste de Shapiro-Wilk
statistic, p_value = stats.shapiro(white_noise)

print(f"Estat√≠stica de Shapiro-Wilk: {statistic:.4f}")
print(f"Valor-p: {p_value:.4f}")
```

    Se o valor-p for maior que um n√≠vel de signific√¢ncia, falhamos em rejeitar a hip√≥tese nula de que a amostra segue uma distribui√ß√£o normal. Por exemplo, `p_value > 0.05` indica que n√£o rejeitamos a normalidade.

2.  **Teste de Jarque-Bera:** Similar ao Shapiro-Wilk, testa a normalidade com base na assimetria e curtose da amostra.

    *Hip√≥tese Nula (H‚ÇÄ):* A amostra segue uma distribui√ß√£o normal.

    > üí° **Exemplo Num√©rico (Teste de Jarque-Bera):** Vamos aplicar o teste de Jarque-Bera √† nossa sequ√™ncia de white noise gaussiano.

```python
import numpy as np
import scipy.stats as stats

# Gerar white noise gaussiano
white_noise = box_muller(1000, sigma=2)

# Realizar o teste de Jarque-Bera
statistic, p_value = stats.jarque_bera(white_noise)

print(f"Estat√≠stica de Jarque-Bera: {statistic:.4f}")
print(f"Valor-p: {p_value:.4f}")
```

Para complementar os testes de normalidade, podemos tamb√©m utilizar o **teste de Kolmogorov-Smirnov**. Este teste compara a fun√ß√£o de distribui√ß√£o cumulativa emp√≠rica da amostra com a fun√ß√£o de distribui√ß√£o cumulativa te√≥rica (neste caso, a distribui√ß√£o normal).

**Teorema 2** (Teste de Kolmogorov-Smirnov): O teste de Kolmogorov-Smirnov avalia a similaridade entre a distribui√ß√£o emp√≠rica de uma amostra e uma distribui√ß√£o de refer√™ncia, quantificando a maior diferen√ßa absoluta entre suas fun√ß√µes de distribui√ß√£o cumulativa.

> **Prova do Teorema 2:**
>
> I. Seja $X_1, X_2, \ldots, X_n$ uma amostra aleat√≥ria de uma popula√ß√£o com fun√ß√£o de distribui√ß√£o cumulativa (CDF) $F(x)$.
>
> II. Seja $F_n(x)$ a fun√ß√£o de distribui√ß√£o cumulativa emp√≠rica (ECDF), definida como:
>
> $$ F_n(x) = \frac{1}{n} \sum_{i=1}^{n} I(X_i \le x) $$
> onde $I(X_i \le x)$ √© a fun√ß√£o indicadora que √© 1 se $X_i \le x$ e 0 caso contr√°rio.
>
> III. O teste de Kolmogorov-Smirnov (KS) quantifica a dist√¢ncia entre $F_n(x)$ e a CDF te√≥rica $F(x)$.
>
> IV. A estat√≠stica de teste KS √© definida como a maior diferen√ßa absoluta entre a ECDF e a CDF:
>
> $$ D_n = \sup_x |F_n(x) - F(x)| $$
>
> V. Sob a hip√≥tese nula de que a amostra √© proveniente da distribui√ß√£o com CDF $F(x)$, a distribui√ß√£o de $D_n$ √© conhecida assintoticamente.
>
> VI. O valor-p do teste √© calculado com base na distribui√ß√£o de $D_n$. Se o valor-p for pequeno (menor que um n√≠vel de signific√¢ncia Œ±), rejeitamos a hip√≥tese nula, indicando que a amostra provavelmente n√£o √© proveniente da distribui√ß√£o especificada por $F(x)$.
>
> VII. A estat√≠stica de KS $D_n$ mede a m√°xima discrep√¢ncia entre as duas fun√ß√µes de distribui√ß√£o, fornecendo uma medida direta de qu√£o bem a distribui√ß√£o emp√≠rica da amostra se ajusta √† distribui√ß√£o te√≥rica. ‚ñ†

> üí° **Exemplo Num√©rico (Teste de Kolmogorov-Smirnov):** Implementaremos o teste de Kolmogorov-Smirnov em Python para verificar a normalidade da nossa sequ√™ncia de white noise gaussiano.

```python
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt

# Gerar white noise gaussiano
white_noise = box_muller(1000, sigma=2)

#```python
# Teste de Shapiro-Wilk
shapiro_test = stats.shapiro(white_noise)
print("Teste de Shapiro-Wilk:", shapiro_test)

# Teste de Kolmogorov-Smirnov
ks_test = stats.kstest(white_noise, 'norm', args=(0, np.std(white_noise)))
print("Teste de Kolmogorov-Smirnov:", ks_test)

# Histograma dos res√≠duos
plt.figure(figsize=(10, 6))
plt.hist(white_noise, bins=30, density=True, alpha=0.6, color='g')

# Sobrepor a distribui√ß√£o normal te√≥rica
x = np.linspace(min(white_noise), max(white_noise), 100)
p = stats.norm.pdf(x, 0, np.std(white_noise))
plt.plot(x, p, 'k', linewidth=2)

plt.title("Histograma do White Noise Gaussiano com Distribui√ß√£o Normal Te√≥rica")
plt.xlabel("Valor")
plt.ylabel("Densidade")
plt.grid(True)
plt.show()
```

Estes testes e o histograma ajudam a validar se a sequ√™ncia gerada se aproxima de uma distribui√ß√£o normal.  Se os p-valores dos testes de Shapiro-Wilk eKolmogorov-Smirnov forem maiores que um n√≠vel de signific√¢ncia (e.g., 0.05), n√£o rejeitamos a hip√≥tese nula de que a amostra √© proveniente de uma distribui√ß√£o normal. No histograma, esperamos ver uma forma de sino sim√©trica centrada na m√©dia da distribui√ß√£o normal.

### Teste de Autocorrela√ß√£o

O teste de autocorrela√ß√£o verifica se existe correla√ß√£o entre os valores da sequ√™ncia e seus valores defasados no tempo. Em outras palavras, avalia se um valor na sequ√™ncia √© influenciado pelos valores anteriores.

**Implementa√ß√£o:**

```python
import numpy as np
import matplotlib.pyplot as plt

def autocorrela√ß√£o(sequ√™ncia, k):
  """
  Calcula a autocorrela√ß√£o de uma sequ√™ncia para um determinado atraso k.
  """
  n = len(sequ√™ncia)
  m√©dia = np.mean(sequ√™ncia)
  numerador = sum([(sequ√™ncia[i] - m√©dia) * (sequ√™ncia[i + k] - m√©dia) for i in range(n - k)])
  denominador = sum([(sequ√™ncia[i] - m√©dia) ** 2 for i in range(n)])
  return numerador / denominador

# Exemplo de uso:
tamanho_sequ√™ncia = 1000
sequ√™ncia_aleat√≥ria = np.random.normal(0, 1, tamanho_sequ√™ncia) # Gere uma sequ√™ncia aleat√≥ria normal
atrasos = range(1, 50) # Calcule a autocorrela√ß√£o para os primeiros 50 atrasos
autocorrela√ß√µes = [autocorrela√ß√£o(sequ√™ncia_aleat√≥ria, k) for k in atrasos]

# Plote a fun√ß√£o de autocorrela√ß√£o
plt.figure(figsize=(10, 6))
plt.stem(atrasos, autocorrela√ß√µes, basefmt=" ")
plt.xlabel("Atraso (k)")
plt.ylabel("Autocorrela√ß√£o")
plt.title("Fun√ß√£o de Autocorrela√ß√£o")
plt.grid(True)
plt.show()
```

**Interpreta√ß√£o:**

*   **Autocorrela√ß√£o Pr√≥xima de Zero:** Indica que os valores na sequ√™ncia s√£o independentes uns dos outros, o que √© desej√°vel para uma sequ√™ncia verdadeiramente aleat√≥ria.
*   **Autocorrela√ß√£o Significativa:** Indica que os valores est√£o correlacionados, o que pode ser um problema em muitas aplica√ß√µes.

### Teste de Entropia

A entropia mede a aleatoriedade ou a incerteza de uma sequ√™ncia. Uma sequ√™ncia com alta entropia √© mais imprevis√≠vel, enquanto uma sequ√™ncia com baixa entropia √© mais previs√≠vel.

**Implementa√ß√£o:**

```python
import numpy as np
from collections import Counter
import matplotlib.pyplot as plt

def entropia(sequ√™ncia):
  """
  Calcula a entropia de Shannon de uma sequ√™ncia.
  """
  n = len(sequ√™ncia)
  contagem = Counter(sequ√™ncia)
  probabilidades = [contagem[x] / n for x in contagem]
  return -sum([p * np.log2(p) for p in probabilidades])

# Exemplo de uso:
tamanho_sequ√™ncia = 1000
sequ√™ncia_inteiros = np.random.randint(0, 256, tamanho_sequ√™ncia) # Gere uma sequ√™ncia de inteiros aleat√≥rios entre 0 e 255
e = entropia(sequ√™ncia_inteiros)
print(f"Entropia da sequ√™ncia: {e}")

# Teste de Entropia para bits
def entropia_bits(sequ√™ncia_bits):
    """Calcula a entropia de Shannon de uma sequ√™ncia de bits."""
    n = len(sequ√™ncia_bits)
    contagem = Counter(sequ√™ncia_bits)
    probabilidades = [contagem[bit] / n for bit in contagem]
    return -sum([p * np.log2(p) for p in probabilidades])

# Gerar sequ√™ncia de bits aleat√≥rios
tamanho_sequ√™ncia = 1000
sequ√™ncia_bits = np.random.randint(0, 2, tamanho_sequ√™ncia) # Gere uma sequ√™ncia de bits aleat√≥rios (0 ou 1)
e_bits = entropia_bits(sequ√™ncia_bits)
print(f"Entropia da sequ√™ncia de bits: {e_bits}")

# Visualiza√ß√£o da entropia
categorias = ['Sequ√™ncia de Inteiros', 'Sequ√™ncia de Bits']
valores = [e, e_bits]

plt.figure(figsize=(8, 6))
plt.bar(categorias, valores, color=['blue', 'green'])
plt.ylabel('Entropia')
plt.title('Compara√ß√£o da Entropia entre Sequ√™ncias')
plt.ylim(0, 8)  # Ajuste o limite superior do eixo y para melhor visualiza√ß√£o
plt.grid(axis='y', linestyle='--')
plt.show()
```

**Interpreta√ß√£o:**

*   **Entropia Alta:** Indica que a sequ√™ncia √© altamente aleat√≥ria e imprevis√≠vel. Em uma sequ√™ncia de bits idealmente aleat√≥ria, a entropia deve ser pr√≥xima de 1 (se normalizada).
*   **Entropia Baixa:** Indica que a sequ√™ncia √© mais previs√≠vel e tem menos aleatoriedade.

### Teste de Runs (Sequ√™ncias)

O teste de runs avalia se a ocorr√™ncia de valores acima e abaixo da mediana em uma sequ√™ncia √© aleat√≥ria. Um "run" √© uma sequ√™ncia consecutiva de valores do mesmo lado da mediana.

**Implementa√ß√£o:**

```python
import numpy as np
from scipy import stats

def teste_runs(sequ√™ncia):
  """
  Realiza o teste de runs para verificar a aleatoriedade de uma sequ√™ncia.
  """
  mediana = np.median(sequ√™ncia)
  sinais = [1 if x > mediana else 0 for x in sequ√™ncia]
  n = len(sinais)
  n1 = sinais.count(1)  # N√∫mero de valores acima da mediana
  n2 = sinais.count(0)  # N√∫mero de valores abaixo da mediana
  runs = 1
  for i in range(1, n):
    if sinais[i] != sinais[i - 1]:
      runs += 1

  # Estat√≠stica de teste e p-valor (usando a aproxima√ß√£o normal)
  E = (2*n1*n2)/ (n1 + n2) + 1
  Var = (2*n1*n2*(2*n1*n2 - n1 - n2))/ ((n1 + n2)**2 * (n1 + n2 - 1))
  z = (runs - E) / np.sqrt(Var)
  p_valor = 2 * (1 - stats.norm.cdf(np.abs(z)))

  return runs, p_valor

# Exemplo de uso:
tamanho_sequ√™ncia = 1000
sequ√™ncia_aleat√≥ria = np.random.normal(0, 1, tamanho_sequ√™ncia) # Gere uma sequ√™ncia aleat√≥ria normal
runs, p_valor = teste_runs(sequ√™ncia_aleat√≥ria)

print(f"N√∫mero de runs: {runs}")
print(f"P-valor: {p_valor}")

# Interpreta√ß√£o do p-valor
alfa = 0.05  # N√≠vel de signific√¢ncia
if p_valor < alfa:
    print("Rejeitamos a hip√≥tese nula de aleatoriedade.")
else:
    print("N√£o rejeitamos a hip√≥tese nula de aleatoriedade.")
```

**Interpreta√ß√£o:**

*   **P-valor Alto:** Indica que o n√∫mero de runs observado √© consistente com a aleatoriedade. N√£o rejeitamos a hip√≥tese nula de que a sequ√™ncia √© aleat√≥ria.
*   **P-valor Baixo:** Indica que o n√∫mero de runs √© significativamente diferente do que seria esperado em uma sequ√™ncia aleat√≥ria. Rejeitamos a hip√≥tese nula de aleatoriedade.

### Teste de Frequ√™ncia (Monobit)

Este teste verifica se a propor√ß√£o de 0s e 1s em uma sequ√™ncia bin√°ria √© aproximadamente igual.

**Implementa√ß√£o:**

```python
import numpy as np
from scipy import stats

def teste_frequencia(sequ√™ncia):
  """
  Realiza o teste de frequ√™ncia (monobit) para verificar se a propor√ß√£o de 0s e 1s √© aproximadamente igual.
  """
  n = len(sequ√™ncia)
  n1 = sequ√™ncia.count(1) # Contagem de 1s
  n0 = sequ√™ncia.count(0) # Contagem de 0s
  S_n = abs(n1 - n0)
  raiz_n = np.sqrt(n)
  stat_teste = S_n / raiz_n
  p_valor = 2 * (1 - stats.norm.cdf(stat_teste))
  return p_valor

# Exemplo de uso:
tamanho_sequ√™ncia = 1000
sequ√™ncia_bits = np.random.randint(0, 2, tamanho_sequ√™ncia).tolist() # Gere uma sequ√™ncia de bits aleat√≥rios (0 ou 1)
p_valor = teste_frequencia(sequ√™ncia_bits)

print(f"P-valor: {p_valor}")

# Interpreta√ß√£o do p-valor
alfa = 0.05  # N√≠vel de signific√¢ncia
if p_valor < alfa:
    print("Rejeitamos a hip√≥tese nula de que a propor√ß√£o de 0s e 1s √© aproximadamente igual.")
else:
    print("N√£o rejeitamos a hip√≥tese nula de que a propor√ß√£o de 0s e 1s √© aproximadamente igual.")
```

**Interpreta√ß√£o:**

*   **P-valor Alto:** Indica que a propor√ß√£o de 0s e 1s √© considerada aproximadamente igual. N√£o rejeitamos a hip√≥tese nula.
*   **P-valor Baixo:** Indica que a propor√ß√£o de 0s e 1s √© significativamente diferente do que seria esperado em uma sequ√™ncia aleat√≥ria. Rejeitamos a hip√≥tese nula.

### Diagrama de Dispers√£o

Um diagrama de dispers√£o pode ajudar a visualizar a rela√ß√£o entre os valores consecutivos na sequ√™ncia gerada. Se a sequ√™ncia for aleat√≥ria, n√£o devemos observar padr√µes discern√≠veis no diagrama de dispers√£o.

**Implementa√ß√£o:**

```python
import matplotlib.pyplot as plt
import numpy as np

def plotar_dispers√£o(sequ√™ncia):
    """
    Plota um diagrama de dispers√£o de uma sequ√™ncia,
    onde cada ponto representa (x[i], x[i+1]).
    """
    x = sequ√™ncia[:-1]
    y = sequ√™ncia[1:]

    plt.figure(figsize=(8, 6))
    plt.scatter(x, y, alpha=0.5)
    plt.title('Diagrama de Dispers√£o da Sequ√™ncia')
    plt.xlabel('x[i]')
    plt.ylabel('x[i+1]')
    plt.grid(True)
    plt.show()

# Exemplo de uso
tamanho_sequ√™ncia = 1000
sequ√™ncia_aleat√≥ria = np.random.normal(0, 1, tamanho_sequ√™ncia)  # Gere uma sequ√™ncia aleat√≥ria normal
plotar_dispers√£o(sequ√™ncia_aleat√≥ria)
```

**Interpreta√ß√£o:**

*   **Sem Padr√µes Vis√≠veis:** Pontos distribu√≠dos aleatoriamente sem forma√ß√£o de clusters ou tend√™ncias indicam que a sequ√™ncia √© aleat√≥ria.
*   **Padr√µes Vis√≠veis:** A presen√ßa de padr√µes, como linhas ou clusters, sugere correla√ß√£o entre os valores consecutivos e, portanto, n√£o aleatoriedade.

<!-- END -->