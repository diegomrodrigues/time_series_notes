## White Noise Independente: Uma Condi√ß√£o Mais Forte

### Introdu√ß√£o

Em continuidade ao estudo do **white noise** [^47], este cap√≠tulo aprofunda-se na condi√ß√£o de independ√™ncia estat√≠stica, que representa uma restri√ß√£o mais forte do que a simples n√£o correla√ß√£o. Examinaremos as implica√ß√µes dessa condi√ß√£o adicional e como ela diferencia o **white noise independente** do **white noise** padr√£o.

### Conceitos Fundamentais

Como vimos anteriormente, um processo de **white noise** {Œµ‚Çú} satisfaz as seguintes propriedades [^47]:

1.  $E(\varepsilon_t) = 0$
2.  $E(\varepsilon_t^2) = \sigma^2$
3.  $E(\varepsilon_t \varepsilon_\tau) = 0 \quad \text{para } t \neq \tau$

A condi√ß√£o de **white noise independente** exige uma propriedade ainda mais forte: que Œµ‚Çú e Œµ‚Çú sejam estatisticamente independentes para todo $t \neq \tau$ [^48]. Em outras palavras, o conhecimento do valor de Œµ‚Çú n√£o fornece informa√ß√£o *alguma* sobre a distribui√ß√£o de Œµ‚Çú, independentemente de qual seja œÑ.

**Defini√ß√£o Formal:** Um processo {Œµ‚Çú} √© **white noise independente** se, para quaisquer $t_1, t_2, \ldots, t_n$ distintos, as vari√°veis aleat√≥rias $\varepsilon_{t_1}, \varepsilon_{t_2}, \ldots, \varepsilon_{t_n}$ s√£o estatisticamente independentes.

> üí° **Exemplo Num√©rico:** Considere um processo onde $\varepsilon_1 = 2$, $\varepsilon_2 = -1$, e $\varepsilon_3 = 0$. Se este processo for white noise independente, ent√£o o valor de $\varepsilon_1$ (que √© 2) n√£o fornece nenhuma informa√ß√£o sobre a probabilidade de $\varepsilon_2$ ser qualquer valor, como -1, 0, ou qualquer outro n√∫mero. Formalmente, $P(\varepsilon_2 = x | \varepsilon_1 = 2) = P(\varepsilon_2 = x)$ para qualquer valor $x$.

**Implica√ß√µes da Independ√™ncia:**

A independ√™ncia estat√≠stica implica n√£o correla√ß√£o, mas o inverso n√£o √© necessariamente verdadeiro [^48]. Para ilustrar essa diferen√ßa crucial, considere o seguinte exemplo:

**Exemplo (N√£o Correla√ß√£o vs. Independ√™ncia):**

Suponha que Œµ‚Çú siga uma distribui√ß√£o tal que:

$$
\varepsilon_t = \begin{cases}
    +1 & \text{com probabilidade } 1/2 \\
    -1 & \text{com probabilidade } 1/2
\end{cases}
$$

Se definirmos $\varepsilon_\tau = \varepsilon_t$  para $\tau \neq t$ ent√£o  $E[\varepsilon_t \varepsilon_\tau] = E[\varepsilon_t^2] = 1$, o que claramente viola a condi√ß√£o de n√£o correla√ß√£o e, portanto, n√£o √© white noise.

Agora, considere um processo {Œµ‚Çú} definido como se segue: Œµ‚ÇÅ √© gerado como acima. Para t > 1,

$$
\varepsilon_t = \begin{cases}
    \varepsilon_{t-1} & \text{com probabilidade } 1/2 \\
    -\varepsilon_{t-1} & \text{com probabilidade } 1/2
\end{cases}
$$

Neste caso, $E[\varepsilon_t] = 0$ e $E[\varepsilon_t \varepsilon_\tau] = 0$ para $t \neq \tau$, satisfazendo as condi√ß√µes de white noise n√£o correlacionado. No entanto, Œµ‚Çú √© claramente dependente de Œµ‚Çú‚Çã‚ÇÅ, pois sua distribui√ß√£o √© completamente determinada por Œµ‚Çú‚Çã‚ÇÅ. Este processo √© white noise, mas *n√£o* √© white noise independente.

*Prova da N√£o Correla√ß√£o:*

Para provar que $E[\varepsilon_t \varepsilon_\tau] = 0$ para $t \neq \tau$ nesse exemplo espec√≠fico, considere:

I. Sem perda de generalidade, assuma $t > \tau$. Ent√£o $\varepsilon_t = \pm \varepsilon_{t-1}$.

II. Podemos escrever: $E[\varepsilon_t \varepsilon_\tau] = E[E[\varepsilon_t \varepsilon_\tau | \varepsilon_{t-1}, ..., \varepsilon_\tau]]$

III. Calculando a expectativa condicional:
$E[\varepsilon_t \varepsilon_\tau | \varepsilon_{t-1}, ..., \varepsilon_\tau] = \varepsilon_\tau E[\varepsilon_t | \varepsilon_{t-1}, ..., \varepsilon_\tau] = \varepsilon_\tau \cdot 0 = 0$, uma vez que $E[\varepsilon_t | \varepsilon_{t-1}] = 0$ devido √†s probabilidades iguais de $\varepsilon_t$ ser $\varepsilon_{t-1}$ ou $-\varepsilon_{t-1}$.

IV. Portanto, $E[\varepsilon_t \varepsilon_\tau] = E[0] = 0$. $\blacksquare$

Este exemplo demonstra que a n√£o correla√ß√£o √© uma condi√ß√£o *necess√°ria* para independ√™ncia, mas *n√£o suficiente*.

> üí° **Exemplo Num√©rico (Simula√ß√£o):** Vamos simular o processo descrito acima por 100 per√≠odos e verificar a n√£o correla√ß√£o amostral.

```python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)  # Para reprodutibilidade
n = 100
epsilon = np.zeros(n)
epsilon[0] = np.random.choice([-1, 1])
for t in range(1, n):
    epsilon[t] = epsilon[t-1] * np.random.choice([-1, 1])

# Calcular a autocorrela√ß√£o amostral
def autocorr(x, lag):
    if lag >= len(x):
        return 0  # Ou tratar de outra forma
    return np.corrcoef(x[:-lag], x[lag:])[0, 1]

lags = range(1, 10)
autocorrelations = [autocorr(epsilon, lag) for lag in lags]

plt.figure(figsize=(10, 6))
plt.stem(lags, autocorrelations, use_line_collection=True)
plt.title("Autocorrela√ß√£o Amostral do White Noise N√£o Independente")
plt.xlabel("Lag")
plt.ylabel("Autocorrela√ß√£o")
plt.grid(True)
plt.show()

# Imprimir as autocorrela√ß√µes para ver os valores
print("Autocorrela√ß√µes:", autocorrelations)
```

Este c√≥digo simula o processo e calcula as autocorrela√ß√µes amostrais.  As autocorrela√ß√µes devem estar pr√≥ximas de zero, confirmando a n√£o correla√ß√£o amostral, embora o processo n√£o seja independente. Observe que em simula√ß√µes com um n√∫mero finito de amostras, voc√™ n√£o ter√° exatamente zero autocorrela√ß√£o, mas valores pr√≥ximos.

**Lema 1:** *Se um processo {Œµ‚Çú} √© white noise independente e $g$ e $h$ s√£o fun√ß√µes mensur√°veis, ent√£o $g(\varepsilon_t)$ e $h(\varepsilon_\tau)$ s√£o n√£o correlacionadas para $t \neq \tau$ se $E[g(\varepsilon_t)] < \infty$ e $E[h(\varepsilon_\tau)] < \infty$ e $E[g(\varepsilon_t)]=0$ ou $E[h(\varepsilon_\tau)]=0$.*

*Prova:* Sem perda de generalidade, suponha que $E[g(\varepsilon_t)] = 0$. Pela independ√™ncia de $\varepsilon_t$ e $\varepsilon_\tau$ para $t \neq \tau$, temos que $g(\varepsilon_t)$ e $h(\varepsilon_\tau)$ tamb√©m s√£o independentes. Portanto, $E[g(\varepsilon_t)h(\varepsilon_\tau)] = E[g(\varepsilon_t)]E[h(\varepsilon_\tau)] = 0 \cdot E[h(\varepsilon_\tau)] = 0$. Logo, $g(\varepsilon_t)$ e $h(\varepsilon_\tau)$ s√£o n√£o correlacionadas. $\blacksquare$

> üí° **Exemplo Num√©rico (Lema 1):** Seja  Œµ‚Çú um white noise independente com m√©dia zero e vari√¢ncia 1. Defina $g(\varepsilon_t) = \varepsilon_t^3$ e $h(\varepsilon_\tau) = \varepsilon_\tau$. Ent√£o $E[g(\varepsilon_t)] = E[\varepsilon_t^3] = 0$ (pois a distribui√ß√£o √© sim√©trica em torno de zero). Pelo Lema 1, $g(\varepsilon_t)$ e $h(\varepsilon_\tau)$ s√£o n√£o correlacionadas para $t \neq \tau$. Isso significa que $E[\varepsilon_t^3 \varepsilon_\tau] = 0$.

**Gaussian White Noise Independente:**

Um caso particular importante √© o **Gaussian white noise independente**, onde cada Œµ‚Çú segue uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia œÉ¬≤, e todas as vari√°veis s√£o independentes [^48]:

$$\varepsilon_t \sim N(0, \sigma^2) \quad \text{e s√£o independentes}$$

Neste caso, a independ√™ncia estat√≠stica √© equivalente √† n√£o correla√ß√£o, pois a distribui√ß√£o normal √© completamente caracterizada por seus dois primeiros momentos (m√©dia e vari√¢ncia).

Para demonstrar formalmente essa equival√™ncia, apresentamos a seguinte prova:

*Prova da Equival√™ncia entre N√£o Correla√ß√£o e Independ√™ncia para Gaussian White Noise:*

I. Seja {Œµ‚Çú} um processo Gaussiano com $E[\varepsilon_t] = 0$ e $E[\varepsilon_t \varepsilon_\tau] = 0$ para $t \neq \tau$.

II. Como {Œµ‚Çú} √© um processo gaussiano, a distribui√ß√£o conjunta de qualquer conjunto finito de vari√°veis $\varepsilon_{t_1}, \varepsilon_{t_2}, ..., \varepsilon_{t_n}$ √© uma distribui√ß√£o normal multivariada.

III. A matriz de covari√¢ncia dessa distribui√ß√£o normal multivariada √© diagonal, pois $E[\varepsilon_t \varepsilon_\tau] = 0$ para $t \neq \tau$.

IV. Para uma distribui√ß√£o normal multivariada, uma matriz de covari√¢ncia diagonal implica que as vari√°veis s√£o independentes.

V. Portanto, $\varepsilon_{t_1}, \varepsilon_{t_2}, ..., \varepsilon_{t_n}$ s√£o independentes. Como isso vale para qualquer conjunto finito de vari√°veis, {Œµ‚Çú} √© um white noise independente. $\blacksquare$

> üí° **Exemplo Num√©rico (Gaussian White Noise):** Suponha que Œµ‚Çú ‚àº N(0, 4), ou seja, m√©dia 0 e vari√¢ncia 4. Simulando alguns valores:

![Generated plot](./../images/plot_14.png)

Cada ponto no gr√°fico √© independente dos outros e segue uma distribui√ß√£o normal com m√©dia 0 e desvio padr√£o 2. A n√£o correla√ß√£o entre os pontos √© garantida pela independ√™ncia na gera√ß√£o dos n√∫meros aleat√≥rios.

**Teorema 1:** *Se {Œµ‚Çú} √© Gaussian white noise independente, ent√£o qualquer transforma√ß√£o linear de {Œµ‚Çú} tamb√©m √© um processo gaussiano.*

*Prova:* Seja $Y_t = \sum_{i=0}^{p} a_i \varepsilon_{t-i}$ uma transforma√ß√£o linear de {Œµ‚Çú}, onde $a_i$ s√£o constantes. Como {Œµ‚Çú} √© um processo gaussiano independente, cada $\varepsilon_{t-i}$ √© normalmente distribu√≠da. Uma soma ponderada de vari√°veis gaussianas independentes √© tamb√©m uma vari√°vel gaussiana. Portanto, $Y_t$ √© gaussiano. Al√©m disso, qualquer conjunto finito de vari√°veis $Y_{t_1}, Y_{t_2}, ..., Y_{t_n}$ √© uma transforma√ß√£o linear de um conjunto de vari√°veis gaussianas independentes, e portanto √© um vetor gaussiano. Logo, {Y‚Çú} √© um processo gaussiano. $\blacksquare$

> üí° **Exemplo Num√©rico (Teorema 1):** Seja Œµ‚Çú ‚àº N(0, 1) um Gaussian white noise independente. Defina $Y_t = 0.5\varepsilon_t + 0.3\varepsilon_{t-1} + 0.2\varepsilon_{t-2}$. Ent√£o, pelo Teorema 1, $Y_t$ tamb√©m √© um processo gaussiano. Podemos calcular a m√©dia e vari√¢ncia de $Y_t$:

> $E[Y_t] = 0.5E[\varepsilon_t] + 0.3E[\varepsilon_{t-1}] + 0.2E[\varepsilon_{t-2}] = 0$
>
> $Var(Y_t) = (0.5)^2Var(\varepsilon_t) + (0.3)^2Var(\varepsilon_{t-1}) + (0.2)^2Var(\varepsilon_{t-2}) = 0.25 + 0.09 + 0.04 = 0.38$
>
> Portanto, $Y_t \sim N(0, 0.38)$.

**Relev√¢ncia Te√≥rica:**

A condi√ß√£o de independ√™ncia √© crucial em certas provas e deriva√ß√µes te√≥ricas. Por exemplo, em alguns teoremas de converg√™ncia assint√≥tica, a independ√™ncia entre os termos do processo simplifica significativamente a an√°lise.

**Observa√ß√£o:** Se um processo √© linear e gaussiano, a n√£o correla√ß√£o implica independ√™ncia. No entanto, essa equival√™ncia *n√£o* se mant√©m para processos n√£o lineares ou n√£o gaussianos.

Para complementar essa observa√ß√£o, podemos apresentar uma condi√ß√£o sob a qual a n√£o correla√ß√£o *implica* independ√™ncia para processos lineares.

**Teorema 1.1:** Seja $\{X_t\}$ um processo linear definido por $X_t = \sum_{i=-\infty}^{\infty} \psi_i \varepsilon_{t-i}$, onde $\{\varepsilon_t\}$ √© um processo gaussiano com m√©dia zero e covari√¢ncia $\gamma_{t,s} = \sigma^2 \delta_{t,s}$ (onde $\delta_{t,s}$ √© a fun√ß√£o delta de Kronecker, igual a 1 se $t=s$ e 0 caso contr√°rio), e $\sum_{i=-\infty}^{\infty} |\psi_i| < \infty$.  Ent√£o, $\{X_t\}$ √© um processo gaussiano, e a n√£o correla√ß√£o de $\{\varepsilon_t\}$ implica a independ√™ncia de $\{\varepsilon_t\}$.

*Prova:* Como $\{X_t\}$ √© uma combina√ß√£o linear de vari√°veis gaussianas, ele pr√≥prio √© um processo gaussiano. A n√£o correla√ß√£o de $\{\varepsilon_t\}$ implica que $\gamma_{t,s} = 0$ para $t \neq s$. Para processos gaussianos, covari√¢ncia zero implica independ√™ncia. Portanto, $\{\varepsilon_t\}$ √© um processo gaussiano independente.  $\blacksquare$

**Implica√ß√µes Pr√°ticas:**

Na pr√°tica, distinguir entre white noise n√£o correlacionado e white noise independente pode ser desafiador. Testes estat√≠sticos para independ√™ncia s√£o geralmente mais complexos e menos poderosos do que testes para n√£o correla√ß√£o (como o teste de Ljung-Box).

> üí° **Exemplo Num√©rico (Teste de Ljung-Box):** Vamos usar o teste de Ljung-Box para testar a n√£o correla√ß√£o em um processo de white noise.

![Generated plot](./../images/plot_15.png)

Se os valores-p forem consistentemente altos (acima de um n√≠vel de signific√¢ncia Œ±, como 0.05), n√£o rejeitamos a hip√≥tese nula de que os dados s√£o independentes e identicamente distribu√≠dos (IID). No entanto, isso n√£o prova a independ√™ncia, apenas que n√£o encontramos evid√™ncias de autocorrela√ß√£o.

Em muitas aplica√ß√µes, a condi√ß√£o de n√£o correla√ß√£o √© suficiente para garantir a validade dos resultados. No entanto, em situa√ß√µes onde a modelagem da depend√™ncia n√£o linear √© importante, √© crucial considerar a possibilidade de que o processo n√£o seja white noise independente.

**Corol√°rio 1:** Seja {Œµ‚Çú} um white noise independente com m√©dia zero e vari√¢ncia œÉ¬≤. Se $g_1, g_2, ..., g_n$ s√£o fun√ß√µes mensur√°veis tais que $E[g_i(\varepsilon_{t_i})^2] < \infty$ para todo $i$, e $t_1, t_2, ..., t_n$ s√£o distintos, ent√£o as vari√°veis aleat√≥rias $g_1(\varepsilon_{t_1}), g_2(\varepsilon_{t_2}), ..., g_n(\varepsilon_{t_n})$ s√£o independentes.

*Prova:* Este resultado segue diretamente da defini√ß√£o de white noise independente. Como as vari√°veis $\varepsilon_{t_1}, \varepsilon_{t_2}, \ldots, \varepsilon_{t_n}$ s√£o independentes, qualquer fun√ß√£o mensur√°vel dessas vari√°veis tamb√©m ser√° independente, desde que os momentos de segunda ordem existam. $\blacksquare$

### Conclus√£o

O **white noise independente** representa uma condi√ß√£o mais forte do que a mera n√£o correla√ß√£o. Embora a n√£o correla√ß√£o seja suficiente em muitas aplica√ß√µes pr√°ticas, a independ√™ncia estat√≠stica √© crucial para certas deriva√ß√µes te√≥ricas e modelagens de depend√™ncia n√£o linear. Compreender a distin√ß√£o entre essas condi√ß√µes √© fundamental para uma an√°lise rigorosa de s√©ries temporais.
<!-- END -->