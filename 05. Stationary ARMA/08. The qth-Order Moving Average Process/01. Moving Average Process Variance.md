## T√≠tulo Conciso
### The qth-Order Moving Average Process: Defini√ß√£o e Vari√¢ncia

### Introdu√ß√£o
Em continuidade ao estudo de processos estoc√°sticos estacion√°rios, este cap√≠tulo se aprofunda nos processos de m√©dias m√≥veis (MA). Anteriormente, foram introduzidos conceitos como a estacionariedade, a fun√ß√£o de autocovari√¢ncia e ru√≠do branco [^4]. Agora, focaremos especificamente nos processos MA de ordem *q*, explorando sua defini√ß√£o formal e derivando uma express√£o detalhada para sua vari√¢ncia.

### Conceitos Fundamentais
Um processo de m√©dias m√≥veis de ordem *q*, denotado como **MA(q)**, √© definido como uma combina√ß√£o linear ponderada dos *q* valores mais recentes de um processo de ru√≠do branco, somada a uma m√©dia constante $\mu$ [^7]. Formalmente, o processo MA(q) √© expresso como:

$$Y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \ldots + \theta_q \varepsilon_{t-q}$$

onde:
*   $Y_t$ representa o valor do processo no instante *t*
*   $\mu$ √© a m√©dia do processo
*   $\varepsilon_t$ √© um processo de ru√≠do branco, que satisfaz as seguintes condi√ß√µes [^4]:
    *   $E(\varepsilon_t) = 0$
    *   $E(\varepsilon_t \varepsilon_\tau) = 0$ para $t \ne \tau$
    *   $E(\varepsilon_t^2) = \sigma^2$
*   $\theta_1, \theta_2, \ldots, \theta_q$ s√£o os coeficientes das m√©dias m√≥veis, representando os pesos atribu√≠dos aos valores passados do ru√≠do branco.

> üí° **Exemplo Num√©rico:** Considere um processo MA(1) com $\mu = 10$, $\theta_1 = 0.5$, e $\sigma^2 = 1$. Se $\varepsilon_t = 0.8$ e $\varepsilon_{t-1} = -0.4$, ent√£o o valor do processo no instante *t* √©:
> $Y_t = 10 + 0.8 + 0.5 * (-0.4) = 10 + 0.8 - 0.2 = 10.6$
>
> Este exemplo ilustra como o valor atual do processo MA(1) √© influenciado pelo ru√≠do branco atual e pelo ru√≠do branco do per√≠odo anterior, ponderado pelo coeficiente $\theta_1$.

**Observa√ß√£o:** √â importante notar que, devido √† natureza da combina√ß√£o linear de ru√≠do branco, um processo MA(q) √© sempre estacion√°rio. Isso decorre do fato de que a m√©dia e a autocovari√¢ncia do processo MA(q) n√£o dependem do tempo *t*.

Para complementar a defini√ß√£o do processo MA(q), √© √∫til considerar sua representa√ß√£o utilizando o operador de retardo (backshift operator) *B*, definido como $B\varepsilon_t = \varepsilon_{t-1}$. Podemos reescrever o processo MA(q) na forma:

$$Y_t = \mu + (1 + \theta_1 B + \theta_2 B^2 + \ldots + \theta_q B^q)\varepsilon_t$$

Definindo o polin√¥mio $\Theta(B) = 1 + \theta_1 B + \theta_2 B^2 + \ldots + \theta_q B^q$, o processo MA(q) pode ser compactamente representado como:

$$Y_t = \mu + \Theta(B)\varepsilon_t$$

Essa nota√ß√£o simplificada ser√° √∫til em an√°lises futuras e na compara√ß√£o com outros processos estoc√°sticos.

A **vari√¢ncia** do processo MA(q), denotada por $\gamma_0$, √© definida como o valor esperado do quadrado do desvio de $Y_t$ em rela√ß√£o √† sua m√©dia $\mu$ [^7]:

$$\gamma_0 = E[(Y_t - \mu)^2]$$

Substituindo a defini√ß√£o de $Y_t$ do processo MA(q), temos:

$$\gamma_0 = E[(\varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \ldots + \theta_q \varepsilon_{t-q})^2]$$

Expandindo o quadrado, obtemos uma soma de termos envolvendo produtos de diferentes $\varepsilon_t$. Devido √†s propriedades do ru√≠do branco, o valor esperado de qualquer produto $\varepsilon_t \varepsilon_\tau$ √© zero se $t \ne \tau$, e $\sigma^2$ se $t = \tau$ [^4]. Portanto, apenas os termos quadrados permanecem na express√£o para $\gamma_0$:

$$\gamma_0 = E[\varepsilon_t^2 + \theta_1^2 \varepsilon_{t-1}^2 + \theta_2^2 \varepsilon_{t-2}^2 + \ldots + \theta_q^2 \varepsilon_{t-q}^2]$$

Aplicando a propriedade $E(\varepsilon_t^2) = \sigma^2$ para todo *t*, obtemos a express√£o final para a vari√¢ncia do processo MA(q):

$$\gamma_0 = \sigma^2 + \theta_1^2 \sigma^2 + \theta_2^2 \sigma^2 + \ldots + \theta_q^2 \sigma^2$$
$$\gamma_0 = \sigma^2 (1 + \theta_1^2 + \theta_2^2 + \ldots + \theta_q^2)$$
$$\gamma_0 = \sigma^2 \left(1 + \sum_{i=1}^{q} \theta_i^2\right)$$

> üí° **Exemplo Num√©rico:** Considere um processo MA(2) com $\sigma^2 = 4$, $\theta_1 = 0.6$ e $\theta_2 = 0.4$. A vari√¢ncia do processo √©:
> $\gamma_0 = 4 * (1 + 0.6^2 + 0.4^2) = 4 * (1 + 0.36 + 0.16) = 4 * 1.52 = 6.08$
>
> Isso significa que a dispers√£o dos valores $Y_t$ em torno da m√©dia $\mu$ √© 6.08. A vari√¢ncia √© maior do que a vari√¢ncia do ru√≠do branco ($\sigma^2 = 4$) devido √† influ√™ncia dos coeficientes $\theta_1$ e $\theta_2$.

Para tornar a deriva√ß√£o da vari√¢ncia mais rigorosa, apresentamos a prova passo a passo:

Prova:
Queremos provar que a vari√¢ncia $\gamma_0$ de um processo MA(q) √© dada por:

$$\gamma_0 = \sigma^2 \left(1 + \sum_{i=1}^{q} \theta_i^2\right)$$

I. Iniciamos com a defini√ß√£o da vari√¢ncia:
   $$\gamma_0 = E[(Y_t - \mu)^2]$$

II. Substitu√≠mos $Y_t$ pela defini√ß√£o do processo MA(q):
    $$Y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \ldots + \theta_q \varepsilon_{t-q}$$
    Assim, $Y_t - \mu = \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \ldots + \theta_q \varepsilon_{t-q}$.

III. Substitu√≠mos $(Y_t - \mu)$ na equa√ß√£o da vari√¢ncia:
     $$\gamma_0 = E\left[\left(\varepsilon_t + \sum_{i=1}^{q} \theta_i \varepsilon_{t-i}\right)^2\right]$$

IV. Expandimos o quadrado:
    $$\gamma_0 = E\left[\varepsilon_t^2 + 2\varepsilon_t \left(\sum_{i=1}^{q} \theta_i \varepsilon_{t-i}\right) + \left(\sum_{i=1}^{q} \theta_i \varepsilon_{t-i}\right)^2\right]$$

V. Expandimos o termo do somat√≥rio ao quadrado:
   $$\gamma_0 = E\left[\varepsilon_t^2 + 2\sum_{i=1}^{q} \theta_i \varepsilon_t \varepsilon_{t-i} + \sum_{i=1}^{q} \sum_{j=1}^{q} \theta_i \theta_j \varepsilon_{t-i} \varepsilon_{t-j}\right]$$

VI. Aplicamos a esperan√ßa e utilizamos as propriedades do ru√≠do branco: $E[\varepsilon_t] = 0$, $E[\varepsilon_t \varepsilon_\tau] = 0$ para $t \ne \tau$ e $E[\varepsilon_t^2] = \sigma^2$:
    $$E[\varepsilon_t^2] = \sigma^2$$
    $$E[\varepsilon_t \varepsilon_{t-i}] = 0 \text{ para } i \ne 0$$
    $$E[\varepsilon_{t-i} \varepsilon_{t-j}] = \begin{cases} \sigma^2, & \text{se } i = j \\ 0, & \text{se } i \ne j \end{cases}$$

VII. Aplicando as propriedades do ru√≠do branco na equa√ß√£o da vari√¢ncia:
     $$\gamma_0 = E[\varepsilon_t^2] + 2\sum_{i=1}^{q} \theta_i E[\varepsilon_t \varepsilon_{t-i}] + \sum_{i=1}^{q} \sum_{j=1}^{q} \theta_i \theta_j E[\varepsilon_{t-i} \varepsilon_{t-j}]$$
     $$\gamma_0 = \sigma^2 + 2\sum_{i=1}^{q} \theta_i (0) + \sum_{i=1}^{q} \theta_i^2 \sigma^2$$
     $$\gamma_0 = \sigma^2 + \sum_{i=1}^{q} \theta_i^2 \sigma^2$$

VIII. Fatorando $\sigma^2$:
      $$\gamma_0 = \sigma^2 \left(1 + \sum_{i=1}^{q} \theta_i^2\right)$$

Assim, provamos que a vari√¢ncia do processo MA(q) √© dada por $\gamma_0 = \sigma^2 \left(1 + \sum_{i=1}^{q} \theta_i^2\right)$. ‚ñ†

Esta f√≥rmula expressa a vari√¢ncia do processo MA(q) em termos da vari√¢ncia do ru√≠do branco ($\sigma^2$) e dos coeficientes das m√©dias m√≥veis ($\theta_i$).

Agora, vamos derivar a fun√ß√£o de autocovari√¢ncia para um processo MA(q).

**Teorema 1**
A fun√ß√£o de autocovari√¢ncia $\gamma_k$ de um processo MA(q) √© dada por:

$$\gamma_k = \begin{cases}
\sigma^2 \left( \sum_{i=0}^{q-k} \theta_i \theta_{i+k} \right), & \text{para } 0 \le k \le q \\
0, & \text{para } k > q
\end{cases}$$

onde $\theta_0 = 1$.

*Proof:*
A autocovari√¢ncia de ordem *k* √© definida como $\gamma_k = E[(Y_t - \mu)(Y_{t-k} - \mu)]$. Substituindo a defini√ß√£o de $Y_t$:

$$\gamma_k = E\left[ \left( \sum_{i=0}^{q} \theta_i \varepsilon_{t-i} \right) \left( \sum_{j=0}^{q} \theta_j \varepsilon_{t-k-j} \right) \right]$$

Expandindo o produto e usando as propriedades do ru√≠do branco, $E[\varepsilon_t \varepsilon_\tau] = 0$ para $t \ne \tau$ e $E[\varepsilon_t^2] = \sigma^2$, obtemos:

$$\gamma_k = \sigma^2 \sum_{i=0}^{q} \sum_{j=0}^{q} \theta_i \theta_j \delta_{i, k+j}$$

onde $\delta_{i, k+j}$ √© a fun√ß√£o delta de Kronecker, que √© igual a 1 se $i = k+j$ e 0 caso contr√°rio.  Fazendo a substitui√ß√£o $i = k + j$, temos $j = i-k$ e portanto:

$$\gamma_k = \sigma^2 \sum_{i=0}^{q} \theta_i \theta_{i-k}$$

Note que $\theta_i = 0$ se $i < 0$ ou $i > q$. Assim, a soma acima somente √© diferente de zero quando $0 \le i \le q$ e $0 \le i - k \le q$, o que implica $k \le i \le q+k$. Combinando as restri√ß√µes em *i* temos $\max(0, k) \le i \le \min(q, q+k)$.  Se $k > q$, ent√£o n√£o existe nenhum valor de *i* que satisfa√ßa a condi√ß√£o.  Se $0 \le k \le q$ ent√£o $k \le i \le q$ e definindo $i' = i-k$ temos:

$$ \gamma_k = \sigma^2 \sum_{i'=0}^{q-k} \theta_{i'+k} \theta_{i'}$$

Finalmente, renomeando $i'$ para $i$, obtemos o resultado desejado. $\Box$

> üí° **Exemplo Num√©rico:** Para um processo MA(1) com $\theta_1 = 0.7$ e $\sigma^2 = 1$, calcule a autocovari√¢ncia $\gamma_1$.
>
> Usando a f√≥rmula: $\gamma_k = \sigma^2 \left( \sum_{i=0}^{q-k} \theta_i \theta_{i+k} \right)$, para $k=1$, temos:
> $\gamma_1 = \sigma^2 (\theta_0 \theta_{1}) = 1 * (1 * 0.7) = 0.7$
>
> Para $k > 1$, $\gamma_k = 0$. Portanto, a autocovari√¢ncia no lag 1 √© 0.7, indicando uma correla√ß√£o entre $Y_t$ e $Y_{t-1}$.

**Corol√°rio 1.1**
A fun√ß√£o de autocorrela√ß√£o $\rho_k$ de um processo MA(q) √© dada por:

$$\rho_k = \begin{cases}
\frac{\sum_{i=0}^{q-k} \theta_i \theta_{i+k}}{1 + \sum_{i=1}^{q} \theta_i^2}, & \text{para } 0 \le k \le q \\
0, & \text{para } k > q
\end{cases}$$

*Proof:*
A fun√ß√£o de autocorrela√ß√£o √© definida como $\rho_k = \frac{\gamma_k}{\gamma_0}$. Usando os resultados do Teorema 1 e a express√£o para $\gamma_0$ derivada anteriormente, obtemos o resultado desejado. $\Box$

> üí° **Exemplo Num√©rico:**  Usando o exemplo anterior de um processo MA(1) com $\theta_1 = 0.7$ e $\sigma^2 = 1$, calcule a autocorrela√ß√£o $\rho_1$. Primeiro, calculamos $\gamma_0 = \sigma^2(1 + \theta_1^2) = 1 * (1 + 0.7^2) = 1.49$.
>
> Ent√£o, $\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{0.7}{1.49} \approx 0.4698$.
>
> Isso indica que a correla√ß√£o entre os valores do processo em lags consecutivos √© de aproximadamente 0.47.

**Observa√ß√£o:** Uma propriedade importante dos processos MA(q) √© que sua fun√ß√£o de autocorrela√ß√£o se anula ap√≥s o lag *q*. Essa caracter√≠stica √© fundamental na identifica√ß√£o e modelagem de s√©ries temporais que exibem esse tipo de comportamento.

**Teorema 2** (Invertibilidade)
Um processo MA(q) √© dito invert√≠vel se ele pode ser reescrito como um processo autorregressivo de ordem infinita (AR($\infty$)). A condi√ß√£o necess√°ria e suficiente para a invertibilidade √© que as ra√≠zes do polin√¥mio $\Theta(B) = 1 + \theta_1 B + \theta_2 B^2 + \ldots + \theta_q B^q$ estejam fora do c√≠rculo unit√°rio, ou seja, $|z| > 1$ para todas as ra√≠zes *z* de $\Theta(z) = 0$.

> üí° **Exemplo Num√©rico:** Considere um processo MA(1) com $\theta_1 = 0.8$. O polin√¥mio √© $\Theta(B) = 1 + 0.8B$. Para encontrar a raiz, resolvemos $\Theta(z) = 1 + 0.8z = 0$, o que d√° $z = -1/0.8 = -1.25$. Como $|-1.25| > 1$, o processo √© invert√≠vel.
>
> Se $\theta_1 = 1.25$, ent√£o $z = -1/1.25 = -0.8$, e $|-0.8| < 1$, o que significa que o processo n√£o √© invert√≠vel.

A invertibilidade √© uma propriedade importante porque garante que podemos expressar o ru√≠do branco $\varepsilon_t$ em termos dos valores passados do processo $Y_t$. Processos n√£o invert√≠veis podem levar a ambiguidades na estima√ß√£o dos par√¢metros e na previs√£o.

### Conclus√£o
Neste cap√≠tulo, apresentamos a defini√ß√£o formal de um processo de m√©dias m√≥veis de ordem *q*, o MA(q), e derivamos uma express√£o expl√≠cita para sua vari√¢ncia [^7]. A vari√¢ncia do processo MA(q) depende diretamente da vari√¢ncia do ru√≠do branco subjacente e da magnitude dos coeficientes das m√©dias m√≥veis. Adicionalmente, derivamos express√µes para a fun√ß√£o de autocovari√¢ncia e autocorrela√ß√£o do processo MA(q). Este resultado ser√° fundamental para a an√°lise posterior das propriedades estat√≠sticas e da modelagem de s√©ries temporais usando processos MA(q). Tamb√©m introduzimos o conceito de invertibilidade, crucial para a modelagem e interpreta√ß√£o de processos MA(q).

### Refer√™ncias
[^4]: Basic definitions and properties of white noise and autocovariance
[^7]: Defining the MA(q) process and establishing the formula for its variance.
<!-- END -->