## The qth-Order Moving Average Process: Autocovariances

### Introdu√ß√£o
Em continuidade ao estudo dos processos de m√©dias m√≥veis (MA) de ordem *q*, e tendo estabelecido a sua defini√ß√£o, representa√ß√£o, vari√¢ncia e autocorrela√ß√£o [^7], este cap√≠tulo foca-se na deriva√ß√£o e an√°lise das autocovari√¢ncias para este tipo de processo. Ser√° demonstrado como as autocovari√¢ncias de um processo MA(q) se comportam em fun√ß√£o do *lag* *j* e como elas se anulam para lags superiores √† ordem *q* do processo.

### Conceitos Fundamentais

Relembrando, um processo MA(q) √© definido como:

$$Y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \ldots + \theta_q \varepsilon_{t-q}$$

onde $\varepsilon_t$ √© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, ou seja, $E[\varepsilon_t] = 0$, $E[\varepsilon_t \varepsilon_s] = 0 \ \forall \ t \neq s$ e $E[\varepsilon_t^2] = \sigma^2$ [^4].

**Proposi√ß√£o 1**

A m√©dia de um processo MA(q) √© dada por $E[Y_t] = \mu$.

*Proof:*
Tomando a esperan√ßa da defini√ß√£o do processo MA(q):
$$E[Y_t] = E\left[\mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \ldots + \theta_q \varepsilon_{t-q}\right]$$
Como a esperan√ßa √© um operador linear e $E[\varepsilon_t] = 0$ para todo *t*, temos:
$$E[Y_t] = E[\mu] + E[\varepsilon_t] + \theta_1 E[\varepsilon_{t-1}] + \ldots + \theta_q E[\varepsilon_{t-q}] = \mu + 0 + \ldots + 0 = \mu$$
Portanto, $E[Y_t] = \mu$. $\Box$

A *autocovari√¢ncia* de ordem *j* ($\gamma_j$) de um processo $Y_t$ √© definida como a covari√¢ncia entre $Y_t$ e $Y_{t-j}$:

$$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$$

Para o processo MA(q), podemos substituir $Y_t$ e $Y_{t-j}$ na defini√ß√£o de autocovari√¢ncia:

$$\gamma_j = E\left[ \left(\varepsilon_t + \sum_{i=1}^{q} \theta_i \varepsilon_{t-i}\right) \left(\varepsilon_{t-j} + \sum_{k=1}^{q} \theta_k \varepsilon_{t-j-k}\right) \right]$$

Expandindo esta express√£o, obtemos:

$$\gamma_j = E\left[ \varepsilon_t \varepsilon_{t-j} + \varepsilon_t \sum_{k=1}^{q} \theta_k \varepsilon_{t-j-k} + \varepsilon_{t-j} \sum_{i=1}^{q} \theta_i \varepsilon_{t-i} + \left(\sum_{i=1}^{q} \theta_i \varepsilon_{t-i}\right) \left(\sum_{k=1}^{q} \theta_k \varepsilon_{t-j-k}\right) \right]$$

Aplicando a propriedade de que $E[\varepsilon_t \varepsilon_s] = 0$ para $t \neq s$ e $E[\varepsilon_t^2] = \sigma^2$, os termos que envolvem diferentes instantes de tempo de $\varepsilon_t$ desaparecem. Para simplificar, podemos analisar as autocovari√¢ncias em dois cen√°rios: $j \le q$ e $j > q$.

**Caso 1: $j \le q$**

Neste caso, alguns termos na expans√£o do produto ser√£o n√£o nulos. Reorganizando a express√£o, obtemos:

$$\gamma_j = E\left[ \left(\varepsilon_t + \sum_{i=1}^{q} \theta_i \varepsilon_{t-i}\right) \left(\varepsilon_{t-j} + \sum_{k=1}^{q} \theta_k \varepsilon_{t-j-k}\right) \right]$$

*Proof:*

I.  Come√ßamos com a defini√ß√£o da autocovari√¢ncia para um processo MA(q):
    $$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$$

II. Substitu√≠mos $Y_t$ e $Y_{t-j}$ pelas suas representa√ß√µes em termos de $\varepsilon_t$:
    $$\gamma_j = E\left[\left(\sum_{i=0}^{q} \theta_i \varepsilon_{t-i}\right)\left(\sum_{k=0}^{q} \theta_k \varepsilon_{t-j-k}\right)\right]$$
    onde $\theta_0 = 1$.

III. Expandimos o produto das somas:
     $$\gamma_j = E\left[\sum_{i=0}^{q} \sum_{k=0}^{q} \theta_i \theta_k \varepsilon_{t-i} \varepsilon_{t-j-k}\right]$$

IV. Usamos a propriedade da esperan√ßa para passar para dentro da soma:
    $$\gamma_j = \sum_{i=0}^{q} \sum_{k=0}^{q} \theta_i \theta_k E[\varepsilon_{t-i} \varepsilon_{t-j-k}]$$

V.  Como $E[\varepsilon_{t-i} \varepsilon_{t-j-k}] = 0$ se $i \neq j+k$ e $E[\varepsilon_{t-i} \varepsilon_{t-j-k}] = \sigma^2$ se $i = j+k$, ent√£o a soma dupla se reduz a uma soma simples sobre os termos onde $i = j+k$:
    $$\gamma_j = \sigma^2 \sum_{\{i, k : i = j+k\}} \theta_i \theta_k$$

VI. Fazemos a mudan√ßa de vari√°vel $k = i-j$, notando que $0 \le k \le q$ implica $0 \le i-j \le q$, ou seja, $j \le i \le q+j$. Como $0 \le i \le q$, ent√£o devemos ter $j \le i \le q$.
   $$\gamma_j = \sigma^2 \sum_{i=j}^{q} \theta_i \theta_{i-j}$$

VII. Fazemos a mudan√ßa de √≠ndice $i' = i-j$, de modo que $i = i'+j$. Quando $i=j$, $i'=0$ e quando $i=q$, $i' = q-j$. Assim,
     $$\gamma_j = \sigma^2 \sum_{i'=0}^{q-j} \theta_{i'+j} \theta_{i'}$$

VIII. Renomeamos $i'$ para $i$:
    $$\gamma_j = \sigma^2 \sum_{i=0}^{q-j} \theta_i \theta_{i+j}$$

Portanto,
$$\gamma_j = \sigma^2 \sum_{i=0}^{q-j} \theta_i \theta_{i+j} $$ ‚ñ†

Ap√≥s aplicar a propriedade da esperan√ßa e do ru√≠do branco, obtemos:

$$\gamma_j = \sigma^2 \sum_{i=0}^{q-j} \theta_i \theta_{i+j} $$

onde $\theta_0 = 1$. Note que os coeficientes $\theta_i$ s√£o definidos apenas para $i = 1, \ldots, q$.

> üí° **Exemplo Num√©rico:** Considere um processo MA(1) com $\theta_1 = 0.6$ e $\sigma^2 = 4$. Vamos calcular $\gamma_0$ e $\gamma_1$.
>
> *   $\gamma_0 = \sigma^2(1 + \theta_1^2) = 4 * (1 + 0.6^2) = 4 * (1 + 0.36) = 4 * 1.36 = 5.44$
> *   $\gamma_1 = \sigma^2(\theta_1) = 4 * 0.6 = 2.4$
>
> Para $j > 1$, $\gamma_j = 0$. Este exemplo ilustra como a autocovari√¢ncia decai rapidamente para zero ap√≥s o primeiro lag em um processo MA(1).

**Caso 2: $j > q$**

Neste caso, nenhum termo na expans√£o do produto ser√° n√£o nulo, pois os *lags* em $\varepsilon_t$ sempre ser√£o diferentes. Portanto:

$$\gamma_j = 0$$

*Proof:*
I.  Come√ßamos com a defini√ß√£o da autocovari√¢ncia para um processo MA(q):
    $$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$$

II. Substitu√≠mos $Y_t$ e $Y_{t-j}$ pelas suas representa√ß√µes em termos de $\varepsilon_t$:
    $$\gamma_j = E\left[\left(\sum_{i=0}^{q} \theta_i \varepsilon_{t-i}\right)\left(\sum_{k=0}^{q} \theta_k \varepsilon_{t-j-k}\right)\right]$$
    onde $\theta_0 = 1$.

III. Expandimos o produto das somas:
     $$\gamma_j = E\left[\sum_{i=0}^{q} \sum_{k=0}^{q} \theta_i \theta_k \varepsilon_{t-i} \varepsilon_{t-j-k}\right]$$

IV. Usamos a propriedade da esperan√ßa para passar para dentro da soma:
    $$\gamma_j = \sum_{i=0}^{q} \sum_{k=0}^{q} \theta_i \theta_k E[\varepsilon_{t-i} \varepsilon_{t-j-k}]$$

V.  Como $j > q$, ent√£o para quaisquer $i$ e $k$ entre $0$ e $q$, $t-i \neq t-j-k$, o que implica que $E[\varepsilon_{t-i} \varepsilon_{t-j-k}] = 0$.

VI. Portanto,
    $$\gamma_j = \sum_{i=0}^{q} \sum_{k=0}^{q} \theta_i \theta_k \cdot 0 = 0$$ ‚ñ†

Resumindo, as autocovari√¢ncias para um processo MA(q) s√£o dadas por:

$$\gamma_j = \begin{cases}
\sigma^2 \left( \sum_{i=0}^{q-j} \theta_i \theta_{i+j} \right), & \text{para } 0 \le j \le q \\
0, & \text{para } j > q
\end{cases}$$

> üí° **Exemplo Num√©rico:** Considere um processo MA(2) com $\theta_1 = 0.5$, $\theta_2 = 0.3$ e $\sigma^2 = 1$. Vamos calcular $\gamma_0$, $\gamma_1$ e $\gamma_2$.
>
> *   $\gamma_0 = \sigma^2(1 + \theta_1^2 + \theta_2^2) = 1 * (1 + 0.5^2 + 0.3^2) = 1 + 0.25 + 0.09 = 1.34$
> *   $\gamma_1 = \sigma^2(\theta_0 \theta_1 + \theta_1 \theta_2) = 1 * (1 * 0.5 + 0.5 * 0.3) = 0.5 + 0.15 = 0.65$
> *   $\gamma_2 = \sigma^2(\theta_0 \theta_2) = 1 * (1 * 0.3) = 0.3$
>
> Para $j > 2$, $\gamma_j = 0$.
>
> üí° **Exemplo Num√©rico:** Python code to calculate autocovariances for MA(2)
> ```python
> import numpy as np
>
> def autocovariance_ma2(sigma_squared, theta1, theta2, j):
>     if j == 0:
>         return sigma_squared * (1 + theta1**2 + theta2**2)
>     elif j == 1:
>         return sigma_squared * (theta1 + theta1 * theta2)
>     elif j == 2:
>         return sigma_squared * theta2
>     else:
>         return 0
>
> sigma_squared = 1
> theta1 = 0.5
> theta2 = 0.3
>
> gamma0 = autocovariance_ma2(sigma_squared, theta1, theta2, 0)
> gamma1 = autocovariance_ma2(sigma_squared, theta1, theta2, 1)
> gamma2 = autocovariance_ma2(sigma_squared, theta1, theta2, 2)
> gamma3 = autocovariance_ma2(sigma_squared, theta1, theta2, 3)
>
> print(f"Gamma_0: {gamma0}")
> print(f"Gamma_1: {gamma1}")
> print(f"Gamma_2: {gamma2}")
> print(f"Gamma_3: {gamma3}")
> ```

**Observa√ß√£o:** A principal caracter√≠stica dos processos MA(q) √© que suas autocovari√¢ncias se tornam zero ap√≥s o *lag* *q*. Isso significa que a depend√™ncia entre $Y_t$ e $Y_{t-j}$ √© nula para $j > q$. Essa propriedade √© fundamental para identificar a ordem de um processo MA em uma s√©rie temporal real.

**Corol√°rio 1**
Para um processo MA(1), as autocovari√¢ncias s√£o dadas por:
$$\gamma_j = \begin{cases}
\sigma^2(1 + \theta_1^2), & \text{para } j = 0 \\
\sigma^2 \theta_1, & \text{para } j = 1 \\
0, & \text{para } j > 1
\end{cases}$$

*Proof:*
Basta substituir $q=1$ na f√≥rmula geral das autocovari√¢ncias de um MA(q). Para $j=0$, temos $\gamma_0 = \sigma^2 \sum_{i=0}^{1-0} \theta_i \theta_{i+0} = \sigma^2 (\theta_0 \theta_0 + \theta_1 \theta_1) = \sigma^2(1 + \theta_1^2)$. Para $j=1$, temos $\gamma_1 = \sigma^2 \sum_{i=0}^{1-1} \theta_i \theta_{i+1} = \sigma^2 (\theta_0 \theta_1) = \sigma^2 \theta_1$. Para $j>1$, $\gamma_j = 0$ pela defini√ß√£o do processo MA(q). $\Box$

**Lema 1**

A fun√ß√£o de autocovari√¢ncia $\gamma_j$ de um processo MA(q) √© sim√©trica, ou seja, $\gamma_j = \gamma_{-j}$.

*Proof:*
Dado que a autocovari√¢ncia √© definida como $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$, podemos reescrever $\gamma_{-j}$ como:
$$\gamma_{-j} = E[(Y_t - \mu)(Y_{t+j} - \mu)] = E[(Y_{t-j} - \mu)(Y_t - \mu)] = \gamma_j$$
Assim, demonstrando que $\gamma_j = \gamma_{-j}$ para qualquer processo estacion√°rio. $\Box$

**Teorema 1**
Se $Y_t$ √© um processo MA(q), ent√£o o processo $Z_t = Y_t - Y_{t-1}$ √© um processo MA(q+1).

*Proof:*
Dado que $Y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \ldots + \theta_q \varepsilon_{t-q}$, ent√£o
$Y_{t-1} = \mu + \varepsilon_{t-1} + \theta_1 \varepsilon_{t-2} + \theta_2 \varepsilon_{t-3} + \ldots + \theta_q \varepsilon_{t-q-1}$.
Subtraindo $Y_{t-1}$ de $Y_t$, obtemos:
$Z_t = Y_t - Y_{t-1} = (\varepsilon_t - \varepsilon_{t-1}) + \theta_1 (\varepsilon_{t-1} - \varepsilon_{t-2}) + \ldots + \theta_q(\varepsilon_{t-q} - \varepsilon_{t-q-1}) = \varepsilon_t + (\theta_1 - 1) \varepsilon_{t-1} + (\theta_2 - \theta_1) \varepsilon_{t-2} + \ldots + (\theta_q - \theta_{q-1})\varepsilon_{t-q} - \theta_q \varepsilon_{t-q-1}$.
Fazendo $\psi_0 = 1$, $\psi_1 = \theta_1 - 1$, $\psi_2 = \theta_2 - \theta_1$, ..., $\psi_q = \theta_q - \theta_{q-1}$, e $\psi_{q+1} = -\theta_q$, temos:
$Z_t = \varepsilon_t + \psi_1 \varepsilon_{t-1} + \psi_2 \varepsilon_{t-2} + \ldots + \psi_q \varepsilon_{t-q} + \psi_{q+1} \varepsilon_{t-q-1}$,
que √© a forma de um processo MA(q+1). $\Box$

Para complementar a an√°lise das autocovari√¢ncias, podemos tamb√©m estudar a fun√ß√£o de autocorrela√ß√£o.

**Defini√ß√£o:** A fun√ß√£o de autocorrela√ß√£o (ACF) de ordem *j* ($\rho_j$) de um processo $Y_t$ √© definida como a autocovari√¢ncia de ordem *j* dividida pela vari√¢ncia do processo:

$$\rho_j = \frac{\gamma_j}{\gamma_0}$$

**Proposi√ß√£o 2**
A fun√ß√£o de autocorrela√ß√£o (ACF) de um processo MA(q) √© dada por:

$$\rho_j = \begin{cases}
\frac{\sum_{i=0}^{q-j} \theta_i \theta_{i+j}}{ \sum_{i=0}^{q} \theta_i^2}, & \text{para } 0 \le j \le q \\
0, & \text{para } j > q
\end{cases}$$

onde $\theta_0 = 1$.

*Proof:*
A prova segue diretamente da defini√ß√£o da fun√ß√£o de autocorrela√ß√£o e da f√≥rmula das autocovari√¢ncias de um processo MA(q). Dividindo $\gamma_j$ por $\gamma_0$, temos:
Para $0 \le j \le q$:
$$\rho_j = \frac{\gamma_j}{\gamma_0} = \frac{\sigma^2 \left( \sum_{i=0}^{q-j} \theta_i \theta_{i+j} \right)}{\sigma^2 \left( \sum_{i=0}^{q} \theta_i^2 \right)} = \frac{\sum_{i=0}^{q-j} \theta_i \theta_{i+j}}{ \sum_{i=0}^{q} \theta_i^2}$$
Para $j > q$, $\gamma_j = 0$, ent√£o $\rho_j = 0$. $\Box$

**Corol√°rio 2**
Para um processo MA(1), a fun√ß√£o de autocorrela√ß√£o (ACF) √© dada por:

$$\rho_j = \begin{cases}
1, & \text{para } j = 0 \\
\frac{\theta_1}{1 + \theta_1^2}, & \text{para } j = 1 \\
0, & \text{para } j > 1
\end{cases}$$

*Proof:*
Basta substituir $q=1$ na f√≥rmula geral da ACF de um MA(q).
Para $j = 0$, $\rho_0 = \frac{\gamma_0}{\gamma_0} = 1$.
Para $j = 1$, $\rho_1 = \frac{\theta_1}{1 + \theta_1^2}$.
Para $j > 1$, $\rho_j = 0$. $\Box$

> üí° **Exemplo Num√©rico:** Considere um processo MA(1) com $\theta_1 = 0.6$. Vamos calcular $\rho_0$ e $\rho_1$.
>
> *   $\rho_0 = 1$
> *   $\rho_1 = \frac{\theta_1}{1 + \theta_1^2} = \frac{0.6}{1 + 0.6^2} = \frac{0.6}{1 + 0.36} = \frac{0.6}{1.36} \approx 0.441$
>
> Para $j > 1$, $\rho_j = 0$. Este exemplo mostra como a autocorrela√ß√£o no lag 1 est√° relacionada ao coeficiente $\theta_1$ no processo MA(1).

### Conclus√£o

Neste cap√≠tulo, derivamos a express√£o para as autocovari√¢ncias de um processo MA(q) e mostramos que elas se anulam para *lags* maiores que a ordem *q* do processo [^7]. Essa propriedade √© uma caracter√≠stica definidora dos processos MA(q) e √© crucial para sua identifica√ß√£o em aplica√ß√µes pr√°ticas. O entendimento das autocovari√¢ncias √© fundamental para a modelagem, previs√£o e an√°lise de s√©ries temporais utilizando processos de m√©dias m√≥veis.

### Refer√™ncias
[^4]: Basic definitions and properties of white noise and autocovariance
[^7]: Defining the MA(q) process and establishing the formula for its variance.
<!-- END -->