## Autoregressive Processes: Deriving Moments of an AR(1) Process

### Introdu√ß√£o
Este cap√≠tulo se dedica √† deriva√ß√£o dos momentos de um processo autorregressivo de primeira ordem (AR(1)), uma tarefa fundamental na an√°lise de s√©ries temporais [^53]. Assumindo a estacionariedade de covari√¢ncia, somos capazes de calcular os momentos diretamente a partir da equa√ß√£o de diferen√ßa que define o processo. Especificamente, focaremos na deriva√ß√£o da m√©dia ao tomar expectativas da equa√ß√£o AR(1) [^53].

### Conceitos Fundamentais

Retomemos o processo AR(1) definido por [^46]:

$$Y_t = c + \phi Y_{t-1} + \epsilon_t,$$

onde:
*   $Y_t$ √© o valor da s√©rie temporal no tempo $t$.
*   $c$ √© uma constante.
*   $\phi$ √© o par√¢metro autorregressivo.
*   $\epsilon_t$ √© um termo de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$ [^47].

A **estacionariedade de covari√¢ncia** √© uma suposi√ß√£o crucial para a deriva√ß√£o dos momentos [^53]. Sob esta suposi√ß√£o, a m√©dia e a vari√¢ncia do processo s√£o constantes ao longo do tempo. Formalmente, isto significa que:

$$E[Y_t] = E[Y_{t-1}] = \mu,$$
$$Var[Y_t] = Var[Y_{t-1}] = \gamma_0,$$

onde $\mu$ √© a m√©dia do processo e $\gamma_0$ √© a vari√¢ncia [^45]. Al√©m da estacionariedade de covari√¢ncia, podemos tamb√©m definir a fun√ß√£o de autocovari√¢ncia.

**Defini√ß√£o:** A fun√ß√£o de autocovari√¢ncia $\gamma_k$ de um processo estacion√°rio AR(1) √© definida como:
$$\gamma_k = Cov(Y_t, Y_{t-k}) = E[(Y_t - \mu)(Y_{t-k} - \mu)],$$
onde $k$ √© o lag (atraso) entre as observa√ß√µes $Y_t$ e $Y_{t-k}$.

> üí° **Exemplo Num√©rico:**
>
> Considere um processo AR(1) com m√©dia $\mu = 10$. Se observarmos $Y_t = 12$ e $Y_{t-2} = 9$, e soubermos que $E[(Y_t - \mu)(Y_{t-2} - \mu)] = (12-10)(9-10) = -2$, ent√£o $\gamma_2 = -2$. Isso indica uma autocovari√¢ncia negativa no lag 2, sugerindo que valores acima da m√©dia tendem a ser seguidos por valores abaixo da m√©dia dois per√≠odos depois.

Para complementar a defini√ß√£o acima, podemos definir a fun√ß√£o de autocorrela√ß√£o.

**Defini√ß√£o:** A fun√ß√£o de autocorrela√ß√£o $\rho_k$ de um processo estacion√°rio AR(1) √© definida como:
$$\rho_k = Corr(Y_t, Y_{t-k}) = \frac{Cov(Y_t, Y_{t-k})}{\sqrt{Var(Y_t)Var(Y_{t-k})}} = \frac{\gamma_k}{\gamma_0},$$
onde $k$ √© o lag (atraso) entre as observa√ß√µes $Y_t$ e $Y_{t-k}$. Note que, devido √† estacionariedade, $Var(Y_t) = Var(Y_{t-k}) = \gamma_0$.

> üí° **Exemplo Num√©rico:**
>
> Suponha que para um processo AR(1), $\gamma_0 = 4$ (vari√¢ncia) e $\gamma_1 = 2$ (autocovari√¢ncia no lag 1). Ent√£o, a autocorrela√ß√£o no lag 1 √©:
>
> $$\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{2}{4} = 0.5.$$
>
> Isso indica uma correla√ß√£o positiva moderada entre observa√ß√µes adjacentes na s√©rie temporal.

### Deriva√ß√£o da M√©dia

Para derivar a m√©dia de um processo AR(1), tomamos a esperan√ßa em ambos os lados da equa√ß√£o [^53]:

$$E[Y_t] = E[c + \phi Y_{t-1} + \epsilon_t].$$

Usando a linearidade do operador de esperan√ßa, obtemos:

$$E[Y_t] = E[c] + \phi E[Y_{t-1}] + E[\epsilon_t].$$

Sob a suposi√ß√£o de estacionariedade, $E[Y_t] = E[Y_{t-1}] = \mu$. Al√©m disso, $E[c] = c$ (pois *c* √© uma constante) e $E[\epsilon_t] = 0$ (pois $\epsilon_t$ √© ru√≠do branco) [^47]. Assim, temos:

$$\mu = c + \phi \mu + 0,$$

o que simplifica para:

$$\mu = c + \phi \mu.$$

Resolvendo para $\mu$, encontramos a m√©dia do processo AR(1) [^53]:

$$\mu = \frac{c}{1 - \phi}.$$

Esta express√£o para a m√©dia √© v√°lida apenas se $|\phi| < 1$, que √© a condi√ß√£o para a estacionariedade de covari√¢ncia do processo AR(1) [^53].

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um processo AR(1) com $c = 5$ e $\phi = 0.7$. A m√©dia do processo √©:
>
> $$\mu = \frac{5}{1 - 0.7} = \frac{5}{0.3} \approx 16.67.$$
>
> Este valor representa o centro em torno do qual os valores da s√©rie temporal ir√£o flutuar ao longo do tempo, sob as condi√ß√µes de estacionariedade.
>
> Agora, se $c=5$ e $\phi = 1.1$, ent√£o $\mu = \frac{5}{1-1.1} = -50$. No entanto, como $|\phi| > 1$, o processo n√£o √© estacion√°rio e a f√≥rmula para a m√©dia n√£o √© v√°lida. A s√©rie temporal explodir√° ao longo do tempo.

Para solidificar a compreens√£o do processo, demonstramos a seguinte proposi√ß√£o:

**Proposi√ß√£o 1.** Para um processo AR(1) estacion√°rio, o estimador da m√©dia amostral converge para a m√©dia populacional √† medida que o tamanho da amostra aumenta.

*Proof (Sketch):*

Considerando a m√©dia amostral $\bar{Y} = \frac{1}{T} \sum_{t=1}^{T} Y_t$. Pela Lei dos Grandes N√∫meros (LGN), se as vari√°veis $Y_t$ s√£o i.i.d ou fracamente dependentes e t√™m m√©dia finita $\mu$, ent√£o a m√©dia amostral converge em probabilidade para $\mu$ √† medida que $T \to \infty$. O processo AR(1) estacion√°rio satisfaz estas condi√ß√µes de depend√™ncia fraca e m√©dia finita. Portanto, a m√©dia amostral converge para a m√©dia populacional $\frac{c}{1-\phi}$ conforme $T$ se aproxima do infinito. ‚ñ†

Para complementar a proposi√ß√£o acima, podemos enunciar o seguinte corol√°rio sobre a vari√¢ncia da m√©dia amostral:

**Corol√°rio 1.1** Para um processo AR(1) estacion√°rio, a vari√¢ncia da m√©dia amostral diminui √† medida que o tamanho da amostra aumenta.

*Proof (Sketch):*

A vari√¢ncia da m√©dia amostral √© dada por $Var(\bar{Y}) = Var(\frac{1}{T} \sum_{t=1}^{T} Y_t) = \frac{1}{T^2} Var(\sum_{t=1}^{T} Y_t)$. Como o processo √© estacion√°rio e fracamente dependente, √† medida que $T \to \infty$, a vari√¢ncia da m√©dia amostral diminui, convergindo para zero. ‚ñ†

Al√©m disso, podemos extender a Proposi√ß√£o 1 para um resultado de converg√™ncia mais forte, atrav√©s do Teorema do Limite Central:

**Teorema 1.1** (Teorema do Limite Central para AR(1)). Para um processo AR(1) estacion√°rio, a distribui√ß√£o da m√©dia amostral, devidamente normalizada, converge para uma distribui√ß√£o normal √† medida que o tamanho da amostra aumenta. Formalmente,
$$\sqrt{T}(\bar{Y} - \mu) \xrightarrow{d} N(0, \sigma_{\bar{Y}}^2),$$
onde $\bar{Y}$ √© a m√©dia amostral, $\mu = \frac{c}{1 - \phi}$ √© a m√©dia populacional, $T$ √© o tamanho da amostra, $\xrightarrow{d}$ denota converg√™ncia em distribui√ß√£o, e $\sigma_{\bar{Y}}^2$ √© a vari√¢ncia assint√≥tica da m√©dia amostral.

*Proof (Sketch):*

Sob as condi√ß√µes de estacionariedade e depend√™ncia fraca do processo AR(1), o Teorema do Limite Central (TLC) para s√©ries temporais dependentes pode ser aplicado. O TLC garante que, para amostras grandes, a distribui√ß√£o da m√©dia amostral se aproxima de uma distribui√ß√£o normal com m√©dia $\mu$ e vari√¢ncia $\sigma_{\bar{Y}}^2 / T$. A vari√¢ncia assint√≥tica $\sigma_{\bar{Y}}^2$ depende das autocovari√¢ncias do processo AR(1). ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Vamos simular 1000 amostras de tamanho $T = 100$ de um processo AR(1) com $c=2$, $\phi=0.5$ e $\sigma=1$. Calcularemos a m√©dia amostral para cada amostra e verificaremos se a distribui√ß√£o das m√©dias amostrais se aproxima de uma normal com m√©dia $\mu = \frac{c}{1-\phi} = \frac{2}{1-0.5} = 4$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy.stats import norm
>
> # Define parameters
> c = 2
> phi = 0.5
> sigma = 1
> T = 100
> num_simulations = 1000
>
> # Function to generate AR(1) series
> def generate_ar1(c, phi, sigma, T):
>     epsilon = np.random.normal(0, sigma, T)
>     Y = np.zeros(T)
>     Y[0] = c + epsilon[0]  # Initialize the first value
>     for t in range(1, T):
>         Y[t] = c + phi * Y[t-1] + epsilon[t]
>     return Y
>
> # Simulate and calculate sample means
> sample_means = []
> for _ in range(num_simulations):
>     ar1_series = generate_ar1(c, phi, sigma, T)
>     sample_mean = np.mean(ar1_series)
>     sample_means.append(sample_mean)
>
> # Plot the distribution of sample means
> plt.hist(sample_means, bins=30, density=True, alpha=0.6, color='g')
>
> # Superimpose the normal distribution
> mu = c / (1 - phi)
> sigma_bar = np.sqrt((sigma**2 / (1 - phi**2)) / T)  # Theoretical standard error
> x = np.linspace(min(sample_means), max(sample_means), 100)
> p = norm.pdf(x, mu, sigma_bar)
> plt.plot(x, p, 'k', linewidth=2)
>
> plt.title('Distribution of Sample Means with Superimposed Normal Distribution')
> plt.xlabel('Sample Mean')
> plt.ylabel('Density')
> plt.show()
>
> print(f"Theoretical Mean: {mu}")
> print(f"Mean of Sample Means: {np.mean(sample_means)}")
> print(f"Theoretical Standard Error: {sigma_bar}")
> print(f"Standard Deviation of Sample Means: {np.std(sample_means)}")
> ```
>
> O histograma das m√©dias amostrais se aproxima de uma distribui√ß√£o normal centrada em torno da m√©dia te√≥rica $\mu = 4$. O desvio padr√£o das m√©dias amostrais tamb√©m se aproxima do erro padr√£o te√≥rico $\sigma_{\bar{Y}}$. Isso confirma o Teorema do Limite Central.

### Implica√ß√µes da Estacionariedade
A suposi√ß√£o de estacionariedade √© crucial porque permite que a m√©dia seja calculada de forma independente do tempo [^53]. Sem estacionariedade, $E[Y_t]$ poderia variar com $t$, tornando a deriva√ß√£o da m√©dia uma tarefa muito mais complexa [^53]. Al√©m disso, a estacionariedade de covari√¢ncia implica que a vari√¢ncia e a autocovari√¢ncia tamb√©m s√£o constantes ao longo do tempo. A estacionariedade tamb√©m impacta a fun√ß√£o de autocorrela√ß√£o (ACF).

**Teorema 2.** Para um processo AR(1) estacion√°rio, a fun√ß√£o de autocorrela√ß√£o decai exponencialmente com o aumento do lag $k$.

*Proof (Sketch):*

Come√ßamos com a defini√ß√£o do processo AR(1): $Y_t = c + \phi Y_{t-1} + \epsilon_t$. Subtraindo a m√©dia $\mu$ de ambos os lados, obtemos: $Y_t - \mu = \phi (Y_{t-1} - \mu) + \epsilon_t$. Multiplicando ambos os lados por $(Y_{t-k} - \mu)$ e tomando a esperan√ßa, obtemos $\gamma_k = \phi \gamma_{k-1}$. Dividindo ambos os lados por $\gamma_0$, obtemos a fun√ß√£o de autocorrela√ß√£o: $\rho_k = \phi \rho_{k-1}$. Dado que $\rho_0 = 1$, temos $\rho_1 = \phi$, $\rho_2 = \phi^2$, e em geral, $\rho_k = \phi^k$. Como $|\phi| < 1$ para estacionariedade, $|\rho_k|$ decai exponencialmente para zero quando $k$ aumenta. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Se $\phi = 0.8$, ent√£o:
>
> *   $\rho_0 = 1$
> *   $\rho_1 = 0.8$
> *   $\rho_2 = 0.8^2 = 0.64$
> *   $\rho_3 = 0.8^3 = 0.512$
>
> Observe como a autocorrela√ß√£o diminui exponencialmente √† medida que o lag aumenta. Isso indica que a depend√™ncia entre as observa√ß√µes diminui com o tempo.

Podemos generalizar o Teorema 2 da seguinte forma:

**Teorema 2.1** Para um processo AR(p) estacion√°rio, a fun√ß√£o de autocorrela√ß√£o parcial (PACF) se torna zero para lags maiores que p.

*Proof (Sketch)*

A fun√ß√£o de autocorrela√ß√£o parcial (PACF) mede a correla√ß√£o entre $Y_t$ e $Y_{t-k}$ ap√≥s remover a influ√™ncia das vari√°veis intermedi√°rias $Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1}$. Para um processo AR(1), a PACF √© significante apenas para lag 1, pois a correla√ß√£o entre $Y_t$ e $Y_{t-k}$ para $k > 1$ √© totalmente explicada pela correla√ß√£o de $Y_t$ com $Y_{t-1}$. Para um processo AR(p), a PACF ter√° valores significativos para os primeiros p lags, e ser√° essencialmente zero para lags maiores que p. Isto ocorre porque, ap√≥s remover a influ√™ncia dos p lags anteriores, n√£o resta nenhuma correla√ß√£o adicional entre $Y_t$ e $Y_{t-k}$ para $k > p$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere um processo AR(2): $Y_t = 0.5Y_{t-1} + 0.3Y_{t-2} + \epsilon_t$. A PACF ter√° valores significativos para os lags 1 e 2, mas ser√° aproximadamente zero para lags maiores que 2. Isso porque a depend√™ncia de $Y_t$ em lags maiores que 2 j√° est√° capturada pelos lags 1 e 2.

### Exemplo Pr√°tico: An√°lise da M√©dia de uma S√©rie Temporal AR(1)

Considere uma s√©rie temporal AR(1) simulada com os seguintes par√¢metros: $c = 1$, $\phi = 0.5$ e $\sigma = 0.2$. Podemos gerar esta s√©rie temporal usando um algoritmo iterativo como descrito nos cap√≠tulos anteriores [^53].
Calculemos a m√©dia te√≥rica:
$$\mu = \frac{1}{1 - 0.5} = 2$$

Ap√≥s gerar a s√©rie temporal, podemos calcular a m√©dia amostral:
```python
import numpy as np
import matplotlib.pyplot as plt

def generate_ar1(c, phi, sigma, T):
    """Gera uma s√©rie temporal AR(1)"""
    epsilon = np.random.normal(0, sigma, T)
    Y = np.zeros(T)
    Y[0] = epsilon[0]
    for t in range(1, T):
        Y[t] = c + phi * Y[t-1] + epsilon[t]
    return Y

# Define parameters
c = 1
phi = 0.5
sigma = 0.2
T = 1000

# Generate AR(1) time series
ar1_series = generate_ar1(c, phi, sigma, T)

# Calculate sample mean
sample_mean = np.mean(ar1_series)
print(f"Sample Mean: {sample_mean}")

# Plot the time series
plt.plot(ar1_series)
plt.axhline(y=2, color='r', linestyle='-', label='Theoretical Mean')
plt.axhline(y=sample_mean, color='g', linestyle='--', label='Sample Mean')
plt.title('AR(1) Time Series with Theoretical and Sample Means')
plt.xlabel('Time')
plt.ylabel('Y_t')
plt.legend()
plt.show()
```

Analisando o gr√°fico, podemos ver como os dados flutuam em torno da m√©dia te√≥rica.

### Conclus√£o
Neste cap√≠tulo, demonstramos como derivar a m√©dia de um processo AR(1) assumindo estacionariedade de covari√¢ncia [^53]. Tomando a esperan√ßa da equa√ß√£o AR(1), e utilizando as propriedades do ru√≠do branco [^47], obtivemos uma express√£o para a m√©dia em termos dos par√¢metros $c$ e $\phi$ [^53]. Este resultado √© fundamental para entender o comportamento de longo prazo de um processo AR(1) e para a an√°lise de s√©ries temporais em geral. Al√©m disso, discutimos como a valida√ß√£o das premissas do modelo, como estacionariedade e comportamento dos res√≠duos, s√£o cruciais para garantir a precis√£o e confiabilidade da an√°lise [^53].

### Refer√™ncias
[^53]: Y‚ÇÅ = c + Y-1 + Œµ.
[^46]: Imagine a battery of I such computers generating sequences {y{1}, {y{2} ... , y{1}}.
[^47]: The basic building block for all the processes considered in this chapter is a sequence {8} - whose elements have mean zero and variance œÉ¬≤.
[^45]: The variance of the random variable Y, (denoted you) is similarly defined as You = E(Y, ‚àí Œº,)¬≤ = ‚à´ (y, ‚àí Œº,)¬≤ fv,(y,) dy.
## T√≠tulo Conciso
### Expectativas, Estacionariedade e Ergodicidade
#### Condi√ß√µes para um processo estacion√°rio com m√©dia zero e vari√¢ncia œÉ¬≤
Como vimos anteriormente, uma sequ√™ncia de vari√°veis aleat√≥rias {$\epsilon_t$}$_{t=-\infty}^{\infty}$ √© considerada **ru√≠do branco** se satisfaz as seguintes condi√ß√µes [^45]:

1.  M√©dia zero: $E(\epsilon_t) = 0$ [^45]
2.  Vari√¢ncia constante: $E(\epsilon_t^2) = \sigma^2$ [^45]
3.  N√£o correla√ß√£o: $E(\epsilon_t \epsilon_\tau) = 0$ para $t \neq \tau$ [^45]

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos uma s√©rie de ru√≠do branco com $\sigma^2 = 4$. Ent√£o:
>
> *   $E(\epsilon_t) = 0$ para qualquer $t$.
> *   $E(\epsilon_1^2) = E(\epsilon_2^2) = \ldots = 4$.
> *   $E(\epsilon_1 \epsilon_2) = E(\epsilon_5 \epsilon_{10}) = 0$.

Estas condi√ß√µes definem um processo *estacion√°rio no sentido amplo* (ou estacion√°rio de segunda ordem), onde apenas os dois primeiros momentos (m√©dia e autocovari√¢ncia) s√£o invariantes no tempo.

**Observa√ß√£o Importante:**

> √â crucial entender que *independ√™ncia* implica em *n√£o correla√ß√£o*, mas o inverso nem sempre √© verdadeiro. Um processo pode ser n√£o correlacionado sem ser independente.

Al√©m disso, √© importante distinguir entre **estacionariedade fraca (ou de covari√¢ncia)** e **estacionariedade estrita**. Um processo √© estritamente estacion√°rio se a distribui√ß√£o conjunta de qualquer conjunto de observa√ß√µes {$Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n}$} √© a mesma que a distribui√ß√£o conjunta de {$Y_{t_1+h}, Y_{t_2+h}, \ldots, Y_{t_n+h}$} para qualquer $h$. Em outras palavras, a distribui√ß√£o conjunta √© invariante a transla√ß√µes no tempo. Se um processo √© estritamente estacion√°rio e possui momentos de segunda ordem finitos, ent√£o ele √© tamb√©m estacion√°rio no sentido de covari√¢ncia.

**Ergodicidade** √© uma propriedade relacionada, mas distinta, da estacionariedade. Intuitivamente, um processo √© *erg√≥dico* se m√©dias de tempo convergem para m√©dias de conjunto. Formalmente, para um processo estacion√°rio, a m√©dia amostral converge em probabilidade para a m√©dia do conjunto:
$$\text{plim}_{T \to \infty} \frac{1}{T} \sum_{t=1}^{T} Y_t = E[Y_t]$$
A ergodicidade garante que podemos estimar as propriedades estat√≠sticas do processo a partir de uma √∫nica realiza√ß√£o suficientemente longa. A ergodicidade √© essencial para infer√™ncia estat√≠stica em s√©ries temporais.

√â poss√≠vel que um processo seja estacion√°rio, mas n√£o erg√≥dico, e vice-versa. No entanto, para muitos processos lineares gaussianos, estacionariedade implica ergodicidade.

**Exemplo:** Considere um processo $Y_t = \mu + \epsilon_t$, onde $\mu$ √© uma vari√°vel aleat√≥ria com m√©dia zero e vari√¢ncia $\lambda^2$ e $\epsilon_t$ √© ru√≠do branco gaussiano com m√©dia zero e vari√¢ncia $\sigma^2$, independente de $\mu$. Este processo √© estacion√°rio, mas n√£o erg√≥dico.

> üí° **Exemplo Num√©rico:**
>
> Suponha que $\mu$ siga uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia $\lambda^2 = 1$, e $\epsilon_t$ seja ru√≠do branco com m√©dia 0 e vari√¢ncia $\sigma^2 = 2$. Ent√£o $E[Y_t] = E[\mu + \epsilon_t] = E[\mu] + E[\epsilon_t] = 0 + 0 = 0$, o que √© constante ao longo do tempo. Al√©m disso, $Var[Y_t] = Var[\mu + \epsilon_t] = Var[\mu] + Var[\epsilon_t] = 1 + 2 = 3$, tamb√©m constante. Portanto, o processo √© estacion√°rio. No entanto, a m√©dia amostral $\frac{1}{T} \sum_{t=1}^{T} Y_t$ converge para $\mu$, que √© uma vari√°vel aleat√≥ria, e n√£o para uma constante. Portanto, o processo n√£o √© erg√≥dico.

### Autocovari√¢ncia e Autocorrela√ß√£o

A **autocovari√¢ncia** mede a depend√™ncia linear entre observa√ß√µes em diferentes pontos no tempo. Para um processo estacion√°rio, a autocovari√¢ncia de ordem *j* √© definida como [^45]:
$$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$$
A autocovari√¢ncia no lag zero ($\gamma_0$) √© a vari√¢ncia do processo.

A **autocorrela√ß√£o** √© a autocovari√¢ncia normalizada pela vari√¢ncia:
$$\rho_j = \frac{\gamma_j}{\gamma_0}$$
A autocorrela√ß√£o varia entre -1 e 1 e mede a for√ßa e a dire√ß√£o da depend√™ncia linear entre observa√ß√µes separadas por *j* per√≠odos de tempo. A fun√ß√£o de autocorrela√ß√£o (ACF) √© um instrumento fundamental para analisar a depend√™ncia temporal em s√©ries temporais.

**Propriedades da ACF:**

1.  $\rho_0 = 1$
2.  $\rho_j = \rho_{-j}$ (para processos estacion√°rios)
3.  $|\rho_j| \leq 1$

> üí° **Exemplo Num√©rico:**
>
> Suponha que para uma s√©rie temporal, calculamos as seguintes autocovari√¢ncias:
>
> *   $\gamma_0 = 9$ (vari√¢ncia)
> *   $\gamma_1 = 4.5$
> *   $\gamma_2 = 2.25$
>
> Ent√£o as autocorrela√ß√µes correspondentes s√£o:
>
> *   $\rho_0 = \frac{\gamma_0}{\gamma_0} = 1$
> *   $\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{4.5}{9} = 0.5$
> *   $\rho_2 = \frac{\gamma_2}{\gamma_0} = \frac{2.25}{9} = 0.25$
>
> A ACF decai √† medida que o lag aumenta, sugerindo um processo com depend√™ncia temporal decrescente.

Para calcular a vari√¢ncia $\gamma_0$ de um processo AR(1), podemos utilizar o seguinte teorema:

**Teorema 3.** A vari√¢ncia $\gamma_0$ de um processo AR(1) estacion√°rio √© dada por:
$$\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$$

*Prova:*
I. Come√ßamos com a equa√ß√£o do processo AR(1):
   $$Y_t = c + \phi Y_{t-1} + \epsilon_t$$

II. Subtraindo a m√©dia $\mu$ de ambos os lados:
    $$Y_t - \mu = \phi (Y_{t-1} - \mu) + \epsilon_t$$

III. Elevando ao quadrado ambos os lados:
     $$(Y_t - \mu)^2 = (\phi (Y_{t-1} - \mu) + \epsilon_t)^2$$
     $$(Y_t - \mu)^2 = \phi^2 (Y_{t-1} - \mu)^2 + 2 \phi (Y_{t-1} - \mu) \epsilon_t + \epsilon_t^2$$

IV. Tomando a esperan√ßa de ambos os lados:
    $$E[(Y_t - \mu)^2] = E[\phi^2 (Y_{t-1} - \mu)^2 + 2 \phi (Y_{t-1} - \mu) \epsilon_t + \epsilon_t^2]$$

V. Usando a linearidade da esperan√ßa:
   $$E[(Y_t - \mu)^2] = \phi^2 E[(Y_{t-1} - \mu)^2] + 2 \phi E[(Y_{t-1} - \mu) \epsilon_t] + E[\epsilon_t^2]$$

VI. Pela estacionariedade, $E[(Y_t - \mu)^2] = E[(Y_{t-1} - \mu)^2] = \gamma_0$. Como $\epsilon_t$ √© ru√≠do branco, $E[\epsilon_t^2] = \sigma^2$. E como $\epsilon_t$ √© independente de $Y_{t-1}$, $E[(Y_{t-1} - \mu)\epsilon_t] = 0$. Assim:
    $$\gamma_0 = \phi^2 \gamma_0 + \sigma^2$$

VII. Resolvendo para $\gamma_0$:
     $$\gamma_0 - \phi^2 \gamma_0 = \sigma^2$$
     $$\gamma_0 (1 - \phi^2) = \sigma^2$$
     $$\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$$ ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere um processo AR(1) com $\phi = 0.6$ e $\sigma^2 = 4$. Ent√£o a vari√¢ncia do processo √©:
>
> $$\gamma_0 = \frac{4}{1 - 0.6^2} = \frac{4}{1 - 0.36} = \frac{4}{0.64} = 6.25$$
>
> Isso indica que a dispers√£o dos valores em torno da m√©dia √© 6.25.

√â poss√≠vel generalizar o Teorema 3 para obter uma express√£o para a autocovari√¢ncia em qualquer lag k:

**Teorema 3.1** A autocovari√¢ncia $\gamma_k$ de um processo AR(1) estacion√°rio √© dada por:
$$\gamma_k = \phi^k \gamma_0 = \phi^k \frac{\sigma^2}{1 - \phi^2}$$

*Prova:*
I.  Come√ßamos com a defini√ß√£o de autocovari√¢ncia:
    $$\gamma_k = E[(Y_t - \mu)(Y_{t-k} - \mu)]$$

II. Usando a equa√ß√£o do processo AR(1) (subtraindo a m√©dia de ambos os lados):
    $$Y_t - \mu = \phi (Y_{t-1} - \mu) + \epsilon_t$$

III. Multiplicando ambos os lados por $(Y_{t-k} - \mu)$:
     $$(Y_t - \mu)(Y_{t-k} - \mu) = (\phi (Y_{t-1} - \mu) + \epsilon_t)(Y_{t-k} - \mu)$$

IV. Tomando a esperan√ßa de ambos os lados:
    $$E[(Y_t - \mu)(Y_{t-k} - \mu)] = E[(\phi (Y_{t-1} - \mu) + \epsilon_t)(Y_{t-k} - \mu)]$$

V. Usando a linearidade da esperan√ßa:
   $$E[(Y_t - \mu)(Y_{t-k} - \mu)] = \phi E[(Y_{t-1} - \mu)(Y_{t-k} - \mu)] + E[\epsilon_t(Y_{t-k} - \mu)]$$

VI. Dado que $\epsilon_t$ √© independente de $Y_{t-k}$ para $k>0$, $E[\epsilon_t(Y_{t-k} - \mu)] = 0$ (para $t>t-k$). Assim:
    $$\gamma_k = \phi \gamma_{k-1}$$

VII. Aplicando esta rela√ß√£o recursivamente:
     $$\gamma_k = \phi \gamma_{k-1} = \phi^2 \gamma_{k-2} = \ldots = \phi^k \gamma_0$$

VIII. Substituindo $\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$:
      $$\gamma_k = \phi^k \frac{\sigma^2}{1 - \phi^2}$$ ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Usando o exemplo anterior com $\phi = 0.6$ e $\sigma^2 = 4$, e sabendo que $\gamma_0 = 6.25$, podemos calcular $\gamma_1$ e $\gamma_2$:
>
> *   $\gamma_1 = \phi \gamma_0 = 0.6 * 6.25 = 3.75$
> *   $\gamma_2 = \phi^2 \gamma_0 = 0.6^2 * 6.25 = 0.36 * 6.25 = 2.25$
>
> A autocovari√¢ncia diminui √† medida que o lag aumenta, consistente com a estacionariedade.

### Conclus√£o

Este cap√≠tulo abordou os conceitos fundamentais de estacionariedade, ergodicidade, autocovari√¢ncia e autocorrela√ß√£o em s√©ries temporais. Estes conceitos s√£o cruciais para modelar e prever s√©ries temporais. A estacionariedade garante que os modelos estat√≠sticos sejam consistentes no tempo, enquanto a ergodicidade permite estimar as propriedades do processo a partir de uma √∫nica realiza√ß√£o. A autocovari√¢ncia e a autocorrela√ß√£o fornecem informa√ß√µes sobre a depend√™ncia temporal e s√£o usados para identificar a estrutura de depend√™ncia em uma s√©rie temporal.
### Refer√™ncias
[^45]: P√°gina 44, Stationary ARMA Processes, Cap√≠tulo 3.
<!-- END -->