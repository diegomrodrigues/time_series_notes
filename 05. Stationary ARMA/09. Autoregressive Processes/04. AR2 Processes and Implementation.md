## Autoregressive Processes: Analyzing and Implementing AR(2) Processes

### Introdu√ß√£o

Expandindo sobre a an√°lise dos modelos autorregressivos, este cap√≠tulo introduz os **processos autorregressivos de segunda ordem (AR(2))**. Enquanto o modelo AR(1) descreve a depend√™ncia do valor atual em rela√ß√£o ao valor anterior [^53], o modelo AR(2) estende essa rela√ß√£o incorporando tamb√©m o segundo valor anterior [^53]. A implementa√ß√£o eficiente de processos AR de ordem superior, particularmente para AR(p) com *p* grande, pode requerer m√©todos num√©ricos para lidar com a complexidade crescente [^53]. Este cap√≠tulo foca nas propriedades, condi√ß√µes de estacionariedade e implementa√ß√£o de processos AR(2), preparando o caminho para a discuss√£o de modelos AR de ordem ainda mais alta.

### Conceitos Fundamentais

Um **processo autorregressivo de segunda ordem**, denotado **AR(2)**, √© definido pela seguinte equa√ß√£o [^53]:

$$Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \epsilon_t,$$

onde:

*   $Y_t$ representa o valor da s√©rie temporal no instante $t$.
*   $c$ √© uma constante.
*   $\phi_1$ e $\phi_2$ s√£o os **par√¢metros autorregressivos**.
*   $\epsilon_t$ √© um termo de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$ [^47].

A equa√ß√£o acima implica que o valor atual da s√©rie temporal ($Y_t$) √© uma fun√ß√£o linear de seus dois valores anteriores ($Y_{t-1}$ e $Y_{t-2}$), somado a um termo de ru√≠do branco e uma constante [^53].

**Condi√ß√µes de Estacionariedade**:

As condi√ß√µes para a **estacionariedade de covari√¢ncia** de um processo AR(2) s√£o mais complexas do que para um processo AR(1). A condi√ß√£o $|\phi|<1$ n√£o √© suficiente. Para garantir que o processo AR(2) seja estacion√°rio, as ra√≠zes da equa√ß√£o caracter√≠stica associada devem estar fora do c√≠rculo unit√°rio. A equa√ß√£o caracter√≠stica √© dada por [^53]:

$$1 - \phi_1 z - \phi_2 z^2 = 0,$$

onde $z$ √© uma vari√°vel complexa. Se $z_1$ e $z_2$ s√£o as ra√≠zes da equa√ß√£o caracter√≠stica, ent√£o a condi√ß√£o de estacionariedade √© [^53]:

$$|z_1| > 1 \text{ e } |z_2| > 1.$$

Esta condi√ß√£o pode ser reescrita em termos dos par√¢metros $\phi_1$ e $\phi_2$ como:

1.  $\phi_1 + \phi_2 < 1$
2.  $\phi_2 - \phi_1 < 1$
3.  $-1 < \phi_2 < 1$

Essas tr√™s condi√ß√µes devem ser satisfeitas simultaneamente para garantir que o processo AR(2) seja estacion√°rio [^53].

> üí° **Exemplo Num√©rico:**
>
> Considere $\phi_1 = 0.6$ e $\phi_2 = 0.3$. Vamos verificar as condi√ß√µes de estacionariedade:
>
> 1.  $\phi_1 + \phi_2 = 0.6 + 0.3 = 0.9 < 1$ (Verdadeiro)
> 2.  $\phi_2 - \phi_1 = 0.3 - 0.6 = -0.3 < 1$ (Verdadeiro)
> 3.  $-1 < \phi_2 = 0.3 < 1$ (Verdadeiro)
>
> Portanto, o processo AR(2) com $\phi_1 = 0.6$ e $\phi_2 = 0.3$ √© estacion√°rio.
>
> Agora considere $\phi_1 = 1.2$ e $\phi_2 = -0.5$:
>
> 1.  $\phi_1 + \phi_2 = 1.2 - 0.5 = 0.7 < 1$ (Verdadeiro)
> 2.  $\phi_2 - \phi_1 = -0.5 - 1.2 = -1.7 < 1$ (Verdadeiro)
> 3.  $-1 < \phi_2 = -0.5 < 1$ (Verdadeiro)
>
> Todas as condi√ß√µes s√£o satisfeitas individualmente.
>
> No entanto, para o caso de $\phi_1 = 0.5$ e $\phi_2 = 0.7$:
>
> 1.  $\phi_1 + \phi_2 = 0.5 + 0.7 = 1.2 > 1$ (Falso)
> 2.  $\phi_2 - \phi_1 = 0.7 - 0.5 = 0.2 < 1$ (Verdadeiro)
> 3.  $-1 < \phi_2 = 0.7 < 1$ (Verdadeiro)
>
> O processo AR(2) com $\phi_1 = 0.5$ e $\phi_2 = 0.7$ n√£o √© estacion√°rio porque a primeira condi√ß√£o n√£o √© satisfeita, mesmo que as outras duas sejam.
>
> Se $\phi_1 = 0.5$ e $\phi_2 = 1.2$:
>
> 1.  $0.5 + 1.2 = 1.7 \nless 1$ (Falso)
> 2.  $1.2 - 0.5 = 0.7 < 1$ (Verdadeiro)
> 3.  $-1 < 1.2 \nless 1$ (Falso)
>
> O processo AR(2) com $\phi_1 = 0.5$ e $\phi_2 = 1.2$ n√£o √© estacion√°rio.
>
> Este exemplo demonstra que as tr√™s condi√ß√µes devem ser verificadas em conjunto para determinar a estacionariedade de um processo AR(2). Visualizar isso em um gr√°fico pode ser √∫til para entender a regi√£o de estacionariedade.

Al√©m dessas condi√ß√µes, podemos expressar a regi√£o de estacionariedade em termos de um tri√¢ngulo no espa√ßo dos par√¢metros $(\phi_1, \phi_2)$.

**Proposi√ß√£o 1:** A regi√£o de estacionariedade para um processo AR(2) √© um tri√¢ngulo no plano $(\phi_1, \phi_2)$ delimitado pelas linhas:

*   $\phi_2 = \phi_1 - 1$
*   $\phi_2 = -\phi_1 - 1$
*   $\phi_2 = 1$

*Proof:*

As tr√™s desigualdades definem uma regi√£o no plano $(\phi_1, \phi_2)$. Vamos analisar cada desigualdade separadamente:

1.  $\phi_1 + \phi_2 < 1 \Rightarrow \phi_2 < -\phi_1 + 1$.  Isto representa a regi√£o abaixo da linha $\phi_2 = -\phi_1 + 1$.

2.  $\phi_2 - \phi_1 < 1 \Rightarrow \phi_2 < \phi_1 + 1$. Isto representa a regi√£o abaixo da linha $\phi_2 = \phi_1 + 1$.

3.  $-1 < \phi_2 < 1$. Isto representa a regi√£o entre as linhas horizontais $\phi_2 = -1$ e $\phi_2 = 1$.

A intersec√ß√£o dessas tr√™s regi√µes √© um tri√¢ngulo com v√©rtices nos pontos:
   *  Intersec√ß√£o de $\phi_2 = -\phi_1 + 1$ e $\phi_2 = \phi_1 + 1$:  $-\phi_1 + 1 = \phi_1 + 1 \Rightarrow 2\phi_1 = 0 \Rightarrow \phi_1 = 0$.  Ent√£o, $\phi_2 = 1$.  Portanto, o v√©rtice √© $(0, 1)$.
   *  Intersec√ß√£o de $\phi_2 = -\phi_1 + 1$ e $\phi_2 = -1$:  $-1 = -\phi_1 + 1 \Rightarrow \phi_1 = 2$.  Portanto, o v√©rtice √© $(2, -1)$.
   *  Intersec√ß√£o de $\phi_2 = \phi_1 + 1$ e $\phi_2 = -1$:  $-1 = \phi_1 + 1 \Rightarrow \phi_1 = -2$.  Portanto, o v√©rtice √© $(-2, -1)$.

Este tri√¢ngulo define a regi√£o de estacionariedade para os par√¢metros $\phi_1$ e $\phi_2$.  Qualquer par de valores $(\phi_1, \phi_2)$ dentro deste tri√¢ngulo garante que o processo AR(2) seja estacion√°rio. ‚ñ†

Agora, vamos analisar o que acontece quando as ra√≠zes da equa√ß√£o caracter√≠stica s√£o complexas.

**Proposi√ß√£o 1.1:** Se as ra√≠zes da equa√ß√£o caracter√≠stica do processo AR(2) s√£o complexas conjugadas, ent√£o o processo exibir√° um comportamento oscilat√≥rio.

*Proof (Sketch):*

Se as ra√≠zes $z_1$ e $z_2$ s√£o complexas conjugadas, elas podem ser escritas na forma $z_{1,2} = r e^{\pm i\theta}$, onde $r$ √© o m√≥dulo e $\theta$ √© o argumento. A condi√ß√£o de estacionariedade $|z_1| > 1$ e $|z_2| > 1$ implica que $r > 1$. A solu√ß√£o geral para o processo AR(2) envolver√° termos da forma $r^{-t} \cos(t\theta)$ e $r^{-t} \sin(t\theta)$. Dado que $r>1$, esses termos decair√£o exponencialmente, mas a presen√ßa das fun√ß√µes seno e cosseno indica um comportamento oscilat√≥rio. ‚ñ†

**M√©dia e Vari√¢ncia de um Processo AR(2) Estacion√°rio**:

Se o processo AR(2) satisfaz as condi√ß√µes de estacionariedade, podemos derivar a sua m√©dia e vari√¢ncia. Tomando a esperan√ßa em ambos os lados da equa√ß√£o $Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \epsilon_t$, e assumindo que $E(Y_t) = E(Y_{t-1}) = E(Y_{t-2}) = \mu$ e $E(\epsilon_t) = 0$, obtemos:

$$\mu = c + \phi_1 \mu + \phi_2 \mu,$$

o que implica que a m√©dia do processo AR(2) √© dada por:

$$\mu = \frac{c}{1 - \phi_1 - \phi_2}.$$

> üí° **Exemplo Num√©rico:**
>
> Considere um processo AR(2) com $c = 5$, $\phi_1 = 0.4$ e $\phi_2 = 0.3$. A m√©dia do processo √©:
>
> $$\mu = \frac{5}{1 - 0.4 - 0.3} = \frac{5}{0.3} \approx 16.67$$
>
> Isso significa que, em m√©dia, os valores da s√©rie temporal flutuar√£o em torno de 16.67.  Um valor maior de *c* resultaria em uma m√©dia ainda maior, enquanto valores maiores de $\phi_1$ e $\phi_2$ (desde que sua soma permane√ßa menor que 1 para a estacionariedade) diminuiriam a m√©dia.

A vari√¢ncia do processo AR(2) pode ser obtida de forma semelhante √† do AR(1), mas a deriva√ß√£o √© mais complexa. A vari√¢ncia √© dada por:

$$\gamma_0 = E[(Y_t - \mu)^2] = \frac{\sigma^2(1 - \phi_2)}{(1 + \phi_2)[(1 - \phi_2)^2 - \phi_1^2]}.$$

*Prova:*

Para provar a f√≥rmula da vari√¢ncia, seguimos os seguintes passos:

I. Seja $Y_t' = Y_t - \mu$. Ent√£o $Y_t' = \phi_1 Y_{t-1}' + \phi_2 Y_{t-2}' + \epsilon_t$.

II. Multiplicamos ambos os lados por $Y_t'$ e tomamos a esperan√ßa:
   $$E[(Y_t')^2] = \phi_1 E[Y_t' Y_{t-1}'] + \phi_2 E[Y_t' Y_{t-2}'] + E[Y_t' \epsilon_t]$$

III. Reconhecendo que $\gamma_0 = E[(Y_t')^2]$, $\gamma_1 = E[Y_t' Y_{t-1}']$, $\gamma_2 = E[Y_t' Y_{t-2}']$ e $E[Y_t' \epsilon_t] = \sigma^2$, temos:
    $$\gamma_0 = \phi_1 \gamma_1 + \phi_2 \gamma_2 + \sigma^2$$

IV. Sabemos tamb√©m que $\gamma_j = \phi_1 \gamma_{j-1} + \phi_2 \gamma_{j-2}$ para $j > 0$. Logo,
    $$\gamma_1 = \phi_1 \gamma_0 + \phi_2 \gamma_1$$
    $$\gamma_2 = \phi_1 \gamma_1 + \phi_2 \gamma_0$$

V. Podemos reescrever $\gamma_1$ como: $\gamma_1 = \frac{\phi_1 \gamma_0}{1 - \phi_2}$. Substitu√≠mos $\gamma_1$ e $\gamma_2$ em $\gamma_0 = \phi_1 \gamma_1 + \phi_2 \gamma_2 + \sigma^2$:
$$\gamma_0 = \phi_1 \left(\frac{\phi_1 \gamma_0}{1 - \phi_2}\right) + \phi_2 \left(\phi_1 \frac{\phi_1 \gamma_0}{1 - \phi_2} + \phi_2 \gamma_0\right) + \sigma^2$$

VI. Simplificando e resolvendo para $\gamma_0$:
$$\gamma_0 = \frac{\phi_1^2 \gamma_0}{1 - \phi_2} + \frac{\phi_1^2 \phi_2 \gamma_0}{1 - \phi_2} + \phi_2^2 \gamma_0 + \sigma^2$$
$$\gamma_0 \left(1 - \frac{\phi_1^2}{1 - \phi_2} - \frac{\phi_1^2 \phi_2}{1 - \phi_2} - \phi_2^2\right) = \sigma^2$$
$$\gamma_0 \left(\frac{(1 - \phi_2) - \phi_1^2 - \phi_1^2 \phi_2 - \phi_2^2 (1 - \phi_2)}{1 - \phi_2}\right) = \sigma^2$$
$$\gamma_0 \left(\frac{1 - \phi_2 - \phi_1^2 - \phi_1^2 \phi_2 - \phi_2^2 + \phi_2^3}{1 - \phi_2}\right) = \sigma^2$$

VII. Simplificando para obter a vari√¢ncia:
$$\gamma_0 = \frac{\sigma^2 (1 - \phi_2)}{1 - \phi_2 - \phi_1^2 - \phi_1^2 \phi_2 - \phi_2^2 + \phi_2^3} = \frac{\sigma^2(1 - \phi_2)}{(1 + \phi_2)[(1 - \phi_2)^2 - \phi_1^2]}.$$

Portanto, a vari√¢ncia do processo AR(2) √© dada por:

$$\gamma_0 = \frac{\sigma^2(1 - \phi_2)}{(1 + \phi_2)[(1 - \phi_2)^2 - \phi_1^2]}.$$ ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Se $\sigma^2 = 2$, $\phi_1 = 0.6$ e $\phi_2 = 0.2$:
>
> $$\gamma_0 = \frac{2(1 - 0.2)}{(1 + 0.2)[(1 - 0.2)^2 - 0.6^2]} = \frac{2(0.8)}{1.2[0.64 - 0.36]} = \frac{1.6}{1.2 \cdot 0.28} = \frac{1.6}{0.336} \approx 4.76$$
>
> Isso significa que a variabilidade dos valores em torno da m√©dia √© de aproximadamente 4.76. Uma vari√¢ncia maior $\sigma^2$ levaria a uma vari√¢ncia maior no processo AR(2).

**Autocovari√¢ncia e Autocorrela√ß√£o**:

As autocovari√¢ncias de um processo AR(2) satisfazem a seguinte rela√ß√£o recursiva:

$$\gamma_j = \phi_1 \gamma_{j-1} + \phi_2 \gamma_{j-2}, \quad \text{para } j > 0.$$

Dividindo ambos os lados por $\gamma_0$, obtemos a rela√ß√£o para as autocorrela√ß√µes:

$$\rho_j = \phi_1 \rho_{j-1} + \phi_2 \rho_{j-2}, \quad \text{para } j > 0,$$

onde $\rho_j = \frac{\gamma_j}{\gamma_0}$ √© a autocorrela√ß√£o no lag $j$ [^49].

Para $j=1$ e $j=2$, temos:

$$\rho_1 = \phi_1 + \phi_2 \rho_1$$
$$\rho_2 = \phi_1 \rho_1 + \phi_2$$

Resolvendo para $\rho_1$, obtemos:

$$\rho_1 = \frac{\phi_1}{1 - \phi_2}$$

Conhecendo $\rho_1$, podemos calcular $\rho_2$:

$$\rho_2 = \phi_1 \frac{\phi_1}{1 - \phi_2} + \phi_2 = \frac{\phi_1^2 + \phi_2(1 - \phi_2)}{1 - \phi_2}$$

*Prova:*

I. Partimos das equa√ß√µes:
   $$\rho_1 = \phi_1 + \phi_2 \rho_1$$
   $$\rho_2 = \phi_1 \rho_1 + \phi_2$$

II. Resolvemos a primeira equa√ß√£o para $\rho_1$:
   $$\rho_1 - \phi_2 \rho_1 = \phi_1$$
   $$\rho_1(1 - \phi_2) = \phi_1$$
   $$\rho_1 = \frac{\phi_1}{1 - \phi_2}$$

III. Substitu√≠mos o valor de $\rho_1$ na segunda equa√ß√£o:
   $$\rho_2 = \phi_1 \left(\frac{\phi_1}{1 - \phi_2}\right) + \phi_2$$
   $$\rho_2 = \frac{\phi_1^2}{1 - \phi_2} + \phi_2$$

IV. Combinamos os termos para obter a express√£o para $\rho_2$:
   $$\rho_2 = \frac{\phi_1^2 + \phi_2(1 - \phi_2)}{1 - \phi_2}$$

Portanto,
$$\rho_1 = \frac{\phi_1}{1 - \phi_2}$$
$$\rho_2 = \frac{\phi_1^2 + \phi_2(1 - \phi_2)}{1 - \phi_2}$$ ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Suponha $\phi_1 = 0.7$ e $\phi_2 = 0.1$. Ent√£o,
>
> $$\rho_1 = \frac{0.7}{1 - 0.1} = \frac{0.7}{0.9} \approx 0.778$$
>
> $$\rho_2 = \frac{(0.7)^2 + 0.1(1 - 0.1)}{1 - 0.1} = \frac{0.49 + 0.1(0.9)}{0.9} = \frac{0.49 + 0.09}{0.9} = \frac{0.58}{0.9} \approx 0.644$$
>
> Uma autocorrela√ß√£o $\rho_1$ de 0.778 indica uma forte correla√ß√£o entre um valor na s√©rie temporal e o valor anterior. Da mesma forma, $\rho_2$ de 0.644 indica uma correla√ß√£o moderada entre um valor na s√©rie e o valor dois per√≠odos atr√°s. Estes valores ajudam a entender como os valores passados influenciam os valores presentes na s√©rie temporal.

De forma mais gen√©rica, pode-se utilizar a seguinte forma da autocovari√¢ncia:

**Teorema 3.** Seja $\{Y_t\}$ um processo AR(2) estacion√°rio com $E[Y_t] = 0$. Ent√£o, a fun√ß√£o de autocovari√¢ncia $\gamma(h) = Cov(Y_t, Y_{t-h})$ satisfaz:

$$\gamma(h) = \phi_1 \gamma(h-1) + \phi_2 \gamma(h-2)$$

para $h = 1, 2, ...$

*Proof:*

I. Partimos da equa√ß√£o do processo AR(2):
   $$Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \epsilon_t$$

II. Multiplicamos ambos os lados por $Y_{t-h}$ e tomamos a esperan√ßa:
    $$E[Y_t Y_{t-h}] = E[(\phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \epsilon_t) Y_{t-h}]$$

III. Usando a linearidade da esperan√ßa:
     $$E[Y_t Y_{t-h}] = \phi_1 E[Y_{t-1} Y_{t-h}] + \phi_2 E[Y_{t-2} Y_{t-h}] + E[\epsilon_t Y_{t-h}]$$

IV. Assumindo que $\epsilon_t$ √© n√£o correlacionado com $Y_{t-h}$ para $h > 0$, temos $E[\epsilon_t Y_{t-h}] = 0$ para $h > 0$. Portanto:
    $$E[Y_t Y_{t-h}] = \phi_1 E[Y_{t-1} Y_{t-h}] + \phi_2 E[Y_{t-2} Y_{t-h}]$$

V. Reconhecendo que $E[Y_t Y_{t-h}] = \gamma(h)$, $E[Y_{t-1} Y_{t-h}] = \gamma(h-1)$ e $E[Y_{t-2} Y_{t-h}] = \gamma(h-2)$:
   $$\gamma(h) = \phi_1 \gamma(h-1) + \phi_2 \gamma(h-2)$$

Portanto, para um processo AR(2) estacion√°rio com m√©dia zero, a fun√ß√£o de autocovari√¢ncia $\gamma(h)$ satisfaz $\gamma(h) = \phi_1 \gamma(h-1) + \phi_2 \gamma(h-2)$ para $h = 1, 2, ...$ ‚ñ†

Al√©m disso, podemos caracterizar a fun√ß√£o de autocovari√¢ncia em termos das ra√≠zes da equa√ß√£o caracter√≠stica.

**Teorema 3.1.** Seja $\{Y_t\}$ um processo AR(2) estacion√°rio e seja $z_1$ e $z_2$ as ra√≠zes da equa√ß√£o caracter√≠stica $1 - \phi_1 z - \phi_2 z^2 = 0$. Se $z_1 \neq z_2$, ent√£o a fun√ß√£o de autocovari√¢ncia pode ser expressa como:

$$\gamma(h) = A z_1^{-h} + B z_2^{-h}$$

Onde $A$ e $B$ s√£o constantes determinadas pelas condi√ß√µes iniciais $\gamma(0)$ e $\gamma(1)$.

*Proof (Sketch):*

A equa√ß√£o $\gamma(h) = \phi_1 \gamma(h-1) + \phi_2 \gamma(h-2)$ √© uma equa√ß√£o de diferen√ßas lineares de segunda ordem. A solu√ß√£o geral para tal equa√ß√£o √© da forma $\gamma(h) = A z_1^{-h} + B z_2^{-h}$, onde $z_1$ e $z_2$ s√£o as ra√≠zes da equa√ß√£o caracter√≠stica associada. As constantes $A$ e $B$ s√£o encontradas resolvendo um sistema de equa√ß√µes usando os valores conhecidos de $\gamma(0)$ e $\gamma(1)$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um processo AR(2) com $\phi_1 = 0.5$ e $\phi_2 = -0.25$. A equa√ß√£o caracter√≠stica √© $1 - 0.5z + 0.25z^2 = 0$. As ra√≠zes s√£o $z_1 = 1+i$ e $z_2 = 1-i$.
>
> Neste caso, a fun√ß√£o de autocovari√¢ncia seria da forma $\gamma(h) = A(1+i)^{-h} + B(1-i)^{-h}$. Para determinar A e B, precisar√≠amos dos valores de $\gamma(0)$ e $\gamma(1)$. Por exemplo, se $\gamma(0) = 2$ e $\gamma(1) = 1$, podemos resolver o sistema de equa√ß√µes:
>
> $A + B = 2$
>
> $A(1+i)^{-1} + B(1-i)^{-1} = 1$
>
> Resolvendo este sistema, encontramos A e B, que ent√£o nos permitem calcular $\gamma(h)$ para qualquer valor de h.

**Fun√ß√£o Geradora de Autocovari√¢ncia (FGAC)**

A fun√ß√£o geradora de autocovari√¢ncia (FGAC) √© uma ferramenta valiosa para analisar as propriedades de um processo de s√©ries temporais. Para um processo AR(2), a FGAC √© definida como:

$$G(z) = \sum_{h=-\infty}^{\infty} \gamma(h) z^h = \frac{\sigma^2}{|1-\phi_1 z - \phi_2 z^2|^2}$$

Onde *z* √© uma vari√°vel complexa. Essa fun√ß√£o gera as autocovari√¢ncias do processo como coeficientes em sua expans√£o de s√©rie. A FGAC √© usada para analisar a estrutura de depend√™ncia da s√©rie temporal, onde os p√≥los de G(z) est√£o relacionados aos par√¢metros do processo.

Para complementar a an√°lise da FGAC, podemos introduzir o conceito de Densidade Espectral de Pot√™ncia (DEP).

**Proposi√ß√£o 4:** A Densidade Espectral de Pot√™ncia (DEP) de um processo AR(2) estacion√°rio √© dada pela Transformada de Fourier da sua fun√ß√£o de autocovari√¢ncia.

*Proof (Sketch):*

A DEP, denotada por $S(\omega)$, √© definida como a Transformada de Fourier da fun√ß√£o de autocovari√¢ncia $\gamma(h)$:

$$S(\omega) = \sum_{h=-\infty}^{\infty} \gamma(h) e^{-j\omega h}$$

Onde $\omega$ representa a frequ√™ncia angular. Para um processo AR(2), substitu√≠mos a express√£o da fun√ß√£o de autocovari√¢ncia na defini√ß√£o da DEP e simplificamos para obter a forma fechada da DEP em termos dos par√¢metros do processo AR(2) e da vari√¢ncia do ru√≠do branco. A DEP revela como a pot√™ncia do sinal √© distribu√≠da ao longo das diferentes frequ√™ncias. ‚ñ†

**Implementa√ß√£o Algor√≠tmica:**

A implementa√ß√£o de um processo AR(2) envolve os seguintes passos:

1.  **Inicializa√ß√£o:** Definir os par√¢metros $c$, $\phi_1$, $\phi_2$ e $\sigma^2$ [^53]. Inicializar $Y_0$ e $Y_1$ (e.g., $Y_0 = Y_1 = 0$ ou valores aleat√≥rios) [^53].

2.  **Itera√ß√£o:** Para $t = 2, 3, \dots, T$:

    a. Gerar um valor aleat√≥rio $\epsilon_t$ de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia $\sigma^2$ [^47].

    b. Calcular $Y_t$ usando a equa√ß√£o do AR(2): $Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \epsilon_t$ [^53].

    c. Armazenar $Y_t$ [^53].

3.  **Sa√≠da:** Retornar a s√©rie temporal $\{Y_2, Y_3, \dots, Y_T\}$ [^53].

> üí° **Exemplo em Python:**
>
> ```python
> import numpy as np
>
> def generate_ar2(c, phi1, phi2, sigma, T):
>     """
>     Gera uma s√©rie temporal AR(2).
>
>     Args:
>         c (float): Termo constante.
>         phi1 (float): Primeiro par√¢metro autorregressivo.
>         phi2 (float): Segundo par√¢metro autorregressivo.
>         sigma (float): Desvio padr√£o do ru√≠do branco.
>         T (int): Comprimento da s√©rie temporal.
>
>     Returns:
>         numpy.ndarray: A s√©rie temporal AR(2).
>     """
>     # Check for stationarity conditions
>     if not (phi1 + phi2 < 1 and phi2 - phi1 < 1 and -1 < phi2 < 1):
>         raise ValueError("Parameters do not satisfy stationarity conditions.")
>
>     epsilon = np.random.normal(0, sigma, T)
>     Y = np.zeros(T)
>     Y[0] = epsilon[0]  # Initialize Y[0] with the first white noise value
>     Y[1] = c + phi1 * Y[0] + epsilon[1] # Initialize Y[1] based on Y[0] and a new epsilon
>
>     for t in range(2, T):
>         Y[t] = c + phi1 * Y[t-1] + phi2 * Y[t-2] + epsilon[t]
>     return Y
>
> # Exemplo de uso:
> c = 0.5
> phi1 = 0.7
> phi2 = -0.2
> sigma = 1
> T = 100
> ar2_series = generate_ar2(c, phi1, phi2, sigma, T)
>
> import matplotlib.pyplot as plt
> plt.plot(ar2_series)
> plt.title('AR(2) Time Series')
> plt.xlabel('Time')
> plt.ylabel('Y_t')
> plt.show()
> ```
>
> Este c√≥digo implementa o algoritmo iterativo para gerar uma s√©rie temporal AR(2), verificando as condi√ß√µes de estacionariedade [^53]. Se as condi√ß√µes n√£o forem satisfeitas, uma exce√ß√£o √© levantada [^53].
> A principal diferen√ßa em rela√ß√£o √† implementa√ß√£o do AR(1) √© a necessidade de inicializar dois valores ($Y_0$ e $Y_1$) e usar dois valores passados na equa√ß√£o recursiva [^53].
>
> **An√°lise de Res√≠duos:**
>
> Ap√≥s ajustar um modelo AR(2) a uma s√©rie temporal real, √© importante analisar os res√≠duos ($\epsilon_t$) para verificar se o modelo √© adequado. Os res√≠duos devem se comportar como ru√≠do branco, ou seja, devem ter m√©dia zero, vari√¢ncia constante e n√£o apresentar autocorrela√ß√£o significativa. Podemos verificar isso plotando o ACF (Autocorrelation Function) dos res√≠duos e realizando testes estat√≠sticos, como o teste de Ljung-Box, para verificar a aleatoriedade dos res√≠duos. Se os res√≠duos n√£o se comportarem como ru√≠do branco, isso indica que o modelo AR(2) pode n√£o ser adequado para a s√©rie temporal em quest√£o, e modelos mais complexos, como ARMA ou ARIMA, podem ser necess√°rios.
>
> **Interpreta√ß√£o no Mundo Real:**
>
> Considere modelar as vendas mensais de uma loja usando um modelo AR(2). Se $\phi_1 = 0.6$, isso significa que as vendas do m√™s atual dependem fortemente das vendas do m√™s anterior. Se $\phi_2 = 0.3$, isso indica que as vendas de dois meses atr√°s tamb√©m t√™m uma influ√™ncia significativa nas vendas atuais. Compreender esses coeficientes pode ajudar a loja a prever as vendas futuras e a tomar decis√µes de estoque mais informadas.

**Complexidade Algor√≠tmica**:

A implementa√ß√£o iterativa do AR(2) tem complexidade de tempo $O(T)$, onde $T$ √© o comprimento da s√©rie temporal [^53]. A valida√ß√£o da estacionariedade tem complexidade de tempo $O(1)$, pois envolve apenas algumas compara√ß√µes aritm√©ticas [^53].

### Conclus√£o

Este cap√≠tulo introduziu os processos autorregressivos de segunda ordem (AR(2)), enfatizando as condi√ß√µes de estacionariedade e a implementa√ß√£o algor√≠tmica eficiente. A verifica√ß√£o das condi√ß√µes de estacionariedade √© crucial para garantir a validade do modelo AR(2). A natureza iterativa do modelo AR(2) permite uma implementa√ß√£o computacionalmente eficiente, tornando-o adequado para a an√°lise de longas s√©ries temporais [^53]. Em continuidade, ser√£o exploradas as implica√ß√µes da estrutura AR(2) para a modelagem de s√©ries temporais complexas e sua rela√ß√£o com outros modelos, como os processos de m√©dias m√≥veis (MA) e os modelos ARMA [^53]. Al√©m disso, t√©cnicas de estimativa dos par√¢metros ser√£o exploradas para tornar o modelo adapt√°vel aos dados e √∫til para previs√£o.

### Refer√™ncias
[^53]: Y‚ÇÅ = c + Y-1 + Œµ.
[^47]: The basic building block for all the processes considered in this chapter is a sequence {8} - whose elements have mean zero and variance œÉ¬≤.
[^49]: The jth autocorrelation of a covariance-stationary process (denoted p‚ÇÅ) is defined as its jth autocovariance divided by the variance: Pj = Œ≥j/Œ≥o
<!-- END -->