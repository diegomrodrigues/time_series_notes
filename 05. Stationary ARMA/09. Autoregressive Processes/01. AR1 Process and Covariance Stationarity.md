## Autoregressive Processes: Exploring the AR(1) Model

### Introdu√ß√£o

Expandindo sobre os processos estoc√°sticos, este cap√≠tulo aprofunda-se nos **processos autorregressivos (AR)**, com foco prim√°rio no modelo **AR(1)**. Como veremos, a estabilidade e as propriedades estat√≠sticas de um modelo AR(1) s√£o intrinsecamente ligadas ao valor do par√¢metro autorregressivo $\phi$. Este cap√≠tulo visa fornecer uma an√°lise detalhada das condi√ß√µes para estacionariedade de covari√¢ncia e do comportamento dos modelos AR(1) sob diferentes regimes de $\phi$.

### Conceitos Fundamentais

Um **processo autorregressivo de primeira ordem**, denotado **AR(1)**, √© definido pela seguinte equa√ß√£o de diferen√ßa [^46]:

$$Y_t = c + \phi Y_{t-1} + \epsilon_t,$$

onde:
*   $Y_t$ representa o valor da s√©rie temporal no instante $t$.
*   $c$ √© uma constante.
*   $\phi$ √© o **par√¢metro autorregressivo**.
*   $\epsilon_t$ √© um termo de **ru√≠do branco** [^46, 47] (uma sequ√™ncia de vari√°veis aleat√≥rias n√£o correlacionadas com m√©dia zero e vari√¢ncia constante, $\sigma^2$). Especificamente, assume-se que $\{\epsilon_t\}$ satisfaz as condi√ß√µes [^47]:

    $$E(\epsilon_t) = 0,$$
    $$E(\epsilon_t \epsilon_\tau) = \begin{cases} \sigma^2, & \text{se } t = \tau \\ 0, & \text{se } t \ne \tau \end{cases}.$$

A equa√ß√£o acima implica que o valor atual da s√©rie temporal ($Y_t$) √© uma fun√ß√£o linear do seu valor anterior ($Y_{t-1}$), somado a um termo de ru√≠do branco e uma constante [^46]. A influ√™ncia do valor anterior √© controlada pelo par√¢metro $\phi$.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um processo AR(1) com $c = 5$, $\phi = 0.7$ e $\epsilon_t$ √© um ru√≠do branco com vari√¢ncia $\sigma^2 = 2$. Ent√£o, a equa√ß√£o para este processo √©:
>
> $Y_t = 5 + 0.7 Y_{t-1} + \epsilon_t$
>
> Se $Y_0 = 10$ e $\epsilon_1 = 0.5$, ent√£o:
>
> $Y_1 = 5 + 0.7 * 10 + 0.5 = 5 + 7 + 0.5 = 12.5$
>
> Se $\epsilon_2 = -1$, ent√£o:
>
> $Y_2 = 5 + 0.7 * 12.5 - 1 = 5 + 8.75 - 1 = 12.75$
>
> Este exemplo ilustra como o valor atual da s√©rie temporal depende do valor anterior e do ru√≠do branco.

**Estacionariedade de Covari√¢ncia**:

A estacionariedade de covari√¢ncia √© uma propriedade crucial para a an√°lise de s√©ries temporais. Um processo √© dito estacion√°rio em covari√¢ncia se sua m√©dia e autocovari√¢ncia n√£o variam com o tempo. Para um processo AR(1), a condi√ß√£o para estacionariedade de covari√¢ncia √© dada por:

$$|\phi| < 1.$$

Se essa condi√ß√£o n√£o for satisfeita ($|\phi| \ge 1$), o processo n√£o ser√° estacion√°rio em covari√¢ncia [^53]. Em outras palavras, para que um processo AR(1) seja estacion√°rio em covari√¢ncia, o valor absoluto do par√¢metro autorregressivo ($\phi$) deve ser menor que 1.

> üí° **Exemplo Num√©rico:**
>
> Se $\phi = 0.5$, ent√£o $|\phi| = 0.5 < 1$, e o processo AR(1) √© estacion√°rio em covari√¢ncia.
>
> Se $\phi = -0.8$, ent√£o $|\phi| = 0.8 < 1$, e o processo AR(1) √© estacion√°rio em covari√¢ncia.
>
> Se $\phi = 1.2$, ent√£o $|\phi| = 1.2 > 1$, e o processo AR(1) n√£o √© estacion√°rio em covari√¢ncia.
>
> Se $\phi = -1$, ent√£o $|\phi| = 1$, e o processo AR(1) n√£o √© estacion√°rio em covari√¢ncia.

**An√°lise da Condi√ß√£o de Estacionariedade**:

A condi√ß√£o $|\phi| < 1$ garante que o impacto de choques passados ($\epsilon_t$) na s√©rie temporal diminua exponencialmente ao longo do tempo [^53]. Quando $|\phi| \geq 1$, os choques passados podem ter um efeito persistente ou crescente sobre a s√©rie temporal, levando a um comportamento n√£o estacion√°rio [^53].

Podemos reescrever a equa√ß√£o do AR(1) como [^53]:

$$Y_t = c + \phi Y_{t-1} + \epsilon_t = \frac{c}{1 - \phi} + \sum_{i=0}^{\infty} \phi^i \epsilon_{t-i}.$$

Essa representa√ß√£o √© v√°lida apenas se $|\phi| < 1$. Caso contr√°rio, a soma infinita n√£o converge, e a vari√¢ncia de $Y_t$ seria infinita, violando a condi√ß√£o de estacionariedade de covari√¢ncia [^53].

> üí° **Exemplo Num√©rico:**
>
> Seja $c = 10$ e $\phi = 0.5$. Ent√£o $\frac{c}{1 - \phi} = \frac{10}{1 - 0.5} = \frac{10}{0.5} = 20$. Portanto, a m√©dia do processo √© 20.  Agora, seja $\epsilon_t$ uma s√©rie de ru√≠do branco.  Ent√£o
>
> $Y_t = 20 + \epsilon_t + 0.5\epsilon_{t-1} + 0.25\epsilon_{t-2} + 0.125\epsilon_{t-3} + \ldots$
>
> Cada termo $\epsilon_{t-i}$ √© ponderado por $\phi^i$, que diminui √† medida que $i$ aumenta, refletindo o decaimento do impacto de choques passados.
>
> Se $\phi = 1.1$, ent√£o a soma $\sum_{i=0}^{\infty} (1.1)^i \epsilon_{t-i}$ diverge, e o processo n√£o √© estacion√°rio.

Para complementar essa an√°lise, podemos formalizar a converg√™ncia da soma infinita atrav√©s do seguinte lema:

**Lema 1** A s√©rie $\sum_{i=0}^{\infty} \phi^i \epsilon_{t-i}$ converge absolutamente se e somente se $|\phi| < 1$, dado que $E[\epsilon_t^2] = \sigma^2 < \infty$.

**Prova:**
A converg√™ncia absoluta da s√©rie implica que $\sum_{i=0}^{\infty} |\phi^i \epsilon_{t-i}|$ converge. Tomando o valor esperado da soma dos quadrados, temos:
$E\left[\left(\sum_{i=0}^{\infty} |\phi^i \epsilon_{t-i}|\right)^2\right] = \sum_{i=0}^{\infty} \sum_{j=0}^{\infty} |\phi|^{i+j} E[\epsilon_{t-i} \epsilon_{t-j}]$.
Como $E[\epsilon_{t-i} \epsilon_{t-j}] = 0$ se $i \neq j$ e $E[\epsilon_{t-i}^2] = \sigma^2$, ent√£o
$E\left[\left(\sum_{i=0}^{\infty} |\phi^i \epsilon_{t-i}|\right)^2\right] = \sigma^2 \sum_{i=0}^{\infty} |\phi|^{2i}$.
A s√©rie $\sum_{i=0}^{\infty} |\phi|^{2i}$ √© uma s√©rie geom√©trica que converge se e somente se $|\phi|^2 < 1$, o que √© equivalente a $|\phi| < 1$. Portanto, a s√©rie converge absolutamente se e somente se $|\phi| < 1$. ‚ñ†

Al√©m disso, a seguinte proposi√ß√£o estabelece uma condi√ß√£o suficiente para a estacionaridade forte de um processo AR(1):

**Proposi√ß√£o 1** Se $|\phi| < 1$ e $\epsilon_t$ s√£o independentes e identicamente distribu√≠dos (i.i.d.) com m√©dia zero e vari√¢ncia finita, ent√£o o processo AR(1) √© fortemente estacion√°rio.

**Prova:**
A estacionaridade forte requer que a distribui√ß√£o conjunta de $(Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n})$ seja a mesma que a distribui√ß√£o conjunta de $(Y_{t_1+k}, Y_{t_2+k}, \ldots, Y_{t_n+k})$ para qualquer inteiro $k$. Como $Y_t$ pode ser expresso como uma fun√ß√£o de $\epsilon_t, \epsilon_{t-1}, \epsilon_{t-2}, \ldots$ e os $\epsilon_t$ s√£o i.i.d., a distribui√ß√£o conjunta de qualquer conjunto de $Y_t$ √© determinada unicamente pela distribui√ß√£o dos $\epsilon_t$. Portanto, a distribui√ß√£o conjunta de $(Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n})$ √© a mesma que a distribui√ß√£o conjunta de $(Y_{t_1+k}, Y_{t_2+k}, \ldots, Y_{t_n+k})$, e o processo √© fortemente estacion√°rio. ‚ñ†

**M√©dia e Vari√¢ncia de um Processo AR(1) Estacion√°rio**:

Sob a condi√ß√£o de estacionariedade ($|\phi| < 1$), podemos derivar a m√©dia e a vari√¢ncia do processo AR(1) [^53]. Tomando a esperan√ßa em ambos os lados da equa√ß√£o $Y_t = c + \phi Y_{t-1} + \epsilon_t$, e assumindo que $E(Y_t) = E(Y_{t-1}) = \mu$ e $E(\epsilon_t) = 0$, obtemos:

$$\mu = c + \phi \mu,$$

o que implica que a m√©dia do processo AR(1) √© dada por [^53]:

$$\mu = \frac{c}{1 - \phi}.$$

> üí° **Exemplo Num√©rico:**
>
> Se $c = 2$ e $\phi = 0.5$, ent√£o a m√©dia do processo AR(1) √©:
>
> $\mu = \frac{2}{1 - 0.5} = \frac{2}{0.5} = 4.$
>
> Se $c = -5$ e $\phi = 0.2$, ent√£o a m√©dia do processo AR(1) √©:
>
> $\mu = \frac{-5}{1 - 0.2} = \frac{-5}{0.8} = -6.25.$

A vari√¢ncia do processo AR(1) pode ser obtida da seguinte forma [^53, 54]:

$$\gamma_0 = E[(Y_t - \mu)^2] = E[(\phi(Y_{t-1} - \mu) + \epsilon_t)^2].$$

Assumindo que $\epsilon_t$ √© n√£o correlacionado com $Y_{t-1}$, temos:

$$\gamma_0 = \phi^2 E[(Y_{t-1} - \mu)^2] + E[\epsilon_t^2] = \phi^2 \gamma_0 + \sigma^2.$$

Portanto, a vari√¢ncia do processo AR(1) √© [^54]:

$$\gamma_0 = \frac{\sigma^2}{1 - \phi^2}.$$

> üí° **Exemplo Num√©rico:**
>
> Se $\sigma^2 = 1$ e $\phi = 0.5$, ent√£o a vari√¢ncia do processo AR(1) √©:
>
> $\gamma_0 = \frac{1}{1 - (0.5)^2} = \frac{1}{1 - 0.25} = \frac{1}{0.75} = \frac{4}{3} \approx 1.33.$
>
> Se $\sigma^2 = 4$ e $\phi = -0.2$, ent√£o a vari√¢ncia do processo AR(1) √©:
>
> $\gamma_0 = \frac{4}{1 - (-0.2)^2} = \frac{4}{1 - 0.04} = \frac{4}{0.96} = \frac{25}{6} \approx 4.17.$

**Autocovari√¢ncia e Autocorrela√ß√£o**:

A autocovari√¢ncia de lag $j$ √© definida como [^45]:

$$ \gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)].$$

Para um AR(1) [^53], a autocovari√¢ncia satisfaz a seguinte rela√ß√£o recursiva [^54]:

$$\gamma_j = \phi \gamma_{j-1}, \quad \text{para } j > 0.$$

**Prova:**
Para derivar a rela√ß√£o recursiva para a autocovari√¢ncia, seguimos os seguintes passos:

I. Come√ßamos com a defini√ß√£o da autocovari√¢ncia no lag $j$:
   $$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)].$$

II. Substitu√≠mos $Y_t$ pela sua express√£o do processo AR(1): $Y_t = c + \phi Y_{t-1} + \epsilon_t$.  Subtraindo a m√©dia $\mu$ de ambos os lados, temos $Y_t - \mu = \phi (Y_{t-1} - \mu) + \epsilon_t$.

III. Substitu√≠mos essa express√£o na f√≥rmula da autocovari√¢ncia:
    $$\gamma_j = E[(\phi (Y_{t-1} - \mu) + \epsilon_t)(Y_{t-j} - \mu)].$$

IV. Expandimos o valor esperado, notando que $E[\epsilon_t (Y_{t-j} - \mu)] = 0$ para $j > 0$ (j√° que $\epsilon_t$ √© ru√≠do branco e n√£o correlacionado com valores passados de $Y$):
    $$\gamma_j = E[\phi (Y_{t-1} - \mu)(Y_{t-j} - \mu)] + E[\epsilon_t (Y_{t-j} - \mu)] = \phi E[(Y_{t-1} - \mu)(Y_{t-j} - \mu)].$$

V.  Reescrevemos o termo dentro do valor esperado, notando que $E[(Y_{t-1} - \mu)(Y_{t-j} - \mu)]$ √© a autocovari√¢ncia no lag $j-1$, ou seja, $\gamma_{j-1}$:
     $$\gamma_j = \phi \gamma_{j-1}.$$

Portanto, a autocovari√¢ncia no lag $j$ √© $\phi$ vezes a autocovari√¢ncia no lag $j-1$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Se $\phi = 0.5$ e $\gamma_0 = \frac{4}{3}$ (como calculado anteriormente), ent√£o:
>
> $\gamma_1 = 0.5 * \frac{4}{3} = \frac{2}{3} \approx 0.67$
>
> $\gamma_2 = 0.5 * \frac{2}{3} = \frac{1}{3} \approx 0.33$
>
> $\gamma_3 = 0.5 * \frac{1}{3} = \frac{1}{6} \approx 0.17$
>
> A autocovari√¢ncia diminui √† medida que o lag aumenta, refletindo o decaimento da depend√™ncia temporal.

A autocorrela√ß√£o de lag $j$, denotada por $\rho_j$, √© definida como a autocovari√¢ncia dividida pela vari√¢ncia [^49]:

$$\rho_j = \frac{\gamma_j}{\gamma_0} = \phi^j.$$

**Prova:**
Para derivar a express√£o para a autocorrela√ß√£o $\rho_j$, seguimos os seguintes passos:

I. Come√ßamos com a defini√ß√£o da autocorrela√ß√£o:
   $$\rho_j = \frac{\gamma_j}{\gamma_0}.$$

II. Usamos a rela√ß√£o recursiva da autocovari√¢ncia $\gamma_j = \phi \gamma_{j-1}$ para expressar $\gamma_j$ em termos de $\gamma_0$:
    $$\gamma_j = \phi \gamma_{j-1} = \phi (\phi \gamma_{j-2}) = \phi^2 \gamma_{j-2} = \ldots = \phi^j \gamma_0.$$

III. Substitu√≠mos essa express√£o na f√≥rmula da autocorrela√ß√£o:
     $$\rho_j = \frac{\phi^j \gamma_0}{\gamma_0} = \phi^j.$$

Portanto, a autocorrela√ß√£o no lag $j$ √© $\phi^j$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Se $\phi = 0.5$, ent√£o:
>
> $\rho_0 = (0.5)^0 = 1$
>
> $\rho_1 = (0.5)^1 = 0.5$
>
> $\rho_2 = (0.5)^2 = 0.25$
>
> $\rho_3 = (0.5)^3 = 0.125$
>
> A autocorrela√ß√£o tamb√©m decai geometricamente, como a autocovari√¢ncia.

A fun√ß√£o de autocorrela√ß√£o (ACF) de um processo AR(1) exibe um decaimento geom√©trico √† medida que o lag $j$ aumenta [^54]. Se $\phi$ for positivo, a ACF decair√° exponencialmente para zero a partir de valores positivos. Se $\phi$ for negativo, a ACF alternar√° entre valores positivos e negativos, decaindo exponencialmente em magnitude [^54].

> üí° **Exemplo Num√©rico:**
>
> Considere $\phi = -0.7$. A ACF ter√° o seguinte padr√£o:
>
> $\rho_0 = 1$
>
> $\rho_1 = -0.7$
>
> $\rho_2 = (-0.7)^2 = 0.49$
>
> $\rho_3 = (-0.7)^3 = -0.343$
>
> $\rho_4 = (-0.7)^4 = 0.2401$
>
> Observe que a ACF alterna entre valores positivos e negativos, decaindo em magnitude.
>
> ```mermaid
> graph LR
>     A[Lag 0: 1] --> B(Lag 1: -0.7)
>     B --> C(Lag 2: 0.49)
>     C --> D(Lag 3: -0.343)
>     D --> E(Lag 4: 0.2401)
> ```

Podemos estender a an√°lise da autocorrela√ß√£o com o seguinte resultado:

**Teorema 1** Para um processo AR(1) estacion√°rio, a fun√ß√£o de autocorrela√ß√£o parcial (PACF) √© zero para todos os lags maiores que 1.

**Prova:**
A fun√ß√£o de autocorrela√ß√£o parcial (PACF) mede a correla√ß√£o entre $Y_t$ e $Y_{t-j}$ ap√≥s remover a influ√™ncia dos lags intermedi√°rios $Y_{t-1}, Y_{t-2}, \ldots, Y_{t-j+1}$. Para um AR(1), a depend√™ncia de $Y_t$ em rela√ß√£o ao passado √© completamente capturada por $Y_{t-1}$. Portanto, a correla√ß√£o entre $Y_t$ e $Y_{t-j}$ para $j > 1$, ap√≥s remover a influ√™ncia de $Y_{t-1}$, √© zero. Matematicamente, $PACF(1) = \rho_1 = \phi$, e $PACF(j) = 0$ para $j > 1$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Se $\phi = 0.6$, ent√£o $PACF(1) = 0.6$ e $PACF(j) = 0$ para $j > 1$. Isso significa que a √∫nica correla√ß√£o parcial significativa √© no lag 1, confirmando a natureza AR(1) do processo.

Dado que conhecemos a autocovari√¢ncia, √© natural investigar a fun√ß√£o geradora de autocovari√¢ncia.

**Teorema 1.1** A fun√ß√£o geradora de autocovari√¢ncia (FGAC) para um processo AR(1) estacion√°rio √© dada por:

$$g(z) = \frac{\sigma^2}{1 - \phi z} \cdot \frac{1}{1 - \phi z^{-1}}.$$

**Prova:**
A fun√ß√£o geradora de autocovari√¢ncia √© definida como $g(z) = \sum_{j=-\infty}^{\infty} \gamma_j z^j$, onde $\gamma_j$ √© a autocovari√¢ncia no lag $j$. Para um processo AR(1), sabemos que $\gamma_j = \phi^{|j|} \gamma_0$, onde $\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$. Portanto:

$$g(z) = \sum_{j=-\infty}^{\infty} \phi^{|j|} \gamma_0 z^j = \gamma_0 \left(\sum_{j=0}^{\infty} \phi^j z^j + \sum_{j=1}^{\infty} \phi^j z^{-j}\right).$$

Usando a f√≥rmula para a soma de uma s√©rie geom√©trica, temos:

$$g(z) = \gamma_0 \left(\frac{1}{1 - \phi z} + \frac{\phi z^{-1}}{1 - \phi z^{-1}}\right) = \gamma_0 \left(\frac{1}{1 - \phi z} + \frac{\phi}{z - \phi}\right).$$

Simplificando, obtemos:

$$g(z) = \frac{\sigma^2}{1 - \phi^2} \left(\frac{1}{1 - \phi z} - \frac{\phi}{ \phi - z}\right) = \frac{\sigma^2}{1 - \phi^2} \left(\frac{1}{1 - \phi z} + \frac{\phi}{z - \phi}\right) = \frac{\sigma^2}{1 - \phi^2} \cdot \frac{z - \phi + \phi - \phi^2 z}{(1 - \phi z)(z - \phi)}  = \frac{\sigma^2}{1 - \phi^2} \cdot \frac{z(1 - \phi^2)}{(1 - \phi z)(z - \phi)}$$
$$g(z) = \frac{\sigma^2}{(1 - \phi z)(1 - \phi z^{-1})}.$$

Este resultado √© √∫til para analisar o espectro do processo AR(1). ‚ñ†

### Conclus√£o

Neste cap√≠tulo, exploramos o modelo autorregressivo de primeira ordem (AR(1)), enfatizando a import√¢ncia do par√¢metro autorregressivo $\phi$ na determina√ß√£o da estacionariedade de covari√¢ncia e do comportamento do processo. A condi√ß√£o $|\phi| < 1$ √© crucial para garantir que o processo AR(1) seja estacion√°rio, permitindo a defini√ß√£o de m√©dia e vari√¢ncia finitas. Adicionalmente, a fun√ß√£o de autocorrela√ß√£o exibe um padr√£o de decaimento geom√©trico, refletindo a influ√™ncia decrescente de valores passados sobre o valor presente da s√©rie temporal. Em continuidade, a an√°lise de modelos AR de ordem superior e a combina√ß√£o com modelos de m√©dias m√≥veis (MA) ser√£o exploradas para uma representa√ß√£o mais rica de s√©ries temporais complexas.

### Refer√™ncias
[^46]: Imagine a battery of I such computers generating sequences {y{1}, {y{2} ... , y{1}}.
[^47]: The basic building block for all the processes considered in this chapter is a sequence {8} - whose elements have mean zero and variance œÉ¬≤.
[^45]: The variance of the random variable Y, (denoted you) is similarly defined as You = E(Y, ‚àí Œº,)¬≤ = ‚à´ (y, ‚àí Œº,)¬≤ fv,(y,) dy.
[^49]: The jth autocorrelation of a covariance-stationary process (denoted p‚ÇÅ) is defined as its jth autocovariance divided by the variance: Pj = Œ≥j/Œ≥o
[^53]: Y‚ÇÅ = c + Y-1 + Œµ.
[^54]: Œï(Œ•, ‚Äì Œº)¬≤ = Œï(Œµ, + Œ¶ŒµŒπ-1 + $28,-2 + $381-3 +)2
<!-- END -->