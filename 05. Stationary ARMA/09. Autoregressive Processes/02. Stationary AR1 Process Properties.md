## Autoregressive Processes: MA($\infty$) Decomposition and Stationary AR(1) Process Properties

### Introduction

Continuing the analysis of first-order autoregressive processes (AR(1)), this chapter is dedicated to exploring in detail the representation of a stationary AR(1) process as an infinite-order moving average (MA($\infty$)) and its properties. As we saw earlier, the condition $|\phi| < 1$ is crucial for the stationarity of the process [^53]. Under this condition, we can express an AR(1) process as a weighted sum of past white noises. This representation allows for a more in-depth analysis of the process's properties and facilitates comparison with other time series models.

### Fundamental Concepts

The representation of a stationary AR(1) process as an MA($\infty$) process is fundamental to understanding its temporal dependence [^53]. Starting from the AR(1) equation:

$$Y_t = c + \phi Y_{t-1} + \epsilon_t,$$

and using the stationarity condition $|\phi| < 1$, we can iteratively rewrite $Y_t$ in terms of past values of $\epsilon_t$. First, we subtract the process's mean from both sides [^53]:

$$Y_t - \mu = \phi (Y_{t-1} - \mu) + \epsilon_t.$$

Iterating this equation, we obtain:

$$Y_t - \mu = \phi [\phi (Y_{t-2} - \mu) + \epsilon_{t-1}] + \epsilon_t = \phi^2 (Y_{t-2} - \mu) + \phi \epsilon_{t-1} + \epsilon_t.$$

Continuing iteratively, we have [^53]:

$$Y_t - \mu = \sum_{j=0}^{\infty} \phi^j \epsilon_{t-j}.$$

Therefore, the AR(1) process can be expressed as [^53]:

$$Y_t = \frac{c}{1 - \phi} + \sum_{j=0}^{\infty} \phi^j \epsilon_{t-j},$$

where $\mu = \frac{c}{1 - \phi}$ is the process's mean [^53, 54]. This is the MA($\infty$) representation of the AR(1) process.

> ðŸ’¡ **Numerical Example:**
>
> If $c = 1$ and $\phi = 0.5$, then $\mu = \frac{1}{1 - 0.5} = 2$.  Thus,
>
> $Y_t = 2 + \epsilon_t + 0.5\epsilon_{t-1} + 0.25\epsilon_{t-2} + 0.125\epsilon_{t-3} + \ldots$
>
> This demonstrates that $Y_t$ is a weighted sum of all past white noise shocks, with weights decaying geometrically.

The MA($\infty$) representation allows us to derive important properties of the AR(1) process, such as the autocorrelation function. As mentioned previously [^45, 49], the autocorrelation function (ACF) for a stationary AR(1) process is given by:

$$\rho_j = \phi^j.$$

This function exhibits a geometric decay as the lag $j$ increases [^54]. If $\phi$ is positive, the ACF will decay exponentially towards zero from positive values. If $\phi$ is negative, the ACF will alternate between positive and negative values, decaying exponentially in magnitude [^54].

> ðŸ’¡ **Numerical Example:**
>
> If $\phi = 0.8$, then:
>
> *   $\rho_0 = 1$
> *   $\rho_1 = 0.8$
> *   $\rho_2 = 0.64$
> *   $\rho_3 = 0.512$
>
> The autocorrelation decays to zero as the lag increases.

As already derived previously, under the stationarity condition, the mean of the AR(1) process is:

$$ \mu = \frac{c}{1-\phi} $$

and the variance is:

$$ \gamma_0 = \frac{\sigma^2}{1 - \phi^2} $$

> ðŸ’¡ **Numerical Example:**
>
> Let's say $c=5$, $\phi = 0.7$ and $\sigma^2 = 2$. Then,
>
> $\mu = \frac{5}{1-0.7} = \frac{5}{0.3} \approx 16.67$
>
> $\gamma_0 = \frac{2}{1 - 0.7^2} = \frac{2}{1 - 0.49} = \frac{2}{0.51} \approx 3.92$
>
> The mean represents the long-term average value of the process and the variance quantifies its variability around this mean.

The autocovariance at lag *j* is given by:

$$\gamma_j = \frac{\phi^j}{1 - \phi^2} \sigma^2 = \phi^j \gamma_0$$

> ðŸ’¡ **Numerical Example:**
>
> Using the same parameters as above ($\phi = 0.7$, $\sigma^2 = 2$, $\gamma_0 \approx 3.92$):
>
> $\gamma_1 = 0.7 * 3.92 \approx 2.74$
>
> $\gamma_2 = 0.7^2 * 3.92 \approx 1.92$
>
> $\gamma_3 = 0.7^3 * 3.92 \approx 1.34$
>
> The autocovariance decreases as the lag increases, showing that the correlation between observations decreases with time.

The following theorem establishes a necessary and sufficient condition for the invertibility of an MA($\infty$) process resulting from the representation of an AR(1) process:

**Theorem 2** Given a stationary AR(1) process with $|\phi| < 1$, the resulting MA($\infty$) representation is always invertible.

**Proof:**
To prove that a stationary AR(1) process with $|\phi| < 1$ results in an invertible MA($\infty$) representation.

I.  Start with the MA($\infty$) representation of the AR(1) process:
    $$Y_t = \frac{c}{1 - \phi} + \sum_{j=0}^{\infty} \phi^j \epsilon_{t-j}$$

II.  Rewrite this as:
     $$Y_t = \mu + \epsilon_t + \phi\epsilon_{t-1} + \phi^2\epsilon_{t-2} + ...$$

III. The invertibility condition for an MA process requires that the roots of the characteristic equation lie outside the unit circle. The characteristic equation for the MA($\infty$) process is:
    $$1 + \phi L + \phi^2 L^2 + ... = 0$$
    where $L$ is the lag operator.

IV. Multiplying both sides by $(1-\phi L)$, we get:
    $$(1-\phi L)(1 + \phi L + \phi^2 L^2 + ...) = 1$$
    Thus, the MA($\infty$) process can be written as:
    $$(1 - \phi L)Y_t = (1 - \phi L)\mu + \epsilon_t$$
    $$Y_t - \phi Y_{t-1} = \mu - \phi\mu + \epsilon_t$$

V. Rearranging, we have:
   $$Y_t = \phi Y_{t-1} + (1 - \phi)\mu + \epsilon_t$$
   Since $\mu = \frac{c}{1-\phi}$, then $(1-\phi)\mu = c$, so
   $$Y_t = c + \phi Y_{t-1} + \epsilon_t$$
   This is our original AR(1) process.

VI. The invertibility condition is satisfied if $|\phi| < 1$, which is the condition for stationarity of the AR(1) process. Hence, the roots of the MA($\infty$) characteristic equation lie outside the unit circle.

VII. Therefore, given a stationary AR(1) process with $|\phi| < 1$, the resulting MA($\infty$) representation is always invertible. â– 

The following proposition establishes a relationship between the AR(1) and MA($\infty$) representations:

**Proposition 2** A stationary AR(1) process can be uniquely represented by an invertible MA($\infty$).

**Proof:**
I. Suppose we have a stationary AR(1) process, $Y_t = c + \phi Y_{t-1} + \epsilon_t$, where $|\phi| < 1$.

II. We have shown earlier that this AR(1) process can be written as an MA($\infty$) process: $Y_t = \frac{c}{1-\phi} + \sum_{j=0}^{\infty} \phi^j \epsilon_{t-j}$.

III. Since $|\phi| < 1$, the MA($\infty$) representation converges.

IV. From Theorem 2, we know that the MA($\infty$) representation is invertible if and only if $|\phi| < 1$, which is the condition for stationarity.

V. Therefore, a stationary AR(1) process can be uniquely represented by an invertible MA($\infty$).  â– 

> ðŸ’¡ **Numerical Example:**
>
> Let the AR(1) process be given by $Y_t = 0.5Y_{t-1} + \epsilon_t$. The root of the characteristic equation of the MA($\infty$) is $z = \frac{1}{0.5} = 2$, which is outside the unit circle. Therefore, the MA($\infty$) is invertible.

The MA($\infty$) representation is useful for analyzing the spectrum of the AR(1) process.

**Theorem 2.1** Given the MA($\infty$) representation of a stationary AR(1) process, the spectrum of the process is given by:

$$S(\omega) = \frac{\sigma^2}{2\pi (1 + \phi^2 - 2\phi \cos(\omega))},$$

where $\omega$ is the frequency and $\sigma^2$ is the variance of the white noise.

**Proof:**
To prove the given spectrum for a stationary AR(1) process using its MA($\infty$) representation.

I.  Begin with the MA($\infty$) representation:
    $$Y_t = \sum_{j=0}^{\infty} \phi^j \epsilon_{t-j}$$

II. The spectral density $S(\omega)$ is the Fourier transform of the autocovariance function $\gamma(k)$:
    $$S(\omega) = \sum_{k=-\infty}^{\infty} \gamma(k) e^{-i\omega k}$$

III. First, find the autocovariance function $\gamma(k)$. Since $Y_t = \sum_{j=0}^{\infty} \phi^j \epsilon_{t-j}$, the autocovariance at lag $k$ is:
    $$\gamma(k) = Cov(Y_t, Y_{t-k}) = Cov\left(\sum_{j=0}^{\infty} \phi^j \epsilon_{t-j}, \sum_{l=0}^{\infty} \phi^l \epsilon_{t-k-l}\right)$$

IV.  Using the properties of covariance and the fact that $Cov(\epsilon_t, \epsilon_s) = \sigma^2$ if $t = s$ and 0 otherwise:
     $$\gamma(k) = \sum_{j=0}^{\infty} \sum_{l=0}^{\infty} \phi^{j+l} Cov(\epsilon_{t-j}, \epsilon_{t-k-l}) = \sum_{j=0}^{\infty} \phi^{j+(j-k)} \sigma^2 = \sigma^2 \phi^{-k} \sum_{j=k}^{\infty} \phi^{2j} = \frac{\sigma^2 \phi^k}{1 - \phi^2}$$
     This result is valid for $k \geq 0$.  Since $\gamma(-k) = \gamma(k)$, we also have:
     $$\gamma(k) = \frac{\sigma^2 \phi^{|k|}}{1 - \phi^2}$$

V. Substitute the autocovariance function into the spectral density formula:
   $$S(\omega) = \sum_{k=-\infty}^{\infty} \frac{\sigma^2 \phi^{|k|}}{1 - \phi^2} e^{-i\omega k} = \frac{\sigma^2}{1 - \phi^2} \sum_{k=-\infty}^{\infty} \phi^{|k|} e^{-i\omega k}$$

VI. Split the summation into three parts: $k = 0$, $k > 0$, and $k < 0$:
    $$S(\omega) = \frac{\sigma^2}{1 - \phi^2} \left(1 + \sum_{k=1}^{\infty} \phi^k e^{-i\omega k} + \sum_{k=-\infty}^{-1} \phi^{-k} e^{-i\omega k}\right)$$

VII. Substitute $m = -k$ for the second summation:
     $$S(\omega) = \frac{\sigma^2}{1 - \phi^2} \left(1 + \sum_{k=1}^{\infty} \phi^k e^{-i\omega k} + \sum_{m=1}^{\infty} \phi^{m} e^{i\omega m}\right)$$

VIII. Use the formula for the sum of an infinite geometric series, $\sum_{k=1}^{\infty} ar^k = \frac{ar}{1-r}$:
      $$S(\omega) = \frac{\sigma^2}{1 - \phi^2} \left(1 + \frac{\phi e^{-i\omega}}{1 - \phi e^{-i\omega}} + \frac{\phi e^{i\omega}}{1 - \phi e^{i\omega}}\right)$$

IX. Combine the fractions:
    $$S(\omega) = \frac{\sigma^2}{1 - \phi^2} \left(\frac{(1 - \phi e^{-i\omega})(1 - \phi e^{i\omega}) + \phi e^{-i\omega}(1 - \phi e^{i\omega}) + \phi e^{i\omega}(1 - \phi e^{-i\omega})}{(1 - \phi e^{-i\omega})(1 - \phi e^{i\omega})}\right)$$

X. Simplify the numerator:
   $$S(\omega) = \frac{\sigma^2}{1 - \phi^2} \left(\frac{1 - \phi e^{i\omega} - \phi e^{-i\omega} + \phi^2 + \phi e^{-i\omega} - \phi^2 + \phi e^{i\omega} - \phi^2}{(1 - \phi e^{-i\omega})(1 - \phi e^{i\omega})}\right) = \frac{\sigma^2}{1 - \phi^2} \left(\frac{1 - \phi^2}{(1 - \phi e^{-i\omega})(1 - \phi e^{i\omega})}\right)$$

XI. Simplify further:
    $$S(\omega) = \frac{\sigma^2}{1 - \phi e^{i\omega} - \phi e^{-i\omega} + \phi^2} = \frac{\sigma^2}{1 + \phi^2 - \phi(e^{i\omega} + e^{-i\omega})}$$

XII. Use the identity $e^{i\omega} + e^{-i\omega} = 2 \cos(\omega)$:
     $$S(\omega) = \frac{\sigma^2}{1 + \phi^2 - 2\phi \cos(\omega)}$$

XIII. Finally, to normalize so that the integral over $(-\pi, \pi)$ is $\sigma^2$, we divide by $2\pi$:
   $$S(\omega) = \frac{\sigma^2}{2\pi(1 + \phi^2 - 2\phi \cos(\omega))}$$

Thus, we have proven the given spectrum for a stationary AR(1) process. â– 

> ðŸ’¡ **Numerical Example:**
>
> Let $\phi = 0.7$ and $\sigma^2 = 2$. Then the spectrum is:
>
> $S(\omega) = \frac{2}{2\pi (1 + 0.7^2 - 2*0.7*\cos(\omega))} = \frac{1}{\pi(1.49 - 1.4\cos(\omega))}$
>
> The spectrum shows how the variance of the time series is distributed over different frequencies. Peaks in the spectrum indicate dominant frequencies in the time series. If $\phi$ is positive, there will be a peak at low frequencies indicating slow fluctuations. If $\phi$ is negative, there will be a peak at high frequencies indicating rapid fluctuations.

Besides the MA($\infty$) representation, we can analyze the partial autocorrelation function (PACF) of the AR(1) process.

**Proposition 3** The partial autocorrelation function (PACF) of a stationary AR(1) process has a cutoff after lag 1. Specifically, $\alpha_1 = \phi$ and $\alpha_j = 0$ for $j > 1$, where $\alpha_j$ denotes the partial autocorrelation at lag $j$.

**Proof:**
To prove that the PACF of a stationary AR(1) process has a cutoff after lag 1.

I. Consider a stationary AR(1) process:
   $$Y_t = c + \phi Y_{t-1} + \epsilon_t$$

II. The PACF $\alpha_j$ measures the correlation between $Y_t$ and $Y_{t-j}$ after removing the effects of the intermediate lags $Y_{t-1}, Y_{t-2}, \dots, Y_{t-j+1}$.

III. For $j = 1$, the PACF is the correlation between $Y_t$ and $Y_{t-1}$, which is $\alpha_1 = \phi$.

IV. For $j = 2$, we want to find the correlation between $Y_t$ and $Y_{t-2}$ after removing the effect of $Y_{t-1}$. In other words, we want to find the coefficient $\alpha_2$ in the regression:
    $$Y_t = c' + \alpha_1' Y_{t-1} + \alpha_2 Y_{t-2} + \epsilon_t'$$

V. However, the AR(1) process is defined such that $Y_t$ only directly depends on $Y_{t-1}$. Therefore, once the effect of $Y_{t-1}$ is removed, there is no remaining correlation between $Y_t$ and $Y_{t-2}$. This means $\alpha_2 = 0$.

VI. More generally, for $j > 1$, the correlation between $Y_t$ and $Y_{t-j}$ is mediated by $Y_{t-1}$. Once the influence of $Y_{t-1}$ is removed, there is no remaining partial correlation. Thus, $\alpha_j = 0$ for $j > 1$.

Therefore, the PACF of a stationary AR(1) process has a cutoff after lag 1, with $\alpha_1 = \phi$ and $\alpha_j = 0$ for $j > 1$. â– 

> ðŸ’¡ **Numerical Example:**
>
> Consider an AR(1) process with $\phi = 0.6$. The PACF will have a value of 0.6 at lag 1 and 0 at all subsequent lags. This means that after removing the influence of $Y_{t-1}$, there is no additional correlation between $Y_t$ and $Y_{t-j}$ for $j > 1$.

We can also analyze the forecasting properties for an AR(1) process.

**Theorem 3** Given a stationary AR(1) process $Y_t = c + \phi Y_{t-1} + \epsilon_t$, the *k*-step-ahead forecast, conditioned on the information available at time *t*, is given by:

$$E[Y_{t+k} | Y_t, Y_{t-1}, ...] = c\sum_{i=0}^{k-1} \phi^i + \phi^k Y_t = \frac{c(1-\phi^k)}{1-\phi} + \phi^k Y_t.$$

**Proof:**
To prove the formula for the k-step-ahead forecast of a stationary AR(1) process.

I. Begin with the AR(1) process:
   $$Y_t = c + \phi Y_{t-1} + \epsilon_t$$

II. For a 1-step-ahead forecast ($k=1$), we have:
    $$E[Y_{t+1} | Y_t, Y_{t-1}, ...] = E[c + \phi Y_t + \epsilon_{t+1} | Y_t, Y_{t-1}, ...]$$

III. Since $E[\epsilon_{t+1} | Y_t, Y_{t-1}, ...] = 0$, we get:
     $$E[Y_{t+1} | Y_t, Y_{t-1}, ...] = c + \phi Y_t$$

IV. For a 2-step-ahead forecast ($k=2$), we have:
    $$E[Y_{t+2} | Y_t, Y_{t-1}, ...] = E[c + \phi Y_{t+1} + \epsilon_{t+2} | Y_t, Y_{t-1}, ...]$$

V. Using the law of iterated expectations:
   $$E[Y_{t+2} | Y_t, Y_{t-1}, ...] = c + \phi E[Y_{t+1} | Y_t, Y_{t-1}, ...] = c + \phi(c + \phi Y_t) = c(1 + \phi) + \phi^2 Y_t$$

VI. Generalizing to *k* steps ahead, we have:
    $$E[Y_{t+k} | Y_t, Y_{t-1}, ...] = E[c + \phi Y_{t+k-1} + \epsilon_{t+k} | Y_t, Y_{t-1}, ...]$$
    $$E[Y_{t+k} | Y_t, Y_{t-1}, ...] = c + \phi E[Y_{t+k-1} | Y_t, Y_{t-1}, ...]$$

VII. By recursion, we can write:
     $$E[Y_{t+k} | Y_t, Y_{t-1}, ...] = c + \phi(c + \phi E[Y_{t+k-2} | Y_t, Y_{t-1}, ...]) = c(1 + \phi) + \phi^2 E[Y_{t+k-2} | Y_t, Y_{t-1}, ...]$$
     Continuing this process, we obtain:
     $$E[Y_{t+k} | Y_t, Y_{t-1}, ...] = c(1 + \phi + \phi^2 + ... + \phi^{k-1}) + \phi^k Y_t = c\sum_{i=0}^{k-1} \phi^i + \phi^k Y_t$$

VIII. Using the formula for the sum of a geometric series, $\sum_{i=0}^{k-1} \phi^i = \frac{1 - \phi^k}{1 - \phi}$, we have:
      $$E[Y_{t+k} | Y_t, Y_{t-1}, ...] = c\frac{1 - \phi^k}{1 - \phi} + \phi^k Y_t = \frac{c(1-\phi^k)}{1-\phi} + \phi^k Y_t$$

Therefore, the k-step-ahead forecast is given by $E[Y_{t+k} | Y_t, Y_{t-1}, ...] = \frac{c(1-\phi^k)}{1-\phi} + \phi^k Y_t$. â– 

> ðŸ’¡ **Numerical Example:**
>
> Let $c=2$, $\phi = 0.8$, and $Y_t = 10$. We want to forecast $Y_{t+3}$.
>
> $E[Y_{t+3} | Y_t, Y_{t-1}, ...] = \frac{2(1-0.8^3)}{1-0.8} + 0.8^3 * 10 = \frac{2(1-0.512)}{0.2} + 0.512 * 10 = 5 * 0.488 + 5.12 = 2.44 + 5.12 = 7.56$
>
> The forecast converges towards the mean of the process as the forecast horizon increases.

**Corollary 3** In the limit as k tends to infinity, the forecast converges to the process's mean:

$$\lim_{k \to \infty} E[Y_{t+k} | Y_t, Y_{t-1}, ...] = \frac{c}{1 - \phi} = \mu$$

**Proof:**
To prove that the k-step-ahead forecast converges to the mean of the AR(1) process as $k \to \infty$.

I. From Theorem 3, we have:
   $$E[Y_{t+k} | Y_t, Y_{t-1}, ...] = \frac{c(1-\phi^k)}{1-\phi} + \phi^k Y_t$$

II. Since the process is stationary, we know that $|\phi| < 1$.

III. Therefore, as $k \to \infty$, $\phi^k \to 0$.

IV. Taking the limit as $k \to \infty$:
    $$\lim_{k \to \infty} E[Y_{t+k} | Y_t, Y_{t-1}, ...] = \lim_{k \to \infty} \left(\frac{c(1-\phi^k)}{1-\phi} + \phi^k Y_t\right)$$
    $$\lim_{k \to \infty} E[Y_{t+k} | Y_t, Y_{t-1}, ...] = \frac{c(1-0)}{1-\phi} + 0 \cdot Y_t = \frac{c}{1-\phi}$$

V. Since $\mu = \frac{c}{1-\phi}$, we have:
   $$\lim_{k \to \infty} E[Y_{t+k} | Y_t, Y_{t-1}, ...] = \mu$$

Therefore, the k-step-ahead forecast converges to the process's mean as $k \to \infty$.  â– 

It is also possible to derive the forecast error for *k* steps ahead.

**Theorem 3.1** Given a stationary AR(1) process $Y_t = c + \phi Y_{t-1} + \epsilon_t$, the *k*-step-ahead forecast error, conditioned on the information available at time *t*, is given by:

$$Y_{t+k} - E[Y_{t+k} | Y_t, Y_{t-1}, ...] = \sum_{i=0}^{k-1} \phi^i \epsilon_{t+k-i}.$$

**Proof:**
To derive the forecast error for *k* steps ahead for a stationary AR(1) process.

I. Start with the AR(1) process and iterate forward *k* steps:
   $$Y_{t+k} = c + \phi Y_{t+k-1} + \epsilon_{t+k}$$
   $$Y_{t+k} = c + \phi(c + \phi Y_{t+k-2} + \epsilon_{t+k-1}) + \epsilon_{t+k} = c(1+\phi) + \phi^2 Y_{t+k-2} + \phi\epsilon_{t+k-1} + \epsilon_{t+k}$$
   Continuing iteratively:
   $$Y_{t+k} = c\sum_{i=0}^{k-1} \phi^i + \phi^k Y_t + \sum_{i=0}^{k-1} \phi^i \epsilon_{t+k-i}$$

II. From Theorem 3, the *k*-step-ahead forecast is:
    $$E[Y_{t+k} | Y_t, Y_{t-1}, ...] = c\sum_{i=0}^{k-1} \phi^i + \phi^k Y_t$$

III. The *k*-step-ahead forecast error is the difference between the actual value and the forecast:
     $$Y_{t+k} - E[Y_{t+k} | Y_t, Y_{t-1}, ...] = \left(c\sum_{i=0}^{k-1} \phi^i + \phi^k Y_t + \sum_{i=0}^{k-1} \phi^i \epsilon_{t+k-i}\right) - \left(c\sum_{i=0}^{k-1} \phi^i + \phi^k Y_t\right)$$

IV. Simplifying, we get:
    $$Y_{t+k} - E[Y_{t+k} | Y_t, Y_{t-1}, ...] = \sum_{i=0}^{k-1} \phi^i \epsilon_{t+k-i}$$

Therefore, the *k*-step-ahead forecast error is given by $\sum_{i=0}^{k-1} \phi^i \epsilon_{t+k-i}$.  â– 

**Corollary 3.1** The variance of the *k*-step-ahead forecast error is given by:

$$Var(Y_{t+k} - E[Y_{t+k} | Y_t, Y_{t-1}, ...]) = \sigma^2 \sum_{i=0}^{k-1} \phi^{2i} = \sigma^2 \frac{1 - \phi^{2k}}{1 - \phi^2}.$$

**Proof:**
To prove the formula for the variance of the k-step-ahead forecast error.

I. From Theorem 3.1, the forecast error is:
   $$Y_{t+k} - E[Y_{t+k} | Y_t, Y_{t-1}, ...] = \sum_{i=0}^{k-1} \phi^i \epsilon_{t+k-i}$$

II. The variance of the forecast error is:
    $$Var(Y_{t+k} - E[Y_{t+k} | Y_t, Y_{t-1}, ...]) = Var\left(\sum_{i=0}^{k-1} \phi^i \epsilon_{t+k-i}\right)$$

III. Since the error terms $\epsilon_t$ are independent and identically distributed with variance $\sigma^2$, we have:
     $$Var\left(\sum_{i=0}^{k-1} \phi^i \epsilon_{t+k-i}\right) = \sum_{i=0}^{k-1} Var(\phi^i \epsilon_{t+k-i}) = \sum_{i=0}^{k-1} \phi^{2i} Var(\epsilon_{t+k-i}) = \sigma^2 \sum_{i=0}^{k-1} \phi^{2i}$$

IV. The sum $\sum_{i=0}^{k-1} \phi^{2i}$ is a geometric series with first term 1, common ratio $\phi^2$, and *k* terms. Thus:
    $$\sum_{i=0}^{k-1} \phi^{2i} = \frac{1 - (\phi^2)^k}{1 - \phi^2} = \frac{1 - \phi^{2k}}{1 - \phi^2}$$

V. Therefore, the variance of the forecast error is:
   $$Var(Y_{t+k} - E[Y_{t+k} | Y_t, Y_{t-1}, ...]) = \sigma^2 \frac{1 - \phi^{2k}}{1 - \phi^2}$$

Therefore, we have proven the given variance of forecast error for *k* steps ahead. â– 

> ðŸ’¡ **Numerical Example:**
>
> Assume $\sigma^2 = 1$ and $\phi = 0.5$. Let's calculate the variance of the 2-step-ahead forecast error:
>
> $Var(Y_{t+2} - E[Y_{t+2} | Y_t, Y_{t-1}, ...]) = 1 * \frac{1 - 0.5^{2*2}}{1 - 0.5^2} = \frac{1 - 0.0625}{0.75} = \frac{0.9375}{0.75} = 1.25$
>
> This shows that the uncertainty increases as the forecast horizon increases.

**Corollary 3.2** In the limit, as k tends to infinity, the variance of the forecast error converges to the unconditional variance of the process:

$$ \lim_{k \to \infty} Var(Y_{t+k} - E[Y_{t+k} | Y_t, Y_{t-1}, ...]) = \frac{\sigma^2}{1 - \phi^2} = \gamma_0$$

**Proof:**
To prove that the variance of the forecast error converges to the unconditional variance of the AR(1) process as $k \to \infty$.

I. From Corollary 3.1, the variance of the *k*-step-ahead forecast error is:
   $$Var(Y_{t+k} - E[Y_{t+k} | Y_t, Y_{t-1}, ...]) = \sigma^2 \frac{1 - \phi^{2k}}{1 - \phi^2}$$

II. Since the process is stationary, we know that $|\phi| < 1$.

III. Therefore, as $k \to \infty$, $\phi^{2k} \to 0$.

IV. Taking the limit as $k \to \infty$:
    $$\lim_{k \to \infty} Var(Y_{t+k} - E[Y_{t+k} | Y_t, Y_{t-1}, ...]) = \lim_{k \to \infty} \sigma^2 \frac{1 - \phi^{2k}}{1 - \phi^2}$$
    $$\lim_{k \to \infty} Var(Y_{t+k} - E[Y_{t+k} | Y_t, Y_{t-1}, ...]) = \sigma^2 \frac{1 - 0}{1 - \phi^2} = \frac{\sigma^2}{1 - \phi^2}$$

V. Recall that the unconditional variance of the AR(1) process is $\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$. Therefore:
   $$\lim_{k \to \infty} Var(Y_{t+k} - E[Y_{t+k} | Y_t, Y_{t-1}, ...]) = \gamma_0$$

Therefore, the variance of the *k*-step-ahead forecast error converges to the unconditional variance of the AR(1) process as $k \to \infty$.  â– 

### Conclusion

This chapter explored the MA($\infty$) representation of a stationary AR(1) process and derived its properties, such as the autocorrelation function. The MA($\infty$) representation provides a useful way to understand the temporal dependence of the AR(1) process and facilitates the analysis of the process's spectrum. Furthermore, we demonstrated the invertibility of the resulting MA($\infty$). The stationarity condition $|\phi| < 1$ ensures that the infinite sum converges, and the MA($\infty$) representation is well-defined.

### Autocovariance and Autocorrelation Functions

The autocovariance function of an AR(1) process provides insights into how the process varies with its past values. For an AR(1) process defined by $X_t = \phi X_{t-1} + Z_t$, where $Z_t \sim WN(0, \sigma^2)$, the autocovariance function $\gamma(h)$ is given by:

$$
\gamma(h) = Cov(X_t, X_{t-h}) = \frac{\sigma^2 \phi^{|h|}}{1 - \phi^2}
$$

The autocorrelation function (ACF), denoted as $\rho(h)$, is the autocovariance function normalized by the variance:

$$
\rho(h) = \frac{\gamma(h)}{\gamma(0)} = \phi^{|h|}
$$

This shows that the ACF decays exponentially as the lag $h$ increases, reflecting the decreasing influence of past values on the current value.

### Spectrum of an AR(1) Process

The spectrum of an AR(1) process, denoted as $S(\omega)$, describes the distribution of power across different frequencies. It is given by the Fourier transform of the autocovariance function:

$$
S(\omega) = \frac{\sigma^2}{2\pi} \frac{1}{|1 - \phi e^{-j\omega}|^2}
$$

where $\omega$ is the frequency. The shape of the spectrum depends on the value of $\phi$. For positive $\phi$, the spectrum has a peak at low frequencies, indicating that the process has a tendency to stay at its current level. For negative $\phi$, the spectrum has a peak at high frequencies, indicating that the process oscillates rapidly.

### Example: Simulating and Analyzing an AR(1) Process

Let's consider an AR(1) process with $\phi = 0.7$ and $\sigma^2 = 1$. We can simulate this process using Python:

```python
import numpy as np
import matplotlib.pyplot as plt

# Parameters
phi = 0.7
sigma = 1
n = 1000  # Number of time steps

# Generate white noise
Z = np.random.normal(0, sigma, n)

# Initialize the AR(1) process
X = np.zeros(n)
X[0] = Z[0]

# Generate the AR(1) process
for t in range(1, n):
    X[t] = phi * X[t-1] + Z[t]

# Plot the AR(1) process
plt.figure(figsize=(10, 6))
plt.plot(X)
plt.title('AR(1) Process with phi = 0.7')
plt.xlabel('Time')
plt.ylabel('X_t')
plt.show()

# Calculate and plot the ACF
def calculate_acf(x, h):
    n = len(x)
    mean = np.mean(x)
    gamma0 = np.sum((x - mean)**2) / n
    gammah = np.sum((x[:n-h] - mean) * (x[h:] - mean)) / n
    return gammah / gamma0

lags = np.arange(0, 50)
acf_values = [calculate_acf(X, h) for h in lags]

plt.figure(figsize=(10, 6))
plt.plot(lags, acf_values, marker='o')
plt.title('Autocorrelation Function (ACF)')
plt.xlabel('Lag')
plt.ylabel('ACF')
plt.show()
```

This code simulates an AR(1) process, plots the time series, and calculates and plots the ACF. The ACF should decay exponentially, as predicted by the theoretical analysis.

### Parameter Estimation

Estimating the parameter $\phi$ in an AR(1) model is a fundamental task in time series analysis. Several methods can be used, including the Yule-Walker equations, the method of moments, and maximum likelihood estimation (MLE).

#### Yule-Walker Equations

The Yule-Walker equations provide a simple and direct way to estimate $\phi$ using the sample autocovariance or autocorrelation function. For an AR(1) process, the Yule-Walker equation is:

$$
\rho(1) = \phi
$$

Thus, an estimate of $\phi$ is simply the sample autocorrelation at lag 1:

$$
\hat{\phi} = \hat{\rho}(1)
$$

#### Method of Moments

The method of moments involves equating sample moments to theoretical moments. In the case of an AR(1) process, we can use the sample variance and the sample autocovariance at lag 1 to estimate $\phi$ and $\sigma^2$.

#### Maximum Likelihood Estimation (MLE)

Maximum likelihood estimation is a more sophisticated method that involves finding the parameter values that maximize the likelihood function. For an AR(1) process, the likelihood function can be derived from the conditional distribution of $X_t$ given $X_{t-1}$. The MLE provides asymptotically efficient estimators of $\phi$ and $\sigma^2$.

### Forecasting with AR(1) Models

AR(1) models are commonly used for forecasting future values of a time series. Given the current value $X_t$ and the estimated parameter $\hat{\phi}$, the one-step-ahead forecast is:

$$
\hat{X}_{t+1} = \hat{\phi} X_t
$$

For longer forecast horizons, we can iterate this equation:

$$
\hat{X}_{t+h} = \hat{\phi}^h X_t
$$

As $h$ increases, the forecast converges to the unconditional mean of the process, which is zero for a mean-zero AR(1) process.

### Conclusion

The AR(1) process is a fundamental building block in time series analysis, offering a simple yet powerful framework for modeling and analyzing time-dependent data. Its properties, including stationarity, autocovariance, spectrum, and invertibility, provide valuable insights into the behavior of time series data. By understanding the AR(1) process, we lay the groundwork for more complex time series models and techniques. $\blacksquare$
<!-- END -->