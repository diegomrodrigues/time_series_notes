## Estacionariedade de Processos MA(q)

### IntroduÃ§Ã£o
Este capÃ­tulo explora em detalhe a propriedade fundamental de estacionariedade em processos de mÃ©dias mÃ³veis de ordem $q$ (MA(q)). Partindo da definiÃ§Ã£o de um processo MA(q) como uma combinaÃ§Ã£o linear de ruÃ­do branco atual e passado [^50], demonstraremos formalmente que tais processos sÃ£o sempre covariance-stationary, independentemente dos valores dos coeficientes $\theta_i$ [^48, 51]. Essa propriedade contrasta com os processos autorregressivos (AR), cuja estacionariedade depende restritamente dos valores de seus coeficientes. AlÃ©m disso, discutiremos as implicaÃ§Ãµes da estacionariedade para a anÃ¡lise e modelagem de sÃ©ries temporais usando modelos MA(q).

> ðŸ’¡ **Exemplo NumÃ©rico:** Considere um processo MA(1) definido por $Y_t = \varepsilon_t + 0.5\varepsilon_{t-1}$, onde $\varepsilon_t$ Ã© ruÃ­do branco com mÃ©dia 0 e variÃ¢ncia 1.  Simularemos este processo para ilustrar sua estacionariedade.

Este cÃ³digo simula um processo MA(1) e plota a sÃ©rie temporal resultante. Visualmente, podemos observar que a sÃ©rie parece oscilar em torno de uma mÃ©dia constante, sugerindo estacionariedade.

### CondiÃ§Ãµes para Covariance-Stationarity

Para um processo ser considerado covariance-stationary, trÃªs condiÃ§Ãµes devem ser satisfeitas [^48]:

1.  A mÃ©dia do processo deve ser constante ao longo do tempo.
2.  A variÃ¢ncia do processo deve ser constante ao longo do tempo.
3.  As autocovariÃ¢ncias do processo devem depender apenas da diferenÃ§a de tempo (lag) entre as observaÃ§Ãµes, e nÃ£o do tempo absoluto em que sÃ£o medidas.

### Prova Formal da Estacionariedade de Processos MA(q)

Vamos provar formalmente que um processo MA(q) satisfaz essas trÃªs condiÃ§Ãµes, garantindo sua covariance-stationarity.

**1. MÃ©dia Constante:**

Como demonstrado no capÃ­tulo anterior [^48, 50], a mÃ©dia de um processo MA(q) Ã© dada por:
$$E(Y_t) = \mu$$
onde $\mu$ Ã© uma constante [^48]. Portanto, a mÃ©dia do processo MA(q) nÃ£o depende do tempo $t$, satisfazendo a primeira condiÃ§Ã£o para covariance-stationarity.

> ðŸ’¡ **Exemplo NumÃ©rico:** Se $\mu = 5$ no nosso processo MA(1), entÃ£o $E(Y_t) = 5$ para todo $t$, confirmando a mÃ©dia constante.

**2. VariÃ¢ncia Constante:**

A variÃ¢ncia de um processo MA(q) Ã© dada por [^50]:
$$ \gamma_0 = \sigma^2 (1 + \theta_1^2 + \theta_2^2 + \ldots + \theta_q^2) $$
onde $\sigma^2$ Ã© a variÃ¢ncia do ruÃ­do branco [^47] e $\theta_i$ sÃ£o os coeficientes do processo MA(q) [^50]. Como $\sigma^2$ e $\theta_i$ sÃ£o constantes, a variÃ¢ncia $\gamma_0$ tambÃ©m Ã© constante e independente do tempo $t$. Isso satisfaz a segunda condiÃ§Ã£o para covariance-stationarity.

> ðŸ’¡ **Exemplo NumÃ©rico:** No nosso processo MA(1) $Y_t = \varepsilon_t + 0.5\varepsilon_{t-1}$, onde $\sigma^2 = 1$ e $\theta_1 = 0.5$, a variÃ¢ncia Ã©: $\gamma_0 = 1 * (1 + 0.5^2) = 1.25$. Esta variÃ¢ncia Ã© constante ao longo do tempo.

**3. AutocovariÃ¢ncias Independentes do Tempo:**

As autocovariÃ¢ncias de um processo MA(q) sÃ£o dadas por [^50, 51]:
$$ \gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] $$
Para $j \leq q$ [^51]:
$$ \gamma_j = [\theta_j + \theta_{j+1}\theta_1 + \theta_{j+2}\theta_2 + \ldots + \theta_q\theta_{q-j}]\sigma^2 $$
onde $\theta_0 = 1$ [^51].

Para $j > q$ [^51]:
$$ \gamma_j = 0 $$

Como $\sigma^2$ e $\theta_i$ sÃ£o constantes, as autocovariÃ¢ncias $\gamma_j$ dependem apenas do lag *j* e nÃ£o do tempo absoluto $t$. Isso satisfaz a terceira condiÃ§Ã£o para covariance-stationarity.

> ðŸ’¡ **Exemplo NumÃ©rico:** Para o processo MA(1) $Y_t = \varepsilon_t + 0.5\varepsilon_{t-1}$:
> - Para j = 0: $\gamma_0 = (1 + 0.5^2) * 1 = 1.25$
> - Para j = 1: $\gamma_1 = (0.5) * 1 = 0.5$
> - Para j > 1: $\gamma_j = 0$
>
> As autocovariÃ¢ncias dependem apenas do lag *j*, e nÃ£o do tempo *t*.

**ConclusÃ£o da Prova:**

Dado que a mÃ©dia, a variÃ¢ncia e as autocovariÃ¢ncias de um processo MA(q) sÃ£o constantes e nÃ£o dependem do tempo $t$, o processo MA(q) Ã© sempre covariance-stationary, independentemente dos valores dos coeficientes $\theta_i$ [^48, 51]. â– 

**Teorema da Estacionariedade Incondicional:** *Qualquer processo MA(q), definido como $Y_t = \mu + \varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \ldots + \theta_q\varepsilon_{t-q}$, onde $\{\varepsilon_t\}$ Ã© um processo de ruÃ­do branco com mÃ©dia zero e variÃ¢ncia finita $\sigma^2$, Ã© incondicionalmente estacionÃ¡rio.*

*Prova:* Este teorema encapsula a essÃªncia da estacionariedade para processos MA(q). A prova segue diretamente das propriedades do ruÃ­do branco e da estrutura linear do modelo MA(q), conforme demonstrado anteriormente.

**CorolÃ¡rio 1:** *A funÃ§Ã£o de autocorrelaÃ§Ã£o (ACF) de um processo MA(q) Ã© zero para lags maiores que q.*

*Prova:* Este resultado decorre diretamente da definiÃ§Ã£o das autocovariÃ¢ncias para um processo MA(q). Como $\gamma_j = 0$ para $j > q$, a funÃ§Ã£o de autocorrelaÃ§Ã£o, que Ã© simplesmente a autocovariÃ¢ncia normalizada pela variÃ¢ncia ($\rho_j = \gamma_j / \gamma_0$), tambÃ©m serÃ¡ zero para $j > q$.  Este corolÃ¡rio Ã© Ãºtil para identificar a ordem *q* de um processo MA(q) a partir da anÃ¡lise da ACF amostral.

> ðŸ’¡ **Exemplo NumÃ©rico:** Calculando a ACF para o MA(1) $Y_t = \varepsilon_t + 0.5\varepsilon_{t-1}$:
> - $\rho_0 = \gamma_0 / \gamma_0 = 1$
> - $\rho_1 = \gamma_1 / \gamma_0 = 0.5 / 1.25 = 0.4$
> - $\rho_j = 0$ para $j > 1$
>
> Isso demonstra que a ACF Ã© zero para lags maiores que 1, como previsto pelo corolÃ¡rio.

O grÃ¡fico da ACF confirmarÃ¡ que o valor de autocorrelaÃ§Ã£o cai para zero apÃ³s o lag 1.

**Teorema 1:** *Se $\{Y_t\}$ Ã© um processo MA(q) com representaÃ§Ã£o $Y_t = \mu + \sum_{i=0}^{q} \theta_i \varepsilon_{t-i}$, onde $\varepsilon_t$ Ã© ruÃ­do branco com mÃ©dia zero e variÃ¢ncia $\sigma^2$, entÃ£o a funÃ§Ã£o geradora de autocovariÃ¢ncia Ã© dada por $\Gamma(z) = \sigma^2 \Theta(z)\Theta(z^{-1})$, onde $\Theta(z) = \sum_{i=0}^{q} \theta_i z^i$.*

*Prova:* A funÃ§Ã£o geradora de autocovariÃ¢ncia Ã© definida como $\Gamma(z) = \sum_{j=-\infty}^{\infty} \gamma_j z^j$. Para um processo MA(q), $\gamma_j = 0$ para $|j| > q$.  Substituindo a expressÃ£o para $\gamma_j$ e utilizando as propriedades da transformada Z, a prova pode ser derivada. Este teorema fornece uma representaÃ§Ã£o Ãºtil para analisar as propriedades de autocovariÃ¢ncia no domÃ­nio da frequÃªncia.

Para melhor entendimento, apresentamos a prova detalhada:

I.  Definindo a funÃ§Ã£o geradora de autocovariÃ¢ncia:
    $$\Gamma(z) = \sum_{j=-\infty}^{\infty} \gamma_j z^j$$

II. Para um processo MA(q), sabemos que $\gamma_j = 0$ para $|j| > q$. Portanto, a soma se torna finita:
$$\Gamma(z) = \sum_{j=-q}^{q} \gamma_j z^j$$

III. Substituindo a expressÃ£o para $\gamma_j$ de um processo MA(q):
$$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E\left[\left(\sum_{i=0}^{q} \theta_i \varepsilon_{t-i}\right)\left(\sum_{k=0}^{q} \theta_k \varepsilon_{t-j-k}\right)\right]$$
Onde $\theta_0 = 1$.

IV. Usando o fato de que $E[\varepsilon_t \varepsilon_s] = \sigma^2$ se $t=s$ e 0 caso contrÃ¡rio:
$$\gamma_j = \sigma^2 \sum_{i=0}^{q} \theta_i \theta_{i-j}$$

V. Substituindo $\gamma_j$ na funÃ§Ã£o geradora de autocovariÃ¢ncia:
$$\Gamma(z) = \sum_{j=-q}^{q} \left(\sigma^2 \sum_{i=0}^{q} \theta_i \theta_{i-j}\right) z^j$$

VI. Reorganizando a soma:
$$\Gamma(z) = \sigma^2 \sum_{j=-q}^{q} \sum_{i=0}^{q} \theta_i \theta_{i-j} z^j$$

VII. Agora, considere $\Theta(z) = \sum_{i=0}^{q} \theta_i z^i$. EntÃ£o, $\Theta(z^{-1}) = \sum_{i=0}^{q} \theta_i z^{-i}$.  Multiplicando essas duas funÃ§Ãµes:
$$\Theta(z)\Theta(z^{-1}) = \left(\sum_{i=0}^{q} \theta_i z^i\right)\left(\sum_{k=0}^{q} \theta_k z^{-k}\right) = \sum_{i=0}^{q} \sum_{k=0}^{q} \theta_i \theta_k z^{i-k}$$

VIII. Fazendo a substituiÃ§Ã£o $j = k - i$, temos:
$$\Theta(z)\Theta(z^{-1}) = \sum_{i=0}^{q} \sum_{j=-i}^{q-i} \theta_i \theta_{i+j} z^{-j} = \sum_{j=-q}^{q} \left(\sum_{i=0}^{q} \theta_i \theta_{i-j}\right) z^{j}$$

IX. Comparando com a expressÃ£o para $\Gamma(z)$, obtemos:
$$\Gamma(z) = \sigma^2 \Theta(z)\Theta(z^{-1})$$

Portanto, a funÃ§Ã£o geradora de autocovariÃ¢ncia Ã© dada por $\Gamma(z) = \sigma^2 \Theta(z)\Theta(z^{-1})$. â– 

### ImplicaÃ§Ãµes da Estacionariedade

A estacionariedade dos processos MA(q) tem importantes implicaÃ§Ãµes prÃ¡ticas e teÃ³ricas na anÃ¡lise de sÃ©ries temporais:

1.  **Facilidade de AnÃ¡lise:** Processos estacionÃ¡rios sÃ£o mais fÃ¡ceis de analisar e modelar do que processos nÃ£o estacionÃ¡rios, pois suas propriedades estatÃ­sticas nÃ£o mudam com o tempo. Isso simplifica a estimaÃ§Ã£o de parÃ¢metros e a previsÃ£o.

2.  **InterpretaÃ§Ã£o Direta:** Em um processo estacionÃ¡rio, podemos interpretar as autocorrelaÃ§Ãµes como medidas diretas da dependÃªncia linear entre as observaÃ§Ãµes em diferentes lags.

3.  **Validade das InferÃªncias:** As inferÃªncias estatÃ­sticas baseadas em processos estacionÃ¡rios sÃ£o mais confiÃ¡veis, pois os testes de hipÃ³teses e os intervalos de confianÃ§a sÃ£o construÃ­dos sob a suposiÃ§Ã£o de que as propriedades estatÃ­sticas do processo sÃ£o constantes.

4.  **PrevisÃ£o:** A estacionariedade Ã© uma propriedade desejÃ¡vel para modelos de previsÃ£o, pois permite que os modelos capturem padrÃµes consistentes na sÃ©rie temporal e gerem previsÃµes razoÃ¡veis.

### ComparaÃ§Ã£o com Processos AR

Em contraste com os processos MA(q), a estacionariedade dos processos autorregressivos (AR) depende crucialmente dos valores dos coeficientes autorregressivos [^48]. Se as raÃ­zes do polinÃ´mio caracterÃ­stico do processo AR estiverem dentro do cÃ­rculo unitÃ¡rio, o processo Ã© nÃ£o estacionÃ¡rio [^48]. Essa diferenÃ§a fundamental na condiÃ§Ã£o de estacionariedade destaca a importÃ¢ncia de escolher o modelo apropriado (AR ou MA) com base nas caracterÃ­sticas da sÃ©rie temporal em anÃ¡lise.

> ðŸ’¡ **Exemplo NumÃ©rico:** Considere um processo AR(1) definido como $Y_t = \phi Y_{t-1} + \varepsilon_t$. Se $|\phi| < 1$, o processo Ã© estacionÃ¡rio. Se $\phi = 0.9$, o processo Ã© estacionÃ¡rio. No entanto, se $\phi = 1.1$, o processo Ã© nÃ£o estacionÃ¡rio e exibirÃ¡ comportamento explosivo.

Este cÃ³digo demonstra a diferenÃ§a no comportamento entre um processo AR(1) estacionÃ¡rio e um nÃ£o estacionÃ¡rio.

**ObservaÃ§Ã£o 1:**  Ã‰ importante ressaltar que embora os processos MA(q) sejam sempre estacionÃ¡rios, eles podem nÃ£o ser invertÃ­veis. A invertibilidade Ã© uma propriedade separada que se refere Ã  capacidade de expressar o processo MA(q) como um processo AR de ordem infinita (AR($\infty$)).

### ConclusÃ£o
A estacionariedade incondicional dos processos MA(q) Ã© uma propriedade fundamental que simplifica a anÃ¡lise e modelagem de sÃ©ries temporais. Ao garantir que as propriedades estatÃ­sticas do processo nÃ£o mudem com o tempo, a estacionariedade permite a aplicaÃ§Ã£o de tÃ©cnicas estatÃ­sticas padrÃ£o e a interpretaÃ§Ã£o direta das autocorrelaÃ§Ãµes. Essa propriedade contrasta com os processos AR, cuja estacionariedade depende das restriÃ§Ãµes dos coeficientes, tornando os modelos MA(q) uma ferramenta valiosa em diversas aplicaÃ§Ãµes de sÃ©ries temporais.

### ReferÃªncias
[^47]: SecÃ§Ã£o 3.2, White Noise
[^48]: SecÃ§Ã£o 3.3, Moving Average Processes
[^50]: SecÃ§Ã£o 3.3, The qth-Order Moving Average Process
[^51]: SecÃ§Ã£o 3.3, The qth-Order Moving Average Process (continuaÃ§Ã£o)
**4.1.1 OtimizaÃ§Ã£o da FunÃ§Ã£o de Perda QuadrÃ¡tica**

Para minimizar a funÃ§Ã£o de perda quadrÃ¡tica $E(Y_{t+1} - Y_{t+1|t})^2$ [^72], precisamos encontrar o valor de $Y_{t+1|t}$ que satisfaÃ§a a condiÃ§Ã£o de primeira ordem. Assumindo que podemos trocar a ordem da diferenciaÃ§Ã£o e da esperanÃ§a, derivamos em relaÃ§Ã£o a $Y_{t+1|t}$ e igualamos a zero:

$$
\frac{\partial}{\partial Y_{t+1|t}} E(Y_{t+1} - Y_{t+1|t})^2 = E \left[ \frac{\partial}{\partial Y_{t+1|t}} (Y_{t+1} - Y_{t+1|t})^2 \right] = E \left[ 2(Y_{t+1} - Y_{t+1|t})(-1) \right] = 0
$$

Isto implica que:

$$
E[Y_{t+1} - Y_{t+1|t}] = 0
$$

$$
E[Y_{t+1}] = E[Y_{t+1|t}]
$$

Portanto, o melhor forecast $Y_{t+1|t}$ sob perda quadrÃ¡tica Ã© a expectativa condicional de $Y_{t+1}$ dado o conjunto de informaÃ§Ãµes $X_t$:

$$
Y_{t+1|t} = E[Y_{t+1} | X_t]
$$

**4.1.2 A ProjeÃ§Ã£o Linear Ã“tima**

Embora a expectativa condicional seja o forecast Ã³timo, calculÃ¡-la pode ser impraticÃ¡vel. Em vez disso, podemos restringir a atenÃ§Ã£o a forecasts que sÃ£o funÃ§Ãµes lineares de $X_t$. Considere o forecast linear:

$$
Y_{t+1|t} = a + b'X_t
$$

onde $a$ Ã© um escalar e $b$ Ã© um vetor de coeficientes. O problema agora Ã© escolher $a$ e $b$ para minimizar a funÃ§Ã£o de perda quadrÃ¡tica $E(Y_{t+1} - a - b'X_t)^2$.

As condiÃ§Ãµes de primeira ordem para este problema sÃ£o:

$$
\frac{\partial}{\partial a} E(Y_{t+1} - a - b'X_t)^2 = -2E(Y_{t+1} - a - b'X_t) = 0
$$

$$
\frac{\partial}{\partial b} E(Y_{t+1} - a - b'X_t)^2 = -2E[X_t(Y_{t+1} - a - b'X_t)] = 0
$$

Da primeira condiÃ§Ã£o de primeira ordem, obtemos:

$$
a = E[Y_{t+1}] - b'E[X_t]
$$

Substituindo este resultado na segunda condiÃ§Ã£o de primeira ordem, obtemos:

$$
E[X_t(Y_{t+1} - E[Y_{t+1}] - b'(X_t - E[X_t]))] = 0
$$

$$
E[X_t(Y_{t+1} - E[Y_{t+1}])] = E[X_t b'(X_t - E[X_t])]
$$

Definimos a matriz de covariÃ¢ncia de $X_t$ como $\Sigma_{XX} = E[(X_t - E[X_t])(X_t - E[X_t])']$ e o vetor de covariÃ¢ncia entre $X_t$ e $Y_{t+1}$ como $\Sigma_{XY} = E[X_t(Y_{t+1} - E[Y_{t+1}])]$. EntÃ£o,

$$
\Sigma_{XY} = E[X_t b'(X_t - E[X_t])] = \Sigma_{XX}b
$$

Se $\Sigma_{XX}$ Ã© nÃ£o singular, entÃ£o

$$
b = \Sigma_{XX}^{-1}\Sigma_{XY}
$$

Portanto, o forecast linear Ã³timo Ã©:

$$
Y_{t+1|t} = E[Y_{t+1}] + \Sigma_{XY}' \Sigma_{XX}^{-1} (X_t - E[X_t])
$$

Este forecast Ã© a projeÃ§Ã£o linear de $Y_{t+1}$ no espaÃ§o gerado por $X_t$. Ã‰ importante notar que se $Y_{t+1}$ e $X_t$ sÃ£o conjuntamente Gaussianos, entÃ£o a projeÃ§Ã£o linear Ã³tima coincide com a expectativa condicional Ã³tima.

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que tenhamos os seguintes dados: $E[Y_{t+1}] = 10$, $E[X_t] = [2, 3]$, $\Sigma_{XX} = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$, e $\Sigma_{XY} = \begin{bmatrix} 0.8 \\ 0.6 \end{bmatrix}$.
>
> Primeiro, calculamos $\Sigma_{XX}^{-1}$:
> $\Sigma_{XX}^{-1} = \frac{1}{(1*1 - 0.5*0.5)} \begin{bmatrix} 1 & -0.5 \\ -0.5 & 1 \end{bmatrix} = \frac{1}{0.75} \begin{bmatrix} 1 & -0.5 \\ -0.5 & 1 \end{bmatrix} = \begin{bmatrix} 1.333 & -0.667 \\ -0.667 & 1.333 \end{bmatrix}$
>
> Em seguida, calculamos $b$:
> $b = \Sigma_{XX}^{-1}\Sigma_{XY} = \begin{bmatrix} 1.333 & -0.667 \\ -0.667 & 1.333 \end{bmatrix} \begin{bmatrix} 0.8 \\ 0.6 \end{bmatrix} = \begin{bmatrix} 1.333*0.8 - 0.667*0.6 \\ -0.667*0.8 + 1.333*0.6 \end{bmatrix} = \begin{bmatrix} 0.666 \\ 0.266 \end{bmatrix}$
>
> EntÃ£o, calculamos $a$:
> $a = E[Y_{t+1}] - b'E[X_t] = 10 - \begin{bmatrix} 0.666 & 0.266 \end{bmatrix} \begin{bmatrix} 2 \\ 3 \end{bmatrix} = 10 - (0.666*2 + 0.266*3) = 10 - (1.332 + 0.798) = 10 - 2.13 = 7.87$
>
> Portanto, o forecast linear Ã³timo Ã©: $Y_{t+1|t} = 7.87 + 0.666X_{1t} + 0.266X_{2t}$, onde $X_{1t}$ e $X_{2t}$ sÃ£o os valores dos dois preditores em $X_t$.

Para melhor clareza, aqui estÃ¡ uma prova passo a passo de como $b = \Sigma_{XX}^{-1}\Sigma_{XY}$:

I. ComeÃ§amos com a equaÃ§Ã£o:
   $$\Sigma_{XY} = \Sigma_{XX}b$$

II. Queremos isolar $b$, entÃ£o multiplicamos ambos os lados da equaÃ§Ã£o pela inversa de $\Sigma_{XX}$, assumindo que $\Sigma_{XX}$ Ã© invertÃ­vel:
   $$\Sigma_{XX}^{-1}\Sigma_{XY} = \Sigma_{XX}^{-1}\Sigma_{XX}b$$

III. Por definiÃ§Ã£o, a inversa de uma matriz multiplicada pela prÃ³pria matriz resulta na matriz identidade $I$:
   $$\Sigma_{XX}^{-1}\Sigma_{XY} = Ib$$

IV. A matriz identidade multiplicada por qualquer vetor resulta no prÃ³prio vetor:
   $$\Sigma_{XX}^{-1}\Sigma_{XY} = b$$

V. Portanto, temos:
   $$b = \Sigma_{XX}^{-1}\Sigma_{XY}$$
â– 

**4.2 Forecasts para Modelos ARMA com InformaÃ§Ã£o Infinita**

Agora vamos considerar o caso em que temos um nÃºmero infinito de observaÃ§Ãµes passadas disponÃ­veis. Isso simplifica a anÃ¡lise e fornece insights Ãºteis para o caso finito.

Suponha que $Y_t$ siga um processo ARMA(p, q) [^16]:

$$
Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \ldots + \theta_q \varepsilon_{t-q}
$$

Nosso objetivo Ã© encontrar $E[Y_{t+1} | Y_t, Y_{t-1}, Y_{t-2}, \ldots]$. Tomando expectativas condicionais de ambos os lados da equaÃ§Ã£o acima, obtemos:

$$
E[Y_{t+1} | Y_t, Y_{t-1}, Y_{t-2}, \ldots] = c + \phi_1 Y_t + \phi_2 Y_{t-1} + \ldots + \phi_p Y_{t-p+1} + E[\varepsilon_{t+1} | Y_t, Y_{t-1}, Y_{t-2}, \ldots] + \theta_1 \varepsilon_t + \theta_2 \varepsilon_{t-1} + \ldots + \theta_q \varepsilon_{t-q+1}
$$

Dado que $\varepsilon_{t+1}$ Ã© independente de $Y_t, Y_{t-1}, Y_{t-2}, \ldots$, temos $E[\varepsilon_{t+1} | Y_t, Y_{t-1}, Y_{t-2}, \ldots] = E[\varepsilon_{t+1}] = 0$. AlÃ©m disso, podemos escrever:

$$
\varepsilon_t = Y_t - c - \phi_1 Y_{t-1} - \phi_2 Y_{t-2} - \ldots - \phi_p Y_{t-p} - \theta_1 \varepsilon_{t-1} - \theta_2 \varepsilon_{t-2} - \ldots - \theta_q \varepsilon_{t-q}
$$

Usando essa expressÃ£o para os erros passados, podemos calcular o forecast Ã³timo.

> ðŸ’¡ **Exemplo NumÃ©rico:** Considere um modelo ARMA(1,1): $Y_t = 2 + 0.5Y_{t-1} + \varepsilon_t + 0.3\varepsilon_{t-1}$. Suponha que $Y_t = 5$, $Y_{t-1} = 4$, e $\varepsilon_{t-1} = 1$.
>
> Primeiro, calculamos $\varepsilon_t$:
> $\varepsilon_t = Y_t - 2 - 0.5Y_{t-1} - 0.3\varepsilon_{t-1} = 5 - 2 - 0.5*4 - 0.3*1 = 5 - 2 - 2 - 0.3 = 0.7$
>
> Em seguida, calculamos $E[Y_{t+1} | Y_t, Y_{t-1}, \ldots]$:
> $E[Y_{t+1} | Y_t, Y_{t-1}, \ldots] = 2 + 0.5Y_t + 0.3\varepsilon_t = 2 + 0.5*5 + 0.3*0.7 = 2 + 2.5 + 0.21 = 4.71$
>
> Portanto, o forecast para $Y_{t+1}$ Ã© 4.71.

**ConclusÃ£o**

Este capÃ­tulo introduziu os princÃ­pios de previsÃ£o, focando em projeÃ§Ãµes lineares e modelos ARMA [^16]. O forecast Ã³timo sob perda quadrÃ¡tica Ã© a expectativa condicional, e a projeÃ§Ã£o linear Ã³tima fornece uma aproximaÃ§Ã£o prÃ¡tica. Para modelos ARMA com informaÃ§Ã£o infinita, o forecast pode ser calculado usando as equaÃ§Ãµes do modelo e os erros passados.

**ReferÃªncias**
[^72]: SeÃ§Ã£o 4.1, PrincÃ­pios de Forecasting
<!-- END -->