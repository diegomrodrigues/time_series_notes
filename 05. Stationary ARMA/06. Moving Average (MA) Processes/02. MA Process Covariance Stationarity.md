## Estacionariedade de Processos MA(q)

### Introdu√ß√£o
Este cap√≠tulo explora em detalhe a propriedade fundamental de estacionariedade em processos de m√©dias m√≥veis de ordem $q$ (MA(q)). Partindo da defini√ß√£o de um processo MA(q) como uma combina√ß√£o linear de ru√≠do branco atual e passado [^50], demonstraremos formalmente que tais processos s√£o sempre covariance-stationary, independentemente dos valores dos coeficientes $\theta_i$ [^48, 51]. Essa propriedade contrasta com os processos autorregressivos (AR), cuja estacionariedade depende restritamente dos valores de seus coeficientes. Al√©m disso, discutiremos as implica√ß√µes da estacionariedade para a an√°lise e modelagem de s√©ries temporais usando modelos MA(q).

> üí° **Exemplo Num√©rico:** Considere um processo MA(1) definido por $Y_t = \varepsilon_t + 0.5\varepsilon_{t-1}$, onde $\varepsilon_t$ √© ru√≠do branco com m√©dia 0 e vari√¢ncia 1.  Simularemos este processo para ilustrar sua estacionariedade.

Este c√≥digo simula um processo MA(1) e plota a s√©rie temporal resultante. Visualmente, podemos observar que a s√©rie parece oscilar em torno de uma m√©dia constante, sugerindo estacionariedade.

### Condi√ß√µes para Covariance-Stationarity

Para um processo ser considerado covariance-stationary, tr√™s condi√ß√µes devem ser satisfeitas [^48]:

1.  A m√©dia do processo deve ser constante ao longo do tempo.
2.  A vari√¢ncia do processo deve ser constante ao longo do tempo.
3.  As autocovari√¢ncias do processo devem depender apenas da diferen√ßa de tempo (lag) entre as observa√ß√µes, e n√£o do tempo absoluto em que s√£o medidas.

### Prova Formal da Estacionariedade de Processos MA(q)

Vamos provar formalmente que um processo MA(q) satisfaz essas tr√™s condi√ß√µes, garantindo sua covariance-stationarity.

**1. M√©dia Constante:**

Como demonstrado no cap√≠tulo anterior [^48, 50], a m√©dia de um processo MA(q) √© dada por:
$$E(Y_t) = \mu$$
onde $\mu$ √© uma constante [^48]. Portanto, a m√©dia do processo MA(q) n√£o depende do tempo $t$, satisfazendo a primeira condi√ß√£o para covariance-stationarity.

> üí° **Exemplo Num√©rico:** Se $\mu = 5$ no nosso processo MA(1), ent√£o $E(Y_t) = 5$ para todo $t$, confirmando a m√©dia constante.

**2. Vari√¢ncia Constante:**

A vari√¢ncia de um processo MA(q) √© dada por [^50]:
$$ \gamma_0 = \sigma^2 (1 + \theta_1^2 + \theta_2^2 + \ldots + \theta_q^2) $$
onde $\sigma^2$ √© a vari√¢ncia do ru√≠do branco [^47] e $\theta_i$ s√£o os coeficientes do processo MA(q) [^50]. Como $\sigma^2$ e $\theta_i$ s√£o constantes, a vari√¢ncia $\gamma_0$ tamb√©m √© constante e independente do tempo $t$. Isso satisfaz a segunda condi√ß√£o para covariance-stationarity.

> üí° **Exemplo Num√©rico:** No nosso processo MA(1) $Y_t = \varepsilon_t + 0.5\varepsilon_{t-1}$, onde $\sigma^2 = 1$ e $\theta_1 = 0.5$, a vari√¢ncia √©: $\gamma_0 = 1 * (1 + 0.5^2) = 1.25$. Esta vari√¢ncia √© constante ao longo do tempo.

**3. Autocovari√¢ncias Independentes do Tempo:**

As autocovari√¢ncias de um processo MA(q) s√£o dadas por [^50, 51]:
$$ \gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] $$
Para $j \leq q$ [^51]:
$$ \gamma_j = [\theta_j + \theta_{j+1}\theta_1 + \theta_{j+2}\theta_2 + \ldots + \theta_q\theta_{q-j}]\sigma^2 $$
onde $\theta_0 = 1$ [^51].

Para $j > q$ [^51]:
$$ \gamma_j = 0 $$

Como $\sigma^2$ e $\theta_i$ s√£o constantes, as autocovari√¢ncias $\gamma_j$ dependem apenas do lag *j* e n√£o do tempo absoluto $t$. Isso satisfaz a terceira condi√ß√£o para covariance-stationarity.

> üí° **Exemplo Num√©rico:** Para o processo MA(1) $Y_t = \varepsilon_t + 0.5\varepsilon_{t-1}$:
> - Para j = 0: $\gamma_0 = (1 + 0.5^2) * 1 = 1.25$
> - Para j = 1: $\gamma_1 = (0.5) * 1 = 0.5$
> - Para j > 1: $\gamma_j = 0$
>
> As autocovari√¢ncias dependem apenas do lag *j*, e n√£o do tempo *t*.

**Conclus√£o da Prova:**

Dado que a m√©dia, a vari√¢ncia e as autocovari√¢ncias de um processo MA(q) s√£o constantes e n√£o dependem do tempo $t$, o processo MA(q) √© sempre covariance-stationary, independentemente dos valores dos coeficientes $\theta_i$ [^48, 51]. ‚ñ†

**Teorema da Estacionariedade Incondicional:** *Qualquer processo MA(q), definido como $Y_t = \mu + \varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \ldots + \theta_q\varepsilon_{t-q}$, onde $\{\varepsilon_t\}$ √© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia finita $\sigma^2$, √© incondicionalmente estacion√°rio.*

*Prova:* Este teorema encapsula a ess√™ncia da estacionariedade para processos MA(q). A prova segue diretamente das propriedades do ru√≠do branco e da estrutura linear do modelo MA(q), conforme demonstrado anteriormente.

**Corol√°rio 1:** *A fun√ß√£o de autocorrela√ß√£o (ACF) de um processo MA(q) √© zero para lags maiores que q.*

*Prova:* Este resultado decorre diretamente da defini√ß√£o das autocovari√¢ncias para um processo MA(q). Como $\gamma_j = 0$ para $j > q$, a fun√ß√£o de autocorrela√ß√£o, que √© simplesmente a autocovari√¢ncia normalizada pela vari√¢ncia ($\rho_j = \gamma_j / \gamma_0$), tamb√©m ser√° zero para $j > q$.  Este corol√°rio √© √∫til para identificar a ordem *q* de um processo MA(q) a partir da an√°lise da ACF amostral.

> üí° **Exemplo Num√©rico:** Calculando a ACF para o MA(1) $Y_t = \varepsilon_t + 0.5\varepsilon_{t-1}$:
> - $\rho_0 = \gamma_0 / \gamma_0 = 1$
> - $\rho_1 = \gamma_1 / \gamma_0 = 0.5 / 1.25 = 0.4$
> - $\rho_j = 0$ para $j > 1$
>
> Isso demonstra que a ACF √© zero para lags maiores que 1, como previsto pelo corol√°rio.

O gr√°fico da ACF confirmar√° que o valor de autocorrela√ß√£o cai para zero ap√≥s o lag 1.

**Teorema 1:** *Se $\{Y_t\}$ √© um processo MA(q) com representa√ß√£o $Y_t = \mu + \sum_{i=0}^{q} \theta_i \varepsilon_{t-i}$, onde $\varepsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, ent√£o a fun√ß√£o geradora de autocovari√¢ncia √© dada por $\Gamma(z) = \sigma^2 \Theta(z)\Theta(z^{-1})$, onde $\Theta(z) = \sum_{i=0}^{q} \theta_i z^i$.*

*Prova:* A fun√ß√£o geradora de autocovari√¢ncia √© definida como $\Gamma(z) = \sum_{j=-\infty}^{\infty} \gamma_j z^j$. Para um processo MA(q), $\gamma_j = 0$ para $|j| > q$.  Substituindo a express√£o para $\gamma_j$ e utilizando as propriedades da transformada Z, a prova pode ser derivada. Este teorema fornece uma representa√ß√£o √∫til para analisar as propriedades de autocovari√¢ncia no dom√≠nio da frequ√™ncia.

Para melhor entendimento, apresentamos a prova detalhada:

I.  Definindo a fun√ß√£o geradora de autocovari√¢ncia:
    $$\Gamma(z) = \sum_{j=-\infty}^{\infty} \gamma_j z^j$$

II. Para um processo MA(q), sabemos que $\gamma_j = 0$ para $|j| > q$. Portanto, a soma se torna finita:
$$\Gamma(z) = \sum_{j=-q}^{q} \gamma_j z^j$$

III. Substituindo a express√£o para $\gamma_j$ de um processo MA(q):
$$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E\left[\left(\sum_{i=0}^{q} \theta_i \varepsilon_{t-i}\right)\left(\sum_{k=0}^{q} \theta_k \varepsilon_{t-j-k}\right)\right]$$
Onde $\theta_0 = 1$.

IV. Usando o fato de que $E[\varepsilon_t \varepsilon_s] = \sigma^2$ se $t=s$ e 0 caso contr√°rio:
$$\gamma_j = \sigma^2 \sum_{i=0}^{q} \theta_i \theta_{i-j}$$

V. Substituindo $\gamma_j$ na fun√ß√£o geradora de autocovari√¢ncia:
$$\Gamma(z) = \sum_{j=-q}^{q} \left(\sigma^2 \sum_{i=0}^{q} \theta_i \theta_{i-j}\right) z^j$$

VI. Reorganizando a soma:
$$\Gamma(z) = \sigma^2 \sum_{j=-q}^{q} \sum_{i=0}^{q} \theta_i \theta_{i-j} z^j$$

VII. Agora, considere $\Theta(z) = \sum_{i=0}^{q} \theta_i z^i$. Ent√£o, $\Theta(z^{-1}) = \sum_{i=0}^{q} \theta_i z^{-i}$.  Multiplicando essas duas fun√ß√µes:
$$\Theta(z)\Theta(z^{-1}) = \left(\sum_{i=0}^{q} \theta_i z^i\right)\left(\sum_{k=0}^{q} \theta_k z^{-k}\right) = \sum_{i=0}^{q} \sum_{k=0}^{q} \theta_i \theta_k z^{i-k}$$

VIII. Fazendo a substitui√ß√£o $j = k - i$, temos:
$$\Theta(z)\Theta(z^{-1}) = \sum_{i=0}^{q} \sum_{j=-i}^{q-i} \theta_i \theta_{i+j} z^{-j} = \sum_{j=-q}^{q} \left(\sum_{i=0}^{q} \theta_i \theta_{i-j}\right) z^{j}$$

IX. Comparando com a express√£o para $\Gamma(z)$, obtemos:
$$\Gamma(z) = \sigma^2 \Theta(z)\Theta(z^{-1})$$

Portanto, a fun√ß√£o geradora de autocovari√¢ncia √© dada por $\Gamma(z) = \sigma^2 \Theta(z)\Theta(z^{-1})$. ‚ñ†

### Implica√ß√µes da Estacionariedade

A estacionariedade dos processos MA(q) tem importantes implica√ß√µes pr√°ticas e te√≥ricas na an√°lise de s√©ries temporais:

1.  **Facilidade de An√°lise:** Processos estacion√°rios s√£o mais f√°ceis de analisar e modelar do que processos n√£o estacion√°rios, pois suas propriedades estat√≠sticas n√£o mudam com o tempo. Isso simplifica a estima√ß√£o de par√¢metros e a previs√£o.

2.  **Interpreta√ß√£o Direta:** Em um processo estacion√°rio, podemos interpretar as autocorrela√ß√µes como medidas diretas da depend√™ncia linear entre as observa√ß√µes em diferentes lags.

3.  **Validade das Infer√™ncias:** As infer√™ncias estat√≠sticas baseadas em processos estacion√°rios s√£o mais confi√°veis, pois os testes de hip√≥teses e os intervalos de confian√ßa s√£o constru√≠dos sob a suposi√ß√£o de que as propriedades estat√≠sticas do processo s√£o constantes.

4.  **Previs√£o:** A estacionariedade √© uma propriedade desej√°vel para modelos de previs√£o, pois permite que os modelos capturem padr√µes consistentes na s√©rie temporal e gerem previs√µes razo√°veis.

### Compara√ß√£o com Processos AR

Em contraste com os processos MA(q), a estacionariedade dos processos autorregressivos (AR) depende crucialmente dos valores dos coeficientes autorregressivos [^48]. Se as ra√≠zes do polin√¥mio caracter√≠stico do processo AR estiverem dentro do c√≠rculo unit√°rio, o processo √© n√£o estacion√°rio [^48]. Essa diferen√ßa fundamental na condi√ß√£o de estacionariedade destaca a import√¢ncia de escolher o modelo apropriado (AR ou MA) com base nas caracter√≠sticas da s√©rie temporal em an√°lise.

> üí° **Exemplo Num√©rico:** Considere um processo AR(1) definido como $Y_t = \phi Y_{t-1} + \varepsilon_t$. Se $|\phi| < 1$, o processo √© estacion√°rio. Se $\phi = 0.9$, o processo √© estacion√°rio. No entanto, se $\phi = 1.1$, o processo √© n√£o estacion√°rio e exibir√° comportamento explosivo.

Este c√≥digo demonstra a diferen√ßa no comportamento entre um processo AR(1) estacion√°rio e um n√£o estacion√°rio.

**Observa√ß√£o 1:**  √â importante ressaltar que embora os processos MA(q) sejam sempre estacion√°rios, eles podem n√£o ser invert√≠veis. A invertibilidade √© uma propriedade separada que se refere √† capacidade de expressar o processo MA(q) como um processo AR de ordem infinita (AR($\infty$)).

### Conclus√£o
A estacionariedade incondicional dos processos MA(q) √© uma propriedade fundamental que simplifica a an√°lise e modelagem de s√©ries temporais. Ao garantir que as propriedades estat√≠sticas do processo n√£o mudem com o tempo, a estacionariedade permite a aplica√ß√£o de t√©cnicas estat√≠sticas padr√£o e a interpreta√ß√£o direta das autocorrela√ß√µes. Essa propriedade contrasta com os processos AR, cuja estacionariedade depende das restri√ß√µes dos coeficientes, tornando os modelos MA(q) uma ferramenta valiosa em diversas aplica√ß√µes de s√©ries temporais.

### Refer√™ncias
[^47]: Sec√ß√£o 3.2, White Noise
[^48]: Sec√ß√£o 3.3, Moving Average Processes
[^50]: Sec√ß√£o 3.3, The qth-Order Moving Average Process
[^51]: Sec√ß√£o 3.3, The qth-Order Moving Average Process (continua√ß√£o)
**4.1.1 Otimiza√ß√£o da Fun√ß√£o de Perda Quadr√°tica**

Para minimizar a fun√ß√£o de perda quadr√°tica $E(Y_{t+1} - Y_{t+1|t})^2$ [^72], precisamos encontrar o valor de $Y_{t+1|t}$ que satisfa√ßa a condi√ß√£o de primeira ordem. Assumindo que podemos trocar a ordem da diferencia√ß√£o e da esperan√ßa, derivamos em rela√ß√£o a $Y_{t+1|t}$ e igualamos a zero:

$$
\frac{\partial}{\partial Y_{t+1|t}} E(Y_{t+1} - Y_{t+1|t})^2 = E \left[ \frac{\partial}{\partial Y_{t+1|t}} (Y_{t+1} - Y_{t+1|t})^2 \right] = E \left[ 2(Y_{t+1} - Y_{t+1|t})(-1) \right] = 0
$$

Isto implica que:

$$
E[Y_{t+1} - Y_{t+1|t}] = 0
$$

$$
E[Y_{t+1}] = E[Y_{t+1|t}]
$$

Portanto, o melhor forecast $Y_{t+1|t}$ sob perda quadr√°tica √© a expectativa condicional de $Y_{t+1}$ dado o conjunto de informa√ß√µes $X_t$:

$$
Y_{t+1|t} = E[Y_{t+1} | X_t]
$$

**4.1.2 A Proje√ß√£o Linear √ìtima**

Embora a expectativa condicional seja o forecast √≥timo, calcul√°-la pode ser impratic√°vel. Em vez disso, podemos restringir a aten√ß√£o a forecasts que s√£o fun√ß√µes lineares de $X_t$. Considere o forecast linear:

$$
Y_{t+1|t} = a + b'X_t
$$

onde $a$ √© um escalar e $b$ √© um vetor de coeficientes. O problema agora √© escolher $a$ e $b$ para minimizar a fun√ß√£o de perda quadr√°tica $E(Y_{t+1} - a - b'X_t)^2$.

As condi√ß√µes de primeira ordem para este problema s√£o:

$$
\frac{\partial}{\partial a} E(Y_{t+1} - a - b'X_t)^2 = -2E(Y_{t+1} - a - b'X_t) = 0
$$

$$
\frac{\partial}{\partial b} E(Y_{t+1} - a - b'X_t)^2 = -2E[X_t(Y_{t+1} - a - b'X_t)] = 0
$$

Da primeira condi√ß√£o de primeira ordem, obtemos:

$$
a = E[Y_{t+1}] - b'E[X_t]
$$

Substituindo este resultado na segunda condi√ß√£o de primeira ordem, obtemos:

$$
E[X_t(Y_{t+1} - E[Y_{t+1}] - b'(X_t - E[X_t]))] = 0
$$

$$
E[X_t(Y_{t+1} - E[Y_{t+1}])] = E[X_t b'(X_t - E[X_t])]
$$

Definimos a matriz de covari√¢ncia de $X_t$ como $\Sigma_{XX} = E[(X_t - E[X_t])(X_t - E[X_t])']$ e o vetor de covari√¢ncia entre $X_t$ e $Y_{t+1}$ como $\Sigma_{XY} = E[X_t(Y_{t+1} - E[Y_{t+1}])]$. Ent√£o,

$$
\Sigma_{XY} = E[X_t b'(X_t - E[X_t])] = \Sigma_{XX}b
$$

Se $\Sigma_{XX}$ √© n√£o singular, ent√£o

$$
b = \Sigma_{XX}^{-1}\Sigma_{XY}
$$

Portanto, o forecast linear √≥timo √©:

$$
Y_{t+1|t} = E[Y_{t+1}] + \Sigma_{XY}' \Sigma_{XX}^{-1} (X_t - E[X_t])
$$

Este forecast √© a proje√ß√£o linear de $Y_{t+1}$ no espa√ßo gerado por $X_t$. √â importante notar que se $Y_{t+1}$ e $X_t$ s√£o conjuntamente Gaussianos, ent√£o a proje√ß√£o linear √≥tima coincide com a expectativa condicional √≥tima.

> üí° **Exemplo Num√©rico:** Suponha que tenhamos os seguintes dados: $E[Y_{t+1}] = 10$, $E[X_t] = [2, 3]$, $\Sigma_{XX} = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$, e $\Sigma_{XY} = \begin{bmatrix} 0.8 \\ 0.6 \end{bmatrix}$.
>
> Primeiro, calculamos $\Sigma_{XX}^{-1}$:
> $\Sigma_{XX}^{-1} = \frac{1}{(1*1 - 0.5*0.5)} \begin{bmatrix} 1 & -0.5 \\ -0.5 & 1 \end{bmatrix} = \frac{1}{0.75} \begin{bmatrix} 1 & -0.5 \\ -0.5 & 1 \end{bmatrix} = \begin{bmatrix} 1.333 & -0.667 \\ -0.667 & 1.333 \end{bmatrix}$
>
> Em seguida, calculamos $b$:
> $b = \Sigma_{XX}^{-1}\Sigma_{XY} = \begin{bmatrix} 1.333 & -0.667 \\ -0.667 & 1.333 \end{bmatrix} \begin{bmatrix} 0.8 \\ 0.6 \end{bmatrix} = \begin{bmatrix} 1.333*0.8 - 0.667*0.6 \\ -0.667*0.8 + 1.333*0.6 \end{bmatrix} = \begin{bmatrix} 0.666 \\ 0.266 \end{bmatrix}$
>
> Ent√£o, calculamos $a$:
> $a = E[Y_{t+1}] - b'E[X_t] = 10 - \begin{bmatrix} 0.666 & 0.266 \end{bmatrix} \begin{bmatrix} 2 \\ 3 \end{bmatrix} = 10 - (0.666*2 + 0.266*3) = 10 - (1.332 + 0.798) = 10 - 2.13 = 7.87$
>
> Portanto, o forecast linear √≥timo √©: $Y_{t+1|t} = 7.87 + 0.666X_{1t} + 0.266X_{2t}$, onde $X_{1t}$ e $X_{2t}$ s√£o os valores dos dois preditores em $X_t$.

Para melhor clareza, aqui est√° uma prova passo a passo de como $b = \Sigma_{XX}^{-1}\Sigma_{XY}$:

I. Come√ßamos com a equa√ß√£o:
   $$\Sigma_{XY} = \Sigma_{XX}b$$

II. Queremos isolar $b$, ent√£o multiplicamos ambos os lados da equa√ß√£o pela inversa de $\Sigma_{XX}$, assumindo que $\Sigma_{XX}$ √© invert√≠vel:
   $$\Sigma_{XX}^{-1}\Sigma_{XY} = \Sigma_{XX}^{-1}\Sigma_{XX}b$$

III. Por defini√ß√£o, a inversa de uma matriz multiplicada pela pr√≥pria matriz resulta na matriz identidade $I$:
   $$\Sigma_{XX}^{-1}\Sigma_{XY} = Ib$$

IV. A matriz identidade multiplicada por qualquer vetor resulta no pr√≥prio vetor:
   $$\Sigma_{XX}^{-1}\Sigma_{XY} = b$$

V. Portanto, temos:
   $$b = \Sigma_{XX}^{-1}\Sigma_{XY}$$
‚ñ†

**4.2 Forecasts para Modelos ARMA com Informa√ß√£o Infinita**

Agora vamos considerar o caso em que temos um n√∫mero infinito de observa√ß√µes passadas dispon√≠veis. Isso simplifica a an√°lise e fornece insights √∫teis para o caso finito.

Suponha que $Y_t$ siga um processo ARMA(p, q) [^16]:

$$
Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \ldots + \theta_q \varepsilon_{t-q}
$$

Nosso objetivo √© encontrar $E[Y_{t+1} | Y_t, Y_{t-1}, Y_{t-2}, \ldots]$. Tomando expectativas condicionais de ambos os lados da equa√ß√£o acima, obtemos:

$$
E[Y_{t+1} | Y_t, Y_{t-1}, Y_{t-2}, \ldots] = c + \phi_1 Y_t + \phi_2 Y_{t-1} + \ldots + \phi_p Y_{t-p+1} + E[\varepsilon_{t+1} | Y_t, Y_{t-1}, Y_{t-2}, \ldots] + \theta_1 \varepsilon_t + \theta_2 \varepsilon_{t-1} + \ldots + \theta_q \varepsilon_{t-q+1}
$$

Dado que $\varepsilon_{t+1}$ √© independente de $Y_t, Y_{t-1}, Y_{t-2}, \ldots$, temos $E[\varepsilon_{t+1} | Y_t, Y_{t-1}, Y_{t-2}, \ldots] = E[\varepsilon_{t+1}] = 0$. Al√©m disso, podemos escrever:

$$
\varepsilon_t = Y_t - c - \phi_1 Y_{t-1} - \phi_2 Y_{t-2} - \ldots - \phi_p Y_{t-p} - \theta_1 \varepsilon_{t-1} - \theta_2 \varepsilon_{t-2} - \ldots - \theta_q \varepsilon_{t-q}
$$

Usando essa express√£o para os erros passados, podemos calcular o forecast √≥timo.

> üí° **Exemplo Num√©rico:** Considere um modelo ARMA(1,1): $Y_t = 2 + 0.5Y_{t-1} + \varepsilon_t + 0.3\varepsilon_{t-1}$. Suponha que $Y_t = 5$, $Y_{t-1} = 4$, e $\varepsilon_{t-1} = 1$.
>
> Primeiro, calculamos $\varepsilon_t$:
> $\varepsilon_t = Y_t - 2 - 0.5Y_{t-1} - 0.3\varepsilon_{t-1} = 5 - 2 - 0.5*4 - 0.3*1 = 5 - 2 - 2 - 0.3 = 0.7$
>
> Em seguida, calculamos $E[Y_{t+1} | Y_t, Y_{t-1}, \ldots]$:
> $E[Y_{t+1} | Y_t, Y_{t-1}, \ldots] = 2 + 0.5Y_t + 0.3\varepsilon_t = 2 + 0.5*5 + 0.3*0.7 = 2 + 2.5 + 0.21 = 4.71$
>
> Portanto, o forecast para $Y_{t+1}$ √© 4.71.

**Conclus√£o**

Este cap√≠tulo introduziu os princ√≠pios de previs√£o, focando em proje√ß√µes lineares e modelos ARMA [^16]. O forecast √≥timo sob perda quadr√°tica √© a expectativa condicional, e a proje√ß√£o linear √≥tima fornece uma aproxima√ß√£o pr√°tica. Para modelos ARMA com informa√ß√£o infinita, o forecast pode ser calculado usando as equa√ß√µes do modelo e os erros passados.

**Refer√™ncias**
[^72]: Se√ß√£o 4.1, Princ√≠pios de Forecasting
<!-- END -->