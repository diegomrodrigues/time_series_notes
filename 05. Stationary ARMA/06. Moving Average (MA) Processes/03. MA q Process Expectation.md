## Expectativa de Processos MA(q)

### Introdu√ß√£o
Este cap√≠tulo √© dedicado √† an√°lise detalhada da expectativa (ou m√©dia) de processos de m√©dias m√≥veis de ordem *q* (MA(q)). Como vimos anteriormente, a expectativa de um processo estoc√°stico √© uma medida central de sua tend√™ncia, representando o valor m√©dio em torno do qual os dados flutuam [^48, 50]. Exploraremos a prova formal de que a expectativa de um processo MA(q) √© igual √† m√©dia do processo, denotada por $\mu$, e discutiremos as implica√ß√µes desse resultado para a an√°lise e modelagem de s√©ries temporais [^48, 50]. Conectaremos este resultado com as propriedades de estacionariedade e ergodicidade discutidas em cap√≠tulos anteriores, quando dispon√≠veis, para fornecer uma compreens√£o abrangente do comportamento dos processos MA(q) [^47].

### Expectativa de um Processo MA(q)

A defini√ß√£o formal de um processo MA(q) √© dada por [^50]:
$$Y_t = \mu + \varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \ldots + \theta_q\varepsilon_{t-q}$$
onde:
*   $Y_t$ representa o valor da s√©rie temporal no instante *t*.
*   $\mu$ √© a m√©dia do processo, uma constante.
*   $\varepsilon_t$ √© um termo de ru√≠do branco no instante *t*, com m√©dia zero e vari√¢ncia $\sigma^2$ [^47].
*   $\theta_1, \theta_2, ..., \theta_q$ s√£o os coeficientes das m√©dias m√≥veis [^50].

**Teorema 1:** *A expectativa de um processo MA(q) √© igual √† m√©dia do processo, ou seja, \$E(Y_t) = \mu\$.*

**Prova:**
Para provar que a expectativa do processo MA(q) √© $\mu$, aplicamos o operador de esperan√ßa a ambos os lados da equa√ß√£o definidora do processo [^48, 50].

I. Partindo da defini√ß√£o do processo MA(q) [^50]:
    $$Y_t = \mu + \varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \ldots + \theta_q\varepsilon_{t-q}$$

II. Aplicando o operador de esperan√ßa $E[\cdot]$ a ambos os lados da equa√ß√£o:
    $$E[Y_t] = E[\mu + \varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \ldots + \theta_q\varepsilon_{t-q}]$$

III. Usando a propriedade da linearidade da esperan√ßa, que permite distribuir o operador $E[\cdot]$ atrav√©s da soma:
    $$E[Y_t] = E[\mu] + E[\varepsilon_t] + \theta_1E[\varepsilon_{t-1}] + \theta_2E[\varepsilon_{t-2}] + \ldots + \theta_qE[\varepsilon_{t-q}]$$

IV. Sabemos que a esperan√ßa de uma constante √© a pr√≥pria constante, ou seja, $E[\mu] = \mu$ [^48]. Tamb√©m sabemos que, por defini√ß√£o, a esperan√ßa do ru√≠do branco √© zero, ou seja, $E[\varepsilon_t] = 0$ para todo *t* [^47].

V. Substituindo esses valores na equa√ß√£o, obtemos:
    $$E[Y_t] = \mu + 0 + \theta_1 \cdot 0 + \theta_2 \cdot 0 + \ldots + \theta_q \cdot 0 = \mu$$

VI. Portanto, a expectativa do processo MA(q) √© igual √† m√©dia do processo:
    $$E[Y_t] = \mu$$ $\blacksquare$

Este resultado demonstra que, em m√©dia, o processo MA(q) se centra em torno do valor $\mu$.

> üí° **Exemplo Num√©rico:** Considere um processo MA(1) definido como $Y_t = 5 + \varepsilon_t + 0.7\varepsilon_{t-1}$, onde $\mu = 5$ e $\theta_1 = 0.7$. Se gerarmos uma longa s√©rie temporal deste processo, a m√©dia amostral dos valores $Y_t$ dever√° se aproximar de 5.

```python
import numpy as np

# Define parameters
mu = 5
theta1 = 0.7
sigma = 1  # Standard deviation of white noise

# Generate white noise
np.random.seed(42)  # for reproducibility
num_samples = 1000
epsilon = np.random.normal(0, sigma, num_samples)

# Generate MA(1) process
Y = np.zeros(num_samples)
Y[0] = mu + epsilon[0]
for t in range(1, num_samples):
    Y[t] = mu + epsilon[t] + theta1 * epsilon[t-1]

# Calculate sample mean
sample_mean = np.mean(Y)

print(f"Theoretical mean (mu): {mu}")
print(f"Sample mean: {sample_mean}")
```

Al√©m do Teorema 1, podemos derivar um resultado relacionado √† esperan√ßa do quadrado do processo MA(q). Este resultado √© √∫til para calcular a vari√¢ncia do processo, que ser√° abordada em detalhes no pr√≥ximo cap√≠tulo.

**Teorema 1.1:** *A esperan√ßa do quadrado de um processo MA(q) √© dada por:*
$$E[Y_t^2] = \mu^2 + \sigma^2(1 + \theta_1^2 + \theta_2^2 + \ldots + \theta_q^2)$$

**Prova:**
I. Come√ßamos elevando ao quadrado a defini√ß√£o do processo MA(q):
   $$Y_t^2 = (\mu + \varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \ldots + \theta_q\varepsilon_{t-q})^2$$

II. Aplicando o operador de esperan√ßa a ambos os lados:
   $$E[Y_t^2] = E[(\mu + \varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \ldots + \theta_q\varepsilon_{t-q})^2]$$

III. Expandindo o quadrado e usando a linearidade da esperan√ßa, juntamente com o fato de que $E[\varepsilon_t] = 0$ e $E[\varepsilon_t \varepsilon_s] = 0$ para $t \neq s$, e $E[\varepsilon_t^2] = \sigma^2$, obtemos:

   $$E[Y_t^2] = E[\mu^2 + \varepsilon_t^2 + \theta_1^2\varepsilon_{t-1}^2 + \ldots + \theta_q^2\varepsilon_{t-q}^2 + 2\mu\varepsilon_t + 2\mu\theta_1\varepsilon_{t-1} + \ldots + 2\mu\theta_q\varepsilon_{t-q} + \text{termos cruzados com } \varepsilon_i\varepsilon_j \text{ para } i\neq j]$$

IV. Como $E[\varepsilon_t] = 0$, todos os termos com $\varepsilon_t$ desaparecem, e como $E[\varepsilon_i \varepsilon_j] = 0$ para $i \neq j$, os termos cruzados tamb√©m desaparecem. Assim, ficamos com:
   $$E[Y_t^2] = \mu^2 + E[\varepsilon_t^2] + \theta_1^2 E[\varepsilon_{t-1}^2] + \ldots + \theta_q^2 E[\varepsilon_{t-q}^2]$$

V. Substituindo $E[\varepsilon_t^2] = \sigma^2$ para todo *t*, obtemos:
   $$E[Y_t^2] = \mu^2 + \sigma^2 + \theta_1^2\sigma^2 + \theta_2^2\sigma^2 + \ldots + \theta_q^2\sigma^2$$

VI. Finalmente, fatorando $\sigma^2$, chegamos a:
   $$E[Y_t^2] = \mu^2 + \sigma^2(1 + \theta_1^2 + \theta_2^2 + \ldots + \theta_q^2)$$ $\blacksquare$

> üí° **Exemplo Num√©rico:** Usando o mesmo processo MA(1) do exemplo anterior ($Y_t = 5 + \varepsilon_t + 0.7\varepsilon_{t-1}$), e assumindo que a vari√¢ncia do ru√≠do branco ($\sigma^2$) √© 1, podemos calcular a esperan√ßa do quadrado de $Y_t$:
>
> $E[Y_t^2] = 5^2 + 1(1 + 0.7^2) = 25 + 1(1 + 0.49) = 25 + 1.49 = 26.49$
>
> Podemos verificar esse resultado simulando o processo e calculando a m√©dia dos quadrados:

```python
import numpy as np

# Define parameters
mu = 5
theta1 = 0.7
sigma = 1  # Standard deviation of white noise

# Generate white noise
np.random.seed(42)  # for reproducibility
num_samples = 1000
epsilon = np.random.normal(0, sigma, num_samples)

# Generate MA(1) process
Y = np.zeros(num_samples)
Y[0] = mu + epsilon[0]
for t in range(1, num_samples):
    Y[t] = mu + epsilon[t] + theta1 * epsilon[t-1]

# Calculate E[Y_t^2] empirically
E_Y2_empirical = np.mean(Y**2)

# Calculate E[Y_t^2] theoretically
E_Y2_theoretical = mu**2 + sigma**2 * (1 + theta1**2)

print(f"Theoretical E[Y_t^2]: {E_Y2_theoretical}")
print(f"Empirical E[Y_t^2]: {E_Y2_empirical}")

```

### Implica√ß√µes da Expectativa Constante

A propriedade de ter uma expectativa constante √© fundamental para a estacionariedade de um processo MA(q), como demonstrado no cap√≠tulo anterior [^48, 51]. Um processo √© dito covariance-stationary se sua m√©dia, vari√¢ncia e autocovari√¢ncia n√£o variam com o tempo [^48]. Como a expectativa de um processo MA(q) √© sempre igual a $\mu$, que √© uma constante, a primeira condi√ß√£o para a estacionariedade √© automaticamente satisfeita.

> üí° **Exemplo Num√©rico:** Imagine uma s√©rie temporal de temperatura m√©dia di√°ria modelada por um MA(q) com $\mu = 25^\circ C$. Se a s√©rie √© estacion√°ria, esperar√≠amos que a temperatura m√©dia permane√ßa em torno de 25 graus ao longo do tempo, mesmo que existam flutua√ß√µes di√°rias devido aos termos de ru√≠do branco.

Al√©m disso, a propriedade $E[Y_t] = \mu$ simplifica a interpreta√ß√£o e a modelagem do processo. Podemos diretamente interpretar $\mu$ como o n√≠vel m√©dio da s√©rie temporal, facilitando a compara√ß√£o entre diferentes s√©ries e a constru√ß√£o de modelos de previs√£o [^48].

> üí° **Exemplo Num√©rico:** Suponha que estejamos modelando o n√∫mero de vendas mensais de um produto com um processo MA(2), e estimemos que $\mu = 100$. Isso significa que, em m√©dia, esperamos vender 100 unidades do produto por m√™s. As flutua√ß√µes em torno desse valor s√£o capturadas pelos termos de ru√≠do branco e seus coeficientes.

### Rela√ß√£o com a Ergodicidade

A ergodicidade, outra propriedade importante dos processos estoc√°sticos, est√° relacionada com a capacidade de estimar as propriedades estat√≠sticas do processo a partir de uma √∫nica realiza√ß√£o da s√©rie temporal [^47, 48]. Em termos simples, um processo √© erg√≥dico se a m√©dia amostral converge para a m√©dia populacional quando o tamanho da amostra tende ao infinito.

Para um processo MA(q) com ru√≠do branco gaussiano, a condi√ß√£o para ergodicidade √© que [^47]:

$$ \sum_{j=0}^{\infty} |\gamma_j| < \infty $$
Como visto anteriormente, essa condi√ß√£o √© satisfeita [^48], o que implica que o processo MA(q) √© erg√≥dico para todos os momentos [^48]. Isso significa que podemos estimar a m√©dia $\mu$ do processo MA(q) usando a m√©dia amostral de uma √∫nica realiza√ß√£o suficientemente longa da s√©rie temporal.

> üí° **Exemplo Num√©rico:** Se simulamos um processo MA(1) com $\mu=10$ por 10000 per√≠odos, a m√©dia amostral dessa √∫nica simula√ß√£o ser√° uma boa estimativa da m√©dia populacional, ilustrando a propriedade erg√≥dica.

```python
import numpy as np

# Define parameters
mu = 10
theta1 = 0.5
sigma = 1

# Generate white noise
np.random.seed(42)
num_samples = 10000
epsilon = np.random.normal(0, sigma, num_samples)

# Generate MA(1) process
Y = np.zeros(num_samples)
Y[0] = mu + epsilon[0]
for t in range(1, num_samples):
    Y[t] = mu + epsilon[t] + theta1 * epsilon[t-1]

# Estimate mean using sample mean
estimated_mean = np.mean(Y)

print(f"True mean: {mu}")
print(f"Estimated mean: {estimated_mean}")

```

Podemos tamb√©m estabelecer uma conex√£o entre a ergodicidade e a esperan√ßa condicional do processo MA(q).

**Proposi√ß√£o 1:** *Seja $Y_t$ um processo MA(q) erg√≥dico e estacion√°rio. Ent√£o, a esperan√ßa condicional de $Y_{t+h}$ dado o passado at√© o tempo *t*, converge para a m√©dia $\mu$ quando *h* tende ao infinito.*
$$ \lim_{h \to \infty} E[Y_{t+h} | Y_t, Y_{t-1}, \ldots ] = \mu $$

**Prova:**
I. A ergodicidade implica que a influ√™ncia das observa√ß√µes passadas diminui √† medida que nos movemos para o futuro.
II. Formalmente, para um processo estacion√°rio e erg√≥dico, as autocorrela√ß√µes decaem para zero quando o lag aumenta [^47, 48].
III. Portanto, √† medida que *h* aumenta, a depend√™ncia de $Y_{t+h}$ em rela√ß√£o a $Y_t, Y_{t-1}, ...$ torna-se desprez√≠vel.
IV. Assim, a melhor previs√£o de $Y_{t+h}$ converge para a m√©dia incondicional, $\mu$ [^48]. $\blacksquare$

> üí° **Exemplo Num√©rico:** Se temos um processo MA(1) com $\mu = 20$ e $\theta_1 = 0.3$, e observamos $Y_1, Y_2, ..., Y_{10}$, a medida que tentamos prever $Y_{10+h}$ com $h$ crescente, nossa previs√£o se aproximar√° cada vez mais de 20, independentemente dos valores iniciais observados.

### Aplica√ß√µes Pr√°ticas

O conhecimento de que $E[Y_t] = \mu$ em processos MA(q) tem diversas aplica√ß√µes pr√°ticas, como:

1.  **Estimativa da M√©dia:** Em an√°lise explorat√≥ria de dados, podemos usar a m√©dia amostral da s√©rie temporal para obter uma estimativa inicial da m√©dia do processo $\mu$ [^48].

> üí° **Exemplo Num√©rico:** Dada uma s√©rie temporal de retornos di√°rios de a√ß√µes nos √∫ltimos 2 anos (500 dias), calcular a m√©dia amostral desses 500 retornos nos fornece uma estimativa da m√©dia do processo MA(q) subjacente.

2.  **Centragem dos Dados:** Para algumas t√©cnicas de an√°lise, como a an√°lise espectral, √© √∫til centrar os dados subtraindo a m√©dia amostral de cada observa√ß√£o. Isso remove a componente de n√≠vel DC e facilita a identifica√ß√£o de padr√µes de frequ√™ncia [^47].

> üí° **Exemplo Num√©rico:** Se temos uma s√©rie temporal com uma m√©dia amostral de 50, subtra√≠mos 50 de cada ponto de dados para criar uma nova s√©rie centrada em torno de zero.

3.  **Valida√ß√£o de Modelos:** Podemos usar o conhecimento de que $E[Y_t] = \mu$ para validar a precis√£o dos modelos MA(q) ajustados aos dados. Por exemplo, podemos comparar a m√©dia estimada do modelo com a m√©dia amostral dos dados [^48].

> üí° **Exemplo Num√©rico:** Ap√≥s ajustar um modelo MA(2) a dados de vendas, o modelo estima $\mu = 105$. Se a m√©dia amostral dos dados de vendas for 102, o modelo pode ser considerado razo√°vel, pois a estimativa de $\mu$ est√° pr√≥xima da m√©dia amostral. Uma discrep√¢ncia muito grande pode indicar um problema com o modelo.

4.  **Previs√£o:** Em combina√ß√£o com outros resultados, saber que $E[Y_t]=\mu$ √© crucial para derivar previs√µes √≥timas a partir de modelos MA(q).

### Conclus√£o

Neste cap√≠tulo, demonstramos formalmente que a expectativa de um processo MA(q) √© igual √† m√©dia do processo, $\mu$ [^48, 50]. Este resultado √© fundamental para a compreens√£o das propriedades estat√≠sticas dos processos MA(q) e tem importantes implica√ß√µes para a an√°lise, modelagem e previs√£o de s√©ries temporais [^48, 51]. Conectamos este resultado com os conceitos de estacionariedade e ergodicidade para fornecer uma vis√£o abrangente do comportamento dos processos MA(q) [^47, 48, 51].

### Refer√™ncias
[^47]: Sec√ß√£o 3.2, White Noise
[^48]: Sec√ß√£o 3.3, Moving Average Processes
[^50]: Sec√ß√£o 3.3, The qth-Order Moving Average Process
[^51]: Sec√ß√£o 3.3, The qth-Order Moving Average Process (continua√ß√£o)
<!-- END -->