## Autocovariâncias de Processos MA(q)

### Introdução
Este capítulo aprofunda-se no estudo das autocovariâncias de processos de médias móveis de ordem *q* (MA(q)). Partindo da definição do processo MA(q) e das propriedades do ruído branco, derivaremos as fórmulas para as autocovariâncias $\gamma_j$ para lags $j = 1, 2, ..., q$ [^50, 51]. Mostraremos formalmente que a função de autocorrelação (ACF) "corta" após o lag *q*, ou seja, $\rho_j = 0$ para $j > q$ [^51]. Discutiremos as implicações desse resultado para a identificação da ordem *q* de um processo MA(q) e para a modelagem e previsão de séries temporais [^48]. Conectaremos este resultado com as propriedades de estacionariedade, ergodicidade e função geradora de autocovariância, apresentadas em capítulos anteriores, para fornecer uma compreensão abrangente do comportamento dos processos MA(q) [^47, 48, 51, 61]. Este capítulo complementa as análises anteriores sobre a expectativa e a variância dos processos MA(q).

### Derivação das Autocovariâncias de um Processo MA(q)

Considere um processo MA(q) definido como [^50]:

$$Y_t = \mu + \varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \ldots + \theta_q\varepsilon_{t-q}$$

onde:

*   $Y_t$ representa o valor da série temporal no instante *t*.
*   $\mu$ é a média do processo (uma constante).
*   $\varepsilon_t$ é o termo de ruído branco no instante *t*, com média zero e variância $\sigma^2$ [^47].
*   $\theta_1, \theta_2, ..., \theta_q$ são os coeficientes das médias móveis [^50].

**Teorema 1:** *As autocovariâncias de um processo MA(q) são dadas por $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$ [^50]. Para $j = 1, 2, ..., q$ [^51]:*
$$ \gamma_j = [\theta_j + \theta_{j+1}\theta_1 + \theta_{j+2}\theta_2 + ... + \theta_q\theta_{q-j}]\sigma^2 $$
*onde $\theta_0 = 1$ [^51].*

*Para $j > q$, a autocovariância é zero [^51]:*
$$ \gamma_j = 0, \quad j > q $$

**Prova:**

A autocovariância de lag *j* é definida como [^50]:
$$ \gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] $$

Substituindo as definições de $Y_t$ e $Y_{t-j}$ do processo MA(q):

$$ \gamma_j = E[(\varepsilon_t + \theta_1\varepsilon_{t-1} + \ldots + \theta_q\varepsilon_{t-q})(\varepsilon_{t-j} + \theta_1\varepsilon_{t-j-1} + \ldots + \theta_q\varepsilon_{t-j-q})] $$

Expandindo o produto:

$$ \gamma_j = E[\varepsilon_t\varepsilon_{t-j} + \theta_1\varepsilon_{t-1}\varepsilon_{t-j} + \ldots + \theta_q\varepsilon_{t-q}\varepsilon_{t-j}] + E[\text{termos cruzados}]$$

Usando a propriedade de que $E[\varepsilon_t \varepsilon_\tau] = 0$ se $t \neq \tau$ (ruído branco não correlacionado) [^48], a maioria dos termos se anula. Para $j \leq q$, os termos que sobrevivem são aqueles em que um dos termos $\varepsilon_{t-i}$ coincide com algum $\varepsilon_{t-j-k}$ [^51].

I.  Para $j=0$, temos:
    $$\gamma_0 = E[(Y_t - \mu)(Y_t - \mu)] = E[(Y_t - \mu)^2] = Var(Y_t)$$
    Substituindo a definição de $Y_t$:
    $$Var(Y_t) = Var(\varepsilon_t + \theta_1\varepsilon_{t-1} + \ldots + \theta_q\varepsilon_{t-q})$$
    Como os $\varepsilon_t$ são não correlacionados:
    $$Var(Y_t) = Var(\varepsilon_t) + \theta_1^2 Var(\varepsilon_{t-1}) + \ldots + \theta_q^2 Var(\varepsilon_{t-q}) = \sigma^2(1 + \theta_1^2 + \ldots + \theta_q^2)$$
    Portanto, $\gamma_0 = \sigma^2(1 + \theta_1^2 + \ldots + \theta_q^2)$.

II. Para $1 \leq j \leq q$, expandindo o produto e usando a propriedade do ruído branco, obtemos [^51]:

$$ \gamma_j = E[\theta_j\varepsilon_{t-j}^2 + \theta_{j+1}\theta_1\varepsilon_{t-j}^2 + \ldots + \theta_q\theta_{q-j}\varepsilon_{t-q}^2 ] = \sigma^2 [\theta_j + \theta_{j+1}\theta_1 + \theta_{j+2}\theta_2 + ... + \theta_q\theta_{q-j}] $$

onde $\theta_0 = 1$ [^51] e $\theta_i = 0$ para $i > q$.

III. Para $j > q$, não há termos de ruído branco com o mesmo índice de tempo nas duas partes do produto, então todos os termos se anulam:
$$ \gamma_j = 0, \quad j > q $$ $\blacksquare$

> 💡 **Exemplo Numérico:** Vamos considerar um processo MA(1) com $\mu = 0$, $\theta_1 = 0.6$, e $\sigma^2 = 1$.  Simularemos 100 valores desse processo e calcularemos as autocovariâncias teóricas e amostrais.
>
> **Cálculo Teórico:**
> $\gamma_0 = \sigma^2(1 + \theta_1^2) = 1 * (1 + 0.6^2) = 1.36$
> $\gamma_1 = \theta_1\sigma^2 = 0.6 * 1 = 0.6$
> $\gamma_j = 0$ para $j > 1$
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Parâmetros do MA(1)
> mu = 0
> theta1 = 0.6
> sigma2 = 1
> T = 100
>
> # Gerar ruído branco
> np.random.seed(42)  # Para reproducibilidade
> epsilon = np.random.normal(0, np.sqrt(sigma2), T)
>
> # Gerar processo MA(1)
> Y = np.zeros(T)
> Y[0] = mu + epsilon[0]
> for t in range(1, T):
>     Y[t] = mu + epsilon[t] + theta1 * epsilon[t-1]
>
> # Calcular autocovariâncias amostrais
> def autocovariance(x, lag):
>     n = len(x)
>     x_mean = np.mean(x)
>     if lag >= n:
>         return 0
>     return np.sum((x[:n-lag] - x_mean) * (x[lag:] - x_mean)) / n
>
> lags = np.arange(0, 5)
> autocovariances_amostrais = [autocovariance(Y, lag) for lag in lags]
>
> # Autocovariâncias teóricas
> gamma0_teorico = sigma2 * (1 + theta1**2)
> gamma1_teorico = theta1 * sigma2
> autocovariances_teoricas = [gamma0_teorico, gamma1_teorico, 0, 0, 0]
>
> # Plotar as autocovariâncias
> plt.figure(figsize=(10, 6))
> plt.stem(lags, autocovariances_amostrais, basefmt="b-", label="Amostral", markerfmt="bo")
> plt.stem(lags, autocovariances_teoricas, basefmt="r-", label="Teórica", markerfmt="ro")
> plt.title("Autocovariâncias do Processo MA(1)")
> plt.xlabel("Lag")
> plt.ylabel("Autocovariância")
> plt.xticks(lags)
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
> A execução desse código gera um gráfico comparando as autocovariâncias teóricas e amostrais do processo MA(1). Podemos observar que a autocovariância amostral para o lag 1 é próxima do valor teórico (0.6), e as autocovariâncias para lags maiores que 1 são próximas de zero, confirmando o "corte" da ACF.

> 💡 **Exemplo Numérico:** Considere um processo MA(2) com $\sigma^2 = 1$, $\theta_1 = 0.5$, e $\theta_2 = -0.3$.
>
> A autocovariância $\gamma_1$ é:
> $\gamma_1 = (\theta_1 + \theta_2\theta_0)\sigma^2 = (0.5 + (-0.3)(1))1 = 0.2$
>
> A autocovariância $\gamma_2$ é:
> $\gamma_2 = (\theta_2 + \theta_3\theta_1)\sigma^2 = (-0.3 + 0)1 = -0.3$, já que $\theta_3 = 0$ (pois estamos em um MA(2)).
>
> Para $j > 2$, $\gamma_j = 0$.

### A Função de Autocorrelação (ACF) e o "Corte" após o Lag q

A função de autocorrelação (ACF) é definida como [^49]:

$$ \rho_j = \frac{\gamma_j}{\gamma_0} $$

onde $\gamma_0$ é a variância do processo.

**Teorema 2:** *A função de autocorrelação (ACF) de um processo MA(q) é zero para lags maiores que q [^51]:*
$$ \rho_j = 0 \quad \text{para} \quad j > q $$
*Isso significa que a ACF "corta" após o lag *q*, o que é uma característica distintiva dos processos MA(q) [^51].*

**Prova:**

I. Por definição, $\rho_j = \frac{\gamma_j}{\gamma_0}$.

II. Do Teorema 1, sabemos que $\gamma_j = 0$ para $j > q$.

III. Portanto, para $j > q$, $\rho_j = \frac{0}{\gamma_0} = 0$. A função ACF para esses lags é, portanto, zero. $\blacksquare$

> 💡 **Exemplo Numérico:** Usando o exemplo anterior do MA(2), se $\gamma_0 = 1.34$, $\gamma_1 = 0.2$, e $\gamma_2 = -0.3$ :
>
> $\rho_1 = \frac{0.2}{1.34} \approx 0.149$
> $\rho_2 = \frac{-0.3}{1.34} \approx -0.224$
>
> Para $j > 2$, $\rho_j = 0$. A ACF corta após o lag 2.
>
> Podemos visualizar essa ACF usando um gráfico de barras:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Autocorrelações
> rho = [1, 0.149, -0.224, 0, 0, 0]  # Incluindo rho_0 = 1
> lags = np.arange(len(rho))
>
> # Plotar a ACF
> plt.figure(figsize=(8, 5))
> plt.stem(lags, rho, basefmt="b-", markerfmt="bo")
> plt.title("Função de Autocorrelação (ACF) para MA(2)")
> plt.xlabel("Lag")
> plt.ylabel("Autocorrelação")
> plt.xticks(lags)
> plt.ylim([-0.5, 1.1])  # Ajustar limites do eixo y para melhor visualização
> plt.grid(True)
> plt.show()
> ```
> Este código gerará um gráfico de barras mostrando a ACF, onde fica evidente o corte após o lag 2.  A autocorrelação no lag 0 é sempre 1.

O gráfico da ACF para esse MA(2) é mostrada abaixo:

Em contrapartida, lembremos do capítulo anterior (Seção 4.1.2 O Projeção Linear Ótima) que, se podemos definir a matriz de covariância de $X_t$ como $\Sigma_{XX} = E[(X_t - E[X_t])(X_t - E[X_t])']$ e o vetor de covariância entre $X_t$ e $Y_{t+1}$ como $\Sigma_{XY} = E[X_t(Y_{t+1} - E[Y_{t+1}])]$, então $b = \Sigma_{XX}^{-1}\Sigma_{XY}$.

Esta equação nos permite quantificar como o modelo de média móvel MA(q) capta a relação com o "passado", que é limitado à ordem q.

**Lema 1:** *A função de autocorrelação (ACF) de um processo MA(1) decresce linearmente do lag 0 até o lag 1, e é zero para lags maiores que 1.*

*Prova:* Já mostrada anteriormente no texto.

**Teorema 2.1:** *A propriedade do "corte" da ACF para processos MA(q) implica que a memória do processo é finita e limitada a q períodos. Ou seja, o valor de $Y_t$ é influenciado apenas pelos q choques aleatórios mais recentes.*

*Prova:* Este resultado decorre diretamente do Teorema 2. Como $\rho_j = 0$ para $j > q$, não há correlação entre $Y_t$ e $Y_{t-j}$ para $j > q$. Isso significa que o processo "esquece" os choques aleatórios que ocorreram há mais de q períodos. $\blacksquare$

### Aplicações da Análise das Autocovariâncias

O estudo das autocovariâncias e da ACF de processos MA(q) tem diversas aplicações práticas:

1.  **Identificação da Ordem do Modelo (q):** A propriedade da ACF "cortar" após o lag *q* é uma ferramenta fundamental para identificar a ordem apropriada de um modelo MA(q) para uma determinada série temporal. Ao analisar o gráfico da ACF amostral, podemos procurar o ponto em que as autocorrelações se tornam insignificantes, indicando a ordem do modelo [^51].

> 💡 **Exemplo Numérico:** Se analisamos a ACF de uma série temporal e observamos que as autocorrelações são significativamente diferentes de zero apenas para os dois primeiros lags, isso sugere que um modelo MA(2) pode ser apropriado. Suponha que temos os seguintes valores de ACF amostral para os primeiros 5 lags:
>
> | Lag (j) | ACF (ρj) |
> |----------|----------|
> | 0        | 1.00     |
> | 1        | 0.55     |
> | 2        | 0.20     |
> | 3        | -0.05    |
> | 4        | 0.02     |
>
> Neste caso, $\rho_0 = 1$ (sempre), $\rho_1 = 0.55$, $\rho_2 = 0.20$, e $\rho_3$ e $\rho_4$ são próximos de zero. Isso sugere que um modelo MA(2) seria uma escolha razoável, pois a ACF parece "cortar" após o lag 2.

2.  **Diagnóstico do Modelo:** Após ajustar um modelo MA(q) aos dados, podemos examinar os resíduos do modelo (a diferença entre os valores observados e os valores previstos) e calcular sua ACF. Se o modelo for adequado, a ACF dos resíduos deve ser insignificante para todos os lags, indicando que os resíduos se comportam como ruído branco.

> 💡 **Exemplo Numérico:** Após ajustar um modelo MA(3), examinamos a ACF dos resíduos. Se a ACF dos resíduos mostrar um pico significativo no lag 4, isso sugere que o modelo pode não capturar toda a dependência temporal nos dados, e um modelo MA de ordem superior pode ser necessário. Para ilustrar, vamos simular resíduos e calcular a ACF:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> import statsmodels.api as sm
> from statsmodels.graphics.tsaplots import plot_acf
>
> # Gerar resíduos simulados (não são ruído branco)
> np.random.seed(42)
> residuos = np.random.normal(0, 1, 100)
> # Introduzir alguma autocorrelação artificialmente no lag 4
> residuos[4:] += 0.5 * residuos[:-4]
>
> # Calcular e plotar a ACF dos resíduos
> plt.figure(figsize=(10, 6))
> plot_acf(residuos, lags=10, ax=plt.gca()) # lags = 10 para visualizar até o lag 10
> plt.title("ACF dos Resíduos (com Autocorrelação no Lag 4)")
> plt.xlabel("Lag")
> plt.ylabel("Autocorrelação")
> plt.grid(True)
> plt.show()
> ```
>
> Neste exemplo, o código simula resíduos e introduz uma autocorrelação artificial no lag 4. O gráfico da ACF mostrará um pico significativo no lag 4, indicando que o modelo original (MA(3) neste caso) não é adequado, e um modelo de ordem superior ou um modelo diferente pode ser necessário.

3.  **Comparação de Modelos:** As autocovariâncias e a ACF podem ser usadas para comparar diferentes modelos MA(q) ajustados aos mesmos dados. Modelos que capturam melhor a estrutura de dependência nos dados (isto é, modelos com ACF mais semelhantes à ACF amostral) geralmente fornecem previsões mais precisas. Critérios de informação como AIC e BIC podem ser usados para comparar modelos de diferentes complexidades.

4.  **Previsão:** As autocovariâncias são usadas nas equações de previsão para calcular os valores esperados futuros da série temporal com base nos valores passados [^16]. Conhecer as autocovariâncias até o lag *q* permite derivar previsões ótimas para o processo MA(q).

> 💡 **Exemplo Numérico:** Para um processo MA(1), a melhor previsão para o próximo período é uma função da média e do erro do período anterior. A autocovariância no lag 1 quantifica como o erro passado influencia o valor esperado futuro.
>
> Suponha um processo MA(1): $Y_t = \mu + \varepsilon_t + \theta_1\varepsilon_{t-1}$ com $\mu = 10$, $\theta_1 = 0.7$, e $\sigma^2 = 1$. Se observamos $Y_{10} = 11$ e sabemos que $\varepsilon_{10} = Y_{10} - \mu - \theta_1\varepsilon_{9}$, precisamos estimar $\varepsilon_{9}$ para calcular $\varepsilon_{10}$.
>
> Suponha que, após algumas iterações, estimamos que $\varepsilon_9 = 0.5$. Então,
> $\varepsilon_{10} = 11 - 10 - (0.7)(0.5) = 1 - 0.35 = 0.65$.
>
> A previsão para $Y_{11}$ é:
> $E[Y_{11}] = \mu + \theta_1\varepsilon_{10} = 10 + (0.7)(0.65) = 10 + 0.455 = 10.455$.
>
> A autocovariância no lag 1 ($\gamma_1 = \theta_1\sigma^2 = 0.7$) influencia diretamente a previsão, ponderando o choque aleatório do período anterior.

**Proposição 1:** A análise da ACF é crucial na seleção da ordem *q* de um modelo MA(q), mas deve ser complementada com outros métodos de diagnóstico para garantir a adequação do modelo.

**Estratégia de Prova:** A escolha da ordem *q* baseada apenas na ACF pode ser enganosa em certas situações, como quando a ACF amostral é ruidosa ou quando o processo subjacente não é estritamente MA(q). Portanto, é importante combinar a análise da ACF com outros métodos de diagnóstico, como a análise dos resíduos, os critérios de informação (AIC, BIC) e os testes de raiz unitária.

### Análise de Autocovariância para Casos Específicos

Vamos analisar as autocovariâncias para alguns casos específicos de processos MA(q).

**Processo MA(1):**

Para um processo MA(1) definido por $Y_t = \mu + \varepsilon_t + \theta_1\varepsilon_{t-1}$:

*   $\gamma_0 = \sigma^2(1 + \theta_1^2)$
*   $\gamma_1 = \theta_1\sigma^2$
*   $\gamma_j = 0$ para $j > 1$

**Processo MA(2):**

Para um processo MA(2) definido por $Y_t = \mu + \varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2}$:

*   $\gamma_0 = \sigma^2(1 + \theta_1^2 + \theta_2^2)$
*   $\gamma_1 = (\theta_1 + \theta_1\theta_2)\sigma^2$
*   $\gamma_2 = \theta_2\sigma^2$
*   $\gamma_j = 0$ para $j > 2$

> 💡 **Exemplo Numérico:** Para ilustrar como os coeficientes $\theta_1$ e $\theta_2$ afetam as autocovariâncias no MA(2), considere dois cenários:
>
> **Cenário 1:** $\theta_1 = 0.8$, $\theta_2 = 0.3$, $\sigma^2 = 1$
> *   $\gamma_0 = 1 * (1 + 0.8^2 + 0.3^2) = 1 + 0.64 + 0.09 = 1.73$
> *   $\gamma_1 = (0.8 + 0.8 * 0.3) * 1 = 0.8 + 0.24 = 1.04$
> *   $\gamma_2 = 0.3 * 1 = 0.3$
>
> **Cenário 2:** $\theta_1 = -0.5$, $\theta_2 = -0.2$, $\sigma^2 = 1$
> *   $\gamma_0 = 1 * (1 + (-0.5)^2 + (-0.2)^2) = 1 + 0.25 + 0.04 = 1.29$
> *   $\gamma_1 = (-0.5 + (-0.5) * (-0.2)) * 1 = -0.5 + 0.1 = -0.4$
> *   $\gamma_2 = -0.2 * 1 = -0.2$
>
> Observamos que os sinais e magnitudes de $\theta_1$ e $\theta_2$ influenciam significativamente os valores e sinais das autocovariâncias $\gamma_1$ e $\gamma_2$. No Cenário 1, ambas são positivas, enquanto no Cenário 2 ambas são negativas.

Esses exemplos ilustram como as autocovariâncias dependem dos coeficientes $\theta_i$ e como a ACF "corta" após o lag *q* para cada processo MA(q).

### Função Geradora de Autocovariância (ACGF) e Autocovariâncias
A função geradora de autocovariância pode ser escrita como [^62]:

$$g_Y(z) = \sigma^2 \Theta(z) \Theta(z^{-1})$$

A avaliação de derivadas da ACGF em z=0, permite recuperar os valores das autocovariâncias para cada lag.

*   Para o caso em que z=1, a avaliação da função geradora não resulta na variância do processo.

**Teorema 3:** As autocovariâncias $\gamma_j$ de um processo MA(q) podem ser obtidas através da expansão em série de potências da função geradora de autocovariância (ACGF) $g_Y(z)$.

**Prova:** Seja $g_Y(z) = \sum_{j=-\infty}^{\infty} \gamma_j z^j$ a função geradora de autocovariância.  Dado que $\gamma_j = 0$ para $|j| > q$ no caso do processo MA(q), temos que $g_Y(z) = \sum_{j=-q}^{q} \gamma_j z^j$. Os coeficientes da expansão em série de potências de $g_Y(z)$ correspondem às autocovariâncias $\gamma_j$ para cada lag j.  $\blacksquare$

> 💡 **Exemplo Numérico:** Considere um processo MA(1) com $\theta_1 = 0.5$ e $\sigma^2 = 1$. A função geradora de autocovariância é:
>
> $g_Y(z) = \sigma^2 \Theta(z) \Theta(z^{-1}) = 1 * (1 + 0.5z)(1 + 0.5z^{-1}) = (1 + 0.5z)(1 + \frac{0.5}{z})$
> $g_Y(z) = 1 + \frac{0.5}{z} + 0.5z + 0.25 = 1.25 + 0.5z + \frac{0.5}{z}$
>
> Expandindo a função, temos:
>
> $g_Y(z) = \gamma_{-1}z^{-1} + \gamma_0 + \gamma_1z$
>
> Comparando os coeficientes, obtemos:
>
> $\gamma_0 = 1.25 = \sigma^2(1 + \theta_1^2) = 1*(1 + 0.5^2) = 1.25$
> $\gamma_1 = 0.5 = \theta_1\sigma^2 = 0.5 * 1 = 0.5$
> $\gamma_{-1} = 0.5$
>
> Para $j > 1$, $\gamma_j = 0$.
>
> Isso demonstra como a função geradora de autocovariância pode ser usada para calcular as autocovariâncias do processo MA(1).

### Conclusão

Neste capítulo, derivamos as fórmulas para as autocovariâncias de processos MA(q) e mostramos que a ACF "corta" após o lag *q* [^50, 51]. Discutimos as implicações desse resultado para a identificação da ordem do modelo, o diagnóstico do modelo e a previsão de séries temporais [^48, 51]. Conectamos este resultado com os conceitos de estacionariedade, ergodicidade e função geradora de autocovariância para fornecer uma compreensão abrangente do comportamento dos processos MA(q) [^47, 48, 51, 61]. Este capítulo fornece as ferramentas necessárias para analisar e modelar séries temporais usando processos MA(q) de forma eficaz.

### Referências
[^16]: Capítulo 4, Forecasting
[^47]: Secção 3.2, White Noise
[^48]: Secção 3.3, Moving Average Processes
[^49]: Secção 3.3, The jth autocorrelation of a covariance-stationary process (denoted ρj) is defined as its jth autocovariance divided by the variance: Pj = γj/γ0 [3.3.6]
[^50]: Secção 3.3, The qth-Order Moving Average Process
[^51]: Secção 3.3, The qth-Order Moving Average Process (continuação)
[^61]: Secção 3.6, The Autocovariance-Generating Function
[^62]: Secção 3.6, As an example of calculating an autocovariance-generating function, consider the MA(1) process. From equations [3.3.3] to [3.3.5], its autocovariance-generating function is
<!-- END -->