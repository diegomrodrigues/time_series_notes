## Variância de Processos MA(q)

### Introdução
Este capítulo detalha a derivação e a análise da variância de processos de médias móveis de ordem $q$ (MA(q)). Construindo sobre o entendimento da expectativa de processos MA(q) apresentado no capítulo anterior [^48, 50], exploraremos como a variância, denotada como $\gamma_0$, quantifica a dispersão dos valores da série temporal em torno de sua média [^48]. Apresentaremos uma prova formal de que a variância de um processo MA(q) é dada por $\gamma_0 = \sigma^2(1 + \theta_1^2 + \theta_2^2 + ... + \theta_q^2)$, onde $\sigma^2$ é a variância do ruído branco e $\theta_i$ são os coeficientes do processo [^50]. Discutiremos também as implicações desse resultado para a modelagem e análise de séries temporais, conectando-o com os conceitos de estacionariedade e autocovariância. Este capítulo se baseia no conhecimento das propriedades do ruído branco [^47] e em capítulos anteriores sobre processos MA(q) [^48, 50, 51].

### Derivação da Variância de um Processo MA(q)

Considere um processo MA(q) definido como [^50]:

$$Y_t = \mu + \varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \ldots + \theta_q\varepsilon_{t-q}$$

onde:

*   $Y_t$ representa o valor da série temporal no instante *t*.
*   $\mu$ é a média do processo (uma constante).
*   $\varepsilon_t$ é o termo de ruído branco no instante *t*, com média zero e variância $\sigma^2$ [^47].
*   $\theta_1, \theta_2, ..., \theta_q$ são os coeficientes das médias móveis [^50].

**Teorema 1:** *A variância de um processo MA(q) é dada por $\gamma_0 = \sigma^2(1 + \theta_1^2 + \theta_2^2 + ... + \theta_q^2)$ [^50].*

**Prova:**

A variância de $Y_t$ é definida como [^50]:

$$\gamma_0 = E[(Y_t - \mu)^2]$$

Substituindo a definição de $Y_t$ do processo MA(q):

$$\gamma_0 = E[(\varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \ldots + \theta_q\varepsilon_{t-q})^2]$$

Expandindo o quadrado:

$$\gamma_0 = E[(\varepsilon_t + \theta_1\varepsilon_{t-1} + \ldots + \theta_q\varepsilon_{t-q})(\varepsilon_t + \theta_1\varepsilon_{t-1} + \ldots + \theta_q\varepsilon_{t-q})]$$

Usando a propriedade de que $E[\varepsilon_t\varepsilon_\tau] = 0$ para $t \neq \tau$ (isto é, os termos de ruído branco em diferentes instantes de tempo são não correlacionados) [^48], todos os termos cruzados (isto é, $\varepsilon_t \varepsilon_{t-1}$, $\varepsilon_t \varepsilon_{t-2}$, etc.) terão esperança zero.  Restam apenas os termos ao quadrado:

$$\gamma_0 = E[\varepsilon_t^2 + \theta_1^2\varepsilon_{t-1}^2 + \theta_2^2\varepsilon_{t-2}^2 + \ldots + \theta_q^2\varepsilon_{t-q}^2]$$

Aplicando a linearidade da esperança:

$$\gamma_0 = E[\varepsilon_t^2] + \theta_1^2E[\varepsilon_{t-1}^2] + \theta_2^2E[\varepsilon_{t-2}^2] + \ldots + \theta_q^2E[\varepsilon_{t-q}^2]$$

Sabendo que $E[\varepsilon_t^2] = \sigma^2$ para todo *t* (por definição de ruído branco) [^47]:

$$\gamma_0 = \sigma^2 + \theta_1^2\sigma^2 + \theta_2^2\sigma^2 + \ldots + \theta_q^2\sigma^2$$

Finalmente, fatorando $\sigma^2$:

$$\gamma_0 = \sigma^2(1 + \theta_1^2 + \theta_2^2 + \ldots + \theta_q^2)$$ $\blacksquare$

> 💡 **Exemplo Numérico:** Considere um processo MA(2) com $\sigma^2 = 4$, $\theta_1 = 0.6$, e $\theta_2 = -0.4$. A variância do processo é:
>
> $\gamma_0 = 4(1 + 0.6^2 + (-0.4)^2) = 4(1 + 0.36 + 0.16) = 4(1.52) = 6.08$
>
> Isso significa que a dispersão dos valores de $Y_t$ em torno da média $\mu$ é 6.08.

**Corolário 1:** *A variância de um processo MA(0) (ruído branco) é simplesmente $\sigma^2$.*

*Prova:* Para um processo MA(0), temos $Y_t = \mu + \varepsilon_t$.  Portanto, a variância é $E[(Y_t - \mu)^2] = E[\varepsilon_t^2] = \sigma^2$.

> 💡 **Exemplo Numérico:** Se $\varepsilon_t$ representa ruído branco com média zero e variância $\sigma^2 = 1$, então a variância do processo MA(0) é simplesmente $\gamma_0 = 1$. Um gráfico de uma série temporal gerada por ruído branco mostraria flutuações aleatórias em torno de zero, com uma dispersão quantificada pela variância de 1.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Generate white noise data
> np.random.seed(0)
> num_samples = 100
> white_noise = np.random.normal(0, 1, num_samples)
>
> # Calculate variance
> variance = np.var(white_noise)
>
> # Plot the white noise
> plt.figure(figsize=(10, 6))
> plt.plot(white_noise)
> plt.title(f"White Noise Process (Variance = {variance:.2f})")
> plt.xlabel("Time")
> plt.ylabel("Amplitude")
> plt.grid(True)
> plt.show()
> ```

**Corolário 2:** *A variância de um processo MA(1) é $\sigma^2(1 + \theta_1^2)$.*

*Prova:* Para um processo MA(1), temos $Y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1}$.  Portanto, a variância é $\sigma^2(1 + \theta_1^2)$.

> 💡 **Exemplo Numérico:** Considere um processo MA(1) com $\sigma^2 = 1$ e $\theta_1 = 0.8$. A variância é $\gamma_0 = 1(1 + 0.8^2) = 1.64$. Isso significa que a variabilidade em $Y_t$ é 64% maior do que a variabilidade do ruído branco subjacente.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Parameters
> sigma_squared = 1
> theta_1 = 0.8
> num_samples = 100
>
> # Generate white noise
> np.random.seed(0)
> white_noise = np.random.normal(0, np.sqrt(sigma_squared), num_samples)
>
> # Generate MA(1) process
> ma_1 = np.zeros(num_samples)
> ma_1[0] = white_noise[0]
> for t in range(1, num_samples):
>     ma_1[t] = white_noise[t] + theta_1 * white_noise[t-1]
>
> # Calculate theoretical variance
> theoretical_variance = sigma_squared * (1 + theta_1**2)
>
> # Calculate empirical variance
> empirical_variance = np.var(ma_1)
>
> # Plot the MA(1) process
> plt.figure(figsize=(10, 6))
> plt.plot(ma_1)
> plt.title(f"MA(1) Process (Theoretical Variance = {theoretical_variance:.2f}, Empirical Variance = {empirical_variance:.2f})")
> plt.xlabel("Time")
> plt.ylabel("Amplitude")
> plt.grid(True)
> plt.show()
> ```

Podemos complementar esses resultados com a função geradora de autocovariância (ACGF), apresentada em capítulos anteriores.

**Teorema 1.1:** *A função geradora de autocovariância (ACGF) de um processo MA(q) avaliada em z=1 é igual à variância do processo.*

*Prova:*
I.  A função geradora de autocovariância (ACGF) é definida como [^61]:
    $$g_Y(z) = \sum_{j=-\infty}^{\infty} \gamma_j z^j$$

II. Avaliando a ACGF em $z = 1$:
    $$g_Y(1) = \sum_{j=-\infty}^{\infty} \gamma_j (1)^j = \sum_{j=-\infty}^{\infty} \gamma_j$$

III. Para um processo MA(q), sabemos que $\gamma_j = 0$ para $|j| > q$ [^50, 51]. Portanto, a soma se torna finita:
    $$g_Y(1) = \gamma_{-q} + \gamma_{-q+1} + ... + \gamma_{-1} + \gamma_0 + \gamma_1 + ... + \gamma_{q-1} + \gamma_{q}$$

IV. Usando a propriedade de que $\gamma_j = \gamma_{-j}$:
    $$g_Y(1) = \gamma_0 + 2\sum_{j=1}^{q} \gamma_j$$

V. Para o caso particular onde j=0, a ACGF é expressa como a variância do processo. No processo MA(q), tem-se [^50]:
    $$ \gamma_0 = E(Y_t - \mu)^2 = E(\varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + ... + \theta_q\varepsilon_{t-q})^2  = \sigma^2 (1 + \theta_1^2 + \theta_2^2 + ... + \theta_q^2) $$
Como $\gamma_0$ é a variância e as demais autocovariâncias são dependentes de $\sigma^2$, avaliar a ACGF em z=1 resulta na variância do processo. $\blacksquare$

> 💡 **Exemplo Numérico:** Considere o processo MA(1) dado por $Y_t = \varepsilon_t + \theta \varepsilon_{t-1}$, onde $\varepsilon_t \sim WN(0, \sigma^2)$. A função geradora de autocovariância é dada por $g_Y(z) = \sigma^2(1+\theta z)(1 + \theta z^{-1})$. Avaliando em z=1, obtemos $g_Y(1) = \sigma^2 (1 + \theta z)(1 + \theta z^{-1})|_{z=1} = \sigma^2(1 + \theta)^2$, que não é igual a $Var(Y_t) = \sigma^2 (1+\theta^2)$.
*Correção:*
A função geradora de autocovariância de um processo MA(1) é dada por:
$$ g_Y(z) = \sigma^2(1 + \theta z)(1 + \theta z^{-1}) = \sigma^2(1 + \theta(z + z^{-1}) + \theta^2) $$
Assim, a variância corresponde a ACGF quando z=0, e não quando z=1.

**Teorema 1.2:** *A variância de um processo MA(q) é sempre não negativa.*

*Prova:*

Da definição, a variância é o valor esperado do quadrado de uma variável aleatória centrada. Ou seja, $\gamma_0 = E[(Y_t - \mu)^2]$. Como o quadrado de qualquer número real é não negativo, $(Y_t - \mu)^2 \geq 0$ para todo *t*.  Portanto, o valor esperado de uma quantidade não negativa também deve ser não negativo. Assim, $\gamma_0 \geq 0$. Alternativamente, podemos observar a fórmula $\gamma_0 = \sigma^2(1 + \theta_1^2 + \theta_2^2 + ... + \theta_q^2)$.  Como $\sigma^2$ é uma variância (e, portanto, não negativa) e cada $\theta_i^2$ é um quadrado (e, portanto, não negativo), a soma de termos não negativos multiplicada por um termo não negativo também é não negativa. $\blacksquare$

### Relação entre Variância e Estacionariedade

No capítulo anterior, mostramos que os processos MA(q) são sempre covariance-stationary, independentemente dos valores dos coeficientes $\theta_i$ [^48, 51]. A variância, sendo uma das propriedades estatísticas que definem a estacionariedade, desempenha um papel crucial nesse resultado. Ao demonstrar que a variância $\gamma_0$ é constante e não depende do tempo *t*, confirmamos que uma das condições para a estacionariedade é satisfeita.

Além disso, a estacionariedade implica que podemos estimar a variância do processo usando dados históricos, sem nos preocuparmos com mudanças nas propriedades estatísticas ao longo do tempo [^48].

> 💡 **Exemplo Numérico:** Se modelamos a volatilidade diária de uma ação com um processo MA(q), e a variância estimada é 0.0004 (ou seja, desvio padrão de 0.02, ou 2%), esperaríamos que, em média, as flutuações diárias da ação sejam de cerca de 2%, desde que o processo permaneça estacionário.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Simulate a stationary MA(1) process for volatility
> np.random.seed(0)
> num_days = 250  # Simulate for one trading year
> sigma_squared = 0.0004
> theta_1 = 0.5
>
> white_noise = np.random.normal(0, np.sqrt(sigma_squared), num_days)
> volatility = np.zeros(num_days)
> volatility[0] = white_noise[0]
> for t in range(1, num_days):
>     volatility[t] = white_noise[t] + theta_1 * white_noise[t-1]
>
> # Ensure volatility is non-negative (as it should be)
> volatility = np.abs(volatility)
>
> # Plot the simulated volatility
> plt.figure(figsize=(10, 6))
> plt.plot(volatility)
> plt.title("Simulated Daily Volatility (MA(1) Process)")
> plt.xlabel("Day")
> plt.ylabel("Volatility")
> plt.grid(True)
> plt.show()
> ```

### Implicações da Variância para a Modelagem

O conhecimento da variância de um processo MA(q) é útil em diversas aplicações práticas:

1.  **Comparação de Modelos:** A variância pode ser usada para comparar diferentes modelos MA(q) ajustados aos mesmos dados. Modelos com menor variância (após levar em conta o número de parâmetros) geralmente fornecem um melhor ajuste aos dados. Critérios de informação como AIC e BIC utilizam a variância estimada como parte de seu cálculo.

2.  **Construção de Intervalos de Confiança:** A variância é um componente essencial na construção de intervalos de confiança para as previsões geradas pelos modelos MA(q). Intervalos de confiança mais estreitos indicam maior precisão nas previsões.

> 💡 **Exemplo Numérico:** Se prevemos as vendas do próximo mês com um modelo MA(1) e a variância do modelo é alta, o intervalo de confiança para a previsão será amplo, refletindo a incerteza na previsão.
>
> Suponha que a previsão de vendas para o próximo mês é de 1000 unidades, e a variância do erro de previsão (estimada a partir do modelo MA(1)) é de 2500.  Assumindo que os erros de previsão são normalmente distribuídos, um intervalo de confiança de 95% para as vendas seria aproximadamente:
>
> Previsão ± 1.96 * sqrt(Variância) = 1000 ± 1.96 * sqrt(2500) = 1000 ± 1.96 * 50 = 1000 ± 98
>
> Portanto, o intervalo de confiança seria de aproximadamente 902 a 1098 unidades. Um intervalo maior indicaria maior incerteza.

3.  **Análise de Risco:** Em aplicações financeiras, a variância é uma medida de risco. Modelar séries temporais financeiras com processos MA(q) permite estimar e gerenciar o risco associado a esses ativos.

> 💡 **Exemplo Numérico:** Se modelamos os retornos de um portfólio de investimentos com um MA(q) e a variância estimada é alta, isso indica que o portfólio é arriscado, ou seja, tem grande potencial de perdas.
>
> Suponha que modelamos os retornos diários de um portfólio usando um MA(1) e estimamos a variância diária dos retornos como 0.0001 (desvio padrão de 0.01 ou 1%).  Para estimar o risco anualizado, podemos multiplicar a variância diária por 252 (o número aproximado de dias úteis em um ano):
>
> Variância Anualizada ≈ 0.0001 * 252 = 0.0252
>
> Desvio Padrão Anualizado (Volatilidade) ≈ sqrt(0.0252) ≈ 0.1587 ou 15.87%
>
> Isso significa que esperaríamos que o portfólio tenha flutuações anuais em torno de sua média de aproximadamente 15.87%, o que pode ser considerado um nível de risco moderado a alto, dependendo do perfil do investidor.

**Relação com Autocorrelações e a Determinação da Ordem do Modelo (q)**

A variância $\gamma_0$, juntamente com as autocovariâncias $\gamma_j$, define as autocorrelações $\rho_j = \gamma_j / \gamma_0$, que são fundamentais para identificar a ordem *q* de um processo MA(q) [^49, 50]. Lembre-se que, para um processo MA(q), a função de autocorrelação (ACF) "corta" após o lag *q*, ou seja, $\rho_j = 0$ para $j > q$ [^51].

> 💡 **Exemplo Numérico:** Se analisamos a ACF de uma série temporal e observamos que as autocorrelações são significativas apenas para os três primeiros lags (lags 1, 2 e 3), e são aproximadamente zero para todos os lags subsequentes, isso sugere que um modelo MA(3) pode ser apropriado para modelar a série. A variância estimada para o modelo, então, nos dá uma escala sobre a qual entender a magnitude dessas autocorrelações.
>
> Suponha que, após ajustar um modelo MA(3) a uma série temporal, estimamos os seguintes valores:
>
> $\gamma_0$ (Variância) = 10
> $\gamma_1$ (Autocovariância no lag 1) = 5
> $\gamma_2$ (Autocovariância no lag 2) = 2.5
> $\gamma_3$ (Autocovariância no lag 3) = 1.25
>
> As autocorrelações seriam:
>
> $\rho_1 = \gamma_1 / \gamma_0 = 5 / 10 = 0.5$
> $\rho_2 = \gamma_2 / \gamma_0 = 2.5 / 10 = 0.25$
> $\rho_3 = \gamma_3 / \gamma_0 = 1.25 / 10 = 0.125$
>
> Se $\rho_4, \rho_5, ...$ são aproximadamente zero, isso reforça a adequação de um modelo MA(3).

**Proposição 2:** *Para um processo MA(q), a variância $\gamma_0$ é sempre maior ou igual a $\sigma^2$*.

*Prova:*
Como demonstrado anteriormente, $\gamma_0 = \sigma^2(1 + \theta_1^2 + \theta_2^2 + ... + \theta_q^2)$. Cada termo $\theta_i^2$ é não negativo, portanto, a soma $1 + \theta_1^2 + \theta_2^2 + ... + \theta_q^2$ é sempre maior ou igual a 1. Multiplicando por $\sigma^2$, que é não negativo, temos que $\gamma_0 \geq \sigma^2$. A igualdade ocorre apenas quando $\theta_1 = \theta_2 = ... = \theta_q = 0$, que corresponde ao caso do ruído branco. $\blacksquare$

Esta proposição reforça a ideia de que um processo MA(q) introduz dependência temporal nos dados, aumentando a variância em relação ao ruído branco.

### Comparação com Processos AR e ARMA

Enquanto a variância de um processo MA(q) tem uma expressão direta em termos da variância do ruído branco e dos coeficientes $\theta_i$, a variância de um processo autorregressivo (AR) ou de um processo ARMA (Autoregressive Moving Average) tem uma forma mais complexa, envolvendo a solução de equações de Yule-Walker ou outras técnicas de estimação [^16]. Essa diferença reflete a estrutura diferente desses modelos e como eles capturam a dependência temporal nos dados.

> 💡 **Exemplo Numérico:** Considere um processo AR(1) definido por $Y_t = \phi Y_{t-1} + \varepsilon_t$, onde $|\phi| < 1$ para garantir a estacionariedade. A variância de $Y_t$ é dada por $\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$.
>
> Se $\sigma^2 = 1$ e $\phi = 0.7$, a variância do processo AR(1) é $\gamma_0 = \frac{1}{1 - 0.7^2} = \frac{1}{1 - 0.49} = \frac{1}{0.51} \approx 1.96$. Observe que a variância de um processo AR(1) é influenciada de forma diferente pelos seus parâmetros do que em um processo MA(1). Em um AR(1), a variância é amplificada pelo fator $\frac{1}{1-\phi^2}$ em relação à variância do ruído branco, enquanto em um MA(1), a variância é aumentada por $1 + \theta_1^2$.

### Conclusão

Neste capítulo, derivamos e analisamos a variância de processos MA(q), demonstrando que $\gamma_0 = \sigma^2(1 + \theta_1^2 + \theta_2^2 + ... + \theta_q^2)$ [^50]. Discutimos as implicações desse resultado para a análise, modelagem e previsão de séries temporais, conectando-o com os conceitos de estacionariedade, autocorrelação e ergodicidade [^47, 48, 51]. O conhecimento da variância e suas relações é crucial para a aplicação eficaz de modelos MA(q) em diversas áreas, desde finanças até engenharia.

### Referências
[^16]: Capítulo 4, Forecasting
[^47]: Secção 3.2, White Noise
[^48]: Secção 3.3, Moving Average Processes
[^49]: Secção 3.3, The jth autocorrelation of a covariance-stationary process (denoted ρj) is defined as its jth autocovariance divided by the variance: Pj = γj/γ0 [3.3.6]
[^50]: Secção 3.3, The qth-Order Moving Average Process
[^51]: Secção 3.3, The qth-Order Moving Average Process (continuação)
<!-- END -->