## Vari√¢ncia de Processos MA(q)

### Introdu√ß√£o
Este cap√≠tulo detalha a deriva√ß√£o e a an√°lise da vari√¢ncia de processos de m√©dias m√≥veis de ordem $q$ (MA(q)). Construindo sobre o entendimento da expectativa de processos MA(q) apresentado no cap√≠tulo anterior [^48, 50], exploraremos como a vari√¢ncia, denotada como $\gamma_0$, quantifica a dispers√£o dos valores da s√©rie temporal em torno de sua m√©dia [^48]. Apresentaremos uma prova formal de que a vari√¢ncia de um processo MA(q) √© dada por $\gamma_0 = \sigma^2(1 + \theta_1^2 + \theta_2^2 + ... + \theta_q^2)$, onde $\sigma^2$ √© a vari√¢ncia do ru√≠do branco e $\theta_i$ s√£o os coeficientes do processo [^50]. Discutiremos tamb√©m as implica√ß√µes desse resultado para a modelagem e an√°lise de s√©ries temporais, conectando-o com os conceitos de estacionariedade e autocovari√¢ncia. Este cap√≠tulo se baseia no conhecimento das propriedades do ru√≠do branco [^47] e em cap√≠tulos anteriores sobre processos MA(q) [^48, 50, 51].

### Deriva√ß√£o da Vari√¢ncia de um Processo MA(q)

Considere um processo MA(q) definido como [^50]:

$$Y_t = \mu + \varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \ldots + \theta_q\varepsilon_{t-q}$$

onde:

*   $Y_t$ representa o valor da s√©rie temporal no instante *t*.
*   $\mu$ √© a m√©dia do processo (uma constante).
*   $\varepsilon_t$ √© o termo de ru√≠do branco no instante *t*, com m√©dia zero e vari√¢ncia $\sigma^2$ [^47].
*   $\theta_1, \theta_2, ..., \theta_q$ s√£o os coeficientes das m√©dias m√≥veis [^50].

**Teorema 1:** *A vari√¢ncia de um processo MA(q) √© dada por $\gamma_0 = \sigma^2(1 + \theta_1^2 + \theta_2^2 + ... + \theta_q^2)$ [^50].*

**Prova:**

A vari√¢ncia de $Y_t$ √© definida como [^50]:

$$\gamma_0 = E[(Y_t - \mu)^2]$$

Substituindo a defini√ß√£o de $Y_t$ do processo MA(q):

$$\gamma_0 = E[(\varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \ldots + \theta_q\varepsilon_{t-q})^2]$$

Expandindo o quadrado:

$$\gamma_0 = E[(\varepsilon_t + \theta_1\varepsilon_{t-1} + \ldots + \theta_q\varepsilon_{t-q})(\varepsilon_t + \theta_1\varepsilon_{t-1} + \ldots + \theta_q\varepsilon_{t-q})]$$

Usando a propriedade de que $E[\varepsilon_t\varepsilon_\tau] = 0$ para $t \neq \tau$ (isto √©, os termos de ru√≠do branco em diferentes instantes de tempo s√£o n√£o correlacionados) [^48], todos os termos cruzados (isto √©, $\varepsilon_t \varepsilon_{t-1}$, $\varepsilon_t \varepsilon_{t-2}$, etc.) ter√£o esperan√ßa zero.  Restam apenas os termos ao quadrado:

$$\gamma_0 = E[\varepsilon_t^2 + \theta_1^2\varepsilon_{t-1}^2 + \theta_2^2\varepsilon_{t-2}^2 + \ldots + \theta_q^2\varepsilon_{t-q}^2]$$

Aplicando a linearidade da esperan√ßa:

$$\gamma_0 = E[\varepsilon_t^2] + \theta_1^2E[\varepsilon_{t-1}^2] + \theta_2^2E[\varepsilon_{t-2}^2] + \ldots + \theta_q^2E[\varepsilon_{t-q}^2]$$

Sabendo que $E[\varepsilon_t^2] = \sigma^2$ para todo *t* (por defini√ß√£o de ru√≠do branco) [^47]:

$$\gamma_0 = \sigma^2 + \theta_1^2\sigma^2 + \theta_2^2\sigma^2 + \ldots + \theta_q^2\sigma^2$$

Finalmente, fatorando $\sigma^2$:

$$\gamma_0 = \sigma^2(1 + \theta_1^2 + \theta_2^2 + \ldots + \theta_q^2)$$ $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um processo MA(2) com $\sigma^2 = 4$, $\theta_1 = 0.6$, e $\theta_2 = -0.4$. A vari√¢ncia do processo √©:
>
> $\gamma_0 = 4(1 + 0.6^2 + (-0.4)^2) = 4(1 + 0.36 + 0.16) = 4(1.52) = 6.08$
>
> Isso significa que a dispers√£o dos valores de $Y_t$ em torno da m√©dia $\mu$ √© 6.08.

**Corol√°rio 1:** *A vari√¢ncia de um processo MA(0) (ru√≠do branco) √© simplesmente $\sigma^2$.*

*Prova:* Para um processo MA(0), temos $Y_t = \mu + \varepsilon_t$.  Portanto, a vari√¢ncia √© $E[(Y_t - \mu)^2] = E[\varepsilon_t^2] = \sigma^2$.

> üí° **Exemplo Num√©rico:** Se $\varepsilon_t$ representa ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2 = 1$, ent√£o a vari√¢ncia do processo MA(0) √© simplesmente $\gamma_0 = 1$. Um gr√°fico de uma s√©rie temporal gerada por ru√≠do branco mostraria flutua√ß√µes aleat√≥rias em torno de zero, com uma dispers√£o quantificada pela vari√¢ncia de 1.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Generate white noise data
> np.random.seed(0)
> num_samples = 100
> white_noise = np.random.normal(0, 1, num_samples)
>
> # Calculate variance
> variance = np.var(white_noise)
>
> # Plot the white noise
> plt.figure(figsize=(10, 6))
> plt.plot(white_noise)
> plt.title(f"White Noise Process (Variance = {variance:.2f})")
> plt.xlabel("Time")
> plt.ylabel("Amplitude")
> plt.grid(True)
> plt.show()
> ```

**Corol√°rio 2:** *A vari√¢ncia de um processo MA(1) √© $\sigma^2(1 + \theta_1^2)$.*

*Prova:* Para um processo MA(1), temos $Y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1}$.  Portanto, a vari√¢ncia √© $\sigma^2(1 + \theta_1^2)$.

> üí° **Exemplo Num√©rico:** Considere um processo MA(1) com $\sigma^2 = 1$ e $\theta_1 = 0.8$. A vari√¢ncia √© $\gamma_0 = 1(1 + 0.8^2) = 1.64$. Isso significa que a variabilidade em $Y_t$ √© 64% maior do que a variabilidade do ru√≠do branco subjacente.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Parameters
> sigma_squared = 1
> theta_1 = 0.8
> num_samples = 100
>
> # Generate white noise
> np.random.seed(0)
> white_noise = np.random.normal(0, np.sqrt(sigma_squared), num_samples)
>
> # Generate MA(1) process
> ma_1 = np.zeros(num_samples)
> ma_1[0] = white_noise[0]
> for t in range(1, num_samples):
>     ma_1[t] = white_noise[t] + theta_1 * white_noise[t-1]
>
> # Calculate theoretical variance
> theoretical_variance = sigma_squared * (1 + theta_1**2)
>
> # Calculate empirical variance
> empirical_variance = np.var(ma_1)
>
> # Plot the MA(1) process
> plt.figure(figsize=(10, 6))
> plt.plot(ma_1)
> plt.title(f"MA(1) Process (Theoretical Variance = {theoretical_variance:.2f}, Empirical Variance = {empirical_variance:.2f})")
> plt.xlabel("Time")
> plt.ylabel("Amplitude")
> plt.grid(True)
> plt.show()
> ```

Podemos complementar esses resultados com a fun√ß√£o geradora de autocovari√¢ncia (ACGF), apresentada em cap√≠tulos anteriores.

**Teorema 1.1:** *A fun√ß√£o geradora de autocovari√¢ncia (ACGF) de um processo MA(q) avaliada em z=1 √© igual √† vari√¢ncia do processo.*

*Prova:*
I.  A fun√ß√£o geradora de autocovari√¢ncia (ACGF) √© definida como [^61]:
    $$g_Y(z) = \sum_{j=-\infty}^{\infty} \gamma_j z^j$$

II. Avaliando a ACGF em $z = 1$:
    $$g_Y(1) = \sum_{j=-\infty}^{\infty} \gamma_j (1)^j = \sum_{j=-\infty}^{\infty} \gamma_j$$

III. Para um processo MA(q), sabemos que $\gamma_j = 0$ para $|j| > q$ [^50, 51]. Portanto, a soma se torna finita:
    $$g_Y(1) = \gamma_{-q} + \gamma_{-q+1} + ... + \gamma_{-1} + \gamma_0 + \gamma_1 + ... + \gamma_{q-1} + \gamma_{q}$$

IV. Usando a propriedade de que $\gamma_j = \gamma_{-j}$:
    $$g_Y(1) = \gamma_0 + 2\sum_{j=1}^{q} \gamma_j$$

V. Para o caso particular onde j=0, a ACGF √© expressa como a vari√¢ncia do processo. No processo MA(q), tem-se [^50]:
    $$ \gamma_0 = E(Y_t - \mu)^2 = E(\varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + ... + \theta_q\varepsilon_{t-q})^2  = \sigma^2 (1 + \theta_1^2 + \theta_2^2 + ... + \theta_q^2) $$
Como $\gamma_0$ √© a vari√¢ncia e as demais autocovari√¢ncias s√£o dependentes de $\sigma^2$, avaliar a ACGF em z=1 resulta na vari√¢ncia do processo. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere o processo MA(1) dado por $Y_t = \varepsilon_t + \theta \varepsilon_{t-1}$, onde $\varepsilon_t \sim WN(0, \sigma^2)$. A fun√ß√£o geradora de autocovari√¢ncia √© dada por $g_Y(z) = \sigma^2(1+\theta z)(1 + \theta z^{-1})$. Avaliando em z=1, obtemos $g_Y(1) = \sigma^2 (1 + \theta z)(1 + \theta z^{-1})|_{z=1} = \sigma^2(1 + \theta)^2$, que n√£o √© igual a $Var(Y_t) = \sigma^2 (1+\theta^2)$.
*Corre√ß√£o:*
A fun√ß√£o geradora de autocovari√¢ncia de um processo MA(1) √© dada por:
$$ g_Y(z) = \sigma^2(1 + \theta z)(1 + \theta z^{-1}) = \sigma^2(1 + \theta(z + z^{-1}) + \theta^2) $$
Assim, a vari√¢ncia corresponde a ACGF quando z=0, e n√£o quando z=1.

**Teorema 1.2:** *A vari√¢ncia de um processo MA(q) √© sempre n√£o negativa.*

*Prova:*

Da defini√ß√£o, a vari√¢ncia √© o valor esperado do quadrado de uma vari√°vel aleat√≥ria centrada. Ou seja, $\gamma_0 = E[(Y_t - \mu)^2]$. Como o quadrado de qualquer n√∫mero real √© n√£o negativo, $(Y_t - \mu)^2 \geq 0$ para todo *t*.  Portanto, o valor esperado de uma quantidade n√£o negativa tamb√©m deve ser n√£o negativo. Assim, $\gamma_0 \geq 0$. Alternativamente, podemos observar a f√≥rmula $\gamma_0 = \sigma^2(1 + \theta_1^2 + \theta_2^2 + ... + \theta_q^2)$.  Como $\sigma^2$ √© uma vari√¢ncia (e, portanto, n√£o negativa) e cada $\theta_i^2$ √© um quadrado (e, portanto, n√£o negativo), a soma de termos n√£o negativos multiplicada por um termo n√£o negativo tamb√©m √© n√£o negativa. $\blacksquare$

### Rela√ß√£o entre Vari√¢ncia e Estacionariedade

No cap√≠tulo anterior, mostramos que os processos MA(q) s√£o sempre covariance-stationary, independentemente dos valores dos coeficientes $\theta_i$ [^48, 51]. A vari√¢ncia, sendo uma das propriedades estat√≠sticas que definem a estacionariedade, desempenha um papel crucial nesse resultado. Ao demonstrar que a vari√¢ncia $\gamma_0$ √© constante e n√£o depende do tempo *t*, confirmamos que uma das condi√ß√µes para a estacionariedade √© satisfeita.

Al√©m disso, a estacionariedade implica que podemos estimar a vari√¢ncia do processo usando dados hist√≥ricos, sem nos preocuparmos com mudan√ßas nas propriedades estat√≠sticas ao longo do tempo [^48].

> üí° **Exemplo Num√©rico:** Se modelamos a volatilidade di√°ria de uma a√ß√£o com um processo MA(q), e a vari√¢ncia estimada √© 0.0004 (ou seja, desvio padr√£o de 0.02, ou 2%), esperar√≠amos que, em m√©dia, as flutua√ß√µes di√°rias da a√ß√£o sejam de cerca de 2%, desde que o processo permane√ßa estacion√°rio.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Simulate a stationary MA(1) process for volatility
> np.random.seed(0)
> num_days = 250  # Simulate for one trading year
> sigma_squared = 0.0004
> theta_1 = 0.5
>
> white_noise = np.random.normal(0, np.sqrt(sigma_squared), num_days)
> volatility = np.zeros(num_days)
> volatility[0] = white_noise[0]
> for t in range(1, num_days):
>     volatility[t] = white_noise[t] + theta_1 * white_noise[t-1]
>
> # Ensure volatility is non-negative (as it should be)
> volatility = np.abs(volatility)
>
> # Plot the simulated volatility
> plt.figure(figsize=(10, 6))
> plt.plot(volatility)
> plt.title("Simulated Daily Volatility (MA(1) Process)")
> plt.xlabel("Day")
> plt.ylabel("Volatility")
> plt.grid(True)
> plt.show()
> ```

### Implica√ß√µes da Vari√¢ncia para a Modelagem

O conhecimento da vari√¢ncia de um processo MA(q) √© √∫til em diversas aplica√ß√µes pr√°ticas:

1.  **Compara√ß√£o de Modelos:** A vari√¢ncia pode ser usada para comparar diferentes modelos MA(q) ajustados aos mesmos dados. Modelos com menor vari√¢ncia (ap√≥s levar em conta o n√∫mero de par√¢metros) geralmente fornecem um melhor ajuste aos dados. Crit√©rios de informa√ß√£o como AIC e BIC utilizam a vari√¢ncia estimada como parte de seu c√°lculo.

2.  **Constru√ß√£o de Intervalos de Confian√ßa:** A vari√¢ncia √© um componente essencial na constru√ß√£o de intervalos de confian√ßa para as previs√µes geradas pelos modelos MA(q). Intervalos de confian√ßa mais estreitos indicam maior precis√£o nas previs√µes.

> üí° **Exemplo Num√©rico:** Se prevemos as vendas do pr√≥ximo m√™s com um modelo MA(1) e a vari√¢ncia do modelo √© alta, o intervalo de confian√ßa para a previs√£o ser√° amplo, refletindo a incerteza na previs√£o.
>
> Suponha que a previs√£o de vendas para o pr√≥ximo m√™s √© de 1000 unidades, e a vari√¢ncia do erro de previs√£o (estimada a partir do modelo MA(1)) √© de 2500.  Assumindo que os erros de previs√£o s√£o normalmente distribu√≠dos, um intervalo de confian√ßa de 95% para as vendas seria aproximadamente:
>
> Previs√£o ¬± 1.96 * sqrt(Vari√¢ncia) = 1000 ¬± 1.96 * sqrt(2500) = 1000 ¬± 1.96 * 50 = 1000 ¬± 98
>
> Portanto, o intervalo de confian√ßa seria de aproximadamente 902 a 1098 unidades. Um intervalo maior indicaria maior incerteza.

3.  **An√°lise de Risco:** Em aplica√ß√µes financeiras, a vari√¢ncia √© uma medida de risco. Modelar s√©ries temporais financeiras com processos MA(q) permite estimar e gerenciar o risco associado a esses ativos.

> üí° **Exemplo Num√©rico:** Se modelamos os retornos de um portf√≥lio de investimentos com um MA(q) e a vari√¢ncia estimada √© alta, isso indica que o portf√≥lio √© arriscado, ou seja, tem grande potencial de perdas.
>
> Suponha que modelamos os retornos di√°rios de um portf√≥lio usando um MA(1) e estimamos a vari√¢ncia di√°ria dos retornos como 0.0001 (desvio padr√£o de 0.01 ou 1%).  Para estimar o risco anualizado, podemos multiplicar a vari√¢ncia di√°ria por 252 (o n√∫mero aproximado de dias √∫teis em um ano):
>
> Vari√¢ncia Anualizada ‚âà 0.0001 * 252 = 0.0252
>
> Desvio Padr√£o Anualizado (Volatilidade) ‚âà sqrt(0.0252) ‚âà 0.1587 ou 15.87%
>
> Isso significa que esperar√≠amos que o portf√≥lio tenha flutua√ß√µes anuais em torno de sua m√©dia de aproximadamente 15.87%, o que pode ser considerado um n√≠vel de risco moderado a alto, dependendo do perfil do investidor.

**Rela√ß√£o com Autocorrela√ß√µes e a Determina√ß√£o da Ordem do Modelo (q)**

A vari√¢ncia $\gamma_0$, juntamente com as autocovari√¢ncias $\gamma_j$, define as autocorrela√ß√µes $\rho_j = \gamma_j / \gamma_0$, que s√£o fundamentais para identificar a ordem *q* de um processo MA(q) [^49, 50]. Lembre-se que, para um processo MA(q), a fun√ß√£o de autocorrela√ß√£o (ACF) "corta" ap√≥s o lag *q*, ou seja, $\rho_j = 0$ para $j > q$ [^51].

> üí° **Exemplo Num√©rico:** Se analisamos a ACF de uma s√©rie temporal e observamos que as autocorrela√ß√µes s√£o significativas apenas para os tr√™s primeiros lags (lags 1, 2 e 3), e s√£o aproximadamente zero para todos os lags subsequentes, isso sugere que um modelo MA(3) pode ser apropriado para modelar a s√©rie. A vari√¢ncia estimada para o modelo, ent√£o, nos d√° uma escala sobre a qual entender a magnitude dessas autocorrela√ß√µes.
>
> Suponha que, ap√≥s ajustar um modelo MA(3) a uma s√©rie temporal, estimamos os seguintes valores:
>
> $\gamma_0$ (Vari√¢ncia) = 10
> $\gamma_1$ (Autocovari√¢ncia no lag 1) = 5
> $\gamma_2$ (Autocovari√¢ncia no lag 2) = 2.5
> $\gamma_3$ (Autocovari√¢ncia no lag 3) = 1.25
>
> As autocorrela√ß√µes seriam:
>
> $\rho_1 = \gamma_1 / \gamma_0 = 5 / 10 = 0.5$
> $\rho_2 = \gamma_2 / \gamma_0 = 2.5 / 10 = 0.25$
> $\rho_3 = \gamma_3 / \gamma_0 = 1.25 / 10 = 0.125$
>
> Se $\rho_4, \rho_5, ...$ s√£o aproximadamente zero, isso refor√ßa a adequa√ß√£o de um modelo MA(3).

**Proposi√ß√£o 2:** *Para um processo MA(q), a vari√¢ncia $\gamma_0$ √© sempre maior ou igual a $\sigma^2$*.

*Prova:*
Como demonstrado anteriormente, $\gamma_0 = \sigma^2(1 + \theta_1^2 + \theta_2^2 + ... + \theta_q^2)$. Cada termo $\theta_i^2$ √© n√£o negativo, portanto, a soma $1 + \theta_1^2 + \theta_2^2 + ... + \theta_q^2$ √© sempre maior ou igual a 1. Multiplicando por $\sigma^2$, que √© n√£o negativo, temos que $\gamma_0 \geq \sigma^2$. A igualdade ocorre apenas quando $\theta_1 = \theta_2 = ... = \theta_q = 0$, que corresponde ao caso do ru√≠do branco. $\blacksquare$

Esta proposi√ß√£o refor√ßa a ideia de que um processo MA(q) introduz depend√™ncia temporal nos dados, aumentando a vari√¢ncia em rela√ß√£o ao ru√≠do branco.

### Compara√ß√£o com Processos AR e ARMA

Enquanto a vari√¢ncia de um processo MA(q) tem uma express√£o direta em termos da vari√¢ncia do ru√≠do branco e dos coeficientes $\theta_i$, a vari√¢ncia de um processo autorregressivo (AR) ou de um processo ARMA (Autoregressive Moving Average) tem uma forma mais complexa, envolvendo a solu√ß√£o de equa√ß√µes de Yule-Walker ou outras t√©cnicas de estima√ß√£o [^16]. Essa diferen√ßa reflete a estrutura diferente desses modelos e como eles capturam a depend√™ncia temporal nos dados.

> üí° **Exemplo Num√©rico:** Considere um processo AR(1) definido por $Y_t = \phi Y_{t-1} + \varepsilon_t$, onde $|\phi| < 1$ para garantir a estacionariedade. A vari√¢ncia de $Y_t$ √© dada por $\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$.
>
> Se $\sigma^2 = 1$ e $\phi = 0.7$, a vari√¢ncia do processo AR(1) √© $\gamma_0 = \frac{1}{1 - 0.7^2} = \frac{1}{1 - 0.49} = \frac{1}{0.51} \approx 1.96$. Observe que a vari√¢ncia de um processo AR(1) √© influenciada de forma diferente pelos seus par√¢metros do que em um processo MA(1). Em um AR(1), a vari√¢ncia √© amplificada pelo fator $\frac{1}{1-\phi^2}$ em rela√ß√£o √† vari√¢ncia do ru√≠do branco, enquanto em um MA(1), a vari√¢ncia √© aumentada por $1 + \theta_1^2$.

### Conclus√£o

Neste cap√≠tulo, derivamos e analisamos a vari√¢ncia de processos MA(q), demonstrando que $\gamma_0 = \sigma^2(1 + \theta_1^2 + \theta_2^2 + ... + \theta_q^2)$ [^50]. Discutimos as implica√ß√µes desse resultado para a an√°lise, modelagem e previs√£o de s√©ries temporais, conectando-o com os conceitos de estacionariedade, autocorrela√ß√£o e ergodicidade [^47, 48, 51]. O conhecimento da vari√¢ncia e suas rela√ß√µes √© crucial para a aplica√ß√£o eficaz de modelos MA(q) em diversas √°reas, desde finan√ßas at√© engenharia.

### Refer√™ncias
[^16]: Cap√≠tulo 4, Forecasting
[^47]: Sec√ß√£o 3.2, White Noise
[^48]: Sec√ß√£o 3.3, Moving Average Processes
[^49]: Sec√ß√£o 3.3, The jth autocorrelation of a covariance-stationary process (denoted œÅj) is defined as its jth autocovariance divided by the variance: Pj = Œ≥j/Œ≥0 [3.3.6]
[^50]: Sec√ß√£o 3.3, The qth-Order Moving Average Process
[^51]: Sec√ß√£o 3.3, The qth-Order Moving Average Process (continua√ß√£o)
<!-- END -->