## Invertibilidade em Processos de M√©dias M√≥veis (MA)

### Introdu√ß√£o
Em continuidade ao estudo de processos estoc√°sticos estacion√°rios, particularmente modelos ARMA (Autoregressive Moving Average), este cap√≠tulo foca na propriedade de **invertibilidade**. A invertibilidade, especialmente em modelos de m√©dias m√≥veis (MA), √© uma caracter√≠stica crucial que garante a unicidade e interpretabilidade do modelo. Nosso objetivo √© aprofundar a compreens√£o te√≥rica dessa propriedade, suas implica√ß√µes e as condi√ß√µes sob as quais ela se manifesta, conforme mencionado anteriormente [^65].

### Conceitos Fundamentais

**Invertibilidade** em processos de m√©dias m√≥veis (MA) refere-se √† capacidade de reescrever uma representa√ß√£o MA como uma representa√ß√£o autorregressiva de ordem infinita (AR($\infty$)) [^65]. Essa propriedade √© fundamental para a identifica√ß√£o e estima√ß√£o de modelos MA, pois garante que o modelo possa ser expresso de forma √∫nica em termos do seu passado.

Considere um processo MA(1) da forma [^65]:

$$ Y_t = \mu + (1 + \theta L)\epsilon_t $$

onde:
*   $Y_t$ √© o valor do processo no tempo $t$
*   $\mu$ √© a m√©dia do processo
*   $\theta$ √© o coeficiente do termo de m√©dia m√≥vel
*   $L$ √© o operador de *lag*
*   $\epsilon_t$ √© o ru√≠do branco no tempo $t$

A invertibilidade desse processo MA(1) significa que ele pode ser expresso como um processo AR($\infty$) [^65]:

$$ (1 - \theta L + \theta^2 L^2 - \theta^3 L^3 + \ldots)(Y_t - \mu) = \epsilon_t $$

Essa representa√ß√£o √© obtida invertendo o operador de m√©dia m√≥vel $(1 + \theta L)$ [^65]. A condi√ß√£o necess√°ria e suficiente para a invertibilidade de um processo MA(1) √© que $|\theta| < 1$. Se essa condi√ß√£o for satisfeita, a s√©rie infinita converge e o processo √© invert√≠vel [^65].

> üí° **Exemplo Num√©rico:**
>
> Considere um processo MA(1) com $\mu = 10$ e $\theta = 0.5$. Ent√£o, $Y_t = 10 + (1 + 0.5L)\epsilon_t$. Como $|\theta| = 0.5 < 1$, o processo √© invert√≠vel. Podemos express√°-lo como um AR($\infty$):
>
> $$ (1 - 0.5L + 0.25L^2 - 0.125L^3 + \ldots)(Y_t - 10) = \epsilon_t $$
>
> Suponha que $Y_t = 12$, $Y_{t-1} = 11$, $Y_{t-2} = 9$, $Y_{t-3} = 10.5$. Ent√£o, uma aproxima√ß√£o para $\epsilon_t$ seria:
>
> $\epsilon_t \approx (1 - 0.5L + 0.25L^2 - 0.125L^3)(Y_t - 10) = (1 - 0.5L + 0.25L^2 - 0.125L^3)(12 - 10) = (1 - 0.5L + 0.25L^2 - 0.125L^3)(2)$
>
> $\epsilon_t \approx 2 - 0.5(11-10) + 0.25(9-10) - 0.125(10.5 - 10) = 2 - 0.5 - 0.25 - 0.0625 = 1.1875$
>
> Este exemplo ilustra como, para um processo invert√≠vel, podemos aproximar o ru√≠do branco $\epsilon_t$ usando valores passados de $Y_t$.

**Teorema 1** (Representa√ß√£o AR($\infty$)). Um processo MA(1) dado por $Y_t = \mu + (1 + \theta L)\epsilon_t$, com $|\theta|<1$, admite a representa√ß√£o AR($\infty$) dada acima, onde os coeficientes da representa√ß√£o AR($\infty$) decaem geometricamente.

*Proof Sketch:* Expandindo $(1 + \theta L)^{-1}$ como uma s√©rie geom√©trica, obtemos a representa√ß√£o AR($\infty$). A condi√ß√£o $|\theta|<1$ garante a converg√™ncia da s√©rie.

**Prova do Teorema 1:**
I. Dado o processo MA(1): $Y_t = \mu + (1 + \theta L)\epsilon_t$, com $|\theta| < 1$.
II. Subtraindo $\mu$ de ambos os lados: $Y_t - \mu = (1 + \theta L)\epsilon_t$.
III. Multiplicando ambos os lados por $(1 + \theta L)^{-1}$: $(1 + \theta L)^{-1}(Y_t - \mu) = \epsilon_t$.
IV. Expandindo $(1 + \theta L)^{-1}$ como uma s√©rie geom√©trica, v√°lida para $|\theta L| < 1$ (ou seja, $|\theta| < 1$):
   $$ (1 + \theta L)^{-1} = 1 - \theta L + \theta^2 L^2 - \theta^3 L^3 + \ldots = \sum_{i=0}^{\infty} (-\theta L)^i$$
V. Substituindo na equa√ß√£o do passo III:
   $$ (1 - \theta L + \theta^2 L^2 - \theta^3 L^3 + \ldots)(Y_t - \mu) = \epsilon_t $$
VI. Os coeficientes da representa√ß√£o AR($\infty$) decaem geometricamente com a taxa $\theta$, o que garante a converg√™ncia da s√©rie para $|\theta| < 1$.
Portanto, um processo MA(1) com $|\theta| < 1$ admite a representa√ß√£o AR($\infty$) dada acima. ‚ñ†

**Corol√°rio 1.1**. A representa√ß√£o AR($\infty$) do processo MA(1) √© unicamente determinada se $|\theta| < 1$.

*Proof Sketch:* A unicidade segue diretamente da converg√™ncia da s√©rie geom√©trica $(1 + \theta L)^{-1}$ quando $|\theta| < 1$. Se $|\theta| \geq 1$, a s√©rie diverge, e a representa√ß√£o AR($\infty$) n√£o √© v√°lida.

**Prova do Corol√°rio 1.1:**
I. Assumindo $|\theta| < 1$, a s√©rie geom√©trica $(1 + \theta L)^{-1} = 1 - \theta L + \theta^2 L^2 - \theta^3 L^3 + \ldots$ converge para um valor finito.
II. A converg√™ncia da s√©rie garante que a representa√ß√£o AR($\infty$) √© bem definida e √∫nica.
III. Se $|\theta| \geq 1$, a s√©rie diverge, o que implica que a representa√ß√£o AR($\infty$) n√£o converge para uma solu√ß√£o finita e, portanto, n√£o √© √∫nica ou v√°lida.
Portanto, a representa√ß√£o AR($\infty$) do processo MA(1) √© unicamente determinada se $|\theta| < 1$. ‚ñ†

**Lema 1.1**. Se $|\theta| < 1$, ent√£o $\sum_{i=0}^{\infty} |\theta|^i = \frac{1}{1-|\theta|}$.

*Proof Sketch:* Esta √© a soma de uma s√©rie geom√©trica infinita convergente.

> üí° **Exemplo Num√©rico:**
>
> Se $\theta = 0.5$, ent√£o $\sum_{i=0}^{\infty} (0.5)^i = 1 + 0.5 + 0.25 + 0.125 + \ldots = \frac{1}{1-0.5} = \frac{1}{0.5} = 2$. Isso ilustra como a soma converge para um valor finito quando $|\theta| < 1$.

**Prova do Lema 1.1:**
I. Considere a s√©rie geom√©trica $\sum_{i=0}^{\infty} x^i$, onde $|x| < 1$.
II. A soma parcial da s√©rie geom√©trica √© dada por $S_n = \sum_{i=0}^{n} x^i = \frac{1 - x^{n+1}}{1 - x}$.
III. Tomando o limite quando $n \to \infty$, como $|x| < 1$, temos $\lim_{n \to \infty} x^{n+1} = 0$.
IV. Portanto, $\lim_{n \to \infty} S_n = \lim_{n \to \infty} \frac{1 - x^{n+1}}{1 - x} = \frac{1 - 0}{1 - x} = \frac{1}{1 - x}$.
V. Substituindo $x$ por $|\theta|$, obtemos $\sum_{i=0}^{\infty} |\theta|^i = \frac{1}{1-|\theta|}$ para $|\theta| < 1$. ‚ñ†

**Condi√ß√£o de Invertibilidade:**

A condi√ß√£o geral para a invertibilidade de um processo MA(q) √© que todas as ra√≠zes do polin√¥mio [^67]:

$$ (1 + \theta_1 z + \theta_2 z^2 + \ldots + \theta_q z^q) = 0 $$

estejam fora do c√≠rculo unit√°rio no plano complexo. Em outras palavras, se as ra√≠zes $z_i$ satisfazem $|z_i| > 1$ para todo $i$, ent√£o o processo MA(q) √© invert√≠vel [^67].

> üí° **Exemplo Num√©rico:**
>
> Considere um processo MA(2) com $\theta_1 = 0.5$ e $\theta_2 = 0.3$. O polin√¥mio √© $(1 + 0.5z + 0.3z^2) = 0$. Usando a f√≥rmula quadr√°tica para encontrar as ra√≠zes:
>
> $z = \frac{-0.5 \pm \sqrt{0.5^2 - 4(0.3)(1)}}{2(0.3)} = \frac{-0.5 \pm \sqrt{0.25 - 1.2}}{0.6} = \frac{-0.5 \pm \sqrt{-0.95}}{0.6}$
>
> Como as ra√≠zes s√£o complexas, podemos calcular seu m√≥dulo:
>
> $|z| = \sqrt{(\frac{-0.5}{0.6})^2 + (\frac{\sqrt{0.95}}{0.6})^2} = \sqrt{\frac{0.25}{0.36} + \frac{0.95}{0.36}} = \sqrt{\frac{1.2}{0.36}} = \sqrt{\frac{10}{3}} \approx 1.8257 > 1$
>
> Como $|z| > 1$, o processo MA(2) √© invert√≠vel.

**Observa√ß√£o:** A condi√ß√£o de invertibilidade para MA(q) pode ser expressa equivalentemente em termos do polin√¥mio caracter√≠stico $\Theta(z) = 1 + \theta_1 z + \ldots + \theta_q z^q$. O processo √© invert√≠vel se e somente se $\Theta(z) \neq 0$ para todos $|z| \leq 1$.

Para complementar a condi√ß√£o de invertibilidade, podemos apresentar o seguinte resultado que relaciona a invertibilidade com a fun√ß√£o de autocovari√¢ncia:

**Teorema 3**. Um processo MA(q) √© invert√≠vel se e somente se sua fun√ß√£o de autocovari√¢ncia determina unicamente os par√¢metros $\theta_1, \theta_2, \ldots, \theta_q$.

*Proof Sketch:* Se o processo √© invert√≠vel, ent√£o existe uma √∫nica representa√ß√£o AR($\infty$). Portanto, a fun√ß√£o de autocovari√¢ncia, que determina a estrutura de depend√™ncia do processo, identifica unicamente os par√¢metros. Reciprocamente, se a fun√ß√£o de autocovari√¢ncia determina unicamente os par√¢metros, ent√£o o processo deve ser invert√≠vel, pois caso contr√°rio, haveria m√∫ltiplas representa√ß√µes MA(q) com a mesma fun√ß√£o de autocovari√¢ncia.

**Consequ√™ncias da N√£o Invertibilidade:**

Se a condi√ß√£o de invertibilidade n√£o for satisfeita, o processo MA n√£o ter√° uma representa√ß√£o AR($\infty$) convergente. Isso implica que o modelo n√£o pode ser expresso de forma √∫nica em termos do seu passado, levando a ambiguidades na identifica√ß√£o e estima√ß√£o do modelo [^65]. Al√©m disso, como mencionado no texto, algoritmos para estimar par√¢metros e realizar previs√µes podem ser v√°lidos apenas se a representa√ß√£o invert√≠vel for utilizada [^65].

**Representa√ß√µes N√£o √önicas:**

O texto demonstra que, para um processo MA(1) n√£o invert√≠vel ( $|\theta| > 1$ ), existe um processo MA(1) invert√≠vel com os mesmos momentos de primeira e segunda ordem [^65]. Isso significa que, apenas com base nas propriedades estat√≠sticas observadas, √© imposs√≠vel distinguir entre as duas representa√ß√µes. Essa ambiguidade destaca a import√¢ncia de impor a condi√ß√£o de invertibilidade para garantir a unicidade do modelo [^65].

**Exemplo:**

Considere um processo n√£o invert√≠vel  $Y_t = \mu + (1 + \tilde{\theta}L)\tilde{\epsilon}_t$, onde $|\tilde{\theta}| > 1$. Podemos encontrar um processo invert√≠vel $Y_t = \mu + (1 + \theta L)\epsilon_t$ equivalente, onde $\theta = \frac{1}{\tilde{\theta}}$ e $\sigma^2 = \tilde{\theta}^2 \tilde{\sigma}^2$ [^65]. Ambos os processos ter√£o a mesma fun√ß√£o geradora de autocovari√¢ncia [^65].

> üí° **Exemplo Num√©rico:**
>
> Seja $\tilde{\theta} = 2$ e $\tilde{\sigma}^2 = 1$. O processo n√£o invert√≠vel √© $Y_t = (1 + 2L)\tilde{\epsilon}_t$. O processo invert√≠vel equivalente √© $Y_t = (1 + (1/2)L)\epsilon_t$, onde $\sigma^2 = 2^2 \cdot 1 = 4$.
>
> Ambos os processos ter√£o a mesma fun√ß√£o de autocovari√¢ncia. A autocovari√¢ncia no lag 0 para o primeiro processo √© $(1 + 2^2) \cdot 1 = 5$. A autocovari√¢ncia no lag 1 para o primeiro processo √© $2 \cdot 1 = 2$.
>
> Para o segundo processo, a autocovari√¢ncia no lag 0 √© $(1 + (1/2)^2) \cdot 4 = (1 + 0.25) \cdot 4 = 5$. A autocovari√¢ncia no lag 1 √© $(1/2) \cdot 4 = 2$.
>
> Portanto, os dois processos, um invert√≠vel e outro n√£o invert√≠vel, t√™m a mesma fun√ß√£o de autocovari√¢ncia, tornando-os indistingu√≠veis apenas com base nessa propriedade.

**Lema 1** (Fun√ß√£o de Autocovari√¢ncia). Dois processos MA(1), $Y_t = (1+\theta L)\epsilon_t$ e $Y_t = (1+(1/\theta) L)\epsilon_t'$, com $\mathrm{Var}(\epsilon_t) = \theta^2 \mathrm{Var}(\epsilon_t')$, possuem a mesma fun√ß√£o de autocovari√¢ncia.

*Proof Sketch:* Calcular as fun√ß√µes de autocovari√¢ncia para ambos os processos e mostrar que s√£o id√™nticas.

**Prova do Lema 1:**
I. Considere o processo MA(1) $Y_t = (1 + \theta L)\epsilon_t$.
II. A fun√ß√£o de autocovari√¢ncia $\gamma_k$ √© dada por:
    - $\gamma_0 = \mathrm{Var}(Y_t) = (1 + \theta^2)\sigma^2$, onde $\sigma^2 = \mathrm{Var}(\epsilon_t)$.
    - $\gamma_1 = \mathrm{Cov}(Y_t, Y_{t-1}) = \theta \sigma^2$.
    - $\gamma_k = 0$ para $k > 1$.
III. Agora considere o processo MA(1) $Y_t = (1 + (1/\theta) L)\epsilon_t'$, com $\mathrm{Var}(\epsilon_t) = \theta^2 \mathrm{Var}(\epsilon_t') = \theta^2 \sigma'^2$.
IV. A fun√ß√£o de autocovari√¢ncia $\gamma_k'$ √© dada por:
   - $\gamma_0' = \mathrm{Var}(Y_t) = (1+(1/\theta)^2) \sigma'^2 = (1+(1/\theta)^2) \frac{\sigma^2}{\theta^2} = \frac{\theta^2+1}{\theta^2} \frac{\sigma^2}{\theta^2}$.
   - $\gamma_1' = \mathrm{Cov}(Y_t, Y_{t-1}) = (1/\theta) \sigma'^2 = (1/\theta) \frac{\sigma^2}{\theta^2} = \frac{\sigma^2}{\theta^3} $.

Let's rederive from scratch:

$Y_t = \epsilon_t + \theta \epsilon_{t-1}$
$Y_t' = \epsilon_t' + \frac{1}{\theta} \epsilon_{t-1}'$

Now $Cov(Y_t, Y_{t-k}) = E[Y_t Y_{t-k}]$.
For the first process, if $k = 0$: $E[Y_t^2] = E[\epsilon_t^2 + \theta^2 \epsilon_{t-1}^2 + 2 \theta \epsilon_t \epsilon_{t-1}] = \sigma^2 + \theta^2 \sigma^2$
If $k=1$: $E[Y_t Y_{t-1}] = E[(\epsilon_t + \theta \epsilon_{t-1})(\epsilon_{t-1} + \theta \epsilon_{t-2})] = \theta \sigma^2$
If $k>1$, $0$.
For the second process, $Y_t' = \epsilon_t' + (1/\theta) \epsilon_{t-1}'$, where $\sigma'^2 = Var(\epsilon_t')$.
$Cov(Y_t', Y_{t-k}') = E[Y_t' Y_{t-k}']$
If $k=0$: $E[(Y_t')^2] = E[(\epsilon_t' + (1/\theta) \epsilon_{t-1}')^2] = (1+(1/\theta)^2) \sigma'^2$
If $k=1$: $E[Y_t' Y_{t-1}'] = E[(\epsilon_t' + (1/\theta) \epsilon_{t-1}') (\epsilon_{t-1}' + (1/\theta) \epsilon_{t-2}')] = (1/\theta) \sigma'^2$.
If $k>1$: $0$.
Now, require the covariances to be identical: $\theta \sigma^2 = (1/\theta) \sigma'^2$. Hence $\sigma'^2 = \theta^2 \sigma^2$.
$\gamma_0' = (1+(1/\theta)^2) \theta^2 \sigma^2 = (\theta^2+1) \sigma^2$.
$\gamma_1' = (1/\theta) \theta^2 \sigma^2 = \theta \sigma^2$.
V. Comparando as fun√ß√µes de autocovari√¢ncia, $\gamma_0 = \gamma_0'$ e $\gamma_1 = \gamma_1'$.
Portanto, dois processos MA(1), $Y_t = (1+\theta L)\epsilon_t$ e $Y_t = (1+(1/\theta) L)\epsilon_t'$, com $\mathrm{Var}(\epsilon_t') = \theta^2 \mathrm{Var}(\epsilon_t)$, possuem a mesma fun√ß√£o de autocovari√¢ncia. ‚ñ†

**Implica√ß√µes Pr√°ticas:**

Embora representa√ß√µes invert√≠veis e n√£o invert√≠veis possam descrever os dados igualmente bem, h√° raz√µes pr√°ticas para preferir a representa√ß√£o invert√≠vel [^66]. Em particular, o c√°lculo da inova√ß√£o fundamental $\epsilon_t$ associada ao processo requer apenas o conhecimento dos valores passados e presentes de $Y_t$ na representa√ß√£o invert√≠vel. Na representa√ß√£o n√£o invert√≠vel, o c√°lculo da inova√ß√£o exigiria o conhecimento de todos os valores futuros de $Y_t$, tornando-o impratic√°vel em muitas aplica√ß√µes [^66].

**Teorema 2** (Melhor Previs√£o Linear). A representa√ß√£o invert√≠vel de um processo MA fornece a melhor previs√£o linear de $Y_{t+h}$ dado o passado de $Y_t$, para $h>0$.

*Proof Sketch:* Usar a representa√ß√£o AR($\infty$) invert√≠vel para expressar $Y_{t+h}$ em termos de ru√≠dos brancos passados e presentes. A melhor previs√£o linear √© ent√£o obtida tomando a esperan√ßa condicional dado o passado de $Y_t$.  A n√£o invertibilidade implicaria a depend√™ncia de ru√≠dos brancos futuros, tornando a previs√£o impratic√°vel.

Al√©m disso, podemos adicionar um corol√°rio que estabelece a rela√ß√£o entre a invertibilidade e a estacionariedade em processos ARMA:

**Corol√°rio 2.1**. Para um processo ARMA(p, q), a invertibilidade da parte MA garante a estacionariedade da representa√ß√£o dual AR($\infty$).

*Proof Sketch:* A estacionariedade da representa√ß√£o dual AR($\infty$) depende da converg√™ncia dos coeficientes autorregressivos. A invertibilidade da parte MA garante que esses coeficientes decaiam suficientemente r√°pido, assegurando a estacionariedade.

### Conclus√£o

A invertibilidade √© uma propriedade essencial para processos de m√©dias m√≥veis, garantindo a unicidade da representa√ß√£o do modelo e facilitando a sua interpreta√ß√£o e aplica√ß√£o pr√°tica. Ao assegurar que a condi√ß√£o de invertibilidade seja satisfeita, podemos evitar ambiguidades na identifica√ß√£o do modelo e garantir a validade dos algoritmos de estima√ß√£o e previs√£o. Como discutido no texto, a prefer√™ncia pela representa√ß√£o invert√≠vel reside na sua capacidade de fornecer uma inova√ß√£o fundamental que pode ser calculada com base apenas em dados passados e presentes [^66].

### Refer√™ncias
[^65]: p. 65: *Invertibility for the MA(1) process...*
[^66]: p. 66: *Imagine calculating a series {et}, defined by...*
[^67]: p. 67: *Invertibility for the MA(q) Process...*
<!-- END -->