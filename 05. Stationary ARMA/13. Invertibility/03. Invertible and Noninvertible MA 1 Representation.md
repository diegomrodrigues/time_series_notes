## Invertibilidade: Representa√ß√µes Equivalentes MA(1) e Implica√ß√µes Pr√°ticas

### Introdu√ß√£o
Este cap√≠tulo aprofunda a discuss√£o sobre **invertibilidade**, focando na equival√™ncia entre representa√ß√µes invert√≠veis e n√£o invert√≠veis de processos MA(1) no que diz respeito aos seus momentos de primeira e segunda ordem. Exploraremos como, para cada representa√ß√£o invert√≠vel MA(1), existe uma representa√ß√£o n√£o invert√≠vel MA(1) com os mesmos momentos, e vice-versa. Examinaremos as consequ√™ncias desta equival√™ncia e as raz√µes pr√°ticas pelas quais a representa√ß√£o invert√≠vel √© geralmente preferida, conforme introduzido nos cap√≠tulos anteriores [^65], [^66].

### Representa√ß√µes Equivalentes MA(1): Uma An√°lise Detalhada

Considere um processo MA(1) invert√≠vel definido como [^65]:

$$ Y_t = \mu + (1 + \theta L) \epsilon_t $$

onde $|\theta| < 1$ e $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$. Como vimos anteriormente [^65], para cada processo MA(1) com par√¢metro $\theta$, onde $|\theta|<1$, existe um processo MA(1) com par√¢metro $\frac{1}{\theta}$ que gera a mesma fun√ß√£o de autocovari√¢ncia.

> üí° **Exemplo Num√©rico:**
>
> Seja $Y_t = 5 + (1 + 0.8L) \epsilon_t$, onde $\epsilon_t \sim N(0, 1)$. Aqui, $\mu = 5$, $\theta = 0.8$, e $\sigma^2 = 1$. Este √© um processo MA(1) invert√≠vel porque $|\theta| = 0.8 < 1$.
>
> A m√©dia do processo √© $E[Y_t] = \mu = 5$. A vari√¢ncia √© $Var(Y_t) = (1 + \theta^2)\sigma^2 = (1 + 0.8^2)(1) = 1.64$. A autocovari√¢ncia de primeira ordem √© $Cov(Y_t, Y_{t-1}) = \theta \sigma^2 = 0.8 * 1 = 0.8$.

Agora, considere um processo MA(1) n√£o invert√≠vel:

$$ \tilde{Y}_t = \mu + (1 + \tilde{\theta} L) \tilde{\epsilon}_t $$

onde $|\tilde{\theta}| > 1$ e $\tilde{\epsilon}_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\tilde{\sigma}^2$. Podemos relacionar este processo n√£o invert√≠vel com um processo invert√≠vel atrav√©s das seguintes rela√ß√µes:

$$ \theta = \frac{1}{\tilde{\theta}} $$
$$ \sigma^2 = \tilde{\theta}^2 \tilde{\sigma}^2 $$

Sob estas condi√ß√µes, os processos $Y_t$ e $\tilde{Y}_t$ ter√£o exatamente os mesmos momentos de primeira e segunda ordem.

> üí° **Exemplo Num√©rico:**
>
> Continuando com o exemplo anterior, para o processo n√£o invert√≠vel, temos $\tilde{\theta} = \frac{1}{0.8} = 1.25$. Para que os momentos de primeira e segunda ordem sejam os mesmos, a vari√¢ncia do ru√≠do branco $\tilde{\epsilon}_t$ deve ser $\tilde{\sigma}^2 = \frac{\sigma^2}{\tilde{\theta}^2} = \frac{1}{1.25^2} = 0.64$. Assim, o processo n√£o invert√≠vel correspondente seria $\tilde{Y}_t = 5 + (1 + 1.25L) \tilde{\epsilon}_t$, onde $\tilde{\epsilon}_t \sim N(0, 0.64)$.

**Teorema 1** (Equival√™ncia em Momentos) Sejam $Y_t = \mu + (1 + \theta L) \epsilon_t$ e $\tilde{Y}_t = \mu + (1 + \tilde{\theta} L) \tilde{\epsilon}_t$ processos MA(1), onde $|\theta| < 1$, $|\tilde{\theta}| > 1$, $\tilde{\theta}=\frac{1}{\theta}$, e $\sigma^2 = \tilde{\theta}^2 \tilde{\sigma}^2$. Ent√£o, $E[Y_t] = E[\tilde{Y}_t]$, $\text{Var}(Y_t) = \text{Var}(\tilde{Y}_t)$ e $\text{Cov}(Y_t, Y_{t-1}) = \text{Cov}(\tilde{Y}_t, \tilde{Y}_{t-1})$.

*Proof:*

I. A m√©dia de $Y_t$ √© $E[Y_t] = \mu$ e a m√©dia de $\tilde{Y}_t$ √© $E[\tilde{Y}_t] = \mu$. Portanto, $E[Y_t] = E[\tilde{Y}_t]$.
II. A vari√¢ncia de $Y_t$ √© $\text{Var}(Y_t) = (1+\theta^2)\sigma^2$. A vari√¢ncia de $\tilde{Y}_t$ √© $\text{Var}(\tilde{Y}_t) = (1+\tilde{\theta}^2)\tilde{\sigma}^2$.
III. Substituindo $\tilde{\theta}=\frac{1}{\theta}$ e $\tilde{\sigma}^2 = \frac{\sigma^2}{\tilde{\theta}^2} = \frac{\sigma^2}{\theta^{-2}} = \theta^2 \sigma^2$, obtemos $\text{Var}(\tilde{Y}_t) = (1+\frac{1}{\theta^2}) \theta^2 \sigma^2 = (\theta^2 + 1)\sigma^2$. Portanto, $\text{Var}(Y_t) = \text{Var}(\tilde{Y}_t)$.
IV. A covari√¢ncia entre $Y_t$ e $Y_{t-1}$ √© $\text{Cov}(Y_t, Y_{t-1}) = \theta \sigma^2$. A covari√¢ncia entre $\tilde{Y}_t$ e $\tilde{Y}_{t-1}$ √© $\text{Cov}(\tilde{Y}_t, \tilde{Y}_{t-1}) = \tilde{\theta} \tilde{\sigma}^2$.
V. Substituindo $\tilde{\theta}=\frac{1}{\theta}$ e $\tilde{\sigma}^2 = \theta^2 \sigma^2$, obtemos $\text{Cov}(\tilde{Y}_t, \tilde{Y}_{t-1}) = \frac{1}{\theta} \theta^2 \sigma^2 = \theta \sigma^2$. Portanto, $\text{Cov}(Y_t, Y_{t-1}) = \text{Cov}(\tilde{Y}_t, \tilde{Y}_{t-1})$.

O teorema acima demonstra que, embora as representa√ß√µes sejam distintas, seus momentos de primeira e segunda ordem s√£o id√™nticos. Isso significa que, com base apenas nesses momentos, n√£o √© poss√≠vel distinguir entre a representa√ß√£o invert√≠vel e a n√£o invert√≠vel.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos dois processos MA(1):
>
> $Y_t = \epsilon_t + 0.5\epsilon_{t-1}$, $\epsilon_t \sim N(0,4)$
> $\tilde{Y}_t = \tilde{\epsilon}_t + 2\tilde{\epsilon}_{t-1}$, $\tilde{\epsilon}_t \sim N(0,1)$
>
> Para o primeiro processo, $\theta = 0.5$ e $\sigma^2 = 4$. A m√©dia √© 0 (assumindo $\mu=0$) e a vari√¢ncia √© $(1 + 0.5^2)(4) = 5$. A covari√¢ncia entre $Y_t$ e $Y_{t-1}$ √© $0.5 * 4 = 2$.
>
> Para o segundo processo, $\tilde{\theta} = 2$ e $\tilde{\sigma}^2 = 1$. A m√©dia √© 0 e a vari√¢ncia √© $(1 + 2^2)(1) = 5$. A covari√¢ncia entre $\tilde{Y}_t$ e $\tilde{Y}_{t-1}$ √© $2 * 1 = 2$.
>
> Os dois processos t√™m a mesma m√©dia, vari√¢ncia e covari√¢ncia, demonstrando a equival√™ncia em termos de momentos de primeira e segunda ordem.
>
> Vamos simular esses processos e verificar os momentos:
>
> ```python
> import numpy as np
>
> # Par√¢metros
> theta = 0.5
> sigma2 = 4
>
> theta_tilde = 2
> sigma2_tilde = 1
>
> # N√∫mero de simula√ß√µes
> n_simulations = 10000
>
> # Simula√ß√£o do processo invert√≠vel
> epsilon = np.random.normal(0, np.sqrt(sigma2), n_simulations)
> Y = epsilon[1:] + theta * epsilon[:-1]
>
> # Simula√ß√£o do processo n√£o invert√≠vel
> epsilon_tilde = np.random.normal(0, np.sqrt(sigma2_tilde), n_simulations)
> Y_tilde = epsilon_tilde[1:] + theta_tilde * epsilon_tilde[:-1]
>
> # C√°lculo dos momentos amostrais
> mean_Y = np.mean(Y)
> var_Y = np.var(Y)
> cov_Y = np.cov(Y[:-1], Y[1:])[0, 1]
>
> mean_Y_tilde = np.mean(Y_tilde)
> var_Y_tilde = np.var(Y_tilde)
> cov_Y_tilde = np.cov(Y_tilde[:-1], Y_tilde[1:])[0, 1]
>
> print(f"Invert√≠vel: M√©dia={mean_Y:.2f}, Vari√¢ncia={var_Y:.2f}, Covari√¢ncia={cov_Y:.2f}")
> print(f"N√£o Invert√≠vel: M√©dia={mean_Y_tilde:.2f}, Vari√¢ncia={var_Y_tilde:.2f}, Covari√¢ncia={cov_Y_tilde:.2f}")
> ```
>
> Os resultados da simula√ß√£o devem confirmar que os momentos amostrais dos dois processos s√£o muito pr√≥ximos, corroborando o teorema.

**Lema 1 (Fun√ß√£o de Autocovari√¢ncia)**.  Dois processos MA(1) $Y_t = \epsilon_t + \theta \epsilon_{t-1}$ e $Y_t = \epsilon_t' + \frac{1}{\theta} \epsilon_{t-1}'$ com $Var(\epsilon_t') = \theta^2 Var(\epsilon_t)$, possuem a mesma fun√ß√£o de autocovari√¢ncia [^66].

> üí° **Exemplo Num√©rico:**
>
> Considere dois processos MA(1):
> 1.  $Y_t = \epsilon_t + 0.5 \epsilon_{t-1}$, onde $\epsilon_t \sim N(0, \sigma^2)$.
> 2.  $Y_t' = \epsilon_t' + 2 \epsilon_{t-1}'$, onde $\epsilon_t' \sim N(0, \sigma'^2)$ e $\sigma'^2 = (0.5)^2 \sigma^2 = 0.25 \sigma^2$.
>
> Vamos calcular as autocovari√¢ncias para ambos os processos:
>
> Para $Y_t$:
>
> *   $\gamma_0 = Var(Y_t) = \sigma^2 + (0.5)^2 \sigma^2 = 1.25 \sigma^2$
> *   $\gamma_1 = Cov(Y_t, Y_{t-1}) = 0.5 \sigma^2$
>
> Para $Y_t'$:
>
> *   $\gamma_0' = Var(Y_t') = \sigma'^2 + (2)^2 \sigma'^2 = 0.25 \sigma^2 + 4(0.25) \sigma^2 = 1.25 \sigma^2$
> *   $\gamma_1' = Cov(Y_t', Y_{t-1}') = 2 \sigma'^2 = 2(0.25) \sigma^2 = 0.5 \sigma^2$
>
> Como $\gamma_0 = \gamma_0'$ e $\gamma_1 = \gamma_1'$, os processos possuem a mesma fun√ß√£o de autocovari√¢ncia.

*Prova:*

I. Seja $Y_t = \epsilon_t + \theta \epsilon_{t-1}$ e $Y_t' = \epsilon_t' + \frac{1}{\theta} \epsilon_{t-1}'$, com $Var(\epsilon_t) = \sigma^2$ e $Var(\epsilon_t') = \sigma'^2$.
II. A autocovari√¢ncia no lag 0 para $Y_t$ √©: $\gamma_0 = E(Y_t^2) = E[(\epsilon_t + \theta \epsilon_{t-1})^2] = \sigma^2 + \theta^2 \sigma^2$.
III. A autocovari√¢ncia no lag 1 para $Y_t$ √©: $\gamma_1 = E(Y_t Y_{t-1}) = E[(\epsilon_t + \theta \epsilon_{t-1})(\epsilon_{t-1} + \theta \epsilon_{t-2})] = \theta \sigma^2$.
IV. A autocovari√¢ncia no lag 0 para $Y_t'$ √©: $\gamma_0' = E((Y_t')^2) = E[(\epsilon_t' + \frac{1}{\theta} \epsilon_{t-1}')^2] = \sigma'^2 + \frac{1}{\theta^2} \sigma'^2$.
V. A autocovari√¢ncia no lag 1 para $Y_t'$ √©: $\gamma_1' = E(Y_t' Y_{t-1}') = E[(\epsilon_t' + \frac{1}{\theta} \epsilon_{t-1}') (\epsilon_{t-1}' + \frac{1}{\theta} \epsilon_{t-2}')] = \frac{1}{\theta} \sigma'^2$.

Se requerermos que $Var(\epsilon_t') = \theta^2 Var(\epsilon_t)$, ent√£o:

* $\sigma'^2 = \theta^2 \sigma^2$
* $\gamma_0' = \theta^2 \sigma^2 + \frac{1}{\theta^2} \theta^2 \sigma^2 = \theta^2 \sigma^2 + \sigma^2 = (1+\theta^2) \sigma^2 = \gamma_0$
* $\gamma_1' = \frac{1}{\theta} \theta^2 \sigma^2 = \theta \sigma^2 = \gamma_1$.

Assim, as fun√ß√µes de autocovari√¢ncia s√£o id√™nticas. ‚ñ†

**Teorema 1.1** (Unicidade da Representa√ß√£o Invert√≠vel) Para um dado processo MA(1) com uma fun√ß√£o de autocovari√¢ncia espec√≠fica, existe apenas uma representa√ß√£o invert√≠vel.

*Proof Sketch:*
A prova se baseia na ideia de que, dada a fun√ß√£o de autocovari√¢ncia de um processo MA(1), podemos determinar o par√¢metro $\theta$ (com $|\theta| < 1$) de forma √∫nica. A condi√ß√£o de invertibilidade restringe o espa√ßo de solu√ß√µes para um √∫nico valor de $\theta$.

*Prova:*

I.  Seja $\gamma_0$ a vari√¢ncia do processo $Y_t$ e $\gamma_1$ a covari√¢ncia entre $Y_t$ e $Y_{t-1}$. A fun√ß√£o de autocovari√¢ncia √© definida por esses dois valores.

II. Para um processo MA(1) invert√≠vel $Y_t = \epsilon_t + \theta \epsilon_{t-1}$, temos $\gamma_0 = (1+\theta^2)\sigma^2$ e $\gamma_1 = \theta \sigma^2$, onde $|\theta|<1$ e $\sigma^2$ √© a vari√¢ncia de $\epsilon_t$.

III. Dividindo $\gamma_1$ por $\gamma_0$, obtemos $\frac{\gamma_1}{\gamma_0} = \frac{\theta \sigma^2}{(1+\theta^2)\sigma^2} = \frac{\theta}{1+\theta^2}$.

IV. Seja $x = \frac{\gamma_1}{\gamma_0}$. Ent√£o, $\theta = x(1+\theta^2)$, o que leva √† equa√ß√£o quadr√°tica $x\theta^2 - \theta + x = 0$.

V. Resolvendo para $\theta$, obtemos $\theta = \frac{1 \pm \sqrt{1-4x^2}}{2x}$.

VI. Para garantir que $\theta$ seja real, devemos ter $1 - 4x^2 \geq 0$, o que implica $x^2 \leq \frac{1}{4}$, ou seja, $|x| \leq \frac{1}{2}$.  Isso sempre ser√° verdade, uma vez que $|\frac{\theta}{1 + \theta^2}| \le \frac{1}{2}$ para $|\theta| < 1$.

VII. Se $1-4x^2 > 0$, ent√£o temos duas solu√ß√µes: $\theta_1 = \frac{1 + \sqrt{1-4x^2}}{2x}$ e $\theta_2 = \frac{1 - \sqrt{1-4x^2}}{2x}$.  Note que $\theta_1 \theta_2 = 1$. Portanto, se $|\theta_1| < 1$, ent√£o $|\theta_2| > 1$, e vice-versa.  Portanto, h√° somente uma solu√ß√£o que satisfaz a condi√ß√£o de invertibilidade $|\theta|<1$.

VIII. Se $1-4x^2 = 0$, ent√£o $\theta = \frac{1}{2x}$, e nesse caso $|\theta| = 1$, o que n√£o √© permitido pela defini√ß√£o de invertibilidade. Logo, essa condi√ß√£o deve ser descartada.

IX. Conclu√≠mos que, dada uma fun√ß√£o de autocovari√¢ncia, h√° uma √∫nica representa√ß√£o invert√≠vel do processo MA(1). ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Suponha que a fun√ß√£o de autocovari√¢ncia de um processo MA(1) seja tal que $\gamma_0 = 2$ e $\gamma_1 = 0.5$. Ent√£o, $x = \frac{\gamma_1}{\gamma_0} = \frac{0.5}{2} = 0.25$. Resolvendo a equa√ß√£o quadr√°tica $0.25\theta^2 - \theta + 0.25 = 0$, obtemos $\theta = \frac{1 \pm \sqrt{1 - 4(0.25)^2}}{2(0.25)} = \frac{1 \pm \sqrt{0.75}}{0.5} = 2 \pm \sqrt{3}$.
>
> As duas solu√ß√µes s√£o $\theta_1 = 2 + \sqrt{3} \approx 3.732$ e $\theta_2 = 2 - \sqrt{3} \approx 0.268$. Como $\theta_1 > 1$, a representa√ß√£o invert√≠vel √© dada por $\theta_2 \approx 0.268$.  Portanto, existe uma √∫nica representa√ß√£o invert√≠vel para a dada fun√ß√£o de autocovari√¢ncia.

Para complementar o Teorema da Unicidade da Representa√ß√£o Invert√≠vel, podemos derivar um corol√°rio que explora a rela√ß√£o entre os res√≠duos de representa√ß√µes invert√≠veis e n√£o invert√≠veis.

**Corol√°rio 1.1.1** (Rela√ß√£o entre Res√≠duos) Sejam $Y_t$ um processo MA(1) com representa√ß√£o invert√≠vel $Y_t = \mu + (1 + \theta L) \epsilon_t$ e representa√ß√£o n√£o invert√≠vel $Y_t = \mu + (1 + \tilde{\theta} L) \tilde{\epsilon}_t$, onde $\tilde{\theta} = \frac{1}{\theta}$.  Ent√£o, a vari√¢ncia dos res√≠duos $\epsilon_t$ e $\tilde{\epsilon}_t$ est√£o relacionados por $\sigma^2 = \tilde{\theta}^2 \tilde{\sigma}^2$, e os res√≠duos da representa√ß√£o invert√≠vel, $\epsilon_t$, representam a inova√ß√£o fundamental, enquanto os res√≠duos da representa√ß√£o n√£o invert√≠vel, $\tilde{\epsilon}_t$, n√£o possuem essa propriedade.

*Proof:*

I. Pelo Teorema 1, sabemos que para que as representa√ß√µes invert√≠vel e n√£o invert√≠vel tenham os mesmos momentos de primeira e segunda ordem, a rela√ß√£o entre as vari√¢ncias dos ru√≠dos brancos deve ser $\sigma^2 = \tilde{\theta}^2 \tilde{\sigma}^2$.

II. Na representa√ß√£o invert√≠vel, $\epsilon_t$ representa a inova√ß√£o no instante $t$, ou seja, a parte de $Y_t$ que n√£o pode ser prevista com base no passado de $Y_t$.  Isso decorre da propriedade de invertibilidade, que permite expressar $Y_t$ como uma fun√ß√£o linear de seus valores passados e $\epsilon_t$.

III. Na representa√ß√£o n√£o invert√≠vel, $\tilde{\epsilon}_t$ n√£o representa a inova√ß√£o no instante $t$.  Para calcular $\tilde{\epsilon}_t$, precisar√≠amos de informa√ß√µes sobre os valores futuros de $Y_t$, o que torna $\tilde{\epsilon}_t$ impratic√°vel como uma inova√ß√£o no sentido usual.

IV. Portanto, os res√≠duos $\epsilon_t$ da representa√ß√£o invert√≠vel s√£o a inova√ß√£o fundamental, enquanto os res√≠duos $\tilde{\epsilon}_t$ da representa√ß√£o n√£o invert√≠vel n√£o s√£o.

> üí° **Exemplo Num√©rico:**
>
> Considere um processo MA(1) $Y_t = \epsilon_t + 0.5 \epsilon_{t-1}$, onde $\epsilon_t \sim N(0, 4)$. A representa√ß√£o n√£o invert√≠vel equivalente √© $Y_t = \tilde{\epsilon}_t + 2 \tilde{\epsilon}_{t-1}$, onde $\tilde{\epsilon}_t \sim N(0, 1)$.
>
> Suponha que observamos $Y_1 = 10$. Usando a representa√ß√£o invert√≠vel, podemos estimar $\epsilon_1 = Y_1 - 0.5 \epsilon_0$. Assumindo $\epsilon_0 = 0$, temos $\epsilon_1 = 10$.
>
> No entanto, usando a representa√ß√£o n√£o invert√≠vel, precisar√≠amos de valores futuros de $Y_t$ para estimar $\tilde{\epsilon}_1$. Portanto, $\epsilon_1$ √© uma inova√ß√£o fundamental, enquanto $\tilde{\epsilon}_1$ n√£o √©.

### Implica√ß√µes Pr√°ticas e Prefer√™ncia pela Representa√ß√£o Invert√≠vel [^66]
Embora as representa√ß√µes invert√≠veis e n√£o invert√≠veis possam descrever os dados igualmente bem, existem v√°rias raz√µes pr√°ticas para preferir a representa√ß√£o invert√≠vel [^66]:

1. **Facilidade de Estima√ß√£o:** A estima√ß√£o de par√¢metros em modelos invert√≠veis √© geralmente mais direta, pois a converg√™ncia dos m√©todos de estima√ß√£o (como m√°xima verossimilhan√ßa) √© garantida sob condi√ß√µes mais brandas.
2. **Validade da Previs√£o:** A representa√ß√£o invert√≠vel fornece previs√µes mais robustas e confi√°veis. A invers√£o do operador MA leva a um processo AR($\infty$) que expressa o valor atual em termos de seus valores passados, permitindo a previs√£o com base em dados hist√≥ricos.
3. **Utiliza√ß√£o de Dados do Mundo Real:** Como mencionado anteriormente [^66], o c√°lculo da inova√ß√£o fundamental $\epsilon_t$ associada ao processo requer apenas o conhecimento dos valores passados e presentes de $Y_t$ na representa√ß√£o invert√≠vel. Na representa√ß√£o n√£o invert√≠vel, o c√°lculo da inova√ß√£o exigiria o conhecimento de todos os valores futuros de $Y_t$, tornando-o impratic√°vel em muitas aplica√ß√µes.
4. **Algoritmos de Estima√ß√£o e Previs√£o:** Alguns algoritmos de estima√ß√£o de par√¢metros e previs√£o s√£o v√°lidos apenas se a representa√ß√£o invert√≠vel for utilizada [^66]. A representa√ß√£o invert√≠vel garante que os algoritmos operem em um espa√ßo param√©trico bem comportado, levando a resultados mais precisos.

**Teorema 2** (Melhor Previs√£o Linear) A representa√ß√£o invert√≠vel de um processo MA(1) fornece a melhor previs√£o linear de $Y_{t+h}$ dado o passado de $Y_t$, para $h > 0$.

*Proof Sketch:*
A representa√ß√£o invert√≠vel expressa o processo em termos de seus ru√≠dos brancos passados e presentes, permitindo a previs√£o com base em dados hist√≥ricos. Um processo n√£o invert√≠vel implicaria na necessidade de ru√≠dos brancos futuros para realizar uma previs√£o, tornando a previs√£o irrealiz√°vel.

**Prova do Teorema 2**
I. Considere a representa√ß√£o invert√≠vel $Y_t = \mu + (1 + \theta L)\epsilon_t$, onde $|\theta|<1$.
II. Queremos prever $Y_{t+h}$, onde $h>0$, dado o conjunto de informa√ß√£o $I_t = \{Y_t, Y_{t-1}, Y_{t-2}, \ldots\}$.
III. Podemos reescrever o processo como $\epsilon_t = (1 + \theta L)^{-1} (Y_t - \mu) = (1 - \theta L + \theta^2 L^2 - \ldots)(Y_t - \mu)$.
IV. Ent√£o, $Y_{t+h} = \mu + \epsilon_{t+h} + \theta \epsilon_{t+h-1}$.
V. Tomando a expectativa condicional de $Y_{t+h}$ dado $I_t$, obtemos: $E[Y_{t+h}|I_t] = E[\mu | I_t] + E[\epsilon_{t+h}|I_t] + \theta E[\epsilon_{t+h-1}|I_t]$.
VI. Como $E[\mu | I_t] = \mu$, $E[\epsilon_{t+h}|I_t] = 0$ (pois $\epsilon_{t+h}$ √© ru√≠do branco futuro) e $E[\epsilon_{t+h-1}|I_t] = 0$ para $h>1$, temos: $E[Y_{t+h}|I_t] = \mu$ para $h>1$.

VII. Para o caso especial de $h=1$, temos: $Y_{t+1} = \mu + \epsilon_{t+1} + \theta \epsilon_{t}$.
VIII. Tomando a expectativa condicional dado $I_t$, obtemos: $E[Y_{t+1}|I_t] = \mu + \theta \epsilon_t = \mu + \theta (1 + \theta L)^{-1}(Y_t - \mu) = \mu + \theta \sum_{i=0}^{\infty} (-\theta)^i (Y_{t-i} - \mu)$.
IX.  A representa√ß√£o n√£o invert√≠vel n√£o permite expressar $\epsilon_t$ em termos dos valores passados e presentes de $Y_t$, tornando impratic√°vel o c√°lculo de $E[Y_{t+h}|I_t]$. Portanto, a representa√ß√£o invert√≠vel fornece a melhor previs√£o linear. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere um processo MA(1) invert√≠vel $Y_t = 10 + \epsilon_t + 0.5 \epsilon_{t-1}$, onde $\epsilon_t \sim N(0, 1)$. Queremos prever $Y_{t+1}$ dado $Y_t = 12$ e $Y_{t-1} = 11$.
>
> Primeiro, estimamos $\epsilon_t = (1 + 0.5L)^{-1} (Y_t - 10) = (1 - 0.5L + 0.25L^2 - \dots)(Y_t - 10)$. Aproximando at√© a segunda ordem, temos $\epsilon_t \approx (Y_t - 10) - 0.5(Y_{t-1} - 10) + 0.25(Y_{t-2} - 10)$. Assumindo $Y_{t-2} = 10$ (a m√©dia), temos $\epsilon_t \approx (12 - 10) - 0.5(11 - 10) + 0.25(10 - 10) = 2 - 0.5 = 1.5$.
>
> Ent√£o, a previs√£o de $Y_{t+1}$ √© $E[Y_{t+1}|I_t] = 10 + 0.5 \epsilon_t = 10 + 0.5(1.5) = 10.75$.
>
> Se tent√°ssemos usar a representa√ß√£o n√£o invert√≠vel, precisar√≠amos de valores futuros de $Y_t$ para calcular a previs√£o, o que n√£o √© pr√°tico.

O teorema implica que a inova√ß√£o $\epsilon_t$ em [3.7.1] √© uma inova√ß√£o fundamental para $Y_t$. Considere calcular a inova√ß√£o $\epsilon_t$ para data $t$. Para calcular $\epsilon_t$ em [3.7.8], precisamos conhecer valores de $Y$ at√© a data t:
$\epsilon_t = (1 + \theta L)^{-1} (Y_t - \mu)$. Por outro lado, considerar calcular $\epsilon_t$ para data $t$ associada com uma representa√ß√£o n√£o invert√≠vel (Yt - Œº) = (1 + Œ∏~L ) Œµ~t , como em [3.7.11]. Para fazer isso, √© preciso saber os valores futuros de Y ,
Œµ~t = Œ∏~(Yt+1 ‚Äì Œº) ‚Äì Œ∏~2 (Yt+2 ‚Äì Œº) + ‚Ä¶ Assim, como discutido acima,
enquanto a inova√ß√£o associada com um ARMA particular(p,q) tem momentos id√™nticos, pode haver importantes raz√µes computacionais para selecionar um particular processo.
Com essa raz√£o em mente, podemos agora voltar para as perguntas de identifica√ß√£o e estima√ß√£o, levando-se para as perguntas de previs√£o.
‚ñ†

**Corol√°rio 2.1** (Consequ√™ncia para Modelagem) Ao modelar um processo MA(1) para previs√£o, a garantia de invertibilidade resulta em estimativas de par√¢metros mais est√°veis e, portanto, em previs√µes mais precisas, especialmente em horizontes de longo prazo.

**Lema 2** (Representa√ß√£o AR($\infty$)) Um processo MA(1) invert√≠vel pode ser expresso como um processo autorregressivo de ordem infinita (AR($\infty$)).

*Proof Sketch:*
A prova envolve expandir o operador $(1 + \theta L)^{-1}$ em uma s√©rie infinita, resultando em uma representa√ß√£o do processo $Y_t$ como uma soma ponderada de seus valores passados.

Para formalizar o *Proof Sketch* do Lema 2, fornecemos a seguir a demonstra√ß√£o completa.

*Prova:*
I. Considere o processo MA(1) invert√≠vel: $Y_t = \mu + (1 + \theta L) \epsilon_t$, onde $|\theta| < 1$.

II. Podemos reescrever a equa√ß√£o como: $Y_t - \mu = (1 + \theta L) \epsilon_t$.

III. Multiplicando ambos os lados por $(1 + \theta L)^{-1}$, obtemos: $\epsilon_t = (1 + \theta L)^{-1} (Y_t - \mu)$.

IV. Como $|\theta| < 1$, podemos expandir $(1 + \theta L)^{-1}$ usando a s√©rie geom√©trica: $(1 + \theta L)^{-1} = 1 - \theta L + \theta^2 L^2 - \theta^3 L^3 + \dots = \sum_{i=0}^{\infty} (-\theta)^i L^i$.

V. Substituindo esta expans√£o na equa√ß√£o para $\epsilon_t$, temos: $\epsilon_t = \sum_{i=0}^{\infty} (-\theta)^i L^i (Y_t - \mu)$.

VI. Rearranjando a equa√ß√£o original, obtemos: $Y_t = \mu + \epsilon_t - \sum_{i=1}^{\infty} (-\theta)^i (Y_{t-i} - \mu)$.

VII. Isolando $Y_t$, obtemos a representa√ß√£o AR($\infty$): $Y_t = \mu(1 - \sum_{i=1}^{\infty} (-\theta)^i) + \sum_{i=1}^{\infty} (-\theta)^i Y_{t-i} + \epsilon_t$.  Definindo $\pi_i = (-\theta)^i$, podemos escrever: $Y_t = \alpha + \sum_{i=1}^{\infty} \pi_i Y_{t-i} + \epsilon_t$, onde $\alpha = \mu(1 - \sum_{i=1}^{\infty} (-\theta)^i)$.  Note que $\sum_{i=1}^{\infty} (-\theta)^i = \frac{-\theta}{1 + \theta}$

Portanto, um processo MA(1) invert√≠vel pode ser expresso como um processo autorregressivo de ordem infinita. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Seja $Y_t = \epsilon_t + 0.5\epsilon_{t-1}$. Podemos reescrever este processo como $Y_t = 0.5 Y_{t-1} - 0.25 Y_{t-2} + 0.125 Y_{t-3} - \dots + \epsilon_t$. Esta √© uma representa√ß√£o AR($\infty$) do processo MA(1) original.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> theta = 0.5
> n = 100
>
> # Ru√≠do branco
> epsilon = np.random.normal(0, 1, n)
>
> # MA(1)
> y_ma = [epsilon[i] + theta*epsilon[i-1] if i > 0 else epsilon[i] for i in range(n)]
>
> # AR(inf) approximation, using a finite number of lags (p)
> p = 10
> y_ar_inf = np.zeros(n)
> for t in range(n):
>     ar_sum = 0
>     for i in range(1, min(t+1, p)):
>         ar_sum += (-theta)**i * y_ma[t-i]
#         print((-theta)**i)
#         print(y_ma[t-i])
#         print(ar_sum)
>     y_ar_inf[t] = epsilon[t] + ar_sum
#     print("---")
>
> # Plotting
> plt.figure(figsize=(10, 6))
> plt.plot(y_ma, label="MA(1)", alpha=0.7)
> plt.plot(y_ar_inf, label="AR(inf) (p=10)", alpha=0.7)
> plt.xlabel("Time")
> plt.ylabel("Value")
> plt.legend()
> plt.title("MA(1) and AR(inf) Approximation")
> plt.show()
>
> ```
> This code generates a MA(1) process and approximates it using an AR(infinity) process. The AR(infinity) is truncated to *p* lags. The plot will show that the approximated AR process behaves similarly to the original MA process.

### Conclus√£o
Embora representa√ß√µes invert√≠veis e n√£o invert√≠veis de processos MA(1) compartilhem os mesmos momentos de primeira e segunda ordem, a representa√ß√£o invert√≠vel √© prefer√≠vel por raz√µes pr√°ticas, incluindo facilidade de estima√ß√£o, validade da previs√£o e utiliza√ß√£o de dados do mundo real. Compreender essas nuances √© crucial para a modelagem precisa e eficaz de s√©ries temporais [^66].

### Refer√™ncias
[^65]: p. 65: *Invertibility for the MA(1) process...*
[^66]: p. 78: *Model Selection Criteria.*
<!-- END -->