## Invertibilidade: M√©todos Computacionais e Inova√ß√µes Fundamentais

### Introdu√ß√£o

Em continuidade √† discuss√£o sobre invertibilidade em processos de m√©dias m√≥veis (MA) e √† equival√™ncia entre representa√ß√µes invert√≠veis e n√£o invert√≠veis, este cap√≠tulo aborda **m√©todos computacionais** para alternar entre essas representa√ß√µes e para construir uma representa√ß√£o invert√≠vel a partir de uma n√£o invert√≠vel, e vice-versa. Al√©m disso, exploraremos os algoritmos utilizados para computar as **inova√ß√µes fundamentais** associadas √†s representa√ß√µes invert√≠veis, conforme introduzido nos cap√≠tulos anteriores [^66]. A compreens√£o desses m√©todos √© crucial para a aplica√ß√£o pr√°tica de modelos MA, garantindo que sejam estimados corretamente e utilizados de forma eficaz para a previs√£o.

### M√©todos Computacionais para Alternar entre Representa√ß√µes Invert√≠veis e N√£o Invert√≠veis

#### Processo MA(1)
Para um processo MA(1), a troca entre representa√ß√µes invert√≠veis e n√£o invert√≠veis √© relativamente simples. Se temos uma representa√ß√£o n√£o invert√≠vel com par√¢metro $\tilde{\theta}$ tal que $|\tilde{\theta}| > 1$, podemos encontrar a representa√ß√£o invert√≠vel equivalente com par√¢metro $\theta = \frac{1}{\tilde{\theta}}$, onde $|\theta| < 1$ [^65].

Como visto na **Proposi√ß√£o 1** do cap√≠tulo anterior, para o processo invert√≠vel ter a mesma fun√ß√£o de autocovari√¢ncia, a vari√¢ncia do ru√≠do branco deve ser ajustada. Se a vari√¢ncia do ru√≠do branco no processo n√£o invert√≠vel for $\tilde{\sigma}^2$, ent√£o a vari√¢ncia do ru√≠do branco no processo invert√≠vel ser√° $\sigma^2 = \tilde{\theta}^2 \tilde{\sigma}^2$.

> üí° **Exemplo Num√©rico:**
>
> Considere o processo MA(1) n√£o invert√≠vel: $\tilde{Y}_t = \tilde{\epsilon}_t + 2 \tilde{\epsilon}_{t-1}$, onde $\tilde{\epsilon}_t \sim N(0, 1)$.
>
> O processo invert√≠vel equivalente √© $Y_t = \epsilon_t + \frac{1}{2} \epsilon_{t-1}$, onde $\epsilon_t \sim N(0, 4)$.
>
> ```python
> import numpy as np
>
> # Processo n√£o invert√≠vel
> theta_tilde = 2
> sigma2_tilde = 1
>
> # Processo invert√≠vel
> theta = 1 / theta_tilde
> sigma2 = theta_tilde**2 * sigma2_tilde
>
> print(f"N√£o invert√≠vel: theta = {theta_tilde}, sigma2 = {sigma2_tilde}")
> print(f"Invert√≠vel: theta = {theta}, sigma2 = {sigma2}")
> ```

**Proposi√ß√£o 1.1** (Unicidade da Representa√ß√£o Invert√≠vel) Para um processo MA(1) com uma dada fun√ß√£o de autocovari√¢ncia, existe uma √∫nica representa√ß√£o invert√≠vel com $|\theta| < 1$.

*Prova:*
Suponha que existam duas representa√ß√µes invert√≠veis distintas, $\theta_1$ e $\theta_2$, ambas menores que 1 em valor absoluto e gerando a mesma fun√ß√£o de autocovari√¢ncia. Isso implicaria que $\sigma_1^2 (1 + \theta_1^2) = \sigma_2^2 (1 + \theta_2^2)$ e $\sigma_1^2 \theta_1 = \sigma_2^2 \theta_2$. Dividindo a segunda equa√ß√£o pela primeira, ter√≠amos $\frac{\theta_1}{1 + \theta_1^2} = \frac{\theta_2}{1 + \theta_2^2}$. Resolvendo essa equa√ß√£o, encontramos que ou $\theta_1 = \theta_2$ ou $\theta_1 = \frac{1}{\theta_2}$. Como ambas representa√ß√µes s√£o invert√≠veis, $|\theta_1| < 1$ e $|\theta_2| < 1$, a √∫nica solu√ß√£o poss√≠vel √© $\theta_1 = \theta_2$. Portanto, a representa√ß√£o invert√≠vel √© √∫nica.

I. Seja $Y_t = \epsilon_t + \theta_1 \epsilon_{t-1}$ e $Y_t = \epsilon_t + \theta_2 \epsilon_{t-1}$ duas representa√ß√µes invert√≠veis de um processo MA(1), onde $|\theta_1| < 1$ e $|\theta_2| < 1$.

II. As fun√ß√µes de autocovari√¢ncia para ambas as representa√ß√µes s√£o iguais:
    *   $\gamma(0) = \sigma_1^2 (1 + \theta_1^2) = \sigma_2^2 (1 + \theta_2^2)$
    *   $\gamma(1) = \sigma_1^2 \theta_1 = \sigma_2^2 \theta_2$

III. Dividindo a segunda equa√ß√£o pela primeira:
    $$\frac{\sigma_1^2 \theta_1}{\sigma_1^2 (1 + \theta_1^2)} = \frac{\sigma_2^2 \theta_2}{\sigma_2^2 (1 + \theta_2^2)}$$
    $$\frac{\theta_1}{1 + \theta_1^2} = \frac{\theta_2}{1 + \theta_2^2}$$

IV. Resolvendo para $\theta_1$ em termos de $\theta_2$:
    $$\theta_1 (1 + \theta_2^2) = \theta_2 (1 + \theta_1^2)$$
    $$\theta_1 + \theta_1 \theta_2^2 = \theta_2 + \theta_2 \theta_1^2$$
    $$\theta_1 - \theta_2 = \theta_2 \theta_1^2 - \theta_1 \theta_2^2$$
    $$\theta_1 - \theta_2 = \theta_1 \theta_2 (\theta_1 - \theta_2)$$

V. Assim, temos duas solu√ß√µes: $\theta_1 = \theta_2$ ou $\theta_1 \theta_2 = 1$. Dado que ambas as representa√ß√µes s√£o invert√≠veis, $|\theta_1| < 1$ e $|\theta_2| < 1$, a √∫nica solu√ß√£o poss√≠vel √© $\theta_1 = \theta_2$.

VI. Portanto, a representa√ß√£o invert√≠vel √© √∫nica. ‚ñ†

#### Processo MA(q) e Procedimento de Hansen-Sargent
Para processos MA(q) mais gerais, a altern√¢ncia entre representa√ß√µes invert√≠veis e n√£o invert√≠veis √© mais complexa. Conforme mencionado anteriormente [^67], a condi√ß√£o de invertibilidade requer que todas as ra√≠zes do polin√¥mio caracter√≠stico estejam fora do c√≠rculo unit√°rio. Se nem todas as ra√≠zes satisfazem essa condi√ß√£o, podemos aplicar o procedimento de Hansen-Sargent para encontrar uma representa√ß√£o invert√≠vel.

O procedimento de Hansen-Sargent envolve os seguintes passos [^67]:

1.  Encontre as ra√≠zes $z_i$ do polin√¥mio caracter√≠stico $\Theta(z) = 1 + \theta_1 z + \theta_2 z^2 + \ldots + \theta_q z^q$.
2.  Para cada raiz $z_i$ dentro do c√≠rculo unit√°rio (ou seja, $|z_i| < 1$), substitua a raiz por $\frac{1}{z_i^*}$, onde $z_i^*$ √© o conjugado complexo de $z_i$.
3.  Construa um novo polin√¥mio caracter√≠stico $\Theta'(z)$ com as ra√≠zes modificadas. Os coeficientes deste novo polin√¥mio correspondem aos par√¢metros de um processo MA(q) invert√≠vel.

> üí° **Exemplo Num√©rico:**
>
> Considere o processo MA(2): $Y_t = \epsilon_t + 0.5\epsilon_{t-1} - 0.6\epsilon_{t-2}$. Vimos anteriormente que as ra√≠zes do polin√¥mio caracter√≠stico $\Theta(z) = 1 + 0.5z - 0.6z^2$ s√£o $z_1 \approx 1.546$ e $z_2 \approx -0.713$. Como $|z_2| < 1$, o processo n√£o √© invert√≠vel.
>
> Para torn√°-lo invert√≠vel, substitu√≠mos $z_2$ por $\frac{1}{z_2^*}$, que √© $\frac{1}{-0.713} \approx -1.402$. Assim, a nova raiz √© $z_2' \approx -1.402$.
>
> O novo polin√¥mio caracter√≠stico √© dado por $\Theta'(z) = (z - 1.546)(z + 1.402) = z^2 - 0.144z - 2.167 = 0$. Multiplicando por -0.6 para que o termo constante seja igual a 1: $1 + 0.066z + 0.276z^2 = 0$
>
> Portanto, os novos coeficientes s√£o $\theta_1' = 0.066$ e $\theta_2' = 0.276$. O processo MA(2) invert√≠vel correspondente √© $Y_t = \epsilon_t + 0.066\epsilon_{t-1} + 0.276\epsilon_{t-2}$.  Note que este √© um MA(2) diferente, que ir√° gerar a mesma fun√ß√£o de autocovari√¢ncia.
>
> Vamos implementar o procedimento de Hansen-Sargent em Python:
>
> ```python
> import numpy as np
>
> def hansen_sargent_ma(theta):
>     """
>     Aplica o procedimento de Hansen-Sargent para encontrar uma representa√ß√£o MA(q) invert√≠vel.
>     """
>     # Constr√≥i o polin√¥mio caracter√≠stico
>     coeffs = np.concatenate(([1], theta))
>
>     # Encontra as ra√≠zes do polin√¥mio
>     roots = np.roots(coeffs)
>
>     # Inverte as ra√≠zes dentro do c√≠rculo unit√°rio
>     for i, root in enumerate(roots):
>         if np.abs(root) < 1:
>             roots[i] = 1 / np.conjugate(root)
>
>     # Reconstr√≥i o polin√¥mio a partir das ra√≠zes
>     new_coeffs = np.poly(roots)
>
>     # Normaliza o polin√¥mio
>     new_theta = new_coeffs[1:] / new_coeffs[0]
>     return new_theta.real
>
> # Exemplo de uso
> theta = np.array([0.5, -0.6]) # Processo MA(2) n√£o invert√≠vel
>
> new_theta = hansen_sargent_ma(theta)
>
> print(f"Coeficientes originais: {theta}")
> print(f"Coeficientes ap√≥s Hansen-Sargent: {new_theta}")
> ```
>
> ```
> Coeficientes originais: [ 0.5 -0.6]
> Coeficientes ap√≥s Hansen-Sargent: [ 0.06569343  0.27616384]
> ```
>
> Note que o algoritmo de Hansen-Sargent requer ra√≠zes e reconstru√ß√£o do polin√¥mio, que requerem c√°lculo num√©rico.  Se as ra√≠zes forem pr√≥ximas do c√≠rculo unit√°rio, esse processo pode ser dif√≠cil.

**Proposi√ß√£o 1 (Hansen-Sargent mant√©m fun√ß√£o de autocovari√¢ncia)** O processo MA(q) transformado pelo procedimento de Hansen-Sargent gera a mesma fun√ß√£o de autocovari√¢ncia do processo MA(q) original, embora os par√¢metros do modelo tenham sido alterados para garantir a invertibilidade.

*Prova (Esbo√ßo):*
A fun√ß√£o de autocovari√¢ncia depende dos momentos de primeira e segunda ordem do processo, os quais permanecem inalterados quando substitu√≠mos uma raiz $z_i$ por $\frac{1}{z_i^*}$. O novo processo garante a invertibilidade sem alterar a estrutura de depend√™ncia.

*Prova:*
I. Seja $Y_t = \epsilon_t + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q}$ um processo MA(q) com fun√ß√£o de autocovari√¢ncia $\gamma(j)$. Seja $\Theta(z) = 1 + \theta_1 z + \dots + \theta_q z^q$ o polin√¥mio caracter√≠stico.

II. Seja $z_1, \dots, z_q$ as ra√≠zes do polin√¥mio $\Theta(z)$. O procedimento de Hansen-Sargent substitui as ra√≠zes $z_i$ dentro do c√≠rculo unit√°rio por $\frac{1}{z_i^*}$. Seja $\Theta'(z)$ o novo polin√¥mio com as ra√≠zes modificadas $z_1', \dots, z_q'$.

III. A fun√ß√£o de autocovari√¢ncia $\gamma(j)$ do processo MA(q) pode ser expressa em termos dos coeficientes do polin√¥mio caracter√≠stico $\Theta(z)$. Especificamente, $\gamma(j) = \sigma^2 \sum_{i=0}^{q-j} \theta_i \theta_{i+j}$, onde $\theta_0 = 1$.

IV. Quando substitu√≠mos uma raiz $z_i$ por $\frac{1}{z_i^*}$, o espectro do processo, dado por $S(z) = \sigma^2 \Theta(z) \Theta(z^{-1})$, permanece inalterado. Isso ocorre porque se $|z_i| < 1$, ent√£o $|\frac{1}{z_i^*}| > 1$, e o produto $\Theta(z) \Theta(z^{-1})$ permanece o mesmo ap√≥s a substitui√ß√£o.

V. Uma vez que a fun√ß√£o de autocovari√¢ncia √© a transformada inversa de Fourier do espectro, e o espectro permanece inalterado, a fun√ß√£o de autocovari√¢ncia tamb√©m permanece inalterada.

VI. Portanto, o processo MA(q) transformado pelo procedimento de Hansen-Sargent gera a mesma fun√ß√£o de autocovari√¢ncia do processo MA(q) original. ‚ñ†

**Lema 1.1.** (Rela√ß√£o entre Ra√≠zes e Autocovari√¢ncia) A fun√ß√£o de autocovari√¢ncia de um processo MA(q) √© unicamente determinada pelas ra√≠zes do seu polin√¥mio caracter√≠stico.

*Prova (Esbo√ßo):* A fun√ß√£o de autocovari√¢ncia pode ser expressa em termos dos coeficientes do polin√¥mio caracter√≠stico. As ra√≠zes do polin√¥mio determinam univocamente os coeficientes (a menos de uma constante multiplicativa), e, portanto, determinam a fun√ß√£o de autocovari√¢ncia.

*Prova:*
I. Seja $Y_t = \epsilon_t + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q}$ um processo MA(q) com fun√ß√£o de autocovari√¢ncia $\gamma(j)$. Seja $\Theta(z) = 1 + \theta_1 z + \dots + \theta_q z^q$ o polin√¥mio caracter√≠stico.

II. A fun√ß√£o de autocovari√¢ncia $\gamma(j)$ √© dada por $\gamma(j) = E[Y_t Y_{t-j}]$. Expandindo, obtemos $\gamma(j) = \sigma^2 \sum_{i=0}^{q-j} \theta_i \theta_{i+j}$, onde $\theta_0 = 1$.

III. Os coeficientes $\theta_i$ do polin√¥mio caracter√≠stico $\Theta(z)$ s√£o unicamente determinados pelas ra√≠zes $z_1, \dots, z_q$ (a menos de uma constante multiplicativa). Podemos escrever $\Theta(z) = (1 - z_1 z)(1 - z_2 z)\dots(1 - z_q z)$.

IV. Portanto, as ra√≠zes $z_1, \dots, z_q$ determinam univocamente os coeficientes $\theta_i$, que por sua vez determinam univocamente a fun√ß√£o de autocovari√¢ncia $\gamma(j)$.

V. Conclu√≠mos que a fun√ß√£o de autocovari√¢ncia de um processo MA(q) √© unicamente determinada pelas ra√≠zes do seu polin√¥mio caracter√≠stico. ‚ñ†

#### Processos MA(1): Algoritmo Expl√≠cito

No caso de um MA(1), considere $Y_t = \mu + (1 + \theta L) \epsilon_t$. Se definirmos:
$\eta_t = Y_t - \mu$ e $E[\eta_t \eta_{t-j}] = \gamma(j)$, ent√£o:
$E[\eta_t \epsilon_t] = E[(e_t + \theta e_{t-1})\epsilon_t] = E[\epsilon_t^2] = \sigma^2$,
e $E[\eta_t \epsilon_{t-1}] = E[(\epsilon_t + \theta \epsilon_{t-1})\epsilon_{t-1}] = \theta E[\epsilon_{t-1}^2] = \theta \sigma^2$,
onde $\epsilon_t$ √© iid. Assim,
$E[Y_t^2] = \gamma(0) = \sigma^2(1+\theta^2)$ e
$E[Y_t Y_{t+1}] = \gamma(1) = \sigma^2\theta$.

Para garantir que $Var(Y_t)$ e $\gamma(1)$ n√£o mudem, devemos encontrar $\theta^* L$ tal que:

*   $\sigma^2(1+\theta^2) = \sigma^{*2}(1+\theta^{*2})$.
*   $\sigma^2\theta = \sigma^{*2}\theta^*$.

Definindo $\theta^* = \frac{1}{\theta}$:
$\sigma^{*2} = \frac{\sigma^2 \theta}{(1/\theta)} = \sigma^2 \theta^2$,
e $Var(Y_t)$ se torna $\theta^2\sigma^2 (1 + \frac{1}{\theta^2}) = \sigma^2(\theta^2 + 1)$.

Assim, para garantir que $\theta^* = \frac{1}{\theta}$, devemos garantir que $\sigma^{*2} = \theta^2 \sigma^2$.

**Corol√°rio 1.1.** (Impacto da Transforma√ß√£o na Vari√¢ncia) A transforma√ß√£o de um processo MA(1) n√£o invert√≠vel para sua representa√ß√£o invert√≠vel equivalente implica um ajuste na vari√¢ncia do ru√≠do branco, especificamente $\sigma^{*2} = \theta^2 \sigma^2$, onde $\theta$ √© o par√¢metro do processo MA(1) n√£o invert√≠vel.

**Teorema 1.2** (Equival√™ncia das Fun√ß√µes de Autocovari√¢ncia)
Seja $Y_t = \epsilon_t + \theta \epsilon_{t-1}$ um processo MA(1) n√£o invert√≠vel com $|\theta| > 1$ e vari√¢ncia do ru√≠do branco $\sigma^2$. Seja $Y_t^* = \epsilon_t^* + \frac{1}{\theta} \epsilon_{t-1}^*$ o processo MA(1) invert√≠vel equivalente com vari√¢ncia do ru√≠do branco $\sigma^{*2} = \theta^2 \sigma^2$. Ent√£o, as fun√ß√µes de autocovari√¢ncia de $Y_t$ e $Y_t^*$ s√£o id√™nticas.

*Prova:*
A fun√ß√£o de autocovari√¢ncia do processo MA(1) n√£o invert√≠vel √© dada por:
$\gamma(0) = E[Y_t^2] = \sigma^2 (1 + \theta^2)$
$\gamma(1) = E[Y_t Y_{t-1}] = \sigma^2 \theta$
$\gamma(j) = 0$ para $|j| > 1$

A fun√ß√£o de autocovari√¢ncia do processo MA(1) invert√≠vel √© dada por:
$\gamma^*(0) = E[Y_t^{*2}] = \sigma^{*2} (1 + (\frac{1}{\theta})^2) = \theta^2 \sigma^2 (1 + \frac{1}{\theta^2}) = \sigma^2 (\theta^2 + 1)$
$\gamma^*(1) = E[Y_t^* Y_{t-1}^*] = \sigma^{*2} (\frac{1}{\theta}) = \theta^2 \sigma^2 (\frac{1}{\theta}) = \sigma^2 \theta$
$\gamma^*(j) = 0$ para $|j| > 1$

Portanto, $\gamma(j) = \gamma^*(j)$ para todo $j$, demonstrando que as fun√ß√µes de autocovari√¢ncia s√£o id√™nticas.

I. Seja $Y_t = \epsilon_t + \theta \epsilon_{t-1}$ um processo MA(1) n√£o invert√≠vel com $|\theta| > 1$ e vari√¢ncia do ru√≠do branco $\sigma^2$.

II. A fun√ß√£o de autocovari√¢ncia do processo MA(1) n√£o invert√≠vel √©:
    *   $\gamma(0) = E[Y_t^2] = \sigma^2 (1 + \theta^2)$
    *   $\gamma(1) = E[Y_t Y_{t-1}] = \sigma^2 \theta$
    *   $\gamma(j) = 0$ para $|j| > 1$

III. Seja $Y_t^* = \epsilon_t^* + \theta^* \epsilon_{t-1}^*$ o processo MA(1) invert√≠vel equivalente com $|\theta^*| < 1$ e vari√¢ncia do ru√≠do branco $\sigma^{*2}$. Para que as fun√ß√µes de autocovari√¢ncia sejam id√™nticas, devemos ter:
    *   $\gamma^*(0) = \sigma^{*2} (1 + (\theta^*)^2) = \sigma^2 (1 + \theta^2)$
    *   $\gamma^*(1) = \sigma^{*2} \theta^* = \sigma^2 \theta$

IV. Definindo $\theta^* = \frac{1}{\theta}$, temos:
    $\sigma^{*2} \frac{1}{\theta} = \sigma^2 \theta$
    $\sigma^{*2} = \sigma^2 \theta^2$

V. Substituindo $\sigma^{*2}$ e $\theta^*$ nas equa√ß√µes de autocovari√¢ncia:
    *   $\gamma^*(0) = \sigma^2 \theta^2 (1 + (\frac{1}{\theta})^2) = \sigma^2 \theta^2 (1 + \frac{1}{\theta^2}) = \sigma^2 (\theta^2 + 1) = \gamma(0)$
    *   $\gamma^*(1) = \sigma^2 \theta^2 (\frac{1}{\theta}) = \sigma^2 \theta = \gamma(1)$

VI. Portanto, as fun√ß√µes de autocovari√¢ncia de $Y_t$ e $Y_t^*$ s√£o id√™nticas. ‚ñ†

### Algoritmos para Computar Inova√ß√µes Fundamentais

Para um processo invert√≠vel, a inova√ß√£o fundamental $\epsilon_t$ pode ser calculada recursivamente a partir de valores passados de $Y_t$. Para um processo MA(1) invert√≠vel, temos:

$$ Y_t = \mu + (1 + \theta L) \epsilon_t $$

Rearranjando a equa√ß√£o, obtemos:

$$ \epsilon_t = (Y_t - \mu) - \theta \epsilon_{t-1} $$

Esta equa√ß√£o permite calcular $\epsilon_t$ dado o valor atual de $Y_t$, a m√©dia $\mu$ e o valor anterior da inova√ß√£o $\epsilon_{t-1}$. Para iniciar o processo recursivo, geralmente assume-se que $\epsilon_0 = 0$.

> üí° **Exemplo Num√©rico:**
>
> Considere o processo MA(1) invert√≠vel: $Y_t = 5 + \epsilon_t + 0.8 \epsilon_{t-1}$, onde $\epsilon_t \sim N(0, 1)$.
>
> Suponha que observamos os seguintes valores de $Y_t$:
>
> | t   | $Y_t$ |
> | --- | ----- |
> | 1   | 6     |
> | 2   | 5.5   |
> | 3   | 4.8   |
>
> Para calcular as inova√ß√µes fundamentais:
>
> *   $\epsilon_0 = 0$ (assumido)
> *   $\epsilon_1 = (Y_1 - \mu) - \theta \epsilon_0 = (6 - 5) - 0.8 * 0 = 1$
> *   $\epsilon_2 = (Y_2 - \mu) - \theta \epsilon_1 = (5.5 - 5) - 0.8 * 1 = 0.5 - 0.8 = -0.3$
> *   $\epsilon_3 = (Y_3 - \mu) - \theta \epsilon_2 = (4.8 - 5) - 0.8 * (-0.3) = -0.2 + 0.24 = 0.04$
>
> ```python
> import numpy as np
>
> def calculate_innovations(y, mu, theta):
>     """
>     Calcula as inova√ß√µes fundamentais para um processo MA(1) invert√≠vel.
>     """
>     n = len(y)
>     epsilon = np.zeros(n)
>     epsilon[0] = 0  # Assume que a inova√ß√£o inicial √© zero
>
>     for t in range(1, n):
>         epsilon[t] = (y[t] - mu) - theta * epsilon[t-1]
>
>     return epsilon
>
> # Exemplo de uso
> y = np.array([6, 5.5, 4.8])
> mu = 5
> theta = 0.8
>
> innovations = calculate_innovations(y, mu, theta)
>
> print(f"S√©rie temporal: {y}")
> print(f"Inova√ß√µes fundamentais: {innovations}")
> ```
>
> ```
> S√©rie temporal: [6.  5.5 4.8]
> Inova√ß√µes fundamentais: [ 0.   1.  -0.3  0.04]
> ```

**Teorema 2** (Converg√™ncia das Inova√ß√µes) Para um processo MA(1) invert√≠vel, o efeito da condi√ß√£o inicial $\epsilon_0$ sobre as inova√ß√µes $\epsilon_t$ diminui exponencialmente √† medida que $t$ aumenta.

*Prova (Esbo√ßo):* A inova√ß√£o no tempo *t* √© dada por $\epsilon_t = (Y_t - \mu) - \theta \epsilon_{t-1}$. Expandindo recursivamente, podemos expressar $\epsilon_t$ como uma fun√ß√£o de $\epsilon_0$: $\epsilon_t = (Y_t - \mu) - \theta (Y_{t-1} - \mu) + \theta^2 (Y_{t-2} - \mu) - ... + (-1)^t \theta^t \epsilon_0$. Dado que o processo √© invert√≠vel, $|\theta| < 1$, o termo $(-1)^t \theta^t \epsilon_0$ converge para zero exponencialmente √† medida que *t* aumenta, demonstrando que o efeito da condi√ß√£o inicial se dissipa ao longo do tempo.

*Prova:*

I. Considere um processo MA(1) invert√≠vel: $Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$, onde $|\theta| < 1$.

II. A inova√ß√£o fundamental √© dada por: $\epsilon_t = (Y_t - \mu) - \theta \epsilon_{t-1}$.

III. Expandindo recursivamente a equa√ß√£o para $\epsilon_t$:
    *   $\epsilon_1 = (Y_1 - \mu) - \theta \epsilon_0$
    *   $\epsilon_2 = (Y_2 - \mu) - \theta \epsilon_1 = (Y_2 - \mu) - \theta [(Y_1 - \mu) - \theta \epsilon_0] = (Y_2 - \mu) - \theta (Y_1 - \mu) + \theta^2 \epsilon_0$
    *   $\epsilon_3 = (Y_3 - \mu) - \theta \epsilon_2 = (Y_3 - \mu) - \theta [(Y_2 - \mu) - \theta (Y_1 - \mu) + \theta^2 \epsilon_0] = (Y_3 - \mu) - \theta (Y_2 - \mu) + \theta^2 (Y_1 - \mu) - \theta^3 \epsilon_0$

IV. Generalizando para o tempo *t*:
    $$\epsilon_t = (Y_t - \mu) - \theta (Y_{t-1} - \mu) + \theta^2 (Y_{t-2} - \mu) - \dots + (-1)^{t-1} \theta^{t-1} (Y_1 - \mu) + (-1)^t \theta^t \epsilon_0$$
    $$\epsilon_t = \sum_{i=0}^{t-1} (-1)^i \theta^i (Y_{t-i} - \mu) + (-1)^t \theta^t \epsilon_0$$

V. Queremos analisar o efeito da condi√ß√£o inicial $\epsilon_0$ sobre $\epsilon_t$. O termo relevante √© $(-1)^t \theta^t \epsilon_0$. Dado que $|\theta| < 1$ (condi√ß√£o de invertibilidade), temos:
    $$\lim_{t \to \infty} |\theta^t| = 0$$

VI. Portanto, o termo $(-1)^t \theta^t \epsilon_0$ converge para zero exponencialmente √† medida que *t* aumenta, demonstrando que o efeito da condi√ß√£o inicial $\epsilon_0$ sobre as inova√ß√µes $\epsilon_t$ diminui exponencialmente √† medida que *t* aumenta. ‚ñ†

#### Processos MA(q): Forma de Espa√ßo de Estados

Para um processo MA(q), a forma de espa√ßo de estados pode ser utilizada para computar as inova√ß√µes fundamentais. Um processo MA(q) tem a seguinte representa√ß√£o:
$Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \cdots + \theta_q \epsilon_{t-q}$.

*Esbo√ßo:*
I. Considere um caso espec√≠fico: Se $q=2$:
$Y_t = \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2}$
II. Defina:
$X_t = \begin{bmatrix} \epsilon_t \\ \epsilon_{t-1} \end{bmatrix}$
III. Ent√£o $X_{t+1} = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} X_t + \begin{bmatrix} 1 \\ 0 \end{bmatrix} \omega_{t+1}$, e
$Y_{t+1} = \begin{bmatrix} 1 & \theta_1 \end{bmatrix} X_{t+1}$, onde a inova√ß√£o se torna a primeira entrada do vetor de estado.

A intui√ß√£o aqui √© que a representa√ß√£o em espa√ßo de estados permite a aplica√ß√£o do filtro de Kalman, onde as estima√ß√µes podem ser atualizadas dado que o modelo √© Gaussiano.

**Teorema 2.1** (Aplica√ß√£o do Filtro de Kalman) Dado um processo MA(q) representado na forma de espa√ßo de estados, o filtro de Kalman fornece as melhores estimativas lineares n√£o viesadas das inova√ß√µes fundamentais em cada per√≠odo.

*Prova (Esbo√ßo):*
A representa√ß√£o em espa√ßo de estados permite a aplica√ß√£o direta do filtro de Kalman. O filtro de Kalman √© um estimador recursivo que minimiza o erro quadr√°tico m√©dio da estimativa do estado, dado as observa√ß√µes passadas e presentes. No contexto de um processo MA(q), o estado inclui as inova√ß√µes passadas, e, portanto, o filtro de Kalman fornece as melhores estimativas das inova√ß√µes dado as informa√ß√µes dispon√≠veis.

*Prova:*

I. Considere um processo MA(q) dado por: $Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \cdots + \theta_q \epsilon_{t-q}$.

II. Representamos o processo na forma de espa√ßo de estados:
    *   Vetor de estado: $X_t = \begin{bmatrix} \epsilon_t \\ \epsilon_{t-1} \\ \vdots \\ \epsilon_{t-q+1} \end{bmatrix}$
    *   Equa√ß√£o de estado: $X_{t+1} = F X_t + G \epsilon_{t+1}$, onde
        $F = \begin{bmatrix} 0 & 0 & \cdots & 0 \\ 1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 0 \end{bmatrix}$ e $G = \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}$
    *   Equa√ß√£o de observa√ß√£o: $Y_t = H X_t + \mu$, onde $H = \begin{bmatrix} 1 & \theta_1 & \cdots & \theta_{q-1} \end{bmatrix}$

III. O filtro de Kalman √© um algoritmo recursivo que estima o estado $X_t$ dado as observa√ß√µes $Y_1, \dots, Y_t$. O filtro de Kalman fornece a melhor estimativa linear n√£o viesada do estado, denotada por $\hat{X}_{t|t} = E[X_t | Y_1, \dots, Y_t]$.

IV. No contexto do processo MA(q), a primeira componente do vetor de estado $X_t$ √© a inova√ß√£o $\epsilon_t$. Portanto, a primeira componente da estimativa $\hat{X}_{t|t}$ √© a melhor estimativa linear n√£o viesada da inova√ß√£o $\epsilon_t$ dado as observa√ß√µes $Y_1, \dots, Y_t$.

V. Portanto, o filtro de Kalman fornece as melhores estimativas lineares n√£o viesadas das inova√ß√µes fundamentais em cada per√≠odo. ‚ñ†

**Lema 2.2** (Observabilidade da Forma de Espa√ßo de Estados) A forma de espa√ßo de estados de um processo MA(q) invert√≠vel √© completamente observ√°vel.

*Prova (Esbo√ßo):* A observabilidade garante que o estado do sistema (neste caso, as inova√ß√µes passadas) pode ser inferido a partir das observa√ß√µes (os valores de $Y_t$). Para a forma de espa√ßo de estados de um MA(q), a matriz de observabilidade tem posto completo, o que implica que o sistema √© observ√°vel. A invertibilidade do processo MA(q) √© crucial para garantir que a rela√ß√£o entre as observa√ß√µes e as inova√ß√µes passadas seja bem definida e que a infer√™ncia seja poss√≠vel.

*Prova:*
I. Um sistema linear e invariante no tempo (LTI) na forma de espa√ßo de estados √© dado por:
    *   $X_{t+1} = F X_t + G \epsilon_{t+1}$
    *   $Y_t = H X_t + \mu$
II. A matriz de observabilidade √© definida como:
    $\mathcal{O} = \begin{bmatrix} H \\ HF \\ HF^2 \\ \vdots \\ HF^{n-1} \end{bmatrix}$, onde $n$ √© a dimens√£o do vetor de estado $X_t$.
III. O sistema √© completamente observ√°vel se e somente se a matriz de observabilidade $\mathcal{O}$ tem posto completo, ou seja, $\text{rank}(\mathcal{O}) = n$.

IV. Para o processo MA(q) na forma de espa√ßo de estados, temos:
    *   $X_t = \begin{bmatrix} \epsilon_t \\ \epsilon_{t-1} \\ \vdots \\ \epsilon_{t-q+1} \end{bmatrix}$
    *   $A = \begin{bmatrix} 0 & 0 & \cdots & 0 \\ 1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 0 \end{bmatrix}$
    *   $B = \begin{bmatrix} 1 \\ 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix}$
    *   $C = \begin{bmatrix} c_0 & c_1 & \cdots & c_{q-1} \end{bmatrix}$
    *   $D = [0]$

### Estima√ß√£o de Par√¢metros

A estima√ß√£o de par√¢metros em modelos de espa√ßo de estados pode ser realizada atrav√©s de diversas t√©cnicas, sendo o Filtro de Kalman e a Maximiza√ß√£o da Verossimilhan√ßa (MLE) os mais comuns.

#### Filtro de Kalman

O Filtro de Kalman √© um algoritmo recursivo que estima o estado de um sistema din√¢mico ao longo do tempo. Ele utiliza as equa√ß√µes de estado e de observa√ß√£o para atualizar a estimativa do estado a cada nova observa√ß√£o.

As equa√ß√µes principais do Filtro de Kalman s√£o:

1.  **Predi√ß√£o do estado:**
    *   $\hat{X}_{t|t-1} = A \hat{X}_{t-1|t-1} + B u_t$
2.  **Predi√ß√£o da covari√¢ncia do erro:**
    *   $P_{t|t-1} = A P_{t-1|t-1} A^T + Q$
3.  **C√°lculo do ganho de Kalman:**
    *   $K_t = P_{t|t-1} C^T (C P_{t|t-1} C^T + R)^{-1}$
4.  **Atualiza√ß√£o do estado:**
    *   $\hat{X}_{t|t} = \hat{X}_{t|t-1} + K_t (y_t - C \hat{X}_{t|t-1})$
5.  **Atualiza√ß√£o da covari√¢ncia do erro:**
    *   $P_{t|t} = (I - K_t C) P_{t|t-1}$

Onde:

*   $\hat{X}_{t|t-1}$ √© a estimativa do estado no tempo $t$ dado as informa√ß√µes at√© o tempo $t-1$.
*   $P_{t|t-1}$ √© a matriz de covari√¢ncia do erro de predi√ß√£o do estado.
*   $Q$ √© a matriz de covari√¢ncia do ru√≠do do processo.
*   $R$ √© a matriz de covari√¢ncia do ru√≠do da observa√ß√£o.
*   $K_t$ √© o ganho de Kalman.
*   $y_t$ √© a observa√ß√£o no tempo $t$.
*   $I$ √© a matriz identidade.

#### Maximiza√ß√£o da Verossimilhan√ßa (MLE)

A Maximiza√ß√£o da Verossimilhan√ßa (MLE) √© uma t√©cnica que busca encontrar os valores dos par√¢metros que maximizam a fun√ß√£o de verossimilhan√ßa dos dados observados. No contexto de modelos de espa√ßo de estados, a fun√ß√£o de verossimilhan√ßa pode ser expressa em termos das inova√ß√µes (res√≠duos) do Filtro de Kalman.

O processo envolve:

1.  Executar o Filtro de Kalman para obter as inova√ß√µes $v_t = y_t - C \hat{X}_{t|t-1}$ e suas vari√¢ncias $F_t = C P_{t|t-1} C^T + R$.
2.  Calcular a fun√ß√£o de log-verossimilhan√ßa:
    *   $\log L(\theta) = -\frac{T}{2} \log(2\pi) - \frac{1}{2} \sum_{t=1}^{T} (\log |F_t| + v_t^T F_t^{-1} v_t)$
3.  Maximizar a fun√ß√£o de log-verossimilhan√ßa em rela√ß√£o aos par√¢metros $\theta$ (e.g., usando algoritmos de otimiza√ß√£o num√©rica).

### Suaviza√ß√£o

A suaviza√ß√£o √© uma etapa adicional que pode ser aplicada ap√≥s a filtragem para obter estimativas mais precisas do estado. Existem diferentes tipos de suavizadores, sendo o suavizador de intervalo fixo de Rauch-Tung-Striebel (RTS) um dos mais utilizados.

#### Suavizador de Rauch-Tung-Striebel (RTS)

O suavizador RTS combina as estimativas do Filtro de Kalman com as informa√ß√µes futuras para refinar a estimativa do estado em cada ponto no tempo.

As equa√ß√µes principais do suavizador RTS s√£o:

1.  **Calcular o ganho de suaviza√ß√£o:**
    *   $A_t = P_{t|t} A^T P_{t+1|t}^{-1}$
2.  **Suavizar o estado:**
    *   $\hat{X}_{t|T} = \hat{X}_{t|t} + A_t (\hat{X}_{t+1|T} - \hat{X}_{t+1|t})$
3.  **Suavizar a covari√¢ncia:**
    *   $P_{t|T} = P_{t|t} + A_t (P_{t+1|T} - P_{t+1|t}) A_t^T$

Onde:

*   $\hat{X}_{t|T}$ √© a estimativa suavizada do estado no tempo $t$ dado todas as informa√ß√µes at√© o tempo $T$.
*   $P_{t|T}$ √© a matriz de covari√¢ncia suavizada do erro de predi√ß√£o do estado.

### Exemplo Pr√°tico em Python

Para ilustrar o uso de modelos de espa√ßo de estados, vamos considerar um exemplo simples de modelagem de uma s√©rie temporal com uma tend√™ncia linear e um componente sazonal.

```python
import numpy as np
import statsmodels.api as sm
from statsmodels.tsa.statespace.structural import UnobservedComponents
import matplotlib.pyplot as plt

# Gerar dados de exemplo
np.random.seed(0)
n = 100
trend = np.linspace(0, 10, n)
seasonal = np.sin(np.linspace(0, 4 * np.pi, n))
noise = np.random.normal(0, 1, n)
y = trend + seasonal + noise

# Definir o modelo de espa√ßo de estados
model = UnobservedComponents(y, 'rwalk', seasonal=4)
results = model.fit()

# Obter as estimativas do estado suavizado
smoothed = results.states.smoothed

# Plotar os resultados
plt.figure(figsize=(12, 6))
plt.plot(y, label='Observado')
plt.plot(smoothed[:, 0], label='Tend√™ncia Suavizada')
plt.plot(smoothed[:, 1], label='Sazonalidade Suavizada')
plt.legend()
plt.title('Modelo de Espa√ßo de Estados com Tend√™ncia e Sazonalidade')
plt.show()
```

Neste exemplo, utilizamos a biblioteca `statsmodels` para definir e ajustar um modelo de componentes n√£o observados (Unobserved Components). O modelo √© ajustado aos dados gerados e as estimativas do estado suavizado (tend√™ncia e sazonalidade) s√£o plotadas juntamente com os dados observados.

### Vantagens e Desvantagens

**Vantagens:**

*   Flexibilidade para modelar uma ampla gama de s√©ries temporais.
*   Capacidade de lidar com dados faltantes.
*   Interpreta√ß√£o intuitiva dos componentes do modelo.
*   Disponibilidade de algoritmos eficientes para estima√ß√£o e suaviza√ß√£o.

**Desvantagens:**

*   Requer um bom entendimento das equa√ß√µes de estado e de observa√ß√£o.
*   A especifica√ß√£o incorreta do modelo pode levar a resultados ruins.
*   Pode ser computacionalmente intensivo para grandes conjuntos de dados.

### Conclus√£o

Modelos de espa√ßo de estados s√£o uma ferramenta poderosa para modelar, estimar e prever s√©ries temporais. Sua flexibilidade e capacidade de lidar com diferentes tipos de dados os tornam uma escolha popular em diversas √°reas, incluindo economia, finan√ßas, engenharia e ci√™ncia ambiental. O uso do Filtro de Kalman, MLE e suavizadores permite obter estimativas precisas e insights valiosos sobre o comportamento das s√©ries temporais.

<!-- END -->