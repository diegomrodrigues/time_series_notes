### Ergodicity in Time Series Analysis

### Introdu√ß√£o

Dando continuidade aos conceitos de ergodicity e √† condi√ß√£o para ergodicity da m√©dia, este cap√≠tulo explorar√° as sutilezas da estacionaridade e da ergodicity, destacando como, embora distintas, muitas vezes levam aos mesmos requisitos em diversas aplica√ß√µes. No entanto, ressaltamos que um processo pode ser estacion√°rio sem ser necessariamente erg√≥dico, enfatizando a import√¢ncia de compreender ambos os conceitos para uma modelagem e an√°lise precisas de s√©ries temporais [^4].

### Distin√ß√£o entre Estacionaridade e Ergodicity

Como vimos anteriormente, a **estacionaridade** de uma s√©rie temporal refere-se √† sua propriedade de ter estat√≠sticas (como m√©dia e vari√¢ncia) que n√£o variam ao longo do tempo [^4]. Formalmente, um processo √© **covariance-stationary** se:

1.  A m√©dia $E(Y_t) = \mu$ √© constante para todo *t*.
2.  A autocovari√¢ncia $Cov(Y_t, Y_{t+j}) = \gamma_j$ depende apenas do lag *j* e n√£o de *t*.

Por outro lado, a **ergodicity** refere-se √† propriedade de que as m√©dias temporais de um processo convergem para as m√©dias do ensemble [^4]. Em termos pr√°ticos, significa que podemos usar uma √∫nica realiza√ß√£o longa da s√©rie temporal para estimar as estat√≠sticas da popula√ß√£o. Para que um processo **covariance-stationary** seja **ergodic para a m√©dia**, a autocovari√¢ncia $\gamma_j$ deve decair para zero suficientemente r√°pido √† medida que *j* aumenta, de acordo com a condi√ß√£o [^4]:

$$ \sum_{j=0}^{\infty} |\gamma_j| < \infty $$

A chave para distinguir entre estacionaridade e ergodicity reside no fato de que **estacionaridade √© uma propriedade das estat√≠sticas em um determinado ponto no tempo, enquanto ergodicity √© uma propriedade do comportamento a longo prazo do processo**. Um processo estacion√°rio pode ter estat√≠sticas constantes ao longo do tempo, mas ainda assim falhar em ser erg√≥dico se suas observa√ß√µes forem fortemente dependentes ao longo do tempo, de modo que uma √∫nica realiza√ß√£o n√£o seja representativa da distribui√ß√£o de probabilidade do processo.

> üí° **Exemplo Num√©rico:**
>
> Imagine duas m√°quinas produzindo rolamentos. A primeira m√°quina (estacion√°ria e erg√≥dica) produz rolamentos com um di√¢metro m√©dio de 5 mm e uma pequena varia√ß√£o. Se voc√™ medir o di√¢metro de uma amostra grande de rolamentos produzidos por essa m√°quina, a m√©dia amostral se aproximar√° de 5 mm, e essa aproxima√ß√£o melhorar√° quanto maior for a amostra.
>
> A segunda m√°quina (estacion√°ria, mas n√£o erg√≥dica) √© mais complexa. Ela √© configurada para produzir rolamentos com um di√¢metro m√©dio de 5 mm, mas, a cada dia, um engenheiro escolhe aleatoriamente uma pequena varia√ß√£o a ser adicionada a todos os rolamentos produzidos naquele dia. Portanto, o di√¢metro m√©dio di√°rio ainda √© centrado em 5 mm, mas em um determinado dia, todos os rolamentos ser√£o ligeiramente maiores ou menores que 5 mm. Medir uma grande amostra de rolamentos produzidos em um √∫nico dia n√£o fornecer√° uma estimativa precisa do di√¢metro m√©dio geral de 5 mm; em vez disso, ele fornecer√° uma estimativa do di√¢metro m√©dio para *aquele dia espec√≠fico*, que pode ser diferente de 5 mm. Embora a s√©rie temporal de di√¢metros de rolamentos seja estacion√°ria (o di√¢metro m√©dio √© constante ao longo do tempo), ela n√£o √© erg√≥dica porque as medi√ß√µes de um √∫nico dia n√£o s√£o representativas da distribui√ß√£o de probabilidade geral.

**Condi√ß√µes que implicam Estacionariedade e/ou Ergodicity:**

1. **Ru√≠do Branco:** Um processo de ru√≠do branco, definido como uma sequ√™ncia de vari√°veis aleat√≥rias n√£o correlacionadas com m√©dia zero e vari√¢ncia constante, √© tanto estacion√°rio quanto erg√≥dico.

   *   *Estacionariedade:* Por defini√ß√£o, as estat√≠sticas do ru√≠do branco n√£o variam ao longo do tempo.

   *   *Ergodicity:* A autocovari√¢ncia $\gamma_j = 0$ para $j \neq 0$, ent√£o $\sum_{j=0}^{\infty} |\gamma_j| = \gamma_0 < \infty$.

    **Prova:** Provaremos que um processo de ru√≠do branco √© erg√≥dico para a m√©dia.

    I. Seja $\{ \epsilon_t \}$ um processo de ru√≠do branco com $E[\epsilon_t] = 0$ e $Var[\epsilon_t] = \sigma^2$ para todo $t$.
    II. A autocovari√¢ncia $\gamma_j = Cov(\epsilon_t, \epsilon_{t+j}) = 0$ para $j \neq 0$, e $\gamma_0 = Cov(\epsilon_t, \epsilon_t) = \sigma^2$.
    III. Considere a condi√ß√£o para ergodicity para a m√©dia: $\sum_{j=0}^{\infty} |\gamma_j| < \infty$.
    IV. Para o ru√≠do branco, $\sum_{j=0}^{\infty} |\gamma_j| = |\gamma_0| + \sum_{j=1}^{\infty} |\gamma_j| = \sigma^2 + 0 = \sigma^2$.
    V. Como $\sigma^2$ √© uma constante finita, $\sum_{j=0}^{\infty} |\gamma_j| < \infty$.

    Portanto, um processo de ru√≠do branco √© erg√≥dico para a m√©dia. ‚ñ†
    > üí° **Exemplo Num√©rico:**
    >
    > Considere uma s√©rie temporal de ru√≠do branco $\{\epsilon_t\}$ com $\sigma^2 = 1$.
    > A m√©dia amostral ap√≥s 1000 observa√ß√µes ser√°:
    >
    > ```python
    > import numpy as np
    >
    > np.random.seed(0) # for reproducibility
    > epsilon = np.random.normal(0, 1, 1000)
    > sample_mean = np.mean(epsilon)
    > print(f"Sample Mean: {sample_mean}")
    > ```
    >
    > Devido √† ergodicidade, a m√©dia amostral se aproximar√° de 0 conforme o tamanho da amostra aumenta.
    >
    > Al√©m disso, a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ √© satisfeita:
    >
    > $\sum_{j=0}^{\infty} |\gamma_j| = |\gamma_0| = 1 < \infty$.
    >
    > Isso confirma que o processo √© erg√≥dico para a m√©dia.

2. **Processo MA(q):** Um processo de m√©dia m√≥vel de ordem *q* √© da forma $Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q}$, onde $\epsilon_t$ √© ru√≠do branco. Processos MA(q) s√£o sempre covariance-stationary. Se $\epsilon_t$ √© ru√≠do branco Gaussiano, eles tamb√©m s√£o erg√≥dicos para todos os momentos.

   *   *Estacionariedade:* A m√©dia e a autocovari√¢ncia de um processo MA(q) n√£o dependem do tempo.

   *   *Ergodicity:* A autocovari√¢ncia $\gamma_j = 0$ para $|j| > q$, ent√£o $\sum_{j=0}^{\infty} |\gamma_j|$ √© uma soma finita e, portanto, converge.

    **Prova:** Provaremos que um processo MA(q) √© erg√≥dico para a m√©dia.

    I. Seja $Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q}$ um processo MA(q), onde $\epsilon_t$ √© ru√≠do branco com $E[\epsilon_t] = 0$ e $Var[\epsilon_t] = \sigma^2$.
    II. A autocovari√¢ncia $\gamma_j = Cov(Y_t, Y_{t+j})$.  Para $|j| > q$, $\gamma_j = 0$ porque $Y_t$ e $Y_{t+j}$ n√£o compartilhar√£o nenhum termo $\epsilon$.
    III. Considere a condi√ß√£o para ergodicity para a m√©dia: $\sum_{j=0}^{\infty} |\gamma_j| < \infty$.
    IV. Para um processo MA(q), $\sum_{j=0}^{\infty} |\gamma_j| = \sum_{j=0}^{q} |\gamma_j|$, pois $\gamma_j = 0$ para $j > q$.
    V. Como $q$ √© finito, $\sum_{j=0}^{q} |\gamma_j|$ √© uma soma finita, e portanto converge para um valor finito.

    Portanto, um processo MA(q) √© erg√≥dico para a m√©dia. ‚ñ†
    > üí° **Exemplo Num√©rico:**
    >
    > Considere um processo MA(1) definido por $Y_t = \epsilon_t + 0.5\epsilon_{t-1}$, onde $\epsilon_t$ √© ru√≠do branco com vari√¢ncia 1.
    >
    > As autocovari√¢ncias s√£o:
    > - $\gamma_0 = Var(Y_t) = 1 + 0.5^2 = 1.25$
    > - $\gamma_1 = Cov(Y_t, Y_{t-1}) = 0.5$
    > - $\gamma_j = 0$ para $j > 1$
    >
    > A soma da autocovari√¢ncia absoluta √©:
    >
    > $\sum_{j=0}^{\infty} |\gamma_j| = |\gamma_0| + |\gamma_1| = 1.25 + 0.5 = 1.75 < \infty$.
    >
    > Portanto, o processo MA(1) √© erg√≥dico para a m√©dia.
    >
    > Para verificar isso numericamente, podemos simular o processo:
    >
    > ```python
    > import numpy as np
    >
    > np.random.seed(0)
    > epsilon = np.random.normal(0, 1, 1000)
    > Y = epsilon[1:] + 0.5 * epsilon[:-1]
    > sample_mean = np.mean(Y)
    > print(f"Sample Mean of MA(1): {sample_mean}")
    > ```
    >
    > A m√©dia amostral deve se aproximar da m√©dia populacional, que √© 0.

3. **Processo AR(p):** Um processo autorregressivo de ordem *p* √© da forma $Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \dots + \phi_p Y_{t-p} + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco. Processos AR(p) s√£o covariance-stationary e erg√≥dicos *se e somente se* as ra√≠zes do polin√¥mio caracter√≠stico associado estiverem fora do c√≠rculo unit√°rio.

   *   *Estacionariedade:* A estacionaridade de um processo AR(p) depende das restri√ß√µes sobre os coeficientes $\phi_i$. Especificamente, as ra√≠zes da equa√ß√£o caracter√≠stica $1 - \phi_1 z - \phi_2 z^2 - \dots - \phi_p z^p = 0$ devem estar fora do c√≠rculo unit√°rio.

   *   *Ergodicity:* Se o processo AR(p) √© estacion√°rio, ent√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$, e o processo √© erg√≥dico para a m√©dia. Para processos gaussianos, a estacionaridade implica ergodicity para todos os momentos.
    > üí° **Exemplo Num√©rico:**
    >
    > Considere um processo AR(1) definido por $Y_t = 0.7Y_{t-1} + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco com vari√¢ncia 1.
    >
    > A raiz da equa√ß√£o caracter√≠stica $1 - 0.7z = 0$ √© $z = \frac{1}{0.7} \approx 1.43$, que est√° fora do c√≠rculo unit√°rio, ent√£o o processo √© estacion√°rio.
    >
    > A vari√¢ncia do processo √© $\gamma_0 = \frac{1}{1 - 0.7^2} = \frac{1}{0.51} \approx 1.96$.
    > A autocovari√¢ncia $\gamma_j = \gamma_0 \cdot 0.7^j$.
    >
    > Verificando a condi√ß√£o de ergodicidade:
    >
    > $\sum_{j=0}^{\infty} |\gamma_j| = \sum_{j=0}^{\infty} |\gamma_0 \cdot 0.7^j| = \gamma_0 \sum_{j=0}^{\infty} 0.7^j = \gamma_0 \cdot \frac{1}{1 - 0.7} = \frac{1}{0.51} \cdot \frac{1}{0.3} \approx 6.54 < \infty$.
    >
    > Portanto, o processo AR(1) √© erg√≥dico para a m√©dia.
    >
    > Para simular e verificar:
    >
    > ```python
    > import numpy as np
    >
    > np.random.seed(0)
    > epsilon = np.random.normal(0, 1, 1001)
    > Y = np.zeros(1001)
    > for t in range(1, 1001):
    >     Y[t] = 0.7 * Y[t-1] + epsilon[t]
    >
    > sample_mean = np.mean(Y[500:]) # Discard initial values
    > print(f"Sample Mean of AR(1): {sample_mean}")
    > ```
    >
    > Se a raiz estivesse dentro do c√≠rculo unit√°rio (ex: 1.3), o processo seria n√£o estacion√°rio e n√£o erg√≥dico.

4. **Processo ARMA(p, q):** Um processo ARMA(p, q) combina as caracter√≠sticas dos processos AR(p) e MA(q), tendo a forma $Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \dots + \phi_p Y_{t-p} + \epsilon_t + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q}$. Processos ARMA(p, q) s√£o covariance-stationary e erg√≥dicos *se e somente se* a parte AR satisfizer as condi√ß√µes de estacionaridade (ra√≠zes do polin√¥mio caracter√≠stico fora do c√≠rculo unit√°rio).

   *   *Estacionariedade:* A estacionaridade depende das restri√ß√µes nos coeficientes $\phi_i$.

   *   *Ergodicity:* Se o processo ARMA(p, q) √© estacion√°rio, ent√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$, e o processo √© erg√≥dico para a m√©dia.
    > üí° **Exemplo Num√©rico:**
    >
    > Considere um processo ARMA(1,1) definido por $Y_t = 0.5Y_{t-1} + \epsilon_t + 0.3\epsilon_{t-1}$, onde $\epsilon_t$ √© ru√≠do branco com vari√¢ncia 1.
    >
    > A raiz da parte AR √© $z = \frac{1}{0.5} = 2$, que est√° fora do c√≠rculo unit√°rio, ent√£o o processo √© estacion√°rio.
    >
    > A estacionaridade implica a ergodicidade para a m√©dia, pois $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ para processos ARMA estacion√°rios.
    >
    > Simula√ß√£o em Python:
    >
    > ```python
    > import numpy as np
    >
    > np.random.seed(0)
    > epsilon = np.random.normal(0, 1, 1001)
    > Y = np.zeros(1001)
    > for t in range(1, 1001):
    >     Y[t] = 0.5 * Y[t-1] + epsilon[t] + 0.3 * epsilon[t-1]
    >
    > sample_mean = np.mean(Y[500:]) # Discard initial values
    > print(f"Sample Mean of ARMA(1,1): {sample_mean}")
    > ```

Para consolidar a compreens√£o das propriedades de estacionariedade e ergodicity, √© √∫til apresentar um resultado que relaciona a ergodicidade √† estacionariedade estrita, um conceito mais forte que covariance-stationarity.

**Teorema 1:** Se um processo estritamente estacion√°rio tem uma fun√ß√£o de autocovari√¢ncia que satisfaz $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, ent√£o o processo √© erg√≥dico para a m√©dia.

*Prova:* (Sketch) A estacionaridade estrita implica que todas as distribui√ß√µes conjuntas finito-dimensionais s√£o invariantes em rela√ß√£o ao tempo. A condi√ß√£o de autocovari√¢ncia garante que as observa√ß√µes no tempo distante s√£o assintoticamente independentes. Assim, a m√©dia da amostra converge para a m√©dia populacional.

Para complementar o Teorema 1, podemos derivar um corol√°rio que explora a rela√ß√£o entre estacionariedade estrita, ergodicidade e a converg√™ncia da m√©dia temporal.

**Corol√°rio 1.1:** Sob as condi√ß√µes do Teorema 1, a m√©dia temporal $\bar{Y}_T = \frac{1}{T} \sum_{t=1}^{T} Y_t$ converge em m√©dia quadr√°tica para a m√©dia populacional $\mu = E[Y_t]$.

*Prova:* (Sketch) Pelo Teorema 1, $\bar{Y}_T$ converge em probabilidade para $\mu$. Para mostrar a converg√™ncia em m√©dia quadr√°tica, precisamos mostrar que $E[(\bar{Y}_T - \mu)^2] \to 0$ quando $T \to \infty$. Usando a condi√ß√£o $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$ e a estacionaridade estrita, podemos demonstrar que a vari√¢ncia de $\bar{Y}_T$ tende a zero quando $T$ tende ao infinito, garantindo a converg√™ncia em m√©dia quadr√°tica.

Uma prova mais detalhada do Corol√°rio 1.1 √© a seguinte:

**Prova:**

I. Seja $\bar{Y}_T = \frac{1}{T} \sum_{t=1}^{T} Y_t$ a m√©dia amostral e $\mu = E[Y_t]$ a m√©dia populacional. Queremos mostrar que $E[(\bar{Y}_T - \mu)^2] \to 0$ quando $T \to \infty$.
II. Expandindo o termo quadr√°tico, temos: $E[(\bar{Y}_T - \mu)^2] = E\left[\left(\frac{1}{T} \sum_{t=1}^{T} Y_t - \mu\right)^2\right] = E\left[\left(\frac{1}{T} \sum_{t=1}^{T} (Y_t - \mu)\right)^2\right]$.
III. Expandindo o quadrado da soma, obtemos: $E[(\bar{Y}_T - \mu)^2] = \frac{1}{T^2} E\left[\sum_{t=1}^{T} \sum_{s=1}^{T} (Y_t - \mu)(Y_s - \mu)\right]$.
IV. Usando a defini√ß√£o de autocovari√¢ncia, $Cov(Y_t, Y_s) = E[(Y_t - \mu)(Y_s - \mu)] = \gamma_{|t-s|}$, temos: $E[(\bar{Y}_T - \mu)^2] = \frac{1}{T^2} \sum_{t=1}^{T} \sum_{s=1}^{T} \gamma_{|t-s|}$.
V. Agora, reescrevemos a soma dupla em termos de lags: $E[(\bar{Y}_T - \mu)^2] = \frac{1}{T^2} \sum_{j=-(T-1)}^{T-1} (T - |j|) \gamma_j = \frac{1}{T} \sum_{j=-(T-1)}^{T-1} \left(1 - \frac{|j|}{T}\right) \gamma_j$.
VI. Como $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, podemos aplicar o teorema da converg√™ncia dominada e tomar o limite quando $T \to \infty$: $\lim_{T \to \infty} E[(\bar{Y}_T - \mu)^2] = \lim_{T \to \infty} \frac{1}{T} \sum_{j=-(T-1)}^{T-1} \left(1 - \frac{|j|}{T}\right) \gamma_j = 0$.

Portanto, a m√©dia temporal $\bar{Y}_T$ converge em m√©dia quadr√°tica para a m√©dia populacional $\mu$. ‚ñ†

### Processos Estacion√°rios N√£o Ergodicos

Como mencionado, √© poss√≠vel que um processo seja estacion√°rio, mas n√£o erg√≥dico [^4]. O exemplo mais simples √© um processo determin√≠stico: $Y_t = C$, onde $C$ √© uma constante. Este processo √© estacion√°rio (m√©dia e vari√¢ncia constantes), mas n√£o √© erg√≥dico, pois uma √∫nica observa√ß√£o √© suficiente para determinar completamente o processo, e a m√©dia temporal n√£o converge para uma m√©dia de ensemble em um sentido probabil√≠stico.

Outro exemplo √© o caso detalhado no t√≥pico anterior, onde $Y_t^{(i)} = \mu^{(i)} + \epsilon_t$ [^4]. Neste caso, a m√©dia amostral converge para $\mu^{(i)}$, e n√£o para a m√©dia populacional $\mu$, violando a condi√ß√£o de ergodicity. Isso ocorre porque cada realiza√ß√£o do processo tem um n√≠vel m√©dio diferente, e uma √∫nica realiza√ß√£o n√£o √© representativa de todo o processo.

> üí° **Exemplo Num√©rico:** Considere um processo onde $Y_t = C$ e $C$ √© uma vari√°vel aleat√≥ria seguindo uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 1, ou seja, $C \sim N(0, 1)$. Para cada realiza√ß√£o do processo, $Y_t$ ser√° constante ao longo do tempo. Este processo √© estacion√°rio, pois para cada realiza√ß√£o, a m√©dia e a vari√¢ncia s√£o constantes. No entanto, n√£o √© erg√≥dico. Se observarmos uma √∫nica realiza√ß√£o por um longo per√≠odo, a m√©dia temporal simplesmente convergir√° para o valor espec√≠fico de $C$ para aquela realiza√ß√£o, ao inv√©s de convergir para a m√©dia populacional de 0.
>
> ```python
> import numpy as np
>
> np.random.seed(0)
> # Simula 5 realiza√ß√µes do processo
> num_realizations = 5
> C_values = np.random.normal(0, 1, num_realizations)
>
> # Cada realiza√ß√£o √© constante ao longo do tempo
> time_points = 100
> Y_realizations = np.zeros((num_realizations, time_points))
> for i in range(num_realizations):
>     Y_realizations[i, :] = C_values[i]
>
> # Calcula a m√©dia temporal para cada realiza√ß√£o
> temporal_means = np.mean(Y_realizations, axis=1)
>
> # Imprime as m√©dias temporais e a m√©dia populacional
> print("Temporal Means for each realization:", temporal_means)
> print("Population Mean (should be close to 0):", np.mean(C_values))
> ```
>
> Cada m√©dia temporal se aproxima do valor de C para aquela realiza√ß√£o espec√≠fica. A m√©dia das m√©dias temporais (populacional) se aproxima de 0, mas uma √∫nica s√©rie temporal n√£o consegue capturar essa informa√ß√£o.

Para expandir sobre o conceito de processos estacion√°rios n√£o erg√≥dicos, considere a seguinte proposi√ß√£o que estabelece uma condi√ß√£o para n√£o-ergodicidade.

**Proposi√ß√£o 2:** Seja $\{Y_t\}$ um processo estacion√°rio. Se $\lim_{j \to \infty} \gamma_j \neq 0$, ent√£o o processo n√£o √© erg√≥dico para a m√©dia.

*Prova:* Se o limite da autocovari√¢ncia quando $j$ tende ao infinito n√£o for zero, isso significa que as observa√ß√µes permanecem correlacionadas mesmo em grandes lags. Consequentemente, a m√©dia temporal n√£o converge para a m√©dia do ensemble, violando a defini√ß√£o de ergodicidade para a m√©dia.

A prova formal da Proposi√ß√£o 2 √© a seguinte:

**Prova:**

I. Assumimos que $\{Y_t\}$ √© um processo estacion√°rio e que $\lim_{j \to \infty} \gamma_j = L \neq 0$.
II. Se o processo fosse erg√≥dico para a m√©dia, ent√£o $\bar{Y}_T = \frac{1}{T} \sum_{t=1}^{T} Y_t$ convergiria em probabilidade para $E[Y_t] = \mu$.
III. Para que $\bar{Y}_T$ convirja para $\mu$, a vari√¢ncia de $\bar{Y}_T$ deve tender a 0 quando $T \to \infty$.
IV. A vari√¢ncia de $\bar{Y}_T$ √© dada por $Var(\bar{Y}_T) = \frac{1}{T^2} \sum_{t=1}^{T} \sum_{s=1}^{T} Cov(Y_t, Y_s) = \frac{1}{T^2} \sum_{t=1}^{T} \sum_{s=1}^{T} \gamma_{|t-s|}$.
V. Podemos reescrever a soma como $Var(\bar{Y}_T) = \frac{1}{T} \sum_{j=-(T-1)}^{T-1} \left(1 - \frac{|j|}{T}\right) \gamma_j$.
VI. Se $\lim_{j \to \infty} \gamma_j = L \neq 0$, ent√£o para $T$ grande, $\frac{1}{T} \sum_{j=-(T-1)}^{T-1} \left(1 - \frac{|j|}{T}\right) \gamma_j$ n√£o tender√° a 0, pois os termos $\gamma_j$ n√£o decaem suficientemente r√°pido.
VII. Portanto, $Var(\bar{Y}_T)$ n√£o tende a 0 quando $T \to \infty$, o que contradiz a necessidade de converg√™ncia em m√©dia quadr√°tica para que a ergodicidade para a m√©dia seja v√°lida.

Assim, se $\lim_{j \to \infty} \gamma_j \neq 0$, o processo n√£o √© erg√≥dico para a m√©dia. ‚ñ†

**Condi√ß√£o Adicional para Processos Gaussianos:**

Para um processo Gaussiano estacion√°rio, uma condi√ß√£o mais forte √© necess√°ria para garantir a ergodicidade: a fun√ß√£o de autocovari√¢ncia deve decair suficientemente r√°pido de forma que a soma dos seus valores absolutos seja finita, como mencionado anteriormente.

**Implica√ß√µes Pr√°ticas:**

A falha em reconhecer a n√£o-ergodicity pode levar a conclus√µes err√¥neas na modelagem e previs√£o de s√©ries temporais. Por exemplo, se usarmos dados de um processo estacion√°rio, mas n√£o erg√≥dico, para estimar os par√¢metros de um modelo, os resultados podem n√£o ser generaliz√°veis para outras realiza√ß√µes do processo. Da mesma forma, a previs√£o baseada em uma √∫nica realiza√ß√£o pode n√£o ser precisa a longo prazo.

> üí° **Exemplo Num√©rico:** Considere um processo onde cada realiza√ß√£o $Y_t^{(i)}$ √© constante ao longo do tempo, mas a constante varia aleatoriamente entre as realiza√ß√µes, de modo que $Y_t^{(i)} = \mu^{(i)}$ onde $\mu^{(i)} \sim N(0, 1)$. Este processo √© estacion√°rio porque a m√©dia e a vari√¢ncia de cada realiza√ß√£o s√£o constantes ao longo do tempo, mas n√£o √© erg√≥dico porque uma √∫nica realiza√ß√£o n√£o fornecer√° informa√ß√µes sobre a distribui√ß√£o de $\mu^{(i)}$. Se tentarmos prever o futuro deste processo com base em uma √∫nica realiza√ß√£o, obteremos resultados ruins porque a realiza√ß√£o n√£o √© representativa da distribui√ß√£o populacional.
>
> Para exemplificar, simulemos 5 realiza√ß√µes desse processo:
>
> ```python
> import numpy as np
>
> np.random.seed(0)
> num_realizations = 5
> time_points = 100
>
> # Simula m√©dias para cada realiza√ß√£o
> mu_values = np.random.normal(0, 1, num_realizations)
>
> # Cria as realiza√ß√µes do processo
> Y_realizations = np.zeros((num_realizations, time_points))
> for i in range(num_realizations):
>     Y_realizations[i, :] = mu_values[i]
>
> # Calcula as m√©dias amostrais para cada realiza√ß√£o
> sample_means = np.mean(Y_realizations, axis=1)
>
> # Imprime as m√©dias amostrais e a m√©dia da popula√ß√£o
> print("Sample Means for each realization:", sample_means)
> print("Population Mean:", np.mean(mu_values))
> ```
>
> Cada "Sample Mean" ser√° aproximadamente igual ao valor de $\mu^{(i)}$ para aquela realiza√ß√£o, e n√£o √† m√©dia populacional de 0. Isso demonstra que uma √∫nica realiza√ß√£o n√£o √© representativa da popula√ß√£o como um todo.

Para formalizar o exemplo num√©rico acima, considere o seguinte lema:

**Lema 2.1:** Seja $Y_t^{(i)} = \mu^{(i)}$, onde $\mu^{(i)}$ s√£o vari√°veis aleat√≥rias independentes e identicamente distribu√≠das com m√©dia $\mu$ e vari√¢ncia $\sigma^2$. Ent√£o, o processo $\{Y_t^{(i)}\}$ √© estacion√°rio, mas n√£o erg√≥dico.

*Prova:* (Sketch) A estacionaridade segue porque $E[Y_t^{(i)}] = \mu$ e $Var[Y_t^{(i)}] = \sigma^2$ para todo $t$. Para mostrar que n√£o √© erg√≥dico, observe que a m√©dia temporal de uma √∫nica realiza√ß√£o √© $\lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^{T} Y_t^{(i)} = \mu^{(i)}$, que converge para um valor diferente da m√©dia populacional $\mu$ com probabilidade 1, a menos que $\mu^{(i)} = \mu$ para todo $i$. Portanto, o processo n√£o √© erg√≥dico.

Uma prova mais detalhada do Lema 2.1 √© a seguinte:

**Prova:**

I. Seja $Y_t^{(i)} = \mu^{(i)}$, onde $\mu^{(i)}$ s√£o vari√°veis aleat√≥rias independentes e identicamente distribu√≠das (i.i.d.) com m√©dia $E[\mu^{(i)}] = \mu$ e vari√¢ncia $Var[\mu^{(i)}] = \sigma^2$.
II. Para mostrar a estacionaridade, precisamos provar que $E[Y_t^{(i)}]$ e $Cov(Y_t^{(i)}, Y_{t+j}^{(i)})$ s√£o independentes de $t$.
III. $E[Y_t^{(i)}] = E[\mu^{(i)}] = \mu$, que √© constante e independente de $t$.
IV. $Cov(Y_t^{(i)}, Y_{t+j}^{(i)}) = Cov(\mu^{(i)}, \mu^{(i)}) = 0$ para $j \neq 0$, pois $Y_t^{(i)}$ √© constante ao longo do tempo para uma dada realiza√ß√£o $i$. Para $j = 0$, $Cov(Y_t^{(i)}, Y_t^{(i)}) = Var(\mu^{(i)}) = \sigma^2$, que tamb√©m √© constante e independente de $t$.
V. Portanto, o processo $\{Y_t^{(i)}\}$ √© estacion√°rio.
VI. Para mostrar que o processo n√£o √© erg√≥dico, considere a m√©dia temporal $\bar{Y}_T^{(i)} = \frac{1}{T} \sum_{t=1}^{T} Y_t^{(i)} = \frac{1}{T} \sum_{t=1}^{T} \mu^{(i)} = \mu^{(i)}$.
VII. Tomando o limite quando $T \to \infty$, temos $\lim_{T \to \infty} \bar{Y}_T^{(i)} = \mu^{(i)}$.
VIII. Para que o processo seja erg√≥dico para a m√©dia, $\lim_{T \to \infty} \bar{Y}_T^{(i)}$ deve convergir para a m√©dia populacional $E[Y_t^{(i)}] = \mu$.
IX. No entanto, $\mu^{(i)}$ √© uma vari√°vel aleat√≥ria com m√©dia $\mu$ e vari√¢ncia $\sigma^2$, ent√£o $\mu^{(i)}$ n√£o √© necessariamente igual a $\mu$.
X. Portanto, o processo $\{Y_t^{(i)}\}$ n√£o √© erg√≥dico, pois a m√©dia temporal converge para $\mu^{(i)}$, que √© diferente da m√©dia populacional $\mu$ com probabilidade 1, a menos que $\sigma^2 = 0$.

Assim, o processo $\{Y_t^{(i)}\}$ √© estacion√°rio, mas n√£o erg√≥dico. ‚ñ†

### Condi√ß√£o Suficiente para Ergodicidade em Processos Gaussianos
Ampliando ainda mais a discuss√£o sobre a ergodidade em processos Gaussianos, vale a pena ressaltar uma condi√ß√£o suficiente essencial que garante a ergodidade para todos os momentos.

Seja $\{Y_t\}$ um processo estacion√°rio Gaussiano com fun√ß√£o de autocovari√¢ncia $\gamma(h)$. Para que o processo seja erg√≥dico para todos os momentos, √© suficiente que [^4]:

$$ \sum_{h=-\infty}^{\infty} |\gamma(h)| < \infty $$

Esta condi√ß√£o afirma que a soma absoluta de todos os valores de autocovari√¢ncia deve convergir. Essencialmente, garante que as depend√™ncias entre as vari√°veis de tempo suficientemente separadas diminuam rapidamente, permitindo que as m√©dias temporais capturem as caracter√≠sticas estat√≠sticas da popula√ß√£o.

> üí° **Exemplo Num√©rico:** Para ilustrar, considere um processo AR(1) Gaussiano definido por $Y_t = \phi Y_{t-1} + \epsilon_t$, onde $|\phi| < 1$ e $\epsilon_t$ √© ru√≠do branco Gaussiano. A fun√ß√£o de autocovari√¢ncia para este processo √© dada por:
>
> $$ \gamma(h) = \frac{\sigma^2}{1 - \phi^2} \phi^{|h|} $$
>
> onde $\sigma^2$ √© a vari√¢ncia de $\epsilon_t$. A soma da autocovari√¢ncia absoluta √©:
>
> $$ \sum_{h=-\infty}^{\infty} |\gamma(h)| = \frac{\sigma^2}{1 - \phi^2} \sum_{h=-\infty}^{\infty} |\phi|^{|h|} $$
>
> Como $|\phi| < 1$, a soma converge, satisfazendo a condi√ß√£o de ergodicidade. Portanto, um processo AR(1) Gaussiano que satisfaz a condi√ß√£o de estacionaridade tamb√©m √© erg√≥dico para todos os momentos.
>
> Se, por exemplo, $\phi = 0.5$ e $\sigma^2 = 1$, ent√£o:
>
> $\gamma(h) = \frac{1}{1 - 0.5^2} (0.5)^{|h|} = \frac{1}{0.75} (0.5)^{|h|} = \frac{4}{3} (0.5)^{|h|}$
>
> $\sum_{h=-\infty}^{\infty} |\gamma(h)| = \frac{4}{3} \sum_{h=-\infty}^{\infty} (0.5)^{|h|} = \frac{4}{3} [1 + 2\sum_{h=1}^{\infty} (0.5)^h] = \frac{4}{3} [1 +2(\frac{0.5}{1-0.5})] = \frac{4}{3} [1 + 2] = \frac{4}{3} \cdot 3 = 4$

Therefore, $\sum_{h=-\infty}^{\infty} |\gamma(h)| = 4 < \infty$.

Since the sum of the absolute values of the autocovariance function is finite, the AR(1) process with $\phi = 0.5$ is covariance stationary.

### Spectral Density of AR(1) Process

The spectral density $S(\omega)$ of a covariance-stationary process is the discrete-time Fourier transform of its autocovariance function $\gamma(h)$:

$S(\omega) = \sum_{h=-\infty}^{\infty} \gamma(h) e^{-i\omega h}$

For the AR(1) process $X_t = \phi X_{t-1} + W_t$ where $W_t \sim WN(0, \sigma_w^2)$, we have:

$\gamma(h) = \frac{\sigma_w^2 \phi^{|h|}}{1 - \phi^2}$

$S(\omega) = \sum_{h=-\infty}^{\infty} \frac{\sigma_w^2 \phi^{|h|}}{1 - \phi^2} e^{-i\omega h} = \frac{\sigma_w^2}{1 - \phi^2} \sum_{h=-\infty}^{\infty} \phi^{|h|} e^{-i\omega h}$

Now, let's simplify the summation:

$\sum_{h=-\infty}^{\infty} \phi^{|h|} e^{-i\omega h} = \sum_{h=-\infty}^{-1} \phi^{-h} e^{-i\omega h} + 1 + \sum_{h=1}^{\infty} \phi^{h} e^{-i\omega h}$

$= \sum_{h=1}^{\infty} \phi^{h} e^{i\omega h} + 1 + \sum_{h=1}^{\infty} \phi^{h} e^{-i\omega h}$

$= 1 + \sum_{h=1}^{\infty} (\phi e^{i\omega})^h + \sum_{h=1}^{\infty} (\phi e^{-i\omega})^h$

Since $\sum_{h=1}^{\infty} x^h = \frac{x}{1 - x}$ for $|x| < 1$,

$S(\omega) = \frac{\sigma_w^2}{1 - \phi^2} \left[ 1 + \frac{\phi e^{i\omega}}{1 - \phi e^{i\omega}} + \frac{\phi e^{-i\omega}}{1 - \phi e^{-i\omega}} \right]$

$= \frac{\sigma_w^2}{1 - \phi^2} \left[ \frac{(1 - \phi e^{i\omega})(1 - \phi e^{-i\omega}) + \phi e^{i\omega}(1 - \phi e^{-i\omega}) + \phi e^{-i\omega}(1 - \phi e^{i\omega})}{(1 - \phi e^{i\omega})(1 - \phi e^{-i\omega})} \right]$

$= \frac{\sigma_w^2}{1 - \phi^2} \left[ \frac{1 - \phi e^{-i\omega} - \phi e^{i\omega} + \phi^2 + \phi e^{i\omega} - \phi^2 + \phi e^{-i\omega} - \phi^2}{(1 - \phi e^{i\omega})(1 - \phi e^{-i\omega})} \right]$

$= \frac{\sigma_w^2}{1 - \phi^2} \left[ \frac{1 - \phi^2}{1 - \phi(e^{i\omega} + e^{-i\omega}) + \phi^2} \right]$

$= \frac{\sigma_w^2}{1 - \phi^2} \left[ \frac{1 - \phi^2}{1 - 2\phi \cos(\omega) + \phi^2} \right] = \frac{\sigma_w^2}{1 - 2\phi \cos(\omega) + \phi^2}$

Thus, $S(\omega) = \frac{\sigma_w^2}{1 - 2\phi \cos(\omega) + \phi^2}$.

This is the spectral density of an AR(1) process.

## Autoregressive Moving Average (ARMA) Models

ARMA models combine autoregressive (AR) and moving average (MA) components. An ARMA(p, q) model has $p$ autoregressive terms and $q$ moving average terms.

### Definition of ARMA(p, q) Model

An ARMA(p, q) model is defined as:

$X_t = \phi_1 X_{t-1} + \phi_2 X_{t-2} + \dots + \phi_p X_{t-p} + W_t + \theta_1 W_{t-1} + \theta_2 W_{t-2} + \dots + \theta_q W_{t-q}$

where:
- $X_t$ is the value of the time series at time $t$
- $\phi_1, \phi_2, \dots, \phi_p$ are the autoregressive parameters
- $\theta_1, \theta_2, \dots, \theta_q$ are the moving average parameters
- $W_t$ is the white noise error term at time $t$

### Stationarity and Invertibility of ARMA(p, q) Models

The ARMA(p, q) model is stationary if the roots of the characteristic equation of the AR part lie outside the unit circle. Similarly, the ARMA(p, q) model is invertible if the roots of the characteristic equation of the MA part lie outside the unit circle.

The characteristic equation for the AR part is:

$1 - \phi_1 z - \phi_2 z^2 - \dots - \phi_p z^p = 0$

And the characteristic equation for the MA part is:

$1 + \theta_1 z + \theta_2 z^2 + \dots + \theta_q z^q = 0$

### Example: ARMA(1, 1) Model

Consider an ARMA(1, 1) model:

$X_t = \phi X_{t-1} + W_t + \theta W_{t-1}$

For stationarity, $|\phi| < 1$.
For invertibility, $|\theta| < 1$.

### Autocovariance Function of ARMA(1, 1)

To find the autocovariance function $\gamma(h)$ of the ARMA(1, 1) model, we first find $\gamma(0)$, $\gamma(1)$, and then use the AR recursion for $h > 1$.

Multiply the ARMA(1, 1) equation by $X_{t-h}$ and take expectations:

$E[X_t X_{t-h}] = \phi E[X_{t-1} X_{t-h}] + E[W_t X_{t-h}] + \theta E[W_{t-1} X_{t-h}]$

$\gamma(h) = \phi \gamma(h-1) + E[W_t X_{t-h}] + \theta E[W_{t-1} X_{t-h}]$

For $h = 0$:

$\gamma(0) = \phi \gamma(1) + E[W_t X_t] + \theta E[W_{t-1} X_t]$

$\gamma(0) = \phi \gamma(1) + \sigma_w^2 + \theta E[W_{t-1} (\phi X_{t-1} + W_t + \theta W_{t-1})]$

$\gamma(0) = \phi \gamma(1) + \sigma_w^2 + \theta E[W_{t-1} (\phi X_{t-1} + W_t + \theta W_{t-1})]$
$\gamma(0) = \phi \gamma(1) + \sigma_w^2 + \theta E[W_{t-1} (\phi X_{t-1})] + \theta E[W_{t-1} W_t] + \theta^2 E[W_{t-1}^2]$
$\gamma(0) = \phi \gamma(1) + \sigma_w^2 + \theta^2 \sigma_w^2$

For $h = 1$:

$\gamma(1) = \phi \gamma(0) + E[W_t X_{t-1}] + \theta E[W_{t-1} X_{t-1}]$

$\gamma(1) = \phi \gamma(0) + 0 + \theta \sigma_w^2$

We have two equations:

1. $\gamma(0) = \phi \gamma(1) + \sigma_w^2 + \theta^2 \sigma_w^2$
2. $\gamma(1) = \phi \gamma(0) + \theta \sigma_w^2$

Substituting (2) into (1):

$\gamma(0) = \phi (\phi \gamma(0) + \theta \sigma_w^2) + \sigma_w^2 + \theta^2 \sigma_w^2$

$\gamma(0) = \phi^2 \gamma(0) + \phi \theta \sigma_w^2 + \sigma_w^2 + \theta^2 \sigma_w^2$

$\gamma(0) (1 - \phi^2) = \sigma_w^2 (1 + \theta^2 + \phi \theta)$

$\gamma(0) = \sigma_w^2 \frac{1 + \theta^2 + \phi \theta}{1 - \phi^2}$

Now, substitute $\gamma(0)$ back into equation (2):

$\gamma(1) = \phi \sigma_w^2 \frac{1 + \theta^2 + \phi \theta}{1 - \phi^2} + \theta \sigma_w^2$

$\gamma(1) = \sigma_w^2 \left( \frac{\phi + \phi \theta^2 + \phi^2 \theta + \theta - \phi^2 \theta}{1 - \phi^2} \right)$

$\gamma(1) = \sigma_w^2 \left( \frac{\phi + \theta + \phi \theta^2 - \phi^2 \theta + \theta - \phi^2 \theta}{1 - \phi^2} \right)$
$\gamma(1) = \sigma_w^2 \frac{\phi + \theta + \phi \theta^2}{1 - \phi^2}$

For $h > 1$,

$\gamma(h) = \phi \gamma(h-1)$

$\gamma(h) = \phi^{h-1} \gamma(1) = \phi^{h-1} \sigma_w^2 \frac{\phi + \theta + \phi \theta^2}{1 - \phi^2}$

### Spectral Density of ARMA(p, q)

The spectral density $S(\omega)$ of an ARMA(p, q) process can be written as:

$S(\omega) = \sigma_w^2 \frac{|1 + \sum_{j=1}^{q} \theta_j e^{-i\omega j}|^2}{|1 - \sum_{k=1}^{p} \phi_k e^{-i\omega k}|^2}$

<!-- END -->