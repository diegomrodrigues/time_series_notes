## Ergodicity in Time Series Analysis

### Introdu√ß√£o

Em continuidade ao t√≥pico anterior sobre ergodicity, exploraremos a condi√ß√£o sob a qual um processo √© erg√≥dico para a m√©dia. Como vimos, a ergodicity permite inferir estat√≠sticas populacionais a partir de uma √∫nica realiza√ß√£o suficientemente longa de um processo estacion√°rio. Agora, vamos analisar a condi√ß√£o espec√≠fica que garante que a autocovari√¢ncia de um processo convirja rapidamente para zero, permitindo que a m√©dia temporal convirja para a m√©dia do ensemble [^4]. Este conceito √© crucial para a aplica√ß√£o pr√°tica de modelos de s√©ries temporais.

### Condi√ß√£o de Ergodicidade para a M√©dia

Conforme mencionado anteriormente, um processo **covariance-stationary** √© considerado **ergodic para a m√©dia** se a m√©dia temporal $\bar{Y} = \frac{1}{T} \sum_{t=1}^{T} Y_t$ converge em probabilidade para a esperan√ßa matem√°tica $E(Y)$ quando $T$ tende ao infinito [^4]. A condi√ß√£o suficiente para que isso ocorra est√° relacionada ao decaimento da **autocovari√¢ncia** $\gamma_j$ √† medida que o lag *j* aumenta.

Especificamente, a condi√ß√£o para ergodicity para a m√©dia √© expressa como [^4]:

$$ \sum_{j=0}^{\infty} |\gamma_j| < \infty $$

Esta condi√ß√£o indica que a soma dos valores absolutos da autocovari√¢ncia deve ser finita. Em outras palavras, a depend√™ncia entre observa√ß√µes distantes no tempo deve diminuir rapidamente para que a m√©dia temporal seja uma estimativa consistente da m√©dia populacional.

> üí° **Exemplo Num√©rico:** Considere um processo onde a autocovari√¢ncia decai exponencialmente com o lag, ou seja, $\gamma_j = a \cdot r^{|j|}$, onde $0 < r < 1$ e $a$ √© uma constante. Neste caso, a soma dos valores absolutos da autocovari√¢ncia √©:
>
> $$ \sum_{j=-\infty}^{\infty} |\gamma_j| = a \sum_{j=-\infty}^{\infty} r^{|j|} = a \left( 1 + 2 \sum_{j=1}^{\infty} r^j \right) = a \left( 1 + 2 \cdot \frac{r}{1-r} \right) = a \left( \frac{1+r}{1-r} \right) $$
>
> Como $0 < r < 1$, esta soma converge para um valor finito, e o processo √© erg√≥dico para a m√©dia. Por exemplo, se $a = 2$ e $r = 0.5$, ent√£o $\sum_{j=-\infty}^{\infty} |\gamma_j| = 2 \cdot \frac{1+0.5}{1-0.5} = 2 \cdot \frac{1.5}{0.5} = 6$, que √© finito. Isso significa que para uma s√©rie temporal gerada por este processo, a m√©dia amostral convergir√° para a m√©dia populacional conforme o tamanho da amostra aumenta.

**Corol√°rio 1.** Se a fun√ß√£o de autocovari√¢ncia $\gamma_j$ √© zero para todo $j > K$, onde $K$ √© um inteiro finito, ent√£o o processo √© erg√≥dico para a m√©dia.

*Proof*. Se $\gamma_j = 0$ para todo $j > K$, ent√£o a soma $\sum_{j=0}^{\infty} |\gamma_j| = \sum_{j=0}^{K} |\gamma_j|$, que √© uma soma finita. Portanto, $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ e o processo √© erg√≥dico para a m√©dia. ‚ñ†

> üí° **Exemplo Num√©rico:**  Considere um processo MA(1) definido por $Y_t = \epsilon_t + \theta \epsilon_{t-1}$, onde $\epsilon_t$ √© ru√≠do branco. A fun√ß√£o de autocovari√¢ncia √© $\gamma_0 = \sigma^2(1+\theta^2)$, $\gamma_1 = \theta \sigma^2$, e $\gamma_j = 0$ para $|j| > 1$. Neste caso, a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ √© automaticamente satisfeita, e o processo √© erg√≥dico para a m√©dia. Por exemplo, se $\theta = 0.5$ e $\sigma^2 = 1$, ent√£o $\gamma_0 = 1(1 + 0.5^2) = 1.25$, $\gamma_1 = 0.5 * 1 = 0.5$, e $\gamma_j = 0$ para $|j| > 1$. A soma $\sum_{j=-\infty}^{\infty} |\gamma_j| = |\gamma_{-1}| + |\gamma_0| + |\gamma_1| = 0.5 + 1.25 + 0.5 = 2.25$, que √© finita. Portanto, o processo MA(1) com esses par√¢metros √© erg√≥dico para a m√©dia.

**Teorema 1.** Seja $\{Y_t\}$ um processo covariance-stationary com m√©dia $E[Y_t] = \mu$. Se $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, ent√£o
$$ \lim_{T \to \infty} Var(\bar{Y}) = 0 $$
onde $\bar{Y} = \frac{1}{T} \sum_{t=1}^{T} Y_t$.

*Proof*.
I. Definimos a vari√¢ncia da m√©dia amostral $\bar{Y}$ como:
    $$Var(\bar{Y}) = Var\left(\frac{1}{T} \sum_{t=1}^{T} Y_t\right)$$

II. Usando a propriedade da vari√¢ncia de uma constante multiplicada por uma vari√°vel aleat√≥ria, temos:
    $$Var(\bar{Y}) = \frac{1}{T^2} Var\left(\sum_{t=1}^{T} Y_t\right)$$

III. Expandindo a vari√¢ncia da soma, obtemos:
    $$Var(\bar{Y}) = \frac{1}{T^2} \sum_{t=1}^{T} \sum_{s=1}^{T} Cov(Y_t, Y_s)$$

IV. Expressando a covari√¢ncia em termos da fun√ß√£o de autocovari√¢ncia $\gamma$, temos:
    $$Var(\bar{Y}) = \frac{1}{T^2} \sum_{t=1}^{T} \sum_{s=1}^{T} \gamma_{|t-s|}$$

V. Reorganizando a dupla soma em termos do lag $j = t - s$, onde $-(T-1) \leq j \leq (T-1)$, podemos reescrever a soma como:
   $$Var(\bar{Y}) = \frac{1}{T^2} \sum_{j=-(T-1)}^{T-1} (T-|j|) \gamma_j$$
   Nota: Para um dado lag $j$, existem $T-|j|$ pares $(t, s)$ tais que $t-s=j$.

VI. Dividindo por $T$, temos:
    $$Var(\bar{Y}) = \frac{1}{T} \sum_{j=-(T-1)}^{T-1} \left(1 - \frac{|j|}{T}\right) \gamma_j$$

VII. Tomando o limite quando $T$ tende ao infinito e usando a condi√ß√£o $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, o termo $\frac{|j|}{T}$ tende a zero para cada $j$, e a soma converge para zero:
    $$\lim_{T \to \infty} Var(\bar{Y}) = \lim_{T \to \infty} \frac{1}{T} \sum_{j=-(T-1)}^{T-1} \left(1 - \frac{|j|}{T}\right) \gamma_j = 0$$
    Como $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, ent√£o $\lim_{T \to \infty} \frac{1}{T} \sum_{j=-(T-1)}^{T-1} \left(1 - \frac{|j|}{T}\right) \gamma_j = 0$. Portanto, $\lim_{T \to \infty} Var(\bar{Y}) = 0$. ‚ñ†

> üí° **Exemplo Num√©rico:**  Considerando um processo AR(1) com $Y_t = 0.7Y_{t-1} + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco com $\sigma^2 = 1$. A autocovari√¢ncia √© $\gamma_j = \frac{\sigma^2}{1-\phi^2} \phi^{|j|} = \frac{1}{1-0.7^2} (0.7)^{|j|} \approx 1.96 (0.7)^{|j|}$. Vamos calcular a vari√¢ncia da m√©dia amostral para $T = 100$ e $T = 1000$:
>
> Para $T = 100$:
>
> $$Var(\bar{Y}) = \frac{1}{100} \sum_{j=-99}^{99} \left(1 - \frac{|j|}{100}\right) \gamma_j \approx \frac{1}{100} \sum_{j=-99}^{99} \left(1 - \frac{|j|}{100}\right) 1.96 (0.7)^{|j|}$$
>
> $$Var(\bar{Y}) \approx 0.01 \cdot [1.96 + 2 \sum_{j=1}^{99} (1 - \frac{j}{100}) \cdot 1.96 (0.7)^j] \approx 0.078$$
>
> Para $T = 1000$:
>
> $$Var(\bar{Y}) = \frac{1}{1000} \sum_{j=-999}^{999} \left(1 - \frac{|j|}{1000}\right) \gamma_j \approx \frac{1}{1000} \sum_{j=-999}^{999} \left(1 - \frac{|j|}{1000}\right) 1.96 (0.7)^{|j|}$$
>
> $$Var(\bar{Y}) \approx 0.001 \cdot [1.96 + 2 \sum_{j=1}^{999} (1 - \frac{j}{1000}) \cdot 1.96 (0.7)^j] \approx 0.007$$
>
> Observamos que a vari√¢ncia da m√©dia amostral diminui √† medida que o tamanho da amostra $T$ aumenta, conforme previsto pelo teorema. Isso ilustra a converg√™ncia da m√©dia amostral para a m√©dia populacional.

**Teorema 1.1.** (Consequ√™ncia do Teorema 1) Seja $\{Y_t\}$ um processo covariance-stationary com m√©dia $E[Y_t] = \mu$. Se $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, ent√£o $\bar{Y}$ converge em probabilidade para $\mu$.

*Proof*.
I. Do Teorema 1, sabemos que $\lim_{T \to \infty} Var(\bar{Y}) = 0$.
II. Tamb√©m sabemos que $E[\bar{Y}] = \frac{1}{T} \sum_{t=1}^{T} E[Y_t] = \frac{1}{T} \sum_{t=1}^{T} \mu = \mu$.
III. Pela desigualdade de Chebyshev, para qualquer $\epsilon > 0$:
    $$P(|\bar{Y} - \mu| > \epsilon) \leq \frac{Var(\bar{Y})}{\epsilon^2}$$
IV. Tomando o limite quando $T$ tende ao infinito:
    $$\lim_{T \to \infty} P(|\bar{Y} - \mu| > \epsilon) \leq \lim_{T \to \infty} \frac{Var(\bar{Y})}{\epsilon^2}$$
V. Como $\lim_{T \to \infty} Var(\bar{Y}) = 0$:
    $$\lim_{T \to \infty} P(|\bar{Y} - \mu| > \epsilon) = 0$$
VI. Isso significa que $\bar{Y}$ converge em probabilidade para $\mu$. ‚ñ†

Al√©m disso, se considerarmos um processo **Gaussiano estacion√°rio**, a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ √© suficiente para garantir a ergodicity para todos os momentos [^4].

Formalmente, para um processo **Gaussiano estacion√°rio**, se $\sum_{j=0}^{\infty} |\gamma_j| < \infty$, ent√£o a amostra converge para a m√©dia populacional, a amostra converge para a vari√¢ncia populacional, e, de forma mais geral, amostrais convergentes para seus momentos populacionais correspondentes [^4]. Em outras palavras, se os momentos de dois processos Gaussianos estacion√°rios forem os mesmos, ent√£o a estrutura das fun√ß√µes de autocovari√¢ncia ser√° id√™ntica, e as propriedades estat√≠sticas derivadas tamb√©m ser√£o id√™nticas.

> üí° **Exemplo Num√©rico:** Seja $Y_t$ um processo Gaussiano estacion√°rio. Se $\sum_{j=0}^{\infty} |\gamma_j| < \infty$, ent√£o podemos estimar n√£o apenas a m√©dia e vari√¢ncia da s√©rie temporal de forma consistente a partir de uma √∫nica realiza√ß√£o, mas tamb√©m momentos de ordem superior, como assimetria e curtose. Isso significa que uma √∫nica s√©rie temporal longa cont√©m informa√ß√µes suficientes para caracterizar completamente a distribui√ß√£o do processo. Por exemplo, se uma s√©rie temporal de pre√ßos de a√ß√µes segue um processo Gaussiano estacion√°rio e satisfaz a condi√ß√£o de ergodicidade, podemos estimar a probabilidade de eventos raros (caudas da distribui√ß√£o) com base em dados hist√≥ricos.

√â importante notar que a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ n√£o √© necess√°ria para a ergodicidade na m√©dia, mas apenas suficiente. Existem processos que s√£o erg√≥dicos na m√©dia, mas n√£o satisfazem esta condi√ß√£o. No entanto, para muitos processos comuns em an√°lise de s√©ries temporais, como AR, MA e ARMA com par√¢metros apropriados, esta condi√ß√£o √© satisfeita, tornando-a uma ferramenta √∫til para verificar a ergodicidade.

[^4] apresenta um exemplo de um processo que √© estacion√°rio, mas n√£o erg√≥dico, onde $Y_t^{(i)} = \mu^{(i)} + \epsilon_t$, onde $\mu^{(i)}$ √© gerado a partir de uma distribui√ß√£o Normal $N(0, \lambda^2)$ e $\{\epsilon_t\}$ √© um processo de ru√≠do branco Gaussiano com m√©dia zero e vari√¢ncia $\sigma^2$. Neste caso, a m√©dia temporal converge para $\mu^{(i)}$, que √© um valor aleat√≥rio diferente de zero, e a autocovari√¢ncia √© $\gamma_j = \lambda^2$ para $j \neq 0$. Como $\sum_{j=0}^{\infty} |\gamma_j|$ diverge, este processo n√£o √© erg√≥dico para a m√©dia [^4].

Para complementar o exemplo de processo n√£o erg√≥dico apresentado, podemos formalizar a n√£o-ergodicidade atrav√©s da seguinte proposi√ß√£o:

**Proposi√ß√£o 1.** Seja $Y_t^{(i)} = \mu^{(i)} + \epsilon_t$, onde $\mu^{(i)}$ √© uma vari√°vel aleat√≥ria com m√©dia $E[\mu^{(i)}] = \mu$ e vari√¢ncia $Var(\mu^{(i)}) = \lambda^2$, e $\{\epsilon_t\}$ √© um processo de ru√≠do branco Gaussiano com m√©dia zero e vari√¢ncia $\sigma^2$, independente de $\mu^{(i)}$. Ent√£o, o processo $\{Y_t^{(i)}\}$ √© estacion√°rio, mas n√£o erg√≥dico para a m√©dia.

*Proof*. O processo √© estacion√°rio porque $E[Y_t^{(i)}] = E[\mu^{(i)}] + E[\epsilon_t] = \mu + 0 = \mu$ e $Cov(Y_t^{(i)}, Y_{t+h}^{(i)}) = Cov(\mu^{(i)} + \epsilon_t, \mu^{(i)} + \epsilon_{t+h}) = Cov(\mu^{(i)}, \mu^{(i)}) + Cov(\epsilon_t, \epsilon_{t+h}) = \lambda^2 + \delta_{0,h}\sigma^2$, onde $\delta_{0,h}$ √© o delta de Kronecker. Portanto, a m√©dia e a autocovari√¢ncia n√£o dependem de t.

Para mostrar que o processo n√£o √© erg√≥dico para a m√©dia, observe que a m√©dia temporal √© $\bar{Y}^{(i)} = \frac{1}{T} \sum_{t=1}^{T} Y_t^{(i)} = \mu^{(i)} + \frac{1}{T} \sum_{t=1}^{T} \epsilon_t$.  Quando $T \to \infty$, $\frac{1}{T} \sum_{t=1}^{T} \epsilon_t \to 0$ em probabilidade. Portanto, $\bar{Y}^{(i)} \to \mu^{(i)}$ em probabilidade.

Para que o processo fosse erg√≥dico para a m√©dia, $\bar{Y}^{(i)}$ deveria convergir em probabilidade para $E[Y_t^{(i)}] = \mu$. No entanto, $\bar{Y}^{(i)}$ converge para $\mu^{(i)}$, que √© uma vari√°vel aleat√≥ria com m√©dia $\mu$ e vari√¢ncia $\lambda^2$. Portanto, se $\lambda^2 > 0$, o processo n√£o √© erg√≥dico para a m√©dia. ‚ñ†

**Proposi√ß√£o 2.** Seja $\{Y_t\}$ um processo estacion√°rio com m√©dia $\mu$. Se o processo √© linearmente regular, isto √©, pode ser representado como $Y_t = \mu + \sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}$, onde $\sum_{i=0}^{\infty} |\psi_i| < \infty$ e $\{\epsilon_t\}$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, ent√£o o processo √© erg√≥dico para a m√©dia.

*Proof*.
I. A fun√ß√£o de autocovari√¢ncia para o processo linearmente regular √© dada por $\gamma_j = \sigma^2 \sum_{i=0}^{\infty} \psi_i \psi_{i+j}$.

II. Tomando a soma dos valores absolutos da autocovari√¢ncia, temos:
$$ \sum_{j=-\infty}^{\infty} |\gamma_j| = \sum_{j=-\infty}^{\infty} \left| \sigma^2 \sum_{i=0}^{\infty} \psi_i \psi_{i+|j|} \right| $$

III. Usando a desigualdade triangular, podemos limitar superiormente a soma:
$$ \sum_{j=-\infty}^{\infty} |\gamma_j| \leq \sigma^2 \sum_{j=-\infty}^{\infty} \sum_{i=0}^{\infty} |\psi_i| |\psi_{i+|j|}| $$

IV. Podemos reescrever a soma dupla trocando a ordem da soma e fazendo uma mudan√ßa de √≠ndice $k = i+j$ :
$$ \sum_{j=-\infty}^{\infty} |\gamma_j| \leq \sigma^2 \sum_{i=0}^{\infty} |\psi_i|  \sum_{k=-\infty}^{\infty} |\psi_{k}|$$

V. Como $\psi_i = 0$ para $i<0$,
$$ \sum_{j=-\infty}^{\infty} |\gamma_j| \leq \sigma^2 \sum_{i=0}^{\infty} |\psi_i|  \sum_{k=0}^{\infty} |\psi_{k}| = \sigma^2 \left( \sum_{i=0}^{\infty} |\psi_i| \right) \left( \sum_{k=0}^{\infty} |\psi_k| \right)$$

VI. Simplificando, temos:
$$ \sum_{j=-\infty}^{\infty} |\gamma_j| \leq \sigma^2 \left( \sum_{i=0}^{\infty} |\psi_i| \right)^2 $$

VII. Como $\sum_{i=0}^{\infty} |\psi_i| < \infty$, ent√£o $\sigma^2 \left( \sum_{i=0}^{\infty} |\psi_i| \right)^2 < \infty$.

VIII. Portanto, $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, e o processo √© erg√≥dico para a m√©dia. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere um processo linear regular com $\psi_0 = 1$, $\psi_1 = 0.5$, $\psi_2 = 0.25$, e $\psi_i = 0$ para $i > 2$, e $\sigma^2 = 1$. Ent√£o, $\sum_{i=0}^{\infty} |\psi_i| = 1 + 0.5 + 0.25 = 1.75 < \infty$. A autocovari√¢ncia √©:
>
> $\gamma_0 = \sigma^2 (\psi_0^2 + \psi_1^2 + \psi_2^2) = 1(1^2 + 0.5^2 + 0.25^2) = 1.3125$
>
> $\gamma_1 = \sigma^2 (\psi_0 \psi_1 + \psi_1 \psi_2) = 1(1 \cdot 0.5 + 0.5 \cdot 0.25) = 0.625$
>
> $\gamma_2 = \sigma^2 (\psi_0 \psi_2) = 1(1 \cdot 0.25) = 0.25$
>
> $\gamma_j = 0$ para $j > 2$.
>
> A soma dos valores absolutos da autocovari√¢ncia √© $\sum_{j=-\infty}^{\infty} |\gamma_j| = |\gamma_{-2}| + |\gamma_{-1}| + |\gamma_0| + |\gamma_1| + |\gamma_2| = 0.25 + 0.625 + 1.3125 + 0.625 + 0.25 = 3.0625 < \infty$. Portanto, o processo √© erg√≥dico para a m√©dia.

### Conclus√£o

A condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ fornece um crit√©rio pr√°tico para determinar se um processo covariance-stationary √© erg√≥dico para a m√©dia [^4]. Esta condi√ß√£o garante que a depend√™ncia entre observa√ß√µes distantes diminua rapidamente, permitindo que estat√≠sticas amostrais estimem com precis√£o as estat√≠sticas populacionais. Para processos Gaussianos estacion√°rios, esta condi√ß√£o tamb√©m implica ergodicity para todos os momentos, permitindo uma caracteriza√ß√£o completa do processo a partir de uma √∫nica realiza√ß√£o.

### Refer√™ncias
[^4]: P√°gina 47: "Whether time averages such as [3.1.14] eventually converge to the ensemble concept E(Y) for a stationary process has to do with ergodicity. A covariance-stationary process is said to be ergodic for the mean if [3.1.14] converges in probability to E(Y) as T‚Üí ‚àû. A process will be ergodic for the mean provided that the autocovariance Œ≥j goes to zero sufficiently quickly as j becomes large... For many applications, stationarity and ergodicity turn out to amount to the same requirements. For purposes of clarifying the concepts of stationarity and ergodicity, however, it may be helpful to consider an example of a process that is stationary but not ergodic. Suppose the mean Œº(i) for the ith realization {y(i)t}t=‚àí‚àû‚àû is generated from a N(0, Œª2) distribution, say Y(i)t=Œº(i)+œµt."
<!-- END -->