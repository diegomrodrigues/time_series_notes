## Ergodicity in Time Series Analysis

### Introdu√ß√£o
Expandindo sobre os conceitos de estacionariedade e autocovari√¢ncia, este cap√≠tulo aprofunda-se no conceito de **ergodicity** em an√°lise de s√©ries temporais. Como veremos, ergodicity permite que estat√≠sticas populacionais sejam inferidas a partir de uma √∫nica realiza√ß√£o suficientemente longa de um processo estacion√°rio, ligando as m√©dias temporais √†s m√©dias de ensemble. Este conceito √© fundamental para a aplica√ß√£o pr√°tica de modelos de s√©ries temporais, pois geralmente temos apenas uma √∫nica realiza√ß√£o dispon√≠vel para an√°lise [^4].

### Conceitos Fundamentais

A **ergodicity** em s√©ries temporais refere-se √† propriedade de um processo estacion√°rio onde as **m√©dias temporais** convergem para as **m√©dias de ensemble**. Em termos mais simples, se um processo √© erg√≥dico, podemos estimar as propriedades estat√≠sticas de toda a popula√ß√£o do processo observando uma √∫nica realiza√ß√£o suficientemente longa desse processo. Isso √© particularmente √∫til porque, na pr√°tica, geralmente temos apenas uma √∫nica realiza√ß√£o de uma s√©rie temporal dispon√≠vel para an√°lise.

Formalmente, considerando um processo **covariance-stationary**, ele √© dito **ergodic para a m√©dia** se a m√©dia temporal, definida como:

$$\bar{Y} = \frac{1}{T} \sum_{t=1}^{T} Y_t$$

converge em probabilidade para a esperan√ßa matem√°tica $E(Y)$ quando $T$ tende ao infinito [^4]. Ou seja:

$$ \text{plim}_{T\to\infty} \frac{1}{T} \sum_{t=1}^{T} Y_t = E(Y) $$

> üí° **Exemplo Num√©rico:** Suponha que temos uma s√©rie temporal de 100 observa√ß√µes geradas a partir de um processo estacion√°rio com m√©dia populacional $E(Y) = 5$. Se o processo for erg√≥dico para a m√©dia, ent√£o, √† medida que aumentamos o n√∫mero de observa√ß√µes que usamos para calcular a m√©dia amostral, essa m√©dia amostral deve se aproximar de 5. Por exemplo, ap√≥s 50 observa√ß√µes, a m√©dia amostral pode ser 4.8, e ap√≥s 100 observa√ß√µes, a m√©dia amostral pode ser 5.1. Se aumentarmos para 1000 observa√ß√µes, a m√©dia amostral deve estar ainda mais perto de 5.

[^1] apresenta a esperan√ßa matem√°tica $E(Y_t)$ como a m√©dia da distribui√ß√£o de probabilidade da *t*-√©sima observa√ß√£o de uma s√©rie temporal, dada por:

$$E(Y_t) = \int_{-\infty}^{\infty} y_t f_{Y_t}(y_t) dy_t$$

onde $f_{Y_t}(y_t)$ √© a densidade incondicional de $Y_t$. A esperan√ßa matem√°tica tamb√©m pode ser vista como o limite de probabilidade da m√©dia do ensemble:

$$E(Y_t) = \text{plim}_{I\to\infty} \frac{1}{I} \sum_{i=1}^{I} Y_t^{(i)}$$

onde $Y_t^{(i)}$ representa a *i*-√©sima realiza√ß√£o no instante *t*.

> üí° **Exemplo Num√©rico:** Imagine que estamos medindo a temperatura di√°ria em uma cidade. $Y_t^{(i)}$ representa a temperatura no dia *t* no *i*-√©simo ano. Se considerarmos 100 anos de dados (I = 100) e calcularmos a m√©dia da temperatura no dia 15 de janeiro ao longo desses 100 anos, essa m√©dia se aproxima de $E(Y_t)$ para o dia 15 de janeiro.

Uma condi√ß√£o suficiente para um processo **covariance-stationary** ser **ergodic para a m√©dia** √© que a autocovari√¢ncia $\gamma_j$ convirja para zero suficientemente r√°pido quando *j* se torna grande [^4]. Mais precisamente, a condi√ß√£o √©:

$$ \sum_{j=0}^{\infty} |\gamma_j| < \infty $$

> üí° **Exemplo Num√©rico:** Suponha que a autocovari√¢ncia de um processo seja dada por $\gamma_j = (0.5)^j$. Ent√£o, $\sum_{j=0}^{\infty} |\gamma_j| = \sum_{j=0}^{\infty} (0.5)^j = \frac{1}{1-0.5} = 2 < \infty$. Portanto, este processo satisfaz a condi√ß√£o suficiente para ergodicity na m√©dia.

Se esta condi√ß√£o for satisfeita, ent√£o podemos confiar que a m√©dia amostral calculada a partir de uma √∫nica realiza√ß√£o da s√©rie temporal fornecer√° uma estimativa consistente da m√©dia populacional [^4].

Para complementar a condi√ß√£o suficiente para ergodicity na m√©dia, podemos formular uma condi√ß√£o necess√°ria e suficiente em termos do espectro do processo.

**Teorema 1** Um processo estacion√°rio com fun√ß√£o de autocovari√¢ncia $\gamma_j$ √© erg√≥dico na m√©dia se e somente se o espectro de pot√™ncia $S(\omega)$ satisfaz

$$ S(0) = 0$$

*Proof*: (Opcional - para ilustrar a conex√£o entre autocovari√¢ncia e espectro). Pelo Teorema de Wiener-Khinchin, $S(\omega)$ √© a transformada de Fourier da fun√ß√£o de autocovari√¢ncia.  Assim, $S(\omega) = \frac{1}{2\pi} \sum_{j=-\infty}^{\infty} \gamma_j e^{-i\omega j}$.
I. Avaliando o espectro em $\omega = 0$:
   $$S(0) = \frac{1}{2\pi} \sum_{j=-\infty}^{\infty} \gamma_j e^{0} = \frac{1}{2\pi} \sum_{j=-\infty}^{\infty} \gamma_j$$
II. Para que o processo seja erg√≥dico na m√©dia, a soma das autocovari√¢ncias deve ser igual a zero:
   $$\sum_{j=-\infty}^{\infty} \gamma_j = 0$$
III. Portanto, a condi√ß√£o $S(0) = 0$ √© equivalente √† condi√ß√£o de ergodicidade. ‚ñ†

De forma similar, um processo covariance-stationary √© considerado **ergodic para segundos momentos** se:

$$ \text{plim}_{I\to\infty} \frac{1}{I} \sum_{i=1}^{I} [Y_t^{(i)} - \mu][Y_{t-j}^{(i)} - \mu] = \gamma_j $$

onde $\mu$ √© a m√©dia populacional. Uma condi√ß√£o suficiente para ergodicity de segundo momento ser√° apresentada no Cap√≠tulo 7 [^4].

> üí° **Exemplo Num√©rico:** Considere o exemplo da vari√¢ncia. Ergodicity para segundos momentos significa que, √† medida que aumentamos o n√∫mero de realiza√ß√µes *I*, a m√©dia amostral da vari√¢ncia (calculada em cada realiza√ß√£o) converge para a vari√¢ncia te√≥rica $\gamma_0$. Por exemplo, se $Y_t^{(i)}$ representa a taxa de retorno de uma a√ß√£o no dia *t* para a *i*-√©sima empresa do setor, a m√©dia das vari√¢ncias das taxas de retorno das a√ß√µes de todas as empresas deve convergir para a vari√¢ncia te√≥rica do setor.

Para clarificar ainda mais a ergodicidade de segunda ordem, podemos introduzir o conceito de ergodicidade para a fun√ß√£o de autocovari√¢ncia.

**Defini√ß√£o:** Um processo estacion√°rio √© dito erg√≥dico para a fun√ß√£o de autocovari√¢ncia se
$$ \text{plim}_{T\to\infty} \frac{1}{T} \sum_{t=1}^{T} (Y_t - \mu)(Y_{t+h} - \mu) = \gamma_h$$
para todo $h$.

**Lema 1** Se um processo √© erg√≥dico para a fun√ß√£o de autocovari√¢ncia, ent√£o ele √© erg√≥dico para a m√©dia.

*Proof*: Seja $h=0$. Ent√£o,
$$ \text{plim}_{T\to\infty} \frac{1}{T} \sum_{t=1}^{T} (Y_t - \mu)^2 = \gamma_0 = Var(Y_t)$$
Se a vari√¢ncia amostral converge para a vari√¢ncia te√≥rica, ent√£o a m√©dia amostral converge para a m√©dia te√≥rica.

I.  A ergodicidade para a fun√ß√£o de autocovari√¢ncia implica que a autocovari√¢ncia amostral converge em probabilidade para a autocovari√¢ncia te√≥rica, isto √©,
    $$\text{plim}_{T\to\infty} \frac{1}{T} \sum_{t=1}^{T} (Y_t - \mu)(Y_{t+h} - \mu) = \gamma_h$$
II.  Em particular, para $h=0$, temos
    $$\text{plim}_{T\to\infty} \frac{1}{T} \sum_{t=1}^{T} (Y_t - \mu)^2 = \gamma_0 = Var(Y_t)$$
III. Definindo $\bar{Y} = \frac{1}{T} \sum_{t=1}^{T} Y_t$, podemos escrever
    $$Var(\bar{Y}) = Var\left(\frac{1}{T} \sum_{t=1}^{T} Y_t\right) = \frac{1}{T^2} Var\left(\sum_{t=1}^{T} Y_t\right) = \frac{1}{T^2} \sum_{t=1}^{T} \sum_{s=1}^{T} Cov(Y_t, Y_s)$$
IV. Assumindo que $Y_t$ √© estacion√°rio, $Cov(Y_t, Y_s) = \gamma_{|t-s|}$. Portanto,
    $$Var(\bar{Y}) = \frac{1}{T^2} \sum_{t=1}^{T} \sum_{s=1}^{T} \gamma_{|t-s|} = \frac{1}{T} \sum_{h=-(T-1)}^{T-1} \left(1 - \frac{|h|}{T}\right) \gamma_h$$
V. Se $\sum_{h=-\infty}^{\infty} |\gamma_h| < \infty$, ent√£o $Var(\bar{Y}) \to 0$ quando $T \to \infty$.
VI. Pela desigualdade de Chebyshev, para qualquer $\epsilon > 0$,
    $$P(|\bar{Y} - \mu| > \epsilon) \leq \frac{Var(\bar{Y})}{\epsilon^2}$$
VII. Como $Var(\bar{Y}) \to 0$, $P(|\bar{Y} - \mu| > \epsilon) \to 0$.
VIII. Portanto, $\text{plim}_{T\to\infty} \bar{Y} = \mu$, o que significa que o processo √© erg√≥dico para a m√©dia. ‚ñ†

**Lema 1.1.** Um processo erg√≥dico para a fun√ß√£o de autocovari√¢ncia √© tamb√©m erg√≥dico para todos os momentos finitos.

*Proof*: (Esbo√ßo) A ergodicidade para a fun√ß√£o de autocovari√¢ncia garante que os segundos momentos amostrais convergem para os seus valores populacionais. Podemos estender este racioc√≠nio para momentos de ordem superior, mostrando que os momentos amostrais de ordem *k* convergem para os momentos populacionais correspondentes √† medida que *T* tende para infinito. Isso requer que as cumulantes conjuntas convirjam para zero suficientemente r√°pido.

**Exemplo de Processo Estacion√°rio N√£o Erg√≥dico:**

Para ilustrar a import√¢ncia da ergodicity, considere o exemplo de um processo estacion√°rio, mas n√£o erg√≥dico [^4]. Suponha que temos a seguinte representa√ß√£o para a *i*-√©sima realiza√ß√£o:

$$Y_t^{(i)} = \mu^{(i)} + \epsilon_t$$

onde $\mu^{(i)}$ √© gerado a partir de uma distribui√ß√£o Normal $N(0, \lambda^2)$ e $\{\epsilon_t\}$ √© um processo de ru√≠do branco Gaussiano com m√©dia zero e vari√¢ncia $\sigma^2$ [^4], independente de $\mu^{(i)}$.

A m√©dia deste processo √©:

$$E(Y_t) = E(\mu^{(i)}) + E(\epsilon_t) = 0$$

A autocovari√¢ncia √©:

$$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = \begin{cases} \lambda^2 + \sigma^2, & j = 0 \\ \lambda^2, & j \neq 0 \end{cases}$$

Este processo √© covariance-stationary, pois a m√©dia e a autocovari√¢ncia n√£o dependem do tempo *t*. No entanto, n√£o satisfaz a condi√ß√£o suficiente para ergodicity para a m√©dia [^4]:

$$\sum_{j=0}^{\infty} |\gamma_j| = \lambda^2 + \sigma^2 + \sum_{j=1}^{\infty} \lambda^2$$

que diverge para infinito.

De fato, a m√©dia temporal deste processo converge para $\mu^{(i)}$ e n√£o para a m√©dia populacional de $Y_t$, que √© zero [^4]:

$$\frac{1}{T} \sum_{t=1}^{T} Y_t^{(i)} = \frac{1}{T} \sum_{t=1}^{T} (\mu^{(i)} + \epsilon_t) = \mu^{(i)} + \frac{1}{T} \sum_{t=1}^{T} \epsilon_t$$

Quando $T$ tende para infinito, a m√©dia amostral converge para $\mu^{(i)}$, que √© um valor aleat√≥rio diferente de zero. Portanto, este processo n√£o √© erg√≥dico para a m√©dia [^4].

> üí° **Exemplo Num√©rico:** Vamos simular este processo. Seja $\lambda = 2$ e $\sigma = 1$. Geramos 100 realiza√ß√µes de $\mu^{(i)}$ a partir de $N(0, 2^2)$ e 1000 pontos de dados para cada realiza√ß√£o de $\epsilon_t$ a partir de $N(0, 1^2)$. Calcularemos a m√©dia temporal para cada realiza√ß√£o e observaremos que ela se aproxima de $\mu^{(i)}$, n√£o de 0.
> ```python
> import numpy as np
>
> # Par√¢metros
> lambda_val = 2
> sigma = 1
> I = 100  # N√∫mero de realiza√ß√µes
> T = 1000 # N√∫mero de pontos de dados por realiza√ß√£o
>
> # Gerar realiza√ß√µes de mu(i)
> mu = np.random.normal(0, lambda_val**2, I)
>
> # Inicializar um array para armazenar as m√©dias temporais
> temporal_means = np.zeros(I)
>
> # Para cada realiza√ß√£o
> for i in range(I):
>     # Gerar ru√≠do branco
>     epsilon = np.random.normal(0, sigma**2, T)
>
>     # Gerar a s√©rie temporal
>     Y = mu[i] + epsilon
>
>     # Calcular a m√©dia temporal
>     temporal_means[i] = np.mean(Y)
>
> # Imprimir as primeiras 10 m√©dias temporais e as correspondentes mu(i)
> print("Primeiras 10 m√©dias temporais:", temporal_means[:10])
> print("Primeiros 10 mu(i):", mu[:10])
>
> # Calcular a m√©dia das m√©dias temporais
> mean_of_temporal_means = np.mean(temporal_means)
> print("M√©dia das m√©dias temporais:", mean_of_temporal_means)
> ```
> Podemos observar que as m√©dias temporais s√£o diferentes de zero e variam de acordo com $\mu^{(i)}$. A m√©dia das m√©dias temporais estar√° pr√≥xima de 0, mas cada m√©dia temporal individual ser√° significativamente diferente de 0. Isso demonstra que o processo n√£o √© erg√≥dico.

Para complementar este exemplo, podemos fornecer outro exemplo de um processo que √© erg√≥dico.

**Exemplo de Processo Estacion√°rio e Erg√≥dico:**

Considere um processo AR(1) dado por
$$Y_t = \phi Y_{t-1} + \epsilon_t$$
onde $|\phi| < 1$ e $\epsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$. Este processo √© estacion√°rio. A fun√ß√£o de autocovari√¢ncia para este processo √© dada por
$$\gamma_j = \frac{\sigma^2 \phi^{|j|}}{1-\phi^2}$$
Verificando a condi√ß√£o de ergodicidade:
$$\sum_{j=-\infty}^{\infty} |\gamma_j| = \sum_{j=-\infty}^{\infty} \frac{\sigma^2 |\phi|^{|j|}}{1-\phi^2} = \frac{\sigma^2}{1-\phi^2} \sum_{j=-\infty}^{\infty} |\phi|^{|j|} = \frac{\sigma^2}{1-\phi^2} \left( 1 + 2\sum_{j=1}^{\infty} |\phi|^j \right) = \frac{\sigma^2}{1-\phi^2} \left( 1 + \frac{2|\phi|}{1-|\phi|} \right)$$
Como $|\phi| < 1$, esta soma converge para um valor finito. Portanto, o processo AR(1) √© erg√≥dico para a m√©dia.

> üí° **Exemplo Num√©rico:** Vamos considerar um processo AR(1) com $\phi = 0.5$ e $\sigma^2 = 1$. Ent√£o, $\gamma_j = \frac{(0.5)^{|j|}}{1 - (0.5)^2} = \frac{4}{3}(0.5)^{|j|}$.  Vamos calcular a soma das autocovari√¢ncias:
>
> $\sum_{j=-\infty}^{\infty} |\gamma_j| = \frac{4}{3} \sum_{j=-\infty}^{\infty} (0.5)^{|j|} = \frac{4}{3} \left( 1 + 2 \sum_{j=1}^{\infty} (0.5)^j \right) = \frac{4}{3} \left( 1 + 2 \cdot \frac{0.5}{1 - 0.5} \right) = \frac{4}{3} (1 + 2) = 4 < \infty$.
>
> Como a soma √© finita, o processo AR(1) √© erg√≥dico. Agora, vamos simular o processo e verificar se a m√©dia amostral converge para a m√©dia populacional (que √© 0 neste caso).
>
> ```python
> import numpy as np
>
> # Par√¢metros
> phi = 0.5
> sigma = 1
> T = 1000  # N√∫mero de pontos de dados
>
> # Gerar ru√≠do branco
> epsilon = np.random.normal(0, sigma**2, T)
>
> # Inicializar a s√©rie temporal
> Y = np.zeros(T)
>
> # Gerar o processo AR(1)
> for t in range(1, T):
>     Y[t] = phi * Y[t-1] + epsilon[t]
>
> # Calcular a m√©dia amostral
> sample_mean = np.mean(Y)
>
> print("M√©dia amostral:", sample_mean)
> ```
> Executando este c√≥digo, podemos observar que a m√©dia amostral est√° pr√≥xima de 0, o que √© consistente com a ergodicidade.

**Proposi√ß√£o 1.** Se $Y_t$ √© um processo Gaussiano estacion√°rio, ent√£o a ergodicidade na m√©dia implica a ergodicidade para todos os momentos.

*Proof*: Para um processo Gaussiano, todos os momentos podem ser expressos em termos da m√©dia e da fun√ß√£o de autocovari√¢ncia. Se o processo √© erg√≥dico na m√©dia, a m√©dia amostral converge para a m√©dia populacional. Se a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty $ √© satisfeita, ent√£o a fun√ß√£o de autocovari√¢ncia amostral tamb√©m converge para a fun√ß√£o de autocovari√¢ncia te√≥rica. Portanto, todos os momentos amostrais convergem para os seus valores populacionais, implicando ergodicidade para todos os momentos.

I.  Seja $Y_t$ um processo Gaussiano estacion√°rio.
II. Pela defini√ß√£o de um processo Gaussiano, a sua distribui√ß√£o conjunta √© completamente determinada pela sua m√©dia e fun√ß√£o de autocovari√¢ncia.
III. Se o processo √© erg√≥dico na m√©dia, ent√£o
    $$\text{plim}_{T\to\infty} \frac{1}{T} \sum_{t=1}^{T} Y_t = E[Y_t] = \mu$$
IV. Como $Y_t$ √© estacion√°rio, a fun√ß√£o de autocovari√¢ncia depende apenas da diferen√ßa de tempo $h$, ou seja, $\gamma(t, s) = \gamma(t-s) = \gamma(h)$.
V. Se o processo satisfaz a condi√ß√£o de ergodicidade na m√©dia, ent√£o
   $$\sum_{h=-\infty}^{\infty} |\gamma(h)| < \infty$$
VI. Isto implica que a fun√ß√£o de autocovari√¢ncia amostral converge para a fun√ß√£o de autocovari√¢ncia te√≥rica:
    $$\text{plim}_{T\to\infty} \frac{1}{T} \sum_{t=1}^{T} (Y_t - \mu)(Y_{t+h} - \mu) = \gamma(h)$$
VII. Como todos os momentos do processo Gaussiano podem ser expressos em termos da m√©dia e da fun√ß√£o de autocovari√¢ncia, e ambos convergem para os seus valores populacionais, ent√£o todos os momentos amostrais tamb√©m convergem para os seus valores populacionais.
VIII. Portanto, o processo √© erg√≥dico para todos os momentos finitos. ‚ñ†

### Conclus√£o

A ergodicity √© uma propriedade crucial para a an√°lise pr√°tica de s√©ries temporais. Ela garante que as estat√≠sticas calculadas a partir de uma √∫nica realiza√ß√£o da s√©rie temporal s√£o representativas das estat√≠sticas populacionais do processo. Em particular, para um processo covariance-stationary ser erg√≥dico para a m√©dia, a sua fun√ß√£o de autocovari√¢ncia deve decair para zero suficientemente r√°pido [^4]. Processos que s√£o stationary mas n√£o ergodic podem levar a conclus√µes enganosas se as an√°lises forem baseadas em uma √∫nica realiza√ß√£o [^4].

### Refer√™ncias
[^1]: P√°gina 44: "The expectation of the tth observation of a time series refers to the mean of this probability distribution, provided it exists:  E(Y_t) = \int_{-\infty}^{\infty} y_t f_{Y_t}(y_t) dy_t  We might view this as the probability limit of the ensemble average:  E(Y_t) = plim_{I->\infty} (1/I) \sum_{i=1}^{I} Y_t^{(i)}."
[^4]: P√°gina 47: "Whether time averages such as [3.1.14] eventually converge to the ensemble concept E(Y) for a stationary process has to do with ergodicity. A covariance-stationary process is said to be ergodic for the mean if [3.1.14] converges in probability to E(Y) as T‚Üí ‚àû. A process will be ergodic for the mean provided that the autocovariance Œ≥j goes to zero sufficiently quickly as j becomes large... For many applications, stationarity and ergodicity turn out to amount to the same requirements. For purposes of clarifying the concepts of stationarity and ergodicity, however, it may be helpful to consider an example of a process that is stationary but not ergodic. Suppose the mean Œº(i) for the ith realization {y(i)t}t=‚àí‚àû‚àû is generated from a N(0, Œª2) distribution, say Y(i)t=Œº(i)+œµt."
<!-- END -->