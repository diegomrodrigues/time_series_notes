## TÃ­tulo Conciso: AnÃ¡lise AvanÃ§ada de Processos ARMA(p, q)

### IntroduÃ§Ã£o
Em continuidade Ã  discussÃ£o sobre modelos de sÃ©ries temporais estacionÃ¡rias, este capÃ­tulo aprofunda-se na estrutura e propriedades dos processos Autorregressivos de MÃ©dia MÃ³vel (ARMA), especificamente os modelos ARMA(p, q). Como vimos anteriormente, os modelos AR (Autorregressivos) capturam a dependÃªncia de uma variÃ¡vel em seus valores passados, enquanto os modelos MA (MÃ©dias MÃ³veis) modelam a dependÃªncia nos termos de erro passados. Os modelos ARMA(p, q) combinam esses dois aspectos para fornecer uma representaÃ§Ã£o mais rica e flexÃ­vel de sÃ©ries temporais, sendo assim capazes de capturar tanto dependÃªncias de curto quanto de longo prazo [^3.5].

### Conceitos Fundamentais
Um processo ARMA(p, q) Ã© definido pela seguinte equaÃ§Ã£o [^3.5.1]:
$$Y_t = c + \sum_{i=1}^{p} \phi_i Y_{t-i} + \epsilon_t + \sum_{i=1}^{q} \theta_i \epsilon_{t-i}$$

Onde:
*   $Y_t$ representa o valor da sÃ©rie temporal no instante $t$.
*   $c$ Ã© uma constante.
*   $\phi_i$ sÃ£o os *coeficientes autorregressivos* para $i = 1, 2, ..., p$.
*   $Y_{t-i}$ sÃ£o os valores passados da sÃ©rie temporal.
*   $\epsilon_t$ Ã© um termo de erro aleatÃ³rio no instante $t$. Assume-se geralmente que $\epsilon_t$ seja ruÃ­do branco com mÃ©dia zero e variÃ¢ncia constante $\sigma^2$ [^3.2.1, ^3.2.2].
*   $\theta_i$ sÃ£o os *coeficientes de mÃ©dia mÃ³vel* para $i = 1, 2, ..., q$.
*   $\epsilon_{t-i}$ sÃ£o os termos de erro passados.
*   $p$ Ã© a *ordem da parte autorregressiva (AR)* do modelo.
*   $q$ Ã© a *ordem da parte de mÃ©dia mÃ³vel (MA)* do modelo.

Em termos do operador de defasagem (lag operator) $L$, onde $LY_t = Y_{t-1}$, a equaÃ§Ã£o acima pode ser escrita como [^3.5.2]:
$$(1 - \phi_1L - \phi_2L^2 - \ldots - \phi_pL^p)Y_t = c + (1 + \theta_1L + \theta_2L^2 - \ldots - \theta_qL^q)\epsilon_t$$

A *estacionariedade* de um processo ARMA(p, q) depende unicamente dos parÃ¢metros autorregressivos $\phi_1, \phi_2, ..., \phi_p$ [^3.5]. Para garantir a estacionariedade, as raÃ­zes do polinÃ´mio autorregressivo devem estar fora do cÃ­rculo unitÃ¡rio no plano complexo. Em outras palavras, as soluÃ§Ãµes $z$ para a equaÃ§Ã£o $1 - \phi_1z - \phi_2z^2 - \ldots - \phi_pz^p = 0$ devem satisfazer $|z| > 1$ [^3.4.18].

**Exemplo:** Considere um processo AR(1) dado por $Y_t = \phi Y_{t-1} + \epsilon_t$. A condiÃ§Ã£o de estacionariedade exige que $|\phi| < 1$.

*Prova:*
I.  Dado o processo AR(1): $Y_t = \phi Y_{t-1} + \epsilon_t$.
II. Podemos reescrever a equaÃ§Ã£o como $Y_t - \phi Y_{t-1} = \epsilon_t$.
III. Usando o operador de defasagem $L$, temos $(1 - \phi L)Y_t = \epsilon_t$.
IV. Para estacionariedade, a raiz da equaÃ§Ã£o caracterÃ­stica $(1 - \phi z) = 0$ deve estar fora do cÃ­rculo unitÃ¡rio.
V.  Resolvendo para $z$, obtemos $z = \frac{1}{\phi}$.
VI. A condiÃ§Ã£o $|z| > 1$ implica $|\frac{1}{\phi}| > 1$, o que Ã© equivalente a $|\phi| < 1$. Portanto, o processo Ã© estacionÃ¡rio se $|\phi| < 1$. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:** Seja $\phi = 0.8$. EntÃ£o, $Y_t = 0.8Y_{t-1} + \epsilon_t$. Como $|0.8| < 1$, o processo Ã© estacionÃ¡rio. Se $\phi = 1.2$, entÃ£o $Y_t = 1.2Y_{t-1} + \epsilon_t$. Como $|1.2| > 1$, o processo nÃ£o Ã© estacionÃ¡rio e tenderÃ¡ a explodir ao longo do tempo. A sÃ©rie temporal terÃ¡ um comportamento instÃ¡vel.
```python
import numpy as np
import matplotlib.pyplot as plt

# Simulate AR(1) process
def simulate_ar1(phi, steps, initial_value, noise_std):
    values = [initial_value]
    noise = np.random.normal(0, noise_std, steps)
    for i in range(1, steps):
        values.append(phi * values[i-1] + noise[i])
    return values

# Parameters
steps = 100
initial_value = 0
noise_std = 1

# Simulate stationary AR(1)
phi_stationary = 0.8
stationary_data = simulate_ar1(phi_stationary, steps, initial_value, noise_std)

# Simulate non-stationary AR(1)
phi_non_stationary = 1.2
non_stationary_data = simulate_ar1(phi_non_stationary, steps, initial_value, noise_std)

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(stationary_data, label=f'AR(1) Stationary (phi={phi_stationary})')
plt.plot(non_stationary_data, label=f'AR(1) Non-Stationary (phi={phi_non_stationary})')
plt.xlabel('Time Step')
plt.ylabel('Value')
plt.title('AR(1) Process: Stationary vs Non-Stationary')
plt.legend()
plt.grid(True)
plt.show()
```
Este cÃ³digo simula um processo AR(1) com $\phi=0.8$ (estacionÃ¡rio) e $\phi=1.2$ (nÃ£o estacionÃ¡rio) e plota os resultados, ilustrando visualmente a diferenÃ§a no comportamento.

Similarmente, a *invertibilidade* de um processo ARMA(p, q) depende dos parÃ¢metros de mÃ©dia mÃ³vel $\theta_1, \theta_2, ..., \theta_q$. Para que o processo seja invertÃ­vel, as raÃ­zes do polinÃ´mio da mÃ©dia mÃ³vel devem estar fora do cÃ­rculo unitÃ¡rio. Isto Ã©, as soluÃ§Ãµes $z$ para a equaÃ§Ã£o $1 + \theta_1z + \theta_2z^2 + \ldots + \theta_qz^q = 0$ devem satisfazer $|z| > 1$ [^3.7.13].

**Exemplo:** Considere um processo MA(1) dado por $Y_t = \epsilon_t + \theta \epsilon_{t-1}$. A condiÃ§Ã£o de invertibilidade exige que $|\theta| < 1$.

*Prova:*
I. Dado o processo MA(1): $Y_t = \epsilon_t + \theta \epsilon_{t-1}$.
II. Usando o operador de defasagem $L$, temos $Y_t = (1 + \theta L)\epsilon_t$.
III. Para invertibilidade, a raiz da equaÃ§Ã£o caracterÃ­stica $(1 + \theta z) = 0$ deve estar fora do cÃ­rculo unitÃ¡rio.
IV. Resolvendo para $z$, obtemos $z = -\frac{1}{\theta}$.
V. A condiÃ§Ã£o $|z| > 1$ implica $|-\frac{1}{\theta}| > 1$, o que Ã© equivalente a $|\theta| < 1$. Portanto, o processo Ã© invertÃ­vel se $|\theta| < 1$. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:** Seja $\theta = 0.6$. EntÃ£o, $Y_t = \epsilon_t + 0.6\epsilon_{t-1}$. Como $|0.6| < 1$, o processo Ã© invertÃ­vel. Se $\theta = 1.5$, entÃ£o $Y_t = \epsilon_t + 1.5\epsilon_{t-1}$. Como $|1.5| > 1$, o processo nÃ£o Ã© invertÃ­vel. A falta de invertibilidade pode levar a dificuldades na estimaÃ§Ã£o dos parÃ¢metros do modelo. Um processo nÃ£o invertÃ­vel nÃ£o tem uma representaÃ§Ã£o AR(âˆ) convergente.

```python
import numpy as np
import matplotlib.pyplot as plt

# Simulate MA(1) process
def simulate_ma1(theta, steps, noise_std):
    noise = np.random.normal(0, noise_std, steps)
    values = [noise[0]]
    for i in range(1, steps):
        values.append(noise[i] + theta * noise[i-1])
    return values

# Parameters
steps = 100
noise_std = 1

# Simulate invertible MA(1)
theta_invertible = 0.6
invertible_data = simulate_ma1(theta_invertible, steps, noise_std)

# Simulate non-invertible MA(1)
theta_non_invertible = 1.5
non_invertible_data = simulate_ma1(theta_non_invertible, steps, noise_std)

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(invertible_data, label=f'MA(1) Invertible (theta={theta_invertible})')
plt.plot(non_invertible_data, label=f'MA(1) Non-Invertible (theta={theta_non_invertible})')
plt.xlabel('Time Step')
plt.ylabel('Value')
plt.title('MA(1) Process: Invertible vs Non-Invertible')
plt.legend()
plt.grid(True)
plt.show()
```
Este cÃ³digo simula um processo MA(1) com $\theta=0.6$ (invertÃ­vel) e $\theta=1.5$ (nÃ£o invertÃ­vel) e plota os resultados. Note que a diferenÃ§a visual direta pode nÃ£o ser tÃ£o clara quanto no caso AR(1), mas as propriedades estatÃ­sticas do processo nÃ£o invertÃ­vel serÃ£o diferentes, especialmente em relaÃ§Ã£o Ã  sua representaÃ§Ã£o AR(âˆ).

Se as condiÃ§Ãµes de estacionariedade e invertibilidade forem satisfeitas, o processo ARMA(p, q) pode ser expresso em termos de uma representaÃ§Ã£o MA(âˆ) ou AR(âˆ). A representaÃ§Ã£o MA(âˆ) expressa $Y_t$ como uma soma ponderada infinita de termos de erro passados [^3.3.13], Ãºtil para analisar o impacto de choques passados na sÃ©rie temporal atual. Por outro lado, a representaÃ§Ã£o AR(âˆ) expressa $Y_t$ como uma soma ponderada infinita de seus prÃ³prios valores passados, juntamente com um termo constante e o termo de erro atual [^3.4.2].

**Teorema 1:** *ExistÃªncia e Unicidade da RepresentaÃ§Ã£o MA(âˆ) e AR(âˆ)*

Se um processo ARMA(p, q) Ã© estacionÃ¡rio e invertÃ­vel, entÃ£o ele possui representaÃ§Ãµes MA(âˆ) e AR(âˆ) Ãºnicas.

*Prova (EsboÃ§o):* A estacionariedade garante que o polinÃ´mio AR tenha raÃ­zes fora do cÃ­rculo unitÃ¡rio, permitindo a expansÃ£o da parte AR como um operador infinito aplicado a $Y_t$. Similarmente, a invertibilidade permite a expansÃ£o da parte MA como um operador infinito aplicado a $\epsilon_t$. A unicidade segue da unicidade da expansÃ£o em sÃ©ries de potÃªncias.

**Teorema 1.1:** *ConvergÃªncia das RepresentaÃ§Ãµes MA(âˆ) e AR(âˆ)*
As representaÃ§Ãµes MA(âˆ) e AR(âˆ) convergem absolutamente se o processo ARMA(p, q) Ã© estacionÃ¡rio e invertÃ­vel.

*Prova (EsboÃ§o):* A estacionariedade e invertibilidade implicam que os coeficientes nas representaÃ§Ãµes MA(âˆ) e AR(âˆ) decaem exponencialmente. Isso garante a convergÃªncia absoluta das sÃ©ries infinitas, pois a soma dos valores absolutos dos coeficientes Ã© finita.

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um processo ARMA(1,1) dado por $Y_t = 0.5Y_{t-1} + \epsilon_t + 0.3\epsilon_{t-1}$. Aqui, $\phi = 0.5$ e $\theta = 0.3$. Como $|\phi| < 1$ e $|\theta| < 1$, o processo Ã© estacionÃ¡rio e invertÃ­vel. Podemos aproximar a representaÃ§Ã£o MA(âˆ) truncando a sÃ©rie apÃ³s alguns termos.

ApÃ³s $q$ defasagens, a funÃ§Ã£o de autocovariÃ¢ncia $\gamma_j$ (e, portanto, a funÃ§Ã£o de autocorrelaÃ§Ã£o $\rho_j$) seguem a mesma equaÃ§Ã£o de diferenÃ§a de ordem p que o processo [^3.5.5]:
$$\gamma_j = \phi_1\gamma_{j-1} + \phi_2\gamma_{j-2} + \ldots + \phi_p\gamma_{j-p} \text{ para } j > q$$
Onde $Î³_j$ Ã© a autocovariÃ¢ncia no lag $j$. Isso implica que o comportamento de longo prazo da funÃ§Ã£o de autocorrelaÃ§Ã£o Ã© determinado pelos parÃ¢metros autorregressivos.

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um processo ARMA(2,1) com $\phi_1 = 0.6$, $\phi_2 = -0.3$ e $\theta_1 = 0.4$. Para $j > 1$, a autocovariÃ¢ncia satisfaz a equaÃ§Ã£o $\gamma_j = 0.6\gamma_{j-1} - 0.3\gamma_0$. Se conhecemos $\gamma_0$, $\gamma_1$, podemos calcular $\gamma_2$, $\gamma_3$ e assim por diante. Por exemplo, $\gamma_2 = 0.6\gamma_1 - 0.3\gamma_0$. O decaimento ou oscilaÃ§Ã£o das autocovariÃ¢ncias Ã© determinado pelas raÃ­zes da equaÃ§Ã£o caracterÃ­stica $1 - 0.6z + 0.3z^2 = 0$.

**ProposiÃ§Ã£o 1:** A funÃ§Ã£o de autocorrelaÃ§Ã£o parcial (PACF) de um processo ARMA(p,q) se anula apÃ³s a defasagem *p* para um processo AR(p), e decai para um processo MA(q).

*Prova:*
I.  Para um processo AR(p), a PACF mede a correlaÃ§Ã£o entre $Y_t$ e $Y_{t-k}$ apÃ³s remover o efeito das defasagens intermediÃ¡rias $Y_{t-1}, Y_{t-2}, ..., Y_{t-(k-1)}$.
II. Para $k > p$, a correlaÃ§Ã£o parcial entre $Y_t$ e $Y_{t-k}$ Ã© zero, pois o processo AR(p) Ã© completamente determinado por seus *p* valores passados mais recentes.
III. Portanto, a PACF se anula apÃ³s a defasagem *p* para um processo AR(p).
IV. Para um processo MA(q), a PACF nÃ£o se anula abruptamente, mas decai. Isto ocorre porque a influÃªncia dos termos de erro passados se estende indefinidamente, embora com pesos decrescentes, nos valores passados da sÃ©rie temporal.
V. Portanto, para um processo MA(q), a PACF decai em vez de se anular apÃ³s uma determinada defasagem. $\blacksquare$

**ProposiÃ§Ã£o 1.1:** Para um processo ARMA(p,q), a funÃ§Ã£o de autocorrelaÃ§Ã£o (ACF) decai exponencialmente ou em padrÃ£o sinusoidal amortecido apÃ³s a defasagem *q*.

*Prova (EsboÃ§o):* Para $j > q$, a autocovariÃ¢ncia $\gamma_j$ satisfaz uma equaÃ§Ã£o de diferenÃ§a homogÃªnea de ordem *p* com coeficientes dados pelos parÃ¢metros AR. As soluÃ§Ãµes desta equaÃ§Ã£o de diferenÃ§a sÃ£o combinaÃ§Ãµes lineares de exponenciais e sinusoides amortecidas, dependendo das raÃ­zes do polinÃ´mio caracterÃ­stico associado.

> ğŸ’¡ **Exemplo NumÃ©rico:** Consideremos um AR(2) com $\phi_1 = 0.5$ e $\phi_2 = -0.25$. A equaÃ§Ã£o de diferenÃ§a para a autocovariÃ¢ncia Ã© $\gamma_j = 0.5\gamma_{j-1} - 0.25\gamma_{j-2}$ para $j > 0$. As raÃ­zes da equaÃ§Ã£o caracterÃ­stica $1 - 0.5z + 0.25z^2 = 0$ sÃ£o complexas, indicando um padrÃ£o sinusoidal amortecido na ACF. Este padrÃ£o pode ser usado para identificar a ordem do modelo AR.

Ã‰ importante notar que existe um potencial para *parametrizaÃ§Ã£o redundante* em modelos ARMA [^3.5]. Isso ocorre quando fatores comuns podem ser cancelados entre as partes AR e MA do modelo, resultando em um modelo mais simples com menos parÃ¢metros. Ã‰ crucial evitar tais parametrizaÃ§Ãµes redundantes para garantir que o modelo seja identificÃ¡vel e que os parÃ¢metros possam ser estimados de forma precisa e eficiente.

**Exemplo:** Considere o modelo $Y_t = 0.5Y_{t-1} + \epsilon_t + 0.5\epsilon_{t-1}$. Este modelo pode ser reescrito como $Y_t = 0.5Y_{t-1} + \epsilon_t + 0.5\epsilon_{t-1}$. Se ambos os lados da equaÃ§Ã£o tiverem um fator comum (1-0.5L), simplificar o modelo.

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere o modelo ARMA(1,1): $Y_t = 0.5Y_{t-1} + \epsilon_t - 0.5\epsilon_{t-1}$. Em termos do operador de defasagem, temos $(1 - 0.5L)Y_t = (1 - 0.5L)\epsilon_t$. Aqui, o termo $(1 - 0.5L)$ Ã© um fator comum em ambos os lados da equaÃ§Ã£o, o que significa que podemos simplificar o modelo para $Y_t = \epsilon_t$, que Ã© um ruÃ­do branco. Estimar o modelo original ARMA(1,1) levaria a erros, pois estarÃ­amos estimando parÃ¢metros desnecessÃ¡rios.

**Lema 1:** *Identificabilidade de Modelos ARMA*
Um modelo ARMA(p, q) Ã© identificÃ¡vel se e somente se os polinÃ´mios AR e MA nÃ£o tÃªm fatores comuns e a variÃ¢ncia do ruÃ­do branco Ã© diferente de zero.

**CorolÃ¡rio 1:** *Modelos ARMA MÃ­nimos*
Para qualquer processo ARMA(p, q) estacionÃ¡rio e invertÃ­vel, existe um modelo ARMA(p', q') mÃ­nimo (i.e., com o menor nÃºmero de parÃ¢metros) que gera a mesma funÃ§Ã£o de autocorrelaÃ§Ã£o. Este modelo mÃ­nimo Ã© Ãºnico e nÃ£o possui parametrizaÃ§Ã£o redundante.

### ConclusÃ£o
Os modelos ARMA(p, q) oferecem uma ferramenta poderosa e flexÃ­vel para modelar sÃ©ries temporais estacionÃ¡rias. Ao combinar componentes autorregressivos e de mÃ©dia mÃ³vel, esses modelos sÃ£o capazes de capturar uma ampla gama de padrÃµes e dependÃªncias observados em dados do mundo real. A anÃ¡lise cuidadosa das condiÃ§Ãµes de estacionariedade e invertibilidade, juntamente com a consideraÃ§Ã£o de possÃ­veis parametrizaÃ§Ãµes redundantes, Ã© essencial para garantir que o modelo seja bem especificado e possa ser usado para inferÃªncia e previsÃ£o precisas.

### ReferÃªncias
[^3.2.1]: E(Îµ_t) = 0
[^3.2.2]: E(Îµ_t^2) = Ïƒ^2
[^3.3.13]: Y, = Î¼ + Î£[j=0 to âˆ] Ïˆ_j Îµ_{t-j}
[^3.4]: A first-order autoregression, denoted AR(1), satisfies the following difference equation: Y_t = c + Ï† Y_{t-1} + Îµ_t.
[^3.4.2]: Y, = (c + Îµ_t) + Ï†(c + Îµ_{t-1}) + Ï†^2(c + Îµ_{t-2}) + Ï†^3(c + Îµ_{t-3}) +... = [c/(1 - Ï†)] + Îµ_t + Ï†Îµ_{t-1} + Ï†^2Îµ_{t-2} + Ï†^3Îµ_{t-3}+...
[^3.4.18]: (1 â€“ Î¦â‚z â€“ Î¦â‚‚zÂ²) = 0 lie outside the unit circle.
[^3.5]: An ARMA(p, q) process includes both autoregressive and moving average terms: Y_t = c + Î¦â‚Y_{t-1} + Î¦â‚‚Y_{t-2} + ... + Î¦_pY_{t-p} + Îµ_t + Î¸_1Îµ_{t-1} + Î¸_2Îµ_{t-2} + Â·Â·Â· + Î¸_qÎµ_{t-q}.
[^3.5.1]: Yâ‚ = c + Î¦1Y-1 + $21-2++ pYâ‚-p + + Î¸Îµ
[^3.5.2]: (1 â€“ Î¦â‚L - Î¦â‚‚LÂ² - Â·Â·Â· - Î¦ÏLP) Y, = c + (1 + 0â‚L + 02L2 + + Î¸Î±Ï„Î±Î»Îµ,
[^3.5.5]: Î¥; = Î¦17-1 + Î¦2Yj-2 + + Î¦Ñ€ÑƒÑ–-Ñ€ for j = q + 1, q+ 2,...
[^3.7.13]: (1 + Î¸â‚2 + Î¸â‚‚zÂ² + Â·Â·Â· + 0,29) = 0 lie outside the unit circle.
[^3.7.12]: (Î¥, â€“ Î¼) = (1 + 0â‚L + 02LÂ² + + Î¸Î±Ï„Î»Îµ,
[^3.7]: Invertibility for the MA(1) Process
<!-- END -->