## T√≠tulo Conciso: An√°lise Avan√ßada de Processos ARMA(p, q)

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre modelos de s√©ries temporais estacion√°rias, este cap√≠tulo aprofunda-se na estrutura e propriedades dos processos Autorregressivos de M√©dia M√≥vel (ARMA), especificamente os modelos ARMA(p, q). Como vimos anteriormente, os modelos AR (Autorregressivos) capturam a depend√™ncia de uma vari√°vel em seus valores passados, enquanto os modelos MA (M√©dias M√≥veis) modelam a depend√™ncia nos termos de erro passados. Os modelos ARMA(p, q) combinam esses dois aspectos para fornecer uma representa√ß√£o mais rica e flex√≠vel de s√©ries temporais, sendo assim capazes de capturar tanto depend√™ncias de curto quanto de longo prazo [^3.5].

### Conceitos Fundamentais
Um processo ARMA(p, q) √© definido pela seguinte equa√ß√£o [^3.5.1]:
$$Y_t = c + \sum_{i=1}^{p} \phi_i Y_{t-i} + \epsilon_t + \sum_{i=1}^{q} \theta_i \epsilon_{t-i}$$

Onde:
*   $Y_t$ representa o valor da s√©rie temporal no instante $t$.
*   $c$ √© uma constante.
*   $\phi_i$ s√£o os *coeficientes autorregressivos* para $i = 1, 2, ..., p$.
*   $Y_{t-i}$ s√£o os valores passados da s√©rie temporal.
*   $\epsilon_t$ √© um termo de erro aleat√≥rio no instante $t$. Assume-se geralmente que $\epsilon_t$ seja ru√≠do branco com m√©dia zero e vari√¢ncia constante $\sigma^2$ [^3.2.1, ^3.2.2].
*   $\theta_i$ s√£o os *coeficientes de m√©dia m√≥vel* para $i = 1, 2, ..., q$.
*   $\epsilon_{t-i}$ s√£o os termos de erro passados.
*   $p$ √© a *ordem da parte autorregressiva (AR)* do modelo.
*   $q$ √© a *ordem da parte de m√©dia m√≥vel (MA)* do modelo.

Em termos do operador de defasagem (lag operator) $L$, onde $LY_t = Y_{t-1}$, a equa√ß√£o acima pode ser escrita como [^3.5.2]:
$$(1 - \phi_1L - \phi_2L^2 - \ldots - \phi_pL^p)Y_t = c + (1 + \theta_1L + \theta_2L^2 - \ldots - \theta_qL^q)\epsilon_t$$

A *estacionariedade* de um processo ARMA(p, q) depende unicamente dos par√¢metros autorregressivos $\phi_1, \phi_2, ..., \phi_p$ [^3.5]. Para garantir a estacionariedade, as ra√≠zes do polin√¥mio autorregressivo devem estar fora do c√≠rculo unit√°rio no plano complexo. Em outras palavras, as solu√ß√µes $z$ para a equa√ß√£o $1 - \phi_1z - \phi_2z^2 - \ldots - \phi_pz^p = 0$ devem satisfazer $|z| > 1$ [^3.4.18].

**Exemplo:** Considere um processo AR(1) dado por $Y_t = \phi Y_{t-1} + \epsilon_t$. A condi√ß√£o de estacionariedade exige que $|\phi| < 1$.

*Prova:*
I.  Dado o processo AR(1): $Y_t = \phi Y_{t-1} + \epsilon_t$.
II. Podemos reescrever a equa√ß√£o como $Y_t - \phi Y_{t-1} = \epsilon_t$.
III. Usando o operador de defasagem $L$, temos $(1 - \phi L)Y_t = \epsilon_t$.
IV. Para estacionariedade, a raiz da equa√ß√£o caracter√≠stica $(1 - \phi z) = 0$ deve estar fora do c√≠rculo unit√°rio.
V.  Resolvendo para $z$, obtemos $z = \frac{1}{\phi}$.
VI. A condi√ß√£o $|z| > 1$ implica $|\frac{1}{\phi}| > 1$, o que √© equivalente a $|\phi| < 1$. Portanto, o processo √© estacion√°rio se $|\phi| < 1$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Seja $\phi = 0.8$. Ent√£o, $Y_t = 0.8Y_{t-1} + \epsilon_t$. Como $|0.8| < 1$, o processo √© estacion√°rio. Se $\phi = 1.2$, ent√£o $Y_t = 1.2Y_{t-1} + \epsilon_t$. Como $|1.2| > 1$, o processo n√£o √© estacion√°rio e tender√° a explodir ao longo do tempo. A s√©rie temporal ter√° um comportamento inst√°vel.
```python
import numpy as np
import matplotlib.pyplot as plt

# Simulate AR(1) process
def simulate_ar1(phi, steps, initial_value, noise_std):
    values = [initial_value]
    noise = np.random.normal(0, noise_std, steps)
    for i in range(1, steps):
        values.append(phi * values[i-1] + noise[i])
    return values

# Parameters
steps = 100
initial_value = 0
noise_std = 1

# Simulate stationary AR(1)
phi_stationary = 0.8
stationary_data = simulate_ar1(phi_stationary, steps, initial_value, noise_std)

# Simulate non-stationary AR(1)
phi_non_stationary = 1.2
non_stationary_data = simulate_ar1(phi_non_stationary, steps, initial_value, noise_std)

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(stationary_data, label=f'AR(1) Stationary (phi={phi_stationary})')
plt.plot(non_stationary_data, label=f'AR(1) Non-Stationary (phi={phi_non_stationary})')
plt.xlabel('Time Step')
plt.ylabel('Value')
plt.title('AR(1) Process: Stationary vs Non-Stationary')
plt.legend()
plt.grid(True)
plt.show()
```
Este c√≥digo simula um processo AR(1) com $\phi=0.8$ (estacion√°rio) e $\phi=1.2$ (n√£o estacion√°rio) e plota os resultados, ilustrando visualmente a diferen√ßa no comportamento.

Similarmente, a *invertibilidade* de um processo ARMA(p, q) depende dos par√¢metros de m√©dia m√≥vel $\theta_1, \theta_2, ..., \theta_q$. Para que o processo seja invert√≠vel, as ra√≠zes do polin√¥mio da m√©dia m√≥vel devem estar fora do c√≠rculo unit√°rio. Isto √©, as solu√ß√µes $z$ para a equa√ß√£o $1 + \theta_1z + \theta_2z^2 + \ldots + \theta_qz^q = 0$ devem satisfazer $|z| > 1$ [^3.7.13].

**Exemplo:** Considere um processo MA(1) dado por $Y_t = \epsilon_t + \theta \epsilon_{t-1}$. A condi√ß√£o de invertibilidade exige que $|\theta| < 1$.

*Prova:*
I. Dado o processo MA(1): $Y_t = \epsilon_t + \theta \epsilon_{t-1}$.
II. Usando o operador de defasagem $L$, temos $Y_t = (1 + \theta L)\epsilon_t$.
III. Para invertibilidade, a raiz da equa√ß√£o caracter√≠stica $(1 + \theta z) = 0$ deve estar fora do c√≠rculo unit√°rio.
IV. Resolvendo para $z$, obtemos $z = -\frac{1}{\theta}$.
V. A condi√ß√£o $|z| > 1$ implica $|-\frac{1}{\theta}| > 1$, o que √© equivalente a $|\theta| < 1$. Portanto, o processo √© invert√≠vel se $|\theta| < 1$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Seja $\theta = 0.6$. Ent√£o, $Y_t = \epsilon_t + 0.6\epsilon_{t-1}$. Como $|0.6| < 1$, o processo √© invert√≠vel. Se $\theta = 1.5$, ent√£o $Y_t = \epsilon_t + 1.5\epsilon_{t-1}$. Como $|1.5| > 1$, o processo n√£o √© invert√≠vel. A falta de invertibilidade pode levar a dificuldades na estima√ß√£o dos par√¢metros do modelo. Um processo n√£o invert√≠vel n√£o tem uma representa√ß√£o AR(‚àû) convergente.

```python
import numpy as np
import matplotlib.pyplot as plt

# Simulate MA(1) process
def simulate_ma1(theta, steps, noise_std):
    noise = np.random.normal(0, noise_std, steps)
    values = [noise[0]]
    for i in range(1, steps):
        values.append(noise[i] + theta * noise[i-1])
    return values

# Parameters
steps = 100
noise_std = 1

# Simulate invertible MA(1)
theta_invertible = 0.6
invertible_data = simulate_ma1(theta_invertible, steps, noise_std)

# Simulate non-invertible MA(1)
theta_non_invertible = 1.5
non_invertible_data = simulate_ma1(theta_non_invertible, steps, noise_std)

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(invertible_data, label=f'MA(1) Invertible (theta={theta_invertible})')
plt.plot(non_invertible_data, label=f'MA(1) Non-Invertible (theta={theta_non_invertible})')
plt.xlabel('Time Step')
plt.ylabel('Value')
plt.title('MA(1) Process: Invertible vs Non-Invertible')
plt.legend()
plt.grid(True)
plt.show()
```
Este c√≥digo simula um processo MA(1) com $\theta=0.6$ (invert√≠vel) e $\theta=1.5$ (n√£o invert√≠vel) e plota os resultados. Note que a diferen√ßa visual direta pode n√£o ser t√£o clara quanto no caso AR(1), mas as propriedades estat√≠sticas do processo n√£o invert√≠vel ser√£o diferentes, especialmente em rela√ß√£o √† sua representa√ß√£o AR(‚àû).

Se as condi√ß√µes de estacionariedade e invertibilidade forem satisfeitas, o processo ARMA(p, q) pode ser expresso em termos de uma representa√ß√£o MA(‚àû) ou AR(‚àû). A representa√ß√£o MA(‚àû) expressa $Y_t$ como uma soma ponderada infinita de termos de erro passados [^3.3.13], √∫til para analisar o impacto de choques passados na s√©rie temporal atual. Por outro lado, a representa√ß√£o AR(‚àû) expressa $Y_t$ como uma soma ponderada infinita de seus pr√≥prios valores passados, juntamente com um termo constante e o termo de erro atual [^3.4.2].

**Teorema 1:** *Exist√™ncia e Unicidade da Representa√ß√£o MA(‚àû) e AR(‚àû)*

Se um processo ARMA(p, q) √© estacion√°rio e invert√≠vel, ent√£o ele possui representa√ß√µes MA(‚àû) e AR(‚àû) √∫nicas.

*Prova (Esbo√ßo):* A estacionariedade garante que o polin√¥mio AR tenha ra√≠zes fora do c√≠rculo unit√°rio, permitindo a expans√£o da parte AR como um operador infinito aplicado a $Y_t$. Similarmente, a invertibilidade permite a expans√£o da parte MA como um operador infinito aplicado a $\epsilon_t$. A unicidade segue da unicidade da expans√£o em s√©ries de pot√™ncias.

**Teorema 1.1:** *Converg√™ncia das Representa√ß√µes MA(‚àû) e AR(‚àû)*
As representa√ß√µes MA(‚àû) e AR(‚àû) convergem absolutamente se o processo ARMA(p, q) √© estacion√°rio e invert√≠vel.

*Prova (Esbo√ßo):* A estacionariedade e invertibilidade implicam que os coeficientes nas representa√ß√µes MA(‚àû) e AR(‚àû) decaem exponencialmente. Isso garante a converg√™ncia absoluta das s√©ries infinitas, pois a soma dos valores absolutos dos coeficientes √© finita.

> üí° **Exemplo Num√©rico:** Considere um processo ARMA(1,1) dado por $Y_t = 0.5Y_{t-1} + \epsilon_t + 0.3\epsilon_{t-1}$. Aqui, $\phi = 0.5$ e $\theta = 0.3$. Como $|\phi| < 1$ e $|\theta| < 1$, o processo √© estacion√°rio e invert√≠vel. Podemos aproximar a representa√ß√£o MA(‚àû) truncando a s√©rie ap√≥s alguns termos.

Ap√≥s $q$ defasagens, a fun√ß√£o de autocovari√¢ncia $\gamma_j$ (e, portanto, a fun√ß√£o de autocorrela√ß√£o $\rho_j$) seguem a mesma equa√ß√£o de diferen√ßa de ordem p que o processo [^3.5.5]:
$$\gamma_j = \phi_1\gamma_{j-1} + \phi_2\gamma_{j-2} + \ldots + \phi_p\gamma_{j-p} \text{ para } j > q$$
Onde $Œ≥_j$ √© a autocovari√¢ncia no lag $j$. Isso implica que o comportamento de longo prazo da fun√ß√£o de autocorrela√ß√£o √© determinado pelos par√¢metros autorregressivos.

> üí° **Exemplo Num√©rico:** Considere um processo ARMA(2,1) com $\phi_1 = 0.6$, $\phi_2 = -0.3$ e $\theta_1 = 0.4$. Para $j > 1$, a autocovari√¢ncia satisfaz a equa√ß√£o $\gamma_j = 0.6\gamma_{j-1} - 0.3\gamma_0$. Se conhecemos $\gamma_0$, $\gamma_1$, podemos calcular $\gamma_2$, $\gamma_3$ e assim por diante. Por exemplo, $\gamma_2 = 0.6\gamma_1 - 0.3\gamma_0$. O decaimento ou oscila√ß√£o das autocovari√¢ncias √© determinado pelas ra√≠zes da equa√ß√£o caracter√≠stica $1 - 0.6z + 0.3z^2 = 0$.

**Proposi√ß√£o 1:** A fun√ß√£o de autocorrela√ß√£o parcial (PACF) de um processo ARMA(p,q) se anula ap√≥s a defasagem *p* para um processo AR(p), e decai para um processo MA(q).

*Prova:*
I.  Para um processo AR(p), a PACF mede a correla√ß√£o entre $Y_t$ e $Y_{t-k}$ ap√≥s remover o efeito das defasagens intermedi√°rias $Y_{t-1}, Y_{t-2}, ..., Y_{t-(k-1)}$.
II. Para $k > p$, a correla√ß√£o parcial entre $Y_t$ e $Y_{t-k}$ √© zero, pois o processo AR(p) √© completamente determinado por seus *p* valores passados mais recentes.
III. Portanto, a PACF se anula ap√≥s a defasagem *p* para um processo AR(p).
IV. Para um processo MA(q), a PACF n√£o se anula abruptamente, mas decai. Isto ocorre porque a influ√™ncia dos termos de erro passados se estende indefinidamente, embora com pesos decrescentes, nos valores passados da s√©rie temporal.
V. Portanto, para um processo MA(q), a PACF decai em vez de se anular ap√≥s uma determinada defasagem. $\blacksquare$

**Proposi√ß√£o 1.1:** Para um processo ARMA(p,q), a fun√ß√£o de autocorrela√ß√£o (ACF) decai exponencialmente ou em padr√£o sinusoidal amortecido ap√≥s a defasagem *q*.

*Prova (Esbo√ßo):* Para $j > q$, a autocovari√¢ncia $\gamma_j$ satisfaz uma equa√ß√£o de diferen√ßa homog√™nea de ordem *p* com coeficientes dados pelos par√¢metros AR. As solu√ß√µes desta equa√ß√£o de diferen√ßa s√£o combina√ß√µes lineares de exponenciais e sinusoides amortecidas, dependendo das ra√≠zes do polin√¥mio caracter√≠stico associado.

> üí° **Exemplo Num√©rico:** Consideremos um AR(2) com $\phi_1 = 0.5$ e $\phi_2 = -0.25$. A equa√ß√£o de diferen√ßa para a autocovari√¢ncia √© $\gamma_j = 0.5\gamma_{j-1} - 0.25\gamma_{j-2}$ para $j > 0$. As ra√≠zes da equa√ß√£o caracter√≠stica $1 - 0.5z + 0.25z^2 = 0$ s√£o complexas, indicando um padr√£o sinusoidal amortecido na ACF. Este padr√£o pode ser usado para identificar a ordem do modelo AR.

√â importante notar que existe um potencial para *parametriza√ß√£o redundante* em modelos ARMA [^3.5]. Isso ocorre quando fatores comuns podem ser cancelados entre as partes AR e MA do modelo, resultando em um modelo mais simples com menos par√¢metros. √â crucial evitar tais parametriza√ß√µes redundantes para garantir que o modelo seja identific√°vel e que os par√¢metros possam ser estimados de forma precisa e eficiente.

**Exemplo:** Considere o modelo $Y_t = 0.5Y_{t-1} + \epsilon_t + 0.5\epsilon_{t-1}$. Este modelo pode ser reescrito como $Y_t = 0.5Y_{t-1} + \epsilon_t + 0.5\epsilon_{t-1}$. Se ambos os lados da equa√ß√£o tiverem um fator comum (1-0.5L), simplificar o modelo.

> üí° **Exemplo Num√©rico:** Considere o modelo ARMA(1,1): $Y_t = 0.5Y_{t-1} + \epsilon_t - 0.5\epsilon_{t-1}$. Em termos do operador de defasagem, temos $(1 - 0.5L)Y_t = (1 - 0.5L)\epsilon_t$. Aqui, o termo $(1 - 0.5L)$ √© um fator comum em ambos os lados da equa√ß√£o, o que significa que podemos simplificar o modelo para $Y_t = \epsilon_t$, que √© um ru√≠do branco. Estimar o modelo original ARMA(1,1) levaria a erros, pois estar√≠amos estimando par√¢metros desnecess√°rios.

**Lema 1:** *Identificabilidade de Modelos ARMA*
Um modelo ARMA(p, q) √© identific√°vel se e somente se os polin√¥mios AR e MA n√£o t√™m fatores comuns e a vari√¢ncia do ru√≠do branco √© diferente de zero.

**Corol√°rio 1:** *Modelos ARMA M√≠nimos*
Para qualquer processo ARMA(p, q) estacion√°rio e invert√≠vel, existe um modelo ARMA(p', q') m√≠nimo (i.e., com o menor n√∫mero de par√¢metros) que gera a mesma fun√ß√£o de autocorrela√ß√£o. Este modelo m√≠nimo √© √∫nico e n√£o possui parametriza√ß√£o redundante.

### Conclus√£o
Os modelos ARMA(p, q) oferecem uma ferramenta poderosa e flex√≠vel para modelar s√©ries temporais estacion√°rias. Ao combinar componentes autorregressivos e de m√©dia m√≥vel, esses modelos s√£o capazes de capturar uma ampla gama de padr√µes e depend√™ncias observados em dados do mundo real. A an√°lise cuidadosa das condi√ß√µes de estacionariedade e invertibilidade, juntamente com a considera√ß√£o de poss√≠veis parametriza√ß√µes redundantes, √© essencial para garantir que o modelo seja bem especificado e possa ser usado para infer√™ncia e previs√£o precisas.

### Refer√™ncias
[^3.2.1]: E(Œµ_t) = 0
[^3.2.2]: E(Œµ_t^2) = œÉ^2
[^3.3.13]: Y, = Œº + Œ£[j=0 to ‚àû] œà_j Œµ_{t-j}
[^3.4]: A first-order autoregression, denoted AR(1), satisfies the following difference equation: Y_t = c + œÜ Y_{t-1} + Œµ_t.
[^3.4.2]: Y, = (c + Œµ_t) + œÜ(c + Œµ_{t-1}) + œÜ^2(c + Œµ_{t-2}) + œÜ^3(c + Œµ_{t-3}) +... = [c/(1 - œÜ)] + Œµ_t + œÜŒµ_{t-1} + œÜ^2Œµ_{t-2} + œÜ^3Œµ_{t-3}+...
[^3.4.18]: (1 ‚Äì Œ¶‚ÇÅz ‚Äì Œ¶‚ÇÇz¬≤) = 0 lie outside the unit circle.
[^3.5]: An ARMA(p, q) process includes both autoregressive and moving average terms: Y_t = c + Œ¶‚ÇÅY_{t-1} + Œ¶‚ÇÇY_{t-2} + ... + Œ¶_pY_{t-p} + Œµ_t + Œ∏_1Œµ_{t-1} + Œ∏_2Œµ_{t-2} + ¬∑¬∑¬∑ + Œ∏_qŒµ_{t-q}.
[^3.5.1]: Y‚ÇÅ = c + Œ¶1Y-1 + $21-2++ pY‚ÇÅ-p + + Œ∏Œµ
[^3.5.2]: (1 ‚Äì Œ¶‚ÇÅL - Œ¶‚ÇÇL¬≤ - ¬∑¬∑¬∑ - Œ¶œÅLP) Y, = c + (1 + 0‚ÇÅL + 02L2 + + Œ∏Œ±œÑŒ±ŒªŒµ,
[^3.5.5]: Œ•; = Œ¶17-1 + Œ¶2Yj-2 + + Œ¶—Ä—É—ñ-—Ä for j = q + 1, q+ 2,...
[^3.7.13]: (1 + Œ∏‚ÇÅ2 + Œ∏‚ÇÇz¬≤ + ¬∑¬∑¬∑ + 0,29) = 0 lie outside the unit circle.
[^3.7.12]: (Œ•, ‚Äì Œº) = (1 + 0‚ÇÅL + 02L¬≤ + + Œ∏Œ±œÑŒªŒµ,
[^3.7]: Invertibility for the MA(1) Process
<!-- END -->