## An√°lise Avan√ßada de Processos ARMA(p, q): Fun√ß√£o de Autocovari√¢ncia e Identifica√ß√£o de Modelos

### Introdu√ß√£o

Em continuidade √† discuss√£o sobre modelos de s√©ries temporais estacion√°rias, este cap√≠tulo aprofunda-se na estrutura e propriedades dos processos Autorregressivos de M√©dia M√≥vel (ARMA), especificamente focando em como a fun√ß√£o de autocovari√¢ncia de um processo ARMA(p, q) segue uma equa√ß√£o de diferen√ßa de ordem $p$ ap√≥s $q$ defasagens [^3.5]. Como vimos anteriormente, a identifica√ß√£o eficiente de modelos e a estima√ß√£o de par√¢metros podem ser alcan√ßadas aproveitando as propriedades estruturais da fun√ß√£o de autocovari√¢ncia, facilitando uma an√°lise mais precisa e robusta das din√¢micas temporais [^3.5].

### Conceitos Fundamentais

Um processo ARMA(p, q) √© definido pela seguinte equa√ß√£o [^3.5.1]:
$$Y_t = c + \sum_{i=1}^{p} \phi_i Y_{t-i} + \epsilon_t + \sum_{i=1}^{q} \theta_i \epsilon_{t-i}$$

Onde:
*   $Y_t$ representa o valor da s√©rie temporal no instante $t$.
*   $c$ √© uma constante.
*   $\phi_i$ s√£o os *coeficientes autorregressivos* para $i = 1, 2, ..., p$.
*   $Y_{t-i}$ s√£o os valores passados da s√©rie temporal.
*   $\epsilon_t$ √© um termo de erro aleat√≥rio no instante $t$. Assume-se geralmente que $\epsilon_t$ seja ru√≠do branco com m√©dia zero e vari√¢ncia constante $\sigma^2$ [^3.2.1, ^3.2.2].
*   $\theta_i$ s√£o os *coeficientes de m√©dia m√≥vel* para $i = 1, 2, ..., q$.
*   $\epsilon_{t-i}$ s√£o os termos de erro passados.
*   $p$ √© a *ordem da parte autorregressiva (AR)* do modelo.
*   $q$ √© a *ordem da parte de m√©dia m√≥vel (MA)* do modelo.

> üí° **Exemplo Num√©rico:** Considere um processo ARMA(1, 1) com $c = 0$, $\phi_1 = 0.7$, $\theta_1 = 0.3$, e $\epsilon_t$ seguindo uma distribui√ß√£o normal com m√©dia 0 e desvio padr√£o 1. Podemos simular alguns valores da s√©rie temporal:
>
> *   $\epsilon_1 = 0.5$
> *   $Y_1 = \epsilon_1 = 0.5$  (j√° que n√£o h√° termos passados no tempo 1)
> *   $\epsilon_2 = -0.2$
> *   $Y_2 = \phi_1 Y_1 + \epsilon_2 + \theta_1 \epsilon_1 = 0.7 * 0.5 - 0.2 + 0.3 * 0.5 = 0.35 - 0.2 + 0.15 = 0.3$
> *   $\epsilon_3 = 1.0$
> *   $Y_3 = \phi_1 Y_2 + \epsilon_3 + \theta_1 \epsilon_2 = 0.7 * 0.3 + 1.0 + 0.3 * (-0.2) = 0.21 + 1.0 - 0.06 = 1.15$
>
> Este exemplo demonstra como os valores passados da s√©rie ($Y_{t-i}$) e os termos de erro passados ($\epsilon_{t-i}$) influenciam o valor atual $Y_t$ atrav√©s dos coeficientes $\phi_i$ e $\theta_i$.

#### Fun√ß√£o de Autocovari√¢ncia (ACF) e Autocorrela√ß√£o Parcial (PACF)

As fun√ß√µes de Autocovari√¢ncia (ACF) e Autocorrela√ß√£o Parcial (PACF) s√£o ferramentas essenciais na an√°lise de s√©ries temporais para identificar a ordem apropriada (p, q) de um modelo ARMA [^3.5]. A ACF mede a correla√ß√£o entre uma s√©rie temporal e suas defasagens, enquanto a PACF mede a correla√ß√£o entre a s√©rie e suas defasagens, removendo o efeito das defasagens intermedi√°rias.

Conforme mencionado anteriormente, a fun√ß√£o de autocovari√¢ncia $\gamma_j$ (e, portanto, a fun√ß√£o de autocorrela√ß√£o $\rho_j$) seguem a mesma equa√ß√£o de diferen√ßa de ordem p que o processo ap√≥s $q$ defasagens [^3.5.5]:
$$\gamma_j = \phi_1\gamma_{j-1} + \phi_2\gamma_{j-2} + \ldots + \phi_p\gamma_{j-p} \text{ para } j > q$$

Esta propriedade permite identificar a ordem $p$ do componente AR examinando o comportamento da ACF ap√≥s $q$ defasagens.

**Exemplo:** Considere um processo ARMA(2, 1) [^3.5] com coeficientes $\phi_1$, $\phi_2$, e $\theta_1$. Para lags $j > 1$, a fun√ß√£o de autocovari√¢ncia satisfaz a equa√ß√£o $\gamma_j = \phi_1 \gamma_{j-1} + \phi_2 \gamma_{j-2}$. Portanto, o comportamento de longo prazo da fun√ß√£o de autocorrela√ß√£o √© governado pelos par√¢metros autorregressivos.

> üí° **Exemplo Num√©rico:** Suponha que tenhamos um processo ARMA(2,1) com $\phi_1 = 0.6$ e $\phi_2 = 0.2$. Se soubermos que $\gamma_2 = 5$ e $\gamma_3 = 3$, podemos calcular $\gamma_4$ usando a equa√ß√£o da fun√ß√£o de autocovari√¢ncia:
>
> $\gamma_4 = \phi_1 \gamma_3 + \phi_2 \gamma_2 = 0.6 * 3 + 0.2 * 5 = 1.8 + 1 = 2.8$
>
> Isso ilustra como as autocovari√¢ncias anteriores podem ser usadas para prever as autocovari√¢ncias futuras com base nos coeficientes AR.

##### Determina√ß√£o da Ordem do Modelo Usando ACF e PACF
*   **Processo AR(p):** A PACF se anula ap√≥s a defasagem $p$, e a ACF diminui gradualmente.
*   **Processo MA(q):** A ACF se anula ap√≥s a defasagem $q$, e a PACF diminui gradualmente.
*   **Processo ARMA(p, q):** Ambas a ACF e a PACF diminuem gradualmente ap√≥s as defasagens $p$ e $q$, respectivamente [Proposi√ß√£o 1].

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos uma s√©rie temporal e, ao analisar suas ACF e PACF, observamos o seguinte:
>
> *   ACF: A ACF mostra valores significativos nos lags 1 e 2, mas se torna insignificante ap√≥s o lag 2.
> *   PACF: A PACF diminui gradualmente.
>
> Com base nessas observa√ß√µes, podemos inferir que a s√©rie temporal pode ser modelada como um processo MA(2), pois a ACF se anula ap√≥s o lag 2.
>
> Agora, considere um outro cen√°rio:
>
> *   ACF: A ACF diminui gradualmente.
> *   PACF: A PACF tem valores significativos nos lags 1 e 2, mas se torna insignificante ap√≥s o lag 2.
>
> Neste caso, podemos inferir que a s√©rie temporal pode ser modelada como um processo AR(2), pois a PACF se anula ap√≥s o lag 2.
>
> Se ambas, ACF e PACF, diminuem gradualmente, isso sugere um processo ARMA(p, q) com $p$ e $q$ maiores que zero.  Determinar as ordens exatas de $p$ e $q$ exigiria uma an√°lise mais aprofundada.

**Teorema 1** (Teorema de Wold): Qualquer processo estoc√°stico estacion√°rio puramente n√£o-determin√≠stico pode ser representado como um processo de m√©dia m√≥vel de ordem infinita. Ou seja, $Y_t = \mu + \sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}$, onde $\mu$ √© a m√©dia do processo, $\epsilon_t$ √© ru√≠do branco, e os coeficientes $\psi_i$ satisfazem $\sum_{i=0}^{\infty} |\psi_i| < \infty$.  Este teorema fornece uma base te√≥rica para representar s√©ries temporais estacion√°rias usando modelos ARMA.

**Teorema 1.1** (Consequ√™ncia do Teorema de Wold): Um processo ARMA(p, q) estacion√°rio pode ser expresso como um processo MA(‚àû). Al√©m disso, os coeficientes $\psi_i$ do processo MA(‚àû) decaem exponencialmente r√°pido.

*Prova (Esbo√ßo):* Dado que um processo ARMA(p, q) √© estacion√°rio, ele satisfaz as condi√ß√µes do Teorema de Wold. Portanto, ele pode ser representado como um processo MA(‚àû). A demonstra√ß√£o de que os coeficientes decaem exponencialmente r√°pido envolve a an√°lise das ra√≠zes do polin√¥mio AR e a sua rela√ß√£o com os coeficientes $\psi_i$.

**Simula√ß√£o em Python:**

```python
import numpy as np
import statsmodels.api as sm
from statsmodels.tsa.arima.model import ARIMA
import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Simular um processo ARMA(2,1)
np.random.seed(0)
n = 200
phi = np.array([0.6, 0.3])
theta = np.array([0.4])
errors = np.random.normal(0, 0, n)
y = np.zeros(n)

for t in range(2, n):
    y[t] = phi[0] * y[t-1] + phi[1] * y[t-2] + errors[t] + theta[0] * errors[t-1] if t > 0 else errors[t]

# Plotar ACF e PACF
fig, axes = plt.subplots(2, 1, figsize=(10, 8))
plot_acf(y, lags=20, ax=axes[0])
plot_pacf(y, lags=20, ax=axes[1])
plt.show()
```

Analisar os gr√°ficos ACF e PACF ajuda a determinar os valores apropriados de $p$ e $q$. No caso do ARMA(2,1), a PACF mostrar√° uma queda gradual, enquanto a ACF tamb√©m mostrar√° uma queda gradual [^3.5].

> üí° **Lembrete:**
>
> *   PACF: Mede a correla√ß√£o entre uma s√©rie temporal e seus valores defasados ap√≥s remover a influ√™ncia das defasagens intermedi√°rias.
> *   ACF: Mede a correla√ß√£o entre uma s√©rie temporal e seus valores defasados.

### Rela√ß√£o entre Coeficientes e Autocovari√¢ncias

A rela√ß√£o entre os coeficientes do modelo ARMA(p, q) ($\phi_i$ e $\theta_i$) e as autocovari√¢ncias $\gamma_j$ √© crucial para a estima√ß√£o de par√¢metros. Para um processo ARMA(p, q), ap√≥s $q$ defasagens, a fun√ß√£o de autocovari√¢ncia obedece a uma equa√ß√£o de diferen√ßa de ordem $p$:
$$\gamma_j = \phi_1\gamma_{j-1} + \phi_2\gamma_{j-2} + \cdots + \phi_p\gamma_{j-p}, \quad j > q$$

Esta rela√ß√£o permite a utiliza√ß√£o eficiente das equa√ß√µes de Yule-Walker generalizadas para estimar os coeficientes AR. Especificamente, dado um conjunto de autocovari√¢ncias amostrais, √© poss√≠vel configurar e resolver um sistema linear de equa√ß√µes para encontrar os valores de $\phi_1, \phi_2, \ldots, \phi_p$.

**Exemplo:** Para um processo ARMA(1, q), com $p=1$, e coeficiente AR $\phi$, para $j>q$, teremos:
$$\gamma_j = \phi \gamma_{j-1}$$

Isso significa que, ap√≥s a defasagem $q$, cada autocovari√¢ncia √© simplesmente um m√∫ltiplo da autocovari√¢ncia anterior, com o fator de multiplica√ß√£o sendo o coeficiente AR $\phi$.

> üí° **Exemplo Num√©rico:** Considere um processo ARMA(1,0) com $\phi = 0.8$. Se $\gamma_1 = 10$, ent√£o $\gamma_2 = 0.8 * 10 = 8$, $\gamma_3 = 0.8 * 8 = 6.4$, e assim por diante. Isto demonstra como a autocovari√¢ncia decai exponencialmente com o aumento das defasagens, com uma taxa determinada pelo coeficiente AR $\phi$.

#### Equa√ß√µes de Yule-Walker Generalizadas
As Equa√ß√µes de Yule-Walker s√£o um conjunto de equa√ß√µes que relacionam as autocovari√¢ncias de um processo ARMA(p, q) com seus par√¢metros. Estas equa√ß√µes s√£o √∫teis para estimar os par√¢metros AR e MA, e tamb√©m podem ser usadas para verificar a estacionaridade e invertibilidade do modelo.

Dado um processo ARMA(p, q):
$$Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t + \theta_1 \epsilon_{t-1} + ... + \theta_q \epsilon_{t-q}$$

Multiplicando ambos os lados por $Y_{t-k}$ e tomando a esperan√ßa, obtemos as Equa√ß√µes de Yule-Walker:
$$\gamma_k = \phi_1 \gamma_{k-1} + ... + \phi_p \gamma_{k-p} + \mathbb{E}[\epsilon_t Y_{t-k}] + \theta_1 \mathbb{E}[\epsilon_{t-1} Y_{t-k}] + ... + \theta_q \mathbb{E}[\epsilon_{t-q} Y_{t-k}]$$

Para $k > q$, os termos envolvendo $\epsilon_t$ s√£o zero, o que simplifica as equa√ß√µes e permite estimar os par√¢metros AR. As autocovari√¢ncias podem ent√£o ser usadas para estimar os par√¢metros MA.

**Lema 1.** Para um processo ARMA(p, q), $\mathbb{E}[\epsilon_{t-i}Y_{t-k}] = 0$ para $k > i$.

*Prova.* Se $k > i$, ent√£o $Y_{t-k}$ √© independente de $\epsilon_{t-i}$, pois $Y_{t-k}$ depende apenas de termos de erro at√© o tempo $t-k$, e $t-k < t-i$. Portanto, $\mathbb{E}[\epsilon_{t-i}Y_{t-k}] = \mathbb{E}[\epsilon_{t-i}]\mathbb{E}[Y_{t-k}] = 0$, j√° que $\mathbb{E}[\epsilon_{t-i}] = 0$.

**Prova do Lema 1:**
I.  Come√ßamos com a defini√ß√£o do lado esquerdo da equa√ß√£o: $\mathbb{E}[\epsilon_{t-i}Y_{t-k}]$.
II. Assumimos que $k > i$. Isso significa que o tempo $t-k$ √© anterior ao tempo $t-i$.
III. Como $Y_{t-k}$ depende apenas de termos de erro at√© o tempo $t-k$, ele √© independente de $\epsilon_{t-i}$.
IV. Portanto, a esperan√ßa do produto pode ser separada: $\mathbb{E}[\epsilon_{t-i}Y_{t-k}] = \mathbb{E}[\epsilon_{t-i}]\mathbb{E}[Y_{t-k}]$.
V. Como $\epsilon_{t-i}$ √© ru√≠do branco com m√©dia zero, temos $\mathbb{E}[\epsilon_{t-i}] = 0$.
VI. Substituindo isso na equa√ß√£o, obtemos $\mathbb{E}[\epsilon_{t-i}Y_{t-k}] = 0 \cdot \mathbb{E}[Y_{t-k}] = 0$.
VII. Assim, $\mathbb{E}[\epsilon_{t-i}Y_{t-k}] = 0$ para $k > i$. $\blacksquare$

**Lema 1.1.** Para um processo ARMA(p, q), $\mathbb{E}[\epsilon_t Y_{t-k}] = 0$ para $k > 0$.

*Prova.* Se $k > 0$, ent√£o $Y_{t-k}$ √© independente de $\epsilon_t$, pois $Y_{t-k}$ depende apenas de termos de erro at√© o tempo $t-k$, e $t-k < t$. Portanto, $\mathbb{E}[\epsilon_t Y_{t-k}] = \mathbb{E}[\epsilon_t]\mathbb{E}[Y_{t-k}] = 0$, j√° que $\mathbb{E}[\epsilon_t] = 0$.

**Prova do Lema 1.1:**
I.  Come√ßamos com a defini√ß√£o do lado esquerdo da equa√ß√£o: $\mathbb{E}[\epsilon_{t}Y_{t-k}]$.
II. Assumimos que $k > 0$. Isso significa que o tempo $t-k$ √© anterior ao tempo $t$.
III. Como $Y_{t-k}$ depende apenas de termos de erro at√© o tempo $t-k$, ele √© independente de $\epsilon_{t}$.
IV. Portanto, a esperan√ßa do produto pode ser separada: $\mathbb{E}[\epsilon_{t}Y_{t-k}] = \mathbb{E}[\epsilon_{t}]\mathbb{E}[Y_{t-k}]$.
V. Como $\epsilon_{t}$ √© ru√≠do branco com m√©dia zero, temos $\mathbb{E}[\epsilon_{t}] = 0$.
VI. Substituindo isso na equa√ß√£o, obtemos $\mathbb{E}[\epsilon_{t}Y_{t-k}] = 0 \cdot \mathbb{E}[Y_{t-k}] = 0$.
VII. Assim, $\mathbb{E}[\epsilon_{t}Y_{t-k}] = 0$ para $k > 0$. $\blacksquare$

#### Implementa√ß√£o em Python para Estima√ß√£o de Par√¢metros

Aqui est√° um exemplo em Python para estimar os par√¢metros de um ARMA(p, q) usando as equa√ß√µes de Yule-Walker:

```python
import numpy as np

def estimate_ar_params(autocovariances, p):
    """
    Estima os par√¢metros AR usando as equa√ß√µes de Yule-Walker.

    Args:
        autocovariances (list): Lista de autocovari√¢ncias.
        p (int): Ordem do modelo AR.

    Returns:
        numpy.ndarray: Coeficientes AR.
    """
    # Construir a matriz Yule-Walker
    R = np.zeros((p, p))
    for i in range(p):
        for j in range(p):
            R[i, j] = autocovariances[abs(i - j)]

    # Construir o vetor de autocovari√¢ncias
    r = np.array(autocovariances[1:p + 1])

    # Resolver as equa√ß√µes de Yule-Walker
    phi = np.linalg.solve(R, r)

    return phi

# Exemplo de uso
autocovariances = [4.0, 2.0, 1.0, 0.5, 0.25]  # Autocovari√¢ncias de exemplo
p = 2  # Ordem do modelo AR

phi = estimate_ar_params(autocovariances, p)

print("Coeficientes AR Estimados:", phi)
```

Este c√≥digo estima os par√¢metros AR de um processo AR(p) dado um conjunto de autocovari√¢ncias usando as equa√ß√µes de Yule-Walker.

> üí° **Exemplo Num√©rico:** Usando as autocovari√¢ncias fornecidas no exemplo anterior `autocovariances = [4.0, 2.0, 1.0, 0.5, 0.25]` e $p = 2$, a matriz $R$ e o vetor $r$ s√£o constru√≠dos da seguinte forma:
>
> $R = \begin{bmatrix} \gamma_0 & \gamma_1 \\ \gamma_1 & \gamma_0 \end{bmatrix} = \begin{bmatrix} 4.0 & 2.0 \\ 2.0 & 4.0 \end{bmatrix}$
>
> $r = \begin{bmatrix} \gamma_1 \\ \gamma_2 \end{bmatrix} = \begin{bmatrix} 2.0 \\ 1.0 \end{bmatrix}$
>
> Resolvendo o sistema linear $R \phi = r$, temos:
>
> $\begin{bmatrix} 4.0 & 2.0 \\ 2.0 & 4.0 \end{bmatrix} \begin{bmatrix} \phi_1 \\ \phi_2 \end{bmatrix} = \begin{bmatrix} 2.0 \\ 1.0 \end{bmatrix}$
>
> $\phi = \begin{bmatrix} \phi_1 \\ \phi_2 \end{bmatrix} = R^{-1} r = \begin{bmatrix} 0.333 \\ 0.0 \end{bmatrix}$ (aproximadamente)
>
> Portanto, os coeficientes AR estimados s√£o $\phi_1 \approx 0.333$ e $\phi_2 \approx 0.0$.

### Implica√ß√µes da Estrutura da Autocovari√¢ncia
A propriedade da fun√ß√£o de autocovari√¢ncia seguir uma equa√ß√£o de diferen√ßa de ordem $p$ ap√≥s $q$ defasagens tem implica√ß√µes importantes na identifica√ß√£o e estima√ß√£o de modelos ARMA(p, q):
*   **Identifica√ß√£o da Ordem do Modelo:** A an√°lise da estrutura da autocovari√¢ncia ajuda a determinar as ordens apropriadas $p$ e $q$ do modelo ARMA. Ao examinar as fun√ß√µes de autocorrela√ß√£o e autocorrela√ß√£o parcial, √© poss√≠vel identificar o comportamento de "corte" ou decaimento que sugere a ordem dos componentes AR e MA.
*   **Estimativa Eficiente dos Par√¢metros:** As equa√ß√µes de Yule-Walker fornecem um m√©todo eficiente para estimar os par√¢metros AR, especialmente quando a ordem $p$ √© conhecida. Ao resolver o sistema de equa√ß√µes lineares, √© poss√≠vel obter estimativas precisas e confi√°veis dos coeficientes AR.
*   **Diagn√≥stico do Modelo:** Comparar a fun√ß√£o de autocorrela√ß√£o te√≥rica do modelo com a fun√ß√£o de autocorrela√ß√£o amostral dos dados pode ajudar a diagnosticar a adequa√ß√£o do modelo. Se houver diferen√ßas significativas entre as duas fun√ß√µes, isso pode indicar que o modelo n√£o est√° capturando adequadamente a din√¢mica da s√©rie temporal.

#### Monitoramento das Ra√≠zes dos Polin√¥mios AR e MA

√â crucial verificar a estacionaridade e invertibilidade dos modelos ARMA monitorando as ra√≠zes dos polin√¥mios AR e MA. Em Python, isso pode ser implementado da seguinte forma:

```python
import numpy as np
from numpy.polynomial import Polynomial

def check_stationarity(ar_params):
    """
    Verifica a estacionaridade de um modelo AR.

    Args:
        ar_params (numpy.ndarray): Coeficientes AR.

    Returns:
        bool: True se o modelo √© estacion√°rio, False caso contr√°rio.
    """
    # Construir o polin√¥mio AR
    polynomial = np.concatenate(([1], -ar_params))
    # Encontrar as ra√≠zes do polin√¥mio
    roots = Polynomial(polynomial).roots()
    # Verificar se todas as ra√≠zes est√£o fora do c√≠rculo unit√°rio
    return np.all(np.abs(roots) > 1)

def check_invertibility(ma_params):
    """
    Verifica a invertibilidade de um modelo MA.

    Args:
        ma_params (numpy.ndarray): Coeficientes MA.

    Returns:
        bool: True se o modelo √© invert√≠vel, False caso contr√°rio.
    """
    # Construir o polin√¥mio MA
    polynomial = np.concatenate(([1], ma_params))
    # Encontrar as ra√≠zes do polin√¥mio
    roots = Polynomial(polynomial).roots()
    # Verificar se todas as ra√≠zes est√£o fora do c√≠rculo unit√°rio
    return np.all(np.abs(roots) > 1)

# Exemplo de uso
ar_params = np.array([0.6, 0.3])
ma_params = np.array([0.4])

is_stationary = check_stationarity(ar_params)
is_invertible = check_invertibility(ma_params)

print("√â Estacion√°rio:", is_stationary)
print("√â Invert√≠vel:", is_invertible)
```

Este c√≥digo usa a biblioteca `numpy.polynomial` para encontrar as ra√≠zes dos polin√¥mios AR e MA e verificar se todas as ra√≠zes est√£o fora do c√≠rculo unit√°rio, garantindo estacionaridade e invertibilidade. O que permite aplicar transforma√ß√µes ou ajustes caso as condi√ß√µes n√£o sejam cumpridas.

> üí° **Exemplo Num√©rico:**
>
> Para os par√¢metros AR $\phi_1 = 0.6$ e $\phi_2 = 0.3$, o polin√¥mio AR √© $1 - 0.6z - 0.3z^2$. Encontrando as ra√≠zes deste polin√¥mio, obtemos aproximadamente $z_1 = 2.41$ e $z_2 = -4.41$. Como ambos os valores absolutos das ra√≠zes s√£o maiores que 1, o modelo AR √© estacion√°rio.
>
> Para o par√¢metro MA $\theta_1 = 0.4$, o polin√¥mio MA √© $1 + 0.4z$. A raiz deste polin√¥mio √© $z = -2.5$. Como o valor absoluto da raiz √© maior que 1, o modelo MA √© invert√≠vel.

**Teorema 2:** (Condi√ß√µes de Estacionaridade e Invertibilidade)
Um processo ARMA(p, q) √© estacion√°rio se todas as ra√≠zes do polin√¥mio AR, $1 - \sum_{i=1}^{p} \phi_i z^i = 0$, estiverem fora do c√≠rculo unit√°rio (i.e., $|z| > 1$ para todas as ra√≠zes $z$). Similarmente, um processo ARMA(p, q) √© invert√≠vel se todas as ra√≠zes do polin√¥mio MA, $1 + \sum_{i=1}^{q} \theta_i z^i = 0$, estiverem fora do c√≠rculo unit√°rio.

**Teorema 2.1:** (Transforma√ß√£o para Estacionaridade e Invertibilidade) Se um processo ARMA(p,q) n√£o √© estacion√°rio ou invert√≠vel, pode-se aplicar transforma√ß√µes como diferencia√ß√£o (para estacionaridade) ou invers√£o dos par√¢metros MA (para invertibilidade) para obter um modelo equivalente que satisfa√ßa essas condi√ß√µes, desde que as ra√≠zes problem√°ticas n√£o estejam exatamente no c√≠rculo unit√°rio.

*Prova (Esbo√ßo):* A diferencia√ß√£o de uma s√©rie temporal remove tend√™ncias e torna a s√©rie mais estacion√°ria. A invers√£o dos par√¢metros MA envolve substituir $\theta_i$ por $-\theta_i$ e verificar se o novo modelo √© invert√≠vel. Se as ra√≠zes problem√°ticas est√£o exatamente no c√≠rculo unit√°rio, a diferencia√ß√£o pode n√£o ser suficiente e outras t√©cnicas podem ser necess√°rias.

### Conclus√£o

A fun√ß√£o de autocovari√¢ncia de um processo ARMA(p, q) segue uma equa√ß√£o de diferen√ßa de ordem $p$ ap√≥s $q$ defasagens, o que tem implica√ß√µes significativas para a identifica√ß√£o e estima√ß√£o do modelo. Ao aproveitar esta propriedade estrutural, podemos desenvolver m√©todos eficientes para estimar os par√¢metros AR, verificar a estacionaridade e invertibilidade, e diagnosticar a adequa√ß√£o do modelo. Isso resulta em uma an√°lise mais robusta e precisa da din√¢mica da s√©rie temporal.

### Refer√™ncias
[^3.2.1]: E(Œµ_t) = 0
[^3.2.2]: E(Œµ_t^2) = œÉ^2
[^3.5]: An ARMA(p, q) process includes both autoregressive and moving average terms: Y_t = c + Œ¶‚ÇÅY_{t-1} + Œ¶‚ÇÇY_{t-2} + ... + Œ¶_pY_{t-p} + Œµ_t + Œ∏_1Œµ_{t-1} + Œ∏_2Œµ_{t-2} + ¬∑¬∑¬∑ + Œ∏_qŒµ_{t-q}.
[^3.5.1]: Y‚ÇÅ = c + Œ¶1Y-1 + $21-2++ pY‚ÇÅ-p + + Œ∏Œµ
[^3.5.5]: Œ•; = Œ¶17-1 + Œ¶2Yj-2 + ... + Œ¶—Ä—É—ñ-—Ä for j = q + 1, q+ 2,...
[^3.8]: Stationarity for AR(1) Model

**Proposi√ß√£o 1:** A fun√ß√£o de autocorrela√ß√£o parcial (PACF) de um processo ARMA(p,q) se anula ap√≥s a defasagem $p$ para um processo AR(p), e decai para um processo MA(q).

*Prova da Proposi√ß√£o 1:*

I. Consideremos um processo AR(p): $Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \cdots + \phi_p Y_{t-p} + \epsilon_t$.

II. A fun√ß√£o de autocorrela√ß√£o parcial (PACF) mede a correla√ß√£o entre $Y_t$ e $Y_{t-k}$ ap√≥s remover a influ√™ncia das defasagens intermedi√°rias $Y_{t-1}, Y_{t-2}, \ldots, Y_{t-(k-1)}$.

III. Para um processo AR(p), a depend√™ncia direta de $Y_t$ em $Y_{t-k}$ cessa ap√≥s a defasagem $p$. Isso significa que, para $k > p$, a correla√ß√£o parcial entre $Y_t$ e $Y_{t-k}$, condicionada em $Y_{t-1}, Y_{t-2}, \ldots, Y_{t-(k-1)}$, √© zero.

IV. Portanto, a PACF se anula ap√≥s a defasagem $p$ para um processo AR(p).

V. Para um processo MA(q), a PACF n√£o se anula abruptamente, mas decai gradualmente. Isso ocorre porque, embora a depend√™ncia direta cesse ap√≥s $q$ defasagens, as correla√ß√µes parciais ainda podem existir devido √† estrutura da m√©dia m√≥vel.

VI. Assim, conclu√≠mos que a PACF se anula ap√≥s a defasagem $p$ para um processo AR(p) e decai para um processo MA(q). $\blacksquare$
<!-- END -->