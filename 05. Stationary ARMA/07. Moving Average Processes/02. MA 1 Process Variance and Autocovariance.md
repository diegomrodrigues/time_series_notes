## Moving Average Processes: Variance and Autocovariance Analysis

### Introdu√ß√£o
Expanding on the concepts of **stationary time series** introduced in Chapter 3 [^44, ^45], this chapter delves into the specifics de **Moving Average (MA) processes**, com foco no processo MA(1) [^48]. Especificamente, analisaremos a vari√¢ncia e a autocovari√¢ncia do processo MA(1), demonstrando como essas medidas revelam caracter√≠sticas importantes da depend√™ncia temporal da s√©rie. Como discutido anteriormente, a an√°lise da estrutura de autocovari√¢ncia √© crucial para entender e modelar s√©ries temporais, e o processo MA(1) oferece um exemplo fundamental para ilustrar esses conceitos.

### Conceitos Fundamentais

O processo MA(1) √© definido como [^48]:

$$Y_t = \mu + \epsilon_t + \theta\epsilon_{t-1}$$,

onde $\mu$ √© a m√©dia do processo, $\{\epsilon_t\}$ √© um processo de **white noise** com m√©dia zero e vari√¢ncia $\sigma^2$ [^47], e $\theta$ √© um par√¢metro constante.

> üí° **Exemplo Num√©rico:** Suponha que $\mu = 10$, $\theta = 0.5$, e $\epsilon_t$ segue uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\sigma^2 = 1$.  Ent√£o, $Y_t = 10 + \epsilon_t + 0.5\epsilon_{t-1}$. Se $\epsilon_t = 0.8$ e $\epsilon_{t-1} = -0.4$, ent√£o $Y_t = 10 + 0.8 + 0.5(-0.4) = 10 + 0.8 - 0.2 = 10.6$.

**Vari√¢ncia do Processo MA(1):**

Para calcular a vari√¢ncia do processo MA(1), primeiramente, subtra√≠mos a m√©dia $\mu$ de ambos os lados da equa√ß√£o:

$$Y_t - \mu = \epsilon_t + \theta\epsilon_{t-1}$$.

Em seguida, elevamos ao quadrado ambos os lados e calculamos a esperan√ßa [^48]:

$$E[(Y_t - \mu)^2] = E[(\epsilon_t + \theta\epsilon_{t-1})^2] = E[\epsilon_t^2 + 2\theta\epsilon_t\epsilon_{t-1} + \theta^2\epsilon_{t-1}^2]$$.

Como $\{\epsilon_t\}$ √© um processo de white noise, temos $E[\epsilon_t^2] = \sigma^2$ [^47], $E[\epsilon_{t-1}^2] = \sigma^2$, e $E[\epsilon_t\epsilon_{t-1}] = 0$ [^47, ^48] para $t \neq \tau$. Portanto, a vari√¢ncia de $Y_t$ √© [^48]:

$$Var(Y_t) = E[(Y_t - \mu)^2] = \sigma^2 + \theta^2\sigma^2 = (1 + \theta^2)\sigma^2$$.

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior, onde $\theta = 0.5$ e $\sigma^2 = 1$, a vari√¢ncia do processo MA(1) √© $Var(Y_t) = (1 + 0.5^2) * 1 = (1 + 0.25) * 1 = 1.25$. Isso significa que a variabilidade em $Y_t$ √© 1.25 vezes a variabilidade do white noise $\epsilon_t$.

A vari√¢ncia do processo MA(1) depende tanto da vari√¢ncia do white noise ($\sigma^2$) quanto do par√¢metro $\theta$ [^48]. Um valor maior de $|\theta|$ implica uma maior vari√¢ncia, indicando que o termo defasado $\epsilon_{t-1}$ tem um impacto maior na variabilidade de $Y_t$.

**Autocovari√¢ncia do Processo MA(1):**

A autocovari√¢ncia de ordem $j$ √© definida como $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$ [^45]. Para o processo MA(1), vamos calcular a autocovari√¢ncia de primeira ordem ($j=1$) [^48]:

$$\gamma_1 = E[(Y_t - \mu)(Y_{t-1} - \mu)] = E[(\epsilon_t + \theta\epsilon_{t-1})(\epsilon_{t-1} + \theta\epsilon_{t-2})] = E[\epsilon_t\epsilon_{t-1} + \theta\epsilon_{t-1}^2 + \theta\epsilon_t\epsilon_{t-2} + \theta^2\epsilon_{t-1}\epsilon_{t-2}]$$.

Usando as propriedades do white noise, $E[\epsilon_t\epsilon_{t-1}] = 0$ [^47], $E[\epsilon_t\epsilon_{t-2}] = 0$ [^47] e $E[\epsilon_{t-1}\epsilon_{t-2}] = 0$ [^47], obtemos [^48]:

$$\gamma_1 = E[\theta\epsilon_{t-1}^2] = \theta\sigma^2$$.

Este resultado demonstra que a autocovari√¢ncia de primeira ordem para um processo MA(1) √© diretamente proporcional ao par√¢metro $\theta$ e √† vari√¢ncia do white noise $\sigma^2$ [^48].

> üí° **Exemplo Num√©rico:** Com $\theta = 0.5$ e $\sigma^2 = 1$, a autocovari√¢ncia de primeira ordem √© $\gamma_1 = 0.5 * 1 = 0.5$.  Isso indica uma correla√ß√£o positiva entre $Y_t$ e $Y_{t-1}$.

Para autocovari√¢ncias de ordem superior ($j > 1$), temos [^48]:

$$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\epsilon_t + \theta\epsilon_{t-1})(\epsilon_{t-j} + \theta\epsilon_{t-j-1})] = 0$$,

pois todos os termos envolvem o produto de erros em diferentes instantes de tempo, cuja esperan√ßa √© zero [^48]. Isso implica que o processo MA(1) tem uma "mem√≥ria" curta, onde a depend√™ncia entre as observa√ß√µes se estende apenas a um √∫nico per√≠odo de tempo [^48].

**Lema 1:** *Uniqueness of the MA(1) Representation*

For a given autocovariance function $\{\gamma_0, \gamma_1, 0, 0, \ldots \}$, where $\gamma_0 > 0$, there exist two possible real values of $\theta$ which yield the same autocovariance function. This lemma highlights the non-uniqueness of the parameter $\theta$.

*Proof:*
Given $\gamma_0 = (1 + \theta^2)\sigma^2$ and $\gamma_1 = \theta\sigma^2$, we can express $\sigma^2$ as $\sigma^2 = \frac{\gamma_1}{\theta}$. Substituting this into the equation for $\gamma_0$, we get $\gamma_0 = (1 + \theta^2)\frac{\gamma_1}{\theta}$, which simplifies to $\gamma_0\theta = \gamma_1 + \gamma_1\theta^2$, or $\gamma_1\theta^2 - \gamma_0\theta + \gamma_1 = 0$.
The solutions for $\theta$ are given by the quadratic formula:
$$\theta = \frac{\gamma_0 \pm \sqrt{\gamma_0^2 - 4\gamma_1^2}}{2\gamma_1}$$.
Since the discriminant $\gamma_0^2 - 4\gamma_1^2$ must be non-negative for real solutions, we need $\gamma_0^2 \geq 4\gamma_1^2$. Since $\gamma_0 = (1 + \theta^2)\sigma^2$ and $\gamma_1 = \theta\sigma^2$, we have $((1 + \theta^2)\sigma^2)^2 \geq 4(\theta\sigma^2)^2$, which simplifies to $(1 + \theta^2)^2 \geq 4\theta^2$, or $1 + 2\theta^2 + \theta^4 \geq 4\theta^2$. This further simplifies to $\theta^4 - 2\theta^2 + 1 \geq 0$, which is $(\theta^2 - 1)^2 \geq 0$. This condition is always true for real $\theta$, which means that there are always two solutions, except when the discriminant is zero, which implies that $\theta = 1$ or $\theta = -1$ (it corresponds to the solution $\theta = 1$). Furthermore, if $\theta_1$ is a solution, so is $\frac{1}{\theta_1}$, thus proving the non-uniqueness.

> üí° **Exemplo Num√©rico:** Suponha $\gamma_0 = 1.25$ e $\gamma_1 = 0.5$. Usando a f√≥rmula, $\theta = \frac{1.25 \pm \sqrt{1.25^2 - 4(0.5)^2}}{2(0.5)} = \frac{1.25 \pm \sqrt{1.5625 - 1}}{1} = 1.25 \pm \sqrt{0.5625} = 1.25 \pm 0.75$.  As duas solu√ß√µes s√£o $\theta_1 = 2$ e $\theta_2 = 0.5$. Ambos os valores de $\theta$ resultam na mesma fun√ß√£o de autocovari√¢ncia.

**Lema 1.1:** *Product of MA(1) Solutions*
If $\theta_1$ and $\theta_2$ are the two solutions for $\theta$ in the MA(1) representation obtained in Lema 1, then $\theta_1 \cdot \theta_2 = 1$.

*Proof:*
From the proof of Lema 1, we have the quadratic equation $\gamma_1\theta^2 - \gamma_0\theta + \gamma_1 = 0$. According to Vieta's formulas, the product of the roots of the quadratic equation $ax^2 + bx + c = 0$ is given by $\frac{c}{a}$. In this case, $a = \gamma_1$ and $c = \gamma_1$, so the product of the roots $\theta_1$ and $\theta_2$ is $\theta_1 \cdot \theta_2 = \frac{\gamma_1}{\gamma_1} = 1$.

> üí° **Exemplo Num√©rico:** No exemplo anterior, as solu√ß√µes foram $\theta_1 = 2$ e $\theta_2 = 0.5$.  O produto dessas solu√ß√µes √© $2 * 0.5 = 1$, o que valida o Lema 1.1.

**Autocorrela√ß√£o:**

A autocorrela√ß√£o de ordem *j* ($\rho_j$) √© definida como a autocovari√¢ncia de ordem *j* dividida pela vari√¢ncia do processo [^49]: $\rho_j = \frac{\gamma_j}{\gamma_0}$, onde $\gamma_0$ √© a vari√¢ncia [^45, ^49].

Para o processo MA(1), temos [^49]:

$$\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{\theta\sigma^2}{(1+\theta^2)\sigma^2} = \frac{\theta}{1+\theta^2}$$.

E para $j > 1$, $\rho_j = 0$ [^49].

Observe que $|\rho_1| \leq 0.5$ [^49], ou seja, o valor m√°ximo da autocorrela√ß√£o de primeira ordem √© 0.5, ocorrendo quando $\theta = 1$ [^49].

**Prova:**
Provaremos que $|\rho_1| \leq 0.5$.

I. Definimos a autocorrela√ß√£o de primeira ordem como:
$$\rho_1 = \frac{\theta}{1 + \theta^2}$$

II. Para encontrar o valor m√°ximo de $\rho_1$, derivamos $\rho_1$ em rela√ß√£o a $\theta$ e igualamos a zero:
$$\frac{d\rho_1}{d\theta} = \frac{(1 + \theta^2)(1) - \theta(2\theta)}{(1 + \theta^2)^2} = \frac{1 - \theta^2}{(1 + \theta^2)^2}$$

III. Igualando a derivada a zero:
$$\frac{1 - \theta^2}{(1 + \theta^2)^2} = 0$$
$$1 - \theta^2 = 0$$
$$\theta^2 = 1$$
$$\theta = \pm 1$$

IV. Calculamos $\rho_1$ para $\theta = 1$ e $\theta = -1$:
Para $\theta = 1$:
$$\rho_1 = \frac{1}{1 + 1^2} = \frac{1}{2} = 0.5$$
Para $\theta = -1$:
$$\rho_1 = \frac{-1}{1 + (-1)^2} = \frac{-1}{2} = -0.5$$

V. Para confirmar que $\theta = \pm 1$ correspondem a um m√°ximo ou m√≠nimo, podemos usar a segunda derivada, mas √© mais direto observar que quando $\theta = 0$, $\rho_1 = 0$, e quando $\theta$ tende ao infinito, $\rho_1$ tende a 0. Portanto, $\theta = 1$ corresponde ao m√°ximo positivo e $\theta = -1$ corresponde ao m√≠nimo negativo.

VI. Portanto, o valor m√°ximo de $\rho_1$ √© 0.5, e o valor m√≠nimo √© -0.5. Isso implica que:
$$|\rho_1| \leq 0.5$$
‚ñ†

> üí° **Exemplo Num√©rico:** Usando $\theta = 0.5$, a autocorrela√ß√£o de primeira ordem √© $\rho_1 = \frac{0.5}{1 + 0.5^2} = \frac{0.5}{1.25} = 0.4$.  Este valor est√° dentro do limite de $|\rho_1| \leq 0.5$. Se usarmos $\theta = 1$, ent√£o $\rho_1 = \frac{1}{1 + 1^2} = 0.5$, demonstrando o valor m√°ximo poss√≠vel.

**Teorema 1:** *Invertibility of the MA(1) Process*

An MA(1) process $Y_t = \mu + \epsilon_t + \theta\epsilon_{t-1}$ is invertible if and only if $|\theta| < 1$.

*Proof:*
An MA(1) process $Y_t = \mu + \epsilon_t + \theta\epsilon_{t-1}$ is invertible if it can be written as an infinite autoregressive (AR) process:

$$\epsilon_t = \sum_{i=0}^{\infty} \pi_i (Y_{t-i} - \mu)$$.

The invertibility condition ensures that the weights $\pi_i$ decay as $i$ increases, meaning that the influence of past $Y_t$ values diminishes over time.
Using backshift operator notation, $Y_t = \mu + (1 + \theta B)\epsilon_t$, where $B$ is the backshift operator ($B\epsilon_t = \epsilon_{t-1}$). For invertibility, we need to express $\epsilon_t$ as a function of past $Y_t$ values:

$$\epsilon_t = \frac{1}{1 + \theta B}(Y_t - \mu)$$.

Expanding $\frac{1}{1 + \theta B}$ as a geometric series, we get:

$$\epsilon_t = (1 - \theta B + \theta^2 B^2 - \theta^3 B^3 + \ldots)(Y_t - \mu)$$.

This expansion converges if and only if $|\theta B| < 1$, which is equivalent to $|\theta| < 1$. If $|\theta| < 1$, the coefficients decay geometrically, ensuring that the past values of $Y_t$ have a diminishing impact on the current value of $\epsilon_t$, satisfying the invertibility condition. Conversely, if $|\theta| \geq 1$, the coefficients do not decay, and the influence of past $Y_t$ values does not diminish, violating the invertibility condition.

> üí° **Exemplo Num√©rico:** Se $\theta = 0.8$ (onde $|\theta| < 1$), o processo √© invert√≠vel. Podemos expressar $\epsilon_t$ como uma soma ponderada de valores passados de $Y_t$. Se $\theta = 1.2$ (onde $|\theta| > 1$), o processo n√£o √© invert√≠vel, e a representa√ß√£o em termos de valores passados n√£o converge.

**Corol√°rio 1.1:** *Consequences of Non-Invertibility*
If $|\theta| \geq 1$, the MA(1) process is not invertible, implying that the process cannot be uniquely represented by its past values. In this case, an alternative invertible representation with parameter $1/\theta$ can be found, resulting in the same autocovariance function.

Building upon the concept of invertibility, we can analyze the implications for forecasting:

**Teorema 2:** *Optimal Forecasts for Invertible MA(1) Processes*

For an invertible MA(1) process, the optimal one-step-ahead forecast is a linear function of the current and past observed values.

*Proof:*
Since the MA(1) process is invertible, we can express $\epsilon_t$ as an infinite autoregressive process:
$$\epsilon_t = (1 - \theta B + \theta^2 B^2 - \theta^3 B^3 + \ldots)(Y_t - \mu)$$.

The optimal one-step-ahead forecast, denoted by $Y_{t+1|t}$, is the conditional expectation of $Y_{t+1}$ given the information available up to time $t$:
$$Y_{t+1|t} = E[Y_{t+1} | Y_t, Y_{t-1}, Y_{t-2}, \ldots]$$.

Substituting the MA(1) equation for $Y_{t+1}$, we have:
$$Y_{t+1|t} = E[\mu + \epsilon_{t+1} + \theta\epsilon_t | Y_t, Y_{t-1}, Y_{t-2}, \ldots]$$.

Since $E[\epsilon_{t+1} | Y_t, Y_{t-1}, Y_{t-2}, \ldots] = 0$, we get:
$$Y_{t+1|t} = \mu + \theta E[\epsilon_t | Y_t, Y_{t-1}, Y_{t-2}, \ldots]$$.

Now, we can substitute the invertible representation of $\epsilon_t$:
$$Y_{t+1|t} = \mu + \theta (1 - \theta B + \theta^2 B^2 - \theta^3 B^3 + \ldots)(Y_t - \mu)$$.

This shows that the optimal one-step-ahead forecast is a linear function of the current and past observed values $Y_t, Y_{t-1}, Y_{t-2}, \ldots$. The weights decay geometrically, reflecting the decreasing influence of past observations on the current forecast.

> üí° **Exemplo Num√©rico:** Seja $\mu = 10$, $\theta = 0.5$, e suponha que a s√©rie temporal seja $Y_t = 10.5$, $Y_{t-1} = 9.8$, $Y_{t-2} = 10.2$.  Ent√£o, $\epsilon_t = Y_t - \mu - \theta \epsilon_{t-1}$.  Para aproximar $\epsilon_t$, podemos usar $\epsilon_t \approx (1 - \theta B)(Y_t - \mu) = (Y_t - \mu) - \theta (Y_{t-1} - \mu) = (10.5 - 10) - 0.5(9.8 - 10) = 0.5 - 0.5(-0.2) = 0.5 + 0.1 = 0.6$. Assim, a previs√£o de um passo √† frente √© $Y_{t+1|t} = \mu + \theta \epsilon_t = 10 + 0.5(0.6) = 10.3$.

### Conclus√£o

A an√°lise da vari√¢ncia e autocovari√¢ncia do processo MA(1) revela caracter√≠sticas importantes da depend√™ncia temporal da s√©rie. A vari√¢ncia √© influenciada tanto pela vari√¢ncia do white noise quanto pelo par√¢metro $\theta$, enquanto a autocovari√¢ncia de primeira ordem demonstra uma depend√™ncia entre observa√ß√µes adjacentes, e as autocovari√¢ncias de ordem superior s√£o nulas, indicando uma mem√≥ria curta [^48]. Esses resultados s√£o cruciais para a identifica√ß√£o e modelagem de s√©ries temporais que podem ser representadas por processos MA(1). O uso da fun√ß√£o de autocovari√¢ncia (ACF) e da fun√ß√£o de autocorrela√ß√£o parcial (PACF) para identificar a ordem de processos MA ser√° explorado em cap√≠tulos subsequentes.

### Refer√™ncias
[^44]: Imagine a battery of I such computers generating sequences {y{1}, {y{2} ...
[^45]: The variance of the random variable Y, (denoted you) is similarly defined as...
[^47]: The basic building block for all the processes considered in this chapter is a sequence {8} - whose elements have mean zero and variance œÉ¬≤,
[^48]: Let {8} be white noise as in [3.2.1] through [3.2.3], and consider the process...
[^49]: The jth autocorrelation of a covariance-stationary process (denoted p‚ÇÅ) is defined as its jth autocovariance divided by the variance:
<!-- END -->