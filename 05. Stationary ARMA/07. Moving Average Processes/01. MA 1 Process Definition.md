## An√°lise Detalhada do Processo de M√©dia M√≥vel de Primeira Ordem (MA(1))

### Introdu√ß√£o
Este cap√≠tulo aprofunda-se na an√°lise de **processos de m√©dias m√≥veis (MA)**, com um foco especial no processo de primeira ordem, MA(1). Em continuidade aos conceitos de **expectativa**, **estacionariedade** e **ergodicidade** apresentados anteriormente no Cap√≠tulo 3 [^44], exploraremos as propriedades estat√≠sticas e as caracter√≠sticas inerentes aos processos MA(1). Este modelo serve como um bloco de constru√ß√£o fundamental para a compreens√£o de modelos de s√©ries temporais mais complexos, como os **modelos ARMA (Autoregressive Moving Average)**.

### Conceitos Fundamentais
O processo MA(1) √© definido pela seguinte equa√ß√£o [^48]:

$$Y_t = \mu + \epsilon_t + \theta\epsilon_{t-1}$$

Onde [^48]:
*   $Y_t$ representa o valor da s√©rie temporal no instante $t$.
*   $\mu$ √© uma constante que representa a m√©dia do processo.
*   $\epsilon_t$ √© um termo de erro de ru√≠do branco no instante $t$, com m√©dia zero e vari√¢ncia constante $\sigma^2$ [^47]. Os $\epsilon_t$ s√£o n√£o correlacionados ao longo do tempo, ou seja, $E(\epsilon_t \epsilon_\tau) = 0$ para $t \neq \tau$ [^47].
*   $\theta$ √© um par√¢metro constante que determina o peso do termo de erro defasado $\epsilon_{t-1}$.

A denomina√ß√£o "m√©dia m√≥vel" deriva da estrutura do modelo, onde o valor atual $Y_t$ √© expresso como uma m√©dia ponderada dos dois valores mais recentes do ru√≠do branco $\epsilon_t$ [^48].

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um processo MA(1) com $\mu = 10$, $\theta = 0.5$ e $\sigma^2 = 1$. Podemos simular alguns valores desse processo. Primeiro, simulamos valores para $\epsilon_t$:
>
> ```python
> import numpy as np
> import pandas as pd
> import matplotlib.pyplot as plt
>
> np.random.seed(42)  # Para reprodutibilidade
> n = 100
> epsilon = np.random.normal(0, 1, n)
>
> # Par√¢metros do MA(1)
> mu = 10
> theta = 0.5
>
> # Inicializar a s√©rie temporal Y
> Y = np.zeros(n)
>
> # Gerar a s√©rie temporal MA(1)
> Y[0] = mu + epsilon[0]  # Valor inicial
> for t in range(1, n):
>     Y[t] = mu + epsilon[t] + theta * epsilon[t-1]
>
> # Criar um DataFrame para facilitar a visualiza√ß√£o
> data = pd.DataFrame({'Epsilon': epsilon, 'Y': Y})
>
> # Plotar a s√©rie temporal
> plt.figure(figsize=(12, 6))
> plt.plot(data['Y'], label='Y_t (MA(1) Process)')
> plt.xlabel('Time (t)')
> plt.ylabel('Value')
> plt.title('Simulated MA(1) Time Series')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> O gr√°fico exibir√° uma s√©rie temporal simulada do processo MA(1).
>
> Para $t = 1$, temos $Y_1 = \mu + \epsilon_1 + \theta\epsilon_0$. Se $\epsilon_0 = 0.2$ e $\epsilon_1 = -0.5$, ent√£o:
>
> $Y_1 = 10 + (-0.5) + 0.5(0.2) = 10 - 0.5 + 0.1 = 9.6$
>
> Para $t = 2$, temos $Y_2 = \mu + \epsilon_2 + \theta\epsilon_1$. Se $\epsilon_2 = 1.0$, ent√£o:
>
> $Y_2 = 10 + 1.0 + 0.5(-0.5) = 11.0 - 0.25 = 10.75$
>
> Podemos continuar simulando valores para obter uma s√©rie temporal.

**Expectativa do Processo MA(1)**

A expectativa (ou m√©dia) do processo MA(1) √© calculada da seguinte forma [^48]:

$$E(Y_t) = E(\mu + \epsilon_t + \theta\epsilon_{t-1}) = \mu + E(\epsilon_t) + \theta E(\epsilon_{t-1}) = \mu$$

Dado que o ru√≠do branco $\epsilon_t$ tem m√©dia zero, $E(\epsilon_t) = E(\epsilon_{t-1}) = 0$ [^47]. Assim, a m√©dia do processo MA(1) √© simplesmente a constante $\mu$, indicando que o processo tem uma m√©dia constante ao longo do tempo [^48].

> üí° **Exemplo Num√©rico:**
>
> Usando o exemplo anterior com $\mu = 10$, a expectativa do processo MA(1) √© simplesmente $E(Y_t) = \mu = 10$. Isso significa que, em m√©dia, os valores da s√©rie temporal flutuar√£o em torno de 10.

**Estacionariedade e Ergodicidade**

Como a m√©dia e as autocovari√¢ncias de um processo MA(1) n√£o s√£o fun√ß√µes do tempo, um processo MA(1) √© covariance-stationary, independentemente do valor de $\theta$ [^48]. Adicionalmente, a condi√ß√£o [3.1.15] do Cap√≠tulo 3 √© satisfeita [^48]:

$$\sum_{j=0}^{\infty} |\gamma_j| < \infty$$

Portanto, se $\{\epsilon_t\}$ for ru√≠do branco Gaussiano, o processo MA(1) √© ergodico para todos os momentos [^48].

Para complementar a discuss√£o sobre estacionariedade, podemos explicitar as condi√ß√µes sob as quais o processo MA(1) √© invert√≠vel.

**Invertibilidade do Processo MA(1)**

Um processo MA(1) √© dito invert√≠vel se pudermos expressar $\epsilon_t$ em termos de $Y_t$ e seus valores passados. Para o processo MA(1), podemos reescrever a equa√ß√£o como:

$$\epsilon_t = Y_t - \mu - \theta\epsilon_{t-1}$$

Podemos expressar $\epsilon_t$ como uma soma infinita de $Y_t$ e seus valores passados se $|\theta| < 1$. Para ver isso, podemos reescrever a equa√ß√£o acima recursivamente:

$$\epsilon_t = (Y_t - \mu) - \theta(Y_{t-1} - \mu) + \theta^2(Y_{t-2} - \mu) - \theta^3(Y_{t-3} - \mu) + \dots$$

$$\epsilon_t = \sum_{i=0}^{\infty} (-\theta)^i (Y_{t-i} - \mu)$$

Esta representa√ß√£o √© convergente se $|\theta| < 1$. Portanto, a condi√ß√£o para invertibilidade do processo MA(1) √© $|\theta| < 1$.

> üí° **Exemplo Num√©rico:**
>
> Se $\theta = 0.5$ (invert√≠vel), ent√£o o impacto de um choque passado diminui a cada per√≠odo. Por exemplo, o impacto do choque $\epsilon_{t-1}$ √© 0.5, o impacto de $\epsilon_{t-2}$ √© $0.5^2 = 0.25$, e assim por diante.
>
> Se $\theta = 2$ (n√£o invert√≠vel), o impacto de um choque passado aumenta a cada per√≠odo. Por exemplo, o impacto do choque $\epsilon_{t-1}$ √© 2, o impacto de $\epsilon_{t-2}$ √© $2^2 = 4$, e assim por diante. Isso pode levar a um comportamento inst√°vel e n√£o estacion√°rio.

**Observa√ß√£o:** A condi√ß√£o de invertibilidade $|\theta| < 1$ garante que o impacto de choques passados diminua exponencialmente ao longo do tempo. Caso $|\theta| \geq 1$, o impacto dos choques passados pode aumentar ou permanecer constante, o que pode levar a comportamentos n√£o estacion√°rios.

**Vari√¢ncia do Processo MA(1)**
A vari√¢ncia do processo MA(1) √© dada por [^48]:

$$Var(Y_t) = E[(Y_t - \mu)^2] = E[(\epsilon_t + \theta\epsilon_{t-1})^2] = E[\epsilon_t^2 + 2\theta\epsilon_t\epsilon_{t-1} + \theta^2\epsilon_{t-1}^2]$$

$$= E[\epsilon_t^2] + 2\theta E[\epsilon_t\epsilon_{t-1}] + \theta^2 E[\epsilon_{t-1}^2] = \sigma^2 + 0 + \theta^2\sigma^2 = (1 + \theta^2)\sigma^2$$

A vari√¢ncia de $Y_t$ √© constante e n√£o depende de $t$, o que refor√ßa a estacionariedade do processo [^48].

> üí° **Exemplo Num√©rico:**
>
> Se $\theta = 0.5$ e $\sigma^2 = 1$, ent√£o a vari√¢ncia do processo MA(1) √©:
>
> $Var(Y_t) = (1 + 0.5^2) \times 1 = 1 + 0.25 = 1.25$
>
> Isso significa que a dispers√£o dos valores da s√©rie temporal em torno da m√©dia (10) √© de 1.25.

**Autocovari√¢ncia do Processo MA(1)**
A autocovari√¢ncia de primeira ordem ($j=1$) √© calculada como [^48]:
$$Cov(Y_t, Y_{t-1}) = E[(Y_t - \mu)(Y_{t-1} - \mu)] = E[(\epsilon_t + \theta\epsilon_{t-1})(\epsilon_{t-1} + \theta\epsilon_{t-2})] = E[\epsilon_t\epsilon_{t-1} + \theta\epsilon_{t-1}^2 + \theta\epsilon_t\epsilon_{t-2} + \theta^2\epsilon_{t-1}\epsilon_{t-2}]$$

$$ = 0 + \theta E[\epsilon_{t-1}^2] + 0 + 0 = \theta\sigma^2$$

Para clareza, podemos formalizar o c√°lculo de $Cov(Y_t, Y_{t-1})$:
*Prova:*
I. Come√ßamos com a defini√ß√£o:
    $$Cov(Y_t, Y_{t-1}) = E[(Y_t - \mu)(Y_{t-1} - \mu)]$$

II. Substitu√≠mos as express√µes de $Y_t$ e $Y_{t-1}$:
    $$Cov(Y_t, Y_{t-1}) = E[(\epsilon_t + \theta\epsilon_{t-1})(\epsilon_{t-1} + \theta\epsilon_{t-2})]$$

III. Expandimos o produto:
     $$Cov(Y_t, Y_{t-1}) = E[\epsilon_t\epsilon_{t-1} + \theta\epsilon_{t-1}^2 + \theta\epsilon_t\epsilon_{t-2} + \theta^2\epsilon_{t-1}\epsilon_{t-2}]$$

IV. Aplicamos a linearidade da expectativa:
    $$Cov(Y_t, Y_{t-1}) = E[\epsilon_t\epsilon_{t-1}] + \theta E[\epsilon_{t-1}^2] + \theta E[\epsilon_t\epsilon_{t-2}] + \theta^2 E[\epsilon_{t-1}\epsilon_{t-2}]$$

V. Usamos as propriedades do ru√≠do branco: $E[\epsilon_i\epsilon_j] = 0$ para $i \neq j$ e $E[\epsilon_i^2] = \sigma^2$:
    $$Cov(Y_t, Y_{t-1}) = 0 + \theta\sigma^2 + 0 + 0 = \theta\sigma^2$$

Assim, $Cov(Y_t, Y_{t-1}) = \theta\sigma^2$ ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Se $\theta = 0.5$ e $\sigma^2 = 1$, ent√£o a autocovari√¢ncia de primeira ordem √©:
>
> $Cov(Y_t, Y_{t-1}) = 0.5 \times 1 = 0.5$
>
> Isso significa que h√° uma correla√ß√£o positiva entre os valores da s√©rie temporal em per√≠odos adjacentes.

As autocovari√¢ncias de ordem superior ($j > 1$) s√£o zero [^48]:

$$Cov(Y_t, Y_{t-j}) = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\epsilon_t + \theta\epsilon_{t-1})(\epsilon_{t-j} + \theta\epsilon_{t-j-1})] = 0 \quad \text{para } j > 1$$

Para provar que $Cov(Y_t, Y_{t-j}) = 0$ para $j>1$:
*Prova:*
I. Come√ßamos com a defini√ß√£o:
$$Cov(Y_t, Y_{t-j}) = E[(Y_t - \mu)(Y_{t-j} - \mu)]$$

II. Substitu√≠mos as express√µes para $Y_t$ e $Y_{t-j}$:
$$Cov(Y_t, Y_{t-j}) = E[(\epsilon_t + \theta\epsilon_{t-1})(\epsilon_{t-j} + \theta\epsilon_{t-j-1})]$$

III. Expandimos o produto:
$$Cov(Y_t, Y_{t-j}) = E[\epsilon_t\epsilon_{t-j} + \theta\epsilon_{t-1}\epsilon_{t-j} + \theta\epsilon_t\epsilon_{t-j-1} + \theta^2\epsilon_{t-1}\epsilon_{t-j-1}]$$

IV. Aplicamos a linearidade da expectativa:
$$Cov(Y_t, Y_{t-j}) = E[\epsilon_t\epsilon_{t-j}] + \theta E[\epsilon_{t-1}\epsilon_{t-j}] + \theta E[\epsilon_t\epsilon_{t-j-1}] + \theta^2 E[\epsilon_{t-1}\epsilon_{t-j-1}]$$

V. Como $j > 1$, todos os termos envolvem o c√°lculo da expectativa do produto de dois termos de erro em diferentes instantes de tempo, que s√£o n√£o correlacionados. Portanto, cada termo √© zero:
$$Cov(Y_t, Y_{t-j}) = 0 + 0 + 0 + 0 = 0$$

Assim, $Cov(Y_t, Y_{t-j}) = 0$ para $j > 1$ ‚ñ†

Este resultado √© crucial, pois indica que, em um processo MA(1), a correla√ß√£o entre observa√ß√µes separadas por mais de um per√≠odo √© nula.

**Autocorrela√ß√£o do Processo MA(1)**

A fun√ß√£o de autocorrela√ß√£o (ACF) √© uma ferramenta importante para identificar e caracterizar processos de s√©ries temporais. Para o processo MA(1), a ACF √© dada por:

$$\rho_j = \frac{Cov(Y_t, Y_{t-j})}{Var(Y_t)}$$

Para $j = 1$:

$$\rho_1 = \frac{Cov(Y_t, Y_{t-1})}{Var(Y_t)} = \frac{\theta\sigma^2}{(1 + \theta^2)\sigma^2} = \frac{\theta}{1 + \theta^2}$$

Para $j > 1$:

$$\rho_j = \frac{Cov(Y_t, Y_{t-j})}{Var(Y_t)} = 0$$

Assim, a ACF de um processo MA(1) √© zero ap√≥s o primeiro lag. O valor de $\rho_1$ est√° restrito ao intervalo $[-0.5, 0.5]$. Para ver isso, considere a fun√ß√£o $f(\theta) = \frac{\theta}{1 + \theta^2}$. Podemos encontrar o m√°ximo e m√≠nimo desta fun√ß√£o derivando em rela√ß√£o a $\theta$ e igualando a zero:

$$f'(\theta) = \frac{(1 + \theta^2) - \theta(2\theta)}{(1 + \theta^2)^2} = \frac{1 - \theta^2}{(1 + \theta^2)^2}$$

Igualando a zero, obtemos $\theta = \pm 1$. Substituindo de volta em $f(\theta)$, obtemos $f(1) = \frac{1}{2}$ e $f(-1) = -\frac{1}{2}$. Portanto, $-\frac{1}{2} \leq \rho_1 \leq \frac{1}{2}$.

> üí° **Exemplo Num√©rico:**
>
> Se $\theta = 0.5$, ent√£o a autocorrela√ß√£o de primeira ordem √©:
>
> $\rho_1 = \frac{0.5}{1 + 0.5^2} = \frac{0.5}{1.25} = 0.4$
>
> Isso indica uma correla√ß√£o positiva moderada entre os valores da s√©rie temporal em per√≠odos adjacentes.
>
> Se $\theta = -0.8$, ent√£o a autocorrela√ß√£o de primeira ordem √©:
>
> $\rho_1 = \frac{-0.8}{1 + (-0.8)^2} = \frac{-0.8}{1 + 0.64} = \frac{-0.8}{1.64} \approx -0.488$
>
> Isso indica uma correla√ß√£o negativa moderadamente forte entre os valores da s√©rie temporal em per√≠odos adjacentes.

**Teorema 1** *Unicidade da Solu√ß√£o para $\theta$*. Para um dado valor de $\rho_1$ no intervalo $(-0.5, 0.5)$, existem duas solu√ß√µes para $\theta$ na equa√ß√£o $\rho_1 = \frac{\theta}{1 + \theta^2}$. No entanto, apenas uma dessas solu√ß√µes satisfaz a condi√ß√£o de invertibilidade $|\theta| < 1$.

*Prova:* Dada a equa√ß√£o $\rho_1 = \frac{\theta}{1 + \theta^2}$, podemos reescrev√™-la como uma equa√ß√£o quadr√°tica em $\theta$:

$$\rho_1(1 + \theta^2) = \theta$$
$$\rho_1\theta^2 - \theta + \rho_1 = 0$$

Usando a f√≥rmula quadr√°tica, as solu√ß√µes para $\theta$ s√£o:

$$\theta = \frac{1 \pm \sqrt{1 - 4\rho_1^2}}{2\rho_1}$$

Seja $\theta_1 = \frac{1 + \sqrt{1 - 4\rho_1^2}}{2\rho_1}$ e $\theta_2 = \frac{1 - \sqrt{1 - 4\rho_1^2}}{2\rho_1}$. Note que $\theta_1\theta_2 = 1$, o que implica que no m√°ximo um dos valores de $\theta$ pode ter magnitude menor que 1.

Para mostrar que $\theta_1 \theta_2 = 1$:
*Prova:*
I. Definimos $\theta_1$ e $\theta_2$ como as solu√ß√µes da equa√ß√£o quadr√°tica:
$$\theta_1 = \frac{1 + \sqrt{1 - 4\rho_1^2}}{2\rho_1}$$
$$\theta_2 = \frac{1 - \sqrt{1 - 4\rho_1^2}}{2\rho_1}$$

II. Multiplicamos $\theta_1$ e $\theta_2$:
$$\theta_1\theta_2 = \left(\frac{1 + \sqrt{1 - 4\rho_1^2}}{2\rho_1}\right)\left(\frac{1 - \sqrt{1 - 4\rho_1^2}}{2\rho_1}\right)$$

III. Simplificamos o produto usando a diferen√ßa de quadrados $(a+b)(a-b) = a^2 - b^2$:
$$\theta_1\theta_2 = \frac{1^2 - (\sqrt{1 - 4\rho_1^2})^2}{(2\rho_1)^2} = \frac{1 - (1 - 4\rho_1^2)}{4\rho_1^2}$$

IV. Simplificamos a express√£o:
$$\theta_1\theta_2 = \frac{4\rho_1^2}{4\rho_1^2} = 1$$

Portanto, $\theta_1\theta_2 = 1$ ‚ñ†

Portanto, para garantir a invertibilidade, escolhemos a solu√ß√£o com $|\theta| < 1$. Se $\rho_1 > 0$, ent√£o $\theta_2$ √© a solu√ß√£o invert√≠vel. Se $\rho_1 < 0$, ent√£o $\theta_2$ √© a solu√ß√£o invert√≠vel.
Assim, para um dado $\rho_1$, existe uma √∫nica solu√ß√£o para $\theta$ que garante a invertibilidade do processo MA(1).

> üí° **Exemplo Num√©rico:**
>
> Se $\rho_1 = 0.4$, as duas solu√ß√µes para $\theta$ s√£o:
>
> $$\theta = \frac{1 \pm \sqrt{1 - 4(0.4)^2}}{2(0.4)} = \frac{1 \pm \sqrt{1 - 0.64}}{0.8} = \frac{1 \pm \sqrt{0.36}}{0.8} = \frac{1 \pm 0.6}{0.8}$$
>
> $$\theta_1 = \frac{1 + 0.6}{0.8} = \frac{1.6}{0.8} = 2$$
>
> $$\theta_2 = \frac{1 - 0.6}{0.8} = \frac{0.4}{0.8} = 0.5$$
>
> A solu√ß√£o invert√≠vel √© $\theta_2 = 0.5$ porque $|\theta_2| < 1$.
>
> Se $\rho_1 = -0.4$, as duas solu√ß√µes para $\theta$ s√£o:
>
> $$\theta = \frac{1 \pm \sqrt{1 - 4(-0.4)^2}}{2(-0.4)} = \frac{1 \pm \sqrt{1 - 0.64}}{-0.8} = \frac{1 \pm \sqrt{0.36}}{-0.8} = \frac{1 \pm 0.6}{-0.8}$$
>
> $$\theta_1 = \frac{1 + 0.6}{-0.8} = \frac{1.6}{-0.8} = -2$$
>
> $$\theta_2 = \frac{1 - 0.6}{-0.8} = \frac{0.4}{-0.8} = -0.5$$
>
> A solu√ß√£o invert√≠vel √© $\theta_2 = -0.5$ porque $|\theta_2| < 1$.

### Conclus√£o
O processo MA(1) √© um modelo fundamental em s√©ries temporais caracterizado por sua simplicidade e propriedades bem definidas. A m√©dia constante, a estacionariedade e a estrutura de autocorrela√ß√£o limitada (apenas a primeira autocorrela√ß√£o √© n√£o nula) tornam o modelo MA(1) uma ferramenta √∫til para modelar s√©ries temporais onde os valores atuais dependem de choques aleat√≥rios recentes. A an√°lise detalhada das propriedades do processo MA(1) fornece uma base s√≥lida para a compreens√£o de modelos mais complexos, como os processos ARMA, que ser√£o explorados nos pr√≥ximos cap√≠tulos.

### Refer√™ncias
[^44]: Cap√≠tulo 3 do livro, "Stationary ARMA Processes".
[^47]: Se√ß√£o 3.2 do livro, "White Noise".
[^48]: Se√ß√£o 3.3 do livro, "Moving Average Processes".
<!-- END -->