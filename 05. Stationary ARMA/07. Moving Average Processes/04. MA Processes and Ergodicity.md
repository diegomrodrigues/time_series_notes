## Covariance Stationarity and Ergodicity in MA Processes

### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise dos processos de m√©dias m√≥veis (MA), concentrando-se nas propriedades de **covari√¢ncia estacion√°ria** e **ergodicidade** [^44, ^45, ^51]. Com base nos conceitos e defini√ß√µes j√° apresentados, exploraremos como essas propriedades se manifestam nos processos MA(1) e MA(q), fornecendo uma base te√≥rica robusta para a modelagem e previs√£o de s√©ries temporais. A estacionariedade garante a aplicabilidade de algoritmos invariantes no tempo, enquanto a ergodicidade permite a substitui√ß√£o de m√©dias de conjunto por m√©dias temporais, crucial para a infer√™ncia estat√≠stica com base em uma √∫nica realiza√ß√£o observada da s√©rie.

### Conceitos Fundamentais

**Covari√¢ncia Estacion√°ria:**

Como previamente definido [^45, ^48], um processo √© considerado **covari√¢ncia estacion√°ria** (ou fracamente estacion√°rio) se sua m√©dia e autocovari√¢ncias n√£o variam com o tempo. Formalmente, isso significa que:

1.  $E(Y_t) = \mu$ para todo $t$, onde $\mu$ √© uma constante.
2.  $Cov(Y_t, Y_{t-j}) = \gamma_j$ para todo $t$ e $j$, onde $\gamma_j$ depende apenas de $j$ (o lag) e n√£o de $t$.

**Teorema 3.1**: *MA(1) e MA(q) s√£o Covari√¢ncia Estacion√°rios*.

Os processos MA(1) e MA(q) s√£o covari√¢ncia estacion√°rios, independentemente dos valores dos par√¢metros $\theta$ (no caso do MA(1)) ou $\theta_i$ (no caso do MA(q)).

*Proof*:

**MA(1):**

J√° demonstrado no cap√≠tulo anterior, para o MA(1), $E(Y_t) = \mu$ e $Var(Y_t) = (1+\theta^2)\sigma^2$ [^48], com $\gamma_1 = \theta\sigma^2$ e $\gamma_j = 0$ para $j>1$. Claramente, a m√©dia e autocovari√¢ncias s√£o independentes de $t$, satisfazendo as condi√ß√µes de covari√¢ncia estacion√°ria.

> üí° **Exemplo Num√©rico:** Considere um processo MA(1) com $\theta = 0.5$ e $\sigma^2 = 1$. Ent√£o, $E(Y_t) = \mu$, $Var(Y_t) = (1 + 0.5^2) * 1 = 1.25$, e $\gamma_1 = 0.5 * 1 = 0.5$. Isso significa que a vari√¢ncia do processo √© constante ao longo do tempo e a autocovari√¢ncia no lag 1 tamb√©m √© constante. Se $\mu = 0$, simulando 1000 pontos de dados e calculando a m√©dia amostral e a vari√¢ncia amostral, os valores obtidos estar√£o pr√≥ximos de 0 e 1.25, respectivamente.

```python
import numpy as np

# Par√¢metros do MA(1)
theta = 0.5
sigma_squared = 1
mu = 0

# Tamanho da amostra
T = 1000

# Gerar ru√≠do branco
epsilon = np.random.normal(0, np.sqrt(sigma_squared), T)

# Gerar o processo MA(1)
Y = np.zeros(T)
Y[0] = mu + epsilon[0]
for t in range(1, T):
    Y[t] = mu + epsilon[t] + theta * epsilon[t-1]

# Calcular a m√©dia amostral
mean_sample = np.mean(Y)

# Calcular a vari√¢ncia amostral
var_sample = np.var(Y)

print(f"M√©dia Amostral: {mean_sample}")
print(f"Vari√¢ncia Amostral: {var_sample}")

# Resultado esperado
# M√©dia Amostral: Pr√≥ximo de 0
# Vari√¢ncia Amostral: Pr√≥ximo de 1.25
```

**MA(q):**

Para o MA(q), $E(Y_t) = \mu$ [^50] e $Var(Y_t) = (1 + \theta_1^2 + \theta_2^2 + \ldots + \theta_q^2)\sigma^2$ [^50], com $\gamma_j = (\theta_j + \theta_{j+1}\theta_1 + \theta_{j+2}\theta_2 + \ldots + \theta_q\theta_{q-j})\sigma^2$ para $j \leq q$ e $\gamma_j = 0$ para $j>q$ [^51], como definido na Proposi√ß√£o 1 do cap√≠tulo anterior. Novamente, a m√©dia e as autocovari√¢ncias n√£o dependem de $t$, garantindo a covari√¢ncia estacion√°ria.

> üí° **Exemplo Num√©rico:** Considere um processo MA(2) com $\theta_1 = 0.6$, $\theta_2 = 0.4$, e $\sigma^2 = 1$. A vari√¢ncia do processo √© $Var(Y_t) = (1 + 0.6^2 + 0.4^2) * 1 = 1 + 0.36 + 0.16 = 1.52$. As autocovari√¢ncias s√£o $\gamma_1 = (0.6 + 0.4 * 0) * 1 = 0.6$ e $\gamma_2 = (0.4) * 1 = 0.4$. Para $j > 2$, $\gamma_j = 0$. Este exemplo ilustra como os par√¢metros $\theta_i$ influenciam a vari√¢ncia e as autocovari√¢ncias, mas estas permanecem constantes ao longo do tempo.

```python
import numpy as np

# Par√¢metros do MA(2)
theta1 = 0.6
theta2 = 0.4
sigma_squared = 1
mu = 0

# Tamanho da amostra
T = 1000

# Gerar ru√≠do branco
epsilon = np.random.normal(0, np.sqrt(sigma_squared), T)

# Gerar o processo MA(2)
Y = np.zeros(T)
Y[0] = mu + epsilon[0]
Y[1] = mu + epsilon[1] + theta1 * epsilon[0]
for t in range(2, T):
    Y[t] = mu + epsilon[t] + theta1 * epsilon[t-1] + theta2 * epsilon[t-2]

# Calcular a m√©dia amostral
mean_sample = np.mean(Y)

# Calcular a vari√¢ncia amostral
var_sample = np.var(Y)

print(f"M√©dia Amostral: {mean_sample}")
print(f"Vari√¢ncia Amostral: {var_sample}")

# Resultado esperado
# M√©dia Amostral: Pr√≥ximo de 0
# Vari√¢ncia Amostral: Pr√≥ximo de 1.52
```

> üí° **Implica√ß√µes Pr√°ticas:** A estacionariedade dos processos MA(1) e MA(q) implica que algoritmos de estima√ß√£o e previs√£o que assumem a invari√¢ncia estat√≠stica ao longo do tempo s√£o aplic√°veis a esses modelos. Isso simplifica significativamente a modelagem de s√©ries temporais que podem ser razoavelmente aproximadas por processos MA.

**Lema 3.1**: *A autocovari√¢ncia de um processo MA(q) decai rapidamente*.
Dado que $\gamma_j = 0$ para $|j| > q$, a autocovari√¢ncia de um processo MA(q) √© n√£o nula apenas para um n√∫mero finito de lags. Isso implica um decaimento abrupto da autocovari√¢ncia al√©m do lag $q$.

> üí° **Exemplo Num√©rico:** No processo MA(2) do exemplo anterior ($\theta_1 = 0.6$, $\theta_2 = 0.4$), a autocovari√¢ncia √© zero para lags maiores que 2. Isso significa que a depend√™ncia entre observa√ß√µes desaparece ap√≥s dois per√≠odos. Este decaimento r√°pido √© uma caracter√≠stica fundamental dos processos MA e contrasta com os processos AR, onde a depend√™ncia pode persistir por mais tempo.

**Proposi√ß√£o 3.2**: *A estacionariedade forte implica covari√¢ncia estacion√°ria, mas o inverso n√£o √© verdadeiro*.
Um processo estoc√°stico √© dito ser fortemente estacion√°rio se sua distribui√ß√£o conjunta √© invariante a deslocamentos no tempo. Enquanto a estacionariedade forte implica que a m√©dia e autocovari√¢ncias s√£o independentes do tempo (covari√¢ncia estacion√°ria), a covari√¢ncia estacion√°ria n√£o garante a estacionariedade forte. Um processo pode ter m√©dia e autocovari√¢ncias constantes, mas sua distribui√ß√£o de probabilidade pode variar ao longo do tempo.

**Lema 3.2**: *Se $\{\epsilon_t\}$ s√£o independentes e identicamente distribu√≠dos (i.i.d.) com m√©dia zero e vari√¢ncia finita, ent√£o o processo MA(q) √© fortemente estacion√°rio.*

*Proof:*
Um processo MA(q) √© definido como $Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q}$. Se $\{\epsilon_t\}$ s√£o i.i.d., ent√£o a distribui√ß√£o conjunta de $(\epsilon_t, \epsilon_{t-1}, \dots, \epsilon_{t-q})$ √© a mesma para todos os $t$. Portanto, a distribui√ß√£o conjunta de $(Y_t, Y_{t-1}, \dots, Y_{t-k})$ √© a mesma para todos os $t$ e $k$, o que implica estacionariedade forte.

**Ergodicidade:**

A **ergodicidade** √© uma propriedade que permite substituir m√©dias de conjunto (ensemble averages) por m√©dias temporais (time averages) ao calcular estat√≠sticas de um processo estoc√°stico [^47]. Em outras palavras, com uma √∫nica realiza√ß√£o suficientemente longa de um processo erg√≥dico, podemos estimar as propriedades estat√≠sticas do processo como se tiv√©ssemos m√∫ltiplas realiza√ß√µes independentes.

Mais formalmente, um processo estacion√°rio $Y_t$ √© dito ser **erg√≥dico para a m√©dia** se [^47]:

$$plim_{T \to \infty} \frac{1}{T} \sum_{t=1}^{T} Y_t = E(Y_t) = \mu$$,

onde $plim$ denota o limite em probabilidade. Em termos pr√°ticos, isso significa que a m√©dia amostral calculada a partir de uma √∫nica realiza√ß√£o do processo converge para a m√©dia te√≥rica do processo √† medida que o tamanho da amostra ($T$) aumenta.

**Condi√ß√£o Suficiente para Ergodicidade na M√©dia:**

Uma condi√ß√£o suficiente para a ergodicidade na m√©dia √© que a soma absoluta das autocovari√¢ncias convirja [^47]:

$$\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$$.

**Teorema 3.2**: *Ergodicidade na M√©dia para MA(1) e MA(q) com Erros Gaussianos*.

Se o processo $\{\epsilon_t\}$ for um processo de **white noise** Gaussiano, ent√£o os processos MA(1) e MA(q) s√£o erg√≥dicos para todos os momentos.

*Proof*:

**MA(1):**

Para o MA(1), as autocovari√¢ncias s√£o $\gamma_0 = (1+\theta^2)\sigma^2$ e $\gamma_1 = \theta\sigma^2$ e $\gamma_j = 0$ para $j>1$ [^48]. Portanto,

$$\sum_{j=-\infty}^{\infty} |\gamma_j| = |\gamma_0| + |\gamma_1| + |\gamma_{-1}| + \sum_{j=2}^{\infty} |\gamma_j| = (1+\theta^2)\sigma^2 + 2|\theta|\sigma^2 = (1 + \theta^2 + 2|\theta|)\sigma^2 < \infty$$.

Como a soma absoluta das autocovari√¢ncias converge, o processo MA(1) √© erg√≥dico para a m√©dia. Adicionalmente, se $\{\epsilon_t\}$ for Gaussiano, o MA(1) √© erg√≥dico para todos os momentos [^48].

> üí° **Exemplo Num√©rico:** Usando o mesmo MA(1) com $\theta = 0.5$ e $\sigma^2 = 1$, a soma das autocovari√¢ncias √© $(1 + 0.5^2 + 2 * 0.5) * 1 = 2.25$, que √© finita. Isso implica que, √† medida que o tamanho da amostra aumenta, a m√©dia amostral convergir√° para a m√©dia te√≥rica (que √© $\mu$). Simular uma longa s√©rie temporal e calcular a m√©dia ao longo do tempo demonstrar√° essa converg√™ncia.

```python
import numpy as np

# Par√¢metros do MA(1)
theta = 0.5
sigma_squared = 1
mu = 0

# Tamanho da amostra
T = 10000  # Aumentar o tamanho da amostra

# Gerar ru√≠do branco Gaussiano
epsilon = np.random.normal(0, np.sqrt(sigma_squared), T)

# Gerar o processo MA(1)
Y = np.zeros(T)
Y[0] = mu + epsilon[0]
for t in range(1, T):
    Y[t] = mu + epsilon[t] + theta * epsilon[t-1]

# Calcular a m√©dia amostral
mean_sample = np.mean(Y)

print(f"M√©dia Amostral com T={T}: {mean_sample}")

# Resultado esperado
# M√©dia Amostral: Pr√≥ximo de 0 (mais pr√≥ximo quanto maior T)
```

**MA(q):**

Para o MA(q), as autocovari√¢ncias s√£o $\gamma_j = (\theta_j + \theta_{j+1}\theta_1 + \theta_{j+2}\theta_2 + \ldots + \theta_q\theta_{q-j})\sigma^2$ para $j \leq q$ e $\gamma_j = 0$ para $j>q$ [^51]. Portanto,

$$\sum_{j=-\infty}^{\infty} |\gamma_j| = \sum_{j=-q}^{q} |\gamma_j| < \infty$$,

j√° que a soma √© finita, envolvendo apenas um n√∫mero finito de termos n√£o nulos. Logo, o processo MA(q) √© erg√≥dico para a m√©dia.  E se o processo √© Gaussiano, como dito anteriormente [^46], e os momentos de primeira e segunda ordem n√£o variam ao longo do tempo, ele tamb√©m √© estacion√°rio em um sentido mais forte. Adicionalmente, se $\{\epsilon_t\}$ for Gaussiano, o MA(q) √© erg√≥dico para todos os momentos [^51].

*Prova*
I. Consideremos a condi√ß√£o suficiente para a ergodicidade na m√©dia: $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$.

II. Para o processo MA(q), as autocovari√¢ncias $\gamma_j$ s√£o dadas por:
    $$
    \gamma_j =
    \begin{cases}
    \sigma^2 \sum_{i=0}^{q-j} \theta_i \theta_{i+j}, & \text{para } j = 0, 1, 2, \ldots, q \\
    0, & \text{para } j > q
    \end{cases}
    $$

III. Como $\gamma_j = 0$ para $|j| > q$, a soma $\sum_{j=-\infty}^{\infty} |\gamma_j|$ √© uma soma finita:
     $$\sum_{j=-\infty}^{\infty} |\gamma_j| = \sum_{j=-q}^{q} |\gamma_j|$$

IV. Cada termo $|\gamma_j|$ na soma √© uma fun√ß√£o dos par√¢metros $\theta_i$ e $\sigma^2$, todos os quais s√£o finitos. Portanto, a soma finita $\sum_{j=-q}^{q} |\gamma_j|$ tamb√©m √© finita.

V. Assim, a condi√ß√£o suficiente para a ergodicidade na m√©dia √© satisfeita, e o processo MA(q) √© erg√≥dico para a m√©dia.

VI. Se $\{\epsilon_t\}$ √© Gaussiano, ent√£o o processo MA(q) √© uma transforma√ß√£o linear de vari√°veis Gaussianas, e portanto √© tamb√©m Gaussiano. Para um processo Gaussiano, a estacionariedade de segunda ordem (covari√¢ncia estacion√°ria) implica estacionariedade forte e ergodicidade para todos os momentos. Portanto, se $\{\epsilon_t\}$ √© Gaussiano, o processo MA(q) √© erg√≥dico para todos os momentos. ‚ñ†

**Teorema 3.3**: *Converg√™ncia da Fun√ß√£o de Autocorrela√ß√£o Amostral*.
Para um processo MA(q) erg√≥dico, a fun√ß√£o de autocorrela√ß√£o amostral converge em probabilidade para a fun√ß√£o de autocorrela√ß√£o te√≥rica √† medida que o tamanho da amostra tende ao infinito.

*Proof Sketch:*
A prova envolve demonstrar que os estimadores amostrais das autocovari√¢ncias convergem em probabilidade para as autocovari√¢ncias te√≥ricas. Usando a ergodicidade e a lei dos grandes n√∫meros, √© poss√≠vel mostrar essa converg√™ncia. A converg√™ncia da fun√ß√£o de autocorrela√ß√£o amostral segue diretamente da converg√™ncia das autocovari√¢ncias amostrais.

> üí° **Exemplo Num√©rico:** Para ilustrar a converg√™ncia da fun√ß√£o de autocorrela√ß√£o amostral, considere um processo MA(1) com $\theta = 0.7$. Podemos simular uma s√©rie temporal longa e calcular a fun√ß√£o de autocorrela√ß√£o amostral para diferentes tamanhos de amostra (e.g., T = 100, 500, 1000). Ao comparar as fun√ß√µes de autocorrela√ß√£o amostrais com a fun√ß√£o de autocorrela√ß√£o te√≥rica ($\rho_1 = \frac{\theta}{1+\theta^2} = \frac{0.7}{1+0.7^2} \approx 0.49$), observaremos que a fun√ß√£o de autocorrela√ß√£o amostral se aproxima da te√≥rica √† medida que o tamanho da amostra aumenta.

```python
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt

# Par√¢metros do MA(1)
theta = 0.7
sigma_squared = 1
mu = 0

# Tamanho da amostra
T = 1000

# Gerar ru√≠do branco Gaussiano
epsilon = np.random.normal(0, np.sqrt(sigma_squared), T)

# Gerar o processo MA(1)
Y = np.zeros(T)
Y[0] = mu + epsilon[0]
for t in range(1, T):
    Y[t] = mu + epsilon[t] + theta * epsilon[t-1]

# Calcular a fun√ß√£o de autocorrela√ß√£o amostral
acf_sample = sm.tsa.acf(Y, nlags=5)  # Calcula at√© o lag 5

# Fun√ß√£o de autocorrela√ß√£o te√≥rica
rho_1_theoretical = theta / (1 + theta**2)

print(f"Fun√ß√£o de Autocorrela√ß√£o Amostral: {acf_sample}")
print(f"Autocorrela√ß√£o Te√≥rica no Lag 1: {rho_1_theoretical}")

# Plot da ACF amostral
lags = np.arange(len(acf_sample))
plt.stem(lags, acf_sample, use_line_collection=True)
plt.axhline(y=0, color='k', linestyle='--')
plt.axhline(y=rho_1_theoretical, color='r', linestyle='--', label='ACF Te√≥rica (Lag 1)')
plt.title('Fun√ß√£o de Autocorrela√ß√£o Amostral (MA(1))')
plt.xlabel('Lag')
plt.ylabel('Autocorrela√ß√£o')
plt.legend()
plt.show()

# Espera-se que acf_sample[1] (autocorrela√ß√£o no lag 1) se aproxime de rho_1_theoretical
```

**Teorema 3.3.1**: *Sob condi√ß√µes de ergodicidade e momentos finitos de ordem quatro, a fun√ß√£o de autocorrela√ß√£o amostral √© assintoticamente normal*.
Se o processo MA(q) √© erg√≥dico e possui momentos finitos de ordem quatro, ent√£o a fun√ß√£o de autocorrela√ß√£o amostral √© assintoticamente normal. Este resultado √© crucial para a constru√ß√£o de intervalos de confian√ßa e testes de hip√≥teses sobre a estrutura de depend√™ncia do processo.

**Proposi√ß√£o 3.1** *Consequ√™ncias da Ergodicidade*. Para um processo MA(q) erg√≥dico, a m√©dia amostral $\bar{Y} = \frac{1}{T}\sum_{t=1}^{T} Y_t$ converge em probabilidade para a m√©dia te√≥rica $\mu$ √† medida que o tamanho da amostra $T$ tende ao infinito. Al√©m disso, os momentos amostrais convergem para os momentos te√≥ricos correspondentes.

*Proof:*
I. A ergodicidade na m√©dia implica que a m√©dia amostral converge em probabilidade para a m√©dia te√≥rica, ou seja:
   $$plim_{T \to \infty} \frac{1}{T} \sum_{t=1}^{T} Y_t = E(Y_t) = \mu$$

II. Considere um momento amostral gen√©rico de ordem $k$:
    $$m_k = \frac{1}{T} \sum_{t=1}^{T} Y_t^k$$

III. Queremos mostrar que $m_k$ converge em probabilidade para o momento te√≥rico correspondente $E[Y_t^k]$.

IV. Dado que o processo √© erg√≥dico para todos os momentos (como demonstrado anteriormente sob a suposi√ß√£o de erros Gaussianos), temos:
    $$plim_{T \to \infty} \frac{1}{T} \sum_{t=1}^{T} g(Y_t) = E[g(Y_t)]$$
    onde $g(Y_t)$ √© uma fun√ß√£o mensur√°vel de $Y_t$.

V. Fazendo $g(Y_t) = Y_t^k$, obtemos:
    $$plim_{T \to \infty} \frac{1}{T} \sum_{t=1}^{T} Y_t^k = E[Y_t^k]$$

VI. Portanto, os momentos amostrais convergem em probabilidade para os momentos te√≥ricos correspondentes. ‚ñ†

**Proposi√ß√£o 3.1.1**: *A vari√¢ncia da m√©dia amostral de um processo MA(q) decresce para zero √† medida que o tamanho da amostra aumenta*.

*Proof Sketch:*
A vari√¢ncia da m√©dia amostral √© dada por $Var(\bar{Y}) = \frac{1}{T^2} \sum_{t=1}^{T} \sum_{s=1}^{T} Cov(Y_t, Y_s)$. Devido √† estacionariedade e ao decaimento r√°pido das autocovari√¢ncias em um processo MA(q), esta vari√¢ncia converge para zero √† medida que $T$ cresce. Este resultado quantifica a precis√£o da estimativa da m√©dia te√≥rica usando a m√©dia amostral.

### Conclus√£o

A estacionariedade e a ergodicidade s√£o propriedades fundamentais que sustentam a an√°lise estat√≠stica de processos MA(1) e MA(q). A covari√¢ncia estacion√°ria garante que algoritmos de estima√ß√£o e previs√£o invariantes no tempo sejam apropriados, enquanto a ergodicidade justifica a utiliza√ß√£o de m√©dias temporais para estimar as propriedades estat√≠sticas do processo com base em uma √∫nica realiza√ß√£o. A garantia de ergodicidade sob a suposi√ß√£o de white noise gaussiano √© particularmente relevante, pois permite infer√™ncias robustas mesmo com dados limitados. A combina√ß√£o dessas propriedades torna os modelos MA(1) e MA(q) ferramentas valiosas para a modelagem e previs√£o de s√©ries temporais em diversas aplica√ß√µes [^47, ^48, ^51].

### Refer√™ncias
[^44]: Imagine a battery of I such computers generating sequences {y{1}, {y{2} ...
[^45]: The variance of the random variable Y, (denoted you) is similarly defined as...
[^46]: In this text the term "stationary" by itself is taken to mean "covariance-stationary."
[^47]: The basic building block for all the processes considered in this chapter is a sequence {8} - whose elements have mean zero and variance œÉ¬≤,
[^48]: Let {8} be white noise as in [3.2.1] through [3.2.3], and consider the process...
[^51]: Since the e's are uncorrelated, the variance [3.3.9] is¬≤
<!-- END -->