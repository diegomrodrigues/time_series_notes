## Autocorrelation Function and Visualization in Moving Average Processes

### Introdu√ß√£o
Em continuidade √† an√°lise das propriedades de estacionariedade e ergodicidade dos processos de m√©dias m√≥veis (MA) [^44, ^45, ^51], este cap√≠tulo se dedica ao estudo da **fun√ß√£o de autocorrela√ß√£o (ACF)**, $\rho_j$, em processos covari√¢ncia-estacion√°rios [^45, ^46]. A ACF quantifica a correla√ß√£o entre uma observa√ß√£o em um determinado ponto no tempo e observa√ß√µes em pontos no tempo anteriores [^49]. Exploraremos em detalhes a forma da ACF para processos MA(1), calculando a primeira autocorrela√ß√£o e demonstrando como ela se relaciona com o par√¢metro $\theta$ [^48]. Al√©m disso, discutiremos algoritmos para visualizar a ACF, o que auxilia na identifica√ß√£o da ordem do processo MA e na interpreta√ß√£o da depend√™ncia temporal nos dados.

### Conceitos Fundamentais

**Defini√ß√£o Formal da Autocorrela√ß√£o:**

Como apresentado anteriormente [^49], a **autocorrela√ß√£o de ordem j** ($\rho_j$) para um processo covari√¢ncia-estacion√°rio √© definida como a autocovari√¢ncia de ordem *j* ($\gamma_j$) dividida pela vari√¢ncia do processo ($\gamma_0$):

$$\rho_j = \frac{\gamma_j}{\gamma_0}$$.

A autocorrela√ß√£o varia entre -1 e 1, com valores pr√≥ximos de 1 indicando forte correla√ß√£o positiva, valores pr√≥ximos de -1 indicando forte correla√ß√£o negativa, e valores pr√≥ximos de 0 indicando correla√ß√£o fraca ou ausente [^49].

> üí° **Exemplo Num√©rico:** Considere um processo onde $\gamma_0 = 2.0$ e $\gamma_1 = 1.0$. Ent√£o a autocorrela√ß√£o no lag 1 √© $\rho_1 = 1.0/2.0 = 0.5$, indicando uma correla√ß√£o positiva moderada entre observa√ß√µes consecutivas. Se $\gamma_0 = 4$ e $\gamma_2 = -1$, ent√£o $\rho_2 = -1/4 = -0.25$ indicando uma leve correla√ß√£o negativa entre observa√ß√µes separadas por dois per√≠odos.

**Autocorrela√ß√£o do Processo MA(1):**

Para o processo MA(1), como j√° demonstrado no cap√≠tulo anterior [^48]:

$$\rho_1 = \frac{\theta}{1+\theta^2}$$.

E para $j > 1$, $\rho_j = 0$ [^49].

Este resultado fundamental implica que, para um processo MA(1), apenas a autocorrela√ß√£o no lag 1 √© possivelmente diferente de zero, enquanto as autocorrela√ß√µes em lags superiores s√£o sempre nulas. Esta propriedade √© uma caracter√≠stica distintiva dos processos MA(1) e √© essencial para sua identifica√ß√£o.

> üí° **Exemplo Num√©rico:** Se $\theta = 0.5$, ent√£o $\rho_1 = \frac{0.5}{1 + 0.5^2} = \frac{0.5}{1.25} = 0.4$. Se $\theta = -0.8$, ent√£o $\rho_1 = \frac{-0.8}{1 + (-0.8)^2} = \frac{-0.8}{1.64} \approx -0.4878$.
>
> Suponha que observamos dados de uma s√©rie temporal e calculamos sua autocorrela√ß√£o no lag 1 como 0.4. Usando a f√≥rmula acima e resolvendo para $\theta$:
>
> $0.4 = \frac{\theta}{1 + \theta^2}$
>
> $0.4 + 0.4\theta^2 = \theta$
>
> $0.4\theta^2 - \theta + 0.4 = 0$
>
> Usando a f√≥rmula quadr√°tica, temos duas solu√ß√µes: $\theta \approx 2$ e $\theta \approx 0.5$. No entanto, apenas $\theta \approx 0.5$ √© invert√≠vel.

**Lema 1.** *Invertibilidade e Autocorrela√ß√£o*. Para um processo MA(1) invert√≠vel, $|\theta| < 1$. Consequentemente, $|\rho_1| < 0.5$.

*Proof.* A condi√ß√£o de invertibilidade para um MA(1) √© $|\theta|<1$. Para mostrar que $|\rho_1| < 0.5$, considere a fun√ß√£o $f(\theta) = \frac{\theta}{1+\theta^2}$. Tomando a derivada de $f(\theta)$ em rela√ß√£o a $\theta$ e igualando a zero, encontramos o m√°ximo de $f(\theta)$ em $\theta=1$, onde $f(1) = 0.5$. Da mesma forma, o m√≠nimo ocorre em $\theta=-1$, onde $f(-1) = -0.5$. Portanto, para $|\theta| < 1$, $|\rho_1| < 0.5$.

**Prova do Lema 1:**

Para provar que $|\rho_1| < 0.5$ quando $|\theta| < 1$:

I. Definimos $\rho_1$ como uma fun√ß√£o de $\theta$:
   $$\rho_1(\theta) = \frac{\theta}{1 + \theta^2}$$

II. Queremos encontrar o valor m√°ximo e m√≠nimo de $\rho_1(\theta)$. Para isso, calculamos a derivada de $\rho_1(\theta)$ em rela√ß√£o a $\theta$:
    $$\frac{d\rho_1}{d\theta} = \frac{(1 + \theta^2)(1) - \theta(2\theta)}{(1 + \theta^2)^2} = \frac{1 - \theta^2}{(1 + \theta^2)^2}$$

III. Para encontrar os pontos cr√≠ticos, igualamos a derivada a zero:
     $$\frac{1 - \theta^2}{(1 + \theta^2)^2} = 0$$
     $$1 - \theta^2 = 0$$
     $$\theta = \pm 1$$

IV. Agora, avaliamos $\rho_1(\theta)$ nesses pontos cr√≠ticos:
    $$\rho_1(1) = \frac{1}{1 + 1^2} = \frac{1}{2} = 0.5$$
    $$\rho_1(-1) = \frac{-1}{1 + (-1)^2} = \frac{-1}{2} = -0.5$$

V.  Como estamos considerando o caso onde $|\theta| < 1$, os valores de $\rho_1$ estar√£o entre -0.5 e 0.5. Portanto:
    $$|\rho_1| < 0.5$$

Assim, demonstramos que para um processo MA(1) invert√≠vel, $|\theta| < 1$ implica que $|\rho_1| < 0.5$. ‚ñ†

> üí° **Exemplo Num√©rico:** Se temos um processo MA(1) com $\theta = 0.6$, ent√£o $|\theta| = 0.6 < 1$, e $\rho_1 = \frac{0.6}{1 + 0.6^2} = \frac{0.6}{1.36} \approx 0.441$. De fato, $|0.441| < 0.5$. Se $\theta = 1.2$, ent√£o $|\theta| = 1.2 > 1$ (n√£o invert√≠vel), e $\rho_1 = \frac{1.2}{1 + 1.2^2} = \frac{1.2}{2.44} \approx 0.492$. Note que embora $|\theta| > 1$, $|\rho_1| < 0.5$, mas esse processo n√£o √© invert√≠vel.

**Lema 1.1**. *Unicidade da Solu√ß√£o para $\theta$*. Para um dado valor de $\rho_1$ no intervalo $(-0.5, 0.5)$, existem duas solu√ß√µes poss√≠veis para $\theta$ na equa√ß√£o $\rho_1 = \frac{\theta}{1+\theta^2}$, uma dentro do c√≠rculo unit√°rio (invert√≠vel) e outra fora.

*Proof*. Seja $\rho_1$ um valor dado no intervalo $(-0.5, 0.5)$.  A equa√ß√£o $\rho_1 = \frac{\theta}{1+\theta^2}$ pode ser reescrita como $\rho_1 \theta^2 - \theta + \rho_1 = 0$.  Esta √© uma equa√ß√£o quadr√°tica em $\theta$, cuja solu√ß√£o √© dada por:

$$\theta = \frac{1 \pm \sqrt{1 - 4\rho_1^2}}{2\rho_1}$$

Como $|\rho_1| < 0.5$, ent√£o $1 - 4\rho_1^2 > 0$, e portanto, existem duas solu√ß√µes reais para $\theta$.  Sejam $\theta_1$ e $\theta_2$ as duas solu√ß√µes.  O produto das ra√≠zes de uma equa√ß√£o quadr√°tica $ax^2 + bx + c = 0$ √© dado por $c/a$.  Neste caso, $\theta_1 \theta_2 = \frac{\rho_1}{\rho_1} = 1$.  Isto significa que se $|\theta_1| < 1$, ent√£o $|\theta_2| > 1$, e vice-versa.  Portanto, para cada valor de $\rho_1$ no intervalo $(-0.5, 0.5)$, existe uma solu√ß√£o para $\theta$ dentro do c√≠rculo unit√°rio (correspondendo a um processo invert√≠vel) e uma solu√ß√£o fora do c√≠rculo unit√°rio (n√£o invert√≠vel).

**Prova do Lema 1.1:**

I. Partimos da equa√ß√£o que relaciona $\rho_1$ e $\theta$:
   $$\rho_1 = \frac{\theta}{1 + \theta^2}$$

II. Reorganizamos a equa√ß√£o para obter uma equa√ß√£o quadr√°tica em termos de $\theta$:
    $$\rho_1 (1 + \theta^2) = \theta$$
    $$\rho_1 \theta^2 - \theta + \rho_1 = 0$$

III. Aplicamos a f√≥rmula quadr√°tica para encontrar as solu√ß√µes para $\theta$:
     $$\theta = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} = \frac{1 \pm \sqrt{1 - 4\rho_1^2}}{2\rho_1}$$

IV. Sejam $\theta_1$ e $\theta_2$ as duas solu√ß√µes:
    $$\theta_1 = \frac{1 + \sqrt{1 - 4\rho_1^2}}{2\rho_1}$$
    $$\theta_2 = \frac{1 - \sqrt{1 - 4\rho_1^2}}{2\rho_1}$$

V.  Calculamos o produto das duas solu√ß√µes:
    $$\theta_1 \theta_2 = \frac{(1 + \sqrt{1 - 4\rho_1^2})(1 - \sqrt{1 - 4\rho_1^2})}{(2\rho_1)(2\rho_1)} = \frac{1 - (1 - 4\rho_1^2)}{4\rho_1^2} = \frac{4\rho_1^2}{4\rho_1^2} = 1$$

VI. Como o produto das duas solu√ß√µes √© 1, se uma solu√ß√£o √© menor que 1 em valor absoluto, a outra deve ser maior que 1 em valor absoluto. Isso significa que uma solu√ß√£o est√° dentro do c√≠rculo unit√°rio (invert√≠vel) e a outra est√° fora (n√£o invert√≠vel).

Portanto, para um dado valor de $\rho_1$ no intervalo $(-0.5, 0.5)$, existem duas solu√ß√µes para $\theta$, uma dentro do c√≠rculo unit√°rio e outra fora. ‚ñ†

> üí° **Exemplo Num√©rico:** Seja $\rho_1 = 0.3$. Encontramos as duas solu√ß√µes para $\theta$ usando a f√≥rmula quadr√°tica:
>
> $\theta = \frac{1 \pm \sqrt{1 - 4(0.3)^2}}{2(0.3)} = \frac{1 \pm \sqrt{1 - 0.36}}{0.6} = \frac{1 \pm \sqrt{0.64}}{0.6} = \frac{1 \pm 0.8}{0.6}$
>
> $\theta_1 = \frac{1 + 0.8}{0.6} = \frac{1.8}{0.6} = 3$
>
> $\theta_2 = \frac{1 - 0.8}{0.6} = \frac{0.2}{0.6} = \frac{1}{3} \approx 0.333$
>
> Observe que $\theta_1 \theta_2 = 3 \cdot \frac{1}{3} = 1$. A solu√ß√£o $\theta_2 \approx 0.333$ est√° dentro do c√≠rculo unit√°rio (invert√≠vel), enquanto $\theta_1 = 3$ est√° fora (n√£o invert√≠vel).

**Algoritmo para Plotar a Autocorrela√ß√£o do MA(1):**

O algoritmo para plotar a fun√ß√£o de autocorrela√ß√£o (ACF) para um processo MA(1) √© relativamente simples. Envolve calcular os valores te√≥ricos da ACF para diferentes valores de $\theta$ e representar graficamente esses valores em fun√ß√£o do lag.

1.  **Definir um intervalo de valores para $\theta$:** Escolha um intervalo de valores para o par√¢metro $\theta$ que seja relevante para a an√°lise. Por exemplo, pode-se escolher um intervalo entre -1 e 1 para garantir a invertibilidade do processo, conforme o Teorema 1 do cap√≠tulo anterior.

2.  **Calcular a autocorrela√ß√£o no lag 1 ($\rho_1$) para cada valor de $\theta$:** Para cada valor de $\theta$ no intervalo definido, calcule $\rho_1$ usando a f√≥rmula $\rho_1 = \frac{\theta}{1 + \theta^2}$.

3.  **As autocorrela√ß√µes para lags superiores s√£o zero:** Para todos os lags $j > 1$, $\rho_j = 0$.

4.  **Representar graficamente a ACF:** Crie um gr√°fico com os lags no eixo horizontal e os valores de autocorrela√ß√£o no eixo vertical. Para o MA(1), o gr√°fico consistir√° de uma barra no lag 1 com altura igual a $\rho_1$ e zero para todos os outros lags.

5.  **Analisar o gr√°fico:** O gr√°fico permite visualizar a for√ßa e a dire√ß√£o da correla√ß√£o entre observa√ß√µes consecutivas em fun√ß√£o do valor de $\theta$.

> üí° **Exemplo Num√©rico (Visualiza√ß√£o):** Para exemplificar a visualiza√ß√£o, se $\theta = 0.7$, $\rho_1 \approx 0.49$. Se $\theta = -0.9$, ent√£o $\rho_1 \approx -0.497$. O gr√°fico da ACF mostraria uma barra com altura 0.49 no lag 1 para o primeiro caso, e uma barra com altura -0.497 no lag 1 para o segundo caso.

**C√≥digo Python para Plotar a ACF Te√≥rica do MA(1):**

```python
import numpy as np
import matplotlib.pyplot as plt

# Define a range of theta values
theta_values = np.linspace(-0.99, 0.99, 100)  # Invertibility condition |theta| < 1

# Calculate rho_1 for each theta
rho_1_values = theta_values / (1 + theta_values**2)

# Plot the ACF
plt.figure(figsize=(8, 6))
plt.plot(theta_values, rho_1_values)
plt.title('Theoretical ACF for MA(1) at Lag 1')
plt.xlabel('Theta Value')
plt.ylabel('Autocorrelation (rho_1)')
plt.grid(True)
plt.show()
```

Este c√≥digo gera um gr√°fico que mostra como a autocorrela√ß√£o no lag 1 varia em fun√ß√£o de $\theta$. Esse tipo de gr√°fico permite entender visualmente a influ√™ncia de $\theta$ na depend√™ncia temporal do processo.

> üí° **Exemplo Num√©rico:** Simula√ß√£o de dados MA(1) e c√°lculo da ACF amostral:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> import statsmodels.api as sm
> from statsmodels.tsa.arima.model import ARIMA
>
> # Define theta
> theta = 0.6
>
> # Generate MA(1) data
> np.random.seed(0)
> errors = np.random.randn(100)
> ma1_data = [errors[i] + theta * errors[i-1] if i > 0 else errors[i] for i in range(len(errors))]
>
> # Calculate and plot the sample ACF
> fig, ax = plt.subplots(figsize=(8, 5))
> sm.graphics.tsa.plot_acf(ma1_data, lags=10, ax=ax)
> plt.title('Sample ACF of MA(1) Data (theta=0.6)')
> plt.xlabel('Lag')
> plt.ylabel('Autocorrelation')
> plt.show()
> ```
>
> Este c√≥digo simula dados de um processo MA(1) com $\theta = 0.6$ e calcula a ACF amostral. A ACF amostral deve mostrar um valor significativo no lag 1 e valores pr√≥ximos de zero para outros lags, confirmando a estrutura MA(1).

**Corol√°rio 1.** *Limites da Autocorrela√ß√£o no Lag 1*. Devido √† condi√ß√£o de invertibilidade $|\theta| < 1$ e √† forma da fun√ß√£o $\rho_1 = \frac{\theta}{1 + \theta^2}$, a autocorrela√ß√£o no lag 1 de um processo MA(1) sempre estar√° no intervalo $(-0.5, 0.5)$.

**Prova do Corol√°rio 1:**

I. Do Lema 1, sabemos que para um processo MA(1) invert√≠vel, $|\theta| < 1$.
II. Tamb√©m sabemos que $\rho_1 = \frac{\theta}{1 + \theta^2}$.
III. Pelo Lema 1, encontramos que o valor m√°ximo de $\rho_1$ ocorre quando $\theta = 1$, e $\rho_1 = 0.5$. Da mesma forma, o valor m√≠nimo ocorre quando $\theta = -1$, e $\rho_1 = -0.5$.
IV. Como estamos restritos a $|\theta| < 1$, a autocorrela√ß√£o no lag 1, $\rho_1$, estar√° sempre no intervalo $(-0.5, 0.5)$.

Portanto, a autocorrela√ß√£o no lag 1 de um processo MA(1) sempre estar√° no intervalo $(-0.5, 0.5)$. ‚ñ†

> üí° **Exemplo Num√©rico:** Seja $\theta = 0.3$. Ent√£o $\rho_1 = \frac{0.3}{1 + 0.3^2} = \frac{0.3}{1.09} \approx 0.275$. Este valor est√° dentro do intervalo $(-0.5, 0.5)$. Se $\theta = -0.7$, ent√£o $\rho_1 = \frac{-0.7}{1 + (-0.7)^2} = \frac{-0.7}{1.49} \approx -0.47$. Este valor tamb√©m est√° dentro do intervalo $(-0.5, 0.5)$.

**Corol√°rio 3.1**. *Simetria na Fun√ßao de Autocorrela√ß√£o Amostral*. Como dito anteriormente [^45], para um processo de covari√¢ncia estacion√°ria, $\gamma_j = \gamma_{-j}$ e $\rho_j = \rho_{-j}$. O que implica, que para gerar uma fun√ß√£o de autocorrela√ß√£o da amostra, as autocovari√¢ncias e autocorrela√ß√µes em defasagens negativas s√£o definidas como iguais √† autocovari√¢ncia e autocorrela√ß√£o em defasagens positivas.

**Prova do Corol√°rio 3.1:**
I. Definimos a autocovari√¢ncia de ordem j como:
    $$\gamma_j = E[(X_t - \mu)(X_{t-j} - \mu)]$$

II. Definimos a autocovari√¢ncia de ordem -j como:
     $$\gamma_{-j} = E[(X_t - \mu)(X_{t+j} - \mu)]$$

III. Fazemos uma mudan√ßa de vari√°vel $k = t+j$, ent√£o $t = k-j$:
      $$\gamma_{-j} = E[(X_{k-j} - \mu)(X_{k} - \mu)]$$

IV. Reorganizamos os termos:
     $$\gamma_{-j} = E[(X_{k} - \mu)(X_{k-j} - \mu)]$$

V. Como o processo √© estacion√°rio, a autocovari√¢ncia depende apenas da diferen√ßa entre os tempos, ent√£o:
    $$\gamma_{-j} = E[(X_{t} - \mu)(X_{t-j} - \mu)] = \gamma_j$$

VI. A autocorrela√ß√£o √© definida como:
     $$\rho_j = \frac{\gamma_j}{\gamma_0}$$
     $$\rho_{-j} = \frac{\gamma_{-j}}{\gamma_0}$$

VII. Substituindo $\gamma_{-j} = \gamma_j$:
      $$\rho_{-j} = \frac{\gamma_{j}}{\gamma_0} = \rho_j$$

Portanto, para um processo de covari√¢ncia estacion√°ria, $\gamma_j = \gamma_{-j}$ e $\rho_j = \rho_{-j}$. ‚ñ†

> üí° **Exemplo Num√©rico:**  Suponha que temos uma s√©rie temporal e calculamos as seguintes autocorrela√ß√µes amostrais: $\hat{\rho}_1 = 0.6$, $\hat{\rho}_2 = 0.4$, $\hat{\rho}_3 = 0.2$. Devido √† simetria, definimos tamb√©m: $\hat{\rho}_{-1} = 0.6$, $\hat{\rho}_{-2} = 0.4$, $\hat{\rho}_{-3} = 0.2$. Ao plotar a ACF, ambos os lags positivos e negativos s√£o exibidos.

**Interpreta√ß√£o da Fun√ß√£o de Autocorrela√ß√£o (ACF):**

A ACF fornece informa√ß√µes cruciais sobre a estrutura de depend√™ncia temporal de uma s√©rie temporal. Para processos MA(1), a ACF tem as seguintes caracter√≠sticas principais:

*   **Decaimento Abrupto:** A ACF "corta" ap√≥s o lag 1, ou seja, $\rho_j = 0$ para $j > 1$. Este √© um sinal distintivo de um processo MA(1).
*   **Valor da Autocorrela√ß√£o no Lag 1:** O valor de $\rho_1$ indica a for√ßa e a dire√ß√£o da correla√ß√£o entre observa√ß√µes consecutivas. Um valor positivo de $\rho_1$ indica correla√ß√£o positiva, enquanto um valor negativo indica correla√ß√£o negativa. A magnitude de $\rho_1$ indica a for√ßa da correla√ß√£o.
*   **Identifica√ß√£o da Ordem do Modelo MA:** Observar um decaimento abrupto da ACF ap√≥s um determinado lag $q$ sugere que um modelo MA(q) pode ser apropriado para modelar a s√©rie temporal.

A interpreta√ß√£o da ACF em conjunto com a **fun√ß√£o de autocorrela√ß√£o parcial (PACF)**, que ser√° discutida em cap√≠tulos subsequentes, √© fundamental para a identifica√ß√£o e sele√ß√£o de modelos ARMA (Autoregressive Moving Average) apropriados.

**Teorema 2.** *Autocorrela√ß√£o para Processos MA(q)*. Para um processo MA(q) dado por $X_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + ... + \theta_q \epsilon_{t-q}$, onde $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, a fun√ß√£o de autocorrela√ß√£o $\rho_j$ tem as seguintes propriedades:

*   $\rho_j = 0$ para $j > q$
*   $\rho_0 = 1$
*   $\rho_j = \frac{\sum_{i=0}^{q-j} \theta_i \theta_{i+j}}{ \sum_{i=0}^{q} \theta_i^2 }$ para $1 \le j \le q$, onde $\theta_0 = 1$

*Proof*. A demonstra√ß√£o segue da defini√ß√£o de autocorrela√ß√£o e das propriedades do ru√≠do branco. Para $j > q$, a autocovari√¢ncia $\gamma_j = E[(X_t - \mu)(X_{t-j} - \mu)]$ envolve termos $\epsilon_t$ e $\epsilon_{t-j}$ que s√£o n√£o correlacionados, resultando em $\gamma_j = 0$ e consequentemente $\rho_j = 0$. Para $1 \le j \le q$, o c√°lculo da autocovari√¢ncia $\gamma_j$ envolve a soma de produtos dos coeficientes $\theta_i$, conforme expresso na f√≥rmula. Dividindo $\gamma_j$ pela vari√¢ncia $\gamma_0 = \sum_{i=0}^{q} \theta_i^2 \sigma^2$, obtemos a express√£o para $\rho_j$.

**Prova do Teorema 2:**

I. Definimos o processo MA(q):
   $$X_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \ldots + \theta_q \epsilon_{t-q}$$
   Onde $\epsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$.

II. Calculamos a autocovari√¢ncia $\gamma_j = E[(X_t - \mu)(X_{t-j} - \mu)]$:
    $$\gamma_j = E\left[\left(\sum_{i=0}^{q} \theta_i \epsilon_{t-i}\right)\left(\sum_{k=0}^{q} \theta_k \epsilon_{t-j-k}\right)\right]$$
    Onde $\theta_0 = 1$.

III. Expandimos a express√£o e usamos o fato de que $E[\epsilon_t \epsilon_s] = \sigma^2$ se $t = s$ e 0 caso contr√°rio:
     $$\gamma_j = E\left[\sum_{i=0}^{q} \sum_{k=0}^{q} \theta_i \theta_k \epsilon_{t-i} \epsilon_{t-j-k}\right] = \sum_{i=0}^{q} \sum_{k=0}^{q} \theta_i \theta_k E[\epsilon_{t-i} \epsilon_{t-j-k}]$$

IV. Para que $E[\epsilon_{t-i} \epsilon_{t-j-k}] \neq 0$, devemos ter $t-i = t-j-k$, o que implica $i = j+k$. Substitu√≠mos isso na soma:
    $$\gamma_j = \sigma^2 \sum_{k=0}^{q-j} \theta_k \theta_{k+j}$$
    Esta soma √© v√°lida para $0 \le j \le q$. Para $j > q$, a soma √© zero porque n√£o h√° termos sobrepostos.

V.  Calculamos a vari√¢ncia $\gamma_0$:
    $$\gamma_0 = E[(X_t - \mu)^2] = E\left[\left(\sum_{i=0}^{q} \theta_i \epsilon_{t-i}\right)^2\right] = \sigma^2 \sum_{i=0}^{q} \theta_i^2$$

VI. Calculamos a autocorrela√ß√£o $\rho_j = \frac{\gamma_j}{\gamma_0}$:
     $$\rho_j = \frac{\sigma^2 \sum_{i=0}^{q-j} \theta_i \theta_{i+j}}{\sigma^2 \sum_{i=0}^{q} \theta_i^2} = \frac{\sum_{i=0}^{q-j} \theta_i \theta_{i+j}}{\sum_{i=0}^{q} \theta_i^2}$$

VII. Portanto, temos:
     $$\rho_j = \begin{cases}
     1 & \text{para } j = 0 \\
     \frac{\sum_{i=0}^{q-j} \theta_i \theta_{i+j}}{\sum_{i=0}^{q} \theta_i^2} & \text{para } 1 \le j \le q \\
     0 & \text{para } j > q
     \end{cases}$$
Portanto, demonstramos as propriedades da fun√ß√£o de autocorrela√ß√£o para processos MA(q). ‚ñ†

> üí° **Exemplo Num√©rico:** Considere um processo MA(2) com $\theta_1 = 0.5$ e $\theta_2 = 0.3$. Usando o Teorema 2:
>
> *   $\rho_0 = 1$
> *   $\rho_1 = \frac{\theta_0 \theta_1 + \theta_1 \theta_2}{\theta_0^2 + \theta_1^2 + \theta_2^2} = \frac{1 \cdot 0.5 + 0.5 \cdot 0.3}{1^2 + 0.5^2 + 0.3^2} = \frac{0.5 + 0.15}{1 + 0.25 + 0.09} = \frac{0.65}{1.34} \approx 0.485$
> *   $\rho_2 = \frac{\theta_0 \theta_2}{\theta_0^2 + \theta_1^2 + \theta_2^2} = \frac{1 \cdot 0.3}{1.34} \approx 0.224$
> *   $\rho_j = 0$ para $j > 2$
>
> O ACF para este processo MA(2) seria significativo nos lags 1 e 2, e zero para lags superiores.

### Conclus√£o

A fun√ß√£o de autocorrela√ß√£o (ACF) √© uma ferramenta essencial na an√°lise de s√©ries temporais, fornecendo informa√ß√µes valiosas sobre a depend√™ncia temporal e a ordem dos processos MA [^49]. Para processos MA(1), a ACF tem uma forma simples, com apenas a autocorrela√ß√£o no lag 1 sendo possivelmente diferente de zero, o que facilita sua identifica√ß√£o. Algoritmos para plotar a ACF auxiliam na visualiza√ß√£o e interpreta√ß√£o da estrutura de depend√™ncia temporal, complementando a an√°lise te√≥rica. A correta interpreta√ß√£o da ACF, em conjunto com outras ferramentas estat√≠sticas, √© crucial para a modelagem e previs√£o de s√©ries temporais com processos MA.

### Refer√™ncias
[^44]: Imagine a battery of I such computers generating sequences {y{1}, {y{2} ...
[^45]: The variance of the random variable Y, (denoted you) is similarly defined as...
[^46]: In this text the term "stationary" by itself is taken to mean "covariance-stationary."
[^48]: Let {8} be white noise as in [3.2.1] through [3.2.3], and consider the process...
[^49]: The jth autocorrelation of a covariance-stationary process (denoted p‚ÇÅ) is defined as its jth autocovariance divided by the variance:
[^51]: Since the e's are uncorrelated, the variance [3.3.9] is¬≤
<!-- END -->