## Autocovari√¢ncia e a Matriz de Vari√¢ncia-Covari√¢ncia do Vetor de Observa√ß√µes Defasadas

### Introdu√ß√£o

Este cap√≠tulo explora a conex√£o entre a **autocovari√¢ncia** e a estrutura da matriz de vari√¢ncia-covari√¢ncia de um vetor contendo as observa√ß√µes defasadas de uma s√©rie temporal. Com base nos conceitos de autocovari√¢ncia em processos estoc√°sticos [^"Autocovariance and Its Properties"], da rela√ß√£o entre a 0¬™ autocovari√¢ncia e a vari√¢ncia [^"The 0th autocovariance (Œ≥_0t) is equivalent to the variance of Y_t, denoted as Var(Y_t), representing the spread or dispersion of the series around its mean at time t, and serves as a measure of the series' volatility."] e do comportamento da autocovari√¢ncia em processos com ru√≠do branco Gaussiano [^"For a process where Y_t = Œº + Œµ_t (constant plus Gaussian white noise), the autocovariances Œ≥_jt are all zero for j ‚â† 0, indicating no correlation between the series and its lagged values, reflecting the random nature of white noise."], aprofundaremos nossa compreens√£o de como a autocovari√¢ncia pode ser interpretada dentro do contexto de uma matriz de vari√¢ncia-covari√¢ncia.

### Conceitos Fundamentais

Considere um vetor $\mathbf{x}_t$ que consiste nas $j+1$ observa√ß√µes mais recentes de uma s√©rie temporal $Y$ no instante *t* [^45]:

$$\mathbf{x}_t =
\begin{bmatrix}
Y_t \\
Y_{t-1} \\
\vdots \\
Y_{t-j}
\end{bmatrix}
$$

A **matriz de vari√¢ncia-covari√¢ncia** de $\mathbf{x}_t$, denotada por $\mathbf{\Sigma}_t$, √© uma matriz quadrada de tamanho $(j+1) \times (j+1)$ cujos elementos representam as covari√¢ncias entre as diferentes vari√°veis aleat√≥rias que comp√µem o vetor $\mathbf{x}_t$. O elemento na linha *k* e coluna *l* de $\mathbf{\Sigma}_t$ √© dado por:

$$\Sigma_{kl} = Cov(Y_{t-(k-1)}, Y_{t-(l-1)}) = E[(Y_{t-(k-1)} - \mu_{t-(k-1)})(Y_{t-(l-1)} - \mu_{t-(l-1)})]$$

onde $\mu_{t-(k-1)}$ e $\mu_{t-(l-1)}$ s√£o as m√©dias de $Y_{t-(k-1)}$ e $Y_{t-(l-1)}$, respectivamente.

Para um processo estacion√°rio, a matriz de vari√¢ncia-covari√¢ncia √© sim√©trica e Toeplitz, ou seja, seus elementos ao longo de cada diagonal s√£o constantes. Nesse caso, podemos expressar $\mathbf{\Sigma}$ em termos das autocovari√¢ncias $\gamma_0, \gamma_1, \dots, \gamma_j$ como:

$$\mathbf{\Sigma} =
\begin{bmatrix}
\gamma_0 & \gamma_1 & \gamma_2 & \cdots & \gamma_j \\
\gamma_1 & \gamma_0 & \gamma_1 & \cdots & \gamma_{j-1} \\
\gamma_2 & \gamma_1 & \gamma_0 & \cdots & \gamma_{j-2} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\gamma_j & \gamma_{j-1} & \gamma_{j-2} & \cdots & \gamma_0
\end{bmatrix}
$$

> üí° **Exemplo Num√©rico:** Suponha que temos uma s√©rie temporal com as seguintes autocovari√¢ncias: $\gamma_0 = 9$, $\gamma_1 = 3$, $\gamma_2 = 1$. Queremos construir a matriz de vari√¢ncia-covari√¢ncia para $j=2$.

$$\mathbf{\Sigma} =
\begin{bmatrix}
9 & 3 & 1 \\
3 & 9 & 3 \\
1 & 3 & 9
\end{bmatrix}
$$
Essa matriz mostra a vari√¢ncia ($Y_t$ com $Y_t$) e as covari√¢ncias ($Y_t$ com $Y_{t-1}$, $Y_t$ com $Y_{t-2}$, etc.). Note que √© sim√©trica e Toeplitz.

Dessa forma, o elemento (1, j+1) da matriz $\mathbf{\Sigma}$ √© dado por:

$$\Sigma_{1, j+1} = Cov(Y_t, Y_{t-j}) = \gamma_{j}$$

Assim, a autocovari√¢ncia $\gamma_j$ pode ser interpretada como o elemento (1, j+1) da matriz de vari√¢ncia-covari√¢ncia $\mathbf{\Sigma}$ do vetor $\mathbf{x}_t$ [^45].

> üí° **Exemplo Num√©rico:** Considere um processo estacion√°rio com $\gamma_0 = 4$, $\gamma_1 = 2$, $\gamma_2 = 1$, e queremos construir a matriz de vari√¢ncia-covari√¢ncia para $j=2$. O vetor $\mathbf{x}_t$ ser√°:

$$\mathbf{x}_t =
\begin{bmatrix}
Y_t \\
Y_{t-1} \\
Y_{t-2}
\end{bmatrix}
$$

A matriz de vari√¢ncia-covari√¢ncia $\mathbf{\Sigma}$ ser√°:

$$\mathbf{\Sigma} =
\begin{bmatrix}
4 & 2 & 1 \\
2 & 4 & 2 \\
1 & 2 & 4
\end{bmatrix}
$$

Nesse caso, o elemento (1,3) da matriz √© $\Sigma_{1,3} = 1 = \gamma_2$. Isso demonstra que o elemento (1, j+1) da matriz corresponde √† autocovari√¢ncia na defasagem *j*.

**Teorema 7:** Para um processo estacion√°rio, a matriz de autocovari√¢ncia $\mathbf{\Sigma}$ √© n√£o negativa definida.

*Demonstra√ß√£o:* Seja $\mathbf{a}$ um vetor arbitr√°rio de tamanho $(j+1) \times 1$. Ent√£o, a forma quadr√°tica associada √† matriz $\mathbf{\Sigma}$ √© dada por $\mathbf{a}^T \mathbf{\Sigma} \mathbf{a}$. Queremos mostrar que $\mathbf{a}^T \mathbf{\Sigma} \mathbf{a} \geq 0$.

I. Seja $Z = \sum_{i=0}^{j} a_{i+1} Y_{t-i}$, onde $a_{i+1}$ s√£o os elementos do vetor $\mathbf{a}$. Ent√£o, $Var(Z) = Var(\sum_{i=0}^{j} a_{i+1} Y_{t-i}) = E[(\sum_{i=0}^{j} a_{i+1} (Y_{t-i} - \mu))^2] = \mathbf{a}^T \mathbf{\Sigma} \mathbf{a}$.

II. Como a vari√¢ncia √© sempre n√£o negativa, temos $Var(Z) \geq 0$.
   *Justificativa*: A vari√¢ncia de qualquer vari√°vel aleat√≥ria real √© sempre n√£o negativa.

III. Portanto, $\mathbf{a}^T \mathbf{\Sigma} \mathbf{a} \geq 0$ para todo vetor $\mathbf{a}$, o que implica que $\mathbf{\Sigma}$ √© n√£o negativa definida. $\blacksquare$

**Corol√°rio 7.1:** Os autovalores da matriz de autocovari√¢ncia $\mathbf{\Sigma}$ de um processo estacion√°rio s√£o n√£o negativos.

*Demonstra√ß√£o:* Seja $\lambda$ um autovalor de $\mathbf{\Sigma}$ e $\mathbf{v}$ o autovetor correspondente. Ent√£o, $\mathbf{\Sigma} \mathbf{v} = \lambda \mathbf{v}$. Multiplicando ambos os lados por $\mathbf{v}^T$, obtemos $\mathbf{v}^T \mathbf{\Sigma} \mathbf{v} = \lambda \mathbf{v}^T \mathbf{v}$. Como $\mathbf{\Sigma}$ √© n√£o negativa definida, $\mathbf{v}^T \mathbf{\Sigma} \mathbf{v} \geq 0$. Al√©m disso, $\mathbf{v}^T \mathbf{v} > 0$ (a menos que $\mathbf{v}$ seja o vetor nulo). Portanto, $\lambda = \frac{\mathbf{v}^T \mathbf{\Sigma} \mathbf{v}}{\mathbf{v}^T \mathbf{v}} \geq 0$, o que significa que todos os autovalores de $\mathbf{\Sigma}$ s√£o n√£o negativos. $\blacksquare$

> üí° **Exemplo Num√©rico:** Usando a matriz do exemplo anterior,

$$\mathbf{\Sigma} =
\begin{bmatrix}
4 & 2 & 1 \\
2 & 4 & 2 \\
1 & 2 & 4
\end{bmatrix}
$$
Podemos calcular os autovalores de $\mathbf{\Sigma}$ e verificar que todos s√£o n√£o negativos.

```python
import numpy as np

Sigma = np.array([[4, 2, 1],
                  [2, 4, 2],
                  [1, 2, 4]])

eigenvalues = np.linalg.eigvals(Sigma)
print(f"Autovalores da matriz Sigma: {eigenvalues}")
```
A sa√≠da ser√°:
```
Autovalores da matriz Sigma: [ 7.         2.         3.]
```
Todos os autovalores s√£o positivos, confirmando que a matriz √© n√£o negativa definida.

> üí° **Exemplo Num√©rico:** Vamos construir um vetor aleat√≥rio $\mathbf{a} = [1, -1, 0]^T$ e verificar que $\mathbf{a}^T \mathbf{\Sigma} \mathbf{a} \geq 0$.

```python
import numpy as np

Sigma = np.array([[4, 2, 1],
                  [2, 4, 2],
                  [1, 2, 4]])

a = np.array([1, -1, 0])

result = np.dot(a.T, np.dot(Sigma, a))
print(f"Resultado de a^T * Sigma * a: {result}")
```

A sa√≠da ser√°:
```
Resultado de a^T * Sigma * a: 2.0
```
O resultado √© 2.0, que √© maior ou igual a zero, confirmando que a matriz √© n√£o negativa definida.

**Lema 8:** Para um processo $Y_t$ com m√©dia zero, se $\mathbf{\Sigma}$ √© a matriz de vari√¢ncia-covari√¢ncia do vetor $\mathbf{x}_t$, ent√£o $E[\mathbf{x}_t \mathbf{x}_t^T] = \mathbf{\Sigma}$.

*Demonstra√ß√£o:* Seja $\mathbf{x}_t = [Y_t, Y_{t-1}, \dots, Y_{t-j}]^T$. Ent√£o, $\mathbf{x}_t \mathbf{x}_t^T$ √© uma matriz de tamanho $(j+1) \times (j+1)$ cujos elementos s√£o $Y_{t-k}Y_{t-l}$ para $k, l = 0, 1, \dots, j$. Portanto, $E[\mathbf{x}_t \mathbf{x}_t^T]$ √© uma matriz cujos elementos s√£o $E[Y_{t-k}Y_{t-l}]$. Como $E[Y_t] = 0$, temos $Cov(Y_{t-k}, Y_{t-l}) = E[Y_{t-k}Y_{t-l}] - E[Y_{t-k}]E[Y_{t-l}] = E[Y_{t-k}Y_{t-l}]$. Portanto, $E[\mathbf{x}_t \mathbf{x}_t^T] = \mathbf{\Sigma}$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que temos as seguintes observa√ß√µes de uma s√©rie temporal com m√©dia zero: $Y_t = 1$, $Y_{t-1} = -1$, $Y_{t-2} = 0$. Ent√£o, $\mathbf{x}_t = [1, -1, 0]^T$ e $j=2$. Calcule $\mathbf{x}_t \mathbf{x}_t^T$.

$$\mathbf{x}_t \mathbf{x}_t^T =
\begin{bmatrix}
1 \\ -1 \\ 0
\end{bmatrix}
\begin{bmatrix}
1 & -1 & 0
\end{bmatrix}
=
\begin{bmatrix}
1 & -1 & 0 \\
-1 & 1 & 0 \\
0 & 0 & 0
\end{bmatrix}
$$
Se repetirmos esse c√°lculo v√°rias vezes e tomarmos a m√©dia, obteremos uma estimativa da matriz de vari√¢ncia-covari√¢ncia.

**Teorema 9:** Se $\mathbf{\Sigma}$ √© a matriz de vari√¢ncia-covari√¢ncia de um processo estacion√°rio, ent√£o $\gamma_0 \geq |\gamma_k|$ para todo $k$.

*Demonstra√ß√£o:*
Considere a correla√ß√£o entre $Y_t$ e $Y_{t-k}$:
$$ \rho_k = \frac{Cov(Y_t, Y_{t-k})}{\sqrt{Var(Y_t)Var(Y_{t-k})}} = \frac{\gamma_k}{\gamma_0} $$
Como a correla√ß√£o est√° sempre entre -1 e 1, temos:
$$ -1 \leq \rho_k \leq 1 $$
$$ -1 \leq \frac{\gamma_k}{\gamma_0} \leq 1 $$
Multiplicando todos os lados por $\gamma_0$ (que √© sempre positivo, pois √© uma vari√¢ncia):
$$ -\gamma_0 \leq \gamma_k \leq \gamma_0 $$
Portanto, $|\gamma_k| \leq \gamma_0$ para todo $k$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um processo estacion√°rio com $\gamma_0 = 9$. Isso significa que todas as outras autocovari√¢ncias, como $\gamma_1$, $\gamma_2$, etc., devem estar entre -9 e 9. Por exemplo, $\gamma_1$ pode ser 3, mas n√£o pode ser 10, pois violaria a condi√ß√£o $|\gamma_k| \leq \gamma_0$. Se $\gamma_1 = 3$, ent√£o a correla√ß√£o $\rho_1 = \frac{3}{9} = \frac{1}{3}$, que est√° dentro dos limites de -1 e 1.

Agora, vamos adicionar um lema que pode ser √∫til para modelagem e identifica√ß√£o de processos:

**Lema 9.1:** Para um processo AR(1) dado por $Y_t = \phi Y_{t-1} + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco com vari√¢ncia $\sigma^2$, as autocovari√¢ncias decaem exponencialmente.

*Demonstra√ß√£o:*
Para um processo AR(1), temos:
$\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$
$\gamma_1 = \phi \gamma_0$
$\gamma_2 = \phi \gamma_1 = \phi^2 \gamma_0$
E, em geral, $\gamma_k = \phi^k \gamma_0$.

Portanto, o padr√£o de autocovari√¢ncia √© $\gamma_k = \phi^k \gamma_0$. Como $|\phi| < 1$ para que o processo seja estacion√°rio, $\gamma_k$ decai exponencialmente √† medida que *k* aumenta. $\blacksquare$

> üí° **Exemplo Num√©rico:** Seja $\phi = 0.5$ e $\sigma^2 = 1$. Ent√£o, $\gamma_0 = \frac{1}{1 - 0.5^2} = \frac{1}{0.75} = \frac{4}{3} \approx 1.33$. As autocovari√¢ncias subsequentes ser√£o: $\gamma_1 = 0.5 * \frac{4}{3} = \frac{2}{3} \approx 0.67$, $\gamma_2 = 0.5^2 * \frac{4}{3} = \frac{1}{3} \approx 0.33$, $\gamma_3 = 0.5^3 * \frac{4}{3} = \frac{1}{6} \approx 0.17$. Podemos ver que as autocovari√¢ncias est√£o decaindo exponencialmente.

Para complementar a an√°lise do processo AR(1), podemos estender esse resultado e considerar o processo AR(p):

**Lema 9.2:** Para um processo AR(p) dado por $Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \dots + \phi_p Y_{t-p} + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco com vari√¢ncia $\sigma^2$, as autocovari√¢ncias satisfazem as equa√ß√µes de Yule-Walker:

$\gamma_k = \phi_1 \gamma_{k-1} + \phi_2 \gamma_{k-2} + \dots + \phi_p \gamma_{k-p}$ para $k > 0$

*Demonstra√ß√£o:*

Multiplicando ambos os lados da equa√ß√£o do processo AR(p) por $Y_{t-k}$ e tomando a esperan√ßa, obtemos:

$E[Y_t Y_{t-k}] = \phi_1 E[Y_{t-1} Y_{t-k}] + \phi_2 E[Y_{t-2} Y_{t-k}] + \dots + \phi_p E[Y_{t-p} Y_{t-k}] + E[\epsilon_t Y_{t-k}]$

Como $E[Y_t Y_{t-k}] = \gamma_k$ e $E[\epsilon_t Y_{t-k}] = 0$ para $k > 0$, e usando a propriedade de estacionariedade, temos:

$\gamma_k = \phi_1 \gamma_{k-1} + \phi_2 \gamma_{k-2} + \dots + \phi_p \gamma_{k-p}$ para $k > 0$

Essas equa√ß√µes s√£o conhecidas como equa√ß√µes de Yule-Walker e s√£o fundamentais para a estimativa dos par√¢metros $\phi_i$ do processo AR(p). $\blacksquare$

> üí° **Exemplo Num√©rico:** Para um processo AR(2) com $Y_t = 0.6 Y_{t-1} + 0.2 Y_{t-2} + \epsilon_t$, temos $\phi_1 = 0.6$ e $\phi_2 = 0.2$. As equa√ß√µes de Yule-Walker s√£o:

> $\gamma_1 = 0.6 \gamma_0 + 0.2 \gamma_{-1} = 0.6 \gamma_0 + 0.2 \gamma_1$ (j√° que $\gamma_{-1} = \gamma_1$)
> $\gamma_2 = 0.6 \gamma_1 + 0.2 \gamma_0$

> Podemos usar essas equa√ß√µes para expressar $\gamma_1$ e $\gamma_2$ em termos de $\gamma_0$. Se soubermos $\gamma_0$ (que est√° relacionado com a vari√¢ncia do ru√≠do), podemos calcular as outras autocovari√¢ncias.

### Implica√ß√µes e Aplica√ß√µes

A interpreta√ß√£o da autocovari√¢ncia como um elemento da matriz de vari√¢ncia-covari√¢ncia de um vetor de observa√ß√µes defasadas tem v√°rias implica√ß√µes e aplica√ß√µes:

1.  **Modelagem de S√©ries Temporais:** Compreender a estrutura de autocovari√¢ncia √© fundamental para a constru√ß√£o de modelos de s√©ries temporais, como modelos ARMA. A matriz de vari√¢ncia-covari√¢ncia fornece informa√ß√µes valiosas sobre as depend√™ncias temporais nos dados, que podem ser utilizadas para estimar os par√¢metros do modelo.
2.  **Previs√£o:** A matriz de vari√¢ncia-covari√¢ncia pode ser usada para calcular previs√µes √≥timas para a s√©rie temporal. Por exemplo, a previs√£o de $Y_{t+1}$ dado o vetor $\mathbf{x}_t$ pode ser expressa em termos da matriz de vari√¢ncia-covari√¢ncia.
3.  **An√°lise de Risco:** Em finan√ßas, a matriz de vari√¢ncia-covari√¢ncia √© amplamente utilizada para an√°lise de risco e gest√£o de portf√≥lio. Ela permite quantificar a volatilidade e a correla√ß√£o entre diferentes ativos financeiros, auxiliando na tomada de decis√µes de investimento.
4.  **Filtragem de Kalman:** A matriz de vari√¢ncia-covari√¢ncia desempenha um papel crucial no filtro de Kalman, um algoritmo recursivo para estimar o estado de um sistema din√¢mico a partir de uma s√©rie de medi√ß√µes ruidosas. O filtro de Kalman utiliza a matriz de vari√¢ncia-covari√¢ncia para ponderar as diferentes fontes de informa√ß√£o e obter uma estimativa √≥tima do estado do sistema.

Al√©m dessas aplica√ß√µes, a estrutura da matriz de vari√¢ncia-covari√¢ncia, especialmente a propriedade de ser Toeplitz para processos estacion√°rios, simplifica muitos c√°lculos e algoritmos. Por exemplo, a invers√£o de matrizes Toeplitz pode ser realizada de forma mais eficiente do que a invers√£o de matrizes gen√©ricas.

Para complementar a discuss√£o sobre modelagem de s√©ries temporais, vamos introduzir um corol√°rio relacionado √† invertibilidade de processos de m√©dias m√≥veis (MA):

**Corol√°rio 9.3:** Um processo de m√©dias m√≥veis MA(q) dado por $Y_t = \epsilon_t + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q}$, onde $\epsilon_t$ √© ru√≠do branco, √© invert√≠vel se e somente se as ra√≠zes do polin√¥mio $\Theta(z) = 1 + \theta_1 z + \dots + \theta_q z^q$ est√£o fora do c√≠rculo unit√°rio.

*Demonstra√ß√£o:*
A invertibilidade de um processo MA(q) significa que ele pode ser reescrito como um processo AR($\infty$). Para que essa representa√ß√£o seja v√°lida, os coeficientes do processo AR($\infty$) devem ser absolutamente som√°veis, o que implica que as ra√≠zes do polin√¥mio caracter√≠stico do processo MA(q) devem estar fora do c√≠rculo unit√°rio. Isso garante que o processo possa ser expresso como uma combina√ß√£o linear convergente das observa√ß√µes passadas $Y_t$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um processo MA(1) dado por $Y_t = \epsilon_t + 0.5 \epsilon_{t-1}$. O polin√¥mio √© $\Theta(z) = 1 + 0.5z$. A raiz √© $z = -2$, que est√° fora do c√≠rculo unit√°rio ($|z| = 2 > 1$). Portanto, este processo MA(1) √© invert√≠vel.

> üí° **Exemplo Num√©rico:** Considere um processo MA(1) dado por $Y_t = \epsilon_t + 2 \epsilon_{t-1}$. O polin√¥mio √© $\Theta(z) = 1 + 2z$. A raiz √© $z = -0.5$, que est√° dentro do c√≠rculo unit√°rio ($|z| = 0.5 < 1$). Portanto, este processo MA(1) n√£o √© invert√≠vel.

### Conclus√£o

A interpreta√ß√£o da autocovari√¢ncia como um elemento da matriz de vari√¢ncia-covari√¢ncia de um vetor de observa√ß√µes defasadas oferece uma perspectiva valiosa sobre a estrutura de depend√™ncia temporal em s√©ries temporais. Essa conex√£o fornece *insights* importantes para modelagem, previs√£o, an√°lise de risco e outras aplica√ß√µes. Al√©m disso, a propriedade de que a matriz de autocovari√¢ncia √© n√£o negativa definida garante que a vari√¢ncia de qualquer combina√ß√£o linear das observa√ß√µes √© sempre n√£o negativa, o que √© fundamental para a consist√™ncia dos modelos e para a interpreta√ß√£o dos resultados.

### Refer√™ncias
[^45]: Given a particular realization such as ...
[^"Autocovariance and Its Properties"]: SECTION_PLACEHOLDER (from previous chapter)
[^"The 0th autocovariance (Œ≥_0t) is equivalent to the variance of Y_t, denoted as Var(Y_t), representing the spread or dispersion of the series around its mean at time t, and serves as a measure of the series' volatility."]: SECTION_PLACEHOLDER (from previous chapter)
[^"For a process where Y_t = Œº + Œµ_t (constant plus Gaussian white noise), the autocovariances Œ≥_jt are all zero for j ‚â† 0, indicating no correlation between the series and its lagged values, reflecting the random nature of white noise."]: SECTION_PLACEHOLDER (from previous chapter)
<!-- END -->