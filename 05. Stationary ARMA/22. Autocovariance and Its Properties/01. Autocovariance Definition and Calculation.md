## Autocovariance and Its Properties

### IntroduÃ§Ã£o

Este capÃ­tulo se dedica ao estudo da **autocovariÃ¢ncia** em sÃ©ries temporais estacionÃ¡rias, um conceito crucial para entender a estrutura de dependÃªncia temporal dos dados. Construindo sobre a definiÃ§Ã£o de **esperanÃ§a** e **variÃ¢ncia** de uma sÃ©rie temporal [^44], exploraremos como a autocovariÃ¢ncia quantifica a relaÃ§Ã£o entre observaÃ§Ãµes em diferentes pontos no tempo. Este estudo Ã© fundamental para modelagem, previsÃ£o e anÃ¡lise de sÃ©ries temporais, particularmente no contexto de processos ARMA (Auto-Regressive Moving Average).

### Conceitos Fundamentais

A **autocovariÃ¢ncia**, denotada por $\gamma_{jt}$, desempenha um papel central na caracterizaÃ§Ã£o de processos estocÃ¡sticos, especialmente em sÃ©ries temporais [^45]. Ela quantifica a covariÃ¢ncia entre uma variÃ¡vel $Y_t$ e sua versÃ£o defasada $Y_{t-j}$, fornecendo *insights* sobre a dependÃªncia temporal da sÃ©rie. Formalmente, a autocovariÃ¢ncia Ã© definida como [^45]:

$$\gamma_{jt} = E[(Y_t - \mu_t)(Y_{t-j} - \mu_{t-j})]$$

onde $\mu_t$ representa a mÃ©dia da sÃ©rie no instante *t*. Ã‰ importante notar que $\mu_t$ pode variar ao longo do tempo, admitindo a possibilidade de que a mÃ©dia seja uma funÃ§Ã£o do tempo de observaÃ§Ã£o [^44].

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que tenhamos uma sÃ©rie temporal de temperaturas diÃ¡rias em uma cidade. Se a mÃ©dia da temperatura no dia *t* Ã© $\mu_t = 25^\circ C$ e a mÃ©dia no dia *t-1* Ã© $\mu_{t-1} = 23^\circ C$, e observamos que a temperatura no dia *t* Ã© $Y_t = 28^\circ C$ e no dia *t-1* Ã© $Y_{t-1} = 26^\circ C$. EntÃ£o, o termo $(Y_t - \mu_t)(Y_{t-j} - \mu_{t-j}) = (28 - 25)(26 - 23) = 3 \times 3 = 9$. Se repetirmos essa mediÃ§Ã£o por vÃ¡rios dias e calcularmos a mÃ©dia desses produtos, obteremos uma estimativa da autocovariÃ¢ncia $\gamma_{1t}$ para essa sÃ©rie temporal.

Quando $j = 0$, a autocovariÃ¢ncia $\gamma_{0t}$ se reduz Ã  variÃ¢ncia da sÃ©rie no instante *t*, ou seja [^45]:

$$\gamma_{0t} = E[(Y_t - \mu_t)^2]$$

Essa observaÃ§Ã£o ressalta a interpretaÃ§Ã£o da autocovariÃ¢ncia como uma medida da variabilidade conjunta de $Y_t$ e $Y_{t-j}$.

> ðŸ’¡ **Exemplo NumÃ©rico:** Usando os dados do exemplo anterior, se quisermos calcular $\gamma_{0t}$ para o dia *t*, calcularÃ­amos $(Y_t - \mu_t)^2 = (28 - 25)^2 = 3^2 = 9$. Ao calcularmos a mÃ©dia desses valores ao longo de vÃ¡rios dias, obteremos uma estimativa da variÃ¢ncia da sÃ©rie temporal no instante *t*.

**ProposiÃ§Ã£o 1.** *Se $Y_t$ e $Y_{t-j}$ sÃ£o independentes, entÃ£o $\gamma_{jt} = 0$.*

*DemonstraÃ§Ã£o.* Se $Y_t$ e $Y_{t-j}$ sÃ£o independentes, entÃ£o $E[Y_t Y_{t-j}] = E[Y_t]E[Y_{t-j}] = \mu_t \mu_{t-j}$. Portanto,
$\gamma_{jt} = E[(Y_t - \mu_t)(Y_{t-j} - \mu_{t-j})] = E[Y_t Y_{t-j} - Y_t \mu_{t-j} - \mu_t Y_{t-j} + \mu_t \mu_{t-j}] = E[Y_t Y_{t-j}] - \mu_t \mu_{t-j} - \mu_t \mu_{t-j} + \mu_t \mu_{t-j} = \mu_t \mu_{t-j} - \mu_t \mu_{t-j} = 0$. $\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico:** Imagine que estamos analisando uma sÃ©rie temporal de vendas de sorvete e outra de nÃºmero de carros vendidos. Se nÃ£o houver relaÃ§Ã£o causal ou correlaÃ§Ã£o entre as duas sÃ©ries, ou seja, as vendas de sorvete nÃ£o influenciam as vendas de carros e vice-versa, entÃ£o a covariÃ¢ncia entre elas serÃ¡ aproximadamente zero. Neste caso, $\gamma_{jt}$ seria prÃ³ximo de zero para qualquer defasagem *j*.

**ProposiÃ§Ã£o 1.1.** *Se $Y_t$ e $Y_{t-j}$ sÃ£o nÃ£o correlacionadas e tem mÃ©dia zero, entÃ£o $\gamma_{jt} = 0$.*

*DemonstraÃ§Ã£o.* Se $Y_t$ e $Y_{t-j}$ sÃ£o nÃ£o correlacionadas, entÃ£o $E[Y_t Y_{t-j}] = 0$. Se a mÃ©dia Ã© zero, $\mu_t = \mu_{t-j} = 0$. Portanto, $\gamma_{jt} = E[Y_t Y_{t-j}] = 0$. $\blacksquare$

**Propriedades da AutocovariÃ¢ncia:**

1.  **Simetria (para processos estacionÃ¡rios):** Para processos covariance-stationary, a autocovariÃ¢ncia depende apenas da defasagem *j* e nÃ£o do tempo *t*. Nesse caso, $\gamma_{jt} = \gamma_j = \gamma_{-j}$ [^46]. Isso significa que a autocovariÃ¢ncia entre $Y_t$ e $Y_{t-j}$ Ã© a mesma que entre $Y_t$ e $Y_{t+j}$. Essa propriedade Ã© demonstrada no texto [^46], substituindo *t* por *t+j* e mostrando a igualdade entre $E[(Y_t - \mu)(Y_{t-j} - \mu)]$ e $E[(Y_{t+j} - \mu)(Y_t - \mu)]$.

    **Prova:**
    Para um processo estacionÃ¡rio, $\mu_t = \mu$ para todo $t$. Queremos mostrar que $\gamma_j = \gamma_{-j}$, ou seja, $E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(Y_t - \mu)(Y_{t+j} - \mu)]$.

    I. Seja $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$.

    II. Agora, considere $\gamma_{-j} = E[(Y_t - \mu)(Y_{t+j} - \mu)]$.

    III. Seja $k = t+j$, entÃ£o $t = k-j$, e substituindo na expressÃ£o de $\gamma_{-j}$:
    $\gamma_{-j} = E[(Y_{k-j} - \mu)(Y_{k} - \mu)]$.

    IV. Como o processo Ã© estacionÃ¡rio, a autocovariÃ¢ncia depende apenas da defasagem, entÃ£o:
    $E[(Y_{k-j} - \mu)(Y_{k} - \mu)] = E[(Y_t - \mu)(Y_{t+j} - \mu)]$ Ã© igual a $E[(Y_t - \mu)(Y_{t-j} - \mu)]$

    V. Portanto, $\gamma_j = \gamma_{-j}$. $\blacksquare$

    > ðŸ’¡ **Exemplo NumÃ©rico:** Considere uma sÃ©rie temporal estacionÃ¡ria de retornos de aÃ§Ãµes com mÃ©dia $\mu = 0.05$. Se a autocovariÃ¢ncia entre os retornos no dia *t* e *t-2* Ã© $\gamma_2 = 0.01$, entÃ£o a autocovariÃ¢ncia entre os retornos no dia *t* e *t+2* tambÃ©m serÃ¡ $\gamma_{-2} = 0.01$. Isso demonstra a simetria da autocovariÃ¢ncia em processos estacionÃ¡rios.

2.  **RelaÃ§Ã£o com a AutocorrelaÃ§Ã£o:** A autocorrelaÃ§Ã£o, denotada por $\rho_j$, Ã© a autocovariÃ¢ncia normalizada pela variÃ¢ncia. Ela fornece uma medida adimensional da dependÃªncia temporal, variando entre -1 e 1 [^49]. A relaÃ§Ã£o entre autocovariÃ¢ncia e autocorrelaÃ§Ã£o Ã© dada por [^49]:

    $$\rho_j = \frac{\gamma_j}{\gamma_0}$$
    
    **Prova:**
    A autocorrelaÃ§Ã£o Ã© definida como a autocovariÃ¢ncia normalizada pela variÃ¢ncia.

    I. Por definiÃ§Ã£o, $\rho_j = \frac{Cov(Y_t, Y_{t-j})}{\sqrt{Var(Y_t)Var(Y_{t-j})}}$.

    II. Para um processo estacionÃ¡rio, $Var(Y_t) = Var(Y_{t-j}) = \gamma_0$.

    III. AlÃ©m disso, $Cov(Y_t, Y_{t-j}) = \gamma_j$.

    IV. Substituindo esses valores na definiÃ§Ã£o de $\rho_j$, obtemos $\rho_j = \frac{\gamma_j}{\sqrt{\gamma_0 \gamma_0}} = \frac{\gamma_j}{\gamma_0}$. $\blacksquare$

    > ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que a autocovariÃ¢ncia na defasagem 1, $\gamma_1$, para uma sÃ©rie temporal seja 0.5, e a variÃ¢ncia da sÃ©rie, $\gamma_0$, seja 2. EntÃ£o, a autocorrelaÃ§Ã£o na defasagem 1 Ã© $\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{0.5}{2} = 0.25$. Isso indica uma correlaÃ§Ã£o positiva moderada entre as observaÃ§Ãµes em instantes de tempo adjacentes.

3.  **AutocovariÃ¢ncia de White Noise:** Para um processo de ruÃ­do branco (white noise) $\{\epsilon_t\}$, que Ã© uma sequÃªncia de variÃ¡veis aleatÃ³rias nÃ£o correlacionadas com mÃ©dia zero e variÃ¢ncia constante $\sigma^2$, a autocovariÃ¢ncia Ã© zero para todas as defasagens nÃ£o nulas [^45]:

    $$E(\epsilon_t \epsilon_{t-\tau}) = 0 \text{ para } t \neq \tau$$

    Isso significa que as observaÃ§Ãµes em um processo de ruÃ­do branco sÃ£o independentes umas das outras.

    **Prova:**
    Um processo de ruÃ­do branco tem as seguintes propriedades: $E[\epsilon_t] = 0$ para todo $t$, $Var(\epsilon_t) = \sigma^2$ para todo $t$, e $Cov(\epsilon_t, \epsilon_s) = 0$ para $t \neq s$. Vamos mostrar que a autocovariÃ¢ncia Ã© zero para defasagens nÃ£o nulas.

    I. A autocovariÃ¢ncia Ã© definida como $\gamma_\tau = E[(\epsilon_t - E[\epsilon_t])(\epsilon_{t-\tau} - E[\epsilon_{t-\tau}])]$.

    II. Como $E[\epsilon_t] = 0$ para todo $t$, a expressÃ£o se simplifica para $\gamma_\tau = E[\epsilon_t \epsilon_{t-\tau}]$.

    III. Para $\tau \neq 0$, $\epsilon_t$ e $\epsilon_{t-\tau}$ sÃ£o nÃ£o correlacionadas, entÃ£o $E[\epsilon_t \epsilon_{t-\tau}] = E[\epsilon_t] E[\epsilon_{t-\tau}] = 0 \cdot 0 = 0$.

    IV. Portanto, $\gamma_\tau = 0$ para $\tau \neq 0$. $\blacksquare$

    > ðŸ’¡ **Exemplo NumÃ©rico:** Imagine que estamos gerando nÃºmeros aleatÃ³rios a partir de uma distribuiÃ§Ã£o normal com mÃ©dia 0 e variÃ¢ncia 1. Essa sequÃªncia de nÃºmeros aleatÃ³rios Ã© um exemplo de ruÃ­do branco. Se calcularmos a autocovariÃ¢ncia entre esses nÃºmeros em diferentes defasagens, esperarÃ­amos obter valores prÃ³ximos de zero para todas as defasagens nÃ£o nulas.
    ```python
    import numpy as np

    # Gerar 1000 nÃºmeros aleatÃ³rios de uma distribuiÃ§Ã£o normal
    np.random.seed(42)  # para reproducibilidade
    white_noise = np.random.normal(0, 1, 1000)

    # Calcular a autocovariÃ¢ncia para defasagens de 1 a 5
    def autocovariance(x, lag):
        n = len(x)
        x_mean = np.mean(x)
        
        # Pad with the mean to maintain length (other methods could be used)
        padded_x = np.concatenate([np.full(lag, x_mean), x])
        
        # Align time series to calculate covariance
        x_t = x - x_mean
        x_t_lagged = padded_x[lag:n+lag] - x_mean
        
        return np.sum(x_t * x_t_lagged) / n

    lags = range(1, 6)
    autocovariances = [autocovariance(white_noise, lag) for lag in lags]

    print("AutocovariÃ¢ncias para diferentes defasagens:")
    for lag, acov in zip(lags, autocovariances):
        print(f"Defasagem {lag}: {acov:.4f}")
    ```
    Resultado:
    ```
    AutocovariÃ¢ncias para diferentes defasagens:
    Defasagem 1: -0.0261
    Defasagem 2: -0.0347
    Defasagem 3: -0.0058
    Defasagem 4: 0.0020
    Defasagem 5: 0.0033
    ```
    Os valores sÃ£o prÃ³ximos de zero, como esperado para um ruÃ­do branco.

4.  **AutocovariÃ¢ncia em Modelos MA(1):** Em um processo MA(1) (Moving Average de primeira ordem) definido como $Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$, onde $\{\epsilon_t\}$ Ã© ruÃ­do branco, a autocovariÃ¢ncia Ã© diferente de zero apenas para a defasagem 1 [^48]. Especificamente, $\gamma_0 = (1+\theta^2)\sigma^2$ e $\gamma_1 = \theta \sigma^2$, enquanto $\gamma_j = 0$ para $|j| > 1$.

    **Prova:**
    Dado o processo MA(1) $Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$, vamos calcular as autocovariÃ¢ncias $\gamma_0$, $\gamma_1$ e $\gamma_j$ para $|j| > 1$.

    I. Primeiro, observe que $E[Y_t] = \mu$, jÃ¡ que $E[\epsilon_t] = 0$.

    II. Para $\gamma_0$, temos $\gamma_0 = E[(Y_t - \mu)^2] = E[(\epsilon_t + \theta \epsilon_{t-1})^2] = E[\epsilon_t^2 + 2\theta \epsilon_t \epsilon_{t-1} + \theta^2 \epsilon_{t-1}^2] = E[\epsilon_t^2] + 2\theta E[\epsilon_t \epsilon_{t-1}] + \theta^2 E[\epsilon_{t-1}^2]$.
    Como $\epsilon_t$ Ã© ruÃ­do branco, $E[\epsilon_t^2] = \sigma^2$ e $E[\epsilon_t \epsilon_{t-1}] = 0$. Portanto, $\gamma_0 = \sigma^2 + \theta^2 \sigma^2 = (1+\theta^2)\sigma^2$.

    III. Para $\gamma_1$, temos $\gamma_1 = E[(Y_t - \mu)(Y_{t-1} - \mu)] = E[(\epsilon_t + \theta \epsilon_{t-1})(\epsilon_{t-1} + \theta \epsilon_{t-2})] = E[\epsilon_t \epsilon_{t-1} + \theta \epsilon_{t-1}^2 + \theta \epsilon_t \epsilon_{t-2} + \theta^2 \epsilon_{t-1} \epsilon_{t-2}]$.
    Como $\epsilon_t$ Ã© ruÃ­do branco, $E[\epsilon_t \epsilon_{t-1}] = 0$, $E[\epsilon_t \epsilon_{t-2}] = 0$, e $E[\epsilon_{t-1} \epsilon_{t-2}] = 0$. Portanto, $\gamma_1 = \theta E[\epsilon_{t-1}^2] = \theta \sigma^2$.

    IV. Para $\gamma_j$ com $|j| > 1$, temos $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\epsilon_t + \theta \epsilon_{t-1})(\epsilon_{t-j} + \theta \epsilon_{t-j-1})]$.
    Expandindo, $\gamma_j = E[\epsilon_t \epsilon_{t-j} + \theta \epsilon_{t-1} \epsilon_{t-j} + \theta \epsilon_t \epsilon_{t-j-1} + \theta^2 \epsilon_{t-1} \epsilon_{t-j-1}]$. Como $|j| > 1$, todos os termos $E[\epsilon_t \epsilon_{t-j}]$, $E[\epsilon_{t-1} \epsilon_{t-j}]$, $E[\epsilon_t \epsilon_{t-j-1}]$ e $E[\epsilon_{t-1} \epsilon_{t-j-1}]$ sÃ£o zero devido Ã  propriedade do ruÃ­do branco. Portanto, $\gamma_j = 0$ para $|j| > 1$. $\blacksquare$

    > ðŸ’¡ **Exemplo NumÃ©rico:** Considere um processo MA(1) com $\mu = 0$, $\theta = 0.6$ e $\sigma^2 = 1$. EntÃ£o, $\gamma_0 = (1 + 0.6^2) \times 1 = 1.36$ e $\gamma_1 = 0.6 \times 1 = 0.6$. Todas as outras autocovariÃ¢ncias $\gamma_j$ para $|j| > 1$ sÃ£o zero.  A autocorrelaÃ§Ã£o em lag 1 seria $\rho_1 = \frac{0.6}{1.36} \approx 0.441$. Isso significa que hÃ¡ uma correlaÃ§Ã£o positiva entre uma observaÃ§Ã£o e a observaÃ§Ã£o anterior.

    **Teorema 1.** A autocovariÃ¢ncia de um processo MA(q) Ã© zero para defasagens maiores que q.

    *DemonstraÃ§Ã£o.* Seja $Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q}$. EntÃ£o, para $j > q$:
    $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\epsilon_t + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q})(\epsilon_{t-j} + \theta_1 \epsilon_{t-j-1} + \dots + \theta_q \epsilon_{t-j-q})]$. Como $j > q$, nenhum termo $\epsilon_{t-i}$ em $(\epsilon_t + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q})$ coincide com algum termo $\epsilon_{t-j-k}$ em $(\epsilon_{t-j} + \theta_1 \epsilon_{t-j-1} + \dots + \theta_q \epsilon_{t-j-q})$, e como $E[\epsilon_t \epsilon_s] = 0$ para $t \neq s$, segue que $\gamma_j = 0$ para $j > q$. $\blacksquare$

    **Teorema 1.1.** A autocovariÃ¢ncia de um processo AR(1) decai exponencialmente com o aumento da defasagem.

    *DemonstraÃ§Ã£o.* Seja $Y_t = \phi Y_{t-1} + \epsilon_t$, onde $|\phi| < 1$ para garantir a estacionariedade. Assumindo que $E[Y_t] = 0$ e $E[\epsilon_t] = 0$. Multiplicando ambos os lados por $Y_{t-j}$ e tomando a esperanÃ§a, obtemos:
    $E[Y_t Y_{t-j}] = \phi E[Y_{t-1} Y_{t-j}] + E[\epsilon_t Y_{t-j}]$. Para $j > 0$, $E[\epsilon_t Y_{t-j}] = 0$ porque $\epsilon_t$ Ã© independente de $Y_{t-j}$. Portanto, $\gamma_j = \phi \gamma_{j-1}$.
    Aplicando recursivamente, $\gamma_j = \phi^j \gamma_0$. Como $|\phi| < 1$, $\gamma_j$ decai exponencialmente com o aumento de $j$. $\blacksquare$

    > ðŸ’¡ **Exemplo NumÃ©rico:** Considere um processo AR(1) com $\phi = 0.7$ e $\sigma^2 = 1$. Se $\gamma_0 = \frac{\sigma^2}{1 - \phi^2} = \frac{1}{1 - 0.7^2} = \frac{1}{0.51} \approx 1.96$, entÃ£o $\gamma_1 = \phi \gamma_0 = 0.7 \times 1.96 = 1.372$, $\gamma_2 = \phi \gamma_1 = 0.7 \times 1.372 = 0.9604$, e assim por diante. Vemos que a autocovariÃ¢ncia decai exponencialmente Ã  medida que a defasagem aumenta.

5. **InterpretaÃ§Ã£o da AutocovariÃ¢ncia:** A autocovariÃ¢ncia pode ser vista como o probability limit de uma mÃ©dia amostral do produto dos desvios em relaÃ§Ã£o Ã  mÃ©dia [^45]:

    $$\gamma_{jt} = \text{plim } (1/I) \sum_{i=1}^I [Y_{t}^{(i)} - \mu_t] [Y_{t-j}^{(i)} - \mu_{t-j}]$$

    Onde $I$ Ã© o nÃºmero de realizaÃ§Ãµes da sÃ©rie temporal.

**Exemplo:**

Para o processo descrito por $Y_t = \mu + \epsilon_t$ [^45], onde $\epsilon_t$ Ã© um processo de ruÃ­do branco Gaussiano, a autocovariÃ¢ncia $\gamma_{jt}$ Ã© zero para todo $j \neq 0$ [^45]. Isso reflete a ausÃªncia de correlaÃ§Ã£o serial nesse processo. Em contraste, para um processo com tendÃªncia temporal $Y_t = \beta t + \epsilon_t$ [^45], a autocovariÃ¢ncia dependerÃ¡ de *t*, indicando que a dependÃªncia temporal varia ao longo do tempo.

> ðŸ’¡ **Exemplo NumÃ©rico:** Para $Y_t = \mu + \epsilon_t$, suponha $\mu = 10$ e $\epsilon_t$ Ã© ruÃ­do branco com variÃ¢ncia $\sigma^2 = 2$. EntÃ£o, $\gamma_0 = Var(Y_t) = Var(\epsilon_t) = 2$, e $\gamma_j = 0$ para $j \neq 0$. Para o processo $Y_t = 2t + \epsilon_t$, a autocovariÃ¢ncia dependerÃ¡ de *t*. Por exemplo, $Cov(Y_t, Y_{t-1}) = Cov(2t + \epsilon_t, 2(t-1) + \epsilon_{t-1}) = Cov(\epsilon_t, \epsilon_{t-1}) = 0$ apenas se $\epsilon_t$ e $\epsilon_{t-1}$ forem nÃ£o correlacionados. Contudo, a mÃ©dia varia com o tempo.

### ConclusÃ£o

A autocovariÃ¢ncia Ã© uma ferramenta essencial para analisar a estrutura de dependÃªncia temporal em sÃ©ries temporais. Ela quantifica a relaÃ§Ã£o entre observaÃ§Ãµes em diferentes pontos no tempo e fornece *insights* valiosos para modelagem, previsÃ£o e anÃ¡lise de dados. AtravÃ©s da autocovariÃ¢ncia e da autocorrelaÃ§Ã£o, podemos identificar padrÃµes de dependÃªncia, como a presenÃ§a de tendÃªncias, sazonalidade ou ciclos, que sÃ£o cruciais para a construÃ§Ã£o de modelos precisos e a obtenÃ§Ã£o de previsÃµes confiÃ¡veis. O entendimento das propriedades da autocovariÃ¢ncia permite distinguir entre diferentes tipos de processos estocÃ¡sticos, desde ruÃ­do branco atÃ© modelos ARMA mais complexos.

### ReferÃªncias
[^44]: Imagine a battery of I such computers generating sequences ...
[^45]: Given a particular realization such as ...
[^46]: Notice that if a process is covariance-stationary, the covariance between Y, and Y_{t-j} depends only on j...
<!-- END -->