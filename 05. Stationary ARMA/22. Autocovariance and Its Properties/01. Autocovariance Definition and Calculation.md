## Autocovariance and Its Properties

### Introdu√ß√£o

Este cap√≠tulo se dedica ao estudo da **autocovari√¢ncia** em s√©ries temporais estacion√°rias, um conceito crucial para entender a estrutura de depend√™ncia temporal dos dados. Construindo sobre a defini√ß√£o de **esperan√ßa** e **vari√¢ncia** de uma s√©rie temporal [^44], exploraremos como a autocovari√¢ncia quantifica a rela√ß√£o entre observa√ß√µes em diferentes pontos no tempo. Este estudo √© fundamental para modelagem, previs√£o e an√°lise de s√©ries temporais, particularmente no contexto de processos ARMA (Auto-Regressive Moving Average).

### Conceitos Fundamentais

A **autocovari√¢ncia**, denotada por $\gamma_{jt}$, desempenha um papel central na caracteriza√ß√£o de processos estoc√°sticos, especialmente em s√©ries temporais [^45]. Ela quantifica a covari√¢ncia entre uma vari√°vel $Y_t$ e sua vers√£o defasada $Y_{t-j}$, fornecendo *insights* sobre a depend√™ncia temporal da s√©rie. Formalmente, a autocovari√¢ncia √© definida como [^45]:

$$\gamma_{jt} = E[(Y_t - \mu_t)(Y_{t-j} - \mu_{t-j})]$$

onde $\mu_t$ representa a m√©dia da s√©rie no instante *t*. √â importante notar que $\mu_t$ pode variar ao longo do tempo, admitindo a possibilidade de que a m√©dia seja uma fun√ß√£o do tempo de observa√ß√£o [^44].

> üí° **Exemplo Num√©rico:** Suponha que tenhamos uma s√©rie temporal de temperaturas di√°rias em uma cidade. Se a m√©dia da temperatura no dia *t* √© $\mu_t = 25^\circ C$ e a m√©dia no dia *t-1* √© $\mu_{t-1} = 23^\circ C$, e observamos que a temperatura no dia *t* √© $Y_t = 28^\circ C$ e no dia *t-1* √© $Y_{t-1} = 26^\circ C$. Ent√£o, o termo $(Y_t - \mu_t)(Y_{t-j} - \mu_{t-j}) = (28 - 25)(26 - 23) = 3 \times 3 = 9$. Se repetirmos essa medi√ß√£o por v√°rios dias e calcularmos a m√©dia desses produtos, obteremos uma estimativa da autocovari√¢ncia $\gamma_{1t}$ para essa s√©rie temporal.

Quando $j = 0$, a autocovari√¢ncia $\gamma_{0t}$ se reduz √† vari√¢ncia da s√©rie no instante *t*, ou seja [^45]:

$$\gamma_{0t} = E[(Y_t - \mu_t)^2]$$

Essa observa√ß√£o ressalta a interpreta√ß√£o da autocovari√¢ncia como uma medida da variabilidade conjunta de $Y_t$ e $Y_{t-j}$.

> üí° **Exemplo Num√©rico:** Usando os dados do exemplo anterior, se quisermos calcular $\gamma_{0t}$ para o dia *t*, calcular√≠amos $(Y_t - \mu_t)^2 = (28 - 25)^2 = 3^2 = 9$. Ao calcularmos a m√©dia desses valores ao longo de v√°rios dias, obteremos uma estimativa da vari√¢ncia da s√©rie temporal no instante *t*.

**Proposi√ß√£o 1.** *Se $Y_t$ e $Y_{t-j}$ s√£o independentes, ent√£o $\gamma_{jt} = 0$.*

*Demonstra√ß√£o.* Se $Y_t$ e $Y_{t-j}$ s√£o independentes, ent√£o $E[Y_t Y_{t-j}] = E[Y_t]E[Y_{t-j}] = \mu_t \mu_{t-j}$. Portanto,
$\gamma_{jt} = E[(Y_t - \mu_t)(Y_{t-j} - \mu_{t-j})] = E[Y_t Y_{t-j} - Y_t \mu_{t-j} - \mu_t Y_{t-j} + \mu_t \mu_{t-j}] = E[Y_t Y_{t-j}] - \mu_t \mu_{t-j} - \mu_t \mu_{t-j} + \mu_t \mu_{t-j} = \mu_t \mu_{t-j} - \mu_t \mu_{t-j} = 0$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Imagine que estamos analisando uma s√©rie temporal de vendas de sorvete e outra de n√∫mero de carros vendidos. Se n√£o houver rela√ß√£o causal ou correla√ß√£o entre as duas s√©ries, ou seja, as vendas de sorvete n√£o influenciam as vendas de carros e vice-versa, ent√£o a covari√¢ncia entre elas ser√° aproximadamente zero. Neste caso, $\gamma_{jt}$ seria pr√≥ximo de zero para qualquer defasagem *j*.

**Proposi√ß√£o 1.1.** *Se $Y_t$ e $Y_{t-j}$ s√£o n√£o correlacionadas e tem m√©dia zero, ent√£o $\gamma_{jt} = 0$.*

*Demonstra√ß√£o.* Se $Y_t$ e $Y_{t-j}$ s√£o n√£o correlacionadas, ent√£o $E[Y_t Y_{t-j}] = 0$. Se a m√©dia √© zero, $\mu_t = \mu_{t-j} = 0$. Portanto, $\gamma_{jt} = E[Y_t Y_{t-j}] = 0$. $\blacksquare$

**Propriedades da Autocovari√¢ncia:**

1.  **Simetria (para processos estacion√°rios):** Para processos covariance-stationary, a autocovari√¢ncia depende apenas da defasagem *j* e n√£o do tempo *t*. Nesse caso, $\gamma_{jt} = \gamma_j = \gamma_{-j}$ [^46]. Isso significa que a autocovari√¢ncia entre $Y_t$ e $Y_{t-j}$ √© a mesma que entre $Y_t$ e $Y_{t+j}$. Essa propriedade √© demonstrada no texto [^46], substituindo *t* por *t+j* e mostrando a igualdade entre $E[(Y_t - \mu)(Y_{t-j} - \mu)]$ e $E[(Y_{t+j} - \mu)(Y_t - \mu)]$.

    **Prova:**
    Para um processo estacion√°rio, $\mu_t = \mu$ para todo $t$. Queremos mostrar que $\gamma_j = \gamma_{-j}$, ou seja, $E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(Y_t - \mu)(Y_{t+j} - \mu)]$.

    I. Seja $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$.

    II. Agora, considere $\gamma_{-j} = E[(Y_t - \mu)(Y_{t+j} - \mu)]$.

    III. Seja $k = t+j$, ent√£o $t = k-j$, e substituindo na express√£o de $\gamma_{-j}$:
    $\gamma_{-j} = E[(Y_{k-j} - \mu)(Y_{k} - \mu)]$.

    IV. Como o processo √© estacion√°rio, a autocovari√¢ncia depende apenas da defasagem, ent√£o:
    $E[(Y_{k-j} - \mu)(Y_{k} - \mu)] = E[(Y_t - \mu)(Y_{t+j} - \mu)]$ √© igual a $E[(Y_t - \mu)(Y_{t-j} - \mu)]$

    V. Portanto, $\gamma_j = \gamma_{-j}$. $\blacksquare$

    > üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal estacion√°ria de retornos de a√ß√µes com m√©dia $\mu = 0.05$. Se a autocovari√¢ncia entre os retornos no dia *t* e *t-2* √© $\gamma_2 = 0.01$, ent√£o a autocovari√¢ncia entre os retornos no dia *t* e *t+2* tamb√©m ser√° $\gamma_{-2} = 0.01$. Isso demonstra a simetria da autocovari√¢ncia em processos estacion√°rios.

2.  **Rela√ß√£o com a Autocorrela√ß√£o:** A autocorrela√ß√£o, denotada por $\rho_j$, √© a autocovari√¢ncia normalizada pela vari√¢ncia. Ela fornece uma medida adimensional da depend√™ncia temporal, variando entre -1 e 1 [^49]. A rela√ß√£o entre autocovari√¢ncia e autocorrela√ß√£o √© dada por [^49]:

    $$\rho_j = \frac{\gamma_j}{\gamma_0}$$
    
    **Prova:**
    A autocorrela√ß√£o √© definida como a autocovari√¢ncia normalizada pela vari√¢ncia.

    I. Por defini√ß√£o, $\rho_j = \frac{Cov(Y_t, Y_{t-j})}{\sqrt{Var(Y_t)Var(Y_{t-j})}}$.

    II. Para um processo estacion√°rio, $Var(Y_t) = Var(Y_{t-j}) = \gamma_0$.

    III. Al√©m disso, $Cov(Y_t, Y_{t-j}) = \gamma_j$.

    IV. Substituindo esses valores na defini√ß√£o de $\rho_j$, obtemos $\rho_j = \frac{\gamma_j}{\sqrt{\gamma_0 \gamma_0}} = \frac{\gamma_j}{\gamma_0}$. $\blacksquare$

    > üí° **Exemplo Num√©rico:** Suponha que a autocovari√¢ncia na defasagem 1, $\gamma_1$, para uma s√©rie temporal seja 0.5, e a vari√¢ncia da s√©rie, $\gamma_0$, seja 2. Ent√£o, a autocorrela√ß√£o na defasagem 1 √© $\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{0.5}{2} = 0.25$. Isso indica uma correla√ß√£o positiva moderada entre as observa√ß√µes em instantes de tempo adjacentes.

3.  **Autocovari√¢ncia de White Noise:** Para um processo de ru√≠do branco (white noise) $\{\epsilon_t\}$, que √© uma sequ√™ncia de vari√°veis aleat√≥rias n√£o correlacionadas com m√©dia zero e vari√¢ncia constante $\sigma^2$, a autocovari√¢ncia √© zero para todas as defasagens n√£o nulas [^45]:

    $$E(\epsilon_t \epsilon_{t-\tau}) = 0 \text{ para } t \neq \tau$$

    Isso significa que as observa√ß√µes em um processo de ru√≠do branco s√£o independentes umas das outras.

    **Prova:**
    Um processo de ru√≠do branco tem as seguintes propriedades: $E[\epsilon_t] = 0$ para todo $t$, $Var(\epsilon_t) = \sigma^2$ para todo $t$, e $Cov(\epsilon_t, \epsilon_s) = 0$ para $t \neq s$. Vamos mostrar que a autocovari√¢ncia √© zero para defasagens n√£o nulas.

    I. A autocovari√¢ncia √© definida como $\gamma_\tau = E[(\epsilon_t - E[\epsilon_t])(\epsilon_{t-\tau} - E[\epsilon_{t-\tau}])]$.

    II. Como $E[\epsilon_t] = 0$ para todo $t$, a express√£o se simplifica para $\gamma_\tau = E[\epsilon_t \epsilon_{t-\tau}]$.

    III. Para $\tau \neq 0$, $\epsilon_t$ e $\epsilon_{t-\tau}$ s√£o n√£o correlacionadas, ent√£o $E[\epsilon_t \epsilon_{t-\tau}] = E[\epsilon_t] E[\epsilon_{t-\tau}] = 0 \cdot 0 = 0$.

    IV. Portanto, $\gamma_\tau = 0$ para $\tau \neq 0$. $\blacksquare$

    > üí° **Exemplo Num√©rico:** Imagine que estamos gerando n√∫meros aleat√≥rios a partir de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 1. Essa sequ√™ncia de n√∫meros aleat√≥rios √© um exemplo de ru√≠do branco. Se calcularmos a autocovari√¢ncia entre esses n√∫meros em diferentes defasagens, esperar√≠amos obter valores pr√≥ximos de zero para todas as defasagens n√£o nulas.
    ```python
    import numpy as np

    # Gerar 1000 n√∫meros aleat√≥rios de uma distribui√ß√£o normal
    np.random.seed(42)  # para reproducibilidade
    white_noise = np.random.normal(0, 1, 1000)

    # Calcular a autocovari√¢ncia para defasagens de 1 a 5
    def autocovariance(x, lag):
        n = len(x)
        x_mean = np.mean(x)
        
        # Pad with the mean to maintain length (other methods could be used)
        padded_x = np.concatenate([np.full(lag, x_mean), x])
        
        # Align time series to calculate covariance
        x_t = x - x_mean
        x_t_lagged = padded_x[lag:n+lag] - x_mean
        
        return np.sum(x_t * x_t_lagged) / n

    lags = range(1, 6)
    autocovariances = [autocovariance(white_noise, lag) for lag in lags]

    print("Autocovari√¢ncias para diferentes defasagens:")
    for lag, acov in zip(lags, autocovariances):
        print(f"Defasagem {lag}: {acov:.4f}")
    ```
    Resultado:
    ```
    Autocovari√¢ncias para diferentes defasagens:
    Defasagem 1: -0.0261
    Defasagem 2: -0.0347
    Defasagem 3: -0.0058
    Defasagem 4: 0.0020
    Defasagem 5: 0.0033
    ```
    Os valores s√£o pr√≥ximos de zero, como esperado para um ru√≠do branco.

4.  **Autocovari√¢ncia em Modelos MA(1):** Em um processo MA(1) (Moving Average de primeira ordem) definido como $Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$, onde $\{\epsilon_t\}$ √© ru√≠do branco, a autocovari√¢ncia √© diferente de zero apenas para a defasagem 1 [^48]. Especificamente, $\gamma_0 = (1+\theta^2)\sigma^2$ e $\gamma_1 = \theta \sigma^2$, enquanto $\gamma_j = 0$ para $|j| > 1$.

    **Prova:**
    Dado o processo MA(1) $Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$, vamos calcular as autocovari√¢ncias $\gamma_0$, $\gamma_1$ e $\gamma_j$ para $|j| > 1$.

    I. Primeiro, observe que $E[Y_t] = \mu$, j√° que $E[\epsilon_t] = 0$.

    II. Para $\gamma_0$, temos $\gamma_0 = E[(Y_t - \mu)^2] = E[(\epsilon_t + \theta \epsilon_{t-1})^2] = E[\epsilon_t^2 + 2\theta \epsilon_t \epsilon_{t-1} + \theta^2 \epsilon_{t-1}^2] = E[\epsilon_t^2] + 2\theta E[\epsilon_t \epsilon_{t-1}] + \theta^2 E[\epsilon_{t-1}^2]$.
    Como $\epsilon_t$ √© ru√≠do branco, $E[\epsilon_t^2] = \sigma^2$ e $E[\epsilon_t \epsilon_{t-1}] = 0$. Portanto, $\gamma_0 = \sigma^2 + \theta^2 \sigma^2 = (1+\theta^2)\sigma^2$.

    III. Para $\gamma_1$, temos $\gamma_1 = E[(Y_t - \mu)(Y_{t-1} - \mu)] = E[(\epsilon_t + \theta \epsilon_{t-1})(\epsilon_{t-1} + \theta \epsilon_{t-2})] = E[\epsilon_t \epsilon_{t-1} + \theta \epsilon_{t-1}^2 + \theta \epsilon_t \epsilon_{t-2} + \theta^2 \epsilon_{t-1} \epsilon_{t-2}]$.
    Como $\epsilon_t$ √© ru√≠do branco, $E[\epsilon_t \epsilon_{t-1}] = 0$, $E[\epsilon_t \epsilon_{t-2}] = 0$, e $E[\epsilon_{t-1} \epsilon_{t-2}] = 0$. Portanto, $\gamma_1 = \theta E[\epsilon_{t-1}^2] = \theta \sigma^2$.

    IV. Para $\gamma_j$ com $|j| > 1$, temos $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\epsilon_t + \theta \epsilon_{t-1})(\epsilon_{t-j} + \theta \epsilon_{t-j-1})]$.
    Expandindo, $\gamma_j = E[\epsilon_t \epsilon_{t-j} + \theta \epsilon_{t-1} \epsilon_{t-j} + \theta \epsilon_t \epsilon_{t-j-1} + \theta^2 \epsilon_{t-1} \epsilon_{t-j-1}]$. Como $|j| > 1$, todos os termos $E[\epsilon_t \epsilon_{t-j}]$, $E[\epsilon_{t-1} \epsilon_{t-j}]$, $E[\epsilon_t \epsilon_{t-j-1}]$ e $E[\epsilon_{t-1} \epsilon_{t-j-1}]$ s√£o zero devido √† propriedade do ru√≠do branco. Portanto, $\gamma_j = 0$ para $|j| > 1$. $\blacksquare$

    > üí° **Exemplo Num√©rico:** Considere um processo MA(1) com $\mu = 0$, $\theta = 0.6$ e $\sigma^2 = 1$. Ent√£o, $\gamma_0 = (1 + 0.6^2) \times 1 = 1.36$ e $\gamma_1 = 0.6 \times 1 = 0.6$. Todas as outras autocovari√¢ncias $\gamma_j$ para $|j| > 1$ s√£o zero.  A autocorrela√ß√£o em lag 1 seria $\rho_1 = \frac{0.6}{1.36} \approx 0.441$. Isso significa que h√° uma correla√ß√£o positiva entre uma observa√ß√£o e a observa√ß√£o anterior.

    **Teorema 1.** A autocovari√¢ncia de um processo MA(q) √© zero para defasagens maiores que q.

    *Demonstra√ß√£o.* Seja $Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q}$. Ent√£o, para $j > q$:
    $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\epsilon_t + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q})(\epsilon_{t-j} + \theta_1 \epsilon_{t-j-1} + \dots + \theta_q \epsilon_{t-j-q})]$. Como $j > q$, nenhum termo $\epsilon_{t-i}$ em $(\epsilon_t + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q})$ coincide com algum termo $\epsilon_{t-j-k}$ em $(\epsilon_{t-j} + \theta_1 \epsilon_{t-j-1} + \dots + \theta_q \epsilon_{t-j-q})$, e como $E[\epsilon_t \epsilon_s] = 0$ para $t \neq s$, segue que $\gamma_j = 0$ para $j > q$. $\blacksquare$

    **Teorema 1.1.** A autocovari√¢ncia de um processo AR(1) decai exponencialmente com o aumento da defasagem.

    *Demonstra√ß√£o.* Seja $Y_t = \phi Y_{t-1} + \epsilon_t$, onde $|\phi| < 1$ para garantir a estacionariedade. Assumindo que $E[Y_t] = 0$ e $E[\epsilon_t] = 0$. Multiplicando ambos os lados por $Y_{t-j}$ e tomando a esperan√ßa, obtemos:
    $E[Y_t Y_{t-j}] = \phi E[Y_{t-1} Y_{t-j}] + E[\epsilon_t Y_{t-j}]$. Para $j > 0$, $E[\epsilon_t Y_{t-j}] = 0$ porque $\epsilon_t$ √© independente de $Y_{t-j}$. Portanto, $\gamma_j = \phi \gamma_{j-1}$.
    Aplicando recursivamente, $\gamma_j = \phi^j \gamma_0$. Como $|\phi| < 1$, $\gamma_j$ decai exponencialmente com o aumento de $j$. $\blacksquare$

    > üí° **Exemplo Num√©rico:** Considere um processo AR(1) com $\phi = 0.7$ e $\sigma^2 = 1$. Se $\gamma_0 = \frac{\sigma^2}{1 - \phi^2} = \frac{1}{1 - 0.7^2} = \frac{1}{0.51} \approx 1.96$, ent√£o $\gamma_1 = \phi \gamma_0 = 0.7 \times 1.96 = 1.372$, $\gamma_2 = \phi \gamma_1 = 0.7 \times 1.372 = 0.9604$, e assim por diante. Vemos que a autocovari√¢ncia decai exponencialmente √† medida que a defasagem aumenta.

5. **Interpreta√ß√£o da Autocovari√¢ncia:** A autocovari√¢ncia pode ser vista como o probability limit de uma m√©dia amostral do produto dos desvios em rela√ß√£o √† m√©dia [^45]:

    $$\gamma_{jt} = \text{plim } (1/I) \sum_{i=1}^I [Y_{t}^{(i)} - \mu_t] [Y_{t-j}^{(i)} - \mu_{t-j}]$$

    Onde $I$ √© o n√∫mero de realiza√ß√µes da s√©rie temporal.

**Exemplo:**

Para o processo descrito por $Y_t = \mu + \epsilon_t$ [^45], onde $\epsilon_t$ √© um processo de ru√≠do branco Gaussiano, a autocovari√¢ncia $\gamma_{jt}$ √© zero para todo $j \neq 0$ [^45]. Isso reflete a aus√™ncia de correla√ß√£o serial nesse processo. Em contraste, para um processo com tend√™ncia temporal $Y_t = \beta t + \epsilon_t$ [^45], a autocovari√¢ncia depender√° de *t*, indicando que a depend√™ncia temporal varia ao longo do tempo.

> üí° **Exemplo Num√©rico:** Para $Y_t = \mu + \epsilon_t$, suponha $\mu = 10$ e $\epsilon_t$ √© ru√≠do branco com vari√¢ncia $\sigma^2 = 2$. Ent√£o, $\gamma_0 = Var(Y_t) = Var(\epsilon_t) = 2$, e $\gamma_j = 0$ para $j \neq 0$. Para o processo $Y_t = 2t + \epsilon_t$, a autocovari√¢ncia depender√° de *t*. Por exemplo, $Cov(Y_t, Y_{t-1}) = Cov(2t + \epsilon_t, 2(t-1) + \epsilon_{t-1}) = Cov(\epsilon_t, \epsilon_{t-1}) = 0$ apenas se $\epsilon_t$ e $\epsilon_{t-1}$ forem n√£o correlacionados. Contudo, a m√©dia varia com o tempo.

### Conclus√£o

A autocovari√¢ncia √© uma ferramenta essencial para analisar a estrutura de depend√™ncia temporal em s√©ries temporais. Ela quantifica a rela√ß√£o entre observa√ß√µes em diferentes pontos no tempo e fornece *insights* valiosos para modelagem, previs√£o e an√°lise de dados. Atrav√©s da autocovari√¢ncia e da autocorrela√ß√£o, podemos identificar padr√µes de depend√™ncia, como a presen√ßa de tend√™ncias, sazonalidade ou ciclos, que s√£o cruciais para a constru√ß√£o de modelos precisos e a obten√ß√£o de previs√µes confi√°veis. O entendimento das propriedades da autocovari√¢ncia permite distinguir entre diferentes tipos de processos estoc√°sticos, desde ru√≠do branco at√© modelos ARMA mais complexos.

### Refer√™ncias
[^44]: Imagine a battery of I such computers generating sequences ...
[^45]: Given a particular realization such as ...
[^46]: Notice that if a process is covariance-stationary, the covariance between Y, and Y_{t-j} depends only on j...
<!-- END -->