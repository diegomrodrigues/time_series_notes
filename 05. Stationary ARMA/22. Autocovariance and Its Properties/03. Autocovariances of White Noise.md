## Autocovari√¢ncia em Processos com Ru√≠do Branco Gaussiano

### Introdu√ß√£o
Este cap√≠tulo visa explorar a autocovari√¢ncia de processos espec√≠ficos, em particular, aqueles onde $Y_t = \mu + \epsilon_t$, com $\epsilon_t$ sendo ru√≠do branco gaussiano. Com base nos conceitos de autocovari√¢ncia e vari√¢ncia [^"Autocovariance and Its Properties"], e da rela√ß√£o entre a 0¬™ autocovari√¢ncia e a vari√¢ncia [^"The 0th autocovariance (Œ≥_0t) is equivalent to the variance of Y_t, denoted as Var(Y_t), representing the spread or dispersion of the series around its mean at time t, and serves as a measure of the series' volatility."], investigaremos como a natureza do ru√≠do branco gaussiano influencia a estrutura de autocovari√¢ncia da s√©rie. Em particular, demonstramos que, nesses processos, as autocovari√¢ncias s√£o zero para todas as defasagens n√£o nulas, refletindo a aus√™ncia de correla√ß√£o serial.

### Conceitos Fundamentais

Considere o processo estoc√°stico definido por:

$$Y_t = \mu + \epsilon_t$$

onde $\mu$ √© uma constante e $\epsilon_t$ √© um processo de **ru√≠do branco gaussiano**. Isso significa que $\epsilon_t$ satisfaz as seguintes condi√ß√µes:

1.  $E[\epsilon_t] = 0$ para todo *t*
2.  $Var[\epsilon_t] = \sigma^2$ para todo *t*
3.  $Cov[\epsilon_t, \epsilon_s] = 0$ para todo $t \neq s$
4.  $\epsilon_t \sim N(0, \sigma^2)$, ou seja, $\epsilon_t$ segue uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\sigma^2$.

A **autocovari√¢ncia** $\gamma_{jt}$ entre $Y_t$ e $Y_{t-j}$ √© definida como [^45]:

$$\gamma_{jt} = E[(Y_t - E[Y_t])(Y_{t-j} - E[Y_{t-j}])]$$

Para o processo em quest√£o, $E[Y_t] = E[\mu + \epsilon_t] = \mu + E[\epsilon_t] = \mu + 0 = \mu$. Portanto,

$$\gamma_{jt} = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\mu + \epsilon_t - \mu)(\mu + \epsilon_{t-j} - \mu)] = E[\epsilon_t \epsilon_{t-j}]$$

Agora, analisaremos dois casos: $j = 0$ e $j \neq 0$.

*   **Caso 1:** $j = 0$

    Neste caso, $\gamma_{0t} = E[\epsilon_t^2] = Var[\epsilon_t] = \sigma^2$ [^"The 0th autocovariance (Œ≥_0t) is equivalent to the variance of Y_t, denoted as Var(Y_t), representing the spread or dispersion of the series around its mean at time t, and serves as a measure of the series' volatility."]. Isso significa que a vari√¢ncia de $Y_t$ √© igual √† vari√¢ncia do ru√≠do branco $\epsilon_t$.

    > üí° **Exemplo Num√©rico:** Suponha que $\epsilon_t$ seja ru√≠do branco gaussiano com m√©dia 0 e vari√¢ncia $\sigma^2 = 2.25$. Ent√£o, $\gamma_{0t} = E[\epsilon_t^2] = Var[\epsilon_t] = 2.25$. Isso significa que a vari√¢ncia de $Y_t$ √© 2.25. Se simulamos uma s√©rie temporal $Y_t$ por 100 per√≠odos com $\mu = 10$ e $\epsilon_t \sim N(0, 2.25)$, esperamos que a vari√¢ncia amostral de $Y_t$ seja aproximadamente 2.25.
    ```python
    import numpy as np

    mu = 10
    sigma2 = 2.25
    T = 100
    epsilon = np.random.normal(0, np.sqrt(sigma2), T) # Gera ru√≠do branco gaussiano
    Y = mu + epsilon
    sample_variance = np.var(Y)

    print(f"Vari√¢ncia te√≥rica: {sigma2}")
    print(f"Vari√¢ncia amostral: {sample_variance}")
    ```
    A sa√≠da seria algo como:
    ```
    Vari√¢ncia te√≥rica: 2.25
    Vari√¢ncia amostral: 2.39123 (valor varia a cada execu√ß√£o)
    ```

*   **Caso 2:** $j \neq 0$

    Neste caso, $\gamma_{jt} = E[\epsilon_t \epsilon_{t-j}] = Cov[\epsilon_t, \epsilon_{t-j}] = 0$, pois $\epsilon_t$ e $\epsilon_{t-j}$ s√£o n√£o correlacionadas quando $t \neq t-j$ devido √† propriedade do ru√≠do branco [^"Autocovariance and Its Properties"].

    > üí° **Exemplo Num√©rico:** Vamos supor que $j = 1$ e queremos calcular a autocovari√¢ncia entre $Y_t$ e $Y_{t-1}$. Como $\epsilon_t$ √© ru√≠do branco, $E[\epsilon_t \epsilon_{t-1}] = 0$. Portanto, a autocovari√¢ncia $\gamma_{1t} = 0$.  Para verificar isso numericamente, podemos gerar uma s√©rie de ru√≠do branco e calcular a autocovari√¢ncia amostral na defasagem 1.
    ```python
    import numpy as np

    mu = 5
    sigma2 = 1
    T = 100
    epsilon = np.random.normal(0, np.sqrt(sigma2), T)
    Y = mu + epsilon

    # Calcula a autocovari√¢ncia amostral na defasagem 1
    def autocovariance(x, lag):
        n = len(x)
        x_mean = np.mean(x)
        return np.sum((x[:n-lag] - x_mean) * (x[lag:] - x_mean)) / n

    lag = 1
    sample_autocovariance = autocovariance(Y, lag)

    print(f"Autocovari√¢ncia te√≥rica na defasagem {lag}: 0")
    print(f"Autocovari√¢ncia amostral na defasagem {lag}: {sample_autocovariance}")
    ```
    A sa√≠da seria algo como:
    ```
    Autocovari√¢ncia te√≥rica na defasagem 1: 0
    Autocovari√¢ncia amostral na defasagem 1: -0.0847 (valor varia a cada execu√ß√£o)
    ```
    Observe que o valor amostral √© pr√≥ximo de zero, conforme esperado.

**Teorema 4.** Para o processo $Y_t = \mu + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco gaussiano, a autocovari√¢ncia $\gamma_{jt}$ √© dada por:

$$\gamma_{jt} =
\begin{cases}
    \sigma^2, & \text{se } j = 0 \\
    0, & \text{se } j \neq 0
\end{cases}
$$

*Demonstra√ß√£o.*
I. Considere $j=0$. Ent√£o, $\gamma_{0t} = E[(Y_t - \mu)(Y_t - \mu)] = E[(Y_t - \mu)^2]$.

II. Como $Y_t = \mu + \epsilon_t$, temos $Y_t - \mu = \epsilon_t$.

III. Substituindo, $\gamma_{0t} = E[\epsilon_t^2] = Var[\epsilon_t]$.

IV. Dado que $Var[\epsilon_t] = \sigma^2$, ent√£o $\gamma_{0t} = \sigma^2$.

V. Considere $j \neq 0$. Ent√£o, $\gamma_{jt} = E[(Y_t - \mu)(Y_{t-j} - \mu)]$.

VI. Como $Y_t = \mu + \epsilon_t$ e $Y_{t-j} = \mu + \epsilon_{t-j}$, temos $Y_t - \mu = \epsilon_t$ e $Y_{t-j} - \mu = \epsilon_{t-j}$.

VII. Substituindo, $\gamma_{jt} = E[\epsilon_t \epsilon_{t-j}]$.

VIII. Dado que $Cov[\epsilon_t, \epsilon_{t-j}] = 0$ para $t \neq t-j$, ent√£o $E[\epsilon_t \epsilon_{t-j}] = 0$.

IX. Portanto, $\gamma_{jt} = 0$ para $j \neq 0$.

X. Combinando os casos,
$$
\gamma_{jt} =
\begin{cases}
    \sigma^2, & \text{se } j = 0 \\
    0, & \text{se } j \neq 0
\end{cases}
$$ ‚ñ†

**Corol√°rio 4.1.** A autocorrela√ß√£o $\rho_j$ para o processo $Y_t = \mu + \epsilon_t$ √© dada por:

$$\rho_j =
\begin{cases}
    1, & \text{se } j = 0 \\
    0, & \text{se } j \neq 0
\end{cases}
$$

*Demonstra√ß√£o.* Como $\rho_j = \frac{\gamma_j}{\gamma_0}$ [^"Autocovariance and Its Properties"], temos:

-   Se $j = 0$, $\rho_0 = \frac{\gamma_0}{\gamma_0} = \frac{\sigma^2}{\sigma^2} = 1$
-   Se $j \neq 0$, $\rho_j = \frac{\gamma_j}{\gamma_0} = \frac{0}{\sigma^2} = 0$

Portanto, a autocorrela√ß√£o √© 1 para a defasagem 0 e 0 para todas as outras defasagens. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que $\sigma^2 = 9$.  Ent√£o, $\rho_0 = \frac{9}{9} = 1$ e $\rho_j = \frac{0}{9} = 0$ para $j \neq 0$.  Isso demonstra que a autocorrela√ß√£o √© 1 para a defasagem 0 e 0 para todas as outras defasagens, como previsto. Para visualizar isso, podemos gerar um gr√°fico da fun√ß√£o de autocorrela√ß√£o (FAC) amostral.
```python
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm

mu = 2
sigma2 = 9
T = 100
epsilon = np.random.normal(0, np.sqrt(sigma2), T)
Y = mu + epsilon

# Calcula a fun√ß√£o de autocorrela√ß√£o amostral (FAC)
acf = sm.tsa.acf(Y, nlags=10)

# Plota a FAC
plt.stem(range(len(acf)), acf, use_line_collection=True)
plt.title("Fun√ß√£o de Autocorrela√ß√£o Amostral (FAC)")
plt.xlabel("Defasagem (Lag)")
plt.ylabel("Autocorrela√ß√£o")
plt.show()
```
O gr√°fico da FAC mostrar√° um pico em 1 na defasagem 0 e valores pr√≥ximos de zero para as outras defasagens. Os valores n√£o ser√£o exatamente zero devido √† natureza amostral da estimativa.

Para complementar a an√°lise da autocorrela√ß√£o, podemos derivar a fun√ß√£o de autocorrela√ß√£o amostral (FAC) para esse processo.

**Teorema 4.2.** A fun√ß√£o de autocorrela√ß√£o amostral (FAC) estimada a partir de uma amostra finita de tamanho *T* do processo $Y_t = \mu + \epsilon_t$, denotada por $\hat{\rho}_j$, converge em probabilidade para a autocorrela√ß√£o te√≥rica $\rho_j$ quando $T \rightarrow \infty$.

*Demonstra√ß√£o.* A FAC amostral √© definida como $\hat{\rho}_j = \frac{\hat{\gamma}_j}{\hat{\gamma}_0}$, onde $\hat{\gamma}_j$ √© a estimativa amostral da autocovari√¢ncia na defasagem *j*. Dado que $\epsilon_t$ √© ru√≠do branco gaussiano, as estimativas amostrais convergem para os seus valores te√≥ricos √† medida que o tamanho da amostra aumenta. Formalmente, $\hat{\gamma}_0 \xrightarrow{p} \gamma_0 = \sigma^2$ e $\hat{\gamma}_j \xrightarrow{p} \gamma_j = 0$ para $j \neq 0$, onde $\xrightarrow{p}$ denota converg√™ncia em probabilidade. Portanto, $\hat{\rho}_j \xrightarrow{p} \rho_j$. $\blacksquare$

**Lema 5.** Se $Y_t = \mu + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco gaussiano com vari√¢ncia $\sigma^2$, ent√£o o processo $Y_t$ √© estacion√°rio no sentido amplo (ou estacion√°rio de segunda ordem).

*Demonstra√ß√£o.* Para mostrar que $Y_t$ √© estacion√°rio no sentido amplo, precisamos demonstrar que a m√©dia $E[Y_t]$ √© constante e que a autocovari√¢ncia $\gamma_{jt}$ depende apenas da defasagem *j* e n√£o de *t*. J√° mostramos que $E[Y_t] = \mu$, que √© uma constante. Al√©m disso, $\gamma_{jt} = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[\epsilon_t \epsilon_{t-j}]$, que depende apenas de *j* (sendo $\sigma^2$ para *j*=0 e 0 caso contr√°rio), e n√£o de *t*. Portanto, $Y_t$ √© estacion√°rio no sentido amplo. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha $\mu = 7$ e $\sigma^2 = 1.5$.  Para verificar a estacionariedade, podemos gerar uma s√©rie temporal e calcular a m√©dia e a autocovari√¢ncia em diferentes pontos no tempo. Se a m√©dia permanecer aproximadamente constante e a autocovari√¢ncia depender apenas da defasagem, podemos concluir que o processo √© estacion√°rio no sentido amplo.
```python
import numpy as np

mu = 7
sigma2 = 1.5
T = 200
epsilon = np.random.normal(0, np.sqrt(sigma2), T)
Y = mu + epsilon

# Calcula a m√©dia em diferentes janelas de tempo
window_size = 50
means = [np.mean(Y[i:i+window_size]) for i in range(0, T-window_size, window_size//2)]

# Calcula a autocovari√¢ncia para a defasagem 1
def autocovariance(x, lag):
    n = len(x)
    x_mean = np.mean(x)
    return np.sum((x[:n-lag] - x_mean) * (x[lag:] - x_mean)) / n

autocovariances = [autocovariance(Y[i:i+window_size], 1) for i in range(0, T-window_size, window_size//2)]

print(f"M√©dias em diferentes janelas de tempo: {means}")
print(f"Autocovari√¢ncias na defasagem 1 em diferentes janelas de tempo: {autocovariances}")
```
A sa√≠da mostrar√° que as m√©dias s√£o aproximadamente constantes e as autocovari√¢ncias na defasagem 1 s√£o pr√≥ximas de zero em diferentes janelas de tempo, indicando a estacionariedade do processo.

**Lema 5.1.** Se $Y_t = \mu + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco gaussiano com vari√¢ncia $\sigma^2$, ent√£o o processo $Y_t$ √© erg√≥dico para a m√©dia.

*Demonstra√ß√£o.*
I. Precisamos mostrar que a m√©dia amostral $\bar{Y} = \frac{1}{T} \sum_{t=1}^{T} Y_t$ converge em probabilidade para a m√©dia populacional $E[Y_t] = \mu$.

II. Como $Y_t = \mu + \epsilon_t$, temos $\bar{Y} = \frac{1}{T} \sum_{t=1}^{T} (\mu + \epsilon_t) = \mu + \frac{1}{T} \sum_{t=1}^{T} \epsilon_t$.

III. Pela Lei dos Grandes N√∫meros, se $E[\epsilon_t] = 0$ e $Var[\epsilon_t] = \sigma^2 < \infty$, ent√£o $\frac{1}{T} \sum_{t=1}^{T} \epsilon_t \xrightarrow{p} E[\epsilon_t] = 0$.

IV. Portanto, $\bar{Y} = \mu + \frac{1}{T} \sum_{t=1}^{T} \epsilon_t \xrightarrow{p} \mu + 0 = \mu$.

V. Assim, a m√©dia amostral $\bar{Y}$ converge em probabilidade para a m√©dia populacional $\mu$. $\blacksquare$

Adicionalmente, podemos analisar o comportamento do estimador de m√°xima verossimilhan√ßa para a vari√¢ncia $\sigma^2$ do ru√≠do branco.

**Teorema 6.** O estimador de m√°xima verossimilhan√ßa (EMV) para a vari√¢ncia $\sigma^2$ do ru√≠do branco gaussiano $\epsilon_t$ no processo $Y_t = \mu + \epsilon_t$, assumindo $\mu$ conhecido, √© dado por:

$$\hat{\sigma}^2 = \frac{1}{T} \sum_{t=1}^{T} (Y_t - \mu)^2$$

Al√©m disso, $\hat{\sigma}^2$ √© um estimador consistente e assintoticamente n√£o viesado para $\sigma^2$.

*Demonstra√ß√£o.*
I. A fun√ß√£o de verossimilhan√ßa para uma amostra de tamanho *T* √© dada por:

$$L(\sigma^2) = \prod_{t=1}^{T} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(Y_t - \mu)^2}{2\sigma^2}\right)$$

II. O log-verossimilhan√ßa √©:

$$\ell(\sigma^2) = \sum_{t=1}^{T} \log\left(\frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(Y_t - \mu)^2}{2\sigma^2}\right)\right)$$
$$\ell(\sigma^2) = \sum_{t=1}^{T} \left[-\frac{1}{2}\log(2\pi) - \frac{1}{2}\log(\sigma^2) - \frac{(Y_t - \mu)^2}{2\sigma^2}\right]$$
$$\ell(\sigma^2) = -\frac{T}{2} \log(2\pi) - \frac{T}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=1}^{T} (Y_t - \mu)^2$$

III. Para encontrar o EMV, derivamos o log-verossimilhan√ßa em rela√ß√£o a $\sigma^2$ e igualamos a zero:

$$\frac{\partial \ell(\sigma^2)}{\partial \sigma^2} = -\frac{T}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_{t=1}^{T} (Y_t - \mu)^2 = 0$$

IV. Resolvendo para $\sigma^2$, obtemos:

$$\frac{1}{2(\sigma^2)^2} \sum_{t=1}^{T} (Y_t - \mu)^2 = \frac{T}{2\sigma^2}$$
$$\sum_{t=1}^{T} (Y_t - \mu)^2 = T\sigma^2$$
$$\hat{\sigma}^2 = \frac{1}{T} \sum_{t=1}^{T} (Y_t - \mu)^2$$

V. Para mostrar a consist√™ncia, notamos que $\hat{\sigma}^2$ √© a m√©dia amostral de $(Y_t - \mu)^2 = \epsilon_t^2$. Pela Lei dos Grandes N√∫meros, $\hat{\sigma}^2 \xrightarrow{p} E[\epsilon_t^2] = \sigma^2$. Portanto, $\hat{\sigma}^2$ √© consistente.

VI. Para mostrar que √© assintoticamente n√£o viesado, calculamos o valor esperado de $\hat{\sigma}^2$:

$$E[\hat{\sigma}^2] = E\left[\frac{1}{T} \sum_{t=1}^{T} (Y_t - \mu)^2\right] = \frac{1}{T} \sum_{t=1}^{T} E[(Y_t - \mu)^2]$$
Como $Y_t = \mu + \epsilon_t$, ent√£o $Y_t - \mu = \epsilon_t$.
$$E[\hat{\sigma}^2] = \frac{1}{T} \sum_{t=1}^{T} E[\epsilon_t^2] = \frac{1}{T} \sum_{t=1}^{T} \sigma^2 = \frac{1}{T} (T\sigma^2) = \sigma^2$$

VII. Portanto, $\hat{\sigma}^2$ √© assintoticamente n√£o viesado. $\blacksquare$

> üí° **Exemplo Num√©rico:**  Suponha que tenhamos $T=50$ observa√ß√µes com $\mu = 3$ e $\sigma^2 = 4$.  Geramos uma amostra de $Y_t$ e calculamos o EMV para $\sigma^2$.
```python
import numpy as np

mu = 3
sigma2 = 4
T = 50
epsilon = np.random.normal(0, np.sqrt(sigma2), T)
Y = mu + epsilon

# Calcula o estimador de m√°xima verossimilhan√ßa para sigma2
sigma2_hat = np.mean((Y - mu)**2)

print(f"Vari√¢ncia verdadeira: {sigma2}")
print(f"Estimativa de m√°xima verossimilhan√ßa da vari√¢ncia: {sigma2_hat}")
```
A sa√≠da mostrar√° que a estimativa de m√°xima verossimilhan√ßa √© pr√≥xima do valor verdadeiro da vari√¢ncia. Por exemplo:
```
Vari√¢ncia verdadeira: 4
Estimativa de m√°xima verossimilhan√ßa da vari√¢ncia: 3.8214 (valor varia a cada execu√ß√£o)
```

### Conclus√£o
Para um processo onde $Y_t = \mu + \epsilon_t$, com $\epsilon_t$ sendo ru√≠do branco gaussiano, a autocovari√¢ncia $\gamma_{jt}$ √© igual a $\sigma^2$ quando *j* √© zero (i.e., a vari√¢ncia da s√©rie) e zero para todas as outras defasagens n√£o nulas. Esse resultado reflete a aus√™ncia de correla√ß√£o serial inerente ao ru√≠do branco, indicando que as observa√ß√µes s√£o independentes umas das outras. Este caso particular fornece um ponto de refer√™ncia importante para a an√°lise de autocovari√¢ncia em modelos mais complexos, servindo como base para entender o efeito de depend√™ncias temporais adicionais na estrutura da s√©rie temporal [^"Autocovariance and Its Properties"].

### Refer√™ncias
[^45]: Given a particular realization such as ...
[^"Autocovariance and Its Properties"]: SECTION_PLACEHOLDER (from previous chapter)
[^"The 0th autocovariance (Œ≥_0t) is equivalent to the variance of Y_t, denoted as Var(Y_t), representing the spread or dispersion of the series around its mean at time t, and serves as a measure of the series' volatility."]: SECTION_PLACEHOLDER (from previous chapter)
<!-- END -->