## Autocorrelação e Decaimento Geométrico em Processos AR(1)

### Introdução
Este capítulo explora a autocorrelação em processos autorregressivos de primeira ordem (AR(1)), um conceito fundamental para a análise de séries temporais. Construindo sobre as definições de autocovariância e estacionaridade introduzidas anteriormente [^1, ^2], aqui vamos detalhar como as autocorrelações de um processo AR(1) exibem um decaimento geométrico e como este comportamento está intimamente ligado ao parâmetro autoregressivo do modelo. A compreensão deste decaimento geométrico é fundamental para caracterizar a dependência temporal nos dados e prever o comportamento futuro de uma série temporal modelada por um AR(1).

### Conceitos Fundamentais
Um processo AR(1) é definido pela seguinte equação de diferença [^3]:
$$Y_t = c + \phi Y_{t-1} + \epsilon_t$$
onde $\{\epsilon_t\}$ é um processo de ruído branco com média zero e variância $\sigma^2$. Aqui, $c$ representa uma constante e $\phi$ é o coeficiente autoregressivo.  Como vimos anteriormente, a autocovariância $\gamma_j$ de um processo estacionário é definida como:
$$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$$ [^1]
Para o processo AR(1), vimos que a autocovariância pode ser expressa como:
$$\gamma_j = \frac{\phi^j}{1-\phi^2} \sigma^2$$ [^4]
E a autocorrelação $\rho_j$ é obtida dividindo a autocovariância pela variância $\gamma_0$:
$$\rho_j = \frac{\gamma_j}{\gamma_0} = \phi^j$$ [^5]
Esta equação é central para a nossa discussão.  Ela indica que a *autocorrelação de um processo AR(1) decai geometricamente com o aumento da defasagem j*. Especificamente, cada autocorrelação $\rho_j$ é igual ao coeficiente autoregressivo $\phi$ elevado à potência da defasagem $j$. Este decaimento geométrico reflete a diminuição da dependência entre observações separadas por defasagens cada vez maiores.
Em um processo AR(1), valores positivos de $\phi$ induzem autocorrelações positivas, o que implica que *valores atípicos em uma direção tendem a ser seguidos por valores na mesma direção*. Por outro lado, valores negativos de $\phi$ implicam autocorrelações alternadas em sinal, o que significa que *um valor maior do que a média pode ser seguido por um valor menor que a média, e vice-versa*. Como exemplo, um valor grande de $Y_t$ estará associado a um valor também grande em $Y_{t+1}$ quando $\phi$ for positivo e a um valor pequeno quando $\phi$ for negativo [^5].  A magnitude de $\phi$ também desempenha um papel importante; quando $\phi$ está próximo de 1 em valor absoluto, o processo exibirá dependência temporal mais forte com uma lenta taxa de decaimento. Por outro lado, quando $\phi$ está próximo de 0, o processo exibirá uma dependência temporal mais fraca e as autocorrelações decairão mais rapidamente para zero.

Além do decaimento geométrico, a estrutura de autocorrelação de um processo AR(1) também pode ser derivada utilizando uma abordagem recursiva. Como visto na equação de diferença do processo AR(1),  podemos multiplicar ambos os lados de
$$Y_t = c + \phi Y_{t-1} + \epsilon_t$$
por $(Y_{t-j} - \mu)$ e tomar esperança [^4]:
$$E[(Y_t - \mu)(Y_{t-j} - \mu)] = \phi E[(Y_{t-1} - \mu)(Y_{t-j} - \mu)] + E[\epsilon_t(Y_{t-j} - \mu)]$$
Quando $j>0$, o termo $E[\epsilon_t(Y_{t-j} - \mu)]$ será 0, porque $\epsilon_t$ é um ruído branco não correlacionado com valores passados de Y. Portanto, temos:
$$\gamma_j = \phi \gamma_{j-1}$$
Esta é uma equação de diferença que relaciona $\gamma_j$ com $\gamma_{j-1}$.  Resolvendo recursivamente esta equação, dado que $\gamma_0$ é a variância do processo, $\gamma_0=\frac{\sigma^2}{1-\phi^2}$ [^4], obtemos o mesmo resultado de antes para o decaimento geométrico:
$$\gamma_j = \phi^j \gamma_0$$
e
$$\rho_j = \phi^j$$
Esta abordagem recursiva demonstra como a estrutura de autocorrelação surge diretamente da natureza autoregressiva do processo. É um método alternativo para derivar a estrutura da autocorrelação que enfatiza a dependência entre valores consecutivos na série temporal.

### Conclusão
Em resumo, a autocorrelação de um processo AR(1) decai geometricamente com a defasagem $j$, com as autocorrelações dadas por $\rho_j = \phi^j$. Este resultado crucial reflete como cada observação em um processo AR(1) é influenciada por observações anteriores e como esta influência diminui à medida que a defasagem aumenta. A natureza do decaimento, seja ela rápida ou lenta, e a sua direção, seja positiva ou negativa, são ditadas pelo valor do parâmetro autoregressivo $\phi$. Além disso, a derivação da estrutura de autocorrelação utilizando a abordagem recursiva reforça que o comportamento de autocorrelação é uma consequência direta da equação de diferença AR(1).  A compreensão desses conceitos é fundamental para o modelagem e análise de dados de séries temporais.

### Referências
[^1]:  E(Y) = µ;  E(Yt − µ)(Yt−j − µ) = γj for all t and any j [^2]
[^2]: The variance of the random variable Y, (denoted you) is similarly defined as
γ0t = E(Yt − µt)²  =  ∫−∞∞ (yt − µt)² fYt(yt) dyt [^1]
[^3]: A first-order autoregression, denoted AR(1), satisfies the following difference equation:  Yt = c + φYt−1 + εt [^10]
[^4]:  γj = E[(Yt − µ)(Yt−j − µ)] = φj γ0 [^10]
[^5]:  pj = γj/γ0 = φj [^11]
### Introdução
Em continuidade ao estudo de processos estocásticos, este capítulo aborda a previsão de séries temporais, focando na teoria de projeções lineares e suas aplicações em modelos ARMA. Como vimos anteriormente, modelos como o AR(1) [^3] possuem características que permitem a análise de autocorrelações e autocovariâncias [^4] [^5], sendo que o entendimento desses conceitos é crucial para o desenvolvimento de previsões acuradas. As seções que seguem exploram métodos de previsão para diferentes cenários, incluindo quando um número finito de observações está disponível e a utilização de abordagens empíricas para modelagem.

### Conceitos Fundamentais
#### Expectativa Condicional e Projeção Linear
O ponto de partida para a previsão é a expectativa condicional. Dados um conjunto de variáveis $X_t$, observadas no instante $t$, e uma variável de interesse $Y_{t+1}$, o objetivo é encontrar uma previsão $Y_{t+1}^*$ que minimize o erro quadrático médio, como expresso em [4.1.1] [^29]. Em outras palavras, buscamos o valor de $Y_{t+1}^*$ que melhor se aproxima de $Y_{t+1}$ em termos de minimizar a variância do erro de previsão.
A projeção linear é um caso especial onde $Y_{t+1}^*$ é uma função linear de $X_t$. Em muitos casos, essa abordagem oferece uma boa aproximação, especialmente em processos Gaussianos onde a projeção linear é a melhor previsão possível, não importa o quão não linear seja a função que se tenta aproximar.

#### Previsão com Modelos ARMA
Quando se trata de modelos ARMA, é possível derivar previsões ótimas, dado um número infinito de observações passadas. Em cenários práticos, entretanto, um número finito de observações está disponível e isso requer o uso de abordagens aproximadas. A teoria de projeções lineares é uma ferramenta fundamental nesses casos, pois permite que se construa previsões utilizando apenas as informações disponíveis.
Para processos ARMA, a previsão ideal pode ser obtida através da expectativa condicional de $Y_{t+h}$ dado o passado $Y_t$, $Y_{t-1}$, e assim por diante, onde $h$ é o horizonte de previsão. Essa expectativa condicional pode ser expressa em termos das autocovariâncias do processo, como vimos no capítulo anterior.

#### Fatorização Triangular e de Cholesky
A fatorização triangular e a fatorização de Cholesky são técnicas para decompor a matriz de variância-covariância. Essas técnicas são importantes para calcular previsões ótimas baseadas em um número finito de observações e são cruciais para entender algumas ferramentas como a análise de vetores autoregressivos e o filtro de Kalman [^29].

#### Atualização de Previsões
A fatorização triangular também é usada para derivar fórmulas de atualização das previsões. Isso permite que as previsões sejam recalculadas e ajustadas conforme novas observações se tornam disponíveis. A fórmula de atualização da projeção linear demonstra como a informação passada influencia a previsão futura.

#### Processos Gaussianos
Para processos gaussianos, a projeção linear é a melhor previsão possível, mesmo quando modelos não lineares são considerados. Isso significa que se o processo subjacente é gaussiano, não é possível encontrar uma previsão que seja melhor do que a projeção linear, independentemente da complexidade do modelo utilizado.

#### Decomposição de Wold
A decomposição de Wold estabelece que qualquer processo estacionário pode ser representado como uma soma de um componente determinístico e um componente de média móvel. Essa decomposição justifica o uso de representações de média móvel para caracterizar a regra de previsão linear de qualquer processo estacionário. Além disso, essa abordagem permite a utilização de modelos MA(∞), cujas propriedades são bem definidas, como vimos no capítulo anterior.

### Conclusão
Este capítulo apresentou os fundamentos teóricos para previsão de séries temporais, utilizando a projeção linear e modelos ARMA como elementos chave. As técnicas de fatorização e as propriedades dos processos gaussianos foram exploradas, estabelecendo a base para a aplicação prática desses conceitos em cenários do mundo real, onde um número finito de observações é comum. Além disso, a decomposição de Wold oferece uma visão geral sobre a representação de processos estocásticos, culminando na análise de como obter a melhor previsão.

### Referências
[^29]: 4.1. Principles of Forecasting
<!-- END -->
