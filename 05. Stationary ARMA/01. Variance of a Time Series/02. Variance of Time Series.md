## AutocovariÃ¢ncia de Processos EstacionÃ¡rios

### IntroduÃ§Ã£o
Em continuidade Ã  discussÃ£o sobre a **variÃ¢ncia** de sÃ©ries temporais [^3], este capÃ­tulo aprofunda-se no conceito de **autocovariÃ¢ncia**. Como vimos anteriormente, a variÃ¢ncia ($\gamma_{tt}$) mede a dispersÃ£o dos valores de uma sÃ©rie temporal em torno da sua mÃ©dia [^1]. A autocovariÃ¢ncia, por sua vez, estende essa ideia medindo a dependÃªncia linear entre valores da sÃ©rie em diferentes pontos no tempo. Especificamente, analisaremos como a autocovariÃ¢ncia se comporta em processos estacionÃ¡rios, com Ãªnfase no caso em que a sÃ©rie temporal Ã© representada como a soma de uma **tendÃªncia de tempo** e **ruÃ­do branco Gaussiano**, $Y_t = \beta t + \epsilon_t$ [^1].

### Conceitos Fundamentais

A **autocovariÃ¢ncia** de uma sÃ©rie temporal $Y_t$ no lag $j$, denotada por $\gamma_{jt}$, quantifica a covariÃ¢ncia entre $Y_t$ e $Y_{t-j}$ [^2]. Formalmente, Ã© definida como [^2]:

$$
\gamma_{jt} = E[(Y_t - \mu_t)(Y_{t-j} - \mu_{t-j})]
$$

onde $\mu_t$ Ã© a mÃ©dia de $Y_t$ no tempo $t$ [^1].

Para um processo **estacionÃ¡rio**, a mÃ©dia e a autocovariÃ¢ncia nÃ£o dependem do tempo, ou seja, $\mu_t = \mu$ e $\gamma_{jt} = \gamma_j$ para todos os tempos $t$ e lags $j$ [^2]. Isso implica que a dependÃªncia linear entre $Y_t$ e $Y_{t-j}$ Ã© constante ao longo do tempo.

> ðŸ’¡ **ObservaÃ§Ã£o Importante:** Em processos estacionÃ¡rios, a autocovariÃ¢ncia no lag zero, $\gamma_{0}$, Ã© igual Ã  variÃ¢ncia $\gamma_{tt}$ [^2].

Para o processo especÃ­fico em consideraÃ§Ã£o, $Y_t = \beta t + \epsilon_t$, onde $\epsilon_t$ Ã© um **ruÃ­do branco Gaussiano** com mÃ©dia zero e variÃ¢ncia $\sigma^2$, podemos calcular a autocovariÃ¢ncia da seguinte forma:

*Prova:*
I. Dado $Y_t = \beta t + \epsilon_t$, onde $E[\epsilon_t] = 0$ e $Var(\epsilon_t) = \sigma^2$.

II. A mÃ©dia de $Y_t$ Ã© $E[Y_t] = E[\beta t + \epsilon_t] = \beta t + E[\epsilon_t] = \beta t + 0 = \beta t$.

III. A autocovariÃ¢ncia no lag $j$ Ã©:
$$
\gamma_{jt} = E[(Y_t - E[Y_t])(Y_{t-j} - E[Y_{t-j}])] = E[(\beta t + \epsilon_t - \beta t)(\beta (t-j) + \epsilon_{t-j} - \beta (t-j))]
$$
$$
\gamma_{jt} = E[\epsilon_t \epsilon_{t-j}]
$$

IV. Como $\epsilon_t$ Ã© um ruÃ­do branco Gaussiano, $E[\epsilon_t \epsilon_{t-j}] = 0$ para $j \neq 0$ e $E[\epsilon_t^2] = \sigma^2$ para $j = 0$. Portanto,
$$
E[\epsilon_t \epsilon_{t-j}] =
\begin{cases}
\sigma^2 & \text{se } j = 0 \\
0 & \text{se } j \neq 0
\end{cases}
$$

V. Portanto,
$$
\gamma_{jt} =
\begin{cases}
\sigma^2 & \text{se } j = 0 \\
0 & \text{se } j \neq 0
\end{cases}
$$
$\blacksquare$

Este resultado mostra que, para o processo $Y_t = \beta t + \epsilon_t$, a autocovariÃ¢ncia Ã© zero para todos os lags diferentes de zero. Isso ocorre porque o termo de tendÃªncia de tempo $\beta t$ Ã© determinÃ­stico, e toda a variabilidade e dependÃªncia temporal sÃ£o capturadas pelo termo de ruÃ­do branco $\epsilon_t$. Especificamente, como $\epsilon_t$ Ã© um ruÃ­do branco, seus valores em diferentes pontos no tempo sÃ£o nÃ£o correlacionados.

> ðŸ’¡ **Exemplo NumÃ©rico:** Seja $\beta = 2$ e $\epsilon_t$ um ruÃ­do branco Gaussiano com variÃ¢ncia $\sigma^2 = 4$. EntÃ£o, $Y_t = 2t + \epsilon_t$. A autocovariÃ¢ncia de $Y_t$ Ã© $\gamma_{jt} = 4$ se $j = 0$ e $\gamma_{jt} = 0$ se $j \neq 0$.  Isto significa que valores de $Y_t$ *nÃ£o* sÃ£o linearmente dependentes de valores de $Y_{t-j}$ quando $j \ne 0$
> ```python
> import numpy as np
>
> beta = 2
> sigma_squared = 4
>
> # Gerando dados de exemplo
> np.random.seed(0)  # Para reprodutibilidade
> T = 100  # NÃºmero de pontos no tempo
> epsilon = np.random.normal(0, np.sqrt(sigma_squared), T)  # RuÃ­do branco Gaussiano
> t = np.arange(1, T + 1)
> Y = beta * t + epsilon
>
> # Calculando a autocovariÃ¢ncia amostral
> def autocovariance(series, lag):
>     n = len(series)
>     series_mean = np.mean(series)
>     if lag == 0:
>         return np.sum((series - series_mean) ** 2) / n
>     else:
>         return np.sum((series[lag:] - series_mean) * (series[:-lag] - series_mean)) / n
>
> # AutocovariÃ¢ncia no lag 0
> gamma_0 = autocovariance(Y, 0)
> print(f"AutocovariÃ¢ncia no lag 0: {gamma_0:.2f}")
>
> # AutocovariÃ¢ncia no lag 1
> gamma_1 = autocovariance(Y, 1)
> print(f"AutocovariÃ¢ncia no lag 1: {gamma_1:.2f}")
>
> # AutocovariÃ¢ncia no lag 5
> gamma_5 = autocovariance(Y, 5)
> print(f"AutocovariÃ¢ncia no lag 5: {gamma_5:.2f}")
> ```
>
> **InterpretaÃ§Ã£o:** Os valores calculados demonstram que a autocovariÃ¢ncia no lag 0 Ã© aproximadamente igual Ã  variÃ¢ncia do ruÃ­do branco (4), enquanto que nos lags maiores (1 e 5) a autocovariÃ¢ncia se aproxima de zero, confirmando a independÃªncia dos resÃ­duos em diferentes pontos no tempo. Essa propriedade Ã© fundamental para verificar a adequaÃ§Ã£o do modelo.

Ã‰ crucial notar que, embora a autocovariÃ¢ncia nÃ£o dependa do tempo (ou seja, $\gamma_{jt} = \gamma_j$) para este processo especÃ­fico, o processo *nÃ£o* Ã© estacionÃ¡rio porque a sua mÃ©dia, $E[Y_t] = \beta t$, depende do tempo. A condiÃ§Ã£o de autocovariÃ¢ncia independente do tempo Ã© apenas uma das condiÃ§Ãµes para estacionariedade fraca [^2].

Para complementar essa discussÃ£o, podemos analisar a autocovariÃ¢ncia de um processo relacionado, onde a tendÃªncia Ã© removida.

**Teorema 1**
Seja $Z_t = Y_t - \beta t = \epsilon_t$. A autocovariÃ¢ncia de $Z_t$ no lag $j$ Ã© dada por:
$$
\gamma_{j} =
\begin{cases}
\sigma^2 & \text{se } j = 0 \\
0 & \text{se } j \neq 0
\end{cases}
$$
*Prova:*
I. Dado $Z_t = Y_t - \beta t = \epsilon_t$.
II. A mÃ©dia de $Z_t$ Ã© $E[Z_t] = E[\epsilon_t] = 0$.
III. A autocovariÃ¢ncia no lag $j$ Ã©:
$$
\gamma_{j} = E[(Z_t - E[Z_t])(Z_{t-j} - E[Z_{t-j}])] = E[(Z_t - 0)(Z_{t-j} - 0)] = E[\epsilon_t \epsilon_{t-j}]
$$
IV. Como $\epsilon_t$ Ã© um ruÃ­do branco Gaussiano, $E[\epsilon_t \epsilon_{t-j}] = 0$ para $j \neq 0$ e $E[\epsilon_t \epsilon_{t-j}] = \sigma^2$ para $j = 0$.
$\blacksquare$

**Teorema 1.1**
O processo $Z_t$ definido acima Ã© fracamente estacionÃ¡rio.

*Prova:*
I. $E[Z_t] = E[\epsilon_t] = 0$, que Ã© constante.
II. $\gamma_j = E[(Z_t - 0)(Z_{t-j} - 0)] = E[\epsilon_t \epsilon_{t-j}]$, que nÃ£o depende de t.
Portanto, $Z_t$ Ã© fracamente estacionÃ¡rio.
$\blacksquare$

A autocovariÃ¢ncia estÃ¡ intimamente relacionada Ã  **autocorrelaÃ§Ã£o**. A autocorrelaÃ§Ã£o no lag $j$, denotada por $\rho_j$, Ã© a autocovariÃ¢ncia no lag $j$ dividida pela variÃ¢ncia:

$$
\rho_j = \frac{\gamma_j}{\gamma_0}
$$

Para o processo $Y_t = \beta t + \epsilon_t$, a autocorrelaÃ§Ã£o Ã©:

$$
\rho_j =
\begin{cases}
1 & \text{se } j = 0 \\
0 & \text{se } j \neq 0
\end{cases}
$$

Isto significa que o coeficiente de autocorrelaÃ§Ã£o para o processo descrito Ã© 1 para lag 0 e 0 para qualquer outro lag.

**Teorema 2**
Seja $W_t = \alpha + \epsilon_t$, onde $\alpha$ Ã© uma constante e $\epsilon_t$ Ã© um ruÃ­do branco Gaussiano com mÃ©dia zero e variÃ¢ncia $\sigma^2$. EntÃ£o a autocorrelaÃ§Ã£o de $W_t$ Ã©:
$$
\rho_j =
\begin{cases}
1 & \text{se } j = 0 \\
0 & \text{se } j \neq 0
\end{cases}
$$

*Prova:*
I. $E[W_t] = E[\alpha + \epsilon_t] = \alpha + E[\epsilon_t] = \alpha + 0 = \alpha$.
II. $\gamma_0 = E[(W_t - E[W_t])^2] = E[(W_t - \alpha)^2] = E[(\alpha + \epsilon_t - \alpha)^2] = E[\epsilon_t^2] = \sigma^2$.
III. $\gamma_j = E[(W_t - E[W_t])(W_{t-j} - E[W_{t-j}])] = E[(W_t - \alpha)(W_{t-j} - \alpha)] = E[(\alpha + \epsilon_t - \alpha)(\alpha + \epsilon_{t-j} - \alpha)] = E[\epsilon_t \epsilon_{t-j}]$. Para $j \neq 0$, $\gamma_j = 0$.
IV. Portanto, $\rho_j = \frac{\gamma_j}{\gamma_0}$. Se $j=0$, $\rho_0 = \frac{\sigma^2}{\sigma^2} = 1$. Se $j \neq 0$, $\rho_j = \frac{0}{\sigma^2} = 0$.
$\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos considerar $W_t = 5 + \epsilon_t$, onde $\epsilon_t$ Ã© ruÃ­do branco com $\sigma^2 = 9$.  Simularemos alguns dados e calcularemos a autocorrelaÃ§Ã£o amostral.
> ```python
> import numpy as np
>
> alpha = 5
> sigma_squared = 9
> np.random.seed(1)
> T = 100
> epsilon = np.random.normal(0, np.sqrt(sigma_squared), T)
> W = alpha + epsilon
>
> def autocorrelation(series, lag):
>     n = len(series)
>     series_mean = np.mean(series)
>     gamma_0 = np.sum((series - series_mean) ** 2) / n
>     if lag == 0:
>         return 1.0
>     gamma_j = np.sum((series[lag:] - series_mean) * (series[:-lag] - series_mean)) / n
>     return gamma_j / gamma_0
>
> rho_0 = autocorrelation(W, 0)
> print(f"AutocorrelaÃ§Ã£o no lag 0: {rho_0:.2f}")
>
> rho_1 = autocorrelation(W, 1)
> print(f"AutocorrelaÃ§Ã£o no lag 1: {rho_1:.2f}")
>
> rho_5 = autocorrelation(W, 5)
> print(f"AutocorrelaÃ§Ã£o no lag 5: {rho_5:.2f}")
> ```
> **InterpretaÃ§Ã£o:** A autocorrelaÃ§Ã£o no lag 0 Ã© sempre 1. A autocorrelaÃ§Ã£o nos lags 1 e 5 sÃ£o prÃ³ximos de zero, indicando que os valores de $W_t$ nÃ£o sÃ£o correlacionados com seus valores passados, o que Ã© esperado para um processo de ruÃ­do branco.

A autocovariÃ¢ncia tambÃ©m pode ser vista como o limite de probabilidade da mÃ©dia do produto dos desvios em relaÃ§Ã£o Ã  mÃ©dia ao longo de vÃ¡rias realizaÃ§Ãµes [^1]:

$$
\gamma_{jt} = plim_{I \to \infty} \frac{1}{I} \sum_{i=1}^{I} [Y_t^{(i)} - \mu_t][Y_{t-j}^{(i)} - \mu_{t-j}]
$$

onde $Y_t^{(i)}$ Ã© a $i$-Ã©sima realizaÃ§Ã£o da sÃ©rie temporal no tempo $t$.

Para ilustrar, considere um exemplo numÃ©rico da autocovariÃ¢ncia para este processo.

> ðŸ’¡ **Exemplo NumÃ©rico:** Sejam os seguintes dados gerados por um processo $Y_t = 0.5t + \epsilon_t$, onde $\epsilon_t$ Ã© um ruÃ­do branco gaussiano com mÃ©dia 0 e variÃ¢ncia 1:
>
> | t   | Îµ_t    | Y_t    |
> | --- | ------ | ------ |
> | 1   | 0.2    | 0.7    |
> | 2   | -0.3   | 0.7    |
> | 3   | 0.1    | 1.6    |
> | 4   | -0.2   | 1.8    |
> | 5   | 0.3    | 2.8    |
>
> A mÃ©dia amostral de Y_t nÃ£o Ã© constante, e a estimativa da autocovariÃ¢ncia no lag 1 Ã©:
>
> $\hat{\gamma}_{1} = \frac{1}{4} \sum_{t=2}^{5} (Y_t - \beta t)(Y_{t-1} - \beta (t-1)) = \frac{1}{4} [(0.7 - 0.5 * 2)(0.7 - 0.5 * 1) + ... ] \approx 0$
>
> Esta estimativa amostral aproxima-se de zero, consistindo com o fato de que a autocovariÃ¢ncia teÃ³rica Ã© zero para $j \neq 0$.
>
> ```python
> import numpy as np
>
> # Dados fornecidos
> t = np.array([1, 2, 3, 4, 5])
> epsilon_t = np.array([0.2, -0.3, 0.1, -0.2, 0.3])
> Y_t = np.array([0.7, 0.7, 1.6, 1.8, 2.8])
> beta = 0.5
>
> # FunÃ§Ã£o para calcular a autocovariÃ¢ncia amostral no lag j
> def sample_autocovariance(Y, beta, lag):
>     n = len(Y)
>     if lag >= n:
>         return 0  # Retorna 0 se o lag for maior ou igual ao tamanho da sÃ©rie
>     sum_term = 0
>     for i in range(lag, n):
>         sum_term += (Y[i] - beta * (i + 1)) * (Y[i - lag] - beta * ((i - lag) + 1))
>     return sum_term / (n - lag)
>
> # Calcula a autocovariÃ¢ncia amostral no lag 1
> lag = 1
> autocovariance_lag_1 = sample_autocovariance(Y_t, beta, lag)
> print(f"AutocovariÃ¢ncia amostral no lag {lag}: {autocovariance_lag_1:.4f}")
>
> # Calcula a autocovariÃ¢ncia amostral no lag 0 (variÃ¢ncia amostral)
> lag = 0
> autocovariance_lag_0 = sample_autocovariance(Y_t, beta, lag)
> print(f"AutocovariÃ¢ncia amostral no lag {lag}: {autocovariance_lag_0:.4f}")
> ```
> **InterpretaÃ§Ã£o:**  O valor da autocovariÃ¢ncia no lag 1 Ã© prÃ³ximo de zero, indicando que nÃ£o hÃ¡ correlaÃ§Ã£o linear significativa entre os valores de $Y_t$ e $Y_{t-1}$ apÃ³s remover a tendÃªncia. O valor da autocovariÃ¢ncia no lag 0 (variÃ¢ncia) Ã© uma medida da dispersÃ£o dos dados em torno da tendÃªncia.

Em resumo, a autocovariÃ¢ncia fornece *insights* valiosos sobre a estrutura de dependÃªncia temporal de uma sÃ©rie temporal. Para o processo $Y_t = \beta t + \epsilon_t$, onde $\epsilon_t$ Ã© um ruÃ­do branco Gaussiano, a autocovariÃ¢ncia Ã© zero para todos os lags diferentes de zero, indicando a ausÃªncia de dependÃªncia linear entre os diferentes pontos no tempo.

### ConclusÃ£o

O estudo da autocovariÃ¢ncia Ã© essencial para compreender as propriedades de dependÃªncia temporal de sÃ©ries temporais.  Ao analisar um processo como $Y_t = \beta t + \epsilon_t$, onde $\epsilon_t$ Ã© um ruÃ­do branco Gaussiano, a autocovariÃ¢ncia destaca como a presenÃ§a de uma tendÃªncia de tempo e um ruÃ­do branco afetam a estrutura de dependÃªncia da sÃ©rie. Apesar de sua variÃ¢ncia ser constante, o processo nÃ£o Ã© estacionÃ¡rio devido Ã  sua mÃ©dia dependente do tempo. A autocovariÃ¢ncia, neste caso, Ã© nÃ£o-zero apenas no lag 0 e zero nos demais, refletindo a natureza nÃ£o correlacionada do termo de ruÃ­do branco. Esse conhecimento Ã© fundamental para a escolha de modelos apropriados para representar e prever sÃ©ries temporais.

### ReferÃªncias
[^1]: Imagine a battery of I such computers generating sequences ... The variance of the random variable $Y_t$ (denoted $\gamma_{tt}$) is similarly defined as $\gamma_{tt} = E[(Y_t - \mu_t)^2] = \int_{-\infty}^{\infty} (y_t - \mu_t)^2 f_{Y_t}(y_t) \, dy_t$.
[^2]: Given a particular realization such as $\{y_t^{(i)}\}_{t=-\infty}^{\infty}$ on a time series process... If neither the mean $\mu_t$ nor the autocovariances $\gamma_{jt}$ depend on the date $t$, then the process for $Y_t$ is said to be covariance-stationary or weakly stationary: $E(Y_t) = \mu$ for all $t$ and $E[(Y_t - \mu)(Y_{t-j} - \mu)] = \gamma_j$ for all $t$ and any $j$.
[^3]: Previous Topics: --- START The variance Î³_tt of a time series Y_t measures the dispersion or variability of the values of Y_t around its mean Î¼_t. It is formally defined as the expected value of the squared deviation from the mean: Î³_tt = E[(Y_t - Î¼_t)Â²]. Mathematically, this is calculated by integrating the squared difference (y_t - Î¼_t)^2 multiplied by the probability density function f_t(y_t) over all possible values of y_t. ---
<!-- END -->