## Autocovari√¢ncia de Processos Estacion√°rios

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre a **vari√¢ncia** de s√©ries temporais [^3], este cap√≠tulo aprofunda-se no conceito de **autocovari√¢ncia**. Como vimos anteriormente, a vari√¢ncia ($\gamma_{tt}$) mede a dispers√£o dos valores de uma s√©rie temporal em torno da sua m√©dia [^1]. A autocovari√¢ncia, por sua vez, estende essa ideia medindo a depend√™ncia linear entre valores da s√©rie em diferentes pontos no tempo. Especificamente, analisaremos como a autocovari√¢ncia se comporta em processos estacion√°rios, com √™nfase no caso em que a s√©rie temporal √© representada como a soma de uma **tend√™ncia de tempo** e **ru√≠do branco Gaussiano**, $Y_t = \beta t + \epsilon_t$ [^1].

### Conceitos Fundamentais

A **autocovari√¢ncia** de uma s√©rie temporal $Y_t$ no lag $j$, denotada por $\gamma_{jt}$, quantifica a covari√¢ncia entre $Y_t$ e $Y_{t-j}$ [^2]. Formalmente, √© definida como [^2]:

$$
\gamma_{jt} = E[(Y_t - \mu_t)(Y_{t-j} - \mu_{t-j})]
$$

onde $\mu_t$ √© a m√©dia de $Y_t$ no tempo $t$ [^1].

Para um processo **estacion√°rio**, a m√©dia e a autocovari√¢ncia n√£o dependem do tempo, ou seja, $\mu_t = \mu$ e $\gamma_{jt} = \gamma_j$ para todos os tempos $t$ e lags $j$ [^2]. Isso implica que a depend√™ncia linear entre $Y_t$ e $Y_{t-j}$ √© constante ao longo do tempo.

> üí° **Observa√ß√£o Importante:** Em processos estacion√°rios, a autocovari√¢ncia no lag zero, $\gamma_{0}$, √© igual √† vari√¢ncia $\gamma_{tt}$ [^2].

Para o processo espec√≠fico em considera√ß√£o, $Y_t = \beta t + \epsilon_t$, onde $\epsilon_t$ √© um **ru√≠do branco Gaussiano** com m√©dia zero e vari√¢ncia $\sigma^2$, podemos calcular a autocovari√¢ncia da seguinte forma:

*Prova:*
I. Dado $Y_t = \beta t + \epsilon_t$, onde $E[\epsilon_t] = 0$ e $Var(\epsilon_t) = \sigma^2$.

II. A m√©dia de $Y_t$ √© $E[Y_t] = E[\beta t + \epsilon_t] = \beta t + E[\epsilon_t] = \beta t + 0 = \beta t$.

III. A autocovari√¢ncia no lag $j$ √©:
$$
\gamma_{jt} = E[(Y_t - E[Y_t])(Y_{t-j} - E[Y_{t-j}])] = E[(\beta t + \epsilon_t - \beta t)(\beta (t-j) + \epsilon_{t-j} - \beta (t-j))]
$$
$$
\gamma_{jt} = E[\epsilon_t \epsilon_{t-j}]
$$

IV. Como $\epsilon_t$ √© um ru√≠do branco Gaussiano, $E[\epsilon_t \epsilon_{t-j}] = 0$ para $j \neq 0$ e $E[\epsilon_t^2] = \sigma^2$ para $j = 0$. Portanto,
$$
E[\epsilon_t \epsilon_{t-j}] =
\begin{cases}
\sigma^2 & \text{se } j = 0 \\
0 & \text{se } j \neq 0
\end{cases}
$$

V. Portanto,
$$
\gamma_{jt} =
\begin{cases}
\sigma^2 & \text{se } j = 0 \\
0 & \text{se } j \neq 0
\end{cases}
$$
$\blacksquare$

Este resultado mostra que, para o processo $Y_t = \beta t + \epsilon_t$, a autocovari√¢ncia √© zero para todos os lags diferentes de zero. Isso ocorre porque o termo de tend√™ncia de tempo $\beta t$ √© determin√≠stico, e toda a variabilidade e depend√™ncia temporal s√£o capturadas pelo termo de ru√≠do branco $\epsilon_t$. Especificamente, como $\epsilon_t$ √© um ru√≠do branco, seus valores em diferentes pontos no tempo s√£o n√£o correlacionados.

> üí° **Exemplo Num√©rico:** Seja $\beta = 2$ e $\epsilon_t$ um ru√≠do branco Gaussiano com vari√¢ncia $\sigma^2 = 4$. Ent√£o, $Y_t = 2t + \epsilon_t$. A autocovari√¢ncia de $Y_t$ √© $\gamma_{jt} = 4$ se $j = 0$ e $\gamma_{jt} = 0$ se $j \neq 0$.  Isto significa que valores de $Y_t$ *n√£o* s√£o linearmente dependentes de valores de $Y_{t-j}$ quando $j \ne 0$
> ```python
> import numpy as np
>
> beta = 2
> sigma_squared = 4
>
> # Gerando dados de exemplo
> np.random.seed(0)  # Para reprodutibilidade
> T = 100  # N√∫mero de pontos no tempo
> epsilon = np.random.normal(0, np.sqrt(sigma_squared), T)  # Ru√≠do branco Gaussiano
> t = np.arange(1, T + 1)
> Y = beta * t + epsilon
>
> # Calculando a autocovari√¢ncia amostral
> def autocovariance(series, lag):
>     n = len(series)
>     series_mean = np.mean(series)
>     if lag == 0:
>         return np.sum((series - series_mean) ** 2) / n
>     else:
>         return np.sum((series[lag:] - series_mean) * (series[:-lag] - series_mean)) / n
>
> # Autocovari√¢ncia no lag 0
> gamma_0 = autocovariance(Y, 0)
> print(f"Autocovari√¢ncia no lag 0: {gamma_0:.2f}")
>
> # Autocovari√¢ncia no lag 1
> gamma_1 = autocovariance(Y, 1)
> print(f"Autocovari√¢ncia no lag 1: {gamma_1:.2f}")
>
> # Autocovari√¢ncia no lag 5
> gamma_5 = autocovariance(Y, 5)
> print(f"Autocovari√¢ncia no lag 5: {gamma_5:.2f}")
> ```
>
> **Interpreta√ß√£o:** Os valores calculados demonstram que a autocovari√¢ncia no lag 0 √© aproximadamente igual √† vari√¢ncia do ru√≠do branco (4), enquanto que nos lags maiores (1 e 5) a autocovari√¢ncia se aproxima de zero, confirmando a independ√™ncia dos res√≠duos em diferentes pontos no tempo. Essa propriedade √© fundamental para verificar a adequa√ß√£o do modelo.

√â crucial notar que, embora a autocovari√¢ncia n√£o dependa do tempo (ou seja, $\gamma_{jt} = \gamma_j$) para este processo espec√≠fico, o processo *n√£o* √© estacion√°rio porque a sua m√©dia, $E[Y_t] = \beta t$, depende do tempo. A condi√ß√£o de autocovari√¢ncia independente do tempo √© apenas uma das condi√ß√µes para estacionariedade fraca [^2].

Para complementar essa discuss√£o, podemos analisar a autocovari√¢ncia de um processo relacionado, onde a tend√™ncia √© removida.

**Teorema 1**
Seja $Z_t = Y_t - \beta t = \epsilon_t$. A autocovari√¢ncia de $Z_t$ no lag $j$ √© dada por:
$$
\gamma_{j} =
\begin{cases}
\sigma^2 & \text{se } j = 0 \\
0 & \text{se } j \neq 0
\end{cases}
$$
*Prova:*
I. Dado $Z_t = Y_t - \beta t = \epsilon_t$.
II. A m√©dia de $Z_t$ √© $E[Z_t] = E[\epsilon_t] = 0$.
III. A autocovari√¢ncia no lag $j$ √©:
$$
\gamma_{j} = E[(Z_t - E[Z_t])(Z_{t-j} - E[Z_{t-j}])] = E[(Z_t - 0)(Z_{t-j} - 0)] = E[\epsilon_t \epsilon_{t-j}]
$$
IV. Como $\epsilon_t$ √© um ru√≠do branco Gaussiano, $E[\epsilon_t \epsilon_{t-j}] = 0$ para $j \neq 0$ e $E[\epsilon_t \epsilon_{t-j}] = \sigma^2$ para $j = 0$.
$\blacksquare$

**Teorema 1.1**
O processo $Z_t$ definido acima √© fracamente estacion√°rio.

*Prova:*
I. $E[Z_t] = E[\epsilon_t] = 0$, que √© constante.
II. $\gamma_j = E[(Z_t - 0)(Z_{t-j} - 0)] = E[\epsilon_t \epsilon_{t-j}]$, que n√£o depende de t.
Portanto, $Z_t$ √© fracamente estacion√°rio.
$\blacksquare$

A autocovari√¢ncia est√° intimamente relacionada √† **autocorrela√ß√£o**. A autocorrela√ß√£o no lag $j$, denotada por $\rho_j$, √© a autocovari√¢ncia no lag $j$ dividida pela vari√¢ncia:

$$
\rho_j = \frac{\gamma_j}{\gamma_0}
$$

Para o processo $Y_t = \beta t + \epsilon_t$, a autocorrela√ß√£o √©:

$$
\rho_j =
\begin{cases}
1 & \text{se } j = 0 \\
0 & \text{se } j \neq 0
\end{cases}
$$

Isto significa que o coeficiente de autocorrela√ß√£o para o processo descrito √© 1 para lag 0 e 0 para qualquer outro lag.

**Teorema 2**
Seja $W_t = \alpha + \epsilon_t$, onde $\alpha$ √© uma constante e $\epsilon_t$ √© um ru√≠do branco Gaussiano com m√©dia zero e vari√¢ncia $\sigma^2$. Ent√£o a autocorrela√ß√£o de $W_t$ √©:
$$
\rho_j =
\begin{cases}
1 & \text{se } j = 0 \\
0 & \text{se } j \neq 0
\end{cases}
$$

*Prova:*
I. $E[W_t] = E[\alpha + \epsilon_t] = \alpha + E[\epsilon_t] = \alpha + 0 = \alpha$.
II. $\gamma_0 = E[(W_t - E[W_t])^2] = E[(W_t - \alpha)^2] = E[(\alpha + \epsilon_t - \alpha)^2] = E[\epsilon_t^2] = \sigma^2$.
III. $\gamma_j = E[(W_t - E[W_t])(W_{t-j} - E[W_{t-j}])] = E[(W_t - \alpha)(W_{t-j} - \alpha)] = E[(\alpha + \epsilon_t - \alpha)(\alpha + \epsilon_{t-j} - \alpha)] = E[\epsilon_t \epsilon_{t-j}]$. Para $j \neq 0$, $\gamma_j = 0$.
IV. Portanto, $\rho_j = \frac{\gamma_j}{\gamma_0}$. Se $j=0$, $\rho_0 = \frac{\sigma^2}{\sigma^2} = 1$. Se $j \neq 0$, $\rho_j = \frac{0}{\sigma^2} = 0$.
$\blacksquare$

> üí° **Exemplo Num√©rico:** Vamos considerar $W_t = 5 + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco com $\sigma^2 = 9$.  Simularemos alguns dados e calcularemos a autocorrela√ß√£o amostral.
> ```python
> import numpy as np
>
> alpha = 5
> sigma_squared = 9
> np.random.seed(1)
> T = 100
> epsilon = np.random.normal(0, np.sqrt(sigma_squared), T)
> W = alpha + epsilon
>
> def autocorrelation(series, lag):
>     n = len(series)
>     series_mean = np.mean(series)
>     gamma_0 = np.sum((series - series_mean) ** 2) / n
>     if lag == 0:
>         return 1.0
>     gamma_j = np.sum((series[lag:] - series_mean) * (series[:-lag] - series_mean)) / n
>     return gamma_j / gamma_0
>
> rho_0 = autocorrelation(W, 0)
> print(f"Autocorrela√ß√£o no lag 0: {rho_0:.2f}")
>
> rho_1 = autocorrelation(W, 1)
> print(f"Autocorrela√ß√£o no lag 1: {rho_1:.2f}")
>
> rho_5 = autocorrelation(W, 5)
> print(f"Autocorrela√ß√£o no lag 5: {rho_5:.2f}")
> ```
> **Interpreta√ß√£o:** A autocorrela√ß√£o no lag 0 √© sempre 1. A autocorrela√ß√£o nos lags 1 e 5 s√£o pr√≥ximos de zero, indicando que os valores de $W_t$ n√£o s√£o correlacionados com seus valores passados, o que √© esperado para um processo de ru√≠do branco.

A autocovari√¢ncia tamb√©m pode ser vista como o limite de probabilidade da m√©dia do produto dos desvios em rela√ß√£o √† m√©dia ao longo de v√°rias realiza√ß√µes [^1]:

$$
\gamma_{jt} = plim_{I \to \infty} \frac{1}{I} \sum_{i=1}^{I} [Y_t^{(i)} - \mu_t][Y_{t-j}^{(i)} - \mu_{t-j}]
$$

onde $Y_t^{(i)}$ √© a $i$-√©sima realiza√ß√£o da s√©rie temporal no tempo $t$.

Para ilustrar, considere um exemplo num√©rico da autocovari√¢ncia para este processo.

> üí° **Exemplo Num√©rico:** Sejam os seguintes dados gerados por um processo $Y_t = 0.5t + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco gaussiano com m√©dia 0 e vari√¢ncia 1:
>
> | t   | Œµ_t    | Y_t    |
> | --- | ------ | ------ |
> | 1   | 0.2    | 0.7    |
> | 2   | -0.3   | 0.7    |
> | 3   | 0.1    | 1.6    |
> | 4   | -0.2   | 1.8    |
> | 5   | 0.3    | 2.8    |
>
> A m√©dia amostral de Y_t n√£o √© constante, e a estimativa da autocovari√¢ncia no lag 1 √©:
>
> $\hat{\gamma}_{1} = \frac{1}{4} \sum_{t=2}^{5} (Y_t - \beta t)(Y_{t-1} - \beta (t-1)) = \frac{1}{4} [(0.7 - 0.5 * 2)(0.7 - 0.5 * 1) + ... ] \approx 0$
>
> Esta estimativa amostral aproxima-se de zero, consistindo com o fato de que a autocovari√¢ncia te√≥rica √© zero para $j \neq 0$.
>
> ```python
> import numpy as np
>
> # Dados fornecidos
> t = np.array([1, 2, 3, 4, 5])
> epsilon_t = np.array([0.2, -0.3, 0.1, -0.2, 0.3])
> Y_t = np.array([0.7, 0.7, 1.6, 1.8, 2.8])
> beta = 0.5
>
> # Fun√ß√£o para calcular a autocovari√¢ncia amostral no lag j
> def sample_autocovariance(Y, beta, lag):
>     n = len(Y)
>     if lag >= n:
>         return 0  # Retorna 0 se o lag for maior ou igual ao tamanho da s√©rie
>     sum_term = 0
>     for i in range(lag, n):
>         sum_term += (Y[i] - beta * (i + 1)) * (Y[i - lag] - beta * ((i - lag) + 1))
>     return sum_term / (n - lag)
>
> # Calcula a autocovari√¢ncia amostral no lag 1
> lag = 1
> autocovariance_lag_1 = sample_autocovariance(Y_t, beta, lag)
> print(f"Autocovari√¢ncia amostral no lag {lag}: {autocovariance_lag_1:.4f}")
>
> # Calcula a autocovari√¢ncia amostral no lag 0 (vari√¢ncia amostral)
> lag = 0
> autocovariance_lag_0 = sample_autocovariance(Y_t, beta, lag)
> print(f"Autocovari√¢ncia amostral no lag {lag}: {autocovariance_lag_0:.4f}")
> ```
> **Interpreta√ß√£o:**  O valor da autocovari√¢ncia no lag 1 √© pr√≥ximo de zero, indicando que n√£o h√° correla√ß√£o linear significativa entre os valores de $Y_t$ e $Y_{t-1}$ ap√≥s remover a tend√™ncia. O valor da autocovari√¢ncia no lag 0 (vari√¢ncia) √© uma medida da dispers√£o dos dados em torno da tend√™ncia.

Em resumo, a autocovari√¢ncia fornece *insights* valiosos sobre a estrutura de depend√™ncia temporal de uma s√©rie temporal. Para o processo $Y_t = \beta t + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco Gaussiano, a autocovari√¢ncia √© zero para todos os lags diferentes de zero, indicando a aus√™ncia de depend√™ncia linear entre os diferentes pontos no tempo.

### Conclus√£o

O estudo da autocovari√¢ncia √© essencial para compreender as propriedades de depend√™ncia temporal de s√©ries temporais.  Ao analisar um processo como $Y_t = \beta t + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco Gaussiano, a autocovari√¢ncia destaca como a presen√ßa de uma tend√™ncia de tempo e um ru√≠do branco afetam a estrutura de depend√™ncia da s√©rie. Apesar de sua vari√¢ncia ser constante, o processo n√£o √© estacion√°rio devido √† sua m√©dia dependente do tempo. A autocovari√¢ncia, neste caso, √© n√£o-zero apenas no lag 0 e zero nos demais, refletindo a natureza n√£o correlacionada do termo de ru√≠do branco. Esse conhecimento √© fundamental para a escolha de modelos apropriados para representar e prever s√©ries temporais.

### Refer√™ncias
[^1]: Imagine a battery of I such computers generating sequences ... The variance of the random variable $Y_t$ (denoted $\gamma_{tt}$) is similarly defined as $\gamma_{tt} = E[(Y_t - \mu_t)^2] = \int_{-\infty}^{\infty} (y_t - \mu_t)^2 f_{Y_t}(y_t) \, dy_t$.
[^2]: Given a particular realization such as $\{y_t^{(i)}\}_{t=-\infty}^{\infty}$ on a time series process... If neither the mean $\mu_t$ nor the autocovariances $\gamma_{jt}$ depend on the date $t$, then the process for $Y_t$ is said to be covariance-stationary or weakly stationary: $E(Y_t) = \mu$ for all $t$ and $E[(Y_t - \mu)(Y_{t-j} - \mu)] = \gamma_j$ for all $t$ and any $j$.
[^3]: Previous Topics: --- START The variance Œ≥_tt of a time series Y_t measures the dispersion or variability of the values of Y_t around its mean Œº_t. It is formally defined as the expected value of the squared deviation from the mean: Œ≥_tt = E[(Y_t - Œº_t)¬≤]. Mathematically, this is calculated by integrating the squared difference (y_t - Œº_t)^2 multiplied by the probability density function f_t(y_t) over all possible values of y_t. ---
<!-- END -->