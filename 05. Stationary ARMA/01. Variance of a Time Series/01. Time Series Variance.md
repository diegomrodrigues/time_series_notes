## The Variance of a Time Series

### Introdu√ß√£o
Este cap√≠tulo explora o conceito de **vari√¢ncia** em s√©ries temporais, um tema fundamental na an√°lise de . Construindo sobre os conceitos de **expectativa** ou m√©dia (**mean**) discutidos anteriormente [^3], a vari√¢ncia quantifica a dispers√£o dos valores de uma s√©rie temporal em torno de sua m√©dia. Vamos analisar a defini√ß√£o formal da vari√¢ncia, seu c√°lculo e sua interpreta√ß√£o, estabelecendo uma base s√≥lida para a compreens√£o de modelos mais complexos.

### Conceitos Fundamentais

A **vari√¢ncia** de uma s√©rie temporal $Y_t$, denotada por $\gamma_{tt}$, √© uma medida de como os valores de $Y_t$ se espalham ou variam em rela√ß√£o √† sua m√©dia $\mu_t$ [^1]. Formalmente, a vari√¢ncia √© definida como o valor esperado do quadrado do desvio de $Y_t$ de sua m√©dia [^1]:

$$
\gamma_{tt} = E[(Y_t - \mu_t)^2]
$$

onde $E[\cdot]$ denota o operador de expectativa [^1].

Para calcular a vari√¢ncia, √© necess√°rio integrar o quadrado da diferen√ßa $(y_t - \mu_t)^2$ multiplicado pela fun√ß√£o densidade de probabilidade $f_t(y_t)$ sobre todos os valores poss√≠veis de $y_t$ [^1]:

$$
\gamma_{tt} = \int_{-\infty}^{\infty} (y_t - \mu_t)^2 f_t(y_t) \, dy_t
$$

> üí° **Exemplo Num√©rico:** Suponha que temos uma s√©rie temporal discreta com os seguintes valores: $Y_t = \{1, 2, 3, 4, 5\}$. Vamos calcular a vari√¢ncia amostral.
>
> 1.  Calcular a m√©dia amostral:
>     $$
>     \bar{y} = \frac{1 + 2 + 3 + 4 + 5}{5} = 3
>     $$
>
> 2.  Calcular a vari√¢ncia amostral:
>     $$
>     \hat{\gamma}_{tt} = \frac{1}{5-1} \sum_{t=1}^{5} (y_t - \bar{y})^2
>     $$
>     $$
>     \hat{\gamma}_{tt} = \frac{1}{4} [(1-3)^2 + (2-3)^2 + (3-3)^2 + (4-3)^2 + (5-3)^2]
>     $$
>     $$
>     \hat{\gamma}_{tt} = \frac{1}{4} [4 + 1 + 0 + 1 + 4] = \frac{10}{4} = 2.5
>     $$
>
> Portanto, a vari√¢ncia amostral desta s√©rie temporal √© 2.5.

√â importante notar que, de acordo com a defini√ß√£o [^1], a m√©dia $\mu_t$ pode ser uma fun√ß√£o do tempo $t$, permitindo assim que a s√©rie temporal exiba um comportamento n√£o estacion√°rio. A vari√¢ncia $\gamma_{tt}$ neste caso mede a dispers√£o em torno dessa m√©dia variante no tempo. No entanto, se a m√©dia for constante, ou seja, $\mu_t = \mu$, ent√£o a vari√¢ncia se torna uma medida da dispers√£o em torno dessa m√©dia constante.

Para um processo espec√≠fico, como o processo de **ru√≠do branco Gaussiano** [^1], a densidade $f_{Y_t}(y_t)$ √© dada por:

$$
f_{Y_t}(y_t) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left[-\frac{y_t^2}{2\sigma^2}\right]
$$

onde $\sigma^2$ √© a vari√¢ncia [^1]. Neste caso, a m√©dia √© zero, e a vari√¢ncia √© simplesmente $\sigma^2$.

**Exemplo:** Para o processo $Y_t = \mu + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco Gaussiano com m√©dia zero, a vari√¢ncia √© [^1]:

$$
\gamma_{tt} = E[(Y_t - \mu)^2] = E[(\epsilon_t)^2] = \sigma^2
$$

Para demonstrar isso, temos a seguinte prova:

*Prova:*
I. Dado $Y_t = \mu + \epsilon_t$, onde $E[\epsilon_t] = 0$.

II. Calcule $E[Y_t]$:
$$
E[Y_t] = E[\mu + \epsilon_t] = E[\mu] + E[\epsilon_t] = \mu + 0 = \mu
$$

III. Calcule a vari√¢ncia $\gamma_{tt}$:
$$
\gamma_{tt} = E[(Y_t - \mu)^2] = E[(\mu + \epsilon_t - \mu)^2] = E[(\epsilon_t)^2]
$$

IV. Como $\epsilon_t$ √© um ru√≠do branco Gaussiano com vari√¢ncia $\sigma^2$, temos $E[(\epsilon_t)^2] = \sigma^2$.

V. Portanto, $\gamma_{tt} = \sigma^2$ ‚ñ†

> üí° **Exemplo Num√©rico:** Seja $\mu = 5$ e $\epsilon_t$ um ru√≠do branco Gaussiano com vari√¢ncia $\sigma^2 = 2$. Ent√£o, $Y_t = 5 + \epsilon_t$. A vari√¢ncia de $Y_t$ √© $\gamma_{tt} = \sigma^2 = 2$. Isto significa que os valores de $Y_t$ se dispersam em torno de 5, com uma vari√¢ncia de 2.

No entanto, para um processo com uma tend√™ncia de tempo, como $Y_t = \beta t + \epsilon_t$, a vari√¢ncia √© [^1]:

$$
\gamma_{tt} = E[(Y_t - \beta t)^2] = E[(\epsilon_t)^2] = \sigma^2
$$

A prova disso √© a seguinte:

*Prova:*
I. Dado $Y_t = \beta t + \epsilon_t$, onde $E[\epsilon_t] = 0$.

II. Calcule $E[Y_t]$:
$$
E[Y_t] = E[\beta t + \epsilon_t] = \beta t + E[\epsilon_t] = \beta t + 0 = \beta t
$$

III. Calcule a vari√¢ncia $\gamma_{tt}$:
$$
\gamma_{tt} = E[(Y_t - \beta t)^2] = E[(\beta t + \epsilon_t - \beta t)^2] = E[(\epsilon_t)^2]
$$

IV. Como $\epsilon_t$ √© um ru√≠do branco Gaussiano com vari√¢ncia $\sigma^2$, temos $E[(\epsilon_t)^2] = \sigma^2$.

V. Portanto, $\gamma_{tt} = \sigma^2$ ‚ñ†

> üí° **Exemplo Num√©rico:** Seja $\beta = 0.5$ e $\epsilon_t$ um ru√≠do branco Gaussiano com vari√¢ncia $\sigma^2 = 1$. Ent√£o, $Y_t = 0.5t + \epsilon_t$. A vari√¢ncia de $Y_t$ √© $\gamma_{tt} = \sigma^2 = 1$. A m√©dia de $Y_t$ varia com o tempo, mas a dispers√£o em torno dessa m√©dia, medida pela vari√¢ncia, permanece constante em 1.

Em ambos os casos, a vari√¢ncia √© igual √† vari√¢ncia do termo de ru√≠do branco, demonstrando como a vari√¢ncia mede a dispers√£o em torno da m√©dia, seja ela constante ou vari√°vel no tempo.

A vari√¢ncia, juntamente com a m√©dia, fornece uma descri√ß√£o fundamental das caracter√≠sticas de uma s√©rie temporal. Em particular, a vari√¢ncia √© usada no c√°lculo das autocovari√¢ncias, que medem a depend√™ncia linear entre diferentes pontos no tempo em uma s√©rie temporal [^2]. A vari√¢ncia tamb√©m est√° intimamente relacionada ao conceito de **estacionariedade**. Uma s√©rie temporal √© dita fracamente estacion√°ria (ou estacion√°ria em covari√¢ncia) se sua m√©dia e autocovari√¢ncia n√£o dependem do tempo [^2]. Isso implica que a vari√¢ncia tamb√©m √© constante ao longo do tempo para uma s√©rie temporal fracamente estacion√°ria.

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal $\{Y_t\}$ com $Y_t = 2 + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco Gaussiano com m√©dia 0 e vari√¢ncia $\sigma^2 = 0.5$.  Esta s√©rie √© estacion√°ria porque sua m√©dia √© constante (2) e sua vari√¢ncia √© constante (0.5).
>
> Agora considere uma s√©rie temporal $\{X_t\}$ com $X_t = t + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco Gaussiano com m√©dia 0 e vari√¢ncia $\sigma^2 = 0.5$.  Esta s√©rie *n√£o* √© estacion√°ria porque sua m√©dia ($E[X_t] = t$) varia com o tempo.  Embora a vari√¢ncia do ru√≠do seja constante, a m√©dia vari√°vel significa que a s√©rie como um todo n√£o √© estacion√°ria.

Para complementar essa discuss√£o, √© √∫til considerar a vari√¢ncia da soma de duas s√©ries temporais.

**Teorema 1:** *Sejam $X_t$ e $Y_t$ duas s√©ries temporais com m√©dias $\mu_{X_t}$ e $\mu_{Y_t}$ e vari√¢ncias $\gamma_{X_{tt}}$ e $\gamma_{Y_{tt}}$, respectivamente. A vari√¢ncia da s√©rie temporal $Z_t = X_t + Y_t$ √© dada por:*

$$
\gamma_{Z_{tt}} = \gamma_{X_{tt}} + \gamma_{Y_{tt}} + 2Cov(X_t, Y_t)
$$

*onde $Cov(X_t, Y_t) = E[(X_t - \mu_{X_t})(Y_t - \mu_{Y_t})]$ √© a covari√¢ncia entre $X_t$ e $Y_t$.*

*Prova:*
\begin{align*}
\gamma_{Z_{tt}} &= E[(Z_t - \mu_{Z_t})^2] \\
&= E[((X_t + Y_t) - (\mu_{X_t} + \mu_{Y_t}))^2] \\
&= E[(X_t - \mu_{X_t} + Y_t - \mu_{Y_t})^2] \\
&= E[(X_t - \mu_{X_t})^2 + (Y_t - \mu_{Y_t})^2 + 2(X_t - \mu_{X_t})(Y_t - \mu_{Y_t})] \\
&= E[(X_t - \mu_{X_t})^2] + E[(Y_t - \mu_{Y_t})^2] + 2E[(X_t - \mu_{X_t})(Y_t - \mu_{Y_t})] \\
&= \gamma_{X_{tt}} + \gamma_{Y_{tt}} + 2Cov(X_t, Y_t)
\end{align*}
‚ñ†

> üí° **Exemplo Num√©rico:** Sejam $X_t$ e $Y_t$ duas s√©ries temporais. Suponha que $X_t$ tem vari√¢ncia $\gamma_{X_{tt}} = 3$, $Y_t$ tem vari√¢ncia $\gamma_{Y_{tt}} = 5$, e a covari√¢ncia entre $X_t$ e $Y_t$ √© $Cov(X_t, Y_t) = 1$. Ent√£o, a vari√¢ncia da s√©rie temporal $Z_t = X_t + Y_t$ √©:
>
> $\gamma_{Z_{tt}} = 3 + 5 + 2(1) = 10$.
>
> Se $X_t$ e $Y_t$ fossem negativamente correlacionadas com $Cov(X_t, Y_t) = -1$, ent√£o a vari√¢ncia de $Z_t$ seria:
>
> $\gamma_{Z_{tt}} = 3 + 5 + 2(-1) = 6$.
>
> Isto demonstra como a covari√¢ncia entre duas s√©ries temporais afeta a vari√¢ncia de sua soma.

Um caso especial importante ocorre quando $X_t$ e $Y_t$ s√£o n√£o correlacionadas, ou seja, $Cov(X_t, Y_t) = 0$. Nesse caso, a vari√¢ncia da soma √© simplesmente a soma das vari√¢ncias.

**Corol√°rio 1.1:** *Se $X_t$ e $Y_t$ s√£o n√£o correlacionadas, ent√£o $\gamma_{Z_{tt}} = \gamma_{X_{tt}} + \gamma_{Y_{tt}}$.*

> üí° **Exemplo Num√©rico:** Se $X_t$ tem vari√¢ncia 4 e $Y_t$ tem vari√¢ncia 6, e $X_t$ e $Y_t$ s√£o n√£o correlacionadas, ent√£o a vari√¢ncia da soma $Z_t = X_t + Y_t$ √© $\gamma_{Z_{tt}} = 4 + 6 = 10$.

Para complementar a discuss√£o sobre vari√¢ncia e estacionariedade, podemos introduzir o conceito de **vari√¢ncia amostral**.

**Defini√ß√£o:** A **vari√¢ncia amostral** de uma s√©rie temporal observada $y_1, y_2, \ldots, y_n$ √© definida como:

$$
\hat{\gamma}_{tt} = \frac{1}{n-1} \sum_{t=1}^{n} (y_t - \bar{y})^2
$$

onde $\bar{y} = \frac{1}{n} \sum_{t=1}^{n} y_t$ √© a m√©dia amostral da s√©rie temporal.

**Observa√ß√£o:** O uso de $(n-1)$ em vez de $n$ no denominador fornece um estimador n√£o viesado da vari√¢ncia populacional [^4].

Al√©m disso, a rela√ß√£o entre a vari√¢ncia e o desvio padr√£o √© fundamental.

**Defini√ß√£o:** O **desvio padr√£o** de uma s√©rie temporal $Y_t$, denotado por $\sigma_t$, √© a raiz quadrada da vari√¢ncia:

$$
\sigma_t = \sqrt{\gamma_{tt}} = \sqrt{E[(Y_t - \mu_t)^2]}
$$

O desvio padr√£o fornece uma medida da dispers√£o dos dados na mesma unidade da s√©rie temporal original, tornando-o mais facilmente interpret√°vel do que a vari√¢ncia.

> üí° **Exemplo Num√©rico:** Se a vari√¢ncia de uma s√©rie temporal √© $\gamma_{tt} = 9$, ent√£o o desvio padr√£o √© $\sigma_t = \sqrt{9} = 3$. Isto significa que, em m√©dia, os valores da s√©rie temporal se desviam da m√©dia por 3 unidades.

**Teorema 2:** *Para uma s√©rie temporal estacion√°ria com m√©dia $\mu$ e vari√¢ncia $\gamma_{tt} = \sigma^2$, o desvio padr√£o √© constante ao longo do tempo e igual a $\sigma$.*

*Prova:*
I. Por defini√ß√£o, uma s√©rie temporal estacion√°ria tem $E[Y_t] = \mu$ e $Var(Y_t) = \sigma^2$ para todo $t$.
II. O desvio padr√£o √© a raiz quadrada da vari√¢ncia: $\sigma_t = \sqrt{Var(Y_t)}$.
III. Como $Var(Y_t) = \sigma^2$ para todo $t$, ent√£o $\sigma_t = \sqrt{\sigma^2} = \sigma$ para todo $t$.
IV. Portanto, o desvio padr√£o √© constante ao longo do tempo e igual a $\sigma$. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal estacion√°ria $Y_t$ com m√©dia $\mu = 10$ e vari√¢ncia $\sigma^2 = 4$. O desvio padr√£o √© $\sigma = \sqrt{4} = 2$. Isso significa que, em qualquer ponto no tempo $t$, os valores de $Y_t$ se dispersam em torno de 10, com um desvio padr√£o de 2. A consist√™ncia da m√©dia e vari√¢ncia (e, portanto, do desvio padr√£o) ao longo do tempo √© o que caracteriza a estacionariedade desta s√©rie.

### Conclus√£o

A vari√¢ncia $\gamma_{tt}$ √© uma medida crucial para entender a dispers√£o dos dados em uma s√©rie temporal em torno de sua m√©dia. Este conceito √© essencial para definir a estacionariedade de uma s√©rie temporal e para calcular as autocovari√¢ncias, que, por sua vez, s√£o usadas para caracterizar a estrutura de depend√™ncia temporal dos dados. Dominar o conceito de vari√¢ncia √©, portanto, um passo fundamental para a an√°lise avan√ßada de s√©ries temporais e para a constru√ß√£o de modelos preditivos eficazes.

### Refer√™ncias
[^1]: Imagine a battery of I such computers generating sequences ... The variance of the random variable $Y_t$ (denoted $\gamma_{tt}$) is similarly defined as $\gamma_{tt} = E[(Y_t - \mu_t)^2] = \int_{-\infty}^{\infty} (y_t - \mu_t)^2 f_{Y_t}(y_t) \, dy_t$.
[^2]: Given a particular realization such as $\{y_t^{(i)}\}_{t=-\infty}^{\infty}$ on a time series process... If neither the mean $\mu_t$ nor the autocovariances $\gamma_{jt}$ depend on the date $t$, then the process for $Y_t$ is said to be covariance-stationary or weakly stationary: $E(Y_t) = \mu$ for all $t$ and $E[(Y_t - \mu)(Y_{t-j} - \mu)] = \gamma_j$ for all $t$ and any $j$.
[^3]: Imagine a battery of I such computers generating sequences ... The expectation of the tth observation of a time series refers to the mean of this probability distribution, provided it exists: $E(Y_t) = \int_{-\infty}^{\infty} y_t f_{Y_t}(y_t) \, dy_t$.
[^4]:  Statistics, 10th Edition, by McClave and Sincich (2006).
<!-- END -->