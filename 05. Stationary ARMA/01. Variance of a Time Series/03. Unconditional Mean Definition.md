## Modelos de S√©ries Temporais com M√©dias Vari√°veis no Tempo: M√©dia Incondicional e Autocovari√¢ncia

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre **autocovari√¢ncia de processos estacion√°rios** [^2], e construindo sobre a no√ß√£o de **vari√¢ncia** [^3], este cap√≠tulo explora a modelagem de s√©ries temporais onde a **m√©dia** pode variar com o tempo. O conceito de **m√©dia incondicional** $\mu_t$, onde $E(Y_t) = \mu_t$ [^1], permite acomodar m√©dias vari√°veis no tempo, como aquelas que surgem de tend√™ncias temporais. Examinaremos como a m√©dia incondicional afeta o c√°lculo e a interpreta√ß√£o da **autocovari√¢ncia** e outras propriedades estat√≠sticas de processos n√£o estacion√°rios.

### Conceitos Fundamentais

A **m√©dia incondicional**, denotada por $\mu_t$, √© o valor esperado da s√©rie temporal $Y_t$ no tempo $t$, permitindo que este valor esperado seja uma fun√ß√£o de $t$ [^1]. Formalmente:

$$
E(Y_t) = \mu_t
$$

Essa flexibilidade √© essencial para modelar s√©ries temporais que exibem um comportamento n√£o estacion√°rio na sua m√©dia, como as s√©ries que possuem uma tend√™ncia de tempo [^1].

**Exemplo:** Considere o processo com uma tend√™ncia de tempo, $Y_t = \beta t + \epsilon_t$, onde $\beta$ √© uma constante que determina a intensidade da tend√™ncia e $\epsilon_t$ √© um ru√≠do branco Gaussiano com m√©dia zero e vari√¢ncia $\sigma^2$ [^1]. Nesse caso, a m√©dia incondicional √©:

$$
\mu_t = E[Y_t] = E[\beta t + \epsilon_t] = \beta t + E[\epsilon_t] = \beta t
$$

Esta demonstra√ß√£o indica que, para este processo, a m√©dia depende linearmente de $t$, caracterizando uma tend√™ncia linear [^1].

> üí° **Exemplo Num√©rico:** Seja $\beta = 2$. Isso significa que a cada unidade de tempo $t$, a m√©dia incondicional aumenta em 2 unidades. Se $t=5$, ent√£o $\mu_5 = 2 \times 5 = 10$. Se $t=10$, ent√£o $\mu_{10} = 2 \times 10 = 20$. Isso ilustra como a m√©dia incondicional varia linearmente com o tempo.

Para um processo estacion√°rio, a m√©dia incondicional √© simplesmente uma constante, ou seja, $\mu_t = \mu$ para todo $t$ [^2]. No entanto, para processos n√£o estacion√°rios, a m√©dia incondicional pode assumir diversas formas funcionais, refletindo padr√µes de varia√ß√£o ao longo do tempo.

**Proposi√ß√£o 1:** Se $Y_t$ √© um processo estacion√°rio com m√©dia $\mu$, ent√£o $E[Y_t] = \mu$ para todo $t$.

*Prova:*
I. Por defini√ß√£o, um processo estacion√°rio tem m√©dia constante ao longo do tempo.
II. Portanto, $E[Y_t] = \mu$ para todo $t$.
$\blacksquare$

> üí° **Exemplo Num√©rico:** Se $Y_t$ representa a temperatura m√©dia di√°ria em uma cidade durante um m√™s e √© estacion√°rio com m√©dia $\mu = 25^\circ C$, ent√£o, em qualquer dia $t$ desse m√™s, $E[Y_t] = 25^\circ C$.

**Proposi√ß√£o 1.1:** Se $Y_t$ √© um processo estacion√°rio com m√©dia $\mu$, ent√£o $E[Y_t - \mu] = 0$ para todo $t$.

*Prova:*
I. Por defini√ß√£o, $E[Y_t] = \mu$ para todo $t$.
II. $E[Y_t - \mu] = E[Y_t] - E[\mu] = \mu - \mu = 0$.
$\blacksquare$

> üí° **Exemplo Num√©rico:** Se $Y_t$ tem m√©dia $\mu = 25$, ent√£o $Y_t - \mu$ representa o desvio da temperatura em rela√ß√£o √† m√©dia. O valor esperado desse desvio, $E[Y_t - 25]$, √© 0, indicando que os desvios positivos e negativos se cancelam ao longo do tempo.

**Autocovari√¢ncia com M√©dia Incondicional Vari√°vel**

A autocovari√¢ncia entre $Y_t$ e $Y_{t-j}$, onde $Y_t$ tem uma m√©dia incondicional $\mu_t$, √© definida como [^2]:

$$
\gamma_{jt} = E[(Y_t - \mu_t)(Y_{t-j} - \mu_{t-j})]
$$

Neste caso, a autocovari√¢ncia depende tanto do lag $j$ quanto do tempo $t$, refletindo a natureza n√£o estacion√°ria do processo. Se a m√©dia incondicional for constante, ent√£o a autocovari√¢ncia depender√° apenas do lag $j$, como demonstrado no cap√≠tulo anterior [^2].

Considerando novamente o processo $Y_t = \beta t + \epsilon_t$, a autocovari√¢ncia √© calculada da seguinte forma:

*Prova:*
I. Dado $Y_t = \beta t + \epsilon_t$, onde $E[\epsilon_t] = 0$ e $Var(\epsilon_t) = \sigma^2$.

II. A m√©dia incondicional √© $\mu_t = E[Y_t] = \beta t$.

III. A autocovari√¢ncia no lag $j$ √©:
$$
\gamma_{jt} = E[(Y_t - \mu_t)(Y_{t-j} - \mu_{t-j})] = E[(\beta t + \epsilon_t - \beta t)(\beta (t-j) + \epsilon_{t-j} - \beta (t-j))]
$$
$$
\gamma_{jt} = E[\epsilon_t \epsilon_{t-j}]
$$

IV. Como $\epsilon_t$ √© um ru√≠do branco Gaussiano, $E[\epsilon_t \epsilon_{t-j}] = 0$ para $j \neq 0$ e $E[\epsilon_t^2] = \sigma^2$ para $j = 0$. Portanto,
$$
E[\epsilon_t \epsilon_{t-j}] =
\begin{cases}
\sigma^2 & \text{se } j = 0 \\
0 & \text{se } j \neq 0
\end{cases}
$$

V. Portanto,
$$
\gamma_{jt} =
\begin{cases}
\sigma^2 & \text{se } j = 0 \\
0 & \text{se } j \neq 0
\end{cases}
$$
$\blacksquare$

Este resultado demonstra que, mesmo com uma m√©dia vari√°vel no tempo, a autocovari√¢ncia √© zero para todos os lags diferentes de zero, refletindo a independ√™ncia do ru√≠do branco [^1].

> üí° **Exemplo Num√©rico:** Suponha que $\sigma^2 = 4$. Isso significa que a vari√¢ncia do ru√≠do branco √© 4. Ent√£o, a autocovari√¢ncia $\gamma_{jt}$ ser√° 4 quando $j=0$ e 0 para todos os outros valores de $j$. Isso implica que cada observa√ß√£o $\epsilon_t$ √© independente das outras, embora a s√©rie $Y_t$ mostre uma tend√™ncia devido ao termo $\beta t$.

**Lema 1:** Se $Y_t = \mu_t + \epsilon_t$ e $\epsilon_t$ √© ru√≠do branco com vari√¢ncia $\sigma^2$, ent√£o $\gamma_{jt} = E[\epsilon_t \epsilon_{t-j}]$.

*Prova:*
I. Dado $Y_t = \mu_t + \epsilon_t$.
II. $\gamma_{jt} = E[(Y_t - \mu_t)(Y_{t-j} - \mu_{t-j})] = E[(\mu_t + \epsilon_t - \mu_t)(\mu_{t-j} + \epsilon_{t-j} - \mu_{t-j})] = E[\epsilon_t \epsilon_{t-j}]$.
$\blacksquare$

**Lema 1.1:** Se $Y_t = \mu_t + \epsilon_t$ e $\epsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, ent√£o $Cov(Y_t, Y_{t-j}) = E[\epsilon_t \epsilon_{t-j}]$.

*Prova:*
I. Dado $Y_t = \mu_t + \epsilon_t$ e $E[\epsilon_t] = 0$.
II. $Cov(Y_t, Y_{t-j}) = E[(Y_t - E[Y_t])(Y_{t-j} - E[Y_{t-j}])] = E[(Y_t - \mu_t)(Y_{t-j} - \mu_{t-j})] = E[(\mu_t + \epsilon_t - \mu_t)(\mu_{t-j} + \epsilon_{t-j} - \mu_{t-j})] = E[\epsilon_t \epsilon_{t-j}]$.
$\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que $Y_t = \mu_t + \epsilon_t$, onde $\mu_t = 0.1t$ e $\epsilon_t$ √© ru√≠do branco com vari√¢ncia $\sigma^2 = 1$. Vamos calcular a covari√¢ncia entre $Y_5$ e $Y_7$. Como $\epsilon_t$ √© ru√≠do branco, a covari√¢ncia entre $\epsilon_5$ e $\epsilon_7$ √© 0. Portanto, $Cov(Y_5, Y_7) = E[\epsilon_5 \epsilon_7] = 0$. No entanto, $E[Y_5] = 0.1 \times 5 = 0.5$ e $E[Y_7] = 0.1 \times 7 = 0.7$.

> üí° **Exemplo Num√©rico:** Suponha que $\beta = 0.5$ e $\epsilon_t$ √© um ru√≠do branco Gaussiano com vari√¢ncia $\sigma^2 = 1$. Ent√£o, $Y_t = 0.5t + \epsilon_t$. Para calcular a autocovari√¢ncia amostral, precisamos remover a tend√™ncia de tempo dos dados antes de calcular a covari√¢ncia. Como vimos no exemplo pr√°tico do cap√≠tulo anterior sobre autocovari√¢ncia, este resultado √© zero.

Para ilustrar como o tipo de m√©dia incondicional pode influenciar a autocovari√¢ncia, considere um processo com uma m√©dia sazonal.

> üí° **Defini√ß√£o:** Uma *s√©rie temporal sazonal* √© aquela que exibe padr√µes que se repetem ao longo de per√≠odos fixos.

**Exemplo:** Suponha que temos uma s√©rie trimestral $Y_t$ com a seguinte m√©dia sazonal:

$$
\mu_t =
\begin{cases}
1 & \text{se } t \text{ mod } 4 = 1 \\
2 & \text{se } t \text{ mod } 4 = 2 \\
3 & \text{se } t \text{ mod } 4 = 3 \\
4 & \text{se } t \text{ mod } 4 = 0
\end{cases}
$$

Nesse caso, a m√©dia incondicional varia ao longo dos trimestres do ano. Se $Y_t = \mu_t + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, ent√£o a autocovari√¢ncia $\gamma_{jt}$ ser√° afetada pela estrutura sazonal da m√©dia [^2].

**Teorema 1:**
Para o processo sazonal $Y_t = \mu_t + \epsilon_t$ definido acima, a autocovari√¢ncia √©:

$$
\gamma_{jt} = E[(Y_t - \mu_t)(Y_{t-j} - \mu_{t-j})] = E[\epsilon_t \epsilon_{t-j}] =
\begin{cases}
\sigma^2 & \text{se } j = 0 \\
0 & \text{se } j \neq 0
\end{cases}
$$
*Prova:*
I. Dado $Y_t = \mu_t + \epsilon_t$.
II.  $E[Y_t] = \mu_t$, que varia sazonalmente.
III. $\gamma_{jt} = E[(Y_t - E[Y_t])(Y_{t-j} - E[Y_{t-j}])] = E[(\mu_t + \epsilon_t - \mu_t)(\mu_{t-j} + \epsilon_{t-j} - \mu_{t-j})] = E[\epsilon_t \epsilon_{t-j}]$.
IV. Devido ao ru√≠do branco $\epsilon_t$, a autocovari√¢ncia √© $\sigma^2$ se $j=0$ e 0 se $j \neq 0$.
$\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que $\sigma^2 = 2$. Ent√£o, a autocovari√¢ncia no lag 0 √© $\gamma_{0t} = 2$, indicando a vari√¢ncia do ru√≠do. A autocovari√¢ncia em qualquer outro lag $j \neq 0$ √© 0, mostrando que os res√≠duos s√£o independentes.

**Teorema 1.1:** Para o processo sazonal $Y_t = \mu_t + \epsilon_t$ definido acima, a fun√ß√£o de autocorrela√ß√£o (ACF) √©:

$$
\rho_{jt} = \frac{\gamma_{jt}}{\gamma_{0t}} =
\begin{cases}
1 & \text{se } j = 0 \\
0 & \text{se } j \neq 0
\end{cases}
$$

*Prova:*
I. A fun√ß√£o de autocorrela√ß√£o √© definida como $\rho_{jt} = \frac{\gamma_{jt}}{\gamma_{0t}}$.
II. Do Teorema 1, $\gamma_{jt} = \begin{cases} \sigma^2 & \text{se } j = 0 \\ 0 & \text{se } j \neq 0 \end{cases}$.
III. Portanto, $\rho_{jt} = \frac{\gamma_{jt}}{\gamma_{0t}} = \begin{cases} \frac{\sigma^2}{\sigma^2} = 1 & \text{se } j = 0 \\ \frac{0}{\sigma^2} = 0 & \text{se } j \neq 0 \end{cases}$.
$\blacksquare$

> üí° **Exemplo Num√©rico:** Se $\sigma^2 = 2$, ent√£o $\rho_{0t} = \frac{2}{2} = 1$, mostrando perfeita autocorrela√ß√£o no lag 0. Para qualquer $j \neq 0$, $\rho_{jt} = \frac{0}{2} = 0$, indicando aus√™ncia de autocorrela√ß√£o em outros lags.

**Teorema 2:**
Para o processo sazonal $Y_t = \mu_t + \epsilon_t$, a s√©rie $Z_t = Y_t - \mu_t$ √© estacion√°ria.

*Prova:*
I. $E[Z_t] = E[Y_t - \mu_t] = E[Y_t] - \mu_t = \mu_t - \mu_t = 0$.
II. A autocovari√¢ncia de $Z_t$ √©:
$$
E[Z_t Z_{t-j}] = E[(Y_t - \mu_t)(Y_{t-j} - \mu_{t-j})] = E[\epsilon_t \epsilon_{t-j}]
$$
Que depende somente de j e n√£o de t.
$\blacksquare$

A s√©rie $Z_t$, ao remover a m√©dia sazonal, torna-se estacion√°ria.

> üí° **Exemplo Num√©rico:** Considere $Y_t$ com a m√©dia sazonal trimestral definida anteriormente e $\epsilon_t$ com vari√¢ncia 1. Se $Y_5 = 2 + \epsilon_5$, ent√£o $\mu_5 = 2$ e $Z_5 = Y_5 - \mu_5 = \epsilon_5$. Portanto, a s√©rie $Z_t$ √© apenas o ru√≠do branco, que √© estacion√°rio.

**Teorema 2.1:** Se $Y_t = \mu_t + \epsilon_t$ e $\epsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia constante $\sigma^2$, ent√£o a vari√¢ncia de $Z_t = Y_t - \mu_t$ √© $\sigma^2$ para todo $t$.

*Prova:*
I.  Dado $Z_t = Y_t - \mu_t = \epsilon_t$.
II.  A vari√¢ncia de $Z_t$ √© $Var(Z_t) = E[(Z_t - E[Z_t])^2] = E[(Z_t - 0)^2] = E[Z_t^2] = E[\epsilon_t^2] = \sigma^2$.
III. Como $\sigma^2$ √© constante, a vari√¢ncia de $Z_t$ √© constante para todo $t$.
$\blacksquare$

> üí° **Exemplo Num√©rico:** Se $\sigma^2 = 3$, ent√£o $Var(Z_t) = 3$ para qualquer $t$. Isso significa que a variabilidade de $Z_t$ em torno de sua m√©dia (zero) √© constante ao longo do tempo.

**Teorema 2.2:** Se $Y_t = \mu_t + \epsilon_t$ e $\epsilon_t$ √© um processo estacion√°rio com m√©dia zero e autocovari√¢ncia $\gamma_j$, ent√£o a autocovari√¢ncia de $Z_t = Y_t - \mu_t$ √© $\gamma_j$.

*Prova:*
I. Dado $Z_t = Y_t - \mu_t = \epsilon_t$.
II. A autocovari√¢ncia de $Z_t$ √© $E[Z_t Z_{t-j}] = E[(Y_t - \mu_t)(Y_{t-j} - \mu_{t-j})] = E[\epsilon_t \epsilon_{t-j}] = \gamma_j$.
$\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que $\epsilon_t$ √© um processo AR(1) estacion√°rio com autocovari√¢ncia $\gamma_j = 0.5^j$. Ent√£o, a autocovari√¢ncia de $Z_t$ tamb√©m ser√° $\gamma_j = 0.5^j$. Isso indica que a depend√™ncia temporal de $Z_t$ √© a mesma que a de $\epsilon_t$.

> üí° **Exemplo Num√©rico:** Vamos considerar o processo sazonal com m√©dias trimestrais 1, 2, 3, 4 e ru√≠do branco com vari√¢ncia 1.  Geramos dados de exemplo e calcularemos a autocovari√¢ncia amostral.
> ```python
> import numpy as np
>
> # N√∫mero de per√≠odos (anos)
> num_years = 10
> # Total de trimestres
> T = num_years * 4
>
> # Fun√ß√£o para gerar as m√©dias sazonais
> def seasonal_mean(t):
>     return (t % 4) + 1
>
> # Gerando as m√©dias sazonais para cada per√≠odo
> mu_t = np.array([seasonal_mean(i) for i in range(T)])
>
> # Gerando o ru√≠do branco
> np.random.seed(2)
> sigma_squared = 1
> epsilon_t = np.random.normal(0, np.sqrt(sigma_squared), T)
>
> # Gerando a s√©rie temporal Y_t
> Y_t = mu_t + epsilon_t
>
> # Fun√ß√£o para calcular a autocovari√¢ncia amostral
> def autocovariance(series, mu, lag):
>     n = len(series)
>     if lag >= n:
>         return 0
>     sum_term = 0
>     for i in range(lag, n):
>         sum_term += (series[i] - mu[i]) * (series[i - lag] - mu[i - lag])
>     return sum_term / (n - lag)
>
> # Calculando a autocovari√¢ncia amostral para lags 0, 1, e 4
> lags = [0, 1, 4]
> for lag in lags:
>     gamma_j = autocovariance(Y_t, mu_t, lag)
>     print(f"Autocovari√¢ncia amostral no lag {lag}: {gamma_j:.4f}")
> ```
>
> **Interpreta√ß√£o:**
>
> *   A autocovari√¢ncia no lag 0 aproxima-se da vari√¢ncia do ru√≠do branco (1), o que √© esperado.
> *   A autocovari√¢ncia no lag 1 √© pr√≥xima de zero, indicando baixa correla√ß√£o entre per√≠odos adjacentes ap√≥s remover as m√©dias sazonais.
> *   A autocovari√¢ncia no lag 4 √© pr√≥xima de zero, consistindo com a independ√™ncia entre os res√≠duos.

Este *insight* √© fundamental porque muitas t√©cnicas de an√°lise de s√©ries temporais, especialmente aquelas relacionadas √† previs√£o, assumem estacionariedade. Se uma s√©rie n√£o for estacion√°ria, frequentemente √© necess√°rio transform√°-la (por exemplo, removendo a tend√™ncia ou a sazonalidade) antes de aplicar essas t√©cnicas.

Al√©m disso, a escolha da m√©dia incondicional correta √© crucial para uma modelagem precisa. Se a forma funcional da m√©dia incondicional for incorreta, a autocovari√¢ncia amostral e outras estat√≠sticas podem fornecer informa√ß√µes enganosas sobre a estrutura de depend√™ncia temporal da s√©rie.

### Conclus√£o

A m√©dia incondicional $\mu_t$ oferece uma ferramenta flex√≠vel para modelar s√©ries temporais que exibem varia√ß√µes na m√©dia ao longo do tempo. Compreender como a m√©dia incondicional afeta a autocovari√¢ncia √© essencial para analisar a estrutura de depend√™ncia temporal de processos n√£o estacion√°rios e para escolher as transforma√ß√µes adequadas para tornar uma s√©rie estacion√°ria.

### Refer√™ncias
[^1]: Imagine a battery of I such computers generating sequences ... Note that this notation allows the general possibility that the mean can be a function of the date of the observation $t$. For the process [3.1.7] involving the time trend, the mean [3.1.8] is a function of time, whereas for the constant plus Gaussian white noise, the mean [3.1.6] is not a function of time.
[^2]: Given a particular realization such as $\{y_t^{(i)}\}_{t=-\infty}^{\infty}$ on a time series process... If neither the mean $\mu_t$ nor the autocovariances $\gamma_{jt}$ depend on the date $t$, then the process for $Y_t$ is said to be covariance-stationary or weakly stationary: $E(Y_t) = \mu$ for all $t$ and $E[(Y_t - \mu)(Y_{t-j} - \mu)] = \gamma_j$ for all $t$ and any $j$.
[^3]: Previous Topics: --- START The variance Œ≥_tt of a time series Y_t measures the dispersion or variability of the values of Y_t around its mean Œº_t. It is formally defined as the expected value of the squared deviation from the mean: Œ≥_tt = E[(Y_t - Œº_t)¬≤]. Mathematically, this is calculated by integrating the squared difference (y_t - Œº_t)^2 multiplied by the probability density function f_t(y_t) over all possible values of y_t. ---
<!-- END -->