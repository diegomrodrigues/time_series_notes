## Processos Autoregressivos de Ordem p (AR(p))
### Introdução
Em continuidade ao estudo de processos autorregressivos, vamos agora expandir o conceito para um processo de ordem $p$, denotado como AR($p$). Este modelo generaliza o AR(1) e AR(2) discutidos anteriormente, permitindo uma modelagem mais rica e flexível de dependências temporais em séries temporais. Veremos como as autocovariâncias e condições de estacionariedade se estendem para esse caso mais geral, conectando com os conceitos previamente estabelecidos de equações de diferença e suas soluções.

### Conceitos Fundamentais
Um processo autorregressivo de ordem $p$, denotado como AR($p$), é definido pela seguinte equação de diferença [^3.4.31]:
$$
Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \dots + \phi_p Y_{t-p} + \epsilon_t,
$$
onde:
- $Y_t$ é o valor da série temporal no instante $t$.
- $c$ é uma constante.
- $\phi_1, \phi_2, \dots, \phi_p$ são os coeficientes autorregressivos.
- $\epsilon_t$ é um ruído branco com média zero e variância $\sigma^2$, satisfazendo as condições [^3.2.1], [^3.2.2] e [^3.2.3].

Em outras palavras, o valor atual da série $Y_t$ é modelado como uma combinação linear ponderada de seus $p$ valores passados, juntamente com um termo de erro aleatório. Essa formulação permite que o modelo capture dependências temporais mais complexas do que os modelos AR(1) e AR(2).

A equação [^3.4.31] também pode ser expressa em termos do operador de defasagem $L$ como [^3.4.31]:
$$
(1 - \phi_1 L - \phi_2 L^2 - \dots - \phi_p L^p) Y_t = c + \epsilon_t.
$$
Esta notação compacta facilita a manipulação e análise do processo.

Para derivar a média do processo AR($p$), tomamos as expectativas de ambos os lados da equação [^3.4.31]:
$$
E(Y_t) = E(c) + \phi_1 E(Y_{t-1}) + \phi_2 E(Y_{t-2}) + \dots + \phi_p E(Y_{t-p}) + E(\epsilon_t).
$$
Assumindo que o processo é estacionário, temos $E(Y_t) = E(Y_{t-1}) = \dots = E(Y_{t-p}) = \mu$, e $E(\epsilon_t) = 0$. Portanto,
$$
\mu = c + \phi_1 \mu + \phi_2 \mu + \dots + \phi_p \mu,
$$
que pode ser reescrito como [^3.4.34]:
$$
\mu = \frac{c}{1 - \phi_1 - \phi_2 - \dots - \phi_p}.
$$
Para derivar as autocovariâncias, primeiro, subtraímos a média $\mu$ de ambos os lados de [^3.4.31] para obter [^3.4.35]:
$$
Y_t - \mu = \phi_1 (Y_{t-1} - \mu) + \phi_2 (Y_{t-2} - \mu) + \dots + \phi_p (Y_{t-p} - \mu) + \epsilon_t.
$$
Multiplicando ambos os lados por $(Y_{t-j} - \mu)$ e tomando a expectativa, encontramos que as autocovariâncias $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$ seguem a seguinte equação de diferença para $j \geq 1$ [^3.4.36]:
$$
\gamma_j = \phi_1 \gamma_{j-1} + \phi_2 \gamma_{j-2} + \dots + \phi_p \gamma_{j-p} ,
$$
onde $\gamma_0 = Var(Y_t)$. Além disso, para $j=0$,
$$
\gamma_0 = \phi_1 \gamma_1 + \phi_2 \gamma_2 + \dots + \phi_p \gamma_p + \sigma^2
$$
Notavelmente, as autocovariâncias $\gamma_j$ também seguem uma equação de diferença de ordem $p$, assim como o próprio processo AR($p$). Essa propriedade permite que encontremos expressões analíticas para as autocovariâncias resolvendo esse sistema de equações.

As **equações de Yule-Walker** [^3.4.37] são derivadas da equação anterior, dividindo por $\gamma_0$:
$$
\rho_j = \phi_1 \rho_{j-1} + \phi_2 \rho_{j-2} + \dots + \phi_p \rho_{j-p},
$$
onde $\rho_j = \frac{\gamma_j}{\gamma_0}$ são as autocorrelações. Essa forma das equações é frequentemente usada para estimação de parâmetros.

Para garantir que um processo AR($p$) seja **estacionário**, as raízes do polinômio autorregressivo [^3.4.32]:
$$
1 - \phi_1 z - \phi_2 z^2 - \dots - \phi_p z^p = 0
$$
devem estar *fora do círculo unitário* [^3.4.32]. Essa condição é crucial, pois garante que o processo tenha momentos finitos (média, variância, autocovariâncias) que não dependem do tempo, o que é um requisito para estacionariedade. Quando as raízes estão dentro do círculo unitário, o processo não é estacionário, e suas propriedades temporais podem divergir ao longo do tempo.

Em termos da representação MA($\infty$), o processo AR($p$) pode ser escrito como [^3.4.33]:
$$
Y_t = \mu + \psi(L) \epsilon_t,
$$
onde $\psi(L) = \frac{1}{1 - \phi_1 L - \phi_2 L^2 - \dots - \phi_p L^p}$ e $\sum_{j=0}^\infty |\psi_j| < \infty$. A condição de estacionariedade garante que essa representação MA($\infty$) seja bem definida.

O polinômio autoregressivo pode ser fatorado de acordo com [^3.4.38], resultando em:
$$
\gamma_j = g_1 \lambda_1^j + g_2 \lambda_2^j + \dots + g_p \lambda_p^j,
$$
onde $\lambda_i$ são as raízes do polinômio, e $g_i$ são coeficientes determinados pelas condições iniciais. Essas raízes (autovalores) fornecem uma forma para descrever o comportamento das autocovariâncias através do tempo.

### Conclusão
Este capítulo estendeu os conceitos de processos autorregressivos para o caso geral AR($p$). Vimos como as autocovariâncias seguem uma equação de diferença de ordem $p$, como as equações de Yule-Walker podem ser utilizadas e como a condição de estacionariedade relacionada às raízes do polinômio autorregressivo é essencial para garantir a validade e aplicabilidade do modelo. O entendimento das propriedades do AR($p$) é fundamental para a modelagem e análise de dados de séries temporais complexas. A capacidade de resolver as equações de diferença e entender as condições de estacionariedade permitem uma análise mais detalhada e precisa de processos autorregressivos e fornece uma base para o estudo de modelos mais complexos, como o ARMA.

### Referências
[^3.2.1]:  ... *A média de ε é zero*
[^3.2.2]:  ... *A variância de ε é σ²*
[^3.2.3]: ... *ε são não correlacionados*
[^3.4.31]: ... *Equação que define o AR(p)*
[^3.4.32]: ... *Equação que define as condições de estacionariedade do processo AR(p)*
[^3.4.33]: ... *Representação MA(∞) de um AR(p)*
[^3.4.34]: ... *Equação para o cálculo da média de um processo AR(p)*
[^3.4.35]: ... *Reescrita da equação AR(p) com relação a média*
[^3.4.36]: ... *Equação de diferença das autocovariâncias do AR(p)*
[^3.4.37]: ... *Equações de Yule-Walker*
[^3.4.38]: ... *Autocovariâncias em função das raízes do polinômio*
<!-- END -->
