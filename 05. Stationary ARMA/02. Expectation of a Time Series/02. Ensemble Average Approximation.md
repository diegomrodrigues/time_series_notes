## Aproxima√ß√£o Computacional da Expectativa de S√©ries Temporais

### Introdu√ß√£o

Em continuidade ao estudo da expectativa $E(Y_t)$ de uma s√©rie temporal, este cap√≠tulo foca em m√©todos computacionais para aproximar essa expectativa. Em muitos cen√°rios pr√°ticos, a obten√ß√£o de uma express√£o anal√≠tica para $E(Y_t)$ √© invi√°vel devido √† complexidade do modelo ou √† falta de informa√ß√µes precisas sobre a distribui√ß√£o de probabilidade de $Y_t$. Nesses casos, t√©cnicas computacionais fornecem uma alternativa valiosa para estimar a expectativa, aproveitando a interpreta√ß√£o da expectativa como o limite de probabilidade da m√©dia do conjunto (ensemble average) [^44].

### Aproxima√ß√£o via M√©dia do Conjunto (Ensemble Average)

Como vimos anteriormente, a expectativa $E(Y_t)$ pode ser interpretada como o limite de probabilidade da m√©dia do conjunto [^44]:

$$ E(Y_t) = \text{plim}_{I \to \infty} \frac{1}{I} \sum_{i=1}^{I} Y_t^{(i)} $$

onde $Y_t^{(i)}$ representa a *t*-√©sima observa√ß√£o da *i*-√©sima realiza√ß√£o da s√©rie temporal, e *I* √© o n√∫mero de realiza√ß√µes. Computacionalmente, essa express√£o sugere o seguinte procedimento [^44]:

1.  **Gerar *I* realiza√ß√µes independentes da s√©rie temporal:** Simular o processo estoc√°stico subjacente *I* vezes, obtendo *I* diferentes trajet√≥rias da s√©rie temporal.

2.  **Para cada instante de tempo *t*, calcular a m√©dia das *I* observa√ß√µes correspondentes:** Calcular a m√©dia dos valores de $Y_t^{(i)}$ para todos os *i* de 1 a *I*.

3.  **Aproximar $E(Y_t)$ pela m√©dia do conjunto:** Usar a m√©dia calculada no passo 2 como uma aproxima√ß√£o para $E(Y_t)$.

A precis√£o dessa aproxima√ß√£o aumenta com o n√∫mero de realiza√ß√µes *I*. Em outras palavras, quanto maior o valor de *I*, mais pr√≥xima a m√©dia do conjunto estar√° do valor verdadeiro da expectativa $E(Y_t)$.

> üí° **Observa√ß√£o:**
>
> √â importante notar que essa abordagem se baseia na ergodicidade do processo. A ergodicidade garante que a m√©dia do conjunto converge para a m√©dia temporal (m√©dia calculada ao longo de uma √∫nica realiza√ß√£o muito longa) √† medida que o n√∫mero de realiza√ß√µes *I* tende ao infinito [^46]. Se o processo n√£o for erg√≥dico, a m√©dia do conjunto pode n√£o ser uma boa aproxima√ß√£o da expectativa para uma √∫nica realiza√ß√£o.

**Exemplo Num√©rico:**

Suponha que queiramos estimar a expectativa de um processo AR(1) dado por $Y_t = 0.5Y_{t-1} + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco Gaussiano com m√©dia 0 e vari√¢ncia 1, e $Y_0 = 0$. Podemos aproximar $E(Y_t)$ para *t* = 10 usando a m√©dia do conjunto.

1.  **Gerar *I* = 1000 realiza√ß√µes independentes do processo AR(1):**
    ```python
    import numpy as np
    import matplotlib.pyplot as plt

    def ar1_simulation(phi, sigma, T, seed=None):
        """Simula um processo AR(1)."""
        if seed is not None:
            np.random.seed(seed)
        epsilon = np.random.normal(0, sigma, T)
        Y = np.zeros(T)
        for t in range(1, T):
            Y[t] = phi * Y[t-1] + epsilon[t]
        return Y

    I = 1000
    T = 50  # Simular para 50 per√≠odos
    phi = 0.5
    sigma = 1

    realizations = np.zeros((I, T))
    for i in range(I):
        realizations[i, :] = ar1_simulation(phi, sigma, T)

    # Plotar algumas realiza√ß√µes para visualiza√ß√£o
    plt.figure(figsize=(10, 6))
    for i in range(min(I, 5)):  # Plota no m√°ximo 5 realiza√ß√µes
        plt.plot(realizations[i, :], label=f'Realiza√ß√£o {i+1}')
    plt.xlabel('Tempo (t)')
    plt.ylabel('Y_t')
    plt.title('Simula√ß√µes do Processo AR(1)')
    plt.legend()
    plt.grid(True)
    plt.show()
    ```

2.  **Calcular a m√©dia das *I* observa√ß√µes para *t* = 10:**
    ```python
    ensemble_average = np.mean(realizations[:, 9]) # Python indexa de 0, ent√£o t=10 √© o √≠ndice 9
    print(f"M√©dia do conjunto para t=10: {ensemble_average}")

    # Calcular a m√©dia do conjunto para todos os tempos e plotar
    mean_over_time = np.mean(realizations, axis=0)

    plt.figure(figsize=(10, 6))
    plt.plot(mean_over_time, label='M√©dia do Conjunto ao Longo do Tempo')
    plt.xlabel('Tempo (t)')
    plt.ylabel('E[Y_t]')
    plt.title('Aproxima√ß√£o da Expectativa E[Y_t] ao Longo do Tempo')
    plt.legend()
    plt.grid(True)
    plt.show()

    ```

    Esse c√≥digo calcular√° a m√©dia do conjunto para *t* = 10, fornecendo uma aproxima√ß√£o da expectativa $E(Y_{10})$. O segundo bloco calcula a m√©dia do conjunto para todos os tempos *t* at√© T, e plota essa m√©dia ao longo do tempo, permitindo visualizar como a expectativa evolui.  No nosso exemplo, como o processo AR(1) √© estacion√°rio e tem m√©dia 0 (j√° que a m√©dia do ru√≠do branco √© 0), esperamos que a m√©dia do conjunto convirja para perto de 0 √† medida que *t* aumenta.

3.  **An√°lise da converg√™ncia:**

    Para analisar a converg√™ncia, podemos plotar a m√©dia do conjunto em fun√ß√£o do n√∫mero de realiza√ß√µes *I* para um tempo fixo *t*:

    ```python
    num_realizations = np.arange(10, I + 1, 10)  # Avaliar com 10, 20, ..., 1000 realiza√ß√µes
    ensemble_averages = []

    for num in num_realizations:
        ensemble_averages.append(np.mean(realizations[:num, 9])) # t=10

    plt.figure(figsize=(10, 6))
    plt.plot(num_realizations, ensemble_averages, marker='o')
    plt.xlabel('N√∫mero de Realiza√ß√µes (I)')
    plt.ylabel('M√©dia do Conjunto para t=10')
    plt.title('Converg√™ncia da M√©dia do Conjunto em Fun√ß√£o do N√∫mero de Realiza√ß√µes')
    plt.grid(True)
    plt.show()
    ```

    Este gr√°fico mostra como a m√©dia do conjunto se estabiliza √† medida que aumentamos o n√∫mero de realiza√ß√µes, ilustrando a converg√™ncia para a expectativa te√≥rica.

**Considera√ß√µes Computacionais:**

Embora conceitualmente simples, a aproxima√ß√£o via m√©dia do conjunto pode ser computacionalmente intensiva, especialmente para modelos complexos ou s√©ries temporais longas. Os seguintes fatores influenciam a complexidade computacional:

*   **Complexidade do modelo:** Simular modelos complexos, como modelos n√£o lineares ou com depend√™ncia de mem√≥ria longa, pode exigir um tempo de computa√ß√£o significativo.

*   **Tamanho da s√©rie temporal:** Calcular a m√©dia do conjunto para s√©ries temporais longas exige armazenar e processar um grande volume de dados.

*   **N√∫mero de realiza√ß√µes (*I*):** A precis√£o da aproxima√ß√£o aumenta com *I*, mas o custo computacional tamb√©m aumenta linearmente com *I*.

**T√©cnicas de Otimiza√ß√£o:**

Para mitigar a complexidade computacional, diversas t√©cnicas de otimiza√ß√£o podem ser empregadas:

*   **Paraleliza√ß√£o:** A gera√ß√£o das *I* realiza√ß√µes √© uma tarefa inerentemente paralela. Podemos distribuir a simula√ß√£o entre v√°rios processadores ou m√°quinas para reduzir o tempo total de computa√ß√£o.

*   **Amostragem por import√¢ncia:** Em vez de gerar *I* realiza√ß√µes aleat√≥rias, podemos usar t√©cnicas de amostragem por import√¢ncia para concentrar os esfor√ßos computacionais em regi√µes do espa√ßo de par√¢metros que s√£o mais relevantes para a estimativa da expectativa.

*   **M√©todos de Monte Carlo:** Para algumas s√©ries temporais com ru√≠dos n√£o gaussianos, ou depend√™ncia n√£o linear, M√©todos de Monte Carlo Markov Chain (MCMC) pode ajudar a criar uma estimativa melhor da integral.

*   **Redu√ß√£o de Vari√¢ncia:** Para algumas s√©ries temporais, t√©cnicas de redu√ß√£o de vari√¢ncia podem ser aplicadas para reduzir a vari√¢ncia do estimador da m√©dia do conjunto, permitindo uma estimativa mais precisa com um n√∫mero menor de realiza√ß√µes.

> üí° **Exemplo Num√©rico: Vari√°veis de Controle**
>
> Vamos usar o mesmo processo AR(1) do exemplo anterior, $Y_t = 0.5Y_{t-1} + \epsilon_t$, e definir uma vari√°vel de controle $Z_t = \epsilon_t$. Sabemos que $E[Z_t] = 0$. Podemos usar $Z_t$ para reduzir a vari√¢ncia da nossa estimativa de $E[Y_t]$.
>
> 1.  **Gerar *I* realiza√ß√µes de $Y_t$ e $Z_t$:**
>
> ```python
> import numpy as np
>
> def ar1_simulation_with_control(phi, sigma, T, seed=None):
>     """Simula um processo AR(1) com a vari√°vel de controle (ru√≠do branco)."""
>     if seed is not None:
>         np.random.seed(seed)
>     epsilon = np.random.normal(0, sigma, T)
>     Y = np.zeros(T)
>     for t in range(1, T):
>         Y[t] = phi * Y[t-1] + epsilon[t]
>     return Y, epsilon
>
> I = 1000
> T = 50
> phi = 0.5
> sigma = 1
>
> Y_realizations = np.zeros((I, T))
> Z_realizations = np.zeros((I, T))
>
> for i in range(I):
>     Y_realizations[i, :], Z_realizations[i, :] = ar1_simulation_with_control(phi, sigma, T)
> ```
>
> 2.  **Calcular as m√©dias do conjunto de $Y_t$ e $Z_t$:**
>
> ```python
> Y_mean = np.mean(Y_realizations[:, T-1]) # t=50
> Z_mean = np.mean(Z_realizations[:, T-1]) # t=50
> print(f"M√©dia do conjunto de Y_t: {Y_mean}")
> print(f"M√©dia do conjunto de Z_t: {Z_mean}")
> ```
>
> 3.  **Estimar o coeficiente *b*:**
>
> ```python
> cov_YZ = np.cov(Y_realizations[:, T-1], Z_realizations[:, T-1])[0, 1]
> var_Z = np.var(Z_realizations[:, T-1])
> b_optimal = cov_YZ / var_Z
> print(f"Coeficiente b √≥timo: {b_optimal}")
> ```
>
> 4.  **Calcular a estimativa da vari√°vel de controle:**
>
> ```python
> mu_Z = 0  # Expectativa conhecida de Z_t
> Y_cv = Y_mean - b_optimal * (Z_mean - mu_Z)
> print(f"Estimativa da m√©dia com vari√°vel de controle: {Y_cv}")
> ```
>
> 5.  **Comparar a vari√¢ncia das estimativas:**
>
> ```python
> var_Y = np.var(Y_realizations[:, T-1])
> Y_cv_realizations = Y_realizations[:, T-1] - b_optimal * (Z_realizations[:, T-1] - mu_Z)
> var_Y_cv = np.var(Y_cv_realizations)
> print(f"Vari√¢ncia da estimativa de Y_t: {var_Y}")
> print(f"Vari√¢ncia da estimativa de Y_t com vari√°vel de controle: {var_Y_cv}")
>
> # Verificar se a vari√¢ncia foi reduzida
> if var_Y_cv < var_Y:
>     print("A vari√¢ncia foi reduzida com a t√©cnica de vari√°vel de controle.")
> else:
>     print("A vari√¢ncia n√£o foi reduzida com a t√©cnica de vari√°vel de controle.")
> ```
>
> Este exemplo demonstra como usar a vari√°vel de controle $Z_t$ para obter uma estimativa mais precisa de $E[Y_t]$. A compara√ß√£o das vari√¢ncias mostra que a t√©cnica de vari√°vel de controle pode reduzir a vari√¢ncia da estimativa, tornando-a mais confi√°vel.
>
> A intui√ß√£o por tr√°s desse exemplo √© que, como $Y_t$ e $Z_t$ est√£o correlacionadas, podemos usar a informa√ß√£o sobre a m√©dia conhecida de $Z_t$ para ajustar a estimativa da m√©dia de $Y_t$, reduzindo assim o erro aleat√≥rio.

Uma t√©cnica de redu√ß√£o de vari√¢ncia comum √© o uso de *vari√°veis de controle*. Se conhecermos a expectativa de uma s√©rie temporal $Z_t$ que est√° correlacionada com $Y_t$, podemos usar $Z_t$ para reduzir a vari√¢ncia da nossa estimativa de $E(Y_t)$. Formalmente, seja $\hat{\mu}_Y = \frac{1}{I}\sum_{i=1}^I Y_t^{(i)}$ a estimativa da m√©dia do conjunto de $Y_t$ e $\hat{\mu}_Z = \frac{1}{I}\sum_{i=1}^I Z_t^{(i)}$ a estimativa da m√©dia do conjunto de $Z_t$. Suponha que conhecemos $E[Z_t] = \mu_Z$. Ent√£o, o estimador de vari√°vel de controle √© dado por:

$$ \hat{\mu}_{Y,cv} = \hat{\mu}_Y - b(\hat{\mu}_Z - \mu_Z),$$

onde $b$ √© um coeficiente escolhido para minimizar a vari√¢ncia de $\hat{\mu}_{Y,cv}$. O valor √≥timo de $b$ √© dado por:

$$ b^* = \frac{Cov(\hat{\mu}_Y, \hat{\mu}_Z)}{Var(\hat{\mu}_Z)} $$

Na pr√°tica, $b^*$ pode ser estimado usando as realiza√ß√µes das s√©ries temporais $Y_t$ e $Z_t$.

Outra t√©cnica de redu√ß√£o de vari√¢ncia √© o uso de *amostragem estratificada*. A amostragem estratificada divide o espa√ßo amostral em subgrupos (estratos) e, em seguida, amostra aleatoriamente dentro de cada estrato. Isso garante que todos os subgrupos sejam representados na amostra, o que pode reduzir a vari√¢ncia da estimativa. Por exemplo, se estivermos simulando um modelo financeiro, podemos estratificar as simula√ß√µes com base nas condi√ß√µes iniciais do mercado (alta volatilidade, baixa volatilidade, etc.).

**Teorema 2:** Seja $Y_t$ uma s√©rie temporal e $X_t$ uma vari√°vel auxiliar. Se $Y_t$ e $X_t$ forem positivamente correlacionadas, ent√£o a amostragem estratificada com base nos valores de $X_t$ pode reduzir a vari√¢ncia da estimativa da m√©dia de $Y_t$ em compara√ß√£o com a amostragem aleat√≥ria simples.

*Prova:* A prova deste teorema envolve mostrar que a vari√¢ncia da estimativa da m√©dia sob amostragem estratificada √© menor ou igual √† vari√¢ncia sob amostragem aleat√≥ria simples, dado que $Y_t$ e $X_t$ s√£o positivamente correlacionadas. Uma prova formal pode ser encontrada em textos padr√£o sobre t√©cnicas de Monte Carlo. A intui√ß√£o √© que a estratifica√ß√£o garante uma representa√ß√£o mais equilibrada de diferentes regi√µes do espa√ßo amostral, reduzindo a variabilidade da estimativa.

### Aplica√ß√µes

A aproxima√ß√£o computacional da expectativa de s√©ries temporais √© amplamente utilizada em diversas √°reas, incluindo:

*   **Finan√ßas:** Estimar o valor esperado de portf√≥lios de ativos, precificar derivativos complexos e avaliar o risco de investimentos.

*   **Engenharia:** Projetar e otimizar sistemas de controle, prever a demanda de energia e analisar a confiabilidade de equipamentos.

*   **Meteorologia e Climatologia:** Prever o tempo, simular modelos clim√°ticos e analisar a variabilidade clim√°tica.

*   **Economia:** Modelar o comportamento de vari√°veis macroecon√¥micas, prever o crescimento econ√¥mico e avaliar o impacto de pol√≠ticas governamentais.

**Teorema 1:** Seja $Y_t$ uma s√©rie temporal estacion√°ria e erg√≥dica com m√©dia $E[Y_t] = \mu_Y$. Seja $\hat{\mu}_Y = \frac{1}{I}\sum_{i=1}^I Y_t^{(i)}$ a estimativa da m√©dia do conjunto de $Y_t$. Ent√£o, $\hat{\mu}_Y$ converge em probabilidade para $\mu_Y$ quando $I \to \infty$.

*Prova:*
I.  Afirmamos que $\hat{\mu}_Y$ converge em probabilidade para $\mu_Y$ quando $I \to \infty$.

II. Pela defini√ß√£o de estacionaridade, $E[Y_t] = \mu_Y$ para todo $t$.
III. Pela defini√ß√£o de ergodicidade, a m√©dia do conjunto converge para a m√©dia populacional √† medida que o n√∫mero de realiza√ß√µes tende ao infinito.
IV. Aplicando a Lei Fraca dos Grandes N√∫meros (LFGN): Dado um conjunto de vari√°veis aleat√≥rias independentes e identicamente distribu√≠das (i.i.d.) $Y_t^{(i)}$ com m√©dia $\mu_Y$ e vari√¢ncia finita $\sigma^2$, a m√©dia amostral $\hat{\mu}_Y$ converge em probabilidade para $\mu_Y$ quando o n√∫mero de amostras $I$ tende ao infinito. Matematicamente, para qualquer $\epsilon > 0$:

$$ \lim_{I \to \infty} P(|\hat{\mu}_Y - \mu_Y| > \epsilon) = 0 $$

V. Portanto, $\hat{\mu}_Y$ converge em probabilidade para $\mu_Y$ quando $I \to \infty$. ‚ñ†

**Teorema 1.1:** Seja $\hat{\mu}_{Y,cv}$ o estimador de vari√°vel de controle definido acima. Se $Cov(\hat{\mu}_Y, \hat{\mu}_Z) \neq 0$, ent√£o $Var(\hat{\mu}_{Y,cv}) < Var(\hat{\mu}_Y)$ quando $b = b^*$.

*Prova:*
I. Queremos provar que $Var(\hat{\mu}_{Y,cv}) < Var(\hat{\mu}_Y)$ quando $b = b^*$, dado que $Cov(\hat{\mu}_Y, \hat{\mu}_Z) \neq 0$.
II. A vari√¢ncia do estimador de vari√°vel de controle √©:

$$ Var(\hat{\mu}_{Y,cv}) = Var(\hat{\mu}_Y - b(\hat{\mu}_Z - \mu_Z)) = Var(\hat{\mu}_Y - b\hat{\mu}_Z + b\mu_Z) $$

III. Como $\mu_Z$ √© uma constante, $Var(b\mu_Z) = 0$. Portanto:

$$ Var(\hat{\mu}_{Y,cv}) = Var(\hat{\mu}_Y - b\hat{\mu}_Z) = Var(\hat{\mu}_Y) + b^2 Var(\hat{\mu}_Z) - 2b Cov(\hat{\mu}_Y, \hat{\mu}_Z). $$

IV. Substituindo $b = b^* = \frac{Cov(\hat{\mu}_Y, \hat{\mu}_Z)}{Var(\hat{\mu}_Z)}$, obtemos:

$$ Var(\hat{\mu}_{Y,cv}) = Var(\hat{\mu}_Y) + \left(\frac{Cov(\hat{\mu}_Y, \hat{\mu}_Z)}{Var(\hat{\mu}_Z)}\right)^2 Var(\hat{\mu}_Z) - 2 \frac{Cov(\hat{\mu}_Y, \hat{\mu}_Z)}{Var(\hat{\mu}_Z)} Cov(\hat{\mu}_Y, \hat{\mu}_Z) $$
$$ Var(\hat{\mu}_{Y,cv}) = Var(\hat{\mu}_Y) + \frac{Cov(\hat{\mu}_Y, \hat{\mu}_Z)^2}{Var(\hat{\mu}_Z)} - 2 \frac{Cov(\hat{\mu}_Y, \hat{\mu}_Z)^2}{Var(\hat{\mu}_Z)} $$
$$ Var(\hat{\mu}_{Y,cv}) = Var(\hat{\mu}_Y) - \frac{Cov(\hat{\mu}_Y, \hat{\mu}_Z)^2}{Var(\hat{\mu}_Z)}. $$

V. Como $Cov(\hat{\mu}_Y, \hat{\mu}_Z) \neq 0$, temos $\frac{Cov(\hat{\mu}_Y, \hat{\mu}_Z)^2}{Var(\hat{\mu}_Z)} > 0$.
VI. Portanto, $Var(\hat{\mu}_{Y,cv}) < Var(\hat{\mu}_Y)$. ‚ñ†

### Conclus√£o

A aproxima√ß√£o computacional da expectativa $E(Y_t)$ atrav√©s da m√©dia do conjunto √© uma ferramenta poderosa para analisar s√©ries temporais quando solu√ß√µes anal√≠ticas s√£o intrat√°veis [^44]. Embora possa ser computacionalmente exigente, a precis√£o da aproxima√ß√£o pode ser controlada pelo n√∫mero de realiza√ß√µes, e diversas t√©cnicas de otimiza√ß√£o podem ser empregadas para reduzir o custo computacional. Em particular, t√©cnicas de redu√ß√£o de vari√¢ncia, como o uso de vari√°veis de controle e amostragem estratificada, podem melhorar significativamente a precis√£o da estimativa com um dado n√∫mero de realiza√ß√µes. Essa abordagem permite obter *insights* valiosos sobre o comportamento de s√©ries temporais em uma ampla gama de aplica√ß√µes [^44].

### Refer√™ncias
[^44]: P√°gina 44 do texto original.
[^46]: P√°gina 47 do texto original.
<!-- END -->