## Expectativa Anal√≠tica de Modelos Espec√≠ficos de S√©ries Temporais

### Introdu√ß√£o

Em continuidade √† discuss√£o sobre a aproxima√ß√£o computacional da expectativa de s√©ries temporais, este cap√≠tulo explora o c√°lculo anal√≠tico da expectativa para modelos espec√≠ficos. Em muitos casos, especialmente para modelos lineares com ru√≠do branco, a expectativa pode ser derivada de forma expl√≠cita, fornecendo uma compreens√£o mais profunda do comportamento do modelo e permitindo implementa√ß√µes mais eficientes [^44]. Focaremos em dois exemplos principais: um processo constante mais ru√≠do branco Gaussiano e um processo com tend√™ncia de tempo mais ru√≠do branco Gaussiano, detalhando as deriva√ß√µes e implica√ß√µes de cada caso [^44].

### Expectativa Anal√≠tica para Modelos Espec√≠ficos

#### 1. Processo Constante Mais Ru√≠do Branco Gaussiano

Considere um processo definido por [^44]:

$$ Y_t = \mu + \epsilon_t $$

onde $\mu$ √© uma constante e $\epsilon_t$ √© ru√≠do branco Gaussiano com m√©dia 0 e vari√¢ncia $\sigma^2$ [^44]. Para calcular a expectativa $E(Y_t)$, aplicamos a propriedade da linearidade da expectativa [^44]:

$$ E(Y_t) = E(\mu + \epsilon_t) = E(\mu) + E(\epsilon_t) $$

Como $\mu$ √© uma constante, $E(\mu) = \mu$. Al√©m disso, por defini√ß√£o, o ru√≠do branco tem m√©dia zero, ou seja, $E(\epsilon_t) = 0$ [^44]. Portanto:

$$ E(Y_t) = \mu + 0 = \mu $$

Assim, a expectativa do processo √© simplesmente a constante $\mu$, independente do tempo *t* [^44].

> üí° **Interpreta√ß√£o:**
>
> Este resultado indica que a m√©dia da s√©rie temporal permanece constante ao longo do tempo e √© igual ao valor da constante $\mu$. O ru√≠do branco $\epsilon_t$ causa flutua√ß√µes aleat√≥rias em torno dessa m√©dia, mas n√£o afeta o valor esperado da s√©rie.

**Exemplo Num√©rico:**

Se $\mu = 10$ e $\epsilon_t$ √© ru√≠do branco Gaussiano com m√©dia 0 e vari√¢ncia 1, ent√£o $E(Y_t) = 10$ para todo *t*.

```python
import numpy as np
import matplotlib.pyplot as plt

# Par√¢metros
mu = 10
T = 100
sigma = 1

# Gera ru√≠do branco
epsilon = np.random.normal(0, sigma, T)

# Gera a s√©rie temporal
Y = mu + epsilon

# Calcula a m√©dia amostral
mean_sample = np.mean(Y)

print(f"Expectativa te√≥rica: {mu}")
print(f"M√©dia amostral: {mean_sample}")

# Plota a s√©rie temporal
plt.plot(Y, label='Y_t')
plt.axhline(y=mu, color='r', linestyle='-', label='E(Y_t) = mu')
plt.xlabel('Tempo (t)')
plt.ylabel('Valor')
plt.title('S√©rie Temporal: Constante + Ru√≠do Branco Gaussiano')
plt.legend()
plt.grid(True)
plt.show()
```

> üí° **Exemplo Num√©rico:**
>
> Considere $\mu = 5$ e $\epsilon_t$ com $\sigma^2 = 4$. Simulemos 5 pontos no tempo e calculemos a m√©dia amostral para verificar a converg√™ncia para a expectativa te√≥rica.
>
> ```python
> import numpy as np
>
> # Par√¢metros
> mu = 5
> sigma = 2  # Desvio padr√£o (raiz quadrada da vari√¢ncia)
>
> # Simula 5 pontos no tempo
> epsilon = np.random.normal(0, sigma, 5)
> Y = mu + epsilon
>
> # Calcula a m√©dia amostral
> mean_sample = np.mean(Y)
>
> print(f"Expectativa te√≥rica: {mu}")
> print(f"S√©rie temporal simulada: {Y}")
> print(f"M√©dia amostral: {mean_sample}")
> ```
>
> **Sa√≠da Exemplo:**
>
> ```
> Expectativa te√≥rica: 5
> S√©rie temporal simulada: [6.32876737 6.74916157 3.8375728  5.61926591 5.29173219]
> M√©dia amostral: 5.565300967970307
> ```
>
> **An√°lise:**
>
> A m√©dia amostral (5.565) est√° pr√≥xima da expectativa te√≥rica (5), como esperado. A discrep√¢ncia se deve √† variabilidade do ru√≠do branco e ao pequeno tamanho da amostra. Com um n√∫mero maior de simula√ß√µes, a m√©dia amostral convergir√° para a expectativa te√≥rica de 5.

#### 2. Processo com Tend√™ncia de Tempo Mais Ru√≠do Branco Gaussiano

Agora, considere um processo definido por [^44]:

$$ Y_t = \beta t + \epsilon_t $$

onde $\beta$ √© uma constante representando a inclina√ß√£o da tend√™ncia de tempo e $\epsilon_t$ √© ru√≠do branco Gaussiano com m√©dia 0 e vari√¢ncia $\sigma^2$ [^44]. Novamente, aplicamos a linearidade da expectativa:

$$ E(Y_t) = E(\beta t + \epsilon_t) = E(\beta t) + E(\epsilon_t) $$

Como $\beta$ √© uma constante e *t* √© determin√≠stico (tempo), $E(\beta t) = \beta t$. E, como antes, $E(\epsilon_t) = 0$. Portanto:

$$ E(Y_t) = \beta t + 0 = \beta t $$

Nesse caso, a expectativa √© uma fun√ß√£o linear do tempo *t*, indicando uma tend√™ncia linear na m√©dia da s√©rie temporal [^44].

> üí° **Interpreta√ß√£o:**
>
> A m√©dia da s√©rie temporal aumenta (ou diminui, se $\beta$ for negativo) linearmente com o tempo. A inclina√ß√£o da tend√™ncia √© dada por $\beta$, representando a taxa de varia√ß√£o da m√©dia ao longo do tempo.

**Exemplo Num√©rico:**

Se $\beta = 0.5$ e $\epsilon_t$ √© ru√≠do branco Gaussiano com m√©dia 0 e vari√¢ncia 1, ent√£o $E(Y_t) = 0.5t$. Para *t* = 10, $E(Y_{10}) = 5$, e para *t* = 20, $E(Y_{20}) = 10$.

```python
import numpy as np
import matplotlib.pyplot as plt

# Par√¢metros
beta = 0.5
T = 100
sigma = 1

# Gera ru√≠do branco
epsilon = np.random.normal(0, sigma, T)

# Gera a s√©rie temporal
t = np.arange(1, T + 1)
Y = beta * t + epsilon

# Calcula a m√©dia te√≥rica
mean_theoretical = beta * t

# Calcula a m√©dia amostral
mean_sample = np.mean(Y)

print(f"Expectativa te√≥rica para t=1: {beta * 1}")
print(f"Expectativa te√≥rica para t={T}: {beta * T}")
print(f"M√©dia amostral: {mean_sample}")

# Plota a s√©rie temporal e a m√©dia te√≥rica
plt.plot(Y, label='Y_t')
plt.plot(mean_theoretical, color='r', linestyle='-', label='E(Y_t) = beta * t')
plt.xlabel('Tempo (t)')
plt.ylabel('Valor')
plt.title('S√©rie Temporal: Tend√™ncia de Tempo + Ru√≠do Branco Gaussiano')
plt.legend()
plt.grid(True)
plt.show()
```

> üí° **Exemplo Num√©rico:**
>
> Seja $\beta = -0.2$ e $\epsilon_t$ com $\sigma^2 = 0.5$. Calculemos a expectativa para $t = 5$ e $t = 10$, e simulemos uma s√©rie temporal para comparar com os valores te√≥ricos.
>
> **C√°lculos Te√≥ricos:**
>
> *   $E(Y_5) = \beta \cdot 5 = -0.2 \cdot 5 = -1$
> *   $E(Y_{10}) = \beta \cdot 10 = -0.2 \cdot 10 = -2$
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> beta = -0.2
> sigma = np.sqrt(0.5)
> T = 20
>
> # Simula ru√≠do branco
> epsilon = np.random.normal(0, sigma, T)
>
> # Calcula a s√©rie temporal
> t = np.arange(1, T + 1)
> Y = beta * t + epsilon
>
> # Calcula a expectativa te√≥rica
> expected_values = beta * t
>
> # Plot
> plt.plot(t, Y, label="Y_t (Simulado)")
> plt.plot(t, expected_values, label="E[Y_t] (Te√≥rico)", color='red')
> plt.xlabel("Tempo (t)")
> plt.ylabel("Valor")
> plt.title("S√©rie Temporal com Tend√™ncia de Tempo")
> plt.legend()
> plt.grid(True)
> plt.show()
>
> print(f"Expectativa te√≥rica em t=5: {beta * 5}")
> print(f"Expectativa te√≥rica em t=10: {beta * 10}")
> print(f"Valor simulado em t=5: {Y[4]}")
> print(f"Valor simulado em t=10: {Y[9]}")
> ```
>
> **An√°lise:**
>
> A expectativa te√≥rica decresce linearmente com o tempo, refletindo a tend√™ncia negativa. Os valores simulados flutuam em torno da linha de expectativa, devido ao ru√≠do branco.

#### 3. Processo Auto-Regressivo de Ordem 1 (AR(1))

Considere um processo AR(1) dado por:
$$Y_t = \phi Y_{t-1} + \epsilon_t$$
onde $|\phi| < 1$ para garantir a estacionariedade, e $\epsilon_t$ √© um ru√≠do branco com m√©dia zero. Tomando a expectativa de ambos os lados da equa√ß√£o, obtemos:
$$E(Y_t) = \phi E(Y_{t-1}) + E(\epsilon_t)$$
Sob a condi√ß√£o de estacionariedade, $E(Y_t) = E(Y_{t-1}) = \mu$. Portanto:
$$\mu = \phi \mu + 0$$
$$\mu (1 - \phi) = 0$$
Como $\phi \neq 1$, ent√£o $\mu = 0$. Assim, para um processo AR(1) estacion√°rio com ru√≠do branco de m√©dia zero, a m√©dia incondicional √© zero.

```python
import numpy as np
import matplotlib.pyplot as plt

# Par√¢metros
phi = 0.7
sigma = 1
T = 100

# Gera ru√≠do branco
epsilon = np.random.normal(0, sigma, T)

# Gera a s√©rie temporal
Y = np.zeros(T)
for t in range(1, T):
    Y[t] = phi * Y[t-1] + epsilon[t]

# Calcula a m√©dia amostral
mean_sample = np.mean(Y)

print(f"Expectativa te√≥rica: 0")
print(f"M√©dia amostral: {mean_sample}")

# Plota a s√©rie temporal
plt.plot(Y, label='Y_t')
plt.axhline(y=0, color='r', linestyle='-', label='E(Y_t) = 0')
plt.xlabel('Tempo (t)')
plt.ylabel('Valor')
plt.title('S√©rie Temporal AR(1)')
plt.legend()
plt.grid(True)
plt.show()
```

> üí° **Exemplo Num√©rico:**
>
> Seja $\phi = 0.5$ e $\epsilon_t \sim N(0, 1)$. Simularemos uma s√©rie temporal AR(1) e verificaremos se a m√©dia amostral se aproxima de zero.
>
> ```python
> import numpy as np
>
> # Par√¢metros
> phi = 0.5
> sigma = 1
> T = 1000
>
> # Ru√≠do branco
> epsilon = np.random.normal(0, sigma, T)
>
> # AR(1)
> Y = np.zeros(T)
> for t in range(1, T):
>  Y[t] = phi * Y[t-1] + epsilon[t]
>
> # M√©dia amostral
> mean_sample = np.mean(Y)
>
> print(f"M√©dia amostral: {mean_sample}")
> ```
>
> **An√°lise:**
>
> Executando o c√≥digo acima, a m√©dia amostral deve ser um valor pr√≥ximo de zero (e.g., -0.05), indicando que a m√©dia incondicional do processo AR(1) √© zero quando o ru√≠do branco tem m√©dia zero.

**3.1 Processo Auto-Regressivo de Ordem 1 (AR(1)) com m√©dia diferente de zero**

Suponha agora que o ru√≠do branco $\epsilon_t$ tem m√©dia $\mu_\epsilon \neq 0$.  Nesse caso, o processo AR(1) √© dado por:

$$Y_t = \phi Y_{t-1} + \epsilon_t$$

Tomando a expectativa de ambos os lados:

$$E(Y_t) = \phi E(Y_{t-1}) + E(\epsilon_t)$$

Sob a condi√ß√£o de estacionariedade, $E(Y_t) = E(Y_{t-1}) = \mu_Y$.  Portanto:

$$\mu_Y = \phi \mu_Y + \mu_\epsilon$$
$$\mu_Y (1 - \phi) = \mu_\epsilon$$
$$\mu_Y = \frac{\mu_\epsilon}{1 - \phi}$$

Assim, a m√©dia incondicional do processo AR(1) √© $\frac{\mu_\epsilon}{1 - \phi}$ quando o ru√≠do branco tem m√©dia $\mu_\epsilon$. Note que a condi√ß√£o $|\phi| < 1$ ainda √© necess√°ria para a estacionariedade e para que a m√©dia seja finita.

```python
import numpy as np
import matplotlib.pyplot as plt

# Par√¢metros
phi = 0.7
mu_epsilon = 2
sigma = 1
T = 100

# Gera ru√≠do branco com m√©dia mu_epsilon
epsilon = np.random.normal(mu_epsilon, sigma, T)

# Gera a s√©rie temporal
Y = np.zeros(T)
for t in range(1, T):
    Y[t] = phi * Y[t-1] + epsilon[t]

# Calcula a m√©dia te√≥rica
mean_theoretical = mu_epsilon / (1 - phi)

# Calcula a m√©dia amostral
mean_sample = np.mean(Y)

print(f"Expectativa te√≥rica: {mean_theoretical}")
print(f"M√©dia amostral: {mean_sample}")

# Plota a s√©rie temporal
plt.plot(Y, label='Y_t')
plt.axhline(y=mean_theoretical, color='r', linestyle='-', label='E(Y_t)')
plt.xlabel('Tempo (t)')
plt.ylabel('Valor')
plt.title('S√©rie Temporal AR(1) com ru√≠do de m√©dia n√£o-nula')
plt.legend()
plt.grid(True)
plt.show()
```

> üí° **Exemplo Num√©rico:**
>
> Seja $\phi = 0.8$ e $\mu_\epsilon = 1$.  Ent√£o, a m√©dia te√≥rica √© $\mu_Y = \frac{1}{1 - 0.8} = 5$.  Simularemos a s√©rie temporal e confirmaremos.
>
> ```python
> import numpy as np
>
> # Par√¢metros
> phi = 0.8
> mu_epsilon = 1
> sigma = 1
> T = 1000
>
> # Ru√≠do branco com m√©dia
> epsilon = np.random.normal(mu_epsilon, sigma, T)
>
> # AR(1)
> Y = np.zeros(T)
> for t in range(1, T):
>  Y[t] = phi * Y[t-1] + epsilon[t]
>
> # M√©dia amostral
> mean_sample = np.mean(Y)
>
> print(f"M√©dia te√≥rica: 5")
> print(f"M√©dia amostral: {mean_sample}")
> ```
>
> **An√°lise:**
>
> A m√©dia amostral (ex: 4.9) √© pr√≥xima √† m√©dia te√≥rica de 5. A diferen√ßa ocorre devido ao tamanho da amostra e √† vari√¢ncia do ru√≠do.

**Lema 1:**

Seja $Y_t$ uma s√©rie temporal definida por $Y_t = a + bX_t$, onde $a$ e $b$ s√£o constantes e $X_t$ √© uma s√©rie temporal com expectativa conhecida $E(X_t)$. Ent√£o, $E(Y_t) = a + bE(X_t)$.

*Proof:* Este lema √© uma consequ√™ncia direta da linearidade da expectativa, j√° abordada anteriormente no Teorema 1 do cap√≠tulo anterior. A prova segue os mesmos passos, aplicando a propriedade de linearidade aos termos da equa√ß√£o $Y_t = a + bX_t$, resultando em $E(Y_t) = E(a) + E(bX_t) = a + bE(X_t)$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Seja $a=2$, $b=3$, e $X_t$ uma s√©rie temporal com $E[X_t] = 4$. Ent√£o, $E[Y_t] = 2 + 3 * 4 = 14$.

**Lema 2:**
Para uma s√©rie temporal $Y_t = \sum_{i=1}^n X_{t,i}$, onde cada $X_{t,i}$ √© uma s√©rie temporal com expectativa conhecida $E[X_{t,i}]$, ent√£o
$$E[Y_t] = \sum_{i=1}^n E[X_{t,i}]$$

*Proof:*
A prova decorre diretamente da aplica√ß√£o repetida da propriedade da linearidade da expectativa.
Come√ßamos com
$$Y_t = \sum_{i=1}^n X_{t,i} = X_{t,1} + X_{t,2} + \ldots + X_{t,n}$$
Aplicamos a expectativa em ambos os lados:
$$E[Y_t] = E[X_{t,1} + X_{t,2} + \ldots + X_{t,n}]$$
Pela propriedade da linearidade da expectativa (aplicada repetidamente):
$$E[Y_t] = E[X_{t,1}] + E[X_{t,2}] + \ldots + E[X_{t,n}]$$
$$E[Y_t] = \sum_{i=1}^n E[X_{t,i}]$$

I.  Come√ßamos com a s√©rie temporal expressa como uma soma: $Y_t = \sum_{i=1}^n X_{t,i}$.
II. Aplicamos a expectativa em ambos os lados: $E[Y_t] = E[\sum_{i=1}^n X_{t,i}]$.
III. Pela propriedade da linearidade da expectativa, a expectativa de uma soma √© a soma das expectativas: $E[Y_t] = \sum_{i=1}^n E[X_{t,i}]$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Seja $Y_t = X_{t,1} + X_{t,2} + X_{t,3}$, onde $E[X_{t,1}] = 1$, $E[X_{t,2}] = 2$, e $E[X_{t,3}] = 3$. Ent√£o, $E[Y_t] = 1 + 2 + 3 = 6$.

#### Implica√ß√µes e Aplica√ß√µes

O c√°lculo anal√≠tico da expectativa √© fundamental para:

*   **Valida√ß√£o de Modelos:** Comparar a expectativa te√≥rica com a m√©dia amostral de dados simulados ou reais permite validar a implementa√ß√£o e calibra√ß√£o do modelo. Se houver uma discrep√¢ncia significativa, isso pode indicar erros no modelo ou na estima√ß√£o de par√¢metros.
*   **Implementa√ß√£o Eficiente:** Conhecer a express√£o anal√≠tica da expectativa permite calcular essa medida de forma direta e eficiente, sem a necessidade de simula√ß√µes computacionais intensivas.
*   **Previs√£o:** A expectativa √© um componente crucial em algoritmos de previs√£o, especialmente em modelos n√£o estacion√°rios, onde a m√©dia varia com o tempo. Compreender como a m√©dia evolui permite construir previs√µes mais precisas.
*   **Compreens√£o do Modelo:** A express√£o anal√≠tica da expectativa fornece *insights* valiosos sobre o comportamento do modelo e a influ√™ncia de diferentes par√¢metros.

**Teorema 1:** (Expectativa de uma Soma Ponderada de Processos)

Sejam $Y_1, Y_2, \ldots, Y_n$ processos estoc√°sticos com expectativas conhecidas $E[Y_i]$, para $i = 1, \ldots, n$. Seja $Y$ um processo definido como uma combina√ß√£o linear desses processos:
$$Y = \sum_{i=1}^n a_i Y_i$$
onde $a_i$ s√£o constantes. Ent√£o, a expectativa de $Y$ √© dada por:
$$E[Y] = \sum_{i=1}^n a_i E[Y_i]$$

*Proof:*
Aplicando a propriedade da linearidade da expectativa, temos:
$$E[Y] = E\left[\sum_{i=1}^n a_i Y_i\right] = \sum_{i=1}^n E[a_i Y_i] = \sum_{i=1}^n a_i E[Y_i]$$

I. Come√ßamos com a defini√ß√£o de $Y$ como uma soma ponderada de processos: $Y = \sum_{i=1}^n a_i Y_i$.
II. Aplicamos o operador de expectativa em ambos os lados: $E[Y] = E[\sum_{i=1}^n a_i Y_i]$.
III. Usamos a propriedade da linearidade da expectativa para distribuir a expectativa sobre a soma: $E[Y] = \sum_{i=1}^n E[a_i Y_i]$.
IV. Como $a_i$ s√£o constantes, podemos retir√°-las da expectativa: $E[Y] = \sum_{i=1}^n a_i E[Y_i]$. $\blacksquare$

A prova decorre diretamente da aplica√ß√£o das propriedades da linearidade da expectativa. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Sejam $Y_1$ e $Y_2$ dois processos com $E[Y_1] = 2$ e $E[Y_2] = 3$. Considere $Y = 0.5Y_1 + 0.5Y_2$. Ent√£o, $E[Y] = 0.5 * 2 + 0.5 * 3 = 1 + 1.5 = 2.5$.
>
> ```python
> import numpy as np
>
> # Par√¢metros
> E_Y1 = 2
> E_Y2 = 3
> a1 = 0.5
> a2 = 0.5
>
> # Calcula a expectativa
> E_Y = a1 * E_Y1 + a2 * E_Y2
>
> print(f"Expectativa de Y: {E_Y}")
> ```

### Conclus√£o

O c√°lculo anal√≠tico da expectativa para modelos espec√≠ficos de s√©ries temporais fornece uma ferramenta poderosa para an√°lise, valida√ß√£o e implementa√ß√£o eficiente de modelos [^44]. Para processos simples, como a soma de uma constante com ru√≠do branco ou um processo com tend√™ncia linear, a expectativa pode ser derivada de forma expl√≠cita, permitindo uma compreens√£o mais profunda do comportamento da s√©rie. A capacidade de calcular a expectativa analiticamente tamb√©m facilita a valida√ß√£o de modelos computacionais e a implementa√ß√£o de algoritmos de previs√£o mais eficientes [^44].

### Refer√™ncias
[^44]: P√°gina 44 do texto original.
<!-- END -->