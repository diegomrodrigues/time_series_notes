## Expectation of a Time Series

### IntroduÃ§Ã£o
Este capÃ­tulo aborda em detalhes a **expectativa $E(Y_t)$** de uma sÃ©rie temporal, explorando sua definiÃ§Ã£o, interpretaÃ§Ã£o e relevÃ¢ncia no contexto de processos estocÃ¡sticos estacionÃ¡rios e nÃ£o estacionÃ¡rios. A expectativa, tambÃ©m conhecida como **mÃ©dia incondicional** $\mu_t$, fornece uma medida central da sÃ©rie temporal em um determinado ponto no tempo e desempenha um papel fundamental na caracterizaÃ§Ã£o do comportamento da sÃ©rie [^44].

### Conceitos Fundamentais

A **expectativa $E(Y_t)$** de uma sÃ©rie temporal representa o valor mÃ©dio da variÃ¡vel aleatÃ³ria $Y_t$ no instante *t*, assumindo que sua distribuiÃ§Ã£o de probabilidade exista [^44]. Formalmente, a expectativa Ã© definida como a mÃ©dia da distribuiÃ§Ã£o de probabilidade da *t*-Ã©sima observaÃ§Ã£o, calculada pela integraÃ§Ã£o do produto dos valores possÃ­veis de $Y_t$ e sua funÃ§Ã£o de densidade de probabilidade $f_t(y_t)$ [^44]:

$$ E(Y_t) = \int_{-\infty}^{\infty} y_t f_t(y_t) dy_t $$

onde:

*   $Y_t$ representa a variÃ¡vel aleatÃ³ria no instante *t*.
*   $f_t(y_t)$ Ã© a funÃ§Ã£o de densidade de probabilidade de $Y_t$.
*   $y_t$ representa os valores possÃ­veis da variÃ¡vel aleatÃ³ria $Y_t$.

A expectativa $E(Y_t)$ tambÃ©m Ã© conhecida como a **mÃ©dia incondicional** $\mu_t$ [^44]. Ã‰ importante notar que $\mu_t$ pode ser uma funÃ§Ã£o do tempo *t*, dependendo do processo estocÃ¡stico subjacente [^44].

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que $Y_t$ seja uma variÃ¡vel aleatÃ³ria que representa a temperatura diÃ¡ria em uma cidade. Em um determinado dia *t*, a temperatura pode variar, e sua distribuiÃ§Ã£o de probabilidade $f_t(y_t)$ descreve essa variaÃ§Ã£o. Se a distribuiÃ§Ã£o $f_t(y_t)$ for centrada em 25 graus Celsius, a expectativa $E(Y_t)$ serÃ¡ aproximadamente 25 graus Celsius. Este valor representa a temperatura mÃ©dia esperada para esse dia.
>
> Para calcular isso numericamente, podemos discretizar a integral. Suponha que a temperatura possa assumir valores entre 20 e 30 graus Celsius, com incrementos de 1 grau. E suponha uma distribuiÃ§Ã£o discreta simplificada:
>
> | Temperatura (y_t) | Probabilidade (f_t(y_t)) |
> |--------------------|--------------------------|
> | 20                 | 0.05                     |
> | 21                 | 0.08                     |
> | 22                 | 0.10                     |
> | 23                 | 0.12                     |
> | 24                 | 0.15                     |
> | 25                 | 0.20                     |
> | 26                 | 0.15                     |
> | 27                 | 0.08                     |
> | 28                 | 0.05                     |
> | 29                 | 0.01                     |
> | 30                 | 0.01                     |
>
> EntÃ£o, a expectativa seria calculada como:
>
> $E(Y_t) = \sum_{i=1}^{11} y_{t_i} f_t(y_{t_i})$
>
> $E(Y_t) = (20 \cdot 0.05) + (21 \cdot 0.08) + (22 \cdot 0.10) + (23 \cdot 0.12) + (24 \cdot 0.15) + (25 \cdot 0.20) + (26 \cdot 0.15) + (27 \cdot 0.08) + (28 \cdot 0.05) + (29 \cdot 0.01) + (30 \cdot 0.01) = 24.37$
>
> Neste exemplo discreto, a temperatura mÃ©dia esperada Ã© 24.37 graus Celsius.

Uma interpretaÃ§Ã£o da expectativa $E(Y_t)$ Ã© como o **limite de probabilidade da mÃ©dia do conjunto (ensemble average)** [^44]:

$$ E(Y_t) = \text{plim}_{I \to \infty} (1/I) \sum_{i=1}^{I} Y_t^{(i)} $$

Aqui, imaginamos uma "bateria" de *I* computadores gerando sequÃªncias ${y_t^{(1)}, y_t^{(2)}, \dots, y_t^{(I)}}$ e selecionamos a observaÃ§Ã£o associada Ã  data *t* de cada sequÃªncia [^44]. Essa mÃ©dia do conjunto converge em probabilidade para a expectativa $E(Y_t)$ conforme o nÃºmero de realizaÃ§Ãµes *I* tende ao infinito [^44].

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere a simulaÃ§Ã£o de um processo estocÃ¡stico. Suponha que tenhamos 5 computadores (*I* = 5), cada um simulando a evoluÃ§Ã£o de um preÃ§o de aÃ§Ã£o. No dia *t* = 10, os preÃ§os simulados pelos computadores sÃ£o:
>
> $Y_{10}^{(1)} = 105, Y_{10}^{(2)} = 110, Y_{10}^{(3)} = 102, Y_{10}^{(4)} = 108, Y_{10}^{(5)} = 115$
>
> A mÃ©dia do conjunto neste instante *t* Ã©:
>
> $(1/5) \sum_{i=1}^{5} Y_{10}^{(i)} = (1/5) (105 + 110 + 102 + 108 + 115) = (1/5) (540) = 108$
>
> Se aumentarmos o nÃºmero de computadores para *I* = 1000 e calcularmos a mÃ©dia do conjunto, esse valor se aproximarÃ¡ da expectativa teÃ³rica $E(Y_{10})$. Por exemplo, se apÃ³s 1000 simulaÃ§Ãµes a mÃ©dia do conjunto for 107.5, podemos dizer que a expectativa $E(Y_{10})$ Ã© aproximadamente 107.5.
>
> ```python
> import numpy as np
>
> # SimulaÃ§Ã£o com I=5
> simulations_5 = np.array([105, 110, 102, 108, 115])
> ensemble_average_5 = np.mean(simulations_5)
> print(f"MÃ©dia do conjunto com I=5: {ensemble_average_5}")
>
> # SimulaÃ§Ã£o com I=1000 (valores hipotÃ©ticos)
> simulations_1000 = np.random.normal(loc=107.5, scale=5, size=1000) # Simula 1000 preÃ§os com mÃ©dia 107.5 e desvio padrÃ£o 5
> ensemble_average_1000 = np.mean(simulations_1000)
> print(f"MÃ©dia do conjunto com I=1000: {ensemble_average_1000}")
> ```

**ProposiÃ§Ã£o 1**

Seja $Y_t$ uma sÃ©rie temporal. Se a funÃ§Ã£o de densidade de probabilidade $f_t(y_t)$ Ã© simÃ©trica em torno de um ponto $c$ para todo $t$, entÃ£o $E(Y_t) = c$, desde que a expectativa exista.

*Proof:*
Como $f_t(y_t)$ Ã© simÃ©trica em torno de $c$, temos $f_t(c+x) = f_t(c-x)$ para todo $x$.  EntÃ£o
$$E(Y_t) = \int_{-\infty}^{\infty} y_t f_t(y_t) dy_t = \int_{-\infty}^{\infty} (y_t - c + c) f_t(y_t) dy_t = \int_{-\infty}^{\infty} (y_t - c) f_t(y_t) dy_t + c \int_{-\infty}^{\infty} f_t(y_t) dy_t$$
Substituindo $x = y_t - c$, temos $y_t = x + c$, e $dy_t = dx$. EntÃ£o
$$\int_{-\infty}^{\infty} x f_t(x+c) dx = \int_{-\infty}^{\infty} x f_t(c-x) dx$$
Usando a substituiÃ§Ã£o $u = -x$, temos $x = -u$ e $dx = -du$, e os limites de integraÃ§Ã£o se invertem:
$$\int_{\infty}^{-\infty} (-u) f_t(c+u) (-du) = - \int_{-\infty}^{\infty} u f_t(c+u) du = - \int_{-\infty}^{\infty} x f_t(x+c) dx$$
Portanto, $\int_{-\infty}^{\infty} x f_t(x+c) dx = 0$, e $E(Y_t) = c \int_{-\infty}^{\infty} f_t(y_t) dy_t = c$.

I.  Dado que $f_t(y_t)$ Ã© simÃ©trica em torno de $c$, entÃ£o $f_t(c+x) = f_t(c-x)$ para todo $x$.
II.  Podemos expressar a expectativa como: $E(Y_t) = \int_{-\infty}^{\infty} y_t f_t(y_t) dy_t$.
III. Reescrevemos $y_t$ como $(y_t - c) + c$, entÃ£o: $E(Y_t) = \int_{-\infty}^{\infty} [(y_t - c) + c] f_t(y_t) dy_t$.
IV. Separamos a integral em duas partes: $E(Y_t) = \int_{-\infty}^{\infty} (y_t - c) f_t(y_t) dy_t + \int_{-\infty}^{\infty} c f_t(y_t) dy_t$.
V.  Fazemos a substituiÃ§Ã£o $x = y_t - c$, entÃ£o $y_t = x + c$ e $dy_t = dx$.
VI.  A primeira integral se torna: $\int_{-\infty}^{\infty} x f_t(x+c) dx$.
VII. Devido Ã  simetria, mostramos que $\int_{-\infty}^{\infty} x f_t(x+c) dx = 0$.
VIII. A segunda integral se torna: $c \int_{-\infty}^{\infty} f_t(y_t) dy_t = c \cdot 1 = c$, pois a integral da funÃ§Ã£o densidade de probabilidade sobre todo o seu domÃ­nio Ã© 1.
IX. Portanto, $E(Y_t) = 0 + c = c$. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que $Y_t$ siga uma distribuiÃ§Ã£o normal com mÃ©dia $c$ e desvio padrÃ£o $\sigma$, ou seja, $Y_t \sim N(c, \sigma^2)$. A funÃ§Ã£o de densidade de probabilidade normal Ã© simÃ©trica em torno da mÃ©dia $c$. Portanto, de acordo com a proposiÃ§Ã£o, $E(Y_t) = c$.
>
> Por exemplo, se $Y_t \sim N(10, 4)$, entÃ£o $E(Y_t) = 10$. A distribuiÃ§Ã£o Ã© simÃ©trica em torno de 10, o que significa que os valores acima e abaixo de 10 tÃªm a mesma probabilidade de ocorrer.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy.stats import norm
>
> # ParÃ¢metros da distribuiÃ§Ã£o normal
> media = 10
> desvio_padrao = 2
>
> # Cria um intervalo de valores para x
> x = np.linspace(media - 4*desvio_padrao, media + 4*desvio_padrao, 100)
>
> # Calcula a funÃ§Ã£o de densidade de probabilidade (PDF)
> pdf = norm.pdf(x, media, desvio_padrao)
>
> # Plota a PDF
> plt.plot(x, pdf, label=f'N({media}, {desvio_padrao**2})')
> plt.xlabel('Y_t')
> plt.ylabel('Densidade de Probabilidade')
> plt.title('DistribuiÃ§Ã£o Normal SimÃ©trica')
> plt.legend()
> plt.grid(True)
> plt.show()
>
> # Simula valores e calcula a mÃ©dia amostral
> amostra = np.random.normal(media, desvio_padrao, 1000)
> media_amostral = np.mean(amostra)
> print(f"MÃ©dia Amostral (aproximaÃ§Ã£o de E(Y_t)): {media_amostral}")
> ```

**Exemplos:**

1.  Se a sÃ©rie temporal ${Y_t}$ representa a soma de uma constante $\mu$ e um processo de ruÃ­do branco Gaussiano ${\epsilon_t}$ [^44]:

    $$Y_t = \mu + \epsilon_t$$

    entÃ£o sua mÃ©dia Ã© [^44]:

    $$E(Y_t) = \mu + E(\epsilon_t) = \mu$$

    pois a expectativa do ruÃ­do branco Ã© zero [^44].

    *Proof:*
    I. Dado que $Y_t = \mu + \epsilon_t$.
    II. Aplicamos a expectativa em ambos os lados: $E(Y_t) = E(\mu + \epsilon_t)$.
    III. Pela linearidade da expectativa: $E(Y_t) = E(\mu) + E(\epsilon_t)$.
    IV. Como $\mu$ Ã© uma constante, $E(\mu) = \mu$.
    V. Dado que $\epsilon_t$ Ã© ruÃ­do branco, $E(\epsilon_t) = 0$.
    VI. Portanto, $E(Y_t) = \mu + 0 = \mu$. $\blacksquare$

    > ğŸ’¡ **Exemplo NumÃ©rico:**
    >
    > Seja $\mu = 5$ e $\epsilon_t$ um ruÃ­do branco com mÃ©dia 0 e variÃ¢ncia 1. EntÃ£o, $Y_t = 5 + \epsilon_t$. A sÃ©rie temporal $Y_t$ flutuarÃ¡ em torno de 5.
    >
    > Para qualquer instante *t*, $E(Y_t) = E(5 + \epsilon_t) = E(5) + E(\epsilon_t) = 5 + 0 = 5$.
    >
    > ```python
    > import numpy as np
    >
    > # ParÃ¢metros
    > mu = 5
    > num_pontos = 100
    >
    > # Gera ruÃ­do branco
    > ruido_branco = np.random.normal(0, 1, num_pontos)
    >
    > # Gera a sÃ©rie temporal
    > Y = mu + ruido_branco
    >
    > # Calcula a mÃ©dia amostral
    > media_amostral = np.mean(Y)
    >
    > print(f"MÃ©dia amostral de Y_t: {media_amostral}")
    > ```

2.  Se $Y_t$ representa uma tendÃªncia de tempo (time trend) mais um ruÃ­do branco Gaussiano ${\epsilon_t}$ [^44]:

    $$Y_t = \beta t + \epsilon_t$$

    entÃ£o sua mÃ©dia Ã© [^44]:

    $$E(Y_t) = \beta t$$

    Nesse caso, a mÃ©dia Ã© uma funÃ§Ã£o do tempo *t* [^44].

    *Proof:*
    I. Dado que $Y_t = \beta t + \epsilon_t$.
    II. Aplicamos a expectativa em ambos os lados: $E(Y_t) = E(\beta t + \epsilon_t)$.
    III. Pela linearidade da expectativa: $E(Y_t) = E(\beta t) + E(\epsilon_t)$.
    IV. Como $\beta$ Ã© uma constante e $t$ Ã© determinÃ­stico, $E(\beta t) = \beta t$.
    V. Dado que $\epsilon_t$ Ã© ruÃ­do branco, $E(\epsilon_t) = 0$.
    VI. Portanto, $E(Y_t) = \beta t + 0 = \beta t$. $\blacksquare$

    > ğŸ’¡ **Exemplo NumÃ©rico:**
    >
    > Seja $\beta = 0.5$ e $\epsilon_t$ um ruÃ­do branco com mÃ©dia 0 e variÃ¢ncia 1. EntÃ£o, $Y_t = 0.5t + \epsilon_t$.
    >
    > Para *t* = 10, $E(Y_{10}) = 0.5 \cdot 10 = 5$.
    > Para *t* = 20, $E(Y_{20}) = 0.5 \cdot 20 = 10$.
    >
    > A mÃ©dia da sÃ©rie temporal aumenta linearmente com o tempo.
    >
    > ```python
    > import numpy as np
    > import matplotlib.pyplot as plt
    >
    > # ParÃ¢metros
    > beta = 0.5
    > num_pontos = 100
    > tempo = np.arange(1, num_pontos + 1)
    >
    > # Gera ruÃ­do branco
    > ruido_branco = np.random.normal(0, 1, num_pontos)
    >
    > # Gera a sÃ©rie temporal
    > Y = beta * tempo + ruido_branco
    >
    > # Calcula a mÃ©dia teÃ³rica
    > media_teorica = beta * tempo
    >
    > # Plota a sÃ©rie temporal e a mÃ©dia teÃ³rica
    > plt.plot(tempo, Y, label='Y_t')
    > plt.plot(tempo, media_teorica, label='E(Y_t) = beta * t', color='red')
    > plt.xlabel('Tempo (t)')
    > plt.ylabel('Valor')
    > plt.title('SÃ©rie Temporal com TendÃªncia')
    > plt.legend()
    > plt.grid(True)
    > plt.show()
    > ```

3. Para complementar os exemplos, considere um processo Auto-Regressivo de ordem 1 (AR(1)) dado por:
    $$Y_t = \phi Y_{t-1} + \epsilon_t$$
    onde $|\phi| < 1$ para garantir a estacionariedade, e $\epsilon_t$ Ã© um ruÃ­do branco com mÃ©dia zero. Tomando a expectativa de ambos os lados da equaÃ§Ã£o, obtemos:
    $$E(Y_t) = \phi E(Y_{t-1}) + E(\epsilon_t)$$
    Sob a condiÃ§Ã£o de estacionariedade, $E(Y_t) = E(Y_{t-1}) = \mu$. Portanto:
    $$\mu = \phi \mu + 0$$
    $$\mu (1 - \phi) = 0$$
    Como $\phi \neq 1$, entÃ£o $\mu = 0$. Assim, para um processo AR(1) estacionÃ¡rio com ruÃ­do branco de mÃ©dia zero, a mÃ©dia incondicional Ã© zero.

    > ğŸ’¡ **Exemplo NumÃ©rico:**
    >
    > Seja $\phi = 0.7$ e $\epsilon_t$ um ruÃ­do branco com mÃ©dia 0 e variÃ¢ncia 1. EntÃ£o, $Y_t = 0.7 Y_{t-1} + \epsilon_t$.
    >
    > Para simular essa sÃ©rie temporal, precisamos de um valor inicial para $Y_0$. Vamos assumir $Y_0 = 0$.
    >
    > EntÃ£o, $E(Y_t) = 0$ para todo *t*, pois a sÃ©rie Ã© estacionÃ¡ria e o ruÃ­do branco tem mÃ©dia zero.
    >
    > ```python
    > import numpy as np
    > import matplotlib.pyplot as plt
    >
    > # ParÃ¢metros
    > phi = 0.7
    > num_pontos = 100
    >
    > # Gera ruÃ­do branco
    > ruido_branco = np.random.normal(0, 1, num_pontos)
    >
    > # Inicializa a sÃ©rie temporal
    > Y = np.zeros(num_pontos)
    > Y[0] = 0  # Valor inicial
    >
    > # Gera a sÃ©rie temporal AR(1)
    > for t in range(1, num_pontos):
    >     Y[t] = phi * Y[t-1] + ruido_branco[t]
    >
    > # Calcula a mÃ©dia amostral
    > media_amostral = np.mean(Y)
    >
    > print(f"MÃ©dia amostral de Y_t: {media_amostral}")
    >
    > # Plota a sÃ©rie temporal
    > plt.plot(Y, label='Y_t')
    > plt.xlabel('Tempo (t)')
    > plt.ylabel('Valor')
    > plt.title('SÃ©rie Temporal AR(1)')
    > plt.legend()
    > plt.grid(True)
    > plt.show()
    > ```

Ã‰ crucial observar que, enquanto a notaÃ§Ã£o $\mu_t$ permite a generalizaÃ§Ã£o de que a mÃ©dia pode variar ao longo do tempo, como demonstrado no processo com tendÃªncia temporal (time trend) [^44], em outros casos, como no processo constante mais ruÃ­do branco Gaussiano, a mÃ©dia permanece constante e independente do tempo [^44].

**Teorema 1**
Sejam $Y_t$ e $X_t$ duas sÃ©ries temporais, e $a$ e $b$ constantes. EntÃ£o,
$$E(aY_t + bX_t) = aE(Y_t) + bE(X_t)$$
*Proof:*
$$E(aY_t + bX_t) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (ay + bx) f_{Y_t, X_t}(y, x) dy dx$$
$$ = a \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y f_{Y_t, X_t}(y, x) dy dx + b \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x f_{Y_t, X_t}(y, x) dy dx$$
$$ = a \int_{-\infty}^{\infty} y f_{Y_t}(y) dy + b \int_{-\infty}^{\infty} x f_{X_t}(x) dx = aE(Y_t) + bE(X_t)$$

I.  ComeÃ§amos com a definiÃ§Ã£o de esperanÃ§a: $E(aY_t + bX_t) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (ay + bx) f_{Y_t, X_t}(y, x) dy dx$.
II. Pela propriedade da integral, separamos a integral dupla: $= a \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y f_{Y_t, X_t}(y, x) dy dx + b \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x f_{Y_t, X_t}(y, x) dy dx$.
III. Integramos sobre $x$ no primeiro termo e sobre $y$ no segundo termo: $= a \int_{-\infty}^{\infty} y f_{Y_t}(y) dy + b \int_{-\infty}^{\infty} x f_{X_t}(x) dx$.
IV. Reconhecemos as integrais como as esperanÃ§as de $Y_t$ e $X_t$: $= aE(Y_t) + bE(X_t)$. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Sejam $Y_t$ a temperatura diÃ¡ria em graus Celsius e $X_t$ a umidade relativa do ar. Suponha que $E(Y_t) = 25$ e $E(X_t) = 70$. Queremos calcular a expectativa de uma nova sÃ©rie temporal $Z_t = 2Y_t + 0.5X_t$.
>
> Usando o Teorema 1, temos:
>
> $E(Z_t) = E(2Y_t + 0.5X_t) = 2E(Y_t) + 0.5E(X_t) = 2 \cdot 25 + 0.5 \cdot 70 = 50 + 35 = 85$
>
> Portanto, $E(Z_t) = 85$.
>
> ```python
> # Dados
> EY = 25  # E(Y_t)
> EX = 70  # E(X_t)
> a = 2
> b = 0.5
>
> # Calcula E(Z_t)
> EZ = a * EY + b * EX
> print(f"E(Z_t) = {EZ}")
> ```

**Teorema 1.1**
Sejam $Y_t, X_t$ duas sÃ©ries temporais independentes e $g, h$ funÃ§Ãµes lineares. EntÃ£o,
$$E[g(Y_t)h(X_t)] = E[g(Y_t)]E[h(X_t)]$$
*Proof:*
Se $Y_t$ e $X_t$ sÃ£o independentes, entÃ£o $f_{Y_t, X_t}(y, x) = f_{Y_t}(y)f_{X_t}(x)$. Seja $g(Y_t)$ e $h(X_t)$ funÃ§Ãµes lineares de $Y_t$ e $X_t$ respectivamente.
$$E[g(Y_t)h(X_t)] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(y)h(x) f_{Y_t, X_t}(y, x) dy dx = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(y)h(x) f_{Y_t}(y)f_{X_t}(x) dy dx$$
$$ = \int_{-\infty}^{\infty} g(y) f_{Y_t}(y) dy \int_{-\infty}^{\infty} h(x)f_{X_t}(x) dx = E[g(Y_t)]E[h(X_t)]$$

I.  Dado que $Y_t$ e $X_t$ sÃ£o independentes, suas funÃ§Ãµes de densidade conjunta podem ser fatoradas: $f_{Y_t, X_t}(y, x) = f_{Y_t}(y)f_{X_t}(x)$.
II. A expectativa do produto de funÃ§Ãµes de variÃ¡veis independentes Ã© dada por: $E[g(Y_t)h(X_t)] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(y)h(x) f_{Y_t, X_t}(y, x) dy dx$.
III. Substituindo a funÃ§Ã£o de densidade conjunta: $E[g(Y_t)h(X_t)] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(y)h(x) f_{Y_t}(y)f_{X_t}(x) dy dx$.
IV. Separamos as integrais, pois as variÃ¡veis sÃ£o independentes: $E[g(Y_t)h(X_t)] = \int_{-\infty}^{\infty} g(y) f_{Y_t}(y) dy \int_{-\infty}^{\infty} h(x)f_{X_t}(x) dx$.
V. Reconhecemos cada integral como a expectativa de uma funÃ§Ã£o da variÃ¡vel correspondente: $E[g(Y_t)h(X_t)] = E[g(Y_t)]E[h(X_t)]$. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Sejam $Y_t$ o retorno de uma aÃ§Ã£o A e $X_t$ o retorno de uma aÃ§Ã£o B. Assume que $Y_t$ e $X_t$ sÃ£o independentes. Seja $g(Y_t) = 1 + Y_t$ e $h(X_t) = 1 + X_t$, representando o retorno bruto de cada aÃ§Ã£o (investimento inicial + retorno). Suponha que $E(Y_t) = 0.10$ (10%) e $E(X_t) = 0.05$ (5%).
>
> Queremos calcular a expectativa do produto dos retornos brutos: $E[(1 + Y_t)(1 + X_t)]$.
>
> Usando o Teorema 1.1, temos:
>
> $E[(1 + Y_t)(1 + X_t)] = E[1 + Y_t] \cdot E[1 + X_t] = (1 + E[Y_t]) \cdot (1 + E[X_t]) = (1 + 0.10) \cdot (1 + 0.05) = 1.10 \cdot 1.05 = 1.155$
>
> Portanto, a expectativa do produto dos retornos brutos Ã© 1.155, o que representa um retorno de 15.5% sobre o investimento combinado.
>
> ```python
> # Dados
> EY = 0.10
> EX = 0.05
>
> # Calcula E[(1 + Y_t)(1 + X_t)]
> EZ = (1 + EY) * (1 + EX)
> print(f"E[(1 + Y_t)(1 + X_t)] = {EZ}")
> ```

**Teorema 2**
Seja $Y_t$ uma sÃ©rie temporal estacionÃ¡ria. EntÃ£o $E(Y_t) = \mu$ para todo $t$, onde $\mu$ Ã© uma constante.
*Proof:*
Por definiÃ§Ã£o, uma sÃ©rie temporal estacionÃ¡ria tem uma distribuiÃ§Ã£o que nÃ£o muda com o tempo. Portanto, a funÃ§Ã£o de densidade de probabilidade $f_t(y_t)$ Ã© a mesma para todo $t$, ou seja, $f_t(y_t) = f(y)$ para alguma funÃ§Ã£o $f$. EntÃ£o
$$E(Y_t) = \int_{-\infty}^{\infty} y_t f_t(y_t) dy_t = \int_{-\infty}^{\infty} y f(y) dy = \mu$$
onde $\mu$ Ã© uma constante que nÃ£o depende de $t$.

I.  Por definiÃ§Ã£o, uma sÃ©rie temporal estacionÃ¡ria tem uma distribuiÃ§Ã£o que nÃ£o muda com o tempo.
II. Isso implica que a funÃ§Ã£o de densidade de probabilidade $f_t(y_t)$ Ã© a mesma para todo $t$. Podemos denotar essa funÃ§Ã£o por $f(y)$.
III. A expectativa de $Y_t$ Ã© dada por $E(Y_t) = \int_{-\infty}^{\infty} y_t f_t(y_t) dy_t$.
IV. Substituindo $f_t(y_t)$ por $f(y)$, temos $E(Y_t) = \int_{-\infty}^{\infty} y f(y) dy$.
V.  Como a integral $\int_{-\infty}^{\infty} y f(y) dy$ nÃ£o depende de $t$ e representa um valor constante, podemos denotÃ¡-la por $\mu$.
VI. Portanto, $E(Y_t) = \mu$ para todo $t$, onde $\mu$ Ã© uma constante. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere uma sÃ©rie temporal estacionÃ¡ria que representa a taxa de juros de curto prazo em um paÃ­s. Se a sÃ©rie for estacionÃ¡ria, isso significa que a distribuiÃ§Ã£o da taxa de juros nÃ£o muda com o tempo. Suponha que a mÃ©dia dessa taxa de juros seja de 2% ao ano.
>
> EntÃ£o, de acordo com o Teorema 2, a expectativa da taxa de juros em qualquer ponto no tempo *t* serÃ¡ sempre 2%, ou seja, $E(Y_t) = 0.02$ para todo *t*.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # ParÃ¢metros
> mu = 0.02  # MÃ©dia (2%)
> num_pontos = 100
>
> # Gera uma sÃ©rie temporal estacionÃ¡ria (ruÃ­do branco com mÃ©dia 0.02)
> Y = np.random.normal(mu, 0.01, num_pontos) # Desvio padrÃ£o de 0.01
>
> # Calcula a mÃ©dia amostral
> media_amostral = np.mean(Y)
>
> print(f"MÃ©dia amostral de Y_t: {media_amostral}")
>
> # Plota a sÃ©rie temporal
> plt.plot(Y, label='Taxa de Juros (Y_t)')
> plt.axhline(y=mu, color='r', linestyle='-', label='E(Y_t) = 0.02')  # Linha horizontal na mÃ©dia
> plt.xlabel('Tempo (t)')
> plt.ylabel('Taxa de Juros')
> plt.title('SÃ©rie Temporal EstacionÃ¡ria (Taxa de Juros)')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```

### ConclusÃ£o

A expectativa $E(Y_t)$, ou mÃ©dia incondicional $\mu_t$, Ã© uma medida fundamental que descreve o valor mÃ©dio de uma sÃ©rie temporal em um determinado ponto no tempo [^44]. Sua dependÃªncia ou independÃªncia do tempo *t* reflete as caracterÃ­sticas do processo estocÃ¡stico subjacente, sendo crucial para a anÃ¡lise e modelagem de sÃ©ries temporais [^44].

### ReferÃªncias
[^44]: PÃ¡gina 44 do texto original.
<!-- END -->