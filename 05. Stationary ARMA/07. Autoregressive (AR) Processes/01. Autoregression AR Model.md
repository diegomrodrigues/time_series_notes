## Autoregress√£o de Ordem *p*: AR(*p*)

### Introdu√ß√£o
Expandindo sobre os conceitos de **processos autorregressivos (AR)** introduzidos anteriormente, este cap√≠tulo aprofunda-se na an√°lise de um processo autorregressivo de ordem *p*, denotado como AR(*p*). Os processos AR(*p*) s√£o extens√µes naturais dos modelos AR(1) e AR(2), permitindo uma modelagem mais flex√≠vel e rica das depend√™ncias temporais em uma s√©rie temporal [^56, 57]. Este cap√≠tulo explora as propriedades, condi√ß√µes de estacionariedade e representa√ß√µes matem√°ticas de processos AR(*p*), construindo sobre a base j√° estabelecida para processos AR de ordem inferior.

### O Modelo AR(*p*)
Um processo autorregressivo de ordem *p*, denotado AR(*p*), satisfaz a seguinte equa√ß√£o [^58]:

$$Y_t = c + \phi_1Y_{t-1} + \phi_2Y_{t-2} + \dots + \phi_pY_{t-p} + \varepsilon_t$$

onde:
*   $Y_t$ representa o valor da s√©rie temporal no instante *t*.
*   $c$ √© uma constante.
*   $\phi_1, \phi_2, \dots, \phi_p$ s√£o os coeficientes autorregressivos.
*   $Y_{t-1}, Y_{t-2}, \dots, Y_{t-p}$ s√£o os *p* valores passados da s√©rie temporal.
*   $\varepsilon_t$ √© um termo de **ru√≠do branco** com m√©dia zero e vari√¢ncia $\sigma^2$ [^47]. Assume-se que o ru√≠do branco $\{\varepsilon_t\}$ satisfaz as condi√ß√µes de m√©dia zero ($E(\varepsilon_t) = 0$) [^47], vari√¢ncia constante ($E(\varepsilon_t^2) = \sigma^2$) [^47] e n√£o correla√ß√£o ($E(\varepsilon_t \varepsilon_\tau) = 0$ para $t \ne \tau$) [^48].

A equa√ß√£o acima expressa que o valor atual da s√©rie temporal √© uma combina√ß√£o linear dos seus *p* valores passados, acrescida de um termo de ru√≠do branco. O par√¢metro *p* define a ordem do modelo autorregressivo, indicando quantos per√≠odos passados influenciam o valor atual.

> üí° **Exemplo Num√©rico:** Considere um modelo AR(2) com $c = 5$, $\phi_1 = 0.6$, $\phi_2 = 0.2$, e $\varepsilon_t \sim N(0, 1)$. Ent√£o, $Y_t = 5 + 0.6Y_{t-1} + 0.2Y_{t-2} + \varepsilon_t$. Se $Y_{t-1} = 10$ e $Y_{t-2} = 12$, e $\varepsilon_t = 0.5$, ent√£o $Y_t = 5 + 6 + 2.4 + 0.5 = 13.9$.

**Observa√ß√£o:** Se $c = 0$, o processo AR(*p*) √© dito centrado. Caso contr√°rio, se $c \neq 0$, podemos recentralizar o processo subtraindo a m√©dia incondicional $\mu = E[Y_t]$ de ambos os lados da equa√ß√£o.

**Prova:** Vamos mostrar como recentralizar o processo AR(*p*) quando $c \neq 0$.

I. Considere o processo AR(*p*) dado por:
   $$Y_t = c + \phi_1Y_{t-1} + \phi_2Y_{t-2} + \dots + \phi_pY_{t-p} + \varepsilon_t$$

II. Assumindo que o processo √© estacion√°rio, a m√©dia incondicional $\mu = E[Y_t]$ √© constante ao longo do tempo.  Tomando o valor esperado de ambos os lados da equa√ß√£o:
   $$E[Y_t] = E[c + \phi_1Y_{t-1} + \phi_2Y_{t-2} + \dots + \phi_pY_{t-p} + \varepsilon_t]$$

III. Usando a linearidade da esperan√ßa e o fato de que $E[Y_t] = \mu$ para todo *t*, e $E[\varepsilon_t]=0$:
   $$\mu = c + \phi_1\mu + \phi_2\mu + \dots + \phi_p\mu$$

IV. Resolvendo para $\mu$:
   $$\mu = c + \mu \sum_{i=1}^{p} \phi_i$$
   $$\mu \left(1 - \sum_{i=1}^{p} \phi_i \right) = c$$
   $$\mu = \frac{c}{1 - \phi_1 - \phi_2 - \dots - \phi_p}$$

V. Agora, defina um novo processo $Y'_t = Y_t - \mu$.  Subtraindo $\mu$ de ambos os lados da equa√ß√£o original:
   $$Y_t - \mu = c + \phi_1Y_{t-1} + \phi_2Y_{t-2} + \dots + \phi_pY_{t-p} + \varepsilon_t - \mu$$

VI. Substituindo $\mu = \frac{c}{1 - \phi_1 - \phi_2 - \dots - \phi_p}$:
    $$Y'_t = Y_t - \mu =  c + \phi_1Y_{t-1} + \phi_2Y_{t-2} + \dots + \phi_pY_{t-p} + \varepsilon_t - \frac{c}{1 - \phi_1 - \phi_2 - \dots - \phi_p}$$

VII. Reorganizando os termos, podemos expressar $Y'_t$ em termos de $Y'_{t-i} = Y_{t-i} - \mu$:
   $$Y'_t = \phi_1(Y_{t-1}-\mu) + \phi_2(Y_{t-2}-\mu) + \dots + \phi_p(Y_{t-p}-\mu) + \varepsilon_t$$
   $$Y'_t = \phi_1Y'_{t-1} + \phi_2Y'_{t-2} + \dots + \phi_pY'_{t-p} + \varepsilon_t$$

VIII. Portanto, o processo recentralizado $Y'_t$ segue um processo AR(*p*) com m√©dia zero. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere o mesmo modelo AR(2) do exemplo anterior: $Y_t = 5 + 0.6Y_{t-1} + 0.2Y_{t-2} + \varepsilon_t$. Para encontrar a m√©dia incondicional $\mu$, calculamos: $\mu = \frac{5}{1 - 0.6 - 0.2} = \frac{5}{0.2} = 25$. Agora, se definirmos $Y'_t = Y_t - 25$, o novo processo ser√° $Y'_t = 0.6Y'_{t-1} + 0.2Y'_{t-2} + \varepsilon_t$, que tem m√©dia zero.

### Condi√ß√£o de Estacionariedade
A **estacionariedade** √© uma propriedade crucial para a an√°lise e interpreta√ß√£o de processos AR(*p*). Um processo AR(*p*) √© estacion√°rio se suas propriedades estat√≠sticas (m√©dia, vari√¢ncia, autocovari√¢ncia) n√£o variam ao longo do tempo [^45]. Para garantir a estacionariedade de um processo AR(*p*), as ra√≠zes do polin√¥mio caracter√≠stico associado devem estar fora do c√≠rculo unit√°rio [^58]. O polin√¥mio caracter√≠stico √© dado por:

$$1 - \phi_1z - \phi_2z^2 - \dots - \phi_pz^p = 0$$

onde *z* √© uma vari√°vel complexa. A condi√ß√£o de estacionariedade requer que todas as *p* ra√≠zes $z_i$ deste polin√¥mio satisfa√ßam $|z_i| > 1$ [^58].

> üí° **Exemplo Num√©rico:** Para o modelo AR(2) $Y_t = \phi_1Y_{t-1} + \phi_2Y_{t-2} + \varepsilon_t$, o polin√¥mio caracter√≠stico √© $1 - \phi_1z - \phi_2z^2 = 0$. Se $\phi_1 = 0.6$ e $\phi_2 = 0.2$, o polin√¥mio √© $1 - 0.6z - 0.2z^2 = 0$. Resolvendo para *z* usando a f√≥rmula quadr√°tica:
> $$z = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} = \frac{0.6 \pm \sqrt{(-0.6)^2 - 4(-0.2)(1)}}{2(-0.2)} = \frac{0.6 \pm \sqrt{0.36 + 0.8}}{-0.4} = \frac{0.6 \pm \sqrt{1.16}}{-0.4}$$
> As ra√≠zes s√£o $z_1 \approx -4.45$ e $z_2 \approx 1.45$. Como $|-4.45| > 1$ e $|1.45| > 1$, o processo √© estacion√°rio.

**Teorema 1:** (Condi√ß√£o de Estacionariedade Equivalente) Um processo AR(*p*) √© estacion√°rio se e somente se todos os polos da fun√ß√£o de transfer√™ncia $H(z) = \frac{1}{1 - \phi_1z - \phi_2z^2 - \dots - \phi_pz^p}$ est√£o dentro do c√≠rculo unit√°rio, ou seja, $|z_i| < 1$ para todas as ra√≠zes $z_i$ do denominador.

*Prova:* A condi√ß√£o $|z_i| > 1$ para as ra√≠zes do polin√¥mio caracter√≠stico √© equivalente a $|1/z_i| < 1$. Como os polos da fun√ß√£o de transfer√™ncia s√£o os inversos das ra√≠zes do polin√¥mio caracter√≠stico, a condi√ß√£o de estacionariedade √© equivalente a exigir que todos os polos estejam dentro do c√≠rculo unit√°rio.

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior com ra√≠zes $z_1 \approx -4.45$ e $z_2 \approx 1.45$, os polos da fun√ß√£o de transfer√™ncia s√£o $1/z_1 \approx -0.22$ e $1/z_2 \approx 0.69$. Como $|-0.22| < 1$ e $|0.69| < 1$, todos os polos est√£o dentro do c√≠rculo unit√°rio, confirmando a estacionariedade do processo.

### Representa√ß√£o em Termos do Operador de Defasagem
O modelo AR(*p*) pode ser expresso de forma concisa usando o operador de defasagem (L), onde $LY_t = Y_{t-1}$ [^57]. A equa√ß√£o do modelo AR(*p*) torna-se:

$$Y_t = c + \phi_1LY_t + \phi_2L^2Y_t + \dots + \phi_pL^pY_t + \varepsilon_t$$

Reorganizando os termos, obtemos:

$$(1 - \phi_1L - \phi_2L^2 - \dots - \phi_pL^p)Y_t = c + \varepsilon_t$$

Se definirmos $\phi(L) = 1 - \phi_1L - \phi_2L^2 - \dots - \phi_pL^p$ [^58], a equa√ß√£o pode ser escrita como:

$$\phi(L)Y_t = c + \varepsilon_t$$

Esta representa√ß√£o compacta √© √∫til para analisar as propriedades do modelo e para derivar sua representa√ß√£o como um processo de **m√©dia m√≥vel (MA)** de ordem infinita.

> üí° **Exemplo Num√©rico:** Para o modelo AR(2) $Y_t = 0.6Y_{t-1} + 0.2Y_{t-2} + \varepsilon_t$, usando o operador de defasagem, temos: $Y_t = 0.6LY_t + 0.2L^2Y_t + \varepsilon_t$. Reorganizando, obtemos $(1 - 0.6L - 0.2L^2)Y_t = \varepsilon_t$. Aqui, $\phi(L) = 1 - 0.6L - 0.2L^2$.

### Representa√ß√£o como um Processo MA(‚àû)
Sob a condi√ß√£o de estacionariedade, o operador $\phi(L)$ pode ser invertido, expressando $Y_t$ como uma soma infinita de termos de ru√≠do branco passados [^58]:

$$Y_t = \mu + \psi(L)\varepsilon_t$$

onde $\mu = c/(1 - \phi_1 - \phi_2 - \dots - \phi_p)$ [^58] e $\psi(L) = \phi(L)^{-1}$ [^58]. A fun√ß√£o $\psi(L)$ √© dada por:

$$\psi(L) = \sum_{j=0}^{\infty} \psi_j L^j$$

Os coeficientes $\psi_j$ representam os pesos dos termos de ru√≠do branco passados na determina√ß√£o do valor atual de $Y_t$. A estacionariedade garante que esses coeficientes decaiam suficientemente r√°pido para que a soma infinita convirja [^52, 53].

> üí° **Exemplo Num√©rico:** Para o modelo AR(1) $Y_t = \phi_1Y_{t-1} + \varepsilon_t$, onde $|\phi_1| < 1$, temos $\phi(L) = 1 - \phi_1L$. Ent√£o, $\psi(L) = \frac{1}{1 - \phi_1L} = 1 + \phi_1L + \phi_1^2L^2 + \phi_1^3L^3 + \dots = \sum_{j=0}^{\infty} \phi_1^j L^j$. Portanto, $Y_t = \sum_{j=0}^{\infty} \phi_1^j \varepsilon_{t-j} = \varepsilon_t + \phi_1\varepsilon_{t-1} + \phi_1^2\varepsilon_{t-2} + \dots$.

**Lema 1:** Se o processo AR(*p*) √© estacion√°rio, ent√£o a sequ√™ncia de coeficientes $\{\psi_j\}$ √© absolutamente som√°vel, ou seja, $\sum_{j=0}^{\infty} |\psi_j| < \infty$.

*Prova:* A estacionariedade implica que as ra√≠zes do polin√¥mio caracter√≠stico est√£o fora do c√≠rculo unit√°rio. Isso garante que a expans√£o $\psi(L) = \phi(L)^{-1}$ converge absolutamente para $|L| \le 1$, o que implica que $\sum_{j=0}^{\infty} |\psi_j| < \infty$.

**Prova detalhada do Lema 1:**

I. Come√ßamos com o processo AR(*p*) na forma do operador de defasagem:
   $$\phi(L)Y_t = c + \varepsilon_t$$
   onde $\phi(L) = 1 - \phi_1L - \phi_2L^2 - \dots - \phi_pL^p$.

II. A representa√ß√£o MA(‚àû) √© obtida invertendo o operador $\phi(L)$:
    $$Y_t = \frac{c}{\phi(L)} + \frac{\varepsilon_t}{\phi(L)} =  \mu + \psi(L)\varepsilon_t$$
    onde $\psi(L) = \frac{1}{\phi(L)} = \sum_{j=0}^{\infty} \psi_j L^j$ e $\mu = \frac{c}{\phi(1)} = \frac{c}{1 - \phi_1 - \phi_2 - \dots - \phi_p}$.

III. A estacionariedade do processo AR(*p*) implica que as ra√≠zes do polin√¥mio $\phi(z) = 1 - \phi_1z - \phi_2z^2 - \dots - \phi_pz^p$ est√£o fora do c√≠rculo unit√°rio, ou seja, $|z_i| > 1$ para todas as ra√≠zes $z_i$.

IV. Considere a fun√ß√£o $\psi(z) = \frac{1}{\phi(z)}$.  Como todas as ra√≠zes de $\phi(z)$ est√£o fora do c√≠rculo unit√°rio, $\psi(z)$ √© anal√≠tica (e, portanto, tem uma expans√£o de s√©rie de pot√™ncias convergente) para $|z| \leq 1 + \epsilon$ para algum $\epsilon > 0$.

V. Uma fun√ß√£o anal√≠tica em um disco fechado tem uma expans√£o de s√©rie de pot√™ncias que converge absolutamente dentro desse disco. Portanto, a s√©rie $\sum_{j=0}^{\infty} \psi_j z^j$ converge absolutamente para $|z| \leq 1 + \epsilon$.

VI. Em particular, para $z = 1$, a s√©rie $\sum_{j=0}^{\infty} \psi_j$ converge absolutamente, o que significa que $\sum_{j=0}^{\infty} |\psi_j| < \infty$.

VII. Portanto, a sequ√™ncia de coeficientes $\{\psi_j\}$ √© absolutamente som√°vel. ‚ñ†

### Autocovari√¢ncia e Autocorrela√ß√£o
As fun√ß√µes de **autocovari√¢ncia** e **autocorrela√ß√£o** desempenham um papel fundamental na caracteriza√ß√£o dos processos AR(*p*) [^45]. As autocovari√¢ncias $\gamma_j$ s√£o definidas como $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$ [^45], e as autocorrela√ß√µes $\rho_j$ s√£o definidas como $\rho_j = \gamma_j / \gamma_0$ [^49].

Para um processo AR(*p*), as autocovari√¢ncias satisfazem a seguinte equa√ß√£o de diferen√ßa [^57]:

$$\gamma_j = \phi_1\gamma_{j-1} + \phi_2\gamma_{j-2} + \dots + \phi_p\gamma_{j-p} \quad \text{para } j \ge 1$$

Dividindo por $\gamma_0$, obtemos as equa√ß√µes de **Yule-Walker** para as autocorrela√ß√µes [^57]:

$$\rho_j = \phi_1\rho_{j-1} + \phi_2\rho_{j-2} + \dots + \phi_p\rho_{j-p} \quad \text{para } j \ge 1$$

As equa√ß√µes de Yule-Walker fornecem um sistema de equa√ß√µes que pode ser usado para estimar os coeficientes $\phi_i$ a partir das autocorrela√ß√µes amostrais.

> üí° **Exemplo Num√©rico:** Para um processo AR(2) $Y_t = \phi_1Y_{t-1} + \phi_2Y_{t-2} + \varepsilon_t$, as equa√ß√µes de Yule-Walker para $j=1$ e $j=2$ s√£o:
> *   $\rho_1 = \phi_1 + \phi_2\rho_1$
> *   $\rho_2 = \phi_1\rho_1 + \phi_2$
> Se $\phi_1 = 0.6$ e $\phi_2 = 0.2$, podemos resolver este sistema para encontrar $\rho_1$ e $\rho_2$. Da primeira equa√ß√£o, $\rho_1 = 0.6 + 0.2\rho_1 \Rightarrow 0.8\rho_1 = 0.6 \Rightarrow \rho_1 = 0.75$. Substituindo na segunda equa√ß√£o, $\rho_2 = 0.6(0.75) + 0.2 = 0.45 + 0.2 = 0.65$.

**Teorema 2:** (Equa√ß√µes de Yule-Walker na Forma Matricial) As equa√ß√µes de Yule-Walker para $j = 1, 2, \dots, p$ podem ser escritas na forma matricial como:

$$
\begin{bmatrix}
\rho_1 \\
\rho_2 \\
\vdots \\
\rho_p
\end{bmatrix}
=
\begin{bmatrix}
1 & \rho_1 & \rho_2 & \cdots & \rho_{p-1} \\
\rho_1 & 1 & \rho_1 & \cdots & \rho_{p-2} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\rho_{p-1} & \rho_{p-2} & \rho_{p-3} & \cdots & 1
\end{bmatrix}
\begin{bmatrix}
\phi_1 \\
\phi_2 \\
\vdots \\
\phi_p
\end{bmatrix}
$$

Esta forma matricial facilita a solu√ß√£o do sistema de equa√ß√µes para os coeficientes $\phi_i$.

*Prova:* A prova consiste em escrever as equa√ß√µes de Yule-Walker para $j = 1, 2, \dots, p$ e rearranjar os termos para obter a forma matricial desejada. A estrutura da matriz de autocorrela√ß√µes √© uma matriz de Toeplitz, o que pode ser explorado para computa√ß√£o eficiente das solu√ß√µes.

**Prova Detalhada do Teorema 2:**

I. Come√ßamos com a equa√ß√£o do processo AR(*p*):
   $$Y_t = c + \phi_1Y_{t-1} + \phi_2Y_{t-2} + \dots + \phi_pY_{t-p} + \varepsilon_t$$
   Assumindo que o processo √© estacion√°rio e subtraindo a m√©dia $\mu$ de ambos os lados:
   $$Y_t - \mu = \phi_1(Y_{t-1}-\mu) + \phi_2(Y_{t-2}-\mu) + \dots + \phi_p(Y_{t-p}-\mu) + \varepsilon_t$$

II. Multiplicando ambos os lados por $(Y_{t-j} - \mu)$ e tomando a esperan√ßa:
   $$E[(Y_t - \mu)(Y_{t-j} - \mu)] = E\left[\left(\sum_{i=1}^{p} \phi_i(Y_{t-i}-\mu) + \varepsilon_t\right)(Y_{t-j} - \mu)\right]$$

III. Usando a defini√ß√£o de autocovari√¢ncia $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$ e a linearidade da esperan√ßa:
    $$\gamma_j = \sum_{i=1}^{p} \phi_i E[(Y_{t-i}-\mu)(Y_{t-j} - \mu)] + E[\varepsilon_t(Y_{t-j} - \mu)]$$

IV. Como $\varepsilon_t$ √© ru√≠do branco, $E[\varepsilon_t(Y_{t-j} - \mu)] = 0$ para $j > 0$.  Portanto:
    $$\gamma_j = \sum_{i=1}^{p} \phi_i \gamma_{j-i}$$
    $$\gamma_j = \phi_1\gamma_{j-1} + \phi_2\gamma_{j-2} + \dots + \phi_p\gamma_{j-p} \quad \text{para } j \ge 1$$

V. Dividindo ambos os lados por $\gamma_0$ e usando a defini√ß√£o de autocorrela√ß√£o $\rho_j = \frac{\gamma_j}{\gamma_0}$:
   $$\rho_j = \phi_1\rho_{j-1} + \phi_2\rho_{j-2} + \dots + \phi_p\rho_{j-p} \quad \text{para } j \ge 1$$

VI. Agora, escrevemos as equa√ß√µes de Yule-Walker para $j = 1, 2, \dots, p$:
   $$\rho_1 = \phi_1\rho_0 + \phi_2\rho_{-1} + \dots + \phi_p\rho_{1-p}$$
   $$\rho_2 = \phi_1\rho_1 + \phi_2\rho_0 + \dots + \phi_p\rho_{2-p}$$
   $$\vdots$$
   $$\rho_p = \phi_1\rho_{p-1} + \phi_2\rho_{p-2} + \dots + \phi_p\rho_0$$

VII. Usando a propriedade que $\rho_0 = 1$ e $\rho_{-j} = \rho_j$, podemos escrever o sistema de equa√ß√µes na forma matricial:
$$
\begin{bmatrix}
\rho_1 \\
\rho_2 \\
\vdots \\
\rho_p
\end{bmatrix}
=
\begin{bmatrix}
1 & \rho_1 & \rho_2 & \cdots & \rho_{p-1} \\
\rho_1 & 1 & \rho_1 & \cdots & \rho_{p-2} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\rho_{p-1} & \rho_{p-2} & \rho_{p-3} & \cdots & 1
\end{bmatrix}
\begin{bmatrix}
\phi_1 \\
\phi_2 \\
\vdots \\
\phi_p
\end{bmatrix}
$$
‚ñ†

> üí° **Exemplo Num√©rico:** Retomando o exemplo do AR(2) com $\phi_1 = 0.6$, $\phi_2 = 0.2$, $\rho_1 = 0.75$, e $\rho_2 = 0.65$, podemos verificar a forma matricial:
> $$
> \begin{bmatrix}
> 0.75 \\
> 0.65
> \end{bmatrix}
> =
> \begin{bmatrix}
> 1 & 0.75 \\
> 0.75 & 1
> \end{bmatrix}
> \begin{bmatrix}
> 0.6 \\
> 0.2
> \end{bmatrix}
> $$
> Multiplicando a matriz e o vetor:
> $$
> \begin{bmatrix}
> (1 * 0.6) + (0.75 * 0.2) \\
> (0.75 * 0.6) + (1 * 0.2)
> \end{bmatrix}
> =
> \begin{bmatrix}
> 0.6 + 0.15 \\
> 0.45 + 0.2
> \end{bmatrix}
> =
> \begin{bmatrix}
> 0.75 \\
> 0.65
> \end{bmatrix}
> $$
> A igualdade √© confirmada.

### Conclus√£o
O processo autorregressivo de ordem *p* (AR(*p*)) oferece uma estrutura flex√≠vel para modelar s√©ries temporais que exibem depend√™ncia temporal. A condi√ß√£o de estacionariedade garante que o processo seja bem comportado e que suas propriedades estat√≠sticas sejam consistentes ao longo do tempo. A representa√ß√£o em termos do operador de defasagem e a representa√ß√£o como um processo MA(‚àû) fornecem ferramentas √∫teis para analisar as propriedades do modelo e para calcular previs√µes. As equa√ß√µes de Yule-Walker estabelecem uma liga√ß√£o entre os coeficientes do modelo e as autocorrela√ß√µes da s√©rie temporal, permitindo a estimativa dos par√¢metros do modelo a partir dos dados observados.

### Refer√™ncias
[^45]: P√°ginas 44-45.
[^47]: P√°gina 47.
[^48]: P√°gina 48.
[^49]: P√°gina 49.
[^52]: P√°gina 52.
[^53]: P√°gina 53.
[^56]: P√°gina 56.
[^57]: P√°gina 57.
[^58]: P√°gina 58.
<!-- END -->