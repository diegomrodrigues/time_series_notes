## An√°lise Detalhada de Processos AR(1) Estacion√°rios

### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise de **processos autorregressivos de primeira ordem (AR(1))** que s√£o estacion√°rios, ou seja, satisfazem a condi√ß√£o $|\phi| < 1$. Baseando-se na defini√ß√£o geral de processos AR(*p*) apresentada anteriormente [^58], exploraremos em detalhes as propriedades estat√≠sticas de um processo AR(1) estacion√°rio, incluindo sua m√©dia, vari√¢ncia, autocovari√¢ncia e fun√ß√£o de autocorrela√ß√£o. Tamb√©m examinaremos como a estacionariedade influencia essas propriedades e como elas se manifestam na fun√ß√£o de autocorrela√ß√£o.

### Processo AR(1) Estacion√°rio: Defini√ß√£o e Condi√ß√£o de Estacionariedade

Um processo AR(1) √© definido pela seguinte equa√ß√£o [^53]:

$$Y_t = c + \phi Y_{t-1} + \varepsilon_t$$

onde:
*   $Y_t$ √© o valor da s√©rie temporal no tempo *t*.
*   $c$ √© uma constante.
*   $\phi$ √© o coeficiente autorregressivo de primeira ordem.
*   $\varepsilon_t$ √© um termo de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$ [^47, 48].

A condi√ß√£o de estacionariedade para um processo AR(1) √© $|\phi| < 1$ [^53]. Esta condi√ß√£o garante que as propriedades estat√≠sticas do processo n√£o variem ao longo do tempo e que o processo n√£o "explode" para valores infinitos. Se $|\phi| \ge 1$, o processo n√£o √© estacion√°rio, e as f√≥rmulas que derivaremos abaixo n√£o ser√£o v√°lidas [^54].

> üí° **Exemplo Num√©rico:** Considere $\phi = 0.8$. Como $|0.8| < 1$, o processo √© estacion√°rio. Se $\phi = 1.2$, como $|1.2| > 1$, o processo n√£o √© estacion√°rio.

### M√©dia de um Processo AR(1) Estacion√°rio

Para um processo AR(1) estacion√°rio, a m√©dia incondicional $\mu = E[Y_t]$ √© dada por [^53]:

$$\mu = \frac{c}{1 - \phi}$$

*Prova:* Assumindo estacionariedade, a m√©dia $\mu$ √© constante ao longo do tempo, ou seja, $E[Y_t] = E[Y_{t-1}] = \mu$. Tomando o valor esperado de ambos os lados da equa√ß√£o do processo AR(1):

$$E[Y_t] = E[c + \phi Y_{t-1} + \varepsilon_t]$$

I. Usando a linearidade da esperan√ßa e o fato de que $E[\varepsilon_t] = 0$, temos:

$$\mu = c + \phi \mu$$

II. Resolvendo para $\mu$:

$$\mu - \phi \mu = c$$
$$\mu (1 - \phi) = c$$
$$\mu = \frac{c}{1 - \phi}$$

A condi√ß√£o $|\phi| < 1$ garante que o denominador $(1 - \phi)$ n√£o seja zero. $\blacksquare$

> üí° **Exemplo Num√©rico:** Se $c = 5$ e $\phi = 0.5$, ent√£o a m√©dia √© $\mu = \frac{5}{1 - 0.5} = \frac{5}{0.5} = 10$. Se $c = -2$ e $\phi = -0.3$, ent√£o a m√©dia √© $\mu = \frac{-2}{1 - (-0.3)} = \frac{-2}{1.3} \approx -1.54$.

### Vari√¢ncia de um Processo AR(1) Estacion√°rio

A vari√¢ncia de um processo AR(1) estacion√°rio, denotada por $\gamma_0 = Var(Y_t) = E[(Y_t - \mu)^2]$, √© dada por [^53]:

$$\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$$

*Prova:* Para derivar a vari√¢ncia, primeiro reescrevemos o processo AR(1) em termos de desvios da m√©dia:

$$Y_t - \mu = \phi (Y_{t-1} - \mu) + \varepsilon_t$$

I. Elevando ambos os lados ao quadrado e tomando a esperan√ßa:

$$E[(Y_t - \mu)^2] = E[(\phi (Y_{t-1} - \mu) + \varepsilon_t)^2]$$

II. Usando a linearidade da esperan√ßa e expandindo o quadrado:

$$E[(Y_t - \mu)^2] = \phi^2 E[(Y_{t-1} - \mu)^2] + 2\phi E[(Y_{t-1} - \mu)\varepsilon_t] + E[\varepsilon_t^2]$$

III. Como $\varepsilon_t$ √© ru√≠do branco e independente de $Y_{t-1}$, o termo $E[(Y_{t-1} - \mu)\varepsilon_t] = 0$. Al√©m disso, $E[(Y_t - \mu)^2] = E[(Y_{t-1} - \mu)^2] = \gamma_0$ devido √† estacionariedade, e $E[\varepsilon_t^2] = \sigma^2$. Portanto:

$$\gamma_0 = \phi^2 \gamma_0 + \sigma^2$$

IV. Resolvendo para $\gamma_0$:

$$\gamma_0 - \phi^2 \gamma_0 = \sigma^2$$
$$\gamma_0 (1 - \phi^2) = \sigma^2$$
$$\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$$

A condi√ß√£o $|\phi| < 1$ garante que o denominador $(1 - \phi^2)$ seja positivo e que a vari√¢ncia seja finita. $\blacksquare$

> üí° **Exemplo Num√©rico:** Se $\sigma^2 = 2$ e $\phi = 0.7$, ent√£o a vari√¢ncia √© $\gamma_0 = \frac{2}{1 - (0.7)^2} = \frac{2}{1 - 0.49} = \frac{2}{0.51} \approx 3.92$. Se $\sigma^2 = 1$ e $\phi = -0.4$, ent√£o a vari√¢ncia √© $\gamma_0 = \frac{1}{1 - (-0.4)^2} = \frac{1}{1 - 0.16} = \frac{1}{0.84} \approx 1.19$.

**Observa√ß√£o:** Uma maneira alternativa de expressar a vari√¢ncia √© utilizando o operador de retrocesso (backshift operator) *B*, onde $BY_t = Y_{t-1}$. Podemos reescrever a equa√ß√£o do processo AR(1) como $(1 - \phi B)Y_t = c + \varepsilon_t$. Se o processo √© estacion√°rio, ent√£o o operador $(1 - \phi B)$ √© invert√≠vel e podemos expressar $Y_t$ como uma soma infinita de choques passados. Esta representa√ß√£o √© √∫til para derivar propriedades do processo.

### Autocovari√¢ncia de um Processo AR(1) Estacion√°rio

A *j*-√©sima autocovari√¢ncia de um processo AR(1) estacion√°rio, denotada por $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$, √© dada por [^53]:

$$\gamma_j = \frac{\phi^j}{1 - \phi^2} \sigma^2 = \phi^j \gamma_0$$

*Prova:* Para derivar a autocovari√¢ncia, multiplicamos ambos os lados da equa√ß√£o de desvio da m√©dia por $(Y_{t-j} - \mu)$ e tomamos a esperan√ßa:

$$E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\phi (Y_{t-1} - \mu) + \varepsilon_t)(Y_{t-j} - \mu)]$$

I. Usando a linearidade da esperan√ßa e expandindo:

$$E[(Y_t - \mu)(Y_{t-j} - \mu)] = \phi E[(Y_{t-1} - \mu)(Y_{t-j} - \mu)] + E[\varepsilon_t(Y_{t-j} - \mu)]$$

II. Para $j > 0$, o termo $E[\varepsilon_t(Y_{t-j} - \mu)] = 0$ porque $\varepsilon_t$ √© independente de $Y_{t-j}$. Portanto:

$$\gamma_j = \phi \gamma_{j-1}$$

III. Aplicando esta recurs√£o repetidamente, obtemos:

$$\gamma_j = \phi \gamma_{j-1} = \phi^2 \gamma_{j-2} = \ldots = \phi^j \gamma_0$$

IV. Substituindo $\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$, temos:

$$\gamma_j = \frac{\phi^j}{1 - \phi^2} \sigma^2 = \phi^j \gamma_0$$

Esta f√≥rmula mostra que a autocovari√¢ncia decai geometricamente com o aumento do lag *j*. A condi√ß√£o $|\phi| < 1$ garante que o decaimento seja para zero √† medida que *j* aumenta. $\blacksquare$

> üí° **Exemplo Num√©rico:** Seja $\sigma^2 = 1$, $\phi = 0.6$. Ent√£o $\gamma_0 = \frac{1}{1 - 0.6^2} = \frac{1}{0.64} = 1.5625$.
>
> $\gamma_1 = \phi \gamma_0 = 0.6 * 1.5625 = 0.9375$
>
> $\gamma_2 = \phi^2 \gamma_0 = 0.6^2 * 1.5625 = 0.5625$
>
> $\gamma_3 = \phi^3 \gamma_0 = 0.6^3 * 1.5625 = 0.3375$
>
> E assim por diante. Note que as autocovari√¢ncias decaem geometricamente.

**Teorema 1** [Yule-Walker Equations for AR(1) Process]
As equa√ß√µes de Yule-Walker para um processo AR(1) estacion√°rio relacionam as autocorrela√ß√µes do processo com os coeficientes autorregressivos. Para um processo AR(1), a equa√ß√£o de Yule-Walker √© dada por:

$$\rho_1 = \phi$$

*Prova:* Dividindo a equa√ß√£o $\gamma_j = \phi \gamma_{j-1}$ por $\gamma_0$, obtemos:

$$\frac{\gamma_j}{\gamma_0} = \phi \frac{\gamma_{j-1}}{\gamma_0}$$

I. Por defini√ß√£o, $\rho_j = \frac{\gamma_j}{\gamma_0}$.  Portanto:

$$\rho_j = \phi \rho_{j-1}$$

II. Para *j* = 1, temos:

$$\rho_1 = \phi \rho_0$$

III. Como $\rho_0 = 1$, segue que:

$$\rho_1 = \phi$$

Este resultado √© fundamental porque permite estimar o coeficiente $\phi$ diretamente da primeira autocorrela√ß√£o amostral da s√©rie temporal. $\blacksquare$

> üí° **Exemplo Num√©rico:** Se a primeira autocorrela√ß√£o amostral $\rho_1$ de uma s√©rie temporal √© 0.75, ent√£o, de acordo com as equa√ß√µes de Yule-Walker, uma estimativa para $\phi$ seria 0.75.

### Fun√ß√£o de Autocorrela√ß√£o (ACF) de um Processo AR(1) Estacion√°rio

A fun√ß√£o de autocorrela√ß√£o (ACF) de um processo AR(1) estacion√°rio, denotada por $\rho_j$, √© definida como a autocovari√¢ncia dividida pela vari√¢ncia [^49]:

$$\rho_j = \frac{\gamma_j}{\gamma_0} = \phi^j$$

Esta f√≥rmula mostra que a ACF de um processo AR(1) estacion√°rio decai geometricamente com o aumento do lag *j*. O sinal de $\phi$ determina se o decaimento √© positivo (se $\phi > 0$) ou alternado (se $\phi < 0$).

> üí° **Exemplo Num√©rico:** Se $\phi = 0.5$, a ACF √© $\rho_j = (0.5)^j$, que decai para zero √† medida que *j* aumenta. Se $\phi = -0.5$, a ACF √© $\rho_j = (-0.5)^j$, que alterna em sinal e decai para zero √† medida que *j* aumenta.

![Generated plot](./../images/plot_24.png)

**Corol√°rio 1.1** [ACF for negative lags]
Para um processo AR(1) estacion√°rio, a fun√ß√£o de autocorrela√ß√£o √© sim√©trica, ou seja, $\rho_j = \rho_{-j}$.

*Prova:* Por defini√ß√£o, $\rho_j = \frac{\gamma_j}{\gamma_0}$, onde $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$.

I. Tamb√©m, $\gamma_{-j} = E[(Y_t - \mu)(Y_{t+j} - \mu)] = E[(Y_{t-j} - \mu)(Y_t - \mu)] = \gamma_j$ devido √† estacionariedade.

II. Portanto, $\rho_{-j} = \frac{\gamma_{-j}}{\gamma_0} = \frac{\gamma_j}{\gamma_0} = \rho_j$.

III. Assim, $\rho_j = \rho_{-j}$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Se $\phi = 0.3$, ent√£o $\rho_2 = (0.3)^2 = 0.09$ e $\rho_{-2} = (0.3)^{-2} = (1/0.3)^2$ n√£o faz sentido. Mas como $\rho_j = \phi^j$, ent√£o $\rho_{-2}$ is not correct. $\rho_{-j} = \rho_{j}$ so $\rho_{-2} = (0.3)^2 = 0.09$.

### Implica√ß√µes da Estacionariedade na Autocorrela√ß√£o

A condi√ß√£o de estacionariedade $|\phi| < 1$ tem implica√ß√µes diretas na forma da fun√ß√£o de autocorrela√ß√£o [^53]:

1.  **Decaimento Geom√©trico:** A ACF decai geometricamente para zero √† medida que o lag *j* aumenta. Isso significa que a depend√™ncia entre observa√ß√µes diminui √† medida que a dist√¢ncia no tempo aumenta.

2.  **Converg√™ncia para Zero:** Como $|\phi| < 1$, $\lim_{j \to \infty} \rho_j = \lim_{j \to \infty} \phi^j = 0$. Isso significa que observa√ß√µes distantes no tempo s√£o assintoticamente n√£o correlacionadas.

3.  **Absoluta Somabilidade:** A condi√ß√£o de estacionariedade tamb√©m implica que a ACF √© absolutamente som√°vel, ou seja, $\sum_{j=-\infty}^{\infty} |\rho_j| < \infty$ [^52]. Isso garante que o processo seja "erg√≥dico para a m√©dia", o que significa que a m√©dia amostral converge para a m√©dia populacional √† medida que o tamanho da amostra aumenta.

> üí° **Exemplo Num√©rico:** Se $\phi = 0.8$, a ACF decai mais lentamente do que se $\phi = 0.2$. Isso implica que as observa√ß√µes passadas t√™m uma influ√™ncia maior no valor atual da s√©rie temporal quando $\phi$ √© maior.

**Teorema 2** [Invertibility of AR(1) process]
Um processo AR(1) estacion√°rio √© sempre invert√≠vel.

*Prova:* Um processo AR(1) √© dito invert√≠vel se ele pode ser expresso como uma representa√ß√£o MA($\infty$). A condi√ß√£o para invertibilidade √© que as ra√≠zes do polin√¥mio autorregressivo estejam fora do c√≠rculo unit√°rio.

I. No caso de um AR(1), o polin√¥mio autorregressivo √© $(1 - \phi B)$, onde *B* √© o operador de retrocesso.

II. A raiz deste polin√¥mio √© dada por $B = \frac{1}{\phi}$.

III. Para que o processo seja invert√≠vel, devemos ter $|B| > 1$, o que implica $|\frac{1}{\phi}| > 1$, ou equivalentemente, $|\phi| < 1$.

IV. Esta √© exatamente a condi√ß√£o de estacionariedade para o processo AR(1). Portanto, um processo AR(1) estacion√°rio √© sempre invert√≠vel. $\blacksquare$

> üí° **Exemplo Num√©rico:** Se $\phi = 0.6$, ent√£o the root of the autoregressive polynomial is $1/0.6 \approx 1.67$, which is outside the unit circle ($|1.67| > 1$). Portanto, o processo √© invert√≠vel.

### Conclus√£o

O processo AR(1) estacion√°rio serve como um bloco de constru√ß√£o fundamental na an√°lise de s√©ries temporais. Suas propriedades estat√≠sticas, incluindo a m√©dia, vari√¢ncia, autocovari√¢ncia e fun√ß√£o de autocorrela√ß√£o, s√£o completamente determinadas pelo coeficiente autorregressivo $\phi$ e pela vari√¢ncia do ru√≠do branco $\sigma^2$. A condi√ß√£o de estacionariedade $|\phi| < 1$ √© crucial para garantir que o processo seja bem comportado e que suas propriedades estat√≠sticas sejam consistentes ao longo do tempo. A forma geom√©trica da fun√ß√£o de autocorrela√ß√£o fornece informa√ß√µes valiosas sobre a depend√™ncia temporal das observa√ß√µes e √© uma ferramenta essencial para modelar e prever s√©ries temporais.

### Refer√™ncias
[^47]: P√°gina 47.
[^48]: P√°gina 48.
[^49]: P√°gina 49.
[^52]: P√°gina 52.
[^53]: P√°gina 53.
[^54]: P√°gina 54.
[^58]: P√°gina 58.
<!-- END -->