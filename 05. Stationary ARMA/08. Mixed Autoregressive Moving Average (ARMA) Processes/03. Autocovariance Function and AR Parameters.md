### Autocovari√¢ncia em Modelos ARMA: Depend√™ncia Autorregressiva Ap√≥s q Lags

### Introdu√ß√£o

Como vimos anteriormente [^51], os modelos ARMA(p, q) combinam componentes autorregressivos (AR) e de m√©dias m√≥veis (MA). A estacionariedade desses modelos √© crucial para garantir a estabilidade das propriedades estat√≠sticas ao longo do tempo, dependendo exclusivamente dos par√¢metros AR [^57, 60]. Agora, focaremos em como a fun√ß√£o de autocovari√¢ncia se comporta em modelos ARMA, especificamente na depend√™ncia da parte autorregressiva ap√≥s *q* lags [^57]. Este comportamento √© fundamental para entender a estrutura de depend√™ncia de longo prazo em processos ARMA.

### Autocovari√¢ncia e a Equa√ß√£o de Diferen√ßa de Ordem p

Para um processo ARMA(p, q), a estrutura de depend√™ncia √© complexa devido √† combina√ß√£o de termos AR e MA. No entanto, para lags maiores que a ordem da parte MA (*q*), a fun√ß√£o de autocovari√¢ncia exibe um comportamento simplificado, seguindo uma equa√ß√£o de diferen√ßa de ordem *p* governada pelos par√¢metros autorregressivos [^57]. Formalmente, para $j > q$, as autocovari√¢ncias $\gamma_j$ satisfazem:

$$\gamma_j = \phi_1 \gamma_{j-1} + \phi_2 \gamma_{j-2} + \dots + \phi_p \gamma_{j-p}$$

onde $\phi_1, \phi_2, \dots, \phi_p$ s√£o os coeficientes autorregressivos do modelo [^57]. Essa equa√ß√£o de diferen√ßa implica que, ap√≥s *q* lags, a autocovari√¢ncia $\gamma_j$ depende linearmente dos *p* valores anteriores da fun√ß√£o de autocovari√¢ncia. Essa propriedade √© uma consequ√™ncia direta da estrutura AR(p) do modelo ARMA.

**Teorema 1:** As autocovari√¢ncias de um processo ARMA(p, q) estacion√°rio com $j > q$ satisfazem uma equa√ß√£o de diferen√ßa linear homog√™nea de ordem *p* cujos coeficientes s√£o os par√¢metros autorregressivos do modelo [^57].

**Demonstra√ß√£o:**

I. Considere um processo ARMA(p, q) estacion√°rio definido por:
$$Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \dots + \phi_p Y_{t-p} + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \dots + \theta_q \varepsilon_{t-q}$$

II. Multiplique ambos os lados por $Y_{t-j} - \mu$ e tome a esperan√ßa, onde $j > q$:

$$E[(Y_t - \mu)(Y_{t-j} - \mu)] = \phi_1 E[(Y_{t-1} - \mu)(Y_{t-j} - \mu)] + \dots + \phi_p E[(Y_{t-p} - \mu)(Y_{t-j} - \mu)] + E[(\varepsilon_t + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q})(Y_{t-j} - \mu)]$$

III. O lado esquerdo √© $\gamma_j$. Para o lado direito, $E[(Y_{t-k} - \mu)(Y_{t-j} - \mu)] = \gamma_{j-k}$ para $k = 1, 2, \dots, p$. Al√©m disso, como $j > q$, os termos de erro $\varepsilon_t, \varepsilon_{t-1}, \dots, \varepsilon_{t-q}$ s√£o n√£o correlacionados com $Y_{t-j}$, ent√£o $E[\varepsilon_{t-k}(Y_{t-j} - \mu)] = 0$ para $k = 0, 1, \dots, q$.

IV. Portanto, a equa√ß√£o se simplifica para:

$$\gamma_j = \phi_1 \gamma_{j-1} + \phi_2 \gamma_{j-2} + \dots + \phi_p \gamma_{j-p}$$

que √© uma equa√ß√£o de diferen√ßa linear homog√™nea de ordem *p*. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um modelo ARMA(2, 1) definido por $Y_t = 0.5Y_{t-1} - 0.3Y_{t-2} + \varepsilon_t + 0.2\varepsilon_{t-1}$. Para $j > 1$, as autocovari√¢ncias satisfazem:
>
> $\gamma_j = 0.5\gamma_{j-1} - 0.3\gamma_{j-2}$
>
> Se $\gamma_0$ e $\gamma_1$ fossem conhecidos, poder√≠amos calcular todas as autocovari√¢ncias subsequentes usando esta equa√ß√£o. Suponha que $\gamma_0 = 1$ e $\gamma_1 = 0.2$. Ent√£o, podemos calcular $\gamma_2$ como:
>
> $\gamma_2 = 0.5\gamma_1 - 0.3\gamma_0 = 0.5(0.2) - 0.3(1) = 0.1 - 0.3 = -0.2$.
>
> De forma semelhante, $\gamma_3 = 0.5\gamma_2 - 0.3\gamma_1 = 0.5(-0.2) - 0.3(0.2) = -0.1 - 0.06 = -0.16$.
>
> Este exemplo mostra como a autocovari√¢ncia decai ou oscila dependendo dos coeficientes $\phi_1$ e $\phi_2$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros do modelo ARMA(2,1)
> phi1 = 0.5
> phi2 = -0.3
>
> # Condi√ß√µes iniciais
> gamma0 = 1.0
> gamma1 = 0.2
>
> # Inicializa√ß√£o do array de autocovari√¢ncias
> num_lags = 10
> gamma = np.zeros(num_lags)
> gamma[0] = gamma0
> gamma[1] = gamma1
>
> # C√°lculo das autocovari√¢ncias para j > q (q=1)
> for j in range(2, num_lags):
>     gamma[j] = phi1 * gamma[j-1] + phi2 * gamma[j-2]
>
> # Plot das autocovari√¢ncias
> plt.figure(figsize=(10, 6))
> plt.plot(range(num_lags), gamma, marker='o')
> plt.title("Autocovari√¢ncia do Modelo ARMA(2,1) para j > q")
> plt.xlabel("Lag (j)")
> plt.ylabel("Autocovari√¢ncia (Œ≥_j)")
> plt.grid(True)
> plt.show()
> ```

**Teorema 1.1:** A solu√ß√£o geral da equa√ß√£o de diferen√ßa linear homog√™nea de ordem *p* dada por $\gamma_j = \phi_1 \gamma_{j-1} + \phi_2 \gamma_{j-2} + \dots + \phi_p \gamma_{j-p}$ para $j > q$ pode ser expressa como uma combina√ß√£o linear das solu√ß√µes da equa√ß√£o caracter√≠stica associada.

*Demonstra√ß√£o:*

I. A equa√ß√£o caracter√≠stica associada √† equa√ß√£o de diferen√ßa √© dada por:
$$x^p - \phi_1 x^{p-1} - \phi_2 x^{p-2} - \dots - \phi_p = 0$$

II. Sejam $x_1, x_2, \dots, x_p$ as ra√≠zes (distintas) desta equa√ß√£o caracter√≠stica.

III. Ent√£o, as solu√ß√µes da equa√ß√£o de diferen√ßa s√£o da forma $x_1^j, x_2^j, \dots, x_p^j$.

IV. A solu√ß√£o geral da equa√ß√£o de diferen√ßa √© uma combina√ß√£o linear dessas solu√ß√µes:
$$\gamma_j = A_1 x_1^j + A_2 x_2^j + \dots + A_p x_p^j$$
onde $A_1, A_2, \dots, A_p$ s√£o constantes determinadas pelas condi√ß√µes iniciais (i.e., os valores de $\gamma_0, \gamma_1, \dots, \gamma_{p-1}$).

V. Se as ra√≠zes n√£o forem distintas, a solu√ß√£o geral envolver√° termos da forma $j^k x_i^j$ para algum inteiro n√£o negativo $k$. $\blacksquare$

### Implica√ß√µes para a Fun√ß√£o de Autocorrela√ß√£o (ACF)

Uma vez que a fun√ß√£o de autocorrela√ß√£o (ACF) √© simplesmente a fun√ß√£o de autocovari√¢ncia normalizada pela vari√¢ncia ($\rho_j = \frac{\gamma_j}{\gamma_0}$), a mesma equa√ß√£o de diferen√ßa de ordem *p* se aplica √† ACF para $j > q$:

$$\rho_j = \phi_1 \rho_{j-1} + \phi_2 \rho_{j-2} + \dots + \phi_p \rho_{j-p}$$

Essa propriedade √© √∫til para entender o comportamento de longo prazo da ACF e para identificar a ordem da parte AR do modelo.

**Corol√°rio 1:** A fun√ß√£o de autocorrela√ß√£o (ACF) de um processo ARMA(p, q) estacion√°rio com $j > q$ satisfaz uma equa√ß√£o de diferen√ßa linear homog√™nea de ordem *p* com os mesmos coeficientes autorregressivos que a fun√ß√£o de autocovari√¢ncia.

*Prova:*
I. Seja $\rho_j = \frac{\gamma_j}{\gamma_0}$ a fun√ß√£o de autocorrela√ß√£o, onde $\gamma_0$ √© a vari√¢ncia do processo.

II. Para $j > q$, temos $\gamma_j = \phi_1 \gamma_{j-1} + \phi_2 \gamma_{j-2} + \dots + \phi_p \gamma_{j-p}$.

III. Dividindo ambos os lados por $\gamma_0$, obtemos:

$$\frac{\gamma_j}{\gamma_0} = \phi_1 \frac{\gamma_{j-1}}{\gamma_0} + \phi_2 \frac{\gamma_{j-2}}{\gamma_0} + \dots + \phi_p \frac{\gamma_{j-p}}{\gamma_0}$$

IV. Substituindo $\rho_j = \frac{\gamma_j}{\gamma_0}$, temos:

$$\rho_j = \phi_1 \rho_{j-1} + \phi_2 \rho_{j-2} + \dots + \phi_p \rho_{j-p}$$

que √© a mesma equa√ß√£o de diferen√ßa de ordem *p* que a autocovari√¢ncia. $\blacksquare$

> üí° **Exemplo Num√©rico:** Para o mesmo modelo ARMA(2, 1) do exemplo anterior, com $\phi_1 = 0.5$ e $\phi_2 = -0.3$, a fun√ß√£o de autocorrela√ß√£o satisfaz:
>
> $\rho_j = 0.5\rho_{j-1} - 0.3\rho_{j-2}$ para $j > 1$.
>
> Novamente, conhecendo $\rho_0$ e $\rho_1$, todas as autocorrela√ß√µes subsequentes podem ser calculadas. Dado que $\rho_0 = 1$ (por defini√ß√£o) e $\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{0.2}{1} = 0.2$, ent√£o
>
> $\rho_2 = 0.5\rho_1 - 0.3\rho_0 = 0.5(0.2) - 0.3(1) = 0.1 - 0.3 = -0.2$.
>
> $\rho_3 = 0.5\rho_2 - 0.3\rho_1 = 0.5(-0.2) - 0.3(0.2) = -0.1 - 0.06 = -0.16$.
>
> Este exemplo ilustra como a ACF tamb√©m segue o padr√£o determinado pelos coeficientes autorregressivos ap√≥s o lag *q*.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros do modelo ARMA(2,1)
> phi1 = 0.5
> phi2 = -0.3
>
> # Condi√ß√µes iniciais
> rho0 = 1.0
> rho1 = 0.2
>
> # Inicializa√ß√£o do array de autocorrela√ß√µes
> num_lags = 10
> rho = np.zeros(num_lags)
> rho[0] = rho0
> rho[1] = rho1
>
> # C√°lculo das autocorrela√ß√µes para j > q (q=1)
> for j in range(2, num_lags):
>     rho[j] = phi1 * rho[j-1] + phi2 * rho[j-2]
>
> # Plot das autocorrela√ß√µes
> plt.figure(figsize=(10, 6))
> plt.plot(range(num_lags), rho, marker='o')
> plt.title("Fun√ß√£o de Autocorrela√ß√£o (ACF) do Modelo ARMA(2,1) para j > q")
> plt.xlabel("Lag (j)")
> plt.ylabel("Autocorrela√ß√£o (œÅ_j)")
> plt.grid(True)
> plt.show()
> ```

### Identifica√ß√£o da Ordem AR: PACF

A fun√ß√£o de autocorrela√ß√£o parcial (PACF) pode ser usada para ajudar a identificar a ordem da parte AR do modelo ARMA. Para um processo ARMA(p, q), a PACF deve ser teoricamente zero para lags *k > p* [^57]. Isso ocorre porque a PACF mede a correla√ß√£o entre $Y_t$ e $Y_{t-k}$ ap√≥s remover a influ√™ncia dos lags intermedi√°rios $Y_{t-1}, Y_{t-2}, \dots, Y_{t-k+1}$. Ap√≥s *p* lags, a parte AR do modelo j√° capturou toda a depend√™ncia relevante, ent√£o n√£o h√° correla√ß√£o parcial adicional [^57].

No entanto, na pr√°tica, devido a erros de amostragem, a PACF raramente ser√° exatamente zero para *k > p*, mas se aproximar√° de zero. Portanto, um corte abrupto na PACF ap√≥s o lag *p* pode indicar a ordem da parte AR do modelo.

**Teorema 2:** Para um processo ARMA(p, q) estacion√°rio, a fun√ß√£o de autocorrela√ß√£o parcial (PACF) $\alpha_k$ √© zero para lags *k > p*, dado que o processo √© condicional em $Y_{t-1}, ..., Y_{t-p}$ [^57].

*Prova:*
I. Seja $\alpha_k$ a autocorrela√ß√£o parcial entre $Y_t$ e $Y_{t-k}$.

II. Por defini√ß√£o, a PACF remove a influ√™ncia dos lags intermedi√°rios $Y_{t-1}, Y_{t-2}, \dots, Y_{t-k+1}$.

III. Para um processo ARMA(p, q), a depend√™ncia de $Y_t$ em seus valores passados √© capturada pelos primeiros *p* lags devido √† parte autorregressiva de ordem *p*.

IV. Portanto, para $k > p$, a PACF $\alpha_k$ mede a correla√ß√£o entre $Y_t$ e $Y_{t-k}$ ap√≥s remover a influ√™ncia de $Y_{t-1}, Y_{t-2}, \dots, Y_{t-k+1}$. No entanto, como a depend√™ncia j√° √© capturada pelos primeiros *p* lags, n√£o h√° correla√ß√£o parcial adicional. Formalmente, ao regredir $Y_t$ sobre $Y_{t-1}, Y_{t-2}, ..., Y_{t-k}$, os coeficientes associados a $Y_{t-p-1}, ..., Y_{t-k}$ devem ser zero, pois sua influ√™ncia j√° est√° incorporada nos termos $Y_{t-1}, ..., Y_{t-p}$ e nos termos de m√©dias m√≥veis.

V. Logo, para $k > p$, $\alpha_k = 0$. Na pr√°tica, devido a erros de amostragem, $\alpha_k$ raramente ser√° exatamente zero, mas se aproximar√° de zero. $\blacksquare$

> üí° **Exemplo Num√©rico:** Para um processo ARMA(1, q), esperamos que a PACF decaia para zero ap√≥s o primeiro lag. Para um processo ARMA(2, q), esperamos que a PACF decaia para zero ap√≥s o segundo lag. Para um ru√≠do branco (ARMA(0, 0)), a PACF √© zero para todos os lags maiores que zero.
>
> Considere um processo AR(1) com $\phi_1 = 0.7$. A PACF neste caso ser√°:
>
> $\alpha_1 = \rho_1 = \phi_1 = 0.7$
>
> $\alpha_k = 0$ para $k > 1$
>
> Na pr√°tica, com dados simulados, a PACF para $k > 1$ n√£o ser√° exatamente zero, mas pr√≥xima de zero.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from statsmodels.tsa.arima_process import ArmaProcess
> from statsmodels.graphics.tsaplots import plot_pacf
>
> # Par√¢metros do modelo AR(1)
> phi = np.array([0.7])
> theta = np.array([0])
>
> # Simula√ß√£o de dados AR(1)
> np.random.seed(0)
> ar_process = ArmaProcess(phi, theta)
> sample = ar_process.sample(200)
>
> # Plot da PACF
> plt.figure(figsize=(10, 6))
> plot_pacf(sample, lags=10, ax=plt.gca(), method='ywm')
> plt.title("Fun√ß√£o de Autocorrela√ß√£o Parcial (PACF) do Modelo AR(1)")
> plt.xlabel("Lag (k)")
> plt.ylabel("Autocorrela√ß√£o Parcial (Œ±_k)")
> plt.grid(True)
> plt.show()
> ```

**Lema 1:** A PACF $\alpha_k$ pode ser calculada recursivamente usando as equa√ß√µes de Yule-Walker.

*Prova:*
I. As equa√ß√µes de Yule-Walker relacionam as autocorrela√ß√µes parciais com as autocorrela√ß√µes. Para um processo AR(k), as equa√ß√µes de Yule-Walker s√£o dadas por:

$$\rho_j = \alpha_{k1} \rho_{j-1} + \alpha_{k2} \rho_{j-2} + \dots + \alpha_{kk} \rho_{j-k}, \quad j = 1, 2, \dots, k$$

onde $\alpha_{ki}$ √© o *i*-√©simo coeficiente do modelo AR(k).

II. A PACF no lag *k*, denotada por $\alpha_k$, √© igual ao √∫ltimo coeficiente do modelo AR(k) estimado usando as equa√ß√µes de Yule-Walker, ou seja, $\alpha_k = \alpha_{kk}$.

III. Para calcular as PACFs recursivamente, podemos resolver as equa√ß√µes de Yule-Walker para cada valor de *k* = 1, 2, 3, ...

IV. Para *k* = 1: $\rho_1 = \alpha_{11} \implies \alpha_1 = \rho_1$

V. Para *k* = 2:
   $\rho_1 = \alpha_{21} + \alpha_{22} \rho_1$
   $\rho_2 = \alpha_{21} \rho_1 + \alpha_{22}$
   Resolvendo este sistema, encontramos $\alpha_2 = \alpha_{22} = \frac{\rho_2 - \rho_1^2}{1 - \rho_1^2}$

VI. Este processo pode ser generalizado para calcular $\alpha_k$ para qualquer *k*. Assim, a PACF pode ser calculada recursivamente usando as equa√ß√µes de Yule-Walker. $\blacksquare$

### Comportamento das Autocovari√¢ncias para j ‚â§ q

Para lags menores ou iguais a *q* ($j \leq q$), as autocovari√¢ncias s√£o mais complexas e dependem tanto dos par√¢metros AR quanto MA, bem como da vari√¢ncia do ru√≠do branco. A raz√£o para essa complexidade √© que para esses lags, os termos de erro $\varepsilon_t$ e $\varepsilon_{t-j}$ est√£o correlacionados com $Y_{t-j}$, complicando o c√°lculo da esperan√ßa $E[(Y_t - \mu)(Y_{t-j} - \mu)]$.

A complexidade das autocovari√¢ncias para $j \leq q$ torna a identifica√ß√£o precisa das ordens *p* e *q* mais desafiadora. Em geral, a an√°lise combinada da ACF e PACF √© necess√°ria para identificar as ordens apropriadas. A ACF exibe um decaimento gradual ou oscilat√≥rio determinado pela parte AR ap√≥s os primeiros *q* lags e um padr√£o mais complexo para os *q* lags iniciais, enquanto a PACF corta ap√≥s *p* lags.

> üí° **Exemplo Num√©rico:** Para um modelo MA(1) com $\theta_1 = 0.6$ e $\sigma^2 = 1$ (vari√¢ncia do ru√≠do branco), temos:
>
> $\gamma_0 = (1 + \theta_1^2)\sigma^2 = (1 + 0.6^2)(1) = 1.36$
>
> $\gamma_1 = \theta_1 \sigma^2 = 0.6$
>
> $\gamma_j = 0$ para $j > 1$
>
> Neste caso, a ACF ser√°:
>
> $\rho_0 = 1$
>
> $\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{0.6}{1.36} \approx 0.44$
>
> $\rho_j = 0$ para $j > 1$
>
> Para um modelo ARMA(1,1) com $\phi_1 = 0.4$, $\theta_1 = 0.6$ e $\sigma^2 = 1$, o c√°lculo de $\gamma_0$ e $\gamma_1$ se torna mais complexo, envolvendo os dois par√¢metros e a vari√¢ncia. Este exemplo demonstra como a presen√ßa do componente MA afeta as autocovari√¢ncias para lags menores ou iguais a *q*.

### Exemplo Pr√°tico

Considere a modelagem de dados de vendas mensais de uma loja. Uma an√°lise inicial da s√©rie temporal revela uma tend√™ncia sazonal anual e autocorrela√ß√£o significativa. Um modelo ARMA(1, 1) pode ser apropriado se a ACF decair lentamente e a PACF cortar ap√≥s o primeiro lag. No entanto, se a ACF exibir um padr√£o sazonal, um modelo ARMA sazonal (SARMA) ou uma transforma√ß√£o sazonal da s√©rie temporal (por exemplo, diferencia√ß√£o sazonal) podem ser necess√°rios. Ap√≥s remover a sazonalidade, um modelo ARMA(p, q) pode ser ajustado aos dados dessazonalizados, com as ordens *p* e *q* identificadas pela an√°lise da ACF e PACF.

> üí° **Exemplo Num√©rico:** Suponha que ap√≥s a an√°lise da ACF e PACF dos dados de vendas dessazonalizados, observemos que a ACF decai geometricamente e a PACF corta ap√≥s o primeiro lag. Isso sugere um modelo AR(1). Estimamos o coeficiente AR(1) como $\hat{\phi}_1 = 0.6$. Portanto, o modelo pode ser expresso como:
>
> $Y_t = 0.6Y_{t-1} + \varepsilon_t$
>
> Se, por outro lado, a ACF corta ap√≥s o primeiro lag e a PACF decai geometricamente, isso sugere um modelo MA(1). Estimamos o coeficiente MA(1) como $\hat{\theta}_1 = 0.4$. Portanto, o modelo pode ser expresso como:
>
> $Y_t = \varepsilon_t + 0.4\varepsilon_{t-1}$
>
> Uma an√°lise mais refinada pode indicar um modelo ARMA(1,1), necessitando a estimativa de ambos os par√¢metros $\phi_1$ e $\theta_1$.

### Conclus√£o

Em resumo, a fun√ß√£o de autocovari√¢ncia em modelos ARMA(p, q) apresenta um comportamento interessante e √∫til. Para lags maiores que a ordem da parte MA (*q*), a autocovari√¢ncia segue uma equa√ß√£o de diferen√ßa de ordem *p* governada pelos par√¢metros autorregressivos. Essa propriedade √© crucial para entender a depend√™ncia de longo prazo na s√©rie temporal e auxilia na identifica√ß√£o da ordem da parte AR do modelo [^57]. A PACF tamb√©m serve como uma ferramenta valiosa para identificar a ordem AR, exibindo um corte te√≥rico ap√≥s *p* lags. A complexidade reside na an√°lise dos primeiros *q* lags, onde a ACF √© influenciada tanto pelos componentes AR quanto MA.

### Refer√™ncias

[^51]: Y‚ÇÅ = c + œÜ1Yt-1 + $...$ + œÜpYt-p + Œµt + Œ∏1Œµt-1 + $...$ + Œ∏qŒµt-q [^51]
[^57]: Para j > q, as autocovari√¢ncias de um processo ARMA(p, q) seguem uma equa√ß√£o de diferen√ßa de ordem *p*. [^57]
[^60]: Se o operador autorregressivo e o operador de m√©dias m√≥veis tiverem ra√≠zes em comum (por exemplo, $\lambda_i = \eta_j$ para algum *i* e *j*), ambos os lados da equa√ß√£o podem ser divididos pelo fator comum. [^60]
[^65]: Um processo de m√©dias m√≥veis √© invert√≠vel se puder ser reescrito como um processo autorregressivo de ordem infinita [^65].
<!-- END -->