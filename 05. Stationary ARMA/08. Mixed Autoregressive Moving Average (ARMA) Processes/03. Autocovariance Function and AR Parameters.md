### Autocovariância em Modelos ARMA: Dependência Autorregressiva Após q Lags

### Introdução

Como vimos anteriormente [^51], os modelos ARMA(p, q) combinam componentes autorregressivos (AR) e de médias móveis (MA). A estacionariedade desses modelos é crucial para garantir a estabilidade das propriedades estatísticas ao longo do tempo, dependendo exclusivamente dos parâmetros AR [^57, 60]. Agora, focaremos em como a função de autocovariância se comporta em modelos ARMA, especificamente na dependência da parte autorregressiva após *q* lags [^57]. Este comportamento é fundamental para entender a estrutura de dependência de longo prazo em processos ARMA.

### Autocovariância e a Equação de Diferença de Ordem p

Para um processo ARMA(p, q), a estrutura de dependência é complexa devido à combinação de termos AR e MA. No entanto, para lags maiores que a ordem da parte MA (*q*), a função de autocovariância exibe um comportamento simplificado, seguindo uma equação de diferença de ordem *p* governada pelos parâmetros autorregressivos [^57]. Formalmente, para $j > q$, as autocovariâncias $\gamma_j$ satisfazem:

$$\gamma_j = \phi_1 \gamma_{j-1} + \phi_2 \gamma_{j-2} + \dots + \phi_p \gamma_{j-p}$$

onde $\phi_1, \phi_2, \dots, \phi_p$ são os coeficientes autorregressivos do modelo [^57]. Essa equação de diferença implica que, após *q* lags, a autocovariância $\gamma_j$ depende linearmente dos *p* valores anteriores da função de autocovariância. Essa propriedade é uma consequência direta da estrutura AR(p) do modelo ARMA.

**Teorema 1:** As autocovariâncias de um processo ARMA(p, q) estacionário com $j > q$ satisfazem uma equação de diferença linear homogênea de ordem *p* cujos coeficientes são os parâmetros autorregressivos do modelo [^57].

**Demonstração:**

I. Considere um processo ARMA(p, q) estacionário definido por:
$$Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \dots + \phi_p Y_{t-p} + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \dots + \theta_q \varepsilon_{t-q}$$

II. Multiplique ambos os lados por $Y_{t-j} - \mu$ e tome a esperança, onde $j > q$:

$$E[(Y_t - \mu)(Y_{t-j} - \mu)] = \phi_1 E[(Y_{t-1} - \mu)(Y_{t-j} - \mu)] + \dots + \phi_p E[(Y_{t-p} - \mu)(Y_{t-j} - \mu)] + E[(\varepsilon_t + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q})(Y_{t-j} - \mu)]$$

III. O lado esquerdo é $\gamma_j$. Para o lado direito, $E[(Y_{t-k} - \mu)(Y_{t-j} - \mu)] = \gamma_{j-k}$ para $k = 1, 2, \dots, p$. Além disso, como $j > q$, os termos de erro $\varepsilon_t, \varepsilon_{t-1}, \dots, \varepsilon_{t-q}$ são não correlacionados com $Y_{t-j}$, então $E[\varepsilon_{t-k}(Y_{t-j} - \mu)] = 0$ para $k = 0, 1, \dots, q$.

IV. Portanto, a equação se simplifica para:

$$\gamma_j = \phi_1 \gamma_{j-1} + \phi_2 \gamma_{j-2} + \dots + \phi_p \gamma_{j-p}$$

que é uma equação de diferença linear homogênea de ordem *p*. $\blacksquare$

> 💡 **Exemplo Numérico:** Considere um modelo ARMA(2, 1) definido por $Y_t = 0.5Y_{t-1} - 0.3Y_{t-2} + \varepsilon_t + 0.2\varepsilon_{t-1}$. Para $j > 1$, as autocovariâncias satisfazem:
>
> $\gamma_j = 0.5\gamma_{j-1} - 0.3\gamma_{j-2}$
>
> Se $\gamma_0$ e $\gamma_1$ fossem conhecidos, poderíamos calcular todas as autocovariâncias subsequentes usando esta equação. Suponha que $\gamma_0 = 1$ e $\gamma_1 = 0.2$. Então, podemos calcular $\gamma_2$ como:
>
> $\gamma_2 = 0.5\gamma_1 - 0.3\gamma_0 = 0.5(0.2) - 0.3(1) = 0.1 - 0.3 = -0.2$.
>
> De forma semelhante, $\gamma_3 = 0.5\gamma_2 - 0.3\gamma_1 = 0.5(-0.2) - 0.3(0.2) = -0.1 - 0.06 = -0.16$.
>
> Este exemplo mostra como a autocovariância decai ou oscila dependendo dos coeficientes $\phi_1$ e $\phi_2$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Parâmetros do modelo ARMA(2,1)
> phi1 = 0.5
> phi2 = -0.3
>
> # Condições iniciais
> gamma0 = 1.0
> gamma1 = 0.2
>
> # Inicialização do array de autocovariâncias
> num_lags = 10
> gamma = np.zeros(num_lags)
> gamma[0] = gamma0
> gamma[1] = gamma1
>
> # Cálculo das autocovariâncias para j > q (q=1)
> for j in range(2, num_lags):
>     gamma[j] = phi1 * gamma[j-1] + phi2 * gamma[j-2]
>
> # Plot das autocovariâncias
> plt.figure(figsize=(10, 6))
> plt.plot(range(num_lags), gamma, marker='o')
> plt.title("Autocovariância do Modelo ARMA(2,1) para j > q")
> plt.xlabel("Lag (j)")
> plt.ylabel("Autocovariância (γ_j)")
> plt.grid(True)
> plt.show()
> ```

**Teorema 1.1:** A solução geral da equação de diferença linear homogênea de ordem *p* dada por $\gamma_j = \phi_1 \gamma_{j-1} + \phi_2 \gamma_{j-2} + \dots + \phi_p \gamma_{j-p}$ para $j > q$ pode ser expressa como uma combinação linear das soluções da equação característica associada.

*Demonstração:*

I. A equação característica associada à equação de diferença é dada por:
$$x^p - \phi_1 x^{p-1} - \phi_2 x^{p-2} - \dots - \phi_p = 0$$

II. Sejam $x_1, x_2, \dots, x_p$ as raízes (distintas) desta equação característica.

III. Então, as soluções da equação de diferença são da forma $x_1^j, x_2^j, \dots, x_p^j$.

IV. A solução geral da equação de diferença é uma combinação linear dessas soluções:
$$\gamma_j = A_1 x_1^j + A_2 x_2^j + \dots + A_p x_p^j$$
onde $A_1, A_2, \dots, A_p$ são constantes determinadas pelas condições iniciais (i.e., os valores de $\gamma_0, \gamma_1, \dots, \gamma_{p-1}$).

V. Se as raízes não forem distintas, a solução geral envolverá termos da forma $j^k x_i^j$ para algum inteiro não negativo $k$. $\blacksquare$

### Implicações para a Função de Autocorrelação (ACF)

Uma vez que a função de autocorrelação (ACF) é simplesmente a função de autocovariância normalizada pela variância ($\rho_j = \frac{\gamma_j}{\gamma_0}$), a mesma equação de diferença de ordem *p* se aplica à ACF para $j > q$:

$$\rho_j = \phi_1 \rho_{j-1} + \phi_2 \rho_{j-2} + \dots + \phi_p \rho_{j-p}$$

Essa propriedade é útil para entender o comportamento de longo prazo da ACF e para identificar a ordem da parte AR do modelo.

**Corolário 1:** A função de autocorrelação (ACF) de um processo ARMA(p, q) estacionário com $j > q$ satisfaz uma equação de diferença linear homogênea de ordem *p* com os mesmos coeficientes autorregressivos que a função de autocovariância.

*Prova:*
I. Seja $\rho_j = \frac{\gamma_j}{\gamma_0}$ a função de autocorrelação, onde $\gamma_0$ é a variância do processo.

II. Para $j > q$, temos $\gamma_j = \phi_1 \gamma_{j-1} + \phi_2 \gamma_{j-2} + \dots + \phi_p \gamma_{j-p}$.

III. Dividindo ambos os lados por $\gamma_0$, obtemos:

$$\frac{\gamma_j}{\gamma_0} = \phi_1 \frac{\gamma_{j-1}}{\gamma_0} + \phi_2 \frac{\gamma_{j-2}}{\gamma_0} + \dots + \phi_p \frac{\gamma_{j-p}}{\gamma_0}$$

IV. Substituindo $\rho_j = \frac{\gamma_j}{\gamma_0}$, temos:

$$\rho_j = \phi_1 \rho_{j-1} + \phi_2 \rho_{j-2} + \dots + \phi_p \rho_{j-p}$$

que é a mesma equação de diferença de ordem *p* que a autocovariância. $\blacksquare$

> 💡 **Exemplo Numérico:** Para o mesmo modelo ARMA(2, 1) do exemplo anterior, com $\phi_1 = 0.5$ e $\phi_2 = -0.3$, a função de autocorrelação satisfaz:
>
> $\rho_j = 0.5\rho_{j-1} - 0.3\rho_{j-2}$ para $j > 1$.
>
> Novamente, conhecendo $\rho_0$ e $\rho_1$, todas as autocorrelações subsequentes podem ser calculadas. Dado que $\rho_0 = 1$ (por definição) e $\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{0.2}{1} = 0.2$, então
>
> $\rho_2 = 0.5\rho_1 - 0.3\rho_0 = 0.5(0.2) - 0.3(1) = 0.1 - 0.3 = -0.2$.
>
> $\rho_3 = 0.5\rho_2 - 0.3\rho_1 = 0.5(-0.2) - 0.3(0.2) = -0.1 - 0.06 = -0.16$.
>
> Este exemplo ilustra como a ACF também segue o padrão determinado pelos coeficientes autorregressivos após o lag *q*.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Parâmetros do modelo ARMA(2,1)
> phi1 = 0.5
> phi2 = -0.3
>
> # Condições iniciais
> rho0 = 1.0
> rho1 = 0.2
>
> # Inicialização do array de autocorrelações
> num_lags = 10
> rho = np.zeros(num_lags)
> rho[0] = rho0
> rho[1] = rho1
>
> # Cálculo das autocorrelações para j > q (q=1)
> for j in range(2, num_lags):
>     rho[j] = phi1 * rho[j-1] + phi2 * rho[j-2]
>
> # Plot das autocorrelações
> plt.figure(figsize=(10, 6))
> plt.plot(range(num_lags), rho, marker='o')
> plt.title("Função de Autocorrelação (ACF) do Modelo ARMA(2,1) para j > q")
> plt.xlabel("Lag (j)")
> plt.ylabel("Autocorrelação (ρ_j)")
> plt.grid(True)
> plt.show()
> ```

### Identificação da Ordem AR: PACF

A função de autocorrelação parcial (PACF) pode ser usada para ajudar a identificar a ordem da parte AR do modelo ARMA. Para um processo ARMA(p, q), a PACF deve ser teoricamente zero para lags *k > p* [^57]. Isso ocorre porque a PACF mede a correlação entre $Y_t$ e $Y_{t-k}$ após remover a influência dos lags intermediários $Y_{t-1}, Y_{t-2}, \dots, Y_{t-k+1}$. Após *p* lags, a parte AR do modelo já capturou toda a dependência relevante, então não há correlação parcial adicional [^57].

No entanto, na prática, devido a erros de amostragem, a PACF raramente será exatamente zero para *k > p*, mas se aproximará de zero. Portanto, um corte abrupto na PACF após o lag *p* pode indicar a ordem da parte AR do modelo.

**Teorema 2:** Para um processo ARMA(p, q) estacionário, a função de autocorrelação parcial (PACF) $\alpha_k$ é zero para lags *k > p*, dado que o processo é condicional em $Y_{t-1}, ..., Y_{t-p}$ [^57].

*Prova:*
I. Seja $\alpha_k$ a autocorrelação parcial entre $Y_t$ e $Y_{t-k}$.

II. Por definição, a PACF remove a influência dos lags intermediários $Y_{t-1}, Y_{t-2}, \dots, Y_{t-k+1}$.

III. Para um processo ARMA(p, q), a dependência de $Y_t$ em seus valores passados é capturada pelos primeiros *p* lags devido à parte autorregressiva de ordem *p*.

IV. Portanto, para $k > p$, a PACF $\alpha_k$ mede a correlação entre $Y_t$ e $Y_{t-k}$ após remover a influência de $Y_{t-1}, Y_{t-2}, \dots, Y_{t-k+1}$. No entanto, como a dependência já é capturada pelos primeiros *p* lags, não há correlação parcial adicional. Formalmente, ao regredir $Y_t$ sobre $Y_{t-1}, Y_{t-2}, ..., Y_{t-k}$, os coeficientes associados a $Y_{t-p-1}, ..., Y_{t-k}$ devem ser zero, pois sua influência já está incorporada nos termos $Y_{t-1}, ..., Y_{t-p}$ e nos termos de médias móveis.

V. Logo, para $k > p$, $\alpha_k = 0$. Na prática, devido a erros de amostragem, $\alpha_k$ raramente será exatamente zero, mas se aproximará de zero. $\blacksquare$

> 💡 **Exemplo Numérico:** Para um processo ARMA(1, q), esperamos que a PACF decaia para zero após o primeiro lag. Para um processo ARMA(2, q), esperamos que a PACF decaia para zero após o segundo lag. Para um ruído branco (ARMA(0, 0)), a PACF é zero para todos os lags maiores que zero.
>
> Considere um processo AR(1) com $\phi_1 = 0.7$. A PACF neste caso será:
>
> $\alpha_1 = \rho_1 = \phi_1 = 0.7$
>
> $\alpha_k = 0$ para $k > 1$
>
> Na prática, com dados simulados, a PACF para $k > 1$ não será exatamente zero, mas próxima de zero.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from statsmodels.tsa.arima_process import ArmaProcess
> from statsmodels.graphics.tsaplots import plot_pacf
>
> # Parâmetros do modelo AR(1)
> phi = np.array([0.7])
> theta = np.array([0])
>
> # Simulação de dados AR(1)
> np.random.seed(0)
> ar_process = ArmaProcess(phi, theta)
> sample = ar_process.sample(200)
>
> # Plot da PACF
> plt.figure(figsize=(10, 6))
> plot_pacf(sample, lags=10, ax=plt.gca(), method='ywm')
> plt.title("Função de Autocorrelação Parcial (PACF) do Modelo AR(1)")
> plt.xlabel("Lag (k)")
> plt.ylabel("Autocorrelação Parcial (α_k)")
> plt.grid(True)
> plt.show()
> ```

**Lema 1:** A PACF $\alpha_k$ pode ser calculada recursivamente usando as equações de Yule-Walker.

*Prova:*
I. As equações de Yule-Walker relacionam as autocorrelações parciais com as autocorrelações. Para um processo AR(k), as equações de Yule-Walker são dadas por:

$$\rho_j = \alpha_{k1} \rho_{j-1} + \alpha_{k2} \rho_{j-2} + \dots + \alpha_{kk} \rho_{j-k}, \quad j = 1, 2, \dots, k$$

onde $\alpha_{ki}$ é o *i*-ésimo coeficiente do modelo AR(k).

II. A PACF no lag *k*, denotada por $\alpha_k$, é igual ao último coeficiente do modelo AR(k) estimado usando as equações de Yule-Walker, ou seja, $\alpha_k = \alpha_{kk}$.

III. Para calcular as PACFs recursivamente, podemos resolver as equações de Yule-Walker para cada valor de *k* = 1, 2, 3, ...

IV. Para *k* = 1: $\rho_1 = \alpha_{11} \implies \alpha_1 = \rho_1$

V. Para *k* = 2:
   $\rho_1 = \alpha_{21} + \alpha_{22} \rho_1$
   $\rho_2 = \alpha_{21} \rho_1 + \alpha_{22}$
   Resolvendo este sistema, encontramos $\alpha_2 = \alpha_{22} = \frac{\rho_2 - \rho_1^2}{1 - \rho_1^2}$

VI. Este processo pode ser generalizado para calcular $\alpha_k$ para qualquer *k*. Assim, a PACF pode ser calculada recursivamente usando as equações de Yule-Walker. $\blacksquare$

### Comportamento das Autocovariâncias para j ≤ q

Para lags menores ou iguais a *q* ($j \leq q$), as autocovariâncias são mais complexas e dependem tanto dos parâmetros AR quanto MA, bem como da variância do ruído branco. A razão para essa complexidade é que para esses lags, os termos de erro $\varepsilon_t$ e $\varepsilon_{t-j}$ estão correlacionados com $Y_{t-j}$, complicando o cálculo da esperança $E[(Y_t - \mu)(Y_{t-j} - \mu)]$.

A complexidade das autocovariâncias para $j \leq q$ torna a identificação precisa das ordens *p* e *q* mais desafiadora. Em geral, a análise combinada da ACF e PACF é necessária para identificar as ordens apropriadas. A ACF exibe um decaimento gradual ou oscilatório determinado pela parte AR após os primeiros *q* lags e um padrão mais complexo para os *q* lags iniciais, enquanto a PACF corta após *p* lags.

> 💡 **Exemplo Numérico:** Para um modelo MA(1) com $\theta_1 = 0.6$ e $\sigma^2 = 1$ (variância do ruído branco), temos:
>
> $\gamma_0 = (1 + \theta_1^2)\sigma^2 = (1 + 0.6^2)(1) = 1.36$
>
> $\gamma_1 = \theta_1 \sigma^2 = 0.6$
>
> $\gamma_j = 0$ para $j > 1$
>
> Neste caso, a ACF será:
>
> $\rho_0 = 1$
>
> $\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{0.6}{1.36} \approx 0.44$
>
> $\rho_j = 0$ para $j > 1$
>
> Para um modelo ARMA(1,1) com $\phi_1 = 0.4$, $\theta_1 = 0.6$ e $\sigma^2 = 1$, o cálculo de $\gamma_0$ e $\gamma_1$ se torna mais complexo, envolvendo os dois parâmetros e a variância. Este exemplo demonstra como a presença do componente MA afeta as autocovariâncias para lags menores ou iguais a *q*.

### Exemplo Prático

Considere a modelagem de dados de vendas mensais de uma loja. Uma análise inicial da série temporal revela uma tendência sazonal anual e autocorrelação significativa. Um modelo ARMA(1, 1) pode ser apropriado se a ACF decair lentamente e a PACF cortar após o primeiro lag. No entanto, se a ACF exibir um padrão sazonal, um modelo ARMA sazonal (SARMA) ou uma transformação sazonal da série temporal (por exemplo, diferenciação sazonal) podem ser necessários. Após remover a sazonalidade, um modelo ARMA(p, q) pode ser ajustado aos dados dessazonalizados, com as ordens *p* e *q* identificadas pela análise da ACF e PACF.

> 💡 **Exemplo Numérico:** Suponha que após a análise da ACF e PACF dos dados de vendas dessazonalizados, observemos que a ACF decai geometricamente e a PACF corta após o primeiro lag. Isso sugere um modelo AR(1). Estimamos o coeficiente AR(1) como $\hat{\phi}_1 = 0.6$. Portanto, o modelo pode ser expresso como:
>
> $Y_t = 0.6Y_{t-1} + \varepsilon_t$
>
> Se, por outro lado, a ACF corta após o primeiro lag e a PACF decai geometricamente, isso sugere um modelo MA(1). Estimamos o coeficiente MA(1) como $\hat{\theta}_1 = 0.4$. Portanto, o modelo pode ser expresso como:
>
> $Y_t = \varepsilon_t + 0.4\varepsilon_{t-1}$
>
> Uma análise mais refinada pode indicar um modelo ARMA(1,1), necessitando a estimativa de ambos os parâmetros $\phi_1$ e $\theta_1$.

### Conclusão

Em resumo, a função de autocovariância em modelos ARMA(p, q) apresenta um comportamento interessante e útil. Para lags maiores que a ordem da parte MA (*q*), a autocovariância segue uma equação de diferença de ordem *p* governada pelos parâmetros autorregressivos. Essa propriedade é crucial para entender a dependência de longo prazo na série temporal e auxilia na identificação da ordem da parte AR do modelo [^57]. A PACF também serve como uma ferramenta valiosa para identificar a ordem AR, exibindo um corte teórico após *p* lags. A complexidade reside na análise dos primeiros *q* lags, onde a ACF é influenciada tanto pelos componentes AR quanto MA.

### Referências

[^51]: Y₁ = c + φ1Yt-1 + $...$ + φpYt-p + εt + θ1εt-1 + $...$ + θqεt-q [^51]
[^57]: Para j > q, as autocovariâncias de um processo ARMA(p, q) seguem uma equação de diferença de ordem *p*. [^57]
[^60]: Se o operador autorregressivo e o operador de médias móveis tiverem raízes em comum (por exemplo, $\lambda_i = \eta_j$ para algum *i* e *j*), ambos os lados da equação podem ser divididos pelo fator comum. [^60]
[^65]: Um processo de médias móveis é invertível se puder ser reescrito como um processo autorregressivo de ordem infinita [^65].
<!-- END -->