## Modelos ARMA(p, q): Uma Combina√ß√£o de Autoregress√£o e M√©dias M√≥veis

### Introdu√ß√£o

Em continuidade ao estudo de processos de s√©ries temporais, expandimos agora a an√°lise para modelos que combinam componentes autorregressivos (AR) e de m√©dias m√≥veis (MA). Esses modelos, conhecidos como ARMA(p, q), fornecem uma estrutura flex√≠vel para modelar uma ampla variedade de s√©ries temporais, capturando tanto a depend√™ncia das observa√ß√µes passadas quanto a influ√™ncia de choques aleat√≥rios passados. Os modelos ARMA representam uma ferramenta poderosa na an√°lise de s√©ries temporais, permitindo uma modelagem mais rica e complexa das din√¢micas presentes nos dados [^51].

### Conceitos Fundamentais

Um processo ARMA(p, q) inclui tanto termos autorregressivos quanto termos de m√©dias m√≥veis, permitindo uma modelagem mais flex√≠vel de s√©ries temporais [^51]. A forma geral de um processo ARMA(p, q) √© dada por:

$$Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \dots + \phi_p Y_{t-p} + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \dots + \theta_q \varepsilon_{t-q}$$

onde:

*   $Y_t$ √© o valor da s√©rie temporal no tempo *t*.
*   *c* √© uma constante.
*   $\phi_1, \phi_2, \dots, \phi_p$ s√£o os coeficientes autorregressivos.
*   $\theta_1, \theta_2, \dots, \theta_q$ s√£o os coeficientes de m√©dias m√≥veis.
*   $\varepsilon_t$ √© o termo de erro (ru√≠do branco) no tempo *t*.
*   *p* √© a ordem da parte autorregressiva.
*   *q* √© a ordem da parte de m√©dias m√≥veis.

> üí° **Exemplo Num√©rico:** Considere um modelo ARMA(1, 1) com $c = 0.5$, $\phi_1 = 0.7$, $\theta_1 = 0.3$ e $\varepsilon_t$ seguindo uma distribui√ß√£o normal com m√©dia 0 e desvio padr√£o 1. A equa√ß√£o para este modelo √©:
>
> $Y_t = 0.5 + 0.7Y_{t-1} + \varepsilon_t + 0.3\varepsilon_{t-1}$
>
> Vamos simular os primeiros 5 valores da s√©rie temporal, assumindo que $Y_0 = 1$ e $\varepsilon_0 = 0$:
>
> *   $Y_1 = 0.5 + 0.7(1) + \varepsilon_1 + 0.3(0) = 1.2 + \varepsilon_1$
> *   Suponha que $\varepsilon_1 = 0.5$. Ent√£o, $Y_1 = 1.7$.
> *   $Y_2 = 0.5 + 0.7(1.7) + \varepsilon_2 + 0.3(0.5) = 0.5 + 1.19 + \varepsilon_2 + 0.15 = 1.84 + \varepsilon_2$
> *   Suponha que $\varepsilon_2 = -0.2$. Ent√£o, $Y_2 = 1.64$.
> *   $Y_3 = 0.5 + 0.7(1.64) + \varepsilon_3 + 0.3(-0.2) = 0.5 + 1.148 + \varepsilon_3 - 0.06 = 1.588 + \varepsilon_3$
> *   Suponha que $\varepsilon_3 = 1.0$. Ent√£o, $Y_3 = 2.588$.
> *   $Y_4 = 0.5 + 0.7(2.588) + \varepsilon_4 + 0.3(1.0) = 0.5 + 1.8116 + \varepsilon_4 + 0.3 = 2.6116 + \varepsilon_4$
> *   Suponha que $\varepsilon_4 = -0.5$. Ent√£o, $Y_4 = 2.1116$.
> *   $Y_5 = 0.5 + 0.7(2.1116) + \varepsilon_5 + 0.3(-0.5) = 0.5 + 1.47812 + \varepsilon_5 - 0.15 = 1.82812 + \varepsilon_5$
> *   Suponha que $\varepsilon_5 = 0.8$. Ent√£o, $Y_5 = 2.62812$.
>
> Assim, a s√©rie temporal simulada nos primeiros 5 instantes √©: 1, 1.7, 1.64, 2.588, 2.1116, 2.62812. Este exemplo ilustra como os valores passados de $Y_t$ e os erros aleat√≥rios afetam o valor atual.

Em nota√ß√£o do operador de retardo (lag operator), o modelo ARMA(p, q) pode ser escrito como [^51]:

$$(1 - \phi_1 L - \phi_2 L^2 - \dots - \phi_p L^p) Y_t = c + (1 + \theta_1 L + \theta_2 L^2 - \dots - \theta_q L^q) \varepsilon_t$$

Essa representa√ß√£o compacta facilita a an√°lise e manipula√ß√£o do modelo. A **estacionariedade** de um processo ARMA(p, q) depende exclusivamente dos par√¢metros autorregressivos ($\phi_1, \phi_2, \dots, \phi_p$) [^57]. Similarmente aos modelos AR(p), as ra√≠zes do polin√¥mio autorregressivo devem estar fora do c√≠rculo unit√°rio para garantir a estacionariedade. Em outras palavras, as solu√ß√µes da equa√ß√£o caracter√≠stica $1 - \phi_1 z - \phi_2 z^2 - \dots - \phi_p z^p = 0$ devem ter magnitude maior que 1 [^57].

> üí° **Exemplo Num√©rico:** Considere um modelo ARMA(2, 1) com $\phi_1 = 0.5$ e $\phi_2 = 0.3$. O polin√¥mio caracter√≠stico √© $1 - 0.5z - 0.3z^2 = 0$. Para verificar a estacionariedade, precisamos encontrar as ra√≠zes *z* dessa equa√ß√£o. Resolvendo a equa√ß√£o quadr√°tica, obtemos duas ra√≠zes:
>
> $z = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} = \frac{0.5 \pm \sqrt{(-0.5)^2 - 4(-0.3)(1)}}{2(-0.3)} = \frac{0.5 \pm \sqrt{0.25 + 1.2}}{-0.6} = \frac{0.5 \pm \sqrt{1.45}}{-0.6}$
>
> $z_1 \approx \frac{0.5 + 1.204}{-0.6} \approx -2.84$
> $z_2 \approx \frac{0.5 - 1.204}{-0.6} \approx 1.17$
>
> Como $|z_1| > 1$ e $|z_2| > 1$, ambas as ra√≠zes est√£o fora do c√≠rculo unit√°rio, garantindo que o processo ARMA(2, 1) seja estacion√°rio.

**Teorema 1:** Um processo ARMA(p, q) √© fracamente estacion√°rio se e somente se as ra√≠zes do polin√¥mio caracter√≠stico autorregressivo estiverem fora do c√≠rculo unit√°rio.

**Demonstra√ß√£o:** A estacionariedade de um ARMA(p, q) depende apenas da parte AR(p), como mencionado. Portanto, a prova segue diretamente da condi√ß√£o de estacionariedade para modelos AR(p).

### Fun√ß√£o de Autocovari√¢ncia

Para $j > q$, as autocovari√¢ncias de um processo ARMA(p, q) seguem uma equa√ß√£o de diferen√ßa de ordem *p* [^57]:

$$\gamma_j = \phi_1 \gamma_{j-1} + \phi_2 \gamma_{j-2} + \dots + \phi_p \gamma_{j-p}$$

Isso significa que, ap√≥s *q* lags, a fun√ß√£o de autocovari√¢ncia $\gamma_j$ (e a fun√ß√£o de autocorrela√ß√£o $\rho_j$) segue a mesma equa√ß√£o de diferen√ßa de ordem *p* que o processo [^57]. A complexidade dos modelos ARMA reside no fato de que as autocovari√¢ncias para lags menores ou iguais a *q* (isto √©, $j \leq q$) s√£o mais complicadas devido √† correla√ß√£o entre $\varepsilon_t$ e $Y_{t-j}$ [^57].

> üí° **Exemplo Num√©rico:** Considere um ARMA(1, 1) com $\phi_1 = 0.6$ e $\theta_1 = 0.4$. Para $j > 1$, a autocovari√¢ncia √© dada por:
>
> $\gamma_j = 0.6\gamma_{j-1}$
>
> Se $\gamma_0 = 5$, ent√£o $\gamma_2 = 0.6\gamma_1$, $\gamma_3 = 0.6^2 \gamma_1$, e assim por diante. As autocovari√¢ncias decaem exponencialmente ap√≥s o primeiro lag, seguindo a parte AR(1) do modelo. Os valores de $\gamma_0$ e $\gamma_1$ dependem tanto dos par√¢metros AR quanto MA e da vari√¢ncia do ru√≠do branco.

**Proposi√ß√£o 1:** As autocorrela√ß√µes parciais (PACF) de um processo ARMA(p, q) exibem um comportamento que pode ajudar na identifica√ß√£o da ordem *p*. Especificamente, para lags *k > p*, a PACF deve teoricamente ser zero.

**Demonstra√ß√£o:** Para lags *k > p*, o efeito condicional de $Y_{t-k}$ em $Y_t$, dado $Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1}$, √© nulo. Isso decorre do fato de que o processo AR(p) modela a depend√™ncia at√© o lag *p*. No entanto, na pr√°tica, devido a erros de amostragem, a PACF raramente ser√° exatamente zero, mas se aproximar√° de zero ap√≥s o lag *p*.

**Prova:**
I.  Considere a defini√ß√£o da autocorrela√ß√£o parcial (PACF) entre $Y_t$ e $Y_{t-k}$, denotada por $\alpha_{k}$, como o coeficiente de $Y_{t-k}$ na regress√£o de $Y_t$ em $Y_{t-1}, Y_{t-2}, ..., Y_{t-k}$.
    $$Y_t = v_1 Y_{t-1} + v_2 Y_{t-2} + \ldots + v_k Y_{t-k} + e_t$$
    onde $e_t$ √© o termo de erro.

II. Para um processo ARMA(p, q), a depend√™ncia de $Y_t$ em seus valores passados √© completamente capturada pelos primeiros *p* lags, devido √† parte autorregressiva de ordem *p*. Portanto, se *k > p*, o coeficiente $\alpha_{k}$ deve ser zero, j√° que $Y_{t-k}$ n√£o adiciona nenhuma informa√ß√£o preditiva sobre $Y_t$ al√©m do que j√° √© fornecido por $Y_{t-1}, Y_{t-2}, ..., Y_{t-p}$.

III. Formalmente, considere a equa√ß√£o do ARMA(p, q):
    $$Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \dots + \phi_p Y_{t-p} + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \dots + \theta_q \varepsilon_{t-q}$$
    Se $k > p$, ao regredir $Y_t$ em $Y_{t-1}, Y_{t-2}, ..., Y_{t-k}$, os coeficientes associados a $Y_{t-p-1}, ..., Y_{t-k}$ devem ser zero, pois sua influ√™ncia j√° est√° incorporada nos termos $Y_{t-1}, ..., Y_{t-p}$ e nos termos de m√©dias m√≥veis.

IV.  Portanto, para $k > p$, $\alpha_{k} = 0$. Na pr√°tica, devido a erros de amostragem, $\alpha_{k}$ raramente ser√° exatamente zero, mas se aproximar√° de zero. ‚ñ†

### Overparameterization

√â importante evitar a overparameteriza√ß√£o em modelos ARMA [^60]. Considere a fatora√ß√£o dos operadores polinomiais de retardo:

$$(1 - \lambda_1 L)(1 - \lambda_2 L) \dots (1 - \lambda_p L)(Y_t - \mu) = (1 + \eta_1 L)(1 + \eta_2 L) \dots (1 + \eta_q L)\varepsilon_t$$

Se o operador autorregressivo e o operador de m√©dias m√≥veis tiverem ra√≠zes em comum (por exemplo, $\lambda_i = \eta_j$ para algum *i* e *j*), ambos os lados da equa√ß√£o podem ser divididos pelo fator comum [^60]. Portanto, modelos ARMA(p, q) que possuem fatores comuns podem ser simplificados para modelos de ordem inferior. A estacionariedade de um processo ARMA depende unicamente dos par√¢metros autorregressivos ($\phi_1, \phi_2, \dots, \phi_p$), e n√£o dos par√¢metros de m√©dias m√≥veis ($\theta_1, \theta_2, \dots, \theta_q$) [^60].

#### Exemplo de Redund√¢ncia

Considere um processo de ru√≠do branco simples [^60]:

$$Y_t = \varepsilon_t$$

Multiplicando ambos os lados por (1 - $\rho L$), obtemos:

$$(1 - \rho L)Y_t = (1 - \rho L)\varepsilon_t$$

Embora essa representa√ß√£o possa ser vista como um modelo ARMA(1, 1), ela √© redundante, uma vez que qualquer valor de $\rho$ descreve os dados igualmente bem [^60]. Tentar estimar $\rho$ por m√°xima verossimilhan√ßa causaria problemas. Manipula√ß√µes te√≥ricas baseadas em tal representa√ß√£o podem n√£o levar em considera√ß√£o cancelamentos cruciais.

> üí° **Exemplo Num√©rico:** Suponha que temos uma s√©rie temporal que √© realmente ru√≠do branco, ou seja, $Y_t = \varepsilon_t$, onde $\varepsilon_t$ √© ru√≠do branco com m√©dia 0 e vari√¢ncia 1. Se tentarmos ajustar um modelo ARMA(1, 1) da forma $(1 - \rho L)Y_t = (1 - \rho L)\varepsilon_t$, estaremos essencialmente introduzindo redund√¢ncia. Se $\rho = 0.5$, tanto o lado esquerdo quanto o direito da equa√ß√£o s√£o multiplicados por $(1 - 0.5L)$. Isso n√£o muda a natureza da s√©rie temporal, que permanece ru√≠do branco. Estimar $\rho$ neste caso levaria a estimativas inst√°veis e n√£o informativas.

### Invertibilidade

Assim como a estacionariedade √© crucial para modelos autorregressivos, a **invertibilidade** √© uma propriedade importante para modelos de m√©dias m√≥veis. Um processo de m√©dias m√≥veis √© invert√≠vel se puder ser reescrito como um processo autorregressivo de ordem infinita [^65]. Para um modelo MA(1), $Y_t = \mu + (1 + \theta L)\varepsilon_t$, a invertibilidade requer que $|\theta| < 1$ [^65]. Se $|\theta| \geq 1$, a representa√ß√£o do modelo em termos de seus valores passados n√£o ser√° bem definida.
Considerando um MA(1):

$$Y_t = \mu + (1 + \theta L)\varepsilon_t = \mu + \varepsilon_t + \theta\varepsilon_{t-1}$$

com $|\theta| < 1$. Podemos reescrever $\varepsilon_t$ como:
$$\varepsilon_t = (1 + \theta L)^{-1}(Y_t - \mu)$$
Expandindo $(1 + \theta L)^{-1}$ em uma s√©rie infinita:

$$(1 + \theta L)^{-1} = 1 - \theta L + \theta^2 L^2 - \theta^3 L^3 + \dots$$
Assim, $\varepsilon_t$ pode ser expresso em termos de valores passados de $Y_t$:
$$\varepsilon_t = (Y_t - \mu) - \theta(Y_{t-1} - \mu) + \theta^2(Y_{t-2} - \mu) - \theta^3(Y_{t-3} - \mu) + \dots$$

Se $|\theta| \geq 1$, os coeficientes n√£o convergir√£o e a representa√ß√£o acima n√£o ser√° v√°lida.

> üí° **Exemplo Num√©rico:** Considere um modelo MA(1) com $\theta = 0.8$. Ent√£o, $Y_t = \varepsilon_t + 0.8\varepsilon_{t-1}$. Podemos expressar $\varepsilon_t$ como:
>
> $\varepsilon_t = Y_t - 0.8Y_{t-1} + 0.8^2 Y_{t-2} - 0.8^3 Y_{t-3} + \dots$
>
> Os coeficientes decaem exponencialmente, e a representa√ß√£o √© v√°lida. Agora, considere $\theta = 1.2$. Ent√£o, $Y_t = \varepsilon_t + 1.2\varepsilon_{t-1}$. Expressando $\varepsilon_t$, temos:
>
> $\varepsilon_t = Y_t - 1.2Y_{t-1} + 1.2^2 Y_{t-2} - 1.2^3 Y_{t-3} + \dots$
>
> Os coeficientes crescem exponencialmente, tornando a representa√ß√£o inst√°vel e n√£o invert√≠vel.

**Teorema 2:** Um processo ARMA(p, q) √© invert√≠vel se e somente se as ra√≠zes do polin√¥mio caracter√≠stico da m√©dia m√≥vel estiverem fora do c√≠rculo unit√°rio.

**Demonstra√ß√£o:** A invertibilidade do ARMA(p, q) depende exclusivamente da parte MA(q). A demonstra√ß√£o √© an√°loga √† da invertibilidade de um MA(q), ou seja, garantir que o processo MA possa ser expresso como um AR de ordem infinita com coeficientes convergentes.

**Prova:**
I. Considere um modelo MA(q) dado por:
   $$Y_t = \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \dots + \theta_q \varepsilon_{t-q}$$
   Em nota√ß√£o do operador de retardo, podemos escrever:
   $$Y_t = (1 + \theta_1 L + \theta_2 L^2 + \dots + \theta_q L^q) \varepsilon_t = \Theta(L) \varepsilon_t$$
   onde $\Theta(L)$ √© o polin√¥mio de m√©dias m√≥veis.

II. Para que o processo seja invert√≠vel, devemos ser capazes de expressar $\varepsilon_t$ como uma fun√ß√£o dos valores passados de $Y_t$.  Isso significa encontrar um polin√¥mio $\Phi(L)$ tal que:
    $$\varepsilon_t = \Phi(L) Y_t$$
    Substituindo a express√£o de $Y_t$ da etapa I, temos:
    $$\varepsilon_t = \Phi(L) \Theta(L) \varepsilon_t$$
    Para que essa igualdade seja v√°lida, devemos ter $\Phi(L) \Theta(L) = 1$, ou equivalentemente, $\Phi(L) = \Theta(L)^{-1}$.

III.  A condi√ß√£o para que $\Theta(L)^{-1}$ possa ser expressa como uma s√©rie convergente (ou seja, um polin√¥mio de ordem infinita em *L* com coeficientes que convergem para zero) √© que as ra√≠zes do polin√¥mio $\Theta(z) = 1 + \theta_1 z + \theta_2 z^2 + \dots + \theta_q z^q$ estejam fora do c√≠rculo unit√°rio. Isso garante que a expans√£o de $\Theta(L)^{-1}$ em termos de *L* converge, e portanto $\varepsilon_t$ pode ser expresso como uma combina√ß√£o linear dos valores passados de $Y_t$.

IV. Portanto, um processo ARMA(p, q) √© invert√≠vel se e somente se as ra√≠zes do polin√¥mio caracter√≠stico da m√©dia m√≥vel estiverem fora do c√≠rculo unit√°rio. ‚ñ†

### Fun√ß√£o Geradora de Autocovari√¢ncia (Autocovariance-Generating Function)

Para um processo estacion√°rio, a fun√ß√£o geradora de autocovari√¢ncia √© definida como [^61]:

$$g_Y(z) = \sum_{j=-\infty}^{\infty} \gamma_j z^j$$

onde *z* √© uma vari√°vel complexa. Essa fun√ß√£o resume as autocovari√¢ncias do processo e pode ser usada para calcular o espectro populacional [^61]. Para um processo ARMA(p, q), a fun√ß√£o geradora de autocovari√¢ncia pode ser expressa como [^62]:

$$g_Y(z) = \frac{\sigma^2 (1 + \theta_1 z + \theta_2 z^2 + \dots + \theta_q z^q)(1 + \theta_1 z^{-1} + \theta_2 z^{-2} + \dots + \theta_q z^{-q})}{(1 - \phi_1 z - \phi_2 z^2 - \dots - \phi_p z^p)(1 - \phi_1 z^{-1} - \phi_2 z^{-2} - \dots - \phi_p z^{-p})}$$

> üí° **Exemplo Num√©rico:** Para um processo ARMA(1, 1) com $\phi_1 = 0.5$, $\theta_1 = 0.3$ e $\sigma^2 = 1$, a fun√ß√£o geradora de autocovari√¢ncia √©:
>
> $g_Y(z) = \frac{(1 + 0.3z)(1 + 0.3z^{-1})}{(1 - 0.5z)(1 - 0.5z^{-1})} = \frac{1 + 0.3z + 0.3z^{-1} + 0.09}{1 - 0.5z - 0.5z^{-1} + 0.25}$
>
> Esta fun√ß√£o pode ser usada para calcular as autocovari√¢ncias $\gamma_j$ do processo.

**Lema 1:** A fun√ß√£o de densidade espectral (Power Spectral Density - PSD) de um processo estacion√°rio √© a transformada de Fourier da sua fun√ß√£o de autocovari√¢ncia.

**Demonstra√ß√£o:** A PSD, denotada por $S_Y(\omega)$, √© definida como $S_Y(\omega) = \frac{1}{2\pi} \sum_{j=-\infty}^{\infty} \gamma_j e^{-i\omega j}$.  Substituindo $z = e^{-i\omega}$ na fun√ß√£o geradora de autocovari√¢ncia, obtemos uma express√£o relacionada √† PSD.

**Prova:**
I.  A fun√ß√£o de autocovari√¢ncia $\gamma_j$ de um processo estacion√°rio $Y_t$ √© definida como:
    $$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$$
    onde $\mu$ √© a m√©dia do processo.

II. A fun√ß√£o de densidade espectral (PSD) $S_Y(\omega)$ √© definida como a transformada de Fourier da fun√ß√£o de autocovari√¢ncia:
    $$S_Y(\omega) = \frac{1}{2\pi} \sum_{j=-\infty}^{\infty} \gamma_j e^{-i\omega j}$$
    onde $\omega$ √© a frequ√™ncia angular.

III.  A fun√ß√£o geradora de autocovari√¢ncia √© dada por:
     $$g_Y(z) = \sum_{j=-\infty}^{\infty} \gamma_j z^j$$

IV. Se substituirmos $z = e^{-i\omega}$ na fun√ß√£o geradora de autocovari√¢ncia, obtemos:
    $$g_Y(e^{-i\omega}) = \sum_{j=-\infty}^{\infty} \gamma_j e^{-i\omega j}$$

V. Comparando com a defini√ß√£o de PSD, temos:
    $$S_Y(\omega) = \frac{1}{2\pi} g_Y(e^{-i\omega})$$
    Isso mostra que a PSD √© essencialmente a transformada de Fourier da fun√ß√£o de autocovari√¢ncia, normalizada por $2\pi$. ‚ñ†

### Filtros

Filtros s√£o aplicados a s√©ries temporais para modificar suas propriedades [^63]. A aplica√ß√£o de um filtro *h(L)* a uma s√©rie temporal $Y_t$ resulta em uma nova s√©rie $X_t = h(L)Y_t$. A fun√ß√£o geradora de autocovari√¢ncia da s√©rie filtrada $X_t$ √© dada por [^63]:

$$g_X(z) = h(z)h(z^{-1})g_Y(z)$$

> üí° **Exemplo Num√©rico:** Considere um processo ARMA(1, 1) com $\phi_1 = 0.5$, $\theta_1 = 0.3$ e $\sigma^2 = 1$. Aplicamos um filtro de m√©dia m√≥vel de ordem 2, $h(L) = \frac{1}{3}(1 + L + L^2)$, √† s√©rie. A fun√ß√£o geradora de autocovari√¢ncia da s√©rie filtrada √©:
>
> $g_X(z) = h(z)h(z^{-1})g_Y(z) = \frac{1}{9}(1 + z + z^2)(1 + z^{-1} + z^{-2}) \frac{(1 + 0.3z)(1 + 0.3z^{-1})}{(1 - 0.5z)(1 - 0.5z^{-1})}$
>
> Este filtro suaviza a s√©rie temporal original e modifica suas propriedades espectrais.

**Corol√°rio 1:** Se $Y_t$ √© um processo ARMA(p, q) e aplicamos um filtro *h(L)* a ele, a fun√ß√£o de densidade espectral da s√©rie filtrada $X_t$ √© dada por $S_X(\omega) = |H(\omega)|^2 S_Y(\omega)$, onde $H(\omega)$ √© a transformada de Fourier de *h(L)*.

**Demonstra√ß√£o:** Isso segue diretamente da rela√ß√£o entre a fun√ß√£o geradora de autocovari√¢ncia e a fun√ß√£o de densidade espectral, e da propriedade da fun√ß√£o geradora de autocovari√¢ncia de uma s√©rie filtrada.

**Prova:**
I. Seja $Y_t$ um processo ARMA(p, q) com fun√ß√£o de densidade espectral $S_Y(\omega)$.

II. Aplicamos um filtro linear $h(L)$ a $Y_t$, resultando na s√©rie filtrada $X_t = h(L)Y_t$.

III. A fun√ß√£o geradora de autocovari√¢ncia de $X_t$ √© dada por:
   $$g_X(z) = h(z) h(z^{-1}) g_Y(z)$$

IV. Usando o Lema 1, sabemos que $S_Y(\omega) = \frac{1}{2\pi} g_Y(e^{-i\omega})$ e $S_X(\omega) = \frac{1}{2\pi} g_X(e^{-i\omega})$.

V. Substituindo $z = e^{-i\omega}$ na express√£o para $g_X(z)$, temos:
   $$g_X(e^{-i\omega}) = h(e^{-i\omega}) h(e^{i\omega}) g_Y(e^{-i\omega})$$

VI. Seja $H(\omega)$ a transformada de Fourier do filtro *h(L)*, ou seja, $H(\omega) = h(e^{-i\omega})$. Ent√£o, $H(-\omega) = h(e^{i\omega})$.

VII. A magnitude ao quadrado de $H(\omega)$ √© dada por $|H(\omega)|^2 = H(\omega) \overline{H(\omega)} = h(e^{-i\omega}) h(e^{i\omega})$.

VIII. Portanto, podemos reescrever a fun√ß√£o de densidade espectral de $X_t$ como:
    $$S_X(\omega) = \frac{1}{2\pi} g_X(e^{-i\omega}) = \frac{1}{2\pi} h(e^{-i\omega}) h(e^{i\omega}) g_Y(e^{-i\omega}) = |H(\omega)|^2 \frac{1}{2\pi} g_Y(e^{-i\omega}) = |H(\omega)|^2 S_Y(\omega)$$

IX. Assim, a fun√ß√£o de densidade espectral da s√©rie filtrada √© o produto da magnitude ao quadrado da transformada de Fourier do filtro e a fun√ß√£o de densidade espectral da s√©rie original. ‚ñ†

### Conclus√£o

Os modelos ARMA(p, q) oferecem uma abordagem vers√°til e poderosa para modelar s√©ries temporais, combinando componentes autorregressivos e de m√©dias m√≥veis [^51]. A an√°lise cuidadosa da estacionariedade, invertibilidade e a preven√ß√£o da overparameteriza√ß√£o s√£o essenciais para a constru√ß√£o de modelos ARMA eficazes. A fun√ß√£o geradora de autocovari√¢ncia fornece uma ferramenta √∫til para analisar as propriedades de autocorrela√ß√£o de um processo ARMA [^61]. Os filtros podem ser aplicados para modificar as caracter√≠sticas de uma s√©rie temporal, e seus efeitos podem ser analisados usando a fun√ß√£o geradora de autocovari√¢ncia [^63].

### Refer√™ncias

[^51]: Y‚ÇÅ = c + œÜ1Yt-1 + $...$ + œÜpYt-p + Œµt + Œ∏1Œµt-1 + $...$ + Œ∏qŒµt-q [^51]
[^57]: Para j > q, as autocovari√¢ncias de um processo ARMA(p, q) seguem uma equa√ß√£o de diferen√ßa de ordem *p*. [^57]
[^60]: Se o operador autorregressivo e o operador de m√©dias m√≥veis tiverem ra√≠zes em comum (por exemplo, $\lambda_i = \eta_j$ para algum *i* e *j*), ambos os lados da equa√ß√£o podem ser divididos pelo fator comum. [^60]
[^61]: Para um processo estacion√°rio, a fun√ß√£o geradora de autocovari√¢ncia √© definida como $g_Y(z) = \sum_{j=-\infty}^{\infty} \gamma_j z^j$ [^61].
[^62]: A fun√ß√£o geradora de autocovari√¢ncia para um processo ARMA(p, q). [^62]
[^63]: A aplica√ß√£o de um filtro *h(L)* resulta em $g_X(z) = h(z)h(z^{-1})g_Y(z)$ [^63].
[^65]: Um processo de m√©dias m√≥veis √© invert√≠vel se puder ser reescrito como um processo autorregressivo de ordem infinita [^65].
<!-- END -->