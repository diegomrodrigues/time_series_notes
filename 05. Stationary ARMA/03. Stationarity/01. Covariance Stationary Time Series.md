## Covari√¢ncia-Estacionaridade em S√©ries Temporais

### Introdu√ß√£o
Dando continuidade ao estudo de processos estoc√°sticos e suas propriedades estat√≠sticas, este cap√≠tulo se aprofunda no conceito de **covari√¢ncia-estacionaridade**, tamb√©m conhecida como estacionaridade fraca. Essa propriedade, crucial na an√°lise de s√©ries temporais, imp√µe restri√ß√µes sobre como as caracter√≠sticas estat√≠sticas de um processo evoluem ao longo do tempo. Em particular, concentraremos nossa aten√ß√£o em processos **covari√¢ncia-estacion√°rios** onde a m√©dia e as autocovari√¢ncias n√£o dependem do tempo. Essa condi√ß√£o simplifica a an√°lise e permite a aplica√ß√£o de ferramentas estat√≠sticas baseadas em amostras √∫nicas de s√©ries temporais.

### Conceitos Fundamentais
A **estacionaridade** √© uma propriedade fundamental de s√©ries temporais que se manifesta em diferentes n√≠veis de rigor. Um processo estoc√°stico $\{Y_t\}$ √© dito ser *estritamente estacion√°rio* se a distribui√ß√£o conjunta de $(Y_{t_1}, Y_{t_2}, ..., Y_{t_n})$ √© a mesma que a distribui√ß√£o conjunta de $(Y_{t_1+h}, Y_{t_2+h}, ..., Y_{t_n+h})$ para qualquer $t_1, t_2, ..., t_n$ e qualquer $h$. Intuitivamente, isso significa que as propriedades estat√≠sticas do processo s√£o invariantes a transla√ß√µes no tempo.

Uma forma mais branda de estacionaridade √© a **covari√¢ncia-estacionaridade** (ou estacionaridade fraca), que imp√µe condi√ß√µes menos restritivas. Um processo $\{Y_t\}$ √© covari√¢ncia-estacion√°rio se satisfaz as seguintes condi√ß√µes:

1.  A m√©dia do processo √© constante ao longo do tempo:
    $$E[Y_t] = \mu \quad \text{para todo } t$$
2.  A autocovari√¢ncia entre $Y_t$ e $Y_{t-j}$ depende apenas da defasagem $j$ e n√£o de $t$:
    $$E[(Y_t - \mu)(Y_{t-j} - \mu)] = \gamma_j \quad \text{para todo } t \text{ e qualquer } j$$

Em outras palavras, para um processo covari√¢ncia-estacion√°rio, a m√©dia $\mu$ √© um valor constante e a fun√ß√£o de autocovari√¢ncia $\gamma_j$ √© uma fun√ß√£o apenas da diferen√ßa de tempo $j$.  A fun√ß√£o $\gamma_j$ √© denominada **fun√ß√£o de autocovari√¢ncia**. Note que $\gamma_0$ representa a vari√¢ncia do processo, ou seja, $\gamma_0 = E[(Y_t - \mu)^2] = Var(Y_t)$.

> üí° **Exemplo Num√©rico:** Suponha que temos uma s√©rie temporal $\{Y_t\}$ com $Y_t = 5 + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco com m√©dia 0 e vari√¢ncia 1. Aqui, $E[Y_t] = 5$ para todo $t$, mostrando que a m√©dia √© constante. Al√©m disso, $E[(Y_t - 5)(Y_{t-j} - 5)] = E[\epsilon_t \epsilon_{t-j}]$. Se $j=0$, ent√£o $E[\epsilon_t^2] = 1$, e se $j \neq 0$, ent√£o $E[\epsilon_t \epsilon_{t-j}] = 0$. Portanto, a autocovari√¢ncia depende apenas de $j$ e n√£o de $t$.

**Teorema 1:** *A fun√ß√£o de autocovari√¢ncia $\gamma_j$ de um processo covari√¢ncia-estacion√°rio √© uma fun√ß√£o par, ou seja, $\gamma_j = \gamma_{-j}$ para todo $j$.*

**Prova:** Provaremos que para um processo covari√¢ncia-estacion√°rio, sua fun√ß√£o de autocovari√¢ncia satisfaz $\gamma_j = \gamma_{-j}$.

I. Iniciamos com a defini√ß√£o da fun√ß√£o de autocovari√¢ncia:
   $$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$$

II. Agora, considere $\gamma_{-j}$:
    $$\gamma_{-j} = E[(Y_t - \mu)(Y_{t+j} - \mu)]$$

III. Para tornar a nota√ß√£o consistente, podemos mudar o √≠ndice de tempo na segunda equa√ß√£o.  Seja $k = t+j$, ent√£o $t = k-j$. Substituindo:
     $$\gamma_{-j} = E[(Y_{k-j} - \mu)(Y_{k} - \mu)]$$

IV. Devido √† propriedade comutativa da multiplica√ß√£o, podemos reescrever a express√£o dentro da esperan√ßa:
    $$\gamma_{-j} = E[(Y_{k} - \mu)(Y_{k-j} - \mu)]$$

V. Observando que a express√£o em IV √© id√™ntica √† express√£o em I (apenas com um √≠ndice diferente), conclu√≠mos que:
   $$\gamma_j = \gamma_{-j}$$
   Portanto, a fun√ß√£o de autocovari√¢ncia √© uma fun√ß√£o par. ‚ñ†

**Exemplo:** Considere o processo de ru√≠do branco Gaussiano, onde $Y_t = \epsilon_t$ e $\epsilon_t \sim N(0, \sigma^2)$ s√£o vari√°veis aleat√≥rias independentes e identicamente distribu√≠das (i.i.d.). Neste caso, $E[Y_t] = E[\epsilon_t] = 0$ para todo $t$. A autocovari√¢ncia √© dada por:

$$
E[Y_tY_{t-j}] = E[\epsilon_t \epsilon_{t-j}] =
\begin{cases}
\sigma^2, & \text{se } j=0 \\
0, & \text{se } j \neq 0
\end{cases}
$$

Portanto, o ru√≠do branco Gaussiano √© um processo covari√¢ncia-estacion√°rio, j√° que a m√©dia e a autocovari√¢ncia n√£o dependem de $t$.

> üí° **Exemplo Num√©rico:** Seja $\sigma^2 = 4$. Ent√£o, para o ru√≠do branco gaussiano, $\gamma_0 = 4$ e $\gamma_j = 0$ para $j \neq 0$. Isso significa que a vari√¢ncia do processo √© constante e igual a 4, e a covari√¢ncia entre quaisquer dois pontos no tempo diferentes √© 0.

**Defini√ß√£o:** A **fun√ß√£o de autocorrela√ß√£o** (FAC) de um processo covari√¢ncia-estacion√°rio √© definida como:

$$
\rho_j = \frac{\gamma_j}{\gamma_0} = \frac{E[(Y_t - \mu)(Y_{t-j} - \mu)]}{Var(Y_t)}
$$

Note que $\rho_0 = 1$ e, como $\gamma_j$ √© uma fun√ß√£o par, $\rho_j$ tamb√©m √© uma fun√ß√£o par, ou seja, $\rho_j = \rho_{-j}$. Al√©m disso, $|\rho_j| \leq 1$ para todo $j$.

**Lema 1.1:** *Para um processo covari√¢ncia-estacion√°rio, a fun√ß√£o de autocorrela√ß√£o $\rho_j$ satisfaz $|\rho_j| \leq 1$ para todo $j$.*

**Prova:** Provaremos que a fun√ß√£o de autocorrela√ß√£o $\rho_j$ de um processo covari√¢ncia-estacion√°rio tem magnitude menor ou igual a 1.

I. Come√ßamos com a desigualdade de Cauchy-Schwarz aplicada a vari√°veis aleat√≥rias centradas:
   $$|E[(Y_t - \mu)(Y_{t-j} - \mu)]| \leq \sqrt{E[(Y_t - \mu)^2] E[(Y_{t-j} - \mu)^2]}$$

II. Reconhecemos que $E[(Y_t - \mu)^2]$ e $E[(Y_{t-j} - \mu)^2]$ representam a vari√¢ncia de $Y_t$ e $Y_{t-j}$, respectivamente.  Para um processo covari√¢ncia-estacion√°rio, essas vari√¢ncias s√£o iguais e denotadas por $\gamma_0$. Assim:
    $$|\gamma_j| \leq \sqrt{\gamma_0 \gamma_0} = \gamma_0$$

III. Dividimos ambos os lados da desigualdade por $\gamma_0$ (que √© a vari√¢ncia e, portanto, positiva) para obter a fun√ß√£o de autocorrela√ß√£o:
     $$\left|\frac{\gamma_j}{\gamma_0}\right| \leq \frac{\gamma_0}{\gamma_0} = 1$$

IV. Pela defini√ß√£o da fun√ß√£o de autocorrela√ß√£o $\rho_j = \frac{\gamma_j}{\gamma_0}$, conclu√≠mos:
   $$|\rho_j| \leq 1$$
   Portanto, a magnitude da fun√ß√£o de autocorrela√ß√£o √© sempre menor ou igual a 1. ‚ñ†

**Exemplo:** Para o processo de ru√≠do branco Gaussiano, a fun√ß√£o de autocorrela√ß√£o √© dada por:

$$
\rho_j =
\begin{cases}
1, & \text{se } j=0 \\
0, & \text{se } j \neq 0
\end{cases}
$$

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior com ru√≠do branco gaussiano e $\sigma^2 = 4$, temos $\gamma_0 = 4$. Portanto, $\rho_0 = \gamma_0 / \gamma_0 = 4/4 = 1$, e $\rho_j = \gamma_j / \gamma_0 = 0/4 = 0$ para $j \neq 0$. Isso confirma que a autocorrela√ß√£o √© 1 no lag 0 e 0 em todos os outros lags.

**Rela√ß√£o entre estacionaridade estrita e covari√¢ncia-estacionaridade:** Se um processo √© estritamente estacion√°rio e possui momentos de segunda ordem finitos (isto √©, m√©dia e vari√¢ncia finitas), ent√£o ele √© tamb√©m covari√¢ncia-estacion√°rio. No entanto, a rec√≠proca n√£o √© necessariamente verdadeira. Um processo pode ser covari√¢ncia-estacion√°rio sem ser estritamente estacion√°rio [^3]. Isso pode ocorrer, por exemplo, se momentos de ordem superior (al√©m da m√©dia e autocovari√¢ncia) dependem do tempo, mesmo que a m√©dia e a autocovari√¢ncia sejam constantes.

**Import√¢ncia da covari√¢ncia-estacionaridade:** A covari√¢ncia-estacionaridade √© uma propriedade desej√°vel em s√©ries temporais, pois permite que a an√°lise estat√≠stica seja realizada com base em uma √∫nica realiza√ß√£o da s√©rie temporal. Se um processo √© covari√¢ncia-estacion√°rio, podemos estimar a m√©dia e a fun√ß√£o de autocovari√¢ncia a partir de uma amostra da s√©rie temporal, e essas estimativas ser√£o representativas do processo em qualquer per√≠odo de tempo. Isso justifica o uso de estimativas de m√©dias amostrais e autocovari√¢ncias amostrais para inferir propriedades do processo estoc√°stico subjacente [^3].

**Transforma√ß√µes para estacionaridade:** Muitas s√©ries temporais encontradas na pr√°tica n√£o s√£o estacion√°rias. Nesses casos, √© comum aplicar transforma√ß√µes aos dados para torn√°-los aproximadamente estacion√°rios. Algumas transforma√ß√µes comuns incluem:

*   **Diferencia√ß√£o:** Calcular a diferen√ßa entre valores consecutivos da s√©rie temporal. Isso pode remover tend√™ncias lineares ou polinomiais.
*   **Transforma√ß√£o logar√≠tmica:** Aplicar o logaritmo aos dados. Isso pode estabilizar a vari√¢ncia e linearizar rela√ß√µes n√£o lineares.
*   **Defla√ß√£o:** Dividir a s√©rie temporal por um √≠ndice de pre√ßos. Isso pode remover os efeitos da infla√ß√£o.

> üí° **Exemplo Num√©rico:** Suponha que temos uma s√©rie temporal de pre√ßos de a√ß√µes que cresce linearmente ao longo do tempo: $Y_t = 10 + 2t + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco. Esta s√©rie n√£o √© estacion√°ria porque sua m√©dia $E[Y_t] = 10 + 2t$ depende do tempo. Ao aplicar a diferencia√ß√£o, obtemos uma nova s√©rie $Z_t = Y_t - Y_{t-1} = (10 + 2t + \epsilon_t) - (10 + 2(t-1) + \epsilon_{t-1}) = 2 + \epsilon_t - \epsilon_{t-1}$.  A m√©dia de $Z_t$ √© $E[Z_t] = 2$, que √© constante.  A autocovari√¢ncia de $Z_t$ tamb√©m n√£o depende de t, tornando $Z_t$ covari√¢ncia-estacion√°ria.

![Generated plot](./../images/plot_6.png)

Ap√≥s aplicar essas transforma√ß√µes, √© importante verificar se a s√©rie resultante √© aproximadamente estacion√°ria. Isso pode ser feito visualmente (por meio de gr√°ficos da s√©rie temporal e de sua fun√ß√£o de autocorrela√ß√£o) ou por meio de testes estat√≠sticos de estacionaridade, como o teste de Dickey-Fuller aumentado (ADF).

### Conclus√£o
A covari√¢ncia-estacionaridade √© um conceito central na an√°lise de s√©ries temporais. Ela simplifica a an√°lise e permite que infer√™ncias sobre o processo estoc√°stico sejam feitas a partir de uma √∫nica realiza√ß√£o da s√©rie temporal. Embora nem todas as s√©ries temporais sejam estacion√°rias, muitas podem ser transformadas para se tornarem aproximadamente estacion√°rias, permitindo a aplica√ß√£o de ferramentas estat√≠sticas baseadas em estacionaridade.

### Refer√™ncias
[^3]: Retomado dos conceitos abordados anteriormente.
<!-- END -->