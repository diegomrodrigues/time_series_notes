## Estacionaridade Estrita em S√©ries Temporais

### Introdu√ß√£o
Este cap√≠tulo expande a discuss√£o sobre estacionaridade em s√©ries temporais, introduzindo o conceito de **estacionaridade estrita**. Enquanto a covari√¢ncia-estacionaridade [^3] se concentra nos momentos de primeira e segunda ordem (m√©dia e autocovari√¢ncias), a estacionaridade estrita imp√µe uma condi√ß√£o mais forte, exigindo que a distribui√ß√£o conjunta de qualquer conjunto de observa√ß√µes da s√©rie temporal seja invariante a deslocamentos no tempo. Veremos como a estacionaridade estrita se relaciona com a covari√¢ncia-estacionaridade e exploraremos suas implica√ß√µes para a modelagem de s√©ries temporais.

### Conceitos Fundamentais
Retomando a discuss√£o sobre estacionaridade, recordemos que um processo estoc√°stico $\{Y_t\}$ √© dito ser **estritamente estacion√°rio** se, para quaisquer inteiros $j_1, j_2, ..., j_n$, a distribui√ß√£o conjunta de $(Y_t, Y_{t+j_1}, ..., Y_{t+j_n})$ depende apenas dos intervalos que separam as datas ($j_1, j_2, ..., j_n$) e n√£o da data $t$ propriamente dita [^3]. Formalmente:

$$
F(Y_t, Y_{t+j_1}, \ldots, Y_{t+j_n}) = F(Y_{t+h}, Y_{t+j_1+h}, \ldots, Y_{t+j_n+h})
$$

para todo $t, h, j_1, ..., j_n$, onde $F(\cdot)$ denota a fun√ß√£o de distribui√ß√£o conjunta.

Em termos mais intuitivos, a estacionaridade estrita implica que o "formato" da s√©rie temporal, em termos de suas caracter√≠sticas probabil√≠sticas, √© o mesmo em qualquer per√≠odo de tempo. N√£o importa quando observamos a s√©rie, as rela√ß√µes estat√≠sticas entre as observa√ß√µes em diferentes defasagens permanecem constantes.

> üí° **Exemplo:** Considere um processo onde $Y_t$ √© uma vari√°vel aleat√≥ria de Bernoulli com probabilidade de sucesso $p$. Se $p$ √© constante ao longo do tempo, ent√£o o processo √© estritamente estacion√°rio, pois a distribui√ß√£o de qualquer sequ√™ncia de $Y_t$'s depende apenas de $p$ e n√£o de $t$.

**Rela√ß√£o com Covari√¢ncia-Estacionaridade:**

A rela√ß√£o entre estacionaridade estrita e covari√¢ncia-estacionaridade √© importante. Se um processo √© estritamente estacion√°rio e possui momentos de primeira e segunda ordem finitos (m√©dia e vari√¢ncia finitas), ent√£o ele √© tamb√©m covari√¢ncia-estacion√°rio [^3]. A prova √© direta: se a distribui√ß√£o conjunta de $(Y_t, Y_{t-j})$ n√£o depende de $t$, ent√£o a m√©dia $E[Y_t]$ e a autocovari√¢ncia $E[(Y_t - \mu)(Y_{t-j} - \mu)]$ tamb√©m n√£o dependem de $t$.

**Prova:**
Para mostrar que a estacionaridade estrita implica covari√¢ncia-estacionaridade quando os momentos de primeira e segunda ordem existem, precisamos provar que a m√©dia $E[Y_t]$ e a autocovari√¢ncia $Cov(Y_t, Y_{t-j})$ s√£o independentes de $t$.
I.  Dado que $\{Y_t\}$ √© estritamente estacion√°rio, a fun√ß√£o de distribui√ß√£o de $Y_t$ √© a mesma para todo $t$. Ou seja, $F_{Y_t}(y) = F_{Y_{t+h}}(y)$ para todo $t$ e $h$.
II. Portanto, a m√©dia $E[Y_t] = \int y \, dF_{Y_t}(y)$ √© a mesma para todo $t$, pois a integral √© calculada em rela√ß√£o √† mesma fun√ß√£o de distribui√ß√£o. Assim, $E[Y_t] = \mu$, onde $\mu$ √© uma constante.
III. Similarmente, a distribui√ß√£o conjunta de $(Y_t, Y_{t-j})$ √© a mesma para todo $t$. Portanto, $F_{Y_t, Y_{t-j}}(y_1, y_2) = F_{Y_{t+h}, Y_{t+h-j}}(y_1, y_2)$ para todo $t$ e $h$.
IV. A autocovari√¢ncia $Cov(Y_t, Y_{t-j}) = E[(Y_t - \mu)(Y_{t-j} - \mu)] = \iint (y_1 - \mu)(y_2 - \mu) \, dF_{Y_t, Y_{t-j}}(y_1, y_2)$ √© ent√£o independente de $t$, pois a integral √© calculada em rela√ß√£o √† mesma fun√ß√£o de distribui√ß√£o conjunta.
V. Portanto, se $\{Y_t\}$ √© estritamente estacion√°rio e tem momentos de primeira e segunda ordem finitos, ent√£o ele √© tamb√©m covari√¢ncia-estacion√°rio. ‚ñ†

No entanto, o inverso n√£o √© necessariamente verdadeiro. Um processo pode ser covari√¢ncia-estacion√°rio sem ser estritamente estacion√°rio [^3]. Isso significa que a m√©dia e a autocovari√¢ncia podem ser constantes ao longo do tempo, mas momentos de ordem superior (por exemplo, assimetria, curtose) ou outras caracter√≠sticas da distribui√ß√£o podem variar com o tempo.

> üí° **Exemplo:** Considere um processo onde $Y_t$ √© uma vari√°vel aleat√≥ria com m√©dia zero e vari√¢ncia 1, mas cuja distribui√ß√£o muda ao longo do tempo. Por exemplo, $Y_t$ pode seguir uma distribui√ß√£o normal $N(0, 1)$ para $t < 100$ e uma distribui√ß√£o uniforme no intervalo $[-\sqrt{3}, \sqrt{3}]$ para $t \geq 100$. Ambos os processos t√™m a mesma m√©dia e vari√¢ncia, mas diferentes momentos de ordem superior e, portanto, n√£o s√£o estritamente estacion√°rios. No entanto, se definirmos dois processos separados: $Y_{t_1} \sim N(0, 1)$ para $t_1 < 100$ e $Y_{t_2} \sim U[-\sqrt{3}, \sqrt{3}]$ para $t_2 \geq 100$, ent√£o cada um desses processos individualmente √© estritamente estacion√°rio.

> üí° **Exemplo Num√©rico:** Para ilustrar a diferen√ßa entre estacionaridade estrita e covari√¢ncia-estacionaridade, considere um processo $Y_t$ definido da seguinte forma:
>
> *   Para $t < 500$, $Y_t \sim N(0, 1)$.
> *   Para $t \geq 500$, $Y_t \sim N(0, 4)$.
>
> Este processo n√£o √© estritamente estacion√°rio, pois a distribui√ß√£o de $Y_t$ muda no tempo $t = 500$. No entanto, se calcularmos a m√©dia e a autocovari√¢ncia amostral para um per√≠odo suficientemente longo, elas podem parecer aproximadamente constantes, especialmente se o per√≠odo anterior a $t = 500$ for relativamente curto em compara√ß√£o com o comprimento total da s√©rie.
>
> Para visualizar isso, podemos simular o processo em Python:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Definir semente para reprodutibilidade
> np.random.seed(42)
>
> # Comprimento da s√©rie temporal
> T = 1000
>
> # Gerar dados
> Y = np.random.normal(0, 1, T)
> Y[500:] = np.random.normal(0, 2, T - 500)
>
> # Calcular m√©dia e vari√¢ncia amostral
> media_amostral = np.mean(Y)
> variancia_amostral = np.var(Y)
>
> print(f"M√©dia Amostral: {media_amostral:.4f}")
> print(f"Vari√¢ncia Amostral: {variancia_amostral:.4f}")
>
> # Plotar a s√©rie temporal
> plt.figure(figsize=(10, 6))
> plt.plot(Y)
> plt.title("S√©rie Temporal N√£o Estritamente Estacion√°ria")
> plt.xlabel("Tempo (t)")
> plt.ylabel("Y_t")
> plt.grid(True)
> plt.show()
>
> # Plotar a fun√ß√£o de autocorrela√ß√£o (ACF)
> from statsmodels.graphics.tsaplots import plot_acf
>
> plt.figure(figsize=(10, 6))
> plot_acf(Y, lags=40, title="Fun√ß√£o de Autocorrela√ß√£o (ACF)")
> plt.xlabel("Lag")
> plt.ylabel("Autocorrela√ß√£o")
> plt.grid(True)
> plt.show()
> ```
>
> Este c√≥digo gera uma s√©rie temporal que muda de distribui√ß√£o no meio do per√≠odo. A m√©dia amostral e a vari√¢ncia amostral s√£o calculadas, mostrando que, apesar da mudan√ßa na distribui√ß√£o, esses momentos podem parecer relativamente est√°veis. O gr√°fico da s√©rie temporal mostra a mudan√ßa na variabilidade, e o ACF pode n√£o ser significativamente diferente de zero para lags maiores, indicando uma aparente estacionaridade de covari√¢ncia.

**Teorema:** *Se $\{Y_t\}$ √© uma sequ√™ncia de vari√°veis aleat√≥rias independentes e identicamente distribu√≠das (i.i.d.), ent√£o $\{Y_t\}$ √© estritamente estacion√°ria.*

**Prova:**
Seja $\{Y_t\}$ uma sequ√™ncia de vari√°veis aleat√≥rias i.i.d. Ent√£o, a fun√ß√£o de distribui√ß√£o conjunta de qualquer conjunto de vari√°veis $Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n}$ √© dada por:

$$F(Y_{t_1} = y_1, Y_{t_2} = y_2, \ldots, Y_{t_n} = y_n) = P(Y_{t_1} = y_1)P(Y_{t_2} = y_2)\cdots P(Y_{t_n} = y_n)$$

Como as vari√°veis s√£o identicamente distribu√≠das, $P(Y_t = y) = P(Y_{t+h} = y)$ para qualquer $t$ e $h$. Portanto,

$$F(Y_{t_1+h} = y_1, Y_{t_2+h} = y_2, \ldots, Y_{t_n+h} = y_n) = P(Y_{t_1+h} = y_1)P(Y_{t_2+h} = y_2)\cdots P(Y_{t_n+h} = y_n)$$

$$= P(Y_{t_1} = y_1)P(Y_{t_2} = y_2)\cdots P(Y_{t_n} = y_n) = F(Y_{t_1} = y_1, Y_{t_2} = y_2, \ldots, Y_{t_n} = y_n)$$

Isto mostra que a distribui√ß√£o conjunta n√£o depende de $t$, e portanto, o processo √© estritamente estacion√°rio. $\blacksquare$

**Teorema 1:** *Se $\{Y_t\}$ √© uma sequ√™ncia de vari√°veis aleat√≥rias independentes, mas n√£o necessariamente identicamente distribu√≠das, com m√©dia e vari√¢ncia constantes ao longo do tempo, ent√£o o processo $\{Y_t\}$ √© covari√¢ncia-estacion√°rio, mas n√£o necessariamente estritamente estacion√°rio.*

**Prova:**
Seja $\{Y_t\}$ uma sequ√™ncia de vari√°veis aleat√≥rias independentes com $E[Y_t] = \mu$ e $Var[Y_t] = \sigma^2$ para todo $t$. Como as vari√°veis s√£o independentes, $Cov(Y_t, Y_s) = 0$ para $t \neq s$. Para $t = s$, $Cov(Y_t, Y_s) = Var(Y_t) = \sigma^2$. Portanto, a autocovari√¢ncia depende apenas da diferen√ßa entre $t$ e $s$, e o processo √© covari√¢ncia-estacion√°rio.

Para mostrar que o processo n√£o √© necessariamente estritamente estacion√°rio, considere o caso em que $Y_t \sim N(\mu, \sigma^2)$ para $t < t_0$ e $Y_t \sim U(\mu - \frac{\sqrt{3}}{2}\sigma, \mu + \frac{\sqrt{3}}{2}\sigma)$ para $t \geq t_0$. Embora a m√©dia e a vari√¢ncia sejam constantes, as distribui√ß√µes s√£o diferentes, o que significa que a distribui√ß√£o conjunta de qualquer conjunto de vari√°veis $Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n}$ depende de $t$ e, portanto, o processo n√£o √© estritamente estacion√°rio. $\blacksquare$

**Processos Gaussianos:** Um caso especial importante √© o de **processos Gaussianos**. Um processo $\{Y_t\}$ √© dito ser Gaussiano se a distribui√ß√£o conjunta de qualquer conjunto finito de vari√°veis $(Y_{t_1}, Y_{t_2}, ..., Y_{t_n})$ √© uma distribui√ß√£o normal multivariada. Para um processo Gaussiano, a estacionaridade estrita √© equivalente √† covari√¢ncia-estacionaridade. Isso ocorre porque a distribui√ß√£o normal multivariada √© completamente caracterizada por sua m√©dia e matriz de covari√¢ncia. Se a m√©dia e a matriz de covari√¢ncia s√£o constantes ao longo do tempo, ent√£o a distribui√ß√£o conjunta √© constante ao longo do tempo, e o processo √© estritamente estacion√°rio. [^3]

> üí° **Exemplo Num√©rico:** Suponha que temos um processo Gaussiano $\{Y_t\}$ onde $Y_t \sim N(\mu, \sigma^2)$ e $Cov(Y_t, Y_{t+k}) = \rho^k \sigma^2$, com $\mu = 2$, $\sigma^2 = 1$, e $\rho = 0.5$. Para verificar a estacionaridade estrita, precisamos garantir que a distribui√ß√£o conjunta de qualquer conjunto de vari√°veis seja invariante no tempo. Para simplificar, consideremos duas vari√°veis: $Y_t$ e $Y_{t+1}$. A distribui√ß√£o conjunta dessas vari√°veis √© uma distribui√ß√£o normal bivariada com m√©dias $\mu_1 = \mu_2 = 2$, vari√¢ncias $\sigma_1^2 = \sigma_2^2 = 1$, e covari√¢ncia $\sigma_{12} = \rho \sigma^2 = 0.5$.
>
> Agora, consideremos as vari√°veis $Y_{t+h}$ e $Y_{t+1+h}$. A distribui√ß√£o conjunta dessas vari√°veis tamb√©m √© uma distribui√ß√£o normal bivariada com m√©dias $\mu_1 = \mu_2 = 2$, vari√¢ncias $\sigma_1^2 = \sigma_2^2 = 1$, e covari√¢ncia $\sigma_{12} = \rho \sigma^2 = 0.5$. Como as distribui√ß√µes conjuntas s√£o id√™nticas para qualquer valor de $t$ e $h$, o processo √© estritamente estacion√°rio.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> mu = 2
> sigma = 1
> rho = 0.5
>
> # Fun√ß√£o para gerar dados de um processo Gaussiano
> def generate_gaussian_process(T, mu, sigma, rho):
>     # Inicializar a s√©rie temporal
>     Y = np.zeros(T)
>     Y[0] = np.random.normal(mu, sigma)
>
>     # Gerar os valores subsequentes
>     for t in range(1, T):
>         Y[t] = mu + rho * (Y[t-1] - mu) + np.random.normal(0, sigma * np.sqrt(1 - rho**2))
>
>     return Y
>
> # Gerar a s√©rie temporal
> T = 100
> Y = generate_gaussian_process(T, mu, sigma, rho)
>
> # Plotar a s√©rie temporal
> plt.figure(figsize=(10, 6))
> plt.plot(Y)
> plt.title("Processo Gaussiano Estritamente Estacion√°rio")
> plt.xlabel("Tempo (t)")
> plt.ylabel("Y_t")
> plt.grid(True)
> plt.show()
>
> # Calcular e plotar a fun√ß√£o de autocorrela√ß√£o
> from statsmodels.graphics.tsaplots import plot_acf
>
> plt.figure(figsize=(10, 6))
> plot_acf(Y, lags=20, title="Fun√ß√£o de Autocorrela√ß√£o (ACF)")
> plt.xlabel("Lag")
> plt.ylabel("Autocorrela√ß√£o")
> plt.grid(True)
> plt.show()
> ```
>
> Este c√≥digo simula um processo Gaussiano e plota a s√©rie temporal e a fun√ß√£o de autocorrela√ß√£o. O ACF mostra um decaimento exponencial t√≠pico de um processo AR(1), consistente com a covari√¢ncia-estacionaridade e, devido √† natureza Gaussiana, tamb√©m com a estacionaridade estrita.

**Lema:** *Se um processo Gaussiano √© covari√¢ncia-estacion√°rio, ent√£o ele tamb√©m √© estritamente estacion√°rio.*

**Prova:**
Seja $\{Y_t\}$ um processo Gaussiano covari√¢ncia-estacion√°rio. Isso significa que a m√©dia $\mu = E[Y_t]$ √© constante e a fun√ß√£o de autocovari√¢ncia $\gamma_{|t-s|} = Cov[Y_t, Y_s]$ depende apenas da diferen√ßa entre $t$ e $s$.

Para provar que o processo √© estritamente estacion√°rio, precisamos mostrar que a distribui√ß√£o conjunta de qualquer conjunto finito de vari√°veis $Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n}$ √© invariante a deslocamentos no tempo. Como $\{Y_t\}$ √© um processo Gaussiano, essa distribui√ß√£o conjunta √© uma distribui√ß√£o normal multivariada, completamente caracterizada por seu vetor de m√©dias e sua matriz de covari√¢ncia.

Seja $\mathbf{Y} = (Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n})'$ e considere o vetor $\mathbf{Y_h} = (Y_{t_1+h}, Y_{t_2+h}, \ldots, Y_{t_n+h})'$ para algum inteiro $h$.

I. O vetor de m√©dias de $\mathbf{Y}$ √© $\mathbf{\mu} = (\mu, \mu, \ldots, \mu)'$ (j√° que $E[Y_t] = \mu$ para todo $t$). Similarmente, o vetor de m√©dias de $\mathbf{Y_h}$ √© tamb√©m $\mathbf{\mu}$, pois a m√©dia √© constante devido √† covari√¢ncia-estacionaridade.
II. A matriz de covari√¢ncia de $\mathbf{Y}$ √© uma matriz $\Sigma$ cujos elementos s√£o $\Sigma_{ij} = Cov[Y_{t_i}, Y_{t_j}] = \gamma_{|t_i - t_j|}$. Da mesma forma, a matriz de covari√¢ncia de $\mathbf{Y_h}$ √© uma matriz $\Sigma'$ cujos elementos s√£o $\Sigma'_{ij} = Cov[Y_{t_i+h}, Y_{t_j+h}] = \gamma_{|(t_i+h) - (t_j+h)|} = \gamma_{|t_i - t_j|}$. Portanto, $\Sigma' = \Sigma$.
III. Como os vetores de m√©dias e as matrizes de covari√¢ncia de $\mathbf{Y}$ e $\mathbf{Y_h}$ s√£o id√™nticos, as distribui√ß√µes normais multivariadas de $\mathbf{Y}$ e $\mathbf{Y_h}$ s√£o id√™nticas.
IV. Isto demonstra que a distribui√ß√£o conjunta de qualquer conjunto finito de vari√°veis $Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n}$ √© invariante a deslocamentos no tempo, e portanto, o processo √© estritamente estacion√°rio. $\blacksquare$

**Lema 1.1:** *Se um processo √© estritamente estacion√°rio e seus momentos de primeira e segunda ordem existem, ent√£o ele √© tamb√©m covari√¢ncia estacion√°rio.*

**Prova:**
Seja $\{Y_t\}$ um processo estritamente estacion√°rio, e suponha que $E[Y_t] = \mu$ e $Var(Y_t) = \sigma^2$ existem e s√£o finitos.  A estacionaridade estrita implica que a distribui√ß√£o conjunta de $(Y_t, Y_{t+j})$ √© a mesma que a distribui√ß√£o conjunta de $(Y_{t+h}, Y_{t+j+h})$ para quaisquer $t, h, j$.

Portanto, $E[Y_t] = E[Y_{t+h}] = \mu$ para todo $t$ e $h$, o que significa que a m√©dia √© constante.

A autocovari√¢ncia entre $Y_t$ e $Y_{t+j}$ √© dada por:
$Cov(Y_t, Y_{t+j}) = E[(Y_t - \mu)(Y_{t+j} - \mu)]$.
Devido √† estacionaridade estrita, a distribui√ß√£o conjunta de $(Y_t, Y_{t+j})$ √© a mesma para qualquer $t$.  Consequentemente, a autocovari√¢ncia $Cov(Y_t, Y_{t+j})$ n√£o depende de $t$, mas apenas do lag $j$.
Assim, $Cov(Y_t, Y_{t+j}) = \gamma(j)$, onde $\gamma(j)$ √© a fun√ß√£o de autocovari√¢ncia, que depende apenas de $j$.
Portanto, o processo √© covari√¢ncia estacion√°rio. $\blacksquare$

**Import√¢ncia da Estacionaridade Estrita:**

Embora a covari√¢ncia-estacionaridade seja suficiente para muitas aplica√ß√µes pr√°ticas, a estacionaridade estrita √© uma condi√ß√£o mais fundamental que garante que todas as propriedades estat√≠sticas do processo s√£o invariantes no tempo. Isso √© particularmente importante em situa√ß√µes onde se deseja modelar n√£o apenas a m√©dia e a autocovari√¢ncia, mas tamb√©m outros aspectos da distribui√ß√£o, como a probabilidade de eventos extremos ou a forma da distribui√ß√£o condicional.

**Exemplo:** Em modelagem financeira, a suposi√ß√£o de estacionaridade estrita pode ser relevante ao analisar retornos de ativos, especialmente quando se procura modelar a cauda da distribui√ß√£o (para an√°lise de risco) ou quando se utiliza modelos de c√≥pulas para capturar depend√™ncias n√£o lineares entre diferentes ativos. Se a distribui√ß√£o dos retornos muda ao longo do tempo (por exemplo, devido a mudan√ßas no ambiente regulat√≥rio ou na estrutura do mercado), ent√£o a suposi√ß√£o de estacionaridade estrita pode ser violada, e modelos que n√£o levam em conta essas mudan√ßas podem produzir resultados imprecisos.

> üí° **Exemplo Num√©rico:** Considere uma s√©rie de retornos di√°rios de a√ß√µes. Suponha que, antes de uma crise financeira (digamos, antes de 2008), os retornos sigam uma distribui√ß√£o normal com m√©dia 0 e desvio padr√£o de 1% ($N(0, 0.01^2)$). Ap√≥s a crise, os retornos podem exibir maior volatilidade e seguir uma distribui√ß√£o normal com m√©dia 0 e desvio padr√£o de 2% ($N(0, 0.02^2)$). Embora a m√©dia permane√ßa constante, a mudan√ßa no desvio padr√£o (e, portanto, na vari√¢ncia) indica que a s√©rie n√£o √© estritamente estacion√°ria, pois a distribui√ß√£o dos retornos muda ao longo do tempo.
>
> Podemos simular isso para ilustrar:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Definir semente para reprodutibilidade
> np.random.seed(42)
>
> # Comprimento da s√©rie temporal
> T = 1000
>
> # Gerar retornos antes e depois da crise
> retornos = np.random.normal(0, 0.01, T)
> retornos[T//2:] = np.random.normal(0, 0.02, T - T//2)
>
> # Plotar a s√©rie temporal
> plt.figure(figsize=(10, 6))
> plt.plot(retornos)
> plt.title("Retornos de A√ß√µes N√£o Estritamente Estacion√°rios (Crise)")
> plt.xlabel("Tempo (Dias)")
> plt.ylabel("Retorno Di√°rio")
> plt.grid(True)
> plt.show()
>
> # Plotar histogramas antes e depois da crise
> plt.figure(figsize=(12, 6))
> plt.subplot(1, 2, 1)
> plt.hist(retornos[:T//2], bins=30, density=True, alpha=0.7, label="Antes da Crise")
> plt.title("Distribui√ß√£o Antes da Crise")
> plt.xlabel("Retorno")
> plt.ylabel("Densidade")
> plt.legend()
>
> plt.subplot(1, 2, 2)
> plt.hist(retornos[T//2:], bins=30, density=True, alpha=0.7, label="Ap√≥s a Crise")
> plt.title("Distribui√ß√£o Ap√≥s a Crise")
> plt.xlabel("Retorno")
> plt.ylabel("Densidade")
> plt.legend()
>
> plt.tight_layout()
> plt.show()
> ```
>
> Os histogramas mostram claramente que as distribui√ß√µes dos retornos antes e depois da crise s√£o diferentes, evidenciando a n√£o estacionaridade estrita.

**Exemplo:** Em modelagem financeira, a suposi√ß√£o de estacionaridade estrita pode ser relevante ao analisar retornos de ativos, especialmente quando se procura modelar a cauda da distribui√ß√£o (para an√°lise de risco) ou quando se utiliza modelos de c√≥pulas para capturar depend√™ncias n√£o lineares entre diferentes ativos. Se a distribui√ß√£o dos retornos muda ao longo do tempo (por exemplo, devido a mudan√ßas no ambiente regulat√≥rio ou na estrutura do mercado), ent√£o a suposi√ß√£o de estacionaridade estrita pode ser violada, e modelos que n√£o levam em conta essas mudan√ßas podem produzir resultados imprecisos.

**Exemplo Num√©rico:** Considere um processo em que $Y_t$ segue uma distribui√ß√£o normal com m√©dia 0, mas a vari√¢ncia varia dependendo do tempo. Por exemplo, $\sigma_t^2 = 1$ se $t$ √© par, e $\sigma_t^2 = 2$ se $t$ √© √≠mpar. Este processo √© covari√¢ncia-estacion√°rio (a m√©dia √© 0 e a autocovari√¢ncia depende apenas do lag), mas n√£o √© estritamente estacion√°rio, pois a distribui√ß√£o em si muda a cada per√≠odo.

### Conclus√£o
A estacionaridade estrita representa uma condi√ß√£o mais forte do que a covari√¢ncia-estacionaridade, garantindo a invari√¢ncia temporal de todas as caracter√≠sticas estat√≠sticas do processo. Embora a covari√¢ncia-estacionaridade seja suficiente para muitas aplica√ß√µes, a estacionaridade estrita √© crucial em cen√°rios onde se deseja modelar aspectos mais detalhados da distribui√ß√£o ou onde se espera que as mudan√ßas na distribui√ß√£o ao longo do tempo tenham um impacto significativo nos resultados.

### Refer√™ncias
[^3]: Retomado dos conceitos abordados anteriormente.
<!-- END -->