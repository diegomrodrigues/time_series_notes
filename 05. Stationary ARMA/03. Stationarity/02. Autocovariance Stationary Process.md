## Autocovari√¢ncia e Simetria em Processos Covari√¢ncia-Estacion√°rios

### Introdu√ß√£o
Este cap√≠tulo se aprofunda nas propriedades da autocovari√¢ncia em processos covari√¢ncia-estacion√°rios, com √™nfase na sua depend√™ncia exclusiva da defasagem e na propriedade de simetria. Expandindo os conceitos j√° apresentados [^3], exploraremos as implica√ß√µes dessas caracter√≠sticas para a modelagem e an√°lise de s√©ries temporais.

### Conceitos Fundamentais
Como vimos anteriormente [^3], um processo $\{Y_t\}$ √© dito ser **covari√¢ncia-estacion√°rio** se sua m√©dia $E[Y_t] = \mu$ √© constante para todo $t$, e sua autocovari√¢ncia $E[(Y_t - \mu)(Y_{t-j} - \mu)] = \gamma_j$ depende apenas da defasagem $j$ e n√£o do tempo $t$. A fun√ß√£o $\gamma_j$ √© a fun√ß√£o de autocovari√¢ncia do processo.

O foco deste cap√≠tulo √© em duas propriedades cruciais da fun√ß√£o de autocovari√¢ncia em processos covari√¢ncia-estacion√°rios:

1.  **Depend√™ncia da Defasagem:** A autocovari√¢ncia entre $Y_t$ e $Y_{t-j}$ depende unicamente da defasagem $j$. Em outras palavras, a for√ßa da rela√ß√£o linear entre dois pontos no tempo √© determinada apenas pela dist√¢ncia entre eles no tempo, e n√£o pela localiza√ß√£o desses pontos no tempo. Isso permite que se fa√ßa infer√™ncias sobre a estrutura de depend√™ncia do processo observando as autocovari√¢ncias em diferentes defasagens.

2.  **Simetria:** A fun√ß√£o de autocovari√¢ncia $\gamma_j$ √© uma fun√ß√£o par, ou seja, $\gamma_j = \gamma_{-j}$ para todo $j$. Isso significa que a autocovari√¢ncia entre $Y_t$ e $Y_{t-j}$ √© a mesma que a autocovari√¢ncia entre $Y_t$ e $Y_{t+j}$. Intuitivamente, isso reflete a ideia de que a rela√ß√£o entre dois pontos no tempo √© a mesma, independentemente de qual ponto vem antes ou depois.

Para formalizar a demonstra√ß√£o da propriedade de simetria:

**Prova da Simetria da Autocovari√¢ncia:** Provaremos que para um processo covari√¢ncia-estacion√°rio, $\gamma_j = \gamma_{-j}$.

I.  Come√ßamos com a defini√ß√£o da autocovari√¢ncia para uma defasagem $j$:
    $$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$$

II. Agora, considere a autocovari√¢ncia para uma defasagem $-j$:
    $$\gamma_{-j} = E[(Y_t - \mu)(Y_{t+j} - \mu)]$$

III. Definimos uma nova vari√°vel $s = t + j$. Isso implica que $t = s - j$. Substitu√≠mos $t$ por $s - j$ na express√£o para $\gamma_{-j}$:
     $$\gamma_{-j} = E[(Y_{s-j} - \mu)(Y_{s} - \mu)]$$

IV. Reorganizamos os termos dentro da esperan√ßa:
    $$\gamma_{-j} = E[(Y_{s} - \mu)(Y_{s-j} - \mu)]$$

V. Devido √† estacionaridade do processo, a autocovari√¢ncia depende apenas da defasagem e n√£o do tempo. Portanto, podemos reescrever a express√£o acima como:
   $$\gamma_{-j} = E[(Y_{t} - \mu)(Y_{t-j} - \mu)]$$

VI. Notamos que a express√£o em V √© id√™ntica √† express√£o em I:
    $$\gamma_{-j} = \gamma_j$$

Portanto, a fun√ß√£o de autocovari√¢ncia √© sim√©trica, ou seja, $\gamma_j = \gamma_{-j}$ para todo $j$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um processo com m√©dia $\mu = 5$ e valores $Y_1 = 7, Y_2 = 8, Y_3 = 6, Y_4 = 9$. Para calcular $\gamma_1$, precisamos de $E[(Y_t - \mu)(Y_{t-1} - \mu)]$. Aproximadamente, isso seria $((Y_2 - \mu)(Y_1 - \mu) + (Y_3 - \mu)(Y_2 - \mu) + (Y_4 - \mu)(Y_3 - \mu)) / 3 = ((8-5)(7-5) + (6-5)(8-5) + (9-5)(6-5)) / 3 = (3*2 + 1*3 + 4*1) / 3 = (6+3+4)/3 = 13/3 \approx 4.33$. Similarmente, $\gamma_{-1}$ seria calculado usando as mesmas diferen√ßas de pares adjacentes, mas em ordem inversa. Devido √† simetria, esperar√≠amos um valor pr√≥ximo a 4.33 tamb√©m.

> üí° **Exemplo Num√©rico:** Considere um processo AR(1) estacion√°rio dado por $Y_t = \phi Y_{t-1} + \epsilon_t$, onde $|\phi| < 1$ e $\epsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$. A autocovari√¢ncia em defasagem 1 √© $\gamma_1 = E[(Y_t - \mu)(Y_{t-1} - \mu)]$, e a autocovari√¢ncia em defasagem -1 √© $\gamma_{-1} = E[(Y_t - \mu)(Y_{t+1} - \mu)]$. Devido √† estacionaridade e √† propriedade comutativa da covari√¢ncia, $\gamma_1 = \gamma_{-1}$.

**Teorema 2:** *Se $\{Y_t\}$ √© um processo covari√¢ncia-estacion√°rio, ent√£o a fun√ß√£o de autocorrela√ß√£o $\rho_j = \frac{\gamma_j}{\gamma_0}$ tamb√©m √© uma fun√ß√£o par, ou seja, $\rho_j = \rho_{-j}$ para todo $j$.*

**Prova:** Dado que $\rho_j = \frac{\gamma_j}{\gamma_0}$ e $\gamma_j = \gamma_{-j}$ para um processo covari√¢ncia-estacion√°rio, provaremos que $\rho_j$ tamb√©m √© uma fun√ß√£o par.

I. Iniciamos com a defini√ß√£o da fun√ß√£o de autocorrela√ß√£o para defasagem $j$:
   $$\rho_j = \frac{\gamma_j}{\gamma_0}$$

II. Agora, considere a fun√ß√£o de autocorrela√ß√£o para defasagem $-j$:
    $$\rho_{-j} = \frac{\gamma_{-j}}{\gamma_0}$$

III. Sabemos que, para um processo covari√¢ncia-estacion√°rio, $\gamma_j = \gamma_{-j}$. Portanto, podemos substituir $\gamma_{-j}$ por $\gamma_j$ na express√£o para $\rho_{-j}$:
     $$\rho_{-j} = \frac{\gamma_j}{\gamma_0}$$

IV. Observando que a express√£o em III √© id√™ntica √† express√£o em I, conclu√≠mos que:
   $$\rho_j = \rho_{-j}$$
   Portanto, a fun√ß√£o de autocorrela√ß√£o √© uma fun√ß√£o par. $\blacksquare$

**Teorema 2.1:** *Se $\{Y_t\}$ √© um processo covari√¢ncia-estacion√°rio com fun√ß√£o de autocovari√¢ncia $\gamma_j$, ent√£o $\gamma_0 \geq |\gamma_j|$ para todo $j$.*

**Prova:**
Considere a vari√¢ncia de $Y_t + Y_{t-j}$:
$$Var(Y_t + Y_{t-j}) = Var(Y_t) + Var(Y_{t-j}) + 2Cov(Y_t, Y_{t-j})$$
Como o processo √© covari√¢ncia-estacion√°rio, $Var(Y_t) = Var(Y_{t-j}) = \gamma_0$ e $Cov(Y_t, Y_{t-j}) = \gamma_j$. Assim,
$$Var(Y_t + Y_{t-j}) = 2\gamma_0 + 2\gamma_j$$
Como a vari√¢ncia √© sempre n√£o negativa, temos $2\gamma_0 + 2\gamma_j \geq 0$, o que implica $\gamma_0 \geq -\gamma_j$.

Agora, considere a vari√¢ncia de $Y_t - Y_{t-j}$:
$$Var(Y_t - Y_{t-j}) = Var(Y_t) + Var(Y_{t-j}) - 2Cov(Y_t, Y_{t-j})$$
$$Var(Y_t - Y_{t-j}) = 2\gamma_0 - 2\gamma_j$$
Novamente, como a vari√¢ncia √© sempre n√£o negativa, temos $2\gamma_0 - 2\gamma_j \geq 0$, o que implica $\gamma_0 \geq \gamma_j$.

Combinando as duas desigualdades, $\gamma_0 \geq \gamma_j$ e $\gamma_0 \geq -\gamma_j$, conclu√≠mos que $\gamma_0 \geq |\gamma_j|$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que $\gamma_0 = 10$. Ent√£o, de acordo com o Teorema 2.1, $|\gamma_j|$ deve ser menor ou igual a 10 para todo $j$. Se calcularmos $\gamma_1$ e obtivermos um valor de 12, isso indicaria que o processo n√£o √© covari√¢ncia-estacion√°rio ou que houve um erro no c√°lculo. Se $\gamma_1 = 6$, ent√£o a condi√ß√£o $\gamma_0 \geq |\gamma_1|$ √© satisfeita.

**Implica√ß√µes da Simetria:** A propriedade de simetria da fun√ß√£o de autocovari√¢ncia (e autocorrela√ß√£o) tem implica√ß√µes importantes na modelagem de s√©ries temporais. Ela implica que, ao estimar a fun√ß√£o de autocorrela√ß√£o a partir de dados, √© suficiente considerar apenas defasagens n√£o negativas, pois os valores para defasagens negativas s√£o simplesmente uma imagem espelhada dos valores para defasagens positivas.

> üí° **Exemplo Num√©rico:** Suponha que temos uma s√©rie temporal e calculamos as autocorrela√ß√µes amostrais $\hat{\rho}_1 = 0.6$ e $\hat{\rho}_2 = 0.4$. Devido √† simetria, sabemos que $\hat{\rho}_{-1} = 0.6$ e $\hat{\rho}_{-2} = 0.4$.

**Exemplo:** Considere o processo MA(1) (Moving Average de ordem 1) definido por $Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$, onde $\epsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, e $\theta$ √© uma constante. Para este processo, a fun√ß√£o de autocovari√¢ncia √©:

$$
\gamma_j =
\begin{cases}
(1 + \theta^2)\sigma^2, & \text{se } j=0 \\
\theta \sigma^2, & \text{se } |j|=1 \\
0, & \text{se } |j|>1
\end{cases}
$$

A fun√ß√£o de autocorrela√ß√£o correspondente √©:

$$
\rho_j =
\begin{cases}
1, & \text{se } j=0 \\
\frac{\theta}{1 + \theta^2}, & \text{se } |j|=1 \\
0, & \text{se } |j|>1
\end{cases}
$$

Observe que tanto $\gamma_j$ quanto $\rho_j$ s√£o fun√ß√µes pares, satisfazendo $\gamma_j = \gamma_{-j}$ e $\rho_j = \rho_{-j}$.

> üí° **Exemplo Num√©rico:** No processo MA(1), se $\theta = 0.5$ e $\sigma^2 = 1$, ent√£o $\gamma_0 = (1 + 0.5^2) * 1 = 1.25$ e $\gamma_1 = 0.5 * 1 = 0.5$. Devido √† simetria, $\gamma_{-1} = 0.5$. Para $|j| > 1$, $\gamma_j = 0$. A autocorrela√ß√£o $\rho_1 = \frac{0.5}{1 + 0.5^2} = \frac{0.5}{1.25} = 0.4$. Portanto, $\rho_{-1} = 0.4$ tamb√©m.

**Proposi√ß√£o 3:** Para um processo covari√¢ncia-estacion√°rio $\{Y_t\}$, a fun√ß√£o de autocovari√¢ncia $\gamma_j$ √© n√£o-negativa definida.

**Prova:** Seja $a_1, a_2, \ldots, a_n$ um conjunto de n√∫meros reais arbitr√°rios e considere a seguinte vari√¢ncia:

$$Var\left(\sum_{t=1}^{n} a_t Y_t\right) = E\left[\left(\sum_{t=1}^{n} a_t Y_t\right)^2\right] - \left(E\left[\sum_{t=1}^{n} a_t Y_t\right]\right)^2$$

Como a vari√¢ncia √© sempre n√£o-negativa, temos:

$$Var\left(\sum_{t=1}^{n} a_t Y_t\right) \geq 0$$

Expandindo a vari√¢ncia, obtemos:

$$Var\left(\sum_{t=1}^{n} a_t Y_t\right) = E\left[\sum_{t=1}^{n} \sum_{s=1}^{n} a_t a_s Y_t Y_s\right] - \left(\sum_{t=1}^{n} a_t E[Y_t]\right)^2 = \sum_{t=1}^{n} \sum_{s=1}^{n} a_t a_s E[Y_t Y_s] - \left(\sum_{t=1}^{n} a_t \mu\right)^2$$

Como $Cov(Y_t, Y_s) = E[Y_t Y_s] - E[Y_t]E[Y_s]$ e $E[Y_t] = \mu$ para todo $t$, podemos escrever $E[Y_t Y_s] = Cov(Y_t, Y_s) + \mu^2 = \gamma_{|t-s|} + \mu^2$. Substituindo na express√£o da vari√¢ncia, obtemos:

$$Var\left(\sum_{t=1}^{n} a_t Y_t\right) = \sum_{t=1}^{n} \sum_{s=1}^{n} a_t a_s (\gamma_{|t-s|} + \mu^2) - \mu^2 \left(\sum_{t=1}^{n} a_t\right)^2 = \sum_{t=1}^{n} \sum_{s=1}^{n} a_t a_s \gamma_{|t-s|} + \mu^2\left(\sum_{t=1}^{n} a_t\right)^2 - \mu^2\left(\sum_{t=1}^{n} a_t\right)^2$$
$$Var\left(\sum_{t=1}^{n} a_t Y_t\right) = \sum_{t=1}^{n} \sum_{s=1}^{n} a_t a_s \gamma_{|t-s|} \geq 0$$

Isso mostra que a fun√ß√£o de autocovari√¢ncia $\gamma_j$ √© n√£o-negativa definida. $\blacksquare$

**Autocovari√¢ncia Amostral:** Na pr√°tica, a fun√ß√£o de autocovari√¢ncia te√≥rica $\gamma_j$ √© geralmente desconhecida e deve ser estimada a partir de uma amostra da s√©rie temporal. A autocovari√¢ncia amostral √© definida como:

$$
\hat{\gamma}_j = \frac{1}{T} \sum_{t=j+1}^{T} (Y_t - \bar{Y})(Y_{t-j} - \bar{Y})
$$

onde $\bar{Y} = \frac{1}{T} \sum_{t=1}^{T} Y_t$ √© a m√©dia amostral. Similarmente, a autocorrela√ß√£o amostral √© definida como:

$$
\hat{\rho}_j = \frac{\hat{\gamma}_j}{\hat{\gamma}_0}
$$

Devido √† natureza da estimativa amostral, a propriedade de simetria da fun√ß√£o de autocovari√¢ncia te√≥rica √© aproximadamente preservada na autocovari√¢ncia amostral, ou seja, $\hat{\gamma}_j \approx \hat{\gamma}_{-j}$.

> üí° **Exemplo Num√©rico:** Em uma s√©rie temporal de 100 pontos, calculamos $\hat{\gamma}_0 = 2.5$ e $\hat{\gamma}_1 = 1.5$. Ent√£o, $\hat{\rho}_1 = 1.5 / 2.5 = 0.6$. Devido √† simetria, podemos assumir com boa aproxima√ß√£o que $\hat{\gamma}_{-1} \approx 1.5$ e $\hat{\rho}_{-1} \approx 0.6$.

> üí° **Exemplo Num√©rico:** Imagine uma s√©rie temporal com os seguintes 5 valores: 2, 4, 6, 4, 2. A m√©dia amostral $\bar{Y}$ √© (2+4+6+4+2)/5 = 3.6. Para calcular $\hat{\gamma}_1$, usar√≠amos a f√≥rmula: $\hat{\gamma}_1 = \frac{1}{5} * [(4-3.6)(2-3.6) + (6-3.6)(4-3.6) + (4-3.6)(6-3.6) + (2-3.6)(4-3.6)] = \frac{1}{5} * [0.4*(-1.6) + 2.4*0.4 + 0.4*2.4 + (-1.6)*0.4] = \frac{1}{5} * [-0.64 + 0.96 + 0.96 - 0.64] = \frac{1}{5} * 0.64 = 0.128$.

**Limita√ß√µes:** √â importante notar que a covari√¢ncia-estacionaridade √© uma condi√ß√£o idealizada. Na pr√°tica, poucas s√©ries temporais s√£o perfeitamente covari√¢ncia-estacion√°rias. No entanto, muitas s√©ries temporais s√£o *aproximadamente* covari√¢ncia-estacion√°rias, o que significa que suas propriedades estat√≠sticas variam lentamente ao longo do tempo. Nesses casos, a an√°lise baseada na suposi√ß√£o de covari√¢ncia-estacionaridade pode fornecer resultados √∫teis, desde que as viola√ß√µes da estacionaridade n√£o sejam muito severas.

> üí° **Exemplo Num√©rico:** Considere os pre√ßos das a√ß√µes de uma empresa de tecnologia ao longo de 10 anos. Embora a m√©dia e a vari√¢ncia dos pre√ßos possam mudar ao longo do tempo devido ao crescimento da empresa, as autocorrela√ß√µes de curto prazo (por exemplo, as autocorrela√ß√µes di√°rias ou semanais) podem ser relativamente est√°veis. Nesse caso, poder√≠amos tratar a s√©rie temporal como aproximadamente covari√¢ncia-estacion√°ria para modelar as flutua√ß√µes de curto prazo.

### Conclus√£o
A depend√™ncia da defasagem e a simetria da fun√ß√£o de autocovari√¢ncia s√£o caracter√≠sticas fundamentais de processos covari√¢ncia-estacion√°rios. Essas propriedades simplificam a an√°lise e a modelagem de s√©ries temporais, permitindo que se fa√ßam infer√™ncias sobre a estrutura de depend√™ncia do processo com base em uma √∫nica realiza√ß√£o da s√©rie temporal e explorando a simetria para reduzir a complexidade das estimativas.

### Refer√™ncias
[^3]: Retomado dos conceitos abordados anteriormente.
<!-- END -->