## AutocovariÃ¢ncia e Simetria em Processos CovariÃ¢ncia-EstacionÃ¡rios

### IntroduÃ§Ã£o
Este capÃ­tulo se aprofunda nas propriedades da autocovariÃ¢ncia em processos covariÃ¢ncia-estacionÃ¡rios, com Ãªnfase na sua dependÃªncia exclusiva da defasagem e na propriedade de simetria. Expandindo os conceitos jÃ¡ apresentados [^3], exploraremos as implicaÃ§Ãµes dessas caracterÃ­sticas para a modelagem e anÃ¡lise de sÃ©ries temporais.

### Conceitos Fundamentais
Como vimos anteriormente [^3], um processo $\{Y_t\}$ Ã© dito ser **covariÃ¢ncia-estacionÃ¡rio** se sua mÃ©dia $E[Y_t] = \mu$ Ã© constante para todo $t$, e sua autocovariÃ¢ncia $E[(Y_t - \mu)(Y_{t-j} - \mu)] = \gamma_j$ depende apenas da defasagem $j$ e nÃ£o do tempo $t$. A funÃ§Ã£o $\gamma_j$ Ã© a funÃ§Ã£o de autocovariÃ¢ncia do processo.

O foco deste capÃ­tulo Ã© em duas propriedades cruciais da funÃ§Ã£o de autocovariÃ¢ncia em processos covariÃ¢ncia-estacionÃ¡rios:

1.  **DependÃªncia da Defasagem:** A autocovariÃ¢ncia entre $Y_t$ e $Y_{t-j}$ depende unicamente da defasagem $j$. Em outras palavras, a forÃ§a da relaÃ§Ã£o linear entre dois pontos no tempo Ã© determinada apenas pela distÃ¢ncia entre eles no tempo, e nÃ£o pela localizaÃ§Ã£o desses pontos no tempo. Isso permite que se faÃ§a inferÃªncias sobre a estrutura de dependÃªncia do processo observando as autocovariÃ¢ncias em diferentes defasagens.

2.  **Simetria:** A funÃ§Ã£o de autocovariÃ¢ncia $\gamma_j$ Ã© uma funÃ§Ã£o par, ou seja, $\gamma_j = \gamma_{-j}$ para todo $j$. Isso significa que a autocovariÃ¢ncia entre $Y_t$ e $Y_{t-j}$ Ã© a mesma que a autocovariÃ¢ncia entre $Y_t$ e $Y_{t+j}$. Intuitivamente, isso reflete a ideia de que a relaÃ§Ã£o entre dois pontos no tempo Ã© a mesma, independentemente de qual ponto vem antes ou depois.

Para formalizar a demonstraÃ§Ã£o da propriedade de simetria:

**Prova da Simetria da AutocovariÃ¢ncia:** Provaremos que para um processo covariÃ¢ncia-estacionÃ¡rio, $\gamma_j = \gamma_{-j}$.

I.  ComeÃ§amos com a definiÃ§Ã£o da autocovariÃ¢ncia para uma defasagem $j$:
    $$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$$

II. Agora, considere a autocovariÃ¢ncia para uma defasagem $-j$:
    $$\gamma_{-j} = E[(Y_t - \mu)(Y_{t+j} - \mu)]$$

III. Definimos uma nova variÃ¡vel $s = t + j$. Isso implica que $t = s - j$. SubstituÃ­mos $t$ por $s - j$ na expressÃ£o para $\gamma_{-j}$:
     $$\gamma_{-j} = E[(Y_{s-j} - \mu)(Y_{s} - \mu)]$$

IV. Reorganizamos os termos dentro da esperanÃ§a:
    $$\gamma_{-j} = E[(Y_{s} - \mu)(Y_{s-j} - \mu)]$$

V. Devido Ã  estacionaridade do processo, a autocovariÃ¢ncia depende apenas da defasagem e nÃ£o do tempo. Portanto, podemos reescrever a expressÃ£o acima como:
   $$\gamma_{-j} = E[(Y_{t} - \mu)(Y_{t-j} - \mu)]$$

VI. Notamos que a expressÃ£o em V Ã© idÃªntica Ã  expressÃ£o em I:
    $$\gamma_{-j} = \gamma_j$$

Portanto, a funÃ§Ã£o de autocovariÃ¢ncia Ã© simÃ©trica, ou seja, $\gamma_j = \gamma_{-j}$ para todo $j$. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um processo com mÃ©dia $\mu = 5$ e valores $Y_1 = 7, Y_2 = 8, Y_3 = 6, Y_4 = 9$. Para calcular $\gamma_1$, precisamos de $E[(Y_t - \mu)(Y_{t-1} - \mu)]$. Aproximadamente, isso seria $((Y_2 - \mu)(Y_1 - \mu) + (Y_3 - \mu)(Y_2 - \mu) + (Y_4 - \mu)(Y_3 - \mu)) / 3 = ((8-5)(7-5) + (6-5)(8-5) + (9-5)(6-5)) / 3 = (3*2 + 1*3 + 4*1) / 3 = (6+3+4)/3 = 13/3 \approx 4.33$. Similarmente, $\gamma_{-1}$ seria calculado usando as mesmas diferenÃ§as de pares adjacentes, mas em ordem inversa. Devido Ã  simetria, esperarÃ­amos um valor prÃ³ximo a 4.33 tambÃ©m.

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um processo AR(1) estacionÃ¡rio dado por $Y_t = \phi Y_{t-1} + \epsilon_t$, onde $|\phi| < 1$ e $\epsilon_t$ Ã© ruÃ­do branco com mÃ©dia zero e variÃ¢ncia $\sigma^2$. A autocovariÃ¢ncia em defasagem 1 Ã© $\gamma_1 = E[(Y_t - \mu)(Y_{t-1} - \mu)]$, e a autocovariÃ¢ncia em defasagem -1 Ã© $\gamma_{-1} = E[(Y_t - \mu)(Y_{t+1} - \mu)]$. Devido Ã  estacionaridade e Ã  propriedade comutativa da covariÃ¢ncia, $\gamma_1 = \gamma_{-1}$.

**Teorema 2:** *Se $\{Y_t\}$ Ã© um processo covariÃ¢ncia-estacionÃ¡rio, entÃ£o a funÃ§Ã£o de autocorrelaÃ§Ã£o $\rho_j = \frac{\gamma_j}{\gamma_0}$ tambÃ©m Ã© uma funÃ§Ã£o par, ou seja, $\rho_j = \rho_{-j}$ para todo $j$.*

**Prova:** Dado que $\rho_j = \frac{\gamma_j}{\gamma_0}$ e $\gamma_j = \gamma_{-j}$ para um processo covariÃ¢ncia-estacionÃ¡rio, provaremos que $\rho_j$ tambÃ©m Ã© uma funÃ§Ã£o par.

I. Iniciamos com a definiÃ§Ã£o da funÃ§Ã£o de autocorrelaÃ§Ã£o para defasagem $j$:
   $$\rho_j = \frac{\gamma_j}{\gamma_0}$$

II. Agora, considere a funÃ§Ã£o de autocorrelaÃ§Ã£o para defasagem $-j$:
    $$\rho_{-j} = \frac{\gamma_{-j}}{\gamma_0}$$

III. Sabemos que, para um processo covariÃ¢ncia-estacionÃ¡rio, $\gamma_j = \gamma_{-j}$. Portanto, podemos substituir $\gamma_{-j}$ por $\gamma_j$ na expressÃ£o para $\rho_{-j}$:
     $$\rho_{-j} = \frac{\gamma_j}{\gamma_0}$$

IV. Observando que a expressÃ£o em III Ã© idÃªntica Ã  expressÃ£o em I, concluÃ­mos que:
   $$\rho_j = \rho_{-j}$$
   Portanto, a funÃ§Ã£o de autocorrelaÃ§Ã£o Ã© uma funÃ§Ã£o par. $\blacksquare$

**Teorema 2.1:** *Se $\{Y_t\}$ Ã© um processo covariÃ¢ncia-estacionÃ¡rio com funÃ§Ã£o de autocovariÃ¢ncia $\gamma_j$, entÃ£o $\gamma_0 \geq |\gamma_j|$ para todo $j$.*

**Prova:**
Considere a variÃ¢ncia de $Y_t + Y_{t-j}$:
$$Var(Y_t + Y_{t-j}) = Var(Y_t) + Var(Y_{t-j}) + 2Cov(Y_t, Y_{t-j})$$
Como o processo Ã© covariÃ¢ncia-estacionÃ¡rio, $Var(Y_t) = Var(Y_{t-j}) = \gamma_0$ e $Cov(Y_t, Y_{t-j}) = \gamma_j$. Assim,
$$Var(Y_t + Y_{t-j}) = 2\gamma_0 + 2\gamma_j$$
Como a variÃ¢ncia Ã© sempre nÃ£o negativa, temos $2\gamma_0 + 2\gamma_j \geq 0$, o que implica $\gamma_0 \geq -\gamma_j$.

Agora, considere a variÃ¢ncia de $Y_t - Y_{t-j}$:
$$Var(Y_t - Y_{t-j}) = Var(Y_t) + Var(Y_{t-j}) - 2Cov(Y_t, Y_{t-j})$$
$$Var(Y_t - Y_{t-j}) = 2\gamma_0 - 2\gamma_j$$
Novamente, como a variÃ¢ncia Ã© sempre nÃ£o negativa, temos $2\gamma_0 - 2\gamma_j \geq 0$, o que implica $\gamma_0 \geq \gamma_j$.

Combinando as duas desigualdades, $\gamma_0 \geq \gamma_j$ e $\gamma_0 \geq -\gamma_j$, concluÃ­mos que $\gamma_0 \geq |\gamma_j|$. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que $\gamma_0 = 10$. EntÃ£o, de acordo com o Teorema 2.1, $|\gamma_j|$ deve ser menor ou igual a 10 para todo $j$. Se calcularmos $\gamma_1$ e obtivermos um valor de 12, isso indicaria que o processo nÃ£o Ã© covariÃ¢ncia-estacionÃ¡rio ou que houve um erro no cÃ¡lculo. Se $\gamma_1 = 6$, entÃ£o a condiÃ§Ã£o $\gamma_0 \geq |\gamma_1|$ Ã© satisfeita.

**ImplicaÃ§Ãµes da Simetria:** A propriedade de simetria da funÃ§Ã£o de autocovariÃ¢ncia (e autocorrelaÃ§Ã£o) tem implicaÃ§Ãµes importantes na modelagem de sÃ©ries temporais. Ela implica que, ao estimar a funÃ§Ã£o de autocorrelaÃ§Ã£o a partir de dados, Ã© suficiente considerar apenas defasagens nÃ£o negativas, pois os valores para defasagens negativas sÃ£o simplesmente uma imagem espelhada dos valores para defasagens positivas.

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que temos uma sÃ©rie temporal e calculamos as autocorrelaÃ§Ãµes amostrais $\hat{\rho}_1 = 0.6$ e $\hat{\rho}_2 = 0.4$. Devido Ã  simetria, sabemos que $\hat{\rho}_{-1} = 0.6$ e $\hat{\rho}_{-2} = 0.4$.

**Exemplo:** Considere o processo MA(1) (Moving Average de ordem 1) definido por $Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$, onde $\epsilon_t$ Ã© ruÃ­do branco com mÃ©dia zero e variÃ¢ncia $\sigma^2$, e $\theta$ Ã© uma constante. Para este processo, a funÃ§Ã£o de autocovariÃ¢ncia Ã©:

$$
\gamma_j =
\begin{cases}
(1 + \theta^2)\sigma^2, & \text{se } j=0 \\
\theta \sigma^2, & \text{se } |j|=1 \\
0, & \text{se } |j|>1
\end{cases}
$$

A funÃ§Ã£o de autocorrelaÃ§Ã£o correspondente Ã©:

$$
\rho_j =
\begin{cases}
1, & \text{se } j=0 \\
\frac{\theta}{1 + \theta^2}, & \text{se } |j|=1 \\
0, & \text{se } |j|>1
\end{cases}
$$

Observe que tanto $\gamma_j$ quanto $\rho_j$ sÃ£o funÃ§Ãµes pares, satisfazendo $\gamma_j = \gamma_{-j}$ e $\rho_j = \rho_{-j}$.

> ğŸ’¡ **Exemplo NumÃ©rico:** No processo MA(1), se $\theta = 0.5$ e $\sigma^2 = 1$, entÃ£o $\gamma_0 = (1 + 0.5^2) * 1 = 1.25$ e $\gamma_1 = 0.5 * 1 = 0.5$. Devido Ã  simetria, $\gamma_{-1} = 0.5$. Para $|j| > 1$, $\gamma_j = 0$. A autocorrelaÃ§Ã£o $\rho_1 = \frac{0.5}{1 + 0.5^2} = \frac{0.5}{1.25} = 0.4$. Portanto, $\rho_{-1} = 0.4$ tambÃ©m.

**ProposiÃ§Ã£o 3:** Para um processo covariÃ¢ncia-estacionÃ¡rio $\{Y_t\}$, a funÃ§Ã£o de autocovariÃ¢ncia $\gamma_j$ Ã© nÃ£o-negativa definida.

**Prova:** Seja $a_1, a_2, \ldots, a_n$ um conjunto de nÃºmeros reais arbitrÃ¡rios e considere a seguinte variÃ¢ncia:

$$Var\left(\sum_{t=1}^{n} a_t Y_t\right) = E\left[\left(\sum_{t=1}^{n} a_t Y_t\right)^2\right] - \left(E\left[\sum_{t=1}^{n} a_t Y_t\right]\right)^2$$

Como a variÃ¢ncia Ã© sempre nÃ£o-negativa, temos:

$$Var\left(\sum_{t=1}^{n} a_t Y_t\right) \geq 0$$

Expandindo a variÃ¢ncia, obtemos:

$$Var\left(\sum_{t=1}^{n} a_t Y_t\right) = E\left[\sum_{t=1}^{n} \sum_{s=1}^{n} a_t a_s Y_t Y_s\right] - \left(\sum_{t=1}^{n} a_t E[Y_t]\right)^2 = \sum_{t=1}^{n} \sum_{s=1}^{n} a_t a_s E[Y_t Y_s] - \left(\sum_{t=1}^{n} a_t \mu\right)^2$$

Como $Cov(Y_t, Y_s) = E[Y_t Y_s] - E[Y_t]E[Y_s]$ e $E[Y_t] = \mu$ para todo $t$, podemos escrever $E[Y_t Y_s] = Cov(Y_t, Y_s) + \mu^2 = \gamma_{|t-s|} + \mu^2$. Substituindo na expressÃ£o da variÃ¢ncia, obtemos:

$$Var\left(\sum_{t=1}^{n} a_t Y_t\right) = \sum_{t=1}^{n} \sum_{s=1}^{n} a_t a_s (\gamma_{|t-s|} + \mu^2) - \mu^2 \left(\sum_{t=1}^{n} a_t\right)^2 = \sum_{t=1}^{n} \sum_{s=1}^{n} a_t a_s \gamma_{|t-s|} + \mu^2\left(\sum_{t=1}^{n} a_t\right)^2 - \mu^2\left(\sum_{t=1}^{n} a_t\right)^2$$
$$Var\left(\sum_{t=1}^{n} a_t Y_t\right) = \sum_{t=1}^{n} \sum_{s=1}^{n} a_t a_s \gamma_{|t-s|} \geq 0$$

Isso mostra que a funÃ§Ã£o de autocovariÃ¢ncia $\gamma_j$ Ã© nÃ£o-negativa definida. $\blacksquare$

**AutocovariÃ¢ncia Amostral:** Na prÃ¡tica, a funÃ§Ã£o de autocovariÃ¢ncia teÃ³rica $\gamma_j$ Ã© geralmente desconhecida e deve ser estimada a partir de uma amostra da sÃ©rie temporal. A autocovariÃ¢ncia amostral Ã© definida como:

$$
\hat{\gamma}_j = \frac{1}{T} \sum_{t=j+1}^{T} (Y_t - \bar{Y})(Y_{t-j} - \bar{Y})
$$

onde $\bar{Y} = \frac{1}{T} \sum_{t=1}^{T} Y_t$ Ã© a mÃ©dia amostral. Similarmente, a autocorrelaÃ§Ã£o amostral Ã© definida como:

$$
\hat{\rho}_j = \frac{\hat{\gamma}_j}{\hat{\gamma}_0}
$$

Devido Ã  natureza da estimativa amostral, a propriedade de simetria da funÃ§Ã£o de autocovariÃ¢ncia teÃ³rica Ã© aproximadamente preservada na autocovariÃ¢ncia amostral, ou seja, $\hat{\gamma}_j \approx \hat{\gamma}_{-j}$.

> ğŸ’¡ **Exemplo NumÃ©rico:** Em uma sÃ©rie temporal de 100 pontos, calculamos $\hat{\gamma}_0 = 2.5$ e $\hat{\gamma}_1 = 1.5$. EntÃ£o, $\hat{\rho}_1 = 1.5 / 2.5 = 0.6$. Devido Ã  simetria, podemos assumir com boa aproximaÃ§Ã£o que $\hat{\gamma}_{-1} \approx 1.5$ e $\hat{\rho}_{-1} \approx 0.6$.

> ğŸ’¡ **Exemplo NumÃ©rico:** Imagine uma sÃ©rie temporal com os seguintes 5 valores: 2, 4, 6, 4, 2. A mÃ©dia amostral $\bar{Y}$ Ã© (2+4+6+4+2)/5 = 3.6. Para calcular $\hat{\gamma}_1$, usarÃ­amos a fÃ³rmula: $\hat{\gamma}_1 = \frac{1}{5} * [(4-3.6)(2-3.6) + (6-3.6)(4-3.6) + (4-3.6)(6-3.6) + (2-3.6)(4-3.6)] = \frac{1}{5} * [0.4*(-1.6) + 2.4*0.4 + 0.4*2.4 + (-1.6)*0.4] = \frac{1}{5} * [-0.64 + 0.96 + 0.96 - 0.64] = \frac{1}{5} * 0.64 = 0.128$.

**LimitaÃ§Ãµes:** Ã‰ importante notar que a covariÃ¢ncia-estacionaridade Ã© uma condiÃ§Ã£o idealizada. Na prÃ¡tica, poucas sÃ©ries temporais sÃ£o perfeitamente covariÃ¢ncia-estacionÃ¡rias. No entanto, muitas sÃ©ries temporais sÃ£o *aproximadamente* covariÃ¢ncia-estacionÃ¡rias, o que significa que suas propriedades estatÃ­sticas variam lentamente ao longo do tempo. Nesses casos, a anÃ¡lise baseada na suposiÃ§Ã£o de covariÃ¢ncia-estacionaridade pode fornecer resultados Ãºteis, desde que as violaÃ§Ãµes da estacionaridade nÃ£o sejam muito severas.

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere os preÃ§os das aÃ§Ãµes de uma empresa de tecnologia ao longo de 10 anos. Embora a mÃ©dia e a variÃ¢ncia dos preÃ§os possam mudar ao longo do tempo devido ao crescimento da empresa, as autocorrelaÃ§Ãµes de curto prazo (por exemplo, as autocorrelaÃ§Ãµes diÃ¡rias ou semanais) podem ser relativamente estÃ¡veis. Nesse caso, poderÃ­amos tratar a sÃ©rie temporal como aproximadamente covariÃ¢ncia-estacionÃ¡ria para modelar as flutuaÃ§Ãµes de curto prazo.

### ConclusÃ£o
A dependÃªncia da defasagem e a simetria da funÃ§Ã£o de autocovariÃ¢ncia sÃ£o caracterÃ­sticas fundamentais de processos covariÃ¢ncia-estacionÃ¡rios. Essas propriedades simplificam a anÃ¡lise e a modelagem de sÃ©ries temporais, permitindo que se faÃ§am inferÃªncias sobre a estrutura de dependÃªncia do processo com base em uma Ãºnica realizaÃ§Ã£o da sÃ©rie temporal e explorando a simetria para reduzir a complexidade das estimativas.

### ReferÃªncias
[^3]: Retomado dos conceitos abordados anteriormente.
<!-- END -->