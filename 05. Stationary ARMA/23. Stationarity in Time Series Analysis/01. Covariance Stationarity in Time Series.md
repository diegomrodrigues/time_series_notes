## Covariance Stationarity in Time Series Analysis

### Introdu√ß√£o
Este cap√≠tulo explora o conceito de **covari√¢ncia estacion√°ria** em s√©ries temporais, um conceito fundamental para a an√°lise e modelagem de dados sequenciais. Conforme definido, uma s√©rie temporal √© dita **covariance-stationary** se sua m√©dia e autocovari√¢ncias n√£o variam com o tempo. Esta propriedade simplifica significativamente a an√°lise, permitindo que as propriedades estat√≠sticas da s√©rie sejam tratadas como constantes ao longo do tempo.

### Conceitos Fundamentais

A defini√ß√£o de **covari√¢ncia estacion√°ria** imp√µe duas condi√ß√µes principais:

1.  A esperan√ßa matem√°tica da s√©rie temporal $Y_t$ deve ser constante para todos os instantes de tempo $t$:

    $$E(Y_t) = \mu \quad \text{para todo } t$$

    Isso significa que o valor m√©dio da s√©rie temporal n√£o apresenta tend√™ncia ou sazonalidade, mantendo-se est√°vel ao longo do tempo. Este valor constante $\mu$ √© referido como a **unconditional mean** da s√©rie temporal, denotado por $\mu_t$ [^3].
    > üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal de temperaturas di√°rias medidas em graus Celsius. Se, ao longo de v√°rios anos, a temperatura m√©dia di√°ria se mant√©m consistentemente em torno de 25¬∞C, sem aumentos ou diminui√ß√µes percept√≠veis ao longo do tempo, ent√£o a m√©dia da s√©rie temporal √© constante e igual a 25¬∞C. Neste caso, $\mu = 25$. Se calcul√°ssemos a m√©dia de cada ano separadamente e observ√°ssemos que ela permanece aproximadamente 25¬∞C, isso refor√ßaria a estacionariedade da m√©dia.
2.  A autocovari√¢ncia entre $Y_t$ e $Y_{t-j}$ depende apenas da defasagem $j$ e n√£o do instante de tempo espec√≠fico $t$:

    $$E[(Y_t - \mu)(Y_{t-j} - \mu)] = \gamma_j \quad \text{para todo } t \text{ e qualquer } j$$

    Em outras palavras, a maneira como os valores da s√©rie temporal se relacionam entre si em diferentes defasagens permanece consistente ao longo do tempo. A autocovari√¢ncia $\gamma_j$ √© definida como a covari√¢ncia entre $Y_t$ e sua vers√£o defasada $Y_{t-j}$ [^3].

    > üí° **Exemplo Num√©rico:** Imagine que estamos analisando o n√∫mero de vendas de um produto diariamente. Se a autocovari√¢ncia entre as vendas de hoje e as vendas de ontem ($j=1$) for sempre aproximadamente 100, independentemente do dia espec√≠fico em que calculamos essa covari√¢ncia, ent√£o essa autocovari√¢ncia depende apenas da defasagem ($j=1$) e n√£o do tempo $t$. Similarmente, se a autocovari√¢ncia entre as vendas de hoje e as vendas de dois dias atr√°s ($j=2$) for sempre aproximadamente 50, ent√£o a autocovari√¢ncia para $j=2$ tamb√©m √© estacion√°ria. Este conceito implica que a rela√ß√£o entre os valores da s√©rie temporal em diferentes defasagens permanece consistente ao longo do tempo.
    > ```python
    > import numpy as np
    > import pandas as pd
    >
    > # Simula√ß√£o de dados estacion√°rios
    > np.random.seed(42)
    > num_days = 365
    > mean_sales = 100
    > sales_today = np.random.normal(mean_sales, 15, num_days)
    > sales_yesterday = np.roll(sales_today, 1)  # Desloca a s√©rie em 1 dia
    > sales_yesterday[0] = np.random.normal(mean_sales, 15) # Preenche o primeiro valor
    >
    > # Calcula a autocovari√¢ncia para j=1
    > autocovariance_j1 = np.mean((sales_today - mean_sales) * (sales_yesterday - mean_sales))
    > print(f"Autocovari√¢ncia (j=1): {autocovariance_j1:.2f}")
    >
    > # Simula√ß√£o de dados n√£o estacion√°rios (tend√™ncia linear)
    > trend = np.linspace(0, 50, num_days)
    > sales_non_stationary = sales_today + trend
    >
    > # Calcula a autocovari√¢ncia para j=1 nos dados n√£o estacion√°rios
    > mean_sales_non_stationary = np.mean(sales_non_stationary)
    > sales_yesterday_non_stationary = np.roll(sales_non_stationary, 1)
    > sales_yesterday_non_stationary[0] = sales_non_stationary[1] # Preenche o primeiro valor
    > autocovariance_j1_non_stationary = np.mean((sales_non_stationary - mean_sales_non_stationary) * (sales_yesterday_non_stationary - mean_sales_non_stationary))
    >
    > print(f"Autocovari√¢ncia (j=1) - N√£o estacion√°rio: {autocovariance_j1_non_stationary:.2f}")
    > ```
    > Neste exemplo, a autocovari√¢ncia para $j=1$ nos dados estacion√°rios deve ser aproximadamente constante ao longo do tempo, enquanto nos dados n√£o estacion√°rios (com tend√™ncia linear), a autocovari√¢ncia pode variar significativamente.

#### Autocovari√¢ncia e Autocorrela√ß√£o
√â importante notar que a autocovari√¢ncia $\gamma_j$ representa a covari√¢ncia entre $Y_t$ e $Y_{t-j}$ [^2]. A *j-th autocorrelation* (denotada por $\rho_j$) √© definida como a autocovari√¢ncia dividida pela vari√¢ncia:

$$\rho_j = \frac{\gamma_j}{\gamma_0}$$

onde $\gamma_0$ √© a vari√¢ncia de $Y_t$ [^6].

> üí° **Exemplo Num√©rico:** Suponha que, para uma s√©rie temporal de retornos de a√ß√µes, calculamos que a autocovari√¢ncia na defasagem 1 ($\gamma_1$) √© 0.002 e a vari√¢ncia ($\gamma_0$) √© 0.01. A autocorrela√ß√£o na defasagem 1 ($\rho_1$) seria ent√£o:
> $$\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{0.002}{0.01} = 0.2$$
> Este valor de 0.2 indica uma correla√ß√£o positiva fraca entre os retornos de hoje e os retornos de ontem.
> ```python
> import numpy as np
>
> # Exemplo de c√°lculo da autocorrela√ß√£o
> gamma_1 = 0.002  # Autocovari√¢ncia na defasagem 1
> gamma_0 = 0.01   # Vari√¢ncia
>
> rho_1 = gamma_1 / gamma_0
> print(f"Autocorrela√ß√£o (rho_1): {rho_1}")
> ```

**Proposi√ß√£o 1** A fun√ß√£o de autocorrela√ß√£o (ACF) de um processo covariance-stationary possui as seguintes propriedades:

1.  $\rho_0 = 1$
2.  $|\rho_j| \leq 1$ para todo $j$
3.  $\rho_j = \rho_{-j}$ para todo $j$ (simetria)

*Proof:*
1.  $\rho_0 = \frac{\gamma_0}{\gamma_0} = 1$ por defini√ß√£o.
2.  Pela desigualdade de Cauchy-Schwarz, $|Cov(Y_t, Y_{t-j})| \leq \sqrt{Var(Y_t)Var(Y_{t-j})}$. Como o processo √© covariance-stationary, $Var(Y_t) = Var(Y_{t-j}) = \gamma_0$. Portanto, $|\gamma_j| \leq \gamma_0$, e $|\rho_j| = |\frac{\gamma_j}{\gamma_0}| \leq 1$.
3.  $\rho_j = \frac{\gamma_j}{\gamma_0} = \frac{E[(Y_t - \mu)(Y_{t-j} - \mu)]}{\gamma_0} = \frac{E[(Y_{t-j} - \mu)(Y_t - \mu)]}{\gamma_0} = \frac{\gamma_{-j}}{\gamma_0} = \rho_{-j}$. $\blacksquare$

**Proposi√ß√£o 1.1** Se $\rho_1 = 0$ e $\rho_j = 0$ para todo $j > 1$, ent√£o o processo $Y_t$ √© um ru√≠do branco.

*Proof:* Por defini√ß√£o, $\rho_j = \frac{\gamma_j}{\gamma_0}$. Se $\rho_1 = 0$ e $\rho_j = 0$ para todo $j > 1$, ent√£o $\gamma_j = 0$ para todo $j \neq 0$. Isso significa que $Cov(Y_t, Y_{t-j}) = 0$ para todo $j \neq 0$.  Um processo com m√©dia constante e autocovari√¢ncia zero para todos os lags n√£o nulos √© definido como ru√≠do branco. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal de n√∫meros aleat√≥rios gerados a partir de uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia 1. Se calcularmos as autocorrela√ß√µes para diferentes defasagens, todos os valores devem ser pr√≥ximos de zero, exceto para a defasagem 0, que ser√° igual a 1.
> ```python
> import numpy as np
> import statsmodels.api as sm
>
> # Gera√ß√£o de ru√≠do branco
> np.random.seed(42)
> white_noise = np.random.normal(0, 1, 100)
>
> # C√°lculo da fun√ß√£o de autocorrela√ß√£o (ACF)
> acf = sm.tsa.acf(white_noise, nlags=10)
>
> print("Fun√ß√£o de Autocorrela√ß√£o (ACF):")
> print(acf)
> ```
> Neste exemplo, os valores da ACF para lags maiores que 0 devem ser pr√≥ximos de zero, confirmando que o processo √© um ru√≠do branco.

#### Implica√ß√µes da Covari√¢ncia Estacion√°ria
A estacionariedade de covari√¢ncia simplifica significativamente a an√°lise de s√©ries temporais. Se uma s√©rie √© covariance-stationary, a covariance entre $Y_t$ e $Y_{t-j}$ depende apenas da defasagem $j$ [^3]. Isso implica que:

*   As propriedades estat√≠sticas da s√©rie temporal s√£o consistentes ao longo do tempo.
*   Podemos usar dados hist√≥ricos para estimar a m√©dia e a autocovari√¢ncia da s√©rie temporal.
*   Modelos estat√≠sticos podem ser constru√≠dos com base nessas estimativas para prever valores futuros da s√©rie temporal.

**Teorema 1** (Wold Decomposition Theorem): Qualquer processo covariance-stationary pode ser representado como a soma de duas componentes n√£o correlacionadas: um processo puramente determin√≠stico e um processo puramente indetermin√≠stico (ou processo moving average infinito).

*Proof:* A demonstra√ß√£o completa do Teorema da Decomposi√ß√£o de Wold est√° al√©m do escopo desta introdu√ß√£o, mas a ideia central √© expressar a s√©rie temporal como uma combina√ß√£o linear de inova√ß√µes passadas (ru√≠do branco) e uma componente determin√≠stica que pode ser perfeitamente prevista a partir de seu hist√≥rico.  A componente determin√≠stica consiste em fun√ß√µes senoidais com frequ√™ncias espec√≠ficas, enquanto a componente indetermin√≠stica √© um processo MA($\infty$).

> üí° **Exemplo Num√©rico:** Imagine uma s√©rie temporal que representa o n√∫mero de passageiros de uma companhia a√©rea. De acordo com o Teorema de Wold, essa s√©rie pode ser decomposta em:
> 1.  Uma componente determin√≠stica que captura a sazonalidade anual (mais passageiros no ver√£o, menos no inverno).
> 2.  Uma componente indetermin√≠stica que captura flutua√ß√µes aleat√≥rias, como o impacto de eventos inesperados (e.g., greves, pandemias).
> A componente determin√≠stica pode ser modelada usando fun√ß√µes trigonom√©tricas, enquanto a componente indetermin√≠stica pode ser modelada usando um processo MA infinito.

#### Exemplos e Contraexemplos
Para ilustrar a estacionariedade de covari√¢ncia, considere os seguintes exemplos:

1.  **Processo de ru√≠do branco Gaussiano**: Se $\{Y_t\}$ representa um processo de ru√≠do branco Gaussiano com m√©dia zero e vari√¢ncia constante $\sigma^2$, ent√£o $Y_t = \mu + \epsilon_t$, onde $\mu$ √© uma constante e $\epsilon_t$ √© um ru√≠do branco Gaussiano [^1, ^5]. Neste caso, $E(Y_t) = \mu$ e $E[(Y_t - \mu)(Y_{t-j} - \mu)] = \sigma^2$ se $j=0$ e $0$ caso contr√°rio. Este processo √© covariance-stationary [^3].

    > üí° **Exemplo Num√©rico:** Suponha que geramos uma s√©rie temporal de 1000 pontos de dados a partir de uma distribui√ß√£o normal com m√©dia 5 e desvio padr√£o 2. Neste caso, $\mu = 5$ e $\sigma = 2$. A m√©dia da s√©rie temporal ser√° aproximadamente 5, e a autocovari√¢ncia ser√° aproximadamente 4 quando $j=0$ e aproximadamente 0 para todos os outros valores de $j$.
    > ```python
    > import numpy as np
    >
    > # Gera√ß√£o de ru√≠do branco Gaussiano
    > np.random.seed(42)
    > mu = 5
    > sigma = 2
    > white_noise = np.random.normal(mu, sigma, 1000)
    >
    > # C√°lculo da m√©dia e autocovari√¢ncia
    > mean = np.mean(white_noise)
    > autocovariance_0 = np.var(white_noise) # Var √© igual a autocovari√¢ncia em lag 0 para ru√≠do branco
    >
    > print(f"M√©dia: {mean:.2f}")
    > print(f"Autocovari√¢ncia (j=0): {autocovariance_0:.2f}")
    > ```

2.  **S√©rie temporal com tend√™ncia linear**: Se $Y_t = \beta t + \epsilon_t$, onde $\beta$ √© uma constante e $\epsilon_t$ √© um ru√≠do branco Gaussiano, ent√£o $E(Y_t) = \beta t$. Neste caso, a m√©dia da s√©rie temporal varia linearmente com o tempo, violando a condi√ß√£o de m√©dia constante. Portanto, esta s√©rie temporal n√£o √© covariance-stationary [^1].

    > üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal de pre√ßos de a√ß√µes que aumentam em m√©dia $0.1 por dia. Se modelarmos isso como $Y_t = 0.1t + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco, ent√£o a m√©dia da s√©rie temporal aumenta $0.1$ a cada dia. Portanto, essa s√©rie temporal n√£o √© covariance-stationary.
    > ```python
    > import numpy as np
    >
    > # Simula√ß√£o de s√©rie temporal com tend√™ncia linear
    > np.random.seed(42)
    > beta = 0.1
    > t = np.arange(100)
    > epsilon = np.random.normal(0, 1, 100)
    > Y_t = beta * t + epsilon
    >
    > # C√°lculo da m√©dia ao longo do tempo
    > mean_Y_t = np.mean(Y_t)
    > print(f"M√©dia da s√©rie temporal: {mean_Y_t:.2f}")
    > ```

3.  **MA(1) process**: Um processo MA(1) √© definido como $Y_t = \mu + \epsilon_t + \theta\epsilon_{t-1}$, onde $\epsilon_t$ √© white noise [^5, ^1]. A m√©dia √© $E(Y_t)=\mu$. A autocovari√¢ncia √© dada por $E[(Y_t - \mu)(Y_{t-j} - \mu)] = (1+\theta^2)\sigma^2$ se $j=0$, $\theta\sigma^2$ se $j=1$, e $0$ caso contr√°rio [^5]. Este processo √© covariance-stationary.

    > üí° **Exemplo Num√©rico:** Considere um processo MA(1) com $\mu = 10$, $\theta = 0.5$ e $\sigma^2 = 1$. Ent√£o, $Y_t = 10 + \epsilon_t + 0.5\epsilon_{t-1}$. Neste caso, $E(Y_t) = 10$ e a autocovari√¢ncia √© $\gamma_0 = (1 + 0.5^2) * 1 = 1.25$ quando $j=0$, $\gamma_1 = 0.5 * 1 = 0.5$ quando $j=1$, e $\gamma_j = 0$ para $j > 1$.
    > ```python
    > import numpy as np
    >
    > # Simula√ß√£o de processo MA(1)
    > np.random.seed(42)
    > mu = 10
    > theta = 0.5
    > sigma = 1
    > epsilon = np.random.normal(0, sigma, 100)
    > Y_t = mu + epsilon[1:] + theta * epsilon[:-1]
    >
    > # C√°lculo da m√©dia e autocovari√¢ncia
    > mean = np.mean(Y_t)
    > autocovariance_0 = np.var(Y_t)
    >
    > print(f"M√©dia: {mean:.2f}")
    > print(f"Autocovari√¢ncia (j=0): {autocovariance_0:.2f}")
    > ```

    *Proof:*

    Para demonstrar formalmente que um processo MA(1) √© covariance-stationary, mostraremos que sua m√©dia √© constante e sua autocovari√¢ncia depende apenas da defasagem.

    I. **M√©dia constante:**
       $E(Y_t) = E(\mu + \epsilon_t + \theta\epsilon_{t-1}) = \mu + E(\epsilon_t) + \theta E(\epsilon_{t-1})$. Como $\epsilon_t$ √© ru√≠do branco, $E(\epsilon_t) = 0$ para todo $t$. Portanto, $E(Y_t) = \mu$, que √© uma constante e n√£o depende de $t$.

    II. **Autocovari√¢ncia dependendo apenas da defasagem:**
        Calcularemos $E[(Y_t - \mu)(Y_{t-j} - \mu)]$ para diferentes valores de $j$:

        *   Para $j = 0$:
            $E[(Y_t - \mu)^2] = E[(\epsilon_t + \theta\epsilon_{t-1})^2] = E[\epsilon_t^2 + 2\theta\epsilon_t\epsilon_{t-1} + \theta^2\epsilon_{t-1}^2] = E[\epsilon_t^2] + 2\theta E[\epsilon_t\epsilon_{t-1}] + \theta^2 E[\epsilon_{t-1}^2]$.  Como $\epsilon_t$ √© ru√≠do branco, $E[\epsilon_t\epsilon_{t-1}] = 0$ e $E[\epsilon_t^2] = \sigma^2$ para todo $t$.  Assim, $E[(Y_t - \mu)^2] = \sigma^2 + \theta^2\sigma^2 = (1 + \theta^2)\sigma^2$.

        *   Para $j = 1$:
            $E[(Y_t - \mu)(Y_{t-1} - \mu)] = E[(\epsilon_t + \theta\epsilon_{t-1})(\epsilon_{t-1} + \theta\epsilon_{t-2})] = E[\epsilon_t\epsilon_{t-1} + \theta\epsilon_{t-1}^2 + \theta\epsilon_t\epsilon_{t-2} + \theta^2\epsilon_{t-1}\epsilon_{t-2}]$.  Como $\epsilon_t$ √© ru√≠do branco, $E[\epsilon_t\epsilon_{t-1}] = E[\epsilon_t\epsilon_{t-2}] = E[\epsilon_{t-1}\epsilon_{t-2}] = 0$, e $E[\epsilon_{t-1}^2] = \sigma^2$. Assim, $E[(Y_t - \mu)(Y_{t-1} - \mu)] = \theta\sigma^2$.

        *   Para $j > 1$:
            $E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\epsilon_t + \theta\epsilon_{t-1})(\epsilon_{t-j} + \theta\epsilon_{t-j-1})] = E[\epsilon_t\epsilon_{t-j} + \theta\epsilon_{t-1}\epsilon_{t-j} + \theta\epsilon_t\epsilon_{t-j-1} + \theta^2\epsilon_{t-1}\epsilon_{t-j-1}]$. Como $\epsilon_t$ √© ru√≠do branco, todos os termos envolvendo produtos de $\epsilon$ em diferentes instantes de tempo s√£o zero. Assim, $E[(Y_t - \mu)(Y_{t-j} - \mu)] = 0$.

        Portanto, a autocovari√¢ncia $E[(Y_t - \mu)(Y_{t-j} - \mu)]$ depende apenas de $j$ (a defasagem) e n√£o de $t$.  Especificamente:
        $$
        E[(Y_t - \mu)(Y_{t-j} - \mu)] =
        \begin{cases}
            (1 + \theta^2)\sigma^2 & \text{se } j = 0 \\
            \theta\sigma^2 & \text{se } j = 1 \\
            0 & \text{se } j > 1
        \end{cases}
        $$

    Como a m√©dia $E(Y_t)$ √© constante e a autocovari√¢ncia $E[(Y_t - \mu)(Y_{t-j} - \mu)]$ depende apenas de $j$, o processo MA(1) √© covariance-stationary. ‚ñ†

Para complementar os exemplos, considere um processo AR(1):

4. **AR(1) process**: Um processo AR(1) √© definido como $Y_t = \mu + \phi(Y_{t-1} - \mu) + \epsilon_t$, onde $\epsilon_t$ √© white noise com vari√¢ncia $\sigma^2$ e $|\phi|<1$. Tomando a expectativa, $E(Y_t) = \mu + \phi(E(Y_{t-1}) - \mu)$. Se o processo √© covariance stationary, ent√£o $E(Y_t) = E(Y_{t-1}) = \mu$, o que se verifica. A vari√¢ncia √© $Var(Y_t) = \phi^2Var(Y_{t-1}) + \sigma^2$. Novamente, pela estacionariedade, $Var(Y_t) = Var(Y_{t-1}) = \gamma_0$. Resolvendo para $\gamma_0$, obtemos $\gamma_0 = \frac{\sigma^2}{1-\phi^2}$. A autocovari√¢ncia $\gamma_j$ pode ser calculada como $\gamma_j = Cov(Y_t, Y_{t-j}) = E[(Y_t - \mu)(Y_{t-j}-\mu)] = \phi^j\gamma_0$ para $j \ge 0$.  Como a m√©dia e autocovari√¢ncia s√£o independentes de *t*, o processo AR(1) √© covariance-stationary quando $|\phi|<1$.
    > üí° **Exemplo Num√©rico:** Considere um processo AR(1) com $\mu = 50$, $\phi = 0.7$ e $\sigma^2 = 4$. Ent√£o, $Y_t = 50 + 0.7(Y_{t-1} - 50) + \epsilon_t$. Neste caso, a vari√¢ncia √© $\gamma_0 = \frac{4}{1-0.7^2} = \frac{4}{0.51} \approx 7.84$. A autocovari√¢ncia na defasagem 1 √© $\gamma_1 = 0.7 * 7.84 \approx 5.49$.
    > ```python
    > import numpy as np
    >
    > # Simula√ß√£o de processo AR(1)
    > np.random.seed(42)
    > mu = 50
    > phi = 0.7
    > sigma = 2
    > num_samples = 100
    >
    > epsilon = np.random.normal(0, sigma, num_samples)
    > Y_t = np.zeros(num_samples)
    > Y_t[0] = mu + epsilon[0]  # Valor inicial
    >
    > for t in range(1, num_samples):
    >     Y_t[t] = mu + phi * (Y_t[t-1] - mu) + epsilon[t]
    >
    > # C√°lculo da m√©dia e autocovari√¢ncia
    > mean = np.mean(Y_t)
    > autocovariance_0 = np.var(Y_t)
    >
    > print(f"M√©dia: {mean:.2f}")
    > print(f"Autocovari√¢ncia (j=0): {autocovariance_0:.2f}")
    > ```

**Teorema 2** Se $Y_t$ √© um processo AR(1) definido como acima e $|\phi| < 1$, ent√£o $Y_t$ √© covariance-stationary.

*Proof:* J√° mostramos no exemplo 4 acima que se $|\phi| < 1$, ent√£o a m√©dia e a autocovari√¢ncia de $Y_t$ s√£o independentes de *t*. Portanto, $Y_t$ √© covariance-stationary. $\blacksquare$

#### Estacionariedade Estrita vs. Covari√¢ncia Estacion√°ria

√â importante distinguir covariance-stationary de *strict stationarity* [^3]. Um processo √© considerado strictly stationary se a distribui√ß√£o conjunta de $(Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n})$ √© a mesma que a distribui√ß√£o conjunta de $(Y_{t_1+k}, Y_{t_2+k}, \ldots, Y_{t_n+k})$ para qualquer $k$ [^3]. Em outras palavras, a distribui√ß√£o da s√©rie temporal n√£o muda com o tempo.

Se um processo √© strictly stationary e possui momentos de segunda ordem finitos, ent√£o ele √© covariance-stationary [^3]. No entanto, o inverso n√£o √© necessariamente verdadeiro. Um processo pode ser covariance-stationary, mas n√£o strictly stationary. Por exemplo, um processo cuja m√©dia e autocovari√¢ncias s√£o constantes, mas cujos momentos de ordem superior variam com o tempo, √© covariance-stationary, mas n√£o strictly stationary [^3].

**Lema 1** Se uma s√©rie temporal $\{Y_t\}$ √© Gaussiana e covariance-stationary, ent√£o ela tamb√©m √© strictly stationary.

*Proof:* Para uma s√©rie temporal Gaussiana, a distribui√ß√£o conjunta √© completamente determinada pela sua m√©dia e fun√ß√£o de autocovari√¢ncia. Se a s√©rie √© covariance-stationary, a m√©dia √© constante e a autocovari√¢ncia depende apenas da defasagem. Portanto, a distribui√ß√£o conjunta de $(Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n})$ √© a mesma que a distribui√ß√£o conjunta de $(Y_{t_1+k}, Y_{t_2+k}, \ldots, Y_{t_n+k})$ para qualquer $k$, o que implica strict stationarity. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere duas s√©ries temporais gaussianas. A primeira √© um ru√≠do branco gaussiano com m√©dia 0 e vari√¢ncia 1. A segunda √© obtida pela transforma√ß√£o $Z_t = Y_t^3$, onde $Y_t$ √© a primeira s√©rie. Ambas as s√©ries t√™m m√©dia e vari√¢ncia constantes, mas os momentos de ordem superior (como a assimetria) s√£o diferentes. A primeira s√©rie √© strictly stationary e covariance-stationary. A segunda s√©rie √© covariance-stationary (seus primeiros dois momentos s√£o constantes), mas n√£o necessariamente strictly stationary, pois sua distribui√ß√£o n√£o √© a mesma para todos os $t$.

### Conclus√£o

A estacionariedade de covari√¢ncia √© um conceito crucial na an√°lise de s√©ries temporais, fornecendo uma base s√≥lida para modelagem e previs√£o. Ao garantir que as propriedades estat√≠sticas da s√©rie temporal permane√ßam consistentes ao longo do tempo, podemos aplicar t√©cnicas estat√≠sticas e construir modelos que capturem as caracter√≠sticas subjacentes dos dados. √â fundamental entender a diferen√ßa entre covariance-stationary e strictly stationary, bem como as implica√ß√µes de cada conceito para a an√°lise de s√©ries temporais. A compreens√£o profunda da estacionariedade √© um pr√©-requisito para a aplica√ß√£o bem-sucedida de modelos ARMA e outras t√©cnicas avan√ßadas de an√°lise de s√©ries temporais.

### Refer√™ncias
[^1]: P√°gina 44: *The expectation of the tth observation of a time series refers to the mean of this probability distribution, provided it exists: E(Yt) = integral y_t f_Y(y_t) dy*
[^2]: P√°gina 45: *From this distribution we can calculate the jth autocovariance of Yt (denoted Œ≥jt): Œ≥jt = integral...integral (y_t - Œº_t)(y_{t-1} - Œº_{t-1})...(y_{t-j} - Œº_{t-j}) fy(y_t,y_{t-1},...,y_{t-j}) dy_t dy_{t-1} dy_{t-j} = E[(Y_t - Œº_t)(Y_{t-j} - Œº_{t-j})] Note that [3.1.10] has the form of a covariance between two variables X and Y: Cov(X, Y) = E(X - Œº_X)(Y - Œº_Y)*
[^3]: P√°gina 45: *If neither the mean Œºt nor the autocovariances Œ≥jt depend on the date t, then the process for Yt is said to be covariance-stationary or weakly stationary: E(Yt) = Œº for all t E[(Yt - Œº)(Y_{t-j}) - Œº)] = Œ≥j for all t and any j*
[^4]: P√°gina 46: *In this text the term "stationary" by itself is taken to mean "covariance-stationary."*
[^5]: P√°gina 48: *Let {Œµt} be white noise as in [3.2.1] through [3.2.3], and consider the process Yt = Œº + Œµt + Œ∏Œµt-1, where u and 6 could be any constants. This time series is called a first-order moving average process, denoted MA(1)*
[^6]: P√°gina 49: *The jth autocorrelation of a covariance-stationary process (denoted œÅj) is defined as its jth autocovariance divided by the variance: œÅj = Œ≥j/Œ≥0*
<!-- END -->