## White Noise in Time Series Analysis

### Introdu√ß√£o
Como vimos anteriormente, o conceito de **white noise** √© fundamental na an√°lise de s√©ries temporais, servindo como um bloco de constru√ß√£o essencial para modelos mais complexos [^47]. J√° exploramos em profundidade as caracter√≠sticas, implica√ß√µes e aplica√ß√µes do processo de white noise, focando em suas propriedades estat√≠sticas, seu papel na filtragem de Kalman e na simula√ß√£o de modelos de s√©ries temporais. Neste cap√≠tulo, aprofundaremos ainda mais a discuss√£o sobre white noise, concentrando-nos em uma condi√ß√£o mais forte para o processo: a *independ√™ncia* atrav√©s do tempo.

### Conceitos Fundamentais
Recordemos que um processo de **white noise** $\{\varepsilon_t\}$ √© caracterizado por tr√™s propriedades essenciais: m√©dia zero, vari√¢ncia constante e n√£o correla√ß√£o [^47].  Agora, vamos considerar uma condi√ß√£o ainda mais restritiva: a *independ√™ncia estat√≠stica*.

**Independ√™ncia Across Time**
Um processo de white noise √© considerado *independente* se, para qualquer par de tempos distintos $t$ e $\tau$, as vari√°veis aleat√≥rias $\varepsilon_t$ e $\varepsilon_\tau$ s√£o estatisticamente independentes [^48].  Isto significa que o conhecimento do valor de $\varepsilon_t$ n√£o fornece informa√ß√£o sobre o valor de $\varepsilon_\tau$, e vice-versa. Formalmente, $P(\varepsilon_t | \varepsilon_\tau) = P(\varepsilon_t)$ para todo $t \neq \tau$.

> Note que a independ√™ncia implica n√£o correla√ß√£o, mas o inverso n√£o √© necessariamente verdadeiro. Duas vari√°veis podem ser n√£o correlacionadas (ter covari√¢ncia zero) sem serem independentes.

Vamos provar que a independ√™ncia implica n√£o correla√ß√£o:

**Prova:**
Provaremos que se duas vari√°veis aleat√≥rias $X$ e $Y$ s√£o independentes, ent√£o elas s√£o n√£o correlacionadas, ou seja, $Cov(X, Y) = 0$.

I. Por defini√ß√£o, a covari√¢ncia entre duas vari√°veis aleat√≥rias $X$ e $Y$ √© dada por:
   $$Cov(X, Y) = E[(X - E[X])(Y - E[Y])]$$

II. Expandindo a express√£o, obtemos:
    $$Cov(X, Y) = E[XY - XE[Y] - YE[X] + E[X]E[Y]]$$

III. Usando a linearidade do operador de esperan√ßa, podemos reescrever como:
     $$Cov(X, Y) = E[XY] - E[X]E[Y] - E[Y]E[X] + E[X]E[Y]$$

IV. Simplificando, temos:
    $$Cov(X, Y) = E[XY] - E[X]E[Y]$$

V. Se $X$ e $Y$ s√£o independentes, ent√£o $E[XY] = E[X]E[Y]$. Substituindo na equa√ß√£o acima:
   $$Cov(X, Y) = E[X]E[Y] - E[X]E[Y] = 0$$

VI. Portanto, se $X$ e $Y$ s√£o independentes, sua covari√¢ncia √© zero, o que significa que elas s√£o n√£o correlacionadas. ‚ñ†

Um processo que satisfaz as condi√ß√µes de m√©dia zero, vari√¢ncia constante e independ√™ncia √© chamado de **independent white noise process** [^48].

> üí° **Exemplo Num√©rico:** Imagine um processo de white noise onde cada $\varepsilon_t$ √© gerado jogando uma moeda justa. Se sair cara ($\varepsilon_t = 1$) e coroa ($\varepsilon_t = -1$). Cada lan√ßamento da moeda √© independente dos outros. Portanto, este √© um exemplo de independent white noise com m√©dia zero e vari√¢ncia 1.

**Gaussian White Noise**

Um caso especial importante √© o processo de **Gaussian white noise**, onde os $\varepsilon_t$ seguem uma distribui√ß√£o normal (Gaussiana) com m√©dia zero e vari√¢ncia $\sigma^2$. A densidade de probabilidade para um Gaussian white noise √© dada por [^44]:

$$
f_Y(y_t) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left[-\frac{y_t^2}{2\sigma^2}\right]
$$

Como a distribui√ß√£o normal √© completamente definida por sua m√©dia e vari√¢ncia, em um Gaussian white noise, a n√£o correla√ß√£o *implica* independ√™ncia. Ou seja, se um processo Gaussiano √© n√£o correlacionado, ent√£o ele tamb√©m √© independente. Esta propriedade simplifica a an√°lise e modelagem de muitos sistemas.

> üí° **Exemplo Num√©rico:** Suponha que temos um Gaussian white noise com $\sigma^2 = 4$. Ent√£o, a densidade de probabilidade para um valor espec√≠fico $y_t = 2$ √©:
>
> $$
> f_Y(2) = \frac{1}{\sqrt{2\pi(4)}} \exp\left[-\frac{2^2}{2(4)}\right] = \frac{1}{\sqrt{8\pi}} \exp\left[-\frac{1}{2}\right] \approx 0.176
> $$
>
> Isso significa que a probabilidade de observar um valor pr√≥ximo a 2 em um determinado momento *t* √© aproximadamente 0.176.
> ```python
> import numpy as np
> import scipy.stats as stats
>
> sigma_squared = 4
> y_t = 2
>
> probability = stats.norm.pdf(y_t, loc=0, scale=np.sqrt(sigma_squared))
> print(f"Probability density at y_t = {y_t}: {probability}")
> ```

Para ilustrar a diferen√ßa entre n√£o correla√ß√£o e independ√™ncia, considere o seguinte exemplo:

**Exemplo 1:** Suponha que $X$ seja uma vari√°vel aleat√≥ria com m√©dia zero e $Y = X^2$. Ent√£o, $Cov(X, Y) = E[XY] - E[X]E[Y] = E[X^3] - 0 \cdot E[X^2] = E[X^3]$. Se $X$ tiver uma distribui√ß√£o sim√©trica em torno de zero (como uma normal), ent√£o $E[X^3] = 0$, e $Cov(X, Y) = 0$. Portanto, $X$ e $Y$ s√£o n√£o correlacionados. No entanto, $X$ e $Y$ n√£o s√£o independentes, porque o valor de $Y$ √© completamente determinado pelo valor de $X$ (i.e., $Y = X^2$).

> üí° **Exemplo Num√©rico:** Seja $X$ uma vari√°vel aleat√≥ria normal com m√©dia 0 e desvio padr√£o 1. Geramos alguns valores e calculamos $Y = X^2$.
> ```python
> import numpy as np
>
> np.random.seed(0) # para reproducibilidade
> X = np.random.normal(0, 1, 100)
> Y = X**2
>
> covariance = np.mean((X - np.mean(X)) * (Y - np.mean(Y)))
> print(f"Covariance between X and Y: {covariance}")
>
> # Verifica a depend√™ncia: se X √© positivo ou negativo, afeta o valor de Y
> print(f"X[0]: {X[0]}, Y[0]: {Y[0]}")
> print(f"X[1]: {X[1]}, Y[1]: {Y[1]}")
> ```
> A covari√¢ncia ser√° pr√≥xima de zero, mostrando n√£o correla√ß√£o, mas os valores impressos de $X$ e $Y$ mostram que $Y$ √© completamente determinado por $X$, demonstrando depend√™ncia.

**Proposi√ß√£o 1:** *Se duas vari√°veis aleat√≥rias $X$ e $Y$ s√£o independentes, ent√£o qualquer fun√ß√£o de $X$, digamos $g(X)$, e qualquer fun√ß√£o de $Y$, digamos $h(Y)$, s√£o tamb√©m independentes.*

*Demonstra√ß√£o:* A independ√™ncia de $X$ e $Y$ implica que a distribui√ß√£o conjunta $P(X, Y) = P(X)P(Y)$. Sejam $A$ e $B$ conjuntos de valores poss√≠veis para $g(X)$ e $h(Y)$, respectivamente. Ent√£o,
$P(g(X) \in A, h(Y) \in B) = P(X \in g^{-1}(A), Y \in h^{-1}(B)) = P(X \in g^{-1}(A))P(Y \in h^{-1}(B)) = P(g(X) \in A)P(h(Y) \in B)$,
o que demonstra a independ√™ncia de $g(X)$ e $h(Y)$.

> üí° **Exemplo Num√©rico:** Se $X$ e $Y$ s√£o independentes e seguem distribui√ß√µes uniformes entre 0 e 1, ent√£o $g(X) = X^2$ e $h(Y) = \sqrt{Y}$ tamb√©m ser√£o independentes. Podemos verificar simulando e calculando a covari√¢ncia.
> ```python
> import numpy as np
>
> np.random.seed(0)
> X = np.random.uniform(0, 1, 100)
> Y = np.random.uniform(0, 1, 100)
>
> g_X = X**2
> h_Y = np.sqrt(Y)
>
> covariance = np.mean((g_X - np.mean(g_X)) * (h_Y - np.mean(h_Y)))
> print(f"Covariance between g(X) and h(Y): {covariance}")
> ```
> A covari√¢ncia deve ser pr√≥xima de zero, indicando independ√™ncia.

**Proposi√ß√£o 1.1:** *Se $X_1, X_2, ..., X_n$ s√£o vari√°veis aleat√≥rias independentes, ent√£o a fun√ß√£o conjunta de densidade (ou massa) de probabilidade √© o produto das fun√ß√µes de densidade (ou massa) de probabilidade marginais.*

*Demonstra√ß√£o:* Por defini√ß√£o de independ√™ncia, $P(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n) = P(X_1 = x_1)P(X_2 = x_2)...P(X_n = x_n)$. Portanto, a fun√ß√£o conjunta de densidade (ou massa) de probabilidade √© o produto das fun√ß√µes de densidade (ou massa) de probabilidade marginais.

> üí° **Exemplo Num√©rico:** Se $X_1$ e $X_2$ s√£o independentes e seguem distribui√ß√µes normais padr√£o, ent√£o a fun√ß√£o de densidade conjunta √© o produto das duas fun√ß√µes de densidade normais padr√£o.
>
> Seja $X_1 \sim N(0, 1)$ e $X_2 \sim N(0, 1)$. Ent√£o,
>
> $$f(x_1, x_2) = f(x_1)f(x_2) = \frac{1}{\sqrt{2\pi}}e^{-x_1^2/2} \cdot \frac{1}{\sqrt{2\pi}}e^{-x_2^2/2} = \frac{1}{2\pi}e^{-(x_1^2 + x_2^2)/2}$$

**T√©cnicas para Garantir Independ√™ncia**
Garantir que as sequ√™ncias de white noise geradas sejam independentes √© crucial para diversas aplica√ß√µes, especialmente em simula√ß√µes de Monte Carlo e m√©todos de bootstrapping [^48]. PRNGs (Pseudo-Random Number Generators) determin√≠sticos, como os discutidos anteriormente, podem apresentar correla√ß√µes sutis que comprometem a independ√™ncia.

Algumas t√©cnicas para mitigar este problema incluem:

1.  **Testes Estat√≠sticos Rigorosos:** Aplicar testes de aleatoriedade rigorosos, como o teste de Ljung-Box (j√° mencionado) e o teste de Kolmogorov-Smirnov, para verificar a independ√™ncia das amostras geradas.
2.  **Uso de N√∫meros Quase Aleat√≥rios:** Em vez de usar PRNGs, empregar sequ√™ncias de n√∫meros quase aleat√≥rios (Quasi-Monte Carlo methods), que s√£o projetadas para preencher o espa√ßo amostral de forma mais uniforme do que os n√∫meros pseudoaleat√≥rios, reduzindo as correla√ß√µes.
3.  **Embaralhamento (Shuffling):** Aplicar algoritmos de embaralhamento √† sequ√™ncia gerada pelo PRNG para quebrar correla√ß√µes residuais. O embaralhamento envolve permutar os elementos da sequ√™ncia para quebrar quaisquer padr√µes existentes.

Vamos examinar o teste de Kolmogorov-Smirnov (KS) em mais detalhes. Este teste compara a distribui√ß√£o emp√≠rica de uma amostra com uma distribui√ß√£o te√≥rica, neste caso, a distribui√ß√£o normal.

**Teorema 4 (Teste de Kolmogorov-Smirnov)**
O teste de Kolmogorov-Smirnov testa se uma amostra √© consistente com uma distribui√ß√£o especificada. Dado um conjunto de dados $X = \{x_1, x_2, \ldots, x_n\}$ e uma fun√ß√£o de distribui√ß√£o cumulativa te√≥rica $F(x)$, a estat√≠stica de KS √© definida como $D = \sup_x |F_n(x) - F(x)|$, onde $F_n(x)$ √© a fun√ß√£o de distribui√ß√£o emp√≠rica dos dados. Sob a hip√≥tese nula de que os dados s√£o amostrados da distribui√ß√£o $F(x)$, a distribui√ß√£o de $D$ √© conhecida, e podemos calcular um valor-p para o teste.

**Lema 4.1**
Para uma sequ√™ncia de Gaussian white noise, o teste de Kolmogorov-Smirnov pode ser usado para verificar se a distribui√ß√£o dos dados √© consistente com uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\sigma^2$.

> üí° **Exemplo Num√©rico:** Vamos gerar 100 valores de Gaussian white noise e aplicar o teste de Kolmogorov-Smirnov para verificar se a distribui√ß√£o √© normal com m√©dia zero e vari√¢ncia unit√°ria.
> ```python
> import numpy as np
> from scipy.stats import kstest
>
> # Gerar 100 valores aleat√≥rios de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 1
> np.random.seed(0) # Define a semente para reproducibilidade
> white_noise = np.random.normal(0, 1, 100)
>
> # Aplicar o teste de Kolmogorov-Smirnov
> ks_statistic, p_value = kstest(white_noise, 'norm') # norm √© a distribui√ß√£o normal padr√£o
>
> print("Estat√≠stica de Kolmogorov-Smirnov:", ks_statistic)
> print("Valor-p:", p_value)
> ```
> Se o valor-p for maior que um n√≠vel de signific√¢ncia (e.g., 0.05), n√£o rejeitamos a hip√≥tese nula de que a distribui√ß√£o dos dados √© consistente com a distribui√ß√£o normal.

Al√©m do teste KS, outro teste importante para verificar a independ√™ncia √© o teste de autocorrela√ß√£o.

**Teorema 5 (Teste de Autocorrela√ß√£o)**
O teste de autocorrela√ß√£o verifica se h√° correla√ß√£o entre os valores de uma s√©rie temporal em diferentes momentos no tempo. Para uma s√©rie temporal $\{x_t\}$, a autocorrela√ß√£o no lag $k$ √© definida como:
$$
\rho_k = \frac{Cov(x_t, x_{t-k})}{\sqrt{Var(x_t)Var(x_{t-k})}}
$$
Para um white noise independente, a autocorrela√ß√£o deve ser pr√≥xima de zero para todos os lags $k \neq 0$.

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal de white noise $\{x_t\}$ onde $x_t$ √© um valor aleat√≥rio retirado de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 1. Vamos calcular a autocorrela√ß√£o no lag 1.
> ```python
> import numpy as np
>
> np.random.seed(0)
> x = np.random.normal(0, 1, 100) # Simula 100 pontos de dados
>
> # Calcula a autocorrela√ß√£o amostral no lag 1
> def autocorr(x, lag):
>     n = len(x)
>     mean = np.mean(x)
>     numerator = np.sum((x[:n-lag] - mean) * (x[lag:] - mean))
>     denominator = np.sum((x - mean)**2)
>     return numerator / denominator
>
> lag = 1
> rho_1 = autocorr(x, lag)
>
> print(f"Autocorrela√ß√£o no lag {lag}: {rho_1}")
> ```
> Para um white noise ideal, $\rho_1$ deve estar pr√≥ximo de 0. Se $\rho_1$ for significativamente diferente de 0, isso sugere que os dados n√£o s√£o white noise independente.

**Lema 5.1**
Para testar a hip√≥tese nula de que uma s√©rie temporal √© um white noise independente, podemos calcular as autocorrela√ß√µes amostrais para diferentes lags e verificar se elas est√£o dentro de um intervalo de confian√ßa apropriado em torno de zero.

> üí° **Exemplo Num√©rico:**
> ```python
> import numpy as np
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
>
> # Gerar 100 valores aleat√≥rios de um white noise independente
> np.random.seed(0)
> white_noise = np.random.normal(0, 1, 100)
>
> # Calcular e plotar a fun√ß√£o de autocorrela√ß√£o (ACF)
> acf = sm.tsa.acf(white_noise, nlags=20)
> plt.stem(acf)
> plt.title("Fun√ß√£o de Autocorrela√ß√£o (ACF)")
> plt.xlabel("Lag")
> plt.ylabel("Autocorrela√ß√£o")
> plt.show()
> ```
> Se a maioria dos valores de autocorrela√ß√£o estiver dentro das bandas de confian√ßa (tipicamente $\pm 2/\sqrt{n}$, onde $n$ √© o tamanho da amostra), podemos concluir que a s√©rie temporal √© provavelmente um white noise independente.

Para complementar o teste de autocorrela√ß√£o, podemos usar o teste de Ljung-Box, que avalia a signific√¢ncia global das autocorrela√ß√µes para um conjunto de lags.

**Teorema 5.1 (Teste de Ljung-Box)**
O teste de Ljung-Box testa a hip√≥tese nula de que as autocorrela√ß√µes de uma s√©rie temporal s√£o todas zero at√© um determinado lag $m$. A estat√≠stica de Ljung-Box √© definida como:
$$
Q = n(n+2)\sum_{k=1}^{m} \frac{\hat{\rho}_k^2}{n-k}
$$
onde $n$ √© o tamanho da amostra, $m$ √© o n√∫mero de lags, e $\hat{\rho}_k$ √© a autocorrela√ß√£o amostral no lag $k$. Sob a hip√≥tese nula, $Q$ segue aproximadamente uma distribui√ß√£o qui-quadrado com $m$ graus de liberdade.

**Lema 5.2**
Para aplicar o teste de Ljung-Box, calculamos a estat√≠stica $Q$ e o valor-p correspondente. Se o valor-p for menor que um n√≠vel de signific√¢ncia (e.g., 0.05), rejeitamos a hip√≥tese nula e conclu√≠mos que h√° autocorrela√ß√£o significativa na s√©rie temporal.

> üí° **Exemplo Num√©rico:**
> ```python
> import numpy as np
> import statsmodels.api as sm
>
> # Gerar 100 valores aleat√≥rios de um white noise independente
> np.random.seed(0)
> white_noise = np.random.normal(0, 1, 100)
>
> # Aplicar o teste de Ljung-Box
> lbvalue, pvalue = sm.stats.acorr_ljungbox(white_noise, lags=[20], return_df=False)
>
> print("Estat√≠stica de Ljung-Box:", lbvalue)
> print("Valor-p:", pvalue)
> ```
> Se o valor-p for maior que o n√≠vel de signific√¢ncia, n√£o rejeitamos a hip√≥tese nula de que a s√©rie temporal √© um white noise independente.

### M√©todos de Monte Carlo e Bootstrapping

Em simula√ß√µes de Monte Carlo, o objetivo √© aproximar a solu√ß√£o de um problema atrav√©s da gera√ß√£o de amostras aleat√≥rias. A precis√£o da aproxima√ß√£o depende criticamente da qualidade dos n√∫meros aleat√≥rios utilizados. Correla√ß√µes ou padr√µes nos n√∫meros aleat√≥rios podem levar a resultados enganosos [^48].

> üí° **Exemplo Num√©rico:** Suponha que estejamos simulando o lan√ßamento de uma moeda 1000 vezes usando um gerador de n√∫meros aleat√≥rios. Se o gerador de n√∫meros aleat√≥rios tiver um vi√©s sutil, como gerar caras com uma probabilidade de 0.51 em vez de 0.5, ap√≥s 1000 lan√ßamentos, podemos observar cerca de 510 caras e 490 coroas, o que pode levar a conclus√µes incorretas sobre a justi√ßa da moeda.

Similarmente, em m√©todos de bootstrapping, o objetivo √© estimar a distribui√ß√£o amostral de uma estat√≠stica atrav√©s da reamostragem com reposi√ß√£o dos dados observados. Se as amostras originais n√£o forem independentes, o procedimento de bootstrapping pode n√£o fornecer uma representa√ß√£o precisa da verdadeira distribui√ß√£o amostral [^48].

**Exemplo 2:** Considere a estima√ß√£o do intervalo de confian√ßa para a m√©dia de uma amostra. Em um bootstrap, criamos muitas amostras bootstrap reamostrando os dados originais. Se os dados originais forem correlated white noise em vez de independent white noise, o bootstrap ir√° subestimar a variabilidade real e fornecer intervalos de confian√ßa excessivamente estreitos.

**Teorema 6:** *A converg√™ncia dos m√©todos de Monte Carlo √© afetada pela independ√™ncia das amostras geradas. Amostras independentes garantem uma converg√™ncia mais r√°pida e confi√°vel para a solu√ß√£o.*

*Discuss√£o:* A taxa de converg√™ncia do m√©todo de Monte Carlo √© tipicamente $O(1/\sqrt{N})$, onde $N$ √© o n√∫mero de amostras. Esta taxa √© garantida sob a suposi√ß√£o de que as amostras s√£o independentes e identicamente distribu√≠das (i.i.d.). Se as amostras n√£o forem independentes, a taxa de converg√™ncia pode ser mais lenta e a precis√£o da estimativa pode ser comprometida.

Vamos provar o teorema central do limite para ilustrar como a independ√™ncia afeta a converg√™ncia.

**Prova (Teorema Central do Limite - Caso i.i.d.):**
Provaremos que a m√©dia amostral de vari√°veis aleat√≥rias independentes e identicamente distribu√≠das converge para uma distribui√ß√£o normal.

I. Seja $X_1, X_2, ..., X_n$ uma sequ√™ncia de vari√°veis aleat√≥rias independentes e identicamente distribu√≠das (i.i.d.) com m√©dia $\mu$ e vari√¢ncia finita $\sigma^2$.

II. Defina a m√©dia amostral $\bar{X}_n$ como:
   $$\bar{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i$$

III. Considere a vari√°vel normalizada $Z_n$:
    $$Z_n = \frac{\bar{X}_n - \mu}{\sigma / \sqrt{n}} = \frac{\sqrt{n}(\bar{X}_n - \mu)}{\sigma}$$

IV. Queremos mostrar que $Z_n$ converge em distribui√ß√£o para uma distribui√ß√£o normal padr√£o $N(0, 1)$ quando $n \to \infty$.

V. Usando a fun√ß√£o caracter√≠stica, a fun√ß√£o caracter√≠stica de $Z_n$ √© dada por:
   $$\phi_{Z_n}(t) = E\left[e^{itZ_n}\right] = E\left[\exp\left(it\frac{\sqrt{n}(\bar{X}_n - \mu)}{\sigma}\right)\right] = E\left[\exp\left(it\frac{1}{\sqrt{n}}\sum_{i=1}^{n} \frac{(X_i - \mu)}{\sigma}\right)\right]$$

VI. Como as vari√°veis s√£o i.i.d., podemos escrever:
    $$\phi_{Z_n}(t) = \left(E\left[\exp\left(it\frac{(X_1 - \mu)}{\sigma\sqrt{n}}\right)\right]\right)^n = \left(\phi_{\frac{X_1 - \mu}{\sigma\sqrt{n}}}(t)\right)^n$$

VII. Expandindo a fun√ß√£o caracter√≠stica usando a s√©rie de Taylor em torno de $t = 0$:
     $$\phi_{\frac{X_1 - \mu}{\sigma\sqrt{n}}}(t) = 1 + \frac{itE\left[X_1 - \mu\right]}{\sigma\sqrt{n}} - \frac{t^2E\left[(X_1 - \mu)^2\right]}{2\sigma^2n} + o\left(\frac{1}{n}\right) = 1 - \frac{t^2}{2n} + o\left(\frac{1}{n}\right)$$

VIII. Portanto,
      $$\phi_{Z_n}(t) = \left(1 - \frac{t^2}{2n} + o\left(\frac{1}{n}\right)\right)^n$$

IX. Tomando o limite quando $n \to \infty$:
    $$\lim_{n \to \infty} \phi_{Z_n}(t) = \lim_{n \to \infty} \left(1 - \frac{t^2}{2n}\right)^n = e^{-t^2/2}$$

X. A fun√ß√£o caracter√≠stica $e^{-t^2/2}$ corresponde √† fun√ß√£o caracter√≠stica de uma distribui√ß√£o normal padr√£o $N(0, 1)$.

XI. Pelo teorema da continuidade de L√©vy, $Z_n$ converge em distribui√ß√£o para $N(0, 1)$ quando $n \to \infty$.  Isto demonstra que a m√©dia amostral $\bar{X}_n$ converge para uma distribui√ß√£o normal com m√©dia $\mu$ e vari√¢ncia $\sigma^2/n$. ‚ñ†

### Conclus√£o

Uma condi√ß√£o mais forte para white noise √© a independ√™ncia temporal. Isto significa que a informa√ß√£o de um determinado ponto no tempo n√£o influencia ou adiciona informa√ß√£o aos demais. Embora n√£o correla√ß√£o √© uma propriedade essencial de white noise, independ√™ncia prov√™ maiores garantias sobre o processo aleat√≥rio. T√©cnicas para gerar white noise independente s√£o essenciais para realizar simula√ß√µes de Monte Carlo e bootstrapping. Para garantir independ√™ncia, e n√£o apenas n√£o correla√ß√£o, pode-se utilizar os testes de Ljung-Box e Kolmogorov-Smirnov para garantir que o conjunto de dados est√° estatisticamente adequado para modelagens de series temporais. O teste de autocorrela√ß√£o tamb√©m se mostra como uma ferramenta importante para verificar a independ√™ncia de white noise.

### Refer√™ncias
[^44]: (p.44)
[^45]: (p.45)
[^47]: (p.47)
[^48]: (p.48)
[^49]: (p.49)
<!-- END -->