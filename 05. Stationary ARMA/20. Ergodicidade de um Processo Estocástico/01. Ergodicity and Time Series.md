## Ergodicidade em Processos Estocásticos
### Introdução
Como discutido anteriormente, a análise de séries temporais muitas vezes envolve a consideração de **médias de conjunto**, como em [^3.1.4] e [^3.1.11]. No entanto, na prática, geralmente temos acesso a apenas uma **única realização** de tamanho $T$ do processo, denotada por  $\{y^{(1)}_1, y^{(1)}_2, \ldots, y^{(1)}_T\}$ [^3.1.14]. A questão crucial é se podemos usar as **médias temporais** calculadas a partir desta única realização para inferir as propriedades estatísticas do processo como um todo. A resposta reside no conceito de **ergodicidade**, que estabelece uma ligação entre as médias temporais e as médias de conjunto. Este capítulo aprofunda o conceito de ergodicidade, explorando as condições necessárias para que um processo seja ergódico e suas implicações práticas para a análise de séries temporais.

### Conceitos Fundamentais
A ergodicidade é uma propriedade que permite que as médias temporais de um processo estocástico sejam usadas para aproximar suas médias de conjunto. Um processo **estacionário em covariância** é dito ser **ergódico para a média** se a **média temporal** [^3.1.14]
$$ \bar{y} = \frac{1}{T} \sum_{t=1}^{T} y_t $$
converge em probabilidade para a **esperança matemática** do processo, $E(Y_t)$, à medida que $T \rightarrow \infty$. Em outras palavras,
$$ \text{plim}_{T \rightarrow \infty} \frac{1}{T} \sum_{t=1}^{T} Y_t = E(Y_t) $$
Essa convergência implica que, para um processo ergódico, uma única realização suficientemente longa pode revelar as propriedades estatísticas de toda a população [^3.1.14].

A **ergodicidade para a média** está intimamente ligada ao comportamento das **autocovariâncias** do processo. De acordo com o texto [^3.1.15], um processo estacionário em covariância $\{Y_t\}$ é **ergódico para a média** se as autocovariâncias $\gamma_j$ satisfazem a condição:
$$ \sum_{j=0}^{\infty} |\gamma_j| < \infty $$
Esta condição garante que as autocovariâncias decaiam para zero rapidamente o suficiente à medida que o intervalo de tempo $j$ aumenta. Isso significa que observações remotas no tempo têm uma influência cada vez menor sobre a média temporal, permitindo que esta última convirja para a média de conjunto.

De forma similar, um processo estacionário em covariância é considerado **ergódico para segundos momentos** se a média temporal dos produtos dos desvios em relação à média converge para as respectivas autocovariâncias. Formalmente:
$$ \text{plim}_{T \rightarrow \infty} \frac{1}{T-j} \sum_{t=j+1}^{T} (Y_t - \mu)(Y_{t-j} - \mu) = \gamma_j $$
para todo $j$. A ergodicidade para segundos momentos é crucial para modelagem e previsão de séries temporais. Essa propriedade garante que podemos estimar as autocovariâncias de um processo usando médias temporais dos produtos das séries.

É importante notar que um processo pode ser **estacionário em covariância** mas não **ergódico**. O texto apresenta um exemplo onde a média $\mu_i$ para a *$i$*-ésima realização de um processo é gerada por uma distribuição normal $N(0, \lambda^2)$ [^3.1.16]. Embora esse processo seja estacionário em covariância, a média temporal converge para $\mu^{(i)}$, e não para zero, que é a média de conjunto [^3.1.16]. Este exemplo demonstra que a estacionariedade não é suficiente para a ergodicidade, e que uma condição sobre o decaimento das autocovariâncias é crucial para que um processo seja considerado ergódico.

O texto também discute o conceito de **estacionariedade estrita**. Um processo é considerado **estritamente estacionário** se a distribuição conjunta de $(Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n})$ depender apenas dos intervalos de tempo entre as observações $(j_1, j_2, \ldots, j_n)$ e não do tempo $t$. Se um processo é estritamente estacionário e tem segundos momentos finitos, ele é **estacionário em covariância**, como mencionado no texto [^3.1.13].  O texto ainda enfatiza que é possível ter um processo estacionário em covariância, mas não estritamente estacionário.

No contexto da modelagem ARMA, se um processo $\{Y_t\}$ é um processo gaussiano estacionário, então a condição [^3.1.15],  $\sum_{j=0}^{\infty} |\gamma_j| < \infty$, é suficiente para garantir a ergodicidade para todos os momentos [^3.1.15]. Isso implica que as médias temporais de quaisquer momentos do processo convergirão para os momentos correspondentes do conjunto.

O texto também define o conceito de **ruído branco**, que serve como um bloco de construção para muitos processos de séries temporais. O ruído branco é uma sequência $\{\epsilon_t\}$ com média zero, variância constante $\sigma^2$, e onde as observações não são correlacionadas [^3.2.1, 3.2.2, 3.2.3]. Se os $\epsilon_t$ forem independentes ao longo do tempo, o ruído branco é dito **independente** [^3.2.4]. Se, ainda, os $\epsilon_t$ forem normalmente distribuídos, temos **ruído branco gaussiano** [^3.2.5].

### Conclusão
A ergodicidade é uma propriedade fundamental para a análise de séries temporais, pois permite que inferências sobre o processo possam ser feitas usando apenas uma única realização da série temporal. A ergodicidade para a média, por exemplo, nos permite utilizar a média amostral para estimar a esperança matemática do processo [^3.1.14]. A condição crucial para a ergodicidade é que as autocovariâncias do processo decaiam para zero à medida que o intervalo de tempo aumenta [^3.1.15]. Em processos gaussianos estacionários, esta condição garante a ergodicidade para todos os momentos [^3.1.15]. Em resumo, a ergodicidade estabelece uma ligação essencial entre a teoria e a prática na análise de séries temporais, permitindo que as médias amostrais se tornem uma proxy das médias populacionais.

### Referências
[^3.1.3]: "The expectation of the tth observation of a time series refers to the mean of this probability distribution, provided it exists: E(Yt) = ∫ −∞ ∞ yt fYt (yt) dyt"
[^3.1.4]: "We might view this as the probability limit of the ensemble average: E(Yt) = plimI→∞ (1/I) ∑i=1 I Yt(i)"
[^3.1.5]: "For example, if {Yt}t=−∞∞ represents the sum of a constant µ plus a Gaussian white noise process {εt}t=−∞∞: Yt = µ + εt"
[^3.1.6]: "then its mean is E(Yt) = µ + E(εt) = µ."
[^3.1.11]: "Again it may be helpful to think of the jth autocovariance as the probability limit of an ensemble average: γjt = plimI→∞ (1/I) ∑i=1 I [Yt(i) − µt][Yt−j(i) − µt−j]"
[^3.1.13]: "Thus, for any covariance-stationary process, γj = γ−j for all integers j."
[^3.1.14]: "We have viewed expectations of a time series in terms of ensemble averages such as [3.1.4] and [3.1.11]. These definitions may seem a bit contrived, since usually all one has available is a single realization of size T from the process, which we earlier denoted {y1(i), y2(i), ...,yT(i)}. From these observations we would calculate the sample mean ȳ. This, of course, is not an ensemble average but rather a time average: ȳ = (1/T) ∑t=1 T yt(i)."
[^3.1.15]:  "A covariance-stationary process is said to be ergodic for the mean provided that the autocovariance γj goes to zero sufficiently quickly as j becomes large.  In Chapter 7 we will see that if the autocovariances for a covariance-stationary process satisfy ∑j=0∞ |γj| < ∞, then {Yt} is ergodic for the mean. "
[^3.1.16]: "Suppose the mean µ(i) for the ith realization {y(i)t}t=−∞∞ is generated from a N(0, λ2) distribution, say Yt(i) = µ(i) + εt"
[^3.2.1]: "The basic building block for all the processes considered in this chapter is a sequence {εt}t=−∞∞ whose elements have mean zero, E(εt)=0"
[^3.2.2]:  "and variance σ², E(εt2)=σ2"
[^3.2.3]:  "and for which the ε’s are uncorrelated across time: E(εtετ)=0 for t≠τ."
[^3.2.4]: "We shall on occasion wish to replace [3.2.3] with the slightly stronger condition that the ε’s are independent across time: εt , ετ independent for t ≠ τ."
[^3.2.5]: "Finally, if [3.2.1] through [3.2.4] hold along with εt ∼ N(0, σ2), then we have the Gaussian white noise process."
<!-- END -->
