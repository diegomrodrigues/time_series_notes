## 3.2 White Noise: Independent White Noise Process

### Introdu√ß√£o
Em continuidade ao estudo de white noise processes como blocos fundamentais na an√°lise de s√©ries temporais, este cap√≠tulo aprofunda-se em uma condi√ß√£o mais forte: a independ√™ncia entre os termos do processo [^5]. Como vimos anteriormente [^4, 5], um white noise process $\{\varepsilon_t\}$ √© caracterizado por m√©dia zero, vari√¢ncia constante e aus√™ncia de autocorrela√ß√£o. Agora, exploraremos as implica√ß√µes de fortalecer a condi√ß√£o de n√£o correla√ß√£o para independ√™ncia e como isso define um **independent white noise process** [^5].

### A Condi√ß√£o de Independ√™ncia

Enquanto a aus√™ncia de correla√ß√£o significa que n√£o h√° *rela√ß√£o linear* entre diferentes termos do processo, a independ√™ncia implica uma aus√™ncia completa de qualquer tipo de depend√™ncia estat√≠stica [^5]. Formalmente, dizemos que $\varepsilon_t$ e $\varepsilon_\tau$ s√£o independentes se o conhecimento do valor de um n√£o altera a distribui√ß√£o de probabilidade do outro, para $t \neq \tau$.

> üí° **Exemplo:** Considere dois lan√ßamentos de uma moeda n√£o viciada. O resultado de um lan√ßamento (cara ou coroa) n√£o influencia o resultado do outro. Portanto, os lan√ßamentos s√£o independentes. De forma an√°loga, em um independent white noise process, o valor de $\varepsilon_t$ em um determinado momento *t* n√£o fornece qualquer informa√ß√£o sobre o valor de $\varepsilon_\tau$ em qualquer outro momento *œÑ*, independentemente da complexidade da rela√ß√£o.

A condi√ß√£o de independ√™ncia √© matematicamente mais forte do que a de n√£o correla√ß√£o, pois independ√™ncia implica n√£o correla√ß√£o, mas o inverso n√£o √© necessariamente verdade [^5]. Isso significa que se $\varepsilon_t$ e $\varepsilon_\tau$ s√£o independentes, ent√£o $E[\varepsilon_t \varepsilon_\tau] = E[\varepsilon_t]E[\varepsilon_\tau]$, e como $E[\varepsilon_t] = E[\varepsilon_\tau] = 0$, ent√£o $E[\varepsilon_t \varepsilon_\tau] = 0$, satisfazendo a condi√ß√£o de n√£o correla√ß√£o. No entanto, a aus√™ncia de correla√ß√£o n√£o garante que n√£o haja outras formas de depend√™ncia n√£o-lineares entre $\varepsilon_t$ e $\varepsilon_\tau$.

**Defini√ß√£o:** Um **independent white noise process** √© um processo estoc√°stico $\{\varepsilon_t\}$ que satisfaz as seguintes condi√ß√µes:

1.  $E(\varepsilon_t) = 0$
2.  $E(\varepsilon_t^2) = \sigma^2$
3.  $\varepsilon_t$ e $\varepsilon_\tau$ s√£o independentes para $t \neq \tau$ [^5]

Essa defini√ß√£o refor√ßada simplifica o design de simula√ß√µes e facilita a implementa√ß√£o de diversos testes e procedimentos estat√≠sticos, conforme mencionado no contexto [^5].

**Implica√ß√µes da Independ√™ncia:**

A independ√™ncia entre os termos de um white noise process simplifica significativamente muitas an√°lises e modelagens. Por exemplo, torna mais f√°cil calcular as distribui√ß√µes de probabilidade de fun√ß√µes dos termos do processo. Al√©m disso, a independ√™ncia √© uma propriedade crucial em muitas provas de converg√™ncia e teoremas limite.

> üí° **Exemplo:** Em modelos de s√©ries temporais, a independ√™ncia do ru√≠do branco permite simplifica√ß√µes na estima√ß√£o de par√¢metros e na constru√ß√£o de intervalos de confian√ßa. Se os erros s√£o independentes, a fun√ß√£o de likelihood (verossimilhan√ßa) do modelo se torna mais f√°cil de manipular.
>
> Suponha que voc√™ tenha um modelo de regress√£o linear simples:
> $y_t = \beta_0 + \beta_1 x_t + \varepsilon_t$
> onde $\varepsilon_t$ √© um independent white noise process com $\sigma^2 = 1$. A fun√ß√£o de likelihood para este modelo, assumindo normalidade dos erros, √© dada por:
>
> $L(\beta_0, \beta_1) = \prod_{t=1}^{n} \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(y_t - \beta_0 - \beta_1 x_t)^2}{2}\right)$
>
> Devido √† independ√™ncia, podemos escrever o log-likelihood como uma soma:
>
> $\log L(\beta_0, \beta_1) = -\frac{n}{2} \log(2\pi) - \frac{1}{2} \sum_{t=1}^{n} (y_t - \beta_0 - \beta_1 x_t)^2$
>
> Maximizar o log-likelihood √© equivalente a minimizar a soma dos quadrados dos res√≠duos, o que simplifica a estima√ß√£o dos par√¢metros $\beta_0$ e $\beta_1$. Se $\varepsilon_t$ n√£o fosse independente, a fun√ß√£o de likelihood seria mais complexa e a estima√ß√£o dos par√¢metros seria mais dif√≠cil.

**Gaussian White Noise Process:**

Um caso particular importante de independent white noise process ocorre quando cada $\varepsilon_t$ segue uma distribui√ß√£o normal (gaussiana) com m√©dia zero e vari√¢ncia $\sigma^2$. Este √© chamado de **Gaussian white noise process** [^5].

A distribui√ß√£o normal √© completamente caracterizada por seus dois primeiros momentos (m√©dia e vari√¢ncia). Portanto, a independ√™ncia, juntamente com a normalidade, torna a an√°lise e a simula√ß√£o do processo muito mais trat√°veis.

**Teorema 1:** *Se $\{\varepsilon_t\}$ √© um Gaussian white noise process com vari√¢ncia $\sigma^2$, ent√£o a fun√ß√£o densidade de probabilidade conjunta de n observa√ß√µes $\varepsilon_1, \varepsilon_2, \ldots, \varepsilon_n$ √© dada por:*

$$f(\varepsilon_1, \varepsilon_2, \ldots, \varepsilon_n) = \prod_{t=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\varepsilon_t^2}{2\sigma^2}\right) = (2\pi\sigma^2)^{-n/2} \exp\left(-\frac{\sum_{t=1}^{n} \varepsilon_t^2}{2\sigma^2}\right)$$

*Prova:* A prova decorre diretamente da independ√™ncia das vari√°veis $\varepsilon_t$ e do fato de cada uma seguir uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\sigma^2$. A fun√ß√£o densidade conjunta √© o produto das fun√ß√µes densidade marginais.

Vamos provar o Teorema 1 passo a passo:

I. Seja $\{\varepsilon_t\}$ um Gaussian white noise process, o que significa que cada $\varepsilon_t$ segue uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\sigma^2$. A fun√ß√£o densidade de probabilidade (PDF) de uma √∫nica observa√ß√£o $\varepsilon_t$ √© dada por:
    $$f(\varepsilon_t) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\varepsilon_t^2}{2\sigma^2}\right)$$

II. Como as vari√°veis $\varepsilon_1, \varepsilon_2, \ldots, \varepsilon_n$ s√£o independentes, a fun√ß√£o densidade de probabilidade conjunta √© o produto das fun√ß√µes densidade marginais:
    $$f(\varepsilon_1, \varepsilon_2, \ldots, \varepsilon_n) = \prod_{t=1}^{n} f(\varepsilon_t)$$

III. Substituindo a express√£o da PDF de cada $\varepsilon_t$:
     $$f(\varepsilon_1, \varepsilon_2, \ldots, \varepsilon_n) = \prod_{t=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\varepsilon_t^2}{2\sigma^2}\right)$$

IV. Simplificando o produto:
    $$f(\varepsilon_1, \varepsilon_2, \ldots, \varepsilon_n) = \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n \prod_{t=1}^{n} \exp\left(-\frac{\varepsilon_t^2}{2\sigma^2}\right)$$

V. Usando a propriedade de exponenciais, o produto das exponenciais √© a exponencial da soma:
   $$f(\varepsilon_1, \varepsilon_2, \ldots, \varepsilon_n) = (2\pi\sigma^2)^{-n/2} \exp\left(-\frac{\sum_{t=1}^{n} \varepsilon_t^2}{2\sigma^2}\right)$$

Portanto, a fun√ß√£o densidade de probabilidade conjunta de n observa√ß√µes $\varepsilon_1, \varepsilon_2, \ldots, \varepsilon_n$ √© dada por:
$$f(\varepsilon_1, \varepsilon_2, \ldots, \varepsilon_n) = (2\pi\sigma^2)^{-n/2} \exp\left(-\frac{\sum_{t=1}^{n} \varepsilon_t^2}{2\sigma^2}\right)$$ $\blacksquare$

**Lema 1:** *Se $\{\varepsilon_t\}$ √© um independent white noise process, ent√£o para quaisquer fun√ß√µes mensur√°veis $g_1, g_2, \ldots, g_n$, as vari√°veis aleat√≥rias $g_1(\varepsilon_{t_1}), g_2(\varepsilon_{t_2}), \ldots, g_n(\varepsilon_{t_n})$ s√£o independentes, desde que $t_i \neq t_j$ para todo $i \neq j$.*

*Prova:* A prova segue diretamente da defini√ß√£o de independ√™ncia de um independent white noise process. Como os $\varepsilon_t$ s√£o independentes para diferentes valores de t, qualquer fun√ß√£o de $\varepsilon_t$ tamb√©m ser√° independente para diferentes valores de t.

Vamos provar o Lema 1:

I. Dado que $\{\varepsilon_t\}$ √© um independent white noise process, por defini√ß√£o, $\varepsilon_t$ e $\varepsilon_\tau$ s√£o independentes para $t \neq \tau$.

II. Sejam $g_1, g_2, \ldots, g_n$ fun√ß√µes mensur√°veis. Queremos mostrar que as vari√°veis aleat√≥rias $g_1(\varepsilon_{t_1}), g_2(\varepsilon_{t_2}), \ldots, g_n(\varepsilon_{t_n})$ s√£o independentes, dado que $t_i \neq t_j$ para todo $i \neq j$.

III. A independ√™ncia de $\varepsilon_t$ e $\varepsilon_\tau$ implica que a distribui√ß√£o conjunta de $(\varepsilon_t, \varepsilon_\tau)$ pode ser escrita como o produto das distribui√ß√µes marginais:
    $$P(\varepsilon_t \in A, \varepsilon_\tau \in B) = P(\varepsilon_t \in A) P(\varepsilon_\tau \in B)$$
    para quaisquer conjuntos mensur√°veis A e B.

IV. Agora, considere as vari√°veis aleat√≥rias transformadas $g_1(\varepsilon_{t_1}), g_2(\varepsilon_{t_2}), \ldots, g_n(\varepsilon_{t_n})$. Queremos mostrar que a distribui√ß√£o conjunta dessas vari√°veis √© o produto das distribui√ß√µes marginais.

V. Como $\varepsilon_{t_i}$ s√£o independentes para $t_i \neq t_j$, ent√£o as transforma√ß√µes $g_i(\varepsilon_{t_i})$ tamb√©m s√£o independentes. Isso ocorre porque a informa√ß√£o contida em $\varepsilon_{t_i}$ n√£o fornece nenhuma informa√ß√£o sobre $\varepsilon_{t_j}$ para $i \neq j$.

VI. Portanto, a distribui√ß√£o conjunta de $g_1(\varepsilon_{t_1}), g_2(\varepsilon_{t_2}), \ldots, g_n(\varepsilon_{t_n})$ √© o produto das distribui√ß√µes marginais:
    $$P(g_1(\varepsilon_{t_1}) \in A_1, g_2(\varepsilon_{t_2}) \in A_2, \ldots, g_n(\varepsilon_{t_n}) \in A_n) = \prod_{i=1}^{n} P(g_i(\varepsilon_{t_i}) \in A_i)$$
    para quaisquer conjuntos mensur√°veis $A_1, A_2, \ldots, A_n$.

VII. Isso demonstra que as vari√°veis aleat√≥rias $g_1(\varepsilon_{t_1}), g_2(\varepsilon_{t_2}), \ldots, g_n(\varepsilon_{t_n})$ s√£o independentes.

Portanto, se $\{\varepsilon_t\}$ √© um independent white noise process, ent√£o para quaisquer fun√ß√µes mensur√°veis $g_1, g_2, \ldots, g_n$, as vari√°veis aleat√≥rias $g_1(\varepsilon_{t_1}), g_2(\varepsilon_{t_2}), \ldots, g_n(\varepsilon_{t_n})$ s√£o independentes, desde que $t_i \neq t_j$ para todo $i \neq j$. ‚ñ†

**Relev√¢ncia em Simula√ß√µes:**

A independ√™ncia do independent white noise process simplifica o design de simula√ß√µes [^5]. Podemos gerar amostras aleat√≥rias de $\varepsilon_t$ em diferentes momentos *t* sem nos preocuparmos com a depend√™ncia entre elas.

> üí° **Exemplo:** Para simular um independent Gaussian white noise process, podemos usar geradores de n√∫meros aleat√≥rios que produzem amostras independentes de uma distribui√ß√£o normal. Isso √© amplamente utilizado em softwares estat√≠sticos e de simula√ß√£o.
Em Python, podemos usar a fun√ß√£o `np.random.normal` para gerar amostras independentes de uma distribui√ß√£o normal [refer√™ncia a simula√ß√£o num√©rica no t√≥pico anterior].
>
> ```python
> import numpy as np
>
> # Definindo os par√¢metros
> n = 100  # Tamanho da amostra
> sigma = 2  # Desvio padr√£o
>
> # Gerando amostras independentes de um Gaussian white noise process
> epsilon = np.random.normal(loc=0, scale=sigma, size=n)
>
> # Imprimindo as primeiras 10 amostras
> print(epsilon[:10])
>
> # Verificando a m√©dia amostral
> media_amostral = np.mean(epsilon)
> print(f"M√©dia amostral: {media_amostral}")
>
> # Verificando a vari√¢ncia amostral
> variancia_amostral = np.var(epsilon)
> print(f"Vari√¢ncia amostral: {variancia_amostral}")
> ```
>
> Neste exemplo, `np.random.normal(loc=0, scale=sigma, size=n)` gera `n` amostras independentes de uma distribui√ß√£o normal com m√©dia `loc=0` e desvio padr√£o `scale=sigma`. A independ√™ncia √© garantida pelo algoritmo de gera√ß√£o de n√∫meros aleat√≥rios. A m√©dia e a vari√¢ncia amostrais devem se aproximar de 0 e $\sigma^2$, respectivamente, para um grande `n`.

**Filtros Lineares e Gaussian White Noise:**

Considere um filtro linear aplicado a um independent Gaussian white noise process:
$$X_t = \sum_{i=0}^{\infty} a_i \varepsilon_{t-i}$$
onde $\{a_i\}$ s√£o coeficientes constantes e $\{\varepsilon_t\}$ √© um independent Gaussian white noise process.
Devido ao Lema 2 [refer√™ncia ao t√≥pico anterior], $X_t$ tamb√©m seguir√° uma distribui√ß√£o normal. Essa propriedade √© fundamental em diversas aplica√ß√µes, incluindo processamento de sinais e sistemas de comunica√ß√£o.

**Teorema 1.1:** *Se $\{\varepsilon_t\}$ √© um Gaussian white noise process com m√©dia zero e vari√¢ncia $\sigma^2$, e $\{a_i\}$ s√£o coeficientes constantes tais que $\sum_{i=0}^{\infty} a_i^2 < \infty$, ent√£o o processo $X_t = \sum_{i=0}^{\infty} a_i \varepsilon_{t-i}$ √© um processo Gaussiano estacion√°rio com m√©dia zero e vari√¢ncia $\sigma^2 \sum_{i=0}^{\infty} a_i^2$.*

*Prova:* A demonstra√ß√£o se baseia no fato de que $X_t$ √© uma combina√ß√£o linear de vari√°veis gaussianas independentes, o que implica que $X_t$ tamb√©m √© gaussiano. A estacionariedade decorre da estacionariedade do processo $\{\varepsilon_t\}$ e da converg√™ncia da soma dos coeficientes ao quadrado.  A m√©dia de $X_t$ √© zero porque $E[\varepsilon_t] = 0$ para todo t. A vari√¢ncia de $X_t$ √© calculada usando a independ√™ncia dos $\varepsilon_t$: $Var(X_t) = Var(\sum_{i=0}^{\infty} a_i \varepsilon_{t-i}) = \sum_{i=0}^{\infty} a_i^2 Var(\varepsilon_{t-i}) = \sigma^2 \sum_{i=0}^{\infty} a_i^2$.

Vamos provar o Teorema 1.1 passo a passo:

I. Dado $\{\varepsilon_t\}$ √© um Gaussian white noise process com m√©dia zero e vari√¢ncia $\sigma^2$, ent√£o $E[\varepsilon_t] = 0$ e $E[\varepsilon_t^2] = \sigma^2$. Al√©m disso, $\varepsilon_t$ e $\varepsilon_s$ s√£o independentes para t ‚â† s.

II. Seja $X_t = \sum_{i=0}^{\infty} a_i \varepsilon_{t-i}$, onde $\{a_i\}$ s√£o coeficientes constantes e $\sum_{i=0}^{\infty} a_i^2 < \infty$.

III. Para mostrar que $X_t$ √© um processo Gaussiano, notamos que $X_t$ √© uma combina√ß√£o linear de vari√°veis gaussianas independentes ($\varepsilon_{t-i}$). Uma combina√ß√£o linear de vari√°veis gaussianas independentes tamb√©m √© gaussiana.

IV. Para mostrar que $X_t$ tem m√©dia zero:
    $$E[X_t] = E\left[\sum_{i=0}^{\infty} a_i \varepsilon_{t-i}\right] = \sum_{i=0}^{\infty} a_i E[\varepsilon_{t-i}] = \sum_{i=0}^{\infty} a_i \cdot 0 = 0$$

V. Para calcular a vari√¢ncia de $X_t$, usamos a independ√™ncia das vari√°veis $\varepsilon_{t-i}$:
    $$Var(X_t) = Var\left(\sum_{i=0}^{\infty} a_i \varepsilon_{t-i}\right) = \sum_{i=0}^{\infty} a_i^2 Var(\varepsilon_{t-i}) = \sum_{i=0}^{\infty} a_i^2 \sigma^2 = \sigma^2 \sum_{i=0}^{\infty} a_i^2$$
    Como $\sum_{i=0}^{\infty} a_i^2 < \infty$, a vari√¢ncia de $X_t$ √© finita.

VI. Para mostrar que $X_t$ √© estacion√°rio, precisamos mostrar que sua m√©dia e autocovari√¢ncia n√£o dependem de t. J√° mostramos que a m√©dia √© zero. A autocovari√¢ncia ser√° calculada no Corol√°rio 1.1.

Portanto, $X_t = \sum_{i=0}^{\infty} a_i \varepsilon_{t-i}$ √© um processo Gaussiano estacion√°rio com m√©dia zero e vari√¢ncia $\sigma^2 \sum_{i=0}^{\infty} a_i^2$. ‚ñ†

**Corol√°rio 1.1:** *Sob as condi√ß√µes do Teorema 1.1, a autocovari√¢ncia $\gamma_X(h)$ do processo $X_t$ √© dada por $\gamma_X(h) = \sigma^2 \sum_{i=0}^{\infty} a_i a_{i+h}$ para $h \geq 0$, e $\gamma_X(h) = \gamma_X(-h)$ para $h < 0$.*

*Prova:* Usando a defini√ß√£o de autocovari√¢ncia, $\gamma_X(h) = E[(X_t - E[X_t])(X_{t+h} - E[X_{t+h}])]$. Como $E[X_t] = 0$, temos $\gamma_X(h) = E[X_t X_{t+h}] = E[(\sum_{i=0}^{\infty} a_i \varepsilon_{t-i})(\sum_{j=0}^{\infty} a_j \varepsilon_{t+h-j})] = \sum_{i=0}^{\infty} \sum_{j=0}^{\infty} a_i a_j E[\varepsilon_{t-i} \varepsilon_{t+h-j}]$. Devido √† independ√™ncia e m√©dia zero de $\varepsilon_t$, $E[\varepsilon_{t-i} \varepsilon_{t+h-j}] = \sigma^2$ se $t-i = t+h-j$ (ou seja, $j = i+h$) e 0 caso contr√°rio. Portanto, $\gamma_X(h) = \sigma^2 \sum_{i=0}^{\infty} a_i a_{i+h}$. A simetria da fun√ß√£o de autocovari√¢ncia, $\gamma_X(h) = \gamma_X(-h)$, decorre da estacionariedade do processo.

Vamos provar o Corol√°rio 1.1 passo a passo:

I. Pela defini√ß√£o de autocovari√¢ncia, $\gamma_X(h) = E[(X_t - E[X_t])(X_{t+h} - E[X_{t+h}])]$.

II. Do Teorema 1.1, sabemos que $E[X_t] = 0$ para todo t. Portanto, $\gamma_X(h) = E[X_t X_{t+h}]$.

III. Substituindo a express√£o de $X_t$:
    $$\gamma_X(h) = E\left[\left(\sum_{i=0}^{\infty} a_i \varepsilon_{t-i}\right)\left(\sum_{j=0}^{\infty} a_j \varepsilon_{t+h-j}\right)\right]$$

IV. Expandindo a express√£o:
    $$\gamma_X(h) = E\left[\sum_{i=0}^{\infty} \sum_{j=0}^{\infty} a_i a_j \varepsilon_{t-i} \varepsilon_{t+h-j}\right] = \sum_{i=0}^{\infty} \sum_{j=0}^{\infty} a_i a_j E[\varepsilon_{t-i} \varepsilon_{t+h-j}]$$

V. Como $\varepsilon_t$ √© um independent white noise process, $E[\varepsilon_{t-i} \varepsilon_{t+h-j}] = 0$ se $t-i \neq t+h-j$, e $E[\varepsilon_{t-i} \varepsilon_{t+h-j}] = \sigma^2$ se $t-i = t+h-j$.  A condi√ß√£o $t-i = t+h-j$ implica $j = i+h$.

VI. Substituindo na express√£o da autocovari√¢ncia:
    $$\gamma_X(h) = \sum_{i=0}^{\infty} a_i a_{i+h} \sigma^2 = \sigma^2 \sum_{i=0}^{\infty} a_i a_{i+h}$$

VII. Para $h \geq 0$, essa √© a autocovari√¢ncia. Para $h < 0$, usamos a propriedade de estacionariedade, que implica $\gamma_X(h) = \gamma_X(-h)$.

Portanto, $\gamma_X(h) = \sigma^2 \sum_{i=0}^{\infty} a_i a_{i+h}$ para $h \geq 0$, e $\gamma_X(h) = \gamma_X(-h)$ para $h < 0$. ‚ñ†

**Exemplo:**
> Considere um filtro de m√©dia m√≥vel (Moving Average Filter) com pesos iguais: $X_t = \frac{1}{3}(\varepsilon_t + \varepsilon_{t-1} + \varepsilon_{t-2})$.
> Se $\{\varepsilon_t\}$ √© um Gaussian white noise process, ent√£o $X_t$ tamb√©m seguir√° uma distribui√ß√£o normal. Isso ocorre porque $X_t$ √© uma combina√ß√£o linear de vari√°veis aleat√≥rias normais independentes.
>
> üí° **Exemplo Num√©rico:**
> Seja $\{\varepsilon_t\}$ um Gaussian white noise process com m√©dia 0 e vari√¢ncia $\sigma^2 = 1$. Considere o filtro de m√©dia m√≥vel $X_t = \frac{1}{3}(\varepsilon_t + \varepsilon_{t-1} + \varepsilon_{t-2})$. Vamos calcular a m√©dia e a vari√¢ncia de $X_t$.
>
> M√©dia:
> $E[X_t] = E\left[\frac{1}{3}(\varepsilon_t + \varepsilon_{t-1} + \varepsilon_{t-2})\right] = \frac{1}{3}(E[\varepsilon_t] + E[\varepsilon_{t-1}] + E[\varepsilon_{t-2}]) = \frac{1}{3}(0 + 0 + 0) = 0$
>
> Vari√¢ncia:
> $Var(X_t) = Var\left(\frac{1}{3}(\varepsilon_t + \varepsilon_{t-1} + \varepsilon_{t-2})\right) = \frac{1}{9}(Var(\varepsilon_t) + Var(\varepsilon_{t-1}) + Var(\varepsilon_{t-2}) + 2Cov(\varepsilon_t, \varepsilon_{t-1}) + 2Cov(\varepsilon_t, \varepsilon_{t-2}) + 2Cov(\varepsilon_{t-1}, \varepsilon_{t-2}))$
>
> Como $\{\varepsilon_t\}$ √© um independent white noise process, $Cov(\varepsilon_i, \varepsilon_j) = 0$ para $i \neq j$. Portanto:
> $Var(X_t) = \frac{1}{9}(Var(\varepsilon_t) + Var(\varepsilon_{t-1}) + Var(\varepsilon_{t-2})) = \frac{1}{9}(1 + 1 + 1) = \frac{3}{9} = \frac{1}{3}$
>
> A m√©dia de $X_t$ √© 0 e a vari√¢ncia √© $\frac{1}{3}$. Isso demonstra como a independ√™ncia do ru√≠do branco simplifica o c√°lculo das propriedades estat√≠sticas do processo filtrado.

**Proposi√ß√£o 1:** *Se $\{\varepsilon_t\}$ √© um independent white noise process, ent√£o para qualquer fun√ß√£o mensur√°vel g, as vari√°veis aleat√≥rias g(Œµ_t) e g(Œµ_œÑ) s√£o n√£o correlacionadas para $t \neq \tau$, desde que $E[g(\varepsilon_t)^2] < \infty$ e $E[g(\varepsilon_\tau)^2] < \infty$.*

*Prova:* Como Œµ_t e Œµ_œÑ s√£o independentes, g(Œµ_t) e g(Œµ_œÑ) tamb√©m s√£o independentes. Portanto, $E[g(\varepsilon_t)g(\varepsilon_\tau)] = E[g(\varepsilon_t)]E[g(\varepsilon_\tau)]$. A covari√¢ncia entre g(Œµ_t) e g(Œµ_œÑ) √© dada por $Cov(g(\varepsilon_t), g(\varepsilon_\tau)) = E[g(\varepsilon_t)g(\varepsilon_\tau)] - E[g(\varepsilon_t)]E[g(\varepsilon_\tau)] = 0$. Logo, g(Œµ_t) e g(Œµ_œÑ) s√£o n√£o correlacionadas.

Vamos provar a Proposi√ß√£o 1 passo a passo:

I.  Dado que $\{\varepsilon_t\}$ √© um independent white noise process, Œµ_t e Œµ_œÑ s√£o independentes para $t \neq \tau$.

II. Seja g uma fun√ß√£o mensur√°vel tal que $E[g(\varepsilon_t)^2] < \infty$ e $E[g(\varepsilon_\tau)^2] < \infty$.

III. Queremos mostrar que g(Œµ_t) e g(Œµ_œÑ) s√£o n√£o correlacionadas, ou seja, $Cov(g(\varepsilon_t), g(\varepsilon_\tau)) = 0$.

IV. Pela defini√ß√£o de covari√¢ncia:
     $$Cov(g(\varepsilon_t), g(\varepsilon_\tau)) = E[g(\varepsilon_t)g(\varepsilon_\tau)] - E[g(\varepsilon_t)]E[g(\varepsilon_\tau)]$$

V. Como Œµ_t e Œµ_œÑ s√£o independentes, g(Œµ_t) e g(Œµ_œÑ) tamb√©m s√£o independentes. Portanto:
    $$E[g(\varepsilon_t)g(\varepsilon_\tau)] = E[g(\varepsilon_t)]E[g(\varepsilon_\tau)]$$

VI. Substituindo na express√£o da covari√¢ncia:
     $$Cov(g(\varepsilon_t), g(\varepsilon_\tau)) = E[g(\varepsilon_t)]E[g(\varepsilon_\tau)] - E[g(\varepsilon_t)]E[g(\varepsilon_\tau)] = 0$$

VII. Isso mostra que a covari√¢ncia entre g(Œµ_t) e g(Œµ_œÑ) √© zero, o que significa que g(Œµ_t) e g(Œµ_œÑ) s√£o n√£o correlacionadas.

Portanto, se $\{\varepsilon_t\}$ √© um independent white noise process, ent√£o para qualquer fun√ß√£o mensur√°vel g, as vari√°veis aleat√≥rias g(Œµ_t) e g(Œµ_œÑ) s√£o n√£o correlacionadas para $t \neq \tau$, desde que $E[g(\varepsilon_t)^2] < \infty$ e $E[g(\varepsilon_\tau)^2] < \infty$. ‚ñ†

### Conclus√£o

A condi√ß√£o de independ√™ncia, mais forte que a n√£o correla√ß√£o, define o **independent white noise process**, simplificando an√°lises, simula√ß√µes e implementa√ß√µes de testes estat√≠sticos [^5]. Em particular, o **Gaussian white noise process**, que combina independ√™ncia com a distribui√ß√£o normal, √© amplamente utilizado em modelagem estat√≠stica devido √†s suas propriedades bem definidas. A independ√™ncia entre os termos do processo facilita a an√°lise e simplifica a implementa√ß√£o de algoritmos em diversas √°reas, desde a modelagem de s√©ries temporais at√© o processamento de sinais.

### Refer√™ncias
[^4]: The basic building block for all the processes considered in this chapter is a sequence {8} - whose elements have mean zero and variance œÉ¬≤,
[^5]: A process satisfying [3.2.1] through [3.2.3] is described as a white noise process.
<!-- END -->