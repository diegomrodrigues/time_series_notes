## 3.4 Algoritmos Eficientes para GeraÃ§Ã£o de White Noise: Garantindo a Validade das SimulaÃ§Ãµes

### IntroduÃ§Ã£o

Dando continuidade ao estudo dos white noise processes e suas variaÃ§Ãµes, este capÃ­tulo foca em um aspecto prÃ¡tico crucial: a **geraÃ§Ã£o eficiente de white noise** para estudos de simulaÃ§Ã£o. A validade dos resultados de simulaÃ§Ãµes que utilizam modelos de sÃ©ries temporais depende fortemente da qualidade do ruÃ­do branco gerado [^4, 5]. Exploraremos algoritmos eficientes para gerar white noise, com Ãªnfase em geradores de nÃºmeros pseudoaleatÃ³rios (PRNGs) com boas propriedades estatÃ­sticas. Adicionalmente, discutiremos tÃ©cnicas para verificar a qualidade do ruÃ­do branco gerado.

### A ImportÃ¢ncia da Qualidade do RuÃ­do Branco em SimulaÃ§Ãµes

Em estudos de simulaÃ§Ã£o, o white noise process atua como uma fonte de aleatoriedade que impulsiona a dinÃ¢mica do sistema. Se o ruÃ­do branco gerado nÃ£o for de alta qualidade, ou seja, se nÃ£o satisfazer as propriedades de mÃ©dia zero, variÃ¢ncia constante e ausÃªncia de autocorrelaÃ§Ã£o, os resultados da simulaÃ§Ã£o podem ser enviesados e levar a conclusÃµes incorretas.

> ğŸ’¡ **Exemplo:** Considere um estudo de simulaÃ§Ã£o para avaliar o desempenho de um filtro de Kalman. Se o ruÃ­do branco utilizado para modelar o ruÃ­do do processo e o ruÃ­do da mediÃ§Ã£o tiver autocorrelaÃ§Ã£o significativa, o filtro de Kalman pode nÃ£o convergir corretamente, e as estimativas dos parÃ¢metros podem ser imprecisas.

Ã‰ essencial garantir que o white noise gerado seja o mais prÃ³ximo possÃ­vel do ideal para que os resultados da simulaÃ§Ã£o sejam confiÃ¡veis e representativos do sistema real que estÃ¡ sendo modelado. A escolha de um algoritmo de geraÃ§Ã£o de ruÃ­do branco adequado e a verificaÃ§Ã£o de suas propriedades estatÃ­sticas sÃ£o etapas cruciais para garantir a validade das simulaÃ§Ãµes.

### Geradores de NÃºmeros PseudoaleatÃ³rios (PRNGs)

A maioria dos algoritmos para gerar white noise em simulaÃ§Ãµes se baseia em **geradores de nÃºmeros pseudoaleatÃ³rios (PRNGs)**. PRNGs sÃ£o algoritmos determinÃ­sticos que produzem sequÃªncias de nÃºmeros que se aproximam de sequÃªncias aleatÃ³rias verdadeiras. No entanto, como sÃ£o determinÃ­sticos, as sequÃªncias geradas sÃ£o apenas pseudoaleatÃ³rias. A qualidade de um PRNG Ã© determinada por suas propriedades estatÃ­sticas, como a uniformidade da distribuiÃ§Ã£o, a ausÃªncia de padrÃµes e a capacidade de passar em testes de aleatoriedade.

> ğŸ’¡ **Exemplo:** Um PRNG simples Ã© o gerador congruencial linear (LCG), definido pela recorrÃªncia:
>
> $$X_{n+1} = (aX_n + c) \mod m$$
>
> onde $X_n$ Ã© o estado atual, $X_{n+1}$ Ã© o prÃ³ximo estado, $a$ Ã© o multiplicador, $c$ Ã© o incremento e $m$ Ã© o mÃ³dulo. Os parÃ¢metros $a$, $c$ e $m$ devem ser cuidadosamente escolhidos para garantir que o LCG tenha um perÃ­odo longo e produza sequÃªncias com boas propriedades estatÃ­sticas. No entanto, LCGs simples como este geralmente nÃ£o sÃ£o adequados para simulaÃ§Ãµes exigentes devido a problemas de serial correlation e outros padrÃµes indesejados.

> ğŸ’¡ **Exemplo:** Um exemplo de LCG Ã© o RANDU, com parÃ¢metros $a=65539$, $c=0$ e $m=2^{31}$. No entanto, o RANDU Ã© conhecido por ter propriedades estatÃ­sticas ruins e nÃ£o Ã© adequado para simulaÃ§Ãµes sÃ©rias. Ele possui um padrÃ£o que se torna visÃ­vel quando os nÃºmeros gerados sÃ£o plotados em 3D.

Existem diversos tipos de PRNGs, com diferentes caracterÃ­sticas e nÃ­veis de complexidade. Alguns dos PRNGs mais utilizados em simulaÃ§Ãµes incluem:

1.  **Mersenne Twister:** Ã‰ um dos PRNGs mais populares e amplamente utilizados devido ao seu longo perÃ­odo ($2^{19937} - 1$) e boas propriedades estatÃ­sticas. Ele Ã© baseado em uma matriz de recorrÃªncia linear sobre um campo binÃ¡rio.

2.  **Xorshift:** Ã‰ uma famÃ­lia de PRNGs que utilizam operaÃ§Ãµes de deslocamento e XOR (ou exclusivo) para gerar sequÃªncias de nÃºmeros aleatÃ³rios. Eles sÃ£o geralmente mais rÃ¡pidos que o Mersenne Twister, mas podem ter propriedades estatÃ­sticas inferiores.

3.  **PCG (Permuted Congruential Generator):** Ã‰ uma famÃ­lia de PRNGs que combinam um gerador congruencial linear com uma funÃ§Ã£o de permutaÃ§Ã£o para melhorar suas propriedades estatÃ­sticas. Eles sÃ£o projetados para serem rÃ¡pidos, eficientes em termos de memÃ³ria e fÃ¡ceis de usar.

4.  **WELL (Well Equidistributed Long-period Linear):** WELL sÃ£o PRNGs que visam melhorar a equidistribuiÃ§Ã£o das sequÃªncias geradas. Eles sÃ£o mais complexos que o Mersenne Twister, mas podem ter propriedades estatÃ­sticas superiores em algumas aplicaÃ§Ãµes.

A escolha do PRNG adequado depende dos requisitos da simulaÃ§Ã£o. Para simulaÃ§Ãµes simples, PRNGs mais rÃ¡pidos como Xorshift podem ser suficientes. Para simulaÃ§Ãµes mais complexas e exigentes, Ã© recomendÃ¡vel usar PRNGs com melhores propriedades estatÃ­sticas, como o Mersenne Twister ou um PCG.

Ã‰ importante ressaltar que, mesmo os melhores PRNGs, geram apenas sequÃªncias pseudoaleatÃ³rias. Portanto, Ã© crucial realizar testes estatÃ­sticos para verificar se o ruÃ­do branco gerado satisfaz as propriedades desejadas.

> ğŸ’¡ **Exemplo:** Em Python, o mÃ³dulo `numpy.random` utiliza o Mersenne Twister como seu PRNG padrÃ£o. No entanto, Ã© possÃ­vel utilizar outros PRNGs atravÃ©s da classe `numpy.random.Generator`, permitindo que vocÃª escolha o algoritmo mais adequado para sua aplicaÃ§Ã£o.
```python
import numpy as np

# Using the default Mersenne Twister
rng_default = np.random.default_rng()
random_numbers = rng_default.random(10)

# Using the PCG64 generator
rng_pcg64 = np.random.PCG64()
random_numbers_pcg64 = rng_pcg64.random(10)
```
Para complementar a discussÃ£o sobre PRNGs, Ã© importante abordar a questÃ£o da semente (seed). A semente Ã© um valor inicial que determina a sequÃªncia de nÃºmeros pseudoaleatÃ³rios gerada por um PRNG. Utilizar a mesma semente resulta na mesma sequÃªncia, o que pode ser Ãºtil para reproduzir resultados de simulaÃ§Ã£o.

> ğŸ’¡ **Exemplo:** Em Python, podemos definir a semente de um PRNG utilizando o mÃ©todo `seed()`.
```python
import numpy as np

# Setting the seed for reproducibility
rng = np.random.default_rng(seed=42)
random_numbers = rng.random(10)
print(random_numbers)

# Generating again with the same seed
rng2 = np.random.default_rng(seed=42)
random_numbers2 = rng2.random(10)
print(random_numbers2)

# random_numbers and random_numbers2 will be the same
```

AlÃ©m disso, alguns PRNGs possuem mÃ©todos para pular Ã  frente na sequÃªncia, o que pode ser Ãºtil para paralelizar simulaÃ§Ãµes.

**Teorema 1:** Seja $X_1, X_2, \ldots, X_n$ uma sequÃªncia de nÃºmeros gerados por um PRNG. Se o PRNG tem um perÃ­odo $P$, entÃ£o a sequÃªncia $X_{i+1}, X_{i+2}, \ldots, X_{i+n}$ Ã© idÃªntica Ã  sequÃªncia $X_{j+1}, X_{j+2}, \ldots, X_{j+n}$ se $i \equiv j \pmod{P}$.

**Prova do Teorema 1:**
I. Dado que o PRNG tem um perÃ­odo $P$, a sequÃªncia de nÃºmeros gerados se repete apÃ³s cada $P$ iteraÃ§Ãµes. Isso significa que $X_{i+P} = X_i$ para todo $i$.

II.  Queremos mostrar que a sequÃªncia $X_{i+1}, X_{i+2}, \ldots, X_{i+n}$ Ã© idÃªntica Ã  sequÃªncia $X_{j+1}, X_{j+2}, \ldots, X_{j+n}$ se $i \equiv j \pmod{P}$.

III. Se $i \equiv j \pmod{P}$, entÃ£o existe um inteiro $k$ tal que $i = j + kP$.

IV. Consideremos o termo $X_{i+l}$ na primeira sequÃªncia, onde $1 \leq l \leq n$. Podemos reescrevÃª-lo como $X_{i+l} = X_{j + kP + l}$.

V. Devido Ã  periodicidade do PRNG, $X_{j + kP + l} = X_{j+l}$.

VI. Portanto, cada termo na sequÃªncia $X_{i+1}, X_{i+2}, \ldots, X_{i+n}$ Ã© idÃªntico ao termo correspondente na sequÃªncia $X_{j+1}, X_{j+2}, \ldots, X_{j+n}$.

VII. ConcluÃ­mos que as duas sequÃªncias sÃ£o idÃªnticas se $i \equiv j \pmod{P}$. $\blacksquare$

**Lema 1:** Para um LCG com parÃ¢metros $a$, $c$, e $m$, o perÃ­odo mÃ¡ximo Ã© $m$. Para atingir esse perÃ­odo mÃ¡ximo, as seguintes condiÃ§Ãµes devem ser satisfeitas:
   1. $c$ e $m$ sÃ£o coprimos.
   2. $a - 1$ Ã© divisÃ­vel por todos os fatores primos de $m$.
   3. $a - 1$ Ã© um mÃºltiplo de 4 se $m$ Ã© um mÃºltiplo de 4.

*Proof:* Este resultado Ã© uma consequÃªncia direta da teoria dos geradores congruenciais lineares. As condiÃ§Ãµes garantem que o LCG explore completamente o espaÃ§o de estados antes de se repetir.

### Testes EstatÃ­sticos para Validar o RuÃ­do Branco Gerado

ApÃ³s gerar uma sequÃªncia de ruÃ­do branco, Ã© fundamental realizar testes estatÃ­sticos para verificar se ela satisfaz as propriedades de mÃ©dia zero, variÃ¢ncia constante e ausÃªncia de autocorrelaÃ§Ã£o. Alguns testes estatÃ­sticos comuns incluem:

1.  **Teste de MÃ©dia:** Verifica se a mÃ©dia da sequÃªncia Ã© estatisticamente igual a zero. Um teste t pode ser usado para testar a hipÃ³tese nula de que a mÃ©dia Ã© zero.

2.  **Teste de VariÃ¢ncia:** Verifica se a variÃ¢ncia da sequÃªncia Ã© constante ao longo do tempo. O teste de Bartlett ou o teste de Levene podem ser usados para testar a hipÃ³tese nula de homoscedasticidade.

3.  **Teste de AutocorrelaÃ§Ã£o:** Verifica se a sequÃªncia apresenta autocorrelaÃ§Ã£o significativa em diferentes lags. O teste de Ljung-Box ou o teste de Box-Pierce podem ser usados para testar a hipÃ³tese nula de ausÃªncia de autocorrelaÃ§Ã£o.

4.  **Teste de Normalidade:** Se for assumido que o ruÃ­do branco segue uma distribuiÃ§Ã£o normal, testes de normalidade como o teste de Shapiro-Wilk ou o teste de Jarque-Bera podem ser aplicados. [^5, 3.3]

> ğŸ’¡ **Exemplo:** Para o teste de autocorrelaÃ§Ã£o, podemos dividir a sequÃªncia de ruÃ­do branco em segmentos e calcular a autocorrelaÃ§Ã£o para cada segmento. Se a autocorrelaÃ§Ã£o for significativamente diferente de zero em algum segmento, isso indica que o ruÃ­do branco pode nÃ£o ser estacionÃ¡rio.

5.  **Teste de ExecuÃ§Ãµes (Runs Test):** O teste de execuÃ§Ãµes verifica se a sequÃªncia apresenta padrÃµes de agrupamento de valores acima ou abaixo da mediana. Ele testa a hipÃ³tese nula de que a sequÃªncia Ã© aleatÃ³ria.

6.  **Teste de Kolmogorov-Smirnov (KS):** Este teste compara a distribuiÃ§Ã£o empÃ­rica da amostra com uma distribuiÃ§Ã£o teÃ³rica (e.g., normal) e testa a hipÃ³tese nula de que a amostra vem da distribuiÃ§Ã£o teÃ³rica.

Ã‰ importante realizar esses testes em diferentes tamanhos de amostra para garantir que o ruÃ­do branco gerado seja de alta qualidade em diversas situaÃ§Ãµes. AlÃ©m disso, Ã© recomendÃ¡vel utilizar diferentes testes estatÃ­sticos para verificar as propriedades do ruÃ­do branco, pois cada teste tem seus pontos fortes e fracos.

> ğŸ’¡ **Exemplo:** Para testar a ausÃªncia de autocorrelaÃ§Ã£o, pode-se utilizar tanto o teste de Ljung-Box quanto a anÃ¡lise visual da funÃ§Ã£o de autocorrelaÃ§Ã£o amostral (ACF). A ACF deve apresentar valores prÃ³ximos de zero para todos os lags, exceto o lag zero, que deve ser igual a 1. A anÃ¡lise visual da ACF pode revelar padrÃµes de autocorrelaÃ§Ã£o que nÃ£o sÃ£o detectados pelo teste de Ljung-Box.

A aplicaÃ§Ã£o de uma bateria de testes estatÃ­sticos, juntamente com uma anÃ¡lise visual das propriedades do ruÃ­do branco gerado, Ã© fundamental para garantir a validade dos resultados de simulaÃ§Ãµes.

> ğŸ’¡ **Exemplo NumÃ©rico:** Vamos gerar 1000 amostras de ruÃ­do branco usando o Mersenne Twister com mÃ©dia 0 e desvio padrÃ£o 1 e aplicar o teste de Ljung-Box para verificar a ausÃªncia de autocorrelaÃ§Ã£o.

```python
import numpy as np
from statsmodels.stats.diagnostic import acorr_ljungbox
import matplotlib.pyplot as plt

# Gerar ruÃ­do branco
np.random.seed(42) # Definir a semente para reprodutibilidade
white_noise = np.random.normal(0, 1, 1000)

# Teste de Ljung-Box
lb_test = acorr_ljungbox(white_noise, lags=[10], return_df=True)
print(lb_test)

# Plot da sequÃªncia
plt.figure(figsize=(10, 5))
plt.plot(white_noise)
plt.title('RuÃ­do Branco Gerado')
plt.xlabel('Amostra')
plt.ylabel('Valor')
plt.show()

# Plot da ACF
from statsmodels.graphics.tsaplots import plot_acf
plot_acf(white_noise, lags=20)
plt.title('FunÃ§Ã£o de AutocorrelaÃ§Ã£o (ACF)')
plt.show()
```

Neste exemplo, o teste de Ljung-Box Ã© aplicado com um lag de 10. O valor-p retornado pelo teste indica se hÃ¡ autocorrelaÃ§Ã£o significativa. Se o valor-p for maior que o nÃ­vel de significÃ¢ncia (e.g., 0.05), falhamos em rejeitar a hipÃ³tese nula de ausÃªncia de autocorrelaÃ§Ã£o. A visualizaÃ§Ã£o da ACF ajuda a identificar padrÃµes de autocorrelaÃ§Ã£o que podem nÃ£o ser capturados pelo teste estatÃ­stico.

**Teorema 2:** Se uma sequÃªncia de variÃ¡veis aleatÃ³rias $X_1, X_2, \ldots, X_n$ Ã© white noise com mÃ©dia zero e variÃ¢ncia $\sigma^2$, entÃ£o a estatÃ­stica de Ljung-Box $Q = n(n+2)\sum_{k=1}^{h} \frac{\hat{\rho}_k^2}{n-k}$ segue aproximadamente uma distribuiÃ§Ã£o $\chi^2$ com $h$ graus de liberdade sob a hipÃ³tese nula de ausÃªncia de autocorrelaÃ§Ã£o, onde $\hat{\rho}_k$ Ã© a autocorrelaÃ§Ã£o amostral no lag $k$.

**CorolÃ¡rio 2.1:** Sob as mesmas condiÃ§Ãµes do Teorema 2, rejeitamos a hipÃ³tese nula de ausÃªncia de autocorrelaÃ§Ã£o ao nÃ­vel de significÃ¢ncia $\alpha$ se $Q > \chi^2_{h, 1-\alpha}$, onde $\chi^2_{h, 1-\alpha}$ Ã© o quantil $1-\alpha$ da distribuiÃ§Ã£o $\chi^2$ com $h$ graus de liberdade.

**Prova do Teorema 2:**

I.  Assumimos que $X_1, X_2, \ldots, X_n$ Ã© uma sequÃªncia de white noise com mÃ©dia zero e variÃ¢ncia $\sigma^2$. Isso implica que $E[X_i] = 0$ e $E[X_i^2] = \sigma^2$ para todo $i$, e $E[X_i X_j] = 0$ para $i \neq j$.

II. A estatÃ­stica de Ljung-Box Ã© definida como $Q = n(n+2)\sum_{k=1}^{h} \frac{\hat{\rho}_k^2}{n-k}$, onde $\hat{\rho}_k$ Ã© a autocorrelaÃ§Ã£o amostral no lag $k$.

III. Sob a hipÃ³tese nula de ausÃªncia de autocorrelaÃ§Ã£o, a autocorrelaÃ§Ã£o teÃ³rica $\rho_k = 0$ para todo $k > 0$.

IV. Para amostras grandes, a autocorrelaÃ§Ã£o amostral $\hat{\rho}_k$ Ã© aproximadamente normalmente distribuÃ­da com mÃ©dia zero e variÃ¢ncia $\frac{1}{n}$. Ou seja, $\hat{\rho}_k \sim N(0, \frac{1}{n})$.

V. Portanto, $n\hat{\rho}_k^2$ Ã© aproximadamente uma variÃ¡vel qui-quadrado com 1 grau de liberdade, ou seja, $n\hat{\rho}_k^2 \sim \chi^2(1)$.

VI. A estatÃ­stica $Q$ Ã© uma soma ponderada de $h$ termos $n\hat{\rho}_k^2$.  Para amostras grandes, a distribuiÃ§Ã£o assintÃ³tica de $Q$ Ã© uma distribuiÃ§Ã£o qui-quadrado com $h$ graus de liberdade.

VII. Logo, $Q \approx \sum_{k=1}^{h} \chi^2_k(1)$, que Ã© distribuÃ­do como $\chi^2(h)$. $\blacksquare$

### TÃ©cnicas para Melhorar a EficiÃªncia dos Algoritmos de GeraÃ§Ã£o de White Noise

Em algumas aplicaÃ§Ãµes, a geraÃ§Ã£o de ruÃ­do branco precisa ser extremamente rÃ¡pida. Nesses casos, Ã© importante utilizar tÃ©cnicas para melhorar a eficiÃªncia dos algoritmos de geraÃ§Ã£o. Algumas tÃ©cnicas comuns incluem:

1.  **UtilizaÃ§Ã£o de Bibliotecas Otimizadas:** A utilizaÃ§Ã£o de bibliotecas numÃ©ricas otimizadas, como NumPy em Python ou BLAS em C/C++, pode acelerar significativamente a geraÃ§Ã£o de ruÃ­do branco. Essas bibliotecas utilizam algoritmos eficientes e exploram a arquitetura do hardware para obter o mÃ¡ximo desempenho.

2.  **VectorizaÃ§Ã£o:** A vectorizaÃ§Ã£o Ã© uma tÃ©cnica que consiste em realizar operaÃ§Ãµes em vetores ou arrays em vez de realizar operaÃ§Ãµes em elementos individuais. Isso permite que o cÃ³digo seja executado de forma mais eficiente, pois as operaÃ§Ãµes sÃ£o realizadas em paralelo.

3.  **ParalelizaÃ§Ã£o:** A paralelizaÃ§Ã£o consiste em dividir o problema em subproblemas menores que podem ser resolvidos em paralelo por diferentes processadores ou threads. Isso pode reduzir significativamente o tempo de execuÃ§Ã£o, especialmente em sistemas com mÃºltiplos processadores.

4.  **GeraÃ§Ã£o PrÃ©via:** Em algumas aplicaÃ§Ãµes, Ã© possÃ­vel gerar uma grande quantidade de ruÃ­do branco previamente e armazenÃ¡-la em um arquivo ou na memÃ³ria. Isso evita a necessidade de gerar o ruÃ­do branco em tempo real, o que pode ser vantajoso se a geraÃ§Ã£o for computacionalmente intensiva. No entanto, Ã© importante garantir que a quantidade de ruÃ­do branco gerada seja suficiente para a simulaÃ§Ã£o e que a sequÃªncia seja adequadamente embaralhada para evitar padrÃµes.

> ğŸ’¡ **Exemplo:** Para simular um sistema dinÃ¢mico por um longo perÃ­odo, Ã© possÃ­vel gerar uma sequÃªncia de ruÃ­do branco de tamanho suficiente previamente e utilizÃ¡-la na simulaÃ§Ã£o. Isso pode reduzir significativamente o tempo de execuÃ§Ã£o, especialmente se o sistema for simulado vÃ¡rias vezes com diferentes parÃ¢metros.

5.  **AproximaÃ§Ãµes:** Em alguns casos, Ã© possÃ­vel utilizar aproximaÃ§Ãµes para a distribuiÃ§Ã£o do ruÃ­do branco que sÃ£o mais rÃ¡pidas de gerar. Por exemplo, em vez de gerar ruÃ­do branco gaussiano, pode-se utilizar uma distribuiÃ§Ã£o uniforme ou triangular com mÃ©dia zero e variÃ¢ncia ajustada. No entanto, Ã© importante avaliar o impacto dessas aproximaÃ§Ãµes nos resultados da simulaÃ§Ã£o.

> ğŸ’¡ **Exemplo:** Em algumas aplicaÃ§Ãµes, a utilizaÃ§Ã£o de um ruÃ­do branco com distribuiÃ§Ã£o uniforme em vez de gaussiana pode ter um impacto negligenciÃ¡vel nos resultados da simulaÃ§Ã£o, desde que a variÃ¢ncia seja ajustada adequadamente. No entanto, Ã© importante verificar essa suposiÃ§Ã£o caso a caso.

Ã‰ importante avaliar o compromisso entre a eficiÃªncia dos algoritmos de geraÃ§Ã£o de ruÃ­do branco e a qualidade do ruÃ­do branco gerado. Algoritmos mais rÃ¡pidos podem ter propriedades estatÃ­sticas inferiores, o que pode comprometer a validade dos resultados da simulaÃ§Ã£o. Portanto, a escolha do algoritmo adequado deve ser baseada em uma anÃ¡lise cuidadosa dos requisitos da simulaÃ§Ã£o e das caracterÃ­sticas dos diferentes algoritmos.

Para ilustrar a aplicaÃ§Ã£o de vectorizaÃ§Ã£o, considere o seguinte exemplo em Python utilizando NumPy:

```python
import numpy as np
import time

# Tamanho da amostra
n = 1000000

# MÃ©todo nÃ£o vectorizado
start_time = time.time()
ruido_nao_vectorizado = []
for i in range(n):
    ruido_nao_vectorizado.append(np.random.normal(0, 1))
end_time = time.time()
tempo_nao_vectorizado = end_time - start_time

# MÃ©todo vectorizado
start_time = time.time()
ruido_vectorizado = np.random.normal(0, 1, n)
end_time = time.time()
tempo_vectorizado = end_time - start_time

print(f"Tempo nÃ£o vectorizado: {tempo_nao_vectorizado:.4f} segundos")
print(f"Tempo vectorizado: {tempo_vectorizado:.4f} segundos")
```

Este exemplo demonstra como a vectorizaÃ§Ã£o pode reduzir significativamente o tempo de geraÃ§Ã£o de ruÃ­do branco.

### Uso de Pseudo-Random Number Generators (PRNG) para simular White Noise
O estudo do White Noise aborda diversas Ã¡reas, a partir da identificaÃ§Ã£o de um conjunto estocÃ¡stico, atÃ© a representaÃ§Ã£o e caracterizaÃ§Ã£o do modelo. A maior parte dos processos de sÃ©ries temporais que sÃ£o representados por um conjunto de ruÃ­dos tem uma vasta gama de aplicaÃ§Ã£o e, portanto, a simulaÃ§Ã£o dos dados tem um papel fundamental [^4].

Para tal, a utilizaÃ§Ã£o de Pseudo-Random Number Generators (PRNG) Ã© uma das soluÃ§Ãµes mais comuns, com o foco em imitar o ruÃ­do branco por meio de PRNG que garantam validade estatÃ­stica nas propriedades. Para garantir que essa mÃ­mica seja de alta qualidade, sÃ£o necessÃ¡rios testes de qualidade nas amostras que visam atestar a aleatoriedade, visto que PRNGs geram resultados determinÃ­sticos.

**Exemplo de aplicaÃ§Ã£o em simulaÃ§Ã£o:**

```python
#Import das bibliotecas necessÃ¡rias
import numpy as np
import matplotlib.pyplot as plt

#Definindo os parÃ¢metros para a simulaÃ§Ã£o
n_samples = 1000 #NÃºmero de amostras
mean = 0 #MÃ©dia
stdev = 1 #Desvio PadrÃ£o

#Gerando os dados com a funÃ§Ã£o normal do NumPy
white_noise = np.random.normal(mean,stdev,n_samples)

#Plotando os dados
plt.plot(white_noise)
plt.title('SimulaÃ§Ã£o do White Noise')
plt.xlabel('Amostras')
plt.ylabel('Valor')
plt.show()
```

Ã‰ fundamental notar que o PRNG utilizado para simular um white noise process precisa passar por testes robustos. A escolha de um gerador inadequado pode comprometer a representatividade dos resultados [^5].

**ProposiÃ§Ã£o 1:** Seja $\{X_t\}$ uma sequÃªncia de variÃ¡veis aleatÃ³rias independentes e identicamente distribuÃ­das (iid) com mÃ©dia zero e variÃ¢ncia finita $\sigma^2$. EntÃ£o $\{X_t\}$ Ã© um white noise process.

**Prova da ProposiÃ§Ã£o 1:**
I. Por definiÃ§Ã£o, um white noise process Ã© uma sequÃªncia de variÃ¡veis aleatÃ³rias nÃ£o correlacionadas com mÃ©dia zero e variÃ¢ncia constante.
II.  Dado que $\{X_t\}$ Ã© uma sequÃªncia de variÃ¡veis aleatÃ³rias independentes e identicamente distribuÃ­das (iid) com mÃ©dia zero e variÃ¢ncia finita $\sigma^2$, temos:
    *   $E[X_t] = 0$ para todo $t$ (mÃ©dia zero).
    *   $Var(X_t] = \sigma^2$ para todo $t$ (variÃ¢ncia constante).
    *   $Cov(X_t, X_s) = E[(X_t - E[X_t])(X_s - E[X_s])] = E[X_t X_s] = 0$ para $t \neq s$ (nÃ£o correlaÃ§Ã£o devido Ã  independÃªncia).
III. Portanto, $\{X_t\}$ satisfaz as condiÃ§Ãµes de um white noise process. $\blacksquare$

**ProposiÃ§Ã£o 1.1:** Se $\{X_t\}$ Ã© um white noise process gaussiano, entÃ£o qualquer conjunto finito de variÃ¡veis aleatÃ³rias $\{X_{t_1}, X_{t_2}, \ldots, X_{t_k}\}$ segue uma distribuiÃ§Ã£o normal multivariada com vetor de mÃ©dias zero e matriz de covariÃ¢ncia diagonal $\sigma^2I$, onde $I$ Ã© a matriz identidade de tamanho $k \times k$.

*Proof:* Esta proposiÃ§Ã£o segue diretamente da definiÃ§Ã£o de um white noise process gaussiano e das propriedades da distribuiÃ§Ã£o normal multivariada. Como as variÃ¡veis sÃ£o independentes, a matriz de covariÃ¢ncia Ã© diagonal.

> ğŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar a ProposiÃ§Ã£o 1.1, vamos gerar um white noise gaussiano com $\sigma^2 = 4$ e verificar se a amostra segue uma distribuiÃ§Ã£o normal.

```python
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt

# ParÃ¢metros
n_samples = 1000
sigma_squared = 4
mean = 0

# Gerar white noise gaussiano
white_noise = np.random.normal(mean, np.sqrt(sigma_squared), n_samples)

# Teste de normalidade Shapiro-Wilk
shapiro_test = stats.shapiro(white_noise)
print(f"Teste de Shapiro-Wilk: {shapiro_test}")

# Histograma
plt.hist(white_noise, bins=30, density=True, alpha=0.6, color='g')
plt.title('Histograma do White Noise Gaussiano')
plt.xlabel('Valor')
plt.ylabel('FrequÃªncia')

# Sobrepor a distribuiÃ§Ã£o normal teÃ³rica
x = np.linspace(mean - 4*np.sqrt(sigma_squared), mean + 4*np.sqrt(sigma_squared), 100)
plt.plot(x, stats.norm.pdf(x, mean, np.sqrt(sigma_squared)), 'k', linewidth=2)
plt.show()
```

O teste de Shapiro-Wilk verifica se a amostra vem de uma distribuiÃ§Ã£o normal. O valor-p retornado indica a probabilidade de observar os dados sob a hipÃ³tese nula de normalidade. Um valor-p alto sugere que a amostra Ã© normalmente distribuÃ­da. O histograma e a distribuiÃ§Ã£o normal teÃ³rica ajudam a visualizar a aderÃªncia da amostra Ã  distribuiÃ§Ã£o normal.

### ConclusÃ£o

A geraÃ§Ã£o eficiente de white noise Ã© um aspecto crucial para a validade dos estudos de simulaÃ§Ã£o em modelos de sÃ©ries temporais [^4, 5]. A escolha de algoritmos de geraÃ§Ã£o de ruÃ­do branco adequados, a realizaÃ§Ã£o de testes estatÃ­sticos rigorosos para verificar as propriedades do ruÃ­do branco gerado e a utilizaÃ§Ã£o de tÃ©cnicas para melhorar a eficiÃªncia dos algoritmos sÃ£o etapas fundamentais para garantir que os resultados das simulaÃ§Ãµes sejam confiÃ¡veis e representativos do sistema real que estÃ¡ sendo modelado. A combinaÃ§Ã£o de uma base teÃ³rica sÃ³lida com uma implementaÃ§Ã£o prÃ¡tica cuidadosa Ã© essencial para o sucesso das simulaÃ§Ãµes de sÃ©ries temporais.

### ReferÃªncias
[^4]: The basic building block for all the processes considered in this chapter is a sequence {8} - whose elements have mean zero and variance ÏƒÂ²,
[^5]: A process satisfying [3.2.1] through [3.2.3] is described as a white noise process.
<!-- END -->