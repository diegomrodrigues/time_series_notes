## 3.3 Gaussian White Noise: Normality in Time Series Analysis

### Introdu√ß√£o

Dando continuidade √† explora√ß√£o dos white noise processes, este cap√≠tulo dedica-se a uma especifica√ß√£o crucial: o **Gaussian white noise process**. Expandindo os conceitos de white noise e independent white noise [^4, 5], introduziremos a condi√ß√£o adicional de que os termos do processo seguem uma distribui√ß√£o normal (gaussiana) com m√©dia zero e vari√¢ncia constante [^5]. A normalidade dos erros √© uma premissa comum em muitos modelos estat√≠sticos, e suas implica√ß√µes para a an√°lise de s√©ries temporais s√£o profundas.

### Defini√ß√£o e Caracter√≠sticas

Um **Gaussian white noise process** $\{\varepsilon_t\}$ √© um processo estoc√°stico que satisfaz as seguintes condi√ß√µes:

1.  $E(\varepsilon_t) = 0$ (M√©dia Zero) [^4]
2.  $E(\varepsilon_t^2) = \sigma^2$ (Vari√¢ncia Constante) [^4]
3.  $\varepsilon_t$ e $\varepsilon_\tau$ s√£o independentes para $t \neq \tau$ (Independ√™ncia)
4.  $\varepsilon_t \sim N(0, \sigma^2)$ (Distribui√ß√£o Normal) [^5]

A √∫ltima condi√ß√£o estabelece que cada termo $\varepsilon_t$ segue uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\sigma^2$. Isso significa que a probabilidade de observar um determinado valor de $\varepsilon_t$ √© dada pela fun√ß√£o densidade de probabilidade (PDF) da distribui√ß√£o normal:

$$f(\varepsilon_t) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\varepsilon_t^2}{2\sigma^2}\right)$$

> üí° **Exemplo Num√©rico:** Suponha que temos um Gaussian white noise process com vari√¢ncia $\sigma^2 = 1$. Queremos calcular a probabilidade de observar um valor de $\varepsilon_t$ dentro do intervalo $[-1, 1]$. Usando a distribui√ß√£o normal padr√£o, podemos aproximar essa probabilidade como:
>
> $\text{Probabilidade} = P(-1 \le \varepsilon_t \le 1) = \int_{-1}^{1} \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x^2}{2}\right) dx \approx 0.6827$
>
> Isso significa que aproximadamente 68.27% dos valores de $\varepsilon_t$ estar√£o dentro de um desvio padr√£o da m√©dia (zero).
> ```python
> import numpy as np
> import scipy.stats as st
>
> # Calculate the probability using scipy.stats.norm.cdf
> prob = st.norm.cdf(1) - st.norm.cdf(-1)
> print(f"Probability of observing a value between -1 and 1: {prob:.4f}")
> ```

> üí° **Exemplo:** Considere um Gaussian white noise process com $\sigma^2 = 1$. A probabilidade de observar $\varepsilon_t = 0$ √© maior do que observar $\varepsilon_t = 3$, pois a distribui√ß√£o normal concentra a probabilidade em torno da m√©dia (zero).

### Parametriza√ß√£o Completa por M√©dia e Vari√¢ncia

Uma propriedade fundamental da distribui√ß√£o normal √© que ela √© completamente caracterizada por seus dois primeiros momentos: a m√©dia e a vari√¢ncia [^5]. Isso significa que, uma vez especificados a m√©dia (zero, no caso do Gaussian white noise) e a vari√¢ncia $\sigma^2$, a distribui√ß√£o de probabilidade do processo est√° totalmente definida [^5].

Essa propriedade tem importantes implica√ß√µes pr√°ticas. Ela simplifica a modelagem e a simula√ß√£o, pois precisamos apenas estimar ou especificar esses dois par√¢metros. Al√©m disso, facilita a utiliza√ß√£o de m√©todos bayesianos, onde priors podem ser facilmente especificados e atualizados [^5].

> üí° **Exemplo Num√©rico:** Vamos considerar dois Gaussian white noise processes: um com $\sigma^2 = 1$ e outro com $\sigma^2 = 4$. Para o primeiro processo, com $\sigma^2 = 1$, um intervalo de confian√ßa de 95% para um √∫nico valor $\varepsilon_t$ seria aproximadamente $[-1.96, 1.96]$, pois $1.96 \times \sqrt{1} \approx 1.96$. Para o segundo processo, com $\sigma^2 = 4$, o mesmo intervalo de confian√ßa seria aproximadamente $[-3.92, 3.92]$, pois $1.96 \times \sqrt{4} = 3.92$. Isso demonstra como a vari√¢ncia afeta a escala dos valores observados.

> üí° **Exemplo:** Se temos um Gaussian white noise process com $\sigma^2 = 4$, sabemos que 95% dos valores de $\varepsilon_t$ estar√£o dentro do intervalo de aproximadamente ¬±2 * sqrt(4) = ¬±4. Isso nos permite ter uma ideia da escala dos valores que esperamos observar.

Para complementar a discuss√£o sobre a parametriza√ß√£o completa, √© importante notar que a independ√™ncia dos termos do processo, combinada com a normalidade, implica que a distribui√ß√£o conjunta de qualquer conjunto finito de termos $\varepsilon_{t_1}, \varepsilon_{t_2}, \ldots, \varepsilon_{t_n}$ √© uma distribui√ß√£o normal multivariada com uma matriz de covari√¢ncia diagonal, onde os elementos da diagonal s√£o todos iguais a $\sigma^2$.

**Teorema 1:** *Se $\{\varepsilon_t\}$ √© um Gaussian white noise process, ent√£o para qualquer conjunto finito de √≠ndices $t_1, t_2, \ldots, t_n$, o vetor $(\varepsilon_{t_1}, \varepsilon_{t_2}, \ldots, \varepsilon_{t_n})$ segue uma distribui√ß√£o normal multivariada com m√©dia zero e matriz de covari√¢ncia $\Sigma = \sigma^2 I$, onde $I$ √© a matriz identidade de tamanho $n \times n$.*

*Prova:* Pela defini√ß√£o de Gaussian white noise, cada $\varepsilon_t$ √© normalmente distribu√≠do com m√©dia zero e vari√¢ncia $\sigma^2$. Al√©m disso, $\varepsilon_t$ e $\varepsilon_\tau$ s√£o independentes para $t \neq \tau$. Portanto, a distribui√ß√£o conjunta de $(\varepsilon_{t_1}, \varepsilon_{t_2}, \ldots, \varepsilon_{t_n})$ √© o produto das distribui√ß√µes marginais, que √© uma distribui√ß√£o normal multivariada com m√©dia zero e matriz de covari√¢ncia diagonal $\Sigma = \sigma^2 I$. ‚ñ†

### Simula√ß√£o de um Gaussian White Noise Process

Simular um Gaussian white noise process √© relativamente simples, pois podemos usar geradores de n√∫meros aleat√≥rios que produzem amostras independentes de uma distribui√ß√£o normal. Na linguagem Python, a fun√ß√£o `np.random.normal` do pacote NumPy √© amplamente utilizada para essa finalidade.

```python
import numpy as np
import matplotlib.pyplot as plt

# Definindo os par√¢metros
n = 1000  # Tamanho da amostra
sigma = 1  # Desvio padr√£o

# Gerando amostras independentes de um Gaussian white noise process
epsilon = np.random.normal(loc=0, scale=sigma, size=n)

# Plotando as primeiras 100 amostras
plt.figure(figsize=(10, 4))
plt.plot(epsilon[:100])
plt.title('Amostra de Gaussian White Noise Process')
plt.xlabel('Tempo')
plt.ylabel('Valor')
plt.show()

# Verificando a m√©dia e a vari√¢ncia amostrais
media_amostral = np.mean(epsilon)
variancia_amostral = np.var(epsilon)
print(f"M√©dia amostral: {media_amostral}")
print(f"Vari√¢ncia amostral: {variancia_amostral}")
```

Este c√≥digo gera `n` amostras independentes de uma distribui√ß√£o normal com m√©dia 0 e desvio padr√£o `sigma`, simulando um Gaussian white noise process. O gr√°fico permite visualizar a aleatoriedade do processo e a aus√™ncia de padr√µes. A m√©dia e a vari√¢ncia amostrais devem se aproximar de 0 e $\sigma^2$, respectivamente, para um grande `n`.

Para complementar o exemplo de simula√ß√£o, podemos adicionar um teste formal para verificar se a amostra gerada realmente se comporta como um Gaussian white noise process. Um teste comum √© o teste de Shapiro-Wilk para normalidade, aplicado aos res√≠duos, e o teste de Ljung-Box para verificar a aus√™ncia de autocorrela√ß√£o.

```python
from scipy.stats import shapiro
import statsmodels.api as sm

# Teste de Shapiro-Wilk para normalidade
stat, p = shapiro(epsilon)
print(f"Teste de Shapiro-Wilk: statistic={stat}, p={p}")

# Teste de Ljung-Box para autocorrela√ß√£o
lbvalue, pvalue = sm.stats.acorr_ljungbox(epsilon, lags=[10]) # Check for autocorrelation up to lag 10
print(f"Teste de Ljung-Box: Q={lbvalue}, p-value={pvalue}")

# Interpreta√ß√£o:
# - Se o p-valor do teste de Shapiro-Wilk for maior que um n√≠vel de signific√¢ncia (e.g., 0.05), n√£o rejeitamos a hip√≥tese nula de normalidade.
# - Se os p-valores do teste de Ljung-Box forem maiores que um n√≠vel de signific√¢ncia, n√£o rejeitamos a hip√≥tese nula de aus√™ncia de autocorrela√ß√£o.
```

Al√©m desses testes, uma an√°lise visual dos res√≠duos pode ser √∫til. Podemos plotar o histograma dos res√≠duos para verificar se ele se aproxima de uma distribui√ß√£o normal, e tamb√©m podemos criar um gr√°fico de quantis normais (QQ-plot) para comparar a distribui√ß√£o dos res√≠duos com a distribui√ß√£o normal te√≥rica.

```python
import scipy.stats as stats

# Histograma dos res√≠duos
plt.figure(figsize=(10, 4))
plt.hist(epsilon, bins=30, density=True, alpha=0.6, color='g')
# Overlay a normal distribution curve
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = stats.norm.pdf(x, 0, sigma)
plt.plot(x, p, 'k', linewidth=2)
plt.title("Histograma dos Res√≠duos")
plt.show()

# QQ-plot
plt.figure(figsize=(10, 4))
stats.probplot(epsilon, dist="norm", plot=plt)
plt.title("QQ-plot dos Res√≠duos")
plt.show()
```

> üí° **Exemplo Num√©rico:** Simule um Gaussian white noise process com $\sigma^2 = 2.25$ e tamanho de amostra $n = 500$. Calcule a m√©dia amostral e a vari√¢ncia amostral. Execute o teste de Shapiro-Wilk e Ljung-Box para verificar a normalidade e a aus√™ncia de autocorrela√ß√£o.
>
> ```python
> import numpy as np
> import scipy.stats as stats
> import statsmodels.api as sm
>
> # Parameters
> n = 500
> sigma_squared = 2.25
> sigma = np.sqrt(sigma_squared)
>
> # Simulate the process
> epsilon = np.random.normal(loc=0, scale=sigma, size=n)
>
> # Calculate sample mean and variance
> sample_mean = np.mean(epsilon)
> sample_variance = np.var(epsilon)
>
> # Shapiro-Wilk test
> shapiro_stat, shapiro_p = stats.shapiro(epsilon)
>
> # Ljung-Box test
> ljungbox_qstat, ljungbox_pvalue = sm.stats.acorr_ljungbox(epsilon, lags=[10], return_df=False)
>
> print(f"Sample Mean: {sample_mean:.4f}")
> print(f"Sample Variance: {sample_variance:.4f}")
> print(f"Shapiro-Wilk Statistic: {shapiro_stat:.4f}, p-value: {shapiro_p:.4f}")
> print(f"Ljung-Box Q-statistic: {ljungbox_qstat[0]:.4f}, p-value: {ljungbox_pvalue[0]:.4f}")
>
> # Interpretation
> alpha = 0.05
> if shapiro_p > alpha:
>     print("Shapiro-Wilk: Sample looks Gaussian (fail to reject H0)")
> else:
>     print("Shapiro-Wilk: Sample does not look Gaussian (reject H0)")
>
> if ljungbox_pvalue[0] > alpha:
>     print("Ljung-Box: No autocorrelation (fail to reject H0)")
> else:
>     print("Ljung-Box: Autocorrelation exists (reject H0)")
> ```
>
> Este exemplo demonstra como simular dados, calcular estat√≠sticas descritivas e aplicar testes estat√≠sticos para verificar as propriedades do Gaussian white noise process.

### Utiliza√ß√£o em M√©todos Bayesianos

Em m√©todos bayesianos, o Gaussian white noise process desempenha um papel importante como modelo para erros ou ru√≠dos em modelos mais complexos. A facilidade de especifica√ß√£o de priors para a m√©dia e a vari√¢ncia do processo simplifica a infer√™ncia bayesiana [^5].

> üí° **Exemplo Num√©rico:** Suponha que estamos modelando uma s√©rie temporal $y_t = \mu + \varepsilon_t$, onde $\mu$ √© uma constante desconhecida e $\varepsilon_t$ √© um Gaussian white noise process com vari√¢ncia $\sigma^2$. Podemos atribuir um prior normal a $\mu$, por exemplo, $\mu \sim N(0, 10)$, e um prior gama inversa a $\sigma^2$, por exemplo, $\sigma^2 \sim \text{InvGamma}(1, 1)$. Dados alguns dados observados $y_1, \ldots, y_n$, podemos usar MCMC para amostrar a distribui√ß√£o posterior de $\mu$ e $\sigma^2$.

> üí° **Exemplo:** Em um modelo de regress√£o linear bayesiano, podemos assumir que os erros seguem um Gaussian white noise process com m√©dia zero e vari√¢ncia $\sigma^2$. Podemos ent√£o atribuir um prior a $\sigma^2$, como uma distribui√ß√£o gama inversa, e usar m√©todos de Monte Carlo via Cadeias de Markov (MCMC) para amostrar a distribui√ß√£o posterior dos par√¢metros do modelo, incluindo $\sigma^2$.

Al√©m do exemplo da regress√£o linear Bayesiana, outro uso comum do Gaussian white noise em Bayesianos √© na modelagem de processos din√¢micos. Por exemplo, em um modelo de espa√ßo de estados, o ru√≠do do processo e o ru√≠do de observa√ß√£o s√£o frequentemente modelados como Gaussian white noise. Isso permite a utiliza√ß√£o do filtro de Kalman para infer√™ncia.

Para ilustrar melhor o uso de priors, podemos detalhar como um prior conjugado para a vari√¢ncia $\sigma^2$ facilita a an√°lise Bayesiana.

**Teorema 2:** *Se $\{\varepsilon_t\}$ √© um Gaussian white noise process com m√©dia zero e vari√¢ncia $\sigma^2$, e assumimos um prior gama inversa para $\sigma^2$, ou seja, $\sigma^2 \sim InvGamma(\alpha, \beta)$, ent√£o a distribui√ß√£o posterior de $\sigma^2$ tamb√©m √© uma gama inversa.*

*Prova:* A distribui√ß√£o posterior √© proporcional ao produto da likelihood e do prior. A likelihood √© dada por:

$$L(\sigma^2 | \varepsilon_1, \ldots, \varepsilon_n) \propto (\sigma^2)^{-n/2} \exp\left(-\frac{\sum_{t=1}^{n} \varepsilon_t^2}{2\sigma^2}\right)$$

I. Dado que $\{\varepsilon_t\}$ √© um Gaussian white noise process, cada $\varepsilon_t$ √© independentemente distribu√≠do como $N(0, \sigma^2)$. Portanto, a fun√ß√£o de verossimilhan√ßa para $\sigma^2$ dado $\varepsilon_1, \ldots, \varepsilon_n$ √© o produto das fun√ß√µes de densidade normal:
    $$L(\sigma^2 | \varepsilon_1, \ldots, \varepsilon_n) = \prod_{t=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\varepsilon_t^2}{2\sigma^2}\right)$$

II. Simplificando a express√£o, obtemos:
     $$L(\sigma^2 | \varepsilon_1, \ldots, \varepsilon_n) = (2\pi\sigma^2)^{-n/2} \exp\left(-\frac{\sum_{t=1}^{n} \varepsilon_t^2}{2\sigma^2}\right)$$

III. Ignorando o fator constante $(2\pi)^{-n/2}$, a verossimilhan√ßa √© proporcional a:
      $$L(\sigma^2 | \varepsilon_1, \ldots, \varepsilon_n) \propto (\sigma^2)^{-n/2} \exp\left(-\frac{\sum_{t=1}^{n} \varepsilon_t^2}{2\sigma^2}\right)$$

O prior √© dado por:

$$p(\sigma^2) \propto (\sigma^2)^{-(\alpha+1)} \exp\left(-\frac{\beta}{\sigma^2}\right)$$

IV. Assumimos um prior gama inversa para $\sigma^2$, ou seja, $\sigma^2 \sim InvGamma(\alpha, \beta)$. A fun√ß√£o densidade de probabilidade da distribui√ß√£o gama inversa √© dada por:
    $$p(\sigma^2) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} (\sigma^2)^{-(\alpha+1)} \exp\left(-\frac{\beta}{\sigma^2}\right)$$
   Onde $\Gamma(\alpha)$ √© a fun√ß√£o gama.

V. Ignorando o fator constante $\frac{\beta^{\alpha}}{\Gamma(\alpha)}$, o prior √© proporcional a:
    $$p(\sigma^2) \propto (\sigma^2)^{-(\alpha+1)} \exp\left(-\frac{\beta}{\sigma^2}\right)$$

O posterior √© ent√£o proporcional a:

$$p(\sigma^2 | \varepsilon_1, \ldots, \varepsilon_n) \propto (\sigma^2)^{-(n/2 + \alpha + 1)} \exp\left(-\frac{\sum_{t=1}^{n} \varepsilon_t^2 + 2\beta}{2\sigma^2}\right)$$

VI. A distribui√ß√£o posterior √© proporcional ao produto da verossimilhan√ßa e do prior:
    $$p(\sigma^2 | \varepsilon_1, \ldots, \varepsilon_n) \propto L(\sigma^2 | \varepsilon_1, \ldots, \varepsilon_n) \cdot p(\sigma^2)$$

VII. Substituindo as express√µes para a verossimilhan√ßa e o prior:
     $$p(\sigma^2 | \varepsilon_1, \ldots, \varepsilon_n) \propto (\sigma^2)^{-n/2} \exp\left(-\frac{\sum_{t=1}^{n} \varepsilon_t^2}{2\sigma^2}\right) \cdot (\sigma^2)^{-(\alpha+1)} \exp\left(-\frac{\beta}{\sigma^2}\right)$$

VIII. Combinando os termos exponenciais e as pot√™ncias de $\sigma^2$:
      $$p(\sigma^2 | \varepsilon_1, \ldots, \varepsilon_n) \propto (\sigma^2)^{-(n/2 + \alpha + 1)} \exp\left(-\frac{\sum_{t=1}^{n} \varepsilon_t^2 + 2\beta}{2\sigma^2}\right)$$

Esta √© a forma funcional de uma distribui√ß√£o gama inversa com par√¢metros $\alpha' = \alpha + n/2$ e $\beta' = \beta + \frac{1}{2}\sum_{t=1}^{n} \varepsilon_t^2$. Portanto, a gama inversa √© um prior conjugado para a vari√¢ncia do Gaussian white noise. ‚ñ†

> üí° **Exemplo Num√©rico:** Vamos usar o teorema acima. Suponha que temos 100 observa√ß√µes de um Gaussian white noise process, ou seja, $n = 100$. Assumimos um prior gama inversa para $\sigma^2$ com $\alpha = 2$ e $\beta = 1$. Ap√≥s observar os dados, calculamos que $\sum_{t=1}^{100} \varepsilon_t^2 = 150$. Ent√£o, os par√¢metros da distribui√ß√£o posterior de $\sigma^2$ ser√£o:
>
> $\alpha' = \alpha + n/2 = 2 + 100/2 = 52$
> $\beta' = \beta + \frac{1}{2}\sum_{t=1}^{n} \varepsilon_t^2 = 1 + \frac{1}{2}(150) = 76$
>
> Portanto, a distribui√ß√£o posterior de $\sigma^2$ √© $\text{InvGamma}(52, 76)$. Podemos usar essa distribui√ß√£o posterior para fazer infer√™ncias sobre $\sigma^2$, como calcular intervalos de credibilidade.

### Transforma√ß√µes e Linearidade

O Gaussian white noise process preserva a normalidade sob transforma√ß√µes lineares. Mais precisamente, qualquer combina√ß√£o linear de vari√°veis aleat√≥rias gaussianas independentes tamb√©m segue uma distribui√ß√£o normal. Esse resultado √© crucial para a an√°lise de modelos lineares de s√©ries temporais.

**Teorema 1.3:** *Se $\{\varepsilon_t\}$ √© um Gaussian white noise process com m√©dia zero e vari√¢ncia $\sigma^2$, e $\{a_i\}$ s√£o coeficientes constantes, ent√£o a combina√ß√£o linear $X = \sum_{i=1}^{n} a_i \varepsilon_i$ tamb√©m segue uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\sigma^2 \sum_{i=1}^{n} a_i^2$.*

*Prova:* A prova deste teorema decorre da propriedade de que combina√ß√µes lineares de vari√°veis aleat√≥rias normais independentes tamb√©m seguem uma distribui√ß√£o normal. J√° vimos que a m√©dia √© zero e a vari√¢ncia √© $\sigma^2 \sum_{i=1}^{n} a_i^2$.

Vamos provar o Teorema 1.3 passo a passo:

I. Seja $\{\varepsilon_t\}$ um Gaussian white noise process, o que significa que cada $\varepsilon_t$ segue uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\sigma^2$.

II. Seja $X = \sum_{i=1}^{n} a_i \varepsilon_i$, onde $a_i$ s√£o coeficientes constantes.

III. Queremos mostrar que X tamb√©m segue uma distribui√ß√£o normal.

IV. A propriedade fundamental que usaremos √© que uma combina√ß√£o linear de vari√°veis aleat√≥rias normais independentes tamb√©m segue uma distribui√ß√£o normal. Formalmente, se $X_1, X_2, \ldots, X_n$ s√£o vari√°veis aleat√≥rias normais independentes e $c_1, c_2, \ldots, c_n$ s√£o constantes, ent√£o a vari√°vel aleat√≥ria $Z = \sum_{i=1}^{n} c_i X_i$ tamb√©m segue uma distribui√ß√£o normal.

V. Como cada $\varepsilon_i$ segue uma distribui√ß√£o normal e s√£o independentes (devido √† defini√ß√£o de Gaussian white noise process), e $a_i$ s√£o constantes, ent√£o a combina√ß√£o linear $X = \sum_{i=1}^{n} a_i \varepsilon_i$ tamb√©m segue uma distribui√ß√£o normal.

VI. A m√©dia de X √©:
    $$E[X] = E\left[\sum_{i=1}^{n} a_i \varepsilon_i\right] = \sum_{i=1}^{n} a_i E[\varepsilon_i] = \sum_{i=1}^{n} a_i \cdot 0 = 0$$

VII. A vari√¢ncia de X √©:
    $$Var(X) = Var\left(\sum_{i=1}^{n} a_i \varepsilon_i\right) = \sum_{i=1}^{n} a_i^2 Var(\varepsilon_i) = \sum_{i=1}^{n} a_i^2 \sigma^2 = \sigma^2 \sum_{i=1}^{n} a_i^2$$

Portanto, $X \sim N\left(0, \sigma^2 \sum_{i=1}^{n} a_i^2\right)$. ‚ñ†

**Lema 3:** Seja $\{\varepsilon_t\}$ um Gaussian white noise process. Seja $f(x)$ uma fun√ß√£o linear. Ent√£o, o processo $\{f(\varepsilon_t)\}$ tamb√©m √© um processo Gaussiano.

*Prova:* Este resultado √© uma consequ√™ncia direta do Teorema 1.3. Como f √© linear, $f(\varepsilon_t) = a\varepsilon_t$ para algum escalar a. Portanto, $f(\varepsilon_t)$ √© apenas uma escala de um processo gaussiano, que ainda √© gaussiano.

Vamos provar o Lema 3 passo a passo:

I.  Dado que $\{\varepsilon_t\}$ √© um Gaussian white noise process, $\varepsilon_t$ segue uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\sigma^2$.

II. Seja f(x) uma fun√ß√£o linear, o que significa que $f(x) = ax + b$, onde a e b s√£o constantes.

III. Queremos mostrar que o processo $\{f(\varepsilon_t)\}$ tamb√©m √© um processo Gaussiano.

IV. Consideremos $f(\varepsilon_t) = a\varepsilon_t + b$. J√° sabemos que $\varepsilon_t$ segue uma distribui√ß√£o normal.

V. Usando as propriedades da distribui√ß√£o normal:
    - Se $\varepsilon_t \sim N(0, \sigma^2)$, ent√£o $a\varepsilon_t \sim N(0, a^2\sigma^2)$.
    - Se $X \sim N(\mu, \sigma^2)$, ent√£o $X + b \sim N(\mu + b, \sigma^2)$.

VI. Portanto, $f(\varepsilon_t) = a\varepsilon_t + b$ segue uma distribui√ß√£o normal com m√©dia $E[f(\varepsilon_t)] = a \cdot 0 + b = b$ e vari√¢ncia $Var(f(\varepsilon_t)) = a^2 \sigma^2$.

VII. Assim, $f(\varepsilon_t) \sim N(b, a^2\sigma^2)$.

Portanto, se $\{\varepsilon_t\}$ √© um Gaussian white noise process e f(x) √© uma fun√ß√£o linear, ent√£o o processo $\{f(\varepsilon_t)\}$ tamb√©m √© um processo Gaussiano. ‚ñ†

Considerando a import√¢ncia das transforma√ß√µes lineares, podemos estender o resultado para transforma√ß√µes afins:

**Corol√°rio 3.1:** *Se $\{\varepsilon_t\}$ √© um Gaussian white noise process e $f(x) = ax + b$ √© uma transforma√ß√£o afim, ent√£o $\{f(\varepsilon_t)\}$ √© um processo Gaussiano com m√©dia $b$ e vari√¢ncia $a^2\sigma^2$.*

*Prova:* Este corol√°rio √© uma consequ√™ncia direta do Lema 3. Se $\varepsilon_t \sim N(0, \sigma^2)$, ent√£o $a\varepsilon_t + b \sim N(b, a^2\sigma^2)$. Portanto, o processo transformado ainda √© Gaussiano, mas com m√©dia e vari√¢ncia ajustadas pela transforma√ß√£o afim. ‚ñ†

> üí° **Exemplo Num√©rico:** Seja $\{\varepsilon_t\}$ um Gaussian white noise process com $\sigma^2 = 1$. Considere a transforma√ß√£o linear $X = 2\varepsilon_1 - \varepsilon_2 + 3\varepsilon_3$. Pelo Teorema 1.3, $X$ tamb√©m segue uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\sigma^2 \sum_{i=1}^{3} a_i^2 = 1 \cdot (2^2 + (-1)^2 + 3^2) = 14$. Portanto, $X \sim N(0, 14)$.

### Relev√¢ncia em Modelos ARMA

A premissa de que o ru√≠do √© um Gaussian white noise process √© fundamental para a validade de muitas t√©cnicas de infer√™ncia em modelos ARMA [^4, 5]. Por exemplo, sob essa premissa, os estimadores de m√°xima verossimilhan√ßa (MLE) dos par√¢metros do modelo s√£o assintoticamente normais, o que permite a constru√ß√£o de intervalos de confian√ßa e testes de hip√≥teses. Al√©m disso, a normalidade dos erros simplifica o c√°lculo das fun√ß√µes de previs√£o e a avalia√ß√£o da incerteza associada √†s previs√µes.

> üí° **Exemplo:** Se ajustamos um modelo ARMA(p, q) aos dados e assumimos que os erros seguem um Gaussian white noise process, podemos usar o teste de Box-Pierce ou o teste de Ljung-Box para verificar se os res√≠duos do modelo se comportam como um Gaussian white noise process. Se os testes falham, isso indica que o modelo pode n√£o ser adequado para os dados.

Um outro teste importante para verificar se os res√≠duos se comportam como um Gaussian white noise process √© o teste de Jarque-Bera, que testa a hip√≥tese nula de que a amostra vem de uma distribui√ß√£o normal.

Ainda sobre a relev√¢ncia em modelos ARMA, vale ressaltar a import√¢ncia da fun√ß√£o de autocorrela√ß√£o (ACF) e da fun√ß√£o de autocorrela√ß√£o parcial (PACF) dos res√≠duos. Se os res√≠duos se comportam como um Gaussian white noise, suas ACF e PACF devem estar dentro dos limites de confian√ßa de zero. Desvios significativos desses limites indicam que o modelo ARMA n√£o capturou toda a estrutura de depend√™ncia dos dados.

> üí° **Exemplo Num√©rico:** Ajuste um modelo ARMA(1,1) a uma s√©rie temporal simulada. Extraia os res√≠duos e verifique se eles se comportam como um Gaussian white noise process.
>
> ```python
> import numpy as np
> import statsmodels.api as sm
> from statsmodels.tsa.arima.model import ARIMA
> import scipy.stats as stats
> import matplotlib.pyplot as plt
>
> # Simulate ARMA(1,1) data
> np.random.seed(0)
> n = 200
> arparams = np.array([.75])
> maparams = np.array([.65])
> ar = np.r_[1, -arparams]
> ma = np.r_[1, maparams]
> y = sm.tsa.arma_generate_sample(ar, ma, n)
>
> # Fit ARMA(1,1) model
> model = ARIMA(y, order=(1, 0, 1))
> model_fit = model.fit()
> residuals = model_fit.resid
>
> # Shapiro-Wilk test
> shapiro_stat, shapiro_p = stats.shapiro(residuals)
> print(f"Shapiro-Wilk Statistic: {shapiro_stat:.4f}, p-value: {shapiro_p:.4f}")
>
> # Ljung-Box test
> ljungbox_qstat, ljungbox_pvalue = sm.stats.acorr_ljungbox(residuals, lags=[10], return_df=False)
> print(f"Ljung-Box Q-statistic: {ljungbox_qstat[0]:.4f}, p-value: {ljungbox_pvalue[0]:.4f}")
>
> # Plot ACF and PACF
> fig, axes = plt.subplots(2, 1, figsize=(10, 8))
> sm.graphics.tsa.plot_acf(residuals, lags=20, ax=axes[0])
> sm.graphics.tsa.plot_pacf(residuals, lags=20, ax=axes[1])
> plt.show()
>
> # Interpretation
> alpha = 0.05
> if shapiro_p > alpha:
>     print("Shapiro-Wilk: Residuals look Gaussian (fail to reject H0)")
> else:
>     print("Shapiro-Wilk: Residuals do not look Gaussian (reject H0)")
>
> if ljungbox_pvalue[0] > alpha:
>     print("Ljung-Box: No autocorrelation in residuals (fail to reject H0)")
> else:
>     print("Ljung-Box: Autocorrelation exists in residuals (reject H0)")
> ```
>
> Este exemplo demonstra como ajustar um modelo ARMA, extrair os res√≠duos e verificar se eles satisfazem as propriedades de um Gaussian white noise process. As ACF e PACF plots ajudam a visualizar qualquer autocorrela√ß√£o restante nos res√≠duos.

### Conclus√£o

O Gaussian white noise process √© uma especifica√ß√£o comum e √∫til do white noise process, adicionando a premissa de normalidade √† m√©dia zero, vari√¢ncia constante e independ√™ncia. Essa premissa simplifica an√°lises, simula√ß√µes e infer√™ncias estat√≠sticas, particularmente em modelos ARMA e m√©todos bayesianos. A facilidade de parametriza√ß√£o, a preserva√ß√£o da normalidade sob transforma√ß√µes lineares e as propriedades bem compreendidas da distribui√ß√£o normal o tornam uma ferramenta valiosa na modelagem e an√°lise de s√©ries temporais.

### Refer√™ncias
[^4]: The basic building block for all the processes considered in this chapter is a sequence {8} - whose elements have mean zero and variance œÉ¬≤,
[^5]: A process satisfying [3.2.1] through [3.2.3] is described as a white noise process.
<!-- END -->