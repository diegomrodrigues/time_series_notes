## 3.3 Gaussian White Noise: Normality in Time Series Analysis

### IntroduÃ§Ã£o

Dando continuidade Ã  exploraÃ§Ã£o dos white noise processes, este capÃ­tulo dedica-se a uma especificaÃ§Ã£o crucial: o **Gaussian white noise process**. Expandindo os conceitos de white noise e independent white noise [^4, 5], introduziremos a condiÃ§Ã£o adicional de que os termos do processo seguem uma distribuiÃ§Ã£o normal (gaussiana) com mÃ©dia zero e variÃ¢ncia constante [^5]. A normalidade dos erros Ã© uma premissa comum em muitos modelos estatÃ­sticos, e suas implicaÃ§Ãµes para a anÃ¡lise de sÃ©ries temporais sÃ£o profundas.

### DefiniÃ§Ã£o e CaracterÃ­sticas

Um **Gaussian white noise process** $\{\varepsilon_t\}$ Ã© um processo estocÃ¡stico que satisfaz as seguintes condiÃ§Ãµes:

1.  $E(\varepsilon_t) = 0$ (MÃ©dia Zero) [^4]
2.  $E(\varepsilon_t^2) = \sigma^2$ (VariÃ¢ncia Constante) [^4]
3.  $\varepsilon_t$ e $\varepsilon_\tau$ sÃ£o independentes para $t \neq \tau$ (IndependÃªncia)
4.  $\varepsilon_t \sim N(0, \sigma^2)$ (DistribuiÃ§Ã£o Normal) [^5]

A Ãºltima condiÃ§Ã£o estabelece que cada termo $\varepsilon_t$ segue uma distribuiÃ§Ã£o normal com mÃ©dia zero e variÃ¢ncia $\sigma^2$. Isso significa que a probabilidade de observar um determinado valor de $\varepsilon_t$ Ã© dada pela funÃ§Ã£o densidade de probabilidade (PDF) da distribuiÃ§Ã£o normal:

$$f(\varepsilon_t) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\varepsilon_t^2}{2\sigma^2}\right)$$

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que temos um Gaussian white noise process com variÃ¢ncia $\sigma^2 = 1$. Queremos calcular a probabilidade de observar um valor de $\varepsilon_t$ dentro do intervalo $[-1, 1]$. Usando a distribuiÃ§Ã£o normal padrÃ£o, podemos aproximar essa probabilidade como:
>
> $\text{Probabilidade} = P(-1 \le \varepsilon_t \le 1) = \int_{-1}^{1} \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x^2}{2}\right) dx \approx 0.6827$
>
> Isso significa que aproximadamente 68.27% dos valores de $\varepsilon_t$ estarÃ£o dentro de um desvio padrÃ£o da mÃ©dia (zero).
> ```python
> import numpy as np
> import scipy.stats as st
>
> # Calculate the probability using scipy.stats.norm.cdf
> prob = st.norm.cdf(1) - st.norm.cdf(-1)
> print(f"Probability of observing a value between -1 and 1: {prob:.4f}")
> ```

> ðŸ’¡ **Exemplo:** Considere um Gaussian white noise process com $\sigma^2 = 1$. A probabilidade de observar $\varepsilon_t = 0$ Ã© maior do que observar $\varepsilon_t = 3$, pois a distribuiÃ§Ã£o normal concentra a probabilidade em torno da mÃ©dia (zero).

### ParametrizaÃ§Ã£o Completa por MÃ©dia e VariÃ¢ncia

Uma propriedade fundamental da distribuiÃ§Ã£o normal Ã© que ela Ã© completamente caracterizada por seus dois primeiros momentos: a mÃ©dia e a variÃ¢ncia [^5]. Isso significa que, uma vez especificados a mÃ©dia (zero, no caso do Gaussian white noise) e a variÃ¢ncia $\sigma^2$, a distribuiÃ§Ã£o de probabilidade do processo estÃ¡ totalmente definida [^5].

Essa propriedade tem importantes implicaÃ§Ãµes prÃ¡ticas. Ela simplifica a modelagem e a simulaÃ§Ã£o, pois precisamos apenas estimar ou especificar esses dois parÃ¢metros. AlÃ©m disso, facilita a utilizaÃ§Ã£o de mÃ©todos bayesianos, onde priors podem ser facilmente especificados e atualizados [^5].

> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos considerar dois Gaussian white noise processes: um com $\sigma^2 = 1$ e outro com $\sigma^2 = 4$. Para o primeiro processo, com $\sigma^2 = 1$, um intervalo de confianÃ§a de 95% para um Ãºnico valor $\varepsilon_t$ seria aproximadamente $[-1.96, 1.96]$, pois $1.96 \times \sqrt{1} \approx 1.96$. Para o segundo processo, com $\sigma^2 = 4$, o mesmo intervalo de confianÃ§a seria aproximadamente $[-3.92, 3.92]$, pois $1.96 \times \sqrt{4} = 3.92$. Isso demonstra como a variÃ¢ncia afeta a escala dos valores observados.

> ðŸ’¡ **Exemplo:** Se temos um Gaussian white noise process com $\sigma^2 = 4$, sabemos que 95% dos valores de $\varepsilon_t$ estarÃ£o dentro do intervalo de aproximadamente Â±2 * sqrt(4) = Â±4. Isso nos permite ter uma ideia da escala dos valores que esperamos observar.

Para complementar a discussÃ£o sobre a parametrizaÃ§Ã£o completa, Ã© importante notar que a independÃªncia dos termos do processo, combinada com a normalidade, implica que a distribuiÃ§Ã£o conjunta de qualquer conjunto finito de termos $\varepsilon_{t_1}, \varepsilon_{t_2}, \ldots, \varepsilon_{t_n}$ Ã© uma distribuiÃ§Ã£o normal multivariada com uma matriz de covariÃ¢ncia diagonal, onde os elementos da diagonal sÃ£o todos iguais a $\sigma^2$.

**Teorema 1:** *Se $\{\varepsilon_t\}$ Ã© um Gaussian white noise process, entÃ£o para qualquer conjunto finito de Ã­ndices $t_1, t_2, \ldots, t_n$, o vetor $(\varepsilon_{t_1}, \varepsilon_{t_2}, \ldots, \varepsilon_{t_n})$ segue uma distribuiÃ§Ã£o normal multivariada com mÃ©dia zero e matriz de covariÃ¢ncia $\Sigma = \sigma^2 I$, onde $I$ Ã© a matriz identidade de tamanho $n \times n$.*

*Prova:* Pela definiÃ§Ã£o de Gaussian white noise, cada $\varepsilon_t$ Ã© normalmente distribuÃ­do com mÃ©dia zero e variÃ¢ncia $\sigma^2$. AlÃ©m disso, $\varepsilon_t$ e $\varepsilon_\tau$ sÃ£o independentes para $t \neq \tau$. Portanto, a distribuiÃ§Ã£o conjunta de $(\varepsilon_{t_1}, \varepsilon_{t_2}, \ldots, \varepsilon_{t_n})$ Ã© o produto das distribuiÃ§Ãµes marginais, que Ã© uma distribuiÃ§Ã£o normal multivariada com mÃ©dia zero e matriz de covariÃ¢ncia diagonal $\Sigma = \sigma^2 I$. â– 

### SimulaÃ§Ã£o de um Gaussian White Noise Process

Simular um Gaussian white noise process Ã© relativamente simples, pois podemos usar geradores de nÃºmeros aleatÃ³rios que produzem amostras independentes de uma distribuiÃ§Ã£o normal. Na linguagem Python, a funÃ§Ã£o `np.random.normal` do pacote NumPy Ã© amplamente utilizada para essa finalidade.

```python
import numpy as np
import matplotlib.pyplot as plt

# Definindo os parÃ¢metros
n = 1000  # Tamanho da amostra
sigma = 1  # Desvio padrÃ£o

# Gerando amostras independentes de um Gaussian white noise process
epsilon = np.random.normal(loc=0, scale=sigma, size=n)

# Plotando as primeiras 100 amostras
plt.figure(figsize=(10, 4))
plt.plot(epsilon[:100])
plt.title('Amostra de Gaussian White Noise Process')
plt.xlabel('Tempo')
plt.ylabel('Valor')
plt.show()

# Verificando a mÃ©dia e a variÃ¢ncia amostrais
media_amostral = np.mean(epsilon)
variancia_amostral = np.var(epsilon)
print(f"MÃ©dia amostral: {media_amostral}")
print(f"VariÃ¢ncia amostral: {variancia_amostral}")
```

Este cÃ³digo gera `n` amostras independentes de uma distribuiÃ§Ã£o normal com mÃ©dia 0 e desvio padrÃ£o `sigma`, simulando um Gaussian white noise process. O grÃ¡fico permite visualizar a aleatoriedade do processo e a ausÃªncia de padrÃµes. A mÃ©dia e a variÃ¢ncia amostrais devem se aproximar de 0 e $\sigma^2$, respectivamente, para um grande `n`.

Para complementar o exemplo de simulaÃ§Ã£o, podemos adicionar um teste formal para verificar se a amostra gerada realmente se comporta como um Gaussian white noise process. Um teste comum Ã© o teste de Shapiro-Wilk para normalidade, aplicado aos resÃ­duos, e o teste de Ljung-Box para verificar a ausÃªncia de autocorrelaÃ§Ã£o.

```python
from scipy.stats import shapiro
import statsmodels.api as sm

# Teste de Shapiro-Wilk para normalidade
stat, p = shapiro(epsilon)
print(f"Teste de Shapiro-Wilk: statistic={stat}, p={p}")

# Teste de Ljung-Box para autocorrelaÃ§Ã£o
lbvalue, pvalue = sm.stats.acorr_ljungbox(epsilon, lags=[10]) # Check for autocorrelation up to lag 10
print(f"Teste de Ljung-Box: Q={lbvalue}, p-value={pvalue}")

# InterpretaÃ§Ã£o:
# - Se o p-valor do teste de Shapiro-Wilk for maior que um nÃ­vel de significÃ¢ncia (e.g., 0.05), nÃ£o rejeitamos a hipÃ³tese nula de normalidade.
# - Se os p-valores do teste de Ljung-Box forem maiores que um nÃ­vel de significÃ¢ncia, nÃ£o rejeitamos a hipÃ³tese nula de ausÃªncia de autocorrelaÃ§Ã£o.
```

AlÃ©m desses testes, uma anÃ¡lise visual dos resÃ­duos pode ser Ãºtil. Podemos plotar o histograma dos resÃ­duos para verificar se ele se aproxima de uma distribuiÃ§Ã£o normal, e tambÃ©m podemos criar um grÃ¡fico de quantis normais (QQ-plot) para comparar a distribuiÃ§Ã£o dos resÃ­duos com a distribuiÃ§Ã£o normal teÃ³rica.

```python
import scipy.stats as stats

# Histograma dos resÃ­duos
plt.figure(figsize=(10, 4))
plt.hist(epsilon, bins=30, density=True, alpha=0.6, color='g')
# Overlay a normal distribution curve
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = stats.norm.pdf(x, 0, sigma)
plt.plot(x, p, 'k', linewidth=2)
plt.title("Histograma dos ResÃ­duos")
plt.show()

# QQ-plot
plt.figure(figsize=(10, 4))
stats.probplot(epsilon, dist="norm", plot=plt)
plt.title("QQ-plot dos ResÃ­duos")
plt.show()
```

> ðŸ’¡ **Exemplo NumÃ©rico:** Simule um Gaussian white noise process com $\sigma^2 = 2.25$ e tamanho de amostra $n = 500$. Calcule a mÃ©dia amostral e a variÃ¢ncia amostral. Execute o teste de Shapiro-Wilk e Ljung-Box para verificar a normalidade e a ausÃªncia de autocorrelaÃ§Ã£o.
>
> ```python
> import numpy as np
> import scipy.stats as stats
> import statsmodels.api as sm
>
> # Parameters
> n = 500
> sigma_squared = 2.25
> sigma = np.sqrt(sigma_squared)
>
> # Simulate the process
> epsilon = np.random.normal(loc=0, scale=sigma, size=n)
>
> # Calculate sample mean and variance
> sample_mean = np.mean(epsilon)
> sample_variance = np.var(epsilon)
>
> # Shapiro-Wilk test
> shapiro_stat, shapiro_p = stats.shapiro(epsilon)
>
> # Ljung-Box test
> ljungbox_qstat, ljungbox_pvalue = sm.stats.acorr_ljungbox(epsilon, lags=[10], return_df=False)
>
> print(f"Sample Mean: {sample_mean:.4f}")
> print(f"Sample Variance: {sample_variance:.4f}")
> print(f"Shapiro-Wilk Statistic: {shapiro_stat:.4f}, p-value: {shapiro_p:.4f}")
> print(f"Ljung-Box Q-statistic: {ljungbox_qstat[0]:.4f}, p-value: {ljungbox_pvalue[0]:.4f}")
>
> # Interpretation
> alpha = 0.05
> if shapiro_p > alpha:
>     print("Shapiro-Wilk: Sample looks Gaussian (fail to reject H0)")
> else:
>     print("Shapiro-Wilk: Sample does not look Gaussian (reject H0)")
>
> if ljungbox_pvalue[0] > alpha:
>     print("Ljung-Box: No autocorrelation (fail to reject H0)")
> else:
>     print("Ljung-Box: Autocorrelation exists (reject H0)")
> ```
>
> Este exemplo demonstra como simular dados, calcular estatÃ­sticas descritivas e aplicar testes estatÃ­sticos para verificar as propriedades do Gaussian white noise process.

### UtilizaÃ§Ã£o em MÃ©todos Bayesianos

Em mÃ©todos bayesianos, o Gaussian white noise process desempenha um papel importante como modelo para erros ou ruÃ­dos em modelos mais complexos. A facilidade de especificaÃ§Ã£o de priors para a mÃ©dia e a variÃ¢ncia do processo simplifica a inferÃªncia bayesiana [^5].

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que estamos modelando uma sÃ©rie temporal $y_t = \mu + \varepsilon_t$, onde $\mu$ Ã© uma constante desconhecida e $\varepsilon_t$ Ã© um Gaussian white noise process com variÃ¢ncia $\sigma^2$. Podemos atribuir um prior normal a $\mu$, por exemplo, $\mu \sim N(0, 10)$, e um prior gama inversa a $\sigma^2$, por exemplo, $\sigma^2 \sim \text{InvGamma}(1, 1)$. Dados alguns dados observados $y_1, \ldots, y_n$, podemos usar MCMC para amostrar a distribuiÃ§Ã£o posterior de $\mu$ e $\sigma^2$.

> ðŸ’¡ **Exemplo:** Em um modelo de regressÃ£o linear bayesiano, podemos assumir que os erros seguem um Gaussian white noise process com mÃ©dia zero e variÃ¢ncia $\sigma^2$. Podemos entÃ£o atribuir um prior a $\sigma^2$, como uma distribuiÃ§Ã£o gama inversa, e usar mÃ©todos de Monte Carlo via Cadeias de Markov (MCMC) para amostrar a distribuiÃ§Ã£o posterior dos parÃ¢metros do modelo, incluindo $\sigma^2$.

AlÃ©m do exemplo da regressÃ£o linear Bayesiana, outro uso comum do Gaussian white noise em Bayesianos Ã© na modelagem de processos dinÃ¢micos. Por exemplo, em um modelo de espaÃ§o de estados, o ruÃ­do do processo e o ruÃ­do de observaÃ§Ã£o sÃ£o frequentemente modelados como Gaussian white noise. Isso permite a utilizaÃ§Ã£o do filtro de Kalman para inferÃªncia.

Para ilustrar melhor o uso de priors, podemos detalhar como um prior conjugado para a variÃ¢ncia $\sigma^2$ facilita a anÃ¡lise Bayesiana.

**Teorema 2:** *Se $\{\varepsilon_t\}$ Ã© um Gaussian white noise process com mÃ©dia zero e variÃ¢ncia $\sigma^2$, e assumimos um prior gama inversa para $\sigma^2$, ou seja, $\sigma^2 \sim InvGamma(\alpha, \beta)$, entÃ£o a distribuiÃ§Ã£o posterior de $\sigma^2$ tambÃ©m Ã© uma gama inversa.*

*Prova:* A distribuiÃ§Ã£o posterior Ã© proporcional ao produto da likelihood e do prior. A likelihood Ã© dada por:

$$L(\sigma^2 | \varepsilon_1, \ldots, \varepsilon_n) \propto (\sigma^2)^{-n/2} \exp\left(-\frac{\sum_{t=1}^{n} \varepsilon_t^2}{2\sigma^2}\right)$$

I. Dado que $\{\varepsilon_t\}$ Ã© um Gaussian white noise process, cada $\varepsilon_t$ Ã© independentemente distribuÃ­do como $N(0, \sigma^2)$. Portanto, a funÃ§Ã£o de verossimilhanÃ§a para $\sigma^2$ dado $\varepsilon_1, \ldots, \varepsilon_n$ Ã© o produto das funÃ§Ãµes de densidade normal:
    $$L(\sigma^2 | \varepsilon_1, \ldots, \varepsilon_n) = \prod_{t=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\varepsilon_t^2}{2\sigma^2}\right)$$

II. Simplificando a expressÃ£o, obtemos:
     $$L(\sigma^2 | \varepsilon_1, \ldots, \varepsilon_n) = (2\pi\sigma^2)^{-n/2} \exp\left(-\frac{\sum_{t=1}^{n} \varepsilon_t^2}{2\sigma^2}\right)$$

III. Ignorando o fator constante $(2\pi)^{-n/2}$, a verossimilhanÃ§a Ã© proporcional a:
      $$L(\sigma^2 | \varepsilon_1, \ldots, \varepsilon_n) \propto (\sigma^2)^{-n/2} \exp\left(-\frac{\sum_{t=1}^{n} \varepsilon_t^2}{2\sigma^2}\right)$$

O prior Ã© dado por:

$$p(\sigma^2) \propto (\sigma^2)^{-(\alpha+1)} \exp\left(-\frac{\beta}{\sigma^2}\right)$$

IV. Assumimos um prior gama inversa para $\sigma^2$, ou seja, $\sigma^2 \sim InvGamma(\alpha, \beta)$. A funÃ§Ã£o densidade de probabilidade da distribuiÃ§Ã£o gama inversa Ã© dada por:
    $$p(\sigma^2) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} (\sigma^2)^{-(\alpha+1)} \exp\left(-\frac{\beta}{\sigma^2}\right)$$
   Onde $\Gamma(\alpha)$ Ã© a funÃ§Ã£o gama.

V. Ignorando o fator constante $\frac{\beta^{\alpha}}{\Gamma(\alpha)}$, o prior Ã© proporcional a:
    $$p(\sigma^2) \propto (\sigma^2)^{-(\alpha+1)} \exp\left(-\frac{\beta}{\sigma^2}\right)$$

O posterior Ã© entÃ£o proporcional a:

$$p(\sigma^2 | \varepsilon_1, \ldots, \varepsilon_n) \propto (\sigma^2)^{-(n/2 + \alpha + 1)} \exp\left(-\frac{\sum_{t=1}^{n} \varepsilon_t^2 + 2\beta}{2\sigma^2}\right)$$

VI. A distribuiÃ§Ã£o posterior Ã© proporcional ao produto da verossimilhanÃ§a e do prior:
    $$p(\sigma^2 | \varepsilon_1, \ldots, \varepsilon_n) \propto L(\sigma^2 | \varepsilon_1, \ldots, \varepsilon_n) \cdot p(\sigma^2)$$

VII. Substituindo as expressÃµes para a verossimilhanÃ§a e o prior:
     $$p(\sigma^2 | \varepsilon_1, \ldots, \varepsilon_n) \propto (\sigma^2)^{-n/2} \exp\left(-\frac{\sum_{t=1}^{n} \varepsilon_t^2}{2\sigma^2}\right) \cdot (\sigma^2)^{-(\alpha+1)} \exp\left(-\frac{\beta}{\sigma^2}\right)$$

VIII. Combinando os termos exponenciais e as potÃªncias de $\sigma^2$:
      $$p(\sigma^2 | \varepsilon_1, \ldots, \varepsilon_n) \propto (\sigma^2)^{-(n/2 + \alpha + 1)} \exp\left(-\frac{\sum_{t=1}^{n} \varepsilon_t^2 + 2\beta}{2\sigma^2}\right)$$

Esta Ã© a forma funcional de uma distribuiÃ§Ã£o gama inversa com parÃ¢metros $\alpha' = \alpha + n/2$ e $\beta' = \beta + \frac{1}{2}\sum_{t=1}^{n} \varepsilon_t^2$. Portanto, a gama inversa Ã© um prior conjugado para a variÃ¢ncia do Gaussian white noise. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos usar o teorema acima. Suponha que temos 100 observaÃ§Ãµes de um Gaussian white noise process, ou seja, $n = 100$. Assumimos um prior gama inversa para $\sigma^2$ com $\alpha = 2$ e $\beta = 1$. ApÃ³s observar os dados, calculamos que $\sum_{t=1}^{100} \varepsilon_t^2 = 150$. EntÃ£o, os parÃ¢metros da distribuiÃ§Ã£o posterior de $\sigma^2$ serÃ£o:
>
> $\alpha' = \alpha + n/2 = 2 + 100/2 = 52$
> $\beta' = \beta + \frac{1}{2}\sum_{t=1}^{n} \varepsilon_t^2 = 1 + \frac{1}{2}(150) = 76$
>
> Portanto, a distribuiÃ§Ã£o posterior de $\sigma^2$ Ã© $\text{InvGamma}(52, 76)$. Podemos usar essa distribuiÃ§Ã£o posterior para fazer inferÃªncias sobre $\sigma^2$, como calcular intervalos de credibilidade.

### TransformaÃ§Ãµes e Linearidade

O Gaussian white noise process preserva a normalidade sob transformaÃ§Ãµes lineares. Mais precisamente, qualquer combinaÃ§Ã£o linear de variÃ¡veis aleatÃ³rias gaussianas independentes tambÃ©m segue uma distribuiÃ§Ã£o normal. Esse resultado Ã© crucial para a anÃ¡lise de modelos lineares de sÃ©ries temporais.

**Teorema 1.3:** *Se $\{\varepsilon_t\}$ Ã© um Gaussian white noise process com mÃ©dia zero e variÃ¢ncia $\sigma^2$, e $\{a_i\}$ sÃ£o coeficientes constantes, entÃ£o a combinaÃ§Ã£o linear $X = \sum_{i=1}^{n} a_i \varepsilon_i$ tambÃ©m segue uma distribuiÃ§Ã£o normal com mÃ©dia zero e variÃ¢ncia $\sigma^2 \sum_{i=1}^{n} a_i^2$.*

*Prova:* A prova deste teorema decorre da propriedade de que combinaÃ§Ãµes lineares de variÃ¡veis aleatÃ³rias normais independentes tambÃ©m seguem uma distribuiÃ§Ã£o normal. JÃ¡ vimos que a mÃ©dia Ã© zero e a variÃ¢ncia Ã© $\sigma^2 \sum_{i=1}^{n} a_i^2$.

Vamos provar o Teorema 1.3 passo a passo:

I. Seja $\{\varepsilon_t\}$ um Gaussian white noise process, o que significa que cada $\varepsilon_t$ segue uma distribuiÃ§Ã£o normal com mÃ©dia zero e variÃ¢ncia $\sigma^2$.

II. Seja $X = \sum_{i=1}^{n} a_i \varepsilon_i$, onde $a_i$ sÃ£o coeficientes constantes.

III. Queremos mostrar que X tambÃ©m segue uma distribuiÃ§Ã£o normal.

IV. A propriedade fundamental que usaremos Ã© que uma combinaÃ§Ã£o linear de variÃ¡veis aleatÃ³rias normais independentes tambÃ©m segue uma distribuiÃ§Ã£o normal. Formalmente, se $X_1, X_2, \ldots, X_n$ sÃ£o variÃ¡veis aleatÃ³rias normais independentes e $c_1, c_2, \ldots, c_n$ sÃ£o constantes, entÃ£o a variÃ¡vel aleatÃ³ria $Z = \sum_{i=1}^{n} c_i X_i$ tambÃ©m segue uma distribuiÃ§Ã£o normal.

V. Como cada $\varepsilon_i$ segue uma distribuiÃ§Ã£o normal e sÃ£o independentes (devido Ã  definiÃ§Ã£o de Gaussian white noise process), e $a_i$ sÃ£o constantes, entÃ£o a combinaÃ§Ã£o linear $X = \sum_{i=1}^{n} a_i \varepsilon_i$ tambÃ©m segue uma distribuiÃ§Ã£o normal.

VI. A mÃ©dia de X Ã©:
    $$E[X] = E\left[\sum_{i=1}^{n} a_i \varepsilon_i\right] = \sum_{i=1}^{n} a_i E[\varepsilon_i] = \sum_{i=1}^{n} a_i \cdot 0 = 0$$

VII. A variÃ¢ncia de X Ã©:
    $$Var(X) = Var\left(\sum_{i=1}^{n} a_i \varepsilon_i\right) = \sum_{i=1}^{n} a_i^2 Var(\varepsilon_i) = \sum_{i=1}^{n} a_i^2 \sigma^2 = \sigma^2 \sum_{i=1}^{n} a_i^2$$

Portanto, $X \sim N\left(0, \sigma^2 \sum_{i=1}^{n} a_i^2\right)$. â– 

**Lema 3:** Seja $\{\varepsilon_t\}$ um Gaussian white noise process. Seja $f(x)$ uma funÃ§Ã£o linear. EntÃ£o, o processo $\{f(\varepsilon_t)\}$ tambÃ©m Ã© um processo Gaussiano.

*Prova:* Este resultado Ã© uma consequÃªncia direta do Teorema 1.3. Como f Ã© linear, $f(\varepsilon_t) = a\varepsilon_t$ para algum escalar a. Portanto, $f(\varepsilon_t)$ Ã© apenas uma escala de um processo gaussiano, que ainda Ã© gaussiano.

Vamos provar o Lema 3 passo a passo:

I.  Dado que $\{\varepsilon_t\}$ Ã© um Gaussian white noise process, $\varepsilon_t$ segue uma distribuiÃ§Ã£o normal com mÃ©dia zero e variÃ¢ncia $\sigma^2$.

II. Seja f(x) uma funÃ§Ã£o linear, o que significa que $f(x) = ax + b$, onde a e b sÃ£o constantes.

III. Queremos mostrar que o processo $\{f(\varepsilon_t)\}$ tambÃ©m Ã© um processo Gaussiano.

IV. Consideremos $f(\varepsilon_t) = a\varepsilon_t + b$. JÃ¡ sabemos que $\varepsilon_t$ segue uma distribuiÃ§Ã£o normal.

V. Usando as propriedades da distribuiÃ§Ã£o normal:
    - Se $\varepsilon_t \sim N(0, \sigma^2)$, entÃ£o $a\varepsilon_t \sim N(0, a^2\sigma^2)$.
    - Se $X \sim N(\mu, \sigma^2)$, entÃ£o $X + b \sim N(\mu + b, \sigma^2)$.

VI. Portanto, $f(\varepsilon_t) = a\varepsilon_t + b$ segue uma distribuiÃ§Ã£o normal com mÃ©dia $E[f(\varepsilon_t)] = a \cdot 0 + b = b$ e variÃ¢ncia $Var(f(\varepsilon_t)) = a^2 \sigma^2$.

VII. Assim, $f(\varepsilon_t) \sim N(b, a^2\sigma^2)$.

Portanto, se $\{\varepsilon_t\}$ Ã© um Gaussian white noise process e f(x) Ã© uma funÃ§Ã£o linear, entÃ£o o processo $\{f(\varepsilon_t)\}$ tambÃ©m Ã© um processo Gaussiano. â– 

Considerando a importÃ¢ncia das transformaÃ§Ãµes lineares, podemos estender o resultado para transformaÃ§Ãµes afins:

**CorolÃ¡rio 3.1:** *Se $\{\varepsilon_t\}$ Ã© um Gaussian white noise process e $f(x) = ax + b$ Ã© uma transformaÃ§Ã£o afim, entÃ£o $\{f(\varepsilon_t)\}$ Ã© um processo Gaussiano com mÃ©dia $b$ e variÃ¢ncia $a^2\sigma^2$.*

*Prova:* Este corolÃ¡rio Ã© uma consequÃªncia direta do Lema 3. Se $\varepsilon_t \sim N(0, \sigma^2)$, entÃ£o $a\varepsilon_t + b \sim N(b, a^2\sigma^2)$. Portanto, o processo transformado ainda Ã© Gaussiano, mas com mÃ©dia e variÃ¢ncia ajustadas pela transformaÃ§Ã£o afim. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:** Seja $\{\varepsilon_t\}$ um Gaussian white noise process com $\sigma^2 = 1$. Considere a transformaÃ§Ã£o linear $X = 2\varepsilon_1 - \varepsilon_2 + 3\varepsilon_3$. Pelo Teorema 1.3, $X$ tambÃ©m segue uma distribuiÃ§Ã£o normal com mÃ©dia zero e variÃ¢ncia $\sigma^2 \sum_{i=1}^{3} a_i^2 = 1 \cdot (2^2 + (-1)^2 + 3^2) = 14$. Portanto, $X \sim N(0, 14)$.

### RelevÃ¢ncia em Modelos ARMA

A premissa de que o ruÃ­do Ã© um Gaussian white noise process Ã© fundamental para a validade de muitas tÃ©cnicas de inferÃªncia em modelos ARMA [^4, 5]. Por exemplo, sob essa premissa, os estimadores de mÃ¡xima verossimilhanÃ§a (MLE) dos parÃ¢metros do modelo sÃ£o assintoticamente normais, o que permite a construÃ§Ã£o de intervalos de confianÃ§a e testes de hipÃ³teses. AlÃ©m disso, a normalidade dos erros simplifica o cÃ¡lculo das funÃ§Ãµes de previsÃ£o e a avaliaÃ§Ã£o da incerteza associada Ã s previsÃµes.

> ðŸ’¡ **Exemplo:** Se ajustamos um modelo ARMA(p, q) aos dados e assumimos que os erros seguem um Gaussian white noise process, podemos usar o teste de Box-Pierce ou o teste de Ljung-Box para verificar se os resÃ­duos do modelo se comportam como um Gaussian white noise process. Se os testes falham, isso indica que o modelo pode nÃ£o ser adequado para os dados.

Um outro teste importante para verificar se os resÃ­duos se comportam como um Gaussian white noise process Ã© o teste de Jarque-Bera, que testa a hipÃ³tese nula de que a amostra vem de uma distribuiÃ§Ã£o normal.

Ainda sobre a relevÃ¢ncia em modelos ARMA, vale ressaltar a importÃ¢ncia da funÃ§Ã£o de autocorrelaÃ§Ã£o (ACF) e da funÃ§Ã£o de autocorrelaÃ§Ã£o parcial (PACF) dos resÃ­duos. Se os resÃ­duos se comportam como um Gaussian white noise, suas ACF e PACF devem estar dentro dos limites de confianÃ§a de zero. Desvios significativos desses limites indicam que o modelo ARMA nÃ£o capturou toda a estrutura de dependÃªncia dos dados.

> ðŸ’¡ **Exemplo NumÃ©rico:** Ajuste um modelo ARMA(1,1) a uma sÃ©rie temporal simulada. Extraia os resÃ­duos e verifique se eles se comportam como um Gaussian white noise process.
>
> ```python
> import numpy as np
> import statsmodels.api as sm
> from statsmodels.tsa.arima.model import ARIMA
> import scipy.stats as stats
> import matplotlib.pyplot as plt
>
> # Simulate ARMA(1,1) data
> np.random.seed(0)
> n = 200
> arparams = np.array([.75])
> maparams = np.array([.65])
> ar = np.r_[1, -arparams]
> ma = np.r_[1, maparams]
> y = sm.tsa.arma_generate_sample(ar, ma, n)
>
> # Fit ARMA(1,1) model
> model = ARIMA(y, order=(1, 0, 1))
> model_fit = model.fit()
> residuals = model_fit.resid
>
> # Shapiro-Wilk test
> shapiro_stat, shapiro_p = stats.shapiro(residuals)
> print(f"Shapiro-Wilk Statistic: {shapiro_stat:.4f}, p-value: {shapiro_p:.4f}")
>
> # Ljung-Box test
> ljungbox_qstat, ljungbox_pvalue = sm.stats.acorr_ljungbox(residuals, lags=[10], return_df=False)
> print(f"Ljung-Box Q-statistic: {ljungbox_qstat[0]:.4f}, p-value: {ljungbox_pvalue[0]:.4f}")
>
> # Plot ACF and PACF
> fig, axes = plt.subplots(2, 1, figsize=(10, 8))
> sm.graphics.tsa.plot_acf(residuals, lags=20, ax=axes[0])
> sm.graphics.tsa.plot_pacf(residuals, lags=20, ax=axes[1])
> plt.show()
>
> # Interpretation
> alpha = 0.05
> if shapiro_p > alpha:
>     print("Shapiro-Wilk: Residuals look Gaussian (fail to reject H0)")
> else:
>     print("Shapiro-Wilk: Residuals do not look Gaussian (reject H0)")
>
> if ljungbox_pvalue[0] > alpha:
>     print("Ljung-Box: No autocorrelation in residuals (fail to reject H0)")
> else:
>     print("Ljung-Box: Autocorrelation exists in residuals (reject H0)")
> ```
>
> Este exemplo demonstra como ajustar um modelo ARMA, extrair os resÃ­duos e verificar se eles satisfazem as propriedades de um Gaussian white noise process. As ACF e PACF plots ajudam a visualizar qualquer autocorrelaÃ§Ã£o restante nos resÃ­duos.

### ConclusÃ£o

O Gaussian white noise process Ã© uma especificaÃ§Ã£o comum e Ãºtil do white noise process, adicionando a premissa de normalidade Ã  mÃ©dia zero, variÃ¢ncia constante e independÃªncia. Essa premissa simplifica anÃ¡lises, simulaÃ§Ãµes e inferÃªncias estatÃ­sticas, particularmente em modelos ARMA e mÃ©todos bayesianos. A facilidade de parametrizaÃ§Ã£o, a preservaÃ§Ã£o da normalidade sob transformaÃ§Ãµes lineares e as propriedades bem compreendidas da distribuiÃ§Ã£o normal o tornam uma ferramenta valiosa na modelagem e anÃ¡lise de sÃ©ries temporais.

### ReferÃªncias
[^4]: The basic building block for all the processes considered in this chapter is a sequence {8} - whose elements have mean zero and variance ÏƒÂ²,
[^5]: A process satisfying [3.2.1] through [3.2.3] is described as a white noise process.
<!-- END -->