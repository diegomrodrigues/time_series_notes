## Fun√ß√£o Geradora de Autocovari√¢ncia para Processos MA(1) e AR(1)

### Introdu√ß√£o

Este cap√≠tulo aprofunda a an√°lise da **fun√ß√£o geradora de autocovari√¢ncia (ACGF)**, com foco espec√≠fico nas formas assumidas pela ACGF para processos de m√©dias m√≥veis de primeira ordem (MA(1)) e processos autorregressivos de primeira ordem (AR(1)). Construindo sobre a defini√ß√£o fundamental e propriedades da ACGF [^61], exploraremos as express√µes espec√≠ficas para esses modelos e suas implica√ß√µes. Como vimos anteriormente [^61], a ACGF √© uma ferramenta valiosa para caracterizar as propriedades de um processo estacion√°rio em covari√¢ncia.

### ACGF para Processos MA(1)

Conforme estabelecido [^62], um processo MA(1) √© definido como:

$$Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$$

onde $\mu$ √© a m√©dia do processo, $\epsilon_t$ √© um ru√≠do branco com vari√¢ncia $\sigma^2$, e $\theta$ √© o coeficiente do termo de m√©dia m√≥vel.

A fun√ß√£o geradora de autocovari√¢ncia para um processo MA(1) √© dada por [^62]:

$$g_Y(z) = \sigma^2 [\theta z^{-1} + (1 + \theta^2) + \theta z]$$

Esta express√£o revela a estrutura das autocovari√¢ncias de um processo MA(1), com $\gamma_0 = \sigma^2 (1 + \theta^2)$, $\gamma_1 = \gamma_{-1} = \sigma^2 \theta$, e $\gamma_j = 0$ para $|j| > 1$.

**Prova da ACGF para MA(1):**

Para verificar a fun√ß√£o geradora de autocovari√¢ncia para o processo MA(1), vamos deriv√°-la a partir da defini√ß√£o do processo e da defini√ß√£o da ACGF.

I. A autocovari√¢ncia de lag k, $\gamma_k$, para um processo MA(1) √© definida como:
$$\gamma_k = E[(Y_t - \mu)(Y_{t-k} - \mu)]$$

II. Substituindo $Y_t$ e $Y_{t-k}$ pelas suas defini√ß√µes em termos do processo MA(1):
Para $k = 0$:
$$\gamma_0 = E[(\epsilon_t + \theta \epsilon_{t-1})(\epsilon_t + \theta \epsilon_{t-1})] = E[\epsilon_t^2 + 2\theta \epsilon_t \epsilon_{t-1} + \theta^2 \epsilon_{t-1}^2] = \sigma^2 + \theta^2 \sigma^2 = \sigma^2(1 + \theta^2)$$
Para $k = 1$:
$$\gamma_1 = E[(\epsilon_t + \theta \epsilon_{t-1})(\epsilon_{t-1} + \theta \epsilon_{t-2})] = E[\epsilon_t \epsilon_{t-1} + \theta \epsilon_{t-1}^2 + \theta \epsilon_t \epsilon_{t-2} + \theta^2 \epsilon_{t-1} \epsilon_{t-2}] = \theta \sigma^2$$
Para $k = -1$:
$$\gamma_{-1} = E[(\epsilon_t + \theta \epsilon_{t-1})(\epsilon_{t+1} + \theta \epsilon_{t})] = E[\epsilon_t \epsilon_{t+1} + \theta \epsilon_{t-1} \epsilon_{t+1} + \theta \epsilon_t^2 + \theta^2 \epsilon_{t-1} \epsilon_{t}] = \theta \sigma^2$$
Para $|k| > 1$:
$$\gamma_k = 0$$

III. A fun√ß√£o geradora de autocovari√¢ncia √© definida como:
$$g_Y(z) = \sum_{k=-\infty}^{\infty} \gamma_k z^k$$

IV. Substituindo os valores das autocovari√¢ncias para o processo MA(1):
$$g_Y(z) = \gamma_{-1}z^{-1} + \gamma_0 + \gamma_1 z = \sigma^2 \theta z^{-1} + \sigma^2(1 + \theta^2) + \sigma^2 \theta z = \sigma^2 [\theta z^{-1} + (1 + \theta^2) + \theta z]$$

V. Portanto, demonstramos que a fun√ß√£o geradora de autocovari√¢ncia para um processo MA(1) √©:
$$g_Y(z) = \sigma^2 [\theta z^{-1} + (1 + \theta^2) + \theta z]$$ ‚ñ†

**An√°lise da ACGF para MA(1):**

A forma da ACGF para um processo MA(1) implica que o processo tem mem√≥ria apenas de um per√≠odo. Isso se reflete no fato de que apenas as autocovari√¢ncias de lag 1 (e -1) s√£o diferentes de zero. A ACGF √© sim√©trica em torno de $z^0$, o que reflete a simetria das autocovari√¢ncias ($\gamma_j = \gamma_{-j}$).

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um processo MA(1) com $\theta = 0.7$ e $\sigma^2 = 2$.
>
> 1.  **C√°lculo das Autocovari√¢ncias:**
>     *   $\gamma_0 = \sigma^2(1 + \theta^2) = 2 * (1 + 0.7^2) = 2 * (1 + 0.49) = 2 * 1.49 = 2.98$
>     *   $\gamma_1 = \gamma_{-1} = \sigma^2 \theta = 2 * 0.7 = 1.4$
>     *   $\gamma_k = 0$ para $|k| > 1$
>
> 2.  **Fun√ß√£o Geradora de Autocovari√¢ncia:**
>     $$g_Y(z) = 2[0.7z^{-1} + (1 + 0.7^2) + 0.7z] = 1.4z^{-1} + 2.98 + 1.4z$$
>
> 3.  **Interpreta√ß√£o:**
>     *   A vari√¢ncia do processo ($\gamma_0$) √© 2.98.
>     *   A autocovari√¢ncia de lag 1 √© 1.4.
>     *   N√£o h√° correla√ß√£o para lags maiores que 1.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> theta = 0.7
> sigma2 = 2
>
> # Autocovari√¢ncias
> gamma0 = sigma2 * (1 + theta**2)
> gamma1 = sigma2 * theta
>
> # Lags
> lags = np.arange(-5, 6)
> autocovariances = np.zeros_like(lags, dtype=float)
> autocovariances[lags == 0] = gamma0
> autocovariances[lags == 1] = gamma1
> autocovariances[lags == -1] = gamma1
>
> # Plot
> plt.figure(figsize=(10, 6))
> plt.stem(lags, autocovariances, basefmt="b-", linefmt="b-", markerfmt="bo")
> plt.title("Autocovari√¢ncias para um Processo MA(1)")
> plt.xlabel("Lag (k)")
> plt.ylabel("Autocovari√¢ncia (Œ≥_k)")
> plt.grid(True)
> plt.xticks(lags)
> plt.show()
> ```

**Teorema 1**
A ACGF de um processo MA(q)
$$Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \ldots + \theta_q \epsilon_{t-q}$$
onde $\mu$ √© a m√©dia do processo, $\epsilon_t$ √© um ru√≠do branco com vari√¢ncia $\sigma^2$, e $\theta_i$ s√£o os coeficientes do termo de m√©dia m√≥vel, √© dada por:
$$g_Y(z) = \sigma^2 \left( \sum_{k=-q}^{q} \gamma_k z^k \right) = \sigma^2 \left( \theta_q z^{-q} + \theta_{q-1} z^{-(q-1)} + \ldots + (1 + \sum_{i=1}^q \theta_i^2) + \ldots + \theta_{q-1} z^{(q-1)} + \theta_q z^{q} \right)$$
onde $\theta_0 = 1$ e $\gamma_k = \sum_{i=0}^{q-k} \theta_i \theta_{i+k}$.

*Proof:*
The autocovariance $\gamma_k$ for a MA(q) process is given by:
$\gamma_k = E[(Y_t - \mu)(Y_{t-k} - \mu)] = E[(\sum_{i=0}^{q} \theta_i \epsilon_{t-i}) (\sum_{j=0}^{q} \theta_j \epsilon_{t-k-j})] = \sigma^2 \sum_{i=0}^{q-k} \theta_i \theta_{i+k}$
where $\theta_0 = 1$ and $\theta_i = 0$ for $i > q$.

The autocovariance generating function (ACGF) is defined as:
$g_Y(z) = \sum_{k=-\infty}^{\infty} \gamma_k z^k$.
For an MA(q) process, $\gamma_k = 0$ for $|k| > q$. Thus, the ACGF becomes:
$g_Y(z) = \sum_{k=-q}^{q} \gamma_k z^k = \sigma^2 \sum_{k=-q}^{q} \left( \sum_{i=0}^{q-k} \theta_i \theta_{i+k} \right) z^k = \sigma^2 \left( \sum_{k=-q}^{q} \gamma_k z^k \right)$.

### ACGF para Processos AR(1)

Conforme mencionado anteriormente [^63], um processo AR(1) estacion√°rio √© definido como:

$$Y_t - \mu = (1 - \phi L)^{-1} \epsilon_t$$

onde $\mu$ √© a m√©dia do processo, $\epsilon_t$ √© um ru√≠do branco com vari√¢ncia $\sigma^2$, e $\phi$ √© o coeficiente autoregressivo, com $|\phi| < 1$ para garantir a estacionariedade.

A fun√ß√£o geradora de autocovari√¢ncia para um processo AR(1) √© dada por [^63]:

$$g_Y(z) = \frac{\sigma^2}{(1 - \phi z)(1 - \phi z^{-1})}$$

Esta express√£o pode ser reescrita usando a expans√£o da s√©rie geom√©trica, como demonstrado no Lema 1 [^Lemma 1]. Isso nos permite expressar as autocovari√¢ncias em termos de $\phi$:

$$\gamma_j = \frac{\sigma^2}{1 - \phi^2} \phi^{|j|}$$

**Prova da ACGF para AR(1):**

Para verificar a fun√ß√£o geradora de autocovari√¢ncia para o processo AR(1), vamos deriv√°-la a partir da defini√ß√£o do processo e da defini√ß√£o da ACGF.

I. O processo AR(1) √© definido como: $Y_t = \mu + \phi(Y_{t-1} - \mu) + \epsilon_t$
II. A autocovari√¢ncia de lag k, $\gamma_k$, √© definida como: $\gamma_k = E[(Y_t - \mu)(Y_{t-k} - \mu)]$
III. Da equa√ß√£o do processo AR(1), temos $(Y_t - \mu) = \phi(Y_{t-1} - \mu) + \epsilon_t$.  Multiplicando ambos os lados por $(Y_{t-k} - \mu)$ e tomando a expectativa, obtemos:
$E[(Y_t - \mu)(Y_{t-k} - \mu)] = \phi E[(Y_{t-1} - \mu)(Y_{t-k} - \mu)] + E[\epsilon_t (Y_{t-k} - \mu)]$
IV.  Portanto, $\gamma_k = \phi \gamma_{k-1} + E[\epsilon_t (Y_{t-k} - \mu)]$.
V. Para $k > 0$, $E[\epsilon_t (Y_{t-k} - \mu)] = 0$, ent√£o $\gamma_k = \phi \gamma_{k-1}$. Para $k = 0$, $\gamma_0 = \phi \gamma_{-1} + \sigma^2 = \phi \gamma_1 + \sigma^2$. Como $\gamma_1 = \phi \gamma_0$, substituindo na equa√ß√£o para $\gamma_0$, temos $\gamma_0 = \phi^2 \gamma_0 + \sigma^2$, o que implica $\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$.
VI. Desde que $\gamma_k = \phi \gamma_{k-1}$, temos $\gamma_k = \phi^k \gamma_0 = \phi^k \frac{\sigma^2}{1 - \phi^2}$. Similarmente, $\gamma_{-k} = \phi^{|k|} \frac{\sigma^2}{1 - \phi^2}$.
VII. A fun√ß√£o geradora de autocovari√¢ncia √©:
$g_Y(z) = \sum_{k=-\infty}^{\infty} \gamma_k z^k = \sum_{k=-\infty}^{\infty} \frac{\sigma^2}{1 - \phi^2} \phi^{|k|} z^k = \frac{\sigma^2}{1 - \phi^2} \left( \sum_{k=-\infty}^{-1} \phi^{-k} z^k + 1 + \sum_{k=1}^{\infty} \phi^k z^k \right)$
VIII. Avaliando as somas geom√©tricas:
$\sum_{k=1}^{\infty} (\phi z)^k = \frac{\phi z}{1 - \phi z}$ e $\sum_{k=-\infty}^{-1} (\phi z)^k = \sum_{k=1}^{\infty} (\phi^{-1} z^{-1})^k = \frac{\phi^{-1} z^{-1}}{1 - \phi^{-1} z^{-1}} = \frac{\phi^{-1} z^{-1}}{1 - \phi^{-1} z^{-1}} \cdot \frac{\phi z}{\phi z} = \frac{\phi z}{\phi z - 1}$
IX. Substituindo de volta na equa√ß√£o para $g_Y(z)$:
$g_Y(z) = \frac{\sigma^2}{1 - \phi^2} \left( \frac{\phi z}{\phi z - 1} + 1 + \frac{\phi z}{1 - \phi z} \right) = \frac{\sigma^2}{1 - \phi^2} \left( \frac{\phi z(1 - \phi z) + (1 - \phi z)(\phi z - 1) + \phi z (\phi z - 1)}{(\phi z - 1)(1 - \phi z)} \right)$
$g_Y(z) = \frac{\sigma^2}{1 - \phi^2} \left( \frac{\phi z - \phi^2 z^2 + \phi z - 1 - \phi^2 z^2 + \phi z + \phi^2 z^2 - \phi z}{(\phi z - 1)(1 - \phi z)} \right) = \frac{\sigma^2}{1 - \phi^2} \left( \frac{-1 + \phi z}{ (\phi z - 1)(1 - \phi z)} \right)$
$g_Y(z) = \frac{\sigma^2}{1 - \phi^2} \left( \frac{1}{(1 - \phi z)(1 - \phi z^{-1})} \right) = \frac{\sigma^2}{(1-\phi z)(1 - \phi z^{-1})}$
X. Portanto, demonstramos que a fun√ß√£o geradora de autocovari√¢ncia para um processo AR(1) √©:
$$g_Y(z) = \frac{\sigma^2}{(1 - \phi z)(1 - \phi z^{-1})}$$ ‚ñ†

**An√°lise da ACGF para AR(1):**

A forma da ACGF para um processo AR(1) indica que o processo tem mem√≥ria de longo alcance, embora a influ√™ncia dos lags passados diminua exponencialmente com o aumento do lag. As autocovari√¢ncias decaem geometricamente com a taxa $\phi$. A ACGF tamb√©m √© sim√©trica, refletindo a simetria das autocovari√¢ncias.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um processo AR(1) com $\phi = 0.6$ e $\sigma^2 = 1$.
>
> 1.  **C√°lculo das Autocovari√¢ncias:**
>
>     *   $\gamma_0 = \frac{\sigma^2}{1 - \phi^2} = \frac{1}{1 - 0.6^2} = \frac{1}{1 - 0.36} = \frac{1}{0.64} = 1.5625$
>     *   $\gamma_1 = \frac{\sigma^2}{1 - \phi^2} \phi = 1.5625 * 0.6 = 0.9375$
>     *   $\gamma_2 = \frac{\sigma^2}{1 - \phi^2} \phi^2 = 1.5625 * 0.6^2 = 1.5625 * 0.36 = 0.5625$
>     *   $\gamma_3 = \frac{\sigma^2}{1 - \phi^2} \phi^3 = 1.5625 * 0.6^3 = 1.5625 * 0.216 = 0.3375$
>
> 2.  **ACGF:**
>
>     $$g_Y(z) = \frac{1}{(1 - 0.6z)(1 - 0.6z^{-1})}$$
>
> 3.  **Interpreta√ß√£o:**
>     *   A vari√¢ncia ($\gamma_0$) √© 1.5625.
>     *   A autocovari√¢ncia diminui √† medida que o lag aumenta, indicando a influ√™ncia decrescente de valores passados.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> phi = 0.6
> sigma2 = 1
>
> # Autocovari√¢ncias
> gamma0 = sigma2 / (1 - phi**2)
>
> # Lags
> lags = np.arange(0, 6)
> autocovariances = gamma0 * phi**lags
>
> # Plot
> plt.figure(figsize=(10, 6))
> plt.stem(lags, autocovariances, basefmt="b-", linefmt="b-", markerfmt="bo")
> plt.title("Autocovari√¢ncias para um Processo AR(1)")
> plt.xlabel("Lag (k)")
> plt.ylabel("Autocovari√¢ncia (Œ≥_k)")
> plt.grid(True)
> plt.xticks(lags)
> plt.show()
> ```

**Teorema 2**
A ACGF para um processo AR(p)
$$Y_t = \mu + \phi_1 (Y_{t-1} - \mu) + \phi_2 (Y_{t-2} - \mu) + \ldots + \phi_p (Y_{t-p} - \mu) + \epsilon_t$$
pode ser expressa como:
$$g_Y(z) = \frac{\sigma^2}{A(z)A(z^{-1})}$$
onde $A(z) = 1 - \phi_1 z - \phi_2 z^2 - \ldots - \phi_p z^p$.

*Proof:*
The AR(p) process can be written as $A(L)Y_t = \epsilon_t$, where $L$ is the lag operator and $A(L) = 1 - \phi_1 L - \phi_2 L^2 - \ldots - \phi_p L^p$.  Therefore, the ACGF is given by:
$$g_Y(z) = \sigma^2 [A(z)A(z^{-1})]^{-1}$$.

**Corol√°rio 2.1**
Se as ra√≠zes do polin√¥mio $A(z)$ est√£o dentro do c√≠rculo unit√°rio, o processo AR(p) √© n√£o estacion√°rio. Para garantir estacionariedade, todas as ra√≠zes devem estar fora do c√≠rculo unit√°rio.

**Proposi√ß√£o 3**
A ACGF pode ser usada para calcular o espectro de pot√™ncia de um processo estacion√°rio. O espectro de pot√™ncia $S_Y(\omega)$ √© dado pela transformada de Fourier da ACGF:
$$S_Y(\omega) = \frac{1}{2\pi} \sum_{k=-\infty}^{\infty} \gamma_k e^{-j\omega k} = \frac{1}{2\pi} g_Y(e^{-j\omega})$$
onde $j$ √© a unidade imagin√°ria e $\omega$ √© a frequ√™ncia.

**Observa√ß√£o:**
A simetria da ACGF ($\gamma_k = \gamma_{-k}$) implica que o espectro de pot√™ncia √© uma fun√ß√£o real e par de $\omega$.

**Comparativo entre MA(1) e AR(1):**

| Caracter√≠stica           | MA(1)                      | AR(1)                                   |
|--------------------------|---------------------------|-----------------------------------------|
| Defini√ß√£o                | $Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$ | $Y_t - \mu = \phi (Y_{t-1} - \mu) + \epsilon_t$ |
| ACGF                     | $\sigma^2 [\theta z^{-1} + (1 + \theta^2) + \theta z]$ | $\frac{\sigma^2}{(1 - \phi z)(1 - \phi z^{-1})}$      |
| Autocovari√¢ncias         | $\gamma_0 = \sigma^2(1 + \theta^2)$, $\gamma_1 = \sigma^2 \theta$ | $\gamma_j = \frac{\sigma^2}{1 - \phi^2} \phi^{|j|}$         |
| Mem√≥ria                  | Curta (lag 1)             | Longa (decai exponencialmente)           |

### Conclus√£o

Este cap√≠tulo explorou as formas espec√≠ficas da fun√ß√£o geradora de autocovari√¢ncia para processos MA(1) e AR(1), demonstrando como a ACGF captura as caracter√≠sticas essenciais desses modelos. A an√°lise das ACGFs revelou que os modelos MA(1) possuem mem√≥ria de curto alcance, enquanto os modelos AR(1) exibem mem√≥ria de longo alcance com autocovari√¢ncias que decaem exponencialmente. A compreens√£o das ACGFs √© fundamental para a an√°lise espectral e para o projeto de filtros para s√©ries temporais, conforme mencionado [^63].

### Refer√™ncias

[^61]: Se√ß√£o 3.6, p√°gina 61
[^62]: Se√ß√£o 3.6, p√°gina 62
[^63]: Se√ß√£o 3.6, p√°gina 63

$\blacksquare$
<!-- END -->