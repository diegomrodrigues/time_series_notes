## Autocovariance Function of the MA(1) Process

### IntroduÃ§Ã£o
Expanding upon our understanding of the expectation and variance of the MA(1) process [^1, 3], this section focuses on the autocovariance function (ACF) and how it is helpful in identification and parameter estimation. We have established that the MA(1) process is given by $Y_t = \mu + \varepsilon_t + \theta\varepsilon_{t-1}$ [^1], where $\mu$ is the mean, $\varepsilon_t$ is a white noise error term with variance $\sigma^2$ [^4, 5], and $\theta$ is the moving average coefficient [^1]. Understanding the autocovariance function provides crucial insights into the correlation structure of the time series [^22]. Furthermore, by observing that autocovariances are zero after lag $q$ for an MA($q$) process, we can derive simplified estimation algorithms and storage optimizations. This is a key difference between MA and AR processes, and can be used to identify the correct model.

### Conceitos Fundamentais

Recall that the *$j$th autocovariance, denoted as $\gamma_j$, measures the covariance between $Y_t$ and $Y_{t-j}$ [^6]. For the MA(1) process, the autocovariance function has a unique structure. We've already established that the variance, which is the autocovariance at lag 0, is given by $\gamma_0 = (1 + \theta^2)\sigma^2$ and the autocovariance at lag 1 is given by $\gamma_1 = \theta\sigma^2$. The central feature of the MA(1) process is that autocovariances beyond lag 1 are zero [^1]. We summarize this in the following theorem.

**Teorema 1 (Autocovariance Function of MA(1)):**
For the MA(1) process defined as $Y_t = \mu + \varepsilon_t + \theta\varepsilon_{t-1}$, the autocovariance function is:
$$
\gamma_j = \begin{cases}
(1 + \theta^2)\sigma^2, & j = 0 \\
\theta\sigma^2, & j = \pm 1 \\
0, & |j| > 1
\end{cases}
$$

*Proof:*
We derived $\gamma_0 = (1 + \theta^2)\sigma^2$ and $\gamma_1 = \theta\sigma^2$ in the prior section. To complete the proof, we need to show that $E[(Y_t - \mu)(Y_{t-j} - \mu)] = 0$ for $j > 1$.

I. ComeÃ§amos com a definiÃ§Ã£o da autocovariÃ¢ncia no atraso j: $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$.
II. SubstituÃ­mos $Y_t$ e $Y_{t-j}$ pelas suas definiÃ§Ãµes no processo MA(1): $\gamma_j = E[(\varepsilon_t + \theta\varepsilon_{t-1})(\varepsilon_{t-j} + \theta\varepsilon_{t-j-1})]$.
III. Expandimos o produto: $\gamma_j = E[\varepsilon_t\varepsilon_{t-j} + \theta\varepsilon_t\varepsilon_{t-j-1} + \theta\varepsilon_{t-1}\varepsilon_{t-j} + \theta^2\varepsilon_{t-1}\varepsilon_{t-j-1}]$.
IV. Aplicamos a linearidade do operador de esperanÃ§a: $\gamma_j = E[\varepsilon_t\varepsilon_{t-j}] + \theta E[\varepsilon_t\varepsilon_{t-j-1}] + \theta E[\varepsilon_{t-1}\varepsilon_{t-j}] + \theta^2 E[\varepsilon_{t-1}\varepsilon_{t-j-1}]$.
V. Como $\varepsilon_t$ Ã© ruÃ­do branco e $j > 1$, todos os termos de esperanÃ§a sÃ£o zero, porque os erros em diferentes perÃ­odos de tempo sÃ£o nÃ£o correlacionados. That is $E[\varepsilon_t\varepsilon_{t-k}] = 0$ for $t \neq k$.
VI. Portanto, $\gamma_j = 0 + 0 + 0 + 0 = 0$.
VII. Assim, a autocovariÃ¢ncia no atraso $j > 1$ Ã© 0. $\blacksquare$

This result highlights a critical property: *the MA(1) process only has short-term memory*. Its correlation with its more distant past is zero. This distinguishes the MA(1) from other time series models, such as autoregressive (AR) models, which can have correlations extending over multiple lags.

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Suppose we have an MA(1) process with $\theta = 0.7$ and $\sigma^2 = 2$. Then, the autocovariance function would be:
>
> -   $\gamma_0 = (1 + 0.7^2) \cdot 2 = (1 + 0.49) \cdot 2 = 1.49 \cdot 2 = 2.98$
> -   $\gamma_1 = 0.7 \cdot 2 = 1.4$
> -   $\gamma_j = 0$ for $|j| > 1$
>
> If we were to calculate the sample autocovariance from simulated data of this process, we would expect the values to be close to these theoretical values, with the sample autocovariances quickly decaying to zero beyond lag 1.
>
> We can simulate this in Python using NumPy:
>
> ```python
> import numpy as np
>
> # Parameters
> theta = 0.7
> sigma2 = 2
> mu = 0
> n = 1000  # Number of observations
>
> # Generate white noise
> errors = np.random.normal(0, np.sqrt(sigma2), n)
>
> # Generate MA(1) process
> y = [mu + errors[0]]
> for t in range(1, n):
>     y.append(mu + errors[t] + theta * errors[t-1])
> y = np.array(y)
>
> # Calculate sample autocovariances
> def autocovariance(x, lag):
>     n = len(x)
>     x_mean = np.mean(x)
>     if lag >= n:
>       return 0
>     return np.sum((x[:n-lag] - x_mean) * (x[lag:] - x_mean)) / n
>
> lags = [0, 1, 2, 3, 4, 5]
> sample_autocovariances = [autocovariance(y, lag) for lag in lags]
>
> print("Theoretical Autocovariances:")
> print(f"Gamma_0: {2.98}")
> print(f"Gamma_1: {1.4}")
> print(f"Gamma_j (j>1): 0")
>
> print("\nSample Autocovariances:")
> for lag, acov in zip(lags, sample_autocovariances):
>     print(f"Lag {lag}: {acov}")
> ```
>
> The output will show that the sample autocovariances at lags 0 and 1 are close to the theoretical values, and the autocovariances at higher lags are close to zero, confirming the MA(1) structure.

**Teorema 1.1 (Autocorrelation Function of MA(1)):**
The autocorrelation function (ACF), $\rho_j$, for the MA(1) process is given by:
$$
\rho_j = \begin{cases}
1, & j = 0 \\
\frac{\theta}{1 + \theta^2}, & j = \pm 1 \\
0, & |j| > 1
\end{cases}
$$

*Proof:*
The autocorrelation function is defined as $\rho_j = \frac{\gamma_j}{\gamma_0}$.  Using the results from Theorem 1, we have:
I. $\rho_0 = \frac{\gamma_0}{\gamma_0} = 1$.
II. $\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{\theta\sigma^2}{(1 + \theta^2)\sigma^2} = \frac{\theta}{1 + \theta^2}$.
III. For $|j| > 1$, $\rho_j = \frac{\gamma_j}{\gamma_0} = \frac{0}{(1 + \theta^2)\sigma^2} = 0$. $\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Using the same values as before, $\theta = 0.7$, the autocorrelation function is:
>
> -   $\rho_0 = 1$
> -   $\rho_1 = \frac{0.7}{1 + 0.7^2} = \frac{0.7}{1 + 0.49} = \frac{0.7}{1.49} \approx 0.4698$
> -   $\rho_j = 0$ for $|j| > 1$
>
> This tells us that the correlation between $Y_t$ and $Y_{t-1}$ is about 0.47, while there is no correlation beyond lag 1.

**Teorema 1.2 (Bounds on the Autocorrelation at Lag 1):**

For an MA(1) process, the autocorrelation at lag 1, $\rho_1$, is bounded between -0.5 and 0.5. That is, $-0.5 \leq \rho_1 \leq 0.5$.

*Proof:*
We know that $\rho_1 = \frac{\theta}{1 + \theta^2}$. To find the bounds, we can analyze the function $f(\theta) = \frac{\theta}{1 + \theta^2}$.
To find the maximum and minimum values, we take the derivative with respect to $\theta$ and set it to zero:
I.  $ f'(\theta) = \frac{(1 + \theta^2) - \theta(2\theta)}{(1 + \theta^2)^2} = \frac{1 - \theta^2}{(1 + \theta^2)^2} $
II. Setting $f'(\theta) = 0$, we have $1 - \theta^2 = 0$, which gives $\theta = \pm 1$.
III. When $\theta = 1$, $\rho_1 = \frac{1}{1 + 1^2} = \frac{1}{2} = 0.5$.
IV. When $\theta = -1$, $\rho_1 = \frac{-1}{1 + (-1)^2} = \frac{-1}{2} = -0.5$.
Therefore, the autocorrelation at lag 1 is bounded between -0.5 and 0.5. $\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Let's consider a few values of $\theta$ and see how $\rho_1$ changes:
>
> *   If $\theta = 0.5$, $\rho_1 = \frac{0.5}{1 + 0.5^2} = \frac{0.5}{1.25} = 0.4$
> *   If $\theta = -0.8$, $\rho_1 = \frac{-0.8}{1 + (-0.8)^2} = \frac{-0.8}{1.64} \approx -0.4878$
> *   If $\theta = 2$, $\rho_1 = \frac{2}{1 + 2^2} = \frac{2}{5} = 0.4$
> *   If $\theta = -3$, $\rho_1 = \frac{-3}{1 + (-3)^2} = \frac{-3}{10} = -0.3$
>
> Notice that regardless of how large $|\theta|$ gets, $\rho_1$ never exceeds 0.5 or goes below -0.5. This illustrates the bounds on $\rho_1$.

**Teorema 2 (Implications for MA(q) Processes):**

For an MA($q$) process defined as $Y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + ... + \theta_q \varepsilon_{t-q}$, the autocovariance function is zero for lags greater than *q*.
$$ \gamma_j = 0 \quad \text{for} \quad |j| > q $$

*Proof:*

Consider the general MA($q$) process given by $Y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \ldots + \theta_q \varepsilon_{t-q}$. Let's analyze the autocovariance at lag $j$, where $j > q$:
$$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E\left[\left(\varepsilon_t + \sum_{i=1}^{q} \theta_i \varepsilon_{t-i}\right) \left(\varepsilon_{t-j} + \sum_{k=1}^{q} \theta_k \varepsilon_{t-j-k}\right)\right]$$
Since $j > q$, there are no common terms between the two summations. As all the error terms are uncorrelated, the expected value of their products is zero:
I. $\gamma_j = E[\varepsilon_t \varepsilon_{t-j}] + \sum_{i=1}^{q} \sum_{k=1}^{q} \theta_i \theta_k E[\varepsilon_{t-i} \varepsilon_{t-j-k}]$
II. Since $E[\varepsilon_t \varepsilon_{t-k}] = 0$ for $t \neq k$, $\gamma_j = 0 + \sum_{i=1}^{q} \sum_{k=1}^{q} \theta_i \theta_k \cdot 0 = 0$
Therefore, for any $j > q$, the autocovariance $\gamma_j = 0$. This result is a key characteristic of MA processes and is crucial for determining the order *q* of an MA process using sample autocorrelation functions. $\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Consider an MA(2) process: $Y_t = \mu + \varepsilon_t + 0.6\varepsilon_{t-1} - 0.4\varepsilon_{t-2}$.  The autocovariances $\gamma_0, \gamma_1, \gamma_2$ will be non-zero, but $\gamma_j = 0$ for all $j > 2$.  This means if we have data from an MA(2) process, we only need to consider the first two lagged error terms to model its behavior.

#### Identifying the Order *q* of an MA(q) Process
The property of the autocovariance function being zero for lags greater than *q* allows us to identify the order of an MA process from a sample autocorrelogram.

**Algoritmo (Order Identification):**

1.  **Calculate the sample ACF:** Estimate the sample autocorrelation function (ACF) of the observed time series data.
2.  **Identify significant spikes:** Look for significant spikes in the sample ACF. A significant spike at lag *k* suggests a non-zero autocovariance at that lag.
3.  **Determine the cutoff point:** Find the lag *q* after which the sample ACF values are approximately zero.
4.  **Conclude the order:** The order of the MA process is then determined to be *q*.

The cutoff point will likely not be perfectly clean in real-world data, so judgment and statistical tests for significance are required.

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Assume you are given the following sample ACF for a time series:
>
> *   Lag 0: 1.0
> *   Lag 1: 0.6
> *   Lag 2: 0.1
> *   Lag 3: -0.05
> *   Lag 4: 0.02
> *   Lag 5: -0.01
>
> In this case, there's a significant spike at lag 1 (0.6) but the ACF values are close to zero (statistically insignificant) after lag 1. This suggests that the data may be well modeled by an MA(1) process.
>
> Now, consider this alternative sample ACF:
>
> *   Lag 0: 1.0
> *   Lag 1: 0.8
> *   Lag 2: 0.4
> *   Lag 3: 0.05
> *   Lag 4: -0.02
> *   Lag 5: 0.01
>
> Here, we have significant spikes at lags 1 and 2. After lag 2, the ACF values are close to zero. This suggests an MA(2) process.
>
> Let's visualize these ACFs using Mermaid:
>
> ```mermaid
> graph LR
>     A[Lag 0: 1.0] --> B(Lag 1: 0.6)
>     B --> C(Lag 2: 0.1)
>     C --> D(Lag 3: -0.05)
>     D --> E(Lag 4: 0.02)
>     E --> F(Lag 5: -0.01)
>     style A fill:#f9f,stroke:#333,stroke-width:2px
>     style B fill:#ccf,stroke:#333,stroke-width:2px
>
> graph LR
>     G[Lag 0: 1.0] --> H(Lag 1: 0.8)
>     H --> I(Lag 2: 0.4)
>     I --> J(Lag 3: 0.05)
>     J --> K(Lag 4: -0.02)
>     K --> L(Lag 5: 0.01)
>     style G fill:#f9f,stroke:#333,stroke-width:2px
>     style H fill:#ccf,stroke:#333,stroke-width:2px
>     style I fill:#ccf,stroke:#333,stroke-width:2px
> ```
> The first graph would point towards MA(1) and the second to MA(2).

**Lema 1 (Invertibility Condition for MA(1)):**
An MA(1) process is invertible if $|\theta| < 1$.

*Proof:*
An MA(1) process is invertible if it can be represented as an infinite order autoregressive process. The MA(1) process $Y_t = \mu + \varepsilon_t + \theta\varepsilon_{t-1}$ can be rewritten as $\varepsilon_t = Y_t - \mu - \theta\varepsilon_{t-1}$. By repeated substitution, we can express $\varepsilon_t$ as an infinite sum of past $Y_t$ values. This representation converges if $|\theta| < 1$, ensuring that the weights on the past observations decay sufficiently quickly. $\blacksquare$

**Lema 1.1 (Implications of Non-Invertibility):**

If the invertibility condition $|\theta| < 1$ is not met (i.e., $|\theta| \geq 1$), the MA(1) process is non-invertible. A non-invertible MA(1) process has implications for forecasting, as the weights on past observations do not decay, potentially leading to unstable or unreliable forecasts. Furthermore, parameter estimation can become more challenging.

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Consider two MA(1) processes:
>
> 1.  $Y_t = \varepsilon_t + 0.5\varepsilon_{t-1}$ (Invertible, $\theta = 0.5$)
> 2.  $Y_t = \varepsilon_t + 2\varepsilon_{t-1}$ (Non-invertible, $\theta = 2$)
>
> In the first case, the impact of past errors on the current value diminishes over time, leading to stable forecasts. In the second case, the impact of past errors grows over time, potentially leading to forecasts that are highly sensitive to initial conditions and past errors.
>
> To see the impact on parameter estimation, consider trying to estimate $\theta$ from a dataset generated by $Y_t = \varepsilon_t + 2\varepsilon_{t-1}$. The estimated value of $\theta$ may be unstable and sensitive to the sample used.

**Teorema 3 (Simplified Estimation):**

The property of zero autocovariances beyond lag *q* simplifies the estimation process of MA models.
Since the autocovariances are zero for $|j|>q$, we only need to estimate the parameters $\theta_1, \theta_2, ..., \theta_q$ and $\sigma^2$. The likelihood function or method of moments estimation is simplified as the number of terms in the covariance matrix is reduced. This reduces computational complexity.

**Teorema 4 (Efficient Storage):**

For an MA($q$) process, only *q* autocovariance values (plus the variance) need to be stored. This efficient memory usage can be valuable when dealing with long time series data or systems with limited storage capacity.

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Suppose you have a time series with 1,000,000 data points and you suspect it's an MA(3) process. Instead of storing the full autocovariance function (which would have 1,000,000 values), you only need to store $\gamma_0, \gamma_1, \gamma_2, \gamma_3$. This significantly reduces the memory requirements.

### ConclusÃ£o

The autocovariance function for an MA(1) process is remarkably simple. It's non-zero only at lags 0 and 1, and zero for higher lags. This is crucial for model identification, as it allows us to quickly assess whether an MA(1) process is appropriate for a given time series. More generally, the autocovariance function for an MA(*q*) process truncates after lag *q*. This truncation allows for simplified parameter estimation algorithms and efficient storage of autocovariance values.  These properties make MA models both theoretically tractable and practically useful in time series analysis [^22].

### ReferÃªncias
[^1]: Imagine a battery of I such computers generating sequences {y{1},...,y{I}}x, and consider selecting the observation associated with date t from each sequence. This would be described as a sample of I realizations of the random variable Y.. This random variable has some density, denoted fr(y,), which is called the un- conditional density of Y.. For example, for the Gaussian white noise process, this density is given by...
[^3]: Sometimes for emphasis the expectation E(Y) is called the unconditional mean of Y,. The unconditional mean is denoted Î¼,: E(Y) = Î¼Î¹
[^4]: The basic building block for all the processes considered in this chapter is a sequence {8} - whose elements have mean zero and variance ÏƒÂ², E(Îµ,) = 0
[^5]: A process satisfying [3.2.1] through [3.2.3] is described as a white noise process. We shall on occasion wish to replace [3.2.3] with the slightly stronger condition that the e's are independent across time: Îµ,, &, independent for t â‰  T.
[^6]: The jth autocorrelation of a covariance-stationary process (denoted p,) is defined as its jth autocovariance divided by the variance: Pj = Î³j/Î³o
[^22]: Consider now a seemingly different MA(1) process, Î«, âˆ’ Î¼ = (1 + Î¸L)áº¼, ...
<!-- END -->