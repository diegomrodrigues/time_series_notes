## The First-Order Moving Average Process: MA(1) - Expectation and Variance

### Introdu√ß√£o
Building upon the definition and properties of the MA(1) process, $Y_t = \mu + \varepsilon_t + \theta\varepsilon_{t-1}$ [^1], this section will delve deeper into the statistical characteristics of the model, specifically focusing on deriving the expectation, $E(Y_t)$, and variance, $E[(Y_t - \mu)^2]$, and emphasizing the insights gained from these properties. As seen in the previous section, the MA(1) process provides a foundational model for capturing short-term dependencies. This section will demonstrate how the parameters of the MA(1) process directly influence these fundamental statistical properties.

### Conceitos Fundamentais
As previously defined, the MA(1) process is expressed as [^1]:
$$Y_t = \mu + \varepsilon_t + \theta\varepsilon_{t-1},$$
where $\mu$ represents a constant mean, $\varepsilon_t$ is a white noise error term with zero mean and variance $\sigma^2$ [^4, 5], and $\theta$ quantifies the impact of the prior period‚Äôs error $\varepsilon_{t-1}$ on the current observation $Y_t$ [^1]. The defining feature of the MA(1) is its reliance on the current and immediately preceding error term, endowing it with a 'memory' of length one [^1].

#### Expectation of the MA(1) Process
To derive the expectation of $Y_t$, we take the expected value of both sides of the defining equation:
$$E(Y_t) = E(\mu + \varepsilon_t + \theta\varepsilon_{t-1}).$$
Utilizing the linearity of the expectation operator, we have:
$$E(Y_t) = E(\mu) + E(\varepsilon_t) + \theta E(\varepsilon_{t-1}).$$
Since $\mu$ is a constant, $E(\mu) = \mu$. Given that $\varepsilon_t$ and $\varepsilon_{t-1}$ are white noise with zero mean [^4, 5], we have $E(\varepsilon_t) = 0$ and $E(\varepsilon_{t-1}) = 0$. Therefore,
$$E(Y_t) = \mu + 0 + \theta \cdot 0 = \mu.$$
This fundamental result reveals that the expected value of the MA(1) process is simply the constant term $\mu$ [^1, 3]. In essence, $\mu$ represents the long-run average level around which the time series fluctuates.

**Teorema 1 (Expectation):** The expectation of an MA(1) process, $Y_t = \mu + \varepsilon_t + \theta\varepsilon_{t-1}$, is equal to $\mu$.

*Proof:* This follows directly from the properties of the expectation operator and the white noise process $\varepsilon_t$, as shown above. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Consider an MA(1) process with $\mu = 10$ and $\theta = 0.6$. This means $Y_t = 10 + \varepsilon_t + 0.6\varepsilon_{t-1}$.  If we simulate this process, the sample mean will converge to 10 as the number of observations increases.  Let's simulate 1000 observations with $\sigma^2 = 1$:
> ```python
> import numpy as np
>
> np.random.seed(42) # for reproducibility
> mu = 10
> theta = 0.6
> sigma_sq = 1
> num_obs = 1000
>
> errors = np.random.normal(0, np.sqrt(sigma_sq), num_obs)
> y = np.zeros(num_obs)
>
> y[0] = mu + errors[0]
> for t in range(1, num_obs):
>     y[t] = mu + errors[t] + theta * errors[t-1]
>
> sample_mean = np.mean(y)
> print(f"Theoretical Mean: {mu}")
> print(f"Sample Mean: {sample_mean}")
> ```
> Output:
> ```
> Theoretical Mean: 10
> Sample Mean: 10.04117458353046
> ```
>
> As you can see, the sample mean (10.04) is close to the theoretical mean (10). The slight difference is due to random fluctuations in the simulated data. If we increased `num_obs`, the sample mean would converge even closer to 10. This illustrates that, even though the observed values of $Y_t$ vary randomly, their average value clusters around $\mu$.

Furthermore, we can extend this result to consider a sample mean of the MA(1) process.

**Teorema 1.1 (Sample Mean):**
Given a sample of $n$ observations from the MA(1) process, $Y_1, Y_2, ..., Y_n$, the expected value of the sample mean, $\bar{Y} = \frac{1}{n}\sum_{t=1}^{n}Y_t$, is also equal to $\mu$.

*Proof:*
$$E(\bar{Y}) = E\left(\frac{1}{n}\sum_{t=1}^{n}Y_t\right) = \frac{1}{n}\sum_{t=1}^{n}E(Y_t) = \frac{1}{n}\sum_{t=1}^{n}\mu = \frac{1}{n}n\mu = \mu.$$
Thus, the expected value of the sample mean remains $\mu$. $\blacksquare$

#### Variance of the MA(1) Process
The variance of the MA(1) process measures the dispersion of the time series values around its mean. It is defined as $E[(Y_t - \mu)^2]$. Substituting the definition of the MA(1) process, we have:
$$E[(Y_t - \mu)^2] = E[(\varepsilon_t + \theta\varepsilon_{t-1})^2].$$
Expanding the square, we obtain:
$$E[(Y_t - \mu)^2] = E[\varepsilon_t^2 + 2\theta\varepsilon_t\varepsilon_{t-1} + \theta^2\varepsilon_{t-1}^2].$$
Applying the linearity of the expectation operator:
$$E[(Y_t - \mu)^2] = E(\varepsilon_t^2) + 2\theta E(\varepsilon_t\varepsilon_{t-1}) + \theta^2 E(\varepsilon_{t-1}^2).$$
Since $\varepsilon_t$ is white noise, $E(\varepsilon_t^2) = \sigma^2$ and $E(\varepsilon_{t-1}^2) = \sigma^2$. Furthermore, the error terms are uncorrelated [^5], meaning $E(\varepsilon_t\varepsilon_{t-1}) = 0$. Therefore:
$$E[(Y_t - \mu)^2] = \sigma^2 + 2\theta \cdot 0 + \theta^2 \sigma^2 = \sigma^2 + \theta^2 \sigma^2 = (1 + \theta^2)\sigma^2.$$
This result shows that the variance of the MA(1) process depends on both the variance of the white noise process ($\sigma^2$) and the moving average coefficient ($\theta$) [^1]. The parameter $\theta$ scales the contribution of the lagged error term to the overall variability of the process.

**Teorema 2 (Variance):** The variance of an MA(1) process, $Y_t = \mu + \varepsilon_t + \theta\varepsilon_{t-1}$, is equal to $(1 + \theta^2)\sigma^2$.

*Proof:*
I. Come√ßamos com a defini√ß√£o da vari√¢ncia: $Var(Y_t) = E[(Y_t - \mu)^2]$.
II. Substitu√≠mos $Y_t$ pela sua defini√ß√£o no processo MA(1): $Var(Y_t) = E[(\mu + \varepsilon_t + \theta\varepsilon_{t-1} - \mu)^2] = E[(\varepsilon_t + \theta\varepsilon_{t-1})^2]$.
III. Expandimos o quadrado: $E[(\varepsilon_t + \theta\varepsilon_{t-1})^2] = E[\varepsilon_t^2 + 2\theta\varepsilon_t\varepsilon_{t-1} + \theta^2\varepsilon_{t-1}^2]$.
IV. Aplicamos a linearidade do operador de esperan√ßa: $E[\varepsilon_t^2] + 2\theta E[\varepsilon_t\varepsilon_{t-1}] + \theta^2E[\varepsilon_{t-1}^2]$.
V. Como $\varepsilon_t$ √© ru√≠do branco, $E[\varepsilon_t^2] = \sigma^2$ e $E[\varepsilon_{t-1}^2] = \sigma^2$. Al√©m disso, $E[\varepsilon_t\varepsilon_{t-1}] = 0$ porque os termos de erro s√£o n√£o correlacionados.
VI. Substitu√≠mos estes resultados na equa√ß√£o: $\sigma^2 + 2\theta \cdot 0 + \theta^2\sigma^2 = \sigma^2 + \theta^2\sigma^2 = (1 + \theta^2)\sigma^2$.
VII. Portanto, a vari√¢ncia do processo MA(1) √© $(1 + \theta^2)\sigma^2$. ‚ñ†

> üí° **Exemplo Num√©rico:**
> Let's consider two MA(1) processes, MA(1)-A and MA(1)-B, with different values for $\theta$ while keeping $\sigma^2$ constant to illustrate the impact of $\theta$ on the variance:
>
> *   MA(1)-A: $Y_t = 5 + \varepsilon_t + 0.2\varepsilon_{t-1}$, where $\sigma^2 = 4$
> *   MA(1)-B: $Y_t = 5 + \varepsilon_t + 0.9\varepsilon_{t-1}$, where $\sigma^2 = 4$
>
> For MA(1)-A, the variance is:
> $Var(Y_t) = (1 + 0.2^2) \cdot 4 = (1 + 0.04) \cdot 4 = 1.04 \cdot 4 = 4.16$
>
> For MA(1)-B, the variance is:
> $Var(Y_t) = (1 + 0.9^2) \cdot 4 = (1 + 0.81) \cdot 4 = 1.81 \cdot 4 = 7.24$
>
> This numerical example shows how a higher value of $\theta$ (0.9 in MA(1)-B compared to 0.2 in MA(1)-A) leads to a larger variance. The process MA(1)-B will have values that are more spread out around its mean of 5 compared to MA(1)-A. This increased spread is directly attributable to the larger influence of the lagged error term, as dictated by $\theta$.
>
> ```python
> import numpy as np
>
> np.random.seed(42) # for reproducibility
> mu = 5
> sigma_sq = 4
> num_obs = 1000
>
> # MA(1)-A
> theta_a = 0.2
> errors_a = np.random.normal(0, np.sqrt(sigma_sq), num_obs)
> y_a = np.zeros(num_obs)
> y_a[0] = mu + errors_a[0]
> for t in range(1, num_obs):
>     y_a[t] = mu + errors_a[t] + theta_a * errors_a[t-1]
> sample_var_a = np.var(y_a)
> theoretical_var_a = (1 + theta_a**2) * sigma_sq
> print(f"MA(1)-A: Sample Variance: {sample_var_a:.2f}, Theoretical Variance: {theoretical_var_a:.2f}")
>
> # MA(1)-B
> theta_b = 0.9
> errors_b = np.random.normal(0, np.sqrt(sigma_sq), num_obs)
> y_b = np.zeros(num_obs)
> y_b[0] = mu + errors_b[0]
> for t in range(1, num_obs):
>     y_b[t] = mu + errors_b[t] + theta_b * errors_b[t-1]
> sample_var_b = np.var(y_b)
> theoretical_var_b = (1 + theta_b**2) * sigma_sq
> print(f"MA(1)-B: Sample Variance: {sample_var_b:.2f}, Theoretical Variance: {theoretical_var_b:.2f}")
> ```
>
> Output:
> ```
> MA(1)-A: Sample Variance: 4.09, Theoretical Variance: 4.16
> MA(1)-B: Sample Variance: 7.21, Theoretical Variance: 7.24
> ```

**Corol√°rio 2.1:**

The autocovariance at lag 0, denoted as $\gamma_0$, is equivalent to the variance of the process, i.e., $\gamma_0 = (1+\theta^2)\sigma^2$.

 Building on this result, it's useful to establish the autocovariance for lag 1.

 **Teorema 2.2 (Autocovariance at Lag 1):** The autocovariance of an MA(1) process at lag 1, denoted as $\gamma_1$, is equal to $\theta\sigma^2$.

 *Proof:*
 The autocovariance at lag 1 is defined as $\gamma_1 = E[(Y_t - \mu)(Y_{t-1} - \mu)]$. Substituting the definition of the MA(1) process:
 $$\gamma_1 = E[(\varepsilon_t + \theta\varepsilon_{t-1})(\varepsilon_{t-1} + \theta\varepsilon_{t-2})].$$
 Expanding the product, we have:
 $$\gamma_1 = E[\varepsilon_t\varepsilon_{t-1} + \theta\varepsilon_t\varepsilon_{t-2} + \theta\varepsilon_{t-1}^2 + \theta^2\varepsilon_{t-1}\varepsilon_{t-2}].$$
 Applying the linearity of the expectation operator:
 $$\gamma_1 = E(\varepsilon_t\varepsilon_{t-1}) + \theta E(\varepsilon_t\varepsilon_{t-2}) + \theta E(\varepsilon_{t-1}^2) + \theta^2 E(\varepsilon_{t-1}\varepsilon_{t-2}).$$
 Since $\varepsilon_t$ is white noise, $E(\varepsilon_t\varepsilon_{t-1}) = 0$, $E(\varepsilon_t\varepsilon_{t-2}) = 0$, $E(\varepsilon_{t-1}\varepsilon_{t-2}) = 0$, and $E(\varepsilon_{t-1}^2) = \sigma^2$. Therefore:
 $$\gamma_1 = 0 + 0 + \theta\sigma^2 + 0 = \theta\sigma^2.$$
 Thus, the autocovariance at lag 1 is $\theta\sigma^2$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Suppose we have an MA(1) process defined by $Y_t = 2 + \varepsilon_t + 0.7\varepsilon_{t-1}$, where $\varepsilon_t$ is white noise with $\sigma^2 = 1.5$.
>
> Then, the autocovariance at lag 1 is $\gamma_1 = \theta\sigma^2 = 0.7 \cdot 1.5 = 1.05$. This positive autocovariance indicates a positive correlation between $Y_t$ and $Y_{t-1}$.  A larger value of $\gamma_1$ would indicate a stronger positive correlation.
>
> Let's compare this to another process with a negative $\theta$: $Y_t = 2 + \varepsilon_t - 0.7\varepsilon_{t-1}$, where $\varepsilon_t$ is white noise with $\sigma^2 = 1.5$. In this case, $\gamma_1 = \theta\sigma^2 = -0.7 \cdot 1.5 = -1.05$. This negative autocovariance indicates a negative correlation between $Y_t$ and $Y_{t-1}$.

 Furthermore, autocovariances at lags greater than 1 are equal to zero.

 **Teorema 2.3 (Autocovariance at Lag k > 1):** The autocovariance of an MA(1) process at lag k, where k > 1, denoted as $\gamma_k$, is equal to 0.

 *Proof:*
 I. Come√ßamos com a defini√ß√£o da autocovari√¢ncia no atraso k: $\gamma_k = E[(Y_t - \mu)(Y_{t-k} - \mu)]$.
 II. Substitu√≠mos $Y_t$ e $Y_{t-k}$ pelas suas defini√ß√µes no processo MA(1): $\gamma_k = E[(\varepsilon_t + \theta\varepsilon_{t-1})(\varepsilon_{t-k} + \theta\varepsilon_{t-k-1})]$.
 III. Expandimos o produto: $\gamma_k = E[\varepsilon_t\varepsilon_{t-k} + \theta\varepsilon_t\varepsilon_{t-k-1} + \theta\varepsilon_{t-1}\varepsilon_{t-k} + \theta^2\varepsilon_{t-1}\varepsilon_{t-k-1}]$.
 IV. Aplicamos a linearidade do operador de esperan√ßa: $\gamma_k = E[\varepsilon_t\varepsilon_{t-k}] + \theta E[\varepsilon_t\varepsilon_{t-k-1}] + \theta E[\varepsilon_{t-1}\varepsilon_{t-k}] + \theta^2 E[\varepsilon_{t-1}\varepsilon_{t-k-1}]$.
 V. Como $\varepsilon_t$ √© ru√≠do branco e $k > 1$, todos os termos de esperan√ßa s√£o zero, porque os erros em diferentes per√≠odos de tempo s√£o n√£o correlacionados.
 VI. Portanto, $\gamma_k = 0 + 0 + 0 + 0 = 0$.
 VII. Assim, a autocovari√¢ncia no atraso $k > 1$ √© 0. ‚ñ†

**Teorema 3 (Autocorrelation Function - ACF):**
The autocorrelation function (ACF) of an MA(1) process, denoted as $\rho_k$, is given by:
  * $\rho_0 = 1$
  * $\rho_1 = \frac{\theta}{1+\theta^2}$
  * $\rho_k = 0$ for $k > 1$

*Proof:*
The autocorrelation function is defined as $\rho_k = \frac{\gamma_k}{\gamma_0}$.

*   For $k = 0$: $\rho_0 = \frac{\gamma_0}{\gamma_0} = \frac{(1+\theta^2)\sigma^2}{(1+\theta^2)\sigma^2} = 1$.
*   For $k = 1$: $\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{\theta\sigma^2}{(1+\theta^2)\sigma^2} = \frac{\theta}{1+\theta^2}$.
*   For $k > 1$: $\rho_k = \frac{\gamma_k}{\gamma_0} = \frac{0}{(1+\theta^2)\sigma^2} = 0$.

Therefore, the ACF of an MA(1) process has a significant spike only at lag 1, and is zero for all lags greater than 1. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Let's calculate the theoretical ACF for an MA(1) process with $\theta = 0.5$.
>
> *   $\rho_0 = 1$
> *   $\rho_1 = \frac{0.5}{1 + 0.5^2} = \frac{0.5}{1 + 0.25} = \frac{0.5}{1.25} = 0.4$
> *   $\rho_k = 0$ for $k > 1$
>
> This means that the ACF will have a value of 1 at lag 0, a value of 0.4 at lag 1, and 0 for all lags greater than 1. This is a key characteristic of the MA(1) process and is used for identifying MA(1) processes in real-world data. If you plot the ACF of a real-world time series and observe this pattern, it suggests that an MA(1) model might be appropriate.
>
> Now, consider an MA(1) process with $\theta = -0.8$.
>
> *   $\rho_0 = 1$
> *   $\rho_1 = \frac{-0.8}{1 + (-0.8)^2} = \frac{-0.8}{1 + 0.64} = \frac{-0.8}{1.64} \approx -0.488$
> *   $\rho_k = 0$ for $k > 1$
>
> Here, the ACF at lag 1 is negative, indicating a negative correlation between adjacent observations. The magnitude of -0.488 indicates the strength of this negative correlation.
>
> We can visualize the ACF using Mermaid:
>
> ```mermaid
> graph LR
>     A[Lag 0] --> B(1.0)
>     A --> C[Lag 1]
>     C --> D(0.4)
>     A --> E[Lag 2]
>     E --> F(0)
>     A --> G[Lag 3]
>     G --> H(0)
>     style B fill:#f9f,stroke:#333,stroke-width:2px
>     style D fill:#f9f,stroke:#333,stroke-width:2px
>     style F fill:#f9f,stroke:#333,stroke-width:2px
>     style H fill:#f9f,stroke:#333,stroke-width:2px
>     linkStyle 0,2,3 stroke-width:0px;
> ```

### Conclus√£o
The expectation of the MA(1) process [^1], $E(Y_t) = \mu$, confirms that the constant term, $\mu$, represents the long-run average level of the series. Meanwhile, the variance of the MA(1) process, $E[(Y_t - \mu)^2] = (1 + \theta^2)\sigma^2$ [^1], highlights the influence of both the white noise variance ($\sigma^2$) and the moving average coefficient ($\theta$) on the overall variability. Understanding these properties is essential for modeling and interpreting time series data using MA(1) processes, and these values impact the autocovariance function as well [^22]. The next sections build on this knowledge to further explore the autocovariance and autocorrelation functions.

### Refer√™ncias
[^1]: Imagine a battery of I such computers generating sequences {y{1},...,y{I}}x, and consider selecting the observation associated with date t from each sequence. This would be described as a sample of I realizations of the random variable Y.. This random variable has some density, denoted fr(y,), which is called the un- conditional density of Y.. For example, for the Gaussian white noise process, this density is given by...
[^3]: Sometimes for emphasis the expectation E(Y) is called the unconditional mean of Y,. The unconditional mean is denoted Œº,: E(Y) = ŒºŒπ
[^4]: The basic building block for all the processes considered in this chapter is a sequence {8} - whose elements have mean zero and variance œÉ¬≤, E(Œµ,) = 0
[^5]: A process satisfying [3.2.1] through [3.2.3] is described as a white noise process. We shall on occasion wish to replace [3.2.3] with the slightly stronger condition that the e's are independent across time: Œµ,, &, independent for t ‚â† T.
[^22]: Consider now a seemingly different MA(1) process, Œ´, ‚àí Œº = (1 + Œ∏L)·∫º, ...
<!-- END -->