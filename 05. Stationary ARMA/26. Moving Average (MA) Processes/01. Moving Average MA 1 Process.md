### The First-Order Moving Average Process: MA(1)

### Introdu√ß√£o
Expanding upon the fundamental concepts of time series analysis, particularly the characteristics of white noise processes, we now delve into a specific type of model known as the Moving Average (MA) process. This chapter section will focus on the first-order moving average process, denoted MA(1), exploring its defining equation, statistical properties, and implications for modeling time series data. Understanding MA(1) processes is crucial for capturing short-term dependencies and smoothing time series data.

### Conceitos Fundamentais
The defining equation for an MA(1) process is given by [^1]
$$Y_t = \mu + \varepsilon_t + \theta\varepsilon_{t-1},$$
where:
- $Y_t$ represents the value of the time series at time *t*.
- $\mu$ is a constant mean term. [^1]
- $\varepsilon_t$ is a white noise error term at time *t*, with $E(\varepsilon_t) = 0$ [^4] and $E(\varepsilon_t \varepsilon_\tau) = 0$ for $t \ne \tau$ [^5].
- $\theta$ is a constant parameter that determines the weight or influence of the previous period's error term ($\varepsilon_{t-1}$) on the current value of the series. [^1]

The MA(1) process is characterized by its memory of only one period's past error. This means that the current value of the series is influenced by the current shock ($\varepsilon_t$) and the shock from the immediately preceding period ($\varepsilon_{t-1}$), weighted by $\theta$. As stated, the term "moving average" arises from the fact that $Y_t$ is constructed from a weighted sum of the two most recent values of $\varepsilon$ [^1].

> üí° **Exemplo Num√©rico:**
> Let's assume we have a time series generated by an MA(1) process with $\mu = 10$, $\theta = 0.5$, and $\sigma^2 = 1$. Suppose we have the following white noise error terms: $\varepsilon_0 = 2$, $\varepsilon_1 = -1$, $\varepsilon_2 = 0.5$, $\varepsilon_3 = -0.2$.
>
> Then, the time series values $Y_t$ would be:
>
> $Y_1 = \mu + \varepsilon_1 + \theta\varepsilon_0 = 10 + (-1) + 0.5(2) = 10$
> $Y_2 = \mu + \varepsilon_2 + \theta\varepsilon_1 = 10 + 0.5 + 0.5(-1) = 9.5$
> $Y_3 = \mu + \varepsilon_3 + \theta\varepsilon_2 = 10 + (-0.2) + 0.5(0.5) = 9.55$
>
> ```mermaid
> graph LR
>     A[Œµ‚ÇÄ = 2] --> B(Y‚ÇÅ = 10)
>     B --> C[Œµ‚ÇÅ = -1]
>     C --> D(Y‚ÇÇ = 9.5)
>     D --> E[Œµ‚ÇÇ = 0.5]
>     E --> F(Y‚ÇÉ = 9.55)
>     F --> G[Œµ‚ÇÉ = -0.2]
> ```
>
> In this example, we can see how the current value of the time series is affected by the current and previous error terms, weighted by $\theta$.

#### Expectation and Variance
The expectation of the MA(1) process is [^1]:
$$E(Y_t) = E(\mu + \varepsilon_t + \theta\varepsilon_{t-1}) = \mu + E(\varepsilon_t) + \theta E(\varepsilon_{t-1}) = \mu,$$
since the expected value of the white noise terms is zero [^1, 4]. This confirms that $\mu$ represents the unconditional mean of the process.

**Prova:**

Para derivar a expectativa do processo MA(1), podemos usar as propriedades da expectativa linear.

I. Comece com a defini√ß√£o do processo MA(1):
   $$Y_t = \mu + \varepsilon_t + \theta\varepsilon_{t-1}$$

II. Aplique o operador de expectativa em ambos os lados da equa√ß√£o:
   $$E(Y_t) = E(\mu + \varepsilon_t + \theta\varepsilon_{t-1})$$

III. Use a propriedade de linearidade da expectativa:
   $$E(Y_t) = E(\mu) + E(\varepsilon_t) + \theta E(\varepsilon_{t-1})$$

IV. Como $\mu$ √© uma constante, $E(\mu) = \mu$. Como $\varepsilon_t$ e $\varepsilon_{t-1}$ s√£o ru√≠dos brancos, $E(\varepsilon_t) = 0$ e $E(\varepsilon_{t-1}) = 0$:
   $$E(Y_t) = \mu + 0 + \theta \cdot 0$$

V. Simplifique a express√£o:
   $$E(Y_t) = \mu$$

Portanto, a expectativa do processo MA(1) √© $\mu$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> If $\mu = 5$ for an MA(1) process, then $E(Y_t) = 5$, regardless of the values of $\varepsilon_t$ and $\theta$.

The variance of the MA(1) process, denoted by $\gamma_0$, is calculated as [^1]:
$$E(Y_t - \mu)^2 = E(\varepsilon_t + \theta\varepsilon_{t-1})^2 = E(\varepsilon_t^2 + 2\theta\varepsilon_t\varepsilon_{t-1} + \theta^2\varepsilon_{t-1}^2).$$
Since the error terms are uncorrelated [^4], $E(\varepsilon_t\varepsilon_{t-1}) = 0$.  Therefore,
$$\gamma_0 = E(\varepsilon_t^2) + \theta^2 E(\varepsilon_{t-1}^2) = \sigma^2 + \theta^2\sigma^2 = (1 + \theta^2)\sigma^2,$$
where $\sigma^2$ is the variance of the white noise process [^1].

**Prova:**

Para derivar a vari√¢ncia do processo MA(1), podemos usar as propriedades da vari√¢ncia e do ru√≠do branco.

I. Comece com a express√£o da vari√¢ncia:
   $$Var(Y_t) = E[(Y_t - \mu)^2]$$

II. Substitua $Y_t$ pela defini√ß√£o do processo MA(1):
   $$Var(Y_t) = E[(\varepsilon_t + \theta\varepsilon_{t-1})^2]$$

III. Expanda o termo quadr√°tico:
   $$Var(Y_t) = E[\varepsilon_t^2 + 2\theta\varepsilon_t\varepsilon_{t-1} + \theta^2\varepsilon_{t-1}^2]$$

IV. Use a linearidade da expectativa:
   $$Var(Y_t) = E(\varepsilon_t^2) + 2\theta E(\varepsilon_t\varepsilon_{t-1}) + \theta^2 E(\varepsilon_{t-1}^2)$$

V. Uma vez que $\varepsilon_t$ √© ru√≠do branco, $E(\varepsilon_t^2) = \sigma^2$, $E(\varepsilon_{t-1}^2) = \sigma^2$, e $E(\varepsilon_t\varepsilon_{t-1}) = 0$ para $t \ne t-1$:
   $$Var(Y_t) = \sigma^2 + 2\theta \cdot 0 + \theta^2 \sigma^2$$

VI. Simplifique a express√£o:
   $$Var(Y_t) = \sigma^2 + \theta^2\sigma^2 = (1 + \theta^2)\sigma^2$$

Portanto, a vari√¢ncia do processo MA(1) √© $(1 + \theta^2)\sigma^2$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Let's say $\theta = 0.6$ and $\sigma^2 = 4$. Then the variance of the MA(1) process is:
> $\gamma_0 = (1 + 0.6^2) \cdot 4 = (1 + 0.36) \cdot 4 = 1.36 \cdot 4 = 5.44$

#### Autocovariance and Autocorrelation
The first autocovariance, $\gamma_1$, which measures the covariance between $Y_t$ and $Y_{t-1}$, is derived as [^1]:
$$E[(Y_t - \mu)(Y_{t-1} - \mu)] = E[(\varepsilon_t + \theta\varepsilon_{t-1})(\varepsilon_{t-1} + \theta\varepsilon_{t-2})] = E[\varepsilon_t\varepsilon_{t-1} + \theta\varepsilon_{t-1}^2 + \theta\varepsilon_t\varepsilon_{t-2} + \theta^2\varepsilon_{t-1}\varepsilon_{t-2}].$$
Again, using the properties of white noise, this simplifies to
$$\gamma_1 = \theta E(\varepsilon_{t-1}^2) = \theta\sigma^2.$$

**Prova:**

Para derivar a primeira autocovari√¢ncia do processo MA(1), podemos usar as propriedades da covari√¢ncia e do ru√≠do branco.

I. Comece com a defini√ß√£o da primeira autocovari√¢ncia:
   $$\gamma_1 = E[(Y_t - \mu)(Y_{t-1} - \mu)]$$

II. Substitua $Y_t$ e $Y_{t-1}$ pelas suas defini√ß√µes do processo MA(1):
   $$\gamma_1 = E[(\varepsilon_t + \theta\varepsilon_{t-1})(\varepsilon_{t-1} + \theta\varepsilon_{t-2})]$$

III. Expanda o produto:
   $$\gamma_1 = E[\varepsilon_t\varepsilon_{t-1} + \theta\varepsilon_{t-1}^2 + \theta\varepsilon_t\varepsilon_{t-2} + \theta^2\varepsilon_{t-1}\varepsilon_{t-2}]$$

IV. Use a linearidade da expectativa:
   $$\gamma_1 = E(\varepsilon_t\varepsilon_{t-1}) + \theta E(\varepsilon_{t-1}^2) + \theta E(\varepsilon_t\varepsilon_{t-2}) + \theta^2 E(\varepsilon_{t-1}\varepsilon_{t-2})$$

V. Uma vez que $\varepsilon_t$ √© ru√≠do branco, $E(\varepsilon_t\varepsilon_{t-1}) = 0$, $E(\varepsilon_t\varepsilon_{t-2}) = 0$, $E(\varepsilon_{t-1}\varepsilon_{t-2}) = 0$, e $E(\varepsilon_{t-1}^2) = \sigma^2$:
   $$\gamma_1 = 0 + \theta\sigma^2 + 0 + 0$$

VI. Simplifique a express√£o:
   $$\gamma_1 = \theta\sigma^2$$

Portanto, a primeira autocovari√¢ncia do processo MA(1) √© $\theta\sigma^2$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> If $\theta = 0.6$ and $\sigma^2 = 4$, then the first autocovariance is:
> $\gamma_1 = 0.6 \cdot 4 = 2.4$

For lags greater than one ($j > 1$), the autocovariances are zero [^1]:
$$E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\varepsilon_t + \theta\varepsilon_{t-1})(\varepsilon_{t-j} + \theta\varepsilon_{t-j-1})] = 0,$$
since there are no overlapping error terms. This reflects the one-period memory of the MA(1) process.

**Prova:**

Para demonstrar que as autocovari√¢ncias s√£o zero para lags maiores que 1 no processo MA(1):

I. Comece com a defini√ß√£o da autocovari√¢ncia para lag $j > 1$:
   $$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$$

II. Substitua $Y_t$ e $Y_{t-j}$ pelas suas defini√ß√µes do processo MA(1):
   $$\gamma_j = E[(\varepsilon_t + \theta\varepsilon_{t-1})(\varepsilon_{t-j} + \theta\varepsilon_{t-j-1})]$$

III. Expanda o produto:
   $$\gamma_j = E[\varepsilon_t\varepsilon_{t-j} + \theta\varepsilon_{t-1}\varepsilon_{t-j} + \theta\varepsilon_t\varepsilon_{t-j-1} + \theta^2\varepsilon_{t-1}\varepsilon_{t-j-1}]$$

IV. Use a linearidade da expectativa:
   $$\gamma_j = E(\varepsilon_t\varepsilon_{t-j}) + \theta E(\varepsilon_{t-1}\varepsilon_{t-j}) + \theta E(\varepsilon_t\varepsilon_{t-j-1}) + \theta^2 E(\varepsilon_{t-1}\varepsilon_{t-j-1})$$

V. Para $j > 1$, todos os termos envolvendo produtos de $\varepsilon$ t√™m expectativa zero devido √† propriedade de ru√≠do branco (n√£o correla√ß√£o para diferentes tempos):
   $$E(\varepsilon_t\varepsilon_{t-j}) = 0, \quad E(\varepsilon_{t-1}\varepsilon_{t-j}) = 0, \quad E(\varepsilon_t\varepsilon_{t-j-1}) = 0, \quad E(\varepsilon_{t-1}\varepsilon_{t-j-1}) = 0$$

VI. Portanto:
   $$\gamma_j = 0 + 0 + 0 + 0 = 0$$

Assim, para $j > 1$, a autocovari√¢ncia $\gamma_j$ √© zero. $\blacksquare$

The *j*th autocorrelation, $\rho_j$, is defined as the *j*th autocovariance divided by the variance [^6]:
$$\rho_j = \frac{\gamma_j}{\gamma_0}.$$
Thus, for the MA(1) process [^6]:
$$ \rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{\theta\sigma^2}{(1 + \theta^2)\sigma^2} = \frac{\theta}{1 + \theta^2},$$
and $\rho_j = 0$ for $j > 1$.

> üí° **Exemplo Num√©rico:**
> Using the previous values $\theta = 0.6$ and $\sigma^2 = 4$, we found $\gamma_1 = 2.4$ and $\gamma_0 = 5.44$. Thus, the first autocorrelation is:
> $\rho_1 = \frac{2.4}{5.44} \approx 0.441$
>
> We can also calculate it directly using the formula:
> $\rho_1 = \frac{0.6}{1 + 0.6^2} = \frac{0.6}{1.36} \approx 0.441$
>
> For $j > 1$, $\rho_j = 0$.

**Proposi√ß√£o 1**
The autocorrelation function (ACF) of an MA(1) process, $\rho_j$, satisfies the inequality $|\rho_1| \le 0.5$.

*Proof:* To demonstrate this, we need to find the maximum possible value of $|\rho_1| = \left| \frac{\theta}{1 + \theta^2} \right|$. To find the maximum, we can take the derivative of $\frac{\theta}{1 + \theta^2}$ with respect to $\theta$ and set it equal to zero:
$$ \frac{d}{d\theta} \left( \frac{\theta}{1 + \theta^2} \right) = \frac{(1 + \theta^2)(1) - \theta(2\theta)}{(1 + \theta^2)^2} = \frac{1 - \theta^2}{(1 + \theta^2)^2}.$$
Setting the derivative to zero, we get $1 - \theta^2 = 0$, which implies $\theta = \pm 1$. When $\theta = 1$, $\rho_1 = \frac{1}{1 + 1^2} = \frac{1}{2} = 0.5$. When $\theta = -1$, $\rho_1 = \frac{-1}{1 + (-1)^2} = -\frac{1}{2} = -0.5$. Thus, the maximum absolute value of $\rho_1$ is 0.5. Therefore, $|\rho_1| \le 0.5$.

To further elaborate on the autocorrelation structure, we can express the autocovariance function in terms of the variance and the parameter $\theta$.

**Teorema 2**
The autocovariance function $\gamma_j$ of an MA(1) process can be expressed as:
$$ \gamma_j = \begin{cases} (1+\theta^2)\sigma^2, & j=0 \\ \theta\sigma^2, & j= \pm 1 \\ 0, & |j| > 1 \end{cases} $$

*Proof:* The proof follows directly from the derivations of $\gamma_0$, $\gamma_1$ and $\gamma_j$ for $j>1$ presented earlier. For $j=0$, we have $\gamma_0 = (1+\theta^2)\sigma^2$. For $j=1$, we have $\gamma_1 = \theta\sigma^2$. Since the autocovariance function is symmetric, $\gamma_{-1} = \gamma_1 = \theta\sigma^2$. For $|j|>1$, $\gamma_j = 0$. Thus, the expression for $\gamma_j$ covers all possible lags.

> üí° **Exemplo Num√©rico:**
> Let's assume we have an MA(1) process with $\theta = 0.8$ and $\sigma^2 = 9$.
>
> Then, the autocovariance function would be:
> - $\gamma_0 = (1 + 0.8^2) \cdot 9 = (1 + 0.64) \cdot 9 = 1.64 \cdot 9 = 14.76$
> - $\gamma_1 = 0.8 \cdot 9 = 7.2$
> - $\gamma_j = 0$ for $|j| > 1$

#### Stationarity and Ergodicity
An MA(1) process is covariance-stationary regardless of the value of $\theta$, since its mean and autocovariances do not depend on time [^1]. Furthermore, since the autocovariances satisfy $\sum_{j=0}^{\infty} |\gamma_j| < \infty$, the MA(1) process is ergodic for all moments when the error term $\varepsilon_t$ is Gaussian [^1, 4, 5].

**Prova:**

Para demonstrar que o processo MA(1) √© estacion√°rio em covari√¢ncia, devemos mostrar que a sua m√©dia e autocovari√¢ncias n√£o dependem do tempo.

I. A m√©dia do processo MA(1) √© $E(Y_t) = \mu$, que √© uma constante e, portanto, n√£o depende do tempo.

II. A autocovari√¢ncia no lag *j* √© dada por $\gamma_j$. J√° mostr√°mos que:
   - $\gamma_0 = (1 + \theta^2)\sigma^2$
   - $\gamma_1 = \theta\sigma^2$
   - $\gamma_j = 0$ para $j > 1$

III. Como $\theta$ e $\sigma^2$ s√£o constantes, os valores de $\gamma_0$, $\gamma_1$ e $\gamma_j$ n√£o dependem de *t*. Dependem apenas do lag *j*.

IV. Uma vez que a m√©dia e as autocovari√¢ncias do processo MA(1) s√£o constantes ao longo do tempo, o processo √© estacion√°rio em covari√¢ncia.

Al√©m disso, a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ implica a ergodicit√†.

V. Verificando a condi√ß√£o de ergodicit√†:
$$\sum_{j=0}^{\infty} |\gamma_j| = |\gamma_0| + |\gamma_1| + \sum_{j=2}^{\infty} |\gamma_j| = |(1 + \theta^2)\sigma^2| + |\theta\sigma^2| + 0$$

VI. Como esta soma √© finita, o processo MA(1) √© erg√≥dico para todos os momentos quando o termo de erro $\varepsilon_t$ √© Gaussiano.

Assim, o processo MA(1) √© estacion√°rio em covari√¢ncia e erg√≥dico. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> For any value of $\theta$ and $\sigma^2$, the sum of the absolute values of the autocovariances of an MA(1) process is always finite, confirming its ergodicity.
>
> For instance, if $\theta = 0.7$ and $\sigma^2 = 2$, then:
> $\sum_{j=0}^{\infty} |\gamma_j| = |(1 + 0.7^2) \cdot 2| + |0.7 \cdot 2| = |1.49 \cdot 2| + |1.4| = 2.98 + 1.4 = 4.38 < \infty$

#### Invertibility

The concept of **invertibility** is crucial in time series analysis [^22]. An MA process is invertible if it can be expressed as an infinite-order autoregressive (AR($\infty$)) process [^22]. For an MA(1) process, invertibility requires that $|\theta| < 1$ [^22].

**Teorema 1**
An MA(1) process is invertible if and only if the root of the characteristic equation lies outside the unit circle.

*Proof:*
The MA(1) process is given by $Y_t = \mu + \varepsilon_t + \theta\varepsilon_{t-1}$. We can rewrite this using the lag operator $L$ as $Y_t = \mu + (1 + \theta L)\varepsilon_t$.
The characteristic equation is then given by $(1 + \theta L) = 0$, which implies $L = -\frac{1}{\theta}$. For invertibility, we require that $|L| > 1$, which means $\left|-\frac{1}{\theta}\right| > 1$, or equivalently, $|\theta| < 1$. Conversely, if $|\theta| < 1$, then $|L| = \left|-\frac{1}{\theta}\right| > 1$, and the process is invertible.

**Corol√°rio 1.1**

If $|\theta| \geq 1$, the MA(1) process is non-invertible. This implies that there is no convergent AR($\infty$) representation for the process. In practice, this means that estimating the past shocks $\varepsilon_t$ from the observed data $Y_t$ becomes unstable or impossible.

The invertibility condition has important implications for the Wold representation of the MA(1) process.

**Lema 1**
If an MA(1) process is invertible, then it has a convergent Wold representation as an infinite autoregressive process.

*Proof:*
Since the MA(1) process is given by $Y_t = \mu + \varepsilon_t + \theta\varepsilon_{t-1}$, we can rewrite this as $Y_t - \mu = (1 + \theta L)\varepsilon_t$, where $L$ is the lag operator.  If $|\theta| < 1$, then $(1 + \theta L)^{-1}$ has a convergent power series representation:
$$(1 + \theta L)^{-1} = 1 - \theta L + \theta^2 L^2 - \theta^3 L^3 + \ldots = \sum_{i=0}^{\infty} (-\theta)^i L^i.$$
Therefore, we can express $\varepsilon_t$ as
$$\varepsilon_t = (1 + \theta L)^{-1}(Y_t - \mu) = \sum_{i=0}^{\infty} (-\theta)^i L^i (Y_t - \mu) = (Y_t - \mu) - \theta(Y_{t-1} - \mu) + \theta^2(Y_{t-2} - \mu) - \ldots $$
Substituting this expression for $\varepsilon_t$ into the original MA(1) equation will give us an AR($\infty$) representation of $Y_t$. This representation is convergent because $|\theta|<1$.

> üí° **Exemplo Num√©rico:**
> Let's consider two MA(1) processes: one with $\theta = 0.5$ (invertible) and another with $\theta = 2$ (non-invertible).
>
> For $\theta = 0.5$, the invertibility condition $|\theta| < 1$ is satisfied. Thus, we can write the AR($\infty$) representation as:
> $\varepsilon_t = (Y_t - \mu) - 0.5(Y_{t-1} - \mu) + 0.25(Y_{t-2} - \mu) - 0.125(Y_{t-3} - \mu) + \ldots$
>
> For $\theta = 2$, the invertibility condition $|\theta| < 1$ is not satisfied. Thus, there is no convergent AR($\infty$) representation for this process, making it non-invertible.
>
> In practice, trying to estimate the shocks $\varepsilon_t$ from the observed data for the non-invertible process will lead to unstable or divergent results.

### Conclus√£o
The first-order moving average process, MA(1), provides a foundational model for capturing short-term dependencies in time series data. Its defining equation, $Y_t = \mu + \varepsilon_t + \theta\varepsilon_{t-1}$, along with its statistical properties, make it a versatile tool for smoothing data and understanding the influence of past shocks on current values. The MA(1) process is covariance-stationary, ergodic, and invertible under certain conditions, making it a fundamental building block for more complex time series models.

### Refer√™ncias
[^1]: Imagine a battery of I such computers generating sequences {y{1},...,y{I}}x, and consider selecting the observation associated with date t from each sequence. This would be described as a sample of I realizations of the random variable Y.. This random variable has some density, denoted fr(y,), which is called the un- conditional density of Y.. For example, for the Gaussian white noise process, this density is given by...
[^2]: The expectation of the tth observation of a time series refers to the mean of this probability distribution, provided it exists: E(Y) =  ‚à´ yif(y) dy
[^3]: Sometimes for emphasis the expectation E(Y) is called the unconditional mean of Y,. The unconditional mean is denoted Œº,: E(Y) = ŒºŒπ
[^4]: The basic building block for all the processes considered in this chapter is a sequence {8} - whose elements have mean zero and variance œÉ¬≤, E(Œµ,) = 0
[^5]: A process satisfying [3.2.1] through [3.2.3] is described as a white noise process. We shall on occasion wish to replace [3.2.3] with the slightly stronger condition that the e's are independent across time: Œµ,, &, independent for t ‚â† T.
[^6]: The jth autocorrelation of a covariance-stationary process (denoted p,) is defined as its jth autocovariance divided by the variance: Pj = Œ≥j/Œ≥o
[^22]: Consider now a seemingly different MA(1) process, Œ´, ‚àí Œº = (1 + Œ∏L)·∫º, ...
<!-- END -->