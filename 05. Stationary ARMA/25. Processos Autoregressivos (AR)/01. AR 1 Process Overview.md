## Processos Autoregressivos de Primeira Ordem (AR(1))

### Introdução
Este capítulo aborda os processos autoregressivos (AR), com foco especial no processo AR(1), o bloco de construção mais simples para modelos de séries temporais que incorporam autocorrelação. Como vimos anteriormente [^3.1], a autocorrelação descreve a dependência entre as observações de uma série temporal em diferentes pontos no tempo. O processo AR(1) introduz essa dependência de uma forma linear e direta, permitindo que o valor atual de uma série temporal seja modelado em função do seu valor imediatamente anterior, mais um componente de ruído aleatório. Exploraremos a estrutura matemática, as propriedades de estacionariedade e a relação com os processos de média móvel (MA), utilizando os conceitos estabelecidos nos capítulos anteriores.

### Conceitos Fundamentais

Um processo autoregressivo de primeira ordem, ou AR(1), é caracterizado pela seguinte equação de diferença [^3.4.1]:
$$Y_t = c + \phi Y_{t-1} + \epsilon_t$$
Onde:
- $Y_t$ representa o valor da série temporal no tempo $t$.
- $c$ é uma constante.
- $\phi$ é o coeficiente autoregressivo, que determina a influência do valor passado $Y_{t-1}$ no valor atual $Y_t$.
- $\epsilon_t$ é o termo de erro ou ruído branco, que tem média zero e variância constante $\sigma^2$ [^3.2.1, 3.2.2, 3.2.3].

A simplicidade desta equação torna o modelo AR(1) um ponto de partida para analisar a autocorrelação. Note que este modelo é um caso especial de uma equação de diferença de primeira ordem [^1.1.1, 2.2.1], onde o "input" é dado por $c + \epsilon_t$.

**Estacionariedade:** Uma condição crucial para a aplicabilidade do modelo AR(1) é a sua estacionariedade, ou seja, que as propriedades estatísticas da série temporal não mudem ao longo do tempo. Para o modelo AR(1), a condição de estacionariedade é que o valor absoluto do coeficiente autoregressivo $\phi$ seja menor que um, ou seja, $|\phi| < 1$ [^3.4]. Se $|\phi| \geq 1$, o efeito dos erros passados se acumula, resultando em um processo não estacionário com variância crescente e que diverge ao longo do tempo [^3.4].

**Representação MA(∞):** Um processo AR(1) estacionário pode ser expresso como um processo de média móvel de ordem infinita (MA(∞)) [^3.4.2]. A representação MA(∞) de um AR(1) pode ser obtida reescrevendo a equação [^3.4.1] da seguinte forma:
$$Y_t = \frac{c}{1 - \phi} + \epsilon_t + \phi \epsilon_{t-1} + \phi^2 \epsilon_{t-2} + \phi^3 \epsilon_{t-3} + \ldots$$

Essa representação mostra que o valor atual $Y_t$ é formado por uma combinação linear ponderada de todo o histórico de ruídos brancos $\{\epsilon_t, \epsilon_{t-1}, \epsilon_{t-2}, \ldots\}$, com pesos dados por potências de $\phi$. A condição $|\phi| < 1$ garante a convergência da série infinita e, portanto, que o processo seja estacionário [^3.3.15].

**Autocorrelação:** A função de autocorrelação de um processo AR(1) estacionário apresenta um decaimento geométrico [^3.4.6]. A autocorrelação no lag j é dada por:
$$\rho_j = \phi^j$$
Isso significa que a correlação entre $Y_t$ e $Y_{t-j}$ diminui exponencialmente à medida que a defasagem j aumenta. A natureza geométrica da autocorrelação em modelos AR(1) é uma característica importante para a identificação desses processos em séries temporais.

**Cálculo da Média e Variância:** A média $\mu$ de um processo AR(1) estacionário é dada por:
$$\mu = \frac{c}{1 - \phi}$$ [^3.4.3, 3.4.9]
A variância $\gamma_0$ é dada por:
$$\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$$ [^3.4.4]
Onde $\sigma^2$ é a variância do ruído branco $\epsilon_t$. Essas expressões são derivadas sob a suposição de que o processo é estacionário (ou seja, $|\phi| < 1$).

**Derivação Alternativa da Média e Variância:**
Uma forma alternativa de calcular a média e variância de um processo AR(1) é assumir que o processo é estacionário e substituir $E(Y_t) = E(Y_{t-1}) = \mu$ na equação [^3.4.1]. Isto leva à seguinte equação:
$$E(Y_t) = c + \phi E(Y_{t-1}) + E(\epsilon_t)$$
$$ \mu = c + \phi \mu + 0 $$
$$ \mu = \frac{c}{1-\phi}$$ [^3.4.9]
Em seguida, para a variância, podemos reescrever a equação [^3.4.1] como:
$$Y_t - \mu = \phi(Y_{t-1} - \mu) + \epsilon_t $$
Elevando ambos os lados ao quadrado e tomando as expectativas:
$$ E(Y_t - \mu)^2 = \phi^2 E(Y_{t-1} - \mu)^2 + 2 \phi E[(Y_{t-1} - \mu)\epsilon_t] + E(\epsilon_t^2) $$
Como $\epsilon_t$ é não correlacionado com $Y_{t-1}$, temos
$$ \gamma_0 = \phi^2 \gamma_0 + \sigma^2$$
$$ \gamma_0 = \frac{\sigma^2}{1-\phi^2} $$

**Autocovariância:** Multiplicando ambos os lados de [^3.4.10] por $(Y_{t-j} - \mu)$ e tomando as expectativas, temos:
$$E[(Y_t - \mu)(Y_{t-j} - \mu)] = \phi E[(Y_{t-1} - \mu)(Y_{t-j} - \mu)] + E[\epsilon_t(Y_{t-j} - \mu)]$$
$$ \gamma_j = \phi \gamma_{j-1} + 0, \quad \text{ para } j > 0$$
Essa relação de recorrência tem a solução $\gamma_j = \phi^j \gamma_0$, reproduzindo a autocovariância decrescente geometricamente [^3.4.15].

### Conclusão
O processo autoregressivo de primeira ordem (AR(1)) fornece uma base fundamental para compreender modelos de séries temporais com autocorrelação. Sua simplicidade matemática permite analisar propriedades essenciais, como estacionariedade e autocorrelação, e suas conexões com processos MA(∞). A representação MA(∞) é uma ferramenta importante para entender como o processo AR(1) acumula os efeitos de ruídos brancos passados para criar a autocorrelação observada. Os conceitos e propriedades do processo AR(1) são cruciais para entender modelos mais complexos e para o desenvolvimento de métodos de previsão e estimação em séries temporais. O decaimento geométrico da autocorrelação oferece um padrão identificável em séries temporais que podem ser modeladas por processos AR(1).

### Referências
[^3.1]:  Imagine a battery of I such computers generating sequences {y{1}, y{2}, ..., y{I}}t=... and consider selecting the observation associated with date t from each sequence: {y{1}t, y{2}t, ..., y{I}t}.
[^3.2.1]:  $E(\epsilon_t) = 0$
[^3.2.2]:  $E(\epsilon_t^2) = \sigma^2$
[^3.2.3]:  $E(\epsilon_t\epsilon_\tau) = 0$ for $t\neq \tau$.
[^3.3.15]: $\sum|\psi_j| < \infty$.
[^3.4]:  A first-order autoregression, denoted AR(1), satisfies the following difference equation: $Y_t = c + \phi Y_{t-1} + \epsilon_t$.
[^3.4.1]: $Y_t = c + \phi Y_{t-1} + \epsilon_t$.
[^3.4.2]: $Y_t = (c + \epsilon_t) + \phi(c + \epsilon_{t-1}) + \phi^2(c + \epsilon_{t-2}) + \phi^3(c + \epsilon_{t-3}) + ...= [c/(1 - \phi)] + \epsilon_t + \phi\epsilon_{t-1} + \phi^2\epsilon_{t-2} + \phi^3\epsilon_{t-3}+....$
[^3.4.3]:  $\mu = c/(1 – \phi)$.
[^3.4.4]: $\gamma_0 = E(Y_t − \mu)^2 = E(\epsilon_t + \phi\epsilon_{t-1} + \phi^2\epsilon_{t-2} + \phi^3\epsilon_{t-3} + ...)^2 = (1 + \phi^2 + \phi^4 + \phi^6 + ...) \cdot \sigma^2 = \sigma^2/(1 – \phi^2)$,
[^3.4.6]: $p_j = \gamma_j/\gamma_0 = \phi^j$,
[^3.4.9]: $\mu = c/(1 - \phi)$,
[^3.4.10]: $(Y_t – \mu) = \phi(Y_{t-1} – \mu) + \epsilon_t$
[^3.4.15]: $\gamma_j = \phi\gamma_{j-1}$
[^1.1.1]:  The first-order difference equation, which says that a variable in time t is equal to a parameter times that variable in time t-1 plus some arbitrary constant w:  $y_t =  c +  \phi y_{t-1}$.
[^2.2.1]:   $y_t=  c +  \phi y_{t-1} + w_t$
The solution to this type of equation was shown in equation [2.2.9] to be
$$y_t = \frac{c}{1-\phi} + \sum_{j=0}^{\infty} \phi^j w_{t-j}.$$
This infinite summation representation is convenient because it indicates how a shock in any past period $t-j$ affects the current value of $y_t$. It also will be shown that the infinite summation representation can be used to derive forecasts easily.
### Linear Projection
The criterion in equation [4.1.1] is minimized by setting the forecast equal to the conditional expectation of $Y_{t+1}$, i.e.,
$$Y_{t+1|t} = E(Y_{t+1}|X_t).$$
However, calculating the conditional expectation can be very difficult, and so we may instead want to use a linear projection. This is an approximation to the conditional expectation, where the forecast is a linear function of the conditioning variables:
$$Y_{t+1|t} = a_0 + a_1 x_{1t} + a_2 x_{2t} + \ldots + a_m x_{mt},$$
where $x_{it}$ are the elements of the vector $X_t$ and the $a_i$'s are coefficients to be determined. We want to find the linear projection that minimizes the mean squared error. That is, we want the $a_i$'s to minimize
$$E(Y_{t+1} - a_0 - a_1 x_{1t} - a_2 x_{2t} - \ldots - a_m x_{mt})^2.$$
It can be shown (see the appendix to this chapter) that the coefficients that achieve this are such that the forecast error is uncorrelated with each of the variables used to form the forecast. That is, we require that
$$E((Y_{t+1} - Y_{t+1|t})x_{it}) = 0 \quad \forall i.$$
To derive the values of the $a_i$'s, the following trick will be used. Suppose that $Y_{t+1}$ and the variables $x_{it}$ have zero means. If this is not the case, then we can simply use the variables expressed in terms of deviations from their means. The constant term can be determined by writing
$$a_0 = E(Y_{t+1}) - a_1 E(x_{1t}) - a_2 E(x_{2t}) - \ldots - a_m E(x_{mt}).$$
In the zero mean case, we can write the system of equations as
$$E(Y_{t+1}x_{1t}) = a_1 E(x_{1t}^2) + a_2 E(x_{1t}x_{2t}) + \ldots + a_m E(x_{1t}x_{mt}),$$
$$E(Y_{t+1}x_{2t}) = a_1 E(x_{2t}x_{1t}) + a_2 E(x_{2t}^2) + \ldots + a_m E(x_{2t}x_{mt}),$$
$$\vdots$$
$$E(Y_{t+1}x_{mt}) = a_1 E(x_{mt}x_{1t}) + a_2 E(x_{mt}x_{2t}) + \ldots + a_m E(x_{mt}^2).$$
This is a set of m linear equations in the m unknowns $a_1, a_2, \ldots, a_m$.

### Example: Forecasting an AR(1) Process
As an example, consider forecasting an AR(1) process, where
$$Y_t = \phi Y_{t-1} + \epsilon_t.$$
We want to find the optimal linear projection of $Y_{t+1}$ based on a constant and $Y_t$, so we have
$$Y_{t+1|t} = a_0 + a_1 Y_t.$$
We can write
$$Y_{t+1} = \phi Y_t + \epsilon_{t+1}.$$
In this case, the equations for the $a_i$'s are
$$E(Y_{t+1}) = a_0 + a_1 E(Y_t),$$
$$E(Y_{t+1}Y_t) = a_0 E(Y_t) + a_1 E(Y_t^2).$$
If we assume that the process is covariance-stationary, then
$$E(Y_{t+1}) = E(Y_t) = \mu,$$
and
$$E(Y_{t+1}Y_t) = E(Y_t(\phi Y_{t-1} + \epsilon_t)) = \phi E(Y_t Y_{t-1}) = \phi \gamma_1.$$
Substituting these into the previous equations gives
$$\mu = a_0 + a_1 \mu,$$
$$\phi \gamma_1 = a_0 \mu + a_1 \gamma_0.$$
We know from equations [3.4.3] and [3.4.6] that $\mu = 0$ and $\gamma_1 = \phi \gamma_0$. Substituting these, we have
$$0 = a_0,$$
$$\phi^2 \gamma_0 = a_1 \gamma_0.$$
The solution is $a_0 = 0$ and $a_1 = \phi^2$, and therefore the optimal linear projection is
$$Y_{t+1|t} = \phi Y_t,$$
which is the same as the conditional expectation. In the more general case, where we use a larger set of variables to form the forecast, we will only be able to find an approximation of the conditional expectation. The approximation will be better in the sense that it minimizes the mean squared error, provided that we use the optimal coefficients.
### References
[^2.2.1]: ...
[^2.2.9]: ...
[^3.4.3]: ...
[^3.4.6]: ...
<!-- END -->
