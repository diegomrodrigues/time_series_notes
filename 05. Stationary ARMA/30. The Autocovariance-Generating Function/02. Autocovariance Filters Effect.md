## An√°lise de Dados Filtrados e a ACGF

### Introdu√ß√£o

Em continuidade ao estudo da **fun√ß√£o geradora de autocovari√¢ncia (ACGF)**, este cap√≠tulo se aprofunda na an√°lise de dados que foram submetidos a um **processo de filtragem** antes da an√°lise [^63]. Frequentemente, em an√°lise de s√©ries temporais, os dados originais s√£o transformados por meio de opera√ß√µes matem√°ticas, como diferencia√ß√£o ou m√©dias m√≥veis, para real√ßar certas caracter√≠sticas ou remover ru√≠dos [^63]. Compreender o impacto dessas transforma√ß√µes na estrutura de autocorrela√ß√£o do processo √© crucial para uma interpreta√ß√£o correta dos resultados. Vamos analisar como essas opera√ß√µes afetam a ACGF e como a ACGF transformada pode ser usada para inferir propriedades do processo filtrado.

### Impacto da Filtragem na ACGF

Considere um processo estacion√°rio $Y_t$ que √© filtrado por uma fun√ß√£o linear de defasagem $h(L)$, onde $L$ √© o operador de defasagem (*lag operator*), tal que $L^k Y_t = Y_{t-k}$ [^63]. O processo filtrado resultante, $X_t$, √© dado por:

$$X_t = h(L) Y_t$$

onde $h(L)$ √© um polin√¥mio em $L$:

$$h(L) = \sum_{j=-\infty}^{\infty} h_j L^j$$

Assumimos que a sequ√™ncia de coeficientes $\{h_j\}$ √© **absolutamente summable**, ou seja, $\sum_{j=-\infty}^{\infty} |h_j| < \infty$ [^64]. Esta condi√ß√£o garante a converg√™ncia da s√©rie e permite a aplica√ß√£o de t√©cnicas de an√°lise complexa.

Para processos com tal condi√ß√£o, a ACGF de $X_t$, denotada por $g_X(z)$, est√° relacionada √† ACGF de $Y_t$, denotada por $g_Y(z)$, da seguinte forma [^61]:

$$g_X(z) = h(z) h(z^{-1}) g_Y(z)$$

onde $h(z)$ √© obtido substituindo $L$ por $z$ em $h(L)$, isto √©, $h(z) = \sum_{j=-\infty}^{\infty} h_j z^j$ [^61].

Esta rela√ß√£o fundamental mostra que a filtragem de um processo estacion√°rio resulta na **multiplica√ß√£o da sua ACGF** pela fun√ß√£o $h(z) h(z^{-1})$, onde $h(z)$ √© a transformada *z* da fun√ß√£o de filtro $h(L)$ [^61]. Esta propriedade simplifica a an√°lise do impacto da filtragem na estrutura de autocorrela√ß√£o do processo.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um processo $Y_t$ com ACGF $g_Y(z) = 1 + 0.5z + 0.5z^{-1}$. Isso significa que a autocovari√¢ncia no lag 0 √© 1, e no lag 1 (e -1) √© 0.5. Consideremos um filtro simples de m√©dia m√≥vel de dois per√≠odos: $h(L) = 0.5 + 0.5L$. Isso significa que cada ponto no processo filtrado √© a m√©dia do ponto atual e do ponto anterior no processo original.
>
> Primeiro, calculamos $h(z) = 0.5 + 0.5z$ e $h(z^{-1}) = 0.5 + 0.5z^{-1}$.
>
> Ent√£o, a ACGF do processo filtrado $X_t$ √©:
>
> $g_X(z) = h(z) h(z^{-1}) g_Y(z) = (0.5 + 0.5z)(0.5 + 0.5z^{-1})(1 + 0.5z + 0.5z^{-1})$
>
> Expandindo a express√£o:
>
> $g_X(z) = (0.25 + 0.25z + 0.25z^{-1} + 0.25)(1 + 0.5z + 0.5z^{-1}) = (0.5 + 0.25z + 0.25z^{-1})(1 + 0.5z + 0.5z^{-1})$
>
> $g_X(z) = 0.5 + 0.25z + 0.25z^{-1} + 0.25z + 0.125z^2 + 0.125 + 0.25z^{-1} + 0.125 + 0.125z^{-2}$
>
> $g_X(z) = 0.75 + 0.5z + 0.5z^{-1} + 0.125z^2 + 0.125z^{-2}$
>
> Assim, a ACGF do processo filtrado √© $g_X(z) = 0.75 + 0.5z + 0.5z^{-1} + 0.125z^2 + 0.125z^{-2}$. As autocovari√¢ncias do processo filtrado s√£o: $\gamma_0 = 0.75$, $\gamma_1 = \gamma_{-1} = 0.5$, $\gamma_2 = \gamma_{-2} = 0.125$, e $\gamma_j = 0$ para $|j| > 2$. A filtragem alterou as autocovari√¢ncias, reduzindo a autocovari√¢ncia no lag 0 e introduzindo autocovari√¢ncias n√£o nulas em lags maiores.
>
> ```python
> import numpy as np
>
> # ACGF dos dados originais
> gamma_0_y = 1
> gamma_1_y = 0.5
>
> # Filtro
> h_0 = 0.5
> h_1 = 0.5
>
> # Calculando ACGF do processo filtrado (X_t)
> gamma_0_x = (h_0**2 + h_1**2) * gamma_0_y + 2 * h_0 * h_1 * gamma_1_y
> gamma_1_x = h_0 * (h_0 * gamma_1_y + h_1 * gamma_0_y) + h_1 * (h_1*0)
> gamma_2_x = h_0*h_1*gamma_1_y
>
> print(f"Autocovari√¢ncia no lag 0 (X_t): {gamma_0_x}")
> print(f"Autocovari√¢ncia no lag 1 (X_t): {gamma_1_x}")
> print(f"Autocovari√¢ncia no lag 2 (X_t): {gamma_2_x}")
> ```

**Proposi√ß√£o 1:** Se $h(L)$ √© um filtro sim√©trico, isto √©, $h_j = h_{-j}$ para todo $j$, ent√£o $h(z) = h(z^{-1})$.

**Prova:** Se $h(L)$ √© sim√©trico, ent√£o $h(z) = \sum_{j=-\infty}^{\infty} h_j z^j = h_0 + \sum_{j=1}^{\infty} h_j (z^j + z^{-j})$. Substituindo $z$ por $z^{-1}$, obtemos $h(z^{-1}) = \sum_{j=-\infty}^{\infty} h_j z^{-j} = h_0 + \sum_{j=1}^{\infty} h_j (z^{-j} + z^j) = h(z)$. $\blacksquare$

*Prova detalhada da Proposi√ß√£o 1:*

Provaremos que se $h(L)$ √© um filtro sim√©trico, i.e., $h_j = h_{-j}$ para todo $j$, ent√£o $h(z) = h(z^{-1})$.

I. Seja $h(L) = \sum_{j=-\infty}^{\infty} h_j L^j$ um filtro sim√©trico, o que significa que $h_j = h_{-j}$ para todo $j$.

II. Definimos a transformada *z* de $h(L)$ como $h(z) = \sum_{j=-\infty}^{\infty} h_j z^j$.

III. Substitu√≠mos $z$ por $z^{-1}$ na express√£o de $h(z)$:
     $$h(z^{-1}) = \sum_{j=-\infty}^{\infty} h_j z^{-j}$$

IV. Fazemos uma mudan√ßa de vari√°vel $k = -j$, ent√£o $j = -k$ e a soma se torna:
    $$h(z^{-1}) = \sum_{k=\infty}^{-\infty} h_{-k} z^{k} = \sum_{k=-\infty}^{\infty} h_{-k} z^{k}$$

V. Como $h(L)$ √© sim√©trico, temos $h_{-k} = h_k$ para todo $k$. Substitu√≠mos $h_{-k}$ por $h_k$ na soma:
     $$h(z^{-1}) = \sum_{k=-\infty}^{\infty} h_{k} z^{k}$$

VI. A express√£o acima √© exatamente a defini√ß√£o de $h(z)$:
      $$h(z^{-1}) = h(z)$$

Portanto, se $h(L)$ √© um filtro sim√©trico, ent√£o $h(z) = h(z^{-1})$. $\blacksquare$

**Corol√°rio 1:** Se $h(L)$ √© um filtro sim√©trico, ent√£o $g_X(z) = h(z)^2 g_Y(z)$.

**Prova:** Imediato da Proposi√ß√£o 1 e da equa√ß√£o $g_X(z) = h(z) h(z^{-1}) g_Y(z)$. $\blacksquare$

Para complementar a discuss√£o sobre filtros sim√©tricos, considere agora o caso de filtros anti-sim√©tricos.

**Proposi√ß√£o 1.1:** Se $h(L)$ √© um filtro anti-sim√©trico, isto √©, $h_j = -h_{-j}$ para todo $j$, ent√£o $h(z) = -h(z^{-1})$.

**Prova:** Se $h(L)$ √© anti-sim√©trico, ent√£o $h(z) = \sum_{j=-\infty}^{\infty} h_j z^j = \sum_{j=1}^{\infty} h_j (z^j - z^{-j})$. Substituindo $z$ por $z^{-1}$, obtemos $h(z^{-1}) = \sum_{j=-\infty}^{\infty} h_j z^{-j} = \sum_{j=1}^{\infty} h_j (z^{-j} - z^j) = -h(z)$. $\blacksquare$

*Prova detalhada da Proposi√ß√£o 1.1:*

Provaremos que se $h(L)$ √© um filtro anti-sim√©trico, i.e., $h_j = -h_{-j}$ para todo $j$, ent√£o $h(z) = -h(z^{-1})$.

I. Seja $h(L) = \sum_{j=-\infty}^{\infty} h_j L^j$ um filtro anti-sim√©trico, o que significa que $h_j = -h_{-j}$ para todo $j$.

II. Definimos a transformada *z* de $h(L)$ como $h(z) = \sum_{j=-\infty}^{\infty} h_j z^j$.

III. Substitu√≠mos $z$ por $z^{-1}$ na express√£o de $h(z)$:
     $$h(z^{-1}) = \sum_{j=-\infty}^{\infty} h_j z^{-j}$$

IV. Fazemos uma mudan√ßa de vari√°vel $k = -j$, ent√£o $j = -k$ e a soma se torna:
    $$h(z^{-1}) = \sum_{k=\infty}^{-\infty} h_{-k} z^{k} = \sum_{k=-\infty}^{\infty} h_{-k} z^{k}$$

V. Como $h(L)$ √© anti-sim√©trico, temos $h_{-k} = -h_k$ para todo $k$. Substitu√≠mos $h_{-k}$ por $-h_k$ na soma:
     $$h(z^{-1}) = \sum_{k=-\infty}^{\infty} -h_{k} z^{k} = -\sum_{k=-\infty}^{\infty} h_{k} z^{k}$$

VI. A express√£o acima √© o negativo da defini√ß√£o de $h(z)$:
      $$h(z^{-1}) = -h(z)$$

Portanto, se $h(L)$ √© um filtro anti-sim√©trico, ent√£o $h(z) = -h(z^{-1})$. $\blacksquare$

**Corol√°rio 1.1:** Se $h(L)$ √© um filtro anti-sim√©trico, ent√£o $g_X(z) = -h(z)^2 g_Y(z)$. Mais precisamente, $g_X(z) = -h(z)h(z^{-1})g_Y(z)= -(-h(z))h(z)g_Y(z) = h(z)^2 g_Y(z)$.

**Prova:** Imediato da Proposi√ß√£o 1.1 e da equa√ß√£o $g_X(z) = h(z) h(z^{-1}) g_Y(z)$. $\blacksquare$

### Espectro Populacional de Processos Filtrados

O espectro populacional do processo filtrado, $s_X(\omega)$, est√° relacionado ao espectro populacional do processo original, $s_Y(\omega)$, da seguinte forma:

$$s_X(\omega) = |h(e^{-i\omega})|^2 s_Y(\omega)$$

Esta equa√ß√£o mostra que o espectro do processo filtrado √© obtido multiplicando o espectro do processo original pelo **m√≥dulo ao quadrado da fun√ß√£o de transfer√™ncia do filtro**, avaliada na frequ√™ncia $\omega$. A fun√ß√£o de transfer√™ncia $h(e^{-i\omega})$ descreve como o filtro modifica as amplitudes e fases das diferentes frequ√™ncias presentes no sinal original.

> üí° **Exemplo Num√©rico:**
>
> Suponha que o espectro do processo original $Y_t$ √© $s_Y(\omega) = \frac{1}{2\pi}$ (ru√≠do branco). Agora, aplicamos um filtro de m√©dia m√≥vel de dois per√≠odos: $h(L) = 0.5 + 0.5L$. Para encontrar o espectro do processo filtrado, precisamos calcular $|h(e^{-i\omega})|^2$.
>
> $h(e^{-i\omega}) = 0.5 + 0.5e^{-i\omega} = 0.5(1 + e^{-i\omega})$
>
> $|h(e^{-i\omega})|^2 = |0.5(1 + e^{-i\omega})|^2 = 0.25 |1 + \cos(\omega) - i\sin(\omega)|^2 = 0.25 [(1 + \cos(\omega))^2 + \sin^2(\omega)]$
>
> $|h(e^{-i\omega})|^2 = 0.25 [1 + 2\cos(\omega) + \cos^2(\omega) + \sin^2(\omega)] = 0.25 [2 + 2\cos(\omega)] = 0.5(1 + \cos(\omega))$
>
> Ent√£o, o espectro do processo filtrado $X_t$ √©:
>
> $s_X(\omega) = |h(e^{-i\omega})|^2 s_Y(\omega) = 0.5(1 + \cos(\omega)) \frac{1}{2\pi} = \frac{1 + \cos(\omega)}{4\pi}$
>
> Este resultado mostra que o filtro de m√©dia m√≥vel suaviza o espectro, atenuando as altas frequ√™ncias e amplificando as baixas frequ√™ncias.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Definindo a fun√ß√£o de espectro para o ru√≠do branco
> def espectro_ruido_branco(omega):
>     return np.ones_like(omega) / (2 * np.pi)
>
> # Definindo a fun√ß√£o de transfer√™ncia do filtro de m√©dia m√≥vel
> def transferencia_media_movel(omega):
>     return 0.5 * (1 + np.exp(-1j * omega))
>
> # Definindo a fun√ß√£o de espectro para o processo filtrado
> def espectro_processo_filtrado(omega):
>     transferencia = transferencia_media_movel(omega)
>     return np.abs(transferencia)**2 * espectro_ruido_branco(omega)
>
> # Criando um array de frequ√™ncias
> omega = np.linspace(-np.pi, np.pi, 500)
>
> # Calculando os espectros
> espectro_ruido = espectro_ruido_branco(omega)
> espectro_filtrado = espectro_processo_filtrado(omega)
>
> # Plotando os espectros
> plt.figure(figsize=(10, 6))
> plt.plot(omega, espectro_ruido, label='Ru√≠do Branco')
> plt.plot(omega, espectro_filtrado, label='Processo Filtrado (M√©dia M√≥vel)')
> plt.title('Espectros do Ru√≠do Branco e do Processo Filtrado')
> plt.xlabel('Frequ√™ncia (œâ)')
> plt.ylabel('Densidade Espectral')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> O c√≥digo acima gera um gr√°fico comparando o espectro do ru√≠do branco original com o espectro do processo filtrado. O gr√°fico demonstra visualmente como o filtro de m√©dia m√≥vel atenua as altas frequ√™ncias, resultando em um espectro mais suave.

**Teorema 2:** Seja $Y_t$ um processo estacion√°rio com ACGF $g_Y(z)$ e espectro populacional $s_Y(\omega)$. Se aplicarmos um filtro $h(L)$ tal que $X_t = h(L)Y_t$, ent√£o o espectro populacional do processo filtrado $X_t$ √© dado por $s_X(\omega) = |h(e^{-i\omega})|^2 s_Y(\omega)$, onde $h(e^{-i\omega})$ √© a fun√ß√£o de transfer√™ncia do filtro avaliada na frequ√™ncia $\omega$.

**Prova:** A autocovari√¢ncia de $X_t$ no lag $j$ √© dada por:
$\gamma_j^X = E[X_t X_{t-j}] = E[h(L)Y_t h(L)Y_{t-j}]$. Usando a propriedade da ACGF, temos que $g_X(z) = h(z) h(z^{-1}) g_Y(z)$. O espectro populacional √© obtido avaliando a ACGF no c√≠rculo unit√°rio e dividindo por $2\pi$:

$s_X(\omega) = \frac{1}{2\pi} g_X(e^{-i\omega}) = \frac{1}{2\pi} h(e^{-i\omega}) h(e^{i\omega}) g_Y(e^{-i\omega})$.

Note que $h(e^{i\omega})$ √© o conjugado complexo de $h(e^{-i\omega})$, de modo que $h(e^{-i\omega}) h(e^{i\omega}) = |h(e^{-i\omega})|^2$. Portanto, $s_X(\omega) = \frac{1}{2\pi} |h(e^{-i\omega})|^2 g_Y(e^{-i\omega}) = |h(e^{-i\omega})|^2 s_Y(\omega)$.

I. Definimos $X_t = h(L) Y_t$, onde $h(L)$ √© um filtro linear.

II. Calculamos a autocovari√¢ncia de $X_t$ no lag $j$:
   $$\gamma_j^X = E[X_t X_{t-j}] = E[(h(L) Y_t)(h(L) Y_{t-j})]$$

III. Expressamos a ACGF de $X_t$:
    $$g_X(z) = \sum_{j=-\infty}^{\infty} \gamma_j^X z^j = \sum_{j=-\infty}^{\infty} E[X_t X_{t-j}] z^j$$

IV. Usamos a rela√ß√£o entre as ACGFs de $X_t$ e $Y_t$:
    $$g_X(z) = h(z) h(z^{-1}) g_Y(z)$$

V. Calculamos o espectro populacional $s_X(\omega)$ avaliando $g_X(z)$ no c√≠rculo unit√°rio $z = e^{-i\omega}$ e dividindo por $2\pi$:
    $$s_X(\omega) = \frac{1}{2\pi} g_X(e^{-i\omega}) = \frac{1}{2\pi} h(e^{-i\omega}) h(e^{i\omega}) g_Y(e^{-i\omega})$$

VI. Reconhecemos que $g_Y(e^{-i\omega}) = 2\pi s_Y(\omega)$ e que $h(e^{-i\omega}) h(e^{i\omega}) = |h(e^{-i\omega})|^2$ (j√° que $h(e^{i\omega})$ √© o conjugado complexo de $h(e^{-i\omega})$):
     $$s_X(\omega) = |h(e^{-i\omega})|^2 s_Y(\omega)$$

Portanto, o espectro populacional de $X_t$ √© $|h(e^{-i\omega})|^2 s_Y(\omega)$. $\blacksquare$

**Teorema 2.1:** Se $h(L)$ √© um filtro tal que $|h(e^{-i\omega})| = 1$ para todo $\omega$, ent√£o $s_X(\omega) = s_Y(\omega)$.

**Prova:** Imediato do Teorema 2. Se $|h(e^{-i\omega})| = 1$, ent√£o $|h(e^{-i\omega})|^2 = 1$, e portanto $s_X(\omega) = s_Y(\omega)$. Filtros com esta propriedade s√£o chamados de filtros "all-pass", pois eles n√£o alteram a magnitude do espectro, apenas a fase. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Um exemplo de filtro all-pass √© o filtro de fase $h(L) = -L$. Neste caso, $h(e^{-i\omega}) = -e^{-i\omega}$, e $|h(e^{-i\omega})| = |-e^{-i\omega}| = 1$. Portanto, o espectro do processo filtrado √© o mesmo que o espectro do processo original. Este filtro apenas inverte a s√©rie temporal.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Definindo a fun√ß√£o de espectro para um sinal qualquer (ex: senoide)
> def espectro_sinal(omega):
>     # Exemplo: senoide com frequ√™ncia 0.1
>     return np.abs(np.fft.fft(np.sin(2 * np.pi * 0.1 * np.arange(50))))[:len(omega)]
>
> # Definindo a fun√ß√£o de transfer√™ncia do filtro all-pass (h(L) = -L)
> def transferencia_all_pass(omega):
>     return -np.exp(-1j * omega)
>
> # Definindo a fun√ß√£o de espectro para o processo filtrado
> def espectro_processo_filtrado(omega, espectro_original):
>     transferencia = transferencia_all_pass(omega)
>     return np.abs(transferencia)**2 * espectro_original
>
> # Criando um array de frequ√™ncias
> omega = np.linspace(0, np.pi, 50) # Frequ√™ncias positivas apenas
>
> # Calculando o espectro original
> espectro_original = espectro_sinal(omega)
>
> # Calculando o espectro do processo filtrado
> espectro_filtrado = espectro_processo_filtrado(omega, espectro_original)
>
> # Plotando os espectros
> plt.figure(figsize=(10, 6))
> plt.plot(omega, espectro_original, label='Sinal Original')
> plt.plot(omega, espectro_filtrado, label='Processo Filtrado (All-Pass)')
> plt.title('Espectros do Sinal Original e do Processo Filtrado (All-Pass)')
> plt.xlabel('Frequ√™ncia (œâ)')
> plt.ylabel('Magnitude Espectral')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```

**Teorema 2.2:** Se $Y_t$ e $Z_t$ s√£o processos independentes com espectros $s_Y(\omega)$ e $s_Z(\omega)$, respectivamente, e $X_t = Y_t + Z_t$, ent√£o $s_X(\omega) = s_Y(\omega) + s_Z(\omega)$.

**Prova:** Como $Y_t$ e $Z_t$ s√£o independentes, $E[Y_t Z_{t-j}] = 0$ para todo $j$. A autocovari√¢ncia de $X_t$ no lag $j$ √©:
$\gamma_j^X = E[X_t X_{t-j}] = E[(Y_t + Z_t)(Y_{t-j} + Z_{t-j})] = E[Y_t Y_{t-j}] + E[Z_t Z_{t-j}] + E[Y_t Z_{t-j}] + E[Z_t Y_{t-j}] = \gamma_j^Y + \gamma_j^Z$.
Portanto, $g_X(z) = g_Y(z) + g_Z(z)$. Avaliando no c√≠rculo unit√°rio e dividindo por $2\pi$, obtemos $s_X(\omega) = s_Y(\omega) + s_Z(\omega)$. $\blacksquare$

*Prova detalhada do Teorema 2.2:*

Provaremos que se $Y_t$ e $Z_t$ s√£o processos independentes com espectros $s_Y(\omega)$ e $s_Z(\omega)$, respectivamente, e $X_t = Y_t + Z_t$, ent√£o $s_X(\omega) = s_Y(\omega) + s_Z(\omega)$.

I. Definimos $X_t = Y_t + Z_t$, onde $Y_t$ e $Z_t$ s√£o processos independentes.

II. Calculamos a autocovari√¢ncia de $X_t$ no lag $j$:
   $$\gamma_j^X = E[X_t X_{t-j}] = E[(Y_t + Z_t)(Y_{t-j} + Z_{t-j})]$$

III. Expandimos a express√£o e usamos a linearidade do operador de esperan√ßa:
    $$\gamma_j^X = E[Y_t Y_{t-j} + Y_t Z_{t-j} + Z_t Y_{t-j} + Z_t Z_{t-j}] = E[Y_t Y_{t-j}] + E[Y_t Z_{t-j}] + E[Z_t Y_{t-j}] + E[Z_t Z_{t-j}]$$

IV. Como $Y_t$ e $Z_t$ s√£o independentes, $E[Y_t Z_{t-j}] = E[Y_t]E[Z_{t-j}] = 0$ e $E[Z_t Y_{t-j}] = E[Z_t]E[Y_{t-j}] = 0$. Portanto:
    $$\gamma_j^X = E[Y_t Y_{t-j}] + E[Z_t Z_{t-j}] = \gamma_j^Y + \gamma_j^Z$$

V. A ACGF de $X_t$ √© a transformada *z* de sua autocovari√¢ncia:
   $$g_X(z) = \sum_{j=-\infty}^{\infty} \gamma_j^X z^j = \sum_{j=-\infty}^{\infty} (\gamma_j^Y + \gamma_j^Z) z^j = \sum_{j=-\infty}^{\infty} \gamma_j^Y z^j + \sum_{j=-\infty}^{\infty} \gamma_j^Z z^j = g_Y(z) + g_Z(z)$$

VI. O espectro populacional √© obtido avaliando a ACGF no c√≠rculo unit√°rio $z = e^{-i\omega}$ e dividindo por $2\pi$:
     $$s_X(\omega) = \frac{1}{2\pi} g_X(e^{-i\omega}) = \frac{1}{2\pi} (g_Y(e^{-i\omega}) + g_Z(e^{-i\omega})) = \frac{1}{2\pi} g_Y(e^{-i\omega}) + \frac{1}{2\pi} g_Z(e^{-i\omega}) = s_Y(\omega) + s_Z(\omega)$$

Portanto, o espectro populacional de $X_t$ √© $s_X(\omega) = s_Y(\omega) + s_Z(\omega)$. $\blacksquare$

### Inversibilidade e Filtragem

A **invertibilidade** de um processo MA(q) est√° relacionada √† capacidade de expressar o processo em termos de suas inova√ß√µes passadas [^65]. Se o processo √© n√£o invert√≠vel, as inova√ß√µes futuras s√£o necess√°rias para representar o processo, o que dificulta a an√°lise e previs√£o [^65].

A aplica√ß√£o de um filtro pode afetar a invertibilidade de um processo MA(q) [^65]. Em particular, se o filtro introduzir ra√≠zes no c√≠rculo unit√°rio, o processo filtrado pode se tornar n√£o invert√≠vel.

> üí° **Exemplo Num√©rico:**
>
> Considere um processo MA(1) $Y_t = \varepsilon_t + 0.8 \varepsilon_{t-1}$, onde $\varepsilon_t$ √© ru√≠do branco com vari√¢ncia $\sigma^2 = 1$. Como $|0.8| < 1$, este processo √© invert√≠vel. Suponha que aplicamos um filtro $h(L) = 1 + 2L$.
>
> O processo filtrado √© $X_t = (1 + 2L)Y_t = (1 + 2L)(\varepsilon_t + 0.8 \varepsilon_{t-1}) = \varepsilon_t + 2.8\varepsilon_{t-1} + 1.6\varepsilon_{t-2}$. Este √© um processo MA(2): $X_t = \varepsilon_t' + 2.8\varepsilon_{t-1}' + 1.6\varepsilon_{t-2}'$.
>
> Para verificar a invertibilidade, precisamos encontrar as ra√≠zes do polin√¥mio $1 + 2.8z + 1.6z^2 = 0$. Usando a f√≥rmula quadr√°tica:
>
> $z = \frac{-2.8 \pm \sqrt{2.8^2 - 4(1.6)}}{2(1.6)} = \frac{-2.8 \pm \sqrt{7.84 - 6.4}}{3.2} = \frac{-2.8 \pm \sqrt{1.44}}{3.2} = \frac{-2.8 \pm 1.2}{3.2}$
>
> $z_1 = \frac{-2.8 + 1.2}{3.2} = \frac{-1.6}{3.2} = -0.5$
>
> $z_2 = \frac{-2.8 - 1.2}{3.2} = \frac{-4}{3.2} = -1.25$
>
> Como $|z_1| = 0.5 < 1$, uma das ra√≠zes est√° dentro do c√≠rculo unit√°rio. Portanto, o processo filtrado n√£o √© invert√≠vel. Isso demonstra como um filtro pode tornar um processo invert√≠vel em um processo n√£o invert√≠vel.
>
> ```python
> import numpy as np
>
> # Coeficientes do processo MA(2) resultante
> coef = [1, 2.8, 1.6]
>
> # Encontrando as ra√≠zes do polin√¥mio
> roots = np.roots(coef)
>
> print(f"Ra√≠zes do polin√¥mio: {roots}")
>
> # Verificando se as ra√≠zes est√£o dentro do c√≠rculo unit√°rio
> for i, root in enumerate(roots):
>     if np.abs(root) < 1:
>         print(f"Raiz {i+1} = {root} est√° dentro do c√≠rculo unit√°rio.")
>     else:
>         print(f"Raiz {i+1} = {root} est√° fora do c√≠rculo unit√°rio.")
> ```

**Lema 1:** Se $Y_t$ √© um processo MA(q) invert√≠vel e $h(L)$ √© um filtro tal que as ra√≠zes de $h(z)$ est√£o fora do c√≠rculo unit√°rio, ent√£o o processo filtrado $X_t = h(L)Y_t$ √© invert√≠vel.

**Prova:** Se $Y_t$ √© invert√≠vel, suas ra√≠zes est√£o fora do c√≠rculo unit√°rio. Se as ra√≠zes de $h(z)$ tamb√©m est√£o fora do c√≠rculo unit√°rio, ent√£o as ra√≠zes do processo filtrado, dadas pelo produto $h(z)g_Y(z)$, tamb√©m estar√£o fora do c√≠rculo unit√°rio. Portanto, o processo filtrado √© invert√≠vel. $\blacksquare$

**Lema 1.1:** Se $Y_t$ √© um processo MA(q) e $h(L)$ √© um filtro, ent√£o $X_t = h(L)Y_t$ √© um processo MA(q+k), onde k √© a ordem do filtro $h(L)$.

**Prova:** Seja $Y_t = \epsilon_t + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q}$. Seja $h(L) = h_0 + h_1 L + \dots + h_k L^k$. Ent√£o
$X_t = h(L) Y_t = (h_0 + h_1 L + \dots + h_k L^k)(\epsilon_t + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q}) = h_0 \epsilon_t + (h_1 + h_0 \theta_1) \epsilon_{t-1} + \dots + h_k \theta_q \epsilon_{t-q-k}$. Portanto, $X_t$ √© um processo MA de ordem $q+k$. $\blacksquare$

*Prova detalhada do Lema 1.1:*

Provaremos que se $Y_t$ √© um processo MA(q) e $h(L)$ √© um filtro, ent√£o $X_t = h(L)Y_t$ √© um processo MA(q+k), onde k √© a ordem do filtro $h(L)$.

I. Seja $Y_t$ um processo MA(q), que pode ser representado como:
   $$Y_t = \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + ... + \theta_q \epsilon_{t-q}$$

   Onde $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia constante $\sigma^2$. Agora, suponha que aplicamos um filtro linear $h(L)$ a $Y_t$, tal que:

   $$X_t = h(L)Y_t = v_0 Y_t + v_1 Y_{t-1} + v_2 Y_{t-2} + ... + v_k Y_{t-k}$$

   Substituindo $Y_t$ na equa√ß√£o acima, obtemos:

   $$X_t = v_0 (\epsilon_t + \theta_1 \epsilon_{t-1} + ... + \theta_q \epsilon_{t-q}) + v_1 (\epsilon_{t-1} + \theta_1 \epsilon_{t-2} + ... + \theta_q \epsilon_{t-q-1}) + ... + v_k (\epsilon_{t-k} + \theta_1 \epsilon_{t-k-1} + ... + \theta_q \epsilon_{t-k-q})$$

   Rearranjando os termos, podemos escrever $X_t$ como uma combina√ß√£o linear de $\epsilon_t, \epsilon_{t-1}, ..., \epsilon_{t-q-k}$. Portanto, $X_t$ √© um processo MA de ordem q+k.

### Modelos ARMA sazonais

Modelos ARMA sazonais s√£o usados para s√©ries temporais que exibem padr√µes sazonais. A componente sazonal √© modelada separadamente da componente n√£o sazonal. Um modelo ARMA sazonal √© denotado como ARMA(p, q)(P, Q)s, onde p e q s√£o as ordens das partes AR e MA n√£o sazonais, P e Q s√£o as ordens das partes AR e MA sazonais, e s √© o per√≠odo da sazonalidade.

A equa√ß√£o geral para um modelo ARMA(p, q)(P, Q)s √©:

$$\phi(L)\Phi(L^s)Y_t = \theta(L)\Theta(L^s)\epsilon_t$$

Onde:
- $\phi(L) = 1 - \phi_1 L - \phi_2 L^2 - ... - \phi_p L^p$ √© o polin√¥mio AR n√£o sazonal.
- $\Phi(L^s) = 1 - \Phi_1 L^s - \Phi_2 L^{2s} - ... - \Phi_P L^{Ps}$ √© o polin√¥mio AR sazonal.
- $\theta(L) = 1 + \theta_1 L + \theta_2 L^2 + ... + \theta_q L^q$ √© o polin√¥mio MA n√£o sazonal.
- $\Theta(L^s) = 1 + \Theta_1 L^s + \Theta_2 L^{2s} + ... + \Theta_Q L^{Qs}$ √© o polin√¥mio MA sazonal.

#### Exemplo

Considere um modelo ARMA(1, 1)(1, 1)$_{12}$ para dados mensais. A equa√ß√£o do modelo seria:

$$(1 - \phi_1 L)(1 - \Phi_1 L^{12})Y_t = (1 + \theta_1 L)(1 + \Theta_1 L^{12})\epsilon_t$$

Expandindo a equa√ß√£o, obtemos:

$$Y_t - \phi_1 Y_{t-1} - \Phi_1 Y_{t-12} + \phi_1 \Phi_1 Y_{t-13} = \epsilon_t + \theta_1 \epsilon_{t-1} + \Theta_1 \epsilon_{t-12} + \theta_1 \Theta_1 \epsilon_{t-13}$$

### Teste de estacionariedade

Para garantir que um modelo ARMA seja apropriado, a s√©rie temporal deve ser estacion√°ria. Um dos testes mais comuns para estacionariedade √© o teste de Dickey-Fuller Aumentado (ADF).

O teste ADF testa a hip√≥tese nula de que a s√©rie temporal tem uma raiz unit√°ria, o que implica que ela n√£o √© estacion√°ria. A hip√≥tese alternativa √© que a s√©rie temporal √© estacion√°ria.

O teste ADF envolve a regress√£o da primeira diferen√ßa da s√©rie temporal em seus valores defasados e, opcionalmente, uma constante e uma tend√™ncia. A estat√≠stica de teste √© calculada como:

$$t = \frac{\hat{\rho}}{SE(\hat{\rho})}$$

Onde $\hat{\rho}$ √© a estimativa do coeficiente do valor defasado da s√©rie temporal e $SE(\hat{\rho})$ √© o erro padr√£o da estimativa.

Se o valor-p do teste for menor que um n√≠vel de signific√¢ncia escolhido (por exemplo, 0,05), rejeitamos a hip√≥tese nula e conclu√≠mos que a s√©rie temporal √© estacion√°ria. Caso contr√°rio, n√£o rejeitamos a hip√≥tese nula e conclu√≠mos que a s√©rie temporal n√£o √© estacion√°ria.

Para tornar uma s√©rie temporal n√£o estacion√°ria estacion√°ria, podemos aplicar diferencia√ß√£o. A diferencia√ß√£o envolve calcular a diferen√ßa entre valores consecutivos na s√©rie temporal. Se a s√©rie temporal ainda n√£o for estacion√°ria ap√≥s a primeira diferencia√ß√£o, podemos aplicar diferencia√ß√£o de ordem superior.

### Autocorrela√ß√£o e Autocorrela√ß√£o Parcial

As fun√ß√µes de autocorrela√ß√£o (ACF) e autocorrela√ß√£o parcial (PACF) s√£o ferramentas importantes para identificar a ordem de um modelo ARMA.

A ACF mede a correla√ß√£o entre uma s√©rie temporal e suas defasagens. O gr√°fico da ACF mostra a correla√ß√£o em diferentes defasagens.

A PACF mede a correla√ß√£o entre uma s√©rie temporal e suas defasagens, controlando a correla√ß√£o nas defasagens intermedi√°rias. O gr√°fico da PACF mostra a correla√ß√£o parcial em diferentes defasagens.

Para um modelo AR(p), a PACF ter√° um corte acentuado ap√≥s a defasagem p, enquanto a ACF diminuir√° exponencialmente. Para um modelo MA(q), a ACF ter√° um corte acentuado ap√≥s a defasagem q, enquanto a PACF diminuir√° exponencialmente.

### Estima√ß√£o de par√¢metros

Os par√¢metros de um modelo ARMA podem ser estimados usando v√°rios m√©todos, incluindo o m√©todo dos m√≠nimos quadrados, o m√©todo da m√°xima verossimilhan√ßa e o m√©todo de Yule-Walker.

O m√©todo dos m√≠nimos quadrados envolve minimizar a soma dos quadrados dos res√≠duos. O m√©todo da m√°xima verossimilhan√ßa envolve maximizar a fun√ß√£o de verossimilhan√ßa dos dados. O m√©todo de Yule-Walker envolve resolver um sistema de equa√ß√µes com base nas fun√ß√µes de autocorrela√ß√£o dos dados.

### Diagn√≥stico do modelo

Depois que um modelo ARMA √© estimado, √© importante diagnosticar sua adequa√ß√£o. Isso pode ser feito examinando os res√≠duos do modelo.

Os res√≠duos devem ser ru√≠do branco, o que significa que eles devem ter m√©dia zero, vari√¢ncia constante e n√£o devem ser autocorrelacionados. Podemos testar a aus√™ncia de autocorrela√ß√£o nos res√≠duos usando o teste de Ljung-Box.

O teste de Ljung-Box testa a hip√≥tese nula de que os res√≠duos n√£o s√£o autocorrelacionados. A estat√≠stica de teste √© calculada como:

$$Q = n(n+2)\sum_{k=1}^{h} \frac{\hat{\rho}_k^2}{n-k}$$

Onde n √© o tamanho da amostra, $\hat{\rho}_k$ √© a autocorrela√ß√£o amostral na defasagem k e h √© o n√∫mero de defasagens consideradas.

Se o valor-p do teste for menor que um n√≠vel de signific√¢ncia escolhido, rejeitamos a hip√≥tese nula e conclu√≠mos que os res√≠duos s√£o autocorrelacionados. Caso contr√°rio, n√£o rejeitamos a hip√≥tese nula e conclu√≠mos que os res√≠duos n√£o s√£o autocorrelacionados.

### Previs√£o

Uma vez que um modelo ARMA foi estimado e diagnosticado, ele pode ser usado para prever valores futuros da s√©rie temporal. A previs√£o envolve o uso dos valores passados da s√©rie temporal e dos par√¢metros estimados do modelo para prever valores futuros.

A previs√£o pode ser feita recursivamente, o que significa que o valor previsto para o pr√≥ximo per√≠odo √© usado para prever o valor para o per√≠odo seguinte e assim por diante.

### Exemplo pr√°tico

Vamos considerar um exemplo pr√°tico de como construir um modelo ARMA usando dados do mundo real. Suponha que temos dados mensais de vendas de uma empresa.

1.  **Visualiza√ß√£o dos dados:** Primeiro, visualizamos os dados para identificar quaisquer padr√µes ou tend√™ncias. Podemos observar que os dados exibem um padr√£o sazonal, com vendas mais altas nos meses de ver√£o.
2.  **Teste de estacionariedade:** Em seguida, testamos a estacionariedade dos dados usando o teste ADF. Se os dados n√£o forem estacion√°rios, aplicaremos diferencia√ß√£o at√© que eles se tornem estacion√°rios.
3.  **Identifica√ß√£o do modelo:** Usamos as fun√ß√µes ACF e PACF para identificar a ordem do modelo ARMA. Com base nos gr√°ficos ACF e PACF, podemos determinar que um modelo ARMA(1, 1)(1, 1)$_{12}$ √© apropriado.
4.  **Estimativa de par√¢metros:** Estimamos os par√¢metros do modelo usando o m√©todo da m√°xima verossimilhan√ßa.
5.  **Diagn√≥stico do modelo:** Diagnosticamos a adequa√ß√£o do modelo examinando os res√≠duos. Usamos o teste de Ljung-Box para testar a aus√™ncia de autocorrela√ß√£o nos res√≠duos.
6.  **Previs√£o:** Finalmente, usamos o modelo estimado para prever valores futuros das vendas.

### Desafios e limita√ß√µes

Embora os modelos ARMA sejam ferramentas poderosas para modelar s√©ries temporais, eles t√™m algumas limita√ß√µes. Um desafio √© que os modelos ARMA exigem que a s√©rie temporal seja estacion√°ria. Se a s√©rie temporal n√£o for estacion√°ria, ela deve ser transformada antes que um modelo ARMA possa ser aplicado.

Outra limita√ß√£o √© que os modelos ARMA s√£o modelos lineares. Se a s√©rie temporal exibir n√£o linearidades, um modelo ARMA pode n√£o ser apropriado.

Al√©m disso, a sele√ß√£o da ordem apropriada para um modelo ARMA pode ser desafiadora. As fun√ß√µes ACF e PACF podem fornecer pistas, mas pode ser necess√°rio experimentar diferentes ordens de modelo para encontrar o melhor ajuste.

### Conclus√£o

Os modelos ARMA s√£o uma ferramenta vers√°til para modelar e prever s√©ries temporais. Eles s√£o baseados nos conceitos de autocorrela√ß√£o e m√©dias m√≥veis e podem capturar uma ampla gama de padr√µes em dados de s√©ries temporais.

Para construir um modelo ARMA, √© importante primeiro garantir que a s√©rie temporal seja estacion√°ria. Em seguida, as fun√ß√µes ACF e PACF podem ser usadas para identificar a ordem do modelo. Os par√¢metros do modelo podem ser estimados usando v√°rios m√©todos, e a adequa√ß√£o do modelo pode ser diagnosticada examinando os res√≠duos.

Embora os modelos ARMA tenham algumas limita√ß√µes, eles podem ser uma ferramenta valiosa para analisar e prever dados de s√©ries temporais. <!-- END -->
