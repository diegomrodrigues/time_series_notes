## Autocovariance and its Computation

### Introdu√ß√£o
Este cap√≠tulo se concentra na **autocovari√¢ncia** (*autocovariance*), um conceito fundamental na an√°lise de s√©ries temporais, que mede a depend√™ncia linear entre os valores de uma s√©rie temporal em diferentes pontos no tempo [^45, ^49]. A autocovari√¢ncia, denotada por $\gamma_j$, quantifica a covari√¢ncia entre $Y_t$ e seu valor defasado $Y_{t-j}$ [^45]. Este cap√≠tulo explora a defini√ß√£o matem√°tica de autocovari√¢ncia, suas propriedades e como ela √© usada para entender a estrutura de depend√™ncia temporal em dados de s√©ries temporais.

### Conceitos Fundamentais

A **autocovari√¢ncia**, $\gamma_j$, entre as observa√ß√µes $Y_t$ e $Y_{t-j}$ de uma s√©rie temporal √© definida como [^45]:
$$
\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]
$$
onde $\mu$ √© a m√©dia da s√©rie temporal [^45]. Esta f√≥rmula calcula o valor esperado do produto das desvia√ß√µes de $Y_t$ e $Y_{t-j}$ de sua m√©dia, medindo assim o grau em que $Y_{t-j}$ influencia $Y_t$ [^45]. Note que $\gamma_0$ √© simplesmente a vari√¢ncia de $Y_t$ [^45].

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal simples com os seguintes valores: $Y_1 = 2, Y_2 = 4, Y_3 = 6, Y_4 = 8, Y_5 = 10$. A m√©dia $\mu = \frac{2+4+6+8+10}{5} = 6$. Para calcular $\gamma_1$, precisamos calcular $E[(Y_t - \mu)(Y_{t-1} - \mu)]$. Isso significa calcular a m√©dia dos produtos das desvia√ß√µes defasadas em 1 unidade de tempo.
>  
> $\gamma_1 = \frac{(4-6)(2-6) + (6-6)(4-6) + (8-6)(6-6) + (10-6)(8-6)}{4} = \frac{(-2)(-4) + (0)(-2) + (2)(0) + (4)(2)}{4} = \frac{8 + 0 + 0 + 8}{4} = \frac{16}{4} = 4$

*Propriedades da Autocovari√¢ncia:*
1.  **Simetria**: Para um processo estacion√°rio em covari√¢ncia (*covariance-stationary*), a autocovari√¢ncia √© uma fun√ß√£o par, o que significa que $\gamma_j = \gamma_{-j}$ [^45]. Isso implica que a covari√¢ncia entre $Y_t$ e $Y_{t-j}$ √© a mesma que entre $Y_{t-j}$ e $Y_t$ [^45]. Matematicamente, isso segue de:
    $$
    \gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(Y_{t-j} - \mu)(Y_t - \mu)] = \gamma_{-j}
    $$

    **Prova da Simetria:**
    I.  Definimos a autocovari√¢ncia como $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$.

    II. Para demonstrar a simetria, precisamos mostrar que $\gamma_j = \gamma_{-j}$.
    Invertendo o lag, temos $\gamma_{-j} = E[(Y_t - \mu)(Y_{t+j} - \mu)]$.

    III. Seja $k = t-j$, ent√£o $t = k+j$. Substituindo em $\gamma_j$, temos $\gamma_j = E[(Y_{k+j} - \mu)(Y_{k} - \mu)]$.

    IV. Seja $t = k$ no lag $-j$, ent√£o $\gamma_{-j} = E[(Y_t - \mu)(Y_{t+j} - \mu)] = E[(Y_t - \mu)(Y_{t+j} - \mu)] = E[(Y_{k} - \mu)(Y_{k+j} - \mu)]$.

    V. Pela propriedade comutativa da multiplica√ß√£o, $E[(Y_{k+j} - \mu)(Y_{k} - \mu)] = E[(Y_{k} - \mu)(Y_{k+j} - \mu)]$.

    VI. Portanto, $\gamma_j = \gamma_{-j}$. ‚ñ†

2.  **Valor M√°ximo em Zero Lag**: A autocovari√¢ncia √© tipicamente maior em lag zero ($\gamma_0$), que √© a vari√¢ncia da s√©rie temporal. √Ä medida que o lag aumenta ($j$ cresce), a autocovari√¢ncia geralmente diminui, indicando que a depend√™ncia entre as observa√ß√µes diminui com o aumento da separa√ß√£o no tempo.

> üí° **Exemplo Num√©rico:** Usando a mesma s√©rie temporal do exemplo anterior ($Y_1 = 2, Y_2 = 4, Y_3 = 6, Y_4 = 8, Y_5 = 10$ e $\mu = 6$), podemos calcular $\gamma_0$ como a vari√¢ncia da s√©rie.
>
> $\gamma_0 = \frac{(2-6)^2 + (4-6)^2 + (6-6)^2 + (8-6)^2 + (10-6)^2}{5} = \frac{16 + 4 + 0 + 4 + 16}{5} = \frac{40}{5} = 8$
>
> Observe que $\gamma_0$ √© maior que $\gamma_1 = 4$, conforme esperado.

*Autocorrela√ß√£o:*
A **autocorrela√ß√£o** (*autocorrelation*) $\rho_j$ √© a autocovari√¢ncia normalizada pela vari√¢ncia [^49]:
$$
\rho_j = \frac{\gamma_j}{\gamma_0}
$$
A autocorrela√ß√£o varia entre -1 e 1, fornecendo uma medida adimensional da for√ßa e dire√ß√£o da depend√™ncia linear entre $Y_t$ e $Y_{t-j}$ [^49]. A fun√ß√£o de autocorrela√ß√£o (ACF) (*autocorrelation function*) plota $\rho_j$ em fun√ß√£o de $j$ e √© uma ferramenta fundamental para identificar padr√µes de depend√™ncia temporal [^49].

> üí° **Exemplo Num√©rico:** Usando os valores calculados anteriormente, $\gamma_1 = 4$ e $\gamma_0 = 8$, podemos calcular a autocorrela√ß√£o no lag 1:
>
> $\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{4}{8} = 0.5$
>
> Isso indica uma correla√ß√£o positiva moderada entre os valores da s√©rie temporal defasados em um per√≠odo.

**Proposi√ß√£o 1.** A autocorrela√ß√£o $\rho_j$ sempre satisfaz $|\rho_j| \leq 1$.

*Prova.* Pela desigualdade de Cauchy-Schwarz, $|E[(Y_t - \mu)(Y_{t-j} - \mu)]| \leq \sqrt{E[(Y_t - \mu)^2]E[(Y_{t-j} - \mu)^2]}$. Como $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$, $\gamma_0 = E[(Y_t - \mu)^2] = E[(Y_{t-j} - \mu)^2]$, temos $|\gamma_j| \leq \gamma_0$. Dividindo ambos os lados por $\gamma_0$ resulta em $|\frac{\gamma_j}{\gamma_0}| \leq 1$, ou seja, $|\rho_j| \leq 1$.

**Proposi√ß√£o 1.1.** Se $Y_t = c$ para todo $t$, onde $c$ √© uma constante, ent√£o $\rho_j = 1$ para todo $j$.

*Prova.* Se $Y_t = c$ para todo $t$, ent√£o $\mu = c$. Portanto, $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(c - c)(c - c)] = E[0] = 0$ para $j \neq 0$. Para $j = 0$, $\gamma_0 = E[(Y_t - \mu)^2] = E[(c-c)^2] = 0$. No entanto, a autocorrela√ß√£o √© definida como o limite quando $\gamma_0$ tende a zero, ou ent√£o, podemos considerar pequenas perturba√ß√µes em torno de $c$.

Assumindo que temos uma pequena varia√ß√£o em torno de $c$, ent√£o $\gamma_0 > 0$ e $\gamma_j$ tender√° a $\gamma_0$ √† medida que a varia√ß√£o diminui. Assim, $\rho_j = \frac{\gamma_j}{\gamma_0} = 1$.

*Exemplo de C√°lculo:*
Considere o processo MA(1) (Moving Average de primeira ordem) [^48]:
$$
Y_t = \mu + \varepsilon_t + \theta \varepsilon_{t-1}
$$
onde $\{\varepsilon_t\}$ √© ru√≠do branco (*white noise*) com m√©dia zero e vari√¢ncia $\sigma^2$ [^48]. A autocovari√¢ncia para este processo √© calculada como segue [^48]:
Para $j = 0$ [^48]:
$$
\gamma_0 = E[(Y_t - \mu)^2] = E[(\varepsilon_t + \theta \varepsilon_{t-1})^2] = \sigma^2 + \theta^2 \sigma^2 = (1 + \theta^2)\sigma^2
$$
Para $j = 1$ [^48]:
$$
\gamma_1 = E[(Y_t - \mu)(Y_{t-1} - \mu)] = E[(\varepsilon_t + \theta \varepsilon_{t-1})(\varepsilon_{t-1} + \theta \varepsilon_{t-2})] = \theta \sigma^2
$$
Para $j > 1$ [^48]:
$$
\gamma_j = 0
$$
Assim, o processo MA(1) tem autocovari√¢ncia diferente de zero apenas para os lags 0 e 1 [^48].

> üí° **Exemplo Num√©rico:**  Assuma $\theta = 0.5$ e $\sigma^2 = 1$. Ent√£o, para um processo MA(1):
>
> $\gamma_0 = (1 + 0.5^2)(1) = 1.25$
> $\gamma_1 = (0.5)(1) = 0.5$
> $\gamma_j = 0$ para $j > 1$
>
> Isso significa que a vari√¢ncia da s√©rie temporal √© 1.25, a covari√¢ncia entre $Y_t$ e $Y_{t-1}$ √© 0.5, e a covari√¢ncia entre $Y_t$ e $Y_{t-j}$ √© 0 para $j > 1$.

**Exemplo de C√°lculo Estendido:** Vamos agora calcular a autocorrela√ß√£o para o processo MA(1). Usando os resultados da autocovari√¢ncia, temos:
Para $j=0$:
$$\rho_0 = \frac{\gamma_0}{\gamma_0} = 1$$
Para $j=1$:
$$\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{\theta \sigma^2}{(1 + \theta^2)\sigma^2} = \frac{\theta}{1 + \theta^2}$$
Para $j > 1$:
$$\rho_j = \frac{\gamma_j}{\gamma_0} = 0$$
Portanto, a fun√ß√£o de autocorrela√ß√£o para o processo MA(1) corta ap√≥s o lag 1.

> üí° **Exemplo Num√©rico:**  Continuando com o exemplo anterior onde $\theta = 0.5$ e $\sigma^2 = 1$:
>
> $\rho_0 = 1$
> $\rho_1 = \frac{0.5}{1 + 0.5^2} = \frac{0.5}{1.25} = 0.4$
> $\rho_j = 0$ para $j > 1$
>
> A fun√ß√£o de autocorrela√ß√£o (ACF) mostra uma correla√ß√£o de 1 no lag 0, uma correla√ß√£o de 0.4 no lag 1, e correla√ß√£o zero para lags maiores que 1.  Esta √© uma caracter√≠stica chave de um processo MA(1).

*Fun√ß√£o de Autocorrela√ß√£o Parcial (PACF) (*Partial Autocorrelation Function*)*:
A PACF mede a correla√ß√£o entre $Y_t$ e $Y_{t-j}$ ap√≥s remover os efeitos dos lags intermedi√°rios [^56]. Em outras palavras, ela mede a depend√™ncia direta entre $Y_t$ e $Y_{t-j}$ [^56]. A PACF √© particularmente √∫til para identificar a ordem de processos autorregressivos (AR) (*autoregressive processes*) [^53].

*Autocovari√¢ncia para Modelos AR(1) (*Autoregressive Models*)*:
Considere o modelo AR(1) [^53]:
$$
Y_t = c + \phi Y_{t-1} + \varepsilon_t
$$
onde $|\phi| < 1$ para garantir a estacionariedade [^53]. A autocovari√¢ncia para este modelo satisfaz a seguinte rela√ß√£o [^53]:
$$
\gamma_j = \phi \gamma_{j-1}
$$
com solu√ß√£o [^53]:
$$
\gamma_j = \phi^j \gamma_0
$$
onde $\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$ [^53].

> üí° **Exemplo Num√©rico:** Seja $\phi = 0.7$ e $\sigma^2 = 1$ para um modelo AR(1). Ent√£o:
>
> $\gamma_0 = \frac{1}{1 - 0.7^2} = \frac{1}{1 - 0.49} = \frac{1}{0.51} \approx 1.96$
> $\gamma_1 = 0.7 \cdot \gamma_0 = 0.7 \cdot 1.96 \approx 1.37$
> $\gamma_2 = 0.7 \cdot \gamma_1 = 0.7 \cdot 1.37 \approx 0.96$
>
> A autocovari√¢ncia decai exponencialmente, como esperado para um modelo AR(1).

**Teorema 1.** Para um modelo AR(1) estacion√°rio, a fun√ß√£o de autocorrela√ß√£o (ACF) decai exponencialmente.

*Proof.* A fun√ß√£o de autocorrela√ß√£o √© dada por $\rho_j = \frac{\gamma_j}{\gamma_0}$. Substituindo $\gamma_j = \phi^j \gamma_0$, obtemos $\rho_j = \frac{\phi^j \gamma_0}{\gamma_0} = \phi^j$. Como $|\phi| < 1$ para estacionariedade, $\rho_j$ decai exponencialmente com o aumento de $j$.

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior com $\phi = 0.7$:
>
> $\rho_0 = 1$
> $\rho_1 = 0.7^1 = 0.7$
> $\rho_2 = 0.7^2 = 0.49$
> $\rho_3 = 0.7^3 = 0.343$
>
> A ACF decai exponencialmente, convergindo para zero √† medida que o lag aumenta.

**Teorema 1.1.** Para um modelo AR(1) estacion√°rio, a fun√ß√£o de autocorrela√ß√£o parcial (PACF) corta ap√≥s o lag 1.

*Proof.* Para um modelo AR(1), a PACF mede a correla√ß√£o entre $Y_t$ e $Y_{t-j}$ ap√≥s remover os efeitos dos lags intermedi√°rios. No caso de $j=1$, a PACF √© simplesmente $\phi$. Para $j>1$, a correla√ß√£o entre $Y_t$ e $Y_{t-j}$ √© totalmente explicada pela depend√™ncia de $Y_t$ em $Y_{t-1}$. Portanto, ap√≥s remover o efeito de $Y_{t-1}$, a correla√ß√£o parcial entre $Y_t$ e $Y_{t-j}$ √© zero para $j > 1$.

> üí° **Exemplo Num√©rico:**  Em um modelo AR(1) com $\phi = 0.7$, a PACF ter√° um valor de 0.7 no lag 1 e ser√° essencialmente zero para todos os lags maiores que 1. Isso ajuda a distinguir um modelo AR(1) de outros modelos de s√©ries temporais.

### Conclus√£o
A autocovari√¢ncia √© uma ferramenta fundamental na an√°lise de s√©ries temporais, fornecendo insights sobre a depend√™ncia temporal dos dados [^45]. O c√°lculo e a interpreta√ß√£o da autocovari√¢ncia, juntamente com a autocorrela√ß√£o e a fun√ß√£o de autocorrela√ß√£o parcial, s√£o essenciais para a constru√ß√£o de modelos preditivos e a compreens√£o dos processos subjacentes que geram a s√©rie temporal [^45, ^49]. As propriedades da autocovari√¢ncia, como simetria e seu comportamento em diferentes lags, auxiliam na identifica√ß√£o da ordem e das caracter√≠sticas de modelos AR, MA e ARMA [^45, ^48, ^53].

### Refer√™ncias
[^45]: P√°ginas 44-45.
[^48]: P√°gina 48.
[^49]: P√°gina 49.
[^53]: P√°gina 53.
[^56]: P√°gina 56.
<!-- END -->