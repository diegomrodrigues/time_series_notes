## Autocovari√¢ncia para Processos com Autocovari√¢ncias Nulas Fora do Lag Zero

### Introdu√ß√£o
Em continuidade ao t√≥pico anterior sobre **Autocovari√¢ncia e sua Computa√ß√£o**, este cap√≠tulo se aprofunda em processos espec√≠ficos onde as autocovari√¢ncias s√£o nulas para todos os lags diferentes de zero ($j \neq 0$) [^45]. Esses processos, que incluem o ru√≠do branco gaussiano (*Gaussian white noise*), desempenham um papel crucial como blocos b√°sicos na modelagem de s√©ries temporais e em diversas aplica√ß√µes de processamento de sinais [^46, ^47]. A an√°lise desses processos simplifica significativamente os c√°lculos de autocovari√¢ncia, permitindo uma valida√ß√£o r√°pida das premissas de ru√≠do branco e facilitando c√°lculos subsequentes.

### Conceitos Fundamentais

Considere um processo estoc√°stico $\{Y_t\}$ definido como:
$$
Y_t = \mu + \varepsilon_t
$$
onde $\mu$ √© uma constante e $\{\varepsilon_t\}$ √© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$ [^45]. Em outras palavras, $E[\varepsilon_t] = 0$ e $E[\varepsilon_t \varepsilon_\tau] = 0$ para $t \neq \tau$ [^47].

Para este tipo de processo, a autocovari√¢ncia $\gamma_j$ √© calculada como [^45]:
$$
\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]
$$
Substituindo $Y_t$ e $Y_{t-j}$ pelas suas defini√ß√µes, temos:
$$
\gamma_j = E[(\varepsilon_t)(\varepsilon_{t-j})]
$$

*C√°lculo da Autocovari√¢ncia para diferentes lags:*
1.  **Para $j = 0$**:
    $$
    \gamma_0 = E[\varepsilon_t^2] = \sigma^2
    $$
    Portanto, a autocovari√¢ncia no lag zero √© igual √† vari√¢ncia do ru√≠do branco [^45].
2.  **Para $j \neq 0$**:
    $$
    \gamma_j = E[\varepsilon_t \varepsilon_{t-j}] = 0
    $$
    Isso ocorre porque as vari√°veis de ru√≠do branco em diferentes instantes de tempo s√£o n√£o correlacionadas [^47].

Dessa forma, para o processo $Y_t = \mu + \varepsilon_t$, a fun√ß√£o de autocovari√¢ncia √© dada por:
$$
\gamma_j =
\begin{cases}
    \sigma^2, & \text{se } j = 0 \\
    0, & \text{se } j \neq 0
\end{cases}
$$

> üí° **Exemplo Num√©rico:** Suponha que temos um processo $Y_t = 5 + \varepsilon_t$, onde $\varepsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2 = 2$. Ent√£o, a autocovari√¢ncia √©:
>
> $\gamma_0 = \sigma^2 = 2$
> $\gamma_j = 0$ para $j \neq 0$
>
> Isso significa que a vari√¢ncia do processo √© 2 e n√£o h√° correla√ß√£o entre os valores do processo em diferentes instantes de tempo.

**Proposi√ß√£o 2.** Para um processo $Y_t = \mu + \varepsilon_t$, a fun√ß√£o de autocorrela√ß√£o (ACF) √© 1 para $j=0$ e 0 para $j \neq 0$.

*Prova.* A fun√ß√£o de autocorrela√ß√£o √© definida como $\rho_j = \frac{\gamma_j}{\gamma_0}$.

*   Para $j=0$, $\rho_0 = \frac{\gamma_0}{\gamma_0} = \frac{\sigma^2}{\sigma^2} = 1$.
*   Para $j \neq 0$, $\rho_j = \frac{\gamma_j}{\gamma_0} = \frac{0}{\sigma^2} = 0$.

Assim, a ACF de um processo deste tipo √© um impulso unit√°rio no lag zero.

> üí° **Exemplo Num√©rico:**  Continuando com o exemplo anterior, onde $\sigma^2 = 2$:
>
> $\rho_0 = 1$
> $\rho_j = 0$ para $j \neq 0$
>
> A ACF mostra uma correla√ß√£o de 1 no lag 0 e correla√ß√£o zero para todos os outros lags, o que √© caracter√≠stico de um ru√≠do branco. Podemos visualizar a ACF usando um gr√°fico de barras:
>
> ```mermaid
> barChart
>     title Autocorrela√ß√£o (ACF)
>     labels [0, 1, 2, 3, 4]
>     values [1, 0, 0, 0, 0]
> ```
> Este gr√°fico confirma que a √∫nica correla√ß√£o significativa est√° no lag 0.

**Proposi√ß√£o 2.1.** Para um processo $Y_t = \mu + \varepsilon_t$, onde $\varepsilon_t$ √© um ru√≠do branco com m√©dia zero, $E[Y_t] = \mu$.

*Prova.*
$$E[Y_t] = E[\mu + \varepsilon_t] = E[\mu] + E[\varepsilon_t] = \mu + 0 = \mu$$

Portanto, o valor esperado do processo $Y_t$ √© igual √† constante $\mu$.

> üí° **Exemplo Num√©rico:**  No exemplo anterior, onde $Y_t = 5 + \varepsilon_t$, o valor esperado do processo √© $E[Y_t] = 5$.

**Proposi√ß√£o 2.2.** Para um processo $Y_t = \mu + \varepsilon_t$, onde $\varepsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, a vari√¢ncia de $Y_t$ √© $\sigma^2$.

*Prova.*
$$Var(Y_t) = E[(Y_t - E[Y_t])^2] = E[(Y_t - \mu)^2]$$
Substituindo $Y_t = \mu + \varepsilon_t$, temos:
$$Var(Y_t) = E[(\mu + \varepsilon_t - \mu)^2] = E[\varepsilon_t^2] = \sigma^2$$

Portanto, a vari√¢ncia do processo $Y_t$ √© igual √† vari√¢ncia do ru√≠do branco $\varepsilon_t$.

> üí° **Exemplo Num√©rico:**  No exemplo anterior, onde $Y_t = 5 + \varepsilon_t$ e $\sigma^2 = 2$, a vari√¢ncia do processo √© $Var(Y_t) = 2$.

*Implica√ß√µes e Aplica√ß√µes*:

1.  **Valida√ß√£o de Premissas**: A simplicidade da fun√ß√£o de autocovari√¢ncia permite uma valida√ß√£o r√°pida e direta se uma determinada s√©rie temporal pode ser considerada ru√≠do branco. Em aplica√ß√µes pr√°ticas, se a ACF estimada de uma s√©rie temporal se aproxima de um impulso unit√°rio no lag zero, a premissa de ru√≠do branco √© considerada razo√°vel [^45, ^47].

**Lema 3.** Seja $\{X_t\}$ um processo estoc√°stico com m√©dia $\mu_X$ e $\{Y_t\}$ um processo estoc√°stico com m√©dia $\mu_Y$, e suponha que esses processos s√£o independentes. Ent√£o, a covari√¢ncia entre $X_t$ e $Y_t$ √© zero.

*Prova.* A covari√¢ncia entre $X_t$ e $Y_t$ √© definida como:
$$Cov(X_t, Y_t) = E[(X_t - \mu_X)(Y_t - \mu_Y)]$$
Como $X_t$ e $Y_t$ s√£o independentes, $E[X_t Y_t] = E[X_t]E[Y_t] = \mu_X \mu_Y$. Assim,
$$Cov(X_t, Y_t) = E[X_t Y_t - \mu_X Y_t - \mu_Y X_t + \mu_X \mu_Y] = E[X_t Y_t] - \mu_X E[Y_t] - \mu_Y E[X_t] + \mu_X \mu_Y$$
$$Cov(X_t, Y_t) = \mu_X \mu_Y - \mu_X \mu_Y - \mu_Y \mu_X + \mu_X \mu_Y = 0$$
Portanto, a covari√¢ncia entre $X_t$ e $Y_t$ √© zero. ‚ñ†

> üí° **Exemplo Num√©rico:** Seja $X_t$ uma s√©rie com m√©dia 2 e $Y_t$ uma s√©rie com m√©dia 3. Se $X_t$ e $Y_t$ s√£o independentes, ent√£o $Cov(X_t, Y_t) = 0$. Isto significa que as varia√ß√µes em $X_t$ n√£o est√£o relacionadas √†s varia√ß√µes em $Y_t$.

2.  **Simplifica√ß√£o de Modelagem**: Em modelos mais complexos, como ARMA (*Autoregressive Moving Average*), a presen√ßa de um termo de ru√≠do branco simplifica as equa√ß√µes e estimativas [^48, ^53]. Por exemplo, no processo MA(1), compreendemos que apenas o primeiro lag influencia a autocorrela√ß√£o [^48].

**Lema 3.1.** Seja $\{X_t\}$ um processo estoc√°stico com fun√ß√£o de autocovari√¢ncia $\gamma_X(j)$ e $\{Y_t\}$ um processo estoc√°stico com fun√ß√£o de autocovari√¢ncia $\gamma_Y(j)$, e suponha que esses processos s√£o independentes. Ent√£o, a autocovari√¢ncia do processo soma $\{Z_t = X_t + Y_t\}$ √© $\gamma_Z(j) = \gamma_X(j) + \gamma_Y(j)$.

*Prova.* A autocovari√¢ncia do processo $\{Z_t\}$ √© definida como:
$$ \gamma_Z(j) = E[(Z_t - E[Z_t])(Z_{t-j} - E[Z_{t-j}])] $$
Como $Z_t = X_t + Y_t$, temos $E[Z_t] = E[X_t] + E[Y_t]$. Substituindo na equa√ß√£o da autocovari√¢ncia:
$$ \gamma_Z(j) = E[((X_t + Y_t) - (E[X_t] + E[Y_t]))((X_{t-j} + Y_{t-j}) - (E[X_{t-j}] + E[Y_{t-j}]))] $$
$$ \gamma_Z(j) = E[((X_t - E[X_t]) + (Y_t - E[Y_t]))((X_{t-j} - E[X_{t-j}]) + (Y_{t-j} - E[Y_{t-j}]))] $$
Expandindo a express√£o, obtemos:
$$ \gamma_Z(j) = E[(X_t - E[X_t])(X_{t-j} - E[X_{t-j}])] + E[(X_t - E[X_t])(Y_{t-j} - E[Y_t])] + E[(Y_t - E[Y_t])(X_{t-j} - E[X_{t-j}])] + E[(Y_t - E[Y_t])(Y_{t-j} - E[Y_{t-j}])] $$
Como $\{X_t\}$ e $\{Y_t\}$ s√£o independentes, as covari√¢ncias cruzadas s√£o zero:
$$ E[(X_t - E[X_t])(Y_{t-j} - E[Y_{t-j}])] = 0 $$
$$ E[(Y_t - E[Y_t])(X_{t-j} - E[X_{t-j}])] = 0 $$
Portanto,
$$ \gamma_Z(j) = E[(X_t - E[X_t])(X_{t-j} - E[X_{t-j}])] + E[(Y_t - E[Y_t])(Y_{t-j} - E[Y_{t-j}])] = \gamma_X(j) + \gamma_Y(j) $$
Assim, a autocovari√¢ncia do processo $\{Z_t\}$ √© a soma das autocovari√¢ncias de $\{X_t\}$ e $\{Y_t\}$. ‚ñ†

> üí° **Exemplo Num√©rico:** Seja $X_t$ um processo AR(1) com $\gamma_X(0) = 4$ e $\gamma_X(1) = 2$, e $Y_t$ um ru√≠do branco com $\gamma_Y(0) = 1$ e $\gamma_Y(j) = 0$ para $j \neq 0$. Se $Z_t = X_t + Y_t$, ent√£o:
>
> $\gamma_Z(0) = \gamma_X(0) + \gamma_Y(0) = 4 + 1 = 5$
> $\gamma_Z(1) = \gamma_X(1) + \gamma_Y(1) = 2 + 0 = 2$
> $\gamma_Z(j) = \gamma_X(j) + \gamma_Y(j) = \gamma_X(j)$ para $j > 1$
>
> Este exemplo demonstra como a autocovari√¢ncia de um processo composto √© a soma das autocovari√¢ncias dos processos individuais, simplificando a an√°lise.

3.  **Processamento de Sinal**: Em processamento de sinal, o ru√≠do branco serve como um modelo para sinais aleat√≥rios e n√£o correlacionados [^47]. Conhecer a autocovari√¢ncia desses sinais √© fundamental para o design de filtros √≥timos e algoritmos de detec√ß√£o [^47].

> üí° **Exemplo Num√©rico:** Em um sistema de comunica√ß√£o, um sinal recebido $Y_t$ pode ser modelado como $Y_t = s_t + \varepsilon_t$, onde $s_t$ √© o sinal transmitido e $\varepsilon_t$ √© ru√≠do branco. Se $\sigma^2 = 0.1$, isso representa a pot√™ncia do ru√≠do. Ao projetar um filtro, o conhecimento de $\sigma^2$ √© crucial para minimizar o efeito do ru√≠do e recuperar o sinal $s_t$ com precis√£o.

4.  **Teste de Res√≠duos**: A an√°lise dos res√≠duos de um modelo de s√©rie temporal √© um passo crucial na valida√ß√£o do modelo. Se os res√≠duos se comportam como ru√≠do branco, isso sugere que o modelo capturou a maior parte da estrutura de depend√™ncia nos dados [^45].

*Exemplo Pr√°tico:*

Considere que estamos modelando um sistema de comunica√ß√£o e recebemos um sinal $Y_t$ que √© composto por um sinal determin√≠stico $s_t$ mais um ru√≠do $\varepsilon_t$:
$$
Y_t = s_t + \varepsilon_t
$$
Assumindo que o ru√≠do $\varepsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, podemos analisar as propriedades estat√≠sticas de $Y_t$ [^47]. Se o sinal $s_t$ √© conhecido, podemos subtra√≠-lo de $Y_t$ e verificar se os res√≠duos se comportam como ru√≠do branco [^47]. A fun√ß√£o de autocovari√¢ncia dos res√≠duos deve ser aproximadamente zero para todos os lags diferentes de zero [^45].

**Teorema 4.** Seja $Y_t = s_t + \varepsilon_t$, onde $s_t$ √© um sinal determin√≠stico e $\varepsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$. Se $s_t$ e $\varepsilon_t$ s√£o independentes, ent√£o a autocovari√¢ncia de $Y_t$ √© dada por $\gamma_Y(j) = \gamma_s(j) + \gamma_{\varepsilon}(j)$, onde $\gamma_s(j)$ √© a autocovari√¢ncia de $s_t$ e $\gamma_{\varepsilon}(j)$ √© a autocovari√¢ncia de $\varepsilon_t$.

*Prova.*
I. A autocovari√¢ncia de $Y_t$ √© definida como:
$$
\gamma_Y(j) = E[(Y_t - E[Y_t])(Y_{t-j} - E[Y_{t-j}])]
$$
II. Dado que $Y_t = s_t + \varepsilon_t$, ent√£o $E[Y_t] = E[s_t + \varepsilon_t] = E[s_t] + E[\varepsilon_t]$. Como $E[\varepsilon_t] = 0$, temos $E[Y_t] = E[s_t]$. Substituindo isso na equa√ß√£o da autocovari√¢ncia:
$$
\gamma_Y(j) = E[(s_t + \varepsilon_t - E[s_t])(s_{t-j} + \varepsilon_{t-j} - E[s_{t-j}])]
$$
III. Expandindo a express√£o:
$$
\gamma_Y(j) = E[((s_t - E[s_t]) + \varepsilon_t)((s_{t-j} - E[s_{t-j}]) + \varepsilon_{t-j})]
$$
$$
\gamma_Y(j) = E[(s_t - E[s_t])(s_{t-j} - E[s_{t-j}])] + E[(s_t - E[s_t])\varepsilon_{t-j}] + E[\varepsilon_t(s_{t-j} - E[s_{t-j}])] + E[\varepsilon_t \varepsilon_{t-j}]
$$
IV. Como $s_t$ e $\varepsilon_t$ s√£o independentes, $E[(s_t - E[s_t])\varepsilon_{t-j}] = E[s_t - E[s_t]]E[\varepsilon_{t-j}] = 0 \cdot 0 = 0$ e $E[\varepsilon_t(s_{t-j} - E[s_{t-j}])] = E[\varepsilon_t]E[s_{t-j} - E[s_{t-j}]] = 0 \cdot 0 = 0$.
V. Portanto,
$$
\gamma_Y(j) = E[(s_t - E[s_t])(s_{t-j} - E[s_{t-j}])] + E[\varepsilon_t \varepsilon_{t-j}] = \gamma_s(j) + \gamma_{\varepsilon}(j)
$$
Assim, a autocovari√¢ncia de $Y_t$ √© a soma das autocovari√¢ncias de $s_t$ e $\varepsilon_t$. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere um sinal determin√≠stico $s_t = \sin(0.1t)$ e um ru√≠do branco $\varepsilon_t$ com $\sigma^2 = 0.5$. A autocovari√¢ncia do ru√≠do branco √© $\gamma_{\varepsilon}(0) = 0.5$ e $\gamma_{\varepsilon}(j) = 0$ para $j \neq 0$.  A autocovari√¢ncia do sinal $s_t$ pode ser calculada numericamente ou aproximada. Ent√£o, a autocovari√¢ncia do sinal composto $Y_t = s_t + \varepsilon_t$ ser√° a soma da autocovari√¢ncia de $s_t$ e $\varepsilon_t$.

**Teorema 4.1.** Seja $Y_t = s_t + \varepsilon_t$, onde $s_t$ √© um sinal determin√≠stico com m√©dia zero e $\varepsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$. Se $s_t$ e $\varepsilon_t$ s√£o independentes, ent√£o a fun√ß√£o de autocorrela√ß√£o (ACF) de $Y_t$ no lag zero √© dada por $\rho_Y(0) = \frac{\gamma_s(0) + \sigma^2}{\gamma_s(0) + \sigma^2} = 1$.

*Prova.*
I. A fun√ß√£o de autocorrela√ß√£o √© definida como $\rho_Y(j) = \frac{\gamma_Y(j)}{\gamma_Y(0)}$.  No lag zero, temos:

$$ \rho_Y(0) = \frac{\gamma_Y(0)}{\gamma_Y(0)} $$

II. Pelo Teorema 4, $\gamma_Y(j) = \gamma_s(j) + \gamma_{\varepsilon}(j)$.  Portanto, $\gamma_Y(0) = \gamma_s(0) + \gamma_{\varepsilon}(0)$.  Como $\varepsilon_t$ √© ru√≠do branco com vari√¢ncia $\sigma^2$, $\gamma_{\varepsilon}(0) = \sigma^2$.  Assim:

$$ \gamma_Y(0) = \gamma_s(0) + \sigma^2 $$

III. Substituindo na express√£o da ACF:

$$ \rho_Y(0) = \frac{\gamma_s(0) + \sigma^2}{\gamma_s(0) + \sigma^2} = 1 $$

Portanto, a fun√ß√£o de autocorrela√ß√£o de $Y_t$ no lag zero √© igual a 1. ‚ñ†

> üí° **Exemplo Num√©rico:** Se $s_t$ √© um sinal com vari√¢ncia $\gamma_s(0) = 1$ e $\varepsilon_t$ √© ru√≠do branco com vari√¢ncia $\sigma^2 = 0.5$, ent√£o a autocorrela√ß√£o no lag zero para o sinal composto $Y_t$ ser√° $\rho_Y(0) = \frac{1 + 0.5}{1 + 0.5} = 1$.

### Conclus√£o

Para processos onde as autocovari√¢ncias s√£o zero para $j \neq 0$, como no processo $Y_t = \mu + \varepsilon_t$, a autocovari√¢ncia se resume a $\gamma_0 = \sigma^2$ [^45]. Essa simplifica√ß√£o permite uma valida√ß√£o r√°pida das premissas de ru√≠do branco e facilita c√°lculos subsequentes em diversas aplica√ß√µes [^47]. A compreens√£o dessas propriedades simplificadas √© fundamental para a modelagem eficiente e a an√°lise de dados de s√©ries temporais, al√©m de ser amplamente utilizada em processamento de sinais e outras √°reas relacionadas [^45, ^46, ^47].

### Refer√™ncias
[^45]: P√°ginas 44-45.
[^46]: P√°gina 46.
[^47]: P√°gina 47.
[^48]: P√°gina 48.
[^53]: P√°gina 53.
<!-- END -->