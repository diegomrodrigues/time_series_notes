## Computa√ß√£o Eficiente de Autocovari√¢ncias

### Introdu√ß√£o
Em continuidade aos t√≥picos anteriores sobre **Autocovari√¢ncia e sua Computa√ß√£o** e **Autocovari√¢ncia para Processos com Autocovari√¢ncias Nulas Fora do Lag Zero**, este cap√≠tulo explora t√©cnicas eficientes para o c√°lculo de autocovari√¢ncias. O c√°lculo eficiente de autocovari√¢ncias √© crucial para a an√°lise de s√©ries temporais em tempo real e para lidar com grandes conjuntos de dados, onde a computa√ß√£o direta pode se tornar proibitivamente cara [^45]. Este cap√≠tulo se concentra no uso de Transformadas R√°pidas de Fourier (FFTs) para acelerar o c√°lculo da fun√ß√£o de autocovari√¢ncia, al√©m de discutir outras otimiza√ß√µes.

### Conceitos Fundamentais

O c√°lculo direto da autocovari√¢ncia para um lag $j$ requer $O(N)$ opera√ß√µes, onde $N$ √© o tamanho da s√©rie temporal. Para calcular a fun√ß√£o de autocovari√¢ncia para todos os lags at√© um valor m√°ximo $M$, o custo computacional se torna $O(NM)$, o que pode ser invi√°vel para grandes $N$ e $M$ [^45].

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal com $N = 10000$ pontos. Se quisermos calcular a autocovari√¢ncia para os primeiros $M = 1000$ lags, o c√°lculo direto exigiria aproximadamente $N \times M = 10000 \times 1000 = 10^7$ opera√ß√µes. Isso pode ser computacionalmente caro em tempo real ou em sistemas embarcados com recursos limitados.

#### Transformada R√°pida de Fourier (FFT) (*Fast Fourier Transform (FFT)*)

A **Transformada R√°pida de Fourier** (FFT) (*Fast Fourier Transform (FFT)*) √© um algoritmo eficiente para calcular a Transformada de Fourier Discreta (DFT) (*Discrete Fourier Transform (DFT)*) [^47]. A DFT transforma uma sequ√™ncia de $N$ n√∫meros complexos em uma sequ√™ncia de $N$ n√∫meros complexos, decompondo um sinal em componentes de diferentes frequ√™ncias [^47]. A FFT reduz a complexidade computacional da DFT de $O(N^2)$ para $O(N \log N)$, tornando-a uma ferramenta essencial para processamento de sinais e an√°lise de dados [^47].

**Teorema 5.** A fun√ß√£o de autocovari√¢ncia de uma s√©rie temporal pode ser eficientemente calculada usando a Transformada R√°pida de Fourier (FFT).

*Proof.* Seja $\{Y_t\}_{t=0}^{N-1}$ uma s√©rie temporal de tamanho $N$ com m√©dia $\mu$. A fun√ß√£o de autocovari√¢ncia $\gamma_j$ pode ser expressa como:
$$
\gamma_j = \frac{1}{N} \sum_{t=0}^{N-j-1} (Y_t - \mu)(Y_{t+j} - \mu)
$$
A DFT de $\{Y_t - \mu\}$ √© dada por:
$$
X_k = \sum_{t=0}^{N-1} (Y_t - \mu) e^{-i2\pi kt/N}, \quad k = 0, 1, \ldots, N-1
$$
onde $i$ √© a unidade imagin√°ria. O periodograma, que √© uma estimativa da densidade espectral de pot√™ncia (PSD) (*power spectral density (PSD)*), √© dado por:
$$
I_k = \frac{1}{N} |X_k|^2
$$
Pelo teorema de Wiener-Khinchin (*Wiener-Khinchin theorem*), a fun√ß√£o de autocovari√¢ncia √© a Transformada Inversa de Fourier Discreta (IDFT) (*Inverse Discrete Fourier Transform (IDFT)*) da densidade espectral de pot√™ncia (PSD) [^47]. Portanto, podemos calcular a fun√ß√£o de autocovari√¢ncia usando:
$$
\gamma_j = \sum_{k=0}^{N-1} I_k e^{i2\pi kj/N}
$$
Como a FFT pode calcular a DFT e a IDFT em $O(N \log N)$ opera√ß√µes, a fun√ß√£o de autocovari√¢ncia pode ser calculada em $O(N \log N)$ opera√ß√µes [^47]. ‚ñ†

> üí° **Exemplo Num√©rico:**  Considere uma s√©rie temporal de tamanho $N = 1024$. O c√°lculo direto da autocovari√¢ncia para todos os lags at√© $M = 512$ requer $O(NM) = O(1024 \cdot 512) = O(524288)$ opera√ß√µes. Usando a FFT, o c√°lculo da fun√ß√£o de autocovari√¢ncia requer aproximadamente $O(N \log N) = O(1024 \cdot \log_2 1024) = O(1024 \cdot 10) = O(10240)$ opera√ß√µes. A FFT oferece uma acelera√ß√£o significativa.
>
> ```python
> import numpy as np
>
> N = 1024
> M = 512
>
> # Opera√ß√µes diretas:
> direct_ops = N * M
> print(f"Opera√ß√µes diretas: {direct_ops}") # Sa√≠da: Opera√ß√µes diretas: 524288
>
> # Opera√ß√µes com FFT:
> fft_ops = N * np.log2(N)
> print(f"Opera√ß√µes com FFT: {fft_ops}") # Sa√≠da: Opera√ß√µes com FFT: 10240.0
> ```

**Proposi√ß√£o 5.1.** O periodograma, definido como $I_k = \frac{1}{N} |X_k|^2$, √© uma estimativa n√£o tendenciosa (*unbiased*) da densidade espectral de pot√™ncia (PSD) se a s√©rie temporal $\{Y_t\}$ √© ru√≠do branco com m√©dia zero.

*Prova.* Para ru√≠do branco com m√©dia zero, $E[Y_t] = 0$ e $E[Y_t Y_{t+j}] = \sigma^2 \delta_j$, onde $\delta_j$ √© a fun√ß√£o delta de Kronecker. A DFT √© dada por $X_k = \sum_{t=0}^{N-1} Y_t e^{-i2\pi kt/N}$. Ent√£o,
$$ E[|X_k|^2] = E[X_k X_k^*] = E\left[\left(\sum_{t=0}^{N-1} Y_t e^{-i2\pi kt/N}\right)\left(\sum_{\tau=0}^{N-1} Y_\tau e^{i2\pi k\tau/N}\right)\right] $$
$$ E[|X_k|^2] = \sum_{t=0}^{N-1} \sum_{\tau=0}^{N-1} E[Y_t Y_\tau] e^{-i2\pi k(t-\tau)/N} = \sum_{t=0}^{N-1} \sum_{\tau=0}^{N-1} \sigma^2 \delta_{t-\tau} e^{-i2\pi k(t-\tau)/N} $$
$$ E[|X_k|^2] = \sigma^2 \sum_{t=0}^{N-1} e^{-i2\pi k(t-t)/N} = \sigma^2 \sum_{t=0}^{N-1} 1 = N\sigma^2 $$
Portanto, $E[I_k] = E\left[\frac{1}{N}|X_k|^2\right] = \frac{1}{N} N\sigma^2 = \sigma^2$. Isso mostra que o periodograma √© uma estimativa n√£o tendenciosa da densidade espectral de pot√™ncia (PSD) para ru√≠do branco. ‚ñ†

> üí° **Exemplo Num√©rico:**  Suponha que geramos uma s√©rie temporal de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2 = 1$. Se calcularmos o periodograma, o valor esperado do periodograma em cada frequ√™ncia $k$ deve ser pr√≥ximo de 1.
>
> ```python
> import numpy as np
>
> N = 1024
> sigma_squared = 1
>
> # Gerar ru√≠do branco
> Y = np.random.normal(0, np.sqrt(sigma_squared), N)
>
> # Calcular a DFT
> X = np.fft.fft(Y)
>
> # Calcular o periodograma
> I = (1/N) * np.abs(X)**2
>
> # Imprimir o valor m√©dio do periodograma
> print(f"Valor m√©dio do periodograma: {np.mean(I)}") # Sa√≠da: Valor m√©dio do periodograma: pr√≥ximo de 1
> ```

**Proposi√ß√£o 5.2.** O periodograma √© uma estimativa consistente da densidade espectral de pot√™ncia (PSD) se a s√©rie temporal $\{Y_t\}$ satisfaz certas condi√ß√µes de regularidade.

*Prova.* A prova da consist√™ncia do periodograma envolve mostrar que, sob certas condi√ß√µes, a vari√¢ncia do periodograma tende a zero √† medida que o tamanho da amostra $N$ aumenta. Especificamente, precisamos que a s√©rie temporal $\{Y_t\}$ seja estacion√°ria e que seus momentos de ordem superior satisfa√ßam certas condi√ß√µes de decaimento. Uma condi√ß√£o suficiente √© que $\{Y_t\}$ seja um processo linear da forma $Y_t = \sum_{k=-\infty}^{\infty} a_k \epsilon_{t-k}$, onde $\{\epsilon_t\}$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, e $\sum_{k=-\infty}^{\infty} |a_k| < \infty$. Sob essas condi√ß√µes, pode-se mostrar que $Var(I_k) \rightarrow 0$ quando $N \rightarrow \infty$, o que implica que o periodograma √© uma estimativa consistente da PSD. A prova completa envolve manipula√ß√µes complexas de momentos e √© encontrada em textos avan√ßados de an√°lise de s√©ries temporais. ‚ñ†

**Algoritmo:**

1.  Remova a m√©dia da s√©rie temporal $\{Y_t\}$ [^45]:
    $$
    Y'_t = Y_t - \mu
    $$
2.  Calcule a FFT de $\{Y'_t\}$ para obter $\{X_k\}$ [^47]:
    $$
    X_k = \text{FFT}(Y'_t)
    $$
3.  Calcule o periodograma [^47]:
    $$
    I_k = \frac{1}{N} |X_k|^2
    $$
4.  Calcule a FFT inversa (IFFT) do periodograma para obter a fun√ß√£o de autocovari√¢ncia [^47]:
    $$
    \gamma_j = \text{IFFT}(I_k)
    $$

**Lema 6.** Para um sinal real, o periodograma √© sim√©trico, ou seja, $I_k = I_{N-k}$.

*Prova.* Se $\{Y_t\}$ √© uma sequ√™ncia real, ent√£o $X_k = \sum_{t=0}^{N-1} Y_t e^{-i2\pi kt/N}$ e $X_{N-k} = \sum_{t=0}^{N-1} Y_t e^{-i2\pi (N-k)t/N} = \sum_{t=0}^{N-1} Y_t e^{i2\pi kt/N} = X_k^*$.
Portanto, $I_k = \frac{1}{N} |X_k|^2 = \frac{1}{N} |X_k^*|^2 = \frac{1}{N} |X_{N-k}|^2 = I_{N-k}$. ‚ñ†

**Lema 6.1.** Devido √† simetria do periodograma para sinais reais, apenas a metade dos valores de $I_k$ (para $k = 0, 1, ..., N/2$) precisa ser armazenada para reconstruir a fun√ß√£o de autocovari√¢ncia.

*Prova.* Como $I_k = I_{N-k}$, todos os valores do periodograma para $k > N/2$ s√£o redundantes e podem ser obtidos a partir dos valores para $k \leq N/2$. Portanto, ao calcular a IFFT, podemos reconstruir o periodograma completo a partir dos valores de $k = 0$ at√© $k = N/2$. ‚ñ†

*Outras Otimiza√ß√µes e Considera√ß√µes Pr√°ticas:*

1.  **Segmenta√ß√£o e M√©dia (*Segmentation and Averaging*)**: Para reduzir a vari√¢ncia da estimativa da autocovari√¢ncia, a s√©rie temporal pode ser segmentada em v√°rios segmentos, e a FFT √© aplicada a cada segmento separadamente [^47]. As estimativas da autocovari√¢ncia dos segmentos s√£o ent√£o m√©dias para obter uma estimativa mais robusta [^47].

2.  **Janelamento (*Windowing*)**: Aplicar uma janela (*window*) √† s√©rie temporal antes de calcular a FFT pode reduzir o vazamento espectral (*spectral leakage*) e melhorar a precis√£o da estimativa da autocovari√¢ncia [^47]. As janelas comuns incluem a janela de Hamming (*Hamming window*) e a janela de Blackman (*Blackman window*) [^47].

> üí° **Exemplo Num√©rico:** Suponha que temos uma s√©rie temporal de tamanho $N = 2048$. Podemos dividir a s√©rie em dois segmentos de tamanho 1024 e calcular a FFT para cada segmento. Em seguida, podemos calcular o periodograma para cada segmento e fazer a m√©dia dos periodogramas. Esse procedimento reduz a vari√¢ncia da estimativa da PSD e, consequentemente, da autocovari√¢ncia.
>
> ```python
> import numpy as np
>
> N = 2048
> segment_size = 1024
> num_segments = N // segment_size
>
> # Gerar s√©rie temporal (exemplo)
> Y = np.random.randn(N)
>
> periodograms = []
> for i in range(num_segments):
>     start = i * segment_size
>     end = (i + 1) * segment_size
>     Y_segment = Y[start:end]
>     X_segment = np.fft.fft(Y_segment)
>     I_segment = (1 / segment_size) * np.abs(X_segment)**2
>     periodograms.append(I_segment)
>
> # Calcular a m√©dia dos periodogramas
> I_averaged = np.mean(periodograms, axis=0)
>
> print(f"Tamanho do periodograma m√©dio: {len(I_averaged)}") # Sa√≠da: Tamanho do periodograma m√©dio: 1024
> ```

3.  **Padding de Zeros (*Zero-Padding*)**: Aumentar o tamanho da s√©rie temporal adicionando zeros (zero-padding) pode aumentar a resolu√ß√£o da fun√ß√£o de autocovari√¢ncia e facilitar a interpreta√ß√£o [^47]. Zero-padding n√£o adiciona informa√ß√£o nova, mas interpola entre os pontos existentes na fun√ß√£o de autocovari√¢ncia [^47].

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal de tamanho $N=64$. Se aplicarmos zero-padding para aumentar o tamanho para $N'=256$, a FFT resultante ter√° mais pontos, o que corresponde a uma interpola√ß√£o na fun√ß√£o de autocovari√¢ncia resultante. Isso permite visualizar detalhes mais finos na fun√ß√£o de autocovari√¢ncia.

4.  **Implementa√ß√µes Otimizadas**: Utilizar bibliotecas FFT otimizadas, como FFTW (*Fastest Fourier Transform in the West*), pode acelerar significativamente os c√°lculos [^47]. Essas bibliotecas empregam t√©cnicas avan√ßadas de otimiza√ß√£o, como vetoriza√ß√£o e paraleliza√ß√£o, para maximizar o desempenho [^47].

5. **Estacionariedade:** A efici√™ncia da FFT para calcular a autocovari√¢ncia √© maior quando aplicada a s√©ries temporais estacion√°rias. Em s√©ries n√£o estacion√°rias, pode ser necess√°rio aplicar t√©cnicas de diferencia√ß√£o ou outras transforma√ß√µes para garantir a estacionariedade antes de aplicar a FFT.

**Teorema 7.** Seja $\{Y_t\}$ uma s√©rie temporal com m√©dia $\mu$ e fun√ß√£o de autocovari√¢ncia $\gamma_j$. Se $\{Y_t\}$ √© transformada para se tornar estacion√°ria por diferencia√ß√£o, ent√£o a fun√ß√£o de autocovari√¢ncia da s√©rie diferenciada √© diferente da fun√ß√£o de autocovari√¢ncia da s√©rie original.

*Prova.* A primeira diferen√ßa de $\{Y_t\}$ √© dada por $\Delta Y_t = Y_t - Y_{t-1}$. A fun√ß√£o de autocovari√¢ncia de $\Delta Y_t$ √©:

$$ \gamma_{\Delta Y}(j) = E[(\Delta Y_t - E[\Delta Y_t])(\Delta Y_{t-j} - E[\Delta Y_{t-j}])] $$

Como $E[\Delta Y_t] = E[Y_t - Y_{t-1}] = E[Y_t] - E[Y_{t-1}] = \mu - \mu = 0$, temos:

$$ \gamma_{\Delta Y}(j) = E[(Y_t - Y_{t-1})(Y_{t-j} - Y_{t-j-1})] $$

Expandindo a express√£o:

$$ \gamma_{\Delta Y}(j) = E[Y_t Y_{t-j} - Y_t Y_{t-j-1} - Y_{t-1} Y_{t-j} + Y_{t-1} Y_{t-j-1}] $$

$$ \gamma_{\Delta Y}(j) = E[Y_t Y_{t-j}] - E[Y_t Y_{t-j-1}] - E[Y_{t-1} Y_{t-j}] + E[Y_{t-1} Y_{t-j-1}] $$

Em termos da fun√ß√£o de autocovari√¢ncia da s√©rie original:

$$ \gamma_{\Delta Y}(j) = \gamma_Y(j) - \gamma_Y(j+1) - \gamma_Y(j-1) + \gamma_Y(j) $$

$$ \gamma_{\Delta Y}(j) = 2\gamma_Y(j) - \gamma_Y(j+1) - \gamma_Y(j-1) $$

Claramente, $\gamma_{\Delta Y}(j) \neq \gamma_Y(j)$, mostrando que a fun√ß√£o de autocovari√¢ncia da s√©rie diferenciada √© diferente da fun√ß√£o de autocovari√¢ncia da s√©rie original. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal $Y_t = t$. A sua primeira diferen√ßa √© $\Delta Y_t = Y_t - Y_{t-1} = 1$. Claramente, a fun√ß√£o de autocovari√¢ncia de $Y_t$ n√£o √© definida (devido √† n√£o-estacionaridade), enquanto a fun√ß√£o de autocovari√¢ncia de $\Delta Y_t$ √© $\gamma_{\Delta Y}(0) = 0$ e $\gamma_{\Delta Y}(j) = 0$ para $j \neq 0$.
>
> ```python
> import numpy as np
>
> # S√©rie temporal n√£o estacion√°ria
> Y = np.arange(10)
>
> # Primeira diferen√ßa
> delta_Y = np.diff(Y)
>
> print(f"S√©rie original: {Y}") # Sa√≠da: S√©rie original: [0 1 2 3 4 5 6 7 8 9]
> print(f"Primeira diferen√ßa: {delta_Y}") # Sa√≠da: Primeira diferen√ßa: [1 1 1 1 1 1 1 1 1]
>
> # Autocovari√¢ncia da primeira diferen√ßa (neste caso √© trivial)
> autocov = np.cov(delta_Y)
> print(f"Autocovari√¢ncia da primeira diferen√ßa: {autocov}") # Sa√≠da: Autocovari√¢ncia da primeira diferen√ßa: 0.0
> ```

**Teorema 7.1.** Se a s√©rie temporal $\{Y_t\}$ √© uma realiza√ß√£o de um processo estacion√°rio de m√©dia zero, ent√£o a vari√¢ncia da primeira diferen√ßa $\Delta Y_t = Y_t - Y_{t-1}$ √© dada por $Var(\Delta Y_t) = 2(\gamma_Y(0) - \gamma_Y(1))$.

*Prova.*
I. Temos que $\Delta Y_t = Y_t - Y_{t-1}$.
II. Como $E[Y_t] = 0$ para todo $t$, ent√£o $E[\Delta Y_t] = E[Y_t - Y_{t-1}] = E[Y_t] - E[Y_{t-1}] = 0$. Portanto,
III. $Var(\Delta Y_t) = E[(\Delta Y_t)^2] = E[(Y_t - Y_{t-1})^2] = E[Y_t^2 - 2Y_t Y_{t-1} + Y_{t-1}^2]$
IV. $Var(\Delta Y_t) = E[Y_t^2] - 2E[Y_t Y_{t-1}] + E[Y_{t-1}^2]$
V. Como $\{Y_t\}$ √© estacion√°rio, $E[Y_t^2] = E[Y_{t-1}^2] = \gamma_Y(0)$ e $E[Y_t Y_{t-1}] = \gamma_Y(1)$.
VI. Portanto, $Var(\Delta Y_t) = \gamma_Y(0) - 2\gamma_Y(1) + \gamma_Y(0) = 2(\gamma_Y(0) - \gamma_Y(1))$ ‚ñ†

> üí° **Exemplo Num√©rico:** Seja $Y_t$ um processo AR(1) dado por $Y_t = 0.5Y_{t-1} + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco com vari√¢ncia 1.  Ent√£o $\gamma_Y(0) = Var(Y_t) = \frac{1}{1-0.5^2} = \frac{4}{3}$ e $\gamma_Y(1) = 0.5 \gamma_Y(0) = \frac{2}{3}$. Portanto, $Var(\Delta Y_t) = 2(\frac{4}{3} - \frac{2}{3}) = \frac{4}{3}$.
>
> ```python
> import numpy as np
>
> # Par√¢metros do processo AR(1)
> phi = 0.5
> sigma_epsilon = 1
>
> # Vari√¢ncia te√≥rica
> gamma_0 = sigma_epsilon**2 / (1 - phi**2)
> gamma_1 = phi * gamma_0
>
> # Vari√¢ncia da primeira diferen√ßa
> var_delta_Y = 2 * (gamma_0 - gamma_1)
> print(f"Vari√¢ncia te√≥rica da primeira diferen√ßa: {var_delta_Y}") # Sa√≠da: Vari√¢ncia te√≥rica da primeira diferen√ßa: 1.3333333333333333
>
> # Simular um processo AR(1) para verificar
> N = 10000
> Y = np.zeros(N)
> epsilon = np.random.normal(0, sigma_epsilon, N)
> for t in range(1, N):
>     Y[t] = phi * Y[t-1] + epsilon[t]
>
> delta_Y = np.diff(Y)
> var_delta_Y_simulated = np.var(delta_Y)
> print(f"Vari√¢ncia simulada da primeira diferen√ßa: {var_delta_Y_simulated}") # Sa√≠da: Vari√¢ncia simulada da primeira diferen√ßa: Pr√≥xima do valor te√≥rico
> ```

**Lema 8.** Se $\{Y_t\}$ √© uma s√©rie temporal estacion√°ria, ent√£o $|\gamma_j| \leq \gamma_0$ para todo $j$.

*Prova.*
I. Pela desigualdade de Cauchy-Schwarz, $|E[XY]|^2 \leq E[X^2]E[Y^2]$.
II. Sejam $X = Y_t$ e $Y = Y_{t+j}$. Ent√£o, $|E[Y_t Y_{t+j}]|^2 \leq E[Y_t^2] E[Y_{t+j}^2]$.
III. Como $\{Y_t\}$ √© estacion√°rio, $E[Y_t^2] = E[Y_{t+j}^2] = \gamma_0$.
IV. Portanto, $|E[Y_t Y_{t+j}]|^2 \leq \gamma_0^2$.
V. Tomando a raiz quadrada, $|E[Y_t Y_{t+j}]| \leq \gamma_0$. Logo, $|\gamma_j| \leq \gamma_0$. ‚ñ†

### Conclus√£o
A computa√ß√£o eficiente de autocovari√¢ncias √© essencial para a an√°lise de s√©ries temporais em tempo real e para lidar com grandes conjuntos de dados [^45]. O uso de Transformadas R√°pidas de Fourier (FFTs) reduz significativamente a complexidade computacional, tornando a an√°lise vi√°vel em muitas aplica√ß√µes pr√°ticas [^47]. Al√©m disso, t√©cnicas como segmenta√ß√£o, janelamento e padding de zeros podem melhorar ainda mais a precis√£o e a interpretabilidade das estimativas da autocovari√¢ncia [^47]. A combina√ß√£o dessas t√©cnicas, juntamente com bibliotecas FFT otimizadas, oferece uma abordagem poderosa para a an√°lise eficiente de dados de s√©ries temporais.

### Refer√™ncias
[^45]: P√°ginas 44-45.
[^47]: P√°gina 47.
<!-- END -->