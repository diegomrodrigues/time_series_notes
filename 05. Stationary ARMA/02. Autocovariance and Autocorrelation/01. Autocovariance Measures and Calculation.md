## Autocovari√¢ncia em Processos Estacion√°rios ARMA

### Introdu√ß√£o
O estudo da autocovari√¢ncia √© fundamental na an√°lise de s√©ries temporais, particularmente no contexto de modelos ARMA (Autoregressive Moving Average). A autocovari√¢ncia quantifica a depend√™ncia linear entre os valores de uma s√©rie temporal em diferentes momentos no tempo, fornecendo insights cruciais sobre a estrutura temporal dos dados. Este cap√≠tulo aprofunda-se no conceito de autocovari√¢ncia, suas propriedades e sua rela√ß√£o com a estrutura de vari√¢ncia-covari√¢ncia de um processo estoc√°stico.

### Conceitos Fundamentais

**Defini√ß√£o de Autocovari√¢ncia**
A autocovari√¢ncia, denotada por $\gamma_{jt}$, √© uma medida da covari√¢ncia entre a vari√°vel $Y_t$ e seu valor defasado $Y_{t-j}$. Formalmente, √© definida como [^3.1.10]:

$$
\gamma_{jt} = E[(Y_t - \mu_t)(Y_{t-j} - \mu_{t-j})]
$$

onde $E[\cdot]$ representa o operador de esperan√ßa, $Y_t$ √© o valor da s√©rie temporal no instante $t$, $Y_{t-j}$ √© o valor da s√©rie temporal no instante $t-j$, $\mu_t = E[Y_t]$ √© a m√©dia da s√©rie temporal no instante $t$ e $\mu_{t-j} = E[Y_{t-j}]$ √© a m√©dia da s√©rie temporal no instante $t-j$.

**Interpreta√ß√£o da Autocovari√¢ncia**
A autocovari√¢ncia $\gamma_{jt}$ quantifica o grau de depend√™ncia linear entre $Y_t$ e $Y_{t-j}$. Um valor positivo de $\gamma_{jt}$ indica que valores acima (ou abaixo) da m√©dia em $t$ tendem a ser seguidos por valores acima (ou abaixo) da m√©dia em $t-j$. Um valor negativo indica uma rela√ß√£o inversa. Um valor pr√≥ximo de zero sugere uma fraca depend√™ncia linear entre os valores nos dois instantes.

> üí° **Exemplo Num√©rico:** Suponha que $\gamma_{2,t} = 5$ para uma s√©rie temporal. Isso significa que existe uma rela√ß√£o positiva entre o valor no tempo $t$ e o valor dois per√≠odos antes, $t-2$. Se observarmos um valor acima da m√©dia no tempo $t$, √© prov√°vel que o valor no tempo $t-2$ tamb√©m estivesse acima da m√©dia. Agora, se $\gamma_{3,t} = -2$, isso indica uma rela√ß√£o negativa, onde valores acima da m√©dia em $t$ tendem a ser seguidos por valores abaixo da m√©dia em $t-3$.

**Rela√ß√£o com a Matriz de Vari√¢ncia-Covari√¢ncia**

A autocovari√¢ncia $\gamma_{jt}$ pode ser vista como o elemento $(1, j+1)$ da matriz de vari√¢ncia-covari√¢ncia do vetor $x_t$ [^3.1.10], onde $x_t$ consiste nas $j+1$ observa√ß√µes mais recentes de $Y$, ou seja, $x_t = [Y_t, Y_{t-1}, \dots, Y_{t-j}]'$. A matriz de vari√¢ncia-covari√¢ncia $\Sigma_t$ de $x_t$ √© definida como:

$$
\Sigma_t = E[(x_t - \mu_x)(x_t - \mu_x)']
$$

onde $\mu_x$ √© o vetor de m√©dias de $x_t$.  O elemento $(1, j+1)$ de $\Sigma_t$ √© precisamente $\gamma_{jt}$.

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal $Y_t$ e queremos analisar a rela√ß√£o entre $Y_t$, $Y_{t-1}$ e $Y_{t-2}$. Ent√£o, $x_t = [Y_t, Y_{t-1}, Y_{t-2}]'$. Suponha que as m√©dias sejam $\mu_t = \mu_{t-1} = \mu_{t-2} = 0$. A matriz de covari√¢ncia $\Sigma_t$ ser√°:
>
> $$
> \Sigma_t = \begin{bmatrix}
>  E[Y_t^2] & E[Y_tY_{t-1}] & E[Y_tY_{t-2}] \\
>  E[Y_{t-1}Y_t] & E[Y_{t-1}^2] & E[Y_{t-1}Y_{t-2}] \\
>  E[Y_{t-2}Y_t] & E[Y_{t-2}Y_{t-1}] & E[Y_{t-2}^2]
> \end{bmatrix} = \begin{bmatrix}
>  \gamma_0 & \gamma_1 & \gamma_2 \\
>  \gamma_1 & \gamma_0 & \gamma_1 \\
>  \gamma_2 & \gamma_1 & \gamma_0
> \end{bmatrix}
> $$
>
> onde $\gamma_0$ √© a vari√¢ncia de $Y_t$, $\gamma_1$ √© a autocovari√¢ncia na defasagem 1, e $\gamma_2$ √© a autocovari√¢ncia na defasagem 2. Se $\gamma_0 = 4$, $\gamma_1 = 2$, e $\gamma_2 = 1$, ent√£o:
>
> $$
> \Sigma_t = \begin{bmatrix}
>  4 & 2 & 1 \\
>  2 & 4 & 2 \\
>  1 & 2 & 4
> \end{bmatrix}
> $$
>
> Isso representa a matriz de vari√¢ncia-covari√¢ncia para as tr√™s observa√ß√µes da s√©rie temporal.

**Autocovari√¢ncia para Processos Estacion√°rios**
Em um processo *covariance-stationary* (ou *weakly stationary*), a m√©dia $\mu_t$ e a autocovari√¢ncia $\gamma_{jt}$ n√£o dependem de $t$ [^Estacionaridade]. Isso significa que $\mu_t = \mu$ para todo $t$ e $\gamma_{jt} = \gamma_j$, onde $\gamma_j$ depende apenas da defasagem $j$ e n√£o do tempo $t$.  Em processos estacion√°rios, a autocovari√¢ncia √© uma fun√ß√£o par, ou seja, $\gamma_j = \gamma_{-j}$ para todo $j$ [^3.1.13].

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal com os seguintes valores de autocovari√¢ncia: $\gamma_0 = 2$, $\gamma_1 = 1.5$, $\gamma_2 = 0.8$, $\gamma_3 = 0.3$.  Para um processo estacion√°rio, esses valores permanecem constantes ao longo do tempo.  Al√©m disso, como a autocovari√¢ncia √© uma fun√ß√£o par, $\gamma_{-1} = \gamma_1 = 1.5$, $\gamma_{-2} = \gamma_2 = 0.8$, e $\gamma_{-3} = \gamma_3 = 0.3$.  Isso simplifica a an√°lise, pois s√≥ precisamos calcular a autocovari√¢ncia para defasagens positivas.

**Proposi√ß√£o 1**
Em um processo estacion√°rio, a vari√¢ncia $\gamma_0$ √© sempre n√£o negativa.

*Proof:* Por defini√ß√£o, $\gamma_0 = E[(Y_t - \mu)^2]$. Como $(Y_t - \mu)^2$ √© sempre n√£o negativo, sua esperan√ßa tamb√©m deve ser n√£o negativa.

**Prova da Proposi√ß√£o 1:**
I. Seja $Y_t$ um processo estacion√°rio com m√©dia $\mu$.
II. A vari√¢ncia $\gamma_0$ √© definida como $\gamma_0 = E[(Y_t - \mu)^2]$.
III. Como $(Y_t - \mu)^2 \geq 0$ para todo $t$, ent√£o $E[(Y_t - \mu)^2] \geq 0$.
IV. Portanto, $\gamma_0 \geq 0$. ‚ñ†

**Exemplo: Ru√≠do Branco Gaussiano**
Para um processo de ru√≠do branco Gaussiano, a densidade √© dada por [^3.1.2]:
$$
f_Y(y_t) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left[-\frac{y_t^2}{2\sigma^2}\right]
$$
e as autocovari√¢ncias s√£o zero para $j \neq 0$, e $\sigma^2$ para $j = 0$ [^3.2.3]. Ou seja, $E[\epsilon_t \epsilon_\tau] = 0 $ para $t \ne \tau$, onde $\epsilon_t$ √© ru√≠do branco. Isso implica que observa√ß√µes em diferentes momentos s√£o n√£o correlacionadas.

> üí° **Exemplo Num√©rico:** Suponha que temos um processo de ru√≠do branco Gaussiano com vari√¢ncia $\sigma^2 = 1$. Ent√£o, $\gamma_0 = \sigma^2 = 1$, e $\gamma_j = 0$ para todo $j \neq 0$. Isso significa que cada observa√ß√£o √© independente das outras. Se simulamos 1000 pontos desse processo, a autocovari√¢ncia estimada para defasagens diferentes de zero deve ser pr√≥xima de zero.

```python
import numpy as np
import matplotlib.pyplot as plt

# Par√¢metros
T = 1000  # Comprimento da s√©rie temporal
sigma = 1  # Desvio padr√£o do ru√≠do branco

# Gerar ru√≠do branco Gaussiano
np.random.seed(42)  # Para reprodutibilidade
epsilon = np.random.normal(0, sigma, T)

# Calcular a autocovari√¢ncia amostral
def autocovariance(x, lag):
    """Calcula a autocovari√¢ncia amostral para uma dada defasagem."""
    n = len(x)
    x_mean = np.mean(x)
    if lag >= n:
      return 0
    return np.sum((x[:n-lag] - x_mean) * (x[lag:] - x_mean)) / n

lags = np.arange(0, 21)  # Defasagens de 0 a 20
gamma = [autocovariance(epsilon, lag) for lag in lags]

# Plotar a autocovari√¢ncia
plt.figure(figsize=(10, 6))
plt.stem(lags, gamma, basefmt="k-", use_line_collection=True) # Avoid DeprecationWarning
plt.title("Autocovari√¢ncia Amostral do Ru√≠do Branco Gaussiano")
plt.xlabel("Defasagem (j)")
plt.ylabel("Autocovari√¢ncia (Œ≥j)")
plt.grid(True)
plt.show()
```

**Exemplo: Processo com M√©dia Constante e Ru√≠do Branco Gaussiano**
Considere o processo $Y_t = \mu + \epsilon_t$, onde $\mu$ √© uma constante e $\epsilon_t$ √© um processo de ru√≠do branco Gaussiano [^3.1.5].  A m√©dia deste processo √© $E[Y_t] = \mu$ [^3.1.6]. A autocovari√¢ncia √© dada por:

$$
\gamma_{jt} = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[\epsilon_t \epsilon_{t-j}]
$$

Portanto, $\gamma_{jt} = 0$ para $j \neq 0$ e $\gamma_{jt} = \sigma^2$ para $j = 0$, onde $\sigma^2$ √© a vari√¢ncia de $\epsilon_t$.

**Prova de $\gamma_{jt} = E[\epsilon_t \epsilon_{t-j}]$:**
I. Dado $Y_t = \mu + \epsilon_t$.
II. Ent√£o $Y_t - \mu = \epsilon_t$ e $Y_{t-j} - \mu = \epsilon_{t-j}$.
III. A autocovari√¢ncia $\gamma_{jt} = E[(Y_t - \mu)(Y_{t-j} - \mu)]$.
IV. Substituindo, temos $\gamma_{jt} = E[\epsilon_t \epsilon_{t-j}]$. ‚ñ†

> üí° **Exemplo Num√©rico:** Seja $\mu = 5$ e $\epsilon_t$ um ru√≠do branco Gaussiano com $\sigma^2 = 2$. Ent√£o, $Y_t = 5 + \epsilon_t$. A autocovari√¢ncia $\gamma_0 = 2$, e $\gamma_j = 0$ para $j \neq 0$.  Adicionar uma constante √† s√©rie temporal de ru√≠do branco n√£o altera sua estrutura de autocovari√¢ncia.

**Exemplo: Processo com Tend√™ncia Linear e Ru√≠do Branco Gaussiano**
Considere o processo $Y_t = \beta t + \epsilon_t$, onde $\beta$ √© uma constante e $\epsilon_t$ √© um processo de ru√≠do branco Gaussiano [^3.1.7]. A m√©dia deste processo √© $E[Y_t] = \beta t$ [^3.1.8]. A autocovari√¢ncia √© dada por:

$$
\gamma_{jt} = E[(Y_t - \beta t)(Y_{t-j} - \beta(t-j))] = E[\epsilon_t \epsilon_{t-j}]
$$

Portanto, $\gamma_{jt} = 0$ para $j \neq 0$ e $\gamma_{jt} = \sigma^2$ para $j = 0$, onde $\sigma^2$ √© a vari√¢ncia de $\epsilon_t$. Embora a autocovari√¢ncia tenha a mesma forma que no exemplo anterior, este processo *n√£o* √© estacion√°rio porque sua m√©dia depende de $t$.

**Prova de $\gamma_{jt} = E[\epsilon_t \epsilon_{t-j}]$:**
I. Dado $Y_t = \beta t + \epsilon_t$.
II. Ent√£o $Y_t - \beta t = \epsilon_t$ e $Y_{t-j} - \beta(t-j) = \epsilon_{t-j}$.
III. A autocovari√¢ncia $\gamma_{jt} = E[(Y_t - \beta t)(Y_{t-j} - \beta(t-j))]$.
IV. Substituindo, temos $\gamma_{jt} = E[\epsilon_t \epsilon_{t-j}]$. ‚ñ†

> üí° **Exemplo Num√©rico:** Seja $\beta = 0.5$ e $\epsilon_t$ um ru√≠do branco Gaussiano com $\sigma^2 = 1$. Ent√£o, $Y_t = 0.5t + \epsilon_t$. Embora a autocovari√¢ncia do ru√≠do seja a mesma ($\gamma_0 = 1$, $\gamma_j = 0$ para $j \neq 0$), o processo n√£o √© estacion√°rio porque a m√©dia $E[Y_t] = 0.5t$ varia com o tempo. Isso significa que, ao longo do tempo, os valores de $Y_t$ tender√£o a aumentar.

**Lema 1**
Se $Y_t$ √© um processo estacion√°rio com m√©dia $\mu$, ent√£o $E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(Y_{t+k} - \mu)(Y_{t+k-j} - \mu)]$ para qualquer inteiro $k$.

*Proof:* Pela defini√ß√£o de estacionaridade, a autocovari√¢ncia depende apenas da defasagem $j$, n√£o do tempo $t$. Portanto, deslocar a s√©rie temporal por $k$ per√≠odos n√£o altera a autocovari√¢ncia.

**Prova do Lema 1:**
I. Seja $Y_t$ um processo estacion√°rio com m√©dia $\mu$.
II. A autocovari√¢ncia na defasagem $j$ √© definida como $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$.
III. Pela defini√ß√£o de estacionaridade, $\gamma_j$ n√£o depende de $t$.
IV. Portanto, $E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(Y_{t+k} - \mu)(Y_{t+k-j} - \mu)]$ para qualquer inteiro $k$. ‚ñ†

**C√°lculo da Autocovari√¢ncia via M√©dia do Ensemble**
A autocovari√¢ncia $\gamma_{jt}$ pode ser estimada usando a m√©dia do ensemble, que √© o limite da m√©dia sobre um n√∫mero infinito de realiza√ß√µes independentes do processo estoc√°stico [^3.1.4, 3.1.11]:
$$
\gamma_{jt} = \text{plim}_{I \to \infty} \frac{1}{I} \sum_{i=1}^{I} (Y_{t,i} - \mu_t)(Y_{t-j,i} - \mu_{t-j})
$$
onde $Y_{t,i}$ representa a $i$-√©sima realiza√ß√£o da s√©rie temporal no instante $t$ e $\text{plim}$ denota o limite em probabilidade.  Na pr√°tica, $I$ √© finito, ent√£o essa √© uma aproxima√ß√£o.

Al√©m da m√©dia do ensemble, em processos *ergodicos*, a autocovari√¢ncia tamb√©m pode ser estimada usando a m√©dia temporal de uma √∫nica realiza√ß√£o longa do processo.

**Teorema 1**
Para um processo estoc√°stico estacion√°rio e *ergodico*, a autocovari√¢ncia $\gamma_j$ pode ser estimada por:

$$
\hat{\gamma}_j = \frac{1}{T} \sum_{t=j+1}^{T} (Y_t - \bar{Y})(Y_{t-j} - \bar{Y})
$$

onde $T$ √© o comprimento da s√©rie temporal e $\bar{Y}$ √© a m√©dia amostral da s√©rie.

*Proof (Esbo√ßo):* A ergodicidade garante que a m√©dia temporal converge para a m√©dia do ensemble quando o comprimento da s√©rie temporal tende ao infinito. Portanto, sob condi√ß√µes de ergodicidade, a m√©dia temporal ponderada pelas defasagens converge para a autocovari√¢ncia.

> üí° **Exemplo Num√©rico:** Suponha que temos uma s√©rie temporal de 100 observa√ß√µes ($T=100$) e queremos estimar a autocovari√¢ncia na defasagem 1 ($\gamma_1$). Primeiro, calculamos a m√©dia amostral $\bar{Y}$. Em seguida, para cada $t$ de 2 a 100, calculamos $(Y_t - \bar{Y})(Y_{t-1} - \bar{Y})$ e somamos esses produtos. Finalmente, dividimos a soma por 100 para obter a estimativa $\hat{\gamma}_1$.

```python
import numpy as np

# Dados de exemplo
np.random.seed(42)
Y = np.random.randn(100)  # S√©rie temporal aleat√≥ria
T = len(Y)
Y_mean = np.mean(Y)

# Defasagem
j = 1

# Estimar a autocovari√¢ncia
gamma_hat = np.sum((Y[j:] - Y_mean) * (Y[:-j] - Y_mean)) / T

print(f"Estimativa da autocovari√¢ncia na defasagem {j}: {gamma_hat}")
```

### Autocovari√¢ncia em Modelos MA(1)

Para ilustrar o c√°lculo da autocovari√¢ncia, considere um processo Moving Average de primeira ordem (MA(1)) definido como [^3.3.1]:

$$
Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}
$$

onde $\mu$ e $\theta$ s√£o constantes e $\epsilon_t$ √© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$. A m√©dia deste processo √© $E[Y_t] = \mu$ [^3.3.2]. A autocovari√¢ncia $\gamma_0$ (vari√¢ncia) √© dada por [^3.3.3]:

$$
\gamma_0 = E[(Y_t - \mu)^2] = E[(\epsilon_t + \theta \epsilon_{t-1})^2] = (1 + \theta^2)\sigma^2
$$

**Prova de $\gamma_0 = (1 + \theta^2)\sigma^2$:**
I. Dado $Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$.
II. Ent√£o $Y_t - \mu = \epsilon_t + \theta \epsilon_{t-1}$.
III. A vari√¢ncia $\gamma_0 = E[(Y_t - \mu)^2] = E[(\epsilon_t + \theta \epsilon_{t-1})^2]$.
IV. Expandindo, $\gamma_0 = E[\epsilon_t^2 + 2\theta \epsilon_t \epsilon_{t-1} + \theta^2 \epsilon_{t-1}^2]$.
V. Usando a linearidade da esperan√ßa, $\gamma_0 = E[\epsilon_t^2] + 2\theta E[\epsilon_t \epsilon_{t-1}] + \theta^2 E[\epsilon_{t-1}^2]$.
VI. Como $\epsilon_t$ √© ru√≠do branco, $E[\epsilon_t^2] = \sigma^2$, $E[\epsilon_{t-1}^2] = \sigma^2$ e $E[\epsilon_t \epsilon_{t-1}] = 0$.
VII. Portanto, $\gamma_0 = \sigma^2 + 0 + \theta^2 \sigma^2 = (1 + \theta^2)\sigma^2$. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere um processo MA(1) com $\mu = 0$, $\theta = 0.5$ e $\sigma^2 = 1$. Ent√£o, $\gamma_0 = (1 + 0.5^2) \times 1 = 1.25$. Isso significa que a vari√¢ncia do processo MA(1) √© 1.25.

A autocovari√¢ncia na defasagem 1 √© dada por [^3.3.4]:

$$
\gamma_1 = E[(Y_t - \mu)(Y_{t-1} - \mu)] = E[(\epsilon_t + \theta \epsilon_{t-1})(\epsilon_{t-1} + \theta \epsilon_{t-2})] = \theta \sigma^2
$$

**Prova de $\gamma_1 = \theta \sigma^2$:**
I. Dado $Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$.
II. Ent√£o $Y_t - \mu = \epsilon_t + \theta \epsilon_{t-1}$ e $Y_{t-1} - \mu = \epsilon_{t-1} + \theta \epsilon_{t-2}$.
III. A autocovari√¢ncia na defasagem 1 √© $\gamma_1 = E[(Y_t - \mu)(Y_{t-1} - \mu)] = E[(\epsilon_t + \theta \epsilon_{t-1})(\epsilon_{t-1} + \theta \epsilon_{t-2})]$.
IV. Expandindo, $\gamma_1 = E[\epsilon_t \epsilon_{t-1} + \theta \epsilon_t \epsilon_{t-2} + \theta \epsilon_{t-1}^2 + \theta^2 \epsilon_{t-1} \epsilon_{t-2}]$.
V. Usando a linearidade da esperan√ßa, $\gamma_1 = E[\epsilon_t \epsilon_{t-1}] + \theta E[\epsilon_t \epsilon_{t-2}] + \theta E[\epsilon_{t-1}^2] + \theta^2 E[\epsilon_{t-1} \epsilon_{t-2}]$.
VI. Como $\epsilon_t$ √© ru√≠do branco, $E[\epsilon_t \epsilon_{t-1}] = 0$, $E[\epsilon_t \epsilon_{t-2}] = 0$, $E[\epsilon_{t-1}^2] = \sigma^2$ e $E[\epsilon_{t-1} \epsilon_{t-2}] = 0$.
VII. Portanto, $\gamma_1 = 0 + 0 + \theta \sigma^2 + 0 = \theta \sigma^2$. ‚ñ†

> üí° **Exemplo Num√©rico:** Usando o mesmo processo MA(1) com $\mu = 0$, $\theta = 0.5$ e $\sigma^2 = 1$, temos $\gamma_1 = 0.5 \times 1 = 0.5$.

Para defasagens $j > 1$, a autocovari√¢ncia √© zero [^3.3.5]:

$$
\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\epsilon_t + \theta \epsilon_{t-1})(\epsilon_{t-j} + \theta \epsilon_{t-j-1})] = 0, \quad j > 1
$$

**Prova de $\gamma_j = 0$ para $j > 1$:**
I. Dado $Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$.
II. Ent√£o $Y_t - \mu = \epsilon_t + \theta \epsilon_{t-1}$ e $Y_{t-j} - \mu = \epsilon_{t-j} + \theta \epsilon_{t-j-1}$.
III. A autocovari√¢ncia na defasagem $j$ √© $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\epsilon_t + \theta \epsilon_{t-1})(\epsilon_{t-j} + \theta \epsilon_{t-j-1})]$.
IV. Expandindo, $\gamma_j = E[\epsilon_t \epsilon_{t-j} + \theta \epsilon_t \epsilon_{t-j-1} + \theta \epsilon_{t-1} \epsilon_{t-j} + \theta^2 \epsilon_{t-1} \epsilon_{t-j-1}]$.
V. Usando a linearidade da esperan√ßa, $\gamma_j = E[\epsilon_t \epsilon_{t-j}] + \theta E[\epsilon_t \epsilon_{t-j-1}] + \theta E[\epsilon_{t-1} \epsilon_{t-j}] + \theta^2 E[\epsilon_{t-1} \epsilon_{t-j-1}]$.
VI. Como $\epsilon_t$ √© ru√≠do branco e $j > 1$, todos os termos s√£o zero: $E[\epsilon_t \epsilon_{t-j}] = 0$, $E[\epsilon_t \epsilon_{t-j-1}] = 0$, $E[\epsilon_{t-1} \epsilon_{t-j}] = 0$ e $E[\epsilon_{t-1} \epsilon_{t-j-1}] = 0$.
VII. Portanto, $\gamma_j = 0 + 0 + 0 + 0 = 0$. ‚ñ†

> üí° **Exemplo Num√©rico:** Para o mesmo processo MA(1), $\gamma_2 = \gamma_3 = \dots = 0$. Isso significa que apenas a observa√ß√£o imediatamente anterior afeta o valor atual do processo.

**Corol√°rio 1.1**
Para o processo MA(1) definido acima, o coeficiente de autocorrela√ß√£o na defasagem 1, denotado por $\rho_1$, √© dado por:

$$
\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{\theta}{1 + \theta^2}
$$

*Proof:* O coeficiente de autocorrela√ß√£o √© definido como a autocovari√¢ncia dividida pela vari√¢ncia. Substituindo os valores de $\gamma_1$ e $\gamma_0$ obtidos anteriormente, obtemos o resultado desejado.

**Prova do Corol√°rio 1.1:**
I. O coeficiente de autocorrela√ß√£o na defasagem 1 √© definido como $\rho_1 = \frac{\gamma_1}{\gamma_0}$.
II. Para o processo MA(1), $\gamma_1 = \theta \sigma^2$ e $\gamma_0 = (1 + \theta^2) \sigma^2$.
III. Substituindo, $\rho_1 = \frac{\theta \sigma^2}{(1 + \theta^2) \sigma^2}$.
IV. Simplificando, $\rho_1 = \frac{\theta}{1 + \theta^2}$. ‚ñ†

> üí° **Exemplo Num√©rico:** Para o processo MA(1) com $\theta = 0.5$, o coeficiente de autocorrela√ß√£o na defasagem 1 √© $\rho_1 = \frac{0.5}{1 + 0.5^2} = \frac{0.5}{1.25} = 0.4$. Isso indica uma correla√ß√£o positiva moderada entre observa√ß√µes consecutivas.

```python
import numpy as np
import matplotlib.pyplot as plt

# Par√¢metros do MA(1)
mu = 0
theta = 0.5
sigma = 1
T = 200  # Comprimento da s√©rie temporal

# Gerar ru√≠do branco
np.random.seed(42)
epsilon = np.random.normal(0, sigma, T)

# Gerar processo MA(1)
Y = mu + epsilon[1:] + theta * epsilon[:-1]

# Calcular autocovari√¢ncias teoricas
gamma0 = (1 + theta**2) * sigma**2
gamma1 = theta * sigma**2
rho1 = gamma1 / gamma0

# Calcular autocovari√¢ncias amostrais
def autocovariance(x, lag):
    """Calcula a autocovari√¢ncia amostral para uma dada defasagem."""
    n = len(x)
    x_mean = np.mean(x)
    if lag >= n:
      return 0
    return np.sum((x[:n-lag] - x_mean) * (x[lag:] - x_mean)) / n

lags = np.arange(0, 6)  # Defasagens de 0 a 5
gamma_hat = [autocovariance(Y, lag) for lag in lags]

# Imprimir resultados
print(f"Par√¢metros do MA(1): mu={mu}, theta={theta}, sigma^2={sigma**2}")
print(f"Autocovari√¢ncia te√≥rica Œ≥0: {gamma0}")
print(f"Autocovari√¢ncia te√≥rica Œ≥1: {gamma1}")
print(f"Autocorrela√ß√£o te√≥rica œÅ1: {rho1}")

print("\nAutocovari√¢ncias amostrais:")
for lag, value in zip(lags, gamma_hat):
    print(f"Œ≥{lag}: {value}")

# Plotar a fun√ß√£o de autocorrela√ß√£o (ACF)
plt.figure(figsize=(10, 6))
plt.stem(lags, [g / gamma_hat[0] for g in gamma_hat], basefmt="k-", use_line_collection=True)
plt.title("Fun√ß√£o de Autocorrela√ß√£o (ACF) Amostral do MA(1)")
plt.xlabel("Defasagem (j)")
plt.ylabel("Autocorrela√ß√£o (œÅj)")
plt.grid(True)
plt.show()
```

**Observa√ß√£o Importante**
Como a m√©dia e as autocovari√¢ncias n√£o s√£o fun√ß√µes do tempo, um processo MA(1) √© *covariance-stationary* [^MA(1) estacion√°rio]. Al√©m disso, a condi√ß√£o [^3.1.15] √© satisfeita, o que significa que um processo MA(1) com ru√≠do branco Gaussiano √© *ergodic* para todos os momentos.

### Conclus√£o
A autocovari√¢ncia √© uma ferramenta essencial para caracterizar a estrutura temporal de s√©ries temporais. Ela quantifica a depend√™ncia linear entre observa√ß√µes em diferentes momentos no tempo e fornece insights valiosos sobre a natureza do processo estoc√°stico subjacente.  O estudo da autocovari√¢ncia √© fundamental para a compreens√£o e modelagem de processos ARMA e outros modelos de s√©ries temporais, permitindo an√°lises preditivas e inferenciais mais precisas.

### Refer√™ncias
[^3.1.2]:  *Texto referente √† densidade do ru√≠do branco gaussiano*.
[^3.1.4]:  *Texto referente ao limite de probabilidade da m√©dia do ensemble*.
[^3.1.5]:  *Texto referente ao processo com m√©dia constante e ru√≠do branco Gaussiano*.
[^3.1.6]:  *Texto referente √† m√©dia do processo com m√©dia constante e ru√≠do branco Gaussiano*.
[^3.1.7]:  *Texto referente ao processo com tend√™ncia linear e ru√≠do branco Gaussiano*.
[^3.1.8]:  *Texto referente √† m√©dia do processo com tend√™ncia linear e ru√≠do branco Gaussiano*.
[^3.1.10]: *Texto referente √† defini√ß√£o de autocovari√¢ncia e sua rela√ß√£o com a matriz de vari√¢ncia-covari√¢ncia*.
[^3.1.11]: *Texto referente √† m√©dia do ensemble para calcular autocovari√¢ncias*.
[^3.1.13]: *Texto referente √† propriedade da autocovari√¢ncia em processos estacion√°rios*.
[^3.1.15]: *Texto referente √† condi√ß√£o de ergodicidade*.
[^3.2.3]:  *Texto referente √†s propriedades do ru√≠do branco*.
[^3.3.1]:  *Texto referente √† defini√ß√£o do processo MA(1)*.
[^3.3.2]:  *Texto referente √† m√©dia do processo MA(1)*.
[^3.3.3]:  *Texto referente √† vari√¢ncia do processo MA(1)*.
[^3.3.4]:  *Texto referente √† autocovari√¢ncia na defasagem 1 do processo MA(1)*.
[^3.3.5]:  *Texto referente √† autocovari√¢ncia em defasagens maiores que 1 do processo MA(1)*.
[^Estacionaridade]: *Texto referente √† estacionaridade*.
[^MA(1) estacion√°rio]: *Texto referente √† estacionaridade do processo MA(1)*.
<!-- END -->