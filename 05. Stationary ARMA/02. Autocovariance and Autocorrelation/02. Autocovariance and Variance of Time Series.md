## A Vari√¢ncia como Autocovari√¢ncia de Ordem Zero em Processos ARMA

### Introdu√ß√£o

No contexto de modelos ARMA, a autocovari√¢ncia de ordem zero, denotada por $\gamma_{0t}$, desempenha um papel fundamental, pois ela coincide com a vari√¢ncia do processo no instante $t$ [^Autocovari√¢ncia (Œ≥jt) measures the covariance between Yt and its lagged value Yt-j, calculated as Œ≥jt = E[(Yt - Œºt)(Yt-j - Œºt-j)]. It can be viewed as the (1, j+1) element of the variance-covariance matrix of the vector xt.]. A vari√¢ncia, por sua vez, quantifica a dispers√£o dos valores da s√©rie temporal em torno de sua m√©dia. Este cap√≠tulo explora em profundidade o significado da autocovari√¢ncia de ordem zero, suas propriedades, e como ela se manifesta em diferentes processos estoc√°sticos, especialmente no contexto de modelos ARMA.

### Conceitos Fundamentais

**Defini√ß√£o Formal**

A autocovari√¢ncia de ordem zero, $\gamma_{0t}$, √© definida como a covari√¢ncia de $Y_t$ consigo mesmo, ou seja [^3.1.9]:

$$
\gamma_{0t} = E[(Y_t - \mu_t)(Y_t - \mu_t)] = E[(Y_t - \mu_t)^2]
$$

onde $E[\cdot]$ representa o operador de esperan√ßa, $Y_t$ √© o valor da s√©rie temporal no instante $t$, e $\mu_t = E[Y_t]$ √© a m√©dia da s√©rie temporal no instante $t$. Esta express√£o √©, por defini√ß√£o, a vari√¢ncia de $Y_t$.

**Interpreta√ß√£o da Vari√¢ncia**

A vari√¢ncia $\gamma_{0t}$ quantifica o grau de dispers√£o ou variabilidade dos valores da s√©rie temporal em torno de sua m√©dia $\mu_t$. Uma vari√¢ncia alta indica que os valores da s√©rie temporal tendem a se desviar significativamente da m√©dia, enquanto uma vari√¢ncia baixa indica que os valores est√£o mais concentrados em torno da m√©dia.

> üí° **Exemplo Num√©rico:** Suponha que $\gamma_{0t} = 10$ para uma s√©rie temporal. Isso indica que os valores da s√©rie temporal exibem uma dispers√£o consider√°vel em torno de sua m√©dia. Em contraste, se $\gamma_{0t} = 0.5$, os valores da s√©rie temporal est√£o muito mais concentrados em torno da m√©dia. Imagine uma s√©rie temporal representando a temperatura di√°ria em uma cidade. Se $\gamma_{0t} = 10$, as temperaturas variam amplamente ao longo dos dias (por exemplo, de 10¬∞C a 30¬∞C, com m√©dia de 20¬∞C). Se $\gamma_{0t} = 0.5$, as temperaturas s√£o mais est√°veis (por exemplo, de 19¬∞C a 21¬∞C, com m√©dia de 20¬∞C).

**Rela√ß√£o com a Estacionaridade**

Em um processo *covariance-stationary*, a vari√¢ncia, assim como a m√©dia e as autocovari√¢ncias, √© constante ao longo do tempo. Isso significa que [^Estacionaridade]:

$$
\gamma_{0t} = \gamma_0 \quad \text{para todo } t
$$

A estacionaridade implica que a dispers√£o dos valores da s√©rie temporal em torno de sua m√©dia n√£o muda ao longo do tempo.

**Proposi√ß√£o 2**
Para um processo estacion√°rio, a autocovari√¢ncia de ordem zero ($\gamma_0$) √© sempre maior ou igual a zero.

*Proof:*
Pela defini√ß√£o de vari√¢ncia, $\gamma_0 = E[(Y_t - \mu)^2]$. Como $(Y_t - \mu)^2$ √© um quadrado, √© sempre n√£o negativo. A esperan√ßa de uma quantidade n√£o negativa √© tamb√©m n√£o negativa. Portanto, $\gamma_0 \geq 0$. $\blacksquare$

**Teorema 3**
Se $Y_t$ √© um processo estacion√°rio com m√©dia $\mu$, ent√£o $E[(Y_t - \mu)^4]$ √© constante ao longo do tempo.

*Proof:*
Como $Y_t$ √© estacion√°rio, a distribui√ß√£o de $Y_t$ √© a mesma para todo $t$. Portanto, todos os momentos de $Y_t$ s√£o constantes ao longo do tempo, incluindo $E[(Y_t - \mu)^4]$. $\blacksquare$

**Exemplo: Ru√≠do Branco Gaussiano**

Em um processo de ru√≠do branco Gaussiano com m√©dia zero e vari√¢ncia $\sigma^2$, a autocovari√¢ncia de ordem zero √© simplesmente a vari√¢ncia $\sigma^2$.  Como as observa√ß√µes s√£o independentes e identicamente distribu√≠das, a vari√¢ncia permanece constante ao longo do tempo. Para ru√≠do branco, a autocovari√¢ncia √© zero para todas as outras defasagens [^3.2.3].

> üí° **Exemplo Num√©rico:** Se um processo de ru√≠do branco Gaussiano tem vari√¢ncia $\sigma^2 = 4$, ent√£o $\gamma_0 = 4$. Isso significa que os valores da s√©rie temporal se desviam em m√©dia por 2 unidades (raiz quadrada da vari√¢ncia) de sua m√©dia, que √© zero.  Podemos simular esse processo em Python e verificar a vari√¢ncia amostral:

```python
import numpy as np

# Define a semente para reprodutibilidade
np.random.seed(0)

# N√∫mero de observa√ß√µes
n_observations = 1000

# Vari√¢ncia do ru√≠do branco
variance = 4

# Gera ru√≠do branco Gaussiano
white_noise = np.random.normal(loc=0, scale=np.sqrt(variance), size=n_observations)

# Calcula a vari√¢ncia amostral
sample_variance = np.var(white_noise)

print(f"Vari√¢ncia te√≥rica: {variance}")
print(f"Vari√¢ncia amostral: {sample_variance}")
```

**Exemplo: Processo com Tend√™ncia Linear e Ru√≠do Branco Gaussiano**

Considere o processo n√£o estacion√°rio $Y_t = \beta t + \epsilon_t$, onde $\beta$ √© uma constante e $\epsilon_t$ √© ru√≠do branco Gaussiano com vari√¢ncia $\sigma^2$ [^3.1.7]. A vari√¢ncia de $Y_t$ √©:

$$
\gamma_{0t} = E[(Y_t - \beta t)^2] = E[\epsilon_t^2] = \sigma^2
$$

Neste caso, embora a vari√¢ncia seja constante ao longo do tempo, o processo *n√£o* √© estacion√°rio porque a m√©dia $E[Y_t] = \beta t$ varia com $t$ [^3.1.8].  A autocovari√¢ncia de ordem zero (vari√¢ncia) deste processo √© constante, mas a m√©dia n√£o, violando a defini√ß√£o de estacionaridade.

> üí° **Exemplo Num√©rico:** Seja $\beta = 0.5$ e $\sigma^2 = 1$.  Simulemos esse processo e visualizemos a s√©rie temporal e sua m√©dia vari√°vel.

![Generated plot](./../images/plot_0.png)

A autocovari√¢ncia de ordem zero (vari√¢ncia) ser√° aproximadamente igual a 1, mas a m√©dia da s√©rie temporal crescer√° linearmente com o tempo, demonstrando a n√£o estacionaridade.

**Proposi√ß√£o 3.1**
Considere o processo n√£o estacion√°rio $Y_t = f(t) + \epsilon_t$, onde $f(t)$ √© uma fun√ß√£o de $t$ e $\epsilon_t$ √© ru√≠do branco Gaussiano com vari√¢ncia $\sigma^2$.  A vari√¢ncia de $Y_t$ √© $\gamma_{0t} = \sigma^2$, que √© constante, mas o processo n√£o √© estacion√°rio se $f(t)$ n√£o for constante.

*Proof:*
A vari√¢ncia de $Y_t$ √© dada por $E[(Y_t - E[Y_t])^2] = E[(f(t) + \epsilon_t - f(t))^2] = E[\epsilon_t^2] = \sigma^2$.  Se $f(t)$ n√£o √© constante, ent√£o $E[Y_t] = f(t)$ tamb√©m n√£o √© constante, e portanto o processo n√£o √© estacion√°rio. $\blacksquare$

**Exemplo: Processo MA(1)**

Considere o processo MA(1) definido como [^3.3.1]:

$$
Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}
$$

onde $\mu$ e $\theta$ s√£o constantes e $\epsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$. A autocovari√¢ncia de ordem zero (vari√¢ncia) √© dada por [^3.3.3]:

$$
\gamma_0 = E[(Y_t - \mu)^2] = (1 + \theta^2)\sigma^2
$$

Neste caso, a vari√¢ncia √© constante e depende apenas dos par√¢metros $\theta$ e $\sigma^2$.  Como mencionado, a independ√™ncia do tempo √© consistente com o processo ser *covariance-stationary*.

> üí° **Exemplo Num√©rico:** Para um processo MA(1) com $\theta = 0.8$ e $\sigma^2 = 1$, a autocovari√¢ncia de ordem zero (vari√¢ncia) √© $\gamma_0 = (1 + 0.8^2) \times 1 = 1.64$. Podemos simular esse processo e estimar a vari√¢ncia amostral:

```python
import numpy as np

# Define a semente para reprodutibilidade
np.random.seed(0)

# N√∫mero de observa√ß√µes
n_observations = 1000

# Par√¢metros
mu = 0
theta = 0.8
sigma_squared = 1

# Gera ru√≠do branco Gaussiano
epsilon = np.random.normal(loc=0, scale=np.sqrt(sigma_squared), size=n_observations)

# Cria a s√©rie temporal MA(1)
Y = mu + epsilon[1:] + theta * epsilon[:-1]

# Calcula a vari√¢ncia amostral
sample_variance = np.var(Y)

print(f"Vari√¢ncia te√≥rica: {(1 + theta**2) * sigma_squared}")
print(f"Vari√¢ncia amostral: {sample_variance}")
```

**Lema 2**

Se $Y_t$ √© um processo *covariance-stationary*, ent√£o a autocovari√¢ncia de ordem zero ($\gamma_0$) √© igual √† vari√¢ncia incondicional de $Y_t$.

*Proof:*
Pela defini√ß√£o de estacionaridade, $\mu_t = \mu$ para todo $t$. Portanto, $\gamma_{0t} = E[(Y_t - \mu)^2]$ √© a vari√¢ncia de $Y_t$, que √© constante e igual a $\gamma_0$. $\blacksquare$

**Teorema 4**
Para um processo AR(1) definido por $Y_t = \phi Y_{t-1} + \epsilon_t$, onde $|\phi| < 1$ e $\epsilon_t$ √© ru√≠do branco com vari√¢ncia $\sigma^2$, a autocovari√¢ncia de ordem zero √© dada por $\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$.

*Proof:*
Provaremos que para um processo AR(1) definido por $Y_t = \phi Y_{t-1} + \epsilon_t$, onde $|\phi| < 1$ e $\epsilon_t$ √© ru√≠do branco com vari√¢ncia $\sigma^2$, a autocovari√¢ncia de ordem zero √© dada por $\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$.

I. Tomando a vari√¢ncia de ambos os lados da equa√ß√£o $Y_t = \phi Y_{t-1} + \epsilon_t$, temos:
   $Var(Y_t) = Var(\phi Y_{t-1} + \epsilon_t)$

II. Assumindo estacionaridade, $Var(Y_t) = Var(Y_{t-1}) = \gamma_0$. Como $Y_{t-1}$ e $\epsilon_t$ s√£o independentes:
   $\gamma_0 = \phi^2 \gamma_0 + \sigma^2$

III. Resolvendo para $\gamma_0$:
    $\gamma_0(1 - \phi^2) = \sigma^2$

IV. Portanto, a autocovari√¢ncia de ordem zero √©:
    $\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$  $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um processo AR(1) com $\phi = 0.5$ e $\sigma^2 = 2$. A autocovari√¢ncia de ordem zero te√≥rica √© $\gamma_0 = \frac{2}{1 - 0.5^2} = \frac{2}{0.75} = 2.6667$. Simulemos esse processo e comparemos com a estimativa amostral.

```python
import numpy as np

# Define a semente para reprodutibilidade
np.random.seed(0)

# N√∫mero de observa√ß√µes
n_observations = 1000

# Par√¢metros
phi = 0.5
sigma_squared = 2

# Gera ru√≠do branco Gaussiano
epsilon = np.random.normal(loc=0, scale=np.sqrt(sigma_squared), size=n_observations)

# Inicializa a s√©rie temporal AR(1)
Y = np.zeros(n_observations)
Y[0] = epsilon[0]  # Define o primeiro valor como ru√≠do branco

# Cria a s√©rie temporal AR(1)
for t in range(1, n_observations):
    Y[t] = phi * Y[t-1] + epsilon[t]

# Calcula a vari√¢ncia amostral
sample_variance = np.var(Y)

print(f"Vari√¢ncia te√≥rica: {sigma_squared / (1 - phi**2)}")
print(f"Vari√¢ncia amostral: {sample_variance}")
```

**C√°lculo Emp√≠rico da Autocovari√¢ncia de Ordem Zero**

Em aplica√ß√µes pr√°ticas, a autocovari√¢ncia de ordem zero (vari√¢ncia) pode ser estimada a partir de uma amostra de dados usando a seguinte f√≥rmula:

$$
\hat{\gamma}_0 = \frac{1}{T} \sum_{t=1}^{T} (Y_t - \bar{Y})^2
$$

onde $T$ √© o n√∫mero de observa√ß√µes na amostra e $\bar{Y}$ √© a m√©dia amostral:

$$
\bar{Y} = \frac{1}{T} \sum_{t=1}^{T} Y_t
$$

Esta estimativa fornece uma medida da dispers√£o dos dados em torno da m√©dia amostral.

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal com os seguintes valores: [2, 4, 6, 8, 10]. Primeiro, calculamos a m√©dia amostral: $\bar{Y} = (2 + 4 + 6 + 8 + 10) / 5 = 6$. Em seguida, calculamos a autocovari√¢ncia de ordem zero:
>
> $$
> \hat{\gamma}_0 = \frac{1}{5} [(2-6)^2 + (4-6)^2 + (6-6)^2 + (8-6)^2 + (10-6)^2] = \frac{1}{5} [16 + 4 + 0 + 4 + 16] = \frac{40}{5} = 8
> $$
>
> Portanto, a estimativa da vari√¢ncia √© 8.

```python
import numpy as np

# Dados de exemplo
Y = np.array([2, 4, 6, 8, 10])
T = len(Y)

# Calcular a m√©dia amostral
Y_mean = np.mean(Y)

# Calcular a autocovari√¢ncia de ordem zero
gamma_hat_0 = np.sum((Y - Y_mean)**2) / T

print(f"M√©dia amostral: {Y_mean}")
print(f"Autocovari√¢ncia de ordem zero (vari√¢ncia) estimada: {gamma_hat_0}")
```

### Conclus√£o

A autocovari√¢ncia de ordem zero, equivalente √† vari√¢ncia, √© uma estat√≠stica fundamental na an√°lise de s√©ries temporais e modelos ARMA. Ela quantifica a dispers√£o dos dados em torno da m√©dia e, em conjunto com outras autocovari√¢ncias, auxilia na caracteriza√ß√£o da estrutura temporal do processo estoc√°stico. Em processos estacion√°rios, a vari√¢ncia √© constante ao longo do tempo, simplificando a an√°lise e modelagem. O c√°lculo emp√≠rico da vari√¢ncia permite estimar a dispers√£o dos dados a partir de uma amostra finita, fornecendo informa√ß√µes valiosas para a compreens√£o do comportamento da s√©rie temporal.

### Refer√™ncias
[^3.1.7]:  *Texto referente ao processo com tend√™ncia linear e ru√≠do branco Gaussiano*.
[^3.1.8]:  *Texto referente √† m√©dia do processo com tend√™ncia linear e ru√≠do branco Gaussiano*.
[^3.1.9]: *Texto referente √† defini√ß√£o de autocovari√¢ncia*.
[^3.2.3]:  *Texto referente √†s propriedades do ru√≠do branco*.
[^3.3.1]:  *Texto referente √† defini√ß√£o do processo MA(1)*.
[^3.3.3]:  *Texto referente √† vari√¢ncia do processo MA(1)*.
[^Autocovari√¢ncia (Œ≥jt) measures the covariance between Yt and its lagged value Yt-j, calculated as Œ≥jt = E[(Yt - Œºt)(Yt-j - Œºt-j)]. It can be viewed as the (1, j+1) element of the variance-covariance matrix of the vector xt.]:  *Texto referente √† defini√ß√£o da autocovari√¢ncia*.
[^Estacionaridade]: *Texto referente √† estacionaridade*.
<!-- END -->