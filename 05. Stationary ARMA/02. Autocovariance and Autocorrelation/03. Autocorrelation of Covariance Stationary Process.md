## Autocorrela√ß√£o em Processos Estacion√°rios ARMA

### Introdu√ß√£o

A autocorrela√ß√£o, denotada por $\rho_j$, √© uma medida de depend√™ncia linear entre uma s√©rie temporal e suas vers√µes defasadas, normalizada pela vari√¢ncia do processo. Em processos estacion√°rios, a autocorrela√ß√£o de ordem $j$ quantifica a similaridade entre $Y_t$ e $Y_{t-j}$, independentemente do tempo $t$. Este cap√≠tulo explora a autocorrela√ß√£o, suas propriedades, sua rela√ß√£o com a autocovari√¢ncia, e como ela se manifesta em diferentes processos estoc√°sticos, com √™nfase em modelos ARMA.

### Conceitos Fundamentais

**Defini√ß√£o Formal**

A autocorrela√ß√£o de ordem $j$, denotada por $\rho_j$, √© definida como a autocovari√¢ncia de ordem $j$ dividida pela vari√¢ncia do processo [^3.3.6]:

$$
\rho_j = \frac{\gamma_j}{\gamma_0}
$$

onde $\gamma_j$ √© a autocovari√¢ncia na defasagem $j$ e $\gamma_0$ √© a autocovari√¢ncia de ordem zero (vari√¢ncia) [^A Vari√¢ncia como Autocovari√¢ncia de Ordem Zero em Processos ARMA].

**Interpreta√ß√£o da Autocorrela√ß√£o**

A autocorrela√ß√£o $\rho_j$ quantifica o grau de correla√ß√£o linear entre $Y_t$ e $Y_{t-j}$, variando entre -1 e 1. Um valor de $\rho_j$ pr√≥ximo de 1 indica uma forte correla√ß√£o positiva, onde valores altos (ou baixos) de $Y_t$ tendem a ser seguidos por valores altos (ou baixos) de $Y_{t-j}$. Um valor de $\rho_j$ pr√≥ximo de -1 indica uma forte correla√ß√£o negativa, onde valores altos de $Y_t$ tendem a ser seguidos por valores baixos de $Y_{t-j}$, e vice-versa. Um valor de $\rho_j$ pr√≥ximo de 0 indica uma fraca correla√ß√£o linear.

> üí° **Exemplo Num√©rico:** Se $\rho_1 = 0.7$ para uma s√©rie temporal, existe uma correla√ß√£o positiva forte entre observa√ß√µes consecutivas. Se observamos um valor acima da m√©dia no tempo $t$, √© prov√°vel que o valor no tempo $t-1$ tamb√©m estivesse acima da m√©dia. Se $\rho_2 = -0.6$, h√° uma correla√ß√£o negativa moderada entre observa√ß√µes separadas por dois per√≠odos.

**Propriedades da Autocorrela√ß√£o**

1.  **Simetria:** Para um processo estacion√°rio, a fun√ß√£o de autocorrela√ß√£o √© sim√©trica, ou seja, $\rho_j = \rho_{-j}$ [^3.1.13].

2.  **Valor em zero:** A autocorrela√ß√£o na defasagem zero √© sempre igual a 1, ou seja, $\rho_0 = 1$ [^3.3.6]. Isso decorre da defini√ß√£o, pois $\rho_0 = \frac{\gamma_0}{\gamma_0} = 1$.

3.  **Limites:** A autocorrela√ß√£o varia entre -1 e 1, ou seja, $|\rho_j| \leq 1$ para todo $j$ [^3.3.6]. Isso √© uma consequ√™ncia da desigualdade de Cauchy-Schwarz.

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal de temperaturas di√°rias. A autocorrela√ß√£o $\rho_1$ (correla√ß√£o entre a temperatura de hoje e a de ontem) provavelmente ser√° alta e positiva (perto de 1) porque dias quentes tendem a ser seguidos por dias quentes, e dias frios por dias frios. A autocorrela√ß√£o $\rho_{180}$ (correla√ß√£o entre a temperatura de hoje e a de 180 dias atr√°s) pode ser negativa (perto de -1) se houver uma forte sazonalidade invertida (ver√£o x inverno).

**Prova da Simetria da Autocorrela√ß√£o:**
I. Pela defini√ß√£o, $\rho_j = \frac{\gamma_j}{\gamma_0}$ e $\rho_{-j} = \frac{\gamma_{-j}}{\gamma_0}$.
II. Para um processo estacion√°rio, $\gamma_j = \gamma_{-j}$.
III. Portanto, $\rho_j = \frac{\gamma_j}{\gamma_0} = \frac{\gamma_{-j}}{\gamma_0} = \rho_{-j}$. $\blacksquare$

**Prova de $\rho_0 = 1$:**
I. Pela defini√ß√£o, $\rho_0 = \frac{\gamma_0}{\gamma_0}$.
II. Como $\gamma_0$ √© a vari√¢ncia do processo, $\gamma_0 \neq 0$.
III. Portanto, $\rho_0 = \frac{\gamma_0}{\gamma_0} = 1$. $\blacksquare$

**Prova de $|\rho_j| \leq 1$:**
I. A correla√ß√£o √© definida como $Corr(Y_t, Y_{t-j}) = \frac{Cov(Y_t, Y_{t-j})}{\sqrt{Var(Y_t)Var(Y_{t-j})}}$.
II. Para um processo estacion√°rio, $Var(Y_t) = Var(Y_{t-j}) = \gamma_0$, e $Cov(Y_t, Y_{t-j}) = \gamma_j$.
III. Ent√£o $\rho_j = \frac{\gamma_j}{\sqrt{\gamma_0\gamma_0}} = \frac{\gamma_j}{\gamma_0}$.
IV. Pela desigualdade de Cauchy-Schwarz, $|Cov(Y_t, Y_{t-j})| \leq \sqrt{Var(Y_t)Var(Y_{t-j})}$.
V. Portanto, $|\gamma_j| \leq \gamma_0$, e $|\frac{\gamma_j}{\gamma_0}| \leq 1$, implicando $|\rho_j| \leq 1$. $\blacksquare$

**Rela√ß√£o com a Autocovari√¢ncia**

A autocorrela√ß√£o √© simplesmente uma vers√£o normalizada da autocovari√¢ncia. Ela facilita a compara√ß√£o da depend√™ncia temporal entre diferentes s√©ries temporais, pois a normaliza√ß√£o remove a influ√™ncia da escala da s√©rie. Conhecendo a vari√¢ncia e a autocorrela√ß√£o, podemos reconstruir a autocovari√¢ncia:

$$
\gamma_j = \rho_j \gamma_0
$$

> üí° **Exemplo Num√©rico:** Se a vari√¢ncia de uma s√©rie temporal ( $\gamma_0$) √© 10 e a autocorrela√ß√£o na defasagem 2 ( $\rho_2$) √© 0.4, ent√£o a autocovari√¢ncia na defasagem 2 ( $\gamma_2$) √© $0.4 \times 10 = 4$.

**Lema 1**
Dado um processo estacion√°rio com autocovari√¢ncia $\gamma_j$ e autocorrela√ß√£o $\rho_j$, a vari√¢ncia pode ser expressa em termos da autocorrela√ß√£o:
$$
\gamma_0 = \frac{\gamma_j}{\rho_j}, \text{ para } \rho_j \neq 0
$$

*Proof:*
I. Por defini√ß√£o, $\rho_j = \frac{\gamma_j}{\gamma_0}$.
II. Multiplicando ambos os lados por $\gamma_0$ e dividindo por $\rho_j$ (assumindo $\rho_j \neq 0$), obtemos $\gamma_0 = \frac{\gamma_j}{\rho_j}$. $\blacksquare$

**Exemplo: Ru√≠do Branco Gaussiano**

Para um processo de ru√≠do branco Gaussiano, a autocorrela√ß√£o √© zero para todas as defasagens diferentes de zero, e 1 para a defasagem zero. Isso significa que as observa√ß√µes s√£o n√£o correlacionadas.

> üí° **Exemplo Num√©rico:** Se temos um ru√≠do branco Gaussiano, ent√£o $\rho_0 = 1$ e $\rho_j = 0$ para $j \neq 0$. Isso implica que n√£o h√° depend√™ncia linear entre observa√ß√µes em diferentes momentos.

**Exemplo: Processo MA(1)**

Considere o processo MA(1) definido como [^3.3.1]:

$$
Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}
$$

A autocorrela√ß√£o na defasagem 1 √© dada por [^3.3.7]:

$$
\rho_1 = \frac{\theta}{1 + \theta^2}
$$

e a autocorrela√ß√£o √© zero para todas as outras defasagens maiores que 1 [^3.3.5].

Para fins de completude, vamos derivar esta autocorrela√ß√£o.

**Deriva√ß√£o de $\rho_1$ para MA(1):**
I. Dado o processo MA(1): $Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$.
II. Assumimos que $E[\epsilon_t] = 0$ e $Var(\epsilon_t) = \sigma^2$.
III. Primeiro, calculamos a autocovari√¢ncia $\gamma_0 = Var(Y_t)$:
   $$\gamma_0 = E[(Y_t - \mu)^2] = E[(\epsilon_t + \theta \epsilon_{t-1})^2] = E[\epsilon_t^2 + 2\theta \epsilon_t \epsilon_{t-1} + \theta^2 \epsilon_{t-1}^2]$$
   Como $E[\epsilon_t \epsilon_{t-1}] = 0$, temos:
   $$\gamma_0 = E[\epsilon_t^2] + \theta^2 E[\epsilon_{t-1}^2] = \sigma^2 + \theta^2 \sigma^2 = (1 + \theta^2)\sigma^2$$
IV. Agora, calculamos a autocovari√¢ncia $\gamma_1 = Cov(Y_t, Y_{t-1})$:
   $$\gamma_1 = E[(Y_t - \mu)(Y_{t-1} - \mu)] = E[(\epsilon_t + \theta \epsilon_{t-1})(\epsilon_{t-1} + \theta \epsilon_{t-2})]$$
   $$\gamma_1 = E[\epsilon_t \epsilon_{t-1} + \theta \epsilon_{t-1}^2 + \theta \epsilon_t \epsilon_{t-2} + \theta^2 \epsilon_{t-1} \epsilon_{t-2}] = \theta E[\epsilon_{t-1}^2] = \theta \sigma^2$$
V. Finalmente, calculamos a autocorrela√ß√£o $\rho_1$:
   $$\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{\theta \sigma^2}{(1 + \theta^2)\sigma^2} = \frac{\theta}{1 + \theta^2}$$  $\blacksquare$

> üí° **Exemplo Num√©rico:** Para um processo MA(1) com $\theta = 0.5$, temos $\rho_1 = \frac{0.5}{1 + 0.5^2} = 0.4$, e $\rho_j = 0$ para $j > 1$.
Para um processo MA(1) com $\theta = 1$, temos $\rho_1 = \frac{1}{1 + 1^2} = 0.5$, e $\rho_j = 0$ para $j > 1$.
Para um processo MA(1) com $\theta = -0.5$, temos $\rho_1 = \frac{-0.5}{1 + (-0.5)^2} = -0.4$, e $\rho_j = 0$ para $j > 1$.
Para um processo MA(1) com $\theta = -1$, temos $\rho_1 = \frac{-1}{1 + (-1)^2} = -0.5$, e $\rho_j = 0$ para $j > 1$.

**Proposi√ß√£o 4**

Para um processo MA(1), a autocorrela√ß√£o na defasagem 1 √© sempre menor ou igual a 0.5 em valor absoluto, ou seja, $|\rho_1| \leq 0.5$.

*Proof:* Para um processo MA(1), $\rho_1 = \frac{\theta}{1 + \theta^2}$. Para encontrar o m√°ximo valor de $\rho_1$, podemos derivar em rela√ß√£o a $\theta$ e igualar a zero:

$$
\frac{d\rho_1}{d\theta} = \frac{(1 + \theta^2) - \theta(2\theta)}{(1 + \theta^2)^2} = \frac{1 - \theta^2}{(1 + \theta^2)^2}
$$

Igualando a zero, obtemos $1 - \theta^2 = 0$, ou $\theta = \pm 1$. Substituindo $\theta = 1$ na equa√ß√£o de $\rho_1$, temos $\rho_1 = \frac{1}{1 + 1^2} = 0.5$. Substituindo $\theta = -1$, temos $\rho_1 = \frac{-1}{1 + (-1)^2} = -0.5$. Portanto, o valor m√°ximo de $|\rho_1|$ √© 0.5. $\blacksquare$

**Lema 2**
Para o processo MA(1) definido anteriormente, a fun√ß√£o de autocorrela√ß√£o (ACF) corta ap√≥s a defasagem 1.

*Proof:*
I. Por defini√ß√£o do processo MA(1), $Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$.
II. A autocorrela√ß√£o $\rho_j$ mede a correla√ß√£o entre $Y_t$ e $Y_{t-j}$.
III. Para $j > 1$, $Y_t$ e $Y_{t-j}$ n√£o compartilham nenhum termo $\epsilon$, pois $\epsilon_t$ s√£o independentes.
IV. Portanto, $Cov(Y_t, Y_{t-j}) = 0$ para $j > 1$.
V. Consequentemente, $\rho_j = \frac{Cov(Y_t, Y_{t-j})}{Var(Y_t)} = 0$ para $j > 1$.
VI. Isso implica que a ACF corta ap√≥s a defasagem 1. $\blacksquare$

**Exemplo: Processo AR(1)**

Considere um processo AR(1) definido como:

$$
Y_t = c + \phi Y_{t-1} + \epsilon_t
$$

onde $c$ e $\phi$ s√£o constantes e $\epsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$. A autocorrela√ß√£o na defasagem $j$ √© dada por [^3.4.6]:

$$
\rho_j = \phi^j
$$

Nesse caso, a autocorrela√ß√£o decai geometricamente com a defasagem $j$. Se $|\phi| < 1$, o processo √© estacion√°rio e a autocorrela√ß√£o converge para zero √† medida que $j$ aumenta.

Para fins de completude, vamos derivar esta autocorrela√ß√£o.

**Deriva√ß√£o de $\rho_j$ para AR(1):**
I. Dado o processo AR(1): $Y_t = \phi Y_{t-1} + \epsilon_t$ (assumindo $c = 0$ para simplificar, sem perda de generalidade).
II. Multiplicando ambos os lados por $Y_{t-j}$ e tomando a expectativa:
   $$E[Y_t Y_{t-j}] = E[\phi Y_{t-1} Y_{t-j} + \epsilon_t Y_{t-j}]$$
   Como $E[\epsilon_t Y_{t-j}] = 0$ para $j > 0$, temos:
   $$\gamma_j = \phi \gamma_{j-1}$$
III. Dividindo ambos os lados por $\gamma_0$:
   $$\frac{\gamma_j}{\gamma_0} = \phi \frac{\gamma_{j-1}}{\gamma_0}$$
   $$\rho_j = \phi \rho_{j-1}$$
IV. Aplicando recursivamente:
   $$\rho_j = \phi \rho_{j-1} = \phi (\phi \rho_{j-2}) = \phi^2 \rho_{j-2} = \ldots = \phi^j \rho_0$$
V. Como $\rho_0 = 1$, temos:
   $$\rho_j = \phi^j$$  $\blacksquare$

> üí° **Exemplo Num√©rico:** Para um processo AR(1) com $\phi = 0.8$, temos $\rho_1 = 0.8$, $\rho_2 = 0.8^2 = 0.64$, $\rho_3 = 0.8^3 = 0.512$, e assim por diante. A autocorrela√ß√£o decai gradualmente √† medida que a defasagem aumenta.
Para um processo AR(1) com $\phi = -0.5$, temos $\rho_1 = -0.5$, $\rho_2 = (-0.5)^2 = 0.25$, $\rho_3 = (-0.5)^3 = -0.125$, e assim por diante. A autocorrela√ß√£o alterna entre valores positivos e negativos, decaindo em magnitude.

> üí° **Exemplo Num√©rico:** Simula√ß√£o e visualiza√ß√£o da ACF para diferentes valores de $\phi$ em um processo AR(1).

```python
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf

# Define os valores de phi a serem testados
phi_values = [0.3, 0.7, -0.5, -0.9]
lags = 10  # N√∫mero de defasagens para a ACF

# Cria a figura e os subplots
fig, axes = plt.subplots(2, 2, figsize=(12, 8))
axes = axes.flatten()

# Itera sobre os valores de phi e plota a ACF para cada um
for i, phi in enumerate(phi_values):
    # Calcula a ACF teoricamente
    acf_theoretical = [phi**k for k in range(lags + 1)]

    # Plota a ACF te√≥rica
    ax = axes[i]
    ax.stem(range(lags + 1), acf_theoretical, basefmt="k-", use_line_collection=True)
    ax.set_title(f'AR(1) com œÜ = {phi}')
    ax.set_xlabel('Defasagem (k)')
    ax.set_ylabel('Autocorrela√ß√£o')
    ax.set_xlim([-1, lags + 1])
    ax.grid(True)

plt.tight_layout()
plt.show()
```

> üí° **Interpreta√ß√£o:** O c√≥digo acima gera a ACF te√≥rica para diferentes valores de $\phi$ em um processo AR(1). Para valores positivos de $\phi$, a ACF decai gradualmente de forma exponencial. Para valores negativos de $\phi$, a ACF alterna entre valores positivos e negativos, decaindo em magnitude. Quanto maior o valor absoluto de $\phi$, mais lento √© o decaimento da ACF.

**Teorema 5**

Se $Y_t$ √© um processo AR(1) definido por $Y_t = \phi Y_{t-1} + \epsilon_t$, onde $|\phi| < 1$ e $\epsilon_t$ √© ru√≠do branco com vari√¢ncia $\sigma^2$, ent√£o $|\rho_j| \rightarrow 0$ quando $j \rightarrow \infty$.

*Proof:*
I. A autocorrela√ß√£o √© dada por $\rho_j = \phi^j$.
II. Se $|\phi| < 1$, ent√£o $|\phi^j| \rightarrow 0$ quando $j \rightarrow \infty$.
III. Portanto, $|\rho_j| \rightarrow 0$ quando $j \rightarrow \infty$. $\blacksquare$

**Teorema 5.1**

Se $Y_t$ √© um processo AR(1) definido por $Y_t = \phi Y_{t-1} + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco com vari√¢ncia $\sigma^2$, ent√£o a soma infinita das autocorrela√ß√µes converge se $|\phi| < 1$.

*Proof:*
I.  A autocorrela√ß√£o √© dada por $\rho_j = \phi^j$.
II. Queremos mostrar que $\sum_{j=0}^{\infty} |\rho_j|$ converge se $|\phi| < 1$.
III. $\sum_{j=0}^{\infty} |\rho_j| = \sum_{j=0}^{\infty} |\phi^j| = \sum_{j=0}^{\infty} |\phi|^j$.
IV. Esta √© uma s√©rie geom√©trica com raz√£o $|\phi|$. Se $|\phi| < 1$, a s√©rie converge para $\frac{1}{1 - |\phi|}$.
V. Portanto, a soma infinita das autocorrela√ß√µes converge se $|\phi| < 1$. $\blacksquare$

**Fun√ß√£o de Autocorrela√ß√£o (ACF)**

A fun√ß√£o de autocorrela√ß√£o (ACF) √© um gr√°fico que mostra a autocorrela√ß√£o $\rho_j$ em fun√ß√£o da defasagem $j$. A ACF √© uma ferramenta valiosa para identificar a ordem de depend√™ncia temporal em uma s√©rie temporal e auxiliar na identifica√ß√£o de modelos ARMA apropriados [^3.4.6].

> üí° **Exemplo Pr√°tico:** Ao analisar a ACF de uma s√©rie temporal, podemos observar um decaimento exponencial das autocorrela√ß√µes, o que sugere um modelo AR. Se a ACF apresentar um corte abrupto ap√≥s uma determinada defasagem, isso sugere um modelo MA.

![Generated plot](./../images/plot_2.png)

> üí° **Exemplo Num√©rico:** Um exemplo de como usar a ACF para diferenciar entre um AR(1) e um MA(1)

![Generated plot](./../images/plot_3.png)

> üí° **Interpreta√ß√£o:** O gr√°fico da ACF para AR(1) mostra um decaimento gradual, enquanto o gr√°fico da ACF para MA(1) mostra um corte ap√≥s a primeira defasagem. Isso demonstra visualmente como a ACF pode ajudar a identificar o tipo de processo gerador da s√©rie temporal.

### Conclus√£o

A autocorrela√ß√£o √© uma estat√≠stica chave na an√°lise de s√©ries temporais estacion√°rias, fornecendo uma medida normalizada da depend√™ncia linear entre observa√ß√µes defasadas. Suas propriedades, como simetria e limites, simplificam a interpreta√ß√£o. A ACF √© uma ferramenta valiosa para identificar padr√µes de depend√™ncia temporal e auxiliar na especifica√ß√£o de modelos ARMA apropriados. A compreens√£o da autocorrela√ß√£o √© essencial para modelagem, previs√£o e an√°lise inferencial de s√©ries temporais.

### Refer√™ncias

[^3.1.13]: *Texto referente √† propriedade da autocovari√¢ncia em processos estacion√°rios*.
[^3.3.1]: *Texto referente √† defini√ß√£o do processo MA(1)*.
[^3.3.5]: *Texto referente √† autocovari√¢ncia em defasagens maiores que 1 do processo MA(1)*.
[^3.3.6]: *Texto referente √† defini√ß√£o de autocorrela√ß√£o*.
[^3.3.7]: *Texto referente √† autocorrela√ß√£o na defasagem 1 do processo MA(1)*.
[^3.4.6]: *Texto referente √† autocorrela√ß√£o para um processo AR(1)*.
[^A Vari√¢ncia como Autocovari√¢ncia de Ordem Zero em Processos ARMA]: *Refer√™ncia ao cap√≠tulo anterior que define e explora a autocovari√¢ncia de ordem zero (vari√¢ncia)*.
<!-- END -->