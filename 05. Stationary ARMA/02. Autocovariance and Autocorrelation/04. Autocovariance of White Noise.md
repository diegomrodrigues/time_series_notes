## Autocovari√¢ncia de Processos Yt = Œº + Œµt

### Introdu√ß√£o
Este cap√≠tulo explora as propriedades de autocovari√¢ncia para um processo espec√≠fico definido como $Y_t = \mu + \epsilon_t$, onde $\mu$ √© uma constante e $\epsilon_t$ representa um processo de ru√≠do branco. Entender a estrutura de autocovari√¢ncia deste processo √© fundamental para a an√°lise de s√©ries temporais, pois serve como um bloco de constru√ß√£o para modelos mais complexos, como os modelos ARMA (Autoregressive Moving Average). Construindo sobre os conceitos de autocovari√¢ncia [^Autocorrela√ß√£o em Processos Estacion√°rios ARMA] e a vari√¢ncia como autocovari√¢ncia de ordem zero [^A Vari√¢ncia como Autocovari√¢ncia de Ordem Zero em Processos ARMA], este cap√≠tulo se concentra em demonstrar que, para este processo particular, as autocovari√¢ncias s√£o zero para todas as defasagens $j \neq 0$.

### Conceitos Fundamentais

**Defini√ß√£o do Processo**
Considere o processo estoc√°stico definido como:

$$
Y_t = \mu + \epsilon_t
$$

onde:
*   $Y_t$ √© o valor do processo no instante $t$
*   $\mu$ √© uma constante
*   $\epsilon_t$ √© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$. Isso significa que $E[\epsilon_t] = 0$ e $E[\epsilon_t \epsilon_\tau] = 0$ para $t \neq \tau$ e $E[\epsilon_t^2] = \sigma^2$ [^3.2.3].

> üí° **Exemplo Num√©rico:**
> Suponha que temos $\mu = 10$ e $\epsilon_t$ √© um ru√≠do branco com $\sigma^2 = 4$. Isso significa que o processo $Y_t$ flutua em torno de 10, com a varia√ß√£o dada pela vari√¢ncia do ru√≠do branco, que √© 4.

**M√©dia do Processo**
A m√©dia do processo $Y_t$ √© dada por:

$$
E[Y_t] = E[\mu + \epsilon_t] = \mu + E[\epsilon_t] = \mu + 0 = \mu
$$

Portanto, a m√©dia do processo √© constante e igual a $\mu$.

> üí° **Exemplo Num√©rico:**
> Se $\mu = 10$, ent√£o $E[Y_t] = 10$ para todo $t$. Isso significa que, em m√©dia, o valor de $Y_t$ ser√° sempre 10.

**Autocovari√¢ncia de Ordem Zero (Vari√¢ncia)**
A autocovari√¢ncia de ordem zero, que √© a vari√¢ncia do processo, √© dada por [^A Vari√¢ncia como Autocovari√¢ncia de Ordem Zero em Processos ARMA]:

$$
\gamma_0 = E[(Y_t - \mu)^2] = E[(\mu + \epsilon_t - \mu)^2] = E[\epsilon_t^2] = \sigma^2
$$

A vari√¢ncia do processo √© igual √† vari√¢ncia do ru√≠do branco $\epsilon_t$.

> üí° **Exemplo Num√©rico:**
> Se a vari√¢ncia do ru√≠do branco √© $\sigma^2 = 4$, ent√£o $\gamma_0 = 4$. Isso indica a dispers√£o dos valores de $Y_t$ em torno da sua m√©dia.

**Autocovari√¢ncia para *j* ‚â† 0**
Para defasagens $j \neq 0$, a autocovari√¢ncia √© dada por:

$$
\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\epsilon_t)(\epsilon_{t-j})]
$$

Como $\epsilon_t$ √© um processo de ru√≠do branco, $E[\epsilon_t \epsilon_{t-j}] = 0$ para $j \neq 0$ [^3.2.3]. Portanto,

$$
\gamma_j = 0, \quad \text{para } j \neq 0
$$

> üí° **Exemplo Num√©rico:**
> Considere $j = 1$. Ent√£o, $\gamma_1 = E[(\epsilon_t)(\epsilon_{t-1})] = 0$, pois o ru√≠do branco n√£o tem correla√ß√£o serial. Isso significa que o valor de $Y_t$ n√£o est√° linearmente relacionado ao valor de $Y_{t-1}$.

**Teorema 6**
Para o processo $Y_t = \mu + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco com vari√¢ncia $\sigma^2$, as autocovari√¢ncias s√£o dadas por:

$$
\gamma_j = \begin{cases}
\sigma^2, & \text{se } j = 0 \\
0, & \text{se } j \neq 0
\end{cases}
$$

*Proof:*
I. A vari√¢ncia (autocovari√¢ncia de ordem zero) √© $\gamma_0 = E[(Y_t - \mu)^2] = E[\epsilon_t^2] = \sigma^2$.
II. Para $j \neq 0$, a autocovari√¢ncia √© $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[\epsilon_t \epsilon_{t-j}]$.
III. Como $\epsilon_t$ √© ru√≠do branco, $E[\epsilon_t \epsilon_{t-j}] = 0$ para $j \neq 0$.
IV. Portanto, $\gamma_j = 0$ para $j \neq 0$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
Considere um processo $Y_t = 5 + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco Gaussiano com m√©dia 0 e vari√¢ncia 2. Ent√£o, $\gamma_0 = 2$ e $\gamma_j = 0$ para todo $j \neq 0$. A m√©dia do processo √© $E[Y_t] = 5$.

> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> mu = 5
> sigma_sq = 2
>
> # Simula√ß√£o de ru√≠do branco
> np.random.seed(0)  # para reprodutibilidade
> epsilon = np.random.normal(0, np.sqrt(sigma_sq), 100)
>
> # Simula√ß√£o do processo Y_t
> Y = mu + epsilon
>
> # C√°lculo da autocovari√¢ncia amostral
> def autocovariance(x, lag):
>     n = len(x)
>     x_mean = np.mean(x)
>     if lag == 0:
>         return np.mean((x - x_mean) ** 2)
>     else:
>         return np.mean((x[:-lag] - x_mean) * (x[lag:] - x_mean))
>
> lags = np.arange(10)
> autocovariances = [autocovariance(Y, lag) for lag in lags]
>
> # Plot da s√©rie temporal
> plt.figure(figsize=(10, 6))
> plt.plot(Y)
> plt.axhline(mu, color='r', linestyle='--', label='M√©dia')
> plt.title("S√©rie Temporal Y_t = Œº + Œµ_t")
> plt.xlabel("Tempo (t)")
> plt.ylabel("Valor de Y_t")
> plt.legend()
> plt.grid(True)
> plt.show()
>
> # Plot da autocovari√¢ncia amostral
> plt.figure(figsize=(10, 6))
> plt.stem(lags, autocovariances, use_line_collection=True)
> plt.title("Autocovari√¢ncia Amostral de Y_t")
> plt.xlabel("Lag (j)")
> plt.ylabel("Autocovari√¢ncia (Œ≥_j)")
> plt.grid(True)
> plt.show()
> ```
>
> Os gr√°ficos mostram a s√©rie temporal simulada e sua autocovari√¢ncia amostral. A autocovari√¢ncia no lag 0 deve estar pr√≥xima da vari√¢ncia do ru√≠do branco (2), e as autocovari√¢ncias para outros lags devem estar pr√≥ximas de zero.

**Autocorrela√ß√£o**
A autocorrela√ß√£o, $\rho_j$, √© definida como [^Autocorrela√ß√£o em Processos Estacion√°rios ARMA]:

$$
\rho_j = \frac{\gamma_j}{\gamma_0}
$$

Para o processo em quest√£o, a autocorrela√ß√£o √©:

$$
\rho_j = \begin{cases}
1, & \text{se } j = 0 \\
0, & \text{se } j \neq 0
\end{cases}
$$

*Proof:*
I. Pela defini√ß√£o, $\rho_j = \frac{\gamma_j}{\gamma_0}$.
II. Quando $j = 0$, $\rho_0 = \frac{\gamma_0}{\gamma_0} = \frac{\sigma^2}{\sigma^2} = 1$.
III. Quando $j \neq 0$, $\rho_j = \frac{\gamma_j}{\gamma_0} = \frac{0}{\sigma^2} = 0$.
IV. Portanto, $\rho_j = 1$ se $j = 0$ e $\rho_j = 0$ se $j \neq 0$. $\blacksquare$

> üí° **Interpreta√ß√£o:** Isso indica que n√£o h√° correla√ß√£o linear entre valores do processo em diferentes momentos no tempo. O √∫nico valor correlacionado √© o pr√≥prio valor no mesmo instante ($j=0$).

> üí° **Exemplo Num√©rico:**
> Se $\gamma_0 = 4$ e $\gamma_1 = 0$, ent√£o $\rho_0 = \frac{4}{4} = 1$ e $\rho_1 = \frac{0}{4} = 0$.

**Lema 3**
O processo $Y_t = \mu + \epsilon_t$ √© *covariance-stationary* (ou *weakly stationary*).

*Proof:*
I. A m√©dia $E[Y_t] = \mu$ √© constante e n√£o depende de $t$.
II. A autocovari√¢ncia $\gamma_j$ n√£o depende de $t$ e √© igual a $\sigma^2$ para $j=0$ e 0 para $j \neq 0$.
III. Portanto, o processo satisfaz as condi√ß√µes de *covariance-stationary*. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Independentemente de qual momento $t$ estamos analisando, a m√©dia do processo sempre ser√° $\mu$, e as autocovari√¢ncias permanecer√£o as mesmas ( $\sigma^2$ para $j=0$ e 0 para $j \neq 0$). Isso significa que as propriedades estat√≠sticas do processo n√£o mudam com o tempo.

**Teorema 7**
O processo $Y_t = \mu + \epsilon_t$ √© *ergodico*.

*Proof (Esbo√ßo):*
I. Para o processo ser *ergodico*, a m√©dia temporal deve convergir para a m√©dia do ensemble [^3.1.4, 3.1.11].
II. A m√©dia temporal de $Y_t$ √© $\bar{Y} = \frac{1}{T} \sum_{t=1}^{T} Y_t = \frac{1}{T} \sum_{t=1}^{T} (\mu + \epsilon_t) = \mu + \frac{1}{T} \sum_{t=1}^{T} \epsilon_t$.
III. Pela Lei dos Grandes N√∫meros, $\frac{1}{T} \sum_{t=1}^{T} \epsilon_t$ converge para $E[\epsilon_t] = 0$ quando $T \rightarrow \infty$.
IV. Portanto, $\bar{Y}$ converge para $\mu$, que √© a m√©dia do ensemble.
V. Similarmente, pode ser mostrado que a autocovari√¢ncia amostral converge para a autocovari√¢ncia te√≥rica.
VI. Portanto, o processo √© *ergodico*.

> üí° **Exemplo Num√©rico:**
> Se simulamos um longo per√≠odo de tempo para $Y_t$ e calculamos a m√©dia amostral, essa m√©dia amostral se aproximar√° de $\mu$. Da mesma forma, a autocovari√¢ncia amostral com defasagem 0 se aproximar√° de $\sigma^2$, e as autocovari√¢ncias com outras defasagens se aproximar√£o de 0.

**Teorema 7.1**
Se $\epsilon_t$ √© um processo Gaussiano de ru√≠do branco, ent√£o $Y_t = \mu + \epsilon_t$ √© um processo Gaussiano.

*Proof:*
I. $Y_t$ √© uma transforma√ß√£o linear de $\epsilon_t$, pois $Y_t = \mu + \epsilon_t$.
II. Como $\epsilon_t$ √© Gaussiano, qualquer transforma√ß√£o linear de $\epsilon_t$ tamb√©m √© Gaussiana.
III. Portanto, $Y_t$ √© um processo Gaussiano. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Se $\epsilon_t$ √© um ru√≠do branco gerado a partir de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia $\sigma^2$, ent√£o $Y_t$ tamb√©m seguir√° uma distribui√ß√£o normal com m√©dia $\mu$ e vari√¢ncia $\sigma^2$.

### Conclus√£o
Para o processo $Y_t = \mu + \epsilon_t$, onde $\mu$ √© uma constante e $\epsilon_t$ √© ru√≠do branco, as autocovari√¢ncias s√£o zero para todas as defasagens diferentes de zero. Isso implica que n√£o h√° correla√ß√£o linear entre os valores do processo em diferentes momentos no tempo. O processo √© estacion√°rio e ergodico, tornando-o um modelo simples mas fundamental na an√°lise de s√©ries temporais. Este processo serve como um exemplo base para entender a estrutura de autocovari√¢ncia em processos mais complexos, como os modelos ARMA.

### Refer√™ncias
[^Autocorrela√ß√£o em Processos Estacion√°rios ARMA]: *Refer√™ncia ao cap√≠tulo que explora o conceito de autocorrela√ß√£o.*
[^A Vari√¢ncia como Autocovari√¢ncia de Ordem Zero em Processos ARMA]: *Refer√™ncia ao cap√≠tulo anterior sobre a vari√¢ncia como autocovari√¢ncia de ordem zero.*
[^3.1.4]: *Texto referente ao limite de probabilidade da m√©dia do ensemble*.
[^3.1.11]: *Texto referente √† m√©dia do ensemble para calcular autocovari√¢ncias*.
[^3.2.3]: *Texto referente √†s propriedades do ru√≠do branco*.
<!-- END -->