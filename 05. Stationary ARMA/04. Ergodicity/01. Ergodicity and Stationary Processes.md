## Ergodicidade e Converg√™ncia em Processos Estacion√°rios

### Introdu√ß√£o
Em an√°lise de s√©ries temporais, a ergodicidade √© um conceito fundamental que estabelece uma liga√ß√£o entre as m√©dias temporais calculadas a partir de uma √∫nica realiza√ß√£o de um processo estoc√°stico e as m√©dias de conjunto, que s√£o expectativas te√≥ricas sobre todas as poss√≠veis realiza√ß√µes do processo. Este cap√≠tulo explora a no√ß√£o de ergodicidade em detalhe, particularmente no contexto de processos estacion√°rios, e investiga as condi√ß√µes sob as quais as m√©dias temporais convergem para as m√©dias de conjunto. A compreens√£o da ergodicidade √© crucial para inferir propriedades estat√≠sticas de um processo a partir de uma √∫nica amostra observada.

### Conceitos Fundamentais
A ergodicidade est√° intrinsecamente ligada √† estacionaridade. Um processo *covariance-stationary* √© dito ser **erg√≥dico para a m√©dia** se a m√©dia amostral, calculada sobre uma √∫nica realiza√ß√£o do processo, converge em probabilidade para a esperan√ßa matem√°tica te√≥rica do processo √† medida que o comprimento da amostra tende ao infinito [^4]. Formalmente, a m√©dia amostral $\bar{y}$ √© definida como:
$$
\bar{y} = \frac{1}{T} \sum_{t=1}^{T} y_t
$$
onde $y_t$ representa a observa√ß√£o no instante *t* e *T* √© o comprimento total da s√©rie temporal [^3]. A ergodicidade para a m√©dia requer que:
$$
\text{plim}_{T \to \infty} \bar{y} = E(Y_t) = \mu
$$
onde $\text{plim}$ denota o limite em probabilidade, $E(Y_t)$ √© a esperan√ßa matem√°tica do processo, e $\mu$ √© a m√©dia do processo [^4].

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal com valores $y_t = \{2, 4, 6, 8, 10\}$ para $t = 1, 2, 3, 4, 5$. O comprimento da s√©rie √© $T = 5$. A m√©dia amostral √©:
> $$
> \bar{y} = \frac{1}{5} (2 + 4 + 6 + 8 + 10) = \frac{30}{5} = 6
> $$
> Se o processo √© erg√≥dico e estacion√°rio, e se $T$ fosse suficientemente grande, esperar√≠amos que $\bar{y}$ convergisse para $E(Y_t) = \mu$.

Um crit√©rio crucial para a ergodicidade de um processo estacion√°rio est√° relacionado com o comportamento das autocovari√¢ncias. Um processo estacion√°rio √© erg√≥dico para a m√©dia se as autocovari√¢ncias $\gamma_j$ (onde *j* √© o *lag* temporal) convergem para zero "suficientemente r√°pido" √† medida que *j* aumenta [^4]. Especificamente, a condi√ß√£o suficiente para a ergodicidade da m√©dia √© dada por:
$$
\sum_{j=0}^{\infty} |\gamma_j| < \infty
$$
Essa condi√ß√£o garante que a influ√™ncia de observa√ß√µes distantes no tempo diminui rapidamente, permitindo que a m√©dia amostral represente adequadamente a m√©dia populacional [^4].

> üí° **Exemplo Num√©rico:** Suponha que as autocovari√¢ncias de um processo estacion√°rio s√£o dadas por $\gamma_j = (0.5)^j$ para $j \geq 0$. Para verificar a ergodicidade, calculamos a soma das autocovari√¢ncias:
> $$
> \sum_{j=0}^{\infty} |\gamma_j| = \sum_{j=0}^{\infty} (0.5)^j
> $$
> Esta √© uma s√©rie geom√©trica com raz√£o $0.5$, que converge para $\frac{1}{1 - 0.5} = 2$. Como a soma √© finita, a condi√ß√£o para a ergodicidade da m√©dia √© satisfeita.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Autocovari√¢ncias
> j = np.arange(0, 10)
> gamma_j = (0.5)**j
>
> # Plotting
> plt.figure(figsize=(10, 6))
> plt.stem(j, gamma_j, basefmt="b-", use_line_collection=True)
> plt.title("Autocovari√¢ncias $\gamma_j = (0.5)^j$")
> plt.xlabel("Lag (j)")
> plt.ylabel("Autocovari√¢ncia $\gamma_j$")
> plt.grid(True)
> plt.show()
> ```
>
> Este gr√°fico mostra como as autocovari√¢ncias decaem rapidamente para zero, indicando que observa√ß√µes distantes no tempo t√™m pouca influ√™ncia umas sobre as outras, o que √© consistente com a ergodicidade.

**Proposi√ß√£o 1** (Converg√™ncia da Vari√¢ncia da M√©dia Amostral). A condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ implica que a vari√¢ncia da m√©dia amostral converge para zero quando $T \to \infty$.

*Demonstra√ß√£o*. A vari√¢ncia da m√©dia amostral √© dada por:
$$
Var(\bar{y}) = Var\left(\frac{1}{T} \sum_{t=1}^{T} Y_t\right) = \frac{1}{T^2} Var\left(\sum_{t=1}^{T} Y_t\right)
$$
Como o processo √© estacion√°rio, podemos expressar a vari√¢ncia como:
$$
Var(\bar{y}) = \frac{1}{T^2} \sum_{t=1}^{T} \sum_{s=1}^{T} Cov(Y_t, Y_s) = \frac{1}{T^2} \sum_{t=1}^{T} \sum_{s=1}^{T} \gamma_{|t-s|}
$$
Reorganizando a soma, obtemos:
$$
Var(\bar{y}) = \frac{1}{T^2} \sum_{j=-(T-1)}^{T-1} (T - |j|) \gamma_j = \frac{1}{T} \sum_{j=-(T-1)}^{T-1} \left(1 - \frac{|j|}{T}\right) \gamma_j
$$
Portanto,
$$
Var(\bar{y}) = \frac{1}{T} \left[ \gamma_0 + 2\sum_{j=1}^{T-1} \left(1 - \frac{j}{T}\right) \gamma_j \right]
$$
Sob a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$, temos que $\lim_{T \to \infty} Var(\bar{y}) = 0$. Isso ocorre porque o termo $\frac{1}{T}$ tende a zero e a soma converge para um valor finito.

> üí° **Exemplo Num√©rico:** Consideremos novamente o caso onde $\gamma_j = (0.5)^j$. Vamos calcular a vari√¢ncia da m√©dia amostral para $T = 10$ e $T = 100$.
>
> Para $T=10$:
> $$
> Var(\bar{y}) = \frac{1}{10} \left[ 1 + 2\sum_{j=1}^{9} \left(1 - \frac{j}{10}\right) (0.5)^j \right]
> $$
> Calculando a soma:
> $$
> \sum_{j=1}^{9} \left(1 - \frac{j}{10}\right) (0.5)^j \approx 0.828
> $$
> Portanto, $Var(\bar{y}) \approx \frac{1}{10} [1 + 2(0.828)] \approx 0.2656$.
>
> Para $T=100$:
> $$
> Var(\bar{y}) = \frac{1}{100} \left[ 1 + 2\sum_{j=1}^{99} \left(1 - \frac{j}{100}\right) (0.5)^j \right]
> $$
> Calculando a soma:
> $$
> \sum_{j=1}^{99} \left(1 - \frac{j}{100}\right) (0.5)^j \approx 0.99
> $$
> Portanto, $Var(\bar{y}) \approx \frac{1}{100} [1 + 2(0.99)] \approx 0.0298$.
>
> Observa-se que a vari√¢ncia da m√©dia amostral diminui √† medida que $T$ aumenta, consistente com a Proposi√ß√£o 1.
>
> ```python
> import numpy as np
>
> def variance_of_sample_mean(T, gamma):
>     """Calculates the variance of the sample mean for a given T and autocovariance function."""
>     gamma_0 = gamma(0)
>     sum_term = 0
>     for j in range(1, T):
>         sum_term += (1 - j/T) * gamma(j)
>     return (1/T) * (gamma_0 + 2 * sum_term)
>
> # Define the autocovariance function
> def gamma_j(j):
>     return (0.5)**j
>
> # Calculate for T=10 and T=100
> T1 = 10
> var_y_bar_10 = variance_of_sample_mean(T1, gamma_j)
>
> T2 = 100
> var_y_bar_100 = variance_of_sample_mean(T2, gamma_j)
>
> print(f"Variance of sample mean for T=10: {var_y_bar_10:.4f}")
> print(f"Variance of sample mean for T=100: {var_y_bar_100:.4f}")
> ```

Al√©m da ergodicidade para a m√©dia, existe a no√ß√£o de **ergodicidade para os segundos momentos**. Um processo √© erg√≥dico para os segundos momentos se as m√©dias temporais dos produtos das observa√ß√µes (centradas em suas m√©dias) convergem para os correspondentes momentos te√≥ricos [^4]. Formalmente, essa condi√ß√£o exige que:
$$
\text{plim}_{T \to \infty} \frac{1}{T-j} \sum_{t=j+1}^{T} (Y_t - \mu)(Y_{t-j} - \mu) = \gamma_j
$$
para todo *j* [^4]. Suficientes condi√ß√µes para ergodicity de segunda ordem ser√£o apresentados no Cap√≠tulo 7 [^4]. No caso especial onde {Yt} √© um processo Gaussiano estacion√°rio, a condi√ß√£o [3.1.15] √© suficiente para garantir a ergodicidade para todos os momentos [^4].

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal com m√©dia $\mu = 3$ e valores $Y_t = \{4, 2, 5, 1, 3, 6\}$ para $t = 1, 2, \ldots, 6$. Vamos calcular a estimativa da autocovari√¢ncia $\gamma_1$ usando a f√≥rmula da ergodicidade para os segundos momentos:
>
> $$
> \hat{\gamma}_1 = \frac{1}{T-1} \sum_{t=2}^{T} (Y_t - \mu)(Y_{t-1} - \mu)
> $$
>
> $$
> \hat{\gamma}_1 = \frac{1}{5} [(2-3)(4-3) + (5-3)(2-3) + (1-3)(5-3) + (3-3)(1-3) + (6-3)(3-3)]
> $$
>
> $$
> \hat{\gamma}_1 = \frac{1}{5} [(-1)(1) + (2)(-1) + (-2)(2) + (0)(-2) + (3)(0)] = \frac{1}{5} [-1 - 2 - 4 + 0 + 0] = \frac{-7}{5} = -1.4
> $$
>
> Se o processo √© erg√≥dico para os segundos momentos, esperar√≠amos que, √† medida que $T \to \infty$, $\hat{\gamma}_1$ convergisse para o valor te√≥rico da autocovari√¢ncia $\gamma_1$.

√â importante notar que a ergodicidade para todos os momentos implica que todas as estat√≠sticas amostrais convergem para seus valores te√≥ricos correspondentes [^4]. Em outras palavras, a distribui√ß√£o da s√©rie temporal pode ser completamente caracterizada por uma √∫nica realiza√ß√£o suficientemente longa.

Para esclarecer a rela√ß√£o entre estacionaridade e ergodicidade, considere o seguinte exemplo. Suponha que a m√©dia $\mu^{(i)}$ para a *i*-√©sima realiza√ß√£o √© gerada a partir de uma distribui√ß√£o $N(0, \lambda^2)$ [^4]:
$$
Y_t^{(i)} = \mu^{(i)} + \epsilon_t
$$
onde $\{\epsilon_t\}$ √© um processo de ru√≠do branco Gaussiano com m√©dia zero e vari√¢ncia $\sigma^2$, independente de $\mu^{(i)}$ [^4]. Neste caso, a m√©dia do processo √©:
$$
E(Y_t) = E(\mu^{(i)}) + E(\epsilon_t) = 0
$$
e as autocovari√¢ncias s√£o dadas por:
$$
\gamma_0 = E[(\mu^{(i)} + \epsilon_t)^2] = \lambda^2 + \sigma^2
$$
$$
\gamma_j = E[(\mu^{(i)} + \epsilon_t)(\mu^{(i)} + \epsilon_{t-j})] = \lambda^2, \quad \text{para } j \neq 0
$$
Este processo √© *covariance-stationary* porque a m√©dia e as autocovari√¢ncias n√£o dependem do tempo *t* [^4]. No entanto, ele n√£o satisfaz a condi√ß√£o suficiente para a ergodicidade da m√©dia [^4]:
$$
\sum_{j=0}^{\infty} |\gamma_j| = \lambda^2 + \sigma^2 + \sum_{j=1}^{\infty} \lambda^2 = \infty
$$
Al√©m disso, a m√©dia temporal n√£o converge para zero:
$$
\frac{1}{T} \sum_{t=1}^{T} Y_t^{(i)} = \frac{1}{T} \sum_{t=1}^{T} (\mu^{(i)} + \epsilon_t) = \mu^{(i)} + \frac{1}{T} \sum_{t=1}^{T} \epsilon_t \xrightarrow{p} \mu^{(i)}
$$
onde $\xrightarrow{p}$ denota a converg√™ncia em probabilidade. Portanto, a m√©dia temporal converge para $\mu^{(i)}$ em vez da m√©dia de conjunto zero [^4]. Este exemplo ilustra um processo que √© *covariance-stationary* mas n√£o erg√≥dico [^4].

> üí° **Exemplo Num√©rico:** Seja $\lambda^2 = 1$ e $\sigma^2 = 2$. Ent√£o, $\gamma_0 = 1 + 2 = 3$ e $\gamma_j = 1$ para $j \neq 0$. A soma das autocovari√¢ncias √© $\sum_{j=0}^{\infty} |\gamma_j| = 3 + \sum_{j=1}^{\infty} 1$, que diverge para infinito, demonstrando que o processo n√£o √© erg√≥dico.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> lambda_sq = 1
> sigma_sq = 2
>
> # Autocovari√¢ncias
> gamma_0 = lambda_sq + sigma_sq
> gamma_j = lambda_sq
>
> # Plotting as autocovari√¢ncias
> j = np.arange(0, 10)
> autocovariances = np.array([gamma_0] + [gamma_j] * 9)
>
> plt.figure(figsize=(10, 6))
> plt.stem(j, autocovariances, basefmt="b-", use_line_collection=True)
> plt.title("Autocovari√¢ncias para um Processo N√£o Ergodico")
> plt.xlabel("Lag (j)")
> plt.ylabel("Autocovari√¢ncia $\gamma_j$")
> plt.grid(True)
> plt.show()
> ```
>
> Este gr√°fico demonstra que as autocovari√¢ncias n√£o decaem para zero √† medida que o *lag* aumenta, o que √© caracter√≠stico de processos estacion√°rios que n√£o s√£o erg√≥dicos.

**Teorema 2** (Ergodicidade e Independ√™ncia Assint√≥tica). Se um processo estacion√°rio tem autocovari√¢ncias que decaem suficientemente r√°pido ($\gamma_j \to 0$ quando $j \to \infty$), ent√£o as vari√°veis $Y_t$ e $Y_{t+j}$ tornam-se assintoticamente independentes quando $j \to \infty$.

*Demonstra√ß√£o*. A condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ implica que $\gamma_j \to 0$ quando $j \to \infty$. Para processos Gaussianos, a independ√™ncia √© equivalente a n√£o-correla√ß√£o. Portanto, para um processo Gaussiano estacion√°rio com autocovari√¢ncias absolutamente som√°veis, $Y_t$ e $Y_{t+j}$ tornam-se assintoticamente independentes quando $j \to \infty$. Para processos n√£o-Gaussianos, a converg√™ncia das autocovari√¢ncias para zero sugere uma redu√ß√£o na depend√™ncia linear entre as vari√°veis, aproximando-se da independ√™ncia √† medida que o *lag* aumenta. Uma demonstra√ß√£o formal para o caso n√£o-Gaussiano exigiria condi√ß√µes adicionais sobre os momentos de ordem superior do processo.

**Prova do fato de que $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ implica que $\gamma_j \to 0$ quando $j \to \infty$:**
I. Assumimos que a s√©rie $\sum_{j=0}^{\infty} |\gamma_j|$ converge para um valor finito, digamos $S$.

II. Pela defini√ß√£o de converg√™ncia de uma s√©rie, para qualquer $\epsilon > 0$, existe um inteiro $N$ tal que para todo $n > N$:
$$
\left| \sum_{j=0}^{n} |\gamma_j| - S \right| < \epsilon
$$

III. Considere a sequ√™ncia de somas parciais $S_n = \sum_{j=0}^{n} |\gamma_j|$. Como a s√©rie converge, a sequ√™ncia $S_n$ converge para $S$.

IV. Isso implica que a sequ√™ncia $\{|\gamma_j|\}$ converge para zero. Para ver isso, note que para $n$ suficientemente grande, $|\gamma_n|$ deve ser arbitrariamente pequeno, caso contr√°rio, a s√©rie n√£o convergiria. Formalmente, dado $\epsilon > 0$, escolha $N$ tal que $\sum_{j=N}^{\infty} |\gamma_j| < \epsilon$.  Ent√£o, para $j > N$, temos $|\gamma_j| < \epsilon$.

V. Portanto, $\lim_{j \to \infty} |\gamma_j| = 0$, o que implica que $\lim_{j \to \infty} \gamma_j = 0$. ‚ñ†

### Conclus√£o
A ergodicidade √© uma propriedade essencial para a an√°lise de s√©ries temporais estacion√°rias. Ela justifica a utiliza√ß√£o de m√©dias temporais para inferir as caracter√≠sticas estat√≠sticas de um processo a partir de uma √∫nica realiza√ß√£o. A condi√ß√£o suficiente para a ergodicidade da m√©dia, envolvendo a converg√™ncia absoluta das autocovari√¢ncias, fornece um crit√©rio pr√°tico para verificar se um processo estacion√°rio √© erg√≥dico. Embora estacionaridade seja uma condi√ß√£o necess√°ria para ergodicidade, ela n√£o √© suficiente, como demonstrado pelo exemplo apresentado. A compreens√£o das nuances entre estacionaridade e ergodicidade √© crucial para a correta interpreta√ß√£o e modelagem de s√©ries temporais.

### Refer√™ncias
[^3]: Page 46, Chapter 3, Stationary ARMA Processes
[^4]: Page 47, Chapter 3, Stationary ARMA Processes
<!-- END -->