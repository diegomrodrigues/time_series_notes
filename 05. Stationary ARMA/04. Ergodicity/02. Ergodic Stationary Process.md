## Ergodicidade para a M√©dia em Processos Covariance-Estacion√°rios

### Introdu√ß√£o
Este cap√≠tulo aprofunda a condi√ß√£o de **ergodicidade para a m√©dia** em processos *covariance-stationary*. Como discutido anteriormente, a ergodicidade √© uma propriedade crucial que permite inferir caracter√≠sticas estat√≠sticas de um processo estoc√°stico a partir de uma √∫nica realiza√ß√£o temporal [^4]. Especificamente, exploraremos a condi√ß√£o sob a qual a m√©dia temporal de um processo *covariance-stationary* converge em probabilidade para a sua esperan√ßa matem√°tica te√≥rica, $E(Y_t)$ [^4]. Esta condi√ß√£o √© fundamental para justificar o uso de m√©dias amostrais como estimadores consistentes da m√©dia populacional.

### Conceitos Fundamentais
Revisando conceitos j√° apresentados, um processo $\{Y_t\}$ √© dito *covariance-stationary* se sua m√©dia $E(Y_t) = \mu$ e sua autocovari√¢ncia $Cov(Y_t, Y_{t-j}) = \gamma_j$ n√£o dependem do tempo *t*, mas apenas do *lag* *j* [^4]. Expandindo o conceito apresentado, a **ergodicidade para a m√©dia** requer que a m√©dia amostral
$$
\bar{Y} = \frac{1}{T} \sum_{t=1}^T Y_t
$$
converja em probabilidade para a m√©dia populacional $\mu$ quando o tamanho da amostra $T$ tende ao infinito [^4]. Formalmente:
$$
\text{plim}_{T\to\infty} \bar{Y} = \mu
$$
Uma condi√ß√£o suficiente para a ergodicidade da m√©dia √© que as autocovari√¢ncias do processo convirjam para zero "suficientemente r√°pido" [^4]. Mais precisamente, o processo √© erg√≥dico para a m√©dia se:
$$
\sum_{j=0}^{\infty} |\gamma_j| < \infty
$$
Essa condi√ß√£o assegura que a influ√™ncia de observa√ß√µes passadas diminui rapidamente, permitindo que a m√©dia amostral seja uma boa aproxima√ß√£o da m√©dia populacional.

> üí° **Exemplo Num√©rico:**
>
> Considere um processo *covariance-stationary* com m√©dia $\mu = 5$ e autocovari√¢ncias dadas por $\gamma_j = (0.5)^{|j|}$. Vamos verificar se a condi√ß√£o de ergodicidade √© satisfeita.
>
> $\sum_{j=0}^{\infty} |\gamma_j| = \sum_{j=0}^{\infty} (0.5)^j = \frac{1}{1 - 0.5} = 2 < \infty$
>
> Como a soma das autocovari√¢ncias converge para um valor finito (2), o processo satisfaz a condi√ß√£o de ergodicidade para a m√©dia. Isso significa que, √† medida que o tamanho da amostra aumenta, a m√©dia amostral $\bar{Y}$ converge em probabilidade para a m√©dia populacional $\mu = 5$. Podemos simular este processo para verificar a converg√™ncia.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> mu = 5
> T = 1000  # Tamanho da amostra
>
> # Fun√ß√£o para gerar dados com autocorrela√ß√£o (aproxima√ß√£o)
> def generate_correlated_data(mu, T, rho=0.5):
>     errors = np.random.normal(0, 1, T)
>     Y = np.zeros(T)
>     Y[0] = mu + errors[0]
>     for t in range(1, T):
>         Y[t] = mu + rho * (Y[t-1] - mu) + errors[t]
>     return Y
>
> # Gerar dados
> Y = generate_correlated_data(mu, T)
>
> # Calcular m√©dias amostrais cumulativas
> cumulative_means = np.cumsum(Y) / np.arange(1, T + 1)
>
> # Plotar as m√©dias amostrais cumulativas
> plt.figure(figsize=(10, 6))
> plt.plot(cumulative_means)
> plt.axhline(y=mu, color='r', linestyle='--', label='M√©dia Populacional')
> plt.title('Converg√™ncia da M√©dia Amostral')
> plt.xlabel('Tamanho da Amostra (T)')
> plt.ylabel('M√©dia Amostral')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> O gr√°fico demonstra que a m√©dia amostral converge para a m√©dia populacional √† medida que o tamanho da amostra aumenta.

**Lema 1:** Se um processo *covariance-stationary* $\{Y_t\}$ satisfaz a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$, ent√£o $\lim_{T \to \infty} E[(\bar{Y} - \mu)^2] = 0$.

*Demonstra√ß√£o*. A express√£o $E[(\bar{Y} - \mu)^2]$ representa o erro quadr√°tico m√©dio da m√©dia amostral $\bar{Y}$. Podemos expandir essa express√£o como:
$$
E[(\bar{Y} - \mu)^2] = E\left[\left(\frac{1}{T} \sum_{t=1}^T Y_t - \mu\right)^2\right] = E\left[\left(\frac{1}{T} \sum_{t=1}^T (Y_t - \mu)\right)^2\right]
$$
I. Expanda o quadrado:
   $$
   E\left[\left(\frac{1}{T} \sum_{t=1}^T (Y_t - \mu)\right)^2\right] = E\left[\frac{1}{T^2}\left(\sum_{t=1}^T (Y_t - \mu)\right)\left(\sum_{s=1}^T (Y_s - \mu)\right)\right]
   $$

II. Intercambie a esperan√ßa e a soma (devido √† linearidade da esperan√ßa):
   $$
   E\left[\frac{1}{T^2}\left(\sum_{t=1}^T (Y_t - \mu)\right)\left(\sum_{s=1}^T (Y_s - \mu)\right)\right] = \frac{1}{T^2} \sum_{t=1}^T \sum_{s=1}^T E[(Y_t - \mu)(Y_s - \mu)]
   $$

III. Reconhe√ßa que $E[(Y_t - \mu)(Y_s - \mu)]$ √© a autocovari√¢ncia $\gamma_{|t-s|}$:
    $$
    \frac{1}{T^2} \sum_{t=1}^T \sum_{s=1}^T E[(Y_t - \mu)(Y_s - \mu)] = \frac{1}{T^2} \sum_{t=1}^T \sum_{s=1}^T \gamma_{|t-s|}
    $$

IV. Reescreva a soma dupla em termos do *lag* $j = t - s$. Para cada valor de $j$, existem aproximadamente $T - |j|$ pares $(t, s)$ tais que $t - s = j$:
    $$
    \frac{1}{T^2} \sum_{t=1}^T \sum_{s=1}^T \gamma_{|t-s|} = \frac{1}{T^2} \sum_{j=-(T-1)}^{T-1} (T-|j|) \gamma_j
    $$

V. Simplifique a express√£o:
   $$
   \frac{1}{T^2} \sum_{j=-(T-1)}^{T-1} (T-|j|) \gamma_j = \frac{1}{T} \sum_{j=-(T-1)}^{T-1} \left(1 - \frac{|j|}{T}\right) \gamma_j
   $$

VI. Tome o limite quando $T \to \infty$.  Sob a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$, o termo $\frac{|j|}{T} \to 0$ e a soma converge absolutamente.
    $$
    \lim_{T \to \infty} \frac{1}{T} \sum_{j=-(T-1)}^{T-1} \left(1 - \frac{|j|}{T}\right) \gamma_j = \lim_{T \to \infty} \frac{1}{T} \sum_{j=-\infty}^{\infty} \gamma_j
    $$

VII. Como $\sum_{j=-\infty}^{\infty} \gamma_j$ converge para um valor finito (devido √† condi√ß√£o dada), e $\frac{1}{T} \to 0$, o limite √© zero:
    $$
    \lim_{T \to \infty} \frac{1}{T} \sum_{j=-\infty}^{\infty} \gamma_j = 0
    $$
Portanto, $\lim_{T \to \infty} E[(\bar{Y} - \mu)^2] = 0$, mostrando que o erro quadr√°tico m√©dio da m√©dia amostral converge para zero. $\blacksquare$

O Lema 1 demonstra que sob a condi√ß√£o de ergodicidade para a m√©dia, o erro quadr√°tico m√©dio da m√©dia amostral converge para zero √† medida que o tamanho da amostra aumenta. Isso √© um forte indicativo de que a m√©dia amostral se torna uma estimativa cada vez mais precisa da m√©dia populacional.

> üí° **Exemplo Num√©rico:**
>
> Seja $\mu = 10$ e $\gamma_j = 0.8^{|j|}$. Vamos calcular $E[(\bar{Y} - \mu)^2]$ para $T = 10$ e $T = 100$. Primeiro, calculamos $\sum_{j=-(T-1)}^{T-1} (T-|j|) \gamma_j$ para cada valor de $T$.
>
> Para $T = 10$:
> $E[(\bar{Y} - \mu)^2] = \frac{1}{10^2} \sum_{j=-9}^{9} (10-|j|) (0.8)^{|j|} \approx 0.436$
>
> Para $T = 100$:
> $E[(\bar{Y} - \mu)^2] = \frac{1}{100^2} \sum_{j=-99}^{99} (100-|j|) (0.8)^{|j|} \approx 0.049$
>
> Observe que $E[(\bar{Y} - \mu)^2]$ diminui √† medida que $T$ aumenta, indicando que a m√©dia amostral est√° se aproximando da m√©dia populacional.

**Lema 1.1:** Se um processo *covariance-stationary* $\{Y_t\}$ satisfaz a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$, ent√£o $\lim_{T \to \infty} Var(\bar{Y}) = 0$.

*Demonstra√ß√£o*. Sabemos que $Var(\bar{Y}) = E[(\bar{Y} - E[\bar{Y}])^2]$. Como o processo √© *covariance-stationary*, $E[Y_t] = \mu$ para todo $t$, ent√£o $E[\bar{Y}] = \mu$. Portanto, $Var(\bar{Y}) = E[(\bar{Y} - \mu)^2]$. Pelo Lema 1, $\lim_{T \to \infty} E[(\bar{Y} - \mu)^2] = 0$. Assim, $\lim_{T \to \infty} Var(\bar{Y}) = 0$. $\blacksquare$

O Lema 1.1 mostra que sob a condi√ß√£o de ergodicidade, a vari√¢ncia da m√©dia amostral converge para zero. Este resultado √© crucial porque uma vari√¢ncia decrescente indica que a m√©dia amostral est√° se concentrando cada vez mais perto de seu valor esperado.

> üí° **Exemplo Num√©rico:**
>
> Usando o exemplo anterior com $\gamma_j = (0.5)^{|j|}$, vamos calcular $Var(\bar{Y})$ para diferentes valores de $T$. Sabemos que $Var(\bar{Y}) = E[(\bar{Y} - \mu)^2]$.
>
> Para $T = 10$: $Var(\bar{Y}) = \frac{1}{T^2} \sum_{j=-(T-1)}^{T-1} (T-|j|) \gamma_j =  \frac{1}{100} \sum_{j=-9}^{9} (10-|j|) (0.5)^{|j|} \approx 0.228$
>
> Para $T = 100$: $Var(\bar{Y}) = \frac{1}{T^2} \sum_{j=-(T-1)}^{T-1} (T-|j|) \gamma_j = \frac{1}{10000} \sum_{j=-99}^{99} (100-|j|) (0.5)^{|j|} \approx 0.020$
>
> Como esperado, a vari√¢ncia da m√©dia amostral diminui quando o tamanho da amostra $T$ aumenta.

**Teorema 1:** Se $E[(\bar{Y} - \mu)^2] \to 0$ quando $T \to \infty$, ent√£o $\text{plim}_{T \to \infty} \bar{Y} = \mu$.

*Demonstra√ß√£o*. Pela desigualdade de Chebyshev, para qualquer $\epsilon > 0$:
$$
P(|\bar{Y} - \mu| > \epsilon) \leq \frac{E[(\bar{Y} - \mu)^2]}{\epsilon^2}
$$
I. Aplique o limite quando $T \to \infty$:
$$
\lim_{T \to \infty} P(|\bar{Y} - \mu| > \epsilon) \leq \lim_{T \to \infty} \frac{E[(\bar{Y} - \mu)^2]}{\epsilon^2}
$$

II. Como √© dado que $\lim_{T \to \infty} E[(\bar{Y} - \mu)^2] = 0$:
$$
\lim_{T \to \infty} P(|\bar{Y} - \mu| > \epsilon) \leq \frac{0}{\epsilon^2} = 0
$$

III. Uma probabilidade n√£o pode ser negativa, ent√£o:
$$
\lim_{T \to \infty} P(|\bar{Y} - \mu| > \epsilon) = 0
$$

IV. Por defini√ß√£o, se para todo $\epsilon > 0$, $\lim_{T \to \infty} P(|\bar{Y} - \mu| > \epsilon) = 0$, ent√£o $\bar{Y}$ converge em probabilidade para $\mu$:
$$
\text{plim}_{T \to \infty} \bar{Y} = \mu
$$
Portanto, se $E[(\bar{Y} - \mu)^2] \to 0$, ent√£o $\text{plim}_{T \to \infty} \bar{Y} = \mu$. $\blacksquare$

O Teorema 1 demonstra formalmente que a converg√™ncia do erro quadr√°tico m√©dio para zero implica que a m√©dia amostral converge em probabilidade para a m√©dia populacional. Este resultado, combinado com o Lema 1, estabelece que a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ √© suficiente para a ergodicidade da m√©dia em processos *covariance-stationary*.

**Teorema 1.1:** Se $Var(\bar{Y}) \to 0$ quando $T \to \infty$, ent√£o $\text{plim}_{T \to \infty} \bar{Y} = \mu$.

*Demonstra√ß√£o*. Esta demonstra√ß√£o √© id√™ntica √† demonstra√ß√£o do Teorema 1, substituindo $E[(\bar{Y} - \mu)^2]$ por $Var(\bar{Y})$. Pela desigualdade de Chebyshev, para qualquer $\epsilon > 0$:
$$
P(|\bar{Y} - \mu| > \epsilon) \leq \frac{Var(\bar{Y})}{\epsilon^2}
$$

I. Aplique o limite quando $T \to \infty$:
    $$
    \lim_{T \to \infty} P(|\bar{Y} - \mu| > \epsilon) \leq \lim_{T \to \infty} \frac{Var(\bar{Y})}{\epsilon^2}
    $$

II. Como √© dado que $\lim_{T \to \infty} Var(\bar{Y}) = 0$:
    $$
    \lim_{T \to \infty} P(|\bar{Y} - \mu| > \epsilon) \leq \frac{0}{\epsilon^2} = 0
    $$

III. Uma probabilidade n√£o pode ser negativa, ent√£o:
     $$
     \lim_{T \to \infty} P(|\bar{Y} - \mu| > \epsilon) = 0
     $$

IV. Por defini√ß√£o, se para todo $\epsilon > 0$, $\lim_{T \to \infty} P(|\bar{Y} - \mu| > \epsilon) = 0$, ent√£o $\bar{Y}$ converge em probabilidade para $\mu$:
    $$
    \text{plim}_{T \to \infty} \bar{Y} = \mu
    $$
Portanto, se $Var(\bar{Y}) \to 0$, ent√£o $\text{plim}_{T \to \infty} \bar{Y} = \mu$. $\blacksquare$

O Teorema 1.1 fornece uma caracteriza√ß√£o alternativa da converg√™ncia em probabilidade, utilizando a vari√¢ncia da m√©dia amostral. Combinado com o Lema 1.1, refor√ßa a conex√£o entre a condi√ß√£o de ergodicidade e a converg√™ncia da m√©dia amostral.

**Corol√°rio 1:** Se um processo ARMA √© *covariance-stationary*, ent√£o ele √© erg√≥dico para a m√©dia.

*Demonstra√ß√£o*. Processos ARMA estacion√°rios t√™m autocovari√¢ncias que decaem exponencialmente [^4]. Consequentemente, para qualquer processo ARMA estacion√°rio, a soma das autocovari√¢ncias converge absolutamente:
$$
\sum_{j=0}^{\infty} |\gamma_j| < \infty
$$
I. Um processo ARMA *covariance-stationary* tem autocovari√¢ncias que decaem exponencialmente.

II. Se as autocovari√¢ncias decaem exponencialmente, ent√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$.

III. Pelo Lema 1 e Teorema 1, se $\sum_{j=0}^{\infty} |\gamma_j| < \infty$, ent√£o o processo √© erg√≥dico para a m√©dia.

IV. Portanto, um processo ARMA *covariance-stationary* √© erg√≥dico para a m√©dia. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Considere um processo AR(1) dado por $Y_t = 0.7Y_{t-1} + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2 = 1$. Este processo √© *covariance-stationary*. As autocovari√¢ncias para um processo AR(1) s√£o dadas por $\gamma_j = \frac{\phi^j \sigma^2}{1 - \phi^2}$, onde $\phi$ √© o coeficiente do AR(1).
>
> Neste caso, $\phi = 0.7$, ent√£o $\gamma_j = \frac{(0.7)^j}{1 - (0.7)^2} = \frac{(0.7)^j}{0.51}$. A soma das autocovari√¢ncias √© $\sum_{j=0}^{\infty} |\gamma_j| = \sum_{j=0}^{\infty} \frac{(0.7)^j}{0.51} = \frac{1}{0.51} \sum_{j=0}^{\infty} (0.7)^j = \frac{1}{0.51} \cdot \frac{1}{1 - 0.7} = \frac{1}{0.51} \cdot \frac{1}{0.3} \approx 6.54 < \infty$.
>
> Como a soma das autocovari√¢ncias converge, o processo AR(1) √© erg√≥dico para a m√©dia.

**Corol√°rio 1.1:** Se $\{Y_t\}$ √© um processo IID (Independent and Identically Distributed) com m√©dia $\mu$ e vari√¢ncia finita $\sigma^2$, ent√£o $\{Y_t\}$ √© erg√≥dico para a m√©dia.

*Demonstra√ß√£o*. Para um processo IID, $Cov(Y_t, Y_s) = 0$ para $t \neq s$. Portanto, $\gamma_j = 0$ para $j \neq 0$ e $\gamma_0 = Var(Y_t) = \sigma^2$. Assim, $\sum_{j=0}^{\infty} |\gamma_j| = |\gamma_0| = \sigma^2 < \infty$.

I. Para um processo IID, as autocovari√¢ncias $\gamma_j = 0$ para $j \neq 0$.

II. A autocovari√¢ncia no *lag* 0 √© a vari√¢ncia: $\gamma_0 = Var(Y_t) = \sigma^2$.

III. A soma das autocovari√¢ncias √© ent√£o $\sum_{j=0}^{\infty} |\gamma_j| = |\gamma_0| = \sigma^2$.

IV. Como $\sigma^2 < \infty$ (dado), ent√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$.

V. Pelo Lema 1 e Teorema 1, se $\sum_{j=0}^{\infty} |\gamma_j| < \infty$, ent√£o o processo √© erg√≥dico para a m√©dia.

VI. Portanto, o processo IID √© erg√≥dico para a m√©dia. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Seja $\{Y_t\}$ um processo IID com distribui√ß√£o normal, m√©dia $\mu = 2$ e vari√¢ncia $\sigma^2 = 4$. Neste caso, $\gamma_0 = \sigma^2 = 4$, e $\gamma_j = 0$ para $j \neq 0$. Ent√£o, $\sum_{j=0}^{\infty} |\gamma_j| = |\gamma_0| = 4 < \infty$.
>
> Portanto, este processo IID √© erg√≥dico para a m√©dia. Podemos gerar uma amostra e verificar a converg√™ncia da m√©dia amostral:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> mu = 2
> sigma = 2  # Desvio padr√£o, pois sigma^2 = 4
> T = 1000
>
> # Gerar dados IID
> Y = np.random.normal(mu, sigma, T)
>
> # Calcular m√©dias amostrais cumulativas
> cumulative_means = np.cumsum(Y) / np.arange(1, T + 1)
>
> # Plotar as m√©dias amostrais cumulativas
> plt.figure(figsize=(10, 6))
> plt.plot(cumulative_means)
> plt.axhline(y=mu, color='r', linestyle='--', label='M√©dia Populacional')
> plt.title('Converg√™ncia da M√©dia Amostral para um Processo IID')
> plt.xlabel('Tamanho da Amostra (T)')
> plt.ylabel('M√©dia Amostral')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> O gr√°fico demonstra que a m√©dia amostral converge para a m√©dia populacional √† medida que o tamanho da amostra aumenta.

O Corol√°rio 1.1 estende a an√°lise para processos IID, demonstrando que essa classe fundamental de processos estoc√°sticos tamb√©m satisfaz a condi√ß√£o de ergodicidade para a m√©dia, desde que a vari√¢ncia seja finita.

### Conclus√£o
Este cap√≠tulo explorou em profundidade a condi√ß√£o para a ergodicidade da m√©dia em processos *covariance-stationary*. Atrav√©s do Lema 1 e do Teorema 1, demonstramos que a converg√™ncia absoluta da soma das autocovari√¢ncias √© uma condi√ß√£o suficiente para garantir que a m√©dia amostral convirja em probabilidade para a m√©dia populacional. Adicionalmente, mostramos que todos os processos ARMA estacion√°rios s√£o erg√≥dicos para a m√©dia, derivado diretamente do seu decaimento exponencial das autocovari√¢ncias. A condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ √© crucial para justificar a utiliza√ß√£o da m√©dia amostral como um estimador consistente da m√©dia populacional em an√°lises de s√©ries temporais. Tamb√©m extendemos a discuss√£o para incluir processos IID, demonstrando que eles tamb√©m s√£o erg√≥dicos para a m√©dia sob condi√ß√µes de vari√¢ncia finita.

### Refer√™ncias
[^4]: Page 47, Chapter 3, Stationary ARMA Processes
<!-- END -->