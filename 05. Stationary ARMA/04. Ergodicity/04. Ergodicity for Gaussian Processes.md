## Ergodicidade Completa em Processos Gaussianos Estacion√°rios

### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise da ergodicidade em processos Gaussianos estacion√°rios, expandindo o conceito apresentado de ergodicidade para a m√©dia para uma no√ß√£o mais abrangente que abrange todos os momentos do processo. Como vimos anteriormente [^4], a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ √© suficiente para garantir a ergodicidade para a m√©dia em processos estacion√°rios. Em continuidade ao t√≥pico anterior, exploraremos que, para processos Gaussianos estacion√°rios, a ergodicidade para a m√©dia implica a ergodicidade para todos os momentos, e que a condi√ß√£o de somabilidade absoluta das autocovari√¢ncias √© suficiente para essa forma mais forte de ergodicidade.

### Conceitos Fundamentais
Um processo estoc√°stico $\{Y_t\}$ √© dito ser **Gaussiano** se, para qualquer conjunto finito de instantes de tempo $t_1, t_2, \dots, t_n$, a distribui√ß√£o conjunta das vari√°veis aleat√≥rias $(Y_{t_1}, Y_{t_2}, \dots, Y_{t_n})$ √© uma distribui√ß√£o normal multivariada [^3]. Em um processo Gaussiano estacion√°rio, a distribui√ß√£o conjunta √© completamente caracterizada pela m√©dia $\mu = E(Y_t)$ e a fun√ß√£o de autocovari√¢ncia $\gamma_j = Cov(Y_t, Y_{t-j})$, onde $\gamma_j$ n√£o depende de $t$ devido √† estacionaridade.

> üí° **Exemplo Num√©rico:**
>
> Considere um processo Gaussiano estacion√°rio $\{Y_t\}$ com m√©dia $\mu = 2$ e fun√ß√£o de autocovari√¢ncia $\gamma_j = \frac{1}{1+|j|}$. Para quaisquer tempos $t_1 = 1$ e $t_2 = 5$, o vetor $(Y_1, Y_5)$ segue uma distribui√ß√£o normal bivariada com m√©dia $\begin{bmatrix} 2 \\ 2 \end{bmatrix}$ e matriz de covari√¢ncia $\begin{bmatrix} 1 & 1/5 \\ 1/5 & 1 \end{bmatrix}$.
>
> A condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ √© satisfeita neste exemplo, como mostra o seguinte c√°lculo:
>
> $\sum_{j=0}^{\infty} |\gamma_j| = \sum_{j=0}^{\infty} \frac{1}{1+j} = 1 + \frac{1}{2} + \frac{1}{3} + \ldots$.
>
> Esta s√©rie diverge. No entanto, se a fun√ß√£o de autocovari√¢ncia fosse $\gamma_j = (0.5)^{|j|}$, ent√£o a condi√ß√£o seria satisfeita:
>
> $\sum_{j=0}^{\infty} |\gamma_j| = \sum_{j=0}^{\infty} (0.5)^{j} = \frac{1}{1 - 0.5} = 2 < \infty$.
>
> Este √∫ltimo exemplo com $\gamma_j = (0.5)^{|j|}$ satisfaz a condi√ß√£o de ergodicidade, enquanto o primeiro n√£o.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Exemplo de c√°lculo da soma das autocovari√¢ncias
> gamma_j = lambda j: 1 / (1 + abs(j))
>
> # Calcula a soma para os primeiros 100 termos
> sum_gamma = sum([gamma_j(j) for j in range(100)])
>
> print(f"Soma das autocovari√¢ncias (100 termos): {sum_gamma}")
>
> # Calcula a soma para os primeiros 100 termos com gamma_j = (0.5)^|j|
> gamma_j_2 = lambda j: (0.5)**abs(j)
> sum_gamma_2 = sum([gamma_j_2(j) for j in range(100)])
> print(f"Soma das autocovari√¢ncias (100 termos) com gamma_j = (0.5)^|j|: {sum_gamma_2}")
>
> # Gera um gr√°fico para visualizar o decaimento da fun√ß√£o de autocovari√¢ncia
> j_values = np.arange(0, 20)
> gamma_values = [gamma_j(j) for j in j_values]
> gamma_values_2 = [gamma_j_2(j) for j in j_values]
>
> plt.figure(figsize=(10, 6))
> plt.plot(j_values, gamma_values, marker='o', label='gamma_j = 1 / (1 + |j|)')
> plt.plot(j_values, gamma_values_2, marker='x', label='gamma_j = (0.5)^|j|')
> plt.title('Fun√ß√£o de Autocovari√¢ncia')
> plt.xlabel('j')
> plt.ylabel('gamma_j')
> plt.grid(True)
> plt.legend()
> plt.show()
> ```

A ergodicidade, em sua forma mais geral (Defini√ß√£o 1 do cap√≠tulo anterior), exige que para qualquer fun√ß√£o mensur√°vel *g*, a m√©dia amostral de $g(Y_t)$ convirja em probabilidade para a esperan√ßa de $g(Y_t)$, ou seja, $\text{plim}_{T \to \infty} \frac{1}{T} \sum_{t=1}^T g(Y_t) = E[g(Y_t)]$. A ergodicidade para todos os momentos √© um caso especial dessa defini√ß√£o geral, onde *g* √© uma fun√ß√£o polinomial das vari√°veis aleat√≥rias.

**Teorema 4** (Ergodicidade para Momentos em Processos Gaussianos). Para um processo Gaussiano estacion√°rio, a ergodicidade para a m√©dia implica a ergodicidade para todos os momentos, e a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ √© suficiente para ambas.

*Demonstra√ß√£o*. A demonstra√ß√£o envolve mostrar que, para um processo Gaussiano estacion√°rio, se a m√©dia amostral converge em probabilidade para a m√©dia populacional, ent√£o todos os momentos amostrais tamb√©m convergem em probabilidade para seus respectivos momentos populacionais. Isto decorre da completa caracteriza√ß√£o da distribui√ß√£o conjunta por meio da m√©dia e da fun√ß√£o de autocovari√¢ncia.

I. Seja $\{Y_t\}$ um processo Gaussiano estacion√°rio com m√©dia $\mu$ e fun√ß√£o de autocovari√¢ncia $\gamma_j$.

II. Se o processo √© erg√≥dico para a m√©dia, ent√£o $\text{plim}_{T \to \infty} \frac{1}{T} \sum_{t=1}^T Y_t = \mu$.

III. Considere um momento amostral de ordem *k*: $\frac{1}{T} \sum_{t=1}^T (Y_t - \mu)^k$. Queremos mostrar que $\text{plim}_{T \to \infty} \frac{1}{T} \sum_{t=1}^T (Y_t - \mu)^k = E[(Y_t - \mu)^k]$.

IV. Como o processo √© Gaussiano, todos os momentos podem ser expressos em termos de $\mu$ e $\gamma_j$ [^3].

V. Para *k* par, $E[(Y_t - \mu)^k]$ √© uma fun√ß√£o das autocovari√¢ncias $\gamma_j$. Para *k* √≠mpar, $E[(Y_t - \mu)^k] = 0$ devido √† simetria da distribui√ß√£o normal.

VI. A converg√™ncia em probabilidade da m√©dia amostral e das autocovari√¢ncias amostrais (decorrente de $\sum_{j=0}^{\infty} |\gamma_j| < \infty$) implica a converg√™ncia em probabilidade de todos os momentos amostrais para seus respectivos momentos populacionais.

VII. Portanto, se o processo Gaussiano estacion√°rio √© erg√≥dico para a m√©dia, ele √© erg√≥dico para todos os momentos. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um processo Gaussiano estacion√°rio com m√©dia $\mu = 1$ e fun√ß√£o de autocovari√¢ncia $\gamma_j = (0.8)^{|j|}$. Vamos calcular o primeiro e o segundo momento populacional e comparar com as estimativas amostrais.
>
> M√©dia populacional: $E[Y_t] = \mu = 1$
>
> Segundo momento populacional: $E[(Y_t - \mu)^2] = \gamma_0 = 1$ (Vari√¢ncia)
>
> Agora, simulamos um processo com $T = 1000$ e calculamos as estimativas amostrais:
>
> ```python
> import numpy as np
>
> # Par√¢metros
> mu = 1
> a = 0.8
> T = 1000
>
> # Fun√ß√£o para gerar dados Gaussianos com autocorrela√ß√£o
> def generate_gaussian_correlated_data(mu, T, a):
>     errors = np.random.normal(0, 1, T)
>     Y = np.zeros(T)
>     Y[0] = mu + errors[0]
>     for t in range(1, T):
>         Y[t] = mu + a * (Y[t-1] - mu) + errors[t]
>     return Y
>
> # Simula√ß√£o
> Y = generate_gaussian_correlated_data(mu, T, a)
>
> # Estimativas amostrais
> sample_mean = np.mean(Y)
> sample_variance = np.var(Y)
>
> print(f"M√©dia amostral: {sample_mean}")
> print(f"Vari√¢ncia amostral: {sample_variance}")
> ```
>
> Esperamos que a m√©dia amostral esteja pr√≥xima de 1 e a vari√¢ncia amostral esteja pr√≥xima de 1. No entanto, devido √† natureza aleat√≥ria da simula√ß√£o, haver√° alguma varia√ß√£o. Por exemplo, uma execu√ß√£o t√≠pica pode resultar em:
>
> M√©dia amostral: 0.97
>
> Vari√¢ncia amostral: 1.25
>
> Para um momento de ordem 4, $E[(Y_t - \mu)^4] = 3\gamma_0^2 = 3$, uma vez que √© um processo gaussiano. Podemos calcular o momento amostral de ordem 4:
>
> ```python
> sample_moment_4 = np.mean((Y - mu)**4)
> print(f"Momento amostral de ordem 4: {sample_moment_4}")
> ```
>
> Em uma execu√ß√£o t√≠pica, o resultado seria:
>
> Momento amostral de ordem 4: 3.5
>
> Esses resultados num√©ricos ilustram como os momentos amostrais convergem para os momentos populacionais √† medida que o tamanho da amostra ($T$) aumenta.

**Teorema 4.1** (Converg√™ncia Forte para a M√©dia). Para um processo Gaussiano estacion√°rio, a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ implica a converg√™ncia quase certa da m√©dia amostral para a m√©dia populacional, ou seja, $\frac{1}{T} \sum_{t=1}^T Y_t \xrightarrow{q.c.} \mu$ quando $T \to \infty$.

*Demonstra√ß√£o*. A demonstra√ß√£o se baseia na desigualdade de Kolmogorov e no lema de Borel-Cantelli.

I. Seja $\bar{Y}_T = \frac{1}{T} \sum_{t=1}^T Y_t$ a m√©dia amostral.

II. Queremos mostrar que $P(\lim_{T \to \infty} \bar{Y}_T = \mu) = 1$.

III. Considere a vari√¢ncia de $\bar{Y}_T$: $Var(\bar{Y}_T) = \frac{1}{T^2} \sum_{t=1}^T \sum_{s=1}^T Cov(Y_t, Y_s) = \frac{1}{T^2} \sum_{t=1}^T \sum_{s=1}^T \gamma_{|t-s|}$.

IV. Sob a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$, temos que $Var(\bar{Y}_T) \to 0$ quando $T \to \infty$.

Para esclarecer o passo IV, podemos detalhar o c√°lculo do limite da vari√¢ncia:
V. Suponha que $\sum_{j=0}^{\infty} |\gamma_j| = C < \infty$. Ent√£o,
$$
Var(\bar{Y}_T) = \frac{1}{T^2} \sum_{t=1}^T \sum_{s=1}^T \gamma_{|t-s|} = \frac{1}{T^2} \sum_{t=1}^T \left( \gamma_0 + 2 \sum_{s=1}^{t-1} \gamma_{t-s} \right) = \frac{1}{T^2} \sum_{t=1}^T \left( \gamma_0 + 2 \sum_{j=1}^{t-1} \gamma_{j} \right)
$$

VI. Uma vez que $\left| \gamma_0 + 2 \sum_{j=1}^{t-1} \gamma_{j} \right| \leq \gamma_0 + 2 \sum_{j=1}^{\infty} |\gamma_j| < \infty$, temos:
$$
\lim_{T \to \infty} Var(\bar{Y}_T) \leq \lim_{T \to \infty} \frac{T (\gamma_0 + 2 \sum_{j=1}^{\infty} |\gamma_j|)}{T^2} = \lim_{T \to \infty} \frac{\gamma_0 + 2 \sum_{j=1}^{\infty} |\gamma_j|}{T} = 0
$$

VII. Portanto, $Var(\bar{Y}_T) \to 0$ quando $T \to \infty$.

V. Aplicando a desigualdade de Kolmogorov e o lema de Borel-Cantelli, podemos mostrar que a sequ√™ncia de m√©dias amostrais converge quase certamente para a m√©dia populacional $\mu$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Considere um processo Gaussiano estacion√°rio com m√©dia $\mu=5$ e $\gamma_j = 0.7^{|j|}$. Vamos simular este processo para diferentes tamanhos de amostra $T$ e observar a converg√™ncia da m√©dia amostral para a m√©dia populacional.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> mu = 5
> a = 0.7
>
> # Fun√ß√£o para gerar dados Gaussianos com autocorrela√ß√£o
> def generate_gaussian_correlated_data(mu, T, a):
>     errors = np.random.normal(0, 1, T)
>     Y = np.zeros(T)
>     Y[0] = mu + errors[0]
>     for t in range(1, T):
>         Y[t] = mu + a * (Y[t-1] - mu) + errors[t]
>     return Y
>
> # Tamanhos de amostra
> sample_sizes = [100, 500, 1000, 5000]
>
> # Simula√ß√µes e c√°lculo das m√©dias amostrais
> sample_means = []
> for T in sample_sizes:
>     Y = generate_gaussian_correlated_data(mu, T, a)
>     sample_means.append(np.mean(Y))
>
> # Plotagem das m√©dias amostrais em fun√ß√£o do tamanho da amostra
> plt.figure(figsize=(10, 6))
> plt.plot(sample_sizes, sample_means, marker='o')
> plt.axhline(y=mu, color='r', linestyle='--', label='M√©dia Populacional')
> plt.title('Converg√™ncia da M√©dia Amostral para a M√©dia Populacional')
> plt.xlabel('Tamanho da Amostra (T)')
> plt.ylabel('M√©dia Amostral')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> Este c√≥digo gera um gr√°fico mostrando como a m√©dia amostral se aproxima da m√©dia populacional √† medida que o tamanho da amostra aumenta. Valores t√≠picos para as m√©dias amostrais seriam:
>
> Para T = 100:  4.8
> Para T = 500:  5.1
> Para T = 1000: 4.95
> Para T = 5000: 5.02
>
> Observa-se que, com o aumento de T, a m√©dia amostral converge para a m√©dia populacional de 5.

**Lema 3:** Se um processo Gaussiano estacion√°rio $\{Y_t\}$ satisfaz a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$, ent√£o a fun√ß√£o caracter√≠stica do processo converge para a fun√ß√£o caracter√≠stica da distribui√ß√£o normal com m√©dia $\mu$ e vari√¢ncia $\sigma^2 = \gamma_0 + 2 \sum_{j=1}^{\infty} \gamma_j$.

*Demonstra√ß√£o*. A fun√ß√£o caracter√≠stica de uma vari√°vel aleat√≥ria $Y$ √© definida como $\phi(u) = E[e^{iuY}]$, onde $i$ √© a unidade imagin√°ria.

I. Para um processo Gaussiano estacion√°rio, a fun√ß√£o caracter√≠stica da m√©dia amostral $\bar{Y}$ √© dada por:
$$
\phi_{\bar{Y}}(u) = E\left[e^{iu\bar{Y}}\right] = E\left[e^{iu\frac{1}{T}\sum_{t=1}^T Y_t}\right]
$$

II. Utilizando as propriedades da distribui√ß√£o normal multivariada, podemos mostrar que $\phi_{\bar{Y}}(u)$ converge para a fun√ß√£o caracter√≠stica de uma distribui√ß√£o normal com m√©dia $\mu$ e vari√¢ncia $\frac{\sigma^2}{T}$, onde $\sigma^2 = \gamma_0 + 2 \sum_{j=1}^{\infty} \gamma_j$.

Para detalhar o passo II:

III.  Como $\{Y_t\}$ √© um processo Gaussiano estacion√°rio, $\bar{Y} = \frac{1}{T}\sum_{t=1}^T Y_t$ tamb√©m √© uma vari√°vel aleat√≥ria Gaussiana. Portanto, $\bar{Y} \sim N(E[\bar{Y}], Var[\bar{Y}])$.

IV.  Temos $E[\bar{Y}] = E\left[\frac{1}{T}\sum_{t=1}^T Y_t\right] = \frac{1}{T}\sum_{t=1}^T E[Y_t] = \frac{1}{T}\sum_{t=1}^T \mu = \mu$.

V.  E $Var[\bar{Y}] = \frac{1}{T^2}\sum_{t=1}^T \sum_{s=1}^T Cov(Y_t, Y_s) = \frac{1}{T^2}\sum_{t=1}^T \sum_{s=1}^T \gamma_{|t-s|} = \frac{\sigma^2}{T}$, onde $\sigma^2 = \gamma_0 + 2\sum_{j=1}^{\infty} \gamma_j$.

VI.  A fun√ß√£o caracter√≠stica de uma vari√°vel aleat√≥ria Gaussiana $X \sim N(\mu, \sigma^2)$ √© dada por $\phi_X(u) = e^{iu\mu - \frac{1}{2}\sigma^2u^2}$.

VII. Portanto, a fun√ß√£o caracter√≠stica de $\bar{Y}$ √© $\phi_{\bar{Y}}(u) = e^{iu\mu - \frac{1}{2}\frac{\sigma^2}{T}u^2}$.

III. √Ä medida que $T \to \infty$, a vari√¢ncia $\frac{\sigma^2}{T}$ tende para zero, e a fun√ß√£o caracter√≠stica converge para $\phi(u) = e^{iu\mu}$, que √© a fun√ß√£o caracter√≠stica de uma distribui√ß√£o degenerada em $\mu$.

IV. Portanto, se $\sum_{j=0}^{\infty} |\gamma_j| < \infty$, ent√£o a fun√ß√£o caracter√≠stica do processo Gaussiano estacion√°rio converge para a fun√ß√£o caracter√≠stica da distribui√ß√£o normal com m√©dia $\mu$ e vari√¢ncia $\sigma^2$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Seja um processo Gaussiano estacion√°rio com m√©dia $\mu = 3$ e $\gamma_j = (0.6)^{|j|}$. Ent√£o, $\gamma_0 = 1$ e $\sum_{j=1}^{\infty} \gamma_j = \sum_{j=1}^{\infty} (0.6)^j = \frac{0.6}{1-0.6} = \frac{3}{2}$.
> Assim, $\sigma^2 = 1 + 2 \cdot \frac{3}{2} = 4$.
> Agora, vamos comparar a fun√ß√£o caracter√≠stica te√≥rica com a fun√ß√£o caracter√≠stica amostral para $T = 1000$ e $u = 0.5$.
>
> Fun√ß√£o caracter√≠stica te√≥rica: $\phi(u) = e^{iu\mu} = e^{i(0.5)(3)} = e^{1.5i}$.
>
> Fun√ß√£o caracter√≠stica amostral: Precisamos estimar a m√©dia amostral $\bar{Y}$ primeiro.
>
> ```python
> import numpy as np
> import cmath
>
> # Par√¢metros
> mu = 3
> a = 0.6
> T = 1000
> u = 0.5
>
> # Fun√ß√£o para gerar dados Gaussianos com autocorrela√ß√£o
> def generate_gaussian_correlated_data(mu, T, a):
>     errors = np.random.normal(0, 1, T)
>     Y = np.zeros(T)
>     Y[0] = mu + errors[0]
>     for t in range(1, T):
>         Y[t] = mu + a * (Y[t-1] - mu) + errors[t]
>     return Y
>
> # Simula√ß√£o
> Y = generate_gaussian_correlated_data(mu, T, a)
>
> # M√©dia amostral
> sample_mean = np.mean(Y)
>
> # Fun√ß√£o caracter√≠stica amostral
> phi_sample = np.mean(np.exp(1j * u * Y))
>
> # Fun√ß√£o caracter√≠stica te√≥rica da m√©dia amostral
> sigma_squared = 1 / (1 - a**2) # variancia do processo
> sigma = np.sqrt(sigma_squared / T) # desvio padr√£o da media amostral
> phi_teorico_amostral = cmath.exp(1j*u*mu - 0.5*(sigma_squared/T)*u**2)
>
> print(f"M√©dia amostral: {sample_mean}")
> print(f"Fun√ß√£o caracter√≠stica amostral: {phi_sample}")
> print(f"Fun√ß√£o caracter√≠stica te√≥rica da m√©dia amostral: {phi_teorico_amostral}")
> ```
>
> Uma poss√≠vel sa√≠da do c√≥digo seria:
>
> M√©dia amostral: 2.95
> Fun√ß√£o caracter√≠stica amostral: (0.122-0.992j)
> Fun√ß√£o caracter√≠stica te√≥rica da m√©dia amostral: (0.123-0.992j)
>
> Podemos ver que a fun√ß√£o caracter√≠stica amostral se aproxima da fun√ß√£o caracter√≠stica te√≥rica quando $T$ √© suficientemente grande.

**Lema 3.1** (Taxa de Converg√™ncia da Fun√ß√£o Caracter√≠stica). Sob as mesmas condi√ß√µes do Lema 3, a taxa de converg√™ncia da fun√ß√£o caracter√≠stica $\phi_{\bar{Y}}(u)$ para $\phi(u) = e^{iu\mu}$ √© da ordem de $O(1/T)$.

*Demonstra√ß√£o*. A demonstra√ß√£o envolve expandir a fun√ß√£o caracter√≠stica $\phi_{\bar{Y}}(u)$ em uma s√©rie de Taylor e analisar o termo de erro.

I. Expandindo $\phi_{\bar{Y}}(u)$ em torno de $u=0$, obtemos:
$$
\phi_{\bar{Y}}(u) = e^{iu\mu} \left( 1 - \frac{u^2}{2}Var(\bar{Y}_T) + O(Var(\bar{Y}_T)^2) \right)
$$

II. Como $Var(\bar{Y}_T) = O(1/T)$, temos que o termo de erro √© da ordem de $O(1/T^2)$.

III. Portanto, a taxa de converg√™ncia da fun√ß√£o caracter√≠stica √© da ordem de $O(1/T)$. $\blacksquare$

Este Lema demonstra que, sob a condi√ß√£o de ergodicidade, a distribui√ß√£o do processo Gaussiano estacion√°rio se concentra cada vez mais perto da m√©dia $\mu$, em consist√™ncia com a converg√™ncia em probabilidade da m√©dia amostral.

**Proposi√ß√£o 3** (Converg√™ncia em Distribui√ß√£o). Se um processo Gaussiano estacion√°rio $\{Y_t\}$ satisfaz a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$, ent√£o a m√©dia amostral $\bar{Y}$ converge em distribui√ß√£o para uma distribui√ß√£o degenerada em $\mu$.

*Demonstra√ß√£o*.

I. Pelo Lema 3, a fun√ß√£o caracter√≠stica da m√©dia amostral $\bar{Y}$ converge para a fun√ß√£o caracter√≠stica de uma distribui√ß√£o degenerada em $\mu$.

II. Pelo Teorema da Continuidade de L√©vy, a converg√™ncia da fun√ß√£o caracter√≠stica implica a converg√™ncia em distribui√ß√£o.

III. Portanto, $\bar{Y}$ converge em distribui√ß√£o para uma distribui√ß√£o degenerada em $\mu$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Considere um processo Gaussiano estacion√°rio com m√©dia $\mu = 10$ e fun√ß√£o de autocovari√¢ncia $\gamma_j = (0.9)^{|j|}$. Queremos verificar numericamente a converg√™ncia em distribui√ß√£o da m√©dia amostral para uma distribui√ß√£o degenerada em $\mu$.
>
> Podemos simular o processo para diferentes tamanhos de amostra e plotar os histogramas das m√©dias amostrais. √Ä medida que o tamanho da amostra aumenta, o histograma deve se concentrar em torno de $\mu = 10$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy.stats import norm
>
> # Par√¢metros
> mu = 10
> a = 0.9
> num_simulations = 1000
> sample_sizes = [100, 500, 1000, 5000]
>
> # Fun√ß√£o para gerar dados Gaussianos com autocorrela√ß√£o
> def generate_gaussian_correlated_data(mu, T, a):
>     errors = np.random.normal(0, 1, T)
>     Y = np.zeros(T)
>     Y[0] = mu + errors[0]
>     for t in range(1, T):
>         Y[t] = mu + a * (Y[t-1] - mu) + errors[t]
>     return Y
>
> # Criar subplots
> fig, axes = plt.subplots(2, 2, figsize=(12, 8))
> axes = axes.flatten()
>
> # Loop atrav√©s dos tamanhos de amostra
> for i, T in enumerate(sample_sizes):
>     # Simula√ß√µes
>     sample_means = []
>     for _ in range(num_simulations):
>         Y = generate_gaussian_correlated_data(mu, T, a)
>         sample_means.append(np.mean(Y))
>
>     # Plotar histograma das m√©dias amostrais
>     ax = axes[i]
>     ax.hist(sample_means, bins=30, density=True, alpha=0.6, color='g')
>
>     # Calcular vari√¢ncia te√≥rica da m√©dia amostral
>     sigma_squared = 1 / (1 - a**2)
>     sigma = np.sqrt(sigma_squared / T)
>
>     # Sobrepor a densidade da distribui√ß√£o normal com m√©dia mu e vari√¢ncia sigma^2/T
>     x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)
>     ax.plot(x, norm.pdf(x, mu, sigma), 'k--', linewidth=2, label='Distribui√ß√£o Normal Limite')
>
>     ax.axvline(x=mu, color='r', linestyle='--', label='M√©dia Populacional')
>     ax.set_title(f'Tamanho da Amostra = {T}')
>     ax.set_xlabel('M√©dia Amostral')
>     ax.set_ylabel('Densidade')
>     ax.legend()
>     ax.grid(True)
>
> plt.tight_layout()
> plt.show()
> ```
>
> Os histogramas mostrar√£o que, √† medida que o tamanho da amostra aumenta, a distribui√ß√£o das m√©dias amostrais se torna mais concentrada em torno de 10, aproximando-se de uma distribui√ß√£o degenerada.

### Conclus√£o
Este cap√≠tulo demonstrou que, para processos Gaussianos estacion√°rios, a ergodicidade para a m√©dia implica a ergodicidade para todos os momentos, e que a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ √© suficiente para ambas. A ergodicidade para todos os momentos garante que todas as estat√≠sticas amostrais convergem para seus valores te√≥ricos correspondentes, permitindo uma caracteriza√ß√£o completa do processo a partir de uma √∫nica realiza√ß√£o suficientemente longa. Adicionalmente, exploramos a converg√™ncia da fun√ß√£o caracter√≠stica e a converg√™ncia em distribui√ß√£o da m√©dia amostral, fornecendo uma vis√£o mais completa do comportamento assint√≥tico de processos Gaussianos estacion√°rios.
<!-- END -->