## Condi√ß√£o Suficiente para Ergodicidade da M√©dia via Autocovari√¢ncias

### Introdu√ß√£o
Este cap√≠tulo examina em detalhe uma **condi√ß√£o suficiente para a ergodicidade da m√©dia** em processos estacion√°rios, focando especificamente no comportamento das autocovari√¢ncias √† medida que o *lag* temporal aumenta. Como estabelecido anteriormente, a ergodicidade para a m√©dia √© crucial para inferir propriedades estat√≠sticas de um processo a partir de uma √∫nica realiza√ß√£o amostral [^4]. Em continuidade ao conceito apresentado, vamos explorar a condi√ß√£o suficiente:
$$
\sum_{j=0}^{\infty} |\gamma_j| < \infty
$$
que garante que as autocovari√¢ncias $\gamma_j$ convirjam para zero "suficientemente r√°pido" √† medida que *j* (o *lag* temporal) aumenta, assegurando a ergodicidade do processo [^4]. O presente cap√≠tulo tem como objetivo elucidar o significado e as implica√ß√µes dessa condi√ß√£o, oferecendo exemplos e demonstra√ß√µes rigorosas.

Para complementar essa introdu√ß√£o, √© importante ressaltar que a ergodicidade n√£o se limita apenas √† m√©dia. Ela pode ser estendida para outras fun√ß√µes das vari√°veis aleat√≥rias do processo.

**Defini√ß√£o 1:** Um processo estacion√°rio $\{Y_t\}$ √© dito ser *erg√≥dico* (no sentido geral) se, para qualquer fun√ß√£o mensur√°vel $g$, a m√©dia amostral de $g(Y_t)$ converge em probabilidade para a esperan√ßa de $g(Y_t)$, ou seja, $\text{plim}_{T \to \infty} \frac{1}{T} \sum_{t=1}^T g(Y_t) = E[g(Y_t)]$.

Essa defini√ß√£o mais geral abrange a ergodicidade da m√©dia como um caso especial, onde $g(Y_t) = Y_t$. As condi√ß√µes para a ergodicidade no sentido geral s√£o mais complexas e envolvem a estrutura de depend√™ncia entre as vari√°veis aleat√≥rias do processo. No entanto, a condi√ß√£o de somabilidade absoluta das autocovari√¢ncias √© um passo fundamental na compreens√£o da ergodicidade de processos estacion√°rios.

### Conceitos Fundamentais
Um processo *covariance-stationary* $\{Y_t\}$ √© erg√≥dico para a m√©dia se a m√©dia amostral $\bar{Y} = \frac{1}{T} \sum_{t=1}^T Y_t$ converge em probabilidade para a m√©dia populacional $\mu = E(Y_t)$ √† medida que $T \to \infty$ [^4]. A condi√ß√£o
$$
\sum_{j=0}^{\infty} |\gamma_j| < \infty
$$
√© suficiente para garantir essa converg√™ncia, onde $\gamma_j = Cov(Y_t, Y_{t-j})$ representa a autocovari√¢ncia no *lag* *j* [^4]. Esta condi√ß√£o implica que a soma dos valores absolutos das autocovari√¢ncias, desde o *lag* 0 at√© o infinito, √© finita. Em termos pr√°ticos, isso significa que a influ√™ncia de observa√ß√µes passadas decresce rapidamente, tornando a m√©dia amostral uma representa√ß√£o precisa da m√©dia populacional.

> üí° **Exemplo Num√©rico:**
>
> Suponha que as autocovari√¢ncias de um processo estacion√°rio s√£o dadas por $\gamma_j = a^j$, onde $0 < a < 1$. Vamos verificar a condi√ß√£o de ergodicidade.
>
> A soma das autocovari√¢ncias √©:
>
> $$
> \sum_{j=0}^{\infty} |\gamma_j| = \sum_{j=0}^{\infty} a^j = \frac{1}{1-a}
> $$
>
> Como $0 < a < 1$, a soma converge para um valor finito $\frac{1}{1-a}$. Portanto, a condi√ß√£o de ergodicidade para a m√©dia √© satisfeita. Por exemplo, se $a = 0.5$, ent√£o $\sum_{j=0}^{\infty} |\gamma_j| = \frac{1}{1-0.5} = 2$.
>
> üí° **Exemplo Num√©rico:**
>
> Considere um processo em que $\gamma_0 = 1$, $\gamma_1 = 0.5$, $\gamma_2 = 0.25$ e $\gamma_j = 0$ para $j > 2$. Vamos verificar a condi√ß√£o de ergodicidade.
>
> $$
> \sum_{j=0}^{\infty} |\gamma_j| = |\gamma_0| + |\gamma_1| + |\gamma_2| + \sum_{j=3}^{\infty} |\gamma_j| = 1 + 0.5 + 0.25 + 0 = 1.75
> $$
>
> Neste caso, a soma das autocovari√¢ncias √© finita (1.75), e portanto, a condi√ß√£o de ergodicidade √© satisfeita. Isso significa que, para este processo espec√≠fico, a m√©dia amostral convergir√° para a m√©dia populacional √† medida que aumentarmos o tamanho da amostra.
>
> üí° **Exemplo Num√©rico:**
>
> Agora, suponha que as autocovari√¢ncias s√£o dadas por $\gamma_j = \frac{1}{j+1}$. Vamos verificar se a condi√ß√£o de ergodicidade √© satisfeita.
>
> $$
> \sum_{j=0}^{\infty} |\gamma_j| = \sum_{j=0}^{\infty} \frac{1}{j+1} = 1 + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \ldots
> $$
>
> Esta √© a s√©rie harm√¥nica, que diverge. Portanto, a condi√ß√£o de ergodicidade n√£o √© satisfeita neste caso. Isso implica que, mesmo com um tamanho de amostra muito grande, a m√©dia amostral pode n√£o convergir para a m√©dia populacional.

Para solidificar o entendimento sobre a rela√ß√£o entre a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ e a estrutura de depend√™ncia do processo, podemos apresentar uma caracteriza√ß√£o alternativa dessa condi√ß√£o em termos do espectro do processo.

**Teorema 1:** Seja $\{Y_t\}$ um processo estacion√°rio com fun√ß√£o de autocovari√¢ncia $\gamma_j$ e densidade espectral $f(\omega)$. Ent√£o, $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$ se e somente se $f(\omega)$ √© cont√≠nua.

*Demonstra√ß√£o*. (Esbo√ßo) A demonstra√ß√£o utiliza a rela√ß√£o entre a fun√ß√£o de autocovari√¢ncia e a densidade espectral, dada pela transformada de Fourier. A condi√ß√£o $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$ implica que a transformada de Fourier inversa (que √© a densidade espectral) √© cont√≠nua. Reciprocamente, se $f(\omega)$ √© cont√≠nua, ent√£o sua transformada de Fourier (que √© a fun√ß√£o de autocovari√¢ncia) satisfaz a condi√ß√£o de somabilidade absoluta. $\blacksquare$

Este teorema oferece uma perspectiva diferente sobre a condi√ß√£o de ergodicidade, conectando-a com a suavidade do espectro do processo.

**Lema 2:** Se $\sum_{j=0}^{\infty} |\gamma_j| < \infty$, ent√£o $\lim_{j \to \infty} \gamma_j = 0$.

*Demonstra√ß√£o*. Assumimos que a s√©rie $\sum_{j=0}^{\infty} |\gamma_j|$ converge para um valor finito, digamos $S$. Pela defini√ß√£o de converg√™ncia de uma s√©rie, para qualquer $\epsilon > 0$, existe um inteiro $N$ tal que para todo $n > N$:
$$
\left| \sum_{j=0}^{n} |\gamma_j| - S \right| < \epsilon
$$
Considere a sequ√™ncia de somas parciais $S_n = \sum_{j=0}^{n} |\gamma_j|$. Como a s√©rie converge, a sequ√™ncia $S_n$ converge para $S$. Isso implica que a sequ√™ncia $\{|\gamma_j|\}$ converge para zero. Para ver isso, note que para $n$ suficientemente grande, $|\gamma_n|$ deve ser arbitrariamente pequeno, caso contr√°rio, a s√©rie n√£o convergiria. Formalmente, dado $\epsilon > 0$, escolha $N$ tal que $\sum_{j=N}^{\infty} |\gamma_j| < \epsilon$. Ent√£o, para $j > N$, temos $|\gamma_j| < \epsilon$. Portanto, $\lim_{j \to \infty} |\gamma_j| = 0$, o que implica que $\lim_{j \to \infty} \gamma_j = 0$. $\blacksquare$

O Lema 2 demonstra que a condi√ß√£o de somabilidade absoluta das autocovari√¢ncias implica que as autocovari√¢ncias individuais convergem para zero √† medida que o *lag* aumenta. Esse resultado refor√ßa a intui√ß√£o de que observa√ß√µes distantes no tempo t√™m influ√™ncia cada vez menor no valor presente do processo.

**Proposi√ß√£o 2** (Converg√™ncia em M√©dia Quadr√°tica). Se $\sum_{j=0}^{\infty} |\gamma_j| < \infty$, ent√£o a m√©dia amostral $\bar{Y}$ converge em m√©dia quadr√°tica para a m√©dia populacional $\mu$, ou seja, $E[(\bar{Y} - \mu)^2] \to 0$ quando $T \to \infty$.

*Demonstra√ß√£o*.
I. Anteriormente [Lema 1, Se√ß√£o: Ergodicidade para a M√©dia em Processos Covariance-Estacion√°rios], mostramos que
$$
E[(\bar{Y} - \mu)^2] = \frac{1}{T} \sum_{j=-(T-1)}^{T-1} \left(1 - \frac{|j|}{T}\right) \gamma_j
$$

II.  Tomando o limite quando $T \to \infty$:
$$
\lim_{T \to \infty} E[(\bar{Y} - \mu)^2] = \lim_{T \to \infty} \frac{1}{T} \sum_{j=-(T-1)}^{T-1} \left(1 - \frac{|j|}{T}\right) \gamma_j
$$

III. Como $\sum_{j=0}^{\infty} |\gamma_j| < \infty$, a sequ√™ncia $\gamma_j$ √© absolutamente som√°vel.  Isto implica que, para $T$ suficientemente grande, $\frac{1}{T}\sum_{j=-(T-1)}^{T-1} \left(1 - \frac{|j|}{T}\right) \gamma_j$ converge para zero.

IV. Portanto,
$$
\lim_{T \to \infty} E[(\bar{Y} - \mu)^2] = 0
$$
que demonstra a converg√™ncia em m√©dia quadr√°tica. $\blacksquare$

A Proposi√ß√£o 2 estabelece que sob a condi√ß√£o de somabilidade absoluta das autocovari√¢ncias, a m√©dia amostral converge em m√©dia quadr√°tica para a m√©dia populacional. Esse resultado √© crucial porque a converg√™ncia em m√©dia quadr√°tica implica converg√™ncia em probabilidade, fortalecendo a garantia de que a m√©dia amostral √© um estimador consistente da m√©dia populacional.

Podemos fortalecer este resultado mostrando que a converg√™ncia em m√©dia quadr√°tica implica converg√™ncia em $L^p$ para $1 \le p \le 2$:

**Proposi√ß√£o 2.1** (Converg√™ncia em $L^p$). Se $\sum_{j=0}^{\infty} |\gamma_j| < \infty$, ent√£o a m√©dia amostral $\bar{Y}$ converge em $L^p$ para a m√©dia populacional $\mu$ para todo $1 \le p \le 2$, ou seja, $E[|\bar{Y} - \mu|^p] \to 0$ quando $T \to \infty$.

*Demonstra√ß√£o*.
I. Pela desigualdade de Lyapunov, para $0 < q < p$, temos $(E[|X|^q])^{1/q} \le (E[|X|^p])^{1/p}$.

II.  Tomando $X = \bar{Y} - \mu$, $p=2$ e $q=p$ com $1 \le p \le 2$, obtemos
$$ (E[|\bar{Y} - \mu|^p])^{1/p} \le (E[|\bar{Y} - \mu|^2])^{1/2}. $$

III. Elevando ambos os lados √† pot√™ncia $p$:
$$ E[|\bar{Y} - \mu|^p] \le (E[|\bar{Y} - \mu|^2])^{p/2}. $$

IV. Como $E[(\bar{Y} - \mu)^2] \to 0$ quando $T \to \infty$ (pela Proposi√ß√£o 2), temos que $(E[|\bar{Y} - \mu|^2])^{p/2} \to 0$.

V. Portanto, $E[|\bar{Y} - \mu|^p] \to 0$, o que demonstra a converg√™ncia em $L^p$. $\blacksquare$

Este resultado generaliza a Proposi√ß√£o 2 e fornece uma caracteriza√ß√£o mais completa da converg√™ncia da m√©dia amostral.

**Teorema 3** (Converg√™ncia em Probabilidade). Se $\sum_{j=0}^{\infty} |\gamma_j| < \infty$, ent√£o a m√©dia amostral $\bar{Y}$ converge em probabilidade para a m√©dia populacional $\mu$, ou seja, $\text{plim}_{T \to \infty} \bar{Y} = \mu$.

*Demonstra√ß√£o*.
I. Pela desigualdade de Chebyshev, para qualquer $\epsilon > 0$:
$$
P(|\bar{Y} - \mu| > \epsilon) \leq \frac{E[(\bar{Y} - \mu)^2]}{\epsilon^2}
$$

II. Como $E[(\bar{Y} - \mu)^2] \to 0$ quando $T \to \infty$ (pela Proposi√ß√£o 2), temos que
$$
\lim_{T \to \infty} P(|\bar{Y} - \mu| > \epsilon) \leq \lim_{T \to \infty} \frac{E[(\bar{Y} - \mu)^2]}{\epsilon^2} = 0
$$

III. Portanto, para qualquer $\epsilon > 0$, $\lim_{T \to \infty} P(|\bar{Y} - \mu| > \epsilon) = 0$.

IV. Por defini√ß√£o, isso significa que $\text{plim}_{T \to \infty} \bar{Y} = \mu$. $\blacksquare$

O Teorema 3 formaliza a conex√£o entre a condi√ß√£o de somabilidade absoluta das autocovari√¢ncias e a ergodicidade para a m√©dia. Ele demonstra que sob essa condi√ß√£o, a m√©dia amostral converge em probabilidade para a m√©dia populacional, garantindo que a m√©dia amostral √© um estimador consistente da m√©dia populacional.

> üí° **Exemplo Num√©rico:**
>
> Considere novamente um processo estacion√°rio com autocovari√¢ncias $\gamma_j = a^j$, onde $0 < a < 1$. Como demonstrado anteriormente, esse processo satisfaz a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$. Portanto, pelo Teorema 3, a m√©dia amostral converge em probabilidade para a m√©dia populacional.
>
> Podemos simular este processo para verificar a converg√™ncia.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> mu = 10
> a = 0.8
> T = 1000
>
> # Fun√ß√£o para gerar dados com autocorrela√ß√£o
> def generate_correlated_data(mu, T, a):
>     errors = np.random.normal(0, 1, T)
>     Y = np.zeros(T)
>     Y[0] = mu + errors[0]
>     for t in range(1, T):
>         Y[t] = mu + a * (Y[t-1] - mu) + errors[t]
>     return Y
>
> # Gerar dados
> Y = generate_correlated_data(mu, T, a)
>
> # Calcular m√©dias amostrais cumulativas
> cumulative_means = np.cumsum(Y) / np.arange(1, T + 1)
>
> # Plotar as m√©dias amostrais cumulativas
> plt.figure(figsize=(10, 6))
> plt.plot(cumulative_means)
> plt.axhline(y=mu, color='r', linestyle='--', label='M√©dia Populacional')
> plt.title('Converg√™ncia da M√©dia Amostral com Autocovari√¢ncias Decaentes Exponencialmente')
> plt.xlabel('Tamanho da Amostra (T)')
> plt.ylabel('M√©dia Amostral')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> O gr√°fico demonstra que a m√©dia amostral converge para a m√©dia populacional √† medida que o tamanho da amostra aumenta, confirmando o resultado do Teorema 3.
>
> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um processo com m√©dia $\mu = 5$ e autocovari√¢ncias $\gamma_j = 0.9^j$. Queremos verificar empiricamente a converg√™ncia em probabilidade.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Define par√¢metros
> mu = 5
> a = 0.9
> num_simulations = 100
> sample_sizes = [100, 500, 1000, 5000]
>
> # Fun√ß√£o para gerar dados com autocorrela√ß√£o
> def generate_correlated_data(mu, T, a):
>     errors = np.random.normal(0, 1, T)
>     Y = np.zeros(T)
>     Y[0] = mu + errors[0]
>     for t in range(1, T):
>         Y[t] = mu + a * (Y[t-1] - mu) + errors[t]
>     return Y
>
> # Simula√ß√µes
> plt.figure(figsize=(12, 8))
> for T in sample_sizes:
>     sample_means = []
>     for _ in range(num_simulations):
>         Y = generate_correlated_data(mu, T, a)
>         sample_means.append(np.mean(Y))
>
>     # Plota histograma das m√©dias amostrais
>     plt.hist(sample_means, bins=20, alpha=0.5, label=f'T = {T}')
>
> plt.axvline(x=mu, color='r', linestyle='--', label='M√©dia Populacional')
> plt.title('Distribui√ß√£o das M√©dias Amostrais para Diferentes Tamanhos de Amostra')
> plt.xlabel('M√©dia Amostral')
> plt.ylabel('Frequ√™ncia')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> Este c√≥digo simula o processo v√°rias vezes para diferentes tamanhos de amostra e plota a distribui√ß√£o das m√©dias amostrais. Observamos que, √† medida que o tamanho da amostra aumenta, a distribui√ß√£o das m√©dias amostrais se concentra mais perto da m√©dia populacional, demonstrando a converg√™ncia em probabilidade.

**Corol√°rio 2:** Se $\{Y_t\}$ √© um processo ARMA estacion√°rio, ent√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$.

*Demonstra√ß√£o*. Processos ARMA estacion√°rios t√™m autocovari√¢ncias que decaem exponencialmente [^4]. Logo, a soma de seus valores absolutos converge para um valor finito. $\blacksquare$

Este corol√°rio conecta a condi√ß√£o estudada com a classe importante de processos ARMA.

Para complementar o corol√°rio 2, podemos apresentar uma demonstra√ß√£o mais detalhada.

**Corol√°rio 2.1:** Se $\{Y_t\}$ √© um processo ARMA(p,q) estacion√°rio, ent√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$.

*Demonstra√ß√£o*.
I. Um processo ARMA(p,q) pode ser expresso como
$$
Y_t = \phi_1 Y_{t-1} + \ldots + \phi_p Y_{t-p} + \epsilon_t + \theta_1 \epsilon_{t-1} + \ldots + \theta_q \epsilon_{t-q}
$$
onde $\{\epsilon_t\}$ √© um ru√≠do branco com vari√¢ncia $\sigma^2$.

II. A estacionariedade do processo ARMA implica que as ra√≠zes do polin√¥mio caracter√≠stico associado √† parte AR est√£o fora do c√≠rculo unit√°rio.

III. As autocovari√¢ncias de um processo ARMA satisfazem uma equa√ß√£o de diferen√ßa linear homog√™nea.

IV. A solu√ß√£o desta equa√ß√£o de diferen√ßa √© uma combina√ß√£o linear de termos que decaem exponencialmente, ou seja, $\gamma_j = \sum_{i=1}^p A_i z_i^j$ onde $z_i$ s√£o as ra√≠zes do polin√¥mio caracter√≠stico e $|z_i| < 1$ devido √† estacionariedade.

V. Portanto, $|\gamma_j| \le \sum_{i=1}^p |A_i| |z_i|^j$ e
$$
\sum_{j=0}^{\infty} |\gamma_j| \le \sum_{j=0}^{\infty} \sum_{i=1}^p |A_i| |z_i|^j = \sum_{i=1}^p |A_i| \sum_{j=0}^{\infty} |z_i|^j = \sum_{i=1}^p |A_i| \frac{1}{1 - |z_i|} < \infty
$$

VI. Como a soma √© finita, a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ √© satisfeita. $\blacksquare$

Esta demonstra√ß√£o mais detalhada fornece uma justificativa rigorosa para a afirma√ß√£o de que processos ARMA estacion√°rios satisfazem a condi√ß√£o de somabilidade absoluta das autocovari√¢ncias.
> üí° **Exemplo Num√©rico:**
>
> Considere um processo AR(1) definido por $Y_t = 0.7 Y_{t-1} + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco com vari√¢ncia 1. Para este processo, $\phi_1 = 0.7$. As autocovari√¢ncias s√£o dadas por $\gamma_j = \phi_1^j \gamma_0$, onde $\gamma_0 = \frac{1}{1 - \phi_1^2}$. Portanto, $\gamma_0 = \frac{1}{1 - 0.7^2} = \frac{1}{1 - 0.49} = \frac{1}{0.51} \approx 1.96$.
>
> Ent√£o, $\gamma_j = 0.7^j \times 1.96$. Vamos calcular as primeiras autocovari√¢ncias e verificar se a soma converge:
>
> $\gamma_0 = 1.96$
>
> $\gamma_1 = 0.7 \times 1.96 = 1.372$
>
> $\gamma_2 = 0.7^2 \times 1.96 = 0.9604$
>
> $\gamma_3 = 0.7^3 \times 1.96 = 0.67228$
>
> A soma dos valores absolutos das autocovari√¢ncias √©:
>
> $\sum_{j=0}^{\infty} |\gamma_j| = \sum_{j=0}^{\infty} |0.7^j \times 1.96| = 1.96 \sum_{j=0}^{\infty} 0.7^j = 1.96 \times \frac{1}{1 - 0.7} = 1.96 \times \frac{1}{0.3} \approx 6.53$
>
> A soma converge para 6.53, que √© um valor finito. Portanto, a condi√ß√£o de ergodicidade √© satisfeita para este processo AR(1).
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> phi = 0.7
> variance_epsilon = 1
> T = 500
>
> # Gerar dados AR(1)
> np.random.seed(42)  # para reproducibilidade
> epsilon = np.random.normal(0, np.sqrt(variance_epsilon), T)
> Y = np.zeros(T)
> Y[0] = epsilon[0]
> for t in range(1, T):
>     Y[t] = phi * Y[t-1] + epsilon[t]
>
> # Calcular autocovari√¢ncias amostrais
> def autocovariance(x, lag):
>     n = len(x)
>     x_mean = np.mean(x)
>     return np.sum((x[:n-lag] - x_mean) * (x[lag:] - x_mean)) / n
>
> lags = np.arange(0, 20)
> sample_autocovariances = [autocovariance(Y, lag) for lag in lags]
>
> # Calcular autocovari√¢ncias te√≥ricas
> gamma0 = variance_epsilon / (1 - phi**2)
> theoretical_autocovariances = gamma0 * phi**lags
>
> # Plotar autocovari√¢ncias
> plt.figure(figsize=(10, 6))
> plt.plot(lags, sample_autocovariances, marker='o', label='Amostral')
> plt.plot(lags, theoretical_autocovariances[:20], linestyle='--', label='Te√≥rica')
> plt.title('Autocovari√¢ncias Amostrais e Te√≥ricas para AR(1)')
> plt.xlabel('Lag')
> plt.ylabel('Autocovari√¢ncia')
> plt.legend()
> plt.grid(True)
> plt.show()
>
> # Calcular e plotar soma cumulativa das autocovari√¢ncias amostrais
> cumulative_sum = np.cumsum(np.abs(sample_autocovariances))
>
> plt.figure(figsize=(10, 6))
> plt.plot(lags, cumulative_sum, marker='o')
> plt.title('Soma Cumulativa dos Valores Absolutos das Autocovari√¢ncias Amostrais')
> plt.xlabel('Lag')
> plt.ylabel('Soma Cumulativa')
> plt.grid(True)
> plt.show()
> ```
>
> Este c√≥digo calcula e plota as autocovari√¢ncias amostrais e te√≥ricas para o processo AR(1), juntamente com a soma cumulativa dos valores absolutos das autocovari√¢ncias amostrais. O gr√°fico da soma cumulativa deve convergir para um valor finito, confirmando a condi√ß√£o de ergodicidade.

### Conclus√£o
Este cap√≠tulo explorou em profundidade a condi√ß√£o suficiente $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ para a ergodicidade da m√©dia em processos estacion√°rios. Demonstramos que essa condi√ß√£o implica a converg√™ncia das autocovari√¢ncias para zero, a converg√™ncia em m√©dia quadr√°tica da m√©dia amostral para a m√©dia populacional e, finalmente, a converg√™ncia em probabilidade da m√©dia amostral. A compreens√£o desses resultados √© fundamental para justificar a utiliza√ß√£o de m√©dias amostrais como estimadores consistentes da m√©dia populacional em an√°lises de s√©ries temporais. Al√©m disso, estabelecemos uma conex√£o direta entre a ergodicidade da m√©dia e os processos ARMA estacion√°rios, mostrando que essa classe de processos satisfaz a condi√ß√£o de somabilidade absoluta das autocovari√¢ncias.
<!-- END -->