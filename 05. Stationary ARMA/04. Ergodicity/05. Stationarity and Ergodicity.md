## Estacionariedade versus Ergodicidade: Distin√ß√µes Conceituais

### Introdu√ß√£o
Este cap√≠tulo aprofunda a distin√ß√£o entre os conceitos de **estacionariedade** e **ergodicidade**, elucidando como um processo estoc√°stico pode ser estacion√°rio sem ser necessariamente erg√≥dico. Como vimos anteriormente [^4], a estacionaridade implica que as propriedades estat√≠sticas de um processo (como a m√©dia e a autocovari√¢ncia) n√£o variam ao longo do tempo. Expandindo o conceito apresentado, a ergodicidade garante que as m√©dias temporais convergem para as m√©dias de conjunto, permitindo inferir as propriedades estat√≠sticas do processo a partir de uma √∫nica realiza√ß√£o [^4]. No entanto, exploraremos um cen√°rio onde um processo estacion√°rio pode falhar em satisfazer a condi√ß√£o de ergodicidade, demonstrando que a estacionaridade √© uma condi√ß√£o necess√°ria, mas n√£o suficiente, para a ergodicidade.

### Conceitos Fundamentais
Um processo estoc√°stico $\{Y_t\}$ √© dito ser *covariance-stationary* se sua m√©dia $E(Y_t) = \mu$ e sua fun√ß√£o de autocovari√¢ncia $Cov(Y_t, Y_{t-j}) = \gamma_j$ n√£o dependem do tempo $t$, mas apenas do *lag* $j$ [^4]. J√° a ergodicidade, em seu sentido mais comum (ergodicidade para a m√©dia), requer que a m√©dia amostral $\bar{Y} = \frac{1}{T} \sum_{t=1}^T Y_t$ convirja em probabilidade para a m√©dia populacional $\mu$ quando o tamanho da amostra $T$ tende ao infinito [^4].

No entanto, √© crucial reconhecer que a ergodicidade e a estacionaridade s√£o propriedades distintas. Enquanto a estacionaridade se refere √† estabilidade das propriedades estat√≠sticas ao longo do tempo, a ergodicidade se refere √† rela√ß√£o entre as m√©dias temporais (calculadas a partir de uma √∫nica realiza√ß√£o) e as m√©dias de conjunto (calculadas sobre todas as poss√≠veis realiza√ß√µes).

Para ilustrar essa distin√ß√£o, considere um processo onde a m√©dia de cada realiza√ß√£o √© um valor aleat√≥rio extra√≠do de uma distribui√ß√£o, mas a m√©dia amostral converge para essa m√©dia espec√≠fica da realiza√ß√£o, em vez da m√©dia geral da distribui√ß√£o das m√©dias. Esse processo √© estacion√°rio, mas n√£o erg√≥dico.

> üí° **Exemplo Num√©rico:**
>
> Considere um processo estoc√°stico $\{Y_t^{(i)}\}$ definido como:
> $$
> Y_t^{(i)} = \mu^{(i)} + \epsilon_t
> $$
> onde:
>
> *   $i$ √© o √≠ndice da realiza√ß√£o.
> *   $\mu^{(i)}$ √© uma vari√°vel aleat√≥ria extra√≠da de uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\lambda^2$, ou seja, $\mu^{(i)} \sim N(0, \lambda^2)$.
> *   $\epsilon_t$ √© um ru√≠do branco Gaussiano com m√©dia zero e vari√¢ncia $\sigma^2$, independente de $\mu^{(i)}$, ou seja, $\epsilon_t \sim N(0, \sigma^2)$.
>
> Para este processo, a m√©dia para cada realiza√ß√£o $i$ √© $\mu^{(i)}$, mas a m√©dia geral do processo √© $E[Y_t] = E[\mu^{(i)}] + E[\epsilon_t] = 0 + 0 = 0$.
>
> A autocovari√¢ncia √© dada por:
> $$
> Cov(Y_t, Y_{t-j}) = E[(Y_t - E[Y_t])(Y_{t-j} - E[Y_{t-j}])] = E[(\mu^{(i)} + \epsilon_t)(\mu^{(i)} + \epsilon_{t-j})]
> $$
>
> Se $j = 0$, ent√£o $Cov(Y_t, Y_t) = E[(\mu^{(i)} + \epsilon_t)^2] = E[(\mu^{(i)})^2] + E[\epsilon_t^2] = \lambda^2 + \sigma^2$.
>
> Se $j \neq 0$, ent√£o $Cov(Y_t, Y_{t-j}) = E[(\mu^{(i)} + \epsilon_t)(\mu^{(i)} + \epsilon_{t-j})] = E[(\mu^{(i)})^2] = \lambda^2$.
>
> Portanto, o processo √© *covariance-stationary* porque a m√©dia e a autocovari√¢ncia n√£o dependem do tempo $t$. No entanto, vamos mostrar que esse processo n√£o √© erg√≥dico.
>
> A m√©dia amostral para cada realiza√ß√£o $i$ √©:
> $$
> \bar{Y}^{(i)} = \frac{1}{T} \sum_{t=1}^{T} Y_t^{(i)} = \frac{1}{T} \sum_{t=1}^{T} (\mu^{(i)} + \epsilon_t) = \mu^{(i)} + \frac{1}{T} \sum_{t=1}^{T} \epsilon_t
> $$
>
> √Ä medida que $T \to \infty$, $\frac{1}{T} \sum_{t=1}^{T} \epsilon_t \xrightarrow{p} 0$ (pela lei dos grandes n√∫meros).
>
> Portanto, $\text{plim}_{T \to \infty} \bar{Y}^{(i)} = \mu^{(i)} \neq E[Y_t] = 0$.
>
> Este exemplo demonstra que a m√©dia amostral converge para a m√©dia espec√≠fica da realiza√ß√£o $\mu^{(i)}$, em vez da m√©dia de conjunto zero [^4]. Portanto, o processo √© estacion√°rio, mas n√£o erg√≥dico [^4].
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> lambda_sq = 1
> sigma_sq = 2
> num_realizations = 5
> T = 500
>
> # Fun√ß√£o para gerar dados
> def generate_data(lambda_sq, sigma_sq, T):
>     mu_i = np.random.normal(0, np.sqrt(lambda_sq))
>     epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
>     Y = mu_i + epsilon
>     return Y, mu_i
>
> # Gera v√°rias realiza√ß√µes
> realizations = []
> realization_means = []
> for _ in range(num_realizations):
>     Y, mu_i = generate_data(lambda_sq, sigma_sq, T)
>     realizations.append(Y)
>     realization_means.append(mu_i)
>
> # Calcula m√©dias amostrais e de conjunto
> ensemble_mean = np.mean(realization_means)
> sample_means = [np.mean(realization) for realization in realizations]
>
> # Plota as realiza√ß√µes
> plt.figure(figsize=(12, 8))
> for i, realization in enumerate(realizations):
>     plt.plot(realization, label=f'Realiza√ß√£o {i+1}, M√©dia = {realization_means[i]:.2f}')
> plt.axhline(y=ensemble_mean, color='k', linestyle='--', label=f'M√©dia de Conjunto = {ensemble_mean:.2f}')
> plt.title('Realiza√ß√µes de um Processo Estacion√°rio N√£o Ergodico')
> plt.xlabel('Tempo')
> plt.ylabel('Valor')
> plt.legend()
> plt.grid(True)
> plt.show()
>
> # Imprime as m√©dias amostrais
> print("M√©dias amostrais para cada realiza√ß√£o:", sample_means)
> ```
>
> Neste gr√°fico, cada linha representa uma realiza√ß√£o do processo. A linha tracejada preta indica a m√©dia de conjunto (que √© zero). Observe que cada realiza√ß√£o tende a flutuar em torno de sua pr√≥pria m√©dia $\mu^{(i)}$ e n√£o converge para a m√©dia de conjunto. Al√©m disso, as m√©dias amostrais para cada realiza√ß√£o s√£o diferentes, enfatizando a falta de ergodicidade.
> Uma sa√≠da t√≠pica do c√≥digo seria:
>
> M√©dias amostrais para cada realiza√ß√£o: [-0.9930270524442854, 0.7349780104777873, -0.09754874862059236, 0.5448470538509617, 0.09230925258297437]
>
> Esses resultados num√©ricos confirmam que as m√©dias amostrais convergem para a m√©dia espec√≠fica da sua realiza√ß√£o, n√£o para a m√©dia de conjunto.

**Defini√ß√£o 2:** Um processo estoc√°stico $\{Y_t\}$ √© dito ser *homog√™neo* se a distribui√ß√£o conjunta de $(Y_{t_1}, Y_{t_2}, \dots, Y_{t_n})$ √© a mesma para qualquer transla√ß√£o temporal, ou seja, se a distribui√ß√£o de $(Y_{t_1}, Y_{t_2}, \dots, Y_{t_n})$ √© id√™ntica √† distribui√ß√£o de $(Y_{t_1+h}, Y_{t_2+h}, \dots, Y_{t_n+h})$ para todo $h$.

**Lema 4:** Se um processo √© estritamente estacion√°rio, ent√£o ele √© homog√™neo.

*Demonstra√ß√£o*. A demonstra√ß√£o segue diretamente da defini√ß√£o de estacionaridade estrita.

I. Se $\{Y_t\}$ √© estritamente estacion√°rio, ent√£o a distribui√ß√£o conjunta de $(Y_{t_1}, Y_{t_2}, \dots, Y_{t_n})$ depende apenas das diferen√ßas temporais $t_i - t_j$ e n√£o dos valores absolutos de $t_i$.

II. Isso implica que a distribui√ß√£o de $(Y_{t_1}, Y_{t_2}, \dots, Y_{t_n})$ √© id√™ntica √† distribui√ß√£o de $(Y_{t_1+h}, Y_{t_2+h}, \dots, Y_{t_n+h})$ para todo $h$.

III. Portanto, se um processo √© estritamente estacion√°rio, ent√£o ele √© homog√™neo. $\blacksquare$

**Lema 5:** Se um processo √© IID (Independent and Identically Distributed), ent√£o ele √© estritamente estacion√°rio.

*Demonstra√ß√£o*. A demonstra√ß√£o segue diretamente das defini√ß√µes de independ√™ncia e distribui√ß√£o id√™ntica.

I. Se $\{Y_t\}$ √© IID, ent√£o cada $Y_t$ tem a mesma distribui√ß√£o, e as vari√°veis s√£o independentes entre si.

II. A distribui√ß√£o conjunta de $(Y_{t_1}, Y_{t_2}, \dots, Y_{t_n})$ √© o produto das distribui√ß√µes marginais, que s√£o todas iguais.

III. Isso implica que a distribui√ß√£o conjunta n√£o depende do tempo $t$ e, portanto, o processo √© estritamente estacion√°rio. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar o Lema 5, vamos gerar um processo IID e verificar sua estacionaridade. Suponha que $Y_t \sim N(0, 1)$ para todo $t$, e cada $Y_t$ √© independente dos outros. Vamos gerar 1000 amostras e verificar visualmente e estatisticamente.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> import statsmodels.api as sm
>
> # Gera√ß√£o de dados IID
> np.random.seed(0)  # Define a semente para reprodutibilidade
> T = 1000
> Y = np.random.normal(0, 1, T)
>
> # Plot da s√©rie temporal
> plt.figure(figsize=(12, 6))
> plt.plot(Y)
> plt.title('Processo IID: Ru√≠do Branco Gaussiano')
> plt.xlabel('Tempo')
> plt.ylabel('Valor')
> plt.grid(True)
> plt.show()
>
> # Autocorrela√ß√£o (ACF)
> fig, ax = plt.subplots(figsize=(12, 6))
> sm.graphics.tsa.plot_acf(Y, lags=40, ax=ax)
> plt.title('Fun√ß√£o de Autocorrela√ß√£o (ACF)')
> plt.xlabel('Lag')
> plt.ylabel('Autocorrela√ß√£o')
> plt.grid(True)
> plt.show()
>
> # Teste de estacionaridade (teste de Dickey-Fuller Aumentado)
> from statsmodels.tsa.stattools import adfuller
>
> adf_result = adfuller(Y)
> print('Teste ADF:')
> print(f'Estat√≠stica ADF: {adf_result[0]}')
> print(f'Valor-p: {adf_result[1]}')
> print('Valores cr√≠ticos:')
> for key, value in adf_result[4].items():
>     print(f'   {key}: {value}')
>
> # Resultado esperado: O valor-p do teste ADF deve ser baixo (normalmente < 0.05), indicando que rejeitamos a hip√≥tese nula de n√£o estacionaridade. A ACF deve mostrar um decaimento r√°pido.
> ```
>
> No gr√°fico da s√©rie temporal, observamos que os valores flutuam aleatoriamente em torno de zero, sem um padr√£o discern√≠vel. A ACF mostra que as autocorrela√ß√µes est√£o pr√≥ximas de zero para todos os lags, indicando aus√™ncia de correla√ß√£o serial. O teste de Dickey-Fuller Augmented retorna um valor-p baixo, refor√ßando a estacionaridade. Este exemplo ilustra que um processo IID √© estritamente estacion√°rio.

**Teorema 5** (Ergodicidade e Independ√™ncia Assint√≥tica). Se um processo estacion√°rio tem autocovari√¢ncias que decaem suficientemente r√°pido ($\gamma_j \to 0$ quando $j \to \infty$) e satisfaz certas condi√ß√µes de mistura, ent√£o as vari√°veis $Y_t$ e $Y_{t+j}$ tornam-se assintoticamente independentes quando $j \to \infty$.

*Demonstra√ß√£o*. Este teorema envolve conceitos de teoria da mistura e √© mais avan√ßado. A intui√ß√£o √© que, se as autocovari√¢ncias decaem rapidamente, ent√£o a depend√™ncia entre as vari√°veis distantes no tempo diminui, aproximando-se da independ√™ncia. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Considere um processo AR(1) definido por $Y_t = 0.5Y_{t-1} + \epsilon_t$, onde $\epsilon_t \sim N(0, 1)$ √© ru√≠do branco. As autocovari√¢ncias deste processo decaem exponencialmente. Vamos simular este processo e verificar o decaimento das autocovari√¢ncias.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> import statsmodels.api as sm
>
> # Par√¢metros do processo AR(1)
> phi = 0.5
> sigma = 1
> T = 200  # Aumentado para uma melhor visualiza√ß√£o da ACF
>
> # Gera√ß√£o de dados AR(1)
> np.random.seed(0)
> epsilon = np.random.normal(0, sigma, T)
> Y = np.zeros(T)
> Y[0] = epsilon[0]
> for t in range(1, T):
>     Y[t] = phi * Y[t-1] + epsilon[t]
>
> # Plot da s√©rie temporal
> plt.figure(figsize=(12, 6))
> plt.plot(Y)
> plt.title('Processo AR(1): Y_t = 0.5Y_{t-1} + epsilon_t')
> plt.xlabel('Tempo')
> plt.ylabel('Valor')
> plt.grid(True)
> plt.show()
>
> # Autocorrela√ß√£o (ACF)
> fig, ax = plt.subplots(figsize=(12, 6))
> sm.graphics.tsa.plot_acf(Y, lags=40, ax=ax)
> plt.title('Fun√ß√£o de Autocorrela√ß√£o (ACF) para o Processo AR(1)')
> plt.xlabel('Lag')
> plt.ylabel('Autocorrela√ß√£o')
> plt.grid(True)
> plt.show()
> ```
>
> No gr√°fico da ACF, observamos que as autocorrela√ß√µes decaem exponencialmente √† medida que o lag aumenta. Isso indica que a depend√™ncia entre $Y_t$ e $Y_{t+j}$ diminui √† medida que $j$ aumenta. Pelo Teorema 5, este processo √© erg√≥dico porque suas autocovari√¢ncias decaem suficientemente r√°pido.

Para melhor entendimento do teorema acima, √© importante apresentar a defini√ß√£o de processos misturados:

**Defini√ß√£o 3:** Um processo estoc√°stico $\{Y_t\}$ √© dito ser *misturado* (mixing) se as vari√°veis $Y_t$ e $Y_{t+j}$ tornam-se assintoticamente independentes quando $j \to \infty$.

Uma condi√ß√£o suficiente para a mistura √© que as autocovari√¢ncias decaiam suficientemente r√°pido. A condi√ß√£o de decaimento r√°pido das autocovari√¢ncias garante que a influ√™ncia de eventos passados diminua com o tempo, permitindo a "mistura" do processo.

**Corol√°rio 3:** Se um processo $\{Y_t\}$ √© IID, ent√£o ele √© erg√≥dico para todos os momentos.

*Demonstra√ß√£o*.
I. Se um processo √© IID, ent√£o ele √© estritamente estacion√°rio (Lema 5).

II. Para um processo IID, considere a m√©dia amostral: $\bar{Y} = \frac{1}{T}\sum_{t=1}^T Y_t$.

III. Pela Lei Forte dos Grandes N√∫meros (LVGN), se $Y_t$ s√£o IID com $E[Y_t] = \mu$ e $E[|Y_t|] < \infty$, ent√£o $\bar{Y} \xrightarrow{a.s.} \mu$, onde $\xrightarrow{a.s.}$ denota converg√™ncia quase certa.

IV. Converg√™ncia quase certa implica converg√™ncia em probabilidade, ou seja, $\bar{Y} \xrightarrow{p} \mu$.

V. Portanto, um processo IID √© erg√≥dico para a m√©dia.

VI. Como as vari√°veis s√£o independentes, todos os momentos amostrais convergem para os momentos populacionais correspondentes.

VII. Portanto, o processo √© erg√≥dico para todos os momentos. $\blacksquare$

**Teorema 6:** (Teorema da representa√ß√£o de Wold) Qualquer processo estoc√°stico estacion√°rio puramente n√£o determin√≠stico pode ser representado como uma m√©dia m√≥vel linear infinita de inova√ß√µes n√£o correlacionadas. Ou seja:

$$ Y_t = \mu + \sum_{i=0}^{\infty} \psi_i \epsilon_{t-i} $$

onde:
* $\mu$ √© a m√©dia do processo
* $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia constante ($\sigma^2$)
* $\psi_i$ s√£o os coeficientes da representa√ß√£o, com $\psi_0 = 1$, e $\sum_{i=0}^{\infty} |\psi_i| < \infty$.

A representa√ß√£o de Wold fornece uma forma √∫til de decompor um processo estacion√°rio em um componente determin√≠stico (a m√©dia) e um componente estoc√°stico (a m√©dia m√≥vel das inova√ß√µes).

> üí° **Exemplo Num√©rico:**
>
> Vamos ilustrar a representa√ß√£o de Wold com um processo MA(1) simples: $Y_t = \epsilon_t + \theta \epsilon_{t-1}$, onde $\epsilon_t \sim N(0, \sigma^2)$. Este processo j√° est√° na forma da representa√ß√£o de Wold. Vamos simular este processo com $\theta = 0.7$ e $\sigma^2 = 1$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros do processo MA(1)
> theta = 0.7
> sigma = 1
> T = 200
>
> # Gera√ß√£o de dados MA(1)
> np.random.seed(0)
> epsilon = np.random.normal(0, sigma, T)
> Y = np.zeros(T)
> for t in range(1, T):
>     Y[t] = epsilon[t] + theta * epsilon[t-1]
>
> # Plot da s√©rie temporal
> plt.figure(figsize=(12, 6))
> plt.plot(Y)
> plt.title('Processo MA(1): Y_t = epsilon_t + 0.7*epsilon_{t-1}')
> plt.xlabel('Tempo')
> plt.ylabel('Valor')
> plt.grid(True)
> plt.show()
> ```
>
> Este processo MA(1) √© um exemplo direto da representa√ß√£o de Wold.  Os coeficientes $\psi_i$ s√£o $\psi_0 = 1$, $\psi_1 = \theta$, e $\psi_i = 0$ para $i > 1$. A condi√ß√£o $\sum_{i=0}^{\infty} |\psi_i| < \infty$ √© satisfeita, pois $1 + |\theta| < \infty$.

**Lema 6:** Se os coeficientes $\psi_i$ na representa√ß√£o de Wold decaem suficientemente r√°pido, ent√£o o processo √© erg√≥dico.

*Demonstra√ß√£o*.
I. Se $\sum_{i=0}^{\infty} |\psi_i| < \infty$, ent√£o o processo representado pela equa√ß√£o do Teorema 6 √© estacion√°rio.

II. As autocovari√¢ncias do processo podem ser expressas em termos dos coeficientes $\psi_i$ e da vari√¢ncia do ru√≠do branco $\sigma^2$. Especificamente, $\gamma_j = \sigma^2 \sum_{i=0}^{\infty} \psi_i \psi_{i+j}$.

III. Se os coeficientes $\psi_i$ decaem suficientemente r√°pido (e.g., exponencialmente), ent√£o as autocovari√¢ncias tamb√©m decaem suficientemente r√°pido, ou seja, $\sum_{j=0}^{\infty} |\gamma_j| < \infty$.

IV. Pelo Teorema 5, se as autocovari√¢ncias decaem suficientemente r√°pido, o processo √© assintoticamente independente, o que implica ergodicidade. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um processo onde os coeficientes $\psi_i$ decaem exponencialmente: $\psi_i = a^i$ com $|a| < 1$. Ent√£o, $Y_t = \sum_{i=0}^{\infty} a^i \epsilon_{t-i}$. Para garantir estacionaridade, precisamos $\sum_{i=0}^{\infty} |a^i| < \infty$. Como $\sum_{i=0}^{\infty} |a^i| = \frac{1}{1 - |a|}$ para $|a| < 1$, a condi√ß√£o de estacionaridade √© satisfeita. Vamos simular este processo com $a = 0.8$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> a = 0.8
> sigma = 1
> T = 200
>
> # Gera√ß√£o de dados
> np.random.seed(0)
> epsilon = np.random.normal(0, sigma, T + 100)  # Gerar mais para "burn-in"
> Y = np.zeros(T)
>
> # Burn-in para reduzir a influ√™ncia das condi√ß√µes iniciais
> for t in range(100, T + 100):
>     Y[t - 100] += epsilon[t]
>     for i in range(1, 100): # Limitar o n√∫mero de termos para evitar problemas de c√°lculo
>         if t - i >= 0:
>             Y[t - 100] += (a**i) * epsilon[t - i]
>
> # Plot da s√©rie temporal
> plt.figure(figsize=(12, 6))
> plt.plot(Y)
> plt.title('Processo com Coeficientes Decaindo Exponencialmente')
> plt.xlabel('Tempo')
> plt.ylabel('Valor')
> plt.grid(True)
> plt.show()
> ```
>
> No c√≥digo acima, simulamos um processo onde os coeficientes decaem exponencialmente. Visualmente, a s√©rie temporal parece estacion√°ria. Como os coeficientes decaem suficientemente r√°pido, o processo √© erg√≥dico.

### Conclus√£o
Este cap√≠tulo destacou a distin√ß√£o fundamental entre estacionaridade e ergodicidade, demonstrando que um processo estoc√°stico pode ser estacion√°rio sem ser erg√≥dico. Atrav√©s do exemplo ilustrativo de um processo onde a m√©dia de cada realiza√ß√£o √© extra√≠da de uma distribui√ß√£o, mostramos que a estacionaridade garante apenas a estabilidade das propriedades estat√≠sticas ao longo do tempo, enquanto a ergodicidade exige uma rela√ß√£o espec√≠fica entre as m√©dias temporais e as m√©dias de conjunto. Al√©m disso, exploramos conex√µes com processos misturados e IID. A representa√ß√£o de Wold e as condi√ß√µes para ergodicidade tamb√©m foram abordadas.

A compreens√£o dessas nuances √© crucial para a correta modelagem e interpreta√ß√£o de s√©ries temporais, garantindo que as infer√™ncias estat√≠sticas sejam baseadas em suposi√ß√µes v√°lidas sobre o comportamento do processo.
<!-- END -->