## Autocovari√¢ncia em S√©ries Temporais: A Vari√¢ncia como Autocovari√¢ncia de Ordem Zero

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre a **autocovari√¢ncia** $\gamma_j$ [^45] e sua import√¢ncia na caracteriza√ß√£o da depend√™ncia temporal em s√©ries temporais, este cap√≠tulo se concentra em um caso especial fundamental: a autocovari√¢ncia de ordem zero, denotada como $\gamma_0$. Como introduzido anteriormente, a autocovari√¢ncia mede a rela√ß√£o linear entre uma s√©rie temporal e suas vers√µes defasadas [^45]. Especificamente, exploraremos a equival√™ncia entre $\gamma_0$ e a **vari√¢ncia** da s√©rie temporal $Y_t$, um resultado crucial com profundas implica√ß√µes te√≥ricas e pr√°ticas [^46].

### Conceitos Fundamentais

A **autocovari√¢ncia de ordem zero** ($\gamma_0$) de uma s√©rie temporal $Y_t$ √© definida como a covari√¢ncia entre $Y_t$ e si pr√≥prio [^46]. Matematicamente, para uma s√©rie temporal *covariance-stationary* com m√©dia $\mu$, temos [^45]:

$$
\gamma_0 = E[(Y_t - \mu)(Y_{t-0} - \mu)]
$$

Como $Y_{t-0} = Y_t$, a equa√ß√£o se simplifica para [^46]:

$$
\gamma_0 = E[(Y_t - \mu)(Y_t - \mu)] = E[(Y_t - \mu)^2]
$$

A express√£o $E[(Y_t - \mu)^2]$ √©, por defini√ß√£o, a **vari√¢ncia** da s√©rie temporal $Y_t$, denotada como $Var(Y_t)$ [^46]:

$$
Var(Y_t) = E[(Y_t - \mu)^2]
$$

Portanto, a autocovari√¢ncia de ordem zero √© igual √† vari√¢ncia da s√©rie temporal [^46]:

$$
\gamma_0 = Var(Y_t)
$$

**Prova Formal:**

Queremos provar que $\gamma_0 = E[(Y_t - \mu)^2] = Var(Y_t)$ [^46].

I. Por defini√ß√£o, a autocovari√¢ncia no lag 0 √© dada por $\gamma_0 = E[(Y_t - \mu)(Y_{t-0} - \mu)]$ [^46].
II. Como $Y_{t-0} = Y_t$, temos $\gamma_0 = E[(Y_t - \mu)(Y_t - \mu)] = E[(Y_t - \mu)^2]$.
III. A vari√¢ncia de $Y_t$ √© definida como $Var(Y_t) = E[(Y_t - \mu)^2]$, onde $\mu = E[Y_t]$.
IV. Portanto, $\gamma_0 = E[(Y_t - \mu)^2] = Var(Y_t)$. $\blacksquare$

**Lema 1.** *Para uma s√©rie temporal com m√©dia zero, a autocovari√¢ncia de ordem zero √© igual ao valor esperado do quadrado da s√©rie temporal.*

**Demonstra√ß√£o:** Se $\mu = 0$, ent√£o $\gamma_0 = E[(Y_t - 0)^2] = E[Y_t^2]$. Este lema simplifica o c√°lculo da vari√¢ncia para s√©ries com m√©dia zero.

> üí° **Exemplo Num√©rico:**
>
> Considere uma s√©rie temporal com m√©dia zero $Y_t = [-2, -1, 0, 1, 2]$. A autocovari√¢ncia de ordem zero √© $\gamma_0 = E[Y_t^2] = \frac{1}{5}[(-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2] = \frac{1}{5}[4 + 1 + 0 + 1 + 4] = 2$.
>
> ```python
> import numpy as np
>
> Y = np.array([-2, -1, 0, 1, 2])
> gamma_0 = np.mean(Y**2)
> print(f"Autocovari√¢ncia de ordem zero: {gamma_0}")
> ```

**Interpreta√ß√µes e Implica√ß√µes:**

*   **Medida de Dispers√£o:** A vari√¢ncia, e consequentemente $\gamma_0$, quantifica a dispers√£o dos valores da s√©rie temporal em torno de sua m√©dia. Uma alta vari√¢ncia indica que os valores da s√©rie est√£o amplamente dispersos, enquanto uma baixa vari√¢ncia indica que os valores est√£o concentrados pr√≥ximos √† m√©dia [^46].
*   **Escala da Autocovari√¢ncia:** A vari√¢ncia estabelece a escala para as autocovari√¢ncias em outros lags. Como a autocorrela√ß√£o √© definida como $\rho_j = \frac{\gamma_j}{\gamma_0}$ [^49], todas as autocorrela√ß√µes s√£o normalizadas pela vari√¢ncia.
*   **Limite Superior:** A autocovari√¢ncia em qualquer lag *j* √© limitada pela vari√¢ncia da s√©rie temporal. Isso decorre da desigualdade de Cauchy-Schwarz, que estabelece que $|\gamma_j| \leq \sqrt{\gamma_0 \gamma_0} = \gamma_0 = Var(Y_t)$.

**Prova da Desigualdade $|\gamma_j| \leq \gamma_0$:**

I.  Pela defini√ß√£o de autocovari√¢ncia, $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$.

II. Aplicando a desigualdade de Cauchy-Schwarz para vari√°veis aleat√≥rias $X$ e $Y$, temos:
    $[E(XY)]^2 \leq E(X^2)E(Y^2)$.

III. Seja $X = (Y_t - \mu)$ e $Y = (Y_{t-j} - \mu)$. Ent√£o,
     $[E((Y_t - \mu)(Y_{t-j} - \mu))]^2 \leq E[(Y_t - \mu)^2]E[(Y_{t-j} - \mu)^2]$.

IV. Substituindo pelas defini√ß√µes de autocovari√¢ncia e vari√¢ncia, temos:
    $\gamma_j^2 \leq \gamma_0 \cdot \gamma_0 = \gamma_0^2$.

V. Tomando a raiz quadrada de ambos os lados, obtemos:
    $|\gamma_j| \leq \gamma_0$.  $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos uma s√©rie temporal com $\gamma_0 = 10$. Pela desigualdade, sabemos que $|\gamma_1| \leq 10$, $|\gamma_2| \leq 10$, e assim por diante. Isso significa que a autocovari√¢ncia em qualquer lag n√£o pode exceder a vari√¢ncia da s√©rie temporal. Se, por exemplo, $\gamma_1 = 5$, ent√£o a autocorrela√ß√£o no lag 1 seria $\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{5}{10} = 0.5$.

*   **Estimativa da Autocovari√¢ncia:** Na pr√°tica, estimamos a autocovari√¢ncia a partir de uma amostra da s√©rie temporal. Uma estimativa comum de $\gamma_0$ √© a vari√¢ncia amostral: $\hat{\gamma}_0 = \frac{1}{I} \sum_{i=1}^{I} (Y_i - \bar{Y})^2$, onde *I* √© o n√∫mero de observa√ß√µes e $\bar{Y}$ √© a m√©dia amostral.

    > üí° **Exemplo Num√©rico:**
    >
    > Considere uma s√©rie temporal $Y_t = [1, 3, 5, 7, 9]$. A m√©dia √© $\mu = 5$. A vari√¢ncia √© $Var(Y_t) = \frac{1}{5} [(1-5)^2 + (3-5)^2 + (5-5)^2 + (7-5)^2 + (9-5)^2] = \frac{1}{5} [16 + 4 + 0 + 4 + 16] = 8$. Portanto, $\gamma_0 = Var(Y_t) = 8$. Isso indica a dispers√£o dos dados em torno da m√©dia.
    >
    > ```python
    > import numpy as np
    >
    > Y = np.array([1, 3, 5, 7, 9])
    > Y_mean = np.mean(Y)
    > gamma_0 = np.mean((Y - Y_mean)**2)
    > print(f"Estimativa da autocovari√¢ncia para lag 0: {gamma_0}")
    > ```

**Autocovari√¢ncia e Modelos ARMA:**

A vari√¢ncia, sendo a autocovari√¢ncia de ordem zero, √© um par√¢metro fundamental em modelos ARMA. Por exemplo, para um processo AR(1) [^53], a vari√¢ncia √© dada por:

$$
\gamma_0 = \frac{\sigma^2}{1 - \phi^2}
$$

onde $\sigma^2$ √© a vari√¢ncia do ru√≠do branco e $\phi$ √© o coeficiente autoregressivo [^53]. Este resultado mostra como a vari√¢ncia do processo AR(1) depende da vari√¢ncia do ru√≠do branco e do coeficiente autoregressivo. Para processos MA(1) [^48], a vari√¢ncia √© dada por:

$$
\gamma_0 = (1 + \theta^2) \sigma^2
$$

onde $\theta$ √© o coeficiente de m√©dia m√≥vel [^48]. Este resultado demonstra como a vari√¢ncia do processo MA(1) depende da vari√¢ncia do ru√≠do branco e do coeficiente de m√©dia m√≥vel.

> üí° **Exemplo Num√©rico:**
>
> Considere um processo AR(1) com $\phi = 0.5$ e $\sigma^2 = 2$. A vari√¢ncia do processo AR(1) √© $\gamma_0 = \frac{2}{1 - 0.5^2} = \frac{2}{0.75} = 2.67$. Em contraste, para um processo MA(1) com $\theta = 0.5$ e $\sigma^2 = 2$, a vari√¢ncia √© $\gamma_0 = (1 + 0.5^2) * 2 = 2.5$.
>
> ```python
> import numpy as np
>
> # AR(1)
> phi = 0.5
> sigma2 = 2
> gamma_0_ar1 = sigma2 / (1 - phi**2)
> print(f"Vari√¢ncia do AR(1): {gamma_0_ar1}")
>
> # MA(1)
> theta = 0.5
> sigma2 = 2
> gamma_0_ma1 = (1 + theta**2) * sigma2
> print(f"Vari√¢ncia do MA(1): {gamma_0_ma1}")
> ```

**Teorema 1.** Para um processo ARMA(p,q) estacion√°rio e invert√≠vel, a autocovari√¢ncia de ordem zero ($\gamma_0$) √© sempre um valor positivo.

**Demonstra√ß√£o:** A autocovari√¢ncia de ordem zero representa a vari√¢ncia do processo. Por defini√ß√£o, a vari√¢ncia √© sempre n√£o negativa. Para um processo estacion√°rio, a vari√¢ncia √© finita. Para um processo ARMA(p,q) com ru√≠do branco com vari√¢ncia positiva, a vari√¢ncia do processo tamb√©m ser√° positiva, garantindo que $\gamma_0 > 0$.  $\blacksquare$

**Corol√°rio 1.** *Para um processo ARMA(p,q), se a vari√¢ncia do ru√≠do branco ($\sigma^2$) for zero, ent√£o a autocovari√¢ncia de ordem zero ($\gamma_0$) tamb√©m ser√° zero.*

**Demonstra√ß√£o:** Se $\sigma^2 = 0$, ent√£o o processo se torna determin√≠stico e n√£o h√° varia√ß√£o em torno da m√©dia. Portanto, a vari√¢ncia do processo √© zero, e consequentemente, $\gamma_0 = 0$.

**Lema 2.**  *A autocovari√¢ncia de ordem zero ($\gamma_0$) √© invariante a adi√ß√µes de constantes √† s√©rie temporal.*

**Demonstra√ß√£o:** Seja $Y_t'$ uma nova s√©rie temporal definida como $Y_t' = Y_t + c$, onde $c$ √© uma constante.  A m√©dia de $Y_t'$ √© $\mu' = \mu + c$. A vari√¢ncia de $Y_t'$ √© dada por:

$Var(Y_t') = E[(Y_t' - \mu')^2] = E[((Y_t + c) - (\mu + c))^2] = E[(Y_t - \mu)^2] = Var(Y_t)$

Portanto, $\gamma_0' = Var(Y_t') = Var(Y_t) = \gamma_0$. Isto demonstra que adicionar uma constante n√£o altera a autocovari√¢ncia de ordem zero. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Seja $Y_t = [1, 2, 3]$ com $\mu = 2$ e $\gamma_0 = \frac{1}{3}[(1-2)^2 + (2-2)^2 + (3-2)^2] = \frac{2}{3}$. Se adicionarmos $c = 5$ a cada valor, obtemos $Y_t' = [6, 7, 8]$ com $\mu' = 7$. A nova autocovari√¢ncia de ordem zero √© $\gamma_0' = \frac{1}{3}[(6-7)^2 + (7-7)^2 + (8-7)^2] = \frac{2}{3}$, que √© igual a $\gamma_0$.

**Teorema 1.1.** *Para um processo ARMA(p,q) estacion√°rio e invert√≠vel, a autocovari√¢ncia de ordem zero ($\gamma_0$) √© uma fun√ß√£o da vari√¢ncia do ru√≠do branco ($\sigma^2$) e dos par√¢metros do modelo.*

**Demonstra√ß√£o:**  A autocovari√¢ncia de ordem zero de um processo ARMA(p,q) pode ser expressa em termos da fun√ß√£o de autocovari√¢ncia (ACF) do processo. A ACF depende, por sua vez, da vari√¢ncia do ru√≠do branco e dos par√¢metros dos componentes AR e MA do modelo. A forma expl√≠cita dessa fun√ß√£o varia dependendo das ordens *p* e *q*. Por exemplo, como mostrado anteriormente, para um AR(1), $\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$, e para um MA(1), $\gamma_0 = (1 + \theta^2) \sigma^2$. Em geral, para um ARMA(p,q), $\gamma_0$ ser√° uma express√£o que envolve $\sigma^2$ e os coeficientes $\phi_1, \ldots, \phi_p, \theta_1, \ldots, \theta_q$. Portanto, $\gamma_0$ √© uma fun√ß√£o desses par√¢metros.  $\blacksquare$

**Proposi√ß√£o 1.** *Se a s√©rie temporal $Y_t$ for multiplicada por uma constante $c$, a autocovari√¢ncia de ordem zero ($\gamma_0$) ser√° multiplicada por $c^2$.*

**Demonstra√ß√£o:** Seja $Y_t' = cY_t$. Ent√£o, $E[Y_t'] = cE[Y_t] = c\mu$. Portanto, a autocovari√¢ncia de ordem zero de $Y_t'$ √©:

$\gamma_0' = E[(Y_t' - c\mu)^2] = E[(cY_t - c\mu)^2] = E[c^2(Y_t - \mu)^2] = c^2 E[(Y_t - \mu)^2] = c^2 \gamma_0$.

Assim, a autocovari√¢ncia de ordem zero √© multiplicada por $c^2$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Seja $Y_t = [1, 2, 3]$ com $\mu = 2$ e $\gamma_0 = \frac{2}{3}$. Se multiplicarmos cada valor por $c = 2$, obtemos $Y_t' = [2, 4, 6]$ com $\mu' = 4$. A nova autocovari√¢ncia de ordem zero √© $\gamma_0' = \frac{1}{3}[(2-4)^2 + (4-4)^2 + (6-4)^2] = \frac{8}{3} = 2^2 * \frac{2}{3} = 4\gamma_0$.

### Conclus√£o

A equival√™ncia entre a autocovari√¢ncia de ordem zero e a vari√¢ncia de uma s√©rie temporal √© um resultado fundamental com amplas implica√ß√µes [^46]. Ela estabelece a escala para a autocovari√¢ncia em outros lags, fornece uma medida crucial da dispers√£o dos dados e desempenha um papel central na an√°lise e estima√ß√£o de modelos ARMA. Ao compreender esta rela√ß√£o, podemos obter *insights* valiosos sobre o comportamento das s√©ries temporais e construir modelos estat√≠sticos mais precisos.

### Refer√™ncias
[^45]: P√°gina 45 do texto original.
[^46]: P√°gina 46 do texto original.
[^48]: P√°gina 48 do texto original.
[^49]: P√°gina 49 do texto original.
[^53]: P√°gina 53 do texto original.
<!-- END -->