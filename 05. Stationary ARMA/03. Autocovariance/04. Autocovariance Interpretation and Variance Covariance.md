## Autocovari√¢ncia em S√©ries Temporais: Autocovari√¢ncia como Elemento da Matriz de Vari√¢ncia-Covari√¢ncia

### Introdu√ß√£o

Em continuidade √† discuss√£o sobre a **autocovari√¢ncia** $\gamma_j$ [^45] e suas propriedades computacionais [^69], este cap√≠tulo explora uma perspectiva crucial: a interpreta√ß√£o da autocovari√¢ncia como um elemento espec√≠fico da **matriz de vari√¢ncia-covari√¢ncia** de um vetor constru√≠do a partir de observa√ß√µes recentes da s√©rie temporal. Esta vis√£o fornece uma liga√ß√£o fundamental entre a autocovari√¢ncia e a estrutura de depend√™ncia do processo estoc√°stico, encontrando aplica√ß√µes pr√°ticas em modelos de espa√ßo de estados e filtragem de Kalman [^45].

### Conceitos Fundamentais

Considere uma s√©rie temporal $Y_t$. Constru√≠mos um vetor $x_t$ composto pelas $j+1$ observa√ß√µes mais recentes de $Y$, ou seja:

$$
x_t = \begin{bmatrix}
Y_t \\
Y_{t-1} \\
\vdots \\
Y_{t-j}
\end{bmatrix}
$$

A **matriz de vari√¢ncia-covari√¢ncia** de $x_t$, denotada por $\Sigma_t$, √© uma matriz de tamanho $(j+1) \times (j+1)$ cujos elementos s√£o as covari√¢ncias entre os diferentes componentes do vetor $x_t$. O elemento na *i*-√©sima linha e *k*-√©sima coluna de $\Sigma_t$ representa a covari√¢ncia entre $Y_{t-(i-1)}$ e $Y_{t-(k-1)}$.

Formalmente, o elemento $(i, k)$ de $\Sigma_t$ √© dado por:

$$
\Sigma_{t(i, k)} = E[(Y_{t-(i-1)} - \mu_{t-(i-1)})(Y_{t-(k-1)} - \mu_{t-(k-1)})]
$$

Para uma s√©rie temporal covariance-stationary, as m√©dias $\mu_{t-(i-1)}$ e $\mu_{t-(k-1)}$ s√£o iguais a $\mu$, e as covari√¢ncias dependem apenas da diferen√ßa entre os √≠ndices de tempo. Portanto, podemos reescrever a express√£o acima como:

$$
\Sigma_{t(i, k)} = E[(Y_{t-(i-1)} - \mu)(Y_{t-(k-1)} - \mu)] = \gamma_{|i-k|}
$$

onde $\gamma_{|i-k|}$ √© a autocovari√¢ncia no lag $|i-k|$. Em particular, a autocovari√¢ncia $\gamma_j$ corresponde ao elemento $(1, j+1)$ (ou equivalentemente $(j+1, 1)$ devido √† simetria) da matriz $\Sigma_t$ [^45]:

$$
\Sigma_{t(1, j+1)} = E[(Y_t - \mu)(Y_{t-j} - \mu)] = \gamma_j
$$

> üí° **Exemplo Num√©rico:**
>
> Considere uma s√©rie temporal $Y_t$ com m√©dia $\mu = 1$ e as seguintes autocovari√¢ncias: $\gamma_0 = 2$, $\gamma_1 = 1$, e $\gamma_2 = 0.5$.  Constru√≠mos um vetor $x_t$ com as tr√™s observa√ß√µes mais recentes ($j=2$):
>
> $$
> x_t = \begin{bmatrix}
> Y_t \\
> Y_{t-1} \\
> Y_{t-2}
> \end{bmatrix}
> $$
>
> A matriz de vari√¢ncia-covari√¢ncia $\Sigma_t$ seria:
>
> $$
> \Sigma_t = \begin{bmatrix}
> \gamma_0 & \gamma_1 & \gamma_2 \\
> \gamma_1 & \gamma_0 & \gamma_1 \\
> \gamma_2 & \gamma_1 & \gamma_0
> \end{bmatrix} = \begin{bmatrix}
> 2 & 1 & 0.5 \\
> 1 & 2 & 1 \\
> 0.5 & 1 & 2
> \end{bmatrix}
> $$
>
> O elemento (1,3) da matriz √© $\gamma_2 = 0.5$, que representa a autocovari√¢ncia no lag 2. Isto significa que a covari√¢ncia entre $Y_t$ e $Y_{t-2}$ √© 0.5.
>
> ```python
> import numpy as np
>
> # Autocovari√¢ncias
> gamma_0 = 2
> gamma_1 = 1
> gamma_2 = 0.5
>
> # Cria a matriz de vari√¢ncia-covari√¢ncia
> Sigma = np.array([
>     [gamma_0, gamma_1, gamma_2],
>     [gamma_1, gamma_0, gamma_1],
>     [gamma_2, gamma_1, gamma_0]
> ])
>
> print("Matriz de Vari√¢ncia-Covari√¢ncia:")
> print(Sigma)
> ```

> üí° **Exemplo Num√©rico:**
>
> Imagine uma s√©rie temporal com os seguintes valores para os primeiros cinco per√≠odos: $Y = [2, 3, 5, 4, 6]$. Vamos calcular a matriz de vari√¢ncia-covari√¢ncia para $j=1$, ou seja, considerando apenas os dois √∫ltimos valores.
>
> 1.  Calculamos a m√©dia da s√©rie temporal: $\mu = (2+3+5+4+6)/5 = 4$.
> 2.  Calculamos as autocovari√¢ncias:
>     *   $\gamma_0 = E[(Y_t - \mu)(Y_t - \mu)] \approx \frac{1}{5} \sum_{t=1}^{5} (Y_t - \mu)^2 = \frac{1}{5}[(2-4)^2 + (3-4)^2 + (5-4)^2 + (4-4)^2 + (6-4)^2] = \frac{1}{5}[4 + 1 + 1 + 0 + 4] = 2$
>     *   $\gamma_1 = E[(Y_t - \mu)(Y_{t-1} - \mu)] \approx \frac{1}{4} \sum_{t=2}^{5} (Y_t - \mu)(Y_{t-1} - \mu) = \frac{1}{4}[(3-4)(2-4) + (5-4)(3-4) + (4-4)(5-4) + (6-4)(4-4)] = \frac{1}{4}[2 - 1 + 0 + 0] = 0.25$
>
> 3.  Constru√≠mos a matriz de vari√¢ncia-covari√¢ncia:
>
>     $$
>     \Sigma_t = \begin{bmatrix}
>     \gamma_0 & \gamma_1 \\
>     \gamma_1 & \gamma_0
>     \end{bmatrix} = \begin{bmatrix}
>     2 & 0.25 \\
>     0.25 & 2
>     \end{bmatrix}
>     $$
>
> ```python
> import numpy as np
>
> # S√©rie temporal
> Y = np.array([2, 3, 5, 4, 6])
>
> # Calcula a m√©dia
> mu = np.mean(Y)
>
> # Calcula autocovari√¢ncia com lag 0 (gamma_0)
> gamma_0 = np.mean((Y - mu)**2)
>
> # Calcula autocovari√¢ncia com lag 1 (gamma_1)
> gamma_1 = np.mean((Y[1:] - mu) * (Y[:-1] - mu))
>
> # Cria a matriz de vari√¢ncia-covari√¢ncia
> Sigma = np.array([
>     [gamma_0, gamma_1],
>     [gamma_1, gamma_0]
> ])
>
> print("M√©dia:", mu)
> print("Autocovari√¢ncia (lag 0):", gamma_0)
> print("Autocovari√¢ncia (lag 1):", gamma_1)
> print("Matriz de Vari√¢ncia-Covari√¢ncia:")
> print(Sigma)
> ```

**Lema 1.** *Para uma s√©rie temporal estacion√°ria, a matriz de vari√¢ncia-covari√¢ncia $\Sigma_t$ √© Toeplitz.*

**Demonstra√ß√£o.** Uma matriz Toeplitz √© uma matriz em que cada diagonal descendente da esquerda para a direita √© constante. Como a autocovari√¢ncia depende apenas da dist√¢ncia entre os pontos no tempo (devido √† estacionariedade) e n√£o da sua posi√ß√£o absoluta no tempo, os elementos da diagonal $i-k = const$ ser√£o iguais. Portanto, $\Sigma_t$ √© uma matriz Toeplitz. $\blacksquare$

**Prova:**
I. Seja $\Sigma_t$ a matriz de vari√¢ncia-covari√¢ncia de uma s√©rie temporal estacion√°ria $Y_t$.

II. O elemento $(i,k)$ de $\Sigma_t$ √© dado por $\Sigma_{t(i,k)} = E[(Y_{t-(i-1)} - \mu)(Y_{t-(k-1)} - \mu)]$, onde $\mu$ √© a m√©dia da s√©rie temporal.

III. Devido √† estacionariedade, a autocovari√¢ncia depende apenas da diferen√ßa entre os √≠ndices de tempo, ou seja, $\Sigma_{t(i,k)} = \gamma_{|i-k|}$.

IV. Considere dois elementos na mesma diagonal descendente, digamos $(i,k)$ e $(i+1,k+1)$.

V. Queremos mostrar que $\Sigma_{t(i,k)} = \Sigma_{t(i+1,k+1)}$.

VI. Temos $\Sigma_{t(i+1,k+1)} = E[(Y_{t-i} - \mu)(Y_{t-k} - \mu)] = \gamma_{|i-k|}$.

VII. Portanto, $\Sigma_{t(i,k)} = \Sigma_{t(i+1,k+1)} = \gamma_{|i-k|}$, o que significa que todos os elementos na mesma diagonal descendente s√£o iguais.

VIII. Isso satisfaz a defini√ß√£o de uma matriz Toeplitz.  $\blacksquare$

**Lema 1.1.** *Se a s√©rie temporal tem m√©dia zero, ent√£o $\Sigma_{t(i, k)} = E[Y_{t-(i-1)}Y_{t-(k-1)}]$.*

**Demonstra√ß√£o.** Se $\mu = 0$, ent√£o $\Sigma_{t(i, k)} = E[(Y_{t-(i-1)} - 0)(Y_{t-(k-1)} - 0)] = E[Y_{t-(i-1)}Y_{t-(k-1)}]$.

**Lema 1.2.** *A matriz de vari√¢ncia-covari√¢ncia de uma s√©rie temporal estacion√°ria √© tamb√©m sim√©trica.*

**Demonstra√ß√£o.** A simetria da matriz de vari√¢ncia-covari√¢ncia decorre da propriedade de simetria da autocovari√¢ncia, ou seja, $\gamma_h = \gamma_{-h}$ para qualquer lag $h$. Assim, $\Sigma_{t(i, k)} = \gamma_{|i-k|} = \gamma_{|k-i|} = \Sigma_{t(k, i)}$, demonstrando que a matriz √© sim√©trica.  $\blacksquare$

**Lema 1.3.** *Para uma s√©rie temporal estacion√°ria, a matriz de vari√¢ncia-covari√¢ncia $\Sigma_t$ √© unicamente definida pelas suas primeiras $j+1$ autocovari√¢ncias, $\gamma_0, \gamma_1, \dots, \gamma_j$.*

**Demonstra√ß√£o.** Como $\Sigma_{t(i, k)} = \gamma_{|i-k|}$, cada elemento da matriz √© determinado por uma das autocovari√¢ncias $\gamma_0, \gamma_1, \dots, \gamma_j$. Especificamente, $\gamma_0$ aparece na diagonal principal, $\gamma_1$ nas diagonais adjacentes, e assim por diante, at√© $\gamma_j$ nos cantos superior direito e inferior esquerdo da matriz. Portanto, o conhecimento das primeiras $j+1$ autocovari√¢ncias define completamente a matriz $\Sigma_t$. $\blacksquare$

**Observa√ß√µes Importantes:**

*   **Estrutura de Depend√™ncia:** A matriz de vari√¢ncia-covari√¢ncia $\Sigma_t$ captura a estrutura de depend√™ncia linear completa entre as $j+1$ observa√ß√µes mais recentes da s√©rie temporal.
*   **Estacionariedade:** Para s√©ries temporais estacion√°rias, a matriz $\Sigma_t$ √© constante ao longo do tempo e depende apenas das autocovari√¢ncias.
*   **Simetria:** A matriz $\Sigma_t$ √© sim√©trica, refletindo a propriedade de simetria da autocovari√¢ncia ($\gamma_j = \gamma_{-j}$ [^46]).

**Aplica√ß√µes Pr√°ticas:**

A interpreta√ß√£o da autocovari√¢ncia como um elemento da matriz de vari√¢ncia-covari√¢ncia encontra aplica√ß√µes importantes em diversos contextos:

*   **Modelos de Espa√ßo de Estados:** Em modelos de espa√ßo de estados, a matriz $\Sigma_t$ desempenha um papel crucial na defini√ß√£o da distribui√ß√£o de probabilidade do estado n√£o observado da s√©rie temporal [^45]. As equa√ß√µes de estado e de observa√ß√£o s√£o expressas em termos de matrizes que dependem da matriz de vari√¢ncia-covari√¢ncia.
*   **Filtragem de Kalman:** O filtro de Kalman √© um algoritmo recursivo utilizado para estimar o estado de um sistema din√¢mico a partir de uma s√©rie de medi√ß√µes ruidosas. A matriz $\Sigma_t$ √© utilizada para calcular o ganho de Kalman, que pondera a import√¢ncia das novas medi√ß√µes em rela√ß√£o √† estimativa anterior do estado.
*   **An√°lise de Componentes Principais (PCA):** A PCA pode ser aplicada ao vetor $x_t$ para identificar os principais modos de varia√ß√£o da s√©rie temporal. A matriz $\Sigma_t$ √© utilizada para calcular os autovalores e autovetores, que representam as dire√ß√µes de maior vari√¢ncia.
*   **Modelagem de Depend√™ncia Multivariada:** Em cen√°rios com m√∫ltiplas s√©ries temporais correlacionadas, a matriz de vari√¢ncia-covari√¢ncia entre as diferentes s√©ries temporais e seus *lags* pode ser utilizada para modelar a depend√™ncia entre elas.

> üí° **Exemplo Num√©rico (PCA):**
>
> Vamos aplicar a An√°lise de Componentes Principais (PCA) √† matriz de vari√¢ncia-covari√¢ncia do exemplo anterior:
>
> $$
> \Sigma_t = \begin{bmatrix}
> 2 & 1 & 0.5 \\
> 1 & 2 & 1 \\
> 0.5 & 1 & 2
> \end{bmatrix}
> $$
>
> ```python
> import numpy as np
> from numpy import linalg as LA
>
> # Matriz de Vari√¢ncia-Covari√¢ncia (do exemplo anterior)
> Sigma = np.array([
>     [2, 1, 0.5],
>     [1, 2, 1],
>     [0.5, 1, 2]
> ])
>
> # Calcula os autovalores e autovetores
> eigenvalues, eigenvectors = LA.eig(Sigma)
>
> print("Autovalores:", eigenvalues)
> print("Autovetores:", eigenvectors)
>
> # A vari√¢ncia explicada por cada componente principal √© proporcional ao autovalor correspondente
> explained_variance_ratio = eigenvalues / np.sum(eigenvalues)
> print("Vari√¢ncia Explicada por Componente:", explained_variance_ratio)
> ```
>
> Os autovalores representam a vari√¢ncia explicada por cada componente principal. O autovetor correspondente ao maior autovalor representa a dire√ß√£o na qual os dados t√™m a maior vari√¢ncia. No contexto da s√©rie temporal, isso pode nos dizer quais combina√ß√µes lineares das observa√ß√µes passadas s√£o mais importantes para explicar o comportamento da s√©rie.

**Proposi√ß√£o 1.** *Para uma s√©rie temporal estacion√°ria e erg√≥dica com m√©dia zero, a matriz de vari√¢ncia-covari√¢ncia amostral converge em probabilidade para a matriz de vari√¢ncia-covari√¢ncia te√≥rica, desde que as condi√ß√µes de estacionariedade e ergodicidade sejam satisfeitas.*

**Demonstra√ß√£o.** (Esbo√ßo) Sob as condi√ß√µes de estacionariedade e ergodicidade, a m√©dia amostral converge em probabilidade para a m√©dia te√≥rica (que √© zero neste caso), e a estimativa amostral da autocovari√¢ncia converge em probabilidade para a autocovari√¢ncia te√≥rica. Como a matriz de vari√¢ncia-covari√¢ncia √© composta por estimativas de autocovari√¢ncia, a matriz amostral converge em probabilidade para a matriz te√≥rica.

**Prova:**
I. Assumimos que a s√©rie temporal $\{Y_i\}_{i=1}^{I}$ √© estacion√°ria e erg√≥dica, e que a m√©dia te√≥rica √© zero, ou seja, $E[Y_i] = 0$.

II. Constru√≠mos o vetor $x_t = [Y_t, Y_{t-1}, \dots, Y_{t-j}]'$ com $j+1$ observa√ß√µes.

III. A matriz de vari√¢ncia-covari√¢ncia te√≥rica $\Sigma$ √© definida como:
$$\Sigma_{kl} = E[Y_{t-(k-1)} Y_{t-(l-1)}] = \gamma_{|k-l|}$$
para $k, l = 1, 2, \dots, j+1$.

IV. A estimativa amostral da matriz de vari√¢ncia-covari√¢ncia $\hat{\Sigma}$ √© dada por:
$$\hat{\Sigma}_{kl} = \frac{1}{I} \sum_{t=j+1}^{I} Y_{t-(k-1)} Y_{t-(l-1)}$$
Note que a soma come√ßa em $t=j+1$ para garantir que todos os termos $Y_{t-(k-1)}$ e $Y_{t-(l-1)}$ estejam definidos dentro do intervalo da s√©rie temporal observada.

V. Queremos mostrar que a estimativa amostral converge em probabilidade para a matriz te√≥rica, ou seja,
$$ \text{plim}_{I \to \infty} \hat{\Sigma}_{kl} = \Sigma_{kl} $$

VI. Tomando o limite de probabilidade:
$$ \text{plim}_{I \to \infty} \hat{\Sigma}_{kl} = \text{plim}_{I \to \infty} \frac{1}{I} \sum_{t=j+1}^{I} Y_{t-(k-1)} Y_{t-(l-1)}$$

VII. Pela defini√ß√£o de ergodicidade, a m√©dia amostral converge em probabilidade para o valor esperado:
$$\text{plim}_{I \to \infty} \frac{1}{I} \sum_{t=j+1}^{I} Y_{t-(k-1)} Y_{t-(l-1)} = E[Y_{t-(k-1)} Y_{t-(l-1)}]$$

VIII. Substituindo de volta:
$$\text{plim}_{I \to \infty} \hat{\Sigma}_{kl} = E[Y_{t-(k-1)} Y_{t-(l-1)}] = \gamma_{|k-l|} = \Sigma_{kl}$$

IX. Portanto, a estimativa amostral da matriz de vari√¢ncia-covari√¢ncia converge em probabilidade para a matriz de vari√¢ncia-covari√¢ncia te√≥rica.  $\blacksquare$

**Proposi√ß√£o 1.1.** *Se a s√©rie temporal n√£o tem m√©dia zero, ent√£o a Proposi√ß√£o 1 ainda se mant√©m, desde que a m√©dia amostral seja utilizada para centralizar os dados antes do c√°lculo da matriz de vari√¢ncia-covari√¢ncia amostral.*

**Demonstra√ß√£o.** (Esbo√ßo) Se a m√©dia n√£o √© zero, devemos subtrair a m√©dia amostral de cada observa√ß√£o antes de calcular a autocovari√¢ncia amostral. Sob as condi√ß√µes de estacionariedade e ergodicidade, a m√©dia amostral converge para a m√©dia te√≥rica, e a autocovari√¢ncia amostral calculada com os dados centralizados converge para a autocovari√¢ncia te√≥rica. Portanto, a matriz de vari√¢ncia-covari√¢ncia amostral converge para a matriz te√≥rica.  $\blacksquare$

**Prova:**
I. Seja $\{Y_t\}_{t=1}^n$ uma s√©rie temporal estacion√°ria e erg√≥dica com m√©dia $\mu \neq 0$.

II. Seja $\bar{Y} = \frac{1}{n}\sum_{t=1}^n Y_t$ a m√©dia amostral da s√©rie temporal.

III. Definimos a s√©rie temporal centralizada como $Y_t' = Y_t - \bar{Y}$.

IV. A matriz de vari√¢ncia-covari√¢ncia te√≥rica $\Sigma$ tem elementos $\Sigma_{ij} = E[(Y_{t-i} - \mu)(Y_{t-j} - \mu)] = \gamma_{|i-j|}$.

V. A matriz de vari√¢ncia-covari√¢ncia amostral $\hat{\Sigma}$ tem elementos $\hat{\Sigma}_{ij} = \frac{1}{n} \sum_{t=\max(i,j)+1}^{n} (Y_{t-i} - \bar{Y})(Y_{t-j} - \bar{Y})$.

VI. Pela ergodicidade, $\bar{Y}$ converge em probabilidade para $\mu$ quando $n \to \infty$.

VII. Assim, $Y_t' = Y_t - \bar{Y}$ converge em probabilidade para $Y_t - \mu$.

VIII. Portanto, $\hat{\Sigma}_{ij} = \frac{1}{n} \sum_{t=\max(i,j)+1}^{n} (Y_{t-i} - \bar{Y})(Y_{t-j} - \bar{Y})$ converge em probabilidade para $E[(Y_{t-i} - \mu)(Y_{t-j} - \mu)] = \gamma_{|i-j|} = \Sigma_{ij}$.

IX. Assim, a matriz de vari√¢ncia-covari√¢ncia amostral converge em probabilidade para a matriz te√≥rica quando a s√©rie temporal √© centralizada pela m√©dia amostral.  $\blacksquare$

**Teorema 1.** *A matriz de vari√¢ncia-covari√¢ncia $\Sigma_t$ √© sempre semidefinida positiva.*

**Demonstra√ß√£o.** (Esbo√ßo) A matriz de vari√¢ncia-covari√¢ncia $\Sigma_t$ √© semidefinida positiva porque, por defini√ß√£o, para qualquer vetor $v$, a forma quadr√°tica $v^T \Sigma_t v$ representa a vari√¢ncia de uma combina√ß√£o linear das vari√°veis aleat√≥rias em $x_t$, que √© sempre n√£o negativa.

Para demonstrar formalmente que $\Sigma$ √© semidefinida positiva, devemos mostrar que para qualquer vetor $z \in \mathbb{R}^{j+1}$, temos $z^T \Sigma z \geq 0$.

I. Seja $z = [z_0, z_1, ..., z_j]^T$ um vetor arbitr√°rio em $\mathbb{R}^{j+1}$.

II. Considere a combina√ß√£o linear das vari√°veis aleat√≥rias $Y_t, Y_{t-1}, ..., Y_{t-j}$:
$W = z_0 Y_t + z_1 Y_{t-1} + ... + z_j Y_{t-j}$.

III. Queremos mostrar que $E[W^2] \geq 0$ (j√° que $W^2$ √© sempre n√£o negativo).
$E[W^2] = E[(z_0 Y_t + z_1 Y_{t-1} + ... + z_j Y_{t-j})^2]$.

IV. Expandindo a express√£o, temos:
$E[W^2] = \sum_{k=0}^{j} \sum_{l=0}^{j} z_k z_l E[Y_{t-k} Y_{t-l}]$.

V. Substituindo $E[Y_{t-k} Y_{t-l}] = \gamma_{|k-l|}$, j√° que a m√©dia √© zero:
$E[W^2] = \sum_{k=0}^{j} \sum_{l=0}^{j} z_k z_l \gamma_{|k-l|}$.

VI. Podemos expressar isso em nota√ß√£o matricial:
$E[W^2] = z^T \Sigma z$, onde $\Sigma_{kl} = \gamma_{|k-l|}$.

VII. Como $E[W^2] \geq 0$, temos $z^T \Sigma z \geq 0$ para qualquer vetor $z$.

VIII. Portanto, a matriz $\Sigma$ √© semidefinida positiva.  $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Vamos verificar se a matriz de vari√¢ncia-covari√¢ncia do exemplo anterior √© semidefinida positiva usando o crit√©rio dos autovalores:
>
> $$
> \Sigma = \begin{bmatrix}
> 2 & 1 & 0.5 \\
> 1 & 2 & 1 \\
> 0.5 & 1 & 2
> \end{bmatrix}
> $$
> Calculamos os autovalores de $\Sigma$ (como feito no exemplo de PCA):
>
> ```python
> import numpy as np
> from numpy import linalg as LA
>
> # Matriz de Vari√¢ncia-Covari√¢ncia (do exemplo anterior)
> Sigma = np.array([
>     [2, 1, 0.5],
>     [1, 2, 1],
>     [0.5, 1, 2]
> ])
>
> # Calcula os autovalores
> eigenvalues = LA.eigvalsh(Sigma) # Usando eigvalsh para matriz sim√©trica
>
> print("Autovalores:", eigenvalues)
> ```
>
> Os autovalores calculados s√£o aproximadamente 3.70, 1.50, e 0.80. Como todos os autovalores s√£o positivos, a matriz $\Sigma$ √© semidefinida positiva (e, neste caso, positiva definida).

**Lema 2.** *Se $\lambda_i$ s√£o os autovalores da matriz $\Sigma_t$, ent√£o $\lambda_i \geq 0$ para todo i.*

**Demonstra√ß√£o.** Segue diretamente do fato de que $\Sigma_t$ √© uma matriz semidefinida positiva. Por defini√ß√£o, uma matriz semidefinida positiva tem todos os seus autovalores n√£o negativos.

> üí° **Exemplo Num√©rico:**
>
> Considere a matriz de vari√¢ncia-covari√¢ncia:
>
> $$
> \Sigma = \begin{bmatrix}
> 2 & 1 \\
> 1 & 2
> \end{bmatrix}
> $$
>
> Os autovalores de $\Sigma$ s√£o $\lambda_1 = 3$ e $\lambda_2 = 1$, ambos n√£o negativos. Isso confirma que $\Sigma$ √© semidefinida positiva.

**Teorema 1.1.** *A fun√ß√£o de autocovari√¢ncia de um processo AR(p) estacion√°rio e invert√≠vel, com ordem de processo suficientemente grande, pode ser aproximada utilizando matrizes de vari√¢ncia-covari√¢ncia obtidas a partir da s√©rie temporal.*

**Demonstra√ß√£o.** (Esbo√ßo) Esta prova se baseia na representa√ß√£o de um processo AR(p) como uma combina√ß√£o linear de observa√ß√µes passadas e na rela√ß√£o entre a matriz de vari√¢ncia-covari√¢ncia e as autocovari√¢ncias do processo.

**Teorema 2.** *Se a s√©rie temporal √© gaussiana, ent√£o a matriz de vari√¢ncia-covari√¢ncia $\Sigma_t$ caracteriza completamente a distribui√ß√£o conjunta das vari√°veis aleat√≥rias $Y_t, Y_{t-1}, \dots, Y_{t-j}$.*

**Demonstra√ß√£o.** (Esbo√ßo) Para uma s√©rie temporal gaussiana, a distribui√ß√£o conjunta de qualquer conjunto de vari√°veis aleat√≥rias √© completamente determinada pela sua m√©dia e matriz de vari√¢ncia-covari√¢ncia. Como a matriz $\Sigma_t$ cont√©m as vari√¢ncias e covari√¢ncias de $Y_t, Y_{t-1}, \dots, Y_{t-j}$, ela especifica completamente a distribui√ß√£o normal multivariada.  $\blacksquare$

**Teorema 2.1.** *Se a s√©rie temporal √© gaussiana e estacion√°ria, ent√£o a distribui√ß√£o conjunta das vari√°veis aleat√≥rias $Y_t, Y_{t-1}, \dots, Y_{t-j}$ √© uma distribui√ß√£o normal multivariada com m√©dia constante e matriz de vari√¢ncia-covari√¢ncia Toeplitz.*

**Demonstra√ß√£o.** (Esbo√ßo) Isso segue diretamente do Teorema 2 e do Lema 1. O Teorema 2 estabelece que a distribui√ß√£o conjunta √© normal multivariada, e o Lema 1 estabelece que para uma s√©rie temporal estacion√°ria, a matriz de vari√¢ncia-covari√¢ncia √© Toeplitz. Como a s√©rie √© estacion√°ria e gaussiana, a m√©dia √© constante. Portanto, a distribui√ß√£o conjunta √© uma distribui√ß√£o normal multivariada com m√©dia constante e matriz de vari√¢ncia-covari√¢ncia Toeplitz. $\blacksquare$

### Conclus√£o

A interpreta√ß√£o da autocovari√¢ncia como um elemento da matriz de vari√¢ncia-covari√¢ncia fornece uma perspectiva valiosa sobre a estrutura de depend√™ncia temporal em s√©ries temporais [^45]. Esta vis√£o encontra aplica√ß√µes pr√°ticas em modelos de espa√ßo de estados, filtragem de Kalman, PCA e modelagem de depend√™ncia multivariada. Ao compreender esta liga√ß√£o fundamental, podemos obter *insights* mais profundos sobre o comportamento das s√©ries temporais e construir modelos estat√≠sticos mais precisos.

### Refer√™ncias
[^45]: P√°gina 45 do texto original.
[^46]: P√°gina 46 do texto original.
[^69]: P√°gina 69 do texto original.
<!-- END -->