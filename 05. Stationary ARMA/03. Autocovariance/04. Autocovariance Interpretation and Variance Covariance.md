## AutocovariÃ¢ncia em SÃ©ries Temporais: AutocovariÃ¢ncia como Elemento da Matriz de VariÃ¢ncia-CovariÃ¢ncia

### IntroduÃ§Ã£o

Em continuidade Ã  discussÃ£o sobre a **autocovariÃ¢ncia** $\gamma_j$ [^45] e suas propriedades computacionais [^69], este capÃ­tulo explora uma perspectiva crucial: a interpretaÃ§Ã£o da autocovariÃ¢ncia como um elemento especÃ­fico da **matriz de variÃ¢ncia-covariÃ¢ncia** de um vetor construÃ­do a partir de observaÃ§Ãµes recentes da sÃ©rie temporal. Esta visÃ£o fornece uma ligaÃ§Ã£o fundamental entre a autocovariÃ¢ncia e a estrutura de dependÃªncia do processo estocÃ¡stico, encontrando aplicaÃ§Ãµes prÃ¡ticas em modelos de espaÃ§o de estados e filtragem de Kalman [^45].

### Conceitos Fundamentais

Considere uma sÃ©rie temporal $Y_t$. ConstruÃ­mos um vetor $x_t$ composto pelas $j+1$ observaÃ§Ãµes mais recentes de $Y$, ou seja:

$$
x_t = \begin{bmatrix}
Y_t \\
Y_{t-1} \\
\vdots \\
Y_{t-j}
\end{bmatrix}
$$

A **matriz de variÃ¢ncia-covariÃ¢ncia** de $x_t$, denotada por $\Sigma_t$, Ã© uma matriz de tamanho $(j+1) \times (j+1)$ cujos elementos sÃ£o as covariÃ¢ncias entre os diferentes componentes do vetor $x_t$. O elemento na *i*-Ã©sima linha e *k*-Ã©sima coluna de $\Sigma_t$ representa a covariÃ¢ncia entre $Y_{t-(i-1)}$ e $Y_{t-(k-1)}$.

Formalmente, o elemento $(i, k)$ de $\Sigma_t$ Ã© dado por:

$$
\Sigma_{t(i, k)} = E[(Y_{t-(i-1)} - \mu_{t-(i-1)})(Y_{t-(k-1)} - \mu_{t-(k-1)})]
$$

Para uma sÃ©rie temporal covariance-stationary, as mÃ©dias $\mu_{t-(i-1)}$ e $\mu_{t-(k-1)}$ sÃ£o iguais a $\mu$, e as covariÃ¢ncias dependem apenas da diferenÃ§a entre os Ã­ndices de tempo. Portanto, podemos reescrever a expressÃ£o acima como:

$$
\Sigma_{t(i, k)} = E[(Y_{t-(i-1)} - \mu)(Y_{t-(k-1)} - \mu)] = \gamma_{|i-k|}
$$

onde $\gamma_{|i-k|}$ Ã© a autocovariÃ¢ncia no lag $|i-k|$. Em particular, a autocovariÃ¢ncia $\gamma_j$ corresponde ao elemento $(1, j+1)$ (ou equivalentemente $(j+1, 1)$ devido Ã  simetria) da matriz $\Sigma_t$ [^45]:

$$
\Sigma_{t(1, j+1)} = E[(Y_t - \mu)(Y_{t-j} - \mu)] = \gamma_j
$$

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere uma sÃ©rie temporal $Y_t$ com mÃ©dia $\mu = 1$ e as seguintes autocovariÃ¢ncias: $\gamma_0 = 2$, $\gamma_1 = 1$, e $\gamma_2 = 0.5$.  ConstruÃ­mos um vetor $x_t$ com as trÃªs observaÃ§Ãµes mais recentes ($j=2$):
>
> $$
> x_t = \begin{bmatrix}
> Y_t \\
> Y_{t-1} \\
> Y_{t-2}
> \end{bmatrix}
> $$
>
> A matriz de variÃ¢ncia-covariÃ¢ncia $\Sigma_t$ seria:
>
> $$
> \Sigma_t = \begin{bmatrix}
> \gamma_0 & \gamma_1 & \gamma_2 \\
> \gamma_1 & \gamma_0 & \gamma_1 \\
> \gamma_2 & \gamma_1 & \gamma_0
> \end{bmatrix} = \begin{bmatrix}
> 2 & 1 & 0.5 \\
> 1 & 2 & 1 \\
> 0.5 & 1 & 2
> \end{bmatrix}
> $$
>
> O elemento (1,3) da matriz Ã© $\gamma_2 = 0.5$, que representa a autocovariÃ¢ncia no lag 2. Isto significa que a covariÃ¢ncia entre $Y_t$ e $Y_{t-2}$ Ã© 0.5.
>
> ```python
> import numpy as np
>
> # AutocovariÃ¢ncias
> gamma_0 = 2
> gamma_1 = 1
> gamma_2 = 0.5
>
> # Cria a matriz de variÃ¢ncia-covariÃ¢ncia
> Sigma = np.array([
>     [gamma_0, gamma_1, gamma_2],
>     [gamma_1, gamma_0, gamma_1],
>     [gamma_2, gamma_1, gamma_0]
> ])
>
> print("Matriz de VariÃ¢ncia-CovariÃ¢ncia:")
> print(Sigma)
> ```

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Imagine uma sÃ©rie temporal com os seguintes valores para os primeiros cinco perÃ­odos: $Y = [2, 3, 5, 4, 6]$. Vamos calcular a matriz de variÃ¢ncia-covariÃ¢ncia para $j=1$, ou seja, considerando apenas os dois Ãºltimos valores.
>
> 1.  Calculamos a mÃ©dia da sÃ©rie temporal: $\mu = (2+3+5+4+6)/5 = 4$.
> 2.  Calculamos as autocovariÃ¢ncias:
>     *   $\gamma_0 = E[(Y_t - \mu)(Y_t - \mu)] \approx \frac{1}{5} \sum_{t=1}^{5} (Y_t - \mu)^2 = \frac{1}{5}[(2-4)^2 + (3-4)^2 + (5-4)^2 + (4-4)^2 + (6-4)^2] = \frac{1}{5}[4 + 1 + 1 + 0 + 4] = 2$
>     *   $\gamma_1 = E[(Y_t - \mu)(Y_{t-1} - \mu)] \approx \frac{1}{4} \sum_{t=2}^{5} (Y_t - \mu)(Y_{t-1} - \mu) = \frac{1}{4}[(3-4)(2-4) + (5-4)(3-4) + (4-4)(5-4) + (6-4)(4-4)] = \frac{1}{4}[2 - 1 + 0 + 0] = 0.25$
>
> 3.  ConstruÃ­mos a matriz de variÃ¢ncia-covariÃ¢ncia:
>
>     $$
>     \Sigma_t = \begin{bmatrix}
>     \gamma_0 & \gamma_1 \\
>     \gamma_1 & \gamma_0
>     \end{bmatrix} = \begin{bmatrix}
>     2 & 0.25 \\
>     0.25 & 2
>     \end{bmatrix}
>     $$
>
> ```python
> import numpy as np
>
> # SÃ©rie temporal
> Y = np.array([2, 3, 5, 4, 6])
>
> # Calcula a mÃ©dia
> mu = np.mean(Y)
>
> # Calcula autocovariÃ¢ncia com lag 0 (gamma_0)
> gamma_0 = np.mean((Y - mu)**2)
>
> # Calcula autocovariÃ¢ncia com lag 1 (gamma_1)
> gamma_1 = np.mean((Y[1:] - mu) * (Y[:-1] - mu))
>
> # Cria a matriz de variÃ¢ncia-covariÃ¢ncia
> Sigma = np.array([
>     [gamma_0, gamma_1],
>     [gamma_1, gamma_0]
> ])
>
> print("MÃ©dia:", mu)
> print("AutocovariÃ¢ncia (lag 0):", gamma_0)
> print("AutocovariÃ¢ncia (lag 1):", gamma_1)
> print("Matriz de VariÃ¢ncia-CovariÃ¢ncia:")
> print(Sigma)
> ```

**Lema 1.** *Para uma sÃ©rie temporal estacionÃ¡ria, a matriz de variÃ¢ncia-covariÃ¢ncia $\Sigma_t$ Ã© Toeplitz.*

**DemonstraÃ§Ã£o.** Uma matriz Toeplitz Ã© uma matriz em que cada diagonal descendente da esquerda para a direita Ã© constante. Como a autocovariÃ¢ncia depende apenas da distÃ¢ncia entre os pontos no tempo (devido Ã  estacionariedade) e nÃ£o da sua posiÃ§Ã£o absoluta no tempo, os elementos da diagonal $i-k = const$ serÃ£o iguais. Portanto, $\Sigma_t$ Ã© uma matriz Toeplitz. $\blacksquare$

**Prova:**
I. Seja $\Sigma_t$ a matriz de variÃ¢ncia-covariÃ¢ncia de uma sÃ©rie temporal estacionÃ¡ria $Y_t$.

II. O elemento $(i,k)$ de $\Sigma_t$ Ã© dado por $\Sigma_{t(i,k)} = E[(Y_{t-(i-1)} - \mu)(Y_{t-(k-1)} - \mu)]$, onde $\mu$ Ã© a mÃ©dia da sÃ©rie temporal.

III. Devido Ã  estacionariedade, a autocovariÃ¢ncia depende apenas da diferenÃ§a entre os Ã­ndices de tempo, ou seja, $\Sigma_{t(i,k)} = \gamma_{|i-k|}$.

IV. Considere dois elementos na mesma diagonal descendente, digamos $(i,k)$ e $(i+1,k+1)$.

V. Queremos mostrar que $\Sigma_{t(i,k)} = \Sigma_{t(i+1,k+1)}$.

VI. Temos $\Sigma_{t(i+1,k+1)} = E[(Y_{t-i} - \mu)(Y_{t-k} - \mu)] = \gamma_{|i-k|}$.

VII. Portanto, $\Sigma_{t(i,k)} = \Sigma_{t(i+1,k+1)} = \gamma_{|i-k|}$, o que significa que todos os elementos na mesma diagonal descendente sÃ£o iguais.

VIII. Isso satisfaz a definiÃ§Ã£o de uma matriz Toeplitz.  $\blacksquare$

**Lema 1.1.** *Se a sÃ©rie temporal tem mÃ©dia zero, entÃ£o $\Sigma_{t(i, k)} = E[Y_{t-(i-1)}Y_{t-(k-1)}]$.*

**DemonstraÃ§Ã£o.** Se $\mu = 0$, entÃ£o $\Sigma_{t(i, k)} = E[(Y_{t-(i-1)} - 0)(Y_{t-(k-1)} - 0)] = E[Y_{t-(i-1)}Y_{t-(k-1)}]$.

**Lema 1.2.** *A matriz de variÃ¢ncia-covariÃ¢ncia de uma sÃ©rie temporal estacionÃ¡ria Ã© tambÃ©m simÃ©trica.*

**DemonstraÃ§Ã£o.** A simetria da matriz de variÃ¢ncia-covariÃ¢ncia decorre da propriedade de simetria da autocovariÃ¢ncia, ou seja, $\gamma_h = \gamma_{-h}$ para qualquer lag $h$. Assim, $\Sigma_{t(i, k)} = \gamma_{|i-k|} = \gamma_{|k-i|} = \Sigma_{t(k, i)}$, demonstrando que a matriz Ã© simÃ©trica.  $\blacksquare$

**Lema 1.3.** *Para uma sÃ©rie temporal estacionÃ¡ria, a matriz de variÃ¢ncia-covariÃ¢ncia $\Sigma_t$ Ã© unicamente definida pelas suas primeiras $j+1$ autocovariÃ¢ncias, $\gamma_0, \gamma_1, \dots, \gamma_j$.*

**DemonstraÃ§Ã£o.** Como $\Sigma_{t(i, k)} = \gamma_{|i-k|}$, cada elemento da matriz Ã© determinado por uma das autocovariÃ¢ncias $\gamma_0, \gamma_1, \dots, \gamma_j$. Especificamente, $\gamma_0$ aparece na diagonal principal, $\gamma_1$ nas diagonais adjacentes, e assim por diante, atÃ© $\gamma_j$ nos cantos superior direito e inferior esquerdo da matriz. Portanto, o conhecimento das primeiras $j+1$ autocovariÃ¢ncias define completamente a matriz $\Sigma_t$. $\blacksquare$

**ObservaÃ§Ãµes Importantes:**

*   **Estrutura de DependÃªncia:** A matriz de variÃ¢ncia-covariÃ¢ncia $\Sigma_t$ captura a estrutura de dependÃªncia linear completa entre as $j+1$ observaÃ§Ãµes mais recentes da sÃ©rie temporal.
*   **Estacionariedade:** Para sÃ©ries temporais estacionÃ¡rias, a matriz $\Sigma_t$ Ã© constante ao longo do tempo e depende apenas das autocovariÃ¢ncias.
*   **Simetria:** A matriz $\Sigma_t$ Ã© simÃ©trica, refletindo a propriedade de simetria da autocovariÃ¢ncia ($\gamma_j = \gamma_{-j}$ [^46]).

**AplicaÃ§Ãµes PrÃ¡ticas:**

A interpretaÃ§Ã£o da autocovariÃ¢ncia como um elemento da matriz de variÃ¢ncia-covariÃ¢ncia encontra aplicaÃ§Ãµes importantes em diversos contextos:

*   **Modelos de EspaÃ§o de Estados:** Em modelos de espaÃ§o de estados, a matriz $\Sigma_t$ desempenha um papel crucial na definiÃ§Ã£o da distribuiÃ§Ã£o de probabilidade do estado nÃ£o observado da sÃ©rie temporal [^45]. As equaÃ§Ãµes de estado e de observaÃ§Ã£o sÃ£o expressas em termos de matrizes que dependem da matriz de variÃ¢ncia-covariÃ¢ncia.
*   **Filtragem de Kalman:** O filtro de Kalman Ã© um algoritmo recursivo utilizado para estimar o estado de um sistema dinÃ¢mico a partir de uma sÃ©rie de mediÃ§Ãµes ruidosas. A matriz $\Sigma_t$ Ã© utilizada para calcular o ganho de Kalman, que pondera a importÃ¢ncia das novas mediÃ§Ãµes em relaÃ§Ã£o Ã  estimativa anterior do estado.
*   **AnÃ¡lise de Componentes Principais (PCA):** A PCA pode ser aplicada ao vetor $x_t$ para identificar os principais modos de variaÃ§Ã£o da sÃ©rie temporal. A matriz $\Sigma_t$ Ã© utilizada para calcular os autovalores e autovetores, que representam as direÃ§Ãµes de maior variÃ¢ncia.
*   **Modelagem de DependÃªncia Multivariada:** Em cenÃ¡rios com mÃºltiplas sÃ©ries temporais correlacionadas, a matriz de variÃ¢ncia-covariÃ¢ncia entre as diferentes sÃ©ries temporais e seus *lags* pode ser utilizada para modelar a dependÃªncia entre elas.

> ğŸ’¡ **Exemplo NumÃ©rico (PCA):**
>
> Vamos aplicar a AnÃ¡lise de Componentes Principais (PCA) Ã  matriz de variÃ¢ncia-covariÃ¢ncia do exemplo anterior:
>
> $$
> \Sigma_t = \begin{bmatrix}
> 2 & 1 & 0.5 \\
> 1 & 2 & 1 \\
> 0.5 & 1 & 2
> \end{bmatrix}
> $$
>
> ```python
> import numpy as np
> from numpy import linalg as LA
>
> # Matriz de VariÃ¢ncia-CovariÃ¢ncia (do exemplo anterior)
> Sigma = np.array([
>     [2, 1, 0.5],
>     [1, 2, 1],
>     [0.5, 1, 2]
> ])
>
> # Calcula os autovalores e autovetores
> eigenvalues, eigenvectors = LA.eig(Sigma)
>
> print("Autovalores:", eigenvalues)
> print("Autovetores:", eigenvectors)
>
> # A variÃ¢ncia explicada por cada componente principal Ã© proporcional ao autovalor correspondente
> explained_variance_ratio = eigenvalues / np.sum(eigenvalues)
> print("VariÃ¢ncia Explicada por Componente:", explained_variance_ratio)
> ```
>
> Os autovalores representam a variÃ¢ncia explicada por cada componente principal. O autovetor correspondente ao maior autovalor representa a direÃ§Ã£o na qual os dados tÃªm a maior variÃ¢ncia. No contexto da sÃ©rie temporal, isso pode nos dizer quais combinaÃ§Ãµes lineares das observaÃ§Ãµes passadas sÃ£o mais importantes para explicar o comportamento da sÃ©rie.

**ProposiÃ§Ã£o 1.** *Para uma sÃ©rie temporal estacionÃ¡ria e ergÃ³dica com mÃ©dia zero, a matriz de variÃ¢ncia-covariÃ¢ncia amostral converge em probabilidade para a matriz de variÃ¢ncia-covariÃ¢ncia teÃ³rica, desde que as condiÃ§Ãµes de estacionariedade e ergodicidade sejam satisfeitas.*

**DemonstraÃ§Ã£o.** (EsboÃ§o) Sob as condiÃ§Ãµes de estacionariedade e ergodicidade, a mÃ©dia amostral converge em probabilidade para a mÃ©dia teÃ³rica (que Ã© zero neste caso), e a estimativa amostral da autocovariÃ¢ncia converge em probabilidade para a autocovariÃ¢ncia teÃ³rica. Como a matriz de variÃ¢ncia-covariÃ¢ncia Ã© composta por estimativas de autocovariÃ¢ncia, a matriz amostral converge em probabilidade para a matriz teÃ³rica.

**Prova:**
I. Assumimos que a sÃ©rie temporal $\{Y_i\}_{i=1}^{I}$ Ã© estacionÃ¡ria e ergÃ³dica, e que a mÃ©dia teÃ³rica Ã© zero, ou seja, $E[Y_i] = 0$.

II. ConstruÃ­mos o vetor $x_t = [Y_t, Y_{t-1}, \dots, Y_{t-j}]'$ com $j+1$ observaÃ§Ãµes.

III. A matriz de variÃ¢ncia-covariÃ¢ncia teÃ³rica $\Sigma$ Ã© definida como:
$$\Sigma_{kl} = E[Y_{t-(k-1)} Y_{t-(l-1)}] = \gamma_{|k-l|}$$
para $k, l = 1, 2, \dots, j+1$.

IV. A estimativa amostral da matriz de variÃ¢ncia-covariÃ¢ncia $\hat{\Sigma}$ Ã© dada por:
$$\hat{\Sigma}_{kl} = \frac{1}{I} \sum_{t=j+1}^{I} Y_{t-(k-1)} Y_{t-(l-1)}$$
Note que a soma comeÃ§a em $t=j+1$ para garantir que todos os termos $Y_{t-(k-1)}$ e $Y_{t-(l-1)}$ estejam definidos dentro do intervalo da sÃ©rie temporal observada.

V. Queremos mostrar que a estimativa amostral converge em probabilidade para a matriz teÃ³rica, ou seja,
$$ \text{plim}_{I \to \infty} \hat{\Sigma}_{kl} = \Sigma_{kl} $$

VI. Tomando o limite de probabilidade:
$$ \text{plim}_{I \to \infty} \hat{\Sigma}_{kl} = \text{plim}_{I \to \infty} \frac{1}{I} \sum_{t=j+1}^{I} Y_{t-(k-1)} Y_{t-(l-1)}$$

VII. Pela definiÃ§Ã£o de ergodicidade, a mÃ©dia amostral converge em probabilidade para o valor esperado:
$$\text{plim}_{I \to \infty} \frac{1}{I} \sum_{t=j+1}^{I} Y_{t-(k-1)} Y_{t-(l-1)} = E[Y_{t-(k-1)} Y_{t-(l-1)}]$$

VIII. Substituindo de volta:
$$\text{plim}_{I \to \infty} \hat{\Sigma}_{kl} = E[Y_{t-(k-1)} Y_{t-(l-1)}] = \gamma_{|k-l|} = \Sigma_{kl}$$

IX. Portanto, a estimativa amostral da matriz de variÃ¢ncia-covariÃ¢ncia converge em probabilidade para a matriz de variÃ¢ncia-covariÃ¢ncia teÃ³rica.  $\blacksquare$

**ProposiÃ§Ã£o 1.1.** *Se a sÃ©rie temporal nÃ£o tem mÃ©dia zero, entÃ£o a ProposiÃ§Ã£o 1 ainda se mantÃ©m, desde que a mÃ©dia amostral seja utilizada para centralizar os dados antes do cÃ¡lculo da matriz de variÃ¢ncia-covariÃ¢ncia amostral.*

**DemonstraÃ§Ã£o.** (EsboÃ§o) Se a mÃ©dia nÃ£o Ã© zero, devemos subtrair a mÃ©dia amostral de cada observaÃ§Ã£o antes de calcular a autocovariÃ¢ncia amostral. Sob as condiÃ§Ãµes de estacionariedade e ergodicidade, a mÃ©dia amostral converge para a mÃ©dia teÃ³rica, e a autocovariÃ¢ncia amostral calculada com os dados centralizados converge para a autocovariÃ¢ncia teÃ³rica. Portanto, a matriz de variÃ¢ncia-covariÃ¢ncia amostral converge para a matriz teÃ³rica.  $\blacksquare$

**Prova:**
I. Seja $\{Y_t\}_{t=1}^n$ uma sÃ©rie temporal estacionÃ¡ria e ergÃ³dica com mÃ©dia $\mu \neq 0$.

II. Seja $\bar{Y} = \frac{1}{n}\sum_{t=1}^n Y_t$ a mÃ©dia amostral da sÃ©rie temporal.

III. Definimos a sÃ©rie temporal centralizada como $Y_t' = Y_t - \bar{Y}$.

IV. A matriz de variÃ¢ncia-covariÃ¢ncia teÃ³rica $\Sigma$ tem elementos $\Sigma_{ij} = E[(Y_{t-i} - \mu)(Y_{t-j} - \mu)] = \gamma_{|i-j|}$.

V. A matriz de variÃ¢ncia-covariÃ¢ncia amostral $\hat{\Sigma}$ tem elementos $\hat{\Sigma}_{ij} = \frac{1}{n} \sum_{t=\max(i,j)+1}^{n} (Y_{t-i} - \bar{Y})(Y_{t-j} - \bar{Y})$.

VI. Pela ergodicidade, $\bar{Y}$ converge em probabilidade para $\mu$ quando $n \to \infty$.

VII. Assim, $Y_t' = Y_t - \bar{Y}$ converge em probabilidade para $Y_t - \mu$.

VIII. Portanto, $\hat{\Sigma}_{ij} = \frac{1}{n} \sum_{t=\max(i,j)+1}^{n} (Y_{t-i} - \bar{Y})(Y_{t-j} - \bar{Y})$ converge em probabilidade para $E[(Y_{t-i} - \mu)(Y_{t-j} - \mu)] = \gamma_{|i-j|} = \Sigma_{ij}$.

IX. Assim, a matriz de variÃ¢ncia-covariÃ¢ncia amostral converge em probabilidade para a matriz teÃ³rica quando a sÃ©rie temporal Ã© centralizada pela mÃ©dia amostral.  $\blacksquare$

**Teorema 1.** *A matriz de variÃ¢ncia-covariÃ¢ncia $\Sigma_t$ Ã© sempre semidefinida positiva.*

**DemonstraÃ§Ã£o.** (EsboÃ§o) A matriz de variÃ¢ncia-covariÃ¢ncia $\Sigma_t$ Ã© semidefinida positiva porque, por definiÃ§Ã£o, para qualquer vetor $v$, a forma quadrÃ¡tica $v^T \Sigma_t v$ representa a variÃ¢ncia de uma combinaÃ§Ã£o linear das variÃ¡veis aleatÃ³rias em $x_t$, que Ã© sempre nÃ£o negativa.

Para demonstrar formalmente que $\Sigma$ Ã© semidefinida positiva, devemos mostrar que para qualquer vetor $z \in \mathbb{R}^{j+1}$, temos $z^T \Sigma z \geq 0$.

I. Seja $z = [z_0, z_1, ..., z_j]^T$ um vetor arbitrÃ¡rio em $\mathbb{R}^{j+1}$.

II. Considere a combinaÃ§Ã£o linear das variÃ¡veis aleatÃ³rias $Y_t, Y_{t-1}, ..., Y_{t-j}$:
$W = z_0 Y_t + z_1 Y_{t-1} + ... + z_j Y_{t-j}$.

III. Queremos mostrar que $E[W^2] \geq 0$ (jÃ¡ que $W^2$ Ã© sempre nÃ£o negativo).
$E[W^2] = E[(z_0 Y_t + z_1 Y_{t-1} + ... + z_j Y_{t-j})^2]$.

IV. Expandindo a expressÃ£o, temos:
$E[W^2] = \sum_{k=0}^{j} \sum_{l=0}^{j} z_k z_l E[Y_{t-k} Y_{t-l}]$.

V. Substituindo $E[Y_{t-k} Y_{t-l}] = \gamma_{|k-l|}$, jÃ¡ que a mÃ©dia Ã© zero:
$E[W^2] = \sum_{k=0}^{j} \sum_{l=0}^{j} z_k z_l \gamma_{|k-l|}$.

VI. Podemos expressar isso em notaÃ§Ã£o matricial:
$E[W^2] = z^T \Sigma z$, onde $\Sigma_{kl} = \gamma_{|k-l|}$.

VII. Como $E[W^2] \geq 0$, temos $z^T \Sigma z \geq 0$ para qualquer vetor $z$.

VIII. Portanto, a matriz $\Sigma$ Ã© semidefinida positiva.  $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Vamos verificar se a matriz de variÃ¢ncia-covariÃ¢ncia do exemplo anterior Ã© semidefinida positiva usando o critÃ©rio dos autovalores:
>
> $$
> \Sigma = \begin{bmatrix}
> 2 & 1 & 0.5 \\
> 1 & 2 & 1 \\
> 0.5 & 1 & 2
> \end{bmatrix}
> $$
> Calculamos os autovalores de $\Sigma$ (como feito no exemplo de PCA):
>
> ```python
> import numpy as np
> from numpy import linalg as LA
>
> # Matriz de VariÃ¢ncia-CovariÃ¢ncia (do exemplo anterior)
> Sigma = np.array([
>     [2, 1, 0.5],
>     [1, 2, 1],
>     [0.5, 1, 2]
> ])
>
> # Calcula os autovalores
> eigenvalues = LA.eigvalsh(Sigma) # Usando eigvalsh para matriz simÃ©trica
>
> print("Autovalores:", eigenvalues)
> ```
>
> Os autovalores calculados sÃ£o aproximadamente 3.70, 1.50, e 0.80. Como todos os autovalores sÃ£o positivos, a matriz $\Sigma$ Ã© semidefinida positiva (e, neste caso, positiva definida).

**Lema 2.** *Se $\lambda_i$ sÃ£o os autovalores da matriz $\Sigma_t$, entÃ£o $\lambda_i \geq 0$ para todo i.*

**DemonstraÃ§Ã£o.** Segue diretamente do fato de que $\Sigma_t$ Ã© uma matriz semidefinida positiva. Por definiÃ§Ã£o, uma matriz semidefinida positiva tem todos os seus autovalores nÃ£o negativos.

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere a matriz de variÃ¢ncia-covariÃ¢ncia:
>
> $$
> \Sigma = \begin{bmatrix}
> 2 & 1 \\
> 1 & 2
> \end{bmatrix}
> $$
>
> Os autovalores de $\Sigma$ sÃ£o $\lambda_1 = 3$ e $\lambda_2 = 1$, ambos nÃ£o negativos. Isso confirma que $\Sigma$ Ã© semidefinida positiva.

**Teorema 1.1.** *A funÃ§Ã£o de autocovariÃ¢ncia de um processo AR(p) estacionÃ¡rio e invertÃ­vel, com ordem de processo suficientemente grande, pode ser aproximada utilizando matrizes de variÃ¢ncia-covariÃ¢ncia obtidas a partir da sÃ©rie temporal.*

**DemonstraÃ§Ã£o.** (EsboÃ§o) Esta prova se baseia na representaÃ§Ã£o de um processo AR(p) como uma combinaÃ§Ã£o linear de observaÃ§Ãµes passadas e na relaÃ§Ã£o entre a matriz de variÃ¢ncia-covariÃ¢ncia e as autocovariÃ¢ncias do processo.

**Teorema 2.** *Se a sÃ©rie temporal Ã© gaussiana, entÃ£o a matriz de variÃ¢ncia-covariÃ¢ncia $\Sigma_t$ caracteriza completamente a distribuiÃ§Ã£o conjunta das variÃ¡veis aleatÃ³rias $Y_t, Y_{t-1}, \dots, Y_{t-j}$.*

**DemonstraÃ§Ã£o.** (EsboÃ§o) Para uma sÃ©rie temporal gaussiana, a distribuiÃ§Ã£o conjunta de qualquer conjunto de variÃ¡veis aleatÃ³rias Ã© completamente determinada pela sua mÃ©dia e matriz de variÃ¢ncia-covariÃ¢ncia. Como a matriz $\Sigma_t$ contÃ©m as variÃ¢ncias e covariÃ¢ncias de $Y_t, Y_{t-1}, \dots, Y_{t-j}$, ela especifica completamente a distribuiÃ§Ã£o normal multivariada.  $\blacksquare$

**Teorema 2.1.** *Se a sÃ©rie temporal Ã© gaussiana e estacionÃ¡ria, entÃ£o a distribuiÃ§Ã£o conjunta das variÃ¡veis aleatÃ³rias $Y_t, Y_{t-1}, \dots, Y_{t-j}$ Ã© uma distribuiÃ§Ã£o normal multivariada com mÃ©dia constante e matriz de variÃ¢ncia-covariÃ¢ncia Toeplitz.*

**DemonstraÃ§Ã£o.** (EsboÃ§o) Isso segue diretamente do Teorema 2 e do Lema 1. O Teorema 2 estabelece que a distribuiÃ§Ã£o conjunta Ã© normal multivariada, e o Lema 1 estabelece que para uma sÃ©rie temporal estacionÃ¡ria, a matriz de variÃ¢ncia-covariÃ¢ncia Ã© Toeplitz. Como a sÃ©rie Ã© estacionÃ¡ria e gaussiana, a mÃ©dia Ã© constante. Portanto, a distribuiÃ§Ã£o conjunta Ã© uma distribuiÃ§Ã£o normal multivariada com mÃ©dia constante e matriz de variÃ¢ncia-covariÃ¢ncia Toeplitz. $\blacksquare$

### ConclusÃ£o

A interpretaÃ§Ã£o da autocovariÃ¢ncia como um elemento da matriz de variÃ¢ncia-covariÃ¢ncia fornece uma perspectiva valiosa sobre a estrutura de dependÃªncia temporal em sÃ©ries temporais [^45]. Esta visÃ£o encontra aplicaÃ§Ãµes prÃ¡ticas em modelos de espaÃ§o de estados, filtragem de Kalman, PCA e modelagem de dependÃªncia multivariada. Ao compreender esta ligaÃ§Ã£o fundamental, podemos obter *insights* mais profundos sobre o comportamento das sÃ©ries temporais e construir modelos estatÃ­sticos mais precisos.

### ReferÃªncias
[^45]: PÃ¡gina 45 do texto original.
[^46]: PÃ¡gina 46 do texto original.
[^69]: PÃ¡gina 69 do texto original.
<!-- END -->