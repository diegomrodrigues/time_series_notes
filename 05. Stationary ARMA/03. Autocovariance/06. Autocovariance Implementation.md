## AutocovariÃ¢ncia em SÃ©ries Temporais: A RelaÃ§Ã£o com a CovariÃ¢ncia entre VariÃ¡veis AleatÃ³rias

### IntroduÃ§Ã£o
Em continuidade Ã  discussÃ£o sobre a **autocovariÃ¢ncia** $\gamma_j$ [^45] e suas propriedades para processos com inovaÃ§Ãµes nÃ£o correlacionadas [^45, 69], este capÃ­tulo explora uma conexÃ£o fundamental: a autocovariÃ¢ncia como uma forma especÃ­fica de **covariÃ¢ncia** entre duas variÃ¡veis aleatÃ³rias. A autocovariÃ¢ncia $\gamma_j$ entre $Y_t$ e $Y_{t-j}$ pode ser expressa como uma covariÃ¢ncia entre duas variÃ¡veis $X$ e $Y$, onde $X = Y_t$ e $Y = Y_{t-j}$. Formalmente, a covariÃ¢ncia entre duas variÃ¡veis aleatÃ³rias $X$ e $Y$ com mÃ©dias $\mu_X$ e $\mu_Y$, respectivamente, Ã© definida como:
$$
Cov(X, Y) = E[(X - \mu_X)(Y - \mu_Y)]
$$
Este capÃ­tulo visa demonstrar essa equivalÃªncia e suas implicaÃ§Ãµes prÃ¡ticas, especialmente no que tange Ã  implementaÃ§Ã£o computacional da autocovariÃ¢ncia.

### Conceitos Fundamentais

Como vimos anteriormente, a **autocovariÃ¢ncia** $\gamma_{jt}$ de uma sÃ©rie temporal $Y_t$ Ã© definida como [^45]:

$$
\gamma_{jt} = E[(Y_t - \mu_t)(Y_{t-j} - \mu_{t-j})]
$$

onde $\mu_t = E[Y_t]$ Ã© a mÃ©dia de $Y_t$. Agora, considere duas variÃ¡veis aleatÃ³rias $X$ e $Y$ definidas como:

$$
X = Y_t
$$

$$
Y = Y_{t-j}
$$

Neste caso, a mÃ©dia de $X$ Ã© $\mu_X = \mu_t$ e a mÃ©dia de $Y$ Ã© $\mu_Y = \mu_{t-j}$. Substituindo essas definiÃ§Ãµes na fÃ³rmula da covariÃ¢ncia, temos:

$$
Cov(X, Y) = E[(X - \mu_X)(Y - \mu_Y)] = E[(Y_t - \mu_t)(Y_{t-j} - \mu_{t-j})]
$$

Comparando esta expressÃ£o com a definiÃ§Ã£o de autocovariÃ¢ncia, vemos que:

$$
\gamma_{jt} = Cov(X, Y)
$$

Portanto, a autocovariÃ¢ncia entre $Y_t$ e $Y_{t-j}$ Ã© exatamente a covariÃ¢ncia entre as variÃ¡veis aleatÃ³rias $X = Y_t$ e $Y = Y_{t-j}$. Em essÃªncia, estamos apenas calculando a covariÃ¢ncia entre a sÃ©rie temporal em um ponto no tempo e ela mesma em um ponto anterior no tempo.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que temos uma sÃ©rie temporal $Y = [2, 4, 6, 8, 10]$ com mÃ©dia $\mu = 6$. Para calcular a autocovariÃ¢ncia no lag 1, definimos $X = Y[1:] = [4, 6, 8, 10]$ e $Y = Y[:-1] = [2, 4, 6, 8]$. As mÃ©dias de X e Y sÃ£o $\mu_X = 7$ e $\mu_Y = 5$, respectivamente.  Usando a fÃ³rmula da covariÃ¢ncia:
>
> $Cov(X, Y) = \frac{1}{N-1} \sum_{i=1}^{N-1}(X_i - \mu_X)(Y_i - \mu_Y)$
>
> $Cov(X, Y) = \frac{1}{4} [(4-7)(2-5) + (6-7)(4-5) + (8-7)(6-5) + (10-7)(8-5)] = \frac{1}{4} [(-3)(-3) + (-1)(-1) + (1)(1) + (3)(3)] = \frac{1}{4} [9 + 1 + 1 + 9] = 5$
>
> Portanto, a autocovariÃ¢ncia no lag 1 Ã© $\gamma_1 = Cov(X, Y) = 5$. Observe que dividimos por N-1 ao invÃ©s de N para um estimador nÃ£o-viesado.
>
> ```python
> import numpy as np
>
> # Serie temporal
> Y = np.array([2, 4, 6, 8, 10])
>
> # Calcula a mÃ©dia
> mu = np.mean(Y)
>
> # Define X e Y para autocovariÃ¢ncia no lag 1
> X = Y[1:]
> Y_lagged = Y[:-1]
>
> # Calcula a mÃ©dia de X e Y
> mu_X = np.mean(X)
> mu_Y = np.mean(Y_lagged)
>
> # Calcula a covariÃ¢ncia
> covariance = np.sum((X - mu_X) * (Y_lagged - mu_Y)) / (len(Y) - 1)
>
> print(f"AutocovariÃ¢ncia (lag 1): {covariance}")
> ```
>
> **InterpretaÃ§Ã£o:** Um valor de autocovariÃ¢ncia de 5 indica uma correlaÃ§Ã£o positiva entre os valores da sÃ©rie temporal em um determinado ponto no tempo e os valores um perÃ­odo anterior. Valores maiores sugerem uma dependÃªncia mais forte, enquanto valores prÃ³ximos de zero indicam uma dependÃªncia fraca.

#### AutocovariÃ¢ncia para uma sÃ©rie temporal *covariance-stationary*

Em continuidade Ã  discussÃ£o sobre a relaÃ§Ã£o da covariÃ¢ncia e a autocovariÃ¢ncia, vamos formalizar a relaÃ§Ã£o para um processo temporal *covariance-stationary*.
**Teorema 1.** *Para um processo covariance-stationary $Y_t$, a autocovariÃ¢ncia no lag $j$ Ã© equivalente Ã  covariÃ¢ncia entre as variÃ¡veis aleatÃ³rias $X = Y_t$ e $Y = Y_{t-j}$, com ambas as sÃ©ries tendo a mesma mÃ©dia $\mu$.*

**DemonstraÃ§Ã£o:**

I. Por definiÃ§Ã£o, a autocovariÃ¢ncia no lag $j$ para uma sÃ©rie temporal *covariance-stationary* Ã© dada por:
   $$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$$

II. Sejam $X = Y_t$ e $Y = Y_{t-j}$. EntÃ£o, $E[X] = E[Y_t] = \mu$ e $E[Y] = E[Y_{t-j}] = \mu$.

III. A covariÃ¢ncia entre $X$ e $Y$ Ã©:
   $$Cov(X, Y) = E[(X - E[X])(Y - E[Y])] = E[(Y_t - \mu)(Y_{t-j} - \mu)]$$

IV. Portanto, $\gamma_j = Cov(X, Y)$.

V. Isso demonstra que para um processo *covariance-stationary*, a autocovariÃ¢ncia no lag $j$ Ã© exatamente a covariÃ¢ncia entre $X = Y_t$ e $Y = Y_{t-j}$, ambas tendo a mesma mÃ©dia $\mu$. â– 

**ImplicaÃ§Ãµes e ObservaÃ§Ãµes:**

*   **Aproveitamento de Bibliotecas NumÃ©ricas:** Essa equivalÃªncia permite aproveitar as funÃ§Ãµes de covariÃ¢ncia jÃ¡ implementadas em diversas bibliotecas numÃ©ricas (e.g., NumPy, SciPy, pandas) para otimizar o cÃ¡lculo da autocovariÃ¢ncia. Em vez de implementar uma funÃ§Ã£o especÃ­fica para autocovariÃ¢ncia, podemos simplesmente utilizar a funÃ§Ã£o de covariÃ¢ncia padrÃ£o, fornecendo os dados apropriados.
*   **Clareza Conceitual:** A relaÃ§Ã£o com a covariÃ¢ncia entre variÃ¡veis aleatÃ³rias facilita a compreensÃ£o e interpretaÃ§Ã£o da autocovariÃ¢ncia. A autocovariÃ¢ncia quantifica a relaÃ§Ã£o linear entre a sÃ©rie temporal e seus valores passados, da mesma forma que a covariÃ¢ncia quantifica a relaÃ§Ã£o linear entre duas variÃ¡veis quaisquer.
*   **Flexibilidade:** Esta interpretaÃ§Ã£o se estende a diferentes tipos de sÃ©ries temporais e diferentes *lags*, desde que os dados sejam fornecidos corretamente Ã  funÃ§Ã£o de covariÃ¢ncia.

> ðŸ’¡ **Exemplo NumÃ©rico (NumPy):**
>
> ```python
> import numpy as np
> import pandas as pd
>
> # SÃ©rie temporal (dados de temperatura diÃ¡ria em graus Celsius)
> Y = np.array([20, 22, 25, 23, 21, 24, 26, 25, 23, 22])
>
> # Calcula a mÃ©dia
> mu = np.mean(Y)
>
> # Define X e Y para autocovariÃ¢ncia no lag 2
> X = Y[2:]  # Y[t]
> Y_lagged = Y[:-2] # Y[t-2]
>
> # Calcula a covariÃ¢ncia usando numpy
> covariance = np.cov(X, Y_lagged)[0, 1] # Elemento (0, 1) da matriz de covariÃ¢ncia
>
> print(f"AutocovariÃ¢ncia (lag 2) usando NumPy: {covariance}")
>
> # Usando pandas para verificar o resultado
> series = pd.Series(Y)
> autocovariance_pandas = series.autocovariance(lag=2)
> print(f"AutocovariÃ¢ncia (lag 2) usando Pandas: {autocovariance_pandas}")
> ```
>
> No exemplo, a funÃ§Ã£o `np.cov` calcula a matriz de covariÃ¢ncia entre X e `Y_lagged`. O elemento (0, 1) dessa matriz corresponde Ã  covariÃ¢ncia entre X e `Y_lagged`, que Ã© a autocovariÃ¢ncia da sÃ©rie temporal no lag 2. A funÃ§Ã£o `autocovariance` do pandas fornece o mesmo resultado, com uma implementaÃ§Ã£o otimizada.
>
> **InterpretaÃ§Ã£o:**
>
> Suponha que o resultado da autocovariÃ¢ncia (lag 2) seja 2.5. Isso sugere que existe uma relaÃ§Ã£o positiva entre a temperatura em um dia e a temperatura dois dias antes. O sinal positivo indica que se a temperatura hoje Ã© acima da mÃ©dia, Ã© provÃ¡vel que a temperatura dois dias atrÃ¡s tambÃ©m estivesse acima da mÃ©dia.
>
> **AnÃ¡lise de ResÃ­duos (Conceitual):**
>
> Para avaliar a adequaÃ§Ã£o do modelo, podemos analisar os resÃ­duos (a diferenÃ§a entre os valores observados e os valores previstos). Em um modelo bem ajustado, os resÃ­duos devem ser nÃ£o correlacionados. Podemos calcular a autocovariÃ¢ncia dos resÃ­duos para verificar se hÃ¡ padrÃµes remanescentes. Se a autocovariÃ¢ncia dos resÃ­duos for significativamente diferente de zero, isso pode indicar que o modelo precisa ser refinado.

**ProposiÃ§Ã£o 1.** *Para sÃ©ries temporais centradas (i.e., com mÃ©dia zero), a autocovariÃ¢ncia Ã© simplesmente o valor esperado do produto das variÃ¡veis aleatÃ³rias $X = Y_t$ e $Y = Y_{t-j}$.*

**DemonstraÃ§Ã£o.**
I.  Se a sÃ©rie temporal Ã© centrada, entÃ£o $\mu_t = \mu_{t-j} = 0$.
II. Portanto, $\gamma_j = E[(Y_t - 0)(Y_{t-j} - 0)] = E[Y_t Y_{t-j}]$.
III. Definindo $X = Y_t$ e $Y = Y_{t-j}$, temos $\gamma_j = E[XY]$.
Este resultado simplifica o cÃ¡lculo da autocovariÃ¢ncia em sÃ©ries centradas. $\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico (SÃ©rie Centrada):**
>
> Considere uma sÃ©rie temporal centrada $Y = [-2, -1, 0, 1, 2]$. Para o lag 1, $X = [-1, 0, 1, 2]$ e $Y = [-2, -1, 0, 1]$.
>
> $E[XY] = \frac{1}{4}[(-1)(-2) + (0)(-1) + (1)(0) + (2)(1)] = \frac{1}{4}[2 + 0 + 0 + 2] = 1$
>
> Portanto, a autocovariÃ¢ncia no lag 1 Ã© $\gamma_1 = 1$.

**ProposiÃ§Ã£o 2.** *A covariÃ¢ncia entre duas variÃ¡veis aleatÃ³rias X e Y pode ser expressa como $Cov(X,Y) = E[XY] - E[X]E[Y]$*.

**DemonstraÃ§Ã£o.**

ComeÃ§amos com a definiÃ§Ã£o de covariÃ¢ncia:
$$
Cov(X, Y) = E[(X - E[X])(Y - E[Y])]
$$

Expandimos o produto:
$$
Cov(X, Y) = E[XY - XE[Y] - YE[X] + E[X]E[Y]]
$$

Aplicamos a linearidade do operador de esperanÃ§a:
$$
Cov(X, Y) = E[XY] - E[XE[Y]] - E[YE[X]] + E[E[X]E[Y]]
$$

Como $E[X]$ e $E[Y]$ sÃ£o constantes, podemos tirÃ¡-las do operador de esperanÃ§a:
$$
Cov(X, Y) = E[XY] - E[Y]E[X] - E[X]E[Y] + E[X]E[Y]
$$

Simplificamos a expressÃ£o:
$$
Cov(X, Y) = E[XY] - E[X]E[Y]
$$
Portanto, a covariÃ¢ncia entre duas variÃ¡veis aleatÃ³rias X e Y pode ser expressa como $Cov(X,Y) = E[XY] - E[X]E[Y]$.  $\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Sejam $X = [1, 2, 3]$ e $Y = [4, 5, 6]$.
>
> $E[X] = (1+2+3)/3 = 2$
> $E[Y] = (4+5+6)/3 = 5$
>
> $E[XY] = (1*4 + 2*5 + 3*6)/3 = (4 + 10 + 18)/3 = 32/3 \approx 10.67$
>
> $Cov(X,Y) = E[XY] - E[X]E[Y] = 10.67 - 2 * 5 = 10.67 - 10 = 0.67$
>
> **InterpretaÃ§Ã£o:** A covariÃ¢ncia entre X e Y Ã© 0.67, o que indica uma relaÃ§Ã£o linear positiva. Quando X aumenta, Y tende a aumentar tambÃ©m.

**Teorema 2.** *Se as variÃ¡veis aleatÃ³rias $X$ e $Y$ sÃ£o independentes, entÃ£o $Cov(X, Y) = 0$*.

**DemonstraÃ§Ã£o.**
I. Por definiÃ§Ã£o, $Cov(X, Y) = E[(X - \mu_X)(Y - \mu_Y)]$.
II. Se X e Y sÃ£o independentes, entÃ£o $E[f(X)g(Y)] = E[f(X)]E[g(Y)]$ para quaisquer funÃ§Ãµes $f$ e $g$.
III. Portanto, $E[(X - \mu_X)(Y - \mu_Y)] = E[X - \mu_X]E[Y - \mu_Y]$.
IV. Como $E[X - \mu_X] = E[X] - \mu_X = 0$ e $E[Y - \mu_Y] = E[Y] - \mu_Y = 0$, temos $Cov(X, Y) = 0$.  $\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere duas variÃ¡veis aleatÃ³rias independentes $X = [1, 2, 3]$ e $Y = [4, 5, 6]$. As mÃ©dias sÃ£o $\mu_X = 2$ e $\mu_Y = 5$. EntÃ£o:
>
>   $Cov(X,Y) = \frac{1}{3}[(1-2)(4-5) + (2-2)(5-5) + (3-2)(6-5)] = \frac{1}{3}[(-1)(-1) + (0)(0) + (1)(1)] = \frac{1}{3}[1 + 0 + 1] = \frac{2}{3} \approx 0.67$.
>
> ObservaÃ§Ã£o: Na prÃ¡tica, mesmo que as sÃ©ries sejam independentes, a covariÃ¢ncia amostral pode nÃ£o ser exatamente zero devido a flutuaÃ§Ãµes amostrais. Em sÃ©ries independentes $Cov(X,Y) \approx 0$. Este pequeno valor surge devido ao tamanho da amostra finita. Quanto maior a amostra, mais prÃ³ximo de zero se espera que seja a covariÃ¢ncia.

**Teorema 2.1.** *Se $Y_t$ Ã© uma sÃ©rie temporal com autocovariÃ¢ncia $\gamma_j = 0$ para todo $j > 0$, entÃ£o para qualquer funÃ§Ã£o linear $f$, o valor esperado de $f(Y_t Y_{t-j})$ Ã© igual ao produto dos valores esperados de cada amostra $E[f(Y_t Y_{t-j})] = E[f(Y_t)]E[f(Y_{t-j})]$.*

**DemonstraÃ§Ã£o.**
I. Para $j > 0$, $Cov(Y_t, Y_{t-j}) = 0$.
II. Sabemos que $Cov(Y_t, Y_{t-j}) = E[Y_t Y_{t-j}] - E[Y_t]E[Y_{t-j}]$, assim $E[Y_t Y_{t-j}] = E[Y_t]E[Y_{t-j}]$.
III. Para qualquer funÃ§Ã£o linear $f$, $E[f(Y_t Y_{t-j})] = E[f(Y_t)]E[f(Y_{t-j})]$.  $\blacksquare$

**Lema 1.** *Se X e Y sÃ£o variÃ¡veis aleatÃ³rias independentes e pelo menos uma delas tem mÃ©dia zero, entÃ£o $E[XY] = 0$.*

**DemonstraÃ§Ã£o.**
I. Se X e Y sÃ£o independentes, $E[XY] = E[X]E[Y]$.
II. Se $E[X] = 0$ ou $E[Y] = 0$, entÃ£o $E[X]E[Y] = 0$.
III. Portanto, $E[XY] = 0$.  $\blacksquare$

**CorolÃ¡rio 1.** *Se $Y_t$ Ã© uma sÃ©rie temporal centrada e $Y_t$ e $Y_{t-j}$ sÃ£o independentes para $j > 0$, entÃ£o $\gamma_j = 0$ para todo $j > 0$.*

**DemonstraÃ§Ã£o.**
I. Como $Y_t$ Ã© centrada, $E[Y_t] = E[Y_{t-j}] = 0$.
II. Como $Y_t$ e $Y_{t-j}$ sÃ£o independentes, pelo Lema 1, $E[Y_t Y_{t-j}] = 0$.
III. Pela ProposiÃ§Ã£o 1, $\gamma_j = E[Y_t Y_{t-j}] = 0$.  $\blacksquare$

Para complementar a discussÃ£o sobre a autocovariÃ¢ncia e sua relaÃ§Ã£o com a covariÃ¢ncia, Ã© importante introduzir o conceito de correlaÃ§Ã£o cruzada, que Ã© uma normalizaÃ§Ã£o da covariÃ¢ncia e fornece uma medida da dependÃªncia linear entre duas sÃ©ries temporais em diferentes lags.

**DefiniÃ§Ã£o (CorrelaÃ§Ã£o Cruzada)**
A correlaÃ§Ã£o cruzada entre duas sÃ©ries temporais $X_t$ e $Y_t$ no lag $j$ Ã© definida como:

$$
\rho_{XY}(j) = \frac{Cov(X_t, Y_{t-j})}{\sigma_X \sigma_Y} = \frac{E[(X_t - \mu_X)(Y_{t-j} - \mu_Y)]}{\sigma_X \sigma_Y}
$$

onde $\mu_X$ e $\mu_Y$ sÃ£o as mÃ©dias de $X_t$ e $Y_t$, respectivamente, e $\sigma_X$ e $\sigma_Y$ sÃ£o os desvios padrÃµes de $X_t$ e $Y_t$, respectivamente.

Para o caso especial da autocorrelaÃ§Ã£o (correlaÃ§Ã£o de uma sÃ©rie consigo mesma), temos:

$$
\rho_{YY}(j) = \frac{Cov(Y_t, Y_{t-j})}{\sigma_Y^2} = \frac{\gamma_j}{\gamma_0}
$$

onde $\gamma_j$ Ã© a autocovariÃ¢ncia no lag $j$ e $\gamma_0$ Ã© a variÃ¢ncia da sÃ©rie temporal $Y_t$.

> ðŸ’¡ **Exemplo NumÃ©rico (CorrelaÃ§Ã£o Cruzada):**
>
> Sejam duas sÃ©ries temporais: $X = [1, 3, 5, 7, 9]$ e $Y = [2, 4, 6, 8, 10]$.
>
> $\mu_X = 5$, $\mu_Y = 6$
>
> $\sigma_X \approx 3.16$, $\sigma_Y \approx 3.16$
>
> Para o lag 0:
>
> $Cov(X, Y) = \frac{1}{4}[(1-5)(2-6) + (3-5)(4-6) + (5-5)(6-6) + (7-5)(8-6) + (9-5)(10-6)] = \frac{1}{4}[16 + 4 + 0 + 4 + 16] = 10$
>
> $\rho_{XY}(0) = \frac{10}{3.16 * 3.16} = \frac{10}{10} = 1$
>
> **InterpretaÃ§Ã£o:** A correlaÃ§Ã£o cruzada no lag 0 Ã© 1, indicando uma correlaÃ§Ã£o linear perfeita entre as duas sÃ©ries temporais.
>
> **Exemplo AutocorrelaÃ§Ã£o:**
>
> Usando a sÃ©rie $Y$ acima, $Y = [2, 4, 6, 8, 10]$:
>
> $\mu_Y = 6$
>
> $\gamma_0 = \sigma_Y^2 = 10$
>
> No Exemplo NumÃ©rico inicial, calculamos $\gamma_1 = 5$
>
> $\rho_{YY}(1) = \frac{5}{10} = 0.5$
>
> **InterpretaÃ§Ã£o:** Existe uma autocorrelaÃ§Ã£o positiva de 0.5 entre cada valor e o valor anterior.
>
> ```python
> import numpy as np
>
> # Series temporais
> X = np.array([1, 3, 5, 7, 9])
> Y = np.array([2, 4, 6, 8, 10])
>
> # Calcula as mÃ©dias
> mu_X = np.mean(X)
> mu_Y = np.mean(Y)
>
> # Calcula os desvios padrÃµes
> sigma_X = np.std(X)
> sigma_Y = np.std(Y)
>
> # Calcula a covariÃ¢ncia entre X e Y no lag 0
> covariance_XY = np.sum((X - mu_X) * (Y - mu_Y)) / (len(X) - 1)
>
> # Calcula a correlaÃ§Ã£o cruzada
> cross_correlation = covariance_XY / (sigma_X * sigma_Y)
>
> print(f"CorrelaÃ§Ã£o Cruzada (lag 0): {cross_correlation}")
>
> # AutocorrelaÃ§Ã£o para lag 1
> Y_lagged = Y[:-1]
> mu_Y_lagged = np.mean(Y_lagged)
> covariance_YY = np.sum((Y[1:] - mu_Y) * (Y_lagged - mu_Y_lagged)) / (len(Y) - 1)
> auto_correlation = covariance_YY / np.var(Y)
> print(f"Auto-CorrelaÃ§Ã£o (lag 1): {auto_correlation}")
> ```

**ProposiÃ§Ã£o 3.** *A funÃ§Ã£o de autocorrelaÃ§Ã£o $\rho_{YY}(j)$ Ã© sempre um valor entre -1 e 1, ou seja, $-1 \leq \rho_{YY}(j) \leq 1$.*

**DemonstraÃ§Ã£o.**

I. Pela desigualdade de Cauchy-Schwarz, $|Cov(X, Y)| \leq \sigma_X \sigma_Y$ para quaisquer variÃ¡veis aleatÃ³rias $X$ e $Y$, onde $\sigma_X$ e $\sigma_Y$ sÃ£o os desvios padrÃµes de $X$ e $Y$, respectivamente.
II. Aplicando a desigualdade de Cauchy-Schwarz a $X = Y_t$ e $Y = Y_{t-j}$, temos $|Cov(Y_t, Y_{t-j})| \leq \sigma_{Y_t} \sigma_{Y_{t-j}}$.
III. Para uma sÃ©rie estacionÃ¡ria, $\sigma_{Y_t} = \sigma_{Y_{t-j}} = \sigma_Y$. Portanto, $|Cov(Y_t, Y_{t-j})| \leq \sigma_Y^2$.
IV. Dividindo ambos os lados da desigualdade por $\sigma_Y^2$, obtemos $|\frac{Cov(Y_t, Y_{t-j})}{\sigma_Y^2}| \leq 1$.
V. Como $\rho_{YY}(j) = \frac{Cov(Y_t, Y_{t-j})}{\sigma_Y^2}$, temos $|\rho_{YY}(j)| \leq 1$, o que implica $-1 \leq \rho_{YY}(j) \leq 1$. $\blacksquare$

### ConclusÃ£o

A interpretaÃ§Ã£o da autocovariÃ¢ncia como uma forma de covariÃ¢ncia entre variÃ¡veis aleatÃ³rias fornece uma conexÃ£o importante entre a anÃ¡lise de sÃ©ries temporais e a teoria de probabilidade [^45, 69]. Essa equivalÃªncia permite aproveitar as funÃ§Ãµes de covariÃ¢ncia jÃ¡ implementadas em bibliotecas numÃ©ricas para otimizar o cÃ¡lculo da autocovariÃ¢ncia e facilita a compreensÃ£o conceitual da autocovariÃ¢ncia como uma medida da relaÃ§Ã£o linear entre a sÃ©rie temporal e seus valores passados. AlÃ©m disso, a introduÃ§Ã£o da correlaÃ§Ã£o cruzada e autocorrelaÃ§Ã£o permite uma anÃ¡lise mais completa da dependÃªncia entre sÃ©ries temporais, normalizando a covariÃ¢ncia para facilitar a comparaÃ§Ã£o entre diferentes lags e diferentes sÃ©ries.

### ReferÃªncias
[^45]: PÃ¡gina 45 do texto original.
[^69]: PÃ¡gina 69 do texto original.
<!-- END -->