## Autocovari√¢ncia em S√©ries Temporais: A Rela√ß√£o com a Covari√¢ncia entre Vari√°veis Aleat√≥rias

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre a **autocovari√¢ncia** $\gamma_j$ [^45] e suas propriedades para processos com inova√ß√µes n√£o correlacionadas [^45, 69], este cap√≠tulo explora uma conex√£o fundamental: a autocovari√¢ncia como uma forma espec√≠fica de **covari√¢ncia** entre duas vari√°veis aleat√≥rias. A autocovari√¢ncia $\gamma_j$ entre $Y_t$ e $Y_{t-j}$ pode ser expressa como uma covari√¢ncia entre duas vari√°veis $X$ e $Y$, onde $X = Y_t$ e $Y = Y_{t-j}$. Formalmente, a covari√¢ncia entre duas vari√°veis aleat√≥rias $X$ e $Y$ com m√©dias $\mu_X$ e $\mu_Y$, respectivamente, √© definida como:
$$
Cov(X, Y) = E[(X - \mu_X)(Y - \mu_Y)]
$$
Este cap√≠tulo visa demonstrar essa equival√™ncia e suas implica√ß√µes pr√°ticas, especialmente no que tange √† implementa√ß√£o computacional da autocovari√¢ncia.

### Conceitos Fundamentais

Como vimos anteriormente, a **autocovari√¢ncia** $\gamma_{jt}$ de uma s√©rie temporal $Y_t$ √© definida como [^45]:

$$
\gamma_{jt} = E[(Y_t - \mu_t)(Y_{t-j} - \mu_{t-j})]
$$

onde $\mu_t = E[Y_t]$ √© a m√©dia de $Y_t$. Agora, considere duas vari√°veis aleat√≥rias $X$ e $Y$ definidas como:

$$
X = Y_t
$$

$$
Y = Y_{t-j}
$$

Neste caso, a m√©dia de $X$ √© $\mu_X = \mu_t$ e a m√©dia de $Y$ √© $\mu_Y = \mu_{t-j}$. Substituindo essas defini√ß√µes na f√≥rmula da covari√¢ncia, temos:

$$
Cov(X, Y) = E[(X - \mu_X)(Y - \mu_Y)] = E[(Y_t - \mu_t)(Y_{t-j} - \mu_{t-j})]
$$

Comparando esta express√£o com a defini√ß√£o de autocovari√¢ncia, vemos que:

$$
\gamma_{jt} = Cov(X, Y)
$$

Portanto, a autocovari√¢ncia entre $Y_t$ e $Y_{t-j}$ √© exatamente a covari√¢ncia entre as vari√°veis aleat√≥rias $X = Y_t$ e $Y = Y_{t-j}$. Em ess√™ncia, estamos apenas calculando a covari√¢ncia entre a s√©rie temporal em um ponto no tempo e ela mesma em um ponto anterior no tempo.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos uma s√©rie temporal $Y = [2, 4, 6, 8, 10]$ com m√©dia $\mu = 6$. Para calcular a autocovari√¢ncia no lag 1, definimos $X = Y[1:] = [4, 6, 8, 10]$ e $Y = Y[:-1] = [2, 4, 6, 8]$. As m√©dias de X e Y s√£o $\mu_X = 7$ e $\mu_Y = 5$, respectivamente.  Usando a f√≥rmula da covari√¢ncia:
>
> $Cov(X, Y) = \frac{1}{N-1} \sum_{i=1}^{N-1}(X_i - \mu_X)(Y_i - \mu_Y)$
>
> $Cov(X, Y) = \frac{1}{4} [(4-7)(2-5) + (6-7)(4-5) + (8-7)(6-5) + (10-7)(8-5)] = \frac{1}{4} [(-3)(-3) + (-1)(-1) + (1)(1) + (3)(3)] = \frac{1}{4} [9 + 1 + 1 + 9] = 5$
>
> Portanto, a autocovari√¢ncia no lag 1 √© $\gamma_1 = Cov(X, Y) = 5$. Observe que dividimos por N-1 ao inv√©s de N para um estimador n√£o-viesado.
>
> ```python
> import numpy as np
>
> # Serie temporal
> Y = np.array([2, 4, 6, 8, 10])
>
> # Calcula a m√©dia
> mu = np.mean(Y)
>
> # Define X e Y para autocovari√¢ncia no lag 1
> X = Y[1:]
> Y_lagged = Y[:-1]
>
> # Calcula a m√©dia de X e Y
> mu_X = np.mean(X)
> mu_Y = np.mean(Y_lagged)
>
> # Calcula a covari√¢ncia
> covariance = np.sum((X - mu_X) * (Y_lagged - mu_Y)) / (len(Y) - 1)
>
> print(f"Autocovari√¢ncia (lag 1): {covariance}")
> ```
>
> **Interpreta√ß√£o:** Um valor de autocovari√¢ncia de 5 indica uma correla√ß√£o positiva entre os valores da s√©rie temporal em um determinado ponto no tempo e os valores um per√≠odo anterior. Valores maiores sugerem uma depend√™ncia mais forte, enquanto valores pr√≥ximos de zero indicam uma depend√™ncia fraca.

#### Autocovari√¢ncia para uma s√©rie temporal *covariance-stationary*

Em continuidade √† discuss√£o sobre a rela√ß√£o da covari√¢ncia e a autocovari√¢ncia, vamos formalizar a rela√ß√£o para um processo temporal *covariance-stationary*.
**Teorema 1.** *Para um processo covariance-stationary $Y_t$, a autocovari√¢ncia no lag $j$ √© equivalente √† covari√¢ncia entre as vari√°veis aleat√≥rias $X = Y_t$ e $Y = Y_{t-j}$, com ambas as s√©ries tendo a mesma m√©dia $\mu$.*

**Demonstra√ß√£o:**

I. Por defini√ß√£o, a autocovari√¢ncia no lag $j$ para uma s√©rie temporal *covariance-stationary* √© dada por:
   $$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$$

II. Sejam $X = Y_t$ e $Y = Y_{t-j}$. Ent√£o, $E[X] = E[Y_t] = \mu$ e $E[Y] = E[Y_{t-j}] = \mu$.

III. A covari√¢ncia entre $X$ e $Y$ √©:
   $$Cov(X, Y) = E[(X - E[X])(Y - E[Y])] = E[(Y_t - \mu)(Y_{t-j} - \mu)]$$

IV. Portanto, $\gamma_j = Cov(X, Y)$.

V. Isso demonstra que para um processo *covariance-stationary*, a autocovari√¢ncia no lag $j$ √© exatamente a covari√¢ncia entre $X = Y_t$ e $Y = Y_{t-j}$, ambas tendo a mesma m√©dia $\mu$. ‚ñ†

**Implica√ß√µes e Observa√ß√µes:**

*   **Aproveitamento de Bibliotecas Num√©ricas:** Essa equival√™ncia permite aproveitar as fun√ß√µes de covari√¢ncia j√° implementadas em diversas bibliotecas num√©ricas (e.g., NumPy, SciPy, pandas) para otimizar o c√°lculo da autocovari√¢ncia. Em vez de implementar uma fun√ß√£o espec√≠fica para autocovari√¢ncia, podemos simplesmente utilizar a fun√ß√£o de covari√¢ncia padr√£o, fornecendo os dados apropriados.
*   **Clareza Conceitual:** A rela√ß√£o com a covari√¢ncia entre vari√°veis aleat√≥rias facilita a compreens√£o e interpreta√ß√£o da autocovari√¢ncia. A autocovari√¢ncia quantifica a rela√ß√£o linear entre a s√©rie temporal e seus valores passados, da mesma forma que a covari√¢ncia quantifica a rela√ß√£o linear entre duas vari√°veis quaisquer.
*   **Flexibilidade:** Esta interpreta√ß√£o se estende a diferentes tipos de s√©ries temporais e diferentes *lags*, desde que os dados sejam fornecidos corretamente √† fun√ß√£o de covari√¢ncia.

> üí° **Exemplo Num√©rico (NumPy):**
>
> ```python
> import numpy as np
> import pandas as pd
>
> # S√©rie temporal (dados de temperatura di√°ria em graus Celsius)
> Y = np.array([20, 22, 25, 23, 21, 24, 26, 25, 23, 22])
>
> # Calcula a m√©dia
> mu = np.mean(Y)
>
> # Define X e Y para autocovari√¢ncia no lag 2
> X = Y[2:]  # Y[t]
> Y_lagged = Y[:-2] # Y[t-2]
>
> # Calcula a covari√¢ncia usando numpy
> covariance = np.cov(X, Y_lagged)[0, 1] # Elemento (0, 1) da matriz de covari√¢ncia
>
> print(f"Autocovari√¢ncia (lag 2) usando NumPy: {covariance}")
>
> # Usando pandas para verificar o resultado
> series = pd.Series(Y)
> autocovariance_pandas = series.autocovariance(lag=2)
> print(f"Autocovari√¢ncia (lag 2) usando Pandas: {autocovariance_pandas}")
> ```
>
> No exemplo, a fun√ß√£o `np.cov` calcula a matriz de covari√¢ncia entre X e `Y_lagged`. O elemento (0, 1) dessa matriz corresponde √† covari√¢ncia entre X e `Y_lagged`, que √© a autocovari√¢ncia da s√©rie temporal no lag 2. A fun√ß√£o `autocovariance` do pandas fornece o mesmo resultado, com uma implementa√ß√£o otimizada.
>
> **Interpreta√ß√£o:**
>
> Suponha que o resultado da autocovari√¢ncia (lag 2) seja 2.5. Isso sugere que existe uma rela√ß√£o positiva entre a temperatura em um dia e a temperatura dois dias antes. O sinal positivo indica que se a temperatura hoje √© acima da m√©dia, √© prov√°vel que a temperatura dois dias atr√°s tamb√©m estivesse acima da m√©dia.
>
> **An√°lise de Res√≠duos (Conceitual):**
>
> Para avaliar a adequa√ß√£o do modelo, podemos analisar os res√≠duos (a diferen√ßa entre os valores observados e os valores previstos). Em um modelo bem ajustado, os res√≠duos devem ser n√£o correlacionados. Podemos calcular a autocovari√¢ncia dos res√≠duos para verificar se h√° padr√µes remanescentes. Se a autocovari√¢ncia dos res√≠duos for significativamente diferente de zero, isso pode indicar que o modelo precisa ser refinado.

**Proposi√ß√£o 1.** *Para s√©ries temporais centradas (i.e., com m√©dia zero), a autocovari√¢ncia √© simplesmente o valor esperado do produto das vari√°veis aleat√≥rias $X = Y_t$ e $Y = Y_{t-j}$.*

**Demonstra√ß√£o.**
I.  Se a s√©rie temporal √© centrada, ent√£o $\mu_t = \mu_{t-j} = 0$.
II. Portanto, $\gamma_j = E[(Y_t - 0)(Y_{t-j} - 0)] = E[Y_t Y_{t-j}]$.
III. Definindo $X = Y_t$ e $Y = Y_{t-j}$, temos $\gamma_j = E[XY]$.
Este resultado simplifica o c√°lculo da autocovari√¢ncia em s√©ries centradas. $\blacksquare$

> üí° **Exemplo Num√©rico (S√©rie Centrada):**
>
> Considere uma s√©rie temporal centrada $Y = [-2, -1, 0, 1, 2]$. Para o lag 1, $X = [-1, 0, 1, 2]$ e $Y = [-2, -1, 0, 1]$.
>
> $E[XY] = \frac{1}{4}[(-1)(-2) + (0)(-1) + (1)(0) + (2)(1)] = \frac{1}{4}[2 + 0 + 0 + 2] = 1$
>
> Portanto, a autocovari√¢ncia no lag 1 √© $\gamma_1 = 1$.

**Proposi√ß√£o 2.** *A covari√¢ncia entre duas vari√°veis aleat√≥rias X e Y pode ser expressa como $Cov(X,Y) = E[XY] - E[X]E[Y]$*.

**Demonstra√ß√£o.**

Come√ßamos com a defini√ß√£o de covari√¢ncia:
$$
Cov(X, Y) = E[(X - E[X])(Y - E[Y])]
$$

Expandimos o produto:
$$
Cov(X, Y) = E[XY - XE[Y] - YE[X] + E[X]E[Y]]
$$

Aplicamos a linearidade do operador de esperan√ßa:
$$
Cov(X, Y) = E[XY] - E[XE[Y]] - E[YE[X]] + E[E[X]E[Y]]
$$

Como $E[X]$ e $E[Y]$ s√£o constantes, podemos tir√°-las do operador de esperan√ßa:
$$
Cov(X, Y) = E[XY] - E[Y]E[X] - E[X]E[Y] + E[X]E[Y]
$$

Simplificamos a express√£o:
$$
Cov(X, Y) = E[XY] - E[X]E[Y]
$$
Portanto, a covari√¢ncia entre duas vari√°veis aleat√≥rias X e Y pode ser expressa como $Cov(X,Y) = E[XY] - E[X]E[Y]$.  $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Sejam $X = [1, 2, 3]$ e $Y = [4, 5, 6]$.
>
> $E[X] = (1+2+3)/3 = 2$
> $E[Y] = (4+5+6)/3 = 5$
>
> $E[XY] = (1*4 + 2*5 + 3*6)/3 = (4 + 10 + 18)/3 = 32/3 \approx 10.67$
>
> $Cov(X,Y) = E[XY] - E[X]E[Y] = 10.67 - 2 * 5 = 10.67 - 10 = 0.67$
>
> **Interpreta√ß√£o:** A covari√¢ncia entre X e Y √© 0.67, o que indica uma rela√ß√£o linear positiva. Quando X aumenta, Y tende a aumentar tamb√©m.

**Teorema 2.** *Se as vari√°veis aleat√≥rias $X$ e $Y$ s√£o independentes, ent√£o $Cov(X, Y) = 0$*.

**Demonstra√ß√£o.**
I. Por defini√ß√£o, $Cov(X, Y) = E[(X - \mu_X)(Y - \mu_Y)]$.
II. Se X e Y s√£o independentes, ent√£o $E[f(X)g(Y)] = E[f(X)]E[g(Y)]$ para quaisquer fun√ß√µes $f$ e $g$.
III. Portanto, $E[(X - \mu_X)(Y - \mu_Y)] = E[X - \mu_X]E[Y - \mu_Y]$.
IV. Como $E[X - \mu_X] = E[X] - \mu_X = 0$ e $E[Y - \mu_Y] = E[Y] - \mu_Y = 0$, temos $Cov(X, Y) = 0$.  $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Considere duas vari√°veis aleat√≥rias independentes $X = [1, 2, 3]$ e $Y = [4, 5, 6]$. As m√©dias s√£o $\mu_X = 2$ e $\mu_Y = 5$. Ent√£o:
>
>   $Cov(X,Y) = \frac{1}{3}[(1-2)(4-5) + (2-2)(5-5) + (3-2)(6-5)] = \frac{1}{3}[(-1)(-1) + (0)(0) + (1)(1)] = \frac{1}{3}[1 + 0 + 1] = \frac{2}{3} \approx 0.67$.
>
> Observa√ß√£o: Na pr√°tica, mesmo que as s√©ries sejam independentes, a covari√¢ncia amostral pode n√£o ser exatamente zero devido a flutua√ß√µes amostrais. Em s√©ries independentes $Cov(X,Y) \approx 0$. Este pequeno valor surge devido ao tamanho da amostra finita. Quanto maior a amostra, mais pr√≥ximo de zero se espera que seja a covari√¢ncia.

**Teorema 2.1.** *Se $Y_t$ √© uma s√©rie temporal com autocovari√¢ncia $\gamma_j = 0$ para todo $j > 0$, ent√£o para qualquer fun√ß√£o linear $f$, o valor esperado de $f(Y_t Y_{t-j})$ √© igual ao produto dos valores esperados de cada amostra $E[f(Y_t Y_{t-j})] = E[f(Y_t)]E[f(Y_{t-j})]$.*

**Demonstra√ß√£o.**
I. Para $j > 0$, $Cov(Y_t, Y_{t-j}) = 0$.
II. Sabemos que $Cov(Y_t, Y_{t-j}) = E[Y_t Y_{t-j}] - E[Y_t]E[Y_{t-j}]$, assim $E[Y_t Y_{t-j}] = E[Y_t]E[Y_{t-j}]$.
III. Para qualquer fun√ß√£o linear $f$, $E[f(Y_t Y_{t-j})] = E[f(Y_t)]E[f(Y_{t-j})]$.  $\blacksquare$

**Lema 1.** *Se X e Y s√£o vari√°veis aleat√≥rias independentes e pelo menos uma delas tem m√©dia zero, ent√£o $E[XY] = 0$.*

**Demonstra√ß√£o.**
I. Se X e Y s√£o independentes, $E[XY] = E[X]E[Y]$.
II. Se $E[X] = 0$ ou $E[Y] = 0$, ent√£o $E[X]E[Y] = 0$.
III. Portanto, $E[XY] = 0$.  $\blacksquare$

**Corol√°rio 1.** *Se $Y_t$ √© uma s√©rie temporal centrada e $Y_t$ e $Y_{t-j}$ s√£o independentes para $j > 0$, ent√£o $\gamma_j = 0$ para todo $j > 0$.*

**Demonstra√ß√£o.**
I. Como $Y_t$ √© centrada, $E[Y_t] = E[Y_{t-j}] = 0$.
II. Como $Y_t$ e $Y_{t-j}$ s√£o independentes, pelo Lema 1, $E[Y_t Y_{t-j}] = 0$.
III. Pela Proposi√ß√£o 1, $\gamma_j = E[Y_t Y_{t-j}] = 0$.  $\blacksquare$

Para complementar a discuss√£o sobre a autocovari√¢ncia e sua rela√ß√£o com a covari√¢ncia, √© importante introduzir o conceito de correla√ß√£o cruzada, que √© uma normaliza√ß√£o da covari√¢ncia e fornece uma medida da depend√™ncia linear entre duas s√©ries temporais em diferentes lags.

**Defini√ß√£o (Correla√ß√£o Cruzada)**
A correla√ß√£o cruzada entre duas s√©ries temporais $X_t$ e $Y_t$ no lag $j$ √© definida como:

$$
\rho_{XY}(j) = \frac{Cov(X_t, Y_{t-j})}{\sigma_X \sigma_Y} = \frac{E[(X_t - \mu_X)(Y_{t-j} - \mu_Y)]}{\sigma_X \sigma_Y}
$$

onde $\mu_X$ e $\mu_Y$ s√£o as m√©dias de $X_t$ e $Y_t$, respectivamente, e $\sigma_X$ e $\sigma_Y$ s√£o os desvios padr√µes de $X_t$ e $Y_t$, respectivamente.

Para o caso especial da autocorrela√ß√£o (correla√ß√£o de uma s√©rie consigo mesma), temos:

$$
\rho_{YY}(j) = \frac{Cov(Y_t, Y_{t-j})}{\sigma_Y^2} = \frac{\gamma_j}{\gamma_0}
$$

onde $\gamma_j$ √© a autocovari√¢ncia no lag $j$ e $\gamma_0$ √© a vari√¢ncia da s√©rie temporal $Y_t$.

> üí° **Exemplo Num√©rico (Correla√ß√£o Cruzada):**
>
> Sejam duas s√©ries temporais: $X = [1, 3, 5, 7, 9]$ e $Y = [2, 4, 6, 8, 10]$.
>
> $\mu_X = 5$, $\mu_Y = 6$
>
> $\sigma_X \approx 3.16$, $\sigma_Y \approx 3.16$
>
> Para o lag 0:
>
> $Cov(X, Y) = \frac{1}{4}[(1-5)(2-6) + (3-5)(4-6) + (5-5)(6-6) + (7-5)(8-6) + (9-5)(10-6)] = \frac{1}{4}[16 + 4 + 0 + 4 + 16] = 10$
>
> $\rho_{XY}(0) = \frac{10}{3.16 * 3.16} = \frac{10}{10} = 1$
>
> **Interpreta√ß√£o:** A correla√ß√£o cruzada no lag 0 √© 1, indicando uma correla√ß√£o linear perfeita entre as duas s√©ries temporais.
>
> **Exemplo Autocorrela√ß√£o:**
>
> Usando a s√©rie $Y$ acima, $Y = [2, 4, 6, 8, 10]$:
>
> $\mu_Y = 6$
>
> $\gamma_0 = \sigma_Y^2 = 10$
>
> No Exemplo Num√©rico inicial, calculamos $\gamma_1 = 5$
>
> $\rho_{YY}(1) = \frac{5}{10} = 0.5$
>
> **Interpreta√ß√£o:** Existe uma autocorrela√ß√£o positiva de 0.5 entre cada valor e o valor anterior.
>
> ```python
> import numpy as np
>
> # Series temporais
> X = np.array([1, 3, 5, 7, 9])
> Y = np.array([2, 4, 6, 8, 10])
>
> # Calcula as m√©dias
> mu_X = np.mean(X)
> mu_Y = np.mean(Y)
>
> # Calcula os desvios padr√µes
> sigma_X = np.std(X)
> sigma_Y = np.std(Y)
>
> # Calcula a covari√¢ncia entre X e Y no lag 0
> covariance_XY = np.sum((X - mu_X) * (Y - mu_Y)) / (len(X) - 1)
>
> # Calcula a correla√ß√£o cruzada
> cross_correlation = covariance_XY / (sigma_X * sigma_Y)
>
> print(f"Correla√ß√£o Cruzada (lag 0): {cross_correlation}")
>
> # Autocorrela√ß√£o para lag 1
> Y_lagged = Y[:-1]
> mu_Y_lagged = np.mean(Y_lagged)
> covariance_YY = np.sum((Y[1:] - mu_Y) * (Y_lagged - mu_Y_lagged)) / (len(Y) - 1)
> auto_correlation = covariance_YY / np.var(Y)
> print(f"Auto-Correla√ß√£o (lag 1): {auto_correlation}")
> ```

**Proposi√ß√£o 3.** *A fun√ß√£o de autocorrela√ß√£o $\rho_{YY}(j)$ √© sempre um valor entre -1 e 1, ou seja, $-1 \leq \rho_{YY}(j) \leq 1$.*

**Demonstra√ß√£o.**

I. Pela desigualdade de Cauchy-Schwarz, $|Cov(X, Y)| \leq \sigma_X \sigma_Y$ para quaisquer vari√°veis aleat√≥rias $X$ e $Y$, onde $\sigma_X$ e $\sigma_Y$ s√£o os desvios padr√µes de $X$ e $Y$, respectivamente.
II. Aplicando a desigualdade de Cauchy-Schwarz a $X = Y_t$ e $Y = Y_{t-j}$, temos $|Cov(Y_t, Y_{t-j})| \leq \sigma_{Y_t} \sigma_{Y_{t-j}}$.
III. Para uma s√©rie estacion√°ria, $\sigma_{Y_t} = \sigma_{Y_{t-j}} = \sigma_Y$. Portanto, $|Cov(Y_t, Y_{t-j})| \leq \sigma_Y^2$.
IV. Dividindo ambos os lados da desigualdade por $\sigma_Y^2$, obtemos $|\frac{Cov(Y_t, Y_{t-j})}{\sigma_Y^2}| \leq 1$.
V. Como $\rho_{YY}(j) = \frac{Cov(Y_t, Y_{t-j})}{\sigma_Y^2}$, temos $|\rho_{YY}(j)| \leq 1$, o que implica $-1 \leq \rho_{YY}(j) \leq 1$. $\blacksquare$

### Conclus√£o

A interpreta√ß√£o da autocovari√¢ncia como uma forma de covari√¢ncia entre vari√°veis aleat√≥rias fornece uma conex√£o importante entre a an√°lise de s√©ries temporais e a teoria de probabilidade [^45, 69]. Essa equival√™ncia permite aproveitar as fun√ß√µes de covari√¢ncia j√° implementadas em bibliotecas num√©ricas para otimizar o c√°lculo da autocovari√¢ncia e facilita a compreens√£o conceitual da autocovari√¢ncia como uma medida da rela√ß√£o linear entre a s√©rie temporal e seus valores passados. Al√©m disso, a introdu√ß√£o da correla√ß√£o cruzada e autocorrela√ß√£o permite uma an√°lise mais completa da depend√™ncia entre s√©ries temporais, normalizando a covari√¢ncia para facilitar a compara√ß√£o entre diferentes lags e diferentes s√©ries.

### Refer√™ncias
[^45]: P√°gina 45 do texto original.
[^69]: P√°gina 69 do texto original.
<!-- END -->