## An√°lise Avan√ßada de Processos de M√©dia M√≥vel de Primeira Ordem (MA(1))

### Introdu√ß√£o
Em continuidade ao estudo de processos estoc√°sticos estacion√°rios, este cap√≠tulo se dedica a uma an√°lise aprofundada dos processos de *M√©dia M√≥vel de Primeira Ordem*, denotados MA(1). Como vimos anteriormente [^47], o conceito de **White Noise** √© fundamental para a constru√ß√£o de modelos de s√©ries temporais. Expandindo este conceito, introduzimos agora um dos modelos mais b√°sicos, por√©m importantes, na an√°lise de s√©ries temporais: o processo MA(1). Este processo, definido como uma combina√ß√£o linear do termo de erro corrente e do termo de erro defasado em um per√≠odo, oferece uma maneira simples, mas eficaz, de modelar depend√™ncias temporais em dados sequenciais.

### Conceitos Fundamentais

Um processo MA(1) √© definido pela seguinte equa√ß√£o:

$$Y_t = \mu + \varepsilon_t + \theta\varepsilon_{t-1}$$

onde:
*   $Y_t$ representa o valor da s√©rie temporal no instante *t*.
*   $\mu$ √© a m√©dia constante do processo [^3.3.1]. √â importante notar que $\mu$ pode ser uma fun√ß√£o do tempo $t$, permitindo uma maior generalidade [^3.1.6, 3.1.7].
*   $\varepsilon_t$ √© um processo de *White Noise* no instante *t*, caracterizado por m√©dia zero, vari√¢ncia constante ($\sigma^2$) e aus√™ncia de autocorrela√ß√£o [^3.2.1, 3.2.2, 3.2.3].
*   $\theta$ √© o coeficiente do termo de m√©dia m√≥vel, determinando o peso da inova√ß√£o passada ($\varepsilon_{t-1}$) no valor atual da s√©rie [^3.3.1].

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um processo MA(1) definido como:
>
> $$Y_t = 10 + \varepsilon_t + 0.5\varepsilon_{t-1}$$
>
> Aqui, $\mu = 10$ e $\theta = 0.5$.  Se $\varepsilon_t$ e $\varepsilon_{t-1}$ s√£o ru√≠dos brancos com m√©dia zero e desvio padr√£o 1 (portanto, vari√¢ncia $\sigma^2 = 1$), podemos simular alguns valores de $Y_t$:
>
> *   Se $\varepsilon_0 = 2$ e $\varepsilon_1 = -1$, ent√£o $Y_1 = 10 + (-1) + 0.5(2) = 10 - 1 + 1 = 10$.
> *   Se $\varepsilon_1 = -1$ e $\varepsilon_2 = 0.5$, ent√£o $Y_2 = 10 + 0.5 + 0.5(-1) = 10 + 0.5 - 0.5 = 10$.
> *   Se $\varepsilon_2 = 0.5$ e $\varepsilon_3 = 1.5$, ent√£o $Y_3 = 10 + 1.5 + 0.5(0.5) = 10 + 1.5 + 0.25 = 11.75$.
>
> Este exemplo mostra como o valor atual da s√©rie temporal ($Y_t$) √© afetado tanto pelo ru√≠do branco atual ($\varepsilon_t$) quanto pelo ru√≠do branco do per√≠odo anterior ($\varepsilon_{t-1}$), ponderado pelo coeficiente $\theta$.

**Expectativa e Vari√¢ncia**

A esperan√ßa matem√°tica do processo MA(1) √© dada por:

$$E[Y_t] = E[\mu + \varepsilon_t + \theta\varepsilon_{t-1}] = \mu + E[\varepsilon_t] + \theta E[\varepsilon_{t-1}] = \mu$$

j√° que $E[\varepsilon_t] = E[\varepsilon_{t-1}] = 0$ [^3.3.2].

> üí° **Exemplo Num√©rico:**
>
> Usando o processo do exemplo anterior, $Y_t = 10 + \varepsilon_t + 0.5\varepsilon_{t-1}$, a expectativa √© simplesmente:
>
> $$E[Y_t] = E[10 + \varepsilon_t + 0.5\varepsilon_{t-1}] = 10 + E[\varepsilon_t] + 0.5E[\varepsilon_{t-1}] = 10 + 0 + 0 = 10$$
>
> Isso confirma que a m√©dia do processo √© igual ao valor de $\mu$, que √© 10 neste caso.

A vari√¢ncia do processo MA(1) √© dada por:

$$Var(Y_t) = E[(Y_t - \mu)^2] = E[(\varepsilon_t + \theta\varepsilon_{t-1})^2] = E[\varepsilon_t^2 + 2\theta\varepsilon_t\varepsilon_{t-1} + \theta^2\varepsilon_{t-1}^2]$$

Como $\varepsilon_t$ e $\varepsilon_{t-1}$ n√£o s√£o correlacionados, $E[\varepsilon_t\varepsilon_{t-1}] = 0$. Assim,

$$Var(Y_t) = E[\varepsilon_t^2] + \theta^2 E[\varepsilon_{t-1}^2] = \sigma^2 + \theta^2\sigma^2 = (1 + \theta^2)\sigma^2$$ [^3.3.3]

> üí° **Exemplo Num√©rico:**
>
> Novamente, com $Y_t = 10 + \varepsilon_t + 0.5\varepsilon_{t-1}$ e $\sigma^2 = 1$:
>
> $$Var(Y_t) = (1 + 0.5^2)(1) = 1 + 0.25 = 1.25$$
>
> Isso significa que a variabilidade dos valores de $Y_t$ em torno de sua m√©dia (10) √© 1.25.

**Autocovari√¢ncia**

A fun√ß√£o de autocovari√¢ncia ($\gamma_j$) mede a covari√¢ncia entre $Y_t$ e $Y_{t-j}$. Para um processo MA(1):

*   Para *j* = 1:

    $$\gamma_1 = E[(Y_t - \mu)(Y_{t-1} - \mu)] = E[(\varepsilon_t + \theta\varepsilon_{t-1})(\varepsilon_{t-1} + \theta\varepsilon_{t-2})] = E[\varepsilon_t\varepsilon_{t-1} + \theta\varepsilon_{t-1}^2 + \theta\varepsilon_t\varepsilon_{t-2} + \theta^2\varepsilon_{t-1}\varepsilon_{t-2}]$$

    Como $E[\varepsilon_t\varepsilon_{t-1}] = E[\varepsilon_t\varepsilon_{t-2}] = E[\varepsilon_{t-1}\varepsilon_{t-2}] = 0$ e $E[\varepsilon_{t-1}^2] = \sigma^2$, temos:

    $$\gamma_1 = \theta\sigma^2$$ [^3.3.4]

    > üí° **Exemplo Num√©rico:**
    >
    > Usando o exemplo anterior, com $\theta = 0.5$ e $\sigma^2 = 1$:
    >
    > $$\gamma_1 = 0.5 \times 1 = 0.5$$
    >
    > Isso indica que a covari√¢ncia entre $Y_t$ e $Y_{t-1}$ √© 0.5.

*   Para *j* > 1:

    $$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\varepsilon_t + \theta\varepsilon_{t-1})(\varepsilon_{t-j} + \theta\varepsilon_{t-j-1})] = 0$$ [^3.3.5]

    Para esclarecer a raz√£o pela qual $\gamma_j = 0$ para $j > 1$, apresentamos a seguinte prova:

    *Prova:*
    Queremos demonstrar que para $j > 1$, $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = 0$.

    I.  Substitu√≠mos a defini√ß√£o de $Y_t$:
    $$E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\varepsilon_t + \theta\varepsilon_{t-1})(\varepsilon_{t-j} + \theta\varepsilon_{t-j-1})]$$

    II. Expandimos o produto:
    $$E[\varepsilon_t\varepsilon_{t-j} + \theta\varepsilon_{t-1}\varepsilon_{t-j} + \theta\varepsilon_t\varepsilon_{t-j-1} + \theta^2\varepsilon_{t-1}\varepsilon_{t-j-1}]$$

    III. Como $\varepsilon_t$ √© um processo de ru√≠do branco, $E[\varepsilon_i\varepsilon_k] = 0$ para $i \neq k$.  Como $j > 1$, todos os termos na expans√£o acima envolvem o produto de termos de erro em diferentes per√≠odos de tempo:
        * $E[\varepsilon_t\varepsilon_{t-j}] = 0$ porque $t \neq t-j$.
        * $E[\varepsilon_{t-1}\varepsilon_{t-j}] = 0$ porque $t-1 \neq t-j$.
        * $E[\varepsilon_t\varepsilon_{t-j-1}] = 0$ porque $t \neq t-j-1$.
        * $E[\varepsilon_{t-1}\varepsilon_{t-j-1}] = 0$ porque $t-1 \neq t-j-1$.

    IV. Portanto,
    $$E[(Y_t - \mu)(Y_{t-j} - \mu)] = 0 + 0 + 0 + 0 = 0$$

    Assim, $\gamma_j = 0$ para $j > 1$. $\blacksquare$

**Autocorrela√ß√£o**

A fun√ß√£o de autocorrela√ß√£o ($\rho_j$) √© a autocovari√¢ncia normalizada pela vari√¢ncia:

$$\rho_j = \frac{\gamma_j}{\gamma_0}$$

onde $\gamma_0 = Var(Y_t)$. Para o processo MA(1), as autocorrela√ß√µes s√£o:

*   $\rho_0 = 1$ (por defini√ß√£o)
*   $\rho_1 = \frac{\theta\sigma^2}{(1 + \theta^2)\sigma^2} = \frac{\theta}{1 + \theta^2}$ [^3.3.7]
*   $\rho_j = 0$ para *j* > 1

> üí° **Exemplo Num√©rico:**
>
> Com $\theta = 0.5$, temos:
>
> $$\rho_1 = \frac{0.5}{1 + 0.5^2} = \frac{0.5}{1.25} = 0.4$$
>
> Todas as outras autocorrela√ß√µes (para lag > 1) s√£o zero.  Isso significa que, no processo MA(1), a correla√ß√£o desaparece ap√≥s o primeiro lag.

**Estacionariedade**

Um processo MA(1) √© sempre **fracamente estacion√°rio** (ou covariance-stationary), pois sua m√©dia e autocovari√¢ncia n√£o dependem do tempo *t* [^3.3.5]. Isso significa que as propriedades estat√≠sticas do processo s√£o constantes ao longo do tempo. Al√©m disso, um processo MA(1) com ru√≠do branco gaussiano √© **erg√≥dico** para todos os momentos, satisfazendo a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j|<\infty$ [^3.1.15, 3.3.5].

Para demonstrar que o processo MA(1) √© erg√≥dico, podemos seguir a seguinte prova:

*Prova:*
Para um processo ser erg√≥dico, a condi√ß√£o $\sum_{j=0}^{\infty} |\gamma_j|<\infty$ deve ser satisfeita.

I. No caso do processo MA(1), sabemos que:
   * $\gamma_0 = (1 + \theta^2)\sigma^2$
   * $\gamma_1 = \gamma_{-1} = \theta\sigma^2$
   * $\gamma_j = 0$ para $|j| > 1$

II. Portanto, a soma dos valores absolutos das autocovari√¢ncias √©:
   $$\sum_{j=-\infty}^{\infty} |\gamma_j| = |\gamma_{-1}| + |\gamma_0| + |\gamma_1| = |\theta\sigma^2| + |(1 + \theta^2)\sigma^2| + |\theta\sigma^2|$$

III. Simplificando a express√£o, obtemos:
   $$\sum_{j=-\infty}^{\infty} |\gamma_j| = 2|\theta|\sigma^2 + (1 + \theta^2)\sigma^2 = (1 + 2|\theta| + \theta^2)\sigma^2$$

IV. Como $\sigma^2$ √© uma constante (a vari√¢ncia do ru√≠do branco) e $\theta$ √© um par√¢metro do modelo, a soma $\sum_{j=-\infty}^{\infty} |\gamma_j|$ √© finita.

V. Portanto, o processo MA(1) √© erg√≥dico para todos os momentos. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Considerando $\theta = 0.5$ e $\sigma^2 = 1$:
>
> $$\sum_{j=-\infty}^{\infty} |\gamma_j| = (1 + 2|0.5| + 0.5^2)(1) = 1 + 1 + 0.25 = 2.25$$
>
> Como 2.25 √© um valor finito, o processo MA(1) com $\theta = 0.5$ e $\sigma^2 = 1$ √© erg√≥dico.

**Invertibilidade**

Um conceito importante associado a processos de m√©dia m√≥vel √© a *invertibilidade*. Um processo MA(1) √© considerado invert√≠vel se puder ser expresso como um processo autorregressivo de ordem infinita (AR($\infty$)). A condi√ß√£o de invertibilidade para o MA(1) √© $|\theta| < 1$ [^3.7.2]. Quando essa condi√ß√£o √© satisfeita, o processo MA(1) pode ser reescrito como uma combina√ß√£o linear infinita de seus valores passados.

Para complementar a discuss√£o sobre invertibilidade, podemos demonstrar como o processo MA(1) se transforma em um AR($\infty$) quando a condi√ß√£o $|\theta| < 1$ √© satisfeita.

**Teorema 1**
Se $|\theta| < 1$, o processo MA(1) dado por $Y_t = \mu + \varepsilon_t + \theta\varepsilon_{t-1}$ pode ser expresso como um processo AR($\infty$).

*Demonstra√ß√£o:*
A partir da defini√ß√£o do processo MA(1), podemos isolar o termo de erro:
$$\varepsilon_t = Y_t - \mu - \theta\varepsilon_{t-1}$$
Defasando a equa√ß√£o, temos:
$$\varepsilon_{t-1} = Y_{t-1} - \mu - \theta\varepsilon_{t-2}$$
Substituindo recursivamente, obtemos:
$$\varepsilon_t = (Y_t - \mu) - \theta(Y_{t-1} - \mu) + \theta^2\varepsilon_{t-2}$$
Continuando a substitui√ß√£o, chegamos a:
$$\varepsilon_t = \sum_{i=0}^{\infty} (-\theta)^i (Y_{t-i} - \mu)$$
Substituindo esta express√£o de $\varepsilon_t$ na equa√ß√£o original do MA(1):
$$Y_t = \mu + \sum_{i=0}^{\infty} (-\theta)^i (Y_{t-i} - \mu) + \theta \sum_{i=0}^{\infty} (-\theta)^i (Y_{t-1-i} - \mu)$$
$$Y_t = \mu + \sum_{i=0}^{\infty} (-\theta)^i (Y_{t-i} - \mu) + \sum_{i=1}^{\infty} (-\theta)^i (Y_{t-i} - \mu)$$
$$Y_t = \mu + (Y_t - \mu) + \sum_{i=1}^{\infty} (-\theta)^i (Y_{t-i} - \mu) + \sum_{i=1}^{\infty} (-\theta)^i (Y_{t-i} - \mu)$$
Rearranjando os termos, obtemos a representa√ß√£o AR($\infty$):
$$Y_t = \sum_{i=1}^{\infty} \phi_i Y_{t-i} + a + \varepsilon_t$$
onde $\phi_i = -(-\theta)^i$ e $a$ √© um termo constante relacionado a $\mu$. Esta representa√ß√£o √© v√°lida apenas se $|\theta| < 1$, garantindo a converg√™ncia da s√©rie.

> üí° **Exemplo Num√©rico:**
>
> Seja $\theta = 0.5$ e $\mu = 10$. Ent√£o, $\phi_i = -(-0.5)^i$.
>
> Para os primeiros termos:
>
> *   $\phi_1 = -(-0.5)^1 = 0.5$
> *   $\phi_2 = -(-0.5)^2 = -0.25$
> *   $\phi_3 = -(-0.5)^3 = 0.125$
> *   $\phi_4 = -(-0.5)^4 = -0.0625$
>
> A representa√ß√£o AR($\infty$) seria:
>
> $$Y_t = 0.5Y_{t-1} - 0.25Y_{t-2} + 0.125Y_{t-3} - 0.0625Y_{t-4} + \ldots + a + \varepsilon_t$$
>
> Como $|\theta| = |0.5| < 1$, a s√©rie converge e a representa√ß√£o AR($\infty$) √© v√°lida.

Para mostrar a necessidade da condi√ß√£o $|\theta|<1$ para a converg√™ncia, podemos analisar a seguinte prova:

*Prova:*
Para que a representa√ß√£o AR($\infty$) seja v√°lida, a s√©rie $\sum_{i=0}^{\infty} (-\theta)^i$ deve convergir.

I. Consideremos a s√©rie geom√©trica $\sum_{i=0}^{\infty} x^i$.  Sabemos que esta s√©rie converge para $\frac{1}{1-x}$ se $|x| < 1$.

II. No nosso caso, $x = -\theta$. Portanto, a s√©rie $\sum_{i=0}^{\infty} (-\theta)^i$ converge se $|-\theta| < 1$, o que √© equivalente a $|\theta| < 1$.

III. Se $|\theta| \geq 1$, a s√©rie diverge, e a representa√ß√£o AR($\infty$) n√£o √© v√°lida. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Se $\theta = 2$, ent√£o a s√©rie $\sum_{i=0}^{\infty} (-2)^i = 1 - 2 + 4 - 8 + 16 - \ldots$ diverge, demonstrando que a condi√ß√£o $|\theta| < 1$ √© necess√°ria para a converg√™ncia e, portanto, para a invertibilidade do processo MA(1).

**Representa√ß√µes N√£o-√önicas e Invertibilidade**

√â importante notar que, para um dado processo MA(1), existem duas representa√ß√µes poss√≠veis, uma invert√≠vel ($|\theta| < 1$) e outra n√£o invert√≠vel ($|\theta| > 1$), que geram as mesmas estat√≠sticas de primeira e segunda ordem [^3.7.4, 3.7.5]. Apesar de ambas as representa√ß√µes serem estatisticamente equivalentes, a representa√ß√£o invert√≠vel √© geralmente preferida para fins de modelagem e previs√£o, pois permite uma interpreta√ß√£o causal e facilita a estima√ß√£o de par√¢metros e a constru√ß√£o de previs√µes [^3.7.1, 3.7.11].

Para ilustrar a n√£o-unicidade, considere o processo MA(1) $Y_t = \varepsilon_t + \theta \varepsilon_{t-1}$.  Existe outro processo MA(1) com par√¢metro $\theta^* = 1/\theta$ que gera a mesma fun√ß√£o de autocorrela√ß√£o.

**Lema 1.1** Se $Y_t = \varepsilon_t + \theta \varepsilon_{t-1}$ √© um processo MA(1) com $|\theta| \neq 1$, ent√£o $Y_t^* = \varepsilon_t + (1/\theta) \varepsilon_{t-1}$ tem a mesma fun√ß√£o de autocorrela√ß√£o que $Y_t$.

*Demonstra√ß√£o:*
Como j√° demonstrado, $\rho_1 = \frac{\theta}{1 + \theta^2}$ para $Y_t$. Para $Y_t^*$, temos $\rho_1^* = \frac{1/\theta}{1 + (1/\theta)^2} = \frac{1/\theta}{1 + 1/\theta^2} = \frac{1/\theta}{(\theta^2+1)/\theta^2} = \frac{\theta^2}{\theta(\theta^2+1)} = \frac{\theta}{1 + \theta^2}$. Portanto, $\rho_1 = \rho_1^*$.  Como $\rho_j = 0$ para $j > 1$ em ambos os processos, a fun√ß√£o de autocorrela√ß√£o √© a mesma.

Este lema demonstra que, dado um processo MA(1), existe um segundo processo MA(1) com o par√¢metro inverso que resulta na mesma estrutura de autocorrela√ß√£o. Isso refor√ßa a import√¢ncia da condi√ß√£o de invertibilidade para garantir a unicidade e a interpretabilidade do modelo.

> üí° **Exemplo Num√©rico:**
>
> Se $Y_t = \varepsilon_t + 0.5\varepsilon_{t-1}$, ent√£o $\theta = 0.5$. O processo n√£o invert√≠vel seria $Y_t^* = \varepsilon_t + (1/0.5)\varepsilon_{t-1} = \varepsilon_t + 2\varepsilon_{t-1}$.
>
> Ambos os processos t√™m a mesma autocorrela√ß√£o no lag 1:
>
> *   Para $Y_t$: $\rho_1 = \frac{0.5}{1 + 0.5^2} = \frac{0.5}{1.25} = 0.4$
> *   Para $Y_t^*$: $\rho_1 = \frac{2}{1 + 2^2} = \frac{2}{5} = 0.4$
>
> No entanto, $Y_t$ √© invert√≠vel porque $|\theta| = |0.5| < 1$, enquanto $Y_t^*$ n√£o √©, porque $|\theta| = |2| > 1$.

Al√©m da representa√ß√£o AR($\infty$), podemos analisar a fun√ß√£o geradora de autocovari√¢ncias do processo MA(1).

**Defini√ß√£o:** A fun√ß√£o geradora de autocovari√¢ncias (FGAC) de um processo estacion√°rio $\{Y_t\}$ √© definida como:

$$g(z) = \sum_{j=-\infty}^{\infty} \gamma_j z^j$$

onde $z$ √© uma vari√°vel complexa e $\gamma_j$ √© a fun√ß√£o de autocovari√¢ncia do processo.

**Teorema 2:** A FGAC do processo MA(1) $Y_t = \mu + \varepsilon_t + \theta\varepsilon_{t-1}$ √© dada por:

$$g(z) = \sigma^2(1 + \theta z)(1 + \theta z^{-1})$$

*Demonstra√ß√£o:*
Para o processo MA(1), temos:
$\gamma_0 = (1 + \theta^2)\sigma^2$
$\gamma_1 = \gamma_{-1} = \theta\sigma^2$
$\gamma_j = 0$ para $|j| > 1$

Portanto, a FGAC √©:
$$g(z) = \sum_{j=-\infty}^{\infty} \gamma_j z^j = \gamma_{-1}z^{-1} + \gamma_0 + \gamma_1 z$$
$$g(z) = \theta\sigma^2 z^{-1} + (1 + \theta^2)\sigma^2 + \theta\sigma^2 z$$
$$g(z) = \sigma^2(\theta z^{-1} + 1 + \theta^2 + \theta z)$$
$$g(z) = \sigma^2(1 + \theta(z + z^{-1}) + \theta^2)$$
Agora, vamos manipular a express√£o desejada:
$$\sigma^2(1 + \theta z)(1 + \theta z^{-1}) = \sigma^2(1 + \theta z^{-1} + \theta z + \theta^2 z z^{-1})$$
$$= \sigma^2(1 + \theta z^{-1} + \theta z + \theta^2) = \sigma^2(\theta z^{-1} + 1 + \theta^2 + \theta z)$$
Assim,
$$g(z) = \sigma^2(1 + \theta z)(1 + \theta z^{-1})$$

A FGAC fornece uma representa√ß√£o alternativa da estrutura de autocovari√¢ncia do processo MA(1) e pode ser √∫til em algumas aplica√ß√µes te√≥ricas e pr√°ticas.

> üí° **Exemplo Num√©rico:**
>
> Seja $\theta = 0.5$ e $\sigma^2 = 1$. Ent√£o a FGAC √©:
>
> $$g(z) = 1(1 + 0.5z)(1 + 0.5z^{-1}) = (1 + 0.5z)(1 + \frac{0.5}{z}) = 1 + \frac{0.5}{z} + 0.5z + 0.25$$
>
> $$g(z) = 1.25 + 0.5(z + z^{-1})$$
>
> Esta fun√ß√£o descreve a autocovari√¢ncia do processo em termos da vari√°vel complexa $z$.

### Conclus√£o

O processo MA(1) oferece uma estrutura fundamental para modelar depend√™ncias temporais em s√©ries temporais. Sua simplicidade anal√≠tica permite uma compreens√£o clara de seus momentos estat√≠sticos e propriedades de estacionariedade e invertibilidade. Embora simples, o MA(1) serve como um bloco de constru√ß√£o essencial para modelos de s√©ries temporais mais complexos, como os modelos ARMA e ARIMA. A an√°lise da fun√ß√£o de autocorrela√ß√£o (ACF) de um processo MA(1) √© uma ferramenta crucial para identificar e estimar os par√¢metros do modelo, fornecendo *insights* valiosos sobre a estrutura temporal dos dados. O conceito de invertibilidade introduzido aqui √© de extrema import√¢ncia para garantir a interpretabilidade e a estabilidade das previs√µes geradas a partir de modelos de m√©dia m√≥vel.

### Refer√™ncias
[^3.1.6]: ...
[^3.1.7]: ...
[^3.1.15]: ...
[^3.2.1]: ...
[^3.2.2]: ...
[^3.2.3]: ...
[^3.3.1]: ...
[^3.3.2]: ...
[^3.3.3]: ...
[^3.3.4]: ...
[^3.3.5]: ...
[^3.3.7]: ...
[^3.7.1]: ...
[^3.7.2]: ...
[^3.7.4]: ...
[^3.7.5]: ...
[^3.7.11]: ...
[^47]: ... <!-- Refer√™ncia ao t√≥pico anterior, que deve ter introduzido White Noise -->
<!-- END -->