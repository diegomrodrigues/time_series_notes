## An√°lise Avan√ßada de Processos de M√©dia M√≥vel de Primeira Ordem (MA(1))

### Introdu√ß√£o
Em continuidade ao estudo de processos estoc√°sticos estacion√°rios, este cap√≠tulo se dedica a uma an√°lise aprofundada dos processos de *M√©dia M√≥vel de Primeira Ordem*, denotados MA(1). Como vimos anteriormente [^47], o conceito de **White Noise** √© fundamental para a constru√ß√£o de modelos de s√©ries temporais. Expandindo este conceito, introduzimos agora um dos modelos mais b√°sicos, por√©m importantes, na an√°lise de s√©ries temporais: o processo MA(1). Este processo, definido como uma combina√ß√£o linear do termo de erro corrente e do termo de erro defasado em um per√≠odo, oferece uma maneira simples, mas eficaz, de modelar depend√™ncias temporais em dados sequenciais.

### Conceitos Fundamentais

Um processo MA(1) √© definido pela seguinte equa√ß√£o:

$$Y_t = \mu + \varepsilon_t + \theta\varepsilon_{t-1}$$

onde:
*   $Y_t$ representa o valor da s√©rie temporal no instante *t*.
*   $\mu$ √© a m√©dia constante do processo [^3.3.1]. √â importante notar que $\mu$ pode ser uma fun√ß√£o do tempo $t$, permitindo uma maior generalidade [^3.1.6, 3.1.7].
*   $\varepsilon_t$ √© um processo de *White Noise* no instante *t*, caracterizado por m√©dia zero, vari√¢ncia constante ($\sigma^2$) e aus√™ncia de autocorrela√ß√£o [^3.2.1, 3.2.2, 3.2.3].
*   $\theta$ √© o coeficiente do termo de m√©dia m√≥vel, determinando o peso da inova√ß√£o passada ($\varepsilon_{t-1}$) no valor atual da s√©rie [^3.3.1].

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um processo MA(1) definido como:
>
> $$Y_t = 10 + \varepsilon_t + 0.5\varepsilon_{t-1}$$
>
> Aqui, $\mu = 10$ e $\theta = 0.5$.  Se $\varepsilon_t$ e $\varepsilon_{t-1}$ s√£o ru√≠dos brancos com m√©dia zero e desvio padr√£o 1 (portanto, vari√¢ncia $\sigma^2 = 1$), podemos simular alguns valores de $Y_t$:
>
> *   Se $\varepsilon_0 = 2$ e $\varepsilon_1 = -1$, ent√£o $Y_1 = 10 + (-1) + 0.5(2) = 10 - 1 + 1 = 10$.
> *   Se $\varepsilon_1 = -1$ e $\varepsilon_2 = 0.5$, ent√£o $Y_2 = 10 + 0.5 + 0.5(-1) = 10 + 0.5 - 0.5 = 10$.
> *   Se $\varepsilon_2 = 0.5$ e $\varepsilon_3 = 1.5$, ent√£o $Y_3 = 10 + 1.5 + 0.5(0.5) = 10 + 1.5 + 0.25 = 11.75$.
>
> Este exemplo mostra como o valor atual da s√©rie temporal ($Y_t$) √© afetado tanto pelo ru√≠do branco atual ($\varepsilon_t$) quanto pelo ru√≠do branco do per√≠odo anterior ($\varepsilon_{t-1}$), ponderado pelo coeficiente $\theta$.

**Expectativa e Vari√¢ncia**

A esperan√ßa matem√°tica do processo MA(1) √© dada por:

$$E[Y_t] = E[\mu + \varepsilon_t + \theta\varepsilon_{t-1}] = \mu + E[\varepsilon_t] + \theta E[\varepsilon_{t-1}] = \mu$$

j√° que $E[\varepsilon_t] = E[\varepsilon_{t-1}] = 0$ [^3.3.2].

*Prova:*
Queremos mostrar que $E[Y_t] = \mu$.

I.  Substitu√≠mos a defini√ß√£o de $Y_t$:
    $$E[Y_t] = E[\mu + \varepsilon_t + \theta\varepsilon_{t-1}]$$

II. Usando a linearidade da expectativa:
    $$E[Y_t] = E[\mu] + E[\varepsilon_t] + \theta E[\varepsilon_{t-1}]$$

III. Como $\mu$ √© constante, $E[\mu] = \mu$. Dado que $\varepsilon_t$ √© um processo de ru√≠do branco, $E[\varepsilon_t] = 0$ e $E[\varepsilon_{t-1}] = 0$:
    $$E[Y_t] = \mu + 0 + \theta \cdot 0$$

IV. Portanto:
    $$E[Y_t] = \mu$$
    $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Usando o processo do exemplo anterior, $Y_t = 10 + \varepsilon_t + 0.5\varepsilon_{t-1}$, a expectativa √© simplesmente:
>
> $$E[Y_t] = E[10 + \varepsilon_t + 0.5\varepsilon_{t-1}] = 10 + E[\varepsilon_t] + 0.5E[\varepsilon_{t-1}] = 10 + 0 + 0 = 10$$
>
> Isso confirma que a m√©dia do processo √© igual ao valor de $\mu$, que √© 10 neste caso.

A vari√¢ncia do processo MA(1) √© dada por:

$$Var(Y_t) = E[(Y_t - \mu)^2] = E[(\varepsilon_t + \theta\varepsilon_{t-1})^2] = E[\varepsilon_t^2 + 2\theta\varepsilon_t\varepsilon_{t-1} + \theta^2\varepsilon_{t-1}^2]$$

Como $\varepsilon_t$ e $\varepsilon_{t-1}$ n√£o s√£o correlacionados, $E[\varepsilon_t\varepsilon_{t-1}] = 0$. Assim,

$$Var(Y_t) = E[\varepsilon_t^2] + \theta^2 E[\varepsilon_{t-1}^2] = \sigma^2 + \theta^2\sigma^2 = (1 + \theta^2)\sigma^2$$ [^3.3.3]

*Prova:*
Queremos provar que $Var(Y_t) = (1 + \theta^2)\sigma^2$.

I. Usamos a defini√ß√£o de vari√¢ncia:
    $$Var(Y_t) = E[(Y_t - E[Y_t])^2]$$

II. Substitu√≠mos $Y_t$ e $E[Y_t] = \mu$:
    $$Var(Y_t) = E[(\mu + \varepsilon_t + \theta\varepsilon_{t-1} - \mu)^2] = E[(\varepsilon_t + \theta\varepsilon_{t-1})^2]$$

III. Expandimos o quadrado:
    $$Var(Y_t) = E[\varepsilon_t^2 + 2\theta\varepsilon_t\varepsilon_{t-1} + \theta^2\varepsilon_{t-1}^2]$$

IV. Usando a linearidade da expectativa:
    $$Var(Y_t) = E[\varepsilon_t^2] + 2\theta E[\varepsilon_t\varepsilon_{t-1}] + \theta^2 E[\varepsilon_{t-1}^2]$$

V. Dado que $\varepsilon_t$ √© um ru√≠do branco, $E[\varepsilon_t^2] = \sigma^2$ e $E[\varepsilon_{t-1}^2] = \sigma^2$. Al√©m disso, como $\varepsilon_t$ e $\varepsilon_{t-1}$ s√£o n√£o correlacionados, $E[\varepsilon_t\varepsilon_{t-1}] = 0$:
    $$Var(Y_t) = \sigma^2 + 2\theta \cdot 0 + \theta^2 \sigma^2$$

VI. Portanto:
    $$Var(Y_t) = \sigma^2 + \theta^2\sigma^2 = (1 + \theta^2)\sigma^2$$
    $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Novamente, com $Y_t = 10 + \varepsilon_t + 0.5\varepsilon_{t-1}$ e $\sigma^2 = 1$:
>
> $$Var(Y_t) = (1 + 0.5^2)(1) = 1 + 0.25 = 1.25$$
>
> Isso significa que a variabilidade dos valores de $Y_t$ em torno de sua m√©dia (10) √© 1.25.
>
> üí° **Exemplo Num√©rico:**
>
> Consideremos outro processo MA(1): $Y_t = 5 + \varepsilon_t - 0.8\varepsilon_{t-1}$, onde $\varepsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2 = 4$.
>
> A vari√¢ncia deste processo √©:
>
> $$Var(Y_t) = (1 + (-0.8)^2)(4) = (1 + 0.64)(4) = 1.64 \times 4 = 6.56$$
>
> Este exemplo demonstra como um valor negativo de $\theta$ ainda contribui positivamente para a vari√¢ncia total do processo, j√° que o termo $\theta$ √© elevado ao quadrado.

**Autocovari√¢ncia**

A fun√ß√£o de autocovari√¢ncia ($\gamma_j$) mede a covari√¢ncia entre $Y_t$ e $Y_{t-j}$. Para um processo MA(1):

*   Para *j* = 1:

    $$\gamma_1 = E[(Y_t - \mu)(Y_{t-1} - \mu)] = E[(\varepsilon_t + \theta\varepsilon_{t-1})(\varepsilon_{t-1} + \theta\varepsilon_{t-2})] = E[\varepsilon_t\varepsilon_{t-1} + \theta\varepsilon_{t-1}^2 + \theta\varepsilon_t\varepsilon_{t-2} + \theta^2\varepsilon_{t-1}\varepsilon_{t-2}]$$

    Como $E[\varepsilon_t\varepsilon_{t-1}] = E[\varepsilon_t\varepsilon_{t-2}] = E[\varepsilon_{t-1}\varepsilon_{t-2}] = 0$ e $E[\varepsilon_{t-1}^2] = \sigma^2$, temos:

    $$\gamma_1 = \theta\sigma^2$$ [^3.3.4]

    *Prova:*
    Queremos mostrar que $\gamma_1 = \theta\sigma^2$.

    I.  Substitu√≠mos a defini√ß√£o de $\gamma_1$ e $Y_t$:
        $$\gamma_1 = E[(Y_t - \mu)(Y_{t-1} - \mu)] = E[(\varepsilon_t + \theta\varepsilon_{t-1})(\varepsilon_{t-1} + \theta\varepsilon_{t-2})]$$

    II. Expandimos o produto:
        $$E[\varepsilon_t\varepsilon_{t-1} + \theta\varepsilon_{t-1}^2 + \theta\varepsilon_t\varepsilon_{t-2} + \theta^2\varepsilon_{t-1}\varepsilon_{t-2}]$$

    III. Usando a linearidade da expectativa:
         $$E[\varepsilon_t\varepsilon_{t-1}] + \theta E[\varepsilon_{t-1}^2] + \theta E[\varepsilon_t\varepsilon_{t-2}] + \theta^2 E[\varepsilon_{t-1}\varepsilon_{t-2}]$$

    IV.  Dado que $\varepsilon_t$ √© um processo de ru√≠do branco, $E[\varepsilon_t\varepsilon_{t-1}] = 0$, $E[\varepsilon_t\varepsilon_{t-2}] = 0$, $E[\varepsilon_{t-1}\varepsilon_{t-2}] = 0$ e $E[\varepsilon_{t-1}^2] = \sigma^2$:
         $$0 + \theta \sigma^2 + \theta \cdot 0 + \theta^2 \cdot 0$$

    V. Portanto:
        $$\gamma_1 = \theta\sigma^2$$
        $\blacksquare$

    > üí° **Exemplo Num√©rico:**
    >
    > Usando o exemplo anterior, com $\theta = 0.5$ e $\sigma^2 = 1$:
    >
    > $$\gamma_1 = 0.5 \times 1 = 0.5$$
    >
    > Isso indica que a covari√¢ncia entre $Y_t$ e $Y_{t-1}$ √© 0.5.
    >
    > üí° **Exemplo Num√©rico:**
    >
    > Se $\theta = -0.7$ e $\sigma^2 = 9$, ent√£o:
    >
    > $$\gamma_1 = -0.7 \times 9 = -6.3$$
    >
    > Neste caso, a autocovari√¢ncia no lag 1 √© negativa, indicando uma rela√ß√£o inversa entre $Y_t$ e $Y_{t-1}$.

*   Para *j* > 1:

    $$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\varepsilon_t + \theta\varepsilon_{t-1})(\varepsilon_{t-j} + \theta\varepsilon_{t-j-1})] = 0$$ [^3.3.5]

    Para esclarecer a raz√£o pela qual $\gamma_j = 0$ para $j > 1$, apresentamos a seguinte prova:

    *Prova:*
    Queremos demonstrar que para $j > 1$, $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = 0$.

    I.  Substitu√≠mos a defini√ß√£o de $Y_t$:
    $$E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\varepsilon_t + \theta\varepsilon_{t-1})(\varepsilon_{t-j} + \theta\varepsilon_{t-j-1})]$$

    II. Expandimos o produto:
    $$E[\varepsilon_t\varepsilon_{t-j} + \theta\varepsilon_{t-1}\varepsilon_{t-j} + \theta\varepsilon_t\varepsilon_{t-j-1} + \theta^2\varepsilon_{t-1}\varepsilon_{t-j-1}]$$

    III. Como $\varepsilon_t$ √© um processo de ru√≠do branco, $E[\varepsilon_i\varepsilon_k] = 0$ para $i \neq k$.  Como $j > 1$, todos os termos na expans√£o acima envolvem o produto de termos de erro em diferentes per√≠odos de tempo:
        * $E[\varepsilon_t\varepsilon_{t-j}] = 0$ porque $t \neq t-j$.
        * $E[\varepsilon_{t-1}\varepsilon_{t-j}] = 0$ porque $t-1 \neq t-j$.
        * $E[\varepsilon_t\varepsilon_{t-j-1}] = 0$ porque $t \neq t-j-1$.
        * $E[\varepsilon_{t-1}\varepsilon_{t-j-1}] = 0$ porque $t-1 \neq t-j-1$.

    IV. Portanto,
    $$E[(Y_t - \mu)(Y_{t-j} - \mu)] = 0 + 0 + 0 + 0 = 0$$

    Assim, $\gamma_j = 0$ para $j > 1$. $\blacksquare$

Al√©m da autocovari√¢ncia, √© fundamental analisar a fun√ß√£o de autocorrela√ß√£o (FAC), que fornece uma medida normalizada da depend√™ncia linear entre os valores da s√©rie em diferentes instantes de tempo.

**Autocorrela√ß√£o**

A fun√ß√£o de autocorrela√ß√£o (FAC), denotada por $\rho_j$, √© definida como a autocovari√¢ncia dividida pela vari√¢ncia:

$$\rho_j = \frac{\gamma_j}{\gamma_0} = \frac{\gamma_j}{Var(Y_t)}$$

Para um processo MA(1):

*   Para *j* = 0:
    $$\rho_0 = \frac{\gamma_0}{Var(Y_t)} = \frac{Var(Y_t)}{Var(Y_t)} = 1$$

*   Para *j* = 1:
    $$\rho_1 = \frac{\gamma_1}{Var(Y_t)} = \frac{\theta\sigma^2}{(1 + \theta^2)\sigma^2} = \frac{\theta}{1 + \theta^2}$$

*   Para *j* > 1:
    $$\rho_j = \frac{\gamma_j}{Var(Y_t)} = \frac{0}{(1 + \theta^2)\sigma^2} = 0$$

Portanto, a FAC de um processo MA(1) tem um corte abrupto ap√≥s o lag 1. Isso significa que apenas a autocorrela√ß√£o no lag 1 √© significativamente diferente de zero, enquanto as autocorrela√ß√µes nos lags subsequentes s√£o zero.

> üí° **Exemplo Num√©rico:**
>
> Usando o exemplo anterior, com $\theta = 0.5$:
>
> $$\rho_1 = \frac{0.5}{1 + 0.5^2} = \frac{0.5}{1.25} = 0.4$$
>
> Isso indica uma autocorrela√ß√£o de 0.4 entre $Y_t$ e $Y_{t-1}$. Para $j > 1$, $\rho_j = 0$.
>
> üí° **Exemplo Num√©rico:**
>
> Se $\theta = -0.8$, ent√£o:
>
> $$\rho_1 = \frac{-0.8}{1 + (-0.8)^2} = \frac{-0.8}{1 + 0.64} = \frac{-0.8}{1.64} \approx -0.4878$$
>
> Este valor negativo indica uma correla√ß√£o negativa no lag 1, o que significa que valores altos de $Y_t$ tendem a ser seguidos por valores baixos de $Y_{t+1}$, e vice-versa.
>
> Podemos visualizar a FAC (Fun√ß√£o de Autocorrela√ß√£o) usando um gr√°fico de barras:
>
> ```mermaid
> graph LR
>     subgraph FAC
>         0[0] --> 1[1.00]
>         0 --> 2[rho_1 = -0.49]
>         0 --> 3[0.00]
>         0 --> 4[0.00]
>         0 --> 5[0.00]
>         style 1 fill:#f9f,stroke:#333,stroke-width:2px
>         style 2 fill:#f9f,stroke:#333,stroke-width:2px
>     end
>     style FAC fill:#fff,stroke:#333,stroke-width:2px
> ```
>
> Este gr√°fico demonstra visualmente o "corte" abrupto na FAC ap√≥s o lag 1, caracter√≠stico dos processos MA(1).

**Invertibilidade**
A invertibilidade √© uma propriedade importante dos modelos MA. Um processo MA(1) √© dito invert√≠vel se puder ser representado como um processo AR(‚àû). A condi√ß√£o de invertibilidade para um processo MA(1) √©:

$$|\theta| < 1$$

Isto significa que o coeficiente $\theta$ deve estar dentro do intervalo (-1, 1). Se essa condi√ß√£o for satisfeita, o processo MA(1) pode ser expresso como uma representa√ß√£o autorregressiva infinita, o que facilita a previs√£o e a interpreta√ß√£o do modelo.

**Teorema 1**
Um processo MA(1) √© invert√≠vel se e somente se $|\theta| < 1$.

*Prova*
Um processo MA(1) pode ser escrito como $Y_t = \mu + \varepsilon_t + \theta \varepsilon_{t-1}$. Para verificar a invertibilidade, precisamos expressar $\varepsilon_t$ em termos de $Y_t$ e seus valores passados. Reescrevendo a equa√ß√£o, temos $\varepsilon_t = Y_t - \mu - \theta \varepsilon_{t-1}$. Podemos iterar essa express√£o para obter:

I. $\varepsilon_t = (Y_t - \mu) - \theta \varepsilon_{t-1}$

II. Substituindo $\varepsilon_{t-1}$ por sua express√£o equivalente: $\varepsilon_{t-1} = (Y_{t-1} - \mu) - \theta \varepsilon_{t-2}$:

III. $\varepsilon_t = (Y_t - \mu) - \theta [(Y_{t-1} - \mu) - \theta \varepsilon_{t-2}] = (Y_t - \mu) - \theta (Y_{t-1} - \mu) + \theta^2 \varepsilon_{t-2}$

IV. Continuando a itera√ß√£o indefinidamente, obtemos:

V. $\varepsilon_t = \sum_{i=0}^{\infty} (-\theta)^i (Y_{t-i} - \mu)$

VI. Para que esta representa√ß√£o seja v√°lida, a soma deve convergir. A condi√ß√£o para converg√™ncia de uma s√©rie geom√©trica $\sum_{i=0}^{\infty} ar^i$ √© $|r| < 1$. Neste caso, $r = -\theta$, portanto a condi√ß√£o para converg√™ncia √© $|-\theta| < 1$, que √© equivalente a $|\theta| < 1$. Portanto, o processo MA(1) √© invert√≠vel se e somente se $|\theta| < 1$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Se $\theta = 0.6$, o processo √© invert√≠vel porque $|0.6| < 1$. No entanto, se $\theta = 1.5$, o processo n√£o √© invert√≠vel porque $|1.5| > 1$.
>
> No caso em que $\theta = 0.6$, podemos expressar $\varepsilon_t$ como uma soma infinita de termos passados de $Y_t$:
>
> $$\varepsilon_t = (Y_t - \mu) - 0.6(Y_{t-1} - \mu) + 0.36(Y_{t-2} - \mu) - 0.216(Y_{t-3} - \mu) + \ldots$$
>
> Observe que os coeficientes dos termos $(Y_{t-i} - \mu)$ diminuem em magnitude √† medida que *i* aumenta, garantindo a converg√™ncia da s√©rie.

Dado que a invertibilidade √© crucial, √© interessante examinar o que acontece quando essa condi√ß√£o n√£o √© satisfeita. Podemos demonstrar que, para cada processo MA(1) n√£o invert√≠vel, existe um processo MA(1) invert√≠vel que gera a mesma fun√ß√£o de autocorrela√ß√£o.

**Teorema 1.1** Para qualquer processo MA(1) com par√¢metro $\theta$ tal que $|\theta| > 1$, existe um processo MA(1) com par√¢metro $\theta^* = 1/\theta$ tal que $|\theta^*| < 1$ e ambos os processos possuem a mesma fun√ß√£o de autocorrela√ß√£o.

*Prova*
Seja $Y_t = \varepsilon_t + \theta \varepsilon_{t-1}$ um processo MA(1) com $|\theta| > 1$. A fun√ß√£o de autocorrela√ß√£o para este processo √© dada por:

$\rho_1 = \frac{\theta}{1 + \theta^2}$

Considere agora um outro processo MA(1) $Y_t^* = \varepsilon_t + \theta^* \varepsilon_{t-1}$, onde $\theta^* = \frac{1}{\theta}$. Claramente, $|\theta^*| < 1$. A fun√ß√£o de autocorrela√ß√£o para este segundo processo √©:

I. $\rho_1^* = \frac{\theta^*}{1 + (\theta^*)^2}$

II. Substitu√≠mos $\theta^*$ por $\frac{1}{\theta}$:

III. $\rho_1^* = \frac{1/\theta}{1 + (1/\theta)^2} = \frac{1/\theta}{1 + 1/\theta^2} = \frac{1/\theta}{(\theta^2 + 1)/\theta^2} = \frac{\theta}{\theta^2 + 1}$

IV. Portanto, $\rho_1^* = \rho_1$

Como $\rho_1^* = \rho_1$, os dois processos t√™m a mesma fun√ß√£o de autocorrela√ß√£o. Portanto, para qualquer processo MA(1) n√£o invert√≠vel (i.e., $|\theta| > 1$), existe um processo MA(1) invert√≠vel (i.e., $|\theta^*| < 1$) que gera a mesma fun√ß√£o de autocorrela√ß√£o. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Considere um processo MA(1) com $\theta = 2$ (n√£o invert√≠vel). Ent√£o, $\theta^* = \frac{1}{2} = 0.5$ (invert√≠vel).
>
> A autocorrela√ß√£o no lag 1 para o processo original √©:
>
> $$\rho_1 = \frac{2}{1 + 2^2} = \frac{2}{5} = 0.4$$
>
> A autocorrela√ß√£o no lag 1 para o processo invert√≠vel √©:
>
> $$\rho_1^* = \frac{0.5}{1 + 0.5^2} = \frac{0.5}{1.25} = 0.4$$
>
> Como $\rho_1 = \rho_1^*$, ambos os processos t√™m a mesma fun√ß√£o de autocorrela√ß√£o, demonstrando o teorema.

Este resultado implica que, na pr√°tica, podemos sempre trabalhar com a vers√£o invert√≠vel de um processo MA(1) sem perda de generalidade no que diz respeito √† modelagem da depend√™ncia temporal.

### Refer√™ncias
[^3.1.6]: ...
[^3.1.7]: ...
[^3.2.1]: ...
[^3.2.2]: ...
[^3.2.3]: ...
[^3.3.1]: ...
[^3.3.2]: ...
[^3.3.3]: ...
[^3.3.4]: ...
[^3.3.5]: ...
[^47]: ...
<!-- END -->