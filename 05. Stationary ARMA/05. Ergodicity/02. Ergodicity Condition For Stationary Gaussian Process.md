## Ergodicity in Stationary Time Series

### Introdu√ß√£o

Em continuidade ao estudo de processos estoc√°sticos estacion√°rios, este cap√≠tulo aprofunda-se no conceito de **ergodicity**, uma propriedade crucial que conecta a an√°lise te√≥rica de conjuntos estat√≠sticos (ensemble averages) com a an√°lise pr√°tica de s√©ries temporais observadas. Como vimos anteriormente, processos estacion√°rios exibem propriedades estat√≠sticas que n√£o variam com o tempo. Ergodicity adiciona uma camada importante, garantindo que as m√©dias temporais calculadas a partir de uma √∫nica realiza√ß√£o da s√©rie temporal convergem para as m√©dias estat√≠sticas te√≥ricas, permitindo infer√™ncias sobre o processo subjacente a partir de uma √∫nica trajet√≥ria observada [^4].

### Conceitos Fundamentais

**Ergodicity** em s√©ries temporais refere-se √† condi√ß√£o sob a qual as m√©dias temporais convergem para as m√©dias de conjunto (ensemble averages) para um processo estacion√°rio [^4]. Mais formalmente, um processo covariance-stationary √© dito *ergodic for the mean* se a m√©dia amostral, definida como:

$$\bar{Y} = \frac{1}{T} \sum_{t=1}^{T} Y_t$$

converge em probabilidade para a esperan√ßa matem√°tica $E(Y)$ quando $T$ tende ao infinito [^4]:

$$ \text{plim}_{T \to \infty} \frac{1}{T} \sum_{t=1}^{T} Y_t = E(Y) $$

Essa converg√™ncia implica que uma √∫nica realiza√ß√£o suficientemente longa da s√©rie temporal pode representar as propriedades estat√≠sticas de todo o conjunto de poss√≠veis realiza√ß√µes do processo estoc√°stico. Em termos pr√°ticos, essa propriedade √© fundamental porque, na maioria das situa√ß√µes reais, temos acesso a apenas uma √∫nica realiza√ß√£o da s√©rie temporal.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos uma s√©rie temporal $Y_t$ gerada por um processo estacion√°rio com m√©dia $E(Y) = 5$. Se o processo √© erg√≥dico para a m√©dia, ent√£o, √† medida que o tamanho da amostra $T$ aumenta, a m√©dia amostral $\bar{Y}$ se aproximar√° de 5. Por exemplo, se observarmos $T = 100$ pontos e calcularmos $\bar{Y} = 5.2$, e depois observarmos $T = 1000$ pontos e calcularmos $\bar{Y} = 5.05$, e finalmente observarmos $T = 10000$ pontos e calcularmos $\bar{Y} = 5.01$, isso sugere que o processo √© erg√≥dico para a m√©dia, pois $\bar{Y}$ est√° convergindo para $E(Y) = 5$ √† medida que $T$ aumenta.
>
> ```python
> import numpy as np
>
> # Definindo a m√©dia populacional
> mean_population = 5
>
> # Gerando uma s√©rie temporal estacion√°ria (exemplo simples com ru√≠do branco)
> np.random.seed(42)  # Definindo uma semente para reprodutibilidade
> epsilon = np.random.normal(0, 1, 10000) # Ru√≠do branco com m√©dia 0 e desvio padr√£o 1
> Y = mean_population + epsilon
>
> # Calculando a m√©dia amostral para diferentes tamanhos de amostra
> sample_sizes = [100, 1000, 10000]
> sample_means = []
>
> for T in sample_sizes:
>     sample_means.append(np.mean(Y[:T]))
>
> # Imprimindo os resultados
> for i, T in enumerate(sample_sizes):
>     print(f"Tamanho da amostra: {T}, M√©dia amostral: {sample_means[i]:.4f}")
> ```
>
> **Interpreta√ß√£o:**
>
> Este exemplo demonstra numericamente como a m√©dia amostral se aproxima da m√©dia populacional √† medida que o tamanho da amostra aumenta, indicando ergodicity.

Para garantir a ergodicity, √© necess√°rio que a autocovari√¢ncia $\gamma_j$ decaia para zero suficientemente r√°pido √† medida que o lag $j$ aumenta [^4]. Uma condi√ß√£o suficiente para a ergodicity da m√©dia em processos covariance-stationary √© que a soma dos valores absolutos das autocovari√¢ncias seja finita [^4]:

$$\sum_{j=0}^{\infty} |\gamma_j| < \infty$$

Essa condi√ß√£o assegura que as correla√ß√µes entre observa√ß√µes distantes no tempo diminuam a um ponto em que a m√©dia amostral forne√ßa uma estimativa consistente da m√©dia populacional. Similarmente, um processo covariance-stationary √© *ergodic para os segundos momentos* se:

$$ \text{plim}_{T \to \infty} \frac{1}{T-j} \sum_{t=j+1}^{T} (Y_t - \mu)(Y_{t-j} - \mu) \xrightarrow{p} \gamma_j $$

para todo $j$ [^4]. Condi√ß√µes suficientes para ergodicity de segunda ordem ser√£o exploradas em cap√≠tulos posteriores [^4]. Em processos Gaussianos estacion√°rios, a condi√ß√£o de ergodicity para a m√©dia √© suficiente para garantir a ergodicity para todos os momentos [^4].

> üí° **Exemplo Num√©rico:**
>
> Considere uma s√©rie temporal de retornos financeiros $R_t$. Se essa s√©rie √© ergodic para os segundos momentos, isso significa que a autocovari√¢ncia amostral entre $R_t$ e $R_{t-j}$ converge para a autocovari√¢ncia te√≥rica $\gamma_j$ √† medida que o tamanho da amostra aumenta. Isso nos permite estimar e analisar a estrutura de depend√™ncia temporal da volatilidade dos retornos. Por exemplo, se calcularmos a autocovari√¢ncia amostral para $j=1$ usando uma amostra de 1000 dias e obtivermos um valor de 0.1, e depois calcularmos usando uma amostra de 10000 dias e obtivermos 0.08, isso sugere que a autocovari√¢ncia amostral est√° convergindo para a verdadeira autocovari√¢ncia $\gamma_1$.
>
> ```python
> import numpy as np
>
> # Definindo os par√¢metros (exemplo com autocovari√¢ncia simulada)
> gamma_1 = 0.08 # Autocovari√¢ncia te√≥rica no lag 1
>
> # Gerando uma s√©rie temporal com depend√™ncia de primeira ordem
> np.random.seed(42)
> T = 10000 # Tamanho da amostra
> epsilon = np.random.normal(0, 1, T) # Ru√≠do branco
> R = [epsilon[0]]
> for t in range(1, T):
>     R.append(0.2 * R[t-1] + epsilon[t]) # Depend√™ncia de primeira ordem
> R = np.array(R)
>
> # Calculando a autocovari√¢ncia amostral
> def autocovariance(x, lag):
>     n = len(x)
>     x_mean = np.mean(x)
>     sum_val = 0
>     for i in range(lag, n):
>         sum_val += (x[i] - x_mean) * (x[i-lag] - x_mean)
>     return sum_val / n
>
> sample_autocov_1 = autocovariance(R, 1)
>
> # Imprimindo os resultados
> print(f"Autocovari√¢ncia te√≥rica (lag 1): {gamma_1:.4f}")
> print(f"Autocovari√¢ncia amostral (lag 1): {sample_autocov_1:.4f}")
> ```
>
> **Interpreta√ß√£o:**
>
> Este exemplo demonstra como a autocovari√¢ncia amostral se aproxima da autocovari√¢ncia te√≥rica √† medida que o tamanho da amostra aumenta, indicando ergodicity para os segundos momentos.

A seguir, apresentamos um resultado que relaciona a condi√ß√£o de ergodicity para a m√©dia com a fun√ß√£o de densidade espectral do processo.

**Teorema 1** Um processo estacion√°rio e covariance-stationary $Y_t$ com fun√ß√£o de densidade espectral $f(\omega)$ √© erg√≥dico para a m√©dia se e somente se $f(0) = 0$, exceto possivelmente para um processo com m√©dia zero.

*Prova (Esbo√ßo):* A prova envolve o uso da representa√ß√£o espectral do processo $Y_t$ e a rela√ß√£o entre a fun√ß√£o de autocovari√¢ncia e a densidade espectral. A condi√ß√£o $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$ garante que a densidade espectral √© bem definida e finita. A converg√™ncia da m√©dia amostral para a m√©dia populacional est√° relacionada ao comportamento da densidade espectral na frequ√™ncia zero. Se $f(0) = 0$, ent√£o a vari√¢ncia da m√©dia amostral tende a zero √† medida que o tamanho da amostra aumenta, implicando ergodicity.

**Corol√°rio 1.1** Se um processo estacion√°rio covariance-stationary tem uma fun√ß√£o de densidade espectral cont√≠nua e limitada, e $f(0) > 0$, ent√£o o processo n√£o √© erg√≥dico para a m√©dia.

**Lema 1.1** Para um processo estacion√°rio e covariance-stationary, a condi√ß√£o $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$ implica que a fun√ß√£o de densidade espectral $f(\omega)$ existe e √© cont√≠nua.

*Prova (Esbo√ßo):* A fun√ß√£o de densidade espectral √© definida como a transformada de Fourier da fun√ß√£o de autocovari√¢ncia: $f(\omega) = \frac{1}{2\pi} \sum_{j=-\infty}^{\infty} \gamma_j e^{-i\omega j}$. A condi√ß√£o $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$ garante a converg√™ncia absoluta da s√©rie de Fourier, o que implica que $f(\omega)$ existe e √© cont√≠nua.

*Prova Detalhada do Lema 1.1:*
Para um processo estacion√°rio e covariance-stationary, a fun√ß√£o de densidade espectral $f(\omega)$ √© definida como a transformada de Fourier da fun√ß√£o de autocovari√¢ncia $\gamma_j$:

$$f(\omega) = \frac{1}{2\pi} \sum_{j=-\infty}^{\infty} \gamma_j e^{-i\omega j}$$

Provemos que se $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, ent√£o $f(\omega)$ existe e √© cont√≠nua.

I. **Exist√™ncia de $f(\omega)$**:
    A exist√™ncia de $f(\omega)$ depende da converg√™ncia da s√©rie $\sum_{j=-\infty}^{\infty} \gamma_j e^{-i\omega j}$. Se a s√©rie converge absolutamente, ent√£o ela converge. Vamos verificar a converg√™ncia absoluta:

    $$\sum_{j=-\infty}^{\infty} |\gamma_j e^{-i\omega j}| = \sum_{j=-\infty}^{\infty} |\gamma_j| |e^{-i\omega j}|$$

    Como $|e^{-i\omega j}| = 1$ para todo $j$ e $\omega$, temos:

    $$\sum_{j=-\infty}^{\infty} |\gamma_j e^{-i\omega j}| = \sum_{j=-\infty}^{\infty} |\gamma_j|$$

    Dado que $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, a s√©rie $\sum_{j=-\infty}^{\infty} \gamma_j e^{-i\omega j}$ converge absolutamente. Portanto, $f(\omega)$ existe.

II. **Continuidade de $f(\omega)$**:
     Para mostrar que $f(\omega)$ √© cont√≠nua, precisamos mostrar que para todo $\omega_0$, $\lim_{\omega \to \omega_0} f(\omega) = f(\omega_0)$. Seja $\omega_0$ um ponto arbitr√°rio. Ent√£o:

     $$\lim_{\omega \to \omega_0} f(\omega) = \lim_{\omega \to \omega_0} \frac{1}{2\pi} \sum_{j=-\infty}^{\infty} \gamma_j e^{-i\omega j}$$

     Como a s√©rie converge absolutamente, podemos trocar o limite e a soma:

     $$\lim_{\omega \to \omega_0} f(\omega) = \frac{1}{2\pi} \sum_{j=-\infty}^{\infty} \gamma_j \lim_{\omega \to \omega_0} e^{-i\omega j}$$

     A fun√ß√£o exponencial $e^{-i\omega j}$ √© cont√≠nua em $\omega$, ent√£o:

     $$\lim_{\omega \to \omega_0} e^{-i\omega j} = e^{-i\omega_0 j}$$

     Portanto:

     $$\lim_{\omega \to \omega_0} f(\omega) = \frac{1}{2\pi} \sum_{j=-\infty}^{\infty} \gamma_j e^{-i\omega_0 j} = f(\omega_0)$$

     Isso mostra que $f(\omega)$ √© cont√≠nua em $\omega_0$. Como $\omega_0$ √© um ponto arbitr√°rio, $f(\omega)$ √© cont√≠nua para todo $\omega$.

III. **Conclus√£o**:
      Provamos que se $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, ent√£o a fun√ß√£o de densidade espectral $f(\omega)$ existe e √© cont√≠nua. ‚ñ†

Agora, podemos estabelecer uma proposi√ß√£o que fornece uma condi√ß√£o suficiente, embora n√£o necess√°ria, para a ergodicity da m√©dia, diretamente em termos das autocovari√¢ncias.

**Proposi√ß√£o 1** Seja $Y_t$ um processo estacion√°rio e covariance-stationary. Se $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, ent√£o $Y_t$ √© erg√≥dico para a m√©dia.

*Prova:* A prova segue da aplica√ß√£o da lei dos grandes n√∫meros para processos fracamente dependentes. A condi√ß√£o $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$ implica que o processo $Y_t$ √© assintoticamente n√£o correlacionado, permitindo a aplica√ß√£o da lei dos grandes n√∫meros. Assim, a m√©dia amostral converge em probabilidade para a m√©dia populacional, garantindo a ergodicity para a m√©dia.

*Prova Detalhada da Proposi√ß√£o 1:*
Seja $Y_t$ um processo estacion√°rio e covariance-stationary com autocovari√¢ncias $\gamma_j$ e m√©dia $\mu = E[Y_t]$. Queremos provar que se $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, ent√£o $Y_t$ √© erg√≥dico para a m√©dia, ou seja, $\text{plim}_{T \to \infty} \bar{Y} = \mu$, onde $\bar{Y} = \frac{1}{T} \sum_{t=1}^{T} Y_t$.

I. **Calcular a m√©dia da m√©dia amostral**:

   A m√©dia da m√©dia amostral √©:

   $$E[\bar{Y}] = E\left[\frac{1}{T} \sum_{t=1}^{T} Y_t\right] = \frac{1}{T} \sum_{t=1}^{T} E[Y_t] = \frac{1}{T} \sum_{t=1}^{T} \mu = \mu$$

   Portanto, $\bar{Y}$ √© um estimador n√£o viesado de $\mu$.

II. **Calcular a vari√¢ncia da m√©dia amostral**:

    A vari√¢ncia da m√©dia amostral √©:

    $$Var(\bar{Y}) = Var\left(\frac{1}{T} \sum_{t=1}^{T} Y_t\right) = \frac{1}{T^2} Var\left(\sum_{t=1}^{T} Y_t\right)$$

    Usando a defini√ß√£o de covari√¢ncia, temos:

    $$Var\left(\sum_{t=1}^{T} Y_t\right) = \sum_{t=1}^{T} \sum_{s=1}^{T} Cov(Y_t, Y_s)$$

    Como o processo √© estacion√°rio, $Cov(Y_t, Y_s) = \gamma_{|t-s|}$. Portanto:

    $$Var\left(\sum_{t=1}^{T} Y_t\right) = \sum_{t=1}^{T} \sum_{s=1}^{T} \gamma_{|t-s|}$$

    Reorganizando a soma, temos:

    $$Var\left(\sum_{t=1}^{T} Y_t\right) = T\gamma_0 + 2\sum_{j=1}^{T-1} (T-j)\gamma_j$$

    Dividindo por $T^2$, obtemos:

    $$Var(\bar{Y}) = \frac{1}{T}\gamma_0 + \frac{2}{T^2}\sum_{j=1}^{T-1} (T-j)\gamma_j$$
    $$Var(\bar{Y}) = \frac{1}{T}\gamma_0 + \frac{2}{T}\sum_{j=1}^{T-1} \gamma_j - \frac{2}{T^2}\sum_{j=1}^{T-1} j\gamma_j$$

III. **Mostrar que a vari√¢ncia tende a zero quando $T \to \infty$**:

     Queremos mostrar que $\lim_{T \to \infty} Var(\bar{Y}) = 0$. Dado que $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, temos que $\sum_{j=-\infty}^{\infty} j|\gamma_j|$ tamb√©m √© finito, pois a converg√™ncia absoluta da s√©rie $\sum_{j=-\infty}^{\infty} |\gamma_j|$ implica que $\gamma_j$ decai para zero mais r√°pido do que $1/j$, garantindo que a s√©rie ponderada $\sum_{j=-\infty}^{\infty} j|\gamma_j|$ tamb√©m converge.
    Assim, $\lim_{T\to\infty} \frac{1}{T}\sum_{j=1}^{T-1} \gamma_j = 0$ e $\lim_{T\to\infty} \frac{1}{T^2}\sum_{j=1}^{T-1} j\gamma_j = 0$.

     Portanto:

     $$\lim_{T \to \infty} Var(\bar{Y}) = \lim_{T \to \infty} \left(\frac{1}{T}\gamma_0 + \frac{2}{T}\sum_{j=1}^{T-1} \gamma_j - \frac{2}{T^2}\sum_{j=1}^{T-1} j\gamma_j\right) = 0$$

IV. **Aplicar a desigualdade de Chebyshev**:

    A desigualdade de Chebyshev estabelece que para qualquer $\epsilon > 0$:

    $$P(|\bar{Y} - \mu| > \epsilon) \leq \frac{Var(\bar{Y})}{\epsilon^2}$$

    Como $\lim_{T \to \infty} Var(\bar{Y}) = 0$, temos:

    $$\lim_{T \to \infty} P(|\bar{Y} - \mu| > \epsilon) = 0$$

    Isso significa que $\bar{Y}$ converge em probabilidade para $\mu$, ou seja:

    $$\text{plim}_{T \to \infty} \bar{Y} = \mu$$

V. **Conclus√£o**:

     Provamos que se $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, ent√£o $\text{plim}_{T \to \infty} \bar{Y} = \mu$, o que significa que $Y_t$ √© erg√≥dico para a m√©dia. ‚ñ†

**Exemplo de um processo estacion√°rio n√£o-erg√≥dico:**
Considere um processo definido como [^4]:

$$Y_t^{(i)} = \mu^{(i)} + \epsilon_t$$

onde $\{\epsilon_t\}$ √© um processo de ru√≠do branco Gaussiano com m√©dia zero e vari√¢ncia $\sigma^2$, independente de $\mu^{(i)}$, e $\mu^{(i)}$ √© gerado por uma distribui√ß√£o $N(0, \lambda^2)$. Neste caso, a m√©dia √© $E(Y_t) = 0$, a autocovari√¢ncia no lag zero √© $\gamma_0 = \lambda^2 + \sigma^2$, e as autocovari√¢ncias em todos os outros lags s√£o $\gamma_j = \lambda^2$ para $j \neq 0$ [^4]. Embora este processo seja covariance-stationary, ele n√£o √© erg√≥dico, pois a m√©dia amostral converge para $\mu^{(i)}$ em vez de convergir para zero [^4]. Assim, a m√©dia temporal n√£o converge para a m√©dia do conjunto, violando a condi√ß√£o de ergodicity.

> üí° **Exemplo Num√©rico:**
>
> Suponha que $\lambda^2 = 1$ e $\sigma^2 = 1$. Ent√£o, $\gamma_0 = 2$ e $\gamma_j = 1$ para $j \neq 0$. A m√©dia amostral de uma √∫nica realiza√ß√£o deste processo converge para um valor espec√≠fico de $\mu^{(i)}$ gerado a partir de $N(0, 1)$, e n√£o para a m√©dia do conjunto, que √© 0.
>
> ```python
> import numpy as np
>
> # Definindo os par√¢metros
> lambda_sq = 1
> sigma_sq = 1
>
> # Gerando um valor para mu_i
> np.random.seed(42)
> mu_i = np.random.normal(0, np.sqrt(lambda_sq), 1)[0]
>
> # Gerando o ru√≠do branco
> epsilon = np.random.normal(0, np.sqrt(sigma_sq), 1000)
>
> # Gerando a s√©rie temporal
> Y = mu_i + epsilon
>
> # Calculando a m√©dia amostral
> sample_mean = np.mean(Y)
>
> # Imprimindo os resultados
> print(f"Valor de mu_i: {mu_i:.4f}")
> print(f"M√©dia amostral da s√©rie temporal: {sample_mean:.4f}")
> ```
>
> **Interpreta√ß√£o:**
>
> O c√≥digo gera uma √∫nica realiza√ß√£o da s√©rie temporal com um $\mu^{(i)}$ espec√≠fico. A m√©dia amostral dessa realiza√ß√£o converge para o valor de $\mu^{(i)}$, demonstrando que o processo n√£o √© erg√≥dico, pois a m√©dia amostral n√£o converge para a m√©dia do conjunto (que √© 0).

Para complementar o exemplo acima, podemos verificar a condi√ß√£o da soma das autocovari√¢ncias:
$$\sum_{j=-\infty}^{\infty} |\gamma_j| = |\gamma_0| + \sum_{j \neq 0} |\gamma_j| = \lambda^2 + \sigma^2 + \sum_{j \neq 0} \lambda^2$$
Como a soma $\sum_{j \neq 0} \lambda^2$ diverge, a condi√ß√£o de ergodicity n√£o √© satisfeita.

√â interessante notar que a n√£o-ergodicity no exemplo acima surge da presen√ßa de um componente fixo aleat√≥rio ($\mu^{(i)}$) que persiste ao longo do tempo. Remover essa depend√™ncia de longo prazo √© crucial para garantir a ergodicity.

**Teorema 2** (Ergodicity para Processos Lineares) Considere um processo linear da forma $Y_t = \sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}$, onde $\{\epsilon_t\}$ √© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, e $\sum_{i=0}^{\infty} |\psi_i| < \infty$. Ent√£o, $Y_t$ √© erg√≥dico para a m√©dia.

*Prova (Esbo√ßo):* Este teorema segue da aplica√ß√£o da Proposi√ß√£o 1. Primeiro, calcula-se a autocovari√¢ncia do processo linear. Em seguida, mostra-se que a condi√ß√£o $\sum_{i=0}^{\infty} |\psi_i| < \infty$ implica que a soma dos valores absolutos das autocovari√¢ncias √© finita, satisfazendo assim a condi√ß√£o para ergodicity da m√©dia estabelecida na Proposi√ß√£o 1.

*Prova Detalhada do Teorema 2:*
Dado um processo linear $Y_t = \sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}$, onde $\{\epsilon_t\}$ √© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, e $\sum_{i=0}^{\infty} |\psi_i| < \infty$. Queremos provar que $Y_t$ √© erg√≥dico para a m√©dia.

I. **Calcular a m√©dia do processo**:
   A m√©dia do processo $Y_t$ √©:
   $$E[Y_t] = E\left[\sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}\right] = \sum_{i=0}^{\infty} \psi_i E[\epsilon_{t-i}] = \sum_{i=0}^{\infty} \psi_i \cdot 0 = 0$$
   Portanto, $E[Y_t] = 0$.

II. **Calcular a autocovari√¢ncia do processo**:
    A autocovari√¢ncia $\gamma_j$ √© dada por:
    $$\gamma_j = Cov(Y_t, Y_{t-j}) = E[Y_t Y_{t-j}] - E[Y_t]E[Y_{t-j}] = E[Y_t Y_{t-j}]$$
    Substituindo a express√£o para $Y_t$:
    $$\gamma_j = E\left[\left(\sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}\right)\left(\sum_{k=0}^{\infty} \psi_k \epsilon_{t-j-k}\right)\right] = E\left[\sum_{i=0}^{\infty} \sum_{k=0}^{\infty} \psi_i \psi_k \epsilon_{t-i} \epsilon_{t-j-k}\right]$$
    Como $E[\epsilon_t \epsilon_s] = \sigma^2$ se $t=s$ e $0$ caso contr√°rio, temos:
    $$\gamma_j = \sum_{i=0}^{\infty} \sum_{k=0}^{\infty} \psi_i \psi_k E[\epsilon_{t-i} \epsilon_{t-j-k}] = \sum_{i=0}^{\infty} \psi_i \psi_{i-j} \sigma^2$$
    Note que $\psi_i = 0$ para $i < 0$.

III. **Verificar a condi√ß√£o para ergodicity**:
      Queremos mostrar que $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$. Substituindo a express√£o para $\gamma_j$:
      $$\sum_{j=-\infty}^{\infty} |\gamma_j| = \sum_{j=-\infty}^{\infty} \left|\sum_{i=0}^{\infty} \psi_i \psi_{i-j} \sigma^2\right| \leq \sigma^2 \sum_{j=-\infty}^{\infty} \sum_{i=0}^{\infty} |\psi_i| |\psi_{i-j}|$$
      Trocando a ordem das somas:
      $$\sum_{j=-\infty}^{\infty} |\gamma_j| \leq \sigma^2 \sum_{i=0}^{\infty} |\psi_i| \sum_{j=-\infty}^{\infty} |\psi_{i-j}|$$
      Fazendo a mudan√ßa de vari√°vel $m = i-j$, temos $j = i-m$, e quando $j \to -\infty$, $m \to \infty$, e quando $j \to \infty$, $m \to -\infty$. Assim:
      $$\sum_{j=-\infty}^{\infty} |\gamma_j| \leq \sigma^2 \sum_{i=0}^{\infty} |\psi_i| \sum_{m=-\infty}^{\infty} |\psi_m|$$
      Como $\psi_m = 0$ para $m < 0$, podemos reescrever a soma interna:
      $$\sum_{j=-\infty}^{\infty} |\gamma_j| \leq \sigma^2 \sum_{i=0}^{\infty} |\psi_i| \sum_{m=0}^{\infty} |\psi_m| = \sigma^2 \left(\sum_{i=0}^{\infty} |\psi_i|\right)^2$$
      Dado que $\sum_{i=0}^{\infty} |\psi_i| < \infty$, temos:
      $$\sum_{j=-\infty}^{\infty} |\gamma_j| \leq \sigma^2 \left(\sum_{i=0}^{\infty} |\psi_i|\right)^2 < \infty$$
      Portanto, a condi√ß√£o para ergodicity $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$ √© satisfeita.

IV. **Conclus√£o**:
      Pela Proposi√ß√£o 1, se $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, ent√£o $Y_t$ √© erg√≥dico para a m√©dia. Assim, provamos que o processo linear $Y_t = \sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}$ √© erg√≥dico para a m√©dia. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere um processo MA(1) definido como $Y_t = \epsilon_t + 0.5\epsilon_{t-1}$, onde $\epsilon_t$ √© ru√≠do branco com m√©dia 0 e vari√¢ncia 1. Aqui, $\psi_0 = 1$, $\psi_1 = 0.5$, e $\psi_i = 0$ para $i > 1$. A condi√ß√£o $\sum_{i=0}^{\infty} |\psi_i| = 1 + 0.5 = 1.5 < \infty$ √© satisfeita. Portanto, este processo √© erg√≥dico para a m√©dia.
>
> ```python
> import numpy as np
>
> # Definindo os par√¢metros
> theta = 0.5  # Coeficiente do MA(1)
> sigma_sq = 1  # Vari√¢ncia do ru√≠do branco
>
> # Gerando o ru√≠do branco
> np.random.seed(42)
> epsilon = np.random.normal(0, np.sqrt(sigma_sq), 1000)
>
> # Gerando a s√©rie temporal MA(1)
> Y = [epsilon[0]]  # O primeiro valor √© apenas o ru√≠do branco
> for t in range(1, len(epsilon)):
>     Y.append(epsilon[t] + theta * epsilon[t-1])
> Y = np.array(Y)
>
> # Calculando a m√©dia amostral
> sample_mean = np.mean(Y)
>
> # Imprimindo os resultados
> print(f"M√©dia amostral da s√©rie temporal MA(1): {sample_mean:.4f}")
> ```
>
> **Interpreta√ß√£o:**
>
> O c√≥digo gera uma s√©rie temporal MA(1) e calcula sua m√©dia amostral. Como o processo MA(1) satisfaz a condi√ß√£o de ergodicity, a m√©dia amostral converge para a m√©dia populacional (que √© 0) √† medida que o tamanho da amostra aumenta.
>
> Al√©m disso, podemos calcular as autocovari√¢ncias te√≥ricas para este processo MA(1):
> $\gamma_0 = Var(Y_t) = Var(\epsilon_t + 0.5\epsilon_{t-1}) = Var(\epsilon_t) + 0.5^2 Var(\epsilon_{t-1}) = 1 + 0.25 = 1.25$
> $\gamma_1 = Cov(Y_t, Y_{t-1}) = Cov(\epsilon_t + 0.5\epsilon_{t-1}, \epsilon_{t-1} + 0.5\epsilon_{t-2}) = 0.5 Var(\epsilon_{t-1}) = 0.5$
> $\gamma_j = 0$ para $j > 1$.
>
> A soma dos valores absolutos das autocovari√¢ncias √©:
> $\sum_{j=-\infty}^{\infty} |\gamma_j| = |\gamma_{-1}| + |\gamma_0| + |\gamma_1| = 0.5 + 1.25 + 0.5 = 2.25 < \infty$, confirmando a condi√ß√£o de ergodicity.

Para complementar o Teorema 2, podemos considerar uma extens√£o para processos ARMA.

**Teorema 2.1** (Ergodicity para Processos ARMA) Considere um processo ARMA(p,q) estacion√°rio definido por:

$$Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t + \theta_1 \epsilon_{t-1} + \ldots + \theta_q \epsilon_{t-q}$$

onde $\{\epsilon_t\}$ √© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$. Se o processo √© estacion√°rio (i.e., as ra√≠zes do polin√¥mio autoregressivo est√£o fora do c√≠rculo unit√°rio), ent√£o $Y_t$ √© erg√≥dico para a m√©dia.

*Prova (Esbo√ßo):* Um processo ARMA estacion√°rio pode ser representado como um processo linear da forma $Y_t = \sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}$ com $\sum_{i=0}^{\infty} |\psi_i| < \infty$ devido √† condi√ß√£o de estacionariedade. Portanto, pelo Teorema 2, o processo ARMA √© erg√≥dico para a m√©dia.

*Prova Detalhada do Teorema 2.1:*
Considere um processo ARMA(p,q) estacion√°rio definido por:
$$Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t + \theta_1 \epsilon_{t-1} + \ldots + \theta_q \epsilon_{t-q}$$
onde $\{\epsilon_t\}$ √© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$.
Queremos provar que se o processo √© estacion√°rio, ent√£o $Y_t$ √© erg√≥dico para a m√©dia.

I. **Representa√ß√£o do Processo ARMA como um Processo Linear**:
   Um processo ARMA(p,q) estacion√°rio pode ser reescrito como um processo linear (MA(‚àû)) da forma:
   $$Y_t = \sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}$$
   onde os coeficientes $\psi_i$ satisfazem $\sum_{i=0}^{\infty} |\psi_i| < \infty$ devido √† condi√ß√£o de estacionariedade do processo ARMA. A estacionariedade implica que as ra√≠zes do polin√¥mio autoregressivo $1 - \phi_1 z - \phi_2 z^2 - \ldots - \phi_p z^p$ est√£o fora do c√≠rculo unit√°rio, o que garante que os coeficientes $\psi_i$ decaiam exponencialmente r√°pido.

II. **Aplicar o Teorema 2**:
   O Teorema 2 afirma que se $Y_t = \sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}$ e $\sum_{i=0}^{\infty} |\psi_i| < \infty$, ent√£o $Y_t$ √© erg√≥dico para a m√©dia.
   Como o processo ARMA estacion√°rio pode ser representado como um processo linear com coeficientes que satisfazem a condi√ß√£o de converg√™ncia absoluta, podemos aplicar diretamente o Teorema 2.

III. **Conclus√£o**:
    Dado que o processo ARMA(p,q) estacion√°rio satisfaz as condi√ß√µes do Teorema 2, conclu√≠mos que $Y_t$tem uma representa√ß√£o Wold. Ou seja, $Y_t$ pode ser expresso como uma combina√ß√£o linear infinita de inova√ß√µes passadas (ru√≠do branco) mais um termo determin√≠stico.

### Representa√ß√£o de Wold do Processo ARMA(1,1)

Para o processo ARMA(1,1) definido por:

$$Y_t = \phi_1 Y_{t-1} + \theta_0 \epsilon_t + \theta_1 \epsilon_{t-1}$$

onde $|\phi_1| < 1$ para garantir a estacionariedade, podemos encontrar a representa√ß√£o de Wold expressando $Y_t$ em termos de $\epsilon_t, \epsilon_{t-1}, \epsilon_{t-2}, ...$.

A representa√ß√£o de Wold √© dada por:

$$Y_t = \sum_{j=0}^{\infty} \psi_j \epsilon_{t-j} + \mu$$

Para um processo de m√©dia zero (ou seja, $E[Y_t] = 0$), $\mu = 0$. Os coeficientes $\psi_j$ podem ser derivados recursivamente.

Para encontrar os coeficientes $\psi_j$ para o ARMA(1,1), podemos reescrever o processo como:

$$Y_t = \phi_1 Y_{t-1} + \theta_0 \epsilon_t + \theta_1 \epsilon_{t-1}$$

Substituindo recursivamente $Y_{t-1}$, $Y_{t-2}$, etc., eventualmente obtemos uma express√£o em termos de $\epsilon_t$ e seus lags.

Os primeiros coeficientes $\psi_j$ s√£o:

*   $\psi_0 = \theta_0$
*   $\psi_1 = \phi_1 \theta_0 + \theta_1$
*   $\psi_2 = \phi_1 \psi_1 = \phi_1 (\phi_1 \theta_0 + \theta_1)$
*   $\psi_3 = \phi_1 \psi_2 = \phi_1^2 (\phi_1 \theta_0 + \theta_1)$

Em geral, para $j \geq 1$:

$$\psi_j = \phi_1 \psi_{j-1} = \phi_1^{j-1} (\phi_1 \theta_0 + \theta_1)$$

Portanto, a representa√ß√£o de Wold para o processo ARMA(1,1) √©:

$$Y_t = \theta_0 \epsilon_t + \sum_{j=1}^{\infty} \phi_1^{j-1} (\phi_1 \theta_0 + \theta_1) \epsilon_{t-j}$$

Esta representa√ß√£o expressa $Y_t$ como uma soma ponderada infinita de inova√ß√µes passadas $\epsilon_{t-j}$, mostrando que o processo ARMA(1,1) estacion√°rio tem uma representa√ß√£o Wold.

### Exemplo: Simula√ß√£o e Representa√ß√£o Gr√°fica

Para ilustrar, podemos simular um processo ARMA(1,1) e exibir sua representa√ß√£o gr√°fica.

```python
import numpy as np
import matplotlib.pyplot as plt

# Par√¢metros do ARMA(1,1)
phi1 = 0.7
theta0 = 1.0
theta1 = 0.5
n = 200

# Gera√ß√£o de ru√≠do branco
epsilon = np.random.normal(0, 1, n)

# Inicializa√ß√£o do processo ARMA(1,1)
Y = np.zeros(n)
Y[0] = epsilon[0]  # Valor inicial

# Simula√ß√£o do processo ARMA(1,1)
for t in range(1, n):
    Y[t] = phi1 * Y[t-1] + theta0 * epsilon[t] + theta1 * epsilon[t-1]

# Plotagem da s√©rie temporal
plt.figure(figsize=(10, 6))
plt.plot(Y)
plt.title('Simula√ß√£o de um Processo ARMA(1,1)')
plt.xlabel('Tempo')
plt.ylabel('Valor de Y_t')
plt.grid(True)
plt.show()
```

Este script em Python simula um processo ARMA(1,1) com $\phi_1 = 0.7$, $\theta_0 = 1.0$ e $\theta_1 = 0.5$, e ent√£o plota a s√©rie temporal resultante. A representa√ß√£o gr√°fica visualiza o comportamento do processo ARMA(1,1) ao longo do tempo, demonstrando sua estacionariedade e depend√™ncia temporal.

<!-- END -->