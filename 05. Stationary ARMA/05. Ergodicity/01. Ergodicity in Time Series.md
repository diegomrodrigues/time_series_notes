## Ergodicity in Stationary Time Series

### Introdu√ß√£o

Em continuidade ao estudo de processos estoc√°sticos estacion√°rios, este cap√≠tulo aprofunda-se no conceito de **ergodicity**, uma propriedade crucial que conecta a an√°lise te√≥rica de conjuntos estat√≠sticos (ensemble averages) com a an√°lise pr√°tica de s√©ries temporais observadas. Como vimos anteriormente, processos estacion√°rios exibem propriedades estat√≠sticas que n√£o variam com o tempo. Ergodicity adiciona uma camada importante, garantindo que as m√©dias temporais calculadas a partir de uma √∫nica realiza√ß√£o da s√©rie temporal convergem para as m√©dias estat√≠sticas te√≥ricas, permitindo infer√™ncias sobre o processo subjacente a partir de uma √∫nica trajet√≥ria observada [^4].

### Conceitos Fundamentais

**Ergodicity** em s√©ries temporais refere-se √† condi√ß√£o sob a qual as m√©dias temporais convergem para as m√©dias de conjunto (ensemble averages) para um processo estacion√°rio [^4]. Mais formalmente, um processo covariance-stationary √© dito *ergodic for the mean* se a m√©dia amostral, definida como:

$$\bar{Y} = \frac{1}{T} \sum_{t=1}^{T} Y_t$$

converge em probabilidade para a esperan√ßa matem√°tica $E(Y)$ quando $T$ tende ao infinito [^4]:

$$ \text{plim}_{T \to \infty} \frac{1}{T} \sum_{t=1}^{T} Y_t = E(Y) $$

Essa converg√™ncia implica que uma √∫nica realiza√ß√£o suficientemente longa da s√©rie temporal pode representar as propriedades estat√≠sticas de todo o conjunto de poss√≠veis realiza√ß√µes do processo estoc√°stico. Em termos pr√°ticos, essa propriedade √© fundamental porque, na maioria das situa√ß√µes reais, temos acesso a apenas uma √∫nica realiza√ß√£o da s√©rie temporal.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos uma s√©rie temporal $Y_t$ gerada por um processo estacion√°rio com m√©dia $E(Y) = 5$. Se o processo √© erg√≥dico para a m√©dia, ent√£o, √† medida que o tamanho da amostra $T$ aumenta, a m√©dia amostral $\bar{Y}$ se aproximar√° de 5. Por exemplo, se observarmos $T = 100$ pontos e calcularmos $\bar{Y} = 5.2$, e depois observarmos $T = 1000$ pontos e calcularmos $\bar{Y} = 5.05$, e finalmente observarmos $T = 10000$ pontos e calcularmos $\bar{Y} = 5.01$, isso sugere que o processo √© erg√≥dico para a m√©dia, pois $\bar{Y}$ est√° convergindo para $E(Y) = 5$ √† medida que $T$ aumenta.
>
> ```python
> import numpy as np
>
> # Definindo a m√©dia populacional
> mean_population = 5
>
> # Gerando uma s√©rie temporal estacion√°ria (exemplo simples com ru√≠do branco)
> np.random.seed(42)  # Definindo uma semente para reprodutibilidade
> epsilon = np.random.normal(0, 1, 10000) # Ru√≠do branco com m√©dia 0 e desvio padr√£o 1
> Y = mean_population + epsilon
>
> # Calculando a m√©dia amostral para diferentes tamanhos de amostra
> sample_sizes = [100, 1000, 10000]
> sample_means = []
>
> for T in sample_sizes:
>     sample_means.append(np.mean(Y[:T]))
>
> # Imprimindo os resultados
> for i, T in enumerate(sample_sizes):
>     print(f"Tamanho da amostra: {T}, M√©dia amostral: {sample_means[i]:.4f}")
> ```
>
> **Interpreta√ß√£o:**
>
> Este exemplo demonstra numericamente como a m√©dia amostral se aproxima da m√©dia populacional √† medida que o tamanho da amostra aumenta, indicando ergodicity.

Para garantir a ergodicity, √© necess√°rio que a autocovari√¢ncia $\gamma_j$ decaia para zero suficientemente r√°pido √† medida que o lag $j$ aumenta [^4]. Uma condi√ß√£o suficiente para a ergodicity da m√©dia em processos covariance-stationary √© que a soma dos valores absolutos das autocovari√¢ncias seja finita [^4]:

$$\sum_{j=0}^{\infty} |\gamma_j| < \infty$$

Essa condi√ß√£o assegura que as correla√ß√µes entre observa√ß√µes distantes no tempo diminuam a um ponto em que a m√©dia amostral forne√ßa uma estimativa consistente da m√©dia populacional. Similarmente, um processo covariance-stationary √© *ergodic para os segundos momentos* se:

$$ \text{plim}_{T \to \infty} \frac{1}{T-j} \sum_{t=j+1}^{T} (Y_t - \mu)(Y_{t-j} - \mu) \xrightarrow{p} \gamma_j $$

para todo $j$ [^4]. Condi√ß√µes suficientes para ergodicity de segunda ordem ser√£o exploradas em cap√≠tulos posteriores [^4]. Em processos Gaussianos estacion√°rios, a condi√ß√£o de ergodicity para a m√©dia √© suficiente para garantir a ergodicity para todos os momentos [^4].

A seguir, apresentamos um resultado que relaciona a condi√ß√£o de ergodicity para a m√©dia com a fun√ß√£o de densidade espectral do processo.

**Teorema 1** Um processo estacion√°rio e covariance-stationary $Y_t$ com fun√ß√£o de densidade espectral $f(\omega)$ √© erg√≥dico para a m√©dia se e somente se $f(0) = 0$, exceto possivelmente para um processo com m√©dia zero.

*Prova (Esbo√ßo):* A prova envolve o uso da representa√ß√£o espectral do processo $Y_t$ e a rela√ß√£o entre a fun√ß√£o de autocovari√¢ncia e a densidade espectral. A condi√ß√£o $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$ garante que a densidade espectral √© bem definida e finita. A converg√™ncia da m√©dia amostral para a m√©dia populacional est√° relacionada ao comportamento da densidade espectral na frequ√™ncia zero. Se $f(0) = 0$, ent√£o a vari√¢ncia da m√©dia amostral tende a zero √† medida que o tamanho da amostra aumenta, implicando ergodicity.

**Corol√°rio 1.1** Se um processo estacion√°rio covariance-stationary tem uma fun√ß√£o de densidade espectral cont√≠nua e limitada, e $f(0) > 0$, ent√£o o processo n√£o √© erg√≥dico para a m√©dia.

**Lema 1.1** Para um processo estacion√°rio e covariance-stationary, a condi√ß√£o $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$ implica que a fun√ß√£o de densidade espectral $f(\omega)$ existe e √© cont√≠nua.

*Prova (Esbo√ßo):* A fun√ß√£o de densidade espectral √© definida como a transformada de Fourier da fun√ß√£o de autocovari√¢ncia: $f(\omega) = \frac{1}{2\pi} \sum_{j=-\infty}^{\infty} \gamma_j e^{-i\omega j}$. A condi√ß√£o $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$ garante a converg√™ncia absoluta da s√©rie de Fourier, o que implica que $f(\omega)$ existe e √© cont√≠nua.

*Prova Detalhada do Lema 1.1:*
Para um processo estacion√°rio e covariance-stationary, a fun√ß√£o de densidade espectral $f(\omega)$ √© definida como a transformada de Fourier da fun√ß√£o de autocovari√¢ncia $\gamma_j$:

$$f(\omega) = \frac{1}{2\pi} \sum_{j=-\infty}^{\infty} \gamma_j e^{-i\omega j}$$

Provemos que se $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, ent√£o $f(\omega)$ existe e √© cont√≠nua.

I. **Exist√™ncia de $f(\omega)$**:
    A exist√™ncia de $f(\omega)$ depende da converg√™ncia da s√©rie $\sum_{j=-\infty}^{\infty} \gamma_j e^{-i\omega j}$. Se a s√©rie converge absolutamente, ent√£o ela converge. Vamos verificar a converg√™ncia absoluta:

    $$\sum_{j=-\infty}^{\infty} |\gamma_j e^{-i\omega j}| = \sum_{j=-\infty}^{\infty} |\gamma_j| |e^{-i\omega j}|$$

    Como $|e^{-i\omega j}| = 1$ para todo $j$ e $\omega$, temos:

    $$\sum_{j=-\infty}^{\infty} |\gamma_j e^{-i\omega j}| = \sum_{j=-\infty}^{\infty} |\gamma_j|$$

    Dado que $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, a s√©rie $\sum_{j=-\infty}^{\infty} \gamma_j e^{-i\omega j}$ converge absolutamente. Portanto, $f(\omega)$ existe.

II. **Continuidade de $f(\omega)$**:
     Para mostrar que $f(\omega)$ √© cont√≠nua, precisamos mostrar que para todo $\omega_0$, $\lim_{\omega \to \omega_0} f(\omega) = f(\omega_0)$. Seja $\omega_0$ um ponto arbitr√°rio. Ent√£o:

     $$\lim_{\omega \to \omega_0} f(\omega) = \lim_{\omega \to \omega_0} \frac{1}{2\pi} \sum_{j=-\infty}^{\infty} \gamma_j e^{-i\omega j}$$

     Como a s√©rie converge absolutamente, podemos trocar o limite e a soma:

     $$\lim_{\omega \to \omega_0} f(\omega) = \frac{1}{2\pi} \sum_{j=-\infty}^{\infty} \gamma_j \lim_{\omega \to \omega_0} e^{-i\omega j}$$

     A fun√ß√£o exponencial $e^{-i\omega j}$ √© cont√≠nua em $\omega$, ent√£o:

     $$\lim_{\omega \to \omega_0} e^{-i\omega j} = e^{-i\omega_0 j}$$

     Portanto:

     $$\lim_{\omega \to \omega_0} f(\omega) = \frac{1}{2\pi} \sum_{j=-\infty}^{\infty} \gamma_j e^{-i\omega_0 j} = f(\omega_0)$$

     Isso mostra que $f(\omega)$ √© cont√≠nua em $\omega_0$. Como $\omega_0$ √© um ponto arbitr√°rio, $f(\omega)$ √© cont√≠nua para todo $\omega$.

III. **Conclus√£o**:
      Provamos que se $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, ent√£o a fun√ß√£o de densidade espectral $f(\omega)$ existe e √© cont√≠nua. ‚ñ†

Agora, podemos estabelecer uma proposi√ß√£o que fornece uma condi√ß√£o suficiente, embora n√£o necess√°ria, para a ergodicity da m√©dia, diretamente em termos das autocovari√¢ncias.

**Proposi√ß√£o 1** Seja $Y_t$ um processo estacion√°rio e covariance-stationary. Se $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, ent√£o $Y_t$ √© erg√≥dico para a m√©dia.

*Prova:* A prova segue da aplica√ß√£o da lei dos grandes n√∫meros para processos fracamente dependentes. A condi√ß√£o $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$ implica que o processo $Y_t$ √© assintoticamente n√£o correlacionado, permitindo a aplica√ß√£o da lei dos grandes n√∫meros. Assim, a m√©dia amostral converge em probabilidade para a m√©dia populacional, garantindo a ergodicity para a m√©dia.

*Prova Detalhada da Proposi√ß√£o 1:*
Seja $Y_t$ um processo estacion√°rio e covariance-stationary com autocovari√¢ncias $\gamma_j$ e m√©dia $\mu = E[Y_t]$. Queremos provar que se $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, ent√£o $Y_t$ √© erg√≥dico para a m√©dia, ou seja, $\text{plim}_{T \to \infty} \bar{Y} = \mu$, onde $\bar{Y} = \frac{1}{T} \sum_{t=1}^{T} Y_t$.

I. **Calcular a m√©dia da m√©dia amostral**:

   A m√©dia da m√©dia amostral √©:

   $$E[\bar{Y}] = E\left[\frac{1}{T} \sum_{t=1}^{T} Y_t\right] = \frac{1}{T} \sum_{t=1}^{T} E[Y_t] = \frac{1}{T} \sum_{t=1}^{T} \mu = \mu$$

   Portanto, $\bar{Y}$ √© um estimador n√£o viesado de $\mu$.

II. **Calcular a vari√¢ncia da m√©dia amostral**:

    A vari√¢ncia da m√©dia amostral √©:

    $$Var(\bar{Y}) = Var\left(\frac{1}{T} \sum_{t=1}^{T} Y_t\right) = \frac{1}{T^2} Var\left(\sum_{t=1}^{T} Y_t\right)$$

    Usando a defini√ß√£o de covari√¢ncia, temos:

    $$Var\left(\sum_{t=1}^{T} Y_t\right) = \sum_{t=1}^{T} \sum_{s=1}^{T} Cov(Y_t, Y_s)$$

    Como o processo √© estacion√°rio, $Cov(Y_t, Y_s) = \gamma_{|t-s|}$. Portanto:

    $$Var\left(\sum_{t=1}^{T} Y_t\right) = \sum_{t=1}^{T} \sum_{s=1}^{T} \gamma_{|t-s|}$$

    Reorganizando a soma, temos:

    $$Var\left(\sum_{t=1}^{T} Y_t\right) = T\gamma_0 + 2\sum_{j=1}^{T-1} (T-j)\gamma_j$$

    Dividindo por $T^2$, obtemos:

    $$Var(\bar{Y}) = \frac{1}{T}\gamma_0 + \frac{2}{T^2}\sum_{j=1}^{T-1} (T-j)\gamma_j$$
    $$Var(\bar{Y}) = \frac{1}{T}\gamma_0 + \frac{2}{T}\sum_{j=1}^{T-1} \gamma_j - \frac{2}{T^2}\sum_{j=1}^{T-1} j\gamma_j$$

III. **Mostrar que a vari√¢ncia tende a zero quando $T \to \infty$**:

     Queremos mostrar que $\lim_{T \to \infty} Var(\bar{Y}) = 0$. Dado que $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, temos que $\sum_{j=-\infty}^{\infty} j|\gamma_j|$ tamb√©m √© finito, pois a converg√™ncia absoluta da s√©rie $\sum_{j=-\infty}^{\infty} |\gamma_j|$ implica que $\gamma_j$ decai para zero mais r√°pido do que $1/j$, garantindo que a s√©rie ponderada $\sum_{j=-\infty}^{\infty} j|\gamma_j|$ tamb√©m converge.
    Assim, $\lim_{T\to\infty} \frac{1}{T}\sum_{j=1}^{T-1} \gamma_j = 0$ e $\lim_{T\to\infty} \frac{1}{T^2}\sum_{j=1}^{T-1} j\gamma_j = 0$.

     Portanto:

     $$\lim_{T \to \infty} Var(\bar{Y}) = \lim_{T \to \infty} \left(\frac{1}{T}\gamma_0 + \frac{2}{T}\sum_{j=1}^{T-1} \gamma_j - \frac{2}{T^2}\sum_{j=1}^{T-1} j\gamma_j\right) = 0$$

IV. **Aplicar a desigualdade de Chebyshev**:

    A desigualdade de Chebyshev estabelece que para qualquer $\epsilon > 0$:

    $$P(|\bar{Y} - \mu| > \epsilon) \leq \frac{Var(\bar{Y})}{\epsilon^2}$$

    Como $\lim_{T \to \infty} Var(\bar{Y}) = 0$, temos:

    $$\lim_{T \to \infty} P(|\bar{Y} - \mu| > \epsilon) = 0$$

    Isso significa que $\bar{Y}$ converge em probabilidade para $\mu$, ou seja:

    $$\text{plim}_{T \to \infty} \bar{Y} = \mu$$

V. **Conclus√£o**:

     Provamos que se $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, ent√£o $\text{plim}_{T \to \infty} \bar{Y} = \mu$, o que significa que $Y_t$ √© erg√≥dico para a m√©dia. ‚ñ†

**Exemplo de um processo estacion√°rio n√£o-erg√≥dico:**
Considere um processo definido como [^4]:

$$Y_t^{(i)} = \mu^{(i)} + \epsilon_t$$

onde $\{\epsilon_t\}$ √© um processo de ru√≠do branco Gaussiano com m√©dia zero e vari√¢ncia $\sigma^2$, independente de $\mu^{(i)}$, e $\mu^{(i)}$ √© gerado por uma distribui√ß√£o $N(0, \lambda^2)$. Neste caso, a m√©dia √© $E(Y_t) = 0$, a autocovari√¢ncia no lag zero √© $\gamma_0 = \lambda^2 + \sigma^2$, e as autocovari√¢ncias em todos os outros lags s√£o $\gamma_j = \lambda^2$ para $j \neq 0$ [^4]. Embora este processo seja covariance-stationary, ele n√£o √© erg√≥dico, pois a m√©dia amostral converge para $\mu^{(i)}$ em vez de convergir para zero [^4]. Assim, a m√©dia temporal n√£o converge para a m√©dia do conjunto, violando a condi√ß√£o de ergodicity.

> üí° **Exemplo Num√©rico:**
>
> Suponha que $\lambda^2 = 1$ e $\sigma^2 = 1$. Ent√£o, $\gamma_0 = 2$ e $\gamma_j = 1$ para $j \neq 0$. A m√©dia amostral de uma √∫nica realiza√ß√£o deste processo converge para um valor espec√≠fico de $\mu^{(i)}$ gerado a partir de $N(0, 1)$, e n√£o para a m√©dia do conjunto, que √© 0.
>
> ```python
> import numpy as np
>
> # Definindo os par√¢metros
> lambda_sq = 1
> sigma_sq = 1
>
> # Gerando um valor para mu_i
> np.random.seed(42)
> mu_i = np.random.normal(0, np.sqrt(lambda_sq), 1)[0]
>
> # Gerando o ru√≠do branco
> epsilon = np.random.normal(0, np.sqrt(sigma_sq), 1000)
>
> # Gerando a s√©rie temporal
> Y = mu_i + epsilon
>
> # Calculando a m√©dia amostral
> sample_mean = np.mean(Y)
>
> # Imprimindo os resultados
> print(f"Valor de mu_i: {mu_i:.4f}")
> print(f"M√©dia amostral da s√©rie temporal: {sample_mean:.4f}")
> ```
>
> **Interpreta√ß√£o:**
>
> O c√≥digo gera uma √∫nica realiza√ß√£o da s√©rie temporal com um $\mu^{(i)}$ espec√≠fico. A m√©dia amostral dessa realiza√ß√£o converge para o valor de $\mu^{(i)}$, demonstrando que o processo n√£o √© erg√≥dico, pois a m√©dia amostral n√£o converge para a m√©dia do conjunto (que √© 0).

Para complementar o exemplo acima, podemos verificar a condi√ß√£o da soma das autocovari√¢ncias:
$$\sum_{j=-\infty}^{\infty} |\gamma_j| = |\gamma_0| + \sum_{j \neq 0} |\gamma_j| = \lambda^2 + \sigma^2 + \sum_{j \neq 0} \lambda^2$$
Como a soma $\sum_{j \neq 0} \lambda^2$ diverge, a condi√ß√£o de ergodicity n√£o √© satisfeita.

√â interessante notar que a n√£o-ergodicity no exemplo acima surge da presen√ßa de um componente fixo aleat√≥rio ($\mu^{(i)}$) que persiste ao longo do tempo. Remover essa depend√™ncia de longo prazo √© crucial para garantir a ergodicity.

**Teorema 2** (Ergodicity para Processos Lineares) Considere um processo linear da forma $Y_t = \sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}$, onde $\{\epsilon_t\}$ √© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, e $\sum_{i=0}^{\infty} |\psi_i| < \infty$. Ent√£o, $Y_t$ √© erg√≥dico para a m√©dia.

*Prova (Esbo√ßo):* Este teorema segue da aplica√ß√£o da Proposi√ß√£o 1. Primeiro, calcula-se a autocovari√¢ncia do processo linear. Em seguida, mostra-se que a condi√ß√£o $\sum_{i=0}^{\infty} |\psi_i| < \infty$ implica que a soma dos valores absolutos das autocovari√¢ncias √© finita, satisfazendo assim a condi√ß√£o para ergodicity da m√©dia estabelecida na Proposi√ß√£o 1.

*Prova Detalhada do Teorema 2:*
Dado um processo linear $Y_t = \sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}$, onde $\{\epsilon_t\}$ √© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, e $\sum_{i=0}^{\infty} |\psi_i| < \infty$. Queremos provar que $Y_t$ √© erg√≥dico para a m√©dia.

I. **Calcular a m√©dia do processo**:
   A m√©dia do processo $Y_t$ √©:
   $$E[Y_t] = E\left[\sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}\right] = \sum_{i=0}^{\infty} \psi_i E[\epsilon_{t-i}] = \sum_{i=0}^{\infty} \psi_i \cdot 0 = 0$$
   Portanto, $E[Y_t] = 0$.

II. **Calcular a autocovari√¢ncia do processo**:
    A autocovari√¢ncia $\gamma_j$ √© dada por:
    $$\gamma_j = Cov(Y_t, Y_{t-j}) = E[Y_t Y_{t-j}] - E[Y_t]E[Y_{t-j}] = E[Y_t Y_{t-j}]$$
    Substituindo a express√£o para $Y_t$:
    $$\gamma_j = E\left[\left(\sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}\right)\left(\sum_{k=0}^{\infty} \psi_k \epsilon_{t-j-k}\right)\right] = E\left[\sum_{i=0}^{\infty} \sum_{k=0}^{\infty} \psi_i \psi_k \epsilon_{t-i} \epsilon_{t-j-k}\right]$$
    Como $E[\epsilon_t \epsilon_s] = \sigma^2$ se $t=s$ e $0$ caso contr√°rio, temos:
    $$\gamma_j = \sum_{i=0}^{\infty} \sum_{k=0}^{\infty} \psi_i \psi_k E[\epsilon_{t-i} \epsilon_{t-j-k}] = \sum_{i=0}^{\infty} \psi_i \psi_{i-j} \sigma^2$$
    Note que $\psi_i = 0$ para $i < 0$.

III. **Verificar a condi√ß√£o para ergodicity**:
      Queremos mostrar que $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$. Substituindo a express√£o para $\gamma_j$:
      $$\sum_{j=-\infty}^{\infty} |\gamma_j| = \sum_{j=-\infty}^{\infty} \left|\sum_{i=0}^{\infty} \psi_i \psi_{i-j} \sigma^2\right| \leq \sigma^2 \sum_{j=-\infty}^{\infty} \sum_{i=0}^{\infty} |\psi_i| |\psi_{i-j}|$$
      Trocando a ordem das somas:
      $$\sum_{j=-\infty}^{\infty} |\gamma_j| \leq \sigma^2 \sum_{i=0}^{\infty} |\psi_i| \sum_{j=-\infty}^{\infty} |\psi_{i-j}|$$
      Fazendo a mudan√ßa de vari√°vel $m = i-j$, temos $j = i-m$, e quando $j \to -\infty$, $m \to \infty$, e quando $j \to \infty$, $m \to -\infty$. Assim:
      $$\sum_{j=-\infty}^{\infty} |\gamma_j| \leq \sigma^2 \sum_{i=0}^{\infty} |\psi_i| \sum_{m=-\infty}^{\infty} |\psi_m|$$
      Como $\psi_m = 0$ para $m < 0$, podemos reescrever a soma interna:
      $$\sum_{j=-\infty}^{\infty} |\gamma_j| \leq \sigma^2 \sum_{i=0}^{\infty} |\psi_i| \sum_{m=0}^{\infty} |\psi_m| = \sigma^2 \left(\sum_{i=0}^{\infty} |\psi_i|\right)^2$$
      Dado que $\sum_{i=0}^{\infty} |\psi_i| < \infty$, temos:
      $$\sum_{j=-\infty}^{\infty} |\gamma_j| \leq \sigma^2 \left(\sum_{i=0}^{\infty} |\psi_i|\right)^2 < \infty$$
      Portanto, a condi√ß√£o para ergodicity $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$ √© satisfeita.

IV. **Conclus√£o**:
      Pela Proposi√ß√£o 1, se $\sum_{j=-\infty}^{\infty} |\gamma_j| < \infty$, ent√£o $Y_t$ √© erg√≥dico para a m√©dia. Assim, provamos que o processo linear $Y_t = \sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}$ √© erg√≥dico para a m√©dia. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere um processo MA(1) definido como $Y_t = \epsilon_t + 0.5\epsilon_{t-1}$, onde $\epsilon_t$ √© ru√≠do branco com m√©dia 0 e vari√¢ncia 1. Aqui, $\psi_0 = 1$, $\psi_1 = 0.5$, e $\psi_i = 0$ para $i > 1$. A condi√ß√£o $\sum_{i=0}^{\infty} |\psi_i| = 1 + 0.5 = 1.5 < \infty$ √© satisfeita. Portanto, este processo √© erg√≥dico para a m√©dia.
>
> ```python
> import numpy as np
>
> # Definindo os par√¢metros
> theta = 0.5  # Coeficiente do MA(1)
> sigma_sq = 1  # Vari√¢ncia do ru√≠do branco
>
> # Gerando o ru√≠do branco
> np.random.seed(42)
> epsilon = np.random.normal(0, np.sqrt(sigma_sq), 1000)
>
> # Gerando a s√©rie temporal MA(1)
> Y = [epsilon[0]]  # O primeiro valor √© apenas o ru√≠do branco
> for t in range(1, len(epsilon)):
>     Y.append(epsilon[t] + theta * epsilon[t-1])
> Y = np.array(Y)
>
> # Calculando a m√©dia amostral
> sample_mean = np.mean(Y)
>
> # Imprimindo os resultados
> print(f"M√©dia amostral da s√©rie temporal MA(1): {sample_mean:.4f}")
> ```
>
> **Interpreta√ß√£o:**
>
> O c√≥digo gera uma s√©rie temporal MA(1) e calcula sua m√©dia amostral. Como o processo MA(1) satisfaz a condi√ß√£o de ergodicity, a m√©dia amostral converge para a m√©dia populacional (que √© 0) √† medida que o tamanho da amostra aumenta.

### Conclus√£o

A propriedade de ergodicity √© fundamental para a aplica√ß√£o pr√°tica da an√°lise de s√©ries temporais. Ela valida a infer√™ncia de propriedades estat√≠sticas do processo subjacente a partir de uma √∫nica realiza√ß√£o observada, conectando a teoria estat√≠stica com a realidade emp√≠rica. Em ess√™ncia, ergodicity permite que, ao observar uma s√©rie temporal por um longo per√≠odo, possamos aprender sobre o processo estoc√°stico que a gerou. Sem ergodicity, a an√°lise de s√©ries temporais seria severamente limitada, pois as m√©dias e autocovari√¢ncias calculadas a partir de uma √∫nica realiza√ß√£o n√£o seriam representativas do comportamento geral do processo [^4].

### Refer√™ncias

[^4]: P√°gina 47 do documento.
<!-- END -->