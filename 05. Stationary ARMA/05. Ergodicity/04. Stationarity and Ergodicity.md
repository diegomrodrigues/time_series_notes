## T√≠tulo Conciso: Estacionariedade e Ergodicity: Distin√ß√µes e Implica√ß√µes

### Introdu√ß√£o

Como explorado no cap√≠tulo anterior [^4], **estacionariedade** e **ergodicity** s√£o conceitos fundamentais na an√°lise de s√©ries temporais. Embora frequentemente interligados e, em muitas aplica√ß√µes, resultando em requisitos similares, √© crucial entender suas distin√ß√µes e implica√ß√µes. Em particular, exploraremos a possibilidade de um processo ser estacion√°rio, mas n√£o erg√≥dico, destacando as condi√ß√µes sob as quais essa diverg√™ncia pode ocorrer e suas consequ√™ncias para a infer√™ncia estat√≠stica. Em continuidade, abordaremos em detalhe um exemplo paradigm√°tico de processo estacion√°rio n√£o-erg√≥dico, detalhando sua constru√ß√£o, propriedades e implica√ß√µes pr√°ticas.

### Estacionariedade vs. Ergodicity: Uma Distin√ß√£o Crucial

A **estacionariedade** refere-se √† invari√¢ncia das propriedades estat√≠sticas de uma s√©rie temporal ao longo do tempo. Um processo estacion√°rio, em sua forma covariance-stationary, possui m√©dia, vari√¢ncia e autocovari√¢ncias constantes ao longo do tempo. Isso significa que a distribui√ß√£o conjunta de $Y_{t_1}, Y_{t_2}, \dots, Y_{t_n}$ √© id√™ntica √† distribui√ß√£o de $Y_{t_1 + h}, Y_{t_2 + h}, \dots, Y_{t_n + h}$ para qualquer $h$, garantindo que a estrutura estat√≠stica do processo n√£o se altera com o tempo [^4].

> üí° **Exemplo Num√©rico:**
>
> Considere uma s√©rie temporal $Y_t = 2 + \epsilon_t$, onde $\epsilon_t \sim N(0,1)$ √© ru√≠do branco.  A m√©dia de $Y_t$ √© sempre 2, e a vari√¢ncia √© sempre 1.  N√£o importa se observamos a s√©rie no tempo $t=1$ ou $t=100$, as propriedades estat√≠sticas s√£o as mesmas. Isto demonstra a estacionariedade.
>
> ```python
> import numpy as np
>
> # Simula√ß√£o da s√©rie temporal
> np.random.seed(42) # Para reprodutibilidade
> epsilon = np.random.normal(0, 1, 100)
> Y = 2 + epsilon
>
> # Calculando a m√©dia nos primeiros 50 pontos e nos √∫ltimos 50 pontos
> mean_first_50 = np.mean(Y[:50])
> mean_last_50 = np.mean(Y[50:])
>
> # Imprimindo os resultados
> print(f"M√©dia dos primeiros 50 pontos: {mean_first_50:.2f}")
> print(f"M√©dia dos √∫ltimos 50 pontos: {mean_last_50:.2f}")
> ```
>
> **Interpreta√ß√£o:**
>
> As m√©dias calculadas em diferentes segmentos da s√©rie temporal ser√£o pr√≥ximas de 2, confirmando a estacionariedade, pois a m√©dia n√£o muda significativamente ao longo do tempo.  As pequenas diferen√ßas s√£o devido √† natureza aleat√≥ria do ru√≠do branco.

Em contraste, a **ergodicity** se refere √† propriedade de um processo em que as m√©dias temporais convergem para as m√©dias de conjunto (ensemble averages). Em outras palavras, uma √∫nica realiza√ß√£o suficientemente longa da s√©rie temporal pode representar as propriedades estat√≠sticas de todo o conjunto de poss√≠veis realiza√ß√µes do processo estoc√°stico [^4]. Como vimos anteriormente, para um processo covariance-stationary ser erg√≥dico para a m√©dia, √© necess√°rio que a m√©dia amostral $\bar{Y} = \frac{1}{T}\sum_{t=1}^{T}Y_t$ convirja em probabilidade para a esperan√ßa matem√°tica $E(Y)$ quando $T \to \infty$ [^4]:

$$\text{plim}_{T \to \infty} \frac{1}{T} \sum_{t=1}^{T} Y_t = E(Y)$$

> üí° **Exemplo Num√©rico:**
>
> Usando a mesma s√©rie temporal do exemplo anterior ($Y_t = 2 + \epsilon_t$), a ergodicity implica que, ao aumentar o tamanho da amostra $T$, a m√©dia amostral $\bar{Y}$ se aproximar√° de 2 (a m√©dia de conjunto).
>
> ```python
> import numpy as np
>
> # Simula√ß√£o da s√©rie temporal com um tamanho de amostra maior
> np.random.seed(42)
> epsilon = np.random.normal(0, 1, 10000)
> Y = 2 + epsilon
>
> # Calculando a m√©dia amostral
> sample_mean = np.mean(Y)
>
> # Imprimindo o resultado
> print(f"M√©dia amostral da s√©rie temporal: {sample_mean:.4f}")
> ```
>
> **Interpreta√ß√£o:**
>
> A m√©dia amostral calculada com um tamanho de amostra maior (10000) estar√° ainda mais pr√≥xima de 2, demonstrando a ergodicity. Uma √∫nica realiza√ß√£o da s√©rie temporal captura a m√©dia de conjunto do processo.

Apesar de ambas as propriedades estarem frequentemente presentes em modelos de s√©ries temporais, elas n√£o s√£o equivalentes. √â poss√≠vel construir processos que s√£o estacion√°rios, mas n√£o ergodicos, e vice-versa. Para elucidar essa distin√ß√£o, examinaremos um exemplo detalhado de um processo estacion√°rio n√£o-erg√≥dico.

#### Exemplo Detalhado: Processo com M√©dia Aleat√≥ria Fixa

Considere um processo definido como [^4]:

$$Y_{t}^{(i)} = \mu^{(i)} + \epsilon_t$$

onde:
*   $Y_t^{(i)}$ representa a *i*-√©sima realiza√ß√£o da s√©rie temporal no tempo *t*.
*   $\mu^{(i)}$ √© uma vari√°vel aleat√≥ria *fixa* para cada realiza√ß√£o *i*, gerada de uma distribui√ß√£o com m√©dia zero e vari√¢ncia $\lambda^2$, ou seja, $\mu^{(i)} \sim N(0, \lambda^2)$.
*   $\epsilon_t$ √© um processo de ru√≠do branco Gaussiano com m√©dia zero e vari√¢ncia $\sigma^2$, independente de $\mu^{(i)}$.

Este processo √© *covariance-stationary* porque sua m√©dia e autocovari√¢ncia s√£o constantes ao longo do tempo.
A m√©dia do processo √©:

$$E[Y_t^{(i)}] = E[\mu^{(i)}] + E[\epsilon_t] = 0 + 0 = 0$$

A autocovari√¢ncia no lag zero √©:

$$\gamma_0 = E[(Y_t^{(i)} - E[Y_t^{(i)}])^2] = E[(\mu^{(i)} + \epsilon_t)^2] = E[(\mu^{(i)})^2] + E[(\epsilon_t)^2] + 2E[\mu^{(i)}\epsilon_t] = \lambda^2 + \sigma^2 + 0 = \lambda^2 + \sigma^2$$

As autocovari√¢ncias em todos os outros lags s√£o:

$$\gamma_j = E[(Y_t^{(i)} - E[Y_t^{(i)}])(Y_{t-j}^{(i)} - E[Y_{t-j}^{(i)}])] = E[(\mu^{(i)} + \epsilon_t)(\mu^{(i)} + \epsilon_{t-j})] = E[(\mu^{(i)})^2] + E[\epsilon_t\epsilon_{t-j}] = \lambda^2 + 0 = \lambda^2$$

para $j \neq 0$ [^4]. Note que $E[\epsilon_t \epsilon_{t-j}]=0$ para $j \neq 0$ devido √† propriedade de ru√≠do branco de $\{\epsilon_t\}$. Portanto, as autocovari√¢ncias dependem apenas do lag *j* e n√£o do tempo *t*, satisfazendo a condi√ß√£o de covariance-stationarity.

> üí° **Exemplo Num√©rico:**
>
> Se $\lambda^2 = 0.5$ e $\sigma^2 = 1$, ent√£o $\gamma_0 = 1.5$ e $\gamma_j = 0.5$ para $j \neq 0$.  Isto mostra que as autocovari√¢ncias s√£o constantes ao longo do tempo, demonstrando a estacionariedade covariance.
>
> **C√°lculos:**
>
> $\gamma_0 = \lambda^2 + \sigma^2 = 0.5 + 1 = 1.5$
>
> $\gamma_1 = \lambda^2 = 0.5$
>
> $\gamma_2 = \lambda^2 = 0.5$
>
> ...e assim por diante.

No entanto, este processo *n√£o √© erg√≥dico*. Para ver isso, considere a m√©dia amostral de uma √∫nica realiza√ß√£o *i*:

$$\bar{Y}^{(i)} = \frac{1}{T}\sum_{t=1}^{T}Y_t^{(i)} = \frac{1}{T}\sum_{t=1}^{T}(\mu^{(i)} + \epsilon_t) = \mu^{(i)} + \frac{1}{T}\sum_{t=1}^{T}\epsilon_t$$

Quando $T \to \infty$, a m√©dia amostral converge para $\mu^{(i)}$, devido √† lei dos grandes n√∫meros aplicada ao ru√≠do branco $\epsilon_t$:

$$\text{plim}_{T \to \infty} \bar{Y}^{(i)} = \mu^{(i)} + \text{plim}_{T \to \infty} \frac{1}{T}\sum_{t=1}^{T}\epsilon_t = \mu^{(i)} + 0 = \mu^{(i)}$$

Como $\mu^{(i)}$ √© uma vari√°vel aleat√≥ria espec√≠fica para cada realiza√ß√£o *i* e, em geral, diferente de zero, a m√©dia amostral converge para um valor aleat√≥rio diferente de zero, n√£o convergindo para a m√©dia de conjunto, que √© $E[Y_t] = 0$ [^4]. A m√©dia temporal n√£o converge para a m√©dia do conjunto, violando a condi√ß√£o de ergodicity [^4].

> üí° **Exemplo Num√©rico:**
>
> Suponha que $\lambda^2 = 1$ e $\sigma^2 = 1$. Ent√£o, $\gamma_0 = 2$ e $\gamma_j = 1$ para $j \neq 0$. A m√©dia amostral de uma √∫nica realiza√ß√£o deste processo converge para um valor espec√≠fico de $\mu^{(i)}$ gerado a partir de $N(0, 1)$, e n√£o para a m√©dia do conjunto, que √© 0.
>
> ```python
> import numpy as np
>
> # Definindo os par√¢metros
> lambda_sq = 1
> sigma_sq = 1
>
> # Gerando um valor para mu_i
> np.random.seed(42)
> mu_i = np.random.normal(0, np.sqrt(lambda_sq), 1)[0]
>
> # Gerando o ru√≠do branco
> epsilon = np.random.normal(0, np.sqrt(sigma_sq), 1000)
>
> # Gerando a s√©rie temporal
> Y = mu_i + epsilon
>
> # Calculando a m√©dia amostral
> sample_mean = np.mean(Y)
>
> # Imprimindo os resultados
> print(f"Valor de mu_i: {mu_i:.4f}")
> print(f"M√©dia amostral da s√©rie temporal: {sample_mean:.4f}")
> ```
>
> **Interpreta√ß√£o:**
>
> O c√≥digo gera uma √∫nica realiza√ß√£o da s√©rie temporal com um $\mu^{(i)}$ espec√≠fico. A m√©dia amostral dessa realiza√ß√£o converge para o valor de $\mu^{(i)}$, demonstrando que o processo n√£o √© erg√≥dico, pois a m√©dia amostral n√£o converge para a m√©dia do conjunto (que √© 0).

Para complementar o exemplo acima, podemos verificar a condi√ß√£o da soma das autocovari√¢ncias:
$$\sum_{j=-\infty}^{\infty} |\gamma_j| = |\gamma_0| + \sum_{j \neq 0} |\gamma_j| = \lambda^2 + \sigma^2 + \sum_{j \neq 0} \lambda^2$$
Como a soma $\sum_{j \neq 0} \lambda^2$ diverge, a condi√ß√£o de ergodicity n√£o √© satisfeita.

**Prova:**

I. A autocovari√¢ncia $\gamma_j$ para $j \neq 0$ √© $\lambda^2$, que √© uma constante positiva.

II.  A soma das autocovari√¢ncias absolutas √© ent√£o:
    $$\sum_{j=-\infty}^{\infty} |\gamma_j| = |\gamma_0| + \sum_{j=-\infty, j\neq 0}^{\infty} |\gamma_j| = \lambda^2 + \sigma^2 + \sum_{j=-\infty, j\neq 0}^{\infty} \lambda^2$$

III.  A soma $\sum_{j=-\infty, j\neq 0}^{\infty} \lambda^2$ diverge, pois estamos somando uma constante positiva um n√∫mero infinito de vezes.

IV. Portanto, $\sum_{j=-\infty}^{\infty} |\gamma_j|$ diverge, violando a condi√ß√£o de ergodicity.‚ñ†

√â interessante notar que a n√£o-ergodicity no exemplo acima surge da presen√ßa de um componente fixo aleat√≥rio ($\mu^{(i)}$) que persiste ao longo do tempo. Remover essa depend√™ncia de longo prazo √© crucial para garantir a ergodicity.

**Proposi√ß√£o 1:** *Um processo da forma* $Y_t^{(i)} = \mu^{(i)} + \epsilon_t$ *√© erg√≥dico se e somente se* $\mu^{(i)} = 0$ *para todas as realiza√ß√µes i.*

*Prova:* J√° mostramos que se $\mu^{(i)} \sim N(0, \lambda^2)$ com $\lambda^2 > 0$, ent√£o o processo n√£o √© erg√≥dico. Agora, suponha que $\mu^{(i)} = 0$ para todas as realiza√ß√µes *i*. Ent√£o $Y_t^{(i)} = \epsilon_t$, e a m√©dia amostral converge para $E[\epsilon_t] = 0$, que √© a m√©dia de conjunto. Portanto, o processo √© erg√≥dico.

**Prova:**

I. Dado que $\mu^{(i)} = 0$ para todas as realiza√ß√µes *i*, temos que $Y_t^{(i)} = \epsilon_t$.

II. A m√©dia amostral √© ent√£o:
$$\bar{Y}^{(i)} = \frac{1}{T}\sum_{t=1}^{T}Y_t^{(i)} = \frac{1}{T}\sum_{t=1}^{T}\epsilon_t$$

III. Pela lei dos grandes n√∫meros, $\text{plim}_{T \to \infty} \frac{1}{T}\sum_{t=1}^{T}\epsilon_t = E[\epsilon_t] = 0$.

IV. Portanto, $\text{plim}_{T \to \infty} \bar{Y}^{(i)} = 0$, que √© a m√©dia de conjunto $E[Y_t] = 0$.

V. Assim, o processo √© erg√≥dico. ‚ñ†

**Teorema 2:** *Seja* $Y_t = \mu + \epsilon_t$, *onde* $\mu$ *√© uma constante e* $\epsilon_t$ *√© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia* $\sigma^2$. *Ent√£o* $Y_t$ *√© estacion√°rio e erg√≥dico.*

*Prova:* A estacionariedade segue diretamente, pois $E[Y_t] = \mu$ e $Cov(Y_t, Y_{t+h}) = 0$ para $h \neq 0$. A ergodicity segue da lei forte dos grandes n√∫meros, que garante que a m√©dia amostral converge para $\mu$ quase certamente.

**Prova:**

I. Estacionariedade:
    *   $E[Y_t] = E[\mu + \epsilon_t] = \mu + E[\epsilon_t] = \mu + 0 = \mu$, que √© constante.
    *   $Cov(Y_t, Y_{t+h}) = E[(Y_t - E[Y_t])(Y_{t+h} - E[Y_{t+h}])] = E[(\epsilon_t)(\epsilon_{t+h})]$.
    *   Como $\epsilon_t$ √© ru√≠do branco, $E[\epsilon_t \epsilon_{t+h}] = 0$ para $h \neq 0$.
    *   $Cov(Y_t, Y_{t+h}) = 0$ para $h \neq 0$. Portanto, a autocovari√¢ncia depende apenas de *h* e n√£o de *t*.
    * Conclu√≠mos que $Y_t$ √© covariance-stationary.

II. Ergodicity:
    *   Consideremos a m√©dia amostral: $\bar{Y} = \frac{1}{T}\sum_{t=1}^{T}Y_t = \frac{1}{T}\sum_{t=1}^{T}(\mu + \epsilon_t) = \mu + \frac{1}{T}\sum_{t=1}^{T}\epsilon_t$.
    *   Pela Lei Forte dos Grandes N√∫meros (LSGN), se $\epsilon_t$ √© um processo i.i.d. com m√©dia finita, ent√£o $\frac{1}{T}\sum_{t=1}^{T}\epsilon_t \to E[\epsilon_t] = 0$ quase certamente quando $T \to \infty$.
    *   Portanto, $\bar{Y} \to \mu$ quase certamente quando $T \to \infty$.
    *   Como a m√©dia temporal converge para a m√©dia de conjunto, o processo √© erg√≥dico. ‚ñ†

**Teorema 2.1:** *Seja* $Y_t = f(X_t)$, *onde* $X_t$ *√© um processo estacion√°rio e erg√≥dico, e* $f$ *√© uma fun√ß√£o mensur√°vel. Ent√£o* $Y_t$ *√© estacion√°rio. Se, adicionalmente,* $f$ *for tal que* $E[Y_t]$ *exista e* $\sum_{j=-\infty}^{\infty} |Cov(Y_t, Y_{t+j})| < \infty$, *ent√£o* $Y_t$ *√© erg√≥dico.*

*Prova:* A estacionariedade de $Y_t$ segue diretamente da estacionariedade de $X_t$. A condi√ß√£o $\sum_{j=-\infty}^{\infty} |Cov(Y_t, Y_{t+j})| < \infty$ garante a ergodicity de $Y_t$, como estabelecido por teoremas padr√£o de ergodicity para processos estacion√°rios.

**Prova:**

I.  Estacionariedade de $Y_t$:
    *   Como $X_t$ √© estacion√°rio, sua distribui√ß√£o √© invariante ao longo do tempo.
    *   Como $Y_t = f(X_t)$ e $f$ √© uma fun√ß√£o mensur√°vel, a distribui√ß√£o de $Y_t$ depende apenas da distribui√ß√£o de $X_t$.
    *   Portanto, a distribui√ß√£o de $Y_t$ tamb√©m √© invariante ao longo do tempo, o que implica que $Y_t$ √© estacion√°rio.

II. Ergodicity de $Y_t$:
    *   A condi√ß√£o $\sum_{j=-\infty}^{\infty} |Cov(Y_t, Y_{t+j})| < \infty$ √© a condi√ß√£o suficiente para a ergodicity de um processo estacion√°rio.
    *   Esta condi√ß√£o garante que as depend√™ncias de longo prazo entre as vari√°veis $Y_t$ e $Y_{t+j}$ decaiam suficientemente r√°pido, de modo que a m√©dia amostral convirja para a m√©dia de conjunto.
    *   Portanto, sob esta condi√ß√£o, $Y_t$ √© erg√≥dico. ‚ñ†

### Implica√ß√µes da N√£o-Ergodicity

A n√£o-ergodicity tem implica√ß√µes significativas para a infer√™ncia estat√≠stica e modelagem de s√©ries temporais. Em um processo n√£o-erg√≥dico, a an√°lise de uma √∫nica realiza√ß√£o da s√©rie temporal n√£o fornece informa√ß√µes sobre as propriedades estat√≠sticas do conjunto de poss√≠veis realiza√ß√µes. Isso significa que a m√©dia amostral e as autocovari√¢ncias amostrais calculadas a partir de uma √∫nica realiza√ß√£o n√£o representam a verdadeira m√©dia e autocovari√¢ncias do processo estoc√°stico subjacente [^4].

No contexto do exemplo acima, cada realiza√ß√£o da s√©rie temporal possui uma m√©dia diferente, $\mu^{(i)}$. A an√°lise de uma √∫nica realiza√ß√£o fornece informa√ß√µes sobre esse valor espec√≠fico de $\mu^{(i)}$, mas n√£o sobre a distribui√ß√£o geral de $\mu$ ou sobre o comportamento do processo como um todo. Portanto, a capacidade de generalizar a partir de uma √∫nica trajet√≥ria observada √© perdida.

**Lema 3:** *Para um processo estacion√°rio n√£o-erg√≥dico como* $Y_{t}^{(i)} = \mu^{(i)} + \epsilon_t$, *onde* $\mu^{(i)} \sim N(0, \lambda^2)$, *o estimador da vari√¢ncia baseado em uma √∫nica realiza√ß√£o subestima a vari√¢ncia do processo como um todo.*

*Prova:* A vari√¢ncia de $Y_{t}^{(i)}$ √© $\lambda^2 + \sigma^2$. No entanto, a vari√¢ncia amostral baseada em uma √∫nica realiza√ß√£o converge para $\sigma^2$ quando $T \to \infty$, pois estima apenas a variabilidade de $\epsilon_t$ e n√£o captura a variabilidade de $\mu^{(i)}$ entre as realiza√ß√µes. Formalmente,
$$Var(\bar{Y}^{(i)}) = Var(\mu^{(i)} + \frac{1}{T}\sum_{t=1}^{T}\epsilon_t) = Var(\mu^{(i)}) + Var(\frac{1}{T}\sum_{t=1}^{T}\epsilon_t) = \lambda^2 + \frac{\sigma^2}{T}$$
Quando $T \to \infty$, $Var(\bar{Y}^{(i)})$ converge para $\lambda^2$, que √© a vari√¢ncia da m√©dia amostral entre as realiza√ß√µes, mas n√£o a vari√¢ncia do processo como um todo. A vari√¢ncia do processo como um todo √© dada por $Var(Y_t) = \lambda^2 + \sigma^2$. A vari√¢ncia amostral calculada a partir de uma √∫nica realiza√ß√£o converge para $\sigma^2$. Portanto, a vari√¢ncia amostral subestima a vari√¢ncia do processo.

**Prova:**

I. A vari√¢ncia do processo para uma √∫nica realiza√ß√£o *i* √©:
$Var(Y_t^{(i)}) = Var(\mu^{(i)} + \epsilon_t) = Var(\mu^{(i)}) + Var(\epsilon_t) = 0 + \sigma^2 = \sigma^2$
Note que $\mu^{(i)}$ √© constante para cada *i*, portanto sua vari√¢ncia √© zero.

II. A vari√¢ncia do processo considerando todas as realiza√ß√µes √©:
$Var(Y_t) = E[(Y_t - E[Y_t])^2] = E[(\mu^{(i)} + \epsilon_t - 0)^2] = E[(\mu^{(i)} + \epsilon_t)^2] = E[(\mu^{(i)})^2] + E[(\epsilon_t)^2] + 2E[\mu^{(i)}\epsilon_t] = \lambda^2 + \sigma^2 + 0 = \lambda^2 + \sigma^2$

III. A vari√¢ncia amostral baseada em uma √∫nica realiza√ß√£o converge para:
$\text{plim}_{T \to \infty} \frac{1}{T}\sum_{t=1}^{T}(Y_t^{(i)} - \bar{Y}^{(i)})^2 = Var(Y_t^{(i)}) = \sigma^2$

IV. Portanto, a vari√¢ncia amostral $\sigma^2$ subestima a vari√¢ncia do processo como um todo $\lambda^2 + \sigma^2$, a menos que $\lambda^2 = 0$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere novamente o caso em que $\lambda^2 = 1$ e $\sigma^2 = 1$.  Se observarmos apenas uma realiza√ß√£o da s√©rie temporal $Y_t^{(i)}$, a vari√¢ncia amostral se aproximar√° de 1 (a vari√¢ncia de $\epsilon_t$). No entanto, a verdadeira vari√¢ncia do processo, considerando todas as poss√≠veis realiza√ß√µes, √© $\lambda^2 + \sigma^2 = 2$.  Isto ilustra a subestima√ß√£o da vari√¢ncia devido √† n√£o-ergodicity.
>
> ```python
> import numpy as np
>
> # Definindo os par√¢metros
> lambda_sq = 1
> sigma_sq = 1
>
> # Gerando um valor para mu_i
> np.random.seed(42)
> mu_i = np.random.normal(0, np.sqrt(lambda_sq), 1)[0]
>
> # Gerando o ru√≠do branco
> epsilon = np.random.normal(0, np.sqrt(sigma_sq), 1000)
>
> # Gerando a s√©rie temporal
> Y = mu_i + epsilon
>
> # Calculando a vari√¢ncia amostral
> sample_variance = np.var(Y)
>
> # Imprimindo os resultados
> print(f"Vari√¢ncia amostral da s√©rie temporal: {sample_variance:.4f}")
> print(f"Vari√¢ncia te√≥rica do processo (lambda_sq + sigma_sq): {lambda_sq + sigma_sq}")
> ```
>
> **Interpreta√ß√£o:**
>
> A vari√¢ncia amostral calculada a partir de uma √∫nica realiza√ß√£o ser√° pr√≥xima de 1, enquanto a verdadeira vari√¢ncia do processo √© 2. A diferen√ßa entre as duas demonstra a subestima√ß√£o da vari√¢ncia devido √† n√£o-ergodicity.

### Implica√ß√µes Pr√°ticas e Solu√ß√µes Potenciais

Para contornar os desafios impostos pela n√£o-ergodicity em aplica√ß√µes pr√°ticas, algumas estrat√©gias podem ser consideradas:

1. **Modelagem Hier√°rquica:** Em vez de tratar $\mu^{(i)}$ como um par√¢metro fixo, pode-se model√°-lo como uma vari√°vel aleat√≥ria com uma distribui√ß√£o espec√≠fica (como uma distribui√ß√£o Normal com m√©dia zero e vari√¢ncia $\lambda^2$). Isso leva a modelos hier√°rquicos que capturam a variabilidade entre diferentes realiza√ß√µes.

2. **An√°lise de M√∫ltiplas Realiza√ß√µes:** Se poss√≠vel, coletar e analisar m√∫ltiplas realiza√ß√µes da s√©rie temporal. Isso permite estimar a distribui√ß√£o de $\mu^{(i)}$ e obter infer√™ncias mais precisas sobre o processo como um todo.

3. **T√©cnicas de Suaviza√ß√£o:** Utilizar t√©cnicas de suaviza√ß√£o para reduzir o impacto do componente aleat√≥rio $\mu^{(i)}$ nas estimativas. Isso pode envolver o uso de m√©dias m√≥veis ou outros filtros que atenuam a variabilidade de curto prazo.

4. **Transforma√ß√µes:** Aplicar transforma√ß√µes aos dados para tentar remover o componente n√£o-erg√≥dico. Por exemplo, se suspeitarmos que a n√£o-ergodicity √© causada por uma tend√™ncia determin√≠stica, podemos diferenciar a s√©rie.

5. **Modelos de Painel:** Se tivermos m√∫ltiplas realiza√ß√µes da s√©rie temporal, podemos usar modelos de painel para estimar os par√¢metros do modelo. Modelos de efeitos fixos podem ser usados para remover o efeito de $\mu^{(i)}$, enquanto modelos de efeitos aleat√≥rios podem ser usados para estimar a vari√¢ncia de $\mu^{(i)}$.

**Proposi√ß√£o 4:** *Em um modelo de efeitos aleat√≥rios aplicado a m√∫ltiplas realiza√ß√µes de* $Y_{t}^{(i)} = \mu^{(i)} + \epsilon_t$, *o estimador da vari√¢ncia entre as unidades (i.e., entre as realiza√ß√µes) converge para* $\lambda^2$.

*Prova:* Em um modelo de efeitos aleat√≥rios, a vari√¢ncia total √© decomposta em vari√¢ncia entre as unidades ($\lambda^2$) e vari√¢ncia dentro das unidades ($\sigma^2$). O estimador da vari√¢ncia entre as unidades, no limite, captura a variabilidade de $\mu^{(i)}$ entre as realiza√ß√µes, que √© dada por $\lambda^2$. Formalmente, o estimador de efeitos aleat√≥rios √© consistente e converge para os verdadeiros par√¢metros do modelo, incluindo as vari√¢ncias dos efeitos aleat√≥rios.

**Prova:**

I. Em um modelo de efeitos aleat√≥rios, temos:
$Y_{t}^{(i)} = \mu + \mu^{(i)} + \epsilon_t$, onde $\mu^{(i)} \sim N(0, \lambda^2)$ e $\epsilon_t \sim N(0, \sigma^2)$.

II. A vari√¢ncia total de $Y_{t}^{(i)}$ √© $Var(Y_{t}^{(i)}) = Var(\mu^{(i)} + \epsilon_t) = Var(\mu^{(i)}) + Var(\epsilon_t) = \lambda^2 + \sigma^2$.

III. O estimador de efeitos aleat√≥rios decomp√µe a vari√¢ncia total em vari√¢ncia entre unidades (entre as diferentes realiza√ß√µes *i*) e vari√¢ncia dentro das unidades (variabilidade ao longo do tempo *t* para uma dada realiza√ß√£o *i*).

IV. O estimador da vari√¢ncia entre as unidades, denotado por $\hat{\lambda}^2$, converge para a verdadeira vari√¢ncia entre as unidades, ou seja, $\text{plim} \hat{\lambda}^2 = \lambda^2$.

V.  Formalmente, a converg√™ncia do estimador de efeitos aleat√≥rios garante que, com um n√∫mero suficientemente grande de realiza√ß√µes, o estimador da vari√¢ncia entre unidades converge para a vari√¢ncia do componente aleat√≥rio $\mu^{(i)}$, que √© $\lambda^2$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Imagine que voc√™ tem dados de vendas de 100 lojas diferentes ($i = 1, 2, ..., 100$) ao longo de 50 semanas ($t = 1, 2, ..., 50$). Cada loja tem um n√≠vel de vendas base diferente ($\mu^{(i)}$) que √© constante ao longo do tempo, mas diferente entre as lojas.  Al√©m disso, h√° flutua√ß√µes aleat√≥rias nas vendas semanais ($\epsilon_t$). Um modelo de efeitos aleat√≥rios permitiria estimar a vari√¢ncia do n√≠vel de vendas base entre as lojas ($\lambda^2$) e a vari√¢ncia das flutua√ß√µes semanais ($\sigma^2$).
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.formula.api as sm
>
> # Definindo os par√¢metros
> n_stores = 100
> n_weeks = 50
> lambda_sq = 2
> sigma_sq = 1
>
> # Gerando os dados
> np.random.seed(42)
> store_effects = np.random.normal(0, np.sqrt(lambda_sq), n_stores)
> data = []
> for i in range(n_stores):
>     for t in range(n_weeks):
>         epsilon = np.random.normal(0, np.sqrt(sigma_sq))
>         sales = store_effects[i] + epsilon
>         data.append({'store_id': i, 'week': t, 'sales': sales})
>
> df = pd.DataFrame(data)
>
> # Estimando o modelo de efeitos aleat√≥rios
> md = sm.mixedlm("sales ~ 1", data=df, groups=df["store_id"], re_formula="~1")
> mdf = md.fit()
>
> # Imprimindo os resultados
> print(mdf.summary())
> print(f"Vari√¢ncia estimada entre lojas: {mdf.cov_re.iloc[0,0]:.4f}")
> print(f"Vari√¢ncia te√≥rica entre lojas (lambda_sq): {lambda_sq}")
> ```
>
> **Interpreta√ß√£o:**
>
> O c√≥digo simula dados de vendas de v√°rias lojas e ajusta um modelo de efeitos aleat√≥rios. A vari√¢ncia estimada entre as lojas, obtida do modelo, ser√° pr√≥xima ao valor de $\lambda^2$ usado na simula√ß√£o, confirmando que o modelo de efeitos aleat√≥rios consegue estimar a vari√¢ncia do componente n√£o-erg√≥dico.

### Conclus√£o

A distin√ß√£o entre estacionariedade e ergodicity √© crucial para a an√°lise de s√©ries temporais. Enquanto a estacionariedade garante que as propriedades estat√≠sticas do processo n√£o mudem ao longo do tempo, a ergodicity assegura que as m√©dias temporais convergem para as m√©dias de conjunto, permitindo infer√™ncias sobre o processo subjacente a partir de uma √∫nica realiza√ß√£o observada [^4]. Processos que s√£o estacion√°rios, mas n√£o ergodicos, apresentam desafios adicionais para a infer√™ncia estat√≠stica, exigindo abordagens de modelagem mais sofisticadas para capturar a variabilidade entre diferentes realiza√ß√µes. Ao compreender essas sutilezas, podemos evitar conclus√µes err√¥neas e desenvolver modelos mais precisos e √∫teis para a an√°lise de s√©ries temporais.

### Refer√™ncias

[^4]: P√°gina 47 do documento.
<!-- END -->