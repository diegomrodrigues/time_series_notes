## Autoregressive (AR) Processes: Stationarity Condition in AR(1)

### Introdu√ß√£o

Em continuidade ao cap√≠tulo anterior sobre processos **autorregressivos (AR)**, particularmente o processo de primeira ordem **AR(1)**, este cap√≠tulo aprofunda-se na condi√ß√£o essencial para que um processo AR(1) seja **estacion√°rio em covari√¢ncia**. Como vimos anteriormente, a estacionariedade √© crucial para a validade de muitas an√°lises e previs√µes em s√©ries temporais [^53]. Exploraremos por que a condi√ß√£o $|\phi| < 1$ √© necess√°ria, suas implica√ß√µes te√≥ricas e consequ√™ncias pr√°ticas quando essa condi√ß√£o n√£o √© satisfeita.

### Conceitos Fundamentais

Conforme definido anteriormente, um processo AR(1) √© dado por [^53]:

$$Y_t = c + \phi Y_{t-1} + \epsilon_t$$

Para que este processo seja **estacion√°rio em covari√¢ncia**, √© imperativo que sua m√©dia e autocovari√¢ncias n√£o dependam do tempo $t$. Isso implica que a vari√¢ncia de $Y_t$ seja finita e constante ao longo do tempo. A condi√ß√£o $|\phi| < 1$ garante esta estabilidade.

> üí° **Condi√ß√£o de Estacionariedade:**
>
> *Para um processo AR(1) ser estacion√°rio em covari√¢ncia, a condi√ß√£o* $|\phi| < 1$ *deve ser satisfeita. Caso contr√°rio, o processo n√£o ser√° estacion√°rio, e a vari√¢ncia de* $Y_t$ *aumentar√° indefinidamente com o tempo.*

**Justificativa Matem√°tica da Condi√ß√£o** $|\phi| < 1$:

Come√ßamos expressando $Y_t$ em termos de sua representa√ß√£o MA(‚àû) [^53]:

$$Y_t = \frac{c}{1 - \phi} + \sum_{j=0}^{\infty} \phi^j \epsilon_{t-j}$$

A vari√¢ncia de $Y_t$ √© dada por:

$$Var(Y_t) = Var\left(\frac{c}{1 - \phi} + \sum_{j=0}^{\infty} \phi^j \epsilon_{t-j}\right) = \sum_{j=0}^{\infty} \phi^{2j} Var(\epsilon_{t-j}) = \sigma^2 \sum_{j=0}^{\infty} \phi^{2j}$$

Para que a vari√¢ncia seja finita, a s√©rie $\sum_{j=0}^{\infty} \phi^{2j}$ deve convergir. Esta √© uma s√©rie geom√©trica que converge se e somente se $|\phi^2| < 1$, o que √© equivalente a $|\phi| < 1$. Se $|\phi| \geq 1$, a s√©rie diverge, e a vari√¢ncia de $Y_t$ se torna infinita, violando a condi√ß√£o de estacionariedade.

> üìù **Observa√ß√£o:**
>
> Se $|\phi| = 1$, a s√©rie temporal exibe um comportamento de *random walk* (passeio aleat√≥rio) com ou sem *drift*, e n√£o √© estacion√°ria em covari√¢ncia.

**Proposi√ß√£o 3:** Se $|\phi| \geq 1$, a vari√¢ncia de $Y_t$ n√£o √© finita, e o processo AR(1) n√£o √© estacion√°rio em covari√¢ncia.

*Proof:* J√° demonstramos que a vari√¢ncia de $Y_t$ √© dada por $\sigma^2 \sum_{j=0}^{\infty} \phi^{2j}$. Se $|\phi| \geq 1$, ent√£o $\phi^{2j}$ n√£o converge para zero √† medida que $j$ aumenta, e a s√©rie $\sum_{j=0}^{\infty} \phi^{2j}$ diverge. Portanto, a vari√¢ncia de $Y_t$ √© infinita, e o processo n√£o pode ser estacion√°rio em covari√¢ncia. $\blacksquare$

Para complementar a Proposi√ß√£o 3, podemos analisar a m√©dia do processo AR(1) quando $|\phi| \geq 1$:

**Proposi√ß√£o 3.1:** Se $|\phi| \geq 1$ e $c \neq 0$, a m√©dia do processo AR(1) n√£o √© finita ou n√£o existe, dependendo das condi√ß√µes iniciais.

*Proof:*
Reescrevendo a equa√ß√£o AR(1), temos $Y_t = c + \phi Y_{t-1} + \epsilon_t$. Suponha que o processo comece em $Y_0$. Ent√£o,
$Y_1 = c + \phi Y_0 + \epsilon_1$
$Y_2 = c + \phi Y_1 + \epsilon_2 = c + \phi(c + \phi Y_0 + \epsilon_1) + \epsilon_2 = c(1 + \phi) + \phi^2 Y_0 + \phi \epsilon_1 + \epsilon_2$
$Y_3 = c + \phi Y_2 + \epsilon_3 = c + \phi(c(1 + \phi) + \phi^2 Y_0 + \phi \epsilon_1 + \epsilon_2) + \epsilon_3 = c(1 + \phi + \phi^2) + \phi^3 Y_0 + \phi^2 \epsilon_1 + \phi \epsilon_2 + \epsilon_3$

Generalizando,
$Y_t = c \sum_{i=0}^{t-1} \phi^i + \phi^t Y_0 + \sum_{i=0}^{t-1} \phi^i \epsilon_{t-i}$

Se $|\phi| > 1$, o termo $c \sum_{i=0}^{t-1} \phi^i$ diverge √† medida que $t$ aumenta, e o termo $\phi^t Y_0$ tamb√©m diverge.  Se $\phi = 1$, ent√£o $Y_t = ct + Y_0 + \sum_{i=0}^{t-1} \epsilon_{t-i}$, que tamb√©m diverge √† medida que $t$ aumenta.

Portanto, a m√©dia de $Y_t$ n√£o converge para um valor finito quando $|\phi| \geq 1$ e $c \neq 0$, indicando que o processo n√£o √© estacion√°rio. $\blacksquare$

**Implica√ß√µes da N√£o Estacionariedade:**

1.  **Previs√µes N√£o Confi√°veis:** Modelos estacion√°rios s√£o cruciais para previs√µes de longo prazo confi√°veis. Se $|\phi| \geq 1$, as previs√µes baseadas no modelo AR(1) podem se tornar cada vez mais incertas com o tempo, perdendo a capacidade de capturar a din√¢mica subjacente da s√©rie temporal.
2.  **Infer√™ncia Estat√≠stica Inv√°lida:** Muitos testes estat√≠sticos e t√©cnicas de estima√ß√£o dependem da estacionariedade dos dados. Se o processo n√£o for estacion√°rio, as estat√≠sticas amostrais, como a m√©dia e a vari√¢ncia, podem variar significativamente ao longo do tempo, tornando a infer√™ncia estat√≠stica inv√°lida.
3.  **Acumula√ß√£o de Choques:** Quando $|\phi| \geq 1$, os choques aleat√≥rios (representados por $\epsilon_t$) acumulam-se ao longo do tempo, em vez de diminuir, resultando em um processo que n√£o retorna a uma m√©dia est√°vel.

**Exemplo Num√©rico:**

Considere o processo AR(1) com $c = 0$ e $\sigma^2 = 1$.

*   **Caso 1:** $\phi = 0.9$ (Estacion√°rio)

    A vari√¢ncia √© $\frac{\sigma^2}{1 - \phi^2} = \frac{1}{1 - 0.9^2} = \frac{1}{0.19} \approx 5.26$. A s√©rie √© estacion√°ria, e a vari√¢ncia √© finita.
*   **Caso 2:** $\phi = 1$ (N√£o Estacion√°rio - Random Walk)

    A vari√¢ncia seria infinita, indicando que o processo n√£o √© estacion√°rio e acumula choques aleat√≥rios indefinidamente.
*   **Caso 3:** $\phi = 1.1$ (N√£o Estacion√°rio)

    A vari√¢ncia tamb√©m seria infinita, e a s√©rie divergir√° exponencialmente.

**Representa√ß√£o em Termos de Equa√ß√µes de Diferen√ßa:**

Como mencionado anteriormente, a equa√ß√£o AR(1) $Y_t = c + \phi Y_{t-1} + \epsilon_t$ pode ser vista como uma equa√ß√£o de diferen√ßa de primeira ordem [^53]. A estabilidade da solu√ß√£o para essa equa√ß√£o depende da magnitude de $\phi$ [^53]. Se $|\phi| < 1$, a solu√ß√£o para a equa√ß√£o de diferen√ßa √© est√°vel, e o processo converge para um valor est√°vel √† medida que o tempo avan√ßa. Se $|\phi| \geq 1$, a solu√ß√£o √© inst√°vel, e o processo diverge.

**Rela√ß√£o com Invertibilidade:**

Embora a estacionariedade e a invertibilidade sejam conceitos distintos, eles s√£o frequentemente confundidos. Para processos AR, a condi√ß√£o $|\phi| < 1$ garante a estacionariedade. Para processos MA, a condi√ß√£o para invertibilidade √© an√°loga em termos de restringir os par√¢metros do modelo.

**Discuss√£o sobre a Representa√ß√£o MA(‚àû):**

A representa√ß√£o MA(‚àû) [^53] fornece uma vis√£o valiosa da condi√ß√£o de estacionariedade. Ao expressar $Y_t$ como uma soma ponderada de termos de erro passados, torna-se claro que a condi√ß√£o $|\phi| < 1$ garante que o impacto dos choques passados diminua ao longo do tempo, evitando que o processo acumule choques indefinidamente e mantenha uma vari√¢ncia finita.

> üí° **Exemplo Num√©rico:**
>
> Considere um processo AR(1) com $\phi = 0.5$, $c = 1$, e $\epsilon_t \sim N(0, 0.25)$. Podemos simular 100 observa√ß√µes deste processo e analisar seu comportamento.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> phi = 0.5
> c = 1
> sigma = 0.5  # Desvio padr√£o do ru√≠do branco
>
> # Inicializa√ß√£o
> Y = np.zeros(100)
> epsilon = np.random.normal(0, sigma, 100)
> Y[0] = c + epsilon[0]  # Valor inicial
>
> # Simula√ß√£o do processo AR(1)
> for t in range(1, 100):
>     Y[t] = c + phi * Y[t-1] + epsilon[t]
>
> # Plot da s√©rie temporal
> plt.plot(Y)
> plt.title('Simula√ß√£o de um Processo AR(1) Estacion√°rio')
> plt.xlabel('Tempo')
> plt.ylabel('Valor')
> plt.show()
>
> # C√°lculo da m√©dia amostral
> media_amostral = np.mean(Y)
> print(f'M√©dia amostral: {media_amostral}')
>
> # C√°lculo da vari√¢ncia amostral
> variancia_amostral = np.var(Y)
> print(f'Vari√¢ncia amostral: {variancia_amostral}')
>
> # M√©dia te√≥rica
> media_teorica = c / (1 - phi)
> print(f'M√©dia te√≥rica: {media_teorica}')
>
> # Vari√¢ncia te√≥rica
> variancia_teorica = sigma**2 / (1 - phi**2)
> print(f'Vari√¢ncia te√≥rica: {variancia_teorica}')
> ```
>
> Este c√≥digo simula um processo AR(1) estacion√°rio. Ao plotar a s√©rie temporal, observamos que ela flutua em torno de uma m√©dia constante. Os valores da m√©dia e vari√¢ncia amostrais devem ser pr√≥ximos aos valores te√≥ricos.

**Teorema 2:** Para um processo AR(1) definido por $Y_t = c + \phi Y_{t-1} + \epsilon_t$, onde $\{\epsilon_t\}$ √© ru√≠do branco com vari√¢ncia $\sigma^2$, a condi√ß√£o necess√°ria e suficiente para estacionariedade em covari√¢ncia √© $|\phi| < 1$.

*Proof:* J√° mostramos que $|\phi| < 1$ √© suficiente para estacionariedade em covari√¢ncia. Para mostrar que √© necess√°rio, suponha que $Y_t$ seja estacion√°rio em covari√¢ncia. Ent√£o, $E[Y_t^2] < \infty$ para todo $t$. Da representa√ß√£o MA(‚àû), $E[Y_t^2] = \frac{c^2}{(1-\phi)^2} + \sigma^2 \sum_{j=0}^\infty \phi^{2j}$. Se $|\phi| \geq 1$, ent√£o $\sum_{j=0}^\infty \phi^{2j}$ diverge e $E[Y_t^2] = \infty$, contradizendo a estacionariedade em covari√¢ncia. Portanto, $|\phi| < 1$ √© necess√°rio. $\blacksquare$

Para complementar o Teorema 2, podemos demonstrar um resultado que fornece uma caracteriza√ß√£o alternativa da estacionariedade em termos da fun√ß√£o de autocorrela√ß√£o:

**Teorema 2.1:** Um processo AR(1) definido por $Y_t = c + \phi Y_{t-1} + \epsilon_t$, onde $\{\epsilon_t\}$ √© ru√≠do branco com vari√¢ncia $\sigma^2$, √© estacion√°rio em covari√¢ncia se e somente se sua fun√ß√£o de autocorrela√ß√£o $\rho_j$ decai para zero √† medida que $j$ tende ao infinito.

*Proof:*
J√° sabemos que, para um processo AR(1) estacion√°rio, $\rho_j = \phi^j$. Se $|\phi| < 1$, ent√£o $\lim_{j \to \infty} \rho_j = \lim_{j \to \infty} \phi^j = 0$.

Agora, suponha que $\lim_{j \to \infty} \rho_j = 0$. Como $\rho_j = \phi^j$, isso implica que $\lim_{j \to \infty} \phi^j = 0$. Isso s√≥ √© poss√≠vel se $|\phi| < 1$. Portanto, a condi√ß√£o de que a fun√ß√£o de autocorrela√ß√£o decai para zero √© equivalente a $|\phi| < 1$, que √© a condi√ß√£o necess√°ria e suficiente para estacionariedade em covari√¢ncia conforme estabelecido no Teorema 2. $\blacksquare$

**Corol√°rio 2.1:** Se um processo AR(1) √© estacion√°rio em covari√¢ncia, ent√£o a soma dos valores absolutos de sua fun√ß√£o de autocorrela√ß√£o converge, ou seja, $\sum_{j=0}^{\infty} |\rho_j| < \infty$.

*Proof:* Se o processo AR(1) √© estacion√°rio, ent√£o $|\phi| < 1$. A fun√ß√£o de autocorrela√ß√£o √© dada por $\rho_j = \phi^j$. Portanto, $\sum_{j=0}^{\infty} |\rho_j| = \sum_{j=0}^{\infty} |\phi^j| = \sum_{j=0}^{\infty} |\phi|^j$. Como $|\phi| < 1$, esta √© uma s√©rie geom√©trica convergente, e sua soma √© $\frac{1}{1 - |\phi|}$, que √© finita. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Considere um processo AR(1) com $\phi = 0.8$. A fun√ß√£o de autocorrela√ß√£o √© $\rho_j = (0.8)^j$. Podemos calcular a soma dos valores absolutos das autocorrela√ß√µes:
>
> $\sum_{j=0}^{\infty} |\rho_j| = \sum_{j=0}^{\infty} |0.8|^j = \frac{1}{1 - 0.8} = \frac{1}{0.2} = 5$
>
> Como a soma √© finita (igual a 5), o processo √© estacion√°rio, confirmando o Corol√°rio 2.1.

**Discuss√£o sobre a escolha de c:**
A constante $c$ em $Y_t = c + \phi Y_{t-1} + \epsilon_t$ desempenha um papel importante no n√≠vel da s√©rie temporal, influenciando diretamente a m√©dia do processo.
Dado que a m√©dia do processo √© $\mu = \frac{c}{1-\phi}$, podemos expressar $c$ em fun√ß√£o de $\mu$ e $\phi$: $c = \mu(1-\phi)$.
Dessa forma, podemos reescrever o processo AR(1) em termos do desvio da sua m√©dia:

$Y_t - \mu = \phi(Y_{t-1} - \mu) + \epsilon_t$

Esta formula√ß√£o √© √∫til porque isola o componente autoregressivo do processo, centrando a s√©rie temporal em torno de sua m√©dia e facilitando a an√°lise das flutua√ß√µes em rela√ß√£o a essa m√©dia.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um processo AR(1) com m√©dia $\mu = 10$ e $\phi = 0.6$. Ent√£o, $c = 10(1 - 0.6) = 4$.
>
> A equa√ß√£o do processo AR(1) pode ser escrita como:
>
> $Y_t = 4 + 0.6 Y_{t-1} + \epsilon_t$
>
> Alternativamente, em termos do desvio da m√©dia:
>
> $Y_t - 10 = 0.6 (Y_{t-1} - 10) + \epsilon_t$
>
> Esta segunda forma destaca as flutua√ß√µes em torno da m√©dia de 10.

### Conclus√£o

A condi√ß√£o $|\phi| < 1$ √© fundamental para a estacionariedade em covari√¢ncia de um processo AR(1) [^53]. Essa condi√ß√£o assegura que a vari√¢ncia do processo permane√ßa finita, permitindo an√°lises estat√≠sticas e previs√µes confi√°veis. Quando essa condi√ß√£o n√£o √© satisfeita, o processo se torna n√£o estacion√°rio, resultando em propriedades estat√≠sticas vari√°veis no tempo e previs√µes n√£o confi√°veis. A an√°lise das propriedades de estacionariedade e n√£o estacionariedade √© crucial para a correta modelagem e interpreta√ß√£o de s√©ries temporais. Este cap√≠tulo refor√ßa a import√¢ncia de verificar essa condi√ß√£o ao construir e utilizar modelos AR(1), pavimentando o caminho para uma compreens√£o mais profunda dos processos AR de ordem superior e modelos ARMA.

### Refer√™ncias
[^53]: P√°gina 53
## Modelos Autoregressivos (AR)
### O Processo Autoregressivo de Primeira Ordem
Um processo autoregressivo de primeira ordem, denotado AR(1), satisfaz a seguinte equa√ß√£o de diferen√ßa:

$Y_t = c + \phi Y_{t-1} + \epsilon_t$ [^53]

Onde $\{\epsilon_t\}$ √© uma sequ√™ncia de ru√≠do branco que satisfaz as condi√ß√µes de [3.2.1] a [3.2.3] [^53]. √â importante notar que a equa√ß√£o [3.4.1] [^53] assume a forma da equa√ß√£o de diferen√ßa de primeira ordem [1.1.1] ou [2.2.1], onde a vari√°vel de entrada $w_t$ √© dada por $w_t = c + \epsilon_t$ [^53].

Como vimos anteriormente, a an√°lise de equa√ß√µes de diferen√ßa de primeira ordem revela que se $|\phi| \geq 1$, os efeitos dos $\epsilon_t$ em $Y$ se acumulam ao inv√©s de desaparecer ao longo do tempo [^53]. Portanto, n√£o √© surpreendente que, quando $|\phi| \geq 1$, n√£o exista um processo estacion√°rio de covari√¢ncia para $Y$ com vari√¢ncia finita que satisfa√ßa [3.4.1] [^53]. No caso em que $|\phi| < 1$, existe um processo estacion√°rio de covari√¢ncia para $Y$ que satisfaz [3.4.1] [^53]. Este processo √© dado pela solu√ß√£o est√°vel de [3.4.1] [^53], caracterizada em [2.2.9] [^53]:

$Y_t = (c + \epsilon_t) + \phi(c + \epsilon_{t-1}) + \phi^2(c + \epsilon_{t-2}) + \phi^3(c + \epsilon_{t-3}) + \ldots$
$Y_t = \frac{c}{1-\phi} + \epsilon_t + \phi\epsilon_{t-1} + \phi^2\epsilon_{t-2} + \phi^3\epsilon_{t-3} + \ldots$ [^53]

Esta equa√ß√£o pode ser vista como um processo MA($\infty$) como em [3.3.13] [^53], com $\psi_j$ dado por $\phi^j$ [^53]. Quando $|\phi| < 1$, a condi√ß√£o [3.3.15] √© satisfeita [^53]:

$\sum_{j=0}^{\infty} |\psi_j| = \sum_{j=0}^{\infty} |\phi^j|$ [^53]

Esta soma √© igual a $\frac{1}{1-|\phi|}$ [^53], desde que $|\phi| < 1$ [^53]. O restante desta discuss√£o sobre processos autoregressivos de primeira ordem assume que $|\phi| < 1$ [^53]. Isso garante que a representa√ß√£o MA($\infty$) exista e possa ser manipulada da maneira usual, e que o processo AR(1) seja *erg√≥dico* para a m√©dia [^53].

Tomando as expectativas de [3.4.2] [^53], vemos que:

$E(Y_t) = \frac{c}{1-\phi} + 0 + 0 + \ldots$ [^53]

Portanto, a m√©dia de um processo estacion√°rio AR(1) √©:

$$\mu = \frac{c}{1-\phi}$$ [^53]

> üí° **Exemplo Num√©rico:**
>
> Se $c = 5$ e $\phi = 0.5$, a m√©dia do processo AR(1) √©:
>
> $\mu = \frac{5}{1 - 0.5} = \frac{5}{0.5} = 10$
>
> Isso significa que, em m√©dia, os valores da s√©rie temporal se concentrar√£o em torno de 10.

A vari√¢ncia √©:

$\gamma_0 = E(Y_t - \mu)^2 = E(\epsilon_t + \phi\epsilon_{t-1} + \phi^2\epsilon_{t-2} + \phi^3\epsilon_{t-3} + \ldots)^2$
$\gamma_0 = (1 + \phi^2 + \phi^4 + \phi^6 + \ldots) \sigma^2$
$$\gamma_0 = \frac{\sigma^2}{1-\phi^2}$$ [^53]

> üí° **Exemplo Num√©rico:**
>
> Se $\sigma^2 = 2$ e $\phi = 0.5$, a vari√¢ncia do processo AR(1) √©:
>
> $\gamma_0 = \frac{2}{1 - 0.5^2} = \frac{2}{1 - 0.25} = \frac{2}{0.75} = \frac{8}{3} \approx 2.67$
>
> Isso quantifica a dispers√£o dos valores da s√©rie temporal em torno de sua m√©dia.

Enquanto a *j-√©sima autocovari√¢ncia* √©:

$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\epsilon_t + \phi\epsilon_{t-1} + \phi^2\epsilon_{t-2} + \ldots + \phi^j\epsilon_{t-j} + \phi^{j+1}\epsilon_{t-j-1} + \ldots) (\epsilon_{t-j} + \phi\epsilon_{t-j-1} + \phi^2\epsilon_{t-j-2} + \ldots)]$ [^53]
$\gamma_j = [\phi^j + \phi^{j+2} + \phi^{j+4} + \ldots]\sigma^2$
$$\gamma_j = \frac{\phi^j}{1-\phi^2}\sigma^2$$ [^53]

> üí° **Exemplo Num√©rico:**
>
> Se $\sigma^2 = 2$, $\phi = 0.5$, e $j = 2$, a autocovari√¢ncia de ordem 2 √©:
>
> $\gamma_2 = \frac{(0.5)^2}{1 - (0.5)^2} \times 2 = \frac{0.25}{0.75} \times 2 = \frac{1}{3} \times 2 = \frac{2}{3} \approx 0.67$
>
> Isso mede a covari√¢ncia entre os valores da s√©rie temporal separados por dois per√≠odos de tempo.

### A Fun√ß√£o de Autocorrela√ß√£o

Segue-se de [3.4.4] e [3.4.5] [^53] que a *fun√ß√£o de autocorrela√ß√£o* √©:

$$\rho_j = \frac{\gamma_j}{\gamma_0} = \phi^j$$ [^53]

> üí° **Exemplo Num√©rico:**
>
> Para um processo AR(1) com $\phi = 0.7$, as primeiras tr√™s autocorrela√ß√µes s√£o:
>
> *   $\rho_0 = (0.7)^0 = 1$
> *   $\rho_1 = (0.7)^1 = 0.7$
> *   $\rho_2 = (0.7)^2 = 0.49$
>
> Isso mostra como a correla√ß√£o entre os valores da s√©rie temporal diminui √† medida que a separa√ß√£o de tempo aumenta.

√â interessante notar que a fun√ß√£o de autocorrela√ß√£o $\rho_j$ de um processo AR(1) estacion√°rio satisfaz a seguinte rela√ß√£o de recorr√™ncia:

**Proposi√ß√£o 4:** Para um processo AR(1) estacion√°rio, $\rho_j = \phi \rho_{j-1}$ para $j \geq 1$.

*Proof:* Sabemos que $\rho_j = \phi^j$ para um processo AR(1). Ent√£o, $\phi \rho_{j-1} = \phi (\phi^{j-1}) = \phi^j = \rho_j$. Portanto, $\rho_j = \phi \rho_{j-1}$. $\blacksquare$

Essa propriedade da fun√ß√£o de autocorrela√ß√£o pode ser √∫til para identificar e estimar modelos AR(1) em s√©ries temporais.

Al√©m disso, podemos caracterizar o decaimento da fun√ß√£o de autocorrela√ß√£o para diferentes valores de $\phi$:

**Observa√ß√£o 4.1:** O comportamento da fun√ß√£o de autocorrela√ß√£o $\rho_j = \phi^j$ depende do sinal e da magnitude de $\phi$:

*   Se $0 < \phi < 1$, $\rho_j$ decresce exponencialmente para 0 √† medida que $j$ aumenta.
*   Se $-1 < \phi < 0$, $\rho_j$ alterna em sinal e decresce exponencialmente para 0 √† medida que $j$ aumenta.  Nesse caso, as autocorrela√ß√µes em lags adjacentes ter√£o sinais opostos.
*   Se $\phi = 0$, $\rho_j = 0$ para todo $j > 0$, indicando que $Y_t$ √© um ru√≠do branco.

> üí° **Exemplo Num√©rico:**
>
> Vamos comparar as fun√ß√µes de autocorrela√ß√£o para dois processos AR(1):
>
> *   Processo 1: $\phi = 0.6$
> *   Processo 2: $\phi = -0.6$
>
> Para o Processo 1, as primeiras autocorrela√ß√µes s√£o: $\rho_0 = 1$, $\rho_1 = 0.6$, $\rho_2 = 0.36$, $\rho_3 = 0.216$. A autocorrela√ß√£o decresce exponencialmente e permanece positiva.
>
> Para o Processo 2, as primeiras autocorrela√ß√µes s√£o: $\rho_0 = 1$, $\rho_1 = -0.6$, $\rho_2 = 0.36$, $\rho_3 = -0.216$. A autocorrela√ß√£o alterna em sinal e decresce exponencialmente em magnitude.

Essa observa√ß√£o fornece insights sobre a estrutura de depend√™ncia temporal capturada pelo processo AR(1) com base no valor do par√¢metro $\phi$.
[^53]: P√°gina 54
<!-- END -->