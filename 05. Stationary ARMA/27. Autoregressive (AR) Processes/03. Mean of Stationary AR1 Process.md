## Autoregressive (AR) Processes: The Mean of a Stationary AR(1) Process

### Introdu√ß√£o

Em continuidade √† discuss√£o sobre processos **autorregressivos (AR)** e a condi√ß√£o de estacionariedade em covari√¢ncia, este cap√≠tulo se dedica a explorar a deriva√ß√£o e o significado da **m√©dia** de um processo AR(1) estacion√°rio. Conforme demonstrado anteriormente, um processo AR(1) √© definido pela equa√ß√£o [^53]:

$$Y_t = c + \phi Y_{t-1} + \epsilon_t$$

onde $|\phi| < 1$ para garantir a estacionariedade. Este cap√≠tulo se concentrar√° na deriva√ß√£o da m√©dia $\mu$ e suas implica√ß√µes para o comportamento do processo AR(1).

### Conceitos Fundamentais

Para derivar a m√©dia de um processo AR(1) estacion√°rio, come√ßamos assumindo que o processo √© **estacion√°rio em covari√¢ncia**, o que implica que a m√©dia $E(Y_t) = \mu$ √© constante ao longo do tempo [^53]. Tomando a expectativa de ambos os lados da equa√ß√£o AR(1) [^54]:

$$E(Y_t) = E(c + \phi Y_{t-1} + \epsilon_t)$$

Usando a linearidade da expectativa, temos:

$$E(Y_t) = E(c) + \phi E(Y_{t-1}) + E(\epsilon_t)$$

Dado que $E(Y_t) = E(Y_{t-1}) = \mu$ e $E(\epsilon_t) = 0$ (j√° que $\epsilon_t$ √© ru√≠do branco) [^47], obtemos:

$$\mu = c + \phi \mu + 0$$

Resolvendo para $\mu$, encontramos:

$$\mu - \phi \mu = c$$
$$\mu(1 - \phi) = c$$
$$\mu = \frac{c}{1 - \phi}$$

Este resultado demonstra que a m√©dia de um processo AR(1) estacion√°rio depende tanto da constante $c$ quanto do coeficiente autorregressivo $\phi$.

**Teorema 3:** Para um processo AR(1) estacion√°rio definido por $Y_t = c + \phi Y_{t-1} + \epsilon_t$, a m√©dia √© dada por $\mu = \frac{c}{1 - \phi}$, onde $|\phi| < 1$.

*Proof:* A prova foi apresentada acima, derivada diretamente da defini√ß√£o do processo AR(1) e da suposi√ß√£o de estacionariedade em covari√¢ncia. $\blacksquare$

A seguir, podemos demonstrar um resultado que mostra que os desvios do processo da sua m√©dia s√£o autorregressivos:

**Teorema 3.1:** Seja $Y_t$ um processo AR(1) estacion√°rio definido por $Y_t = c + \phi Y_{t-1} + \epsilon_t$, onde $|\phi| < 1$. Ent√£o, o processo centrado $X_t = Y_t - \mu$ satisfaz a equa√ß√£o $X_t = \phi X_{t-1} + \epsilon_t$.

*Proof:* Subtraindo a m√©dia $\mu = \frac{c}{1-\phi}$ de ambos os lados da equa√ß√£o $Y_t = c + \phi Y_{t-1} + \epsilon_t$, obtemos:

$Y_t - \mu = c + \phi Y_{t-1} + \epsilon_t - \frac{c}{1 - \phi} = \phi(Y_{t-1} - \mu) + \epsilon_t$

Como $X_t = Y_t - \mu$, temos $X_t = \phi X_{t-1} + \epsilon_t$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Considere um processo AR(1) com $c = 10$ e $\phi = 0.6$. A m√©dia do processo √©:
>
> $\mu = \frac{10}{1 - 0.6} = \frac{10}{0.4} = 25$
>
> Agora, considere um novo processo $X_t = Y_t - 25$. Este processo centrado segue a equa√ß√£o:
>
> $X_t = 0.6 X_{t-1} + \epsilon_t$
>
> Isso significa que os desvios de $Y_t$ em rela√ß√£o √† sua m√©dia de 25 seguem um processo AR(1) com m√©dia zero e o mesmo coeficiente autorregressivo $\phi = 0.6$.

**Corol√°rio 3.1.1:** Se $Y_t$ √© um processo AR(1) estacion√°rio com $c=0$, ent√£o $E[Y_t] = 0$ e $Y_t = \phi Y_{t-1} + \epsilon_t$.

*Proof:* Imediatamente segue de Teorema 3 e Teorema 3.1. Se $c=0$, ent√£o $\mu = \frac{0}{1-\phi} = 0$. Portanto, $X_t = Y_t - 0 = Y_t$, e assim $Y_t = \phi Y_{t-1} + \epsilon_t$. $\blacksquare$

**Implica√ß√µes da F√≥rmula da M√©dia:**

1.  **Impacto da Constante *c*:** A constante $c$ influencia diretamente o n√≠vel da s√©rie temporal. Se $c > 0$, a m√©dia ser√° positiva, e se $c < 0$, a m√©dia ser√° negativa (assumindo $0 < \phi < 1$).
2.  **Influ√™ncia do Coeficiente** $\phi$: O coeficiente $\phi$ modula o impacto da constante $c$ na m√©dia. Se $\phi$ estiver pr√≥ximo de 1 (mas ainda menor que 1 para garantir a estacionariedade), um pequeno valor de $c$ pode resultar em uma m√©dia significativamente maior.
3.  **Processo Centrado:** Se $c = 0$, a m√©dia do processo AR(1) √© zero ($\mu = 0$). Nesses casos, a s√©rie temporal flutua em torno de zero.

**Exemplo Num√©rico:**

Considere os seguintes casos para ilustrar o impacto de $c$ e $\phi$ na m√©dia:

*   **Caso 1:** $c = 5$, $\phi = 0.5$

    $\mu = \frac{5}{1 - 0.5} = \frac{5}{0.5} = 10$
*   **Caso 2:** $c = 5$, $\phi = 0.9$

    $\mu = \frac{5}{1 - 0.9} = \frac{5}{0.1} = 50$
*   **Caso 3:** $c = -5$, $\phi = 0.5$

    $\mu = \frac{-5}{1 - 0.5} = \frac{-5}{0.5} = -10$
*   **Caso 4:** $c = 0$, $\phi = 0.7$

    $\mu = \frac{0}{1 - 0.7} = 0$

Estes exemplos demonstram como diferentes valores de $c$ e $\phi$ afetam a m√©dia do processo AR(1) estacion√°rio, alterando o n√≠vel em torno do qual a s√©rie temporal oscila.

> üí° **Exemplo Num√©rico:**
>
> Suponha que estamos modelando o pre√ßo de um produto ao longo do tempo. Se $c = 2$ representa um componente constante do pre√ßo e $\phi = 0.7$ representa a influ√™ncia do pre√ßo do per√≠odo anterior, a m√©dia do pre√ßo ao longo do tempo seria:
>
> $\mu = \frac{2}{1 - 0.7} = \frac{2}{0.3} \approx 6.67$
>
> Isso sugere que, em m√©dia, o pre√ßo do produto se estabilizar√° em torno de 6.67 unidades monet√°rias.
**Condi√ß√£o de Estacionariedade e M√©dia:**

√â fundamental reiterar que a exist√™ncia da m√©dia finita $\mu = \frac{c}{1 - \phi}$ depende crucialmente da condi√ß√£o de estacionariedade $|\phi| < 1$. Se $|\phi| \geq 1$, a m√©dia n√£o estar√° bem definida, e o processo n√£o ser√° estacion√°rio em covari√¢ncia, como discutido anteriormente [^54].

> üìù **Aten√ß√£o:**
>
> *A f√≥rmula* $\mu = \frac{c}{1 - \phi}$ *√© v√°lida apenas sob a condi√ß√£o de estacionariedade* $|\phi| < 1$. *Se esta condi√ß√£o n√£o for satisfeita, a m√©dia n√£o √© definida de forma consistente.*

**Rela√ß√£o com a Representa√ß√£o MA(‚àû):**

A representa√ß√£o MA(‚àû) [^53] fornece uma vis√£o alternativa da m√©dia. Ao expressar $Y_t$ como uma soma ponderada de termos de erro passados:

$$Y_t = \frac{c}{1 - \phi} + \sum_{j=0}^{\infty} \phi^j \epsilon_{t-j}$$

Tomando a expectativa, obtemos:

$$E(Y_t) = \frac{c}{1 - \phi} + \sum_{j=0}^{\infty} \phi^j E(\epsilon_{t-j}) = \frac{c}{1 - \phi}$$

Isso confirma que a m√©dia √© consistente com a representa√ß√£o MA(‚àû) e enfatiza a import√¢ncia da condi√ß√£o de estacionariedade para garantir a converg√™ncia da s√©rie.

> üí° **Exemplo Num√©rico:**
>
> Considere um processo AR(1) com $c = 3$ e $\phi = 0.8$, e $\epsilon_t$ com m√©dia 0. A representa√ß√£o MA(‚àû) seria:
>
> $Y_t = \frac{3}{1 - 0.8} + \epsilon_t + 0.8\epsilon_{t-1} + 0.8^2\epsilon_{t-2} + \ldots$
>
> $Y_t = 15 + \epsilon_t + 0.8\epsilon_{t-1} + 0.64\epsilon_{t-2} + \ldots$
>
> A m√©dia do processo, $E(Y_t)$, √© $\frac{3}{1 - 0.8} = 15$, pois $E(\epsilon_{t-j}) = 0$ para todo $j$. Isso mostra que, mesmo olhando para a representa√ß√£o MA(‚àû), a m√©dia do processo AR(1) estacion√°rio √© consistente.

**Teorema 4:** A vari√¢ncia de um processo AR(1) estacion√°rio definido por $Y_t = c + \phi Y_{t-1} + \epsilon_t$, onde $|\phi| < 1$ e $\epsilon_t \sim WN(0, \sigma^2)$, √© dada por $\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$.

*Proof:*
I. Come√ßamos com o processo AR(1) estacion√°rio:
    $$Y_t = c + \phi Y_{t-1} + \epsilon_t$$
II. Defina o processo centrado $X_t = Y_t - \mu$, onde $\mu = \frac{c}{1-\phi}$ √© a m√©dia do processo $Y_t$.  Como mostrado no Teorema 3.1, o processo centrado satisfaz:
    $$X_t = \phi X_{t-1} + \epsilon_t$$
III. Multiplicando ambos os lados por $X_t$, obtemos:
     $$X_t^2 = \phi X_t X_{t-1} + \epsilon_t X_t$$
IV. Tomando a expectativa de ambos os lados:
    $$E[X_t^2] = \phi E[X_t X_{t-1}] + E[\epsilon_t X_t]$$
V. Note que $E[X_t^2] = Var(X_t) = Var(Y_t) = \gamma_0$, que √© a vari√¢ncia do processo $Y_t$. Tamb√©m, $E[X_t X_{t-1}] = \gamma_1$, que √© a autocovari√¢ncia no lag 1.

VI. Podemos expressar $X_t$ como:
     $$X_t = \epsilon_t + \phi \epsilon_{t-1} + \phi^2 \epsilon_{t-2} + \ldots$$
     Ent√£o, $$E[\epsilon_t X_t] = E[\epsilon_t (\epsilon_t + \phi \epsilon_{t-1} + \phi^2 \epsilon_{t-2} + \ldots)]$$
     Como $\epsilon_t$ √© ru√≠do branco com m√©dia 0 e vari√¢ncia $\sigma^2$, temos $E[\epsilon_t \epsilon_{t-k}] = 0$ para $k \neq 0$ e $E[\epsilon_t^2] = \sigma^2$. Portanto,
     $$E[\epsilon_t X_t] = E[\epsilon_t^2] = \sigma^2$$
VII. Substituindo na equa√ß√£o da expectativa, obtemos:
      $$\gamma_0 = \phi \gamma_1 + \sigma^2$$
VIII. Multiplicando ambos os lados de $X_t = \phi X_{t-1} + \epsilon_t$ por $X_{t-1}$, temos:
       $$X_t X_{t-1} = \phi X_{t-1}^2 + \epsilon_t X_{t-1}$$
IX. Tomando a expectativa de ambos os lados:
    $$E[X_t X_{t-1}] = \phi E[X_{t-1}^2] + E[\epsilon_t X_{t-1}]$$
X. Novamente, $E[X_t X_{t-1}] = \gamma_1$ e $E[X_{t-1}^2] = \gamma_0$.  Since $\epsilon_t$ is independent of $X_{t-1}$, $E[\epsilon_t X_{t-1}] = 0$. Assim,
    $$\gamma_1 = \phi \gamma_0$$
XI. Substituindo $\gamma_1 = \phi \gamma_0$ na equa√ß√£o $\gamma_0 = \phi \gamma_1 + \sigma^2$, obtemos:
    $$\gamma_0 = \phi (\phi \gamma_0) + \sigma^2 = \phi^2 \gamma_0 + \sigma^2$$
XII. Resolvendo para $\gamma_0$, obtemos:
     $$\gamma_0 (1 - \phi^2) = \sigma^2$$
     $$\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$$
Portanto, a vari√¢ncia do processo AR(1) estacion√°rio √© dada por $\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Considere um processo AR(1) com $\phi = 0.7$ e $\epsilon_t \sim WN(0, 4)$. Isso significa que o ru√≠do branco tem uma vari√¢ncia de $\sigma^2 = 4$. A vari√¢ncia do processo AR(1) √©:
>
> $\gamma_0 = \frac{4}{1 - 0.7^2} = \frac{4}{1 - 0.49} = \frac{4}{0.51} \approx 7.84$
>
> Este exemplo mostra como a vari√¢ncia do ru√≠do branco e o coeficiente $\phi$ afetam a vari√¢ncia do processo AR(1).

**Teorema 4.1:** A autocovari√¢ncia de um processo AR(1) estacion√°rio definido por $Y_t = c + \phi Y_{t-1} + \epsilon_t$ no lag $k$ √© dada por $\gamma_k = \phi^k \gamma_0 = \phi^k \frac{\sigma^2}{1 - \phi^2}$, onde $|\phi| < 1$ e $\epsilon_t \sim WN(0, \sigma^2)$.

*Proof:* Usando o resultado do Teorema 4 e a equa√ß√£o $E[X_t X_{t-k}] = \phi E[X_{t-1} X_{t-k}] + E[\epsilon_t X_{t-k}]$, podemos derivar a fun√ß√£o de autocovari√¢ncia. Para $k > 0$, temos $E[\epsilon_t X_{t-k}] = 0$, pois $\epsilon_t$ √© independente de $X_{t-k}$. Assim, $\gamma_k = \phi \gamma_{k-1}$.

Aplicando esta rela√ß√£o recursivamente, obtemos:

$\gamma_k = \phi \gamma_{k-1} = \phi (\phi \gamma_{k-2}) = \phi^2 \gamma_{k-2} = \ldots = \phi^k \gamma_0$.

Substituindo $\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$ (do Teorema 4), temos $\gamma_k = \phi^k \frac{\sigma^2}{1 - \phi^2}$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Usando o exemplo anterior com $\phi = 0.7$ e $\sigma^2 = 4$, calcule a autocovari√¢ncia no lag 2:
>
> $\gamma_2 = (0.7)^2 \frac{4}{1 - 0.7^2} = 0.49 \times 7.84 \approx 3.84$
>
> Este valor indica a covari√¢ncia entre os valores do processo separados por dois per√≠odos de tempo.

**Corol√°rio 4.1.1:** A fun√ß√£o de autocorrela√ß√£o (ACF) de um processo AR(1) estacion√°rio √© dada por $\rho_k = \phi^k$.

*Proof:* A fun√ß√£o de autocorrela√ß√£o √© definida como $\rho_k = \frac{\gamma_k}{\gamma_0}$. Usando o resultado do Teorema 4.1, temos:

$\rho_k = \frac{\phi^k \frac{\sigma^2}{1 - \phi^2}}{\frac{\sigma^2}{1 - \phi^2}} = \phi^k$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Para um processo AR(1) com $\phi = 0.7$, a autocorrela√ß√£o no lag 1 √©:
>
> $\rho_1 = (0.7)^1 = 0.7$
>
> E a autocorrela√ß√£o no lag 3 √©:
>
> $\rho_3 = (0.7)^3 = 0.343$
>
> Isso significa que a correla√ß√£o entre os valores do processo diminui exponencialmente √† medida que o lag aumenta.

**Conclus√£o:**

A m√©dia de um processo AR(1) estacion√°rio, dada por $\mu = \frac{c}{1 - \phi}$, √© um par√¢metro fundamental que influencia o n√≠vel em torno do qual a s√©rie temporal oscila. Sua deriva√ß√£o e interpreta√ß√£o dependem crucialmente da condi√ß√£o de estacionariedade $|\phi| < 1$. Compreender a rela√ß√£o entre a constante $c$, o coeficiente $\phi$ e a m√©dia √© essencial para a correta modelagem e interpreta√ß√£o de processos AR(1) em aplica√ß√µes pr√°ticas.

### Refer√™ncias
[^47]: P√°gina 47
[^54]: P√°gina 54
[^53]: P√°gina 53
<!-- END -->