## Autoregressive (AR) Processes: Autocorrelation Function and Geometric Decay

### Introdu√ß√£o
Este cap√≠tulo continua a explora√ß√£o dos processos **autorregressivos (AR)**, focando especificamente na **fun√ß√£o de autocorrela√ß√£o (ACF)** de um processo AR(1) estacion√°rio. J√° estabelecemos que a ACF para um processo AR(1) √© dada por $\rho_j = \phi^j$ [^53]. Agora, analisaremos em profundidade o padr√£o de decaimento geom√©trico desta fun√ß√£o, sua import√¢ncia para a identifica√ß√£o do par√¢metro AR e suas implica√ß√µes para a simplifica√ß√£o do modelo.

### Conceitos Fundamentais
A fun√ß√£o de autocorrela√ß√£o (ACF) $\rho_j$ de um processo AR(1) estacion√°rio √© definida como a correla√ß√£o entre $Y_t$ e $Y_{t-j}$ [^53]:

$$\rho_j = \frac{Cov(Y_t, Y_{t-j})}{\sqrt{Var(Y_t)Var(Y_{t-j})}}$$

Para um processo AR(1) estacion√°rio $Y_t = c + \phi Y_{t-1} + \epsilon_t$, a ACF √© dada por:

$$\rho_j = \phi^j$$

onde $|\phi| < 1$ para garantir a estacionariedade. O fator $\phi^j$ √© fundamental para entender o decaimento geom√©trico da ACF.

**Padr√£o de Decaimento Geom√©trico:**

A ACF de um processo AR(1) exibe um padr√£o de decaimento geom√©trico, o que significa que a correla√ß√£o entre $Y_t$ e $Y_{t-j}$ diminui exponencialmente √† medida que o lag $j$ aumenta. A taxa de decaimento √© determinada pelo valor de $\phi$.

**Teorema 5:** Para um processo AR(1) estacion√°rio com ACF $\rho_j = \phi^j$, a magnitude da autocorrela√ß√£o diminui geometricamente √† medida que o lag $j$ aumenta, ou seja, $|\rho_j| \to 0$ quando $j \to \infty$.

*Proof:* Dado que $|\phi| < 1$ para a estacionariedade, temos que $\lim_{j \to \infty} |\phi^j| = 0$. Portanto, $\lim_{j \to \infty} |\rho_j| = 0$. $\blacksquare$

**Corol√°rio 5.1:** O decaimento da ACF √© mais r√°pido para valores menores de $|\phi|$.

*Proof:* Considere dois processos AR(1) com coeficientes $\phi_1$ e $\phi_2$ tais que $|\phi_1| < |\phi_2| < 1$. Ent√£o, $|\phi_1^j| < |\phi_2^j|$ para todo $j > 0$. Isso implica que a sequ√™ncia $|\phi_1^j|$ converge para zero mais rapidamente do que a sequ√™ncia $|\phi_2^j|$, mostrando que o decaimento da ACF √© mais r√°pido para valores menores de $|\phi|$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Compare o decaimento da ACF para $\phi = 0.3$ e $\phi = 0.7$:
>
> Para $\phi = 0.3$:
>
> $\rho_0 = 1$, $\rho_1 = 0.3$, $\rho_2 = 0.09$, $\rho_3 = 0.027$
>
> Para $\phi = 0.7$:
>
> $\rho_0 = 1$, $\rho_1 = 0.7$, $\rho_2 = 0.49$, $\rho_3 = 0.343$
>
> Observe que a ACF decai muito mais rapidamente para $\phi = 0.3$ do que para $\phi = 0.7$. Intuitivamente, com $\phi=0.3$, o passado tem menos influ√™ncia no presente do que com $\phi=0.7$.

**Identifica√ß√£o do Par√¢metro AR (œÜ):**

O padr√£o de decaimento geom√©trico da ACF facilita a identifica√ß√£o do par√¢metro AR ($\phi$). Ao analisar a ACF amostral de uma s√©rie temporal, pode-se estimar $\phi$ a partir da autocorrela√ß√£o no lag 1 ($\rho_1$) [^53]:

$$\hat{\phi} \approx \rho_1$$

Esta aproxima√ß√£o √© particularmente √∫til na fase inicial de modelagem, onde o objetivo √© obter uma estimativa razo√°vel do par√¢metro AR para come√ßar o processo de ajuste.

**Teorema 6:** Uma estimativa do par√¢metro autoregressivo em um processo AR(1) estacion√°rio pode ser obtida a partir da fun√ß√£o de autocorrela√ß√£o amostral no lag 1, ou seja, $\hat{\phi} = \hat{\rho}_1$, onde $\hat{\rho}_1$ √© a autocorrela√ß√£o amostral no lag 1.

*Proof:* Como demonstrado anteriormente, para um processo AR(1) estacion√°rio, temos $\rho_1 = \phi$. Na pr√°tica, estimamos $\rho_1$ usando a autocorrela√ß√£o amostral $\hat{\rho}_1$. Assim, uma estimativa razo√°vel para $\phi$ √© $\hat{\phi} = \hat{\rho}_1$, assumindo que o processo √© estacion√°rio. $\blacksquare$

**Lema 6.1:** Se o processo AR(1) √© causal, ent√£o a estimativa $\hat{\phi} = \hat{\rho}_1$ √© uma estimativa consistente de $\phi$.

*Proof:* A causalidade implica estacionariedade no processo AR(1). Pela lei dos grandes n√∫meros, a autocorrela√ß√£o amostral $\hat{\rho}_1$ converge para a autocorrela√ß√£o te√≥rica $\rho_1$ √† medida que o tamanho da amostra aumenta. Como $\rho_1 = \phi$ para um processo AR(1) estacion√°rio e causal, $\hat{\rho}_1$ converge para $\phi$, demonstrando a consist√™ncia da estimativa. $\blacksquare$

**Proposi√ß√£o 6.2:** A vari√¢ncia da estimativa $\hat{\phi}$ diminui √† medida que o tamanho da amostra aumenta.

*Proof:* Sob condi√ß√µes de regularidade, o estimador $\hat{\phi}$ derivado do m√©todo de m√≠nimos quadrados √© consistente e assintoticamente normal. A vari√¢ncia assint√≥tica do estimador $\hat{\phi}$ √© inversamente proporcional ao tamanho da amostra $T$. Portanto, √† medida que $T$ aumenta, a vari√¢ncia de $\hat{\phi}$ diminui, indicando que a estimativa se torna mais precisa com amostras maiores. $\blacksquare$

**Estimativa Formal:**

Para um processo AR(1), podemos estimar $\phi$ usando o m√©todo de m√≠nimos quadrados ordin√°rios (OLS). Dada a equa√ß√£o AR(1) $Y_t = c + \phi Y_{t-1} + \epsilon_t$, podemos minimizar a soma dos quadrados dos erros:

$$S(\phi) = \sum_{t=2}^T (Y_t - c - \phi Y_{t-1})^2$$

Tomando a derivada em rela√ß√£o a $\phi$ e igualando a zero, obtemos:

$$\frac{\partial S(\phi)}{\partial \phi} = -2 \sum_{t=2}^T (Y_t - c - \phi Y_{t-1})Y_{t-1} = 0$$

Resolvendo para $\phi$, obtemos o estimador de m√≠nimos quadrados:

$$\hat{\phi} = \frac{\sum_{t=2}^T (Y_t - c)Y_{t-1}}{\sum_{t=2}^T Y_{t-1}^2}$$

Se a m√©dia da s√©rie √© conhecida (ou estimada), podemos recentralizar os dados e obter uma estimativa ligeiramente diferente:

$$\hat{\phi} = \frac{\sum_{t=2}^T (Y_t - \mu)(Y_{t-1} - \mu)}{\sum_{t=2}^T (Y_{t-1} - \mu)^2}$$

Sob condi√ß√µes de regularidade, este estimador √© consistente e assintoticamente normal.

**Prova da Deriva√ß√£o do Estimador OLS:**

Aqui, provaremos como o estimador de m√≠nimos quadrados √© derivado.

I. Dada a fun√ß√£o de soma de erros quadrados:
    $$S(\phi) = \sum_{t=2}^T (Y_t - c - \phi Y_{t-1})^2$$

II. Diferencie $S(\phi)$ em rela√ß√£o a $\phi$:
    $$\frac{\partial S(\phi)}{\partial \phi} = \sum_{t=2}^T 2(Y_t - c - \phi Y_{t-1})(-Y_{t-1})$$

III. Iguale a derivada a zero para encontrar o valor m√≠nimo:
    $$\sum_{t=2}^T (Y_t - c - \phi Y_{t-1})(-Y_{t-1}) = 0$$
    $$\sum_{t=2}^T (Y_t - c - \phi Y_{t-1})Y_{t-1} = 0$$

IV. Expanda a soma:
    $$\sum_{t=2}^T (Y_t - c)Y_{t-1} - \sum_{t=2}^T \phi Y_{t-1}^2 = 0$$

V. Resolva para $\phi$:
    $$\phi \sum_{t=2}^T Y_{t-1}^2 = \sum_{t=2}^T (Y_t - c)Y_{t-1}$$
    $$\hat{\phi} = \frac{\sum_{t=2}^T (Y_t - c)Y_{t-1}}{\sum_{t=2}^T Y_{t-1}^2}$$

Assim, demonstramos que o estimador de m√≠nimos quadrados para $\phi$ √©:
$$\hat{\phi} = \frac{\sum_{t=2}^T (Y_t - c)Y_{t-1}}{\sum_{t=2}^T Y_{t-1}^2}$$ ‚ñ†

**Simplifica√ß√£o do Modelo:**

O padr√£o de decaimento geom√©trico da ACF n√£o apenas facilita a estimativa inicial de $\phi$, mas tamb√©m auxilia na simplifica√ß√£o do modelo. Se a ACF amostral de uma s√©rie temporal exibe um padr√£o de decaimento geom√©trico, sugere-se que um modelo AR(1) pode ser apropriado.

**Exemplo Num√©rico:**
Suponha que analisamos a ACF amostral de uma s√©rie temporal e observamos os seguintes valores:

$$\hat{\rho}_0 = 1$$
$$\hat{\rho}_1 = 0.6$$
$$\hat{\rho}_2 = 0.35$$
$$\hat{\rho}_3 = 0.20$$

O padr√£o de decaimento geom√©trico √© evidente, sugerindo que um modelo AR(1) pode ser uma boa escolha. A estimativa inicial do par√¢metro AR seria $\hat{\phi} \approx 0.6$. Em seguida, podemos ajustar formalmente o modelo AR(1) para obter estimativas mais precisas.

> üí° **Exemplo Num√©rico:**
>
> Vamos supor que temos os seguintes dados de uma s√©rie temporal:
>
> `Y = [2.1, 2.5, 2.9, 2.3, 2.6, 3.0, 3.3, 2.8, 3.1, 3.5]`
>
> Calculamos a autocorrela√ß√£o no lag 1:
>
> ```python
> import numpy as np
>
> Y = np.array([2.1, 2.5, 2.9, 2.3, 2.6, 3.0, 3.3, 2.8, 3.1, 3.5])
> Y_lagged = Y[:-1]
> Y_current = Y[1:]
>
> # Estimativa da m√©dia
> mean_Y = np.mean(Y)
>
> # Calcula a autocorrela√ß√£o amostral no lag 1
> numerator = np.sum((Y_current - mean_Y) * (Y_lagged - mean_Y))
> denominator = np.sum((Y_lagged - mean_Y)**2)
> phi_hat = numerator / denominator
>
> print(f"Estimativa de phi (phi_hat): {phi_hat:.3f}")
> ```
>
> Isso nos d√° uma estimativa de $\hat{\phi} \approx 0.785$. Isso sugere que o valor atual da s√©rie temporal depende fortemente do valor anterior. Podemos usar este valor como um ponto de partida para ajustar um modelo AR(1) formalmente.
>
> √â importante notar que esta √© apenas uma estimativa inicial e pode ser refinada usando m√©todos de estima√ß√£o mais formais, como OLS.

**Condi√ß√£o de Estacionariedade:**

√â crucial reiterar que a interpreta√ß√£o da ACF como um padr√£o de decaimento geom√©trico e a estimativa de $\phi$ a partir de $\rho_1$ s√£o v√°lidas apenas se o processo for estacion√°rio, ou seja, $|\phi| < 1$. Se a ACF n√£o decair ou exibir um padr√£o diferente, isso sugere que o processo n√£o √© estacion√°rio, e outras abordagens de modelagem podem ser necess√°rias.

**Observa√ß√£o 5.2:** Se $|\phi| \geq 1$, a fun√ß√£o de autocorrela√ß√£o n√£o converge para 0 quando $j$ tende ao infinito, indicando n√£o-estacionariedade.

> üìù **Aten√ß√£o:**
>
> *A an√°lise da ACF e a interpreta√ß√£o do decaimento geom√©trico s√£o v√°lidas apenas se a condi√ß√£o de estacionariedade* $|\phi| < 1$ *for satisfeita. Se o processo n√£o for estacion√°rio, a ACF n√£o exibir√° o padr√£o de decaimento esperado.*

**Rela√ß√£o com Autocorrela√ß√£o Parcial (PACF):**

Embora a ACF exiba um padr√£o de decaimento geom√©trico para um processo AR(1), a fun√ß√£o de autocorrela√ß√£o parcial (PACF) oferece uma vis√£o complementar. A PACF mede a correla√ß√£o entre $Y_t$ e $Y_{t-j}$ ap√≥s remover a influ√™ncia dos lags intermedi√°rios $Y_{t-1}, Y_{t-2}, \dots, Y_{t-j+1}$. Para um processo AR(1), a PACF √© zero para todos os lags maiores que 1 [^55].

**Prova de que a PACF corta ap√≥s o lag 1 para um processo AR(1):**

I. Considere um processo AR(1): $Y_t = c + \phi Y_{t-1} + \epsilon_t$.

II. A autocorrela√ß√£o parcial no lag $k$, denotada por $\alpha_k$, mede a correla√ß√£o entre $Y_t$ e $Y_{t-k}$ depois de remover o efeito das vari√°veis intermedi√°rias $Y_{t-1}, Y_{t-2}, \dots, Y_{t-k+1}$.

III. Para $k = 1$, $\alpha_1 = Corr(Y_t, Y_{t-1}) = \phi$.

IV. Para $k > 1$, a autocorrela√ß√£o parcial $\alpha_k$ representa o coeficiente de $Y_{t-k}$ na regress√£o de $Y_t$ em $Y_{t-1}, Y_{t-2}, \dots, Y_{t-k}$.

V. Como $Y_t$ depende apenas de $Y_{t-1}$ em um processo AR(1), os coeficientes de $Y_{t-k}$ para $k > 1$ s√£o zero.

VI. Portanto, $\alpha_k = 0$ para $k > 1$. Isso mostra que a PACF corta ap√≥s o lag 1 para um processo AR(1). ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere uma s√©rie temporal gerada por um processo AR(1) com $\phi = 0.6$. A ACF exibir√° um decaimento geom√©trico. A PACF ser√° significativa no lag 1 (aproximadamente 0.6) e pr√≥ximo de zero para todos os outros lags. Isso confirma que a PACF "corta" ap√≥s o lag 1.
>
> ```mermaid
> graph LR
>     A[Y(t)] --> B(Y(t-1), Corr=0.6)
>     A --> C(Y(t-2), Corr‚âà0)
>     A --> D(Y(t-3), Corr‚âà0)
>     style A fill:#f9f,stroke:#333,stroke-width:2px
> ```
> Este diagrama Mermaid ilustra a rela√ß√£o onde $Y(t)$ est√° fortemente correlacionado com $Y(t-1)$ e fracamente com os demais lags.

**Resumo:**

*   Um processo AR(1) tem um ACF que decai geometricamente, ou seja, $\rho_j = \phi^j$.
*   A magnitude da autocorrela√ß√£o diminui exponencialmente com o aumento de $j$, e a taxa de decaimento depende do valor de $\phi$.
*   Essa diminui√ß√£o ocorre se $|\phi| < 1$, que √© a condi√ß√£o necess√°ria para a estacionariedade de um processo AR(1).
*   O coeficiente de autocorrela√ß√£o parcial (PACF) √© 0 para todos os lags maiores que 1.

> üí° **Exemplo Num√©rico:**
>
> Suponha que analisamos uma s√©rie temporal e sua ACF exibe um decaimento geom√©trico com $\rho_1 \approx 0.8$, $\rho_2 \approx 0.64$, $\rho_3 \approx 0.512$. O PACF corta ap√≥s o lag 1, ou seja, PACF(1) √© significativo e PACF(j) ‚âà 0 para j > 1. Esses padr√µes sugerem um processo AR(1) com $\phi \approx 0.8$. O gr√°fico de ACF apresentar√° um decaimento exponencial, enquanto o gr√°fico de PACF exibir√° apenas uma barra significativa no lag 1.

**Teorema 7:** Se a fun√ß√£o de autocorrela√ß√£o de uma s√©rie temporal decai geometricamente e a fun√ß√£o de autocorrela√ß√£o parcial corta ap√≥s o lag 1, ent√£o a s√©rie temporal pode ser adequadamente modelada por um processo AR(1).

*Proof:* Este resultado decorre das propriedades da ACF e PACF para um processo AR(1). O decaimento geom√©trico na ACF indica que a correla√ß√£o entre observa√ß√µes diminui exponencialmente com o aumento do lag, e a PACF cortando ap√≥s o lag 1 indica que a depend√™ncia condicional de $Y_t$ em $Y_{t-k}$, dado $Y_{t-1}, \dots, Y_{t-k+1}$, √© insignificante para $k>1$. Portanto, um modelo AR(1), que captura apenas a depend√™ncia no lag 1, √© suficiente para modelar a estrutura de correla√ß√£o da s√©rie temporal. $\blacksquare$

O padr√£o de decaimento geom√©trico da fun√ß√£o de autocorrela√ß√£o de um processo AR(1) estacion√°rio facilita a identifica√ß√£o do par√¢metro AR e facilita a simplifica√ß√£o do modelo. Uma ACF que exibiu decaimento geom√©trico indicava a modelagem AR(1).

> üí° **Exemplo Num√©rico:**
>
> Em finan√ßas, o retorno de uma a√ß√£o pode apresentar um decaimento geom√©trico em sua ACF, sugerindo que o retorno atual √© influenciado pelo retorno anterior. Se a autocorrela√ß√£o no lag 1 for 0.4, o modelo AR(1) seria $Y_t = c + 0.4 Y_{t-1} + \epsilon_t$. A m√©dia do retorno da a√ß√£o e outras propriedades podem ent√£o ser analisadas com base neste modelo.
**Observa√ß√£o 7.1:** Um valor de $\phi$ pr√≥ximo de 1, mas menor que 1, corresponde a uma s√©rie temporal com forte persist√™ncia.

**Teorema 8:** Para um processo AR(p), a PACF √© zero para todos os lags maiores que p.

*Proof:* Em um processo AR(p), $Y_t$ depende linearmente de seus p lags anteriores, ou seja, $Y_{t-1}, Y_{t-2}, \ldots, Y_{t-p}$. A PACF mede a correla√ß√£o entre $Y_t$ e $Y_{t-k}$ ap√≥s remover a influ√™ncia dos lags intermedi√°rios $Y_{t-1}, Y_{t-2}, \dots, Y_{t-k+1}$. Para $k > p$, a correla√ß√£o parcial entre $Y_t$ e $Y_{t-k}$ √© zero porque toda a depend√™ncia de $Y_t$ nos lags passados j√° √© capturada pelos primeiros p lags. Formalmente, isso pode ser provado usando as equa√ß√µes de Yule-Walker e a defini√ß√£o de PACF. $\blacksquare$

**Corol√°rio 8.1:** Para um processo AR(p) causal, as ra√≠zes do polin√¥mio autoregressivo associado est√£o fora do c√≠rculo unit√°rio no plano complexo.

*Proof:* A causalidade de um processo AR(p) implica que ele pode ser expresso como uma fun√ß√£o linear dos ru√≠dos brancos passados. Isso, por sua vez, implica que as ra√≠zes do polin√¥mio autoregressivo $1 - \phi_1 z - \phi_2 z^2 - \ldots - \phi_p z^p = 0$ devem estar fora do c√≠rculo unit√°rio no plano complexo. Caso contr√°rio, o processo n√£o seria causal, ou seja, n√£o dependeria apenas de inova√ß√µes passadas e estaria sujeito a problemas de n√£o estacionariedade. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Suponha que tenhamos um processo AR(2) definido por $Y_t = 0.5Y_{t-1} + 0.3Y_{t-2} + \epsilon_t$. O polin√¥mio autoregressivo associado √© $1 - 0.5z - 0.3z^2 = 0$. Resolvendo para as ra√≠zes, encontramos que ambas as ra√≠zes est√£o fora do c√≠rculo unit√°rio, o que confirma a causalidade e a estacionariedade do processo.
>
> ```python
> import numpy as np
>
> # Coeficientes do polin√¥mio autoregressivo (1 - 0.5z - 0.3z^2)
> coefficients = [1, -0.5, -0.3]
>
> # Encontra as ra√≠zes do polin√¥mio
> roots = np.roots(coefficients[::-1]) # Inverte a ordem dos coeficientes
>
> print(f"Ra√≠zes do polin√¥mio: {roots}")
>
> # Verifica se as ra√≠zes est√£o fora do c√≠rculo unit√°rio
> for root in roots:
>     magnitude = np.abs(root)
>     print(f"Magnitude da raiz {root}: {magnitude}")
>     if magnitude > 1:
>         print("A raiz est√° fora do c√≠rculo unit√°rio.")
>     else:
>         print("A raiz est√° dentro do c√≠rculo unit√°rio.")
> ```
>
> O output deste c√≥digo mostrar√° que as magnitudes das ra√≠zes s√£o maiores que 1, confirmando que o processo AR(2) √© causal e estacion√°rio.

### Conclus√£o

A fun√ß√£o de autocorrela√ß√£o de um processo AR(1) estacion√°rio, com seu padr√£o de decaimento geom√©trico, fornece insights valiosos sobre a estrutura de depend√™ncia temporal da s√©rie. A an√°lise da ACF amostral permite uma estimativa inicial do par√¢metro AR e auxilia na simplifica√ß√£o do modelo. √â importante notar que essa an√°lise √© v√°lida apenas sob a condi√ß√£o de estacionariedade $|\phi| < 1$. Entender a rela√ß√£o entre o par√¢metro AR, o decaimento geom√©trico e a estacionariedade √© crucial para a modelagem correta e interpreta√ß√£o de s√©ries temporais. Os padr√µes da ACF e PACF juntos indicam um modelo AR(1) adequado.

### Refer√™ncias
[^53]: P√°gina 53
[^55]: Se√ß√£o Autoregressive (AR) Processes
<!-- END -->