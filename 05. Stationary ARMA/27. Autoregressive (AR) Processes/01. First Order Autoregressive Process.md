## Autoregressive (AR) Processes: The First-Order Case

### Introdu√ß√£o
Expandindo sobre a an√°lise de processos de **m√©dias m√≥veis (MA)** apresentados anteriormente, este cap√≠tulo se dedica ao estudo dos processos **autorregressivos (AR)**. Um processo AR √© uma classe fundamental de modelos de s√©ries temporais onde o valor atual da s√©rie √© expresso em termos de seus valores passados e um termo de erro aleat√≥rio. Este cap√≠tulo se concentrar√° inicialmente no processo **autorregressivo de primeira ordem, AR(1)**, explorando suas propriedades, caracter√≠sticas e representa√ß√µes.

### Conceitos Fundamentais
Um processo autorregressivo de primeira ordem, denotado **AR(1)**, √© definido pela seguinte equa√ß√£o de diferen√ßa [^53]:
$$Y_t = c + \phi Y_{t-1} + \epsilon_t$$
onde:
- $Y_t$ representa o valor da s√©rie temporal no instante $t$.
- $c$ √© uma constante.
- $\phi$ √© o coeficiente autorregressivo, que determina a influ√™ncia do valor anterior $Y_{t-1}$ no valor atual $Y_t$.
- $\epsilon_t$ √© um termo de erro de **ru√≠do branco**, que satisfaz as condi√ß√µes de [3.2.1] a [3.2.3] [^53].

> üí° **Exemplo Num√©rico:**
>
> Suponha que tenhamos um processo AR(1) com $c = 5$, $\phi = 0.7$ e $\epsilon_t$ seguindo uma distribui√ß√£o normal com m√©dia 0 e desvio padr√£o 2 (ou seja, $\epsilon_t \sim N(0, 4)$). Para o instante $t=1$, se $Y_0 = 10$ e $\epsilon_1 = 1.5$, ent√£o
>
> $Y_1 = 5 + 0.7 \times 10 + 1.5 = 5 + 7 + 1.5 = 13.5$
>
> No instante $t=2$, se $\epsilon_2 = -0.5$, ent√£o
>
> $Y_2 = 5 + 0.7 \times 13.5 - 0.5 = 5 + 9.45 - 0.5 = 13.95$
>
> Este processo continua iterativamente, com cada valor $Y_t$ dependendo do valor anterior e do termo de erro atual.

A equa√ß√£o AR(1) expressa $Y_t$ como uma fun√ß√£o linear de seu valor passado e um termo de ru√≠do branco, permitindo a predi√ß√£o recursiva e capturando depend√™ncias din√¢micas na s√©rie temporal [^53]. A an√°lise das propriedades do processo AR(1) √© crucial para entender a din√¢mica de muitas s√©ries temporais encontradas em diversas √°reas, como economia, finan√ßas e engenharia.

Para a sequ√™ncia de ru√≠do branco {$\epsilon_t$}, as condi√ß√µes [3.2.1] a [3.2.3] indicam [^5]:

*   M√©dia zero: $E(\epsilon_t) = 0$ [^47]
*   Vari√¢ncia constante: $E(\epsilon_t^2) = \sigma^2$ [^47]
*   N√£o correla√ß√£o: $E(\epsilon_t \epsilon_\tau) = 0$ para $t \neq \tau$ [^48]

> üí° **Exemplo Num√©rico:**
>
> Imagine uma s√©rie de ru√≠do branco com $\sigma^2 = 4$. Isso significa que a vari√¢ncia de cada termo de erro $\epsilon_t$ √© 4. Se gerarmos 5 valores aleat√≥rios de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 4, poder√≠amos obter:
>
> $\epsilon_1 = -1.2, \epsilon_2 = 0.5, \epsilon_3 = 2.1, \epsilon_4 = -0.8, \epsilon_5 = 1.5$
>
> A m√©dia desses valores deve ser pr√≥xima de zero, e a vari√¢ncia deve ser pr√≥xima de 4. Calculando a m√©dia e a vari√¢ncia amostral:
>
> $\text{M√©dia amostral} = \frac{-1.2 + 0.5 + 2.1 - 0.8 + 1.5}{5} = 0.42$
>
> $\text{Vari√¢ncia amostral} = \frac{(-1.2 - 0.42)^2 + (0.5 - 0.42)^2 + (2.1 - 0.42)^2 + (-0.8 - 0.42)^2 + (1.5 - 0.42)^2}{5-1} \approx 1.92$
>
> Com um n√∫mero maior de amostras, a m√©dia e a vari√¢ncia amostrais se aproximariam dos valores te√≥ricos de 0 e 4, respectivamente. A n√£o correla√ß√£o significa que n√£o h√° rela√ß√£o linear entre os valores de $\epsilon$ em diferentes instantes de tempo.

A condi√ß√£o de n√£o correla√ß√£o pode ser substitu√≠da por uma condi√ß√£o mais forte, onde os $\epsilon_t$ s√£o independentes ao longo do tempo [^48]:
$\epsilon_t, \epsilon_\tau \text{ independentes para } t \neq \tau$ [^48]

**Rela√ß√£o com Equa√ß√µes de Diferen√ßa:**
A equa√ß√£o [3.4.1] [^53] toma a forma da equa√ß√£o de diferen√ßa de primeira ordem [1.1.1] ou [2.2.1], onde a vari√°vel de entrada $w_t$ √© dada por $w_t = c + \epsilon_t$ [^53].

**Condi√ß√£o de Estacionariedade:**
√â crucial notar que se $|\phi| \geq 1$, n√£o existe um processo estacion√°rio de covari√¢ncia para $Y_t$ com vari√¢ncia finita que satisfa√ßa [3.4.1] [^53]. No entanto, se $|\phi| < 1$, existe um processo estacion√°rio de covari√¢ncia para $Y_t$ que satisfaz [3.4.1] [^53]. Este processo √© dado pela solu√ß√£o est√°vel de [3.4.1] [^53], caracterizada em [2.2.9] [^53]:

$$Y_t = (c + \epsilon_t) + \phi(c + \epsilon_{t-1}) + \phi^2(c + \epsilon_{t-2}) + \phi^3(c + \epsilon_{t-3}) + \dots$$

$$Y_t = \frac{c}{1 - \phi} + \epsilon_t + \phi\epsilon_{t-1} + \phi^2\epsilon_{t-2} + \phi^3\epsilon_{t-3} + \dots$$

> üí° **Exemplo Num√©rico:**
>
> Se $\phi = 0.5$ e $c = 2$, a s√©rie se torna:
>
> $Y_t = \frac{2}{1 - 0.5} + \epsilon_t + 0.5\epsilon_{t-1} + 0.5^2\epsilon_{t-2} + 0.5^3\epsilon_{t-3} + \dots$
>
> $Y_t = 4 + \epsilon_t + 0.5\epsilon_{t-1} + 0.25\epsilon_{t-2} + 0.125\epsilon_{t-3} + \dots$
>
> Observe que o efeito dos termos de erro passados diminui exponencialmente √† medida que o lag aumenta, devido ao termo $\phi^j$. Isso garante que a s√©rie permane√ßa estacion√°ria, j√° que a influ√™ncia dos valores passados √© limitada.

**Representa√ß√£o MA(‚àû):**
A equa√ß√£o [3.4.2] [^53] pode ser vista como um processo de m√©dia m√≥vel de ordem infinita (MA(‚àû)) como em [3.3.13] com $\psi_j$ dado por $\phi^j$ [^53]. Quando $|\phi| < 1$, a condi√ß√£o [3.3.15] √© satisfeita [^53]:

$$\sum_{j=0}^{\infty} |\psi_j| = \sum_{j=0}^{\infty} |\phi^j|$$

que √© igual a $\frac{1}{1-|\phi|}$ desde que $|\phi|<1$ [^53].

**Lema 1:** Se $|\phi| < 1$, ent√£o $\sum_{j=0}^{\infty} |\phi^j| = \frac{1}{1-|\phi|}$.

*Proof:* A demonstra√ß√£o segue diretamente da f√≥rmula para a soma de uma s√©rie geom√©trica infinita. Dado que $|\phi| < 1$, a s√©rie converge, e sua soma √© $\frac{1}{1-|\phi|}$. $\blacksquare$

**Expectativa:**
Tomando as expectativas de [3.4.2] [^53], vemos que:

$$E(Y_t) = \frac{c}{1 - \phi} + 0 + 0 + \dots$$

de modo que a m√©dia de um processo AR(1) estacion√°rio √© [3.4.3]:
$$\mu = \frac{c}{1 - \phi}$$

> üí° **Exemplo Num√©rico:**
>
> Se $c = 10$ e $\phi = 0.8$, a m√©dia do processo AR(1) √©:
>
> $\mu = \frac{10}{1 - 0.8} = \frac{10}{0.2} = 50$
>
> Isso significa que, em m√©dia, os valores da s√©rie temporal se aproximar√£o de 50 ao longo do tempo.

**Prova da Expectativa:**
Provaremos que $E(Y_t) = \frac{c}{1 - \phi}$

I. Come√ßamos com a equa√ß√£o AR(1): $Y_t = c + \phi Y_{t-1} + \epsilon_t$.
II. Tomamos a expectativa de ambos os lados: $E(Y_t) = E(c + \phi Y_{t-1} + \epsilon_t)$.
III. Usando a linearidade da expectativa, temos: $E(Y_t) = E(c) + \phi E(Y_{t-1}) + E(\epsilon_t)$.
IV. Dado que o processo √© estacion√°rio, $E(Y_t) = E(Y_{t-1}) = \mu$.  Tamb√©m, $E(c) = c$ e $E(\epsilon_t) = 0$.
V. Substituindo, obtemos: $\mu = c + \phi \mu + 0$.
VI. Resolvendo para $\mu$, temos: $\mu - \phi \mu = c$, o que implica $\mu(1 - \phi) = c$.
VII. Portanto, $\mu = \frac{c}{1 - \phi}$.$\blacksquare$

**Vari√¢ncia:**
A vari√¢ncia √© dada por [3.4.4]:

$$\gamma_0 = E(Y_t - \mu)^2 = E(\epsilon_t + \phi\epsilon_{t-1} + \phi^2\epsilon_{t-2} + \phi^3\epsilon_{t-3} + \dots)^2$$

$$= (1 + \phi^2 + \phi^4 + \phi^6 + \dots)\sigma^2 = \frac{\sigma^2}{1 - \phi^2}$$

> üí° **Exemplo Num√©rico:**
>
> Se $\phi = 0.6$ e $\sigma^2 = 9$, a vari√¢ncia do processo AR(1) √©:
>
> $\gamma_0 = \frac{9}{1 - 0.6^2} = \frac{9}{1 - 0.36} = \frac{9}{0.64} = 14.0625$
>
> Isso indica a dispers√£o dos valores da s√©rie temporal em torno de sua m√©dia.

**Prova da Vari√¢ncia:**
Provaremos que $\gamma_0 = E(Y_t - \mu)^2 = \frac{\sigma^2}{1 - \phi^2}$

I.  Come√ßamos com a representa√ß√£o MA(‚àû): $Y_t = \frac{c}{1 - \phi} + \epsilon_t + \phi\epsilon_{t-1} + \phi^2\epsilon_{t-2} + \dots$
II.  Subtraindo a m√©dia $\mu = \frac{c}{1 - \phi}$ de $Y_t$, obtemos: $Y_t - \mu = \epsilon_t + \phi\epsilon_{t-1} + \phi^2\epsilon_{t-2} + \dots$
III. Calculamos a vari√¢ncia: $\gamma_0 = E[(Y_t - \mu)^2] = E[(\epsilon_t + \phi\epsilon_{t-1} + \phi^2\epsilon_{t-2} + \dots)^2]$
IV. Expandindo o quadrado e usando o fato de que $E[\epsilon_t \epsilon_s] = 0$ para $t \neq s$ e $E[\epsilon_t^2] = \sigma^2$, obtemos:
    $E[(\epsilon_t + \phi\epsilon_{t-1} + \phi^2\epsilon_{t-2} + \dots)^2] = E[\epsilon_t^2 + \phi^2\epsilon_{t-1}^2 + \phi^4\epsilon_{t-2}^2 + \dots]$
V.  Assim, $\gamma_0 = \sigma^2 + \phi^2\sigma^2 + \phi^4\sigma^2 + \dots = \sigma^2(1 + \phi^2 + \phi^4 + \dots)$
VI. A express√£o entre par√™nteses √© uma s√©rie geom√©trica com raz√£o $\phi^2$.  Como $|\phi| < 1$, a s√©rie converge para $\frac{1}{1 - \phi^2}$.
VII. Portanto, $\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$. $\blacksquare$

**Autocovari√¢ncia:**
A *j*-√©sima autocovari√¢ncia √© [3.4.5]:

$$\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$$

$$= E[(\epsilon_t + \phi\epsilon_{t-1} + \phi^2\epsilon_{t-2} + \dots)(\epsilon_{t-j} + \phi\epsilon_{t-j-1} + \phi^2\epsilon_{t-j-2} + \dots)]$$

$$= [\phi^j + \phi^{j+2} + \phi^{j+4} + \dots]\sigma^2$$

$$= \frac{\phi^j}{1 - \phi^2}\sigma^2$$

> üí° **Exemplo Num√©rico:**
>
> Se $\phi = 0.6$, $\sigma^2 = 9$ e $j = 2$, a autocovari√¢ncia de lag 2 √©:
>
> $\gamma_2 = \frac{0.6^2}{1 - 0.6^2} \times 9 = \frac{0.36}{0.64} \times 9 = 0.5625 \times 9 = 5.0625$
>
> Isso quantifica a covari√¢ncia entre os valores da s√©rie temporal separados por dois per√≠odos de tempo.

**Prova da Autocovari√¢ncia:**
Provaremos que $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = \frac{\phi^j}{1 - \phi^2}\sigma^2$

I.  Sabemos que $Y_t - \mu = \epsilon_t + \phi\epsilon_{t-1} + \phi^2\epsilon_{t-2} + \dots$ e $Y_{t-j} - \mu = \epsilon_{t-j} + \phi\epsilon_{t-j-1} + \phi^2\epsilon_{t-j-2} + \dots$
II. Calculamos a autocovari√¢ncia: $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)] = E[(\epsilon_t + \phi\epsilon_{t-1} + \phi^2\epsilon_{t-2} + \dots)(\epsilon_{t-j} + \phi\epsilon_{t-j-1} + \phi^2\epsilon_{t-j-2} + \dots)]$
III. Quando expandimos este produto, notamos que os √∫nicos termos que ter√£o expectativa diferente de zero ser√£o aqueles onde os √≠ndices dos $\epsilon$ correspondem.  Ou seja, $E[\epsilon_i \epsilon_k] = \sigma^2$ se $i=k$ e 0 caso contr√°rio.
IV. Portanto, $\gamma_j = E[\epsilon_t \epsilon_{t-j} + \phi \epsilon_{t-1} \epsilon_{t-j} + \phi^2 \epsilon_{t-2} \epsilon_{t-j} + \ldots + \phi^j \epsilon_{t-j} \epsilon_{t-j} + \phi^{j+1} \epsilon_{t-j-1} \epsilon_{t-j} + \ldots ]$
V. Isso simplifica para $\gamma_j = \sigma^2 (\phi^j + \phi^{j+2} + \phi^{j+4} + \ldots)$
VI.  Fatorando $\phi^j$, temos: $\gamma_j = \phi^j \sigma^2 (1 + \phi^2 + \phi^4 + \ldots)$
VII. A s√©rie entre par√™nteses √© uma s√©rie geom√©trica que converge para $\frac{1}{1 - \phi^2}$ quando $|\phi| < 1$.
VIII. Portanto, $\gamma_j = \frac{\phi^j}{1 - \phi^2}\sigma^2$. $\blacksquare$

**Fun√ß√£o de Autocorrela√ß√£o:**
Segue-se de [3.4.4] e [3.4.5] que a fun√ß√£o de autocorrela√ß√£o √© [3.4.6]:

$$\rho_j = \frac{\gamma_j}{\gamma_0} = \phi^j$$

> üí° **Exemplo Num√©rico:**
>
> Se $\phi = 0.6$, a fun√ß√£o de autocorrela√ß√£o para lags 0, 1, 2 e 3 √©:
>
> $\rho_0 = 0.6^0 = 1$
>
> $\rho_1 = 0.6^1 = 0.6$
>
> $\rho_2 = 0.6^2 = 0.36$
>
> $\rho_3 = 0.6^3 = 0.216$
>
> Este padr√£o de decaimento geom√©trico √© uma caracter√≠stica fundamental da fun√ß√£o de autocorrela√ß√£o de um processo AR(1).
>
> ```mermaid
> graph LR
>     A[Lag 0] --> B(1.0)
>     A[Lag 0] --> C(0.6)
>     C --> D(0.36)
>     D --> E(0.216)
>     style A fill:#f9f,stroke:#333,stroke-width:2px
> ```

**Prova da Fun√ß√£o de Autocorrela√ß√£o:**
Provaremos que $\rho_j = \frac{\gamma_j}{\gamma_0} = \phi^j$

I.  Sabemos que $\gamma_j = \frac{\phi^j}{1 - \phi^2}\sigma^2$ e $\gamma_0 = \frac{\sigma^2}{1 - \phi^2}$.
II. A fun√ß√£o de autocorrela√ß√£o √© definida como $\rho_j = \frac{\gamma_j}{\gamma_0}$.
III. Substituindo as express√µes para $\gamma_j$ e $\gamma_0$, obtemos: $\rho_j = \frac{\frac{\phi^j}{1 - \phi^2}\sigma^2}{\frac{\sigma^2}{1 - \phi^2}}$.
IV. Simplificando, vemos que $\rho_j = \phi^j$. $\blacksquare$

A fun√ß√£o de autocorrela√ß√£o [3.4.6] para um processo AR(1) estacion√°rio mostra um padr√£o de decaimento geom√©trico [^54].

**Proposi√ß√£o 1:** A fun√ß√£o de autocorrela√ß√£o $\rho_j$ satisfaz a seguinte rela√ß√£o recursiva: $\rho_j = \phi \rho_{j-1}$ para $j \geq 1$, com $\rho_0 = 1$.

*Proof:*  Dado que $\rho_j = \phi^j$, ent√£o $\rho_{j-1} = \phi^{j-1}$. Portanto, $\phi \rho_{j-1} = \phi \cdot \phi^{j-1} = \phi^j = \rho_j$. A condi√ß√£o inicial $\rho_0 = \phi^0 = 1$ tamb√©m √© satisfeita. $\blacksquare$

Agora, vamos considerar o caso onde a m√©dia do processo AR(1) n√£o √© zero.

**Teorema 1:** Seja $Y_t$ um processo AR(1) estacion√°rio definido por $Y_t = c + \phi Y_{t-1} + \epsilon_t$, onde $|\phi| < 1$. Ent√£o, o processo centrado $X_t = Y_t - \mu$ tamb√©m √© um processo AR(1) e satisfaz a equa√ß√£o $X_t = \phi X_{t-1} + \epsilon_t$.

*Proof:* Subtraindo a m√©dia $\mu = \frac{c}{1 - \phi}$ de ambos os lados da equa√ß√£o original, obtemos:

$Y_t - \mu = c + \phi Y_{t-1} + \epsilon_t - \frac{c}{1 - \phi}$

$Y_t - \mu = c + \phi Y_{t-1} + \epsilon_t - \frac{c}{1 - \phi} - \phi \frac{c}{1 - \phi} + \phi \frac{c}{1 - \phi}$

$Y_t - \mu = \phi(Y_{t-1} - \mu) + \epsilon_t + c - \frac{c}{1 - \phi} + \phi \frac{c}{1 - \phi}$

$Y_t - \mu = \phi(Y_{t-1} - \mu) + \epsilon_t + \frac{c(1 - \phi) - c + \phi c}{1 - \phi}$

$Y_t - \mu = \phi(Y_{t-1} - \mu) + \epsilon_t$

Substituindo $X_t = Y_t - \mu$, temos $X_t = \phi X_{t-1} + \epsilon_t$. $\blacksquare$

Este teorema mostra que podemos sempre trabalhar com uma vers√£o centrada do processo AR(1) sem perda de generalidade, o que simplifica a an√°lise em muitos casos.

**Teorema 1.1:** Se $X_t = \phi X_{t-1} + \epsilon_t$ √© um processo AR(1) centrado com $|\phi| < 1$ e $\epsilon_t \sim N(0, \sigma^2)$, ent√£o $X_t$ tem uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia $\frac{\sigma^2}{1 - \phi^2}$.

*Proof:* Como $X_t = \sum_{j=0}^{\infty} \phi^j \epsilon_{t-j}$ e $\epsilon_t$ s√£o independentes e normalmente distribu√≠dos, ent√£o $X_t$ √© uma soma ponderada de vari√°veis normais independentes. Portanto, $X_t$ tamb√©m √© normalmente distribu√≠do. J√° sabemos que $E[X_t] = 0$ e $Var(X_t) = \frac{\sigma^2}{1 - \phi^2}$. $\blacksquare$

**Corol√°rio 1:** Para um processo AR(1) estacion√°rio $Y_t = c + \phi Y_{t-1} + \epsilon_t$ com $|\phi| < 1$ e $\epsilon_t \sim N(0, \sigma^2)$, $Y_t$ tem uma distribui√ß√£o normal com m√©dia $\frac{c}{1 - \phi}$ e vari√¢ncia $\frac{\sigma^2}{1 - \phi^2}$.

*Proof:*  Como $Y_t = X_t + \mu$, onde $X_t$ √© definido como no Teorema 1.1 e $\mu = \frac{c}{1-\phi}$ √© uma constante, ent√£o a distribui√ß√£o de $Y_t$ √© simplesmente uma transla√ß√£o da distribui√ß√£o de $X_t$. Portanto, $Y_t$ tamb√©m √© normalmente distribu√≠do com m√©dia $E[Y_t] = E[X_t] + \mu = \mu = \frac{c}{1 - \phi}$ e vari√¢ncia $Var(Y_t) = Var(X_t) = \frac{\sigma^2}{1 - \phi^2}$. $\blacksquare$

**Proposi√ß√£o 2:** A fun√ß√£o de autocorrela√ß√£o parcial (PACF) de um processo AR(1) √© zero para todos os lags maiores que 1.

*Proof:* A PACF mede a correla√ß√£o entre $Y_t$ e $Y_{t-k}$ ap√≥s remover a influ√™ncia dos lags intermedi√°rios $Y_{t-1}, Y_{t-2}, \dots, Y_{t-k+1}$. Para um processo AR(1), a depend√™ncia de $Y_t$ em rela√ß√£o ao seu passado √© completamente capturada por $Y_{t-1}$. Portanto, a correla√ß√£o parcial entre $Y_t$ e $Y_{t-k}$ para $k > 1$, condicionada aos lags intermedi√°rios, √© zero. Matematicamente, $PACF(1) = \rho_1 = \phi$ e $PACF(k) = 0$ para $k > 1$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Considere um processo AR(1) com $\phi = 0.7$. A PACF neste caso seria:
>
> *   PACF(1) = 0.7
> *   PACF(2) = 0
> *   PACF(3) = 0
> *   ...
>
> Isso significa que, ap√≥s remover a influ√™ncia de $Y_{t-1}$, n√£o h√° correla√ß√£o adicional entre $Y_t$ e os lags maiores. Este √© um indicador chave de um processo AR(1).

### Conclus√£o
Este cap√≠tulo estabeleceu os fundamentos do processo autorregressivo de primeira ordem, **AR(1)**, definindo sua estrutura matem√°tica, explorando as condi√ß√µes de estacionariedade e derivando suas principais propriedades estat√≠sticas. A representa√ß√£o MA(‚àû) do processo AR(1) fornece uma perspectiva adicional sobre seu comportamento, ligando-o aos processos de m√©dias m√≥veis discutidos anteriormente. As propriedades da fun√ß√£o de autocorrela√ß√£o, com seu padr√£o de decaimento geom√©trico, s√£o essenciais para identificar e modelar processos AR(1) em aplica√ß√µes pr√°ticas. A compreens√£o detalhada do modelo AR(1) serve como base para o estudo de modelos autorregressivos de ordem superior e modelos ARMA mais gerais, que ser√£o explorados nos cap√≠tulos seguintes.

### Refer√™ncias
[^53]: P√°gina 53
[^54]: P√°gina 54
[^5]: P√°gina 48
[^47]: P√°gina 47
[^48]: P√°gina 48
<!-- END -->