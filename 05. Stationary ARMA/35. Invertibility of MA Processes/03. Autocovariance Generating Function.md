## T√≠tulo Conciso
Invertibilidade e Autocovari√¢ncia em Processos MA: Implica√ß√µes para a Modelagem

### Introdu√ß√£o
Em continuidade ao estudo de processos *Moving Average (MA)* e suas propriedades [^47], este cap√≠tulo aprofunda a import√¢ncia da **invertibilidade** em processos MA, focando em como essa caracter√≠stica permite reescrever um processo MA como um processo *Autoregressivo (AR)* de ordem infinita [^64]. Tal transforma√ß√£o √© de suma import√¢ncia para a previs√£o e o desenvolvimento de algoritmos de estima√ß√£o est√°veis e eficientes. A discuss√£o se concentrar√° principalmente no processo MA(1), ilustrando os conceitos-chave e as implica√ß√µes da invertibilidade. Ser√° explorado como, para um processo MA(1) n√£o invert√≠vel, uma representa√ß√£o invert√≠vel pode ser encontrada, mantendo os mesmos momentos de primeira e segunda ordem, o que destaca potenciais ambiguidades na identifica√ß√£o do modelo. A an√°lise culminar√° na demonstra√ß√£o de que a **fun√ß√£o geradora de autocovari√¢ncia** permanece a mesma para representa√ß√µes invert√≠veis e n√£o invert√≠veis, salientando a necessidade de cautela na sele√ß√£o de modelos.

### Conceitos Fundamentais

**Invertibilidade** √© uma propriedade essencial de um processo MA que garante que ele possa ser expresso como um processo AR de ordem infinita [^64]. Essa representa√ß√£o alternativa √© crucial para diversas aplica√ß√µes, incluindo previs√£o e estima√ß√£o de par√¢metros.

Considere um processo MA(1) dado por:

$$Y_t = \mu + (1 + \theta L)\epsilon_t$$ [^64, 3.7.1]

onde:
*   $Y_t$ √© o valor do processo no tempo $t$,
*   $\mu$ √© a m√©dia do processo,
*   $\theta$ √© o coeficiente do termo MA(1),
*   $L$ √© o operador *lag*,
*   $\epsilon_t$ √© o ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$ [^47, 3.2.1, 3.2.2, 3.2.3].

> üí° **Exemplo Num√©rico:** Seja $\mu = 10$, $\theta = 0.5$, e $\epsilon_t$ uma sequ√™ncia de ru√≠do branco com m√©dia 0 e vari√¢ncia 1. Ent√£o, $Y_t = 10 + (1 + 0.5L)\epsilon_t = 10 + \epsilon_t + 0.5\epsilon_{t-1}$. Se $\epsilon_t = 2$ e $\epsilon_{t-1} = -1$, ent√£o $Y_t = 10 + 2 + 0.5(-1) = 11.5$.

Para que este processo MA(1) seja invert√≠vel, √© necess√°rio que $|\theta| < 1$ [^65]. Esta condi√ß√£o garante que possamos reescrever o processo MA(1) como um processo AR(‚àû) [^64]:

$$(1 - \theta L + \theta^2 L^2 - \theta^3 L^3 + \dots)(Y_t - \mu) = \epsilon_t$$ [^65, 3.7.2]

Essa representa√ß√£o AR(‚àû) √© obtida multiplicando ambos os lados da equa√ß√£o do processo MA(1) pelo operador inverso $(1 + \theta L)^{-1}$ [^65]. A condi√ß√£o $|\theta| < 1$ assegura que a sequ√™ncia infinita $\{\theta^j\}_{j=0}^{\infty}$ convirja, tornando a representa√ß√£o AR(‚àû) bem definida [^65].

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal gerada por um processo MA(1) com $\theta = 0.7$ e $\sigma^2 = 1$. Podemos gerar 1000 valores de $\epsilon_t$ de uma distribui√ß√£o normal padr√£o usando Python e ent√£o calcular $Y_t$:
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> np.random.seed(42) # for reproducibility
> num_samples = 1000
> theta = 0.7
> sigma = 1
>
> # Generate white noise
> epsilon = np.random.normal(0, sigma, num_samples)
>
> # Calculate MA(1) process
> y = np.zeros(num_samples)
> for t in range(1, num_samples):
>     y[t] = epsilon[t] + theta * epsilon[t-1]
>
> # Plot the MA(1) process
> plt.figure(figsize=(10, 6))
> plt.plot(y)
> plt.title('MA(1) Process with Œ∏ = 0.7')
> plt.xlabel('Time')
> plt.ylabel('Y_t')
> plt.grid(True)
> plt.show()
> ```
> Este c√≥digo gera uma realiza√ß√£o de um processo MA(1) e plota a s√©rie temporal resultante. A condi√ß√£o $|\theta| < 1$ garante que o processo seja invert√≠vel, permitindo sua representa√ß√£o como um AR(‚àû).

**Prova da Converg√™ncia:** Para mostrar que a condi√ß√£o $|\theta| < 1$ garante a converg√™ncia da representa√ß√£o AR($\infty$), considere a s√©rie geom√©trica $\sum_{j=0}^{\infty} (-\theta L)^j$.

I.  A s√©rie converge se $|-\theta L| < 1$.

II. Como $L$ √© o operador *lag*, $\|L\| = 1$ (a norma do operador *lag* √© 1).

III. Portanto, a condi√ß√£o para converg√™ncia √© $|\theta| < 1$.

IV. Quando $|\theta| < 1$, a s√©rie converge para $(1 + \theta L)^{-1}$, que √© o operador inverso necess√°rio para a representa√ß√£o AR($\infty$). $\blacksquare$

> üí° **Exemplo Num√©rico:** Se $\theta = 0.5$, ent√£o a sequ√™ncia √© $\{1, -0.5, 0.25, -0.125, ...\}$. A soma dos primeiros 5 termos √© $1 - 0.5 + 0.25 - 0.125 + 0.0625 = 0.6875$. Teoricamente, a soma infinita converge para $\frac{1}{1 + 0.5} = \frac{1}{1.5} \approx 0.6667$. Se $\theta = 2$, a sequ√™ncia √© $\{1, -2, 4, -8, ...\}$, que diverge.
>
> ```mermaid
> graph LR
>     A[t-0: 1] --> B(t-1: -0.5)
>     B --> C(t-2: 0.25)
>     C --> D(t-3: -0.125)
>     D --> E(t-4: 0.0625)
>     E --> F(...)
>     style A fill:#f9f,stroke:#333,stroke-width:2px
> ```

**Observa√ß√£o:** A converg√™ncia da s√©rie geom√©trica $\sum_{j=0}^{\infty} \theta^j L^j$ para $|\theta| < 1$ √© fundamental para a invertibilidade. Essa converg√™ncia implica que o efeito de choques passados ($\epsilon_{t-j}$) diminui exponencialmente √† medida que $j$ aumenta, permitindo expressar o processo $Y_t$ como uma combina√ß√£o linear ponderada de seus valores passados.

> üí° **Exemplo Num√©rico:** Seja $\theta = 0.8$. Ent√£o, o efeito de $\epsilon_{t-1}$ em $Y_t$ √© $0.8\epsilon_{t-1}$, o efeito de $\epsilon_{t-2}$ √© $(0.8)^2\epsilon_{t-2} = 0.64\epsilon_{t-2}$, e o efeito de $\epsilon_{t-3}$ √© $(0.8)^3\epsilon_{t-3} = 0.512\epsilon_{t-3}$. Como pode ser visto, o efeito dos choques passados diminui exponencialmente. Se $\epsilon_{t-1}=1$, $\epsilon_{t-2}=2$ e $\epsilon_{t-3}=-1$, a contribui√ß√£o desses termos para a representa√ß√£o AR(‚àû) seria $0.8(1) + 0.64(2) + 0.512(-1) = 0.8 + 1.28 - 0.512 = 1.568$.

Para consolidar o entendimento da converg√™ncia, podemos apresentar a seguinte proposi√ß√£o:

**Proposi√ß√£o 1:** A converg√™ncia da s√©rie $\sum_{j=0}^{\infty} \theta^j L^j$ √© uniforme para $|\theta| < 1$.

**Demonstra√ß√£o:** Considere a s√©rie geom√©trica $\sum_{j=0}^{\infty} z^j$, onde $z = \theta L$. Se $|z| = |\theta L| < 1$, a s√©rie converge para $\frac{1}{1-z}$. Dado que $|\theta| < 1$ e $\|L\| = 1$ (a norma do operador *lag* √© 1), ent√£o $|z| = |\theta| < 1$. A converg√™ncia uniforme segue do fato de que a s√©rie geom√©trica converge uniformemente para $|z| < r$ para qualquer $r < 1$. Portanto, a converg√™ncia √© uniforme para $|\theta| < 1$. $\blacksquare$

**Demonstra√ß√£o da Invertibilidade**

Para demonstrar formalmente a invertibilidade, considere o operador inverso do processo MA(1) [^65]:

$$(1 + \theta L)^{-1} = 1 - \theta L + \theta^2 L^2 - \theta^3 L^3 + \dots$$

Se $|\theta| < 1$, a sequ√™ncia converge e o operador inverso √© bem definido [^65]. Multiplicando ambos os lados do processo MA(1) por este operador, obtemos:

$$(1 - \theta L + \theta^2 L^2 - \theta^3 L^3 + \dots)(Y_t - \mu) = (1 - \theta L + \theta^2 L^2 - \theta^3 L^3 + \dots)(1 + \theta L)\epsilon_t = \epsilon_t$$

que representa o processo MA(1) como um processo AR(‚àû) [^65].

**Prova Formal:** Para demonstrar que $(1 - \theta L + \theta^2 L^2 - \theta^3 L^3 + \dots)(1 + \theta L) = 1$, podemos multiplicar os dois operadores:

I. $(1 - \theta L + \theta^2 L^2 - \theta^3 L^3 + \dots)(1 + \theta L) = 1 + \theta L - \theta L - \theta^2 L^2 + \theta^2 L^2 + \theta^3 L^3 - \theta^3 L^3 + \dots$

II. Os termos se cancelam em pares, restando apenas 1.

III. Portanto, $(1 - \theta L + \theta^2 L^2 - \theta^3 L^3 + \dots)(1 + \theta L) = 1$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Seja $Y_t = 5 + (1 + 0.6L)\epsilon_t$, onde $\mu = 5$ e $\theta = -0.6$ (note the change to a negative value for a more illustrative AR(‚àû)). A representa√ß√£o AR(‚àû) √© $\epsilon_t = (1 + 0.6L + 0.36L^2 + 0.216L^3 + ...)(Y_t - 5)$. Se $Y_t = 7$, $Y_{t-1} = 6$, $Y_{t-2} = 5.5$, e $Y_{t-3} = 5.2$, ent√£o
> $\epsilon_t \approx (7-5) + 0.6(6-5) + 0.36(5.5-5) + 0.216(5.2-5) = 2 + 0.6 + 0.18 + 0.0432 = 2.8232$.
> Note that this is an approximation, as the AR(‚àû) series is truncated.  This shows how past values of $Y_t$ can be used to estimate the current shock $\epsilon_t$.  A larger value of $Y_t$ relative to its mean, and/or larger values of past $Y_t$ also relative to the mean, suggest a larger positive shock $\epsilon_t$.  This is because the AR(‚àû) representation expresses $\epsilon_t$ as a weighted sum of deviations of $Y_t$ from its mean.

**Lema 1:** A representa√ß√£o AR(‚àû) de um processo MA(1) invert√≠vel √© √∫nica.

**Demonstra√ß√£o:** Suponha que existam duas representa√ß√µes AR(‚àû) para o mesmo processo MA(1) invert√≠vel:
$$ (1 + \sum_{i=1}^{\infty} \phi_i L^i) (Y_t - \mu) = \epsilon_t $$
$$ (1 + \sum_{i=1}^{\infty} \psi_i L^i) (Y_t - \mu) = \epsilon_t $$
Subtraindo as duas equa√ß√µes, obtemos:
$$ (\sum_{i=1}^{\infty} (\phi_i - \psi_i) L^i) (Y_t - \mu) = 0 $$
Para que esta igualdade se mantenha para todo $t$, √© necess√°rio que $\phi_i = \psi_i$ para todo $i$, o que demonstra a unicidade da representa√ß√£o.

**Prova Detalhada:**

I. Assumimos que existem duas representa√ß√µes AR(‚àû) para o mesmo processo MA(1) invert√≠vel, como declarado acima.

II. Subtraindo as duas equa√ß√µes, obtemos a equa√ß√£o mostrada acima, que representa a diferen√ßa entre as duas representa√ß√µes.

III. Para que essa diferen√ßa seja igual a zero para todo $t$, cada coeficiente da s√©rie de pot√™ncias de $L$ deve ser zero.

IV. Isso implica que $\phi_i - \psi_i = 0$ para todo $i$, o que significa que $\phi_i = \psi_i$ para todo $i$.

V. Portanto, as duas representa√ß√µes s√£o id√™nticas, demonstrando a unicidade da representa√ß√£o AR(‚àû) de um processo MA(1) invert√≠vel. $\blacksquare$

Para complementar o Lema 1, considere a seguinte proposi√ß√£o sobre a causalidade da representa√ß√£o AR(‚àû):

**Proposi√ß√£o 2:** A representa√ß√£o AR(‚àû) de um processo MA(1) invert√≠vel √© causal.

**Demonstra√ß√£o:** Um processo AR(‚àû) √© causal se os coeficientes da representa√ß√£o AR(‚àû) forem absolutamente som√°veis. No caso do processo MA(1) invert√≠vel, a representa√ß√£o AR(‚àû) √© dada por $\epsilon_t = (1 - \theta L + \theta^2 L^2 - \theta^3 L^3 + \dots)(Y_t - \mu)$. Os coeficientes s√£o $\{\pm \theta^i\}_{i=0}^{\infty}$. A soma absoluta dos coeficientes √© $\sum_{i=0}^{\infty} |\theta^i| = \sum_{i=0}^{\infty} |\theta|^i$. Como $|\theta| < 1$, a s√©rie converge para $\frac{1}{1 - |\theta|}$, que √© um valor finito. Portanto, a representa√ß√£o AR(‚àû) √© causal.

**Demonstra√ß√£o Formal:**

I. A representa√ß√£o AR(‚àû) √© dada por $\epsilon_t = (1 - \theta L + \theta^2 L^2 - \theta^3 L^3 + \dots)(Y_t - \mu)$.

II. Os coeficientes da representa√ß√£o AR(‚àû) s√£o $\{\pm \theta^i\}_{i=0}^{\infty}$.

III. Para que a representa√ß√£o seja causal, a soma absoluta dos coeficientes deve ser finita: $\sum_{i=0}^{\infty} |\theta^i|$.

IV. Esta √© uma s√©rie geom√©trica com raz√£o $|\theta|$. A s√©rie converge se $|\theta| < 1$.

V. Se $|\theta| < 1$, a soma da s√©rie √© $\frac{1}{1 - |\theta|}$, que √© um valor finito.

VI. Portanto, a representa√ß√£o AR(‚àû) √© causal. $\blacksquare$

**Lema 2:** Se um processo MA(1) √© invert√≠vel, ent√£o sua representa√ß√£o AR(‚àû) √© est√°vel.

**Demonstra√ß√£o:** Um processo AR(‚àû) √© est√°vel se seus coeficientes decaem suficientemente r√°pido para garantir que a vari√¢ncia de $Y_t$ seja finita. Para o processo MA(1) invert√≠vel, os coeficientes da representa√ß√£o AR(‚àû) s√£o dados por $(-\theta)^i$, onde $|\theta| < 1$. A condi√ß√£o $|\theta| < 1$ garante que os coeficientes decaem exponencialmente para zero √† medida que $i$ aumenta. Portanto, a representa√ß√£o AR(‚àû) √© est√°vel.

**Demonstra√ß√£o Formal:**

I. A representa√ß√£o AR(‚àû) tem coeficientes $(-\theta)^i$, onde $|\theta| < 1$.

II. A estabilidade requer que a vari√¢ncia de $Y_t$ seja finita. Isso depende do decaimento dos coeficientes.

III. Como $|\theta| < 1$, os coeficientes decaem exponencialmente para zero √† medida que $i$ aumenta: $\lim_{i \to \infty} |(-\theta)^i| = 0$.

IV. O decaimento exponencial garante que o efeito de valores passados de $Y_t$ diminui rapidamente, resultando em uma vari√¢ncia finita.

V. Portanto, a representa√ß√£o AR(‚àû) √© est√°vel. $\blacksquare$

**Lema 2.1:** A representa√ß√£o AR(‚àû) de um processo MA(1) invert√≠vel tem coeficientes que convergem para zero √† medida que o lag aumenta.

**Demonstra√ß√£o:** No processo MA(1) invert√≠vel, a representa√ß√£o AR(‚àû) tem coeficientes dados por $(-\theta)^i$, onde $|\theta| < 1$. Conforme $i$ aumenta, $|\theta|^i$ tende a zero, j√° que $|\theta|$ √© uma fra√ß√£o entre 0 e 1. Portanto, os coeficientes da representa√ß√£o AR(‚àû) convergem para zero √† medida que o lag aumenta.

**Demonstra√ß√£o Formal:**

I. Os coeficientes da representa√ß√£o AR(‚àû) s√£o dados por $(-\theta)^i$, onde $|\theta| < 1$.

II. Queremos mostrar que $\lim_{i \to \infty} (-\theta)^i = 0$.

III. Como $|\theta| < 1$, temos que $\lim_{i \to \infty} |\theta|^i = 0$.

IV. Portanto, $\lim_{i \to \infty} (-\theta)^i = 0$.

V. Assim, os coeficientes da representa√ß√£o AR(‚àû) convergem para zero √† medida que o lag aumenta. $\blacksquare$

**Processos N√£o Invert√≠veis e Ambiguidades na Identifica√ß√£o do Modelo**

Para um processo MA(1) n√£o invert√≠vel, $Y_t = \mu + (1 + \theta L)\epsilon_t$ com $|\theta| > 1$, uma representa√ß√£o invert√≠vel com $\theta' = 1/\theta$ pode ser encontrada [^65]. Essa representa√ß√£o invert√≠vel tem momentos de primeira e segunda ordem id√™nticos ao processo n√£o invert√≠vel original, o que destaca as potenciais ambiguidades na identifica√ß√£o do modelo [^65]. A autocovari√¢ncia do processo n√£o invert√≠vel √© id√™ntica √† do seu correspondente invert√≠vel, tornando dif√≠cil distinguir os dois processos apenas com base nesses momentos [^65]. Isso levanta a quest√£o de qual representa√ß√£o usar, pois ambas descrevem o processo de forma equivalente em termos de autocovari√¢ncias.

Suponha que temos um processo MA(1) n√£o invert√≠vel:

$$\tilde{Y}_t - \mu = (1 + \tilde{\theta} L)\tilde{\epsilon}_t$$

onde $|\tilde{\theta}| > 1$ e $\tilde{\epsilon}_t$ √© ru√≠do branco [^65]. Podemos definir um processo invert√≠vel equivalente com $\theta = \tilde{\theta}^{-1}$ e $\sigma^2 = \tilde{\theta}^2\tilde{\sigma}^2$ [^65]. Ambos os processos, invert√≠vel e n√£o invert√≠vel, ter√£o a mesma fun√ß√£o de autocovari√¢ncia, conforme demonstrado pelas equa√ß√µes [3.7.3] e [3.7.5] [^65].

Para ilustrar a equival√™ncia na fun√ß√£o de autocovari√¢ncia, considere o processo MA(1) invert√≠vel $Y_t = (1 + \theta L) \epsilon_t$, onde $|\theta| < 1$, e o processo MA(1) n√£o invert√≠vel $\tilde{Y}_t = (1 + \tilde{\theta} L) \tilde{\epsilon}_t$, onde $\tilde{\theta} = 1/\theta$ e $\tilde{\epsilon}_t = \theta \epsilon_t$. A autocovari√¢ncia no lag 0 (vari√¢ncia) para o processo invert√≠vel √© $\gamma_0 = (1 + \theta^2) \sigma^2$, e para o processo n√£o invert√≠vel √© $\tilde{\gamma}_0 = (1 + \tilde{\theta}^2) \tilde{\sigma}^2 = (1 + (1/\theta)^2) (\theta^2 \sigma^2) = (\theta^2 + 1) \sigma^2 = \gamma_0$. Similarmente, a autocovari√¢ncia no lag 1 para o processo invert√≠vel √© $\gamma_1 = \theta \sigma^2$, e para o processo n√£o invert√≠vel √© $\tilde{\gamma}_1 = \tilde{\theta} \tilde{\sigma}^2 = (1/\theta) (\theta^2 \sigma^2) = \theta \sigma^2 = \gamma_1$. Para lags maiores que 1, a autocovari√¢ncia √© zero para ambos os processos. Portanto, ambos os processos t√™m a mesma fun√ß√£o de autocovari√¢ncia.

**Prova Formal da Equival√™ncia da Autocovari√¢ncia:**

I. Processo invert√≠vel: $Y_t = (1 + \theta L) \epsilon_t$, onde $|\theta| < 1$.
II. Processo n√£o invert√≠vel: $\tilde{Y}_t = (1 + \tilde{\theta} L) \tilde{\epsilon}_t$, onde $\tilde{\theta} = 1/\theta$ e $\tilde{\epsilon}_t = \theta \epsilon_t$.

III. Autocovari√¢ncia no lag 0 (vari√¢ncia) para o processo invert√≠vel:
   $\gamma_0 = \text{Var}(Y_t) = \text{Var}(\epsilon_t + \theta \epsilon_{t-1}) = \text{Var}(\epsilon_t) + \theta^2 \text{Var}(\epsilon_{t-1}) = (1 + \theta^2) \sigma^2$.

IV. Autocovari√¢ncia no lag 0 para o processo n√£o invert√≠vel:
    $\tilde{\gamma}_0 = \text{Var}(\tilde{Y}_t) = \text{Var}(\tilde{\epsilon}_t + \tilde{\theta} \tilde{\epsilon}_{t-1}) = \text{Var}(\theta \epsilon_t + (1/\theta) \theta \epsilon_{t-1}) = \theta^2 \text{Var}(\epsilon_t) + \text{Var}(\epsilon_{t-1}) = (\theta^2 + 1) \sigma^2 = \gamma_0$.

V. Autocovari√¢ncia no lag 1 para o processo invert√≠vel:
   $\gamma_1 = \text{Cov}(Y_t, Y_{t-1}) = \text{Cov}(\epsilon_t + \theta \epsilon_{t-1}, \epsilon_{t-1} + \theta \epsilon_{t-2}) = \theta \text{Var}(\epsilon_{t-1}) = \theta \sigma^2$.

VI. Autocovari√¢ncia no lag 1 para o processo n√£o invert√≠vel:
    $\tilde{\gamma}_1 = \text{Cov}(\tilde{Y}_t, \tilde{Y}_{t-1}) = \text{Cov}(\theta \epsilon_t + \epsilon_{t-1}, \theta \epsilon_{t-1} + \epsilon_{t-2}) = \theta \text{Var}(\epsilon_{t-1}) = \theta \sigma^2 = \gamma_1$.

VII. Para lags maiores que 1, a autocovari√¢ncia √© zero para ambos os processos.

VIII. Portanto, ambos os processos t√™m a mesma fun√ß√£o de autocovari√¢ncia. $\blacksquare$

> üí° **Exemplo Num√©rico:** Seja $\theta = 0.5$ e $\sigma^2 = 4$ para o processo invert√≠vel. Ent√£o $\gamma_0 = (1 + 0.5^2)(4) = (1+0.25)(4) = 5$ e $\gamma_1 = 0.5(4) = 2$. Para o processo n√£o invert√≠vel, $\tilde{\theta} = 1/0.5 = 2$ e $\tilde{\sigma}^2 = 0.5^2(4) = 0.25(4) = 1$. Then $\tilde{\gamma}_0 = (1 + 2^2)(1) = 5(1) = 5$ e $\tilde{\gamma}_1 = 2(1) = 2$. The autocovariances are identical. This demonstrates how two different MA(1) models can produce the same autocorrelation structure in a time series.  Without further information (such as knowledge of the underlying shocks), it is impossible to distinguish these models based only on observed autocorrelations.

**O Processo Causal Invert√≠vel**
Em geral, existe uma prefer√™ncia para o processo causal invert√≠vel na modelagem de s√©ries temporais. O processo causal invert√≠vel √© definido como o processo onde $|\theta|<1$. O motivo para essa prefer√™ncia √© pragm√°tico. Para encontrar o valor de $\epsilon_t$ para a data $t$ associada com a representa√ß√£o invert√≠vel como em [3.7.8], √© necess√°rio saber os valores presentes e passados de $Y$. Em contrapartida, para encontrar o valor do processo n√£o invert√≠vel $\tilde{\epsilon}_t$, √© necess√°rio o uso de todos os valores futuros de $Y$ [^65].

> üí° **Exemplo Num√©rico:** Suponha que queremos estimate $\epsilon_t$ using the invertible and non-invertible representations. For the invertible process with $\theta = 0.5$, we can approximate $\epsilon_t \approx (Y_t - \mu) - \theta(Y_{t-1} - \mu) - \theta^2(Y_{t-2} - \mu)$. For the non-invertible process with $\tilde{\theta} = 2$, we would need future values of $Y_t$ to approximate $\tilde{\epsilon}_t$. In practice, this is not feasible as future values are unknown, making the invertible process more practical for real-time estimation and forecasting.

**Teorema 1:** Dado um processo MA(q) estacion√°rio, existe um processo MA(q) invert√≠vel que gera a mesma fun√ß√£o de autocovari√¢ncia.

**Demonstra√ß√£o (Esbo√ßo):** A demonstra√ß√£o envolve encontrar as ra√≠zes do polin√¥mio associado ao processo MA(q). Se alguma raiz estiver dentro do c√≠rculo unit√°rio, ela pode ser invertida para obter uma raiz fora do c√≠rculo unit√°rio, resultando em um processo invert√≠vel com a mesma fun√ß√£o de autocovari√¢ncia.

Para o Teorema 1, podemos fornecer a seguinte prova detalhada:

**Prova do Teorema 1:** Dado um processo MA(q) estacion√°rio, existe um processo MA(q) invert√≠vel que gera a mesma fun√ß√£o de autocovari√¢ncia.

I. Seja o processo MA(q) dado por $Y_t = \Theta(L) \epsilon_t$, onde $\Theta(L) = 1 + \theta_1 L + \dots + \theta_q L^q$ e $\epsilon_t$ √© ru√≠do branco com vari√¢ncia $\sigma^2$.

II. A fun√ß√£o de autocovari√¢ncia do processo MA(q) √© determinada pelos coeficientes $\theta_1, \dots, \theta_q$.

III. Seja $z_1, \dots, z_q$ as ra√≠zes do polin√¥mio $\Theta(z) = 1 + \theta_1 z + \dots + \theta_q z^q = 0$. Se o processo MA(q) n√£o for invert√≠vel, ent√£o pelo menos uma das ra√≠zes estar√° dentro ou sobre o c√≠rculo unit√°rio, ou seja, $|z_i| \leq 1$ para algum $i$.

IV. Para cada raiz $z_i$ tal que $|z_i| \leq 1$, defina uma nova raiz $z_i^* = 1/z_i$. Note que $|z_i^*| \geq 1$. Se $z_i$ √© complexo, ent√£o $z_i^*$ tamb√©m √© complexo, e o conjugado de $z_i$, denotado por $\overline{z_i}$, √© tamb√©m uma raiz de $\Theta(z)$.

V. Forme um novo polin√¥mio $\Theta^*(z)$ substituindo cada raiz $z_i$ dentro do c√≠rculo unit√°rio por $z_i^*$. Assim, todas as ra√≠zes de $\Theta^*(z)$ est√£o fora do c√≠rculo unit√°rio, garantindo a invertibilidade.

VI. O polin√¥mio $\Theta^*(z)$ pode ser escrito como $\Theta^*(z) = 1 + \theta_1^* z + \dots + \theta_q^* z^q$.

VII. Mostrar que o processo MA(q) definido por $Y_t^* = \Theta^*(L) \epsilon_t$ tem a mesma fun√ß√£o de autocovari√¢ncia que o processo original $Y_t$. Isso segue do fato que a fun√ß√£o de autocovari√¢ncia √© unicamente determinada pelas ra√≠zes do polin√¥mio, e as ra√≠zes que foram alteradas (invertidas) n√£o alteram a fun√ß√£o de autocovari√¢ncia.

VIII. Portanto, dado um processo MA(q) estacion√°rio, existe um processo MA(q) invert√≠vel que gera a mesma fun√ß√£o de autocovari√¢ncia. $\blacksquare$

> üí° **Exemplo Num√©rico:** Consider a MA(2) process with $\Theta(L) = 1 + 1.5L + 0.5L^2$. The roots of the polynomial $\Theta(z) = 1 + 1.5z + 0.5z^2 = 0$ are $z_1 = -1$ and $z_2 = -2$. Since $|z_1| = 1$, this MA(2) process is not invertible.  We can find an invertible representation by replacing $z_1$ with $z_1^* = 1/z_1 = -1$.  The invertible polynomial is then $\Theta^*(z) = (1 + z)(1 + 0.5z) = 1 + 1.5z + 0.5z^2$, which doesn't change! In this specific instance, only one root required inverting. If $z_1 = 0.5$ and $z_2 = 2$ then the non-invertible process would be $\Theta(L) = (1-2L)(1-0.5L)= 1 - 2.5L + L^2$. Inverting 0.5 means the invertibe process would be $\Theta^*(L) = (1-2L)(1-2L) = 1 - 4L + 4L^2$.
>
> ```python
> import numpy as np
> from scipy import signal
> import matplotlib.pyplot as plt
>
> # Define the non-invertible MA coefficients
> ma_coeffs_non_invertible = np.array([1, -2.5, 1])
>
> # Find the roots of the MA polynomial
> roots = np.roots(ma_coeffs_non_invertible)
> print("Roots of the non-invertible MA polynomial:", roots)
>
> # Invert the root inside the unit circle
> roots[np.abs(roots) < 1] = 1 / roots[np.abs(roots) < 1]
> print("Inverted roots:", roots)
>
> # Compute the MA coefficients for the invertible process
> ma_coeffs_invertible = np.poly(roots)
> print("MA coefficients for the invertible process:", ma_coeffs_invertible)
>
> # Generate some random data
> np.random.seed(0)
> num_samples = 100
> white_noise = np.random.normal(0, 1, num_samples)
>
> # Apply the non-invertible MA filter
> output_non_invertible = signal.lfilter(ma_coeffs_non_invertible, [1], white_noise)
>
> # Apply the invertible MA filter
> output_invertible = signal.lfilter(ma_coeffs_invertible, [1], white_noise)
>
> # Plot the results
> plt.figure(figsize=(12, 6))
> plt.plot(output_non_invertible, label='Non-Invertible MA(2)')
> plt.plot(output_invertible, label='Invertible MA(2)')
> plt.xlabel('Time')
> plt.ylabel('Output')
> plt.title('Comparison of Non-Invertible and Invertible MA(2) Processes')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
> This example demonstrates how to find the roots of the MA polynomial, invert the root inside the unit circle, and compute the MA coefficients for the invertible process. It then generates some random data, applies the non-invertible and invertible MA filters, and plots the results for comparison. Note that the output series will look different even though they have the same autocovariance function.

**Teorema 1.1:** Seja $\Theta(L) = 1 + \theta_1 L + ... + \theta_q L^q$ o polin√¥mio do operador *lag* associado a um processo MA(q). O processo MA(q) √© invert√≠vel se e somente se as ra√≠zes de $\Theta(z) = 0$ est√£o fora do c√≠rculo unit√°rio, ou seja, $|z_i| > 1$ para todas as ra√≠zes $z_i$.

**Demonstra√ß√£o (Esbo√ßo):** A invertibilidade do processo MA(q) depende da converg√™ncia da sua representa√ß√£o AR(‚àû). A representa√ß√£o AR(‚àû) existe se e somente se o operador inverso $\Theta(L)^{-1}$ pode ser expresso como uma s√©rie de pot√™ncias convergente em $L$. Isso ocorre se e somente se as ra√≠zes do polin√¥mio $\Theta(z)$ est√£o fora do c√≠rculo unit√°rio.

Para o Teorema 1.1, podemos fornecer a seguinte prova detalhada:

**Prova do Teorema 1.1:** Seja $\Theta(L) = 1 + \theta_1 L + ... + \theta_q L^q$ o polin√¥mio do operador *lag* associado a um processo MA(q). O processo MA(q) √© invert√≠vel se e somente se as ra√≠zes de $\Theta(z) = 0$ est√£o fora do c√≠rculo unit√°rio, ou seja, $|z_i| > 1$ para todas as ra√≠zes $z_i$.

I. Considere o processo MA(q) definido por $Y_t = \Theta(L) \epsilon_t$, onde $\Theta(L) = 1 + \theta_1 L + \dots + \theta_q L^q$ e $\epsilon_t$ √© ru√≠do branco com vari√¢ncia $\sigma^2$.

II. O processo √© invert√≠vel se ele pode ser escrito como um processo AR(‚àû) da forma $\Pi(L) Y_t = \epsilon_t$, onde $\Pi(L) = 1 + \pi_1 L + \pi_2 L^2 + \dots$.

III. Equivalentemente, $\Pi(L) = \Theta(L)^{-1} \Phi(L)$.

IV. A fun√ß√£o de autocovari√¢ncia $\gamma_k$ satisfaz $\gamma_k = E[(Y_t - \mu)(Y_{t-k} - \mu)]$.

V. A fun√ß√£o de autocorrela√ß√£o (ACF) √© definida como $\rho_k = \frac{\gamma_k}{\gamma_0}$.

## Estacionariedade e Causalidade

Um processo ARMA √© estacion√°rio se suas estat√≠sticas (m√©dia, vari√¢ncia, autocovari√¢ncias) n√£o mudam com o tempo. Para um processo ARMA ser causal (estacion√°rio e invert√≠vel), as ra√≠zes do polin√¥mio AR ($\Phi(z) = 0$) e do polin√¥mio MA ($\Theta(z) = 0$) devem estar fora do c√≠rculo unit√°rio. Ou seja, $|z| > 1$ para todas as ra√≠zes $z$.

### Condi√ß√µes de Estacionariedade para AR(p)

Para um processo AR(p) da forma:

$Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \dots + \phi_p Y_{t-p} + \epsilon_t$

O polin√¥mio caracter√≠stico √©:

$\Phi(z) = 1 - \phi_1 z - \phi_2 z^2 - \dots - \phi_p z^p$

O processo √© estacion√°rio se todas as ra√≠zes de $\Phi(z) = 0$ estiverem fora do c√≠rculo unit√°rio (i.e., $|z_i| > 1$ para todas as ra√≠zes $z_i$).

### Condi√ß√µes de Invertibilidade para MA(q)

Para um processo MA(q) da forma:

$Y_t = \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q}$

O polin√¥mio caracter√≠stico √©:

$\Theta(z) = 1 + \theta_1 z + \theta_2 z^2 + \dots + \theta_q z^q$

O processo √© invert√≠vel se todas as ra√≠zes de $\Theta(z) = 0$ estiverem fora do c√≠rculo unit√°rio (i.e., $|z_i| > 1$ para todas as ra√≠zes $z_i$).

## Fun√ß√µes de Autocorrela√ß√£o (ACF) e Autocorrela√ß√£o Parcial (PACF)

As fun√ß√µes de autocorrela√ß√£o (ACF) e autocorrela√ß√£o parcial (PACF) s√£o ferramentas essenciais para identificar a ordem (p, q) de um processo ARMA.

### ACF

A ACF mede a correla√ß√£o entre $Y_t$ e $Y_{t-k}$ para diferentes valores de $k$.

- **AR(p):** A ACF decai exponencialmente ou em forma sinusoidal.
- **MA(q):** A ACF corta ap√≥s o lag q.
- **ARMA(p, q):** A ACF exibe um padr√£o de decaimento ap√≥s os primeiros q lags.

### PACF

A PACF mede a correla√ß√£o entre $Y_t$ e $Y_{t-k}$ ap√≥s remover o efeito dos lags intermedi√°rios.

- **AR(p):** A PACF corta ap√≥s o lag p.
- **MA(q):** A PACF decai exponencialmente ou em forma sinusoidal.
- **ARMA(p, q):** A PACF exibe um padr√£o de decaimento ap√≥s os primeiros p lags.

### Exemplo: AR(1)

Considere um processo AR(1):

$Y_t = \phi_1 Y_{t-1} + \epsilon_t$

A ACF para um AR(1) √©:

$\rho_k = \phi_1^k$

Se $|\phi_1| < 1$, a ACF decai exponencialmente. Se $\phi_1 > 0$, a ACF decai positivamente. Se $\phi_1 < 0$, a ACF alterna entre positivo e negativo.

A PACF para um AR(1) corta ap√≥s o lag 1.

### Exemplo: MA(1)

Considere um processo MA(1):

$Y_t = \epsilon_t + \theta_1 \epsilon_{t-1}$

A ACF para um MA(1) √©:

$\rho_1 = \frac{\theta_1}{1 + \theta_1^2}$
$\rho_k = 0$ para $k > 1$

A ACF corta ap√≥s o lag 1.

A PACF para um MA(1) decai exponencialmente ou em forma sinusoidal.

## Estima√ß√£o de Modelos ARMA

A estima√ß√£o de modelos ARMA envolve encontrar os valores dos par√¢metros $\phi$ e $\theta$ que melhor se ajustam aos dados observados. Os m√©todos comuns incluem:

1.  **M√©todo dos Momentos:** Estima os par√¢metros igualando os momentos te√≥ricos (calculados a partir do modelo) aos momentos amostrais (calculados a partir dos dados).
2.  **M√≠nimos Quadrados:** Minimiza a soma dos quadrados dos res√≠duos.
3.  **M√°xima Verossimilhan√ßa (MLE):** Encontra os valores dos par√¢metros que maximizam a fun√ß√£o de verossimilhan√ßa dos dados.

### M√°xima Verossimilhan√ßa (MLE)

A MLE √© um m√©todo estat√≠stico para estimar os par√¢metros de um modelo probabil√≠stico. Para modelos ARMA, a fun√ß√£o de verossimilhan√ßa √© baseada na distribui√ß√£o dos res√≠duos $\epsilon_t$. Sob a suposi√ß√£o de que os res√≠duos s√£o normalmente distribu√≠dos, a fun√ß√£o de verossimilhan√ßa pode ser escrita como:

$L(\phi, \theta | Y) = \prod_{t=1}^{T} f(\epsilon_t | \phi, \theta)$

Onde $f(\epsilon_t | \phi, \theta)$ √© a fun√ß√£o de densidade de probabilidade dos res√≠duos.

A MLE envolve maximizar o log da fun√ß√£o de verossimilhan√ßa:

$\log L(\phi, \theta | Y) = \sum_{t=1}^{T} \log f(\epsilon_t | \phi, \theta)$

Os valores de $\phi$ e $\theta$ que maximizam $\log L$ s√£o as estimativas de m√°xima verossimilhan√ßa.

## Diagn√≥stico do Modelo

Ap√≥s estimar um modelo ARMA, √© crucial realizar um diagn√≥stico para verificar se o modelo √© adequado para os dados. Os testes de diagn√≥stico comuns incluem:

1.  **An√°lise de Res√≠duos:**
    -   Verificar se os res√≠duos s√£o aproximadamente normalmente distribu√≠dos.
    -   Verificar se os res√≠duos s√£o independentes e n√£o correlacionados.
    -   Analisar o ACF e PACF dos res√≠duos para detectar padr√µes.
2.  **Testes de Ljung-Box:**
    -   Testar a hip√≥tese nula de que as autocorrela√ß√µes dos res√≠duos s√£o zero.
3.  **Crit√©rios de Informa√ß√£o (AIC, BIC):**
    -   Usar AIC (Akaike Information Criterion) e BIC (Bayesian Information Criterion) para comparar diferentes modelos e selecionar o modelo com o melhor trade-off entre ajuste e complexidade.

### Teste de Ljung-Box

O teste de Ljung-Box testa a hip√≥tese nula de que as autocorrela√ß√µes dos res√≠duos s√£o zero at√© um certo lag $m$. A estat√≠stica de teste √©:

$Q = n(n+2) \sum_{k=1}^{m} \frac{\hat{\rho}_k^2}{n-k}$

Onde:
- $n$ √© o tamanho da amostra.
- $\hat{\rho}_k$ √© a autocorrela√ß√£o amostral dos res√≠duos no lag $k$.
- $m$ √© o n√∫mero de lags.

Sob a hip√≥tese nula, $Q$ segue uma distribui√ß√£o $\chi^2$ com $m - p - q$ graus de liberdade. Se o valor de $p$ for menor que um n√≠vel de signific√¢ncia $\alpha$, rejeitamos a hip√≥tese nula e conclu√≠mos que os res√≠duos s√£o autocorrelacionados.

### Crit√©rios de Informa√ß√£o

Os crit√©rios de informa√ß√£o, como AIC e BIC, s√£o usados para comparar diferentes modelos e selecionar o modelo com o melhor trade-off entre ajuste e complexidade.

- **AIC (Akaike Information Criterion):**

$AIC = -2 \log L + 2(p + q + K)$

Onde:
- $L$ √© a fun√ß√£o de verossimilhan√ßa.
- $p$ √© a ordem do AR.
- $q$ √© a ordem do MA.
- $K$ √© o n√∫mero de par√¢metros adicionais (por exemplo, a vari√¢ncia do erro).

- **BIC (Bayesian Information Criterion):**

$BIC = -2 \log L + \log(n)(p + q + K)$

Onde $n$ √© o tamanho da amostra.

Modelos com valores de AIC ou BIC menores s√£o preferidos. O BIC penaliza modelos mais complexos mais fortemente do que o AIC.

## Previs√£o com Modelos ARMA

Modelos ARMA podem ser usados para prever valores futuros de uma s√©rie temporal. A previs√£o envolve usar os valores estimados dos par√¢metros $\phi$ e $\theta$ e os valores passados da s√©rie temporal para calcular previs√µes para os per√≠odos futuros.

Para um processo ARMA(p, q), a previs√£o para $Y_{t+h}$ (h per√≠odos √† frente) √©:

$\hat{Y}_{t+h} = \phi_1 Y_{t+h-1} + \dots + \phi_p Y_{t+h-p} + \epsilon_{t+h} + \theta_1 \epsilon_{t+h-1} + \dots + \theta_q \epsilon_{t+h-q}$

Onde $\hat{Y}_{t+h}$ √© a previs√£o para $Y_{t+h}$, e $\epsilon_{t+h}$ s√£o os erros de previs√£o.

A precis√£o das previs√µes depende da qualidade do modelo e da estabilidade da s√©rie temporal.

## Implementa√ß√£o em Python

```python
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.tsa.arima.model import ARIMA
import matplotlib.pyplot as plt

# Gerar dados de exemplo
np.random.seed(0)
n = 200
ar_params = np.array([0.6])
ma_params = np.array([0.3])
errors = np.random.normal(0, 1, n)

# Simular um processo ARMA(1, 1)
data = sm.tsa.arma_generate_sample(ar_params, ma_params, nsample=n)
ts = pd.Series(data)

# Ajustar um modelo ARMA(1, 1)
model = ARIMA(ts, order=(1, 0, 1)) # ordem (AR, I, MA)
model_fit = model.fit()

# Imprimir os resultados
print(model_fit.summary())

# Diagn√≥stico do modelo
residuals = pd.DataFrame(model_fit.resid)
residuals.plot()
plt.title('Res√≠duos do Modelo')
plt.show()

# Plotar ACF dos res√≠duos
fig = sm.graphics.tsa.plot_acf(residuals, lags=20)
plt.title('ACF dos Res√≠duos')
plt.show()

# Previs√£o
predictions = model_fit.predict(start=len(ts), end=len(ts)+9) # prever os pr√≥ximos 10 valores
print(predictions)

# Visualizar os resultados
plt.plot(ts, label='Dados Observados')
plt.plot(predictions, color='red', label='Previs√µes')
plt.legend()
plt.show()
```

## Considera√ß√µes Finais

Os modelos ARMA s√£o ferramentas poderosas para modelagem e previs√£o de s√©ries temporais. No entanto, √© importante entender as premissas e limita√ß√µes dos modelos e realizar diagn√≥sticos adequados para garantir que o modelo seja apropriado para os dados. A escolha da ordem (p, q) do modelo √© crucial e pode ser orientada pelas fun√ß√µes ACF e PACF, bem como pelos crit√©rios de informa√ß√£o.

<!-- END -->