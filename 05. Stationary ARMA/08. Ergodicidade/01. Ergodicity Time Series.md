## Ergodicidade em Séries Temporais: Uma Análise Detalhada

### Introdução
A **ergodicidade** é um conceito fundamental na análise de séries temporais, estabelecendo uma ligação crucial entre as propriedades estatísticas de uma série temporal e as estimativas que podemos obter a partir de uma única realização dessa série. Como veremos, a ergodicidade, juntamente com a estacionariedade, nos permite inferir propriedades populacionais a partir de uma única amostra observada, o que é essencial em muitas aplicações práticas. Este capítulo explorará em profundidade a definição de ergodicidade, suas diferentes formas e as condições necessárias para que um processo seja considerado ergódico.

### Conceitos Fundamentais
Em um contexto de séries temporais, temos duas formas principais de calcular médias: as **médias de conjunto** (ensemble averages) e as **médias temporais**. As médias de conjunto, como definido em [^3], são calculadas tomando a média de um certo valor ao longo de um grande número de realizações independentes do processo em um instante específico. Formalmente, o valor esperado de uma observação *$Y_t$* de uma série temporal é expresso como $E(Y_t) = \int_{-\infty}^{\infty} y_t f_{Y_t}(y_t) dy_t$ [^3], onde *$f_{Y_t}(y_t)$* é a densidade incondicional de *$Y_t$*. Uma forma de visualizar isso é considerando várias máquinas gerando sequências, e para um tempo *$t$* dado, calculamos a média dos valores de *$y_t$* nessas diferentes sequências [^1]. Por outro lado, as médias temporais são calculadas tomando a média de uma única realização da série temporal ao longo do tempo. Uma média temporal é expressa como $\bar{y} = (1/T)\sum_{t=1}^{T} y_t$ [^14].

A **ergodicidade** é a propriedade que garante que, sob certas condições, as médias temporais convergem para as médias de conjunto à medida que o tamanho da amostra aumenta. Essencialmente, se um processo é ergódico, podemos usar uma única realização longa da série temporal para inferir propriedades estatísticas do processo que, de outra forma, só seriam acessíveis através de uma infinidade de realizações.

#### Ergodicidade para a Média
Um processo é dito ser **ergódico para a média** se a média amostral de uma única realização da série converge em probabilidade para a média do processo quando o tamanho da amostra tende ao infinito [^15]. Formalmente, isto é expresso como:
$$
E(Y_t) = \mu = \underset{T \to \infty}{\text{plim}} \frac{1}{T} \sum_{t=1}^T Y_t
$$
onde plim denota o limite em probabilidade.  Esta condição implica que a média de uma única realização longa da série temporal converge para a média populacional, isto é, a média de todo o conjunto de realizações possíveis. Esta definição é expressa matematicamente como "o limite em probabilidade da média temporal converge para a média populacional E(Yt) quando T tende a infinito" [^1].

#### Ergodicidade para Segundos Momentos
Analogamente à ergodicidade para a média, um processo é dito ser **ergódico para segundos momentos** se as médias amostrais de quadrados e produtos defasados convergem em probabilidade para suas contrapartes populacionais, que são as autocovariâncias. Isso significa que podemos estimar a variância e as autocovariâncias do processo através de uma única realização longa da série temporal. Mais precisamente, para qualquer defasagem *$j$*, a autocovariância amostral $\frac{1}{T-j}\sum_{t=j+1}^T (Y_t - \mu)(Y_{t-j} - \mu)$ converge em probabilidade para a autocovariância populacional $\gamma_j = E[(Y_t - \mu)(Y_{t-j} - \mu)]$ à medida que *$T$* tende ao infinito [^11].

#### Relação com a Estacionariedade
A ergodicidade é uma propriedade que frequentemente aparece em conjunto com a **estacionariedade**. Um processo é **estacionário em covariância** ou **fracamente estacionário** se sua média e autocovariância não dependem do tempo [^1]. Formalmente, isso significa que $E(Y_t) = \mu$ para todos os tempos *$t$* e $E[(Y_t - \mu)(Y_{t-j} - \mu)] = \gamma_j$ para todos os tempos *$t$* e defasagens *$j$*. Se um processo é estritamente estacionário com momentos finitos, ele é automaticamente estacionário em covariância, mas o contrário nem sempre é verdadeiro [^1]. É importante observar que nem todos os processos estacionários são ergódicos e vice-versa [^16]. Um exemplo de um processo estacionário mas não ergódico é dado em [^16], onde a média para cada realização é retirada de uma distribuição normal N(0, λ^2) e adicinada a um ruído branco gaussiano com média zero e variância σ^2. Este processo é estacionário mas não é ergódico para a média.

#### Condições para Ergodicidade
Para que um processo estacionário seja ergódico, algumas condições devem ser atendidas. Para a **ergodicidade para a média**, é necessário que as autocovariâncias da série tendam a zero suficientemente rápido quando a defasagem aumenta. Formalmente, isso é expresso como $\sum_{j=0}^{\infty} |\gamma_j| < \infty$ [^15]. Esta condição garante que as dependências entre observações defasadas da série se tornem desprezíveis para defasagens suficientemente grandes, permitindo que a média amostral convirja para a média populacional. Em particular, para o processo gaussiano estacionário, a condição de ergodicidade para a média é suficiente para a ergodicidade para todos os momentos [^15].

### Implicações da Ergodicidade
A ergodicidade é crucial na análise de séries temporais por permitir que se faça inferência estatística sobre a série com base em uma única amostra.  Em essência, a ergodicidade justifica o uso de estatísticas amostrais como estimadores das propriedades populacionais de um processo. Isso é fundamental para aplicações práticas, onde geralmente temos acesso a apenas uma realização da série temporal.

#### Exemplo
Considerando o processo de ruído branco gaussiano definido em [^5]:
$$
Y_t = \mu + \epsilon_t
$$
Onde *ε<sub>t</sub>* segue uma distribuição normal com média zero e variância σ².  A autocovariância de um processo de ruído branco, como calculado em [^1], é zero para todas as defasagens *$j$* ≠ 0.  A autocovariância para *$j=0$* é igual à variância σ².  Como as autocovariâncias não dependem do tempo e são absolutamente somáveis ($\sum_{j=0}^{\infty} |\gamma_j| < \infty$) [^15], este processo é estacionário e ergódico tanto para a média como para todos os momentos.

### Conclusão
A ergodicidade é uma propriedade essencial para a análise de séries temporais. Ela nos permite obter estimativas consistentes das propriedades estatísticas de um processo a partir de uma única realização da série temporal. Enquanto a estacionariedade garante que as propriedades do processo não mudam com o tempo, a ergodicidade estabelece a ligação entre as médias temporais e as médias de conjunto, permitindo que se faça inferência estatística com base em dados observados. A compreensão da ergodicidade é fundamental para o desenvolvimento de modelos e métodos para análise de séries temporais.

### Referências
[^1]: Imagine a battery of I such computers generating sequences {y{1}, {y{2} {y}x, and consider selecting the observation associated with date t from each sequence: {y{1), y?),...,y}. This would be described as a sample of I realizations of the random variable Y.. This random variable has some density, denoted fr(y,), which is called the un- conditional density of Y.. For example, for the Gaussian white noise process, this density is given by ... The expectation of the tth observation of a time series refers to the mean of this probability distribution, provided it exists: E(Y) = ... We might view this as the probability limit of the ensemble average: E(Y) = plim ... For example, if {Y} represents the sum of a constant u plus a Gaussian white noise process {} then its mean is Y = μ + ε... Sometimes for emphasis the expectation E(Y) is called the unconditional mean of Y,. The unconditional mean is denoted μ, ... The variance of the random variable Y, (denoted you) is similarly defined as ...
[^3]: The expectation of the tth observation of a time series refers to the mean of this probability distribution, provided it exists: E(Y) = ...
[^5]: For example, if {Y} represents the sum of a constant u plus a Gaussian white noise process {} then its mean is Y = μ + ε...
[^11]: Thus [3.1.10] could be described as the covariance of Y, with its own lagged value; hence, the term "autocovariance." Notice further from [3.1.10] that the Oth au- toc covariance is just the variance of Y,, as anticipated by the notation Yo, in [3.1.9]. ... Again it may be helpful to think of the jth autocovariance as the probability limit of an ensemble average:
[^14]: We have viewed expectations of a time series in terms of ensemble averages such as [3.1.4] and [3.1.11]. ... From these observations we would cal- culate the sample mean y. This, of course, is not an ensemble average but rather a time average:  y=(1/T) \sum y(i)t
[^15]: Whether time averages such as [3.1.14] eventually converge to the ensemble concept E(Y) for a stationary process has to do with ergodicity. A covariance-stationary process is said to be ergodic for the mean if [3.1.14] converges in probability to E(Y) as T→ ∞. A process will be ergodic for the mean provided that the auto- covariance y; goes to zero sufficiently quickly as j becomes large. In Chapter 7 we will see that if the autocovariances for a covariance-stationary process satisfy \sum<0, then {Y} is ergodic for the mean.
[^16]: For purposes of clarifying the concepts of stationarity and ergodicity, however, it may be helpful to consider an example of a process that is stationary but not ergodic. Suppose the mean μ) for the ith realization {y(i)} is generated from a N(0, λ²) distribution, say Y(i) = μ(i) + εt... Thus the process of [3.1.16] is covariance-stationary. It does not satisfy the sufficient condition [3.1.15] for ergodicity for the mean, however, and indeed, the time average ... converges to μ(i) rather than to zero, the mean of Y,.
<!-- END -->
