## T√≠tulo Conciso: M√©dia Incondicional de S√©ries Temporais

### Introdu√ß√£o
Este cap√≠tulo explora o conceito de **m√©dia incondicional** de uma s√©rie temporal $Y_t$, denotada como $\mu_t$ ou $E(Y_t)$ [^8], e como ela se relaciona com a expectativa da observa√ß√£o no instante $t$. Em continuidade com o estudo das propriedades de s√©ries temporais, a m√©dia incondicional oferece *insights* sobre o comportamento central da s√©rie, considerando a possibilidade de varia√ß√£o ao longo do tempo. Compreender essa caracter√≠stica √© fundamental para a modelagem e previs√£o de s√©ries temporais.

### Conceitos Fundamentais

A **expectativa da *t*-√©sima observa√ß√£o** de uma s√©rie temporal se refere √† m√©dia de sua distribui√ß√£o de probabilidade, desde que exista [^3]. Formalmente, essa expectativa √© definida como:

$$
E(Y_t) = \int_{-\infty}^{\infty} y_t f_{Y_t}(y_t) \, dy_t
$$

onde $f_{Y_t}(y_t)$ representa a **densidade incondicional** da vari√°vel aleat√≥ria $Y_t$ [^2]. Essa integral calcula a m√©dia ponderada de todos os valores poss√≠veis de $Y_t$, utilizando a fun√ß√£o densidade de probabilidade como peso.

> üí° **Exemplo Num√©rico:** Suponha que $Y_t$ siga uma distribui√ß√£o normal com m√©dia 5 e desvio padr√£o 2. Ent√£o, $f_{Y_t}(y_t)$ √© a fun√ß√£o densidade normal com esses par√¢metros. Neste caso, $E(Y_t)$ seria simplesmente 5, pois a m√©dia da distribui√ß√£o normal √© um de seus par√¢metros. A integral acima confirma esse resultado, mas sua avalia√ß√£o anal√≠tica pode ser complexa dependendo da distribui√ß√£o.

Em termos pr√°ticos, a expectativa $E(Y_t)$ pode ser vista como o **limite de probabilidade da m√©dia do ensemble** [^4]. Para ilustrar, imagine $I$ computadores gerando sequ√™ncias $\{y_t^{(1)}, y_t^{(2)}, \dots, y_t^{(I)}\}$ [^1]. A m√©dia do ensemble no instante $t$ √© dada por:

$$
\frac{1}{I} \sum_{i=1}^{I} Y_t^{(i)}
$$

O limite de probabilidade desta m√©dia, quando $I$ tende ao infinito, √© a expectativa $E(Y_t)$:

$$
E(Y_t) = \text{plim}_{I \to \infty} \left( \frac{1}{I} \sum_{i=1}^{I} Y_t^{(i)} \right)
$$

A nota√ß√£o $\mu_t$ permite que a m√©dia seja uma fun√ß√£o do tempo $t$ [^9], abrangendo casos em que a s√©rie temporal exibe uma tend√™ncia temporal. Por exemplo, se $Y_t$ √© um processo composto por uma tend√™ncia linear ($\beta t$) mais ru√≠do branco Gaussiano ($\epsilon_t$) [^7], ou seja,

$$
Y_t = \beta t + \epsilon_t
$$

ent√£o sua m√©dia incondicional √©:

$$
E(Y_t) = \beta t
$$
*Proof:* Para demonstrar que $E(Y_t) = \beta t$, dado que $Y_t = \beta t + \epsilon_t$ e $E(\epsilon_t) = 0$:

I. Tomamos a esperan√ßa de ambos os lados da equa√ß√£o $Y_t = \beta t + \epsilon_t$:
   $$E(Y_t) = E(\beta t + \epsilon_t)$$

II. Aplicamos a propriedade da linearidade da esperan√ßa:
    $$E(Y_t) = E(\beta t) + E(\epsilon_t)$$

III. Como $\beta$ e $t$ s√£o constantes, $E(\beta t) = \beta t$. Al√©m disso, dado que $E(\epsilon_t) = 0$:
     $$E(Y_t) = \beta t + 0$$

IV. Portanto, a m√©dia incondicional √©:
    $$E(Y_t) = \beta t$$ ‚ñ†

> üí° **Exemplo Num√©rico:** Seja $\beta = 0.5$. Ent√£o, $Y_t = 0.5t + \epsilon_t$.  A m√©dia incondicional no instante $t=10$ √© $E(Y_{10}) = 0.5 \times 10 = 5$. No instante $t=20$, a m√©dia incondicional √© $E(Y_{20}) = 0.5 \times 20 = 10$. Isso ilustra como a m√©dia incondicional aumenta linearmente com o tempo.

Por outro lado, se $Y_t$ √© a soma de uma constante $\mu$ e ru√≠do branco Gaussiano $\epsilon_t$ [^5],

$$
Y_t = \mu + \epsilon_t
$$

sua m√©dia incondicional √© simplesmente:

$$
E(Y_t) = \mu
$$

Uma vez que $E(\epsilon_t) = 0$ [^6].

*Proof:* Para demonstrar que $E(Y_t) = \mu$, dado que $Y_t = \mu + \epsilon_t$ e $E(\epsilon_t) = 0$:

I. Tomamos a esperan√ßa de ambos os lados da equa√ß√£o $Y_t = \mu + \epsilon_t$:
   $$E(Y_t) = E(\mu + \epsilon_t)$$

II. Aplicamos a propriedade da linearidade da esperan√ßa:
    $$E(Y_t) = E(\mu) + E(\epsilon_t)$$

III. Como $\mu$ √© uma constante, $E(\mu) = \mu$. Al√©m disso, dado que $E(\epsilon_t) = 0$:
     $$E(Y_t) = \mu + 0$$

IV. Portanto, a m√©dia incondicional √©:
    $$E(Y_t) = \mu$$ ‚ñ†

> üí° **Exemplo Num√©rico:** Seja $\mu = 10$. Ent√£o, $Y_t = 10 + \epsilon_t$. Independentemente do valor de $t$, a m√©dia incondicional √© sempre $E(Y_t) = 10$. Isso significa que a s√©rie temporal flutua em torno do valor 10, sem apresentar tend√™ncia. Se $\epsilon_t$ fosse um ru√≠do branco com distribui√ß√£o normal com m√©dia 0 e desvio padr√£o 1, ent√£o 68% dos valores de $Y_t$ estariam entre 9 e 11.

√â importante notar que o processo descrito em [^5] √© **covariance-stationary**, o que significa que sua m√©dia $E(Y_t)=\mu$ √© constante ao longo do tempo. Em contraste, o processo descrito em [^7] **n√£o √© covariance-stationary**, pois sua m√©dia $E(Y_t) = \beta t$ varia com o tempo.

Para formalizar a no√ß√£o de estacionariedade em m√©dia, podemos definir:

**Defini√ß√£o 1:** Uma s√©rie temporal $Y_t$ √© dita *estacion√°ria em m√©dia* se $E(Y_t) = \mu$ para todo $t$, onde $\mu$ √© uma constante.

> üí° **Exemplo Num√©rico:** A s√©rie temporal $Y_t = 5 + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco, √© estacion√°ria em m√©dia porque $E(Y_t) = 5$ para todo $t$. No entanto, a s√©rie $Y_t = t + \epsilon_t$ n√£o √© estacion√°ria em m√©dia, pois $E(Y_t) = t$ varia com o tempo.

Podemos extender essa defini√ß√£o para considerar a estacionariedade em vari√¢ncia, preparando o terreno para o conceito de covariance-stationarity.

**Defini√ß√£o 1.1:** Uma s√©rie temporal $Y_t$ √© dita *estacion√°ria em vari√¢ncia* se $Var(Y_t) = \sigma^2$ para todo $t$, onde $\sigma^2$ √© uma constante.

> üí° **Exemplo Num√©rico:** Considere duas s√©ries temporais: $Y_t = 5 + \epsilon_t$ e $Z_t = 5 + t\epsilon_t$, onde $\epsilon_t \sim N(0, 1)$.  A s√©rie $Y_t$ √© estacion√°ria em vari√¢ncia porque $Var(Y_t) = Var(\epsilon_t) = 1$ para todo $t$. J√° a s√©rie $Z_t$ n√£o √© estacion√°ria em vari√¢ncia, pois $Var(Z_t) = t^2 Var(\epsilon_t) = t^2$, que varia com o tempo.

A partir dessas defini√ß√µes, podemos introduzir a defini√ß√£o formal de covariance-stationarity.

**Defini√ß√£o 2:** Uma s√©rie temporal $Y_t$ √© dita *covariance-stationary* (ou *fracamente estacion√°ria*) se satisfaz as seguintes condi√ß√µes:

1.  $E(Y_t) = \mu$ para todo $t$ (estacionariedade em m√©dia).
2.  $Var(Y_t) = \sigma^2 < \infty$ para todo $t$ (estacionariedade em vari√¢ncia).
3.  $Cov(Y_t, Y_{t-k}) = \gamma_k$ para todo $t$ e $k$, onde $\gamma_k$ √© a autocovari√¢ncia de ordem $k$ e depende apenas de $k$ e n√£o de $t$.

> üí° **Exemplo Num√©rico:** Uma s√©rie temporal gerada por $Y_t = 0.7Y_{t-1} + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia 1, √© covariance-stationary. Sua m√©dia √© 0, sua vari√¢ncia √© constante e finita, e sua autocovari√¢ncia depende apenas da dist√¢ncia $k$ entre os pontos no tempo.

Al√©m dos exemplos j√° citados, √© √∫til considerar o caso de um processo AutoRegressivo de ordem 1 (AR(1)). Suponha que $Y_t$ seja gerado por:

$$
Y_t = \phi Y_{t-1} + \epsilon_t
$$

onde $|\phi| < 1$ e $\epsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$.  Para encontrar a m√©dia incondicional deste processo, assumimos que o processo √© estacion√°rio e tomamos a expectativa de ambos os lados da equa√ß√£o:

$$
E(Y_t) = E(\phi Y_{t-1} + \epsilon_t) = \phi E(Y_{t-1}) + E(\epsilon_t)
$$

Se o processo √© estacion√°rio em m√©dia, $E(Y_t) = E(Y_{t-1}) = \mu$, ent√£o:

$$
\mu = \phi \mu + 0
$$

Resolvendo para $\mu$, obtemos:

$$
\mu = 0
$$

*Proof:* Para demonstrar que $\mu = 0$, dado que $\mu = \phi \mu + 0$ e $|\phi| < 1$:

I.  Partimos da equa√ß√£o:
    $$\mu = \phi \mu + 0$$

II. Subtra√≠mos $\phi \mu$ de ambos os lados:
    $$\mu - \phi \mu = 0$$

III. Fatoramos $\mu$ do lado esquerdo:
     $$\mu(1 - \phi) = 0$$

IV. Dividimos ambos os lados por $(1 - \phi)$. Como $|\phi| < 1$, ent√£o $(1 - \phi) \neq 0$:
    $$\mu = \frac{0}{1 - \phi}$$

V. Portanto:
   $$\mu = 0$$ ‚ñ†

Portanto, a m√©dia incondicional de um processo AR(1) com $|\phi| < 1$ e ru√≠do branco com m√©dia zero √© zero.

> üí° **Exemplo Num√©rico:** Seja $\phi = 0.5$ e $\epsilon_t$ um ru√≠do branco com m√©dia zero e vari√¢ncia 1. Ent√£o, $Y_t = 0.5Y_{t-1} + \epsilon_t$. A m√©dia incondicional desta s√©rie √© 0. Se simul√°ssemos essa s√©rie por um longo per√≠odo e calcul√°ssemos a m√©dia dos valores simulados, o resultado se aproximaria de 0.

Podemos estender este resultado para um processo AR(1) com uma constante.

**Teorema 1:** Considere o processo AR(1) definido por $Y_t = c + \phi Y_{t-1} + \epsilon_t$, onde $|\phi| < 1$, $E(\epsilon_t) = 0$ e $Var(\epsilon_t) = \sigma^2$.  Ent√£o, a m√©dia incondicional de $Y_t$ √© dada por $\mu = \frac{c}{1 - \phi}$.

*Proof:* Assumindo estacionariedade em m√©dia, $E(Y_t) = E(Y_{t-1}) = \mu$. Tomando a expectativa de ambos os lados da equa√ß√£o, temos:

$$
E(Y_t) = E(c + \phi Y_{t-1} + \epsilon_t)
$$
$$
\mu = c + \phi \mu + E(\epsilon_t)
$$
$$
\mu = c + \phi \mu + 0
$$
$$
\mu - \phi \mu = c
$$
$$
\mu(1 - \phi) = c
$$
$$
\mu = \frac{c}{1 - \phi}
$$

Este resultado √© v√°lido se $|\phi| < 1$ para garantir a converg√™ncia.

> üí° **Exemplo Num√©rico:** Seja $c = 2$ e $\phi = 0.8$. Ent√£o, $Y_t = 2 + 0.8Y_{t-1} + \epsilon_t$. A m√©dia incondicional √© $\mu = \frac{2}{1 - 0.8} = \frac{2}{0.2} = 10$. Isso significa que, em m√©dia, a s√©rie temporal flutuar√° em torno do valor 10.
>
> ```python
> import numpy as np
>
> # Par√¢metros
> c = 2
> phi = 0.8
> sigma = 1  # Desvio padr√£o do ru√≠do branco
>
> # N√∫mero de simula√ß√µes e per√≠odo
> n_simulations = 1000
> time_period = 1000
>
> # Inicializa√ß√£o
> series = np.zeros((n_simulations, time_period))
>
> # Simula√ß√£o das s√©ries temporais
> for i in range(n_simulations):
>     epsilon = np.random.normal(0, sigma, time_period)
>     series[i, 0] = np.random.normal(c / (1 - phi), sigma)  # Valor inicial
>     for t in range(1, time_period):
>         series[i, t] = c + phi * series[i, t-1] + epsilon[t]
>
> # C√°lculo da m√©dia amostral
> sample_mean = np.mean(series[:, -100:])  # M√©dia dos √∫ltimos 100 pontos
>
> # M√©dia te√≥rica
> theoretical_mean = c / (1 - phi)
>
> print(f"M√©dia amostral: {sample_mean}")
> print(f"M√©dia te√≥rica: {theoretical_mean}")
> ```

### Conclus√£o
A m√©dia incondicional $E(Y_t)$ ou $\mu_t$ desempenha um papel crucial na caracteriza√ß√£o de s√©ries temporais, fornecendo informa√ß√µes sobre o seu n√≠vel central ao longo do tempo. Seja constante ou dependente do tempo, o entendimento da m√©dia incondicional √© essencial para modelar, prever e interpretar o comportamento de uma s√©rie temporal.

### Refer√™ncias
[^1]: Imagine a battery of I such computers generating sequences {y{1}, {y{2},..., {y}x, and consider selecting the observation associated with date t from each sequence: {y{1), y?),...,y}.
[^2]: This would be described as a sample of I realizations of the random variable Y.. This random variable has some density, denoted fr(y,), which is called the un-conditional density of Y..
[^3]: The expectation of the tth observation of a time series refers to the mean of this probability distribution, provided it exists: E(Y) = ‚à´ yif(y) dy
[^4]: We might view this as the probability limit of the ensemble average: E(Y) = plim (1/1) Œ£Œ•.
[^5]: For example, if {Y} represents the sum of a constant u plus a Gaussian white noise process {}.
[^6]: then its mean is Y = Œº + Œµ,, [3.1.5] E(Y) = Œº + Œï(Œµ) = Œº. [3.1.6]
[^7]: If Y, is a time trend plus Gaussian white noise, Y‚ÇÅ = Œ≤Œπ + Œµ,, [3.1.7] then its mean is Œï(Œ•) = Œ≤Œπ. [3.1.8]
[^8]: Sometimes for emphasis the expectation E(Y) is called the unconditional mean of Y,. The unconditional mean is denoted Œº,: E(Y) = ŒºŒπ
[^9]: Note that this notation allows the general possibility that the mean can be a function of the date of the observation t. For the process [3.1.7] involving the time trend, the mean [3.1.8) is a function of time, whereas for the constant plus Gaussian white noise, the mean [3.1.6] is not a function of time.
<!-- END -->