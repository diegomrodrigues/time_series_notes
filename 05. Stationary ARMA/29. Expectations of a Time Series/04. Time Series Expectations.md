## O Significado da M√©dia Incondicional em S√©ries Temporais Estacion√°rias

### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise de s√©ries temporais estacion√°rias, com foco espec√≠fico no conceito da **m√©dia incondicional** $\mu_t$ de um processo estoc√°stico $Y_t$ [^44, ^45]. Como vimos anteriormente, a m√©dia e a autocovari√¢ncia s√£o fundamentais para a caracteriza√ß√£o de processos estacion√°rios [^45]. Expandindo o conceito apresentado, exploraremos como a m√©dia incondicional, denotada por $\mu_t$ ou $E(Y_t)$, representa o valor esperado de $Y_t$ e pode variar ou n√£o com o tempo, dependendo da natureza do processo [^44]. Especificamente, analisaremos como calcular as expectativas diretamente para modelos espec√≠ficos, como $Y_t = \mu + \epsilon_t$ (constante mais ru√≠do branco Gaussiano) e $Y_t = \beta t + \epsilon_t$ (tend√™ncia de tempo mais ru√≠do branco Gaussiano) [^44].

### Conceitos Fundamentais

A **m√©dia incondicional** $E(Y_t)$ representa o valor esperado da *t-√©sima* observa√ß√£o de uma s√©rie temporal [^44]. Formalmente, essa expectativa √© definida como a m√©dia da distribui√ß√£o de probabilidade de $Y_t$, desde que essa m√©dia exista [^44]:
$$
E(Y_t) = \int_{-\infty}^{\infty} y_t f_{Y_t}(y_t) \, dy_t
$$
onde $f_{Y_t}(y_t)$ √© a fun√ß√£o de densidade de probabilidade (pdf) incondicional de $Y_t$ [^44].

A Equa√ß√£o [3.1.3] [^44] expressa $E(Y_t)$ como a integral de $y_t$ ponderada pela sua densidade de probabilidade $f_{Y_t}(y_t)$. Isso representa a m√©dia te√≥rica da vari√°vel aleat√≥ria $Y_t$ no instante *t*.

**Interpreta√ß√£o da M√©dia Incondicional:**

A m√©dia incondicional pode ser interpretada como o limite de probabilidade da m√©dia do conjunto (ensemble average) [^44]:
$$
E(Y_t) = \text{plim}_{I \to \infty} \frac{1}{I} \sum_{i=1}^{I} Y_t^{(i)}
$$
Aqui, $Y_t^{(i)}$ representa a *i-√©sima* realiza√ß√£o da vari√°vel aleat√≥ria $Y_t$ no tempo *t*, e *I* √© o n√∫mero total de realiza√ß√µes [^44]. Essa formula√ß√£o [3.1.4] indica que, √† medida que o n√∫mero de realiza√ß√µes *I* tende ao infinito, a m√©dia amostral converge em probabilidade para a m√©dia te√≥rica $E(Y_t)$.

> üí° **Exemplo Num√©rico:** Imagine que voc√™ est√° modelando o pre√ßo di√°rio de uma a√ß√£o. Voc√™ simula esse processo 1000 vezes (I = 1000) e, para cada dia *t*, voc√™ calcula a m√©dia dos pre√ßos simulados nessas 1000 simula√ß√µes. Essa m√©dia, √† medida que *I* fica grande, converge para a m√©dia incondicional do pre√ßo da a√ß√£o no dia *t*. Se o processo for estacion√°rio, essa m√©dia ser√° a mesma para todos os dias *t*.
```python
import numpy as np

# Simula√ß√£o de I realiza√ß√µes de uma s√©rie temporal (exemplo simples)
I = 1000 # N√∫mero de realiza√ß√µes
T = 100   # N√∫mero de per√≠odos de tempo
mu = 10   # M√©dia incondicional (valor verdadeiro)
sigma = 1 # Desvio padr√£o do ru√≠do branco

Y = np.zeros((I, T))
for i in range(I):
  Y[i, :] = mu + np.random.normal(0, sigma, T)

# Calculando a m√©dia do conjunto (ensemble average) para cada ponto no tempo
ensemble_average = np.mean(Y, axis=0)

print(f"Valor verdadeiro da m√©dia incondicional: {mu}")
print(f"M√©dia do conjunto no tempo t=0: {ensemble_average[0]}")
print(f"M√©dia do conjunto no tempo t=49: {ensemble_average[49]}")
print(f"M√©dia do conjunto no tempo t=99: {ensemble_average[99]}")

# Calculando a m√©dia das m√©dias do conjunto
print(f"M√©dia das m√©dias do conjunto: {np.mean(ensemble_average)}")
```
> Nesse exemplo, simulamos uma s√©rie temporal simples onde cada observa√ß√£o √© igual a uma constante *mu* (a m√©dia incondicional) mais ru√≠do gaussiano. O c√≥digo calcula a "m√©dia do conjunto" para diferentes pontos no tempo, demonstrando que essas m√©dias se aproximam do valor verdadeiro de *mu* conforme aumentamos o n√∫mero de realiza√ß√µes.

**Lema 1.** *A m√©dia incondicional √© um operador linear.*

*Demonstra√ß√£o:* Seja $Y_t = aX_t + bZ_t$, onde $a$ e $b$ s√£o constantes e $X_t$ e $Z_t$ s√£o vari√°veis aleat√≥rias. Ent√£o:
$$
E(Y_t) = E(aX_t + bZ_t) = \int_{-\infty}^{\infty} (ax_t + bz_t) f_{X_t, Z_t}(x_t, z_t) \, dx_t \, dz_t
$$
$$
= a \int_{-\infty}^{\infty} x_t f_{X_t}(x_t) \, dx_t + b \int_{-\infty}^{\infty} z_t f_{Z_t}(z_t) \, dz_t = aE(X_t) + bE(Z_t).
$$
Portanto, a m√©dia incondicional √© um operador linear.

*Prova:* Para provar que a m√©dia incondicional √© um operador linear, devemos demonstrar que para quaisquer vari√°veis aleat√≥rias $X_t$ e $Z_t$ e constantes $a$ e $b$, $E[aX_t + bZ_t] = aE[X_t] + bE[Z_t]$.

I.  Come√ßamos com a defini√ß√£o da esperan√ßa de uma combina√ß√£o linear de vari√°veis aleat√≥rias:
    $$E[aX_t + bZ_t] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (ax_t + bz_t) f_{X_t, Z_t}(x_t, z_t) \, dx_t \, dz_t$$
    onde $f_{X_t, Z_t}(x_t, z_t)$ √© a fun√ß√£o de densidade de probabilidade conjunta de $X_t$ e $Z_t$.

II.  Usando a propriedade da integral de uma soma, podemos separar a integral em duas integrais:
    $$\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (ax_t + bz_t) f_{X_t, Z_t}(x_t, z_t) \, dx_t \, dz_t = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} ax_t f_{X_t, Z_t}(x_t, z_t) \, dx_t \, dz_t + \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} bz_t f_{X_t, Z_t}(x_t, z_t) \, dx_t \, dz_t$$

III. Como $a$ e $b$ s√£o constantes, podemos retir√°-las das integrais:
    $$a \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x_t f_{X_t, Z_t}(x_t, z_t) \, dx_t \, dz_t + b \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} z_t f_{X_t, Z_t}(x_t, z_t) \, dx_t \, dz_t$$

IV.  Agora, usamos a propriedade da densidade marginal: $\int_{-\infty}^{\infty} f_{X_t, Z_t}(x_t, z_t) \, dz_t = f_{X_t}(x_t)$ e $\int_{-\infty}^{\infty} f_{X_t, Z_t}(x_t, z_t) \, dx_t = f_{Z_t}(z_t)$.  Portanto, as integrais duplas se reduzem a:
     $$a \int_{-\infty}^{\infty} x_t f_{X_t}(x_t) \, dx_t + b \int_{-\infty}^{\infty} z_t f_{Z_t}(z_t) \, dx_t$$

V.  Reconhecemos que as integrais restantes s√£o as defini√ß√µes de $E[X_t]$ e $E[Z_t]$:
     $$a E[X_t] + b E[Z_t]$$

VI. Portanto, demonstramos que $E[aX_t + bZ_t] = aE[X_t] + bE[Z_t]$, provando que a m√©dia incondicional √© um operador linear. ‚ñ†

> üí° **Exemplo Num√©rico:** Suponha que voc√™ tenha duas s√©ries temporais, $X_t$ representando o crescimento do PIB e $Z_t$ representando as vendas de uma empresa. Voc√™ cria uma nova s√©rie temporal $Y_t = 0.5X_t + 0.2Z_t$. Se $E(X_t) = 2$ (crescimento m√©dio do PIB de 2%) e $E(Z_t) = 100$ (vendas m√©dias de 100 unidades), ent√£o $E(Y_t) = 0.5 * 2 + 0.2 * 100 = 1 + 20 = 21$. Este exemplo demonstra a linearidade da m√©dia incondicional.

**Lema 1.1.** *Se $X_t$ √© uma vari√°vel aleat√≥ria com m√©dia incondicional $E(X_t) = \mu_X$, ent√£o $E(X_t - \mu_X) = 0$.*

*Demonstra√ß√£o:*
Pelo Lema 1, a m√©dia incondicional √© um operador linear. Portanto, $E(X_t - \mu_X) = E(X_t) - E(\mu_X)$. Como $\mu_X$ √© uma constante, $E(\mu_X) = \mu_X$. Assim, $E(X_t - \mu_X) = \mu_X - \mu_X = 0$.

*Prova:* Para mostrar que $E(X_t - \mu_X) = 0$, onde $\mu_X = E(X_t)$.

I. Come√ßamos com a defini√ß√£o da esperan√ßa: $E(X_t - \mu_X) = \int_{-\infty}^{\infty} (x_t - \mu_X) f_{X_t}(x_t) \, dx_t$, onde $f_{X_t}(x_t)$ √© a fun√ß√£o de densidade de probabilidade de $X_t$.

II.  Usando a propriedade da integral, separamos a integral:
    $$\int_{-\infty}^{\infty} (x_t - \mu_X) f_{X_t}(x_t) \, dx_t = \int_{-\infty}^{\infty} x_t f_{X_t}(x_t) \, dx_t - \int_{-\infty}^{\infty} \mu_X f_{X_t}(x_t) \, dx_t$$

III. Como $\mu_X$ √© uma constante, podemos retir√°-la da integral:
    $$\int_{-\infty}^{\infty} x_t f_{X_t}(x_t) \, dx_t - \mu_X \int_{-\infty}^{\infty} f_{X_t}(x_t) \, dx_t$$

IV. Reconhecemos que a primeira integral √© a defini√ß√£o de $E[X_t] = \mu_X$, e a segunda integral √© a integral da fun√ß√£o de densidade de probabilidade sobre todo o seu dom√≠nio, que √© igual a 1:
     $$\mu_X - \mu_X \cdot 1 = \mu_X - \mu_X$$

V. Portanto, $E(X_t - \mu_X) = \mu_X - \mu_X = 0$. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal de temperaturas di√°rias em graus Celsius, onde a m√©dia incondicional √© $\mu_X = 25$. Se subtrairmos 25 de cada observa√ß√£o, a nova s√©rie temporal ter√° uma m√©dia incondicional de 0. Isso significa que, em m√©dia, as temperaturas flutuam em torno de 25 graus Celsius.
```python
import numpy as np

# S√©rie temporal de exemplo
mu_X = 25
temperaturas = np.array([23, 26, 24, 27, 25, 22, 28])

# Calculando a m√©dia da s√©rie original
media_original = np.mean(temperaturas)
print(f"M√©dia da s√©rie original: {media_original}")

# Subtraindo a m√©dia incondicional de cada observa√ß√£o
temperaturas_centralizadas = temperaturas - mu_X

# Calculando a m√©dia da s√©rie centralizada
media_centralizada = np.mean(temperaturas_centralizadas)
print(f"M√©dia da s√©rie centralizada: {media_centralizada}")
```

**Lema 1.2.** *Se $c$ √© uma constante, ent√£o $E(c) = c$.*

*Demonstra√ß√£o:* A esperan√ßa de uma constante √© a pr√≥pria constante. Isso ocorre porque a distribui√ß√£o de probabilidade de uma constante √© uma fun√ß√£o delta de Dirac centrada nessa constante.

*Prova:*
Seja $c$ uma constante. A fun√ß√£o de densidade de probabilidade de $c$ √© $f_c(x) = \delta(x - c)$, onde $\delta$ √© a fun√ß√£o delta de Dirac.

I. A esperan√ßa de $c$ √© definida como:
$$E(c) = \int_{-\infty}^{\infty} x f_c(x) \, dx = \int_{-\infty}^{\infty} x \delta(x - c) \, dx$$

II. Pela propriedade da fun√ß√£o delta de Dirac, $\int_{-\infty}^{\infty} x \delta(x - c) \, dx = c$.

III. Portanto, $E(c) = c$. ‚ñ†

> üí° **Exemplo Num√©rico:** Se tivermos uma s√©rie temporal onde cada valor √© sempre 10 (uma constante), ent√£o a m√©dia incondicional dessa s√©rie temporal ser√° 10.

**Exemplos de M√©dia Incondicional:**

1.  **Soma de uma Constante e Ru√≠do Gaussiano:**
    Se $Y_t$ √© composto por uma constante $\mu$ somada a um processo de ru√≠do branco Gaussiano $\epsilon_t$ [^44]:
    $$
    Y_t = \mu + \epsilon_t
    $$
    Ent√£o, a m√©dia incondicional √© [^44]:
    $$
    E(Y_t) = \mu + E(\epsilon_t) = \mu
    $$
    J√° que a expectativa do ru√≠do branco Gaussiano √© zero ($E(\epsilon_t) = 0$) [^44]. A equa√ß√£o [3.1.6] demonstra que a m√©dia incondicional √© simplesmente a constante $\mu$.

    *Prova:* Para provar que $E(Y_t) = \mu$, dado que $Y_t = \mu + \epsilon_t$ e $E(\epsilon_t) = 0$.

    I.  Tomamos a esperan√ßa de ambos os lados da equa√ß√£o:
        $$E(Y_t) = E(\mu + \epsilon_t)$$

    II.  Usando a linearidade do operador de esperan√ßa (Lema 1), separamos a esperan√ßa da soma:
         $$E(Y_t) = E(\mu) + E(\epsilon_t)$$

    III. Como $\mu$ √© uma constante, $E(\mu) = \mu$, e dado que $E(\epsilon_t) = 0$:
         $$E(Y_t) = \mu + 0$$

    IV. Portanto, $E(Y_t) = \mu$. ‚ñ†

> üí° **Exemplo Num√©rico:** Se $\mu = 5$ e $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia 1, ent√£o $Y_t = 5 + \epsilon_t$. A m√©dia incondicional de $Y_t$ √© simplesmente 5.  Qualquer flutua√ß√£o em $Y_t$ ao redor de 5 √© devido ao ru√≠do branco $\epsilon_t$.
```python
import numpy as np

# Par√¢metros
mu = 5
T = 100 # N√∫mero de per√≠odos de tempo

# Gerando ru√≠do branco gaussiano
epsilon = np.random.normal(0, 1, T)

# Gerando a s√©rie temporal Y_t
Y = mu + epsilon

# Calculando a m√©dia amostral
media_amostral = np.mean(Y)

print(f"M√©dia incondicional (par√¢metro mu): {mu}")
print(f"M√©dia amostral da s√©rie Y_t: {media_amostral}")
```

2.  **Tend√™ncia de Tempo Mais Ru√≠do Gaussiano:**
    Se $Y_t$ consiste em uma tend√™ncia de tempo linear $\beta t$ mais ru√≠do branco Gaussiano $\epsilon_t$ [^44]:
    $$
    Y_t = \beta t + \epsilon_t
    $$
    Ent√£o, a m√©dia incondicional √© [^44]:
    $$
    E(Y_t) = \beta t
    $$
    Neste caso, a m√©dia incondicional √© uma fun√ß√£o do tempo, variando linearmente com *t* [^44]. A equa√ß√£o [3.1.8] mostra que a m√©dia incondicional segue a tend√™ncia de tempo $\beta t$.

    *Prova:* Para provar que $E(Y_t) = \beta t$, dado que $Y_t = \beta t + \epsilon_t$ e $E(\epsilon_t) = 0$.

    I. Tomamos a esperan√ßa de ambos os lados da equa√ß√£o:
       $$E(Y_t) = E(\beta t + \epsilon_t)$$

    II. Usando a linearidade do operador de esperan√ßa, separamos a esperan√ßa da soma:
        $$E(Y_t) = E(\beta t) + E(\epsilon_t)$$

    III. Como $\beta$ e $t$ s√£o constantes em rela√ß√£o ao operador de esperan√ßa, $E(\beta t) = \beta t$, e dado que $E(\epsilon_t) = 0$:
         $$E(Y_t) = \beta t + 0$$

    IV. Portanto, $E(Y_t) = \beta t$. ‚ñ†

> üí° **Exemplo Num√©rico:** Se $\beta = 0.5$, ent√£o a m√©dia incondicional no tempo $t = 10$ √© $E(Y_{10}) = 0.5 * 10 = 5$. Isso significa que, em m√©dia, o valor de $Y_t$ aumenta em 0.5 unidades a cada per√≠odo de tempo.
```python
import numpy as np
import matplotlib.pyplot as plt

# Par√¢metros
beta = 0.5
T = 100 # N√∫mero de per√≠odos de tempo

# Gerando ru√≠do branco gaussiano
epsilon = np.random.normal(0, 1, T)

# Gerando a s√©rie temporal Y_t
t = np.arange(T)
Y = beta * t + epsilon

# Calculando a m√©dia incondicional em cada ponto no tempo
media_incondicional = beta * t

# Plotando a s√©rie temporal e a m√©dia incondicional
plt.plot(t, Y, label="Y_t (Tend√™ncia + Ru√≠do)")
plt.plot(t, media_incondicional, label="M√©dia Incondicional (beta * t)", color='red')
plt.xlabel("Tempo (t)")
plt.ylabel("Valor")
plt.title("S√©rie Temporal com Tend√™ncia Linear")
plt.legend()
plt.grid(True)
plt.show()
```
> Este c√≥digo gera uma s√©rie temporal com uma tend√™ncia linear e plota a s√©rie junto com a m√©dia incondicional, que √© simplesmente a linha de tend√™ncia.

Para ilustrar ainda mais o conceito, considere o seguinte exemplo:

3.  **Processo AR(1) com Constante:**
    Se $Y_t$ segue um processo Autorregressivo de ordem 1 (AR(1)) com uma constante $c$:
    $$
    Y_t = c + \phi Y_{t-1} + \epsilon_t
    $$
    onde $|\phi| < 1$ para garantir a estacionariedade, e $\epsilon_t$ √© ru√≠do branco com m√©dia zero.  Para encontrar a m√©dia incondicional, assumimos que $E(Y_t) = E(Y_{t-1}) = \mu$.  Aplicando o operador de esperan√ßa:
    $$
    E(Y_t) = E(c + \phi Y_{t-1} + \epsilon_t) = c + \phi E(Y_{t-1}) + E(\epsilon_t)
    $$
    $$
    \mu = c + \phi \mu + 0
    $$
    Resolvendo para $\mu$:
    $$
    \mu = \frac{c}{1 - \phi}
    $$
    Este exemplo demonstra como a m√©dia incondicional pode ser calculada para processos mais complexos, dependendo dos par√¢metros do modelo.

    *Prova:* Para derivar a m√©dia incondicional $\mu$ do processo AR(1) $Y_t = c + \phi Y_{t-1} + \epsilon_t$, onde $E(\epsilon_t) = 0$ e $|\phi| < 1$.

    I.  Assumindo estacionariedade, a m√©dia incondicional √© constante ao longo do tempo, ou seja, $E(Y_t) = E(Y_{t-1}) = \mu$.

    II. Tomamos a esperan√ßa de ambos os lados da equa√ß√£o do processo AR(1):
         $$E(Y_t) = E(c + \phi Y_{t-1} + \epsilon_t)$$

    III. Usando a linearidade do operador de esperan√ßa, separamos a esperan√ßa da soma:
          $$E(Y_t) = E(c) + E(\phi Y_{t-1}) + E(\epsilon_t)$$

    IV. Como $c$ e $\phi$ s√£o constantes, $E(c) = c$ e $E(\phi Y_{t-1}) = \phi E(Y_{t-1}) = \phi \mu$.  Tamb√©m, $E(\epsilon_t) = 0$ (dado que $\epsilon_t$ √© ru√≠do branco com m√©dia zero):
         $$\mu = c + \phi \mu + 0$$

    V.  Agora, resolvemos para $\mu$:
         $$\mu - \phi \mu = c$$
         $$\mu (1 - \phi) = c$$
         $$\mu = \frac{c}{1 - \phi}$$

    VI. Portanto, a m√©dia incondicional do processo AR(1) √© $\mu = \frac{c}{1 - \phi}$. ‚ñ†

> üí° **Exemplo Num√©rico:**  Se $c = 2$ e $\phi = 0.5$, ent√£o $\mu = \frac{2}{1 - 0.5} = 4$. Isso significa que a s√©rie temporal flutua em torno de 4, e o valor atual √© influenciado pelo valor anterior com um peso de 0.5.
```python
import numpy as np

# Par√¢metros do AR(1)
c = 2
phi = 0.5
T = 100

# Gerando ru√≠do branco gaussiano
epsilon = np.random.normal(0, 1, T)

# Inicializando a s√©rie temporal
Y = np.zeros(T)
Y[0] = c + epsilon[0]  # Primeiro valor

# Gerando a s√©rie temporal AR(1)
for t in range(1, T):
  Y[t] = c + phi * Y[t-1] + epsilon[t]

# Calculando a m√©dia amostral
media_amostral = np.mean(Y)

# Calculando a m√©dia incondicional te√≥rica
media_incondicional = c / (1 - phi)

print(f"M√©dia incondicional te√≥rica: {media_incondicional}")
print(f"M√©dia amostral da s√©rie Y_t: {media_amostral}")
```

**Proposi√ß√£o 2.** *Para um processo AR(p) estacion√°rio dado por $Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco com m√©dia zero e $|\phi_1 + \phi_2 + \ldots + \phi_p| < 1$, a m√©dia incondicional √© dada por $\mu = \frac{c}{1 - \phi_1 - \phi_2 - \ldots - \phi_p}$.*

*Demonstra√ß√£o:*
Assumindo estacionariedade, $E(Y_t) = E(Y_{t-1}) = \ldots = E(Y_{t-p}) = \mu$. Tomando a esperan√ßa de ambos os lados da equa√ß√£o do processo AR(p):
$E(Y_t) = E(c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t)$
$\mu = c + \phi_1 \mu + \phi_2 \mu + \ldots + \phi_p \mu + E(\epsilon_t)$
Como $E(\epsilon_t) = 0$:
$\mu = c + \mu (\phi_1 + \phi_2 + \ldots + \phi_p)$
$\mu (1 - \phi_1 - \phi_2 - \ldots - \phi_p) = c$
$\mu = \frac{c}{1 - \phi_1 - \phi_2 - \ldots - \phi_p}$

*Prova:* Para derivar a m√©dia incondicional $\mu$ do processo AR(p) $Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t$, onde $E(\epsilon_t) = 0$ e o processo √© estacion√°rio.

I. Assumindo estacionariedade, a m√©dia incondicional √© constante ao longo do tempo, ou seja, $E(Y_t) = E(Y_{t-1}) = \ldots = E(Y_{t-p}) = \mu$.

II. Tomamos a esperan√ßa de ambos os lados da equa√ß√£o do processo AR(p):
    $$E(Y_t) = E(c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t)$$

III. Usando a linearidade do operador de esperan√ßa, separamos a esperan√ßa da soma:
     $$E(Y_t) = E(c) + \phi_1 E(Y_{t-1}) + \phi_2 E(Y_{t-2}) + \ldots + \phi_p E(Y_{t-p}) + E(\epsilon_t)$$

IV. Como $c$, $\phi_1$, $\phi_2$, ..., $\phi_p$ s√£o constantes, $E(c) = c$ e $E(\phi_i Y_{t-i}) = \phi_i E(Y_{t-i}) = \phi_i \mu$ para $i = 1, 2, \ldots, p$. Tamb√©m, $E(\epsilon_t) = 0$ (dado que $\epsilon_t$ √© ru√≠do branco com m√©dia zero):
    $$\mu = c + \phi_1 \mu + \phi_2 \mu + \ldots + \phi_p \mu + 0$$

V. Agora, resolvemos para $\mu$:
   $$\mu = c + \mu (\phi_1 + \phi_2 + \ldots + \phi_p)$$
   $$\mu - \mu (\phi_1 + \phi_2 + \ldots + \phi_p) = c$$
   $$\mu (1 - \phi_1 - \phi_2 - \ldots - \phi_p) = c$$
   $$\mu = \frac{c}{1 - \phi_1 - \phi_2 - \ldots - \phi_p}$$

VI. Portanto, a m√©dia incondicional do processo AR(p) √© $\mu = \frac{c}{1 - \phi_1 - \phi_2 - \ldots - \phi_p}$. ‚ñ†

> üí° **Exemplo Num√©rico:** Para um processo AR(2) com $c = 5$, $\phi_1 = 0.3$ e $\phi_2 = 0.2$, a m√©dia incondicional √© $\mu = \frac{5}{1 - 0.3 - 0.2} = \frac{5}{0.5} = 10$.

**Proposi√ß√£o 2.1.** *Se um processo AR(p) √© causal e estacion√°rio, ent√£o as ra√≠zes do polin√¥mio caracter√≠stico associado est√£o fora do c√≠rculo unit√°rio.*

*Demonstra√ß√£o:*
Um processo AR(p) √© causal se ele pode ser escrito como uma fun√ß√£o linear infinita de seus choques passados. Isso √© equivalente a dizer que o processo √© est√°vel. A condi√ß√£o para causalidade e estacionariedade √© que as ra√≠zes do polin√¥mio caracter√≠stico associado ao processo AR(p) estejam fora do c√≠rculo unit√°rio no plano complexo.

*Prova:*
Seja o processo AR(p) dado por:
$Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t$
onde $\epsilon_t$ √© ru√≠do branco com m√©dia zero.

I. O polin√¥mio caracter√≠stico associado ao processo AR(p) √© definido como:
$\Phi(z) = 1 - \phi_1 z - \phi_2 z^2 - \ldots - \phi_p z^p$

II. A causalidade e a estacionariedade do processo AR(p) implicam que ele pode ser expresso como uma representa√ß√£o de m√©dia m√≥vel (MA) infinita:
$Y_t = \mu + \sum_{i=0}^{\infty} \psi_i \epsilon_{t-i}$
onde $\mu$ √© a m√©dia incondicional do processo e os $\psi_i$ s√£o os coeficientes da representa√ß√£o MA.

III. Para que a representa√ß√£o MA infinita seja bem definida (isto √©, para que a soma convirja), os coeficientes $\psi_i$ devem decair para zero √† medida que $i$ tende ao infinito. Isso ocorre se e somente se as ra√≠zes do polin√¥mio caracter√≠stico $\Phi(z)$ estiverem fora do c√≠rculo unit√°rio no plano complexo.

IV. Se alguma raiz $z_0$ do polin√¥mio caracter√≠stico estiver dentro ou sobre o c√≠rculo unit√°rio (ou seja, $|z_0| \leq 1$), ent√£o o processo AR(p) n√£o ser√° causal nem estacion√°rio. Neste caso, a representa√ß√£o MA infinita n√£o convergir√°, e o processo n√£o ser√° est√°vel.

V. Portanto, para que um processo AR(p) seja causal e estacion√°rio, todas as ra√≠zes do polin√¥mio caracter√≠stico associado devem estar fora do c√≠rculo unit√°rio. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere um processo AR(1) dado por $Y_t = 0.5 Y_{t-1} + \epsilon_t$. O polin√¥mio caracter√≠stico √© $\Phi(z) = 1 - 0.5z$. A raiz √© $z = 2$, que est√° fora do c√≠rculo unit√°rio. Portanto, o processo √© causal e estacion√°rio.

**Estacionariedade e M√©dia Incondicional:**
A estacionariedade de um processo estoc√°stico est√° diretamente relacionada ao comportamento de sua m√©dia incondicional e autocovari√¢ncias [^45]. Em particular, um processo √© considerado *covariance-stationary* (ou *weakly stationary*) se [^45]:
*   A m√©dia incondicional $E(Y_t) = \mu$ √© constante e n√£o depende do tempo *t* [^45].
*   A autocovari√¢ncia $\gamma_{jt} = E[(Y_t - \mu)(Y_{t-j} - \mu)]$ depende apenas da diferen√ßa de tempo *j* e n√£o de *t* [^45].

A independ√™ncia do tempo tanto da m√©dia quanto das autocovari√¢ncias √© crucial para a estacionariedade, facilitando a an√°lise e a modelagem do processo [^45].

Al√©m da estacionariedade fraca, √© importante mencionar a estacionariedade forte:

*Um processo estoc√°stico √© dito fortemente estacion√°rio se a distribui√ß√£o conjunta de $(Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n})$ √© a mesma que a distribui√ß√£o conjunta de $(Y_{t_1+k}, Y_{t_2+k}, \ldots, Y_{t_n+k})$ para qualquer inteiro $k$ e qualquer conjunto de instantes de tempo $t_1, t_2, \ldots, t_n$.*

A estacionariedade forte implica que todas as estat√≠sticas do processo s√£o invariantes ao tempo, n√£o apenas a m√©dia e a autocovari√¢ncia.

A conex√£o entre estacionariedade e m√©dia incondicional pode ser formalizada atrav√©s do seguinte lema:

**Lema 3.** *Se um processo estoc√°stico $Y_t$ √© fracamente estacion√°rio, ent√£o sua m√©dia incondicional $E[Y_t]$ √© constante.*

*Prova:* A estacionariedade fraca (ou estacionariedade de covari√¢ncia) requer que a m√©dia do processo seja independente do tempo. Por defini√ß√£o, $E[Y_t] = \mu$ para todo $t$, onde $\mu$ √© uma constante. Portanto, se $Y_t$ √© fracamente estacion√°rio, sua m√©dia incondicional √© constante. ‚ñ†

**Corol√°rio 3.1.** *Se a m√©dia incondicional $E[Y_t]$ de um processo estoc√°stico $Y_t$ varia com o tempo, ent√£o o processo n√£o √© fracamente estacion√°rio.*

Este corol√°rio √© a contrapositiva do Lema 3 e segue diretamente dele. Ele fornece um teste simples para a n√£o estacionariedade: se voc√™ conseguir mostrar que a m√©dia incondicional de um processo varia com o tempo, ent√£o voc√™ pode concluir que o processo n√£o √© fracamente estacion√°rio. Este corol√°rio √© extremamente √∫til na pr√°tica para identificar s√©ries temporais que precisam ser transformadas (por exemplo, atrav√©s de diferencia√ß√£o) para torn√°-las estacion√°rias antes da modelagem.

**Observa√ß√£o:** √â importante notar que um processo fracamente estacion√°rio n√£o √© necessariamente fortemente estacion√°rio, e vice-versa. No entanto, se um processo √© Gaussiano e fracamente estacion√°rio, ent√£o ele tamb√©m √© fortemente estacion√°rio, pois a distribui√ß√£o normal √© completamente caracterizada por sua m√©dia e vari√¢ncia (e, portanto, autocovari√¢ncia).

### Conclus√£o

A m√©dia incondicional $\mu_t$ √© um conceito central na an√°lise de s√©ries temporais, representando o valor esperado de $Y_t$ e influenciando diretamente a estacionariedade do processo [^44, ^45]. A remo√ß√£o da m√©dia, quando necess√°ria, √© um passo crucial para modelar adequadamente a s√©rie e aplicar t√©cnicas estat√≠sticas apropriadas.

### Autocovari√¢ncia e Autocorrela√ß√£o

A **autocovari√¢ncia** e a **autocorrela√ß√£o** s√£o medidas que quantificam a depend√™ncia linear entre os valores de uma s√©rie temporal em diferentes pontos no tempo.

#### Autocovari√¢ncia

A autocovari√¢ncia em um lag $k$, denotada por $\gamma_k$, √© definida como:

$$
\gamma_k = Cov(Y_t, Y_{t-k}) = E[(Y_t - \mu)(Y_{t-k} - \mu)]
$$

onde:
- $Y_t$ √© o valor da s√©rie no tempo $t$,
- $Y_{t-k}$ √© o valor da s√©rie $k$ per√≠odos antes,
- $\mu$ √© a m√©dia da s√©rie.

A autocovari√¢ncia mede como os valores da s√©rie em um dado ponto no tempo se relacionam com os valores em momentos anteriores.

#### Autocorrela√ß√£o

A autocorrela√ß√£o em um lag $k$, denotada por $\rho_k$, √© a autocovari√¢ncia normalizada pela vari√¢ncia da s√©rie:

$$
\rho_k = \frac{\gamma_k}{\gamma_0}
$$

onde:
- $\gamma_k$ √© a autocovari√¢ncia no lag $k$,
- $\gamma_0$ √© a vari√¢ncia da s√©rie (autocovari√¢ncia no lag 0).

A autocorrela√ß√£o varia entre -1 e 1 e indica a for√ßa e a dire√ß√£o da rela√ß√£o linear entre os valores da s√©rie em diferentes pontos no tempo.

### Fun√ß√£o de Autocorrela√ß√£o (ACF)

A **Fun√ß√£o de Autocorrela√ß√£o (ACF)** √© um gr√°fico que mostra os valores de $\rho_k$ para diferentes lags $k$. A ACF √© uma ferramenta essencial para identificar padr√µes de depend√™ncia temporal em uma s√©rie.

#### Interpreta√ß√£o da ACF

- **Decaimento Lento:** Um decaimento lento na ACF indica que a s√©rie tem uma alta persist√™ncia e pode ser n√£o estacion√°ria.
- **Cortes:** Cortes abruptos na ACF ap√≥s um certo lag indicam que a s√©rie pode ser modelada com um modelo de m√©dia m√≥vel (MA).
- **Padr√µes Sazonais:** Picos em lags correspondentes a per√≠odos sazonais indicam a presen√ßa de sazonalidade na s√©rie.

### Fun√ß√£o de Autocorrela√ß√£o Parcial (PACF)

A **Fun√ß√£o de Autocorrela√ß√£o Parcial (PACF)** mede a correla√ß√£o entre $Y_t$ e $Y_{t-k}$ ap√≥s remover a influ√™ncia dos lags intermedi√°rios $Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1}$. A PACF ajuda a identificar a ordem de um modelo autorregressivo (AR).

#### Interpreta√ß√£o da PACF

- **Cortes:** Cortes abruptos na PACF ap√≥s um certo lag indicam que a s√©rie pode ser modelada com um modelo autorregressivo (AR).
- **Decaimento Lento:** Um decaimento lento na PACF pode indicar que a s√©rie n√£o √© estacion√°ria ou que requer diferencia√ß√£o.

### Testes de Estacionariedade

Os **testes de estacionariedade** s√£o utilizados para determinar se uma s√©rie temporal √© estacion√°ria.

#### Teste de Dickey-Fuller Aumentado (ADF)

O **Teste de Dickey-Fuller Aumentado (ADF)** √© um teste estat√≠stico para verificar a presen√ßa de raiz unit√°ria em uma s√©rie temporal. A hip√≥tese nula do teste ADF √© que a s√©rie tem uma raiz unit√°ria (n√£o √© estacion√°ria), enquanto a hip√≥tese alternativa √© que a s√©rie √© estacion√°ria.

##### Formula√ß√£o do Teste ADF

O teste ADF envolve a estima√ß√£o da seguinte regress√£o:

$$
\Delta Y_t = \alpha + \beta Y_{t-1} + \sum_{i=1}^{p} \gamma_i \Delta Y_{t-i} + \varepsilon_t
$$

onde:
- $\Delta Y_t = Y_t - Y_{t-1}$ √© a primeira diferen√ßa da s√©rie,
- $\alpha$ √© uma constante (opcional),
- $\beta$ √© o coeficiente de $Y_{t-1}$,
- $\gamma_i$ s√£o os coeficientes dos lags da primeira diferen√ßa,
- $p$ √© o n√∫mero de lags inclu√≠dos na regress√£o,
- $\varepsilon_t$ √© o termo de erro.

A estat√≠stica de teste √© calculada como:

$$
t = \frac{\hat{\beta}}{SE(\hat{\beta})}
$$

onde:
- $\hat{\beta}$ √© a estimativa do coeficiente $\beta$,
- $SE(\hat{\beta})$ √© o erro padr√£o de $\hat{\beta}$.

##### Interpreta√ß√£o do Teste ADF

- **Rejei√ß√£o da Hip√≥tese Nula:** Se o valor da estat√≠stica de teste for menor do que o valor cr√≠tico (ou o p-valor for menor do que o n√≠vel de signific√¢ncia), rejeitamos a hip√≥tese nula de que a s√©rie tem uma raiz unit√°ria e conclu√≠mos que a s√©rie √© estacion√°ria.
- **N√£o Rejei√ß√£o da Hip√≥tese Nula:** Se o valor da estat√≠stica de teste for maior do que o valor cr√≠tico (ou o p-valor for maior do que o n√≠vel de signific√¢ncia), n√£o rejeitamos a hip√≥tese nula e conclu√≠mos que a s√©rie n√£o √© estacion√°ria.

#### Teste de Kwiatkowski-Phillips-Schmidt-Shin (KPSS)

O **Teste de Kwiatkowski-Phillips-Schmidt-Shin (KPSS)** √© um teste estat√≠stico para verificar a estacionariedade em torno de uma m√©dia ou tend√™ncia determin√≠stica. Diferente do teste ADF, a hip√≥tese nula do teste KPSS √© que a s√©rie √© estacion√°ria, enquanto a hip√≥tese alternativa √© que a s√©rie tem uma raiz unit√°ria (n√£o √© estacion√°ria).

##### Formula√ß√£o do Teste KPSS

O teste KPSS envolve a estima√ß√£o da seguinte regress√£o:

$$
Y_t = \alpha + \beta t + r_t + \varepsilon_t
$$

onde:
- $Y_t$ √© o valor da s√©rie no tempo $t$,
- $\alpha$ √© uma constante,
- $\beta$ √© o coeficiente da tend√™ncia temporal $t$,
- $r_t$ √© um passeio aleat√≥rio,
- $\varepsilon_t$ √© o termo de erro.

A estat√≠stica de teste √© calculada com base na soma cumulativa dos res√≠duos da regress√£o.

##### Interpreta√ß√£o do Teste KPSS

- **Rejei√ß√£o da Hip√≥tese Nula:** Se o valor da estat√≠stica de teste for maior do que o valor cr√≠tico, rejeitamos a hip√≥tese nula de que a s√©rie √© estacion√°ria e conclu√≠mos que a s√©rie n√£o √© estacion√°ria.
- **N√£o Rejei√ß√£o da Hip√≥tese Nula:** Se o valor da estat√≠stica de teste for menor do que o valor cr√≠tico, n√£o rejeitamos a hip√≥tese nula e conclu√≠mos que a s√©rie √© estacion√°ria.

### Transforma√ß√µes para Estacionaridade

Quando uma s√©rie temporal n√£o √© estacion√°ria, √© necess√°rio aplicar transforma√ß√µes para torn√°-la estacion√°ria.

#### Diferencia√ß√£o

A **diferencia√ß√£o** envolve calcular a diferen√ßa entre os valores consecutivos da s√©rie. A primeira diferen√ßa √© dada por:

$$
\Delta Y_t = Y_t - Y_{t-1}
$$

Se a primeira diferen√ßa n√£o tornar a s√©rie estacion√°ria, pode-se aplicar a diferencia√ß√£o de ordem superior:

$$
\Delta^2 Y_t = \Delta Y_t - \Delta Y_{t-1}
$$

#### Transforma√ß√£o Logar√≠tmica

A **transforma√ß√£o logar√≠tmica** √© utilizada para estabilizar a vari√¢ncia de uma s√©rie temporal. A transforma√ß√£o logar√≠tmica √© dada por:

$$
Y_t' = \log(Y_t)
$$

#### Defla√ß√£o

A **defla√ß√£o** √© utilizada para remover a influ√™ncia da infla√ß√£o em s√©ries financeiras. A defla√ß√£o envolve dividir a s√©rie por um √≠ndice de pre√ßos:

$$
Y_t' = \frac{Y_t}{P_t}
$$

onde $P_t$ √© o √≠ndice de pre√ßos no tempo $t$.

### Exemplo Pr√°tico

Considere a s√©rie temporal do Produto Interno Bruto (PIB) do Brasil. Inicialmente, a s√©rie pode apresentar tend√™ncia crescente e n√£o ser estacion√°ria. Para tornar a s√©rie estacion√°ria, podemos aplicar a transforma√ß√£o logar√≠tmica e a diferencia√ß√£o.

1.  **Transforma√ß√£o Logar√≠tmica:** Aplicamos a transforma√ß√£o logar√≠tmica para estabilizar a vari√¢ncia da s√©rie.
2.  **Diferencia√ß√£o:** Calculamos a primeira diferen√ßa da s√©rie logar√≠tmica para remover a tend√™ncia.
3.  **Teste de Estacionariedade:** Aplicamos o teste ADF e KPSS para verificar se a s√©rie transformada √© estacion√°ria.

### Conclus√£o

A estacionariedade √© uma propriedade fundamental para a modelagem de s√©ries temporais. A an√°lise da m√©dia, autocovari√¢ncia, autocorrela√ß√£o, ACF e PACF, juntamente com os testes de estacionariedade e transforma√ß√µes apropriadas, permite a modelagem adequada da s√©rie temporal e a obten√ß√£o de previs√µes precisas.

[^44]: Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time series analysis: Forecasting and control*. John Wiley & Sons.
[^45]: Enders, W. (2010). *Applied econometric time series*. John Wiley & Sons.
<!-- END -->