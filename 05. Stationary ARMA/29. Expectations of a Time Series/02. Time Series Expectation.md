## T√≠tulo Conciso: M√©dia Incondicional com Tend√™ncia Temporal e Ru√≠do

### Introdu√ß√£o

Este cap√≠tulo aprofunda a an√°lise da **m√©dia incondicional** $E(Y_t)$ em s√©ries temporais, com foco espec√≠fico em dois casos distintos: quando $Y_t$ √© modelado como uma **tend√™ncia temporal linear** ($\beta t$) somada a um **ru√≠do branco Gaussiano** ($\epsilon_t$) e quando $Y_t$ √© representado como a soma de uma **constante** ($\mu$) com um **ru√≠do branco Gaussiano** ($\epsilon_t$) [^5, ^7]. Expandindo o conceito apresentado anteriormente [^8], exploraremos as implica√ß√µes de cada modelagem para a expectativa da s√©rie temporal e, consequentemente, para sua estacionariedade.

### Conceitos Fundamentais

Como vimos anteriormente [^3], a expectativa da *$t$*-√©sima observa√ß√£o de uma s√©rie temporal, $E(Y_t)$, representa a m√©dia de sua distribui√ß√£o de probabilidade. Essa expectativa pode ser interpretada como o limite de probabilidade da m√©dia do ensemble [^4], refletindo o valor m√©dio que se esperaria observar no instante $t$ ao longo de m√∫ltiplas realiza√ß√µes da s√©rie temporal.

**Caso 1: Tend√™ncia Temporal Linear e Ru√≠do Branco Gaussiano**

Quando modelamos $Y_t$ como uma tend√™ncia temporal linear somada a um ru√≠do branco Gaussiano, temos:

$$
Y_t = \beta t + \epsilon_t
$$

onde $\beta$ √© o coeficiente da tend√™ncia temporal e $\epsilon_t$ √© um processo de ru√≠do branco Gaussiano com m√©dia zero, ou seja, $E(\epsilon_t) = 0$. Nesse caso, a m√©dia incondicional de $Y_t$ √© dada por:

$$
E(Y_t) = E(\beta t + \epsilon_t) = \beta t + E(\epsilon_t) = \beta t + 0 = \beta t
$$

Esse resultado implica que a m√©dia da s√©rie temporal varia linearmente com o tempo, com uma taxa de varia√ß√£o determinada pelo coeficiente $\beta$.

*Proof:*
I. Dado que $Y_t = \beta t + \epsilon_t$, onde $\beta$ √© uma constante e $\epsilon_t$ √© um ru√≠do branco com $E(\epsilon_t) = 0$.

II. Aplicando o operador de expectativa, temos:
    $$E(Y_t) = E(\beta t + \epsilon_t)$$

III. Usando a linearidade do operador de expectativa:
    $$E(Y_t) = E(\beta t) + E(\epsilon_t)$$

IV. Como $\beta$ e $t$ s√£o determin√≠sticos, $E(\beta t) = \beta t$.

V. Dado que $E(\epsilon_t) = 0$, temos:
    $$E(Y_t) = \beta t + 0 = \beta t$$

Portanto, $E(Y_t) = \beta t$ ‚ñ†

> üí° **Exemplo Num√©rico:** Suponha que $\beta = 0.5$. Isso significa que, em m√©dia, a s√©rie temporal aumenta em 0.5 unidades a cada per√≠odo. Se observarmos os primeiros 5 per√≠odos, teremos:
>
> *   $E(Y_1) = 0.5 * 1 = 0.5$
> *   $E(Y_2) = 0.5 * 2 = 1.0$
> *   $E(Y_3) = 0.5 * 3 = 1.5$
> *   $E(Y_4) = 0.5 * 4 = 2.0$
> *   $E(Y_5) = 0.5 * 5 = 2.5$
>
> Isso demonstra o aumento linear da m√©dia da s√©rie temporal ao longo do tempo.

**Caso 2: Constante e Ru√≠do Branco Gaussiano**

Alternativamente, se modelamos $Y_t$ como a soma de uma constante $\mu$ e um ru√≠do branco Gaussiano $\epsilon_t$, temos:

$$
Y_t = \mu + \epsilon_t
$$

Nesse cen√°rio, a m√©dia incondicional de $Y_t$ √© simplesmente:

$$
E(Y_t) = E(\mu + \epsilon_t) = \mu + E(\epsilon_t) = \mu + 0 = \mu
$$

Isso indica que a m√©dia da s√©rie temporal √© constante e igual a $\mu$, independentemente do tempo $t$.

*Proof:*
I. Dado que $Y_t = \mu + \epsilon_t$, onde $\mu$ √© uma constante e $\epsilon_t$ √© um ru√≠do branco com $E(\epsilon_t) = 0$.

II. Aplicando o operador de expectativa, temos:
    $$E(Y_t) = E(\mu + \epsilon_t)$$

III. Usando a linearidade do operador de expectativa:
    $$E(Y_t) = E(\mu) + E(\epsilon_t)$$

IV. Como $\mu$ √© uma constante, $E(\mu) = \mu$.

V. Dado que $E(\epsilon_t) = 0$, temos:
    $$E(Y_t) = \mu + 0 = \mu$$

Portanto, $E(Y_t) = \mu$ ‚ñ†

> üí° **Exemplo Num√©rico:** Suponha que $\mu = 10$. Ent√£o, $E(Y_t) = 10$ para todo $t$. Mesmo que os valores observados de $Y_t$ flutuem devido ao ru√≠do $\epsilon_t$, a m√©dia da s√©rie temporal permanecer√° constante em 10. Por exemplo, se tivermos as seguintes realiza√ß√µes para os primeiros 3 per√≠odos:
>
> *   $Y_1 = 10.5$ (com $\epsilon_1 = 0.5$)
> *   $Y_2 = 9.8$ (com $\epsilon_2 = -0.2$)
> *   $Y_3 = 10.2$ (com $\epsilon_3 = 0.2$)
>
> A m√©dia $E(Y_t)$ permanece constante em 10, independentemente das flutua√ß√µes do ru√≠do.

**Implica√ß√µes para a Estacionariedade**

A distin√ß√£o entre esses dois casos √© fundamental para determinar a estacionariedade da s√©rie temporal. Como definimos anteriormente, uma s√©rie temporal √© covariance-stationary se sua m√©dia, vari√¢ncia e autocovari√¢ncia n√£o dependem do tempo.

No **Caso 1**, onde $E(Y_t) = \beta t$, a m√©dia incondicional varia linearmente com o tempo. Portanto, a s√©rie temporal *n√£o √©* covariance-stationary.

No **Caso 2**, onde $E(Y_t) = \mu$, a m√©dia incondicional √© constante. Para que a s√©rie seja covariance-stationary, a vari√¢ncia e a autocovari√¢ncia tamb√©m devem ser independentes do tempo. Se $\epsilon_t$ for um ru√≠do branco Gaussiano com vari√¢ncia constante, ent√£o a s√©rie temporal *pode ser* covariance-stationary, dependendo das propriedades das autocovari√¢ncias.

**Lemma 1:** Se $Y_t = \mu + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, ent√£o $Y_t$ √© estacion√°rio em m√©dia e vari√¢ncia.

*Proof:* J√° demonstramos que $E(Y_t) = \mu$. Para a vari√¢ncia:

$$
Var(Y_t) = Var(\mu + \epsilon_t) = Var(\epsilon_t) = \sigma^2
$$

Como $\mu$ e $\sigma^2$ s√£o constantes, $Y_t$ √© estacion√°rio em m√©dia e vari√¢ncia. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere $Y_t = 25 + \epsilon_t$, onde $\epsilon_t \sim N(0, 4)$. Isso significa que $\mu = 25$ e $\sigma^2 = 4$. A m√©dia de $Y_t$ √© sempre 25, e a vari√¢ncia √© sempre 4, independentemente de *$t$*. Portanto, a s√©rie √© estacion√°ria em m√©dia e vari√¢ncia. Podemos simular essa s√©rie temporal e verificar sua estacionariedade visualmente e por meio de testes estat√≠sticos.

**Teorema 2:** Se $Y_t = \mu + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco *n√£o correlacionado* com m√©dia zero e vari√¢ncia $\sigma^2$, ent√£o $Y_t$ √© covariance-stationary.

*Proof:* J√° demonstramos que $E(Y_t) = \mu$ e $Var(Y_t) = \sigma^2$. Para a autocovari√¢ncia:

$$
Cov(Y_t, Y_{t-k}) = E[(Y_t - E(Y_t))(Y_{t-k} - E(Y_{t-k}))] = E[(\epsilon_t)(\epsilon_{t-k})]
$$

Como $\epsilon_t$ √© um ru√≠do branco n√£o correlacionado, $E[\epsilon_t \epsilon_{t-k}] = 0$ para $k \neq 0$ e $E[\epsilon_t \epsilon_{t-k}] = \sigma^2$ para $k = 0$. Portanto, $Cov(Y_t, Y_{t-k})$ depende apenas de $k$ e n√£o de $t$. Assim, $Y_t$ √© covariance-stationary. $\blacksquare$

> üí° **Exemplo Num√©rico:** Seja $Y_t = 5 + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco com m√©dia 0 e vari√¢ncia 1. Calculando a autocovari√¢ncia para diferentes lags:
>
> *   $Cov(Y_t, Y_t) = E[(\epsilon_t)^2] = 1$
> *   $Cov(Y_t, Y_{t-1}) = E[\epsilon_t \epsilon_{t-1}] = 0$
> *   $Cov(Y_t, Y_{t-2}) = E[\epsilon_t \epsilon_{t-2}] = 0$
>
> Isso mostra que a autocovari√¢ncia √© 1 para lag 0 (a vari√¢ncia) e 0 para todos os outros lags, o que confirma a covariance-stationarity.
>

Para complementar a an√°lise da estacionariedade no Caso 2, podemos considerar o seguinte resultado:

**Teorema 2.1:** Seja $Y_t = \mu + \epsilon_t$, onde $\epsilon_t$ √© um processo i.i.d. (independentemente e identicamente distribu√≠do) com m√©dia zero e vari√¢ncia $\sigma^2$. Ent√£o, $Y_t$ √© estritamente estacion√°rio.

*Proof:* Para mostrar que $Y_t$ √© estritamente estacion√°rio, precisamos provar que a distribui√ß√£o conjunta de $(Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n})$ √© a mesma que a distribui√ß√£o conjunta de $(Y_{t_1+h}, Y_{t_2+h}, \ldots, Y_{t_n+h})$ para qualquer $t_1, t_2, \ldots, t_n$ e qualquer $h$.

Como $\epsilon_t$ √© i.i.d., a distribui√ß√£o conjunta de $(\epsilon_{t_1}, \epsilon_{t_2}, \ldots, \epsilon_{t_n})$ √© a mesma que a distribui√ß√£o conjunta de $(\epsilon_{t_1+h}, \epsilon_{t_2+h}, \ldots, \epsilon_{t_n+h})$.  Como $Y_t = \mu + \epsilon_t$, ent√£o $Y_{t_i} = \mu + \epsilon_{t_i}$ e $Y_{t_i+h} = \mu + \epsilon_{t_i+h}$ para todo $i$.  Adicionar uma constante $\mu$ n√£o altera a equival√™ncia das distribui√ß√µes conjuntas, portanto, a distribui√ß√£o conjunta de $(Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n})$ √© a mesma que a distribui√ß√£o conjunta de $(Y_{t_1+h}, Y_{t_2+h}, \ldots, Y_{t_n+h})$.  Portanto, $Y_t$ √© estritamente estacion√°rio. $\blacksquare$

Este teorema refor√ßa a estacionariedade do Caso 2 sob condi√ß√µes mais fortes, especificamente quando o ru√≠do √© i.i.d., garantindo que a s√©rie temporal n√£o apenas tenha m√©dia, vari√¢ncia e autocovari√¢ncia constantes, mas tamb√©m que sua distribui√ß√£o estat√≠stica seja invariante no tempo.

**Teorema 3:** Considere o Caso 1, onde $Y_t = \beta t + \epsilon_t$ e $\epsilon_t$ √© ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$. A vari√¢ncia de $Y_t$ √© constante, mas a s√©rie n√£o √© covariance-stationary.

*Proof:* J√° mostramos que $E(Y_t) = \beta t$, que depende de *$t$*. Agora, vamos calcular a vari√¢ncia de $Y_t$:

$$
Var(Y_t) = Var(\beta t + \epsilon_t) = Var(\epsilon_t) = \sigma^2
$$

A vari√¢ncia √© constante e igual a $\sigma^2$. No entanto, como a m√©dia $E(Y_t) = \beta t$ depende de *$t$*, a s√©rie n√£o √© covariance-stationary, mesmo que sua vari√¢ncia seja constante. $\blacksquare$

> üí° **Exemplo Num√©rico:** Seja $Y_t = 2t + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco com m√©dia 0 e vari√¢ncia 9.
>
> Para $t = 1$, $E(Y_1) = 2(1) = 2$ e $Var(Y_1) = Var(\epsilon_1) = 9$.
> Para $t = 5$, $E(Y_5) = 2(5) = 10$ e $Var(Y_5) = Var(\epsilon_5) = 9$.
>
> Embora a vari√¢ncia permane√ßa constante em 9, a m√©dia aumenta de 2 para 10. Essa mudan√ßa na m√©dia ao longo do tempo demonstra claramente a n√£o-estacionariedade da s√©rie.
>

**Lema 3.1:** Para a s√©rie $Y_t = \beta t + \epsilon_t$, a autocovari√¢ncia $Cov(Y_t, Y_{t-k})$ depende de t, refor√ßando a n√£o-estacionariedade.

*Proof:*
$$
Cov(Y_t, Y_{t-k}) = E[(Y_t - E(Y_t))(Y_{t-k} - E(Y_{t-k}))]
$$
$$
= E[(\beta t + \epsilon_t - \beta t)(\beta (t-k) + \epsilon_{t-k} - \beta (t-k))]
$$
$$
= E[\epsilon_t \epsilon_{t-k}]
$$

Como $\epsilon_t$ √© ru√≠do branco, $E[\epsilon_t \epsilon_{t-k}] = 0$ para $k \neq 0$ e $E[\epsilon_t \epsilon_{t-k}] = \sigma^2$ para $k = 0$. Assim,

$$
Cov(Y_t, Y_{t-k}) = \begin{cases}
\sigma^2, & k = 0 \\
0, & k \neq 0
\end{cases}
$$
Note que embora $Cov(Y_t, Y_{t-k})$ n√£o *explicitamente* dependa de $t$ quando $k \neq 0$, a n√£o-estacionariedade da m√©dia j√° √© suficiente para concluir que a s√©rie n√£o √© covariance-stationary. Al√©m disso, como a m√©dia depende de t, a autocorrela√ß√£o tamb√©m pode ser afetada indiretamente. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha $Y_t = 0.1t + \epsilon_t$, com $\epsilon_t$ sendo ru√≠do branco com vari√¢ncia $\sigma^2 = 1$.
>
> Para $k=0$, $Cov(Y_t, Y_t) = Var(Y_t) = Var(0.1t + \epsilon_t) = Var(\epsilon_t) = 1$.
> Para $k=1$, $Cov(Y_t, Y_{t-1}) = E[(Y_t - E[Y_t])(Y_{t-1} - E[Y_{t-1}])] = E[\epsilon_t \epsilon_{t-1}] = 0$, pois o ru√≠do branco n√£o tem autocorrela√ß√£o.
>
> Embora a autocovari√¢ncia seja zero para $k \neq 0$ e igual √† vari√¢ncia para $k=0$, a m√©dia dependente de $t$ ($E[Y_t] = 0.1t$) √© suficiente para determinar que a s√©rie n√£o √© estacion√°ria.

Em resumo, a forma como $Y_t$ √© modelado, seja com uma tend√™ncia temporal linear ou como a soma de uma constante com ru√≠do, tem um impacto direto na sua m√©dia incondicional e, consequentemente, na sua estacionariedade.

### Conclus√£o

Este cap√≠tulo detalhou a import√¢ncia da **m√©dia incondicional** na caracteriza√ß√£o de s√©ries temporais, especificamente em cen√°rios com tend√™ncia temporal linear e ru√≠do branco Gaussiano. A depend√™ncia temporal da m√©dia incondicional implica a n√£o estacionariedade da s√©rie temporal, enquanto uma m√©dia constante, combinada com outras propriedades apropriadas (vari√¢ncia constante e autocovari√¢ncias independentes do tempo), pode levar √† covariance-stationarity. Essa distin√ß√£o √© crucial para a escolha de modelos e m√©todos de an√°lise apropriados para cada tipo de s√©rie temporal.

### Refer√™ncias
[^1]: Imagine a battery of I such computers generating sequences {y{1}, {y{2},..., {y}x, and consider selecting the observation associated with date t from each sequence: {y{1), y?),...,y}.
[^2]: This would be described as a sample of I realizations of the random variable Y.. This random variable has some density, denoted fr(y,), which is called the un-conditional density of Y..
[^3]: The expectation of the tth observation of a time series refers to the mean of this probability distribution, provided it exists: E(Y) = ‚à´ yif(y) dy
[^4]: We might view this as the probability limit of the ensemble average: E(Y) = plim (1/1) Œ£Œ•.
[^5]: For example, if {Y} represents the sum of a constant u plus a Gaussian white noise process {}.
[^6]: then its mean is Y = Œº + Œµ,, [3.1.5] E(Y) = Œº + Œï(Œµ) = Œº. [3.1.6]
[^7]: If Y, is a time trend plus Gaussian white noise, Y‚ÇÅ = Œ≤Œπ + Œµ,, [3.1.7] then its mean is Œï(Œ•) = Œ≤Œπ. [3.1.8]
[^8]: Sometimes for emphasis the expectation E(Y) is called the unconditional mean of Y,. The unconditional mean is denoted Œº,: E(Y) = ŒºŒπ
[^9]: Note that this notation allows the general possibility that the mean can be a function of the date of the observation t. For the process [3.1.7] involving the time trend, the mean [3.1.8) is a function of time, whereas for the constant plus Gaussian white noise, the mean [3.1.6] is not a function of time.
<!-- END -->