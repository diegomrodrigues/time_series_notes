## O Significado da M√©dia Incondicional em S√©ries Temporais Estacion√°rias

### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise de s√©ries temporais estacion√°rias, com foco espec√≠fico no conceito da **m√©dia incondicional** $\mu_t$ de um processo estoc√°stico $Y_t$ [^44, ^45]. Como vimos anteriormente, a m√©dia e a autocovari√¢ncia s√£o fundamentais para a caracteriza√ß√£o de processos estacion√°rios [^45]. Expandindo o conceito apresentado, exploraremos como a m√©dia incondicional, denotada por $\mu_t$ ou $E(Y_t)$, representa o valor esperado de $Y_t$ e pode variar ou n√£o com o tempo, dependendo da natureza do processo [^44].

### Conceitos Fundamentais

A **m√©dia incondicional** $E(Y_t)$ representa o valor esperado da *t-√©sima* observa√ß√£o de uma s√©rie temporal [^44]. Formalmente, essa expectativa √© definida como a m√©dia da distribui√ß√£o de probabilidade de $Y_t$, desde que essa m√©dia exista [^44]:
$$
E(Y_t) = \int_{-\infty}^{\infty} y_t f_{Y_t}(y_t) \, dy_t
$$
onde $f_{Y_t}(y_t)$ √© a fun√ß√£o de densidade de probabilidade (pdf) incondicional de $Y_t$ [^44].

A Equa√ß√£o [3.1.3] [^44] expressa $E(Y_t)$ como a integral de $y_t$ ponderada pela sua densidade de probabilidade $f_{Y_t}(y_t)$. Isso representa a m√©dia te√≥rica da vari√°vel aleat√≥ria $Y_t$ no instante *t*.

**Interpreta√ß√£o da M√©dia Incondicional:**

A m√©dia incondicional pode ser interpretada como o limite de probabilidade da m√©dia do conjunto (ensemble average) [^44]:
$$
E(Y_t) = \text{plim}_{I \to \infty} \frac{1}{I} \sum_{i=1}^{I} Y_t^{(i)}
$$
Aqui, $Y_t^{(i)}$ representa a *i-√©sima* realiza√ß√£o da vari√°vel aleat√≥ria $Y_t$ no tempo *t*, e *I* √© o n√∫mero total de realiza√ß√µes [^44]. Essa formula√ß√£o [3.1.4] indica que, √† medida que o n√∫mero de realiza√ß√µes *I* tende ao infinito, a m√©dia amostral converge em probabilidade para a m√©dia te√≥rica $E(Y_t)$.

> üí° **Exemplo Num√©rico:** Imagine que voc√™ est√° modelando o pre√ßo di√°rio de uma a√ß√£o. Voc√™ simula esse processo 1000 vezes (I = 1000) e, para cada dia *t*, voc√™ calcula a m√©dia dos pre√ßos simulados nessas 1000 simula√ß√µes. Essa m√©dia, √† medida que *I* fica grande, converge para a m√©dia incondicional do pre√ßo da a√ß√£o no dia *t*. Se o processo for estacion√°rio, essa m√©dia ser√° a mesma para todos os dias *t*.
```python
import numpy as np

# Simula√ß√£o de I realiza√ß√µes de uma s√©rie temporal (exemplo simples)
I = 1000 # N√∫mero de realiza√ß√µes
T = 100   # N√∫mero de per√≠odos de tempo
mu = 10   # M√©dia incondicional (valor verdadeiro)
sigma = 1 # Desvio padr√£o do ru√≠do branco

Y = np.zeros((I, T))
for i in range(I):
  Y[i, :] = mu + np.random.normal(0, sigma, T)

# Calculando a m√©dia do conjunto (ensemble average) para cada ponto no tempo
ensemble_average = np.mean(Y, axis=0)

print(f"Valor verdadeiro da m√©dia incondicional: {mu}")
print(f"M√©dia do conjunto no tempo t=0: {ensemble_average[0]}")
print(f"M√©dia do conjunto no tempo t=49: {ensemble_average[49]}")
print(f"M√©dia do conjunto no tempo t=99: {ensemble_average[99]}")

# Calculando a m√©dia das m√©dias do conjunto
print(f"M√©dia das m√©dias do conjunto: {np.mean(ensemble_average)}")
```
> Nesse exemplo, simulamos uma s√©rie temporal simples onde cada observa√ß√£o √© igual a uma constante *mu* (a m√©dia incondicional) mais ru√≠do gaussiano. O c√≥digo calcula a "m√©dia do conjunto" para diferentes pontos no tempo, demonstrando que essas m√©dias se aproximam do valor verdadeiro de *mu* conforme aumentamos o n√∫mero de realiza√ß√µes.

**Lema 1.** *A m√©dia incondicional √© um operador linear.*

*Demonstra√ß√£o:* Seja $Y_t = aX_t + bZ_t$, onde $a$ e $b$ s√£o constantes e $X_t$ e $Z_t$ s√£o vari√°veis aleat√≥rias. Ent√£o:
$$
E(Y_t) = E(aX_t + bZ_t) = \int_{-\infty}^{\infty} (ax_t + bz_t) f_{X_t, Z_t}(x_t, z_t) \, dx_t \, dz_t
$$
$$
= a \int_{-\infty}^{\infty} x_t f_{X_t}(x_t) \, dx_t + b \int_{-\infty}^{\infty} z_t f_{Z_t}(z_t) \, dz_t = aE(X_t) + bE(Z_t).
$$
Portanto, a m√©dia incondicional √© um operador linear.

*Prova:* Para provar que a m√©dia incondicional √© um operador linear, devemos demonstrar que para quaisquer vari√°veis aleat√≥rias $X_t$ e $Z_t$ e constantes $a$ e $b$, $E[aX_t + bZ_t] = aE[X_t] + bE[Z_t]$.

I.  Come√ßamos com a defini√ß√£o da esperan√ßa de uma combina√ß√£o linear de vari√°veis aleat√≥rias:
    $$E[aX_t + bZ_t] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (ax_t + bz_t) f_{X_t, Z_t}(x_t, z_t) \, dx_t \, dz_t$$
    onde $f_{X_t, Z_t}(x_t, z_t)$ √© a fun√ß√£o de densidade de probabilidade conjunta de $X_t$ e $Z_t$.

II.  Usando a propriedade da integral de uma soma, podemos separar a integral em duas integrais:
    $$\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (ax_t + bz_t) f_{X_t, Z_t}(x_t, z_t) \, dx_t \, dz_t = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} ax_t f_{X_t, Z_t}(x_t, z_t) \, dx_t \, dz_t + \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} bz_t f_{X_t, Z_t}(x_t, z_t) \, dx_t \, dz_t$$

III. Como $a$ e $b$ s√£o constantes, podemos retir√°-las das integrais:
    $$a \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x_t f_{X_t, Z_t}(x_t, z_t) \, dx_t \, dz_t + b \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} z_t f_{X_t, Z_t}(x_t, z_t) \, dx_t \, dz_t$$

IV.  Agora, usamos a propriedade da densidade marginal: $\int_{-\infty}^{\infty} f_{X_t, Z_t}(x_t, z_t) \, dz_t = f_{X_t}(x_t)$ e $\int_{-\infty}^{\infty} f_{X_t, Z_t}(x_t, z_t) \, dx_t = f_{Z_t}(z_t)$.  Portanto, as integrais duplas se reduzem a:
     $$a \int_{-\infty}^{\infty} x_t f_{X_t}(x_t) \, dx_t + b \int_{-\infty}^{\infty} z_t f_{Z_t}(z_t) \, dz_t$$

V.  Reconhecemos que as integrais restantes s√£o as defini√ß√µes de $E[X_t]$ e $E[Z_t]$:
     $$a E[X_t] + b E[Z_t]$$

VI. Portanto, demonstramos que $E[aX_t + bZ_t] = aE[X_t] + bE[Z_t]$, provando que a m√©dia incondicional √© um operador linear. ‚ñ†

> üí° **Exemplo Num√©rico:** Suponha que voc√™ tenha duas s√©ries temporais, $X_t$ representando o crescimento do PIB e $Z_t$ representando as vendas de uma empresa. Voc√™ cria uma nova s√©rie temporal $Y_t = 0.5X_t + 0.2Z_t$. Se $E(X_t) = 2$ (crescimento m√©dio do PIB de 2%) e $E(Z_t) = 100$ (vendas m√©dias de 100 unidades), ent√£o $E(Y_t) = 0.5 * 2 + 0.2 * 100 = 1 + 20 = 21$. Este exemplo demonstra a linearidade da m√©dia incondicional.

**Lema 1.1.** *Se $X_t$ √© uma vari√°vel aleat√≥ria com m√©dia incondicional $E(X_t) = \mu_X$, ent√£o $E(X_t - \mu_X) = 0$.*

*Demonstra√ß√£o:*
Pelo Lema 1, a m√©dia incondicional √© um operador linear. Portanto, $E(X_t - \mu_X) = E(X_t) - E(\mu_X)$. Como $\mu_X$ √© uma constante, $E(\mu_X) = \mu_X$. Assim, $E(X_t - \mu_X) = \mu_X - \mu_X = 0$.

*Prova:* Para mostrar que $E(X_t - \mu_X) = 0$, onde $\mu_X = E(X_t)$.

I. Come√ßamos com a defini√ß√£o da esperan√ßa: $E(X_t - \mu_X) = \int_{-\infty}^{\infty} (x_t - \mu_X) f_{X_t}(x_t) \, dx_t$, onde $f_{X_t}(x_t)$ √© a fun√ß√£o de densidade de probabilidade de $X_t$.

II.  Usando a propriedade da integral, separamos a integral:
    $$\int_{-\infty}^{\infty} (x_t - \mu_X) f_{X_t}(x_t) \, dx_t = \int_{-\infty}^{\infty} x_t f_{X_t}(x_t) \, dx_t - \int_{-\infty}^{\infty} \mu_X f_{X_t}(x_t) \, dx_t$$

III. Como $\mu_X$ √© uma constante, podemos retir√°-la da integral:
    $$\int_{-\infty}^{\infty} x_t f_{X_t}(x_t) \, dx_t - \mu_X \int_{-\infty}^{\infty} f_{X_t}(x_t) \, dx_t$$

IV. Reconhecemos que a primeira integral √© a defini√ß√£o de $E[X_t] = \mu_X$, e a segunda integral √© a integral da fun√ß√£o de densidade de probabilidade sobre todo o seu dom√≠nio, que √© igual a 1:
     $$\mu_X - \mu_X \cdot 1 = \mu_X - \mu_X$$

V. Portanto, $E(X_t - \mu_X) = \mu_X - \mu_X = 0$. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal de temperaturas di√°rias em graus Celsius, onde a m√©dia incondicional √© $\mu_X = 25$. Se subtrairmos 25 de cada observa√ß√£o, a nova s√©rie temporal ter√° uma m√©dia incondicional de 0. Isso significa que, em m√©dia, as temperaturas flutuam em torno de 25 graus Celsius.
```python
import numpy as np

# S√©rie temporal de exemplo
mu_X = 25
temperaturas = np.array([23, 26, 24, 27, 25, 22, 28])

# Calculando a m√©dia da s√©rie original
media_original = np.mean(temperaturas)
print(f"M√©dia da s√©rie original: {media_original}")

# Subtraindo a m√©dia incondicional de cada observa√ß√£o
temperaturas_centralizadas = temperaturas - mu_X

# Calculando a m√©dia da s√©rie centralizada
media_centralizada = np.mean(temperaturas_centralizadas)
print(f"M√©dia da s√©rie centralizada: {media_centralizada}")
```

**Exemplos de M√©dia Incondicional:**

1.  **Soma de uma Constante e Ru√≠do Gaussiano:**
    Se $Y_t$ √© composto por uma constante $\mu$ somada a um processo de ru√≠do branco Gaussiano $\epsilon_t$ [^44]:
    $$
    Y_t = \mu + \epsilon_t
    $$
    Ent√£o, a m√©dia incondicional √© [^44]:
    $$
    E(Y_t) = \mu + E(\epsilon_t) = \mu
    $$
    J√° que a expectativa do ru√≠do branco Gaussiano √© zero ($E(\epsilon_t) = 0$) [^44]. A equa√ß√£o [3.1.6] demonstra que a m√©dia incondicional √© simplesmente a constante $\mu$.

    *Prova:* Para provar que $E(Y_t) = \mu$, dado que $Y_t = \mu + \epsilon_t$ e $E(\epsilon_t) = 0$.

    I.  Tomamos a esperan√ßa de ambos os lados da equa√ß√£o:
        $$E(Y_t) = E(\mu + \epsilon_t)$$

    II.  Usando a linearidade do operador de esperan√ßa (Lema 1), separamos a esperan√ßa da soma:
         $$E(Y_t) = E(\mu) + E(\epsilon_t)$$

    III. Como $\mu$ √© uma constante, $E(\mu) = \mu$, e dado que $E(\epsilon_t) = 0$:
         $$E(Y_t) = \mu + 0$$

    IV. Portanto, $E(Y_t) = \mu$. ‚ñ†

> üí° **Exemplo Num√©rico:** Se $\mu = 5$ e $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia 1, ent√£o $Y_t = 5 + \epsilon_t$. A m√©dia incondicional de $Y_t$ √© simplesmente 5.  Qualquer flutua√ß√£o em $Y_t$ ao redor de 5 √© devido ao ru√≠do branco $\epsilon_t$.
```python
import numpy as np

# Par√¢metros
mu = 5
T = 100 # N√∫mero de per√≠odos de tempo

# Gerando ru√≠do branco gaussiano
epsilon = np.random.normal(0, 1, T)

# Gerando a s√©rie temporal Y_t
Y = mu + epsilon

# Calculando a m√©dia amostral
media_amostral = np.mean(Y)

print(f"M√©dia incondicional (par√¢metro mu): {mu}")
print(f"M√©dia amostral da s√©rie Y_t: {media_amostral}")
```

2.  **Tend√™ncia de Tempo Mais Ru√≠do Gaussiano:**
    Se $Y_t$ consiste em uma tend√™ncia de tempo linear $\beta t$ mais ru√≠do branco Gaussiano $\epsilon_t$ [^44]:
    $$
    Y_t = \beta t + \epsilon_t
    $$
    Ent√£o, a m√©dia incondicional √© [^44]:
    $$
    E(Y_t) = \beta t
    $$
    Neste caso, a m√©dia incondicional √© uma fun√ß√£o do tempo, variando linearmente com *t* [^44]. A equa√ß√£o [3.1.8] mostra que a m√©dia incondicional segue a tend√™ncia de tempo $\beta t$.

    *Prova:* Para provar que $E(Y_t) = \beta t$, dado que $Y_t = \beta t + \epsilon_t$ e $E(\epsilon_t) = 0$.

    I. Tomamos a esperan√ßa de ambos os lados da equa√ß√£o:
       $$E(Y_t) = E(\beta t + \epsilon_t)$$

    II. Usando a linearidade do operador de esperan√ßa, separamos a esperan√ßa da soma:
        $$E(Y_t) = E(\beta t) + E(\epsilon_t)$$

    III. Como $\beta$ e $t$ s√£o constantes em rela√ß√£o ao operador de esperan√ßa, $E(\beta t) = \beta t$, e dado que $E(\epsilon_t) = 0$:
         $$E(Y_t) = \beta t + 0$$

    IV. Portanto, $E(Y_t) = \beta t$. ‚ñ†

> üí° **Exemplo Num√©rico:** Se $\beta = 0.5$, ent√£o a m√©dia incondicional no tempo $t = 10$ √© $E(Y_{10}) = 0.5 * 10 = 5$. Isso significa que, em m√©dia, o valor de $Y_t$ aumenta em 0.5 unidades a cada per√≠odo de tempo.
```python
import numpy as np
import matplotlib.pyplot as plt

# Par√¢metros
beta = 0.5
T = 100 # N√∫mero de per√≠odos de tempo

# Gerando ru√≠do branco gaussiano
epsilon = np.random.normal(0, 1, T)

# Gerando a s√©rie temporal Y_t
t = np.arange(T)
Y = beta * t + epsilon

# Calculando a m√©dia incondicional em cada ponto no tempo
media_incondicional = beta * t

# Plotando a s√©rie temporal e a m√©dia incondicional
plt.plot(t, Y, label="Y_t (Tend√™ncia + Ru√≠do)")
plt.plot(t, media_incondicional, label="M√©dia Incondicional (beta * t)", color='red')
plt.xlabel("Tempo (t)")
plt.ylabel("Valor")
plt.title("S√©rie Temporal com Tend√™ncia Linear")
plt.legend()
plt.grid(True)
plt.show()
```
> Este c√≥digo gera uma s√©rie temporal com uma tend√™ncia linear e plota a s√©rie junto com a m√©dia incondicional, que √© simplesmente a linha de tend√™ncia.

Para ilustrar ainda mais o conceito, considere o seguinte exemplo:

3.  **Processo AR(1) com Constante:**
    Se $Y_t$ segue um processo Autorregressivo de ordem 1 (AR(1)) com uma constante $c$:
    $$
    Y_t = c + \phi Y_{t-1} + \epsilon_t
    $$
    onde $|\phi| < 1$ para garantir a estacionariedade, e $\epsilon_t$ √© ru√≠do branco com m√©dia zero.  Para encontrar a m√©dia incondicional, assumimos que $E(Y_t) = E(Y_{t-1}) = \mu$.  Aplicando o operador de esperan√ßa:
    $$
    E(Y_t) = E(c + \phi Y_{t-1} + \epsilon_t) = c + \phi E(Y_{t-1}) + E(\epsilon_t)
    $$
    $$
    \mu = c + \phi \mu + 0
    $$
    Resolvendo para $\mu$:
    $$
    \mu = \frac{c}{1 - \phi}
    $$
    Este exemplo demonstra como a m√©dia incondicional pode ser calculada para processos mais complexos, dependendo dos par√¢metros do modelo.

    *Prova:* Para derivar a m√©dia incondicional $\mu$ do processo AR(1) $Y_t = c + \phi Y_{t-1} + \epsilon_t$, onde $E(\epsilon_t) = 0$ e $|\phi| < 1$.

    I.  Assumindo estacionariedade, a m√©dia incondicional √© constante ao longo do tempo, ou seja, $E(Y_t) = E(Y_{t-1}) = \mu$.

    II. Tomamos a esperan√ßa de ambos os lados da equa√ß√£o do processo AR(1):
         $$E(Y_t) = E(c + \phi Y_{t-1} + \epsilon_t)$$

    III. Usando a linearidade do operador de esperan√ßa, separamos a esperan√ßa da soma:
          $$E(Y_t) = E(c) + E(\phi Y_{t-1}) + E(\epsilon_t)$$

    IV. Como $c$ e $\phi$ s√£o constantes, $E(c) = c$ e $E(\phi Y_{t-1}) = \phi E(Y_{t-1}) = \phi \mu$.  Tamb√©m, $E(\epsilon_t) = 0$ (dado que $\epsilon_t$ √© ru√≠do branco com m√©dia zero):
         $$\mu = c + \phi \mu + 0$$

    V.  Agora, resolvemos para $\mu$:
         $$\mu - \phi \mu = c$$
         $$\mu (1 - \phi) = c$$
         $$\mu = \frac{c}{1 - \phi}$$

    VI. Portanto, a m√©dia incondicional do processo AR(1) √© $\mu = \frac{c}{1 - \phi}$. ‚ñ†

> üí° **Exemplo Num√©rico:**  Se $c = 2$ e $\phi = 0.5$, ent√£o $\mu = \frac{2}{1 - 0.5} = 4$. Isso significa que a s√©rie temporal flutua em torno de 4, e o valor atual √© influenciado pelo valor anterior com um peso de 0.5.
```python
import numpy as np

# Par√¢metros do AR(1)
c = 2
phi = 0.5
T = 100

# Gerando ru√≠do branco gaussiano
epsilon = np.random.normal(0, 1, T)

# Inicializando a s√©rie temporal
Y = np.zeros(T)
Y[0] = c + epsilon[0]  # Primeiro valor

# Gerando a s√©rie temporal AR(1)
for t in range(1, T):
  Y[t] = c + phi * Y[t-1] + epsilon[t]

# Calculando a m√©dia amostral
media_amostral = np.mean(Y)

# Calculando a m√©dia incondicional te√≥rica
media_incondicional = c / (1 - phi)

print(f"M√©dia incondicional te√≥rica: {media_incondicional}")
print(f"M√©dia amostral da s√©rie Y_t: {media_amostral}")
```

**Proposi√ß√£o 2.** *Para um processo AR(p) estacion√°rio dado por $Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t$, onde $\epsilon_t$ √© ru√≠do branco com m√©dia zero e $|\phi_1 + \phi_2 + \ldots + \phi_p| < 1$, a m√©dia incondicional √© dada por $\mu = \frac{c}{1 - \phi_1 - \phi_2 - \ldots - \phi_p}$.*

*Demonstra√ß√£o:*
Assumindo estacionariedade, $E(Y_t) = E(Y_{t-1}) = \ldots = E(Y_{t-p}) = \mu$. Tomando a esperan√ßa de ambos os lados da equa√ß√£o do processo AR(p):
$E(Y_t) = E(c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t)$
$\mu = c + \phi_1 \mu + \phi_2 \mu + \ldots + \phi_p \mu + E(\epsilon_t)$
Como $E(\epsilon_t) = 0$:
$\mu = c + \mu (\phi_1 + \phi_2 + \ldots + \phi_p)$
$\mu (1 - \phi_1 - \phi_2 - \ldots - \phi_p) = c$
$\mu = \frac{c}{1 - \phi_1 - \phi_2 - \ldots - \phi_p}$

*Prova:* Para derivar a m√©dia incondicional $\mu$ do processo AR(p) $Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t$, onde $E(\epsilon_t) = 0$ e o processo √© estacion√°rio.

I. Assumindo estacionariedade, a m√©dia incondicional √© constante ao longo do tempo, ou seja, $E(Y_t) = E(Y_{t-1}) = \ldots = E(Y_{t-p}) = \mu$.

II. Tomamos a esperan√ßa de ambos os lados da equa√ß√£o do processo AR(p):
    $$E(Y_t) = E(c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t)$$

III. Usando a linearidade do operador de esperan√ßa, separamos a esperan√ßa da soma:
     $$E(Y_t) = E(c) + \phi_1 E(Y_{t-1}) + \phi_2 E(Y_{t-2}) + \ldots + \phi_p E(Y_{t-p}) + E(\epsilon_t)$$

IV. Como $c$, $\phi_1$, $\phi_2$, ..., $\phi_p$ s√£o constantes, $E(c) = c$ e $E(\phi_i Y_{t-i}) = \phi_i E(Y_{t-i}) = \phi_i \mu$ para $i = 1, 2, \ldots, p$. Tamb√©m, $E(\epsilon_t) = 0$ (dado que $\epsilon_t$ √© ru√≠do branco com m√©dia zero):
    $$\mu = c + \phi_1 \mu + \phi_2 \mu + \ldots + \phi_p \mu + 0$$

V. Agora, resolvemos para $\mu$:
   $$\mu = c + \mu (\phi_1 + \phi_2 + \ldots + \phi_p)$$
   $$\mu - \mu (\phi_1 + \phi_2 + \ldots + \phi_p) = c$$
   $$\mu (1 - \phi_1 - \phi_2 - \ldots - \phi_p) = c$$
   $$\mu = \frac{c}{1 - \phi_1 - \phi_2 - \ldots - \phi_p}$$

VI. Portanto, a m√©dia incondicional do processo AR(p) √© $\mu = \frac{c}{1 - \phi_1 - \phi_2 - \ldots - \phi_p}$. ‚ñ†

> üí° **Exemplo Num√©rico:** Para um processo AR(2) com $c = 5$, $\phi_1 = 0.3$ e $\phi_2 = 0.2$, a m√©dia incondicional √© $\mu = \frac{5}{1 - 0.3 - 0.2} = \frac{5}{0.5} = 10$.

**Estacionariedade e M√©dia Incondicional:**
A estacionariedade de um processo estoc√°stico est√° diretamente relacionada ao comportamento de sua m√©dia incondicional e autocovari√¢ncias [^45]. Em particular, um processo √© considerado *covariance-stationary* (ou *weakly stationary*) se [^45]:
*   A m√©dia incondicional $E(Y_t) = \mu$ √© constante e n√£o depende do tempo *t* [^45].
*   A autocovari√¢ncia $\gamma_{jt} = E[(Y_t - \mu)(Y_{t-j} - \mu)]$ depende apenas da diferen√ßa de tempo *j* e n√£o de *t* [^45].

A independ√™ncia do tempo tanto da m√©dia quanto das autocovari√¢ncias √© crucial para a estacionariedade, facilitando a an√°lise e a modelagem do processo [^45].

Al√©m da estacionariedade fraca, √© importante mencionar a estacionariedade forte:

*Um processo estoc√°stico √© dito fortemente estacion√°rio se a distribui√ß√£o conjunta de $(Y_{t_1}, Y_{t_2}, \ldots, Y_{t_n})$ √© a mesma que a distribui√ß√£o conjunta de $(Y_{t_1+k}, Y_{t_2+k}, \ldots, Y_{t_n+k})$ para qualquer inteiro $k$ e qualquer conjunto de instantes de tempo $t_1, t_2, \ldots, t_n$.*

A estacionariedade forte implica que todas as estat√≠sticas do processo s√£o invariantes ao tempo, n√£o apenas a m√©dia e a autocovari√¢ncia.

**Observa√ß√£o:** √â importante notar que um processo fracamente estacion√°rio n√£o √© necessariamente fortemente estacion√°rio, e vice-versa. No entanto, se um processo √© Gaussiano e fracamente estacion√°rio, ent√£o ele tamb√©m √© fortemente estacion√°rio, pois a distribui√ß√£o normal √© completamente caracterizada por sua m√©dia e vari√¢ncia (e, portanto, autocovari√¢ncia).

### Conclus√£o

A m√©dia incondicional $\mu_t$ √© um conceito central na an√°lise de s√©ries temporais, representando o valor esperado de $Y_t$ e influenciando diretamente a estacionariedade do processo [^44, ^45]. Compreender como $\mu_t$ se comporta (se √© constante ou varia com o tempo) √© essencial para selecionar modelos apropriados e interpretar os resultados da an√°lise [^44]. Nos pr√≥ximos cap√≠tulos, exploraremos como a m√©dia incondicional interage com outros componentes de modelos ARMA e como ela afeta as propriedades estat√≠sticas desses modelos [^45].

### Refer√™ncias
[^44]: Page 44 of context
[^45]: Page 45 of context
<!-- END -->