## O Papel da Inova√ß√£o Fundamental $\epsilon_t$ na Estima√ß√£o e Previs√£o em Processos MA(1)

### Introdu√ß√£o
Este cap√≠tulo expande a discuss√£o sobre a invertibilidade de processos *Moving Average* (MA), concentrando-se no conceito de **inova√ß√£o fundamental** $\epsilon_t$. Em ess√™ncia, a inova√ß√£o fundamental √© o processo de ru√≠do branco que *impulsiona* a s√©rie temporal em um processo MA invert√≠vel [^65] [^Previous Topics]. A identifica√ß√£o e o uso da inova√ß√£o fundamental s√£o cruciais para a constru√ß√£o de algoritmos de estima√ß√£o de par√¢metros e gera√ß√£o de previs√µes precisas. Embora um processo MA(1) invert√≠vel e seu correspondente n√£o invert√≠vel compartilhem os mesmos momentos de primeira e segunda ordem (e, portanto, a mesma fun√ß√£o geradora de autocovari√¢ncia), suas inova√ß√µes fundamentais diferem substancialmente, levando a implica√ß√µes distintas para an√°lise, estima√ß√£o e previs√£o.

### Inova√ß√£o Fundamental e Representa√ß√£o AR(‚àû)
Como discutido anteriormente, um processo MA(1) invert√≠vel pode ser expresso como uma representa√ß√£o autorregressiva de ordem infinita (AR(‚àû)) [^65] [^Previous Topics]:

$$Y_t = \mu + \sum_{j=1}^{\infty} \pi_j (Y_{t-j} - \mu) + \epsilon_t$$

Nessa representa√ß√£o, $\epsilon_t$ √© a **inova√ß√£o fundamental**. Essa inova√ß√£o representa a parte do processo $Y_t$ que n√£o pode ser explicada por seus valores passados. Em outras palavras, $\epsilon_t$ √© o *choque* ou *inova√ß√£o* que *impulsiona* o processo $Y_t$ e que n√£o est√° correlacionado com o passado da s√©rie temporal.

Em contraste, para um processo MA(1) n√£o invert√≠vel, a inova√ß√£o fundamental n√£o est√° diretamente relacionada ao processo passado. Em vez disso, ela est√° relacionada a valores futuros, o que torna a interpreta√ß√£o e o uso da inova√ß√£o fundamental no contexto de um processo n√£o invert√≠vel muito mais desafiadores.

> üí° **Exemplo Num√©rico:**
>
> Considere um processo MA(1) invert√≠vel com $\mu = 10$ e $\theta = 0.7$. Isso significa que cada valor da s√©rie temporal √© influenciado pelo ru√≠do branco atual ($\epsilon_t$) e 70% do ru√≠do branco anterior ($\epsilon_{t-1}$). Suponha que observemos $Y_t = 12$.  Podemos expressar a inova√ß√£o fundamental como $\epsilon_t = Y_t - \mu - \sum_{j=1}^{\infty} \pi_j (Y_{t-j} - \mu)$, onde $\pi_j = -(-\theta)^j$. Para calcular $\epsilon_t$, precisamos dos valores passados de $Y$. Suponha $Y_{t-1} = 11$, $Y_{t-2} = 9$, $Y_{t-3} = 10.5$.
>
> $\text{Step 1: Calculate } \pi_1 = -(-\theta)^1 = 0.7$
> $\text{Step 2: Calculate } \pi_2 = -(-\theta)^2 = -0.49$
> $\text{Step 3: Calculate } \pi_3 = -(-\theta)^3 = 0.343$
>
>  Ent√£o, $\epsilon_t \approx (Y_t - \mu) - \pi_1(Y_{t-1} - \mu) - \pi_2(Y_{t-2} - \mu) - \pi_3(Y_{t-3} - \mu) = (12-10) - 0.7(11-10) - (-0.49)(9-10) - 0.343(10.5-10) = 2 - 0.7 + 0.49 + (-0.1715) = 1.6185$.
>
> Isso nos d√° uma estimativa da inova√ß√£o fundamental $\epsilon_t$ com base nos valores passados de $Y$.  Este c√°lculo aproxima a representa√ß√£o AR(‚àû) usando apenas os tr√™s primeiros termos. Quanto mais termos incluirmos, mais precisa ser√° a estimativa de $\epsilon_t$. Para um processo n√£o invert√≠vel, precisar√≠amos de valores futuros de $Y$, o que √© impratic√°vel em muitos cen√°rios de previs√£o.

**Proposi√ß√£o 6**. A inova√ß√£o fundamental $\epsilon_t$ em um processo MA(1) invert√≠vel √© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia constante, e √© n√£o correlacionado com os valores passados do processo $Y_t$.

*Prova*. Por constru√ß√£o, a representa√ß√£o AR(‚àû) de um processo MA(1) invert√≠vel garante que a inova√ß√£o fundamental tenha as propriedades de ru√≠do branco e seja n√£o correlacionada com o passado do processo.

I. Dado o processo MA(1) invert√≠vel $Y_t = \mu + \epsilon_t + \theta \epsilon_{t-1}$, com $|\theta| < 1$.
II. A representa√ß√£o AR(‚àû) √© $Y_t - \mu = \sum_{j=1}^{\infty} \pi_j (Y_{t-j} - \mu) + \epsilon_t$, onde $\pi_j = -(-\theta)^j$.
III. Por defini√ß√£o, $\epsilon_t$ √© o res√≠duo da regress√£o de $Y_t$ sobre seus *lags* passados, o que implica que $E[\epsilon_t] = 0$ e $Cov(\epsilon_t, Y_{t-j}) = 0$ para $j > 0$.
IV. Al√©m disso, como $\epsilon_t$ √© um ru√≠do branco, $Var(\epsilon_t) = \sigma^2$ √© constante e $Cov(\epsilon_t, \epsilon_{t-k}) = 0$ para $k \neq 0$.
V. Portanto, $\epsilon_t$ √© um processo de ru√≠do branco com m√©dia zero, vari√¢ncia constante e √© n√£o correlacionado com os valores passados de $Y_t$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Simulando um processo MA(1) invert√≠vel e calculando a correla√ß√£o entre a inova√ß√£o fundamental e *lags* passados de $Y$, verificamos que essa correla√ß√£o √© essencialmente zero.
>
> ```python
> import numpy as np
> from scipy.stats import skew
>
> # Parameters
> theta = 0.5
> mu = 0
> n = 1000
>
> # Generate random errors from a normal distribution
> errors = np.random.normal(0, 1, n)
>
> # Simulate the invertible MA(1) process
> y = [errors[0]]
> for i in range(1, n):
>     y.append(mu + errors[i] + theta * errors[i - 1])
> y = np.array(y)
>
> # Estimate the fundamental innovation
> epsilon = [y[0] - mu]
> for i in range(1, n):
>     epsilon.append(y[i] - mu - theta * errors[i - 1]) #Corre√ß√£o: utilizar os erros originais
> epsilon = np.array(epsilon)
>
> # Calculate the correlation between epsilon and lagged values of y
> lags = 5  # Number of lags to test
> correlations = []
> for lag in range(1, lags + 1):
>     correlation = np.corrcoef(epsilon[lag:], y[:-lag])[0, 1]
>     correlations.append(correlation)
>
> print("Correlations between epsilon and lagged values of y:")
> print(correlations)
>
> # Check skewness of fundamental innovation
> skewness_epsilon = skew(epsilon)
> print(f"Skewness of epsilon: {skewness_epsilon}")
> ```
>
> Ao executar este c√≥digo, voc√™ notar√° que as correla√ß√µes entre $\epsilon$ e os valores defasados de $Y$ s√£o muito pr√≥ximas de zero, consistentes com a proposi√ß√£o. Al√©m disso, a assimetria (skewness) de $\epsilon$ √© pr√≥xima de zero. Isso confirma que $\epsilon$ √©, de fato, uma inova√ß√£o de ru√≠do branco, n√£o correlacionada com o passado de Y.

**Proposi√ß√£o 6.1**. Se $\epsilon_t$ segue uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\sigma^2$, ent√£o $\epsilon_t$ √© independente dos valores passados do processo $Y_t$.

*Prova*. A independ√™ncia segue da n√£o correla√ß√£o e da normalidade conjunta.

I. Da Proposi√ß√£o 6, sabemos que $E[\epsilon_t] = 0$, $Var(\epsilon_t) = \sigma^2$, e $Cov(\epsilon_t, Y_{t-j}) = 0$ para $j > 0$.

II. Assumindo que $\epsilon_t$ e $Y_{t-j}$ s√£o conjuntamente normais (o que √© razo√°vel se o processo de ru√≠do branco subjacente for normal), a n√£o correla√ß√£o implica independ√™ncia.

III. Portanto, se $\epsilon_t$ segue uma distribui√ß√£o normal, ent√£o $\epsilon_t$ √© independente dos valores passados do processo $Y_t$. ‚ñ†

**Proposi√ß√£o 6.2**. Seja $W_t$ um processo de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma_w^2$. Se $Y_t$ √© um processo MA(1) invert√≠vel gerado por $Y_t = \mu + W_t + \theta W_{t-1}$, ent√£o a inova√ß√£o fundamental $\epsilon_t$ √© um m√∫ltiplo escalar de $W_t$, especificamente $\epsilon_t = W_t$.

*Prova*. A prova segue diretamente da defini√ß√£o da representa√ß√£o AR(‚àû) e da invertibilidade do processo MA(1).

I. Dado o processo MA(1) invert√≠vel $Y_t = \mu + W_t + \theta W_{t-1}$, onde $W_t$ √© ru√≠do branco.
II. A representa√ß√£o AR(‚àû) √© $Y_t - \mu = \sum_{j=1}^{\infty} \pi_j (Y_{t-j} - \mu) + \epsilon_t$.
III. Substituindo $Y_t$ na representa√ß√£o AR(‚àû), podemos mostrar que $\epsilon_t = W_t$. Portanto, a inova√ß√£o fundamental √© o pr√≥prio ru√≠do branco que gera o processo. ‚ñ†

### Estima√ß√£o de Par√¢metros usando a Inova√ß√£o Fundamental
A inova√ß√£o fundamental $\epsilon_t$ desempenha um papel crucial nos algoritmos de estima√ß√£o de par√¢metros. Um m√©todo comum para estimar os par√¢metros de um modelo MA(1) invert√≠vel √© o m√©todo da **m√°xima verossimilhan√ßa**. Este m√©todo envolve encontrar os valores dos par√¢metros ( $\mu$, $\theta$, e $\sigma^2$ ) que maximizam a fun√ß√£o de verossimilhan√ßa dos dados.

Para um processo MA(1) invert√≠vel, podemos expressar a fun√ß√£o de verossimilhan√ßa em termos da inova√ß√£o fundamental. Assumindo que $\epsilon_t$ siga uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\sigma^2$, a fun√ß√£o de verossimilhan√ßa √© dada por:
$$L(\mu, \theta, \sigma^2) = \prod_{t=1}^{T} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\epsilon_t^2}{2\sigma^2}\right)$$
onde $T$ √© o n√∫mero de observa√ß√µes. O objetivo √© encontrar os valores de $\mu$, $\theta$ e $\sigma^2$ que maximizam essa fun√ß√£o de verossimilhan√ßa. Em termos pr√°ticos, √© mais conveniente maximizar a fun√ß√£o de log-verossimilhan√ßa:
$$\ell(\mu, \theta, \sigma^2) = -\frac{T}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=1}^{T} \epsilon_t^2$$
onde $\epsilon_t$ √© derivado recursivamente dos dados usando a representa√ß√£o AR(‚àû) e os valores atuais dos par√¢metros $\mu$ e $\theta$.

> üí° **Exemplo Num√©rico:**
>
> Para maximizar a fun√ß√£o de log-verossimilhan√ßa, podemos usar t√©cnicas de otimiza√ß√£o num√©rica. Dado um conjunto de dados $Y_t$, podemos iterar sobre diferentes valores de $\mu$ e $\theta$, calcular as inova√ß√µes fundamentais correspondentes e avaliar a fun√ß√£o de log-verossimilhan√ßa. O conjunto de par√¢metros que maximiza a fun√ß√£o de log-verossimilhan√ßa √© ent√£o escolhido como a estimativa de m√°xima verossimilhan√ßa.
>
> ```python
> import numpy as np
> from scipy.optimize import minimize
> from scipy.stats import norm
>
> # Simulate MA(1) data
> def simulate_ma1(theta, sigma2, n):
>     errors = np.random.normal(0, np.sqrt(sigma2), n)
>     y = [errors[0]]
>     for i in range(1, n):
>         y.append(errors[i] + theta * errors[i-1])
>     return np.array(y)
>
> # Log-likelihood function
> def log_likelihood(params, data):
>     mu, theta, sigma2 = params
>     T = len(data)
>
>     # Calculate fundamental innovations
>     epsilon = [data[0] - mu]
>     for i in range(1, T):
>         epsilon.append(data[i] - mu - theta * epsilon[i - 1])
>     epsilon = np.array(epsilon)
>
>     # Log likelihood
>     log_likelihood = -T/2 * np.log(2*np.pi*sigma2) - 1/(2*sigma2) * np.sum(epsilon**2)
>     return -log_likelihood  # SciPy minimize function needs to minimize
>
> # Parameters
> theta_true = 0.7
> sigma2_true = 1.0
> mu_true = 0
> n = 1000
>
> # Simulate data
> data = simulate_ma1(theta_true, sigma2_true, n)
>
> # Initial guesses for parameters
> initial_guess = [0, 0.5, 0.5]
>
> # Minimize the negative log-likelihood function
> results = minimize(log_likelihood, initial_guess, args=(data,), method='L-BFGS-B', bounds=[(None, None), (-0.999, 0.999), (0.001, None)])
>
> # Estimated parameters
> mu_estimated, theta_estimated, sigma2_estimated = results.x
>
> print(f"True mu: {mu_true}, Estimated mu: {mu_estimated}")
> print(f"True theta: {theta_true}, Estimated theta: {theta_estimated}")
> print(f"True sigma2: {sigma2_true}, Estimated sigma2: {sigma2_estimated}")
> ```
>
> Este exemplo mostra como o uso da inova√ß√£o fundamental na fun√ß√£o de log-verossimilhan√ßa nos permite estimar os par√¢metros do modelo MA(1). Observe como os par√¢metros estimados se aproximam dos valores verdadeiros √† medida que o tamanho da amostra aumenta. A fun√ß√£o `minimize` da biblioteca `SciPy` √© usada para encontrar os valores √≥timos de $\mu$, $\theta$ e $\sigma^2$ iterativamente, usando a representa√ß√£o AR(‚àû) para derivar os valores de $\epsilon_t$ para cada conjunto de par√¢metros.
>
> Para uma interpreta√ß√£o pr√°tica, imagine que estamos modelando o n√∫mero di√°rio de vendas de um produto. A m√©dia ($\mu$) representa o n√≠vel base de vendas, $\theta$ captura a persist√™ncia dos choques (erros) passados nas vendas atuais e $\sigma^2$ quantifica a variabilidade desses choques. Ao maximizar a fun√ß√£o de log-verossimilhan√ßa, estamos encontrando os valores de $\mu$, $\theta$ e $\sigma^2$ que melhor se ajustam aos dados hist√≥ricos de vendas, permitindo-nos entender e prever melhor as vendas futuras.

**Lema 5**. O M√©todo da M√°xima Verossimilhan√ßa aplicado √† estima√ß√£o dos par√¢metros de um processo MA(1) invert√≠vel converge para os verdadeiros valores dos par√¢metros √† medida que o tamanho da amostra aumenta, sob condi√ß√µes de regularidade padr√£o.

*Prova*. As condi√ß√µes de regularidade padr√£o garantem que a fun√ß√£o de log-verossimilhan√ßa seja bem comportada e que o estimador de m√°xima verossimilhan√ßa seja consistente e assintoticamente normal.

I. A fun√ß√£o de log-verossimilhan√ßa $\ell(\mu, \theta, \sigma^2)$ √© suave e diferenci√°vel com rela√ß√£o aos par√¢metros $\mu, \theta, \sigma^2$.
II. O estimador de m√°xima verossimilhan√ßa √© consistente, o que significa que converge em probabilidade para os verdadeiros valores dos par√¢metros √† medida que o tamanho da amostra tende ao infinito.
III. O estimador de m√°xima verossimilhan√ßa √© assintoticamente normal, o que significa que a distribui√ß√£o do estimador se aproxima de uma distribui√ß√£o normal √† medida que o tamanho da amostra aumenta.
IV. Portanto, sob condi√ß√µes de regularidade padr√£o, o m√©todo da m√°xima verossimilhan√ßa converge para os verdadeiros valores dos par√¢metros √† medida que o tamanho da amostra aumenta. ‚ñ†

**Lema 5.1**. A consist√™ncia do estimador de M√°xima Verossimilhan√ßa requer a invertibilidade do processo MA(1), para garantir a exist√™ncia de uma representa√ß√£o AR(‚àû) √∫nica e est√°vel.

*Prova*. Em um processo MA(1) n√£o invert√≠vel, n√£o existe uma representa√ß√£o AR(‚àû) √∫nica e est√°vel, invalidando a abordagem de estima√ß√£o baseada na decomposi√ß√£o de Wold.

I. Para um processo MA(1) n√£o invert√≠vel com $|\theta| > 1$, a fun√ß√£o de log-verossimilhan√ßa n√£o √© unicamente identificada devido √† n√£o converg√™ncia da representa√ß√£o AR(‚àû).
II. Isso implica que diferentes valores de $\theta$ e $\epsilon_t$ podem levar √† mesma fun√ß√£o de verossimilhan√ßa.
III. Consequentemente, o estimador de M√°xima Verossimilhan√ßa n√£o converge para um valor √∫nico, violando a consist√™ncia. ‚ñ†

**Teorema 4**. A vari√¢ncia do estimador de m√°xima verossimilhan√ßa para o par√¢metro $\theta$ em um processo MA(1) invert√≠vel, sob condi√ß√µes de regularidade, √© assintoticamente igual a $(1-\theta^2)/T$, onde $T$ √© o tamanho da amostra.

*Prova*. A prova envolve o c√°lculo da informa√ß√£o de Fisher e a invers√£o da matriz de informa√ß√£o.

I. A fun√ß√£o de log-verossimilhan√ßa para um processo MA(1) invert√≠vel √© dada por $\ell(\mu, \theta, \sigma^2) = -\frac{T}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=1}^{T} \epsilon_t^2$.
II. A informa√ß√£o de Fisher para $\theta$ √© $I(\theta) = -E\left[\frac{\partial^2 \ell}{\partial \theta^2}\right]$.
III. Calculando a segunda derivada parcial e tomando o valor esperado, obtemos $I(\theta) = \frac{T}{1-\theta^2}$.
IV. A vari√¢ncia assint√≥tica do estimador de m√°xima verossimilhan√ßa √© dada pelo inverso da informa√ß√£o de Fisher: $Var(\hat{\theta}) \approx \frac{1}{I(\theta)} = \frac{1-\theta^2}{T}$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Suponha que estimemos $\theta$ como $\hat{\theta} = 0.6$ usando uma amostra de tamanho $T = 200$.  A vari√¢ncia assint√≥tica de $\hat{\theta}$ √© ent√£o estimada como $Var(\hat{\theta}) \approx (1 - 0.6^2) / 200 = 0.0032$.  O desvio padr√£o de $\hat{\theta}$ √© $\sqrt{0.0032} \approx 0.0566$.  Isso nos permite construir um intervalo de confian√ßa para $\theta$.  Por exemplo, um intervalo de confian√ßa de 95% √© aproximadamente $\hat{\theta} \pm 1.96 \times 0.0566 = 0.6 \pm 0.111$, ou seja, $[0.489, 0.711]$. Isso significa que estamos 95% confiantes de que o verdadeiro valor de $\theta$ est√° dentro deste intervalo, dada a nossa amostra e as condi√ß√µes do modelo.

**Teorema 4.1**. Sob condi√ß√µes de regularidade e para amostras suficientemente grandes, o estimador de m√°xima verossimilhan√ßa $\hat{\theta}$ para o par√¢metro $\theta$ em um processo MA(1) invert√≠vel √© assintoticamente normal, com m√©dia $\theta$ e vari√¢ncia $(1-\theta^2)/T$. Ou seja, $\sqrt{T}(\hat{\theta} - \theta) \xrightarrow{d} N(0, 1-\theta^2)$.

*Prova*. A prova segue da teoria assint√≥tica dos estimadores de m√°xima verossimilhan√ßa.

I. Sob condi√ß√µes de regularidade, o estimador de m√°xima verossimilhan√ßa √© consistente e assintoticamente normal.
II. Do Teorema 4, sabemos que a vari√¢ncia assint√≥tica de $\hat{\theta}$ √© $(1-\theta^2)/T$.
III. Portanto, $\sqrt{T}(\hat{\theta} - \theta)$ converge em distribui√ß√£o para uma normal com m√©dia zero e vari√¢ncia $1-\theta^2$. ‚ñ†

### O Papel da Inova√ß√£o Fundamental na Previs√£o
Como mencionado anteriormente, a inova√ß√£o fundamental $\epsilon_t$ desempenha um papel crucial na previs√£o de processos MA(1) invert√≠veis. Dado que podemos expressar $Y_t$ em termos de seus valores passados e da inova√ß√£o fundamental, podemos usar essa representa√ß√£o para fazer previs√µes.

A melhor previs√£o linear de $Y_{t+1}$ com base nas informa√ß√µes dispon√≠veis at√© o tempo $t$ √© dada por:

$$E[Y_{t+1} | Y_t, Y_{t-1}, \ldots] = \mu + \sum_{j=1}^{\infty} \pi_j (Y_{t-j} - \mu)$$

Como os coeficientes $\pi_j$ decaem rapidamente, podemos aproximar essa previs√£o usando um n√∫mero finito de termos. Essa abordagem de previs√£o explora diretamente a representa√ß√£o AR(‚àû) e, portanto, a inova√ß√£o fundamental.

> üí° **Exemplo Num√©rico:**
>
> Suponha que tenhamos estimado um processo MA(1) invert√≠vel com $\mu = 20$, $\theta = 0.8$ e $\sigma^2 = 4$. Isso significa que o valor atual √© altamente dependente do erro anterior. Se observarmos $Y_t = 22$, $Y_{t-1} = 21$ e $Y_{t-2} = 19$, podemos aproximar a previs√£o de $Y_{t+1}$ usando os tr√™s primeiros termos da representa√ß√£o AR(‚àû):
>
> $\text{Step 1: Calculate } \pi_1 = -(-\theta)^1 = 0.8$
> $\text{Step 2: Calculate } \pi_2 = -(-\theta)^2 = -0.64$
> $\text{Step 3: Calculate } \pi_3 = -(-\theta)^3 = 0.512$
>
> $E[Y_{t+1} | Y_t, Y_{t-1}, Y_{t-2}] \approx 20 + 0.8(22 - 20) - 0.64(21 - 20) + 0.512(19 - 20) = 20 + 1.6 - 0.64 - 0.512 = 20.448$.
>
> Assim, nossa previs√£o para $Y_{t+1}$ √© 20.448. Essa previs√£o leva em considera√ß√£o os valores passados de $Y$ e o decaimento dos coeficientes $\pi_j$, e se torna cada vez mais precisa a cada termo adicionado.  Na pr√°tica, essa previs√£o poderia representar a expectativa de vendas no pr√≥ximo dia, com base no hist√≥rico recente de vendas. O par√¢metro $\theta$ alto indica que um aumento nas vendas em um dia provavelmente levar√° a um aumento nas vendas no dia seguinte.

**Proposi√ß√£o 7**. A previs√£o de $Y_{t+h}$ para um horizonte $h$ usando a representa√ß√£o AR(‚àû) com a inova√ß√£o fundamental converge para a m√©dia incondicional $\mu$ √† medida que $h$ aumenta.

*Prova*. √Ä medida que o horizonte de previs√£o aumenta, o impacto das observa√ß√µes passadas na previs√£o diminui exponencialmente, e a previs√£o converge para a m√©dia incondicional do processo.

I. Seja $Y_{t+h} = \mu + \sum_{j=1}^{\infty} \pi_j (Y_{t+h-j} - \mu) + \epsilon_{t+h}$ a representa√ß√£o AR(‚àû).

II. Dado o valor esperado condicional $E[Y_{t+h} | Y_t, Y_{t-1}, \ldots] = \mu + \sum_{j=1}^{\infty} \pi_j (E[Y_{t+h-j} | Y_t, Y_{t-1}, \ldots] - \mu)$.

III. Como a representa√ß√£o AR(‚àû) √© convergente (invertibilidade), os coeficientes $\pi_j$ decaem exponencialmente √† medida que j aumenta. Isso significa que, para grandes valores de h, o impacto das observa√ß√µes at√© o tempo t diminui, e a previs√£o converge para a m√©dia incondicional $\mu$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considerando que estamos dez per√≠odos adiante ($Y_{t+10}$) com $\theta = 0.5$ e $\pi_j = -( -0.5)^j$, a estimativa de $E[Y_{t+10} | Y_t, Y_{t-1}, Y_{t-2}, \ldots] $ ser√° mais pr√≥xima de $\mu$ j√° que os pesos dos $Y_t, Y_{t-1}, \ldots$ ser√£o muito pequenos.
>
> Por exemplo, se $\mu = 50$, ent√£o, independentemente dos valores de $Y_t, Y_{t-1}, Y_{t-2}, \ldots$, nossa previs√£o de $Y_{t+10}$ se aproximar√° de 50. Isso ocorre porque o efeito dos valores passados diminui √† medida que prevemos mais adiante no futuro. Em termos pr√°ticos, isso significa que, para previs√µes de longo prazo, a m√©dia do processo se torna a melhor estimativa, j√° que os choques de curto prazo se dissipam.

**Teorema 5**. As previs√µes geradas com base na inova√ß√£o fundamental (isto √©, usando a representa√ß√£o AR(‚àû) de um processo MA(1) invert√≠vel) s√£o otimamente eficientes no sentido de que incorporam todas as informa√ß√µes relevantes dispon√≠veis no momento da previs√£o.

*Prova*. O teorema segue-se do fato de que a representa√ß√£o AR(‚àû) inclui todos os *lags* passados da s√©rie temporal, ponderados de forma apropriada de acordo com as autocorrela√ß√µes do processo.

I. Seja $E[Y_{t+1} | I_t]$ a previs√£o de $Y_{t+1}$ condicional ao conjunto de informa√ß√µes $I_t$ dispon√≠veis no tempo t.

II. Para um processo MA(1) invert√≠vel, o conjunto de informa√ß√µes $I_t$ inclui todos os valores passados da s√©rie temporal: $I_t = \{Y_t, Y_{t-1}, Y_{t-2}, \ldots\}$.

III. A representa√ß√£o AR(‚àû) do processo MA(1) utiliza todos os *lags* passados de forma que $E[Y_{t+1} | I_t] = \mu + \sum_{j=1}^{\infty} \pi_j (Y_{t+1-j} - \mu)$.

IV. Assim, a previs√£o √© otimamente eficiente porque incorpora todas as informa√ß√µes relevantes dispon√≠veis. ‚ñ†

**Teorema 5.1**. Para um processo MA(1) invert√≠vel, o erro de previs√£o de h per√≠odos √† frente, $e_{t+h} = Y_{t+h} - E[Y_{t+h} | Y_t, Y_{t-1}, ...]$, √© dado por $e_{t+h} = \sum_{i=0}^{h-1} \epsilon_{t+h-i} \theta^i$.

*Prova*. A prova utiliza a representa√ß√£o MA(1) e as propriedades da esperan√ßa condicional.

I. Considere o processo MA(1): $Y_{t+h} = \mu + \epsilon_{t+h} + \theta \epsilon_{t+h-1}$.
II. A previs√£o de $Y_{t+h}$ baseada nas informa√ß√µes at√© o tempo $t$ √© $E[Y_{t+h} | Y_t, Y_{t-1}, ...] = \mu + \sum_{j=1}^{\infty} \pi_j (Y_{t+h-j} - \mu)$. Para h = 1, isso se reduz a $\mu + \theta \epsilon_t$ se tiv√©ssemos a informa√ß√£o de $\epsilon_t$.
III. O erro de previs√£o √© ent√£o $e_{t+h} = Y_{t+h} - E[Y_{t+h} | Y_t, Y_{t-1}, ...] = \epsilon_{t+h} + \theta \epsilon_{t+h-1} + \theta^2 \epsilon_{t+h-2} + \ldots + \theta^{h-1}\epsilon_{t+1}$
IV. Logo, $e_{t+h} = \sum_{i=0}^{h-1} \epsilon_{t+h-i} \theta^i$.‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Se $\theta=0.6$, e h=3, ent√£o o erro de previs√£o de 3 per√≠odos √† frente √© $e_{t+3} = \epsilon_{t+3} + 0.6\epsilon_{t+2} + 0.36\epsilon_{t+1}$.  Este erro √© uma combina√ß√£o dos choques (inova√ß√£o fundamental) nos per√≠odos $t+1$, $t+2$ e $t+3$, cada um ponderado por $\theta$ elevado √† pot√™ncia apropriada.  O valor de $\theta$ dita o quanto os choques passados impactam o erro de previs√£o.  Se $\theta$ √© pequeno, os choques passados t√™m pouco efeito; se $\theta$ √© grande, os choques passados podem ter um impacto significativo no erro de previs√£o.

**Teorema 5.2**. A vari√¢ncia do erro de previs√£o de *h* per√≠odos √† frente para um processo MA(1) invert√≠vel √© dada por $Var(e_{t+h}) = \sigma^2 \sum_{i=0}^{h-1} \theta^{2i} = \sigma^2 \frac{1 - \theta^{2h}}{1 - \theta^2}$.

*Prova*. A prova utiliza a representa√ß√£o do erro de previs√£o e as propriedades da vari√¢ncia.

I. Do Teorema 5.1, temos que $e_{t+h} = \sum_{i=0}^{h-1} \epsilon_{t+h-i} \theta^i$.
II. Assumindo que os $\epsilon_t$ s√£o independentes e identicamente distribu√≠dos com m√©dia zero e vari√¢ncia $\sigma^2$, ent√£o $Var(e_{t+h}) = Var(\sum_{i=0}^{h-1} \epsilon_{t+h-i} \theta^i) = \sum_{i=0}^{h-1} Var(\epsilon_{t+h-i}) \theta^{2i} = \sigma^2 \sum_{i=0}^{h-1} \theta^{2i}$.
III. Usando a f√≥rmula para a soma de uma progress√£o geom√©trica, temos $\sum_{i=0}^{h-1} \theta^{2i} = \frac{1 - \theta^{2h}}{1 - \theta^2}$.
IV. Portanto, $Var(e_{t+h}) = \sigma^2 \frac{1 - \theta^{2h}}{1 - \theta^2}$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Se $\sigma^2 = 9$ (o que significa que os choques t√™m um desvio padr√£o de 3) e $\theta = 0.7$, ent√£o a vari√¢ncia do erro de previs√£o para h = 5 per√≠odos √† frente √© $Var(e_{t+5}) = 9 * (1 - 0.7^{2*5}) / (1 - 0.7^2) = 9 * (1 - 0.028) / (1 - 0.49) = 9 * 0.972 / 0.51 = 17.17$.
>
> Isso nos diz que a incerteza em torno de nossa previs√£o aumenta √† medida que prevemos mais adiante no futuro. A vari√¢ncia do erro de previs√£o converge para $\sigma^2 / (1 - \theta^2)$ quando h tende ao infinito, que neste caso √© $9 / (1 - 0.49) = 17.65$. Isso representa a incerteza m√°xima que teremos ao prever este processo MA(1) no longo prazo.

### 6.8.3 Invertibilidade para Implementa√ß√µes Pr√°ticas
Na pr√°tica, a garantia de invertibilidade na estima√ß√£o de modelos MA(1) √© crucial. A raz√£o √© que a abordagem da M√°xima Verossimilhan√ßa e outros m√©todos de estima√ß√£o frequentemente dependem de expressar o modelo em termos de uma representa√ß√£o causal ou quase causal.

Como j√° vimos, existem algoritmos que estimam a fun√ß√£o por meio da representa√ß√£o AR(‚àû). Por√©m, podem ocorrer erros de estima√ß√£o. Sendo assim, recomenda-se:
I. Estimar a fun√ß√£o da representa√ß√£o MA(1)
II. Garantir que o theta estimado seja menor que 1.

> üí° **Exemplo Num√©rico:**
>
> Considere o m√©todo de estima√ß√£o por M√°xima Verossimilhan√ßa:
>
> Se $\theta = 2$ √© estimado em vez de $\theta = 0.5$ (dado o lema 1 que ambos levam a mesmas autocorrela√ß√µes), ent√£o a fun√ß√£o de verossimilhan√ßa estaria invertida (pois a densidade condicional est√° mal especificada), levando a erros de estima√ß√£o.
>
> ```python
> # Demonstra√ß√£o do efeito da invertibilidade nos par√¢metros estimados
> from statsmodels.tsa.arima.model import ARIMA
> import numpy as np
>
> # Fun√ß√£o para simular um processo MA(1)
> def simulate_ma1(theta, sigma2, n):
>     errors = np.random.normal(0, np.sqrt(sigma2), n)
>     y = [errors[0]]
>     for i in range(1, n):
>         y.append(errors[i] + theta * errors[i - 1])
>     return y
>
> # Par√¢metros da simula√ß√£o
> mu = 0
> sigma2 = 1
> n = 200
>
> # Simular uma s√©rie temporal com theta = 0.5 (invert√≠vel)
> theta = 0.5
> data_inv = simulate_ma1(theta, sigma2, n)
>
> # Ajustar um modelo MA(1) e obter os resultados
> model_inv = ARIMA(data_inv, order=(0,0,1))
> results_inv = model_inv.fit()
> print(results_inv.summary())
>
> # Plotar os res√≠duos
> residuals_inv = pd.DataFrame(results_inv.resid)
> residuals_inv.plot(title="Res√≠duos do Modelo MA(1) (Invert√≠vel)")
> plt.show()
>
> # Gr√°fico ACF dos res√≠duos
> plot_acf(residuals_inv, lags=40, title="ACF dos Res√≠duos (Invert√≠vel)")
> plt.show()
>
> # Simular uma s√©rie temporal com theta = 2 (n√£o invert√≠vel)
> theta = 2
> data_noninv = simulate_ma1(theta, sigma2, n)
>
> # Ajustar um modelo MA(1) e obter os resultados
> model_noninv = ARIMA(data_noninv, order=(0,0,1))
> results_noninv = model_noninv.fit()
> print(results_noninv.summary())
>
> # Plotar os res√≠duos
> residuals_noninv = pd.DataFrame(results_noninv.resid)
> residuals_noninv.plot(title="Res√≠duos do Modelo MA(1) (N√£o Invert√≠vel)")
> plt.show()
>
> # Gr√°fico ACF dos res√≠duos
> plot_acf(residuals_noninv, lags=40, title="ACF dos Res√≠duos (N√£o Invert√≠vel)")
> plt.show()

### Discuss√£o
Nos exemplos acima, simulamos duas s√©ries temporais MA(1), uma com $\theta = 0.5$ (invert√≠vel) e outra com $\theta = 2$ (n√£o invert√≠vel). Ajustamos um modelo MA(1) a cada s√©rie e analisamos os res√≠duos.

Para a s√©rie invert√≠vel ($\theta = 0.5$), os res√≠duos devem se comportar como ru√≠do branco, indicando um bom ajuste do modelo. O gr√°fico ACF dos res√≠duos deve mostrar pouca ou nenhuma autocorrela√ß√£o significativa.

Para a s√©rie n√£o invert√≠vel ($\theta = 2$), os res√≠duos podem exibir padr√µes de autocorrela√ß√£o, indicando que o modelo MA(1) n√£o capturou completamente a estrutura da s√©rie temporal.

A an√°lise dos res√≠duos √© crucial para validar a adequa√ß√£o do modelo ajustado. Se os res√≠duos n√£o se comportarem como ru√≠do branco, pode ser necess√°rio reconsiderar a ordem do modelo ou explorar outras abordagens de modelagem. <!-- END -->
