## Impacto of AR(‚àû) Representation for Forecasting and Control in Invertible MA Processes

### Introdu√ß√£o
Expandindo o conceito de invertibilidade em processos *Moving Average* (MA), este cap√≠tulo explora a import√¢ncia de uma representa√ß√£o *Autoregressive* de ordem infinita (AR(‚àû)) bem definida para fins de previs√£o e controle. Como vimos anteriormente, a invertibilidade garante que um processo MA possa ser expresso como um processo AR(‚àû) est√°vel e causal [^65]. Aqui, nos aprofundaremos nas implica√ß√µes pr√°ticas dessa representa√ß√£o, demonstrando como a propriedade de um decaimento temporal no impacto dos *shocks* passados √© crucial para a constru√ß√£o de modelos preditivos eficazes e para a implementa√ß√£o de estrat√©gias de controle robustas. Al√©m disso, exploraremos como diferentes taxas de decaimento influenciam a efici√™ncia da previs√£o e a agressividade das pol√≠ticas de controle.

### Impacto da Representa√ß√£o AR(‚àû) na Previs√£o
Para um processo MA invert√≠vel, a representa√ß√£o AR(‚àû) implica que o impacto de *shocks* passados decai ao longo do tempo. Isso √© essencial para a precis√£o da previs√£o. Considere um processo MA(1) invert√≠vel:
$$Y_t = \mu + (1 + \theta L)\epsilon_t, \quad |\theta| < 1$$
Como demonstrado anteriormente, podemos expressar este processo como um AR(‚àû):
$$Y_t = \mu + \sum_{j=1}^{\infty} \pi_j (Y_{t-j} - \mu) + \epsilon_t$$
onde $\pi_j = -(-\theta)^j$ [^Previous Topics]. Devido √† condi√ß√£o de invertibilidade $|\theta| < 1$, os coeficientes $\pi_j$ decaem exponencialmente √† medida que $j$ aumenta.

> üí° **Exemplo Num√©rico:**
>
> Considere um processo MA(1) com $\mu = 10$ e $\theta = 0.7$. Vamos calcular os primeiros coeficientes $\pi_j$ da representa√ß√£o AR(‚àû):
>
> *   $\pi_1 = -(-\theta)^1 = -(-0.7)^1 = 0.7$
> *   $\pi_2 = -(-\theta)^2 = -(-0.7)^2 = -0.49$
> *   $\pi_3 = -(-\theta)^3 = -(-0.7)^3 = 0.343$
> *   $\pi_4 = -(-\theta)^4 = -(-0.7)^4 = -0.2401$
>
> A representa√ß√£o AR(‚àû) truncada aos primeiros quatro termos √©:
> $$Y_t \approx 10 + 0.7(Y_{t-1} - 10) - 0.49(Y_{t-2} - 10) + 0.343(Y_{t-3} - 10) - 0.2401(Y_{t-4} - 10) + \epsilon_t$$
>
> Observe o decaimento exponencial dos coeficientes. Um valor de $\theta$ mais pr√≥ximo de zero resultaria em um decaimento ainda mais r√°pido. Por exemplo, com $\theta = 0.2$:
>
> *   $\pi_1 = 0.2$
> *   $\pi_2 = -0.04$
> *   $\pi_3 = 0.008$
> *   $\pi_4 = -0.0016$
>
> O decaimento √© muito mais r√°pido.

> üí° **Exemplo Num√©rico:**
>
> Se $\theta = 0.5$, a representa√ß√£o AR(‚àû) √©:
> $$Y_t = \mu + 0.5(Y_{t-1} - \mu) - 0.25(Y_{t-2} - \mu) + 0.125(Y_{t-3} - \mu) - \ldots + \epsilon_t$$
> O impacto de $Y_{t-1}$ √© 0.5, o impacto de $Y_{t-2}$ √© -0.25, e assim por diante. Observe como os impactos diminuem √† medida que o *lag* aumenta.

Agora, considere fazer uma previs√£o de $Y_{t+1}$ com base nas informa√ß√µes dispon√≠veis at√© o tempo $t$. Usando a representa√ß√£o AR(‚àû), temos:
$$E[Y_{t+1} | Y_t, Y_{t-1}, Y_{t-2}, \ldots] = \mu + \sum_{j=1}^{\infty} \pi_j (Y_{t+1-j} - \mu)$$
Como os coeficientes $\pi_j$ decaem, as observa√ß√µes mais recentes t√™m um peso maior na previs√£o. Isso significa que a previs√£o √© mais sens√≠vel √†s informa√ß√µes recentes e menos sens√≠vel aos *shocks* mais antigos, que j√° tiveram seu impacto diminu√≠do.

> üí° **Exemplo Num√©rico:**
>
> Suponha que $\mu = 0$ e $\theta = 0.5$. Queremos prever $Y_{t+1}$ usando $Y_t = 1$, $Y_{t-1} = 0.5$, $Y_{t-2} = -0.25$. Usando os tr√™s primeiros termos da representa√ß√£o AR(‚àû), temos:
> $$E[Y_{t+1} | Y_t, Y_{t-1}, Y_{t-2}] \approx 0 + 0.5(1 - 0) - 0.25(0.5 - 0) + 0.125(-0.25 - 0) = 0.5 - 0.125 - 0.03125 = 0.34375$$
> A previs√£o leva em considera√ß√£o as observa√ß√µes recentes, com pesos que diminuem √† medida que o tempo retrocede.
>
> Para comparar, se us√°ssemos apenas o primeiro termo (lag 1):
> $$E[Y_{t+1} | Y_t] \approx 0 + 0.5(1 - 0) = 0.5$$
>
> A inclus√£o de mais termos refina a previs√£o, embora a contribui√ß√£o marginal de cada termo adicional diminua devido ao decaimento exponencial.

Em contraste, se o processo n√£o fosse invert√≠vel (ou seja, $|\theta| \geq 1$), os coeficientes na representa√ß√£o AR(‚àû) cresceriam em magnitude, resultando em um modelo de previs√£o inst√°vel e pouco confi√°vel.

**Lema 3**. Em um processo MA(1), previs√µes baseadas na representa√ß√£o AR(‚àû) de um processo invert√≠vel s√£o mais robustas a erros de especifica√ß√£o nos *lags* mais distantes do que previs√µes baseadas em uma representa√ß√£o n√£o invert√≠vel.

*Prova*. Em um processo invert√≠vel, o impacto de *lags* distantes diminui exponencialmente. Portanto, erros na especifica√ß√£o desses *lags* t√™m um impacto limitado na precis√£o da previs√£o. No entanto, em uma representa√ß√£o n√£o invert√≠vel, *lags* distantes t√™m um impacto crescente, tornando as previs√µes altamente sens√≠veis a erros de especifica√ß√£o.

I. Seja $Y_t = \mu + \sum_{j=1}^{\infty} \pi_j (Y_{t-j} - \mu) + \epsilon_t$ a representa√ß√£o AR(‚àû) de um processo MA(1) invert√≠vel, onde $|\theta| < 1$ e $\pi_j = - (-\theta)^j$.

II. Seja $\hat{Y}_{t+1} = \mu + \sum_{j=1}^{N} \pi_j (Y_{t+1-j} - \mu)$ uma previs√£o truncada usando os primeiros N termos da representa√ß√£o AR(‚àû). O erro de especifica√ß√£o √© $e = \sum_{j=N+1}^{\infty} \pi_j (Y_{t+1-j} - \mu)$.

III. Como $|\theta| < 1$, os coeficientes $\pi_j$ decaem exponencialmente com o aumento de $j$. Assim, o erro de especifica√ß√£o $e$ √© limitado, pois os termos com grande $j$ contribuem pouco para a soma.

IV. Para um processo n√£o invert√≠vel, a representa√ß√£o AR(‚àû) tem coeficientes que aumentam em magnitude. Portanto, erros na especifica√ß√£o dos *lags* mais distantes t√™m um impacto crescente, tornando as previs√µes altamente sens√≠veis a esses erros.

V. Portanto, previs√µes baseadas na representa√ß√£o AR(‚àû) de um processo invert√≠vel s√£o mais robustas a erros de especifica√ß√£o nos *lags* mais distantes. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Seja $\theta = 0.5$ e assuma um erro de especifica√ß√£o de 0.1 nos coeficientes de lag 10. O impacto desse erro √© 0.1 * (0.5)^10 = 0.1 * 0.0009765625 ‚âà 0.00009765625. Se Œ∏ = 2, o impacto √© 0.1 * (2)^10 = 0.1 * 1024 = 102.4, que √© significativamente maior.

**Lema 3.1**. A vari√¢ncia do erro de previs√£o diminui √† medida que mais termos da representa√ß√£o AR(‚àû) s√£o inclu√≠dos na previs√£o, para um processo MA(1) invert√≠vel.

*Prova*. Incluir mais termos na representa√ß√£o AR(‚àû) captura uma por√ß√£o maior da depend√™ncia temporal do processo, reduzindo a incerteza na previs√£o.

I. Seja $e_N = Y_{t+1} - \hat{Y}_{t+1} = \sum_{j=N+1}^{\infty} \pi_j (Y_{t+1-j} - \mu) + \epsilon_{t+1}$ o erro de previs√£o ao usar N termos.

II. A vari√¢ncia do erro de previs√£o √© $Var(e_N) = Var(\sum_{j=N+1}^{\infty} \pi_j (Y_{t+1-j} - \mu) + \epsilon_{t+1})$.

III. Assumindo que $Y_t$ e $\epsilon_t$ s√£o independentes e que $E[Y_t]=\mu$, temos:
$Var(e_N) = Var(\sum_{j=N+1}^{\infty} \pi_j (Y_{t+1-j} - \mu)) + Var(\epsilon_{t+1}) = \sum_{j=N+1}^{\infty} \pi_j^2 Var(Y_{t+1-j}) + \sigma^2$.

IV. Como o processo √© invert√≠vel, os coeficientes $\pi_j$ decaem exponencialmente. Assim, √† medida que N aumenta, a soma $\sum_{j=N+1}^{\infty} \pi_j^2 Var(Y_{t+1-j})$ diminui.

V. Portanto, $Var(e_N)$ diminui √† medida que N aumenta, convergindo para $Var(\epsilon_{t+1}) = \sigma^2$ quando N tende ao infinito. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Suponha que $Var(Y_t) = 1$ e $\sigma^2 = 0.5$. Vamos comparar a vari√¢ncia do erro de previs√£o com N=1 e N=3 para $\theta = 0.6$:
>
> Para N=1:
> $Var(e_1) = \sum_{j=2}^{\infty} \pi_j^2 Var(Y_{t+1-j}) + \sigma^2 = \pi_2^2 Var(Y_{t-1}) + \pi_3^2 Var(Y_{t-2}) + ... + \sigma^2$
> $Var(e_1) \approx (0.6^2)^2 * 1 + (-(0.6)^3)^2 * 1 + \sigma^2 = 0.1296 + 0.046656 + ... + 0.5 \approx 0.676$
>
> Para N=3:
> $Var(e_3) = \sum_{j=4}^{\infty} \pi_j^2 Var(Y_{t+1-j}) + \sigma^2 = \pi_4^2 Var(Y_{t-3}) + \pi_5^2 Var(Y_{t-4}) + ... + \sigma^2$
> $Var(e_3) \approx (0.6^4)^2 * 1 + (-(0.6)^5)^2 * 1 + \sigma^2 = 0.01679616 + 0.0060466176 + ... + 0.5 \approx 0.523$
>
> A vari√¢ncia do erro de previs√£o √© menor com N=3 do que com N=1, confirmando o lema.

**Teorema 1**. Para dois processos MA(1) invert√≠veis com par√¢metros $\theta_1$ e $\theta_2$, onde $|\theta_1| < |\theta_2| < 1$, o processo com $\theta_1$ ter√° um decaimento mais r√°pido dos coeficientes na representa√ß√£o AR(‚àû) e, portanto, previs√µes baseadas em um n√∫mero menor de *lags* ser√£o compar√°veis em precis√£o √†s previs√µes do processo com $\theta_2$ que usam mais *lags*.

*Prova*. A taxa de decaimento dos coeficientes $\pi_j$ na representa√ß√£o AR(‚àû) √© determinada pela magnitude de $\theta$. Quanto menor a magnitude de $\theta$, mais r√°pido o decaimento.

I. Para o processo MA(1) com par√¢metro $\theta_1$, os coeficientes AR(‚àû) s√£o $\pi_{1j} = -(-\theta_1)^j$.
II. Para o processo MA(1) com par√¢metro $\theta_2$, os coeficientes AR(‚àû) s√£o $\pi_{2j} = -(-\theta_2)^j$.
III. Dado que $|\theta_1| < |\theta_2|$, para qualquer $j$, $|\pi_{1j}| < |\pi_{2j}|$. Al√©m disso, a taxa de converg√™ncia de $\sum_{j=1}^{\infty} \pi_{1j}$ √© maior do que a de $\sum_{j=1}^{\infty} \pi_{2j}$.
IV. Isso implica que, para um determinado n√≠vel de precis√£o na previs√£o, o n√∫mero de *lags* necess√°rios para o processo com $\theta_1$ ser√° menor do que para o processo com $\theta_2$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Sejam $\theta_1 = 0.2$ e $\theta_2 = 0.8$. Ap√≥s 5 *lags*, o coeficiente para $\theta_1$ √© aproximadamente 0.00032, enquanto para $\theta_2$ √© aproximadamente 0.32768. Isso demonstra que o impacto dos *lags* diminui muito mais rapidamente para $\theta_1$.
>
> Mais precisamente:
>
> *   $|\pi_{1,5}| = |-(-\theta_1)^5| = |-( -0.2)^5| = 0.00032$
> *   $|\pi_{2,5}| = |-(-\theta_2)^5| = |-( -0.8)^5| = 0.32768$
>
> Isso significa que para obter uma precis√£o similar na previs√£o, precisar√≠amos incluir muito mais *lags* para o processo com $\theta_2 = 0.8$ do que para o processo com $\theta_1 = 0.2$.

### Import√¢ncia da Representa√ß√£o AR(‚àû) no Controle
Em contextos de controle, a invertibilidade e a representa√ß√£o AR(‚àû) correspondente s√£o cruciais para projetar pol√≠ticas de controle eficazes. Suponha que desejamos controlar o processo $Y_t$ para mant√™-lo pr√≥ximo a um determinado n√≠vel alvo $Y^*$. Podemos usar a representa√ß√£o AR(‚àû) para derivar uma regra de controle que ajuste o processo com base em seus desvios passados do alvo.

Considere o processo MA(1) invert√≠vel:
$$Y_t = \mu + (1 + \theta L)\epsilon_t$$
Podemos projetar uma pol√≠tica de controle que ajuste $\epsilon_t$ para compensar os desvios passados do alvo. Usando a representa√ß√£o AR(‚àû):
$$Y_t - \mu = \sum_{j=1}^{\infty} \pi_j (Y_{t-j} - \mu) + \epsilon_t$$
Podemos definir uma regra de controle para $\epsilon_t$ como:
$$\epsilon_t = -\sum_{j=1}^{\infty} \pi_j (Y_{t-j} - \mu)$$
Esta regra ajusta o valor atual de $\epsilon_t$ com base nos desvios passados de $Y_t$ da sua m√©dia $\mu$. Como os coeficientes $\pi_j$ decaem, a pol√≠tica de controle d√° mais peso aos desvios recentes e menos peso aos desvios mais antigos.

Se o processo n√£o fosse invert√≠vel, a regra de controle resultante seria inst√°vel e ineficaz, pois os desvios passados teriam um peso crescente, levando a ajustes excessivos e potencialmente divergentes.

**Teorema 2**. Para um processo MA(q) invert√≠vel, uma pol√≠tica de controle projetada com base na representa√ß√£o AR(‚àû) garante a estabilidade do sistema controlado.

*Prova*. A estabilidade de um sistema controlado requer que a resposta ao *feedback* seja limitada e que o sistema n√£o convirja para um estado ilimitado. Uma representa√ß√£o AR(‚àû) convergente garante que os pesos nos *lags* passados decaiam, levando a uma resposta de *feedback* limitada e garantindo a estabilidade.

I. Seja $Y_t = \mu + \sum_{i=1}^{q} \theta_i \epsilon_{t-i} + \epsilon_t$ um processo MA(q) invert√≠vel.

II. A representa√ß√£o AR(‚àû) √© dada por $\epsilon_t = (1 + \sum_{i=1}^{q} \theta_i L^i)^{-1} (Y_t - \mu) = \sum_{j=0}^{\infty} \pi_j (Y_{t-j} - \mu)$.

III. A pol√≠tica de controle √© projetada para compensar os desvios passados do alvo, de forma que $\epsilon_t = -\sum_{j=1}^{\infty} \pi_j (Y_{t-j} - \mu)$.

IV. Substituindo a pol√≠tica de controle na representa√ß√£o MA(q), temos:
$Y_t = \mu + \sum_{i=1}^{q} \theta_i \epsilon_{t-i} + \epsilon_t = \mu + \sum_{i=1}^{q} \theta_i [-\sum_{j=1}^{\infty} \pi_j (Y_{t-i-j} - \mu)] - \sum_{j=1}^{\infty} \pi_j (Y_{t-j} - \mu)$.

V. Para a estabilidade, os coeficientes $\pi_j$ devem convergir, o que √© garantido pela invertibilidade do processo. Isso significa que a influ√™ncia dos valores passados diminui com o tempo, garantindo que o sistema n√£o convirja para um estado ilimitado.

VI. Portanto, uma pol√≠tica de controle projetada com base na representa√ß√£o AR(‚àû) de um processo MA(q) invert√≠vel garante a estabilidade do sistema controlado. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere um processo MA(1) com $\theta = 0.5$ e $\mu = 0$. A pol√≠tica de controle √© $\epsilon_t = -0.5Y_{t-1} + 0.25Y_{t-2} - 0.125Y_{t-3} + \ldots$. Se $Y_{t-1} = 1$, o ajuste inicial √© $\epsilon_t = -0.5$. Os ajustes subsequentes ser√£o menores devido ao decaimento dos coeficientes, garantindo que o sistema permane√ßa est√°vel.
>
> Vamos supor que o valor alvo $Y^* = 0$. Inicialmente, $Y_{t-1} = 1$. A pol√≠tica de controle ajusta $\epsilon_t$:
>
> *   $\epsilon_t = -0.5 * 1 = -0.5$
> *   $Y_t = 0 + (1 + 0.5L)\epsilon_t = \epsilon_t + 0.5\epsilon_{t-1} $. Se $\epsilon_{t-1} = 0$ (inicial), ent√£o $Y_t = -0.5$
> *   Agora, para o pr√≥ximo per√≠odo: $\epsilon_{t+1} = -0.5Y_t = -0.5*(-0.5) = 0.25$
> *   $Y_{t+1} = \epsilon_{t+1} + 0.5\epsilon_{t} = 0.25 + 0.5*(-0.5) = 0$
>
> O sistema se estabiliza rapidamente em torno do valor alvo.

**Teorema 2.1**. Em um processo MA(1) invert√≠vel, quanto menor o valor absoluto de $\theta$, menos agressiva ser√° a pol√≠tica de controle necess√°ria para manter o processo pr√≥ximo ao n√≠vel alvo.

*Prova*. Um menor $|\theta|$ implica um decaimento mais r√°pido dos coeficientes AR(‚àû). Isso significa que os desvios passados t√™m um impacto menor no valor atual, exigindo ajustes menores para manter a estabilidade.

I. Seja $\epsilon_t = -\sum_{j=1}^{\infty} \pi_j (Y_{t-j} - \mu)$ a pol√≠tica de controle baseada na representa√ß√£o AR(‚àû).

II. Os coeficientes $\pi_j = -(-\theta)^j$ determinam a magnitude dos ajustes em resposta aos desvios passados.

III. Se $|\theta|$ √© pequeno, os $\pi_j$ decaem rapidamente, o que implica que os ajustes $\epsilon_t$ ser√£o relativamente pequenos, mesmo para grandes desvios recentes.

IV. Se $|\theta|$ est√° pr√≥ximo de 1, os $\pi_j$ decaem mais lentamente, exigindo ajustes maiores em $\epsilon_t$ para compensar os desvios passados e manter a estabilidade.

V. Portanto, quanto menor o valor absoluto de $\theta$, menos agressiva ser√° a pol√≠tica de controle necess√°ria. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Comparando $\theta = 0.1$ e $\theta = 0.9$, para um desvio de $Y_{t-1} = 1$ do alvo, a corre√ß√£o inicial para $\theta = 0.1$ √© $\epsilon_t = -0.1$, enquanto para $\theta = 0.9$ √© $\epsilon_t = -0.9$. Isso mostra que a pol√≠tica de controle √© muito mais agressiva para $\theta = 0.9$.
>
> Seja $\mu=0$ e $Y_{t-1} = 1$.
>
> *   Se $\theta = 0.1$, $\epsilon_t = -0.1(1) = -0.1$. Assim, o ajuste √© pequeno.
> *   Se $\theta = 0.9$, $\epsilon_t = -0.9(1) = -0.9$. Assim, o ajuste √© muito maior.
>
> A pol√≠tica de controle com $\theta = 0.9$ √© mais agressiva para trazer o processo de volta ao alvo.

Em resumo, a representa√ß√£o AR(‚àû) de um processo MA invert√≠vel fornece uma base s√≥lida para previs√µes precisas e pol√≠ticas de controle eficazes, garantindo que o impacto de *shocks* passados decaia ao longo do tempo e que o sistema permane√ßa est√°vel. Al√©m disso, a magnitude do par√¢metro de invertibilidade influencia tanto a taxa de decaimento do impacto dos *shocks* quanto a agressividade necess√°ria das pol√≠ticas de controle.

### Conclus√£o
A representa√ß√£o AR(‚àû) de um processo MA invert√≠vel √© crucial para fins de previs√£o e controle [^65]. A propriedade de decaimento temporal do impacto de *shocks* passados, garantida pela invertibilidade, permite a constru√ß√£o de modelos de previs√£o robustos e a implementa√ß√£o de estrat√©gias de controle eficazes. A estabilidade de tais sistemas √© uma consequ√™ncia direta da representa√ß√£o AR(‚àû) bem definida, tornando a invertibilidade uma considera√ß√£o essencial na modelagem e an√°lise de s√©ries temporais. Adicionalmente, a taxa de decaimento, influenciada pelos par√¢metros do processo MA, impacta diretamente a precis√£o da previs√£o com um n√∫mero limitado de *lags* e a agressividade necess√°ria das pol√≠ticas de controle.

### Refer√™ncias
[^65]: Provided that |0| < 1, both sides of [3.7.1] can be multiplied by (1 + 8L) 1 to obtain
[^Previous Topics]:  J√° demonstramos que $(1 + \theta L)^{-1}(Y_t - \mu) = \epsilon_t$ pode ser expandido para $(1 - \theta L + \theta^2 L^2 - \theta^3 L^3 + \ldots)(Y_t - \mu) = \epsilon_t$. Rearranjando, temos $Y_t - \mu = \theta (Y_{t-1} - \mu) - \theta^2(Y_{t-2} - \mu) + \theta^3 (Y_{t-3} - \mu) - \ldots + \epsilon_t$.
<!-- END -->