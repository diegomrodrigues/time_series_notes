## A Distribui√ß√£o Assint√≥tica dos Estimadores de MQO para a Regress√£o Original

### Introdu√ß√£o
Este cap√≠tulo explora a distribui√ß√£o assint√≥tica dos estimadores de M√≠nimos Quadrados Ordin√°rios (MQO) em modelos de regress√£o com tend√™ncias temporais determin√≠sticas. Conforme mencionado anteriormente [^1], o modelo de regress√£o linear simples com tend√™ncia temporal √© definido como $y_t = \alpha + \delta t + \epsilon_t$, onde $\epsilon_t$ √© um processo de ru√≠do branco. A principal diferen√ßa em rela√ß√£o a modelos estacion√°rios √© que os estimadores de MQO para os par√¢metros $\alpha$ e $\delta$ convergem a taxas diferentes [^1], o que exige uma an√°lise especial para determinar suas distribui√ß√µes assint√≥ticas. Baseado em [16.3.12] [^1], a distribui√ß√£o assint√≥tica de $\mathbf{b}$ √© inferida a partir da distribui√ß√£o de $\mathbf{b}^*$ atrav√©s da matriz de transforma√ß√£o $\mathbf{G}$, uma vez que $\mathbf{b} = \mathbf{G}'\mathbf{b}^*$.

### Conceitos Fundamentais
Como visto anteriormente, o modelo de regress√£o linear com tend√™ncia temporal pode ser escrito na forma matricial como
$$y_t = \mathbf{x}_t' \boldsymbol{\beta} + \epsilon_t$$
onde $\mathbf{x}_t = [1, t]'$ e $\boldsymbol{\beta} = [\alpha, \delta]'$. A abordagem tradicional de an√°lise assint√≥tica para modelos estacion√°rios, discutida no Cap√≠tulo 8 [^2], n√£o se aplica diretamente aqui devido √†s diferentes taxas de converg√™ncia dos estimadores de $\alpha$ e $\delta$ [^1]. Em particular, $\hat{\alpha}$ converge a uma taxa de $\sqrt{T}$, enquanto $\hat{\delta}$ converge a uma taxa de $T^{3/2}$.

A transforma√ß√£o dos regressores, conforme detalhado em [16.3.2] [^1], √© essencial para isolar as componentes com diferentes ordens de converg√™ncia, permitindo uma an√°lise assint√≥tica mais direta. O modelo transformado √© dado por:
$$y_t = [\mathbf{x}_t^*]' \boldsymbol{\beta}^* + \epsilon_t$$
onde $\boldsymbol{\beta}^*$ representa os par√¢metros transformados [^1]. A rela√ß√£o entre os coeficientes do modelo original e transformado √© dada por $\mathbf{b} = \mathbf{G}'\mathbf{b}^*$, conforme discutido anteriormente [^1], [^2]. O Ap√™ndice 16.A [^1] fornece a deriva√ß√£o detalhada da distribui√ß√£o assint√≥tica de $\mathbf{b}^*$, demonstrando que
$$\mathbf{Y}_T(\mathbf{b}^* - \boldsymbol{\beta}^*) \overset{d}{\longrightarrow} N(\mathbf{0}, \sigma^2[\mathbf{Q}^*]^{-1})$$
onde $\mathbf{Y}_T$ √© uma matriz diagonal com elementos $\sqrt{T}$ para os coeficientes estacion√°rios e $T^{3/2}$ para o coeficiente da tend√™ncia temporal, e $\mathbf{Q}^*$ √© uma matriz de momentos populacionais dos regressores transformados [^1].

**Lema 1:** A matriz $\mathbf{Y}_T$ ajusta a escala da taxa de converg√™ncia dos estimadores, com $\sqrt{T}$ para o intercepto e $T^{3/2}$ para a inclina√ß√£o da tend√™ncia temporal.

> üí° **Exemplo Num√©rico:** No modelo simples com tend√™ncia linear, a matriz $\mathbf{Y}_T$ √© definida como:
>$$\mathbf{Y}_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}$$
> Isso reflete que o estimador de $\alpha$ converge a uma taxa de $\sqrt{T}$ e o estimador de $\delta$ converge a uma taxa de $T^{3/2}$, o que √© central para a an√°lise assint√≥tica. Por exemplo, se tivermos um tamanho amostral $T=100$, a matriz $\mathbf{Y}_T$ seria:
>
> $$\mathbf{Y}_{100} = \begin{bmatrix} \sqrt{100} & 0 \\ 0 & 100^{3/2} \end{bmatrix} = \begin{bmatrix} 10 & 0 \\ 0 & 1000 \end{bmatrix}$$
> Isso significa que, para obter uma distribui√ß√£o limite n√£o degenerada, o erro de estimativa do intercepto ($\hat{\alpha} - \alpha$) precisa ser escalado por $\sqrt{100} = 10$, e o erro da estimativa do coeficiente de tend√™ncia ($\hat{\delta} - \delta$) precisa ser escalado por $100^{3/2} = 1000$. A matriz $\mathbf{Y}_T$ √© crucial para entender as diferentes taxas de converg√™ncia dos estimadores.

A transforma√ß√£o linear $\mathbf{b} = \mathbf{G}'\mathbf{b}^*$ conecta os estimadores do modelo original com os do modelo transformado. Ao aplicar [16.3.8] [^1] e [16.3.12] [^1], podemos relacionar os estimadores transformados e originais, sendo que:

$$\begin{bmatrix}
\hat{\phi_1}\\
\hat{\phi_2}\\
\vdots\\
\hat{\phi_p}\\
\hat{\alpha}\\
\hat{\delta}
\end{bmatrix} =
\begin{bmatrix}
1 & 0 & \cdots & 0 & 0 & 0\\
0 & 1 & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\
0 & 0 & \cdots & 1 & 0 & 0\\
-\alpha + \delta & -\alpha + 2\delta & \cdots & -\alpha + p\delta & 1 & 0\\
-\delta & -\delta & \cdots & -\delta & 0 & 1
\end{bmatrix}
\begin{bmatrix}
\hat{\phi_1^*}\\
\hat{\phi_2^*}\\
\vdots\\
\hat{\phi_p^*}\\
\hat{\alpha^*}\\
\hat{\delta^*}
\end{bmatrix}$$

O importante √© notar que os estimadores do modelo original s√£o linearmente relacionados aos estimadores do modelo transformado. A distribui√ß√£o assint√≥tica de $\hat{\phi}_j$, que correspondem aos coeficientes das vari√°veis defasadas, √© dada por [16.3.13] [^1]. O estimador $\hat{\alpha}$ converge a uma taxa de $\sqrt{T}$, enquanto o estimador $\hat{\delta}$ converge a uma taxa de $T^{3/2}$ [^1], [^2].

**Teorema 1:** (Taxas de Converg√™ncia e Distribui√ß√£o Assint√≥tica de $\hat{\delta}$): O estimador $\hat{\delta}$ converge a uma taxa de $T^{3/2}$, e a sua distribui√ß√£o assint√≥tica √© gaussiana, com o componente dominante sendo a pr√≥pria tend√™ncia temporal, conforme discutido em [16.3.17] [^1]. Formalmente,  $T^{3/2}(\hat{\delta} - \delta)$ converge para uma distribui√ß√£o normal.

*Prova (Esbo√ßo)*: Como $\hat{\delta}$ √© uma combina√ß√£o linear de vari√°veis com diferentes taxas de converg√™ncia, o componente com a menor taxa de converg√™ncia domina assintoticamente. Ao aplicar a matriz de transforma√ß√£o $\mathbf{G}$ apropriada, nota-se que o termo dominante em $\hat{\delta}$ √© o associado √† tend√™ncia temporal, cuja taxa de converg√™ncia √© de ordem $T^{3/2}$. A prova detalhada, usando a forma fechada de $\hat{\delta}$ obtida atrav√©s dos estimadores de MQO e a propriedade assint√≥tica do termo de erro, mostra que $\hat{\delta}-\delta = O_p(T^{-3/2})$, assim, $T^{3/2}(\hat{\delta} - \delta)$ converge em distribui√ß√£o para uma distribui√ß√£o normal.

*Prova Detalhada:*
Seguindo os passos detalhados anteriormente, e usando as propriedades de que  $\sum_{t=1}^T (t-\bar{t})^2 \approx \frac{T^3}{12}$ e que  $\sum_{t=1}^T (t-\bar{t})(\epsilon_t-\bar{\epsilon})$ converge em distribui√ß√£o ap√≥s normaliza√ß√£o por $\sqrt{T}$, o seguinte resultado emerge:

$$\hat{\delta} - \delta = \frac{\sum_{t=1}^T (t - \bar{t})(\epsilon_t - \bar{\epsilon})}{\sum_{t=1}^T (t - \bar{t})^2} \approx \frac{O_p(T^{3/2})}{O(T^3)} = O_p(T^{-3/2})$$

Assim, multiplicando por $T^{3/2}$, obtemos:

$$T^{3/2}(\hat{\delta} - \delta) \approx T^{3/2} O_p(T^{-3/2}) = O_p(1)$$

Isto implica que $T^{3/2}(\hat{\delta} - \delta)$ converge para uma distribui√ß√£o normal. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere um modelo simulado com $y_t = 5 + 0.3t + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco gaussiano, para $T=100$.  Ao ajustar um modelo de regress√£o OLS, os estimadores para $\alpha$ e $\delta$ s√£o obtidos.
```python
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
from scipy import stats

# Tamanho da amostra
T = 100
# Par√¢metros verdadeiros
alpha_true = 5
delta_true = 0.3
# Gera√ß√£o de dados com ru√≠do branco
np.random.seed(123)
epsilon = np.random.normal(0, 1, T)
t = np.arange(1, T + 1)
y = alpha_true + delta_true * t + epsilon

# Cria um DataFrame
data = pd.DataFrame({'t': t, 'y': y})

# Adiciona uma constante ao modelo
X = sm.add_constant(data['t'])

# Ajusta o modelo de regress√£o linear com OLS
model = sm.OLS(data['y'], X)
results = model.fit()

# Exibe os resultados
print(results.summary())

# Intervalos de confian√ßa
confidence_intervals = results.conf_int(alpha=0.05)
print("\nIntervalos de Confian√ßa:")
print(confidence_intervals)


# Plot dos dados e da linha de regress√£o
plt.figure(figsize=(10, 6))
plt.scatter(data['t'], data['y'], label='Dados Observados', alpha=0.7)
plt.plot(data['t'], results.fittedvalues, color='red', label='Linha de Regress√£o')
plt.xlabel('Tempo (t)')
plt.ylabel('y')
plt.title('Regress√£o Linear com Tend√™ncia Temporal')
plt.legend()
plt.show()


# Calcula os res√≠duos
residuals = results.resid
# Cria um QQ-plot para os res√≠duos
plt.figure(figsize=(10, 6))
stats.probplot(residuals, dist="norm", plot=plt)
plt.title("QQ-plot dos Res√≠duos")
plt.show()
```
> Executando o c√≥digo, observamos que os estimadores de $\alpha$ e $\delta$ est√£o pr√≥ximos de seus valores verdadeiros (5 e 0.3, respectivamente). Os erros padr√£o de  $\hat{\alpha}$ e  $\hat{\delta}$ refletem as diferentes taxas de converg√™ncia e, assim como seus intervalos de confian√ßa, podem ser obtidos no resumo do modelo. Por exemplo, o erro padr√£o de $\hat{\alpha}$ ser√° aproximadamente $0.1$, e o erro padr√£o de $\hat{\delta}$ ser√° pr√≥ximo a $0.002$. O gr√°fico de dispers√£o mostra os dados gerados e a linha ajustada da regress√£o. Finalmente o QQ-plot serve para verificar a normalidade dos residuos. Isso confirma que o estimador de $\delta$ tem um erro padr√£o menor e portanto converge mais rapidamente, refletindo o resultado te√≥rico de que $\hat{\delta}$ √© superconsistente. Os intervalos de confian√ßa tamb√©m ser√£o mais estreitos para $\hat{\delta}$ comparado com $\hat{\alpha}$, dado o mesmo n√≠vel de signific√¢ncia.

**Lema 1.1:** A matriz de transforma√ß√£o $\mathbf{G}$ desempenha um papel crucial na obten√ß√£o da distribui√ß√£o assint√≥tica dos estimadores originais a partir dos estimadores transformados. Ela ajusta as taxas de converg√™ncia e permite a conex√£o entre os espa√ßos de par√¢metros transformados e originais.
*Prova (Esbo√ßo):* A rela√ß√£o $\mathbf{b} = \mathbf{G}'\mathbf{b}^*$ mostra que os estimadores originais s√£o uma transforma√ß√£o linear dos estimadores transformados. Como a distribui√ß√£o assint√≥tica de $\mathbf{b}^*$ √© conhecida, a matriz $\mathbf{G}$ permite determinar a distribui√ß√£o assint√≥tica de $\mathbf{b}$ atrav√©s das propriedades das transforma√ß√µes lineares de vari√°veis aleat√≥rias.

**Teorema 1.1:** (Taxas de Converg√™ncia e Distribui√ß√£o Assint√≥tica de $\hat{\alpha}$): O estimador $\hat{\alpha}$ converge a uma taxa de $\sqrt{T}$, e a sua distribui√ß√£o assint√≥tica √© gaussiana, com a taxa de converg√™ncia refletindo a vari√¢ncia do ru√≠do branco, conforme discutido em [16.3.17] [^1]. Formalmente, $\sqrt{T}(\hat{\alpha} - \alpha)$ converge para uma distribui√ß√£o normal.

*Prova (Esbo√ßo)*: Assim como para $\hat{\delta}$, $\hat{\alpha}$ √© uma combina√ß√£o linear de vari√°veis com diferentes taxas de converg√™ncia. No entanto, o termo dominante em $\hat{\alpha}$ √© o associado ao intercepto, que converge a uma taxa de $\sqrt{T}$. A prova detalhada envolve usar a forma fechada de $\hat{\alpha}$ obtida atrav√©s dos estimadores de MQO, juntamente com as propriedades assint√≥ticas do termo de erro, mostrando que $\hat{\alpha}-\alpha = O_p(T^{-1/2})$, portanto $\sqrt{T}(\hat{\alpha} - \alpha)$ converge em distribui√ß√£o para uma distribui√ß√£o normal.

*Prova Detalhada:*
I. Come√ßamos com o estimador de MQO para $\alpha$, dado por $\hat{\alpha} = \bar{y} - \hat{\delta}\bar{t}$, onde $\bar{y} = \frac{1}{T}\sum_{t=1}^T y_t$ e $\bar{t} = \frac{1}{T}\sum_{t=1}^T t$.
   
II.  Substituindo $y_t = \alpha + \delta t + \epsilon_t$, temos $\bar{y} = \alpha + \delta\bar{t} + \bar{\epsilon}$, onde $\bar{\epsilon} = \frac{1}{T}\sum_{t=1}^T \epsilon_t$.
   
III.  Substituindo $\bar{y}$ em $\hat{\alpha}$, obtemos $\hat{\alpha} = \alpha + \delta\bar{t} + \bar{\epsilon} - \hat{\delta}\bar{t}$, ou seja, $\hat{\alpha} - \alpha = \bar{\epsilon} - (\hat{\delta} - \delta)\bar{t}$.

IV. Sabemos que $\bar{\epsilon} = O_p(T^{-1/2})$ e que $\hat{\delta}-\delta=O_p(T^{-3/2})$. Al√©m disso,  $\bar{t} = O(T)$. Portanto, $(\hat{\delta} - \delta)\bar{t} = O_p(T^{-3/2})O(T) = O_p(T^{-1/2})$.

V. Portanto, $\hat{\alpha} - \alpha = O_p(T^{-1/2}) + O_p(T^{-1/2}) = O_p(T^{-1/2})$.

VI. Multiplicando por $\sqrt{T}$, temos $\sqrt{T}(\hat{\alpha} - \alpha) = \sqrt{T} O_p(T^{-1/2}) = O_p(1)$.
    
VII.  Isto implica que $\sqrt{T}(\hat{\alpha} - \alpha)$ converge para uma distribui√ß√£o normal. ‚ñ†

### Conclus√£o

A distribui√ß√£o assint√≥tica dos estimadores de MQO para o modelo original √© obtida por meio da transforma√ß√£o dos estimadores transformados [^1], [^2]. A transforma√ß√£o dos regressores permite que as componentes com diferentes taxas de converg√™ncia sejam isoladas. A distribui√ß√£o assint√≥tica de $\mathbf{b}$ √© inferida pela transforma√ß√£o linear da distribui√ß√£o assint√≥tica de $\mathbf{b}^*$.  A matriz $\mathbf{G}$ desempenha um papel essencial, conectando os coeficientes dos modelos original e transformado. A an√°lise da matriz $\mathbf{G}$ e da distribui√ß√£o assint√≥tica de $\mathbf{b}^*$ permite inferir as propriedades assint√≥ticas de $\mathbf{b}$ [^1], [^2]. Os resultados mostram que as infer√™ncias estat√≠sticas s√£o validas, ainda que os estimadores convirjam para os valores verdadeiros a diferentes taxas.

**Corol√°rio 1:** (Superconsist√™ncia e Testes de Hip√≥teses): O estimador $\hat{\delta}$ √© superconsistente, ou seja, converge para o valor verdadeiro a uma taxa superior a $\sqrt{T}$, o que implica que testes de hip√≥teses envolvendo $\delta$ podem ser conduzidos utilizando a distribui√ß√£o normal, e que a diferen√ßa entre o estimador e o valor verdadeiro, multiplicado por uma constante de escala, converge para um ponto [^1]. As infer√™ncias estat√≠sticas usuais, como testes t e F, s√£o assintoticamente v√°lidas. No entanto, √© preciso ter cautela em amostras pequenas, onde as aproxima√ß√µes assint√≥ticas podem n√£o ser precisas.

> üí° **Exemplo Num√©rico:** Ao analisar os resultados da simula√ß√£o anterior, nota-se que os erros padr√£o dos estimadores refletem as diferentes taxas de converg√™ncia. Os testes t e os intervalos de confian√ßa, embora assintoticamente v√°lidos, devem ser interpretados com cautela para amostras pequenas, como explicitado anteriormente no Corol√°rio 1. Por exemplo, ao testar a hip√≥tese nula de que $\delta = 0.3$, podemos usar o teste t:
> $$t = \frac{\hat{\delta} - 0.3}{se(\hat{\delta})}$$
> onde $se(\hat{\delta})$ √© o erro padr√£o do estimador de $\delta$. Sob a hip√≥tese nula, esse teste t segue uma distribui√ß√£o normal assintoticamente.  Se o valor calculado de $t$ estiver distante de zero (por exemplo, acima de 1.96 ou abaixo de -1.96 para um n√≠vel de signific√¢ncia de 5%), podemos rejeitar a hip√≥tese nula.  No entanto, se a amostra fosse muito pequena (por exemplo, T=20), a distribui√ß√£o do teste t poderia se desviar da normal, levando a conclus√µes erradas. Em geral, quanto maior a amostra, mais precisos ser√£o os resultados das infer√™ncias baseadas na distribui√ß√£o normal assint√≥tica. Com T=100 o teste t  resulta em um valor pr√≥ximo de 0, e como o valor-p √© pr√≥ximo de 1, a hip√≥tese nula n√£o √© rejeitada.

**Corol√°rio 1.1:** (Infer√™ncia Assint√≥tica para $\alpha$): Semelhantemente ao caso do $\hat{\delta}$, as infer√™ncias estat√≠sticas para $\hat{\alpha}$ s√£o v√°lidas assintoticamente, o que significa que os testes de hip√≥teses envolvendo $\alpha$ podem ser conduzidos utilizando a distribui√ß√£o normal assint√≥tica. A precis√£o dessas infer√™ncias √© baseada na taxa de converg√™ncia de $\sqrt{T}$ e melhoram com o tamanho amostral, ainda que seja menos rapida do que a convergencia de $\hat{\delta}$.

### Refer√™ncias
[^1]: Cap√≠tulo 16 do texto base: "Processes with Deterministic Time Trends".
[^2]: Cap√≠tulo 8 do texto base.
<!-- END -->
