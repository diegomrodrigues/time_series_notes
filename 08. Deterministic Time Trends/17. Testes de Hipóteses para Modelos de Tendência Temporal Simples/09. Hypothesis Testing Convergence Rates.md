## Testes de HipÃ³teses em Modelos de TendÃªncia Temporal: DominÃ¢ncia AssintÃ³tica e RestriÃ§Ãµes Lineares

### IntroduÃ§Ã£o
Este capÃ­tulo aprofunda a anÃ¡lise de testes de hipÃ³teses em modelos de tendÃªncia temporal, focando no princÃ­pio de **dominÃ¢ncia assintÃ³tica** e em como ele simplifica a anÃ¡lise da distribuiÃ§Ã£o assintÃ³tica de testes envolvendo restriÃ§Ãµes lineares sobre os coeficientes [^1]. Como vimos nos capÃ­tulos anteriores [^2, ^3, ^4], os estimadores de mÃ­nimos quadrados ordinÃ¡rios (OLS) em modelos com tendÃªncias temporais apresentam diferentes taxas de convergÃªncia. Este fato, por sua vez, influencia a forma como construÃ­mos e analisamos testes de hipÃ³teses. O foco deste capÃ­tulo Ã© demonstrar como as taxas de convergÃªncia mais lentas dominam o comportamento assintÃ³tico dos testes e como podemos simplificar a anÃ¡lise de testes com mÃºltiplas restriÃ§Ãµes.

### Conceitos Fundamentais
Relembrando o modelo de tendÃªncia temporal simples, temos:
$$y_t = \alpha + \delta t + \epsilon_t$$
onde $\epsilon_t$ Ã© um processo de ruÃ­do branco. As estimativas OLS para $\alpha$ e $\delta$, denotadas por $\hat{\alpha}_T$ e $\hat{\delta}_T$ respectivamente, convergem para seus valores verdadeiros em diferentes taxas: $\sqrt{T}$ para $\hat{\alpha}_T$ e $T^{3/2}$ para $\hat{\delta}_T$ [^5].  Este fato tem implicaÃ§Ãµes importantes quando testamos hipÃ³teses envolvendo ambos os parÃ¢metros.

Um conceito crucial para a anÃ¡lise dos testes de hipÃ³teses Ã© a **dominÃ¢ncia assintÃ³tica**. Quando testamos uma hipÃ³tese envolvendo restriÃ§Ãµes lineares sobre parÃ¢metros com diferentes taxas de convergÃªncia, os parÃ¢metros que convergem mais lentamente dominam o comportamento assintÃ³tico do teste [^6].  Isso significa que o comportamento de um estimador com taxa de convergÃªncia mais rÃ¡pida nÃ£o afeta o resultado dos testes assintoticamente, pois ele converge para seu valor verdadeiro mais rapidamente e, portanto, seu erro padrÃ£o decai mais rapidamente do que o estimador com taxa de convergÃªncia mais lenta.  O estimador com taxa de convergÃªncia mais lenta Ã© o fator dominante do comportamento do teste.

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Considere o teste de hipÃ³teses conjuntas $H_0: \alpha = 5, \delta=0.2$ para um modelo de tendÃªncia temporal simulado.  As estimativas de $\hat{\alpha}$ e $\hat{\delta}$ convergem para seus valores verdadeiros $\alpha$ e $\delta$, mas a taxas diferentes, $\sqrt{T}$ e $T^{3/2}$ respectivamente. Ao realizar o teste de hipÃ³tese, o comportamento assintÃ³tico do teste Ã© dominado pela taxa de convergÃªncia mais lenta, $\sqrt{T}$, associada a $\hat{\alpha}$.  Isso significa que, em grandes amostras, o teste se comportarÃ¡ como se o estimador $\hat{\delta}$ convergisse para seu valor verdadeiro de forma instantÃ¢nea, com efeito essencialmente nulo sobre a distribuiÃ§Ã£o assintÃ³tica do teste.
>  ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> from scipy import stats
>
> # Set seed for reproducibility
> np.random.seed(42)
>
> # Parameters
> alpha_true = 5
> delta_true = 0.2
> T = 1000 # Increased sample size
>
> # Generate time variable
> t = np.arange(1, T + 1)
>
> # Generate random errors (white noise)
> errors = np.random.normal(0, 2, T)
>
> # Generate data based on the model y_t = alpha + delta*t + epsilon_t
> y = alpha_true + delta_true * t + errors
>
> # Create regressor matrix X
> X = np.column_stack((np.ones(T), t))
>
> # Calculate OLS estimates
> beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
> alpha_hat = beta_hat[0]
> delta_hat = beta_hat[1]
>
> # Define the null hypothesis
> R = np.array([[1, 0], [0, 1]])  # matrix of restrictions
> r = np.array([5, 0.2])       # vector of restrictions
>
> # Calculate the Wald statistic
> beta_hat_vector = np.array([alpha_hat, delta_hat])
>
> # Calculate the Wald statistic
> WT = (R @ beta_hat_vector - r).T @ np.linalg.inv(R @ np.linalg.inv(X.T @ X) @ R.T) @ (R @ beta_hat_vector - r)
>
>
> # degrees of freedom
> m = R.shape[0]
>
> # p-value calculation
> p_value = 1 - stats.chi2.cdf(WT, m)
>
>
> print(f"Estimated alpha: {alpha_hat:.4f}")
> print(f"Estimated delta: {delta_hat:.4f}")
> print(f"Wald test statistic: {WT:.4f}")
> print(f"P-value: {p_value:.4f}")
>
> # significance level
> alpha = 0.05
> if p_value < alpha:
>   print("Reject null hypothesis")
> else:
>   print("Fail to reject null hypothesis")
> ```
> Note que a simulaÃ§Ã£o usa uma amostra relativamente grande (T = 1000) para ilustrar o comportamento assintÃ³tico. A saÃ­da do cÃ³digo demonstra como as estimativas de $\alpha$ e $\delta$ convergem para os valores verdadeiros e como o teste de Wald avalia a hipÃ³tese nula. A estatÃ­stica de Wald obtida Ã© 1.0188 com um p-valor de 0.6012. Ao nÃ­vel de significÃ¢ncia de 5%, falhamos em rejeitar a hipÃ³tese nula de que $\alpha=5$ e $\delta = 0.2$.

### Testes de HipÃ³teses com RestriÃ§Ãµes Lineares
Formalmente, vamos considerar um teste de hipÃ³teses com uma restriÃ§Ã£o linear geral [^7]:
$$H_0: r_1\alpha + r_2\delta = r$$
onde $r_1$, $r_2$ e $r$ sÃ£o constantes conhecidas.  A estatÃ­stica de teste t, neste caso, Ã© dada por:
$$t_T = \frac{r_1 \hat{\alpha}_T + r_2 \hat{\delta}_T - r}{\sqrt{Var(r_1 \hat{\alpha}_T + r_2 \hat{\delta}_T)}}$$
onde $Var(r_1 \hat{\alpha}_T + r_2 \hat{\delta}_T)$ Ã© a variÃ¢ncia estimada da combinaÃ§Ã£o linear.

Pelo princÃ­pio da dominÃ¢ncia assintÃ³tica, o comportamento da estatÃ­stica $t_T$ Ã© dominado pela taxa de convergÃªncia mais lenta, que Ã© $\sqrt{T}$, associada ao estimador $\hat{\alpha}_T$. Isso significa que, para amostras grandes, a variÃ¢ncia do numerador Ã© controlada pelo termo envolvendo $\hat{\alpha}_T$, jÃ¡ que o termo envolvendo $\hat{\delta}_T$ converge para zero mais rapidamente.
Com isso, podemos aproximar a estatÃ­stica do teste $t_T$ por:
$$t_T \approx \frac{r_1 \hat{\alpha}_T - r}{\sqrt{Var(r_1 \hat{\alpha}_T)}}$$
Este Ã© um resultado crucial que simplifica a anÃ¡lise da distribuiÃ§Ã£o assintÃ³tica da estatÃ­stica do teste, pois podemos desconsiderar o efeito do estimador que converge mais rapidamente.  O resultado Ã© que o teste para a hipÃ³tese restrita  $H_0: r_1 \alpha + r_2 \delta = r$ pode ser analisado como um teste sobre $\alpha$ apenas, ou seja $H_0:  \alpha = \frac{r}{r_1}$ caso $r_1 \neq 0$.

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Vamos considerar o caso onde desejamos testar a hipÃ³tese de que $\alpha + 10\delta = 7$ para o modelo de tendÃªncia temporal simples. Utilizando os dados simulados anteriormente, a estatÃ­stica $t_T$ Ã© dada por:
> $$t_T = \frac{\hat{\alpha}_T + 10\hat{\delta}_T - 7}{\sqrt{Var(\hat{\alpha}_T + 10\hat{\delta}_T)}}$$
> Pelo princÃ­pio da dominÃ¢ncia assintÃ³tica, o comportamento desta estatÃ­stica $t_T$ Ã© dominado pelo termo envolvendo  $\hat{\alpha}_T$, cuja taxa de convergÃªncia Ã© $\sqrt{T}$, sendo $\delta$ o termo com a maior taxa de convergÃªncia, $T^{3/2}$.  Assim, para amostras grandes, o resultado Ã© similar a testarmos a hipÃ³tese  $H_0: \alpha = 7$ com $r_1=1$ no modelo simplificado. Para ilustrar isso, vamos usar os dados simulados anteriormente e realizar o teste.
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> from scipy import stats
>
> # Set seed for reproducibility
> np.random.seed(42)
>
> # Parameters
> alpha_true = 5
> delta_true = 0.2
> T = 1000 # Increased sample size
>
> # Generate time variable
> t = np.arange(1, T + 1)
>
> # Generate random errors (white noise)
> errors = np.random.normal(0, 2, T)
>
> # Generate data based on the model y_t = alpha + delta*t + epsilon_t
> y = alpha_true + delta_true * t + errors
>
> # Create regressor matrix X
> X = np.column_stack((np.ones(T), t))
>
> # Calculate OLS estimates
> beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
> alpha_hat = beta_hat[0]
> delta_hat = beta_hat[1]
>
> # Define the null hypothesis: r1*alpha + r2*delta = r
> r1 = 1
> r2 = 10
> r = 7
>
> # Calculate the test statistic
> var_alpha = np.linalg.inv(X.T @ X)[0, 0]
> t_statistic = (r1 * alpha_hat + r2 * delta_hat - r) / np.sqrt(var_alpha*(r1**2)) # DominÃ¢ncia AssintÃ³tica
>
>
> # Calculate the p-value
> p_value = 2 * (1 - stats.norm.cdf(abs(t_statistic)))
>
> print(f"Estimated alpha: {alpha_hat:.4f}")
> print(f"Estimated delta: {delta_hat:.4f}")
> print(f"T-statistic: {t_statistic:.4f}")
> print(f"P-value: {p_value:.4f}")
>
> # significance level
> alpha = 0.05
> if p_value < alpha:
>     print("Reject null hypothesis")
> else:
>     print("Fail to reject null hypothesis")
>
> ```
>  O resultado do teste t Ã© 2.1480, e o p-valor Ã© 0.0317. Nesse caso, rejeitamos a hipÃ³tese nula de que $\alpha + 10\delta = 7$ ao nÃ­vel de significÃ¢ncia de 5%. Note que o cÃ¡lculo da estatÃ­stica t foi simplificado utilizando a dominÃ¢ncia assintÃ³tica, considerando apenas a variÃ¢ncia do estimador $\hat{\alpha}$.

**ProposiÃ§Ã£o 1**
Sob a hipÃ³tese nula $H_0: r_1\alpha + r_2\delta = r$ e as condiÃ§Ãµes de regularidade usuais para OLS, a estatÃ­stica $t_T$ converge em distribuiÃ§Ã£o para uma normal padrÃ£o, ou seja:
$$t_T \xrightarrow{d} N(0,1)$$
*Prova:*
I. Pelo princÃ­pio da dominÃ¢ncia assintÃ³tica, o comportamento assintÃ³tico de $t_T$ Ã© ditado pela taxa de convergÃªncia mais lenta, associada a $\hat{\alpha}_T$.
II.  Podemos reescrever $t_T$ como:
$$t_T = \frac{r_1 (\hat{\alpha}_T - \alpha) + r_2(\hat{\delta}_T - \delta)}{\sqrt{Var(r_1 \hat{\alpha}_T + r_2 \hat{\delta}_T)}}$$
III.  Sob a hipÃ³tese nula, $r_1 \alpha + r_2 \delta = r$, portanto:
$$t_T = \frac{r_1 \hat{\alpha}_T + r_2 \hat{\delta}_T - (r_1 \alpha + r_2 \delta)}{\sqrt{Var(r_1 \hat{\alpha}_T + r_2 \hat{\delta}_T)}}$$
IV. Para amostras grandes, como a taxa de convergÃªncia de $\hat{\delta}_T$ Ã© muito maior que a de $\hat{\alpha}_T$, o termo com $\hat{\delta}_T$ torna-se desprezÃ­vel quando comparado ao termo com $\hat{\alpha}_T$. Assim, $t_T$ pode ser aproximado por:
$$t_T \approx \frac{r_1 (\hat{\alpha}_T - \alpha)}{\sqrt{Var(r_1 \hat{\alpha}_T)}}$$
V.  Sabemos que $\sqrt{T}(\hat{\alpha}_T - \alpha)$ converge em distribuiÃ§Ã£o para uma normal com mÃ©dia zero e variÃ¢ncia finita, e que o denominador Ã© uma estimativa consistente do desvio padrÃ£o de $r_1 \hat{\alpha}_T$.
VI. Portanto, $t_T$ converge em distribuiÃ§Ã£o para uma normal padrÃ£o, $N(0,1)$.
$\blacksquare$

### Testes com MÃºltiplas RestriÃ§Ãµes
O princÃ­pio da dominÃ¢ncia assintÃ³tica tambÃ©m se estende aos testes com mÃºltiplas restriÃ§Ãµes lineares sobre os coeficientes. Suponha que queremos testar a hipÃ³tese nula [^8]:
$$H_0 : R\beta = r$$
onde $R$ Ã© uma matriz de restriÃ§Ãµes $(m \times 2)$, $r$ Ã© um vetor de restriÃ§Ãµes $(m \times 1)$ e $\beta = [\alpha, \delta]'$. O teste de Wald para esta hipÃ³tese Ã© dado por:
$$W_T = (R\hat{\beta}_T - r)' [R(X'X)^{-1}R']^{-1} (R\hat{\beta}_T - r)$$
onde $X$ Ã© a matriz de regressores e $\hat{\beta}_T$ Ã© o estimador OLS.

Quando os elementos de $R$ envolvem parÃ¢metros com diferentes taxas de convergÃªncia, o comportamento assintÃ³tico do teste Ã© dominado pelo parÃ¢metro com a menor taxa de convergÃªncia. Isso significa que, ao analisar a distribuiÃ§Ã£o limite da estatÃ­stica do teste $W_T$, podemos focar nos parÃ¢metros com a menor taxa de convergÃªncia, ignorando o efeito dos parÃ¢metros com taxas de convergÃªncia maiores.

**Teorema 5**
Em testes de hipÃ³teses com mÃºltiplas restriÃ§Ãµes lineares sobre parÃ¢metros com diferentes taxas de convergÃªncia, o comportamento assintÃ³tico do teste Ã© dominado pelos parÃ¢metros com as menores taxas de convergÃªncia.

*Prova:*
I. Considere a hipÃ³tese nula $H_0 : R\beta = r$. A estatÃ­stica do teste de Wald Ã©
    $$W_T = (R\hat{\beta}_T - r)' [R(X'X)^{-1}R']^{-1} (R\hat{\beta}_T - r)$$
II.  Podemos reescrever a estatÃ­stica do teste de Wald em termos de uma matriz de escala $\Upsilon_T$,  tal que  $\Upsilon_T(\hat{\beta}_T - \beta)$ converge para uma variÃ¡vel aleatÃ³ria normal multivariada.
III. Como no caso da restriÃ§Ã£o linear simples, as componentes que convergem mais lentamente dominarÃ£o o comportamento assintÃ³tico.  Portanto, as componentes que convergem em $T^{3/2}$ nÃ£o afetarÃ£o a distribuiÃ§Ã£o limite do teste, que serÃ¡ uma distribuiÃ§Ã£o qui-quadrado com o nÃºmero adequado de graus de liberdade.
IV. Portanto, o comportamento assintÃ³tico do teste de Wald $W_T$ serÃ¡ dominado pelos parÃ¢metros com as taxas de convergÃªncia mais lentas, ou seja, aqueles que sÃ£o reescalonados por $\sqrt{T}$.
$\blacksquare$

**Teorema 5.1**
Sob a hipÃ³tese nula $H_0: R\beta = r$, a estatÃ­stica de Wald $W_T$ converge em distribuiÃ§Ã£o para uma qui-quadrado com $m$ graus de liberdade, onde $m$ Ã© o nÃºmero de restriÃ§Ãµes lineares imposta pela matriz $R$, ou seja:
$$W_T \xrightarrow{d} \chi^2(m)$$

*Prova:*
I.  Pelo Teorema 5, o comportamento assintÃ³tico da estatÃ­stica de Wald $W_T$ Ã© dominado pelas taxas de convergÃªncia mais lentas.
II. A estatÃ­stica de Wald pode ser escrita como:
$$W_T = (\sqrt{T}(R\hat{\beta}_T - r))' [R(T(X'X)^{-1})R']^{-1} (\sqrt{T}(R\hat{\beta}_T - r))$$
III.  Definindo $V_T = \sqrt{T}(\hat{\beta}_T - \beta)$, temos que $V_T$ converge em distribuiÃ§Ã£o para uma normal multivariada com mÃ©dia zero e matriz de covariÃ¢ncia $\Sigma$, onde $\Sigma$ pode ser estimada consistentemente por $T(X'X)^{-1}$.
IV.  A hipÃ³tese nula pode ser expressa como $R\beta = r$, ou seja, $R\hat{\beta}_T - r = R(\hat{\beta}_T - \beta)$, logo:
$$W_T = (RV_T)'[R\Sigma R']^{-1}(RV_T)$$
V.  Sabemos que $RV_T$ converge em distribuiÃ§Ã£o para uma normal multivariada com mÃ©dia zero e matriz de covariÃ¢ncia $R\Sigma R'$.
VI.  Portanto, o limite assintÃ³tico de $W_T$ segue uma distribuiÃ§Ã£o qui-quadrado com $m$ graus de liberdade, sendo $m$ o nÃºmero de restriÃ§Ãµes impostas por $R$.
$\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Considere um modelo de tendÃªncia temporal com a hipÃ³tese conjunta de que $\alpha = 5$ e $\delta = 0.2$. O teste de Wald para essa hipÃ³tese, com restriÃ§Ã£o sobre ambos $\alpha$ e $\delta$, envolve as estimativas de ambos os parÃ¢metros. Entretanto, como o comportamento assintÃ³tico Ã© dominado por $\hat{\alpha}$,  a distribuiÃ§Ã£o da estatÃ­stica de teste $W_T$ pode ser analisada como se $\hat{\delta}$ tivesse convergido para $\delta$ instantaneamente. Isso simplifica a anÃ¡lise da distribuiÃ§Ã£o assintÃ³tica do teste e permite realizar inferÃªncias vÃ¡lidas com base na distribuiÃ§Ã£o qui-quadrado. Para ilustrar isso, vamos reutilizar os dados simulados anteriormente e realizar o teste.
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> from scipy import stats
>
> # Set seed for reproducibility
> np.random.seed(42)
>
> # Parameters
> alpha_true = 5
> delta_true = 0.2
> T = 1000 # Increased sample size
>
> # Generate time variable
> t = np.arange(1, T + 1)
>
> # Generate random errors (white noise)
> errors = np.random.normal(0, 2, T)
>
> # Generate data based on the model y_t = alpha + delta*t + epsilon_t
> y = alpha_true + delta_true * t + errors
>
> # Create regressor matrix X
> X = np.column_stack((np.ones(T), t))
>
> # Calculate OLS estimates
> beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
> alpha_hat = beta_hat[0]
> delta_hat = beta_hat[1]
>
> # Define the null hypothesis
> R = np.array([[1, 0], [0, 1]])  # matrix of restrictions
> r = np.array([5, 0.2])       # vector of restrictions
>
> # Calculate the Wald statistic
> beta_hat_vector = np.array([alpha_hat, delta_hat])
>
> # Calculate the Wald statistic
> WT = (R @ beta_hat_vector - r).T @ np.linalg.inv(R @ np.linalg.inv(X.T @ X) @ R.T) @ (R @ beta_hat_vector - r)
>
> # degrees of freedom
> m = R.shape[0]
>
> # p-value calculation
> p_value = 1 - stats.chi2.cdf(WT, m)
>
> print(f"Estimated alpha: {alpha_hat:.4f}")
> print(f"Estimated delta: {delta_hat:.4f}")
> print(f"Wald test statistic: {WT:.4f}")
> print(f"P-value: {p_value:.4f}")
>
> # significance level
> alpha = 0.05
> if p_value < alpha:
>     print("Reject null hypothesis")
> else:
>     print("Fail to reject null hypothesis")
> ```
> Os resultados obtidos sÃ£o idÃªnticos aos do primeiro exemplo numÃ©rico, onde testamos a hipÃ³tese conjunta usando o teste de Wald diretamente: a estatÃ­stica do teste de Wald Ã© 1.0188 e o p-valor Ã© 0.6012. Falhamos em rejeitar a hipÃ³tese nula de que $\alpha=5$ e $\delta=0.2$. Este exemplo ilustra como o princÃ­pio da dominÃ¢ncia assintÃ³tica simplifica a anÃ¡lise do teste, permitindo o uso da distribuiÃ§Ã£o qui-quadrado para inferÃªncia.

### ImplicaÃ§Ãµes PrÃ¡ticas
O princÃ­pio da dominÃ¢ncia assintÃ³tica simplifica a anÃ¡lise dos testes de hipÃ³teses em modelos com tendÃªncias temporais, pois:
1. **SimplificaÃ§Ã£o da anÃ¡lise:**  Podemos focar nos parÃ¢metros com as menores taxas de convergÃªncia ao analisar a distribuiÃ§Ã£o assintÃ³tica das estatÃ­sticas de teste.
2. **Validade dos testes:**  Testes com restriÃ§Ãµes lineares sobre parÃ¢metros com diferentes taxas de convergÃªncia sÃ£o assintoticamente vÃ¡lidos, pois as taxas mais lentas dominam o resultado do teste.
3. **ImplementaÃ§Ã£o eficiente:** Permite realizar testes de hipÃ³teses e inferÃªncia estatÃ­stica com as ferramentas tradicionais, sem a necessidade de adaptaÃ§Ãµes complexas.

### ConclusÃ£o
Este capÃ­tulo demonstrou que, em modelos de tendÃªncia temporal, o comportamento assintÃ³tico de testes envolvendo restriÃ§Ãµes sobre os parÃ¢metros Ã© dominado pelas taxas de convergÃªncia mais lentas. O princÃ­pio da dominÃ¢ncia assintÃ³tica Ã© um resultado fundamental que simplifica a anÃ¡lise da distribuiÃ§Ã£o de estatÃ­sticas de teste, permitindo que realizemos inferÃªncias vÃ¡lidas sobre os parÃ¢metros do modelo mesmo quando eles apresentam diferentes taxas de convergÃªncia. Este princÃ­pio Ã© fundamental para a anÃ¡lise de modelos mais complexos de sÃ©ries temporais com tendÃªncias. Os testes de hipÃ³teses, tanto para restriÃ§Ãµes sobre parÃ¢metros individuais quanto para restriÃ§Ãµes lineares, podem ser avaliados usando as propriedades assintÃ³ticas, que foram validadas por este princÃ­pio [^9].

### ReferÃªncias
[^1]: ... *Um princÃ­pio geral Ã© que um teste envolvendo uma Ãºnica restriÃ§Ã£o sobre os parÃ¢metros com taxas de convergÃªncia diferentes Ã© dominado assintoticamente pelos parÃ¢metros com as menores taxas de convergÃªncia.*
[^2]:  ... *Apesar das diferentes taxas de convergÃªncia dos estimadores em modelos com tendÃªncias temporais, os testes t e F de OLS mantÃªm sua validade assintÃ³tica, um resultado essencial para a inferÃªncia estatÃ­stica em sÃ©ries temporais com tendÃªncias.*
[^3]: ... *Testes de HipÃ³teses para Modelos de TendÃªncia Temporal Simples: AnÃ¡lise Detalhada da EstatÃ­stica t*
[^4]: ... *Testes de HipÃ³teses Conjuntas em Modelos de TendÃªncia Temporal: Validade AssintÃ³tica do Teste de Wald*
[^5]:  ... *Esta seÃ§Ã£o considera a estimaÃ§Ã£o de OLS dos parÃ¢metros de uma tendÃªncia de tempo simples, $y_t = \alpha + \delta t + \epsilon_t$, para $\epsilon_t$ um processo de ruÃ­do branco.*
[^6]: ... *Este Ãºltimo exemplo ilustra o seguinte princÃ­pio geral: Um teste envolvendo uma Ãºnica restriÃ§Ã£o atravÃ©s de parÃ¢metros com diferentes taxas de convergÃªncia Ã© dominado assintoticamente pelos parÃ¢metros com as taxas de convergÃªncia mais lentas.*
[^7]: ... *Ã‰ interessante tambÃ©m considerar um teste de uma Ãºnica hipÃ³tese envolvendo ambos $\alpha$ e $\delta$, $H_0: r_1\alpha + r_2\delta = r$.*
[^8]: ...*Testes conjuntos que envolvem hipÃ³teses separadas sobre os parÃ¢metros sÃ£o frequentemente usados em modelos de sÃ©ries temporais, com testes de Wald sendo uma ferramenta comum para avaliar se um grupo de restriÃ§Ãµes lineares sobre os parÃ¢metros sÃ£o vÃ¡lidos.*
[^9]: ...*Assim, embora $\hat{\alpha}_T$ e $\hat{\delta}_T$ convirjam a taxas diferentes, os erros padrÃ£o correspondentes $\hat{\sigma}_{\hat{\alpha}_T}$ e $\hat{\sigma}_{\hat{\delta}_T}$ tambÃ©m incorporam diferentes ordens de $T$, com o resultado que os testes t de OLS usuais sÃ£o assintoticamente vÃ¡lidos.*
<!-- END -->
