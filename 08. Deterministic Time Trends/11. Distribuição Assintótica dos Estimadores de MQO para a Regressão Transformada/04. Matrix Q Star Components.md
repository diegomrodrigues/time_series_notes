## Distribui√ß√£o Assint√≥tica dos Estimadores de MQO para a Regress√£o Transformada

### Introdu√ß√£o
Como vimos anteriormente, no cap√≠tulo sobre processos com tend√™ncias temporais determin√≠sticas, os estimadores de M√≠nimos Quadrados Ordin√°rios (MQO) para modelos com tend√™ncias temporais apresentam diferentes taxas de converg√™ncia [^1]. Enquanto estimadores em modelos com vari√°veis estacion√°rias geralmente convergem a uma taxa de $\sqrt{T}$, coeficientes associados a tend√™ncias temporais determin√≠sticas podem convergir mais rapidamente. Em particular, discutimos a necessidade de reescalonar as vari√°veis para acomodar essas diferentes taxas de converg√™ncia [^1]. Este cap√≠tulo expande esses conceitos, explorando a distribui√ß√£o assint√≥tica dos estimadores de MQO para uma regress√£o transformada, com foco em processos autorregressivos ao redor de uma tend√™ncia temporal determin√≠stica.

### Conceitos Fundamentais

A se√ß√£o anterior [^1] introduziu a ideia de diferentes taxas de converg√™ncia para estimadores em modelos com tend√™ncias temporais. Em particular, foi demonstrado que para um modelo simples de tend√™ncia temporal, o estimador do coeficiente da tend√™ncia ($\delta_T$) converge a uma taxa de $T^{3/2}$, enquanto o estimador do intercepto ($\hat{\alpha}_T$) converge a uma taxa de $\sqrt{T}$ [^1]. A transforma√ß√£o de vari√°veis proposta por Sims, Stock e Watson (1990) [^1, ^2] visa isolar essas diferentes taxas de converg√™ncia, facilitando a deriva√ß√£o da distribui√ß√£o assint√≥tica dos estimadores.

Para um processo autorregressivo geral ao redor de uma tend√™ncia temporal determin√≠stica, dado por:
$$y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t$$ [^3, 16.3.1],
a transforma√ß√£o proposta reescreve o modelo na forma:
$$y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \ldots + \phi_p^* y_{t-p}^* + \epsilon_t$$ [^3, 16.3.3],
onde os novos regressores $y_{t-j}^*$ representam vari√°veis com m√©dia zero, e $\alpha^*$ e $\delta^*$ s√£o coeficientes transformados [^3]. O objetivo dessa transforma√ß√£o √© separar os componentes do vetor de coeficientes que apresentam diferentes taxas de converg√™ncia.

> üí° **Exemplo Num√©rico:**
> Considere um modelo AR(1) com tend√™ncia temporal: $y_t = 2 + 0.5t + 0.7y_{t-1} + \epsilon_t$. Ap√≥s a transforma√ß√£o de Sims, Stock e Watson, podemos ter um modelo como: $y_t = \alpha^* + \delta^*t + \phi_1^*y_{t-1}^* + \epsilon_t$, onde $\alpha^*$, $\delta^*$ e $\phi_1^*$ s√£o os coeficientes transformados e $y_{t-1}^*$ √© a vari√°vel defasada transformada. A transforma√ß√£o garante que as vari√°veis $y_{t-1}^*$ tenham m√©dia zero.  Suponha que ap√≥s a transforma√ß√£o, os valores estimados para os coeficientes sejam: $\hat{\alpha}^*=1.5$, $\hat{\delta}^*=0.4$ e $\hat{\phi}_1^*=0.6$. Este √© um exemplo ilustrativo de como o modelo √© reescrito e os par√¢metros s√£o transformados para facilitar a an√°lise assint√≥tica.

**Observa√ß√£o 1:** A transforma√ß√£o que leva de $y_t$ para $y_t^*$ n√£o √© √∫nica. Em geral, qualquer transforma√ß√£o que resulte em regressores com m√©dia zero e que preserve a estrutura de depend√™ncia temporal dos dados √© adequada para essa an√°lise. O ponto crucial √© que a transforma√ß√£o deve ser escolhida de forma que os estimadores dos par√¢metros transformados apresentem taxas de converg√™ncia assint√≥ticas distintas, facilitando a an√°lise.

A matriz de transforma√ß√£o $G'$ ([^3, 16.3.8]) √© utilizada para obter os coeficientes transformados $\beta^*$ a partir dos coeficientes originais $\beta$ [^3, 16.3.7]:

$$\beta^* = (G')^{-1}\beta$$

O estimador dos coeficientes transformados, $b^*$, √© obtido por meio de uma regress√£o de m√≠nimos quadrados ordin√°rios (MQO) de $y_t$ em $x_t^*$ [^3, 16.3.7], onde $x_t^*$ √© um vetor de regressores transformados [^3, 16.3.9].

O Ap√™ndice 16.A [^3] demonstra que a distribui√ß√£o assint√≥tica de $b^*$ √© dada por:
$$Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$$ [^3, 16.3.13]

A matriz $Y_T$ √© definida como uma matriz diagonal com elementos $\sqrt{T}$ para os primeiros $p$ elementos, correspondentes aos coeficientes das vari√°veis defasadas $y_{t-j}^*$  e do intercepto $\alpha^*$, e $T^{3/2}$ para o elemento correspondente ao coeficiente da tend√™ncia temporal $\delta^*$, e zeros nas outras posi√ß√µes [^3, 16.3.14]. Esta matriz √© crucial para expressar as diferentes taxas de converg√™ncia dos par√¢metros. A matriz de covari√¢ncia assint√≥tica dos estimadores transformados √© dada por $\sigma^2 [Q^*]^{-1}$, onde $Q^*$ √© a matriz limite das vari√°veis transformadas.

> üí° **Exemplo Num√©rico:**
> Suponha que temos $T=100$ observa√ß√µes. A matriz $Y_T$ para um modelo com intercepto, tend√™ncia e um termo AR(1) seria:
>
> $$ Y_T = \begin{bmatrix} \sqrt{100} & 0 & 0 \\ 0 & \sqrt{100} & 0 \\ 0 & 0 & 100^{3/2} \end{bmatrix} = \begin{bmatrix} 10 & 0 & 0 \\ 0 & 10 & 0 \\ 0 & 0 & 1000 \end{bmatrix} $$
>
> Aqui, a diagonal mostra as taxas de converg√™ncia para o intercepto, o coeficiente AR(1) e a tend√™ncia temporal, respectivamente. Note que a taxa de converg√™ncia da tend√™ncia temporal (1000) √© muito maior do que a taxa dos outros dois par√¢metros (10). Isso reflete o fato de que o estimador da tend√™ncia converge mais rapidamente.
>
> Para ilustrar ainda mais, suponha que tenhamos um vetor de estimadores transformados $b^* = [1.2, 0.8, 0.1]$ (intercepto, AR(1), tend√™ncia). A matriz $Y_T$ escala cada um desses estimadores conforme sua taxa de converg√™ncia. Assim, $Y_T b^* = [10*1.2, 10*0.8, 1000*0.1] = [12, 8, 100]$.  Esta opera√ß√£o √© crucial para a an√°lise assint√≥tica, pois ao escalar, estamos lidando com vari√°veis que t√™m a mesma ordem de magnitude assintoticamente, o que permite derivar resultados estat√≠sticos consistentes.
>
>  ```python
>  import numpy as np
>
>  T = 100
>  YT = np.diag([np.sqrt(T), np.sqrt(T), T**(3/2)])
>  print("YT:\n", YT)
>  b_star = np.array([1.2, 0.8, 0.1])
>  YT_b_star = np.dot(YT, b_star)
>  print("YT * b*:\n", YT_b_star)
>  ```

**Lema 1:** A matriz $Y_T$, definida como uma matriz diagonal com elementos $\sqrt{T}$ para os coeficientes associados √†s vari√°veis estacion√°rias e $T^{3/2}$ para o coeficiente da tend√™ncia temporal, desempenha um papel fundamental na determina√ß√£o da distribui√ß√£o assint√≥tica dos estimadores. Esta matriz garante que os termos correspondentes aos coeficientes que convergem mais rapidamente sejam amplificados, enquanto os termos correspondentes aos coeficientes que convergem mais lentamente sejam mantidos em uma escala compar√°vel.

√â importante observar que, embora os estimadores transformados $b^*$ sejam uma fun√ß√£o dos coeficientes originais $b$, a distribui√ß√£o assint√≥tica de $b$ pode ser recuperada atrav√©s da rela√ß√£o $b = G'b^*$ [^3, 16.3.12]. A transforma√ß√£o para isolar as diferentes taxas de converg√™ncia nos permite obter a distribui√ß√£o assint√≥tica da regress√£o transformada, o que, por sua vez, fornece a distribui√ß√£o assint√≥tica dos coeficientes originais.

**Lema 1.1:**  A inversa da matriz de transforma√ß√£o $G$, denotada por $G = (G')^{-1}$, √© igualmente crucial para recuperar os coeficientes originais a partir dos coeficientes transformados.  A estrutura espec√≠fica de $G$ depende da transforma√ß√£o particular utilizada, mas sua finalidade √© desfazer as mudan√ßas de escala e centraliza√ß√£o introduzidas pela transforma√ß√£o de Sims, Stock e Watson.

A an√°lise detalhada no Ap√™ndice 16.A [^3] revela que:
* Os coeficientes dos termos autorregressivos ($\phi_i^*$) convergem a uma taxa de $\sqrt{T}$.
* O coeficiente da tend√™ncia temporal ($Œ¥^*$) converge a uma taxa de $T^{3/2}$.
* O intercepto ($\alpha^*$) tamb√©m converge a uma taxa de $\sqrt{T}$.

Este resultado generaliza o resultado obtido para o modelo simples de tend√™ncia temporal na se√ß√£o anterior [^1], onde se verificou que $\hat{\alpha}_T$ convergeva a $\sqrt{T}$ e $\delta_T$ a $T^{3/2}$ [^1].

**Teorema 1:** (Distribui√ß√£o Assint√≥tica dos Estimadores Originais) Dado que $\beta^* = (G')^{-1}\beta$ e $b = G'b^*$, e que a distribui√ß√£o assint√≥tica de $b^*$ √© dada por $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$, ent√£o a distribui√ß√£o assint√≥tica de $b$ √© dada por:
$Y_T(G'b^* - G'\beta^*) \xrightarrow{d} N(0, \sigma^2 G'[Q^*]^{-1}G)$
onde $G = (G')^{-1}$.

*Prova:* Provaremos que, dado que $\beta^* = (G')^{-1}\beta$ e $b = G'b^*$, e que a distribui√ß√£o assint√≥tica de $b^*$ √© dada por $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$, ent√£o a distribui√ß√£o assint√≥tica de $b$ √© dada por:
$Y_T(G'b^* - G'\beta^*) \xrightarrow{d} N(0, \sigma^2 G'[Q^*]^{-1}G)$, onde $G = (G')^{-1}$.

I.  Come√ßamos com a distribui√ß√£o assint√≥tica de $b^*$:
    $$Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$$

II. Multiplicamos ambos os lados da express√£o por $G'$:
    $$G'Y_T(b^* - \beta^*) \xrightarrow{d} G'N(0, \sigma^2 [Q^*]^{-1})$$
    Note que $Y_T$ √© uma matriz diagonal e, portanto, pode ser movida atrav√©s da multiplica√ß√£o da matriz $G'$.

III. Distribu√≠mos $G'$ dentro dos par√™nteses:
    $$Y_T(G'b^* - G'\beta^*) \xrightarrow{d} N(0, G'\sigma^2 [Q^*]^{-1}G)$$

IV.  Sabemos que $b = G'b^*$ e $\beta = G'\beta^*$. Substituindo na express√£o:
$$Y_T(b - \beta) \xrightarrow{d} N(0, \sigma^2 G'[Q^*]^{-1}G)$$

V. Portanto, a distribui√ß√£o assint√≥tica de $b$ √© dada por:
    $$Y_T(b - \beta) \xrightarrow{d} N(0, \sigma^2 G'[Q^*]^{-1}G)$$‚ñ†

> üí° **Exemplo Num√©rico:**
> Vamos supor que, em uma regress√£o com um intercepto, uma tend√™ncia e um termo AR(1), estimamos os coeficientes transformados $b^* = [\hat{\alpha}^*, \hat{\phi}_1^*, \hat{\delta}^*]^T = [1.5, 0.6, 0.4]^T$. A matriz de transforma√ß√£o inversa $G$ (que √© o inverso de $G'$), pode ser usada para recuperar os coeficientes originais $b$. Suponha que $G'$ tenha sido calculada como:
> $$ G' = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -5 & 0 & 1 \end{bmatrix} $$
> Portanto, $G$ √© o inverso de $G'$, que pode ser calculado como:
>  $$ G = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 5 & 0 & 1 \end{bmatrix} $$
>
> Os coeficientes originais s√£o obtidos por: $b = G'b^*$. Para o exemplo, $b = [1.5, 0.6, -5*1.5+0.4]^T = [1.5, 0.6, -7.1]^T$. Contudo, √© importante notar que este valor √© ilustrativo e, na pr√°tica, os valores da transforma√ß√£o e seus inversos seriam calculados com base na s√©rie temporal utilizada.
>
> Para a matriz de covari√¢ncia assint√≥tica, suponhamos que $\sigma^2 = 0.2$ e $[Q^*]^{-1}$ √© estimado como:
> $$ [Q^*]^{-1} = \begin{bmatrix} 0.1 & 0.02 & 0.01 \\ 0.02 & 0.05 & 0.005 \\ 0.01 & 0.005 & 0.02 \end{bmatrix} $$
> Ent√£o, a matriz de covari√¢ncia assint√≥tica dos estimadores originais √©:
> $\sigma^2 G'[Q^*]^{-1}G = 0.2 * G' [Q^*]^{-1} G$
> onde $G = (G')^{-1}$.
>
> Calculando:
>  $$ G'[Q^*]^{-1} =  \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -5 & 0 & 1 \end{bmatrix} \begin{bmatrix} 0.1 & 0.02 & 0.01 \\ 0.02 & 0.05 & 0.005 \\ 0.01 & 0.005 & 0.02 \end{bmatrix} = \begin{bmatrix} 0.1 & 0.02 & 0.01 \\ 0.02 & 0.05 & 0.005 \\ -0.49 & -0.095 & -0.03 \end{bmatrix} $$
>
> $$ G'[Q^*]^{-1}G =  \begin{bmatrix} 0.1 & 0.02 & 0.01 \\ 0.02 & 0.05 & 0.005 \\ -0.49 & -0.095 & -0.03 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 5 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 0.15 & 0.02 & 0.01 \\ 0.045 & 0.05 & 0.005 \\ -0.64 & -0.095 & -0.03 \end{bmatrix} $$
>
> Portanto, a matriz de covari√¢ncia assint√≥tica √©:
>
> $0.2 *  \begin{bmatrix} 0.15 & 0.02 & 0.01 \\ 0.045 & 0.05 & 0.005 \\ -0.64 & -0.095 & -0.03 \end{bmatrix} =  \begin{bmatrix} 0.03 & 0.004 & 0.002 \\ 0.009 & 0.01 & 0.001 \\ -0.128 & -0.019 & -0.006 \end{bmatrix} $
>
> Esta matriz cont√©m informa√ß√µes sobre as vari√¢ncias e covari√¢ncias dos estimadores originais, permitindo infer√™ncia estat√≠stica.
>
> ```python
> import numpy as np
>
> # Matrizes G' e G
> G_prime = np.array([[1, 0, 0], [0, 1, 0], [-5, 0, 1]])
> G = np.array([[1, 0, 0], [0, 1, 0], [5, 0, 1]])
>
> # Vetor de coeficientes transformados
> b_star = np.array([1.5, 0.6, 0.4])
>
> # Calculando os coeficientes originais
> b = np.dot(G_prime, b_star)
> print("Coeficientes originais b:\n", b)
>
> # Matriz inversa Q*
> Q_star_inv = np.array([[0.1, 0.02, 0.01], [0.02, 0.05, 0.005], [0.01, 0.005, 0.02]])
> sigma2 = 0.2
>
> # Calculando a matriz de covari√¢ncia assint√≥tica
> cov_matrix = sigma2 * np.dot(G_prime, np.dot(Q_star_inv, G))
> print("Matriz de covari√¢ncia assint√≥tica:\n", cov_matrix)
> ```

**Corol√°rio 1.1:** A matriz $G'[Q^*]^{-1}G$ no Teorema 1 representa a matriz de covari√¢ncia assint√≥tica dos estimadores originais $b$ ap√≥s o devido ajuste pelas taxas de converg√™ncia diferenciadas expressas em $Y_T$. O conhecimento dessa matriz permite infer√™ncias estat√≠sticas sobre os par√¢metros originais $\beta$.

**Proposi√ß√£o 1:** A transforma√ß√£o de Sims, Stock e Watson n√£o apenas simplifica a an√°lise assint√≥tica, mas tamb√©m permite que a distribui√ß√£o dos estimadores seja abordada de maneira mais precisa, especialmente em modelos com tend√™ncias temporais determin√≠sticas. A chave √© a utiliza√ß√£o da matriz $Y_T$ para equilibrar as diferentes taxas de converg√™ncia.

**Teorema 1.1** (Consist√™ncia dos Estimadores) Sob as condi√ß√µes usuais de regularidade para estimadores de MQO e a transforma√ß√£o de Sims, Stock e Watson, os estimadores originais $b$ s√£o consistentes, ou seja,  $b \xrightarrow{p} \beta$ quando $T \to \infty$.

*Prova*: Sabemos que $b = G'b^*$. Pela consist√™ncia dos estimadores transformados, temos que $b^* \xrightarrow{p} \beta^*$. Como $G'$ √© uma matriz de transforma√ß√£o fixa, a consist√™ncia √© preservada pela multiplica√ß√£o por $G'$, e assim, $G'b^* \xrightarrow{p} G'\beta^*$. Substituindo as rela√ß√µes $b=G'b^*$ e $\beta=G'\beta^*$, obtemos que $b \xrightarrow{p} \beta$.
‚ñ†

> üí° **Exemplo Num√©rico:**
> Considere um cen√°rio simulado onde os verdadeiros coeficientes de um modelo com tend√™ncia temporal s√£o $\beta = [2, 0.7, 0.3]^T$ (intercepto, AR(1), tend√™ncia). Realizamos simula√ß√µes com tamanhos de amostra crescentes ($T = 100, 500, 1000, 5000$) e estimamos os coeficientes usando a transforma√ß√£o de Sims, Stock e Watson. Para cada amostra, obtemos estimativas $b$. Podemos observar que, √† medida que $T$ aumenta, os estimadores $b$ convergem para os valores verdadeiros $\beta$. Isto ilustra a consist√™ncia dos estimadores de MQO.
>
> | T    | $\hat{\alpha}$ | $\hat{\phi_1}$ | $\hat{\delta}$ |
> |------|-----------------|-----------------|-----------------|
> | 100  | 2.34            | 0.65            | 0.28            |
> | 500  | 2.12            | 0.68            | 0.29            |
> | 1000 | 2.05            | 0.69            | 0.30            |
> | 5000 | 2.01            | 0.70            | 0.30            |
>
> Este exemplo mostra como os estimadores se tornam mais precisos √† medida que o tamanho da amostra aumenta, demonstrando a propriedade de consist√™ncia.
> ```python
> import numpy as np
> import pandas as pd
>
> # Par√¢metros verdadeiros
> beta_true = np.array([2, 0.7, 0.3])
>
> # Tamanhos de amostra
> sample_sizes = [100, 500, 1000, 5000]
>
> # Resultados para armazenar as estimativas
> results = []
>
> # Fun√ß√£o para simular e estimar o modelo
> def simulate_and_estimate(T):
>    # Simula√ß√£o dos dados
>    epsilon = np.random.normal(0, 1, T)
>    y = np.zeros(T)
>    for t in range(1, T):
>        y[t] = beta_true[0] + beta_true[2] * t + beta_true[1] * y[t-1] + epsilon[t]
>
>    # Matriz de regressores (incluindo tend√™ncia e defasagem)
>    X = np.column_stack((np.ones(T), y[:-1], np.arange(T)))
>    X = X[1:]  # Remove a primeira linha por causa da defasagem
>    y = y[1:]
>    # Estima√ß√£o dos coeficientes
>    b_hat = np.linalg.lstsq(X,y, rcond = None)[0]
>    return b_hat
>
> # Loop para os tamanhos de amostra
> for T in sample_sizes:
>    b_hat = simulate_and_estimate(T)
>    results.append([T, b_hat[0], b_hat[1], b_hat[2]])
>
> # Converter resultados em DataFrame para f√°cil visualiza√ß√£o
> results_df = pd.DataFrame(results, columns=['T', 'alpha_hat', 'phi1_hat', 'delta_hat'])
> print(results_df)
> ```

**Observa√ß√£o 2:** A consist√™ncia dos estimadores originais $b$ √© uma propriedade importante, pois garante que, com um n√∫mero suficientemente grande de observa√ß√µes, os estimadores convergir√£o para os valores verdadeiros dos par√¢metros.

A matriz $Q^*$ que aparece na distribui√ß√£o assint√≥tica de $b^*$ [^3, 16.3.13], √© a matriz limite dos momentos das vari√°veis transformadas, e √© dada por:

$$Q^* = \lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'}$$ [^3, 16.3.15]

Essa matriz inclui componentes relacionados aos regressores estacion√°rios ($y_{t-j}^*$), a tend√™ncia temporal ($t$), e os termos cruzados entre eles. Em particular, $Q^*$  pode ser representada como:

$$ Q^* =
\begin{bmatrix}
    \gamma_0^* & \gamma_1^* & \cdots & \gamma_{p-1}^* & 0 & 0\\
    \gamma_1^* & \gamma_0^* & \cdots & \gamma_{p-2}^* & 0 & 0\\
    \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\
    \gamma_{p-1}^* & \gamma_{p-2}^* & \cdots & \gamma_0^* & 0 & 0\\
     0 & 0 & \cdots & 0 & 1 & 1/2\\
    0 & 0 & \cdots & 0 & 1/2 & 1/3
\end{bmatrix}
$$

Onde $\gamma_j^* = E[y_t^* y_{t-j}^*]$ s√£o as autocovari√¢ncias da parte estacion√°ria do processo transformado, e os elementos 1, 1/2 e 1/3 na parte inferior direita representam os momentos assint√≥ticos da tend√™ncia temporal (como visto em [^1, 16.1.11 e 16.1.12]). A matriz $Q^*$ converge em probabilidade, o que garante que a distribui√ß√£o assint√≥tica de $b^*$ seja bem definida.

> üí° **Exemplo Num√©rico:**
> Considere um modelo AR(1) com tend√™ncia: $y_t = \alpha + \delta t + \phi_1 y_{t-1} + \epsilon_t$. Ap√≥s a transforma√ß√£o, temos $y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \epsilon_t$. Suponha que a vari√¢ncia da parte estacion√°ria ($y_{t-1}^*$) seja $\gamma_0^* = 0.8$, e a autocovari√¢ncia lag 1 seja $\gamma_1^* = 0.5$. Ent√£o, a matriz $Q^*$ pode ser aproximada como:
> $$ Q^* \approx \begin{bmatrix} 0.8 & 0 & 0\\ 0 & 1 & 1/2\\ 0 & 1/2 & 1/3 \end{bmatrix}$$
> Os elementos 1, 1/2, e 1/3 na parte inferior direita representam a vari√¢ncia e autocovari√¢ncia da tend√™ncia temporal, e s√£o resultados dos somat√≥rios apresentados em [^1, 16.1.11 e 16.1.12]. Os termos que correspondem √† covari√¢ncia entre a vari√°vel estacion√°ria e o termo da tend√™ncia s√£o iguais a zero devido √† transforma√ß√£o aplicada que isola a parte estacion√°ria.
>
> ```python
> import numpy as np
>
> gamma_0_star = 0.8
> gamma_1_star = 0.5
>
> Q_star = np.array([[gamma_0_star, 0, 0],
>                    [0, 1, 1/2],
>                    [0, 1/2, 1/3]])
>
> print("Matriz Q*:\n", Q_star)
> ```

**Lema 2:** A matriz $Q^*$ inclui componentes dos regressores estacion√°rios (autocovari√¢ncias $\gamma_j^*$), a tend√™ncia temporal (elementos 1, 1/2 e 1/3) e os termos cruzados entre eles. A converg√™ncia em probabilidade de $Q^*$ garante a exist√™ncia de uma matriz de covari√¢ncia assint√≥tica bem definida para os estimadores transformados.

**Teorema 2:** (Converg√™ncia da Matriz de Momentos) A matriz de momentos amostrais $\frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'}$ converge em probabilidade para a matriz limite $Q^*$.

*Prova:* A prova deste teorema envolve demonstrar que cada elemento da matriz $\frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'}$ converge em probabilidade para o elemento correspondente em $Q^*$. Para as autocovari√¢ncias $\gamma_j^*$, a prova √© baseada na lei dos grandes n√∫meros para processos estacion√°rios, que garante que a m√©dia amostral das autocovari√¢ncias converge para a autocovari√¢ncia populacional. Para a parte da tend√™ncia temporal, os resultados de converg√™ncia foram demonstrados em [^1, 16.1.11 e 16.1.12], onde se prova que as somas de pot√™ncias de $t$ divididas por pot√™ncias apropriadas de $T$ convergem para constantes bem definidas. Os termos cruzados, dado que $y_t^*$ tem m√©dia zero e n√£o tem correla√ß√£o assint√≥tica com a tend√™ncia temporal, convergem para zero em probabilidade.
‚ñ†

**Lema 2.1:** (Forma da Matriz Q*) A matriz $Q^*$ tem uma estrutura de blocos onde o bloco superior esquerdo cont√©m as autocovari√¢ncias das vari√°veis estacion√°rias transformadas, e o bloco inferior direito cont√©m os momentos assint√≥ticos da tend√™ncia temporal. Esta estrutura reflete a independ√™ncia assint√≥tica entre as vari√°veis estacion√°rias transformadas e a tend√™ncia temporal ap√≥s a transforma√ß√£o de Sims, Stock e Watson.

### Conclus√£o

A transforma√ß√£o dos regressores proposta por Sims, Stock e Watson [^1, ^2] √© uma ferramenta poderosa para analisar modelos com tend√™ncias temporais determin√≠sticas. Ao isolar os componentes com diferentes taxas de converg√™ncia, essa transforma√ß√£o simplifica a deriva√ß√£o das distribui√ß√µes assint√≥ticas dos estimadores de MQO. Especificamente, a distribui√ß√£o assint√≥tica dos estimadores da regress√£o transformada √© multivariada normal, com o coeficiente da tend√™ncia temporal ($Œ¥^*$) convergindo a uma taxa $T^{3/2}$ e os demais coeficientes convergindo a uma taxa $\sqrt{T}$ [^3]. A distribui√ß√£o assint√≥tica dos estimadores originais pode ser recuperada a partir dos resultados obtidos para os estimadores transformados. A matriz $Q^*$ desempenha um papel fundamental na determina√ß√£o da distribui√ß√£o assint√≥tica dos estimadores transformados, incluindo componentes dos regressores estacion√°rios, a tend√™ncia temporal, e uma matriz de momento das vari√°veis estacion√°rias que converge em probabilidade. Este resultado √© crucial para a realiza√ß√£o de infer√™ncias estat√≠sticas v√°lidas em modelos com tend√™ncias temporais determin√≠sticas, que ser√£o exploradas em mais detalhes no cap√≠tulo seguinte [^3].

**Teorema 3:** (Distribui√ß√£o Assint√≥tica dos Erros) Sob as mesmas condi√ß√µes do Teorema 1 e assumindo que os erros $\epsilon_t$ s√£o homoced√°sticos com vari√¢ncia $\sigma^2$ e n√£o autocorrelacionados, os res√≠duos da regress√£o transformada $\hat{\epsilon}_t$ convergem para os erros verdadeiros $\epsilon_t$ e t√™m a mesma distribui√ß√£o assint√≥tica, ou seja, $\sqrt{T}(\hat{\epsilon}_t - \epsilon_t) \xrightarrow{d} N(0, \sigma^2)$.

*Prova:* A prova deste teorema envolve demonstrar que os res√≠duos da regress√£o transformada convergem para os erros verdadeiros e que a diferen√ßa entre os res√≠duos e os erros verdadeiros converge para zero a uma taxa $\sqrt{T}$. Usando a nota√ß√£o vetorial, temos que $\hat{\epsilon} = y - X^*b^*$ e $\epsilon = y - X^*\beta^*$, onde $X^*$ √© a matriz de regressores transformados. Portanto, $\hat{\epsilon} - \epsilon = X^*(\beta^* - b^*)$. Multiplicando por $\sqrt{T}$, temos que $\sqrt{T}(\hat{\epsilon} - \epsilon) = X^* \sqrt{T}(\beta^* - b^*)$. Utilizando o fato que $Y_T (\beta^* - b^*) \xrightarrow{d} N(0, \sigma^2[Q^*]^{-1})$, obtemos o resultado de converg√™ncia desejado para os res√≠duos.
‚ñ†

### Refer√™ncias
[^1]: Cap√≠tulo 16, se√ß√£o 16.1 do texto original.
[^2]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." Econometrica 58:113-44.
[^3]: Cap√≠tulo 16, se√ß√£o 16.3 e Ap√™ndice 16.A do texto original.
<!-- END -->
