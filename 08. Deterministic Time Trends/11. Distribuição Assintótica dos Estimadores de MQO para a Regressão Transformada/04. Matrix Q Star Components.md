## DistribuiÃ§Ã£o AssintÃ³tica dos Estimadores de MQO para a RegressÃ£o Transformada

### IntroduÃ§Ã£o
Como vimos anteriormente, no capÃ­tulo sobre processos com tendÃªncias temporais determinÃ­sticas, os estimadores de MÃ­nimos Quadrados OrdinÃ¡rios (MQO) para modelos com tendÃªncias temporais apresentam diferentes taxas de convergÃªncia [^1]. Enquanto estimadores em modelos com variÃ¡veis estacionÃ¡rias geralmente convergem a uma taxa de $\sqrt{T}$, coeficientes associados a tendÃªncias temporais determinÃ­sticas podem convergir mais rapidamente. Em particular, discutimos a necessidade de reescalonar as variÃ¡veis para acomodar essas diferentes taxas de convergÃªncia [^1]. Este capÃ­tulo expande esses conceitos, explorando a distribuiÃ§Ã£o assintÃ³tica dos estimadores de MQO para uma regressÃ£o transformada, com foco em processos autorregressivos ao redor de uma tendÃªncia temporal determinÃ­stica.

### Conceitos Fundamentais

A seÃ§Ã£o anterior [^1] introduziu a ideia de diferentes taxas de convergÃªncia para estimadores em modelos com tendÃªncias temporais. Em particular, foi demonstrado que para um modelo simples de tendÃªncia temporal, o estimador do coeficiente da tendÃªncia ($\delta_T$) converge a uma taxa de $T^{3/2}$, enquanto o estimador do intercepto ($\hat{\alpha}_T$) converge a uma taxa de $\sqrt{T}$ [^1]. A transformaÃ§Ã£o de variÃ¡veis proposta por Sims, Stock e Watson (1990) [^1, ^2] visa isolar essas diferentes taxas de convergÃªncia, facilitando a derivaÃ§Ã£o da distribuiÃ§Ã£o assintÃ³tica dos estimadores.

Para um processo autorregressivo geral ao redor de uma tendÃªncia temporal determinÃ­stica, dado por:
$$y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t$$ [^3, 16.3.1],
a transformaÃ§Ã£o proposta reescreve o modelo na forma:
$$y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \ldots + \phi_p^* y_{t-p}^* + \epsilon_t$$ [^3, 16.3.3],
onde os novos regressores $y_{t-j}^*$ representam variÃ¡veis com mÃ©dia zero, e $\alpha^*$ e $\delta^*$ sÃ£o coeficientes transformados [^3]. O objetivo dessa transformaÃ§Ã£o Ã© separar os componentes do vetor de coeficientes que apresentam diferentes taxas de convergÃªncia.

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Considere um modelo AR(1) com tendÃªncia temporal: $y_t = 2 + 0.5t + 0.7y_{t-1} + \epsilon_t$. ApÃ³s a transformaÃ§Ã£o de Sims, Stock e Watson, podemos ter um modelo como: $y_t = \alpha^* + \delta^*t + \phi_1^*y_{t-1}^* + \epsilon_t$, onde $\alpha^*$, $\delta^*$ e $\phi_1^*$ sÃ£o os coeficientes transformados e $y_{t-1}^*$ Ã© a variÃ¡vel defasada transformada. A transformaÃ§Ã£o garante que as variÃ¡veis $y_{t-1}^*$ tenham mÃ©dia zero.  Suponha que apÃ³s a transformaÃ§Ã£o, os valores estimados para os coeficientes sejam: $\hat{\alpha}^*=1.5$, $\hat{\delta}^*=0.4$ e $\hat{\phi}_1^*=0.6$. Este Ã© um exemplo ilustrativo de como o modelo Ã© reescrito e os parÃ¢metros sÃ£o transformados para facilitar a anÃ¡lise assintÃ³tica.

**ObservaÃ§Ã£o 1:** A transformaÃ§Ã£o que leva de $y_t$ para $y_t^*$ nÃ£o Ã© Ãºnica. Em geral, qualquer transformaÃ§Ã£o que resulte em regressores com mÃ©dia zero e que preserve a estrutura de dependÃªncia temporal dos dados Ã© adequada para essa anÃ¡lise. O ponto crucial Ã© que a transformaÃ§Ã£o deve ser escolhida de forma que os estimadores dos parÃ¢metros transformados apresentem taxas de convergÃªncia assintÃ³ticas distintas, facilitando a anÃ¡lise.

A matriz de transformaÃ§Ã£o $G'$ ([^3, 16.3.8]) Ã© utilizada para obter os coeficientes transformados $\beta^*$ a partir dos coeficientes originais $\beta$ [^3, 16.3.7]:

$$\beta^* = (G')^{-1}\beta$$

O estimador dos coeficientes transformados, $b^*$, Ã© obtido por meio de uma regressÃ£o de mÃ­nimos quadrados ordinÃ¡rios (MQO) de $y_t$ em $x_t^*$ [^3, 16.3.7], onde $x_t^*$ Ã© um vetor de regressores transformados [^3, 16.3.9].

O ApÃªndice 16.A [^3] demonstra que a distribuiÃ§Ã£o assintÃ³tica de $b^*$ Ã© dada por:
$$Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$$ [^3, 16.3.13]

A matriz $Y_T$ Ã© definida como uma matriz diagonal com elementos $\sqrt{T}$ para os primeiros $p$ elementos, correspondentes aos coeficientes das variÃ¡veis defasadas $y_{t-j}^*$  e do intercepto $\alpha^*$, e $T^{3/2}$ para o elemento correspondente ao coeficiente da tendÃªncia temporal $\delta^*$, e zeros nas outras posiÃ§Ãµes [^3, 16.3.14]. Esta matriz Ã© crucial para expressar as diferentes taxas de convergÃªncia dos parÃ¢metros. A matriz de covariÃ¢ncia assintÃ³tica dos estimadores transformados Ã© dada por $\sigma^2 [Q^*]^{-1}$, onde $Q^*$ Ã© a matriz limite das variÃ¡veis transformadas.

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Suponha que temos $T=100$ observaÃ§Ãµes. A matriz $Y_T$ para um modelo com intercepto, tendÃªncia e um termo AR(1) seria:
>
> $$ Y_T = \begin{bmatrix} \sqrt{100} & 0 & 0 \\ 0 & \sqrt{100} & 0 \\ 0 & 0 & 100^{3/2} \end{bmatrix} = \begin{bmatrix} 10 & 0 & 0 \\ 0 & 10 & 0 \\ 0 & 0 & 1000 \end{bmatrix} $$
>
> Aqui, a diagonal mostra as taxas de convergÃªncia para o intercepto, o coeficiente AR(1) e a tendÃªncia temporal, respectivamente. Note que a taxa de convergÃªncia da tendÃªncia temporal (1000) Ã© muito maior do que a taxa dos outros dois parÃ¢metros (10). Isso reflete o fato de que o estimador da tendÃªncia converge mais rapidamente.
>
> Para ilustrar ainda mais, suponha que tenhamos um vetor de estimadores transformados $b^* = [1.2, 0.8, 0.1]$ (intercepto, AR(1), tendÃªncia). A matriz $Y_T$ escala cada um desses estimadores conforme sua taxa de convergÃªncia. Assim, $Y_T b^* = [10*1.2, 10*0.8, 1000*0.1] = [12, 8, 100]$.  Esta operaÃ§Ã£o Ã© crucial para a anÃ¡lise assintÃ³tica, pois ao escalar, estamos lidando com variÃ¡veis que tÃªm a mesma ordem de magnitude assintoticamente, o que permite derivar resultados estatÃ­sticos consistentes.
>
>  ```python
>  import numpy as np
>
>  T = 100
>  YT = np.diag([np.sqrt(T), np.sqrt(T), T**(3/2)])
>  print("YT:\n", YT)
>  b_star = np.array([1.2, 0.8, 0.1])
>  YT_b_star = np.dot(YT, b_star)
>  print("YT * b*:\n", YT_b_star)
>  ```

**Lema 1:** A matriz $Y_T$, definida como uma matriz diagonal com elementos $\sqrt{T}$ para os coeficientes associados Ã s variÃ¡veis estacionÃ¡rias e $T^{3/2}$ para o coeficiente da tendÃªncia temporal, desempenha um papel fundamental na determinaÃ§Ã£o da distribuiÃ§Ã£o assintÃ³tica dos estimadores. Esta matriz garante que os termos correspondentes aos coeficientes que convergem mais rapidamente sejam amplificados, enquanto os termos correspondentes aos coeficientes que convergem mais lentamente sejam mantidos em uma escala comparÃ¡vel.

Ã‰ importante observar que, embora os estimadores transformados $b^*$ sejam uma funÃ§Ã£o dos coeficientes originais $b$, a distribuiÃ§Ã£o assintÃ³tica de $b$ pode ser recuperada atravÃ©s da relaÃ§Ã£o $b = G'b^*$ [^3, 16.3.12]. A transformaÃ§Ã£o para isolar as diferentes taxas de convergÃªncia nos permite obter a distribuiÃ§Ã£o assintÃ³tica da regressÃ£o transformada, o que, por sua vez, fornece a distribuiÃ§Ã£o assintÃ³tica dos coeficientes originais.

**Lema 1.1:**  A inversa da matriz de transformaÃ§Ã£o $G$, denotada por $G = (G')^{-1}$, Ã© igualmente crucial para recuperar os coeficientes originais a partir dos coeficientes transformados.  A estrutura especÃ­fica de $G$ depende da transformaÃ§Ã£o particular utilizada, mas sua finalidade Ã© desfazer as mudanÃ§as de escala e centralizaÃ§Ã£o introduzidas pela transformaÃ§Ã£o de Sims, Stock e Watson.

A anÃ¡lise detalhada no ApÃªndice 16.A [^3] revela que:
* Os coeficientes dos termos autorregressivos ($\phi_i^*$) convergem a uma taxa de $\sqrt{T}$.
* O coeficiente da tendÃªncia temporal ($Î´^*$) converge a uma taxa de $T^{3/2}$.
* O intercepto ($\alpha^*$) tambÃ©m converge a uma taxa de $\sqrt{T}$.

Este resultado generaliza o resultado obtido para o modelo simples de tendÃªncia temporal na seÃ§Ã£o anterior [^1], onde se verificou que $\hat{\alpha}_T$ convergeva a $\sqrt{T}$ e $\delta_T$ a $T^{3/2}$ [^1].

**Teorema 1:** (DistribuiÃ§Ã£o AssintÃ³tica dos Estimadores Originais) Dado que $\beta^* = (G')^{-1}\beta$ e $b = G'b^*$, e que a distribuiÃ§Ã£o assintÃ³tica de $b^*$ Ã© dada por $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$, entÃ£o a distribuiÃ§Ã£o assintÃ³tica de $b$ Ã© dada por:
$Y_T(G'b^* - G'\beta^*) \xrightarrow{d} N(0, \sigma^2 G'[Q^*]^{-1}G)$
onde $G = (G')^{-1}$.

*Prova:* Provaremos que, dado que $\beta^* = (G')^{-1}\beta$ e $b = G'b^*$, e que a distribuiÃ§Ã£o assintÃ³tica de $b^*$ Ã© dada por $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$, entÃ£o a distribuiÃ§Ã£o assintÃ³tica de $b$ Ã© dada por:
$Y_T(G'b^* - G'\beta^*) \xrightarrow{d} N(0, \sigma^2 G'[Q^*]^{-1}G)$, onde $G = (G')^{-1}$.

I.  ComeÃ§amos com a distribuiÃ§Ã£o assintÃ³tica de $b^*$:
    $$Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$$

II. Multiplicamos ambos os lados da expressÃ£o por $G'$:
    $$G'Y_T(b^* - \beta^*) \xrightarrow{d} G'N(0, \sigma^2 [Q^*]^{-1})$$
    Note que $Y_T$ Ã© uma matriz diagonal e, portanto, pode ser movida atravÃ©s da multiplicaÃ§Ã£o da matriz $G'$.

III. DistribuÃ­mos $G'$ dentro dos parÃªnteses:
    $$Y_T(G'b^* - G'\beta^*) \xrightarrow{d} N(0, G'\sigma^2 [Q^*]^{-1}G)$$

IV.  Sabemos que $b = G'b^*$ e $\beta = G'\beta^*$. Substituindo na expressÃ£o:
$$Y_T(b - \beta) \xrightarrow{d} N(0, \sigma^2 G'[Q^*]^{-1}G)$$

V. Portanto, a distribuiÃ§Ã£o assintÃ³tica de $b$ Ã© dada por:
    $$Y_T(b - \beta) \xrightarrow{d} N(0, \sigma^2 G'[Q^*]^{-1}G)$$â– 

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Vamos supor que, em uma regressÃ£o com um intercepto, uma tendÃªncia e um termo AR(1), estimamos os coeficientes transformados $b^* = [\hat{\alpha}^*, \hat{\phi}_1^*, \hat{\delta}^*]^T = [1.5, 0.6, 0.4]^T$. A matriz de transformaÃ§Ã£o inversa $G$ (que Ã© o inverso de $G'$), pode ser usada para recuperar os coeficientes originais $b$. Suponha que $G'$ tenha sido calculada como:
> $$ G' = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -5 & 0 & 1 \end{bmatrix} $$
> Portanto, $G$ Ã© o inverso de $G'$, que pode ser calculado como:
>  $$ G = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 5 & 0 & 1 \end{bmatrix} $$
>
> Os coeficientes originais sÃ£o obtidos por: $b = G'b^*$. Para o exemplo, $b = [1.5, 0.6, -5*1.5+0.4]^T = [1.5, 0.6, -7.1]^T$. Contudo, Ã© importante notar que este valor Ã© ilustrativo e, na prÃ¡tica, os valores da transformaÃ§Ã£o e seus inversos seriam calculados com base na sÃ©rie temporal utilizada.
>
> Para a matriz de covariÃ¢ncia assintÃ³tica, suponhamos que $\sigma^2 = 0.2$ e $[Q^*]^{-1}$ Ã© estimado como:
> $$ [Q^*]^{-1} = \begin{bmatrix} 0.1 & 0.02 & 0.01 \\ 0.02 & 0.05 & 0.005 \\ 0.01 & 0.005 & 0.02 \end{bmatrix} $$
> EntÃ£o, a matriz de covariÃ¢ncia assintÃ³tica dos estimadores originais Ã©:
> $\sigma^2 G'[Q^*]^{-1}G = 0.2 * G' [Q^*]^{-1} G$
> onde $G = (G')^{-1}$.
>
> Calculando:
>  $$ G'[Q^*]^{-1} =  \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -5 & 0 & 1 \end{bmatrix} \begin{bmatrix} 0.1 & 0.02 & 0.01 \\ 0.02 & 0.05 & 0.005 \\ 0.01 & 0.005 & 0.02 \end{bmatrix} = \begin{bmatrix} 0.1 & 0.02 & 0.01 \\ 0.02 & 0.05 & 0.005 \\ -0.49 & -0.095 & -0.03 \end{bmatrix} $$
>
> $$ G'[Q^*]^{-1}G =  \begin{bmatrix} 0.1 & 0.02 & 0.01 \\ 0.02 & 0.05 & 0.005 \\ -0.49 & -0.095 & -0.03 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 5 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 0.15 & 0.02 & 0.01 \\ 0.045 & 0.05 & 0.005 \\ -0.64 & -0.095 & -0.03 \end{bmatrix} $$
>
> Portanto, a matriz de covariÃ¢ncia assintÃ³tica Ã©:
>
> $0.2 *  \begin{bmatrix} 0.15 & 0.02 & 0.01 \\ 0.045 & 0.05 & 0.005 \\ -0.64 & -0.095 & -0.03 \end{bmatrix} =  \begin{bmatrix} 0.03 & 0.004 & 0.002 \\ 0.009 & 0.01 & 0.001 \\ -0.128 & -0.019 & -0.006 \end{bmatrix} $
>
> Esta matriz contÃ©m informaÃ§Ãµes sobre as variÃ¢ncias e covariÃ¢ncias dos estimadores originais, permitindo inferÃªncia estatÃ­stica.
>
> ```python
> import numpy as np
>
> # Matrizes G' e G
> G_prime = np.array([[1, 0, 0], [0, 1, 0], [-5, 0, 1]])
> G = np.array([[1, 0, 0], [0, 1, 0], [5, 0, 1]])
>
> # Vetor de coeficientes transformados
> b_star = np.array([1.5, 0.6, 0.4])
>
> # Calculando os coeficientes originais
> b = np.dot(G_prime, b_star)
> print("Coeficientes originais b:\n", b)
>
> # Matriz inversa Q*
> Q_star_inv = np.array([[0.1, 0.02, 0.01], [0.02, 0.05, 0.005], [0.01, 0.005, 0.02]])
> sigma2 = 0.2
>
> # Calculando a matriz de covariÃ¢ncia assintÃ³tica
> cov_matrix = sigma2 * np.dot(G_prime, np.dot(Q_star_inv, G))
> print("Matriz de covariÃ¢ncia assintÃ³tica:\n", cov_matrix)
> ```

**CorolÃ¡rio 1.1:** A matriz $G'[Q^*]^{-1}G$ no Teorema 1 representa a matriz de covariÃ¢ncia assintÃ³tica dos estimadores originais $b$ apÃ³s o devido ajuste pelas taxas de convergÃªncia diferenciadas expressas em $Y_T$. O conhecimento dessa matriz permite inferÃªncias estatÃ­sticas sobre os parÃ¢metros originais $\beta$.

**ProposiÃ§Ã£o 1:** A transformaÃ§Ã£o de Sims, Stock e Watson nÃ£o apenas simplifica a anÃ¡lise assintÃ³tica, mas tambÃ©m permite que a distribuiÃ§Ã£o dos estimadores seja abordada de maneira mais precisa, especialmente em modelos com tendÃªncias temporais determinÃ­sticas. A chave Ã© a utilizaÃ§Ã£o da matriz $Y_T$ para equilibrar as diferentes taxas de convergÃªncia.

**Teorema 1.1** (ConsistÃªncia dos Estimadores) Sob as condiÃ§Ãµes usuais de regularidade para estimadores de MQO e a transformaÃ§Ã£o de Sims, Stock e Watson, os estimadores originais $b$ sÃ£o consistentes, ou seja,  $b \xrightarrow{p} \beta$ quando $T \to \infty$.

*Prova*: Sabemos que $b = G'b^*$. Pela consistÃªncia dos estimadores transformados, temos que $b^* \xrightarrow{p} \beta^*$. Como $G'$ Ã© uma matriz de transformaÃ§Ã£o fixa, a consistÃªncia Ã© preservada pela multiplicaÃ§Ã£o por $G'$, e assim, $G'b^* \xrightarrow{p} G'\beta^*$. Substituindo as relaÃ§Ãµes $b=G'b^*$ e $\beta=G'\beta^*$, obtemos que $b \xrightarrow{p} \beta$.
â– 

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Considere um cenÃ¡rio simulado onde os verdadeiros coeficientes de um modelo com tendÃªncia temporal sÃ£o $\beta = [2, 0.7, 0.3]^T$ (intercepto, AR(1), tendÃªncia). Realizamos simulaÃ§Ãµes com tamanhos de amostra crescentes ($T = 100, 500, 1000, 5000$) e estimamos os coeficientes usando a transformaÃ§Ã£o de Sims, Stock e Watson. Para cada amostra, obtemos estimativas $b$. Podemos observar que, Ã  medida que $T$ aumenta, os estimadores $b$ convergem para os valores verdadeiros $\beta$. Isto ilustra a consistÃªncia dos estimadores de MQO.
>
> | T    | $\hat{\alpha}$ | $\hat{\phi_1}$ | $\hat{\delta}$ |
> |------|-----------------|-----------------|-----------------|
> | 100  | 2.34            | 0.65            | 0.28            |
> | 500  | 2.12            | 0.68            | 0.29            |
> | 1000 | 2.05            | 0.69            | 0.30            |
> | 5000 | 2.01            | 0.70            | 0.30            |
>
> Este exemplo mostra como os estimadores se tornam mais precisos Ã  medida que o tamanho da amostra aumenta, demonstrando a propriedade de consistÃªncia.
> ```python
> import numpy as np
> import pandas as pd
>
> # ParÃ¢metros verdadeiros
> beta_true = np.array([2, 0.7, 0.3])
>
> # Tamanhos de amostra
> sample_sizes = [100, 500, 1000, 5000]
>
> # Resultados para armazenar as estimativas
> results = []
>
> # FunÃ§Ã£o para simular e estimar o modelo
> def simulate_and_estimate(T):
>    # SimulaÃ§Ã£o dos dados
>    epsilon = np.random.normal(0, 1, T)
>    y = np.zeros(T)
>    for t in range(1, T):
>        y[t] = beta_true[0] + beta_true[2] * t + beta_true[1] * y[t-1] + epsilon[t]
>
>    # Matriz de regressores (incluindo tendÃªncia e defasagem)
>    X = np.column_stack((np.ones(T), y[:-1], np.arange(T)))
>    X = X[1:]  # Remove a primeira linha por causa da defasagem
>    y = y[1:]
>    # EstimaÃ§Ã£o dos coeficientes
>    b_hat = np.linalg.lstsq(X,y, rcond = None)[0]
>    return b_hat
>
> # Loop para os tamanhos de amostra
> for T in sample_sizes:
>    b_hat = simulate_and_estimate(T)
>    results.append([T, b_hat[0], b_hat[1], b_hat[2]])
>
> # Converter resultados em DataFrame para fÃ¡cil visualizaÃ§Ã£o
> results_df = pd.DataFrame(results, columns=['T', 'alpha_hat', 'phi1_hat', 'delta_hat'])
> print(results_df)
> ```

**ObservaÃ§Ã£o 2:** A consistÃªncia dos estimadores originais $b$ Ã© uma propriedade importante, pois garante que, com um nÃºmero suficientemente grande de observaÃ§Ãµes, os estimadores convergirÃ£o para os valores verdadeiros dos parÃ¢metros.

A matriz $Q^*$ que aparece na distribuiÃ§Ã£o assintÃ³tica de $b^*$ [^3, 16.3.13], Ã© a matriz limite dos momentos das variÃ¡veis transformadas, e Ã© dada por:

$$Q^* = \lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'}$$ [^3, 16.3.15]

Essa matriz inclui componentes relacionados aos regressores estacionÃ¡rios ($y_{t-j}^*$), a tendÃªncia temporal ($t$), e os termos cruzados entre eles. Em particular, $Q^*$  pode ser representada como:

$$ Q^* =
\begin{bmatrix}
    \gamma_0^* & \gamma_1^* & \cdots & \gamma_{p-1}^* & 0 & 0\\
    \gamma_1^* & \gamma_0^* & \cdots & \gamma_{p-2}^* & 0 & 0\\
    \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\
    \gamma_{p-1}^* & \gamma_{p-2}^* & \cdots & \gamma_0^* & 0 & 0\\
     0 & 0 & \cdots & 0 & 1 & 1/2\\
    0 & 0 & \cdots & 0 & 1/2 & 1/3
\end{bmatrix}
$$

Onde $\gamma_j^* = E[y_t^* y_{t-j}^*]$ sÃ£o as autocovariÃ¢ncias da parte estacionÃ¡ria do processo transformado, e os elementos 1, 1/2 e 1/3 na parte inferior direita representam os momentos assintÃ³ticos da tendÃªncia temporal (como visto em [^1, 16.1.11 e 16.1.12]). A matriz $Q^*$ converge em probabilidade, o que garante que a distribuiÃ§Ã£o assintÃ³tica de $b^*$ seja bem definida.

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Considere um modelo AR(1) com tendÃªncia: $y_t = \alpha + \delta t + \phi_1 y_{t-1} + \epsilon_t$. ApÃ³s a transformaÃ§Ã£o, temos $y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \epsilon_t$. Suponha que a variÃ¢ncia da parte estacionÃ¡ria ($y_{t-1}^*$) seja $\gamma_0^* = 0.8$, e a autocovariÃ¢ncia lag 1 seja $\gamma_1^* = 0.5$. EntÃ£o, a matriz $Q^*$ pode ser aproximada como:
> $$ Q^* \approx \begin{bmatrix} 0.8 & 0 & 0\\ 0 & 1 & 1/2\\ 0 & 1/2 & 1/3 \end{bmatrix}$$
> Os elementos 1, 1/2, e 1/3 na parte inferior direita representam a variÃ¢ncia e autocovariÃ¢ncia da tendÃªncia temporal, e sÃ£o resultados dos somatÃ³rios apresentados em [^1, 16.1.11 e 16.1.12]. Os termos que correspondem Ã  covariÃ¢ncia entre a variÃ¡vel estacionÃ¡ria e o termo da tendÃªncia sÃ£o iguais a zero devido Ã  transformaÃ§Ã£o aplicada que isola a parte estacionÃ¡ria.
>
> ```python
> import numpy as np
>
> gamma_0_star = 0.8
> gamma_1_star = 0.5
>
> Q_star = np.array([[gamma_0_star, 0, 0],
>                    [0, 1, 1/2],
>                    [0, 1/2, 1/3]])
>
> print("Matriz Q*:\n", Q_star)
> ```

**Lema 2:** A matriz $Q^*$ inclui componentes dos regressores estacionÃ¡rios (autocovariÃ¢ncias $\gamma_j^*$), a tendÃªncia temporal (elementos 1, 1/2 e 1/3) e os termos cruzados entre eles. A convergÃªncia em probabilidade de $Q^*$ garante a existÃªncia de uma matriz de covariÃ¢ncia assintÃ³tica bem definida para os estimadores transformados.

**Teorema 2:** (ConvergÃªncia da Matriz de Momentos) A matriz de momentos amostrais $\frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'}$ converge em probabilidade para a matriz limite $Q^*$.

*Prova:* A prova deste teorema envolve demonstrar que cada elemento da matriz $\frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'}$ converge em probabilidade para o elemento correspondente em $Q^*$. Para as autocovariÃ¢ncias $\gamma_j^*$, a prova Ã© baseada na lei dos grandes nÃºmeros para processos estacionÃ¡rios, que garante que a mÃ©dia amostral das autocovariÃ¢ncias converge para a autocovariÃ¢ncia populacional. Para a parte da tendÃªncia temporal, os resultados de convergÃªncia foram demonstrados em [^1, 16.1.11 e 16.1.12], onde se prova que as somas de potÃªncias de $t$ divididas por potÃªncias apropriadas de $T$ convergem para constantes bem definidas. Os termos cruzados, dado que $y_t^*$ tem mÃ©dia zero e nÃ£o tem correlaÃ§Ã£o assintÃ³tica com a tendÃªncia temporal, convergem para zero em probabilidade.
â– 

**Lema 2.1:** (Forma da Matriz Q*) A matriz $Q^*$ tem uma estrutura de blocos onde o bloco superior esquerdo contÃ©m as autocovariÃ¢ncias das variÃ¡veis estacionÃ¡rias transformadas, e o bloco inferior direito contÃ©m os momentos assintÃ³ticos da tendÃªncia temporal. Esta estrutura reflete a independÃªncia assintÃ³tica entre as variÃ¡veis estacionÃ¡rias transformadas e a tendÃªncia temporal apÃ³s a transformaÃ§Ã£o de Sims, Stock e Watson.

### ConclusÃ£o

A transformaÃ§Ã£o dos regressores proposta por Sims, Stock e Watson [^1, ^2] Ã© uma ferramenta poderosa para analisar modelos com tendÃªncias temporais determinÃ­sticas. Ao isolar os componentes com diferentes taxas de convergÃªncia, essa transformaÃ§Ã£o simplifica a derivaÃ§Ã£o das distribuiÃ§Ãµes assintÃ³ticas dos estimadores de MQO. Especificamente, a distribuiÃ§Ã£o assintÃ³tica dos estimadores da regressÃ£o transformada Ã© multivariada normal, com o coeficiente da tendÃªncia temporal ($Î´^*$) convergindo a uma taxa $T^{3/2}$ e os demais coeficientes convergindo a uma taxa $\sqrt{T}$ [^3]. A distribuiÃ§Ã£o assintÃ³tica dos estimadores originais pode ser recuperada a partir dos resultados obtidos para os estimadores transformados. A matriz $Q^*$ desempenha um papel fundamental na determinaÃ§Ã£o da distribuiÃ§Ã£o assintÃ³tica dos estimadores transformados, incluindo componentes dos regressores estacionÃ¡rios, a tendÃªncia temporal, e uma matriz de momento das variÃ¡veis estacionÃ¡rias que converge em probabilidade. Este resultado Ã© crucial para a realizaÃ§Ã£o de inferÃªncias estatÃ­sticas vÃ¡lidas em modelos com tendÃªncias temporais determinÃ­sticas, que serÃ£o exploradas em mais detalhes no capÃ­tulo seguinte [^3].

**Teorema 3:** (DistribuiÃ§Ã£o AssintÃ³tica dos Erros) Sob as mesmas condiÃ§Ãµes do Teorema 1 e assumindo que os erros $\epsilon_t$ sÃ£o homocedÃ¡sticos com variÃ¢ncia $\sigma^2$ e nÃ£o autocorrelacionados, os resÃ­duos da regressÃ£o transformada $\hat{\epsilon}_t$ convergem para os erros verdadeiros $\epsilon_t$ e tÃªm a mesma distribuiÃ§Ã£o assintÃ³tica, ou seja, $\sqrt{T}(\hat{\epsilon}_t - \epsilon_t) \xrightarrow{d} N(0, \sigma^2)$.

*Prova:* A prova deste teorema envolve demonstrar que os resÃ­duos da regressÃ£o transformada convergem para os erros verdadeiros e que a diferenÃ§a entre os resÃ­duos e os erros verdadeiros converge para zero a uma taxa $\sqrt{T}$. Usando a notaÃ§Ã£o vetorial, temos que $\hat{\epsilon} = y - X^*b^*$ e $\epsilon = y - X^*\beta^*$, onde $X^*$ Ã© a matriz de regressores transformados. Portanto, $\hat{\epsilon} - \epsilon = X^*(\beta^* - b^*)$. Multiplicando por $\sqrt{T}$, temos que $\sqrt{T}(\hat{\epsilon} - \epsilon) = X^* \sqrt{T}(\beta^* - b^*)$. Utilizando o fato que $Y_T (\beta^* - b^*) \xrightarrow{d} N(0, \sigma^2[Q^*]^{-1})$, obtemos o resultado de convergÃªncia desejado para os resÃ­duos.
â– 

### ReferÃªncias
[^1]: CapÃ­tulo 16, seÃ§Ã£o 16.1 do texto original.
[^2]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." Econometrica 58:113-44.
[^3]: CapÃ­tulo 16, seÃ§Ã£o 16.3 e ApÃªndice 16.A do texto original.
<!-- END -->
