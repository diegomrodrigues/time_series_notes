## Distribui√ß√£o Assint√≥tica dos Estimadores de MQO para a Regress√£o Transformada

### Introdu√ß√£o
Como vimos anteriormente, no cap√≠tulo sobre processos com tend√™ncias temporais determin√≠sticas, os estimadores de M√≠nimos Quadrados Ordin√°rios (MQO) para modelos com tend√™ncias temporais apresentam diferentes taxas de converg√™ncia [^1]. Enquanto estimadores em modelos com vari√°veis estacion√°rias geralmente convergem a uma taxa de $\sqrt{T}$, coeficientes associados a tend√™ncias temporais determin√≠sticas podem convergir mais rapidamente. Em particular, discutimos a necessidade de reescalonar as vari√°veis para acomodar essas diferentes taxas de converg√™ncia [^1]. Este cap√≠tulo expande esses conceitos, explorando a distribui√ß√£o assint√≥tica dos estimadores de MQO para uma regress√£o transformada, com foco em processos autorregressivos ao redor de uma tend√™ncia temporal determin√≠stica.

### Conceitos Fundamentais

A se√ß√£o anterior [^1] introduziu a ideia de diferentes taxas de converg√™ncia para estimadores em modelos com tend√™ncias temporais. Em particular, foi demonstrado que para um modelo simples de tend√™ncia temporal, o estimador do coeficiente da tend√™ncia ($\delta_T$) converge a uma taxa de $T^{3/2}$, enquanto o estimador do intercepto ($\hat{\alpha}_T$) converge a uma taxa de $\sqrt{T}$ [^1]. A transforma√ß√£o de vari√°veis proposta por Sims, Stock e Watson (1990) [^1, ^2] visa isolar essas diferentes taxas de converg√™ncia, facilitando a deriva√ß√£o da distribui√ß√£o assint√≥tica dos estimadores.

Para um processo autorregressivo geral ao redor de uma tend√™ncia temporal determin√≠stica, dado por:
$$y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t$$ [^3, 16.3.1],
a transforma√ß√£o proposta reescreve o modelo na forma:
$$y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \ldots + \phi_p^* y_{t-p}^* + \epsilon_t$$ [^3, 16.3.3],
onde os novos regressores $y_{t-j}^*$ representam vari√°veis com m√©dia zero, e $\alpha^*$ e $\delta^*$ s√£o coeficientes transformados [^3]. O objetivo dessa transforma√ß√£o √© separar os componentes do vetor de coeficientes que apresentam diferentes taxas de converg√™ncia.

> üí° **Exemplo Num√©rico:**
> Considere um modelo AR(1) com tend√™ncia temporal: $y_t = 2 + 0.5t + 0.7y_{t-1} + \epsilon_t$. Ap√≥s a transforma√ß√£o de Sims, Stock e Watson, podemos ter um modelo como: $y_t = \alpha^* + \delta^*t + \phi_1^*y_{t-1}^* + \epsilon_t$, onde $\alpha^*$, $\delta^*$ e $\phi_1^*$ s√£o os coeficientes transformados e $y_{t-1}^*$ √© a vari√°vel defasada transformada. A transforma√ß√£o garante que as vari√°veis $y_{t-1}^*$ tenham m√©dia zero.  Suponha que ap√≥s a transforma√ß√£o, os valores estimados para os coeficientes sejam: $\hat{\alpha}^*=1.5$, $\hat{\delta}^*=0.4$ e $\hat{\phi}_1^*=0.6$. Este √© um exemplo ilustrativo de como o modelo √© reescrito e os par√¢metros s√£o transformados para facilitar a an√°lise assint√≥tica.

**Observa√ß√£o 1:** A transforma√ß√£o que leva de $y_t$ para $y_t^*$ n√£o √© √∫nica. Em geral, qualquer transforma√ß√£o que resulte em regressores com m√©dia zero e que preserve a estrutura de depend√™ncia temporal dos dados √© adequada para essa an√°lise. O ponto crucial √© que a transforma√ß√£o deve ser escolhida de forma que os estimadores dos par√¢metros transformados apresentem taxas de converg√™ncia assint√≥ticas distintas, facilitando a an√°lise.

A matriz de transforma√ß√£o $G'$ ([^3, 16.3.8]) √© utilizada para obter os coeficientes transformados $\beta^*$ a partir dos coeficientes originais $\beta$ [^3, 16.3.7]:

$$\beta^* = (G')^{-1}\beta$$

O estimador dos coeficientes transformados, $b^*$, √© obtido por meio de uma regress√£o de m√≠nimos quadrados ordin√°rios (MQO) de $y_t$ em $x_t^*$ [^3, 16.3.7], onde $x_t^*$ √© um vetor de regressores transformados [^3, 16.3.9].

O Ap√™ndice 16.A [^3] demonstra que a distribui√ß√£o assint√≥tica de $b^*$ √© dada por:
$$Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$$ [^3, 16.3.13]

A matriz $Y_T$ √© definida como uma matriz diagonal com elementos $\sqrt{T}$ para os primeiros $p$ elementos, correspondentes aos coeficientes das vari√°veis defasadas $y_{t-j}^*$  e do intercepto $\alpha^*$, e $T^{3/2}$ para o elemento correspondente ao coeficiente da tend√™ncia temporal $\delta^*$, e zeros nas outras posi√ß√µes [^3, 16.3.14]. Esta matriz √© crucial para expressar as diferentes taxas de converg√™ncia dos par√¢metros. A matriz de covari√¢ncia assint√≥tica dos estimadores transformados √© dada por $\sigma^2 [Q^*]^{-1}$, onde $Q^*$ √© a matriz limite das vari√°veis transformadas.

> üí° **Exemplo Num√©rico:**
> Suponha que temos $T=100$ observa√ß√µes. A matriz $Y_T$ para um modelo com intercepto, tend√™ncia e um termo AR(1) seria:
>
> $$ Y_T = \begin{bmatrix} \sqrt{100} & 0 & 0 \\ 0 & \sqrt{100} & 0 \\ 0 & 0 & 100^{3/2} \end{bmatrix} = \begin{bmatrix} 10 & 0 & 0 \\ 0 & 10 & 0 \\ 0 & 0 & 1000 \end{bmatrix} $$
>
> Aqui, a diagonal mostra as taxas de converg√™ncia para o intercepto, o coeficiente AR(1) e a tend√™ncia temporal, respectivamente. Note que a taxa de converg√™ncia da tend√™ncia temporal (1000) √© muito maior do que a taxa dos outros dois par√¢metros (10). Isso reflete o fato de que o estimador da tend√™ncia converge mais rapidamente.
>
> Para ilustrar ainda mais, suponha que tenhamos um vetor de estimadores transformados $b^* = [1.2, 0.8, 0.1]$ (intercepto, AR(1), tend√™ncia). A matriz $Y_T$ escala cada um desses estimadores conforme sua taxa de converg√™ncia. Assim, $Y_T b^* = [10*1.2, 10*0.8, 1000*0.1] = [12, 8, 100]$.  Esta opera√ß√£o √© crucial para a an√°lise assint√≥tica, pois ao escalar, estamos lidando com vari√°veis que t√™m a mesma ordem de magnitude assintoticamente, o que permite derivar resultados estat√≠sticos consistentes.

**Lema 1:** A matriz $Y_T$, definida como uma matriz diagonal com elementos $\sqrt{T}$ para os coeficientes associados √†s vari√°veis estacion√°rias e $T^{3/2}$ para o coeficiente da tend√™ncia temporal, desempenha um papel fundamental na determina√ß√£o da distribui√ß√£o assint√≥tica dos estimadores. Esta matriz garante que os termos correspondentes aos coeficientes que convergem mais rapidamente sejam amplificados, enquanto os termos correspondentes aos coeficientes que convergem mais lentamente sejam mantidos em uma escala compar√°vel.

√â importante observar que, embora os estimadores transformados $b^*$ sejam uma fun√ß√£o dos coeficientes originais $b$, a distribui√ß√£o assint√≥tica de $b$ pode ser recuperada atrav√©s da rela√ß√£o $b = G'b^*$ [^3, 16.3.12]. A transforma√ß√£o para isolar as diferentes taxas de converg√™ncia nos permite obter a distribui√ß√£o assint√≥tica da regress√£o transformada, o que, por sua vez, fornece a distribui√ß√£o assint√≥tica dos coeficientes originais.

**Lema 1.1:**  A inversa da matriz de transforma√ß√£o $G$, denotada por $G = (G')^{-1}$, √© igualmente crucial para recuperar os coeficientes originais a partir dos coeficientes transformados.  A estrutura espec√≠fica de $G$ depende da transforma√ß√£o particular utilizada, mas sua finalidade √© desfazer as mudan√ßas de escala e centraliza√ß√£o introduzidas pela transforma√ß√£o de Sims, Stock e Watson.

A an√°lise detalhada no Ap√™ndice 16.A [^3] revela que:
* Os coeficientes dos termos autorregressivos ($\phi_i^*$) convergem a uma taxa de $\sqrt{T}$.
* O coeficiente da tend√™ncia temporal ($Œ¥^*$) converge a uma taxa de $T^{3/2}$.
* O intercepto ($\alpha^*$) tamb√©m converge a uma taxa de $\sqrt{T}$.

Este resultado generaliza o resultado obtido para o modelo simples de tend√™ncia temporal na se√ß√£o anterior [^1], onde se verificou que $\hat{\alpha}_T$ convergeva a $\sqrt{T}$ e $\delta_T$ a $T^{3/2}$ [^1].

**Teorema 1:** (Distribui√ß√£o Assint√≥tica dos Estimadores Originais) Dado que $\beta^* = (G')^{-1}\beta$ e $b = G'b^*$, e que a distribui√ß√£o assint√≥tica de $b^*$ √© dada por $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$, ent√£o a distribui√ß√£o assint√≥tica de $b$ √© dada por:
$Y_T(G'b^* - G'\beta^*) \xrightarrow{d} N(0, \sigma^2 G'[Q^*]^{-1}G)$
onde $G = (G')^{-1}$.

*Prova:* Provaremos que, dado que $\beta^* = (G')^{-1}\beta$ e $b = G'b^*$, e que a distribui√ß√£o assint√≥tica de $b^*$ √© dada por $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$, ent√£o a distribui√ß√£o assint√≥tica de $b$ √© dada por:
$Y_T(G'b^* - G'\beta^*) \xrightarrow{d} N(0, \sigma^2 G'[Q^*]^{-1}G)$, onde $G = (G')^{-1}$.

I.  Come√ßamos com a distribui√ß√£o assint√≥tica de $b^*$:
    $$Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$$

II. Multiplicamos ambos os lados da express√£o por $G'$:
    $$G'Y_T(b^* - \beta^*) \xrightarrow{d} G'N(0, \sigma^2 [Q^*]^{-1})$$
    Note que $Y_T$ √© uma matriz diagonal e, portanto, pode ser movida atrav√©s da multiplica√ß√£o da matriz $G'$.

III. Distribu√≠mos $G'$ dentro dos par√™nteses:
    $$Y_T(G'b^* - G'\beta^*) \xrightarrow{d} N(0, G'\sigma^2 [Q^*]^{-1}G)$$

IV.  Sabemos que $b = G'b^*$ e $\beta = G'\beta^*$. Substituindo na express√£o:
$$Y_T(b - \beta) \xrightarrow{d} N(0, \sigma^2 G'[Q^*]^{-1}G)$$

V. Portanto, a distribui√ß√£o assint√≥tica de $b$ √© dada por:
    $$Y_T(b - \beta) \xrightarrow{d} N(0, \sigma^2 G'[Q^*]^{-1}G)$$‚ñ†

> üí° **Exemplo Num√©rico:**
> Vamos supor que, em uma regress√£o com um intercepto, uma tend√™ncia e um termo AR(1), estimamos os coeficientes transformados $b^* = [\hat{\alpha}^*, \hat{\phi}_1^*, \hat{\delta}^*]^T = [1.5, 0.6, 0.4]^T$. A matriz de transforma√ß√£o inversa $G$ (que √© o inverso de $G'$), pode ser usada para recuperar os coeficientes originais $b$. Suponha que $G'$ tenha sido calculada como:
> $$ G' = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -5 & 0 & 1 \end{bmatrix} $$
> Portanto, $G$ √© o inverso de $G'$, que pode ser calculado como:
>  $$ G = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 5 & 0 & 1 \end{bmatrix} $$
>
> Os coeficientes originais s√£o obtidos por: $b = G'b^*$. Para o exemplo, $b = [1.5, 0.6, -5*1.5+0.4]^T = [1.5, 0.6, -7.1]^T$. Contudo, √© importante notar que este valor √© ilustrativo e, na pr√°tica, os valores da transforma√ß√£o e seus inversos seriam calculados com base na s√©rie temporal utilizada.
>
> Para a matriz de covari√¢ncia assint√≥tica, suponhamos que $\sigma^2 = 0.2$ e $[Q^*]^{-1}$ √© estimado como:
> $$ [Q^*]^{-1} = \begin{bmatrix} 0.1 & 0.02 & 0.01 \\ 0.02 & 0.05 & 0.005 \\ 0.01 & 0.005 & 0.02 \end{bmatrix} $$
> Ent√£o, a matriz de covari√¢ncia assint√≥tica dos estimadores originais √©:
> $\sigma^2 G'[Q^*]^{-1}G = 0.2 * G' [Q^*]^{-1} G$
> onde $G = (G')^{-1}$.
>
> Calculando:
>  $$ G'[Q^*]^{-1} =  \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -5 & 0 & 1 \end{bmatrix} \begin{bmatrix} 0.1 & 0.02 & 0.01 \\ 0.02 & 0.05 & 0.005 \\ 0.01 & 0.005 & 0.02 \end{bmatrix} = \begin{bmatrix} 0.1 & 0.02 & 0.01 \\ 0.02 & 0.05 & 0.005 \\ -0.49 & -0.095 & -0.03 \end{bmatrix} $$
>
> $$ G'[Q^*]^{-1}G =  \begin{bmatrix} 0.1 & 0.02 & 0.01 \\ 0.02 & 0.05 & 0.005 \\ -0.49 & -0.095 & -0.03 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 5 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 0.15 & 0.02 & 0.01 \\ 0.045 & 0.05 & 0.005 \\ -0.64 & -0.095 & -0.03 \end{bmatrix} $$
>
> Portanto, a matriz de covari√¢ncia assint√≥tica √©:
>
> $0.2 *  \begin{bmatrix} 0.15 & 0.02 & 0.01 \\ 0.045 & 0.05 & 0.005 \\ -0.64 & -0.095 & -0.03 \end{bmatrix} =  \begin{bmatrix} 0.03 & 0.004 & 0.002 \\ 0.009 & 0.01 & 0.001 \\ -0.128 & -0.019 & -0.006 \end{bmatrix} $
>
> Esta matriz cont√©m informa√ß√µes sobre as vari√¢ncias e covari√¢ncias dos estimadores originais, permitindo infer√™ncia estat√≠stica.

**Corol√°rio 1.1:** A matriz $G'[Q^*]^{-1}G$ no Teorema 1 representa a matriz de covari√¢ncia assint√≥tica dos estimadores originais $b$ ap√≥s o devido ajuste pelas taxas de converg√™ncia diferenciadas expressas em $Y_T$. O conhecimento dessa matriz permite infer√™ncias estat√≠sticas sobre os par√¢metros originais $\beta$.

**Proposi√ß√£o 1:** A transforma√ß√£o de Sims, Stock e Watson n√£o apenas simplifica a an√°lise assint√≥tica, mas tamb√©m permite que a distribui√ß√£o dos estimadores seja abordada de maneira mais precisa, especialmente em modelos com tend√™ncias temporais determin√≠sticas. A chave √© a utiliza√ß√£o da matriz $Y_T$ para equilibrar as diferentes taxas de converg√™ncia.

**Teorema 1.1** (Consist√™ncia dos Estimadores) Sob as condi√ß√µes usuais de regularidade para estimadores de MQO e a transforma√ß√£o de Sims, Stock e Watson, os estimadores originais $b$ s√£o consistentes, ou seja,  $b \xrightarrow{p} \beta$ quando $T \to \infty$.

*Prova*: Sabemos que $b = G'b^*$. Pela consist√™ncia dos estimadores transformados, temos que $b^* \xrightarrow{p} \beta^*$. Como $G'$ √© uma matriz de transforma√ß√£o fixa, a consist√™ncia √© preservada pela multiplica√ß√£o por $G'$, e assim, $G'b^* \xrightarrow{p} G'\beta^*$. Substituindo as rela√ß√µes $b=G'b^*$ e $\beta=G'\beta^*$, obtemos que $b \xrightarrow{p} \beta$.
<!-- Nota: Aqui, usamos que se $X_n \xrightarrow{p} X$ e $c$ √© uma constante, ent√£o $cX_n \xrightarrow{p} cX$. Al√©m disso, se $A$ √© uma matriz de constantes, ent√£o $AX_n \xrightarrow{p} AX$. -->
‚ñ†

> üí° **Exemplo Num√©rico:**
> Considere um cen√°rio simulado onde os verdadeiros coeficientes de um modelo com tend√™ncia temporal s√£o $\beta = [2, 0.7, 0.3]^T$ (intercepto, AR(1), tend√™ncia). Realizamos simula√ß√µes com tamanhos de amostra crescentes ($T = 100, 500, 1000, 5000$) e estimamos os coeficientes usando a transforma√ß√£o de Sims, Stock e Watson. Para cada amostra, obtemos estimativas $b$. Podemos observar que, √† medida que $T$ aumenta, os estimadores $b$ convergem para os valores verdadeiros $\beta$. Isto ilustra a consist√™ncia dos estimadores de MQO.
>
> | T    | $\hat{\alpha}$ | $\hat{\phi_1}$ | $\hat{\delta}$ |
> |------|-----------------|-----------------|-----------------|
> | 100  | 2.34            | 0.65            | 0.28            |
> | 500  | 2.12            | 0.68            | 0.29            |
> | 1000 | 2.05            | 0.69            | 0.30            |
> | 5000 | 2.01            | 0.70            | 0.30            |
>
> Este exemplo mostra como os estimadores se tornam mais precisos √† medida que o tamanho da amostra aumenta, demonstrando a propriedade de consist√™ncia.

**Observa√ß√£o 2:** A consist√™ncia dos estimadores originais $b$ √© uma propriedade importante, pois garante que, com um n√∫mero suficientemente grande de observa√ß√µes, os estimadores convergir√£o para os valores verdadeiros dos par√¢metros.

### Conclus√£o

A transforma√ß√£o dos regressores proposta por Sims, Stock e Watson [^1, ^2] √© uma ferramenta poderosa para analisar modelos com tend√™ncias temporais determin√≠sticas. Ao isolar os componentes com diferentes taxas de converg√™ncia, essa transforma√ß√£o simplifica a deriva√ß√£o das distribui√ß√µes assint√≥ticas dos estimadores de MQO. Especificamente, a distribui√ß√£o assint√≥tica dos estimadores da regress√£o transformada √© multivariada normal, com o coeficiente da tend√™ncia temporal ($Œ¥^*$) convergindo a uma taxa $T^{3/2}$ e os demais coeficientes convergindo a uma taxa $\sqrt{T}$ [^3]. A distribui√ß√£o assint√≥tica dos estimadores originais pode ser recuperada a partir dos resultados obtidos para os estimadores transformados. Este resultado √© crucial para a realiza√ß√£o de infer√™ncias estat√≠sticas v√°lidas em modelos com tend√™ncias temporais determin√≠sticas, que ser√£o exploradas em mais detalhes no cap√≠tulo seguinte [^3].

### Refer√™ncias
[^1]: Cap√≠tulo 16, se√ß√£o 16.1 do texto original.
[^2]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." Econometrica 58:113-44.
[^3]: Cap√≠tulo 16, se√ß√£o 16.3 e Ap√™ndice 16.A do texto original.
<!-- END -->
