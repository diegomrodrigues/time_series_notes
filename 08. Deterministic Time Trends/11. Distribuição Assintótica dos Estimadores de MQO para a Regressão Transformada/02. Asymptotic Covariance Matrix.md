## DistribuiÃ§Ã£o AssintÃ³tica dos Estimadores de MQO para a RegressÃ£o Transformada

### IntroduÃ§Ã£o
Como vimos anteriormente, no capÃ­tulo sobre processos com tendÃªncias temporais determinÃ­sticas, os estimadores de MÃ­nimos Quadrados OrdinÃ¡rios (MQO) para modelos com tendÃªncias temporais apresentam diferentes taxas de convergÃªncia [^1]. Enquanto estimadores em modelos com variÃ¡veis estacionÃ¡rias geralmente convergem a uma taxa de $\sqrt{T}$, coeficientes associados a tendÃªncias temporais determinÃ­sticas podem convergir mais rapidamente. Em particular, discutimos a necessidade de reescalonar as variÃ¡veis para acomodar essas diferentes taxas de convergÃªncia [^1]. Este capÃ­tulo expande esses conceitos, explorando a distribuiÃ§Ã£o assintÃ³tica dos estimadores de MQO para uma regressÃ£o transformada, com foco em processos autorregressivos ao redor de uma tendÃªncia temporal determinÃ­stica.

### Conceitos Fundamentais

A seÃ§Ã£o anterior [^1] introduziu a ideia de diferentes taxas de convergÃªncia para estimadores em modelos com tendÃªncias temporais. Em particular, foi demonstrado que para um modelo simples de tendÃªncia temporal, o estimador do coeficiente da tendÃªncia ($\delta_T$) converge a uma taxa de $T^{3/2}$, enquanto o estimador do intercepto ($\hat{\alpha}_T$) converge a uma taxa de $\sqrt{T}$ [^1]. A transformaÃ§Ã£o de variÃ¡veis proposta por Sims, Stock e Watson (1990) [^1, ^2] visa isolar essas diferentes taxas de convergÃªncia, facilitando a derivaÃ§Ã£o da distribuiÃ§Ã£o assintÃ³tica dos estimadores.

Para um processo autorregressivo geral ao redor de uma tendÃªncia temporal determinÃ­stica, dado por:
$$y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t$$ [^3, 16.3.1],
a transformaÃ§Ã£o proposta reescreve o modelo na forma:
$$y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \ldots + \phi_p^* y_{t-p}^* + \epsilon_t$$ [^3, 16.3.3],
onde os novos regressores $y_{t-j}^*$ representam variÃ¡veis com mÃ©dia zero, e $\alpha^*$ e $\delta^*$ sÃ£o coeficientes transformados [^3]. O objetivo dessa transformaÃ§Ã£o Ã© separar os componentes do vetor de coeficientes que apresentam diferentes taxas de convergÃªncia.

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Considere um modelo AR(1) com tendÃªncia temporal: $y_t = 2 + 0.5t + 0.7y_{t-1} + \epsilon_t$. ApÃ³s a transformaÃ§Ã£o de Sims, Stock e Watson, podemos ter um modelo como: $y_t = \alpha^* + \delta^*t + \phi_1^*y_{t-1}^* + \epsilon_t$, onde $\alpha^*$, $\delta^*$ e $\phi_1^*$ sÃ£o os coeficientes transformados e $y_{t-1}^*$ Ã© a variÃ¡vel defasada transformada. A transformaÃ§Ã£o garante que as variÃ¡veis $y_{t-1}^*$ tenham mÃ©dia zero.  Suponha que apÃ³s a transformaÃ§Ã£o, os valores estimados para os coeficientes sejam: $\hat{\alpha}^*=1.5$, $\hat{\delta}^*=0.4$ e $\hat{\phi}_1^*=0.6$. Este Ã© um exemplo ilustrativo de como o modelo Ã© reescrito e os parÃ¢metros sÃ£o transformados para facilitar a anÃ¡lise assintÃ³tica.

**ObservaÃ§Ã£o 1:** A transformaÃ§Ã£o que leva de $y_t$ para $y_t^*$ nÃ£o Ã© Ãºnica. Em geral, qualquer transformaÃ§Ã£o que resulte em regressores com mÃ©dia zero e que preserve a estrutura de dependÃªncia temporal dos dados Ã© adequada para essa anÃ¡lise. O ponto crucial Ã© que a transformaÃ§Ã£o deve ser escolhida de forma que os estimadores dos parÃ¢metros transformados apresentem taxas de convergÃªncia assintÃ³ticas distintas, facilitando a anÃ¡lise.

A matriz de transformaÃ§Ã£o $G'$ ([^3, 16.3.8]) Ã© utilizada para obter os coeficientes transformados $\beta^*$ a partir dos coeficientes originais $\beta$ [^3, 16.3.7]:

$$\beta^* = (G')^{-1}\beta$$

O estimador dos coeficientes transformados, $b^*$, Ã© obtido por meio de uma regressÃ£o de mÃ­nimos quadrados ordinÃ¡rios (MQO) de $y_t$ em $x_t^*$ [^3, 16.3.7], onde $x_t^*$ Ã© um vetor de regressores transformados [^3, 16.3.9].

O ApÃªndice 16.A [^3] demonstra que a distribuiÃ§Ã£o assintÃ³tica de $b^*$ Ã© dada por:
$$Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$$ [^3, 16.3.13]

A matriz $Y_T$ Ã© definida como uma matriz diagonal com elementos $\sqrt{T}$ para os primeiros $p$ elementos, correspondentes aos coeficientes das variÃ¡veis defasadas $y_{t-j}^*$  e do intercepto $\alpha^*$, e $T^{3/2}$ para o elemento correspondente ao coeficiente da tendÃªncia temporal $\delta^*$, e zeros nas outras posiÃ§Ãµes [^3, 16.3.14]. Esta matriz Ã© crucial para expressar as diferentes taxas de convergÃªncia dos parÃ¢metros. A matriz de covariÃ¢ncia assintÃ³tica dos estimadores transformados Ã© dada por $\sigma^2 [Q^*]^{-1}$, onde $Q^*$ Ã© a matriz limite das variÃ¡veis transformadas.

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Suponha que temos $T=100$ observaÃ§Ãµes. A matriz $Y_T$ para um modelo com intercepto, tendÃªncia e um termo AR(1) seria:
>
> $$ Y_T = \begin{bmatrix} \sqrt{100} & 0 & 0 \\ 0 & \sqrt{100} & 0 \\ 0 & 0 & 100^{3/2} \end{bmatrix} = \begin{bmatrix} 10 & 0 & 0 \\ 0 & 10 & 0 \\ 0 & 0 & 1000 \end{bmatrix} $$
>
> Aqui, a diagonal mostra as taxas de convergÃªncia para o intercepto, o coeficiente AR(1) e a tendÃªncia temporal, respectivamente. Note que a taxa de convergÃªncia da tendÃªncia temporal (1000) Ã© muito maior do que a taxa dos outros dois parÃ¢metros (10). Isso reflete o fato de que o estimador da tendÃªncia converge mais rapidamente.
>
> Para ilustrar ainda mais, suponha que tenhamos um vetor de estimadores transformados $b^* = [1.2, 0.8, 0.1]$ (intercepto, AR(1), tendÃªncia). A matriz $Y_T$ escala cada um desses estimadores conforme sua taxa de convergÃªncia. Assim, $Y_T b^* = [10*1.2, 10*0.8, 1000*0.1] = [12, 8, 100]$.  Esta operaÃ§Ã£o Ã© crucial para a anÃ¡lise assintÃ³tica, pois ao escalar, estamos lidando com variÃ¡veis que tÃªm a mesma ordem de magnitude assintoticamente, o que permite derivar resultados estatÃ­sticos consistentes.

**Lema 1:** A matriz $Y_T$, definida como uma matriz diagonal com elementos $\sqrt{T}$ para os coeficientes associados Ã s variÃ¡veis estacionÃ¡rias e $T^{3/2}$ para o coeficiente da tendÃªncia temporal, desempenha um papel fundamental na determinaÃ§Ã£o da distribuiÃ§Ã£o assintÃ³tica dos estimadores. Esta matriz garante que os termos correspondentes aos coeficientes que convergem mais rapidamente sejam amplificados, enquanto os termos correspondentes aos coeficientes que convergem mais lentamente sejam mantidos em uma escala comparÃ¡vel.

Ã‰ importante observar que, embora os estimadores transformados $b^*$ sejam uma funÃ§Ã£o dos coeficientes originais $b$, a distribuiÃ§Ã£o assintÃ³tica de $b$ pode ser recuperada atravÃ©s da relaÃ§Ã£o $b = G'b^*$ [^3, 16.3.12]. A transformaÃ§Ã£o para isolar as diferentes taxas de convergÃªncia nos permite obter a distribuiÃ§Ã£o assintÃ³tica da regressÃ£o transformada, o que, por sua vez, fornece a distribuiÃ§Ã£o assintÃ³tica dos coeficientes originais.

**Lema 1.1:**  A inversa da matriz de transformaÃ§Ã£o $G$, denotada por $G = (G')^{-1}$, Ã© igualmente crucial para recuperar os coeficientes originais a partir dos coeficientes transformados.  A estrutura especÃ­fica de $G$ depende da transformaÃ§Ã£o particular utilizada, mas sua finalidade Ã© desfazer as mudanÃ§as de escala e centralizaÃ§Ã£o introduzidas pela transformaÃ§Ã£o de Sims, Stock e Watson.

A anÃ¡lise detalhada no ApÃªndice 16.A [^3] revela que:
* Os coeficientes dos termos autorregressivos ($\phi_i^*$) convergem a uma taxa de $\sqrt{T}$.
* O coeficiente da tendÃªncia temporal ($Î´^*$) converge a uma taxa de $T^{3/2}$.
* O intercepto ($\alpha^*$) tambÃ©m converge a uma taxa de $\sqrt{T}$.

Este resultado generaliza o resultado obtido para o modelo simples de tendÃªncia temporal na seÃ§Ã£o anterior [^1], onde se verificou que $\hat{\alpha}_T$ convergeva a $\sqrt{T}$ e $\delta_T$ a $T^{3/2}$ [^1].

**Teorema 1:** (DistribuiÃ§Ã£o AssintÃ³tica dos Estimadores Originais) Dado que $\beta^* = (G')^{-1}\beta$ e $b = G'b^*$, e que a distribuiÃ§Ã£o assintÃ³tica de $b^*$ Ã© dada por $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$, entÃ£o a distribuiÃ§Ã£o assintÃ³tica de $b$ Ã© dada por:
$Y_T(G'b^* - G'\beta^*) \xrightarrow{d} N(0, \sigma^2 G'[Q^*]^{-1}G)$
onde $G = (G')^{-1}$.

*Prova:* Provaremos que, dado que $\beta^* = (G')^{-1}\beta$ e $b = G'b^*$, e que a distribuiÃ§Ã£o assintÃ³tica de $b^*$ Ã© dada por $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$, entÃ£o a distribuiÃ§Ã£o assintÃ³tica de $b$ Ã© dada por:
$Y_T(G'b^* - G'\beta^*) \xrightarrow{d} N(0, \sigma^2 G'[Q^*]^{-1}G)$, onde $G = (G')^{-1}$.

I.  ComeÃ§amos com a distribuiÃ§Ã£o assintÃ³tica de $b^*$:
    $$Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$$

II. Multiplicamos ambos os lados da expressÃ£o por $G'$:
    $$G'Y_T(b^* - \beta^*) \xrightarrow{d} G'N(0, \sigma^2 [Q^*]^{-1})$$
    Note que $Y_T$ Ã© uma matriz diagonal e, portanto, pode ser movida atravÃ©s da multiplicaÃ§Ã£o da matriz $G'$.

III. DistribuÃ­mos $G'$ dentro dos parÃªnteses:
    $$Y_T(G'b^* - G'\beta^*) \xrightarrow{d} N(0, G'\sigma^2 [Q^*]^{-1}G)$$

IV.  Sabemos que $b = G'b^*$ e $\beta = G'\beta^*$. Substituindo na expressÃ£o:
$$Y_T(b - \beta) \xrightarrow{d} N(0, \sigma^2 G'[Q^*]^{-1}G)$$

V. Portanto, a distribuiÃ§Ã£o assintÃ³tica de $b$ Ã© dada por:
    $$Y_T(b - \beta) \xrightarrow{d} N(0, \sigma^2 G'[Q^*]^{-1}G)$$â– 

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Vamos supor que, em uma regressÃ£o com um intercepto, uma tendÃªncia e um termo AR(1), estimamos os coeficientes transformados $b^* = [\hat{\alpha}^*, \hat{\phi}_1^*, \hat{\delta}^*]^T = [1.5, 0.6, 0.4]^T$. A matriz de transformaÃ§Ã£o inversa $G$ (que Ã© o inverso de $G'$), pode ser usada para recuperar os coeficientes originais $b$. Suponha que $G'$ tenha sido calculada como:
> $$ G' = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -5 & 0 & 1 \end{bmatrix} $$
> Portanto, $G$ Ã© o inverso de $G'$, que pode ser calculado como:
>  $$ G = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 5 & 0 & 1 \end{bmatrix} $$
>
> Os coeficientes originais sÃ£o obtidos por: $b = G'b^*$. Para o exemplo, $b = [1.5, 0.6, -5*1.5+0.4]^T = [1.5, 0.6, -7.1]^T$. Contudo, Ã© importante notar que este valor Ã© ilustrativo e, na prÃ¡tica, os valores da transformaÃ§Ã£o e seus inversos seriam calculados com base na sÃ©rie temporal utilizada.
>
> Para a matriz de covariÃ¢ncia assintÃ³tica, suponhamos que $\sigma^2 = 0.2$ e $[Q^*]^{-1}$ Ã© estimado como:
> $$ [Q^*]^{-1} = \begin{bmatrix} 0.1 & 0.02 & 0.01 \\ 0.02 & 0.05 & 0.005 \\ 0.01 & 0.005 & 0.02 \end{bmatrix} $$
> EntÃ£o, a matriz de covariÃ¢ncia assintÃ³tica dos estimadores originais Ã©:
> $\sigma^2 G'[Q^*]^{-1}G = 0.2 * G' [Q^*]^{-1} G$
> onde $G = (G')^{-1}$.
>
> Calculando:
>  $$ G'[Q^*]^{-1} =  \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -5 & 0 & 1 \end{bmatrix} \begin{bmatrix} 0.1 & 0.02 & 0.01 \\ 0.02 & 0.05 & 0.005 \\ 0.01 & 0.005 & 0.02 \end{bmatrix} = \begin{bmatrix} 0.1 & 0.02 & 0.01 \\ 0.02 & 0.05 & 0.005 \\ -0.49 & -0.095 & -0.03 \end{bmatrix} $$
>
> $$ G'[Q^*]^{-1}G =  \begin{bmatrix} 0.1 & 0.02 & 0.01 \\ 0.02 & 0.05 & 0.005 \\ -0.49 & -0.095 & -0.03 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 5 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 0.15 & 0.02 & 0.01 \\ 0.045 & 0.05 & 0.005 \\ -0.64 & -0.095 & -0.03 \end{bmatrix} $$
>
> Portanto, a matriz de covariÃ¢ncia assintÃ³tica Ã©:
>
> $0.2 *  \begin{bmatrix} 0.15 & 0.02 & 0.01 \\ 0.045 & 0.05 & 0.005 \\ -0.64 & -0.095 & -0.03 \end{bmatrix} =  \begin{bmatrix} 0.03 & 0.004 & 0.002 \\ 0.009 & 0.01 & 0.001 \\ -0.128 & -0.019 & -0.006 \end{bmatrix} $
>
> Esta matriz contÃ©m informaÃ§Ãµes sobre as variÃ¢ncias e covariÃ¢ncias dos estimadores originais, permitindo inferÃªncia estatÃ­stica.

**CorolÃ¡rio 1.1:** A matriz $G'[Q^*]^{-1}G$ no Teorema 1 representa a matriz de covariÃ¢ncia assintÃ³tica dos estimadores originais $b$ apÃ³s o devido ajuste pelas taxas de convergÃªncia diferenciadas expressas em $Y_T$. O conhecimento dessa matriz permite inferÃªncias estatÃ­sticas sobre os parÃ¢metros originais $\beta$.

**ProposiÃ§Ã£o 1:** A transformaÃ§Ã£o de Sims, Stock e Watson nÃ£o apenas simplifica a anÃ¡lise assintÃ³tica, mas tambÃ©m permite que a distribuiÃ§Ã£o dos estimadores seja abordada de maneira mais precisa, especialmente em modelos com tendÃªncias temporais determinÃ­sticas. A chave Ã© a utilizaÃ§Ã£o da matriz $Y_T$ para equilibrar as diferentes taxas de convergÃªncia.

**Teorema 1.1** (ConsistÃªncia dos Estimadores) Sob as condiÃ§Ãµes usuais de regularidade para estimadores de MQO e a transformaÃ§Ã£o de Sims, Stock e Watson, os estimadores originais $b$ sÃ£o consistentes, ou seja,  $b \xrightarrow{p} \beta$ quando $T \to \infty$.

*Prova*: Sabemos que $b = G'b^*$. Pela consistÃªncia dos estimadores transformados, temos que $b^* \xrightarrow{p} \beta^*$. Como $G'$ Ã© uma matriz de transformaÃ§Ã£o fixa, a consistÃªncia Ã© preservada pela multiplicaÃ§Ã£o por $G'$, e assim, $G'b^* \xrightarrow{p} G'\beta^*$. Substituindo as relaÃ§Ãµes $b=G'b^*$ e $\beta=G'\beta^*$, obtemos que $b \xrightarrow{p} \beta$.
<!-- Nota: Aqui, usamos que se $X_n \xrightarrow{p} X$ e $c$ Ã© uma constante, entÃ£o $cX_n \xrightarrow{p} cX$. AlÃ©m disso, se $A$ Ã© uma matriz de constantes, entÃ£o $AX_n \xrightarrow{p} AX$. -->
â– 

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Considere um cenÃ¡rio simulado onde os verdadeiros coeficientes de um modelo com tendÃªncia temporal sÃ£o $\beta = [2, 0.7, 0.3]^T$ (intercepto, AR(1), tendÃªncia). Realizamos simulaÃ§Ãµes com tamanhos de amostra crescentes ($T = 100, 500, 1000, 5000$) e estimamos os coeficientes usando a transformaÃ§Ã£o de Sims, Stock e Watson. Para cada amostra, obtemos estimativas $b$. Podemos observar que, Ã  medida que $T$ aumenta, os estimadores $b$ convergem para os valores verdadeiros $\beta$. Isto ilustra a consistÃªncia dos estimadores de MQO.
>
> | T    | $\hat{\alpha}$ | $\hat{\phi_1}$ | $\hat{\delta}$ |
> |------|-----------------|-----------------|-----------------|
> | 100  | 2.34            | 0.65            | 0.28            |
> | 500  | 2.12            | 0.68            | 0.29            |
> | 1000 | 2.05            | 0.69            | 0.30            |
> | 5000 | 2.01            | 0.70            | 0.30            |
>
> Este exemplo mostra como os estimadores se tornam mais precisos Ã  medida que o tamanho da amostra aumenta, demonstrando a propriedade de consistÃªncia.

**ObservaÃ§Ã£o 2:** A consistÃªncia dos estimadores originais $b$ Ã© uma propriedade importante, pois garante que, com um nÃºmero suficientemente grande de observaÃ§Ãµes, os estimadores convergirÃ£o para os valores verdadeiros dos parÃ¢metros.

### ConclusÃ£o

A transformaÃ§Ã£o dos regressores proposta por Sims, Stock e Watson [^1, ^2] Ã© uma ferramenta poderosa para analisar modelos com tendÃªncias temporais determinÃ­sticas. Ao isolar os componentes com diferentes taxas de convergÃªncia, essa transformaÃ§Ã£o simplifica a derivaÃ§Ã£o das distribuiÃ§Ãµes assintÃ³ticas dos estimadores de MQO. Especificamente, a distribuiÃ§Ã£o assintÃ³tica dos estimadores da regressÃ£o transformada Ã© multivariada normal, com o coeficiente da tendÃªncia temporal ($Î´^*$) convergindo a uma taxa $T^{3/2}$ e os demais coeficientes convergindo a uma taxa $\sqrt{T}$ [^3]. A distribuiÃ§Ã£o assintÃ³tica dos estimadores originais pode ser recuperada a partir dos resultados obtidos para os estimadores transformados. Este resultado Ã© crucial para a realizaÃ§Ã£o de inferÃªncias estatÃ­sticas vÃ¡lidas em modelos com tendÃªncias temporais determinÃ­sticas, que serÃ£o exploradas em mais detalhes no capÃ­tulo seguinte [^3].

### ReferÃªncias
[^1]: CapÃ­tulo 16, seÃ§Ã£o 16.1 do texto original.
[^2]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." Econometrica 58:113-44.
[^3]: CapÃ­tulo 16, seÃ§Ã£o 16.3 e ApÃªndice 16.A do texto original.
<!-- END -->
