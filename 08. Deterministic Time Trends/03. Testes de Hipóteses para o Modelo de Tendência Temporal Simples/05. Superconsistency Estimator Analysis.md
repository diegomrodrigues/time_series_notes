## An√°lise da Superconsist√™ncia e Testes de Hip√≥teses com Diferentes Taxas de Converg√™ncia

### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise dos testes de hip√≥teses em modelos de tend√™ncia temporal determin√≠stica, com foco na superconsist√™ncia do estimador do coeficiente de tend√™ncia ($\delta$) e nas implica√ß√µes desta propriedade para a realiza√ß√£o de testes envolvendo restri√ß√µes lineares sobre os par√¢metros. Especificamente, exploraremos como a superconsist√™ncia do estimador $\hat{\delta}_T$ influencia os testes de hip√≥teses, mostrando que, em testes envolvendo restri√ß√µes sobre par√¢metros com diferentes taxas de converg√™ncia, o par√¢metro com a menor taxa de converg√™ncia domina o comportamento assint√≥tico do teste. Adicionalmente, vamos investigar o comportamento de testes conjuntos sobre $\alpha$ e $\delta$ utilizando o teste de Wald, e comprovar que os resultados mant√™m a validade assint√≥tica dos testes de OLS usuais [^1].

### Superconsist√™ncia do Estimador de Tend√™ncia ($\hat{\delta}_T$)

Como discutido em cap√≠tulos anteriores, no modelo de tend√™ncia temporal simples

$$y_t = \alpha + \delta t + \epsilon_t,$$

onde $\epsilon_t$ √© um processo de ru√≠do branco, os estimadores de M√≠nimos Quadrados Ordin√°rios (OLS), $\hat{\alpha}_T$ e $\hat{\delta}_T$, convergem para seus valores verdadeiros, $\alpha$ e $\delta$, com diferentes taxas: $\sqrt{T}$ para $\hat{\alpha}_T$ e $T^{3/2}$ para $\hat{\delta}_T$ [^1, 6]. A propriedade de **superconsist√™ncia** de $\hat{\delta}_T$ se manifesta pelo fato de que $T(\hat{\delta}_T - \delta) \xrightarrow{p} 0$, ou seja, $\hat{\delta}_T$ converge para $\delta$ mais r√°pido que um estimador consistente usual. Isso implica que, mesmo que $\hat{\delta}_T$ convirja para $\delta$ em uma taxa muito r√°pida, os testes de hip√≥teses lineares s√£o dominados assintoticamente pelos par√¢metros com as taxas de converg√™ncia mais lentas, como $\alpha$. A taxa de converg√™ncia de $\hat{\delta}_T$ √©  $T^{3/2}$, o que torna este estimador superconsistente.

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar a superconsist√™ncia de $\hat{\delta}_T$, vamos simular dados de um modelo de tend√™ncia temporal com $\alpha=2$, $\delta=0.5$ e $\sigma^2=1$. Simularemos dados para $T = 100$, $T = 1000$ e $T = 10000$ e ajustaremos modelos de regress√£o. Observaremos a diferen√ßa entre os valores estimados e seus valores reais.
> ```python
> import numpy as np
> import statsmodels.api as sm
> import pandas as pd
>
> # Par√¢metros do modelo
> alpha_true = 2
> delta_true = 0.5
> sigma_sq = 1
>
> # Tamanhos de amostra
> T_values = [100, 1000, 10000]
>
> results_df = pd.DataFrame(columns=['T', 'alpha_hat', 'delta_hat', 'T*(delta_hat - delta_true)'])
>
> for T in T_values:
>    # Gera√ß√£o de dados
>    np.random.seed(42)
>    t = np.arange(1, T + 1)
>    epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
>    y = alpha_true + delta_true * t + epsilon
>
>    # Ajuste do modelo OLS
>    X = sm.add_constant(t)
>    model = sm.OLS(y, X)
>    results = model.fit()
>
>    alpha_hat = results.params[0]
>    delta_hat = results.params[1]
>    results_df = results_df.append({'T': T, 'alpha_hat': alpha_hat, 'delta_hat': delta_hat, 'T*(delta_hat - delta_true)': T * (delta_hat - delta_true)}, ignore_index=True)
>
> print(results_df)
> ```
> Ao executar este exemplo, observamos que $\hat{\delta}_T$ se aproxima rapidamente de seu valor verdadeiro ($\delta = 0.5$) √† medida que o tamanho da amostra $T$ aumenta. A coluna 'T*(delta_hat - delta_true)' mostra que, mesmo multiplicando a diferen√ßa entre a estimativa e o valor verdadeiro por $T$, o resultado tende a se aproximar de 0 com o aumento do tamanho da amostra. Isso ilustra a propriedade da superconsist√™ncia, onde a diferen√ßa entre o estimador e o valor verdadeiro converge mais r√°pido do que $\sqrt{T}$, no caso $T^{3/2}$.
>
> **Interpreta√ß√£o:** A propriedade de superconsist√™ncia do estimador $\hat{\delta}_T$ tem implica√ß√µes importantes para testes de hip√≥teses. Em modelos com tend√™ncias temporais determin√≠sticas, o comportamento assint√≥tico dos testes envolvendo m√∫ltiplos par√¢metros √© dominado pelo par√¢metro com a taxa de converg√™ncia mais lenta, neste caso $\alpha$.
  
   **Lema 1**
  Seja $\hat{\theta}_T$ um estimador de $\theta$ com taxa de converg√™ncia $r_T$, ou seja, $r_T(\hat{\theta}_T - \theta) \xrightarrow{d} Z$, onde $Z$ √© uma vari√°vel aleat√≥ria. Se $r_T$ cresce mais rapidamente que $\sqrt{T}$ ent√£o $\hat{\theta}_T$ √© um estimador superconsistente. 
  *Prova:*
  I. Se $r_T$ cresce mais r√°pido que $\sqrt{T}$, existe uma sequ√™ncia $s_T$ tal que $\sqrt{T}s_T \to 0$ e $r_T = s_T \sqrt{T}$.
  II. Ent√£o $r_T(\hat{\theta}_T - \theta) = \sqrt{T}s_T(\hat{\theta}_T - \theta) \xrightarrow{d} Z$.
  III. Como $ \sqrt{T}s_T \to 0$ segue que $\sqrt{T}(\hat{\theta}_T - \theta)  \xrightarrow{p} 0$, mostrando que $\hat{\theta}_T$ converge mais r√°pido que $\sqrt{T}$ para $\theta$ e, portanto, √© superconsistente. $\blacksquare$

### Testes de Hip√≥teses com Restri√ß√µes Lineares

Ao considerarmos o teste de uma hip√≥tese linear sobre os par√¢metros $\alpha$ e $\delta$ da forma:

$$H_0: r_1\alpha + r_2\delta = r,$$

a estat√≠stica t correspondente √© definida como:

$$
t_r = \frac{r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r}{\sqrt{\hat{\sigma}^2 [r_1 \, r_2](X^TX)^{-1} [r_1 \, r_2]^T}}.
$$
Como discutido em cap√≠tulos anteriores, para estudar a validade assint√≥tica desse teste, reescalonamos o numerador e o denominador pela raiz da taxa de converg√™ncia mais lenta, que √© $\sqrt{T}$:

$$
t_r = \frac{\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)}{\sqrt{\hat{\sigma}^2 \sqrt{T}^2 [r_1 \, r_2](X^TX)^{-1} [r_1 \, r_2]^T}}.
$$
Em virtude da superconsist√™ncia de $\hat{\delta}_T$, qualquer restri√ß√£o envolvendo tanto $\alpha$ quanto $\delta$ √© dominada assintoticamente pelo componente com a taxa de converg√™ncia mais lenta, que √© $\alpha$. Isso significa que o comportamento assint√≥tico da estat√≠stica $t_r$ √© o mesmo que ter√≠amos se estiv√©ssemos testando apenas uma restri√ß√£o sobre $\alpha$.

**Lema 3**
Em testes de hip√≥teses lineares sobre par√¢metros com diferentes taxas de converg√™ncia, a estat√≠stica de teste √© dominada assintoticamente pelo par√¢metro com a menor taxa de converg√™ncia.

*Prova:*
I. Seja $t_r$ uma estat√≠stica de teste para a hip√≥tese linear $H_0: r_1\alpha + r_2\delta = r$, como definida acima.
II. O numerador da estat√≠stica $t_r$ cont√©m um termo envolvendo o estimador $\hat{\delta}_T$ que converge a uma taxa $T^{3/2}$. Ao reescalonar o numerador pela taxa de converg√™ncia mais lenta, $\sqrt{T}$, o termo envolvendo  $\hat{\delta}_T$ torna-se assintoticamente desprez√≠vel.
III. O denominador da estat√≠stica de teste, que envolve o erro padr√£o, tamb√©m √© reescalonado pela taxa de converg√™ncia mais lenta,  $\sqrt{T}$.
IV. Como $\hat{\delta}_T$ √© superconsistente, sua contribui√ß√£o para a distribui√ß√£o limite da estat√≠stica $t_r$ torna-se nula em compara√ß√£o com a contribui√ß√£o de $\hat{\alpha}_T$.
V. Portanto, o comportamento assint√≥tico de $t_r$ √© determinado predominantemente pela distribui√ß√£o limite de $\hat{\alpha}_T$, que converge a uma taxa $\sqrt{T}$. Em outras palavras,  $t_r$ converge para uma normal padr√£o $N(0,1)$.  $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Vamos ilustrar a influ√™ncia da taxa de converg√™ncia de $\delta$ simulando dados e testando a hip√≥tese $H_0: \alpha + 2\delta = 3$. Utilizaremos tamanhos de amostra diferentes e analisaremos os p-valores dos testes.
>
> ```python
> import numpy as np
> import statsmodels.api as sm
> from scipy.stats import t
>
> # Par√¢metros do modelo
> alpha_true = 2
> delta_true = 0.5
> sigma_sq = 1
>
> # Tamanhos de amostra
> T_values = [100, 1000, 10000]
>
> results_list = []
> for T in T_values:
>    # Gerando dados
>    np.random.seed(42)
>    t = np.arange(1, T+1)
>    epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
>    y = alpha_true + delta_true * t + epsilon
>
>    # Ajustando o modelo OLS
>    X = sm.add_constant(t)
>    model = sm.OLS(y, X)
>    results = model.fit()
>
>    # Estimativas dos par√¢metros
>    alpha_hat = results.params[0]
>    delta_hat = results.params[1]
>    sigma_hat_sq = results.mse_resid
>
>    # Definindo a hip√≥tese linear
>    r1 = 1
>    r2 = 2
>    r = 3
>
>    # Calculando a estat√≠stica t para a hip√≥tese linear
>    X_mat = np.column_stack((np.ones(T),t))
>    cov_matrix = sigma_hat_sq * np.linalg.inv(X_mat.T @ X_mat)
>    se_linear = np.sqrt(np.array([r1, r2]) @ cov_matrix @ np.array([r1, r2]).T)
>    t_r = (r1 * alpha_hat + r2 * delta_hat - r) / se_linear
>
>    # Grau de liberdade
>    df = T - 2
>
>    # Calculando o p-valor
>    p_valor_r = 2 * (1 - t.cdf(abs(t_r), df))
>
>    results_list.append((T, t_r, p_valor_r))
>
> print("T\tEstat√≠stica t\tP-valor")
> for T, t_r, p_valor_r in results_list:
>    print(f"{T}\t{t_r:.4f}\t\t{p_valor_r:.4f}")
> ```
>
> Analisando os resultados deste exemplo, observamos que, mesmo que $\delta$ convirja mais rapidamente para seu valor verdadeiro, o teste de hip√≥teses linear √© dominado pelo comportamento de $\alpha$, que tem uma taxa de converg√™ncia mais lenta. Os p-valores dos testes demonstram a validade assint√≥tica, mesmo com diferentes taxas de converg√™ncia dos par√¢metros.

### Teste Conjunto de Hip√≥teses com o Teste de Wald

Quando desejamos testar hip√≥teses sobre $\alpha$ e $\delta$ separadamente, podemos utilizar o teste de Wald. A hip√≥tese nula neste caso √© dada por:

$$
H_0:
\begin{bmatrix}
    \alpha \\
    \delta
\end{bmatrix}
=
\begin{bmatrix}
    \alpha_0 \\
    \delta_0
\end{bmatrix}
$$

A estat√≠stica de teste de Wald √© constru√≠da como:

$$
\chi^2 = (\hat{\beta}_T - \beta_0)' [V(\hat{\beta}_T)]^{-1} (\hat{\beta}_T - \beta_0)
$$

onde $\hat{\beta}_T = [\hat{\alpha}_T, \hat{\delta}_T]^T$ e $\beta_0 = [\alpha_0, \delta_0]^T$, e $V(\hat{\beta}_T)$ √© uma estimativa consistente da matriz de covari√¢ncia assint√≥tica de $\hat{\beta}_T$. Esta estat√≠stica converge para uma distribui√ß√£o qui-quadrado com dois graus de liberdade ($\chi^2(2)$) sob a hip√≥tese nula [^12].

> üí° **Exemplo Num√©rico:**
>
> Vamos simular dados e testar a hip√≥tese conjunta $H_0: \alpha = 2$ e $\delta = 0.5$ utilizando o teste de Wald.
>
> ```python
> import numpy as np
> import statsmodels.api as sm
> from scipy.stats import chi2
>
> # Par√¢metros do modelo
> alpha_true = 2
> delta_true = 0.5
> sigma_sq = 1
> T = 1000
>
> # Gerando dados
> np.random.seed(42)
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
> y = alpha_true + delta_true * t + epsilon
>
> # Ajustando o modelo OLS
> X = sm.add_constant(t)
> model = sm.OLS(y, X)
> results = model.fit()
>
> # Estimativas dos par√¢metros
> alpha_hat = results.params[0]
> delta_hat = results.params[1]
> sigma_hat_sq = results.mse_resid
>
> # Hip√≥tese conjunta
> beta_hat = np.array([alpha_hat, delta_hat])
> beta_0 = np.array([2, 0.5])
> X_mat = np.column_stack((np.ones(T),t))
> cov_matrix = sigma_hat_sq * np.linalg.inv(X_mat.T @ X_mat)
>
> # Calculando a estat√≠stica de Wald
> wald_stat = (beta_hat - beta_0) @ np.linalg.inv(cov_matrix) @ (beta_hat - beta_0)
>
> # Grau de liberdade
> df = 2
>
> # Calculando o p-valor
> p_valor_wald = 1 - chi2.cdf(wald_stat, df)
>
> print(f"Estat√≠stica de Wald: {wald_stat:.4f}")
> print(f"P-valor para o teste de Wald: {p_valor_wald:.4f}")
>
> # Compara o p-valor com o n√≠vel de signific√¢ncia
> nivel_significancia = 0.05
> if p_valor_wald < nivel_significancia:
>    print("Rejeita a hip√≥tese nula conjunta H0: alpha = 2 e delta = 0.5")
> else:
>    print("N√£o rejeita a hip√≥tese nula conjunta H0: alpha = 2 e delta = 0.5")
> ```
>
> Este exemplo ilustra o c√°lculo da estat√≠stica de Wald e o seu p-valor associado. Rejeitamos a hip√≥tese nula conjunta se o p-valor for menor do que o n√≠vel de signific√¢ncia estabelecido.
>
> **Interpreta√ß√£o:** A distribui√ß√£o qui-quadrado obtida para o teste de Wald demonstra que, mesmo quando testamos hip√≥teses conjuntas sobre par√¢metros com diferentes taxas de converg√™ncia, os testes de OLS s√£o assintoticamente v√°lidos.

**Teorema 4**
A estat√≠stica de Wald para testar uma hip√≥tese conjunta sobre os par√¢metros $\alpha$ e $\delta$ em modelos de tend√™ncia temporal determin√≠stica, converge em distribui√ß√£o para uma qui-quadrado com dois graus de liberdade sob a hip√≥tese nula.
*Prova:*
I. Sob a hip√≥tese nula, os estimadores OLS $\hat{\alpha}_T$ e $\hat{\delta}_T$ convergem para seus valores verdadeiros $\alpha_0$ e $\delta_0$.
II. A estat√≠stica de Wald, dada por:
$$ \chi^2 = (\hat{\beta}_T - \beta_0)' [V(\hat{\beta}_T)]^{-1} (\hat{\beta}_T - \beta_0) $$
onde $ \hat{\beta}_T = [\hat{\alpha}_T, \hat{\delta}_T]^T$ e $\beta_0 = [\alpha_0, \delta_0]^T$ √© uma forma quadr√°tica, o que a torna apropriada para a distribui√ß√£o qui-quadrado.
III. A matriz de covari√¢ncia assint√≥tica dos estimadores √© dada por $V(\hat{\beta}_T)$ e √© proporcional a $Q^{-1}$, onde $Q$ √© a matriz de covari√¢ncia dos regressores.
IV. Usando o resultado da distribui√ß√£o assint√≥tica dos estimadores reescalonados, e sabendo que $V(\hat{\beta}_T)$ converge para um valor n√£o nulo, conclu√≠mos que a estat√≠stica de Wald converge para uma qui-quadrado com 2 graus de liberdade. $\blacksquare$
  
**Lema 4.1**
Seja $V(\hat{\beta}_T)$ a matriz de covari√¢ncia assint√≥tica de $\hat{\beta}_T = [\hat{\alpha}_T, \hat{\delta}_T]^T$, ent√£o $V(\hat{\beta}_T)$ converge para uma matriz n√£o singular.
*Prova:*
I.  A matriz de covari√¢ncia assint√≥tica $V(\hat{\beta}_T)$  √© proporcional a $(X^TX)^{-1}$ onde $X$ √© a matriz de regressores.
II.  Conforme visto em [16.1.19], quando reescalonamos  $(X^TX)^{-1}$  para estudar o comportamento assint√≥tico dos estimadores  $\hat{\alpha}_T$ e  $\hat{\delta}_T$,  obtemos que
    $$ \left\{ \mathbf{Y}_T \begin{bmatrix} \sum_{t=1}^T \mathbf{x}_t\mathbf{x}_t' \end{bmatrix}^{-1} \mathbf{Y}_T^{-1} \right\} \rightarrow \mathbf{Q}^{-1} $$
    onde $\mathbf{Q}$ √© uma matriz n√£o singular.
III.  Como  $V(\hat{\beta}_T)$  √© proporcional ao inverso da matriz de covari√¢ncia dos regressores, ap√≥s o devido reescalonamento, temos que  $V(\hat{\beta}_T)$  converge para uma matriz n√£o singular  $\mathbf{Q}^{-1}$. $\blacksquare$
  
  **Corol√°rio 4.1** 
Sob as condi√ß√µes do Teorema 4, a estat√≠stica de Wald tem uma distribui√ß√£o assint√≥tica qui-quadrado com dois graus de liberdade.
*Prova:*
I. Pelo Teorema 4, a estat√≠stica de Wald converge em distribui√ß√£o para uma qui-quadrado com 2 graus de liberdade sob a hip√≥tese nula.
II. A prova do Teorema 4, combinada com o Lema 4.1, garante que a matriz de covari√¢ncia utilizada na estat√≠stica de Wald converge para uma matriz n√£o singular.
III. Assim, a estat√≠stica de Wald tem uma distribui√ß√£o assint√≥tica qui-quadrado com dois graus de liberdade. $\blacksquare$

### Implica√ß√µes da Superconsist√™ncia para Testes de Hip√≥teses

A superconsist√™ncia do estimador do coeficiente de tend√™ncia $\hat{\delta}_T$ implica que a distribui√ß√£o assint√≥tica de testes de hip√≥teses lineares envolvendo ambos $\alpha$ e $\delta$ √© dominada pelo comportamento assint√≥tico de $\alpha$. Isto √©, mesmo que $\hat{\delta}_T$ convirja mais rapidamente para seu valor verdadeiro, a distribui√ß√£o do teste √© assintoticamente a mesma que ter√≠amos ao testar apenas uma restri√ß√£o sobre $\alpha$. Isso simplifica a an√°lise, pois podemos focar na distribui√ß√£o do estimador com a menor taxa de converg√™ncia, que √© $\hat{\alpha}_T$.

### Conclus√£o

Neste cap√≠tulo, investigamos a superconsist√™ncia do estimador $\hat{\delta}_T$ e como esta propriedade influencia os testes de hip√≥teses em modelos de tend√™ncia temporal. Verificamos que, mesmo com as diferentes taxas de converg√™ncia dos estimadores, os testes de hip√≥teses lineares s√£o dominados assintoticamente pelos par√¢metros com a menor taxa de converg√™ncia, neste caso $\alpha$. Al√©m disso, mostramos que o teste conjunto de hip√≥teses sobre os par√¢metros $\alpha$ e $\delta$, realizado utilizando a estat√≠stica de Wald, converge para uma distribui√ß√£o qui-quadrado, demonstrando a validade assint√≥tica dos testes de OLS usuais. As conclus√µes deste cap√≠tulo s√£o cruciais para a compreens√£o da infer√™ncia estat√≠stica em modelos com tend√™ncias temporais, e para o desenvolvimento de m√©todos de testes em modelos mais avan√ßados nos cap√≠tulos subsequentes.
### Refer√™ncias
[^1]: The coefficients of regression models involving unit roots or deterministic time trends are typically estimated by ordinary least squares. However, the asymptotic distributions of the coefficient estimates cannot be calculated in the same way as are those for regression models involving stationary variables. Among other difficulties, the estimates of different parameters will in general have different asymptotic rates of convergence.
[^2]: If e, ~ N(0, œÉ¬≤), then the model [16.1.1] satisfies the classical regression assumptions and the standard OLS t or F statistics in equations [8.1.26] and [8.1.32] would have exact small-sample t or F distributions.
[^3]: This chapter introduces this technique, which will prove useful not only for studying time trends but also for analyzing estimators for a variety of nonstationary processes in Chapters 17 and 18.
[^4]: Recall the approach used to find asymptotic distributions for regressions with stationary explanatory variables in Chapter 8.
[^5]: The usual assumption was that (1/T) \sum_1 x_t x_t, converged in probability to a nonsingular matrix Q while (1/\sqrt{T}) \sum_1^T x_t \epsilon_t, converged in distribution to a N(0, \sigma^2 Q) random variable, implying that \sqrt{T}(b+- \beta) N(0, 0^2Q-1).
[^6]: It turns out that the OLS estimates √¢, and 87 have different asymptotic rates of convergence. To arrive at nondegenerate limiting distributions, √¢, is multiplied by \sqrt{T}, whereas y must be multiplied by T^{3/2}!
[^7]: From [16.1.19] and [16.1.24], the asymptotic distribution of [16.1.18] can be calculated as in Example 7.5 of Chapter 7: [\sqrt{T}(\hat{\alpha}_T - \alpha) , T^{3/2}(\hat{\delta}_T - \delta)]  N(0, \sigma^2Q^{-1}).
[^8]: Thus, although √¢ and 87 converge at different rates, the corresponding standard errors day and √¥s, also incorporate different orders of T, with the result that the usual OLS t tests are asymptotically valid.
[^9]: It is interesting also to consider a test of a single hypothesis involving both a and 8, Hra + r28 = r, where r1, 12, and r are parameters that describe the hypothesis.
[^10]: At test of H, can be obtained from the square root of the OLS F test (expression [8.1.32]).
[^11]:  A test involving a single restriction across parameters with different rates of convergence is dominated asymptotically by the parameters with the slowest rates of convergence.
[^12]: Thus, again, the usual OLS test is asymptotically valid.
[^13]: Consider a sample of T + p observations on y, {y-p+1, y-p+2,...,y}, and let ·æ∂œÑ, Œ¥œÑ, Œ¶1.œÑ, ..... Œ¶œÅ. denote coefficient estimates based on ordinary least squares estimation of [16.3.1] for t = 1, 2, ..., T.
[^14]:  The idea of transforming the regression into a form such as [16.3.3] is due to Sims, Stock, and Watson (1990).
[^15]:  The system of [16.3.7] is just an algebraically equivalent representation of the regression model [16.3.5].
[^16]: This means that some of the elements of R* may be irrelevant asymptotically, so that [16.3.22] has the same asymptotic distribution as a simpler expression.
[^17]: It turns out that the OLS estimates √¢, and 87 have different asymptotic rates of convergence. To arrive at nondegenerate limiting distributions, √¢, is multiplied by \sqrt{T}, whereas 8 must be multiplied by T^{3/2}! We can think of this adjustment as premultiplying [16.1.6] or [16.1.8] by the matrix Y = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}.
resulting in
$$
\begin{bmatrix}
\sqrt{T}(\hat{\alpha}_T - \alpha) \\
T^{3/2}(\hat{\delta}_T - \delta)
\end{bmatrix} =
\mathbf{Y}_T
\begin{bmatrix}
\sum_{t=1}^T \mathbf{x}_t\mathbf{x}_t'
\end{bmatrix}^{-1}
\begin{bmatrix}
\sum_{t=1}^T \mathbf{x}_t \epsilon_t
\end{bmatrix}
$$
$$
= \mathbf{Y}_T
\begin{bmatrix}
\sum_{t=1}^T \mathbf{x}_t\mathbf{x}_t'
\end{bmatrix}^{-1}
\mathbf{Y}_T^{-1}
\mathbf{Y}_T
\begin{bmatrix}
\sum_{t=1}^T \mathbf{x}_t \epsilon_t
\end{bmatrix}
$$
$$
= \left\{\mathbf{Y}_T
\begin{bmatrix}
\sum_{t=1}^T \mathbf{x}_t\mathbf{x}_t'
\end{bmatrix}^{-1}
\mathbf{Y}_T^{-1} \right\}
\left\{ \mathbf{Y}_T
\begin{bmatrix}
\sum_{t=1}^T \mathbf{x}_t \epsilon_t
\end{bmatrix} \right\}
$$
[16.1.18]

Consider the first term in the last expression of [16.1.18]. Substituting from [16.1.17] and [16.1.16],
$$
\left\{
\mathbf{Y}_T
\begin{bmatrix}
\sum_{t=1}^T \mathbf{x}_t\mathbf{x}_t'
\end{bmatrix}^{-1}
\mathbf{Y}_T^{-1}
\right\}
=
\begin{bmatrix}
T^{-1/2} & 0 \\
0 & T^{-3/2}
\end{bmatrix}
\begin{bmatrix}
\sum_{t=1}^T 1 & \sum_{t=1}^T t \\
\sum_{t=1}^T t & \sum_{t=1}^T t^2
\end{bmatrix}^{-1}
\begin{bmatrix}
T^{-1/2} & 0 \\
0 & T^{-3/2}
\end{bmatrix}
$$
$$
=
\begin{bmatrix}
T^{-1/2} & 0 \\
0 & T^{-3/2}
\end{bmatrix}
\begin{bmatrix}
T & T(T+1)/2 \\
T(T+1)/2 & T(T+1)(2T+1)/6
\end{bmatrix}^{-1}
\begin{bmatrix}
T^{-1/2} & 0 \\
0 & T^{-3/2}
\end{bmatrix}
$$
Thus, it follows from [16.1.11] and [16.1.12] that
$$
\left\{
\mathbf{Y}_T
\begin{bmatrix}
\sum_{t=1}^T \mathbf{x}_t\mathbf{x}_t'
\end{bmatrix}^{-1}
\mathbf{Y}_T^{-1}
\right\} \rightarrow \mathbf{Q}^{-1}.
$$
[16.1.19]
where
$$
\mathbf{Q} =
\begin{bmatrix}
1 & 1/2 \\
1/2 & 1/3
\end{bmatrix}
$$
[16.1.20]
Turning next to the second term in [16.1.18],
$$
\mathbf{Y}_T
\begin{bmatrix}
\sum_{t=1}^T \mathbf{x}_t \epsilon_t
\end{bmatrix}
=
\begin{bmatrix}
T^{1/2} & 0 \\
0 & T^{3/2}
\end{bmatrix}
\begin{bmatrix}
\sum_{t=1}^T \epsilon_t \\
\sum_{t=1}^T t\epsilon_t
\end{bmatrix} =
\begin{bmatrix}
(1/\sqrt{T})\sum_{t=1}^T \epsilon_t \\
(1/\sqrt{T})\sum_{t=1}^T (t/T)\epsilon_t
\end{bmatrix}
$$
[16.1.21]
Under standard assumptions about $\epsilon_t$, this vector will be asymptotically Gaussian. For example, suppose that $\epsilon_t$ is i.i.d. with mean zero, variance $\sigma^2$, and finite fourth moment. Then the first element of the vector in [16.1.21] satisfies
$$
(1/\sqrt{T})\sum_{t=1}^T \epsilon_t \overset{d}{\longrightarrow} N(0,\sigma^2)
$$
by the central limit theorem.
For the second element of the vector in [16.1.21], observe that $\{(t/T)\epsilon_t\}$ is a martingale difference sequence that satisfies the conditions of Proposition 7.8. Specifically, its variance is
$$
\sigma_T^2 = E[(t/T)\epsilon_t]^2 = \sigma^2 (t^2/T^2),
$$
where
$$
(1/T)\sum_{t=1}^T \sigma_T^2 = \sigma^2(1/T^3) \sum_{t=1}^T t^2 \rightarrow \sigma^2/3
$$

Furthermore, $(1/T) \sum_{t=1}^T [(t/T)\epsilon_t]^2 \overset{p}{\longrightarrow} \sigma^2/3$. To verify this last claim, notice that
$$
E\left( (1/T) \sum_{t=1}^T [(t/T)\epsilon_t]^2 - (1/T) \sum_{t=1}^T \sigma^2_t \right)^2
$$
$$
= E \left((1/T) \sum_{t=1}^T [(t/T)^2 \epsilon_t^2] - (1/T) \sum_{t=1}^T [(t/T)^2 \sigma^2] \right)^2
$$
$$
= E \left( (1/T) \sum_{t=1}^T (t/T)^2(\epsilon_t^2 - \sigma^2) \right)^2
$$
$$
= (1/T^2) \sum_{t=1}^T (t/T)^2 E(\epsilon_t^2 - \sigma^2)^2.
$$
[16.1.22]
But from [16.1.13], $T$ times the magnitude in [16.1.22] converges to
$$
(1/T) \sum_{t=1}^T (t/T)^2E(\epsilon_t^2-\sigma^2)^2 \rightarrow (1/5) \cdot E(\epsilon_t^2 - \sigma^2)^2,
$$
meaning that [16.1.22] itself converges to zero:
$$
(1/T) \sum_{t=1}^T [(t/T)\epsilon_t]^2 - (1/T) \sum_{t=1}^T \sigma^2_t \overset{ms}{\longrightarrow} 0.
$$
But this implies that
$$
(1/T) \sum_{t=1}^T [(t/T)\epsilon_t]^2  \overset{p}{\longrightarrow} \sigma^2/3,
$$
as claimed. Hence, from Proposition 7.8, $(1/\sqrt{T}) \sum_{t=1}^T (t/T)\epsilon_t$ satisfies the central limit theorem:
$$
(1/\sqrt{T})\sum_{t=1}^T (t/T) \epsilon_t  \overset{d}{\longrightarrow} N(0, \sigma^2/3).
$$
Finally, consider the joint distribution of the two elements in the (2 x 1) vector described by [16.1.21]. Any linear combination of these elements takes the form
$$
(1/\sqrt{T}) \sum_{t=1}^T [\lambda_1 + \lambda_2 (t/T)] \epsilon_t.
$$
Then $[\lambda_1 + \lambda_2(t/T)]\epsilon_t$ is also a martingale difference sequence with positive variance given by $\sigma^2 [\lambda_1^2 + 2\lambda_1 \lambda_2 (t/T) + \lambda_2^2(t/T)^2]$ satisfying
$$
(1/T) \sum_{t=1}^T \sigma^2[\lambda_1^2 + 2\lambda_1 \lambda_2(t/T) + \lambda_2^2(t/T)^2] \rightarrow \sigma^2[\lambda_1^2 + 2\lambda_1\lambda_2(1/2) + \lambda_2^2(1/3)]
$$
as $T \rightarrow \infty$.

This means that if we have a time-varying coefficient model with a linear trend, the asymptotic variance of the estimator will depend on the average variance of the error term and the specific values of the coefficients $\lambda_1$ and $\lambda_2$.

*   **Specific Case: No Trend**
    If $\lambda_2=0$, then the model simplifies to
    $$
    y_t = (\lambda_1 + \epsilon_t)x_t
    $$
    and the asymptotic variance becomes
    $$
    \text{Avar}(\hat{\lambda}_1) = \sigma^2 \frac{1}{T}\sum_{t=1}^T x_t^2 \rightarrow \sigma^2 \mathbb{E}(x_t^2)
    $$

    In this simpler case, it's more straightforward to see that the estimator's variance converges to the average variance of the error term scaled by the expected square value of $x_t$.

**Important Considerations:**

*   The presented analysis assumes that the time series is stationary.
*   The derivations rely on the assumption that the errors are serially uncorrelated and have constant variance $\sigma^2$.
*   In real-world applications, it's crucial to check if the assumptions of the model are satisfied, and if not, to adjust the estimation procedure appropriately.
*   If you have any specific context, for example, if the model is a regression where $x_t$ is a vector of covariates, the analysis would be analogous, but the notation would need to accommodate the vector-valued nature of $x_t$.

**Further Extensions:**

This framework can be extended to other functional forms of time variation in the coefficient, beyond the linear trend explored here. One could, for instance, analyze a quadratic trend or include sinusoidal variations, or any other deterministic time function. The key is to plug that specific time function in the estimation procedure and then compute the asymptotic variance. It is also possible to consider stochastic trends in the coefficients, though the assumptions and derivations can become quite complicated. The analysis can also be extended to models with non-constant error variance. For instance, one may consider a model where the variance changes over time or is a function of the regressors.
<!-- END -->
