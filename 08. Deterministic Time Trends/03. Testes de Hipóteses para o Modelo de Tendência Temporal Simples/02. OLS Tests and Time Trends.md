## Testes de Hipóteses para o Modelo de Tendência Temporal Simples

### Introdução
Como vimos anteriormente, os coeficientes de modelos de regressão envolvendo raízes unitárias ou tendências temporais determinísticas são tipicamente estimados por Mínimos Quadrados Ordinários (OLS) [^1]. No entanto, as distribuições assintóticas das estimativas dos coeficientes não podem ser calculadas da mesma forma que aquelas para modelos de regressão envolvendo variáveis estacionárias [^1]. Este capítulo introduz a ideia de diferentes taxas de convergência e desenvolve uma abordagem geral para obter distribuições assintóticas, com foco inicial em processos envolvendo tendências temporais determinísticas, mas sem raízes unitárias [^1]. Em particular, exploraremos como os testes estatísticos t e F de OLS se comportam nesses contextos.

### Conceitos Fundamentais
No contexto de um modelo de tendência temporal simples,
$$ y_t = \alpha + \delta t + \epsilon_t $$
onde $\epsilon_t$ é um processo de ruído branco com $\epsilon_t \sim N(0, \sigma^2)$, o modelo satisfaz as suposições clássicas de regressão. Os testes t e F de OLS, neste caso, teriam distribuições exatas para amostras pequenas [^2]. No entanto, o interesse reside em entender como esses testes se comportam quando as inovações não são Gaussianas ou quando estamos interessados no comportamento assintótico.

**Distribuições Assintóticas dos Estimadores OLS**
Como discutido anteriormente, a distribuição assintótica dos estimadores OLS $\hat{\alpha}_T$ e $\hat{\delta}_T$ é crucial. Vimos que $\hat{\alpha}_T$ e $\hat{\delta}_T$ convergem para seus valores verdadeiros com diferentes taxas [^1]: $\hat{\alpha}_T$ converge a uma taxa de $T^{1/2}$, enquanto $\hat{\delta}_T$ converge a uma taxa de $T^{3/2}$ [^1, 2]. Especificamente, as distribuições assintóticas desses estimadores, após o devido reescalonamento, são dadas por [^7]:
$$
\begin{bmatrix}
    \sqrt{T}(\hat{\alpha}_T - \alpha) \\
    T^{3/2}(\hat{\delta}_T - \delta)
\end{bmatrix}
\xrightarrow{d} N\left(0, \sigma^2 \begin{bmatrix} 1 & 0 \\ 0 & 4 \end{bmatrix}^{-1}\right)
$$
onde a matriz de covariância é a inversa da matriz Q definida anteriormente [^7, 5]. Apesar das diferentes taxas de convergência, os erros padrão de $\hat{\alpha}_T$ e $\hat{\delta}_T$, denotados por $\hat{\sigma}_{\hat{\alpha}_T}$ e $\hat{\sigma}_{\hat{\delta}_T}$, também incorporam diferentes ordens de $T$, o que resulta em testes t e F válidos assintoticamente [^8].

> 💡 **Exemplo Numérico:** Suponha que temos um modelo $y_t = 2 + 0.5t + \epsilon_t$, onde $\epsilon_t \sim N(0, 1)$. Simulamos dados para $T=100$ e estimamos o modelo usando OLS.
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
>
> # Parâmetros verdadeiros
> alpha_true = 2
> delta_true = 0.5
> sigma_true = 1
> T = 100
>
> # Cria o vetor tempo
> t = np.arange(1, T+1)
>
> # Gera os dados
> np.random.seed(42) # para reprodutibilidade
> eps = np.random.normal(0, sigma_true, T)
> y = alpha_true + delta_true * t + eps
>
> # Estima o modelo com OLS
> X = np.column_stack((np.ones(T), t))
> model = sm.OLS(y, X)
> results = model.fit()
>
> # Imprime os resultados
> print(results.summary())
>
> # Extrai os estimadores e seus erros padrão
> alpha_hat = results.params[0]
> delta_hat = results.params[1]
> se_alpha_hat = results.bse[0]
> se_delta_hat = results.bse[1]
>
> print(f"Estimativa de alpha: {alpha_hat:.4f}, Erro padrão: {se_alpha_hat:.4f}")
> print(f"Estimativa de delta: {delta_hat:.4f}, Erro padrão: {se_delta_hat:.4f}")
>
> # Visualização
> plt.figure(figsize=(10,6))
> plt.plot(t, y, 'o', label='Dados Simulados')
> plt.plot(t, results.fittedvalues, 'r-', label='Regressão OLS')
> plt.xlabel('Tempo (t)')
> plt.ylabel('y_t')
> plt.title('Regressão Linear com Tendência Temporal')
> plt.legend()
> plt.show()
> ```
> Os resultados nos fornecem as estimativas $\hat{\alpha}_T$ e $\hat{\delta}_T$ e seus erros padrão. Observe como os erros padrão se comportam.
> $\hat{\alpha}_T \approx 2.3047$ e $\hat{\delta}_T \approx 0.4920$ e os erros padrão são $se(\hat{\alpha}_T) \approx 0.1736$ e $se(\hat{\delta}_T) \approx 0.0030 $. Observe que os erros padrão são muito diferentes, confirmando diferentes taxas de convergência.

**Lema 1**
A matriz $Q$ mencionada na distribuição assintótica dos estimadores OLS, no contexto do modelo de tendência temporal simples, é dada por:
$$ Q = \begin{bmatrix} 1 & \frac{1}{T}\sum_{t=1}^T t \\ \frac{1}{T}\sum_{t=1}^T t & \frac{1}{T^2}\sum_{t=1}^T t^2 \end{bmatrix} $$
*Prova:*
I. A matriz $Q$ é dada pela esperança do produto externo dos regressores, nesse caso $x_t = [1, t]$. Portanto, $Q = E[x_tx_t']$.
II. No nosso caso $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$ e $x_t' = \begin{bmatrix} 1 & t \end{bmatrix}$.
III. Assim, $x_t x_t' = \begin{bmatrix} 1 & t \\ t & t^2 \end{bmatrix}$.
IV. Tomando as médias amostrais, obtemos a matriz $Q$ apresentada no lema. $\square$

> 💡 **Exemplo Numérico:** Usando o exemplo anterior com $T=100$, podemos calcular a matriz $Q$:
>
> ```python
> import numpy as np
>
> T = 100
> t = np.arange(1, T + 1)
>
> Q = np.array([[1, np.mean(t)],
>               [np.mean(t), np.mean(t**2)]])
> print("Matriz Q:\n", Q)
> ```
> Temos:
> $$ Q = \begin{bmatrix} 1 & 50.5 \\ 50.5 & 3383.5 \end{bmatrix} $$
> Esta matriz é usada para calcular a variância assintótica dos estimadores.

**Teste t de OLS para Hipóteses sobre o Intercepto**
Considere o teste t de OLS da hipótese nula $H_0: \alpha = \alpha_0$, que é expresso como:
$$ t_{\alpha} = \frac{\hat{\alpha}_T - \alpha_0}{s_{\alpha}} $$
onde $s_{\alpha}$ é o desvio padrão estimado de $\hat{\alpha}_T$. Reescalonando o numerador e o denominador por $\sqrt{T}$, obtemos:
$$ t_{\alpha} = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{s_{\alpha}\sqrt{T}} $$
Como $\sqrt{T}(\hat{\alpha}_T - \alpha_0)$ converge em distribuição para uma normal com média zero e variância finita e $s_{\alpha} \sqrt{T}$ converge em probabilidade para um valor não nulo, o teste $t_{\alpha}$ terá uma distribuição assintótica normal padrão $N(0,1)$ [^8].

> 💡 **Exemplo Numérico:** Usando os dados simulados acima, testemos a hipótese $H_0: \alpha = 2$ contra $H_1: \alpha \neq 2$.
>
> ```python
> alpha_0 = 2
> t_alpha = (alpha_hat - alpha_0) / se_alpha_hat
> print(f"Estatística t para alpha: {t_alpha:.4f}")
> ```
>
>  O resultado da estatística t é $t_\alpha \approx 1.755$. Comparando com o valor crítico de uma distribuição normal padrão para um nível de significância de 5% (aproximadamente 1.96), não rejeitamos a hipótese nula de que $\alpha = 2$ para este conjunto de dados simulados. Contudo, a aproximação assintótica da distribuição normal padrão é mais precisa quando $T$ é grande.

**Teste t de OLS para Hipóteses sobre a Inclinação**
De forma análoga, considere o teste t de OLS para a hipótese nula $H_0: \delta = \delta_0$, que é expresso como:
$$ t_{\delta} = \frac{\hat{\delta}_T - \delta_0}{s_{\delta}} $$
Reescalonando o numerador e o denominador por $T^{3/2}$, obtemos:
$$ t_{\delta} = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{s_{\delta}T^{3/2}} $$
Aqui, $T^{3/2}(\hat{\delta}_T - \delta_0)$ converge em distribuição para uma normal com média zero e variância finita, e $s_{\delta} T^{3/2}$ converge em probabilidade para um valor não nulo. Assim, o teste $t_{\delta}$ também terá uma distribuição assintótica normal padrão $N(0,1)$ [^9].

> 💡 **Exemplo Numérico:** Agora testemos a hipótese $H_0: \delta = 0.5$ contra $H_1: \delta \neq 0.5$.
> ```python
> delta_0 = 0.5
> t_delta = (delta_hat - delta_0) / se_delta_hat
> print(f"Estatística t para delta: {t_delta:.4f}")
> ```
> O resultado da estatística t é $t_\delta \approx -2.620$. Comparando com o valor crítico de uma distribuição normal padrão para um nível de significância de 5% (aproximadamente 1.96), rejeitamos a hipótese nula de que $\delta = 0.5$.

**Teorema 1**
Sob as condições usuais de regressão, onde os erros $\epsilon_t$ são i.i.d. com média zero e variância constante $\sigma^2$,  e dado o modelo de tendência temporal simples $y_t = \alpha + \delta t + \epsilon_t$, os estimadores de mínimos quadrados ordinários $\hat{\alpha}_T$ e $\hat{\delta}_T$ são consistentes, ou seja, eles convergem em probabilidade para seus respectivos valores verdadeiros, $\alpha$ e $\delta$.
*Prova:*
I. A consistência dos estimadores de OLS, neste modelo, segue das condições de regularidade para a convergência dos estimadores de mínimos quadrados.
II. Estas condições são satisfeitas dado que os regressores não são perfeitamente colineares e os erros têm média zero.
III. A convergência em probabilidade para $\hat{\alpha}_T$ e $\hat{\delta}_T$ é uma condição necessária para que os testes t e F sejam assintoticamente válidos. $\square$

**Testes de Hipóteses Conjuntas**
Consideremos um teste de hipótese conjunta envolvendo ambos $\alpha$ e $\delta$, dado por:
$$ H_0: r_1\alpha + r_2\delta = r $$
Onde $r_1$, $r_2$, e $r$ são parâmetros conhecidos. O teste F de OLS pode ser utilizado para testar tal hipótese. O teste F é equivalente ao quadrado de um teste t de OLS. O teste é construído usando uma combinação linear dos estimadores, que podem ser expressos como:
$$ t_r = \frac{r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r}{\sqrt{s^2 [r_1 \, r_2](X^TX)^{-1} [r_1 \, r_2]^T}} $$
onde $s^2$ é a variância do erro estimada e $(X^TX)^{-1}$ é a matriz de covariância das variáveis. Multiplicando o numerador e o denominador pela taxa de convergência mais lenta, $\sqrt{T}$, e seguindo os passos acima, podemos concluir que o teste t de OLS também terá distribuição assintótica normal padrão [^9].

> 💡 **Exemplo Numérico:** Suponha que desejamos testar a hipótese conjunta $H_0: \alpha + 2\delta = 3$ (ou seja, $r_1 = 1$, $r_2 = 2$, $r=3$).  Podemos usar os resultados da regressão OLS para calcular a estatística t.
>
> ```python
> r1 = 1
> r2 = 2
> r = 3
> s2 = results.mse_resid
> cov_matrix = results.cov_params()
> X = np.column_stack((np.ones(T), t))
>
> t_r_num = r1 * alpha_hat + r2 * delta_hat - r
> t_r_den = np.sqrt(s2 * np.dot(np.array([r1, r2]), np.dot(cov_matrix, np.array([r1, r2]).T)))
> t_r = t_r_num / t_r_den
>
> print(f"Estatística t para a hipótese conjunta: {t_r:.4f}")
> ```
> O valor resultante para a estatística t é $t_r \approx 0.4338 $. Usando um nível de significância de 5%, não rejeitamos a hipótese nula de que $r_1\alpha + r_2\delta = r$.

**Generalização para Testes de Hipóteses Lineares**
Este resultado se generaliza para testes de hipóteses lineares mais gerais da forma $R\beta=r$, onde $R$ é uma matriz de restrições e $r$ um vetor de restrições. A estatística de Wald para testar esta hipótese é dada por:
$$ \chi^2 = (R\hat{\beta}_T - r)' [R(X^TX)^{-1}R']^{-1} (R\hat{\beta}_T - r)/s^2 $$
Essa estatística é assintoticamente distribuída como uma $\chi^2$ com um número de graus de liberdade igual ao número de restrições (o número de linhas em R). Importante notar que os testes t e F baseados nos estimadores OLS no modelo original são assintoticamente equivalentes aos testes correspondentes baseados nos estimadores na forma transformada (canonical form) [^15, 16].

**Proposição 1**
Sob a hipótese nula $H_0: r_1\alpha + r_2\delta = r$, a estatística $t_r$ definida anteriormente converge em distribuição para uma normal padrão $N(0,1)$.
*Prova:*
I. Dado que $\hat{\alpha}_T$ e $\hat{\delta}_T$ são estimadores consistentes e suas distribuições assintóticas são conhecidas, podemos reescrever a estatística $t_r$ como:
$$ t_r = \frac{r_1(\hat{\alpha}_T - \alpha) + r_2(\hat{\delta}_T - \delta)}{\sqrt{s^2 [r_1 \, r_2](X^TX)^{-1} [r_1 \, r_2]^T}} $$
II. Multiplicando o numerador e o denominador por $\sqrt{T}$, e usando as taxas de convergência apropriadas, $\sqrt{T}$ para $\hat{\alpha}_T$ e $T^{3/2}$ para $\hat{\delta}_T$, o numerador converge em distribuição para uma normal com média zero.
III. O denominador, ao convergir em probabilidade para um valor não nulo, garante que $t_r$ convirja em distribuição para $N(0,1)$. $\square$

### Conclusão
Em resumo, para o modelo de tendência temporal simples, os testes t e F de OLS têm distribuições exatas de amostra pequena quando as inovações são Gaussianas. Mais importante, eles são assintoticamente válidos mesmo para inovações não Gaussianas. Isso ocorre devido ao comportamento compensatório assintótico entre os estimadores e seus erros padrão. As diferentes taxas de convergência dos estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$ são acomodadas pelos erros padrão que incorporam diferentes ordens de $T$, garantindo assim inferências assintoticamente válidas para ambos os parâmetros, separadamente ou conjuntamente. Como resultado, podemos utilizar os procedimentos usuais de testes de hipóteses lineares sem grandes preocupações com as taxas de convergência distintas dos estimadores, devido ao comportamento de compensação dos erros padrão [^8, 9]. Esta análise também se estende para processos auto-regressivos com uma tendência determinística, conforme apresentado adiante no capítulo.

### Referências
[^1]:  The coefficients of regression models involving unit roots or deterministic time trends are typically estimated by ordinary least squares. However, the asymptotic distributions of the coefficient estimates cannot be calculated in the same way as are those for regression models involving stationary variables. Among other difficulties, the estimates of different parameters will in general have different asymptotic rates of convergence.
[^2]: If e, ~ N(0, σ²), then the model [16.1.1] satisfies the classical regression assumptions and the standard OLS t or F statistics in equations [8.1.26] and [8.1.32] would have exact small-sample t or F distributions.
[^3]: This chapter introduces this technique, which will prove useful not only for studying time trends but also for analyzing estimators for a variety of nonstationary processes in Chapters 17 and 18.
[^4]: Recall the approach used to find asymptotic distributions for regressions with stationary explanatory variables in Chapter 8.
[^5]: The usual assumption was that (1/T) Σ₁x,x, converged in probability to a nonsingular matrix Q while (1/V/T) ΣΤ, χ,ε, converged in distribution to a N(0, σ²Q) random variable, implying that VT(b+- β) N(0, 0²Q-1).
[^6]: It turns out that the OLS estimates â, and 87 have different asymptotic rates of convergence. To arrive at nondegenerate limiting distributions, â, is multiplied by VT, whereas y must be multiplied by T3/2!
[^7]: From [16.1.19] and [16.1.24], the asymptotic distribution of [16.1.18] can be calculated as in Example 7.5 of Chapter 7: [ντάτ - α) , T32(8τ - δ)]  N(0, σ²Q-1).
[^8]: Thus, although â and 87 converge at different rates, the corresponding standard errors day and ôs, also incorporate different orders of T, with the result that the usual OLS t tests are asymptotically valid.
[^9]: It is interesting also to consider a test of a single hypothesis involving both a and 8, Hra + r28 = r, where r1, 12, and r are parameters that describe the hypothesis.
[^10]: At test of H, can be obtained from the square root of the OLS F test (expression [8.1.32]).
[^11]:  A test involving a single restriction across parameters with different rates of convergence is dominated asymptotically by the parameters with the slowest rates of convergence.
[^12]: Thus, again, the usual OLS test is asymptotically valid.
[^13]: Consider a sample of T + p observations on y, {y-p+1, y-p+2,...,y}, and let ᾶτ, δτ, Φ1.τ, ..... Φρ. denote coefficient estimates based on ordinary least squares estimation of [16.3.1] for t = 1, 2, ..., T.
[^14]:  The idea of transforming the regression into a form such as [16.3.3] is due to Sims, Stock, and Watson (1990).
[^15]:  The system of [16.3.7] is just an algebraically equivalent representation of the regression model [16.3.5].
[^16]: This means that some of the elements of R* may be irrelevant asymptotically, so that [16.3.22] has the same asymptotic distribution as a simpler expression.
<!-- END -->
