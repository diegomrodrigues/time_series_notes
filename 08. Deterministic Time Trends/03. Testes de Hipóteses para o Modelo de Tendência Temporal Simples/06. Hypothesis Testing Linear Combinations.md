## Testes de Hip√≥teses Lineares e Conjuntas em Modelos de Tend√™ncia Temporal

### Introdu√ß√£o
Este cap√≠tulo continua a an√°lise sobre o modelo de tend√™ncia temporal simples, concentrando-se na realiza√ß√£o de testes de hip√≥teses lineares e conjuntas sobre os par√¢metros do modelo [^1]. Expandindo a an√°lise anterior sobre a validade assint√≥tica dos testes t para par√¢metros individuais, exploraremos como esses princ√≠pios se aplicam a combina√ß√µes lineares dos coeficientes ($\alpha$ e $\delta$) e a testes conjuntos envolvendo ambos os par√¢metros [^9]. O objetivo √© demonstrar que os testes de hip√≥teses constru√≠dos com os estimadores de M√≠nimos Quadrados Ordin√°rios (MQO), mesmo sob a presen√ßa de diferentes taxas de converg√™ncia, mant√™m sua validade assint√≥tica devido ao comportamento compensat√≥rio entre os estimadores e seus erros padr√£o. A validade assint√≥tica ser√° provada atrav√©s da manipula√ß√£o da estat√≠stica para que converja em distribui√ß√£o para uma normal padr√£o ou para uma qui-quadrado, quando apropriado.

### Conceitos Fundamentais

Relembrando, o modelo de tend√™ncia temporal simples √© definido como:

$$y_t = \alpha + \delta t + \epsilon_t$$

onde $\epsilon_t$ √© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$ [^2]. Os estimadores de OLS, $\hat{\alpha}_T$ e $\hat{\delta}_T$, convergem para seus valores verdadeiros com taxas de $\sqrt{T}$ e $T^{3/2}$, respectivamente [^1, 6]. A distribui√ß√£o assint√≥tica conjunta dos estimadores, devidamente reescalonada, √© dada por:

$$
\begin{bmatrix}
\sqrt{T}(\hat{\alpha}_T - \alpha) \\
T^{3/2}(\hat{\delta}_T - \delta)
\end{bmatrix}
\xrightarrow{d} N\left(0, \sigma^2 Q^{-1}\right)
$$

onde $Q$ √© a matriz de covari√¢ncia dos regressores [^5], e a sua inversa $Q^{-1}$ √© fundamental para obter as vari√¢ncias assint√≥ticas dos estimadores.

**Observa√ß√£o:** √â importante notar que a matriz $Q$ mencionada acima √© obtida a partir da matriz de covari√¢ncia dos regressores, que neste caso s√£o a constante e a tend√™ncia temporal $t$.  Especificamente, $Q = \lim_{T\to\infty} \frac{1}{T} X^T X $, onde $X$ √© a matriz de regressores.

**Testes de Hip√≥teses Lineares: Abordagem via Teste t**

Consideremos uma hip√≥tese linear geral envolvendo ambos os par√¢metros, $\alpha$ e $\delta$, da forma:

$$H_0: r_1\alpha + r_2\delta = r$$

onde $r_1$, $r_2$ e $r$ s√£o constantes conhecidas [^9]. Para testar essa hip√≥tese, constru√≠mos a estat√≠stica t correspondente:

$$
t_r = \frac{r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r}{\sqrt{\hat{\sigma}^2 [r_1 \, r_2](X^TX)^{-1} [r_1 \, r_2]^T}}
$$

onde $\hat{\sigma}^2$ √© a vari√¢ncia do erro estimada e $(X^TX)^{-1}$ √© a matriz de covari√¢ncia dos regressores. Para analisar o comportamento assint√≥tico de $t_r$, multiplicamos o numerador e o denominador pela raiz da taxa de converg√™ncia mais lenta ($\sqrt{T}$):

$$
t_r = \frac{\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)}{\sqrt{\hat{\sigma}^2 \sqrt{T}^2 [r_1 \, r_2](X^TX)^{-1} [r_1 \, r_2]^T}}
$$

A chave para a validade assint√≥tica do teste reside no fato de que o numerador, quando devidamente reescalonado, converge para uma distribui√ß√£o normal com m√©dia zero, enquanto o denominador converge em probabilidade para uma constante n√£o nula. Essa converg√™ncia garante que a estat√≠stica $t_r$ siga uma distribui√ß√£o normal padr√£o $N(0,1)$ sob a hip√≥tese nula [^9].

> üí° **Exemplo Num√©rico:**
>
>  Vamos simular um modelo com $\alpha = 2$, $\delta = 0.5$, $\sigma^2 = 1$ e $T = 100$. Iremos gerar os dados e ajustar o modelo por OLS.  Em seguida, testaremos a hip√≥tese linear $H_0: \alpha + 2\delta = 3$.
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> from scipy.stats import t
>
> # Par√¢metros do modelo
> alpha_true = 2
> delta_true = 0.5
> sigma_sq = 1
> T = 100
>
> # Gera√ß√£o de dados
> np.random.seed(42)
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
> y = alpha_true + delta_true * t + epsilon
>
> # Ajuste do modelo OLS
> X = sm.add_constant(t)
> model = sm.OLS(y, X)
> results = model.fit()
>
> alpha_hat = results.params[0]
> delta_hat = results.params[1]
> sigma_hat_sq = results.mse_resid
>
> # Definindo a hip√≥tese linear
> r1 = 1
> r2 = 2
> r = 3
>
> # Calculando a estat√≠stica t
> X_mat = np.column_stack((np.ones(T), t))
> cov_matrix = sigma_hat_sq * np.linalg.inv(X_mat.T @ X_mat)
> se_linear = np.sqrt(np.array([r1, r2]) @ cov_matrix @ np.array([r1, r2]).T)
> t_r = (r1 * alpha_hat + r2 * delta_hat - r) / se_linear
>
> # Grau de liberdade
> df = T - 2
>
> # C√°lculo do p-valor
> p_valor_r = 2 * (1 - t.cdf(abs(t_r), df))
>
> print(f"Estat√≠stica t para a hip√≥tese linear: {t_r:.4f}")
> print(f"P-valor para a hip√≥tese linear: {p_valor_r:.4f}")
>
> # Compara o p-valor com o n√≠vel de signific√¢ncia
> nivel_significancia = 0.05
> if p_valor_r < nivel_significancia:
>    print("Rejeita a hip√≥tese nula H0: alpha + 2*delta = 3")
> else:
>    print("N√£o rejeita a hip√≥tese nula H0: alpha + 2*delta = 3")
> ```
> Este exemplo ilustra o c√°lculo da estat√≠stica $t_r$ e do seu p-valor associado para a hip√≥tese linear $H_0: \alpha + 2\delta = 3$. Se o p-valor for inferior ao n√≠vel de signific√¢ncia estabelecido (ex: 0.05), rejeitamos a hip√≥tese nula.
>
> **Interpreta√ß√£o:**  No exemplo acima, ao simularmos dados com $\alpha=2$, $\delta=0.5$, e testarmos a hip√≥tese $\alpha + 2\delta = 3$, a estat√≠stica t √© calculada com os valores estimados a partir dos dados simulados. O p-valor nos diz a probabilidade de observarmos uma estat√≠stica t t√£o ou mais extrema do que a que obtivemos, se a hip√≥tese nula fosse verdadeira. Se o p-valor fosse muito pequeno (menor que o n√≠vel de signific√¢ncia), ent√£o rejeitar√≠amos a hip√≥tese nula. Caso contr√°rio, n√£o a rejeitamos.

**Teste F de OLS e sua Rela√ß√£o com o Teste t**

O teste F de OLS tamb√©m pode ser utilizado para testar hip√≥teses lineares conjuntas. Em particular, quando temos apenas uma restri√ß√£o linear, o teste F √© equivalente ao quadrado do teste t de OLS [^10]. Portanto, a raiz quadrada da estat√≠stica F tamb√©m converge assintoticamente para uma distribui√ß√£o normal padr√£o $N(0,1)$.

> üí° **Exemplo Num√©rico:**
>
>  Usando o mesmo exemplo anterior, podemos calcular a estat√≠stica F para testar a hip√≥tese linear $H_0: \alpha + 2\delta = 3$. O teste F, neste caso, √© o quadrado da estat√≠stica t.
> ```python
> # Calculando a estat√≠stica F (equivalente ao quadrado da estat√≠stica t para uma restri√ß√£o linear)
> F_r = t_r**2
>
> # Calculando o p-valor para o teste F (usando uma distribui√ß√£o F com 1 e T-2 graus de liberdade)
> from scipy.stats import f
> p_valor_F = 1 - f.cdf(F_r, 1, df)
>
> print(f"Estat√≠stica F para a hip√≥tese linear: {F_r:.4f}")
> print(f"P-valor para a hip√≥tese linear usando F: {p_valor_F:.4f}")
>
> # Compara o p-valor com o n√≠vel de signific√¢ncia
> nivel_significancia = 0.05
> if p_valor_F < nivel_significancia:
>    print("Rejeita a hip√≥tese nula H0: alpha + 2*delta = 3")
> else:
>    print("N√£o rejeita a hip√≥tese nula H0: alpha + 2*delta = 3")
> ```
> Este exemplo mostra como a estat√≠stica F e o seu p-valor s√£o calculados. A decis√£o sobre rejeitar ou n√£o a hip√≥tese nula √© a mesma obtida com o teste t. Note que o p-valor do teste F √© igual ao p-valor do teste t.
>
> **Interpreta√ß√£o:**  Como esperado, o p-valor do teste F √© o mesmo que o p-valor do teste t, dado que testamos a mesma hip√≥tese e temos uma √∫nica restri√ß√£o. Esta equival√™ncia entre o teste t e a raiz quadrada do teste F ocorre quando se testa uma √∫nica restri√ß√£o linear nos par√¢metros.

**Lema 2**
A estat√≠stica $t_r$, definida como:
$$ t_r = \frac{r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r}{\sqrt{\hat{\sigma}^2 [r_1 \, r_2](X^TX)^{-1} [r_1 \, r_2]^T}} $$
converge em distribui√ß√£o para uma normal padr√£o $N(0,1)$ sob a hip√≥tese nula $H_0: r_1\alpha + r_2\delta = r$.

*Prova:*
I. O numerador da estat√≠stica $t_r$, que √© $r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r$, converge em distribui√ß√£o para uma normal com m√©dia zero sob a hip√≥tese nula. Isto √© consequ√™ncia do fato de que $\hat{\alpha}_T$ e $\hat{\delta}_T$ s√£o estimadores consistentes que convergem para seus respectivos valores verdadeiros, $\alpha$ e $\delta$, e que qualquer combina√ß√£o linear de vari√°veis normais tamb√©m resulta em uma vari√°vel normal.
II. O denominador, que √© o erro padr√£o da combina√ß√£o linear dos estimadores, converge em probabilidade para um valor n√£o nulo, e √© reescalonado pela raiz de $T$ para compensar as taxas de converg√™ncia diferentes.
III. Utilizando o teorema de Slutsky, que garante que a raz√£o entre uma vari√°vel que converge para uma normal e uma vari√°vel que converge em probabilidade para uma constante n√£o nula tamb√©m converge para uma normal, temos que a estat√≠stica $t_r$ converge para uma normal padr√£o $N(0,1)$. $\blacksquare$

**Lema 2.1**
Sob a hip√≥tese nula $H_0: r_1\alpha + r_2\delta = r$, a estat√≠stica $t_r^2$ converge em distribui√ß√£o para uma qui-quadrado com um grau de liberdade, ou seja, $t_r^2 \xrightarrow{d} \chi^2(1)$.

*Prova:*
I. Como demonstrado no Lema 2, sob a hip√≥tese nula $H_0$, a estat√≠stica $t_r$ converge em distribui√ß√£o para uma normal padr√£o, ou seja, $t_r \xrightarrow{d} N(0,1)$.
II. Sabemos que o quadrado de uma vari√°vel aleat√≥ria com distribui√ß√£o normal padr√£o segue uma distribui√ß√£o qui-quadrado com um grau de liberdade.
III. Portanto, $t_r^2 \xrightarrow{d} \chi^2(1)$. $\blacksquare$

**Testes Conjuntos de Hip√≥teses: Abordagem via Teste de Wald**

Agora, consideremos o caso em que desejamos testar conjuntamente hip√≥teses sobre os par√¢metros $\alpha$ e $\delta$ separadamente. Este teste √© da forma:

$$
H_0:
\begin{bmatrix}
    \alpha \\
    \delta
\end{bmatrix}
=
\begin{bmatrix}
    \alpha_0 \\
    \delta_0
\end{bmatrix}
$$

onde $\alpha_0$ e $\delta_0$ s√£o valores espec√≠ficos para os par√¢metros. O teste de Wald √© adequado para essa situa√ß√£o. A estat√≠stica de teste de Wald √© dada por [^12]:

$$
\chi^2 = (\hat{\beta}_T - \beta_0)' [V(\hat{\beta}_T)]^{-1} (\hat{\beta}_T - \beta_0)
$$

onde $\hat{\beta}_T = [\hat{\alpha}_T, \hat{\delta}_T]^T$ √© o vetor de estimadores, $\beta_0 = [\alpha_0, \delta_0]^T$ √© o vetor de valores nulos, e $V(\hat{\beta}_T)$ √© a matriz de covari√¢ncia assint√≥tica dos estimadores. O teste de Wald pode ser escrito em termos de uma forma quadr√°tica da seguinte forma [^12]:

$$ \chi^2 = (\hat{\beta}_T - \beta_0)' (s^2 (X^TX)^{-1})^{-1}(\hat{\beta}_T - \beta_0) $$
Essa estat√≠stica converge em distribui√ß√£o para uma qui-quadrado com 2 graus de liberdade sob a hip√≥tese nula, ou seja, $\chi^2 \sim \chi^2(2)$ [^12].

> üí° **Exemplo Num√©rico:**
>
> Vamos ilustrar o teste de Wald. Utilizaremos os dados simulados anteriormente e testaremos a hip√≥tese conjunta $H_0: \alpha = 2$ e $\delta = 0.5$.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> from scipy.stats import chi2
>
> # Par√¢metros do modelo
> alpha_true = 2
> delta_true = 0.5
> sigma_sq = 1
> T = 100
>
> # Gera√ß√£o de dados
> np.random.seed(42)
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
> y = alpha_true + delta_true * t + epsilon
>
> # Ajuste do modelo OLS
> X = sm.add_constant(t)
> model = sm.OLS(y, X)
> results = model.fit()
>
> alpha_hat = results.params[0]
> delta_hat = results.params[1]
> sigma_hat_sq = results.mse_resid
>
> # Hip√≥tese conjunta
> beta_hat = np.array([alpha_hat, delta_hat])
> beta_0 = np.array([2, 0.5])
> X_mat = np.column_stack((np.ones(T), t))
> cov_matrix = sigma_hat_sq * np.linalg.inv(X_mat.T @ X_mat)
>
> # C√°lculo da estat√≠stica de Wald
> wald_stat = (beta_hat - beta_0) @ np.linalg.inv(cov_matrix) @ (beta_hat - beta_0)
>
> # Grau de liberdade
> df = 2
>
> # C√°lculo do p-valor
> p_valor_wald = 1 - chi2.cdf(wald_stat, df)
>
> print(f"Estat√≠stica de Wald: {wald_stat:.4f}")
> print(f"P-valor para o teste de Wald: {p_valor_wald:.4f}")
>
> # Compara o p-valor com o n√≠vel de signific√¢ncia
> nivel_significancia = 0.05
> if p_valor_wald < nivel_significancia:
>    print("Rejeita a hip√≥tese nula conjunta H0: alpha = 2 e delta = 0.5")
> else:
>    print("N√£o rejeita a hip√≥tese nula conjunta H0: alpha = 2 e delta = 0.5")
> ```
> Este exemplo calcula a estat√≠stica de Wald e o p-valor correspondente para a hip√≥tese conjunta $H_0: \alpha = 2$ e $\delta = 0.5$. Rejeitamos a hip√≥tese nula se o p-valor for inferior ao n√≠vel de signific√¢ncia escolhido.
>
> **Interpreta√ß√£o:** No teste de Wald, estamos testando a hip√≥tese de que ambos os par√¢metros, $\alpha$ e $\delta$, s√£o iguais a valores espec√≠ficos simultaneamente. A estat√≠stica de Wald √© calculada utilizando a diferen√ßa entre os valores estimados e os valores hipot√©ticos, ponderada pela inversa da matriz de covari√¢ncia dos estimadores. O p-valor nos diz a probabilidade de observarmos uma estat√≠stica de Wald t√£o ou mais extrema do que a que obtivemos, se a hip√≥tese nula fosse verdadeira. Se o p-valor for muito pequeno (menor que o n√≠vel de signific√¢ncia), ent√£o rejeitamos a hip√≥tese nula conjunta.

**Teorema 3**
A estat√≠stica de Wald, definida como
$$\chi^2 = (\hat{\beta}_T - \beta_0)' (s^2 (X^TX)^{-1})^{-1}(\hat{\beta}_T - \beta_0)$$
converge em distribui√ß√£o para uma qui-quadrado com 2 graus de liberdade sob a hip√≥tese nula $H_0: \beta = \beta_0$.

*Prova:*
I. Sob a hip√≥tese nula, temos que $H_0: \beta = \beta_0$.
II. As estimativas de m√≠nimos quadrados ordin√°rios, $\hat{\beta}_T = [\hat{\alpha}_T, \hat{\delta}_T]^T$, s√£o assintoticamente normais, ou seja, $\sqrt{T} (\hat{\beta}_T - \beta_0)$ converge para uma distribui√ß√£o normal com m√©dia zero e matriz de covari√¢ncia igual a  $\sigma^2 Q^{-1}$, onde $Q = \lim_{T\rightarrow\infty} (1/T)(X^TX)$.
III. Dado que a matriz de covari√¢ncia do erro estimado, $(X^TX)^{-1}$, converge para $Q^{-1}$, a forma quadr√°tica $(\hat{\beta}_T - \beta_0)' (s^2 (X^TX)^{-1})^{-1}(\hat{\beta}_T - \beta_0)$ converge em distribui√ß√£o para uma qui-quadrado com o n√∫mero de graus de liberdade igual ao n√∫mero de restri√ß√µes (2 neste caso), isto √©, $ \chi^2 \sim \chi^2(2)$. $\blacksquare$

**Corol√°rio 3.1**
Se a hip√≥tese nula conjunta $H_0: \alpha = \alpha_0$ e $\delta = \delta_0$ √© verdadeira, ent√£o a estat√≠stica de Wald, $\chi^2$, √© assintoticamente equivalente √† soma dos quadrados das estat√≠sticas $t$ para os testes de hip√≥teses individuais sobre cada par√¢metro, ajustados pelas taxas de converg√™ncia apropriadas.

*Prova:*
I. Quando as restri√ß√µes s√£o individuais, i.e., $H_{01}: \alpha = \alpha_0$ e $H_{02}: \delta = \delta_0$, temos as estat√≠sticas t associadas: $t_\alpha = \frac{\hat{\alpha}_T - \alpha_0}{se(\hat{\alpha}_T)}$ e $t_\delta = \frac{\hat{\delta}_T - \delta_0}{se(\hat{\delta}_T)}$.
II. As vari√¢ncias assint√≥ticas dos estimadores s√£o proporcionais a $\frac{1}{T}$ para $\hat{\alpha}_T$ e $\frac{1}{T^3}$ para $\hat{\delta}_T$. Portanto, ao quadrado e escalonadas, as estat√≠sticas $t$ convergem para $\chi^2(1)$ cada.
III. Quando as restri√ß√µes s√£o conjuntas, a estat√≠stica de Wald √© uma forma quadr√°tica que considera a covari√¢ncia entre $\hat{\alpha}_T$ e $\hat{\delta}_T$. Se esta covari√¢ncia for nula (o que √© o caso assintoticamente), a estat√≠stica de Wald se reduz √† soma dos quadrados das estat√≠sticas $t$ individuais.
IV. Portanto, em geral, $\chi^2 \approx t_\alpha^2 + t_\delta^2$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar o corol√°rio 3.1, podemos calcular as estat√≠sticas t para as hip√≥teses individuais e verificar se a soma de seus quadrados √© aproximadamente igual √† estat√≠stica de Wald.
> ```python
> # Calculando as estat√≠sticas t para as hip√≥teses individuais
> se_alpha = np.sqrt(cov_matrix[0,0])
> se_delta = np.sqrt(cov_matrix[1,1])
>
> t_alpha = (alpha_hat - 2) / se_alpha
> t_delta = (delta_hat - 0.5) / se_delta
>
> # Calculando a soma dos quadrados das estat√≠sticas t individuais
> chi2_approx = t_alpha**2 + t_delta**2
>
> print(f"Estat√≠stica t para alpha: {t_alpha:.4f}")
> print(f"Estat√≠stica t para delta: {t_delta:.4f}")
> print(f"Soma dos quadrados das estat√≠sticas t: {chi2_approx:.4f}")
> print(f"Estat√≠stica de Wald: {wald_stat:.4f}")
> ```
> O exemplo acima ilustra que a soma dos quadrados das estat√≠sticas t individuais √© aproximadamente igual √† estat√≠stica de Wald.
>
> **Interpreta√ß√£o:** Este exemplo demonstra que o teste de Wald pode ser interpretado como uma generaliza√ß√£o do teste t para testes conjuntos, pois a estat√≠stica de Wald √© numericamente pr√≥xima da soma dos quadrados das estat√≠sticas t individuais quando testamos hip√≥teses separadamente. Isso ocorre porque o teste de Wald leva em considera√ß√£o a covari√¢ncia entre os estimadores, que √© zero assintoticamente, confirmando o corol√°rio 3.1.

### Generaliza√ß√£o para Testes de Hip√≥teses Lineares mais Gerais

Os resultados acima se generalizam para testes de hip√≥teses lineares mais gerais da forma $H_0: R\beta = r$, onde $R$ √© uma matriz de restri√ß√µes e $r$ √© um vetor de restri√ß√µes. A estat√≠stica de Wald para testar esta hip√≥tese √© dada por:

$$ \chi^2 = (R\hat{\beta}_T - r)' [R(X^TX)^{-1}R']^{-1} (R\hat{\beta}_T - r)/\hat{\sigma}^2 $$

Essa estat√≠stica √© assintoticamente distribu√≠da como uma qui-quadrado com um n√∫mero de graus de liberdade igual ao n√∫mero de restri√ß√µes (o n√∫mero de linhas em $R$). √â importante ressaltar que a validade assint√≥tica dos testes t e F constru√≠dos com os estimadores OLS do modelo original s√£o assintoticamente equivalentes aos testes correspondentes baseados nos estimadores da forma transformada (canonical form), conforme demonstrado anteriormente [^15, 16].

### Conclus√£o

Este cap√≠tulo demonstrou a validade assint√≥tica dos testes de hip√≥teses lineares e conjuntas em modelos de tend√™ncia temporal determin√≠stica, mesmo sob a presen√ßa de diferentes taxas de converg√™ncia dos estimadores. Os testes t, quando utilizados para testar hip√≥teses sobre combina√ß√µes lineares dos par√¢metros, e o teste F de OLS, que √© equivalente ao quadrado do teste t no caso de uma √∫nica restri√ß√£o, convergem para distribui√ß√µes assint√≥ticas normais padr√£o. Adicionalmente, testes conjuntos envolvendo ambos os par√¢metros podem ser conduzidos usando a estat√≠stica de Wald, que converge para uma qui-quadrado, dada a condi√ß√£o de que os estimadores sejam consistentes e que os erros padr√£o sejam consistentes com as taxas de converg√™ncia dos estimadores. A utiliza√ß√£o de matrizes de reescalonamento garante que os testes mantenham sua validade assint√≥tica, mesmo na presen√ßa de diferentes taxas de converg√™ncia, como explicitado pela aplica√ß√£o dos teoremas de converg√™ncia para os estimadores e suas vari√¢ncias.

### Refer√™ncias
[^1]: The coefficients of regression models involving unit roots or deterministic time trends are typically estimated by ordinary least squares. However, the asymptotic distributions of the coefficient estimates cannot be calculated in the same way as are those for regression models involving stationary variables. Among other difficulties, the estimates of different parameters will in general have different asymptotic rates of convergence.
[^2]: If e, ~ N(0, œÉ¬≤), then the model [16.1.1] satisfies the classical regression assumptions and the standard OLS t or F statistics in equations [8.1.26] and [8.1.32] would have exact small-sample t or F distributions.
[^3]: This chapter introduces this technique, which will prove useful not only for studying time trends but also for analyzing estimators for a variety of nonstationary processes in Chapters 17 and 18.
[^4]: Recall the approach used to find asymptotic distributions for regressions with stationary explanatory variables in Chapter 8.
[^5]: The usual assumption was that (1/T) \sum_1 x_t x_t, converged in probability to a nonsingular matrix Q while (1/\sqrt{T}) \sum_1^T x_t \epsilon_t, converged in distribution to a N(0, \sigma^2 Q) random variable, implying that \sqrt{T}(b+- \beta) N(0, 0^2Q-1).
[^6]: It turns out that the OLS estimates √¢, and 87 have different asymptotic rates of convergence. To arrive at nondegenerate limiting distributions, √¢, is multiplied by \sqrt{T}, whereas y must be multiplied by T^{3/2}!
[^7]: From [16.1.19] and [16.1.24], the asymptotic distribution of [16.1.18] can be calculated as in Example 7.5 of Chapter 7: [\sqrt{T}(\hat{\alpha}_T - \alpha) , T^{3/2}(\hat{\delta}_T - \delta)]  N(0, \sigma^2Q^{-1}).
[^8]: Thus, although √¢ and 87 converge at different rates, the corresponding standard errors day and √¥s, also incorporate different orders of T, with the result that the usual OLS t tests are asymptotically valid.
[^9]: It is interesting also to consider a test of a single hypothesis involving both a and 8, Hra + r28 = r, where r1, 12, and r are parameters that describe the hypothesis.
[^10]: At test of H, can be obtained from the square root of the OLS F test (expression [8.1.32]).
[^11]:  A test involving a single restriction across parameters with different rates of convergence is dominated asymptotically by the parameters with the slowest rates of convergence.
[^12]: Thus, again, the usual OLS test is asymptotically valid.
[^13]: Consider a sample of T + p observations on y, {y-p+1, y-p+2,...,y}, and let ·æ∂œÑ, Œ¥œÑ, Œ¶1.œÑ, ..... Œ¶œÅ. denote coefficient estimates based on ordinary least squares estimation of [16.3.1] for t = 1, 2, ..., T.
[^14]:  The idea of transforming the regression into a form such as [16.3.3] is due to Sims, Stock, and Watson (1990).
[^15]:  The system of [16.3.7] is just an algebraically equivalent representation of the regression model [16.3.5].
[^16]: This means that some of the elements of R* may be irrelevant asymptotically, so that [16.3.22] has the same asymptotic distribution as a simpler expression.
[^17]: It turns out that the OLS estimates √¢, and 87 have different asymptotic rates of convergence. To arrive at nondegenerate limiting distributions, √¢, is multiplied by \sqrt{T}, whereas 8 must be multiplied by T^{3/2}! We can think of this adjustment as premultiplying [16.1.6] or [16.1.8] by the matrix Y = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}.
<!-- END -->
