## Testes de Hip√≥teses para o Modelo de Tend√™ncia Temporal Simples

### Introdu√ß√£o
Como vimos anteriormente, os coeficientes de modelos de regress√£o envolvendo ra√≠zes unit√°rias ou tend√™ncias temporais determin√≠sticas s√£o tipicamente estimados por M√≠nimos Quadrados Ordin√°rios (OLS) [^1]. No entanto, as distribui√ß√µes assint√≥ticas das estimativas dos coeficientes n√£o podem ser calculadas da mesma forma que aquelas para modelos de regress√£o envolvendo vari√°veis estacion√°rias [^1]. Este cap√≠tulo introduz a ideia de diferentes taxas de converg√™ncia e desenvolve uma abordagem geral para obter distribui√ß√µes assint√≥ticas, com foco inicial em processos envolvendo tend√™ncias temporais determin√≠sticas, mas sem ra√≠zes unit√°rias [^1]. Em particular, exploraremos como os testes estat√≠sticos t e F de OLS se comportam nesses contextos.

### Conceitos Fundamentais
No contexto de um modelo de tend√™ncia temporal simples,
$$ y_t = \alpha + \delta t + \epsilon_t $$
onde $\epsilon_t$ √© um processo de ru√≠do branco com $\epsilon_t \sim N(0, \sigma^2)$, o modelo satisfaz as suposi√ß√µes cl√°ssicas de regress√£o. Os testes t e F de OLS, neste caso, teriam distribui√ß√µes exatas para amostras pequenas [^2]. No entanto, o interesse reside em entender como esses testes se comportam quando as inova√ß√µes n√£o s√£o Gaussianas ou quando estamos interessados no comportamento assint√≥tico.

**Distribui√ß√µes Assint√≥ticas dos Estimadores OLS**
Como discutido anteriormente, a distribui√ß√£o assint√≥tica dos estimadores OLS $\hat{\alpha}_T$ e $\hat{\delta}_T$ √© crucial. Vimos que $\hat{\alpha}_T$ e $\hat{\delta}_T$ convergem para seus valores verdadeiros com diferentes taxas [^1]: $\hat{\alpha}_T$ converge a uma taxa de $T^{1/2}$, enquanto $\hat{\delta}_T$ converge a uma taxa de $T^{3/2}$ [^1, 2]. Especificamente, as distribui√ß√µes assint√≥ticas desses estimadores, ap√≥s o devido reescalonamento, s√£o dadas por [^7]:
$$
\begin{bmatrix}
    \sqrt{T}(\hat{\alpha}_T - \alpha) \\
    T^{3/2}(\hat{\delta}_T - \delta)
\end{bmatrix}
\xrightarrow{d} N\left(0, \sigma^2 \begin{bmatrix} 1 & 0 \\ 0 & 4 \end{bmatrix}^{-1}\right)
$$
onde a matriz de covari√¢ncia √© a inversa da matriz Q definida anteriormente [^7, 5]. Apesar das diferentes taxas de converg√™ncia, os erros padr√£o de $\hat{\alpha}_T$ e $\hat{\delta}_T$, denotados por $\hat{\sigma}_{\hat{\alpha}_T}$ e $\hat{\sigma}_{\hat{\delta}_T}$, tamb√©m incorporam diferentes ordens de $T$, o que resulta em testes t e F v√°lidos assintoticamente [^8].

> üí° **Exemplo Num√©rico:** Suponha que temos um modelo $y_t = 2 + 0.5t + \epsilon_t$, onde $\epsilon_t \sim N(0, 1)$. Simulamos dados para $T=100$ e estimamos o modelo usando OLS.
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
>
> # Par√¢metros verdadeiros
> alpha_true = 2
> delta_true = 0.5
> sigma_true = 1
> T = 100
>
> # Cria o vetor tempo
> t = np.arange(1, T+1)
>
> # Gera os dados
> np.random.seed(42) # para reprodutibilidade
> eps = np.random.normal(0, sigma_true, T)
> y = alpha_true + delta_true * t + eps
>
> # Estima o modelo com OLS
> X = np.column_stack((np.ones(T), t))
> model = sm.OLS(y, X)
> results = model.fit()
>
> # Imprime os resultados
> print(results.summary())
>
> # Extrai os estimadores e seus erros padr√£o
> alpha_hat = results.params[0]
> delta_hat = results.params[1]
> se_alpha_hat = results.bse[0]
> se_delta_hat = results.bse[1]
>
> print(f"Estimativa de alpha: {alpha_hat:.4f}, Erro padr√£o: {se_alpha_hat:.4f}")
> print(f"Estimativa de delta: {delta_hat:.4f}, Erro padr√£o: {se_delta_hat:.4f}")
>
> # Visualiza√ß√£o
> plt.figure(figsize=(10,6))
> plt.plot(t, y, 'o', label='Dados Simulados')
> plt.plot(t, results.fittedvalues, 'r-', label='Regress√£o OLS')
> plt.xlabel('Tempo (t)')
> plt.ylabel('y_t')
> plt.title('Regress√£o Linear com Tend√™ncia Temporal')
> plt.legend()
> plt.show()
> ```
> Os resultados nos fornecem as estimativas $\hat{\alpha}_T$ e $\hat{\delta}_T$ e seus erros padr√£o. Observe como os erros padr√£o se comportam.
> $\hat{\alpha}_T \approx 2.3047$ e $\hat{\delta}_T \approx 0.4920$ e os erros padr√£o s√£o $se(\hat{\alpha}_T) \approx 0.1736$ e $se(\hat{\delta}_T) \approx 0.0030 $. Observe que os erros padr√£o s√£o muito diferentes, confirmando diferentes taxas de converg√™ncia.

**Lema 1**
A matriz $Q$ mencionada na distribui√ß√£o assint√≥tica dos estimadores OLS, no contexto do modelo de tend√™ncia temporal simples, √© dada por:
$$ Q = \begin{bmatrix} 1 & \frac{1}{T}\sum_{t=1}^T t \\ \frac{1}{T}\sum_{t=1}^T t & \frac{1}{T^2}\sum_{t=1}^T t^2 \end{bmatrix} $$
*Prova:*
I. A matriz $Q$ √© dada pela esperan√ßa do produto externo dos regressores, nesse caso $x_t = [1, t]$. Portanto, $Q = E[x_tx_t']$.
II. No nosso caso $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$ e $x_t' = \begin{bmatrix} 1 & t \end{bmatrix}$.
III. Assim, $x_t x_t' = \begin{bmatrix} 1 & t \\ t & t^2 \end{bmatrix}$.
IV. Tomando as m√©dias amostrais, obtemos a matriz $Q$ apresentada no lema. $\square$

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior com $T=100$, podemos calcular a matriz $Q$:
>
> ```python
> import numpy as np
>
> T = 100
> t = np.arange(1, T + 1)
>
> Q = np.array([[1, np.mean(t)],
>               [np.mean(t), np.mean(t**2)]])
> print("Matriz Q:\n", Q)
> ```
> Temos:
> $$ Q = \begin{bmatrix} 1 & 50.5 \\ 50.5 & 3383.5 \end{bmatrix} $$
> Esta matriz √© usada para calcular a vari√¢ncia assint√≥tica dos estimadores.

**Teste t de OLS para Hip√≥teses sobre o Intercepto**
Considere o teste t de OLS da hip√≥tese nula $H_0: \alpha = \alpha_0$, que √© expresso como:
$$ t_{\alpha} = \frac{\hat{\alpha}_T - \alpha_0}{s_{\alpha}} $$
onde $s_{\alpha}$ √© o desvio padr√£o estimado de $\hat{\alpha}_T$. Reescalonando o numerador e o denominador por $\sqrt{T}$, obtemos:
$$ t_{\alpha} = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{s_{\alpha}\sqrt{T}} $$
Como $\sqrt{T}(\hat{\alpha}_T - \alpha_0)$ converge em distribui√ß√£o para uma normal com m√©dia zero e vari√¢ncia finita e $s_{\alpha} \sqrt{T}$ converge em probabilidade para um valor n√£o nulo, o teste $t_{\alpha}$ ter√° uma distribui√ß√£o assint√≥tica normal padr√£o $N(0,1)$ [^8].

> üí° **Exemplo Num√©rico:** Usando os dados simulados acima, testemos a hip√≥tese $H_0: \alpha = 2$ contra $H_1: \alpha \neq 2$.
>
> ```python
> alpha_0 = 2
> t_alpha = (alpha_hat - alpha_0) / se_alpha_hat
> print(f"Estat√≠stica t para alpha: {t_alpha:.4f}")
> ```
>
>  O resultado da estat√≠stica t √© $t_\alpha \approx 1.755$. Comparando com o valor cr√≠tico de uma distribui√ß√£o normal padr√£o para um n√≠vel de signific√¢ncia de 5% (aproximadamente 1.96), n√£o rejeitamos a hip√≥tese nula de que $\alpha = 2$ para este conjunto de dados simulados. Contudo, a aproxima√ß√£o assint√≥tica da distribui√ß√£o normal padr√£o √© mais precisa quando $T$ √© grande.

**Teste t de OLS para Hip√≥teses sobre a Inclina√ß√£o**
De forma an√°loga, considere o teste t de OLS para a hip√≥tese nula $H_0: \delta = \delta_0$, que √© expresso como:
$$ t_{\delta} = \frac{\hat{\delta}_T - \delta_0}{s_{\delta}} $$
Reescalonando o numerador e o denominador por $T^{3/2}$, obtemos:
$$ t_{\delta} = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{s_{\delta}T^{3/2}} $$
Aqui, $T^{3/2}(\hat{\delta}_T - \delta_0)$ converge em distribui√ß√£o para uma normal com m√©dia zero e vari√¢ncia finita, e $s_{\delta} T^{3/2}$ converge em probabilidade para um valor n√£o nulo. Assim, o teste $t_{\delta}$ tamb√©m ter√° uma distribui√ß√£o assint√≥tica normal padr√£o $N(0,1)$ [^9].

> üí° **Exemplo Num√©rico:** Agora testemos a hip√≥tese $H_0: \delta = 0.5$ contra $H_1: \delta \neq 0.5$.
> ```python
> delta_0 = 0.5
> t_delta = (delta_hat - delta_0) / se_delta_hat
> print(f"Estat√≠stica t para delta: {t_delta:.4f}")
> ```
> O resultado da estat√≠stica t √© $t_\delta \approx -2.620$. Comparando com o valor cr√≠tico de uma distribui√ß√£o normal padr√£o para um n√≠vel de signific√¢ncia de 5% (aproximadamente 1.96), rejeitamos a hip√≥tese nula de que $\delta = 0.5$.

**Teorema 1**
Sob as condi√ß√µes usuais de regress√£o, onde os erros $\epsilon_t$ s√£o i.i.d. com m√©dia zero e vari√¢ncia constante $\sigma^2$,  e dado o modelo de tend√™ncia temporal simples $y_t = \alpha + \delta t + \epsilon_t$, os estimadores de m√≠nimos quadrados ordin√°rios $\hat{\alpha}_T$ e $\hat{\delta}_T$ s√£o consistentes, ou seja, eles convergem em probabilidade para seus respectivos valores verdadeiros, $\alpha$ e $\delta$.
*Prova:*
I. A consist√™ncia dos estimadores de OLS, neste modelo, segue das condi√ß√µes de regularidade para a converg√™ncia dos estimadores de m√≠nimos quadrados.
II. Estas condi√ß√µes s√£o satisfeitas dado que os regressores n√£o s√£o perfeitamente colineares e os erros t√™m m√©dia zero.
III. A converg√™ncia em probabilidade para $\hat{\alpha}_T$ e $\hat{\delta}_T$ √© uma condi√ß√£o necess√°ria para que os testes t e F sejam assintoticamente v√°lidos. $\square$

**Testes de Hip√≥teses Conjuntas**
Consideremos um teste de hip√≥tese conjunta envolvendo ambos $\alpha$ e $\delta$, dado por:
$$ H_0: r_1\alpha + r_2\delta = r $$
Onde $r_1$, $r_2$, e $r$ s√£o par√¢metros conhecidos. O teste F de OLS pode ser utilizado para testar tal hip√≥tese. O teste F √© equivalente ao quadrado de um teste t de OLS. O teste √© constru√≠do usando uma combina√ß√£o linear dos estimadores, que podem ser expressos como:
$$ t_r = \frac{r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r}{\sqrt{s^2 [r_1 \, r_2](X^TX)^{-1} [r_1 \, r_2]^T}} $$
onde $s^2$ √© a vari√¢ncia do erro estimada e $(X^TX)^{-1}$ √© a matriz de covari√¢ncia das vari√°veis. Multiplicando o numerador e o denominador pela taxa de converg√™ncia mais lenta, $\sqrt{T}$, e seguindo os passos acima, podemos concluir que o teste t de OLS tamb√©m ter√° distribui√ß√£o assint√≥tica normal padr√£o [^9].

> üí° **Exemplo Num√©rico:** Suponha que desejamos testar a hip√≥tese conjunta $H_0: \alpha + 2\delta = 3$ (ou seja, $r_1 = 1$, $r_2 = 2$, $r=3$).  Podemos usar os resultados da regress√£o OLS para calcular a estat√≠stica t.
>
> ```python
> r1 = 1
> r2 = 2
> r = 3
> s2 = results.mse_resid
> cov_matrix = results.cov_params()
> X = np.column_stack((np.ones(T), t))
>
> t_r_num = r1 * alpha_hat + r2 * delta_hat - r
> t_r_den = np.sqrt(s2 * np.dot(np.array([r1, r2]), np.dot(cov_matrix, np.array([r1, r2]).T)))
> t_r = t_r_num / t_r_den
>
> print(f"Estat√≠stica t para a hip√≥tese conjunta: {t_r:.4f}")
> ```
> O valor resultante para a estat√≠stica t √© $t_r \approx 0.4338 $. Usando um n√≠vel de signific√¢ncia de 5%, n√£o rejeitamos a hip√≥tese nula de que $r_1\alpha + r_2\delta = r$.

**Generaliza√ß√£o para Testes de Hip√≥teses Lineares**
Este resultado se generaliza para testes de hip√≥teses lineares mais gerais da forma $R\beta=r$, onde $R$ √© uma matriz de restri√ß√µes e $r$ um vetor de restri√ß√µes. A estat√≠stica de Wald para testar esta hip√≥tese √© dada por:
$$ \chi^2 = (R\hat{\beta}_T - r)' [R(X^TX)^{-1}R']^{-1} (R\hat{\beta}_T - r)/s^2 $$
Essa estat√≠stica √© assintoticamente distribu√≠da como uma $\chi^2$ com um n√∫mero de graus de liberdade igual ao n√∫mero de restri√ß√µes (o n√∫mero de linhas em R). Importante notar que os testes t e F baseados nos estimadores OLS no modelo original s√£o assintoticamente equivalentes aos testes correspondentes baseados nos estimadores na forma transformada (canonical form) [^15, 16].

**Proposi√ß√£o 1**
Sob a hip√≥tese nula $H_0: r_1\alpha + r_2\delta = r$, a estat√≠stica $t_r$ definida anteriormente converge em distribui√ß√£o para uma normal padr√£o $N(0,1)$.
*Prova:*
I. Dado que $\hat{\alpha}_T$ e $\hat{\delta}_T$ s√£o estimadores consistentes e suas distribui√ß√µes assint√≥ticas s√£o conhecidas, podemos reescrever a estat√≠stica $t_r$ como:
$$ t_r = \frac{r_1(\hat{\alpha}_T - \alpha) + r_2(\hat{\delta}_T - \delta)}{\sqrt{s^2 [r_1 \, r_2](X^TX)^{-1} [r_1 \, r_2]^T}} $$
II. Multiplicando o numerador e o denominador por $\sqrt{T}$, e usando as taxas de converg√™ncia apropriadas, $\sqrt{T}$ para $\hat{\alpha}_T$ e $T^{3/2}$ para $\hat{\delta}_T$, o numerador converge em distribui√ß√£o para uma normal com m√©dia zero.
III. O denominador, ao convergir em probabilidade para um valor n√£o nulo, garante que $t_r$ convirja em distribui√ß√£o para $N(0,1)$. $\square$

### Conclus√£o
Em resumo, para o modelo de tend√™ncia temporal simples, os testes t e F de OLS t√™m distribui√ß√µes exatas de amostra pequena quando as inova√ß√µes s√£o Gaussianas. Mais importante, eles s√£o assintoticamente v√°lidos mesmo para inova√ß√µes n√£o Gaussianas. Isso ocorre devido ao comportamento compensat√≥rio assint√≥tico entre os estimadores e seus erros padr√£o. As diferentes taxas de converg√™ncia dos estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$ s√£o acomodadas pelos erros padr√£o que incorporam diferentes ordens de $T$, garantindo assim infer√™ncias assintoticamente v√°lidas para ambos os par√¢metros, separadamente ou conjuntamente. Como resultado, podemos utilizar os procedimentos usuais de testes de hip√≥teses lineares sem grandes preocupa√ß√µes com as taxas de converg√™ncia distintas dos estimadores, devido ao comportamento de compensa√ß√£o dos erros padr√£o [^8, 9]. Esta an√°lise tamb√©m se estende para processos auto-regressivos com uma tend√™ncia determin√≠stica, conforme apresentado adiante no cap√≠tulo.

### Refer√™ncias
[^1]:  The coefficients of regression models involving unit roots or deterministic time trends are typically estimated by ordinary least squares. However, the asymptotic distributions of the coefficient estimates cannot be calculated in the same way as are those for regression models involving stationary variables. Among other difficulties, the estimates of different parameters will in general have different asymptotic rates of convergence.
[^2]: If e, ~ N(0, œÉ¬≤), then the model [16.1.1] satisfies the classical regression assumptions and the standard OLS t or F statistics in equations [8.1.26] and [8.1.32] would have exact small-sample t or F distributions.
[^3]: This chapter introduces this technique, which will prove useful not only for studying time trends but also for analyzing estimators for a variety of nonstationary processes in Chapters 17 and 18.
[^4]: Recall the approach used to find asymptotic distributions for regressions with stationary explanatory variables in Chapter 8.
[^5]: The usual assumption was that (1/T) Œ£‚ÇÅx,x, converged in probability to a nonsingular matrix Q while (1/V/T) Œ£Œ§, œá,Œµ, converged in distribution to a N(0, œÉ¬≤Q) random variable, implying that VT(b+- Œ≤) N(0, 0¬≤Q-1).
[^6]: It turns out that the OLS estimates √¢, and 87 have different asymptotic rates of convergence. To arrive at nondegenerate limiting distributions, √¢, is multiplied by VT, whereas y must be multiplied by T3/2!
[^7]: From [16.1.19] and [16.1.24], the asymptotic distribution of [16.1.18] can be calculated as in Example 7.5 of Chapter 7: [ŒΩœÑŒ¨œÑ - Œ±) , T32(8œÑ - Œ¥)]  N(0, œÉ¬≤Q-1).
[^8]: Thus, although √¢ and 87 converge at different rates, the corresponding standard errors day and √¥s, also incorporate different orders of T, with the result that the usual OLS t tests are asymptotically valid.
[^9]: It is interesting also to consider a test of a single hypothesis involving both a and 8, Hra + r28 = r, where r1, 12, and r are parameters that describe the hypothesis.
[^10]: At test of H, can be obtained from the square root of the OLS F test (expression [8.1.32]).
[^11]:  A test involving a single restriction across parameters with different rates of convergence is dominated asymptotically by the parameters with the slowest rates of convergence.
[^12]: Thus, again, the usual OLS test is asymptotically valid.
[^13]: Consider a sample of T + p observations on y, {y-p+1, y-p+2,...,y}, and let ·æ∂œÑ, Œ¥œÑ, Œ¶1.œÑ, ..... Œ¶œÅ. denote coefficient estimates based on ordinary least squares estimation of [16.3.1] for t = 1, 2, ..., T.
[^14]:  The idea of transforming the regression into a form such as [16.3.3] is due to Sims, Stock, and Watson (1990).
[^15]:  The system of [16.3.7] is just an algebraically equivalent representation of the regression model [16.3.5].
[^16]: This means that some of the elements of R* may be irrelevant asymptotically, so that [16.3.22] has the same asymptotic distribution as a simpler expression.
<!-- END -->
