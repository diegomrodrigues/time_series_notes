## Testes de Hip√≥teses para o Modelo de Tend√™ncia Temporal Simples: An√°lise Detalhada da Validade Assint√≥tica do Teste t de OLS

### Introdu√ß√£o
Este cap√≠tulo, em continuidade √† an√°lise pr√©via sobre modelos de regress√£o com tend√™ncias temporais determin√≠sticas, aprofunda-se no comportamento assint√≥tico dos testes t de M√≠nimos Quadrados Ordin√°rios (MQO) [^1]. Exploramos em detalhes como os testes t de OLS se comportam ao testar hip√≥teses sobre os par√¢metros do modelo, $\alpha$ (intercepto) e $\delta$ (inclina√ß√£o), considerando suas diferentes taxas de converg√™ncia. A relev√¢ncia da an√°lise reside no fato de que, embora os estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$ convirjam para seus valores verdadeiros em taxas distintas, os testes t de OLS, devidamente reescalonados, permanecem assintoticamente v√°lidos. Essa validade √© garantida pela forma como os erros padr√£o dos estimadores, $\hat{\sigma}_{\hat{\alpha}_T}$ e $\hat{\sigma}_{\hat{\delta}_T}$, tamb√©m incorporam diferentes ordens de $T$ [^8].

### Conceitos Fundamentais

Como visto anteriormente, o modelo de tend√™ncia temporal simples √© definido como:
$$y_t = \alpha + \delta t + \epsilon_t,$$
onde $\epsilon_t$ √© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$ [^2]. Ao discutirmos as distribui√ß√µes assint√≥ticas dos estimadores de OLS, enfatizamos que $\hat{\alpha}_T$ e $\hat{\delta}_T$ convergem para seus valores verdadeiros, $\alpha$ e $\delta$, com taxas diferentes: $\sqrt{T}$ para $\hat{\alpha}_T$ e $T^{3/2}$ para $\hat{\delta}_T$ [^1, 6].

**Distribui√ß√µes Assint√≥ticas e Reescalonamento**
A distribui√ß√£o assint√≥tica conjunta dos estimadores de OLS, ap√≥s o devido reescalonamento, √© dada por [^7]:
$$
\begin{bmatrix}
    \sqrt{T}(\hat{\alpha}_T - \alpha) \\
    T^{3/2}(\hat{\delta}_T - \delta)
\end{bmatrix}
\xrightarrow{d} N\left(0, \sigma^2 Q^{-1}\right)
$$
onde $Q$ √© uma matriz de covari√¢ncia que captura a estrutura dos regressores. A matriz $Q$ √© definida como [^5]:
$$ Q = \begin{bmatrix} 1 & \frac{1}{T}\sum_{t=1}^T t \\ \frac{1}{T}\sum_{t=1}^T t & \frac{1}{T^2}\sum_{t=1}^T t^2 \end{bmatrix} $$
Como demonstrado no Lema 1 do t√≥pico anterior, esta matriz descreve a estrutura assint√≥tica dos regressores do modelo [^5]. A matriz inversa $Q^{-1}$ √© fundamental para a determina√ß√£o das vari√¢ncias assint√≥ticas dos estimadores, que se tornam a base para os testes de hip√≥teses.

**O Comportamento Compensat√≥rio dos Erros Padr√£o**
O ponto crucial para a validade assint√≥tica dos testes t de OLS reside no comportamento compensat√≥rio entre os estimadores e seus erros padr√£o [^8]. Embora $\hat{\alpha}_T$ e $\hat{\delta}_T$ convirjam com taxas diferentes, seus respectivos erros padr√£o, $\hat{\sigma}_{\hat{\alpha}_T}$ e $\hat{\sigma}_{\hat{\delta}_T}$, tamb√©m incorporam diferentes ordens de $T$ de tal forma que as estat√≠sticas t resultantes convergem para uma distribui√ß√£o normal padr√£o $N(0,1)$. Isso implica que, mesmo que os estimadores convirjam rapidamente ou lentamente, o teste t, baseado na raz√£o entre a estimativa e seu erro padr√£o, possui uma distribui√ß√£o assint√≥tica bem comportada.

**Lema 1**
A matriz $Q$ converge para uma matriz n√£o singular quando $T \to \infty$.

*Prova:*
O termo $(1/T)\sum_{t=1}^{T} t$ converge para $T/2$ e o termo $(1/T^2)\sum_{t=1}^{T} t^2$ converge para $T/3$ quando $T\rightarrow \infty$. Assim a matriz Q converge para:

$$ Q = \begin{bmatrix} 1 & T/2 \\ T/2 & T/3 \end{bmatrix} $$
A matriz $Q$ √© n√£o singular para $T>0$, dado que seu determinante $T/3 - T^2/4 \neq 0$, a n√£o ser quando $T = 4/3$. $\blacksquare$

**Lema 1.1**
A matriz inversa $Q^{-1}$ tamb√©m converge para uma matriz n√£o singular quando $T \to \infty$.

*Prova:*
Como demonstrado no Lema 1, a matriz $Q$ converge para uma matriz n√£o singular. Portanto, sua inversa $Q^{-1}$ tamb√©m converge para uma matriz n√£o singular quando $T\rightarrow\infty$. $\blacksquare$

### An√°lise Detalhada dos Testes t de OLS

**Teste t para Hip√≥tese sobre o Intercepto (Œ±)**

Considere o teste da hip√≥tese nula $H_0: \alpha = \alpha_0$, que √© formulado como:
$$ t_{\alpha} = \frac{\hat{\alpha}_T - \alpha_0}{\hat{\sigma}_{\hat{\alpha}_T}} $$
onde $\hat{\sigma}_{\hat{\alpha}_T}$ √© o erro padr√£o estimado de $\hat{\alpha}_T$. Para analisar o comportamento assint√≥tico, reescalonamos o numerador e o denominador por $\sqrt{T}$:
$$ t_{\alpha} = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{\hat{\sigma}_{\hat{\alpha}_T}\sqrt{T}} $$
O numerador, $\sqrt{T}(\hat{\alpha}_T - \alpha_0)$, converge em distribui√ß√£o para uma normal com m√©dia zero e vari√¢ncia finita, como definido pela distribui√ß√£o assint√≥tica conjunta dos estimadores. O denominador, $\hat{\sigma}_{\hat{\alpha}_T}\sqrt{T}$, converge em probabilidade para um valor n√£o nulo. Portanto, a raz√£o, $t_{\alpha}$, converge em distribui√ß√£o para $N(0, 1)$ [^8].

> üí° **Exemplo Num√©rico:**
> Vamos simular um modelo de tend√™ncia temporal com $\alpha = 2$, $\delta = 0.5$, e $\sigma^2 = 1$, usando um tamanho de amostra $T=100$. Assim, o modelo √© dado por $y_t = 2 + 0.5t + \epsilon_t$, onde $\epsilon_t \sim N(0,1)$. Geramos 100 observa√ß√µes para simular este modelo e ajustamos o modelo de regress√£o linear por M√≠nimos Quadrados Ordin√°rios.
> ```python
> import numpy as np
> import statsmodels.api as sm
> import pandas as pd
>
> # Par√¢metros do modelo
> alpha_true = 2
> delta_true = 0.5
> sigma_sq = 1
> T = 100
>
> # Gerar dados simulados
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
> y = alpha_true + delta_true * t + epsilon
>
> # Ajustar o modelo de regress√£o
> X = sm.add_constant(t)
> model = sm.OLS(y, X)
> results = model.fit()
>
> # Extrair os resultados
> alpha_hat = results.params[0]
> delta_hat = results.params[1]
> sigma_alpha_hat = results.bse[0]
> sigma_delta_hat = results.bse[1]
>
> print(f"Estimativa de alpha: {alpha_hat:.4f}")
> print(f"Erro padr√£o de alpha: {sigma_alpha_hat:.4f}")
>
> # Testar a hip√≥tese nula H0: alpha = 2
> alpha_0 = 2
> t_alpha = (alpha_hat - alpha_0) / sigma_alpha_hat
> print(f"Estat√≠stica t para alpha: {t_alpha:.3f}")
> ```
>
> Executando o c√≥digo, obtivemos $\hat{\alpha}_T = 1.9980$ e $\hat{\sigma}_{\hat{\alpha}_T} = 0.1802$. Desejamos testar a hip√≥tese $H_0: \alpha = 2$. A estat√≠stica do teste t √©:
>
>$$ t_{\alpha} = \frac{1.9980 - 2}{0.1802} \approx -0.011 $$
>
>Esta estat√≠stica, sob a hip√≥tese nula, se aproxima de uma distribui√ß√£o $N(0,1)$ para amostras grandes.

**Teste t para Hip√≥tese sobre a Inclina√ß√£o (Œ¥)**
Para a hip√≥tese nula $H_0: \delta = \delta_0$, o teste t √© definido como:
$$ t_{\delta} = \frac{\hat{\delta}_T - \delta_0}{\hat{\sigma}_{\hat{\delta}_T}} $$
Reescalonando o numerador e o denominador por $T^{3/2}$:
$$ t_{\delta} = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\hat{\sigma}_{\hat{\delta}_T}T^{3/2}} $$
O numerador, $T^{3/2}(\hat{\delta}_T - \delta_0)$, converge em distribui√ß√£o para uma normal com m√©dia zero e vari√¢ncia finita. Analogamente ao caso do intercepto, o denominador, $\hat{\sigma}_{\hat{\delta}_T}T^{3/2}$, converge em probabilidade para um valor n√£o nulo. Portanto, a estat√≠stica $t_{\delta}$ converge em distribui√ß√£o para uma normal padr√£o $N(0,1)$ [^9].

> üí° **Exemplo Num√©rico:**
> Usando o mesmo modelo simulado do exemplo anterior, obtivemos $\hat{\delta}_T = 0.4997$ e $\hat{\sigma}_{\hat{\delta}_T} = 0.0031$. Testamos $H_0: \delta = 0.5$. A estat√≠stica t √©:
> ```python
> print(f"Estimativa de delta: {delta_hat:.4f}")
> print(f"Erro padr√£o de delta: {sigma_delta_hat:.4f}")
>
> # Testar a hip√≥tese nula H0: delta = 0.5
> delta_0 = 0.5
> t_delta = (delta_hat - delta_0) / sigma_delta_hat
> print(f"Estat√≠stica t para delta: {t_delta:.3f}")
> ```
>
> Executando o c√≥digo, obtivemos:
>
> $$ t_{\delta} = \frac{0.4997 - 0.5}{0.0031} \approx -0.097 $$
>
> Como no caso anterior, esta estat√≠stica converge para uma distribui√ß√£o $N(0,1)$.

**Testes de Hip√≥teses Lineares Envolvendo Par√¢metros com Diferentes Taxas de Converg√™ncia**

A an√°lise da validade assint√≥tica dos testes t se estende a hip√≥teses lineares envolvendo m√∫ltiplos par√¢metros que convergem em taxas diferentes. Consideremos a hip√≥tese linear $H_0: r_1\alpha + r_2\delta = r$, onde $r_1$, $r_2$, e $r$ s√£o constantes conhecidas. A estat√≠stica t correspondente √©:
$$ t_r = \frac{r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r}{\sqrt{\hat{\sigma}^2 [r_1 \, r_2](X^TX)^{-1} [r_1 \, r_2]^T}} $$
O denominador √© o erro padr√£o da combina√ß√£o linear dos estimadores. Reescalonando a estat√≠stica $t_r$, usando a menor taxa de converg√™ncia (i.e., $\sqrt{T}$), temos:
$$ t_r = \frac{\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)}{\sqrt{\hat{\sigma}^2 \sqrt{T}^2 [r_1 \, r_2](X^TX)^{-1} [r_1 \, r_2]^T}} $$

√â crucial notar que, no numerador, $\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)$ cont√©m o termo $r_2\sqrt{T}\hat{\delta}_T$, que n√£o converge para uma distribui√ß√£o normal. Contudo, ao introduzir o reescalonamento e o erro padr√£o, a estat√≠stica de teste resultante converge para uma distribui√ß√£o normal. A principal raz√£o √© que os par√¢metros com a taxa de converg√™ncia mais lenta (neste caso, o termo em $\hat{\alpha}_T$ )  dominar√£o o comportamento assint√≥tico da estat√≠stica [^11]. Isso garante que a infer√™ncia seja assintoticamente v√°lida mesmo sob a presen√ßa de diferentes taxas de converg√™ncia.

> üí° **Exemplo Num√©rico:** Vamos testar $H_0: \alpha + 2\delta = 3$ com nossos dados simulados. Usando as estimativas anteriores, temos $r_1 = 1$, $r_2 = 2$, e $r = 3$. A estat√≠stica $t_r$ √© calculada da seguinte forma:
>
> ```python
> r1 = 1
> r2 = 2
> r = 3
>
> # Calcular a matriz (X'X)^-1
> cov_matrix = np.linalg.inv(X.T @ X)
>
> # Calcular o erro padr√£o da combina√ß√£o linear
> std_err_linear_combination = np.sqrt(results.mse_resid * np.array([r1, r2]) @ cov_matrix @ np.array([r1, r2]).T)
>
> # Calcular a estat√≠stica t para a hip√≥tese linear
> t_r = (r1 * alpha_hat + r2 * delta_hat - r) / std_err_linear_combination
> print(f"Estat√≠stica t para hip√≥tese linear: {t_r:.3f}")
> ```
>
>  Com o resultado do c√≥digo, temos $t_r \approx -0.021$, que, sob a hip√≥tese nula, se aproxima de uma distribui√ß√£o $N(0,1)$.

**Generaliza√ß√£o e Testes de Hip√≥teses Lineares**
Esta an√°lise se generaliza para testes de hip√≥teses lineares mais gerais da forma $R\beta=r$ [^16]. A estat√≠stica de Wald para testar esta hip√≥tese √© dada por:
$$ \chi^2 = (R\hat{\beta}_T - r)' [R(X^TX)^{-1}R']^{-1} (R\hat{\beta}_T - r)/\hat{\sigma}^2 $$
Essa estat√≠stica √© assintoticamente distribu√≠da como uma $\chi^2$ com graus de liberdade iguais ao n√∫mero de restri√ß√µes. A equival√™ncia assint√≥tica entre os testes baseados nos estimadores do modelo original e da forma transformada (canonical form) tamb√©m √© mantida.

**A Proposi√ß√£o Chave**
A estat√≠stica $t_r$, sob a hip√≥tese nula, converge em distribui√ß√£o para uma normal padr√£o $N(0,1)$ [^9]. Isto √© uma consequ√™ncia direta do fato de que os estimadores, mesmo convergindo em taxas diferentes, s√£o consistentes (Teorema 1) e os erros padr√£o compensam essas diferen√ßas de taxas de converg√™ncia.

**Proposi√ß√£o 1**
Os estimadores de OLS, $\hat{\alpha}_T$ e $\hat{\delta}_T$, s√£o consistentes, ou seja, convergem em probabilidade para seus valores verdadeiros $\alpha$ e $\delta$, respectivamente.

*Prova:*
I. A consist√™ncia dos estimadores de OLS √© uma consequ√™ncia das propriedades de converg√™ncia do m√©todo de m√≠nimos quadrados.
II. Sob as condi√ß√µes do modelo de regress√£o cl√°ssico (incluindo o erro com m√©dia zero e vari√¢ncia finita), os estimadores de OLS s√£o consistentes.
III. Dado que os erros padr√£o tamb√©m convergem para um valor n√£o nulo, os estimadores de OLS convergem para seus valores reais. [^1]  $\blacksquare$

**Teorema 1.1**
Sob a hip√≥tese nula $H_0: r_1\alpha + r_2\delta = r$, a estat√≠stica $t_r$ definida acima converge em distribui√ß√£o para uma distribui√ß√£o normal padr√£o $N(0,1)$.

*Prova:*
I. O numerador da estat√≠stica $t_r$, $\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)$, converge em distribui√ß√£o para uma normal com m√©dia zero, j√° que $\hat{\alpha}_T$ e $\hat{\delta}_T$ s√£o estimadores consistentes.
II. Os erros padr√£o capturam as diferentes taxas de converg√™ncia de $\hat{\alpha}_T$ e $\hat{\delta}_T$.
III. O denominador, que √© o erro padr√£o reescalonado da combina√ß√£o linear, converge em probabilidade para um valor n√£o nulo.
IV. Portanto, a raz√£o, $t_r$, converge em distribui√ß√£o para uma distribui√ß√£o normal padr√£o $N(0,1)$. $\blacksquare$

### Conclus√£o
A an√°lise detalhada dos testes t de OLS no contexto de modelos de tend√™ncia temporal determin√≠stica demonstra a robustez desses testes, apesar das diferentes taxas de converg√™ncia dos estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$. O reescalonamento adequado das estat√≠sticas de teste com suas respectivas taxas de converg√™ncia e a incorpora√ß√£o dessas taxas nos erros padr√£o garantem que os testes sejam assintoticamente v√°lidos, ou seja, que sigam uma distribui√ß√£o normal padr√£o. Essa valida√ß√£o se estende a hip√≥teses lineares envolvendo par√¢metros com diferentes taxas de converg√™ncia, devido ao dom√≠nio dos par√¢metros com taxas mais lentas, e a testes mais gerais envolvendo m√∫ltiplas restri√ß√µes lineares. A equival√™ncia assint√≥tica entre testes no modelo original e em formas transformadas (canonical form) tamb√©m √© preservada. Esses resultados s√£o fundamentais para a aplica√ß√£o pr√°tica de modelos de tend√™ncia temporal, permitindo infer√™ncias estat√≠sticas confi√°veis sobre os par√¢metros do modelo. As propriedades aqui demonstradas s√£o cruciais para o desenvolvimento de testes de raiz unit√°ria nos pr√≥ximos cap√≠tulos.

### Refer√™ncias

[^1]:  The coefficients of regression models involving unit roots or deterministic time trends are typically estimated by ordinary least squares. However, the asymptotic distributions of the coefficient estimates cannot be calculated in the same way as are those for regression models involving stationary variables. Among other difficulties, the estimates of different parameters will in general have different asymptotic rates of convergence.
[^2]: If e, ~ N(0, œÉ¬≤), then the model [16.1.1] satisfies the classical regression assumptions and the standard OLS t or F statistics in equations [8.1.26] and [8.1.32] would have exact small-sample t or F distributions.
[^3]: This chapter introduces this technique, which will prove useful not only for studying time trends but also for analyzing estimators for a variety of nonstationary processes in Chapters 17 and 18.
[^4]: Recall the approach used to find asymptotic distributions for regressions with stationary explanatory variables in Chapter 8.
[^5]: The usual assumption was that (1/T) Œ£‚ÇÅx,x, converged in probability to a nonsingular matrix Q while (1/V/T) Œ£Œ§, œá,Œµ, converged in distribution to a N(0, œÉ¬≤Q) random variable, implying that VT(b+- Œ≤) N(0, 0¬≤Q-1).
[^6]: It turns out that the OLS estimates √¢, and 87 have different asymptotic rates of convergence. To arrive at nondegenerate limiting distributions, √¢, is multiplied by VT, whereas y must be multiplied by T3/2!
[^7]: From [16.1.19] and [16.1.24], the asymptotic distribution of [16.1.18] can be calculated as in Example 7.5 of Chapter 7: [ŒΩœÑŒ¨œÑ - Œ±) , T32(8œÑ - Œ¥)]  N(0, œÉ¬≤Q-1).
[^8]: Thus, although √¢ and 87 converge at different rates, the corresponding standard errors day and √¥s, also incorporate different orders of T, with the result that the usual OLS t tests are asymptotically valid.
[^9]: It is interesting also to consider a test of a single hypothesis involving both a and 8, Hra + r28 = r, where r1, 12, and r are parameters that describe the hypothesis.
[^10]: At test of H, can be obtained from the square root of the OLS F test (expression [8.1.32]).
[^11]:  A test involving a single restriction across parameters with different rates of convergence is dominated asymptotically by the parameters with the slowest rates of convergence.
[^12]: Thus, again, the usual OLS test is asymptotically valid.
[^13]: Consider a sample of T + p observations on y, {y-p+1, y-p+2,...,y}, and let ·æ∂œÑ, Œ¥œÑ, Œ¶1.œÑ, ..... Œ¶œÅ. denote coefficient estimates based on ordinary least squares estimation of [16.3.1] for t = 1, 2, ..., T.
[^14]:  The idea of transforming the regression into a form such as [16.3.3] is due to Sims, Stock, and Watson (1990).
[^15]:  The system of [16.3.7] is just an algebraically equivalent representation of the regression model [16.3.5].
[^16]: This means that some of the elements of R* may be irrelevant asymptotically, so that [16.3.22] has the same asymptotic distribution as a simpler expression.
<!-- END -->
