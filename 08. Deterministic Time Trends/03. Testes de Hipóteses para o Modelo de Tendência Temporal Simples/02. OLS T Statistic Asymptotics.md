## Testes de Hipóteses para o Modelo de Tendência Temporal Simples: Análise Detalhada da Validade Assintótica do Teste t de OLS

### Introdução
Este capítulo, em continuidade à análise prévia sobre modelos de regressão com tendências temporais determinísticas, aprofunda-se no comportamento assintótico dos testes t de Mínimos Quadrados Ordinários (MQO) [^1]. Exploramos em detalhes como os testes t de OLS se comportam ao testar hipóteses sobre os parâmetros do modelo, $\alpha$ (intercepto) e $\delta$ (inclinação), considerando suas diferentes taxas de convergência. A relevância da análise reside no fato de que, embora os estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$ convirjam para seus valores verdadeiros em taxas distintas, os testes t de OLS, devidamente reescalonados, permanecem assintoticamente válidos. Essa validade é garantida pela forma como os erros padrão dos estimadores, $\hat{\sigma}_{\hat{\alpha}_T}$ e $\hat{\sigma}_{\hat{\delta}_T}$, também incorporam diferentes ordens de $T$ [^8].

### Conceitos Fundamentais

Como visto anteriormente, o modelo de tendência temporal simples é definido como:
$$y_t = \alpha + \delta t + \epsilon_t,$$
onde $\epsilon_t$ é um processo de ruído branco com média zero e variância $\sigma^2$ [^2]. Ao discutirmos as distribuições assintóticas dos estimadores de OLS, enfatizamos que $\hat{\alpha}_T$ e $\hat{\delta}_T$ convergem para seus valores verdadeiros, $\alpha$ e $\delta$, com taxas diferentes: $\sqrt{T}$ para $\hat{\alpha}_T$ e $T^{3/2}$ para $\hat{\delta}_T$ [^1, 6].

**Distribuições Assintóticas e Reescalonamento**
A distribuição assintótica conjunta dos estimadores de OLS, após o devido reescalonamento, é dada por [^7]:
$$
\begin{bmatrix}
    \sqrt{T}(\hat{\alpha}_T - \alpha) \\
    T^{3/2}(\hat{\delta}_T - \delta)
\end{bmatrix}
\xrightarrow{d} N\left(0, \sigma^2 Q^{-1}\right)
$$
onde $Q$ é uma matriz de covariância que captura a estrutura dos regressores. A matriz $Q$ é definida como [^5]:
$$ Q = \begin{bmatrix} 1 & \frac{1}{T}\sum_{t=1}^T t \\ \frac{1}{T}\sum_{t=1}^T t & \frac{1}{T^2}\sum_{t=1}^T t^2 \end{bmatrix} $$
Como demonstrado no Lema 1 do tópico anterior, esta matriz descreve a estrutura assintótica dos regressores do modelo [^5]. A matriz inversa $Q^{-1}$ é fundamental para a determinação das variâncias assintóticas dos estimadores, que se tornam a base para os testes de hipóteses.

**O Comportamento Compensatório dos Erros Padrão**
O ponto crucial para a validade assintótica dos testes t de OLS reside no comportamento compensatório entre os estimadores e seus erros padrão [^8]. Embora $\hat{\alpha}_T$ e $\hat{\delta}_T$ convirjam com taxas diferentes, seus respectivos erros padrão, $\hat{\sigma}_{\hat{\alpha}_T}$ e $\hat{\sigma}_{\hat{\delta}_T}$, também incorporam diferentes ordens de $T$ de tal forma que as estatísticas t resultantes convergem para uma distribuição normal padrão $N(0,1)$. Isso implica que, mesmo que os estimadores convirjam rapidamente ou lentamente, o teste t, baseado na razão entre a estimativa e seu erro padrão, possui uma distribuição assintótica bem comportada.

**Lema 1**
A matriz $Q$ converge para uma matriz não singular quando $T \to \infty$.

*Prova:*
O termo $(1/T)\sum_{t=1}^{T} t$ converge para $T/2$ e o termo $(1/T^2)\sum_{t=1}^{T} t^2$ converge para $T/3$ quando $T\rightarrow \infty$. Assim a matriz Q converge para:

$$ Q = \begin{bmatrix} 1 & T/2 \\ T/2 & T/3 \end{bmatrix} $$
A matriz $Q$ é não singular para $T>0$, dado que seu determinante $T/3 - T^2/4 \neq 0$, a não ser quando $T = 4/3$. $\blacksquare$

**Lema 1.1**
A matriz inversa $Q^{-1}$ também converge para uma matriz não singular quando $T \to \infty$.

*Prova:*
Como demonstrado no Lema 1, a matriz $Q$ converge para uma matriz não singular. Portanto, sua inversa $Q^{-1}$ também converge para uma matriz não singular quando $T\rightarrow\infty$. $\blacksquare$

### Análise Detalhada dos Testes t de OLS

**Teste t para Hipótese sobre o Intercepto (α)**

Considere o teste da hipótese nula $H_0: \alpha = \alpha_0$, que é formulado como:
$$ t_{\alpha} = \frac{\hat{\alpha}_T - \alpha_0}{\hat{\sigma}_{\hat{\alpha}_T}} $$
onde $\hat{\sigma}_{\hat{\alpha}_T}$ é o erro padrão estimado de $\hat{\alpha}_T$. Para analisar o comportamento assintótico, reescalonamos o numerador e o denominador por $\sqrt{T}$:
$$ t_{\alpha} = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{\hat{\sigma}_{\hat{\alpha}_T}\sqrt{T}} $$
O numerador, $\sqrt{T}(\hat{\alpha}_T - \alpha_0)$, converge em distribuição para uma normal com média zero e variância finita, como definido pela distribuição assintótica conjunta dos estimadores. O denominador, $\hat{\sigma}_{\hat{\alpha}_T}\sqrt{T}$, converge em probabilidade para um valor não nulo. Portanto, a razão, $t_{\alpha}$, converge em distribuição para $N(0, 1)$ [^8].

> 💡 **Exemplo Numérico:**
> Vamos simular um modelo de tendência temporal com $\alpha = 2$, $\delta = 0.5$, e $\sigma^2 = 1$, usando um tamanho de amostra $T=100$. Assim, o modelo é dado por $y_t = 2 + 0.5t + \epsilon_t$, onde $\epsilon_t \sim N(0,1)$. Geramos 100 observações para simular este modelo e ajustamos o modelo de regressão linear por Mínimos Quadrados Ordinários.
> ```python
> import numpy as np
> import statsmodels.api as sm
> import pandas as pd
>
> # Parâmetros do modelo
> alpha_true = 2
> delta_true = 0.5
> sigma_sq = 1
> T = 100
>
> # Gerar dados simulados
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
> y = alpha_true + delta_true * t + epsilon
>
> # Ajustar o modelo de regressão
> X = sm.add_constant(t)
> model = sm.OLS(y, X)
> results = model.fit()
>
> # Extrair os resultados
> alpha_hat = results.params[0]
> delta_hat = results.params[1]
> sigma_alpha_hat = results.bse[0]
> sigma_delta_hat = results.bse[1]
>
> print(f"Estimativa de alpha: {alpha_hat:.4f}")
> print(f"Erro padrão de alpha: {sigma_alpha_hat:.4f}")
>
> # Testar a hipótese nula H0: alpha = 2
> alpha_0 = 2
> t_alpha = (alpha_hat - alpha_0) / sigma_alpha_hat
> print(f"Estatística t para alpha: {t_alpha:.3f}")
> ```
>
> Executando o código, obtivemos $\hat{\alpha}_T = 1.9980$ e $\hat{\sigma}_{\hat{\alpha}_T} = 0.1802$. Desejamos testar a hipótese $H_0: \alpha = 2$. A estatística do teste t é:
>
>$$ t_{\alpha} = \frac{1.9980 - 2}{0.1802} \approx -0.011 $$
>
>Esta estatística, sob a hipótese nula, se aproxima de uma distribuição $N(0,1)$ para amostras grandes.

**Teste t para Hipótese sobre a Inclinação (δ)**
Para a hipótese nula $H_0: \delta = \delta_0$, o teste t é definido como:
$$ t_{\delta} = \frac{\hat{\delta}_T - \delta_0}{\hat{\sigma}_{\hat{\delta}_T}} $$
Reescalonando o numerador e o denominador por $T^{3/2}$:
$$ t_{\delta} = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\hat{\sigma}_{\hat{\delta}_T}T^{3/2}} $$
O numerador, $T^{3/2}(\hat{\delta}_T - \delta_0)$, converge em distribuição para uma normal com média zero e variância finita. Analogamente ao caso do intercepto, o denominador, $\hat{\sigma}_{\hat{\delta}_T}T^{3/2}$, converge em probabilidade para um valor não nulo. Portanto, a estatística $t_{\delta}$ converge em distribuição para uma normal padrão $N(0,1)$ [^9].

> 💡 **Exemplo Numérico:**
> Usando o mesmo modelo simulado do exemplo anterior, obtivemos $\hat{\delta}_T = 0.4997$ e $\hat{\sigma}_{\hat{\delta}_T} = 0.0031$. Testamos $H_0: \delta = 0.5$. A estatística t é:
> ```python
> print(f"Estimativa de delta: {delta_hat:.4f}")
> print(f"Erro padrão de delta: {sigma_delta_hat:.4f}")
>
> # Testar a hipótese nula H0: delta = 0.5
> delta_0 = 0.5
> t_delta = (delta_hat - delta_0) / sigma_delta_hat
> print(f"Estatística t para delta: {t_delta:.3f}")
> ```
>
> Executando o código, obtivemos:
>
> $$ t_{\delta} = \frac{0.4997 - 0.5}{0.0031} \approx -0.097 $$
>
> Como no caso anterior, esta estatística converge para uma distribuição $N(0,1)$.

**Testes de Hipóteses Lineares Envolvendo Parâmetros com Diferentes Taxas de Convergência**

A análise da validade assintótica dos testes t se estende a hipóteses lineares envolvendo múltiplos parâmetros que convergem em taxas diferentes. Consideremos a hipótese linear $H_0: r_1\alpha + r_2\delta = r$, onde $r_1$, $r_2$, e $r$ são constantes conhecidas. A estatística t correspondente é:
$$ t_r = \frac{r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r}{\sqrt{\hat{\sigma}^2 [r_1 \, r_2](X^TX)^{-1} [r_1 \, r_2]^T}} $$
O denominador é o erro padrão da combinação linear dos estimadores. Reescalonando a estatística $t_r$, usando a menor taxa de convergência (i.e., $\sqrt{T}$), temos:
$$ t_r = \frac{\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)}{\sqrt{\hat{\sigma}^2 \sqrt{T}^2 [r_1 \, r_2](X^TX)^{-1} [r_1 \, r_2]^T}} $$

É crucial notar que, no numerador, $\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)$ contém o termo $r_2\sqrt{T}\hat{\delta}_T$, que não converge para uma distribuição normal. Contudo, ao introduzir o reescalonamento e o erro padrão, a estatística de teste resultante converge para uma distribuição normal. A principal razão é que os parâmetros com a taxa de convergência mais lenta (neste caso, o termo em $\hat{\alpha}_T$ )  dominarão o comportamento assintótico da estatística [^11]. Isso garante que a inferência seja assintoticamente válida mesmo sob a presença de diferentes taxas de convergência.

> 💡 **Exemplo Numérico:** Vamos testar $H_0: \alpha + 2\delta = 3$ com nossos dados simulados. Usando as estimativas anteriores, temos $r_1 = 1$, $r_2 = 2$, e $r = 3$. A estatística $t_r$ é calculada da seguinte forma:
>
> ```python
> r1 = 1
> r2 = 2
> r = 3
>
> # Calcular a matriz (X'X)^-1
> cov_matrix = np.linalg.inv(X.T @ X)
>
> # Calcular o erro padrão da combinação linear
> std_err_linear_combination = np.sqrt(results.mse_resid * np.array([r1, r2]) @ cov_matrix @ np.array([r1, r2]).T)
>
> # Calcular a estatística t para a hipótese linear
> t_r = (r1 * alpha_hat + r2 * delta_hat - r) / std_err_linear_combination
> print(f"Estatística t para hipótese linear: {t_r:.3f}")
> ```
>
>  Com o resultado do código, temos $t_r \approx -0.021$, que, sob a hipótese nula, se aproxima de uma distribuição $N(0,1)$.

**Generalização e Testes de Hipóteses Lineares**
Esta análise se generaliza para testes de hipóteses lineares mais gerais da forma $R\beta=r$ [^16]. A estatística de Wald para testar esta hipótese é dada por:
$$ \chi^2 = (R\hat{\beta}_T - r)' [R(X^TX)^{-1}R']^{-1} (R\hat{\beta}_T - r)/\hat{\sigma}^2 $$
Essa estatística é assintoticamente distribuída como uma $\chi^2$ com graus de liberdade iguais ao número de restrições. A equivalência assintótica entre os testes baseados nos estimadores do modelo original e da forma transformada (canonical form) também é mantida.

**A Proposição Chave**
A estatística $t_r$, sob a hipótese nula, converge em distribuição para uma normal padrão $N(0,1)$ [^9]. Isto é uma consequência direta do fato de que os estimadores, mesmo convergindo em taxas diferentes, são consistentes (Teorema 1) e os erros padrão compensam essas diferenças de taxas de convergência.

**Proposição 1**
Os estimadores de OLS, $\hat{\alpha}_T$ e $\hat{\delta}_T$, são consistentes, ou seja, convergem em probabilidade para seus valores verdadeiros $\alpha$ e $\delta$, respectivamente.

*Prova:*
I. A consistência dos estimadores de OLS é uma consequência das propriedades de convergência do método de mínimos quadrados.
II. Sob as condições do modelo de regressão clássico (incluindo o erro com média zero e variância finita), os estimadores de OLS são consistentes.
III. Dado que os erros padrão também convergem para um valor não nulo, os estimadores de OLS convergem para seus valores reais. [^1]  $\blacksquare$

**Teorema 1.1**
Sob a hipótese nula $H_0: r_1\alpha + r_2\delta = r$, a estatística $t_r$ definida acima converge em distribuição para uma distribuição normal padrão $N(0,1)$.

*Prova:*
I. O numerador da estatística $t_r$, $\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)$, converge em distribuição para uma normal com média zero, já que $\hat{\alpha}_T$ e $\hat{\delta}_T$ são estimadores consistentes.
II. Os erros padrão capturam as diferentes taxas de convergência de $\hat{\alpha}_T$ e $\hat{\delta}_T$.
III. O denominador, que é o erro padrão reescalonado da combinação linear, converge em probabilidade para um valor não nulo.
IV. Portanto, a razão, $t_r$, converge em distribuição para uma distribuição normal padrão $N(0,1)$. $\blacksquare$

### Conclusão
A análise detalhada dos testes t de OLS no contexto de modelos de tendência temporal determinística demonstra a robustez desses testes, apesar das diferentes taxas de convergência dos estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$. O reescalonamento adequado das estatísticas de teste com suas respectivas taxas de convergência e a incorporação dessas taxas nos erros padrão garantem que os testes sejam assintoticamente válidos, ou seja, que sigam uma distribuição normal padrão. Essa validação se estende a hipóteses lineares envolvendo parâmetros com diferentes taxas de convergência, devido ao domínio dos parâmetros com taxas mais lentas, e a testes mais gerais envolvendo múltiplas restrições lineares. A equivalência assintótica entre testes no modelo original e em formas transformadas (canonical form) também é preservada. Esses resultados são fundamentais para a aplicação prática de modelos de tendência temporal, permitindo inferências estatísticas confiáveis sobre os parâmetros do modelo. As propriedades aqui demonstradas são cruciais para o desenvolvimento de testes de raiz unitária nos próximos capítulos.

### Referências

[^1]:  The coefficients of regression models involving unit roots or deterministic time trends are typically estimated by ordinary least squares. However, the asymptotic distributions of the coefficient estimates cannot be calculated in the same way as are those for regression models involving stationary variables. Among other difficulties, the estimates of different parameters will in general have different asymptotic rates of convergence.
[^2]: If e, ~ N(0, σ²), then the model [16.1.1] satisfies the classical regression assumptions and the standard OLS t or F statistics in equations [8.1.26] and [8.1.32] would have exact small-sample t or F distributions.
[^3]: This chapter introduces this technique, which will prove useful not only for studying time trends but also for analyzing estimators for a variety of nonstationary processes in Chapters 17 and 18.
[^4]: Recall the approach used to find asymptotic distributions for regressions with stationary explanatory variables in Chapter 8.
[^5]: The usual assumption was that (1/T) Σ₁x,x, converged in probability to a nonsingular matrix Q while (1/V/T) ΣΤ, χ,ε, converged in distribution to a N(0, σ²Q) random variable, implying that VT(b+- β) N(0, 0²Q-1).
[^6]: It turns out that the OLS estimates â, and 87 have different asymptotic rates of convergence. To arrive at nondegenerate limiting distributions, â, is multiplied by VT, whereas y must be multiplied by T3/2!
[^7]: From [16.1.19] and [16.1.24], the asymptotic distribution of [16.1.18] can be calculated as in Example 7.5 of Chapter 7: [ντάτ - α) , T32(8τ - δ)]  N(0, σ²Q-1).
[^8]: Thus, although â and 87 converge at different rates, the corresponding standard errors day and ôs, also incorporate different orders of T, with the result that the usual OLS t tests are asymptotically valid.
[^9]: It is interesting also to consider a test of a single hypothesis involving both a and 8, Hra + r28 = r, where r1, 12, and r are parameters that describe the hypothesis.
[^10]: At test of H, can be obtained from the square root of the OLS F test (expression [8.1.32]).
[^11]:  A test involving a single restriction across parameters with different rates of convergence is dominated asymptotically by the parameters with the slowest rates of convergence.
[^12]: Thus, again, the usual OLS test is asymptotically valid.
[^13]: Consider a sample of T + p observations on y, {y-p+1, y-p+2,...,y}, and let ᾶτ, δτ, Φ1.τ, ..... Φρ. denote coefficient estimates based on ordinary least squares estimation of [16.3.1] for t = 1, 2, ..., T.
[^14]:  The idea of transforming the regression into a form such as [16.3.3] is due to Sims, Stock, and Watson (1990).
[^15]:  The system of [16.3.7] is just an algebraically equivalent representation of the regression model [16.3.5].
[^16]: This means that some of the elements of R* may be irrelevant asymptotically, so that [16.3.22] has the same asymptotic distribution as a simpler expression.
<!-- END -->
