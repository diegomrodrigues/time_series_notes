## Testes t de OLS e a Validade Assint√≥tica: Uma An√°lise Detalhada

### Introdu√ß√£o
Este cap√≠tulo se aprofunda na an√°lise da validade assint√≥tica do teste t de M√≠nimos Quadrados Ordin√°rios (OLS) em modelos com tend√™ncias temporais determin√≠sticas, especificamente no contexto do modelo de tend√™ncia temporal simples [^1]. Em particular, focaremos em como as estat√≠sticas de teste t se comportam quando testamos hip√≥teses nulas sobre o intercepto ($\alpha$) e o coeficiente de tend√™ncia ($\delta$), e como essas estat√≠sticas, embora envolvam estimadores com diferentes taxas de converg√™ncia, convergem para uma distribui√ß√£o normal padr√£o $N(0,1)$. Em outras palavras, este cap√≠tulo detalha os mecanismos pelos quais o reescalonamento adequado da estat√≠stica t e a incorpora√ß√£o das diferentes ordens de $T$ nos erros padr√£o garantem infer√™ncias assintoticamente v√°lidas.

### Conceitos Fundamentais
Como estabelecido anteriormente, o modelo de tend√™ncia temporal simples √© definido como:

$$y_t = \alpha + \delta t + \epsilon_t$$

onde $\epsilon_t$ representa um processo de ru√≠do branco com m√©dia zero e vari√¢ncia constante $\sigma^2$ [^2]. Os estimadores de m√≠nimos quadrados ordin√°rios (OLS) para $\alpha$ e $\delta$, denotados como $\hat{\alpha}_T$ e $\hat{\delta}_T$, respectivamente, convergem para seus valores verdadeiros com taxas distintas. Especificamente, $\hat{\alpha}_T$ converge a uma taxa de $\sqrt{T}$, enquanto $\hat{\delta}_T$ converge a uma taxa de $T^{3/2}$ [^1, 6].

> üí° **Exemplo Num√©rico:**
>
> Vamos simular um conjunto de dados para ilustrar essas taxas de converg√™ncia. Considere um modelo com $\alpha = 2$, $\delta = 0.5$, $\sigma^2 = 1$ e $T = 100$. Geraremos dados usando este modelo e ajustaremos um modelo OLS para estimar $\hat{\alpha}_T$ e $\hat{\delta}_T$.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
>
> # Par√¢metros do modelo
> alpha_true = 2
> delta_true = 0.5
> sigma_sq = 1
> T = 100
>
> # Gera√ß√£o de dados
> np.random.seed(42)
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
> y = alpha_true + delta_true * t + epsilon
>
> # Ajuste do modelo OLS
> X = sm.add_constant(t)
> model = sm.OLS(y, X)
> results = model.fit()
>
> alpha_hat = results.params[0]
> delta_hat = results.params[1]
>
> print(f"Estimativa de alpha (alpha_hat): {alpha_hat:.4f}")
> print(f"Estimativa de delta (delta_hat): {delta_hat:.4f}")
>
> # Gr√°fico dos dados e da reta ajustada
> plt.scatter(t, y, label='Dados Simulados')
> plt.plot(t, results.fittedvalues, color='red', label='Reta Ajustada')
> plt.xlabel('t')
> plt.ylabel('y')
> plt.title('Dados Simulados e Reta de Regress√£o')
> plt.legend()
> plt.show()
>
> # C√°lculo dos erros padr√£o para alpha e delta
> se_alpha = results.bse[0]
> se_delta = results.bse[1]
>
> print(f"Erro padr√£o de alpha (se_alpha): {se_alpha:.4f}")
> print(f"Erro padr√£o de delta (se_delta): {se_delta:.4f}")
> ```
> Este exemplo demonstra como os estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$ s√£o calculados. √Ä medida que $T$ aumenta, $\hat{\alpha}_T$ e $\hat{\delta}_T$ se aproximam de seus valores reais e os erros padr√£o (se_alpha, se_delta) diminuem. O c√≥digo tamb√©m plota os dados simulados e a linha de regress√£o ajustada, o que pode ajudar a visualizar o modelo.

**Distribui√ß√µes Assint√≥ticas e o Papel do Reescalonamento**

A distribui√ß√£o assint√≥tica conjunta dos estimadores, ap√≥s o reescalonamento apropriado, √© dada por:

$$
\begin{bmatrix}
\sqrt{T}(\hat{\alpha}_T - \alpha) \\
T^{3/2}(\hat{\delta}_T - \delta)
\end{bmatrix}
\xrightarrow{d} N\left(0, \sigma^2 Q^{-1}\right)
$$

onde $Q$ √© a matriz de covari√¢ncia dos regressores, definida como:

$$ Q = \begin{bmatrix} 1 & \frac{1}{T}\sum_{t=1}^T t \\ \frac{1}{T}\sum_{t=1}^T t & \frac{1}{T^2}\sum_{t=1}^T t^2 \end{bmatrix} $$

Essa matriz captura a estrutura dos regressores no modelo [^5]. A matriz inversa $Q^{-1}$ √© fundamental para determinar as vari√¢ncias assint√≥ticas dos estimadores, que s√£o usadas para construir os testes de hip√≥teses.

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar a matriz $Q$, vamos considerar o caso em que $T=100$.
>
> ```python
> T = 100
> t = np.arange(1, T + 1)
> Q = np.array([[1, np.mean(t)], [np.mean(t), np.mean(t**2)]])
> print("Matriz Q para T=100:\n", Q)
> ```
>
> Este exemplo calcula a matriz $Q$ para $T=100$. Note que os termos da matriz $Q$ convergem para valores espec√≠ficos √† medida que $T$ aumenta, conforme demonstrado no Lema 1. Os valores calculados da matriz Q ser√£o cruciais para calcular as vari√¢ncias assint√≥ticas dos estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$.

O conceito fundamental para entender a validade assint√≥tica dos testes t reside no comportamento compensat√≥rio entre os estimadores e seus erros padr√£o [^8]. Apesar das diferentes taxas de converg√™ncia, os erros padr√£o dos estimadores, $\hat{\sigma}_{\hat{\alpha}_T}$ e $\hat{\sigma}_{\hat{\delta}_T}$, tamb√©m incorporam diferentes ordens de $T$, garantindo que as estat√≠sticas de teste t, quando devidamente reescalonadas, convirjam para uma distribui√ß√£o normal padr√£o $N(0,1)$.

**Lema 1**
A matriz $Q$, definida anteriormente, converge para uma matriz n√£o singular √† medida que $T$ tende ao infinito.
*Prova:*
I. O termo $(1/T) \sum_{t=1}^T t$ pode ser reescrito como $(1/T) * T(T+1)/2 = (T+1)/2$ que converge para $T/2$ quando $T\rightarrow \infty$.
II. O termo $(1/T^2) \sum_{t=1}^T t^2$ pode ser reescrito como $(1/T^2) * T(T+1)(2T+1)/6 = (1+1/T)(2+1/T)T/6$ que converge para $T/3$ quando $T\rightarrow \infty$.
III.  Assim, a matriz Q converge para:
$$ \lim_{T\to\infty} Q = \begin{bmatrix} 1 & T/2 \\ T/2 & T^2/3 \end{bmatrix} $$
IV. Essa matriz √© n√£o singular para $T > 0$, dado que seu determinante, $T^2/3 - T^2/4 = T^2/12 \neq 0$  para $T \neq 0$. $\blacksquare$

**Lema 1.1**
A matriz inversa $Q^{-1}$ tamb√©m converge para uma matriz n√£o singular quando $T\rightarrow \infty$.
*Prova:*
I. Como demonstrado no Lema 1, a matriz $Q$ converge para uma matriz n√£o singular quando $T\rightarrow \infty$.
II.  Portanto, sua inversa, $Q^{-1}$, tamb√©m converge para uma matriz n√£o singular para $T\rightarrow \infty$. $\blacksquare$

**Lema 1.2**
A matriz $Q$ converge para uma matriz que, quando normalizada por $T$, tem um limite n√£o singular. Formalmente, a matriz $\frac{1}{T}Q$ converge para uma matriz n√£o singular quando $T\rightarrow \infty$.
*Prova:*
I. Do Lema 1, sabemos que $\lim_{T\to\infty} Q = \begin{bmatrix} 1 & T/2 \\ T/2 & T^2/3 \end{bmatrix}$.
II. Multiplicando $Q$ por $1/T$, obtemos
$$\frac{1}{T}Q = \begin{bmatrix} 1/T & 1/2 \\ 1/2 & T/3 \end{bmatrix}$$
III.  Para analisar o comportamento desta matriz quando $T \to \infty$, considere a matriz $Q^* = \frac{1}{T}Q$
$$ \lim_{T\to\infty} \frac{1}{T} Q = \lim_{T\to\infty} Q^* = \begin{bmatrix} 0 & 1/2 \\ 1/2 & T/3 \end{bmatrix} $$
IV. Para  normalizar  a matriz $\frac{1}{T}Q$ por $T$ temos:
$$ \frac{1}{T}\begin{bmatrix} 1 & T/2 \\ T/2 & T^2/3 \end{bmatrix} = \begin{bmatrix} 1/T & 1/2 \\ 1/2 & T/3 \end{bmatrix} $$
V. Para obter uma matriz limite n√£o-singular, multiplicamos a matriz $Q^*$ por $1/T$:
$$ \frac{1}{T}Q^* = \begin{bmatrix} \frac{1}{T^2} & \frac{1}{2T} \\ \frac{1}{2T} & \frac{1}{3} \end{bmatrix} $$
VI. Assim,
$$ \lim_{T\to\infty} \frac{1}{T}Q^* = \begin{bmatrix} 0 & 0 \\ 0 & 1/3 \end{bmatrix} $$
Esta matriz ainda √© singular. Para obtermos uma matriz n√£o singular podemos usar a matriz de reescalonamento.
VII. No entanto, podemos reescalonar a matriz original da seguinte forma:
 $$ \lim_{T \to \infty} \begin{bmatrix} 1/T & 0 \\ 0 & 1/T^2 \end{bmatrix}  \begin{bmatrix} 1 & T/2 \\ T/2 & T^2/3 \end{bmatrix} =  \lim_{T \to \infty} \begin{bmatrix} 1/T & 1/2 \\ 1/2 & 1/3 \end{bmatrix} =  \begin{bmatrix} 0 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} $$
Esta matriz tem determinante diferente de zero. $\blacksquare$

### Detalhamento da Validade Assint√≥tica do Teste t de OLS

**Teste t para Hip√≥tese sobre o Intercepto ($\alpha$)**

Considere o teste da hip√≥tese nula $H_0: \alpha = \alpha_0$. A estat√≠stica t correspondente √© dada por:

$$ t_{\alpha} = \frac{\hat{\alpha}_T - \alpha_0}{\hat{\sigma}_{\hat{\alpha}_T}} $$

onde $\hat{\sigma}_{\hat{\alpha}_T}$ √© o erro padr√£o estimado de $\hat{\alpha}_T$. Para demonstrar a validade assint√≥tica do teste, reescalonamos o numerador e o denominador por $\sqrt{T}$:

$$ t_{\alpha} = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{\hat{\sigma}_{\hat{\alpha}_T}\sqrt{T}} $$

O numerador, $\sqrt{T}(\hat{\alpha}_T - \alpha_0)$, converge em distribui√ß√£o para uma vari√°vel normal com m√©dia zero e vari√¢ncia finita. O denominador, $\hat{\sigma}_{\hat{\alpha}_T}\sqrt{T}$, converge em probabilidade para um valor n√£o nulo. Assim, a estat√≠stica de teste $t_{\alpha}$ converge em distribui√ß√£o para uma normal padr√£o $N(0, 1)$ [^8].

*Prova Detalhada:*

I. O numerador, $\sqrt{T}(\hat{\alpha}_T - \alpha_0)$, pode ser expresso como:
$$ \sqrt{T}(\hat{\alpha}_T - \alpha_0) = \left[\sqrt{T},0\right]\begin{bmatrix}\hat{\alpha}_T - \alpha_0 \\\hat{\delta}_T - \delta_0 \end{bmatrix} $$
II. Utilizando o resultado da distribui√ß√£o assint√≥tica conjunta dos estimadores, temos:
$$
\begin{bmatrix}
\sqrt{T}(\hat{\alpha}_T - \alpha) \\
T^{3/2}(\hat{\delta}_T - \delta)
\end{bmatrix}
\xrightarrow{d} N\left(0, \sigma^2 Q^{-1}\right)
$$
III. Portanto, $\sqrt{T}(\hat{\alpha}_T - \alpha)$ converge para uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia dada pelo elemento (1,1) da matriz $\sigma^2 Q^{-1}$.
IV. O denominador √© o erro padr√£o de $\hat{\alpha}_T$ multiplicado por $\sqrt{T}$. Pela consist√™ncia dos estimadores e de suas vari√¢ncias, $\hat{\sigma}_{\hat{\alpha}_T}\sqrt{T}$ converge em probabilidade para um valor n√£o nulo.
V. Assim, pela aplica√ß√£o do teorema de Slutsky, a estat√≠stica $t_{\alpha}$ converge em distribui√ß√£o para uma normal padr√£o $N(0, 1)$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Vamos testar a hip√≥tese nula $H_0: \alpha = 2$ usando os dados simulados anteriormente e calcular a estat√≠stica $t_{\alpha}$.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> from scipy.stats import t
>
> # Par√¢metros do modelo
> alpha_true = 2
> delta_true = 0.5
> sigma_sq = 1
> T = 100
>
> # Gera√ß√£o de dados
> np.random.seed(42)
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
> y = alpha_true + delta_true * t + epsilon
>
> # Ajuste do modelo OLS
> X = sm.add_constant(t)
> model = sm.OLS(y, X)
> results = model.fit()
>
> alpha_hat = results.params[0]
> se_alpha = results.bse[0]
>
> # Valor nulo para o teste
> alpha_0 = 2
>
> # C√°lculo da estat√≠stica t
> t_alpha = (alpha_hat - alpha_0) / se_alpha
>
> # Grau de liberdade
> df = T - 2
>
> # C√°lculo do p-valor
> p_valor_alpha = 2 * (1 - t.cdf(abs(t_alpha), df))
>
> print(f"Estat√≠stica t para alpha: {t_alpha:.4f}")
> print(f"P-valor para alpha: {p_valor_alpha:.4f}")
>
> # Compara o p-valor com o n√≠vel de signific√¢ncia
> nivel_significancia = 0.05
> if p_valor_alpha < nivel_significancia:
>    print("Rejeita a hip√≥tese nula H0: alpha = 2")
> else:
>    print("N√£o rejeita a hip√≥tese nula H0: alpha = 2")
> ```
>
> Este exemplo calcula a estat√≠stica $t_{\alpha}$ e seu p-valor associado. O resultado indica se temos evid√™ncias suficientes para rejeitar a hip√≥tese nula de que $\alpha = 2$. Com um p-valor baixo, rejeitamos a hip√≥tese nula. Se o p-valor for alto (acima do n√≠vel de signific√¢ncia), n√£o podemos rejeitar a hip√≥tese nula.

**Teste t para Hip√≥tese sobre a Inclina√ß√£o ($\delta$)**
De maneira an√°loga, para testar a hip√≥tese nula $H_0: \delta = \delta_0$, a estat√≠stica t √© definida como:
$$ t_{\delta} = \frac{\hat{\delta}_T - \delta_0}{\hat{\sigma}_{\hat{\delta}_T}} $$

Reescalonando o numerador e o denominador por $T^{3/2}$:

$$ t_{\delta} = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\hat{\sigma}_{\hat{\delta}_T}T^{3/2}} $$

Aqui, $T^{3/2}(\hat{\delta}_T - \delta_0)$ converge em distribui√ß√£o para uma normal com m√©dia zero e vari√¢ncia finita. O denominador, $\hat{\sigma}_{\hat{\delta}_T}T^{3/2}$, converge em probabilidade para um valor n√£o nulo. Portanto, $t_{\delta}$ converge em distribui√ß√£o para uma normal padr√£o $N(0, 1)$ [^9].

*Prova Detalhada:*

I. O numerador, $T^{3/2}(\hat{\delta}_T - \delta_0)$, pode ser expresso como:
$$ T^{3/2}(\hat{\delta}_T - \delta_0) = \left[0,T^{3/2}\right]\begin{bmatrix}\hat{\alpha}_T - \alpha_0 \\\hat{\delta}_T - \delta_0 \end{bmatrix} $$
II. Utilizando o resultado da distribui√ß√£o assint√≥tica conjunta dos estimadores, temos:
$$
\begin{bmatrix}
\sqrt{T}(\hat{\alpha}_T - \alpha) \\
T^{3/2}(\hat{\delta}_T - \delta)
\end{bmatrix}
\xrightarrow{d} N\left(0, \sigma^2 Q^{-1}\right)
$$
III. Portanto, $T^{3/2}(\hat{\delta}_T - \delta)$ converge para uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia dada pelo elemento (2,2) da matriz $\sigma^2 Q^{-1}$.
IV. O denominador √© o erro padr√£o de $\hat{\delta}_T$ multiplicado por $T^{3/2}$. Pela consist√™ncia dos estimadores e de suas vari√¢ncias, $\hat{\sigma}_{\hat{\delta}_T}T^{3/2}$ converge em probabilidade para um valor n√£o nulo.
V.  Assim, pela aplica√ß√£o do teorema de Slutsky, a estat√≠stica $t_{\delta}$ converge em distribui√ß√£o para uma normal padr√£o $N(0, 1)$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Agora, testaremos a hip√≥tese nula $H_0: \delta = 0.5$ e calcularemos a estat√≠stica $t_{\delta}$ usando o mesmo conjunto de dados.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> from scipy.stats import t
>
> # Par√¢metros do modelo
> alpha_true = 2
> delta_true = 0.5
> sigma_sq = 1
> T = 100
>
> # Gera√ß√£o de dados
> np.random.seed(42)
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
> y = alpha_true + delta_true * t + epsilon
>
> # Ajuste do modelo OLS
> X = sm.add_constant(t)
> model = sm.OLS(y, X)
> results = model.fit()
>
> delta_hat = results.params[1]
> se_delta = results.bse[1]
>
> # Valor nulo para o teste
> delta_0 = 0.5
>
> # C√°lculo da estat√≠stica t
> t_delta = (delta_hat - delta_0) / se_delta
>
> # Grau de liberdade
> df = T - 2
>
> # C√°lculo do p-valor
> p_valor_delta = 2 * (1 - t.cdf(abs(t_delta), df))
>
> print(f"Estat√≠stica t para delta: {t_delta:.4f}")
> print(f"P-valor para delta: {p_valor_delta:.4f}")
>
> # Compara o p-valor com o n√≠vel de signific√¢ncia
> nivel_significancia = 0.05
> if p_valor_delta < nivel_significancia:
>    print("Rejeita a hip√≥tese nula H0: delta = 0.5")
> else:
>    print("N√£o rejeita a hip√≥tese nula H0: delta = 0.5")
> ```
>
> Este exemplo calcula a estat√≠stica $t_{\delta}$ e seu p-valor associado. O p-valor ser√° crucial para determinar se temos evid√™ncia estat√≠stica suficiente para rejeitar a hip√≥tese nula de que $\delta = 0.5$.

**Teste t para Hip√≥teses Lineares**
Quando testamos uma hip√≥tese linear envolvendo tanto $\alpha$ quanto $\delta$, como $H_0: r_1\alpha + r_2\delta = r$, a estat√≠stica t √© dada por:

$$ t_r = \frac{r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r}{\sqrt{\hat{\sigma}^2 [r_1 \, r_2](X^TX)^{-1} [r_1 \, r_2]^T}} $$

Para demonstrar a validade assint√≥tica desse teste, multiplicamos o numerador e o denominador pela raiz da taxa de converg√™ncia mais lenta, ou seja, $\sqrt{T}$:

$$ t_r = \frac{\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)}{\sqrt{\hat{\sigma}^2 \sqrt{T}^2 [r_1 \, r_2](X^TX)^{-1} [r_1 \, r_2]^T}} $$
O numerador converge para uma distribui√ß√£o normal devido ao comportamento assint√≥tico dos estimadores. O denominador converge em probabilidade para uma constante n√£o nula, garantindo a converg√™ncia da estat√≠stica t para uma distribui√ß√£o normal padr√£o $N(0, 1)$. A estat√≠stica t para testar hip√≥teses lineares envolvendo par√¢metros com diferentes taxas de converg√™ncia √© dominada assintoticamente pelos par√¢metros com as taxas de converg√™ncia mais lentas [^11].

*Prova Detalhada:*
I. A estat√≠stica de teste pode ser reescrita como:
$$ t_r = \frac{r_1(\hat{\alpha}_T - \alpha) + r_2(\hat{\delta}_T - \delta)}{\hat{\sigma} \sqrt{[r_1,r_2](X^TX)^{-1}[r_1,r_2]^T}} $$
II. Multiplicando o numerador e denominador por $\sqrt{T}$, temos:
$$ t_r = \frac{\sqrt{T}(r_1(\hat{\alpha}_T - \alpha) + r_2(\hat{\delta}_T - \delta))}{\hat{\sigma} \sqrt{T [r_1,r_2](X^TX)^{-1}[r_1,r_2]^T}} $$
III. O termo $\sqrt{T}(r_1(\hat{\alpha}_T - \alpha) + r_2(\hat{\delta}_T - \delta))$ converge em distribui√ß√£o para uma normal com m√©dia zero e vari√¢ncia dada pela combina√ß√£o linear da matriz assint√≥tica de covari√¢ncia dos estimadores.
IV. O termo do denominador converge em probabilidade para uma constante n√£o nula.
V. Portanto, pela aplica√ß√£o do teorema de Slutsky, a estat√≠stica $t_r$ converge em distribui√ß√£o para uma normal padr√£o $N(0, 1)$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Vamos testar a hip√≥tese linear $H_0: \alpha + 2\delta = 3$.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> from scipy.stats import t
>
> # Par√¢metros do modelo
> alpha_true = 2
> delta_true = 0.5
> sigma_sq = 1
> T = 100
>
> # Gera√ß√£o de dados
> np.random.seed(42)
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
> y = alpha_true + delta_true * t + epsilon
>
> # Ajuste do modelo OLS
> X = sm.add_constant(t)
> model = sm.OLS(y, X)
> results = model.fit()
>
> alpha_hat = results.params[0]
> delta_hat = results.params[1]
> sigma_hat_sq = results.mse_resid
>
> # Hip√≥tese linear
> r1 = 1
> r2 = 2
> r = 3
>
> # C√°lculo da estat√≠stica t
> X_mat = np.column_stack((np.ones(T),t))
> cov_matrix = sigma_hat_sq * np.linalg.inv(X_mat.T @ X_mat)
> se_linear = np.sqrt(np.array([r1, r2]) @ cov_matrix @ np.array([r1, r2]).T)
>
> t_r = (r1 * alpha_hat + r2 * delta_hat - r) / se_linear
>
> # Grau de liberdade
> df = T - 2
>
> # C√°lculo do p-valor
> p_valor_r = 2 * (1 - t.cdf(abs(t_r), df))
>
> print(f"Estat√≠stica t para a hip√≥tese linear: {t_r:.4f}")
> print(f"P-valor para a hip√≥tese linear: {p_valor_r:.4f}")
>
> # Compara o p-valor com o n√≠vel de signific√¢ncia
> nivel_significancia = 0.05
> if p_valor_r < nivel_significancia:
>    print("Rejeita a hip√≥tese nula H0: alpha + 2*delta = 3")
> else:
>    print("N√£o rejeita a hip√≥tese nula H0: alpha + 2*delta = 3")
> ```
>
> Este exemplo calcula a estat√≠stica $t_r$ e o p-valor correspondente para a hip√≥tese linear especificada. A interpreta√ß√£o √© semelhante aos testes t para $\alpha$ e $\delta$: se o p-valor for inferior ao n√≠vel de signific√¢ncia, rejeitamos a hip√≥tese nula.

**Observa√ß√£o 1**
√â importante notar que, embora a estat√≠stica t para hip√≥teses lineares seja dominada assintoticamente pelos par√¢metros com as taxas de converg√™ncia mais lentas, a inclus√£o de par√¢metros com taxas de converg√™ncia mais r√°pidas n√£o invalida o resultado. A converg√™ncia em distribui√ß√£o da estat√≠stica t para uma normal padr√£o $N(0,1)$ se mant√©m, garantindo a validade assint√≥tica do teste.

**Teorema 2**
Sob as condi√ß√µes estabelecidas, a estat√≠stica t para qualquer hip√≥tese linear da forma $H_0: R\beta = r$, onde $\beta = [\alpha, \delta]^T$, $R$ √© uma matriz de restri√ß√µes e $r$ √© um vetor de constantes, converge para uma distribui√ß√£o normal padr√£o $N(0,1)$, desde que a hip√≥tese seja testada utilizando os erros padr√£o adequadamente reescalonados.

*Prova:*
I. A estat√≠stica t para a hip√≥tese linear $H_0: R\beta = r$ √© dada por:
$$ t = \frac{R\hat{\beta} - r}{\sqrt{\hat{\sigma}^2 R(X^TX)^{-1}R^T}} $$
II. Reescalonando o numerador e o denominador pela taxa de converg√™ncia apropriada, que √© determinada pela taxa de converg√™ncia mais lenta dos estimadores, neste caso $\sqrt{T}$, temos:
$$ t = \frac{\sqrt{T}(R\hat{\beta} - r)}{\sqrt{\hat{\sigma}^2 \sqrt{T}^2 R(X^TX)^{-1}R^T}} $$
III. O numerador, $\sqrt{T}(R\hat{\beta} - r)$, converge em distribui√ß√£o para uma vari√°vel normal com m√©dia zero. Isso segue da distribui√ß√£o assint√≥tica conjunta dos estimadores, onde $\sqrt{T}(\hat{\alpha}_T-\alpha)$ e $T^{3/2}(\hat{\delta}_T-\delta)$ convergem para uma distribui√ß√£o normal. A combina√ß√£o linear desses termos tamb√©m convergir√° para uma distribui√ß√£o normal.
IV. O denominador, $\sqrt{\hat{\sigma}^2 \sqrt{T}^2 R(X^TX)^{-1}R^T}$, converge em probabilidade para uma constante n√£o nula, dado que $\hat{\sigma}^2$ converge para $\sigma^2$ e a matriz $R(X^TX)^{-1}R^T$ devidamente reescalonada converge para uma matriz n√£o singular.
V. Assim, pela aplica√ß√£o do teorema de Slutsky, a estat√≠stica $t$ converge em distribui√ß√£o para uma normal padr√£o $N(0, 1)$. $\blacksquare$

**O Papel das Matrizes de Reescalonamento**
A utiliza√ß√£o de matrizes de reescalonamento como $\Upsilon_T$ (definidas como [^17]):

$$
\Upsilon_T =
\begin{bmatrix}
\sqrt{T} & 0 \\
0 & T^{3/2}
\end{bmatrix}
$$

permite que os estimadores reescalonados convirjam para distribui√ß√µes limitantes adequadas. Isso garante que, embora os estimadores de $\alpha$ e $\delta$ tenham taxas de converg√™ncia diferentes, os testes t, quando constru√≠dos com os erros padr√£o apropriadamente reescalonados, resultem em infer√™ncias assintoticamente v√°lidas.

### Conclus√£o
A validade assint√≥tica dos testes t de OLS para hip√≥teses sobre os par√¢metros do modelo de tend√™ncia temporal simples √© mantida atrav√©s da combina√ß√£o de estimadores consistentes, erros padr√£o que capturam as diferentes taxas de converg√™ncia e o reescalonamento apropriado da estat√≠stica de teste [^8, 9]. As estat√≠sticas de teste t, quer envolvam testes sobre o intercepto, a inclina√ß√£o ou combina√ß√µes lineares desses par√¢metros, convergem para uma distribui√ß√£o normal padr√£o $N(0,1)$. A an√°lise detalhada apresentada aqui confirma que a infer√™ncia estat√≠stica usando os testes t de OLS √© v√°lida em modelos de tend√™ncia temporal, mesmo com a presen√ßa de diferentes taxas de converg√™ncia para os estimadores, como demonstrado pelas provas detalhadas dos testes acima. As propriedades aqui estabelecidas s√£o cruciais para as discuss√µes posteriores sobre testes de raiz unit√°ria.

### Refer√™ncias
[^1]: The coefficients of regression models involving unit roots or deterministic time trends are typically estimated by ordinary least squares. However, the asymptotic distributions of the coefficient estimates cannot be calculated in the same way as are those for regression models involving stationary variables. Among other difficulties, the estimates of different parameters will in general have different asymptotic rates of convergence.
[^2]: If e, ~ N(0, œÉ¬≤), then the model [16.1.1] satisfies the classical regression assumptions and the standard OLS t or F statistics in equations [8.1.26] and [8.1.32] would have exact small-sample t or F distributions.
[^3]: This chapter introduces this technique, which will prove useful not only for studying time trends but also for analyzing estimators for a variety of nonstationary processes in Chapters 17 and 18.
[^4]: Recall the approach used to find asymptotic distributions for regressions with stationary explanatory variables in Chapter 8.
[^5]: The usual assumption was that (1/T) Œ£‚ÇÅx,x, converged in probability to a nonsingular matrix Q while (1/V/T) Œ£Œ§, œá,Œµ, converged in distribution to a N(0, œÉ¬≤Q) random variable, implying that VT(b+- Œ≤) N(0, 0¬≤Q-1).
[^6]: It turns out that the OLS estimates √¢, and 87 have different asymptotic rates of convergence. To arrive at nondegenerate limiting distributions, √¢, is multiplied by VT, whereas y must be multiplied by T3/2!
[^7]: From [16.1.19] and [16.1.24], the asymptotic distribution of [16.1.18] can be calculated as in Example 7.5 of Chapter 7: [ŒΩœÑŒ¨œÑ - Œ±) , T32(8œÑ - Œ¥)]  N(0, œÉ¬≤Q-1).
[^8]: Thus, although √¢ and 87 converge at different rates, the corresponding standard errors day and √¥s, also incorporate different orders of T, with the result that the usual OLS t tests are asymptotically valid.
[^9]: It is interesting also to consider a test of a single hypothesis involving both a and 8, Hra + r28 = r, where r1, 12, and r are parameters that describe the hypothesis.
[^10]: At test of H, can be obtained from the square root of the OLS F test (expression [8.1.32]).
[^11]:  A test involving a single restriction across parameters with different rates of convergence is dominated asymptotically by the parameters with the slowest rates of convergence.
[^12]: Thus, again, the usual OLS test is asymptotically valid.
[^13]: Consider a sample of T + p observations on y, {y-p+1, y-p+2,...,y}, and let ·æ∂œÑ, Œ¥œÑ, Œ¶1.œÑ, ..... Œ¶œÅ. denote coefficient estimates based on ordinary least squares estimation of [16.3.1] for t = 1, 2, ..., T.
[^14]:  The idea of transforming the regression into a form such as [16.3.3] is due to Sims, Stock, and Watson (1990).
[^15]:  The system of [16.3.7] is just an algebraically equivalent representation of the regression model [16.3.5].
[^16]: This means that some of the elements of R* may be irrelevant asymptotically, so that [16.3.22] has the same asymptotic distribution as a simpler expression.
[^17]: It turns out that the OLS estimates √¢, and 87 have different asymptotic rates of convergence. To arrive at nondegenerate limiting distributions, √¢, is multiplied by VT, whereas 8 must be multiplied by T3/2! We can think of this adjustment as premultiplying [16.1.6] or [16.1.8] by the matrix Y = VT 0; 0  T3/2.
<!--The result is that $\hat{\alpha}$ and $\hat{\beta}$ are scaled up by $\sqrt{T}$ and $T^{3/2}$, respectively. This is a familiar feature of superconsistent estimation.

### 16.2.2 Inference

Under the null hypothesis that $\alpha = \alpha_0$ and $\beta = \beta_0$, the limiting distribution of the estimated parameters is given by

$$
\sqrt{T}(\hat{\alpha} - \alpha_0) \rightarrow N(0, \sigma^2V_1)
$$

$$
T^{3/2}(\hat{\beta} - \beta_0) \rightarrow N(0, \sigma^2V_2)
$$

where $V_1$ and $V_2$ are derived from the matrix $V$. This is a rather complicated expression, and we can usually rely on the software to calculate these values for us, but it is important to understand the concept. We can construct a Wald statistic to test the null hypothesis. The Wald test statistic is given by

$$
W = T (\hat{\alpha} - \alpha_0)' \hat{\Sigma}_{\alpha}^{-1} (\hat{\alpha} - \alpha_0) + T^3 (\hat{\beta} - \beta_0)' \hat{\Sigma}_{\beta}^{-1} (\hat{\beta} - \beta_0)
$$

where $\hat{\Sigma}_{\alpha}$ and $\hat{\Sigma}_{\beta}$ are consistent estimates of the variances of the limiting distributions of $\hat{\alpha}$ and $\hat{\beta}$, respectively. Under the null hypothesis, $W$ converges to a chi-squared distribution with degrees of freedom equal to the number of restrictions imposed by the null hypothesis. In the case where there is only one variable in each of the $\alpha$ and $\beta$ components, then the Wald test is equivalent to the usual t-test.

### 16.2.3 Cointegration and Error Correction Models

Cointegration is closely related to error correction models. If two or more time series are cointegrated, it means that there exists a long-run equilibrium relationship between them. This long-run relationship can be modeled using an error correction model (ECM).

An ECM incorporates both the short-run dynamics and the long-run equilibrium relationship between the cointegrated variables. It takes the form of a vector autoregression (VAR) with the addition of an error correction term. The error correction term is the lagged value of the cointegrating relationship.

Consider two time series, $y_t$ and $x_t$, that are cointegrated with a cointegrating vector $(1, -\beta)$. The error correction term would be $e_{t-1} = y_{t-1} - \beta x_{t-1}$. An ECM for $y_t$ might look like:

$$
\Delta y_t = \alpha_0 + \alpha_1 \Delta y_{t-1} + \alpha_2 \Delta x_{t-1} + \gamma e_{t-1} + \epsilon_t
$$

where $\Delta$ denotes the first difference operator, and $\gamma$ is the error correction coefficient. The term $\gamma e_{t-1}$ is the error correction term. If $\gamma$ is negative, this means that any deviation from the equilibrium relationship in the previous period will be corrected in the current period.

## 16.3 Conclusion

Cointegration provides a powerful way to analyze non-stationary time series data. It allows us to identify long-run equilibrium relationships between variables and to build models that capture both short-run dynamics and long-run dependencies. It is essential for economists to understand these ideas as it has become quite standard to examine cointegration relationships in empirical analysis, and they play an important role in understanding financial data.
<!-- END -->
