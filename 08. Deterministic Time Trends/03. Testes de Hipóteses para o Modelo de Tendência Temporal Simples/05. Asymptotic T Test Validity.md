## Testes t de OLS e a Validade Assintótica: Uma Análise Detalhada

### Introdução
Este capítulo se aprofunda na análise da validade assintótica do teste t de Mínimos Quadrados Ordinários (OLS) em modelos com tendências temporais determinísticas, especificamente no contexto do modelo de tendência temporal simples [^1]. Em particular, focaremos em como as estatísticas de teste t se comportam quando testamos hipóteses nulas sobre o intercepto ($\alpha$) e o coeficiente de tendência ($\delta$), e como essas estatísticas, embora envolvam estimadores com diferentes taxas de convergência, convergem para uma distribuição normal padrão $N(0,1)$. Em outras palavras, este capítulo detalha os mecanismos pelos quais o reescalonamento adequado da estatística t e a incorporação das diferentes ordens de $T$ nos erros padrão garantem inferências assintoticamente válidas.

### Conceitos Fundamentais
Como estabelecido anteriormente, o modelo de tendência temporal simples é definido como:

$$y_t = \alpha + \delta t + \epsilon_t$$

onde $\epsilon_t$ representa um processo de ruído branco com média zero e variância constante $\sigma^2$ [^2]. Os estimadores de mínimos quadrados ordinários (OLS) para $\alpha$ e $\delta$, denotados como $\hat{\alpha}_T$ e $\hat{\delta}_T$, respectivamente, convergem para seus valores verdadeiros com taxas distintas. Especificamente, $\hat{\alpha}_T$ converge a uma taxa de $\sqrt{T}$, enquanto $\hat{\delta}_T$ converge a uma taxa de $T^{3/2}$ [^1, 6].

> 💡 **Exemplo Numérico:**
>
> Vamos simular um conjunto de dados para ilustrar essas taxas de convergência. Considere um modelo com $\alpha = 2$, $\delta = 0.5$, $\sigma^2 = 1$ e $T = 100$. Geraremos dados usando este modelo e ajustaremos um modelo OLS para estimar $\hat{\alpha}_T$ e $\hat{\delta}_T$.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
>
> # Parâmetros do modelo
> alpha_true = 2
> delta_true = 0.5
> sigma_sq = 1
> T = 100
>
> # Geração de dados
> np.random.seed(42)
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
> y = alpha_true + delta_true * t + epsilon
>
> # Ajuste do modelo OLS
> X = sm.add_constant(t)
> model = sm.OLS(y, X)
> results = model.fit()
>
> alpha_hat = results.params[0]
> delta_hat = results.params[1]
>
> print(f"Estimativa de alpha (alpha_hat): {alpha_hat:.4f}")
> print(f"Estimativa de delta (delta_hat): {delta_hat:.4f}")
>
> # Gráfico dos dados e da reta ajustada
> plt.scatter(t, y, label='Dados Simulados')
> plt.plot(t, results.fittedvalues, color='red', label='Reta Ajustada')
> plt.xlabel('t')
> plt.ylabel('y')
> plt.title('Dados Simulados e Reta de Regressão')
> plt.legend()
> plt.show()
>
> # Cálculo dos erros padrão para alpha e delta
> se_alpha = results.bse[0]
> se_delta = results.bse[1]
>
> print(f"Erro padrão de alpha (se_alpha): {se_alpha:.4f}")
> print(f"Erro padrão de delta (se_delta): {se_delta:.4f}")
> ```
> Este exemplo demonstra como os estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$ são calculados. À medida que $T$ aumenta, $\hat{\alpha}_T$ e $\hat{\delta}_T$ se aproximam de seus valores reais e os erros padrão (se_alpha, se_delta) diminuem. O código também plota os dados simulados e a linha de regressão ajustada, o que pode ajudar a visualizar o modelo.

**Distribuições Assintóticas e o Papel do Reescalonamento**

A distribuição assintótica conjunta dos estimadores, após o reescalonamento apropriado, é dada por:

$$
\begin{bmatrix}
\sqrt{T}(\hat{\alpha}_T - \alpha) \\
T^{3/2}(\hat{\delta}_T - \delta)
\end{bmatrix}
\xrightarrow{d} N\left(0, \sigma^2 Q^{-1}\right)
$$

onde $Q$ é a matriz de covariância dos regressores, definida como:

$$ Q = \begin{bmatrix} 1 & \frac{1}{T}\sum_{t=1}^T t \\ \frac{1}{T}\sum_{t=1}^T t & \frac{1}{T^2}\sum_{t=1}^T t^2 \end{bmatrix} $$

Essa matriz captura a estrutura dos regressores no modelo [^5]. A matriz inversa $Q^{-1}$ é fundamental para determinar as variâncias assintóticas dos estimadores, que são usadas para construir os testes de hipóteses.

> 💡 **Exemplo Numérico:**
>
> Para ilustrar a matriz $Q$, vamos considerar o caso em que $T=100$.
>
> ```python
> T = 100
> t = np.arange(1, T + 1)
> Q = np.array([[1, np.mean(t)], [np.mean(t), np.mean(t**2)]])
> print("Matriz Q para T=100:\n", Q)
> ```
>
> Este exemplo calcula a matriz $Q$ para $T=100$. Note que os termos da matriz $Q$ convergem para valores específicos à medida que $T$ aumenta, conforme demonstrado no Lema 1. Os valores calculados da matriz Q serão cruciais para calcular as variâncias assintóticas dos estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$.

O conceito fundamental para entender a validade assintótica dos testes t reside no comportamento compensatório entre os estimadores e seus erros padrão [^8]. Apesar das diferentes taxas de convergência, os erros padrão dos estimadores, $\hat{\sigma}_{\hat{\alpha}_T}$ e $\hat{\sigma}_{\hat{\delta}_T}$, também incorporam diferentes ordens de $T$, garantindo que as estatísticas de teste t, quando devidamente reescalonadas, convirjam para uma distribuição normal padrão $N(0,1)$.

**Lema 1**
A matriz $Q$, definida anteriormente, converge para uma matriz não singular à medida que $T$ tende ao infinito.
*Prova:*
I. O termo $(1/T) \sum_{t=1}^T t$ pode ser reescrito como $(1/T) * T(T+1)/2 = (T+1)/2$ que converge para $T/2$ quando $T\rightarrow \infty$.
II. O termo $(1/T^2) \sum_{t=1}^T t^2$ pode ser reescrito como $(1/T^2) * T(T+1)(2T+1)/6 = (1+1/T)(2+1/T)T/6$ que converge para $T/3$ quando $T\rightarrow \infty$.
III.  Assim, a matriz Q converge para:
$$ \lim_{T\to\infty} Q = \begin{bmatrix} 1 & T/2 \\ T/2 & T^2/3 \end{bmatrix} $$
IV. Essa matriz é não singular para $T > 0$, dado que seu determinante, $T^2/3 - T^2/4 = T^2/12 \neq 0$  para $T \neq 0$. $\blacksquare$

**Lema 1.1**
A matriz inversa $Q^{-1}$ também converge para uma matriz não singular quando $T\rightarrow \infty$.
*Prova:*
I. Como demonstrado no Lema 1, a matriz $Q$ converge para uma matriz não singular quando $T\rightarrow \infty$.
II.  Portanto, sua inversa, $Q^{-1}$, também converge para uma matriz não singular para $T\rightarrow \infty$. $\blacksquare$

**Lema 1.2**
A matriz $Q$ converge para uma matriz que, quando normalizada por $T$, tem um limite não singular. Formalmente, a matriz $\frac{1}{T}Q$ converge para uma matriz não singular quando $T\rightarrow \infty$.
*Prova:*
I. Do Lema 1, sabemos que $\lim_{T\to\infty} Q = \begin{bmatrix} 1 & T/2 \\ T/2 & T^2/3 \end{bmatrix}$.
II. Multiplicando $Q$ por $1/T$, obtemos
$$\frac{1}{T}Q = \begin{bmatrix} 1/T & 1/2 \\ 1/2 & T/3 \end{bmatrix}$$
III.  Para analisar o comportamento desta matriz quando $T \to \infty$, considere a matriz $Q^* = \frac{1}{T}Q$
$$ \lim_{T\to\infty} \frac{1}{T} Q = \lim_{T\to\infty} Q^* = \begin{bmatrix} 0 & 1/2 \\ 1/2 & T/3 \end{bmatrix} $$
IV. Para  normalizar  a matriz $\frac{1}{T}Q$ por $T$ temos:
$$ \frac{1}{T}\begin{bmatrix} 1 & T/2 \\ T/2 & T^2/3 \end{bmatrix} = \begin{bmatrix} 1/T & 1/2 \\ 1/2 & T/3 \end{bmatrix} $$
V. Para obter uma matriz limite não-singular, multiplicamos a matriz $Q^*$ por $1/T$:
$$ \frac{1}{T}Q^* = \begin{bmatrix} \frac{1}{T^2} & \frac{1}{2T} \\ \frac{1}{2T} & \frac{1}{3} \end{bmatrix} $$
VI. Assim,
$$ \lim_{T\to\infty} \frac{1}{T}Q^* = \begin{bmatrix} 0 & 0 \\ 0 & 1/3 \end{bmatrix} $$
Esta matriz ainda é singular. Para obtermos uma matriz não singular podemos usar a matriz de reescalonamento.
VII. No entanto, podemos reescalonar a matriz original da seguinte forma:
 $$ \lim_{T \to \infty} \begin{bmatrix} 1/T & 0 \\ 0 & 1/T^2 \end{bmatrix}  \begin{bmatrix} 1 & T/2 \\ T/2 & T^2/3 \end{bmatrix} =  \lim_{T \to \infty} \begin{bmatrix} 1/T & 1/2 \\ 1/2 & 1/3 \end{bmatrix} =  \begin{bmatrix} 0 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} $$
Esta matriz tem determinante diferente de zero. $\blacksquare$

### Detalhamento da Validade Assintótica do Teste t de OLS

**Teste t para Hipótese sobre o Intercepto ($\alpha$)**

Considere o teste da hipótese nula $H_0: \alpha = \alpha_0$. A estatística t correspondente é dada por:

$$ t_{\alpha} = \frac{\hat{\alpha}_T - \alpha_0}{\hat{\sigma}_{\hat{\alpha}_T}} $$

onde $\hat{\sigma}_{\hat{\alpha}_T}$ é o erro padrão estimado de $\hat{\alpha}_T$. Para demonstrar a validade assintótica do teste, reescalonamos o numerador e o denominador por $\sqrt{T}$:

$$ t_{\alpha} = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{\hat{\sigma}_{\hat{\alpha}_T}\sqrt{T}} $$

O numerador, $\sqrt{T}(\hat{\alpha}_T - \alpha_0)$, converge em distribuição para uma variável normal com média zero e variância finita. O denominador, $\hat{\sigma}_{\hat{\alpha}_T}\sqrt{T}$, converge em probabilidade para um valor não nulo. Assim, a estatística de teste $t_{\alpha}$ converge em distribuição para uma normal padrão $N(0, 1)$ [^8].

*Prova Detalhada:*

I. O numerador, $\sqrt{T}(\hat{\alpha}_T - \alpha_0)$, pode ser expresso como:
$$ \sqrt{T}(\hat{\alpha}_T - \alpha_0) = \left[\sqrt{T},0\right]\begin{bmatrix}\hat{\alpha}_T - \alpha_0 \\\hat{\delta}_T - \delta_0 \end{bmatrix} $$
II. Utilizando o resultado da distribuição assintótica conjunta dos estimadores, temos:
$$
\begin{bmatrix}
\sqrt{T}(\hat{\alpha}_T - \alpha) \\
T^{3/2}(\hat{\delta}_T - \delta)
\end{bmatrix}
\xrightarrow{d} N\left(0, \sigma^2 Q^{-1}\right)
$$
III. Portanto, $\sqrt{T}(\hat{\alpha}_T - \alpha)$ converge para uma distribuição normal com média zero e variância dada pelo elemento (1,1) da matriz $\sigma^2 Q^{-1}$.
IV. O denominador é o erro padrão de $\hat{\alpha}_T$ multiplicado por $\sqrt{T}$. Pela consistência dos estimadores e de suas variâncias, $\hat{\sigma}_{\hat{\alpha}_T}\sqrt{T}$ converge em probabilidade para um valor não nulo.
V. Assim, pela aplicação do teorema de Slutsky, a estatística $t_{\alpha}$ converge em distribuição para uma normal padrão $N(0, 1)$. $\blacksquare$

> 💡 **Exemplo Numérico:**
>
> Vamos testar a hipótese nula $H_0: \alpha = 2$ usando os dados simulados anteriormente e calcular a estatística $t_{\alpha}$.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> from scipy.stats import t
>
> # Parâmetros do modelo
> alpha_true = 2
> delta_true = 0.5
> sigma_sq = 1
> T = 100
>
> # Geração de dados
> np.random.seed(42)
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
> y = alpha_true + delta_true * t + epsilon
>
> # Ajuste do modelo OLS
> X = sm.add_constant(t)
> model = sm.OLS(y, X)
> results = model.fit()
>
> alpha_hat = results.params[0]
> se_alpha = results.bse[0]
>
> # Valor nulo para o teste
> alpha_0 = 2
>
> # Cálculo da estatística t
> t_alpha = (alpha_hat - alpha_0) / se_alpha
>
> # Grau de liberdade
> df = T - 2
>
> # Cálculo do p-valor
> p_valor_alpha = 2 * (1 - t.cdf(abs(t_alpha), df))
>
> print(f"Estatística t para alpha: {t_alpha:.4f}")
> print(f"P-valor para alpha: {p_valor_alpha:.4f}")
>
> # Compara o p-valor com o nível de significância
> nivel_significancia = 0.05
> if p_valor_alpha < nivel_significancia:
>    print("Rejeita a hipótese nula H0: alpha = 2")
> else:
>    print("Não rejeita a hipótese nula H0: alpha = 2")
> ```
>
> Este exemplo calcula a estatística $t_{\alpha}$ e seu p-valor associado. O resultado indica se temos evidências suficientes para rejeitar a hipótese nula de que $\alpha = 2$. Com um p-valor baixo, rejeitamos a hipótese nula. Se o p-valor for alto (acima do nível de significância), não podemos rejeitar a hipótese nula.

**Teste t para Hipótese sobre a Inclinação ($\delta$)**
De maneira análoga, para testar a hipótese nula $H_0: \delta = \delta_0$, a estatística t é definida como:
$$ t_{\delta} = \frac{\hat{\delta}_T - \delta_0}{\hat{\sigma}_{\hat{\delta}_T}} $$

Reescalonando o numerador e o denominador por $T^{3/2}$:

$$ t_{\delta} = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\hat{\sigma}_{\hat{\delta}_T}T^{3/2}} $$

Aqui, $T^{3/2}(\hat{\delta}_T - \delta_0)$ converge em distribuição para uma normal com média zero e variância finita. O denominador, $\hat{\sigma}_{\hat{\delta}_T}T^{3/2}$, converge em probabilidade para um valor não nulo. Portanto, $t_{\delta}$ converge em distribuição para uma normal padrão $N(0, 1)$ [^9].

*Prova Detalhada:*

I. O numerador, $T^{3/2}(\hat{\delta}_T - \delta_0)$, pode ser expresso como:
$$ T^{3/2}(\hat{\delta}_T - \delta_0) = \left[0,T^{3/2}\right]\begin{bmatrix}\hat{\alpha}_T - \alpha_0 \\\hat{\delta}_T - \delta_0 \end{bmatrix} $$
II. Utilizando o resultado da distribuição assintótica conjunta dos estimadores, temos:
$$
\begin{bmatrix}
\sqrt{T}(\hat{\alpha}_T - \alpha) \\
T^{3/2}(\hat{\delta}_T - \delta)
\end{bmatrix}
\xrightarrow{d} N\left(0, \sigma^2 Q^{-1}\right)
$$
III. Portanto, $T^{3/2}(\hat{\delta}_T - \delta)$ converge para uma distribuição normal com média zero e variância dada pelo elemento (2,2) da matriz $\sigma^2 Q^{-1}$.
IV. O denominador é o erro padrão de $\hat{\delta}_T$ multiplicado por $T^{3/2}$. Pela consistência dos estimadores e de suas variâncias, $\hat{\sigma}_{\hat{\delta}_T}T^{3/2}$ converge em probabilidade para um valor não nulo.
V.  Assim, pela aplicação do teorema de Slutsky, a estatística $t_{\delta}$ converge em distribuição para uma normal padrão $N(0, 1)$. $\blacksquare$

> 💡 **Exemplo Numérico:**
>
> Agora, testaremos a hipótese nula $H_0: \delta = 0.5$ e calcularemos a estatística $t_{\delta}$ usando o mesmo conjunto de dados.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> from scipy.stats import t
>
> # Parâmetros do modelo
> alpha_true = 2
> delta_true = 0.5
> sigma_sq = 1
> T = 100
>
> # Geração de dados
> np.random.seed(42)
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
> y = alpha_true + delta_true * t + epsilon
>
> # Ajuste do modelo OLS
> X = sm.add_constant(t)
> model = sm.OLS(y, X)
> results = model.fit()
>
> delta_hat = results.params[1]
> se_delta = results.bse[1]
>
> # Valor nulo para o teste
> delta_0 = 0.5
>
> # Cálculo da estatística t
> t_delta = (delta_hat - delta_0) / se_delta
>
> # Grau de liberdade
> df = T - 2
>
> # Cálculo do p-valor
> p_valor_delta = 2 * (1 - t.cdf(abs(t_delta), df))
>
> print(f"Estatística t para delta: {t_delta:.4f}")
> print(f"P-valor para delta: {p_valor_delta:.4f}")
>
> # Compara o p-valor com o nível de significância
> nivel_significancia = 0.05
> if p_valor_delta < nivel_significancia:
>    print("Rejeita a hipótese nula H0: delta = 0.5")
> else:
>    print("Não rejeita a hipótese nula H0: delta = 0.5")
> ```
>
> Este exemplo calcula a estatística $t_{\delta}$ e seu p-valor associado. O p-valor será crucial para determinar se temos evidência estatística suficiente para rejeitar a hipótese nula de que $\delta = 0.5$.

**Teste t para Hipóteses Lineares**
Quando testamos uma hipótese linear envolvendo tanto $\alpha$ quanto $\delta$, como $H_0: r_1\alpha + r_2\delta = r$, a estatística t é dada por:

$$ t_r = \frac{r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r}{\sqrt{\hat{\sigma}^2 [r_1 \, r_2](X^TX)^{-1} [r_1 \, r_2]^T}} $$

Para demonstrar a validade assintótica desse teste, multiplicamos o numerador e o denominador pela raiz da taxa de convergência mais lenta, ou seja, $\sqrt{T}$:

$$ t_r = \frac{\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)}{\sqrt{\hat{\sigma}^2 \sqrt{T}^2 [r_1 \, r_2](X^TX)^{-1} [r_1 \, r_2]^T}} $$
O numerador converge para uma distribuição normal devido ao comportamento assintótico dos estimadores. O denominador converge em probabilidade para uma constante não nula, garantindo a convergência da estatística t para uma distribuição normal padrão $N(0, 1)$. A estatística t para testar hipóteses lineares envolvendo parâmetros com diferentes taxas de convergência é dominada assintoticamente pelos parâmetros com as taxas de convergência mais lentas [^11].

*Prova Detalhada:*
I. A estatística de teste pode ser reescrita como:
$$ t_r = \frac{r_1(\hat{\alpha}_T - \alpha) + r_2(\hat{\delta}_T - \delta)}{\hat{\sigma} \sqrt{[r_1,r_2](X^TX)^{-1}[r_1,r_2]^T}} $$
II. Multiplicando o numerador e denominador por $\sqrt{T}$, temos:
$$ t_r = \frac{\sqrt{T}(r_1(\hat{\alpha}_T - \alpha) + r_2(\hat{\delta}_T - \delta))}{\hat{\sigma} \sqrt{T [r_1,r_2](X^TX)^{-1}[r_1,r_2]^T}} $$
III. O termo $\sqrt{T}(r_1(\hat{\alpha}_T - \alpha) + r_2(\hat{\delta}_T - \delta))$ converge em distribuição para uma normal com média zero e variância dada pela combinação linear da matriz assintótica de covariância dos estimadores.
IV. O termo do denominador converge em probabilidade para uma constante não nula.
V. Portanto, pela aplicação do teorema de Slutsky, a estatística $t_r$ converge em distribuição para uma normal padrão $N(0, 1)$. $\blacksquare$

> 💡 **Exemplo Numérico:**
>
> Vamos testar a hipótese linear $H_0: \alpha + 2\delta = 3$.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> from scipy.stats import t
>
> # Parâmetros do modelo
> alpha_true = 2
> delta_true = 0.5
> sigma_sq = 1
> T = 100
>
> # Geração de dados
> np.random.seed(42)
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
> y = alpha_true + delta_true * t + epsilon
>
> # Ajuste do modelo OLS
> X = sm.add_constant(t)
> model = sm.OLS(y, X)
> results = model.fit()
>
> alpha_hat = results.params[0]
> delta_hat = results.params[1]
> sigma_hat_sq = results.mse_resid
>
> # Hipótese linear
> r1 = 1
> r2 = 2
> r = 3
>
> # Cálculo da estatística t
> X_mat = np.column_stack((np.ones(T),t))
> cov_matrix = sigma_hat_sq * np.linalg.inv(X_mat.T @ X_mat)
> se_linear = np.sqrt(np.array([r1, r2]) @ cov_matrix @ np.array([r1, r2]).T)
>
> t_r = (r1 * alpha_hat + r2 * delta_hat - r) / se_linear
>
> # Grau de liberdade
> df = T - 2
>
> # Cálculo do p-valor
> p_valor_r = 2 * (1 - t.cdf(abs(t_r), df))
>
> print(f"Estatística t para a hipótese linear: {t_r:.4f}")
> print(f"P-valor para a hipótese linear: {p_valor_r:.4f}")
>
> # Compara o p-valor com o nível de significância
> nivel_significancia = 0.05
> if p_valor_r < nivel_significancia:
>    print("Rejeita a hipótese nula H0: alpha + 2*delta = 3")
> else:
>    print("Não rejeita a hipótese nula H0: alpha + 2*delta = 3")
> ```
>
> Este exemplo calcula a estatística $t_r$ e o p-valor correspondente para a hipótese linear especificada. A interpretação é semelhante aos testes t para $\alpha$ e $\delta$: se o p-valor for inferior ao nível de significância, rejeitamos a hipótese nula.

**Observação 1**
É importante notar que, embora a estatística t para hipóteses lineares seja dominada assintoticamente pelos parâmetros com as taxas de convergência mais lentas, a inclusão de parâmetros com taxas de convergência mais rápidas não invalida o resultado. A convergência em distribuição da estatística t para uma normal padrão $N(0,1)$ se mantém, garantindo a validade assintótica do teste.

**Teorema 2**
Sob as condições estabelecidas, a estatística t para qualquer hipótese linear da forma $H_0: R\beta = r$, onde $\beta = [\alpha, \delta]^T$, $R$ é uma matriz de restrições e $r$ é um vetor de constantes, converge para uma distribuição normal padrão $N(0,1)$, desde que a hipótese seja testada utilizando os erros padrão adequadamente reescalonados.

*Prova:*
I. A estatística t para a hipótese linear $H_0: R\beta = r$ é dada por:
$$ t = \frac{R\hat{\beta} - r}{\sqrt{\hat{\sigma}^2 R(X^TX)^{-1}R^T}} $$
II. Reescalonando o numerador e o denominador pela taxa de convergência apropriada, que é determinada pela taxa de convergência mais lenta dos estimadores, neste caso $\sqrt{T}$, temos:
$$ t = \frac{\sqrt{T}(R\hat{\beta} - r)}{\sqrt{\hat{\sigma}^2 \sqrt{T}^2 R(X^TX)^{-1}R^T}} $$
III. O numerador, $\sqrt{T}(R\hat{\beta} - r)$, converge em distribuição para uma variável normal com média zero. Isso segue da distribuição assintótica conjunta dos estimadores, onde $\sqrt{T}(\hat{\alpha}_T-\alpha)$ e $T^{3/2}(\hat{\delta}_T-\delta)$ convergem para uma distribuição normal. A combinação linear desses termos também convergirá para uma distribuição normal.
IV. O denominador, $\sqrt{\hat{\sigma}^2 \sqrt{T}^2 R(X^TX)^{-1}R^T}$, converge em probabilidade para uma constante não nula, dado que $\hat{\sigma}^2$ converge para $\sigma^2$ e a matriz $R(X^TX)^{-1}R^T$ devidamente reescalonada converge para uma matriz não singular.
V. Assim, pela aplicação do teorema de Slutsky, a estatística $t$ converge em distribuição para uma normal padrão $N(0, 1)$. $\blacksquare$

**O Papel das Matrizes de Reescalonamento**
A utilização de matrizes de reescalonamento como $\Upsilon_T$ (definidas como [^17]):

$$
\Upsilon_T =
\begin{bmatrix}
\sqrt{T} & 0 \\
0 & T^{3/2}
\end{bmatrix}
$$

permite que os estimadores reescalonados convirjam para distribuições limitantes adequadas. Isso garante que, embora os estimadores de $\alpha$ e $\delta$ tenham taxas de convergência diferentes, os testes t, quando construídos com os erros padrão apropriadamente reescalonados, resultem em inferências assintoticamente válidas.

### Conclusão
A validade assintótica dos testes t de OLS para hipóteses sobre os parâmetros do modelo de tendência temporal simples é mantida através da combinação de estimadores consistentes, erros padrão que capturam as diferentes taxas de convergência e o reescalonamento apropriado da estatística de teste [^8, 9]. As estatísticas de teste t, quer envolvam testes sobre o intercepto, a inclinação ou combinações lineares desses parâmetros, convergem para uma distribuição normal padrão $N(0,1)$. A análise detalhada apresentada aqui confirma que a inferência estatística usando os testes t de OLS é válida em modelos de tendência temporal, mesmo com a presença de diferentes taxas de convergência para os estimadores, como demonstrado pelas provas detalhadas dos testes acima. As propriedades aqui estabelecidas são cruciais para as discussões posteriores sobre testes de raiz unitária.

### Referências
[^1]: The coefficients of regression models involving unit roots or deterministic time trends are typically estimated by ordinary least squares. However, the asymptotic distributions of the coefficient estimates cannot be calculated in the same way as are those for regression models involving stationary variables. Among other difficulties, the estimates of different parameters will in general have different asymptotic rates of convergence.
[^2]: If e, ~ N(0, σ²), then the model [16.1.1] satisfies the classical regression assumptions and the standard OLS t or F statistics in equations [8.1.26] and [8.1.32] would have exact small-sample t or F distributions.
[^3]: This chapter introduces this technique, which will prove useful not only for studying time trends but also for analyzing estimators for a variety of nonstationary processes in Chapters 17 and 18.
[^4]: Recall the approach used to find asymptotic distributions for regressions with stationary explanatory variables in Chapter 8.
[^5]: The usual assumption was that (1/T) Σ₁x,x, converged in probability to a nonsingular matrix Q while (1/V/T) ΣΤ, χ,ε, converged in distribution to a N(0, σ²Q) random variable, implying that VT(b+- β) N(0, 0²Q-1).
[^6]: It turns out that the OLS estimates â, and 87 have different asymptotic rates of convergence. To arrive at nondegenerate limiting distributions, â, is multiplied by VT, whereas y must be multiplied by T3/2!
[^7]: From [16.1.19] and [16.1.24], the asymptotic distribution of [16.1.18] can be calculated as in Example 7.5 of Chapter 7: [ντάτ - α) , T32(8τ - δ)]  N(0, σ²Q-1).
[^8]: Thus, although â and 87 converge at different rates, the corresponding standard errors day and ôs, also incorporate different orders of T, with the result that the usual OLS t tests are asymptotically valid.
[^9]: It is interesting also to consider a test of a single hypothesis involving both a and 8, Hra + r28 = r, where r1, 12, and r are parameters that describe the hypothesis.
[^10]: At test of H, can be obtained from the square root of the OLS F test (expression [8.1.32]).
[^11]:  A test involving a single restriction across parameters with different rates of convergence is dominated asymptotically by the parameters with the slowest rates of convergence.
[^12]: Thus, again, the usual OLS test is asymptotically valid.
[^13]: Consider a sample of T + p observations on y, {y-p+1, y-p+2,...,y}, and let ᾶτ, δτ, Φ1.τ, ..... Φρ. denote coefficient estimates based on ordinary least squares estimation of [16.3.1] for t = 1, 2, ..., T.
[^14]:  The idea of transforming the regression into a form such as [16.3.3] is due to Sims, Stock, and Watson (1990).
[^15]:  The system of [16.3.7] is just an algebraically equivalent representation of the regression model [16.3.5].
[^16]: This means that some of the elements of R* may be irrelevant asymptotically, so that [16.3.22] has the same asymptotic distribution as a simpler expression.
[^17]: It turns out that the OLS estimates â, and 87 have different asymptotic rates of convergence. To arrive at nondegenerate limiting distributions, â, is multiplied by VT, whereas 8 must be multiplied by T3/2! We can think of this adjustment as premultiplying [16.1.6] or [16.1.8] by the matrix Y = VT 0; 0  T3/2.
<!--The result is that $\hat{\alpha}$ and $\hat{\beta}$ are scaled up by $\sqrt{T}$ and $T^{3/2}$, respectively. This is a familiar feature of superconsistent estimation.

### 16.2.2 Inference

Under the null hypothesis that $\alpha = \alpha_0$ and $\beta = \beta_0$, the limiting distribution of the estimated parameters is given by

$$
\sqrt{T}(\hat{\alpha} - \alpha_0) \rightarrow N(0, \sigma^2V_1)
$$

$$
T^{3/2}(\hat{\beta} - \beta_0) \rightarrow N(0, \sigma^2V_2)
$$

where $V_1$ and $V_2$ are derived from the matrix $V$. This is a rather complicated expression, and we can usually rely on the software to calculate these values for us, but it is important to understand the concept. We can construct a Wald statistic to test the null hypothesis. The Wald test statistic is given by

$$
W = T (\hat{\alpha} - \alpha_0)' \hat{\Sigma}_{\alpha}^{-1} (\hat{\alpha} - \alpha_0) + T^3 (\hat{\beta} - \beta_0)' \hat{\Sigma}_{\beta}^{-1} (\hat{\beta} - \beta_0)
$$

where $\hat{\Sigma}_{\alpha}$ and $\hat{\Sigma}_{\beta}$ are consistent estimates of the variances of the limiting distributions of $\hat{\alpha}$ and $\hat{\beta}$, respectively. Under the null hypothesis, $W$ converges to a chi-squared distribution with degrees of freedom equal to the number of restrictions imposed by the null hypothesis. In the case where there is only one variable in each of the $\alpha$ and $\beta$ components, then the Wald test is equivalent to the usual t-test.

### 16.2.3 Cointegration and Error Correction Models

Cointegration is closely related to error correction models. If two or more time series are cointegrated, it means that there exists a long-run equilibrium relationship between them. This long-run relationship can be modeled using an error correction model (ECM).

An ECM incorporates both the short-run dynamics and the long-run equilibrium relationship between the cointegrated variables. It takes the form of a vector autoregression (VAR) with the addition of an error correction term. The error correction term is the lagged value of the cointegrating relationship.

Consider two time series, $y_t$ and $x_t$, that are cointegrated with a cointegrating vector $(1, -\beta)$. The error correction term would be $e_{t-1} = y_{t-1} - \beta x_{t-1}$. An ECM for $y_t$ might look like:

$$
\Delta y_t = \alpha_0 + \alpha_1 \Delta y_{t-1} + \alpha_2 \Delta x_{t-1} + \gamma e_{t-1} + \epsilon_t
$$

where $\Delta$ denotes the first difference operator, and $\gamma$ is the error correction coefficient. The term $\gamma e_{t-1}$ is the error correction term. If $\gamma$ is negative, this means that any deviation from the equilibrium relationship in the previous period will be corrected in the current period.

## 16.3 Conclusion

Cointegration provides a powerful way to analyze non-stationary time series data. It allows us to identify long-run equilibrium relationships between variables and to build models that capture both short-run dynamics and long-run dependencies. It is essential for economists to understand these ideas as it has become quite standard to examine cointegration relationships in empirical analysis, and they play an important role in understanding financial data.
<!-- END -->
