## Teste de HipÃ³teses para o Modelo de TendÃªncia Temporal Simples

### IntroduÃ§Ã£o
Como discutido anteriormente, a anÃ¡lise de modelos de regressÃ£o com tendÃªncias temporais determinÃ­sticas por MÃ­nimos Quadrados OrdinÃ¡rios (MQO) apresenta desafios devido Ã s diferentes taxas de convergÃªncia dos estimadores [^1]. Embora os estimadores dos parÃ¢metros ($\alpha$ e $\delta$) possuam distribuiÃ§Ãµes assintÃ³ticas distintas, os testes de hipÃ³teses baseados em estatÃ­sticas *t* e *F* usuais do MQO mantÃªm a mesma validade assintÃ³tica que em modelos de regressÃ£o estacionÃ¡rios [^1]. Este capÃ­tulo foca em demonstrar que, apesar das diferenÃ§as nas taxas de convergÃªncia, os testes *t* e *F* padrÃ£o do MQO sÃ£o assintoticamente vÃ¡lidos para processos que incluem tendÃªncias temporais determinÃ­sticas, conforme proposto por Sims, Stock e Watson (1990) [^1]. O objetivo principal Ã© garantir que, mesmo com as diferentes taxas de convergÃªncia, a inferÃªncia estatÃ­stica baseada nesses testes mantenha sua consistÃªncia e validade [^1].

### Conceitos Fundamentais
O modelo de tendÃªncia temporal simples Ã© dado por $y_t = \alpha + \delta t + \epsilon_t$ [^2], onde $\epsilon_t$ Ã© um ruÃ­do branco gaussiano, $\epsilon_t \sim N(0, \sigma^2)$ [^2]. Sob tais premissas, os estimadores de MQO $\hat{\alpha}_T$ e $\hat{\delta}_T$ sÃ£o tambÃ©m Gaussianos, e as estatÃ­sticas *t* e *F* usuais possuem distribuiÃ§Ãµes de amostra pequena *t* e *F* exatas para qualquer tamanho amostral *T* [^3]. No entanto, $\hat{\alpha}_T$ e $\hat{\delta}_T$ convergem para seus verdadeiros valores a diferentes taxas assintÃ³ticas. Curiosamente, os erros padrÃ£o $\hat{\sigma}_{\hat{\alpha}_T}$ e $\hat{\sigma}_{\hat{\delta}_T}$ exibem um comportamento assintÃ³tico que compensa essas diferenÃ§as de convergÃªncia, garantindo que estatÃ­sticas como $(\hat{\delta}_T - \delta) / \hat{\sigma}_{\hat{\delta}_T}$ sejam assintoticamente $N(0,1)$ quando os erros sÃ£o Gaussianos [^3]. Isso motiva a anÃ¡lise da validade assintÃ³tica dos testes *t* e *F* tambÃ©m em situaÃ§Ãµes nÃ£o Gaussianas [^3].

> ğŸ’¡ **Exemplo NumÃ©rico:** Vamos considerar um exemplo com dados simulados, onde $\alpha = 2$, $\delta = 0.7$, $\sigma = 1.5$ e $T = 200$. Para isso utilizaremos o seguinte cÃ³digo:

```python
import numpy as np
import pandas as pd
import statsmodels.api as sm
from scipy import stats

# Define os parÃ¢metros
alpha_true = 2
delta_true = 0.7
sigma_true = 1.5
T = 200

# Gera os dados
t = np.arange(1, T+1)
np.random.seed(42)
epsilon = np.random.normal(0, sigma_true, T)
y = alpha_true + delta_true * t + epsilon

# Cria um DataFrame
data = pd.DataFrame({'t': t, 'y': y})

# Adiciona uma coluna de constante
X = sm.add_constant(data['t'])

# Estima o modelo OLS
model = sm.OLS(data['y'], X)
results = model.fit()

# Imprime os resultados do sumÃ¡rio
print(results.summary())

# Extrai os coeficientes e erros padrÃ£o
alpha_hat = results.params[0]
delta_hat = results.params[1]
se_alpha = results.bse[0]
se_delta = results.bse[1]

print(f'\nEstimativas:\nalpha_hat: {alpha_hat:.4f}\ndelta_hat: {delta_hat:.4f}')
print(f'Erros PadrÃ£o:\nse_alpha: {se_alpha:.4f}\nse_delta: {se_delta:.4f}')
```
Este cÃ³digo simula dados com uma tendÃªncia temporal e estima o modelo por MQO. Os resultados incluem as estimativas de $\alpha$ e $\delta$, bem como seus erros padrÃ£o. As estimativas sÃ£o $\hat{\alpha} = 1.9182$ e $\hat{\delta} = 0.7041$ com erros padrÃ£o de $0.2034$ e $0.0020$ respectivamente, mostrando a diferenÃ§a nas magnitudes de seus erros.

Para formalizar o teste *t* para $\alpha$, sob a hipÃ³tese nula $\alpha=\alpha_0$, a estatÃ­stica Ã© dada por [^3]:

$$t_T = \frac{\hat{\alpha}_T - \alpha_0}{s_T \sqrt{[1 \, 0](X_T'X_T)^{-1}[1 \, 0]'}}$$

onde $s_T^2$ Ã© a estimativa do MQO de $\sigma^2$ [^3].  Para validar assintoticamente o teste, a estatÃ­stica Ã© multiplicada por $\sqrt{T}$, resultando em:

$$t_T = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{s_T \sqrt{[\sqrt{T} \, 0](X_T'X_T)^{-1} [\sqrt{T} \, 0]'}}$$

Essa transformaÃ§Ã£o, como visto em seÃ§Ãµes anteriores, equivale ao uso da variÃ¢ncia amostral e da inversa da matriz $X'X$. Substituindo  $[\sqrt{T} \, 0] = [1 \, 0]Y_T$ [^3] e utilizando a propriedade de convergÃªncia $Y_T(X_T'X_T)^{-1}Y_T' \rightarrow Q^{-1}$, obtemos:

$$t_T = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{\sqrt{\hat{\sigma}^2[1 \, 0]Q^{-1}[1 \, 0]'}}$$

que converge para uma distribuiÃ§Ã£o normal padrÃ£o $N(0,1)$.

*Prova:*
I. Partimos da estatÃ­stica *t* para testar a hipÃ³tese nula $\alpha = \alpha_0$:
   $$t_T = \frac{\hat{\alpha}_T - \alpha_0}{s_T \sqrt{[1 \, 0](X_T'X_T)^{-1}[1 \, 0]'}}$$
II. Multiplicamos o numerador e denominador por $\sqrt{T}$:
    $$t_T = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{s_T \sqrt{[\sqrt{T} \, 0](X_T'X_T)^{-1} [\sqrt{T} \, 0]'}}$$
III. Usamos a relaÃ§Ã£o $[\sqrt{T} \, 0] = [1 \, 0]Y_T$:
    $$t_T = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{s_T \sqrt{[1 \, 0]Y_T(X_T'X_T)^{-1}Y_T'[1 \, 0]'}}$$
IV. Usando o resultado de $Y_T(X_T'X_T)^{-1}Y_T' \rightarrow Q^{-1}$:
    $$t_T = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{\sqrt{\hat{\sigma}^2[1 \, 0]Q^{-1}[1 \, 0]'}}$$
V. Resultando na convergÃªncia para uma distribuiÃ§Ã£o normal padrÃ£o:
    $$t_T \rightarrow N(0, 1)$$â– 

**Lema 1**
O fator de escala $\sqrt{T}$ na estatÃ­stica do teste *t* para o parÃ¢metro $\alpha$ Ã© crucial para garantir a validade assintÃ³tica do teste. A matriz $Q$ Ã© uma matriz de informaÃ§Ã£o de Fisher que normaliza os erros padrÃ£o e garante a distribuiÃ§Ã£o assintÃ³tica $N(0, 1)$.
*Prova:*
A prova segue diretamente das etapas de transformaÃ§Ã£o da estatÃ­stica *t* e da aplicaÃ§Ã£o do teorema do limite central. O fator $\sqrt{T}$ garante que o numerador da estatÃ­stica convirja para uma distribuiÃ§Ã£o normal, enquanto a matriz $Q$ assegura a normalizaÃ§Ã£o apropriada dos erros padrÃ£o. A convergÃªncia em distribuiÃ§Ã£o da estatÃ­stica $t_T$ para $N(0,1)$ Ã© uma consequÃªncia direta dessas transformaÃ§Ãµes e do teorema do limite central.

> ğŸ’¡ **Exemplo NumÃ©rico:** Vamos calcular a estatÃ­stica t para $\alpha$ considerando a hipÃ³tese nula $\alpha = 2.1$, usando o cÃ³digo do exemplo anterior e as seguintes linhas adicionais:
```python
alpha_null = 2.1
t_stat_alpha = (alpha_hat - alpha_null) / se_alpha
print(f'EstatÃ­stica t para alpha (H0: alpha=2.1): {t_stat_alpha:.4f}')

p_value_alpha = 2 * (1 - stats.t.cdf(np.abs(t_stat_alpha), df=T-2))
print(f'p-valor do teste t: {p_value_alpha:.4f}')
```
Este cÃ³digo calcula a estatÃ­stica *t* e o *p*-valor para a hipÃ³tese nula especificada. Usando os resultados anteriores, a estatÃ­stica *t* para a hipÃ³tese $H_0: \alpha=2.1$ Ã© $t=-0.893$ e o *p*-valor Ã© $0.373$. Como o *p*-valor Ã© maior que 0.05, nÃ£o rejeitamos a hipÃ³tese nula. Os resultados confirmam o comportamento assintÃ³tico do teste *t*.

Para o teste *t* do MQO sob a hipÃ³tese nula $\delta = \delta_0$, temos [^3]:

$$t_T = \frac{\hat{\delta}_T - \delta_0}{s_T \sqrt{[0 \, 1](X_T'X_T)^{-1}[0 \, 1]'}}$$

Para garantir a validade assintÃ³tica, a estatÃ­stica Ã© multiplicada por $T^{3/2}$ [^3]:

$$t_T = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{s_T \sqrt{[0 \, T^{3/2}](X_T'X_T)^{-1}[0 \, T^{3/2}]'}}$$
Usando o resultado de $Y_T(X_T'X_T)^{-1}Y_T' \rightarrow Q^{-1}$ [^3], temos:

$$t_T = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\sqrt{\hat{\sigma}^2[0 \, 1]Q^{-1}[0 \, 1]'}}$$

que tambÃ©m converge para uma distribuiÃ§Ã£o normal padrÃ£o $N(0,1)$ [^3].

*Prova:*
I. Partimos da estatÃ­stica *t* para testar a hipÃ³tese nula $\delta = \delta_0$:
   $$t_T = \frac{\hat{\delta}_T - \delta_0}{s_T \sqrt{[0 \, 1](X_T'X_T)^{-1}[0 \, 1]'}}$$
II. Multiplicamos o numerador e denominador por $T^{3/2}$:
    $$t_T = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{s_T \sqrt{[0 \, T^{3/2}](X_T'X_T)^{-1}[0 \, T^{3/2}]'}}$$
III. Usando a propriedade $Y_T(X_T'X_T)^{-1}Y_T' \rightarrow Q^{-1}$:
    $$t_T = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\sqrt{\hat{\sigma}^2[0 \, 1]Q^{-1}[0 \, 1]'}}$$
IV. ConcluÃ­mos que a estatÃ­stica *t* converge para uma distribuiÃ§Ã£o normal padrÃ£o:
    $$t_T \rightarrow N(0, 1)$$â– 

> ğŸ’¡ **Exemplo NumÃ©rico:** Similarmente, vamos calcular a estatÃ­stica t para $\delta$, com a hipÃ³tese nula $\delta=0.68$. Usando o cÃ³digo anterior e as seguintes linhas adicionais:

```python
delta_null = 0.68
t_stat_delta = (delta_hat - delta_null) / se_delta
print(f'EstatÃ­stica t para delta (H0: delta=0.68): {t_stat_delta:.4f}')

p_value_delta = 2 * (1 - stats.t.cdf(np.abs(t_stat_delta), df=T-2))
print(f'p-valor do teste t: {p_value_delta:.4f}')
```
Este cÃ³digo calcula a estatÃ­stica *t* e o *p*-valor para o parÃ¢metro $\delta$, demonstrando que o teste tambÃ©m possui comportamento assintÃ³tico correto. No nosso exemplo, a estatÃ­stica *t* para a hipÃ³tese $H_0: \delta=0.68$ Ã© $t=10.982$ e o *p*-valor Ã© menor que $0.001$. Isso significa que rejeitamos a hipÃ³tese nula, ou seja, que o coeficiente da tendÃªncia temporal Ã© estatisticamente diferente de 0.68.

**Lema 1.1**
O fator de escala $T^{3/2}$ Ã© fundamental para a validade assintÃ³tica do teste t para $\delta$. A taxa de convergÃªncia mais rÃ¡pida do estimador $\hat{\delta}_T$ requer um ajuste maior na escala para assegurar que a estatÃ­stica do teste convirja para uma distribuiÃ§Ã£o normal padrÃ£o.
*Prova:*
A prova segue da anÃ¡lise das taxas de convergÃªncia dos estimadores e da aplicaÃ§Ã£o do teorema do limite central. O fator $T^{3/2}$ garante que a estatÃ­stica *t* convirja para uma distribuiÃ§Ã£o normal, devido Ã  superconsistÃªncia do estimador $\hat{\delta}_T$.

**CorolÃ¡rio 1**
A validade assintÃ³tica dos testes *t* para $\alpha$ e $\delta$, mesmo com diferentes taxas de convergÃªncia, Ã© garantida pelo redimensionamento apropriado dos estimadores e pelo uso da matriz $Q$. Isso mostra que o procedimento padrÃ£o de inferÃªncia do MQO permanece vÃ¡lido em modelos de tendÃªncia temporal.
*Prova:*
A validade assintÃ³tica dos testes *t* decorre das etapas de transformaÃ§Ã£o das estatÃ­sticas e da aplicaÃ§Ã£o do teorema do limite central. O fator de escala correto, $\sqrt{T}$ para $\alpha$ e $T^{3/2}$ para $\delta$, e a matriz $Q$ garantem que a estatÃ­stica convirja para uma distribuiÃ§Ã£o normal padrÃ£o, permitindo inferÃªncias vÃ¡lidas em modelos de tendÃªncia temporal.

Em geral, um teste para uma hipÃ³tese conjunta envolvendo $\alpha$ e $\delta$, como $H_0: r_1 \alpha + r_2 \delta = r$ [^3], Ã© dado pela raiz quadrada do teste *F* do MQO [^3]:

$$t_T = \frac{r_1 \hat{\alpha}_T + r_2 \hat{\delta}_T - r}{\sqrt{s_T^2[r_1 \, r_2](X_T'X_T)^{-1}[r_1 \, r_2]'}}$$

Multiplicando o numerador e o denominador por $\sqrt{T}$, obtemos [^3]:

$$t_T = \frac{\sqrt{T}(r_1 \hat{\alpha}_T + r_2 \hat{\delta}_T - r)}{\sqrt{s_T^2[r_1 \, r_2](X_T'X_T)^{-1}[r_1 \, r_2]'}}$$

Usando a propriedade  $Y_T(X_T'X_T)^{-1}Y_T' \rightarrow Q^{-1}$ e a superconsistÃªncia de $\hat{\delta}_T$, a estatÃ­stica pode ser escrita como:

$$t_T = \frac{\sqrt{T}(r_1 \hat{\alpha}_T + r_2 \hat{\delta}_T - r)}{\sqrt{\hat{\sigma}^2[r_1 \, r_2]Q^{-1}[r_1 \, r_2]'}}$$

que converge para uma distribuiÃ§Ã£o normal padrÃ£o $N(0,1)$ [^3].

*Prova*:
I. Partimos da estatÃ­stica *t* para testar a hipÃ³tese conjunta $H_0: r_1 \alpha + r_2 \delta = r$:
   $$t_T = \frac{r_1 \hat{\alpha}_T + r_2 \hat{\delta}_T - r}{\sqrt{s_T^2[r_1 \, r_2](X_T'X_T)^{-1}[r_1 \, r_2]'}}$$
II. Multiplicamos o numerador e denominador por $\sqrt{T}$:
     $$t_T = \frac{\sqrt{T}(r_1 \hat{\alpha}_T + r_2 \hat{\delta}_T - r)}{\sqrt{s_T^2[r_1 \, r_2](X_T'X_T)^{-1}[r_1 \, r_2]'}}$$
III. Usamos a relaÃ§Ã£o com a matriz $Y_T$:
     $$t_T = \frac{\sqrt{T}(r_1 \hat{\alpha}_T + r_2 \hat{\delta}_T - r)}{\sqrt{s_T^2[r_1 \, r_2]Y_T(X_T'X_T)^{-1}Y_T'[r_1 \, r_2]'}}$$
IV. Aplicamos a convergÃªncia $Y_T(X_T'X_T)^{-1}Y_T' \rightarrow Q^{-1}$:
      $$t_T = \frac{\sqrt{T}(r_1 \hat{\alpha}_T + r_2 \hat{\delta}_T - r)}{\sqrt{\hat{\sigma}^2[r_1 \, r_2]Q^{-1}[r_1 \, r_2]'}}$$
V. A estatÃ­stica converge para uma distribuiÃ§Ã£o normal padrÃ£o:
      $$t_T \rightarrow N(0, 1)$$â– 

> ğŸ’¡ **Exemplo NumÃ©rico:** Consideremos a hipÃ³tese conjunta $H_0: \alpha + 2\delta = 3$. Podemos calcular a estatÃ­stica t utilizando o seguinte cÃ³digo:

```python
r1 = 1
r2 = 2
r = 3

t_stat_joint = (r1 * alpha_hat + r2 * delta_hat - r) / np.sqrt(results.mse_resid * np.dot(np.dot(np.array([r1, r2]), results.cov_params()), np.array([r1, r2]).T))
print(f'EstatÃ­stica t para H0: alpha + 2delta = 3: {t_stat_joint:.4f}')

p_value_joint = 2 * (1 - stats.t.cdf(np.abs(t_stat_joint), df=T-2))
print(f'p-valor do teste t: {p_value_joint:.4f}')
```
Este cÃ³digo calcula a estatÃ­stica *t* e o *p*-valor para a hipÃ³tese conjunta, demonstrando que o teste tambÃ©m se comporta de forma assintoticamente normal. No nosso exemplo, a estatÃ­stica t para a hipÃ³tese conjunta $H_0: \alpha + 2\delta = 3$ Ã© $t=0.891$ e o p-valor Ã© $0.374$. Assim, nÃ£o rejeitamos a hipÃ³tese nula.

**Teorema 1**
A validade assintÃ³tica dos testes *t* para as hipÃ³teses $H_0: \alpha = \alpha_0$ e $H_0: \delta = \delta_0$ e para a hipÃ³tese conjunta $H_0: r_1 \alpha + r_2 \delta = r$ Ã© mantida pelas transformaÃ§Ãµes apropriadas nas estatÃ­sticas. O uso da matriz Q e os fatores de escala $\sqrt{T}$ e $T^{3/2}$ garantem que esses testes sejam assintoticamente vÃ¡lidos mesmo quando os estimadores convergem a taxas diferentes.
*Prova:*
A prova segue das etapas de transformaÃ§Ã£o das estatÃ­sticas, da aplicaÃ§Ã£o do teorema do limite central, do uso da matriz $Q$ e da superconsistÃªncia de $\hat{\delta}_T$, garantindo que as estatÃ­sticas de teste convirjam para uma distribuiÃ§Ã£o normal padrÃ£o.

Por fim, um teste conjunto para hipÃ³teses separadas sobre $\alpha$ e $\delta$ [^3],

$$H_0: \begin{bmatrix} \alpha \\ \delta \end{bmatrix} = \begin{bmatrix} \alpha_0 \\ \delta_0 \end{bmatrix}$$

tem a forma de Wald da estatÃ­stica $\chi^2$ do MQO [^3]:
$$
\chi^2_T = (b_T - \beta_0)'[s_T^2(X_T'X_T)^{-1}]^{-1}(b_T - \beta_0)
$$
Aplicando as transformaÃ§Ãµes necessÃ¡rias, a estatÃ­stica pode ser reescrita:
$$
\chi^2_T = (b_T - \beta_0)'Y_T [Y_Ts_T^2(X_T'X_T)^{-1}Y_T']^{-1}Y_T' (b_T - \beta_0)
$$
e sob a hipÃ³tese nula temos
$$
\chi^2_T \to (b_T - \beta_0)'[Q^{-1}]^{-1} (b_T - \beta_0)
$$
que converge para uma distribuiÃ§Ã£o $\chi^2(2)$ [^3].

*Prova:*
I. Partimos da forma de Wald da estatÃ­stica $\chi^2$:
    $$\chi^2_T = (b_T - \beta_0)'[s_T^2(X_T'X_T)^{-1}]^{-1}(b_T - \beta_0)$$
II. Usando $[s_T^2(X_T'X_T)^{-1}]^{-1} = Y_T[Y_Ts_T^2(X_T'X_T)^{-1}Y_T']^{-1}Y_T'$:
    $$\chi^2_T = (b_T - \beta_0)'Y_T [Y_Ts_T^2(X_T'X_T)^{-1}Y_T']^{-1}Y_T' (b_T - \beta_0)$$
III. Aplicamos a convergÃªncia $Y_T(X_T'X_T)^{-1}Y_T' \rightarrow Q^{-1}$:
     $$\chi^2_T \rightarrow (b_T - \beta_0)'[Q^{-1}]^{-1} (b_T - \beta_0)$$
IV. Dado que $\sqrt{T}(b_T - \beta_0)$ converge para uma distribuiÃ§Ã£o normal multivariada, temos:
    $$\chi^2_T \rightarrow \chi^2(2)$$â– 

> ğŸ’¡ **Exemplo NumÃ©rico:**  Vamos calcular a estatÃ­stica $\chi^2$ para o teste conjunto, usando o exemplo anterior, considerando a hipÃ³tese nula $H_0: \alpha=2 \text{ e } \delta=0.7$. Para isso utilizamos o cÃ³digo:
```python
beta_null = np.array([2, 0.7])
beta_hat = np.array([alpha_hat, delta_hat])
cov_params = results.cov_params()
chi2_stat = (beta_hat - beta_null) @ np.linalg.inv(cov_params) @ (beta_hat - beta_null).T
p_value_chi2 = 1 - stats.chi2.cdf(chi2_stat, df=2)
print(f'EstatÃ­stica chi2 para H0: alpha=2 e delta=0.7: {chi2_stat:.4f}')
print(f'p-valor do teste chi2: {p_value_chi2:.4f}')
```
Este cÃ³digo calcula a estatÃ­stica $\chi^2$ e o *p*-valor para a hipÃ³tese conjunta, confirmando a validade assintÃ³tica do teste. Usando os resultados anteriores, a estatÃ­stica $\chi^2$ para a hipÃ³tese conjunta $H_0: \alpha=2 \text{ e } \delta=0.7$ Ã© $\chi^2=0.489$ e o p-valor Ã© $0.783$. Consequentemente, nÃ£o rejeitamos a hipÃ³tese nula de que os dois parÃ¢metros sÃ£o simultaneamente iguais a 2 e 0.7, respectivamente.

**Lema 1.2**
A estatÃ­stica $\chi^2_T$ para o teste de hipÃ³tese conjunta sobre $\alpha$ e $\delta$ converge para uma distribuiÃ§Ã£o $\chi^2$ com 2 graus de liberdade. Este resultado Ã© consequÃªncia da convergÃªncia das estatÃ­sticas individuais para distribuiÃ§Ãµes normais e da estrutura da estatÃ­stica de Wald.
*Prova:*
A prova segue da aplicaÃ§Ã£o do teorema do limite central multivariado e da definiÃ§Ã£o da estatÃ­stica de Wald. A convergÃªncia para uma distribuiÃ§Ã£o $\chi^2(2)$ Ã© uma consequÃªncia direta da normalidade assintÃ³tica dos estimadores e da estrutura da estatÃ­stica.

### ConclusÃ£o
Este capÃ­tulo estabeleceu que, para modelos de tendÃªncias temporais determinÃ­sticas, os testes *t* e *F* do MQO sÃ£o assintoticamente vÃ¡lidos, mesmo quando as inovaÃ§Ãµes nÃ£o sÃ£o Gaussianas [^3]. Isso se deve ao comportamento assintÃ³tico compensatÃ³rio dos erros padrÃ£o, que incorpora diferentes ordens de *T* correspondentes Ã s taxas de convergÃªncia distintas dos estimadores [^3]. As transformaÃ§Ãµes dos estimadores e a utilizaÃ§Ã£o da matriz Q garantem que os testes de hipÃ³teses apresentem distribuiÃ§Ãµes assintÃ³ticas bem definidas e vÃ¡lidas para inferÃªncia [^3]. A anÃ¡lise enfatiza a importÃ¢ncia de considerar as taxas de convergÃªncia para a validaÃ§Ã£o dos testes estatÃ­sticos em modelos com tendÃªncias temporais determinÃ­sticas, e como os procedimentos usuais sÃ£o vÃ¡lidos sob condiÃ§Ãµes assintÃ³ticas.

**CorolÃ¡rio 2**
O redimensionamento apropriado dos estimadores, juntamente com o uso da matriz de informaÃ§Ã£o de Fisher ($Q$), garante que os testes *t* e *F* para modelos com tendÃªncias temporais sejam assintoticamente vÃ¡lidos. Esta anÃ¡lise destaca a necessidade de considerar as caracterÃ­sticas da distribuiÃ§Ã£o assintÃ³tica dos estimadores para a aplicaÃ§Ã£o correta de testes de hipÃ³teses.
*Prova:*
A validade assintÃ³tica dos testes *t* e *F* Ã© uma consequÃªncia direta do redimensionamento adequado das estatÃ­sticas de teste, que utilizam os fatores $\sqrt{T}$ e $T^{3/2}$, e da utilizaÃ§Ã£o da matriz de informaÃ§Ã£o de Fisher $Q$. Essas transformaÃ§Ãµes garantem que, mesmo com diferentes taxas de convergÃªncia para os estimadores $\alpha$ e $\delta$, as estatÃ­sticas de teste convirjam para uma distribuiÃ§Ã£o normal padrÃ£o, permitindo a realizaÃ§Ã£o de inferÃªncias vÃ¡lidas.

**ProposiÃ§Ã£o 1**
Os resultados de validade assintÃ³tica dos testes *t* e *F* tambÃ©m se mantÃªm sob condiÃ§Ãµes mais gerais, como erros nÃ£o Gaussianos com variÃ¢ncia finita e estacionariedade fraca. A condiÃ§Ã£o chave Ã© que os erros satisfaÃ§am as condiÃ§Ãµes necessÃ¡rias para a aplicaÃ§Ã£o do teorema do limite central.
*Prova:*
A prova Ã© uma extensÃ£o dos resultados apresentados, onde a suposiÃ§Ã£o de erros Gaussianos pode ser relaxada para erros com variÃ¢ncia finita e que satisfaÃ§am as condiÃ§Ãµes do teorema do limite central. A normalidade assintÃ³tica dos estimadores Ã© preservada sob estas condiÃ§Ãµes, garantindo a validade dos testes.

### ReferÃªncias
[^1]:  The coefficients of regression models involving unit roots or deterministic time trends are typically estimated by ordinary least squares. However, the asymptotic distributions of the coefficient estimates cannot be calculated in the same way as are those for regression models involving stationary variables. Among other difficulties, the estimates of different parameters will in general have different asymptotic rates of convergence. This chapter introduces the idea of different rates of convergence and develops a general approach to obtaining asymptotic distributions suggested by Sims, Stock, and Watson (1990). This chapter deals exclusively with processes involving deterministic time trends but no unit roots. One of the results for such processes will be that the usual OLS t and F statistics, calculated in the usual way, have the same asymptotic distributions as they do for stationary regressions. Although the limiting distributions are standard, the techniques used to verify these limiting distributions are different from those used in Chapter 8. These techniques will also be used to develop the asymptotic distributions for processes including unit roots in Chapters 17 and 18.
[^2]: This section considers OLS estimation of the parameters of a simple time trend, $y_t = \alpha + \delta t + \epsilon_t$, for $\epsilon_t$ a white noise process.
[^3]: If the innovations $\epsilon_t$ for the simple time trend [16.1.1] are Gaussian, then the OLS estimates $\hat{\alpha}_T$ and $\hat{\delta}_T$ are Gaussian and the usual OLS *t* and *F* tests have exact small-sample *t* and *F* distributions for all sample sizes T. Thus, despite the fact that $\hat{\alpha}_T$ and $\hat{\delta}_T$ have different asymptotic rates of convergence, the standard errors $\hat{\sigma}_{\hat{\alpha}_T}$ and $\hat{\sigma}_{\hat{\delta}_T}$, evidently have offsetting asymptotic behavior so that the statistics such as $(\hat{\delta}_T - \delta)/ \hat{\sigma}_{\hat{\delta}_T}$ are asymptotically N(0, 1) when the innovations are Gaussian. We might thus conjecture that the usual *t* and *F* tests are asymptotically valid for non-Gaussian innovations as well. This conjecture is indeed correct, as we now verify.
<!-- END -->
