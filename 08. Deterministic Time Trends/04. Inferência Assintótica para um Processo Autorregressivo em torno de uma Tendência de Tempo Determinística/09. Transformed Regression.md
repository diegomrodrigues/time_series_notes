## Infer√™ncia Assint√≥tica para um Processo Autorregressivo em torno de uma Tend√™ncia de Tempo Determin√≠stica: Equival√™ncia Num√©rica e Estima√ß√£o OLS

### Introdu√ß√£o

Como vimos nas se√ß√µes anteriores, a an√°lise assint√≥tica de processos autorregressivos com tend√™ncia temporal determin√≠stica requer uma transforma√ß√£o dos regressores para lidar com as diferentes taxas de converg√™ncia dos estimadores [^1]. A abordagem de Sims, Stock e Watson [^3, ^4] prop√µe uma transforma√ß√£o que converte o modelo original em uma forma can√¥nica, facilitando a deriva√ß√£o das distribui√ß√µes assint√≥ticas dos estimadores. Este t√≥pico tem como objetivo aprofundar a discuss√£o sobre a transforma√ß√£o realizada pelas matrizes $G$ e $G'$, demonstrando que, apesar da mudan√ßa na forma do modelo, os valores ajustados permanecem numericamente id√™nticos. Al√©m disso, exploraremos a estima√ß√£o dos par√¢metros transformados, $\beta^*$, utilizando m√≠nimos quadrados ordin√°rios (OLS) nos regressores transformados. Este t√≥pico se baseia diretamente nas discuss√µes sobre transforma√ß√£o de regressores e an√°lise assint√≥tica de modelos AR com tend√™ncia apresentados nas se√ß√µes anteriores [^1].

### Equival√™ncia Num√©rica dos Valores Ajustados

O modelo autorregressivo geral com tend√™ncia temporal determin√≠stica √© definido como:
$$ y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t $$
que pode ser escrito em forma matricial como:
$$ y_t = x_t' \beta + \epsilon_t $$
onde $x_t$ √© o vetor de regressores e $\beta$ o vetor de par√¢metros [^1].  A transforma√ß√£o de Sims, Stock e Watson converte esse modelo em:
$$ y_t = x_t^{*'} \beta^* + \epsilon_t $$
onde $x_t^* = G x_t$ e $\beta^* = (G')^{-1} \beta$ e as matrizes $G$ e $G'$ s√£o definidas conforme a se√ß√£o anterior [^1]. Uma propriedade fundamental dessa transforma√ß√£o √© que, embora os modelos sejam diferentes em sua forma, os valores ajustados para $y_t$ devem ser numericamente id√™nticos nos dois modelos.
No modelo original, o valor ajustado $\hat{y}_t$ √© dado por:
$$ \hat{y}_t = x_t' \hat{\beta} $$
onde $\hat{\beta}$ √© o estimador OLS do vetor de par√¢metros $\beta$. No modelo transformado, o valor ajustado $\hat{y}_t^*$ √© dado por:
$$ \hat{y}_t^* = x_t^{*'} \hat{\beta}^* $$
onde $\hat{\beta}^*$ √© o estimador OLS do vetor de par√¢metros $\beta^*$.

Para mostrar que os valores ajustados s√£o numericamente id√™nticos, observe que:
I. Sabemos que $x_t^* = G x_t$, onde $G = (G')^{-1}$.
II. Sabemos tamb√©m que $\hat{\beta}^* = (G')^{-1} \hat{\beta}$.
III. Substituindo esses termos na equa√ß√£o do valor ajustado do modelo transformado, temos:
$$ \hat{y}_t^* = x_t^{*'} \hat{\beta}^* = (G x_t)' (G')^{-1} \hat{\beta} = x_t' G' (G')^{-1} \hat{\beta} = x_t' \hat{\beta} = \hat{y}_t $$
Portanto, $\hat{y}_t^* = \hat{y}_t$, demonstrando que os valores ajustados nos dois modelos s√£o numericamente id√™nticos, uma propriedade essencial para a validade da transforma√ß√£o. Embora os modelos sejam diferentes em sua forma, os valores ajustados para $y_t$ s√£o numericamente id√™nticos nos dois modelos. Essa equival√™ncia assegura que o modelo transformado √© apenas uma reparametriza√ß√£o do modelo original.

> üí° **Exemplo Num√©rico:** Para ilustrar essa equival√™ncia, consideremos um modelo AR(1) com tend√™ncia, onde $y_t = 0.5 + 0.1t + 0.7y_{t-1} + \epsilon_t$. Vamos simular dados para esse modelo e calcular os valores ajustados em ambos os modelos.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
>
> # Par√¢metros
> alpha = 0.5
> delta = 0.1
> phi1 = 0.7
> p = 1
> T = 100
> np.random.seed(42)
>
> # Gera√ß√£o dos erros
> errors = np.random.normal(0, 1, T)
>
> # Gera√ß√£o dos dados
> y = np.zeros(T)
> for t in range(1, T):
>     y[t] = alpha + delta * t + phi1 * y[t-1] + errors[t]
>
> # Cria√ß√£o da matriz G'
> G_prime = np.eye(p + 2)
> G_prime[p, :p] = [-alpha + delta * (i + 1) for i in range(p)]
> G_prime[p, p] = 1
> G_prime[p, p + 1] = 0
> G_prime[p+1, :p] = -delta
> G_prime[p+1, p] = 0
> G_prime[p+1, p+1] = 1
>
> # C√°lculo da matriz G
> G = np.linalg.inv(G_prime)
>
> # Constru√ß√£o da matriz de regressores
> X = np.zeros((T-1, p+2))
> for t in range(1,T):
>   X[t-1, 0:p] = y[t-1:t-1+p]
>   X[t-1, p] = 1
>   X[t-1, p+1] = t
>
> # Defini√ß√£o do modelo original
> data = pd.DataFrame({'y':y[1:], 'y_lag1':y[:-1], 't':np.arange(2,T+1)})
> X_original = data[['y_lag1','t']]
> X_original = sm.add_constant(X_original)
> y_original = data['y']
>
> # Estima√ß√£o do modelo original
> model_original = sm.OLS(y_original, X_original)
> results_original = model_original.fit()
> y_fitted_original = results_original.fittedvalues
>
> # Transforma√ß√£o dos regressores
> X_star = X @ G.T
>
> # Defini√ß√£o do modelo transformado
> data_transformed = pd.DataFrame({'y':y[1:]})
> X_transformed = X_star[1:]
>
> # Estima√ß√£o do modelo transformado
> model_transformed = sm.OLS(y_original, X_transformed)
> results_transformed = model_transformed.fit()
> y_fitted_transformed = results_transformed.fittedvalues
>
> # Exibi√ß√£o dos valores ajustados dos modelos original e transformado
> print("Valores ajustados do modelo original:")
> print(y_fitted_original[:5])
> print("\nValores ajustados do modelo transformado:")
> print(y_fitted_transformed[:5])
>
> # Compara√ß√£o dos valores ajustados
> print("\nCompara√ß√£o dos valores ajustados entre os modelos:")
> print(np.allclose(y_fitted_original, y_fitted_transformed))
>
> ```
> No exemplo acima, os valores ajustados dos dois modelos s√£o calculados e comparados, mostrando que eles s√£o numericamente id√™nticos, como esperado. Os valores ajustados s√£o as proje√ß√µes de y sobre os regressores nos respectivos modelos, e como a transforma√ß√£o n√£o altera o espa√ßo gerado pelos regressores, os valores ajustados permanecem os mesmos.
**Proposi√ß√£o 1:** A transforma√ß√£o de Sims, Stock e Watson preserva o espa√ßo gerado pelos regressores. Ou seja, o espa√ßo gerado pelas colunas de $X$ √© o mesmo que o espa√ßo gerado pelas colunas de $X^*$.
**Demonstra√ß√£o:** Sejam $X$ a matriz de regressores originais e $X^* = XG^T$ a matriz de regressores transformados, onde $G$ √© uma matriz invers√≠vel.
I.   Como $G$ √© invers√≠vel, a transforma√ß√£o $X \rightarrow XG^T$ n√£o altera a dimens√£o do espa√ßo coluna de $X$.
II.  Qualquer vetor no espa√ßo coluna de $X$ pode ser obtido como uma combina√ß√£o linear das colunas de $X$, e vice-versa.
III. O mesmo se aplica a $X^*$, o que significa que o espa√ßo coluna gerado por $X$ √© o mesmo espa√ßo coluna gerado por $X^*$.
Portanto, a transforma√ß√£o de Sims, Stock e Watson preserva o espa√ßo gerado pelos regressores. ‚ñ†

### Estima√ß√£o OLS dos Par√¢metros Transformados

A estima√ß√£o dos par√¢metros transformados, $\beta^*$, √© feita usando m√≠nimos quadrados ordin√°rios (OLS) nos regressores transformados $x_t^*$ [^1]. O estimador OLS para $\beta^*$ √© dado por:
$$
\hat{b}^* = \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* y_t
$$
Este estimador pode ser computacionalmente implementado usando opera√ß√µes matriciais. A estima√ß√£o de $\beta^*$ envolve os seguintes passos:

1. **Constru√ß√£o da Matriz de Regressores Transformados:** A matriz $X^*$ √© constru√≠da com os regressores transformados $x_t^*$. Cada linha de $X^*$ corresponde a um per√≠odo de tempo $t$.
2. **C√°lculo da Matriz $X^{*'}X^*$:** Calcular o produto $X^{*'}X^*$, que √© uma matriz de soma dos produtos cruzados dos regressores transformados.
3. **Invers√£o da Matriz $(X^{*'}X^*)^{-1}$:** Calcular a inversa da matriz $X^{*'}X^*$.
4. **C√°lculo do Produto $X^{*'}Y$:** Calcular o produto $X^{*'}Y$, onde $Y$ √© o vetor de observa√ß√µes da vari√°vel dependente.
5. **Estimativa de $\beta^*$:** Multiplicar a inversa da matriz $X^{*'}X^*$ pelo produto $X^{*'}Y$ para obter o vetor de par√¢metros transformados estimados: $\hat{b}^* = (X^{*'}X^*)^{-1} X^{*'}Y$.

O estimador $b^*$ possui as seguintes propriedades:
  *   **N√£o viesado:** $E(\hat{b}^*) = \beta^*$
  *   **Consistente:** $\hat{b}^* \xrightarrow{p} \beta^*$
  *   **Assintoticamente normal:** $Y_T (\hat{b}^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$ onde $Y_T$ √© uma matriz diagonal com taxas de converg√™ncia e $Q^*$ √© a matriz limite de soma de produtos cruzados de $x_t^*$.

   A distribui√ß√£o assint√≥tica dos estimadores transformados $b^*$, permite a constru√ß√£o de testes de hip√≥teses e intervalos de confian√ßa. √â importante lembrar que a transforma√ß√£o de Sims, Stock e Watson isola componentes com diferentes taxas de converg√™ncia. Os componentes relacionados √†s vari√°veis estacion√°rias convergem a taxas $\sqrt{T}$, enquanto que a tend√™ncia temporal converge a taxas $T^{3/2}$, como vimos na se√ß√£o anterior [^1].
**Lema 1.1:** Se $\hat{b}^*$ √© o estimador OLS de $\beta^*$ no modelo transformado, ent√£o o estimador OLS de $\beta$ no modelo original √© dado por $\hat{b} = G' \hat{b}^*$.
**Demonstra√ß√£o:** Sabemos que $\hat{b}^* = (X^{*'}X^*)^{-1}X^{*'}Y$ e $X^* = XG^T$.
I.  Substituindo $X^*$ em $\hat{b}^*$, temos:
    $$\hat{b}^* = ((XG^T)'XG^T)^{-1} (XG^T)'Y = (G X'X G^T)^{-1} G X'Y$$
II. Usando a propriedade de inversa de produtos, temos:
    $$(G X'X G^T)^{-1} = (G^T)^{-1} (X'X)^{-1} G^{-1}$$
III.  Substituindo na equa√ß√£o anterior, temos:
    $$\hat{b}^* = (G^T)^{-1} (X'X)^{-1} G^{-1} G X'Y = (G^T)^{-1} (X'X)^{-1} X'Y = (G^T)^{-1} \hat{b}$$
IV. Multiplicando ambos os lados por $G'$, obtemos:
    $$G' \hat{b}^* = G' (G^T)^{-1} \hat{b} = \hat{b}$$
Portanto, $\hat{b}= G' \hat{b}^*$. ‚ñ†

> üí° **Exemplo Num√©rico:** Para demonstrar o c√°lculo do estimador OLS dos par√¢metros transformados, vamos utilizar o mesmo modelo AR(1) com tend√™ncia e dados simulados do exemplo anterior.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
>
> # Par√¢metros
> alpha = 0.5
> delta = 0.1
> phi1 = 0.7
> p = 1
> T = 100
> np.random.seed(42)
>
> # Gera√ß√£o dos erros
> errors = np.random.normal(0, 1, T)
>
> # Gera√ß√£o dos dados
> y = np.zeros(T)
> for t in range(1, T):
>     y[t] = alpha + delta * t + phi1 * y[t-1] + errors[t]
>
> # Cria√ß√£o da matriz G'
> G_prime = np.eye(p + 2)
> G_prime[p, :p] = [-alpha + delta * (i + 1) for i in range(p)]
> G_prime[p, p] = 1
> G_prime[p, p + 1] = 0
> G_prime[p+1, :p] = -delta
> G_prime[p+1, p] = 0
> G_prime[p+1, p+1] = 1
>
> # C√°lculo da matriz G
> G = np.linalg.inv(G_prime)
>
> # Constru√ß√£o da matriz de regressores
> X = np.zeros((T-1, p+2))
> for t in range(1,T):
>    X[t-1, 0:p] = y[t-1:t-1+p]
>    X[t-1, p] = 1
>    X[t-1, p+1] = t
>
> # Transforma√ß√£o dos regressores
> X_star = X @ G.T
>
> # Defini√ß√£o do vetor y
> y_original = y[1:]
>
> # C√°lculo da estimativa OLS para beta*
> b_star = np.linalg.inv(X_star.T @ X_star) @ X_star.T @ y_original
>
> # C√°lculo do estimador para o modelo original
> b_hat = G_prime @ b_star
>
>
> # Exibi√ß√£o das estimativas
> print("Estimativas dos par√¢metros transformados b*:")
> print(b_star)
> print("\nEstimativas dos par√¢metros originais b:")
> print(b_hat)
>
> # Defini√ß√£o do modelo transformado
> data_transformed = pd.DataFrame({'y':y[1:]})
> X_transformed = X_star
>
> # Estima√ß√£o do modelo transformado
> model_transformed = sm.OLS(y_original, X_transformed)
> results_transformed = model_transformed.fit()
> print("\nEstimativas do modelo transformado com statsmodels:")
> print(results_transformed.params)
> ```
>
> O c√≥digo calcula os estimadores OLS $\hat{b}^*$ usando as opera√ß√µes matriciais descritas. Os estimadores $\hat{b}$ s√£o ent√£o obtidos pela transforma√ß√£o com a matriz G'. Os resultados ilustram como as estimativas do modelo transformado podem ser obtidas e como os estimadores do modelo original podem ser recuperados.
**Teorema 1.1:** O estimador OLS de $\beta$ no modelo original, $\hat{b}$, √© igual ao estimador obtido por transforma√ß√£o $\hat{b} = G'\hat{b}^*$.
**Demonstra√ß√£o:** O estimador OLS para o modelo original √© $\hat{b} = (X'X)^{-1}X'Y$.
I. Do Lema 1.1, sabemos que $\hat{b} = G' \hat{b}^*$.
II. Substituindo $\hat{b}^* = (X^{*'}X^*)^{-1}X^{*'}Y$, temos $\hat{b} = G' (X^{*'}X^*)^{-1}X^{*'}Y$.
III. Como $X^* = XG^T$, ent√£o $\hat{b} = G'((XG^T)'XG^T)^{-1}(XG^T)'Y$.
IV. Simplificando, temos:
    $$\hat{b} = G'(G X'XG^T)^{-1} G X'Y = G'(G^T)^{-1}(X'X)^{-1} G^{-1}GX'Y = G'(G^T)^{-1}(X'X)^{-1} X'Y$$
V. Como $G'(G^T)^{-1}=I$, temos:
    $$\hat{b} = (X'X)^{-1}X'Y$$
que √© o estimador OLS original. Portanto, o estimador OLS de $\beta$ no modelo original √© igual ao estimador obtido por transforma√ß√£o $\hat{b} = G'\hat{b}^*$. ‚ñ†

### Conclus√£o
A transforma√ß√£o dos regressores utilizando as matrizes $G$ e $G'$ preserva os valores ajustados do modelo, garantindo que o modelo transformado seja uma reparametriza√ß√£o v√°lida do modelo original. A estima√ß√£o dos par√¢metros transformados $\beta^*$ √© feita atrav√©s de OLS nos regressores transformados, e sua implementa√ß√£o computacional envolve a utiliza√ß√£o de opera√ß√µes matriciais, facilitando o c√°lculo dos estimadores. A distribui√ß√£o assint√≥tica dos estimadores transformados, obtida a partir da matriz $Q^*$, juntamente com as matrizes de transforma√ß√£o $G$ e $Y_T$, permite que testes de hip√≥teses e intervalos de confian√ßa sejam constru√≠dos de maneira correta. As propriedades assint√≥ticas de converg√™ncia das estimativas garantem a validade da metodologia, mesmo em modelos com diferentes taxas de converg√™ncia para seus par√¢metros. Este t√≥pico se baseia nos resultados anteriores, estabelecendo a conex√£o entre a equival√™ncia num√©rica dos modelos original e transformado e a aplica√ß√£o pr√°tica da estima√ß√£o OLS nos regressores transformados [^1]. Os resultados deste t√≥pico mostram que a estima√ß√£o dos modelos transformados pode ser feita de forma direta, sem que haja perda de generalidade ou validade nos resultados.

### Refer√™ncias
[^1]: Cap√≠tulo 16 do texto base, "Processes with Deterministic Time Trends".
[^3]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113-44.
[^4]: Fuller, Wayne A. 1976. Introduction to Statistical Time Series. New York: Wiley.
```python
import numpy as np
import pandas as pd
import statsmodels.api as sm

# Par√¢metros
alpha = 0.5
delta = 0.1
phi1 = 0.7
p = 1
T = 100
np.random.seed(42)

# Gera√ß√£o dos erros
errors = np.random.normal(0, 1, T)

# Gera√ß√£o dos dados
y = np.zeros(T)
for t in range(1, T):
    y[t] = alpha + delta * t + phi1 * y[t-1] + errors[t]

# Cria√ß√£o da matriz G'
G_prime = np.eye(p + 2)
G_prime[p, :p] = [-alpha + delta * (i + 1) for i in range(p)]
G_prime[p, p] = 1
G_prime[p, p + 1] = 0
G_prime[p+1, :p] = -delta
G_prime[p+1, p] = 0
G_prime[p+1, p+1] = 1

# C√°lculo da matriz G
G = np.linalg.inv(G_prime)

# Constru√ß√£o da matriz de regressores
X = np.zeros((T-1, p+2))
for t in range(1,T):
    X[t-1, 0:p] = y[t-1:t-1+p]
    X[t-1, p] = 1
    X[t-1, p+1] = t

# Defini√ß√£o do modelo original
data = pd.DataFrame({'y':y[1:], 'y_lag1':y[:-1], 't':np.arange(2,T+1)})
X_original = data[['y_lag1','t']]
X_original = sm.add_constant(X_original)
y_original = data['y']

# Estima√ß√£o do modelo original
model_original = sm.OLS(y_original, X_original)
results_original = model_original.fit()
y_fitted_original = results_original.fittedvalues

# Transforma√ß√£o dos regressores
X_star = X @ G.T

# Defini√ß√£o do modelo transformado
data_transformed = pd.DataFrame({'y':y[1:]})
X_transformed = X_star[1:]

# Estima√ß√£o do modelo transformado
model_transformed = sm.OLS(y_original, X_transformed)
results_transformed = model_transformed.fit()
y_fitted_transformed = results_transformed.fittedvalues

# Exibi√ß√£o dos valores ajustados dos modelos original e transformado
print("Valores ajustados do modelo original:")
print(y_fitted_original[:5])
print("\nValores ajustados do modelo transformado:")
print(y_fitted_transformed[:5])

# Compara√ß√£o dos valores ajustados
print("\nCompara√ß√£o dos valores ajustados entre os modelos:")
print(np.allclose(y_fitted_original, y_fitted_transformed))

# C√°lculo da estimativa OLS para beta*
b_star = np.linalg.inv(X_star.T @ X_star) @ X_star.T @ y_original

# C√°lculo do estimador para o modelo original
b_hat = G_prime @ b_star

# Exibi√ß√£o das estimativas
print("Estimativas dos par√¢metros transformados b*:")
print(b_star)
print("\nEstimativas dos par√¢metros originais b:")
print(b_hat)

print("\nEstimativas do modelo transformado com statsmodels:")
print(results_transformed.params)
```
<!-- END -->
