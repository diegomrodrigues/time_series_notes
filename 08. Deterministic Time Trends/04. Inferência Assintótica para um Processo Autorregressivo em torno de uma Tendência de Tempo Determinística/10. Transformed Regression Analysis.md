## InferÃªncia AssintÃ³tica para um Processo Autorregressivo em torno de uma TendÃªncia de Tempo DeterminÃ­stica: EquivalÃªncia NumÃ©rica e EstimaÃ§Ã£o OLS

### IntroduÃ§Ã£o

Como vimos nas seÃ§Ãµes anteriores, a anÃ¡lise assintÃ³tica de processos autorregressivos com tendÃªncia temporal determinÃ­stica requer uma transformaÃ§Ã£o dos regressores para lidar com as diferentes taxas de convergÃªncia dos estimadores [^1]. A abordagem de Sims, Stock e Watson [^3, ^4] propÃµe uma transformaÃ§Ã£o que converte o modelo original em uma forma canÃ´nica, facilitando a derivaÃ§Ã£o das distribuiÃ§Ãµes assintÃ³ticas dos estimadores. Este tÃ³pico tem como objetivo aprofundar a discussÃ£o sobre a transformaÃ§Ã£o realizada pelas matrizes $G$ e $G'$, demonstrando que, apesar da mudanÃ§a na forma do modelo, os valores ajustados permanecem numericamente idÃªnticos. AlÃ©m disso, exploraremos a estimaÃ§Ã£o dos parÃ¢metros transformados, $\beta^*$, utilizando mÃ­nimos quadrados ordinÃ¡rios (OLS) nos regressores transformados. Este tÃ³pico se baseia diretamente nas discussÃµes sobre transformaÃ§Ã£o de regressores e anÃ¡lise assintÃ³tica de modelos AR com tendÃªncia apresentados nas seÃ§Ãµes anteriores [^1].

### EquivalÃªncia NumÃ©rica dos Valores Ajustados

O modelo autorregressivo geral com tendÃªncia temporal determinÃ­stica Ã© definido como:
$$ y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t $$
que pode ser escrito em forma matricial como:
$$ y_t = x_t' \beta + \epsilon_t $$
onde $x_t$ Ã© o vetor de regressores e $\beta$ o vetor de parÃ¢metros [^1].  A transformaÃ§Ã£o de Sims, Stock e Watson converte esse modelo em:
$$ y_t = x_t^{*'} \beta^* + \epsilon_t $$
onde $x_t^* = G x_t$ e $\beta^* = (G')^{-1} \beta$ e as matrizes $G$ e $G'$ sÃ£o definidas conforme a seÃ§Ã£o anterior [^1]. Uma propriedade fundamental dessa transformaÃ§Ã£o Ã© que, embora os modelos sejam diferentes em sua forma, os valores ajustados para $y_t$ devem ser numericamente idÃªnticos nos dois modelos.
No modelo original, o valor ajustado $\hat{y}_t$ Ã© dado por:
$$ \hat{y}_t = x_t' \hat{\beta} $$
onde $\hat{\beta}$ Ã© o estimador OLS do vetor de parÃ¢metros $\beta$. No modelo transformado, o valor ajustado $\hat{y}_t^*$ Ã© dado por:
$$ \hat{y}_t^* = x_t^{*'} \hat{\beta}^* $$
onde $\hat{\beta}^*$ Ã© o estimador OLS do vetor de parÃ¢metros $\beta^*$.

Para mostrar que os valores ajustados sÃ£o numericamente idÃªnticos, observe que:
I. Sabemos que $x_t^* = G x_t$, onde $G = (G')^{-1}$.
II. Sabemos tambÃ©m que $\hat{\beta}^* = (G')^{-1} \hat{\beta}$.
III. Substituindo esses termos na equaÃ§Ã£o do valor ajustado do modelo transformado, temos:
$$ \hat{y}_t^* = x_t^{*'} \hat{\beta}^* = (G x_t)' (G')^{-1} \hat{\beta} = x_t' G' (G')^{-1} \hat{\beta} = x_t' \hat{\beta} = \hat{y}_t $$
Portanto, $\hat{y}_t^* = \hat{y}_t$, demonstrando que os valores ajustados nos dois modelos sÃ£o numericamente idÃªnticos, uma propriedade essencial para a validade da transformaÃ§Ã£o. Embora os modelos sejam diferentes em sua forma, os valores ajustados para $y_t$ sÃ£o numericamente idÃªnticos nos dois modelos. Essa equivalÃªncia assegura que o modelo transformado Ã© apenas uma reparametrizaÃ§Ã£o do modelo original.

> ğŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar essa equivalÃªncia, consideremos um modelo AR(1) com tendÃªncia, onde $y_t = 0.5 + 0.1t + 0.7y_{t-1} + \epsilon_t$. Vamos simular dados para esse modelo e calcular os valores ajustados em ambos os modelos.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
>
> # ParÃ¢metros
> alpha = 0.5
> delta = 0.1
> phi1 = 0.7
> p = 1
> T = 100
> np.random.seed(42)
>
> # GeraÃ§Ã£o dos erros
> errors = np.random.normal(0, 1, T)
>
> # GeraÃ§Ã£o dos dados
> y = np.zeros(T)
> for t in range(1, T):
>     y[t] = alpha + delta * t + phi1 * y[t-1] + errors[t]
>
> # CriaÃ§Ã£o da matriz G'
> G_prime = np.eye(p + 2)
> G_prime[p, :p] = [-alpha + delta * (i + 1) for i in range(p)]
> G_prime[p, p] = 1
> G_prime[p, p + 1] = 0
> G_prime[p+1, :p] = -delta
> G_prime[p+1, p] = 0
> G_prime[p+1, p+1] = 1
>
> # CÃ¡lculo da matriz G
> G = np.linalg.inv(G_prime)
>
> # ConstruÃ§Ã£o da matriz de regressores
> X = np.zeros((T-1, p+2))
> for t in range(1,T):
>   X[t-1, 0:p] = y[t-1:t-1+p]
>   X[t-1, p] = 1
>   X[t-1, p+1] = t
>
> # DefiniÃ§Ã£o do modelo original
> data = pd.DataFrame({'y':y[1:], 'y_lag1':y[:-1], 't':np.arange(2,T+1)})
> X_original = data[['y_lag1','t']]
> X_original = sm.add_constant(X_original)
> y_original = data['y']
>
> # EstimaÃ§Ã£o do modelo original
> model_original = sm.OLS(y_original, X_original)
> results_original = model_original.fit()
> y_fitted_original = results_original.fittedvalues
>
> # TransformaÃ§Ã£o dos regressores
> X_star = X @ G.T
>
> # DefiniÃ§Ã£o do modelo transformado
> data_transformed = pd.DataFrame({'y':y[1:]})
> X_transformed = X_star[1:]
>
> # EstimaÃ§Ã£o do modelo transformado
> model_transformed = sm.OLS(y_original, X_transformed)
> results_transformed = model_transformed.fit()
> y_fitted_transformed = results_transformed.fittedvalues
>
> # ExibiÃ§Ã£o dos valores ajustados dos modelos original e transformado
> print("Valores ajustados do modelo original:")
> print(y_fitted_original[:5])
> print("\nValores ajustados do modelo transformado:")
> print(y_fitted_transformed[:5])
>
> # ComparaÃ§Ã£o dos valores ajustados
> print("\nComparaÃ§Ã£o dos valores ajustados entre os modelos:")
> print(np.allclose(y_fitted_original, y_fitted_transformed))
>
> ```
> No exemplo acima, os valores ajustados dos dois modelos sÃ£o calculados e comparados, mostrando que eles sÃ£o numericamente idÃªnticos, como esperado. Os valores ajustados sÃ£o as projeÃ§Ãµes de y sobre os regressores nos respectivos modelos, e como a transformaÃ§Ã£o nÃ£o altera o espaÃ§o gerado pelos regressores, os valores ajustados permanecem os mesmos.
**ProposiÃ§Ã£o 1:** A transformaÃ§Ã£o de Sims, Stock e Watson preserva o espaÃ§o gerado pelos regressores. Ou seja, o espaÃ§o gerado pelas colunas de $X$ Ã© o mesmo que o espaÃ§o gerado pelas colunas de $X^*$.
**DemonstraÃ§Ã£o:** Sejam $X$ a matriz de regressores originais e $X^* = XG^T$ a matriz de regressores transformados, onde $G$ Ã© uma matriz inversÃ­vel.
I.   Como $G$ Ã© inversÃ­vel, a transformaÃ§Ã£o $X \rightarrow XG^T$ nÃ£o altera a dimensÃ£o do espaÃ§o coluna de $X$.
II.  Qualquer vetor no espaÃ§o coluna de $X$ pode ser obtido como uma combinaÃ§Ã£o linear das colunas de $X$, e vice-versa.
III. O mesmo se aplica a $X^*$, o que significa que o espaÃ§o coluna gerado por $X$ Ã© o mesmo espaÃ§o coluna gerado por $X^*$.
Portanto, a transformaÃ§Ã£o de Sims, Stock e Watson preserva o espaÃ§o gerado pelos regressores. â– 

### EstimaÃ§Ã£o OLS dos ParÃ¢metros Transformados

A estimaÃ§Ã£o dos parÃ¢metros transformados, $\beta^*$, Ã© feita usando mÃ­nimos quadrados ordinÃ¡rios (OLS) nos regressores transformados $x_t^*$ [^1]. O estimador OLS para $\beta^*$ Ã© dado por:
$$
\hat{b}^* = \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* y_t
$$
Este estimador pode ser computacionalmente implementado usando operaÃ§Ãµes matriciais. A estimaÃ§Ã£o de $\beta^*$ envolve os seguintes passos:

1. **ConstruÃ§Ã£o da Matriz de Regressores Transformados:** A matriz $X^*$ Ã© construÃ­da com os regressores transformados $x_t^*$. Cada linha de $X^*$ corresponde a um perÃ­odo de tempo $t$.
2. **CÃ¡lculo da Matriz $X^{*'}X^*$:** Calcular o produto $X^{*'}X^*$, que Ã© uma matriz de soma dos produtos cruzados dos regressores transformados.
3. **InversÃ£o da Matriz $(X^{*'}X^*)^{-1}$:** Calcular a inversa da matriz $X^{*'}X^*$.
4. **CÃ¡lculo do Produto $X^{*'}Y$:** Calcular o produto $X^{*'}Y$, onde $Y$ Ã© o vetor de observaÃ§Ãµes da variÃ¡vel dependente.
5. **Estimativa de $\beta^*$:** Multiplicar a inversa da matriz $X^{*'}X^*$ pelo produto $X^{*'}Y$ para obter o vetor de parÃ¢metros transformados estimados: $\hat{b}^* = (X^{*'}X^*)^{-1} X^{*'}Y$.

O estimador $b^*$ possui as seguintes propriedades:
  *   **NÃ£o viesado:** $E(\hat{b}^*) = \beta^*$
  *   **Consistente:** $\hat{b}^* \xrightarrow{p} \beta^*$
  *   **Assintoticamente normal:** $Y_T (\hat{b}^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$ onde $Y_T$ Ã© uma matriz diagonal com taxas de convergÃªncia e $Q^*$ Ã© a matriz limite de soma de produtos cruzados de $x_t^*$.

   A distribuiÃ§Ã£o assintÃ³tica dos estimadores transformados $b^*$, permite a construÃ§Ã£o de testes de hipÃ³teses e intervalos de confianÃ§a. Ã‰ importante lembrar que a transformaÃ§Ã£o de Sims, Stock e Watson isola componentes com diferentes taxas de convergÃªncia. Os componentes relacionados Ã s variÃ¡veis estacionÃ¡rias convergem a taxas $\sqrt{T}$, enquanto que a tendÃªncia temporal converge a taxas $T^{3/2}$, como vimos na seÃ§Ã£o anterior [^1].
**Lema 1.1:** Se $\hat{b}^*$ Ã© o estimador OLS de $\beta^*$ no modelo transformado, entÃ£o o estimador OLS de $\beta$ no modelo original Ã© dado por $\hat{b} = G' \hat{b}^*$.
**DemonstraÃ§Ã£o:** Sabemos que $\hat{b}^* = (X^{*'}X^*)^{-1}X^{*'}Y$ e $X^* = XG^T$.
I.  Substituindo $X^*$ em $\hat{b}^*$, temos:
    $$\hat{b}^* = ((XG^T)'XG^T)^{-1} (XG^T)'Y = (G X'X G^T)^{-1} G X'Y$$
II. Usando a propriedade de inversa de produtos, temos:
    $$(G X'X G^T)^{-1} = (G^T)^{-1} (X'X)^{-1} G^{-1}$$
III.  Substituindo na equaÃ§Ã£o anterior, temos:
    $$\hat{b}^* = (G^T)^{-1} (X'X)^{-1} G^{-1} G X'Y = (G^T)^{-1} (X'X)^{-1} X'Y = (G^T)^{-1} \hat{b}$$
IV. Multiplicando ambos os lados por $G'$, obtemos:
    $$G' \hat{b}^* = G' (G^T)^{-1} \hat{b} = \hat{b}$$
Portanto, $\hat{b}= G' \hat{b}^*$. â– 

> ğŸ’¡ **Exemplo NumÃ©rico:** Para demonstrar o cÃ¡lculo do estimador OLS dos parÃ¢metros transformados, vamos utilizar o mesmo modelo AR(1) com tendÃªncia e dados simulados do exemplo anterior.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
>
> # ParÃ¢metros
> alpha = 0.5
> delta = 0.1
> phi1 = 0.7
> p = 1
> T = 100
> np.random.seed(42)
>
> # GeraÃ§Ã£o dos erros
> errors = np.random.normal(0, 1, T)
>
> # GeraÃ§Ã£o dos dados
> y = np.zeros(T)
> for t in range(1, T):
>     y[t] = alpha + delta * t + phi1 * y[t-1] + errors[t]
>
> # CriaÃ§Ã£o da matriz G'
> G_prime = np.eye(p + 2)
> G_prime[p, :p] = [-alpha + delta * (i + 1) for i in range(p)]
> G_prime[p, p] = 1
> G_prime[p, p + 1] = 0
> G_prime[p+1, :p] = -delta
> G_prime[p+1, p] = 0
> G_prime[p+1, p+1] = 1
>
> # CÃ¡lculo da matriz G
> G = np.linalg.inv(G_prime)
>
> # ConstruÃ§Ã£o da matriz de regressores
> X = np.zeros((T-1, p+2))
> for t in range(1,T):
>    X[t-1, 0:p] = y[t-1:t-1+p]
>    X[t-1, p] = 1
>    X[t-1, p+1] = t
>
> # TransformaÃ§Ã£o dos regressores
> X_star = X @ G.T
>
> # DefiniÃ§Ã£o do vetor y
> y_original = y[1:]
>
> # CÃ¡lculo da estimativa OLS para beta*
> b_star = np.linalg.inv(X_star.T @ X_star) @ X_star.T @ y_original
>
> # CÃ¡lculo do estimador para o modelo original
> b_hat = G_prime @ b_star
>
>
> # ExibiÃ§Ã£o das estimativas
> print("Estimativas dos parÃ¢metros transformados b*:")
> print(b_star)
> print("\nEstimativas dos parÃ¢metros originais b:")
> print(b_hat)
>
> # DefiniÃ§Ã£o do modelo transformado
> data_transformed = pd.DataFrame({'y':y[1:]})
> X_transformed = X_star
>
> # EstimaÃ§Ã£o do modelo transformado
> model_transformed = sm.OLS(y_original, X_transformed)
> results_transformed = model_transformed.fit()
> print("\nEstimativas do modelo transformado com statsmodels:")
> print(results_transformed.params)
> ```
>
> O cÃ³digo calcula os estimadores OLS $\hat{b}^*$ usando as operaÃ§Ãµes matriciais descritas. Os estimadores $\hat{b}$ sÃ£o entÃ£o obtidos pela transformaÃ§Ã£o com a matriz G'. Os resultados ilustram como as estimativas do modelo transformado podem ser obtidas e como os estimadores do modelo original podem ser recuperados.
**Teorema 1.1:** O estimador OLS de $\beta$ no modelo original, $\hat{b}$, Ã© igual ao estimador obtido por transformaÃ§Ã£o $\hat{b} = G'\hat{b}^*$.
**DemonstraÃ§Ã£o:** O estimador OLS para o modelo original Ã© $\hat{b} = (X'X)^{-1}X'Y$.
I. Do Lema 1.1, sabemos que $\hat{b} = G' \hat{b}^*$.
II. Substituindo $\hat{b}^* = (X^{*'}X^*)^{-1}X^{*'}Y$, temos $\hat{b} = G' (X^{*'}X^*)^{-1}X^{*'}Y$.
III. Como $X^* = XG^T$, entÃ£o $\hat{b} = G'((XG^T)'XG^T)^{-1}(XG^T)'Y$.
IV. Simplificando, temos:
    $$\hat{b} = G'(G X'XG^T)^{-1} G X'Y = G'(G^T)^{-1}(X'X)^{-1} G^{-1}GX'Y = G'(G^T)^{-1}(X'X)^{-1} X'Y$$
V. Como $G'(G^T)^{-1}=I$, temos:
    $$\hat{b} = (X'X)^{-1}X'Y$$
que Ã© o estimador OLS original. Portanto, o estimador OLS de $\beta$ no modelo original Ã© igual ao estimador obtido por transformaÃ§Ã£o $\hat{b} = G'\hat{b}^*$. â– 

### ConclusÃ£o
A transformaÃ§Ã£o dos regressores utilizando as matrizes $G$ e $G'$ preserva os valores ajustados do modelo, garantindo que o modelo transformado seja uma reparametrizaÃ§Ã£o vÃ¡lida do modelo original. A estimaÃ§Ã£o dos parÃ¢metros transformados $\beta^*$ Ã© feita atravÃ©s de OLS nos regressores transformados, e sua implementaÃ§Ã£o computacional envolve a utilizaÃ§Ã£o de operaÃ§Ãµes matriciais, facilitando o cÃ¡lculo dos estimadores. A distribuiÃ§Ã£o assintÃ³tica dos estimadores transformados, obtida a partir da matriz $Q^*$, juntamente com as matrizes de transformaÃ§Ã£o $G$ e $Y_T$, permite que testes de hipÃ³teses e intervalos de confianÃ§a sejam construÃ­dos de maneira correta. As propriedades assintÃ³ticas de convergÃªncia das estimativas garantem a validade da metodologia, mesmo em modelos com diferentes taxas de convergÃªncia para seus parÃ¢metros. Este tÃ³pico se baseia nos resultados anteriores, estabelecendo a conexÃ£o entre a equivalÃªncia numÃ©rica dos modelos original e transformado e a aplicaÃ§Ã£o prÃ¡tica da estimaÃ§Ã£o OLS nos regressores transformados [^1]. Os resultados deste tÃ³pico mostram que a estimaÃ§Ã£o dos modelos transformados pode ser feita de forma direta, sem que haja perda de generalidade ou validade nos resultados.

### ReferÃªncias
[^1]: CapÃ­tulo 16 do texto base, "Processes with Deterministic Time Trends".
[^3]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113-44.
[^4]: Fuller, Wayne A. 1976. Introduction to Statistical Time Series. New York: Wiley.
```python
import numpy as np
import pandas as pd
import statsmodels.api as sm

# ParÃ¢metros
alpha = 0.5
delta = 0.1
phi1 = 0.7
p = 1
T = 100
np.random.seed(42)

# GeraÃ§Ã£o dos erros
errors = np.random.normal(0, 1, T)

# GeraÃ§Ã£o dos dados
y = np.zeros(T)
for t in range(1, T):
    y[t] = alpha + delta * t + phi1 * y[t-1] + errors[t]

# CriaÃ§Ã£o da matriz G'
G_prime = np.eye(p + 2)
G_prime[p, :p] = [-alpha + delta * (i + 1) for i in range(p)]
G_prime[p, p] = 1
G_prime[p, p + 1] = 0
G_prime[p+1, :p] = -delta
G_prime[p+1, p] = 0
G_prime[p+1, p+1] = 1

# CÃ¡lculo da matriz G
G = np.linalg.inv(G_prime)

# ConstruÃ§Ã£o da matriz de regressores
X = np.zeros((T-1, p+2))
for t in range(1,T):
    X[t-1, 0:p] = y[t-1:t-1+p]
    X[t-1, p] = 1
    X[t-1, p+1] = t

# DefiniÃ§Ã£o do modelo original
data = pd.DataFrame({'y':y[1:], 'y_lag1':y[:-1], 't':np.arange(2,T+1)})
X_original = data[['y_lag1','t']]
X_original = sm.add_constant(X_original)
y_original = data['y']

# EstimaÃ§Ã£o do modelo original
model_original = sm.OLS(y_original, X_original)
results_original = model_original.fit()
y_fitted_original = results_original.fittedvalues

# TransformaÃ§Ã£o dos regressores
X_star = X @ G.T

# DefiniÃ§Ã£o do modelo transformado
data_transformed = pd.DataFrame({'y':y[1:]})
X_transformed = X_star[1:]

# EstimaÃ§Ã£o do modelo transformado
model_transformed = sm.OLS(y_original, X_transformed)
results_transformed = model_transformed.fit()
y_fitted_transformed = results_transformed.fittedvalues

# ExibiÃ§Ã£o dos valores ajustados dos modelos original e transformado
print("Valores ajustados do modelo original:")
print(y_fitted_original[:5])
print("\nValores ajustados do modelo transformado:")
print(y_fitted_transformed[:5])

# ComparaÃ§Ã£o dos valores ajustados
print("\nComparaÃ§Ã£o dos valores ajustados entre os modelos:")
print(np.allclose(y_fitted_original, y_fitted_transformed))

# CÃ¡lculo da estimativa OLS para beta*
b_star = np.linalg.inv(X_star.T @ X_star) @ X_star.T @ y_original

# CÃ¡lculo do estimador para o modelo original
b_hat = G_prime @ b_star

# ExibiÃ§Ã£o das estimativas
print("Estimativas dos parÃ¢metros transformados b*:")
print(b_star)
print("\nEstimativas dos parÃ¢metros originais b:")
print(b_hat)

print("\nEstimativas do modelo transformado com statsmodels:")
print(results_transformed.params)
```
<!-- END -->
