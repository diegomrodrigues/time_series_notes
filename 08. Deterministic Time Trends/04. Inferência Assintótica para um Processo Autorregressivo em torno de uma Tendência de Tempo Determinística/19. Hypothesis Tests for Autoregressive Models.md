## Testes de HipÃ³teses em Modelos Autorregressivos com TendÃªncia DeterminÃ­stica

### IntroduÃ§Ã£o
Este capÃ­tulo aborda a aplicaÃ§Ã£o de testes de hipÃ³teses em modelos autorregressivos (AR) com tendÃªncia temporal determinÃ­stica. Como discutido anteriormente, a transformaÃ§Ã£o de regressores proposta por Sims, Stock e Watson [^1] simplifica a anÃ¡lise assintÃ³tica, separando os componentes com diferentes taxas de convergÃªncia [^2, ^3]. O foco aqui Ã© demonstrar que os testes de hipÃ³teses usuais, baseados nas estatÃ­sticas *$t$* e *$F$*, sÃ£o assintoticamente vÃ¡lidos tanto no modelo original quanto no transformado, dada a estrutura linear e a distribuiÃ§Ã£o gaussiana assintÃ³tica dos estimadores [^4]. AlÃ©m disso, exploraremos como a independÃªncia assintÃ³tica entre os estimadores facilita a implementaÃ§Ã£o desses testes.

### Conceitos Fundamentais
Retomando o modelo autorregressivo com tendÃªncia temporal determinÃ­stica [16.3.1], temos:
$$ y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t $$
onde $\epsilon_t$ Ã© um ruÃ­do branco i.i.d. com mÃ©dia zero, variÃ¢ncia $\sigma^2$ e quarto momento finito. As raÃ­zes da equaÃ§Ã£o caracterÃ­stica estÃ£o fora do cÃ­rculo unitÃ¡rio [^1]. A transformaÃ§Ã£o dos regressores leva ao modelo transformado [16.3.3]:
$$ y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \ldots + \phi_p^* y_{t-p}^* + \epsilon_t $$
onde $\alpha^*$, $\delta^*$ e $y_{t-j}^*$ sÃ£o definidos conforme anteriormente [^2]. O modelo original e o transformado podem ser expressos em forma matricial como [16.3.5, 16.3.7]:
$$ y_t = x_t'\beta + \epsilon_t \quad \text{e} \quad y_t = [x_t^*]' \beta^* + \epsilon_t $$
onde $x_t^* = G x_t$ e $\beta^* = (G')^{-1} \beta$, e $G'$ Ã© a matriz de transformaÃ§Ã£o [16.3.8]:
$$ G' = \begin{bmatrix}
1 & 0 & 0 & \ldots & 0 & 0 & 0 \\
0 & 1 & 0 & \ldots & 0 & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & 1 & 0 & 0 \\
-\alpha + \delta & -\alpha + 2\delta & \ldots & -\alpha + p\delta & 1 & 0 \\
-\delta & -\delta & \ldots & -\delta & 0 & 1
\end{bmatrix} $$
A matriz $G'$ Ã© crucial para expressar os regressores originais em termos de variÃ¡veis estacionÃ¡rias, uma constante e uma tendÃªncia temporal [^2].

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um modelo AR(1) com tendÃªncia: $y_t = \alpha + \delta t + \phi y_{t-1} + \epsilon_t$. A matriz $G'$ para este modelo Ã©:
>
>$$ G' = \begin{bmatrix} 1 & 0 & 0 \\ -\alpha + \delta & 1 & 0 \\ -\delta & 0 & 1 \end{bmatrix} $$
>
>Se os verdadeiros parÃ¢metros fossem $\alpha=2$ e $\delta=0.3$, a matriz seria:
>
>$$ G' = \begin{bmatrix} 1 & 0 & 0 \\ -1.7 & 1 & 0 \\ -0.3 & 0 & 1 \end{bmatrix} $$
>
>Essa matriz transforma os regressores originais em termos de uma variÃ¡vel estacionÃ¡ria ($y_{t-1}^*$), uma constante e uma tendÃªncia temporal. Essa transformaÃ§Ã£o simplifica a inferÃªncia assintÃ³tica.

O estimador OLS para $\beta^*$, denotado por $b^*$, Ã© dado por [16.3.11]:
$$ b^* = [\sum_{t=1}^T x_t^* x_t^{*'}]^{-1} \sum_{t=1}^T x_t^* y_t = (G')^{-1} b $$
onde $b$ Ã© o estimador OLS para o modelo original.

**Teorema 1 (DistribuiÃ§Ã£o AssintÃ³tica de $b^*$)**
Sob as condiÃ§Ãµes estabelecidas, o estimador $b^*$ converge para uma distribuiÃ§Ã£o gaussiana assintÃ³tica apÃ³s o reescalonamento adequado:
$$ Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1}) $$
onde $Y_T$ Ã© a matriz de escalonamento [16.3.14] que reflete as diferentes taxas de convergÃªncia e $Q^*$ Ã© a matriz de variÃ¢ncia assintÃ³tica dos regressores transformados [16.3.15] [^4]. A matriz $Y_T$ Ã© dada por:
$$
Y_T = \begin{bmatrix}
\sqrt{T} & 0 & 0 & \cdots & 0 & 0 \\
0 & \sqrt{T} & 0 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \cdots & \sqrt{T} & 0 \\
0 & 0 & 0 & \cdots & 0 & \sqrt{T} \\
0 & 0 & 0 & \cdots & 0 & T^{3/2}
\end{bmatrix}
$$
e a matriz $Q^*$ Ã© definida como:
$$
Q^* = \begin{bmatrix}
\gamma_{0}^* & \gamma_{1}^* & \gamma_{2}^* & \cdots & \gamma_{p-1}^* & 0 & 0 \\
\gamma_{1}^* & \gamma_{0}^* & \gamma_{1}^* & \cdots & \gamma_{p-2}^* & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
\gamma_{p-1}^* & \gamma_{p-2}^* & \gamma_{p-3}^* & \cdots & \gamma_{0}^* & 0 & 0 \\
0 & 0 & 0 & \cdots & 0 & 1 & 0 \\
0 & 0 & 0 & \cdots & 0 & 0 & 1/3
\end{bmatrix}
$$
Aqui, $\gamma_{j}^* = E(y_t^* y_{t-j}^*)$ sÃ£o as autocovariÃ¢ncias da variÃ¡vel estacionÃ¡ria $y_t^*$.

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que temos um modelo AR(1) com tendÃªncia com os seguintes parÃ¢metros verdadeiros: $\alpha = 5$, $\delta = 0.1$, $\phi = 0.7$ e $\sigma^2 = 1$. Temos uma amostra de tamanho $T=100$. Podemos calcular os elementos da matriz $Q^*$.  Sabemos que para o modelo AR(1), $\gamma_0^* = Var(y_t^*) = \frac{\sigma^2}{1-\phi^2} = \frac{1}{1 - 0.7^2} \approx 1.96$. E $\gamma_1^* = Cov(y_t^*, y_{t-1}^*) = \phi \gamma_0^* = 0.7 * 1.96 = 1.37$. Assim, $Q^*$ pode ser aproximadamente expressa como:
>
>$$
>Q^* \approx \begin{bmatrix}
>1.96 & 0 & 0 \\
>0 & 1 & 0 \\
>0 & 0 & 1/3
>\end{bmatrix}
>$$
>A matriz de escalonamento $Y_T$ seria:
>
>$$
>Y_T = \begin{bmatrix}
>10 & 0 & 0 \\
>0 & 10 & 0 \\
>0 & 0 & 1000
>\end{bmatrix}
>$$
>
>Note que a matriz $Q^*$ reflete a estrutura de covariÃ¢ncia dos regressores transformados.

**Lema 1.1** (ConvergÃªncia do Estimador da VariÃ¢ncia)
O estimador da variÃ¢ncia dos erros $\hat{\sigma}^2$ converge em probabilidade para a verdadeira variÃ¢ncia $\sigma^2$.
*Proof:*
I. O estimador $\hat{\sigma}^2$ Ã© definido como $\frac{1}{T-k}\sum_{t=1}^T \hat{\epsilon}_t^2$, onde $\hat{\epsilon}_t$ sÃ£o os resÃ­duos do modelo e $k$ Ã© o nÃºmero de parÃ¢metros.
II. Sob as condiÃ§Ãµes do Teorema 1, os resÃ­duos $\hat{\epsilon}_t$ convergem para os erros $\epsilon_t$.
III.  Usando a lei dos grandes nÃºmeros, a mÃ©dia amostral $\frac{1}{T}\sum_{t=1}^T \hat{\epsilon}_t^2$ converge em probabilidade para $E[\epsilon_t^2] = \sigma^2$.
IV. Portanto, $\hat{\sigma}^2$ converge para $\sigma^2$ em probabilidade.
$\blacksquare$

### Testes de HipÃ³teses: Validade AssintÃ³tica
Uma propriedade notÃ¡vel desta transformaÃ§Ã£o Ã© que os testes de hipÃ³teses usuais, como o teste *$t$* para uma Ãºnica restriÃ§Ã£o linear ou o teste *$F$* para restriÃ§Ãµes lineares mÃºltiplas, sÃ£o assintoticamente vÃ¡lidos, tanto no modelo original quanto no transformado [^1]. Isso ocorre porque a transformaÃ§Ã£o de Sims, Stock e Watson preserva as propriedades fundamentais dos estimadores OLS, como a linearidade e a distribuiÃ§Ã£o gaussiana assintÃ³tica.

**Teste t para uma Ãšnica RestriÃ§Ã£o Linear**
Considere o teste de hipÃ³tese nula $H_0: R\beta = r$, onde $R$ Ã© um vetor de restriÃ§Ã£o e $r$ Ã© um escalar. A estatÃ­stica *$t$* para testar essa hipÃ³tese no modelo original Ã© dada por [16.2.1]:
$$ t = \frac{R\hat{\beta} - r}{\sqrt{\hat{\sigma}^2 R(\sum x_t x_t')^{-1} R'}} $$
onde $\hat{\beta}$ Ã© o estimador OLS de $\beta$ e $\hat{\sigma}^2$ Ã© o estimador da variÃ¢ncia dos erros. Equivalentemente, podemos obter a estatÃ­stica $t$ para o modelo transformado, substituindo $x_t$, $\hat{\beta}$ e $\sigma^2$ pelas suas versÃµes transformadas, $x_t^*$, $\hat{\beta}^*$ e $\hat{\sigma}^{*2}$.
De acordo com os resultados do capÃ­tulo 16, a estatÃ­stica *$t$* Ã© assintoticamente distribuÃ­da como uma normal padrÃ£o $N(0,1)$, sob a hipÃ³tese nula, independentemente de aplicarmos o teste no modelo original ou no modelo transformado [^1].

> ğŸ’¡ **Exemplo NumÃ©rico:**  Considere um teste para a hipÃ³tese $H_0: \phi = 0.7$ em um modelo AR(1) com tendÃªncia: $y_t = \alpha + \delta t + \phi y_{t-1} + \epsilon_t$.  Suponha que ajustamos um modelo aos dados, obtendo as seguintes estimativas: $\hat{\phi} = 0.75$, $\hat{\sigma}^2 = 0.9$, $T=100$, e $\sum_{t=2}^T (y_{t-1} - \bar{y_{-1}})^2 = 200$. O teste $t$ seria dado por:
>
> $$ t = \frac{0.75 - 0.7}{\sqrt{0.9 * (200)^{-1}}} = \frac{0.05}{\sqrt{0.0045}} \approx \frac{0.05}{0.067} \approx 0.746 $$
>
>Onde $\bar{y_{-1}}$ Ã© a mÃ©dia amostral da variÃ¡vel defasada.  O valor-p desse teste, usando uma distribuiÃ§Ã£o normal, Ã© aproximadamente 0.45. Portanto, nÃ£o rejeitamos a hipÃ³tese nula a um nÃ­vel de significÃ¢ncia de 5%.
>
> Usando a distribuiÃ§Ã£o assintÃ³tica dos estimadores, temos:
>
> $$ \frac{\hat{\phi} - 0.7}{\sqrt{\frac{\sigma^2 \gamma_0^*}{T}}} \xrightarrow{d} N(0,1) $$
>
>Assim, podemos construir um intervalo de confianÃ§a com base nos quantis de uma distribuiÃ§Ã£o normal. Para este exemplo, o intervalo de confianÃ§a de 95% para $\phi$ seria aproximadamente $0.75 \pm 1.96 * \sqrt{\frac{0.9*1.96}{100}} \approx 0.75 \pm 0.26$.

**Teste F para MÃºltiplas RestriÃ§Ãµes Lineares**
Considere o teste de hipÃ³tese nula $H_0: R\beta = r$, onde $R$ Ã© uma matriz de restriÃ§Ãµes e $r$ Ã© um vetor. A estatÃ­stica *$F$* para testar essa hipÃ³tese Ã© dada por [16.3.19]:
$$ \chi^2 = (R\hat{\beta} - r)' [R(\sum x_t x_t')^{-1} R']^{-1} (R\hat{\beta} - r) $$
Esta estatÃ­stica, quando os erros sÃ£o normais, segue uma distribuiÃ§Ã£o $\chi^2$ com *$m$* graus de liberdade, onde *$m$* Ã© o nÃºmero de restriÃ§Ãµes. No caso assintÃ³tico, a estatÃ­stica converge para uma distribuiÃ§Ã£o $\chi^2$ com *$m$* graus de liberdade sob a hipÃ³tese nula. Novamente, este resultado Ã© vÃ¡lido tanto no modelo original quanto no transformado, ou seja, calculando $x_t$, $\hat{\beta}$ e $\sigma^2$ no modelo original, ou calculando $x_t^*$, $\hat{\beta}^*$ e $\sigma^{*2}$ no modelo transformado, os resultados seriam assintoticamente idÃªnticos.

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que desejamos testar a hipÃ³tese conjunta $H_0: \alpha = 1$ e $\delta = 0.5$ em um modelo AR(1) com tendÃªncia. Suponha que ajustamos o modelo e obtemos as estimativas $\hat{\alpha} = 1.2$, $\hat{\delta} = 0.55$ e a matriz de covariÃ¢ncia do estimador $\hat{\beta}$ como $[\sum x_t x_t']^{-1} \hat{\sigma}^2 = \begin{bmatrix} 0.01 & 0.002 & 0.0001\\ 0.002 & 0.005 & 0.0002 \\ 0.0001 & 0.0002 & 0.001\end{bmatrix}$. A matriz $R = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix}$ e $r = \begin{bmatrix} 1 \\ 0.5 \end{bmatrix}$. A estatÃ­stica $\chi^2$ seria:
>
> $$ \chi^2 = (R\hat{\beta} - r)' [R(\sum x_t x_t')^{-1} R']^{-1} (R\hat{\beta} - r) $$
>
>Primeiro, calculamos $R\hat{\beta} - r = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} 1.2 \\ 0.55 \\ 0.7 \end{bmatrix} - \begin{bmatrix} 1 \\ 0.5 \end{bmatrix} = \begin{bmatrix} 0.2 \\ 0.05 \end{bmatrix}$.
>
>Segundo, calculamos $R(\sum x_t x_t')^{-1} R' = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} 0.01 & 0.002 & 0.0001\\ 0.002 & 0.005 & 0.0002 \\ 0.0001 & 0.0002 & 0.001\end{bmatrix}  \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 0.01 & 0.002 \\ 0.002 & 0.005\end{bmatrix}$.
>
>Terceiro, calculamos a inversa: $[R(\sum x_t x_t')^{-1} R']^{-1} = \begin{bmatrix} 0.01 & 0.002 \\ 0.002 & 0.005\end{bmatrix}^{-1} = \begin{bmatrix} 125 & -50 \\ -50 & 250\end{bmatrix}$
>
>Finalmente, calculamos  $\chi^2 = \begin{bmatrix} 0.2 & 0.05 \end{bmatrix} \begin{bmatrix} 125 & -50 \\ -50 & 250\end{bmatrix} \begin{bmatrix} 0.2 \\ 0.05 \end{bmatrix} = \begin{bmatrix} 22.5 & 2.5 \end{bmatrix} \begin{bmatrix} 0.2 \\ 0.05 \end{bmatrix} = 4.5 + 0.125 = 4.625$
>
> Note que, sob a hipÃ³tese nula, a estatÃ­stica se aproxima de uma distribuiÃ§Ã£o $\chi^2$ com dois graus de liberdade. O valor-p correspondente para um teste $\chi^2$ com 2 graus de liberdade Ã© de aproximadamente 0.099, o que significa que, a um nÃ­vel de significÃ¢ncia de 5%, nÃ£o rejeitamos a hipÃ³tese nula. Analogamente, um teste  para $H_0: \alpha = 1$ e $\phi = 0.8$ seguiria a mesma metodologia.

**Teorema 2 (Validade AssintÃ³tica dos Testes)**
Os testes *$t$* e *$F$* calculados com base nos estimadores OLS do modelo original ou do modelo transformado sÃ£o assintoticamente vÃ¡lidos, ou seja, sob a hipÃ³tese nula, as estatÃ­sticas convergem para as distribuiÃ§Ãµes usuais (normal padrÃ£o e qui-quadrado, respectivamente).
*Proof:*
I. Os estimadores de mÃ­nimos quadrados ordinÃ¡rios do modelo original e do modelo transformado sÃ£o relacionados atravÃ©s da matriz $G'$, isto Ã©, $b = G'b^*$.
II. A matriz de covariÃ¢ncia assintÃ³tica dos estimadores transformados, dada por $\sigma^2 [Q^*]^{-1}$, Ã© usada para derivar as distribuiÃ§Ãµes limite dos estimadores.
III. Tanto o modelo original como o transformado preservam a linearidade dos estimadores em relaÃ§Ã£o aos regressores transformados, uma vez que $b = G'b^*$
IV. Como os estimadores dos dois modelos tem as mesmas propriedades assintÃ³ticas, e os testes t e F sÃ£o baseados nessas propriedades, entÃ£o os testes t e F sÃ£o validos em ambos os modelos.
V. Portanto, tanto as estatÃ­sticas *$t$* quanto as estatÃ­sticas *$F$* calculadas com os estimadores do modelo original convergem para as mesmas distribuiÃ§Ãµes assintÃ³ticas que aquelas calculadas com os estimadores transformados.
$\blacksquare$

**Teorema 2.1** (InvariÃ¢ncia AssintÃ³tica dos Testes Sob TransformaÃ§Ãµes Lineares)
Se os testes *$t$* e *$F$* sÃ£o assintoticamente vÃ¡lidos para um modelo linear, entÃ£o eles tambÃ©m sÃ£o assintoticamente vÃ¡lidos para qualquer transformaÃ§Ã£o linear dos regressores, desde que a transformaÃ§Ã£o seja invertÃ­vel.
*Proof:*
I. Seja o modelo original $y = X\beta + \epsilon$ e o modelo transformado $y = X^* \beta^* + \epsilon$, onde $X^* = XG$ e $\beta^* = G^{-1}\beta$.
II.  As estatÃ­sticas de teste *$t$* e *$F$* sÃ£o funÃ§Ãµes dos estimadores OLS e suas matrizes de covariÃ¢ncia.
III.  A transformaÃ§Ã£o linear preserva a linearidade dos estimadores.
IV.  Se as distribuiÃ§Ãµes assintÃ³ticas dos estimadores OLS em ambos os modelos sÃ£o gaussianas, entÃ£o as estatÃ­sticas *$t$* e *$F$* convergirÃ£o para suas distribuiÃ§Ãµes limite padrÃ£o, independentemente da transformaÃ§Ã£o linear utilizada.
V. A invertibilidade da transformaÃ§Ã£o garante a equivalÃªncia dos dois modelos e, portanto, a equivalÃªncia assintÃ³tica das estatÃ­sticas de teste.
$\blacksquare$

Este teorema Ã© de suma importÃ¢ncia, pois ele garante a flexibilidade de escolha de qual modelo usar para as anÃ¡lises estatÃ­sticas, sem comprometer a validade assintÃ³tica das inferÃªncias.

**ObservaÃ§Ã£o 1** A validade assintÃ³tica dos testes *$t$* e *$F$* tanto no modelo original quanto no transformado Ã© uma consequÃªncia da linearidade dos estimadores e da sua distribuiÃ§Ã£o gaussiana assintÃ³tica, apÃ³s o devido reescalonamento. A transformaÃ§Ã£o de Sims, Stock e Watson preserva essas propriedades.

### IndependÃªncia AssintÃ³tica e Testes de HipÃ³teses
A independÃªncia assintÃ³tica entre os estimadores dos parÃ¢metros estacionÃ¡rios e o parÃ¢metro da tendÃªncia temporal, estabelecida em capÃ­tulos anteriores, Ã© fundamental para a construÃ§Ã£o e interpretaÃ§Ã£o dos testes de hipÃ³teses. Como esses parÃ¢metros convergem em taxas distintas e sÃ£o assintoticamente independentes, a anÃ¡lise estatÃ­stica pode ser simplificada.

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que desejamos testar a hipÃ³tese conjunta $H_0: \phi_1 = 0.7 \text{ e } \delta=0.5$ no modelo AR(1) com tendÃªncia. Dada a independÃªncia assintÃ³tica, podemos construir testes separados para cada hipÃ³tese, calculando os intervalos de confianÃ§a de cada um. Isso Ã© facilitado pela estrutura bloco diagonal da matriz $Q^*$.
>
> Suponha que as estimativas obtidas foram: $\hat{\phi} = 0.75$, $\hat{\delta} = 0.48$, $\hat{\sigma}^2 = 0.9$,  $\gamma_0^* \approx 1.96$,  $T = 100$ e que o elemento relevante da inversa da matriz de covariÃ¢ncia assintÃ³tica para $\delta$ Ã© $g_{\delta} [Q^*]^{-1} g_{\delta}^{'} \approx 3$. A estatÃ­stica para testar a primeira hipÃ³tese Ã©:
>
> $$ z_1 = \frac{0.75 - 0.7}{\sqrt{\frac{0.9 * 1.96}{100}}} \approx \frac{0.05}{0.133}  \approx 0.376 \xrightarrow{d} N(0,1) $$
>
>E para testar a segunda hipÃ³tese:
>
>$$ z_2 = \frac{0.48 - 0.5}{\sqrt{\frac{0.9 * 3}{100^3}}} \approx \frac{-0.02}{0.0052} \approx -3.846   \xrightarrow{d} N(0,1) $$
>
>onde os estimadores sÃ£o obtidos a partir do modelo OLS. O valor-p associado ao primeiro teste Ã© de aproximadamente 0.707 e, portanto, nÃ£o hÃ¡ evidÃªncia para rejeitar a hipÃ³tese nula de que $\phi_1 = 0.7$. Para o segundo teste, o valor-p Ã© muito pequeno (<0.001), o que significa que rejeitamos a hipÃ³tese nula de que $\delta = 0.5$. A independÃªncia assintÃ³tica permite analisar esses testes separadamente, facilitando a interpretaÃ§Ã£o dos resultados.

**Lema 1 (IndependÃªncia dos estimadores)**
Se o modelo Ã© transformado usando o mÃ©todo de Sims, Stock e Watson, entÃ£o os estimadores dos parÃ¢metros autoregressivos, intercepto e tendÃªncia temporal sÃ£o assintoticamente independentes.
*Proof:*
I. Os estimadores da parte autoregressiva, intercepto e tendÃªncia temporal sÃ£o obtidos por combinaÃ§Ã£o linear dos regressores transformados.
II. A matriz de covariÃ¢ncia assintÃ³tica $Q^*$ tem uma estrutura bloco diagonal. O bloco superior corresponde aos regressores autorregressivos e o bloco inferior corresponde aos parÃ¢metros da tendÃªncia temporal.
III.  Quando combinados, esses resultados implicam a independÃªncia assintÃ³tica dos estimadores do intercepto e da tendÃªncia temporal em relaÃ§Ã£o aos estimadores dos parÃ¢metros autorregressivos.
$\blacksquare$

**CorolÃ¡rio 1.1** (Testes Separados)
Devido Ã  independÃªncia assintÃ³tica dos estimadores, os testes de hipÃ³teses sobre os parÃ¢metros autoregressivos e os parÃ¢metros de tendÃªncia podem ser realizados separadamente.
*Proof:*
I. A independÃªncia assintÃ³tica implica que a covariÃ¢ncia entre os estimadores dos parÃ¢metros autoregressivos e os parÃ¢metros da tendÃªncia converge para zero.
II.  Isso permite a construÃ§Ã£o de testes independentes com base nas distribuiÃ§Ãµes assintÃ³ticas de cada conjunto de estimadores.
III. A matriz de covariÃ¢ncia Ã© bloco diagonal, simplificando a anÃ¡lise.
$\blacksquare$

### ConclusÃ£o

Este capÃ­tulo demonstrou que os testes de hipÃ³teses usuais (testes *$t$* e *$F$*) sÃ£o assintoticamente vÃ¡lidos tanto no modelo original quanto no transformado para modelos autorregressivos com tendÃªncia temporal determinÃ­stica. A transformaÃ§Ã£o de Sims, Stock e Watson preserva a linearidade e a distribuiÃ§Ã£o gaussiana assintÃ³tica dos estimadores, o que garante a validade desses testes. AlÃ©m disso, a independÃªncia assintÃ³tica entre os estimadores dos parÃ¢metros estacionÃ¡rios e o parÃ¢metro da tendÃªncia simplifica a construÃ§Ã£o e interpretaÃ§Ã£o dos resultados dos testes. O framework teÃ³rico e os resultados apresentados neste capÃ­tulo sÃ£o cruciais para a inferÃªncia estatÃ­stica em modelos de sÃ©ries temporais com componentes autorregressivos e tendÃªncias determinÃ­sticas, fornecendo as bases para a anÃ¡lise de modelos mais complexos que serÃ£o abordados posteriormente.

### ReferÃªncias
[^1]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113â€“44.
[^2]: SeÃ§Ã£o 16.3 do texto original.
[^3]: SeÃ§Ã£o 16.1 do texto original.
[^4]: SeÃ§Ã£o 16.3 do texto original.
<!-- END -->
