## InferÃªncia AssintÃ³tica para um Processo Autorregressivo em Torno de uma TendÃªncia de Tempo DeterminÃ­stica: Uma AnÃ¡lise Detalhada

### IntroduÃ§Ã£o
Este capÃ­tulo aprofunda a inferÃªncia assintÃ³tica em modelos autorregressivos (AR) com tendÃªncia temporal determinÃ­stica, explorando a transformaÃ§Ã£o de regressores proposta por Sims, Stock e Watson [^1]. Como vimos anteriormente [^2], essa transformaÃ§Ã£o separa componentes com diferentes taxas de convergÃªncia, o que Ã© crucial para a anÃ¡lise da distribuiÃ§Ã£o assintÃ³tica dos estimadores de MÃ­nimos Quadrados OrdinÃ¡rios (OLS). O foco aqui Ã© detalhar a aplicaÃ§Ã£o desses conceitos e fornecer uma base sÃ³lida para testes de hipÃ³teses e inferÃªncias estatÃ­sticas em modelos de sÃ©ries temporais com tendÃªncias e componentes autorregressivos.

### Conceitos Fundamentais
Retomando o modelo autorregressivo geral com tendÃªncia temporal determinÃ­stica [16.3.1], temos:
$$ y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t $$
onde $\epsilon_t$ Ã© um ruÃ­do branco i.i.d. com mÃ©dia zero, variÃ¢ncia $\sigma^2$ e quarto momento finito. As raÃ­zes da equaÃ§Ã£o caracterÃ­stica $1 - \phi_1 z - \phi_2 z^2 - \ldots - \phi_p z^p = 0$ estÃ£o fora do cÃ­rculo unitÃ¡rio [^1].

A transformaÃ§Ã£o chave Ã© a reescrita do modelo [16.3.2]:
$$ y_t = \alpha(1 + \phi_1 + \phi_2 + \ldots + \phi_p) + \delta(1 + \phi_1 + 2\phi_2 + \ldots + p\phi_p) + \delta(\phi_1 + 2\phi_2 + \ldots + p\phi_p) + \phi_1[y_{t-1} - \alpha - \delta(t-1)] + \phi_2[y_{t-2} - \alpha - \delta(t-2)] + \ldots + \phi_p[y_{t-p} - \alpha - \delta(t-p)] + \epsilon_t $$
que leva ao modelo transformado [16.3.3]:
$$ y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \ldots + \phi_p^* y_{t-p}^* + \epsilon_t $$
onde $\alpha^*$, $\delta^*$ e $y_{t-j}^*$ sÃ£o definidos como:
$$ \alpha^* = \alpha(1 + \phi_1 + \phi_2 + \ldots + \phi_p) - \delta(\phi_1 + 2\phi_2 + \ldots + p\phi_p) $$
$$ \delta^* = \delta(1 + \phi_1 + \phi_2 + \ldots + \phi_p) $$
e
$$ y_{t-j}^* = y_{t-j} - \alpha - \delta(t-j), \quad j = 1, 2, \ldots, p $$
Essa transformaÃ§Ã£o isola os componentes com diferentes taxas de convergÃªncia: os termos $y_{t-j}^*$ sÃ£o variÃ¡veis estacionÃ¡rias de mÃ©dia zero, enquanto os termos constante e de tendÃªncia temporal convergem em taxas distintas. Esta separaÃ§Ã£o Ã© essencial para facilitar a anÃ¡lise assintÃ³tica, como observado anteriormente [^2].

> ðŸ’¡ **Exemplo NumÃ©rico:** Considere um modelo AR(1) com tendÃªncia linear: $y_t = \alpha + \delta t + \phi y_{t-1} + \epsilon_t$. Suponha que os parÃ¢metros verdadeiros sejam $\alpha = 2$, $\delta = 0.5$ e $\phi = 0.8$. EntÃ£o, a transformaÃ§Ã£o resulta em:
> $$ y_t = \alpha(1+\phi) + \delta(1+\phi)t - \delta\phi + \phi[y_{t-1} - \alpha - \delta(t-1)] + \epsilon_t$$
> $$ y_t = \alpha^* + \delta^* t + \phi y_{t-1}^* + \epsilon_t$$
> Onde:
> $$\alpha^* = 2(1+0.8) - 0.5(0.8) = 3.6 - 0.4 = 3.2$$
> $$\delta^* = 0.5(1+0.8) = 0.9$$
> $$ y_{t-1}^* = y_{t-1} - 2 - 0.5(t-1)$$
> Assim, o modelo transformado serÃ¡:
> $$ y_t = 3.2 + 0.9t + 0.8y_{t-1}^* + \epsilon_t $$
> Este exemplo ilustra como a transformaÃ§Ã£o separa a parte estacionÃ¡ria ($y_{t-1}^*$) da tendÃªncia determinÃ­stica.
O modelo original [16.3.5]:
$$ y_t = x_t'\beta + \epsilon_t $$
Ã© transformado em [16.3.7]:
$$ y_t = x_t G' (G')^{-1} \beta + \epsilon_t = [x_t^*]' \beta^* + \epsilon_t $$
onde $x_t^* = G x_t$ e $\beta^* = (G')^{-1} \beta$. A matriz $G'$ Ã© definida como [16.3.8]:
$$ G' = \begin{bmatrix}
1 & 0 & 0 & \ldots & 0 & 0 & 0 \\
0 & 1 & 0 & \ldots & 0 & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & 1 & 0 & 0 \\
-\alpha + \delta & -\alpha + 2\delta & \ldots & -\alpha + p\delta & 1 & 0 \\
-\delta & -\delta & \ldots & -\delta & 0 & 1
\end{bmatrix} $$
A matriz $G'$ Ã© crucial para expressar os regressores originais em termos de variÃ¡veis estacionÃ¡rias, uma constante e uma tendÃªncia temporal [^2].

O estimador OLS para $\beta^*$ Ã© dado por [16.3.11]:
$$ b^* = [ \sum_{t=1}^T x_t^* x_t^{*'} ]^{-1} \sum_{t=1}^T x_t^* y_t = (G')^{-1} b $$
onde $b$ Ã© o estimador OLS para o modelo original.

### DistribuiÃ§Ã£o AssintÃ³tica dos Estimadores OLS Transformados
A distribuiÃ§Ã£o assintÃ³tica de $b^*$ Ã© dada pelo teorema [16.3.13]:
$$ Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1}) $$
onde $Y_T$ Ã© a matriz de escalonamento [16.3.14]:
$$
Y_T = \begin{bmatrix}
\sqrt{T} & 0 & 0 & \cdots & 0 & 0 \\
0 & \sqrt{T} & 0 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \cdots & \sqrt{T} & 0 \\
0 & 0 & 0 & \cdots & 0 & \sqrt{T} \\
0 & 0 & 0 & \cdots & 0 & T^{3/2}
\end{bmatrix}
$$
e $Q^*$ Ã© a matriz de variÃ¢ncia assintÃ³tica dos regressores transformados [16.3.15]:
$$
Q^* = \begin{bmatrix}
\gamma_{0}^* & \gamma_{1}^* & \gamma_{2}^* & \cdots & \gamma_{p-1}^* & 0 & 0 \\
\gamma_{1}^* & \gamma_{0}^* & \gamma_{1}^* & \cdots & \gamma_{p-2}^* & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
\gamma_{p-1}^* & \gamma_{p-2}^* & \gamma_{p-3}^* & \cdots & \gamma_{0}^* & 0 & 0 \\
0 & 0 & 0 & \cdots & 0 & 1 & 0 \\
0 & 0 & 0 & \cdots & 0 & 0 & 1/3
\end{bmatrix}
$$
Aqui, $\gamma_{j}^* = E(y_t^* y_{t-j}^*)$ sÃ£o as autocovariÃ¢ncias da variÃ¡vel estacionÃ¡ria $y_t^*$. A matriz $Y_T$ reflete as diferentes taxas de convergÃªncia: $\sqrt{T}$ para os coeficientes das variÃ¡veis estacionÃ¡rias e $T^{3/2}$ para o coeficiente da tendÃªncia temporal [^1].

> ðŸ’¡ **Exemplo NumÃ©rico:** Considere novamente o modelo AR(1) com tendÃªncia. Suponha que as autocovariÃ¢ncias de $y_t^*$ sejam $\gamma_0^* = 1.5$ e que a variÃ¢ncia do erro seja $\sigma^2=0.8$. A matriz $Q^*$ e sua inversa sÃ£o:
>
>$$ Q^* = \begin{bmatrix} 1.5 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1/3 \end{bmatrix} \quad \text{e} \quad [Q^*]^{-1} = \begin{bmatrix} 2/3 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 3 \end{bmatrix} $$
>
>A matriz de escalonamento $Y_T$ Ã©:
>
>$$Y_T = \begin{bmatrix} \sqrt{T} & 0 & 0 \\ 0 & \sqrt{T} & 0 \\ 0 & 0 & T^{3/2} \end{bmatrix} $$
>
>A distribuiÃ§Ã£o assintÃ³tica dos estimadores transformados, apÃ³s o devido escalonamento, Ã©:
>$$Y_T(b^* - \beta^*) \xrightarrow{d} N(0, 0.8 [Q^*]^{-1}) = N(0, \begin{bmatrix} 16/30 & 0 & 0 \\ 0 & 0.8 & 0 \\ 0 & 0 & 2.4 \end{bmatrix}) $$
>
>Isso significa que a variÃ¢ncia do estimador de $\phi$ Ã© $\frac{16}{30T}$, do intercepto $\frac{0.8}{T}$, e da tendÃªncia $\frac{2.4}{T^3}$. Observe que a variÃ¢ncia do estimador da tendÃªncia decresce mais rapidamente do que as demais.

A prova deste resultado Ã© baseada na anÃ¡lise do comportamento assintÃ³tico dos regressores transformados e no Teorema do Limite Central. O ApÃªndice 16.A fornece uma demonstraÃ§Ã£o detalhada, mostrando que a matriz $Q^*$ Ã© o limite da esperanÃ§a da matriz de regressores, e que os estimadores dos parÃ¢metros do modelo transformado convergem para uma distribuiÃ§Ã£o normal com mÃ©dia centrada no valor verdadeiro e variÃ¢ncia dada por $\sigma^2 [Q^*]^{-1}$ apÃ³s a devida escalonagem.

**Lema 2.1 (Positividade da matriz Q\*)**
A matriz $Q^*$ definida em [16.3.15] Ã© positiva definida sob as condiÃ§Ãµes estabelecidas, garantindo que a matriz de covariÃ¢ncia assintÃ³tica dos estimadores seja bem definida e positiva definida.
*Proof:*
I. A matriz $Q^*$ Ã© composta pelas autocovariÃ¢ncias do processo estacionÃ¡rio $y_t^*$ e elementos diagonais correspondentes ao intercepto e Ã  tendÃªncia.
II. A submatriz contendo as autocovariÃ¢ncias de $y_t^*$ Ã© positiva definida porque Ã© a matriz de covariÃ¢ncia de um processo estacionÃ¡rio, desde que o processo nÃ£o seja determinÃ­stico.
III. Os elementos diagonais correspondentes ao intercepto e Ã  tendÃªncia tambÃ©m sÃ£o estritamente positivos (1 e 1/3, respectivamente).
IV. Uma vez que $Q^*$ Ã© uma matriz bloco-diagonal com blocos positivos definidos, ela prÃ³pria Ã© positiva definida.
â– 

**Lema 2.2 (Inversibilidade de Q\*)**
Sob as mesmas condiÃ§Ãµes do Lema 2.1, a matriz $Q^*$ Ã© tambÃ©m invertÃ­vel, o que permite o cÃ¡lculo da matriz de covariÃ¢ncia assintÃ³tica dos estimadores.
*Proof:*
I. Pelo Lema 2.1, a matriz $Q^*$ Ã© positiva definida.
II. Uma matriz positiva definida Ã© sempre invertÃ­vel.
III. Portanto, a matriz $Q^*$ Ã© invertÃ­vel.
â– 
Este resultado garante que a variÃ¢ncia assintÃ³tica dos estimadores, dada por $\sigma^2 [Q^*]^{-1}$, estÃ¡ bem definida.

### DistribuiÃ§Ã£o AssintÃ³tica dos Estimadores OLS do Modelo Original
Para obter a distribuiÃ§Ã£o assintÃ³tica dos estimadores do modelo original, $b$, aplicamos a relaÃ§Ã£o $b = G' b^*$ [16.3.12], resultando em:
$$ Y_T(b-\beta) = Y_T G' (b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 G' [Q^*]^{-1} G'^{'}) $$
onde $\beta$ Ã© o vetor de coeficientes do modelo original.

As distribuiÃ§Ãµes assintÃ³ticas dos estimadores de $\alpha$ e $\delta$ sÃ£o dadas por [16.3.17]:
$$ \sqrt{T}(\hat{\alpha} - \alpha) \xrightarrow{d} N(0, \sigma^2 g_{\alpha} [Q^*]^{-1} g_{\alpha}^{'}) $$
$$ T^{3/2}(\hat{\delta} - \delta) \xrightarrow{d} N(0, \sigma^2 g_{\delta} [Q^*]^{-1} g_{\delta}^{'}) $$
onde $g_{\alpha}$ e $g_{\delta}$ sÃ£o vetores de pesos que isolam os componentes $\alpha$ e $\delta$ no vetor $b$, respectivamente.

> ðŸ’¡ **Exemplo NumÃ©rico (continuaÃ§Ã£o):**
> Retomando o exemplo anterior do modelo AR(1) com tendÃªncia, temos $g_{\alpha} = [-\alpha + \delta, 1, 0] = [-2 + 0.5, 1, 0] = [-1.5, 1, 0]$ e $g_{\delta} = [-\delta, 0, 1] = [-0.5, 0, 1]$.  Utilizando $\sigma^2=0.8$ e $[Q^*]^{-1}$ como calculado anteriormente, temos:
>
>$\text{Var}(\sqrt{T}(\hat{\alpha}-\alpha)) = 0.8 \times g_{\alpha} [Q^*]^{-1} g_{\alpha}' =  0.8 \times ([-1.5, 1, 0] \begin{bmatrix} 2/3 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 3 \end{bmatrix} [-1.5, 1, 0]^T) = 0.8 \times (\frac{2}{3} \times 2.25 + 1) = 0.8 \times (1.5 + 1) = 2$
>
>$\text{Var}(T^{3/2}(\hat{\delta}-\delta)) = 0.8 \times g_{\delta} [Q^*]^{-1} g_{\delta}' = 0.8 \times ([-0.5, 0, 1] \begin{bmatrix} 2/3 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 3 \end{bmatrix} [-0.5, 0, 1]^T) = 0.8 \times (\frac{2}{3} \times 0.25 + 3) = 0.8 \times (\frac{1}{6} + 3) = 2.466...$
>
>Assim, $\sqrt{T}(\hat{\alpha}-\alpha)$ converge para uma normal com mÃ©dia 0 e variÃ¢ncia 2, e $T^{3/2}(\hat{\delta}-\delta)$ converge para uma normal com mÃ©dia 0 e variÃ¢ncia 2.466.... Note que as variÃ¢ncias dos estimadores escalonados indicam a velocidade com que a incerteza diminui com o tamanho da amostra.

**Teorema 3 (IndependÃªncia AssintÃ³tica)**
Os estimadores dos parÃ¢metros estacionÃ¡rios, que convergem a uma taxa $\sqrt{T}$, sÃ£o assintoticamente independentes do estimador do parÃ¢metro da tendÃªncia, que converge a uma taxa $T^{3/2}$ [^1].
*Proof:*
I. De [16.3.16] e [16.3.17], o estimador da tendÃªncia temporal $\delta_T$ Ã© dado por
$$ \delta_T = g_{\delta} b^* = g_{\delta} (Y_T^{-1} Y_T  b^*)  \approx g_{\delta} Y_T^{-1} N( \beta^*,  \sigma^2 Q^{*-1})  $$
II. Da mesma forma, o estimador do intercepto $\alpha_T$ Ã© dado por
$$ \alpha_T = g_{\alpha} b^* = g_{\alpha} (Y_T^{-1} Y_T  b^*)  \approx g_{\alpha} Y_T^{-1} N( \beta^*,  \sigma^2 Q^{*-1})  $$
III. A matriz $Q^*$ de [16.3.15] tem uma estrutura bloco diagonal. O bloco superior contÃ©m as covariÃ¢ncias dos componentes AR estacionÃ¡rios, e o bloco inferior contÃ©m as variÃ¢ncias da constante e da tendÃªncia. Esta estrutura implica que a covariÃ¢ncia entre os erros que multiplicam os componentes AR estacionÃ¡rios e os erros que multiplicam a constante e a tendÃªncia Ã© zero.
IV. Assim, as distribuiÃ§Ãµes limite dos estimadores sÃ£o assintoticamente independentes, o que implica que os prÃ³prios estimadores tambÃ©m sÃ£o assintoticamente independentes.
â– 

**CorolÃ¡rio 3.1 (IndependÃªncia AssintÃ³tica dos ParÃ¢metros AR)**
Sob as mesmas condiÃ§Ãµes do Teorema 3, os estimadores dos parÃ¢metros autorregressivos ($\phi_1, \phi_2, \dots, \phi_p$) sÃ£o assintoticamente independentes entre si e assintoticamente independentes dos estimadores do intercepto e da tendÃªncia [^1].
*Proof:*
I. A estrutura bloco diagonal de $Q^*$ implica que a covariÃ¢ncia assintÃ³tica entre os estimadores de coeficientes AR distintos Ã© zero.
II. A estrutura da matriz $Y_T$ introduz fatores de escala que garantem uma distribuiÃ§Ã£o assintÃ³tica bem definida.
III. Esta diagonalidade de bloco, conforme mostrado na prova do Teorema 3, resulta em independÃªncia assintÃ³tica.
IV. Portanto, os estimadores dos parÃ¢metros AR, intercepto e tendÃªncia sÃ£o todos assintoticamente independentes.
â– 
Este corolÃ¡rio Ã© uma consequÃªncia direta da estrutura bloco diagonal da matriz $Q^*$. Fornece simplificaÃ§Ãµes cruciais para a inferÃªncia estatÃ­stica, permitindo a anÃ¡lise separada dos parÃ¢metros autorregressivos individuais.

**ProposiÃ§Ã£o 3.2 (ConvergÃªncia em Probabilidade)**
AlÃ©m da convergÃªncia em distribuiÃ§Ã£o, os estimadores OLS transformados $b^*$ convergem em probabilidade para seus valores verdadeiros $\beta^*$. Ou seja, $b^* \xrightarrow{p} \beta^*$.
*Proof:*
I. Pelo teorema [16.3.13], temos que $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$.
II. A convergÃªncia em distribuiÃ§Ã£o para uma constante (neste caso, 0) implica convergÃªncia em probabilidade para essa constante. Ou seja, $Y_T(b^* - \beta^*) \xrightarrow{p} 0$.
III. Multiplicando ambos os lados por $Y_T^{-1}$, temos que $b^* - \beta^* \xrightarrow{p} 0$, o que implica que $b^* \xrightarrow{p} \beta^*$.
â– 
Esta proposiÃ§Ã£o Ã© um resultado importante, pois garante que os estimadores convergem para seus verdadeiros valores Ã  medida que o tamanho da amostra aumenta.

### ImplicaÃ§Ãµes para Testes de HipÃ³teses
A independÃªncia assintÃ³tica e as taxas de convergÃªncia diferentes implicam que testes de hipÃ³teses podem ser conduzidos de forma mais eficiente. Por exemplo, a hipÃ³tese de que um dado parÃ¢metro autorregressivo Ã© igual a um valor especÃ­fico pode ser testada sem considerar a presenÃ§a da tendÃªncia temporal, dada a independÃªncia assintÃ³tica.

> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos supor que desejamos testar a hipÃ³tese nula $H_0: \phi = 0.8$ contra a alternativa $H_1: \phi \neq 0.8$ em um modelo AR(1) com tendÃªncia.  Usando a distribuiÃ§Ã£o assintÃ³tica de $\hat{\phi}$ e a independÃªncia assintÃ³tica, podemos construir um teste estatÃ­stico utilizando:
> $$ \frac{\hat{\phi} - 0.8}{\sqrt{\frac{\sigma^2 \gamma_0^*}{T}}} \xrightarrow{d} N(0,1) $$
>
>Suponha que, para uma amostra de tamanho $T=100$, tenhamos $\hat{\phi} = 0.85$, $\sigma^2=0.8$ e $\gamma_0^*=1.5$. O valor da estatÃ­stica de teste seria:
>
>$$z = \frac{0.85 - 0.8}{\sqrt{\frac{0.8 \times 1.5}{100}}} = \frac{0.05}{0.1095} \approx 0.456$$
>
>Para um nÃ­vel de significÃ¢ncia de 5%, o valor crÃ­tico de um teste bicaudal seria cerca de 1.96. Como $|0.456| < 1.96$, nÃ£o rejeitamos a hipÃ³tese nula de que $\phi=0.8$. Este exemplo mostra como a independÃªncia assintÃ³tica nos permite realizar testes sobre parÃ¢metros AR sem se preocupar com a tendÃªncia.

A transformaÃ§Ã£o dos regressores proposta por Sims, Stock e Watson [^1] Ã© um instrumento poderoso para a anÃ¡lise assintÃ³tica de modelos AR com tendÃªncia temporal determinÃ­stica, permitindo a separaÃ§Ã£o de componentes com diferentes taxas de convergÃªncia e a obtenÃ§Ã£o de resultados precisos para a distribuiÃ§Ã£o dos estimadores. A independÃªncia assintÃ³tica dos estimadores, demonstrada pelos resultados do Teorema 3 e CorolÃ¡rio 3.1, simplifica a anÃ¡lise estatÃ­stica e permite a realizaÃ§Ã£o de testes de hipÃ³teses de forma mais eficaz e direta, como serÃ¡ discutido nas seÃ§Ãµes seguintes deste texto.

### ConclusÃ£o

Este capÃ­tulo aprofundou a anÃ¡lise da inferÃªncia assintÃ³tica para processos autorregressivos com tendÃªncia temporal determinÃ­stica. A transformaÃ§Ã£o de regressores, o uso da matriz de escalonamento $Y_T$ e a estrutura bloco diagonal da matriz $Q^*$ foram fundamentais para a obtenÃ§Ã£o das distribuiÃ§Ãµes assintÃ³ticas dos estimadores OLS. A independÃªncia assintÃ³tica dos estimadores de parÃ¢metros estacionÃ¡rios e de parÃ¢metros de tendÃªncia permite testes de hipÃ³teses mais diretos e eficientes. O uso da transformaÃ§Ã£o, portanto, fornece um framework robusto para anÃ¡lise de modelos de sÃ©ries temporais com tendÃªncia e componentes autorregressivos, o qual serÃ¡ Ãºtil no estudo de modelos com raÃ­zes unitÃ¡rias.

### ReferÃªncias
[^1]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113â€“44.
[^2]: SeÃ§Ã£o 16.3 do texto original.
<!-- END -->
