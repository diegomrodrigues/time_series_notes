## Infer√™ncia Assint√≥tica para um Processo Autorregressivo em Torno de uma Tend√™ncia de Tempo Determin√≠stica

### Introdu√ß√£o
Em continuidade ao estudo de processos com tend√™ncias temporais determin√≠sticas, este cap√≠tulo avan√ßa para a an√°lise de processos autorregressivos (AR) em torno de tais tend√™ncias. Como vimos anteriormente, a presen√ßa de tend√™ncias temporais determin√≠sticas exige abordagens espec√≠ficas para o c√°lculo das distribui√ß√µes assint√≥ticas dos estimadores, dada a n√£o estacionariedade inerente a esses processos. O foco desta se√ß√£o √© a transforma√ß√£o do modelo original para facilitar a infer√™ncia assint√≥tica, uma t√©cnica introduzida por Sims, Stock e Watson [^1, ^3]. Ao isolar componentes com diferentes taxas de converg√™ncia, essa transforma√ß√£o simplifica a an√°lise e permite obter as distribui√ß√µes assint√≥ticas de forma mais direta.

### Conceitos Fundamentais

A an√°lise come√ßa com o modelo autorregressivo geral [16.3.1]:
$$ y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t $$
onde $\epsilon_t$ √© um ru√≠do branco i.i.d. com m√©dia zero, vari√¢ncia $\sigma^2$ e quarto momento finito. Al√©m disso, as ra√≠zes da equa√ß√£o $1 - \phi_1 z - \phi_2 z^2 - \ldots - \phi_p z^p = 0$ est√£o fora do c√≠rculo unit√°rio [^1].

> üí° **Exemplo Num√©rico:** Considere um processo AR(1) com tend√™ncia linear: $y_t = 1 + 0.5t + 0.7y_{t-1} + \epsilon_t$. Aqui, $\alpha = 1$, $\delta = 0.5$, $\phi_1 = 0.7$, e assumimos que $\epsilon_t$ √© um ru√≠do branco. A raiz da equa√ß√£o $1 - 0.7z = 0$ √© $z = 1/0.7 \approx 1.43$, que est√° fora do c√≠rculo unit√°rio.

Uma transforma√ß√£o crucial √© aplicada para reescrever este modelo em termos de vari√°veis estacion√°rias. Adicionando e subtraindo $\phi_j[\alpha + \delta(t-j)]$ para $j = 1, 2, \ldots, p$, obtemos [16.3.2]:
$$ y_t = \alpha(1 + \phi_1 + \phi_2 + \ldots + \phi_p) + \delta(1 + \phi_1 + 2\phi_2 + \ldots + p\phi_p) + \delta(\phi_1 + 2\phi_2 + \ldots + p\phi_p) + \phi_1[y_{t-1} - \alpha - \delta(t-1)] + \phi_2[y_{t-2} - \alpha - \delta(t-2)] + \ldots + \phi_p[y_{t-p} - \alpha - \delta(t-p)] + \epsilon_t $$
Este modelo √© ent√£o reescrito como [16.3.3]:
$$ y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \ldots + \phi_p^* y_{t-p}^* + \epsilon_t $$
onde
$$ \alpha^* = \alpha(1 + \phi_1 + \phi_2 + \ldots + \phi_p) - \delta(\phi_1 + 2\phi_2 + \ldots + p\phi_p) $$
$$ \delta^* = \delta(1 + \phi_1 + \phi_2 + \ldots + \phi_p) $$
e
$$ y_{t-j}^* = y_{t-j} - \alpha - \delta(t-j), \quad j = 1, 2, \ldots, p $$

> üí° **Exemplo Num√©rico (continua√ß√£o):** Para o AR(1) do exemplo anterior, temos:
> $\alpha^* = 1(1+0.7) - 0.5(0.7) = 1.7 - 0.35 = 1.35$
> $\delta^* = 0.5(1+0.7) = 0.5(1.7) = 0.85$
> $y_{t-1}^* = y_{t-1} - 1 - 0.5(t-1)$
>
> Assim, o modelo transformado √©: $y_t = 1.35 + 0.85t + 0.7y_{t-1}^* + \epsilon_t$

Essa transforma√ß√£o, seguindo a abordagem de Sims, Stock e Watson [^1], isola componentes com diferentes taxas de converg√™ncia, como os termos $y_{t-j}^*$, que s√£o vari√°veis estacion√°rias de m√©dia zero, e os termos constantes e de tend√™ncia temporal, que convergem a taxas diferentes. A import√¢ncia dessa transforma√ß√£o reside na sua capacidade de simplificar a an√°lise da distribui√ß√£o assint√≥tica dos estimadores de m√≠nimos quadrados ordin√°rios (OLS) [^1].

**Proposi√ß√£o 1**
A transforma√ß√£o descrita acima preserva a estrutura autorregressiva do processo, no sentido de que os coeficientes $\phi_1, \phi_2, \ldots, \phi_p$ permanecem inalterados ap√≥s a transforma√ß√£o.
*Proof:* Note that in the transformed model the coefficients $\phi_1^*, \phi_2^*, \ldots, \phi_p^*$ are associated to $y_{t-j}^* = y_{t-j} - \alpha - \delta(t-j)$. If we expand this and substitute back into the original model [16.3.1], we find that the coefficients of the lagged $y$ are still $\phi_1, \phi_2, \ldots, \phi_p$. This transformation only shifts the intercept and the time trend, not the autoregressive structure.
‚ñ†

O modelo original [16.3.1] pode ser expresso em forma matricial como [16.3.5]:
$$ y_t = x_t'\beta + \epsilon_t $$
onde $x_t$ √© um vetor de regressores e $\beta$ √© o vetor de coeficientes. Ap√≥s a transforma√ß√£o, o modelo torna-se [16.3.7]:
$$ y_t = x_t G' (G')^{-1} \beta + \epsilon_t = [x_t^*]' \beta^* + \epsilon_t $$
onde $x_t^* = G x_t$ e $\beta^* = (G')^{-1} \beta$ [^1]. A matriz $G'$ [16.3.8] √© definida como:
$$ G' = \begin{bmatrix}
1 & 0 & 0 & \ldots & 0 & 0 & 0 \\
0 & 1 & 0 & \ldots & 0 & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & 1 & 0 & 0 \\
-\alpha + \delta & -\alpha + 2\delta & \ldots & -\alpha + p\delta & 1 & 0 \\
-\delta & -\delta & \ldots & -\delta & 0 & 1
\end{bmatrix} $$

> üí° **Exemplo Num√©rico:** Para um modelo AR(2) com intercepto e tend√™ncia, ter√≠amos $p=2$, e a matriz $G'$ seria:
> $$ G' = \begin{bmatrix}
> 1 & 0 & 0 & 0 \\
> 0 & 1 & 0 & 0 \\
> -\alpha + \delta & -\alpha + 2\delta & 1 & 0 \\
> -\delta & -\delta & 0 & 1
> \end{bmatrix} $$
> Se $\alpha = 2$ e $\delta = 0.3$, ent√£o:
>  $$ G' = \begin{bmatrix}
> 1 & 0 & 0 & 0 \\
> 0 & 1 & 0 & 0 \\
> -1.7 & -1.4 & 1 & 0 \\
> -0.3 & -0.3 & 0 & 1
> \end{bmatrix} $$

A transforma√ß√£o envolve reescrever os regressores em termos de vari√°veis estacion√°rias de m√©dia zero, uma constante e uma tend√™ncia temporal. Essa abordagem simplifica a obten√ß√£o das distribui√ß√µes assint√≥ticas dos estimadores OLS, pois os coeficientes das vari√°veis estacion√°rias convergem a uma taxa $\sqrt{T}$, enquanto os coeficientes da tend√™ncia temporal convergem a uma taxa $T^{3/2}$ [^1].

**Lema 1** The matrix $G'$ given in [16.3.8] is invertible provided at least one of the parameters $\alpha$ or $\delta$ is non-zero.
*Proof*:
I.   We can compute the determinant of $G'$ by block matrices. Notice that $G'$ can be partitioned as
$$
G' = \begin{bmatrix}
I_p & 0 \\
A & I_2
\end{bmatrix}
$$
where $I_p$ is the $p \times p$ identity matrix, and
$$
A = \begin{bmatrix}
-\alpha + \delta & -\alpha + 2\delta & \ldots & -\alpha + p\delta  \\
-\delta & -\delta & \ldots & -\delta
\end{bmatrix}
$$
and the two $I_2$ represents the $2 \times 2$ identity matrix.
II.  The determinant of a block matrix with such structure is given by $\det(I_p)\det(I_2)$.
III. Since the determinant of the identity matrix is 1, we have $\det(I_p) = 1$ and $\det(I_2) = 1$.
IV. Therefore, $\det(G') = \det(I_p) \det(I_2) = 1 \cdot 1 = 1$
V. As the determinant is nonzero, $G'$ is invertible.
This result is independent of the parameters $\alpha$ and $\delta$.
‚ñ†

**Observa√ß√£o 1**
A matriz $G'$ √© utilizada para transformar os regressores originais para o espa√ßo de regressores transformados. Esta transforma√ß√£o n√£o modifica o espa√ßo gerado pelos regressores, mas sim sua representa√ß√£o. Em particular, note que o n√∫mero de colunas em $G'$ √© igual ao n√∫mero de regressores no modelo original, e o n√∫mero de linhas em $G'$ corresponde ao n√∫mero de regressores no modelo transformado.

O estimador de m√≠nimos quadrados para $\beta^*$ √© dado por [16.3.11]:
$$ b^* = [ \sum_{t=1}^T x_t^* x_t^{*'} ]^{-1} \sum_{t=1}^T x_t^* y_t = (G')^{-1} b $$
onde $b$ √© o estimador OLS dos par√¢metros do modelo original [^1].

> üí° **Exemplo Num√©rico:** Suponha que ap√≥s estimar um modelo AR(1) com tend√™ncia, voc√™ obt√©m o vetor de estimativas $b = [1.2, 0.4, 0.6]$ correspondente aos par√¢metros $\alpha$, $\delta$, e $\phi_1$. Com os valores do exemplo anterior $\alpha = 1$, $\delta = 0.5$, temos a matriz $G'$ para $p=1$:
> $$ G' = \begin{bmatrix}
> 1 & 0 & 0 \\
> -1 + 0.5 & 1 & 0 \\
> -0.5 & 0 & 1
> \end{bmatrix} = \begin{bmatrix}
> 1 & 0 & 0 \\
> -0.5 & 1 & 0 \\
> -0.5 & 0 & 1
> \end{bmatrix} $$
>
> Para obter $(G')^{-1}$, usamos:
>
> $$ (G')^{-1} = \begin{bmatrix}
> 1 & 0 & 0 \\
> 0.5 & 1 & 0 \\
> 0.5 & 0 & 1
> \end{bmatrix} $$
>
>
> Ent√£o, $b^* = (G')^{-1}b$:
>
> $$ b^* =  \begin{bmatrix}
> 1 & 0 & 0 \\
> 0.5 & 1 & 0 \\
> 0.5 & 0 & 1
> \end{bmatrix} \begin{bmatrix}
> 1.2 \\ 0.4 \\ 0.6
> \end{bmatrix} = \begin{bmatrix}
> 1.2 \\ 0.5(1.2) + 0.4 \\ 0.5(1.2) + 0.6
> \end{bmatrix} = \begin{bmatrix}
> 1.2 \\ 1 \\ 1.2
> \end{bmatrix}
> $$
>
> Aqui, o vetor $b^*$ representa as estimativas dos par√¢metros no modelo transformado.

**Teorema 1**
Assumindo que os erros $\epsilon_t$ s√£o i.i.d com m√©dia zero, vari√¢ncia $\sigma^2$ e quarto momento finito, e que as ra√≠zes da equa√ß√£o $1 - \phi_1 z - \phi_2 z^2 - \ldots - \phi_p z^p = 0$ est√£o fora do c√≠rculo unit√°rio, o estimador de m√≠nimos quadrados $b^*$ √© consistente para $\beta^*$, ou seja, $b^* \xrightarrow{p} \beta^*$ quando $T \rightarrow \infty$.
*Proof*:
I. From standard OLS theory it is known that consistency is guaranteed as long as the regressors are non-collinear and are uncorrelated with the error term $\epsilon_t$ asymptotically.
II. In the transformed model, the regressors $x_t^*$ are constructed to eliminate any correlation with $\epsilon_t$ as $T$ grows, due to the fact that $y_{t-j}^*$ are zero mean stationary processes by construction.
III. The constant and trend components in the model are deterministic, thus they are independent of the errors.
IV. Also, the transformation preserves the rank of the regression matrix as shown by the fact that $G'$ is invertible, thus $x_t^*$ are not asymptotically collinear.
V. Therefore, under the stated conditions, the OLS estimator $b^*$ is consistent for $\beta^*$, as $T \to \infty$, i.e., $b^* \xrightarrow{p} \beta^*$.
‚ñ†

### Conclus√£o

Em resumo, a transforma√ß√£o dos regressores em um modelo autorregressivo com tend√™ncia temporal determin√≠stica, conforme proposto por Sims, Stock e Watson, √© fundamental para simplificar a an√°lise assint√≥tica. A separa√ß√£o dos componentes com diferentes taxas de converg√™ncia permite derivar as distribui√ß√µes assint√≥ticas dos estimadores OLS de forma mais direta e eficiente. Como ser√° visto nas pr√≥ximas se√ß√µes, essa metodologia possibilita realizar testes de hip√≥teses de maneira consistente e obter infer√™ncias estat√≠sticas v√°lidas sobre os par√¢metros do modelo. Essa transforma√ß√£o serve como um passo crucial para entender o comportamento de modelos mais complexos, como os que incluem ra√≠zes unit√°rias, que ser√£o explorados nos pr√≥ximos cap√≠tulos [^1].

### Refer√™ncias
[^1]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113‚Äì44.
[^2]: Se√ß√£o 16.1 do texto original
[^3]: Se√ß√£o 16.3 do texto original
<!-- END -->
