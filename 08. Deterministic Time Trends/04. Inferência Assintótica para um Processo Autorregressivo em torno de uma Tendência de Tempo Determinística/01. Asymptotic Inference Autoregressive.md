## Infer√™ncia Assint√≥tica para um Processo Autorregressivo em torno de uma Tend√™ncia de Tempo Determin√≠stica

### Introdu√ß√£o
Como vimos anteriormente, a an√°lise de processos com tend√™ncias temporais determin√≠sticas requer abordagens distintas daquelas utilizadas para s√©ries estacion√°rias. Este cap√≠tulo expande esses conceitos, explorando a infer√™ncia assint√≥tica para processos autorregressivos (AR) em torno de uma tend√™ncia temporal determin√≠stica, um cen√°rio comum em muitas aplica√ß√µes econ√¥micas e financeiras. Expandindo o que vimos no cap√≠tulo anterior [^1], este t√≥pico adota a metodologia proposta por Sims, Stock e Watson [^3, ^4], transformando o modelo original para simplificar a deriva√ß√£o da distribui√ß√£o assint√≥tica dos estimadores.

### Conceitos Fundamentais
O modelo autorregressivo geral com uma tend√™ncia temporal determin√≠stica √© dado por [^1]:
$$ y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t $$
onde $\epsilon_t$ representa um ru√≠do branco com m√©dia zero, vari√¢ncia $\sigma^2$ e momentos finitos de quarta ordem. As ra√≠zes do polin√¥mio caracter√≠stico $1 - \phi_1z - \phi_2z^2 - \ldots - \phi_pz^p = 0$ est√£o fora do c√≠rculo unit√°rio.

A abordagem de Sims, Stock e Watson envolve uma transforma√ß√£o do modelo que separa as componentes com diferentes taxas de converg√™ncia [^3, ^4]. Essa transforma√ß√£o pode ser expressa como [^1]:
$$ y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \ldots + \phi_p^* y_{t-p}^* + \epsilon_t $$
onde os novos regressores $y_{t-j}^*$ s√£o definidos como $y_{t-j}^* = y_{t-j} - \alpha - \delta(t-j)$ para $j = 1, 2, \ldots, p$. Os coeficientes transformados s√£o dados por:
$$
\begin{aligned}
\alpha^* &= \alpha(1+\phi_1+\phi_2+ \ldots + \phi_p) - \delta(\phi_1 + 2\phi_2 + \ldots + p\phi_p) \\
\delta^* &= \delta(1 + \phi_1 + \phi_2 + \ldots + \phi_p) \\
\phi_j^* &= \phi_j, \quad j=1,2,...,p
\end{aligned}
$$
Essa transforma√ß√£o √© crucial porque os coeficientes originais do modelo AR e o coeficiente da tend√™ncia temporal $\delta$ convergem a diferentes taxas, como visto na se√ß√£o anterior [^1]. Ao separar os regressores em componentes de m√©dia zero, uma tend√™ncia temporal e uma constante, torna-se poss√≠vel estudar a distribui√ß√£o assint√≥tica dos estimadores. A forma matricial dessa transforma√ß√£o √© dada por [^1]:
$$ y_t = x_t'G'(G')^{-1}\beta + \epsilon_t = x_t^{*'}\beta^* + \epsilon_t $$
onde:
$$x_t = \begin{bmatrix} y_{t-1} \\ y_{t-2} \\ \vdots \\ y_{t-p} \\ 1 \\ t \end{bmatrix}, \quad \beta = \begin{bmatrix} \phi_1 \\ \phi_2 \\ \vdots \\ \phi_p \\ \alpha \\ \delta \end{bmatrix}, \quad G' = \begin{bmatrix}
1 & 0 & \ldots & 0 & 0 & 0 \\
0 & 1 & \ldots & 0 & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & \ldots & 1 & 0 & 0 \\
-\alpha + \delta & -\alpha + 2\delta & \ldots & -\alpha + p\delta & 1 & 0 \\
-\delta & -\delta & \ldots & -\delta & 0 & 1
\end{bmatrix}$$
e
$$
x_t^* = G x_t = \begin{bmatrix}
y_{t-1}^* \\ y_{t-2}^* \\ \vdots \\ y_{t-p}^* \\ 1 \\ t
\end{bmatrix}, \quad
\beta^* = (G')^{-1}\beta = \begin{bmatrix} \phi_1^* \\ \phi_2^* \\ \vdots \\ \phi_p^* \\ \alpha^* \\ \delta^* \end{bmatrix}
$$
**Proposi√ß√£o 1:** A matriz $G$ √© invers√≠vel.

*Prova:*
I. A matriz $G'$ pode ser vista como uma matriz de mudan√ßa de base, transformando o vetor de regressores $x_t$ para um novo vetor $x_t^*$ que separa a tend√™ncia temporal dos componentes autorregressivos.
II. A matriz $G'$ √© formada por colunas linearmente independentes. Para provar isso, observe que os primeiros $p$ vetores can√¥nicos (os primeiros $p$ colunas de $G'$) s√£o linearmente independentes. Al√©m disso, a $(p+1)$-√©sima coluna √© uma combina√ß√£o linear dos vetores can√¥nicos e o vetor $[1,2,\ldots,p,1,0]^T$, e a $(p+2)$-√©sima coluna √© uma combina√ß√£o linear dos vetores can√¥nicos e o vetor $[1,1,\ldots,1,0,1]^T$.  Assim, nenhuma coluna √© uma combina√ß√£o linear das outras, e portanto todas as colunas s√£o linearmente independentes.
III.  Colunas linearmente independentes implicam que o determinante de $G'$ √© diferente de zero, ent√£o $G'$ √© invers√≠vel.
IV. Como $G'$ √© invers√≠vel, sua inversa $G = (G')^{-1}$ tamb√©m √© invers√≠vel.  $\blacksquare$

> üí° **Exemplo Num√©rico:** Para ilustrar a matriz G', vamos considerar um modelo AR(1) com tend√™ncia temporal (p=1).
> Sejam $\alpha = 2$, $\delta = 0.5$, e $\phi_1 = 0.7$. Ent√£o, a matriz G' ser√°:
> $$ G' = \begin{bmatrix} 1 & 0 \\ -2 + 0.5 & 1 & 0 \\ -0.5 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 \\ -1.5 & 1 & 0 \\ -0.5 & 0 & 1 \end{bmatrix} $$
> A matriz G, que √© a inversa de G', pode ser calculada como:
>  $$ G = (G')^{-1} = \begin{bmatrix} 1 & 0 & 0 \\ 1.5 & 1 & 0 \\ 0.5 & 0 & 1 \end{bmatrix} $$
>
> Suponha que tenhamos um vetor de regressores $x_t = \begin{bmatrix} y_{t-1} \\ 1 \\ t \end{bmatrix} = \begin{bmatrix} 5 \\ 1 \\ 2 \end{bmatrix}$.  Ent√£o, o vetor transformado $x_t^* = G x_t$ ser√°:
> $$ x_t^* = \begin{bmatrix} 1 & 0 & 0 \\ 1.5 & 1 & 0 \\ 0.5 & 0 & 1 \end{bmatrix} \begin{bmatrix} 5 \\ 1 \\ 2 \end{bmatrix} = \begin{bmatrix} 5 \\ 8.5 \\ 4.5 \end{bmatrix} $$
> O vetor transformado agora cont√©m $y_{t-1}^* = y_{t-1} - \alpha - \delta(t-1)$, onde neste caso, o valor de $y_{t-1}^* =  5 - 2 - 0.5(2-1) = 5 - 2 - 0.5 = 2.5$, but due to the matrix operation, we have $y_{t-1}^* = 5$, the 1 is transformed into $1 + 1.5*5 = 8.5$ and the t=2 is transformed into $2 + 0.5*5 = 4.5$

Essa reparametriza√ß√£o n√£o altera os valores ajustados do modelo, mas simplifica a an√°lise assint√≥tica dos estimadores. O estimador de m√≠nimos quadrados ordin√°rios (OLS) para $\beta^*$ √© dado por [^1]:
$$ b^* = \left( \sum_{t=1}^T x_t^*x_t^{*'} \right)^{-1} \left( \sum_{t=1}^T x_t^* y_t \right) = (G')^{-1} b $$
onde $b$ √© o estimador OLS de $\beta$ no modelo original.

### An√°lise Assint√≥tica
O ap√™ndice do cap√≠tulo demonstra que a distribui√ß√£o assint√≥tica de $\hat{\beta}^*$ √© dada por [^1]:
$$ Y_T (\hat{b}^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1}) $$
onde $Y_T$ √© uma matriz diagonal com $\sqrt{T}$ para as componentes estacion√°rias e $T^{3/2}$ para o componente da tend√™ncia temporal, e $Q^*$ √© a matriz limite da soma dos produtos cruzados dos regressores transformados.
$$
Y_T = \begin{bmatrix} \sqrt{T} & 0 & \cdots & 0 & 0 & 0 \\
0 & \sqrt{T} & \cdots & 0 & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & \cdots & \sqrt{T} & 0 & 0 \\
0 & 0 & \cdots & 0 & \sqrt{T} & 0 \\
0 & 0 & \cdots & 0 & 0 & T^{3/2} \end{bmatrix}
$$
e $Q^*$ √© dada por:
$$
Q^* = \begin{bmatrix}
\gamma_0^* & \gamma_1^* & \cdots & \gamma_{p-1}^* & 0 & 0 \\
\gamma_1^* & \gamma_0^* & \cdots & \gamma_{p-2}^* & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
\gamma_{p-1}^* & \gamma_{p-2}^* & \cdots & \gamma_0^* & 0 & 0 \\
0 & 0 & \cdots & 0 & 1 & 0 \\
0 & 0 & \cdots & 0 & 0 & 1/3
\end{bmatrix}
$$
onde $\gamma_j^* = E(y_t^* y_{t-j}^*)$. Em outras palavras, os coeficientes dos componentes estacion√°rios do modelo convergem √† taxa $\sqrt{T}$, enquanto que o coeficiente da tend√™ncia temporal converge √† taxa $T^{3/2}$, o que demonstra diferentes taxas de converg√™ncia. A transforma√ß√£o isola esses componentes e permite a an√°lise de suas distribui√ß√µes assint√≥ticas separadamente.

> üí° **Exemplo Num√©rico:** Para um modelo AR(1) com tend√™ncia (p=1), considere $\gamma_0^* = 2$ e $\gamma_1^* = 1.5$ (autocovari√¢ncias dos $y_t^*$). Ent√£o a matriz $Q^*$ para este caso ser√°:
> $$ Q^* = \begin{bmatrix} 2 & 0 & 0 \\ 1.5 & 1 & 0 \\ 0 & 0 & 1/3 \end{bmatrix} $$
> A inversa de $Q^*$ que √© necess√°ria para calcular a matriz de vari√¢ncia-covari√¢ncia assint√≥tica dos estimadores √©:
> $$(Q^*)^{-1} = \begin{bmatrix} 0.5 & 0 & 0 \\ -0.75 & 1 & 0 \\ 0 & 0 & 3 \end{bmatrix}$$
>Se a vari√¢ncia do erro $\sigma^2$ fosse igual a 1, e o tamanho da amostra fosse T=100, ent√£o a matriz de vari√¢ncia-covari√¢ncia assint√≥tica dos estimadores transformados seria aproximadamente:
>$$Cov(\hat{b}^*) \approx \sigma^2(Y_T^{-1} Q^{*-1} Y_T^{-1})  =   \begin{bmatrix} 1/\sqrt{100} & 0 & 0 \\ 0 & 1/\sqrt{100} & 0 \\ 0 & 0 & 1/100^{3/2} \end{bmatrix}  \begin{bmatrix} 0.5 & -0.75 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 3 \end{bmatrix}  \begin{bmatrix} 1/\sqrt{100} & 0 & 0 \\ 0 & 1/\sqrt{100} & 0 \\ 0 & 0 & 1/100^{3/2} \end{bmatrix} $$
>
> $$Cov(\hat{b}^*) \approx  \begin{bmatrix} 0.005 & -0.0075 & 0 \\ -0.0075 & 0.01 & 0 \\ 0 & 0 & 0.000003\end{bmatrix}  $$
>Observamos que a vari√¢ncia do estimador do coeficiente da tend√™ncia temporal √© muito menor (converge para 0 em uma taxa muito maior) do que os outros coeficientes, o que est√° em linha com a teoria.

**Lema 1:**  A matriz $Q^*$ √© sim√©trica e positiva definida.

*Prova:*
I. A matriz $Q^*$ √© sim√©trica por constru√ß√£o, dado que $\gamma_j^* = E(y_t^* y_{t-j}^*) = E(y_{t-j}^* y_t^*) = \gamma_{-j}^*$.
II. Para demonstrar que $Q^*$ √© positiva definida, precisamos mostrar que $z'Q^*z > 0$ para qualquer vetor $z \neq 0$. Seja $z = [z_1, z_2, \ldots, z_p, z_{p+1}, z_{p+2}]'$. Ent√£o,
$$
z'Q^*z = \sum_{i=1}^p \sum_{j=1}^p z_i z_j \gamma_{|i-j|}^* + z_{p+1}^2 + \frac{1}{3}z_{p+2}^2
$$
III. O primeiro termo √© a forma quadr√°tica da matriz de autocovari√¢ncias dos $y_t^*$, que √© positiva definida uma vez que as ra√≠zes do polin√¥mio caracter√≠stico do processo AR est√£o fora do c√≠rculo unit√°rio (uma propriedade bem conhecida de processos AR estacion√°rios [^4]). O segundo e terceiro termos s√£o n√£o-negativos e s√£o nulos apenas se $z_{p+1} = 0$ e $z_{p+2} = 0$, respectivamente.
IV. Como $z \neq 0$, pelo menos um dos elementos de $z$ deve ser diferente de zero. Se algum dos $z_1, \ldots, z_p$ forem diferentes de zero, ent√£o o primeiro termo √© estritamente positivo. Se todos $z_1, \ldots, z_p$ forem zero e pelo menos um de $z_{p+1}$ ou $z_{p+2}$ for diferente de zero, ent√£o pelo menos um dos termos restantes tamb√©m √© estritamente positivo.
V.  Portanto, $z'Q^*z > 0$ para qualquer $z \neq 0$, o que demonstra que $Q^*$ √© positiva definida. $\blacksquare$

**Teorema 1:** A distribui√ß√£o assint√≥tica do estimador OLS do modelo original √© dada por:
$$ Y_T (\hat{b} - \beta) \xrightarrow{d} N(0, \sigma^2 G' [Q^*]^{-1} G) $$

*Prova:*
I. Sabemos que $b^* = (G')^{-1} b$. Portanto, $b = G' b^*$.
II. Logo, $\hat{b} - \beta = G' (\hat{b}^* - \beta^*)$.
III. Multiplicando por $Y_T$ temos:
$$
Y_T (\hat{b} - \beta) = Y_T G' (\hat{b}^* - \beta^*)
$$
IV. Como $G'$ √© uma matriz constante, ela pode ser movida para fora do produto com a matriz diagonal $Y_T$:
$$ Y_T (\hat{b} - \beta) = G' Y_T (\hat{b}^* - \beta^*)
$$
V. Sabemos que $ Y_T (\hat{b}^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$.
VI. Dado que $G'$ √© uma matriz constante, podemos usar o teorema de Slutsky (que afirma que a multiplica√ß√£o de uma sequ√™ncia convergente em distribui√ß√£o por uma constante mant√©m a converg√™ncia em distribui√ß√£o) para obter a distribui√ß√£o assint√≥tica de $Y_T (\hat{b} - \beta)$:
$$
Y_T (\hat{b} - \beta) \xrightarrow{d} N(0, \sigma^2 G' [Q^*]^{-1} G)
$$
$\blacksquare$

> üí° **Exemplo Num√©rico:** Continuando com o exemplo anterior, seja $G'$ como na primeira se√ß√£o, $G = (G')^{-1}$,  $Q^{*-1}$ como na se√ß√£o anterior e $\sigma^2 = 1$. Ent√£o, a matriz de vari√¢ncia-covari√¢ncia assint√≥tica do estimador do modelo original $\hat{b}$ √© dada por:
> $$Cov(\hat{b}) \approx  \sigma^2 (Y_T^{-1} G' Q^{*-1} G Y_T^{-1}) $$
> $$Cov(\hat{b}) \approx  \begin{bmatrix} 1/\sqrt{100} & 0 & 0 \\ 0 & 1/\sqrt{100} & 0 \\ 0 & 0 & 1/100^{3/2} \end{bmatrix} \begin{bmatrix} 1 & -1.5 & -0.5 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 0.5 & 0 & 0 \\ -0.75 & 1 & 0 \\ 0 & 0 & 3 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \\ 1.5 & 1 & 0 \\ 0.5 & 0 & 1 \end{bmatrix} \begin{bmatrix} 1/\sqrt{100} & 0 & 0 \\ 0 & 1/\sqrt{100} & 0 \\ 0 & 0 & 1/100^{3/2} \end{bmatrix} $$
>
> $$Cov(\hat{b}) \approx  \begin{bmatrix} 0.005 & 0.00 & 0.0 \\ 0.00 & 0.01 & 0.0 \\ 0.0 & 0.0 & 0.000003 \end{bmatrix} $$
>
> Note que a vari√¢ncia dos coeficientes $\alpha$ e $\phi_1$ s√£o maiores que a vari√¢ncia do coeficiente $\delta$ (tend√™ncia temporal), confirmando que a converg√™ncia do estimador de $\delta$ ocorre a uma taxa muito mais r√°pida. Note tamb√©m que a matriz resultante, de acordo com a teoria, n√£o √© mais diagonal devido √† transforma√ß√£o $G$.

### Conclus√£o
A transforma√ß√£o dos regressores, conforme proposto por Sims, Stock e Watson [^3, ^4], √© uma t√©cnica valiosa para lidar com processos autorregressivos em torno de uma tend√™ncia temporal determin√≠stica. A reparametriza√ß√£o permite isolar as componentes com diferentes taxas de converg√™ncia, fornecendo uma estrutura para a infer√™ncia assint√≥tica dos estimadores. A an√°lise dos estimadores transformados fornece intui√ß√µes importantes sobre o comportamento assint√≥tico do modelo original e possibilita a constru√ß√£o de testes de hip√≥teses v√°lidos. Este t√≥pico se conecta diretamente com a discuss√£o do cap√≠tulo anterior sobre processos com tend√™ncia temporal determin√≠stica [^1] e estabelece a base para a discuss√£o de testes de hip√≥teses sobre esses modelos [^1]. Os resultados obtidos neste t√≥pico, em conjunto com os resultados da se√ß√£o anterior, mostram que a infer√™ncia sobre os par√¢metros do modelo pode ser realizada usando m√©todos assint√≥ticos padr√£o ap√≥s a transforma√ß√£o apropriada.

### Refer√™ncias
[^1]: Cap√≠tulo 16 do texto base, "Processes with Deterministic Time Trends".
[^3]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113-44.
[^4]: Fuller, Wayne A. 1976. Introduction to Statistical Time Series. New York: Wiley.
<!-- END -->
