## Distribui√ß√£o Assint√≥tica dos Estimadores OLS em Modelos AR com Tend√™ncia Determin√≠stica

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre a transforma√ß√£o dos regressores em modelos autorregressivos com tend√™ncia temporal determin√≠stica, este cap√≠tulo explora a distribui√ß√£o assint√≥tica dos estimadores de M√≠nimos Quadrados Ordin√°rios (OLS) ap√≥s a referida transforma√ß√£o. Como estabelecido anteriormente, a abordagem de Sims, Stock e Watson [^1] permite analisar separadamente os componentes com diferentes taxas de converg√™ncia, facilitando a infer√™ncia assint√≥tica [^2]. O foco agora √© detalhar como essa transforma√ß√£o afeta a distribui√ß√£o assint√≥tica dos estimadores e derivar resultados que nos permitem realizar infer√™ncias estat√≠sticas v√°lidas.

### Conceitos Fundamentais
Retomando o modelo transformado [16.3.3], temos:
$$ y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \ldots + \phi_p^* y_{t-p}^* + \epsilon_t $$
onde $\alpha^*$, $\delta^*$ e $y_{t-j}^*$ s√£o definidos como anteriormente [^2]. Como vimos, esta transforma√ß√£o separa os componentes com diferentes taxas de converg√™ncia: os termos $y_{t-j}^*$, que s√£o vari√°veis estacion√°rias de m√©dia zero, e os termos constante e de tend√™ncia temporal, que exibem diferentes comportamentos assint√≥ticos [^1].

> üí° **Exemplo Num√©rico:** Suponha um modelo AR(1) com tend√™ncia linear, onde o modelo original √© dado por $y_t = \alpha + \delta t + \phi_1 y_{t-1} + \epsilon_t$. Ap√≥s a transforma√ß√£o de Sims, Stock e Watson, obtemos o modelo transformado: $y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \epsilon_t$.  Vamos considerar que, ap√≥s a transforma√ß√£o, temos os seguintes coeficientes transformados: $\alpha^* = 1.35$, $\delta^* = 0.85$, e $\phi_1^* = 0.6$. O regressor transformado √© dado por $y_{t-1}^* = y_{t-1} - \hat{\mu}_{t-1}$, onde $\hat{\mu}_{t-1}$ √© a tend√™ncia estimada. O foco agora √© analisar o comportamento assint√≥tico dos estimadores para $\alpha^*$, $\delta^*$ e $\phi_1^*$.

O modelo transformado pode ser expresso na forma matricial como [16.3.7]:
$$ y_t = [x_t^*]' \beta^* + \epsilon_t $$
onde $x_t^*$ √© o vetor de regressores transformado e $\beta^*$ √© o vetor de coeficientes transformado [^2]. O estimador OLS para $\beta^*$, denotado por $b^*$, √© dado por [16.3.11]:
$$ b^* = [\sum_{t=1}^T x_t^* x_t^{*'}]^{-1} \sum_{t=1}^T x_t^* y_t = (G')^{-1}b $$
onde $b$ √© o estimador OLS para o modelo original e $G'$ √© a matriz de transforma√ß√£o [^2].

**Teorema 2 (Distribui√ß√£o Assint√≥tica de $b^*$)**
Sob as condi√ß√µes estabelecidas, ou seja, $\epsilon_t$ i.i.d. com m√©dia zero, vari√¢ncia $\sigma^2$, quarto momento finito e ra√≠zes da equa√ß√£o caracter√≠stica fora do c√≠rculo unit√°rio, a distribui√ß√£o assint√≥tica de $b^*$ √© dada por [16.3.13]:
$$ Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1}) $$
onde $Y_T$ √© uma matriz diagonal de escalonamento e $Q^*$ √© a matriz de vari√¢ncia assint√≥tica dos regressores transformados [^1].

A matriz $Y_T$ √© definida como [16.3.14]:
$$
Y_T = \begin{bmatrix}
\sqrt{T} & 0 & 0 & \cdots & 0 & 0 \\
0 & \sqrt{T} & 0 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \cdots & \sqrt{T} & 0 \\
0 & 0 & 0 & \cdots & 0 & \sqrt{T} \\
0 & 0 & 0 & \cdots & 0 & T^{3/2}
\end{bmatrix}
$$
Esta matriz diagonal reflete as diferentes taxas de converg√™ncia dos estimadores: $\sqrt{T}$ para os coeficientes das vari√°veis estacion√°rias ($y_{t-j}^*$) e $T^{3/2}$ para o coeficiente da tend√™ncia temporal ($\delta^*$) [^1]. A matriz $Q^*$ √© dada por [16.3.15]:
$$
Q^* = \begin{bmatrix}
\gamma_{0}^* & \gamma_{1}^* & \gamma_{2}^* & \cdots & \gamma_{p-1}^* & 0 & 0 \\
\gamma_{1}^* & \gamma_{0}^* & \gamma_{1}^* & \cdots & \gamma_{p-2}^* & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
\gamma_{p-1}^* & \gamma_{p-2}^* & \gamma_{p-3}^* & \cdots & \gamma_{0}^* & 0 & 0 \\
0 & 0 & 0 & \cdots & 0 & 1 & 0 \\
0 & 0 & 0 & \cdots & 0 & 0 & 1/3
\end{bmatrix}
$$
onde $\gamma_{j}^* = E(y_t^* y_{t-j}^*)$ s√£o as autocovari√¢ncias da vari√°vel estacion√°ria $y_t^*$.

*Proof:*
I. O Ap√™ndice 16.A demonstra que a matriz $Q^*$ dada por [16.3.15] √© a matriz limite da esperan√ßa da matriz de regressores [16.A.4]
$$ \frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'} \xrightarrow{p} Q^* $$
II. A mesma an√°lise, seguindo o padr√£o de [16.1.17], mostra que
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^T x_t^* \epsilon_t \xrightarrow{d} N(0, \sigma^2 Q^*) $$
III. O resultado segue da defini√ß√£o do estimador OLS e do Teorema do Limite Central. Primeiro, reescrevemos o estimador OLS $b^*$ como:
$$b^* - \beta^* = \left(\sum_{t=1}^{T} x_t^* x_t^{*'}\right)^{-1} \sum_{t=1}^{T} x_t^* \epsilon_t$$
Multiplicamos ambos os lados por $Y_T$ e obtemos:
$$ Y_T(b^* - \beta^*) =  Y_T \left(\sum_{t=1}^{T} x_t^* x_t^{*'}\right)^{-1} \sum_{t=1}^{T} x_t^* \epsilon_t $$
$$ Y_T(b^* - \beta^*) =  \left( \frac{1}{T}  Y_T \left[ \sum_{t=1}^T x_t^* x_t^{*'}\right] Y_T \right)^{-1}  Y_T \sum_{t=1}^T x_t^* \epsilon_t $$
IV. Aplicando os resultados dos passos anteriores e usando o lema de Slutsky, obtemos o resultado do teorema:
$$ Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1}) $$
‚ñ†

Este teorema √© fundamental pois demonstra que os estimadores dos par√¢metros do modelo transformado convergem para uma distribui√ß√£o normal com m√©dia centrada no valor verdadeiro e vari√¢ncia dada por $\sigma^2 [Q^*]^{-1}$, ap√≥s o devido ajuste de escala pela matriz $Y_T$.

**Observa√ß√£o 2**
A matriz $Y_T$ introduzida em [16.3.14] reflete as diferentes taxas de converg√™ncia dos estimadores. Os primeiros $p+1$ elementos da diagonal, relacionados aos par√¢metros autorregressivos e ao intercepto, s√£o $\sqrt{T}$, enquanto o √∫ltimo elemento, relacionado ao par√¢metro da tend√™ncia temporal, √© $T^{3/2}$. Esta caracter√≠stica √© fundamental para entendermos a distribui√ß√£o assint√≥tica dos estimadores.

> üí° **Exemplo Num√©rico (continua√ß√£o):** Para um modelo AR(1) com tend√™ncia linear, temos $p=1$. Vamos supor que temos uma amostra de tamanho $T=100$. A matriz $Y_T$ seria:
>
> $$ Y_T = \begin{bmatrix} \sqrt{100} & 0 & 0 \\ 0 & \sqrt{100} & 0 \\ 0 & 0 & 100^{3/2} \end{bmatrix} = \begin{bmatrix} 10 & 0 & 0 \\ 0 & 10 & 0 \\ 0 & 0 & 1000 \end{bmatrix} $$
>
>  E a matriz $Q^*$ seria:
> $$ Q^* = \begin{bmatrix} \gamma_{0}^* & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1/3 \end{bmatrix} $$
> onde $\gamma_{0}^* = E(y_t^{*2})$. Suponha que $\gamma_0^* = 2$. Ent√£o,
> $$ Q^* = \begin{bmatrix} 2 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1/3 \end{bmatrix} $$
>  A matriz de covari√¢ncia assint√≥tica dos estimadores seria $\sigma^2 [Q^*]^{-1}$. Se a vari√¢ncia dos erros, $\sigma^2$, fosse igual a 1, ent√£o:
>
> $$ [Q^*]^{-1} = \begin{bmatrix} 1/2 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 3 \end{bmatrix} $$
>  
> Assim, o teorema estabelece que $Y_T(b^* - \beta^*)$ converge em distribui√ß√£o para uma normal com m√©dia zero e matriz de covari√¢ncia $\sigma^2 [Q^*]^{-1}$. As taxas de converg√™ncia s√£o  $\sqrt{T}$ para os par√¢metros de $\alpha^*$ e $\phi_1^*$ e $T^{3/2}$ para o par√¢metro $\delta^*$, mostrando que este √∫ltimo converge mais rapidamente para o valor verdadeiro.

**Lema 2.1**  A matriz $Q^*$ √© positiva definida sob as condi√ß√µes do Teorema 2.
*Proof:*
I. A matriz $Q^*$ √© composta pelas autocovari√¢ncias do processo estacion√°rio $y_t^*$ e elementos diagonais correspondentes ao intercepto e √† tend√™ncia.
II. A submatriz contendo as autocovari√¢ncias de $y_t^*$ √© positiva definida porque √© a matriz de covari√¢ncia de um processo estacion√°rio, desde que o processo n√£o seja determin√≠stico. Esta condi√ß√£o √© garantida pela suposi√ß√£o de que as ra√≠zes da equa√ß√£o caracter√≠stica est√£o fora do c√≠rculo unit√°rio.
III. Os elementos diagonais correspondentes ao intercepto e √† tend√™ncia tamb√©m s√£o estritamente positivos (1 e 1/3, respectivamente).
IV. Uma vez que $Q^*$ √© uma matriz bloco-diagonal com blocos positivos definidos, ela pr√≥pria √© positiva definida.
‚ñ†

This lemma is important to ensure that the asymptotic variance-covariance matrix $[Q^*]^{-1}$ exists and is also positive definite, which is a necessary condition for the asymptotic normality of the estimators.

### Distribui√ß√£o Assint√≥tica dos Estimadores OLS do Modelo Original

Para obter a distribui√ß√£o assint√≥tica dos estimadores do modelo original, $b$, basta aplicar a rela√ß√£o $b = G' b^*$ [16.3.12]. Como $G'$ √© uma matriz constante, temos que:
$$ Y_T(b-\beta) = Y_T G' (b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 G' [Q^*]^{-1} G'^{'}) $$
onde $\beta$ √© o vetor de coeficientes do modelo original. Note que a matriz $Y_T$ √© usada para garantir uma distribui√ß√£o n√£o degenerada para as estimativas, dada as suas diferentes taxas de converg√™ncia [^1].

A distribui√ß√£o assint√≥tica de $\hat{\alpha}$ √© dada por [16.3.17]:
$$ \sqrt{T}(\hat{\alpha} - \alpha) \xrightarrow{d} N(0, \sigma^2 g_{\alpha} [Q^*]^{-1} g_{\alpha}^{'}) $$
onde $g_{\alpha}$ √© um vetor de pesos que isola o componente $\alpha$ no vetor $b$ [^3].
Analogamente, a distribui√ß√£o assint√≥tica de $\hat{\delta}$ √© dada por:
$$ T^{3/2}(\hat{\delta} - \delta) \xrightarrow{d} N(0, \sigma^2 g_{\delta} [Q^*]^{-1} g_{\delta}^{'}) $$
onde $g_{\delta}$ √© um vetor de pesos que isola o componente $\delta$ no vetor $b$ [^3].

> üí° **Exemplo Num√©rico (continua√ß√£o):**  Vamos considerar novamente o modelo AR(1) com tend√™ncia linear e os valores de $\alpha^*$, $\delta^*$ e $\phi_1^*$ previamente definidos. Para o modelo transformado,  $g_{\alpha} = [1, 0, 0]$, $g_{\delta} = [0, 0, 1]$, e $g_{\phi_1} = [0,1,0]$.
> No contexto do modelo original $y_t = \alpha + \delta t + \phi_1 y_{t-1} + \epsilon_t$, onde os regressores n√£o s√£o transformados, os vetores $g_{\alpha}$ e $g_{\delta}$ s√£o definidos de forma diferente.
> Especificamente, $g_{\alpha} = [-\alpha + \delta, 1, 0]$ e $g_{\delta} = [-\delta, 0, 1]$. Assim, a distribui√ß√£o assint√≥tica de $\hat{\alpha}$ e $\hat{\delta}$ dependem de suas taxas de converg√™ncia ($\sqrt{T}$ e $T^{3/2}$, respectivamente), e dos elementos da matriz $Q^*$.
>  Se usarmos os valores do exemplo anterior, com $\sigma^2 = 1$, $T=100$, $\gamma_0^*=2$ e $Q^*$ j√° calculado,  podemos obter a vari√¢ncia assint√≥tica dos estimadores:
>
> $\text{Var}(\sqrt{T}(\hat{\alpha}-\alpha)) \approx g_{\alpha} [Q^*]^{-1} g_{\alpha}' =  \begin{bmatrix} -\alpha+\delta & 1 & 0 \end{bmatrix} \begin{bmatrix} 1/2 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 3 \end{bmatrix} \begin{bmatrix} -\alpha+\delta \\ 1 \\ 0 \end{bmatrix} = \frac{(-\alpha+\delta)^2}{2} + 1$
>
> $\text{Var}(T^{3/2}(\hat{\delta}-\delta)) \approx g_{\delta} [Q^*]^{-1} g_{\delta}' =  \begin{bmatrix} -\delta & 0 & 1 \end{bmatrix} \begin{bmatrix} 1/2 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 3 \end{bmatrix} \begin{bmatrix} -\delta \\ 0 \\ 1 \end{bmatrix} = \frac{\delta^2}{2} + 3$
>
> Note que a vari√¢ncia do estimador de $\delta$ escala com $T^{3/2}$, indicando sua converg√™ncia mais r√°pida, enquanto a de $\alpha$ escala com $\sqrt{T}$.

**Teorema 3** Os estimadores dos par√¢metros estacion√°rios do modelo AR, convergindo a uma taxa $\sqrt{T}$, s√£o assintoticamente independentes do estimador do par√¢metro da tend√™ncia, que converge a uma taxa $T^{3/2}$.
*Proof:*
I. De [16.3.16] e [16.3.17], o estimador da tend√™ncia temporal $\delta_T$ √© dado por
$$ \delta_T = g_{\delta} b^* = g_{\delta} (Y_T^{-1} Y_T  b^*)  \approx g_{\delta} Y_T^{-1} N( \beta^*,  \sigma^2 Q^{*-1})  $$
II. Da mesma forma, o estimador do intercepto $\alpha_T$ √© dado por
$$ \alpha_T = g_{\alpha} b^* = g_{\alpha} (Y_T^{-1} Y_T  b^*)  \approx g_{\alpha} Y_T^{-1} N( \beta^*,  \sigma^2 Q^{*-1})  $$
III. Observamos que a matriz $Q^*$ de [16.3.15] tem uma estrutura bloco diagonal. O bloco superior $p \times p$ cont√©m as covari√¢ncias relacionadas aos componentes AR estacion√°rios, enquanto o bloco inferior $2 \times 2$ cont√©m as vari√¢ncias da constante e da tend√™ncia temporal. Esta estrutura implica que a covari√¢ncia entre os erros que multiplicam os componentes AR estacion√°rios √© zero com os erros que multiplicam a constante e a tend√™ncia temporal.
IV. Assim, as distribui√ß√µes limite dos estimadores s√£o assintoticamente independentes, o que implica que os pr√≥prios estimadores tamb√©m s√£o assintoticamente independentes.
‚ñ†

Este teorema, de grande import√¢ncia pr√°tica, estabelece a independ√™ncia assint√≥tica entre os estimadores dos par√¢metros estacion√°rios e o par√¢metro da tend√™ncia. Essa propriedade simplifica os testes de hip√≥teses conjuntas e permite analisar os par√¢metros de forma mais isolada.

**Corol√°rio 3.1** Sob as mesmas condi√ß√µes do Teorema 3, os estimadores dos par√¢metros autorregressivos ($\phi_1, \phi_2, \dots, \phi_p$) s√£o assintoticamente independentes entre si, e assintoticamente independentes dos estimadores do intercepto e da tend√™ncia.
*Proof:*
I. A estrutura bloco diagonal de $Q^*$ implica que a covari√¢ncia assint√≥tica entre os estimadores de coeficientes AR distintos √© zero.
II. A estrutura da matriz $Y_T$ introduz fatores de escala que garantem uma distribui√ß√£o assint√≥tica bem definida.
III. Esta diagonalidade de bloco, conforme mostrado na prova do Teorema 3, resulta em independ√™ncia assint√≥tica.
IV. Portanto, os estimadores dos par√¢metros AR, intercepto e tend√™ncia s√£o todos assintoticamente independentes.
‚ñ†

This corollary is a direct consequence of the block diagonal structure of the $Q^*$ matrix. It provides crucial simplifications for statistical inference, allowing for the separate analysis of individual autoregressive parameters.

### Conclus√£o

Este cap√≠tulo detalhou a distribui√ß√£o assint√≥tica dos estimadores OLS em modelos AR com tend√™ncia temporal determin√≠stica. A transforma√ß√£o de Sims, Stock e Watson [^1] desempenha um papel crucial ao separar os componentes com diferentes taxas de converg√™ncia e permitir a obten√ß√£o de distribui√ß√µes assint√≥ticas n√£o degeneradas. A matriz $Y_T$ e a estrutura da matriz $Q^*$  s√£o fundamentais para entender o comportamento assint√≥tico dos estimadores e para a realiza√ß√£o de testes de hip√≥teses. A independ√™ncia assint√≥tica entre os estimadores dos par√¢metros estacion√°rios e o par√¢metro da tend√™ncia, derivada neste cap√≠tulo, simplifica a an√°lise estat√≠stica e permite obter infer√™ncias mais precisas. Os resultados apresentados s√£o cruciais para o desenvolvimento de testes de hip√≥teses v√°lidos e para a compreens√£o das propriedades dos modelos de s√©ries temporais com tend√™ncia e componentes autorregressivos [^1].

### Refer√™ncias
[^1]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113‚Äì44.
[^2]: Se√ß√£o 16.3 do texto original.
[^3]: Se√ß√£o 16.3 do texto original.
<!-- END -->
