## Testes de Hip√≥teses em Modelos Autorregressivos com Tend√™ncia Temporal Determin√≠stica

### Introdu√ß√£o
Como discutido nas se√ß√µes anteriores, a infer√™ncia assint√≥tica para modelos autorregressivos com tend√™ncia temporal determin√≠stica exige uma transforma√ß√£o dos regressores para acomodar as diferentes taxas de converg√™ncia dos estimadores [^1]. Ap√≥s essa transforma√ß√£o, podemos aplicar as ferramentas de infer√™ncia assint√≥tica para obter as distribui√ß√µes limites dos estimadores transformados e, por meio dessas, as distribui√ß√µes dos estimadores originais [^1]. Nesta se√ß√£o, exploraremos a implementa√ß√£o de testes de hip√≥teses para esses modelos, demonstrando que os testes usuais de Wald, constru√≠dos a partir dos estimadores originais, mant√™m sua validade assint√≥tica ap√≥s a transforma√ß√£o. Esta se√ß√£o se baseia diretamente nas discuss√µes de infer√™ncia assint√≥tica sobre estimadores OLS em modelos com tend√™ncia determin√≠stica e modelos AR com tend√™ncia discutidos anteriormente [^1].

### Testes de Hip√≥teses: Uma Abordagem Geral
Consideremos o modelo autorregressivo geral com tend√™ncia temporal determin√≠stica, como definido anteriormente [^1]:
$$ y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t $$
Em forma matricial, este modelo √© expresso como:
$$ y_t = x_t'\beta + \epsilon_t $$
onde $x_t$ √© o vetor de regressores e $\beta$ o vetor de par√¢metros. J√° vimos que podemos transformar esse modelo usando a abordagem de Sims, Stock e Watson [^1, ^3, ^4]:
$$ y_t = x_t^{*'}\beta^* + \epsilon_t $$
onde $x_t^* = Gx_t$ e $\beta^* = (G')^{-1}\beta$, onde $G$ √© uma matriz de transforma√ß√£o adequada [^1].

Desejamos testar hip√≥teses da forma:
$$ H_0: R\beta = r $$
onde $R$ √© uma matriz de restri√ß√µes e $r$ um vetor de constantes. No contexto transformado, a hip√≥tese correspondente √©
$$ H_0: R^*\beta^* = r $$
onde $R^* = RG'$. Como demonstrado na se√ß√£o anterior [^1], o estimador OLS transformado $b^*$ √© dado por:
$$ b^* = \left(\sum_{t=1}^T x_t^*x_t^{*'}\right)^{-1} \left(\sum_{t=1}^T x_t^* y_t\right) = (G')^{-1} b $$
onde $b$ √© o estimador OLS do modelo original.

A estat√≠stica de teste de Wald para testar $H_0: R\beta = r$ √© dada por:
$$ \chi^2_T = (Rb - r)' \left[ R \left( \sum_{t=1}^T x_t x_t' \right)^{-1} R' \right]^{-1} (Rb - r) $$
onde $s^2$ √© o estimador da vari√¢ncia do erro do modelo original. Esta estat√≠stica tem distribui√ß√£o assint√≥tica $\chi^2(m)$ sob a hip√≥tese nula, onde $m$ √© o n√∫mero de restri√ß√µes (o n√∫mero de linhas de $R$). No modelo transformado, a estat√≠stica de Wald correspondente √©:
$$ \chi^2_T^* = (R^*b^* - r)' \left[ R^* \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} R^{*'} \right]^{-1} (R^*b^* - r) $$
Como $b^* = (G')^{-1} b$ e $R^* = RG'$, podemos mostrar que $\chi^2_T = \chi^2_T^*$ [^1]. Isso implica que, mesmo que estejamos trabalhando com as estimativas do modelo original, a distribui√ß√£o assint√≥tica do teste de hip√≥teses √© a mesma que se trabalh√°ssemos com os estimadores transformados.

> üí° **Exemplo Num√©rico:** Consideremos um modelo AR(1) com tend√™ncia, onde $p=1$. A hip√≥tese nula √© que a tend√™ncia √© zero, i.e., $H_0: \delta = 0$. Neste caso, temos $R = [0, 0, 1]$ e $r = 0$, e o vetor de par√¢metros √© $\beta = [\phi_1, \alpha, \delta]'$. Vamos simular um conjunto de dados para este modelo.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
>
> np.random.seed(42) # Setting the seed for reproducibility
> T = 100
> phi1 = 0.7
> alpha = 2
> delta = 0.1
> errors = np.random.normal(0, 1, T)
> y = np.zeros(T)
> y[0] = alpha / (1-phi1) + errors[0]
> for t in range(1, T):
>     y[t] = alpha + delta * t + phi1 * y[t-1] + errors[t]
>
> data = pd.DataFrame({'y':y, 't':np.arange(1, T+1)})
> data['y_lag1'] = data['y'].shift(1)
> data = data.dropna()
> X = data[['y_lag1','t']]
> X = sm.add_constant(X)
> y = data['y']
>
> model = sm.OLS(y, X)
> results = model.fit()
> print(results.summary())
> ```
> Usando o estimador OLS $b = [\hat{\phi}_1, \hat{\alpha}, \hat{\delta}]'$, a estat√≠stica de Wald para testar a hip√≥tese √©:
> $$ \chi^2_T = (0, 0, 1) \begin{bmatrix} \hat{\phi}_1 \\ \hat{\alpha} \\ \hat{\delta} \end{bmatrix} \left[ (0,0,1)\left(\sum_{t=1}^T x_t x_t'\right)^{-1} \begin{bmatrix}0 \\0 \\1\end{bmatrix} \right]^{-1} (0, 0, 1) \begin{bmatrix} \hat{\phi}_1 \\ \hat{\alpha} \\ \hat{\delta} \end{bmatrix} = \frac{\hat{\delta}^2}{\text{Var}(\hat{\delta})} $$
> onde $\text{Var}(\hat{\delta})$ √© o elemento (3,3) da matriz $\left( \sum_{t=1}^T x_t x_t' \right)^{-1}$. Esta estat√≠stica pode ser computada diretamente usando as estimativas e o c√°lculo da matriz de vari√¢ncia-covari√¢ncia dos coeficientes, sem a necessidade de transformar os regressores. No entanto, para a infer√™ncia assint√≥tica, devemos utilizar a matriz de proje√ß√£o na forma correta para obter a distribui√ß√£o limite desejada.
>
> ```python
> R = np.array([0, 0, 1])
> b = results.params
> cov_matrix = results.cov_params()
> var_delta = cov_matrix[2,2]
> wald_stat = (R @ b)**2 / (R @ cov_matrix @ R.T)
> print(f"Wald statistic: {wald_stat:.4f}")
> p_value = 1 - stats.chi2.cdf(wald_stat, 1)
> print(f"P-value: {p_value:.4f}")
> ```
>
> A sa√≠da do modelo mostra o valor de $\hat{\delta}$, seu erro padr√£o e a estat√≠stica t. O teste de Wald calculado manualmente mostra que o resultado √© equivalente a fazer $\hat{\delta}^2 / \text{Var}(\hat{\delta})$, o qual segue uma distribui√ß√£o $\chi^2(1)$ sob a hip√≥tese nula. O valor p indica se rejeitamos a hip√≥tese nula de que $\delta = 0$. No c√≥digo, o valor da estat√≠stica de Wald e o p-valor s√£o calculados e mostrados para este exemplo.

**Proposi√ß√£o 1:** A matriz de informa√ß√£o do modelo transformado e do modelo original est√£o relacionadas pela transforma√ß√£o $G$.
*Prova:*
A matriz de informa√ß√£o do modelo original √© dada por $\sum_{t=1}^T x_t x_t'$. A matriz de informa√ß√£o do modelo transformado √© $\sum_{t=1}^T x_t^* x_t^{*'}$. Como $x_t^* = Gx_t$, temos:
$$\sum_{t=1}^T x_t^* x_t^{*'} = \sum_{t=1}^T (Gx_t)(Gx_t)' = \sum_{t=1}^T G x_t x_t' G' = G \left( \sum_{t=1}^T x_t x_t' \right) G' $$
Portanto, a matriz de informa√ß√£o do modelo transformado √© obtida pela pr√© e p√≥s multiplica√ß√£o da matriz de informa√ß√£o do modelo original pela transforma√ß√£o $G$ e sua transposta, respectivamente. $\blacksquare$

### Implementa√ß√£o Computacional da Estat√≠stica de Wald
A estat√≠stica de Wald para testar a hip√≥tese $H_0: R\beta = r$ pode ser escrita como [^1]:
$$ \chi^2_T = (Rb - r)' \left[ R(X'X)^{-1}R' \right]^{-1} (Rb - r) $$
onde $X$ √© a matriz com as observa√ß√µes dos regressores no modelo original.  Podemos reescrever a estat√≠stica de Wald usando a transforma√ß√£o de Sims, Stock e Watson:
$$ \chi^2_T = (R^*b^* - r)' \left[ R^* (X^{*'} X^*)^{-1} R^{*'} \right]^{-1} (R^*b^* - r) $$
onde $X^* = XG'$,  $b^* = (G')^{-1}b$ e $R^* = RG'$.  As estat√≠sticas $\chi^2_T$ e $\chi^2_T^*$ s√£o numericamente id√™nticas, como demonstrado anteriormente [^1]. No entanto, do ponto de vista assint√≥tico, algumas representa√ß√µes s√£o mais convenientes do que outras, especialmente quando os par√¢metros t√™m diferentes taxas de converg√™ncia.
A implementa√ß√£o computacional deve levar em considera√ß√£o a taxa de converg√™ncia de cada par√¢metro, escalando os estimadores apropriadamente. Isso significa que, ao realizar testes de hip√≥tese, √© importante usar a distribui√ß√£o limite correta, que √© obtida por meio do escalonamento adequado das estat√≠sticas do teste [^1].

**Lema 1:** O estimador OLS no modelo transformado, $b^*$, √© consistente para $\beta^*$.
*Prova:*
I.   O estimador OLS no modelo transformado √© dado por:
$$ b^* = \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* y_t $$
II.  Substituindo $y_t = x_t^{*'} \beta^* + \epsilon_t$, temos:
$$ b^* = \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* (x_t^{*'} \beta^* + \epsilon_t) $$
$$ b^* = \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* x_t^{*'} \beta^* +  \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* \epsilon_t $$
$$ b^* = \beta^* +  \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* \epsilon_t $$
III. Para mostrar consist√™ncia, precisamos mostrar que o segundo termo converge para zero em probabilidade quando T tende ao infinito. Sob as condi√ß√µes usuais para a consist√™ncia de estimadores OLS, temos que $\left( \frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'} \right)$ converge para uma matriz definida positiva e $\left( \frac{1}{T} \sum_{t=1}^T x_t^* \epsilon_t \right)$ converge para zero em probabilidade. Portanto:
$$ \text{plim} \left[ \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* \epsilon_t \right] = \text{plim} \left[ \left( \frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \frac{1}{T} \sum_{t=1}^T x_t^* \epsilon_t \right] = 0 $$
IV. Consequentemente, $b^*$ √© consistente para $\beta^*$. $\blacksquare$

### Testes de Hip√≥teses Conjuntas e suas Implica√ß√µes
Na pr√°tica, muitas vezes estamos interessados em testar hip√≥teses conjuntas sobre m√∫ltiplos par√¢metros, tais como:
$$ H_0: \delta = 0 \quad \text{e} \quad \phi_1 = 0 $$
ou
$$ H_0: \phi_1 = \phi_2 = \ldots = \phi_p = 0 $$
Nesses casos, a matriz de restri√ß√µes $R$ ter√° m√∫ltiplas linhas, cada uma representando uma restri√ß√£o diferente. O teste de Wald continuar√° sendo v√°lido, mas agora a estat√≠stica de teste ser√° distribu√≠da assintoticamente como uma $\chi^2(m)$, onde $m$ √© o n√∫mero de restri√ß√µes [^1].
A utiliza√ß√£o do teste de Wald para hip√≥teses conjuntas √© fundamental para avaliar a import√¢ncia de diferentes componentes do modelo, como a signific√¢ncia da tend√™ncia temporal ou a relev√¢ncia das defasagens do processo autorregressivo. Em particular, a transforma√ß√£o de Sims, Stock e Watson [^1, ^3, ^4] garante que os resultados assint√≥ticos dos testes de hip√≥teses sejam v√°lidos, mesmo quando h√° diferentes taxas de converg√™ncia entre os par√¢metros.

> üí° **Exemplo Num√©rico:**  Consideremos o teste de hip√≥tese conjunta de que a tend√™ncia temporal e o primeiro coeficiente autorregressivo s√£o ambos iguais a zero. No modelo AR(1) com tend√™ncia, a hip√≥tese √© dada por $H_0: \delta = 0, \phi_1 = 0$, onde o vetor de par√¢metros √© $\beta = [\phi_1, \alpha, \delta]'$
>
> A matriz $R$ e o vetor $r$ s√£o dados por:
> $$ R = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix}, \quad r = \begin{bmatrix} 0 \\ 0 \end{bmatrix} $$
> A estat√≠stica de Wald para esta hip√≥tese √©:
> $$ \chi^2_T = (R\hat{b} - r)' [R(X'X)^{-1}R']^{-1} (R\hat{b} - r) $$
> que pode ser calculada como:
> $$ \chi^2_T = \begin{bmatrix} \hat{\phi}_1 \\ \hat{\delta} \end{bmatrix}' \left[ \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix} (X'X)^{-1} \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \end{bmatrix} \right]^{-1} \begin{bmatrix} \hat{\phi}_1 \\ \hat{\delta} \end{bmatrix} $$
> A estat√≠stica de teste resultante segue uma distribui√ß√£o $\chi^2(2)$ assintoticamente. Assim, a estat√≠stica de teste pode ser utilizada da forma usual, mesmo que os coeficientes tenham diferentes taxas de converg√™ncia.
>
>  ```python
> R = np.array([[1, 0, 0], [0, 0, 1]])
> b = results.params
> cov_matrix = results.cov_params()
> wald_stat = (R @ b).T @ np.linalg.inv(R @ cov_matrix @ R.T) @ (R @ b)
> print(f"Wald statistic for joint hypothesis: {wald_stat:.4f}")
> p_value = 1 - stats.chi2.cdf(wald_stat, 2)
> print(f"P-value for joint hypothesis: {p_value:.4f}")
> ```
>
> No c√≥digo, a matriz R, os coeficientes estimados e a matriz de covari√¢ncia s√£o utilizados para calcular a estat√≠stica de Wald e o p-valor. O p-valor do teste conjunto pode ser usado para verificar se devemos rejeitar a hip√≥tese conjunta.
>
> Observe que a estat√≠stica de Wald, calculada usando a matriz de covari√¢ncia dos estimadores, fornece um resultado assintoticamente v√°lido. A transforma√ß√£o de Sims, Stock e Watson garante que as propriedades assint√≥ticas dos testes sejam v√°lidas, mesmo na presen√ßa de taxas de converg√™ncia diferentes.

**Lema 2:** A estat√≠stica de teste de Wald, $\chi^2_T$, √© numericamente id√™ntica √† estat√≠stica de teste $\chi^2_T^*$ constru√≠da a partir do modelo transformado.

*Prova:*
I.   A estat√≠stica de Wald do modelo original √© dada por
 $$
\chi^2_T = (R\hat{b} - r)' \left[R (X'X)^{-1} R'\right]^{-1} (R\hat{b} - r)
$$
II. A estat√≠stica de Wald do modelo transformado √© dada por
$$
\chi^2_T^* = (R^* \hat{b}^* - r)' \left[R^* (X^{*'} X^*)^{-1} R^{*'}\right]^{-1} (R^* \hat{b}^* - r)
$$
III. Sabemos que $\hat{b}^* = (G')^{-1} \hat{b}$, $R^* = R G'$ e $X^* = XG'$. Substituindo essas express√µes em $\chi^2_T^*$, temos
$$
\chi^2_T^* = (R G' (G')^{-1} \hat{b} - r)' \left[R G' ( (X G')' (X G') )^{-1} (R G')' \right]^{-1} (R G' (G')^{-1} \hat{b} - r)
$$
$$
\chi^2_T^* = (R \hat{b} - r)' \left[R G' (G' X'X G')^{-1} G R' \right]^{-1} (R \hat{b} - r)
$$
IV. Usando a propriedade da inversa de um produto, $(ABC)^{-1} = C^{-1} B^{-1} A^{-1}$, obtemos:
$$
\chi^2_T^* = (R \hat{b} - r)' \left[R G' (G')^{-1} (X'X)^{-1} (G')^{-1} G R' \right]^{-1} (R \hat{b} - r)
$$
$$
\chi^2_T^* = (R \hat{b} - r)' \left[R  (X'X)^{-1}  R' \right]^{-1} (R \hat{b} - r)
$$
V. Portanto, $\chi^2_T^* = \chi^2_T$, demonstrando que as duas estat√≠sticas s√£o numericamente id√™nticas. $\blacksquare$

**Teorema 1:** A estat√≠stica de teste de Wald, $\chi^2_T$, converge em distribui√ß√£o para uma qui-quadrado com $m$ graus de liberdade sob a hip√≥tese nula, onde $m$ √© o n√∫mero de restri√ß√µes.

*Prova:*
I.   Sob a hip√≥tese nula $H_0: R\beta = r$, temos que $R \hat{b} - r \xrightarrow{d} N(0, R \text{Var}(\hat{b}) R')$.
II.  A estat√≠stica de Wald pode ser escrita como
 $$ \chi^2_T = (R\hat{b} - r)' \left[R \text{Var}(\hat{b}) R'\right]^{-1} (R\hat{b} - r) $$
III. Sabemos que $\text{Var}(\hat{b}) = (X'X)^{-1}s^2$, onde $s^2$ √© o estimador da vari√¢ncia do erro. Substituindo, temos
$$ \chi^2_T = (R\hat{b} - r)' \left[R (X'X)^{-1} s^2 R'\right]^{-1} (R\hat{b} - r) $$
IV. Sob as condi√ß√µes usuais,  $s^2 \xrightarrow{p} \sigma^2$, onde $\sigma^2$ √© a verdadeira vari√¢ncia do erro, logo
$$ \chi^2_T = (R\hat{b} - r)' \left[R (X'X)^{-1} R'\right]^{-1} (R\hat{b} - r) $$
V. Pela teoria assint√≥tica, sabemos que $(R\hat{b} - r)' \left[R (X'X)^{-1} R'\right]^{-1} (R\hat{b} - r)$ converge em distribui√ß√£o para uma qui-quadrado com $m$ graus de liberdade. Este resultado decorre do fato que $R \hat{b}$ √© assintoticamente normal e que a estat√≠stica de Wald se baseia na dist√¢ncia de Mahalanobis entre $R\hat{b}$ e seu valor sob a hip√≥tese nula, $r$. $\blacksquare$

### Conclus√£o
A implementa√ß√£o de testes de hip√≥teses em modelos autorregressivos com tend√™ncia temporal determin√≠stica exige uma compreens√£o clara das diferentes taxas de converg√™ncia dos estimadores. A transforma√ß√£o de Sims, Stock e Watson [^1, ^3, ^4] permite a constru√ß√£o de testes de hip√≥teses v√°lidos assintoticamente, mesmo quando as taxas de converg√™ncia variam entre os par√¢metros. A estat√≠stica de Wald, calculada sobre as estimativas do modelo original, preserva suas propriedades assint√≥ticas ap√≥s a transforma√ß√£o, desde que as distribui√ß√µes limites sejam consideradas corretamente. A an√°lise assint√≥tica mostra que as estat√≠sticas de teste podem ser constru√≠das sem necessitar a transforma√ß√£o dos regressores para fins de c√°lculo, mas a transforma√ß√£o √© fundamental para analisar as propriedades assint√≥ticas dos testes e obter a distribui√ß√£o limite correta. O presente t√≥pico se baseia nas discuss√µes de infer√™ncia assint√≥tica sobre estimadores OLS em modelos com tend√™ncia determin√≠stica e modelos AR com tend√™ncia discutidos anteriormente [^1], al√©m de estabelecer os fundamentos para a implementa√ß√£o pr√°tica de testes de hip√≥teses sobre esses modelos. Os testes de hip√≥teses podem ser implementados de forma computacional usando a formula√ß√£o original do modelo e suas propriedades assint√≥ticas s√£o v√°lidas ap√≥s a transforma√ß√£o, como demonstrado aqui.

### Refer√™ncias
[^1]: Cap√≠tulo 16 do texto base, "Processes with Deterministic Time Trends".
[^3]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113-44.
[^4]: Fuller, Wayne A. 1976. Introduction to Statistical Time Series. New York: Wiley.
```python
import numpy as np
import pandas as pd
import statsmodels.api as sm
from scipy import stats

np.random.seed(42) # Setting the seed for reproducibility
T = 100
phi1 = 0.7
alpha = 2
delta = 0.1
errors = np.random.normal(0, 1, T)
y = np.zeros(T)
y[0] = alpha / (1-phi1) + errors[0]
for t in range(1, T):
    y[t] = alpha + delta * t + phi1 * y[t-1] + errors[t]

data = pd.DataFrame({'y':y, 't':np.arange(1, T+1)})
data['y_lag1'] = data['y'].shift(1)
data = data.dropna()
X = data[['y_lag1','t']]
X = sm.add_constant(X)
y = data['y']

model = sm.OLS(y, X)
results = model.fit()
print(results.summary())

R = np.array([0, 0, 1])
b = results.params
cov_matrix = results.cov_params()
var_delta = cov_matrix[2,2]
wald_stat = (R @ b)**2 / (R @ cov_matrix @ R.T)
print(f"Wald statistic: {wald_stat:.4f}")
p_value = 1 - stats.chi2.cdf(wald_stat, 1)
print(f"P-value: {p_value:.4f}")

R = np.array([[1, 0, 0], [0, 0, 1]])
b = results.params
cov_matrix = results.cov_params()
wald_stat = (R @ b).T @ np.linalg.inv(R @ cov_matrix @ R.T) @ (R @ b)
print(f"Wald statistic for joint hypothesis: {wald_stat:.4f}")
p_value = 1 - stats.chi2.cdf(wald_stat, 2)
print(f"P-value for joint hypothesis: {p_value:.4f}")
```
<!-- END -->
