## Infer√™ncia Assint√≥tica em Modelos AR com Tend√™ncia: Linearidade e Distribui√ß√£o Gaussiana

### Introdu√ß√£o
Este cap√≠tulo continua a an√°lise de modelos autorregressivos (AR) com tend√™ncias temporais determin√≠sticas, focando na linearidade dos estimadores em rela√ß√£o aos regressores transformados e na distribui√ß√£o gaussiana assint√≥tica dos estimadores de m√≠nimos quadrados ordin√°rios (OLS). Como visto anteriormente, a transforma√ß√£o de regressores, proposta por Sims, Stock e Watson [^1], permite analisar os componentes dos modelos com diferentes taxas de converg√™ncia, o que facilita a infer√™ncia estat√≠stica [^2, ^3]. O objetivo principal √© demonstrar como essa abordagem leva √† distribui√ß√£o gaussiana assint√≥tica dos estimadores, ap√≥s o devido reescalonamento, e como isso permite realizar testes de hip√≥teses v√°lidos.

### Conceitos Fundamentais
Retomando o modelo autorregressivo com tend√™ncia temporal [16.3.1]:
$$ y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t $$
onde $\epsilon_t$ √© um ru√≠do branco i.i.d. com m√©dia zero, vari√¢ncia $\sigma^2$ e quarto momento finito. As ra√≠zes da equa√ß√£o caracter√≠stica est√£o fora do c√≠rculo unit√°rio. A transforma√ß√£o de regressores [16.3.2] leva ao modelo transformado [16.3.3]:
$$ y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \ldots + \phi_p^* y_{t-p}^* + \epsilon_t $$
onde
$$ \alpha^* = \alpha(1 + \phi_1 + \phi_2 + \ldots + \phi_p) - \delta(\phi_1 + 2\phi_2 + \ldots + p\phi_p) $$
$$ \delta^* = \delta(1 + \phi_1 + \phi_2 + \ldots + \phi_p) $$
e
$$ y_{t-j}^* = y_{t-j} - \alpha - \delta(t-j), \quad j = 1, 2, \ldots, p $$
Este modelo transformado √© crucial para analisar os estimadores dos par√¢metros de forma mais eficiente. A transforma√ß√£o separa componentes com diferentes taxas de converg√™ncia: os termos $y_{t-j}^*$ s√£o vari√°veis estacion√°rias de m√©dia zero, enquanto os termos constante e de tend√™ncia convergem em taxas distintas [^1, ^2].

> üí° **Exemplo Num√©rico:** Considere um modelo AR(2) com tend√™ncia:
> $y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \epsilon_t$.  Ap√≥s a transforma√ß√£o de Sims, Stock e Watson, este modelo √© expresso como:
> $y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \epsilon_t$
> Os termos $\alpha^*$, $\delta^*$ e $y_{t-j}^*$ s√£o definidos em fun√ß√£o dos par√¢metros do modelo original. Por exemplo, se  $\alpha = 1$, $\delta=0.5$, $\phi_1 = 0.4$ e $\phi_2 = 0.2$, temos:
>
>$\alpha^* = 1(1+0.4+0.2) - 0.5(0.4+2(0.2)) = 1.6 - 0.4 = 1.2$
>
>$\delta^* = 0.5(1+0.4+0.2) = 0.8$
>
>e $y_{t-1}^* = y_{t-1} - 1 - 0.5(t-1)$, $y_{t-2}^* = y_{t-2} - 1 - 0.5(t-2)$. Assim, podemos reescrever o modelo original como:
>
> $y_t = 1.2 + 0.8t + 0.4y_{t-1}^* + 0.2y_{t-2}^* + \epsilon_t$.
> O modelo transformado separa os componentes com diferentes taxas de converg√™ncia.

Em forma matricial, o modelo original √© [16.3.5]:
$$ y_t = x_t'\beta + \epsilon_t $$
e o modelo transformado √© [16.3.7]:
$$ y_t = [x_t^*]' \beta^* + \epsilon_t $$
onde $x_t^* = G x_t$ e $\beta^* = (G')^{-1} \beta$. A matriz $G'$ √© definida como [16.3.8]:
$$ G' = \begin{bmatrix}
1 & 0 & 0 & \ldots & 0 & 0 & 0 \\
0 & 1 & 0 & \ldots & 0 & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & 1 & 0 & 0 \\
-\alpha + \delta & -\alpha + 2\delta & \ldots & -\alpha + p\delta & 1 & 0 \\
-\delta & -\delta & \ldots & -\delta & 0 & 1
\end{bmatrix} $$
Essa matriz √© crucial para transformar os regressores originais em vari√°veis estacion√°rias, uma constante e uma tend√™ncia temporal [^2].

> üí° **Exemplo Num√©rico:**  Vamos ilustrar a matriz $G'$ para um modelo AR(1) com tend√™ncia. O modelo original √© $y_t = \alpha + \delta t + \phi_1 y_{t-1} + \epsilon_t$. O vetor de regressores $x_t$ √© $[1, t, y_{t-1}]^T$, e o vetor de par√¢metros √© $\beta = [\alpha, \delta, \phi_1]^T$. A matriz $G'$ correspondente √©:
>
>$$ G' = \begin{bmatrix} 1 & 0 & 0 \\ -\alpha + \delta & 1 & 0 \\ -\delta & 0 & 1 \end{bmatrix} $$
>
>Se $\alpha = 2$ e $\delta = 0.5$, a matriz $G'$ torna-se:
>
>$$ G' = \begin{bmatrix} 1 & 0 & 0 \\ -1.5 & 1 & 0 \\ -0.5 & 0 & 1 \end{bmatrix} $$
>
>Esta matriz transforma o vetor de regressores $x_t$ em $x_t^* = G x_t$, que √© usado para a estima√ß√£o no modelo transformado.

O estimador OLS para $\beta^*$ √© dado por [16.3.11]:
$$ b^* = [\sum_{t=1}^T x_t^* x_t^{*'}]^{-1} \sum_{t=1}^T x_t^* y_t = (G')^{-1} b $$
onde $b$ √© o estimador OLS para o modelo original. Este estimador √© uma fun√ß√£o linear dos regressores transformados.

### Linearidade dos Estimadores
A linearidade do estimador $b^*$ em rela√ß√£o aos regressores transformados, $x_t^*$, √© fundamental para a an√°lise da sua distribui√ß√£o assint√≥tica. Para verificar essa linearidade, reescrevemos a equa√ß√£o do estimador:
$$ b^* = [\sum_{t=1}^T x_t^* x_t^{*'}]^{-1} \sum_{t=1}^T x_t^* y_t $$
Substituindo $y_t = [x_t^*]' \beta^* + \epsilon_t$ no somat√≥rio, obtemos:
$$ b^* = [\sum_{t=1}^T x_t^* x_t^{*'}]^{-1} \sum_{t=1}^T x_t^* ([x_t^*]' \beta^* + \epsilon_t) $$
$$ b^* = [\sum_{t=1}^T x_t^* x_t^{*'}]^{-1} \sum_{t=1}^T x_t^* [x_t^*]' \beta^* + [\sum_{t=1}^T x_t^* x_t^{*'}]^{-1} \sum_{t=1}^T x_t^* \epsilon_t $$
$$ b^* = \beta^* + [\sum_{t=1}^T x_t^* x_t^{*'}]^{-1} \sum_{t=1}^T x_t^* \epsilon_t $$
Este resultado mostra que o estimador $b^*$ √© uma combina√ß√£o linear do valor verdadeiro $\beta^*$ e um termo que depende dos erros $\epsilon_t$ e dos regressores transformados $x_t^*$. A linearidade em rela√ß√£o aos regressores transformados permite aplicar o Teorema do Limite Central (TLC) para derivar a distribui√ß√£o assint√≥tica dos estimadores, que √© o objetivo da pr√≥xima se√ß√£o.

> üí° **Exemplo Num√©rico:** Suponha um modelo AR(1) com tend√™ncia linear, onde ap√≥s a transforma√ß√£o obtemos o vetor de regressores transformados como $x_t^* = [y_{t-1}^*, 1, t]^T$. O estimador OLS para este modelo √© dado por:
>
> $b^* = [\sum_{t=1}^T x_t^* x_t^{*'}]^{-1} \sum_{t=1}^T x_t^* y_t$
>
> Note que o vetor $x_t^*$ √© composto pela vari√°vel autorregressiva transformada, um intercepto e um termo de tend√™ncia. O estimador OLS $b^*$, por sua vez, √© uma combina√ß√£o linear desses regressores transformados e dos erros do modelo. Essa linearidade √© fundamental para estabelecer a distribui√ß√£o gaussiana assint√≥tica dos estimadores.
>
> Para ilustrar numericamente, suponha que temos $T=100$ e que os regressores transformados e os valores de $y_t$ resultam nas seguintes somas:
>
> $\sum_{t=1}^T x_t^* x_t^{*'} = \begin{bmatrix} 50 & 0 & 0 \\ 0 & 100 & 5000 \\ 0 & 5000 & 338350 \end{bmatrix}$
>
> $\sum_{t=1}^T x_t^* y_t = \begin{bmatrix} 25 \\ 150 \\ 10000 \end{bmatrix}$
>
>  Calculando o inverso da matriz de regressores e multiplicando pelo vetor de $x_t^*y_t$, obtemos o vetor de estimativas $b^*$:
>
>  $b^* = \begin{bmatrix} 50 & 0 & 0 \\ 0 & 100 & 5000 \\ 0 & 5000 & 338350 \end{bmatrix}^{-1} \begin{bmatrix} 25 \\ 150 \\ 10000 \end{bmatrix}  \approx \begin{bmatrix} 0.5 \\ 0.075 \\ 0.015 \end{bmatrix}$
>
> Se  $\beta^* = \begin{bmatrix} 0.4 \\ 0.1 \\ 0.01 \end{bmatrix}$, ent√£o o erro de estimativa  $b^* - \beta^*= [\sum_{t=1}^T x_t^* x_t^{*'}]^{-1} \sum_{t=1}^T x_t^* \epsilon_t \approx \begin{bmatrix} 0.1 \\ -0.025 \\ 0.005 \end{bmatrix}$
>
> Este exemplo num√©rico ilustra como $b^*$ √© linear em fun√ß√£o dos regressores transformados e dos erros, e como podemos expressar o erro de estima√ß√£o.

**Proposi√ß√£o 1** A linearidade do estimador $b^*$ pode ser expressa de forma equivalente como:
$$b^* = \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* y_t = \beta^* + \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* \epsilon_t $$
Esta formula√ß√£o explicita que o estimador $b^*$ √© composto pelo valor real $\beta^*$ e um termo de erro que depende da matriz de covari√¢ncia dos regressores transformados e dos res√≠duos.

### Distribui√ß√£o Gaussiana Assint√≥tica dos Estimadores OLS
A distribui√ß√£o assint√≥tica dos estimadores OLS transformados, $b^*$, √© dada por [16.3.13]:
$$ Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1}) $$
onde $Y_T$ √© a matriz de escalonamento [16.3.14]:
$$
Y_T = \begin{bmatrix}
\sqrt{T} & 0 & 0 & \cdots & 0 & 0 \\
0 & \sqrt{T} & 0 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \cdots & \sqrt{T} & 0 \\
0 & 0 & 0 & \cdots & 0 & \sqrt{T} \\
0 & 0 & 0 & \cdots & 0 & T^{3/2}
\end{bmatrix}
$$
e $Q^*$ √© a matriz de vari√¢ncia assint√≥tica dos regressores transformados [16.3.15]:
$$
Q^* = \begin{bmatrix}
\gamma_{0}^* & \gamma_{1}^* & \gamma_{2}^* & \cdots & \gamma_{p-1}^* & 0 & 0 \\
\gamma_{1}^* & \gamma_{0}^* & \gamma_{1}^* & \cdots & \gamma_{p-2}^* & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
\gamma_{p-1}^* & \gamma_{p-2}^* & \gamma_{p-3}^* & \cdots & \gamma_{0}^* & 0 & 0 \\
0 & 0 & 0 & \cdots & 0 & 1 & 0 \\
0 & 0 & 0 & \cdots & 0 & 0 & 1/3
\end{bmatrix}
$$
Aqui, $\gamma_{j}^* = E(y_t^* y_{t-j}^*)$ s√£o as autocovari√¢ncias da vari√°vel estacion√°ria $y_t^*$. A matriz $Y_T$ reflete as diferentes taxas de converg√™ncia dos estimadores: $\sqrt{T}$ para os coeficientes das vari√°veis estacion√°rias e $T^{3/2}$ para o coeficiente da tend√™ncia temporal [^1, ^2].

> üí° **Exemplo Num√©rico:** Suponha um modelo AR(1) com tend√™ncia linear. Se $\gamma_0^* = 2$, e  $T = 100$  e $\sigma^2 = 0.25$. Ent√£o, a matriz $Y_T$ √© dada por:
>
> $$ Y_T = \begin{bmatrix} 10 & 0 & 0 \\ 0 & 10 & 0 \\ 0 & 0 & 1000 \end{bmatrix} $$
>
>  E a matriz $Q^*$ √©:
> $$Q^* = \begin{bmatrix} 2 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1/3 \end{bmatrix} $$
>
>  A distribui√ß√£o assint√≥tica dos estimadores transformados √©:
>
>  $$Y_T(b^* - \beta^*) \xrightarrow{d} N(0, 0.25 [Q^*]^{-1}) = N(0, \begin{bmatrix} 0.25/2 & 0 & 0 \\ 0 & 0.25 & 0 \\ 0 & 0 & 0.25*3 \end{bmatrix}) = N(0, \begin{bmatrix} 0.125 & 0 & 0 \\ 0 & 0.25 & 0 \\ 0 & 0 & 0.75 \end{bmatrix}) $$
>
> Este exemplo mostra que a vari√¢ncia assint√≥tica do estimador do par√¢metro autoregressivo, ap√≥s o reescalonamento por $\sqrt{T}$, √© 0.125, e que a vari√¢ncia assint√≥tica do par√¢metro da tend√™ncia, ap√≥s o reescalonamento por $T^{3/2}$, √© 0.75. Os estimadores do intercepto tamb√©m convergem com $\sqrt{T}$ e tem vari√¢ncia assint√≥tica de 0.25.

**Prova da Distribui√ß√£o Assint√≥tica (Esbo√ßo)**
I. A partir da linearidade do estimador OLS, sabemos que o erro de estima√ß√£o pode ser expresso como:
$$ b^* - \beta^* = [\sum_{t=1}^T x_t^* x_t^{*'}]^{-1} \sum_{t=1}^T x_t^* \epsilon_t $$
II. Premultiplicando por $Y_T$ e utilizando a lei dos grandes n√∫meros, mostramos que:
$$ [\sum_{t=1}^T \frac{x_t^* x_t^{*'}}{T}] \xrightarrow{p} Q^* $$
III. Aplicando o Teorema do Limite Central para a parte aleat√≥ria, mostramos que:
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^T x_t^* \epsilon_t \xrightarrow{d} N(0, \sigma^2 Q^*) $$
IV. Combinando esses resultados, obtemos:
$$ Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1}) $$
‚ñ†

> üí° **Exemplo Num√©rico:**  Vamos usar um exemplo simples para ilustrar a prova da distribui√ß√£o assint√≥tica. Considere um modelo AR(1) com tend√™ncia, onde $x_t^* = [y_{t-1}^*, 1, t]^T$.  Vamos assumir que $\sigma^2 = 1$, $\gamma_0^* = 1$, e o tamanho da amostra seja $T = 100$.
>
> Passo I:  O erro de estima√ß√£o √© dado por: $b^* - \beta^* = [\sum_{t=1}^T x_t^* x_t^{*'}]^{-1} \sum_{t=1}^T x_t^* \epsilon_t$
>
> Passo II:  A matriz de vari√¢ncia assint√≥tica dos regressores transformados converge em probabilidade:
>  $\frac{1}{T}\sum_{t=1}^T x_t^* x_t^{*'}  \xrightarrow{p} Q^*$.  Podemos ilustrar a converg√™ncia por meio de simula√ß√£o.  Por exemplo, se simulamos os regressores $x_t^* x_t^{*'}$ para $T = 100$ e calculamos a m√©dia, a matriz $\frac{1}{T}\sum_{t=1}^T x_t^* x_t^{*'}$ deve ser pr√≥xima de:
>
>    $Q^* = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1/3 \end{bmatrix}$.
>
> Passo III: A parte aleat√≥ria converge para uma normal: $\frac{1}{\sqrt{T}} \sum_{t=1}^T x_t^* \epsilon_t \xrightarrow{d} N(0, Q^*)$. Simula√ß√µes com amostras finitas e  histogramas aproximar√£o uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia igual a $Q^*$.
>
> Passo IV: Combinando os resultados, obtemos a distribui√ß√£o assint√≥tica final. O reescalonamento pelos fatores em $Y_T$ √© crucial para assegurar a converg√™ncia da distribui√ß√£o para uma normal n√£o-degenerada.

**Lema 1** A converg√™ncia em probabilidade do termo $\frac{1}{T}\sum_{t=1}^T x_t^* x_t^{*'}$ para $Q^*$ √© uma aplica√ß√£o direta da Lei dos Grandes N√∫meros.  Especificamente, se $x_t^* x_t^{*'}$ √© uma sequ√™ncia de vari√°veis aleat√≥rias estacion√°rias e erg√≥dicas, ent√£o:
$$ \frac{1}{T}\sum_{t=1}^T x_t^* x_t^{*'} \xrightarrow{p} E[x_t^* x_t^{*'}] = Q^* $$
Este lema justifica a passagem do somat√≥rio para o valor esperado ao analisarmos o comportamento assint√≥tico dos estimadores.

**Teorema 1.1** (Extens√£o da Distribui√ß√£o Assint√≥tica) Sob as mesmas condi√ß√µes do Teorema da Distribui√ß√£o Assint√≥tica dos Estimadores OLS e a condi√ß√£o adicional de que os erros $\epsilon_t$ sejam independentes e identicamente distribu√≠dos com m√©dia zero, vari√¢ncia $\sigma^2$ e momento de quarta ordem finito, ent√£o a distribui√ß√£o assint√≥tica dos estimadores pode ser expressa como:
$$ \sqrt{T} (b^*_1 - \beta^*_1) \xrightarrow{d} N(0, \sigma^2 [Q^*_1]^{-1}) $$
$$ T^{3/2} (b^*_2 - \beta^*_2) \xrightarrow{d} N(0, \sigma^2 [Q^*_2]^{-1}) $$
onde $b^* = \begin{bmatrix} b^*_1 \\ b^*_2 \end{bmatrix}$ e $\beta^* = \begin{bmatrix} \beta^*_1 \\ \beta^*_2 \end{bmatrix}$, com $\beta^*_1$ representando os par√¢metros dos regressores estacion√°rios e $\beta^*_2$ representando o par√¢metro da tend√™ncia.  $Q^*_1$ e $Q^*_2$ correspondem aos blocos apropriados da matriz $Q^*$. Esta formula√ß√£o explicita a converg√™ncia distinta das partes estacion√°rias e da tend√™ncia do modelo.

### Distribui√ß√£o Assint√≥tica dos Estimadores OLS do Modelo Original
A distribui√ß√£o assint√≥tica dos estimadores OLS do modelo original, $b$, √© obtida atrav√©s da rela√ß√£o $b = G' b^*$ [16.3.12]:
$$ Y_T(b-\beta) = Y_T G' (b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 G' [Q^*]^{-1} G'^{'}) $$
As distribui√ß√µes assint√≥ticas dos estimadores $\hat{\alpha}$ e $\hat{\delta}$ s√£o dadas por [16.3.17]:
$$ \sqrt{T}(\hat{\alpha} - \alpha) \xrightarrow{d} N(0, \sigma^2 g_{\alpha} [Q^*]^{-1} g_{\alpha}^{'}) $$
$$ T^{3/2}(\hat{\delta} - \delta) \xrightarrow{d} N(0, \sigma^2 g_{\delta} [Q^*]^{-1} g_{\delta}^{'}) $$
onde $g_{\alpha}$ e $g_{\delta}$ s√£o vetores de pesos que isolam os componentes $\alpha$ e $\delta$ em $b$, respectivamente.

**Implica√ß√µes para Testes de Hip√≥teses**
A distribui√ß√£o gaussiana assint√≥tica dos estimadores permite a constru√ß√£o de testes de hip√≥teses v√°lidos. Por exemplo, para testar a hip√≥tese nula $H_0: \phi_1 = \phi_{1,0}$ em um modelo AR(1), podemos utilizar a estat√≠stica de teste:
$$ \frac{\hat{\phi}_1 - \phi_{1,0}}{\sqrt{\frac{\sigma^2 \gamma_0^*}{T}}} \xrightarrow{d} N(0,1) $$
onde $\hat{\phi}_1$ √© o estimador OLS do par√¢metro autorregressivo, e $\phi_{1,0}$ √© o valor especificado na hip√≥tese nula. A distribui√ß√£o gaussiana permite calcular o p-valor e comparar com o n√≠vel de signific√¢ncia desejado.

> üí° **Exemplo Num√©rico:** Vamos realizar um teste de hip√≥tese para o coeficiente AR(1) $\phi_1$ em um modelo AR(1) com tend√™ncia. Suponha que a hip√≥tese nula seja $H_0: \phi_1 = 0.6$, e que a estimativa obtida seja $\hat{\phi}_1 = 0.65$. Suponha que a vari√¢ncia dos erros seja $\sigma^2 = 0.4$, a autocovari√¢ncia $\gamma_0^* = 1.2$ e o tamanho da amostra seja $T = 200$. A estat√≠stica de teste √©:
>
> $$ z = \frac{\hat{\phi}_1 - \phi_{1,0}}{\sqrt{\frac{\sigma^2 \gamma_0^*}{T}}} = \frac{0.65 - 0.6}{\sqrt{\frac{0.4 \times 1.2}{200}}} = \frac{0.05}{\sqrt{0.0024}} \approx 1.02 $$
>
>Para um n√≠vel de signific√¢ncia de 5%, o valor cr√≠tico do teste bilateral √© aproximadamente 1.96. Como o valor da estat√≠stica de teste ($1.02$) √© menor do que o valor cr√≠tico (1.96), n√£o rejeitamos a hip√≥tese nula de que $\phi_1 = 0.6$. O p-valor associado a esse teste seria maior que 0.05.

**Teorema da Independ√™ncia Assint√≥tica (Revis√£o)**
Os estimadores dos par√¢metros estacion√°rios, que convergem a uma taxa $\sqrt{T}$, s√£o assintoticamente independentes do estimador do par√¢metro da tend√™ncia, que converge a uma taxa $T^{3/2}$ [^1]. Este teorema, detalhado nos cap√≠tulos anteriores, simplifica a realiza√ß√£o de testes de hip√≥teses conjuntas e permite analisar os par√¢metros de forma mais isolada.

**Observa√ß√£o 1** A independ√™ncia assint√≥tica entre os estimadores dos par√¢metros estacion√°rios e o estimador do par√¢metro da tend√™ncia √© uma consequ√™ncia direta da estrutura da matriz $Y_T$ e da matriz $Q^*$.  A matriz $Y_T$ possui blocos diagonais que escalonam diferentemente os estimadores, e a matriz $Q^*$ possui zeros nas posi√ß√µes de covari√¢ncia entre os regressores estacion√°rios e a tend√™ncia, levando √† independ√™ncia assint√≥tica.

### Conclus√£o
Este cap√≠tulo demonstrou como a transforma√ß√£o de regressores proposta por Sims, Stock e Watson [^1] leva a estimadores OLS lineares em rela√ß√£o aos regressores transformados e como isso, em conjunto com o Teorema do Limite Central, permite obter uma distribui√ß√£o gaussiana assint√≥tica para esses estimadores ap√≥s o devido reescalonamento. A an√°lise da distribui√ß√£o assint√≥tica √© fundamental para infer√™ncia estat√≠stica, como a constru√ß√£o de testes de hip√≥teses v√°lidos sobre os par√¢metros dos modelos AR com tend√™ncia. A independ√™ncia assint√≥tica entre os estimadores, j√° estabelecida em cap√≠tulos anteriores, simplifica a an√°lise e permite obter resultados mais precisos para modelos complexos de s√©ries temporais. A abordagem detalhada e os resultados obtidos fornecem uma base s√≥lida para o estudo de modelos com ra√≠zes unit√°rias nos pr√≥ximos cap√≠tulos [^1].

### Refer√™ncias
[^1]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113‚Äì44.
[^2]: Se√ß√£o 16.3 do texto original.
[^3]: Se√ß√£o 16.1 do texto original.
<!-- END -->
