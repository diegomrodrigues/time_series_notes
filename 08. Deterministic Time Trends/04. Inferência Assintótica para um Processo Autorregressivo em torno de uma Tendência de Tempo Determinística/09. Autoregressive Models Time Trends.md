## InferÃªncia AssintÃ³tica em Modelos AR com TendÃªncia: Linearidade e DistribuiÃ§Ã£o Gaussiana

### IntroduÃ§Ã£o
Este capÃ­tulo continua a anÃ¡lise de modelos autorregressivos (AR) com tendÃªncias temporais determinÃ­sticas, focando na linearidade dos estimadores em relaÃ§Ã£o aos regressores transformados e na distribuiÃ§Ã£o gaussiana assintÃ³tica dos estimadores de mÃ­nimos quadrados ordinÃ¡rios (OLS). Como visto anteriormente, a transformaÃ§Ã£o de regressores, proposta por Sims, Stock e Watson [^1], permite analisar os componentes dos modelos com diferentes taxas de convergÃªncia, o que facilita a inferÃªncia estatÃ­stica [^2, ^3]. O objetivo principal Ã© demonstrar como essa abordagem leva Ã  distribuiÃ§Ã£o gaussiana assintÃ³tica dos estimadores, apÃ³s o devido reescalonamento, e como isso permite realizar testes de hipÃ³teses vÃ¡lidos.

### Conceitos Fundamentais
Retomando o modelo autorregressivo com tendÃªncia temporal [16.3.1]:
$$ y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t $$
onde $\epsilon_t$ Ã© um ruÃ­do branco i.i.d. com mÃ©dia zero, variÃ¢ncia $\sigma^2$ e quarto momento finito. As raÃ­zes da equaÃ§Ã£o caracterÃ­stica estÃ£o fora do cÃ­rculo unitÃ¡rio. A transformaÃ§Ã£o de regressores [16.3.2] leva ao modelo transformado [16.3.3]:
$$ y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \ldots + \phi_p^* y_{t-p}^* + \epsilon_t $$
onde
$$ \alpha^* = \alpha(1 + \phi_1 + \phi_2 + \ldots + \phi_p) - \delta(\phi_1 + 2\phi_2 + \ldots + p\phi_p) $$
$$ \delta^* = \delta(1 + \phi_1 + \phi_2 + \ldots + \phi_p) $$
e
$$ y_{t-j}^* = y_{t-j} - \alpha - \delta(t-j), \quad j = 1, 2, \ldots, p $$
Este modelo transformado Ã© crucial para analisar os estimadores dos parÃ¢metros de forma mais eficiente. A transformaÃ§Ã£o separa componentes com diferentes taxas de convergÃªncia: os termos $y_{t-j}^*$ sÃ£o variÃ¡veis estacionÃ¡rias de mÃ©dia zero, enquanto os termos constante e de tendÃªncia convergem em taxas distintas [^1, ^2].

> ðŸ’¡ **Exemplo NumÃ©rico:** Considere um modelo AR(2) com tendÃªncia:
> $y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \epsilon_t$.  ApÃ³s a transformaÃ§Ã£o de Sims, Stock e Watson, este modelo Ã© expresso como:
> $y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \epsilon_t$
> Os termos $\alpha^*$, $\delta^*$ e $y_{t-j}^*$ sÃ£o definidos em funÃ§Ã£o dos parÃ¢metros do modelo original. Por exemplo, se  $\alpha = 1$, $\delta=0.5$, $\phi_1 = 0.4$ e $\phi_2 = 0.2$, temos:
>
>$\alpha^* = 1(1+0.4+0.2) - 0.5(0.4+2(0.2)) = 1.6 - 0.4 = 1.2$
>
>$\delta^* = 0.5(1+0.4+0.2) = 0.8$
>
>e $y_{t-1}^* = y_{t-1} - 1 - 0.5(t-1)$, $y_{t-2}^* = y_{t-2} - 1 - 0.5(t-2)$. Assim, podemos reescrever o modelo original como:
>
> $y_t = 1.2 + 0.8t + 0.4y_{t-1}^* + 0.2y_{t-2}^* + \epsilon_t$.
> O modelo transformado separa os componentes com diferentes taxas de convergÃªncia.

Em forma matricial, o modelo original Ã© [16.3.5]:
$$ y_t = x_t'\beta + \epsilon_t $$
e o modelo transformado Ã© [16.3.7]:
$$ y_t = [x_t^*]' \beta^* + \epsilon_t $$
onde $x_t^* = G x_t$ e $\beta^* = (G')^{-1} \beta$. A matriz $G'$ Ã© definida como [16.3.8]:
$$ G' = \begin{bmatrix}
1 & 0 & 0 & \ldots & 0 & 0 & 0 \\
0 & 1 & 0 & \ldots & 0 & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & 1 & 0 & 0 \\
-\alpha + \delta & -\alpha + 2\delta & \ldots & -\alpha + p\delta & 1 & 0 \\
-\delta & -\delta & \ldots & -\delta & 0 & 1
\end{bmatrix} $$
Essa matriz Ã© crucial para transformar os regressores originais em variÃ¡veis estacionÃ¡rias, uma constante e uma tendÃªncia temporal [^2].

> ðŸ’¡ **Exemplo NumÃ©rico:**  Vamos ilustrar a matriz $G'$ para um modelo AR(1) com tendÃªncia. O modelo original Ã© $y_t = \alpha + \delta t + \phi_1 y_{t-1} + \epsilon_t$. O vetor de regressores $x_t$ Ã© $[1, t, y_{t-1}]^T$, e o vetor de parÃ¢metros Ã© $\beta = [\alpha, \delta, \phi_1]^T$. A matriz $G'$ correspondente Ã©:
>
>$$ G' = \begin{bmatrix} 1 & 0 & 0 \\ -\alpha + \delta & 1 & 0 \\ -\delta & 0 & 1 \end{bmatrix} $$
>
>Se $\alpha = 2$ e $\delta = 0.5$, a matriz $G'$ torna-se:
>
>$$ G' = \begin{bmatrix} 1 & 0 & 0 \\ -1.5 & 1 & 0 \\ -0.5 & 0 & 1 \end{bmatrix} $$
>
>Esta matriz transforma o vetor de regressores $x_t$ em $x_t^* = G x_t$, que Ã© usado para a estimaÃ§Ã£o no modelo transformado.

O estimador OLS para $\beta^*$ Ã© dado por [16.3.11]:
$$ b^* = [\sum_{t=1}^T x_t^* x_t^{*'}]^{-1} \sum_{t=1}^T x_t^* y_t = (G')^{-1} b $$
onde $b$ Ã© o estimador OLS para o modelo original. Este estimador Ã© uma funÃ§Ã£o linear dos regressores transformados.

### Linearidade dos Estimadores
A linearidade do estimador $b^*$ em relaÃ§Ã£o aos regressores transformados, $x_t^*$, Ã© fundamental para a anÃ¡lise da sua distribuiÃ§Ã£o assintÃ³tica. Para verificar essa linearidade, reescrevemos a equaÃ§Ã£o do estimador:
$$ b^* = [\sum_{t=1}^T x_t^* x_t^{*'}]^{-1} \sum_{t=1}^T x_t^* y_t $$
Substituindo $y_t = [x_t^*]' \beta^* + \epsilon_t$ no somatÃ³rio, obtemos:
$$ b^* = [\sum_{t=1}^T x_t^* x_t^{*'}]^{-1} \sum_{t=1}^T x_t^* ([x_t^*]' \beta^* + \epsilon_t) $$
$$ b^* = [\sum_{t=1}^T x_t^* x_t^{*'}]^{-1} \sum_{t=1}^T x_t^* [x_t^*]' \beta^* + [\sum_{t=1}^T x_t^* x_t^{*'}]^{-1} \sum_{t=1}^T x_t^* \epsilon_t $$
$$ b^* = \beta^* + [\sum_{t=1}^T x_t^* x_t^{*'}]^{-1} \sum_{t=1}^T x_t^* \epsilon_t $$
Este resultado mostra que o estimador $b^*$ Ã© uma combinaÃ§Ã£o linear do valor verdadeiro $\beta^*$ e um termo que depende dos erros $\epsilon_t$ e dos regressores transformados $x_t^*$. A linearidade em relaÃ§Ã£o aos regressores transformados permite aplicar o Teorema do Limite Central (TLC) para derivar a distribuiÃ§Ã£o assintÃ³tica dos estimadores, que Ã© o objetivo da prÃ³xima seÃ§Ã£o.

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha um modelo AR(1) com tendÃªncia linear, onde apÃ³s a transformaÃ§Ã£o obtemos o vetor de regressores transformados como $x_t^* = [y_{t-1}^*, 1, t]^T$. O estimador OLS para este modelo Ã© dado por:
>
> $b^* = [\sum_{t=1}^T x_t^* x_t^{*'}]^{-1} \sum_{t=1}^T x_t^* y_t$
>
> Note que o vetor $x_t^*$ Ã© composto pela variÃ¡vel autorregressiva transformada, um intercepto e um termo de tendÃªncia. O estimador OLS $b^*$, por sua vez, Ã© uma combinaÃ§Ã£o linear desses regressores transformados e dos erros do modelo. Essa linearidade Ã© fundamental para estabelecer a distribuiÃ§Ã£o gaussiana assintÃ³tica dos estimadores.
>
> Para ilustrar numericamente, suponha que temos $T=100$ e que os regressores transformados e os valores de $y_t$ resultam nas seguintes somas:
>
> $\sum_{t=1}^T x_t^* x_t^{*'} = \begin{bmatrix} 50 & 0 & 0 \\ 0 & 100 & 5000 \\ 0 & 5000 & 338350 \end{bmatrix}$
>
> $\sum_{t=1}^T x_t^* y_t = \begin{bmatrix} 25 \\ 150 \\ 10000 \end{bmatrix}$
>
>  Calculando o inverso da matriz de regressores e multiplicando pelo vetor de $x_t^*y_t$, obtemos o vetor de estimativas $b^*$:
>
>  $b^* = \begin{bmatrix} 50 & 0 & 0 \\ 0 & 100 & 5000 \\ 0 & 5000 & 338350 \end{bmatrix}^{-1} \begin{bmatrix} 25 \\ 150 \\ 10000 \end{bmatrix}  \approx \begin{bmatrix} 0.5 \\ 0.075 \\ 0.015 \end{bmatrix}$
>
> Se  $\beta^* = \begin{bmatrix} 0.4 \\ 0.1 \\ 0.01 \end{bmatrix}$, entÃ£o o erro de estimativa  $b^* - \beta^*= [\sum_{t=1}^T x_t^* x_t^{*'}]^{-1} \sum_{t=1}^T x_t^* \epsilon_t \approx \begin{bmatrix} 0.1 \\ -0.025 \\ 0.005 \end{bmatrix}$
>
> Este exemplo numÃ©rico ilustra como $b^*$ Ã© linear em funÃ§Ã£o dos regressores transformados e dos erros, e como podemos expressar o erro de estimaÃ§Ã£o.

**ProposiÃ§Ã£o 1** A linearidade do estimador $b^*$ pode ser expressa de forma equivalente como:
$$b^* = \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* y_t = \beta^* + \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* \epsilon_t $$
Esta formulaÃ§Ã£o explicita que o estimador $b^*$ Ã© composto pelo valor real $\beta^*$ e um termo de erro que depende da matriz de covariÃ¢ncia dos regressores transformados e dos resÃ­duos.

### DistribuiÃ§Ã£o Gaussiana AssintÃ³tica dos Estimadores OLS
A distribuiÃ§Ã£o assintÃ³tica dos estimadores OLS transformados, $b^*$, Ã© dada por [16.3.13]:
$$ Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1}) $$
onde $Y_T$ Ã© a matriz de escalonamento [16.3.14]:
$$
Y_T = \begin{bmatrix}
\sqrt{T} & 0 & 0 & \cdots & 0 & 0 \\
0 & \sqrt{T} & 0 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \cdots & \sqrt{T} & 0 \\
0 & 0 & 0 & \cdots & 0 & \sqrt{T} \\
0 & 0 & 0 & \cdots & 0 & T^{3/2}
\end{bmatrix}
$$
e $Q^*$ Ã© a matriz de variÃ¢ncia assintÃ³tica dos regressores transformados [16.3.15]:
$$
Q^* = \begin{bmatrix}
\gamma_{0}^* & \gamma_{1}^* & \gamma_{2}^* & \cdots & \gamma_{p-1}^* & 0 & 0 \\
\gamma_{1}^* & \gamma_{0}^* & \gamma_{1}^* & \cdots & \gamma_{p-2}^* & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
\gamma_{p-1}^* & \gamma_{p-2}^* & \gamma_{p-3}^* & \cdots & \gamma_{0}^* & 0 & 0 \\
0 & 0 & 0 & \cdots & 0 & 1 & 0 \\
0 & 0 & 0 & \cdots & 0 & 0 & 1/3
\end{bmatrix}
$$
Aqui, $\gamma_{j}^* = E(y_t^* y_{t-j}^*)$ sÃ£o as autocovariÃ¢ncias da variÃ¡vel estacionÃ¡ria $y_t^*$. A matriz $Y_T$ reflete as diferentes taxas de convergÃªncia dos estimadores: $\sqrt{T}$ para os coeficientes das variÃ¡veis estacionÃ¡rias e $T^{3/2}$ para o coeficiente da tendÃªncia temporal [^1, ^2].

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha um modelo AR(1) com tendÃªncia linear. Se $\gamma_0^* = 2$, e  $T = 100$  e $\sigma^2 = 0.25$. EntÃ£o, a matriz $Y_T$ Ã© dada por:
>
> $$ Y_T = \begin{bmatrix} 10 & 0 & 0 \\ 0 & 10 & 0 \\ 0 & 0 & 1000 \end{bmatrix} $$
>
>  E a matriz $Q^*$ Ã©:
> $$Q^* = \begin{bmatrix} 2 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1/3 \end{bmatrix} $$
>
>  A distribuiÃ§Ã£o assintÃ³tica dos estimadores transformados Ã©:
>
>  $$Y_T(b^* - \beta^*) \xrightarrow{d} N(0, 0.25 [Q^*]^{-1}) = N(0, \begin{bmatrix} 0.25/2 & 0 & 0 \\ 0 & 0.25 & 0 \\ 0 & 0 & 0.25*3 \end{bmatrix}) = N(0, \begin{bmatrix} 0.125 & 0 & 0 \\ 0 & 0.25 & 0 \\ 0 & 0 & 0.75 \end{bmatrix}) $$
>
> Este exemplo mostra que a variÃ¢ncia assintÃ³tica do estimador do parÃ¢metro autoregressivo, apÃ³s o reescalonamento por $\sqrt{T}$, Ã© 0.125, e que a variÃ¢ncia assintÃ³tica do parÃ¢metro da tendÃªncia, apÃ³s o reescalonamento por $T^{3/2}$, Ã© 0.75. Os estimadores do intercepto tambÃ©m convergem com $\sqrt{T}$ e tem variÃ¢ncia assintÃ³tica de 0.25.

**Prova da DistribuiÃ§Ã£o AssintÃ³tica (EsboÃ§o)**
I. A partir da linearidade do estimador OLS, sabemos que o erro de estimaÃ§Ã£o pode ser expresso como:
$$ b^* - \beta^* = [\sum_{t=1}^T x_t^* x_t^{*'}]^{-1} \sum_{t=1}^T x_t^* \epsilon_t $$
II. Premultiplicando por $Y_T$ e utilizando a lei dos grandes nÃºmeros, mostramos que:
$$ [\sum_{t=1}^T \frac{x_t^* x_t^{*'}}{T}] \xrightarrow{p} Q^* $$
III. Aplicando o Teorema do Limite Central para a parte aleatÃ³ria, mostramos que:
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^T x_t^* \epsilon_t \xrightarrow{d} N(0, \sigma^2 Q^*) $$
IV. Combinando esses resultados, obtemos:
$$ Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1}) $$
â– 

> ðŸ’¡ **Exemplo NumÃ©rico:**  Vamos usar um exemplo simples para ilustrar a prova da distribuiÃ§Ã£o assintÃ³tica. Considere um modelo AR(1) com tendÃªncia, onde $x_t^* = [y_{t-1}^*, 1, t]^T$.  Vamos assumir que $\sigma^2 = 1$, $\gamma_0^* = 1$, e o tamanho da amostra seja $T = 100$.
>
> Passo I:  O erro de estimaÃ§Ã£o Ã© dado por: $b^* - \beta^* = [\sum_{t=1}^T x_t^* x_t^{*'}]^{-1} \sum_{t=1}^T x_t^* \epsilon_t$
>
> Passo II:  A matriz de variÃ¢ncia assintÃ³tica dos regressores transformados converge em probabilidade:
>  $\frac{1}{T}\sum_{t=1}^T x_t^* x_t^{*'}  \xrightarrow{p} Q^*$.  Podemos ilustrar a convergÃªncia por meio de simulaÃ§Ã£o.  Por exemplo, se simulamos os regressores $x_t^* x_t^{*'}$ para $T = 100$ e calculamos a mÃ©dia, a matriz $\frac{1}{T}\sum_{t=1}^T x_t^* x_t^{*'}$ deve ser prÃ³xima de:
>
>    $Q^* = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1/3 \end{bmatrix}$.
>
> Passo III: A parte aleatÃ³ria converge para uma normal: $\frac{1}{\sqrt{T}} \sum_{t=1}^T x_t^* \epsilon_t \xrightarrow{d} N(0, Q^*)$. SimulaÃ§Ãµes com amostras finitas e  histogramas aproximarÃ£o uma distribuiÃ§Ã£o normal com mÃ©dia zero e variÃ¢ncia igual a $Q^*$.
>
> Passo IV: Combinando os resultados, obtemos a distribuiÃ§Ã£o assintÃ³tica final. O reescalonamento pelos fatores em $Y_T$ Ã© crucial para assegurar a convergÃªncia da distribuiÃ§Ã£o para uma normal nÃ£o-degenerada.

**Lema 1** A convergÃªncia em probabilidade do termo $\frac{1}{T}\sum_{t=1}^T x_t^* x_t^{*'}$ para $Q^*$ Ã© uma aplicaÃ§Ã£o direta da Lei dos Grandes NÃºmeros.  Especificamente, se $x_t^* x_t^{*'}$ Ã© uma sequÃªncia de variÃ¡veis aleatÃ³rias estacionÃ¡rias e ergÃ³dicas, entÃ£o:
$$ \frac{1}{T}\sum_{t=1}^T x_t^* x_t^{*'} \xrightarrow{p} E[x_t^* x_t^{*'}] = Q^* $$
Este lema justifica a passagem do somatÃ³rio para o valor esperado ao analisarmos o comportamento assintÃ³tico dos estimadores.

**Teorema 1.1** (ExtensÃ£o da DistribuiÃ§Ã£o AssintÃ³tica) Sob as mesmas condiÃ§Ãµes do Teorema da DistribuiÃ§Ã£o AssintÃ³tica dos Estimadores OLS e a condiÃ§Ã£o adicional de que os erros $\epsilon_t$ sejam independentes e identicamente distribuÃ­dos com mÃ©dia zero, variÃ¢ncia $\sigma^2$ e momento de quarta ordem finito, entÃ£o a distribuiÃ§Ã£o assintÃ³tica dos estimadores pode ser expressa como:
$$ \sqrt{T} (b^*_1 - \beta^*_1) \xrightarrow{d} N(0, \sigma^2 [Q^*_1]^{-1}) $$
$$ T^{3/2} (b^*_2 - \beta^*_2) \xrightarrow{d} N(0, \sigma^2 [Q^*_2]^{-1}) $$
onde $b^* = \begin{bmatrix} b^*_1 \\ b^*_2 \end{bmatrix}$ e $\beta^* = \begin{bmatrix} \beta^*_1 \\ \beta^*_2 \end{bmatrix}$, com $\beta^*_1$ representando os parÃ¢metros dos regressores estacionÃ¡rios e $\beta^*_2$ representando o parÃ¢metro da tendÃªncia.  $Q^*_1$ e $Q^*_2$ correspondem aos blocos apropriados da matriz $Q^*$. Esta formulaÃ§Ã£o explicita a convergÃªncia distinta das partes estacionÃ¡rias e da tendÃªncia do modelo.

### DistribuiÃ§Ã£o AssintÃ³tica dos Estimadores OLS do Modelo Original
A distribuiÃ§Ã£o assintÃ³tica dos estimadores OLS do modelo original, $b$, Ã© obtida atravÃ©s da relaÃ§Ã£o $b = G' b^*$ [16.3.12]:
$$ Y_T(b-\beta) = Y_T G' (b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 G' [Q^*]^{-1} G'^{'}) $$
As distribuiÃ§Ãµes assintÃ³ticas dos estimadores $\hat{\alpha}$ e $\hat{\delta}$ sÃ£o dadas por [16.3.17]:
$$ \sqrt{T}(\hat{\alpha} - \alpha) \xrightarrow{d} N(0, \sigma^2 g_{\alpha} [Q^*]^{-1} g_{\alpha}^{'}) $$
$$ T^{3/2}(\hat{\delta} - \delta) \xrightarrow{d} N(0, \sigma^2 g_{\delta} [Q^*]^{-1} g_{\delta}^{'}) $$
onde $g_{\alpha}$ e $g_{\delta}$ sÃ£o vetores de pesos que isolam os componentes $\alpha$ e $\delta$ em $b$, respectivamente.

**ImplicaÃ§Ãµes para Testes de HipÃ³teses**
A distribuiÃ§Ã£o gaussiana assintÃ³tica dos estimadores permite a construÃ§Ã£o de testes de hipÃ³teses vÃ¡lidos. Por exemplo, para testar a hipÃ³tese nula $H_0: \phi_1 = \phi_{1,0}$ em um modelo AR(1), podemos utilizar a estatÃ­stica de teste:
$$ \frac{\hat{\phi}_1 - \phi_{1,0}}{\sqrt{\frac{\sigma^2 \gamma_0^*}{T}}} \xrightarrow{d} N(0,1) $$
onde $\hat{\phi}_1$ Ã© o estimador OLS do parÃ¢metro autorregressivo, e $\phi_{1,0}$ Ã© o valor especificado na hipÃ³tese nula. A distribuiÃ§Ã£o gaussiana permite calcular o p-valor e comparar com o nÃ­vel de significÃ¢ncia desejado.

> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos realizar um teste de hipÃ³tese para o coeficiente AR(1) $\phi_1$ em um modelo AR(1) com tendÃªncia. Suponha que a hipÃ³tese nula seja $H_0: \phi_1 = 0.6$, e que a estimativa obtida seja $\hat{\phi}_1 = 0.65$. Suponha que a variÃ¢ncia dos erros seja $\sigma^2 = 0.4$, a autocovariÃ¢ncia $\gamma_0^* = 1.2$ e o tamanho da amostra seja $T = 200$. A estatÃ­stica de teste Ã©:
>
> $$ z = \frac{\hat{\phi}_1 - \phi_{1,0}}{\sqrt{\frac{\sigma^2 \gamma_0^*}{T}}} = \frac{0.65 - 0.6}{\sqrt{\frac{0.4 \times 1.2}{200}}} = \frac{0.05}{\sqrt{0.0024}} \approx 1.02 $$
>
>Para um nÃ­vel de significÃ¢ncia de 5%, o valor crÃ­tico do teste bilateral Ã© aproximadamente 1.96. Como o valor da estatÃ­stica de teste ($1.02$) Ã© menor do que o valor crÃ­tico (1.96), nÃ£o rejeitamos a hipÃ³tese nula de que $\phi_1 = 0.6$. O p-valor associado a esse teste seria maior que 0.05.

**Teorema da IndependÃªncia AssintÃ³tica (RevisÃ£o)**
Os estimadores dos parÃ¢metros estacionÃ¡rios, que convergem a uma taxa $\sqrt{T}$, sÃ£o assintoticamente independentes do estimador do parÃ¢metro da tendÃªncia, que converge a uma taxa $T^{3/2}$ [^1]. Este teorema, detalhado nos capÃ­tulos anteriores, simplifica a realizaÃ§Ã£o de testes de hipÃ³teses conjuntas e permite analisar os parÃ¢metros de forma mais isolada.

**ObservaÃ§Ã£o 1** A independÃªncia assintÃ³tica entre os estimadores dos parÃ¢metros estacionÃ¡rios e o estimador do parÃ¢metro da tendÃªncia Ã© uma consequÃªncia direta da estrutura da matriz $Y_T$ e da matriz $Q^*$.  A matriz $Y_T$ possui blocos diagonais que escalonam diferentemente os estimadores, e a matriz $Q^*$ possui zeros nas posiÃ§Ãµes de covariÃ¢ncia entre os regressores estacionÃ¡rios e a tendÃªncia, levando Ã  independÃªncia assintÃ³tica.

### ConclusÃ£o
Este capÃ­tulo demonstrou como a transformaÃ§Ã£o de regressores proposta por Sims, Stock e Watson [^1] leva a estimadores OLS lineares em relaÃ§Ã£o aos regressores transformados e como isso, em conjunto com o Teorema do Limite Central, permite obter uma distribuiÃ§Ã£o gaussiana assintÃ³tica para esses estimadores apÃ³s o devido reescalonamento. A anÃ¡lise da distribuiÃ§Ã£o assintÃ³tica Ã© fundamental para inferÃªncia estatÃ­stica, como a construÃ§Ã£o de testes de hipÃ³teses vÃ¡lidos sobre os parÃ¢metros dos modelos AR com tendÃªncia. A independÃªncia assintÃ³tica entre os estimadores, jÃ¡ estabelecida em capÃ­tulos anteriores, simplifica a anÃ¡lise e permite obter resultados mais precisos para modelos complexos de sÃ©ries temporais. A abordagem detalhada e os resultados obtidos fornecem uma base sÃ³lida para o estudo de modelos com raÃ­zes unitÃ¡rias nos prÃ³ximos capÃ­tulos [^1].

### ReferÃªncias
[^1]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113â€“44.
[^2]: SeÃ§Ã£o 16.3 do texto original.
[^3]: SeÃ§Ã£o 16.1 do texto original.
<!-- END -->
