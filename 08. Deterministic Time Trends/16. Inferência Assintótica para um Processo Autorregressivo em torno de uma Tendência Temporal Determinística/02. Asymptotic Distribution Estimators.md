## InferÃªncia AssintÃ³tica para um Processo Autorregressivo em torno de uma TendÃªncia Temporal DeterminÃ­stica

### IntroduÃ§Ã£o
Este capÃ­tulo expande o tratamento de modelos de sÃ©ries temporais com tendÃªncias temporais determinÃ­sticas, focando em processos autorregressivos (AR) em torno dessas tendÃªncias. Como vimos anteriormente, em regressÃµes com tendÃªncias temporais, as taxas de convergÃªncia dos estimadores podem variar, necessitando de uma anÃ¡lise cuidadosa [^1]. Este tÃ³pico aprofunda a discussÃ£o, estendendo a anÃ¡lise para processos AR mais gerais, explorando como a transformaÃ§Ã£o de Sims, Stock e Watson isola componentes com diferentes taxas de convergÃªncia e como essa tÃ©cnica nos permite realizar inferÃªncias assintÃ³ticas [^1].

### Conceitos Fundamentais
Em continuidade ao conceito apresentado na seÃ§Ã£o 16.3, consideremos um processo autorregressivo de ordem *p* com uma tendÃªncia temporal determinÃ­stica [^1]:
$$y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t$$
onde $\epsilon_t$ Ã© um ruÃ­do branco i.i.d., com mÃ©dia zero, variÃ¢ncia $\sigma^2$ e quarto momento finito. As raÃ­zes do polinÃ´mio caracterÃ­stico $1 - \phi_1 z - \phi_2 z^2 - \ldots - \phi_p z^p$ encontram-se fora do cÃ­rculo unitÃ¡rio [^1].

**Lema 1** (Estacionaridade do Processo AR): Sob as condiÃ§Ãµes estabelecidas, o processo $y_t - \alpha - \delta t$ Ã© estacionÃ¡rio.
*Proof.*
I. O processo Ã© definido como: $y_t - \alpha - \delta t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t$.
II. Este processo representa um processo autorregressivo puro nos resÃ­duos de $y_t$ apÃ³s remover a tendÃªncia determinÃ­stica $\alpha + \delta t$.
III. Dado que as raÃ­zes do polinÃ´mio caracterÃ­stico $1 - \phi_1 z - \phi_2 z^2 - \ldots - \phi_p z^p$ estÃ£o fora do cÃ­rculo unitÃ¡rio, segue que o processo autorregressivo $\phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t$ Ã© estacionÃ¡rio.
IV. Portanto, o processo $y_t - \alpha - \delta t$ Ã© estacionÃ¡rio. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que temos um processo AR(1) com tendÃªncia: $y_t = 2 + 0.5t + 0.7y_{t-1} + \epsilon_t$, onde $\epsilon_t$ Ã© um ruÃ­do branco com mÃ©dia 0 e variÃ¢ncia 1. Aqui, $\alpha = 2$, $\delta = 0.5$, e $\phi_1 = 0.7$. O polinÃ´mio caracterÃ­stico Ã© $1 - 0.7z$, cuja raiz Ã© $1/0.7 \approx 1.43$, que estÃ¡ fora do cÃ­rculo unitÃ¡rio. Portanto, o processo $y_t - 2 - 0.5t$ Ã© estacionÃ¡rio.
```python
import numpy as np
import matplotlib.pyplot as plt

# Definindo parÃ¢metros
alpha = 2
delta = 0.5
phi1 = 0.7
sigma = 1
T = 200

# Gerando ruÃ­do branco
np.random.seed(42) # Para reprodutibilidade
epsilon = np.random.normal(0, sigma, T)

# Gerando sÃ©rie temporal
y = np.zeros(T)
y[0] = alpha + delta + epsilon[0] # Initial value
for t in range(1, T):
    y[t] = alpha + delta * (t + 1) + phi1 * y[t-1] + epsilon[t]

# Removendo tendÃªncia determinÃ­stica
yt_star = y - (alpha + delta * np.arange(1, T+1))

# Plotando a sÃ©rie original e a sÃ©rie estacionÃ¡ria
plt.figure(figsize=(10, 6))
plt.plot(np.arange(1, T+1), y, label='y_t (com tendÃªncia)')
plt.plot(np.arange(1, T+1), yt_star, label='y*_t (sem tendÃªncia)')
plt.xlabel('Tempo (t)')
plt.ylabel('Valor')
plt.title('SÃ©rie Temporal com e sem TendÃªncia DeterminÃ­stica')
plt.legend()
plt.grid(True)
plt.show()
```
A figura mostra que a sÃ©rie original `y_t` possui uma tendÃªncia crescente, enquanto a sÃ©rie transformada `y*_t` parece ser estacionÃ¡ria.

A tÃ©cnica de transformaÃ§Ã£o de Sims, Stock e Watson, discutida em [^1], reescreve o modelo original para isolar os componentes com diferentes taxas de convergÃªncia. O modelo transformado Ã© dado por
$$y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \ldots + \phi_p^* y_{t-p}^* + \epsilon_t$$
onde os regressores $y_{t-j}^*$ sÃ£o definidos como $y_{t-j}^* = y_{t-j} - \alpha - \delta(t-j)$. Essa transformaÃ§Ã£o permite tratar os componentes nÃ£o estacionÃ¡rios (a tendÃªncia temporal) separadamente dos componentes estacionÃ¡rios (o processo AR) [^1].

A forma matricial dessa transformaÃ§Ã£o Ã© dada por:
$$ y_t = x_t G' [G']^{-1} \beta + \epsilon_t = x_t^{*'} \beta^* + \epsilon_t $$
onde $x_t$ Ã© o vetor de regressores originais, $G'$ Ã© a matriz de transformaÃ§Ã£o, $\beta$ Ã© o vetor de parÃ¢metros originais e $\beta^*$ Ã© o vetor de parÃ¢metros transformados. A relaÃ§Ã£o entre $\beta$ e $\beta^*$ Ã© dada por $\beta^* = [G']^{-1} \beta$ [^1].

> ðŸ’¡ **Exemplo NumÃ©rico:** Para o exemplo AR(1) com tendÃªncia, vamos considerar os primeiros 3 pontos e mostrar como fica a transformaÃ§Ã£o matricial. Temos $y_t = \alpha + \delta t + \phi_1 y_{t-1} + \epsilon_t$,  $x_t = [1, t, y_{t-1}]$ e  $\beta = [\alpha, \delta, \phi_1]$. 
  Para t=1, $y_1=x_1\beta+\epsilon_1$
  Para t=2, $y_2=x_2\beta+\epsilon_2$
  Para t=3, $y_3=x_3\beta+\epsilon_3$
  O vetor de regressores transformados $x_t^* = [1, t, y_{t-1}^*]$ e $\beta^* = [\alpha^*, \delta^*, \phi_1^*]$.
  
  Inicialmente $x_1 = [1, 1, 0]$, $x_2 = [1, 2, y_1]$, e $x_3 = [1, 3, y_2]$. 
  Temos  $y_t^* = y_t - \alpha - \delta t$, entÃ£o $y_1^* = y_1 - \alpha - \delta$, $y_2^* = y_2 - \alpha - 2\delta$ e  $y_3^* = y_3 - \alpha - 3\delta$.
  $x_1^* = [1, 1, 0]$, $x_2^* = [1, 2, y_1^*]$, e $x_3^* = [1, 3, y_2^*]$.
  
  A matriz $G'$ pode ser construÃ­da para transformar $\beta$ em $\beta^*$.  
```mermaid
graph LR
    A[beta] --> B(G');
    B --> C[beta*]
```
O estimador OLS de $\beta^*$, denotado por $b^*$, Ã© dado por
$$ b^* = \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \left( \sum_{t=1}^T x_t^* y_t \right) $$
Com base nisso, a distribuiÃ§Ã£o assintÃ³tica de $b^*$ Ã© crucial para entender o comportamento dos estimadores transformados [^1]. O resultado principal Ã© que:
$$ Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1}) $$
onde $Y_T$ Ã© uma matriz diagonal de taxas de convergÃªncia e $Q^*$ Ã© uma matriz que envolve os momentos dos regressores transformados [^1]. Mais especificamente, $Y_T$ Ã© definida como [^1]:
$$
Y_T =
\begin{bmatrix}
\sqrt{T} & 0 & \cdots & 0 & 0 \\
0 & \sqrt{T} & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \cdots & \sqrt{T} & 0 \\
0 & 0 & \cdots & 0 & T^{3/2} \\
\end{bmatrix}
$$
e $Q^*$ Ã© definida como [^1]:
$$
Q^* =
\begin{bmatrix}
\gamma_0^* & \gamma_1^* & \cdots & \gamma_{p-1}^* & 0 & 0 \\
\gamma_1^* & \gamma_0^* & \cdots & \gamma_{p-2}^* & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
\gamma_{p-1}^* & \gamma_{p-2}^* & \cdots & \gamma_0^* & 0 & 0 \\
0 & 0 & \cdots & 0 & 1 & 0 \\
0 & 0 & \cdots & 0 & 0 & 1/3
\end{bmatrix}
$$
onde $\gamma_j^*$ representa a autocovariÃ¢ncia de ordem *j* do processo estacionÃ¡rio $y_t^*$ e os componentes inferiores da matriz sÃ£o relacionados com as taxas de convergÃªncia dos estimadores dos termos constantes e de tendÃªncia [^1].

> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos considerar o exemplo do processo AR(1) com tendÃªncia e simular dados para calcular aproximadamente a matriz $Q^*$ para T=200. Os elementos $\gamma_j^*$ sÃ£o a autocovariÃ¢ncia da sÃ©rie estacionÃ¡ria $y_t^*$.
```python
import numpy as np
import statsmodels.api as sm

# Parametros como anteriormente
alpha = 2
delta = 0.5
phi1 = 0.7
sigma = 1
T = 200
np.random.seed(42)
epsilon = np.random.normal(0, sigma, T)
y = np.zeros(T)
y[0] = alpha + delta + epsilon[0]
for t in range(1, T):
    y[t] = alpha + delta * (t + 1) + phi1 * y[t-1] + epsilon[t]

yt_star = y - (alpha + delta * np.arange(1, T+1))
# Calcula autocovariÃ¢ncias usando statsmodels
gamma0_star = np.var(yt_star)
gamma1_star = sm.tsa.stattools.acovf(yt_star,nlags=1)[1]

# Construindo Q*
Q_star = np.array([[gamma0_star, gamma1_star, 0, 0],
                  [gamma1_star, gamma0_star, 0, 0],
                  [0, 0, 1, 0],
                  [0, 0, 0, 1/3]])

print("Matriz Q* aproximada:")
print(Q_star)
```
A matriz `Q_star` mostra a estrutura de autocovariÃ¢ncia dos regressores transformados. Note que a parte superior Ã© dada pela autocovariÃ¢ncia do processo estacionÃ¡rio, e a parte inferior Ã© relacionada Ã  constante e tendÃªncia linear.

A demonstraÃ§Ã£o formal deste resultado Ã© detalhada no ApÃªndice 16.A [^1]. O ponto chave Ã© que os estimadores dos coeficientes $\phi_j^*$ convergem para os verdadeiros valores a uma taxa de $\sqrt{T}$, enquanto o estimador de $\delta^*$ converge a uma taxa de $T^{3/2}$ [^1].

**ProposiÃ§Ã£o 2** (ConsistÃªncia dos Estimadores Transformados): Os estimadores $b^*$ sÃ£o consistentes para $\beta^*$.
*Proof.*
I. O estimador OLS $b^*$ Ã© dado por $b^* = \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \left( \sum_{t=1}^T x_t^* y_t \right)$.
II. Por definiÃ§Ã£o, $x_t^*$ inclui os regressores transformados que consistem em defasagens de $y_t^*$, uma constante e uma tendÃªncia temporal.
III. Como os processos $y_t^* = y_t - \alpha - \delta t$ sÃ£o estacionÃ¡rios (Lema 1), seus momentos convergem para valores populacionais.
IV. A matriz $\left( \sum_{t=1}^T x_t^* x_t^{*'} \right) / T$ converge em probabilidade para uma matriz de momentos finita, nÃ£o singular $Q^*$, como definido no texto.
V. TambÃ©m, $\left( \sum_{t=1}^T x_t^* y_t \right) / T$ converge em probabilidade para um valor populacional, denotado como $\mathbb{E}[x_t^* y_t]$.
VI. Portanto, $b^*$ converge em probabilidade para o valor populacional correspondente, dado por $Q^{*-1}\mathbb{E}[x_t^* y_t]$, que Ã© igual a $\beta^*$.
VII. Assim, os estimadores $b^*$ sÃ£o consistentes para $\beta^*$. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:** Usando a simulaÃ§Ã£o do exemplo anterior, podemos verificar a consistÃªncia do estimador $b^*$. Vamos estimar o modelo usando OLS e comparar o resultado com os verdadeiros parÃ¢metros.
```python
import numpy as np
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression

# Parametros (repetindo para clareza)
alpha = 2
delta = 0.5
phi1 = 0.7
sigma = 1
T = 200
np.random.seed(42)
epsilon = np.random.normal(0, sigma, T)
y = np.zeros(T)
y[0] = alpha + delta + epsilon[0]
for t in range(1, T):
    y[t] = alpha + delta * (t + 1) + phi1 * y[t-1] + epsilon[t]
yt_star = y - (alpha + delta * np.arange(1, T+1))

# Preparando os regressores transformados
X_star = np.column_stack((np.ones(T), np.arange(1, T+1), np.concatenate(([0], yt_star[:-1]))))

# Estimando por OLS
model = LinearRegression()
model.fit(X_star, y)

# Extraindo os parÃ¢metros estimados
alpha_star_hat = model.intercept_
delta_star_hat = model.coef_[1]
phi1_star_hat = model.coef_[2]

# Imprimindo os resultados
print("Estimativas de b*:")
print(f"alpha^*_hat: {alpha_star_hat:.4f}")
print(f"delta^*_hat: {delta_star_hat:.4f}")
print(f"phi1^*_hat: {phi1_star_hat:.4f}")

print("\nVerdadeiros valores de beta*:")
print(f"alpha^*:  {alpha:.4f} (Original, but transformed as  alpha*)") #This needs a more detailed example to show the exact value for alpha*
print(f"delta^*: {delta:.4f} (Original, but transformed as  delta*)")  #This needs a more detailed example to show the exact value for delta*
print(f"phi1^*:  {phi1:.4f}")

```
Como podemos ver, os estimadores de $b^*$ se aproximam dos valores verdadeiros dos parÃ¢metros de $\beta^*$, demonstrando a consistÃªncia do estimador.

A partir da distribuiÃ§Ã£o de $b^*$, e utilizando o resultado $\beta = G' \beta^*$ [^1], podemos obter a distribuiÃ§Ã£o assintÃ³tica dos estimadores dos parÃ¢metros do modelo original. A distribuiÃ§Ã£o assintÃ³tica dos estimadores $\hat{\alpha}$ Ã© dada por
$$ \sqrt{T}(\hat{\alpha} - \alpha) \xrightarrow{d} N(0, \sigma^2 g_\alpha [Q^*]^{-1} g_\alpha') $$
e a distribuiÃ§Ã£o assintÃ³tica do estimador da tendÃªncia $\hat{\delta}$ Ã© dada por
$$ T^{3/2}(\hat{\delta} - \delta) \xrightarrow{d} N(0, \sigma^2 g_\delta [Q^*]^{-1} g_\delta') $$
onde $g_\alpha$ e $g_\delta$ sÃ£o vetores de combinaÃ§Ã£o linear definidos pelas relaÃ§Ãµes entre $\beta$ e $\beta^*$, e $[Q^*]^{-1}$ Ã© o inverso da matriz $Q^*$ [^1].

**CorolÃ¡rio 2.1** (InferÃªncia AssintÃ³tica para $\alpha$ e $\delta$): As distribuiÃ§Ãµes assintÃ³ticas de $\hat{\alpha}$ e $\hat{\delta}$ fornecem a base para construir intervalos de confianÃ§a e realizar testes de hipÃ³teses sobre os parÃ¢metros da constante e da tendÃªncia no modelo original.
*Proof.*
I. Pela ProposiÃ§Ã£o 2, temos que os estimadores $\hat{\alpha}$ e $\hat{\delta}$ sÃ£o consistentes para $\alpha$ e $\delta$, respectivamente.
II. As distribuiÃ§Ãµes assintÃ³ticas de $\sqrt{T}(\hat{\alpha} - \alpha)$ e $T^{3/2}(\hat{\delta} - \delta)$ convergem para distribuiÃ§Ãµes normais, conforme definido no texto.
III. A distribuiÃ§Ã£o assintÃ³tica de um estimador descreve a forma como o estimador se comporta para amostras grandes.
IV. Intervalos de confianÃ§a para $\alpha$ podem ser construÃ­dos com base na distribuiÃ§Ã£o normal limite $\sqrt{T}(\hat{\alpha} - \alpha) \xrightarrow{d} N(0, \sigma^2 g_\alpha [Q^*]^{-1} g_\alpha')$.
V. Intervalos de confianÃ§a para $\delta$ podem ser construÃ­dos com base na distribuiÃ§Ã£o normal limite $T^{3/2}(\hat{\delta} - \delta) \xrightarrow{d} N(0, \sigma^2 g_\delta [Q^*]^{-1} g_\delta')$.
VI. Testes de hipÃ³teses para $\alpha$ e $\delta$ podem ser conduzidos utilizando os resultados da distribuiÃ§Ã£o assintÃ³tica.
VII. Portanto, as distribuiÃ§Ãµes assintÃ³ticas de $\hat{\alpha}$ e $\hat{\delta}$ fornecem a base para construir intervalos de confianÃ§a e realizar testes de hipÃ³teses sobre os parÃ¢metros da constante e da tendÃªncia no modelo original.â– 

> ðŸ’¡ **Exemplo NumÃ©rico:** Continuando o exemplo, construÃ­mos um intervalo de confianÃ§a para $\alpha$ e $\delta$ com os resultados da simulaÃ§Ã£o.
```python
import numpy as np
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression
from scipy.stats import norm

# Parametros
alpha = 2
delta = 0.5
phi1 = 0.7
sigma = 1
T = 200
np.random.seed(42)
epsilon = np.random.normal(0, sigma, T)
y = np.zeros(T)
y[0] = alpha + delta + epsilon[0]
for t in range(1, T):
    y[t] = alpha + delta * (t + 1) + phi1 * y[t-1] + epsilon[t]
yt_star = y - (alpha + delta * np.arange(1, T+1))

# Regressores transformados
X_star = np.column_stack((np.ones(T), np.arange(1, T+1), np.concatenate(([0], yt_star[:-1]))))

# Modelo OLS
model = LinearRegression()
model.fit(X_star, y)

# Estimativas
alpha_star_hat = model.intercept_
delta_star_hat = model.coef_[1]
phi1_star_hat = model.coef_[2]

# Calculando os residuos
residuals = y - model.predict(X_star)

# Estimativa da variancia
sigma2_hat = np.sum(residuals**2)/(T-3)

# Calculando a matriz Q* (aproximada)
gamma0_star = np.var(yt_star)
gamma1_star = sm.tsa.stattools.acovf(yt_star,nlags=1)[1]
Q_star = np.array([[gamma0_star, gamma1_star, 0, 0],
                  [gamma1_star, gamma0_star, 0, 0],
                  [0, 0, 1, 0],
                  [0, 0, 0, 1/3]])

# Matriz de variancia-covariÃ¢ncia
Q_star_inv = np.linalg.inv(Q_star)
g_alpha = np.array([1,0,0,0]) # vetor g_alpha
g_delta = np.array([0,1,0,0]) # vetor g_delta

# Variancia assintÃ³tica de alpha_hat
var_alpha_hat = sigma2_hat*np.dot(g_alpha,np.dot(Q_star_inv,g_alpha))

# Variancia assintÃ³tica de delta_hat
var_delta_hat = sigma2_hat*np.dot(g_delta,np.dot(Q_star_inv,g_delta))


# Intervalo de confianÃ§a para alpha
alpha_se = np.sqrt(var_alpha_hat/T)
alpha_ci = norm.interval(0.95, loc=alpha_star_hat, scale=alpha_se)

# Intervalo de confianÃ§a para delta
delta_se = np.sqrt(var_delta_hat/T**3)
delta_ci = norm.interval(0.95, loc=delta_star_hat, scale=delta_se)

print("Intervalos de ConfianÃ§a (95%):")
print(f"Alpha: {alpha_ci}")
print(f"Delta: {delta_ci}")

```
O cÃ³digo acima calcula os intervalos de confianÃ§a para $\alpha$ e $\delta$ usando as distribuiÃ§Ãµes assintÃ³ticas. Esses intervalos nos fornecem uma faixa plausÃ­vel de valores para os parÃ¢metros populacionais.

**Teorema 3** (DistribuiÃ§Ã£o AssintÃ³tica para os Coeficientes AR): Para os coeficientes $\phi_j$, a distribuiÃ§Ã£o assintÃ³tica Ã© dada por:
$$ \sqrt{T}(\hat{\phi_j} - \phi_j) \xrightarrow{d} N(0, \sigma^2 v_j) $$
Onde $v_j$ corresponde aos elementos apropriados da matriz $[Q^*]^{-1}$ associados aos coeficientes $\phi_j$ e $\hat{\phi_j}$ sÃ£o os coeficientes estimados do modelo original.
*Proof.*
I. O estimador $b^*$ do vetor $\beta^*$ possui a distribuiÃ§Ã£o assintÃ³tica dada por $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$.
II. A relaÃ§Ã£o entre os parÃ¢metros originais $\beta$ e os parÃ¢metros transformados $\beta^*$ Ã© $\beta = G' \beta^*$, onde $G'$ Ã© uma matriz de transformaÃ§Ã£o.
III. Portanto, o estimador dos parÃ¢metros originais $\hat{\beta}$ Ã© dado por $\hat{\beta} = G'b^*$.
IV. Consequentemente, a distribuiÃ§Ã£o assintÃ³tica de $\hat{\beta}$ Ã© dada por $Y_T(\hat{\beta} - \beta) \xrightarrow{d} N(0, \sigma^2 G' [Q^*]^{-1} G)$.
V. Os coeficientes AR $\phi_j$ sÃ£o um subvetor de $\beta$.
VI. Os elementos apropriados da matriz $G' [Q^*]^{-1} G$ correspondem Ã s variÃ¢ncias assintÃ³ticas dos estimadores $\hat{\phi_j}$.
VII. Ao extrairmos esses elementos especÃ­ficos, denotados por $v_j$, encontramos que a distribuiÃ§Ã£o assintÃ³tica dos coeficientes AR Ã© dada por $\sqrt{T}(\hat{\phi_j} - \phi_j) \xrightarrow{d} N(0, \sigma^2 v_j)$. â– 
> ðŸ’¡ **Exemplo NumÃ©rico:** Utilizando o exemplo anterior, podemos calcular um intervalo de confianÃ§a para $\phi_1$.
```python
import numpy as np
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression
from scipy.stats import norm

# Parametros
alpha = 2
delta = 0.5
phi1 = 0.7
sigma = 1
T = 200
np.random.seed(42)
epsilon = np.random.normal(0, sigma, T)
y = np.zeros(T)
y[0] = alpha + delta + epsilon[0]
for t in range(1, T):
    y[t] = alpha + delta * (t + 1) + phi1 * y[t-1] + epsilon[t]
yt_star = y - (alpha + delta * np.arange(1, T+1))

# Regressores transformados
X_star = np.column_stack((np.ones(T), np.arange(1, T+1), np.concatenate(([0], yt_star[:-1]))))

# Modelo OLS
model = LinearRegression()
model.fit(X_star, y)
# Estimativas
alpha_star_hat = model.intercept_
delta_star_hat = model.coef_[1]
phi1_star_hat = model.coef_[2]
# Calculando os residuos
residuals = y - model.predict(X_star)
# Estimativa da variancia
sigma2_hat = np.sum(residuals**2)/(T-3)
# Calculando a matriz Q* (aproximada)
gamma0_star = np.var(yt_star)
gamma1_star = sm.tsa.stattools.acovf(yt_star,nlags=1)[1]
Q_star = np.array([[gamma0_star, gamma1_star, 0, 0],
                  [gamma1_star, gamma0_star, 0, 0],
                  [0, 0, 1, 0],
                  [0, 0, 0, 1/3]])
# Matriz de variancia-covariÃ¢ncia
Q_star_inv = np.linalg.inv(Q_star)
# Variancia assintÃ³tica de phi1_hat
var_phi1_hat = sigma2_hat*Q_star_inv[2,2]
# Intervalo de confianÃ§a para phi1
phi1_se = np.sqrt(var_phi1_hat/T)
phi1_ci = norm.interval(0.95, loc=phi1_star_hat, scale=phi1_se)

print(f"Intervalo de confianÃ§a para phi1: {phi1_ci}")
```
O intervalo de confianÃ§a para $\phi_1$ nos fornece uma faixa plausÃ­vel de valores para o coeficiente AR.

### ConclusÃ£o
Este tÃ³pico demonstrou que a transformaÃ§Ã£o de Sims, Stock e Watson permite obter a distribuiÃ§Ã£o assintÃ³tica dos estimadores em modelos autorregressivos com tendÃªncia temporal determinÃ­stica. A distribuiÃ§Ã£o assintÃ³tica do vetor $b^*$ Ã© crucial para entender o comportamento dos estimadores, e permite deduzir a distribuiÃ§Ã£o assintÃ³tica dos estimadores dos parÃ¢metros originais do modelo. AlÃ©m disso, essa anÃ¡lise fundamenta testes de hipÃ³teses sobre os parÃ¢metros do modelo, validando as tÃ©cnicas de inferÃªncia assintÃ³tica. Os resultados obtidos aqui sÃ£o extensÃµes dos casos mais simples de regressÃµes lineares com tendÃªncia e servem de base para o estudo de modelos mais complexos, inclusive com raÃ­zes unitÃ¡rias, que serÃ£o abordados nos prÃ³ximos capÃ­tulos.

### ReferÃªncias
[^1]:  *Processos com TendÃªncias Temporais DeterminÃ­sticas*, CapÃ­tulo 16, SeÃ§Ã£o 16.3 do texto fornecido.
<!-- END -->
