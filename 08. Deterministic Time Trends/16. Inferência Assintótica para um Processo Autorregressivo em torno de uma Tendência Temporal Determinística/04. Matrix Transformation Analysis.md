## Infer√™ncia Assint√≥tica para um Processo Autorregressivo em torno de uma Tend√™ncia Temporal Determin√≠stica

### Introdu√ß√£o
Este cap√≠tulo expande o tratamento de modelos de s√©ries temporais com tend√™ncias temporais determin√≠sticas, focando em processos autorregressivos (AR) em torno dessas tend√™ncias. Como vimos anteriormente, em regress√µes com tend√™ncias temporais, as taxas de converg√™ncia dos estimadores podem variar, necessitando de uma an√°lise cuidadosa [^1]. Este t√≥pico aprofunda a discuss√£o, estendendo a an√°lise para processos AR mais gerais, explorando como a transforma√ß√£o de Sims, Stock e Watson isola componentes com diferentes taxas de converg√™ncia e como essa t√©cnica nos permite realizar infer√™ncias assint√≥ticas [^1]. O foco principal aqui ser√° detalhar o comportamento assint√≥tico espec√≠fico dos estimadores OLS dos par√¢metros originais, incluindo a taxa de converg√™ncia diferenciada entre os par√¢metros da tend√™ncia e os demais coeficientes do modelo. Vamos explorar, em particular, como a representa√ß√£o matricial da transforma√ß√£o de Sims, Stock e Watson nos permite entender as diferentes taxas de converg√™ncia e como essa an√°lise leva √† infer√™ncia sobre os par√¢metros originais [^1].

### Conceitos Fundamentais
Em continuidade ao conceito apresentado na se√ß√£o 16.3, consideremos um processo autorregressivo de ordem *p* com uma tend√™ncia temporal determin√≠stica [^1]:
$$y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t$$
onde $\epsilon_t$ √© um ru√≠do branco i.i.d., com m√©dia zero, vari√¢ncia $\sigma^2$ e quarto momento finito. As ra√≠zes do polin√¥mio caracter√≠stico $1 - \phi_1 z - \phi_2 z^2 - \ldots - \phi_p z^p$ encontram-se fora do c√≠rculo unit√°rio [^1].

**Lema 1** (Estacionaridade do Processo AR): Sob as condi√ß√µes estabelecidas, o processo $y_t - \alpha - \delta t$ √© estacion√°rio.
*Proof.*
I. O processo √© definido como: $y_t - \alpha - \delta t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t$.
II. Este processo representa um processo autorregressivo puro nos res√≠duos de $y_t$ ap√≥s remover a tend√™ncia determin√≠stica $\alpha + \delta t$.
III. Dado que as ra√≠zes do polin√¥mio caracter√≠stico $1 - \phi_1 z - \phi_2 z^2 - \ldots - \phi_p z^p$ est√£o fora do c√≠rculo unit√°rio, segue que o processo autorregressivo $\phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t$ √© estacion√°rio.
IV. Portanto, o processo $y_t - \alpha - \delta t$ √© estacion√°rio. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que temos um processo AR(1) com tend√™ncia: $y_t = 2 + 0.5t + 0.7y_{t-1} + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco com m√©dia 0 e vari√¢ncia 1. Aqui, $\alpha = 2$, $\delta = 0.5$, e $\phi_1 = 0.7$. O polin√¥mio caracter√≠stico √© $1 - 0.7z$, cuja raiz √© $1/0.7 \approx 1.43$, que est√° fora do c√≠rculo unit√°rio. Portanto, o processo $y_t - 2 - 0.5t$ √© estacion√°rio.
```python
import numpy as np
import matplotlib.pyplot as plt

# Definindo par√¢metros
alpha = 2
delta = 0.5
phi1 = 0.7
sigma = 1
T = 200

# Gerando ru√≠do branco
np.random.seed(42) # Para reprodutibilidade
epsilon = np.random.normal(0, sigma, T)

# Gerando s√©rie temporal
y = np.zeros(T)
y[0] = alpha + delta + epsilon[0] # Initial value
for t in range(1, T):
    y[t] = alpha + delta * (t + 1) + phi1 * y[t-1] + epsilon[t]

# Removendo tend√™ncia determin√≠stica
yt_star = y - (alpha + delta * np.arange(1, T+1))

# Plotando a s√©rie original e a s√©rie estacion√°ria
plt.figure(figsize=(10, 6))
plt.plot(np.arange(1, T+1), y, label='y_t (com tend√™ncia)')
plt.plot(np.arange(1, T+1), yt_star, label='y*_t (sem tend√™ncia)')
plt.xlabel('Tempo (t)')
plt.ylabel('Valor')
plt.title('S√©rie Temporal com e sem Tend√™ncia Determin√≠stica')
plt.legend()
plt.grid(True)
plt.show()
```
A figura mostra que a s√©rie original `y_t` possui uma tend√™ncia crescente, enquanto a s√©rie transformada `y*_t` parece ser estacion√°ria.

**Lema 1.1** (Representa√ß√£o de m√©dias m√≥veis do processo estacion√°rio): Dado que o processo $y_t - \alpha - \delta t$ √© estacion√°rio, ele pode ser representado por uma m√©dia m√≥vel infinita dos choques $\epsilon_t$. Mais especificamente, existe uma sequ√™ncia de coeficientes $\psi_j$ tal que
$$ y_t - \alpha - \delta t = \sum_{j=0}^\infty \psi_j \epsilon_{t-j} $$
onde $\sum_{j=0}^\infty |\psi_j| < \infty$.
*Proof.*
I. O processo $y_t - \alpha - \delta t$ √© estacion√°rio pelo Lema 1.
II.  Pela teoria de processos autorregressivos estacion√°rios, qualquer processo AR estacion√°rio pode ser expresso como uma m√©dia m√≥vel infinita dos seus choques.
III. Os coeficientes $\psi_j$ s√£o determinados recursivamente a partir dos coeficientes $\phi_1, \phi_2, \ldots, \phi_p$.
IV. A condi√ß√£o $\sum_{j=0}^\infty |\psi_j| < \infty$ garante a converg√™ncia da s√©rie e a estacionariedade do processo.
V. Portanto, $y_t - \alpha - \delta t$ pode ser representado por uma m√©dia m√≥vel infinita dos choques $\epsilon_t$. $\blacksquare$

A t√©cnica de transforma√ß√£o de Sims, Stock e Watson, discutida em [^1], reescreve o modelo original para isolar os componentes com diferentes taxas de converg√™ncia.  Este m√©todo consiste em adicionar e subtrair termos da forma $\phi_j[\alpha + \delta(t-j)]$ para cada defasagem no modelo original [^1]. Ao fazer isso, o modelo pode ser reescrito em termos de vari√°veis aleat√≥rias estacion√°rias de m√©dia zero (os termos $y_{t-j}^*$), um termo constante ($\alpha^*$) e uma tend√™ncia temporal ($\delta^*t$) [^1]. O modelo transformado √© dado por
$$y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \ldots + \phi_p^* y_{t-p}^* + \epsilon_t$$
onde os regressores $y_{t-j}^*$ s√£o definidos como $y_{t-j}^* = y_{t-j} - \alpha - \delta(t-j)$. Essa transforma√ß√£o permite tratar os componentes n√£o estacion√°rios (a tend√™ncia temporal) separadamente dos componentes estacion√°rios (o processo AR) [^1].

A representa√ß√£o matricial desta transforma√ß√£o √© fundamental para entender as taxas de converg√™ncia dos estimadores. O modelo original pode ser expresso como:
$$ y_t = x_t' \beta + \epsilon_t $$
onde $x_t = [1, t, y_{t-1}, \ldots, y_{t-p}]'$ √© o vetor de regressores originais, e $\beta = [\alpha, \delta, \phi_1, \ldots, \phi_p]'$.  A transforma√ß√£o de Sims, Stock e Watson permite reescrever este modelo em termos de regressores transformados $x_t^*$ e par√¢metros transformados $\beta^*$, tal que:
$$ y_t = x_t^{*'} \beta^* + \epsilon_t $$
Em termos de matrizes, essa transforma√ß√£o pode ser representada como:
$$ y_t = x_t' G' (G')^{-1} \beta + \epsilon_t = x_t^{*'} \beta^* + \epsilon_t $$
onde $x_t^* = Gx_t$ e $\beta^* = (G')^{-1} \beta$. A matriz $G$ √© uma matriz de transforma√ß√£o que isola os componentes com diferentes taxas de converg√™ncia [^1]. Em particular, $x_t^*$ cont√©m os termos com comportamento de estacionariedade (derivados de $y_t - \alpha - \delta t$), uma constante e uma tend√™ncia linear, enquanto $\beta^*$ s√£o os coeficientes associados a esses regressores transformados.

> üí° **Exemplo Num√©rico:** Para o exemplo AR(1) com tend√™ncia, onde $y_t = \alpha + \delta t + \phi_1 y_{t-1} + \epsilon_t$,  temos $x_t = [1, t, y_{t-1}]'$ e $\beta = [\alpha, \delta, \phi_1]'$. Ap√≥s a transforma√ß√£o, $x_t^* = [1, t, y_{t-1}^*]'$  e  $\beta^* = [\alpha^*, \delta^*, \phi_1^*]'$. A matriz $G'$ √© definida de forma que a rela√ß√£o entre $\beta$ e $\beta^*$ seja estabelecida. Para o caso espec√≠fico do AR(1), a matriz de transforma√ß√£o $G'$ e seu inverso $(G')^{-1}$ s√£o dadas por:
```
G' = | 1 0 0  |
    | -Œ± 1 0 |
    | -Œ¥ 0 1 |

(G')‚Åª¬π = | 1 0 0  |
       | Œ± 1 0 |
       | Œ¥  0 1 |
```
No exemplo, note que a matriz G' apresentada no texto est√° incorreta e foi corrigida acima. A matriz inversa tamb√©m est√° corrigida.
A rela√ß√£o entre os par√¢metros originais e os transformados √© dada por:

$\beta = G' \beta^*$

$\begin{bmatrix} \alpha \\ \delta \\ \phi_1 \end{bmatrix} = \begin{bmatrix} 1 & -\alpha & -\delta \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}  \begin{bmatrix} \alpha^* \\ \delta^* \\ \phi_1^* \end{bmatrix}$

$\alpha = \alpha^* - \alpha\delta^* - \delta\phi_1^*$.
$\delta = \delta^*$
$\phi_1 = \phi_1^*$

Portanto, $\beta^* = (G')^{-1} \beta$
$\begin{bmatrix} \alpha^* \\ \delta^* \\ \phi_1^* \end{bmatrix} = \begin{bmatrix} 1 & \alpha & \delta \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}  \begin{bmatrix} \alpha \\ \delta \\ \phi_1 \end{bmatrix}$

$\alpha^* = \alpha + \alpha\delta + \delta\phi_1$
$\delta^* = \delta$
$\phi_1^* = \phi_1$
Vamos usar o mesmo exemplo num√©rico anterior, onde $\alpha = 2$, $\delta = 0.5$, e $\phi_1 = 0.7$.
$\alpha^* = 2 + 2*0.5+ 0.5*0.7 = 2 + 1 + 0.35 = 3.35$
$\delta^* = 0.5$
$\phi_1^* = 0.7$
√â importante notar que, embora $b^*$ seja calculado a partir dos regressores transformados, os valores ajustados pelo modelo transformado (i.e.,  $x_t^{*'} b^*$) s√£o numericamente id√™nticos aos valores ajustados pelo modelo original (i.e., $x_t' b$, onde $b$ √© o estimador OLS de $\beta$). Essa equival√™ncia √© uma consequ√™ncia da constru√ß√£o da transforma√ß√£o e garante que estamos analisando o mesmo modelo, por√©m em uma representa√ß√£o diferente.

O vetor de estimativas OLS dos coeficientes $\beta^*$ (denotado por $b^*$) √© dado por:
$$ b^* = \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* y_t $$

Com base nisso, a distribui√ß√£o assint√≥tica de $b^*$ √© crucial para entender o comportamento dos estimadores transformados [^1]. O resultado principal √© que:
$$ Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1}) $$
onde $Y_T$ √© uma matriz diagonal de taxas de converg√™ncia e $Q^*$ √© uma matriz que envolve os momentos dos regressores transformados [^1]. Mais especificamente, $Y_T$ √© definida como [^1]:
$$
Y_T =
\begin{bmatrix}
\sqrt{T} & 0 & \cdots & 0 & 0 \\
0 & \sqrt{T} & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \cdots & \sqrt{T} & 0 \\
0 & 0 & \cdots & 0 & T^{3/2} \\
\end{bmatrix}
$$
e $Q^*$ √© definida como [^1]:
$$
Q^* =
\begin{bmatrix}
\gamma_0^* & \gamma_1^* & \cdots & \gamma_{p-1}^* & 0 & 0 \\
\gamma_1^* & \gamma_0^* & \cdots & \gamma_{p-2}^* & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
\gamma_{p-1}^* & \gamma_{p-2}^* & \cdots & \gamma_0^* & 0 & 0 \\
0 & 0 & \cdots & 0 & 1 & 0 \\
0 & 0 & \cdots & 0 & 0 & 1/3
\end{bmatrix}
$$
onde $\gamma_j^*$ representa a autocovari√¢ncia de ordem *j* do processo estacion√°rio $y_t^*$ e os componentes inferiores da matriz s√£o relacionados com as taxas de converg√™ncia dos estimadores dos termos constantes e de tend√™ncia [^1].

> üí° **Exemplo Num√©rico:** Vamos considerar o exemplo do processo AR(1) com tend√™ncia e simular dados para calcular aproximadamente a matriz $Q^*$ para T=200. Os elementos $\gamma_j^*$ s√£o a autocovari√¢ncia da s√©rie estacion√°ria $y_t^*$.
```python
import numpy as np
import statsmodels.api as sm

# Parametros como anteriormente
alpha = 2
delta = 0.5
phi1 = 0.7
sigma = 1
T = 200
np.random.seed(42)
epsilon = np.random.normal(0, sigma, T)
y = np.zeros(T)
y[0] = alpha + delta + epsilon[0]
for t in range(1, T):
    y[t] = alpha + delta * (t + 1) + phi1 * y[t-1] + epsilon[t]

yt_star = y - (alpha + delta * np.arange(1, T+1))
# Calcula autocovari√¢ncias usando statsmodels
gamma0_star = np.var(yt_star)
gamma1_star = sm.tsa.stattools.acovf(yt_star,nlags=1)[1]

# Construindo Q*
Q_star = np.array([[gamma0_star, gamma1_star, 0, 0],
                  [gamma1_star, gamma0_star, 0, 0],
                  [0, 0, 1, 0],
                  [0, 0, 0, 1/3]])

print("Matriz Q* aproximada:")
print(Q_star)
```
A matriz `Q_star` mostra a estrutura de autocovari√¢ncia dos regressores transformados. Note que a parte superior √© dada pela autocovari√¢ncia do processo estacion√°rio, e a parte inferior √© relacionada √† constante e tend√™ncia linear.

A demonstra√ß√£o formal deste resultado √© detalhada no Ap√™ndice 16.A [^1]. O ponto chave √© que os estimadores dos coeficientes $\phi_j^*$ convergem para os verdadeiros valores a uma taxa de $\sqrt{T}$, enquanto o estimador de $\delta^*$ converge a uma taxa de $T^{3/2}$ [^1].

**Proposi√ß√£o 2** (Consist√™ncia dos Estimadores Transformados): Os estimadores $b^*$ s√£o consistentes para $\beta^*$.
*Proof.*
I. O estimador OLS $b^*$ √© dado por $b^* = \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \left( \sum_{t=1}^T x_t^* y_t \right)$.
II. Por defini√ß√£o, $x_t^*$ inclui os regressores transformados que consistem em defasagens de $y_t^*$, uma constante e uma tend√™ncia temporal.
III. Como os processos $y_t^* = y_t - \alpha - \delta t$ s√£o estacion√°rios (Lema 1), seus momentos convergem para valores populacionais.
IV. A matriz $\left( \sum_{t=1}^T x_t^* x_t^{*'} \right) / T$ converge em probabilidade para uma matriz de momentos finita, n√£o singular $Q^*$, como definido no texto.
V. Tamb√©m, $\left( \sum_{t=1}^T x_t^* y_t \right) / T$ converge em probabilidade para um valor populacional, denotado como $\mathbb{E}[x_t^* y_t]$.
VI. Portanto, $b^*$ converge em probabilidade para o valor populacional correspondente, dado por $Q^{*-1}\mathbb{E}[x_t^* y_t]$, que √© igual a $\beta^*$.
VII. Assim, os estimadores $b^*$ s√£o consistentes para $\beta^*$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Usando a simula√ß√£o do exemplo anterior, podemos verificar a consist√™ncia do estimador $b^*$. Vamos estimar o modelo usando OLS e comparar o resultado com os verdadeiros par√¢metros.
```python
import numpy as np
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression

# Parametros (repetindo para clareza)
alpha = 2
delta = 0.5
phi1 = 0.7
sigma = 1
T = 200
np.random.seed(42)
epsilon = np.random.normal(0, sigma, T)
y = np.zeros(T)
y[0] = alpha + delta + epsilon[0]
for t in range(1, T):
    y[t] = alpha + delta * (t + 1) + phi1 * y[t-1] + epsilon[t]
yt_star = y - (alpha + delta * np.arange(1, T+1))

# Preparando os regressores transformados
X_star = np.column_stack((np.ones(T), np.arange(1, T+1), np.concatenate(([0], yt_star[:-1]))))

# Estimando por OLS
model = LinearRegression()
model.fit(X_star, y)

# Extraindo os par√¢metros estimados
alpha_star_hat = model.intercept_
delta_star_hat = model.coef_[1]
phi1_star_hat = model.coef_[2]

# Imprimindo os resultados
print("Estimativas de b*:")
print(f"alpha^*_hat: {alpha_star_hat:.4f}")
print(f"delta^*_hat: {delta_star_hat:.4f}")
print(f"phi1^*_hat: {phi1_star_hat:.4f}")

print("\nVerdadeiros valores de beta*:")
alpha_star_true = alpha + alpha*delta + delta*phi1
print(f"alpha^*:  {alpha_star_true:.4f}") #This needs a more detailed example to show the exact value for alpha*
print(f"delta^*: {delta:.4f}")  #This needs a more detailed example to show the exact value for delta*
print(f"phi1^*:  {phi1:.4f}")

```
Como podemos ver, os estimadores de $b^*$ se aproximam dos valores verdadeiros dos par√¢metros de $\beta^*$, demonstrando a consist√™ncia do estimador. Note que os valores verdadeiros de $\alpha^*$, $\delta^*$ e $\phi_1^*$ foram calculados usando a rela√ß√£o $\beta^* = (G')^{-1} \beta$ conforme explicado no exemplo anterior.

A partir da distribui√ß√£o de $b^*$, e utilizando o resultado $\beta = G' \beta^*$ [^1], podemos obter a distribui√ß√£o assint√≥tica dos estimadores dos par√¢metros do modelo original. A distribui√ß√£o assint√≥tica dos estimadores $\hat{\alpha}$ √© dada por
$$ \sqrt{T}(\hat{\alpha} - \alpha) \xrightarrow{d} N(0, \sigma^2 g_\alpha [Q^*]^{-1} g_\alpha') $$
e a distribui√ß√£o assint√≥tica do estimador da tend√™ncia $\hat{\delta}$ √© dada por
$$ T^{3/2}(\hat{\delta} - \delta) \xrightarrow{d} N(0, \sigma^2 g_\delta [Q^*]^{-1} g_\delta') $$
onde $g_\alpha$ e $g_\delta$ s√£o vetores de combina√ß√£o linear definidos pelas rela√ß√µes entre $\beta$ e $\beta^*$, e $[Q^*]^{-1}$ √© o inverso da matriz $Q^*$ [^1].

> üí° **Exemplo Num√©rico:** Para entender a diferen√ßa na taxa de converg√™ncia entre $\hat{\alpha}$ e $\hat{\delta}$, vamos simular o processo v√°rias vezes e observar o comportamento dos estimadores. O c√≥digo abaixo gera 100 simula√ß√µes de um processo AR(1) com tend√™ncia, estima os par√¢metros $\alpha$ e $\delta$ por OLS, e calcula os erros para cada simula√ß√£o. Em seguida, plota os histogramas dos erros multiplicados por suas respectivas taxas de converg√™ncia.

```python
import numpy as np
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# Parametros
alpha = 2
delta = 0.5
phi1 = 0.7
sigma = 1
T = 200
num_simulations = 100
np.random.seed(42)

alpha_errors = []
delta_errors = []
for _ in range(num_simulations):
    epsilon = np.random.normal(0, sigma, T)
    y = np.zeros(T)
    y[0] = alpha + delta + epsilon[0]
    for t in range(1, T):
        y[t] = alpha + delta * (t + 1) + phi1 * y[t-1] + epsilon[t]
    yt_star = y - (alpha + delta * np.arange(1, T+1))
    X_star = np.column_stack((np.ones(T), np.arange(1, T+1), np.concatenate(([0], yt_star[:-1]))))

    model = LinearRegression()
    model.fit(X_star, y)

    alpha_hat = model.intercept_
    delta_hat = model.coef_[1]

    alpha_errors.append(np.sqrt(T) * (alpha_hat - alpha))
    delta_errors.append(T**(3/2) * (delta_hat - delta))


plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.hist(alpha_errors, bins=20, density=True, alpha=0.6, color='blue')
plt.title("Distribui√ß√£o de Erros de Alpha (sqrt(T) * (alpha_hat - alpha))")
plt.xlabel("Erro Escalonado")
plt.ylabel("Densidade")

plt.subplot(1, 2, 2)
plt.hist(delta_errors, bins=20, density=True, alpha=0.6, color='green')
plt.title("Distribui√ß√£o de Erros de Delta (T^(3/2) * (delta_hat - delta))")
plt.xlabel("Erro Escalonado")
plt.ylabel("Densidade")
plt.tight_layout()
plt.show()
```
Este exemplo mostra que, ao multiplicar os erros pela sua respectiva taxa de converg√™ncia, ambos os estimadores convergem para distribui√ß√µes normais com m√©dia zero e vari√¢ncias diferentes, confirmando o resultado te√≥rico.

**Proposi√ß√£o 2.1** (Taxas de Converg√™ncia):  O estimador de $\alpha$, $\hat{\alpha}$, converge para $\alpha$ a uma taxa de $\sqrt{T}$, enquanto o estimador de $\delta$, $\hat{\delta}$, converge para $\delta$ a uma taxa de $T^{3/2}$.
*Proof.*
I. A distribui√ß√£o assint√≥tica de $\hat{\alpha}$ √© dada por $\sqrt{T}(\hat{\alpha} - \alpha) \xrightarrow{d} N(0, \sigma^2 g_\alpha [Q^*]^{-1} g_\alpha')$.
II. Esta distribui√ß√£o demonstra que $\sqrt{T}(\hat{\alpha} - \alpha)$ converge para uma distribui√ß√£o normal com vari√¢ncia finita.
III. Impl√≠citamente, $\hat{\alpha}$ converge para $\alpha$ a uma taxa de $\sqrt{T}$.
IV. A distribui√ß√£o assint√≥tica de $\hat{\delta}$ √© dada por $T^{3/2}(\hat{\delta} - \delta) \xrightarrow{d} N(0, \sigma^2 g_\delta [Q^*]^{-1} g_\delta')$.
V. Esta distribui√ß√£o demonstra que $T^{3/2}(\hat{\delta} - \delta)$ converge para uma distribui√ß√£o normal com vari√¢ncia finita.
VI. Impl√≠citamente, $\hat{\delta}$ converge para $\delta$ a uma taxa de $T^{3/2}$.
VII. Portanto, as taxas de converg√™ncia s√£o $\sqrt{T}$ para $\hat{\alpha}$ e $T^{3/2}$ para $\hat{\delta}$. $\blacksquare$

**Corol√°rio 2.1** (Infer√™ncia Assint√≥tica para $\alpha$ e $\delta$): As distribui√ß√µes assint√≥ticas de $\hat{\alpha}$ e $\hat{\delta}$ fornecem a base para construir intervalos de confian√ßa e realizar testes de hip√≥teses sobre os par√¢metros da constante e da tend√™ncia no modelo original.
*Proof.*
I. Pela Proposi√ß√£o 2, temos que os estimadores $\hat{\alpha}$ e $\hat{\delta}$ s√£o consistentes para $\alpha$ e $\delta$, respectivamente.
II. As distribui√ß√µes assint√≥ticas de $\sqrt{T}(\hat{\alpha} - \alpha)$ e $T^{3/2}(\hat{\delta} - \delta)$ convergem para distribui√ß√µes normais, conforme definido no texto.
III. A distribui√ß√£o assint√≥tica de um estimador descreve a forma como o estimador se comporta para amostras grandes.
IV. Intervalos de confian√ßa para $\alpha$ podem ser constru√≠dos com base na distribui√ß√£o normal limite $\sqrt{T}(\hat{\alpha} - \alpha) \xrightarrow{d} N(0, \sigma^2 g_\alpha [Q^*]^{-1} g_\alpha')$.
V. Intervalos de confian√ßa para $\delta$ podem ser constru√≠dos com base na distribui√ß√£o normal limite $T^{3/2}(\hat{\delta} - \delta) \xrightarrow{d} N(0, \sigma^2 g_\delta [Q^*]^{-1} g_\delta')$.
VI. Testes de hip√≥teses para $\alpha$ e $\delta$ podem ser conduzidos utilizando os resultados da distribui√ß√£o assint√≥tica.
VII. Portanto, as distribui√ß√µes assint√≥ticas de $\hat{\alpha}$ e $\hat{\delta}$ fornecem a base para construir intervalos de confian√ßa e realizar testes de hip√≥teses sobre os par√¢metros da constante e da tend√™ncia no modelo original. $\blacksquare$

> üí° **Exemplo Num√©rico:** Continuando o exemplo, constru√≠mos um intervalo de confian√ßa para $\alpha$ e $\delta$ com os resultados da simula√ß√£o.
```python
import numpy as np
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression
from scipy.stats import norm

# Parametros
alpha = 2
delta = 0.5
phi1 = 0.7
sigma = 1
T = 200
np.random.seed(42)
epsilon = np.random.normal(0, sigma, T)
y = np.zeros(T)
y[0] = alpha + delta + epsilon[0]
for t in range(1, T):
    y[t] = alpha + delta * (t + 1) + phi1 * y[t-1] + epsilon[t]
yt_star = y - (alpha + delta * np.arange(1, T+1))

# Regressores transformados
X_star = np.column_stack((np.ones(T), np.arange(1, T+1), np.concatenate(([0], yt_star[:-1]))))

# Modelo OLS
model = LinearRegression()
model.fit(X_star, y)

# Estimativas
alpha_star_hat = model.intercept_
delta_star_hat = model.coef_[1]
phi1_star_hat = model.coef_[2]

# Calculando os residuos
residuals = y - model.predict(X_star)

# Estimativa da variancia
sigma2_hat = np.sum(residuals**2)/(T-3)

# Calculando a matriz Q* (aproximada)
gamma0_star = np.var(yt_star)
gamma1_star = sm.tsa.stattools.acovf(yt_star,nlags=1)[1]
Q_star = np.array([[gamma0_star, gamma1_star, 0, 0],
                  [gamma1_star, gamma0_star, 0, 0],
                  [0, 0, 1, 0],
                  [0, 0, 0, 1/3]])

# Matriz de variancia-covari√¢ncia
Q_star_inv = np.linalg.inv(Q_star)
g_alpha = np.array([1,0,0,0]) # vetor g_alpha
g_delta = np.array([0,1,0,0]) # vetor g_delta

# Variancia assint√≥tica de alpha_hat
var_alpha_hat = sigma2_hat*np.dot(g_alpha,np.dot(Q_star_inv,g_alpha))

# Variancia assint√≥tica de delta_hat
var_delta_hat = sigma2_hat*np.dot(g_delta,np.dot(Q_star_inv,g_delta))


# Intervalo de confian√ßa para alpha
alpha_se = np.sqrt(var_alpha_hat/T)
alpha_ci = norm.interval(0.95, loc=alpha_star_hat, scale=alpha_se)

# Intervalo de confian√ßa para delta
delta_se = np.sqrt(var_delta_hat/T**3)
delta_ci = norm.interval(0.95, loc=delta_star_hat, scale=delta_se)

print("Intervalos de Confian√ßa (95%):")
print(f"Alpha: {alpha_ci}")
print(f"Delta: {delta_ci}")

```
O c√≥digo acima calcula os intervalos de confian√ßa para $\alpha$ e $\delta$ usando as distribui√ß√µes assint√≥ticas. Esses intervalos nos fornecem uma faixa plaus√≠vel de valores para os par√¢metros populacionais.

**Teorema 3** (Distribui√ß√£o Assint√≥tica para os Coeficientes AR): Para os coeficientes $\phi_j$, a distribui√ß√£o assint√≥tica √© dada por:
$$ \sqrt{T}(\hat{\phi_j} - \phi_j) \xrightarrow{d} N(0, \sigma^2 v_j) $$
Onde $v_j$ corresponde aos elementos apropriados da matriz $[Q^*]^{-1}$ associados aos coeficientes $\phi_j$ e $\hat{\phi_j}$ s√£o os coeficientes estimados do modelo original.
*Proof.*
I. O estimador $b^*$ do vetor $\beta^*$ possui a distribui√ß√£o assint√≥tica dada por $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$.
II. A rela√ß√£o entre os par√¢metros originais $\beta$ e os par√¢metros transformados $\beta^*$ √© $\beta = G' \beta^*$, onde $G'$ √© uma matriz de transforma√ß√£o.
III. Portanto, o estimador dos par√¢metros originais $\hat{\beta}$ √© dado por $\hat{\beta} = G'b^*$.
IV. Consequentemente, a distribui√ß√£o assint√≥tica de $\hat{\beta}$ √© dada por $Y_T(\hat{\beta} - \beta) \xrightarrow{d} N(0, \sigma^2 G' [Q^*]^{-1} G)$.
V. Os coeficientes AR $\phi_j$ s√£o um subvetor de $\beta$.
VI. Os elementos apropriados da matriz $G' [Q^*]^{-1} G$ correspondem √†s vari√¢ncias assint√≥ticas dos estimadores $\hat{\phi_j}$.
VII. Ao extrairmos esses elementos espec√≠ficos, denotados por $v_j$, encontramos que a distribui√ß√£o assint√≥tica dos coeficientes AR √© dada por $\sqrt{T}(\hat{\phi_j} - \phi_j) \xrightarrow{d} N(0, \sigma^2 v_j)$. $\blacksquare$

**Teorema 3.1** (Distribui√ß√£o Assint√≥tica conjunta de $\hat{\alpha}$, $\hat{\delta}$, e $\hat{\phi_j}$): A distribui√ß√£o assint√≥tica conjunta dos estimadores $\hat{\alpha}$, $\hat{\delta}$, e $\hat{\phi_j}$ √© dada por:
$$
\begin{pmatrix}
\sqrt{T}(\hat{\alpha} - \alpha) \\
T^{3/2}(\hat{\delta} - \delta) \\
\sqrt{T}(\hat{\phi_1} - \phi_1) \\
\vdots \\
\sqrt{T}(\hat{\phi_p} - \phi_p) \\
\end{pmatrix}
\xrightarrow{d} N(0, \Sigma)
$$
onde $\Sigma$ √© a matriz de covari√¢ncia assint√≥tica dada por $\sigma^2 G' [Q^*]^{-1} G$, e G √© a matriz de transforma√ß√£o correspondente.
*Proof.*
I.  A distribui√ß√£o assint√≥tica de $b^*$ √© dada por $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$.
II. A rela√ß√£o entre os par√¢metros originais eospar√¢metros transformados √© $b = h(\beta)$, onde $h(\cdot)$ √© uma fun√ß√£o diferenci√°vel.
III. Pelo Teorema da Fun√ß√£o Delta, a distribui√ß√£o assint√≥tica de $b^*$ pode ser transformada na distribui√ß√£o assint√≥tica de $b$:
$Y_T(b^* - b) = Y_T(h(\beta^*) - h(\beta)) \xrightarrow{d} N(0, \sigma^2 [H Q^{*-1} H^T])$, onde $H$ √© a matriz jacobiana $\frac{\partial h(\beta)}{\partial \beta'}$ avaliada em $\beta$.

Conclus√£o: A transforma√ß√£o dos par√¢metros n√£o afeta a validade da infer√™ncia assint√≥tica, desde que a fun√ß√£o de transforma√ß√£o seja diferenci√°vel.
<!-- END -->
