## DistribuiÃ§Ã£o AssintÃ³tica das Estimativas de MQO no Modelo de TendÃªncia Temporal Simples

### IntroduÃ§Ã£o
Este capÃ­tulo aborda a anÃ¡lise de processos com tendÃªncias temporais determinÃ­sticas, focando na derivaÃ§Ã£o da distribuiÃ§Ã£o assintÃ³tica das estimativas de mÃ­nimos quadrados ordinÃ¡rios (MQO) em modelos com essas tendÃªncias. Em contraste com modelos de regressÃ£o com variÃ¡veis estacionÃ¡rias, onde as distribuiÃ§Ãµes assintÃ³ticas das estimativas de coeficientes sÃ£o obtidas com mÃ©todos padrÃ£o, modelos com tendÃªncias temporais requerem uma abordagem diferente devido Ã s taxas de convergÃªncia distintas dos parÃ¢metros. Este capÃ­tulo constrÃ³i sobre os conceitos de estimaÃ§Ã£o por MQO e distribuiÃ§Ãµes assintÃ³ticas apresentados no capÃ­tulo 8, mas expande esses conceitos para lidar com a nÃ£o estacionariedade induzida por tendÃªncias temporais determinÃ­sticas.

### Conceitos Fundamentais
Consideremos o modelo de tendÃªncia temporal simples:
$$y_t = \alpha + \delta t + \epsilon_t$$
onde $\epsilon_t$ Ã© um processo de ruÃ­do branco, com $\epsilon_t \sim N(0, \sigma^2)$. Este modelo satisfaz as premissas clÃ¡ssicas de regressÃ£o. A forma da regressÃ£o padrÃ£o Ã© dada por:
$$y_t = x_t'\beta + \epsilon_t$$
onde $x_t = [1 \quad t]'$ e $\beta = [\alpha \quad \delta]' $. A estimativa de MQO de $\beta$, denotada por $b_T$, Ã© dada por:
$$b_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_t$$

O desvio do estimador MQO do valor verdadeiro pode ser expresso como:
$$b_T - \beta = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t$$
Para obter a distribuiÃ§Ã£o limitante para regressÃµes com variÃ¡veis estacionÃ¡rias, multiplicamos o resultado acima por $\sqrt{T}$:
$$\sqrt{T}(b_T - \beta) = \left(\frac{1}{T} \sum_{t=1}^T x_t x_t' \right)^{-1} \left(\frac{1}{\sqrt{T}} \sum_{t=1}^T x_t \epsilon_t \right)$$
A suposiÃ§Ã£o usual Ã© que $(1/T) \sum_{t=1}^T x_t x_t'$ converge em probabilidade para uma matriz nÃ£o singular $Q$ e que $(1/\sqrt{T}) \sum_{t=1}^T x_t \epsilon_t$ converge em distribuiÃ§Ã£o para uma variÃ¡vel aleatÃ³ria $N(0, \sigma^2 Q)$. Isto implica que $\sqrt{T}(b_T - \beta)$ converge para $N(0, \sigma^2 Q^{-1})$.

Entretanto, para uma tendÃªncia temporal determinÃ­stica, tal argumento nÃ£o Ã© vÃ¡lido. Para $x_t$ e $\beta$ dados, a expressÃ£o torna-se:

$$\begin{bmatrix} \hat{\alpha}_T - \alpha \\ \hat{\delta}_T - \delta \end{bmatrix} = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix}^{-1} \begin{bmatrix} \sum_{t=1}^T \epsilon_t \\ \sum_{t=1}^T t\epsilon_t \end{bmatrix} $$
> ğŸ’¡ **Exemplo NumÃ©rico:** Vamos supor que temos uma sÃ©rie temporal com $T=100$ e que os valores de $\epsilon_t$ sÃ£o gerados a partir de uma distribuiÃ§Ã£o normal com mÃ©dia 0 e desvio padrÃ£o 1. As somas de interesse sÃ£o: $\sum_{t=1}^{100} 1 = 100$, $\sum_{t=1}^{100} t = \frac{100(101)}{2} = 5050$ e $\sum_{t=1}^{100} t^2 = \frac{100(101)(201)}{6} = 338350$. Se simulamos $\epsilon_t$ para esses 100 pontos, podemos calcular os valores de $\sum_{t=1}^{100} \epsilon_t$ e $\sum_{t=1}^{100} t\epsilon_t$. Suponha que obtemos $\sum_{t=1}^{100} \epsilon_t = 5.2$ e $\sum_{t=1}^{100} t\epsilon_t = 120$. A matriz a ser invertida seria $\begin{bmatrix} 100 & 5050 \\ 5050 & 338350 \end{bmatrix}$. O resultado de $(X^TX)^{-1}X^T y$ nos daria as estimativas dos parÃ¢metros.
```python
import numpy as np
from numpy.linalg import inv

T = 100
t = np.arange(1, T+1)
X = np.vstack([np.ones(T), t]).T
epsilon = np.random.normal(0, 1, T)
sum_epsilon = np.sum(epsilon)
sum_t_epsilon = np.sum(t*epsilon)
XTX_inv = inv(X.T @ X)
beta_hat = XTX_inv @ np.array([sum_epsilon, sum_t_epsilon])

print(f"Soma dos erros: {sum_epsilon:.2f}")
print(f"Soma de t*erros: {sum_t_epsilon:.2f}")
print("Matriz (X'X)^-1:")
print(XTX_inv)
print("Estimativas dos parÃ¢metros:")
print(beta_hat)

```
As somas de sÃ©ries temporais determinÃ­sticas podem ser expressas como:
$$\sum_{t=1}^T t = \frac{T(T+1)}{2}$$
$$\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6}$$
Assim, o termo dominante em $\sum_{t=1}^T t$ Ã© $T^2/2$ e o termo dominante em $\sum_{t=1}^T t^2$ Ã© $T^3/3$. Uma forma geral para o termo dominante em $\sum_{t=1}^T t^v$ Ã© $\frac{T^{v+1}}{v+1}$.
**ProposiÃ§Ã£o 1**
*Para qualquer inteiro positivo $v$, a soma $\sum_{t=1}^T t^v$ Ã© assintoticamente equivalente a $\frac{T^{v+1}}{v+1}$, ou seja,
$$ \lim_{T \to \infty} \frac{\sum_{t=1}^T t^v}{\frac{T^{v+1}}{v+1}} = 1. $$*
*Prova:*
I. Vamos definir $S_v(T) = \sum_{t=1}^T t^v$. O objetivo Ã© mostrar que
    $$ \lim_{T \to \infty} \frac{S_v(T)}{\frac{T^{v+1}}{v+1}} = 1 $$
II. A prova serÃ¡ por induÃ§Ã£o. O caso base $v=1$ jÃ¡ foi dado como $\sum_{t=1}^T t = \frac{T(T+1)}{2}$.
    $$ \lim_{T \to \infty} \frac{\frac{T(T+1)}{2}}{\frac{T^2}{2}} = \lim_{T \to \infty} \frac{T^2+T}{T^2} = \lim_{T \to \infty} (1 + \frac{1}{T}) = 1 $$
III. O caso base $v=2$ tambÃ©m foi fornecido como $\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6}$.
$$ \lim_{T \to \infty} \frac{\frac{T(T+1)(2T+1)}{6}}{\frac{T^3}{3}} = \lim_{T \to \infty} \frac{2T^3 + 3T^2 + T}{2T^3} = \lim_{T \to \infty} (1 + \frac{3}{2T} + \frac{1}{2T^2}) = 1 $$
IV. Suponha que o resultado seja verdadeiro para um $v=k$, ou seja, $\lim_{T \to \infty} \frac{S_k(T)}{\frac{T^{k+1}}{k+1}} = 1$. Isso significa que $S_k(T) = \frac{T^{k+1}}{k+1} + O(T^k)$.
V. A FÃ³rmula de Faulhaber expressa $S_v(T)$ como um polinÃ´mio de grau $v+1$ em $T$ da forma:
$$ S_v(T) = \frac{T^{v+1}}{v+1} + \frac{1}{2}T^v + \sum_{k=2}^v \frac{B_k}{k!} v^{\underline{k-1}} T^{v-k+1} $$
   onde $B_k$ sÃ£o os nÃºmeros de Bernoulli e $v^{\underline{k-1}}$ denota o fatorial descendente.
VI. O termo dominante desse polinÃ´mio Ã© $\frac{T^{v+1}}{v+1}$. Assim, podemos escrever:
$$ S_v(T) = \frac{T^{v+1}}{v+1} + O(T^v) $$
VII. Agora podemos calcular o limite:
$$ \lim_{T \to \infty} \frac{S_v(T)}{\frac{T^{v+1}}{v+1}} = \lim_{T \to \infty} \frac{\frac{T^{v+1}}{v+1} + O(T^v)}{\frac{T^{v+1}}{v+1}} = \lim_{T \to \infty} \left( 1 + \frac{O(T^v)}{\frac{T^{v+1}}{v+1}} \right) $$
VIII. Como o termo $O(T^v)$ Ã© de ordem inferior a $T^{v+1}$, o limite se torna:
$$ \lim_{T \to \infty} \left( 1 + \frac{O(T^v)}{\frac{T^{v+1}}{v+1}} \right) = 1 + 0 = 1 $$
Portanto, a proposiÃ§Ã£o Ã© vÃ¡lida. â– 

Para a matriz de covariÃ¢ncia:
$$\sum_{t=1}^T x_t x_t' = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix} = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} $$
> ğŸ’¡ **Exemplo NumÃ©rico:**  Usando $T=100$, temos a matriz $\sum_{t=1}^T x_t x_t' = \begin{bmatrix} 100 & 5050 \\ 5050 & 338350 \end{bmatrix}$. Se dividirmos essa matriz por $T=100$, temos $\begin{bmatrix} 1 & 50.5 \\ 50.5 & 3383.5 \end{bmatrix}$, que diverge conforme $T \rightarrow \infty$. No entanto, se dividirmos por $T^3=1000000$, temos $\begin{bmatrix} 0.0001 & 0.00505 \\ 0.00505 & 0.33835 \end{bmatrix}$, que converge para uma matriz zero Ã  medida que $T$ aumenta. Isso mostra a necessidade de ajustar as taxas de convergÃªncia.

O termo $(1/T) \sum_{t=1}^T x_t x_t'$ diverge, em contraste com os casos de regressÃ£o estacionÃ¡ria. Para obter uma matriz convergente, a matriz acima deve ser dividida por $T^3$ em vez de $T$. A matriz limitante, nesse caso, nÃ£o Ã© invertÃ­vel, sendo necessÃ¡rio uma abordagem diferente para o cÃ¡lculo da distribuiÃ§Ã£o assintÃ³tica de $b_T$.

As estimativas de MQO $\hat{\alpha}_T$ e $\hat{\delta}_T$ possuem diferentes taxas de convergÃªncia. Para derivar distribuiÃ§Ãµes assintÃ³ticas nÃ£o degeneradas, $\hat{\alpha}_T$ Ã© multiplicado por $\sqrt{T}$, enquanto $\hat{\delta}_T$ Ã© multiplicado por $T^{3/2}$. Isso pode ser considerado como a prÃ©-multiplicaÃ§Ã£o de pela matriz:

$$Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}$$
Resultando em:
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1} Y_T \sum_{t=1}^T x_t \epsilon_t =  Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1}  \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix} $$

O primeiro termo na expressÃ£o acima, usando, pode ser simplificado, e seu limite Ã©:
$$Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1} \rightarrow Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix} $$
> ğŸ’¡ **Exemplo NumÃ©rico:** Para $T=100$, podemos calcular $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1}$ com:
```python
T = 100
t = np.arange(1, T + 1)
X = np.vstack([np.ones(T), t]).T
Y_T = np.array([[np.sqrt(T), 0], [0, T**(3/2)]])
XTX = X.T @ X
XTX_inv = inv(XTX)
Q_hat = Y_T @ XTX_inv @ Y_T.T

print("Y_T (X'X)^-1 Y_T':")
print(Q_hat)

Q = np.array([[1, 1/2], [1/2, 1/3]])
print("Q:")
print(Q)
```
Podemos observar que o resultado de `Q_hat` converge para a matriz $Q$ conforme $T$ aumenta. Isso demonstra como a matriz $Y_T$ corrige a divergÃªncia da matriz original.
O segundo termo Ã©:
$$ Y_T \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix} $$

Sob as suposiÃ§Ãµes padrÃ£o de $\epsilon_t$ (i.i.d., mÃ©dia zero, variÃ¢ncia $\sigma^2$ e quarto momento finito), esse vetor converge para uma distribuiÃ§Ã£o normal bivariada. O primeiro elemento, $(1/\sqrt{T}) \sum_{t=1}^T \epsilon_t$, converge em distribuiÃ§Ã£o para $N(0, \sigma^2)$ pelo teorema do limite central. O segundo elemento, $(1/\sqrt{T})\sum_{t=1}^T (t/T)\epsilon_t$, Ã© uma sequÃªncia de diferenÃ§as de martingales e converge para $N(0, \sigma^2/3)$. A variÃ¢ncia Ã© dada por:
$$\sigma_T^2 = E[(t/T)\epsilon_t]^2 = \sigma^2(t^2/T^2)$$
e
$$(1/T) \sum_{t=1}^T \sigma_t^2 = (1/T)\sigma^2\sum_{t=1}^T(t^2/T^2) \rightarrow \sigma^2/3$$
> ğŸ’¡ **Exemplo NumÃ©rico:** Para verificar a convergÃªncia da variÃ¢ncia do segundo elemento, vamos gerar algumas amostras de $\epsilon_t$ com $\sigma^2 = 1$ e calcular a variÃ¢ncia:
```python
import numpy as np

T = 1000
num_simulations = 1000
variances = []
for _ in range(num_simulations):
    epsilon = np.random.normal(0, 1, T)
    weighted_epsilon = (1/np.sqrt(T)) * np.sum((np.arange(1, T+1)/T) * epsilon)
    variances.append(weighted_epsilon**2)
empirical_variance = np.mean(variances)
theoretical_variance = 1/3
print(f"VariÃ¢ncia empÃ­rica: {empirical_variance:.4f}")
print(f"VariÃ¢ncia teÃ³rica: {theoretical_variance:.4f}")
```
Os valores mostram que a variÃ¢ncia empÃ­rica se aproxima de 1/3, como esperado.

A covariÃ¢ncia entre os dois elementos converge para zero, entÃ£o eles sÃ£o assintoticamente independentes. Assim, o vetor converge para $N(0, \sigma^2 Q)$, onde $Q$ Ã© a matriz definida.
*Prova da convergÃªncia para $N(0, \sigma^2/3)$ do segundo elemento:*
I. O segundo elemento Ã© dado por:
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t $$
II. Definindo $Z_t = \frac{t}{T}\epsilon_t$, temos uma sequÃªncia de diferenÃ§as de martingales com respeito Ã  sua prÃ³pria filtragem, tal que $E[Z_t | Z_{t-1}, Z_{t-2}, ...] = 0$.
III. A variÃ¢ncia condicional de $Z_t$ Ã© dada por
    $$ \sigma_t^2 = E[Z_t^2 | Z_{t-1}, Z_{t-2}, ...] = E \left[ \left(\frac{t}{T} \epsilon_t\right)^2 \right] = \frac{t^2}{T^2} E[\epsilon_t^2] = \frac{t^2}{T^2} \sigma^2 $$
IV.  Pelo teorema do limite central para martingales, sabemos que $\frac{1}{\sqrt{T}} \sum_{t=1}^T Z_t$ converge em distribuiÃ§Ã£o para $N(0, \sigma^2_Z)$, onde $\sigma^2_Z = \lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^T \sigma_t^2$.
V. EntÃ£o,
$$ \sigma^2_Z = \lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^T \frac{t^2}{T^2} \sigma^2 = \sigma^2 \lim_{T \to \infty} \frac{1}{T^3} \sum_{t=1}^T t^2 $$
VI. Pela ProposiÃ§Ã£o 1, sabemos que $\sum_{t=1}^T t^2$ Ã© assintoticamente equivalente a $\frac{T^3}{3}$, entÃ£o:
$$ \sigma^2_Z = \sigma^2 \lim_{T \to \infty} \frac{1}{T^3} \frac{T^3}{3} = \frac{\sigma^2}{3} $$
VII. Portanto, $\frac{1}{\sqrt{T}}\sum_{t=1}^T \frac{t}{T} \epsilon_t$ converge em distribuiÃ§Ã£o para $N(0, \sigma^2/3)$. â– 

Utilizando o resultado do exemplo 7.5 do capÃ­tulo 7, a distribuiÃ§Ã£o assintÃ³tica de Ã© dada por:
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \rightarrow N(0, \sigma^2 Q^{-1})$$
**Teorema 1.1**
*Seja $\hat{\beta}_T = [\hat{\alpha}_T, \hat{\delta}_T]' $ o estimador de MQO do modelo $y_t = \alpha + \delta t + \epsilon_t$, onde $\epsilon_t$ Ã© um ruÃ­do branco com mÃ©dia zero, variÃ¢ncia $\sigma^2$ e quarto momento finito. EntÃ£o,
$$
\begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q^{-1})
$$
onde $Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$ e $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$*
> ğŸ’¡ **Exemplo NumÃ©rico:** Se $\sigma^2 = 1$ e $T=100$, a matriz de covariÃ¢ncia da distribuiÃ§Ã£o assintÃ³tica Ã© $\sigma^2 Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$. Isso significa que a variÃ¢ncia assintÃ³tica de $\sqrt{T}(\hat{\alpha}_T - \alpha)$ Ã© 4 e a variÃ¢ncia assintÃ³tica de $T^{3/2}(\hat{\delta}_T - \delta)$ Ã© 12.
*Prova:*
I. A partir da discussÃ£o anterior, sabemos que
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} = Y_T(b_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}  \sum_{t=1}^T x_t \epsilon_t $$
II. TambÃ©m sabemos que $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1} \rightarrow Q$, onde $Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$
III. Definimos
$$ Y_T \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix} $$
IV. Anteriormente mostramos que esse vetor converge para uma distribuiÃ§Ã£o normal com mÃ©dia zero e matriz de covariÃ¢ncia $\sigma^2 Q$. Especificamente,
$$\begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q)$$
V. Portanto, pela aplicaÃ§Ã£o do teorema de Slutsky, temos que
$$Y_T(b_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1})$$
Onde $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$.
Portanto, o teorema estÃ¡ demonstrado.â– 

### ConclusÃ£o
Esta seÃ§Ã£o apresentou a derivaÃ§Ã£o da distribuiÃ§Ã£o assintÃ³tica das estimativas de MQO para o modelo de tendÃªncia temporal simples. As principais conclusÃµes sÃ£o que: (1) As estimativas de MQO convergem para seus verdadeiros valores, mas em taxas diferentes, especificamente $\hat{\alpha}_T$ converge a uma taxa de $\sqrt{T}$ e $\hat{\delta}_T$ converge a uma taxa de $T^{3/2}$. (2) Para obter distribuiÃ§Ãµes limitantes nÃ£o degeneradas, Ã© necessÃ¡rio redimensionar as estimativas com essas taxas de convergÃªncia. (3) A distribuiÃ§Ã£o limitante dos estimadores redimensionados Ã© normal bivariada, com mÃ©dia zero e uma matriz de covariÃ¢ncia que depende da variÃ¢ncia do erro e de uma matriz Q relacionada com as taxas de convergÃªncia.

Esta anÃ¡lise demonstra a importÃ¢ncia de considerar as taxas de convergÃªncia assintÃ³tica em modelos com tendÃªncias temporais, destacando que os resultados padrÃ£o para regressÃ£o com variÃ¡veis estacionÃ¡rias nÃ£o sÃ£o diretamente aplicÃ¡veis. O capÃ­tulo prossegue para desenvolver esses resultados e investigar testes de hipÃ³teses e a sua aplicabilidade em contextos de regressÃ£o com tendÃªncias temporais.

### ReferÃªncias
[^1]:  *Os coeficientes de modelos de regressÃ£o envolvendo raÃ­zes unitÃ¡rias ou tendÃªncias temporais determinÃ­sticas sÃ£o tipicamente estimados por mÃ­nimos quadrados ordinÃ¡rios.*
[^2]:  *No entanto, as distribuiÃ§Ãµes assintÃ³ticas das estimativas dos coeficientes nÃ£o podem ser calculadas da mesma forma que aquelas para modelos de regressÃ£o envolvendo variÃ¡veis estacionÃ¡rias.*
[^3]: *This section considers OLS estimation of the parameters of a simple time trend, y_t = \alpha + \delta t + \epsilon_t, for \epsilon_t a white noise process. If \epsilon_t ~ N(0, \sigmaÂ²), then the model [16.1.1] satisfies the classical regression assumptions...*
[^4]: *Write [16.1.1] in the form of the standard regression model, y_t = x_t'\beta + \epsilon_t*
[^5]: *where x_t = [1  t]'*
[^6]: *Î² = [Î±  Î´]'*
[^7]: *Let b_T denote the OLS estimate of Î² based on a sample of size T: b_T = [Î±_T  Î´_T]' = (Î£ x_t x_t')^-1 Î£ x_t y_t*
[^8]: *Recall from equation [8.2.3] that the deviation of the OLS estimate from the true value can be expressed as (b_T - Î²) = (Î£ x_t x_t')^-1 Î£ x_t \epsilon_t*
[^9]: *(b_T - Î²) = (Î£ x_t x_t')^-1 Î£ x_t \epsilon_t*
[^10]: *To find the limiting distribution for a regression with stationary explanatory variables, the approach in Chapter 8 was to multiply [16.1.6] by âˆšT, resulting in âˆšT(b_T - Î²) = [(1/T) Î£ x_t x_t']^-1 [(1/âˆšT) Î£ x_t \epsilon_t]*
[^11]: *The usual assumption was that (1/T) Î£ x_t x_t' converged in probability to a nonsingular matrix Q while (1/âˆšT) Î£ x_t \epsilon_t converged in distribution to a N(0, \sigmaÂ²Q) random variable, implying that âˆšT(b_T - Î²) -> N(0, \sigmaÂ²Q^-1).*
[^12]: *...expression [16.1.6] would be [(Î±_T - Î±)  (Î´_T - Î´)]' = [Î£ 1  Î£ t ; Î£ t  Î£ t^2]^-1 [Î£ \epsilon_t ; Î£ t\epsilon_t]*
[^13]: *[(Î±_T - Î±)  (Î´_T - Î´)]' = [Î£ 1  Î£ t ; Î£ t  Î£ t^2]^-1 [Î£ \epsilon_t ; Î£ t\epsilon_t]*
[^14]: *It is straightforward to show by induction that Î£ t = T(T+1)/2*
[^15]: *Î£ t^2 = T(T+1)(2T+1)/6.*
[^16]: *Thus, the leading term in Î£ t is TÂ²/2; that is, (1/TÂ²) Î£ t = 1/2 + 1/(2T) -> 1/2*
[^17]: *Similarly, the leading term in Î£ tÂ² is TÂ³/3: (1/TÂ³) Î£ tÂ² = 1/3 + 1/(2T) + 1/(6TÂ²) -> 1/3.*
[^18]: *For future reference, we note here the general pattern-the leading term in Î£ t^v is T^(v+1)/(v+1): (1/T^(v+1)) Î£ t^v -> 1/(v+1)*
[^19]: *For x, given in [16.1.3], results [16.1.9] and [16.1.10] imply that Î£ x_t x_t' = [Î£ 1  Î£ t ; Î£ t  Î£ t^2] = [T  T(T+1)/2 ;  T(T+1)/2  T(T+1)(2T+1)/6]*
[^20]: *Î£ x_t x_t' = [Î£ 1  Î£ t ; Î£ t  Î£ t^2] = [T  T(T+1)/2 ;  T(T+1)/2  T(T+1)(2T+1)/6]*
[^21]: *In contrast to the usual result for stationary regressions, for the matrix in [16.1.16], (1/T) Î£ x_t x_t' diverges. To obtain a convergent matrix, [16.1.16] would have to be divided by TÂ³ rather than T.*
[^22]: *Unfortunately, this limiting matrix cannot be inverted, as (1/T) Î£ x_t x_t' can be in the usual case. Hence, a different approach from that in the stationary case will be needed to calculate the asymptotic distribution of b_T.*
[^23]: *It turns out that the OLS estimates Î±_T and Î´_T have different asymptotic rates of convergence. To arrive at nondegenerate limiting distributions, Î±_T is multiplied by âˆšT, whereas Î´_T must be multiplied by T^3/2.*
[^24]: *We can think of this adjustment as premultiplying [16.1.6] or [16.1.8] by the matrix Y_T = [âˆšT  0 ; 0  T^(3/2)]*
[^25]: *Y_T = [âˆšT  0 ; 0  T^(3/2)]*
[^26]: *resulting in [âˆšT(Î±_T - Î±)  ; T^(3/2)(Î´_T - Î´)]' = Y_T (Î£ x_t x_t')^-1  Y_T^-1 [ (1/âˆšT) Î£ x_t \epsilon_t]*
[^27]: *[âˆšT(Î±_T - Î±)  ; T^(3/2)(Î´_T - Î´)]' = Y_T (Î£ x_t x_t')^-1  Y_T^-1 [ (1/âˆšT) Î£ x_t \epsilon_t]*
[^28]: *Consider the first term in the last expression of [16.1.18]. Substituting from [16.1.17] and [16.1.16], {Y_T (Î£ x_t x_t')^-1 Y_T^-1} = [T^(-1/2)  0 ; 0  T^(-3/2)]  [ Î£ 1  Î£ t ; Î£ t  Î£ t^2 ]^-1 [T^(-1/2) 0 ; 0 T^(-3/2)] = [T^(-1/2)  0 ; 0  T^(-3/2)] [T  T(T+1)/2 ; T(T+1)/2  T(T+1)(2T+1)/6 ]^-1 [T^(-1/2) 0 ; 0 T^(-3/2)]*
[^29]: *Thus, it follows from [16.1.11] and [16.1.12] that  {Y_T (Î£ x_t x_t')^-1 Y_T^-1}  -> Q = [1  1/2 ; 1/2  1/3]*
[^30]: *Turning next to the second term in [16.1.18], Y_T Î£ x_t \epsilon_t = [ T^(-1/2) 0 ; 0 T^(-3/2) ] [Î£ \epsilon_t  ; Î£ t \epsilon_t] = [ (1/âˆšT) Î£ \epsilon_t ; (1/âˆšT) Î£ (t/T) \epsilon_t]*
[^31]: *Y_T Î£ x_t \epsilon_t = [ (1/âˆšT) Î£ \epsilon_t ; (1/âˆšT) Î£ (t/T) \epsilon_t]. Under standard assumptions about \epsilon_t , this vector will be asymptotically Gaussian.*
[^32]: *For example, suppose that \epsilon_t is i.i.d. with mean zero, variance \sigmaÂ², and finite fourth moment. Then the first element of the vector in [16.1.21] satisfies (1/âˆšT) Î£ \epsilon_t -> N(0, \sigmaÂ²), by the central limit theorem.*
[^33]: *For the second element of the vector in [16.1.21], observe that {(t/T)\epsilon_t} is a martingale difference sequence that satisfies the conditions of Proposition 7.8. Specifically, its variance is \sigma_T^2 = E[(t/T)\epsilon_t]Â² = \sigmaÂ²(tÂ²/TÂ²)*
[^34]: *\sigma_T^2 = E[(t/T)\epsilon_t]Â² = \sigmaÂ²(tÂ²/TÂ²)*
[^35]: *(1/T) Î£ \sigma_t^2 = \sigmaÂ²(1/T) Î£(tÂ²/TÂ²) -> \sigmaÂ²/3*
[^36]: *But from [16.1.13], T times the magnitude in [16.1.22] converges to (1/5)E(\epsilon_t^4 - \sigma^4), meaning that [16.1.22] itself converges to zero: (1/T) Î£ [ (t/T) \epsilon_t]^2 - (1/T) Î£ (t/T)Â² \sigma^2  -> 0. But this implies that (1/âˆšT) Î£ (t/T)\epsilon_t -> N(0, \sigmaÂ²/3), as claimed.*
[^37]: *Finally, consider the joint distribution of the two elements in the (2x1) vector described by [16.1]. We have already established that âˆšT \bar{\epsilon} -> N(0, \sigma^2) and (1/âˆšT) Î£ (t/T)\epsilon_t -> N(0, \sigmaÂ²/3). To see that these are also jointly Gaussian, we can use the same type of argument we employed previously, by considering any linear combination of these two elements. Specifically, we can analyze:

$$a\sqrt{T}\bar{\epsilon} + b\frac{1}{\sqrt{T}}\sum_{t=1}^{T}\frac{t}{T}\epsilon_t$$

$$ = \frac{1}{\sqrt{T}}\sum_{t=1}^{T}a\epsilon_t + \frac{1}{\sqrt{T}}\sum_{t=1}^{T}\frac{bt}{T}\epsilon_t $$

$$ = \frac{1}{\sqrt{T}}\sum_{t=1}^{T}\left(a+\frac{bt}{T}\right)\epsilon_t $$

This is a linear combination of independent random variables, so by the CLT, this expression converges in distribution to a normal with zero mean and variance given by:

$$\lim_{T \to \infty} \frac{1}{T}\sum_{t=1}^{T} \left(a+\frac{bt}{T}\right)^2\sigma^2 $$

$$ = \sigma^2 \lim_{T \to \infty} \frac{1}{T}\sum_{t=1}^{T} \left(a^2 + \frac{2abt}{T} + \frac{b^2t^2}{T^2}\right) $$

$$ = \sigma^2 \lim_{T \to \infty} \left( a^2 + \frac{2ab}{T^2} \sum_{t=1}^{T} t + \frac{b^2}{T^3} \sum_{t=1}^{T} t^2 \right) $$

$$ = \sigma^2 \left( a^2 + \frac{2ab}{T^2}\frac{T(T+1)}{2} + \frac{b^2}{T^3} \frac{T(T+1)(2T+1)}{6} \right) $$

Taking the limits yields:

$$ = \sigma^2 \left( a^2 + ab + \frac{b^2}{3} \right) $$

Therefore, the joint distribution of âˆšT \bar{\epsilon} and (1/âˆšT) Î£ (t/T)\epsilon_t is jointly Gaussian, with variance covariance matrix:

$$ \begin{bmatrix} \sigma^2 & \sigma^2/2 \\ \sigma^2/2 & \sigma^2/3 \end{bmatrix} $$

This completes the proof. $\blacksquare$

<!-- END -->
