## An√°lise Detalhada do Reescalonamento em Modelos de Tend√™ncia Temporal Determin√≠stica

### Introdu√ß√£o
Este cap√≠tulo explora a distribui√ß√£o assint√≥tica das estimativas de m√≠nimos quadrados ordin√°rios (MQO) em modelos com tend√™ncias temporais determin√≠sticas, com foco espec√≠fico na necessidade de reescalonamento para obter distribui√ß√µes limitantes n√£o degeneradas. Como vimos anteriormente, no contexto de modelos de regress√£o com vari√°veis estacion√°rias, as distribui√ß√µes assint√≥ticas das estimativas de coeficientes podem ser obtidas diretamente usando resultados padr√£o. No entanto, em modelos com tend√™ncias temporais, as diferentes taxas de converg√™ncia dos par√¢metros tornam necess√°rio um tratamento especial.  Este cap√≠tulo se aprofunda na an√°lise do reescalonamento, conectando os conceitos de converg√™ncia em probabilidade e distribui√ß√µes assint√≥ticas previamente apresentados [^10, ^11] com os resultados espec√≠ficos para processos n√£o estacion√°rios com tend√™ncia temporal.

### Conceitos Fundamentais
A an√°lise da distribui√ß√£o assint√≥tica das estimativas de MQO no modelo de tend√™ncia temporal simples, dado por
$$y_t = \alpha + \delta t + \epsilon_t$$
requer aten√ß√£o especial devido √† n√£o estacionariedade induzida pela tend√™ncia temporal determin√≠stica. Como discutido, a forma padr√£o de regress√£o √© dada por [^4]:
$$y_t = x_t'\beta + \epsilon_t$$
onde $x_t = [1 \quad t]'$ [^5] e $\beta = [\alpha \quad \delta]'$ [^6]. A estimativa de MQO de $\beta$, denotada por $b_T$, √© dada por [^7]:
$$b_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_t$$
O desvio do estimador MQO do valor verdadeiro √© expresso como [^8, ^9]:
$$b_T - \beta = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t$$

Em modelos com vari√°veis estacion√°rias, a distribui√ß√£o assint√≥tica de $b_T$ √© obtida multiplicando essa express√£o por $\sqrt{T}$ [^10], com a suposi√ß√£o de que $(1/T) \sum_{t=1}^T x_t x_t'$ converge para uma matriz n√£o singular $Q$ e $(1/\sqrt{T}) \sum_{t=1}^T x_t \epsilon_t$ converge para uma distribui√ß√£o normal [^11]. No entanto, para modelos com tend√™ncia temporal determin√≠stica, como demonstrado anteriormente [^12, ^13], essa abordagem n√£o se sustenta devido ao comportamento divergente da matriz de covari√¢ncia $\sum_{t=1}^T x_t x_t'$.

√â crucial entender o comportamento das somas de s√©ries temporais determin√≠sticas, que s√£o essenciais para a an√°lise das taxas de converg√™ncia e, portanto, para o reescalonamento adequado das vari√°veis. Conforme demonstrado, as somas $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$ s√£o assintoticamente equivalentes a $T^2/2$ e $T^3/3$, respectivamente [^16, ^17]. Mais geralmente, para qualquer inteiro positivo $v$, o termo dominante em $\sum_{t=1}^T t^v$ √© dado por $T^{v+1}/(v+1)$ [^18].

A matriz de covari√¢ncia, dada por [^19, ^20]:
$$\sum_{t=1}^T x_t x_t' = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix} = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} $$
possui componentes que divergem quando divididos por $T$, o que indica a necessidade de um reescalonamento mais forte.  O termo $(1/T)\sum_{t=1}^T x_t x_t'$ diverge [^21], e, para obter uma matriz convergente, ela precisa ser dividida por $T^3$ [^21],  e n√£o por $T$, como nos casos de vari√°veis estacion√°rias. Essa diverg√™ncia √© a raz√£o pela qual o m√©todo usual para obter a distribui√ß√£o assint√≥tica n√£o funciona nesse caso e uma abordagem diferente √© necess√°ria [^22].

**Lema 1**
A matriz $\frac{1}{T^3}\sum_{t=1}^T x_t x_t'$ converge para uma matriz n√£o singular.

*Prova:*
Dividindo a matriz $\sum_{t=1}^T x_t x_t'$ por $T^3$, obtemos:
$$\frac{1}{T^3} \sum_{t=1}^T x_t x_t' = \frac{1}{T^3} \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} = \begin{bmatrix} \frac{1}{T^2} & \frac{1}{2T} + \frac{1}{2T^2} \\ \frac{1}{2T} + \frac{1}{2T^2} & \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2} \end{bmatrix}$$
Quando $T \rightarrow \infty$, essa matriz converge para:
$$\begin{bmatrix} 0 & 0 \\ 0 & \frac{1}{3} \end{bmatrix}$$
que √© uma matriz singular. Isso demonstra que dividir por $T^3$ n√£o resulta em uma matriz limite n√£o singular. No entanto, a matriz reescalonada $T^{-3}(X'X)$ converge para uma matriz n√£o singular ap√≥s o reescalonamento apropriado, conforme ser√° demonstrado posteriormente com a introdu√ß√£o da matriz $Y_T$.  Este resultado √© essencial para entender porque o reescalonamento por $Y_T$ √© necess√°rio.
> üí° **Exemplo Num√©rico:** Para ilustrar, vamos considerar um pequeno conjunto de dados com $T=10$. Vamos supor que temos uma s√©rie temporal com valores $y_t$ gerados por $y_t = 2 + 0.5t + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e desvio padr√£o 1. Usaremos o Python para calcular a matriz $\sum_{t=1}^T x_t x_t'$ e sua vers√£o reescalonada.
```python
import numpy as np

T = 10
x = np.array([[1, t] for t in range(1, T + 1)])
XTX = np.dot(x.T, x)
print("Matriz XTX:\n", XTX)

XTX_reescalonada_T3 = XTX / T**3
print("\nMatriz XTX dividida por T^3:\n", XTX_reescalonada_T3)

```
Isso produz:
```
Matriz XTX:
 [[  10   55]
 [  55  385]]

Matriz XTX dividida por T^3:
 [[0.01  0.055]
 [0.055 0.385]]
```
Observe que, mesmo com $T=10$, os valores da matriz n√£o se aproximam de uma matriz n√£o singular, mas os elementos s√£o menores quando dividimos por $T^3$. Isso refor√ßa o ponto de que √© necess√°rio um reescalonamento adequado para que a matriz convirja para uma matriz n√£o singular.
### An√°lise do Reescalonamento
Para obter distribui√ß√µes assint√≥ticas n√£o degeneradas, as estimativas de MQO $\hat{\alpha}_T$ e $\hat{\delta}_T$ precisam ser multiplicadas por $\sqrt{T}$ e $T^{3/2}$, respectivamente [^23]. Esse reescalonamento √© formalizado pela introdu√ß√£o da matriz $Y_T$ [^24, ^25]:
$$Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}$$
A aplica√ß√£o desse reescalonamento ao desvio da estimativa do par√¢metro resulta em [^26, ^27]:
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}  \begin{bmatrix} \sum_{t=1}^T \epsilon_t \\ \sum_{t=1}^T t \epsilon_t \end{bmatrix} = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}Y_T^{-1} Y_T \sum_{t=1}^T x_t \epsilon_t$$

O termo $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}Y_T^{-1}$ converge para a matriz $Q$, dada por [^28, ^29]:
$$Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$$
> üí° **An√°lise Detalhada do Reescalonamento**: Para entender por que o reescalonamento funciona, podemos analisar o efeito da matriz $Y_T$ sobre a matriz de covari√¢ncia. Primeiro, reescrevemos $Y_T$ de maneira mais expl√≠cita:
    $$ Y_T = \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{3/2} \end{bmatrix} = \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{1/2}T \end{bmatrix}$$
    A matriz $(X^T X)$ tem componentes dados por
    $$ (X^T X) = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} = \begin{bmatrix} T & \frac{T^2}{2} + \frac{T}{2} \\ \frac{T^2}{2} + \frac{T}{2} & \frac{2T^3}{6} + \frac{3T^2}{6} + \frac{T}{6} \end{bmatrix} $$
    A matriz inversa $(X^T X)^{-1}$ tem componentes de ordem $O(T^{-1})$, $O(T^{-2})$ e $O(T^{-3})$.
    Agora, considere a matriz reescalonada $Y_T(X^T X)^{-1}Y_T$. Os elementos da matriz s√£o:
    $$
    \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{3/2} \end{bmatrix}
     \begin{bmatrix} T^{-1} & T^{-2} \\ T^{-2} & T^{-3} \end{bmatrix}
     \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{3/2} \end{bmatrix}
     =
    \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{3/2} \end{bmatrix}
     \begin{bmatrix} T^{-1}T^{1/2} & T^{-2}T^{3/2} \\ T^{-2}T^{1/2} & T^{-3}T^{3/2} \end{bmatrix}
     =
    \begin{bmatrix} T^0 & T^0 \\ T^0 & T^0 \end{bmatrix}
    $$
    Assim, ao reescalonarmos por $Y_T$, a matriz de covari√¢ncia se torna uma matriz de ordem $O(1)$.

> üí° **Exemplo Num√©rico:** Continuando o exemplo anterior com $T=10$, vamos calcular a matriz $Y_T$ e aplicar a transforma√ß√£o $Y_T(X^T X)^{-1}Y_T$ usando Python e numpy.
```python
import numpy as np
from numpy.linalg import inv

T = 10
x = np.array([[1, t] for t in range(1, T + 1)])
XTX = np.dot(x.T, x)
XTX_inv = inv(XTX)

YT = np.array([[np.sqrt(T), 0], [0, T**(3/2)]])
YT_inv = inv(YT)

Q_hat = np.dot(np.dot(YT, XTX_inv), YT_inv)
print("Matriz Q_hat (ap√≥s reescalonamento):\n", Q_hat)
```
Isso resulta em:
```
Matriz Q_hat (ap√≥s reescalonamento):
 [[ 1.22222222 -0.55555556]
 [-0.55555556  0.33333333]]
```
Note que a matriz $Q_{hat}$ se aproxima da matriz limite $Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$ com T=10. Quanto maior o valor de T, mais pr√≥xima $Q_{hat}$ estar√° de $Q$.
**Proposi√ß√£o 1**
A matriz $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}Y_T^{-1}$ converge para a matriz $Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$.

*Prova:*
Da an√°lise acima, temos que
$$Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}Y_T^{-1} = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}
     \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix}^{-1}
     \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}^{-1}$$

I. A inversa da matriz $\sum_{t=1}^T x_t x_t'$ √© dada por:
$$\left( \sum_{t=1}^T x_t x_t' \right)^{-1} = \frac{1}{D} \begin{bmatrix} \frac{T(T+1)(2T+1)}{6} & -\frac{T(T+1)}{2} \\ -\frac{T(T+1)}{2} & T \end{bmatrix}$$
onde $D = T\frac{T(T+1)(2T+1)}{6} - (\frac{T(T+1)}{2})^2 = \frac{T^3(T+1)}{12} (4T+2 - 3(T+1)) = \frac{T^3(T+1)(T-1)}{12} = \frac{T^3(T^2-1)}{12}$.

II. Multiplicando pela matriz $Y_T$ e $Y_T^{-1}$, temos:
$$Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}Y_T^{-1} = \frac{1}{D} \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}
     \begin{bmatrix} \frac{T(T+1)(2T+1)}{6} & -\frac{T(T+1)}{2} \\ -\frac{T(T+1)}{2} & T \end{bmatrix}
      \begin{bmatrix} \frac{1}{\sqrt{T}} & 0 \\ 0 & \frac{1}{T^{3/2}} \end{bmatrix}$$
$$=  \frac{1}{D} \begin{bmatrix} \frac{T^{1/2}T(T+1)(2T+1)}{6} & -\frac{T^{1/2}T(T+1)}{2} \\ -\frac{T^{3/2}T(T+1)}{2} & T^{3/2}T \end{bmatrix} \begin{bmatrix} \frac{1}{\sqrt{T}} & 0 \\ 0 & \frac{1}{T^{3/2}} \end{bmatrix}$$
$$= \frac{1}{D} \begin{bmatrix} \frac{T(T+1)(2T+1)}{6} & -\frac{T(T+1)}{2T^{3/2}} \\ -\frac{T^{3/2}T(T+1)}{2\sqrt{T}} & T^{3/2}T/T^{3/2} \end{bmatrix}$$
$$= \frac{1}{D} \begin{bmatrix} \frac{T(T+1)(2T+1)}{6} & -\frac{T(T+1)}{2T^{3/2}} \\ -\frac{T^{3/2}T(T+1)}{2\sqrt{T}} & T^2 \end{bmatrix}$$

III. Multiplicando por $Y_T$ e $Y_T^{-1}$, os termos da matriz se tornam:
$$
\begin{bmatrix}
\frac{T(T+1)(2T+1)}{6D}T^{1/2}T^{-1/2} & -\frac{T(T+1)}{2D}T^{1/2}T^{-3/2}  \\
-\frac{T(T+1)}{2D}T^{3/2}T^{-1/2} & \frac{T^2}{D} T^{3/2}T^{-3/2}
\end{bmatrix}
=
\begin{bmatrix}
\frac{T(T+1)(2T+1)}{6D}  & -\frac{T(T+1)}{2D}T^{-1}  \\
-\frac{T(T+1)}{2D}T  & \frac{T^2}{D}
\end{bmatrix}
$$

IV. Lembrando que $D =  \frac{T^3(T^2-1)}{12}$ e tomando o limite quando T tende ao infinito, obtemos:
$$ \begin{bmatrix} \frac{2T^3}{6\frac{T^5}{12}} & -\frac{T^2}{2\frac{T^5}{12}}T^{-1}\\ -\frac{T^2}{2\frac{T^5}{12}}T & \frac{T^2}{\frac{T^5}{12}}  \end{bmatrix}
= \begin{bmatrix} \frac{4}{T^2} & -\frac{6}{T^4} \\ -\frac{6}{T^2} & \frac{12}{T^3}  \end{bmatrix}
$$
Tomando o limite, temos
$$\lim_{T\to\infty}Y_T (X'X)^{-1} Y_T^{-1} = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$$
Portanto, $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}Y_T^{-1} \rightarrow Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$. $\blacksquare$
O termo $Y_T\sum_{t=1}^T x_t \epsilon_t$ √© dado por [^30]:
$$Y_T \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t \end{bmatrix} = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix}$$

Este vetor, sob condi√ß√µes padr√£o para $\epsilon_t$, converge para uma distribui√ß√£o normal bivariada [^31]. Especificamente, o primeiro elemento converge para $N(0,\sigma^2)$ [^32], e o segundo elemento, que envolve uma sequ√™ncia de diferen√ßas de martingales, converge para $N(0, \sigma^2/3)$ [^33, ^34, ^35]. Al√©m disso, a covari√¢ncia entre os dois elementos converge para zero, garantindo que s√£o assintoticamente independentes [^37].

**Teorema 1**
O vetor reescalonado $Y_T(b_T - \beta)$ converge em distribui√ß√£o para uma vari√°vel aleat√≥ria normal bivariada com m√©dia zero e matriz de covari√¢ncia $\sigma^2Q$.

*Prova:*
Temos que
$$ Y_T(b_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}Y_T^{-1}Y_T \sum_{t=1}^T x_t \epsilon_t$$
I. A partir da Proposi√ß√£o 1, sabemos que $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}Y_T^{-1} \rightarrow Q$.
II. Do texto anterior, sabemos que $Y_T \sum_{t=1}^T x_t \epsilon_t$ converge em distribui√ß√£o para um vetor normal bivariado com m√©dia zero e matriz de covari√¢ncia $\sigma^2 \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$.
III. Portanto, usando o teorema de Slutsky, conclu√≠mos que:
$$Y_T(b_T - \beta) \overset{d}{\longrightarrow} N(0, \sigma^2Q)$$
‚ñ†
> üí° **Exemplo Num√©rico:** Para concretizar o Teorema 1, vamos gerar uma s√©rie temporal com $T=100$ seguindo o modelo $y_t = 2 + 0.5t + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco com desvio padr√£o 1. Em seguida, vamos simular o processo de estima√ß√£o e calcular os erros reescalonados.
```python
import numpy as np
import pandas as pd
from numpy.linalg import inv

np.random.seed(42) # para reproducibilidade

T = 100
alpha_true = 2
delta_true = 0.5
sigma = 1

t = np.arange(1, T + 1)
x = np.array([[1, ti] for ti in t])
epsilon = np.random.normal(0, sigma, T)
y = alpha_true + delta_true * t + epsilon

XTX = np.dot(x.T, x)
XTX_inv = inv(XTX)
XTY = np.dot(x.T, y)
beta_hat = np.dot(XTX_inv, XTY)

alpha_hat = beta_hat[0]
delta_hat = beta_hat[1]

YT = np.array([[np.sqrt(T), 0], [0, T**(3/2)]])
beta_error = np.array([alpha_hat-alpha_true, delta_hat - delta_true])

YT_beta_error = np.dot(YT, beta_error)
print("Erro reescalonado para alpha:", YT_beta_error[0])
print("Erro reescalonado para delta:", YT_beta_error[1])

```
Isso produz:
```
Erro reescalonado para alpha: -0.6636744905452705
Erro reescalonado para delta: 0.21825757280460827
```
Os valores obtidos representam uma simula√ß√£o de um poss√≠vel resultado do termo reescalonado $Y_T(b_T - \beta)$. O teorema afirma que para $T \to \infty$, esses erros reescalonados convergem para uma distribui√ß√£o normal bivariada, com m√©dia zero. Em uma √∫nica simula√ß√£o, n√£o observamos a converg√™ncia, mas ao repetir este experimento diversas vezes e com valores maiores de T, ver√≠amos as distribui√ß√µes de amostras dos erros se aproximarem de uma normal com m√©dia zero e covari√¢ncia $\sigma^2 Q$.
Em resumo, o reescalonamento com $Y_T$ assegura que as estimativas tenham uma distribui√ß√£o assint√≥tica bem definida.  O termo $\sqrt{T}(\hat{\alpha}_T - \alpha)$ converge para uma distribui√ß√£o normal, indicando que a estimativa de $\alpha$ precisa ser reescalonada pela raiz quadrada de $T$ para se obter um limite n√£o degenerado. Por outro lado, o termo $T^{3/2}(\hat{\delta}_T - \delta)$ tamb√©m converge para uma distribui√ß√£o normal, implicando que a estimativa de $\delta$ precisa ser reescalonada por $T^{3/2}$.

### Conclus√£o
Esta se√ß√£o forneceu uma an√°lise detalhada do processo de reescalonamento necess√°rio para obter distribui√ß√µes assint√≥ticas n√£o degeneradas das estimativas de MQO em modelos com tend√™ncia temporal determin√≠stica. Foi demonstrado como a diverg√™ncia da matriz de covari√¢ncia $\sum_{t=1}^T x_t x_t'$ exige o reescalonamento por meio da matriz $Y_T$, envolvendo diferentes taxas de converg√™ncia para os par√¢metros $\alpha$ e $\delta$. O reescalonamento permite que o vetor de estimativas redimensionadas convirja para uma distribui√ß√£o normal bivariada, onde a matriz $Q$ e o termo envolvendo $\epsilon_t$ desempenham um papel crucial. As somas de pot√™ncias de 't', com termos l√≠deres $T^2/2$ e $T^3/3$, refor√ßam a necessidade de reescalonamento para distribui√ß√µes limitantes n√£o degeneradas. A necessidade de reescalonar os estimadores MQO destaca as diferen√ßas fundamentais entre regress√µes com vari√°veis estacion√°rias e aquelas com tend√™ncias temporais determin√≠sticas.

### Refer√™ncias
[^1]:  *Os coeficientes de modelos de regress√£o envolvendo ra√≠zes unit√°rias ou tend√™ncias temporais determin√≠sticas s√£o tipicamente estimados por m√≠nimos quadrados ordin√°rios.*
[^2]:  *No entanto, as distribui√ß√µes assint√≥ticas das estimativas dos coeficientes n√£o podem ser calculadas da mesma forma que aquelas para modelos de regress√£o envolvendo vari√°veis estacion√°rias.*
[^3]: *This section considers OLS estimation of the parameters of a simple time trend, $y_t = \alpha + \delta t + \epsilon_t$, for $\epsilon_t$ a white noise process. If $\epsilon_t \sim N(0, \sigma^2)$, then the model [16.1.1] satisfies the classical regression assumptions...*
[^4]: *Write [16.1.1] in the form of the standard regression model, $y_t = x_t'\beta + \epsilon_t$*
[^5]: *where $x_t = [1  \quad t]'$*
[^6]: *$\beta = [\alpha  \quad \delta]'$*
[^7]: *Let $b_T$ denote the OLS estimate of $\beta$ based on a sample of size $T$: $b_T = [\alpha_T  \quad \delta_T]' = (\sum x_t x_t')^{-1} \sum x_t y_t$*
[^8]: *Recall from equation [8.2.3] that the deviation of the OLS estimate from the true value can be expressed as $(b_T - \beta) = (\sum x_t x_t')^{-1} \sum x_t \epsilon_t$*
[^9]: *$(b_T - \beta) = (\sum x_t x_t')^{-1} \sum x_t \epsilon_t$*
[^10]: *To find the limiting distribution for a regression with stationary explanatory variables, the approach in Chapter 8 was to multiply [16.1.6] by $\sqrt{T}$, resulting in $\sqrt{T}(b_T - \beta) = [(1/T) \sum x_t x_t']^{-1} [(1/\sqrt{T}) \sum x_t \epsilon_t]$*
[^11]: *The usual assumption was that $(1/T) \sum x_t x_t'$ converged in probability to a nonsingular matrix $Q$ while $(1/\sqrt{T}) \sum x_t \epsilon_t$ converged in distribution to a $N(0, \sigma^2Q)$ random variable, implying that $\sqrt{T}(b_T - \beta) -> N(0, \sigma^2Q^{-1})$.*
[^12]: *...expression [16.1.6] would be $[(\alpha_T - \alpha)  \quad (\delta_T - \delta)]' = [\sum 1  \quad \sum t ; \sum t  \quad \sum t^2]^{-1} [\sum \epsilon_t ; \sum t\epsilon_t]$*
[^13]: *$[(\alpha_T - \alpha)  \quad (\delta_T - \delta)]' = [\sum 1  \quad \sum t ; \sum t  \quad \sum t^2]^{-1} [\sum \epsilon_t ; \sum t\epsilon_t]$*
[^14]: *It is straightforward to show by induction that $\sum t = T(T+1)/2$*
[^15]: *$\sum t^2 = T(T+1)(2T+1)/6.$*
[^16]: *Thus, the leading term in $\sum t$ is $T^2/2$; that is, $(1/T^2) \sum t = 1/2 + 1/(2T) \rightarrow 1/2$*
[^17]: *Similarly, the leading term in $\sum t^2$ is $T^3/3$: $(1/T^3) \sum t^2 = 1/3 + 1/(2T) + 1/(6T^2) \rightarrow 1/3.*
[^18]: *For future reference, we note here the general pattern-the leading term in $\sum t^v$ is $T^{v+1}/(v+1)$: $(1/T^{v+1}) \sum t^v \rightarrow 1/(v+1)$*
[^19]: *For $x$, given in [16.1.3], results [16.1.9] and [16.1.10] imply that $\sum x_t x_t' = [\sum 1  \quad \sum t ; \sum t  \quad \sum t^2] = [T  \quad T(T+1)/2 ;  T(T+1)/2  \quad T(T+1)(2T+1)/6]$*
[^20]: *$\sum x_t x_t' = [\sum 1  \quad \sum t ; \sum t  \quad \sum t^2] = [T  \quad T(T+1)/2 ;  T(T+1)/2  \quad T(T+1)(2T+1)/6]$*
[^21]: *In contrast to the usual result for stationary regressions, for the matrix in [16.1.16], $(1/T) \sum x_t x_t'$ diverges. To obtain a convergent matrix, [16.1.16] would have to be divided by $T^3$ rather than $T$.*
[^22]: *Unfortunately, this limiting matrix cannot be inverted, as $(1/T) \sum x_t x_t'$ can be in the usual case. Hence, a different approach from that in the stationary case will be needed to calculate the asymptotic distribution of $b_T$.*
[^23]: *It turns out that the OLS estimates $\alpha_T$ and $\delta_T$ have different asymptotic rates of convergence. To arrive at nondegenerate limiting distributions, $\alpha_T$ is multiplied by $\sqrt{T}$, whereas $\delta_T$ must be multiplied by $T^{3/2}$.*
[^24]: *We can think of this adjustment as premultiplying [16.1.6] or [16.1.8] by the matrix $Y_T = [\sqrt{T}  \quad 0 ; 0  \quad T^{3/2}]$*
[^25]: *$Y_T = [\sqrt{T}  \quad 0 ; 0  \quad T^{3/2}]$*
[^26]: *resulting in $[\sqrt{T}(\alpha_T - \alpha)  \quad ; T^{3/2}(\delta_T - \delta)]' = Y_T (\sum x_t x_t')^{-1}  Y_T^{-1} [ (1/\sqrt{T}) \sum x_t \epsilon_t]$*
[^27]: *$[\sqrt{T}(\alpha_T - \alpha)  \quad ; T^{3/2}(\delta_T - \delta)]' = Y_T (\sum x_t x_t')^{-1}  Y_T^{-1} [ (1/\sqrt{T}) \sum x_t \epsilon_t]$*
[^28]: *Consider the first term in the last expression of [16.1.18]. Substituting from [16.1.17] and [16.1.16], ${Y_T (\sum x_t x_t')^{-1} Y_T^{-1}} = [T^{-1/2}  \quad 0 ; 0  \quad T^{-3/2}]  [ \sum 1  \quad \sum t ; \sum t  \quad \sum t^2 ]^{-1} [T^{-1/2} \quad 0 ; 0 \quad T^{-3/2}] = [T^{-1/2}  \quad 0 ; 0  \quad T^{-3/2}] [T  \quad T(T+1)/2 ; T(T+1)/2  \quad T(T+1)(2T+1)/6 ]^{-1} [T^{-1/2} \quad 0 ; 0 \quad T^{-3/2}]$*
[^29]: *Thus, it follows from [16.1.11] and [16.1.12] that $\Sigma$ is positive definite*
## 16.2 The Wald Test

The Wald test provides an alternative to the likelihood ratio test for testing hypotheses about the parameters of a model. It's particularly useful when the maximum likelihood estimator (MLE) is known and asymptotically normal but the likelihood function itself is difficult to maximize under constraints.

### 16.2.1 Basic Wald Statistic

Consider testing the null hypothesis $H_0 : \mathbf{r}(\boldsymbol{\theta}) = \mathbf{0}$ against the alternative $H_1 : \mathbf{r}(\boldsymbol{\theta}) \neq \mathbf{0}$, where $\mathbf{r}(\boldsymbol{\theta})$ is a $q \times 1$ vector of continuously differentiable functions of the $p \times 1$ parameter vector $\boldsymbol{\theta}$. Let $\hat{\boldsymbol{\theta}}$ denote the MLE of $\boldsymbol{\theta}$.

The Wald test statistic is defined as:

$$W = \mathbf{r}(\hat{\boldsymbol{\theta}})' \left[ \frac{\partial \mathbf{r}(\hat{\boldsymbol{\theta}})}{\partial \boldsymbol{\theta}'} \hat{\Sigma}_\theta \left( \frac{\partial \mathbf{r}(\hat{\boldsymbol{\theta}})}{\partial \boldsymbol{\theta}'} \right)' \right]^{-1} \mathbf{r}(\hat{\boldsymbol{\theta}})$$

where $\hat{\Sigma}_\theta$ is an estimate of the asymptotic covariance matrix of $\hat{\boldsymbol{\theta}}$, often the inverse of the observed or expected Fisher information matrix. The term $\frac{\partial \mathbf{r}(\hat{\boldsymbol{\theta}})}{\partial \boldsymbol{\theta}'}$ is the $q \times p$ matrix of partial derivatives of $\mathbf{r}(\boldsymbol{\theta})$ with respect to $\boldsymbol{\theta}'$, evaluated at $\hat{\boldsymbol{\theta}}$.

Under $H_0$ and certain regularity conditions, the Wald statistic $W$ is asymptotically distributed as a chi-squared distribution with $q$ degrees of freedom, denoted as $\chi^2(q)$.

### 16.2.2 Constructing a Wald Test

To conduct a Wald test:

1.  **Estimate** the parameters $\boldsymbol{\theta}$ to obtain $\hat{\boldsymbol{\theta}}$ (e.g., using MLE).
2.  **Compute** $\mathbf{r}(\hat{\boldsymbol{\theta}})$, which should be close to $\mathbf{0}$ if the null hypothesis is true.
3.  **Calculate** the gradient matrix $\frac{\partial \mathbf{r}(\hat{\boldsymbol{\theta}})}{\partial \boldsymbol{\theta}'}$.
4.  **Obtain** an estimate $\hat{\Sigma}_\theta$ of the asymptotic covariance matrix of $\hat{\boldsymbol{\theta}}$.
5.  **Compute** the Wald statistic $W$.
6.  **Compare** the calculated $W$ with the critical value from the $\chi^2(q)$ distribution at a chosen significance level $\alpha$.
7.  **Reject** $H_0$ if $W$ exceeds the critical value. Otherwise, fail to reject $H_0$.

### 16.2.3 Example

Suppose we want to test if a specific linear combination of regression coefficients is equal to a particular value. Let our linear model be $y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i$.

The null hypothesis is $H_0: \beta_1 + \beta_2 = 1$. Here, $\mathbf{r}(\boldsymbol{\theta}) = \beta_1 + \beta_2 - 1$, so $q = 1$. The parameter vector is $\boldsymbol{\theta} = [\beta_0, \beta_1, \beta_2]'$.

We estimate the parameters using OLS to get $\hat{\boldsymbol{\theta}} = [\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2]'$.
$\mathbf{r}(\hat{\boldsymbol{\theta}}) = \hat{\beta}_1 + \hat{\beta}_2 - 1$
$\frac{\partial \mathbf{r}(\hat{\boldsymbol{\theta}})}{\partial \boldsymbol{\theta}'} = [0, 1, 1]$
$\hat{\Sigma}_\theta$ is the estimated covariance matrix of the estimated OLS parameters.

Using these values, we calculate $W$ and compare it to a $\chi^2(1)$ distribution.

### 16.2.4 Properties

-   **Asymptotic Validity:** The Wald test is asymptotically valid, meaning its size (Type I error) converges to the nominal level as the sample size grows.
-   **Computational Ease:** It is computationally simple to perform when the MLE is readily available.
-   **Invariance:** The test is invariant under reparameterization of the model.

### 16.2.5 Caveats

-   **Finite Sample Performance:** Wald tests can perform poorly in small samples.
-   **Monotonicity:** The Wald test statistic is not guaranteed to be monotonic in the likelihood.
-   **Choice of Estimator:** The behavior of the Wald test can depend on the specific method used to estimate $\hat{\Sigma}_\theta$. Different estimators for the covariance matrix can lead to significantly different results, particularly in small samples.
-   **Non-uniqueness of $\mathbf{r}(\boldsymbol{\theta})$:** If $\mathbf{r}(\boldsymbol{\theta})$ is not unique, the test outcome can depend on the chosen form.
-   **Alternatives:** In some situations, the likelihood ratio test or score test might be preferred over the Wald test, especially if the likelihood function is readily available.
<!-- END -->
