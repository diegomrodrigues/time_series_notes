## An√°lise das Taxas de Converg√™ncia e Reescalonamento em Modelos de Tend√™ncia Temporal

### Introdu√ß√£o
Este cap√≠tulo aborda a an√°lise das taxas de converg√™ncia dos estimadores de m√≠nimos quadrados ordin√°rios (MQO) em modelos de regress√£o com tend√™ncias temporais determin√≠sticas. Especificamente, focamos em como as diferentes taxas de converg√™ncia dos estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$, e como a diverg√™ncia da matriz de momentos $(1/T)\sum_{t=1}^T x_t x_t'$ impactam o c√°lculo da distribui√ß√£o assint√≥tica e a necessidade de reescalonamento. Expandindo sobre os conceitos de estima√ß√£o por MQO e distribui√ß√µes assint√≥ticas j√° introduzidos [^10, ^11, ^21, ^22, ^23, ^24, ^25], este cap√≠tulo aprofunda a compreens√£o da necessidade de reescalonar as matrizes de momentos para obter resultados estat√≠sticos v√°lidos.

### Taxas de Converg√™ncia Distintas e a Matriz de Momentos
Como vimos, o modelo de tend√™ncia temporal simples √© dado por [^3]:
$$y_t = \alpha + \delta t + \epsilon_t$$
onde $\epsilon_t$ √© um ru√≠do branco com $\epsilon_t \sim N(0, \sigma^2)$. A forma padr√£o de regress√£o √© [^4]:
$$y_t = x_t'\beta + \epsilon_t$$
com $x_t = [1 \quad t]'$ [^5] e $\beta = [\alpha \quad \delta]'$ [^6]. O estimador MQO de $\beta$ √© dado por [^7]:
$$b_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_t$$
e o desvio do estimador √© [^8, ^9]:
$$b_T - \beta = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t$$
Em contraste com regress√µes com vari√°veis estacion√°rias, onde multiplicamos por $\sqrt{T}$ para obter uma distribui√ß√£o limite, em modelos com tend√™ncias temporais, as estimativas $\hat{\alpha}_T$ e $\hat{\delta}_T$ convergem para seus valores verdadeiros em taxas diferentes [^23]. Especificamente, $\hat{\alpha}_T$ converge √† taxa de $\sqrt{T}$, enquanto $\hat{\delta}_T$ converge √† taxa de $T^{3/2}$ [^23]. Para obter distribui√ß√µes limitantes n√£o degeneradas, √© preciso reescalonar as estimativas com essas taxas de converg√™ncia.

A matriz de momentos, dada por [^19, ^20]:
$$\sum_{t=1}^T x_t x_t' = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix} = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} $$
desempenha um papel crucial na an√°lise das propriedades assint√≥ticas. Os termos $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$ s√£o assintoticamente equivalentes a $T^2/2$ e $T^3/3$, respectivamente [^16, ^17]. De forma geral, o termo dominante de $\sum_{t=1}^T t^v$ √© $\frac{T^{v+1}}{v+1}$ [^18]. Como resultado, a matriz de momentos, dividida por $T$, diverge [^21]. Em vez disso, para obter uma matriz que convirja, ela deve ser dividida por $T^3$ [^21]. No entanto, mesmo com essa divis√£o, a matriz resultante n√£o √© invert√≠vel, conforme visto anteriormente no Lema 1. Essa √© a principal motiva√ß√£o para o reescalonamento com a matriz $Y_T$.

> üí° **Exemplo Num√©rico:** Para ilustrar o comportamento da matriz de momentos, vamos usar um exemplo com T=100 e analisar como a matriz $(1/T)\sum_{t=1}^T x_t x_t'$ diverge e como a matriz $(1/T^3)\sum_{t=1}^T x_t x_t'$ tamb√©m n√£o converge para uma matriz n√£o-singular.
```python
import numpy as np

T = 100
t = np.arange(1, T + 1)
X = np.vstack([np.ones(T), t]).T
XTX = X.T @ X

XTX_div_T = XTX / T
print("Matriz X'X dividida por T:\n", XTX_div_T)

XTX_div_T3 = XTX / T**3
print("\nMatriz X'X dividida por T^3:\n", XTX_div_T3)
```
O resultado mostra que a matriz dividida por $T$ diverge, enquanto a matriz dividida por $T^3$ converge para uma matriz singular, refor√ßando a necessidade de usar o reescalonamento adequado com a matriz $Y_T$ para obter uma matriz limite n√£o-singular.

### Reescalonamento com a Matriz Y_T e sua Necessidade
O reescalonamento das estimativas √© formalizado pela matriz $Y_T$ [^24, ^25]:
$$Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}$$
Esta matriz √© usada para reescalonar o desvio do estimador da seguinte forma [^26, ^27]:
$$\begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} = Y_T (b_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t$$
Ao multiplicar o desvio por $Y_T$, estamos essencialmente aplicando as taxas de converg√™ncia corretas para cada par√¢metro. O termo $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1}$ converge para uma matriz $Q$ n√£o singular [^28, ^29]:
$$Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$$
Essa converg√™ncia √© crucial para a deriva√ß√£o da distribui√ß√£o assint√≥tica.

A import√¢ncia da matriz $Y_T$ reside em sua capacidade de transformar as estimativas originais em vari√°veis que t√™m uma distribui√ß√£o limitante n√£o degenerada. Como vimos no cap√≠tulo anterior, isso √© obtido multiplicando os estimadores pelas suas taxas de converg√™ncia correspondentes [^23].

> üí° **Exemplo Num√©rico:** Para exemplificar a atua√ß√£o da matriz $Y_T$, vamos considerar um cen√°rio em que $T = 100$ e calcular $Y_T (X'X)^{-1} Y_T^{-1}$ para demonstrar sua converg√™ncia √† matriz $Q$.
```python
import numpy as np
from numpy.linalg import inv

T = 100
t = np.arange(1, T + 1)
X = np.vstack([np.ones(T), t]).T
Y_T = np.array([[np.sqrt(T), 0], [0, T**(3/2)]])
XTX = X.T @ X
XTX_inv = inv(XTX)
Q_hat = Y_T @ XTX_inv @ Y_T.T
Q = np.array([[1, 1/2], [1/2, 1/3]])

print("Matriz Y_T (X'X)^-1 Y_T':")
print(Q_hat)
print("\nMatriz Q:")
print(Q)
```
O exemplo mostra que a matriz $Y_T (X'X)^{-1} Y_T^{-1}$ converge para a matriz $Q$ conforme $T$ aumenta, demonstrando o efeito do reescalonamento para obter uma matriz n√£o-singular.

### O Desvio do Estimador e a Necessidade de Reescalonamento
O desvio do estimador MQO, $b_T - \beta$, √© fun√ß√£o da matriz de momentos e dos erros. Especificamente:
$$b_T - \beta = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t$$
Como discutido, a matriz de momentos $\sum_{t=1}^T x_t x_t'$ tem elementos que crescem em taxas diferentes com o aumento de $T$. Isso implica que a matriz inversa $\left( \sum_{t=1}^T x_t x_t' \right)^{-1}$ ter√° elementos que decrescem em taxas diferentes, afetando as taxas de converg√™ncia de $\hat{\alpha}_T$ e $\hat{\delta}_T$.
O termo $\sum_{t=1}^T x_t \epsilon_t$ tamb√©m possui taxas de converg√™ncia distintas para $\sum_{t=1}^T \epsilon_t$ e $\sum_{t=1}^T t \epsilon_t$, com os termos dominantes na ordem de $\sqrt{T}$ e $T^{3/2}$ respectivamente.
Para compensar essas diferentes taxas de converg√™ncia, √© necess√°rio reescalonar tanto a matriz de momentos quanto os erros. O reescalonamento com $Y_T$ assegura que os estimadores reescalonados convirjam para uma distribui√ß√£o assint√≥tica bem definida.
O termo $Y_T \sum_{t=1}^T x_t \epsilon_t$ √© dado por [^30]:
$$Y_T \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} \sqrt{T} \frac{1}{T} \sum_{t=1}^T \epsilon_t \\ T^{3/2} \frac{1}{T^2} \sum_{t=1}^T t \epsilon_t \end{bmatrix} = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix}$$
Este vetor converge para uma distribui√ß√£o normal bivariada, com o primeiro componente convergindo para $N(0, \sigma^2)$ e o segundo para $N(0, \sigma^2/3)$ [^31, ^32, ^33, ^34, ^35].

> üí° **Exemplo Num√©rico:** Para ilustrar a converg√™ncia de  $Y_T \sum_{t=1}^T x_t \epsilon_t$, vamos gerar um conjunto de dados com $T=100$, com $\epsilon_t \sim N(0, 1)$, e calcular as estat√≠sticas do vetor  $Y_T \sum_{t=1}^T x_t \epsilon_t$.  Vamos repetir o processo 1000 vezes para verificar a distribui√ß√£o assint√≥tica.
```python
import numpy as np

T = 100
num_simulations = 1000
results = np.zeros((num_simulations, 2))

for i in range(num_simulations):
    epsilon_t = np.random.normal(0, 1, T)
    t = np.arange(1, T + 1)
    x_t = np.vstack([np.ones(T), t]).T
    
    Y_T_sum_x_eps = np.array([
        (1/np.sqrt(T)) * np.sum(epsilon_t),
        (1/np.sqrt(T)) * np.sum((t/T) * epsilon_t)
    ])
    results[i] = Y_T_sum_x_eps
    
mean_vector = np.mean(results, axis=0)
cov_matrix = np.cov(results, rowvar=False)

print("Vetor de M√©dias da amostra:", mean_vector)
print("\nMatriz de Covari√¢ncia da amostra:\n", cov_matrix)
print("\nVari√¢ncia te√≥rica para o primeiro elemento:", 1)
print("Vari√¢ncia te√≥rica para o segundo elemento:", 1/3)
```
Este exemplo mostra que as m√©dias amostrais dos elementos de $Y_T \sum_{t=1}^T x_t \epsilon_t$ s√£o pr√≥ximas de zero e que a matriz de covari√¢ncia se aproxima de $\begin{bmatrix} 1 & 0 \\ 0 & 1/3 \end{bmatrix}$, confirmando a converg√™ncia para a distribui√ß√£o normal bivariada com vari√¢ncias $\sigma^2$ e $\sigma^2/3$, respectivamente.

**Lema 3.1**
*Seja $Z_T =  \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t$ e $W_T =  \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t$. Se  $\epsilon_t$ √© um ru√≠do branco com m√©dia zero, vari√¢ncia $\sigma^2$ e quarto momento finito, ent√£o $Z_T \xrightarrow{d} N(0, \sigma^2)$ e $W_T \xrightarrow{d} N(0, \sigma^2/3)$.*

*Prova:*

I. A prova de $Z_T \xrightarrow{d} N(0, \sigma^2)$ segue diretamente do Teorema do Limite Central, j√° que $\epsilon_t$ √© i.i.d. com m√©dia zero e vari√¢ncia $\sigma^2$.
II. A prova de $W_T \xrightarrow{d} N(0, \sigma^2/3)$ foi demonstrada nas refer√™ncias [^33, ^34, ^35], que detalham a converg√™ncia para uma distribui√ß√£o normal usando argumentos de martingale difference sequence e converg√™ncia quadr√°tica m√©dia.
Portanto, o lema est√° provado. ‚ñ†

**Teorema 3.1**
*Seja $\hat{\beta}_T = [\hat{\alpha}_T, \hat{\delta}_T]' $ o estimador de MQO do modelo $y_t = \alpha + \delta t + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco com m√©dia zero, vari√¢ncia $\sigma^2$ e quarto momento finito. A distribui√ß√£o assint√≥tica do vetor reescalonado √© dada por
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q^{-1}) $$
onde $Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$ e $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$*

*Prova:*
I. O resultado segue diretamente da discuss√£o anterior. O desvio do estimador MQO √© dado por:
   $$b_T - \beta = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t$$
II. Para obter a distribui√ß√£o assint√≥tica, multiplicamos por $Y_T$:
$$Y_T(b_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}  \sum_{t=1}^T x_t \epsilon_t = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1} Y_T \sum_{t=1}^T x_t \epsilon_t$$
III. Mostramos anteriormente que:
$$Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1} \rightarrow Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$$
IV. Al√©m disso, demonstramos no Lema 3.1 que
   $$ Y_T \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q)$$
V. Pelo teorema de Slutsky:
$$Y_T(b_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1})$$
Portanto, o resultado √© comprovado. ‚ñ†

**Corol√°rio 3.1** *Sob as mesmas condi√ß√µes do Teorema 3.1, as distribui√ß√µes assint√≥ticas dos estimadores individuais reescalonados s√£o dadas por:
   $$\sqrt{T}(\hat{\alpha}_T - \alpha) \xrightarrow{d} N(0, 4\sigma^2)$$
   $$T^{3/2}(\hat{\delta}_T - \delta) \xrightarrow{d} N(0, 12\sigma^2)$$*

*Prova:*
I. O resultado segue diretamente da matriz de covari√¢ncia obtida no Teorema 3.1. A matriz $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$ implica que a vari√¢ncia assint√≥tica de $\sqrt{T}(\hat{\alpha}_T - \alpha)$ √© $4\sigma^2$, e a vari√¢ncia assint√≥tica de $T^{3/2}(\hat{\delta}_T - \delta)$ √© $12\sigma^2$.
Portanto, o corol√°rio est√° provado. ‚ñ†
> üí° **Exemplo Num√©rico:** Para verificar o Corol√°rio 3.1, vamos simular um modelo de tend√™ncia temporal com $\alpha = 2$, $\delta = 0.5$, $\sigma^2=1$ e $T = 100$. Vamos repetir o processo 1000 vezes, estimar os par√¢metros e calcular a distribui√ß√£o emp√≠rica dos estimadores reescalonados.
```python
import numpy as np
import pandas as pd
from numpy.linalg import inv

T = 100
alpha = 2
delta = 0.5
sigma2 = 1
num_simulations = 1000
results_alpha = np.zeros(num_simulations)
results_delta = np.zeros(num_simulations)

for i in range(num_simulations):
    epsilon_t = np.random.normal(0, np.sqrt(sigma2), T)
    t = np.arange(1, T + 1)
    y_t = alpha + delta * t + epsilon_t
    X = np.vstack([np.ones(T), t]).T
    beta_hat = inv(X.T @ X) @ X.T @ y_t
    results_alpha[i] = np.sqrt(T) * (beta_hat[0] - alpha)
    results_delta[i] = T**(3/2) * (beta_hat[1] - delta)

mean_alpha = np.mean(results_alpha)
var_alpha = np.var(results_alpha)
mean_delta = np.mean(results_delta)
var_delta = np.var(results_delta)

print("M√©dia amostral de sqrt(T)(alpha_hat - alpha):", mean_alpha)
print("Vari√¢ncia amostral de sqrt(T)(alpha_hat - alpha):", var_alpha)
print("M√©dia amostral de T^(3/2)(delta_hat - delta):", mean_delta)
print("Vari√¢ncia amostral de T^(3/2)(delta_hat - delta):", var_delta)
print("Vari√¢ncia te√≥rica de sqrt(T)(alpha_hat - alpha):", 4 * sigma2)
print("Vari√¢ncia te√≥rica de T^(3/2)(delta_hat - delta):", 12 * sigma2)

```
O exemplo mostra que as vari√¢ncias amostrais dos estimadores reescalonados se aproximam dos valores te√≥ricos, confirmando o Corol√°rio 3.1.

Este teorema demonstra formalmente como o reescalonamento garante que as estimativas dos par√¢metros possuam uma distribui√ß√£o assint√≥tica bem definida, permitindo realizar testes de hip√≥teses e construir intervalos de confian√ßa v√°lidos. O Corol√°rio 3.1  especifica as distribui√ß√µes assint√≥ticas para os estimadores individuais, que s√£o √∫teis para construir intervalos de confian√ßa para cada par√¢metro separadamente.

### Conclus√£o
Este cap√≠tulo detalhou a necessidade do reescalonamento em modelos de regress√£o com tend√™ncias temporais determin√≠sticas. Demonstramos que as diferentes taxas de converg√™ncia de $\hat{\alpha}_T$ e $\hat{\delta}_T$, juntamente com a diverg√™ncia da matriz de momentos $(1/T)\sum_{t=1}^T x_t x_t'$, exigem um tratamento especial. O reescalonamento com a matriz $Y_T$ permite obter uma distribui√ß√£o assint√≥tica n√£o degenerada para os estimadores MQO, corrigindo o problema da diverg√™ncia. Analisamos como as matrizes de momentos divergem e como o reescalonamento por $Y_T$ assegura que a matriz resultante convirja para uma matriz n√£o singular. Al√©m disso, apresentamos o Teorema 3.1, que demonstra formalmente como os estimadores reescalonados convergem para uma distribui√ß√£o normal multivariada, permitindo realizar infer√™ncia assint√≥tica.

### Refer√™ncias
[^1]:  *Os coeficientes de modelos de regress√£o envolvendo ra√≠zes unit√°rias ou tend√™ncias temporais determin√≠sticas s√£o tipicamente estimados por m√≠nimos quadrados ordin√°rios.*
[^2]:  *No entanto, as distribui√ß√µes assint√≥ticas das estimativas dos coeficientes n√£o podem ser calculadas da mesma forma que aquelas para modelos de regress√£o envolvendo vari√°veis estacion√°rias.*
[^3]: *This section considers OLS estimation of the parameters of a simple time trend, $y_t = \alpha + \delta t + \epsilon_t$, for $\epsilon_t$ a white noise process. If $\epsilon_t \sim N(0, \sigma^2)$, then the model [16.1.1] satisfies the classical regression assumptions...*
[^4]: *Write [16.1.1] in the form of the standard regression model, $y_t = x_t'\beta + \epsilon_t$*
[^5]: *where $x_t = [1  \quad t]'$*
[^6]: *$\beta = [\alpha  \quad \delta]'$*
[^7]: *Let $b_T$ denote the OLS estimate of $\beta$ based on a sample of size $T$: $b_T = [\alpha_T  \quad \delta_T]' = (\sum x_t x_t')^{-1} \sum x_t y_t$*
[^8]: *Recall from equation [8.2.3] that the deviation of the OLS estimate from the true value can be expressed as $(b_T - \beta) = (\sum x_t x_t')^{-1} \sum x_t \epsilon_t$*
[^9]: *$(b_T - \beta) = (\sum x_t x_t')^{-1} \sum x_t \epsilon_t$*
[^10]: *To find the limiting distribution for a regression with stationary explanatory variables, the approach in Chapter 8 was to multiply [16.1.6] by $\sqrt{T}$, resulting in $\sqrt{T}(b_T - \beta) = [(1/T) \sum x_t x_t']^{-1} [(1/\sqrt{T}) \sum x_t \epsilon_t]$*
[^11]: *The usual assumption was that $(1/T) \sum x_t x_t'$ converged in probability to a nonsingular matrix $Q$ while $(1/\sqrt{T}) \sum x_t \epsilon_t$ converged in distribution to a $N(0, \sigma^2Q)$ random variable, implying that $\sqrt{T}(b_T - \beta) -> N(0, \sigma^2Q^{-1})$.*
[^12]: *...expression [16.1.6] would be $[(\alpha_T - \alpha)  \quad (\delta_T - \delta)]' = [\sum 1  \quad \sum t ; \sum t  \quad \sum t^2]^{-1} [\sum \epsilon_t ; \sum t\epsilon_t]$*
[^13]: *$[(\alpha_T - \alpha)  \quad (\delta_T - \delta)]' = [\sum 1  \quad \sum t ; \sum t  \quad \sum t^2]^{-1} [\sum \epsilon_t ; \sum t\epsilon_t]$*
[^14]: *It is straightforward to show by induction that $\sum t = T(T+1)/2$*
[^15]: *$\sum t^2 = T(T+1)(2T+1)/6.$*
[^16]: *Thus, the leading term in $\sum t$ is $T^2/2$; that is, $(1/T^2) \sum t = 1/2 + 1/(2T) \rightarrow 1/2$*
[^17]: *Similarly, the leading term in $\sum t^2$ is $T^3/3$: $(1/T^3) \sum t^2 = 1/3 + 1/(2T) + 1/(6T^2) \rightarrow 1/3.*
[^18]: *For future reference, we note here the general pattern-the leading term in $\sum t^v$ is $T^{v+1}/(v+1)$: $(1/T^{v+1}) \sum t^v \rightarrow 1/(v+1)$*
[^19]: *For $x$, given in [16.1.3], results [16.1.9] and [16.1.10] imply that $\sum x_t x_t' = [\sum 1  \quad \sum t ; \sum t  \quad \sum t^2] = [T  \quad T(T+1)/2 ;  T(T+1)/2  \quad T(T+1)(2T+1)/6]$*
[^20]: *$\sum x_t x_t' = [\sum 1  \quad \sum t ; \sum t  \quad \sum t^2] = [T  \quad T(T+1)/2 ;  T(T+1)/2  \quad T(T+1)(2T+1)/6]$*
[^21]: *In contrast to the usual result for stationary regressions, for the matrix in [16.1.16], $(1/T) \sum x_t x_t'$ diverges. To obtain a convergent matrix, [16.1.16] would have to be divided by $T^3$ rather than $T$.*
[^22]: *Unfortunately, this limiting matrix cannot be inverted, as $(1/T) \sum x_t x_t'$ can be in the usual case. Hence, a different approach from that in the stationary case will be needed to calculate the asymptotic distribution of $b_T$.*
[^23]: *It turns out that the OLS estimates $\alpha_T$ and $\delta_T$ have different asymptotic rates of convergence. To arrive at nondegenerate limiting distributions, $\alpha_T$ is multiplied by $\sqrt{T}$, whereas $\delta_T$ must be multiplied by $T^{3/2}$.*
[^24]: *We can think of this adjustment as premultiplying [16.1.6] or [16.1.8] by the matrix $Y_T = [\sqrt{T}  \quad 0 ; 0  \quad T^{3/2}]$*
[^25]: *$Y_T = [\sqrt{T}  \quad 0 ; 0  \quad T^{3/2}]$*
[^26]: *resulting in $[\sqrt{T}(\alpha_T - \alpha)  \quad ; T^{3/2}(\delta_T - \delta)]' = Y_T (\sum x_t x_t')^{-1}  Y_T^{-1} [ (1/\sqrt{T}) \sum x_t \epsilon_t]$*
[^27]: *$[\sqrt{T}(\alpha_T - \alpha)  \quad ; T^{3/2}(\delta_T - \delta)]' = Y_T (\sum x_t x_t')^{-1}  Y_T^{-1} [ (1/\sqrt{T}) \sum x_t \epsilon_t]$*
[^28]: *Consider the first term in the last expression of [16.1.18]. Substituting from [16.1.17] and [16.1.16], ${Y_T (\sum x_t x_t')^{-1} Y_T^{-1}} = [T^{-1/2}  \quad 0 ; 0  \quad T^{-3/2}]  [ \sum 1  \quad \sum t ; \sum t  \quad \sum t^2 ]^{-1} [T^{-1/2} \quad 0 ; 0 \quad T^{-3/2}] = [T^{-1/2}  \quad 0 ; 0  \quad T^{-3/2}] [T  \quad T(T+1)/2 ; T(T+1)/2  \quad T(T+1)(2T+1)/6 ]^{-1} [T^{-1/2} \quad 0 ; 0 \quad T^{-3/2}]$*
[^29]: *Thus, it follows from [16.1.11] and [16.1.12] that  {Y_T (Œ£ x_t x_t')^-1 Y_T^-1}  -> Q = [1  1/2 ; 1/2  1/3]*
[^30]: *Turning next to the second term in [16.1.18], $Y_T \sum x_t \epsilon_t = [ T^{-1/2} 0 ; 0 T^{-3/2} ] [Œ£ \epsilon_t  ; Œ£ t \epsilon_t] = [ (1/\sqrt{T}) Œ£ \epsilon_t ; (1/‚àöT) Œ£ (t/T) \epsilon_t]$*
[^31]: *$Y_T \sum x_t \epsilon_t = [ (1/‚àöT) \sum \epsilon_t ; (1/‚àöT) \sum (t/T) \epsilon_t]. Under standard assumptions about \epsilon_t , this vector will be asymptotically Gaussian.*
[^32]: *For example, suppose that $\epsilon_t$ is i.i.d. with mean zero, variance $\sigma¬≤$, and finite fourth moment. Then the first element of the vector in [16.1.21] satisfies $(1/‚àöT) \sum \epsilon_t -> N(0, \sigma¬≤)$, by the central limit theorem.*
[^33]: *For the second element of the vector in [16.1.21], observe that $\{(t/T)\epsilon_t\}$ is a martingale difference sequence that satisfies the conditions of Proposition 7.8. Specifically, its variance is $\sigma_T^2 = E[(t/T)\epsilon_t]¬≤ = \sigma¬≤(t¬≤/T¬≤)$*
[^34]: *$\sigma_T^2 = E[(t/T)\epsilon_t]¬≤ = \sigma¬≤(t¬≤/T¬≤)$*
[^35]: *(1/T) Œ£ \sigma_t^2 = \sigma¬≤(1/T) Œ£(t¬≤/T¬≤) -> \sigma¬≤/3*
Furthermore, (1/T) $\sum_{t=1}^T [(t/T)\epsilon_t]^2 \rightarrow \sigma^2/3$. To verify this last claim, notice that

$E((1/T) \sum_{t=1}^T [(t/T)\epsilon_t]^2 - (1/T) \sum_{t=1}^T \sigma_t^2)^2 = E((1/T) \sum_{t=1}^T (t/T)^2\epsilon_t^2  - (1/T) \sum_{t=1}^T  (t/T)^2\sigma^2)^2 =  E((1/T) \sum_{t=1}^T (t/T)^2(\epsilon_t^2 - \sigma^2))^2 = (1/T)^2 \sum_{t=1}^T (t/T)^2 E(\epsilon_t^2 - \sigma^2)^2 $ [^22]

But from [16.1.13], $T$ times the magnitude in [16.1.22] converges to
$(1/T) \sum_{t=1}^T (t/T)E(\epsilon_t^2 - \sigma^2)^2 \rightarrow (1/5) \cdot E(\epsilon_t^2 - \sigma^2)^2$, meaning that [16.1.22] itself converges to zero: $(1/T) \sum_{t=1}^T [(t/T)\epsilon_t]^2 - (1/T)\sum_{t=1}^T \sigma_t^2 \xrightarrow{m.s.} 0$.

But this implies that $(1/T) \sum_{t=1}^T [(t/T)\epsilon_t]^2 \xrightarrow{p} \sigma^2/3$, as claimed. Hence, from Proposition 7.8, $(1/\sqrt{T}) \sum_{t=1}^T (t/T)\epsilon_t$ satisfies the central limit theorem:

$(1/\sqrt{T}) \sum_{t=1}^T (t/T)\epsilon_t \xrightarrow{d} N(0, \sigma^2/3)$.

Finally, consider the joint distribution of the two elements in the $(2 \times 1)$ vector described by [16.1.21]. Any linear combination of these elements takes the form

$(1/\sqrt{T}) \sum_{t=1}^T [\lambda_1 + \lambda_2(t/T)]\epsilon_t$.

Then $[\lambda_1 + \lambda_2(t/T)]\epsilon_t$ is also a martingale difference sequence with positive variance given by $\sigma^2[\lambda_1^2 + 2\lambda_1\lambda_2(t/T) + \lambda_2^2(t/T)^2]$ satisfying

$(1/T) \sum_{t=1}^T \sigma^2[\lambda_1^2 + 2\lambda_1\lambda_2(t/T) + \lambda__2^2(t/T)^2] = \sigma^2[\lambda_1^2 + \lambda_1\lambda_2 + (1/3)\lambda_2^2]$.

In the context of the AR(1) model, the estimated variance is given by

$\hat{\sigma}^2 = \frac{1}{T-1} \sum_{t=2}^{T} (\epsilon_t - \hat{\rho} \epsilon_{t-1})^2$,

where $\hat{\rho}$ is the sample autocorrelation coefficient.  In the case of heteroskedasticity, the OLS estimator remains unbiased, but it is no longer the Best Linear Unbiased Estimator (BLUE). This means that although the estimator‚Äôs expected value equals the true parameter, there exists another linear unbiased estimator with a lower variance. We can obtain a more efficient estimator in the presence of heteroskedasticity by using Generalized Least Squares (GLS). GLS takes into account the heteroskedasticity structure and weights the data accordingly to provide minimum variance estimates.

The White test is a statistical test that can be used to detect heteroskedasticity in regression models. The test involves regressing the squared residuals from the original model on the original regressors, their squares, and cross-products. If this auxiliary regression is statistically significant, this indicates the presence of heteroskedasticity. The null hypothesis for the White test is that the error terms are homoskedastic. If the p-value associated with the test statistic is below a chosen significance level (e.g. 0.05), the null hypothesis is rejected, and we have evidence of heteroskedasticity.

Another method for detecting heteroskedasticity is the Breusch-Pagan test. This test regresses the squared residuals on the original regressors, and a significant relationship is an indication of heteroskedasticity.

The solutions to handle heteroskedasticity include:

1.  **Using Heteroskedasticity-Robust Standard Errors:** These are standard errors that are adjusted for heteroskedasticity, giving valid inference even when the standard assumption of homoskedasticity is violated.
2.  **Generalized Least Squares (GLS):** As mentioned before, GLS uses knowledge of the heteroskedasticity structure to transform the data, leading to more efficient parameter estimates than OLS.
3.  **Transforming the Data:** This involves taking a transformation (such as logarithms or square roots) on the dependent variable which can stabilize the variance across different values of the independent variables.

These solutions ensure that accurate inference can be made in regression models that present heteroskedastic errors. It is crucial to examine model residuals for the presence of heteroskedasticity and take appropriate corrective actions, in order to obtain reliable statistical inference.

<!-- END -->
