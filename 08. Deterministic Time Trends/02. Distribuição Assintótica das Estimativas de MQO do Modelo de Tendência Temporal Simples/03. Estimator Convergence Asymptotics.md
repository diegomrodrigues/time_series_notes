## An√°lise Detalhada do Reescalonamento e Distribui√ß√£o Assint√≥tica em Modelos de Tend√™ncia Temporal

### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise da distribui√ß√£o assint√≥tica das estimativas de m√≠nimos quadrados ordin√°rios (MQO) em modelos com tend√™ncias temporais determin√≠sticas. O foco principal √© a necessidade de reescalonamento para obter distribui√ß√µes limitantes n√£o degeneradas e como a matriz $Y_T$ √© utilizada para ajustar as diferentes taxas de converg√™ncia dos par√¢metros [^23, ^24, ^25]. Expandindo sobre os conceitos de estima√ß√£o por MQO e distribui√ß√µes assint√≥ticas abordados nos cap√≠tulos anteriores, este cap√≠tulo busca fornecer uma compreens√£o profunda dos m√©todos aplic√°veis a processos n√£o estacion√°rios com tend√™ncias temporais determin√≠sticas. Em particular, estabelecemos que a multiplica√ß√£o da diferen√ßa entre o estimador MQO e o valor verdadeiro do par√¢metro por $\sqrt{T}$ resulta em uma vari√°vel aleat√≥ria que converge em distribui√ß√£o para uma normal multivariada.

### Conceitos Fundamentais e a Necessidade de Reescalonamento
Como visto anteriormente, o modelo de tend√™ncia temporal simples √© dado por [^3]:
$$y_t = \alpha + \delta t + \epsilon_t$$
onde $\epsilon_t$ √© um ru√≠do branco com $\epsilon_t \sim N(0, \sigma^2)$. A forma padr√£o de regress√£o √© dada por [^4]:
$$y_t = x_t'\beta + \epsilon_t$$
onde $x_t = [1 \quad t]'$ [^5] e $\beta = [\alpha \quad \delta]'$ [^6]. A estimativa de MQO de $\beta$, denotada por $b_T$, √© expressa como [^7]:
$$b_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_t$$
O desvio do estimador MQO do valor verdadeiro √© expresso como [^8, ^9]:
$$b_T - \beta = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t$$
Para regress√µes com vari√°veis estacion√°rias, a distribui√ß√£o limitante √© obtida multiplicando-se o desvio do estimador por $\sqrt{T}$ [^10]:
$$ \sqrt{T}(b_T - \beta) = \left( \frac{1}{T} \sum_{t=1}^T x_t x_t' \right)^{-1} \left( \frac{1}{\sqrt{T}} \sum_{t=1}^T x_t \epsilon_t \right) $$
Entretanto, em modelos com tend√™ncia temporal determin√≠stica, a matriz $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ diverge, n√£o convergindo para uma matriz n√£o singular [^21, ^22]. Isso ocorre porque as somas $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$ s√£o de ordens $T^2/2$ e $T^3/3$, respectivamente [^16, ^17]. Em geral, o termo dominante em $\sum_{t=1}^T t^v$ √© $T^{v+1}/(v+1)$ [^18].

Para lidar com essas taxas de converg√™ncia diferentes, reescalonamos os estimadores usando a matriz $Y_T$ [^24, ^25]:
$$Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}$$
Essa matriz ajusta as estimativas de $\alpha$ e $\delta$ de forma que ambos os par√¢metros tenham uma distribui√ß√£o limite n√£o degenerada, multiplicando $\hat{\alpha}_T$ por $\sqrt{T}$ e $\hat{\delta}_T$ por $T^{3/2}$ [^23]. Ao fazer isso, o desvio do estimador √© transformado em:
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} = Y_T (b_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1}  Y_T \sum_{t=1}^T x_t \epsilon_t$$

### Distribui√ß√£o Assint√≥tica com Reescalonamento
A chave para obter distribui√ß√µes assint√≥ticas n√£o degeneradas √© o uso da matriz $Y_T$. A aplica√ß√£o de $Y_T$ sobre a matriz de covari√¢ncia $\sum_{t=1}^T x_t x_t'$ leva a um termo que converge para uma matriz n√£o singular $Q$ [^28, ^29]:
$$Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1} \rightarrow Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$$
Este resultado √© crucial porque mostra que, ao reescalonar a matriz de covari√¢ncia com $Y_T$, a diverg√™ncia √© resolvida, permitindo que a an√°lise assint√≥tica prossiga.
> üí° **Exemplo Num√©rico:** Para ilustrar o efeito do reescalonamento, vamos usar um exemplo concreto com T=100 e analisar o comportamento do termo $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1}$. Este termo converge para a matriz $Q$. Podemos calcular esse termo utilizando Python:
```python
import numpy as np
from numpy.linalg import inv
T = 100
t = np.arange(1, T+1)
X = np.vstack([np.ones(T), t]).T
Y_T = np.array([[np.sqrt(T), 0], [0, T**(3/2)]])
XTX = X.T @ X
XTX_inv = inv(XTX)
Q_hat = Y_T @ XTX_inv @ Y_T.T
Q = np.array([[1, 1/2], [1/2, 1/3]])

print("Matriz Y_T (X'X)^-1 Y_T':")
print(Q_hat)
print("\nMatriz Q:")
print(Q)
```
O exemplo num√©rico demonstra como $Q_{hat}$ se aproxima da matriz limite $Q$ conforme $T$ aumenta, confirmando que a matriz $Y_T$ corrige a diverg√™ncia do termo $\left( \sum_{t=1}^T x_t x_t' \right)^{-1}$.
O segundo termo da express√£o reescalonada √©:
$$Y_T \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t \end{bmatrix} = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix}$$
O primeiro elemento, $(1/\sqrt{T})\sum_{t=1}^T \epsilon_t$, converge em distribui√ß√£o para $N(0, \sigma^2)$ pelo Teorema do Limite Central (TLC). O segundo elemento, $(1/\sqrt{T})\sum_{t=1}^T (t/T)\epsilon_t$, converge para $N(0, \sigma^2/3)$ devido a ser uma sequ√™ncia de diferen√ßas de martingales [^33, ^34, ^35]. A demonstra√ß√£o deste resultado foi feita no cap√≠tulo anterior. Al√©m disso, a covari√¢ncia entre os dois elementos converge para zero, indicando que s√£o assintoticamente independentes [^37].
> üí° **Exemplo Num√©rico:** Para visualizar a converg√™ncia do segundo termo, vamos gerar uma s√©rie de erros aleat√≥rios (ru√≠do branco) e calcular o segundo elemento do vetor acima para diferentes valores de T, e verificar como a vari√¢ncia se aproxima de  $\sigma^2/3$. Usaremos  $\sigma^2 = 1$
```python
import numpy as np

np.random.seed(42) # para reproducibilidade
num_simulations = 100
variances = []
Ts = [100, 500, 1000, 5000]
sigma_squared = 1

for T in Ts:
    second_terms = []
    for _ in range(num_simulations):
        epsilon = np.random.normal(0, np.sqrt(sigma_squared), T)
        t = np.arange(1, T + 1)
        second_term = (1/np.sqrt(T)) * np.sum((t/T) * epsilon)
        second_terms.append(second_term)
    variances.append(np.var(second_terms))

print("Vari√¢ncia do segundo termo para diferentes T:")
for T, var in zip(Ts, variances):
    print(f"T = {T}: Vari√¢ncia = {var:.4f}")
print(f"\nValor te√≥rico da vari√¢ncia: {sigma_squared/3:.4f}")
```
Este exemplo mostra que √† medida que $T$ aumenta, a vari√¢ncia emp√≠rica do segundo termo se aproxima do valor te√≥rico $\sigma^2 / 3$, ilustrando a converg√™ncia em distribui√ß√£o.

**Teorema 2.1**
*Seja $\hat{\beta}_T = [\hat{\alpha}_T, \hat{\delta}_T]' $ o estimador de MQO do modelo $y_t = \alpha + \delta t + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco com m√©dia zero, vari√¢ncia $\sigma^2$ e quarto momento finito. Ent√£o,
$$
\begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q^{-1})
$$
onde $Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$ e $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$*
*Prova:*
I. Pela an√°lise feita anteriormente, sabemos que:
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} = Y_T(b_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}  \sum_{t=1}^T x_t \epsilon_t $$
II. Tamb√©m mostramos que $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1} \rightarrow Q$, onde $Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$.
III. Al√©m disso, demonstramos que:
$$ Y_T \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q)$$
IV. Aplicando o teorema de Slutsky, conclu√≠mos que:
$$Y_T(b_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1})$$
Onde $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$.
Isso demonstra que a multiplica√ß√£o do desvio da estimativa do par√¢metro por $\sqrt{T}$ e $T^{3/2}$ gera uma vari√°vel aleat√≥ria que converge em distribui√ß√£o para uma normal multivariada com uma matriz de covari√¢ncia bem definida.‚ñ†

O teorema 2.1 estabelece que o vetor reescalonado dos erros de estimativa, $Y_T (b_T - \beta)$, converge em distribui√ß√£o para uma normal multivariada com m√©dia zero e matriz de covari√¢ncia $\sigma^2 Q^{-1}$. Isso permite realizar infer√™ncia assint√≥tica sobre os par√¢metros do modelo com tend√™ncia temporal determin√≠stica. Em particular, esse resultado explica porque o reescalonamento por $\sqrt{T}$ e $T^{3/2}$ para $\hat{\alpha}_T$ e $\hat{\delta}_T$ resulta em uma distribui√ß√£o assint√≥tica n√£o degenerada.
> üí° **Exemplo Num√©rico:** Continuando o exemplo anterior, vamos gerar uma s√©rie temporal com T=100, estimar os par√¢metros, reescalonar os erros e observar o comportamento das estimativas. Para fins de ilustra√ß√£o, vamos gerar 100 simula√ß√µes e analisar os resultados das estimativas reescalonadas.
```python
import numpy as np
from numpy.linalg import inv
import matplotlib.pyplot as plt
from scipy.stats import norm

np.random.seed(42) # para reproducibilidade
num_simulations = 100
T = 100
alpha_true = 2
delta_true = 0.5
sigma = 1
alpha_errors = []
delta_errors = []

for _ in range(num_simulations):
    t = np.arange(1, T + 1)
    x = np.array([[1, ti] for ti in t])
    epsilon = np.random.normal(0, sigma, T)
    y = alpha_true + delta_true * t + epsilon
    XTX = x.T @ x
    XTX_inv = inv(XTX)
    beta_hat = XTX_inv @ x.T @ y
    alpha_hat = beta_hat[0]
    delta_hat = beta_hat[1]
    alpha_errors.append(np.sqrt(T)*(alpha_hat - alpha_true))
    delta_errors.append(T**(3/2)*(delta_hat - delta_true))
empirical_variance_alpha = np.var(alpha_errors)
empirical_variance_delta = np.var(delta_errors)
theoretical_variance_alpha = 4
theoretical_variance_delta = 12
print(f"Vari√¢ncia emp√≠rica do erro reescalonado de alpha: {empirical_variance_alpha:.2f}")
print(f"Vari√¢ncia te√≥rica do erro reescalonado de alpha: {theoretical_variance_alpha:.2f}")
print(f"Vari√¢ncia emp√≠rica do erro reescalonado de delta: {empirical_variance_delta:.2f}")
print(f"Vari√¢ncia te√≥rica do erro reescalonado de delta: {theoretical_variance_delta:.2f}")

# Histograma para alpha_errors
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.hist(alpha_errors, bins=10, density=True, alpha=0.6, color='skyblue', label='Distribui√ß√£o Emp√≠rica')
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = norm.pdf(x, 0, np.sqrt(theoretical_variance_alpha))
plt.plot(x, p, 'k', linewidth=2, label='Distribui√ß√£o Normal Te√≥rica')
plt.title(f'Distribui√ß√£o dos Erros Reescalonados de Œ±')
plt.xlabel('Erro Reescalonado de Œ±')
plt.ylabel('Densidade')
plt.legend()

# Histograma para delta_errors
plt.subplot(1, 2, 2)
plt.hist(delta_errors, bins=10, density=True, alpha=0.6, color='salmon', label='Distribui√ß√£o Emp√≠rica')
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = norm.pdf(x, 0, np.sqrt(theoretical_variance_delta))
plt.plot(x, p, 'k', linewidth=2, label='Distribui√ß√£o Normal Te√≥rica')
plt.title(f'Distribui√ß√£o dos Erros Reescalonados de Œ¥')
plt.xlabel('Erro Reescalonado de Œ¥')
plt.ylabel('Densidade')
plt.legend()

plt.tight_layout()
plt.show()
```
Este exemplo num√©rico demonstra que, ao reescalonar os erros da estimativa, a vari√¢ncia emp√≠rica se aproxima dos valores te√≥ricos, comprovando o resultado de que $Y_T(b_T - \beta)$ converge para uma distribui√ß√£o normal com matriz de covari√¢ncia $\sigma^2 Q^{-1}$, onde $\sigma^2 = 1$. Os histogramas mostram a distribui√ß√£o emp√≠rica dos erros, juntamente com as distribui√ß√µes normais te√≥ricas, refor√ßando a converg√™ncia.

**Proposi√ß√£o 2.1** *Considere o modelo de tend√™ncia temporal $y_t = \alpha + \delta t + \gamma t^2 + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$. Se $\hat{\beta}_T = [\hat{\alpha}_T, \hat{\delta}_T, \hat{\gamma}_T]'$ √© o estimador de MQO, ent√£o para obter uma distribui√ß√£o assint√≥tica n√£o degenerada, os erros de estimativa devem ser reescalonados como:
$$\begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \\ T^{5/2}(\hat{\gamma}_T - \gamma) \end{bmatrix}$$*

*Prova:*
I. A matriz $X$ ser√° agora definida como $x_t = [1 \quad t \quad t^2]'$.
II. Para que a matriz $\frac{1}{T^k}\sum_{t=1}^T x_t x_t'$ convirja para uma matriz n√£o singular, devemos dividir cada elemento por sua ordem de converg√™ncia apropriada.
III. Como vimos, $\sum_{t=1}^T 1$ √© de ordem $T$, $\sum_{t=1}^T t$ √© de ordem $T^2$, $\sum_{t=1}^T t^2$ √© de ordem $T^3$, $\sum_{t=1}^T t^3$ √© de ordem $T^4$ e $\sum_{t=1}^T t^4$ √© de ordem $T^5$.
IV. Assim, a matriz $Y_T$ deve ser dada por
$$Y_T = \begin{bmatrix} \sqrt{T} & 0 & 0 \\ 0 & T^{3/2} & 0 \\ 0 & 0 & T^{5/2} \end{bmatrix}$$
V. Dessa forma, o reescalonamento dos erros de estimativa  $Y_T(\hat{\beta}_T - \beta)$  resultar√° em uma distribui√ß√£o assint√≥tica n√£o degenerada.‚ñ†
> üí° **Exemplo Num√©rico:** Para verificar a necessidade do reescalonamento em um modelo quadr√°tico, vamos simular um modelo com tend√™ncia quadr√°tica e analisar a converg√™ncia dos estimadores sem e com reescalonamento.
```python
import numpy as np
from numpy.linalg import inv
import matplotlib.pyplot as plt

np.random.seed(42)
T = 100
num_simulations = 100
alpha_true = 1
delta_true = 0.5
gamma_true = 0.1
sigma = 1

alpha_errors_no_rescale = []
delta_errors_no_rescale = []
gamma_errors_no_rescale = []

alpha_errors_rescale = []
delta_errors_rescale = []
gamma_errors_rescale = []

for _ in range(num_simulations):
    t = np.arange(1, T + 1)
    x = np.array([[1, ti, ti**2] for ti in t])
    epsilon = np.random.normal(0, sigma, T)
    y = alpha_true + delta_true * t + gamma_true * t**2 + epsilon
    XTX = x.T @ x
    XTX_inv = inv(XTX)
    beta_hat = XTX_inv @ x.T @ y
    alpha_hat = beta_hat[0]
    delta_hat = beta_hat[1]
    gamma_hat = beta_hat[2]
    
    alpha_errors_no_rescale.append(alpha_hat - alpha_true)
    delta_errors_no_rescale.append(delta_hat - delta_true)
    gamma_errors_no_rescale.append(gamma_hat - gamma_true)

    alpha_errors_rescale.append(np.sqrt(T) * (alpha_hat - alpha_true))
    delta_errors_rescale.append(T**(3/2) * (delta_hat - delta_true))
    gamma_errors_rescale.append(T**(5/2) * (gamma_hat - gamma_true))

plt.figure(figsize=(15, 8))

plt.subplot(2, 3, 1)
plt.hist(alpha_errors_no_rescale, bins=10, alpha=0.6, color='skyblue', label='Sem Reescalonamento')
plt.title('Œ± - Sem Reescalonamento')
plt.xlabel('Erro')
plt.legend()

plt.subplot(2, 3, 2)
plt.hist(delta_errors_no_rescale, bins=10, alpha=0.6, color='skyblue', label='Sem Reescalonamento')
plt.title('Œ¥ - Sem Reescalonamento')
plt.xlabel('Erro')
plt.legend()

plt.subplot(2, 3, 3)
plt.hist(gamma_errors_no_rescale, bins=10, alpha=0.6, color='skyblue', label='Sem Reescalonamento')
plt.title('Œ≥ - Sem Reescalonamento')
plt.xlabel('Erro')
plt.legend()

plt.subplot(2, 3, 4)
plt.hist(alpha_errors_rescale, bins=10, alpha=0.6, color='salmon', label='Com Reescalonamento')
plt.title('Œ± - Com Reescalonamento')
plt.xlabel('Erro Reescalonado')
plt.legend()

plt.subplot(2, 3, 5)
plt.hist(delta_errors_rescale, bins=10, alpha=0.6, color='salmon', label='Com Reescalonamento')
plt.title('Œ¥ - Com Reescalonamento')
plt.xlabel('Erro Reescalonado')
plt.legend()

plt.subplot(2, 3, 6)
plt.hist(gamma_errors_rescale, bins=10, alpha=0.6, color='salmon', label='Com Reescalonamento')
plt.title('Œ≥ - Com Reescalonamento')
plt.xlabel('Erro Reescalonado')
plt.legend()

plt.tight_layout()
plt.show()

print("Vari√¢ncia dos erros sem reescalonamento:")
print(f"Vari√¢ncia de alpha: {np.var(alpha_errors_no_rescale):.4f}")
print(f"Vari√¢ncia de delta: {np.var(delta_errors_no_rescale):.4f}")
print(f"Vari√¢ncia de gamma: {np.var(gamma_errors_no_rescale):.4f}")

print("\nVari√¢ncia dos erros com reescalonamento:")
print(f"Vari√¢ncia de alpha: {np.var(alpha_errors_rescale):.4f}")
print(f"Vari√¢ncia de delta: {np.var(delta_errors_rescale):.4f}")
print(f"Vari√¢ncia de gamma: {np.var(gamma_errors_rescale):.4f}")
```
Este exemplo ilustra a import√¢ncia do reescalonamento. Sem o reescalonamento, os erros dos par√¢metros n√£o convergem para uma distribui√ß√£o com vari√¢ncia finita. Com o reescalonamento, a vari√¢ncia dos erros se estabiliza, como esperado.

**Lema 2.1** *A matriz $Y_T$ necess√°ria para reescalonar os erros de estimativa em um modelo com uma tend√™ncia polinomial de grau $p$ √© dada por uma matriz diagonal com elementos $T^{(2k+1)/2}$ na diagonal, para $k=0,1, \ldots, p$. Formalmente, se $x_t = [1 \quad t \quad t^2 \quad \ldots \quad t^p ]'$ ent√£o,
$$Y_T = diag[T^{1/2}, T^{3/2}, T^{5/2}, \ldots, T^{(2p+1)/2}]$$*

*Prova:*
I. Segue diretamente da Proposi√ß√£o 2.1 e da discuss√£o sobre as ordens de converg√™ncia das somas $\sum_{t=1}^T t^v$.
II. O termo dominante em $\sum_{t=1}^T t^v$ √© $T^{v+1}/(v+1)$.
III. Para o termo $t^k$, a ordem de converg√™ncia √© $T^{k+1}$.
IV. Para obter uma matriz convergente, √© necess√°rio reescalonar cada par√¢metro dividindo a soma das vari√°veis explicativas pela sua ordem de converg√™ncia.
V. Considerando que a matriz de covari√¢ncia √© invertida e que os par√¢metros s√£o multiplicados por  $Y_T$, o par√¢metro associado a $t^k$ deve ser multiplicado por $T^{(2k+1)/2}$.‚ñ†

**Corol√°rio 2.1** *Seja o modelo $y_t = x_t'\beta + \epsilon_t$, onde $x_t = [1 \quad t \quad t^2 \quad \ldots \quad t^p ]'$ e $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$. Se $\hat{\beta}_T$ √© o estimador de MQO para $\beta$, ent√£o a distribui√ß√£o assint√≥tica do vetor reescalonado $Y_T(\hat{\beta}_T - \beta)$ converge para uma normal multivariada com m√©dia zero e matriz de covari√¢ncia $\sigma^2 Q^{-1}$, onde $Q$ √© a matriz limite de $Y_T (\sum_{t=1}^T x_t x_t')^{-1} Y_T^{-1}$, e $Y_T$ √© definido pelo Lema 2.1*

*Prova:*
I. Este resultado segue diretamente da aplica√ß√£o do Teorema do Limite Central e do Teorema de Slutsky.
II. Ele tamb√©m usa os resultados do Lema 2.1 e da mesma l√≥gica usada no Teorema 2.1.
III. A matriz $Y_T$ reescala os erros de estimativa para que os termos envolvidos nas estimativas de MQO convirjam para uma matriz n√£o singular.
IV. Como consequ√™ncia, a distribui√ß√£o assint√≥tica de $Y_T(\hat{\beta}_T - \beta)$ converge para uma normal multivariada.‚ñ†

### Conclus√£o
Neste cap√≠tulo, exploramos detalhadamente o processo de reescalonamento em modelos com tend√™ncias temporais determin√≠sticas. A introdu√ß√£o da matriz $Y_T$ √© fundamental para ajustar as diferentes taxas de converg√™ncia dos par√¢metros e obter distribui√ß√µes assint√≥ticas n√£o degeneradas. Foi demonstrado como o reescalonamento, junto com o Teorema do Limite Central e as propriedades de converg√™ncia de martingales, leva √† converg√™ncia dos estimadores para uma distribui√ß√£o normal multivariada, permitindo a realiza√ß√£o de infer√™ncia assint√≥tica.
Os resultados apresentados validam a import√¢ncia de considerar cuidadosamente as propriedades das s√©ries temporais e seus efeitos nas distribui√ß√µes assint√≥ticas das estimativas. A necessidade de um reescalonamento espec√≠fico para par√¢metros em modelos com tend√™ncias temporais sublinha a diferen√ßa fundamental entre a an√°lise de s√©ries estacion√°rias e n√£o estacion√°rias.
O uso da matriz $Y_T$ √© uma solu√ß√£o chave para contornar a diverg√™ncia presente nas estimativas de MQO sob tend√™ncias temporais, assegurando que possamos realizar an√°lises estat√≠sticas v√°lidas em modelos de s√©ries temporais com tend√™ncias determin√≠sticas.

### Refer√™ncias
[^1]:  *Os coeficientes de modelos de regress√£o envolvendo ra√≠zes unit√°rias ou tend√™ncias temporais determin√≠sticas s√£o tipicamente estimados por m√≠nimos quadrados ordin√°rios.*
[^2]:  *No entanto, as distribui√ß√µes assint√≥ticas das estimativas dos coeficientes n√£o podem ser calculadas da mesma forma que aquelas para modelos de regress√£o envolvendo vari√°veis estacion√°rias.*
[^3]: *This section considers OLS estimation of the parameters of a simple time trend, $y_t = \alpha + \delta t + \epsilon_t$, for $\epsilon_t$ a white noise process. If $\epsilon_t \sim N(0, \sigma^2)$, then the model [16.1.1] satisfies the classical regression assumptions...*
[^4]: *Write [16.1.1] in the form of the standard regression model, $y_t = x_t'\beta + \epsilon_t$*
[^5]: *where $x_t = [1  \quad t]'$*
[^6]: *$\beta = [\alpha  \quad \delta]'$*
[^7]: *Let $b_T$ denote the OLS estimate of $\beta$ based on a sample of size $T$: $b_T = [\alpha_T  \quad \delta_T]' = (\sum x_t x_t')^{-1} \sum x_t y_t$*
[^8]: *Recall from equation [8.2.3] that the deviation of the OLS estimate from the true value can be expressed as $(b_T - \beta) = (\sum x_t x_t')^{-1} \sum x_t \epsilon_t$*
[^9]: *$(b_T - \beta) = (\sum x_t x_t')^{-1} \sum x_t \epsilon_t$*
[^10]: *To find the limiting distribution for a regression with stationary explanatory variables, the approach in Chapter 8 was to multiply [16.1.6] by $\sqrt{T}$, resulting in $\sqrt{T}(b_T - \beta) = [(1/T) \sum x_t x_t']^{-1} [(1/\sqrt{T}) \sum x_t \epsilon_t]$*
[^11]: *The usual assumption was that $(1/T) \sum x_t x_t'$ converged in probability to a nonsingular matrix $Q$ while $(1/\sqrt{T}) \sum x_t \epsilon_t$ converged in distribution to a $N(0, \sigma^2Q)$ random variable, implying that $\sqrt{T}(b_T - \beta) -> N(0, \sigma^2Q^{-1})$.*
[^12]: *...expression [16.1.6] would be $[(\alpha_T - \alpha)  \quad (\delta_T - \delta)]' = [\sum 1  \quad \sum t ; \sum t  \quad \sum t^2]^{-1} [\sum \epsilon_t ; \sum t\epsilon_t]$*
[^13]: *$[(\alpha_T - \alpha)  \quad (\delta_T - \delta)]' = [\sum 1  \quad \sum t ; \sum t  \quad \sum t^2]^{-1} [\sum \epsilon_t ; \sum t\epsilon_t]$*
[^14]: *It is straightforward to show by induction that $\sum t = T(T+1)/2$*
[^15]: *$\sum t^2 = T(T+1)(2T+1)/6.$*
[^16]: *Thus, the leading term in $\sum t$ is $T^2/2$; that is, $(1/T^2) \sum t = 1/2 + 1/(2T) \rightarrow 1/2$*
[^17]: *Similarly, the leading term in $\sum t^2$ is $T^3/3$: $(1/T^3) \sum t^2 = 1/3 + 1/(2T) + 1/(6T^2) \rightarrow 1/3.*
[^18]: *For future reference, we note here the general pattern-the leading term in $\sum t^v$ is $T^{v+1}/(v+1)$: $(1/T^{v+1}) \sum t^v \rightarrow 1/(v+1)$*
[^19]: *For $x$, given in [16.1.3], results [16.1.9] and [16.1.10] imply that $\sum x_t x_t' = [\sum 1  \quad \sum t ; \sum t  \quad \sum t^2] = [T  \quad T(T+1)/2 ;  T(T+1)/2  \quad T(T+1)(2T+1)/6]$*
[^20]: *$\sum x_t x_t' = [\sum 1  \quad \sum t ; \sum t  \quad \sum t^2] = [T  \quad T(T+1)/2 ;  T(T+1)/2  \quad T(T+1)(2T+1)/6]$*
[^21]: *In contrast to the usual result for stationary regressions, for the matrix in [16.1.16], $(1/T) \sum x_t x_t'$ diverges. To obtain a convergent matrix, [16.1.16] would have to be divided by $T^3$ rather than $T$.*
[^22]: *Unfortunately, this limiting matrix cannot be inverted, as $(1/T) \sum x_t x_t'$ can be in the usual case. Hence, a different approach from that in the stationary case will be needed to calculate the asymptotic distribution of $b_T$.*
[^23]: *It turns out that the OLS estimates $\alpha_T$ and $\delta_T$ have different asymptotic rates of convergence. To arrive at nondegenerate limiting distributions, $\alpha_T$ is multiplied by $\sqrt{T}$, whereas $\delta_T$ must be multiplied by $T^{3/2}$.*
[^24]: *We can think of this adjustment as premultiplying [16.1.6] or [16.1.8] by the matrix $Y_T = [\sqrt{T}  \quad 0 ; 0  \quad T^{3/2}]$*
[^25]: *$Y_T = [\sqrt{T}  \quad 0 ; 0  \quad T^{3/2}]$*.

### 16.1.2  Asymptotic Properties of the OLS Estimator

With the above notation, the OLS estimator can be represented as:

$$\hat{\beta} = \left( \sum_{t=1}^T X_t X_t' \right)^{-1} \left( \sum_{t=1}^T X_t Y_t \right)$$

Using the notation introduced in the previous section, and specifically using the matrix $Y_T$, this can be written as:

$$ \hat{\beta} =  \left( X'X\right)^{-1} X'Y = \left( X'X\right)^{-1} X' Y_T \beta_0 +  \left( X'X\right)^{-1} X' \epsilon  $$

Multiplying the equation by $Y_T^{-1}$, we have:

$$ Y_T^{-1} \hat{\beta} =  \left( Y_T^{-1} X'X Y_T^{-1} \right)^{-1} Y_T^{-1} X' Y_T \beta_0 + \left( Y_T^{-1} X'X Y_T^{-1} \right)^{-1} Y_T^{-1} X' \epsilon  $$

$$ Y_T^{-1} \hat{\beta} =  \beta_0 + \left( Y_T^{-1} X'X Y_T^{-1} \right)^{-1} Y_T^{-1} X' \epsilon  $$

$$ Y_T^{-1} (\hat{\beta} - \beta_0) = \left( Y_T^{-1} X'X Y_T^{-1} \right)^{-1} Y_T^{-1} X' \epsilon  $$

Let's define:
$$ \hat{\theta} = Y_T^{-1} \hat{\beta} $$

And
$$ \theta_0 = Y_T^{-1} \beta_0  $$

And let:
$$ Q_T = Y_T^{-1} X'X Y_T^{-1} $$

$$ \hat{\theta} - \theta_0 = Q_T^{-1} Y_T^{-1} X' \epsilon $$

Or:

$$ \hat{\theta} - \theta_0 = Q_T^{-1} \frac{1}{T} \sum_{t=1}^T  Y_T^{-1}X_t \epsilon_t  $$

### 16.1.3  Consistency

Assuming that $Q_T \xrightarrow{p} Q$, and that the term $\frac{1}{T} \sum_{t=1}^T Y_T^{-1}X_t \epsilon_t \xrightarrow{p} 0$, then we have that:

$$ \hat{\theta} \xrightarrow{p} \theta_0  $$

And, consequently, we have that:
$$ \hat{\beta} \xrightarrow{p} \beta_0  $$

### 16.1.4  Asymptotic Distribution

Let's consider the term:
$$ \sqrt{T}(\hat{\theta} - \theta_0) = \sqrt{T} Q_T^{-1} \frac{1}{T} \sum_{t=1}^T  Y_T^{-1}X_t \epsilon_t  $$

$$ \sqrt{T}(\hat{\theta} - \theta_0) = Q_T^{-1} \frac{1}{\sqrt{T}} \sum_{t=1}^T  Y_T^{-1}X_t \epsilon_t  $$

Assuming that:

$$ \frac{1}{\sqrt{T}} \sum_{t=1}^T Y_T^{-1}X_t \epsilon_t  \xrightarrow{d} N(0, \Omega) $$

Where $\Omega$ is the variance-covariance matrix.

And that $Q_T \xrightarrow{p} Q$, then:

$$ \sqrt{T}(\hat{\theta} - \theta_0)  \xrightarrow{d} N(0, Q^{-1} \Omega Q^{-1}) $$

And therefore, the asymptotic distribution of the OLS estimator, is:

$$ \sqrt{T}(\hat{\beta} - \beta_0)  \xrightarrow{d} N(0, Y_T Q^{-1} \Omega Q^{-1} Y_T ) $$

Where:

$$Y_T Q^{-1} \Omega Q^{-1} Y_T = \text{Avar}(\hat{\beta}) $$

<!-- END -->
