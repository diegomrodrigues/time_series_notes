## AnÃ¡lise Detalhada do Reescalonamento e DistribuiÃ§Ã£o AssintÃ³tica em Modelos de TendÃªncia Temporal

### IntroduÃ§Ã£o
Este capÃ­tulo aprofunda a anÃ¡lise da distribuiÃ§Ã£o assintÃ³tica das estimativas de mÃ­nimos quadrados ordinÃ¡rios (MQO) em modelos com tendÃªncias temporais determinÃ­sticas. O foco principal Ã© a necessidade de reescalonamento para obter distribuiÃ§Ãµes limitantes nÃ£o degeneradas e como a matriz $Y_T$ Ã© utilizada para ajustar as diferentes taxas de convergÃªncia dos parÃ¢metros [^23, ^24, ^25]. Expandindo sobre os conceitos de estimaÃ§Ã£o por MQO e distribuiÃ§Ãµes assintÃ³ticas abordados nos capÃ­tulos anteriores, este capÃ­tulo busca fornecer uma compreensÃ£o profunda dos mÃ©todos aplicÃ¡veis a processos nÃ£o estacionÃ¡rios com tendÃªncias temporais determinÃ­sticas. Em particular, estabelecemos que a multiplicaÃ§Ã£o da diferenÃ§a entre o estimador MQO e o valor verdadeiro do parÃ¢metro por $\sqrt{T}$ resulta em uma variÃ¡vel aleatÃ³ria que converge em distribuiÃ§Ã£o para uma normal multivariada.

### Conceitos Fundamentais e a Necessidade de Reescalonamento
Como visto anteriormente, o modelo de tendÃªncia temporal simples Ã© dado por [^3]:
$$y_t = \alpha + \delta t + \epsilon_t$$
onde $\epsilon_t$ Ã© um ruÃ­do branco com $\epsilon_t \sim N(0, \sigma^2)$. A forma padrÃ£o de regressÃ£o Ã© dada por [^4]:
$$y_t = x_t'\beta + \epsilon_t$$
onde $x_t = [1 \quad t]'$ [^5] e $\beta = [\alpha \quad \delta]'$ [^6]. A estimativa de MQO de $\beta$, denotada por $b_T$, Ã© expressa como [^7]:
$$b_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_t$$
O desvio do estimador MQO do valor verdadeiro Ã© expresso como [^8, ^9]:
$$b_T - \beta = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t$$
Para regressÃµes com variÃ¡veis estacionÃ¡rias, a distribuiÃ§Ã£o limitante Ã© obtida multiplicando-se o desvio do estimador por $\sqrt{T}$ [^10]:
$$ \sqrt{T}(b_T - \beta) = \left( \frac{1}{T} \sum_{t=1}^T x_t x_t' \right)^{-1} \left( \frac{1}{\sqrt{T}} \sum_{t=1}^T x_t \epsilon_t \right) $$
Entretanto, em modelos com tendÃªncia temporal determinÃ­stica, a matriz $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ diverge, nÃ£o convergindo para uma matriz nÃ£o singular [^21, ^22]. Isso ocorre porque as somas $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$ sÃ£o de ordens $T^2/2$ e $T^3/3$, respectivamente [^16, ^17]. Em geral, o termo dominante em $\sum_{t=1}^T t^v$ Ã© $T^{v+1}/(v+1)$ [^18].

Para lidar com essas taxas de convergÃªncia diferentes, reescalonamos os estimadores usando a matriz $Y_T$ [^24, ^25]:
$$Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}$$
Essa matriz ajusta as estimativas de $\alpha$ e $\delta$ de forma que ambos os parÃ¢metros tenham uma distribuiÃ§Ã£o limite nÃ£o degenerada, multiplicando $\hat{\alpha}_T$ por $\sqrt{T}$ e $\hat{\delta}_T$ por $T^{3/2}$ [^23]. Ao fazer isso, o desvio do estimador Ã© transformado em:
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} = Y_T (b_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1}  Y_T \sum_{t=1}^T x_t \epsilon_t$$

### DistribuiÃ§Ã£o AssintÃ³tica com Reescalonamento
A chave para obter distribuiÃ§Ãµes assintÃ³ticas nÃ£o degeneradas Ã© o uso da matriz $Y_T$. A aplicaÃ§Ã£o de $Y_T$ sobre a matriz de covariÃ¢ncia $\sum_{t=1}^T x_t x_t'$ leva a um termo que converge para uma matriz nÃ£o singular $Q$ [^28, ^29]:
$$Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1} \rightarrow Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$$
Este resultado Ã© crucial porque mostra que, ao reescalonar a matriz de covariÃ¢ncia com $Y_T$, a divergÃªncia Ã© resolvida, permitindo que a anÃ¡lise assintÃ³tica prossiga.
> ðŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar o efeito do reescalonamento, vamos usar um exemplo concreto com T=100 e analisar o comportamento do termo $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1}$. Este termo converge para a matriz $Q$. Podemos calcular esse termo utilizando Python:
```python
import numpy as np
from numpy.linalg import inv
T = 100
t = np.arange(1, T+1)
X = np.vstack([np.ones(T), t]).T
Y_T = np.array([[np.sqrt(T), 0], [0, T**(3/2)]])
XTX = X.T @ X
XTX_inv = inv(XTX)
Q_hat = Y_T @ XTX_inv @ Y_T.T
Q = np.array([[1, 1/2], [1/2, 1/3]])

print("Matriz Y_T (X'X)^-1 Y_T':")
print(Q_hat)
print("\nMatriz Q:")
print(Q)
```
O exemplo numÃ©rico demonstra como $Q_{hat}$ se aproxima da matriz limite $Q$ conforme $T$ aumenta, confirmando que a matriz $Y_T$ corrige a divergÃªncia do termo $\left( \sum_{t=1}^T x_t x_t' \right)^{-1}$.
O segundo termo da expressÃ£o reescalonada Ã©:
$$Y_T \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t \end{bmatrix} = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix}$$
O primeiro elemento, $(1/\sqrt{T})\sum_{t=1}^T \epsilon_t$, converge em distribuiÃ§Ã£o para $N(0, \sigma^2)$ pelo Teorema do Limite Central (TLC). O segundo elemento, $(1/\sqrt{T})\sum_{t=1}^T (t/T)\epsilon_t$, converge para $N(0, \sigma^2/3)$ devido a ser uma sequÃªncia de diferenÃ§as de martingales [^33, ^34, ^35]. A demonstraÃ§Ã£o deste resultado foi feita no capÃ­tulo anterior. AlÃ©m disso, a covariÃ¢ncia entre os dois elementos converge para zero, indicando que sÃ£o assintoticamente independentes [^37].
> ðŸ’¡ **Exemplo NumÃ©rico:** Para visualizar a convergÃªncia do segundo termo, vamos gerar uma sÃ©rie de erros aleatÃ³rios (ruÃ­do branco) e calcular o segundo elemento do vetor acima para diferentes valores de T, e verificar como a variÃ¢ncia se aproxima de  $\sigma^2/3$. Usaremos  $\sigma^2 = 1$
```python
import numpy as np

np.random.seed(42) # para reproducibilidade
num_simulations = 100
variances = []
Ts = [100, 500, 1000, 5000]
sigma_squared = 1

for T in Ts:
    second_terms = []
    for _ in range(num_simulations):
        epsilon = np.random.normal(0, np.sqrt(sigma_squared), T)
        t = np.arange(1, T + 1)
        second_term = (1/np.sqrt(T)) * np.sum((t/T) * epsilon)
        second_terms.append(second_term)
    variances.append(np.var(second_terms))

print("VariÃ¢ncia do segundo termo para diferentes T:")
for T, var in zip(Ts, variances):
    print(f"T = {T}: VariÃ¢ncia = {var:.4f}")
print(f"\nValor teÃ³rico da variÃ¢ncia: {sigma_squared/3:.4f}")
```
Este exemplo mostra que Ã  medida que $T$ aumenta, a variÃ¢ncia empÃ­rica do segundo termo se aproxima do valor teÃ³rico $\sigma^2 / 3$, ilustrando a convergÃªncia em distribuiÃ§Ã£o.

**Teorema 2.1**
*Seja $\hat{\beta}_T = [\hat{\alpha}_T, \hat{\delta}_T]' $ o estimador de MQO do modelo $y_t = \alpha + \delta t + \epsilon_t$, onde $\epsilon_t$ Ã© um ruÃ­do branco com mÃ©dia zero, variÃ¢ncia $\sigma^2$ e quarto momento finito. EntÃ£o,
$$
\begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q^{-1})
$$
onde $Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$ e $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$*
*Prova:*
I. Pela anÃ¡lise feita anteriormente, sabemos que:
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} = Y_T(b_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}  \sum_{t=1}^T x_t \epsilon_t $$
II. TambÃ©m mostramos que $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1} \rightarrow Q$, onde $Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$.
III. AlÃ©m disso, demonstramos que:
$$ Y_T \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q)$$
IV. Aplicando o teorema de Slutsky, concluÃ­mos que:
$$Y_T(b_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1})$$
Onde $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$.
Isso demonstra que a multiplicaÃ§Ã£o do desvio da estimativa do parÃ¢metro por $\sqrt{T}$ e $T^{3/2}$ gera uma variÃ¡vel aleatÃ³ria que converge em distribuiÃ§Ã£o para uma normal multivariada com uma matriz de covariÃ¢ncia bem definida.â– 

O teorema 2.1 estabelece que o vetor reescalonado dos erros de estimativa, $Y_T (b_T - \beta)$, converge em distribuiÃ§Ã£o para uma normal multivariada com mÃ©dia zero e matriz de covariÃ¢ncia $\sigma^2 Q^{-1}$. Isso permite realizar inferÃªncia assintÃ³tica sobre os parÃ¢metros do modelo com tendÃªncia temporal determinÃ­stica. Em particular, esse resultado explica porque o reescalonamento por $\sqrt{T}$ e $T^{3/2}$ para $\hat{\alpha}_T$ e $\hat{\delta}_T$ resulta em uma distribuiÃ§Ã£o assintÃ³tica nÃ£o degenerada.
> ðŸ’¡ **Exemplo NumÃ©rico:** Continuando o exemplo anterior, vamos gerar uma sÃ©rie temporal com T=100, estimar os parÃ¢metros, reescalonar os erros e observar o comportamento das estimativas. Para fins de ilustraÃ§Ã£o, vamos gerar 100 simulaÃ§Ãµes e analisar os resultados das estimativas reescalonadas.
```python
import numpy as np
from numpy.linalg import inv
import matplotlib.pyplot as plt
from scipy.stats import norm

np.random.seed(42) # para reproducibilidade
num_simulations = 100
T = 100
alpha_true = 2
delta_true = 0.5
sigma = 1
alpha_errors = []
delta_errors = []

for _ in range(num_simulations):
    t = np.arange(1, T + 1)
    x = np.array([[1, ti] for ti in t])
    epsilon = np.random.normal(0, sigma, T)
    y = alpha_true + delta_true * t + epsilon
    XTX = x.T @ x
    XTX_inv = inv(XTX)
    beta_hat = XTX_inv @ x.T @ y
    alpha_hat = beta_hat[0]
    delta_hat = beta_hat[1]
    alpha_errors.append(np.sqrt(T)*(alpha_hat - alpha_true))
    delta_errors.append(T**(3/2)*(delta_hat - delta_true))
empirical_variance_alpha = np.var(alpha_errors)
empirical_variance_delta = np.var(delta_errors)
theoretical_variance_alpha = 4
theoretical_variance_delta = 12
print(f"VariÃ¢ncia empÃ­rica do erro reescalonado de alpha: {empirical_variance_alpha:.2f}")
print(f"VariÃ¢ncia teÃ³rica do erro reescalonado de alpha: {theoretical_variance_alpha:.2f}")
print(f"VariÃ¢ncia empÃ­rica do erro reescalonado de delta: {empirical_variance_delta:.2f}")
print(f"VariÃ¢ncia teÃ³rica do erro reescalonado de delta: {theoretical_variance_delta:.2f}")

# Histograma para alpha_errors
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.hist(alpha_errors, bins=10, density=True, alpha=0.6, color='skyblue', label='DistribuiÃ§Ã£o EmpÃ­rica')
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = norm.pdf(x, 0, np.sqrt(theoretical_variance_alpha))
plt.plot(x, p, 'k', linewidth=2, label='DistribuiÃ§Ã£o Normal TeÃ³rica')
plt.title(f'DistribuiÃ§Ã£o dos Erros Reescalonados de Î±')
plt.xlabel('Erro Reescalonado de Î±')
plt.ylabel('Densidade')
plt.legend()

# Histograma para delta_errors
plt.subplot(1, 2, 2)
plt.hist(delta_errors, bins=10, density=True, alpha=0.6, color='salmon', label='DistribuiÃ§Ã£o EmpÃ­rica')
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = norm.pdf(x, 0, np.sqrt(theoretical_variance_delta))
plt.plot(x, p, 'k', linewidth=2, label='DistribuiÃ§Ã£o Normal TeÃ³rica')
plt.title(f'DistribuiÃ§Ã£o dos Erros Reescalonados de Î´')
plt.xlabel('Erro Reescalonado de Î´')
plt.ylabel('Densidade')
plt.legend()

plt.tight_layout()
plt.show()
```
Este exemplo numÃ©rico demonstra que, ao reescalonar os erros da estimativa, a variÃ¢ncia empÃ­rica se aproxima dos valores teÃ³ricos, comprovando o resultado de que $Y_T(b_T - \beta)$ converge para uma distribuiÃ§Ã£o normal com matriz de covariÃ¢ncia $\sigma^2 Q^{-1}$, onde $\sigma^2 = 1$. Os histogramas mostram a distribuiÃ§Ã£o empÃ­rica dos erros, juntamente com as distribuiÃ§Ãµes normais teÃ³ricas, reforÃ§ando a convergÃªncia.

**ProposiÃ§Ã£o 2.1** *Considere o modelo de tendÃªncia temporal $y_t = \alpha + \delta t + \gamma t^2 + \epsilon_t$, onde $\epsilon_t$ Ã© um ruÃ­do branco com mÃ©dia zero e variÃ¢ncia $\sigma^2$. Se $\hat{\beta}_T = [\hat{\alpha}_T, \hat{\delta}_T, \hat{\gamma}_T]'$ Ã© o estimador de MQO, entÃ£o para obter uma distribuiÃ§Ã£o assintÃ³tica nÃ£o degenerada, os erros de estimativa devem ser reescalonados como:
$$\begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \\ T^{5/2}(\hat{\gamma}_T - \gamma) \end{bmatrix}$$*

*Prova:*
I. A matriz $X$ serÃ¡ agora definida como $x_t = [1 \quad t \quad t^2]'$.
II. Para que a matriz $\frac{1}{T^k}\sum_{t=1}^T x_t x_t'$ convirja para uma matriz nÃ£o singular, devemos dividir cada elemento por sua ordem de convergÃªncia apropriada.
III. Como vimos, $\sum_{t=1}^T 1$ Ã© de ordem $T$, $\sum_{t=1}^T t$ Ã© de ordem $T^2$, $\sum_{t=1}^T t^2$ Ã© de ordem $T^3$, $\sum_{t=1}^T t^3$ Ã© de ordem $T^4$ e $\sum_{t=1}^T t^4$ Ã© de ordem $T^5$.
IV. Assim, a matriz $Y_T$ deve ser dada por
$$Y_T = \begin{bmatrix} \sqrt{T} & 0 & 0 \\ 0 & T^{3/2} & 0 \\ 0 & 0 & T^{5/2} \end{bmatrix}$$
V. Dessa forma, o reescalonamento dos erros de estimativa  $Y_T(\hat{\beta}_T - \beta)$  resultarÃ¡ em uma distribuiÃ§Ã£o assintÃ³tica nÃ£o degenerada.â– 
> ðŸ’¡ **Exemplo NumÃ©rico:** Para verificar a necessidade do reescalonamento em um modelo quadrÃ¡tico, vamos simular um modelo com tendÃªncia quadrÃ¡tica e analisar a convergÃªncia dos estimadores sem e com reescalonamento.
```python
import numpy as np
from numpy.linalg import inv
import matplotlib.pyplot as plt

np.random.seed(42)
T = 100
num_simulations = 100
alpha_true = 1
delta_true = 0.5
gamma_true = 0.1
sigma = 1

alpha_errors_no_rescale = []
delta_errors_no_rescale = []
gamma_errors_no_rescale = []

alpha_errors_rescale = []
delta_errors_rescale = []
gamma_errors_rescale = []

for _ in range(num_simulations):
    t = np.arange(1, T + 1)
    x = np.array([[1, ti, ti**2] for ti in t])
    epsilon = np.random.normal(0, sigma, T)
    y = alpha_true + delta_true * t + gamma_true * t**2 + epsilon
    XTX = x.T @ x
    XTX_inv = inv(XTX)
    beta_hat = XTX_inv @ x.T @ y
    alpha_hat = beta_hat[0]
    delta_hat = beta_hat[1]
    gamma_hat = beta_hat[2]
    
    alpha_errors_no_rescale.append(alpha_hat - alpha_true)
    delta_errors_no_rescale.append(delta_hat - delta_true)
    gamma_errors_no_rescale.append(gamma_hat - gamma_true)

    alpha_errors_rescale.append(np.sqrt(T) * (alpha_hat - alpha_true))
    delta_errors_rescale.append(T**(3/2) * (delta_hat - delta_true))
    gamma_errors_rescale.append(T**(5/2) * (gamma_hat - gamma_true))

plt.figure(figsize=(15, 8))

plt.subplot(2, 3, 1)
plt.hist(alpha_errors_no_rescale, bins=10, alpha=0.6, color='skyblue', label='Sem Reescalonamento')
plt.title('Î± - Sem Reescalonamento')
plt.xlabel('Erro')
plt.legend()

plt.subplot(2, 3, 2)
plt.hist(delta_errors_no_rescale, bins=10, alpha=0.6, color='skyblue', label='Sem Reescalonamento')
plt.title('Î´ - Sem Reescalonamento')
plt.xlabel('Erro')
plt.legend()

plt.subplot(2, 3, 3)
plt.hist(gamma_errors_no_rescale, bins=10, alpha=0.6, color='skyblue', label='Sem Reescalonamento')
plt.title('Î³ - Sem Reescalonamento')
plt.xlabel('Erro')
plt.legend()

plt.subplot(2, 3, 4)
plt.hist(alpha_errors_rescale, bins=10, alpha=0.6, color='salmon', label='Com Reescalonamento')
plt.title('Î± - Com Reescalonamento')
plt.xlabel('Erro Reescalonado')
plt.legend()

plt.subplot(2, 3, 5)
plt.hist(delta_errors_rescale, bins=10, alpha=0.6, color='salmon', label='Com Reescalonamento')
plt.title('Î´ - Com Reescalonamento')
plt.xlabel('Erro Reescalonado')
plt.legend()

plt.subplot(2, 3, 6)
plt.hist(gamma_errors_rescale, bins=10, alpha=0.6, color='salmon', label='Com Reescalonamento')
plt.title('Î³ - Com Reescalonamento')
plt.xlabel('Erro Reescalonado')
plt.legend()

plt.tight_layout()
plt.show()

print("VariÃ¢ncia dos erros sem reescalonamento:")
print(f"VariÃ¢ncia de alpha: {np.var(alpha_errors_no_rescale):.4f}")
print(f"VariÃ¢ncia de delta: {np.var(delta_errors_no_rescale):.4f}")
print(f"VariÃ¢ncia de gamma: {np.var(gamma_errors_no_rescale):.4f}")

print("\nVariÃ¢ncia dos erros com reescalonamento:")
print(f"VariÃ¢ncia de alpha: {np.var(alpha_errors_rescale):.4f}")
print(f"VariÃ¢ncia de delta: {np.var(delta_errors_rescale):.4f}")
print(f"VariÃ¢ncia de gamma: {np.var(gamma_errors_rescale):.4f}")
```
Este exemplo ilustra a importÃ¢ncia do reescalonamento. Sem o reescalonamento, os erros dos parÃ¢metros nÃ£o convergem para uma distribuiÃ§Ã£o com variÃ¢ncia finita. Com o reescalonamento, a variÃ¢ncia dos erros se estabiliza, como esperado.

**Lema 2.1** *A matriz $Y_T$ necessÃ¡ria para reescalonar os erros de estimativa em um modelo com uma tendÃªncia polinomial de grau $p$ Ã© dada por uma matriz diagonal com elementos $T^{(2k+1)/2}$ na diagonal, para $k=0,1, \ldots, p$. Formalmente, se $x_t = [1 \quad t \quad t^2 \quad \ldots \quad t^p ]'$ entÃ£o,
$$Y_T = diag[T^{1/2}, T^{3/2}, T^{5/2}, \ldots, T^{(2p+1)/2}]$$*

*Prova:*
I. Segue diretamente da ProposiÃ§Ã£o 2.1 e da discussÃ£o sobre as ordens de convergÃªncia das somas $\sum_{t=1}^T t^v$.
II. O termo dominante em $\sum_{t=1}^T t^v$ Ã© $T^{v+1}/(v+1)$.
III. Para o termo $t^k$, a ordem de convergÃªncia Ã© $T^{k+1}$.
IV. Para obter uma matriz convergente, Ã© necessÃ¡rio reescalonar cada parÃ¢metro dividindo a soma das variÃ¡veis explicativas pela sua ordem de convergÃªncia.
V. Considerando que a matriz de covariÃ¢ncia Ã© invertida e que os parÃ¢metros sÃ£o multiplicados por  $Y_T$, o parÃ¢metro associado a $t^k$ deve ser multiplicado por $T^{(2k+1)/2}$.â– 

**CorolÃ¡rio 2.1** *Seja o modelo $y_t = x_t'\beta + \epsilon_t$, onde $x_t = [1 \quad t \quad t^2 \quad \ldots \quad t^p ]'$ e $\epsilon_t$ Ã© um ruÃ­do branco com mÃ©dia zero e variÃ¢ncia $\sigma^2$. Se $\hat{\beta}_T$ Ã© o estimador de MQO para $\beta$, entÃ£o a distribuiÃ§Ã£o assintÃ³tica do vetor reescalonado $Y_T(\hat{\beta}_T - \beta)$ converge para uma normal multivariada com mÃ©dia zero e matriz de covariÃ¢ncia $\sigma^2 Q^{-1}$, onde $Q$ Ã© a matriz limite de $Y_T (\sum_{t=1}^T x_t x_t')^{-1} Y_T^{-1}$, e $Y_T$ Ã© definido pelo Lema 2.1*

*Prova:*
I. Este resultado segue diretamente da aplicaÃ§Ã£o do Teorema do Limite Central e do Teorema de Slutsky.
II. Ele tambÃ©m usa os resultados do Lema 2.1 e da mesma lÃ³gica usada no Teorema 2.1.
III. A matriz $Y_T$ reescala os erros de estimativa para que os termos envolvidos nas estimativas de MQO convirjam para uma matriz nÃ£o singular.
IV. Como consequÃªncia, a distribuiÃ§Ã£o assintÃ³tica de $Y_T(\hat{\beta}_T - \beta)$ converge para uma normal multivariada.â– 

### ConclusÃ£o
Neste capÃ­tulo, exploramos detalhadamente o processo de reescalonamento em modelos com tendÃªncias temporais determinÃ­sticas. A introduÃ§Ã£o da matriz $Y_T$ Ã© fundamental para ajustar as diferentes taxas de convergÃªncia dos parÃ¢metros e obter distribuiÃ§Ãµes assintÃ³ticas nÃ£o degeneradas. Foi demonstrado como o reescalonamento, junto com o Teorema do Limite Central e as propriedades de convergÃªncia de martingales, leva Ã  convergÃªncia dos estimadores para uma distribuiÃ§Ã£o normal multivariada, permitindo a realizaÃ§Ã£o de inferÃªncia assintÃ³tica.
Os resultados apresentados validam a importÃ¢ncia de considerar cuidadosamente as propriedades das sÃ©ries temporais e seus efeitos nas distribuiÃ§Ãµes assintÃ³ticas das estimativas. A necessidade de um reescalonamento especÃ­fico para parÃ¢metros em modelos com tendÃªncias temporais sublinha a diferenÃ§a fundamental entre a anÃ¡lise de sÃ©ries estacionÃ¡rias e nÃ£o estacionÃ¡rias.
O uso da matriz $Y_T$ Ã© uma soluÃ§Ã£o chave para contornar a divergÃªncia presente nas estimativas de MQO sob tendÃªncias temporais, assegurando que possamos realizar anÃ¡lises estatÃ­sticas vÃ¡lidas em modelos de sÃ©ries temporais com tendÃªncias determinÃ­sticas.

### ReferÃªncias
[^1]:  *Os coeficientes de modelos de regressÃ£o envolvendo raÃ­zes unitÃ¡rias ou tendÃªncias temporais determinÃ­sticas sÃ£o tipicamente estimados por mÃ­nimos quadrados ordinÃ¡rios.*
[^2]:  *No entanto, as distribuiÃ§Ãµes assintÃ³ticas das estimativas dos coeficientes nÃ£o podem ser calculadas da mesma forma que aquelas para modelos de regressÃ£o envolvendo variÃ¡veis estacionÃ¡rias.*
[^3]: *This section considers OLS estimation of the parameters of a simple time trend, $y_t = \alpha + \delta t + \epsilon_t$, for $\epsilon_t$ a white noise process. If $\epsilon_t \sim N(0, \sigma^2)$, then the model [16.1.1] satisfies the classical regression assumptions...*
[^4]: *Write [16.1.1] in the form of the standard regression model, $y_t = x_t'\beta + \epsilon_t$*
[^5]: *where $x_t = [1  \quad t]'$*
[^6]: *$\beta = [\alpha  \quad \delta]'$*
[^7]: *Let $b_T$ denote the OLS estimate of $\beta$ based on a sample of size $T$: $b_T = [\alpha_T  \quad \delta_T]' = (\sum x_t x_t')^{-1} \sum x_t y_t$*
[^8]: *Recall from equation [8.2.3] that the deviation of the OLS estimate from the true value can be expressed as $(b_T - \beta) = (\sum x_t x_t')^{-1} \sum x_t \epsilon_t$*
[^9]: *$(b_T - \beta) = (\sum x_t x_t')^{-1} \sum x_t \epsilon_t$*
[^10]: *To find the limiting distribution for a regression with stationary explanatory variables, the approach in Chapter 8 was to multiply [16.1.6] by $\sqrt{T}$, resulting in $\sqrt{T}(b_T - \beta) = [(1/T) \sum x_t x_t']^{-1} [(1/\sqrt{T}) \sum x_t \epsilon_t]$*
[^11]: *The usual assumption was that $(1/T) \sum x_t x_t'$ converged in probability to a nonsingular matrix $Q$ while $(1/\sqrt{T}) \sum x_t \epsilon_t$ converged in distribution to a $N(0, \sigma^2Q)$ random variable, implying that $\sqrt{T}(b_T - \beta) -> N(0, \sigma^2Q^{-1})$.*
[^12]: *...expression [16.1.6] would be $[(\alpha_T - \alpha)  \quad (\delta_T - \delta)]' = [\sum 1  \quad \sum t ; \sum t  \quad \sum t^2]^{-1} [\sum \epsilon_t ; \sum t\epsilon_t]$*
[^13]: *$[(\alpha_T - \alpha)  \quad (\delta_T - \delta)]' = [\sum 1  \quad \sum t ; \sum t  \quad \sum t^2]^{-1} [\sum \epsilon_t ; \sum t\epsilon_t]$*
[^14]: *It is straightforward to show by induction that $\sum t = T(T+1)/2$*
[^15]: *$\sum t^2 = T(T+1)(2T+1)/6.$*
[^16]: *Thus, the leading term in $\sum t$ is $T^2/2$; that is, $(1/T^2) \sum t = 1/2 + 1/(2T) \rightarrow 1/2$*
[^17]: *Similarly, the leading term in $\sum t^2$ is $T^3/3$: $(1/T^3) \sum t^2 = 1/3 + 1/(2T) + 1/(6T^2) \rightarrow 1/3.*
[^18]: *For future reference, we note here the general pattern-the leading term in $\sum t^v$ is $T^{v+1}/(v+1)$: $(1/T^{v+1}) \sum t^v \rightarrow 1/(v+1)$*
[^19]: *For $x$, given in [16.1.3], results [16.1.9] and [16.1.10] imply that $\sum x_t x_t' = [\sum 1  \quad \sum t ; \sum t  \quad \sum t^2] = [T  \quad T(T+1)/2 ;  T(T+1)/2  \quad T(T+1)(2T+1)/6]$*
[^20]: *$\sum x_t x_t' = [\sum 1  \quad \sum t ; \sum t  \quad \sum t^2] = [T  \quad T(T+1)/2 ;  T(T+1)/2  \quad T(T+1)(2T+1)/6]$*
[^21]: *In contrast to the usual result for stationary regressions, for the matrix in [16.1.16], $(1/T) \sum x_t x_t'$ diverges. To obtain a convergent matrix, [16.1.16] would have to be divided by $T^3$ rather than $T$.*
[^22]: *Unfortunately, this limiting matrix cannot be inverted, as $(1/T) \sum x_t x_t'$ can be in the usual case. Hence, a different approach from that in the stationary case will be needed to calculate the asymptotic distribution of $b_T$.*
[^23]: *It turns out that the OLS estimates $\alpha_T$ and $\delta_T$ have different asymptotic rates of convergence. To arrive at nondegenerate limiting distributions, $\alpha_T$ is multiplied by $\sqrt{T}$, whereas $\delta_T$ must be multiplied by $T^{3/2}$.*
[^24]: *We can think of this adjustment as premultiplying [16.1.6] or [16.1.8] by the matrix $Y_T = [\sqrt{T}  \quad 0 ; 0  \quad T^{3/2}]$*
[^25]: *$Y_T = [\sqrt{T}  \quad 0 ; 0  \quad T^{3/2}]$*.

### 16.1.2  Asymptotic Properties of the OLS Estimator

With the above notation, the OLS estimator can be represented as:

$$\hat{\beta} = \left( \sum_{t=1}^T X_t X_t' \right)^{-1} \left( \sum_{t=1}^T X_t Y_t \right)$$

Using the notation introduced in the previous section, and specifically using the matrix $Y_T$, this can be written as:

$$ \hat{\beta} =  \left( X'X\right)^{-1} X'Y = \left( X'X\right)^{-1} X' Y_T \beta_0 +  \left( X'X\right)^{-1} X' \epsilon  $$

Multiplying the equation by $Y_T^{-1}$, we have:

$$ Y_T^{-1} \hat{\beta} =  \left( Y_T^{-1} X'X Y_T^{-1} \right)^{-1} Y_T^{-1} X' Y_T \beta_0 + \left( Y_T^{-1} X'X Y_T^{-1} \right)^{-1} Y_T^{-1} X' \epsilon  $$

$$ Y_T^{-1} \hat{\beta} =  \beta_0 + \left( Y_T^{-1} X'X Y_T^{-1} \right)^{-1} Y_T^{-1} X' \epsilon  $$

$$ Y_T^{-1} (\hat{\beta} - \beta_0) = \left( Y_T^{-1} X'X Y_T^{-1} \right)^{-1} Y_T^{-1} X' \epsilon  $$

Let's define:
$$ \hat{\theta} = Y_T^{-1} \hat{\beta} $$

And
$$ \theta_0 = Y_T^{-1} \beta_0  $$

And let:
$$ Q_T = Y_T^{-1} X'X Y_T^{-1} $$

$$ \hat{\theta} - \theta_0 = Q_T^{-1} Y_T^{-1} X' \epsilon $$

Or:

$$ \hat{\theta} - \theta_0 = Q_T^{-1} \frac{1}{T} \sum_{t=1}^T  Y_T^{-1}X_t \epsilon_t  $$

### 16.1.3  Consistency

Assuming that $Q_T \xrightarrow{p} Q$, and that the term $\frac{1}{T} \sum_{t=1}^T Y_T^{-1}X_t \epsilon_t \xrightarrow{p} 0$, then we have that:

$$ \hat{\theta} \xrightarrow{p} \theta_0  $$

And, consequently, we have that:
$$ \hat{\beta} \xrightarrow{p} \beta_0  $$

### 16.1.4  Asymptotic Distribution

Let's consider the term:
$$ \sqrt{T}(\hat{\theta} - \theta_0) = \sqrt{T} Q_T^{-1} \frac{1}{T} \sum_{t=1}^T  Y_T^{-1}X_t \epsilon_t  $$

$$ \sqrt{T}(\hat{\theta} - \theta_0) = Q_T^{-1} \frac{1}{\sqrt{T}} \sum_{t=1}^T  Y_T^{-1}X_t \epsilon_t  $$

Assuming that:

$$ \frac{1}{\sqrt{T}} \sum_{t=1}^T Y_T^{-1}X_t \epsilon_t  \xrightarrow{d} N(0, \Omega) $$

Where $\Omega$ is the variance-covariance matrix.

And that $Q_T \xrightarrow{p} Q$, then:

$$ \sqrt{T}(\hat{\theta} - \theta_0)  \xrightarrow{d} N(0, Q^{-1} \Omega Q^{-1}) $$

And therefore, the asymptotic distribution of the OLS estimator, is:

$$ \sqrt{T}(\hat{\beta} - \beta_0)  \xrightarrow{d} N(0, Y_T Q^{-1} \Omega Q^{-1} Y_T ) $$

Where:

$$Y_T Q^{-1} \Omega Q^{-1} Y_T = \text{Avar}(\hat{\beta}) $$

<!-- END -->
