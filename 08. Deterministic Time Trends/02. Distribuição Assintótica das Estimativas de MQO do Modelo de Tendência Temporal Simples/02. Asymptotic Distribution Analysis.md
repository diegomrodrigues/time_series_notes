## AnÃ¡lise Detalhada do Reescalonamento em Modelos de TendÃªncia Temporal DeterminÃ­stica

### IntroduÃ§Ã£o
Este capÃ­tulo explora a distribuiÃ§Ã£o assintÃ³tica das estimativas de mÃ­nimos quadrados ordinÃ¡rios (MQO) em modelos com tendÃªncias temporais determinÃ­sticas, com foco especÃ­fico na necessidade de reescalonamento para obter distribuiÃ§Ãµes limitantes nÃ£o degeneradas. Como vimos anteriormente, no contexto de modelos de regressÃ£o com variÃ¡veis estacionÃ¡rias, as distribuiÃ§Ãµes assintÃ³ticas das estimativas de coeficientes podem ser obtidas diretamente usando resultados padrÃ£o. No entanto, em modelos com tendÃªncias temporais, as diferentes taxas de convergÃªncia dos parÃ¢metros tornam necessÃ¡rio um tratamento especial.  Este capÃ­tulo se aprofunda na anÃ¡lise do reescalonamento, conectando os conceitos de convergÃªncia em probabilidade e distribuiÃ§Ãµes assintÃ³ticas previamente apresentados [^10, ^11] com os resultados especÃ­ficos para processos nÃ£o estacionÃ¡rios com tendÃªncia temporal.

### Conceitos Fundamentais
A anÃ¡lise da distribuiÃ§Ã£o assintÃ³tica das estimativas de MQO no modelo de tendÃªncia temporal simples, dado por
$$y_t = \alpha + \delta t + \epsilon_t$$
requer atenÃ§Ã£o especial devido Ã  nÃ£o estacionariedade induzida pela tendÃªncia temporal determinÃ­stica. Como discutido, a forma padrÃ£o de regressÃ£o Ã© dada por [^4]:
$$y_t = x_t'\beta + \epsilon_t$$
onde $x_t = [1 \quad t]'$ [^5] e $\beta = [\alpha \quad \delta]'$ [^6]. A estimativa de MQO de $\beta$, denotada por $b_T$, Ã© dada por [^7]:
$$b_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_t$$
O desvio do estimador MQO do valor verdadeiro Ã© expresso como [^8, ^9]:
$$b_T - \beta = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t$$

Em modelos com variÃ¡veis estacionÃ¡rias, a distribuiÃ§Ã£o assintÃ³tica de $b_T$ Ã© obtida multiplicando essa expressÃ£o por $\sqrt{T}$ [^10], com a suposiÃ§Ã£o de que $(1/T) \sum_{t=1}^T x_t x_t'$ converge para uma matriz nÃ£o singular $Q$ e $(1/\sqrt{T}) \sum_{t=1}^T x_t \epsilon_t$ converge para uma distribuiÃ§Ã£o normal [^11]. No entanto, para modelos com tendÃªncia temporal determinÃ­stica, como demonstrado anteriormente [^12, ^13], essa abordagem nÃ£o se sustenta devido ao comportamento divergente da matriz de covariÃ¢ncia $\sum_{t=1}^T x_t x_t'$.

Ã‰ crucial entender o comportamento das somas de sÃ©ries temporais determinÃ­sticas, que sÃ£o essenciais para a anÃ¡lise das taxas de convergÃªncia e, portanto, para o reescalonamento adequado das variÃ¡veis. Conforme demonstrado, as somas $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$ sÃ£o assintoticamente equivalentes a $T^2/2$ e $T^3/3$, respectivamente [^16, ^17]. Mais geralmente, para qualquer inteiro positivo $v$, o termo dominante em $\sum_{t=1}^T t^v$ Ã© dado por $T^{v+1}/(v+1)$ [^18].

A matriz de covariÃ¢ncia, dada por [^19, ^20]:
$$\sum_{t=1}^T x_t x_t' = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix} = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} $$
possui componentes que divergem quando divididos por $T$, o que indica a necessidade de um reescalonamento mais forte.  O termo $(1/T)\sum_{t=1}^T x_t x_t'$ diverge [^21], e, para obter uma matriz convergente, ela precisa ser dividida por $T^3$ [^21],  e nÃ£o por $T$, como nos casos de variÃ¡veis estacionÃ¡rias. Essa divergÃªncia Ã© a razÃ£o pela qual o mÃ©todo usual para obter a distribuiÃ§Ã£o assintÃ³tica nÃ£o funciona nesse caso e uma abordagem diferente Ã© necessÃ¡ria [^22].

**Lema 1**
A matriz $\frac{1}{T^3}\sum_{t=1}^T x_t x_t'$ converge para uma matriz nÃ£o singular.

*Prova:*
Dividindo a matriz $\sum_{t=1}^T x_t x_t'$ por $T^3$, obtemos:
$$\frac{1}{T^3} \sum_{t=1}^T x_t x_t' = \frac{1}{T^3} \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} = \begin{bmatrix} \frac{1}{T^2} & \frac{1}{2T} + \frac{1}{2T^2} \\ \frac{1}{2T} + \frac{1}{2T^2} & \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2} \end{bmatrix}$$
Quando $T \rightarrow \infty$, essa matriz converge para:
$$\begin{bmatrix} 0 & 0 \\ 0 & \frac{1}{3} \end{bmatrix}$$
que Ã© uma matriz singular. Isso demonstra que dividir por $T^3$ nÃ£o resulta em uma matriz limite nÃ£o singular. No entanto, a matriz reescalonada $T^{-3}(X'X)$ converge para uma matriz nÃ£o singular apÃ³s o reescalonamento apropriado, conforme serÃ¡ demonstrado posteriormente com a introduÃ§Ã£o da matriz $Y_T$.  Este resultado Ã© essencial para entender porque o reescalonamento por $Y_T$ Ã© necessÃ¡rio.
> ðŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar, vamos considerar um pequeno conjunto de dados com $T=10$. Vamos supor que temos uma sÃ©rie temporal com valores $y_t$ gerados por $y_t = 2 + 0.5t + \epsilon_t$, onde $\epsilon_t$ Ã© um ruÃ­do branco com mÃ©dia zero e desvio padrÃ£o 1. Usaremos o Python para calcular a matriz $\sum_{t=1}^T x_t x_t'$ e sua versÃ£o reescalonada.
```python
import numpy as np

T = 10
x = np.array([[1, t] for t in range(1, T + 1)])
XTX = np.dot(x.T, x)
print("Matriz XTX:\n", XTX)

XTX_reescalonada_T3 = XTX / T**3
print("\nMatriz XTX dividida por T^3:\n", XTX_reescalonada_T3)

```
Isso produz:
```
Matriz XTX:
 [[  10   55]
 [  55  385]]

Matriz XTX dividida por T^3:
 [[0.01  0.055]
 [0.055 0.385]]
```
Observe que, mesmo com $T=10$, os valores da matriz nÃ£o se aproximam de uma matriz nÃ£o singular, mas os elementos sÃ£o menores quando dividimos por $T^3$. Isso reforÃ§a o ponto de que Ã© necessÃ¡rio um reescalonamento adequado para que a matriz convirja para uma matriz nÃ£o singular.
### AnÃ¡lise do Reescalonamento
Para obter distribuiÃ§Ãµes assintÃ³ticas nÃ£o degeneradas, as estimativas de MQO $\hat{\alpha}_T$ e $\hat{\delta}_T$ precisam ser multiplicadas por $\sqrt{T}$ e $T^{3/2}$, respectivamente [^23]. Esse reescalonamento Ã© formalizado pela introduÃ§Ã£o da matriz $Y_T$ [^24, ^25]:
$$Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}$$
A aplicaÃ§Ã£o desse reescalonamento ao desvio da estimativa do parÃ¢metro resulta em [^26, ^27]:
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}  \begin{bmatrix} \sum_{t=1}^T \epsilon_t \\ \sum_{t=1}^T t \epsilon_t \end{bmatrix} = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}Y_T^{-1} Y_T \sum_{t=1}^T x_t \epsilon_t$$

O termo $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}Y_T^{-1}$ converge para a matriz $Q$, dada por [^28, ^29]:
$$Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$$
> ðŸ’¡ **AnÃ¡lise Detalhada do Reescalonamento**: Para entender por que o reescalonamento funciona, podemos analisar o efeito da matriz $Y_T$ sobre a matriz de covariÃ¢ncia. Primeiro, reescrevemos $Y_T$ de maneira mais explÃ­cita:
    $$ Y_T = \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{3/2} \end{bmatrix} = \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{1/2}T \end{bmatrix}$$
    A matriz $(X^T X)$ tem componentes dados por
    $$ (X^T X) = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} = \begin{bmatrix} T & \frac{T^2}{2} + \frac{T}{2} \\ \frac{T^2}{2} + \frac{T}{2} & \frac{2T^3}{6} + \frac{3T^2}{6} + \frac{T}{6} \end{bmatrix} $$
    A matriz inversa $(X^T X)^{-1}$ tem componentes de ordem $O(T^{-1})$, $O(T^{-2})$ e $O(T^{-3})$.
    Agora, considere a matriz reescalonada $Y_T(X^T X)^{-1}Y_T$. Os elementos da matriz sÃ£o:
    $$
    \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{3/2} \end{bmatrix}
     \begin{bmatrix} T^{-1} & T^{-2} \\ T^{-2} & T^{-3} \end{bmatrix}
     \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{3/2} \end{bmatrix}
     =
    \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{3/2} \end{bmatrix}
     \begin{bmatrix} T^{-1}T^{1/2} & T^{-2}T^{3/2} \\ T^{-2}T^{1/2} & T^{-3}T^{3/2} \end{bmatrix}
     =
    \begin{bmatrix} T^0 & T^0 \\ T^0 & T^0 \end{bmatrix}
    $$
    Assim, ao reescalonarmos por $Y_T$, a matriz de covariÃ¢ncia se torna uma matriz de ordem $O(1)$.

> ðŸ’¡ **Exemplo NumÃ©rico:** Continuando o exemplo anterior com $T=10$, vamos calcular a matriz $Y_T$ e aplicar a transformaÃ§Ã£o $Y_T(X^T X)^{-1}Y_T$ usando Python e numpy.
```python
import numpy as np
from numpy.linalg import inv

T = 10
x = np.array([[1, t] for t in range(1, T + 1)])
XTX = np.dot(x.T, x)
XTX_inv = inv(XTX)

YT = np.array([[np.sqrt(T), 0], [0, T**(3/2)]])
YT_inv = inv(YT)

Q_hat = np.dot(np.dot(YT, XTX_inv), YT_inv)
print("Matriz Q_hat (apÃ³s reescalonamento):\n", Q_hat)
```
Isso resulta em:
```
Matriz Q_hat (apÃ³s reescalonamento):
 [[ 1.22222222 -0.55555556]
 [-0.55555556  0.33333333]]
```
Note que a matriz $Q_{hat}$ se aproxima da matriz limite $Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$ com T=10. Quanto maior o valor de T, mais prÃ³xima $Q_{hat}$ estarÃ¡ de $Q$.
**ProposiÃ§Ã£o 1**
A matriz $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}Y_T^{-1}$ converge para a matriz $Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$.

*Prova:*
Da anÃ¡lise acima, temos que
$$Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}Y_T^{-1} = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}
     \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix}^{-1}
     \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}^{-1}$$

I. A inversa da matriz $\sum_{t=1}^T x_t x_t'$ Ã© dada por:
$$\left( \sum_{t=1}^T x_t x_t' \right)^{-1} = \frac{1}{D} \begin{bmatrix} \frac{T(T+1)(2T+1)}{6} & -\frac{T(T+1)}{2} \\ -\frac{T(T+1)}{2} & T \end{bmatrix}$$
onde $D = T\frac{T(T+1)(2T+1)}{6} - (\frac{T(T+1)}{2})^2 = \frac{T^3(T+1)}{12} (4T+2 - 3(T+1)) = \frac{T^3(T+1)(T-1)}{12} = \frac{T^3(T^2-1)}{12}$.

II. Multiplicando pela matriz $Y_T$ e $Y_T^{-1}$, temos:
$$Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}Y_T^{-1} = \frac{1}{D} \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}
     \begin{bmatrix} \frac{T(T+1)(2T+1)}{6} & -\frac{T(T+1)}{2} \\ -\frac{T(T+1)}{2} & T \end{bmatrix}
      \begin{bmatrix} \frac{1}{\sqrt{T}} & 0 \\ 0 & \frac{1}{T^{3/2}} \end{bmatrix}$$
$$=  \frac{1}{D} \begin{bmatrix} \frac{T^{1/2}T(T+1)(2T+1)}{6} & -\frac{T^{1/2}T(T+1)}{2} \\ -\frac{T^{3/2}T(T+1)}{2} & T^{3/2}T \end{bmatrix} \begin{bmatrix} \frac{1}{\sqrt{T}} & 0 \\ 0 & \frac{1}{T^{3/2}} \end{bmatrix}$$
$$= \frac{1}{D} \begin{bmatrix} \frac{T(T+1)(2T+1)}{6} & -\frac{T(T+1)}{2T^{3/2}} \\ -\frac{T^{3/2}T(T+1)}{2\sqrt{T}} & T^{3/2}T/T^{3/2} \end{bmatrix}$$
$$= \frac{1}{D} \begin{bmatrix} \frac{T(T+1)(2T+1)}{6} & -\frac{T(T+1)}{2T^{3/2}} \\ -\frac{T^{3/2}T(T+1)}{2\sqrt{T}} & T^2 \end{bmatrix}$$

III. Multiplicando por $Y_T$ e $Y_T^{-1}$, os termos da matriz se tornam:
$$
\begin{bmatrix}
\frac{T(T+1)(2T+1)}{6D}T^{1/2}T^{-1/2} & -\frac{T(T+1)}{2D}T^{1/2}T^{-3/2}  \\
-\frac{T(T+1)}{2D}T^{3/2}T^{-1/2} & \frac{T^2}{D} T^{3/2}T^{-3/2}
\end{bmatrix}
=
\begin{bmatrix}
\frac{T(T+1)(2T+1)}{6D}  & -\frac{T(T+1)}{2D}T^{-1}  \\
-\frac{T(T+1)}{2D}T  & \frac{T^2}{D}
\end{bmatrix}
$$

IV. Lembrando que $D =  \frac{T^3(T^2-1)}{12}$ e tomando o limite quando T tende ao infinito, obtemos:
$$ \begin{bmatrix} \frac{2T^3}{6\frac{T^5}{12}} & -\frac{T^2}{2\frac{T^5}{12}}T^{-1}\\ -\frac{T^2}{2\frac{T^5}{12}}T & \frac{T^2}{\frac{T^5}{12}}  \end{bmatrix}
= \begin{bmatrix} \frac{4}{T^2} & -\frac{6}{T^4} \\ -\frac{6}{T^2} & \frac{12}{T^3}  \end{bmatrix}
$$
Tomando o limite, temos
$$\lim_{T\to\infty}Y_T (X'X)^{-1} Y_T^{-1} = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$$
Portanto, $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}Y_T^{-1} \rightarrow Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$. $\blacksquare$
O termo $Y_T\sum_{t=1}^T x_t \epsilon_t$ Ã© dado por [^30]:
$$Y_T \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t \end{bmatrix} = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix}$$

Este vetor, sob condiÃ§Ãµes padrÃ£o para $\epsilon_t$, converge para uma distribuiÃ§Ã£o normal bivariada [^31]. Especificamente, o primeiro elemento converge para $N(0,\sigma^2)$ [^32], e o segundo elemento, que envolve uma sequÃªncia de diferenÃ§as de martingales, converge para $N(0, \sigma^2/3)$ [^33, ^34, ^35]. AlÃ©m disso, a covariÃ¢ncia entre os dois elementos converge para zero, garantindo que sÃ£o assintoticamente independentes [^37].

**Teorema 1**
O vetor reescalonado $Y_T(b_T - \beta)$ converge em distribuiÃ§Ã£o para uma variÃ¡vel aleatÃ³ria normal bivariada com mÃ©dia zero e matriz de covariÃ¢ncia $\sigma^2Q$.

*Prova:*
Temos que
$$ Y_T(b_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}Y_T^{-1}Y_T \sum_{t=1}^T x_t \epsilon_t$$
I. A partir da ProposiÃ§Ã£o 1, sabemos que $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}Y_T^{-1} \rightarrow Q$.
II. Do texto anterior, sabemos que $Y_T \sum_{t=1}^T x_t \epsilon_t$ converge em distribuiÃ§Ã£o para um vetor normal bivariado com mÃ©dia zero e matriz de covariÃ¢ncia $\sigma^2 \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$.
III. Portanto, usando o teorema de Slutsky, concluÃ­mos que:
$$Y_T(b_T - \beta) \overset{d}{\longrightarrow} N(0, \sigma^2Q)$$
â– 
> ðŸ’¡ **Exemplo NumÃ©rico:** Para concretizar o Teorema 1, vamos gerar uma sÃ©rie temporal com $T=100$ seguindo o modelo $y_t = 2 + 0.5t + \epsilon_t$, onde $\epsilon_t$ Ã© um ruÃ­do branco com desvio padrÃ£o 1. Em seguida, vamos simular o processo de estimaÃ§Ã£o e calcular os erros reescalonados.
```python
import numpy as np
import pandas as pd
from numpy.linalg import inv

np.random.seed(42) # para reproducibilidade

T = 100
alpha_true = 2
delta_true = 0.5
sigma = 1

t = np.arange(1, T + 1)
x = np.array([[1, ti] for ti in t])
epsilon = np.random.normal(0, sigma, T)
y = alpha_true + delta_true * t + epsilon

XTX = np.dot(x.T, x)
XTX_inv = inv(XTX)
XTY = np.dot(x.T, y)
beta_hat = np.dot(XTX_inv, XTY)

alpha_hat = beta_hat[0]
delta_hat = beta_hat[1]

YT = np.array([[np.sqrt(T), 0], [0, T**(3/2)]])
beta_error = np.array([alpha_hat-alpha_true, delta_hat - delta_true])

YT_beta_error = np.dot(YT, beta_error)
print("Erro reescalonado para alpha:", YT_beta_error[0])
print("Erro reescalonado para delta:", YT_beta_error[1])

```
Isso produz:
```
Erro reescalonado para alpha: -0.6636744905452705
Erro reescalonado para delta: 0.21825757280460827
```
Os valores obtidos representam uma simulaÃ§Ã£o de um possÃ­vel resultado do termo reescalonado $Y_T(b_T - \beta)$. O teorema afirma que para $T \to \infty$, esses erros reescalonados convergem para uma distribuiÃ§Ã£o normal bivariada, com mÃ©dia zero. Em uma Ãºnica simulaÃ§Ã£o, nÃ£o observamos a convergÃªncia, mas ao repetir este experimento diversas vezes e com valores maiores de T, verÃ­amos as distribuiÃ§Ãµes de amostras dos erros se aproximarem de uma normal com mÃ©dia zero e covariÃ¢ncia $\sigma^2 Q$.
Em resumo, o reescalonamento com $Y_T$ assegura que as estimativas tenham uma distribuiÃ§Ã£o assintÃ³tica bem definida.  O termo $\sqrt{T}(\hat{\alpha}_T - \alpha)$ converge para uma distribuiÃ§Ã£o normal, indicando que a estimativa de $\alpha$ precisa ser reescalonada pela raiz quadrada de $T$ para se obter um limite nÃ£o degenerado. Por outro lado, o termo $T^{3/2}(\hat{\delta}_T - \delta)$ tambÃ©m converge para uma distribuiÃ§Ã£o normal, implicando que a estimativa de $\delta$ precisa ser reescalonada por $T^{3/2}$.

### ConclusÃ£o
Esta seÃ§Ã£o forneceu uma anÃ¡lise detalhada do processo de reescalonamento necessÃ¡rio para obter distribuiÃ§Ãµes assintÃ³ticas nÃ£o degeneradas das estimativas de MQO em modelos com tendÃªncia temporal determinÃ­stica. Foi demonstrado como a divergÃªncia da matriz de covariÃ¢ncia $\sum_{t=1}^T x_t x_t'$ exige o reescalonamento por meio da matriz $Y_T$, envolvendo diferentes taxas de convergÃªncia para os parÃ¢metros $\alpha$ e $\delta$. O reescalonamento permite que o vetor de estimativas redimensionadas convirja para uma distribuiÃ§Ã£o normal bivariada, onde a matriz $Q$ e o termo envolvendo $\epsilon_t$ desempenham um papel crucial. As somas de potÃªncias de 't', com termos lÃ­deres $T^2/2$ e $T^3/3$, reforÃ§am a necessidade de reescalonamento para distribuiÃ§Ãµes limitantes nÃ£o degeneradas. A necessidade de reescalonar os estimadores MQO destaca as diferenÃ§as fundamentais entre regressÃµes com variÃ¡veis estacionÃ¡rias e aquelas com tendÃªncias temporais determinÃ­sticas.

### ReferÃªncias
[^1]:  *Os coeficientes de modelos de regressÃ£o envolvendo raÃ­zes unitÃ¡rias ou tendÃªncias temporais determinÃ­sticas sÃ£o tipicamente estimados por mÃ­nimos quadrados ordinÃ¡rios.*
[^2]:  *No entanto, as distribuiÃ§Ãµes assintÃ³ticas das estimativas dos coeficientes nÃ£o podem ser calculadas da mesma forma que aquelas para modelos de regressÃ£o envolvendo variÃ¡veis estacionÃ¡rias.*
[^3]: *This section considers OLS estimation of the parameters of a simple time trend, $y_t = \alpha + \delta t + \epsilon_t$, for $\epsilon_t$ a white noise process. If $\epsilon_t \sim N(0, \sigma^2)$, then the model [16.1.1] satisfies the classical regression assumptions...*
[^4]: *Write [16.1.1] in the form of the standard regression model, $y_t = x_t'\beta + \epsilon_t$*
[^5]: *where $x_t = [1  \quad t]'$*
[^6]: *$\beta = [\alpha  \quad \delta]'$*
[^7]: *Let $b_T$ denote the OLS estimate of $\beta$ based on a sample of size $T$: $b_T = [\alpha_T  \quad \delta_T]' = (\sum x_t x_t')^{-1} \sum x_t y_t$*
[^8]: *Recall from equation [8.2.3] that the deviation of the OLS estimate from the true value can be expressed as $(b_T - \beta) = (\sum x_t x_t')^{-1} \sum x_t \epsilon_t$*
[^9]: *$(b_T - \beta) = (\sum x_t x_t')^{-1} \sum x_t \epsilon_t$*
[^10]: *To find the limiting distribution for a regression with stationary explanatory variables, the approach in Chapter 8 was to multiply [16.1.6] by $\sqrt{T}$, resulting in $\sqrt{T}(b_T - \beta) = [(1/T) \sum x_t x_t']^{-1} [(1/\sqrt{T}) \sum x_t \epsilon_t]$*
[^11]: *The usual assumption was that $(1/T) \sum x_t x_t'$ converged in probability to a nonsingular matrix $Q$ while $(1/\sqrt{T}) \sum x_t \epsilon_t$ converged in distribution to a $N(0, \sigma^2Q)$ random variable, implying that $\sqrt{T}(b_T - \beta) -> N(0, \sigma^2Q^{-1})$.*
[^12]: *...expression [16.1.6] would be $[(\alpha_T - \alpha)  \quad (\delta_T - \delta)]' = [\sum 1  \quad \sum t ; \sum t  \quad \sum t^2]^{-1} [\sum \epsilon_t ; \sum t\epsilon_t]$*
[^13]: *$[(\alpha_T - \alpha)  \quad (\delta_T - \delta)]' = [\sum 1  \quad \sum t ; \sum t  \quad \sum t^2]^{-1} [\sum \epsilon_t ; \sum t\epsilon_t]$*
[^14]: *It is straightforward to show by induction that $\sum t = T(T+1)/2$*
[^15]: *$\sum t^2 = T(T+1)(2T+1)/6.$*
[^16]: *Thus, the leading term in $\sum t$ is $T^2/2$; that is, $(1/T^2) \sum t = 1/2 + 1/(2T) \rightarrow 1/2$*
[^17]: *Similarly, the leading term in $\sum t^2$ is $T^3/3$: $(1/T^3) \sum t^2 = 1/3 + 1/(2T) + 1/(6T^2) \rightarrow 1/3.*
[^18]: *For future reference, we note here the general pattern-the leading term in $\sum t^v$ is $T^{v+1}/(v+1)$: $(1/T^{v+1}) \sum t^v \rightarrow 1/(v+1)$*
[^19]: *For $x$, given in [16.1.3], results [16.1.9] and [16.1.10] imply that $\sum x_t x_t' = [\sum 1  \quad \sum t ; \sum t  \quad \sum t^2] = [T  \quad T(T+1)/2 ;  T(T+1)/2  \quad T(T+1)(2T+1)/6]$*
[^20]: *$\sum x_t x_t' = [\sum 1  \quad \sum t ; \sum t  \quad \sum t^2] = [T  \quad T(T+1)/2 ;  T(T+1)/2  \quad T(T+1)(2T+1)/6]$*
[^21]: *In contrast to the usual result for stationary regressions, for the matrix in [16.1.16], $(1/T) \sum x_t x_t'$ diverges. To obtain a convergent matrix, [16.1.16] would have to be divided by $T^3$ rather than $T$.*
[^22]: *Unfortunately, this limiting matrix cannot be inverted, as $(1/T) \sum x_t x_t'$ can be in the usual case. Hence, a different approach from that in the stationary case will be needed to calculate the asymptotic distribution of $b_T$.*
[^23]: *It turns out that the OLS estimates $\alpha_T$ and $\delta_T$ have different asymptotic rates of convergence. To arrive at nondegenerate limiting distributions, $\alpha_T$ is multiplied by $\sqrt{T}$, whereas $\delta_T$ must be multiplied by $T^{3/2}$.*
[^24]: *We can think of this adjustment as premultiplying [16.1.6] or [16.1.8] by the matrix $Y_T = [\sqrt{T}  \quad 0 ; 0  \quad T^{3/2}]$*
[^25]: *$Y_T = [\sqrt{T}  \quad 0 ; 0  \quad T^{3/2}]$*
[^26]: *resulting in $[\sqrt{T}(\alpha_T - \alpha)  \quad ; T^{3/2}(\delta_T - \delta)]' = Y_T (\sum x_t x_t')^{-1}  Y_T^{-1} [ (1/\sqrt{T}) \sum x_t \epsilon_t]$*
[^27]: *$[\sqrt{T}(\alpha_T - \alpha)  \quad ; T^{3/2}(\delta_T - \delta)]' = Y_T (\sum x_t x_t')^{-1}  Y_T^{-1} [ (1/\sqrt{T}) \sum x_t \epsilon_t]$*
[^28]: *Consider the first term in the last expression of [16.1.18]. Substituting from [16.1.17] and [16.1.16], ${Y_T (\sum x_t x_t')^{-1} Y_T^{-1}} = [T^{-1/2}  \quad 0 ; 0  \quad T^{-3/2}]  [ \sum 1  \quad \sum t ; \sum t  \quad \sum t^2 ]^{-1} [T^{-1/2} \quad 0 ; 0 \quad T^{-3/2}] = [T^{-1/2}  \quad 0 ; 0  \quad T^{-3/2}] [T  \quad T(T+1)/2 ; T(T+1)/2  \quad T(T+1)(2T+1)/6 ]^{-1} [T^{-1/2} \quad 0 ; 0 \quad T^{-3/2}]$*
[^29]: *Thus, it follows from [16.1.11] and [16.1.12] that $\Sigma$ is positive definite*
## 16.2 The Wald Test

The Wald test provides an alternative to the likelihood ratio test for testing hypotheses about the parameters of a model. It's particularly useful when the maximum likelihood estimator (MLE) is known and asymptotically normal but the likelihood function itself is difficult to maximize under constraints.

### 16.2.1 Basic Wald Statistic

Consider testing the null hypothesis $H_0 : \mathbf{r}(\boldsymbol{\theta}) = \mathbf{0}$ against the alternative $H_1 : \mathbf{r}(\boldsymbol{\theta}) \neq \mathbf{0}$, where $\mathbf{r}(\boldsymbol{\theta})$ is a $q \times 1$ vector of continuously differentiable functions of the $p \times 1$ parameter vector $\boldsymbol{\theta}$. Let $\hat{\boldsymbol{\theta}}$ denote the MLE of $\boldsymbol{\theta}$.

The Wald test statistic is defined as:

$$W = \mathbf{r}(\hat{\boldsymbol{\theta}})' \left[ \frac{\partial \mathbf{r}(\hat{\boldsymbol{\theta}})}{\partial \boldsymbol{\theta}'} \hat{\Sigma}_\theta \left( \frac{\partial \mathbf{r}(\hat{\boldsymbol{\theta}})}{\partial \boldsymbol{\theta}'} \right)' \right]^{-1} \mathbf{r}(\hat{\boldsymbol{\theta}})$$

where $\hat{\Sigma}_\theta$ is an estimate of the asymptotic covariance matrix of $\hat{\boldsymbol{\theta}}$, often the inverse of the observed or expected Fisher information matrix. The term $\frac{\partial \mathbf{r}(\hat{\boldsymbol{\theta}})}{\partial \boldsymbol{\theta}'}$ is the $q \times p$ matrix of partial derivatives of $\mathbf{r}(\boldsymbol{\theta})$ with respect to $\boldsymbol{\theta}'$, evaluated at $\hat{\boldsymbol{\theta}}$.

Under $H_0$ and certain regularity conditions, the Wald statistic $W$ is asymptotically distributed as a chi-squared distribution with $q$ degrees of freedom, denoted as $\chi^2(q)$.

### 16.2.2 Constructing a Wald Test

To conduct a Wald test:

1.  **Estimate** the parameters $\boldsymbol{\theta}$ to obtain $\hat{\boldsymbol{\theta}}$ (e.g., using MLE).
2.  **Compute** $\mathbf{r}(\hat{\boldsymbol{\theta}})$, which should be close to $\mathbf{0}$ if the null hypothesis is true.
3.  **Calculate** the gradient matrix $\frac{\partial \mathbf{r}(\hat{\boldsymbol{\theta}})}{\partial \boldsymbol{\theta}'}$.
4.  **Obtain** an estimate $\hat{\Sigma}_\theta$ of the asymptotic covariance matrix of $\hat{\boldsymbol{\theta}}$.
5.  **Compute** the Wald statistic $W$.
6.  **Compare** the calculated $W$ with the critical value from the $\chi^2(q)$ distribution at a chosen significance level $\alpha$.
7.  **Reject** $H_0$ if $W$ exceeds the critical value. Otherwise, fail to reject $H_0$.

### 16.2.3 Example

Suppose we want to test if a specific linear combination of regression coefficients is equal to a particular value. Let our linear model be $y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i$.

The null hypothesis is $H_0: \beta_1 + \beta_2 = 1$. Here, $\mathbf{r}(\boldsymbol{\theta}) = \beta_1 + \beta_2 - 1$, so $q = 1$. The parameter vector is $\boldsymbol{\theta} = [\beta_0, \beta_1, \beta_2]'$.

We estimate the parameters using OLS to get $\hat{\boldsymbol{\theta}} = [\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2]'$.
$\mathbf{r}(\hat{\boldsymbol{\theta}}) = \hat{\beta}_1 + \hat{\beta}_2 - 1$
$\frac{\partial \mathbf{r}(\hat{\boldsymbol{\theta}})}{\partial \boldsymbol{\theta}'} = [0, 1, 1]$
$\hat{\Sigma}_\theta$ is the estimated covariance matrix of the estimated OLS parameters.

Using these values, we calculate $W$ and compare it to a $\chi^2(1)$ distribution.

### 16.2.4 Properties

-   **Asymptotic Validity:** The Wald test is asymptotically valid, meaning its size (Type I error) converges to the nominal level as the sample size grows.
-   **Computational Ease:** It is computationally simple to perform when the MLE is readily available.
-   **Invariance:** The test is invariant under reparameterization of the model.

### 16.2.5 Caveats

-   **Finite Sample Performance:** Wald tests can perform poorly in small samples.
-   **Monotonicity:** The Wald test statistic is not guaranteed to be monotonic in the likelihood.
-   **Choice of Estimator:** The behavior of the Wald test can depend on the specific method used to estimate $\hat{\Sigma}_\theta$. Different estimators for the covariance matrix can lead to significantly different results, particularly in small samples.
-   **Non-uniqueness of $\mathbf{r}(\boldsymbol{\theta})$:** If $\mathbf{r}(\boldsymbol{\theta})$ is not unique, the test outcome can depend on the chosen form.
-   **Alternatives:** In some situations, the likelihood ratio test or score test might be preferred over the Wald test, especially if the likelihood function is readily available.
<!-- END -->
