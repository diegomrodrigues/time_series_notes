## AnÃ¡lise dos Termos Dominantes nas Somas de PotÃªncias de $t$ e Suas ConvergÃªncias

### IntroduÃ§Ã£o
Neste capÃ­tulo, estamos explorando os modelos de regressÃ£o com tendÃªncias temporais determinÃ­sticas, e como a matriz de rescalonamento $Y_T$ desempenha um papel crucial na obtenÃ§Ã£o de distribuiÃ§Ãµes assintÃ³ticas bem definidas para os estimadores OLS [^1]. Na seÃ§Ã£o anterior, derivamos a necessidade de rescalonar a matriz de covariÃ¢ncia $\sum_{t=1}^T x_t x_t'$ para lidar com suas taxas de divergÃªncia. Esta seÃ§Ã£o se concentrarÃ¡ nos termos dominantes nas somas de potÃªncias de $t$, especificamente $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$, e demonstraremos como esses termos, quando normalizados, convergem para valores especÃ­ficos [^3]. AlÃ©m disso, generalizaremos esse resultado para $\sum_{t=1}^T t^v$, onde $v$ Ã© um nÃºmero inteiro positivo, estabelecendo um padrÃ£o para convergÃªncia das somas normalizadas de potÃªncias de $t$ [^3].

### Conceitos Fundamentais
No modelo de regressÃ£o com tendÃªncia de tempo determinÃ­stica $y_t = \alpha + \delta t + \epsilon_t$, as matrizes que surgem nos cÃ¡lculos de OLS envolvem as somas de potÃªncias de $t$, isto Ã©, $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$. Conforme discutido anteriormente, as fÃ³rmulas fechadas para estas somas sÃ£o dadas por:

$$ \sum_{t=1}^T t = \frac{T(T+1)}{2} $$

$$ \sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6} $$

O objetivo desta seÃ§Ã£o Ã© analisar os termos dominantes nestas somas, ou seja, os termos que crescem mais rapidamente quando $T$ tende ao infinito, e como normalizar essas somas para obter uma convergÃªncia para valores finitos [^3].

#### O Termo Dominante em $\sum_{t=1}^T t$ e sua ConvergÃªncia
Analisando a expressÃ£o $\sum_{t=1}^T t = \frac{T(T+1)}{2} = \frac{T^2 + T}{2}$, o termo dominante, ou seja, aquele que cresce mais rapidamente quando $T$ tende ao infinito, Ã© $\frac{T^2}{2}$. Ao normalizarmos a soma $\sum_{t=1}^T t$ por $T^2$, obtemos:

$$ \frac{1}{T^2} \sum_{t=1}^T t = \frac{1}{T^2} \frac{T(T+1)}{2} = \frac{T^2 + T}{2T^2} = \frac{1}{2} + \frac{1}{2T} $$

Quando $T$ tende ao infinito, o termo $\frac{1}{2T}$ tende a zero, e a expressÃ£o converge para $\frac{1}{2}$. Assim, podemos afirmar que o termo dominante em $\sum_{t=1}^T t$ Ã© $T^2/2$, e que a soma normalizada $\frac{1}{T^2} \sum_{t=1}^T t$ converge para $1/2$ [^3]:

$$ \frac{1}{T^2} \sum_{t=1}^T t \xrightarrow{T\rightarrow \infty} \frac{1}{2} $$
> ðŸ’¡ **Exemplo NumÃ©rico:** Considere os valores de T=10, 100, e 1000:
>
> $$
> \frac{1}{10^2}\sum_{t=1}^{10} t = \frac{1}{100}\frac{10(11)}{2} = \frac{55}{100} = 0.55
> $$
> $$
> \frac{1}{100^2}\sum_{t=1}^{100} t = \frac{1}{10000}\frac{100(101)}{2} = \frac{5050}{10000} = 0.505
> $$
> $$
> \frac{1}{1000^2}\sum_{t=1}^{1000} t = \frac{1}{1000000}\frac{1000(1001)}{2} = \frac{500500}{1000000} = 0.5005
> $$
>
> Note que, conforme T aumenta, a normalizaÃ§Ã£o $\frac{1}{T^2}\sum_{t=1}^T t$ se aproxima de 0.5, conforme demonstrado na teoria.
>
> ```python
> import numpy as np
>
> T_values = [10, 100, 1000]
>
> for T in T_values:
>     sum_t = np.sum(np.arange(1, T + 1))
>     normalized_sum = sum_t / T**2
>     print(f"T = {T}, Normalized Sum = {normalized_sum:.4f}")
> ```
>
> Este cÃ³digo Python calcula a soma dos inteiros de 1 a T, normaliza por $T^2$, e imprime o resultado para diferentes valores de T. Isso confirma que a normalizaÃ§Ã£o converge para 0.5 conforme T aumenta.

#### O Termo Dominante em $\sum_{t=1}^T t^2$ e sua ConvergÃªncia
Analisando a expressÃ£o $\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6} = \frac{2T^3 + 3T^2 + T}{6}$, o termo dominante Ã© $\frac{2T^3}{6} = \frac{T^3}{3}$. Ao normalizarmos a soma $\sum_{t=1}^T t^2$ por $T^3$, obtemos:

$$ \frac{1}{T^3} \sum_{t=1}^T t^2 = \frac{1}{T^3} \frac{T(T+1)(2T+1)}{6} = \frac{2T^3 + 3T^2 + T}{6T^3} = \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2} $$

Quando $T$ tende ao infinito, os termos $\frac{1}{2T}$ e $\frac{1}{6T^2}$ tendem a zero, e a expressÃ£o converge para $\frac{1}{3}$. Assim, podemos afirmar que o termo dominante em $\sum_{t=1}^T t^2$ Ã© $T^3/3$, e que a soma normalizada $\frac{1}{T^3} \sum_{t=1}^T t^2$ converge para $1/3$:

$$ \frac{1}{T^3} \sum_{t=1}^T t^2 \xrightarrow{T\rightarrow \infty} \frac{1}{3} $$
> ðŸ’¡ **Exemplo NumÃ©rico:** Considere os valores de T=10, 100 e 1000:
>
> $$
> \frac{1}{10^3}\sum_{t=1}^{10} t^2 = \frac{1}{1000}\frac{10(11)(21)}{6} = \frac{385}{1000} = 0.385
> $$
> $$
> \frac{1}{100^3}\sum_{t=1}^{100} t^2 = \frac{1}{1000000}\frac{100(101)(201)}{6} = \frac{338350}{1000000} = 0.33835
> $$
> $$
> \frac{1}{1000^3}\sum_{t=1}^{1000} t^2 = \frac{1}{1000000000}\frac{1000(1001)(2001)}{6} = \frac{333833500}{1000000000} = 0.3338335
> $$
> Observe que conforme T aumenta, a normalizaÃ§Ã£o $\frac{1}{T^3}\sum_{t=1}^T t^2$ se aproxima de 0.333, o valor esperado pelo resultado teÃ³rico.
>
> ```python
> import numpy as np
>
> T_values = [10, 100, 1000]
>
> for T in T_values:
>     sum_t_squared = np.sum(np.arange(1, T + 1)**2)
>     normalized_sum = sum_t_squared / T**3
>     print(f"T = {T}, Normalized Sum = {normalized_sum:.5f}")
> ```
> Este cÃ³digo Python calcula a soma dos quadrados dos inteiros de 1 a T, normaliza por $T^3$, e imprime o resultado para diferentes valores de T, demonstrando a convergÃªncia para 1/3.

**Lema 1:** Para qualquer inteiro positivo $v$, a soma $\sum_{t=1}^T t^v$ pode ser expressa como um polinÃ´mio em $T$ de grau $v+1$.

*Prova:*

Este resultado pode ser demonstrado por induÃ§Ã£o.
I. O caso base, $v=1$, Ã© dado por $\sum_{t=1}^T t = \frac{T(T+1)}{2} = \frac{1}{2}T^2 + \frac{1}{2}T$, que Ã© um polinÃ´mio de grau 2.
II. Suponha que a afirmaÃ§Ã£o seja verdadeira para $v=k$, isto Ã©, $\sum_{t=1}^T t^k$ Ã© um polinÃ´mio de grau $k+1$.
III. Consideremos entÃ£o o caso $v=k+1$. Usaremos a identidade
$$ \sum_{t=1}^T ( (t+1)^{k+2} - t^{k+2}) = (T+1)^{k+2} - 1 $$
IV. Expandindo o lado esquerdo utilizando o binÃ´mio de Newton, obtemos:
$$ \sum_{t=1}^T \left( \sum_{j=0}^{k+1} \binom{k+2}{j} t^j + t^{k+2} - t^{k+2} \right) = \sum_{j=0}^{k+1} \binom{k+2}{j} \sum_{t=1}^T t^j = (T+1)^{k+2} - 1 $$
V. Assim,
$$ \sum_{t=1}^T t^{k+1} = \frac{1}{\binom{k+2}{k+1}} \left( (T+1)^{k+2} - 1 - \sum_{j=0}^{k} \binom{k+2}{j} \sum_{t=1}^T t^j \right) $$
VI. Pela hipÃ³tese de induÃ§Ã£o, $\sum_{t=1}^T t^j$ Ã© um polinÃ´mio em $T$ de grau $j+1$ para $j \leq k$, o que implica que o lado direito da equaÃ§Ã£o Ã© um polinÃ´mio em $T$ de grau $k+2$, o que conclui a prova do lema.
â– 

**CorolÃ¡rio 1.1:** O termo de maior grau no polinÃ´mio de $\sum_{t=1}^T t^v$ Ã© da forma $c_v T^{v+1}$, onde $c_v$ Ã© uma constante que depende de $v$.
*Prova:*
O CorolÃ¡rio segue diretamente do Lema 1, pois o termo de maior grau em um polinÃ´mio da forma $a_0 + a_1 T + a_2 T^2 + \ldots + a_{v+1}T^{v+1}$ Ã© o termo $a_{v+1}T^{v+1}$, onde $a_{v+1}$ Ã© uma constante, que no caso da soma das potencias de $t$, pode depender de $v$.
â– 

#### GeneralizaÃ§Ã£o para $\sum_{t=1}^T t^v$ e sua ConvergÃªncia
Podemos generalizar este resultado para qualquer potÃªncia inteira positiva $v$. A fÃ³rmula para a soma $\sum_{t=1}^T t^v$ Ã© um polinÃ´mio de grau $v+1$ em $T$. O termo dominante neste polinÃ´mio serÃ¡ da forma $\frac{T^{v+1}}{v+1}$. Portanto, ao normalizarmos a soma $\sum_{t=1}^T t^v$ por $T^{v+1}$, obtemos:
$$ \frac{1}{T^{v+1}} \sum_{t=1}^T t^v $$
Este termo converge para $\frac{1}{v+1}$ quando $T$ tende ao infinito:

$$ \frac{1}{T^{v+1}} \sum_{t=1}^T t^v \xrightarrow{T\rightarrow \infty} \frac{1}{v+1} $$
Este resultado Ã© chave, pois estabelece um padrÃ£o para a convergÃªncia das somas normalizadas de potÃªncias de $t$, que Ã© fundamental para derivar os resultados assintÃ³ticos em modelos de regressÃ£o com tendÃªncias de tempo determinÃ­sticas.

**ProposiÃ§Ã£o 7:** O termo dominante em $\sum_{t=1}^T t^v$, onde $v$ Ã© um inteiro positivo, Ã© $\frac{T^{v+1}}{v+1}$. AlÃ©m disso, a soma normalizada $\frac{1}{T^{v+1}} \sum_{t=1}^T t^v$ converge para $\frac{1}{v+1}$ quando $T$ tende ao infinito.
*Prova:*
I. A fÃ³rmula para $\sum_{t=1}^T t^v$ Ã© um polinÃ´mio de grau $v+1$ em $T$, com o termo de maior grau sendo da forma $cT^{v+1}$, onde $c$ Ã© uma constante.
II. Usando a aproximaÃ§Ã£o de soma por integrais, podemos ver que
$$ \sum_{t=1}^T t^v \approx \int_1^T x^v dx = \frac{x^{v+1}}{v+1} \bigg|_1^T = \frac{T^{v+1} - 1}{v+1} $$
III. O termo dominante nesta expressÃ£o Ã© $\frac{T^{v+1}}{v+1}$.
IV. Assim, ao normalizar a soma por $T^{v+1}$, obtemos:
$$ \frac{1}{T^{v+1}} \sum_{t=1}^T t^v \approx \frac{1}{T^{v+1}} \frac{T^{v+1} - 1}{v+1} = \frac{1}{v+1} - \frac{1}{(v+1)T^{v+1}} $$
V. Quando $T$ tende ao infinito, o termo $\frac{1}{(v+1)T^{v+1}}$ tende a zero, e a expressÃ£o converge para $\frac{1}{v+1}$:
$$ \frac{1}{T^{v+1}} \sum_{t=1}^T t^v \xrightarrow{T \to \infty} \frac{1}{v+1} $$
Portanto, o termo dominante em $\sum_{t=1}^T t^v$ Ã© $\frac{T^{v+1}}{v+1}$, e a soma normalizada $\frac{1}{T^{v+1}} \sum_{t=1}^T t^v$ converge para $\frac{1}{v+1}$ quando $T$ tende ao infinito.
â– 
Este resultado generaliza as anÃ¡lises anteriores e Ã© fundamental para a compreensÃ£o das propriedades assintÃ³ticas dos estimadores OLS em modelos com tendÃªncias de tempo determinÃ­sticas.

> ðŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar a ProposiÃ§Ã£o 7, vamos analisar $\sum_{t=1}^T t^3$.
>
> A fÃ³rmula para $\sum_{t=1}^T t^3$ Ã© dada por $\left(\frac{T(T+1)}{2}\right)^2 = \frac{T^4 + 2T^3 + T^2}{4}$. O termo dominante Ã© $\frac{T^4}{4}$. Ao normalizarmos $\sum_{t=1}^T t^3$ por $T^4$, obtemos:
> $$ \frac{1}{T^4}\sum_{t=1}^T t^3 = \frac{T^4 + 2T^3 + T^2}{4T^4} = \frac{1}{4} + \frac{1}{2T} + \frac{1}{4T^2} $$
> Quando $T$ tende ao infinito, os termos $\frac{1}{2T}$ e $\frac{1}{4T^2}$ tendem a zero, e a expressÃ£o converge para $\frac{1}{4}$. Portanto, de acordo com a ProposiÃ§Ã£o 7 (com $v=3$), $\frac{1}{T^4} \sum_{t=1}^T t^3$ converge para $\frac{1}{3+1}=\frac{1}{4}$.
>
> Este exemplo numÃ©rico demonstra que o resultado teÃ³rico se mantÃ©m para $v=3$.
>
> ```python
> import numpy as np
>
> def calculate_normalized_sum(T, v):
>     t = np.arange(1, T + 1)
>     sum_tv = np.sum(t**v)
>     return sum_tv / T**(v + 1)
>
> v_values = [1, 2, 3]
> T_values = [10, 100, 1000, 10000]
>
> for v in v_values:
>     print(f"v = {v}")
>     for T in T_values:
>         result = calculate_normalized_sum(T, v)
>         print(f"  T = {T}: {result:.6f}, Expected = {1/(v+1):.6f}")
>
> ```
> Este cÃ³digo python calcula a soma normalizada para diferentes valores de $v$ e $T$, mostrando que elas convergem para os valores teÃ³ricos, confirmando a ProposiÃ§Ã£o 7.

**ObservaÃ§Ã£o 1:** A ProposiÃ§Ã£o 7 pode ser reescrita usando a notaÃ§Ã£o de "O Grande" (Big O notation). Especificamente, podemos dizer que $\sum_{t=1}^T t^v = O(T^{v+1})$. Esta notaÃ§Ã£o significa que existe uma constante $M$ tal que para $T$ suficientemente grande, $|\sum_{t=1}^T t^v| \leq M T^{v+1}$. Equivalentemente, podemos escrever que $\sum_{t=1}^T t^v = \frac{T^{v+1}}{v+1} + o(T^{v+1})$, onde $o(T^{v+1})$ representa um termo que cresce mais lentamente do que $T^{v+1}$ quando $T$ tende ao infinito.

### ConclusÃ£o
Nesta seÃ§Ã£o, analisamos os termos dominantes nas somas de potÃªncias de $t$, $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$, e demonstramos como normalizar essas somas para que convirjam para valores especÃ­ficos [^3]. O termo dominante em $\sum_{t=1}^T t$ Ã© $\frac{T^2}{2}$, e a soma normalizada $\frac{1}{T^2} \sum_{t=1}^T t$ converge para $1/2$, enquanto o termo dominante em $\sum_{t=1}^T t^2$ Ã© $\frac{T^3}{3}$, e a soma normalizada $\frac{1}{T^3} \sum_{t=1}^T t^2$ converge para $1/3$. Generalizamos este resultado, demonstrando que o termo dominante em $\sum_{t=1}^T t^v$ Ã© $\frac{T^{v+1}}{v+1}$, e que a soma normalizada $\frac{1}{T^{v+1}} \sum_{t=1}^T t^v$ converge para $\frac{1}{v+1}$ quando $T$ tende ao infinito. Esses resultados sÃ£o cruciais para entender as propriedades assintÃ³ticas dos estimadores OLS em modelos com tendÃªncias de tempo determinÃ­sticas e sÃ£o essenciais para a derivaÃ§Ã£o das distribuiÃ§Ãµes assintÃ³ticas que utilizamos para inferÃªncia estatÃ­stica.
### ReferÃªncias
[^1]: Rescaling OLS Estimates with Matrix $Y_T$.
[^2]: From [16.1.19] and [16.1.24], the asymptotic distribution of [16.1.18] can be calculated as in Example 7.5 of Chapter 7.
[^3]: where $\sum$ denotes summation for t = 1 through T.
<!-- END -->
