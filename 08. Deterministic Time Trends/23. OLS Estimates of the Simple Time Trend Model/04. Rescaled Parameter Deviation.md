## ConvergÃªncia da Matriz de CovariÃ¢ncia Rescalonada e AplicaÃ§Ãµes Inferenciais

### IntroduÃ§Ã£o
Dando continuidade Ã  nossa anÃ¡lise sobre os modelos de regressÃ£o com tendÃªncias de tempo determinÃ­sticas e utilizando a matriz de rescalonamento $Y_T$ [^1], esta seÃ§Ã£o se concentra no primeiro termo da expressÃ£o que define a distribuiÃ§Ã£o assintÃ³tica dos estimadores OLS rescalonados. Este termo envolve o produto matricial de $Y_T$, a soma $\sum_{t=1}^T x_t x_t'$, e $Y_T^{-1}$. Demonstraremos que este produto converge para uma matriz $Q$, cujos elementos sÃ£o definidos pelas fraÃ§Ãµes 1/2 e 1/3 [^1]. Este resultado Ã© fundamental para o desenvolvimento de intervalos de confianÃ§a e testes de hipÃ³teses.

### Conceitos Fundamentais
Relembrando, o modelo de regressÃ£o com tendÃªncia de tempo determinÃ­stica Ã© dado por:

$$ y_t = \alpha + \delta t + \epsilon_t $$
onde $\epsilon_t$ Ã© um ruÃ­do branco com mÃ©dia zero e variÃ¢ncia $\sigma^2$. O vetor de estimadores OLS Ã© $\hat{\beta}_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix}$. O desvio entre o estimador OLS e o verdadeiro valor de $\beta = \begin{bmatrix} \alpha \\ \delta \end{bmatrix}$ pode ser expresso como:

$$ (\hat{\beta}_T - \beta) = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t $$

Para lidar com as diferentes taxas de convergÃªncia dos estimadores, introduzimos a matriz de rescalonamento:
$$ Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} $$

O vetor de estimadores OLS rescalonados Ã© dado por:

$$ Y_T (\hat{\beta}_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t = \left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right]  Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$$

O primeiro termo, que Ã© o foco desta seÃ§Ã£o, Ã©:
$$ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T $$

**ProposiÃ§Ã£o 6:** O termo $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T$ converge em probabilidade para a matriz $Q^{-1}$ quando $T$ tende ao infinito, onde $Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$, ou seja,
$$ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \xrightarrow{p} Q^{-1} $$
*Prova:*
I.  Primeiramente, vamos analisar a matriz $\sum_{t=1}^T x_t x_t'$, que Ã© dada por:
$$ \sum_{t=1}^T x_t x_t' = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix} = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} $$
II. Multiplicando a matriz $\sum_{t=1}^T x_t x_t'$ por $Y_T^{-1}$ por ambos os lados, temos:
$$ Y_T^{-1} \left( \sum_{t=1}^T x_t x_t' \right) Y_T^{-1} =
\begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix}
\begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix}
\begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix}
$$

III. Calculando o produto matricial, obtemos:
$$ = \begin{bmatrix} T^{-1}T & T^{-1}\frac{T(T+1)}{2} \\ T^{-3/2}\frac{T(T+1)}{2} & T^{-3}\frac{T(T+1)(2T+1)}{6} \end{bmatrix}
\begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix}
= \begin{bmatrix} 1 & \frac{T+1}{2T} \\ \frac{T+1}{2T} & \frac{(T+1)(2T+1)}{6T^2} \end{bmatrix}
$$
$$ =
\begin{bmatrix} 1 & \frac{1}{2} + \frac{1}{2T} \\ \frac{1}{2} + \frac{1}{2T} & \frac{2T^2 + 3T + 1}{6T^2} \end{bmatrix} =
\begin{bmatrix} 1 & \frac{1}{2} + \frac{1}{2T} \\ \frac{1}{2} + \frac{1}{2T} & \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2} \end{bmatrix}
$$
IV.  Conforme $T$ tende ao infinito, os termos $\frac{1}{2T}$, $\frac{1}{2T}$ e $\frac{1}{6T^2}$ tendem a zero, e a expressÃ£o converge para:
$$ Y_T^{-1} \left( \sum_{t=1}^T x_t x_t' \right) Y_T^{-1} \xrightarrow{p} \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix} = Q $$
V. Invertendo ambos os lados, obtemos:
$$ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \xrightarrow{p} Q^{-1} $$
Portanto, a proposiÃ§Ã£o estÃ¡ demonstrada.
â– 

> ğŸ’¡ **Exemplo NumÃ©rico:** Vamos calcular essa convergÃªncia para T=10, 100 e 1000:
>
> Para T=10:
>
> $$
> Y_{10} \left( \sum_{t=1}^{10} x_t x_t' \right)^{-1} Y_{10} = \begin{bmatrix} \sqrt{10} & 0 \\ 0 & 10^{3/2} \end{bmatrix}  \begin{bmatrix} 10 & 55 \\ 55 & 385 \end{bmatrix}^{-1} \begin{bmatrix} \sqrt{10} & 0 \\ 0 & 10^{3/2} \end{bmatrix} \approx  \begin{bmatrix} 4.00 & -6.00 \\ -6.00 & 12.00 \end{bmatrix}
> $$
>
> Para T=100:
>
> $$
> Y_{100} \left( \sum_{t=1}^{100} x_t x_t' \right)^{-1} Y_{100} \approx \begin{bmatrix} 4.00 & -6.00 \\ -6.00 & 12.00 \end{bmatrix}
> $$
>
> Para T=1000:
>
> $$
> Y_{1000} \left( \sum_{t=1}^{1000} x_t x_t' \right)^{-1} Y_{1000} \approx \begin{bmatrix} 4.00 & -6.00 \\ -6.00 & 12.00 \end{bmatrix}
> $$
>
> Os valores numÃ©ricos demonstram como a matriz converge para $Q^{-1}$ a medida que $T$ cresce.
>
> ```python
> import numpy as np
>
> def calculate_scaled_matrix(T):
>     x = np.ones((T, 2))
>     x[:, 1] = np.arange(1, T + 1)
>
>     yt = np.diag([np.sqrt(T), T**(3/2)])
>     xtx = x.T @ x
>     return yt @ np.linalg.inv(xtx) @ yt
>
> T_values = [10, 100, 1000]
> for T in T_values:
>    result = calculate_scaled_matrix(T)
>    print(f"T = {T}:")
>    print(result)
>
> q_inverse = np.array([[4, -6], [-6, 12]])
> print("\nQ_inverse:\n",q_inverse)
> ```
>
>Este exemplo numÃ©rico ilustra como, Ã  medida que o tamanho da amostra (T) aumenta, a matriz rescalonada converge para a matriz Q-inversa, conforme previsto pela teoria.

**CorolÃ¡rio 6.1:**  A matriz $Q$ Ã© definida como:
$$ Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} $$
A inversa da matriz Q, $Q^{-1}$, Ã© dada por:
$$ Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} $$
*Prova:*
A prova deste resultado foi detalhada na seÃ§Ã£o anterior (Lema 1).
â– 

**ProposiÃ§Ã£o 6.1:**  A matriz $Q$ Ã© positiva definida.
*Prova:*
Para verificar que $Q$ Ã© positiva definida, precisamos mostrar que para qualquer vetor nÃ£o nulo $v = \begin{bmatrix} v_1 \\ v_2 \end{bmatrix}$, temos $v'Qv > 0$.  Calculando:
$$
v' Q v = \begin{bmatrix} v_1 & v_2 \end{bmatrix} \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = v_1^2 + v_1 v_2 + \frac{1}{3}v_2^2 = \left(v_1 + \frac{1}{2}v_2\right)^2 + \frac{1}{12}v_2^2
$$
Como ambos os termos sÃ£o nÃ£o negativos e pelo menos um deles Ã© estritamente positivo (se $v$ Ã© nÃ£o nulo), $v'Qv > 0$ para todo $v \neq 0$. Portanto, $Q$ Ã© positiva definida.
â– 
Este resultado Ã© crucial pois, juntamente com a convergÃªncia do segundo termo da expressÃ£o, $Y_T^{-1}\sum_{t=1}^T x_t \epsilon_t$, para uma normal multivariada com matriz de covariÃ¢ncia $\sigma^2 Q$, garante que a distribuiÃ§Ã£o assintÃ³tica dos estimadores OLS rescalonados esteja bem definida.

**Teorema 3:** O vetor de estimadores OLS rescalonado $Y_T(\hat{\beta}_T - \beta)$ converge em distribuiÃ§Ã£o para uma distribuiÃ§Ã£o normal multivariada com mÃ©dia zero e matriz de covariÃ¢ncia $\sigma^2 Q^{-1}$, ou seja,
$$ Y_T(\hat{\beta}_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1}) $$

*Prova:*
I. Demonstramos que $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \xrightarrow{p} Q^{-1}$.
II. Pela ProposiÃ§Ã£o 4 da seÃ§Ã£o anterior, sabemos que $Y_T^{-1}\sum_{t=1}^T x_t \epsilon_t \xrightarrow{d} N(0, \sigma^2 Q)$.
III. Combinando os resultados acima, e utilizando o Teorema de Slutsky, temos que:
$$ Y_T(\hat{\beta}_T - \beta) =  \left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right]  Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t \xrightarrow{d} Q^{-1} N(0, \sigma^2 Q) = N(0, \sigma^2 Q^{-1}) $$
Portanto,  $Y_T(\hat{\beta}_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1})$.
â– 

**Lema 3.1:** A matriz $Q^{-1}$ Ã© tambÃ©m positiva definida.
*Prova:*
Uma vez que $Q$ Ã© positiva definida (como demonstrado na ProposiÃ§Ã£o 6.1), sua inversa $Q^{-1}$ tambÃ©m Ã© positiva definida. Este Ã© um resultado conhecido da Ã¡lgebra linear: a inversa de uma matriz positiva definida Ã© tambÃ©m positiva definida. Isso garante que a matriz de covariÃ¢ncia assintÃ³tica dos estimadores seja vÃ¡lida.
â– 
Este teorema formaliza a distribuiÃ§Ã£o assintÃ³tica dos estimadores OLS rescalonados, permitindo o desenvolvimento de mÃ©todos para inferÃªncia estatÃ­stica.

### AplicaÃ§Ãµes Inferenciais
A convergÃªncia da matriz $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T$ para $Q^{-1}$ e do termo $Y_T^{-1}\sum_{t=1}^T x_t \epsilon_t$ para uma normal com matriz de covariÃ¢ncia $\sigma^2 Q$ tem implicaÃ§Ãµes importantes para inferÃªncia estatÃ­stica:
1. **Intervalos de ConfianÃ§a:** Podemos usar a distribuiÃ§Ã£o assintÃ³tica $N(0, \sigma^2 Q^{-1})$ para construir intervalos de confianÃ§a para $\alpha$ e $\delta$ e para quaisquer combinaÃ§Ãµes lineares destes. Especificamente, um intervalo de confianÃ§a de $(1-\alpha)\%$ para o parÃ¢metro $\alpha$, por exemplo, Ã© dado por
$$ \hat{\alpha}_T \pm z_{\alpha/2} \sqrt{\frac{4\sigma^2}{T}} $$
Onde $z_{\alpha/2}$ Ã© o quantil superior da distribuiÃ§Ã£o normal padrÃ£o para um nÃ­vel de confianÃ§a de $(1-\alpha)$ e $\frac{4\sigma^2}{T}$ Ã© o termo (1,1) da matriz $\frac{\sigma^2 Q^{-1}}{Y_T}$. De forma similar, um intervalo de confianÃ§a de $(1-\alpha)\%$ para o parÃ¢metro $\delta$, por exemplo, Ã© dado por
$$ \hat{\delta}_T \pm z_{\alpha/2} \sqrt{\frac{12\sigma^2}{T^3}} $$
Onde $\frac{12\sigma^2}{T^3}$ Ã© o termo (2,2) da matriz $\frac{\sigma^2 Q^{-1}}{Y_T}$.

2. **Testes de HipÃ³teses:** A distribuiÃ§Ã£o assintÃ³tica permite realizar testes de hipÃ³teses sobre os parÃ¢metros. Por exemplo, para testar a hipÃ³tese nula $H_0: \alpha = \alpha_0$ contra uma hipÃ³tese alternativa $H_1: \alpha \neq \alpha_0$, podemos construir a seguinte estatÃ­stica de teste
$$ \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{2\sigma} \xrightarrow{d} N(0,1) $$
e para a hipÃ³tese nula $H_0: \delta = \delta_0$ contra uma hipÃ³tese alternativa $H_1: \delta \neq \delta_0$, podemos construir a estatÃ­stica de teste
$$ \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\sqrt{12} \sigma} \xrightarrow{d} N(0,1) $$
Essas estatÃ­sticas podem ser comparadas com a distribuiÃ§Ã£o normal padrÃ£o para verificar se hÃ¡ evidÃªncia estatÃ­stica contra as hipÃ³teses nulas.

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Considere uma sÃ©rie temporal com $T = 100$ observaÃ§Ãµes, onde a estimativa para $\alpha$ Ã© $\hat{\alpha} = 2.5$ e a estimativa para $\delta$ Ã© $\hat{\delta} = 0.1$, e assumimos $\hat{\sigma}^2 = 0.25$. Vamos calcular intervalos de confianÃ§a para $\alpha$ e $\delta$, considerando um nÃ­vel de confianÃ§a de 95% ($z_{\alpha/2} = 1.96$).
>
> O intervalo de confianÃ§a para $\alpha$ Ã© dado por:
> $$ 2.5 \pm 1.96 \sqrt{\frac{4(0.25)}{100}} = 2.5 \pm 1.96 \times 0.1 = 2.5 \pm 0.196 $$
> Portanto, o intervalo de confianÃ§a para $\alpha$ Ã© aproximadamente $[2.304, 2.696]$.
>
> O intervalo de confianÃ§a para $\delta$ Ã© dado por:
> $$ 0.1 \pm 1.96 \sqrt{\frac{12(0.25)}{100^3}} = 0.1 \pm 1.96 \times 0.005477 = 0.1 \pm 0.0107 $$
> Portanto, o intervalo de confianÃ§a para $\delta$ Ã© aproximadamente $[0.0893, 0.1107]$.
>
> Note que os intervalos de confianÃ§a se baseiam nas distribuiÃ§Ãµes assintÃ³ticas dos estimadores.
>
> AlÃ©m disso, vamos fazer um teste de hipÃ³tese para $H_0 : \alpha = 2$ contra $H_1 : \alpha \neq 2$
> $$
> \frac{\sqrt{100}(2.5 - 2)}{2 \sqrt{0.25}} = \frac{10*0.5}{1} = 5
> $$
>
> Como a estatÃ­stica Ã© maior que 1.96, e o p-valor do teste serÃ¡ menor que 0.05, rejeitamos a hipÃ³tese nula de que $\alpha = 2$ ao nÃ­vel de significÃ¢ncia de 5%.
>
> Agora vamos testar a hipÃ³tese $H_0 : \delta = 0$ contra $H_1 : \delta \neq 0$.
>
> $$
> \frac{100^{3/2}(0.1-0)}{\sqrt{12*0.25}} = \frac{1000 * 0.1}{\sqrt{3}} = \frac{100}{\sqrt{3}} = 57.73
> $$
> Como a estatÃ­stica Ã© maior que 1.96, e o p-valor do teste serÃ¡ menor que 0.05, rejeitamos a hipÃ³tese nula de que $\delta = 0$ ao nÃ­vel de significÃ¢ncia de 5%.
>
>
> ```python
> import numpy as np
> from scipy.stats import norm
>
> # Dados do exemplo
> T = 100
> alpha_hat = 2.5
> delta_hat = 0.1
> sigma_sq_hat = 0.25
> confidence_level = 0.95
> z_critical = norm.ppf(1 - (1 - confidence_level) / 2)
>
> # Intervalo de confianÃ§a para alpha
> alpha_se = np.sqrt(4 * sigma_sq_hat / T)
> alpha_ci_lower = alpha_hat - z_critical * alpha_se
> alpha_ci_upper = alpha_hat + z_critical * alpha_se
> print(f"Intervalo de confianÃ§a para alpha: [{alpha_ci_lower:.4f}, {alpha_ci_upper:.4f}]")
>
> # Intervalo de confianÃ§a para delta
> delta_se = np.sqrt(12 * sigma_sq_hat / T**3)
> delta_ci_lower = delta_hat - z_critical * delta_se
> delta_ci_upper = delta_hat + z_critical * delta_se
> print(f"Intervalo de confianÃ§a para delta: [{delta_ci_lower:.4f}, {delta_ci_upper:.4f}]")
>
> # Teste de hipÃ³tese para alpha
> alpha_null = 2
> alpha_test_stat = (np.sqrt(T) * (alpha_hat - alpha_null)) / (2 * np.sqrt(sigma_sq_hat))
> alpha_p_value = 2 * (1 - norm.cdf(np.abs(alpha_test_stat)))
> print(f"EstatÃ­stica de teste para alpha: {alpha_test_stat:.2f}, p-valor: {alpha_p_value:.4f}")
>
> # Teste de hipÃ³tese para delta
> delta_null = 0
> delta_test_stat = (T**(3/2) * (delta_hat - delta_null)) / (np.sqrt(12 * sigma_sq_hat))
> delta_p_value = 2 * (1 - norm.cdf(np.abs(delta_test_stat)))
> print(f"EstatÃ­stica de teste para delta: {delta_test_stat:.2f}, p-valor: {delta_p_value:.4f}")
>
> ```
>
>Este exemplo numÃ©rico mostra como, na prÃ¡tica, podemos usar os resultados assintÃ³ticos para construir intervalos de confianÃ§a e realizar testes de hipÃ³teses para os parÃ¢metros do modelo, permitindo inferÃªncias estatÃ­sticas vÃ¡lidas sobre a tendÃªncia temporal.

**ObservaÃ§Ã£o 1:** Os intervalos de confianÃ§a e os testes de hipÃ³teses descritos acima sÃ£o baseados em aproximaÃ§Ãµes assintÃ³ticas, sendo vÃ¡lidos para amostras suficientemente grandes. Para amostras menores, podem ser necessÃ¡rias correÃ§Ãµes ou o uso de mÃ©todos de inferÃªncia alternativos. AlÃ©m disso, a validade desses resultados depende da correta especificaÃ§Ã£o do modelo e das propriedades do termo de erro $\epsilon_t$.

### ConclusÃ£o
A convergÃªncia da expressÃ£o $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T$ para a matriz $Q^{-1}$ Ã© um resultado fundamental para a anÃ¡lise de modelos de regressÃ£o com tendÃªncias de tempo determinÃ­sticas [^1]. Esta convergÃªncia, em conjunto com a distribuiÃ§Ã£o assintÃ³tica do termo $Y_T^{-1}\sum_{t=1}^T x_t \epsilon_t$, permite obter a distribuiÃ§Ã£o assintÃ³tica dos estimadores OLS rescalonados. Desta forma, podemos construir intervalos de confianÃ§a e realizar testes de hipÃ³teses vÃ¡lidos para os parÃ¢metros do modelo, abrindo caminho para a inferÃªncia estatÃ­stica em modelos com tendÃªncias temporais determinÃ­sticas.

### ReferÃªncias
[^1]: Rescaling OLS Estimates with Matrix $Y_T$.
[^2]: From [16.1.19] and [16.1.24], the asymptotic distribution of [16.1.18] can be calculated as in Example 7.5 of Chapter 7.
<!-- END -->
