## An√°lise dos Termos Dominantes nas Somas de Pot√™ncias de $t$ e Suas Converg√™ncias

### Introdu√ß√£o
Neste cap√≠tulo, estamos explorando os modelos de regress√£o com tend√™ncias temporais determin√≠sticas, e como a matriz de rescalonamento $Y_T$ desempenha um papel crucial na obten√ß√£o de distribui√ß√µes assint√≥ticas bem definidas para os estimadores OLS [^1]. Na se√ß√£o anterior, derivamos a necessidade de rescalonar a matriz de covari√¢ncia $\sum_{t=1}^T x_t x_t'$ para lidar com suas taxas de diverg√™ncia. Esta se√ß√£o se concentrar√° nos termos dominantes nas somas de pot√™ncias de $t$, especificamente $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$, e demonstraremos como esses termos, quando normalizados, convergem para valores espec√≠ficos [^3]. Al√©m disso, generalizaremos esse resultado para $\sum_{t=1}^T t^v$, onde $v$ √© um n√∫mero inteiro positivo, estabelecendo um padr√£o para converg√™ncia das somas normalizadas de pot√™ncias de $t$ [^3].

### Conceitos Fundamentais
No modelo de regress√£o com tend√™ncia de tempo determin√≠stica $y_t = \alpha + \delta t + \epsilon_t$, as matrizes que surgem nos c√°lculos de OLS envolvem as somas de pot√™ncias de $t$, isto √©, $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$. Conforme discutido anteriormente, as f√≥rmulas fechadas para estas somas s√£o dadas por:

$$ \sum_{t=1}^T t = \frac{T(T+1)}{2} $$

$$ \sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6} $$

O objetivo desta se√ß√£o √© analisar os termos dominantes nestas somas, ou seja, os termos que crescem mais rapidamente quando $T$ tende ao infinito, e como normalizar essas somas para obter uma converg√™ncia para valores finitos [^3].

#### O Termo Dominante em $\sum_{t=1}^T t$ e sua Converg√™ncia
Analisando a express√£o $\sum_{t=1}^T t = \frac{T(T+1)}{2} = \frac{T^2 + T}{2}$, o termo dominante, ou seja, aquele que cresce mais rapidamente quando $T$ tende ao infinito, √© $\frac{T^2}{2}$. Ao normalizarmos a soma $\sum_{t=1}^T t$ por $T^2$, obtemos:

$$ \frac{1}{T^2} \sum_{t=1}^T t = \frac{1}{T^2} \frac{T(T+1)}{2} = \frac{T^2 + T}{2T^2} = \frac{1}{2} + \frac{1}{2T} $$

Quando $T$ tende ao infinito, o termo $\frac{1}{2T}$ tende a zero, e a express√£o converge para $\frac{1}{2}$. Assim, podemos afirmar que o termo dominante em $\sum_{t=1}^T t$ √© $T^2/2$, e que a soma normalizada $\frac{1}{T^2} \sum_{t=1}^T t$ converge para $1/2$ [^3]:

$$ \frac{1}{T^2} \sum_{t=1}^T t \xrightarrow{T\rightarrow \infty} \frac{1}{2} $$
> üí° **Exemplo Num√©rico:** Considere os valores de T=10, 100, e 1000:
>
> $$
> \frac{1}{10^2}\sum_{t=1}^{10} t = \frac{1}{100}\frac{10(11)}{2} = \frac{55}{100} = 0.55
> $$
> $$
> \frac{1}{100^2}\sum_{t=1}^{100} t = \frac{1}{10000}\frac{100(101)}{2} = \frac{5050}{10000} = 0.505
> $$
> $$
> \frac{1}{1000^2}\sum_{t=1}^{1000} t = \frac{1}{1000000}\frac{1000(1001)}{2} = \frac{500500}{1000000} = 0.5005
> $$
>
> Note que, conforme T aumenta, a normaliza√ß√£o $\frac{1}{T^2}\sum_{t=1}^T t$ se aproxima de 0.5, conforme demonstrado na teoria.
>
> ```python
> import numpy as np
>
> T_values = [10, 100, 1000]
>
> for T in T_values:
>     sum_t = np.sum(np.arange(1, T + 1))
>     normalized_sum = sum_t / T**2
>     print(f"T = {T}, Normalized Sum = {normalized_sum:.4f}")
> ```
>
> Este c√≥digo Python calcula a soma dos inteiros de 1 a T, normaliza por $T^2$, e imprime o resultado para diferentes valores de T. Isso confirma que a normaliza√ß√£o converge para 0.5 conforme T aumenta.

#### O Termo Dominante em $\sum_{t=1}^T t^2$ e sua Converg√™ncia
Analisando a express√£o $\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6} = \frac{2T^3 + 3T^2 + T}{6}$, o termo dominante √© $\frac{2T^3}{6} = \frac{T^3}{3}$. Ao normalizarmos a soma $\sum_{t=1}^T t^2$ por $T^3$, obtemos:

$$ \frac{1}{T^3} \sum_{t=1}^T t^2 = \frac{1}{T^3} \frac{T(T+1)(2T+1)}{6} = \frac{2T^3 + 3T^2 + T}{6T^3} = \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2} $$

Quando $T$ tende ao infinito, os termos $\frac{1}{2T}$ e $\frac{1}{6T^2}$ tendem a zero, e a express√£o converge para $\frac{1}{3}$. Assim, podemos afirmar que o termo dominante em $\sum_{t=1}^T t^2$ √© $T^3/3$, e que a soma normalizada $\frac{1}{T^3} \sum_{t=1}^T t^2$ converge para $1/3$:

$$ \frac{1}{T^3} \sum_{t=1}^T t^2 \xrightarrow{T\rightarrow \infty} \frac{1}{3} $$
> üí° **Exemplo Num√©rico:** Considere os valores de T=10, 100 e 1000:
>
> $$
> \frac{1}{10^3}\sum_{t=1}^{10} t^2 = \frac{1}{1000}\frac{10(11)(21)}{6} = \frac{385}{1000} = 0.385
> $$
> $$
> \frac{1}{100^3}\sum_{t=1}^{100} t^2 = \frac{1}{1000000}\frac{100(101)(201)}{6} = \frac{338350}{1000000} = 0.33835
> $$
> $$
> \frac{1}{1000^3}\sum_{t=1}^{1000} t^2 = \frac{1}{1000000000}\frac{1000(1001)(2001)}{6} = \frac{333833500}{1000000000} = 0.3338335
> $$
> Observe que conforme T aumenta, a normaliza√ß√£o $\frac{1}{T^3}\sum_{t=1}^T t^2$ se aproxima de 0.333, o valor esperado pelo resultado te√≥rico.
>
> ```python
> import numpy as np
>
> T_values = [10, 100, 1000]
>
> for T in T_values:
>     sum_t_squared = np.sum(np.arange(1, T + 1)**2)
>     normalized_sum = sum_t_squared / T**3
>     print(f"T = {T}, Normalized Sum = {normalized_sum:.5f}")
> ```
> Este c√≥digo Python calcula a soma dos quadrados dos inteiros de 1 a T, normaliza por $T^3$, e imprime o resultado para diferentes valores de T, demonstrando a converg√™ncia para 1/3.

**Lema 1:** Para qualquer inteiro positivo $v$, a soma $\sum_{t=1}^T t^v$ pode ser expressa como um polin√¥mio em $T$ de grau $v+1$.

*Prova:*

Este resultado pode ser demonstrado por indu√ß√£o.
I. O caso base, $v=1$, √© dado por $\sum_{t=1}^T t = \frac{T(T+1)}{2} = \frac{1}{2}T^2 + \frac{1}{2}T$, que √© um polin√¥mio de grau 2.
II. Suponha que a afirma√ß√£o seja verdadeira para $v=k$, isto √©, $\sum_{t=1}^T t^k$ √© um polin√¥mio de grau $k+1$.
III. Consideremos ent√£o o caso $v=k+1$. Usaremos a identidade
$$ \sum_{t=1}^T ( (t+1)^{k+2} - t^{k+2}) = (T+1)^{k+2} - 1 $$
IV. Expandindo o lado esquerdo utilizando o bin√¥mio de Newton, obtemos:
$$ \sum_{t=1}^T \left( \sum_{j=0}^{k+1} \binom{k+2}{j} t^j + t^{k+2} - t^{k+2} \right) = \sum_{j=0}^{k+1} \binom{k+2}{j} \sum_{t=1}^T t^j = (T+1)^{k+2} - 1 $$
V. Assim,
$$ \sum_{t=1}^T t^{k+1} = \frac{1}{\binom{k+2}{k+1}} \left( (T+1)^{k+2} - 1 - \sum_{j=0}^{k} \binom{k+2}{j} \sum_{t=1}^T t^j \right) $$
VI. Pela hip√≥tese de indu√ß√£o, $\sum_{t=1}^T t^j$ √© um polin√¥mio em $T$ de grau $j+1$ para $j \leq k$, o que implica que o lado direito da equa√ß√£o √© um polin√¥mio em $T$ de grau $k+2$, o que conclui a prova do lema.
‚ñ†

**Corol√°rio 1.1:** O termo de maior grau no polin√¥mio de $\sum_{t=1}^T t^v$ √© da forma $c_v T^{v+1}$, onde $c_v$ √© uma constante que depende de $v$.
*Prova:*
O Corol√°rio segue diretamente do Lema 1, pois o termo de maior grau em um polin√¥mio da forma $a_0 + a_1 T + a_2 T^2 + \ldots + a_{v+1}T^{v+1}$ √© o termo $a_{v+1}T^{v+1}$, onde $a_{v+1}$ √© uma constante, que no caso da soma das potencias de $t$, pode depender de $v$.
‚ñ†

#### Generaliza√ß√£o para $\sum_{t=1}^T t^v$ e sua Converg√™ncia
Podemos generalizar este resultado para qualquer pot√™ncia inteira positiva $v$. A f√≥rmula para a soma $\sum_{t=1}^T t^v$ √© um polin√¥mio de grau $v+1$ em $T$. O termo dominante neste polin√¥mio ser√° da forma $\frac{T^{v+1}}{v+1}$. Portanto, ao normalizarmos a soma $\sum_{t=1}^T t^v$ por $T^{v+1}$, obtemos:
$$ \frac{1}{T^{v+1}} \sum_{t=1}^T t^v $$
Este termo converge para $\frac{1}{v+1}$ quando $T$ tende ao infinito:

$$ \frac{1}{T^{v+1}} \sum_{t=1}^T t^v \xrightarrow{T\rightarrow \infty} \frac{1}{v+1} $$
Este resultado √© chave, pois estabelece um padr√£o para a converg√™ncia das somas normalizadas de pot√™ncias de $t$, que √© fundamental para derivar os resultados assint√≥ticos em modelos de regress√£o com tend√™ncias de tempo determin√≠sticas.

**Proposi√ß√£o 7:** O termo dominante em $\sum_{t=1}^T t^v$, onde $v$ √© um inteiro positivo, √© $\frac{T^{v+1}}{v+1}$. Al√©m disso, a soma normalizada $\frac{1}{T^{v+1}} \sum_{t=1}^T t^v$ converge para $\frac{1}{v+1}$ quando $T$ tende ao infinito.
*Prova:*
I. A f√≥rmula para $\sum_{t=1}^T t^v$ √© um polin√¥mio de grau $v+1$ em $T$, com o termo de maior grau sendo da forma $cT^{v+1}$, onde $c$ √© uma constante.
II. Usando a aproxima√ß√£o de soma por integrais, podemos ver que
$$ \sum_{t=1}^T t^v \approx \int_1^T x^v dx = \frac{x^{v+1}}{v+1} \bigg|_1^T = \frac{T^{v+1} - 1}{v+1} $$
III. O termo dominante nesta express√£o √© $\frac{T^{v+1}}{v+1}$.
IV. Assim, ao normalizar a soma por $T^{v+1}$, obtemos:
$$ \frac{1}{T^{v+1}} \sum_{t=1}^T t^v \approx \frac{1}{T^{v+1}} \frac{T^{v+1} - 1}{v+1} = \frac{1}{v+1} - \frac{1}{(v+1)T^{v+1}} $$
V. Quando $T$ tende ao infinito, o termo $\frac{1}{(v+1)T^{v+1}}$ tende a zero, e a express√£o converge para $\frac{1}{v+1}$:
$$ \frac{1}{T^{v+1}} \sum_{t=1}^T t^v \xrightarrow{T \to \infty} \frac{1}{v+1} $$
Portanto, o termo dominante em $\sum_{t=1}^T t^v$ √© $\frac{T^{v+1}}{v+1}$, e a soma normalizada $\frac{1}{T^{v+1}} \sum_{t=1}^T t^v$ converge para $\frac{1}{v+1}$ quando $T$ tende ao infinito.
‚ñ†
Este resultado generaliza as an√°lises anteriores e √© fundamental para a compreens√£o das propriedades assint√≥ticas dos estimadores OLS em modelos com tend√™ncias de tempo determin√≠sticas.

> üí° **Exemplo Num√©rico:** Para ilustrar a Proposi√ß√£o 7, vamos analisar $\sum_{t=1}^T t^3$.
>
> A f√≥rmula para $\sum_{t=1}^T t^3$ √© dada por $\left(\frac{T(T+1)}{2}\right)^2 = \frac{T^4 + 2T^3 + T^2}{4}$. O termo dominante √© $\frac{T^4}{4}$. Ao normalizarmos $\sum_{t=1}^T t^3$ por $T^4$, obtemos:
> $$ \frac{1}{T^4}\sum_{t=1}^T t^3 = \frac{T^4 + 2T^3 + T^2}{4T^4} = \frac{1}{4} + \frac{1}{2T} + \frac{1}{4T^2} $$
> Quando $T$ tende ao infinito, os termos $\frac{1}{2T}$ e $\frac{1}{4T^2}$ tendem a zero, e a express√£o converge para $\frac{1}{4}$. Portanto, de acordo com a Proposi√ß√£o 7 (com $v=3$), $\frac{1}{T^4} \sum_{t=1}^T t^3$ converge para $\frac{1}{3+1}=\frac{1}{4}$.
>
> Este exemplo num√©rico demonstra que o resultado te√≥rico se mant√©m para $v=3$.
>
> ```python
> import numpy as np
>
> def calculate_normalized_sum(T, v):
>     t = np.arange(1, T + 1)
>     sum_tv = np.sum(t**v)
>     return sum_tv / T**(v + 1)
>
> v_values = [1, 2, 3]
> T_values = [10, 100, 1000, 10000]
>
> for v in v_values:
>     print(f"v = {v}")
>     for T in T_values:
>         result = calculate_normalized_sum(T, v)
>         print(f"  T = {T}: {result:.6f}, Expected = {1/(v+1):.6f}")
>
> ```
> Este c√≥digo python calcula a soma normalizada para diferentes valores de $v$ e $T$, mostrando que elas convergem para os valores te√≥ricos, confirmando a Proposi√ß√£o 7.

**Observa√ß√£o 1:** A Proposi√ß√£o 7 pode ser reescrita usando a nota√ß√£o de "O Grande" (Big O notation). Especificamente, podemos dizer que $\sum_{t=1}^T t^v = O(T^{v+1})$. Esta nota√ß√£o significa que existe uma constante $M$ tal que para $T$ suficientemente grande, $|\sum_{t=1}^T t^v| \leq M T^{v+1}$. Equivalentemente, podemos escrever que $\sum_{t=1}^T t^v = \frac{T^{v+1}}{v+1} + o(T^{v+1})$, onde $o(T^{v+1})$ representa um termo que cresce mais lentamente do que $T^{v+1}$ quando $T$ tende ao infinito.

### Conclus√£o
Nesta se√ß√£o, analisamos os termos dominantes nas somas de pot√™ncias de $t$, $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$, e demonstramos como normalizar essas somas para que convirjam para valores espec√≠ficos [^3]. O termo dominante em $\sum_{t=1}^T t$ √© $\frac{T^2}{2}$, e a soma normalizada $\frac{1}{T^2} \sum_{t=1}^T t$ converge para $1/2$, enquanto o termo dominante em $\sum_{t=1}^T t^2$ √© $\frac{T^3}{3}$, e a soma normalizada $\frac{1}{T^3} \sum_{t=1}^T t^2$ converge para $1/3$. Generalizamos este resultado, demonstrando que o termo dominante em $\sum_{t=1}^T t^v$ √© $\frac{T^{v+1}}{v+1}$, e que a soma normalizada $\frac{1}{T^{v+1}} \sum_{t=1}^T t^v$ converge para $\frac{1}{v+1}$ quando $T$ tende ao infinito. Esses resultados s√£o cruciais para entender as propriedades assint√≥ticas dos estimadores OLS em modelos com tend√™ncias de tempo determin√≠sticas e s√£o essenciais para a deriva√ß√£o das distribui√ß√µes assint√≥ticas que utilizamos para infer√™ncia estat√≠stica.
### Refer√™ncias
[^1]: Rescaling OLS Estimates with Matrix $Y_T$.
[^2]: From [16.1.19] and [16.1.24], the asymptotic distribution of [16.1.18] can be calculated as in Example 7.5 of Chapter 7.
[^3]: where $\sum$ denotes summation for t = 1 through T.
<!-- END -->
