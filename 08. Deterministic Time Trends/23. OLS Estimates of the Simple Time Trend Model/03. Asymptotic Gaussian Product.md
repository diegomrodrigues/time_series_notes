## DistribuiÃ§Ã£o AssintÃ³tica da Soma Ponderada dos Erros e AnÃ¡lise da VariÃ¢ncia
### IntroduÃ§Ã£o
Nesta seÃ§Ã£o, complementando a discussÃ£o anterior sobre a matriz de rescalonamento $Y_T$ e sua aplicaÃ§Ã£o aos estimadores OLS em modelos de tendÃªncia temporal determinÃ­stica [^1], vamos nos aprofundar na anÃ¡lise do segundo termo da expressÃ£o que define a distribuiÃ§Ã£o assintÃ³tica desses estimadores. Especificamente, examinaremos o termo $Y_T^{-1}\sum_{t=1}^T x_t \epsilon_t$ e demonstraremos que, sob certas condiÃ§Ãµes, ele converge para uma distribuiÃ§Ã£o gaussiana assintÃ³tica, estabelecendo a base para anÃ¡lises inferenciais subsequentes [^2]. AlÃ©m disso, analisaremos a variÃ¢ncia do termo de erro ponderado e sua convergÃªncia.

### Conceitos Fundamentais
Na seÃ§Ã£o anterior, estabelecemos que, para um modelo de regressÃ£o linear com tendÃªncia temporal determinÃ­stica, como $y_t = \alpha + \delta t + \epsilon_t$, o vetor de estimadores OLS rescalonados pode ser expresso como:

$$ Y_T (\hat{\beta}_T - \beta) = \left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right]  Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t $$

Onde $Y_T$ Ã© a matriz de rescalonamento, $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$, $\beta = \begin{bmatrix} \alpha \\ \delta \end{bmatrix}$, e $\epsilon_t$ Ã© o termo de erro. JÃ¡ mostramos que o termo $\left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right]$ converge para uma matriz $Q^{-1}$, que Ã© dada por $\begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$.

Agora, vamos nos concentrar no segundo termo, $Y_T^{-1}\sum_{t=1}^T x_t \epsilon_t$, que pode ser escrito explicitamente como:

$$ Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} T^{-1/2} \sum_{t=1}^T \epsilon_t \\ T^{-3/2} \sum_{t=1}^T t \epsilon_t \end{bmatrix} $$
> ðŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar, considere o caso com T=5, com erros $\epsilon_t$ sendo [0.5, -0.2, 0.8, -0.1, 0.3].
> $$ \sum_{t=1}^{5} \epsilon_t = 0.5 - 0.2 + 0.8 - 0.1 + 0.3 = 1.3 $$
> $$ \sum_{t=1}^{5} t \epsilon_t = (1)(0.5) + (2)(-0.2) + (3)(0.8) + (4)(-0.1) + (5)(0.3) = 0.5 - 0.4 + 2.4 - 0.4 + 1.5 = 3.6 $$
> Aplicando a matriz $Y_5^{-1}$:
> $$ Y_5^{-1} \sum_{t=1}^{5} x_t \epsilon_t = \begin{bmatrix} 5^{-1/2} & 0 \\ 0 & 5^{-3/2} \end{bmatrix} \begin{bmatrix} 1.3 \\ 3.6 \end{bmatrix} \approx \begin{bmatrix} 0.447 & 0 \\ 0 & 0.089 \end{bmatrix} \begin{bmatrix} 1.3 \\ 3.6 \end{bmatrix} \approx \begin{bmatrix} 0.581 \\ 0.320 \end{bmatrix} $$
> Isso ilustra que a primeira componente deste vetor Ã© uma soma dos erros, rescalonada por $\sqrt{T}$, e a segunda componente Ã© uma soma ponderada por $t$, rescalonada por $T^{3/2}$.

**ProposiÃ§Ã£o 4:** Se $\epsilon_t$ Ã© uma sequÃªncia de variÃ¡veis aleatÃ³rias i.i.d. com mÃ©dia zero, variÃ¢ncia $\sigma^2$, e quarto momento finito, entÃ£o o vetor $Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$ converge para uma distribuiÃ§Ã£o gaussiana assintÃ³tica com mÃ©dia zero e matriz de covariÃ¢ncia $\sigma^2 Q$, ou seja:

$$ Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t \xrightarrow{d} N(0, \sigma^2 Q) $$
Onde $Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$
*Prova:*
I.  Podemos analisar o vetor  $Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$ como um vetor de duas componentes:
$$  \begin{bmatrix} T^{-1/2} \sum_{t=1}^T \epsilon_t \\ T^{-3/2} \sum_{t=1}^T t \epsilon_t \end{bmatrix} $$
II. Pelo Teorema Central do Limite (TCL), sabemos que $T^{-1/2} \sum_{t=1}^T \epsilon_t$ converge em distribuiÃ§Ã£o para $N(0, \sigma^2)$.
III.  Para o segundo elemento, vamos reescrever $T^{-3/2} \sum_{t=1}^T t \epsilon_t$ como $\frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t$. A sequÃªncia $\left\{ \frac{t}{T} \epsilon_t \right\}_{t=1}^T$ forma uma sequÃªncia de diferenÃ§as martingales, onde $\mathbb{E}\left[ \frac{t}{T} \epsilon_t | \epsilon_{t-1}, \epsilon_{t-2}, \dots \right] = \frac{t}{T}\mathbb{E}[\epsilon_t] = 0$. Assim, podemos usar o TCL para martingales (ProposiÃ§Ã£o 7.8) para garantir a convergÃªncia para uma distribuiÃ§Ã£o normal.
IV. Para o segundo termo, a variÃ¢ncia de $\frac{t}{T} \epsilon_t$ Ã© dada por $\mathbb{E} \left[ \left( \frac{t}{T}\epsilon_t \right)^2 \right] = \frac{t^2}{T^2} \mathbb{E}[\epsilon_t^2] = \sigma^2 \frac{t^2}{T^2}$. Assim, a variÃ¢ncia de  $\frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t$ converge para:
$$
 \frac{1}{T} \sum_{t=1}^T \sigma^2 \frac{t^2}{T^2} = \frac{\sigma^2}{T^3}\sum_{t=1}^T t^2 \rightarrow \sigma^2/3
$$
V. AlÃ©m disso, pela ProposiÃ§Ã£o 7.8, o termo $\frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t$  converge em distribuiÃ§Ã£o para uma normal com mÃ©dia zero e variÃ¢ncia $\sigma^2/3$.
VI. A covariÃ¢ncia entre os dois termos, $T^{-1/2} \sum_{t=1}^T \epsilon_t$ e $T^{-3/2} \sum_{t=1}^T t \epsilon_t$, Ã© dada por
$$ \frac{1}{T^2} \mathbb{E}\left[\sum_{t=1}^T \epsilon_t \sum_{s=1}^T s \epsilon_s \right] = \frac{1}{T^2} \sum_{t=1}^T t \mathbb{E}[\epsilon_t^2] = \frac{\sigma^2}{T^2} \sum_{t=1}^T t \rightarrow \sigma^2/2 $$
VII. Combinando os resultados, obtemos que o vetor $Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$ converge em distribuiÃ§Ã£o para uma normal bivariada com mÃ©dia zero e matriz de covariÃ¢ncia
$$
\sigma^2 \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} = \sigma^2 Q
$$
Portanto, $ Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t \xrightarrow{d} N(0, \sigma^2 Q)$.
â– 
Este resultado Ã© essencial pois estabelece a distribuiÃ§Ã£o assintÃ³tica do termo $Y_T^{-1}\sum_{t=1}^T x_t \epsilon_t$, que, juntamente com a convergÃªncia da matriz $\left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right]$ para $Q^{-1}$, possibilita derivar a distribuiÃ§Ã£o assintÃ³tica do vetor de estimadores OLS rescalonados.

> ðŸ’¡ **Exemplo NumÃ©rico:** Para visualizar a convergÃªncia, vamos gerar 1000 amostras de tamanho T=100 de um modelo com $\alpha=2$, $\delta=0.5$ e $\epsilon_t$ sendo um ruÃ­do branco normal com $\sigma^2=1$. Calcularemos a mÃ©dia e a covariÃ¢ncia dos termos $T^{-1/2} \sum_{t=1}^T \epsilon_t$ e $T^{-3/2} \sum_{t=1}^T t \epsilon_t$ e compararemos com os valores teÃ³ricos.
>
> ```python
> import numpy as np
> import pandas as pd
> from numpy.linalg import inv
> from scipy.stats import norm
>
> # Set parameters
> T = 100
> num_simulations = 1000
> alpha = 2
> delta = 0.5
> sigma = 1
>
> # Initialize arrays to store the terms
> term1_simulations = np.zeros(num_simulations)
> term2_simulations = np.zeros(num_simulations)
>
> # Generate the data and calculate terms for each simulation
> for i in range(num_simulations):
>     t = np.arange(1, T + 1)
>     epsilon = np.random.normal(0, sigma, T)
>     term1_simulations[i] = np.sum(epsilon) / np.sqrt(T)
>     term2_simulations[i] = np.sum(t * epsilon) / T**(3/2)
>
> # Calculate the empirical mean and covariance matrix
> empirical_mean = np.array([np.mean(term1_simulations), np.mean(term2_simulations)])
> empirical_cov = np.cov(term1_simulations, term2_simulations)
>
> # Calculate the theoretical covariance matrix
> Q = np.array([[1, 1/2], [1/2, 1/3]])
> theoretical_cov = sigma**2 * Q
>
> # Display results
> print(f"Empirical Mean: {empirical_mean}")
> print(f"Empirical Covariance:\n{empirical_cov}")
> print(f"Theoretical Covariance:\n{theoretical_cov}")
>
> ```
>
> O output mostrarÃ¡ que a mÃ©dia empÃ­rica do vetor $Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$ se aproxima de zero e que a covariÃ¢ncia empÃ­rica se aproxima da covariÃ¢ncia teÃ³rica $\sigma^2Q$. Isso ilustra a convergÃªncia em distribuiÃ§Ã£o para uma normal multivariada, conforme demonstrado na proposiÃ§Ã£o.

**ObservaÃ§Ã£o 4.1:** A ProposiÃ§Ã£o 4 Ã© fundamental pois ela estabelece a distribuiÃ§Ã£o assintÃ³tica do termo $Y_T^{-1}\sum_{t=1}^T x_t \epsilon_t$, que Ã© crucial para derivar a distribuiÃ§Ã£o assintÃ³tica do vetor de estimadores OLS rescalonados. Em conjunto com os resultados anteriores, podemos concluir que o vetor de estimadores OLS rescalonado, $Y_T(\hat{\beta}_T - \beta)$, converge para uma distribuiÃ§Ã£o normal com mÃ©dia zero e matriz de covariÃ¢ncia $\sigma^2 Q^{-1}$, com $Q$ sendo a matriz que Ã© limite de $Y_T^{-1}(\sum_{t=1}^T x_t x_t')Y_T^{-1}$.

**ProposiÃ§Ã£o 4.2**
Sob as mesmas condiÃ§Ãµes da ProposiÃ§Ã£o 4, a soma ponderada dos erros rescalonada, $T^{-1} \sum_{t=1}^T \frac{t}{T} \epsilon_t$, tambÃ©m converge para uma distribuiÃ§Ã£o normal com mÃ©dia zero e variÃ¢ncia $\sigma^2/3$. Formalmente,
$$ T^{-1} \sum_{t=1}^T \frac{t}{T} \epsilon_t  \xrightarrow{d} N(0, \sigma^2/3) $$

*Prova:*
I.  Pela ProposiÃ§Ã£o 4, sabemos que  $T^{-3/2} \sum_{t=1}^T t \epsilon_t$  converge para uma distribuiÃ§Ã£o $N(0, \sigma^2/3)$.
II.  Podemos reescrever $T^{-1} \sum_{t=1}^T \frac{t}{T} \epsilon_t$ como $T^{1/2} \left( T^{-3/2} \sum_{t=1}^T t \epsilon_t \right)$.
III. Uma vez que $T^{1/2} \rightarrow \infty$ e  $T^{-3/2} \sum_{t=1}^T t \epsilon_t$ converge para uma distribuiÃ§Ã£o $N(0, \sigma^2/3)$, o termo $T^{-1} \sum_{t=1}^T \frac{t}{T} \epsilon_t$ Ã© uma versÃ£o rescalonada do segundo termo da ProposiÃ§Ã£o 4.  Embora $T^{1/2}$ nÃ£o convirja para um valor, essa expressÃ£o ainda Ã© util.
IV. Note que $T^{-1} \sum_{t=1}^T \frac{t}{T} \epsilon_t = \frac{1}{T} \sum_{t=1}^T \frac{t}{T} \epsilon_t =  \frac{1}{T}  \sum_{t=1}^T  \left( \frac{t}{T} \right) \epsilon_t$. Usando o TCL para martingales (ProposiÃ§Ã£o 7.8), notamos que $\frac{t}{T}$ sÃ£o coeficientes que variam com $t$ e que a esperanÃ§a condicional de $\frac{t}{T} \epsilon_t$ Ã© zero.
V. A variÃ¢ncia do termo $\frac{1}{T}  \sum_{t=1}^T  \left( \frac{t}{T} \right) \epsilon_t$ converge para $\sigma^2/3$ quando $T\rightarrow \infty$, como demonstrado na ProposiÃ§Ã£o 4.
Portanto,  $T^{-1} \sum_{t=1}^T \frac{t}{T} \epsilon_t  \xrightarrow{d} N(0, \sigma^2/3)$.
â– 

> ðŸ’¡ **Exemplo NumÃ©rico:** Para demonstrar a convergÃªncia da ProposiÃ§Ã£o 4.2, vamos simular 1000 amostras com T=100, os mesmos parÃ¢metros do exemplo anterior, e calcular a variÃ¢ncia empÃ­rica de $T^{-1} \sum_{t=1}^T \frac{t}{T} \epsilon_t$.
> ```python
> import numpy as np
>
> # Parameters
> T = 100
> num_simulations = 1000
> sigma = 1
>
> # Initialize array
> term_simulations = np.zeros(num_simulations)
>
> # Simulations
> for i in range(num_simulations):
>     t = np.arange(1, T + 1)
>     epsilon = np.random.normal(0, sigma, T)
>     term_simulations[i] = np.sum((t/T)*epsilon) / T
>
> # Calculate empirical variance
> empirical_variance = np.var(term_simulations)
>
> # Calculate theoretical variance
> theoretical_variance = sigma**2 / 3
>
> # Print results
> print(f"Empirical Variance: {empirical_variance}")
> print(f"Theoretical Variance: {theoretical_variance}")
> ```
>
> Os resultados mostram que a variÃ¢ncia empÃ­rica de $T^{-1} \sum_{t=1}^T \frac{t}{T} \epsilon_t$ se aproxima de $\sigma^2/3$, comprovando a proposiÃ§Ã£o.

Agora, vamos analisar a variÃ¢ncia do segundo elemento do termo $Y_T^{-1}\sum_{t=1}^T x_t \epsilon_t$.

**ProposiÃ§Ã£o 5:** A variÃ¢ncia do segundo elemento do vetor $Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$,  dado por $T^{-3/2} \sum_{t=1}^T t \epsilon_t$, converge assintoticamente para $\sigma^2/3$.
*Prova:*
I. A variÃ¢ncia do segundo termo Ã© dada por:
$$ \text{Var} \left( T^{-3/2} \sum_{t=1}^T t \epsilon_t \right) = T^{-3} \text{Var} \left( \sum_{t=1}^T t \epsilon_t \right) $$
II. Como as variÃ¡veis aleatÃ³rias $\epsilon_t$ sÃ£o independentes, a variÃ¢ncia da soma Ã© a soma das variÃ¢ncias:
$$  T^{-3} \sum_{t=1}^T \text{Var}(t \epsilon_t) = T^{-3} \sum_{t=1}^T t^2 \text{Var}(\epsilon_t) = T^{-3} \sigma^2 \sum_{t=1}^T t^2 $$
III. Sabemos que $\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6}$, entÃ£o:
$$  T^{-3} \sigma^2 \frac{T(T+1)(2T+1)}{6} = \sigma^2 \frac{1}{6} \frac{T(T+1)(2T+1)}{T^3} $$
IV.  Conforme $T \rightarrow \infty$, $\frac{T(T+1)(2T+1)}{T^3} \rightarrow 2$, assim:
$$  \sigma^2 \frac{1}{6} \frac{T(T+1)(2T+1)}{T^3} \rightarrow \sigma^2 \frac{1}{6} * 2 = \frac{\sigma^2}{3} $$
Portanto, a variÃ¢ncia converge para $\sigma^2/3$.
â– 
Este resultado mostra como a variÃ¢ncia do termo de erro ponderado por t se comporta quando $T$ tende ao infinito, confirmando que, apÃ³s o devido rescalonamento, temos uma distribuiÃ§Ã£o assintÃ³tica bem definida.

> ðŸ’¡ **Exemplo NumÃ©rico:** Para confirmar a convergÃªncia da variÃ¢ncia, vamos calcular a variÃ¢ncia empÃ­rica de $T^{-3/2} \sum_{t=1}^T t \epsilon_t$ para vÃ¡rios valores de T. Usaremos 1000 simulaÃ§Ãµes para cada T e compararemos a variÃ¢ncia empÃ­rica com $\sigma^2/3$.
> ```python
> import numpy as np
>
> # Parameters
> num_simulations = 1000
> sigma = 1
> T_values = [50, 100, 200, 500, 1000]
>
> # Loop through different T values
> for T in T_values:
>     term_simulations = np.zeros(num_simulations)
>     for i in range(num_simulations):
>         t = np.arange(1, T + 1)
>         epsilon = np.random.normal(0, sigma, T)
>         term_simulations[i] = np.sum(t * epsilon) / T**(3/2)
>
>     empirical_variance = np.var(term_simulations)
>     theoretical_variance = sigma**2 / 3
>     print(f"T = {T}: Empirical Variance = {empirical_variance:.4f}, Theoretical Variance = {theoretical_variance:.4f}")
> ```
> Este cÃ³digo mostra como a variÃ¢ncia empÃ­rica se aproxima do valor teÃ³rico de $\sigma^2/3$ Ã  medida que $T$ aumenta, demonstrando a validade da proposiÃ§Ã£o.

**ObservaÃ§Ã£o 5.1:** A ProposiÃ§Ã£o 5 Ã© um resultado especÃ­fico que estabelece a variÃ¢ncia do segundo componente do vetor $Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$. Isso demonstra a convergÃªncia da variÃ¢ncia da soma ponderada dos erros rescalonada para um valor finito, um resultado chave na anÃ¡lise de convergÃªncia.

**Lema 5.2:** A variÃ¢ncia do primeiro elemento do vetor $Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$, dado por $T^{-1/2} \sum_{t=1}^T \epsilon_t$, converge para $\sigma^2$.
*Prova:*
I. A variÃ¢ncia do primeiro termo Ã© dada por:
$$ \text{Var} \left( T^{-1/2} \sum_{t=1}^T \epsilon_t \right) = T^{-1} \text{Var} \left( \sum_{t=1}^T \epsilon_t \right) $$
II. Como as variÃ¡veis aleatÃ³rias $\epsilon_t$ sÃ£o independentes, a variÃ¢ncia da soma Ã© a soma das variÃ¢ncias:
$$  T^{-1} \sum_{t=1}^T \text{Var}(\epsilon_t) = T^{-1} \sum_{t=1}^T \sigma^2 = T^{-1} T \sigma^2 = \sigma^2 $$
Portanto, a variÃ¢ncia converge para $\sigma^2$.
â– 
Este lema mostra que a variÃ¢ncia do primeiro componente do vetor converge para a variÃ¢ncia dos erros $\epsilon_t$.

> ðŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar o Lema 5.2, vamos calcular a variÃ¢ncia empÃ­rica do primeiro componente do vetor, $T^{-1/2} \sum_{t=1}^T \epsilon_t$, para diferentes valores de T usando simulaÃ§Ãµes.
> ```python
> import numpy as np
>
> # Parameters
> num_simulations = 1000
> sigma = 1
> T_values = [50, 100, 200, 500, 1000]
>
> # Loop through different T values
> for T in T_values:
>    term_simulations = np.zeros(num_simulations)
>    for i in range(num_simulations):
>        epsilon = np.random.normal(0, sigma, T)
>        term_simulations[i] = np.sum(epsilon) / np.sqrt(T)
>
>    empirical_variance = np.var(term_simulations)
>    theoretical_variance = sigma**2
>    print(f"T = {T}: Empirical Variance = {empirical_variance:.4f}, Theoretical Variance = {theoretical_variance:.4f}")
> ```
> O resultado mostrarÃ¡ que a variÃ¢ncia empÃ­rica se aproxima do valor teÃ³rico de $\sigma^2$ conforme T aumenta, demonstrando o lema.

### ConclusÃ£o
Nesta seÃ§Ã£o, mostramos que o termo $Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$ converge assintoticamente para uma distribuiÃ§Ã£o gaussiana multivariada com mÃ©dia zero e matriz de covariÃ¢ncia $\sigma^2 Q$, demonstrando sua estabilidade assintÃ³tica [^2]. Calculamos explicitamente o valor limite da variÃ¢ncia dos erros rescalonados, com e sem peso temporal, mostrando que o processo de rescalonamento Ã© crucial para a obtenÃ§Ã£o de resultados assintoticamente estÃ¡veis. A anÃ¡lise da variÃ¢ncia do termo de erro ponderado tambÃ©m confirma a validade da teoria assintÃ³tica e prepara o terreno para testes de hipÃ³teses e inferÃªncia estatÃ­stica em modelos de sÃ©ries temporais com tendÃªncias determinÃ­sticas. Os resultados apresentados consolidam nossa compreensÃ£o sobre o comportamento dos estimadores OLS em presenÃ§a de tendÃªncias determinÃ­sticas e fornecem a base teÃ³rica para a realizaÃ§Ã£o de inferÃªncias vÃ¡lidas.

### ReferÃªncias
[^1]: Rescaling OLS Estimates with Matrix $Y_T$.
[^2]: From [16.1.19] and [16.1.24], the asymptotic distribution of [16.1.18] can be calculated as in Example 7.5 of Chapter 7.
<!-- END -->
