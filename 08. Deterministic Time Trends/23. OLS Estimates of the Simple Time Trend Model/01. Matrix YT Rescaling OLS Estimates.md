## Rescaling OLS Estimates with Matrix $Y_T$

### Introdu√ß√£o
Este cap√≠tulo explora processos com tend√™ncias de tempo determin√≠sticas, um t√≥pico crucial na an√°lise de s√©ries temporais. Conforme discutido anteriormente, estimadores de m√≠nimos quadrados ordin√°rios (OLS) para modelos com ra√≠zes unit√°rias ou tend√™ncias de tempo determin√≠sticas n√£o convergem da mesma maneira que aqueles em modelos com vari√°veis estacion√°rias [^1]. As taxas de converg√™ncia para diferentes par√¢metros geralmente diferem, exigindo abordagens espec√≠ficas para obter distribui√ß√µes assint√≥ticas. O objetivo principal √© estabelecer um m√©todo geral, apresentado por Sims, Stock e Watson (1990), para derivar essas distribui√ß√µes assint√≥ticas, focando em processos com tend√™ncias de tempo determin√≠sticas sem ra√≠zes unit√°rias. Este cap√≠tulo come√ßa analisando o modelo mais simples, um processo com inova√ß√µes i.i.d. em torno de uma tend√™ncia de tempo determin√≠stica [^1].

### Conceitos Fundamentais

O modelo base considerado √© uma simples tend√™ncia de tempo definida como:

$$ y_t = \alpha + \delta t + \epsilon_t $$

onde $\epsilon_t$ √© um processo de ru√≠do branco [^1]. Em cen√°rios onde $\epsilon_t$ √© distribu√≠do normalmente, o modelo se adequa √†s premissas cl√°ssicas de regress√£o. No entanto, as distribui√ß√µes assint√≥ticas dos estimadores OLS de $\alpha$ e $\delta$ exigem t√©cnicas espec√≠ficas devido a diferentes taxas de converg√™ncia.

A forma padr√£o do modelo de regress√£o √© dada por:

$$ y_t = x_t' \beta + \epsilon_t $$

onde $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$ e $\beta = \begin{bmatrix} \alpha \\ \delta \end{bmatrix}$ [^2]. O estimador OLS $\hat{\beta}_T$, baseado em uma amostra de tamanho $T$, √© definido como:

$$ \hat{\beta}_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_t $$

O desvio do estimador OLS do valor verdadeiro √© expresso por:

$$ (\hat{\beta}_T - \beta) = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t $$

Para encontrar a distribui√ß√£o limite, o m√©todo padr√£o envolve multiplicar por $\sqrt{T}$ [^2]. Contudo, isso n√£o funciona diretamente devido ao termo $\sum_{t=1}^T x_t x_t'$ divergir quando $T$ tende ao infinito [^4].

As somas de $t$ e $t^2$ s√£o dadas por:

$$ \sum_{t=1}^T t = \frac{T(T+1)}{2} $$

$$ \sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6} $$
> üí° **Exemplo Num√©rico:** Para ilustrar, vamos considerar um caso simples com T=10.
>
> $$ \sum_{t=1}^{10} t = \frac{10(10+1)}{2} = \frac{10 \times 11}{2} = 55 $$
>
> $$ \sum_{t=1}^{10} t^2 = \frac{10(10+1)(2 \times 10 + 1)}{6} = \frac{10 \times 11 \times 21}{6} = 385 $$
>
> Isso mostra que, conforme T aumenta, essas somas crescem rapidamente, com as somas de $t^2$ crescendo mais rapidamente do que as somas de $t$.

Observa-se que o termo dominante em $\sum_{t=1}^T t$ √© $T^2/2$ e em $\sum_{t=1}^T t^2$ √© $T^3/3$ [^3]. Portanto, a matriz $\sum_{t=1}^T x_t x_t'$ cresce √† taxa de $T^3$. Especificamente:

$$ \sum_{t=1}^T x_t x_t' = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix} = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} $$

Para obter uma matriz convergente, ela deve ser dividida por $T^3$, em vez de $T$ [^4]. No entanto, a matriz limite resultante n√£o √© invers√≠vel, pois $\sum_{t=1}^T x_t x_t'$ tem ordem de converg√™ncia $T^3$ ao inv√©s de $T$ [^4].

Para lidar com as diferentes taxas de converg√™ncia, introduzimos uma matriz diagonal $Y_T$:

$$ Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} $$

Esta matriz √© usada como um pr√©-multiplicador para o desvio do estimador OLS [^4]. Isso leva a uma nova express√£o:

$$ Y_T (\hat{\beta}_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t = \left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right]  Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$$

Esta transforma√ß√£o rescala os estimadores de acordo com suas taxas de converg√™ncia, garantindo que a distribui√ß√£o assint√≥tica esteja bem definida.  Multiplicando a matriz de covari√¢ncia $\sum_{t=1}^T x_t x_t'$ pela matriz $Y_T$ por ambos os lados, obtemos:

$$ Y_T \left( \sum_{t=1}^T x_t x_t' \right) Y_T = \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} $$

$$ = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix} = Q $$
> üí° **Exemplo Num√©rico:** Vamos demonstrar como essa matriz $Q$ √© calculada para T=10.
>
> Primeiro, calcule $\sum_{t=1}^{10} x_t x_t'$:
> $$
> \sum_{t=1}^{10} x_t x_t' = \begin{bmatrix} 10 & \frac{10(10+1)}{2} \\ \frac{10(10+1)}{2} & \frac{10(10+1)(2(10)+1)}{6} \end{bmatrix} = \begin{bmatrix} 10 & 55 \\ 55 & 385 \end{bmatrix}
> $$
>
> Em seguida, calcule $Y_{10}$:
> $$
> Y_{10} = \begin{bmatrix} \sqrt{10} & 0 \\ 0 & 10^{3/2} \end{bmatrix} \approx \begin{bmatrix} 3.16 & 0 \\ 0 & 31.62 \end{bmatrix}
> $$
>
> Agora, calcule $Y_{10}^{-1}$:
> $$
> Y_{10}^{-1} = \begin{bmatrix} 1/\sqrt{10} & 0 \\ 0 & 1/10^{3/2} \end{bmatrix} \approx \begin{bmatrix} 0.316 & 0 \\ 0 & 0.0316 \end{bmatrix}
> $$
>
> Finalmente, calcule  $Y_{10} \left( \sum_{t=1}^{10} x_t x_t' \right) Y_{10}$:
>
> $$
> Y_{10}^{-1} \left( \sum_{t=1}^{10} x_t x_t' \right) Y_{10}^{-1} =  \begin{bmatrix} 0.316 & 0 \\ 0 & 0.0316 \end{bmatrix} \begin{bmatrix} 10 & 55 \\ 55 & 385 \end{bmatrix}  \begin{bmatrix} 0.316 & 0 \\ 0 & 0.0316 \end{bmatrix} \approx
> \begin{bmatrix} 1 & 1.73 \\ 1.73 & 3.82  \end{bmatrix} \begin{bmatrix} 0.316 & 0 \\ 0 & 0.0316 \end{bmatrix} =  \begin{bmatrix} 0.316 & 0.054 \\ 0.547 & 0.120 \end{bmatrix}
> $$
>
> Observe que, conforme T aumenta, essa aproxima√ß√£o se aproxima da matriz $Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$

Este processo de rescalonamento possibilita o desenvolvimento de distribui√ß√µes assint√≥ticas n√£o degeneradas para os estimadores OLS.

**Lema 1** A matriz $Q$ definida acima √© invers√≠vel e sua inversa √© dada por:

$$ Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} $$
*Prova:* Podemos verificar diretamente que $QQ^{-1} = Q^{-1}Q = I$, onde $I$ √© a matriz identidade de ordem 2.
I. Calculando o produto $QQ^{-1}$:
$$
QQ^{-1} = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix} \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} = \begin{bmatrix} (1)(4) + (\frac{1}{2})(-6) & (1)(-6) + (\frac{1}{2})(12) \\ (\frac{1}{2})(4) + (\frac{1}{3})(-6) & (\frac{1}{2})(-6) + (\frac{1}{3})(12) \end{bmatrix}
$$
II. Simplificando os termos:
$$
= \begin{bmatrix} 4 - 3 & -6 + 6 \\ 2 - 2 & -3 + 4 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = I
$$
III. Calculando o produto $Q^{-1}Q$:
$$
Q^{-1}Q = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix} = \begin{bmatrix} (4)(1) + (-6)(\frac{1}{2}) & (4)(\frac{1}{2}) + (-6)(\frac{1}{3}) \\ (-6)(1) + (12)(\frac{1}{2}) & (-6)(\frac{1}{2}) + (12)(\frac{1}{3}) \end{bmatrix}
$$
IV. Simplificando os termos:
$$
= \begin{bmatrix} 4 - 3 & 2 - 2 \\ -6 + 6 & -3 + 4 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = I
$$
V. Como $QQ^{-1} = Q^{-1}Q = I$, $Q^{-1}$ √© de fato a inversa de $Q$.
‚ñ†

Al√©m disso, a matriz $Y_T^{-1}$ √© dada por:

$$ Y_T^{-1} = \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} $$
> üí° **Exemplo Num√©rico:** Para T=10, $Y_{10}^{-1} = \begin{bmatrix} 1/\sqrt{10} & 0 \\ 0 & 1/10^{3/2} \end{bmatrix} \approx \begin{bmatrix} 0.316 & 0 \\ 0 & 0.0316 \end{bmatrix}$. Isso demonstra como a matriz $Y_T^{-1}$ rescala os estimadores, notavelmente com o efeito de rescalonar a tend√™ncia temporal mais fortemente (por $T^{3/2}$) que o intercepto (por $T^{1/2}$).

Agora, vamos analisar o termo $Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$.

**Proposi√ß√£o 1** Se $\epsilon_t$ √© i.i.d. com m√©dia 0 e vari√¢ncia $\sigma^2$, ent√£o:
$$ Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t =  \begin{bmatrix} T^{-1/2} \sum_{t=1}^T \epsilon_t \\ T^{-3/2} \sum_{t=1}^T t \epsilon_t \end{bmatrix} $$
*Prova:* A prova segue diretamente da defini√ß√£o de $Y_T^{-1}$ e $x_t$. Expandindo a multiplica√ß√£o, temos:

I. Defini√ß√£o de $Y_T^{-1}$ e $x_t$:
    $$ Y_T^{-1} = \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix}, \quad x_t = \begin{bmatrix} 1 \\ t \end{bmatrix} $$
II. Substituindo na express√£o:
    $$ Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} \sum_{t=1}^T \begin{bmatrix} 1 \\ t \end{bmatrix} \epsilon_t $$
III. Distribuindo o somat√≥rio e a multiplica√ß√£o da matriz:
$$ = \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} \begin{bmatrix} \sum_{t=1}^T \epsilon_t \\ \sum_{t=1}^T t \epsilon_t \end{bmatrix} $$
IV. Multiplicando as matrizes:
$$ = \begin{bmatrix} T^{-1/2} \sum_{t=1}^T \epsilon_t \\ T^{-3/2} \sum_{t=1}^T t \epsilon_t \end{bmatrix} $$
Portanto, demonstrarmos o resultado desejado.
‚ñ†
> üí° **Exemplo Num√©rico:**  Suponha que temos uma s√©rie de erros $\epsilon_t$ para $T=5$, com valores [0.5, -0.2, 0.8, -0.1, 0.3].
> $$
> \sum_{t=1}^{5} \epsilon_t = 0.5 - 0.2 + 0.8 - 0.1 + 0.3 = 1.3
> $$
> $$
> \sum_{t=1}^{5} t \epsilon_t = (1)(0.5) + (2)(-0.2) + (3)(0.8) + (4)(-0.1) + (5)(0.3) = 0.5 - 0.4 + 2.4 - 0.4 + 1.5 = 3.6
> $$
>
> Usando $Y_5^{-1}$:
> $$
> Y_5^{-1} \sum_{t=1}^{5} x_t \epsilon_t = \begin{bmatrix} 5^{-1/2} & 0 \\ 0 & 5^{-3/2} \end{bmatrix} \begin{bmatrix} 1.3 \\ 3.6 \end{bmatrix} = \begin{bmatrix} 0.447 & 0 \\ 0 & 0.089 \end{bmatrix} \begin{bmatrix} 1.3 \\ 3.6 \end{bmatrix} = \begin{bmatrix} 0.581 \\ 0.320 \end{bmatrix}
> $$
> Isso demonstra que o primeiro elemento do vetor resultante √© $\frac{1}{\sqrt{T}}$ multiplicado pela soma dos erros e o segundo elemento √© $\frac{1}{T^{3/2}}$ multiplicado pela soma dos erros multiplicados por t.

Agora, podemos analisar o comportamento assint√≥tico deste termo.

**Teorema 1** Se $\epsilon_t$ s√£o vari√°veis aleat√≥rias i.i.d. com m√©dia 0 e vari√¢ncia $\sigma^2$, ent√£o:
$$ Y_T (\hat{\beta}_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1}) $$
*Prova:* Pelo Teorema Central do Limite multivariado, temos que
$$ \begin{bmatrix} T^{-1/2} \sum_{t=1}^T \epsilon_t \\ T^{-3/2} \sum_{t=1}^T t \epsilon_t \end{bmatrix} \xrightarrow{d} N \left( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \sigma^2 \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} \right) = N(0, \sigma^2 Q) $$
Assim
$$ Y_T (\hat{\beta}_T - \beta)  = \left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right]  Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t \xrightarrow{d} Q^{-1} N(0, \sigma^2 Q)  = N(0, \sigma^2 Q^{-1}) $$
Onde usamos a converg√™ncia de $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \xrightarrow{p} Q^{-1}$ e o resultado anterior para o termo $Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$.

I. Pelo Teorema Central do Limite (TCL) multivariado:
$$
\frac{1}{\sqrt{T}} \sum_{t=1}^T \begin{bmatrix} \epsilon_t \\ \frac{t}{T} \epsilon_t \end{bmatrix} \xrightarrow{d} N \left( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \sigma^2 \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} \right)
$$
O termo $\frac{1}{\sqrt{T}}\sum_{t=1}^T \begin{bmatrix} \epsilon_t \\ \frac{t}{T} \epsilon_t \end{bmatrix}$ √© a mesma coisa que $\begin{bmatrix} T^{-1/2} \sum_{t=1}^T \epsilon_t \\ T^{-3/2} \sum_{t=1}^T t \epsilon_t \end{bmatrix}$.

II. Definindo a matriz $Q$ como:
$$
Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}
$$
Podemos reescrever o resultado do TCL como:
$$
\begin{bmatrix} T^{-1/2} \sum_{t=1}^T \epsilon_t \\ T^{-3/2} \sum_{t=1}^T t \epsilon_t \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q)
$$

III. Sabemos que:
$$
Y_T (\hat{\beta}_T - \beta) = \left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right]  Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t
$$

IV. Tamb√©m sabemos que:
$$
Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \xrightarrow{p} Q^{-1}
$$
V.  Assim, substituindo:
$$ Y_T (\hat{\beta}_T - \beta)  \xrightarrow{d} Q^{-1} N(0, \sigma^2 Q) = N(0, \sigma^2 Q^{-1}) $$
Portanto, $ Y_T (\hat{\beta}_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1})$
‚ñ†
> üí° **Exemplo Num√©rico:** Vamos simular um conjunto de dados com T=100, $\alpha=2$, $\delta=0.5$ e $\epsilon_t$ sendo um ru√≠do branco normal com vari√¢ncia $\sigma^2=1$.
>
> ```python
> import numpy as np
> import pandas as pd
> from numpy.linalg import inv
> from scipy.stats import norm
>
> # Set parameters
> T = 100
> alpha = 2
> delta = 0.5
> sigma = 1
>
> # Generate the data
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, sigma, T)
> y = alpha + delta * t + epsilon
>
> # Create the matrix X
> X = np.stack([np.ones(T), t], axis=1)
>
> # Calculate the OLS estimate
> beta_hat = inv(X.T @ X) @ X.T @ y
>
> # Calculate the true beta
> beta = np.array([alpha, delta])
>
> # Calculate Y_T
> YT = np.array([[np.sqrt(T), 0], [0, T**(3/2)]])
>
> # Calculate the scaled estimate
> scaled_beta_hat = YT @ (beta_hat - beta)
>
> # Calculate the theoretical covariance
> Q = np.array([[1, 1/2], [1/2, 1/3]])
> Q_inv = np.array([[4, -6], [-6, 12]])
> cov_beta_hat = sigma**2 * Q_inv
>
> # Display results
> print(f"Estimated Beta: {beta_hat}")
> print(f"True Beta: {beta}")
> print(f"Scaled Beta: {scaled_beta_hat}")
> print(f"Theoretical Covariance: {cov_beta_hat}")
> ```
>
> Este c√≥digo simula os dados, calcula os estimadores OLS, o desvio dos estimadores, rescala este desvio usando $Y_T$ e calcula a matriz de covari√¢ncia te√≥rica. A sa√≠da do c√≥digo mostra o valor estimado de beta, o valor verdadeiro, o valor rescalonado de beta e a matriz de covari√¢ncia. Repetindo esta simula√ß√£o v√°rias vezes, podemos verificar que a distribui√ß√£o emp√≠rica de $Y_T(\hat{\beta}_T - \beta)$ converge para $N(0, \sigma^2 Q^{-1})$. A matriz de covari√¢ncia $ \sigma^2 Q^{-1}$ permite construir intervalos de confian√ßa para os estimadores $\alpha$ e $\delta$.

### Conclus√£o
A matriz $Y_T$ desempenha um papel fundamental na an√°lise de modelos de regress√£o com tend√™ncias de tempo determin√≠sticas. Ao pr√©-multiplicar o desvio do estimador OLS, ela compensa as diferentes taxas de converg√™ncia dos par√¢metros, levando a distribui√ß√µes assint√≥ticas bem definidas. A t√©cnica de transformar os dados usando $Y_T$ e a ideia de rescalonar por meio de matrizes de taxas de converg√™ncia, s√£o essenciais para realizar infer√™ncia estat√≠stica precisa nesses modelos. Esta abordagem prepara o terreno para an√°lises mais complexas, incluindo processos autoregressivos com tend√™ncias de tempo determin√≠sticas e outros modelos n√£o estacion√°rios que ser√£o explorados nos pr√≥ximos cap√≠tulos [^1].

### Refer√™ncias
[^1]:  Processes with Deterministic Time Trends.
[^2]: 16.1. Asymptotic Distribution of OLS Estimates of the Simple Time Trend Model.
[^3]: where $\sum$ denotes summation for t = 1 through T.
[^4]: In contrast to the usual result for stationary regressions, for the matrix in (16.1.16], (1/T) $\sum_{1}$ x,x, diverges. To obtain a convergent matrix, [16.1.16] would have to be divided by T¬≥ rather than T:.
<!-- END -->
