## Converg√™ncia da Matriz de Covari√¢ncia Rescalonada e Aplica√ß√µes Inferenciais

### Introdu√ß√£o
Dando continuidade √† nossa an√°lise sobre os modelos de regress√£o com tend√™ncias de tempo determin√≠sticas e utilizando a matriz de rescalonamento $Y_T$ [^1], esta se√ß√£o se concentra no primeiro termo da express√£o que define a distribui√ß√£o assint√≥tica dos estimadores OLS rescalonados. Este termo envolve o produto matricial de $Y_T$, a soma $\sum_{t=1}^T x_t x_t'$, e $Y_T^{-1}$. Demonstraremos que este produto converge para uma matriz $Q$, cujos elementos s√£o definidos pelas fra√ß√µes 1/2 e 1/3 [^1]. Este resultado √© fundamental para o desenvolvimento de intervalos de confian√ßa e testes de hip√≥teses.

### Conceitos Fundamentais
Relembrando, o modelo de regress√£o com tend√™ncia de tempo determin√≠stica √© dado por:

$$ y_t = \alpha + \delta t + \epsilon_t $$
onde $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$. O vetor de estimadores OLS √© $\hat{\beta}_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix}$. O desvio entre o estimador OLS e o verdadeiro valor de $\beta = \begin{bmatrix} \alpha \\ \delta \end{bmatrix}$ pode ser expresso como:

$$ (\hat{\beta}_T - \beta) = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t $$

Para lidar com as diferentes taxas de converg√™ncia dos estimadores, introduzimos a matriz de rescalonamento:
$$ Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} $$

O vetor de estimadores OLS rescalonados √© dado por:

$$ Y_T (\hat{\beta}_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t = \left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right]  Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$$

O primeiro termo, que √© o foco desta se√ß√£o, √©:
$$ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T $$

**Proposi√ß√£o 6:** O termo $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T$ converge em probabilidade para a matriz $Q^{-1}$ quando $T$ tende ao infinito, onde $Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$, ou seja,
$$ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \xrightarrow{p} Q^{-1} $$
*Prova:*
I.  Primeiramente, vamos analisar a matriz $\sum_{t=1}^T x_t x_t'$, que √© dada por:
$$ \sum_{t=1}^T x_t x_t' = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix} = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} $$
II. Multiplicando a matriz $\sum_{t=1}^T x_t x_t'$ por $Y_T^{-1}$ por ambos os lados, temos:
$$ Y_T^{-1} \left( \sum_{t=1}^T x_t x_t' \right) Y_T^{-1} =
\begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix}
\begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix}
\begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix}
$$

III. Calculando o produto matricial, obtemos:
$$ = \begin{bmatrix} T^{-1}T & T^{-1}\frac{T(T+1)}{2} \\ T^{-3/2}\frac{T(T+1)}{2} & T^{-3}\frac{T(T+1)(2T+1)}{6} \end{bmatrix}
\begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix}
= \begin{bmatrix} 1 & \frac{T+1}{2T} \\ \frac{T+1}{2T} & \frac{(T+1)(2T+1)}{6T^2} \end{bmatrix}
$$
$$ =
\begin{bmatrix} 1 & \frac{1}{2} + \frac{1}{2T} \\ \frac{1}{2} + \frac{1}{2T} & \frac{2T^2 + 3T + 1}{6T^2} \end{bmatrix} =
\begin{bmatrix} 1 & \frac{1}{2} + \frac{1}{2T} \\ \frac{1}{2} + \frac{1}{2T} & \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2} \end{bmatrix}
$$
IV.  Conforme $T$ tende ao infinito, os termos $\frac{1}{2T}$, $\frac{1}{2T}$ e $\frac{1}{6T^2}$ tendem a zero, e a express√£o converge para:
$$ Y_T^{-1} \left( \sum_{t=1}^T x_t x_t' \right) Y_T^{-1} \xrightarrow{p} \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix} = Q $$
V. Invertendo ambos os lados, obtemos:
$$ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \xrightarrow{p} Q^{-1} $$
Portanto, a proposi√ß√£o est√° demonstrada.
‚ñ†

> üí° **Exemplo Num√©rico:** Vamos calcular essa converg√™ncia para T=10, 100 e 1000:
>
> Para T=10:
>
> $$
> Y_{10} \left( \sum_{t=1}^{10} x_t x_t' \right)^{-1} Y_{10} = \begin{bmatrix} \sqrt{10} & 0 \\ 0 & 10^{3/2} \end{bmatrix}  \begin{bmatrix} 10 & 55 \\ 55 & 385 \end{bmatrix}^{-1} \begin{bmatrix} \sqrt{10} & 0 \\ 0 & 10^{3/2} \end{bmatrix} \approx  \begin{bmatrix} 4.00 & -6.00 \\ -6.00 & 12.00 \end{bmatrix}
> $$
>
> Para T=100:
>
> $$
> Y_{100} \left( \sum_{t=1}^{100} x_t x_t' \right)^{-1} Y_{100} \approx \begin{bmatrix} 4.00 & -6.00 \\ -6.00 & 12.00 \end{bmatrix}
> $$
>
> Para T=1000:
>
> $$
> Y_{1000} \left( \sum_{t=1}^{1000} x_t x_t' \right)^{-1} Y_{1000} \approx \begin{bmatrix} 4.00 & -6.00 \\ -6.00 & 12.00 \end{bmatrix}
> $$
>
> Os valores num√©ricos demonstram como a matriz converge para $Q^{-1}$ a medida que $T$ cresce.
>
> ```python
> import numpy as np
>
> def calculate_scaled_matrix(T):
>     x = np.ones((T, 2))
>     x[:, 1] = np.arange(1, T + 1)
>
>     yt = np.diag([np.sqrt(T), T**(3/2)])
>     xtx = x.T @ x
>     return yt @ np.linalg.inv(xtx) @ yt
>
> T_values = [10, 100, 1000]
> for T in T_values:
>    result = calculate_scaled_matrix(T)
>    print(f"T = {T}:")
>    print(result)
>
> q_inverse = np.array([[4, -6], [-6, 12]])
> print("\nQ_inverse:\n",q_inverse)
> ```
>
>Este exemplo num√©rico ilustra como, √† medida que o tamanho da amostra (T) aumenta, a matriz rescalonada converge para a matriz Q-inversa, conforme previsto pela teoria.

**Corol√°rio 6.1:**  A matriz $Q$ √© definida como:
$$ Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} $$
A inversa da matriz Q, $Q^{-1}$, √© dada por:
$$ Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} $$
*Prova:*
A prova deste resultado foi detalhada na se√ß√£o anterior (Lema 1).
‚ñ†

**Proposi√ß√£o 6.1:**  A matriz $Q$ √© positiva definida.
*Prova:*
Para verificar que $Q$ √© positiva definida, precisamos mostrar que para qualquer vetor n√£o nulo $v = \begin{bmatrix} v_1 \\ v_2 \end{bmatrix}$, temos $v'Qv > 0$.  Calculando:
$$
v' Q v = \begin{bmatrix} v_1 & v_2 \end{bmatrix} \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = v_1^2 + v_1 v_2 + \frac{1}{3}v_2^2 = \left(v_1 + \frac{1}{2}v_2\right)^2 + \frac{1}{12}v_2^2
$$
Como ambos os termos s√£o n√£o negativos e pelo menos um deles √© estritamente positivo (se $v$ √© n√£o nulo), $v'Qv > 0$ para todo $v \neq 0$. Portanto, $Q$ √© positiva definida.
‚ñ†
Este resultado √© crucial pois, juntamente com a converg√™ncia do segundo termo da express√£o, $Y_T^{-1}\sum_{t=1}^T x_t \epsilon_t$, para uma normal multivariada com matriz de covari√¢ncia $\sigma^2 Q$, garante que a distribui√ß√£o assint√≥tica dos estimadores OLS rescalonados esteja bem definida.

**Teorema 3:** O vetor de estimadores OLS rescalonado $Y_T(\hat{\beta}_T - \beta)$ converge em distribui√ß√£o para uma distribui√ß√£o normal multivariada com m√©dia zero e matriz de covari√¢ncia $\sigma^2 Q^{-1}$, ou seja,
$$ Y_T(\hat{\beta}_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1}) $$

*Prova:*
I. Demonstramos que $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \xrightarrow{p} Q^{-1}$.
II. Pela Proposi√ß√£o 4 da se√ß√£o anterior, sabemos que $Y_T^{-1}\sum_{t=1}^T x_t \epsilon_t \xrightarrow{d} N(0, \sigma^2 Q)$.
III. Combinando os resultados acima, e utilizando o Teorema de Slutsky, temos que:
$$ Y_T(\hat{\beta}_T - \beta) =  \left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right]  Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t \xrightarrow{d} Q^{-1} N(0, \sigma^2 Q) = N(0, \sigma^2 Q^{-1}) $$
Portanto,  $Y_T(\hat{\beta}_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1})$.
‚ñ†

**Lema 3.1:** A matriz $Q^{-1}$ √© tamb√©m positiva definida.
*Prova:*
Uma vez que $Q$ √© positiva definida (como demonstrado na Proposi√ß√£o 6.1), sua inversa $Q^{-1}$ tamb√©m √© positiva definida. Este √© um resultado conhecido da √°lgebra linear: a inversa de uma matriz positiva definida √© tamb√©m positiva definida. Isso garante que a matriz de covari√¢ncia assint√≥tica dos estimadores seja v√°lida.
‚ñ†
Este teorema formaliza a distribui√ß√£o assint√≥tica dos estimadores OLS rescalonados, permitindo o desenvolvimento de m√©todos para infer√™ncia estat√≠stica.

### Aplica√ß√µes Inferenciais
A converg√™ncia da matriz $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T$ para $Q^{-1}$ e do termo $Y_T^{-1}\sum_{t=1}^T x_t \epsilon_t$ para uma normal com matriz de covari√¢ncia $\sigma^2 Q$ tem implica√ß√µes importantes para infer√™ncia estat√≠stica:
1. **Intervalos de Confian√ßa:** Podemos usar a distribui√ß√£o assint√≥tica $N(0, \sigma^2 Q^{-1})$ para construir intervalos de confian√ßa para $\alpha$ e $\delta$ e para quaisquer combina√ß√µes lineares destes. Especificamente, um intervalo de confian√ßa de $(1-\alpha)\%$ para o par√¢metro $\alpha$, por exemplo, √© dado por
$$ \hat{\alpha}_T \pm z_{\alpha/2} \sqrt{\frac{4\sigma^2}{T}} $$
Onde $z_{\alpha/2}$ √© o quantil superior da distribui√ß√£o normal padr√£o para um n√≠vel de confian√ßa de $(1-\alpha)$ e $\frac{4\sigma^2}{T}$ √© o termo (1,1) da matriz $\frac{\sigma^2 Q^{-1}}{Y_T}$. De forma similar, um intervalo de confian√ßa de $(1-\alpha)\%$ para o par√¢metro $\delta$, por exemplo, √© dado por
$$ \hat{\delta}_T \pm z_{\alpha/2} \sqrt{\frac{12\sigma^2}{T^3}} $$
Onde $\frac{12\sigma^2}{T^3}$ √© o termo (2,2) da matriz $\frac{\sigma^2 Q^{-1}}{Y_T}$.

2. **Testes de Hip√≥teses:** A distribui√ß√£o assint√≥tica permite realizar testes de hip√≥teses sobre os par√¢metros. Por exemplo, para testar a hip√≥tese nula $H_0: \alpha = \alpha_0$ contra uma hip√≥tese alternativa $H_1: \alpha \neq \alpha_0$, podemos construir a seguinte estat√≠stica de teste
$$ \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{2\sigma} \xrightarrow{d} N(0,1) $$
e para a hip√≥tese nula $H_0: \delta = \delta_0$ contra uma hip√≥tese alternativa $H_1: \delta \neq \delta_0$, podemos construir a estat√≠stica de teste
$$ \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\sqrt{12} \sigma} \xrightarrow{d} N(0,1) $$
Essas estat√≠sticas podem ser comparadas com a distribui√ß√£o normal padr√£o para verificar se h√° evid√™ncia estat√≠stica contra as hip√≥teses nulas.

> üí° **Exemplo Num√©rico:**
> Considere uma s√©rie temporal com $T = 100$ observa√ß√µes, onde a estimativa para $\alpha$ √© $\hat{\alpha} = 2.5$ e a estimativa para $\delta$ √© $\hat{\delta} = 0.1$, e assumimos $\hat{\sigma}^2 = 0.25$. Vamos calcular intervalos de confian√ßa para $\alpha$ e $\delta$, considerando um n√≠vel de confian√ßa de 95% ($z_{\alpha/2} = 1.96$).
>
> O intervalo de confian√ßa para $\alpha$ √© dado por:
> $$ 2.5 \pm 1.96 \sqrt{\frac{4(0.25)}{100}} = 2.5 \pm 1.96 \times 0.1 = 2.5 \pm 0.196 $$
> Portanto, o intervalo de confian√ßa para $\alpha$ √© aproximadamente $[2.304, 2.696]$.
>
> O intervalo de confian√ßa para $\delta$ √© dado por:
> $$ 0.1 \pm 1.96 \sqrt{\frac{12(0.25)}{100^3}} = 0.1 \pm 1.96 \times 0.005477 = 0.1 \pm 0.0107 $$
> Portanto, o intervalo de confian√ßa para $\delta$ √© aproximadamente $[0.0893, 0.1107]$.
>
> Note que os intervalos de confian√ßa se baseiam nas distribui√ß√µes assint√≥ticas dos estimadores.
>
> Al√©m disso, vamos fazer um teste de hip√≥tese para $H_0 : \alpha = 2$ contra $H_1 : \alpha \neq 2$
> $$
> \frac{\sqrt{100}(2.5 - 2)}{2 \sqrt{0.25}} = \frac{10*0.5}{1} = 5
> $$
>
> Como a estat√≠stica √© maior que 1.96, e o p-valor do teste ser√° menor que 0.05, rejeitamos a hip√≥tese nula de que $\alpha = 2$ ao n√≠vel de signific√¢ncia de 5%.
>
> Agora vamos testar a hip√≥tese $H_0 : \delta = 0$ contra $H_1 : \delta \neq 0$.
>
> $$
> \frac{100^{3/2}(0.1-0)}{\sqrt{12*0.25}} = \frac{1000 * 0.1}{\sqrt{3}} = \frac{100}{\sqrt{3}} = 57.73
> $$
> Como a estat√≠stica √© maior que 1.96, e o p-valor do teste ser√° menor que 0.05, rejeitamos a hip√≥tese nula de que $\delta = 0$ ao n√≠vel de signific√¢ncia de 5%.
>
>
> ```python
> import numpy as np
> from scipy.stats import norm
>
> # Dados do exemplo
> T = 100
> alpha_hat = 2.5
> delta_hat = 0.1
> sigma_sq_hat = 0.25
> confidence_level = 0.95
> z_critical = norm.ppf(1 - (1 - confidence_level) / 2)
>
> # Intervalo de confian√ßa para alpha
> alpha_se = np.sqrt(4 * sigma_sq_hat / T)
> alpha_ci_lower = alpha_hat - z_critical * alpha_se
> alpha_ci_upper = alpha_hat + z_critical * alpha_se
> print(f"Intervalo de confian√ßa para alpha: [{alpha_ci_lower:.4f}, {alpha_ci_upper:.4f}]")
>
> # Intervalo de confian√ßa para delta
> delta_se = np.sqrt(12 * sigma_sq_hat / T**3)
> delta_ci_lower = delta_hat - z_critical * delta_se
> delta_ci_upper = delta_hat + z_critical * delta_se
> print(f"Intervalo de confian√ßa para delta: [{delta_ci_lower:.4f}, {delta_ci_upper:.4f}]")
>
> # Teste de hip√≥tese para alpha
> alpha_null = 2
> alpha_test_stat = (np.sqrt(T) * (alpha_hat - alpha_null)) / (2 * np.sqrt(sigma_sq_hat))
> alpha_p_value = 2 * (1 - norm.cdf(np.abs(alpha_test_stat)))
> print(f"Estat√≠stica de teste para alpha: {alpha_test_stat:.2f}, p-valor: {alpha_p_value:.4f}")
>
> # Teste de hip√≥tese para delta
> delta_null = 0
> delta_test_stat = (T**(3/2) * (delta_hat - delta_null)) / (np.sqrt(12 * sigma_sq_hat))
> delta_p_value = 2 * (1 - norm.cdf(np.abs(delta_test_stat)))
> print(f"Estat√≠stica de teste para delta: {delta_test_stat:.2f}, p-valor: {delta_p_value:.4f}")
>
> ```
>
>Este exemplo num√©rico mostra como, na pr√°tica, podemos usar os resultados assint√≥ticos para construir intervalos de confian√ßa e realizar testes de hip√≥teses para os par√¢metros do modelo, permitindo infer√™ncias estat√≠sticas v√°lidas sobre a tend√™ncia temporal.

**Observa√ß√£o 1:** Os intervalos de confian√ßa e os testes de hip√≥teses descritos acima s√£o baseados em aproxima√ß√µes assint√≥ticas, sendo v√°lidos para amostras suficientemente grandes. Para amostras menores, podem ser necess√°rias corre√ß√µes ou o uso de m√©todos de infer√™ncia alternativos. Al√©m disso, a validade desses resultados depende da correta especifica√ß√£o do modelo e das propriedades do termo de erro $\epsilon_t$.

### Conclus√£o
A converg√™ncia da express√£o $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T$ para a matriz $Q^{-1}$ √© um resultado fundamental para a an√°lise de modelos de regress√£o com tend√™ncias de tempo determin√≠sticas [^1]. Esta converg√™ncia, em conjunto com a distribui√ß√£o assint√≥tica do termo $Y_T^{-1}\sum_{t=1}^T x_t \epsilon_t$, permite obter a distribui√ß√£o assint√≥tica dos estimadores OLS rescalonados. Desta forma, podemos construir intervalos de confian√ßa e realizar testes de hip√≥teses v√°lidos para os par√¢metros do modelo, abrindo caminho para a infer√™ncia estat√≠stica em modelos com tend√™ncias temporais determin√≠sticas.

### Refer√™ncias
[^1]: Rescaling OLS Estimates with Matrix $Y_T$.
[^2]: From [16.1.19] and [16.1.24], the asymptotic distribution of [16.1.18] can be calculated as in Example 7.5 of Chapter 7.
<!-- END -->
