## Rescaling OLS Estimates with Matrix $Y_T$

### IntroduÃ§Ã£o
Este capÃ­tulo explora processos com tendÃªncias de tempo determinÃ­sticas, um tÃ³pico crucial na anÃ¡lise de sÃ©ries temporais. Conforme discutido anteriormente, estimadores de mÃ­nimos quadrados ordinÃ¡rios (OLS) para modelos com raÃ­zes unitÃ¡rias ou tendÃªncias de tempo determinÃ­sticas nÃ£o convergem da mesma maneira que aqueles em modelos com variÃ¡veis estacionÃ¡rias [^1]. As taxas de convergÃªncia para diferentes parÃ¢metros geralmente diferem, exigindo abordagens especÃ­ficas para obter distribuiÃ§Ãµes assintÃ³ticas. O objetivo principal Ã© estabelecer um mÃ©todo geral, apresentado por Sims, Stock e Watson (1990), para derivar essas distribuiÃ§Ãµes assintÃ³ticas, focando em processos com tendÃªncias de tempo determinÃ­sticas sem raÃ­zes unitÃ¡rias. Este capÃ­tulo comeÃ§a analisando o modelo mais simples, um processo com inovaÃ§Ãµes i.i.d. em torno de uma tendÃªncia de tempo determinÃ­stica [^1].

### Conceitos Fundamentais

O modelo base considerado Ã© uma simples tendÃªncia de tempo definida como:

$$ y_t = \alpha + \delta t + \epsilon_t $$

onde $\epsilon_t$ Ã© um processo de ruÃ­do branco [^1]. Em cenÃ¡rios onde $\epsilon_t$ Ã© distribuÃ­do normalmente, o modelo se adequa Ã s premissas clÃ¡ssicas de regressÃ£o. No entanto, as distribuiÃ§Ãµes assintÃ³ticas dos estimadores OLS de $\alpha$ e $\delta$ exigem tÃ©cnicas especÃ­ficas devido a diferentes taxas de convergÃªncia.

A forma padrÃ£o do modelo de regressÃ£o Ã© dada por:

$$ y_t = x_t' \beta + \epsilon_t $$

onde $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$ e $\beta = \begin{bmatrix} \alpha \\ \delta \end{bmatrix}$ [^2]. O estimador OLS $\hat{\beta}_T$, baseado em uma amostra de tamanho $T$, Ã© definido como:

$$ \hat{\beta}_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_t $$

O desvio do estimador OLS do valor verdadeiro Ã© expresso por:

$$ (\hat{\beta}_T - \beta) = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t $$

Para encontrar a distribuiÃ§Ã£o limite, o mÃ©todo padrÃ£o envolve multiplicar por $\sqrt{T}$ [^2]. Contudo, isso nÃ£o funciona diretamente devido ao termo $\sum_{t=1}^T x_t x_t'$ divergir quando $T$ tende ao infinito [^4].

As somas de $t$ e $t^2$ sÃ£o dadas por:

$$ \sum_{t=1}^T t = \frac{T(T+1)}{2} $$

$$ \sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6} $$
> ðŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar, vamos considerar um caso simples com T=10.
>
> $$ \sum_{t=1}^{10} t = \frac{10(10+1)}{2} = \frac{10 \times 11}{2} = 55 $$
>
> $$ \sum_{t=1}^{10} t^2 = \frac{10(10+1)(2 \times 10 + 1)}{6} = \frac{10 \times 11 \times 21}{6} = 385 $$
>
> Isso mostra que, conforme T aumenta, essas somas crescem rapidamente, com as somas de $t^2$ crescendo mais rapidamente do que as somas de $t$.

Observa-se que o termo dominante em $\sum_{t=1}^T t$ Ã© $T^2/2$ e em $\sum_{t=1}^T t^2$ Ã© $T^3/3$ [^3]. Portanto, a matriz $\sum_{t=1}^T x_t x_t'$ cresce Ã  taxa de $T^3$. Especificamente:

$$ \sum_{t=1}^T x_t x_t' = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix} = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} $$

Para obter uma matriz convergente, ela deve ser dividida por $T^3$, em vez de $T$ [^4]. No entanto, a matriz limite resultante nÃ£o Ã© inversÃ­vel, pois $\sum_{t=1}^T x_t x_t'$ tem ordem de convergÃªncia $T^3$ ao invÃ©s de $T$ [^4].

Para lidar com as diferentes taxas de convergÃªncia, introduzimos uma matriz diagonal $Y_T$:

$$ Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} $$

Esta matriz Ã© usada como um prÃ©-multiplicador para o desvio do estimador OLS [^4]. Isso leva a uma nova expressÃ£o:

$$ Y_T (\hat{\beta}_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t = \left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right]  Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$$

Esta transformaÃ§Ã£o rescala os estimadores de acordo com suas taxas de convergÃªncia, garantindo que a distribuiÃ§Ã£o assintÃ³tica esteja bem definida.  Multiplicando a matriz de covariÃ¢ncia $\sum_{t=1}^T x_t x_t'$ pela matriz $Y_T$ por ambos os lados, obtemos:

$$ Y_T \left( \sum_{t=1}^T x_t x_t' \right) Y_T = \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} $$

$$ = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix} = Q $$
> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos demonstrar como essa matriz $Q$ Ã© calculada para T=10.
>
> Primeiro, calcule $\sum_{t=1}^{10} x_t x_t'$:
> $$
> \sum_{t=1}^{10} x_t x_t' = \begin{bmatrix} 10 & \frac{10(10+1)}{2} \\ \frac{10(10+1)}{2} & \frac{10(10+1)(2(10)+1)}{6} \end{bmatrix} = \begin{bmatrix} 10 & 55 \\ 55 & 385 \end{bmatrix}
> $$
>
> Em seguida, calcule $Y_{10}$:
> $$
> Y_{10} = \begin{bmatrix} \sqrt{10} & 0 \\ 0 & 10^{3/2} \end{bmatrix} \approx \begin{bmatrix} 3.16 & 0 \\ 0 & 31.62 \end{bmatrix}
> $$
>
> Agora, calcule $Y_{10}^{-1}$:
> $$
> Y_{10}^{-1} = \begin{bmatrix} 1/\sqrt{10} & 0 \\ 0 & 1/10^{3/2} \end{bmatrix} \approx \begin{bmatrix} 0.316 & 0 \\ 0 & 0.0316 \end{bmatrix}
> $$
>
> Finalmente, calcule  $Y_{10} \left( \sum_{t=1}^{10} x_t x_t' \right) Y_{10}$:
>
> $$
> Y_{10}^{-1} \left( \sum_{t=1}^{10} x_t x_t' \right) Y_{10}^{-1} =  \begin{bmatrix} 0.316 & 0 \\ 0 & 0.0316 \end{bmatrix} \begin{bmatrix} 10 & 55 \\ 55 & 385 \end{bmatrix}  \begin{bmatrix} 0.316 & 0 \\ 0 & 0.0316 \end{bmatrix} \approx
> \begin{bmatrix} 1 & 1.73 \\ 1.73 & 3.82  \end{bmatrix} \begin{bmatrix} 0.316 & 0 \\ 0 & 0.0316 \end{bmatrix} =  \begin{bmatrix} 0.316 & 0.054 \\ 0.547 & 0.120 \end{bmatrix}
> $$
>
> Observe que, conforme T aumenta, essa aproximaÃ§Ã£o se aproxima da matriz $Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$

Este processo de rescalonamento possibilita o desenvolvimento de distribuiÃ§Ãµes assintÃ³ticas nÃ£o degeneradas para os estimadores OLS.

**Lema 1** A matriz $Q$ definida acima Ã© inversÃ­vel e sua inversa Ã© dada por:

$$ Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} $$
*Prova:* Podemos verificar diretamente que $QQ^{-1} = Q^{-1}Q = I$, onde $I$ Ã© a matriz identidade de ordem 2.
I. Calculando o produto $QQ^{-1}$:
$$
QQ^{-1} = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix} \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} = \begin{bmatrix} (1)(4) + (\frac{1}{2})(-6) & (1)(-6) + (\frac{1}{2})(12) \\ (\frac{1}{2})(4) + (\frac{1}{3})(-6) & (\frac{1}{2})(-6) + (\frac{1}{3})(12) \end{bmatrix}
$$
II. Simplificando os termos:
$$
= \begin{bmatrix} 4 - 3 & -6 + 6 \\ 2 - 2 & -3 + 4 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = I
$$
III. Calculando o produto $Q^{-1}Q$:
$$
Q^{-1}Q = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix} = \begin{bmatrix} (4)(1) + (-6)(\frac{1}{2}) & (4)(\frac{1}{2}) + (-6)(\frac{1}{3}) \\ (-6)(1) + (12)(\frac{1}{2}) & (-6)(\frac{1}{2}) + (12)(\frac{1}{3}) \end{bmatrix}
$$
IV. Simplificando os termos:
$$
= \begin{bmatrix} 4 - 3 & 2 - 2 \\ -6 + 6 & -3 + 4 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = I
$$
V. Como $QQ^{-1} = Q^{-1}Q = I$, $Q^{-1}$ Ã© de fato a inversa de $Q$.
â– 

AlÃ©m disso, a matriz $Y_T^{-1}$ Ã© dada por:

$$ Y_T^{-1} = \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} $$
> ðŸ’¡ **Exemplo NumÃ©rico:** Para T=10, $Y_{10}^{-1} = \begin{bmatrix} 1/\sqrt{10} & 0 \\ 0 & 1/10^{3/2} \end{bmatrix} \approx \begin{bmatrix} 0.316 & 0 \\ 0 & 0.0316 \end{bmatrix}$. Isso demonstra como a matriz $Y_T^{-1}$ rescala os estimadores, notavelmente com o efeito de rescalonar a tendÃªncia temporal mais fortemente (por $T^{3/2}$) que o intercepto (por $T^{1/2}$).

Agora, vamos analisar o termo $Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$.

**ProposiÃ§Ã£o 1** Se $\epsilon_t$ Ã© i.i.d. com mÃ©dia 0 e variÃ¢ncia $\sigma^2$, entÃ£o:
$$ Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t =  \begin{bmatrix} T^{-1/2} \sum_{t=1}^T \epsilon_t \\ T^{-3/2} \sum_{t=1}^T t \epsilon_t \end{bmatrix} $$
*Prova:* A prova segue diretamente da definiÃ§Ã£o de $Y_T^{-1}$ e $x_t$. Expandindo a multiplicaÃ§Ã£o, temos:

I. DefiniÃ§Ã£o de $Y_T^{-1}$ e $x_t$:
    $$ Y_T^{-1} = \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix}, \quad x_t = \begin{bmatrix} 1 \\ t \end{bmatrix} $$
II. Substituindo na expressÃ£o:
    $$ Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} \sum_{t=1}^T \begin{bmatrix} 1 \\ t \end{bmatrix} \epsilon_t $$
III. Distribuindo o somatÃ³rio e a multiplicaÃ§Ã£o da matriz:
$$ = \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} \begin{bmatrix} \sum_{t=1}^T \epsilon_t \\ \sum_{t=1}^T t \epsilon_t \end{bmatrix} $$
IV. Multiplicando as matrizes:
$$ = \begin{bmatrix} T^{-1/2} \sum_{t=1}^T \epsilon_t \\ T^{-3/2} \sum_{t=1}^T t \epsilon_t \end{bmatrix} $$
Portanto, demonstrarmos o resultado desejado.
â– 
> ðŸ’¡ **Exemplo NumÃ©rico:**  Suponha que temos uma sÃ©rie de erros $\epsilon_t$ para $T=5$, com valores [0.5, -0.2, 0.8, -0.1, 0.3].
> $$
> \sum_{t=1}^{5} \epsilon_t = 0.5 - 0.2 + 0.8 - 0.1 + 0.3 = 1.3
> $$
> $$
> \sum_{t=1}^{5} t \epsilon_t = (1)(0.5) + (2)(-0.2) + (3)(0.8) + (4)(-0.1) + (5)(0.3) = 0.5 - 0.4 + 2.4 - 0.4 + 1.5 = 3.6
> $$
>
> Usando $Y_5^{-1}$:
> $$
> Y_5^{-1} \sum_{t=1}^{5} x_t \epsilon_t = \begin{bmatrix} 5^{-1/2} & 0 \\ 0 & 5^{-3/2} \end{bmatrix} \begin{bmatrix} 1.3 \\ 3.6 \end{bmatrix} = \begin{bmatrix} 0.447 & 0 \\ 0 & 0.089 \end{bmatrix} \begin{bmatrix} 1.3 \\ 3.6 \end{bmatrix} = \begin{bmatrix} 0.581 \\ 0.320 \end{bmatrix}
> $$
> Isso demonstra que o primeiro elemento do vetor resultante Ã© $\frac{1}{\sqrt{T}}$ multiplicado pela soma dos erros e o segundo elemento Ã© $\frac{1}{T^{3/2}}$ multiplicado pela soma dos erros multiplicados por t.

Agora, podemos analisar o comportamento assintÃ³tico deste termo.

**Teorema 1** Se $\epsilon_t$ sÃ£o variÃ¡veis aleatÃ³rias i.i.d. com mÃ©dia 0 e variÃ¢ncia $\sigma^2$, entÃ£o:
$$ Y_T (\hat{\beta}_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1}) $$
*Prova:* Pelo Teorema Central do Limite multivariado, temos que
$$ \begin{bmatrix} T^{-1/2} \sum_{t=1}^T \epsilon_t \\ T^{-3/2} \sum_{t=1}^T t \epsilon_t \end{bmatrix} \xrightarrow{d} N \left( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \sigma^2 \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} \right) = N(0, \sigma^2 Q) $$
Assim
$$ Y_T (\hat{\beta}_T - \beta)  = \left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right]  Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t \xrightarrow{d} Q^{-1} N(0, \sigma^2 Q)  = N(0, \sigma^2 Q^{-1}) $$
Onde usamos a convergÃªncia de $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \xrightarrow{p} Q^{-1}$ e o resultado anterior para o termo $Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$.

I. Pelo Teorema Central do Limite (TCL) multivariado:
$$
\frac{1}{\sqrt{T}} \sum_{t=1}^T \begin{bmatrix} \epsilon_t \\ \frac{t}{T} \epsilon_t \end{bmatrix} \xrightarrow{d} N \left( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \sigma^2 \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} \right)
$$
O termo $\frac{1}{\sqrt{T}}\sum_{t=1}^T \begin{bmatrix} \epsilon_t \\ \frac{t}{T} \epsilon_t \end{bmatrix}$ Ã© a mesma coisa que $\begin{bmatrix} T^{-1/2} \sum_{t=1}^T \epsilon_t \\ T^{-3/2} \sum_{t=1}^T t \epsilon_t \end{bmatrix}$.

II. Definindo a matriz $Q$ como:
$$
Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}
$$
Podemos reescrever o resultado do TCL como:
$$
\begin{bmatrix} T^{-1/2} \sum_{t=1}^T \epsilon_t \\ T^{-3/2} \sum_{t=1}^T t \epsilon_t \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q)
$$

III. Sabemos que:
$$
Y_T (\hat{\beta}_T - \beta) = \left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right]  Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t
$$

IV. TambÃ©m sabemos que:
$$
Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \xrightarrow{p} Q^{-1}
$$
V.  Assim, substituindo:
$$ Y_T (\hat{\beta}_T - \beta)  \xrightarrow{d} Q^{-1} N(0, \sigma^2 Q) = N(0, \sigma^2 Q^{-1}) $$
Portanto, $ Y_T (\hat{\beta}_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1})$
â– 
> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos simular um conjunto de dados com T=100, $\alpha=2$, $\delta=0.5$ e $\epsilon_t$ sendo um ruÃ­do branco normal com variÃ¢ncia $\sigma^2=1$.
>
> ```python
> import numpy as np
> import pandas as pd
> from numpy.linalg import inv
> from scipy.stats import norm
>
> # Set parameters
> T = 100
> alpha = 2
> delta = 0.5
> sigma = 1
>
> # Generate the data
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, sigma, T)
> y = alpha + delta * t + epsilon
>
> # Create the matrix X
> X = np.stack([np.ones(T), t], axis=1)
>
> # Calculate the OLS estimate
> beta_hat = inv(X.T @ X) @ X.T @ y
>
> # Calculate the true beta
> beta = np.array([alpha, delta])
>
> # Calculate Y_T
> YT = np.array([[np.sqrt(T), 0], [0, T**(3/2)]])
>
> # Calculate the scaled estimate
> scaled_beta_hat = YT @ (beta_hat - beta)
>
> # Calculate the theoretical covariance
> Q = np.array([[1, 1/2], [1/2, 1/3]])
> Q_inv = np.array([[4, -6], [-6, 12]])
> cov_beta_hat = sigma**2 * Q_inv
>
> # Display results
> print(f"Estimated Beta: {beta_hat}")
> print(f"True Beta: {beta}")
> print(f"Scaled Beta: {scaled_beta_hat}")
> print(f"Theoretical Covariance: {cov_beta_hat}")
> ```
>
> Este cÃ³digo simula os dados, calcula os estimadores OLS, o desvio dos estimadores, rescala este desvio usando $Y_T$ e calcula a matriz de covariÃ¢ncia teÃ³rica. A saÃ­da do cÃ³digo mostra o valor estimado de beta, o valor verdadeiro, o valor rescalonado de beta e a matriz de covariÃ¢ncia. Repetindo esta simulaÃ§Ã£o vÃ¡rias vezes, podemos verificar que a distribuiÃ§Ã£o empÃ­rica de $Y_T(\hat{\beta}_T - \beta)$ converge para $N(0, \sigma^2 Q^{-1})$. A matriz de covariÃ¢ncia $ \sigma^2 Q^{-1}$ permite construir intervalos de confianÃ§a para os estimadores $\alpha$ e $\delta$.

### ConclusÃ£o
A matriz $Y_T$ desempenha um papel fundamental na anÃ¡lise de modelos de regressÃ£o com tendÃªncias de tempo determinÃ­sticas. Ao prÃ©-multiplicar o desvio do estimador OLS, ela compensa as diferentes taxas de convergÃªncia dos parÃ¢metros, levando a distribuiÃ§Ãµes assintÃ³ticas bem definidas. A tÃ©cnica de transformar os dados usando $Y_T$ e a ideia de rescalonar por meio de matrizes de taxas de convergÃªncia, sÃ£o essenciais para realizar inferÃªncia estatÃ­stica precisa nesses modelos. Esta abordagem prepara o terreno para anÃ¡lises mais complexas, incluindo processos autoregressivos com tendÃªncias de tempo determinÃ­sticas e outros modelos nÃ£o estacionÃ¡rios que serÃ£o explorados nos prÃ³ximos capÃ­tulos [^1].

### ReferÃªncias
[^1]:  Processes with Deterministic Time Trends.
[^2]: 16.1. Asymptotic Distribution of OLS Estimates of the Simple Time Trend Model.
[^3]: where $\sum$ denotes summation for t = 1 through T.
[^4]: In contrast to the usual result for stationary regressions, for the matrix in (16.1.16], (1/T) $\sum_{1}$ x,x, diverges. To obtain a convergent matrix, [16.1.16] would have to be divided by TÂ³ rather than T:.
<!-- END -->
