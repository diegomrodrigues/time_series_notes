## Distribui√ß√£o Assint√≥tica da Soma Ponderada dos Erros e An√°lise da Vari√¢ncia
### Introdu√ß√£o
Nesta se√ß√£o, complementando a discuss√£o anterior sobre a matriz de rescalonamento $Y_T$ e sua aplica√ß√£o aos estimadores OLS em modelos de tend√™ncia temporal determin√≠stica [^1], vamos nos aprofundar na an√°lise do segundo termo da express√£o que define a distribui√ß√£o assint√≥tica desses estimadores. Especificamente, examinaremos o termo $Y_T^{-1}\sum_{t=1}^T x_t \epsilon_t$ e demonstraremos que, sob certas condi√ß√µes, ele converge para uma distribui√ß√£o gaussiana assint√≥tica, estabelecendo a base para an√°lises inferenciais subsequentes [^2]. Al√©m disso, analisaremos a vari√¢ncia do termo de erro ponderado e sua converg√™ncia.

### Conceitos Fundamentais
Na se√ß√£o anterior, estabelecemos que, para um modelo de regress√£o linear com tend√™ncia temporal determin√≠stica, como $y_t = \alpha + \delta t + \epsilon_t$, o vetor de estimadores OLS rescalonados pode ser expresso como:

$$ Y_T (\hat{\beta}_T - \beta) = \left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right]  Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t $$

Onde $Y_T$ √© a matriz de rescalonamento, $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$, $\beta = \begin{bmatrix} \alpha \\ \delta \end{bmatrix}$, e $\epsilon_t$ √© o termo de erro. J√° mostramos que o termo $\left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right]$ converge para uma matriz $Q^{-1}$, que √© dada por $\begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$.

Agora, vamos nos concentrar no segundo termo, $Y_T^{-1}\sum_{t=1}^T x_t \epsilon_t$, que pode ser escrito explicitamente como:

$$ Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} T^{-1/2} \sum_{t=1}^T \epsilon_t \\ T^{-3/2} \sum_{t=1}^T t \epsilon_t \end{bmatrix} $$
> üí° **Exemplo Num√©rico:** Para ilustrar, considere o caso com T=5, com erros $\epsilon_t$ sendo [0.5, -0.2, 0.8, -0.1, 0.3].
> $$ \sum_{t=1}^{5} \epsilon_t = 0.5 - 0.2 + 0.8 - 0.1 + 0.3 = 1.3 $$
> $$ \sum_{t=1}^{5} t \epsilon_t = (1)(0.5) + (2)(-0.2) + (3)(0.8) + (4)(-0.1) + (5)(0.3) = 0.5 - 0.4 + 2.4 - 0.4 + 1.5 = 3.6 $$
> Aplicando a matriz $Y_5^{-1}$:
> $$ Y_5^{-1} \sum_{t=1}^{5} x_t \epsilon_t = \begin{bmatrix} 5^{-1/2} & 0 \\ 0 & 5^{-3/2} \end{bmatrix} \begin{bmatrix} 1.3 \\ 3.6 \end{bmatrix} \approx \begin{bmatrix} 0.447 & 0 \\ 0 & 0.089 \end{bmatrix} \begin{bmatrix} 1.3 \\ 3.6 \end{bmatrix} \approx \begin{bmatrix} 0.581 \\ 0.320 \end{bmatrix} $$
> Isso ilustra que a primeira componente deste vetor √© uma soma dos erros, rescalonada por $\sqrt{T}$, e a segunda componente √© uma soma ponderada por $t$, rescalonada por $T^{3/2}$.

**Proposi√ß√£o 4:** Se $\epsilon_t$ √© uma sequ√™ncia de vari√°veis aleat√≥rias i.i.d. com m√©dia zero, vari√¢ncia $\sigma^2$, e quarto momento finito, ent√£o o vetor $Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$ converge para uma distribui√ß√£o gaussiana assint√≥tica com m√©dia zero e matriz de covari√¢ncia $\sigma^2 Q$, ou seja:

$$ Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t \xrightarrow{d} N(0, \sigma^2 Q) $$
Onde $Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$
*Prova:*
I.  Podemos analisar o vetor  $Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$ como um vetor de duas componentes:
$$  \begin{bmatrix} T^{-1/2} \sum_{t=1}^T \epsilon_t \\ T^{-3/2} \sum_{t=1}^T t \epsilon_t \end{bmatrix} $$
II. Pelo Teorema Central do Limite (TCL), sabemos que $T^{-1/2} \sum_{t=1}^T \epsilon_t$ converge em distribui√ß√£o para $N(0, \sigma^2)$.
III.  Para o segundo elemento, vamos reescrever $T^{-3/2} \sum_{t=1}^T t \epsilon_t$ como $\frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t$. A sequ√™ncia $\left\{ \frac{t}{T} \epsilon_t \right\}_{t=1}^T$ forma uma sequ√™ncia de diferen√ßas martingales, onde $\mathbb{E}\left[ \frac{t}{T} \epsilon_t | \epsilon_{t-1}, \epsilon_{t-2}, \dots \right] = \frac{t}{T}\mathbb{E}[\epsilon_t] = 0$. Assim, podemos usar o TCL para martingales (Proposi√ß√£o 7.8) para garantir a converg√™ncia para uma distribui√ß√£o normal.
IV. Para o segundo termo, a vari√¢ncia de $\frac{t}{T} \epsilon_t$ √© dada por $\mathbb{E} \left[ \left( \frac{t}{T}\epsilon_t \right)^2 \right] = \frac{t^2}{T^2} \mathbb{E}[\epsilon_t^2] = \sigma^2 \frac{t^2}{T^2}$. Assim, a vari√¢ncia de  $\frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t$ converge para:
$$
 \frac{1}{T} \sum_{t=1}^T \sigma^2 \frac{t^2}{T^2} = \frac{\sigma^2}{T^3}\sum_{t=1}^T t^2 \rightarrow \sigma^2/3
$$
V. Al√©m disso, pela Proposi√ß√£o 7.8, o termo $\frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t$  converge em distribui√ß√£o para uma normal com m√©dia zero e vari√¢ncia $\sigma^2/3$.
VI. A covari√¢ncia entre os dois termos, $T^{-1/2} \sum_{t=1}^T \epsilon_t$ e $T^{-3/2} \sum_{t=1}^T t \epsilon_t$, √© dada por
$$ \frac{1}{T^2} \mathbb{E}\left[\sum_{t=1}^T \epsilon_t \sum_{s=1}^T s \epsilon_s \right] = \frac{1}{T^2} \sum_{t=1}^T t \mathbb{E}[\epsilon_t^2] = \frac{\sigma^2}{T^2} \sum_{t=1}^T t \rightarrow \sigma^2/2 $$
VII. Combinando os resultados, obtemos que o vetor $Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$ converge em distribui√ß√£o para uma normal bivariada com m√©dia zero e matriz de covari√¢ncia
$$
\sigma^2 \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} = \sigma^2 Q
$$
Portanto, $ Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t \xrightarrow{d} N(0, \sigma^2 Q)$.
‚ñ†
Este resultado √© essencial pois estabelece a distribui√ß√£o assint√≥tica do termo $Y_T^{-1}\sum_{t=1}^T x_t \epsilon_t$, que, juntamente com a converg√™ncia da matriz $\left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right]$ para $Q^{-1}$, possibilita derivar a distribui√ß√£o assint√≥tica do vetor de estimadores OLS rescalonados.

> üí° **Exemplo Num√©rico:** Para visualizar a converg√™ncia, vamos gerar 1000 amostras de tamanho T=100 de um modelo com $\alpha=2$, $\delta=0.5$ e $\epsilon_t$ sendo um ru√≠do branco normal com $\sigma^2=1$. Calcularemos a m√©dia e a covari√¢ncia dos termos $T^{-1/2} \sum_{t=1}^T \epsilon_t$ e $T^{-3/2} \sum_{t=1}^T t \epsilon_t$ e compararemos com os valores te√≥ricos.
>
> ```python
> import numpy as np
> import pandas as pd
> from numpy.linalg import inv
> from scipy.stats import norm
>
> # Set parameters
> T = 100
> num_simulations = 1000
> alpha = 2
> delta = 0.5
> sigma = 1
>
> # Initialize arrays to store the terms
> term1_simulations = np.zeros(num_simulations)
> term2_simulations = np.zeros(num_simulations)
>
> # Generate the data and calculate terms for each simulation
> for i in range(num_simulations):
>     t = np.arange(1, T + 1)
>     epsilon = np.random.normal(0, sigma, T)
>     term1_simulations[i] = np.sum(epsilon) / np.sqrt(T)
>     term2_simulations[i] = np.sum(t * epsilon) / T**(3/2)
>
> # Calculate the empirical mean and covariance matrix
> empirical_mean = np.array([np.mean(term1_simulations), np.mean(term2_simulations)])
> empirical_cov = np.cov(term1_simulations, term2_simulations)
>
> # Calculate the theoretical covariance matrix
> Q = np.array([[1, 1/2], [1/2, 1/3]])
> theoretical_cov = sigma**2 * Q
>
> # Display results
> print(f"Empirical Mean: {empirical_mean}")
> print(f"Empirical Covariance:\n{empirical_cov}")
> print(f"Theoretical Covariance:\n{theoretical_cov}")
>
> ```
>
> O output mostrar√° que a m√©dia emp√≠rica do vetor $Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$ se aproxima de zero e que a covari√¢ncia emp√≠rica se aproxima da covari√¢ncia te√≥rica $\sigma^2Q$. Isso ilustra a converg√™ncia em distribui√ß√£o para uma normal multivariada, conforme demonstrado na proposi√ß√£o.

**Observa√ß√£o 4.1:** A Proposi√ß√£o 4 √© fundamental pois ela estabelece a distribui√ß√£o assint√≥tica do termo $Y_T^{-1}\sum_{t=1}^T x_t \epsilon_t$, que √© crucial para derivar a distribui√ß√£o assint√≥tica do vetor de estimadores OLS rescalonados. Em conjunto com os resultados anteriores, podemos concluir que o vetor de estimadores OLS rescalonado, $Y_T(\hat{\beta}_T - \beta)$, converge para uma distribui√ß√£o normal com m√©dia zero e matriz de covari√¢ncia $\sigma^2 Q^{-1}$, com $Q$ sendo a matriz que √© limite de $Y_T^{-1}(\sum_{t=1}^T x_t x_t')Y_T^{-1}$.

**Proposi√ß√£o 4.2**
Sob as mesmas condi√ß√µes da Proposi√ß√£o 4, a soma ponderada dos erros rescalonada, $T^{-1} \sum_{t=1}^T \frac{t}{T} \epsilon_t$, tamb√©m converge para uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\sigma^2/3$. Formalmente,
$$ T^{-1} \sum_{t=1}^T \frac{t}{T} \epsilon_t  \xrightarrow{d} N(0, \sigma^2/3) $$

*Prova:*
I.  Pela Proposi√ß√£o 4, sabemos que  $T^{-3/2} \sum_{t=1}^T t \epsilon_t$  converge para uma distribui√ß√£o $N(0, \sigma^2/3)$.
II.  Podemos reescrever $T^{-1} \sum_{t=1}^T \frac{t}{T} \epsilon_t$ como $T^{1/2} \left( T^{-3/2} \sum_{t=1}^T t \epsilon_t \right)$.
III. Uma vez que $T^{1/2} \rightarrow \infty$ e  $T^{-3/2} \sum_{t=1}^T t \epsilon_t$ converge para uma distribui√ß√£o $N(0, \sigma^2/3)$, o termo $T^{-1} \sum_{t=1}^T \frac{t}{T} \epsilon_t$ √© uma vers√£o rescalonada do segundo termo da Proposi√ß√£o 4.  Embora $T^{1/2}$ n√£o convirja para um valor, essa express√£o ainda √© util.
IV. Note que $T^{-1} \sum_{t=1}^T \frac{t}{T} \epsilon_t = \frac{1}{T} \sum_{t=1}^T \frac{t}{T} \epsilon_t =  \frac{1}{T}  \sum_{t=1}^T  \left( \frac{t}{T} \right) \epsilon_t$. Usando o TCL para martingales (Proposi√ß√£o 7.8), notamos que $\frac{t}{T}$ s√£o coeficientes que variam com $t$ e que a esperan√ßa condicional de $\frac{t}{T} \epsilon_t$ √© zero.
V. A vari√¢ncia do termo $\frac{1}{T}  \sum_{t=1}^T  \left( \frac{t}{T} \right) \epsilon_t$ converge para $\sigma^2/3$ quando $T\rightarrow \infty$, como demonstrado na Proposi√ß√£o 4.
Portanto,  $T^{-1} \sum_{t=1}^T \frac{t}{T} \epsilon_t  \xrightarrow{d} N(0, \sigma^2/3)$.
‚ñ†

> üí° **Exemplo Num√©rico:** Para demonstrar a converg√™ncia da Proposi√ß√£o 4.2, vamos simular 1000 amostras com T=100, os mesmos par√¢metros do exemplo anterior, e calcular a vari√¢ncia emp√≠rica de $T^{-1} \sum_{t=1}^T \frac{t}{T} \epsilon_t$.
> ```python
> import numpy as np
>
> # Parameters
> T = 100
> num_simulations = 1000
> sigma = 1
>
> # Initialize array
> term_simulations = np.zeros(num_simulations)
>
> # Simulations
> for i in range(num_simulations):
>     t = np.arange(1, T + 1)
>     epsilon = np.random.normal(0, sigma, T)
>     term_simulations[i] = np.sum((t/T)*epsilon) / T
>
> # Calculate empirical variance
> empirical_variance = np.var(term_simulations)
>
> # Calculate theoretical variance
> theoretical_variance = sigma**2 / 3
>
> # Print results
> print(f"Empirical Variance: {empirical_variance}")
> print(f"Theoretical Variance: {theoretical_variance}")
> ```
>
> Os resultados mostram que a vari√¢ncia emp√≠rica de $T^{-1} \sum_{t=1}^T \frac{t}{T} \epsilon_t$ se aproxima de $\sigma^2/3$, comprovando a proposi√ß√£o.

Agora, vamos analisar a vari√¢ncia do segundo elemento do termo $Y_T^{-1}\sum_{t=1}^T x_t \epsilon_t$.

**Proposi√ß√£o 5:** A vari√¢ncia do segundo elemento do vetor $Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$,  dado por $T^{-3/2} \sum_{t=1}^T t \epsilon_t$, converge assintoticamente para $\sigma^2/3$.
*Prova:*
I. A vari√¢ncia do segundo termo √© dada por:
$$ \text{Var} \left( T^{-3/2} \sum_{t=1}^T t \epsilon_t \right) = T^{-3} \text{Var} \left( \sum_{t=1}^T t \epsilon_t \right) $$
II. Como as vari√°veis aleat√≥rias $\epsilon_t$ s√£o independentes, a vari√¢ncia da soma √© a soma das vari√¢ncias:
$$  T^{-3} \sum_{t=1}^T \text{Var}(t \epsilon_t) = T^{-3} \sum_{t=1}^T t^2 \text{Var}(\epsilon_t) = T^{-3} \sigma^2 \sum_{t=1}^T t^2 $$
III. Sabemos que $\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6}$, ent√£o:
$$  T^{-3} \sigma^2 \frac{T(T+1)(2T+1)}{6} = \sigma^2 \frac{1}{6} \frac{T(T+1)(2T+1)}{T^3} $$
IV.  Conforme $T \rightarrow \infty$, $\frac{T(T+1)(2T+1)}{T^3} \rightarrow 2$, assim:
$$  \sigma^2 \frac{1}{6} \frac{T(T+1)(2T+1)}{T^3} \rightarrow \sigma^2 \frac{1}{6} * 2 = \frac{\sigma^2}{3} $$
Portanto, a vari√¢ncia converge para $\sigma^2/3$.
‚ñ†
Este resultado mostra como a vari√¢ncia do termo de erro ponderado por t se comporta quando $T$ tende ao infinito, confirmando que, ap√≥s o devido rescalonamento, temos uma distribui√ß√£o assint√≥tica bem definida.

> üí° **Exemplo Num√©rico:** Para confirmar a converg√™ncia da vari√¢ncia, vamos calcular a vari√¢ncia emp√≠rica de $T^{-3/2} \sum_{t=1}^T t \epsilon_t$ para v√°rios valores de T. Usaremos 1000 simula√ß√µes para cada T e compararemos a vari√¢ncia emp√≠rica com $\sigma^2/3$.
> ```python
> import numpy as np
>
> # Parameters
> num_simulations = 1000
> sigma = 1
> T_values = [50, 100, 200, 500, 1000]
>
> # Loop through different T values
> for T in T_values:
>     term_simulations = np.zeros(num_simulations)
>     for i in range(num_simulations):
>         t = np.arange(1, T + 1)
>         epsilon = np.random.normal(0, sigma, T)
>         term_simulations[i] = np.sum(t * epsilon) / T**(3/2)
>
>     empirical_variance = np.var(term_simulations)
>     theoretical_variance = sigma**2 / 3
>     print(f"T = {T}: Empirical Variance = {empirical_variance:.4f}, Theoretical Variance = {theoretical_variance:.4f}")
> ```
> Este c√≥digo mostra como a vari√¢ncia emp√≠rica se aproxima do valor te√≥rico de $\sigma^2/3$ √† medida que $T$ aumenta, demonstrando a validade da proposi√ß√£o.

**Observa√ß√£o 5.1:** A Proposi√ß√£o 5 √© um resultado espec√≠fico que estabelece a vari√¢ncia do segundo componente do vetor $Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$. Isso demonstra a converg√™ncia da vari√¢ncia da soma ponderada dos erros rescalonada para um valor finito, um resultado chave na an√°lise de converg√™ncia.

**Lema 5.2:** A vari√¢ncia do primeiro elemento do vetor $Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$, dado por $T^{-1/2} \sum_{t=1}^T \epsilon_t$, converge para $\sigma^2$.
*Prova:*
I. A vari√¢ncia do primeiro termo √© dada por:
$$ \text{Var} \left( T^{-1/2} \sum_{t=1}^T \epsilon_t \right) = T^{-1} \text{Var} \left( \sum_{t=1}^T \epsilon_t \right) $$
II. Como as vari√°veis aleat√≥rias $\epsilon_t$ s√£o independentes, a vari√¢ncia da soma √© a soma das vari√¢ncias:
$$  T^{-1} \sum_{t=1}^T \text{Var}(\epsilon_t) = T^{-1} \sum_{t=1}^T \sigma^2 = T^{-1} T \sigma^2 = \sigma^2 $$
Portanto, a vari√¢ncia converge para $\sigma^2$.
‚ñ†
Este lema mostra que a vari√¢ncia do primeiro componente do vetor converge para a vari√¢ncia dos erros $\epsilon_t$.

> üí° **Exemplo Num√©rico:** Para ilustrar o Lema 5.2, vamos calcular a vari√¢ncia emp√≠rica do primeiro componente do vetor, $T^{-1/2} \sum_{t=1}^T \epsilon_t$, para diferentes valores de T usando simula√ß√µes.
> ```python
> import numpy as np
>
> # Parameters
> num_simulations = 1000
> sigma = 1
> T_values = [50, 100, 200, 500, 1000]
>
> # Loop through different T values
> for T in T_values:
>    term_simulations = np.zeros(num_simulations)
>    for i in range(num_simulations):
>        epsilon = np.random.normal(0, sigma, T)
>        term_simulations[i] = np.sum(epsilon) / np.sqrt(T)
>
>    empirical_variance = np.var(term_simulations)
>    theoretical_variance = sigma**2
>    print(f"T = {T}: Empirical Variance = {empirical_variance:.4f}, Theoretical Variance = {theoretical_variance:.4f}")
> ```
> O resultado mostrar√° que a vari√¢ncia emp√≠rica se aproxima do valor te√≥rico de $\sigma^2$ conforme T aumenta, demonstrando o lema.

### Conclus√£o
Nesta se√ß√£o, mostramos que o termo $Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$ converge assintoticamente para uma distribui√ß√£o gaussiana multivariada com m√©dia zero e matriz de covari√¢ncia $\sigma^2 Q$, demonstrando sua estabilidade assint√≥tica [^2]. Calculamos explicitamente o valor limite da vari√¢ncia dos erros rescalonados, com e sem peso temporal, mostrando que o processo de rescalonamento √© crucial para a obten√ß√£o de resultados assintoticamente est√°veis. A an√°lise da vari√¢ncia do termo de erro ponderado tamb√©m confirma a validade da teoria assint√≥tica e prepara o terreno para testes de hip√≥teses e infer√™ncia estat√≠stica em modelos de s√©ries temporais com tend√™ncias determin√≠sticas. Os resultados apresentados consolidam nossa compreens√£o sobre o comportamento dos estimadores OLS em presen√ßa de tend√™ncias determin√≠sticas e fornecem a base te√≥rica para a realiza√ß√£o de infer√™ncias v√°lidas.

### Refer√™ncias
[^1]: Rescaling OLS Estimates with Matrix $Y_T$.
[^2]: From [16.1.19] and [16.1.24], the asymptotic distribution of [16.1.18] can be calculated as in Example 7.5 of Chapter 7.
<!-- END -->
