## Distribui√ß√£o Assint√≥tica Bivariada Gaussiana e Infer√™ncia Estat√≠stica

### Introdu√ß√£o
Expandindo o conceito apresentado na se√ß√£o anterior, onde introduzimos a matriz de rescalonamento $Y_T$ e as taxas de converg√™ncia distintas dos estimadores OLS em modelos com tend√™ncia de tempo determin√≠stica [^1], este cap√≠tulo foca em como as distribui√ß√µes assint√≥ticas obtidas podem ser utilizadas para infer√™ncia estat√≠stica. Especificamente, vamos demonstrar que qualquer combina√ß√£o linear dos estimadores rescalonados converge para uma distribui√ß√£o Gaussiana bivariada assint√≥tica [^2], possibilitando a realiza√ß√£o de testes de hip√≥teses e constru√ß√£o de intervalos de confian√ßa. Esta propriedade √© fundamental para realizar infer√™ncias estat√≠sticas v√°lidas em modelos com tend√™ncias temporais determin√≠sticas [^2].

### Conceitos Fundamentais
Na se√ß√£o anterior, derivamos que, sob certas condi√ß√µes, o vetor rescalonado dos estimadores OLS, dado por $Y_T(\hat{\beta}_T - \beta)$, converge em distribui√ß√£o para uma normal multivariada com m√©dia zero e matriz de covari√¢ncia $\sigma^2 Q^{-1}$, ou seja:

$$ Y_T (\hat{\beta}_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1}) $$
Onde $Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}$, $\beta = \begin{bmatrix} \alpha \\ \delta \end{bmatrix}$ e $Q^{-1}$ √© a inversa da matriz
$ Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} $, que √© dada por $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$ [^1].

Agora, consideremos uma combina√ß√£o linear desses estimadores, dada por:

$$ L = \lambda_1 (\sqrt{T}(\hat{\alpha}_T - \alpha)) + \lambda_2(T^{3/2}(\hat{\delta}_T - \delta)) $$
Onde $\lambda = [\lambda_1, \lambda_2]^T$ √© um vetor de constantes. Podemos reescrever isso em nota√ß√£o matricial como:

$$ L = \lambda^T Y_T(\hat{\beta}_T - \beta) $$

**Lema 2:** Se $Y_T (\hat{\beta}_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1})$, ent√£o $L = \lambda^T Y_T(\hat{\beta}_T - \beta)$ converge em distribui√ß√£o para uma normal univariada com m√©dia zero e vari√¢ncia $\sigma^2 \lambda^T Q^{-1} \lambda$.

*Prova:*

I. Sabemos que $Y_T(\hat{\beta}_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1})$. Isso significa que existe um vetor aleat√≥rio $Z \sim N(0, \sigma^2 Q^{-1})$ tal que $Y_T(\hat{\beta}_T - \beta)$ √© assintoticamente distribu√≠do como $Z$.
II. Consideremos a transforma√ß√£o linear $L = \lambda^T Y_T(\hat{\beta}_T - \beta)$. Assim, usando a distribui√ß√£o assint√≥tica de $Y_T(\hat{\beta}_T - \beta)$, podemos escrever $L \approx \lambda^T Z$.
III. Como $Z \sim N(0, \sigma^2 Q^{-1})$ e $\lambda$ √© um vetor de constantes, sabemos que $\lambda^T Z$ √© uma combina√ß√£o linear de vari√°veis normais. Pela propriedade de combina√ß√µes lineares de vari√°veis normais, $\lambda^T Z$ segue tamb√©m uma distribui√ß√£o normal.
IV. A m√©dia de $\lambda^T Z$ √© dada por $\lambda^T E[Z] = \lambda^T 0 = 0$, pois a m√©dia de $Z$ √© zero.
V. A vari√¢ncia de $\lambda^T Z$ √© dada por $Var(\lambda^T Z) = \lambda^T Var(Z) \lambda = \lambda^T (\sigma^2 Q^{-1}) \lambda = \sigma^2 \lambda^T Q^{-1} \lambda$.

Portanto, $L = \lambda^T Y_T(\hat{\beta}_T - \beta) \xrightarrow{d} N(0, \sigma^2 \lambda^T Q^{-1} \lambda)$.
‚ñ†

Este resultado demonstra que qualquer combina√ß√£o linear dos estimadores rescalonados tamb√©m converge para uma distribui√ß√£o normal, o que √© crucial para realizar testes de hip√≥teses e construir intervalos de confian√ßa [^2].

A express√£o para a vari√¢ncia da combina√ß√£o linear $L$ √© dada por:
$$ \text{Var}(L) = \sigma^2 \lambda^T Q^{-1} \lambda $$
Substituindo a matriz $Q^{-1}$ temos:
$$ \text{Var}(L) = \sigma^2 \begin{bmatrix} \lambda_1 & \lambda_2 \end{bmatrix} \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} \begin{bmatrix} \lambda_1 \\ \lambda_2 \end{bmatrix} $$
$$ \text{Var}(L) = \sigma^2 (4\lambda_1^2 -12\lambda_1\lambda_2 + 12\lambda_2^2) $$

**Observa√ß√£o 2.1:** √â interessante notar que a vari√¢ncia da combina√ß√£o linear $L$ depende da escolha espec√≠fica de $\lambda_1$ e $\lambda_2$.  Diferentes combina√ß√µes lineares podem levar a diferentes vari√¢ncias assint√≥ticas. O termo $\lambda^T Q^{-1} \lambda$ √© uma forma quadr√°tica que determina a magnitude da vari√¢ncia de $L$, dado o valor de $\sigma^2$.

> üí° **Exemplo Num√©rico:** Suponha que temos uma s√©rie temporal com $T=100$ observa√ß√µes e estimamos um modelo de regress√£o com uma tend√™ncia linear: $y_t = \alpha + \delta t + u_t$.  Obtemos as seguintes estimativas para os par√¢metros: $\hat{\alpha} = 2.5$ e $\hat{\delta} = 0.1$. Suponha tamb√©m que estimamos a vari√¢ncia do erro como $\hat{\sigma}^2 = 0.25$. Queremos testar a hip√≥tese de que a tend√™ncia √© igual a $0.1$, ou seja, $H_0: \delta = 0.1$ (que √© nosso valor estimado, apenas para fins de ilustra√ß√£o). Uma poss√≠vel combina√ß√£o linear a ser testada seria $\lambda_1 = 0$ e $\lambda_2 = 1$,  ent√£o:
>
> $$L = 0 \cdot (\sqrt{T}(\hat{\alpha}_T - \alpha)) + 1 \cdot (T^{3/2}(\hat{\delta}_T - \delta)) = T^{3/2}(\hat{\delta}_T - \delta)$$
>
> Nesse caso, $\text{Var}(L) = \sigma^2 (12\lambda_2^2) = 12 \sigma^2$. Uma estimativa dessa vari√¢ncia √© $12 \hat{\sigma}^2 = 12 * 0.25 = 3$.  A estat√≠stica de teste para $H_0: \delta = 0.1$ √© dada por:
> $$
>  \frac{T^{3/2}(\hat{\delta}_T - \delta)}{\sqrt{12\sigma^2}}
> $$
> Usando o valor observado de $\hat{\delta} = 0.1$ e sob a hip√≥tese nula $\delta = 0.1$, temos:
> $$
> \frac{100^{3/2}(0.1 - 0.1)}{\sqrt{12 \times 0.25}} = \frac{1000(0)}{\sqrt{3}} = 0
> $$
> O valor observado da estat√≠stica de teste √© zero. Como a distribui√ß√£o assint√≥tica de $L$ √© normal com m√©dia zero e vari√¢ncia $12\sigma^2$, essa estat√≠stica de teste, ap√≥s ser dividida pelo seu desvio padr√£o estimado, segue uma distribui√ß√£o normal padr√£o.  Como o valor observado da estat√≠stica de teste √© zero, n√£o temos evid√™ncia para rejeitar a hip√≥tese nula de que $\delta = 0.1$, dado o nosso resultado amostral. Note que se o valor verdadeiro de $\delta$ fosse outro, a estat√≠stica teria um valor n√£o nulo que seria usado para tomar uma decis√£o sobre a hip√≥tese nula.
>
> Agora suponha que desejamos testar a hip√≥tese de que $\alpha$ e $\delta$ se relacionam de tal forma que $2\alpha + 10\delta = 8$, ou seja, $H_0: 2\alpha + 10\delta = 8$.  Podemos expressar isso como uma combina√ß√£o linear dos estimadores, rescalonando cada estimador para ter taxas de converg√™ncia similares. Para isso, definimos $\lambda_1 = 2 \sqrt{T}$ e $\lambda_2 = 10 T^{3/2}$, assim
>
>  $$ L = 2\sqrt{T} (\hat{\alpha}_T - \alpha) + 10 T^{3/2}(\hat{\delta}_T - \delta) $$
> Sob a hip√≥tese nula $2\alpha + 10\delta = 8$, temos que $L \xrightarrow{d} N(0, \sigma^2 \lambda^T Q^{-1} \lambda)$. Substituindo os valores de $\lambda$ na vari√¢ncia, obtemos:
> $$ \text{Var}(L) = \sigma^2 [4 (2\sqrt{T})^2 - 12 (2\sqrt{T}) (10T^{3/2}) + 12 (10T^{3/2})^2] $$
> $$ \text{Var}(L) = \sigma^2 [16T - 240T^2 + 1200T^3] = \sigma^2 T [16 -240T + 1200T^2] $$
>  Isso significa que a estat√≠stica $L/ \sqrt{\text{Var}(L)}$ tem uma distribui√ß√£o assint√≥tica $N(0, 1)$, permitindo testar a hip√≥tese nula. Conforme $T \rightarrow \infty$, o termo $1200T^3$ domina, e a vari√¢ncia se comporta aproximadamente como $1200 \sigma^2 T^3$, logo, uma estat√≠stica de teste conveniente seria
>  $$ \frac{2\sqrt{T}(\hat{\alpha}_T - \alpha) + 10T^{3/2}(\hat{\delta}_T - \delta)}{\sqrt{1200 \sigma^2 T^3}} \approx \frac{2\hat{\alpha}_T + 10\hat{\delta}_T - 8}{\sqrt{1200 \sigma^2/T^2}} \xrightarrow{d} N(0,1) $$
> Note que, a estat√≠stica convergir para uma normal padr√£o significa que podemos usar esta estat√≠stica para fazer infer√™ncia estat√≠stica.

Em resumo, a combina√ß√£o linear $L$ tem as seguintes propriedades:
1. **M√©dia Assint√≥tica:** $E[L] \rightarrow 0$
2. **Vari√¢ncia Assint√≥tica:** $Var(L) \rightarrow \sigma^2 \lambda^T Q^{-1} \lambda$
3. **Distribui√ß√£o Assint√≥tica:** $L \xrightarrow{d} N(0, \sigma^2 \lambda^T Q^{-1} \lambda)$

A distribui√ß√£o assint√≥tica de combina√ß√µes lineares de par√¢metros permite a realiza√ß√£o de testes de hip√≥teses e infer√™ncia estat√≠stica para fun√ß√µes lineares de $\alpha$ e $\delta$.

**Teorema 2:** Qualquer combina√ß√£o linear dos elementos do vetor $\begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix}$ converge para uma distribui√ß√£o gaussiana assint√≥tica univariada.

*Prova:*
I. Seja $L = \lambda^T Y_T(\hat{\beta}_T - \beta)$ uma combina√ß√£o linear dos elementos de $Y_T(\hat{\beta}_T - \beta)$, onde $\lambda$ √© um vetor de constantes.
II. Pelo Lema 2, sabemos que $L$ converge em distribui√ß√£o para uma distribui√ß√£o normal univariada com m√©dia zero e vari√¢ncia $\sigma^2 \lambda^T Q^{-1} \lambda$.
III. Como o Lema 2 √© v√°lido para qualquer vetor $\lambda$, segue que qualquer combina√ß√£o linear dos elementos de $Y_T(\hat{\beta}_T - \beta)$ converge para uma distribui√ß√£o gaussiana assint√≥tica univariada.
‚ñ†

Este resultado √© essencial pois garante que, mesmo que as estimativas de $\alpha$ e $\delta$ individualmente tenham taxas de converg√™ncia diferentes, qualquer combina√ß√£o linear delas, devidamente rescalonada, converge para uma distribui√ß√£o normal, permitindo realizar testes estat√≠sticos significativos sobre essas combina√ß√µes.

**Corol√°rio 2.1:** Uma consequ√™ncia direta do Teorema 2 √© que, ao escolher um vetor $\lambda$ espec√≠fico, podemos construir testes de hip√≥teses para combina√ß√µes lineares dos par√¢metros $\alpha$ e $\delta$. Em particular, se tivermos uma hip√≥tese nula da forma $H_0 : \lambda_1 \alpha + \lambda_2 \delta = c$, podemos construir uma estat√≠stica de teste que converge para uma distribui√ß√£o normal padr√£o sob $H_0$, utilizando a distribui√ß√£o assint√≥tica de $L$.

### Conclus√£o
A propriedade de que qualquer combina√ß√£o linear dos estimadores rescalonados converge para uma distribui√ß√£o gaussiana bivariada assint√≥tica √© crucial para realizar infer√™ncia estat√≠stica em modelos com tend√™ncias de tempo determin√≠sticas [^2]. Este resultado, combinado com o conhecimento das taxas de converg√™ncia espec√≠ficas de cada par√¢metro, permite construir intervalos de confian√ßa e realizar testes de hip√≥teses sobre combina√ß√µes dos par√¢metros, como o intercepto e o coeficiente da tend√™ncia temporal [^2]. A capacidade de realizar infer√™ncia precisa em modelos com tend√™ncias de tempo √© fundamental para a modelagem e previs√£o de s√©ries temporais. O uso da matriz de rescalonamento $Y_T$ e a obten√ß√£o da distribui√ß√£o normal assint√≥tica s√£o elementos chave para essa an√°lise, que ser√° expandida em cap√≠tulos futuros.

### Refer√™ncias
[^1]: Rescaling OLS Estimates with Matrix $Y_T$.
[^2]: From [16.1.19] and [16.1.24], the asymptotic distribution of [16.1.18] can be calculated as in Example 7.5 of Chapter 7.
## T√≠tulo: Processos Univariados com Ra√≠zes Unit√°rias
### Introdu√ß√£o
Como vimos anteriormente, o modelo AR(1) √© dado por $y_t = \rho y_{t-1} + u_t$ [^2], onde $u_t$ √© ru√≠do branco. No entanto, a an√°lise da se√ß√£o anterior √© feita assumindo que $|\rho| < 1$, o que garante a estacionariedade do processo. Nesta se√ß√£o, iremos focar nos casos onde $\rho = 1$, o que leva a um processo n√£o estacion√°rio chamado random walk. O estudo de processos com raiz unit√°ria √© crucial em s√©ries temporais, pois muitas s√©ries econ√¥micas e financeiras exibem comportamento n√£o estacion√°rio, que pode ser modelado com processos deste tipo.

### Conceitos Fundamentais
#### As Distribui√ß√µes Assint√≥ticas de Estimadores OLS em Processos com Ra√≠zes Unit√°rias
Ao contr√°rio dos processos estacion√°rios, onde os estimadores OLS convergem em taxa $T^{-1/2}$ para uma distribui√ß√£o normal, em processos com ra√≠zes unit√°rias, a taxa de converg√™ncia √© diferente e a distribui√ß√£o assint√≥tica n√£o √© normal. A distribui√ß√£o dos estimadores neste caso, envolve funcionais de movimento Browniano.
A ideia chave √© que, quando $\rho = 1$, temos um random walk, onde $y_t = y_{t-1} + u_t$. Se reescrevermos $y_t$ iterativamente, temos $y_t = \sum_{i=1}^{t} u_i + y_0$. Nesse caso, a vari√¢ncia de $y_t$ aumenta com o tempo, e esse crescimento √© o que gera taxas de converg√™ncia diferentes para os estimadores.

#### Movimento Browniano
O movimento Browniano √© um processo estoc√°stico cont√≠nuo no tempo, caracterizado por ter incrementos independentes e gaussianos com vari√¢ncia proporcional ao tempo. Formalmente, seja $B(t)$ o movimento Browniano padr√£o, temos que:
1. $B(0) = 0$
2. $B(t)$ √© cont√≠nuo em $t$
3. Para $0 \le s < t$, $B(t) - B(s)$ √© uma vari√°vel aleat√≥ria normal com m√©dia zero e vari√¢ncia $t-s$.
4. Para $0 \le t_1 < t_2 < \dots < t_n$, os incrementos $B(t_2) - B(t_1), B(t_3) - B(t_2), \ldots, B(t_n) - B(t_{n-1})$ s√£o independentes.

A representa√ß√£o do estimador OLS em termos do movimento Browniano permite uma melhor compreens√£o das suas propriedades assint√≥ticas.

#### Funcionais do Movimento Browniano
Os funcionais do movimento Browniano s√£o fun√ß√µes que tomam o caminho do movimento Browniano como entrada e produzem um valor num√©rico. Os estimadores OLS em processos de raiz unit√°ria, quando rescalonados e aproximados, podem ser expressos em termos destes funcionais. Em particular, as distribui√ß√µes assint√≥ticas dos estimadores envolvem integrais estoc√°sticas do movimento Browniano.
Por exemplo, a integral $\int_0^1 B(r) dr$ √© um funcional do movimento Browniano que pode aparecer na distribui√ß√£o assint√≥tica do estimador.

#### Deriva√ß√£o da Distribui√ß√£o Assint√≥tica para o Caso de Random Walk Simples
Vamos analisar o caso mais simples de um random walk: $y_t = y_{t-1} + u_t$, com $u_t \sim i.i.d.(0, \sigma^2)$. O estimador OLS do par√¢metro $\rho$ √© dado por
$$
\hat{\rho}_T = \frac{\sum_{t=1}^T y_{t-1}y_t}{\sum_{t=1}^T y_{t-1}^2}
$$
No caso da raiz unit√°ria, esse estimador n√£o converge para uma distribui√ß√£o normal com taxa $T^{1/2}$ como nos casos estacion√°rios. √â necess√°rio realizar uma an√°lise assint√≥tica mais cuidadosa.
Substituindo $y_t = y_{t-1} + u_t$, obtemos:
$$
\hat{\rho}_T = \frac{\sum_{t=1}^T y_{t-1}^2 + \sum_{t=1}^T y_{t-1}u_t}{\sum_{t=1}^T y_{t-1}^2} = 1 + \frac{\sum_{t=1}^T y_{t-1}u_t}{\sum_{t=1}^T y_{t-1}^2}
$$
Em vez de olhar para $\hat{\rho}_T$, vamos analisar a distribui√ß√£o assint√≥tica de $T(\hat{\rho}_T - 1)$. Usando argumentos de converg√™ncia em distribui√ß√£o e utilizando a teoria de funcionais do movimento browniano, pode ser demonstrado que
$$
T(\hat{\rho}_T - 1) \xrightarrow{d} \frac{\frac{1}{2}(W(1)^2 - 1)}{\int_0^1 W(r)^2 dr}
$$
onde $W(r)$ √© um movimento Browniano padr√£o. Essa √© uma distribui√ß√£o n√£o-normal e demonstra que a converg√™ncia de $\hat{\rho}_T$ para o verdadeiro valor de $\rho$ ocorre a uma taxa $T$ e n√£o $T^{1/2}$.
Essa an√°lise destaca a necessidade de usar ferramentas diferentes para analisar processos n√£o estacion√°rios.

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal simulada de um random walk, onde $y_t = y_{t-1} + u_t$, com $u_t$ sendo ru√≠do branco com $\sigma^2 = 1$, e $y_0 = 0$. Simulamos $T = 100$ observa√ß√µes, e estimamos $\rho$ usando OLS. A estimativa de $\rho$ ser√°, em geral, muito pr√≥xima de 1, dado que o processo tem raiz unit√°ria. Agora vamos calcular e analisar $T(\hat{\rho}_T - 1)$ e vamos comparar com a distribui√ß√£o te√≥rica apresentada.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> np.random.seed(42) # para reproducibilidade
> T = 100
> u = np.random.normal(0, 1, T)
> y = np.zeros(T)
> for t in range(1, T):
>     y[t] = y[t-1] + u[t]
>
> # Estima√ß√£o OLS
> y_lag = y[:-1]
> y_current = y[1:]
> rho_hat = np.sum(y_lag * y_current) / np.sum(y_lag**2)
>
> stat = T*(rho_hat - 1)
> print(f"Estimativa de rho: {rho_hat:.4f}")
> print(f"T(rho_hat - 1): {stat:.4f}")
>
> # Simula√ß√£o da distribui√ß√£o assint√≥tica (muitas simula√ß√µes)
> num_sim = 1000
> sim_stats = np.zeros(num_sim)
> for i in range(num_sim):
>     u_sim = np.random.normal(0, 1, T)
>     y_sim = np.zeros(T)
>     for t in range(1, T):
>         y_sim[t] = y_sim[t-1] + u_sim[t]
>
>     y_lag_sim = y_sim[:-1]
>     y_current_sim = y_sim[1:]
>     rho_hat_sim = np.sum(y_lag_sim * y_current_sim) / np.sum(y_lag_sim**2)
>     sim_stats[i] = T*(rho_hat_sim - 1)
>
> # Plot do histograma
> plt.hist(sim_stats, bins = 50, density=True, alpha = 0.7, label = "Simulada")
> plt.axvline(stat, color = "red", linestyle='dashed', linewidth=1, label = "Valor amostral")
> plt.title("Distribui√ß√£o Emp√≠rica de T(rho_hat - 1)")
> plt.xlabel("Valor de T(rho_hat - 1)")
> plt.ylabel("Densidade")
> plt.legend()
> plt.show()
>
> ```
> Este c√≥digo simula um random walk e estima $\rho$ atrav√©s de OLS, calcula $T(\hat{\rho}_T - 1)$ e simula a distribui√ß√£o assint√≥tica realizando muitas estima√ß√µes do mesmo processo. O histograma desta distribui√ß√£o simulada pode ser visualizado e comparado com o valor da estat√≠stica obtida pela simula√ß√£o inicial. A distribui√ß√£o simulada resultante n√£o √© normal, ilustrando a discuss√£o te√≥rica.
>

**Proposi√ß√£o 3:** A distribui√ß√£o assint√≥tica de $T(\hat{\rho}_T - 1)$ no caso do random walk simples √© um caso particular de uma fam√≠lia de distribui√ß√µes que surge quando analisamos estimadores em processos com ra√≠zes unit√°rias. Essa distribui√ß√£o n√£o √© normal e √© caracterizada por funcionais do movimento Browniano, o que torna a an√°lise de processos com ra√≠zes unit√°rias mais complexa.
### Conclus√£o
Nesta se√ß√£o, estabelecemos os fundamentos para analisar processos com ra√≠zes unit√°rias. Introduzimos o conceito de movimento Browniano e sua import√¢ncia para a descri√ß√£o das distribui√ß√µes assint√≥ticas dos estimadores OLS nestes casos. Vimos que a taxa de converg√™ncia dos estimadores em processos com ra√≠zes unit√°rias √© diferente daquela dos processos estacion√°rios. Na pr√≥xima se√ß√£o, vamos construir testes de raiz unit√°ria a partir dos conceitos apresentados, baseando-nos na distribui√ß√£o n√£o normal de $T(\hat{\rho}_T - 1)$.
### Refer√™ncias
[^1]: Rescaling OLS Estimates with Matrix $Y_T$.
[^2]: From [16.1.19] and [16.1.24], the asymptotic distribution of [16.1.18] can be calculated as in Example 7.5 of Chapter 7.
<!-- END -->
