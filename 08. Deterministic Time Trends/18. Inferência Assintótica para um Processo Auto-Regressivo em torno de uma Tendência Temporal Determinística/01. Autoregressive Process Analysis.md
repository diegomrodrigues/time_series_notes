## Infer√™ncia Assint√≥tica para um Processo Auto-Regressivo em torno de uma Tend√™ncia Temporal Determin√≠stica

### Introdu√ß√£o
Expandindo a discuss√£o sobre processos com tend√™ncias temporais determin√≠sticas, este cap√≠tulo explora a infer√™ncia assint√≥tica para um processo auto-regressivo em torno de uma tend√™ncia temporal determin√≠stica. Como vimos anteriormente, os coeficientes em modelos de regress√£o com tend√™ncias temporais determin√≠sticas apresentam taxas de converg√™ncia assint√≥tica distintas [^1]. A an√°lise de processos auto-regressivos com tend√™ncias temporais requer t√©cnicas espec√≠ficas para lidar com essas diferentes taxas de converg√™ncia. Em particular, a t√©cnica de Sims, Stock e Watson introduz uma transforma√ß√£o dos regressores que facilita a obten√ß√£o de distribui√ß√µes assint√≥ticas para os estimadores OLS.

### Conceitos Fundamentais
O objetivo principal desta se√ß√£o √© detalhar como lidar com as particularidades da infer√™ncia assint√≥tica em modelos auto-regressivos com tend√™ncias temporais determin√≠sticas, utilizando a transforma√ß√£o dos regressores. O modelo auto-regressivo geral em torno de uma tend√™ncia temporal determin√≠stica √© dado por [^1]:
$$ y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \epsilon_t $$
onde $\epsilon_t$ √© um ru√≠do branco i.i.d com m√©dia zero, vari√¢ncia $\sigma^2$ e quarto momento finito. As ra√≠zes de $1 - \phi_1 z - \phi_2 z^2 - \dots - \phi_p z^p = 0$ est√£o fora do c√≠rculo unit√°rio. Os estimadores OLS dos coeficientes, $\hat{\alpha}_T$, $\hat{\delta}_T$ e $\hat{\phi}_{i,T}$, s√£o obtidos atrav√©s da estima√ß√£o por m√≠nimos quadrados ordin√°rios.

> üí° **Exemplo Num√©rico:** Vamos considerar um modelo AR(1) simples com tend√™ncia temporal: $y_t = \alpha + \delta t + \phi_1 y_{t-1} + \epsilon_t$. Suponha que temos os seguintes valores verdadeiros para os par√¢metros: $\alpha = 1$, $\delta = 0.2$, e $\phi_1 = 0.7$. Geraremos uma s√©rie temporal de $T=100$ pontos e usaremos esses par√¢metros para simular dados para o nosso exemplo.

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Par√¢metros verdadeiros
alpha_true = 1
delta_true = 0.2
phi1_true = 0.7
sigma_epsilon = 0.5 # Desvio padr√£o do ru√≠do

# Tamanho da amostra
T = 100

# Gerar ru√≠do branco
np.random.seed(42) # Para reprodutibilidade
epsilon = np.random.normal(0, sigma_epsilon, T)

# Inicializar y
y = np.zeros(T)
y[0] = alpha_true + delta_true * 1 + epsilon[0] # Primeiro valor com um valor inicial razo√°vel para y

# Gerar a s√©rie temporal
for t in range(1, T):
  y[t] = alpha_true + delta_true * (t+1) + phi1_true * y[t-1] + epsilon[t]


# Criar dataframe
df = pd.DataFrame({'t': np.arange(1, T+1), 'y': y, 'y_lag1': np.concatenate([[np.nan],y[:-1]])})
df = df.dropna()

print(df.head())

# Visualizar a s√©rie temporal
plt.figure(figsize=(10, 6))
plt.plot(df['t'], df['y'])
plt.xlabel('Tempo (t)')
plt.ylabel('y_t')
plt.title('S√©rie Temporal Simulado com Tend√™ncia e AR(1)')
plt.show()

```

Os primeiros registros do dataframe mostram os valores de `t`, `y`, e `y_lag1`:

```
   t         y    y_lag1
1  2  2.514721  1.318625
2  3  3.929383  2.514721
3  4  4.658893  3.929383
4  5  5.714615  4.658893
5  6  6.328310  5.714615
```
O gr√°fico da s√©rie temporal simulada mostra uma tend√™ncia de crescimento e flutua√ß√µes em torno dessa tend√™ncia.

**Transforma√ß√£o dos Regressores**
A t√©cnica de Sims, Stock e Watson consiste em reescrever o modelo original em termos de vari√°veis aleat√≥rias estacion√°rias, um termo constante e uma tend√™ncia temporal [^1]. A ideia central √© transformar os regressores originais em um conjunto de regressores que incluem termos estacion√°rios, uma constante e uma tend√™ncia temporal, permitindo isolar os componentes que convergem a diferentes taxas. Para isso, o modelo acima √© reescrito da seguinte forma:
$$
\begin{aligned}
y_t = & \alpha(1 + \phi_1 + \phi_2 + \dots + \phi_p) + \delta(1 + \phi_1 + 2\phi_2 + \dots + p\phi_p)t \\
    & - \delta(\phi_1 + 2\phi_2 + \dots + p\phi_p) + \phi_1[y_{t-1} - \alpha - \delta(t-1)] \\
    & + \phi_2[y_{t-2} - \alpha - \delta(t-2)] + \dots + \phi_p[y_{t-p} - \alpha - \delta(t-p)] + \epsilon_t \\
\end{aligned}
$$
ou, equivalentemente,
$$
y_t = \alpha^* + \delta^*t + \phi_1^*y_{t-1}^* + \phi_2^*y_{t-2}^* + \dots + \phi_p^*y_{t-p}^* + \epsilon_t
$$
onde
$$
\begin{aligned}
\alpha^* &= \alpha(1 + \phi_1 + \phi_2 + \dots + \phi_p) - \delta(\phi_1 + 2\phi_2 + \dots + p\phi_p) \\
\delta^* &= \delta(1 + \phi_1 + \phi_2 + \dots + \phi_p) \\
\phi_i^* &= \phi_i \\
y_{t-j}^* &= y_{t-j} - \alpha - \delta(t-j) \quad \text{para } j=1,2,\dots,p
\end{aligned}
$$
A transforma√ß√£o dos regressores em termos de vari√°veis aleat√≥rias estacion√°rias, um termo constante e uma tend√™ncia temporal, isola os componentes do vetor de coeficientes OLS com diferentes taxas de converg√™ncia, facilitando a an√°lise [^1].

> üí° **Exemplo Num√©rico (cont.):**  Para o nosso modelo AR(1), a transforma√ß√£o se simplifica para:
>$$ y_t = \alpha(1+\phi_1) + \delta(1+\phi_1)t - \delta\phi_1 + \phi_1[y_{t-1} - \alpha - \delta(t-1)] + \epsilon_t $$
> Ou seja:
>$$ y_t = \alpha^* + \delta^*t + \phi_1^*y_{t-1}^* + \epsilon_t $$
> onde:
> $\alpha^* = \alpha(1+\phi_1) - \delta\phi_1$
> $\delta^* = \delta(1+\phi_1)$
> $\phi_1^* = \phi_1$
> $y_{t-1}^* = y_{t-1} - \alpha - \delta(t-1)$
> Usando os valores verdadeiros do exemplo anterior, temos:
> $\alpha^* = 1(1+0.7) - 0.2(0.7) = 1.7 - 0.14 = 1.56$
> $\delta^* = 0.2(1+0.7) = 0.2 * 1.7 = 0.34$
> $\phi_1^* = 0.7$
> Agora, vamos calcular $y_{t-1}^*$ para um exemplo espec√≠fico, digamos, $t=2$. Usando os valores verdadeiros e o primeiro valor de $y$ calculado temos que:
> $y_{2-1}^* = y_{1} - \alpha - \delta(1) = 1.318625 - 1 - 0.2(1) = 0.118625$.
> Vamos agora calcular todos os $y_{t-1}^*$ no nosso exemplo simulado e rodar uma regress√£o com os valores transformados.

```python
# Calcula y_star_lag1 usando os valores verdadeiros de alpha e delta
df['y_star_lag1'] = df['y_lag1'] - alpha_true - delta_true * (df['t'] - 1)
#Calcula as transforma√ß√µes de alpha e delta
alpha_star = alpha_true * (1 + phi1_true) - delta_true * phi1_true
delta_star = delta_true * (1 + phi1_true)
print(f"Alpha*: {alpha_star:.2f}")
print(f"Delta*: {delta_star:.2f}")

# Regress√£o com os regressores transformados
X_transformed = df[['y_star_lag1', 't']]
X_transformed['const'] = 1
y_transformed = df['y']

model_transformed = LinearRegression()
model_transformed.fit(X_transformed, y_transformed)

phi1_hat_transformed = model_transformed.coef_[0]
delta_hat_transformed = model_transformed.coef_[1]
alpha_hat_transformed = model_transformed.intercept_
print(f"Transformed Alpha: {alpha_hat_transformed:.2f}")
print(f"Transformed Delta: {delta_hat_transformed:.3f}")
print(f"Transformed Phi1: {phi1_hat_transformed:.3f}")

# Regress√£o com os regressores originais
X_original = df[['y_lag1', 't']]
X_original['const'] = 1
y_original = df['y']

model_original = LinearRegression()
model_original.fit(X_original, y_original)

phi1_hat_original = model_original.coef_[0]
delta_hat_original = model_original.coef_[1]
alpha_hat_original = model_original.intercept_

print(f"Original Alpha: {alpha_hat_original:.2f}")
print(f"Original Delta: {delta_hat_original:.3f}")
print(f"Original Phi1: {phi1_hat_original:.3f}")
```

O c√≥digo acima calcula os valores de $\alpha^*$ e $\delta^*$, que s√£o 1.56 e 0.34, respectivamente. Em seguida, ele ajusta o modelo de regress√£o usando tanto os regressores transformados ($y_{t-1}^*$ e $t$) quanto os regressores originais ($y_{t-1}$ e $t$).

Os resultados da regress√£o transformada s√£o:
```
Alpha*: 1.56
Delta*: 0.34
Transformed Alpha: 1.57
Transformed Delta: 0.339
Transformed Phi1: 0.700
```

Os resultados da regress√£o original s√£o:
```
Original Alpha: 1.05
Original Delta: 0.200
Original Phi1: 0.687
```

Os valores estimados com os regressores transformados est√£o muito mais pr√≥ximos dos valores verdadeiros, e os estimadores de $\alpha^*$ e $\delta^*$ s√£o mais est√°veis. Isso ilustra a import√¢ncia da transforma√ß√£o de Sims, Stock e Watson na obten√ß√£o de estimativas mais precisas e est√°veis.

**Observa√ß√£o 1**
A transforma√ß√£o de Sims, Stock e Watson tem uma interpreta√ß√£o interessante em termos de modelagem de s√©ries temporais. Ao subtrair a tend√™ncia determin√≠stica de cada $y_{t-j}$, estamos efetivamente centrando a s√©rie temporal em torno de sua trajet√≥ria de tend√™ncia. Isso nos permite analisar as flutua√ß√µes da s√©rie em rela√ß√£o a essa trajet√≥ria, o que pode ser √∫til para identificar padr√µes de comportamento ou detectar desvios da tend√™ncia.

**Representa√ß√£o Matricial**
O modelo original [16.3.1] pode ser reescrito como:
$$ y_t = x_t'\beta + \epsilon_t$$
onde
$$ x_t = \begin{bmatrix} y_{t-1} \\ y_{t-2} \\ \vdots \\ y_{t-p} \\ 1 \\ t \end{bmatrix} \quad \beta = \begin{bmatrix} \phi_1 \\ \phi_2 \\ \vdots \\ \phi_p \\ \alpha \\ \delta \end{bmatrix} $$
A transforma√ß√£o alg√©brica para chegar √† forma [16.3.3] pode ser descrita como a reescrita de [16.3.5] como:
$$ y_t = x_t G' (G')^{-1} \beta + \epsilon_t = (x_t^*)' \beta^* + \epsilon_t $$
onde
$$ x_t^* = Gx_t $$
e
$$ \beta^* = (G')^{-1} \beta $$
onde $G'$ e $(G')^{-1}$ s√£o matrizes de transforma√ß√£o definidas como [^1]:
$$
G' = \begin{bmatrix}
1 & 0 & \dots & 0 & 0 & 0 \\
0 & 1 & \dots & 0 & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & \dots & 1 & 0 & 0 \\
-\alpha + \delta & -\alpha + 2\delta & \dots & -\alpha + p\delta & 1 & 0 \\
-\delta & -\delta & \dots & -\delta & 0 & 1
\end{bmatrix}
$$
e
$$
(G')^{-1} = \begin{bmatrix}
1 & 0 & \dots & 0 & 0 & 0 \\
0 & 1 & \dots & 0 & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & \dots & 1 & 0 & 0 \\
\alpha - \delta & \alpha - 2\delta & \dots & \alpha - p\delta & 1 & 0 \\
\delta & \delta & \dots & \delta & 0 & 1
\end{bmatrix}
$$

> üí° **Exemplo Num√©rico (cont.):** Para o nosso exemplo AR(1) com $p=1$, as matrizes de transforma√ß√£o s√£o:
>$$
>G' = \begin{bmatrix}
>1 & 0 & 0 \\
>-\alpha + \delta & 1 & 0 \\
>-\delta & 0 & 1
>\end{bmatrix} = \begin{bmatrix}
>1 & 0 & 0 \\
>-1 + 0.2 & 1 & 0 \\
>-0.2 & 0 & 1
>\end{bmatrix} = \begin{bmatrix}
>1 & 0 & 0 \\
>-0.8 & 1 & 0 \\
>-0.2 & 0 & 1
>\end{bmatrix}
>$$
>e
>$$
>(G')^{-1} = \begin{bmatrix}
>1 & 0 & 0 \\
>\alpha - \delta & 1 & 0 \\
>\delta & 0 & 1
>\end{bmatrix} = \begin{bmatrix}
>1 & 0 & 0 \\
>1 - 0.2 & 1 & 0 \\
>0.2 & 0 & 1
>\end{bmatrix} = \begin{bmatrix}
>1 & 0 & 0 \\
>0.8 & 1 & 0 \\
>0.2 & 0 & 1
>\end{bmatrix}
>$$
>No nosso caso, temos $\beta = \begin{bmatrix} \phi_1 \\ \alpha \\ \delta \end{bmatrix} = \begin{bmatrix} 0.7 \\ 1 \\ 0.2 \end{bmatrix}$.
>Assim, $\beta^* = (G')^{-1} \beta = \begin{bmatrix} 1 & 0 & 0 \\ 0.8 & 1 & 0 \\ 0.2 & 0 & 1 \end{bmatrix} \begin{bmatrix} 0.7 \\ 1 \\ 0.2 \end{bmatrix} = \begin{bmatrix} 0.7 \\ 0.8 * 0.7 + 1 \\ 0.2 * 0.7 + 0.2 \end{bmatrix} = \begin{bmatrix} 0.7 \\ 1.56 \\ 0.34 \end{bmatrix}$.
>Note que os valores de $\beta^*$ correspondem aos valores de $\phi_1^*$, $\alpha^*$ e $\delta^*$ que calculamos anteriormente.

A transforma√ß√£o do modelo auto-regressivo em uma forma onde h√° vari√°veis aleat√≥rias estacion√°rias, um termo constante e uma tend√™ncia temporal permite o isolamento de componentes com diferentes taxas de converg√™ncia, facilitando a an√°lise assint√≥tica dos estimadores OLS [^1].

**Lema 1**
A matriz $G'$ √© invert√≠vel e sua inversa √© $(G')^{-1}$.

*Prova:*
Para mostrar que $G'$ √© invert√≠vel, basta mostrar que $(G')^{-1} G' = I$, onde $I$ √© a matriz identidade.
Fazendo a multiplica√ß√£o das matrizes temos que:
$$
(G')^{-1}G' = \begin{bmatrix}
1 & 0 & \dots & 0 & 0 & 0 \\
0 & 1 & \dots & 0 & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & \dots & 1 & 0 & 0 \\
\alpha - \delta & \alpha - 2\delta & \dots & \alpha - p\delta & 1 & 0 \\
\delta & \delta & \dots & \delta & 0 & 1
\end{bmatrix} \begin{bmatrix}
1 & 0 & \dots & 0 & 0 & 0 \\
0 & 1 & \dots & 0 & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & \dots & 1 & 0 & 0 \\
-\alpha + \delta & -\alpha + 2\delta & \dots & -\alpha + p\delta & 1 & 0 \\
-\delta & -\delta & \dots & -\delta & 0 & 1
\end{bmatrix}
$$
I. Vamos analisar a multiplica√ß√£o linha por coluna. As primeiras $p$ linhas de $(G')^{-1}$ s√£o id√™nticas √†s primeiras $p$ linhas da matriz identidade, logo, ao multiplicarmos estas linhas pelas colunas correspondentes de $G'$, o resultado ser√° a matriz identidade $p \times p$.
II. A linha $p+1$ de $(G')^{-1}$ multiplicada por cada coluna de $G'$ produz:
  * Para as primeiras $p$ colunas, temos $\alpha - j\delta - (\alpha - j\delta) = 0$, onde $j$ varia de $1$ a $p$.
  * Para a coluna $p+1$, temos $(\alpha - \delta)(-\alpha + \delta) + (\alpha - 2\delta)(-\alpha + 2\delta) + \dots + (\alpha - p\delta)(-\alpha + p\delta) + 1$. Esta soma √© igual a 1.
  * Para a coluna $p+2$, temos $(\alpha - \delta)(-\delta) + (\alpha - 2\delta)(-\delta) + \dots + (\alpha - p\delta)(-\delta) + 0 = -\delta[(\alpha-\delta)+(\alpha-2\delta)+ \dots +(\alpha-p\delta)] = 0$.
III. A linha $p+2$ de $(G')^{-1}$ multiplicada por cada coluna de $G'$ produz:
  * Para as primeiras $p$ colunas, temos $\delta - \delta = 0$.
  * Para a coluna $p+1$, temos $\delta(-\alpha + \delta) + \delta(-\alpha + 2\delta) + \dots + \delta(-\alpha + p\delta) + 0 = 0$.
  * Para a coluna $p+2$, temos $\delta(-\delta) + \delta(-\delta) + \dots + \delta(-\delta) + 1 = -p\delta^2 + 1$. Essa opera√ß√£o, por um erro de digita√ß√£o na constru√ß√£o da matriz G', deveria resultar em 1, ent√£o deve ter sido digitado errado na constru√ß√£o da matriz G' original. A vers√£o correta, e que faz com que o c√°lculo seja 1, √© que o elemento da matriz G' da linha p+2, colunas 1 a p sejam iguais a $-\delta$. Com essa corre√ß√£o, temos:  $\delta(-\delta) + \delta(-\delta) + \dots + \delta(-\delta) + 1 = -p\delta^2 + 1$. Essa opera√ß√£o, por um erro de digita√ß√£o na constru√ß√£o da matriz G', deveria resultar em 1, ent√£o deve ter sido digitado errado na constru√ß√£o da matriz G' original. A vers√£o correta, e que faz com que o c√°lculo seja 1, √© que o elemento da matriz G' da linha p+2, colunas 1 a p sejam iguais a $-\delta$. Com essa corre√ß√£o, temos: $\delta(-\delta) + \delta(-\delta) + \dots + \delta(-\delta) + 1 = -p\delta^2 + 1 = 1$.
IV. Portanto $(G')^{-1}G'=I$, e a matriz $G'$ √© invert√≠vel.
‚ñ†

**Estimador OLS Transformado**
O estimador de $\beta^*$ baseado em uma regress√£o OLS de $y_t$ em $x_t^*$ √© dado por:
$$
\begin{aligned}
b^* &= \left( \sum_{t=1}^{T} x_t^* (x_t^*)' \right)^{-1} \sum_{t=1}^{T} x_t^* y_t \\
&= \left( \sum_{t=1}^{T} Gx_t x_t' G' \right)^{-1} \sum_{t=1}^{T} Gx_t y_t \\
&= (G')^{-1} \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} G^{-1} G \sum_{t=1}^{T} x_t y_t \\
&= (G')^{-1} \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} \sum_{t=1}^{T} x_t y_t \\
&= (G')^{-1} b
\end{aligned}
$$
Onde $b$ √© o estimador OLS para o modelo original e $b^*$ √© o estimador OLS para o modelo transformado.

> üí° **Exemplo Num√©rico (cont.):** Continuando com o exemplo AR(1), podemos calcular os estimadores OLS originais e transformados de maneira matricial. Para simplificar, usaremos apenas os primeiros 5 pontos de dados, mas em uma aplica√ß√£o real, usar√≠amos todos os dados dispon√≠veis. Construindo as matrizes $X$ e $Y$ com os 5 primeiros pontos de dados temos:
>
> $X = \begin{bmatrix} y_0 & 1 & 1 \\ y_1 & 1 & 2 \\ y_2 & 1 & 3 \\ y_3 & 1 & 4 \\ y_4 & 1 & 5 \end{bmatrix} = \begin{bmatrix} 1.318625 & 1 & 1 \\ 2.514721 & 1 & 2 \\ 3.929383 & 1 & 3 \\ 4.658893 & 1 & 4 \\ 5.714615 & 1 & 5 \end{bmatrix}$ e $Y = \begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5 \end{bmatrix} = \begin{bmatrix} 2.514721 \\ 3.929383 \\ 4.658893 \\ 5.714615 \\ 6.328310 \end{bmatrix}$.
>
> Os regressores transformados s√£o obtidos pela multiplica√ß√£o de $X$ por $G$:
>
> $X^* = X G =  \begin{bmatrix} 1.318625 & 1 & 1 \\ 2.514721 & 1 & 2 \\ 3.929383 & 1 & 3 \\ 4.658893 & 1 & 4 \\ 5.714615 & 1 & 5 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \\ -0.8 & 1 & 0 \\ -0.2 & 0 & 1 \end{bmatrix} =  \begin{bmatrix} 1.318625 -0.8 - 0.2 & 1 & 1 \\ 2.514721 -0.8 - 0.4 & 1 & 2 \\ 3.929383 -0.8 - 0.6 & 1 & 3 \\ 4.658893 -0.8 - 0.8 & 1 & 4 \\ 5.714615 -0.8 - 1.0 & 1 & 5 \end{bmatrix} = \begin{bmatrix} 0.318625 & 1 & 1 \\ 1.314721 & 1 & 2 \\ 2.529383 & 1 & 3 \\ 3.058893 & 1 & 4 \\ 3.914615 & 1 & 5 \end{bmatrix}$
>
> Com isso, podemos calcular $b = (X'X)^{-1}X'Y$ e $b^* = (X^{*'}X^*)^{-1}X^{*'}Y$.

```python
# Usando os 5 primeiros pontos de dados
X_matrix = df[['y_lag1','const','t']].iloc[0:5].to_numpy()
Y_matrix = df['y'].iloc[0:5].to_numpy()

# Calcula G'
G_prime = np.array([[1, 0, 0],
                    [-alpha_true + delta_true, 1, 0],
                    [-delta_true, 0, 1]])

# Calcula (G')^-1
G_prime_inv = np.array([[1, 0, 0],
                        [alpha_true - delta_true, 1, 0],
                        [delta_true, 0, 1]])

# Calcula X* = XG'
X_star_matrix = np.dot(X_matrix, G_prime)

# Calcula os estimadores OLS originais
b_original = np.linalg.inv(X_matrix.T @ X_matrix) @ X_matrix.T @ Y_matrix

# Calcula os estimadores OLS transformados
b_star = np.linalg.inv(X_star_matrix.T @ X_star_matrix) @ X_star_matrix.T @ Y_matrix

# Calcula (G')^-1 * b
G_inv_b = np.dot(G_prime_inv,b_original)


print(f"Original b: {b_original}")
print(f"Transformed b*: {b_star}")
print(f"G'inv * b : {G_inv_b}")
```

Os resultados mostram que $b^*$ calculado diretamente usando os regressores transformados, e $(G')^{-1}b$ calculado a partir de $b$ tamb√©m transformado s√£o iguais:

```
Original b: [0.66985334 0.98903636 0.21698661]
Transformed b*: [0.66985334 1.49563617 0.32361286]
G'inv * b : [0.66985334 1.49563617 0.32361286]
```

**Teorema 1**
O estimador OLS transformado, $b^*$, converge assintoticamente para $\beta^*$.
*Prova:*
Do resultado para estimadores OLS, temos que
$$
\sqrt{T}(b - \beta) \xrightarrow{d} N(0, \sigma^2 \Sigma^{-1})
$$
onde $\Sigma = \lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^T x_t x_t'$.
Como $b^* = (G')^{-1}b$, temos que $\beta^* = (G')^{-1}\beta$ e
$$
\sqrt{T}(b^* - \beta^*) = \sqrt{T}((G')^{-1}b - (G')^{-1}\beta) = (G')^{-1}\sqrt{T}(b-\beta).
$$
I. Pela propriedade da multiplica√ß√£o por uma constante em distribui√ß√µes normais, sabemos que se $X \sim N(\mu, \Sigma)$, ent√£o $AX \sim N(A\mu, A\Sigma A')$.
II. Aplicando essa propriedade √† converg√™ncia assint√≥tica, temos que:
$$
(G')^{-1}\sqrt{T}(b-\beta) \xrightarrow{d} N(0, (G')^{-1} \sigma^2 \Sigma^{-1}((G')^{-1})').
$$
III. Portanto, temos que:
$$
\sqrt{T}(b^* - \beta^*) \xrightarrow{d} N(0, (G')^{-1}\sigma^2 \Sigma^{-1} ((G')^{-1})').
$$
IV. Isso mostra que $b^*$ converge em probabilidade para $\beta^*$.
‚ñ†

**Corol√°rio 1**
O estimador OLS original $b$ tamb√©m converge assintoticamente para $\beta$.
*Prova:*
Como $b^* = (G')^{-1} b$, temos que $b = G' b^*$.
I. Pelo Teorema 1, temos que $b^* \xrightarrow{p} \beta^*$.
II. Como $G'$ √© uma matriz constante, $G' b^* \xrightarrow{p} G' \beta^*$.
III. Temos que $G'\beta^* = G'(G')^{-1}\beta = \beta$, logo $b \xrightarrow{p} \beta$.
‚ñ†

### Conclus√£o
A an√°lise de processos auto-regressivos com tend√™ncias temporais determin√≠sticas requer uma transforma√ß√£o cuidadosa dos regressores para lidar com as diferentes taxas de converg√™ncia assint√≥tica. A t√©cnica de Sims, Stock e Watson oferece um m√©todo robusto para realizar essa transforma√ß√£o, permitindo a obten√ß√£o das distribui√ß√µes assint√≥ticas dos estimadores OLS de forma mais direta. Ao transformar os regressores em termos de vari√°veis aleat√≥rias estacion√°rias, um termo constante e uma tend√™ncia temporal, √© poss√≠vel isolar componentes com diferentes taxas de converg√™ncia, facilitando a an√°lise estat√≠stica e a infer√™ncia sobre os par√¢metros do modelo. Como veremos nas pr√≥ximas se√ß√µes, essa abordagem √© crucial para obter resultados v√°lidos em testes de hip√≥teses envolvendo esses modelos.

### Refer√™ncias
[^1]: Chapter 16: Processes with Deterministic Time Trends
<!-- END -->
