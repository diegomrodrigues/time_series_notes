## Testes de Hip√≥teses em Modelos Auto-Regressivos com Tend√™ncia Temporal

### Introdu√ß√£o
Expandindo a an√°lise de processos auto-regressivos com tend√™ncias temporais determin√≠sticas, este cap√≠tulo aborda os testes de hip√≥teses para os par√¢metros desses modelos. Como visto anteriormente, a transforma√ß√£o de Sims, Stock e Watson √© fundamental para lidar com as diferentes taxas de converg√™ncia dos estimadores [^1]. Agora, vamos examinar como essas taxas de converg√™ncia afetam a validade assint√≥tica dos testes de hip√≥teses, e como podemos implementar esses testes tanto no modelo original quanto no modelo transformado. Em particular, vamos nos concentrar em como as taxas de converg√™ncia de diferentes estimadores afetam a interpreta√ß√£o dos testes de hip√≥teses, mostrando que, apesar dessas diferen√ßas, podemos usar os testes usuais de forma assintoticamente v√°lida.

### Conceitos Fundamentais
Em modelos auto-regressivos com tend√™ncia temporal, como o modelo expresso por:
$$ y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t $$
os testes de hip√≥teses s√£o frequentemente utilizados para inferir sobre os valores dos par√¢metros. A transforma√ß√£o de Sims, Stock e Watson, ao transformar o modelo para:
$$ y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \ldots + \phi_p^* y_{t-p}^* + \epsilon_t $$
isola componentes com diferentes taxas de converg√™ncia, como j√° discutido [^1].  Apesar dessas diferen√ßas, os testes de hip√≥teses usuais, como os testes t e F, s√£o assintoticamente v√°lidos, tanto no modelo original quanto no modelo transformado.

> üí° **Exemplo Num√©rico:** Para ilustrar, considere um modelo AR(1) com tend√™ncia temporal:
> $$ y_t = \alpha + \delta t + \phi_1 y_{t-1} + \epsilon_t $$
> onde, como antes, $y_t$ √© a s√©rie temporal no instante $t$, $\alpha$ √© o intercepto, $\delta$ √© o coeficiente da tend√™ncia temporal, $\phi_1$ √© o coeficiente auto-regressivo, e $\epsilon_t$ √© um ru√≠do branco. A transforma√ß√£o de Sims, Stock e Watson transforma este modelo para:
> $$ y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \epsilon_t $$
> onde $\alpha^*$, $\delta^*$ e $\phi_1^*$ s√£o transforma√ß√µes dos par√¢metros originais.
>
> Suponha que desejamos testar as seguintes hip√≥teses:
>
> 1. **Hip√≥tese sobre o par√¢metro da tend√™ncia temporal original:** $H_0: \delta = 0$ versus $H_1: \delta \neq 0$. Este teste avalia se h√° uma tend√™ncia temporal significativa na s√©rie.
> 2. **Hip√≥tese conjunta sobre os par√¢metros originais:** $H_0: \alpha = 0 \text{ e } \phi_1 = 0$ versus $H_1: \alpha \neq 0 \text{ ou } \phi_1 \neq 0$. Este teste avalia se o intercepto e o par√¢metro auto-regressivo s√£o conjuntamente significativos.
>
> Usando uma simula√ß√£o, podemos obter estimativas OLS para o modelo original e o modelo transformado. Para facilitar a an√°lise, vamos supor que a estimativa dos par√¢metros no modelo original e transformado s√£o os mesmos da se√ß√£o anterior do exemplo num√©rico:
>
> **Regressores Transformados:**
> $\hat{\alpha}^* = 1.57$
> $\hat{\delta}^* = 0.339$
> $\hat{\phi}_1^* = 0.700$
>
> **Regressores Originais:**
> $\hat{\alpha} = 1.05$
> $\hat{\delta} = 0.200$
> $\hat{\phi}_1 = 0.687$
>
> Com esses valores, podemos construir as estat√≠sticas de teste apropriadas. O importante √© que, apesar de termos estimativas diferentes, os testes de hip√≥teses resultantes s√£o equivalentes.

**Testes de Hip√≥teses com as Estat√≠sticas Usuais**
Apesar das diferentes taxas de converg√™ncia, os testes de hip√≥teses para os par√¢metros desses modelos podem ser implementados usando as estat√≠sticas usuais. Tanto o teste *t* quanto o teste *F* s√£o assintoticamente v√°lidos tanto no modelo original quanto no transformado [^1]. Isso significa que, embora os estimadores dos par√¢metros de tend√™ncia temporal convirjam mais rapidamente, os testes de hip√≥teses podem ser realizados de forma consistente.

**Teste *t* para um √∫nico par√¢metro**
Para testar a hip√≥tese nula de que um par√¢metro espec√≠fico √© igual a um valor espec√≠fico (por exemplo, $H_0: \delta = \delta_0$), o teste *t* pode ser calculado da maneira usual:
$$ t = \frac{\hat{\delta} - \delta_0}{\text{erro padr√£o}(\hat{\delta})} $$
A validade assint√≥tica deste teste decorre do fato de que o erro padr√£o do estimador $\hat{\delta}$ tamb√©m converge a uma taxa que compensa a converg√™ncia mais r√°pida do estimador em si.

**Teste *F* para m√∫ltiplas restri√ß√µes**
Para testar hip√≥teses conjuntas sobre m√∫ltiplos par√¢metros, utiliza-se o teste *F*.  A forma geral para testar a hip√≥tese nula $H_0: R\beta = r$ √© dada pela estat√≠stica *F*:
$$ F = \frac{(R\hat{\beta} - r)'[R(X'X)^{-1}R']^{-1}(R\hat{\beta} - r)/m}{s^2} $$
onde $R$ √© a matriz de restri√ß√£o, $r$ √© o vetor de valores de restri√ß√£o, $X$ √© a matriz de regressores, $\hat{\beta}$ √© o vetor de estimadores OLS e $s^2$ √© a vari√¢ncia dos erros. Sob a hip√≥tese nula, esta estat√≠stica tem uma distribui√ß√£o *F* com *m* graus de liberdade no numerador, onde *m* √© o n√∫mero de restri√ß√µes. No entanto, quando a amostra √© grande, esta estat√≠stica *F* converge para uma distribui√ß√£o $\chi^2$ com $m$ graus de liberdade.

> üí° **Exemplo Num√©rico (cont.):**  Vamos construir a estat√≠stica t para testar a hip√≥tese nula $H_0 : \delta = 0$ no modelo original e no transformado. Usando os resultados da se√ß√£o anterior do exemplo num√©rico, e considerando que o erro padr√£o para $\hat{\delta}$ no modelo original foi 0.02 e para $\hat{\delta}^*$ foi 0.007, temos que:
>
> Para o modelo original:
> $$ t = \frac{\hat{\delta} - 0}{SE(\hat{\delta})} = \frac{0.200 - 0}{0.02} = 10 $$
>
> Para o modelo transformado:
> $$ t = \frac{\hat{\delta}^* - 0}{SE(\hat{\delta}^*)} = \frac{0.339 - 0}{0.007} \approx 48.43 $$
>
> Note que ambos os testes s√£o significativos. Al√©m disso, vamos construir o teste F para a hip√≥tese conjunta $H_0 : \alpha = 0 \text{ e } \phi_1 = 0$ (considerando um modelo de regress√£o com dois regressores):
>
>Para isso, precisamos dos estimadores dos par√¢metros e da matriz de covari√¢ncia dos estimadores (que obtemos ao calcular o inverso de (X'X)). Por simplicidade, vamos assumir que:
>
>No modelo original: $\hat{\beta} = \begin{bmatrix} 1.05 \\ 0.687 \end{bmatrix}$ e  $(X'X)^{-1} = \begin{bmatrix} 0.05 & 0.01 \\ 0.01 & 0.02 \end{bmatrix}$ e $s^2 = 0.2$. Para testar a hip√≥tese conjunta, usamos $R = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$ e $r = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$
>
>No modelo transformado: $\hat{\beta}^* = \begin{bmatrix} 1.57 \\ 0.700 \end{bmatrix}$ e $(X^{*'}X^*)^{-1} = \begin{bmatrix} 0.0001 & 0.00005 \\ 0.00005 & 0.0002 \end{bmatrix}$ e $s^2 = 0.2$. Para testar a hip√≥tese conjunta, usamos $R = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$ e $r = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$
>
>Assim, temos:
>
>Para o modelo original:
> $$ F = \frac{(R\hat{\beta} - r)'[R(X'X)^{-1}R']^{-1}(R\hat{\beta} - r)/m}{s^2} = \frac{\begin{bmatrix} 1.05 & 0.687\end{bmatrix}\begin{bmatrix} 0.05 & 0.01 \\ 0.01 & 0.02 \end{bmatrix}^{-1}\begin{bmatrix} 1.05 \\ 0.687\end{bmatrix} /2}{0.2} \approx  68.04
> $$
>
>Para o modelo transformado:
> $$ F^* = \frac{(R\hat{\beta}^* - r)'[R(X^{*'}X^*)^{-1}R']^{-1}(R\hat{\beta}^* - r)/m}{s^2} = \frac{\begin{bmatrix} 1.57 & 0.7\end{bmatrix}\begin{bmatrix} 0.0001 & 0.00005 \\ 0.00005 & 0.0002 \end{bmatrix}^{-1}\begin{bmatrix} 1.57 \\ 0.7\end{bmatrix} /2}{0.2} \approx  10587.5
> $$
>
> Ambos os testes indicam que a hip√≥tese nula conjunta deve ser rejeitada, apesar dos valores diferentes.
> Importante notar que esses valores s√£o apenas ilustrativos e foram simplificados para fins de clareza. Em uma aplica√ß√£o real, devemos usar os resultados dos dados para realizar as opera√ß√µes.
>
> üí° **Exemplo Num√©rico:** Para demonstrar o c√°lculo do teste F com dados simulados e o uso do Python, considere a seguinte simula√ß√£o em um modelo AR(1) com tend√™ncia:
> ```python
> import numpy as np
> import statsmodels.api as sm
>
> # Simulate data
> np.random.seed(42)
> n = 100
> t = np.arange(1, n + 1)
> y = np.zeros(n)
> alpha = 1.0
> delta = 0.2
> phi1 = 0.7
> errors = np.random.normal(0, 0.5, n)
>
> y[0] = alpha + delta * 1 + errors[0]
> for i in range(1, n):
>     y[i] = alpha + delta * (i + 1) + phi1 * y[i-1]  + errors[i]
>
> # Prepare data for regression
> X = np.column_stack((np.ones(n), t, np.concatenate(([0], y[:-1])) ))
>
> # OLS regression
> model = sm.OLS(y, X)
> results = model.fit()
>
> # Test joint hypothesis H0: alpha = 0 and phi1 = 0
> R = np.array([[1, 0, 0], [0, 0, 1]])
> r = np.array([0, 0])
> f_test = results.f_test(R, r)
>
> print(f"F-statistic: {f_test.fvalue}")
> print(f"P-value: {f_test.pvalue}")
>
> ```
>
> Este c√≥digo simula dados de um modelo AR(1) com tend√™ncia, realiza uma regress√£o OLS e, em seguida, realiza um teste F para a hip√≥tese conjunta de que o intercepto e o coeficiente AR(1) s√£o iguais a zero. O resultado do teste F e o seu valor p s√£o impressos. Se o valor p for inferior a um n√≠vel de signific√¢ncia (por exemplo, 0,05), rejeitar√≠amos a hip√≥tese nula.

**Domin√¢ncia Assint√≥tica**
Em testes de hip√≥teses envolvendo par√¢metros com diferentes taxas de converg√™ncia, o comportamento assint√≥tico do teste √© dominado pelos par√¢metros com as menores taxas de converg√™ncia [^1]. Isso significa que, em testes de hip√≥teses com diferentes taxas de converg√™ncia, o resultado do teste √© essencialmente determinado pelo comportamento do estimador que converge mais lentamente. O que significa que em testes envolvendo par√¢metros com taxas de converg√™ncia diferentes, como o teste t para $\delta$, o resultado √© assintoticamente equivalente a realizar um teste t para o mesmo par√¢metro, com o mesmo erro padr√£o, utilizando o estimador de $\delta$ obtido utilizando o valor verdadeiro de todos os outros par√¢metros, e n√£o seus valores estimados [^1].
No caso de uma hip√≥tese conjunta, a situa√ß√£o √© similar. A distribui√ß√£o assint√≥tica do teste *F* √© determinada pelos estimadores que convergem mais lentamente.

**Restri√ß√µes Lineares sobre os Par√¢metros**
As restri√ß√µes lineares sobre os par√¢metros podem envolver todos os coeficientes do modelo ou apenas um subconjunto deles. Por exemplo, podemos querer testar a hip√≥tese de que:
  1. Uma combina√ß√£o linear de todos os par√¢metros √© igual a zero: $H_0: r_1 \alpha + r_2 \delta + r_3 \phi_1 + \ldots + r_{p+2} \phi_p = 0$.
  2. Um subconjunto dos coeficientes √© igual a zero, como, por exemplo, testar a signific√¢ncia apenas do coeficiente da tend√™ncia temporal: $H_0: \delta = 0$.
  3. Testar que o coeficiente da tend√™ncia √© um m√∫ltiplo de outro par√¢metro: $H_0: \delta = c\alpha$.

Nesses casos, a validade assint√≥tica dos testes √© garantida pela transforma√ß√£o de Sims, Stock e Watson, que permite isolar os componentes do modelo com diferentes taxas de converg√™ncia [^1].

**An√°lise das Restri√ß√µes**
A an√°lise das restri√ß√µes pode ser feita tanto no modelo original quanto no transformado. No entanto, a an√°lise no modelo transformado simplifica a interpreta√ß√£o dos resultados, porque permite a separa√ß√£o dos componentes com diferentes taxas de converg√™ncia.
Quando a restri√ß√£o envolve todos os par√¢metros, o teste resultante √© assintoticamente dominado pelos par√¢metros que convergem mais lentamente. Isso significa que, a distribui√ß√£o assint√≥tica do teste se comporta como se o teste fosse realizado apenas sobre esses par√¢metros, com todos os outros par√¢metros sendo considerados como seus verdadeiros valores (como descrito anteriormente).
Quando a restri√ß√£o envolve apenas o par√¢metro da tend√™ncia, os resultados s√£o simplificados, dado que o mesmo possui a taxa de converg√™ncia mais r√°pida, e √© dominante.

> üí° **Exemplo Num√©rico (cont.):**  Vamos considerar o teste da hip√≥tese de que o coeficiente da tend√™ncia √© igual ao intercepto: $H_0: \alpha = \delta$. Para isso, podemos reescrever a hip√≥tese como $H_0: \alpha - \delta = 0$, e podemos usar essa informa√ß√£o para definir a matriz de restri√ß√£o: $R = \begin{bmatrix} 1 & -1 & 0 & 0 \ldots\end{bmatrix}$ e o vetor de restri√ß√£o $r = 0$.
>
>No modelo original temos:
>$$ F = \frac{(R\hat{\beta} - r)'[R(X'X)^{-1}R']^{-1}(R\hat{\beta} - r)/m}{s^2} = \frac{(\hat{\alpha} - \hat{\delta} - 0)[(X'X)^{-1}]_{1,2}(\hat{\alpha} - \hat{\delta} - 0)}{s^2} $$
>No modelo transformado, podemos expressar a hip√≥tese equivalente de que a diferen√ßa entre $\alpha^*$ e $\delta^*$ √© zero: $H_0: \alpha^* - \delta^* = 0$, com $R^* =  \begin{bmatrix} 1 & -1 & 0 & 0 \ldots\end{bmatrix}$.
>
>Para o nosso exemplo, $R =  \begin{bmatrix} 1 & -1 & 0  \end{bmatrix}$. Utilizando as estimativas obtidas e o mesmo erro padr√£o apresentado anteriormente, podemos calcular a estat√≠stica do teste.
>Para o modelo original:
>$$ F = \frac{(1.05 - 0.200)^2}{(0.05 + 0.02)}  \approx 10.25
> $$
>
>Para o modelo transformado:
>$$ F = \frac{(1.57 - 0.339)^2}{(0.0001 + 0.0002)}  \approx 5007.7
> $$
>
> Note que ambos os testes s√£o significativos.
>
> üí° **Exemplo Num√©rico:** Para exemplificar o teste da restri√ß√£o linear $\alpha = \delta$ com Python, podemos estender o exemplo num√©rico anterior:
> ```python
> # Code from previous example
> import numpy as np
> import statsmodels.api as sm
>
> # Simulate data
> np.random.seed(42)
> n = 100
> t = np.arange(1, n + 1)
> y = np.zeros(n)
> alpha = 1.0
> delta = 0.2
> phi1 = 0.7
> errors = np.random.normal(0, 0.5, n)
>
> y[0] = alpha + delta * 1 + errors[0]
> for i in range(1, n):
>     y[i] = alpha + delta * (i + 1) + phi1 * y[i-1]  + errors[i]
>
> # Prepare data for regression
> X = np.column_stack((np.ones(n), t, np.concatenate(([0], y[:-1])) ))
>
> # OLS regression
> model = sm.OLS(y, X)
> results = model.fit()
>
> # Test linear constraint H0: alpha = delta
> R_linear = np.array([[1, -1, 0]]) # alpha - delta = 0
> r_linear = np.array([0])
> f_test_linear = results.f_test(R_linear, r_linear)
>
> print(f"F-statistic (linear constraint): {f_test_linear.fvalue}")
> print(f"P-value (linear constraint): {f_test_linear.pvalue}")
> ```
>
> Aqui, definimos a matriz de restri√ß√£o `R_linear` como `[[1, -1, 0]]`, que corresponde √† restri√ß√£o $\alpha - \delta = 0$. O teste F √© realizado usando o m√©todo `f_test` do objeto `results`, e os resultados s√£o impressos.  Novamente, um valor p baixo indica que a hip√≥tese nula deve ser rejeitada.

**Observa√ß√£o 3**
√â importante notar que as estat√≠sticas de testes s√£o v√°lidas assintoticamente, ou seja, a distribui√ß√£o dessas estat√≠sticas se aproxima da distribui√ß√£o te√≥rica (t ou chi-quadrado) conforme o tamanho da amostra cresce. Em amostras pequenas, as distribui√ß√µes podem ser diferentes, o que pode levar a conclus√µes equivocadas.

**Teorema 3**
Os testes *t* e *F* calculados sobre os par√¢metros de um modelo auto-regressivo com tend√™ncia temporal determin√≠stica s√£o assintoticamente v√°lidos, seja para o modelo original ou transformado.
*Prova:*
I. A validade assint√≥tica dos testes *t* e *F* decorre do fato de que os estimadores dos par√¢metros convergem para uma distribui√ß√£o gaussiana, e que a vari√¢ncia dos estimadores converge para uma distribui√ß√£o com um comportamento assint√≥tico consistente com as taxas de converg√™ncia.
II. A transforma√ß√£o de Sims, Stock e Watson isola os componentes dos regressores que convergem a diferentes taxas.
III. Os resultados assint√≥ticos para os estimadores transformados s√£o tais que a estat√≠stica do teste *t* para um √∫nico par√¢metro √© assintoticamente equivalente a dividir uma vari√°vel Gaussiana por seu desvio padr√£o. Isso garante a validade assint√≥tica do teste t.
IV. A estat√≠stica do teste F para m√∫ltiplas restri√ß√µes √© uma forma quadr√°tica de vari√°veis gaussianas que converge assintoticamente para uma distribui√ß√£o chi-quadrado.
V. Como os modelos original e transformado possuem a mesma distribui√ß√£o assint√≥tica, o resultado √© v√°lido para ambos os modelos.
‚ñ†

**Lema 3.1**
Sob a hip√≥tese nula, a estat√≠stica *t* converge em distribui√ß√£o para uma vari√°vel aleat√≥ria com distribui√ß√£o normal padr√£o, isto √©,
$$ t \xrightarrow{d} N(0,1) $$
*Prova:*
I. A estat√≠stica *t* √© definida como $t = \frac{\hat{\delta} - \delta_0}{SE(\hat{\delta})}$.
II. Sabemos que o estimador $\hat{\delta}$ converge para uma distribui√ß√£o normal, e o erro padr√£o $SE(\hat{\delta})$ converge para a vari√¢ncia assint√≥tica do estimador.
III. Sob a hip√≥tese nula, $\delta = \delta_0$, logo a estat√≠stica *t* se torna a raz√£o de uma vari√°vel normal centrada em zero e seu desvio padr√£o, que converge para a distribui√ß√£o normal padr√£o.
IV. A mesma l√≥gica se aplica aos modelos original e transformado, devido √† equival√™ncia assint√≥tica entre eles.
‚ñ†

**Lema 3.2**
Sob a hip√≥tese nula, a estat√≠stica *F* converge em distribui√ß√£o para uma vari√°vel aleat√≥ria com distribui√ß√£o qui-quadrado com *m* graus de liberdade, isto √©,
$$ F \xrightarrow{d} \chi^2(m) $$
*Prova:*
I. O teste *F* √© dado por $F = \frac{(R\hat{\beta} - r)'[R(X'X)^{-1}R']^{-1}(R\hat{\beta} - r)/m}{s^2}$.
II. Sabemos que $(R\hat{\beta} - r)$ converge para uma distribui√ß√£o normal multivariada com m√©dia zero sob a hip√≥tese nula, e $[R(X'X)^{-1}R']^{-1}$ e $s^2$ convergem para matrizes de covari√¢ncia assint√≥ticas, e vari√¢ncia dos erros, respectivamente.
III. Assim, $F$ converge para uma forma quadr√°tica de vari√°veis gaussianas, que √© assintoticamente uma distribui√ß√£o qui-quadrado com *m* graus de liberdade, onde *m* √© o n√∫mero de restri√ß√µes.
IV. Este resultado se mant√©m para ambos os modelos, original e transformado, devido a suas equival√™ncias assint√≥ticas.
‚ñ†

**Teorema 3.1**
Os resultados dos testes de hip√≥teses s√£o assintoticamente equivalentes entre o modelo original e o transformado, ou seja, para um n√≠vel de signific√¢ncia $\alpha$, a probabilidade de rejeitar a hip√≥tese nula √© a mesma em ambos os modelos quando o tamanho da amostra tende ao infinito.
*Prova:*
I. Este teorema √© uma consequ√™ncia direta do Teorema 3 e dos Lemas 3.1 e 3.2.
II. Como os testes *t* e *F* convergem para as mesmas distribui√ß√µes limites no modelo original e transformado, seus valores p convergem para os mesmos limites assint√≥ticos.
III. Isso significa que a decis√£o de rejeitar ou n√£o a hip√≥tese nula √© a mesma em ambos os modelos para amostras suficientemente grandes.
‚ñ†

### Conclus√£o
Os testes de hip√≥teses para modelos auto-regressivos com tend√™ncias temporais determin√≠sticas podem ser implementados utilizando os m√©todos usuais, como os testes *t* e *F*, com os resultados sendo assintoticamente v√°lidos [^1]. A transforma√ß√£o de Sims, Stock e Watson permite que os testes sejam aplicados tanto no modelo original quanto no transformado. Em testes de hip√≥teses sobre par√¢metros com diferentes taxas de converg√™ncia, o resultado do teste √© dominado pelas vari√°veis que convergem mais lentamente. As restri√ß√µes lineares podem envolver todos os coeficientes ou apenas aqueles relacionados √† tend√™ncia temporal, com o comportamento assint√≥tico dos testes sendo determinado pelas taxas de converg√™ncia dos par√¢metros envolvidos e pela estrutura da matriz de restri√ß√µes. Portanto, a aplica√ß√£o cuidadosa desses testes, considerando as propriedades assint√≥ticas dos estimadores, fornece infer√™ncias estat√≠sticas robustas sobre os par√¢metros do modelo.

### Refer√™ncias
[^1]: Chapter 16: Processes with Deterministic Time Trends
<!-- END -->
