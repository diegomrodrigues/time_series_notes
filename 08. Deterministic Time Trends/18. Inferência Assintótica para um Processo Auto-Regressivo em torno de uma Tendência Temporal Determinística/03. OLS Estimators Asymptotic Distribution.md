## A Distribui√ß√£o Assint√≥tica dos Estimadores OLS em Modelos Auto-Regressivos Transformados

### Introdu√ß√£o
Em continuidade √† an√°lise de processos auto-regressivos com tend√™ncias temporais determin√≠sticas, este cap√≠tulo aprofunda o estudo da distribui√ß√£o assint√≥tica dos estimadores OLS ap√≥s a aplica√ß√£o da transforma√ß√£o de Sims, Stock e Watson. Como estabelecido anteriormente, essa transforma√ß√£o √© crucial para lidar com as diferentes taxas de converg√™ncia dos estimadores em modelos com tend√™ncia temporal [^1]. Agora, vamos examinar como essa transforma√ß√£o afeta a distribui√ß√£o assint√≥tica dos estimadores OLS, revelando um comportamento espec√≠fico para o coeficiente da tend√™ncia temporal.

### Conceitos Fundamentais
A transforma√ß√£o dos regressores, discutida anteriormente, resulta em um modelo onde os componentes com diferentes taxas de converg√™ncia s√£o isolados [^1]. Especificamente, o modelo transformado pode ser expresso como:
$$
y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \dots + \phi_p^* y_{t-p}^* + \epsilon_t
$$
Os coeficientes $\alpha^*$, $\delta^*$ e $\phi_i^*$ s√£o estimadores OLS obtidos a partir do modelo transformado. O objetivo principal desta se√ß√£o √© detalhar a distribui√ß√£o assint√≥tica desses estimadores, mostrando que eles convergem para uma distribui√ß√£o Gaussiana com taxas de converg√™ncia distintas.

> üí° **Exemplo Num√©rico:** Para ilustrar a transforma√ß√£o, considere um modelo AR(1) com uma tend√™ncia temporal:
> $$y_t = \alpha + \delta t + \phi_1 y_{t-1} + \epsilon_t$$
> onde $y_t$ √© a s√©rie temporal no instante $t$, $\alpha$ √© o intercepto, $\delta$ √© o coeficiente da tend√™ncia temporal, $\phi_1$ √© o coeficiente auto-regressivo, e $\epsilon_t$ √© um ru√≠do branco. Usando a transforma√ß√£o de Sims, Stock e Watson, definimos $y_t^* = y_t - \alpha - \delta t$, e o modelo transformado torna-se:
> $$y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \epsilon_t$$
> onde $\alpha^* = \alpha(1 - \phi_1) + \delta \phi_1$, $\delta^* = \delta(1 - \phi_1)$ e $y_{t-1}^* = y_{t-1} - \alpha - \delta(t-1)$. Este processo separa os componentes de tend√™ncia e estacionariedade, que convergem em taxas diferentes.
>
> Para um exemplo num√©rico concreto, vamos supor $\alpha=2$, $\delta=0.5$, $\phi_1=0.8$ e que temos observa√ß√µes de $t = 1, 2, 3, 4$.
>
> $\text{Passo 1: Calcular } y_t$
>
> Suponha que $\epsilon_t$ s√£o valores aleat√≥rios: $\epsilon_1 = 0.1, \epsilon_2 = -0.2, \epsilon_3 = 0.3, \epsilon_4 = -0.1$ e $y_0=0$.
> $y_1 = 2 + 0.5(1) + 0.8(0) + 0.1 = 2.6$
> $y_2 = 2 + 0.5(2) + 0.8(2.6) - 0.2 = 4.88$
> $y_3 = 2 + 0.5(3) + 0.8(4.88) + 0.3 = 7.104$
> $y_4 = 2 + 0.5(4) + 0.8(7.104) - 0.1 = 8.7832$
>
> $\text{Passo 2: Calcular } y_t^*$:
> $y_1^* = 2.6 - 2 - 0.5(1) = 0.1$
> $y_2^* = 4.88 - 2 - 0.5(2) = 1.88$
> $y_3^* = 7.104 - 2 - 0.5(3) = 3.604$
> $y_4^* = 8.7832 - 2 - 0.5(4) = 4.7832$
>
> $\text{Passo 3: Calcular } \alpha^*, \delta^*, \text{e} \phi_1^*$
>
> $\alpha^* = 2(1 - 0.8) + 0.5(0.8) = 0.8$
> $\delta^* = 0.5(1 - 0.8) = 0.1$
> $\phi_1^*$ ser√° estimado por regress√£o OLS
>
> Este exemplo mostra como os dados s√£o transformados. A estimativa dos par√¢metros $\alpha^*$, $\delta^*$ e $\phi_1^*$ ser√° feita usando regress√£o OLS no modelo transformado.

**Distribui√ß√£o Assint√≥tica**
Um resultado chave √© que os estimadores do modelo transformado, obtidos por OLS, possuem distribui√ß√µes assint√≥ticas Gaussianas, com taxas de converg√™ncia que dependem da natureza do regressor.  Em particular, os coeficientes associados a vari√°veis aleat√≥rias estacion√°rias ($\phi_i^*$), convergem √† taxa de $\sqrt{T}$ para uma distribui√ß√£o Gaussiana, enquanto o coeficiente da tend√™ncia temporal ($Œ¥^*$) converge a uma taxa de $T^{3/2}$ [^1]. Essa diferen√ßa nas taxas de converg√™ncia reflete o comportamento superconsistente do estimador da tend√™ncia temporal.

> üí° **Exemplo Num√©rico (cont.):** Retomando o exemplo AR(1), vamos examinar a distribui√ß√£o assint√≥tica dos estimadores $\hat{\alpha}^*$, $\hat{\delta}^*$, e $\hat{\phi}_1^*$ obtidos com a transforma√ß√£o. Para isso, vamos simular v√°rias amostras independentes, calcular os estimadores para cada amostra, e analisar suas distribui√ß√µes.
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Par√¢metros verdadeiros
alpha_true = 1
delta_true = 0.2
phi1_true = 0.7
sigma_epsilon = 0.5

# Tamanho da amostra
T = 100

# N√∫mero de amostras
num_simulations = 1000

# Listas para armazenar os estimadores
alpha_star_hats = []
delta_star_hats = []
phi1_star_hats = []

# Simula√ß√µes
np.random.seed(42) # Para reprodutibilidade
for sim in range(num_simulations):
    # Gerar ru√≠do branco
    epsilon = np.random.normal(0, sigma_epsilon, T)

    # Inicializar y
    y = np.zeros(T)
    y[0] = alpha_true + delta_true * 1 + epsilon[0] # Primeiro valor com um valor inicial razo√°vel para y

    # Gerar a s√©rie temporal
    for t in range(1, T):
        y[t] = alpha_true + delta_true * (t+1) + phi1_true * y[t-1] + epsilon[t]


    # Criar dataframe
    df = pd.DataFrame({'t': np.arange(1, T+1), 'y': y, 'y_lag1': np.concatenate([[np.nan],y[:-1]])})
    df = df.dropna()

    # Calcula y_star_lag1 usando os valores verdadeiros de alpha e delta
    df['y_star_lag1'] = df['y_lag1'] - alpha_true - delta_true * (df['t'] - 1)
    #Calcula as transforma√ß√µes de alpha e delta
    alpha_star = alpha_true * (1 + phi1_true) - delta_true * phi1_true
    delta_star = delta_true * (1 + phi1_true)

    # Regress√£o com os regressores transformados
    X_transformed = df[['y_star_lag1', 't']]
    X_transformed['const'] = 1
    y_transformed = df['y']

    model_transformed = LinearRegression()
    model_transformed.fit(X_transformed, y_transformed)

    phi1_hat_transformed = model_transformed.coef_[0]
    delta_hat_transformed = model_transformed.coef_[1]
    alpha_hat_transformed = model_transformed.intercept_


    alpha_star_hats.append(alpha_hat_transformed)
    delta_star_hats.append(delta_hat_transformed)
    phi1_star_hats.append(phi1_hat_transformed)


# Converter listas em numpy arrays
alpha_star_hats = np.array(alpha_star_hats)
delta_star_hats = np.array(delta_star_hats)
phi1_star_hats = np.array(phi1_star_hats)

# Visualizar a distribui√ß√£o dos estimadores
plt.figure(figsize=(15, 5))

# Histograma do Alpha*
plt.subplot(1, 3, 1)
plt.hist(alpha_star_hats, bins=30, density=True, alpha=0.6, color='blue')
plt.axvline(np.mean(alpha_star_hats), color='red', linestyle='dashed', linewidth=1, label=f'M√©dia = {np.mean(alpha_star_hats):.2f}')
plt.axvline(alpha_star, color='black', linestyle='-', linewidth=1, label=f'Valor Verdadeiro = {alpha_star:.2f}')
plt.title('Distribui√ß√£o de alpha*')
plt.xlabel('alpha*')
plt.ylabel('Densidade')
plt.legend()

# Histograma do Delta*
plt.subplot(1, 3, 2)
plt.hist(delta_star_hats, bins=30, density=True, alpha=0.6, color='green')
plt.axvline(np.mean(delta_star_hats), color='red', linestyle='dashed', linewidth=1, label=f'M√©dia = {np.mean(delta_star_hats):.3f}')
plt.axvline(delta_star, color='black', linestyle='-', linewidth=1, label=f'Valor Verdadeiro = {delta_star:.3f}')
plt.title('Distribui√ß√£o de delta*')
plt.xlabel('delta*')
plt.ylabel('Densidade')
plt.legend()

# Histograma do Phi1*
plt.subplot(1, 3, 3)
plt.hist(phi1_star_hats, bins=30, density=True, alpha=0.6, color='purple')
plt.axvline(np.mean(phi1_star_hats), color='red', linestyle='dashed', linewidth=1, label=f'M√©dia = {np.mean(phi1_star_hats):.3f}')
plt.axvline(phi1_true, color='black', linestyle='-', linewidth=1, label=f'Valor Verdadeiro = {phi1_true:.1f}')
plt.title('Distribui√ß√£o de phi1*')
plt.xlabel('phi1*')
plt.ylabel('Densidade')
plt.legend()

plt.tight_layout()
plt.show()

```
O c√≥digo acima simula 1000 s√©ries temporais e estima os valores de $\hat{\alpha}^*$, $\hat{\delta}^*$ e $\hat{\phi}_1^*$ para cada uma delas. Em seguida, plota os histogramas dos estimadores, mostrando a distribui√ß√£o de cada um. O histograma de $\hat{\delta}^*$ mostra uma distribui√ß√£o bem concentrada em torno do seu valor verdadeiro, indicando a sua converg√™ncia r√°pida. Em compara√ß√£o, a distribui√ß√£o dos outros estimadores √© mais dispersa, mas ainda concentrada em torno dos seus respectivos valores verdadeiros. Isso ilustra o comportamento assint√≥tico dos estimadores, onde $\hat{\delta}^*$ converge a uma taxa mais r√°pida que os outros estimadores.

**Resultados Assint√≥ticos**
O resultado principal para a distribui√ß√£o assint√≥tica dos estimadores transformados √© dado por [^1]:
$$
Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})
$$
onde $Y_T$ √© uma matriz diagonal que cont√©m as taxas de converg√™ncia apropriadas para cada par√¢metro e $Q^*$ √© uma matriz que descreve a estrutura de autocovari√¢ncia dos regressores transformados. Especificamente, $Y_T$ √© dada por:
$$
Y_T = \begin{bmatrix}
\sqrt{T} & 0 & 0 & \ldots & 0 & 0 \\
0 & \sqrt{T} & 0 & \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & \sqrt{T} & 0 \\
0 & 0 & 0 & \ldots & 0 & T^{3/2}
\end{bmatrix}
$$
Essa matriz diagonal reflete o fato de que todos os coeficientes, exceto o da tend√™ncia temporal, convergem √† taxa $\sqrt{T}$, enquanto o coeficiente da tend√™ncia converge √† taxa $T^{3/2}$ [^1]. A matriz $Q^*$ √© dada por:
$$
Q^* = \lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^{T} (x_t^*)(x_t^*)'
$$
onde $x_t^*$ s√£o os regressores transformados.

> üí° **Exemplo Num√©rico:** Para ilustrar a matriz $Y_T$, se tivermos um modelo com 2 vari√°veis estacion√°rias e uma tend√™ncia temporal, e $T=100$, ent√£o a matriz $Y_T$ ser√°:
> $$
> Y_{100} = \begin{bmatrix}
> \sqrt{100} & 0 & 0 \\
> 0 & \sqrt{100} & 0 \\
> 0 & 0 & 100^{3/2}
> \end{bmatrix} = \begin{bmatrix}
> 10 & 0 & 0 \\
> 0 & 10 & 0 \\
> 0 & 0 & 1000
> \end{bmatrix}
> $$
> Note que o terceiro elemento da diagonal, correspondente √† tend√™ncia temporal, √© muito maior que os outros, refletindo a converg√™ncia mais r√°pida.
>
> A matriz $Q^*$ representa a vari√¢ncia e covari√¢ncia dos regressores transformados.
> Suponha que ap√≥s calcular $\frac{1}{T} \sum_{t=1}^{T} (x_t^*)(x_t^*)'$ e tomar o limite quando $T\to\infty$, obtivemos
> $$
> Q^* = \begin{bmatrix}
> 1 & 0.5 & 0.2 \\
> 0.5 & 1.5 & 0.1 \\
> 0.2 & 0.1 & 2
> \end{bmatrix}
> $$
>
> Ent√£o, $[Q^*]^{-1}$ seria:
> $$
> [Q^*]^{-1} = \begin{bmatrix}
> 1.34 & -0.44 & -0.02 \\
> -0.44 & 0.72 & 0.05 \\
> -0.02 & 0.05 & 0.51
> \end{bmatrix}
> $$
>
>  A matriz $[Q^*]^{-1}$ √© usada para calcular a vari√¢ncia assint√≥tica dos estimadores.

A distribui√ß√£o assint√≥tica dos estimadores do modelo original ($b$) pode ser obtida pela transforma√ß√£o dos resultados do modelo transformado usando a matriz $G'$. De forma espec√≠fica, a distribui√ß√£o assint√≥tica para o modelo original √© obtida atrav√©s de:
$$
\sqrt{T}(\hat{\alpha}_T - \alpha) \xrightarrow{d} N(0, \sigma^2 g_{\alpha} [Q^*]^{-1}g_{\alpha}')
$$
e
$$
T^{3/2}(\hat{\delta}_T - \delta) \xrightarrow{d} N(0, \sigma^2 g_{\delta} [Q^*]^{-1}g_{\delta}')
$$
onde $g_\alpha$ e $g_\delta$ s√£o vetores espec√≠ficos relacionados √† transforma√ß√£o original. Essa distribui√ß√£o assint√≥tica permite realizar infer√™ncias estat√≠sticas sobre os par√¢metros originais.

**Observa√ß√£o 2**
√â importante notar que, embora a distribui√ß√£o assint√≥tica dos estimadores seja gaussiana, essa converg√™ncia ocorre quando o tamanho da amostra tende ao infinito. Em amostras finitas, as distribui√ß√µes podem n√£o ser exatamente gaussianas, especialmente para o estimador do par√¢metro de tend√™ncia, que converge mais rapidamente e pode apresentar um comportamento diferente em amostras menores.

**Teorema 2**
A distribui√ß√£o assint√≥tica do estimador OLS transformado, $b^*$, √© dada por:
$$
Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})
$$
onde $Y_T$ √© uma matriz diagonal com as taxas de converg√™ncia apropriadas para cada par√¢metro.
*Prova:*
I.  Pelo Teorema 1, temos que $\sqrt{T}(b^* - \beta^*) \xrightarrow{d} N(0, (G')^{-1} \sigma^2 \Sigma^{-1} ((G')^{-1})')$.
II. A matriz $\Sigma$ converge para uma matriz $Q^*$, que representa o limite da matriz de covari√¢ncia dos regressores transformados.
III.  A matriz $Y_T$ cont√©m as taxas de converg√™ncia apropriadas para cada par√¢metro, especificamente $\sqrt{T}$ para os par√¢metros relacionados as vari√°veis estacion√°rias, e $T^{3/2}$ para o coeficiente da tend√™ncia. Assim, ao multiplicarmos por $Y_T$ estamos escalando cada um dos estimadores por sua taxa de converg√™ncia apropriada.
IV. O resultado assint√≥tico para a matriz $Y_T(b^* - \beta^*)$ √© obtido multiplicando a matriz por $Y_T$. Como $Y_T$ √© uma matriz diagonal, a multiplica√ß√£o por $Y_T$ afeta apenas a vari√¢ncia de cada estimador.
V. Assim, temos que $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$.
‚ñ†

**Lema 2.1**
A matriz $Q^*$ √© positiva definida se e somente se os regressores transformados n√£o forem linearmente dependentes no limite.

*Prova:*
I. A matriz $Q^*$ √© definida como o limite da m√©dia amostral do produto externo dos regressores transformados: $Q^* = \lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^{T} (x_t^*)(x_t^*)'$.
II. Se os regressores transformados n√£o s√£o linearmente dependentes no limite, ent√£o para qualquer vetor n√£o nulo $v$, $v'Q^*v > 0$, o que significa que $Q^*$ √© positiva definida.
III. Reciprocamente, se $Q^*$ √© positiva definida, ent√£o $v'Q^*v > 0$ para todo $v \ne 0$, implicando que os regressores transformados n√£o podem ser linearmente dependentes no limite. Caso contr√°rio, haveria um vetor $v$ tal que $v'x_t^* = 0$ para todo $t$ no limite, fazendo com que $v'Q^*v = 0$, o que √© uma contradi√ß√£o.
‚ñ†

**Corol√°rio 2**
Os estimadores originais tamb√©m possuem distribui√ß√µes assint√≥ticas gaussianas, obtidas atrav√©s da transforma√ß√£o dos resultados do modelo transformado.
*Prova:*
I. Sabemos que $b=G'b^*$, logo $b - \beta = G'(b^*-\beta^*)$.
II. Assim, $Y_T(b-\beta) = Y_TG'(b^*-\beta^*)$.
III. Pelo Teorema 2, temos que $Y_T(b^*-\beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$.
IV.  Assim, $Y_T(b - \beta) \xrightarrow{d} N(0, G' \sigma^2 [Q^*]^{-1}(G')')$.
V. O resultado de IV nos permite obter as distribui√ß√µes assint√≥ticas gaussianas dos par√¢metros originais. Por exemplo, os estimadores de $\alpha$ e $\delta$ s√£o obtidos da seguinte forma:
$$ \sqrt{T}(\hat{\alpha}_T - \alpha) \xrightarrow{d} N(0, \sigma^2 g_{\alpha} [Q^*]^{-1}g_{\alpha}') $$
e
$$ T^{3/2}(\hat{\delta}_T - \delta) \xrightarrow{d} N(0, \sigma^2 g_{\delta} [Q^*]^{-1}g_{\delta}') $$
onde $g_\alpha$ e $g_\delta$ s√£o vetores relacionados com a matriz de transforma√ß√£o $G'$.
‚ñ†

**Teorema 2.1**
Se a matriz $Q^*$ √© positiva definida, ent√£o os estimadores OLS transformados $b^*$ s√£o consistentes para $\beta^*$.

*Prova:*
I. Do Teorema 2, sabemos que $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$.
II. Se $Q^*$ √© positiva definida, ent√£o $[Q^*]^{-1}$ existe e √© finita.
III. Como $Y_T$ diverge com $T$, ent√£o para que o resultado em I seja convergente, $b^* - \beta^*$ deve convergir para 0 em probabilidade.
IV. Portanto, $b^* \xrightarrow{p} \beta^*$, demonstrando a consist√™ncia dos estimadores transformados.
‚ñ†
### Conclus√£o
A distribui√ß√£o assint√≥tica dos estimadores OLS em um modelo auto-regressivo transformado √© Gaussiana, com a particularidade de que o coeficiente da tend√™ncia temporal converge a uma taxa de $T^{3/2}$, enquanto os outros coeficientes convergem a uma taxa de $\sqrt{T}$ [^1]. Essa diferen√ßa nas taxas de converg√™ncia √© uma consequ√™ncia direta da transforma√ß√£o de Sims, Stock e Watson e reflete o comportamento superconsistente do estimador da tend√™ncia temporal. A compreens√£o dessas distribui√ß√µes √© crucial para a realiza√ß√£o de infer√™ncias estat√≠sticas v√°lidas em modelos auto-regressivos com tend√™ncias temporais determin√≠sticas. Al√©m disso, a distribui√ß√£o assint√≥tica dos par√¢metros originais pode ser obtida atrav√©s da distribui√ß√£o dos estimadores transformados, atrav√©s da utiliza√ß√£o da matriz G.

### Refer√™ncias
[^1]: Chapter 16: Processes with Deterministic Time Trends
<!-- END -->
