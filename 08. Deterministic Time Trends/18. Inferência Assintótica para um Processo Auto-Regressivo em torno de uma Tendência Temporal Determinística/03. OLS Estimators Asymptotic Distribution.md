## A DistribuiÃ§Ã£o AssintÃ³tica dos Estimadores OLS em Modelos Auto-Regressivos Transformados

### IntroduÃ§Ã£o
Em continuidade Ã  anÃ¡lise de processos auto-regressivos com tendÃªncias temporais determinÃ­sticas, este capÃ­tulo aprofunda o estudo da distribuiÃ§Ã£o assintÃ³tica dos estimadores OLS apÃ³s a aplicaÃ§Ã£o da transformaÃ§Ã£o de Sims, Stock e Watson. Como estabelecido anteriormente, essa transformaÃ§Ã£o Ã© crucial para lidar com as diferentes taxas de convergÃªncia dos estimadores em modelos com tendÃªncia temporal [^1]. Agora, vamos examinar como essa transformaÃ§Ã£o afeta a distribuiÃ§Ã£o assintÃ³tica dos estimadores OLS, revelando um comportamento especÃ­fico para o coeficiente da tendÃªncia temporal.

### Conceitos Fundamentais
A transformaÃ§Ã£o dos regressores, discutida anteriormente, resulta em um modelo onde os componentes com diferentes taxas de convergÃªncia sÃ£o isolados [^1]. Especificamente, o modelo transformado pode ser expresso como:
$$
y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \dots + \phi_p^* y_{t-p}^* + \epsilon_t
$$
Os coeficientes $\alpha^*$, $\delta^*$ e $\phi_i^*$ sÃ£o estimadores OLS obtidos a partir do modelo transformado. O objetivo principal desta seÃ§Ã£o Ã© detalhar a distribuiÃ§Ã£o assintÃ³tica desses estimadores, mostrando que eles convergem para uma distribuiÃ§Ã£o Gaussiana com taxas de convergÃªncia distintas.

> ðŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar a transformaÃ§Ã£o, considere um modelo AR(1) com uma tendÃªncia temporal:
> $$y_t = \alpha + \delta t + \phi_1 y_{t-1} + \epsilon_t$$
> onde $y_t$ Ã© a sÃ©rie temporal no instante $t$, $\alpha$ Ã© o intercepto, $\delta$ Ã© o coeficiente da tendÃªncia temporal, $\phi_1$ Ã© o coeficiente auto-regressivo, e $\epsilon_t$ Ã© um ruÃ­do branco. Usando a transformaÃ§Ã£o de Sims, Stock e Watson, definimos $y_t^* = y_t - \alpha - \delta t$, e o modelo transformado torna-se:
> $$y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \epsilon_t$$
> onde $\alpha^* = \alpha(1 - \phi_1) + \delta \phi_1$, $\delta^* = \delta(1 - \phi_1)$ e $y_{t-1}^* = y_{t-1} - \alpha - \delta(t-1)$. Este processo separa os componentes de tendÃªncia e estacionariedade, que convergem em taxas diferentes.
>
> Para um exemplo numÃ©rico concreto, vamos supor $\alpha=2$, $\delta=0.5$, $\phi_1=0.8$ e que temos observaÃ§Ãµes de $t = 1, 2, 3, 4$.
>
> $\text{Passo 1: Calcular } y_t$
>
> Suponha que $\epsilon_t$ sÃ£o valores aleatÃ³rios: $\epsilon_1 = 0.1, \epsilon_2 = -0.2, \epsilon_3 = 0.3, \epsilon_4 = -0.1$ e $y_0=0$.
> $y_1 = 2 + 0.5(1) + 0.8(0) + 0.1 = 2.6$
> $y_2 = 2 + 0.5(2) + 0.8(2.6) - 0.2 = 4.88$
> $y_3 = 2 + 0.5(3) + 0.8(4.88) + 0.3 = 7.104$
> $y_4 = 2 + 0.5(4) + 0.8(7.104) - 0.1 = 8.7832$
>
> $\text{Passo 2: Calcular } y_t^*$:
> $y_1^* = 2.6 - 2 - 0.5(1) = 0.1$
> $y_2^* = 4.88 - 2 - 0.5(2) = 1.88$
> $y_3^* = 7.104 - 2 - 0.5(3) = 3.604$
> $y_4^* = 8.7832 - 2 - 0.5(4) = 4.7832$
>
> $\text{Passo 3: Calcular } \alpha^*, \delta^*, \text{e} \phi_1^*$
>
> $\alpha^* = 2(1 - 0.8) + 0.5(0.8) = 0.8$
> $\delta^* = 0.5(1 - 0.8) = 0.1$
> $\phi_1^*$ serÃ¡ estimado por regressÃ£o OLS
>
> Este exemplo mostra como os dados sÃ£o transformados. A estimativa dos parÃ¢metros $\alpha^*$, $\delta^*$ e $\phi_1^*$ serÃ¡ feita usando regressÃ£o OLS no modelo transformado.

**DistribuiÃ§Ã£o AssintÃ³tica**
Um resultado chave Ã© que os estimadores do modelo transformado, obtidos por OLS, possuem distribuiÃ§Ãµes assintÃ³ticas Gaussianas, com taxas de convergÃªncia que dependem da natureza do regressor.  Em particular, os coeficientes associados a variÃ¡veis aleatÃ³rias estacionÃ¡rias ($\phi_i^*$), convergem Ã  taxa de $\sqrt{T}$ para uma distribuiÃ§Ã£o Gaussiana, enquanto o coeficiente da tendÃªncia temporal ($Î´^*$) converge a uma taxa de $T^{3/2}$ [^1]. Essa diferenÃ§a nas taxas de convergÃªncia reflete o comportamento superconsistente do estimador da tendÃªncia temporal.

> ðŸ’¡ **Exemplo NumÃ©rico (cont.):** Retomando o exemplo AR(1), vamos examinar a distribuiÃ§Ã£o assintÃ³tica dos estimadores $\hat{\alpha}^*$, $\hat{\delta}^*$, e $\hat{\phi}_1^*$ obtidos com a transformaÃ§Ã£o. Para isso, vamos simular vÃ¡rias amostras independentes, calcular os estimadores para cada amostra, e analisar suas distribuiÃ§Ãµes.
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# ParÃ¢metros verdadeiros
alpha_true = 1
delta_true = 0.2
phi1_true = 0.7
sigma_epsilon = 0.5

# Tamanho da amostra
T = 100

# NÃºmero de amostras
num_simulations = 1000

# Listas para armazenar os estimadores
alpha_star_hats = []
delta_star_hats = []
phi1_star_hats = []

# SimulaÃ§Ãµes
np.random.seed(42) # Para reprodutibilidade
for sim in range(num_simulations):
    # Gerar ruÃ­do branco
    epsilon = np.random.normal(0, sigma_epsilon, T)

    # Inicializar y
    y = np.zeros(T)
    y[0] = alpha_true + delta_true * 1 + epsilon[0] # Primeiro valor com um valor inicial razoÃ¡vel para y

    # Gerar a sÃ©rie temporal
    for t in range(1, T):
        y[t] = alpha_true + delta_true * (t+1) + phi1_true * y[t-1] + epsilon[t]


    # Criar dataframe
    df = pd.DataFrame({'t': np.arange(1, T+1), 'y': y, 'y_lag1': np.concatenate([[np.nan],y[:-1]])})
    df = df.dropna()

    # Calcula y_star_lag1 usando os valores verdadeiros de alpha e delta
    df['y_star_lag1'] = df['y_lag1'] - alpha_true - delta_true * (df['t'] - 1)
    #Calcula as transformaÃ§Ãµes de alpha e delta
    alpha_star = alpha_true * (1 + phi1_true) - delta_true * phi1_true
    delta_star = delta_true * (1 + phi1_true)

    # RegressÃ£o com os regressores transformados
    X_transformed = df[['y_star_lag1', 't']]
    X_transformed['const'] = 1
    y_transformed = df['y']

    model_transformed = LinearRegression()
    model_transformed.fit(X_transformed, y_transformed)

    phi1_hat_transformed = model_transformed.coef_[0]
    delta_hat_transformed = model_transformed.coef_[1]
    alpha_hat_transformed = model_transformed.intercept_


    alpha_star_hats.append(alpha_hat_transformed)
    delta_star_hats.append(delta_hat_transformed)
    phi1_star_hats.append(phi1_hat_transformed)


# Converter listas em numpy arrays
alpha_star_hats = np.array(alpha_star_hats)
delta_star_hats = np.array(delta_star_hats)
phi1_star_hats = np.array(phi1_star_hats)

# Visualizar a distribuiÃ§Ã£o dos estimadores
plt.figure(figsize=(15, 5))

# Histograma do Alpha*
plt.subplot(1, 3, 1)
plt.hist(alpha_star_hats, bins=30, density=True, alpha=0.6, color='blue')
plt.axvline(np.mean(alpha_star_hats), color='red', linestyle='dashed', linewidth=1, label=f'MÃ©dia = {np.mean(alpha_star_hats):.2f}')
plt.axvline(alpha_star, color='black', linestyle='-', linewidth=1, label=f'Valor Verdadeiro = {alpha_star:.2f}')
plt.title('DistribuiÃ§Ã£o de alpha*')
plt.xlabel('alpha*')
plt.ylabel('Densidade')
plt.legend()

# Histograma do Delta*
plt.subplot(1, 3, 2)
plt.hist(delta_star_hats, bins=30, density=True, alpha=0.6, color='green')
plt.axvline(np.mean(delta_star_hats), color='red', linestyle='dashed', linewidth=1, label=f'MÃ©dia = {np.mean(delta_star_hats):.3f}')
plt.axvline(delta_star, color='black', linestyle='-', linewidth=1, label=f'Valor Verdadeiro = {delta_star:.3f}')
plt.title('DistribuiÃ§Ã£o de delta*')
plt.xlabel('delta*')
plt.ylabel('Densidade')
plt.legend()

# Histograma do Phi1*
plt.subplot(1, 3, 3)
plt.hist(phi1_star_hats, bins=30, density=True, alpha=0.6, color='purple')
plt.axvline(np.mean(phi1_star_hats), color='red', linestyle='dashed', linewidth=1, label=f'MÃ©dia = {np.mean(phi1_star_hats):.3f}')
plt.axvline(phi1_true, color='black', linestyle='-', linewidth=1, label=f'Valor Verdadeiro = {phi1_true:.1f}')
plt.title('DistribuiÃ§Ã£o de phi1*')
plt.xlabel('phi1*')
plt.ylabel('Densidade')
plt.legend()

plt.tight_layout()
plt.show()

```
O cÃ³digo acima simula 1000 sÃ©ries temporais e estima os valores de $\hat{\alpha}^*$, $\hat{\delta}^*$ e $\hat{\phi}_1^*$ para cada uma delas. Em seguida, plota os histogramas dos estimadores, mostrando a distribuiÃ§Ã£o de cada um. O histograma de $\hat{\delta}^*$ mostra uma distribuiÃ§Ã£o bem concentrada em torno do seu valor verdadeiro, indicando a sua convergÃªncia rÃ¡pida. Em comparaÃ§Ã£o, a distribuiÃ§Ã£o dos outros estimadores Ã© mais dispersa, mas ainda concentrada em torno dos seus respectivos valores verdadeiros. Isso ilustra o comportamento assintÃ³tico dos estimadores, onde $\hat{\delta}^*$ converge a uma taxa mais rÃ¡pida que os outros estimadores.

**Resultados AssintÃ³ticos**
O resultado principal para a distribuiÃ§Ã£o assintÃ³tica dos estimadores transformados Ã© dado por [^1]:
$$
Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})
$$
onde $Y_T$ Ã© uma matriz diagonal que contÃ©m as taxas de convergÃªncia apropriadas para cada parÃ¢metro e $Q^*$ Ã© uma matriz que descreve a estrutura de autocovariÃ¢ncia dos regressores transformados. Especificamente, $Y_T$ Ã© dada por:
$$
Y_T = \begin{bmatrix}
\sqrt{T} & 0 & 0 & \ldots & 0 & 0 \\
0 & \sqrt{T} & 0 & \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & \sqrt{T} & 0 \\
0 & 0 & 0 & \ldots & 0 & T^{3/2}
\end{bmatrix}
$$
Essa matriz diagonal reflete o fato de que todos os coeficientes, exceto o da tendÃªncia temporal, convergem Ã  taxa $\sqrt{T}$, enquanto o coeficiente da tendÃªncia converge Ã  taxa $T^{3/2}$ [^1]. A matriz $Q^*$ Ã© dada por:
$$
Q^* = \lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^{T} (x_t^*)(x_t^*)'
$$
onde $x_t^*$ sÃ£o os regressores transformados.

> ðŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar a matriz $Y_T$, se tivermos um modelo com 2 variÃ¡veis estacionÃ¡rias e uma tendÃªncia temporal, e $T=100$, entÃ£o a matriz $Y_T$ serÃ¡:
> $$
> Y_{100} = \begin{bmatrix}
> \sqrt{100} & 0 & 0 \\
> 0 & \sqrt{100} & 0 \\
> 0 & 0 & 100^{3/2}
> \end{bmatrix} = \begin{bmatrix}
> 10 & 0 & 0 \\
> 0 & 10 & 0 \\
> 0 & 0 & 1000
> \end{bmatrix}
> $$
> Note que o terceiro elemento da diagonal, correspondente Ã  tendÃªncia temporal, Ã© muito maior que os outros, refletindo a convergÃªncia mais rÃ¡pida.
>
> A matriz $Q^*$ representa a variÃ¢ncia e covariÃ¢ncia dos regressores transformados.
> Suponha que apÃ³s calcular $\frac{1}{T} \sum_{t=1}^{T} (x_t^*)(x_t^*)'$ e tomar o limite quando $T\to\infty$, obtivemos
> $$
> Q^* = \begin{bmatrix}
> 1 & 0.5 & 0.2 \\
> 0.5 & 1.5 & 0.1 \\
> 0.2 & 0.1 & 2
> \end{bmatrix}
> $$
>
> EntÃ£o, $[Q^*]^{-1}$ seria:
> $$
> [Q^*]^{-1} = \begin{bmatrix}
> 1.34 & -0.44 & -0.02 \\
> -0.44 & 0.72 & 0.05 \\
> -0.02 & 0.05 & 0.51
> \end{bmatrix}
> $$
>
>  A matriz $[Q^*]^{-1}$ Ã© usada para calcular a variÃ¢ncia assintÃ³tica dos estimadores.

A distribuiÃ§Ã£o assintÃ³tica dos estimadores do modelo original ($b$) pode ser obtida pela transformaÃ§Ã£o dos resultados do modelo transformado usando a matriz $G'$. De forma especÃ­fica, a distribuiÃ§Ã£o assintÃ³tica para o modelo original Ã© obtida atravÃ©s de:
$$
\sqrt{T}(\hat{\alpha}_T - \alpha) \xrightarrow{d} N(0, \sigma^2 g_{\alpha} [Q^*]^{-1}g_{\alpha}')
$$
e
$$
T^{3/2}(\hat{\delta}_T - \delta) \xrightarrow{d} N(0, \sigma^2 g_{\delta} [Q^*]^{-1}g_{\delta}')
$$
onde $g_\alpha$ e $g_\delta$ sÃ£o vetores especÃ­ficos relacionados Ã  transformaÃ§Ã£o original. Essa distribuiÃ§Ã£o assintÃ³tica permite realizar inferÃªncias estatÃ­sticas sobre os parÃ¢metros originais.

**ObservaÃ§Ã£o 2**
Ã‰ importante notar que, embora a distribuiÃ§Ã£o assintÃ³tica dos estimadores seja gaussiana, essa convergÃªncia ocorre quando o tamanho da amostra tende ao infinito. Em amostras finitas, as distribuiÃ§Ãµes podem nÃ£o ser exatamente gaussianas, especialmente para o estimador do parÃ¢metro de tendÃªncia, que converge mais rapidamente e pode apresentar um comportamento diferente em amostras menores.

**Teorema 2**
A distribuiÃ§Ã£o assintÃ³tica do estimador OLS transformado, $b^*$, Ã© dada por:
$$
Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})
$$
onde $Y_T$ Ã© uma matriz diagonal com as taxas de convergÃªncia apropriadas para cada parÃ¢metro.
*Prova:*
I.  Pelo Teorema 1, temos que $\sqrt{T}(b^* - \beta^*) \xrightarrow{d} N(0, (G')^{-1} \sigma^2 \Sigma^{-1} ((G')^{-1})')$.
II. A matriz $\Sigma$ converge para uma matriz $Q^*$, que representa o limite da matriz de covariÃ¢ncia dos regressores transformados.
III.  A matriz $Y_T$ contÃ©m as taxas de convergÃªncia apropriadas para cada parÃ¢metro, especificamente $\sqrt{T}$ para os parÃ¢metros relacionados as variÃ¡veis estacionÃ¡rias, e $T^{3/2}$ para o coeficiente da tendÃªncia. Assim, ao multiplicarmos por $Y_T$ estamos escalando cada um dos estimadores por sua taxa de convergÃªncia apropriada.
IV. O resultado assintÃ³tico para a matriz $Y_T(b^* - \beta^*)$ Ã© obtido multiplicando a matriz por $Y_T$. Como $Y_T$ Ã© uma matriz diagonal, a multiplicaÃ§Ã£o por $Y_T$ afeta apenas a variÃ¢ncia de cada estimador.
V. Assim, temos que $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$.
â– 

**Lema 2.1**
A matriz $Q^*$ Ã© positiva definida se e somente se os regressores transformados nÃ£o forem linearmente dependentes no limite.

*Prova:*
I. A matriz $Q^*$ Ã© definida como o limite da mÃ©dia amostral do produto externo dos regressores transformados: $Q^* = \lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^{T} (x_t^*)(x_t^*)'$.
II. Se os regressores transformados nÃ£o sÃ£o linearmente dependentes no limite, entÃ£o para qualquer vetor nÃ£o nulo $v$, $v'Q^*v > 0$, o que significa que $Q^*$ Ã© positiva definida.
III. Reciprocamente, se $Q^*$ Ã© positiva definida, entÃ£o $v'Q^*v > 0$ para todo $v \ne 0$, implicando que os regressores transformados nÃ£o podem ser linearmente dependentes no limite. Caso contrÃ¡rio, haveria um vetor $v$ tal que $v'x_t^* = 0$ para todo $t$ no limite, fazendo com que $v'Q^*v = 0$, o que Ã© uma contradiÃ§Ã£o.
â– 

**CorolÃ¡rio 2**
Os estimadores originais tambÃ©m possuem distribuiÃ§Ãµes assintÃ³ticas gaussianas, obtidas atravÃ©s da transformaÃ§Ã£o dos resultados do modelo transformado.
*Prova:*
I. Sabemos que $b=G'b^*$, logo $b - \beta = G'(b^*-\beta^*)$.
II. Assim, $Y_T(b-\beta) = Y_TG'(b^*-\beta^*)$.
III. Pelo Teorema 2, temos que $Y_T(b^*-\beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$.
IV.  Assim, $Y_T(b - \beta) \xrightarrow{d} N(0, G' \sigma^2 [Q^*]^{-1}(G')')$.
V. O resultado de IV nos permite obter as distribuiÃ§Ãµes assintÃ³ticas gaussianas dos parÃ¢metros originais. Por exemplo, os estimadores de $\alpha$ e $\delta$ sÃ£o obtidos da seguinte forma:
$$ \sqrt{T}(\hat{\alpha}_T - \alpha) \xrightarrow{d} N(0, \sigma^2 g_{\alpha} [Q^*]^{-1}g_{\alpha}') $$
e
$$ T^{3/2}(\hat{\delta}_T - \delta) \xrightarrow{d} N(0, \sigma^2 g_{\delta} [Q^*]^{-1}g_{\delta}') $$
onde $g_\alpha$ e $g_\delta$ sÃ£o vetores relacionados com a matriz de transformaÃ§Ã£o $G'$.
â– 

**Teorema 2.1**
Se a matriz $Q^*$ Ã© positiva definida, entÃ£o os estimadores OLS transformados $b^*$ sÃ£o consistentes para $\beta^*$.

*Prova:*
I. Do Teorema 2, sabemos que $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$.
II. Se $Q^*$ Ã© positiva definida, entÃ£o $[Q^*]^{-1}$ existe e Ã© finita.
III. Como $Y_T$ diverge com $T$, entÃ£o para que o resultado em I seja convergente, $b^* - \beta^*$ deve convergir para 0 em probabilidade.
IV. Portanto, $b^* \xrightarrow{p} \beta^*$, demonstrando a consistÃªncia dos estimadores transformados.
â– 
### ConclusÃ£o
A distribuiÃ§Ã£o assintÃ³tica dos estimadores OLS em um modelo auto-regressivo transformado Ã© Gaussiana, com a particularidade de que o coeficiente da tendÃªncia temporal converge a uma taxa de $T^{3/2}$, enquanto os outros coeficientes convergem a uma taxa de $\sqrt{T}$ [^1]. Essa diferenÃ§a nas taxas de convergÃªncia Ã© uma consequÃªncia direta da transformaÃ§Ã£o de Sims, Stock e Watson e reflete o comportamento superconsistente do estimador da tendÃªncia temporal. A compreensÃ£o dessas distribuiÃ§Ãµes Ã© crucial para a realizaÃ§Ã£o de inferÃªncias estatÃ­sticas vÃ¡lidas em modelos auto-regressivos com tendÃªncias temporais determinÃ­sticas. AlÃ©m disso, a distribuiÃ§Ã£o assintÃ³tica dos parÃ¢metros originais pode ser obtida atravÃ©s da distribuiÃ§Ã£o dos estimadores transformados, atravÃ©s da utilizaÃ§Ã£o da matriz G.

### ReferÃªncias
[^1]: Chapter 16: Processes with Deterministic Time Trends
<!-- END -->
