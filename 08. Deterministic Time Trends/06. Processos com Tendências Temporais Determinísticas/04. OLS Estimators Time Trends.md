## Processos com Tendências Temporais Determinísticas: Distribuições Assintóticas e Transformações Canônicas

### Introdução
Este capítulo explora a análise de processos com **tendências temporais determinísticas**, um tema fundamental no estudo de séries temporais. Diferentemente de modelos com variáveis estacionárias, a presença de tendências temporais exige uma abordagem especial para a derivação das distribuições assintóticas dos estimadores. Como mencionado anteriormente, coeficientes de modelos de regressão envolvendo raízes unitárias ou tendências temporais determinísticas não podem ser tratados da mesma forma que coeficientes de modelos com variáveis estacionárias [^1]. As estimativas de diferentes parâmetros podem ter diferentes taxas de convergência assintótica. Este capítulo introduz a ideia de diferentes taxas de convergência e desenvolve uma abordagem geral para obter distribuições assintóticas, seguindo a metodologia sugerida por Sims, Stock e Watson (1990) [^1].

Este texto foca exclusivamente em processos com tendências temporais determinísticas, excluindo raízes unitárias. É demonstrado que as estatísticas *$t$* e *$F$* usuais, calculadas da forma convencional, possuem as mesmas distribuições assintóticas que em regressões estacionárias [^1]. No entanto, as técnicas usadas para verificar essas distribuições limites são distintas das utilizadas no Capítulo 8 [^1]. As técnicas desenvolvidas neste capítulo serão usadas para analisar a distribuição assintótica de processos que incluem raízes unitárias nos Capítulos 17 e 18 [^1].

### Conceitos Fundamentais
O capítulo começa com o exemplo mais simples de inovações *i.i.d.* em torno de uma tendência temporal determinística [^1]. A Seção 16.1 deriva as distribuições assintóticas das estimativas dos coeficientes para esse modelo, demonstrando a necessidade de reescalonar as variáveis para acomodar as diferentes taxas de convergência [^1]. A Seção 16.2 mostra que, apesar dessas diferentes taxas de convergência, as estatísticas *$t$* e *$F$* padrão têm as distribuições limites usuais para esse modelo [^1]. A Seção 16.3 desenvolve resultados análogos para uma autorregressão estacionária por covariância em torno de uma tendência temporal determinística, apresentando a técnica de Sims, Stock e Watson [^1].

#### Modelo de Tendência Temporal Simples
Consideramos a estimação por **mínimos quadrados ordinários (OLS)** dos parâmetros de uma tendência temporal simples [^1]:

$$ y_t = \alpha + \delta t + \epsilon_t $$ [16.1.1]

onde $\epsilon_t$ é um processo de ruído branco. Se $\epsilon_t \sim N(0, \sigma^2)$, o modelo [16.1.1] satisfaz as pressuposições clássicas de regressão [^1]. No entanto, a distribuição assintótica dos estimadores OLS de $\alpha$ e $\delta$ requer uma análise diferente daquela utilizada para regressões estacionárias [^2].

Para encontrar a distribuição limite em regressões com variáveis explicativas estacionárias, multiplica-se a Equação [16.1.6] por $\sqrt{T}$ [^2]:

$$ \sqrt{T}(b_T - \beta) = \left[ \frac{1}{T} \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \frac{1}{\sqrt{T}} \sum_{t=1}^T x_t \epsilon_t \right] $$ [16.1.7]

O procedimento usual assume que $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ converge em probabilidade para uma matriz não singular $Q$, enquanto $\frac{1}{\sqrt{T}} \sum_{t=1}^T x_t \epsilon_t$ converge em distribuição para uma variável aleatória $N(0, \sigma^2 Q)$, o que implica que $\sqrt{T}(b_T - \beta) \rightarrow N(0, \sigma^2 Q^{-1})$ [^2]. Contudo, esse argumento não pode ser diretamente aplicado a uma tendência temporal determinística, dada a natureza de $x_t$ e $\beta$ nas Equações [16.1.3] e [16.1.4] [^2]. Em particular, a Equação [16.1.6] torna-se:

$$ \begin{bmatrix} \hat{\alpha}_T - \alpha \\ \hat{\delta}_T - \delta \end{bmatrix} = \begin{bmatrix} \sum_1 \epsilon_t & \sum t \epsilon_t \\ \sum t \epsilon_t & \sum t^2 \epsilon_t \end{bmatrix}^{-1} \begin{bmatrix} \sum \epsilon_t \\ \sum t \epsilon_t \end{bmatrix} $$ [16.1.8]

Onde $\sum$ denota a soma de $t=1$ até $T$. É demonstrado por indução que:
$$ \sum_{t=1}^{T} t = \frac{T(T+1)}{2} $$ [16.1.9]
$$ \sum_{t=1}^{T} t^2 = \frac{T(T+1)(2T+1)}{6} $$ [16.1.10]

*Prova da Equação [16.1.9]:*
I. Caso base: para $T=1$, $\sum_{t=1}^1 t = 1 = \frac{1(1+1)}{2}$.
II. Hipótese indutiva: assuma que $\sum_{t=1}^k t = \frac{k(k+1)}{2}$ para algum inteiro $k \geq 1$.
III. Passo indutivo: precisamos mostrar que $\sum_{t=1}^{k+1} t = \frac{(k+1)(k+2)}{2}$.
     $\sum_{t=1}^{k+1} t = \sum_{t=1}^k t + (k+1) = \frac{k(k+1)}{2} + (k+1) = \frac{k(k+1) + 2(k+1)}{2} = \frac{(k+1)(k+2)}{2}$
IV. Portanto, por indução matemática, $\sum_{t=1}^{T} t = \frac{T(T+1)}{2}$ para todo $T \geq 1$. $\blacksquare$

*Prova da Equação [16.1.10]:*
I. Caso base: para $T=1$, $\sum_{t=1}^1 t^2 = 1 = \frac{1(1+1)(2(1)+1)}{6}$.
II. Hipótese indutiva: assuma que $\sum_{t=1}^k t^2 = \frac{k(k+1)(2k+1)}{6}$ para algum inteiro $k \geq 1$.
III. Passo indutivo: precisamos mostrar que $\sum_{t=1}^{k+1} t^2 = \frac{(k+1)(k+2)(2(k+1)+1)}{6} = \frac{(k+1)(k+2)(2k+3)}{6}$.
    $\sum_{t=1}^{k+1} t^2 = \sum_{t=1}^k t^2 + (k+1)^2 = \frac{k(k+1)(2k+1)}{6} + (k+1)^2 = \frac{k(k+1)(2k+1) + 6(k+1)^2}{6}$
    $= \frac{(k+1)[k(2k+1) + 6(k+1)]}{6} = \frac{(k+1)[2k^2+k + 6k+6]}{6} = \frac{(k+1)(2k^2+7k+6)}{6}$
   $= \frac{(k+1)(k+2)(2k+3)}{6}$
IV. Portanto, por indução matemática, $\sum_{t=1}^{T} t^2 = \frac{T(T+1)(2T+1)}{6}$ para todo $T \geq 1$. $\blacksquare$

O termo dominante em $\sum_{t=1}^T t$ é $\frac{T^2}{2}$, ou seja:
$$ \frac{1}{T^2} \sum_{t=1}^T t = \frac{1}{T^2} \left[ \frac{T^2}{2} + \frac{T}{2} \right] = \frac{1}{2} + \frac{1}{2T} \rightarrow \frac{1}{2} $$ [16.1.11]
Analogamente, o termo dominante em $\sum_{t=1}^T t^2$ é $\frac{T^3}{3}$:
$$ \frac{1}{T^3} \sum_{t=1}^T t^2 = \frac{1}{T^3} \left[ \frac{2T^3}{6} + \frac{3T^2}{6} + \frac{T}{6} \right] = \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2} \rightarrow \frac{1}{3} $$ [16.1.12]
O padrão geral para o termo dominante em $\sum_{t=1}^T t^v$ é $\frac{T^{v+1}}{v+1}$:
$$ \frac{1}{T^{v+1}} \sum_{t=1}^T t^v \rightarrow \frac{1}{v+1} $$ [16.1.13]
Em particular,
$$ \frac{1}{T^{v+1}} \sum_{t=1}^T t^v = \frac{1}{T} \sum_{t=1}^T \left( \frac{t}{T} \right)^v $$ [16.1.14]
O lado direito de [16.1.14] pode ser visto como uma aproximação da área sob a curva $f(r) = r^v$ para $r$ entre zero e um [^3].

Para $x_t$ dado em [16.1.3], os resultados [16.1.9] e [16.1.10] implicam que:

$$ \sum_{t=1}^T x_t x_t' = \begin{bmatrix} \sum 1 & \sum t \\ \sum t & \sum t^2 \end{bmatrix} = \begin{bmatrix} T & T(T+1)/2 \\ T(T+1)/2 & T(T+1)(2T+1)/6 \end{bmatrix} $$ [16.1.16]

A matriz $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ diverge, o que difere do resultado usual para regressões estacionárias. Para obter uma matriz convergente, a matriz em [16.1.16] teria que ser dividida por $T^3$ em vez de $T$ [^4]. No entanto, essa matriz limite não é invertível [^4].

Para contornar esse problema, os estimadores OLS $\hat{\alpha}_T$ e $\hat{\delta}_T$ têm diferentes taxas de convergência assintótica. Para obter distribuições limites não degeneradas, $\hat{\alpha}_T$ é multiplicado por $\sqrt{T}$, enquanto $\hat{\delta}_T$ é multiplicado por $T^{3/2}$ [^4]. Essa correção pode ser vista como uma pré-multiplicação de [16.1.6] ou [16.1.8] pela matriz [^4]:

$$ \Upsilon_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} $$ [16.1.17]
Resultando em:

$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} = \Upsilon_T \left[ \sum_{t=1}^T x_t x_t' \right]^{-1} \sum_{t=1}^T x_t \epsilon_t = \Upsilon_T \left[ \sum_{t=1}^T x_t x_t' \right]^{-1} \Upsilon_T^{-1} \Upsilon_T \sum_{t=1}^T x_t \epsilon_t = \left\{ \Upsilon_T \left[ \sum_{t=1}^T x_t x_t' \right]^{-1} \Upsilon_T^{-1} \right\} \left\{ \Upsilon_T \sum_{t=1}^T x_t \epsilon_t \right\} $$ [16.1.18]
O primeiro termo da expressão [16.1.18], substituindo [16.1.17] e [16.1.16], é:
$$ \Upsilon_T \left[ \sum_{t=1}^T x_t x_t' \right]^{-1} \Upsilon_T^{-1} = \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} \begin{bmatrix} \sum 1 & \sum t \\ \sum t & \sum t^2 \end{bmatrix}^{-1} \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} = \begin{bmatrix} T^{-1}\sum 1 & T^{-2} \sum t \\ T^{-2} \sum t & T^{-3} \sum t^2 \end{bmatrix}^{-1} \rightarrow Q $$ [16.1.19]

Onde $Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$ [16.1.20].

O segundo termo em [16.1.18] é:
$$ \Upsilon_T \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} \sum \epsilon_t \\ \sum t \epsilon_t \end{bmatrix} = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum \epsilon_t \\ \frac{1}{\sqrt{T}} \sum \frac{t}{T} \epsilon_t \end{bmatrix} $$ [16.1.21]
Sob pressuposições padrões sobre $\epsilon_t$, este vetor é assintoticamente gaussiano. O primeiro elemento converge para $N(0, \sigma^2)$ pelo teorema do limite central [^5]. O segundo elemento satisfaz as condições da Proposição 7.8, sendo uma sequência de diferença martingale [^5].

A distribuição conjunta dos dois elementos no vetor em [16.1.21] é assintoticamente gaussiana [^6]. A forma geral para qualquer combinação linear destes elementos é:
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^T [\lambda_1 + \lambda_2 \frac{t}{T}] \epsilon_t $$
Para $\lambda = (\lambda_1, \lambda_2)'$, a matriz $Q$ em [16.1.20] e a variância é:
$$ \frac{1}{T} \sum_{t=1}^T [\lambda_1 + \lambda_2 \frac{t}{T}]^2 \rightarrow \sigma^2 \lambda' Q \lambda $$
Portanto, qualquer combinação linear dos dois elementos no vetor em [16.1.21] é assintoticamente gaussiana, o que implica uma distribuição bivariada gaussiana limite [^6].

A distribuição assintótica de [16.1.18] pode ser calculada como no Exemplo 7.5 do Capítulo 7 [^7]:

$$ \begin{bmatrix} \sqrt{T} (\hat{\alpha}_T - \alpha) \\ T^{3/2} (\hat{\delta}_T - \delta) \end{bmatrix} \rightarrow N(0, \sigma^2 Q^{-1}) $$ [16.1.25]
Em resumo, se $y_t$ é gerado por uma tendência temporal determinística simples [16.1.1] e $\epsilon_t$ é *i.i.d.* com $E(\epsilon_t^2) = \sigma^2$ e $E(\epsilon_t^4) < \infty$, então [^7]:
$$ \begin{bmatrix} \sqrt{T} (\hat{\alpha}_T - \alpha) \\ T^{3/2} (\hat{\delta}_T - \delta) \end{bmatrix} \rightarrow N \left( 0, \sigma^2 \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} \right) $$ [16.1.26]
Note que o estimador do coeficiente da tendência temporal ($\hat{\delta}_T$) é super consistente, isto é, $T(\hat{\delta}_T - \delta) \rightarrow 0$ [^7].

> 💡 **Exemplo Numérico:**
> Vamos simular um processo com tendência temporal e observar as taxas de convergência dos estimadores.
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
>
> # Parâmetros
> T = 1000
> alpha = 5
> delta = 0.2
> sigma = 2
>
> # Simulação dos dados
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, sigma, T)
> y = alpha + delta * t + epsilon
>
> # Regressão OLS
> X = np.column_stack((np.ones(T), t))
> model = sm.OLS(y, X)
> results = model.fit()
>
> # Estimativas
> alpha_hat = results.params[0]
> delta_hat = results.params[1]
>
> # Erros
> alpha_error = alpha_hat - alpha
> delta_error = delta_hat - delta
>
> # Visualização dos dados
> plt.figure(figsize=(10, 5))
> plt.plot(t,y, label='Série Temporal com Tendência')
> plt.plot(t, alpha+delta*t, label='Tendência Determinística')
> plt.xlabel('Tempo (t)')
> plt.ylabel('y')
> plt.title('Série Temporal com Tendência')
> plt.legend()
> plt.show()
>
> # Resultados
> print(f"Verdadeiro alpha: {alpha}")
> print(f"Estimado alpha: {alpha_hat}")
> print(f"Erro de alpha: {alpha_error}")
> print(f"Verdadeiro delta: {delta}")
> print(f"Estimado delta: {delta_hat}")
> print(f"Erro de delta: {delta_error}")
>
> # Análise das taxas de convergência
> print("\nAnálise da taxa de convergência:")
> print(f"sqrt(T) * erro de alpha: {np.sqrt(T) * alpha_error}")
> print(f"T^(3/2) * erro de delta: {T**(3/2) * delta_error}")
>
> # Simulação para vários valores de T
> num_sims = 100
> T_values = [100, 500, 1000, 2000, 5000]
> convergence_results = {}
>
> for T in T_values:
>     alpha_errors = []
>     delta_errors = []
>     for _ in range(num_sims):
>         t = np.arange(1, T + 1)
>         epsilon = np.random.normal(0, sigma, T)
>         y = alpha + delta * t + epsilon
>         X = np.column_stack((np.ones(T), t))
>         model = sm.OLS(y, X)
>         results = model.fit()
>         alpha_errors.append(results.params[0] - alpha)
>         delta_errors.append(results.params[1] - delta)
>     convergence_results[T] = {
>         'alpha_error_scaled': [np.sqrt(T) * err for err in alpha_errors],
>         'delta_error_scaled': [T**(3/2) * err for err in delta_errors]
>     }
>
> # Comparação das taxas de convergência para diferentes T's
> print("\nComparação das taxas de convergência:")
> for T in T_values:
>    print(f"\nT = {T}:")
>    print(f"  Média de sqrt(T) * erro de alpha: {np.mean(convergence_results[T]['alpha_error_scaled']):.4f}")
>    print(f"  Desvio padrão de sqrt(T) * erro de alpha: {np.std(convergence_results[T]['alpha_error_scaled']):.4f}")
>    print(f"  Média de T^(3/2) * erro de delta: {np.mean(convergence_results[T]['delta_error_scaled']):.4f}")
>    print(f"  Desvio padrão de T^(3/2) * erro de delta: {np.std(convergence_results[T]['delta_error_scaled']):.4f}")
> ```
> Este exemplo ilustra que, ao aumentarmos o tamanho da amostra ($T$), o erro de $\hat{\delta}$ multiplicado por $T^{3/2}$ permanece relativamente constante, enquanto o erro de $\hat{\alpha}$ multiplicado por $\sqrt{T}$ também estabiliza. Isso confirma as diferentes taxas de convergência assintótica. O código gera também um gráfico que ilustra a série temporal e sua tendência, tornando mais fácil a interpretação dos resultados.
>
> Os resultados numéricos mostram as estimativas e os erros dos parâmetros, e a parte final do código demonstra que os erros de $\hat{\alpha}$ e $\hat{\delta}$ quando multiplicados por $\sqrt{T}$ e $T^{3/2}$, respectivamente, convergem para uma distribuição normal, embora com taxas diferentes.
>

**Lema 1.1.** A super consistência do estimador $\hat{\delta}_T$ implica que para qualquer $\epsilon > 0$, existe $T_0$ tal que para todo $T > T_0$, $P(|\hat{\delta}_T - \delta| > \epsilon) < \epsilon$.

*Prova:*
I. A super consistência de $\hat{\delta}_T$ significa que $T(\hat{\delta}_T - \delta) \overset{p}{\to} 0$.
II. Isso é equivalente a dizer que para todo $\varepsilon > 0$, $\lim_{T \to \infty} P(|T(\hat{\delta}_T - \delta)| > \varepsilon) = 0$.
III.  Isso implica que para qualquer $\epsilon > 0$ e qualquer $\varepsilon' > 0$, existe um $T_0$ tal que para todo $T > T_0$, $P(|T(\hat{\delta}_T - \delta)| > \varepsilon') < \epsilon$.
IV. Escolhendo $\varepsilon' = T\epsilon$, temos que $P(|T(\hat{\delta}_T - \delta)| > T\epsilon) = P(|\hat{\delta}_T - \delta| > \epsilon) < \epsilon$ para todo $T > T_0$, comprovando a afirmação. $\blacksquare$

**Lema 1.2.** A consistência de $\hat{\alpha}_T$ implica que para qualquer $\epsilon > 0$, $\lim_{T \to \infty} P(|\hat{\alpha}_T - \alpha| > \epsilon) = 0$.

*Prova:*
I. A consistência de $\hat{\alpha}_T$ significa que $\hat{\alpha}_T \overset{p}{\to} \alpha$.
II. Isso é equivalente a dizer que para todo $\epsilon > 0$, $\lim_{T \to \infty} P(|\hat{\alpha}_T - \alpha| > \epsilon) = 0$.
III. Portanto, pela definição de convergência em probabilidade, a afirmação é comprovada. $\blacksquare$

#### Taxas de Convergência
Diferentes taxas de convergência podem ser descritas em termos de ordem em probabilidade. Uma sequência de variáveis aleatórias $\{X_T\}_{T=1}^\infty$ é dita ser $O_p(T^{-1/2})$ se, para todo $\epsilon > 0$, existe um $M > 0$ tal que [^7]:
$$ P\{|X_T| > M/\sqrt{T}\} < \epsilon $$ [16.1.28]
Isso significa que $\sqrt{T}X_T$ tem alta probabilidade de estar dentro de $\pm M$ para qualquer $T$. Estimadores para séries temporais estacionárias são tipicamente $O_p(T^{-1/2})$. Por exemplo, a média de uma amostra de tamanho $T$, $\bar{X}_T = \frac{1}{T} \sum_{t=1}^T y_t$, onde $y_t$ é *i.i.d.* com média zero e variância $\sigma^2$, tem variância $\sigma^2/T$ e é $O_p(T^{-1/2})$ [^7]. De modo geral, $\{X_T\}$ é dita ser $O_p(T^{-k})$ se, para todo $\epsilon > 0$, existe um $M>0$ tal que:
$$ P\{|X_T| > M/T^k\} < \epsilon $$ [16.1.29]

Assim, o estimador $\hat{\delta}_T$ em [16.1.26] é $O_p(T^{-3/2})$, visto que existe uma faixa $\pm M$ em torno de $T^{3/2}(\hat{\delta}_T - \delta)$ que contém a maior parte da distribuição de probabilidade [^7].

#### Testes de Hipóteses
Se as inovações $\epsilon_t$ para a tendência temporal simples [16.1.1] forem gaussianas, os estimadores OLS $\hat{\alpha}_T$ e $\hat{\delta}_T$ são gaussianos e os testes *$t$* e *$F$* padrão têm distribuições exatas de *$t$* e *$F$* para todos os tamanhos de amostra $T$ [^8]. Isso sugere que os testes usuais *$t$* e *$F$* são assintoticamente válidos mesmo quando as inovações não são gaussianas.

O teste *$t$* OLS da hipótese nula $\alpha = \alpha_0$ pode ser escrito como [^8]:
$$ t_\tau = \frac{\hat{\alpha}_T - \alpha_0}{s_T \left\{ [1 \ \ 0] (X_T' X_T)^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} \right\}^{1/2}} $$ [16.2.1]
onde $s_T^2$ é o estimador OLS da variância $\sigma^2$ [^8]. Multiplicando o numerador e o denominador por $\sqrt{T}$:

$$ t_\tau = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{s_T \left\{ [ \sqrt{T} \ \ 0] (X_T' X_T)^{-1} \begin{bmatrix} \sqrt{T} \\ 0 \end{bmatrix} \right\}^{1/2}} $$ [16.2.3]
Da Equação [16.1.17], $[\sqrt{T} \ \ 0] = [1 \ \ 0] \Upsilon_T$. Substituindo em [16.2.3]:

$$ t_\tau = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{s_T \left\{ [1 \ \ 0] \Upsilon_T (X_T' X_T)^{-1} \Upsilon_T \begin{bmatrix} 1 \\ 0 \end{bmatrix} \right\}^{1/2}} $$ [16.2.5]
Da Equação [16.1.19], $\Upsilon_T(X_T' X_T)^{-1} \Upsilon_T \rightarrow Q^{-1}$, onde $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$. Portanto:
$$ t_\tau = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{\sigma \sqrt{q^{11}}} $$ [16.2.7]
que é uma variável gaussiana assintótica dividida pela raiz quadrada de sua variância, que resulta em uma distribuição $N(0, 1)$ [^8].

Analogamente, o teste *$t$* OLS de $\delta = \delta_0$ é [^8]:

$$ t_\tau = \frac{\hat{\delta}_T - \delta_0}{s_T \left\{ [0 \ \ 1] (X_T' X_T)^{-1} \begin{bmatrix} 0 \\ 1 \end{bmatrix} \right\}^{1/2}} $$
Multiplicando o numerador e o denominador por $T^{3/2}$:
$$ t_\tau = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{s_T \left\{ [0 \ \ T^{3/2}] (X_T' X_T)^{-1} \begin{bmatrix} 0 \\ T^{3/2} \end{bmatrix} \right\}^{1/2}} = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\sigma \sqrt{q^{22}}} $$

Essa estatística é assintoticamente uma variável $N(0,1)$. Embora $\hat{\alpha}_T$ e $\hat{\delta}_T$ convirjam em taxas diferentes, seus erros padrão também incorporam diferentes ordens de $T$, o que torna os testes *$t$* usuais assintoticamente válidos [^8].

> 💡 **Exemplo Numérico:**
> Vamos testar as hipóteses nulas para $\alpha$ e $\delta$ utilizando os dados simulados do exemplo anterior.
> ```python
> import numpy as np
> import statsmodels.api as sm
>
> # Parâmetros
> T = 1000
> alpha = 5
> delta = 0.2
> sigma = 2
>
> # Simulação dos dados
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, sigma, T)
> y = alpha + delta * t + epsilon
>
> # Regressão OLS
> X = np.column_stack((np.ones(T), t))
> model = sm.OLS(y, X)
> results = model.fit()
>
> # Teste de hipótese para alpha
> alpha_0 = 0
> t_alpha = results.tvalues[0]
> p_alpha = results.pvalues[0]
>
> print(f"Teste t para H0: alpha = {alpha_0}")
> print(f"Estatística t: {t_alpha:.4f}")
> print(f"Valor p: {p_alpha:.4f}")
>
> # Teste de hipótese para delta
> delta_0 = 0
> t_delta = results.tvalues[1]
> p_delta = results.pvalues[1]
>
> print(f"\nTeste t para H0: delta = {delta_0}")
> print(f"Estatística t: {t_delta:.4f}")
> print(f"Valor p: {p_delta:.4f}")
>
> # Cálculo dos intervalos de confiança para alpha e delta
> alpha_ci = results.conf_int()[0]
> delta_ci = results.conf_int()[1]
> print(f"\nIntervalo de confiança 95% para alpha: {alpha_ci}")
> print(f"Intervalo de confiança 95% para delta: {delta_ci}")
>
> ```
> Este código executa testes *$t$* para as hipóteses nulas $\alpha = 0$ e $\delta = 0$, e calcula os respectivos valores p, além dos intervalos de confiança dos parâmetros. A baixa probabilidade de rejeitar a hipótese nula de que os coeficientes são zero fornece evidências estatísticas de que ambos são significativamente diferentes de zero. Os intervalos de confiança também ajudam a entender a precisão das estimativas.

**Teorema 1.1.** Os testes *$t$* para $\alpha$ e $\delta$ convergem em distribuição para uma variável $N(0,1)$ sob as condições estabelecidas.

*Prova:*
I. Como demonstrado na análise das equações 16.2.7 e posteriores, as estatísticas *$t$* para os estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$ convergem para variáveis aleatórias com distribuição $N(0,1)$ sob as premissas dadas para o erro $\epsilon_t$.
II. Portanto, os testes *$t$* são assintoticamente válidos, como afirmado.  $\blacksquare$

A técnica de Sims, Stock e Watson (1990), transforma o modelo de regressão original em uma forma canônica, onde a distribuição assintótica é mais simples de descrever [^1].

**Proposição 1.1**  Se as premissas para os testes t forem satisfeitas, então os testes t para $\alpha$ e $\delta$ são assintoticamente válidos.

*Prova:*
I. Como demonstrado, as estatísticas de teste t para $\alpha$ e $\delta$ convergem em distribuição para variáveis $N(0,1)$, sob as premissas estabelecidas (inovações i.i.d. com variância finita).
II. Assim, a distribuição assintótica das estatísticas de teste é a distribuição normal padrão.
III. Isso garante a validade assintótica dos testes. $\blacksquare$

#### Teste de Hipóteses Envolvendo Múltiplos Parâmetros

Considere o teste da hipótese $H_0: r_1 \alpha + r_2 \delta = r$. A estatística *$t$* para esta hipótese é [^9]:
$$ t_T = \frac{(r_1 \hat{\alpha}_T + r_2 \hat{\delta}_T - r)}{\sqrt{s_T^2 [r_1 \ \ r_2] (X_T' X_T)^{-1} \begin{bmatrix} r_1 \\ r_2 \end{bmatrix} }} $$

Multiplicando o numerador e o denominador por $\sqrt{T}$:
$$ t_T = \frac{\sqrt{T}(r_1 \hat{\alpha}_T + r_2 \hat{\delta}_T - r)}{\sqrt{s_T^2 [r_1 \ \ r_2] (X_T' X_T)^{-1} \begin{bmatrix} r_1 \\ r_2 \end{bmatrix} }} $$

Definindo $r_T = \begin{bmatrix} r_1 \\ r_2/T \end{bmatrix}$,e $R_T = [1 \ \ 0]$, podemos reescrever a estatística de teste como:

$$ t = \frac{R_T \hat{\beta}_T - r_T}{\sqrt{s_T^2 R_T (X_T' X_T)^{-1} R_T' }} $$

Sob a hipótese nula $H_0: R_T \beta = r_T$, esta estatística segue uma distribuição t de Student com $T-k$ graus de liberdade.

### Teste de Wald

O teste de Wald é uma abordagem alternativa para testar restrições lineares sobre os coeficientes do modelo. Ele usa a distribuição assintótica do estimador de mínimos quadrados ordinários (MQO). A estatística de teste de Wald é dada por:

$$ W = (R_T \hat{\beta}_T - r_T)' [R_T Var(\hat{\beta}_T) R_T']^{-1} (R_T \hat{\beta}_T - r_T) $$

Onde $Var(\hat{\beta}_T) = s_T^2 (X_T' X_T)^{-1}$.  Sob a hipótese nula, a estatística de Wald segue uma distribuição qui-quadrado com $q$ graus de liberdade, onde $q$ é o número de restrições (o número de linhas em $R_T$).

### Teste da Razão de Verossimilhança (RV)

O teste da razão de verossimilhança (RV) compara a verossimilhança do modelo estimado sem restrições com a verossimilhança do modelo estimado com restrições. A estatística do teste é dada por:

$$ LR = -2 [log(L_R) - log(L_{UR})] $$

Onde $L_R$ é a verossimilhança do modelo restrito e $L_{UR}$ é a verossimilhança do modelo irrestrito. Sob a hipótese nula, a estatística LR segue uma distribuição qui-quadrado com $q$ graus de liberdade.

### Comparação dos Testes

* **Teste t:** Útil para testar hipóteses sobre um único coeficiente ou uma única restrição linear. É baseado na distribuição t de Student, que é mais apropriada para amostras pequenas.

* **Teste de Wald:**  Pode ser usado para testar múltiplas restrições lineares. É baseado na distribuição qui-quadrado, que é uma aproximação assintótica.

* **Teste da Razão de Verossimilhança (RV):** Uma abordagem geral para testar restrições de modelos estatísticos. É também baseado na distribuição qui-quadrado e pode ser usado para testar restrições não lineares.

Em geral, o teste t é mais apropriado quando se tem uma única restrição. Os testes de Wald e RV são mais adequados para testar múltiplas restrições. O teste de RV é um teste mais geral, mas pode ser computacionalmente mais intensivo que o teste de Wald.
<!-- END -->
