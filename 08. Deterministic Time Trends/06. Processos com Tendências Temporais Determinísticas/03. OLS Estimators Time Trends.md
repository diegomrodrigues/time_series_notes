## Processos com Tend√™ncias Temporais Determin√≠sticas: Distribui√ß√µes Assint√≥ticas e Transforma√ß√µes Can√¥nicas

### Introdu√ß√£o
Este cap√≠tulo explora a an√°lise de processos com **tend√™ncias temporais determin√≠sticas**, um tema fundamental no estudo de s√©ries temporais. Diferentemente de modelos com vari√°veis estacion√°rias, a presen√ßa de tend√™ncias temporais exige uma abordagem especial para a deriva√ß√£o das distribui√ß√µes assint√≥ticas dos estimadores. Como mencionado anteriormente, coeficientes de modelos de regress√£o envolvendo ra√≠zes unit√°rias ou tend√™ncias temporais determin√≠sticas n√£o podem ser tratados da mesma forma que coeficientes de modelos com vari√°veis estacion√°rias [^1]. As estimativas de diferentes par√¢metros podem ter diferentes taxas de converg√™ncia assint√≥tica. Este cap√≠tulo introduz a ideia de diferentes taxas de converg√™ncia e desenvolve uma abordagem geral para obter distribui√ß√µes assint√≥ticas, seguindo a metodologia sugerida por Sims, Stock e Watson (1990) [^1].

Este texto foca exclusivamente em processos com tend√™ncias temporais determin√≠sticas, excluindo ra√≠zes unit√°rias. √â demonstrado que as estat√≠sticas *$t$* e *$F$* usuais, calculadas da forma convencional, possuem as mesmas distribui√ß√µes assint√≥ticas que em regress√µes estacion√°rias [^1]. No entanto, as t√©cnicas usadas para verificar essas distribui√ß√µes limites s√£o distintas das utilizadas no Cap√≠tulo 8 [^1]. As t√©cnicas desenvolvidas neste cap√≠tulo ser√£o usadas para analisar a distribui√ß√£o assint√≥tica de processos que incluem ra√≠zes unit√°rias nos Cap√≠tulos 17 e 18 [^1].

### Conceitos Fundamentais
O cap√≠tulo come√ßa com o exemplo mais simples de inova√ß√µes *i.i.d.* em torno de uma tend√™ncia temporal determin√≠stica [^1]. A Se√ß√£o 16.1 deriva as distribui√ß√µes assint√≥ticas das estimativas dos coeficientes para esse modelo, demonstrando a necessidade de reescalonar as vari√°veis para acomodar as diferentes taxas de converg√™ncia [^1]. A Se√ß√£o 16.2 mostra que, apesar dessas diferentes taxas de converg√™ncia, as estat√≠sticas *$t$* e *$F$* padr√£o t√™m as distribui√ß√µes limites usuais para esse modelo [^1]. A Se√ß√£o 16.3 desenvolve resultados an√°logos para uma autorregress√£o estacion√°ria por covari√¢ncia em torno de uma tend√™ncia temporal determin√≠stica, apresentando a t√©cnica de Sims, Stock e Watson [^1].

#### Modelo de Tend√™ncia Temporal Simples
Consideramos a estima√ß√£o por **m√≠nimos quadrados ordin√°rios (OLS)** dos par√¢metros de uma tend√™ncia temporal simples [^1]:

$$ y_t = \alpha + \delta t + \epsilon_t $$ [16.1.1]

onde $\epsilon_t$ √© um processo de ru√≠do branco. Se $\epsilon_t \sim N(0, \sigma^2)$, o modelo [16.1.1] satisfaz as pressuposi√ß√µes cl√°ssicas de regress√£o [^1]. No entanto, a distribui√ß√£o assint√≥tica dos estimadores OLS de $\alpha$ e $\delta$ requer uma an√°lise diferente daquela utilizada para regress√µes estacion√°rias [^2].

Para encontrar a distribui√ß√£o limite em regress√µes com vari√°veis explicativas estacion√°rias, multiplica-se a Equa√ß√£o [16.1.6] por $\sqrt{T}$ [^2]:

$$ \sqrt{T}(b_T - \beta) = \left[ \frac{1}{T} \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \frac{1}{\sqrt{T}} \sum_{t=1}^T x_t \epsilon_t \right] $$ [16.1.7]

O procedimento usual assume que $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ converge em probabilidade para uma matriz n√£o singular $Q$, enquanto $\frac{1}{\sqrt{T}} \sum_{t=1}^T x_t \epsilon_t$ converge em distribui√ß√£o para uma vari√°vel aleat√≥ria $N(0, \sigma^2 Q)$, o que implica que $\sqrt{T}(b_T - \beta) \rightarrow N(0, \sigma^2 Q^{-1})$ [^2]. Contudo, esse argumento n√£o pode ser diretamente aplicado a uma tend√™ncia temporal determin√≠stica, dada a natureza de $x_t$ e $\beta$ nas Equa√ß√µes [16.1.3] e [16.1.4] [^2]. Em particular, a Equa√ß√£o [16.1.6] torna-se:

$$ \begin{bmatrix} \hat{\alpha}_T - \alpha \\ \hat{\delta}_T - \delta \end{bmatrix} = \begin{bmatrix} \sum_1 \epsilon_t & \sum t \epsilon_t \\ \sum t \epsilon_t & \sum t^2 \epsilon_t \end{bmatrix}^{-1} \begin{bmatrix} \sum \epsilon_t \\ \sum t \epsilon_t \end{bmatrix} $$ [16.1.8]

Onde $\sum$ denota a soma de $t=1$ at√© $T$. √â demonstrado por indu√ß√£o que:
$$ \sum_{t=1}^{T} t = \frac{T(T+1)}{2} $$ [16.1.9]
$$ \sum_{t=1}^{T} t^2 = \frac{T(T+1)(2T+1)}{6} $$ [16.1.10]

*Prova da Equa√ß√£o [16.1.9]:*
I. Caso base: para $T=1$, $\sum_{t=1}^1 t = 1 = \frac{1(1+1)}{2}$.
II. Hip√≥tese indutiva: assuma que $\sum_{t=1}^k t = \frac{k(k+1)}{2}$ para algum inteiro $k \geq 1$.
III. Passo indutivo: precisamos mostrar que $\sum_{t=1}^{k+1} t = \frac{(k+1)(k+2)}{2}$.
     $\sum_{t=1}^{k+1} t = \sum_{t=1}^k t + (k+1) = \frac{k(k+1)}{2} + (k+1) = \frac{k(k+1) + 2(k+1)}{2} = \frac{(k+1)(k+2)}{2}$
IV. Portanto, por indu√ß√£o matem√°tica, $\sum_{t=1}^{T} t = \frac{T(T+1)}{2}$ para todo $T \geq 1$. $\blacksquare$

*Prova da Equa√ß√£o [16.1.10]:*
I. Caso base: para $T=1$, $\sum_{t=1}^1 t^2 = 1 = \frac{1(1+1)(2(1)+1)}{6}$.
II. Hip√≥tese indutiva: assuma que $\sum_{t=1}^k t^2 = \frac{k(k+1)(2k+1)}{6}$ para algum inteiro $k \geq 1$.
III. Passo indutivo: precisamos mostrar que $\sum_{t=1}^{k+1} t^2 = \frac{(k+1)(k+2)(2(k+1)+1)}{6} = \frac{(k+1)(k+2)(2k+3)}{6}$.
    $\sum_{t=1}^{k+1} t^2 = \sum_{t=1}^k t^2 + (k+1)^2 = \frac{k(k+1)(2k+1)}{6} + (k+1)^2 = \frac{k(k+1)(2k+1) + 6(k+1)^2}{6}$
    $= \frac{(k+1)[k(2k+1) + 6(k+1)]}{6} = \frac{(k+1)[2k^2+k + 6k+6]}{6} = \frac{(k+1)(2k^2+7k+6)}{6}$
   $= \frac{(k+1)(k+2)(2k+3)}{6}$
IV. Portanto, por indu√ß√£o matem√°tica, $\sum_{t=1}^{T} t^2 = \frac{T(T+1)(2T+1)}{6}$ para todo $T \geq 1$. $\blacksquare$

O termo dominante em $\sum_{t=1}^T t$ √© $\frac{T^2}{2}$, ou seja:
$$ \frac{1}{T^2} \sum_{t=1}^T t = \frac{1}{T^2} \left[ \frac{T^2}{2} + \frac{T}{2} \right] = \frac{1}{2} + \frac{1}{2T} \rightarrow \frac{1}{2} $$ [16.1.11]
Analogamente, o termo dominante em $\sum_{t=1}^T t^2$ √© $\frac{T^3}{3}$:
$$ \frac{1}{T^3} \sum_{t=1}^T t^2 = \frac{1}{T^3} \left[ \frac{2T^3}{6} + \frac{3T^2}{6} + \frac{T}{6} \right] = \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2} \rightarrow \frac{1}{3} $$ [16.1.12]
O padr√£o geral para o termo dominante em $\sum_{t=1}^T t^v$ √© $\frac{T^{v+1}}{v+1}$:
$$ \frac{1}{T^{v+1}} \sum_{t=1}^T t^v \rightarrow \frac{1}{v+1} $$ [16.1.13]
Em particular,
$$ \frac{1}{T^{v+1}} \sum_{t=1}^T t^v = \frac{1}{T} \sum_{t=1}^T \left( \frac{t}{T} \right)^v $$ [16.1.14]
O lado direito de [16.1.14] pode ser visto como uma aproxima√ß√£o da √°rea sob a curva $f(r) = r^v$ para $r$ entre zero e um [^3].

Para $x_t$ dado em [16.1.3], os resultados [16.1.9] e [16.1.10] implicam que:

$$ \sum_{t=1}^T x_t x_t' = \begin{bmatrix} \sum 1 & \sum t \\ \sum t & \sum t^2 \end{bmatrix} = \begin{bmatrix} T & T(T+1)/2 \\ T(T+1)/2 & T(T+1)(2T+1)/6 \end{bmatrix} $$ [16.1.16]

A matriz $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ diverge, o que difere do resultado usual para regress√µes estacion√°rias. Para obter uma matriz convergente, a matriz em [16.1.16] teria que ser dividida por $T^3$ em vez de $T$ [^4]. No entanto, essa matriz limite n√£o √© invert√≠vel [^4].

Para contornar esse problema, os estimadores OLS $\hat{\alpha}_T$ e $\hat{\delta}_T$ t√™m diferentes taxas de converg√™ncia assint√≥tica. Para obter distribui√ß√µes limites n√£o degeneradas, $\hat{\alpha}_T$ √© multiplicado por $\sqrt{T}$, enquanto $\hat{\delta}_T$ √© multiplicado por $T^{3/2}$ [^4]. Essa corre√ß√£o pode ser vista como uma pr√©-multiplica√ß√£o de [16.1.6] ou [16.1.8] pela matriz [^4]:

$$ \Upsilon_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} $$ [16.1.17]
Resultando em:

$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} = \Upsilon_T \left[ \sum_{t=1}^T x_t x_t' \right]^{-1} \sum_{t=1}^T x_t \epsilon_t = \Upsilon_T \left[ \sum_{t=1}^T x_t x_t' \right]^{-1} \Upsilon_T^{-1} \Upsilon_T \sum_{t=1}^T x_t \epsilon_t = \left\{ \Upsilon_T \left[ \sum_{t=1}^T x_t x_t' \right]^{-1} \Upsilon_T^{-1} \right\} \left\{ \Upsilon_T \sum_{t=1}^T x_t \epsilon_t \right\} $$ [16.1.18]
O primeiro termo da express√£o [16.1.18], substituindo [16.1.17] e [16.1.16], √©:
$$ \Upsilon_T \left[ \sum_{t=1}^T x_t x_t' \right]^{-1} \Upsilon_T^{-1} = \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} \begin{bmatrix} \sum 1 & \sum t \\ \sum t & \sum t^2 \end{bmatrix}^{-1} \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} = \begin{bmatrix} T^{-1}\sum 1 & T^{-2} \sum t \\ T^{-2} \sum t & T^{-3} \sum t^2 \end{bmatrix}^{-1} \rightarrow Q $$ [16.1.19]

Onde $Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$ [16.1.20].

O segundo termo em [16.1.18] √©:
$$ \Upsilon_T \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} \sum \epsilon_t \\ \sum t \epsilon_t \end{bmatrix} = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum \epsilon_t \\ \frac{1}{\sqrt{T}} \sum \frac{t}{T} \epsilon_t \end{bmatrix} $$ [16.1.21]
Sob pressuposi√ß√µes padr√µes sobre $\epsilon_t$, este vetor √© assintoticamente gaussiano. O primeiro elemento converge para $N(0, \sigma^2)$ pelo teorema do limite central [^5]. O segundo elemento satisfaz as condi√ß√µes da Proposi√ß√£o 7.8, sendo uma sequ√™ncia de diferen√ßa martingale [^5].

A distribui√ß√£o conjunta dos dois elementos no vetor em [16.1.21] √© assintoticamente gaussiana [^6]. A forma geral para qualquer combina√ß√£o linear destes elementos √©:
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^T [\lambda_1 + \lambda_2 \frac{t}{T}] \epsilon_t $$
Para $\lambda = (\lambda_1, \lambda_2)'$, a matriz $Q$ em [16.1.20] e a vari√¢ncia √©:
$$ \frac{1}{T} \sum_{t=1}^T [\lambda_1 + \lambda_2 \frac{t}{T}]^2 \rightarrow \sigma^2 \lambda' Q \lambda $$
Portanto, qualquer combina√ß√£o linear dos dois elementos no vetor em [16.1.21] √© assintoticamente gaussiana, o que implica uma distribui√ß√£o bivariada gaussiana limite [^6].

A distribui√ß√£o assint√≥tica de [16.1.18] pode ser calculada como no Exemplo 7.5 do Cap√≠tulo 7 [^7]:

$$ \begin{bmatrix} \sqrt{T} (\hat{\alpha}_T - \alpha) \\ T^{3/2} (\hat{\delta}_T - \delta) \end{bmatrix} \rightarrow N(0, \sigma^2 Q^{-1}) $$ [16.1.25]
Em resumo, se $y_t$ √© gerado por uma tend√™ncia temporal determin√≠stica simples [16.1.1] e $\epsilon_t$ √© *i.i.d.* com $E(\epsilon_t^2) = \sigma^2$ e $E(\epsilon_t^4) < \infty$, ent√£o [^7]:
$$ \begin{bmatrix} \sqrt{T} (\hat{\alpha}_T - \alpha) \\ T^{3/2} (\hat{\delta}_T - \delta) \end{bmatrix} \rightarrow N \left( 0, \sigma^2 \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} \right) $$ [16.1.26]
Note que o estimador do coeficiente da tend√™ncia temporal ($\hat{\delta}_T$) √© super consistente, isto √©, $T(\hat{\delta}_T - \delta) \rightarrow 0$ [^7].

> üí° **Exemplo Num√©rico:**
> Vamos simular um processo com tend√™ncia temporal e observar as taxas de converg√™ncia dos estimadores.
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> T = 1000
> alpha = 5
> delta = 0.2
> sigma = 2
>
> # Simula√ß√£o dos dados
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, sigma, T)
> y = alpha + delta * t + epsilon
>
> # Regress√£o OLS
> X = np.column_stack((np.ones(T), t))
> model = sm.OLS(y, X)
> results = model.fit()
>
> # Estimativas
> alpha_hat = results.params[0]
> delta_hat = results.params[1]
>
> # Erros
> alpha_error = alpha_hat - alpha
> delta_error = delta_hat - delta
>
> # Visualiza√ß√£o dos dados
> plt.figure(figsize=(10, 5))
> plt.plot(t,y, label='S√©rie Temporal com Tend√™ncia')
> plt.plot(t, alpha+delta*t, label='Tend√™ncia Determin√≠stica')
> plt.xlabel('Tempo (t)')
> plt.ylabel('y')
> plt.title('S√©rie Temporal com Tend√™ncia')
> plt.legend()
> plt.show()
>
> # Resultados
> print(f"Verdadeiro alpha: {alpha}")
> print(f"Estimado alpha: {alpha_hat}")
> print(f"Erro de alpha: {alpha_error}")
> print(f"Verdadeiro delta: {delta}")
> print(f"Estimado delta: {delta_hat}")
> print(f"Erro de delta: {delta_error}")
>
> # An√°lise das taxas de converg√™ncia
> print("\nAn√°lise da taxa de converg√™ncia:")
> print(f"sqrt(T) * erro de alpha: {np.sqrt(T) * alpha_error}")
> print(f"T^(3/2) * erro de delta: {T**(3/2) * delta_error}")
>
> # Simula√ß√£o para v√°rios valores de T
> num_sims = 100
> T_values = [100, 500, 1000, 2000, 5000]
> convergence_results = {}
>
> for T in T_values:
>     alpha_errors = []
>     delta_errors = []
>     for _ in range(num_sims):
>         t = np.arange(1, T + 1)
>         epsilon = np.random.normal(0, sigma, T)
>         y = alpha + delta * t + epsilon
>         X = np.column_stack((np.ones(T), t))
>         model = sm.OLS(y, X)
>         results = model.fit()
>         alpha_errors.append(results.params[0] - alpha)
>         delta_errors.append(results.params[1] - delta)
>     convergence_results[T] = {
>         'alpha_error_scaled': [np.sqrt(T) * err for err in alpha_errors],
>         'delta_error_scaled': [T**(3/2) * err for err in delta_errors]
>     }
>
> # Compara√ß√£o das taxas de converg√™ncia para diferentes T's
> print("\nCompara√ß√£o das taxas de converg√™ncia:")
> for T in T_values:
>    print(f"\nT = {T}:")
>    print(f"  M√©dia de sqrt(T) * erro de alpha: {np.mean(convergence_results[T]['alpha_error_scaled']):.4f}")
>    print(f"  Desvio padr√£o de sqrt(T) * erro de alpha: {np.std(convergence_results[T]['alpha_error_scaled']):.4f}")
>    print(f"  M√©dia de T^(3/2) * erro de delta: {np.mean(convergence_results[T]['delta_error_scaled']):.4f}")
>    print(f"  Desvio padr√£o de T^(3/2) * erro de delta: {np.std(convergence_results[T]['delta_error_scaled']):.4f}")
> ```
> Este exemplo ilustra que, ao aumentarmos o tamanho da amostra ($T$), o erro de $\hat{\delta}$ multiplicado por $T^{3/2}$ permanece relativamente constante, enquanto o erro de $\hat{\alpha}$ multiplicado por $\sqrt{T}$ tamb√©m estabiliza. Isso confirma as diferentes taxas de converg√™ncia assint√≥tica. O c√≥digo gera tamb√©m um gr√°fico que ilustra a s√©rie temporal e sua tend√™ncia, tornando mais f√°cil a interpreta√ß√£o dos resultados.
>
> Os resultados num√©ricos mostram as estimativas e os erros dos par√¢metros, e a parte final do c√≥digo demonstra que os erros de $\hat{\alpha}$ e $\hat{\delta}$ quando multiplicados por $\sqrt{T}$ e $T^{3/2}$, respectivamente, convergem para uma distribui√ß√£o normal, embora com taxas diferentes.
>

**Lema 1.1.** A super consist√™ncia do estimador $\hat{\delta}_T$ implica que para qualquer $\epsilon > 0$, existe $T_0$ tal que para todo $T > T_0$, $P(|\hat{\delta}_T - \delta| > \epsilon) < \epsilon$.

*Prova:*
I. A super consist√™ncia de $\hat{\delta}_T$ significa que $T(\hat{\delta}_T - \delta) \overset{p}{\to} 0$.
II. Isso √© equivalente a dizer que para todo $\varepsilon > 0$, $\lim_{T \to \infty} P(|T(\hat{\delta}_T - \delta)| > \varepsilon) = 0$.
III.  Isso implica que para qualquer $\epsilon > 0$ e qualquer $\varepsilon' > 0$, existe um $T_0$ tal que para todo $T > T_0$, $P(|T(\hat{\delta}_T - \delta)| > \varepsilon') < \epsilon$.
IV. Escolhendo $\varepsilon' = T\epsilon$, temos que $P(|T(\hat{\delta}_T - \delta)| > T\epsilon) = P(|\hat{\delta}_T - \delta| > \epsilon) < \epsilon$ para todo $T > T_0$, comprovando a afirma√ß√£o. $\blacksquare$

**Lema 1.2.** A consist√™ncia de $\hat{\alpha}_T$ implica que para qualquer $\epsilon > 0$, $\lim_{T \to \infty} P(|\hat{\alpha}_T - \alpha| > \epsilon) = 0$.

*Prova:*
I. A consist√™ncia de $\hat{\alpha}_T$ significa que $\hat{\alpha}_T \overset{p}{\to} \alpha$.
II. Isso √© equivalente a dizer que para todo $\epsilon > 0$, $\lim_{T \to \infty} P(|\hat{\alpha}_T - \alpha| > \epsilon) = 0$.
III. Portanto, pela defini√ß√£o de converg√™ncia em probabilidade, a afirma√ß√£o √© comprovada. $\blacksquare$

#### Taxas de Converg√™ncia
Diferentes taxas de converg√™ncia podem ser descritas em termos de ordem em probabilidade. Uma sequ√™ncia de vari√°veis aleat√≥rias $\{X_T\}_{T=1}^\infty$ √© dita ser $O_p(T^{-1/2})$ se, para todo $\epsilon > 0$, existe um $M > 0$ tal que [^7]:
$$ P\{|X_T| > M/\sqrt{T}\} < \epsilon $$ [16.1.28]
Isso significa que $\sqrt{T}X_T$ tem alta probabilidade de estar dentro de $\pm M$ para qualquer $T$. Estimadores para s√©ries temporais estacion√°rias s√£o tipicamente $O_p(T^{-1/2})$. Por exemplo, a m√©dia de uma amostra de tamanho $T$, $\bar{X}_T = \frac{1}{T} \sum_{t=1}^T y_t$, onde $y_t$ √© *i.i.d.* com m√©dia zero e vari√¢ncia $\sigma^2$, tem vari√¢ncia $\sigma^2/T$ e √© $O_p(T^{-1/2})$ [^7]. De modo geral, $\{X_T\}$ √© dita ser $O_p(T^{-k})$ se, para todo $\epsilon > 0$, existe um $M>0$ tal que:
$$ P\{|X_T| > M/T^k\} < \epsilon $$ [16.1.29]

Assim, o estimador $\hat{\delta}_T$ em [16.1.26] √© $O_p(T^{-3/2})$, visto que existe uma faixa $\pm M$ em torno de $T^{3/2}(\hat{\delta}_T - \delta)$ que cont√©m a maior parte da distribui√ß√£o de probabilidade [^7].

#### Testes de Hip√≥teses
Se as inova√ß√µes $\epsilon_t$ para a tend√™ncia temporal simples [16.1.1] forem gaussianas, os estimadores OLS $\hat{\alpha}_T$ e $\hat{\delta}_T$ s√£o gaussianos e os testes *$t$* e *$F$* padr√£o t√™m distribui√ß√µes exatas de *$t$* e *$F$* para todos os tamanhos de amostra $T$ [^8]. Isso sugere que os testes usuais *$t$* e *$F$* s√£o assintoticamente v√°lidos mesmo quando as inova√ß√µes n√£o s√£o gaussianas.

O teste *$t$* OLS da hip√≥tese nula $\alpha = \alpha_0$ pode ser escrito como [^8]:
$$ t_\tau = \frac{\hat{\alpha}_T - \alpha_0}{s_T \left\{ [1 \ \ 0] (X_T' X_T)^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} \right\}^{1/2}} $$ [16.2.1]
onde $s_T^2$ √© o estimador OLS da vari√¢ncia $\sigma^2$ [^8]. Multiplicando o numerador e o denominador por $\sqrt{T}$:

$$ t_\tau = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{s_T \left\{ [ \sqrt{T} \ \ 0] (X_T' X_T)^{-1} \begin{bmatrix} \sqrt{T} \\ 0 \end{bmatrix} \right\}^{1/2}} $$ [16.2.3]
Da Equa√ß√£o [16.1.17], $[\sqrt{T} \ \ 0] = [1 \ \ 0] \Upsilon_T$. Substituindo em [16.2.3]:

$$ t_\tau = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{s_T \left\{ [1 \ \ 0] \Upsilon_T (X_T' X_T)^{-1} \Upsilon_T \begin{bmatrix} 1 \\ 0 \end{bmatrix} \right\}^{1/2}} $$ [16.2.5]
Da Equa√ß√£o [16.1.19], $\Upsilon_T(X_T' X_T)^{-1} \Upsilon_T \rightarrow Q^{-1}$, onde $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$. Portanto:
$$ t_\tau = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{\sigma \sqrt{q^{11}}} $$ [16.2.7]
que √© uma vari√°vel gaussiana assint√≥tica dividida pela raiz quadrada de sua vari√¢ncia, que resulta em uma distribui√ß√£o $N(0, 1)$ [^8].

Analogamente, o teste *$t$* OLS de $\delta = \delta_0$ √© [^8]:

$$ t_\tau = \frac{\hat{\delta}_T - \delta_0}{s_T \left\{ [0 \ \ 1] (X_T' X_T)^{-1} \begin{bmatrix} 0 \\ 1 \end{bmatrix} \right\}^{1/2}} $$
Multiplicando o numerador e o denominador por $T^{3/2}$:
$$ t_\tau = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{s_T \left\{ [0 \ \ T^{3/2}] (X_T' X_T)^{-1} \begin{bmatrix} 0 \\ T^{3/2} \end{bmatrix} \right\}^{1/2}} = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\sigma \sqrt{q^{22}}} $$

Essa estat√≠stica √© assintoticamente uma vari√°vel $N(0,1)$. Embora $\hat{\alpha}_T$ e $\hat{\delta}_T$ convirjam em taxas diferentes, seus erros padr√£o tamb√©m incorporam diferentes ordens de $T$, o que torna os testes *$t$* usuais assintoticamente v√°lidos [^8].

> üí° **Exemplo Num√©rico:**
> Vamos testar as hip√≥teses nulas para $\alpha$ e $\delta$ utilizando os dados simulados do exemplo anterior.
> ```python
> import numpy as np
> import statsmodels.api as sm
>
> # Par√¢metros
> T = 1000
> alpha = 5
> delta = 0.2
> sigma = 2
>
> # Simula√ß√£o dos dados
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, sigma, T)
> y = alpha + delta * t + epsilon
>
> # Regress√£o OLS
> X = np.column_stack((np.ones(T), t))
> model = sm.OLS(y, X)
> results = model.fit()
>
> # Teste de hip√≥tese para alpha
> alpha_0 = 0
> t_alpha = results.tvalues[0]
> p_alpha = results.pvalues[0]
>
> print(f"Teste t para H0: alpha = {alpha_0}")
> print(f"Estat√≠stica t: {t_alpha:.4f}")
> print(f"Valor p: {p_alpha:.4f}")
>
> # Teste de hip√≥tese para delta
> delta_0 = 0
> t_delta = results.tvalues[1]
> p_delta = results.pvalues[1]
>
> print(f"\nTeste t para H0: delta = {delta_0}")
> print(f"Estat√≠stica t: {t_delta:.4f}")
> print(f"Valor p: {p_delta:.4f}")
>
> # C√°lculo dos intervalos de confian√ßa para alpha e delta
> alpha_ci = results.conf_int()[0]
> delta_ci = results.conf_int()[1]
> print(f"\nIntervalo de confian√ßa 95% para alpha: {alpha_ci}")
> print(f"Intervalo de confian√ßa 95% para delta: {delta_ci}")
>
> ```
> Este c√≥digo executa testes *$t$* para as hip√≥teses nulas $\alpha = 0$ e $\delta = 0$, e calcula os respectivos valores p, al√©m dos intervalos de confian√ßa dos par√¢metros. A baixa probabilidade de rejeitar a hip√≥tese nula de que os coeficientes s√£o zero fornece evid√™ncias estat√≠sticas de que ambos s√£o significativamente diferentes de zero. Os intervalos de confian√ßa tamb√©m ajudam a entender a precis√£o das estimativas.

**Teorema 1.1.** Os testes *$t$* para $\alpha$ e $\delta$ convergem em distribui√ß√£o para uma vari√°vel $N(0,1)$ sob as condi√ß√µes estabelecidas.

*Prova:*
I. Como demonstrado na an√°lise das equa√ß√µes 16.2.7 e posteriores, as estat√≠sticas *$t$* para os estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$ convergem para vari√°veis aleat√≥rias com distribui√ß√£o $N(0,1)$ sob as premissas dadas para o erro $\epsilon_t$.
II. Portanto, os testes *$t$* s√£o assintoticamente v√°lidos, como afirmado.  $\blacksquare$

A t√©cnica de Sims, Stock e Watson (1990), transforma o modelo de regress√£o original em uma forma can√¥nica, onde a distribui√ß√£o assint√≥tica √© mais simples de descrever [^1].

**Proposi√ß√£o 1.1**  Se as premissas para os testes t forem satisfeitas, ent√£o os testes t para $\alpha$ e $\delta$ s√£o assintoticamente v√°lidos.

*Prova:*
I. Como demonstrado, as estat√≠sticas de teste t para $\alpha$ e $\delta$ convergem em distribui√ß√£o para vari√°veis $N(0,1)$, sob as premissas estabelecidas (inova√ß√µes i.i.d. com vari√¢ncia finita).
II. Assim, a distribui√ß√£o assint√≥tica das estat√≠sticas de teste √© a distribui√ß√£o normal padr√£o.
III. Isso garante a validade assint√≥tica dos testes. $\blacksquare$

#### Teste de Hip√≥teses Envolvendo M√∫ltiplos Par√¢metros

Considere o teste da hip√≥tese $H_0: r_1 \alpha + r_2 \delta = r$. A estat√≠stica *$t$* para esta hip√≥tese √© [^9]:
$$ t_T = \frac{(r_1 \hat{\alpha}_T + r_2 \hat{\delta}_T - r)}{\sqrt{s_T^2 [r_1 \ \ r_2] (X_T' X_T)^{-1} \begin{bmatrix} r_1 \\ r_2 \end{bmatrix} }} $$

Multiplicando o numerador e o denominador por $\sqrt{T}$:
$$ t_T = \frac{\sqrt{T}(r_1 \hat{\alpha}_T + r_2 \hat{\delta}_T - r)}{\sqrt{s_T^2 [r_1 \ \ r_2] (X_T' X_T)^{-1} \begin{bmatrix} r_1 \\ r_2 \end{bmatrix} }} $$

Definindo $r_T = \begin{bmatrix} r_1 \\ r_2/T \end{bmatrix}$,e $R_T = [1 \ \ 0]$, podemos reescrever a estat√≠stica de teste como:

$$ t = \frac{R_T \hat{\beta}_T - r_T}{\sqrt{s_T^2 R_T (X_T' X_T)^{-1} R_T' }} $$

Sob a hip√≥tese nula $H_0: R_T \beta = r_T$, esta estat√≠stica segue uma distribui√ß√£o t de Student com $T-k$ graus de liberdade.

### Teste de Wald

O teste de Wald √© uma abordagem alternativa para testar restri√ß√µes lineares sobre os coeficientes do modelo. Ele usa a distribui√ß√£o assint√≥tica do estimador de m√≠nimos quadrados ordin√°rios (MQO). A estat√≠stica de teste de Wald √© dada por:

$$ W = (R_T \hat{\beta}_T - r_T)' [R_T Var(\hat{\beta}_T) R_T']^{-1} (R_T \hat{\beta}_T - r_T) $$

Onde $Var(\hat{\beta}_T) = s_T^2 (X_T' X_T)^{-1}$.  Sob a hip√≥tese nula, a estat√≠stica de Wald segue uma distribui√ß√£o qui-quadrado com $q$ graus de liberdade, onde $q$ √© o n√∫mero de restri√ß√µes (o n√∫mero de linhas em $R_T$).

### Teste da Raz√£o de Verossimilhan√ßa (RV)

O teste da raz√£o de verossimilhan√ßa (RV) compara a verossimilhan√ßa do modelo estimado sem restri√ß√µes com a verossimilhan√ßa do modelo estimado com restri√ß√µes. A estat√≠stica do teste √© dada por:

$$ LR = -2 [log(L_R) - log(L_{UR})] $$

Onde $L_R$ √© a verossimilhan√ßa do modelo restrito e $L_{UR}$ √© a verossimilhan√ßa do modelo irrestrito. Sob a hip√≥tese nula, a estat√≠stica LR segue uma distribui√ß√£o qui-quadrado com $q$ graus de liberdade.

### Compara√ß√£o dos Testes

* **Teste t:** √ötil para testar hip√≥teses sobre um √∫nico coeficiente ou uma √∫nica restri√ß√£o linear. √â baseado na distribui√ß√£o t de Student, que √© mais apropriada para amostras pequenas.

* **Teste de Wald:**  Pode ser usado para testar m√∫ltiplas restri√ß√µes lineares. √â baseado na distribui√ß√£o qui-quadrado, que √© uma aproxima√ß√£o assint√≥tica.

* **Teste da Raz√£o de Verossimilhan√ßa (RV):** Uma abordagem geral para testar restri√ß√µes de modelos estat√≠sticos. √â tamb√©m baseado na distribui√ß√£o qui-quadrado e pode ser usado para testar restri√ß√µes n√£o lineares.

Em geral, o teste t √© mais apropriado quando se tem uma √∫nica restri√ß√£o. Os testes de Wald e RV s√£o mais adequados para testar m√∫ltiplas restri√ß√µes. O teste de RV √© um teste mais geral, mas pode ser computacionalmente mais intensivo que o teste de Wald.
<!-- END -->
