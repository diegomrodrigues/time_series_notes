## Processos com Tend√™ncias Temporais Determin√≠sticas: Distribui√ß√µes Assint√≥ticas e Transforma√ß√µes Can√¥nicas

### Introdu√ß√£o
Este cap√≠tulo explora a an√°lise de processos com **tend√™ncias temporais determin√≠sticas**, um tema fundamental no estudo de s√©ries temporais. Diferentemente de modelos com vari√°veis estacion√°rias, a presen√ßa de tend√™ncias temporais exige uma abordagem especial para a deriva√ß√£o das distribui√ß√µes assint√≥ticas dos estimadores. Como mencionado anteriormente, coeficientes de modelos de regress√£o envolvendo ra√≠zes unit√°rias ou tend√™ncias temporais determin√≠sticas n√£o podem ser tratados da mesma forma que coeficientes de modelos com vari√°veis estacion√°rias [^1]. As estimativas de diferentes par√¢metros podem ter diferentes taxas de converg√™ncia assint√≥tica. Este cap√≠tulo introduz a ideia de diferentes taxas de converg√™ncia e desenvolve uma abordagem geral para obter distribui√ß√µes assint√≥ticas, seguindo a metodologia sugerida por Sims, Stock e Watson (1990) [^1].

Este texto foca exclusivamente em processos com tend√™ncias temporais determin√≠sticas, excluindo ra√≠zes unit√°rias. √â demonstrado que as estat√≠sticas *$t$* e *$F$* usuais, calculadas da forma convencional, possuem as mesmas distribui√ß√µes assint√≥ticas que em regress√µes estacion√°rias [^1]. No entanto, as t√©cnicas usadas para verificar essas distribui√ß√µes limites s√£o distintas das utilizadas no Cap√≠tulo 8 [^1]. As t√©cnicas desenvolvidas neste cap√≠tulo ser√£o usadas para analisar a distribui√ß√£o assint√≥tica de processos que incluem ra√≠zes unit√°rias nos Cap√≠tulos 17 e 18 [^1].

### Conceitos Fundamentais
O cap√≠tulo come√ßa com o exemplo mais simples de inova√ß√µes *i.i.d.* em torno de uma tend√™ncia temporal determin√≠stica [^1]. A Se√ß√£o 16.1 deriva as distribui√ß√µes assint√≥ticas das estimativas dos coeficientes para esse modelo, demonstrando a necessidade de reescalonar as vari√°veis para acomodar as diferentes taxas de converg√™ncia [^1]. A Se√ß√£o 16.2 mostra que, apesar dessas diferentes taxas de converg√™ncia, as estat√≠sticas *$t$* e *$F$* padr√£o t√™m as distribui√ß√µes limites usuais para esse modelo [^1]. A Se√ß√£o 16.3 desenvolve resultados an√°logos para uma autorregress√£o estacion√°ria por covari√¢ncia em torno de uma tend√™ncia temporal determin√≠stica, apresentando a t√©cnica de Sims, Stock e Watson [^1].

#### Modelo de Tend√™ncia Temporal Simples
Consideramos a estima√ß√£o por **m√≠nimos quadrados ordin√°rios (OLS)** dos par√¢metros de uma tend√™ncia temporal simples [^1]:

$$ y_t = \alpha + \delta t + \epsilon_t $$ [16.1.1]

onde $\epsilon_t$ √© um processo de ru√≠do branco. Se $\epsilon_t \sim N(0, \sigma^2)$, o modelo [16.1.1] satisfaz as pressuposi√ß√µes cl√°ssicas de regress√£o [^1]. No entanto, a distribui√ß√£o assint√≥tica dos estimadores OLS de $\alpha$ e $\delta$ requer uma an√°lise diferente daquela utilizada para regress√µes estacion√°rias [^2].

Em regress√µes com vari√°veis explicativas estacion√°rias, a distribui√ß√£o limite √© encontrada multiplicando a Equa√ß√£o [16.1.6] por $\sqrt{T}$ [^2]:

$$ \sqrt{T}(b_T - \beta) = \left[ \frac{1}{T} \sum_{t=1}^T x_t x_t' \right]^{-1} \left[ \frac{1}{\sqrt{T}} \sum_{t=1}^T x_t \epsilon_t \right] $$ [16.1.7]

Neste caso, assume-se que $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ converge em probabilidade para uma matriz n√£o singular $Q$, e que $\frac{1}{\sqrt{T}} \sum_{t=1}^T x_t \epsilon_t$ converge em distribui√ß√£o para uma vari√°vel aleat√≥ria $N(0, \sigma^2 Q)$, resultando em $\sqrt{T}(b_T - \beta) \rightarrow N(0, \sigma^2 Q^{-1})$ [^2]. Contudo, este argumento n√£o pode ser aplicado diretamente a uma tend√™ncia temporal determin√≠stica devido √† natureza de $x_t$ e $\beta$ nas Equa√ß√µes [16.1.3] e [16.1.4] [^2]. A Equa√ß√£o [16.1.6] torna-se:

$$ \begin{bmatrix} \hat{\alpha}_T - \alpha \\ \hat{\delta}_T - \delta \end{bmatrix} = \begin{bmatrix} \sum_1 \epsilon_t & \sum t \epsilon_t \\ \sum t \epsilon_t & \sum t^2 \epsilon_t \end{bmatrix}^{-1} \begin{bmatrix} \sum \epsilon_t \\ \sum t \epsilon_t \end{bmatrix} $$ [16.1.8]

Onde $\sum$ denota a soma de $t=1$ at√© $T$. √â demonstrado por indu√ß√£o que:
$$ \sum_{t=1}^{T} t = \frac{T(T+1)}{2} $$ [16.1.9]
$$ \sum_{t=1}^{T} t^2 = \frac{T(T+1)(2T+1)}{6} $$ [16.1.10]

*Prova da Equa√ß√£o [16.1.9]:*
I. Caso base: para $T=1$, $\sum_{t=1}^1 t = 1 = \frac{1(1+1)}{2}$.
II. Hip√≥tese indutiva: assuma que $\sum_{t=1}^k t = \frac{k(k+1)}{2}$ para algum inteiro $k \geq 1$.
III. Passo indutivo: precisamos mostrar que $\sum_{t=1}^{k+1} t = \frac{(k+1)(k+2)}{2}$.
     $\sum_{t=1}^{k+1} t = \sum_{t=1}^k t + (k+1) = \frac{k(k+1)}{2} + (k+1) = \frac{k(k+1) + 2(k+1)}{2} = \frac{(k+1)(k+2)}{2}$
IV. Portanto, por indu√ß√£o matem√°tica, $\sum_{t=1}^{T} t = \frac{T(T+1)}{2}$ para todo $T \geq 1$. $\blacksquare$

*Prova da Equa√ß√£o [16.1.10]:*
I. Caso base: para $T=1$, $\sum_{t=1}^1 t^2 = 1 = \frac{1(1+1)(2(1)+1)}{6}$.
II. Hip√≥tese indutiva: assuma que $\sum_{t=1}^k t^2 = \frac{k(k+1)(2k+1)}{6}$ para algum inteiro $k \geq 1$.
III. Passo indutivo: precisamos mostrar que $\sum_{t=1}^{k+1} t^2 = \frac{(k+1)(k+2)(2(k+1)+1)}{6} = \frac{(k+1)(k+2)(2k+3)}{6}$.
    $\sum_{t=1}^{k+1} t^2 = \sum_{t=1}^k t^2 + (k+1)^2 = \frac{k(k+1)(2k+1)}{6} + (k+1)^2 = \frac{k(k+1)(2k+1) + 6(k+1)^2}{6}$
    $= \frac{(k+1)[k(2k+1) + 6(k+1)]}{6} = \frac{(k+1)[2k^2+k + 6k+6]}{6} = \frac{(k+1)(2k^2+7k+6)}{6}$
   $= \frac{(k+1)(k+2)(2k+3)}{6}$
IV. Portanto, por indu√ß√£o matem√°tica, $\sum_{t=1}^{T} t^2 = \frac{T(T+1)(2T+1)}{6}$ para todo $T \geq 1$. $\blacksquare$

O termo dominante em $\sum_{t=1}^T t$ √© $\frac{T^2}{2}$, ou seja:
$$ \frac{1}{T^2} \sum_{t=1}^T t = \frac{1}{T^2} \left[ \frac{T^2}{2} + \frac{T}{2} \right] = \frac{1}{2} + \frac{1}{2T} \rightarrow \frac{1}{2} $$ [16.1.11]
Analogamente, o termo dominante em $\sum_{t=1}^T t^2$ √© $\frac{T^3}{3}$:
$$ \frac{1}{T^3} \sum_{t=1}^T t^2 = \frac{1}{T^3} \left[ \frac{2T^3}{6} + \frac{3T^2}{6} + \frac{T}{6} \right] = \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2} \rightarrow \frac{1}{3} $$ [16.1.12]
O padr√£o geral para o termo dominante em $\sum_{t=1}^T t^v$ √© $\frac{T^{v+1}}{v+1}$:
$$ \frac{1}{T^{v+1}} \sum_{t=1}^T t^v \rightarrow \frac{1}{v+1} $$ [16.1.13]
Em particular,
$$ \frac{1}{T^{v+1}} \sum_{t=1}^T t^v = \frac{1}{T} \sum_{t=1}^T \left( \frac{t}{T} \right)^v $$ [16.1.14]
O lado direito de [16.1.14] pode ser visto como uma aproxima√ß√£o da √°rea sob a curva $f(r) = r^v$ para $r$ entre zero e um [^3].

Para $x_t$ dado em [16.1.3], os resultados [16.1.9] e [16.1.10] implicam que:

$$ \sum_{t=1}^T x_t x_t' = \begin{bmatrix} \sum 1 & \sum t \\ \sum t & \sum t^2 \end{bmatrix} = \begin{bmatrix} T & T(T+1)/2 \\ T(T+1)/2 & T(T+1)(2T+1)/6 \end{bmatrix} $$ [16.1.16]

A matriz $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ diverge, o que difere do resultado usual para regress√µes estacion√°rias. Para obter uma matriz convergente, a matriz em [16.1.16] teria que ser dividida por $T^3$ em vez de $T$ [^4]. No entanto, essa matriz limite n√£o √© invert√≠vel [^4].

Para contornar esse problema, os estimadores OLS $\hat{\alpha}_T$ e $\hat{\delta}_T$ t√™m diferentes taxas de converg√™ncia assint√≥tica. Para obter distribui√ß√µes limites n√£o degeneradas, $\hat{\alpha}_T$ √© multiplicado por $\sqrt{T}$, enquanto $\hat{\delta}_T$ √© multiplicado por $T^{3/2}$ [^4]. Essa corre√ß√£o pode ser vista como uma pr√©-multiplica√ß√£o de [16.1.6] ou [16.1.8] pela matriz [^4]:

$$ \Upsilon_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} $$ [16.1.17]
Resultando em:

$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} = \Upsilon_T \left[ \sum_{t=1}^T x_t x_t' \right]^{-1} \sum_{t=1}^T x_t \epsilon_t = \Upsilon_T \left[ \sum_{t=1}^T x_t x_t' \right]^{-1} \Upsilon_T^{-1} \Upsilon_T \sum_{t=1}^T x_t \epsilon_t = \left\{ \Upsilon_T \left[ \sum_{t=1}^T x_t x_t' \right]^{-1} \Upsilon_T^{-1} \right\} \left\{ \Upsilon_T \sum_{t=1}^T x_t \epsilon_t \right\} $$ [16.1.18]
O primeiro termo da express√£o [16.1.18], substituindo [16.1.17] e [16.1.16], √©:
$$ \Upsilon_T \left[ \sum_{t=1}^T x_t x_t' \right]^{-1} \Upsilon_T^{-1} = \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} \begin{bmatrix} \sum 1 & \sum t \\ \sum t & \sum t^2 \end{bmatrix}^{-1} \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} = \begin{bmatrix} T^{-1}\sum 1 & T^{-2} \sum t \\ T^{-2} \sum t & T^{-3} \sum t^2 \end{bmatrix}^{-1} \rightarrow Q $$ [16.1.19]

Onde $Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$ [16.1.20].

O segundo termo em [16.1.18] √©:
$$ \Upsilon_T \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} \sum \epsilon_t \\ \sum t \epsilon_t \end{bmatrix} = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum \epsilon_t \\ \frac{1}{T^{3/2}} \sum t \epsilon_t \end{bmatrix} = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum \epsilon_t \\ \frac{1}{\sqrt{T}} \sum \frac{t}{T} \epsilon_t \end{bmatrix} $$ [16.1.21]
Sob pressuposi√ß√µes padr√µes sobre $\epsilon_t$, este vetor √© assintoticamente gaussiano. O primeiro elemento converge para $N(0, \sigma^2)$ pelo teorema do limite central [^5]. O segundo elemento satisfaz as condi√ß√µes da Proposi√ß√£o 7.8, sendo uma sequ√™ncia de diferen√ßa martingale [^5].

A distribui√ß√£o conjunta dos dois elementos no vetor em [16.1.21] √© assintoticamente gaussiana [^6]. A forma geral para qualquer combina√ß√£o linear destes elementos √©:
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^T [\lambda_1 + \lambda_2 \frac{t}{T}] \epsilon_t $$
Para $\lambda = (\lambda_1, \lambda_2)'$, a matriz $Q$ em [16.1.20] e a vari√¢ncia √©:
$$ \frac{1}{T} \sum_{t=1}^T [\lambda_1 + \lambda_2 \frac{t}{T}]^2 \rightarrow \sigma^2 \lambda' Q \lambda $$
Portanto, qualquer combina√ß√£o linear dos dois elementos no vetor em [16.1.21] √© assintoticamente gaussiana, o que implica uma distribui√ß√£o bivariada gaussiana limite [^6].

A distribui√ß√£o assint√≥tica de [16.1.18] pode ser calculada como no Exemplo 7.5 do Cap√≠tulo 7 [^7]:

$$ \begin{bmatrix} \sqrt{T} (\hat{\alpha}_T - \alpha) \\ T^{3/2} (\hat{\delta}_T - \delta) \end{bmatrix} \rightarrow N(0, \sigma^2 Q^{-1}) $$ [16.1.25]
Em resumo, se $y_t$ √© gerado por uma tend√™ncia temporal determin√≠stica simples [16.1.1] e $\epsilon_t$ √© *i.i.d.* com $E(\epsilon_t^2) = \sigma^2$ e $E(\epsilon_t^4) < \infty$, ent√£o [^7]:
$$ \begin{bmatrix} \sqrt{T} (\hat{\alpha}_T - \alpha) \\ T^{3/2} (\hat{\delta}_T - \delta) \end{bmatrix} \rightarrow N \left( 0, \sigma^2 \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} \right) $$ [16.1.26]
Note que o estimador do coeficiente da tend√™ncia temporal ($\hat{\delta}_T$) √© super consistente, isto √©, $T(\hat{\delta}_T - \delta) \rightarrow 0$ [^7].

> üí° **Exemplo Num√©rico:**
> Vamos simular um processo com tend√™ncia temporal e observar as taxas de converg√™ncia dos estimadores.
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> T = 1000
> alpha = 5
> delta = 0.2
> sigma = 2
>
> # Simula√ß√£o dos dados
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, sigma, T)
> y = alpha + delta * t + epsilon
>
> # Regress√£o OLS
> X = np.column_stack((np.ones(T), t))
> model = sm.OLS(y, X)
> results = model.fit()
>
> # Estimativas
> alpha_hat = results.params[0]
> delta_hat = results.params[1]
>
> # Erros
> alpha_error = alpha_hat - alpha
> delta_error = delta_hat - delta
>
> # Visualiza√ß√£o dos dados
> plt.figure(figsize=(10, 5))
> plt.plot(t,y, label='S√©rie Temporal com Tend√™ncia')
> plt.plot(t, alpha+delta*t, label='Tend√™ncia Determin√≠stica')
> plt.xlabel('Tempo (t)')
> plt.ylabel('y')
> plt.title('S√©rie Temporal com Tend√™ncia')
> plt.legend()
> plt.show()
>
> # Resultados
> print(f"Verdadeiro alpha: {alpha}")
> print(f"Estimado alpha: {alpha_hat}")
> print(f"Erro de alpha: {alpha_error}")
> print(f"Verdadeiro delta: {delta}")
> print(f"Estimado delta: {delta_hat}")
> print(f"Erro de delta: {delta_error}")
>
> # An√°lise das taxas de converg√™ncia
> print("\nAn√°lise da taxa de converg√™ncia:")
> print(f"sqrt(T) * erro de alpha: {np.sqrt(T) * alpha_error}")
> print(f"T^(3/2) * erro de delta: {T**(3/2) * delta_error}")
>
> # Simula√ß√£o para v√°rios valores de T
> num_sims = 100
> T_values = [100, 500, 1000, 2000, 5000]
> convergence_results = {}
>
> for T in T_values:
>     alpha_errors = []
>     delta_errors = []
>     for _ in range(num_sims):
>         t = np.arange(1, T + 1)
>         epsilon = np.random.normal(0, sigma, T)
>         y = alpha + delta * t + epsilon
>         X = np.column_stack((np.ones(T), t))
>         model = sm.OLS(y, X)
>         results = model.fit()
>         alpha_errors.append(results.params[0] - alpha)
>         delta_errors.append(results.params[1] - delta)
>     convergence_results[T] = {
>         'alpha_error_scaled': [np.sqrt(T) * err for err in alpha_errors],
>         'delta_error_scaled': [T**(3/2) * err for err in delta_errors]
>     }
>
> # Compara√ß√£o das taxas de converg√™ncia para diferentes T's
> print("\nCompara√ß√£o das taxas de converg√™ncia:")
> for T in T_values:
>    print(f"\nT = {T}:")
>    print(f"  M√©dia de sqrt(T) * erro de alpha: {np.mean(convergence_results[T]['alpha_error_scaled']):.4f}")
>    print(f"  Desvio padr√£o de sqrt(T) * erro de alpha: {np.std(convergence_results[T]['alpha_error_scaled']):.4f}")
>    print(f"  M√©dia de T^(3/2) * erro de delta: {np.mean(convergence_results[T]['delta_error_scaled']):.4f}")
>    print(f"  Desvio padr√£o de T^(3/2) * erro de delta: {np.std(convergence_results[T]['delta_error_scaled']):.4f}")
> ```
> Este exemplo ilustra que, ao aumentarmos o tamanho da amostra ($T$), o erro de $\hat{\delta}$ multiplicado por $T^{3/2}$ permanece relativamente constante, enquanto o erro de $\hat{\alpha}$ multiplicado por $\sqrt{T}$ tamb√©m estabiliza. Isso confirma as diferentes taxas de converg√™ncia assint√≥tica. O c√≥digo gera tamb√©m um gr√°fico que ilustra a s√©rie temporal e sua tend√™ncia, tornando mais f√°cil a interpreta√ß√£o dos resultados.
>
> Os resultados num√©ricos mostram as estimativas e os erros dos par√¢metros, e a parte final do c√≥digo demonstra que os erros de $\hat{\alpha}$ e $\hat{\delta}$ quando multiplicados por $\sqrt{T}$ e $T^{3/2}$, respectivamente, convergem para uma distribui√ß√£o normal, embora com taxas diferentes.
>

**Lema 1.1.** A super consist√™ncia do estimador $\hat{\delta}_T$ implica que para qualquer $\epsilon > 0$, existe $T_0$ tal que para todo $T > T_0$, $P(|\hat{\delta}_T - \delta| > \epsilon) < \epsilon$.

*Prova:*
I. A super consist√™ncia de $\hat{\delta}_T$ significa que $T(\hat{\delta}_T - \delta) \overset{p}{\to} 0$.
II. Isso √© equivalente a dizer que para todo $\varepsilon > 0$, $\lim_{T \to \infty} P(|T(\hat{\delta}_T - \delta)| > \varepsilon) = 0$.
III. Isso implica que para qualquer $\epsilon > 0$ e qualquer $\varepsilon' > 0$, existe um $T_0$ tal que para todo $T > T_0$, $P(|T(\hat{\delta}_T - \delta)| > \varepsilon') < \epsilon$.
IV. Escolhendo $\varepsilon' = T\epsilon$, temos que $P(|T(\hat{\delta}_T - \delta)| > T\epsilon) = P(|\hat{\delta}_T - \delta| > \epsilon) < \epsilon$ para todo $T > T_0$, comprovando a afirma√ß√£o. $\blacksquare$

**Lema 1.2.** A consist√™ncia de $\hat{\alpha}_T$ implica que para qualquer $\epsilon > 0$, $\lim_{T \to \infty} P(|\hat{\alpha}_T - \alpha| > \epsilon) = 0$.

*Prova:*
I. A consist√™ncia de $\hat{\alpha}_T$ significa que $\hat{\alpha}_T \overset{p}{\to} \alpha$.
II. Isso √© equivalente a dizer que para todo $\epsilon > 0$, $\lim_{T \to \infty} P(|\hat{\alpha}_T - \alpha| > \epsilon) = 0$.
III. Portanto, pela defini√ß√£o de converg√™ncia em probabilidade, a afirma√ß√£o √© comprovada. $\blacksquare$

#### Taxas de Converg√™ncia
Diferentes taxas de converg√™ncia podem ser descritas em termos de ordem em probabilidade. Uma sequ√™ncia de vari√°veis aleat√≥rias $\{X_T\}_{T=1}^\infty$ √© dita ser $O_p(T^{-1/2})$ se, para todo $\epsilon > 0$, existe um $M > 0$ tal que [^7]:
$$ P\{|X_T| > M/\sqrt{T}\} < \epsilon $$ [16.1.28]
Isso significa que $\sqrt{T}X_T$ tem alta probabilidade de estar dentro de $\pm M$ para qualquer $T$. Estimadores para s√©ries temporais estacion√°rias s√£o tipicamente $O_p(T^{-1/2})$. Por exemplo, a m√©dia de uma amostra de tamanho $T$, $\bar{X}_T = \frac{1}{T} \sum_{t=1}^T y_t$, onde $y_t$ √© *i.i.d.* com m√©dia zero e vari√¢ncia $\sigma^2$, tem vari√¢ncia $\sigma^2/T$ e √© $O_p(T^{-1/2})$ [^7]. De modo geral, $\{X_T\}$ √© dita ser $O_p(T^{-k})$ se, para todo $\epsilon > 0$, existe um $M>0$ tal que:
$$ P\{|X_T| > M/T^k\} < \epsilon $$ [16.1.29]

Assim, o estimador $\hat{\delta}_T$ em [16.1.26] √© $O_p(T^{-3/2})$, visto que existe uma faixa $\pm M$ em torno de $T^{3/2}(\hat{\delta}_T - \delta)$ que cont√©m a maior parte da distribui√ß√£o de probabilidade [^7].

**Lema 1.3.** Se $X_T$ √© $O_p(T^{-a})$ e $Y_T$ √© $O_p(T^{-b})$, ent√£o $X_T Y_T$ √© $O_p(T^{-(a+b)})$.
*Prova:*
I. Se $X_T$ √© $O_p(T^{-a})$, para todo $\epsilon_1 > 0$, existe $M_1 > 0$ tal que $P(|X_T| > M_1 / T^a) < \epsilon_1$.
II. Similarmente, se $Y_T$ √© $O_p(T^{-b})$, para todo $\epsilon_2 > 0$, existe $M_2 > 0$ tal que $P(|Y_T| > M_2 / T^b) < \epsilon_2$.
III. Queremos mostrar que $X_T Y_T$ √© $O_p(T^{-(a+b)})$, ou seja, para todo $\epsilon > 0$, existe $M > 0$ tal que $P(|X_T Y_T| > M / T^{a+b}) < \epsilon$.
IV. Considere o evento $|X_T Y_T| > M / T^{a+b}$. Este evento ocorre se $|X_T| |Y_T| > M / T^{a+b}$.
V. Se $|X_T| > M_1/T^a$ e $|Y_T| > M_2/T^b$, ent√£o $|X_T| |Y_T| > M_1 M_2 / T^{a+b}$.
VI. Escolhendo $M = M_1 M_2$, temos que $P(|X_T Y_T| > M / T^{a+b}) \leq P(|X_T| > M_1 / T^a) + P(|Y_T| > M_2 / T^b) < \epsilon_1 + \epsilon_2$.
VII. Como $\epsilon_1$ e $\epsilon_2$ s√£o arbitr√°rios, podemos tornar a soma $\epsilon_1 + \epsilon_2$ arbitrariamente pequena, de modo que $X_T Y_T$ √© $O_p(T^{-(a+b)})$. $\blacksquare$

#### Testes de Hip√≥teses
Se as inova√ß√µes $\epsilon_t$ para a tend√™ncia temporal simples [16.1.1] forem gaussianas, os estimadores OLS $\hat{\alpha}_T$ e $\hat{\delta}_T$ s√£o gaussianos e os testes *$t$* e *$F$* padr√£o t√™m distribui√ß√µes exatas de *$t$* e *$F$* para todos os tamanhos de amostra $T$ [^8]. Isso sugere que os testes usuais *$t$* e *$F$* s√£o assintoticamente v√°lidos mesmo quando as inova√ß√µes n√£o s√£o gaussianas.

O teste *$t$* OLS da hip√≥tese nula $\alpha = \alpha_0$ pode ser escrito como [^8]:
$$ t_\tau = \frac{\hat{\alpha}_T - \alpha_0}{s_T \left\{ [1 \ \ 0] (X_T' X_T)^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} \right\}^{1/2}} $$ [16.2.1]
onde $s_T^2$ √© o estimador OLS da vari√¢ncia $\sigma^2$ [^8]. Multiplicando o numerador e o denominador por $\sqrt{T}$:

$$ t_\tau = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{s_T \left\{ [ \sqrt{T} \ \ 0] (X_T' X_T)^{-1} \begin{bmatrix} \sqrt{T} \\ 0 \end{bmatrix} \right\}^{1/2}} $$ [16.2.3]
Da Equa√ß√£o [16.1.17], $[\sqrt{T} \ \ 0] = [1 \ \ 0] \Upsilon_T$. Substituindo em [16.2.3]:

$$ t_\tau = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{s_T \left\{ [1 \ \ 0] \Upsilon_T (X_T' X_T)^{-1} \Upsilon_T \begin{bmatrix} 1 \\ 0 \end{bmatrix} \right\}^{1/2}} $$ [16.2.5]
Da Equa√ß√£o [16.1.19], $\Upsilon_T(X_T' X_T)^{-1} \Upsilon_T \rightarrow Q^{-1}$, onde $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$. Portanto:
$$ t_\tau = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{\sigma \sqrt{q^{11}}} $$ [16.2.7]
que √© uma vari√°vel gaussiana assint√≥tica dividida pela raiz quadrada de sua vari√¢ncia, que resulta em uma distribui√ß√£o $N(0, 1)$ [^8].

Analogamente, o teste *$t$* OLS de $\delta = \delta_0$ √© [^8]:

$$ t_\tau = \frac{\hat{\delta}_T - \delta_0}{s_T \left\{ [0 \ \ 1] (X_T' X_T)^{-1} \begin{bmatrix} 0 \\ 1 \end{bmatrix} \right\}^{1/2}} $$
Multiplicando o numerador e o denominador por $T^{3/2}$:
$$ t_\tau = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{s_T \left\{ [0 \ \ T^{3/2}] (X_T' X_T)^{-1} \begin{bmatrix} 0 \\ T^{3/2} \end{bmatrix} \right\}^{1/2}} = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\sigma \sqrt{q^{22}}} $$

Essa estat√≠stica √© assintoticamente uma vari√°vel $N(0,1)$. Embora $\hat{\alpha}_T$ e $\hat{\delta}_T$ convirjam em taxas diferentes, seus erros padr√£o tamb√©m incorporam diferentes ordens de $T$, o que torna os testes *$t$* usuais assintoticamente v√°lidos [^8].

> üí° **Exemplo Num√©rico:**
> Vamos testar as hip√≥teses nulas para $\alpha$ e $\delta$ utilizando os dados simulados do exemplo anterior.
> ```python
> import numpy as np
> import statsmodels.api as sm
>
> # Par√¢metros
> T = 1000
> alpha = 5
> delta = 0.2
> sigma = 2
>
> # Simula√ß√£o dos dados
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, sigma, T)
> y = alpha + delta * t + epsilon
>
> # Regress√£o OLS
> X = np.column_stack((np.ones(T), t))
> model = sm.OLS(y, X)
> results = model.fit()
>
> # Teste de hip√≥tese para alpha
> alpha_0 = 0
> t_alpha = results.tvalues[0]
> p_alpha = results.pvalues[0]
>
> print(f"Teste t para H0: alpha = {alpha_0}")
> print(f"Estat√≠stica t: {t_alpha:.4f}")
> print(f"Valor p: {p_alpha:.4f}")
>
> # Teste de hip√≥tese para delta
> delta_0 = 0
> t_delta = results.tvalues[1]
> p_delta = results.pvalues[1]
>
> print(f"\nTeste t para H0: delta = {delta_0}")
> print(f"Estat√≠stica t: {t_delta:.4f}")
> print(f"Valor p: {p_delta:.4f}")
>
> # C√°lculo dos intervalos de confian√ßa para alpha e delta
> alpha_ci = results.conf_int()[0]
> delta_ci = results.conf_int()[1]
> print(f"\nIntervalo de confian√ßa 95% para alpha: {alpha_ci}")
> print(f"Intervalo de confian√ßa 95% para delta: {delta_ci}")
>
> ```
> Este c√≥digo executa testes *$t$* para as hip√≥teses nulas $\alpha = 0$ e $\delta = 0$, e calcula os respectivos valores p, al√©m dos intervalos de confian√ßa dos par√¢metros. A baixa probabilidade de rejeitar a hip√≥tese nula de que os coeficientes s√£o zero fornece evid√™ncias estat√≠sticas de que ambos s√£o significativamente diferentes de zero. Os intervalos de confian√ßa tamb√©m ajudam a entender a precis√£o das estimativas.

**Teorema 1.1.** Os testes *$t$* para $\alpha$ e $\delta$ convergem em distribui√ß√£o para uma vari√°vel $N(0,1)$ sob as condi√ß√µes estabelecidas.

*Prova:*A demonstra√ß√£o deste teorema envolve mostrar que as estat√≠sticas de teste, quando normalizadas, se aproximam de uma distribui√ß√£o normal padr√£o √† medida que o tamanho da amostra aumenta. Este resultado √© fundamental para realizar infer√™ncias estat√≠sticas sobre os par√¢metros do modelo.

Agora, vamos mergulhar em como podemos usar os resultados obtidos para realizar infer√™ncias sobre as mudan√ßas m√©dias que estamos modelando.

**Infer√™ncia sobre a mudan√ßa m√©dia**

Uma das principais aplica√ß√µes da modelagem de dados longitudinais √© inferir sobre as mudan√ßas m√©dias ao longo do tempo. Usando o modelo de intercepto e inclina√ß√£o aleat√≥rios, podemos fazer infer√™ncias sobre a mudan√ßa m√©dia em cada unidade ao longo do tempo e tamb√©m sobre a mudan√ßa m√©dia na popula√ß√£o.

O modelo que temos √©:

$$
y_{ij} = \alpha_i + \delta_i t_{ij} + \epsilon_{ij}
$$

Onde:
- $y_{ij}$ √© a medida da resposta para a unidade *i* no tempo *j*.
- $\alpha_i$ √© o intercepto aleat√≥rio para a unidade *i*.
- $\delta_i$ √© a inclina√ß√£o aleat√≥ria para a unidade *i*.
- $t_{ij}$ √© o tempo em que a medida √© feita.
- $\epsilon_{ij}$ √© o erro aleat√≥rio.

Podemos estimar a mudan√ßa m√©dia no tempo da seguinte forma. A mudan√ßa m√©dia na unidade *i* √© dada por $\delta_i$. Se assumirmos que as inclina√ß√µes aleat√≥rias s√£o distribu√≠das normalmente com m√©dia $\bar{\delta}$, ou seja, $\delta_i \sim N(\bar{\delta}, \sigma_\delta^2)$, ent√£o $\bar{\delta}$ √© a mudan√ßa m√©dia na popula√ß√£o. Podemos ent√£o inferir sobre $\bar{\delta}$ usando o teste t apresentado no Teorema 1.1.

√â importante notar que a validade das infer√™ncias depende da validade das suposi√ß√µes do modelo, especialmente a normalidade dos erros e dos efeitos aleat√≥rios.

**Exemplo de aplica√ß√£o**

Imagine que estamos estudando o crescimento de plantas ao longo do tempo. Medimos a altura de v√°rias plantas em diferentes momentos. Usando o modelo de intercepto e inclina√ß√£o aleat√≥rios, podemos estimar a taxa de crescimento individual de cada planta, bem como a taxa de crescimento m√©dio na popula√ß√£o. Podemos ent√£o usar o teste t para testar a hip√≥tese de que a taxa de crescimento m√©dio √© diferente de zero.

**Limita√ß√µes e considera√ß√µes**

√â importante notar que a an√°lise de dados longitudinais usando modelos de efeitos mistos pode ser afetada por diversos fatores. Por exemplo:

1.  **Dados faltantes:** Se houver dados faltantes, √© importante entender por que esses dados est√£o faltando (Missing Completely at Random (MCAR), Missing at Random (MAR), or Missing Not at Random (MNAR)) e usar as t√©cnicas apropriadas para lidar com esses dados.
2.  **N√£o-linearidade:** Se a mudan√ßa ao longo do tempo n√£o for linear, um modelo mais complexo pode ser necess√°rio.
3.  **Heterocedasticidade:** Se a vari√¢ncia dos erros n√£o for constante, pode ser necess√°rio usar transforma√ß√µes ou modelos mais robustos.

A escolha do modelo e a interpreta√ß√£o dos resultados devem ser feitas com cautela e com um bom entendimento do contexto dos dados.

Em seguida, vamos analisar algumas alternativas para o modelo de intercepto e inclina√ß√£o aleat√≥rios.

<!-- END -->
