## Deriva√ß√£o da Distribui√ß√£o Assint√≥tica dos Estimadores de MQO para Modelos com Tend√™ncias Determin√≠sticas

### Introdu√ß√£o
Este cap√≠tulo explora a deriva√ß√£o da distribui√ß√£o assint√≥tica dos estimadores de M√≠nimos Quadrados Ordin√°rios (MQO) em modelos de s√©ries temporais que incluem tend√™ncias determin√≠sticas. Em contraste com modelos que envolvem vari√°veis estacion√°rias, os modelos com tend√™ncias determin√≠sticas apresentam taxas de converg√™ncia diferentes para os estimadores de par√¢metros. Este cap√≠tulo, baseado nas ideias de Sims, Stock e Watson (1990) [^1], foca na an√°lise dessas taxas e na aplica√ß√£o de t√©cnicas para derivar as distribui√ß√µes assint√≥ticas.

### Conceitos Fundamentais
A an√°lise inicia-se com o modelo de tend√™ncia temporal simples [16.1.1], dado por $y_t = \alpha + \delta t + \epsilon_t$, onde $\epsilon_t$ representa um processo de ru√≠do branco. Este modelo satisfaz as pressuposi√ß√µes cl√°ssicas de regress√£o quando $\epsilon_t \sim N(0, \sigma^2)$ [^1]. A distribui√ß√£o assint√≥tica dos estimadores de MQO $\hat{\alpha}$ e $\hat{\delta}$ difere daquelas encontradas em regress√µes com vari√°veis estacion√°rias devido √†s diferentes taxas de converg√™ncia. Para acomodar essas diferen√ßas, √© necess√°rio rescalar as vari√°veis.

> üí° **Exemplo Num√©rico:**
> Vamos considerar um conjunto de dados simulados onde $y_t = 2 + 0.5t + \epsilon_t$, com $\epsilon_t \sim N(0, 1)$ e $T=100$.
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
>
> # Gerando dados simulados
> np.random.seed(42)
> T = 100
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, 1, T)
> y = 2 + 0.5 * t + epsilon
>
> # Criando a matriz X
> X = np.column_stack((np.ones(T), t))
>
> # Estimando os par√¢metros por MQO
> model = sm.OLS(y, X)
> results = model.fit()
> alpha_hat, delta_hat = results.params
>
> # Imprimindo os resultados
> print(f"Estimativa de alpha (intercepto): {alpha_hat:.4f}")
> print(f"Estimativa de delta (coeficiente da tend√™ncia): {delta_hat:.4f}")
>
> # Gr√°fico
> plt.figure(figsize=(10, 6))
> plt.plot(t, y, 'o', label='Dados Observados')
> plt.plot(t, results.fittedvalues, 'r-', label='Linha de Regress√£o')
> plt.xlabel('Tempo (t)')
> plt.ylabel('y_t')
> plt.title('Regress√£o com Tend√™ncia Temporal')
> plt.legend()
> plt.show()
> ```
> Neste exemplo, $\hat{\alpha}$ e $\hat{\delta}$ s√£o os estimadores de MQO calculados a partir dos dados simulados. Observe que o valor estimado para $\alpha$ e $\delta$ s√£o pr√≥ximos aos verdadeiros. A diferen√ßa entre os par√¢metros estimados e os verdadeiros ir√° convergir para zero quando $T \rightarrow \infty$, por√©m, as taxas de converg√™ncia s√£o diferentes para $\alpha$ e $\delta$, o que √© abordado nas pr√≥ximas se√ß√µes.

#### Deriva√ß√£o da Distribui√ß√£o Assint√≥tica

A deriva√ß√£o da distribui√ß√£o assint√≥tica come√ßa com a express√£o do desvio dos estimadores de MQO de seus valores verdadeiros, como definido na equa√ß√£o [8.2.3] [^2]:
$$(b_T - \beta) = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \left( \sum_{t=1}^T x_t \epsilon_t \right)$$
onde $b_T$ √© o estimador de MQO, $\beta$ √© o vetor de par√¢metros verdadeiros, e $x_t = [1, t]'$. Para encontrar a distribui√ß√£o limite, multiplicamos ambos os lados por $\sqrt{T}$ [^2]:
$$\sqrt{T}(b_T - \beta) = \left[ \frac{1}{T} \sum_{t=1}^T x_t x_t' \right]^{-1} \left( \frac{1}{\sqrt{T}} \sum_{t=1}^T x_t \epsilon_t \right)$$
No entanto, para um modelo com tend√™ncia determin√≠stica, a matriz $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ n√£o converge para uma matriz n√£o singular. Portanto, a an√°lise √© modificada. Para o modelo com tend√™ncia temporal, a express√£o [16.1.6] torna-se [^1]:
$$\begin{bmatrix} \hat{\alpha}_T - \alpha \\ \hat{\delta}_T - \delta \end{bmatrix} = \begin{bmatrix} \sum_1 \quad \sum t \\ \sum t \quad \sum t^2 \end{bmatrix}^{-1} \begin{bmatrix} \sum \epsilon_t \\ \sum t \epsilon_t \end{bmatrix}$$
Para obter uma matriz convergente, a matriz de momentos precisa ser dividida por $T^3$, e n√£o por $T$. Para lidar com taxas de converg√™ncia diferentes para os estimadores $\hat{\alpha}$ e $\hat{\delta}$, os estimadores s√£o premultiplicados pela matriz $\Upsilon_T$ [^1]:
$$\Upsilon_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}$$
Aplicando essa transforma√ß√£o √† express√£o do desvio, obtemos [^1]:
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} = \Upsilon_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \left( \sum_{t=1}^T x_t \epsilon_t \right) $$
$$ = \left[ \Upsilon_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \Upsilon_T \right] \left[ \Upsilon_T^{-1} \sum_{t=1}^T x_t \epsilon_t \right] $$
O primeiro termo no lado direito, que envolve a matriz de momentos, converge para uma matriz $Q$ n√£o singular, dada por [^1]:
$$Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$$

> üí° **Exemplo Num√©rico:**
> Para ilustrar a converg√™ncia da matriz de momentos para $Q$, vamos calcular a matriz $\frac{1}{T^3} \sum_{t=1}^T x_t x_t'$ para diferentes valores de $T$. Aqui, $x_t = [1, t]'$.
>
> Vamos definir a matriz $X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ \vdots & \vdots \\ 1 & T \end{bmatrix}$. Ent√£o, $\sum_{t=1}^T x_t x_t' = X^TX$
> ```python
> import numpy as np
>
> def calculate_scaled_moment_matrix(T):
>    t = np.arange(1, T+1)
>    X = np.column_stack((np.ones(T), t))
>    moment_matrix = X.T @ X
>    scaled_matrix = moment_matrix / T**3
>    return scaled_matrix
>
> # Calculando para diferentes valores de T
> T_values = [100, 1000, 10000]
> for T in T_values:
>    scaled_moment_matrix = calculate_scaled_moment_matrix(T)
>    print(f"Matriz de momentos escalada para T = {T}:\n {scaled_moment_matrix}")
>    print("---")
>
> Q_target = np.array([[1, 0.5], [0.5, 1/3]])
> print(f"Matriz Q:\n {Q_target}")
> ```
> Observe como a matriz de momentos escalada se aproxima da matriz $Q$ √† medida que $T$ aumenta. Isso ilustra a converg√™ncia da matriz de momentos para uma matriz n√£o singular ap√≥s o devido escalonamento.

O segundo termo, que envolve os erros, pode ser reescrito como [^1]:
$$ \Upsilon_T^{-1} \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t \end{bmatrix} $$
Sob as suposi√ß√µes padr√£o sobre $\epsilon_t$, esse vetor converge para uma distribui√ß√£o normal bivariada [^1]:
$$ \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q) $$

**Proposi√ß√£o 1** A converg√™ncia da segunda componente do vetor de erros pode ser reescrita utilizando uma soma ponderada de $\epsilon_t$:
$$ \frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t = \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t $$
*Proof:*
I. Esta reescrita √© direta, uma vez que $\frac{t}{T}$ √© um fator que pondera o erro $\epsilon_t$.
II. Reorganizando $\frac{1}{T^{3/2}}$ como $\frac{1}{\sqrt{T}}\frac{1}{T}$, temos:
$$ \frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t = \frac{1}{\sqrt{T}}\frac{1}{T}\sum_{t=1}^T t \epsilon_t $$
III. Trazendo o fator $\frac{1}{T}$ para dentro da soma, obtemos:
$$ \frac{1}{\sqrt{T}}\frac{1}{T}\sum_{t=1}^T t \epsilon_t  = \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t $$
IV. Portanto, a express√£o original pode ser reescrita como uma soma ponderada de $\epsilon_t$.

$$ \frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t = \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t $$
‚ñ†

**Teorema 1** A distribui√ß√£o assint√≥tica da express√£o $ \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t \end{bmatrix} $ pode ser escrita como:
$$ \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q) $$
*Proof:*
I.  Definimos o vetor $z_T = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix}$.
II.  A primeira componente de $z_T$ √© uma soma de vari√°veis aleat√≥rias i.i.d. com m√©dia zero e vari√¢ncia $\sigma^2$. Pelo Teorema do Limite Central (TLC) univariado, $\frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \xrightarrow{d} N(0, \sigma^2)$.
III. A segunda componente de $z_T$ tamb√©m √© uma soma de vari√°veis aleat√≥rias com m√©dia zero.
IV.  Podemos expressar o vetor de vari√°veis aleat√≥rias como uma soma ponderada de vari√°veis i.i.d., o que nos permite aplicar o Teorema do Limite Central Multivariado (TLCM).
V.  Dado que $\epsilon_t$ s√£o vari√°veis aleat√≥rias i.i.d. com m√©dia zero e vari√¢ncia $\sigma^2$, e $\frac{t}{T}$ √© um fator que pondera $\epsilon_t$, o TLCM estabelece que o vetor $z_T$ converge para uma distribui√ß√£o normal multivariada com m√©dia zero.
VI. A matriz de covari√¢ncia do limite distribucional √© dada por $\sigma^2 Q$, onde $Q$ √© definida como $Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$. Essa matriz representa as covari√¢ncias limite das vari√°veis normalizadas.

Portanto, temos:
$$ \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q) $$
‚ñ†

Aplicando o Teorema do Limite Central, a distribui√ß√£o assint√≥tica dos estimadores pode ser expressa como:
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q^{-1}) $$

> üí° **Exemplo Num√©rico:**
>  Vamos simular erros $\epsilon_t \sim N(0, 1)$ para $T=1000$ e calcular $\frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t$ e $\frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t$. Depois vamos repetir esta simula√ß√£o 1000 vezes e verificar a distribui√ß√£o emp√≠rica dos valores resultantes.
>
> ```python
> import numpy as np
> import pandas as pd
> import matplotlib.pyplot as plt
>
> # Define T e o n√∫mero de simula√ß√µes
> T = 1000
> num_simulations = 1000
>
> # Cria uma lista para armazenar os resultados
> results = []
>
> for _ in range(num_simulations):
>    # Simula os erros
>    epsilon = np.random.normal(0, 1, T)
>
>    # Calcula a primeira componente do vetor
>    sum_epsilon_sqrtT = np.sum(epsilon) / np.sqrt(T)
>
>    # Calcula a segunda componente do vetor
>    t = np.arange(1, T + 1)
>    sum_t_epsilon_T32 = np.sum(t * epsilon) / T**(3/2)
>
>    # Adiciona os resultados √† lista
>    results.append([sum_epsilon_sqrtT, sum_t_epsilon_T32])
>
> results = np.array(results)
>
> # Cria um DataFrame para os resultados
> df_results = pd.DataFrame(results, columns=['sum_epsilon_sqrtT', 'sum_t_epsilon_T32'])
>
> # Visualiza√ß√£o dos histogramas
> plt.figure(figsize=(12, 6))
> plt.subplot(1, 2, 1)
> plt.hist(df_results['sum_epsilon_sqrtT'], bins=30, density=True, alpha=0.7, label='Emp√≠rico')
> plt.title(r'$\frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t$')
>
>
> import scipy.stats as stats
> # Calcula a m√©dia e o desvio padr√£o
> mean = df_results['sum_epsilon_sqrtT'].mean()
> std_dev = df_results['sum_epsilon_sqrtT'].std()
>
> # Cria um range para os valores do eixo x
> x = np.linspace(mean - 3*std_dev, mean + 3*std_dev, 100)
>
> # Plota a curva da distribui√ß√£o normal
> plt.plot(x, stats.norm.pdf(x, mean, std_dev), label='Normal', color = 'r')
>
> plt.legend()
>
>
> plt.subplot(1, 2, 2)
> plt.hist(df_results['sum_t_epsilon_T32'], bins=30, density=True, alpha=0.7, label = 'Emp√≠rico')
> plt.title(r'$\frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t$')
>
> # Calcula a m√©dia e o desvio padr√£o
> mean = df_results['sum_t_epsilon_T32'].mean()
> std_dev = df_results['sum_t_epsilon_T32'].std()
>
> # Cria um range para os valores do eixo x
> x = np.linspace(mean - 3*std_dev, mean + 3*std_dev, 100)
>
> # Plota a curva da distribui√ß√£o normal
> plt.plot(x, stats.norm.pdf(x, mean, std_dev), label='Normal', color = 'r')
>
> plt.legend()
> plt.tight_layout()
> plt.show()
>
> # Calcula a matriz de covari√¢ncia emp√≠rica
> cov_matrix = df_results.cov()
> print("Matriz de covari√¢ncia emp√≠rica:\n", cov_matrix)
>
>
> print(f"Q * sigma¬≤:\n", np.array([[1, 0.5], [0.5, 1/3]]))
> ```
> Este exemplo demonstra que a distribui√ß√£o emp√≠rica dos vetores aleat√≥rios simulados se aproxima de uma distribui√ß√£o normal bivariada, conforme estabelecido no teorema. A matriz de covari√¢ncia emp√≠rica se aproxima de $\sigma^2 Q$ (no nosso caso $\sigma=1$), confirmando o resultado te√≥rico.

**Lema 1.1** A matriz inversa de Q √© dada por:
$$Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$$
*Proof:*
I. Para encontrar a matriz inversa $Q^{-1}$, come√ßamos calculando o determinante da matriz $Q$.
$$det(Q) = (1 \times \frac{1}{3}) - (\frac{1}{2} \times \frac{1}{2}) = \frac{1}{3} - \frac{1}{4} = \frac{4-3}{12} = \frac{1}{12}$$
II. Em seguida, encontramos a matriz adjunta de $Q$, que √© a transposta da matriz de cofatores.
$$adj(Q) = \begin{bmatrix} 1/3 & -1/2 \\ -1/2 & 1 \end{bmatrix}$$
III. Finalmente, multiplicamos a adjunta pelo inverso do determinante para obter a matriz inversa $Q^{-1}$.
$$Q^{-1} = \frac{1}{det(Q)} adj(Q) = 12 \begin{bmatrix} 1/3 & -1/2 \\ -1/2 & 1 \end{bmatrix} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$$
‚ñ†
> üí° **Exemplo Num√©rico:**
> O c√°lculo da matriz inversa de Q pode ser facilmente feito em Python com NumPy.
> ```python
> import numpy as np
>
> Q = np.array([[1, 0.5], [0.5, 1/3]])
> Q_inv = np.linalg.inv(Q)
> print(f"A inversa da matriz Q √©:\n {Q_inv}")
> ```
> Este exemplo confirma o c√°lculo manual apresentado no lema.

#### Deriva√ß√£o Detalhada em Ap√™ndice 16.A

O Ap√™ndice 16.A [^2] detalha a deriva√ß√£o das equa√ß√µes selecionadas do cap√≠tulo. Inicialmente, a diferen√ßa entre os estimadores e seus valores populacionais √© expressa como em [16.A.1]. Em seguida, a equa√ß√£o √© premultiplicada por $Y_T$ para obter [16.A.2]:
$$Y_T(b^*_T - \beta^*) = \left( \sum_{t=1}^T x_t^* {x_t^*}' \right)^{-1} Y_T \left( \sum_{t=1}^T x_t^* \epsilon_t \right)$$
As matrizes de momentos, $ \sum_{t=1}^T x_t^* {x_t^*}'$  e de erros $Y_T\left( \sum_{t=1}^T x_t^* \epsilon_t \right) $, s√£o calculadas e expressas em [16.A.3] e [16.A.5], respectivamente.  As matrizes de momentos, ap√≥s normaliza√ß√£o, convergem para uma matriz  Q*, e o vetor de erros converge para uma distribui√ß√£o normal. A express√£o [16.A.4] resume este processo. Finalmente, a aplica√ß√£o do teorema do limite central leva √† distribui√ß√£o assint√≥tica dos estimadores em [16.A.6]:
$$Y_T(b^*_T - \beta^*) \xrightarrow{d} N(0, \sigma^2 Q^*)$$

### Conclus√£o

A deriva√ß√£o da distribui√ß√£o assint√≥tica dos estimadores de MQO em modelos com tend√™ncias determin√≠sticas requer a considera√ß√£o de diferentes taxas de converg√™ncia. A t√©cnica de rescalonamento de vari√°veis e a aplica√ß√£o do teorema do limite central permitem obter as distribui√ß√µes limites dos estimadores. A an√°lise detalhada no Ap√™ndice 16.A complementa a discuss√£o, fornecendo uma compreens√£o profunda do processo de deriva√ß√£o. Esses resultados s√£o cruciais para a infer√™ncia estat√≠stica em modelos de s√©ries temporais com tend√™ncias determin√≠sticas e oferecem a base para a constru√ß√£o de testes de hip√≥teses.

### Refer√™ncias
[^1]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113-44.
[^2]: Cap√≠tulo 16 do livro base.
<!-- END -->
