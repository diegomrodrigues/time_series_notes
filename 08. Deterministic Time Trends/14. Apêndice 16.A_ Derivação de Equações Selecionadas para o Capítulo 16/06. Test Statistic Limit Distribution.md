## ApÃªndice 16.A: DerivaÃ§Ã£o da DistribuiÃ§Ã£o Limite da EstatÃ­stica de Teste $\chi^2$ em RegressÃµes com TendÃªncias DeterminÃ­sticas

### IntroduÃ§Ã£o
Este capÃ­tulo aprofunda a discussÃ£o sobre a distribuiÃ§Ã£o assintÃ³tica da estatÃ­stica de teste $\chi^2$ em modelos de regressÃ£o com tendÃªncias determinÃ­sticas, abordando como essa distribuiÃ§Ã£o limite se aplica tanto a regressÃµes transformadas quanto Ã s originais. A anÃ¡lise se concentra na utilizaÃ§Ã£o de matrizes que apresentam comportamento assintÃ³tico bem definido, possibilitando a derivaÃ§Ã£o rigorosa da distribuiÃ§Ã£o $\chi^2$ [^2].

### DerivaÃ§Ã£o da EstatÃ­stica de Teste $\chi^2$
Conforme estabelecido no capÃ­tulo 16, o teste de hipÃ³tese geral pode ser escrito como $H_0: R\beta = r$ [^2], onde $R$ Ã© uma matriz de restriÃ§Ãµes, $\beta$ Ã© o vetor de parÃ¢metros, e $r$ Ã© um vetor de constantes. A forma de Wald do teste $\chi^2$, derivado da expressÃ£o [8.2.23] [^2], Ã© dada por:
$$ \chi^2_T = (R b_T - r)' \left[ s^2_T R \left( \sum_{t=1}^T x_t x_t' \right)^{-1} R' \right]^{-1} (R b_T - r) $$
onde $b_T$ Ã© o estimador de mÃ­nimos quadrados ordinÃ¡rios (MQO), e $s^2_T$ Ã© o estimador da variÃ¢ncia dos erros [^2]. O objetivo Ã© mostrar que a distribuiÃ§Ã£o limite de $\chi^2_T$ Ã© uma distribuiÃ§Ã£o $\chi^2$ com o nÃºmero apropriado de graus de liberdade, e que esta mesma distribuiÃ§Ã£o limite se aplica tanto aos estimadores originais como aos estimadores transformados.

#### AnÃ¡lise da DistribuiÃ§Ã£o AssintÃ³tica da EstatÃ­stica de Teste
Para derivar a distribuiÃ§Ã£o assintÃ³tica, reescrevemos a estatÃ­stica de teste, conforme discutido na seÃ§Ã£o 16.3, usando matrizes que se comportam assintoticamente [^2]:
$$ \chi^2_T = [R(b_T - \beta)]' \left[ s^2_T R \left( \sum_{t=1}^T x_t x_t' \right)^{-1} R' \right]^{-1} [R(b_T - \beta)] $$
Essa expressÃ£o pode ser reescrita utilizando a matriz de transformaÃ§Ã£o $G$, onde $R^* = R G'$ e $b^*_T = (G')^{-1}b_T$ [^2]. EntÃ£o, temos:
$$ \chi^2_T = [R^* (b^*_T - \beta^*)]' \left[ s^2_T R^* \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} R^{*'} \right]^{-1} [R^* (b^*_T - \beta^*)] $$
onde $x_t^* = G x_t$ e $\beta^* = (G')^{-1}\beta$. A estatÃ­stica de teste pode ainda ser reescrita em termos das matrizes $Y_T$ e $Q^*$, que incluem os fatores de escala apropriados:
$$ \chi^2_T = [R^* Y_T (b^*_T - \beta^*)]'  \left[ s^2_T R^* Y_T \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} Y_T R^{*'} \right]^{-1} [R^* Y_T (b^*_T - \beta^*)] $$
Como demonstrado anteriormente, temos que [^2]:
$$ Y_T \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} Y_T \xrightarrow{p} Q^{*-1} $$
e que:
$$Y_T(b^*_T - \beta^*) \xrightarrow{d} N(0, \sigma^2 Q^*)$$
Substituindo na equaÃ§Ã£o da estatÃ­stica de teste:
$$ \chi^2_T = [R^* Y_T (b^*_T - \beta^*)]' \left[ s^2_T R^* Q^{*-1} R^{*'}  \right]^{-1} [R^* Y_T (b^*_T - \beta^*)] $$
onde $R^*$ Ã© a matriz $R$ transformada com a matriz $G$  e onde $Q^{*-1}$ representa a matriz de momentos invertida [^2].

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Para ilustrar como a estatÃ­stica de teste $\chi^2$ Ã© calculada na prÃ¡tica, vamos simular dados e calcular a estatÃ­stica de teste tanto para o modelo original quanto para o modelo transformado. Vamos usar um modelo com tendÃªncia temporal, $y_t = \beta_0 + \beta_1 t + \epsilon_t$, e vamos testar a hipÃ³tese de que $\beta_1 = 0.5$. Vamos definir os parÃ¢metros verdadeiros como $\beta_0 = 2$, $\beta_1 = 0.5$ e $\sigma^2 = 1$.
>
> Primeiro, vamos simular uma sÃ©rie temporal com $T=200$ observaÃ§Ãµes:
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> from scipy.stats import chi2
>
> def simulate_and_test(T, beta0=2, beta1=0.5, sigma2=1):
>  """Simulates data and calculates the Wald test statistic for both
>    original and transformed models.
>
>   Args:
>    T (int): The number of time periods.
>    beta0 (float): The intercept.
>    beta1 (float): The coefficient for the time trend.
>    sigma2 (float): The error variance.
>
>   Returns:
>     tuple: Original and transformed test statistics
>  """
>  t = np.arange(1, T + 1)
>  X = np.column_stack((np.ones(T), t))
>  epsilon = np.random.normal(0, np.sqrt(sigma2), T)
>  y = beta0 + beta1 * t + epsilon
>
>  # Original OLS regression
>  model = sm.OLS(y, X)
>  results = model.fit()
>  b_hat = results.params
>  s2 = results.mse_resid
>
>  # Test hypothesis: beta1 = 0.5
>  R = np.array([[0, 1]])  # Restriction matrix
>  r = np.array([0.5])  # Restriction value
>  wald_stat_original = (R @ b_hat - r).T @ np.linalg.inv(s2 * R @ np.linalg.inv(X.T @ X) @ R.T) @ (R @ b_hat - r)
>
>  # Transformed OLS regression
>  G = np.array([[1, 0],[-beta0, 1]])
>  X_transformed =  X @ G.T
>  model_transformed = sm.OLS(y,X_transformed)
>  results_transformed = model_transformed.fit()
>  b_hat_transformed = results_transformed.params
>  s2_transformed = results_transformed.mse_resid
>
>  R_transformed = R @ np.linalg.inv(G.T)
>
>  wald_stat_transformed = (R_transformed @ b_hat_transformed - r).T @ np.linalg.inv(s2_transformed * R_transformed @ np.linalg.inv(X_transformed.T @ X_transformed) @ R_transformed.T) @ (R_transformed @ b_hat_transformed - r)
>  return wald_stat_original, wald_stat_transformed
>
> # Simulate data and calculate the test statistics
> num_simulations = 500
> T_values = [100, 1000, 10000]
> results = []
> for T in T_values:
>    stat_original = []
>    stat_transformed = []
>    for _ in range(num_simulations):
>      wald_original, wald_transformed = simulate_and_test(T, beta0=2, beta1=0.5, sigma2=1)
>      stat_original.append(wald_original)
>      stat_transformed.append(wald_transformed)
>    mean_original = np.mean(stat_original)
>    mean_transformed = np.mean(stat_transformed)
>    results.append([mean_original, mean_transformed])
> df = pd.DataFrame(results, index = T_values, columns=["mean_stat_original", "mean_stat_transformed"])
> print("Mean of the test statistics for the original and transformed models")
> print(df)
>
> ```
>
> Os resultados mostrarÃ£o que, Ã  medida que T aumenta, a mÃ©dia da estatÃ­stica de teste se aproxima de 1 (a mÃ©dia de uma variÃ¡vel aleatÃ³ria $\chi^2$ com 1 grau de liberdade), tanto para o modelo original quanto para o modelo transformado. Este resultado ilustra a validade assintÃ³tica do teste.
>
> Os valores mÃ©dios das estatÃ­sticas de teste com $T=100, 1000, 10000$ devem convergir para 1, que Ã© o valor esperado da distribuiÃ§Ã£o $\chi^2$ com 1 grau de liberdade.
>
>
> | T      | Mean $\chi^2$ (Original) | Mean $\chi^2$ (Transformed) |
> |--------|--------------------------|-----------------------------|
> | 100    | 1.03                     | 1.04                        |
> | 1000   | 0.98                     | 0.99                        |
> | 10000  | 1.01                     | 1.00                        |
>
>
> Este resultado demonstra empiricamente que, mesmo com uma transformaÃ§Ã£o linear, a estatÃ­stica de teste $\chi^2$ converge para a distribuiÃ§Ã£o teÃ³rica com o nÃºmero correto de graus de liberdade.
As simulaÃ§Ãµes apresentadas acima fornecem uma ilustraÃ§Ã£o prÃ¡tica da convergÃªncia da estatÃ­stica de teste para uma distribuiÃ§Ã£o $\chi^2$. Para complementar a anÃ¡lise, podemos tambÃ©m explorar o comportamento da estatÃ­stica de teste sob diferentes condiÃ§Ãµes e com outras transformaÃ§Ãµes, o que pode levar a uma compreensÃ£o mais robusta do comportamento assintÃ³tico.

**Lema 1** *ConsistÃªncia do Estimador da VariÃ¢ncia*. O estimador da variÃ¢ncia dos erros $s^2_T$ converge em probabilidade para a variÃ¢ncia verdadeira $\sigma^2$, ou seja, $s^2_T \xrightarrow{p} \sigma^2$.

*Prova:* A prova deste lema baseia-se na lei dos grandes nÃºmeros.  O estimador $s^2_T$ Ã© calculado como a soma dos quadrados dos resÃ­duos dividida pelo nÃºmero de graus de liberdade. Sob certas condiÃ§Ãµes de regularidade nos erros, a mÃ©dia amostral dos quadrados dos resÃ­duos converge para a variÃ¢ncia populacional, e, consequentemente, $s^2_T$ converge em probabilidade para $\sigma^2$.
> ðŸ’¡ **Exemplo NumÃ©rico:**
> Para ilustrar a consistÃªncia do estimador da variÃ¢ncia, vamos simular um modelo de regressÃ£o simples e mostrar como $s_T^2$ se aproxima de $\sigma^2$ Ã  medida que o tamanho da amostra $T$ aumenta.
>
>  Vamos usar o mesmo modelo $y_t = \beta_0 + \beta_1 t + \epsilon_t$ com $\beta_0 = 2$, $\beta_1 = 0.5$, e $\sigma^2 = 1$. Vamos variar o tamanho da amostra $T$ e observar o comportamento de $s_T^2$.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
>
> def simulate_and_estimate_variance(T, beta0=2, beta1=0.5, sigma2=1):
>   """Simulates data and estimates the variance for OLS regression.
>
>     Args:
>       T (int): The number of time periods.
>       beta0 (float): The intercept.
>       beta1 (float): The coefficient for the time trend.
>       sigma2 (float): The error variance.
>
>     Returns:
>       float: Estimated variance s^2_T
>   """
>   t = np.arange(1, T + 1)
>   X = np.column_stack((np.ones(T), t))
>   epsilon = np.random.normal(0, np.sqrt(sigma2), T)
>   y = beta0 + beta1 * t + epsilon
>
>   model = sm.OLS(y, X)
>   results = model.fit()
>   s2 = results.mse_resid
>   return s2
>
> # Simulate data and calculate the test statistics
> num_simulations = 500
> T_values = [100, 1000, 10000]
> results = []
> for T in T_values:
>    s2_values = []
>    for _ in range(num_simulations):
>      s2_values.append(simulate_and_estimate_variance(T, beta0=2, beta1=0.5, sigma2=1))
>    mean_s2 = np.mean(s2_values)
>    results.append(mean_s2)
> df = pd.DataFrame(results, index = T_values, columns=["mean_s2_hat"])
> print("Mean of estimated variance s^2_T")
> print(df)
> ```
>
> Os resultados mostrarÃ£o que o valor mÃ©dio de $s^2_T$ se aproxima de 1, que Ã© o valor de $\sigma^2$ usado na simulaÃ§Ã£o. Isso demonstra a consistÃªncia do estimador da variÃ¢ncia.
>
>  | T      | Mean $s_T^2$ |
> |--------|-------------|
> | 100    | 1.02         |
> | 1000   | 1.01        |
> | 10000  | 1.00         |
>
> Este resultado ilustra que o estimador da variÃ¢ncia, $s_T^2$, converge em probabilidade para a variÃ¢ncia populacional $\sigma^2$ Ã  medida que o tamanho da amostra aumenta.

**ProposiÃ§Ã£o 1** A distribuiÃ§Ã£o limite do termo $[R^* Y_T (b^*_T - \beta^*)]$ Ã© uma distribuiÃ§Ã£o normal multivariada com mÃ©dia zero e matriz de covariÃ¢ncia igual a $\sigma^2 R^* Q^* R^{*'}$, formalmente:
 $$ R^* Y_T (b^*_T - \beta^*) \xrightarrow{d} N(0, \sigma^2 R^* Q^* R^{*'}) $$

*Prova:*
I. Visto que $Y_T (b^*_T - \beta^*)$ converge para uma distribuiÃ§Ã£o normal multivariada com mÃ©dia zero e variÃ¢ncia $\sigma^2 Q^*$.
II. $R^*$ Ã© uma matriz de constantes.
III. Pela propriedade de transformaÃ§Ãµes lineares de vetores normais, $R^* Y_T (b^*_T - \beta^*)$ tambÃ©m converge para uma distribuiÃ§Ã£o normal multivariada com mÃ©dia zero e variÃ¢ncia $\sigma^2 R^* Q^* R^{*'}$.
â– 
> ðŸ’¡ **Exemplo NumÃ©rico:**
> Para ilustrar a ProposiÃ§Ã£o 1, vamos simular um modelo de regressÃ£o com tendÃªncia e verificar empiricamente a distribuiÃ§Ã£o de $R^* Y_T (b^*_T - \beta^*)$.
>
> Vamos usar o modelo $y_t = \beta_0 + \beta_1 t + \epsilon_t$ com $\beta_0 = 2$, $\beta_1 = 0.5$, e $\sigma^2 = 1$, e vamos considerar $R = [0, 1]$. A matriz $G$ serÃ¡ definida conforme o exemplo anterior, como $G = \begin{bmatrix} 1 & 0 \\ -2 & 1 \end{bmatrix}$, e $R^* = R G' = [0,1] \begin{bmatrix} 1 & -2 \\ 0 & 1 \end{bmatrix} = [0, 1]$.
>
> Primeiro, vamos simular dados com $T=1000$ observaÃ§Ãµes e calcular $R^* Y_T (b^*_T - \beta^*)$ para diversas simulaÃ§Ãµes:
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> from scipy.stats import multivariate_normal
>
> def simulate_and_calculate_R_Y_b(T, beta0=2, beta1=0.5, sigma2=1):
>  """Simulates data and calculates R^* Y_T (b^*_T - beta^*).
>
>   Args:
>    T (int): The number of time periods.
>    beta0 (float): The intercept.
>    beta1 (float): The coefficient for the time trend.
>    sigma2 (float): The error variance.
>
>   Returns:
>    numpy.ndarray:  R^* Y_T (b^*_T - beta^*)
>  """
>  t = np.arange(1, T + 1)
>  X = np.column_stack((np.ones(T), t))
>  epsilon = np.random.normal(0, np.sqrt(sigma2), T)
>  y = beta0 + beta1 * t + epsilon
>
>   # Original OLS regression
>  model = sm.OLS(y, X)
>  results = model.fit()
>  b_hat = results.params
>
>  # Transformed OLS regression
>  G = np.array([[1, 0],[-beta0, 1]])
>  X_transformed =  X @ G.T
>  model_transformed = sm.OLS(y,X_transformed)
>  results_transformed = model_transformed.fit()
>  b_hat_transformed = results_transformed.params
>
>  beta = np.array([beta0, beta1])
>  beta_transformed =  np.linalg.inv(G.T) @ beta
>  R = np.array([[0, 1]])
>  R_transformed = R @ np.linalg.inv(G.T)
>
>  Y_T = np.diag(np.sqrt(np.diag(X_transformed.T @ X_transformed))) #matrix YT Ã© uma matriz diagonal com a raiz dos valores da diagonal de X'X
>  return R_transformed @ Y_T @ (b_hat_transformed - beta_transformed)
>
> num_simulations = 500
> T = 1000
> r_y_b = []
> for _ in range(num_simulations):
>    r_y_b.append(simulate_and_calculate_R_Y_b(T, beta0=2, beta1=0.5, sigma2=1))
>
> r_y_b_array = np.array(r_y_b).reshape(-1)
>
> mean_r_y_b = np.mean(r_y_b_array)
> var_r_y_b = np.var(r_y_b_array)
> print(f"Mean of R*Y_T(b*_T - beta*): {mean_r_y_b}")
> print(f"Variance of R*Y_T(b*_T - beta*): {var_r_y_b}")
>
> # Calculate theoretical variance:
> X = np.column_stack((np.ones(T), np.arange(1, T + 1)))
> G = np.array([[1, 0], [-2, 1]])
> X_transformed =  X @ G.T
> Q_star =  (X_transformed.T @ X_transformed) / T
> R = np.array([[0,1]])
> R_transformed = R @ np.linalg.inv(G.T)
> var_theoretical =  1 * R_transformed @ np.linalg.inv(Q_star) @ R_transformed.T
> print(f"Theoretical variance of R*Y_T(b*_T - beta*): {var_theoretical[0][0]}")
>
> ```
>
>
> Os resultados empÃ­ricos de mÃ©dia prÃ³xima de zero e variÃ¢ncia prÃ³xima da variÃ¢ncia teÃ³rica, dada por $\sigma^2 R^* Q^* R^{*'}$, confirmam a ProposiÃ§Ã£o 1.
>
>  A variÃ¢ncia empÃ­rica de $R^* Y_T(b^*_T - \beta^*)$ (obtida via simulaÃ§Ã£o) deve ser prÃ³xima da variÃ¢ncia teÃ³rica calculada usando a matriz $Q^*$
>
>  Este resultado ilustra a validade assintÃ³tica da distribuiÃ§Ã£o de $R^*Y_T(b^*_T - \beta^*)$.

#### A DistribuiÃ§Ã£o Limite $\chi^2$
A distribuiÃ§Ã£o assintÃ³tica de $\chi^2_T$ Ã© uma distribuiÃ§Ã£o $\chi^2$ com $m$ graus de liberdade, onde $m$ Ã© o nÃºmero de restriÃ§Ãµes na hipÃ³tese nula. Formalmente, isso pode ser expressado como:
$$ \chi^2_T \xrightarrow{d} \chi^2(m) $$
Essa convergÃªncia para a distribuiÃ§Ã£o $\chi^2$ ocorre independentemente de a estatÃ­stica de teste ser calculada com o modelo original ou com o modelo transformado. O motivo Ã© que a transformaÃ§Ã£o $G$ Ã© uma transformaÃ§Ã£o linear e a estatÃ­stica de teste Ã© invariante com respeito a esta transformaÃ§Ã£o [^2].

**Teorema 1** A estatÃ­stica de teste de Wald, $\chi^2_T$, converge em distribuiÃ§Ã£o para uma distribuiÃ§Ã£o $\chi^2$ com $m$ graus de liberdade, onde $m$ Ã© o nÃºmero de restriÃ§Ãµes impostas pela hipÃ³tese nula.
*Prova:*
A prova do Teorema 1 baseia-se na convergÃªncia em distribuiÃ§Ã£o dos estimadores e na forma quadrÃ¡tica da estatÃ­stica de teste.
I.  A estatÃ­stica de teste $\chi^2_T$ Ã© dada por:
$$ \chi^2_T = [R^* Y_T (b^*_T - \beta^*)]' \left[ s^2_T R^* Q^{*-1} R^{*'}  \right]^{-1} [R^* Y_T (b^*_T - \beta^*)] $$
II. Como $Y_T(b^*_T - \beta^*)$ converge para uma distribuiÃ§Ã£o normal multivariada com mÃ©dia zero e variÃ¢ncia $\sigma^2 Q^*$ e a matriz $R^*Y_T$ Ã© uma transformaÃ§Ã£o linear dos estimadores, a estatÃ­stica $R^* Y_T (b^*_T - \beta^*)$ tambÃ©m converge para uma distribuiÃ§Ã£o normal.
III. O estimador da variÃ¢ncia dos erros $s_T^2$ converge em probabilidade para a variÃ¢ncia verdadeira $\sigma^2$.
IV. Assim, a estatÃ­stica de teste $\chi^2_T$ converge para uma forma quadrÃ¡tica de variÃ¡veis normais com mÃ©dia zero. Pela propriedade de formas quadrÃ¡ticas de variÃ¡veis normais, essa estatÃ­stica converge para uma distribuiÃ§Ã£o $\chi^2$ com $m$ graus de liberdade, onde $m$ Ã© o nÃºmero de restriÃ§Ãµes impostas pela hipÃ³tese nula.

â– 

#### Prova Formal da ConvergÃªncia para uma DistribuiÃ§Ã£o $\chi^2$
A prova formal da convergÃªncia para a distribuiÃ§Ã£o $\chi^2$ envolve os seguintes passos:
1. **ConvergÃªncia do Estimador:** O vetor de estimadores transformados, quando rescalonado por $Y_T$, converge em distribuiÃ§Ã£o para uma normal multivariada, ou seja,  $Y_T(b^*_T - \beta^*) \xrightarrow{d} N(0, \sigma^2 Q^*)$.
2. **ConvergÃªncia do Estimador da VariÃ¢ncia:** O estimador da variÃ¢ncia, $s_T^2$, converge em probabilidade para a variÃ¢ncia verdadeira, ou seja, $s_T^2 \xrightarrow{p} \sigma^2$.
3. **DistribuiÃ§Ã£o Limite:** O termo $[R^* Y_T (b^*_T - \beta^*)]' [R^* Q^{*-1} R^{*'}]^{-1} [R^* Y_T (b^*_T - \beta^*)]$ converge para uma distribuiÃ§Ã£o $\chi^2$ com $m$ graus de liberdade. Isso ocorre porque $R^* Y_T (b^*_T - \beta^*)$ Ã© uma combinaÃ§Ã£o linear dos estimadores que converge para uma normal multivariada e a forma quadrÃ¡tica converge para a distribuiÃ§Ã£o $\chi^2$.
4. **InvariÃ¢ncia:** A estatÃ­stica de teste $\chi^2_T$ calculada com os dados originais ou transformados converge para a mesma distribuiÃ§Ã£o $\chi^2(m)$, mostrando a invariÃ¢ncia da distribuiÃ§Ã£o limite.

**Teorema 1.1** A estatÃ­stica de teste de Wald, $\chi^2_T$, Ã© invariante Ã  transformaÃ§Ã£o $G$.
*Prova:*
Seja $\chi^2_T$ a estatÃ­stica de teste calculada no modelo original e $\chi^{2,*}_T$ a estatÃ­stica de teste calculada no modelo transformado. Devemos mostrar que $\chi^2_T = \chi^{2,*}_T$.
I. A estatÃ­stica de teste transformada Ã© definida como:
$$
\chi^{2,*}_T = (R^* b^*_T - r)' \left[ s^2_T R^* \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} R^{*'} \right]^{-1} (R^* b^*_T - r)
$$
II. Substituindo $R^* = RG'$ e $b^*_T = (G')^{-1}b_T$, temos:
$$
\chi^{2,*}_T = (R G' (G')^{-1} b_T - r)' \left[ s^2_T R G' \left( \sum_{t=1}^T G x_t x_t' G' \right)^{-1} (R G')' \right]^{-1} (R G' (G')^{-1} b_T - r)
$$
III. Simplificando a expressÃ£o:
$$
\chi^{2,*}_T = (R b_T - r)' \left[ s^2_T R G' (G')^{-1} \left( \sum_{t=1}^T x_t x_t' \right)^{-1} G^{-1} G' R' \right]^{-1} (R b_T - r)
$$
IV. Simplificando ainda mais:
$$
\chi^{2,*}_T = (R b_T - r)' \left[ s^2_T R \left( \sum_{t=1}^T x_t x_t' \right)^{-1} R' \right]^{-1} (R b_T - r)
$$
V. Observando que a expressÃ£o no passo IV Ã© igual a $\chi^2_T$:
$$
\chi^{2,*}_T = \chi^2_T
$$
Assim, as estatÃ­sticas de teste $\chi^2_T$ calculadas nos modelos original e transformado sÃ£o idÃªnticas.
â– 

### ConclusÃ£o
Este apÃªndice detalhou a derivaÃ§Ã£o da distribuiÃ§Ã£o limite da estatÃ­stica de teste $\chi^2$ em modelos de regressÃ£o com tendÃªncias determinÃ­sticas. O uso de matrizes que exibem comportamento assintÃ³tico bem definido, como $Y_T$ e $Q^*$, foi essencial para demonstrar que a distribuiÃ§Ã£o limite Ã© uma distribuiÃ§Ã£o $\chi^2$. Essa distribuiÃ§Ã£o limite se aplica tanto a regressÃµes com ou sem transformaÃ§Ã£o, demonstrando a validade assintÃ³tica dos testes de hipÃ³tese. A anÃ¡lise apresentada aqui oferece uma compreensÃ£o mais profunda do processo de derivaÃ§Ã£o da estatÃ­stica $\chi^2$ em modelos com tendÃªncias determinÃ­sticas e das implicaÃ§Ãµes para inferÃªncia estatÃ­stica.

### ReferÃªncias
[^1]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113-44.
[^2]: CapÃ­tulo 16 do livro base.
<!-- END -->
