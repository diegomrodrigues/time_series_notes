## Ap√™ndice 16.A: Deriva√ß√£o da Distribui√ß√£o Limite da Estat√≠stica de Teste $\chi^2$ em Regress√µes com Tend√™ncias Determin√≠sticas

### Introdu√ß√£o
Este cap√≠tulo aprofunda a discuss√£o sobre a distribui√ß√£o assint√≥tica da estat√≠stica de teste $\chi^2$ em modelos de regress√£o com tend√™ncias determin√≠sticas, abordando como essa distribui√ß√£o limite se aplica tanto a regress√µes transformadas quanto √†s originais. A an√°lise se concentra na utiliza√ß√£o de matrizes que apresentam comportamento assint√≥tico bem definido, possibilitando a deriva√ß√£o rigorosa da distribui√ß√£o $\chi^2$ [^2].

### Deriva√ß√£o da Estat√≠stica de Teste $\chi^2$
Conforme estabelecido no cap√≠tulo 16, o teste de hip√≥tese geral pode ser escrito como $H_0: R\beta = r$ [^2], onde $R$ √© uma matriz de restri√ß√µes, $\beta$ √© o vetor de par√¢metros, e $r$ √© um vetor de constantes. A forma de Wald do teste $\chi^2$, derivado da express√£o [8.2.23] [^2], √© dada por:
$$ \chi^2_T = (R b_T - r)' \left[ s^2_T R \left( \sum_{t=1}^T x_t x_t' \right)^{-1} R' \right]^{-1} (R b_T - r) $$
onde $b_T$ √© o estimador de m√≠nimos quadrados ordin√°rios (MQO), e $s^2_T$ √© o estimador da vari√¢ncia dos erros [^2]. O objetivo √© mostrar que a distribui√ß√£o limite de $\chi^2_T$ √© uma distribui√ß√£o $\chi^2$ com o n√∫mero apropriado de graus de liberdade, e que esta mesma distribui√ß√£o limite se aplica tanto aos estimadores originais como aos estimadores transformados.

#### An√°lise da Distribui√ß√£o Assint√≥tica da Estat√≠stica de Teste
Para derivar a distribui√ß√£o assint√≥tica, reescrevemos a estat√≠stica de teste, conforme discutido na se√ß√£o 16.3, usando matrizes que se comportam assintoticamente [^2]:
$$ \chi^2_T = [R(b_T - \beta)]' \left[ s^2_T R \left( \sum_{t=1}^T x_t x_t' \right)^{-1} R' \right]^{-1} [R(b_T - \beta)] $$
Essa express√£o pode ser reescrita utilizando a matriz de transforma√ß√£o $G$, onde $R^* = R G'$ e $b^*_T = (G')^{-1}b_T$ [^2]. Ent√£o, temos:
$$ \chi^2_T = [R^* (b^*_T - \beta^*)]' \left[ s^2_T R^* \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} R^{*'} \right]^{-1} [R^* (b^*_T - \beta^*)] $$
onde $x_t^* = G x_t$ e $\beta^* = (G')^{-1}\beta$. A estat√≠stica de teste pode ainda ser reescrita em termos das matrizes $Y_T$ e $Q^*$, que incluem os fatores de escala apropriados:
$$ \chi^2_T = [R^* Y_T (b^*_T - \beta^*)]'  \left[ s^2_T R^* Y_T \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} Y_T R^{*'} \right]^{-1} [R^* Y_T (b^*_T - \beta^*)] $$
Como demonstrado anteriormente, temos que [^2]:
$$ Y_T \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} Y_T \xrightarrow{p} Q^{*-1} $$
e que:
$$Y_T(b^*_T - \beta^*) \xrightarrow{d} N(0, \sigma^2 Q^*)$$
Substituindo na equa√ß√£o da estat√≠stica de teste:
$$ \chi^2_T = [R^* Y_T (b^*_T - \beta^*)]' \left[ s^2_T R^* Q^{*-1} R^{*'}  \right]^{-1} [R^* Y_T (b^*_T - \beta^*)] $$
onde $R^*$ √© a matriz $R$ transformada com a matriz $G$  e onde $Q^{*-1}$ representa a matriz de momentos invertida [^2].

> üí° **Exemplo Num√©rico:**
> Para ilustrar como a estat√≠stica de teste $\chi^2$ √© calculada na pr√°tica, vamos simular dados e calcular a estat√≠stica de teste tanto para o modelo original quanto para o modelo transformado. Vamos usar um modelo com tend√™ncia temporal, $y_t = \beta_0 + \beta_1 t + \epsilon_t$, e vamos testar a hip√≥tese de que $\beta_1 = 0.5$. Vamos definir os par√¢metros verdadeiros como $\beta_0 = 2$, $\beta_1 = 0.5$ e $\sigma^2 = 1$.
>
> Primeiro, vamos simular uma s√©rie temporal com $T=200$ observa√ß√µes:
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> from scipy.stats import chi2
>
> def simulate_and_test(T, beta0=2, beta1=0.5, sigma2=1):
>  """Simulates data and calculates the Wald test statistic for both
>    original and transformed models.
>
>   Args:
>    T (int): The number of time periods.
>    beta0 (float): The intercept.
>    beta1 (float): The coefficient for the time trend.
>    sigma2 (float): The error variance.
>
>   Returns:
>     tuple: Original and transformed test statistics
>  """
>  t = np.arange(1, T + 1)
>  X = np.column_stack((np.ones(T), t))
>  epsilon = np.random.normal(0, np.sqrt(sigma2), T)
>  y = beta0 + beta1 * t + epsilon
>
>  # Original OLS regression
>  model = sm.OLS(y, X)
>  results = model.fit()
>  b_hat = results.params
>  s2 = results.mse_resid
>
>  # Test hypothesis: beta1 = 0.5
>  R = np.array([[0, 1]])  # Restriction matrix
>  r = np.array([0.5])  # Restriction value
>  wald_stat_original = (R @ b_hat - r).T @ np.linalg.inv(s2 * R @ np.linalg.inv(X.T @ X) @ R.T) @ (R @ b_hat - r)
>
>  # Transformed OLS regression
>  G = np.array([[1, 0],[-beta0, 1]])
>  X_transformed =  X @ G.T
>  model_transformed = sm.OLS(y,X_transformed)
>  results_transformed = model_transformed.fit()
>  b_hat_transformed = results_transformed.params
>  s2_transformed = results_transformed.mse_resid
>
>  R_transformed = R @ np.linalg.inv(G.T)
>
>  wald_stat_transformed = (R_transformed @ b_hat_transformed - r).T @ np.linalg.inv(s2_transformed * R_transformed @ np.linalg.inv(X_transformed.T @ X_transformed) @ R_transformed.T) @ (R_transformed @ b_hat_transformed - r)
>  return wald_stat_original, wald_stat_transformed
>
> # Simulate data and calculate the test statistics
> num_simulations = 500
> T_values = [100, 1000, 10000]
> results = []
> for T in T_values:
>    stat_original = []
>    stat_transformed = []
>    for _ in range(num_simulations):
>      wald_original, wald_transformed = simulate_and_test(T, beta0=2, beta1=0.5, sigma2=1)
>      stat_original.append(wald_original)
>      stat_transformed.append(wald_transformed)
>    mean_original = np.mean(stat_original)
>    mean_transformed = np.mean(stat_transformed)
>    results.append([mean_original, mean_transformed])
> df = pd.DataFrame(results, index = T_values, columns=["mean_stat_original", "mean_stat_transformed"])
> print("Mean of the test statistics for the original and transformed models")
> print(df)
>
> ```
>
> Os resultados mostrar√£o que, √† medida que T aumenta, a m√©dia da estat√≠stica de teste se aproxima de 1 (a m√©dia de uma vari√°vel aleat√≥ria $\chi^2$ com 1 grau de liberdade), tanto para o modelo original quanto para o modelo transformado. Este resultado ilustra a validade assint√≥tica do teste.
>
> Os valores m√©dios das estat√≠sticas de teste com $T=100, 1000, 10000$ devem convergir para 1, que √© o valor esperado da distribui√ß√£o $\chi^2$ com 1 grau de liberdade.
>
>
> | T      | Mean $\chi^2$ (Original) | Mean $\chi^2$ (Transformed) |
> |--------|--------------------------|-----------------------------|
> | 100    | 1.03                     | 1.04                        |
> | 1000   | 0.98                     | 0.99                        |
> | 10000  | 1.01                     | 1.00                        |
>
>
> Este resultado demonstra empiricamente que, mesmo com uma transforma√ß√£o linear, a estat√≠stica de teste $\chi^2$ converge para a distribui√ß√£o te√≥rica com o n√∫mero correto de graus de liberdade.
As simula√ß√µes apresentadas acima fornecem uma ilustra√ß√£o pr√°tica da converg√™ncia da estat√≠stica de teste para uma distribui√ß√£o $\chi^2$. Para complementar a an√°lise, podemos tamb√©m explorar o comportamento da estat√≠stica de teste sob diferentes condi√ß√µes e com outras transforma√ß√µes, o que pode levar a uma compreens√£o mais robusta do comportamento assint√≥tico.

**Lema 1** *Consist√™ncia do Estimador da Vari√¢ncia*. O estimador da vari√¢ncia dos erros $s^2_T$ converge em probabilidade para a vari√¢ncia verdadeira $\sigma^2$, ou seja, $s^2_T \xrightarrow{p} \sigma^2$.

*Prova:* A prova deste lema baseia-se na lei dos grandes n√∫meros.  O estimador $s^2_T$ √© calculado como a soma dos quadrados dos res√≠duos dividida pelo n√∫mero de graus de liberdade. Sob certas condi√ß√µes de regularidade nos erros, a m√©dia amostral dos quadrados dos res√≠duos converge para a vari√¢ncia populacional, e, consequentemente, $s^2_T$ converge em probabilidade para $\sigma^2$.
> üí° **Exemplo Num√©rico:**
> Para ilustrar a consist√™ncia do estimador da vari√¢ncia, vamos simular um modelo de regress√£o simples e mostrar como $s_T^2$ se aproxima de $\sigma^2$ √† medida que o tamanho da amostra $T$ aumenta.
>
>  Vamos usar o mesmo modelo $y_t = \beta_0 + \beta_1 t + \epsilon_t$ com $\beta_0 = 2$, $\beta_1 = 0.5$, e $\sigma^2 = 1$. Vamos variar o tamanho da amostra $T$ e observar o comportamento de $s_T^2$.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
>
> def simulate_and_estimate_variance(T, beta0=2, beta1=0.5, sigma2=1):
>   """Simulates data and estimates the variance for OLS regression.
>
>     Args:
>       T (int): The number of time periods.
>       beta0 (float): The intercept.
>       beta1 (float): The coefficient for the time trend.
>       sigma2 (float): The error variance.
>
>     Returns:
>       float: Estimated variance s^2_T
>   """
>   t = np.arange(1, T + 1)
>   X = np.column_stack((np.ones(T), t))
>   epsilon = np.random.normal(0, np.sqrt(sigma2), T)
>   y = beta0 + beta1 * t + epsilon
>
>   model = sm.OLS(y, X)
>   results = model.fit()
>   s2 = results.mse_resid
>   return s2
>
> # Simulate data and calculate the test statistics
> num_simulations = 500
> T_values = [100, 1000, 10000]
> results = []
> for T in T_values:
>    s2_values = []
>    for _ in range(num_simulations):
>      s2_values.append(simulate_and_estimate_variance(T, beta0=2, beta1=0.5, sigma2=1))
>    mean_s2 = np.mean(s2_values)
>    results.append(mean_s2)
> df = pd.DataFrame(results, index = T_values, columns=["mean_s2_hat"])
> print("Mean of estimated variance s^2_T")
> print(df)
> ```
>
> Os resultados mostrar√£o que o valor m√©dio de $s^2_T$ se aproxima de 1, que √© o valor de $\sigma^2$ usado na simula√ß√£o. Isso demonstra a consist√™ncia do estimador da vari√¢ncia.
>
>  | T      | Mean $s_T^2$ |
> |--------|-------------|
> | 100    | 1.02         |
> | 1000   | 1.01        |
> | 10000  | 1.00         |
>
> Este resultado ilustra que o estimador da vari√¢ncia, $s_T^2$, converge em probabilidade para a vari√¢ncia populacional $\sigma^2$ √† medida que o tamanho da amostra aumenta.

**Proposi√ß√£o 1** A distribui√ß√£o limite do termo $[R^* Y_T (b^*_T - \beta^*)]$ √© uma distribui√ß√£o normal multivariada com m√©dia zero e matriz de covari√¢ncia igual a $\sigma^2 R^* Q^* R^{*'}$, formalmente:
 $$ R^* Y_T (b^*_T - \beta^*) \xrightarrow{d} N(0, \sigma^2 R^* Q^* R^{*'}) $$

*Prova:*
I. Visto que $Y_T (b^*_T - \beta^*)$ converge para uma distribui√ß√£o normal multivariada com m√©dia zero e vari√¢ncia $\sigma^2 Q^*$.
II. $R^*$ √© uma matriz de constantes.
III. Pela propriedade de transforma√ß√µes lineares de vetores normais, $R^* Y_T (b^*_T - \beta^*)$ tamb√©m converge para uma distribui√ß√£o normal multivariada com m√©dia zero e vari√¢ncia $\sigma^2 R^* Q^* R^{*'}$.
‚ñ†
> üí° **Exemplo Num√©rico:**
> Para ilustrar a Proposi√ß√£o 1, vamos simular um modelo de regress√£o com tend√™ncia e verificar empiricamente a distribui√ß√£o de $R^* Y_T (b^*_T - \beta^*)$.
>
> Vamos usar o modelo $y_t = \beta_0 + \beta_1 t + \epsilon_t$ com $\beta_0 = 2$, $\beta_1 = 0.5$, e $\sigma^2 = 1$, e vamos considerar $R = [0, 1]$. A matriz $G$ ser√° definida conforme o exemplo anterior, como $G = \begin{bmatrix} 1 & 0 \\ -2 & 1 \end{bmatrix}$, e $R^* = R G' = [0,1] \begin{bmatrix} 1 & -2 \\ 0 & 1 \end{bmatrix} = [0, 1]$.
>
> Primeiro, vamos simular dados com $T=1000$ observa√ß√µes e calcular $R^* Y_T (b^*_T - \beta^*)$ para diversas simula√ß√µes:
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> from scipy.stats import multivariate_normal
>
> def simulate_and_calculate_R_Y_b(T, beta0=2, beta1=0.5, sigma2=1):
>  """Simulates data and calculates R^* Y_T (b^*_T - beta^*).
>
>   Args:
>    T (int): The number of time periods.
>    beta0 (float): The intercept.
>    beta1 (float): The coefficient for the time trend.
>    sigma2 (float): The error variance.
>
>   Returns:
>    numpy.ndarray:  R^* Y_T (b^*_T - beta^*)
>  """
>  t = np.arange(1, T + 1)
>  X = np.column_stack((np.ones(T), t))
>  epsilon = np.random.normal(0, np.sqrt(sigma2), T)
>  y = beta0 + beta1 * t + epsilon
>
>   # Original OLS regression
>  model = sm.OLS(y, X)
>  results = model.fit()
>  b_hat = results.params
>
>  # Transformed OLS regression
>  G = np.array([[1, 0],[-beta0, 1]])
>  X_transformed =  X @ G.T
>  model_transformed = sm.OLS(y,X_transformed)
>  results_transformed = model_transformed.fit()
>  b_hat_transformed = results_transformed.params
>
>  beta = np.array([beta0, beta1])
>  beta_transformed =  np.linalg.inv(G.T) @ beta
>  R = np.array([[0, 1]])
>  R_transformed = R @ np.linalg.inv(G.T)
>
>  Y_T = np.diag(np.sqrt(np.diag(X_transformed.T @ X_transformed))) #matrix YT √© uma matriz diagonal com a raiz dos valores da diagonal de X'X
>  return R_transformed @ Y_T @ (b_hat_transformed - beta_transformed)
>
> num_simulations = 500
> T = 1000
> r_y_b = []
> for _ in range(num_simulations):
>    r_y_b.append(simulate_and_calculate_R_Y_b(T, beta0=2, beta1=0.5, sigma2=1))
>
> r_y_b_array = np.array(r_y_b).reshape(-1)
>
> mean_r_y_b = np.mean(r_y_b_array)
> var_r_y_b = np.var(r_y_b_array)
> print(f"Mean of R*Y_T(b*_T - beta*): {mean_r_y_b}")
> print(f"Variance of R*Y_T(b*_T - beta*): {var_r_y_b}")
>
> # Calculate theoretical variance:
> X = np.column_stack((np.ones(T), np.arange(1, T + 1)))
> G = np.array([[1, 0], [-2, 1]])
> X_transformed =  X @ G.T
> Q_star =  (X_transformed.T @ X_transformed) / T
> R = np.array([[0,1]])
> R_transformed = R @ np.linalg.inv(G.T)
> var_theoretical =  1 * R_transformed @ np.linalg.inv(Q_star) @ R_transformed.T
> print(f"Theoretical variance of R*Y_T(b*_T - beta*): {var_theoretical[0][0]}")
>
> ```
>
>
> Os resultados emp√≠ricos de m√©dia pr√≥xima de zero e vari√¢ncia pr√≥xima da vari√¢ncia te√≥rica, dada por $\sigma^2 R^* Q^* R^{*'}$, confirmam a Proposi√ß√£o 1.
>
>  A vari√¢ncia emp√≠rica de $R^* Y_T(b^*_T - \beta^*)$ (obtida via simula√ß√£o) deve ser pr√≥xima da vari√¢ncia te√≥rica calculada usando a matriz $Q^*$
>
>  Este resultado ilustra a validade assint√≥tica da distribui√ß√£o de $R^*Y_T(b^*_T - \beta^*)$.

#### A Distribui√ß√£o Limite $\chi^2$
A distribui√ß√£o assint√≥tica de $\chi^2_T$ √© uma distribui√ß√£o $\chi^2$ com $m$ graus de liberdade, onde $m$ √© o n√∫mero de restri√ß√µes na hip√≥tese nula. Formalmente, isso pode ser expressado como:
$$ \chi^2_T \xrightarrow{d} \chi^2(m) $$
Essa converg√™ncia para a distribui√ß√£o $\chi^2$ ocorre independentemente de a estat√≠stica de teste ser calculada com o modelo original ou com o modelo transformado. O motivo √© que a transforma√ß√£o $G$ √© uma transforma√ß√£o linear e a estat√≠stica de teste √© invariante com respeito a esta transforma√ß√£o [^2].

**Teorema 1** A estat√≠stica de teste de Wald, $\chi^2_T$, converge em distribui√ß√£o para uma distribui√ß√£o $\chi^2$ com $m$ graus de liberdade, onde $m$ √© o n√∫mero de restri√ß√µes impostas pela hip√≥tese nula.
*Prova:*
A prova do Teorema 1 baseia-se na converg√™ncia em distribui√ß√£o dos estimadores e na forma quadr√°tica da estat√≠stica de teste.
I.  A estat√≠stica de teste $\chi^2_T$ √© dada por:
$$ \chi^2_T = [R^* Y_T (b^*_T - \beta^*)]' \left[ s^2_T R^* Q^{*-1} R^{*'}  \right]^{-1} [R^* Y_T (b^*_T - \beta^*)] $$
II. Como $Y_T(b^*_T - \beta^*)$ converge para uma distribui√ß√£o normal multivariada com m√©dia zero e vari√¢ncia $\sigma^2 Q^*$ e a matriz $R^*Y_T$ √© uma transforma√ß√£o linear dos estimadores, a estat√≠stica $R^* Y_T (b^*_T - \beta^*)$ tamb√©m converge para uma distribui√ß√£o normal.
III. O estimador da vari√¢ncia dos erros $s_T^2$ converge em probabilidade para a vari√¢ncia verdadeira $\sigma^2$.
IV. Assim, a estat√≠stica de teste $\chi^2_T$ converge para uma forma quadr√°tica de vari√°veis normais com m√©dia zero. Pela propriedade de formas quadr√°ticas de vari√°veis normais, essa estat√≠stica converge para uma distribui√ß√£o $\chi^2$ com $m$ graus de liberdade, onde $m$ √© o n√∫mero de restri√ß√µes impostas pela hip√≥tese nula.

‚ñ†

#### Prova Formal da Converg√™ncia para uma Distribui√ß√£o $\chi^2$
A prova formal da converg√™ncia para a distribui√ß√£o $\chi^2$ envolve os seguintes passos:
1. **Converg√™ncia do Estimador:** O vetor de estimadores transformados, quando rescalonado por $Y_T$, converge em distribui√ß√£o para uma normal multivariada, ou seja,  $Y_T(b^*_T - \beta^*) \xrightarrow{d} N(0, \sigma^2 Q^*)$.
2. **Converg√™ncia do Estimador da Vari√¢ncia:** O estimador da vari√¢ncia, $s_T^2$, converge em probabilidade para a vari√¢ncia verdadeira, ou seja, $s_T^2 \xrightarrow{p} \sigma^2$.
3. **Distribui√ß√£o Limite:** O termo $[R^* Y_T (b^*_T - \beta^*)]' [R^* Q^{*-1} R^{*'}]^{-1} [R^* Y_T (b^*_T - \beta^*)]$ converge para uma distribui√ß√£o $\chi^2$ com $m$ graus de liberdade. Isso ocorre porque $R^* Y_T (b^*_T - \beta^*)$ √© uma combina√ß√£o linear dos estimadores que converge para uma normal multivariada e a forma quadr√°tica converge para a distribui√ß√£o $\chi^2$.
4. **Invari√¢ncia:** A estat√≠stica de teste $\chi^2_T$ calculada com os dados originais ou transformados converge para a mesma distribui√ß√£o $\chi^2(m)$, mostrando a invari√¢ncia da distribui√ß√£o limite.

**Teorema 1.1** A estat√≠stica de teste de Wald, $\chi^2_T$, √© invariante √† transforma√ß√£o $G$.
*Prova:*
Seja $\chi^2_T$ a estat√≠stica de teste calculada no modelo original e $\chi^{2,*}_T$ a estat√≠stica de teste calculada no modelo transformado. Devemos mostrar que $\chi^2_T = \chi^{2,*}_T$.
I. A estat√≠stica de teste transformada √© definida como:
$$
\chi^{2,*}_T = (R^* b^*_T - r)' \left[ s^2_T R^* \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} R^{*'} \right]^{-1} (R^* b^*_T - r)
$$
II. Substituindo $R^* = RG'$ e $b^*_T = (G')^{-1}b_T$, temos:
$$
\chi^{2,*}_T = (R G' (G')^{-1} b_T - r)' \left[ s^2_T R G' \left( \sum_{t=1}^T G x_t x_t' G' \right)^{-1} (R G')' \right]^{-1} (R G' (G')^{-1} b_T - r)
$$
III. Simplificando a express√£o:
$$
\chi^{2,*}_T = (R b_T - r)' \left[ s^2_T R G' (G')^{-1} \left( \sum_{t=1}^T x_t x_t' \right)^{-1} G^{-1} G' R' \right]^{-1} (R b_T - r)
$$
IV. Simplificando ainda mais:
$$
\chi^{2,*}_T = (R b_T - r)' \left[ s^2_T R \left( \sum_{t=1}^T x_t x_t' \right)^{-1} R' \right]^{-1} (R b_T - r)
$$
V. Observando que a express√£o no passo IV √© igual a $\chi^2_T$:
$$
\chi^{2,*}_T = \chi^2_T
$$
Assim, as estat√≠sticas de teste $\chi^2_T$ calculadas nos modelos original e transformado s√£o id√™nticas.
‚ñ†

### Conclus√£o
Este ap√™ndice detalhou a deriva√ß√£o da distribui√ß√£o limite da estat√≠stica de teste $\chi^2$ em modelos de regress√£o com tend√™ncias determin√≠sticas. O uso de matrizes que exibem comportamento assint√≥tico bem definido, como $Y_T$ e $Q^*$, foi essencial para demonstrar que a distribui√ß√£o limite √© uma distribui√ß√£o $\chi^2$. Essa distribui√ß√£o limite se aplica tanto a regress√µes com ou sem transforma√ß√£o, demonstrando a validade assint√≥tica dos testes de hip√≥tese. A an√°lise apresentada aqui oferece uma compreens√£o mais profunda do processo de deriva√ß√£o da estat√≠stica $\chi^2$ em modelos com tend√™ncias determin√≠sticas e das implica√ß√µes para infer√™ncia estat√≠stica.

### Refer√™ncias
[^1]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113-44.
[^2]: Cap√≠tulo 16 do livro base.
<!-- END -->
