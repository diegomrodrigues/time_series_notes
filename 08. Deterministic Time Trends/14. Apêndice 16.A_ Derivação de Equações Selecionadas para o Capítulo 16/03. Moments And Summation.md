## Ap√™ndice 16.A: Detalhes da Deriva√ß√£o da Distribui√ß√£o Assint√≥tica para Modelos com Tend√™ncias Determin√≠sticas

### Introdu√ß√£o

Este cap√≠tulo expande a discuss√£o da distribui√ß√£o assint√≥tica dos estimadores de M√≠nimos Quadrados Ordin√°rios (MQO) em modelos com tend√™ncias determin√≠sticas. O foco √© o detalhamento da deriva√ß√£o das equa√ß√µes selecionadas do Cap√≠tulo 16, como mencionado anteriormente [^2], enfatizando a manipula√ß√£o dos momentos e somat√≥rios, para finalmente chegar √† distribui√ß√£o limite da matriz de momentos e da matriz de covari√¢ncia dos estimadores.

### Deriva√ß√£o Detalhada dos Momentos e Somat√≥rios

Come√ßando com a express√£o [16.A.1] para a diferen√ßa entre os estimadores e seus valores populacionais, temos:
$$b^*_T - \beta^* = \left( \sum_{t=1}^T x_t^* {x_t^*}' \right)^{-1} \left( \sum_{t=1}^T x_t^* \epsilon_t \right)$$
Esta equa√ß√£o representa a diferen√ßa entre o estimador MQO ($\hat{\beta}^*$) e seu valor verdadeiro ($\beta^*$) [^2]. Para obter a distribui√ß√£o assint√≥tica, premultiplicamos ambos os lados por $Y_T$ [^2], conforme em [16.A.2]:
$$Y_T(b^*_T - \beta^*) = Y_T \left( \sum_{t=1}^T x_t^* {x_t^*}' \right)^{-1} \left( \sum_{t=1}^T x_t^* \epsilon_t \right)$$
Essa transforma√ß√£o √© crucial para acomodar as diferentes taxas de converg√™ncia dos estimadores. Os pr√≥ximos passos envolvem a manipula√ß√£o e simplifica√ß√£o das matrizes de momentos e de erros. A matriz de momentos, $\sum_{t=1}^T x_t^* {x_t^*}'$, √© detalhada em [16.A.3] [^2]:
$$
\sum_{t=1}^T x_t^* {x_t^*}' =
\begin{bmatrix}
\sum (y_{t-1}^*)^2 & \sum y_{t-1}^* y_{t-2}^* & \cdots & \sum y_{t-1}^* y_{t-p}^* & \sum y_{t-1}^* & \sum t y_{t-1}^* \\
\sum y_{t-2}^* y_{t-1}^* & \sum (y_{t-2}^*)^2 & \cdots & \sum y_{t-2}^* y_{t-p}^* & \sum y_{t-2}^* & \sum t y_{t-2}^* \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
\sum y_{t-p}^* y_{t-1}^* & \sum y_{t-p}^* y_{t-2}^* & \cdots & \sum (y_{t-p}^*)^2 & \sum y_{t-p}^* & \sum t y_{t-p}^* \\
\sum y_{t-1}^* & \sum y_{t-2}^* & \cdots & \sum y_{t-p}^* & \sum 1 & \sum t \\
\sum t y_{t-1}^* & \sum t y_{t-2}^* & \cdots & \sum t y_{t-p}^* & \sum t & \sum t^2
\end{bmatrix}
$$
onde $y_t^*$ representa a componente estacion√°ria da regress√£o. Para entender o comportamento assint√≥tico dessa matriz, precisamos analisar os termos que a comp√µem. Notavelmente, os termos que envolvem os $y_t^*$ s√£o estacion√°rios (por constru√ß√£o) [^2]. Em outras palavras, os momentos $\frac{1}{T} \sum (y_{t-i}^*y_{t-j}^*)$ convergem para valores finitos, enquanto os termos envolvendo $t$ e $t^2$ exibem comportamento de tend√™ncia.
A matriz $Y_T \left( \sum_{t=1}^T x_t^* \epsilon_t \right)$ √© detalhada em [16.A.5] [^2]:
$$
Y_T \left( \sum_{t=1}^T x_t^* \epsilon_t \right) =
\begin{bmatrix}
\frac{1}{\sqrt{T}} \sum \epsilon_t \\
\frac{1}{\sqrt{T}} \sum y_{t-1}^* \epsilon_t \\
\vdots \\
\frac{1}{\sqrt{T}} \sum y_{t-p}^* \epsilon_t \\
\frac{1}{\sqrt{T}} \sum \epsilon_t \\
\frac{1}{T^{3/2}} \sum t \epsilon_t
\end{bmatrix}
$$

### An√°lise da Converg√™ncia da Matriz de Momentos

O objetivo √© determinar o comportamento assint√≥tico da matriz de momentos. Isso envolve identificar quais termos s√£o dominantes √† medida que $T$ aumenta. Dividimos a matriz $\sum_{t=1}^T x_t^* {x_t^*}'$ por $T$ elevado a uma pot√™ncia apropriada para garantir converg√™ncia.
A matriz $\frac{1}{T} \sum_{t=1}^T x_t^* {x_t^*}'$ n√£o converge para uma matriz n√£o singular por causa da presen√ßa de componentes de tend√™ncia [^2]. Para obter uma matriz convergente, aplicamos diferentes fatores de escala para diferentes partes da matriz, ou seja, multiplicamos $\sum_{t=1}^T x_t^* {x_t^*}'$ por $\Upsilon_T$ em ambos os lados.
Para os primeiros $p$ blocos da matriz, que envolvem termos $y_{t-i}^*y_{t-j}^*$, o fator de escala apropriado √© $\frac{1}{T}$. Como os $y_{t-i}^*$ s√£o, por constru√ß√£o, componentes estacion√°rios,  o resultado √© que
$$\frac{1}{T} \sum_{t=1}^T y_{t-i}^* y_{t-j}^* \xrightarrow{p} E(y_{t-i}^* y_{t-j}^*)$$
Para os termos da matriz que envolvem um termo $y_{t-i}^*$ e um $1$, $\sum_{t=1}^T y_{t-i}^*$, o fator de escala apropriado √© $\frac{1}{\sqrt{T}}$. O resultado √© que
$$\frac{1}{\sqrt{T}} \sum_{t=1}^T y_{t-i}^*  \xrightarrow{p} 0$$
porque $y_{t-i}^*$ tem m√©dia zero.
Para os termos da matriz que envolvem um termo $y_{t-i}^*$ e um $t$, $\sum_{t=1}^T t y_{t-i}^*$,  o fator de escala apropriado √© $\frac{1}{T^{3/2}}$. O resultado √© que
$$\frac{1}{T^{3/2}} \sum_{t=1}^T t y_{t-i}^* \xrightarrow{p} 0$$
Para o termo $\sum_{t=1}^T 1$, o fator de escala √© $\frac{1}{T}$. O resultado √© que
$$ \frac{1}{T} \sum_{t=1}^T 1 = 1$$
Para o termo $\sum_{t=1}^T t$, o fator de escala √© $\frac{1}{T^2}$. O resultado √© que
$$ \frac{1}{T^2} \sum_{t=1}^T t  \rightarrow \frac{1}{2}$$
Para o termo $\sum_{t=1}^T t^2$, o fator de escala √© $\frac{1}{T^3}$. O resultado √© que
$$ \frac{1}{T^3} \sum_{t=1}^T t^2  \rightarrow \frac{1}{3}$$

Assim, a matriz de momentos, quando corretamente escalada, converge para uma matriz finita $Q^*$ [^2]:
$$ \frac{1}{T} Y_T \left( \sum_{t=1}^T x_t^* {x_t^*}' \right) Y_T \xrightarrow{p} Q^*$$
onde $Y_T$ √© a matriz que cont√©m os fatores de escala apropriados para cada linha e coluna.

> üí° **Exemplo Num√©rico:**
> Para ilustrar a converg√™ncia da matriz de momentos, vamos considerar um modelo simples com uma tend√™ncia temporal e sem componentes auto-regressivos ($p=0$). Isso significa que $x_t^* = [1, t]^T$. Vamos calcular a matriz de momentos escalada para diferentes valores de $T$ e observar a converg√™ncia. Aqui, a matriz $\sum_{t=1}^T x_t^* x_t^{*'}$ ser√° uma matriz $2 \times 2$, e o fator de escala $Y_T$ ser√° uma matriz diagonal com elementos $\frac{1}{\sqrt{T}}$ e $\frac{1}{T^{3/2}}$, correspondentes aos elementos de $x_t^*$.
>
> ```python
> import numpy as np
> import pandas as pd
>
> def calculate_scaled_moment_matrix(T):
>  """Calculates the scaled moment matrix for a model with a constant and time trend.
>
>   Args:
>    T (int): The number of time periods.
>
>   Returns:
>    numpy.ndarray: The scaled moment matrix.
>  """
>  t = np.arange(1, T + 1)
>  X = np.column_stack((np.ones(T), t))  # Design matrix with constant and time trend
>  moment_matrix = X.T @ X
>  # Scale the moment matrix
>  scaled_matrix = np.zeros_like(moment_matrix, dtype=float)
>  scaled_matrix[0, 0] = moment_matrix[0, 0] / T # Scale (1,1) by 1/T
>  scaled_matrix[0, 1] = scaled_matrix[1, 0] = moment_matrix[0, 1] / (T**2)  # Scale (1,2) and (2,1) by 1/T^2
>  scaled_matrix[1, 1] = moment_matrix[1, 1] / (T**3)  # Scale (2,2) by 1/T^3
>  return scaled_matrix
>
> # Calculate and print the scaled moment matrices for different T values
> T_values = [100, 1000, 10000]
> results = []
> for T in T_values:
>  scaled_moment_matrix = calculate_scaled_moment_matrix(T)
>  results.append(scaled_moment_matrix.flatten())
> df = pd.DataFrame(results, index=T_values, columns=["(1,1)","(1,2)","(2,1)","(2,2)"])
> print("Scaled Moment Matrices for Different T:")
> print(df)
> ```
>
> Os resultados mostrar√£o que, √† medida que T aumenta, o elemento (1,1) da matriz escalada se aproxima de 1, o elemento (2,2) se aproxima de 1/3, e os elementos (1,2) e (2,1) se aproximam de 1/2, demonstrando a converg√™ncia da matriz de momentos escalada. Isso ilustra como a escolha dos fatores de escala garante que a matriz resultante convirja para uma matriz finita, como discutido teoricamente.

**Lema 1** A converg√™ncia dos termos $\frac{1}{T^2} \sum_{t=1}^T t$ e $\frac{1}{T^3} \sum_{t=1}^T t^2$ para $\frac{1}{2}$ e $\frac{1}{3}$, respectivamente, decorre diretamente das somas de s√©ries aritm√©ticas. Especificamente, $\sum_{t=1}^T t = \frac{T(T+1)}{2}$ e $\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6}$.

*Prova.*  
A prova para o lema 1 segue diretamente da aplica√ß√£o das f√≥rmulas para somas de s√©ries aritm√©ticas.  
I. Para o primeiro caso, temos que:
$$\frac{1}{T^2}\sum_{t=1}^T t = \frac{1}{T^2}\frac{T(T+1)}{2} = \frac{T^2+T}{2T^2} = \frac{1}{2} + \frac{1}{2T}$$
II. Quando $T \to \infty$, temos que $\frac{1}{2} + \frac{1}{2T} \to \frac{1}{2}$.
III. Para o segundo caso, temos que:
$$ \frac{1}{T^3}\sum_{t=1}^T t^2 = \frac{1}{T^3} \frac{T(T+1)(2T+1)}{6} = \frac{2T^3 + 3T^2 + T}{6T^3} = \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2}$$
IV. Quando $T \to \infty$, temos que  $\frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2} \to \frac{1}{3}$.
‚ñ†

### An√°lise da Converg√™ncia da Matriz de Erros

O pr√≥ximo passo √© analisar o comportamento assint√≥tico da matriz de erros. Os termos envolvendo $\epsilon_t$ dentro da matriz $Y_T\left( \sum_{t=1}^T x_t^* \epsilon_t \right)$ s√£o de duas formas: $\frac{1}{\sqrt{T}}\sum_{t=1}^T y_{t-i}^* \epsilon_t$ e $\frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t$.
Como j√° discutido no cap√≠tulo 16, o termo $\frac{1}{\sqrt{T}}\sum_{t=1}^T \epsilon_t$ √© uma soma de vari√°veis i.i.d. com m√©dia zero, e pelo teorema do limite central converge para uma normal com m√©dia zero e vari√¢ncia $\sigma^2$ [^1]. De maneira similar, $\frac{1}{\sqrt{T}}\sum_{t=1}^T y_{t-i}^* \epsilon_t$ e $\frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t$ tamb√©m convergem para distribui√ß√µes normais com m√©dia zero.

**Proposi√ß√£o 1** A converg√™ncia em distribui√ß√£o de $\frac{1}{\sqrt{T}}\sum_{t=1}^T y_{t-i}^* \epsilon_t$ e $\frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t$ para distribui√ß√µes normais com m√©dia zero segue do Teorema do Limite Central para martingales e da condi√ß√£o de estacionaridade dos $y_{t-i}^*$.

*Prova.*
A prova para a Proposi√ß√£o 1 envolve a aplica√ß√£o do Teorema do Limite Central para martingales.
I. Para mostrar que $\frac{1}{\sqrt{T}}\sum_{t=1}^T y_{t-i}^* \epsilon_t$ converge para uma normal com m√©dia zero, podemos aplicar uma vers√£o do Teorema do Limite Central para martingales. Dado que $y_{t-i}^*$ √© estacion√°rio e $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, a sequ√™ncia $y_{t-i}^* \epsilon_t$ forma uma sequ√™ncia de diferen√ßas de martingale, cuja soma, quando escalada por $\frac{1}{\sqrt{T}}$, converge para uma distribui√ß√£o normal com m√©dia zero. A vari√¢ncia limite seria $E[(y_{t-i}^*)^2]\sigma^2$.
II. Para o termo $\frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t$, podemos usar um argumento similar, por√©m, o termo $t$ n√£o √© estacion√°rio. Para isso podemos reescrever $\frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t =  \frac{1}{\sqrt{T}}\sum_{t=1}^T \frac{t}{T} \epsilon_t$. Considerando a converg√™ncia $\frac{t}{T} \rightarrow z$ onde $z \in [0,1]$, essa soma, pode ser aproximada por uma integral de um processo de Wiener (um processo estoc√°stico com trajet√≥rias cont√≠nuas). Assim $\frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t$ converge para uma vari√°vel aleat√≥ria normal com m√©dia zero e vari√¢ncia proporcional a $\sigma^2$.
III. Portanto, $\frac{1}{\sqrt{T}}\sum_{t=1}^T y_{t-i}^* \epsilon_t$ e $\frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t$ convergem para distribui√ß√µes normais com m√©dia zero.
‚ñ†

Em termos gerais, o vetor $Y_T \left( \sum_{t=1}^T x_t^* \epsilon_t \right)$ converge em distribui√ß√£o para uma normal com m√©dia zero e matriz de covari√¢ncia proporcional a  $Q^*$ [^2].

### Distribui√ß√£o Assint√≥tica dos Estimadores

Com a an√°lise das matrizes de momentos e de erros, podemos agora obter a distribui√ß√£o assint√≥tica dos estimadores, de acordo com o resultado em [16.A.6] [^2]:
$$Y_T(b^*_T - \beta^*) \xrightarrow{d} N(0, \sigma^2 Q^*)$$
Onde $Q^*$ √© a matriz limite obtida no processo anterior, e $\sigma^2$ √© a vari√¢ncia dos erros.
Esse resultado estabelece que o vetor dos estimadores, quando escalonado adequadamente, converge para uma distribui√ß√£o normal multivariada com m√©dia zero e matriz de covari√¢ncia proporcional a $Q^{*-1}$.

**Teorema 1.1** A matriz de covari√¢ncia assint√≥tica dos estimadores pode ser expressa como $\sigma^2 Q^{*-1}$, onde $Q^*$ √© a matriz limite dos momentos escalados.

*Prova.*
A prova do Teorema 1.1 baseia-se na converg√™ncia em distribui√ß√£o de $Y_T(b^*_T - \beta^*)$ e nas propriedades do operador de covari√¢ncia.
I. Pela converg√™ncia em distribui√ß√£o, sabemos que $Y_T(b^*_T - \beta^*)$ converge para $N(0, \sigma^2 Q^*)$.
II. Aplicando o operador de covari√¢ncia, temos que:
$$Cov[Y_T(b^*_T - \beta^*)] = \sigma^2 Q^*$$
III. Como $Y_T(b^*_T - \beta^*)$ converge para uma normal com m√©dia zero e vari√¢ncia $\sigma^2 Q^*$, isso implica que $Y_T(b^*_T - \beta^*)$ se aproxima de uma vari√°vel com distribui√ß√£o $N(0,\sigma^2 Q^*)$, e a vari√¢ncia de $b^*_T$ converge para $\sigma^2 (Y_T^{-1} Q^* Y_T^{-1'})$. Contudo, por constru√ß√£o, temos que $Y_T^{-1} Q^* Y_T^{-1'} = Q^{*-1}$, assim temos que
$$Cov(b^*_T) \to \sigma^2 Q^{*-1}$$
IV. Portanto, a matriz de covari√¢ncia assint√≥tica dos estimadores √© $\sigma^2 Q^{*-1}$.
‚ñ†

> üí° **Exemplo Num√©rico:**
> Para demonstrar o Teorema 1.1, vamos simular dados de um modelo com tend√™ncia temporal e calcular a matriz de covari√¢ncia dos estimadores. Vamos usar um modelo simples, $y_t = \beta_0 + \beta_1 t + \epsilon_t$, onde $\epsilon_t$ s√£o ru√≠dos brancos com vari√¢ncia $\sigma^2$. Vamos usar $\beta_0 = 2$, $\beta_1 = 0.5$, e $\sigma^2 = 1$.
>
> ```python
> import numpy as np
> import pandas as pd
>
> def simulate_data_and_estimate(T, beta0=2, beta1=0.5, sigma2=1):
>  """Simulates data with a time trend and estimates OLS coefficients.
>
>   Args:
>    T (int): The number of time periods.
>    beta0 (float): The intercept.
>    beta1 (float): The coefficient for the time trend.
>    sigma2 (float): The error variance.
>
>   Returns:
>    numpy.ndarray: The estimated coefficients.
>  """
>  t = np.arange(1, T + 1)
>  X = np.column_stack((np.ones(T), t))
>  epsilon = np.random.normal(0, np.sqrt(sigma2), T)
>  y = beta0 + beta1 * t + epsilon
>  beta_hat = np.linalg.inv(X.T @ X) @ (X.T @ y)
>  return beta_hat
>
> # Simulate data and calculate the covariance of the estimators
> num_simulations = 500
> T_values = [100, 1000, 10000]
> results = []
> for T in T_values:
>  beta_hats = np.array([simulate_data_and_estimate(T) for _ in range(num_simulations)])
>  cov_beta_hat = np.cov(beta_hats, rowvar=False)
>  scaled_cov = T * cov_beta_hat[0,0],T**2*cov_beta_hat[0,1],T**2*cov_beta_hat[1,0],T**3*cov_beta_hat[1,1]
>  results.append(scaled_cov)
>
> df = pd.DataFrame(results, index=T_values, columns=["T*Cov(b0,b0)","T^2*Cov(b0,b1)","T^2*Cov(b1,b0)","T^3*Cov(b1,b1)"])
> print("Scaled Covariance Matrices for Different T:")
> print(df)
> ```
>
> O c√≥digo acima estima os par√¢metros do modelo via simula√ß√£o, e computa a matriz de covari√¢ncia dos estimadores para diferentes valores de T. Note que a matriz de covari√¢ncia dos estimadores, quando escalada apropriadamente ($T$ para $b_0$ e $T^3$ para $b_1$), converge para a matriz $Q^{*-1}\sigma^2$.

### Conclus√£o

Este ap√™ndice detalhou a deriva√ß√£o da distribui√ß√£o assint√≥tica dos estimadores de MQO em modelos com tend√™ncias determin√≠sticas. O c√°lculo dos momentos e somat√≥rios, a identifica√ß√£o dos termos dominantes, e a aplica√ß√£o de fatores de escala s√£o elementos chave para obter a distribui√ß√£o limite da matriz de momentos e da matriz de covari√¢ncia dos estimadores. A t√©cnica de rescalonamento das vari√°veis permite tratar adequadamente as diferentes taxas de converg√™ncia dos estimadores, e a aplica√ß√£o do teorema do limite central permite obter a distribui√ß√£o assint√≥tica desejada. A an√°lise detalhada aqui complementa a discuss√£o principal do cap√≠tulo, oferecendo um entendimento mais profundo e preciso do processo de deriva√ß√£o da distribui√ß√£o assint√≥tica dos estimadores.

### Refer√™ncias
[^1]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113-44.
[^2]: Cap√≠tulo 16 do livro base.
<!-- END -->
