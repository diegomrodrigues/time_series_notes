## Taxas de Converg√™ncia e Distribui√ß√µes Limite em Modelos de Tend√™ncia Temporal

### Introdu√ß√£o
No cap√≠tulo anterior, exploramos a estima√ß√£o por m√≠nimos quadrados ordin√°rios (OLS) em modelos de regress√£o com tend√™ncias temporais determin√≠sticas. Vimos que, embora o modelo linear simples $y_t = \alpha + \delta t + \epsilon_t$ satisfa√ßa as suposi√ß√µes cl√°ssicas de regress√£o quando $\epsilon_t \sim N(0, \sigma^2)$ [^1], as distribui√ß√µes assint√≥ticas dos estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$ requerem um tratamento especial quando $\epsilon_t$ n√£o √© Gaussiano [^2]. Al√©m disso, a taxa de converg√™ncia dos estimadores para seus valores verdadeiros difere, com $\hat{\alpha}_T$ convergindo a uma taxa de $\sqrt{T}$ e $\hat{\delta}_T$ convergindo a uma taxa de $T^{3/2}$ [^7]. Este cap√≠tulo aprofunda essa an√°lise, focando em como obter distribui√ß√µes limites n√£o degeneradas para esses estimadores por meio do ajuste apropriado de suas taxas de converg√™ncia. A t√©cnica de transforma√ß√£o do modelo de regress√£o, introduzida neste contexto, √© fundamental para lidar com as diferentes taxas de converg√™ncia e ser√° expandida em cap√≠tulos subsequentes. O estimador do coeficiente da tend√™ncia temporal $(\hat{\delta}_T)$ √© superconsistente, o que implica que $T(\hat{\delta}_T - \delta) \xrightarrow{p} 0$ [^7]. Este comportamento assint√≥tico espec√≠fico requer que sejam utilizadas t√©cnicas distintas daquelas aplic√°veis em modelos com vari√°veis estacion√°rias.

**Observa√ß√£o 1:** A superconsist√™ncia do estimador $\hat{\delta}_T$ √© uma propriedade not√°vel, pois implica que ele converge para o valor verdadeiro $\delta$ mais rapidamente do que os estimadores em modelos tradicionais de s√©ries temporais com vari√°veis estacion√°rias. Essa propriedade √© uma consequ√™ncia direta da presen√ßa da tend√™ncia temporal determin√≠stica e da acumula√ß√£o de informa√ß√£o ao longo do tempo.

### Conceitos Fundamentais

O modelo de tend√™ncia temporal linear simples pode ser expresso como:
$$y_t = \alpha + \delta t + \epsilon_t$$ [^1] [16.1.1]
onde $\epsilon_t$ √© um processo de ru√≠do branco. A representa√ß√£o matricial deste modelo √© dada por [^2]:
$$y_t = x_t'\beta + \epsilon_t$$ [16.1.2]
onde $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$ [^2] [16.1.3] e $\beta = \begin{bmatrix} \alpha \\ \delta \end{bmatrix}$ [^2] [16.1.4]. O estimador OLS de $\beta$, denotado por $b_T$, √© dado por:

$$b_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} = \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} \sum_{t=1}^{T} x_t y_t$$ [^2] [16.1.5]

O desvio do estimador OLS de seu valor verdadeiro pode ser expresso como:

$$ (b_T - \beta) = \left(\sum_{t=1}^{T} x_t x_t'\right)^{-1} \sum_{t=1}^{T} x_t \epsilon_t$$ [^2] [16.1.6]

**Lema 1:** A matriz $\sum_{t=1}^T x_t x_t'$ pode ser expressa explicitamente como:

$$ \sum_{t=1}^T x_t x_t' = \sum_{t=1}^T \begin{bmatrix} 1 \\ t \end{bmatrix} \begin{bmatrix} 1 & t \end{bmatrix} = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix} = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} $$
*Prova:* A prova segue diretamente da defini√ß√£o de $x_t$ e do produto matricial.
I. Definimos $x_t$ como $\begin{bmatrix} 1 \\ t \end{bmatrix}$.
II. Calculamos o produto $x_t x_t' = \begin{bmatrix} 1 \\ t \end{bmatrix} \begin{bmatrix} 1 & t \end{bmatrix} = \begin{bmatrix} 1 & t \\ t & t^2 \end{bmatrix}$.
III. Somamos os produtos de $t=1$ a $T$ para obter $\sum_{t=1}^T x_t x_t' = \sum_{t=1}^T \begin{bmatrix} 1 & t \\ t & t^2 \end{bmatrix} = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix}$.
IV. Usando as f√≥rmulas para somas de inteiros, $\sum_{t=1}^T 1 = T$, $\sum_{t=1}^T t = \frac{T(T+1)}{2}$ e $\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6}$, obtemos a matriz resultante.
Portanto,  $\sum_{t=1}^T x_t x_t' = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix}$ ‚ñ†

> üí° **Exemplo Num√©rico:**  Vamos considerar um exemplo com $T=5$.
>
> 1.  Calculamos $\sum_{t=1}^5 x_t x_t'$:
> $$ \sum_{t=1}^5 x_t x_t' = \begin{bmatrix} 5 & \frac{5(5+1)}{2} \\ \frac{5(5+1)}{2} & \frac{5(5+1)(2\times5+1)}{6} \end{bmatrix} = \begin{bmatrix} 5 & 15 \\ 15 & 55 \end{bmatrix} $$
> Este resultado exemplifica o Lema 1 com um valor de T espec√≠fico.

Para obter uma distribui√ß√£o limite n√£o degenerada, vimos que √© necess√°rio multiplicar $\hat{\alpha}_T$ por $\sqrt{T}$ e $\hat{\delta}_T$ por $T^{3/2}$ [^4]. Isso √© equivalente a pr√©-multiplicar o desvio do estimador OLS $(b_T - \beta)$ pela matriz $Y_T$, definida como:

$$ Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} $$ [^4] [16.1.17]
Essa matriz de ajuste √© crucial para lidar com as diferentes taxas de converg√™ncia dos estimadores. A matriz $\sum_{t=1}^{T} x_t x_t'$ n√£o converge quando dividida por $T$ e, para obter uma matriz convergente, ela precisa ser dividida por $T^3$ [^4]. Contudo, tal matriz limite n√£o √© invert√≠vel, necessitando de abordagens alternativas.

Ao multiplicar [16.1.6] por $Y_T$, obtemos:
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} = Y_T \left(\sum_{t=1}^{T} x_t x_t'\right)^{-1} \sum_{t=1}^{T} x_t \epsilon_t =  Y_T \left(\sum_{t=1}^{T} x_t x_t'\right)^{-1} Y_T^{-1} Y_T \sum_{t=1}^{T} x_t \epsilon_t $$ [^5] [16.1.18]
O primeiro termo do lado direito da equa√ß√£o acima √© o resultado de aplicar o ajuste da taxa de converg√™ncia:
$$ Y_T \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} Y_T = \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} \begin{bmatrix} \sum_{t=1}^{T} 1 & \sum_{t=1}^{T} t \\ \sum_{t=1}^{T} t & \sum_{t=1}^{T} t^2 \end{bmatrix}^{-1} \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} $$ [^5]
Este termo converge para uma matriz n√£o singular $Q$ quando $T$ tende ao infinito:

$$  Y_T \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} Y_T \xrightarrow{p}  Q  = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$$ [^5] [16.1.19], [16.1.20]

**Proposi√ß√£o 1:** A converg√™ncia da matriz $ Y_T \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} Y_T$ para $Q$ pode ser demonstrada utilizando os resultados do Lema 1 e as propriedades das somas de pot√™ncias de inteiros.
*Prova:*
I. Do Lema 1, sabemos que $\sum_{t=1}^T x_t x_t' = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix}$.
II. Pr√© e p√≥s-multiplicando pela matriz $Y_T$ e sua inversa, temos:
 $$Y_T \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} Y_T =  \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{3/2} \end{bmatrix}  \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix}^{-1}  \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{3/2} \end{bmatrix}$$
III. Simplificando a express√£o e utilizando que  $\sum_{t=1}^{T} 1= T$, $\sum_{t=1}^{T} t= \frac{T(T+1)}{2}$, $\sum_{t=1}^{T} t^2 = \frac{T(T+1)(2T+1)}{6}$, obtemos:
$$ Y_T \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} Y_T  =  \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix}  \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix}^{-1}  \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} $$
IV. Calculando a inversa da matriz central e multiplicando por $Y_T$ e $Y_T^{-1}$, obtemos:
  $$ Y_T \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} Y_T = \begin{bmatrix} \frac{T(T+1)(2T+1)}{6}  & -\frac{T(T+1)}{2} \\ -\frac{T(T+1)}{2} & T \end{bmatrix} \frac{1}{\frac{T^2(T+1)(2T+1)}{6}-\frac{T^2(T+1)^2}{4}} \begin{bmatrix} T^{-1} & 0 \\ 0 & T^{-3} \end{bmatrix} $$
V. Simplificando as express√µes, temos:
$$ \begin{bmatrix}  \frac{\frac{T(T+1)(2T+1)}{6}}{T^3 \frac{T(T+1)(2T+1)}{6} - \frac{T^2(T+1)^2}{4} }   & -\frac{\frac{T(T+1)}{2}}{T^3 \frac{T(T+1)(2T+1)}{6} - \frac{T^2(T+1)^2}{4} }   \\ -\frac{\frac{T(T+1)}{2}}{T^3 \frac{T(T+1)(2T+1)}{6} - \frac{T^2(T+1)^2}{4} }   & \frac{T}{T^3 \frac{T(T+1)(2T+1)}{6} - \frac{T^2(T+1)^2}{4} }    \end{bmatrix} $$

VI. Simplificando os termos, dividindo por $T^3$, e tomando o limite quando $T \rightarrow \infty$, chegamos a matriz $Q$:

$$ \lim_{T\to\infty} Y_T \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} Y_T = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} = Q $$ ‚ñ†

> üí° **Exemplo Num√©rico:** Continuemos com o exemplo onde $T=5$.
>
> 1. Calculamos $Y_5$:
>    $$Y_5 = \begin{bmatrix} \sqrt{5} & 0 \\ 0 & 5^{3/2} \end{bmatrix} = \begin{bmatrix} 2.236 & 0 \\ 0 & 11.180 \end{bmatrix} $$
>
> 2. Calculamos $Y_5^{-1}$:
>    $$Y_5^{-1} = \begin{bmatrix} \frac{1}{\sqrt{5}} & 0 \\ 0 & \frac{1}{5^{3/2}} \end{bmatrix} = \begin{bmatrix} 0.447 & 0 \\ 0 & 0.089 \end{bmatrix}$$
>
> 3. Usamos o resultado do exemplo anterior para $\sum_{t=1}^5 x_t x_t'$:
>
> $$ \left( \sum_{t=1}^{5} x_t x_t' \right)^{-1} = \begin{bmatrix} 5 & 15 \\ 15 & 55 \end{bmatrix}^{-1} \approx  \begin{bmatrix} 0.22 & -0.06 \\ -0.06 & 0.02 \end{bmatrix} $$
>
> 4. Calculamos $Y_5 \left( \sum_{t=1}^{5} x_t x_t' \right)^{-1} Y_5$:
>
> $$  Y_5 \left( \sum_{t=1}^{5} x_t x_t' \right)^{-1} Y_5 \approx  \begin{bmatrix} 2.236 & 0 \\ 0 & 11.180 \end{bmatrix}  \begin{bmatrix} 0.22 & -0.06 \\ -0.06 & 0.02 \end{bmatrix}  \begin{bmatrix} 2.236 & 0 \\ 0 & 11.180 \end{bmatrix}  \approx \begin{bmatrix} 1.1 & -1.5 \\ -1.5 & 2.25 \end{bmatrix} $$
>   Note que esta matriz est√° se aproximando da matriz $Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$ quando $T$ aumenta. Este exemplo ilustra como a transforma√ß√£o com $Y_T$ permite obter uma matriz convergente.

O segundo termo do lado direito de [16.1.18] √© dado por:

$$Y_T \sum_{t=1}^{T} x_t \epsilon_t = \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} \sum_{t=1}^{T} \epsilon_t \\ \sum_{t=1}^{T} t \epsilon_t \end{bmatrix} = \begin{bmatrix} T^{-1/2} \sum_{t=1}^{T} \epsilon_t \\ T^{-1} \sum_{t=1}^{T} (t/T) \epsilon_t \end{bmatrix}$$ [^5] [16.1.21]

Sob as suposi√ß√µes padr√£o de que $\epsilon_t$ √© i.i.d. com m√©dia zero, vari√¢ncia $\sigma^2$ e um quarto momento finito, este vetor converge assintoticamente para uma distribui√ß√£o normal bivariada:
$$ \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^{T} \epsilon_t \\ \frac{1}{T^{3/2}} \sum_{t=1}^{T} t \epsilon_t \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q) $$ [^7] [16.1.24]

**Lema 2:** O resultado da distribui√ß√£o assint√≥tica
$$ \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^{T} \epsilon_t \\ \frac{1}{T^{3/2}} \sum_{t=1}^{T} t \epsilon_t \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q) $$ √© uma aplica√ß√£o do Teorema do Limite Central Multivariado e do Teorema de Slutsky.
*Prova:*
I. Pelo Teorema do Limite Central (TLC),  $\frac{1}{\sqrt{T}} \sum_{t=1}^{T} \epsilon_t \xrightarrow{d} N(0, \sigma^2)$.
II. Para o segundo termo,  $\frac{1}{T^{3/2}} \sum_{t=1}^{T} t \epsilon_t = \frac{1}{T} \sum_{t=1}^{T} \frac{t}{T^{1/2}} \epsilon_t$. Dado que $\epsilon_t$ s√£o i.i.d com m√©dia zero e vari√¢ncia $\sigma^2$, temos que $\frac{1}{T} \sum_{t=1}^{T} \frac{t}{T} \epsilon_t$  converge para uma distribui√ß√£o normal com vari√¢ncia $\sigma^2/3$ [^7].
III. A covari√¢ncia entre os dois termos √© dada por:
$Cov\left(\frac{1}{\sqrt{T}}\sum_{t=1}^{T} \epsilon_t, \frac{1}{T^{3/2}}\sum_{t=1}^{T} t \epsilon_t \right) = \frac{1}{T^{2}}\sum_{t=1}^{T} t \sigma^2  = \frac{\sigma^2}{T^{2}} \sum_{t=1}^{T} t= \frac{\sigma^2}{T^{2}}\frac{T(T+1)}{2}  \rightarrow \sigma^2/2$.
IV. Aplicando o Teorema do Limite Central Multivariado, conclu√≠mos que o vetor converge para uma distribui√ß√£o normal bivariada com matriz de covari√¢ncia $\sigma^2 Q$. A matriz $Q$ tem elementos $q_{11} = 1$, $q_{12} = q_{21} = 1/2$, e $q_{22}=1/3$.‚ñ†

> üí° **Exemplo Num√©rico:** Vamos simular um conjunto de dados com $T=100$, $\alpha = 5$, $\delta = 0.5$, e $\epsilon_t \sim N(0, 1)$.
> ```python
> import numpy as np
> import pandas as pd
> from scipy import stats
>
> # Set seed for reproducibility
> np.random.seed(42)
>
> # Define parameters
> T = 100
> alpha = 5
> delta = 0.5
> sigma = 1
>
> # Generate errors
> errors = np.random.normal(0, sigma, T)
>
> # Generate time trend
> time = np.arange(1, T+1)
>
> # Generate the dependent variable
> y = alpha + delta * time + errors
>
> # Create the design matrix
> X = np.vstack([np.ones(T), time]).T
>
> # Calculate OLS estimator
> beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
>
> # Extract individual estimators
> alpha_hat = beta_hat[0]
> delta_hat = beta_hat[1]
>
> # Calculate adjusted estimators
> alpha_hat_adjusted = np.sqrt(T) * (alpha_hat - alpha)
> delta_hat_adjusted = T**(3/2) * (delta_hat - delta)
>
> # Calculate Y_T
> Y_T = np.array([[np.sqrt(T), 0], [0, T**(3/2)]])
>
> # Create the vector of the scaled OLS estimates
> scaled_estimates = Y_T @ (beta_hat - np.array([alpha, delta]))
>
> print(f"Estimated alpha: {alpha_hat}")
> print(f"Estimated delta: {delta_hat}")
> print(f"Adjusted alpha_hat: {alpha_hat_adjusted}")
>print(f"Adjusted delta_hat: {delta_hat_adjusted}")
> print(f"Scaled estimates: {scaled_estimates}")
>
> # Calculate the sample covariance matrix of the scaled residuals.
> residuals = y - X @ beta_hat
> # Scaled residuals
> scaled_residuals = (Y_T @ np.linalg.inv(X.T @ X) @ X.T @ residuals).T
>
> sample_covariance_matrix =  (scaled_residuals.T @ scaled_residuals)/(T-2)
> print(f"Sample Covariance Matrix of Scaled Residuals:\n{sample_covariance_matrix}")
>
> # Estimated Sigma^2
> sigma_squared_hat = np.sum(residuals**2) / (T-2)
>
> # Calculate the asymptotic covariance matrix
> Q_inv = np.array([[4,-6],[-6,12]])
> asymptotic_covariance_matrix = sigma_squared_hat * Q_inv
> print(f"Asymptotic Covariance Matrix: \n{asymptotic_covariance_matrix}")
>
> # Confidence intervals
> alpha_conf_interval = (alpha_hat - 1.96*np.sqrt(asymptotic_covariance_matrix[0,0]/T),alpha_hat + 1.96*np.sqrt(asymptotic_covariance_matrix[0,0]/T))
> delta_conf_interval = (delta_hat - 1.96*np.sqrt(asymptotic_covariance_matrix[1,1]/T**3), delta_hat + 1.96*np.sqrt(asymptotic_covariance_matrix[1,1]/T**3))
>
> print(f"Confidence interval for alpha:{alpha_conf_interval}")
> print(f"Confidence interval for delta: {delta_conf_interval}")
>
> # Hypothesis testing for delta
> # Null Hypothesis: delta = 0
> t_stat = delta_hat/np.sqrt(asymptotic_covariance_matrix[1,1]/T**3)
> p_value = 2*(1 - stats.t.cdf(abs(t_stat), T-2))
> print(f"T-statistic: {t_stat}, P-value: {p_value}")
> ```
>
> Este exemplo num√©rico mostra como os estimadores ajustados, multiplicados pelas taxas de converg√™ncia corretas, se comportam e podem ser usados para a realiza√ß√£o de testes de hip√≥teses e constru√ß√£o de intervalos de confian√ßa. Observe que as matrizes de covari√¢ncia amostral e assint√≥tica s√£o bem pr√≥ximas e que o valor-p do teste de hip√≥teses para a inclina√ß√£o √© pequeno, rejeitando a hip√≥tese nula de que a inclina√ß√£o $\delta$ √© zero.

Combinando [16.1.19] e [16.1.24], obtemos a distribui√ß√£o assint√≥tica dos estimadores:

$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q^{-1}) $$ [^7] [16.1.25]
Onde $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$. Portanto, a corre√ß√£o pela matriz $Y_T$ √© necess√°ria para garantir que as distribui√ß√µes limites tenham vari√¢ncias n√£o nulas.

### Conclus√£o

Em resumo, a abordagem apresentada neste cap√≠tulo demonstra que a multiplica√ß√£o de $\hat{\alpha}_T$ por $\sqrt{T}$ e de $\hat{\delta}_T$ por $T^{3/2}$ √© crucial para obter distribui√ß√µes limites n√£o degeneradas para os estimadores OLS em modelos de tend√™ncia temporal determin√≠stica [^7]. A matriz $Y_T$, ao ajustar as taxas de converg√™ncia, garante que a distribui√ß√£o assint√≥tica dos estimadores tenha uma vari√¢ncia n√£o nula e que possamos aplicar as estat√≠sticas padr√£o de teste de hip√≥teses de forma assintoticamente v√°lida. A t√©cnica de transforma√ß√£o do modelo de regress√£o, com a introdu√ß√£o da matriz $Y_T$, √© um passo fundamental para lidar com processos n√£o estacion√°rios e ser√° fundamental para as discuss√µes de modelos mais complexos nos cap√≠tulos subsequentes [^2]. A superconsist√™ncia de $\hat{\delta}_T$, como apontado na observa√ß√£o 1, √© uma caracter√≠stica chave que distingue este tipo de modelo de outros tipos de modelos de s√©ries temporais [^7].

**Corol√°rio 1:**  A converg√™ncia dos estimadores de $\alpha$ e $\delta$ , ap√≥s a devida corre√ß√£o, para uma distribui√ß√£o normal permite realizar testes de hip√≥teses sobre os par√¢metros, como por exemplo, a hip√≥tese nula de que o par√¢metro $\delta=0$. Tais testes s√£o  fundamentais para verificar a signific√¢ncia estat√≠stica de uma tend√™ncia temporal em um modelo de regress√£o.
*Prova:*
I. A Proposi√ß√£o 1 e o Lema 2 estabelecem que, ap√≥s o ajuste adequado pelas taxas de converg√™ncia, os estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$ convergem assintoticamente para uma distribui√ß√£o normal. Especificamente, $\sqrt{T}(\hat{\alpha}_T - \alpha)$ e $T^{3/2}(\hat{\delta}_T - \delta)$ convergem para uma distribui√ß√£o normal.
II. A converg√™ncia assint√≥tica para uma distribui√ß√£o normal permite a aplica√ß√£o de testes de hip√≥teses assint√≥ticos padr√£o. Isso significa que podemos utilizar estat√≠sticas de teste (como testes t e testes F) que se baseiam em aproxima√ß√µes normais quando o tamanho da amostra (T) √© grande.
III. A hip√≥tese nula de que o par√¢metro da tend√™ncia temporal $\delta=0$ √© um exemplo direto de um teste de hip√≥teses que pode ser realizado usando a distribui√ß√£o assint√≥tica do estimador $\hat{\delta}_T$.  A estat√≠stica de teste constru√≠da a partir de $\hat{\delta}_T$ , ap√≥s a corre√ß√£o pela taxa de converg√™ncia (multiplica√ß√£o por $T^{3/2}$) , converge para uma distribui√ß√£o normal padr√£o sob a hip√≥tese nula, permitindo a determina√ß√£o de um valor-p e a realiza√ß√£o do teste com o n√≠vel de signific√¢ncia desejado.
Portanto, a converg√™ncia para uma distribui√ß√£o normal, ap√≥s o ajuste, justifica o uso de testes de hip√≥teses assint√≥ticos para infer√™ncia sobre os par√¢metros, inclusive para a hip√≥tese de que a tend√™ncia temporal n√£o √© significativa ($\delta=0$). ‚ñ†

<!-- END -->
### Refer√™ncias
[^1]:  [16.1]
[^2]:  [16.1], [16.1.1] - [16.1.6]
[^3]:  [16.1], [16.1.9] - [16.1.14]
[^4]:  [16.1], [16.1.16] - [16.1.17]
[^5]:  [16.1], [16.1.18] - [16.1.21]
[^6]:  [16.1], [16.1.22] - [16.1.24]
[^7]:  [16.1], [16.1.25] - [16.1.27]
