## Converg√™ncia, Lei dos Grandes N√∫meros e Teorema do Limite Central em Modelos de Tend√™ncia Temporal

### Introdu√ß√£o
Este cap√≠tulo explora os conceitos de converg√™ncia em probabilidade, converg√™ncia em distribui√ß√£o, a lei dos grandes n√∫meros (LLN) e o teorema do limite central (TLC), que s√£o cruciais para compreender o comportamento assint√≥tico de estimadores em modelos de tend√™ncia temporal determin√≠stica. Como j√° vimos, a an√°lise da matriz de covari√¢ncia e a obten√ß√£o de distribui√ß√µes limites para os estimadores em modelos com tend√™ncias temporais requerem uma abordagem diferente daquela utilizada para modelos com vari√°veis estacion√°rias [^1]. Este cap√≠tulo detalha a aplica√ß√£o desses conceitos fundamentais na an√°lise de modelos com tend√™ncias temporais, fornecendo uma base te√≥rica s√≥lida para a infer√™ncia estat√≠stica nesses modelos [^7].

### Converg√™ncia em Probabilidade e a Lei dos Grandes N√∫meros
A converg√™ncia em probabilidade descreve o comportamento de uma sequ√™ncia de vari√°veis aleat√≥rias quando o tamanho da amostra tende ao infinito. Formalmente, uma sequ√™ncia de vari√°veis aleat√≥rias $\{X_T\}_{T=1}^{\infty}$ converge em probabilidade para uma constante $c$, denotado por $X_T \xrightarrow{p} c$, se para qualquer $\epsilon > 0$, tivermos
$$ \lim_{T\to\infty} P(|X_T - c| > \epsilon) = 0 $$
Essa defini√ß√£o implica que, √† medida que o tamanho da amostra $T$ aumenta, a probabilidade de que a vari√°vel aleat√≥ria $X_T$ se desvie de $c$ por mais do que $\epsilon$ se torna arbitrariamente pequena.

A Lei dos Grandes N√∫meros (LLN) estabelece condi√ß√µes sob as quais a m√©dia amostral de uma sequ√™ncia de vari√°veis aleat√≥rias converge em probabilidade para a m√©dia populacional. Em termos gerais, se $\{X_t\}_{t=1}^{\infty}$ s√£o vari√°veis aleat√≥rias independentes e identicamente distribu√≠das (i.i.d.) com m√©dia $\mu$ e vari√¢ncia finita, ent√£o a m√©dia amostral $\bar{X}_T = \frac{1}{T} \sum_{t=1}^T X_t$ converge em probabilidade para $\mu$:
$$ \bar{X}_T \xrightarrow{p} \mu $$
A LLN √© crucial na an√°lise assint√≥tica, pois garante que as m√©dias amostrais convergem para os par√¢metros populacionais √† medida que o tamanho da amostra aumenta. Em outras palavras, para um n√∫mero suficientemente grande de amostras, a m√©dia amostral √© uma boa aproxima√ß√£o da m√©dia populacional.

Na an√°lise de modelos de tend√™ncia temporal, a LLN √© frequentemente utilizada para mostrar que as m√©dias amostrais de produtos de regressores e erros convergem para valores esperados. Por exemplo, na an√°lise da matriz de covari√¢ncia $\sum_{t=1}^T x_t x_t'$ [^4], usamos a LLN para estabelecer a converg√™ncia das somas de quadrados e produtos de vari√°veis explicativas e para obter uma matriz limite para essas somas. Embora a LLN n√£o se aplique diretamente √† matriz $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ devido √† n√£o estacionariedade dos regressores, os princ√≠pios da LLN s√£o importantes para analisar a converg√™ncia de vers√µes ajustadas dessa matriz.

> üí° **Exemplo Num√©rico:** Para ilustrar a LLN, vamos simular vari√°veis aleat√≥rias i.i.d. com m√©dia $\mu = 5$ e vari√¢ncia $\sigma^2= 4$, para diferentes tamanhos de amostra $T$, e calcular a m√©dia amostral e o erro m√©dio.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> import pandas as pd
>
> # Definir par√¢metros
> mu = 5
> sigma = 2
> num_simulations = 100
> T_values = [100, 500, 1000, 5000]
>
> # Simular amostras e calcular m√©dias amostrais
> results = []
> for T in T_values:
>     sample_means = []
>     for _ in range(num_simulations):
>         sample = np.random.normal(mu, sigma, T)
>         sample_mean = np.mean(sample)
>         sample_means.append(sample_mean)
>     results.append([T, np.mean(sample_means), np.std(sample_means)])
>
> results_df = pd.DataFrame(results, columns=['T','Mean of Sample Means', 'Std of Sample Means'])
> print(results_df)
>
> # Plot da converg√™ncia das m√©dias amostrais
> plt.figure(figsize=(10,5))
> plt.plot(T_values, [r[1] for r in results] , label = 'Mean of Sample Means')
> plt.xlabel('Tamanho da amostra (T)')
> plt.ylabel('M√©dia amostral')
> plt.title('Converg√™ncia da M√©dia Amostral para a M√©dia Populacional')
> plt.axhline(mu, color='red', linestyle='--', label = 'M√©dia Populacional')
> plt.legend()
> plt.show()
> ```
> Os resultados da simula√ß√£o mostram que, √† medida que $T$ aumenta, as m√©dias amostrais convergem para a m√©dia populacional $\mu = 5$, com o desvio padr√£o das m√©dias amostrais decrescendo, o que evidencia a converg√™ncia em probabilidade.

**Lema 1:** *Desigualdade de Chebychev*. Seja $X$ uma vari√°vel aleat√≥ria com m√©dia $\mu$ e vari√¢ncia $\sigma^2$. Para qualquer $\epsilon > 0$, temos:
$$ P(|X - \mu| \geq \epsilon) \leq \frac{\sigma^2}{\epsilon^2} $$
*Prova:*
A desigualdade de Chebychev √© uma ferramenta importante para demonstrar a converg√™ncia em probabilidade, fornecendo um limite superior para a probabilidade de uma vari√°vel aleat√≥ria se desviar de sua m√©dia. A prova desta desigualdade √© dada da seguinte forma:

I. Come√ßamos com a defini√ß√£o da vari√¢ncia de $X$:
$$ \sigma^2 = E[(X-\mu)^2] = \int_{-\infty}^{\infty} (x-\mu)^2 f(x) \, dx$$
II. Separamos a integral em duas partes, uma onde $|x-\mu| \geq \epsilon$ e outra onde $|x-\mu| < \epsilon$:
$$\sigma^2 = \int_{|x-\mu| \geq \epsilon} (x-\mu)^2 f(x) \, dx + \int_{|x-\mu| < \epsilon} (x-\mu)^2 f(x) \, dx$$
III. Como $(x-\mu)^2$ √© sempre n√£o negativo,  a segunda integral √© maior ou igual a zero. Logo:
$$ \sigma^2 \geq \int_{|x-\mu| \geq \epsilon} (x-\mu)^2 f(x) \, dx$$
IV. J√° que, na regi√£o de integra√ß√£o, $|x-\mu| \geq \epsilon$, ent√£o $(x-\mu)^2 \geq \epsilon^2$. Assim:
$$ \sigma^2 \geq \int_{|x-\mu| \geq \epsilon} \epsilon^2 f(x) \, dx = \epsilon^2 \int_{|x-\mu| \geq \epsilon}  f(x) \, dx = \epsilon^2 P(|X-\mu| \geq \epsilon)$$
V. Dividindo ambos os lados por $\epsilon^2$ temos:
$$P(|X-\mu| \geq \epsilon) \leq \frac{\sigma^2}{\epsilon^2}$$
O que prova a desigualdade de Chebychev. $\blacksquare$

A Desigualdade de Chebychev pode ser usada para demonstrar a Lei Fraca dos Grandes N√∫meros, que √© uma vers√£o menos restritiva da LLN, pois assume apenas que as vari√°veis aleat√≥rias s√£o n√£o correlacionadas e t√™m vari√¢ncia limitada (e n√£o necessariamente i.i.d.).

**Teorema 2:** *Lei Fraca dos Grandes N√∫meros (LFGN)*. Se $\{X_t\}_{t=1}^{\infty}$ √© uma sequ√™ncia de vari√°veis aleat√≥rias n√£o correlacionadas com m√©dia $\mu$ e vari√¢ncia $\sigma^2_t$ tal que $\lim_{T \to \infty} \frac{1}{T^2} \sum_{t=1}^T \sigma^2_t = 0$, ent√£o a m√©dia amostral $\bar{X}_T = \frac{1}{T} \sum_{t=1}^T X_t$ converge em probabilidade para $\mu$:
$$ \bar{X}_T \xrightarrow{p} \mu $$

*Prova:*
I. Pela defini√ß√£o da m√©dia amostral $\bar{X}_T$, temos que $E[\bar{X}_T] = \frac{1}{T}\sum_{t=1}^{T} E[X_t] = \frac{1}{T}\sum_{t=1}^{T} \mu = \mu$.
II. A vari√¢ncia da m√©dia amostral √© dada por:
$$Var(\bar{X}_T) = Var(\frac{1}{T}\sum_{t=1}^{T} X_t) = \frac{1}{T^2}\sum_{t=1}^{T} Var(X_t) + \frac{1}{T^2} \sum_{i \neq j} Cov(X_i, X_j)$$
III. Como assumimos que as vari√°veis s√£o n√£o correlacionadas, ent√£o $Cov(X_i, X_j) = 0$. Logo:
$$Var(\bar{X}_T) = \frac{1}{T^2} \sum_{t=1}^{T} Var(X_t) = \frac{1}{T^2}\sum_{t=1}^T \sigma^2_t$$
IV. Pela desigualdade de Chebychev (Lema 1), temos que:
$$ P(|\bar{X}_T - \mu| \geq \epsilon) \leq \frac{Var(\bar{X}_T)}{\epsilon^2} =  \frac{\frac{1}{T^2}\sum_{t=1}^T \sigma^2_t}{\epsilon^2}$$
V. Como $\lim_{T \to \infty} \frac{1}{T^2} \sum_{t=1}^T \sigma^2_t = 0$, ent√£o $\lim_{T \to \infty}  \frac{\frac{1}{T^2}\sum_{t=1}^T \sigma^2_t}{\epsilon^2}=0$, e portanto:
$$ \lim_{T\to\infty} P(|\bar{X}_T - \mu| \geq \epsilon) = 0$$
O que mostra que  $\bar{X}_T$ converge em probabilidade para $\mu$. $\blacksquare$

**Corol√°rio 2:** Se as vari√°veis aleat√≥rias s√£o i.i.d. com vari√¢ncia finita $\sigma^2$, ent√£o a LFGN √© satisfeita, pois $\lim_{T \to \infty} \frac{1}{T^2} \sum_{t=1}^T \sigma^2 = \lim_{T \to \infty} \frac{1}{T^2} T\sigma^2 =  \lim_{T \to \infty} \frac{\sigma^2}{T} = 0$. Portanto, a LLN padr√£o, enunciada anteriormente, √© um caso particular da Lei Fraca dos Grandes N√∫meros.

### Converg√™ncia em Distribui√ß√£o e o Teorema do Limite Central
A converg√™ncia em distribui√ß√£o descreve o comportamento da distribui√ß√£o de uma sequ√™ncia de vari√°veis aleat√≥rias quando o tamanho da amostra tende ao infinito. Formalmente, uma sequ√™ncia de vari√°veis aleat√≥rias $\{X_T\}_{T=1}^{\infty}$ converge em distribui√ß√£o para uma vari√°vel aleat√≥ria $X$, denotado por $X_T \xrightarrow{d} X$, se para todo ponto $x$ em que a fun√ß√£o de distribui√ß√£o cumulativa (CDF) de $X$, denotada por $F_X(x)$, √© cont√≠nua, tivermos
$$ \lim_{T\to\infty} F_{X_T}(x) = F_X(x) $$
onde $F_{X_T}(x)$ √© a CDF da vari√°vel aleat√≥ria $X_T$. Esta defini√ß√£o implica que, √† medida que o tamanho da amostra aumenta, a distribui√ß√£o de $X_T$ se torna cada vez mais semelhante √† distribui√ß√£o de $X$.

O Teorema do Limite Central (TLC) estabelece que a distribui√ß√£o da m√©dia amostral de uma sequ√™ncia de vari√°veis aleat√≥rias i.i.d., quando normalizada, converge para uma distribui√ß√£o normal, mesmo que a distribui√ß√£o original n√£o seja normal. Formalmente, se $\{X_t\}_{t=1}^{\infty}$ s√£o vari√°veis aleat√≥rias i.i.d. com m√©dia $\mu$ e vari√¢ncia $\sigma^2$, ent√£o a m√©dia amostral normalizada $\frac{\bar{X}_T - \mu}{\sigma/\sqrt{T}}$ converge em distribui√ß√£o para uma vari√°vel aleat√≥ria normal padr√£o $Z \sim N(0,1)$:
$$ \frac{\bar{X}_T - \mu}{\sigma/\sqrt{T}} \xrightarrow{d} N(0,1) $$
O TLC √© uma ferramenta fundamental na infer√™ncia estat√≠stica, pois permite aproximar a distribui√ß√£o de estimadores por uma distribui√ß√£o normal, possibilitando a constru√ß√£o de testes de hip√≥teses e intervalos de confian√ßa.

Na an√°lise de modelos de tend√™ncia temporal, o TLC √© crucial para determinar o comportamento assint√≥tico da componente aleat√≥ria dos modelos transformados. Por exemplo, ao analisar a distribui√ß√£o limite do termo $Y_T \sum_{t=1}^T x_t \epsilon_t$, mostramos que a distribui√ß√£o da componente aleat√≥ria desse termo se aproxima de uma normal multivariada [^7], o que permite usar as ferramentas padr√£o de infer√™ncia assint√≥tica [^7].

> üí° **Exemplo Num√©rico:** Para ilustrar o TLC, vamos simular vari√°veis aleat√≥rias i.i.d. com distribui√ß√£o uniforme no intervalo $[0, 1]$, para diferentes tamanhos de amostra $T$. Vamos normalizar as m√©dias amostrais e verificar sua converg√™ncia para uma distribui√ß√£o normal.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy.stats import norm
> import pandas as pd
>
> # Definir par√¢metros
> num_simulations = 100
> T_values = [100, 500, 1000, 5000]
>
> # Simular amostras, calcular m√©dias amostrais e normaliz√°-las
> results = []
> for T in T_values:
>     normalized_means = []
>     for _ in range(num_simulations):
>         sample = np.random.uniform(0, 1, T)
>         sample_mean = np.mean(sample)
>         sample_std = np.std(sample, ddof = 1) #ddof =1 para calculo correto do desvio padr√£o
>         normalized_mean = (sample_mean - 0.5) / (sample_std/np.sqrt(T)) #mean and std for uniform [0,1] is 0.5 and sqrt(1/12)
>         normalized_means.append(normalized_mean)
>     results.append([T, normalized_means])
>
>
> # Plot dos histogramas das m√©dias amostrais normalizadas e da PDF de uma normal padr√£o
> fig, axs = plt.subplots(2, 2, figsize=(12, 8))
> axs = axs.flatten()
> for i, T in enumerate(T_values):
>   normalized_means = results[i][1]
>   axs[i].hist(normalized_means, bins = 30, density=True, alpha = 0.6, label = 'Histograma')
>   x = np.linspace(-5,5,100)
>   axs[i].plot(x, norm.pdf(x,0,1), label = 'Normal Distribution')
>   axs[i].set_title(f'Histograma das M√©dias Normalizadas para T = {T}')
>   axs[i].set_xlabel('Valor')
>   axs[i].set_ylabel('Densidade')
>   axs[i].legend()
> plt.tight_layout()
> plt.show()
> ```
> Os histogramas mostram que, √† medida que o tamanho da amostra $T$ aumenta, a distribui√ß√£o das m√©dias amostrais normalizadas se aproxima da distribui√ß√£o normal padr√£o $N(0,1)$, ilustrando o TLC.

### Aplica√ß√£o em Modelos de Tend√™ncia Temporal
Na an√°lise de modelos de tend√™ncia temporal determin√≠stica, os conceitos de converg√™ncia em probabilidade e em distribui√ß√£o, juntamente com a LLN e o TLC, s√£o utilizados para obter as distribui√ß√µes assint√≥ticas dos estimadores OLS. A matriz de covari√¢ncia amostral dos regressores, dada por $\sum_{t=1}^T x_t x_t'$, n√£o converge para uma matriz n√£o singular quando dividida por $T$ [^4]. Em vez disso, ela diverge [^4]. Para lidar com essa diverg√™ncia, precisamos pr√© e p√≥s-multiplicar por matrizes de "rescaling" como a $Y_T$ [^4].  Usamos a LLN e o conceito de converg√™ncia em probabilidade para garantir que certas m√©dias amostrais, envolvendo os regressores, seus quadrados e produtos, convirjam para os valores esperados. Embora n√£o tenhamos usado a LLN diretamente para demonstrar a converg√™ncia de $\frac{1}{T} \sum_{t=1}^T x_t x_t'$, os princ√≠pios da LLN s√£o importantes para compreender que, quando multiplicada por $Y_T$, essa matriz tem uma distribui√ß√£o limite.

O TLC √© crucial para determinar a distribui√ß√£o limite da parte estoc√°stica do modelo. Ao aplicar a transforma√ß√£o $Y_T$, o termo de erro transformado $Y_T\sum_{t=1}^T x_t\epsilon_t$ converge para uma distribui√ß√£o normal multivariada [^7]. A aplica√ß√£o do TLC √© feita em um cen√°rio multivariado e  envolve a an√°lise conjunta da converg√™ncia dos estimadores e dos res√≠duos. A combina√ß√£o da converg√™ncia das matrizes com o TLC permite obter a distribui√ß√£o assint√≥tica dos estimadores OLS transformados, que √© ent√£o utilizada para infer√™ncia estat√≠stica [^7].

**Teorema 1:** A aplica√ß√£o do Teorema do Limite Central e a Lei dos Grandes N√∫meros, juntamente com a t√©cnica de rescaling por meio de $Y_T$, garante que o estimador OLS transformado em um modelo de tend√™ncia temporal linear converge para uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia finita.
*Prova:*
I. Pela Lei dos Grandes N√∫meros, a matriz $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T$, quando $T \rightarrow \infty$, converge para uma matriz n√£o singular $Q$ [^5].
II. Pelo Teorema do Limite Central, o termo $Y_T\sum_{t=1}^T x_t\epsilon_t$ converge para uma distribui√ß√£o normal multivariada com m√©dia zero e matriz de covari√¢ncia $\sigma^2 Q$, ou seja,
$$  Y_T \sum_{t=1}^T x_t \epsilon_t  \xrightarrow{d}  N(0, \sigma^2 Q) $$ [^7] [16.1.24]
III. Combinando os resultados, o estimador OLS transformado, dado por:
$$ Y_T(b_T - \beta) = \left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right] \left[ Y_T \sum_{t=1}^T x_t \epsilon_t \right] $$
converge em distribui√ß√£o para uma normal multivariada com m√©dia zero e vari√¢ncia $\sigma^2 Q^{-1}$, dado por:
$$ Y_T(b_T - \beta)  \xrightarrow{d} N(0, \sigma^2 Q^{-1}) $$ [^7] [16.1.25]
IV. Este resultado garante que a an√°lise assint√≥tica dos estimadores OLS em modelos de tend√™ncia temporal √© v√°lida. $\blacksquare$

> üí° **Exemplo Num√©rico:** Ilustremos a aplica√ß√£o do TLC e da LLN no modelo de tend√™ncia temporal. Simulemos os dados, estimemos o modelo, e verifiquemos a distribui√ß√£o dos estimadores.
> ```python
> import numpy as np
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
> from scipy.stats import norm
> import pandas as pd
>
> np.random.seed(42)
> # Define parameters
> alpha = 5
> delta = 0.5
> sigma = 1
>
> def simulate_and_estimate(T):
>   time = np.arange(1, T+1)
>   errors = np.random.normal(0, sigma, T)
>   y = alpha + delta*time + errors
>   X = sm.add_constant(time)
>   model = sm.OLS(y, X)
>   results = model.fit()
>   return results.params, results.bse
>
> num_simulations = 1000
> T_values = [100, 500, 1000]
>
> results = []
> for T in T_values:
>   scaled_alpha = []
>   scaled_delta = []
>   for i in range(num_simulations):
>     params, std_err = simulate_and_estimate(T)
>     scaled_alpha.append(np.sqrt(T) * (params[0] - alpha))
>     scaled_delta.append(T**(3/2) * (params[1] - delta))
>
>   results.append([T, scaled_alpha, scaled_delta])
>
> # Plot histogram
> fig, axs = plt.subplots(1, 3, figsize=(18, 5))
>
> for i, T in enumerate(T_values):
>  scaled_alpha_values = results[i][1]
>  scaled_delta_values = results[i][2]
>
>  axs[i].hist(scaled_alpha_values, bins = 30, density=True, alpha = 0.6, label = 'Histogram')
>  mu_alpha = np.mean(scaled_alpha_values)
>  sigma_alpha = np.std(scaled_alpha_values)
>  x = np.linspace(mu_alpha - 3*sigma_alpha, mu_alpha + 3*sigma_alpha, 100)
>  axs[i].plot(x, norm.pdf(x,mu_alpha, sigma_alpha), label = 'Normal Distribution')
>  axs[i].set_title(f'Distribui√ß√£o do Estimador Transformado (T={T})')
>  axs[i].set_xlabel('Valor do Estimador')
>  axs[i].set_ylabel('Densidade')
>  axs[i].legend()
>
> plt.tight_layout()
> plt.show()
>
> #Demonstration of convergence of residuals and parameters to zero
> for T in T_values:
>    params, std_err = simulate_and_estimate(T)
>    print(f"For T = {T},  Estimates {params}, Standard Error {std_err}")
>
> ```
> Este exemplo num√©rico simula a aplica√ß√£o do TLC no contexto do modelo de tend√™ncia temporal. Os histogramas da distribui√ß√£o dos estimadores transformados se aproximam da distribui√ß√£o normal. Os resultados do c√≥digo tamb√©m demonstram que, √† medida que $T$ aumenta, os estimadores se aproximam dos valores verdadeiros, e seus desvios padr√£o decrescem, confirmando as taxas de converg√™ncia te√≥ricas.

**Corol√°rio 1:** A converg√™ncia da matriz $Y_T\left(\sum_{t=1}^T x_t x_t'\right)^{-1} Y_T$ para uma matriz n√£o singular $Q$ e a converg√™ncia da parte estoc√°stica $Y_T \sum_{t=1}^T x_t \epsilon_t$ para uma distribui√ß√£o normal multivariada, quando combinadas, garantem a validade dos testes de hip√≥teses assint√≥ticos sobre os par√¢metros do modelo.
*Prova:*
I. A converg√™ncia de $Y_T\left(\sum_{t=1}^T x_t x_t'\right)^{-1} Y_T$ para uma matriz n√£o singular $Q$ e a converg√™ncia de  $Y_T \sum_{t=1}^T x_t \epsilon_t$  para uma distribui√ß√£o normal multivariada $N(0,\sigma^2 Q)$ estabelecem que:
$$ Y_T (b_T - \beta) = \left(Y_T \left(\sum_{t=1}^T x_t x_t'\right)^{-1} Y_T\right)\left(Y_T \sum_{t=1}^T x_t \epsilon_t \right) \xrightarrow{d}  N(0, \sigma^2 Q^{-1}) $$
II. Essa converg√™ncia para uma distribui√ß√£o normal possibilita utilizar as ferramentas de infer√™ncia estat√≠stica assint√≥tica, como a constru√ß√£o de testes de hip√≥teses e intervalos de confian√ßa, sobre os par√¢metros do modelo.
III. Portanto, o conhecimento do comportamento assint√≥tico da componente aleat√≥ria e da matriz de covari√¢ncia dos regressores, juntamente com as taxas de converg√™ncia dos estimadores, √© essencial para validar a infer√™ncia assint√≥tica sobre os par√¢metros do modelo de tend√™ncia temporal. $\blacksquare$

**Proposi√ß√£o 1:** A converg√™ncia em distribui√ß√£o do estimador transformado $Y_T(b_T - \beta)$ para uma normal multivariada permite construir intervalos de confian√ßa assint√≥ticos para os par√¢metros do modelo de tend√™ncia temporal.
*Prova:*
I. Pelo Teorema 1, sabemos que $Y_T(b_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1})$, onde $Q$ √© o limite da matriz $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T$.
II. Seja $\hat{\sigma}^2$ um estimador consistente para $\sigma^2$ e $\hat{Q}$ um estimador consistente para $Q$. Ent√£o, a vari√¢ncia assint√≥tica do estimador transformado $Y_T(b_T)$ √© dada por $\sigma^2 Q^{-1}$, que pode ser estimada por $\hat{\sigma}^2 \hat{Q}^{-1}$.
III. Usando a distribui√ß√£o assint√≥tica, podemos construir um intervalo de confian√ßa para os elementos de $Y_T(b_T - \beta)$, que √© dado por:
$$Y_T(b_T - \beta)_i \pm z_{\alpha/2} \sqrt{(\hat{\sigma}^2 \hat{Q}^{-1})_{ii}}$$
onde $z_{\alpha/2}$ √© o quantil da distribui√ß√£o normal padr√£o correspondente ao n√≠vel de confian√ßa desejado, e $(\hat{\sigma}^2 \hat{Q}^{-1})_{ii}$ √© o i-√©simo elemento da diagonal da matriz de vari√¢ncia-covari√¢ncia estimada.
IV.  Com esse intervalo de confian√ßa, podemos, por exemplo, testar hip√≥teses sobre os par√¢metros do modelo de tend√™ncia temporal utilizando testes estat√≠sticos assint√≥ticos. $\blacksquare$

### Conclus√£o
Este cap√≠tulo abordou os conceitos de converg√™ncia em probabilidade, converg√™ncia em distribui√ß√£o, a lei dos grandes n√∫meros (LLN) e o teorema do limite central (TLC) e suas aplica√ß√µes na an√°lise de modelos de tend√™ncia temporal determin√≠stica. A diverg√™ncia da matriz de covari√¢ncia dos regressores $\sum_{t=1}^T x_t x_t'$ e a necessidade de utilizar a t√©cnica de *rescaling* (ajuste) com a matriz $Y_T$ demonstram a import√¢ncia do uso correto desses conceitos na obten√ß√£o de distribui√ß√µes limites n√£o degeneradas [^4]. A LLN garante que as m√©dias amostrais de certas vari√°veis convergem para os seus valores esperados, enquanto o TLC garante que a componente estoc√°stica do modelo, ap√≥s a transforma√ß√£o apropriada, converge para uma distribui√ß√£o normal. A combina√ß√£o desses resultados possibilita realizar infer√™ncia estat√≠stica v√°lida sobre os par√¢metros do modelo de tend√™ncia temporal, conforme a Proposi√ß√£o 1 [^7].

### Refer√™ncias
[^1]:  [16.1]
[^2]:  [16.1], [16.1.1] - [16.1.6]
[^3]:  [16.1], [16.1.9] - [16.1.10]
[^4]:  [16.1], [16.1.16] - [16.1.17]
[^5]:  [16.1], [16.1.18] - [16.1.21]
[^7]:  [16.1], [16.1.24] - [16.1.27]
<!-- END -->
