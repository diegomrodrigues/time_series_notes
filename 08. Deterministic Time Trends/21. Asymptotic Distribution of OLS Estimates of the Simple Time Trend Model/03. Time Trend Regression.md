## A Diverg√™ncia da Matriz de Covari√¢ncia e a Necessidade de Ajuste em Modelos de Tend√™ncia Temporal

### Introdu√ß√£o
Em cap√≠tulos anteriores, examinamos a estima√ß√£o de modelos de regress√£o, com foco especial na distribui√ß√£o assint√≥tica dos estimadores de m√≠nimos quadrados ordin√°rios (OLS) para o modelo de tend√™ncia temporal linear simples. A abordagem padr√£o em regress√µes com vari√°veis estacion√°rias envolve a an√°lise da converg√™ncia de $\frac{1}{T}\sum_{t=1}^T x_t x_t'$ para uma matriz n√£o singular, e a converg√™ncia de $\frac{1}{\sqrt{T}}\sum_{t=1}^T x_t \epsilon_t$ para uma vari√°vel aleat√≥ria normal, $N(0,\sigma^2 Q)$ [^2]. No entanto, como vimos, essa suposi√ß√£o n√£o √© v√°lida para modelos com tend√™ncias temporais determin√≠sticas, o que nos obriga a buscar t√©cnicas alternativas para a an√°lise assint√≥tica [^1]. Este cap√≠tulo detalha a raz√£o pela qual a abordagem usual falha e introduz a necessidade de um ajuste nas taxas de converg√™ncia e na forma da matriz de covari√¢ncia do estimador OLS. O entendimento dessas diverg√™ncias √© fundamental para a an√°lise de processos n√£o estacion√°rios e para o desenvolvimento de testes de hip√≥teses v√°lidos nesses modelos [^7].

### An√°lise da Diverg√™ncia da Matriz de Covari√¢ncia
Como j√° vimos, no contexto de modelos de regress√£o com vari√°veis explicativas estacion√°rias, √© comum assumir que a matriz $\frac{1}{T}\sum_{t=1}^T x_t x_t'$ converge em probabilidade para uma matriz n√£o singular $Q$, e que $\frac{1}{\sqrt{T}}\sum_{t=1}^T x_t \epsilon_t$ converge em distribui√ß√£o para uma vari√°vel aleat√≥ria normal, $N(0, \sigma^2 Q)$ [^2]. Essa suposi√ß√£o √© fundamental para derivar as propriedades assint√≥ticas dos estimadores OLS e para realizar testes de hip√≥teses. Contudo, quando o modelo de regress√£o envolve uma tend√™ncia temporal determin√≠stica, essa suposi√ß√£o n√£o se sustenta e a matriz $\frac{1}{T}\sum_{t=1}^T x_t x_t'$ diverge.

O modelo de tend√™ncia temporal linear simples √© dado por:
$$ y_t = \alpha + \delta t + \epsilon_t $$ [^1] [16.1.1]
com $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$ [^2] [16.1.3] e $\beta = \begin{bmatrix} \alpha \\ \delta \end{bmatrix}$ [^2] [16.1.4]. O estimador OLS de $\beta$ √© dado por:

$$ b_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} = \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} \sum_{t=1}^{T} x_t y_t $$ [^2] [16.1.5]
A matriz $\sum_{t=1}^T x_t x_t'$ √© expressa como:

$$ \sum_{t=1}^T x_t x_t' = \sum_{t=1}^T \begin{bmatrix} 1 & t \\ t & t^2 \end{bmatrix} = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix} = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} $$ [^4] [16.1.16]

O comportamento assint√≥tico da matriz $\sum_{t=1}^T x_t x_t'$ √© determinado pelos seus elementos. Sabemos que [^3]:

$$ \sum_{t=1}^T 1 = T $$ [16.1.9]
$$ \sum_{t=1}^T t = \frac{T(T+1)}{2} $$ [16.1.9]
$$ \sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6} $$ [16.1.10]

Portanto, os termos dominantes s√£o de ordem $T$, $T^2$ e $T^3$, respectivamente. Isso significa que ao dividir $\sum_{t=1}^T x_t x_t'$ por $T$, obtemos:

$$ \frac{1}{T} \sum_{t=1}^T x_t x_t' = \begin{bmatrix} 1 & \frac{T+1}{2} \\ \frac{T+1}{2} & \frac{(T+1)(2T+1)}{6} \end{bmatrix} $$
que diverge quando $T \rightarrow \infty$. Em vez de convergir para uma matriz n√£o singular, os elementos dessa matriz tendem ao infinito.

> üí° **Exemplo Num√©rico:** Para ilustrar a diverg√™ncia, vamos calcular $\frac{1}{T}\sum_{t=1}^T x_t x_t'$ para diferentes valores de $T$:
>
> Para $T = 10$:
> $$ \frac{1}{10} \sum_{t=1}^{10} x_t x_t' = \begin{bmatrix} 1 & \frac{11}{2} \\ \frac{11}{2} & \frac{11 \cdot 21}{6} \end{bmatrix} = \begin{bmatrix} 1 & 5.5 \\ 5.5 & 38.5 \end{bmatrix} $$
>
> Para $T = 100$:
> $$ \frac{1}{100} \sum_{t=1}^{100} x_t x_t' = \begin{bmatrix} 1 & \frac{101}{2} \\ \frac{101}{2} & \frac{101 \cdot 201}{6} \end{bmatrix} = \begin{bmatrix} 1 & 50.5 \\ 50.5 & 3383.5 \end{bmatrix} $$
>
> Para $T = 1000$:
> $$ \frac{1}{1000} \sum_{t=1}^{1000} x_t x_t' = \begin{bmatrix} 1 & \frac{1001}{2} \\ \frac{1001}{2} & \frac{1001 \cdot 2001}{6} \end{bmatrix} = \begin{bmatrix} 1 & 500.5 \\ 500.5 & 333833.5 \end{bmatrix} $$
>
> Observa-se que os elementos fora da diagonal e o elemento inferior direito aumentam com $T$, demonstrando a diverg√™ncia.

Para obter uma matriz convergente, a abordagem usual de dividir por $T$ n√£o √© suficiente. A matriz $\sum_{t=1}^T x_t x_t'$ teria que ser dividida por $T^3$, para que os termos tenham ordem $T^0$ quando $T \rightarrow \infty$, resultando em:
$$ \frac{1}{T^3} \sum_{t=1}^T x_t x_t' = \begin{bmatrix} 1/T^2 & \frac{1}{2T} + \frac{1}{2T^2} \\ \frac{1}{2T} + \frac{1}{2T^2} & \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2} \end{bmatrix} $$
No entanto, quando $T \to \infty$, esta matriz converge para
$$ \lim_{T \to \infty} \frac{1}{T^3} \sum_{t=1}^T x_t x_t' = \begin{bmatrix} 0 & 0 \\ 0 & 1/3 \end{bmatrix} $$
que √© uma matriz singular e n√£o invert√≠vel, tornando a abordagem usual de matrizes inversas invi√°vel para a an√°lise da distribui√ß√£o assint√≥tica dos estimadores.

**Observa√ß√£o 1:** O comportamento da matriz $\sum_{t=1}^T x_t x_t'$ destaca uma das principais dificuldades na an√°lise de modelos com tend√™ncias temporais. A diverg√™ncia da matriz impede o uso direto das t√©cnicas de regress√£o tradicionais, que dependem da converg√™ncia da matriz de covari√¢ncia dos regressores.

**Proposi√ß√£o 1:** Para obter uma matriz que convirja para uma matriz n√£o singular, a matriz $\sum_{t=1}^T x_t x_t'$ precisa ser dividida por $T^3$, o que torna a matriz limite n√£o invert√≠vel, impossibilitando a an√°lise tradicional da distribui√ß√£o assint√≥tica dos estimadores OLS.
*Prova:*
I. A matriz $\sum_{t=1}^T x_t x_t'$ √© dada por:
$$ \sum_{t=1}^T x_t x_t' = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} $$
II. Dividindo por $T$, a matriz resultante n√£o converge, pois os elementos fora da diagonal e o elemento inferior direito crescem indefinidamente com $T$:
$$ \frac{1}{T} \sum_{t=1}^T x_t x_t' = \begin{bmatrix} 1 & \frac{T+1}{2} \\ \frac{T+1}{2} & \frac{(T+1)(2T+1)}{6} \end{bmatrix} $$
III. Dividindo por $T^2$, a matriz ainda n√£o converge, pois os elementos fora da diagonal e o elemento inferior direito tamb√©m crescem indefinidamente com $T$:
$$ \frac{1}{T^2} \sum_{t=1}^T x_t x_t' = \begin{bmatrix} 1/T & \frac{1}{2} + \frac{1}{2T} \\ \frac{1}{2} + \frac{1}{2T} & \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2} \end{bmatrix} $$
IV. Dividindo por $T^3$, a matriz converge para uma matriz singular, j√° que o elemento superior esquerdo converge para zero:
$$ \frac{1}{T^3} \sum_{t=1}^T x_t x_t' = \begin{bmatrix} 1/T^2 & \frac{1}{2T} + \frac{1}{2T^2} \\ \frac{1}{2T} + \frac{1}{2T^2} & \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2} \end{bmatrix} \rightarrow \begin{bmatrix} 0 & 0 \\ 0 & 1/3 \end{bmatrix} $$
V. Como a matriz limite obtida √© singular e n√£o invert√≠vel, a an√°lise assint√≥tica padr√£o com a inversa da matriz de covari√¢ncia n√£o √© aplic√°vel nesse cen√°rio. $\blacksquare$

> üí° **Exemplo Num√©rico:** Demonstra√ß√£o da converg√™ncia para matriz singular ao dividir por T¬≥.
> Para T = 100,
> $$ \frac{1}{100^3} \sum_{t=1}^{100} x_t x_t' = \begin{bmatrix} 1/100^2 & \frac{1}{2*100} + \frac{1}{2*100^2} \\ \frac{1}{2*100} + \frac{1}{2*100^2} & \frac{1}{3} + \frac{1}{2*100} + \frac{1}{6*100^2} \end{bmatrix} = \begin{bmatrix} 0.0001 & 0.00505 \\ 0.00505 & 0.338333 \end{bmatrix} $$
>
> Para T = 1000,
> $$\frac{1}{1000^3} \sum_{t=1}^{1000} x_t x_t' = \begin{bmatrix} 1/1000^2 & \frac{1}{2*1000} + \frac{1}{2*1000^2} \\ \frac{1}{2*1000} + \frac{1}{2*1000^2} & \frac{1}{3} + \frac{1}{2*1000} + \frac{1}{6*1000^2} \end{bmatrix} = \begin{bmatrix} 0.000001 & 0.0005005 \\ 0.0005005 & 0.333833 \end{bmatrix} $$
>
> Como podemos notar, √† medida que T aumenta, os elementos da matriz tendem a 0, 0 e 1/3, demonstrando a converg√™ncia para uma matriz singular.

**Lema 1:** A matriz $\sum_{t=1}^T x_t x_t'$ pode ser decomposta em termos de suas taxas de crescimento, sendo cada elemento uma fun√ß√£o polinomial em T.
*Prova:*
I. A matriz $\sum_{t=1}^T x_t x_t'$ √© expressa como:
$$\sum_{t=1}^T x_t x_t' = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix}$$
II. Os somat√≥rios podem ser reescritos utilizando as f√≥rmulas conhecidas:
$$ \sum_{t=1}^T 1 = T $$
$$ \sum_{t=1}^T t = \frac{T(T+1)}{2} = \frac{T^2}{2} + \frac{T}{2} $$
$$ \sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6} = \frac{2T^3+3T^2+T}{6} = \frac{T^3}{3} + \frac{T^2}{2} + \frac{T}{6} $$
III. Portanto, a matriz pode ser escrita como:
$$ \sum_{t=1}^T x_t x_t' = \begin{bmatrix} T & \frac{T^2}{2} + \frac{T}{2} \\ \frac{T^2}{2} + \frac{T}{2} & \frac{T^3}{3} + \frac{T^2}{2} + \frac{T}{6} \end{bmatrix} $$
IV. Cada elemento da matriz √© um polin√¥mio em $T$, com grau correspondente √† taxa de crescimento. $\blacksquare$

### A Necessidade de Ajuste e Rescaling
Dada a diverg√™ncia da matriz $\frac{1}{T}\sum_{t=1}^T x_t x_t'$ e a n√£o invertibilidade da matriz limite obtida ao dividir por $T^3$, √© necess√°rio adotar uma abordagem diferente para analisar as propriedades assint√≥ticas dos estimadores OLS. A solu√ß√£o, como j√° discutido no contexto anterior, envolve um ajuste ou *rescaling* das vari√°veis por meio da matriz $Y_T$, dada por:
$$ Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} $$ [^4] [16.1.17]
Essa matriz √© utilizada para pr√©-multiplicar o desvio do estimador OLS, permitindo a obten√ß√£o de uma distribui√ß√£o limite n√£o degenerada, pois ajusta as diferentes taxas de converg√™ncia dos estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$ [^4].

A raz√£o para essa abordagem reside no fato de que, enquanto $\hat{\alpha}_T$ converge para $\alpha$ a uma taxa de $\sqrt{T}$, $\hat{\delta}_T$ converge para $\delta$ a uma taxa mais r√°pida, de $T^{3/2}$.  Ao aplicar a matriz $Y_T$, obtemos:
$$ Y_T (b_T - \beta) = \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} $$
que converge para uma distribui√ß√£o normal multivariada n√£o singular [^7]. Esta abordagem garante que os componentes da matriz de covari√¢ncia do estimador OLS tenham ordem de grandeza apropriada e permite a realiza√ß√£o de infer√™ncia estat√≠stica v√°lida.

> üí° **Exemplo Num√©rico:** Suponha que temos um modelo de tend√™ncia temporal linear, com $\alpha = 2$ e $\delta = 0.5$. Geramos dados com $T = 100$, e os estimadores OLS obtidos s√£o $\hat{\alpha}_{100} = 2.1$ e $\hat{\delta}_{100} = 0.505$. Agora, vamos aplicar a matriz $Y_T$:
>
> $$ Y_{100} (b_{100} - \beta) = \begin{bmatrix} \sqrt{100}(2.1 - 2) \\ 100^{3/2}(0.505 - 0.5) \end{bmatrix} = \begin{bmatrix} 10(0.1) \\ 1000(0.005) \end{bmatrix} = \begin{bmatrix} 1 \\ 5 \end{bmatrix} $$
>
> Observe que sem o rescaling, o erro estimado em $\hat{\delta}$ √© muito pequeno, em compara√ß√£o com $\hat{\alpha}$. Ao utilizar o rescaling, a escala dos erros em rela√ß√£o aos seus par√¢metros de converg√™ncia fica compat√≠vel.

**Teorema 1:** A matriz $Y_T$ √© uma matriz de rescaling apropriada para obter converg√™ncia na distribui√ß√£o do estimador OLS.
*Prova:*
I. A matriz $Y_T$ tem como objetivo ajustar as diferentes taxas de converg√™ncia dos estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$.
II. Multiplicando o desvio do estimador OLS $(b_T - \beta)$ por $Y_T$, obtemos:
$$ Y_T (b_T - \beta) = \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} $$
III. Conforme a teoria assint√≥tica para modelos de tend√™ncia temporal, $\sqrt{T}(\hat{\alpha}_T - \alpha)$ converge em distribui√ß√£o para uma normal com m√©dia zero e vari√¢ncia finita. Similarmente, $T^{3/2}(\hat{\delta}_T - \delta)$ tamb√©m converge em distribui√ß√£o para uma normal com m√©dia zero e vari√¢ncia finita.
IV. Portanto, $Y_T(b_T-\beta)$ converge em distribui√ß√£o para uma normal multivariada com m√©dia zero e uma matriz de covari√¢ncia n√£o singular, possibilitando a infer√™ncia estat√≠stica. $\blacksquare$

### Conclus√£o
Este cap√≠tulo detalhou a falha da abordagem padr√£o para an√°lise assint√≥tica da matriz $\frac{1}{T}\sum_{t=1}^T x_t x_t'$ em modelos de regress√£o com tend√™ncias temporais determin√≠sticas. A diverg√™ncia desta matriz e a necessidade de dividi-la por $T^3$ para obter converg√™ncia (resultando em uma matriz limite singular e n√£o invert√≠vel) destaca a necessidade de um ajuste nas taxas de converg√™ncia dos estimadores por meio da matriz $Y_T$ [^4]. Esse ajuste, ao multiplicar $\hat{\alpha}_T$ por $\sqrt{T}$ e $\hat{\delta}_T$ por $T^{3/2}$, garante que as distribui√ß√µes limites sejam n√£o degeneradas e que a infer√™ncia estat√≠stica possa ser conduzida de forma apropriada [^7]. A necessidade dessa corre√ß√£o e *rescaling* √© um conceito central para a an√°lise assint√≥tica de modelos com tend√™ncias temporais determin√≠sticas e estabelecer√° a base para o estudo de modelos mais complexos nos cap√≠tulos seguintes [^1].

> üí° **Exemplo Num√©rico:** Demonstra√ß√£o da diverg√™ncia e necessidade de ajuste da matriz de covari√¢ncia.
> ```python
> import numpy as np
>
> def calculate_matrix_components(T):
>   """Calcula os componentes da matriz de covari√¢ncia para um modelo com tend√™ncia linear."""
>   sum_t = T * (T + 1) / 2
>   sum_t_squared = T * (T + 1) * (2 * T + 1) / 6
>   return T, sum_t, sum_t_squared
>
> # Par√¢metros
> sample_sizes = [10, 100, 1000, 10000]
>
> for T in sample_sizes:
>     T, sum_t, sum_t_squared = calculate_matrix_components(T)
>
>     covariance_matrix = np.array([[T, sum_t], [sum_t, sum_t_squared]])
>     print(f"Matriz de Covari√¢ncia para T = {T}:\n {covariance_matrix}")
>
>     scaled_covariance_matrix_T = covariance_matrix / T
>     print(f"Matriz de Covari√¢ncia Dividida por T para T = {T}:\n {scaled_covariance_matrix_T}")
>
>     scaled_covariance_matrix_T3 = covariance_matrix / T**3
>     print(f"Matriz de Covari√¢ncia Dividida por T¬≥ para T = {T}:\n {scaled_covariance_matrix_T3}")
>     print("-------------------")
> ```
> Este c√≥digo calcula e imprime a matriz de covari√¢ncia $\sum_{t=1}^T x_t x_t'$, dividida por $T$ e $T^3$ para diferentes valores de $T$. Observa-se que a matriz dividida por $T$ diverge quando $T$ cresce, enquanto a matriz dividida por $T^3$ converge para uma matriz singular, demonstrando a necessidade do ajuste por meio da matriz $Y_T$ .

**Corol√°rio 1:** A diverg√™ncia da matriz $\frac{1}{T}\sum_{t=1}^T x_t x_t'$ implica que a abordagem padr√£o da an√°lise assint√≥tica n√£o pode ser aplicada diretamente. A necessidade de dividir por $T^3$ para obter converg√™ncia, e a singularidade da matriz limite, demonstram a import√¢ncia de ajustar as taxas de converg√™ncia dos estimadores atrav√©s da matriz $Y_T$.
*Prova:*
I. A Proposi√ß√£o 1 estabeleceu que a matriz $\frac{1}{T}\sum_{t=1}^T x_t x_t'$ diverge, n√£o convergindo para uma matriz n√£o singular como em regress√µes com vari√°veis estacion√°rias.
II. Dividindo $\sum_{t=1}^T x_t x_t'$ por $T^3$ obt√©m-se uma matriz convergente, cuja matriz limite √© singular.
III. Essa singularidade impede a aplica√ß√£o direta da an√°lise assint√≥tica tradicional para modelos de regress√£o, uma vez que a invers√£o da matriz limite n√£o √© poss√≠vel.
IV. O uso da matriz $Y_T$ para ajustar as taxas de converg√™ncia permite contornar a singularidade da matriz limite.
V. Ao multiplicar $\hat{\alpha}_T$ por $\sqrt{T}$ e $\hat{\delta}_T$ por $T^{3/2}$, obtemos uma matriz de covari√¢ncia de ordem apropriada e que converge para uma matriz n√£o singular, possibilitando a an√°lise assint√≥tica. $\blacksquare$

**Corol√°rio 1.1:** A matriz $Y_T$ define uma forma de "normaliza√ß√£o" dos estimadores, garantindo que suas vari√¢ncias assint√≥ticas n√£o colapsem para zero ou divirjam para o infinito.
*Prova:*
I. Do Teorema 1, sabe-se que $Y_T(b_T-\beta)$ converge para uma distribui√ß√£o normal multivariada n√£o singular.
II. Isso implica que a matriz de covari√¢ncia assint√≥tica de $Y_T(b_T-\beta)$ √© bem definida, sendo finita e positiva definida.
III. Se n√£o utilizarmos $Y_T$, a vari√¢ncia de $\hat{\alpha}_T$ e $\hat{\delta}_T$ iriam para zero quando $T \to \infty$, pois $\hat{\alpha}_T$ converge a uma taxa $\sqrt{T}$ e $\hat{\delta}_T$ a uma taxa $T^{3/2}$.
IV. Portanto, $Y_T$ ajusta as escalas dos estimadores de forma que suas vari√¢ncias assint√≥ticas permane√ßam est√°veis. $\blacksquare$


### Refer√™ncias
[^1]:  [16.1]
[^2]:  [16.1], [16.1.1] - [16.1.6]
[^3]:  [16.1], [16.1.9] - [16.1.10]
[^4]:  [16.1], [16.1.16] - [16.1.17]
[^7]:  [16.1], [16.1.25] - [16.1.27]
<!-- END -->
