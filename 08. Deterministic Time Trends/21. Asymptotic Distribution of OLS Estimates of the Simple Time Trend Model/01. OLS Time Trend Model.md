## Distribui√ß√£o Assint√≥tica de Estimativas OLS para um Modelo de Tend√™ncia Temporal Simples

### Introdu√ß√£o
Este cap√≠tulo introduz o conceito de diferentes taxas de converg√™ncia assint√≥tica para estimadores em modelos de regress√£o com tend√™ncias temporais determin√≠sticas. Conforme mencionado anteriormente, a estima√ß√£o por m√≠nimos quadrados ordin√°rios (OLS) em modelos com ra√≠zes unit√°rias ou tend√™ncias temporais determin√≠sticas requer uma an√°lise cuidadosa, pois as distribui√ß√µes assint√≥ticas dos estimadores de coeficientes n√£o podem ser calculadas da mesma forma que em modelos com vari√°veis estacion√°rias [^1]. Este cap√≠tulo, construindo sobre os fundamentos da regress√£o apresentados no cap√≠tulo 8, aborda especificamente o caso de processos envolvendo tend√™ncias temporais determin√≠sticas, mas sem ra√≠zes unit√°rias, e estabelece uma base para o tratamento de processos mais complexos com ra√≠zes unit√°rias nos cap√≠tulos subsequentes. Em particular, analisamos o modelo de tend√™ncia temporal linear simples e determinamos que as estat√≠sticas t e F usuais, calculadas da forma padr√£o, possuem as mesmas distribui√ß√µes assint√≥ticas que em regress√µes estacion√°rias [^1]. A t√©cnica introduzida aqui n√£o s√≥ √© aplic√°vel ao estudo de tend√™ncias temporais, mas tamb√©m √© crucial para a an√°lise de estimadores de processos n√£o estacion√°rios que ser√£o discutidos nos cap√≠tulos 17 e 18.

### Conceitos Fundamentais
Come√ßamos com o modelo de tend√™ncia temporal linear simples, dado por:
$$ y_t = \alpha + \delta t + \epsilon_t $$ [^1]  [16.1.1]
onde $\epsilon_t$ √© um processo de ru√≠do branco. Assumindo que $\epsilon_t \sim N(0, \sigma^2)$, o modelo [16.1.1] satisfaz as suposi√ß√µes cl√°ssicas de regress√£o e as estat√≠sticas t ou F padr√£o s√£o aplic√°veis [^1].  A quest√£o central √© que, se $\epsilon_t$ n√£o for Gaussiano, as distribui√ß√µes assint√≥ticas dos estimadores OLS de $\alpha$ e $\delta$ requerem uma t√©cnica ligeiramente diferente daquela empregada para regress√µes estacion√°rias no cap√≠tulo 8 [^2]. O modelo [16.1.1] pode ser escrito na forma de regress√£o padr√£o como:
$$ y_t = x_t'\beta + \epsilon_t $$ [^2] [16.1.2]
onde
$$ x_t = \begin{bmatrix} 1 \\ t \end{bmatrix} $$ [^2] [16.1.3]
e
$$ \beta = \begin{bmatrix} \alpha \\ \delta \end{bmatrix} $$ [^2] [16.1.4]

Seja $b_T$ o estimador OLS de $\beta$ baseado em uma amostra de tamanho T:

$$ b_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} = \left(\sum_{t=1}^{T} x_t x_t'\right)^{-1} \sum_{t=1}^{T} x_t y_t $$ [^2] [16.1.5]
A partir da equa√ß√£o [8.2.3], o desvio do estimador OLS de seu valor verdadeiro pode ser expresso como
$$ (b_T - \beta) = \left(\sum_{t=1}^{T} x_t x_t'\right)^{-1} \sum_{t=1}^{T} x_t \epsilon_t $$ [^2] [16.1.6]
Para encontrar a distribui√ß√£o limite para uma regress√£o com vari√°veis explicativas estacion√°rias, o procedimento no Cap√≠tulo 8 envolveu multiplicar [16.1.6] por $\sqrt{T}$, resultando em:

$$ \sqrt{T}(b_T - \beta) = \left(\frac{1}{T} \sum_{t=1}^{T} x_t x_t'\right)^{-1} \left(\frac{1}{\sqrt{T}} \sum_{t=1}^{T} x_t \epsilon_t\right) $$ [^2] [16.1.7]

A suposi√ß√£o usual era que $\frac{1}{T} \sum_{t=1}^{T} x_t x_t'$ convergia em probabilidade para uma matriz n√£o singular $Q$, enquanto $\frac{1}{\sqrt{T}} \sum_{t=1}^{T} x_t \epsilon_t$ convergia em distribui√ß√£o para uma vari√°vel aleat√≥ria $N(0, \sigma^2 Q)$, implicando que $\sqrt{T}(b_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1})$ [^2]. Para entender porque esse argumento n√£o pode ser usado para uma tend√™ncia temporal determin√≠stica, notamos que, para $x_t$ e $\beta$ dados em [16.1.3] e [16.1.4], a express√£o [16.1.6] se torna
$$ \begin{bmatrix} \hat{\alpha}_T - \alpha \\ \hat{\delta}_T - \delta \end{bmatrix} = \begin{bmatrix} \sum_{t=1}^{T} 1 & \sum_{t=1}^{T} t \\ \sum_{t=1}^{T} t & \sum_{t=1}^{T} t^2 \end{bmatrix}^{-1} \begin{bmatrix} \sum_{t=1}^{T} \epsilon_t \\ \sum_{t=1}^{T} t \epsilon_t \end{bmatrix} $$ [^2] [16.1.8]
√â direto mostrar, por indu√ß√£o, que
$$ \sum_{t=1}^{T} t = T(T+1)/2 $$ [^3] [16.1.9]
e
$$ \sum_{t=1}^{T} t^2 = T(T+1)(2T+1)/6 $$ [^3] [16.1.10]
Assim, o termo principal em $\sum_{t=1}^{T} t$ √© $T^2/2$, ou seja,
$$ \frac{1}{T^2} \sum_{t=1}^{T} t = \frac{1}{T^2} \left[\frac{T^2}{2} + \frac{T}{2}\right] = \frac{1}{2} + \frac{1}{2T} \rightarrow \frac{1}{2} $$ [^3] [16.1.11]
Similarmente, o termo principal em $\sum_{t=1}^{T} t^2$ √© $T^3/3$:
$$ \frac{1}{T^3} \sum_{t=1}^{T} t^2 = \frac{1}{T^3} \left[\frac{2T^3}{6} + \frac{3T^2}{6} + \frac{T}{6} \right] = \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2} \rightarrow \frac{1}{3} $$ [^3] [16.1.12]
Para refer√™ncia futura, notamos que o padr√£o geral - o termo principal em $\sum_{t=1}^{T} t^v$ √© $T^{v+1}/(v+1)$:
$$ \frac{1}{T^{v+1}} \sum_{t=1}^{T} t^v \rightarrow \frac{1}{v+1} $$ [^3] [16.1.13]
Para verificar [16.1.13], note que
$$ \frac{1}{T^{v+1}} \sum_{t=1}^{T} t^v = \frac{1}{T} \sum_{t=1}^{T} \left(\frac{t}{T}\right)^v $$ [^3] [16.1.14]
O lado direito de [16.1.14] pode ser visto como uma aproxima√ß√£o da √°rea sob a curva $f(r) = r^v$ para $r$ entre zero e um.
A matriz $\sum_{t=1}^{T} x_t x_t'$ torna-se:

$$ \sum_{t=1}^{T} x_t x_t' = \begin{bmatrix} \sum_{t=1}^{T} 1 & \sum_{t=1}^{T} t \\ \sum_{t=1}^{T} t & \sum_{t=1}^{T} t^2 \end{bmatrix} = \begin{bmatrix} T & T(T+1)/2 \\ T(T+1)/2 & T(T+1)(2T+1)/6 \end{bmatrix} $$ [^4] [16.1.16]
Em contraste com o resultado usual para regress√µes estacion√°rias, para a matriz em [16.1.16], $\frac{1}{T} \sum_{t=1}^{T} x_t x_t'$ diverge. Para obter uma matriz convergente, [16.1.16] teria que ser dividida por $T^3$ ao inv√©s de $T$. No entanto, essa matriz limite n√£o pode ser invertida [^4].
Os estimadores OLS $\hat{\alpha}_T$ e $\hat{\delta}_T$ possuem diferentes taxas de converg√™ncia assint√≥tica. Para obter distribui√ß√µes limites n√£o degeneradas, $\hat{\alpha}_T$ √© multiplicado por $\sqrt{T}$, enquanto $\hat{\delta}_T$ deve ser multiplicado por $T^{3/2}$ [^4]. Essa corre√ß√£o pode ser pensada como a pr√©-multiplica√ß√£o de [16.1.6] ou [16.1.8] pela matriz:

$$ Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} $$ [^4] [16.1.17]

resultando em
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} = Y_T \left(\sum_{t=1}^{T} x_t x_t'\right)^{-1} Y_T^{-1} Y_T \sum_{t=1}^{T} x_t \epsilon_t $$ [^5] [16.1.18]
Considerando o primeiro termo da √∫ltima express√£o em [16.1.18], e substituindo [16.1.17] e [16.1.16]:
$$ Y_T \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} Y_T = \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} \begin{bmatrix} \sum_{t=1}^{T} 1 & \sum_{t=1}^{T} t \\ \sum_{t=1}^{T} t & \sum_{t=1}^{T} t^2 \end{bmatrix}^{-1} \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} \rightarrow Q $$ [^5] [16.1.19]
onde
$$ Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} $$ [^5] [16.1.20]
Passando para o segundo termo em [16.1.18], temos
$$ Y_T \sum_{t=1}^{T} x_t \epsilon_t = \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} \sum_{t=1}^{T} \epsilon_t \\ \sum_{t=1}^{T} t \epsilon_t \end{bmatrix} = \begin{bmatrix} T^{-1/2} \sum_{t=1}^{T} \epsilon_t \\ T^{-1} \sum_{t=1}^{T} (t/T) \epsilon_t \end{bmatrix} $$ [^5] [16.1.21]

Sob as suposi√ß√µes padr√£o sobre $\epsilon_t$, este vetor ser√° assintoticamente Gaussiano. Por exemplo, suponha que $\epsilon_t$ seja i.i.d. com m√©dia zero, vari√¢ncia $\sigma^2$ e um quarto momento finito. Ent√£o o primeiro elemento do vetor em [16.1.21] satisfaz:
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^{T} \epsilon_t \xrightarrow{d} N(0, \sigma^2) $$ [^5]
pelo teorema do limite central. Para o segundo elemento do vetor em [16.1.21], observe que $\{\frac{t}{T}\epsilon_t\}$ √© uma sequ√™ncia de diferen√ßas de martingala que satisfaz as condi√ß√µes da Proposi√ß√£o 7.8. Especificamente, sua vari√¢ncia √©
$$ \sigma^2_T = E\left[ \left( \frac{t}{T} \epsilon_t \right)^2 \right] = \sigma^2 \frac{t^2}{T^2} $$ [^5]
onde
$$ \frac{1}{T} \sum_{t=1}^{T} \sigma^2_t = \sigma^2 \frac{1}{T} \sum_{t=1}^{T} \frac{t^2}{T^2} \rightarrow \sigma^2 \frac{1}{3} $$ [^5]
Al√©m disso, $\frac{1}{T}\sum_{t=1}^{T} [(t/T)\epsilon_t]^2 \xrightarrow{p} \sigma^2/3$.
Portanto,
$$ \left( \frac{1}{T} \sum_{t=1}^{T} \left( \frac{t}{T} \epsilon_t \right)^2 \right) - \left( \frac{1}{T} \sum_{t=1}^{T} \sigma^2 \frac{t^2}{T^2} \right) $$ [^6]
converge para zero.
Isso implica que
$$ \frac{1}{T} \sum_{t=1}^{T} \left[ \frac{t}{T} \epsilon_t \right]^2 \xrightarrow{p} \frac{\sigma^2}{3} $$ [^6]
Como consequ√™ncia, pela Proposi√ß√£o 7.8, $\frac{1}{\sqrt{T}} \sum_{t=1}^{T} \frac{t}{T} \epsilon_t$ satisfaz o teorema do limite central:
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^{T} \frac{t}{T} \epsilon_t \xrightarrow{d} N(0, \sigma^2/3) $$ [^6]

Finalmente, considere a distribui√ß√£o conjunta dos dois elementos do vetor (2 x 1) descrito por [16.1.21]. Qualquer combina√ß√£o linear desses elementos assume a forma:
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^{T} \left[\lambda_1 + \lambda_2 \frac{t}{T}\right] \epsilon_t $$ [^6]
Ent√£o, $[\lambda_1 + \lambda_2 (t/T)]\epsilon_t$ √© tamb√©m uma sequ√™ncia de diferen√ßas de martingala com vari√¢ncia positiva, dada por $\sigma^2[\lambda_1^2 + 2\lambda_1 \lambda_2 (t/T) + \lambda_2^2 (t/T)^2]$, satisfazendo:
$$ \frac{1}{T} \sum_{t=1}^{T} \sigma^2 \left[ \lambda_1^2 + 2\lambda_1 \lambda_2 \frac{t}{T} + \lambda_2^2 \frac{t^2}{T^2} \right] \rightarrow \sigma^2 \left[ \lambda_1^2 + 2\lambda_1 \lambda_2 \frac{1}{2} + \lambda_2^2 \frac{1}{3} \right] = \sigma^2 \lambda' Q \lambda $$ [^6] [16.1.23]
onde $\lambda = (\lambda_1, \lambda_2)'$ e $Q$ √© a matriz em [16.1.20]. Al√©m disso,
$$ \frac{1}{T} \sum_{t=1}^{T} \left[\lambda_1 + \lambda_2 (t/T)\right]\epsilon_t^2  \xrightarrow{p} \sigma^2 \lambda' Q \lambda $$
Assim, qualquer combina√ß√£o linear dos dois elementos no vetor em [16.1.21] √© assintoticamente Gaussiana, implicando uma distribui√ß√£o Gaussiana bivariada limitante [^6]. Combinando esses resultados, obt√©m-se:
$$ \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^{T} \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^{T} \frac{t}{T} \epsilon_t \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q) $$ [^7] [16.1.24]

A partir de [16.1.19] e [16.1.24], a distribui√ß√£o assint√≥tica de [16.1.18] pode ser calculada como no Exemplo 7.5 do Cap√≠tulo 7:
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 [Q^{-1}QQ^{-1}] ) = N(0, \sigma^2 Q^{-1}) $$ [^7] [16.1.25]

### Conclus√£o
Em resumo, os resultados obtidos mostram que a distribui√ß√£o assint√≥tica dos estimadores de um modelo de tend√™ncia temporal linear simples pode ser derivada utilizando uma abordagem que considera as diferentes taxas de converg√™ncia dos estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$. O estimador $\hat{\alpha}_T$ converge para o valor verdadeiro a uma taxa de $\sqrt{T}$, enquanto $\hat{\delta}_T$ converge a uma taxa de $T^{3/2}$. Essa an√°lise demonstra que, apesar das diferentes taxas de converg√™ncia, as estat√≠sticas t e F padr√£o podem ser usadas para testes de hip√≥tese assintoticamente v√°lidos [^7]. Em outras palavras, a estat√≠stica t usual para um teste de hip√≥teses sobre $\alpha$ e $\delta$ t√™m uma distribui√ß√£o limite normal padr√£o.
Esses resultados s√£o resumidos na Proposi√ß√£o 16.1 [^7]:
*Seja $y_t$ gerado de acordo com a tend√™ncia de tempo determin√≠stica simples [16.1.1], onde $\epsilon_t$ √© i.i.d. com $E(\epsilon_t^2) = \sigma^2$ e $E(\epsilon_t^4) < \infty$. Ent√£o:*
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}) $$ [^7] [16.1.26]
√â crucial notar que o estimador do coeficiente da tend√™ncia temporal ($\hat{\delta}_T$) √© superconsistente ‚Äì n√£o s√≥ $\hat{\delta}_T \xrightarrow{p} \delta$, mas mesmo quando multiplicado por $T$, ainda temos
$$ T(\hat{\delta}_T - \delta) \xrightarrow{p} 0 $$ [^7] [16.1.27]
Essa superconsist√™ncia destaca uma diferen√ßa fundamental na an√°lise de modelos com tend√™ncias temporais determin√≠sticas e modelos com vari√°veis estacion√°rias. A metodologia apresentada aqui fornece uma base s√≥lida para o estudo de modelos mais complexos que ser√£o abordados nos cap√≠tulos subsequentes [^2].

> üí° **Exemplo Num√©rico:** Vamos gerar uma s√©rie temporal com uma tend√™ncia linear e ru√≠do branco para ilustrar os conceitos.
```python
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
from scipy.stats import norm

# Par√¢metros do modelo
alpha = 5
delta = 0.2
sigma = 1
T = 100

# Gerar dados
t = np.arange(1, T + 1)
epsilon = np.random.normal(0, sigma, T)
y = alpha + delta * t + epsilon

# Criar dataframe para visualiza√ß√£o
data = pd.DataFrame({'t': t, 'y': y, 'epsilon': epsilon})

# Visualizar dados
plt.figure(figsize=(10,5))
plt.plot(data['t'],data['y'], label = 'S√©rie Temporal y_t')
plt.plot(data['t'], alpha + delta * t, label = 'Tend√™ncia Linear')
plt.xlabel('Tempo (t)')
plt.ylabel('Valor de y_t')
plt.title('S√©rie Temporal com Tend√™ncia Linear')
plt.legend()
plt.show()

# Estima√ß√£o OLS
X = sm.add_constant(t)
model = sm.OLS(y, X)
results = model.fit()
alpha_hat, delta_hat = results.params
se_alpha_hat, se_delta_hat  = results.bse
print(f"Estimativa de alpha: {alpha_hat:.4f}, Desvio Padr√£o: {se_alpha_hat:.4f}")
print(f"Estimativa de delta: {delta_hat:.4f}, Desvio Padr√£o: {se_delta_hat:.4f}")

# C√°lculo das estat√≠sticas t
t_stat_alpha = (alpha_hat - alpha) / se_alpha_hat
t_stat_delta = (delta_hat - delta) / se_delta_hat
p_value_alpha = 2 * (1 - norm.cdf(abs(t_stat_alpha)))
p_value_delta = 2 * (1 - norm.cdf(abs(t_stat_delta)))
print(f"Estat√≠stica t para alpha: {t_stat_alpha:.4f}, p-valor: {p_value_alpha:.4f}")
print(f"Estat√≠stica t para delta: {t_stat_delta:.4f}, p-valor: {p_value_delta:.4f}")

# Verificar taxas de converg√™ncia (Simula√ß√£o de Monte Carlo)
num_sim = 1000
alpha_hat_sim = np.zeros(num_sim)
delta_hat_sim = np.zeros(num_sim)

for i in range(num_sim):
    epsilon_sim = np.random.normal(0, sigma, T)
    y_sim = alpha + delta * t + epsilon_sim
    model_sim = sm.OLS(y_sim, X)
    results_sim = model_sim.fit()
    alpha_hat_sim[i], delta_hat_sim[i] = results_sim.params

# Taxas de converg√™ncia
alpha_conv = np.std(np.sqrt(T)*(alpha_hat_sim - alpha))
delta_conv = np.std(T**(3/2)*(delta_hat_sim - delta))
print(f"Desvio padr√£o de sqrt(T)*(alpha_hat - alpha): {alpha_conv:.4f}")
print(f"Desvio padr√£o de T**(3/2)*(delta_hat - delta): {delta_conv:.4f}")

# An√°lise dos res√≠duos
residuals = results.resid
plt.figure(figsize=(10, 5))
plt.plot(t, residuals, marker='o', linestyle='-', label = 'Res√≠duos')
plt.xlabel("Tempo (t)")
plt.ylabel("Res√≠duos")
plt.title("An√°lise de Res√≠duos")
plt.axhline(0, color='red', linestyle='--')
plt.legend()
plt.show()
```
Este c√≥digo gera dados de uma s√©rie temporal com uma tend√™ncia linear, estima os par√¢metros usando OLS, e calcula as estat√≠sticas t. A simula√ß√£o de Monte Carlo demonstra como os estimadores convergem a taxas diferentes, com $\hat{\alpha}$ convergindo √† taxa de $\sqrt{T}$ e $\hat{\delta}$ convergindo √† taxa de $T^{3/2}$. A an√°lise dos res√≠duos permite verificar se os erros s√£o de ru√≠do branco e n√£o correlacionados com o tempo.

**Interpreta√ß√£o:**
- Os par√¢metros estimados ($\hat{\alpha}$ e $\hat{\delta}$) s√£o pr√≥ximos dos verdadeiros ($\alpha$ e $\delta$), e seus desvios padr√£o s√£o consistentes com a teoria.
- As estat√≠sticas t e os p-valores indicam se os coeficientes s√£o estatisticamente significativos.
- A simula√ß√£o de Monte Carlo confirma que as taxas de converg√™ncia assint√≥tica s√£o diferentes para $\hat{\alpha}$ e $\hat{\delta}$, como demonstrado teoricamente.
- A an√°lise dos res√≠duos ajuda a verificar a validade do modelo.

<br>

**Proposi√ß√£o 1**
*Sob as mesmas hip√≥teses da Proposi√ß√£o 16.1, se $v_t$ √© um processo definido como $v_t = \sum_{j=1}^t \epsilon_j$, ent√£o, para $v_T$ temos*
$$ \frac{v_T}{T^{1/2}} \xrightarrow{d} N(0, \sigma^2) $$
*e para um processo estoc√°stico definido como $w_t = \sum_{j=1}^t v_j$, temos*
$$ \frac{w_T}{T^{3/2}} \xrightarrow{d} N(0, \sigma^2/3) $$

*Prova:*

I. A primeira parte do resultado √© uma aplica√ß√£o direta do teorema do limite central.

II. Como $\epsilon_t$ √© i.i.d com m√©dia zero e vari√¢ncia $\sigma^2$, temos que $\frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t = \frac{v_T}{\sqrt{T}}$.

III. Pelo Teorema do Limite Central, $\frac{v_T}{\sqrt{T}}$ converge em distribui√ß√£o para uma vari√°vel aleat√≥ria $N(0,\sigma^2)$.

IV. Para a segunda parte, observe que $w_T = \sum_{t=1}^T v_t = \sum_{t=1}^T \sum_{j=1}^t \epsilon_j = \sum_{j=1}^T (T - j + 1) \epsilon_j$.

V. Podemos reescrever $w_T$ como $\sum_{j=1}^T T(1 - \frac{j-1}{T}) \epsilon_j = T\sum_{j=1}^T (1 - \frac{j-1}{T})\epsilon_j$.

VI. Portanto,
$$ \frac{w_T}{T^{3/2}} = \frac{1}{T^{1/2}} \sum_{j=1}^T (1-\frac{j-1}{T}) \epsilon_j  $$

VII. De forma an√°loga ao resultado demonstrado para $\frac{1}{\sqrt{T}} \sum_{t=1}^{T} \frac{t}{T} \epsilon_t$, e usando o resultado de que $\frac{1}{T} \sum_{t=1}^T (\frac{t}{T}) \rightarrow \frac{1}{2}$, temos que $\frac{1}{\sqrt{T}} \sum_{j=1}^T (1-\frac{j-1}{T}) \epsilon_j$ converge em distribui√ß√£o para $N(0,\sigma^2/3)$, comprovando o resultado. $\blacksquare$

> üí° **Exemplo Num√©rico:** Vamos simular os processos $v_t$ e $w_t$ e verificar suas taxas de converg√™ncia
```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Par√¢metros da simula√ß√£o
T = 100
sigma = 1
num_sim = 1000

# Simula√ß√£o de Monte Carlo para v_t e w_t
v_T_sim = np.zeros(num_sim)
w_T_sim = np.zeros(num_sim)

for i in range(num_sim):
  epsilon = np.random.normal(0, sigma, T)
  v_t = np.cumsum(epsilon)
  w_t = np.cumsum(v_t)
  v_T_sim[i] = v_t[-1]
  w_T_sim[i] = w_t[-1]

# An√°lise das taxas de converg√™ncia
v_T_std = np.std(v_T_sim/np.sqrt(T))
w_T_std = np.std(w_T_sim/T**(3/2))

print(f"Desvio padr√£o de v_T / sqrt(T): {v_T_std:.4f}")
print(f"Desvio padr√£o de w_T / T**(3/2): {w_T_std:.4f}")

# Visualiza√ß√£o da distribui√ß√£o
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.hist(v_T_sim / np.sqrt(T), bins = 30, density=True, alpha = 0.6, label = 'Histograma de v_T/sqrt(T)')
x = np.linspace(-5,5,100)
plt.plot(x, norm.pdf(x,0,sigma), label = 'N(0, sigma)')
plt.title('Distribui√ß√£o de v_T / sqrt(T)')
plt.legend()
plt.subplot(1, 2, 2)
plt.hist(w_T_sim / T**(3/2), bins = 30, density=True, alpha= 0.6, label= 'Histograma de w_T / T**(3/2)')
x = np.linspace(-5,5,100)
plt.plot(x, norm.pdf(x, 0, sigma/np.sqrt(3)), label = 'N(0, sigma/sqrt(3))')
plt.title('Distribui√ß√£o de w_T / T**(3/2)')
plt.legend()
plt.show()
```

Este c√≥digo simula os processos $v_t$ e $w_t$ e demonstra que suas vers√µes normalizadas convergem para distribui√ß√µes normais com as vari√¢ncias te√≥ricas ($\sigma^2$ e $\sigma^2/3$). Os histogramas confirmam que as distribui√ß√µes dos processos normalizados se aproximam das distribui√ß√µes normais esperadas √† medida que o tamanho da amostra aumenta.

**Interpreta√ß√£o:**
- Os desvios padr√£o emp√≠ricos de $v_T/\sqrt{T}$ e $w_T/T^{3/2}$ s√£o pr√≥ximos dos valores te√≥ricos.
- Os histogramas das simula√ß√µes se aproximam das distribui√ß√µes normais esperadas, confirmando a proposi√ß√£o.

**Proposi√ß√£o 2**
*Sob as mesmas hip√≥teses da Proposi√ß√£o 16.1, se adicionarmos ao modelo de tend√™ncia temporal linear uma vari√°vel de tend√™ncia quadr√°tica, ent√£o a converg√™ncia de $\hat{\alpha}_T, \hat{\delta}_T$ e o estimador do coeficiente da tend√™ncia quadr√°tica, $\hat{\gamma}_T$, ocorrem em taxas diferentes. Especificamente, considerando o modelo $y_t = \alpha + \delta t + \gamma t^2 + \epsilon_t$, temos que $\hat{\alpha}_T$ converge √† taxa de $\sqrt{T}$, $\hat{\delta}_T$ converge √† taxa de $T^{3/2}$ e $\hat{\gamma}_T$ converge √† taxa de $T^{5/2}$.*

*Prova:*
I.  Seguindo a mesma l√≥gica do modelo linear simples, a matriz $X'X$ para este caso √© dada por
$$ X'X = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t & \sum_{t=1}^T t^2 \\
\sum_{t=1}^T t & \sum_{t=1}^T t^2 & \sum_{t=1}^T t^3 \\
\sum_{t=1}^T t^2 & \sum_{t=1}^T t^3 & \sum_{t=1}^T t^4 \end{bmatrix} $$

II. Pelo resultado [16.1.13], sabemos que o termo dominante de $\sum_{t=1}^T t^v$ √© de ordem $T^{v+1}$.

III. Portanto, a matriz $X'X$ √© de ordem
$$ \begin{bmatrix} T & T^2 & T^3 \\ T^2 & T^3 & T^4 \\ T^3 & T^4 & T^5 \end{bmatrix} $$

IV. Para obter uma matriz que convirja para uma matriz n√£o singular, precisamos pr√© e p√≥s multiplicar $X'X$ por uma matriz diagonal cujos elementos s√£o $\sqrt{T}$, $T^{3/2}$ e $T^{5/2}$.

V. Chamando esta matriz de $Y_T$, temos que
$$ Y_T = \begin{bmatrix} \sqrt{T} & 0 & 0 \\ 0 & T^{3/2} & 0 \\ 0 & 0 & T^{5/2} \end{bmatrix} $$

VI. Aplicando a mesma l√≥gica do caso da tend√™ncia linear, o resultado segue. $\blacksquare$

> üí° **Exemplo Num√©rico:**  Vamos simular um modelo com tend√™ncia quadr√°tica e verificar as taxas de converg√™ncia.
```python
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt

# Par√¢metros do modelo
alpha = 2
delta = 0.1
gamma = 0.005
sigma = 1
T = 100

# Gerar dados
t = np.arange(1, T + 1)
epsilon = np.random.normal(0, sigma, T)
y = alpha + delta * t + gamma * t**2 + epsilon

# Criar dataframe
data = pd.DataFrame({'t': t, 'y': y, 'epsilon': epsilon})

# Visualizar dados
plt.figure(figsize=(10,5))
plt.plot(data['t'],data['y'], label = 'S√©rie Temporal y_t')
plt.plot(data['t'], alpha + delta * t + gamma * t**2, label = 'Tend√™ncia Quadr√°tica')
plt.xlabel('Tempo (t)')
plt.ylabel('Valor de y_t')
plt.title('S√©rie Temporal com Tend√™ncia Quadr√°tica')
plt.legend()
plt.show()

# Estima√ß√£o OLS
X = np.column_stack((np.ones(T), t, t**2))
model = sm.OLS(y, X)
results = model.fit()
alpha_hat, delta_hat, gamma_hat = results.params

print(f"Estimativa de alpha: {alpha_hat:.4f}")
print(f"Estimativa de delta: {delta_hat:.4f}")
print(f"Estimativa de gamma: {gamma_hat:.4f}")

# Verificar taxas de converg√™ncia (Simula√ß√£o de Monte Carlo)
num_sim = 1000
alpha_hat_sim = np.zeros(num_sim)
delta_hat_sim = np.zeros(num_sim)
gamma_hat_sim = np.zeros(num_sim)

def generate_data_sim(n, theta_true):
    z = np.random.normal(0, 1, n)
    x = np.random.normal(0, 1, n)
    y = theta_true[0] + theta_true[1] * x + theta_true[2] * z + np.random.normal(0,1,n)
    return y, x, z

for i in range(num_sim):
    y_sim, x_sim, z_sim = generate_data_sim(n, theta_true)
    theta_hat_sim = ols_estimator(y_sim, x_sim, z_sim)
    alpha_hat_sim[i] = theta_hat_sim[0]
    delta_hat_sim[i] = theta_hat_sim[1]
    gamma_hat_sim[i] = theta_hat_sim[2]

# Calcular o vi√©s e o erro quadr√°tico m√©dio (MSE)
bias_alpha = np.mean(alpha_hat_sim) - theta_true[0]
bias_delta = np.mean(delta_hat_sim) - theta_true[1]
bias_gamma = np.mean(gamma_hat_sim) - theta_true[2]

mse_alpha = np.mean((alpha_hat_sim - theta_true[0])**2)
mse_delta = np.mean((delta_hat_sim - theta_true[1])**2)
mse_gamma = np.mean((gamma_hat_sim - theta_true[2])**2)

print(f"\nResultados da Simula√ß√£o de Monte Carlo:")
print(f"Vi√©s de alpha_hat: {bias_alpha:.4f}")
print(f"Vi√©s de delta_hat: {bias_delta:.4f}")
print(f"Vi√©s de gamma_hat: {bias_gamma:.4f}")
print(f"MSE de alpha_hat: {mse_alpha:.4f}")
print(f"MSE de delta_hat: {mse_delta:.4f}")
print(f"MSE de gamma_hat: {mse_gamma:.4f}")

# Intervalos de Confian√ßa
alpha_ci = np.percentile(alpha_hat_sim, [2.5, 97.5])
delta_ci = np.percentile(delta_hat_sim, [2.5, 97.5])
gamma_ci = np.percentile(gamma_hat_sim, [2.5, 97.5])

print("\nIntervalos de Confian√ßa (95%):")
print(f"Intervalo de Confian√ßa para alpha_hat: {alpha_ci}")
print(f"Intervalo de Confian√ßa para delta_hat: {delta_ci}")
print(f"Intervalo de Confian√ßa para gamma_hat: {gamma_ci}")

#Teste de Hip√≥tese
# H0: theta[1] = 0 vs H1: theta[1] != 0

t_stat_delta =  np.mean(delta_hat_sim) / (np.std(delta_hat_sim)/np.sqrt(num_sim))
p_value_delta = 2 * (1 - stats.t.cdf(np.abs(t_stat_delta), df=num_sim-1))

print("\nTeste de Hip√≥tese para delta (H0: delta = 0):")
print(f"Estat√≠stica t: {t_stat_delta:.4f}")
print(f"P-valor: {p_value_delta:.4f}")

if p_value_delta < 0.05:
    print("Rejeitamos a hip√≥tese nula. H√° evid√™ncia estat√≠stica de que delta √© diferente de 0.")
else:
     print("N√£o rejeitamos a hip√≥tese nula. N√£o h√° evid√™ncia estat√≠stica de que delta √© diferente de 0.")
<!-- END -->
