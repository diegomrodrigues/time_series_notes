## Distribuição Assintótica de Estimativas OLS para um Modelo de Tendência Temporal Simples

### Introdução
Este capítulo introduz o conceito de diferentes taxas de convergência assintótica para estimadores em modelos de regressão com tendências temporais determinísticas. Conforme mencionado anteriormente, a estimação por mínimos quadrados ordinários (OLS) em modelos com raízes unitárias ou tendências temporais determinísticas requer uma análise cuidadosa, pois as distribuições assintóticas dos estimadores de coeficientes não podem ser calculadas da mesma forma que em modelos com variáveis estacionárias [^1]. Este capítulo, construindo sobre os fundamentos da regressão apresentados no capítulo 8, aborda especificamente o caso de processos envolvendo tendências temporais determinísticas, mas sem raízes unitárias, e estabelece uma base para o tratamento de processos mais complexos com raízes unitárias nos capítulos subsequentes. Em particular, analisamos o modelo de tendência temporal linear simples e determinamos que as estatísticas t e F usuais, calculadas da forma padrão, possuem as mesmas distribuições assintóticas que em regressões estacionárias [^1]. A técnica introduzida aqui não só é aplicável ao estudo de tendências temporais, mas também é crucial para a análise de estimadores de processos não estacionários que serão discutidos nos capítulos 17 e 18.

### Conceitos Fundamentais
Começamos com o modelo de tendência temporal linear simples, dado por:
$$ y_t = \alpha + \delta t + \epsilon_t $$ [^1]  [16.1.1]
onde $\epsilon_t$ é um processo de ruído branco. Assumindo que $\epsilon_t \sim N(0, \sigma^2)$, o modelo [16.1.1] satisfaz as suposições clássicas de regressão e as estatísticas t ou F padrão são aplicáveis [^1].  A questão central é que, se $\epsilon_t$ não for Gaussiano, as distribuições assintóticas dos estimadores OLS de $\alpha$ e $\delta$ requerem uma técnica ligeiramente diferente daquela empregada para regressões estacionárias no capítulo 8 [^2]. O modelo [16.1.1] pode ser escrito na forma de regressão padrão como:
$$ y_t = x_t'\beta + \epsilon_t $$ [^2] [16.1.2]
onde
$$ x_t = \begin{bmatrix} 1 \\ t \end{bmatrix} $$ [^2] [16.1.3]
e
$$ \beta = \begin{bmatrix} \alpha \\ \delta \end{bmatrix} $$ [^2] [16.1.4]

Seja $b_T$ o estimador OLS de $\beta$ baseado em uma amostra de tamanho T:

$$ b_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} = \left(\sum_{t=1}^{T} x_t x_t'\right)^{-1} \sum_{t=1}^{T} x_t y_t $$ [^2] [16.1.5]
A partir da equação [8.2.3], o desvio do estimador OLS de seu valor verdadeiro pode ser expresso como
$$ (b_T - \beta) = \left(\sum_{t=1}^{T} x_t x_t'\right)^{-1} \sum_{t=1}^{T} x_t \epsilon_t $$ [^2] [16.1.6]
Para encontrar a distribuição limite para uma regressão com variáveis explicativas estacionárias, o procedimento no Capítulo 8 envolveu multiplicar [16.1.6] por $\sqrt{T}$, resultando em:

$$ \sqrt{T}(b_T - \beta) = \left(\frac{1}{T} \sum_{t=1}^{T} x_t x_t'\right)^{-1} \left(\frac{1}{\sqrt{T}} \sum_{t=1}^{T} x_t \epsilon_t\right) $$ [^2] [16.1.7]

A suposição usual era que $\frac{1}{T} \sum_{t=1}^{T} x_t x_t'$ convergia em probabilidade para uma matriz não singular $Q$, enquanto $\frac{1}{\sqrt{T}} \sum_{t=1}^{T} x_t \epsilon_t$ convergia em distribuição para uma variável aleatória $N(0, \sigma^2 Q)$, implicando que $\sqrt{T}(b_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1})$ [^2]. Para entender porque esse argumento não pode ser usado para uma tendência temporal determinística, notamos que, para $x_t$ e $\beta$ dados em [16.1.3] e [16.1.4], a expressão [16.1.6] se torna
$$ \begin{bmatrix} \hat{\alpha}_T - \alpha \\ \hat{\delta}_T - \delta \end{bmatrix} = \begin{bmatrix} \sum_{t=1}^{T} 1 & \sum_{t=1}^{T} t \\ \sum_{t=1}^{T} t & \sum_{t=1}^{T} t^2 \end{bmatrix}^{-1} \begin{bmatrix} \sum_{t=1}^{T} \epsilon_t \\ \sum_{t=1}^{T} t \epsilon_t \end{bmatrix} $$ [^2] [16.1.8]
É direto mostrar, por indução, que
$$ \sum_{t=1}^{T} t = T(T+1)/2 $$ [^3] [16.1.9]
e
$$ \sum_{t=1}^{T} t^2 = T(T+1)(2T+1)/6 $$ [^3] [16.1.10]
Assim, o termo principal em $\sum_{t=1}^{T} t$ é $T^2/2$, ou seja,
$$ \frac{1}{T^2} \sum_{t=1}^{T} t = \frac{1}{T^2} \left[\frac{T^2}{2} + \frac{T}{2}\right] = \frac{1}{2} + \frac{1}{2T} \rightarrow \frac{1}{2} $$ [^3] [16.1.11]
Similarmente, o termo principal em $\sum_{t=1}^{T} t^2$ é $T^3/3$:
$$ \frac{1}{T^3} \sum_{t=1}^{T} t^2 = \frac{1}{T^3} \left[\frac{2T^3}{6} + \frac{3T^2}{6} + \frac{T}{6} \right] = \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2} \rightarrow \frac{1}{3} $$ [^3] [16.1.12]
Para referência futura, notamos que o padrão geral - o termo principal em $\sum_{t=1}^{T} t^v$ é $T^{v+1}/(v+1)$:
$$ \frac{1}{T^{v+1}} \sum_{t=1}^{T} t^v \rightarrow \frac{1}{v+1} $$ [^3] [16.1.13]
Para verificar [16.1.13], note que
$$ \frac{1}{T^{v+1}} \sum_{t=1}^{T} t^v = \frac{1}{T} \sum_{t=1}^{T} \left(\frac{t}{T}\right)^v $$ [^3] [16.1.14]
O lado direito de [16.1.14] pode ser visto como uma aproximação da área sob a curva $f(r) = r^v$ para $r$ entre zero e um.
A matriz $\sum_{t=1}^{T} x_t x_t'$ torna-se:

$$ \sum_{t=1}^{T} x_t x_t' = \begin{bmatrix} \sum_{t=1}^{T} 1 & \sum_{t=1}^{T} t \\ \sum_{t=1}^{T} t & \sum_{t=1}^{T} t^2 \end{bmatrix} = \begin{bmatrix} T & T(T+1)/2 \\ T(T+1)/2 & T(T+1)(2T+1)/6 \end{bmatrix} $$ [^4] [16.1.16]
Em contraste com o resultado usual para regressões estacionárias, para a matriz em [16.1.16], $\frac{1}{T} \sum_{t=1}^{T} x_t x_t'$ diverge. Para obter uma matriz convergente, [16.1.16] teria que ser dividida por $T^3$ ao invés de $T$. No entanto, essa matriz limite não pode ser invertida [^4].
Os estimadores OLS $\hat{\alpha}_T$ e $\hat{\delta}_T$ possuem diferentes taxas de convergência assintótica. Para obter distribuições limites não degeneradas, $\hat{\alpha}_T$ é multiplicado por $\sqrt{T}$, enquanto $\hat{\delta}_T$ deve ser multiplicado por $T^{3/2}$ [^4]. Essa correção pode ser pensada como a pré-multiplicação de [16.1.6] ou [16.1.8] pela matriz:

$$ Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} $$ [^4] [16.1.17]

resultando em
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} = Y_T \left(\sum_{t=1}^{T} x_t x_t'\right)^{-1} Y_T^{-1} Y_T \sum_{t=1}^{T} x_t \epsilon_t $$ [^5] [16.1.18]
Considerando o primeiro termo da última expressão em [16.1.18], e substituindo [16.1.17] e [16.1.16]:
$$ Y_T \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} Y_T = \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} \begin{bmatrix} \sum_{t=1}^{T} 1 & \sum_{t=1}^{T} t \\ \sum_{t=1}^{T} t & \sum_{t=1}^{T} t^2 \end{bmatrix}^{-1} \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} \rightarrow Q $$ [^5] [16.1.19]
onde
$$ Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} $$ [^5] [16.1.20]
Passando para o segundo termo em [16.1.18], temos
$$ Y_T \sum_{t=1}^{T} x_t \epsilon_t = \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} \sum_{t=1}^{T} \epsilon_t \\ \sum_{t=1}^{T} t \epsilon_t \end{bmatrix} = \begin{bmatrix} T^{-1/2} \sum_{t=1}^{T} \epsilon_t \\ T^{-1} \sum_{t=1}^{T} (t/T) \epsilon_t \end{bmatrix} $$ [^5] [16.1.21]

Sob as suposições padrão sobre $\epsilon_t$, este vetor será assintoticamente Gaussiano. Por exemplo, suponha que $\epsilon_t$ seja i.i.d. com média zero, variância $\sigma^2$ e um quarto momento finito. Então o primeiro elemento do vetor em [16.1.21] satisfaz:
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^{T} \epsilon_t \xrightarrow{d} N(0, \sigma^2) $$ [^5]
pelo teorema do limite central. Para o segundo elemento do vetor em [16.1.21], observe que $\{\frac{t}{T}\epsilon_t\}$ é uma sequência de diferenças de martingala que satisfaz as condições da Proposição 7.8. Especificamente, sua variância é
$$ \sigma^2_T = E\left[ \left( \frac{t}{T} \epsilon_t \right)^2 \right] = \sigma^2 \frac{t^2}{T^2} $$ [^5]
onde
$$ \frac{1}{T} \sum_{t=1}^{T} \sigma^2_t = \sigma^2 \frac{1}{T} \sum_{t=1}^{T} \frac{t^2}{T^2} \rightarrow \sigma^2 \frac{1}{3} $$ [^5]
Além disso, $\frac{1}{T}\sum_{t=1}^{T} [(t/T)\epsilon_t]^2 \xrightarrow{p} \sigma^2/3$.
Portanto,
$$ \left( \frac{1}{T} \sum_{t=1}^{T} \left( \frac{t}{T} \epsilon_t \right)^2 \right) - \left( \frac{1}{T} \sum_{t=1}^{T} \sigma^2 \frac{t^2}{T^2} \right) $$ [^6]
converge para zero.
Isso implica que
$$ \frac{1}{T} \sum_{t=1}^{T} \left[ \frac{t}{T} \epsilon_t \right]^2 \xrightarrow{p} \frac{\sigma^2}{3} $$ [^6]
Como consequência, pela Proposição 7.8, $\frac{1}{\sqrt{T}} \sum_{t=1}^{T} \frac{t}{T} \epsilon_t$ satisfaz o teorema do limite central:
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^{T} \frac{t}{T} \epsilon_t \xrightarrow{d} N(0, \sigma^2/3) $$ [^6]

Finalmente, considere a distribuição conjunta dos dois elementos do vetor (2 x 1) descrito por [16.1.21]. Qualquer combinação linear desses elementos assume a forma:
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^{T} \left[\lambda_1 + \lambda_2 \frac{t}{T}\right] \epsilon_t $$ [^6]
Então, $[\lambda_1 + \lambda_2 (t/T)]\epsilon_t$ é também uma sequência de diferenças de martingala com variância positiva, dada por $\sigma^2[\lambda_1^2 + 2\lambda_1 \lambda_2 (t/T) + \lambda_2^2 (t/T)^2]$, satisfazendo:
$$ \frac{1}{T} \sum_{t=1}^{T} \sigma^2 \left[ \lambda_1^2 + 2\lambda_1 \lambda_2 \frac{t}{T} + \lambda_2^2 \frac{t^2}{T^2} \right] \rightarrow \sigma^2 \left[ \lambda_1^2 + 2\lambda_1 \lambda_2 \frac{1}{2} + \lambda_2^2 \frac{1}{3} \right] = \sigma^2 \lambda' Q \lambda $$ [^6] [16.1.23]
onde $\lambda = (\lambda_1, \lambda_2)'$ e $Q$ é a matriz em [16.1.20]. Além disso,
$$ \frac{1}{T} \sum_{t=1}^{T} \left[\lambda_1 + \lambda_2 (t/T)\right]\epsilon_t^2  \xrightarrow{p} \sigma^2 \lambda' Q \lambda $$
Assim, qualquer combinação linear dos dois elementos no vetor em [16.1.21] é assintoticamente Gaussiana, implicando uma distribuição Gaussiana bivariada limitante [^6]. Combinando esses resultados, obtém-se:
$$ \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^{T} \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^{T} \frac{t}{T} \epsilon_t \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q) $$ [^7] [16.1.24]

A partir de [16.1.19] e [16.1.24], a distribuição assintótica de [16.1.18] pode ser calculada como no Exemplo 7.5 do Capítulo 7:
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 [Q^{-1}QQ^{-1}] ) = N(0, \sigma^2 Q^{-1}) $$ [^7] [16.1.25]

### Conclusão
Em resumo, os resultados obtidos mostram que a distribuição assintótica dos estimadores de um modelo de tendência temporal linear simples pode ser derivada utilizando uma abordagem que considera as diferentes taxas de convergência dos estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$. O estimador $\hat{\alpha}_T$ converge para o valor verdadeiro a uma taxa de $\sqrt{T}$, enquanto $\hat{\delta}_T$ converge a uma taxa de $T^{3/2}$. Essa análise demonstra que, apesar das diferentes taxas de convergência, as estatísticas t e F padrão podem ser usadas para testes de hipótese assintoticamente válidos [^7]. Em outras palavras, a estatística t usual para um teste de hipóteses sobre $\alpha$ e $\delta$ têm uma distribuição limite normal padrão.
Esses resultados são resumidos na Proposição 16.1 [^7]:
*Seja $y_t$ gerado de acordo com a tendência de tempo determinística simples [16.1.1], onde $\epsilon_t$ é i.i.d. com $E(\epsilon_t^2) = \sigma^2$ e $E(\epsilon_t^4) < \infty$. Então:*
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}) $$ [^7] [16.1.26]
É crucial notar que o estimador do coeficiente da tendência temporal ($\hat{\delta}_T$) é superconsistente – não só $\hat{\delta}_T \xrightarrow{p} \delta$, mas mesmo quando multiplicado por $T$, ainda temos
$$ T(\hat{\delta}_T - \delta) \xrightarrow{p} 0 $$ [^7] [16.1.27]
Essa superconsistência destaca uma diferença fundamental na análise de modelos com tendências temporais determinísticas e modelos com variáveis estacionárias. A metodologia apresentada aqui fornece uma base sólida para o estudo de modelos mais complexos que serão abordados nos capítulos subsequentes [^2].

> 💡 **Exemplo Numérico:** Vamos gerar uma série temporal com uma tendência linear e ruído branco para ilustrar os conceitos.
```python
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
from scipy.stats import norm

# Parâmetros do modelo
alpha = 5
delta = 0.2
sigma = 1
T = 100

# Gerar dados
t = np.arange(1, T + 1)
epsilon = np.random.normal(0, sigma, T)
y = alpha + delta * t + epsilon

# Criar dataframe para visualização
data = pd.DataFrame({'t': t, 'y': y, 'epsilon': epsilon})

# Visualizar dados
plt.figure(figsize=(10,5))
plt.plot(data['t'],data['y'], label = 'Série Temporal y_t')
plt.plot(data['t'], alpha + delta * t, label = 'Tendência Linear')
plt.xlabel('Tempo (t)')
plt.ylabel('Valor de y_t')
plt.title('Série Temporal com Tendência Linear')
plt.legend()
plt.show()

# Estimação OLS
X = sm.add_constant(t)
model = sm.OLS(y, X)
results = model.fit()
alpha_hat, delta_hat = results.params
se_alpha_hat, se_delta_hat  = results.bse
print(f"Estimativa de alpha: {alpha_hat:.4f}, Desvio Padrão: {se_alpha_hat:.4f}")
print(f"Estimativa de delta: {delta_hat:.4f}, Desvio Padrão: {se_delta_hat:.4f}")

# Cálculo das estatísticas t
t_stat_alpha = (alpha_hat - alpha) / se_alpha_hat
t_stat_delta = (delta_hat - delta) / se_delta_hat
p_value_alpha = 2 * (1 - norm.cdf(abs(t_stat_alpha)))
p_value_delta = 2 * (1 - norm.cdf(abs(t_stat_delta)))
print(f"Estatística t para alpha: {t_stat_alpha:.4f}, p-valor: {p_value_alpha:.4f}")
print(f"Estatística t para delta: {t_stat_delta:.4f}, p-valor: {p_value_delta:.4f}")

# Verificar taxas de convergência (Simulação de Monte Carlo)
num_sim = 1000
alpha_hat_sim = np.zeros(num_sim)
delta_hat_sim = np.zeros(num_sim)

for i in range(num_sim):
    epsilon_sim = np.random.normal(0, sigma, T)
    y_sim = alpha + delta * t + epsilon_sim
    model_sim = sm.OLS(y_sim, X)
    results_sim = model_sim.fit()
    alpha_hat_sim[i], delta_hat_sim[i] = results_sim.params

# Taxas de convergência
alpha_conv = np.std(np.sqrt(T)*(alpha_hat_sim - alpha))
delta_conv = np.std(T**(3/2)*(delta_hat_sim - delta))
print(f"Desvio padrão de sqrt(T)*(alpha_hat - alpha): {alpha_conv:.4f}")
print(f"Desvio padrão de T**(3/2)*(delta_hat - delta): {delta_conv:.4f}")

# Análise dos resíduos
residuals = results.resid
plt.figure(figsize=(10, 5))
plt.plot(t, residuals, marker='o', linestyle='-', label = 'Resíduos')
plt.xlabel("Tempo (t)")
plt.ylabel("Resíduos")
plt.title("Análise de Resíduos")
plt.axhline(0, color='red', linestyle='--')
plt.legend()
plt.show()
```
Este código gera dados de uma série temporal com uma tendência linear, estima os parâmetros usando OLS, e calcula as estatísticas t. A simulação de Monte Carlo demonstra como os estimadores convergem a taxas diferentes, com $\hat{\alpha}$ convergindo à taxa de $\sqrt{T}$ e $\hat{\delta}$ convergindo à taxa de $T^{3/2}$. A análise dos resíduos permite verificar se os erros são de ruído branco e não correlacionados com o tempo.

**Interpretação:**
- Os parâmetros estimados ($\hat{\alpha}$ e $\hat{\delta}$) são próximos dos verdadeiros ($\alpha$ e $\delta$), e seus desvios padrão são consistentes com a teoria.
- As estatísticas t e os p-valores indicam se os coeficientes são estatisticamente significativos.
- A simulação de Monte Carlo confirma que as taxas de convergência assintótica são diferentes para $\hat{\alpha}$ e $\hat{\delta}$, como demonstrado teoricamente.
- A análise dos resíduos ajuda a verificar a validade do modelo.

<br>

**Proposição 1**
*Sob as mesmas hipóteses da Proposição 16.1, se $v_t$ é um processo definido como $v_t = \sum_{j=1}^t \epsilon_j$, então, para $v_T$ temos*
$$ \frac{v_T}{T^{1/2}} \xrightarrow{d} N(0, \sigma^2) $$
*e para um processo estocástico definido como $w_t = \sum_{j=1}^t v_j$, temos*
$$ \frac{w_T}{T^{3/2}} \xrightarrow{d} N(0, \sigma^2/3) $$

*Prova:*

I. A primeira parte do resultado é uma aplicação direta do teorema do limite central.

II. Como $\epsilon_t$ é i.i.d com média zero e variância $\sigma^2$, temos que $\frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t = \frac{v_T}{\sqrt{T}}$.

III. Pelo Teorema do Limite Central, $\frac{v_T}{\sqrt{T}}$ converge em distribuição para uma variável aleatória $N(0,\sigma^2)$.

IV. Para a segunda parte, observe que $w_T = \sum_{t=1}^T v_t = \sum_{t=1}^T \sum_{j=1}^t \epsilon_j = \sum_{j=1}^T (T - j + 1) \epsilon_j$.

V. Podemos reescrever $w_T$ como $\sum_{j=1}^T T(1 - \frac{j-1}{T}) \epsilon_j = T\sum_{j=1}^T (1 - \frac{j-1}{T})\epsilon_j$.

VI. Portanto,
$$ \frac{w_T}{T^{3/2}} = \frac{1}{T^{1/2}} \sum_{j=1}^T (1-\frac{j-1}{T}) \epsilon_j  $$

VII. De forma análoga ao resultado demonstrado para $\frac{1}{\sqrt{T}} \sum_{t=1}^{T} \frac{t}{T} \epsilon_t$, e usando o resultado de que $\frac{1}{T} \sum_{t=1}^T (\frac{t}{T}) \rightarrow \frac{1}{2}$, temos que $\frac{1}{\sqrt{T}} \sum_{j=1}^T (1-\frac{j-1}{T}) \epsilon_j$ converge em distribuição para $N(0,\sigma^2/3)$, comprovando o resultado. $\blacksquare$

> 💡 **Exemplo Numérico:** Vamos simular os processos $v_t$ e $w_t$ e verificar suas taxas de convergência
```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Parâmetros da simulação
T = 100
sigma = 1
num_sim = 1000

# Simulação de Monte Carlo para v_t e w_t
v_T_sim = np.zeros(num_sim)
w_T_sim = np.zeros(num_sim)

for i in range(num_sim):
  epsilon = np.random.normal(0, sigma, T)
  v_t = np.cumsum(epsilon)
  w_t = np.cumsum(v_t)
  v_T_sim[i] = v_t[-1]
  w_T_sim[i] = w_t[-1]

# Análise das taxas de convergência
v_T_std = np.std(v_T_sim/np.sqrt(T))
w_T_std = np.std(w_T_sim/T**(3/2))

print(f"Desvio padrão de v_T / sqrt(T): {v_T_std:.4f}")
print(f"Desvio padrão de w_T / T**(3/2): {w_T_std:.4f}")

# Visualização da distribuição
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.hist(v_T_sim / np.sqrt(T), bins = 30, density=True, alpha = 0.6, label = 'Histograma de v_T/sqrt(T)')
x = np.linspace(-5,5,100)
plt.plot(x, norm.pdf(x,0,sigma), label = 'N(0, sigma)')
plt.title('Distribuição de v_T / sqrt(T)')
plt.legend()
plt.subplot(1, 2, 2)
plt.hist(w_T_sim / T**(3/2), bins = 30, density=True, alpha= 0.6, label= 'Histograma de w_T / T**(3/2)')
x = np.linspace(-5,5,100)
plt.plot(x, norm.pdf(x, 0, sigma/np.sqrt(3)), label = 'N(0, sigma/sqrt(3))')
plt.title('Distribuição de w_T / T**(3/2)')
plt.legend()
plt.show()
```

Este código simula os processos $v_t$ e $w_t$ e demonstra que suas versões normalizadas convergem para distribuições normais com as variâncias teóricas ($\sigma^2$ e $\sigma^2/3$). Os histogramas confirmam que as distribuições dos processos normalizados se aproximam das distribuições normais esperadas à medida que o tamanho da amostra aumenta.

**Interpretação:**
- Os desvios padrão empíricos de $v_T/\sqrt{T}$ e $w_T/T^{3/2}$ são próximos dos valores teóricos.
- Os histogramas das simulações se aproximam das distribuições normais esperadas, confirmando a proposição.

**Proposição 2**
*Sob as mesmas hipóteses da Proposição 16.1, se adicionarmos ao modelo de tendência temporal linear uma variável de tendência quadrática, então a convergência de $\hat{\alpha}_T, \hat{\delta}_T$ e o estimador do coeficiente da tendência quadrática, $\hat{\gamma}_T$, ocorrem em taxas diferentes. Especificamente, considerando o modelo $y_t = \alpha + \delta t + \gamma t^2 + \epsilon_t$, temos que $\hat{\alpha}_T$ converge à taxa de $\sqrt{T}$, $\hat{\delta}_T$ converge à taxa de $T^{3/2}$ e $\hat{\gamma}_T$ converge à taxa de $T^{5/2}$.*

*Prova:*
I.  Seguindo a mesma lógica do modelo linear simples, a matriz $X'X$ para este caso é dada por
$$ X'X = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t & \sum_{t=1}^T t^2 \\
\sum_{t=1}^T t & \sum_{t=1}^T t^2 & \sum_{t=1}^T t^3 \\
\sum_{t=1}^T t^2 & \sum_{t=1}^T t^3 & \sum_{t=1}^T t^4 \end{bmatrix} $$

II. Pelo resultado [16.1.13], sabemos que o termo dominante de $\sum_{t=1}^T t^v$ é de ordem $T^{v+1}$.

III. Portanto, a matriz $X'X$ é de ordem
$$ \begin{bmatrix} T & T^2 & T^3 \\ T^2 & T^3 & T^4 \\ T^3 & T^4 & T^5 \end{bmatrix} $$

IV. Para obter uma matriz que convirja para uma matriz não singular, precisamos pré e pós multiplicar $X'X$ por uma matriz diagonal cujos elementos são $\sqrt{T}$, $T^{3/2}$ e $T^{5/2}$.

V. Chamando esta matriz de $Y_T$, temos que
$$ Y_T = \begin{bmatrix} \sqrt{T} & 0 & 0 \\ 0 & T^{3/2} & 0 \\ 0 & 0 & T^{5/2} \end{bmatrix} $$

VI. Aplicando a mesma lógica do caso da tendência linear, o resultado segue. $\blacksquare$

> 💡 **Exemplo Numérico:**  Vamos simular um modelo com tendência quadrática e verificar as taxas de convergência.
```python
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt

# Parâmetros do modelo
alpha = 2
delta = 0.1
gamma = 0.005
sigma = 1
T = 100

# Gerar dados
t = np.arange(1, T + 1)
epsilon = np.random.normal(0, sigma, T)
y = alpha + delta * t + gamma * t**2 + epsilon

# Criar dataframe
data = pd.DataFrame({'t': t, 'y': y, 'epsilon': epsilon})

# Visualizar dados
plt.figure(figsize=(10,5))
plt.plot(data['t'],data['y'], label = 'Série Temporal y_t')
plt.plot(data['t'], alpha + delta * t + gamma * t**2, label = 'Tendência Quadrática')
plt.xlabel('Tempo (t)')
plt.ylabel('Valor de y_t')
plt.title('Série Temporal com Tendência Quadrática')
plt.legend()
plt.show()

# Estimação OLS
X = np.column_stack((np.ones(T), t, t**2))
model = sm.OLS(y, X)
results = model.fit()
alpha_hat, delta_hat, gamma_hat = results.params

print(f"Estimativa de alpha: {alpha_hat:.4f}")
print(f"Estimativa de delta: {delta_hat:.4f}")
print(f"Estimativa de gamma: {gamma_hat:.4f}")

# Verificar taxas de convergência (Simulação de Monte Carlo)
num_sim = 1000
alpha_hat_sim = np.zeros(num_sim)
delta_hat_sim = np.zeros(num_sim)
gamma_hat_sim = np.zeros(num_sim)

def generate_data_sim(n, theta_true):
    z = np.random.normal(0, 1, n)
    x = np.random.normal(0, 1, n)
    y = theta_true[0] + theta_true[1] * x + theta_true[2] * z + np.random.normal(0,1,n)
    return y, x, z

for i in range(num_sim):
    y_sim, x_sim, z_sim = generate_data_sim(n, theta_true)
    theta_hat_sim = ols_estimator(y_sim, x_sim, z_sim)
    alpha_hat_sim[i] = theta_hat_sim[0]
    delta_hat_sim[i] = theta_hat_sim[1]
    gamma_hat_sim[i] = theta_hat_sim[2]

# Calcular o viés e o erro quadrático médio (MSE)
bias_alpha = np.mean(alpha_hat_sim) - theta_true[0]
bias_delta = np.mean(delta_hat_sim) - theta_true[1]
bias_gamma = np.mean(gamma_hat_sim) - theta_true[2]

mse_alpha = np.mean((alpha_hat_sim - theta_true[0])**2)
mse_delta = np.mean((delta_hat_sim - theta_true[1])**2)
mse_gamma = np.mean((gamma_hat_sim - theta_true[2])**2)

print(f"\nResultados da Simulação de Monte Carlo:")
print(f"Viés de alpha_hat: {bias_alpha:.4f}")
print(f"Viés de delta_hat: {bias_delta:.4f}")
print(f"Viés de gamma_hat: {bias_gamma:.4f}")
print(f"MSE de alpha_hat: {mse_alpha:.4f}")
print(f"MSE de delta_hat: {mse_delta:.4f}")
print(f"MSE de gamma_hat: {mse_gamma:.4f}")

# Intervalos de Confiança
alpha_ci = np.percentile(alpha_hat_sim, [2.5, 97.5])
delta_ci = np.percentile(delta_hat_sim, [2.5, 97.5])
gamma_ci = np.percentile(gamma_hat_sim, [2.5, 97.5])

print("\nIntervalos de Confiança (95%):")
print(f"Intervalo de Confiança para alpha_hat: {alpha_ci}")
print(f"Intervalo de Confiança para delta_hat: {delta_ci}")
print(f"Intervalo de Confiança para gamma_hat: {gamma_ci}")

#Teste de Hipótese
# H0: theta[1] = 0 vs H1: theta[1] != 0

t_stat_delta =  np.mean(delta_hat_sim) / (np.std(delta_hat_sim)/np.sqrt(num_sim))
p_value_delta = 2 * (1 - stats.t.cdf(np.abs(t_stat_delta), df=num_sim-1))

print("\nTeste de Hipótese para delta (H0: delta = 0):")
print(f"Estatística t: {t_stat_delta:.4f}")
print(f"P-valor: {p_value_delta:.4f}")

if p_value_delta < 0.05:
    print("Rejeitamos a hipótese nula. Há evidência estatística de que delta é diferente de 0.")
else:
     print("Não rejeitamos a hipótese nula. Não há evidência estatística de que delta é diferente de 0.")
<!-- END -->
