## Distribui√ß√µes Limite e Taxas de Converg√™ncia em Modelos de Tend√™ncia Temporal com Vari√°veis N√£o Estacion√°rias

### Introdu√ß√£o
Este cap√≠tulo se aprofunda na an√°lise da distribui√ß√£o limite de estimadores OLS em modelos de regress√£o que envolvem tend√™ncias temporais determin√≠sticas, como discutido anteriormente [^1]. Em particular, exploramos a necessidade de multiplicar o desvio do estimador OLS do seu valor verdadeiro por $\sqrt{T}$ para obter distribui√ß√µes limites adequadas em regress√µes com vari√°veis n√£o estacion√°rias [^2]. Como j√° estabelecido, para modelos de tend√™ncia temporal, as taxas de converg√™ncia assint√≥tica para os estimadores $\alpha$ e $\delta$ s√£o diferentes, requerendo ajustes espec√≠ficos para obter distribui√ß√µes limites n√£o degeneradas [^4]. Este cap√≠tulo detalha como lidar com essas diferentes taxas de converg√™ncia, utilizando a t√©cnica de *rescaling* (ajuste) dos estimadores, atrav√©s da matriz $Y_T$, a qual vimos no cap√≠tulo anterior. Tal t√©cnica √© essencial para garantir que a an√°lise assint√≥tica seja v√°lida e para realizar infer√™ncias precisas sobre os par√¢metros do modelo.

### Distribui√ß√µes Limite com Vari√°veis N√£o Estacion√°rias

No contexto de regress√µes com vari√°veis estacion√°rias, a abordagem padr√£o √© multiplicar o desvio do estimador OLS do seu valor verdadeiro por $\sqrt{T}$, o que possibilita obter uma distribui√ß√£o limite normal [^2]. No entanto, como vimos anteriormente, essa abordagem n√£o √© v√°lida para modelos com tend√™ncias temporais determin√≠sticas, dada a diverg√™ncia da matriz $\frac{1}{T}\sum_{t=1}^T x_t x_t'$ [^4]. Para o modelo de tend√™ncia temporal linear simples
$$ y_t = \alpha + \delta t + \epsilon_t $$ [^1] [16.1.1]
vimos que a matriz $\sum_{t=1}^T x_t x_t'$ √© dada por:
$$ \sum_{t=1}^T x_t x_t' = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} $$ [^4] [16.1.16]
e que, diferentemente de modelos com vari√°veis estacion√°rias, a matriz $\frac{1}{T}\sum_{t=1}^T x_t x_t'$ diverge [^4].

Conforme discutido, para o modelo de tend√™ncia temporal linear, a abordagem usual de multiplicar o desvio do estimador OLS, $(b_T - \beta)$, por $\sqrt{T}$ n√£o √© suficiente para obter uma distribui√ß√£o limite n√£o degenerada. A raz√£o para isso √© que os estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$ convergem para os valores verdadeiros $\alpha$ e $\delta$ a taxas diferentes [^4]. Especificamente, $\hat{\alpha}_T$ converge a uma taxa de $\sqrt{T}$, enquanto $\hat{\delta}_T$ converge a uma taxa de $T^{3/2}$ [^7]. Para lidar com essas taxas de converg√™ncia diferentes, √© necess√°rio aplicar uma transforma√ß√£o adequada.

> üí° **Exemplo Num√©rico:** Para ilustrar a diferen√ßa nas taxas de converg√™ncia, vamos simular um modelo com $\alpha = 2$, $\delta = 0.5$ e $\epsilon_t \sim N(0,1)$, para diferentes tamanhos de amostra $T$. Calcularemos o erro padr√£o dos estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$.
> ```python
> import numpy as np
> import statsmodels.api as sm
> import pandas as pd
>
> np.random.seed(42)
>
> def simulate_and_estimate(T, alpha=2, delta=0.5, sigma=1):
>     time = np.arange(1, T + 1)
>     errors = np.random.normal(0, sigma, T)
>     y = alpha + delta * time + errors
>     X = sm.add_constant(time)
>     model = sm.OLS(y, X)
>     results = model.fit()
>     return results.params[0], results.params[1], results.bse[0], results.bse[1]
>
> T_values = [100, 500, 1000, 5000]
> results_list = []
>
> for T in T_values:
>   alpha_hat_list = []
>   delta_hat_list = []
>   alpha_se_list = []
>   delta_se_list = []
>   for _ in range(100):
>     alpha_hat, delta_hat, alpha_se, delta_se = simulate_and_estimate(T)
>     alpha_hat_list.append(alpha_hat)
>     delta_hat_list.append(delta_hat)
>     alpha_se_list.append(alpha_se)
>     delta_se_list.append(delta_se)
>   results_list.append(
>       [T, np.mean(alpha_hat_list),np.std(alpha_hat_list),np.mean(delta_hat_list),np.std(delta_hat_list), np.mean(alpha_se_list), np.mean(delta_se_list)]
>   )
>
> results_df = pd.DataFrame(results_list, columns=['T','Mean Alpha Hat', 'Std Alpha Hat','Mean Delta Hat','Std Delta Hat','Mean SE Alpha','Mean SE Delta'])
> print(results_df)
>
> ```
> Os resultados da simula√ß√£o mostram que o desvio padr√£o de $\hat{\alpha}_T$ decresce aproximadamente com $\sqrt{T}$, e o desvio padr√£o de $\hat{\delta}_T$ decresce aproximadamente com $T^{3/2}$, confirmando as taxas de converg√™ncia te√≥ricas. Note que,  a medida que T aumenta, os estimadores ficam mais pr√≥ximos dos verdadeiros, e que o erro padr√£o dos estimadores decresce em concord√¢ncia com as taxas de converg√™ncia.

O desvio do estimador OLS de seu valor verdadeiro √© dado por:
$$ (b_T - \beta) = \left(\sum_{t=1}^T x_t x_t'\right)^{-1} \sum_{t=1}^T x_t \epsilon_t $$ [^2] [16.1.6]
Para obter distribui√ß√µes limites n√£o degeneradas, $\hat{\alpha}_T$ deve ser multiplicado por $\sqrt{T}$, enquanto $\hat{\delta}_T$ deve ser multiplicado por $T^{3/2}$ [^4]. Essa corre√ß√£o pode ser obtida atrav√©s da pr√©-multiplica√ß√£o de $(b_T - \beta)$ pela matriz $Y_T$, dada por:

$$ Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} $$ [^4] [16.1.17]
Dessa forma, obtemos:
$$ Y_T(b_T - \beta) = \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} = Y_T \left(\sum_{t=1}^T x_t x_t'\right)^{-1} \sum_{t=1}^T x_t \epsilon_t $$ [^5] [16.1.18]

**Lema 1:** A aplica√ß√£o da matriz $Y_T$ ao desvio do estimador OLS ajusta corretamente as taxas de converg√™ncia assint√≥tica dos estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$.
*Prova:*
I. Definimos o desvio do estimador OLS como:
$$ (b_T - \beta) = \begin{bmatrix} \hat{\alpha}_T - \alpha \\ \hat{\delta}_T - \delta \end{bmatrix} $$
II. Pr√©-multiplicamos esse desvio pela matriz $Y_T$, dada por:
$$ Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} $$
III. A multiplica√ß√£o resulta em:
$$ Y_T (b_T - \beta) = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} \hat{\alpha}_T - \alpha \\ \hat{\delta}_T - \delta \end{bmatrix} = \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} $$
IV. A teoria assint√≥tica para modelos com tend√™ncia temporal determin√≠stica estabelece que o estimador $\hat{\alpha}_T$ converge a uma taxa $\sqrt{T}$, enquanto $\hat{\delta}_T$ converge a uma taxa $T^{3/2}$. A multiplica√ß√£o pela matriz $Y_T$ corrige essas taxas de forma que ambas as componentes do vetor $Y_T (b_T - \beta)$ possuam uma distribui√ß√£o limite com vari√¢ncia finita e n√£o nula. Isso estabelece que o "rescaling" feito por $Y_T$ ajusta corretamente as taxas de converg√™ncia.  $\blacksquare$

Para obter a distribui√ß√£o limite de $Y_T(b_T - \beta)$, analisamos o primeiro termo do lado direito de [16.1.18]:
$$ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1} $$
que converge para uma matriz $Q$ quando $T \rightarrow \infty$:
$$ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \xrightarrow{p} Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} $$ [^5] [16.1.19], [16.1.20]

> üí° **Exemplo Num√©rico:** Vamos demonstrar numericamente a converg√™ncia da matriz  $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T$ para a matriz $Q$. Utilizaremos diferentes valores de $T$ para observar a converg√™ncia.
> ```python
> import numpy as np
>
> def calculate_yt_xtx_inv_yt(T):
>     # Construct the X matrix (including a constant term)
>     X = np.column_stack((np.ones(T), np.arange(1, T + 1)))
>
>     # Calculate the matrix X'X
>     XTX = X.T @ X
>
>     # Calculate the inverse of X'X
>     XTX_inv = np.linalg.inv(XTX)
>
>     # Construct the Y_T matrix
>     YT = np.diag([np.sqrt(T), T**(3/2)])
>
>     # Calculate the final matrix Y_T (X'X)^-1 Y_T
>     result_matrix = YT @ XTX_inv @ YT
>
>     return result_matrix
>
> # Define the different values of T
> T_values = [100, 500, 1000, 5000]
>
> # Calculate the matrices for each value of T and display
> for T in T_values:
>   result_matrix = calculate_yt_xtx_inv_yt(T)
>   print(f"T = {T}:\n{result_matrix}\n")
>
> # Define the theoretical limit matrix Q
> Q = np.array([[1, 1/2], [1/2, 1/3]])
> print(f"Theoretical limit matrix Q:\n{Q}")
> ```
> Os resultados mostram que, √† medida que $T$ aumenta, a matriz $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T$ se aproxima da matriz te√≥rica $Q$, confirmando a converg√™ncia.

Al√©m disso, o segundo termo em [16.1.18], dado por:
$$ Y_T \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{T} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix} $$ [^5] [16.1.21]
converge para uma distribui√ß√£o normal multivariada com m√©dia zero e matriz de covari√¢ncia $\sigma^2 Q$:
$$ \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q) $$ [^7] [16.1.24]
O resultado crucial √© que a pr√©-multiplica√ß√£o por $Y_T$ e a an√°lise das matrizes resultantes, tanto a matriz dos regressores quanto a matriz dos res√≠duos, fornece uma forma de obter a distribui√ß√£o assint√≥tica dos estimadores.

**Proposi√ß√£o 1:** A matriz $Y_T$, ao pr√©-multiplicar o desvio do estimador OLS, garante que os estimadores tenham distribui√ß√µes limites n√£o degeneradas.
*Prova:*
I. O desvio do estimador OLS √© dado por $(b_T - \beta)$. A distribui√ß√£o limite desse desvio, sem nenhum ajuste, √© singular.
II. Aplicando a matriz $Y_T$ a $(b_T - \beta)$, obtemos:
$$ Y_T(b_T - \beta) =  \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} $$
III. O termo
$$ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T $$
converge para uma matriz n√£o singular $Q$.
IV. O termo
$$ Y_T \sum_{t=1}^T x_t \epsilon_t $$
converge para uma distribui√ß√£o normal com m√©dia zero e matriz de covari√¢ncia $\sigma^2 Q$.
V. Pelo Teorema de Slutsky, a distribui√ß√£o limite do produto dos dois termos converge para uma distribui√ß√£o normal multivariada com m√©dia zero e vari√¢ncia $\sigma^2 Q^{-1}$. Portanto, $Y_T(b_T - \beta)$ converge para uma distribui√ß√£o limite n√£o degenerada.
$$\begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix}  \xrightarrow{d} N(0, \sigma^2 Q^{-1}) $$
$\blacksquare$

> üí° **Exemplo Num√©rico:** Vamos verificar a distribui√ß√£o dos estimadores ap√≥s a transforma√ß√£o por $Y_T$. Simularmos os estimadores, calcularmos os erros, multiplicarmos pela matriz $Y_T$, e observarmos o comportamento das distribui√ß√µes.
> ```python
> import numpy as np
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
> import scipy.stats as stats
>
> np.random.seed(42)
>
> def simulate_and_estimate(T, alpha=5, delta=0.2, sigma=1):
>     time = np.arange(1, T + 1)
>     errors = np.random.normal(0, sigma, T)
>     y = alpha + delta * time + errors
>     X = sm.add_constant(time)
>     model = sm.OLS(y, X)
>     results = model.fit()
>     return results.params[0], results.params[1]
>
> T = 1000
> num_simulations = 500
> alpha_hat_transformed = []
> delta_hat_transformed = []
>
> for _ in range(num_simulations):
>     alpha_hat, delta_hat = simulate_and_estimate(T)
>     alpha_hat_transformed.append(np.sqrt(T) * (alpha_hat - 5))
>     delta_hat_transformed.append(T**(3/2) * (delta_hat - 0.2))
>
> # Plotting the histograms
> fig, axs = plt.subplots(1, 2, figsize=(12, 5))
>
> axs[0].hist(alpha_hat_transformed, bins=30, density=True, alpha=0.6, color='skyblue', label = 'Histogram')
> mu = np.mean(alpha_hat_transformed)
> sigma = np.std(alpha_hat_transformed)
> x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)
> axs[0].plot(x, stats.norm.pdf(x, mu, sigma), color = 'darkblue',label = 'Normal Distribution')
> axs[0].set_title('Distribution of $\sqrt{T}(\hat{\alpha}_T - \alpha)$')
> axs[0].set_xlabel('Value')
> axs[0].set_ylabel('Density')
> axs[0].legend()
>
> axs[1].hist(delta_hat_transformed, bins=30, density=True, alpha=0.6, color='lightcoral', label = 'Histogram')
> mu = np.mean(delta_hat_transformed)
> sigma = np.std(delta_hat_transformed)
> x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)
> axs[1].plot(x, stats.norm.pdf(x, mu, sigma), color = 'darkred',label = 'Normal Distribution')
> axs[1].set_title('Distribution of $T^{3/2}(\hat{\delta}_T - \delta)$')
> axs[1].set_xlabel('Value')
> axs[1].set_ylabel('Density')
> axs[1].legend()
> plt.tight_layout()
> plt.show()
>
> ```
> Os histogramas mostram que a distribui√ß√£o dos estimadores transformados se aproxima de uma distribui√ß√£o normal, confirmando a teoria de que a multiplica√ß√£o pela matriz $Y_T$ gera estimadores com distribui√ß√£o limite n√£o degenerada. Note que o erro padr√£o dos estimadores diminui com o aumento do tamanho da amostra $T$.

**Lema 1.1:** A matriz $Q$ √© dada por $Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$ e sua inversa √© $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$.
*Prova:*
I.  A matriz $Q$ √© obtida como o limite da matriz $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T$.
II.  Sabemos que $\sum_{t=1}^T x_t x_t' = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix}$.
III. Assim, $\left( \sum_{t=1}^T x_t x_t' \right)^{-1} = \frac{1}{\det(\sum_{t=1}^T x_t x_t')} \begin{bmatrix} \frac{T(T+1)(2T+1)}{6} & -\frac{T(T+1)}{2} \\ -\frac{T(T+1)}{2} & T \end{bmatrix}$.
IV. O determinante de $\sum_{t=1}^T x_t x_t'$ √© $T \frac{T(T+1)(2T+1)}{6} - (\frac{T(T+1)}{2})^2 = \frac{T^2(T+1)(2T+1)}{6} - \frac{T^2(T+1)^2}{4} = \frac{T^2(T+1)}{12} (2(2T+1)-3(T+1)) = \frac{T^2(T+1)}{12}(4T+2-3T-3)=\frac{T^2(T+1)(T-1)}{12} = \frac{T^2(T^2-1)}{12}$
V. Multiplicando $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} \frac{12}{T^2(T^2-1)} \begin{bmatrix} \frac{T(T+1)(2T+1)}{6} & -\frac{T(T+1)}{2} \\ -\frac{T(T+1)}{2} & T \end{bmatrix} \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}$
VI. Simplificando e tomando o limite quando $T \to \infty$ obtemos $Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$.
VII. Para calcular a inversa de $Q$, temos $\det(Q) = 1/3 - 1/4 = 1/12$. Logo, $Q^{-1} = \frac{1}{1/12} \begin{bmatrix} 1/3 & -1/2 \\ -1/2 & 1 \end{bmatrix} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$. $\blacksquare$

**Lema 1.2:** A matriz $Y_T \sum_{t=1}^T x_t \epsilon_t$ converge em distribui√ß√£o para uma normal multivariada com m√©dia zero e matriz de covari√¢ncia $\sigma^2 Q$.
*Prova:*
I.  $Y_T \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} \sum_{t=1}^T \epsilon_t \\ \sum_{t=1}^T t \epsilon_t \end{bmatrix} = \begin{bmatrix} \sqrt{T} \sum_{t=1}^T \epsilon_t \\ T^{3/2} \sum_{t=1}^T t \epsilon_t \end{bmatrix} = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t \end{bmatrix}$
II.  Dividindo e multiplicando por $T$ o segundo elemento, temos $\begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{T} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix} $.
III.  Pelo teorema do limite central multivariado, sabemos que $\frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t$ e $\frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t$  convergem para uma normal multivariada com m√©dia zero, e matriz de covari√¢ncia $\sigma^2 \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} = \sigma^2 Q$. $\blacksquare$

### Conclus√£o
Em s√≠ntese, a distribui√ß√£o limite dos estimadores OLS em modelos de tend√™ncia temporal com vari√°veis n√£o estacion√°rias requer um tratamento especial. A abordagem usual de multiplicar o desvio do estimador OLS do seu valor verdadeiro por $\sqrt{T}$ n√£o √© suficiente para obter distribui√ß√µes limites n√£o degeneradas. A diverg√™ncia da matriz $\frac{1}{T} \sum_{t=1}^T x_t x_t'$, como visto anteriormente, exige uma corre√ß√£o nas taxas de converg√™ncia dos estimadores. Para tanto, multiplicamos $\hat{\alpha}_T$ por $\sqrt{T}$ e $\hat{\delta}_T$ por $T^{3/2}$, por meio da pr√©-multiplica√ß√£o pela matriz $Y_T$. Essa transforma√ß√£o garante que as distribui√ß√µes limites sejam n√£o degeneradas, permitindo realizar infer√™ncias estat√≠sticas v√°lidas. A abordagem apresentada aqui √© um passo fundamental para a an√°lise de modelos de s√©ries temporais com tend√™ncia temporal e para a compreens√£o da necessidade de ajustes nos modelos com n√£o estacionariedade [^4].

**Corol√°rio 1:** A transforma√ß√£o dos estimadores atrav√©s da matriz $Y_T$ √© fundamental para garantir que as infer√™ncias assint√≥ticas em modelos de tend√™ncia temporal sejam v√°lidas, pois a aplica√ß√£o da matriz resulta em estimadores cujas vari√¢ncias s√£o finitas e n√£o nulas.
*Prova:*
I. Os estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$ t√™m taxas de converg√™ncia diferentes. Sem a transforma√ß√£o por $Y_T$, suas distribui√ß√µes limites s√£o degeneradas.
II. Pela Proposi√ß√£o 1, sabemos que ao aplicar a matriz $Y_T$, obtemos a converg√™ncia para uma distribui√ß√£o normal multivariada, com m√©dia zero e vari√¢ncia finita e n√£o nula.
III. Isso permite realizar testes de hip√≥teses e construir intervalos de confian√ßa utilizando as distribui√ß√µes limites derivadas.
IV. Conclu√≠mos que a matriz $Y_T$ √© uma forma de transformar os estimadores, de forma que seus res√≠duos possuam vari√¢ncia n√£o nula, possibilitando a realiza√ß√£o de testes estat√≠sticos com as ferramentas da infer√™ncia assint√≥tica. $\blacksquare$

**Observa√ß√£o 1:** A matriz $Y_T$ n√£o √© a √∫nica forma de obter converg√™ncia para uma distribui√ß√£o normal multivariada. No entanto, ela √© uma forma eficiente e direta, que demonstra a necessidade de ajustar as taxas de converg√™ncia dos estimadores em modelos com tend√™ncias temporais determin√≠sticas, garantindo a validade das infer√™ncias assint√≥ticas sobre os par√¢metros do modelo.

**Observa√ß√£o 2:** A escolha da matriz $Y_T$ √© diretamente ligada √† estrutura do modelo, em particular, √† presen√ßa da tend√™ncia temporal linear. Caso o modelo possua outras componentes, como tend√™ncias polinomiais de ordem superior, a matriz $Y_T$ teria que ser adaptada para ajustar as taxas de converg√™ncia assint√≥tica de cada um dos estimadores.

**Teorema 1:** Se o modelo de regress√£o linear possui uma tend√™ncia temporal polinomial de ordem $p$, ou seja, $y_t = \sum_{j=0}^p \beta_j t^j + \epsilon_t$, ent√£o a matriz de "rescaling" adequada para obter distribui√ß√µes limites n√£o degeneradas dos estimadores OLS √© dada por $Y_T = \text{diag}(T^{j/2})$, para $j=0,1,\ldots,p$, sendo $T^0 = \sqrt{T}$.
*Prova:*
I. Para um modelo com tend√™ncia polinomial de ordem $p$, a matriz de regressores $X$ cont√©m termos $1, t, t^2, \ldots, t^p$.
II. A matriz $\sum_{t=1}^T x_t x_t'$ cont√©m termos da forma $\sum_{t=1}^T t^j t^k$ que s√£o da ordem de $T^{j+k+1}$. Portanto, o elemento $(j,k)$ da matriz $(X^TX)^{-1}$ ser√° de ordem $T^{-(j+k+1)}$
III. Para que a matriz $Y_T (X^TX)^{-1}Y_T$ convirja para uma matriz finita, a matriz $Y_T$ deve conter termos de ordem $T^{j/2}$.
IV. Assim, cada estimador $\hat{\beta_j}$ dever√° ser multiplicado por $T^{(j+1)/2}$, e o "rescaling" adequado ser√° dado por $Y_T = \text{diag}(T^{(j+1)/2})$, para $j=0,1,\ldots,p$.  Note que $\sqrt{T} = T^{1/2}$ e $T^{3/2} = T^{(1+2)/2}$.
V. Assim, a matriz de "rescaling" $Y_T$ √© dada por $Y_T = \text{diag}(1, \sqrt{T}, T, T^{3/2},\ldots,T^{(p+1)/2})$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Vamos considerar um modelo com uma tend√™ncia quadr√°tica, $y_t = \beta_0 + \beta_1 t + \beta_2 t^2 + \epsilon_t$, onde $\beta_0=1$, $\beta_1=0.3$, $\beta_2 = 0.01$ e $\epsilon_t$ √© um ru√≠do branco com desvio padr√£o $\sigma=1$. Vamos ilustrar a constru√ß√£o da matriz $Y_T$ para esse modelo e como ela escala os estimadores.
> ```python
> import numpy as np
> import statsmodels.api as sm
> import pandas as pd
>
> np.random.seed(42)
>
> def simulate_and_estimate_quadratic(T, beta_0=1, beta_1=0.3, beta_2=0.01, sigma=1):
>    time = np.arange(1, T + 1)
>    errors = np.random.normal(0, sigma, T)
>    y = beta_0 + beta_1 * time + beta_2 * time**2 + errors
>    X = np.column_stack((np.ones(T), time, time**2))
>    model = sm.OLS(y, X)
>    results = model.fit()
>    return results.params, results.bse
>
> T_values = [100, 500, 1000]
> results_list = []
>
> for T in T_values:
>   beta_hat, se = simulate_and_estimate_quadratic(T)
>   Y_T = np.diag([np.sqrt(T), T, T**(3/2)])
>   scaled_beta_hat = Y_T @ (beta_hat - np.array([1, 0.3, 0.01]))
>   results_list.append(
>        [T, beta_hat[0], beta_hat[1],beta_hat[2],
>         se[0], se[1], se[2], scaled_beta_hat[0], scaled_beta_hat[1], scaled_beta_hat[2]]
>   )
> results_df = pd.DataFrame(results_list, columns=[
>    'T','beta_0_hat', 'beta_1_hat', 'beta_2_hat',
>    'se_beta_0', 'se_beta_1', 'se_beta_2',
>    'scaled_beta_0', 'scaled_beta_1', 'scaled_beta_2'])
> print(results_df)
> ```
>  A sa√≠da do c√≥digo apresenta as estimativas e seus erros padr√£o, bem como as estimativas escaladas pela matriz $Y_T$. Nota-se que os erros padr√£o decrescem de acordo com a taxa de converg√™ncia te√≥rica, e que a matriz $Y_T$ escala os estimadores adequadamente, gerando resultados mais est√°veis √† medida que $T$ aumenta.

### Refer√™ncias
[^1]:  [16.1]
[^2]:  [16.1], [16.1.1] - [16.1.6]
[^3]:  [16.1], [16.1.9] - [16.1.10]
[^4]:  [16.1], [16.1.16] - [16.1.17]
[^5]:  [16.1], [16.1.18] - [16.1.21]
[^7]:  [16.1], [16.1.24] - [16.1.27]
<!-- END -->
