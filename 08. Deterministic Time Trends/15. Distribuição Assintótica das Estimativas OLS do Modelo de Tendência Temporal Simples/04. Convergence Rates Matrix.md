## A Matriz de Ajuste das Taxas de Converg√™ncia e a Distribui√ß√£o Assint√≥tica

### Introdu√ß√£o
Em continuidade √† an√°lise da distribui√ß√£o assint√≥tica das estimativas OLS em modelos de tend√™ncia temporal determin√≠stica [^SECTION_PLACEHOLDER], esta se√ß√£o detalha a import√¢ncia da matriz de ajuste $Y_T$ e como ela garante a converg√™ncia das estimativas para distribui√ß√µes limites n√£o degeneradas. Exploraremos o processo de rescalonamento das estimativas, demonstrando como a multiplica√ß√£o por $Y_T$ leva √† converg√™ncia da matriz $\frac{1}{T^3} \sum_{t=1}^T x_t x_t'$ para uma matriz n√£o singular $Q$ e como o termo envolvendo as inova√ß√µes √© afetado, culminando em distribui√ß√µes assint√≥ticas Gaussianas para as estimativas dos coeficientes.

### Conceitos Fundamentais
Como discutido anteriormente [^SECTION_PLACEHOLDER], a abordagem padr√£o de multiplicar o desvio da estimativa OLS por $\sqrt{T}$ n√£o √© suficiente para obter uma distribui√ß√£o limite √∫til no caso de tend√™ncias temporais determin√≠sticas [^10]. Isso ocorre porque as somas $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$ crescem a taxas diferentes, $T^2/2$ e $T^3/3$ respectivamente [^11, ^12, ^13, ^14], fazendo com que a matriz $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ n√£o convirja para uma matriz n√£o singular [^15]. Para contornar este problema, introduzimos a matriz $Y_T$ [^17], definida como:
$$Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}$$

Esta matriz √© crucial para ajustar as diferentes taxas de converg√™ncia dos estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$. $\hat{\alpha}_T$ converge para $\alpha$ a uma taxa de $\sqrt{T}$, enquanto $\hat{\delta}_T$ converge para $\delta$ a uma taxa de $T^{3/2}$. Para obter distribui√ß√µes limites n√£o degeneradas, √© necess√°rio pr√©-multiplicar o desvio da estimativa OLS, $(b_T - \beta)$, pela matriz $Y_T$. Assim, temos:
$$Y_T(b_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t$$
Reescrevendo a equa√ß√£o, podemos isolar o efeito da matriz $Y_T$:
$$Y_T(b_T - \beta) =  \left( Y_T \left(\sum_{t=1}^T x_t x_t'\right)^{-1} Y_T \right)  \left( Y_T \sum_{t=1}^T x_t \epsilon_t \right)$$
O primeiro termo da equa√ß√£o demonstra como a matriz $Y_T$ √© usada para redimensionar a matriz de somas dos regressores:
$$Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T = \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix}^{-1} \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{3/2} \end{bmatrix}$$
Ao aplicar a matriz $Y_T$ e tomar o limite quando $T$ tende ao infinito, obt√©m-se:
$$Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \rightarrow \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} T^{-1} \sum_{t=1}^T 1 & T^{-2} \sum_{t=1}^T t \\ T^{-2} \sum_{t=1}^T t & T^{-3} \sum_{t=1}^T t^2 \end{bmatrix}^{-1} \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}  \rightarrow Q^{-1} $$

onde $Q$ √© dada por [^19, ^20]:
$$Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$$
e sua inversa, $Q^{-1}$, √© dada por:
$$Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$$
**Proposi√ß√£o 1:** A matriz $Q$ definida acima √© positiva definida.

*Prova:* Uma matriz sim√©trica √© positiva definida se todos os seus autovalores s√£o estritamente positivos. A matriz $Q$ √© sim√©trica. Calculando seu determinante, obtemos $\det(Q) = 1 \cdot \frac{1}{3} - \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{3} - \frac{1}{4} = \frac{1}{12} > 0$. Al√©m disso, o elemento da diagonal superior esquerda de $Q$ √© $1>0$. Como os autovalores de uma matriz 2x2 sim√©trica s√£o ambos positivos se o determinante e o primeiro elemento da diagonal s√£o positivos, segue que a matriz $Q$ √© positiva definida. $\blacksquare$

> üí° **Exemplo Num√©rico:** Vamos considerar um exemplo com $T=10$. Primeiro, vamos calcular as somas necess√°rias:
>  $\sum_{t=1}^{10} 1 = 10$, $\sum_{t=1}^{10} t = 55$, and $\sum_{t=1}^{10} t^2 = 385$. Ent√£o, a matriz $\sum_{t=1}^T x_t x_t'$ √©:
> $$\sum_{t=1}^{10} x_t x_t' = \begin{bmatrix} 10 & 55 \\ 55 & 385 \end{bmatrix}$$
> Sua inversa √©:
> $$\left( \sum_{t=1}^{10} x_t x_t' \right)^{-1} = \begin{bmatrix} 0.206 & -0.029 \\ -0.029 & 0.0053 \end{bmatrix}$$
> A matriz $Y_{10}$ √©:
> $$Y_{10} = \begin{bmatrix} \sqrt{10} & 0 \\ 0 & 10^{3/2} \end{bmatrix} \approx \begin{bmatrix} 3.162 & 0 \\ 0 & 31.623 \end{bmatrix}$$
> Multiplicando $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T$:
> $$ Y_{10} \left( \sum_{t=1}^{10} x_t x_t' \right)^{-1} Y_{10} = \begin{bmatrix} 3.162 & 0 \\ 0 & 31.623 \end{bmatrix} \begin{bmatrix} 0.206 & -0.029 \\ -0.029 & 0.0053 \end{bmatrix} \begin{bmatrix} 3.162 & 0 \\ 0 & 31.623 \end{bmatrix} $$
> $$ \approx \begin{bmatrix} 3.162 & 0 \\ 0 & 31.623 \end{bmatrix} \begin{bmatrix} 0.651 & -0.916 \\ -0.0916 & 0.168 \end{bmatrix}  \approx \begin{bmatrix} 2.059 & -28.96 \\ -2.896 & 5.31 \end{bmatrix}$$
>
>  $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T = \begin{bmatrix} 2.059 & -2.896 \\ -2.896 & 5.31 \end{bmatrix} $. Note que essa matriz ainda n√£o √© $Q^{-1}$, mas se $T$ fosse para o infinito, a matriz convergiria para $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$.
>  No exemplo anterior [^SECTION_PLACEHOLDER], vimos que essa matriz converge para $Q^{-1}$. Vemos que a matriz $Y_T$ √© crucial para obter a converg√™ncia para a matriz $Q^{-1}$.

O segundo termo na equa√ß√£o, que envolve as inova√ß√µes, tamb√©m √© afetado pelo redimensionamento:
$$ Y_T \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} \sum_{t=1}^T \epsilon_t \\ \sum_{t=1}^T t \epsilon_t \end{bmatrix} = \begin{bmatrix} \sqrt{T} \sum_{t=1}^T \epsilon_t \\ T^{3/2} \sum_{t=1}^T t \epsilon_t \end{bmatrix}$$
Este termo tamb√©m converge para uma distribui√ß√£o normal multivariada, como demonstrado em [^21, ^22], com matriz de covari√¢ncia dada por $\sigma^2 Q$. A matriz $Y_T$ √©, portanto, fundamental para garantir a converg√™ncia das estimativas para uma distribui√ß√£o assint√≥tica Gaussiana n√£o degenerada.

**Lema 1:**  Se $\epsilon_t$ s√£o i.i.d com m√©dia zero e vari√¢ncia $\sigma^2$, ent√£o  $\frac{1}{\sqrt{T}}\sum_{t=1}^T \epsilon_t \overset{d}{\longrightarrow} N(0, \sigma^2)$ e $\frac{1}{T^{3/2}}\sum_{t=1}^T t\epsilon_t \overset{d}{\longrightarrow} N(0, \frac{\sigma^2}{3})$.

*Prova:* 
I. A primeira parte √© uma aplica√ß√£o direta do Teorema do Limite Central (TLC). Como $\epsilon_t$ s√£o i.i.d. com m√©dia zero e vari√¢ncia $\sigma^2$, a soma $\sum_{t=1}^T \epsilon_t$ tem m√©dia zero e vari√¢ncia $T\sigma^2$. Pelo TLC, $\frac{1}{\sqrt{T}}\sum_{t=1}^T \epsilon_t$ converge em distribui√ß√£o para uma normal com m√©dia zero e vari√¢ncia $\sigma^2$.
II. Para a segunda parte, note que $\text{Var}(\frac{1}{T^{3/2}}\sum_{t=1}^T t\epsilon_t) = \frac{1}{T^3} \sum_{t=1}^T t^2 \text{Var}(\epsilon_t) = \frac{\sigma^2}{T^3}\sum_{t=1}^T t^2$.
III. Sabendo que $\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6}$, temos que:
$$\frac{1}{T^3}\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6T^3} = \frac{2T^3 + 3T^2 + T}{6T^3} = \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2}$$
IV. Portanto, $\lim_{T\to\infty} \frac{1}{T^3}\sum_{t=1}^T t^2 = \frac{1}{3}$.
V. Assim, a vari√¢ncia converge para $\frac{\sigma^2}{3}$. A converg√™ncia em distribui√ß√£o para a normal segue do TLC de Lindeberg-Feller, dado que a condi√ß√£o de Lindeberg-Feller √© satisfeita para essa sequ√™ncia de vari√°veis. $\blacksquare$

> üí° **Exemplo Num√©rico:** Vamos simular uma s√©rie temporal com $T=100$ e $\sigma^2 = 1$. Geramos $\epsilon_t$ como uma amostra de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 1. Podemos ent√£o calcular $\frac{1}{\sqrt{T}}\sum_{t=1}^T \epsilon_t$ e $\frac{1}{T^{3/2}}\sum_{t=1}^T t\epsilon_t$ e verificar se eles se aproximam das distribui√ß√µes normais previstas pelo Lema 1.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> T = 100
> sigma_sq = 1
> np.random.seed(42) # para reprodutibilidade
>
> eps = np.random.normal(0, np.sqrt(sigma_sq), T)
> sum_eps = np.sum(eps)
> sum_t_eps = np.sum(np.arange(1, T+1) * eps)
>
> scaled_sum_eps = (1/np.sqrt(T)) * sum_eps
> scaled_sum_t_eps = (1/T**(3/2)) * sum_t_eps
>
> print(f"Scaled sum of eps: {scaled_sum_eps:.4f}")
> print(f"Scaled sum of t*eps: {scaled_sum_t_eps:.4f}")
>
> #Histogramas para visualiza√ß√£o
> num_simulations = 1000
> scaled_sum_eps_simulations = []
> scaled_sum_t_eps_simulations = []
> for _ in range(num_simulations):
>     eps_sim = np.random.normal(0, np.sqrt(sigma_sq), T)
>     scaled_sum_eps_simulations.append((1/np.sqrt(T))*np.sum(eps_sim))
>     scaled_sum_t_eps_simulations.append((1/T**(3/2))*np.sum(np.arange(1, T+1)*eps_sim))
>
> plt.figure(figsize=(10, 4))
> plt.subplot(1, 2, 1)
> plt.hist(scaled_sum_eps_simulations, bins=30, density=True, alpha=0.6, label='Simulated')
> x = np.linspace(-4, 4, 100)
> plt.plot(x, (1/np.sqrt(2*np.pi*sigma_sq))*np.exp(-x**2/(2*sigma_sq)), 'r', label='N(0,1)')
> plt.title("Scaled Sum of eps")
> plt.legend()
>
> plt.subplot(1, 2, 2)
> plt.hist(scaled_sum_t_eps_simulations, bins=30, density=True, alpha=0.6, label='Simulated')
> x = np.linspace(-1, 1, 100)
> plt.plot(x, (1/np.sqrt(2*np.pi*(sigma_sq/3)))*np.exp(-x**2/(2*(sigma_sq/3))), 'r', label='N(0,1/3)')
> plt.title("Scaled Sum of t*eps")
> plt.legend()
> plt.tight_layout()
> plt.show()
> ```
>
> Os histogramas simulados se aproximam das distribui√ß√µes normais esperadas, confirmando o Lema 1.

**Teorema 1:**  Se as condi√ß√µes de Lema 1 s√£o satisfeitas, ent√£o:
$$Y_T \sum_{t=1}^T x_t \epsilon_t \overset{d}{\longrightarrow} N(0, \sigma^2 Q)$$

*Prova:* 
I. Do Lema 1, temos que $\frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t$ e $\frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t$ convergem em distribui√ß√£o para normais com vari√¢ncias $\sigma^2$ e $\frac{\sigma^2}{3}$, respectivamente.

II. Precisamos verificar a covari√¢ncia assint√≥tica entre esses termos:
$$ Cov\left( \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t ,  \frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t \right) = \frac{1}{T^2} \sum_{t=1}^T t Cov(\epsilon_t, \epsilon_t) = \frac{1}{T^2} \sum_{t=1}^T t \sigma^2$$

III.  Sabemos que $\sum_{t=1}^T t = \frac{T(T+1)}{2}$. Portanto,
$$Cov\left( \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t ,  \frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t \right) = \frac{\sigma^2}{T^2} \frac{T(T+1)}{2} = \frac{\sigma^2}{2} \frac{T^2+T}{T^2} = \frac{\sigma^2}{2}\left( 1 + \frac{1}{T}\right)$$
IV. Tomando o limite quando $T \rightarrow \infty$, obtemos:
$$\lim_{T \to \infty} Cov\left( \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t ,  \frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t \right) = \frac{\sigma^2}{2}$$
V. Assim, a matriz de covari√¢ncia assint√≥tica √© $\sigma^2 Q$, onde $Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$. Logo,
$$ Y_T \sum_{t=1}^T x_t \epsilon_t \overset{d}{\longrightarrow} N(0, \sigma^2 Q)$$  $\blacksquare$

> üí° **Exemplo Num√©rico:** Continuando o exemplo anterior com $T=100$ e $\sigma^2=1$, vamos agora simular $Y_T \sum_{t=1}^T x_t \epsilon_t$ e verificar sua distribui√ß√£o.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> T = 100
> sigma_sq = 1
> np.random.seed(42)
>
> def simulate_scaled_sum(T, sigma_sq):
>    eps = np.random.normal(0, np.sqrt(sigma_sq), T)
>    sum_eps = np.sum(eps)
>    sum_t_eps = np.sum(np.arange(1, T+1) * eps)
>    YT = np.array([[np.sqrt(T), 0], [0, T**(3/2)]])
>    scaled_sum = YT @ np.array([sum_eps, sum_t_eps])
>    return scaled_sum
>
> num_simulations = 1000
> scaled_sums_simulations = np.array([simulate_scaled_sum(T, sigma_sq) for _ in range(num_simulations)])
>
> mean_simulated = np.mean(scaled_sums_simulations, axis=0)
> cov_simulated = np.cov(scaled_sums_simulations, rowvar=False)
>
> Q = np.array([[1, 1/2], [1/2, 1/3]])
> expected_cov = sigma_sq * Q
>
> print("Mean of simulated scaled sum (should be close to zero):")
> print(mean_simulated)
> print("\nCovariance of simulated scaled sum:")
> print(cov_simulated)
> print("\nExpected Covariance (sigma^2*Q):")
> print(expected_cov)
>
> plt.figure(figsize=(10, 5))
> plt.subplot(1, 2, 1)
> plt.hist(scaled_sums_simulations[:, 0], bins = 30, density=True, alpha=0.6, label='Simulated')
> x = np.linspace(-20,20,100)
> plt.plot(x, (1/np.sqrt(2*np.pi*sigma_sq))*np.exp(-x**2/(2*sigma_sq)), 'r', label='N(0,sigma^2)')
> plt.title("Scaled Sum of eps")
> plt.legend()
>
> plt.subplot(1, 2, 2)
> plt.hist(scaled_sums_simulations[:, 1], bins=30, density=True, alpha=0.6, label='Simulated')
> x = np.linspace(-10,10,100)
> plt.plot(x, (1/np.sqrt(2*np.pi*(sigma_sq/3)))*np.exp(-x**2/(2*(sigma_sq/3))), 'r', label='N(0, sigma^2/3)')
> plt.title("Scaled Sum of t*eps")
> plt.legend()
> plt.tight_layout()
> plt.show()
>
> ```
>
>  O resultado das simula√ß√µes confirma que a distribui√ß√£o de $Y_T \sum_{t=1}^T x_t \epsilon_t$ se aproxima da normal multivariada com m√©dia zero e matriz de covari√¢ncia $\sigma^2 Q$. Os histogramas e as covari√¢ncias simuladas est√£o muito pr√≥ximos do previsto pelo Teorema 1.

### Conclus√£o
A introdu√ß√£o da matriz de ajuste $Y_T$ √© essencial para compreender as distribui√ß√µes assint√≥ticas das estimativas OLS em modelos com tend√™ncias temporais determin√≠sticas [^24]. Ao rescalonar as estimativas, a matriz $Y_T$ garante a converg√™ncia da matriz $\frac{1}{T^3} \sum_{t=1}^T x_t x_t'$ para uma matriz n√£o singular $Q$ e ajusta o termo das inova√ß√µes, levando a distribui√ß√µes limites Gaussianas com matriz de covari√¢ncia $\sigma^2 Q^{-1}$. Este procedimento permite an√°lises inferenciais v√°lidas mesmo quando as taxas de converg√™ncia dos estimadores s√£o distintas. Na pr√≥xima se√ß√£o, veremos que, apesar dessas diferen√ßas, os testes t e F padr√£o podem ser aplicados de forma apropriada.

### Refer√™ncias
[^1]: Se√ß√£o "Processes with Deterministic Time Trends" do cap√≠tulo 16.
[^2]: Se√ß√£o 16.1, equa√ß√£o [16.1.1].
[^3]: Se√ß√£o 16.1, equa√ß√£o [16.1.2].
[^4]: Se√ß√£o 16.1, equa√ß√£o [16.1.3].
[^5]: Se√ß√£o 16.1, equa√ß√£o [16.1.4].
[^6]: Se√ß√£o 16.1, equa√ß√£o [16.1.5].
[^7]: Se√ß√£o 16.1, equa√ß√£o [16.1.6].
[^8]: Se√ß√£o 16.1, equa√ß√£o [16.1.6].
[^9]: Se√ß√£o 16.1, equa√ß√£o [16.1.7].
[^10]: Se√ß√£o 16.1, par√°grafo ap√≥s a equa√ß√£o [16.1.8].
[^11]: Se√ß√£o 16.1, equa√ß√£o [16.1.9].
[^12]: Se√ß√£o 16.1, equa√ß√£o [16.1.10].
[^13]: Se√ß√£o 16.1, equa√ß√£o [16.1.11].
[^14]: Se√ß√£o 16.1, equa√ß√£o [16.1.12].
[^15]: Se√ß√£o 16.1, par√°grafo ap√≥s a equa√ß√£o [16.1.16].
[^16]: Se√ß√£o 16.1, par√°grafo ap√≥s a equa√ß√£o [16.1.17].
[^17]: Se√ß√£o 16.1, equa√ß√£o [16.1.17].
[^18]: Se√ß√£o 16.1, equa√ß√£o [16.1.18].
[^19]: Se√ß√£o 16.1, equa√ß√£o [16.1.19].
[^20]: Se√ß√£o 16.1, equa√ß√£o [16.1.20].
[^21]: Se√ß√£o 16.1, par√°grafo ap√≥s a equa√ß√£o [16.1.21].
[^22]: Se√ß√£o 16.1, equa√ß√£o [16.1.23].
[^23]: Se√ß√£o 16.1, equa√ß√£o [16.1.25].
[^24]: Se√ß√£o 16.1, equa√ß√£o [16.1.26] e [16.1.27].
[^SECTION_PLACEHOLDER]: Se√ß√£o "Distribui√ß√£o Assint√≥tica das Estimativas OLS do Modelo de Tend√™ncia Temporal Simples".
<!-- END -->
