## As Somas dos Regressores e as Taxas de Converg√™ncia em Modelos de Tend√™ncia Temporal

### Introdu√ß√£o
Dando continuidade √† discuss√£o sobre a distribui√ß√£o assint√≥tica de estimadores em modelos com tend√™ncias temporais determin√≠sticas [^SECTION_PLACEHOLDER], esta se√ß√£o aprofunda a an√°lise sobre o comportamento das somas dos regressores, em especial os termos envolvendo $t$ e $t^2$, e como seus termos principais influenciam as diferentes taxas de converg√™ncia dos estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$. Detalharemos como o comportamento de $\sum_{t=1}^T t^v$ afeta a necessidade de rescalonamento dos estimadores para obter distribui√ß√µes limites n√£o degeneradas e justificaremos o uso da matriz $Y_T$ [^17].

### Conceitos Fundamentais
Como visto anteriormente [^SECTION_PLACEHOLDER], no modelo de tend√™ncia temporal simples, $y_t = \alpha + \delta t + \epsilon_t$, as somas dos regressores, dadas por $\sum_{t=1}^T 1$, $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$, s√£o cruciais para a obten√ß√£o das estimativas OLS dos par√¢metros $\alpha$ e $\delta$ [^2, ^3, ^4, ^5, ^6, ^7, ^8, ^9]. As somas $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$ possuem um comportamento espec√≠fico que impacta a taxa de converg√™ncia dos estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$.

As somas $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$ s√£o expressas por:
$$ \sum_{t=1}^T t = \frac{T(T+1)}{2} $$
$$ \sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6} $$
Analisando estas express√µes, √© evidente que o termo principal de $\sum_{t=1}^T t$ √© $T^2/2$ e o termo principal de $\sum_{t=1}^T t^2$ √© $T^3/3$ [^11, ^12, ^13, ^14]. De forma mais geral, para um valor inteiro n√£o-negativo $v$, o termo principal de $\sum_{t=1}^T t^v$ √© dado por:
$$ \sum_{t=1}^T t^v \approx \frac{T^{v+1}}{v+1} $$
Este resultado √© fundamental para entender o comportamento assint√≥tico dos estimadores. A matriz $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ √©:
$$ \frac{1}{T} \sum_{t=1}^T x_t x_t' = \begin{bmatrix} \frac{1}{T}\sum_{t=1}^T 1 & \frac{1}{T}\sum_{t=1}^T t \\ \frac{1}{T}\sum_{t=1}^T t & \frac{1}{T}\sum_{t=1}^T t^2 \end{bmatrix} = \begin{bmatrix} 1 & \frac{T+1}{2} \\ \frac{T+1}{2} & \frac{(T+1)(2T+1)}{6} \end{bmatrix} $$
> üí° **Exemplo Num√©rico:** Considere $T=10$. Ent√£o,
> $\sum_{t=1}^{10} 1 = 10$, $\sum_{t=1}^{10} t = \frac{10(10+1)}{2} = 55$, e $\sum_{t=1}^{10} t^2 = \frac{10(10+1)(2*10+1)}{6} = 385$.
>
>  A matriz  $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ √© ent√£o:
>  $\frac{1}{10} \begin{bmatrix} 10 & 55 \\ 55 & 385 \end{bmatrix} = \begin{bmatrix} 1 & 5.5 \\ 5.5 & 38.5 \end{bmatrix}$.
>
> Note que, enquanto o elemento (1,1) converge para 1, os elementos (1,2) e (2,1) convergem para infinito (a uma taxa de T), e o elemento (2,2) converge para infinito a uma taxa de T¬≤.
>
> De forma geral, quando $T$ tende para infinito, o elemento (1,1) converge para 1, o elemento (1,2) e (2,1) s√£o da ordem de $T/2$, e o elemento (2,2) √© da ordem de $T^2/3$.
>
> Ao calcular $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ para diferentes valores de $T$ (e.g., $T=10, 100, 1000, 10000$) e analisar como os elementos variam com $T$, podemos observar que esses elementos n√£o convergem para uma matriz finita, invalidando as premissas dos resultados assint√≥ticos do caso estacion√°rio.
>
> ```python
> import numpy as np
>
> def calculate_XtX(T):
>    sum_1 = T
>    sum_t = T*(T+1)/2
>    sum_t2 = T*(T+1)*(2*T+1)/6
>    XtX = np.array([[sum_1, sum_t], [sum_t, sum_t2]])
>    return XtX/T
>
> T_values = [10, 100, 1000, 10000]
>
> print("Matrix (1/T) * sum(x_t x_t') for different values of T:")
> for T in T_values:
>    matrix = calculate_XtX(T)
>    print(f"T = {T}:")
>    print(matrix)
>    print("-----")
> ```
>
> O c√≥digo acima calcula a matriz $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ para diferentes valores de T, demonstrando que os elementos fora da diagonal principal aumentam com o aumento de T, o que justifica a necessidade de rescalonamento.

Como a matriz $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ n√£o converge para uma matriz n√£o singular,  √© necess√°rio rescalonar as somas [^15]. Para obter uma matriz convergente, a matriz de somas dos regressores  deve ser dividida por $T^3$ ao inv√©s de $T$, quando consideramos a matrix com somas de $1$, $t$ e $t^2$. Isso pode ser visto aplicando a matriz de ajuste $Y_T$ [^17]. A matriz $Y_T$ rescala os estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$ de forma que suas distribui√ß√µes assint√≥ticas sejam n√£o degeneradas, como visto em [^18, ^19, ^20].  A matriz de ajuste √© dada por:
$$ Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} $$

Esta matriz garante que os elementos da matriz de somas dos regressores rescalonada, $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T$, convirjam para uma matriz n√£o singular $Q^{-1}$, permitindo an√°lises assint√≥ticas v√°lidas [^SECTION_PLACEHOLDER].

O efeito do rescalonamento √© refletido na express√£o:
$$ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T = \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix}^{-1} \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{3/2} \end{bmatrix} \rightarrow Q^{-1} $$
onde $Q$ √© dada por:
$$ Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} $$
A matriz $Y_T$ √© crucial para ajustar as diferentes taxas de converg√™ncia dos estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$ [^16]. A necessidade de multiplicar $\hat{\alpha}_T$ por $\sqrt{T}$ e $\hat{\delta}_T$ por $T^{3/2}$ decorre do comportamento assint√≥tico das somas dos regressores [^24], e √© essencial para obter distribui√ß√µes limites n√£o degeneradas.

**Proposi√ß√£o 1:** Se $\sum_{t=1}^T t^v \approx \frac{T^{v+1}}{v+1}$, ent√£o a matriz $\frac{1}{T^{v+1}} \sum_{t=1}^T t^v$ converge para uma constante quando $T \rightarrow \infty$.
*Prova:* Dada a aproxima√ß√£o $\sum_{t=1}^T t^v \approx \frac{T^{v+1}}{v+1}$, temos que
$$ \frac{1}{T^{v+1}} \sum_{t=1}^T t^v \approx \frac{1}{T^{v+1}} \frac{T^{v+1}}{v+1} = \frac{1}{v+1} $$
Portanto,
$$ \lim_{T \to \infty} \frac{1}{T^{v+1}} \sum_{t=1}^T t^v = \frac{1}{v+1} $$
Ou seja, $\frac{1}{T^{v+1}} \sum_{t=1}^T t^v$ converge para uma constante (dependente de $v$) quando $T \rightarrow \infty$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Consideremos o caso em que $v=1$. Ent√£o a proposi√ß√£o diz que $\frac{1}{T^2} \sum_{t=1}^T t$ deve convergir para $\frac{1}{1+1} = \frac{1}{2}$ quando $T$ tende para infinito.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def sum_t_scaled(T):
>    return (1/T**2) * np.sum(np.arange(1, T+1))
>
> T_values = [10, 100, 1000, 10000]
> values = [sum_t_scaled(T) for T in T_values]
>
> print("Scaled sum of t for different values of T:")
> for i, T in enumerate(T_values):
>    print(f"T = {T}: {values[i]:.5f}")
>
> plt.figure(figsize=(8, 5))
> plt.plot(T_values, values, marker = 'o')
> plt.xlabel("T")
> plt.ylabel("Scaled sum of t")
> plt.title("Convergence of (1/T^2) sum(t)")
> plt.axhline(y=1/2, color='r', linestyle='--', label="Limit = 1/2")
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> O resultado do c√≥digo mostra que $\frac{1}{T^2} \sum_{t=1}^T t$ se aproxima de $\frac{1}{2}$ quando T aumenta, conforme previsto pela proposi√ß√£o.

> üí° **Exemplo Num√©rico:** Similarmente, para $v=2$, a proposi√ß√£o nos diz que $\frac{1}{T^3}\sum_{t=1}^T t^2$ converge para $1/(2+1) = 1/3$. Podemos verificar isso numericamente:
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def sum_t2_scaled(T):
>    return (1/T**3) * np.sum(np.arange(1, T+1)**2)
>
> T_values = [10, 100, 1000, 10000]
> values = [sum_t2_scaled(T) for T in T_values]
>
> print("Scaled sum of t^2 for different values of T:")
> for i, T in enumerate(T_values):
>    print(f"T = {T}: {values[i]:.5f}")
>
> plt.figure(figsize=(8, 5))
> plt.plot(T_values, values, marker='o')
> plt.xlabel("T")
> plt.ylabel("Scaled sum of t^2")
> plt.title("Convergence of (1/T^3) sum(t^2)")
> plt.axhline(y=1/3, color='r', linestyle='--', label="Limit = 1/3")
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> O resultado das simula√ß√µes demonstra que $\frac{1}{T^3}\sum_{t=1}^T t^2$ converge para $1/3$ com o aumento de T, conforme previsto pela proposi√ß√£o.

**Lema 1.1:** Para $v \geq 0$,  $\sum_{t=1}^T t^v = \frac{T^{v+1}}{v+1} + O(T^v)$.
*Prova:*  A prova pode ser encontrada em [^Apostol]. Utiliza-se o m√©todo de integra√ß√£o e deriva√ß√£o para encontrar um limitante superior para a diferen√ßa entre a soma e a integral correspondente. Especificamente, pode-se mostrar que a diferen√ßa entre a soma e a integral √© da ordem de $T^v$. $\blacksquare$

**Proposi√ß√£o 1.1:** Se $\sum_{t=1}^T t^v = \frac{T^{v+1}}{v+1} + O(T^v)$, ent√£o $\frac{1}{T^{v+1}}\sum_{t=1}^T t^v$ converge para $\frac{1}{v+1}$ quando $T\to\infty$.
*Prova:*  
I. Da hip√≥tese, temos que:
$$ \frac{1}{T^{v+1}}\sum_{t=1}^T t^v = \frac{1}{T^{v+1}}\left(\frac{T^{v+1}}{v+1} + O(T^v)\right) $$
II. Expandindo a express√£o, obtemos:
$$ \frac{1}{T^{v+1}}\sum_{t=1}^T t^v = \frac{1}{v+1} + \frac{O(T^v)}{T^{v+1}} $$
III. Simplificando o termo de erro:
$$ \frac{1}{T^{v+1}}\sum_{t=1}^T t^v = \frac{1}{v+1} + O\left(\frac{1}{T}\right) $$
IV. Como $O(1/T) \to 0$ quando $T \to \infty$, temos que:
$$ \lim_{T\to\infty}\frac{1}{T^{v+1}}\sum_{t=1}^T t^v = \frac{1}{v+1} $$
Portanto, $\frac{1}{T^{v+1}}\sum_{t=1}^T t^v$ converge para $\frac{1}{v+1}$ quando $T\to\infty$. $\blacksquare$

√â importante observar que a Proposi√ß√£o 1 √© um caso especial da Proposi√ß√£o 1.1, onde o termo $O(T^v)$ √© ignorado na an√°lise assint√≥tica. A Proposi√ß√£o 1.1 fornece um resultado mais geral ao explicitar a ordem do termo de erro na aproxima√ß√£o da soma.

**Corol√°rio 1.1**:  A matriz $\frac{1}{T^{3}} \sum_{t=1}^T x_t x_t'$ converge para uma matriz n√£o-singular.
*Prova:*
I. Como demonstrado na Proposi√ß√£o 1, $\frac{1}{T^{v+1}} \sum_{t=1}^T t^v$ converge para uma constante dependendo de $v$, quando $T \to \infty$.
II. Usando o resultado da Proposi√ß√£o 1.1, e considerando $v=0, 1, 2$ temos que:
  - $\frac{1}{T} \sum_{t=1}^T 1 \rightarrow 1$
  - $\frac{1}{T^2} \sum_{t=1}^T t \rightarrow 1/2$
  - $\frac{1}{T^3} \sum_{t=1}^T t^2 \rightarrow 1/3$
III. Logo,
$$ \frac{1}{T^3}\sum_{t=1}^T x_t x_t' = \begin{bmatrix} \frac{1}{T^3}\sum_{t=1}^T 1 & \frac{1}{T^3}\sum_{t=1}^T t \\ \frac{1}{T^3}\sum_{t=1}^T t & \frac{1}{T^3}\sum_{t=1}^T t^2 \end{bmatrix} = \begin{bmatrix} \frac{1}{T^2} & \frac{1}{T^2}\frac{T+1}{2} \\ \frac{1}{T^2}\frac{T+1}{2} & \frac{1}{T^3}\frac{(T+1)(2T+1)}{6} \end{bmatrix} $$
IV. Calculando o limite quando $T$ tende a infinito:
$$ \lim_{T \to \infty} \frac{1}{T^3}\sum_{t=1}^T x_t x_t' = \begin{bmatrix} 0 & 0 \\ 0 & 1/3 \end{bmatrix} $$
V. Portanto, ao escalar pela matriz $Y_T$, temos que:
$$ \lim_{T \to \infty}  Y_T \left( \frac{1}{T^3} \sum_{t=1}^T x_t x_t' \right) Y_T =   \lim_{T \to \infty}  \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} 1/T^2 & 1/2T \\ 1/2T & 1/3 \end{bmatrix}  \begin{bmatrix} T^{1/2} & 0 \\ 0 & T^{3/2} \end{bmatrix} =  \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} = Q $$
que √© uma matriz n√£o singular. $\blacksquare$

> üí° **Exemplo Num√©rico:** Para ilustrar o Corol√°rio 1.1, vamos calcular numericamente a matriz $\frac{1}{T^3} \sum_{t=1}^T x_t x_t'$ e depois aplicar o rescalonamento com a matriz $Y_T$ para alguns valores de T:
> ```python
> import numpy as np
>
> def calculate_XtX_scaled(T):
>  sum_1 = T
>  sum_t = T*(T+1)/2
>  sum_t2 = T*(T+1)*(2*T+1)/6
>  XtX = np.array([[sum_1, sum_t], [sum_t, sum_t2]])
>  return XtX/T**3
>
> def rescale_matrix(matrix, T):
>    YT = np.array([[np.sqrt(T), 0], [0, T**(3/2)]])
>    return np.dot(np.dot(YT, matrix), YT)
>
> T_values = [10, 100, 1000, 10000]
>
> print("Rescaled Matrix for different values of T:")
> for T in T_values:
>  matrix = calculate_XtX_scaled(T)
>  rescaled_matrix = rescale_matrix(matrix, T)
>  print(f"T = {T}:")
>  print(f"Scaled XtX: \n{matrix}")
>  print(f"Rescaled Matrix: \n{rescaled_matrix}")
>  print("-----")
>
> Q = np.array([[1, 1/2], [1/2, 1/3]])
> print("Matrix Q:\n",Q)
> ```
> O exemplo num√©rico demonstra como a matriz rescalonada se aproxima da matriz Q √† medida que T aumenta, conforme o Corol√°rio 1.1.

### Conclus√£o
As diferentes taxas de crescimento das somas dos regressores, especialmente os termos envolvendo $t$ e $t^2$, imp√µem a necessidade de rescalonar os estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$. O uso da matriz $Y_T$ √© fundamental para obter distribui√ß√µes assint√≥ticas n√£o degeneradas e garantir a validade da an√°lise estat√≠stica no modelo de tend√™ncia temporal. O redimensionamento dos estimadores atrav√©s da matriz $Y_T$ leva a matriz $(1/T^3) \sum_{t=1}^T x_t x_t^T$ a convergir para uma matriz n√£o singular $Q$, ao mesmo tempo em que ajusta o termo das inova√ß√µes, levando a distribui√ß√µes Gaussianas assint√≥ticas. Essas t√©cnicas permitem an√°lises inferenciais v√°lidas mesmo em face de diferentes taxas de converg√™ncia dos estimadores.

### Refer√™ncias
[^1]: Se√ß√£o "Processes with Deterministic Time Trends" do cap√≠tulo 16.
[^2]: Se√ß√£o 16.1, equa√ß√£o [16.1.1].
[^3]: Se√ß√£o 16.1, equa√ß√£o [16.1.2].
[^4]: Se√ß√£o 16.1, equa√ß√£o [16.1.3].
[^5]: Se√ß√£o 16.1, equa√ß√£o [16.1.4].
[^6]: Se√ß√£o 16.1, equa√ß√£o [16.1.5].
[^7]: Se√ß√£o 16.1, equa√ß√£o [16.1.6].
[^8]: Se√ß√£o 16.1, equa√ß√£o [16.1.6].
[^9]: Se√ß√£o 16.1, equa√ß√£o [16.1.7].
[^10]: Se√ß√£o 16.1, par√°grafo ap√≥s a equa√ß√£o [16.1.8].
[^11]: Se√ß√£o 16.1, equa√ß√£o [16.1.9].
[^12]: Se√ß√£o 16.1, equa√ß√£o [16.1.10].
[^13]: Se√ß√£o 16.1, equa√ß√£o [16.1.11].
[^14]: Se√ß√£o 16.1, equa√ß√£o [16.1.12].
[^15]: Se√ß√£o 16.1, par√°grafo ap√≥s a equa√ß√£o [16.1.16].
[^16]: Se√ß√£o 16.1, par√°grafo ap√≥s a equa√ß√£o [16.1.17].
[^17]: Se√ß√£o 16.1, equa√ß√£o [16.1.17].
[^18]: Se√ß√£o 16.1, equa√ß√£o [16.1.18].
[^19]: Se√ß√£o 16.1, equa√ß√£o [16.1.19].
[^20]: Se√ß√£o 16.1, equa√ß√£o [16.1.20].
[^21]: Se√ß√£o 16.1, par√°grafo ap√≥s a equa√ß√£o [16.1.21].
[^22]: Se√ß√£o 16.1, equa√ß√£o [16.1.23].
[^23]: Se√ß√£o 16.1, equa√ß√£o [16.1.25].
[^24]: Se√ß√£o 16.1, equa√ß√£o [16.1.26] e [16.1.27].
[^SECTION_PLACEHOLDER]: Se√ß√£o "Distribui√ß√£o Assint√≥tica das Estimativas OLS do Modelo de Tend√™ncia Temporal Simples".
[^Apostol]: Apostol, T. M. (1974). *Mathematical analysis*. Addison-Wesley.
<!-- END -->
