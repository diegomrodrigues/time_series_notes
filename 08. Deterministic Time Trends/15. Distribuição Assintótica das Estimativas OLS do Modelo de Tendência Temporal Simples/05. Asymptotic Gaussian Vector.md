## Distribui√ß√£o Assint√≥tica dos Termos de Inova√ß√£o em Modelos de Tend√™ncia Temporal

### Introdu√ß√£o
Prosseguindo com a an√°lise da distribui√ß√£o assint√≥tica de estimadores em modelos de tend√™ncia temporal determin√≠stica [^SECTION_PLACEHOLDER], e com base na compreens√£o da import√¢ncia da matriz de ajuste $Y_T$ [^SECTION_PLACEHOLDER] e no rescalonamento dos regressores [^SECTION_PLACEHOLDER], esta se√ß√£o detalha o comportamento assint√≥tico dos termos envolvendo as inova√ß√µes $\epsilon_t$. Em particular, examinaremos como a combina√ß√£o linear de $\frac{1}{\sqrt{T}}\sum_{t=1}^T \epsilon_t$ e $\frac{1}{\sqrt{T}}\sum_{t=1}^T \frac{t}{T} \epsilon_t$ leva a uma distribui√ß√£o gaussiana bivariada limite, crucial para a deriva√ß√£o da distribui√ß√£o assint√≥tica dos estimadores OLS.

### Conceitos Fundamentais
Como j√° estabelecido [^SECTION_PLACEHOLDER], no modelo de tend√™ncia temporal simples $y_t = \alpha + \delta t + \epsilon_t$, a estimativa OLS dos par√¢metros $\alpha$ e $\delta$ envolve o termo $\sum_{t=1}^T x_t \epsilon_t$, onde $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$ [^2].  Ao analisar a distribui√ß√£o assint√≥tica dos estimadores rescalonados [^18],  o termo das inova√ß√µes  $Y_T \sum_{t=1}^T x_t \epsilon_t$ desempenha um papel fundamental:
$$ Y_T \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} \sum_{t=1}^T \epsilon_t \\ \sum_{t=1}^T t \epsilon_t \end{bmatrix} = \begin{bmatrix} \sqrt{T} \sum_{t=1}^T \epsilon_t \\ T^{3/2} \sum_{t=1}^T t \epsilon_t \end{bmatrix} $$
Sob as premissas padr√£o de que $\epsilon_t$ √© i.i.d., com m√©dia zero, vari√¢ncia $\sigma^2$ e quarto momento finito, podemos analisar separadamente os componentes do termo das inova√ß√µes rescalonado, nomeadamente,
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \quad \text{e} \quad \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t $$

> üí° **Exemplo Num√©rico:** Para ilustrar o comportamento desses termos, vamos considerar o caso onde $T = 100$ e simular uma s√©rie $\epsilon_t$ com m√©dia 0 e vari√¢ncia $\sigma^2 = 1$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> T = 100
> sigma_sq = 1
> np.random.seed(42)
>
> eps = np.random.normal(0, np.sqrt(sigma_sq), T)
> scaled_sum_eps = (1/np.sqrt(T)) * np.sum(eps)
> scaled_sum_t_eps = (1/np.sqrt(T)) * np.sum((np.arange(1, T + 1)/T) * eps)
>
> print(f"Scaled sum of eps: {scaled_sum_eps:.4f}")
> print(f"Scaled sum of (t/T)eps: {scaled_sum_t_eps:.4f}")
>
> #histogramas
> num_simulations = 1000
> scaled_sum_eps_simulations = []
> scaled_sum_t_eps_simulations = []
> for _ in range(num_simulations):
>    eps_sim = np.random.normal(0, np.sqrt(sigma_sq), T)
>    scaled_sum_eps_simulations.append((1/np.sqrt(T)) * np.sum(eps_sim))
>    scaled_sum_t_eps_simulations.append((1/np.sqrt(T)) * np.sum((np.arange(1, T + 1)/T) * eps_sim))
>
> plt.figure(figsize=(10, 5))
> plt.subplot(1, 2, 1)
> plt.hist(scaled_sum_eps_simulations, bins=30, density=True, alpha=0.6, label='Simulated')
> x = np.linspace(-4, 4, 100)
> plt.plot(x, (1/np.sqrt(2*np.pi*sigma_sq))*np.exp(-x**2/(2*sigma_sq)), 'r', label='N(0,1)')
> plt.title("Scaled sum of eps")
> plt.legend()
>
> plt.subplot(1, 2, 2)
> plt.hist(scaled_sum_t_eps_simulations, bins=30, density=True, alpha=0.6, label='Simulated')
> x = np.linspace(-1, 1, 100)
> plt.plot(x, (1/np.sqrt(2*np.pi*(sigma_sq/3)))*np.exp(-x**2/(2*(sigma_sq/3))), 'r', label='N(0,1/3)')
> plt.title("Scaled sum of (t/T)eps")
> plt.legend()
> plt.tight_layout()
> plt.show()
> ```
> Os histogramas das amostras simuladas mostram que ambos os termos se aproximam de distribui√ß√µes normais. O primeiro, com uma vari√¢ncia de $\sigma^2$, e o segundo com uma vari√¢ncia de $\sigma^2/3$.

Analisando o primeiro termo, $\frac{1}{\sqrt{T}}\sum_{t=1}^T \epsilon_t$,  pelo Teorema do Limite Central (TLC), sabemos que sua distribui√ß√£o assint√≥tica √© normal, com m√©dia zero e vari√¢ncia $\sigma^2$. Formalmente:
$$ \frac{1}{\sqrt{T}}\sum_{t=1}^T \epsilon_t \overset{d}{\longrightarrow} N(0, \sigma^2) $$

O segundo termo, $\frac{1}{\sqrt{T}}\sum_{t=1}^T \frac{t}{T} \epsilon_t$, representa a soma ponderada das inova√ß√µes, onde o peso de cada inova√ß√£o √© dado por $\frac{t}{T}$. Observe que esta express√£o pode ser reescrita como
$$\frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t$$
Esta sequ√™ncia tamb√©m converge para uma distribui√ß√£o normal, pois √© uma sequ√™ncia de diferen√ßas martingale. Para determinar a vari√¢ncia limite, calculamos:
$$Var\left( \frac{1}{\sqrt{T}}\sum_{t=1}^T \frac{t}{T} \epsilon_t \right) = E\left[ \left( \frac{1}{\sqrt{T}}\sum_{t=1}^T \frac{t}{T} \epsilon_t \right)^2 \right] = \frac{1}{T} \sum_{t=1}^T \frac{t^2}{T^2} E[\epsilon_t^2] = \frac{\sigma^2}{T^3} \sum_{t=1}^T t^2$$
Sabemos que $\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6}$, ent√£o:
$$ \frac{\sigma^2}{T^3} \sum_{t=1}^T t^2 = \sigma^2 \frac{T(T+1)(2T+1)}{6T^3} = \sigma^2 \frac{2T^3+3T^2+T}{6T^3} = \sigma^2 \left( \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2} \right) $$
Tomando o limite quando $T$ tende ao infinito:
$$ \lim_{T \to \infty} \frac{\sigma^2}{T^3} \sum_{t=1}^T t^2 = \frac{\sigma^2}{3} $$
Portanto, a vari√¢ncia limite do segundo termo √© $\sigma^2/3$ [^21]. Assim,  o segundo termo tem a seguinte distribui√ß√£o limite:
$$ \frac{1}{\sqrt{T}}\sum_{t=1}^T \frac{t}{T} \epsilon_t \overset{d}{\longrightarrow} N(0, \sigma^2/3) $$

**Lema 1:** Se $\{\epsilon_t\}$ √© uma sequ√™ncia de vari√°veis aleat√≥rias i.i.d. com m√©dia zero e vari√¢ncia $\sigma^2$, ent√£o a sequ√™ncia $\{\frac{t}{T}\epsilon_t\}$ √© uma sequ√™ncia de diferen√ßas martingale, e a sua vari√¢ncia converge para $\sigma^2/3$ quando $T \rightarrow \infty$.

*Prova:*
I.  Para demonstrar que $\{\frac{t}{T}\epsilon_t\}$ √© uma sequ√™ncia de diferen√ßas martingale, devemos mostrar que:
$$E\left(\frac{t}{T} \epsilon_t | \epsilon_{t-1}, \epsilon_{t-2}, \ldots, \epsilon_1 \right) = 0$$
II. Como $\epsilon_t$ √© i.i.d. com m√©dia zero, temos que:
$$E\left(\frac{t}{T} \epsilon_t | \epsilon_{t-1}, \epsilon_{t-2}, \ldots, \epsilon_1 \right) = \frac{t}{T}E(\epsilon_t) = \frac{t}{T} \cdot 0 = 0$$
III. Para encontrar a vari√¢ncia limite, calculamos:
$$Var\left( \frac{1}{T^{3/2}}\sum_{t=1}^T t \epsilon_t \right) = \frac{1}{T^3} \sum_{t=1}^T t^2 E[\epsilon_t^2] = \frac{\sigma^2}{T^3} \sum_{t=1}^T t^2$$
IV. Sabemos que $\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6}$, ent√£o:
$$ \frac{\sigma^2}{T^3} \sum_{t=1}^T t^2 = \sigma^2 \frac{T(T+1)(2T+1)}{6T^3} = \sigma^2 \left( \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2} \right) $$
V. Tomando o limite quando $T$ tende ao infinito:
$$ \lim_{T \to \infty} \frac{\sigma^2}{T^3} \sum_{t=1}^T t^2 = \frac{\sigma^2}{3} $$
Portanto, a vari√¢ncia limite do termo $\frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t$ √© $\frac{\sigma^2}{3}$. $\blacksquare$

O pr√≥ximo passo √© demonstrar que a combina√ß√£o linear destes dois termos tamb√©m √© assintoticamente gaussiana. Qualquer combina√ß√£o linear dos termos $\frac{1}{\sqrt{T}}\sum_{t=1}^T \epsilon_t$ e $\frac{1}{\sqrt{T}}\sum_{t=1}^T \frac{t}{T} \epsilon_t$ pode ser expressa como:
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^T \left( \lambda_1 + \lambda_2 \frac{t}{T} \right) \epsilon_t $$
onde $\lambda_1$ e $\lambda_2$ s√£o constantes arbitr√°rias.
Este resultado segue diretamente do Teorema do Limite Central para sequ√™ncias de diferen√ßas martingale, desde que a vari√¢ncia da combina√ß√£o linear satisfa√ßa a condi√ß√£o de Lindeberg-Feller. Para ver que isso acontece, notamos que a vari√¢ncia dessa combina√ß√£o linear √© dada por:
$$ Var\left( \frac{1}{\sqrt{T}} \sum_{t=1}^T \left( \lambda_1 + \lambda_2 \frac{t}{T} \right) \epsilon_t \right) = \frac{1}{T} \sum_{t=1}^T \left( \lambda_1^2 + 2\lambda_1 \lambda_2 \frac{t}{T} + \lambda_2^2 \frac{t^2}{T^2} \right) Var(\epsilon_t) $$
$$  = \frac{\sigma^2}{T} \sum_{t=1}^T \left( \lambda_1^2 + 2\lambda_1 \lambda_2 \frac{t}{T} + \lambda_2^2 \frac{t^2}{T^2} \right) $$
Usando os resultados anteriores [^SECTION_PLACEHOLDER], temos que:
$$ \lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^T  \left( \lambda_1^2 + 2\lambda_1 \lambda_2 \frac{t}{T} + \lambda_2^2 \frac{t^2}{T^2} \right) = \lambda_1^2 + \lambda_1 \lambda_2 + \frac{\lambda_2^2}{3} $$
Assim, a vari√¢ncia da combina√ß√£o linear converge para um valor finito e positivo.

**Teorema 1:** Sob as premissas padr√µes sobre $\epsilon_t$, o vetor $\begin{bmatrix} (1/\sqrt{T})\sum_{t=1}^T \epsilon_t \\ (1/\sqrt{T})\sum_{t=1}^T (t/T) \epsilon_t \end{bmatrix}$ converge para uma distribui√ß√£o normal bivariada, com m√©dia zero e matriz de covari√¢ncia $\sigma^2 Q$ onde:
$$ Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} $$

*Prova:*
I. Do Lema 1, temos que $\frac{1}{\sqrt{T}}\sum_{t=1}^T \epsilon_t \overset{d}{\longrightarrow} N(0, \sigma^2)$ e $\frac{1}{\sqrt{T}}\sum_{t=1}^T \frac{t}{T} \epsilon_t \overset{d}{\longrightarrow} N(0, \sigma^2/3)$
II. Al√©m disso, o lema demonstra que $\frac{1}{\sqrt{T}}\sum_{t=1}^T \frac{t}{T} \epsilon_t$ √© uma sequ√™ncia de diferen√ßas martingale.
III. Calculando a covari√¢ncia entre os dois termos:
$$ Cov\left( \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t , \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \right) = \frac{1}{T} \sum_{t=1}^T \frac{t}{T} E[\epsilon_t^2] = \frac{\sigma^2}{T^2} \sum_{t=1}^T t = \sigma^2 \frac{T(T+1)}{2T^2} $$
IV. Tomando o limite quando $T$ tende ao infinito:
$$ \lim_{T \to \infty} Cov\left( \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t , \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \right) = \frac{\sigma^2}{2} $$
V.  Qualquer combina√ß√£o linear dos dois termos √© dada por
$$\frac{1}{\sqrt{T}} \sum_{t=1}^T (\lambda_1 + \lambda_2\frac{t}{T})\epsilon_t$$
onde $\lambda_1$ e $\lambda_2$ s√£o constantes arbitr√°rias.
VI. Como a vari√¢ncia dessa combina√ß√£o converge para $\sigma^2 (\lambda_1^2 + \lambda_1\lambda_2 + \frac{\lambda_2^2}{3})$ pelo resultado apresentado acima, e a sequ√™ncia √© uma diferen√ßa martingale, pelo Teorema do Limite Central para sequ√™ncias de diferen√ßas martingale, temos que a distribui√ß√£o assint√≥tica da combina√ß√£o linear √© gaussiana, com m√©dia zero e vari√¢ncia igual ao limite da vari√¢ncia.
VII. O resultado segue pelo Teorema do Limite Central Multivariado [^21, ^22]. Portanto, a distribui√ß√£o limite conjunta √© dada por:
$$ \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix} \overset{d}{\longrightarrow} N\left( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \sigma^2  \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} \right) $$
$\blacksquare$

**Teorema 1.1:** Sob as mesmas premissas do Teorema 1, e dado um vetor de constantes $c = \begin{bmatrix} c_1 \\ c_2 \end{bmatrix}$, a combina√ß√£o linear
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^T (c_1 + c_2 \frac{t}{T}) \epsilon_t $$
converge em distribui√ß√£o para uma normal com m√©dia zero e vari√¢ncia dada por $\sigma^2(c_1^2 + c_1c_2 + \frac{c_2^2}{3})$.

*Prova:*
I. Pelo Teorema 1, o vetor $\begin{bmatrix} (1/\sqrt{T})\sum_{t=1}^T \epsilon_t \\ (1/\sqrt{T})\sum_{t=1}^T (t/T) \epsilon_t \end{bmatrix}$ converge para uma distribui√ß√£o normal bivariada com m√©dia zero e matriz de covari√¢ncia $\sigma^2 Q$.
II. A combina√ß√£o linear pode ser escrita como o produto escalar do vetor de constantes $c$ e o vetor de vari√°veis aleat√≥rias:
$$ c^T \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix} = \frac{1}{\sqrt{T}} \sum_{t=1}^T (c_1 + c_2 \frac{t}{T}) \epsilon_t $$
III. Pela propriedade de combina√ß√£o linear de vari√°veis normais, essa combina√ß√£o √© tamb√©m uma vari√°vel normal, com m√©dia dada por $c^T \begin{bmatrix} 0 \\ 0 \end{bmatrix} = 0$.
IV. A vari√¢ncia da combina√ß√£o linear √© dada por:
$$ Var\left(c^T \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix} \right) = c^T (\sigma^2 Q) c = \sigma^2 c^T Q c = \sigma^2 \begin{bmatrix} c_1 & c_2 \end{bmatrix} \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} \begin{bmatrix} c_1 \\ c_2 \end{bmatrix} $$
$$ = \sigma^2 (c_1^2 + c_1c_2 + \frac{c_2^2}{3}) $$
V. Portanto, a combina√ß√£o linear converge em distribui√ß√£o para uma normal com m√©dia zero e vari√¢ncia $\sigma^2(c_1^2 + c_1c_2 + \frac{c_2^2}{3})$. $\blacksquare$

Este resultado √© fundamental para a an√°lise assint√≥tica do modelo, uma vez que demonstra como os termos de inova√ß√µes rescalonados se comportam. O pr√≥ximo passo √© analisar como estes termos se combinam para formar a distribui√ß√£o limite dos estimadores de $\alpha$ e $\delta$.

> üí° **Exemplo Num√©rico:** Para ilustrar o Teorema 1, vamos simular o vetor $\begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix}$ para diferentes valores de $T$ e verificar que a distribui√ß√£o se aproxima da distribui√ß√£o normal bivariada com matriz de covari√¢ncia $\sigma^2 Q$, onde $Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$. Seja  $\sigma^2=1$:
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy.stats import multivariate_normal
>
> def simulate_scaled_innovation_terms(T, sigma_sq):
>  eps = np.random.normal(0, np.sqrt(sigma_sq), T)
>  sum_eps = np.sum(eps)
>  sum_t_eps = np.sum((np.arange(1, T + 1) / T) * eps)
>  return np.array([(1 / np.sqrt(T)) * sum_eps, (1 / np.sqrt(T)) * sum_t_eps])
>
> T_values = [100, 1000, 10000]
> num_simulations = 1000
> sigma_sq = 1
>
> Q = np.array([[1, 1/2], [1/2, 1/3]])
>
> for T in T_values:
>   simulated_innovations = np.array([simulate_scaled_innovation_terms(T, sigma_sq) for _ in range(num_simulations)])
>   mean_simulated = np.mean(simulated_innovations, axis=0)
>   cov_simulated = np.cov(simulated_innovations, rowvar=False)
>   expected_cov = sigma_sq * Q
>
>   print(f"T = {T}:")
>   print(f"Mean of scaled terms (should be close to zero): {mean_simulated}")
>   print(f"Covariance of scaled terms:\n{cov_simulated}")
>   print(f"Expected Covariance (sigma^2*Q):\n{expected_cov}")
>   print("-----")
>
>   x, y = np.mgrid[-4:4:.1, -4:4:.1]
>   pos = np.dstack((x, y))
>   rv = multivariate_normal(mean=[0,0], cov=expected_cov)
>
>   plt.figure(figsize=(8, 6))
>   plt.subplot(1, 1, 1)
>   plt.contourf(x, y, rv.pdf(pos), cmap='viridis')
>   plt.scatter(simulated_innovations[:, 0], simulated_innovations[:, 1], marker='.', alpha=0.5, label='Simulated', color = 'red')
>   plt.title(f"Simulated vs Asymptotic Distribution (T = {T})")
>   plt.xlabel("Scaled sum of eps")
>   plt.ylabel("Scaled sum of (t/T)eps")
>   plt.legend()
>   plt.show()
>
> ```
>
> O c√≥digo simula os termos $\frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t$ e $\frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t$ para diferentes valores de $T$. Ao aumentarmos $T$, vemos que a distribui√ß√£o das simula√ß√µes se aproxima da normal multivariada esperada com m√©dia zero e matriz de covari√¢ncia $\sigma^2 Q$, confirmando o Teorema 1.

### Conclus√£o
Sob as premissas padr√£o sobre os erros, o vetor formado por $\frac{1}{\sqrt{T}}\sum_{t=1}^T \epsilon_t$ e $\frac{1}{\sqrt{T}}\sum_{t=1}^T \frac{t}{T} \epsilon_t$  √© assintoticamente gaussiano. A vari√¢ncia do √∫ltimo termo converge para $\sigma^2/3$, e a covari√¢ncia entre os dois termos converge para $\sigma^2/2$.  Al√©m disso, uma combina√ß√£o linear desses termos tamb√©m possui uma distribui√ß√£o assint√≥tica gaussiana. Este resultado √© um passo fundamental para estabelecer a distribui√ß√£o limite dos estimadores OLS rescalonados em modelos com tend√™ncias temporais determin√≠sticas, e ser√° utilizado na pr√≥xima se√ß√£o para concluir a an√°lise assint√≥tica.

### Refer√™ncias
[^1]: Se√ß√£o "Processes with Deterministic Time Trends" do cap√≠tulo 16.
[^2]: Se√ß√£o 16.1, equa√ß√£o [16.1.1].
[^3]: Se√ß√£o 16.1, equa√ß√£o [16.1.2].
[^4]: Se√ß√£o 16.1, equa√ß√£o [16.1.3].
[^5]: Se√ß√£o 16.1, equa√ß√£o [16.1.4].
[^6]: Se√ß√£o 16.1, equa√ß√£o [16.1.5].
[^7]: Se√ß√£o 16.1, equa√ß√£o [16.1.6].
[^8]: Se√ß√£o 16.1, equa√ß√£o [16.1.6].
[^9]: Se√ß√£o 16.1, equa√ß√£o [16.1.7].
[^10]: Se√ß√£o 16.1, par√°grafo ap√≥s a equa√ß√£o [16.1.8].
[^11]: Se√ß√£o 16.1, equa√ß√£o [16.1.9].
[^12]: Se√ß√£o 16.1, equa√ß√£o [16.1.10].
[^13]: Se√ß√£o 16.1, equa√ß√£o [16.1.11].
[^14]: Se√ß√£o 16.1, equa√ß√£o [16.1.12].
[^15]: Se√ß√£o 16.1, par√°grafo ap√≥s a equa√ß√£o [16.1.16].
[^16]: Se√ß√£o 16.1, par√°grafo ap√≥s a equa√ß√£o [16.1.17].
[^17]: Se√ß√£o 16.1, equa√ß√£o [16.1.17].
[^18]: Se√ß√£o 16.1, equa√ß√£o [16.1.18].
[^19]: Se√ß√£o 16.1, equa√ß√£o [16.1.19].
[^20]: Se√ß√£o 16.1, equa√ß√£o [16.1.20].
[^21]: Se√ß√£o 16.1, par√°grafo ap√≥s a equa√ß√£o [16.1.21].
[^22]: Se√ß√£o 16.1, equa√ß√£o [16.1.23].
[^23]: Se√ß√£o 16.1, equa√ß√£o [16.1.25].
[^24]: Se√ß√£o 16.1, equa√ß√£o [16.1.26] e [16.1.27].
[^SECTION_PLACEHOLDER]: Se√ß√£o "Distribui√ß√£o Assint√≥tica das Estimativas OLS do Modelo de Tend√™ncia Temporal Simples".
[^SECTION_PLACEHOLDER]: Se√ß√£o "A Matriz de Ajuste das Taxas de Converg√™ncia e a Distribui√ß√£o Assint√≥tica".
[^SECTION_PLACEHOLDER]: Se√ß√£o "As Somas dos Regressores e as Taxas de Converg√™ncia em Modelos de Tend√™ncia Temporal".
<!-- END -->
