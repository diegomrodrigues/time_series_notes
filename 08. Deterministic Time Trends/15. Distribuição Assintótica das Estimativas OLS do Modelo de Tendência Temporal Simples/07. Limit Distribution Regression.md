## A Abordagem para Distribui√ß√µes Assint√≥ticas com Tend√™ncias Temporais Determin√≠sticas

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre modelos de tend√™ncia temporal determin√≠stica, e baseando-se na an√°lise da necessidade da matriz de ajuste $Y_T$ [^SECTION_PLACEHOLDER] e no comportamento das somas dos regressores [^SECTION_PLACEHOLDER], esta se√ß√£o explora em detalhe a abordagem para derivar a distribui√ß√£o limite para regress√µes com vari√°veis explicativas que incluem tend√™ncias temporais determin√≠sticas. Particularmente, detalharemos a raz√£o pela qual a multiplica√ß√£o por $\sqrt{T}$ n√£o √© suficiente nesse contexto, a necessidade de ajustes nas taxas de converg√™ncia, e o m√©todo correto para obter uma distribui√ß√£o limite n√£o degenerada para os estimadores OLS.

### Conceitos Fundamentais
Como vimos anteriormente [^SECTION_PLACEHOLDER], a abordagem usual para encontrar a distribui√ß√£o limite de uma regress√£o com vari√°veis explicativas estacion√°rias envolve multiplicar o termo do desvio da estimativa OLS por $\sqrt{T}$ [^9]. Esta multiplica√ß√£o, no caso de vari√°veis estacion√°rias, resulta em um termo que converge para uma matriz n√£o-singular e um termo envolvendo as inova√ß√µes que converge para uma distribui√ß√£o normal. Entretanto, esta abordagem n√£o se aplica diretamente a regress√µes com tend√™ncias temporais determin√≠sticas, como a que estamos analisando, $y_t = \alpha + \delta t + \epsilon_t$ [^2].

> üí° **Exemplo Num√©rico:** Para ilustrar por que a abordagem usual n√£o funciona, vamos considerar um exemplo onde $T = 100$ e analisemos o comportamento dos termos da matriz $\frac{1}{T} \sum_{t=1}^T x_t x_t'$:
>  
> $\frac{1}{T} \sum_{t=1}^T x_t x_t' = \frac{1}{100} \begin{bmatrix} \sum_{t=1}^{100} 1 & \sum_{t=1}^{100} t \\ \sum_{t=1}^{100} t & \sum_{t=1}^{100} t^2 \end{bmatrix} = \frac{1}{100} \begin{bmatrix} 100 & 5050 \\ 5050 & 338350 \end{bmatrix} = \begin{bmatrix} 1 & 50.5 \\ 50.5 & 3383.5 \end{bmatrix} $
>
> Note que os elementos fora da diagonal principal da matriz crescem com o aumento de $T$. Enquanto o elemento (1,1) converge para uma constante (1), os elementos (1,2) e (2,1) crescem linearmente com $T$, e o elemento (2,2) cresce quadraticamente com $T$, o que impede a converg√™ncia para uma matriz n√£o-singular, um requisito para a aplica√ß√£o das abordagens assint√≥ticas usuais.

A raz√£o fundamental para esta incompatibilidade √© que os termos $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$ crescem a taxas diferentes. Como vimos anteriormente [^SECTION_PLACEHOLDER],  o termo dominante de $\sum_{t=1}^T t$ √© $T^2/2$ e o termo dominante de $\sum_{t=1}^T t^2$ √© $T^3/3$ [^11, ^12, ^13, ^14]. Ao contr√°rio de vari√°veis estacion√°rias, onde as somas dos regressores s√£o da ordem de $T$, as somas dos regressores no modelo com tend√™ncia temporal s√£o de ordens mais elevadas.

Para demonstrar formalmente essa diferen√ßa, vamos revisitar a express√£o do desvio da estimativa OLS [^7]:
$$ (b_T - \beta) = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t $$
onde $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$ e $\beta = \begin{bmatrix} \alpha \\ \delta \end{bmatrix}$. Ao substituir as somas, obtemos:
$$ \begin{bmatrix} \hat{\alpha}_T - \alpha \\ \hat{\delta}_T - \delta \end{bmatrix} = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix}^{-1} \begin{bmatrix} \sum_{t=1}^T \epsilon_t \\ \sum_{t=1}^T t \epsilon_t \end{bmatrix} $$
Se multiplicarmos ambos os lados por $\sqrt{T}$, obtemos:
$$ \sqrt{T} \begin{bmatrix} \hat{\alpha}_T - \alpha \\ \hat{\delta}_T - \delta \end{bmatrix} = \sqrt{T} \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix}^{-1} \begin{bmatrix} \sum_{t=1}^T \epsilon_t \\ \sum_{t=1}^T t \epsilon_t \end{bmatrix} $$
Como o determinante da matriz $\sum_{t=1}^T x_t x_t'$ √© da ordem de $T^3$ [^SECTION_PLACEHOLDER], a sua inversa ser√° da ordem de $T^{-3}$, e os elementos dessa matriz inversa ser√£o da ordem de $T^{-1}$, $T^{-2}$ e $T^{-3}$.  Assim, quando multiplicamos por $\sqrt{T}$, a matriz $(1/T)\sum x_t x_t'$ n√£o converge para uma matriz n√£o-singular [^15].

> üí° **Exemplo Num√©rico:** Para ilustrar o problema com a converg√™ncia, vamos simular um conjunto de dados com uma tend√™ncia linear e calcular o desvio do estimador OLS para diferentes valores de T. Vamos definir $\alpha = 2$, $\delta = 0.5$, e $\sigma^2 = 1$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def generate_data(T, alpha, delta, sigma_sq):
>   t = np.arange(1, T + 1)
>   epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
>   y = alpha + delta * t + epsilon
>   X = np.column_stack((np.ones(T), t))
>   return X, y
>
> def estimate_ols(X, y):
>    return np.linalg.inv(X.T @ X) @ X.T @ y
>
> def calculate_deviations(T_values, alpha, delta, sigma_sq):
>    alpha_deviations = []
>    delta_deviations = []
>
>    for T in T_values:
>      X, y = generate_data(T, alpha, delta, sigma_sq)
>      beta_hat = estimate_ols(X, y)
>      alpha_hat, delta_hat = beta_hat
>      alpha_deviations.append(alpha_hat - alpha)
>      delta_deviations.append(delta_hat - delta)
>
>    return alpha_deviations, delta_deviations
>
> T_values = [100, 500, 1000, 5000, 10000]
> alpha = 2
> delta = 0.5
> sigma_sq = 1
> alpha_deviations, delta_deviations = calculate_deviations(T_values, alpha, delta, sigma_sq)
>
> plt.figure(figsize=(10, 6))
>
> plt.subplot(1, 2, 1)
> plt.plot(T_values, alpha_deviations, marker='o')
> plt.title("Desvios de Alpha")
> plt.xlabel("T")
> plt.ylabel("$\\hat{\\alpha}_T - \\alpha$")
>
> plt.subplot(1, 2, 2)
> plt.plot(T_values, delta_deviations, marker='o')
> plt.title("Desvios de Delta")
> plt.xlabel("T")
> plt.ylabel("$\\hat{\\delta}_T - \\delta$")
>
> plt.tight_layout()
> plt.show()
>
> ```
>
> As figuras mostram que os desvios de $\hat{\alpha}_T$ e $\hat{\delta}_T$ convergem para zero, mas em diferentes taxas. O problema √© que a converg√™ncia n√£o √© da ordem $\sqrt{T}$, e assim, $\sqrt{T}(\hat{\alpha}_T - \alpha)$ e $\sqrt{T}(\hat{\delta}_T - \delta)$ n√£o convergem para uma distribui√ß√£o normal n√£o-degenerada.
>
> Este exemplo num√©rico ilustra que a abordagem usual de multiplicar por $\sqrt{T}$ n√£o √© suficiente para obter uma distribui√ß√£o limite n√£o degenerada quando h√° tend√™ncias temporais determin√≠sticas.

Para resolver este problema, a abordagem correta envolve um ajuste nas taxas de converg√™ncia, o que √© realizado pela matriz $Y_T$ [^17], definida como:
$$ Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} $$
Esta matriz aplica uma corre√ß√£o, multiplicando $\hat{\alpha}_T$ por $\sqrt{T}$ e $\hat{\delta}_T$ por $T^{3/2}$, que √© essencial para obter distribui√ß√µes assint√≥ticas n√£o degeneradas. Ao multiplicar o desvio da estimativa OLS por $Y_T$, obtemos:
$$ Y_T (b_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t$$
ou, de forma mais detalhada:
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix}^{-1} \begin{bmatrix} \sum_{t=1}^T \epsilon_t \\ \sum_{t=1}^T t \epsilon_t \end{bmatrix} $$
Esta opera√ß√£o equivale a multiplicar o desvio da estimativa por $Y_T$, e tamb√©m pr√© e p√≥s-multiplicar a matriz de somas dos regressores pela matriz $Y_T$. Isto √©, como j√° vimos anteriormente [^SECTION_PLACEHOLDER]:
$$ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \rightarrow Q^{-1} $$
onde a matriz $Q$ √© dada por:
$$ Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} $$
e a sua inversa √© [^SECTION_PLACEHOLDER]:
$$ Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} $$
> üí° **Exemplo Num√©rico:** Considerando $T = 100$, vamos obter numericamente a matriz rescalonada  $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T$.  Neste caso,
> $\sum_{t=1}^{100} 1 = 100$, $\sum_{t=1}^{100} t = \frac{100(101)}{2} = 5050$ e $\sum_{t=1}^{100} t^2 = \frac{100(101)(201)}{6} = 338350$
>
> $\left( \sum_{t=1}^{100} x_t x_t' \right) = \begin{bmatrix} 100 & 5050 \\ 5050 & 338350 \end{bmatrix} $
>
> $\left( \sum_{t=1}^{100} x_t x_t' \right)^{-1}  \approx  \begin{bmatrix} 0.0385 & -0.0057 \\ -0.0057 & 0.0006 \end{bmatrix}$
>
>
> A matriz $Y_{100} = \begin{bmatrix} 10 & 0 \\ 0 & 1000 \end{bmatrix}$.
>
>
> Aplicando o rescalonamento,
> $Y_{100}  \left( \sum_{t=1}^{100} x_t x_t' \right)^{-1} Y_{100}  \approx  \begin{bmatrix} 10 & 0 \\ 0 & 1000 \end{bmatrix} \begin{bmatrix} 0.0385 & -0.0057 \\ -0.0057 & 0.0006 \end{bmatrix} \begin{bmatrix} 10 & 0 \\ 0 & 1000 \end{bmatrix} \approx \begin{bmatrix} 3.85 & -5.7 \\ -5.7 & 5.9 \end{bmatrix}$.
>
> Note que esta matriz j√° est√° se aproximando de $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$, ao mesmo tempo em que a matriz $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ diverge quando $T \to \infty$.

O termo envolvendo as inova√ß√µes tamb√©m √© rescalonado pela matriz $Y_T$:
$$ Y_T \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} \sqrt{T} \sum_{t=1}^T \epsilon_t \\ T^{3/2} \sum_{t=1}^T t \epsilon_t \end{bmatrix} $$
Este termo converge para uma distribui√ß√£o normal multivariada com matriz de covari√¢ncia dada por $\sigma^2 Q$ [^SECTION_PLACEHOLDER],  como demonstrado em [^21, ^22].

**Teorema 1:**  Se $\epsilon_t$ s√£o i.i.d com m√©dia zero e vari√¢ncia $\sigma^2$, ent√£o a distribui√ß√£o assint√≥tica do termo envolvendo as inova√ß√µes rescalonado por $Y_T$ √©:
$$ Y_T \sum_{t=1}^T x_t \epsilon_t \overset{d}{\longrightarrow} N(0, \sigma^2 Q) $$

*Prova:*
I. Expandindo a express√£o, obtemos:
$$ Y_T \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} \sqrt{T} \sum_{t=1}^T \epsilon_t \\ T^{3/2} \sum_{t=1}^T t \epsilon_t \end{bmatrix} $$
II. Podemos reescrever a express√£o como:
$$ \begin{bmatrix} \sqrt{T} \sum_{t=1}^T \epsilon_t \\ T^{3/2} \sum_{t=1}^T t \epsilon_t \end{bmatrix} = \begin{bmatrix} \sqrt{T} \sum_{t=1}^T \epsilon_t \\ \sqrt{T} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix} $$
III. Do Lema 1 [^SECTION_PLACEHOLDER], temos que $\frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \overset{d}{\longrightarrow} N(0, \sigma^2)$ e $\frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t \overset{d}{\longrightarrow} N(0, \frac{\sigma^2}{3})$.
IV. O termo $\frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t$ √© uma sequ√™ncia de diferen√ßas martingale cuja vari√¢ncia converge para $\sigma^2/3$, e sua covari√¢ncia com  $\frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t$ converge para $\sigma^2/2$.
V. Pelo Teorema do Limite Central Multivariado, a express√£o em (III) converge para uma distribui√ß√£o normal com matriz de covari√¢ncia dada por:
$$\sigma^2  \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} = \sigma^2 Q$$
VI. Logo, temos que
$$ Y_T \sum_{t=1}^T x_t \epsilon_t \overset{d}{\longrightarrow} N(0, \sigma^2 Q) $$
$\blacksquare$

**Teorema 1.1:** Se $\epsilon_t$ s√£o i.i.d com m√©dia zero e vari√¢ncia $\sigma^2$, ent√£o a distribui√ß√£o assint√≥tica dos estimadores OLS rescalonados por $Y_T$ √©:
$$ Y_T (b_T - \beta) \overset{d}{\longrightarrow} N(0, \sigma^2 Q^{-1}) $$
*Prova:*
I. Partindo da express√£o do desvio da estimativa OLS rescalonado:
$$Y_T (b_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t$$
II.  Multiplicando e dividindo o termo $\left( \sum_{t=1}^T x_t x_t' \right)^{-1} $ por $Y_T$ e  $Y_T^{-1}$, obtemos:
$$Y_T (b_T - \beta) = \left( Y_T \left( \sum_{t=1}^T x_t x_t' \right) Y_T \right)^{-1} Y_T \sum_{t=1}^T x_t \epsilon_t$$
III. Como j√° estabelecido, temos que $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \rightarrow Q^{-1}$ e $Y_T \sum_{t=1}^T x_t \epsilon_t \overset{d}{\longrightarrow} N(0, \sigma^2 Q)$
IV. Pelo Teorema de Slutsky, podemos combinar os resultados (III) para obter a distribui√ß√£o assint√≥tica de $Y_T (b_T - \beta)$:
$$ Y_T (b_T - \beta) \overset{d}{\longrightarrow} Q^{-1} N(0, \sigma^2 Q) = N(0, \sigma^2 Q^{-1} Q Q^{-1})  = N(0, \sigma^2 Q^{-1}) $$
$\blacksquare$

> üí° **Exemplo Num√©rico:** Vamos simular $Y_T (b_T - \beta)$ para alguns valores de T e comparar a distribui√ß√£o simulada com a distribui√ß√£o assint√≥tica. Definimos $\alpha = 2$, $\delta = 0.5$, e $\sigma^2 = 1$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy.stats import multivariate_normal
>
> def generate_data(T, alpha, delta, sigma_sq):
>  t = np.arange(1, T + 1)
>  epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
>  y = alpha + delta * t + epsilon
>  X = np.column_stack((np.ones(T), t))
>  return X, y
>
> def estimate_ols(X, y):
>    return np.linalg.inv(X.T @ X) @ X.T @ y
>
> def calculate_scaled_deviations(T, alpha, delta, sigma_sq):
>    X, y = generate_data(T, alpha, delta, sigma_sq)
>    beta_hat = estimate_ols(X, y)
>    beta = np.array([alpha, delta])
>    YT = np.array([[np.sqrt(T), 0], [0, T**(3/2)]])
>    return YT @ (beta_hat - beta)
>
>
> T_values = [100, 1000, 10000]
> num_simulations = 1000
> alpha = 2
> delta = 0.5
> sigma_sq = 1
>
> Q_inv = np.array([[4, -6], [-6, 12]])
>
> for T in T_values:
>    simulated_deviations = np.array([calculate_scaled_deviations(T, alpha, delta, sigma_sq) for _ in range(num_simulations)])
>    mean_simulated = np.mean(simulated_deviations, axis=0)
>    cov_simulated = np.cov(simulated_deviations, rowvar=False)
>    expected_cov = sigma_sq * Q_inv
>
>    print(f"T={T}:")
>    print(f"Mean of scaled deviation (should be close to zero): {mean_simulated}")
>    print(f"Covariance of scaled deviation:\n{cov_simulated}")
>    print(f"Expected Covariance (sigma^2*Q^-1):\n{expected_cov}")
>    print("-----")
>
>
>    x, y = np.mgrid[-10:10:.1, -10:10:.1]
>    pos = np.dstack((x, y))
>    rv = multivariate_normal(mean=[0,0], cov=expected_cov)
>
>    plt.figure(figsize=(8, 6))
>    plt.subplot(1, 1, 1)
>    plt.contourf(x, y, rv.pdf(pos), cmap='viridis')
>    plt.scatter(simulated_deviations[:, 0], simulated_deviations[:, 1], marker='.', alpha=0.5, label='Simulated', color='red')
>    plt.title(f"Simulated vs Asymptotic Distribution T = {T}")
>    plt.xlabel(f"Scaled Alpha Deviation")
>    plt.ylabel(f"Scaled Delta Deviation")
>    plt.legend()
>    plt.show()
> ```
>
> O c√≥digo simula o desvio dos estimadores OLS rescalonado e compara com a distribui√ß√£o normal assint√≥tica. O resultado mostra que, para valores maiores de T, as simula√ß√µes se aproximam da distribui√ß√£o assint√≥tica com matriz de covari√¢ncia $\sigma^2 Q^{-1}$.

> üí° **Exemplo Num√©rico:** Vamos agora considerar um exemplo com valores concretos para os par√¢metros, onde $\alpha = 5$, $\delta = 0.2$ e $\sigma^2=0.8$. Vamos supor $T = 150$. Usando o estimador OLS:
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
>
> np.random.seed(42)
>
> T = 150
> alpha = 5
> delta = 0.2
> sigma_sq = 0.8
>
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
> y = alpha + delta * t + epsilon
> X = np.column_stack((np.ones(T), t))
>
> model = sm.OLS(y, X)
> results = model.fit()
>
> print(results.summary())
>
> alpha_hat = results.params[0]
> delta_hat = results.params[1]
>
> print(f"Estimated alpha: {alpha_hat:.4f}")
> print(f"Estimated delta: {delta_hat:.4f}")
>
> ```
> O modelo ajustado usando o statsmodels fornece os seguintes resultados:
>
> ```
> OLS Regression Results
> ==============================================================================
> Dep. Variable:                      y   R-squared:                       0.999
> Model:                            OLS   Adj. R-squared:                  0.999
> Method:                 Least Squares   F-statistic:                 1.374e+05
> Date:                Sat, 15 Jun 2024   Prob (F-statistic):           1.06e-209
> Time:                        16:43:24   Log-Likelihood:                -203.49
> No. Observations:                 150   AIC:                             411.0
> Df Residuals:                     148   BIC:                             417.0
> Df Model:                           1
> Covariance Type:            nonrobust
> ==============================================================================
>                  coef    std err          t      P>|t|      [0.025      0.975]
> ------------------------------------------------------------------------------
> const          5.2043      0.182     28.561      0.000       4.845       5.564
> x1             0.1990      0.002    370.708      0.000       0.198       0.200
> ==============================================================================
> Omnibus:                        0.993   Durbin-Watson:                   1.969
> Prob(Omnibus):                  0.609   Jarque-Bera (JB):                0.989
> Skew:                           0.207   Prob(JB):                        0.610
> Kurtosis:                       2.902   Cond. No.                         174.
> ==============================================================================
> ```
>
> Note que o valor estimado para $\alpha$ √© $\hat{\alpha} = 5.2043$ e o valor estimado para $\delta$ √© $\hat{\delta} = 0.1990$. Os desvios, portanto, s√£o $\hat{\alpha} - \alpha = 0.2043$ e $\hat{\delta} - \delta = -0.0010$.
>
> Podemos calcular os erros padr√£o dos estimadores rescalonados:
>
> $$SE(\sqrt{T}(\hat{\alpha}_T - \alpha)) = \sqrt{4\sigma^2} = \sqrt{4*0.8} \approx 1.788$$
>
> $$SE(T^{3/2}(\hat{\delta}_T - \delta)) = \sqrt{12\sigma^2} = \sqrt{12*0.8} \approx 3.098$$
>
> Para $T = 150$:
> $$\sqrt{T}(\hat{\alpha} - \alpha) = \sqrt{150} (0.2043) \approx 2.504$$
> $$T^{3/2}(\hat{\delta} - \delta) = 150^{3/2} (-0.0010) \approx -1.837$$
>
> Note que os erros padr√£o da distribui√ß√£o assint√≥tica est√£o na ordem de grandeza dos valores simulados. Isto nos permite construir intervalos de confian√ßa e realizar testes de hip√≥teses sobre os par√¢metros do modelo.

Com esta an√°lise, conclu√≠mos que, ao pr√©-multiplicar o desvio da estimativa OLS por $Y_T$, a matriz de somas dos regressores converge para uma matriz n√£o-singular e o termo envolvendo as inova√ß√µes converge para uma distribui√ß√£o normal multivariada. Este √© o passo crucial para obter uma distribui√ß√£o assint√≥tica n√£o degenerada para os estimadores OLS em modelos com tend√™ncias temporais determin√≠sticas.

### Conclus√£o
A abordagem para obter a distribui√ß√£o assint√≥tica em modelos de tend√™ncia temporal determin√≠stica difere significativamente daquela utilizada em modelos com vari√°veis estacion√°rias. A multiplica√ß√£o por $\sqrt{T}$ n√£o √© suficiente devido √†s diferentes taxas de crescimento das somas dos regressores. A matriz de ajuste $Y_T$ surge como a ferramenta essencial para corrigir essas diferen√ßas, garantindo que a matriz de somas dos regressores convirja para uma matriz n√£o singular e o termo envolvendo as inova√ß√µes convirja para uma distribui√ß√£o normal multivariada. Esta abordagem permite obter distribui√ß√µes assint√≥ticas n√£o degeneradas para as estimativas dos par√¢metros $\alpha$ e $\delta$, possibilitando an√°lises estat√≠sticas v√°lidas em modelos com tend√™ncias temporais.

**Corol√°rio 1.1:** A distribui√ß√£o assint√≥tica do estimador de $\alpha$, $\hat{\alpha}_T$, √© dada por:
$$ \sqrt{T}(\hat{\alpha}_T - \alpha) \overset{d}{\longrightarrow} N(0, 4\sigma^2) $$
*Prova:*
I. Do Teorema 1.1, temos que
$$ Y_T (b_T - \beta) \overset{d}{\longrightarrow} N(0, \sigma^2 Q^{-1}) $$
II. Expandindo a express√£o, obtemos:
$$  \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \overset{d}{\longrightarrow} N\left( \begin{bmatrix} 0 \\ 0 \end{bmatrix} , \sigma^2  \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} \right) $$
III. A distribui√ß√£o marginal de $\sqrt{T}(\hat{\alpha}_T - \alpha)$ √© dada pela primeira linha da matriz de covari√¢ncia, que √© $4\sigma^2$. Portanto,
$$ \sqrt{T}(\hat{\alpha}_T - \alpha) \overset{d}{\longrightarrow} N(0, 4\sigma^2) $$
$\blacksquare$

**Corol√°rio 1.2:** A distribui√ß√£o assint√≥tica do estimador de $\delta$, $\hat{\delta}_T$, √© dada por:
$$ T^{3/2}(\hat{\delta}_T - \delta) \overset{d}{\longrightarrow} N(0, 12\sigma^2) $$
*Prova:*
I. Do Teorema 1.1, temos que
$$ Y_T (b_T - \beta) \overset{d}{\longrightarrow} N(0, \sigma^2 Q^{-1}) $$
II. Expandindo a express√£o, obtemos:
$$  \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \overset{d}{\longrightarrow} N\left( \begin{bmatrix} 0 \\ 0 \end{bmatrix} , \sigma^2  \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} \right) $$
III. A distribui√ß√£o marginal de $T^{3/2}(\hat{\delta}_T - \delta)$ √© dada pela segunda linha da matriz de covari√¢ncia, que √© $12\sigma^2$. Portanto,
$$ T^{3/2}(\hat{\delta}_T - \delta) \overset{d}{\longrightarrow} N(0, 12\sigma^2) $$
$\blacksquare$

### Refer√™ncias
[^1]: Se√ß√£o "Processes with Deterministic Time Trends" do cap√≠tulo 16.
[^2]: Se√ß√£o 16.1, equa√ß√£o [16.1.1].
[^3]: Se√ß√£o 16.1, equa√ß√£o [16.1.2].
[^4]: Se√ß√£o 16.1, equa√ß√£o [16.1.3].
[^5]: Se√ß√£o 16.1, equa√ß√£o [16.1.4].
[^6]: Se√ß√£o 16.1, equa√ß√£o [16.1.5].
[^7]: Se√ß√£o 16.1, equa√ß√£o [16.1.6].
[^8]: Se√ß√£o 16.1, equa√ß√£o [16.1.6].
[^9]: Se√ß√£o 16.1, equa√ß√£o [16.1.7].
[^10]: Se√ß√£o 16.1, par√°grafo ap√≥s a equa√ß√£o [16.1.8].
[^11]: Se√ß√£o 16.1, equa√ß√£o [16.1.9].
[^12]: Se√ß√£o 16.1, equa√ß√£o [16.1.10].
[^13]: Se√ß√£o 16.1, equa√ß√£o [16.1.11].
[^14]: Se√ß√£o 16.1, equa√ß√£o [16.1.12].
[^15]: Se√ß√£o 16.1, par√°grafo ap√≥s a equa√ß√£o [16.1.16].
[^16]: Se√ß√£o 16.1, par√°grafo ap√≥s a equa√ß√£o [16.1.17].
[^17]: Se√ß√£o 16.1, equa√ß√£o [16.1.17].
[^18]: Se√ß√£o 16.1, equa√ß√£o [16.1.18].
[^19]: Se√ß√£o 16.1, equa√ß√£o [16.1.19].
[^20]: Se√ß√£o 16.1, equa√ß√£o [16.1.20].
[^21]: Se√ß√£o 16.1, par√°grafo ap√≥s a equa√ß√£o [16.1.21].
[^22]: Se√ß√£o 16.1, equa√ß√£o [16.1.23].
[^23]: Se√ß√£o 16.1, equa√ß√£o [16.1.25].
[^24]: Se√ß√£o 16.1, equa√ß√£o [16.1.26] e [16.1.27].
[^SECTION_PLACEHOLDER]: Se√ß√£o "A Matriz de Ajuste das Taxas de Converg√™ncia e a Distribui√ß√£o Assint√≥tica".
[^SECTION_PLACEHOLDER]: Se√ß√£o "As Somas dos Regressores e as Taxas de Converg√™ncia em Modelos de Tend√™ncia Temporal".
<!-- END -->
