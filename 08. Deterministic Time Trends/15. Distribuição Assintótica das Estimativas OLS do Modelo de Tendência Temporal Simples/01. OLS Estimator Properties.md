## Distribui√ß√£o Assint√≥tica das Estimativas OLS do Modelo de Tend√™ncia Temporal Simples

### Introdu√ß√£o
Este cap√≠tulo trata de processos envolvendo tend√™ncias temporais determin√≠sticas e explora as peculiaridades na estima√ß√£o de coeficientes por Ordinary Least Squares (OLS) quando comparado com modelos que envolvem vari√°veis estacion√°rias. Em particular, esta se√ß√£o detalha a distribui√ß√£o assint√≥tica das estimativas OLS para um modelo de tend√™ncia temporal simples, demonstrando as diferen√ßas nas taxas de converg√™ncia e introduzindo a necessidade de rescalonamento de vari√°veis para acomodar estas diferen√ßas. O foco aqui √© a an√°lise da distribui√ß√£o assint√≥tica dos estimadores dos par√¢metros $\alpha$ e $\delta$, onde $\alpha$ representa o intercepto e $\delta$ a inclina√ß√£o da tend√™ncia temporal no modelo $y_t = \alpha + \delta t + \epsilon_t$.

### Conceitos Fundamentais
O modelo de tend√™ncia temporal simples, dado por $y_t = \alpha + \delta t + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco, serve como ponto de partida para an√°lise. Sob as premissas cl√°ssicas de regress√£o, que incluem $\epsilon_t \sim N(0, \sigma^2)$, o modelo satisfaz as condi√ß√µes para aplica√ß√£o de estat√≠sticas OLS padr√£o. Contudo, as distribui√ß√µes assint√≥ticas das estimativas dos coeficientes n√£o podem ser calculadas da mesma forma que em regress√µes com vari√°veis estacion√°rias devido √†s diferentes taxas de converg√™ncia.

Para melhor compreens√£o, o modelo √© reescrito na forma padr√£o de regress√£o:
$$y_t = x_t'\beta + \epsilon_t$$
onde $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$ e $\beta = \begin{bmatrix} \alpha \\ \delta \end{bmatrix}$.
A estimativa OLS de $\beta$, denotada por $b_T$, com base em uma amostra de tamanho $T$, √© dada por:
$$b_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_t$$
O desvio da estimativa OLS do seu valor verdadeiro, como derivado anteriormente, √© expresso por:
$$(b_T - \beta) = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t$$
Para encontrar a distribui√ß√£o limite de uma regress√£o com vari√°veis explicativas estacion√°rias, a abordagem usual era multiplicar por $\sqrt{T}$, resultando em:
$$\sqrt{T}(b_T - \beta) = \left( \frac{1}{T} \sum_{t=1}^T x_t x_t' \right)^{-1} \left( \frac{1}{\sqrt{T}} \sum_{t=1}^T x_t \epsilon_t \right)$$
A abordagem usual assumia que $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ convergia em probabilidade para uma matriz n√£o singular Q e que $\frac{1}{\sqrt{T}} \sum_{t=1}^T x_t \epsilon_t$ convergia em distribui√ß√£o para uma vari√°vel aleat√≥ria $N(0, \sigma^2 Q)$, implicando $\sqrt{T}(b_T - \beta) \rightarrow N(0, \sigma^2 Q^{-1})$.

Entretanto, essa abordagem n√£o √© v√°lida para tend√™ncias temporais determin√≠sticas. Substituindo $x_t$ e $\beta$ em, obtemos:
$$\begin{bmatrix} \hat{\alpha}_T - \alpha \\ \hat{\delta}_T - \delta \end{bmatrix} = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix}^{-1} \begin{bmatrix} \sum_{t=1}^T \epsilon_t \\ \sum_{t=1}^T t \epsilon_t \end{bmatrix}$$

> üí° **Exemplo Num√©rico:** Vamos considerar um exemplo com T=10.  Ent√£o,
> $\sum_{t=1}^{10} 1 = 10$, $\sum_{t=1}^{10} t = \frac{10(10+1)}{2} = 55$, e $\sum_{t=1}^{10} t^2 = \frac{10(10+1)(2*10+1)}{6} = 385$
> A matriz $\sum_{t=1}^T x_t x_t'$ ser√°:
> $\begin{bmatrix} 10 & 55 \\ 55 & 385 \end{bmatrix}$
> Sua inversa √©:
> $\begin{bmatrix} 10 & 55 \\ 55 & 385 \end{bmatrix}^{-1} = \frac{1}{10*385 - 55*55}\begin{bmatrix} 385 & -55 \\ -55 & 10 \end{bmatrix} = \frac{1}{770}\begin{bmatrix} 385 & -55 \\ -55 & 10 \end{bmatrix} = \begin{bmatrix} 0.5 & -0.0714 \\ -0.0714 & 0.0129 \end{bmatrix}$
> Note que os elementos desta matriz, quando multiplicados por $\frac{1}{T}$, $\frac{1}{T^2}$, $\frac{1}{T^2}$ e $\frac{1}{T^3}$ respectivamente, n√£o convergem para uma matriz n√£o-singular, que demonstra o problema na abordagem usual para s√©ries temporais com tend√™ncia determin√≠stica.

As somas necess√°rias podem ser avaliadas como:
$$\sum_{t=1}^T t = \frac{T(T+1)}{2}$$
$$\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6}$$
Os termos dominantes em $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$ s√£o, respectivamente, $T^2/2$ e $T^3/3$. Assim, a matriz $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ diverge, ao contr√°rio do caso estacion√°rio. Para obter uma matriz convergente, seria necess√°rio dividir por $T^3$ ao inv√©s de $T$.

Para obter distribui√ß√µes limites n√£o degeneradas, $\hat{\alpha}_T$ deve ser multiplicado por $\sqrt{T}$, enquanto $\hat{\delta}_T$ deve ser multiplicado por $T^{3/2}$. Esta transforma√ß√£o pode ser expressa pr√©-multiplicando pela matriz:
$$Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}$$
Resultando em:
$$\begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t =  Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1}  Y_T \sum_{t=1}^T x_t \epsilon_t$$
O primeiro termo na express√£o, com a substitui√ß√£o, converge para:
$$Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T = \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} \begin{bmatrix} \sum 1 & \sum t \\ \sum t & \sum t^2 \end{bmatrix}^{-1} \begin{bmatrix} T^{-1/2} & 0 \\ 0 & T^{-3/2} \end{bmatrix} \rightarrow Q^{-1}$$
onde $$ Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$$

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior com T=10, vamos calcular a matriz $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T$:
> $Y_T = \begin{bmatrix} \sqrt{10} & 0 \\ 0 & 10^{3/2} \end{bmatrix} = \begin{bmatrix} 3.162 & 0 \\ 0 & 31.623 \end{bmatrix}$
> $Y_T^{-1} = \begin{bmatrix} 1/\sqrt{10} & 0 \\ 0 & 1/10^{3/2} \end{bmatrix} = \begin{bmatrix} 0.316 & 0 \\ 0 & 0.0316 \end{bmatrix}$
>
> $\left( \sum_{t=1}^T x_t x_t' \right)^{-1} = \begin{bmatrix} 0.5 & -0.0714 \\ -0.0714 & 0.0129 \end{bmatrix}$ (calculado no exemplo anterior).
>
> Ent√£o, $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1} =  \begin{bmatrix} 3.162 & 0 \\ 0 & 31.623 \end{bmatrix} \begin{bmatrix} 0.5 & -0.0714 \\ -0.0714 & 0.0129 \end{bmatrix} \begin{bmatrix} 0.316 & 0 \\ 0 & 0.0316 \end{bmatrix} = \begin{bmatrix} 0.499 & -0.0226 \\ -0.226 & 0.129 \end{bmatrix}  \begin{bmatrix} 0.316 & 0 \\ 0 & 0.0316 \end{bmatrix} = \begin{bmatrix} 0.158 & -0.000715 \\ -0.0715 & 0.00408 \end{bmatrix}$
>
> Esta matriz, quando $T$ tende para infinito, converge para $Q^{-1}$

O segundo termo converge para uma distribui√ß√£o normal multivariada. O primeiro elemento, $\frac{1}{\sqrt{T}}\sum_{t=1}^T \epsilon_t$, converge para $N(0,\sigma^2)$ pelo Teorema do Limite Central. O segundo elemento, $\frac{1}{\sqrt{T}}\sum_{t=1}^T \frac{t}{T} \epsilon_t$, √© uma sequ√™ncia de diferen√ßas martingale, onde sua vari√¢ncia converge para $\sigma^2/3$. Considerando uma combina√ß√£o linear dos dois elementos, sua distribui√ß√£o limite √© uma normal bivariada, dada por
$$\begin{bmatrix} (1/\sqrt{T}) \sum_{t=1}^T \epsilon_t \\ (1/\sqrt{T}) \sum_{t=1}^T (t/T) \epsilon_t \end{bmatrix} \rightarrow N(0, \sigma^2 Q)$$
Assim, a distribui√ß√£o assint√≥tica das estimativas pode ser calculada e √© dada por:
$$\begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \rightarrow N(0, \sigma^2 Q^{-1})$$

**Teorema 1**  A matriz $Q$ definida √© positiva definida.
*Proof:* A matriz $Q$ √© dada por
$$Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}.$$
Para mostrar que $Q$ √© positiva definida, devemos verificar que todos os seus autovalores s√£o positivos. Ou, equivalentemente, que seus determinantes principais s√£o positivos.
I. O primeiro determinante principal √© 1, que √© positivo.
II. O determinante de $Q$ √© $(1)(1/3) - (1/2)(1/2) = 1/3 - 1/4 = 1/12$, que tamb√©m √© positivo.
III. Como todos os determinantes principais s√£o positivos, $Q$ √© positiva definida. ‚ñ†

**Lema 1** A inversa da matriz $Q$ definida √© dada por
$$Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}.$$
*Proof:* Para encontrar a inversa de $Q$, podemos usar a f√≥rmula para a inversa de uma matriz 2x2:
$$Q^{-1} = \frac{1}{\det(Q)} \begin{bmatrix} 1/3 & -1/2 \\ -1/2 & 1 \end{bmatrix}.$$
I. O determinante de Q √© $\det(Q) = (1)(1/3) - (1/2)(1/2) = 1/3 - 1/4 = 1/12$.
II. Substituindo o determinante na f√≥rmula da inversa, temos:
$$Q^{-1} = \frac{1}{1/12} \begin{bmatrix} 1/3 & -1/2 \\ -1/2 & 1 \end{bmatrix} = 12 \begin{bmatrix} 1/3 & -1/2 \\ -1/2 & 1 \end{bmatrix} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}.$$ ‚ñ†

**Corol√°rio 1.1** A distribui√ß√£o assint√≥tica de $\sqrt{T}(\hat{\alpha}_T - \alpha)$ √© $N(0, 4\sigma^2)$, e a distribui√ß√£o assint√≥tica de $T^{3/2}(\hat{\delta}_T - \delta)$ √© $N(0, 12\sigma^2)$. Al√©m disso, a covari√¢ncia assint√≥tica entre $\sqrt{T}(\hat{\alpha}_T - \alpha)$ e $T^{3/2}(\hat{\delta}_T - \delta)$ √© $-6\sigma^2$.

> üí° **Exemplo Num√©rico:** Suponha que simulamos uma s√©rie temporal com $\alpha = 10$, $\delta = 0.5$, $T = 100$ e $\sigma^2 = 1$. Ao executar a regress√£o OLS, obtemos $\hat{\alpha}_T = 10.2$ e $\hat{\delta}_T = 0.49$.
> Calculando os erros: $\hat{\alpha}_T - \alpha = 0.2$ e $\hat{\delta}_T - \delta = -0.01$.
>
> Conforme o corol√°rio 1.1:
> $\sqrt{T}(\hat{\alpha}_T - \alpha) = \sqrt{100}(0.2) = 2$.  A distribui√ß√£o assint√≥tica √© $N(0, 4\sigma^2) = N(0, 4)$
> $T^{3/2}(\hat{\delta}_T - \delta) = 100^{3/2}(-0.01) = -10$. A distribui√ß√£o assint√≥tica √© $N(0, 12\sigma^2) = N(0, 12)$
> A covari√¢ncia √© $-6\sigma^2 = -6$.
>
> Realizando a regress√£o diversas vezes e calculando os valores de $\sqrt{T}(\hat{\alpha}_T - \alpha)$ e $T^{3/2}(\hat{\delta}_T - \delta)$, vemos que os valores seguem as distribui√ß√µes assint√≥ticas $N(0, 4)$ e $N(0, 12)$ respectivamente.

*Proof:* Pelo Teorema 1 e Lema 1, sabemos que $Q$ √© positiva definida e que $Q^{-1}$ √© dada por
$$Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}.$$
I. Da equa√ß√£o, temos:
$$\begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \rightarrow N \left(0, \sigma^2 \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} \right).$$
II.  A distribui√ß√£o assint√≥tica de $\sqrt{T}(\hat{\alpha}_T - \alpha)$ √© obtida pela primeira entrada da matriz de covari√¢ncia assint√≥tica, resultando em $N(0, 4\sigma^2)$.
III. Similarmente, a distribui√ß√£o assint√≥tica de $T^{3/2}(\hat{\delta}_T - \delta)$ √© obtida pela segunda entrada da matriz, resultando em $N(0, 12\sigma^2)$.
IV. A covari√¢ncia assint√≥tica entre $\sqrt{T}(\hat{\alpha}_T - \alpha)$ e $T^{3/2}(\hat{\delta}_T - \delta)$ √© dada pela entrada fora da diagonal da matriz de covari√¢ncia, resultando em $-6\sigma^2$. ‚ñ†

### Conclus√£o
Este resultado mostra explicitamente que $\hat{\alpha}_T$ e $\hat{\delta}_T$ possuem diferentes taxas de converg√™ncia para seus valores verdadeiros, sendo $\hat{\delta}_T$ superconsistente. A distribui√ß√£o limite de $(\hat{\alpha}_T - \alpha)$ e $(\hat{\delta}_T - \delta)$ √© dada por $N(0, \sigma^2 Q^{-1})$ com uma matriz $Q$ espec√≠fica que reflete as particularidades das somas dos regressores no modelo de tend√™ncia temporal. √â importante salientar que, apesar das diferentes taxas de converg√™ncia, a an√°lise dos testes de hip√≥teses (como o teste t e F) segue a mesma abordagem usual, como ser√° discutido em mais detalhes nas se√ß√µes seguintes.

### Refer√™ncias
[^1]: Se√ß√£o "Processes with Deterministic Time Trends" do cap√≠tulo 16.
[^2]: Se√ß√£o 16.1, equa√ß√£o [16.1.1].
[^3]: Se√ß√£o 16.1, equa√ß√£o [16.1.2].
[^4]: Se√ß√£o 16.1, equa√ß√£o [16.1.3].
[^5]: Se√ß√£o 16.1, equa√ß√£o [16.1.4].
[^6]: Se√ß√£o 16.1, equa√ß√£o [16.1.5].
[^7]: Se√ß√£o 16.1, equa√ß√£o [16.1.6].
[^8]: Se√ß√£o 16.1, equa√ß√£o [16.1.6].
[^9]: Se√ß√£o 16.1, equa√ß√£o [16.1.7].
[^10]: Se√ß√£o 16.1, equa√ß√£o [16.1.8].
[^11]: Se√ß√£o 16.1, equa√ß√£o [16.1.9].
[^12]: Se√ß√£o 16.1, equa√ß√£o [16.1.10].
[^13]: Se√ß√£o 16.1, equa√ß√£o [16.1.11].
[^14]: Se√ß√£o 16.1, equa√ß√£o [16.1.12].
[^15]: Se√ß√£o 16.1, par√°grafo ap√≥s a equa√ß√£o [16.1.16].
[^16]: Se√ß√£o 16.1, par√°grafo ap√≥s a equa√ß√£o [16.1.17].
[^17]: Se√ß√£o 16.1, equa√ß√£o [16.1.17].
[^18]: Se√ß√£o 16.1, equa√ß√£o [16.1.18].
[^19]: Se√ß√£o 16.1, equa√ß√£o [16.1.19].
[^20]: Se√ß√£o 16.1, equa√ß√£o [16.1.20].
[^21]: Se√ß√£o 16.1, par√°grafo ap√≥s a equa√ß√£o [16.1.21].
[^22]: Se√ß√£o 16.1, equa√ß√£o [16.1.23].
[^23]: Se√ß√£o 16.1, equa√ß√£o [16.1.25].
[^24]: Se√ß√£o 16.1, equa√ß√£o [16.1.26] e [16.1.27].
<!-- END -->
