## The Asymptotic Distribution of OLS Estimates for the Transformed Regression

### Introduction
This chapter delves into the asymptotic properties of ordinary least squares (OLS) estimators when applied to a transformed model. As established previously [^1], time series models incorporating deterministic trends necessitate specialized techniques for analyzing their asymptotic behavior. The objective of this section is to ascertain the asymptotic distribution of the OLS estimators within this transformed model, elucidating the distinct convergence rates exhibited by different parameters.

### Fundamental Concepts
The preceding discussion demonstrated that OLS-estimated coefficients in models with deterministic time trends exhibit varying rates of convergence. Coefficients associated with stationary variables converge at a rate of $\sqrt{T}$, while the coefficient corresponding to the time trend converges at a rate of $T^{3/2}$ [^1]. This disparity underscores the necessity for a transformation that isolates components of the coefficient vector, each having distinct convergence properties.

The transformation proposed by Sims, Stock, and Watson (1990) [^1, ^2] entails rewriting the original regression model to segregate stationary terms from time trend components. This transformation allows the model to be expressed as:
$$ y_t = x_t' G' (G')^{-1} \beta + \epsilon_t = [x_t^*]' \beta^* + \epsilon_t $$
where $x_t^*$ are the transformed regressors, and $\beta^*$ is the associated vector of coefficients [^1]. The coefficient vector of the transformed model ($\beta^*$) is related to the original model's coefficient vector ($\beta$) by $\beta^* = [G']^{-1} \beta$ [^1]. The OLS estimator for $\beta^*$ is then:
$$ b^* = \left( \sum_{t=1}^{T} x_t^* [x_t^*]' \right)^{-1} \sum_{t=1}^{T} x_t^* y_t $$
As previously explained [^1], the inherent difficulty in directly analyzing the asymptotic distribution of the OLS estimator in the original model motivates the analysis of this transformed counterpart.  Appendix 16.A demonstrates that
$$ Y_T (b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1}), $$ [^1]
where $Y_T$ is a diagonal matrix defined by the convergence rates:
$$ Y_T = \begin{bmatrix}
\sqrt{T} & 0 & \cdots & 0 & 0 & 0 \\
0 & \sqrt{T} & \cdots & 0 & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & \cdots & \sqrt{T} & 0 & 0 \\
0 & 0 & \cdots & 0 & \sqrt{T} & 0 \\
0 & 0 & \cdots & 0 & 0 & T^{3/2}
\end{bmatrix} $$ [^1]
and $Q^*$ is the limit matrix of the covariance matrix of the transformed regressors, given by:
$$
Q^* = \begin{bmatrix}
\gamma^*_0 & \gamma^*_1 & \cdots & \gamma^*_{p-1} & 0 & 0 \\
\gamma^*_1 & \gamma^*_0 & \cdots & \gamma^*_{p-2} & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
\gamma^*_{p-1} & \gamma^*_{p-2} & \cdots & \gamma^*_0 & 0 & 0 \\
0 & 0 & \cdots & 0 & 1 & 0 \\
0 & 0 & \cdots & 0 & 0 & 1/3
\end{bmatrix}
$$ [^1]
Here, $\gamma^*_j$ are the autocovariances of the stationary components of the transformed regressors. This expression explicitly shows that the coefficients associated with stationary variables ($\phi^*$) converge at a rate of $\sqrt{T}$, while the coefficient associated with the time trend ($Î´^*$) converges at a rate of $T^{3/2}$. The matrix $Y_T$ is constructed based on these convergence rates.

> ðŸ’¡ **Numerical Example:**
>
> Let's consider a simple model with one stationary regressor and a time trend:
> $$ y_t = \phi x_t + \delta t + \epsilon_t $$
> where $x_t$ follows an AR(1) process, $x_t = \rho x_{t-1} + \nu_t$, with $\rho=0.8$, and $\nu_t$ and $\epsilon_t$ are white noise processes with $\sigma_{\nu}^2=1$ and $\sigma_{\epsilon}^2=0.5$.
>
> Let's assume true parameters to be $\phi = 0.6$ and $\delta = 0.1$, and the sample size $T=200$.
> First, we simulate the data using `numpy`:
> ```python
> import numpy as np
>
> # Set seed for reproducibility
> np.random.seed(42)
>
> T = 200
> rho = 0.8
> phi = 0.6
> delta = 0.1
> sigma_nu = 1
> sigma_epsilon = 0.5
>
> # Simulate AR(1) process
> x = np.zeros(T)
> nu = np.random.normal(0, sigma_nu, T)
> for t in range(1, T):
>     x[t] = rho * x[t-1] + nu[t]
>
> # Simulate error term
> epsilon = np.random.normal(0, sigma_epsilon, T)
>
> # Generate time trend
> time_trend = np.arange(1, T+1)
>
> # Generate dependent variable
> y = phi * x + delta * time_trend + epsilon
> ```
> Now, we estimate the model using OLS without transformation. We can create the regressor matrix and use the linear algebra module in `numpy`:
> ```python
> # Create regressor matrix
> X = np.column_stack((x, time_trend))
>
> # Calculate OLS estimator
> b = np.linalg.inv(X.T @ X) @ X.T @ y
>
> print(f"OLS estimate of phi: {b[0]:.4f}")
> print(f"OLS estimate of delta: {b[1]:.4f}")
> ```
> This code will produce estimates around $b = [\hat{\phi}, \hat{\delta}]^T \approx [0.58, 0.115]$. Note that the coefficients have different convergence rates. The matrix $Y_T$ will be:
> $$
> Y_T = \begin{bmatrix} \sqrt{200} & 0 \\ 0 & 200^{3/2} \end{bmatrix} \approx \begin{bmatrix} 14.14 & 0 \\ 0 & 2828.43 \end{bmatrix}
> $$
> We can estimate the autocovariances of $x_t$ to get an estimate of $Q^*$. For example, we can use the sample autocovariance with lag 1, $\hat{\gamma}_1 = \frac{1}{T} \sum_{t=2}^{T} (x_t - \bar{x})(x_{t-1}-\bar{x})$. We will find that $\hat{\gamma}_0 \approx 6$ and $\hat{\gamma}_1 \approx 4.7$, so that:
> $$
> Q^* \approx \begin{bmatrix} 6 & 0 \\ 0 & 1/3 \end{bmatrix}
> $$
> If we now assume a transformation $G' = \begin{bmatrix} 1 & 0 \\ -0.4 & 1 \end{bmatrix}$ (for example, to remove some correlation between stationary regressor and time trend) we can find $\beta^* = [G']^{-1}\beta$,
>  and $b^* = [G']^{-1}b$. This example highlights the different convergence rates and the use of $Y_T$ and $Q^*$.

**Lemma 1** The matrix $Q^*$ is positive definite.

*Proof Strategy:* We demonstrate that the quadratic form $z'Q^*z$ is strictly positive for any non-zero vector $z$. Partition $z$ into $z_1$, $z_2$, and $z_3$ corresponding to the stationary part, time trend, and quadratic trend, respectively. The quadratic form can be expressed as the sum of three positive components, guaranteeing that $Q^*$ is positive definite.

*Proof:*
I. Let $z = [z_1', z_2, z_3]'$ be a non-zero vector, where $z_1$ is a vector of size $p$, and $z_2$ and $z_3$ are scalars.
II. The quadratic form $z'Q^*z$ can be expressed as:
$$z'Q^*z = \begin{bmatrix} z_1' & z_2 & z_3 \end{bmatrix} \begin{bmatrix}
\Gamma^* & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1/3
\end{bmatrix} \begin{bmatrix} z_1 \\ z_2 \\ z_3 \end{bmatrix} $$
    where $\Gamma^*$ is the $p \times p$ autocovariance matrix of the stationary regressors.
III. Expanding this yields:
$$z'Q^*z = z_1' \Gamma^* z_1 + z_2^2 + \frac{1}{3}z_3^2 $$
IV. Since $\Gamma^*$ is the autocovariance matrix of a stationary process, it is positive definite. Therefore, $z_1' \Gamma^* z_1 > 0$ for any non-zero $z_1$.
V. Also, $z_2^2 \geq 0$ and $\frac{1}{3}z_3^2 \geq 0$.
VI. If $z \neq 0$, at least one of $z_1$, $z_2$, or $z_3$ is non-zero, making $z'Q^*z > 0$.
VII. Therefore, $Q^*$ is positive definite. â– 

**Lemma 1.1**
The matrix $Q^*$ is invertible.

*Proof Strategy:* A matrix is invertible if and only if it is positive definite or negative definite. Lemma 1 already established that $Q^*$ is positive definite, therefore, $Q^*$ is also invertible.

*Proof:*
I. From Lemma 1, we know that $Q^*$ is positive definite.
II. A positive definite matrix is always invertible.
III. Therefore, $Q^*$ is invertible. â– 

**Theorem 1.1**  Let $b$ be the OLS estimator and $\beta$ the vector of coefficients of the original model, then
$$
Y_T (b - \beta) \xrightarrow{d} N(0, \sigma^2 [G'Q^*G]^{-1}),
$$
where $G'$ is the transformation matrix defined by $\beta^* = [G']^{-1} \beta$.

*Proof Strategy:* This result derives directly from the asymptotic distribution of the transformed estimator, $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$. The relation $\beta^* = [G']^{-1} \beta$ implies $b^* = [G']^{-1} b$. Thus, $Y_T(b^* - \beta^*)$ can be rewritten in terms of $Y_T(b-\beta)$. This allows obtaining the asymptotic covariance matrix for the original coefficients by multiplying $[Q^*]^{-1}$ by $G$ and $G'$ from the right and left.

*Proof:*
I. We start with the known asymptotic distribution of the transformed estimator: $Y_T (b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$.
II. From the transformation, $\beta^* = [G']^{-1} \beta$ and $b^* = [G']^{-1} b$. Thus, $\beta = G' \beta^*$ and $b = G' b^*$.
III.  Substituting the expressions of $\beta^*$ and $b^*$ in the transformed estimator's asymptotic distribution:
     $$Y_T([G']^{-1}b - [G']^{-1}\beta) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$$
IV. Factoring out $[G']^{-1}$ on the left:
    $$Y_T [G']^{-1} (b - \beta) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$$
V.  Multiplying by $G'$ from the left:
    $$G' Y_T [G']^{-1} (b - \beta) \xrightarrow{d} G'N(0, \sigma^2 [Q^*]^{-1})$$
VI. The term $G' Y_T [G']^{-1}$ can be shown to be equal to $Y_T$. Applying Slutsky's theorem, the limit distribution of $G'Y_T(b^*-\beta^*)$ is the same as the one for $Y_T(b-\beta)$, which is:
    $$Y_T(b-\beta) \xrightarrow{d} G'N(0, \sigma^2 [Q^*]^{-1})$$
VII. Now we analyze the distribution of $G'N(0, \sigma^2[Q^*]^{-1})$.
VIII. Since $Y_T(b^*-\beta^*)\xrightarrow{d}N(0,\sigma^2[Q^*]^{-1})$ and $Y_T(b-\beta) = G'Y_T(b^*-\beta^*)$, the asymptotic distribution of $Y_T(b-\beta)$ is $N(0,\sigma^2G'[Q^*]^{-1}G)$. Since $[G'Q^*G]^{-1} = G'[Q^*]^{-1}G$ we have:
    $$Y_T (b - \beta) \xrightarrow{d} N(0, \sigma^2 [G'Q^*G]^{-1})$$
IX. Therefore, $Y_T(b-\beta)$ converges to a normal distribution with mean 0 and variance covariance matrix given by $\sigma^2[G'Q^*G]^{-1}$. â– 

> ðŸ’¡ **Numerical Example (Continuation):**
>
> Continuing from the previous example, consider that the true parameters are $\beta = [\phi, \delta]^T = [0.6, 0.1]^T$ and the OLS estimated parameters are $b=[\hat{\phi}, \hat{\delta}]^T = [0.58, 0.115]^T$. Thus, $b-\beta = [-0.02, 0.015]^T$.
> We also determined that the matrix $Y_T$ was:
> $$
> Y_T \approx \begin{bmatrix} 14.14 & 0 \\ 0 & 2828.43 \end{bmatrix}
> $$
> We also had $Q^* \approx \begin{bmatrix} 6 & 0 \\ 0 & 1/3 \end{bmatrix}$. Now, let's use the transformation $G' = \begin{bmatrix} 1 & 0 \\ -0.4 & 1 \end{bmatrix}$, for which
> $$[G']^{-1} = \begin{bmatrix} 1 & 0 \\ 0.4 & 1 \end{bmatrix}$$
>
> The matrix $[G'Q^*G]^{-1}$ can be calculated as follows:
> $$
> [G'Q^*G]^{-1} =  \left( \begin{bmatrix} 1 & -0.4 \\ 0 & 1 \end{bmatrix}
> \begin{bmatrix} 6 & 0 \\ 0 & 1/3 \end{bmatrix}
> \begin{bmatrix} 1 & 0 \\ -0.4 & 1 \end{bmatrix} \right)^{-1} =
> \begin{bmatrix} 6.067 & -2 \\ -2 & 3 \end{bmatrix}^{-1} \approx \begin{bmatrix} 0.203 & 0.135 \\ 0.135 & 0.409 \end{bmatrix}
> $$
>
> The asymptotic variance of $Y_T(b-\beta)$ is then $\sigma^2[G'Q^*G]^{-1}$. If we use $\sigma^2 = 0.5$, then the asymptotic variance is approximately:
> $$ \sigma^2[G'Q^*G]^{-1} \approx 0.5 \begin{bmatrix} 0.203 & 0.135 \\ 0.135 & 0.409 \end{bmatrix} \approx \begin{bmatrix} 0.101 & 0.067 \\ 0.067 & 0.204 \end{bmatrix} $$
>
> We have:
> $$Y_T(b-\beta) \approx \begin{bmatrix} 14.14 & 0 \\ 0 & 2828.43 \end{bmatrix} \begin{bmatrix} -0.02 \\ 0.015 \end{bmatrix} = \begin{bmatrix} -0.2828 \\ 42.426 \end{bmatrix} $$
>
> The asymptotic distribution of $Y_T(b-\beta)$ is given by:
> $$ Y_T (b - \beta) \xrightarrow{d} N(0, \sigma^2 [G'Q^*G]^{-1}) = N(0, \begin{bmatrix} 0.101 & 0.067 \\ 0.067 & 0.204 \end{bmatrix}) $$
>
> This example demonstrates that the asymptotic variance-covariance matrix differs from the one obtained in the transformed model and confirms that the parameters in the original model have different convergence rates.

**Corollary 1.1**
The asymptotic covariance matrix of $b$ is given by $Var(b) \approx \sigma^2(Y_T^{-1})[G'Q^*G]^{-1}(Y_T^{-1})$.

*Proof Strategy:* This corollary follows directly from Theorem 1.1 by analyzing the asymptotic distribution of $Y_T(b-\beta)$. By multiplying the asymptotic variance matrix by $Y_T^{-1}$ from both sides, we can obtain the asymptotic variance of the OLS estimator $b$.

*Proof:*
I. From Theorem 1.1, we know that $Y_T (b - \beta) \xrightarrow{d} N(0, \sigma^2 [G'Q^*G]^{-1})$.
II. This implies that the asymptotic variance of $Y_T (b - \beta)$ is $\sigma^2[G'Q^*G]^{-1}$.
III. To obtain the asymptotic variance of $b$, we can multiply the asymptotic variance of $Y_T(b-\beta)$ by $Y_T^{-1}$ from both the right and left:
$$Var(b) \approx  Y_T^{-1} \sigma^2 [G'Q^*G]^{-1} (Y_T^{-1})' $$
IV. Since $Y_T$ is a diagonal matrix, $Y_T^{-1}$ is also diagonal and  $(Y_T^{-1})' = Y_T^{-1}$. Therefore,
$$Var(b) \approx \sigma^2(Y_T^{-1})[G'Q^*G]^{-1}(Y_T^{-1})$$
V. This shows that the asymptotic covariance matrix of $b$ is given by $\sigma^2(Y_T^{-1})[G'Q^*G]^{-1}(Y_T^{-1})$. â– 

> ðŸ’¡ **Numerical Example (Continuation):**
>
> Continuing with the previous example, we found that:
> $$ Y_T^{-1} \approx \begin{bmatrix} 0.0707 & 0 \\ 0 & 0.00035 \end{bmatrix} $$
> and
> $$ [G'Q^*G]^{-1} \approx \begin{bmatrix} 0.203 & 0.135 \\ 0.135 & 0.409 \end{bmatrix} $$
> Then, with $\sigma^2 = 0.5$, the asymptotic variance-covariance matrix of $b$ is:
>
> $$
> Var(b) \approx \sigma^2 (Y_T^{-1})[G'Q^*G]^{-1}(Y_T^{-1}) \approx  0.5 \begin{bmatrix} 0.0707 & 0 \\ 0 & 0.00035 \end{bmatrix}
> \begin{bmatrix} 0.203 & 0.135 \\ 0.135 & 0.409 \end{bmatrix}
> \begin{bmatrix} 0.0707 & 0 \\ 0 & 0.00035 \end{bmatrix}
> $$
>
> $$
> Var(b) \approx
> \begin{bmatrix}
> 0.00051 & 0.00000 \\
> 0.00000 & 0.000000025
> \end{bmatrix}
> $$
>
> This final result confirms the different convergence rates. The asymptotic variance of the stationary coefficient is of order $1/T$, while the variance of the time trend coefficient is of order $1/T^3$.

Theorem 1.1  formalizes that the transformation isolates different convergence rates and the asymptotic distribution of the original parameters can be derived based on the results obtained for the transformed model. Corollary 1.1 allows to compute the asymptotic variance-covariance matrix of the original model's estimator, which is crucial for inference.

### Conclusion
The asymptotic analysis of the OLS estimator within the transformed model reveals a crucial characteristic: coefficients associated with stationary terms and the time trend converge at distinct rates. Specifically, the product of the estimated coefficient vector by the matrix $Y_T$ converges to a normal distribution with zero mean and covariance matrix $\sigma^2 [Q^*]^{-1}$. This is key to statistical inference in time series models with deterministic trends. It allows for hypothesis testing and construction of confidence intervals for model parameters while accounting for the diverse convergence rates. Further, it formalizes the intuition that time trend coefficient converges faster than the other parameters. This analysis builds upon prior concepts [^1], providing the necessary foundation for the exploration of autoregressive models around a deterministic trend.

### References
[^1]: Provided text.
[^2]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113-44.
<!-- END -->
