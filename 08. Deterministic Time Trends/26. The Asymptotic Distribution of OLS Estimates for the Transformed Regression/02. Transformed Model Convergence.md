## The Asymptotic Distribution of OLS Estimates for the Transformed Regression

### Introduction
Expanding on the analysis of models with deterministic trends, the present chapter advances to the analysis of the asymptotic properties of ordinary least squares (OLS) estimators applied to a transformed model. As we saw earlier [^1], time series models with deterministic trends require specific techniques to analyze their asymptotic properties. The goal of this section is to establish the asymptotic distribution of the OLS estimators in the transformed model, revealing the different convergence rates for distinct parameters.

### Fundamental Concepts
The previous section demonstrated that the OLS-estimated coefficients in models with deterministic trends exhibit different convergence rates, with coefficients associated with stationary variables converging at a rate of $\sqrt{T}$ and the coefficient of the time trend converging at a rate of $T^{3/2}$ [^1]. This characteristic motivates the need for a transformation that isolates the components of the coefficient vector that have different convergence rates.

The transformation proposed by Sims, Stock, and Watson (1990) [^1, ^2] involves rewriting the original regression model, isolating stationary terms and time trend terms. This transformation, detailed in the previous section, allows expressing the model in the form
$$ y_t = x_t' G' (G')^{-1} \beta + \epsilon_t = [x_t^*]' \beta^* + \epsilon_t $$
where $x_t^*$ are the transformed regressors and $\beta^*$ is the corresponding vector of coefficients [^1]. The coefficient vector of the transformed model ($\beta^*$) is related to the coefficient vector of the original model ($\beta$) by $\beta^* = [G']^{-1} \beta$ [^1]. The OLS estimator for $\beta^*$ is given by
$$ b^* = \left( \sum_{t=1}^{T} x_t^* [x_t^*]' \right)^{-1} \sum_{t=1}^{T} x_t^* y_t $$
As discussed previously [^1], the difficulty in directly analyzing the asymptotic distribution of the OLS estimator in the original model motivates the analysis of the transformed model. Appendix 16.A of this chapter demonstrates that
$$ Y_T (b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1}), $$ [^1]
where $Y_T$ is a diagonal matrix defined as:
$$ Y_T = \begin{bmatrix}
\sqrt{T} & 0 & \cdots & 0 & 0 & 0 \\
0 & \sqrt{T} & \cdots & 0 & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & \cdots & \sqrt{T} & 0 & 0 \\
0 & 0 & \cdots & 0 & \sqrt{T} & 0 \\
0 & 0 & \cdots & 0 & 0 & T^{3/2}
\end{bmatrix} $$ [^1]
and $Q^*$ is the limit matrix of the covariance matrix of the transformed regressors, given by:
$$
Q^* = \begin{bmatrix}
\gamma^*_0 & \gamma^*_1 & \cdots & \gamma^*_{p-1} & 0 & 0 \\
\gamma^*_1 & \gamma^*_0 & \cdots & \gamma^*_{p-2} & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
\gamma^*_{p-1} & \gamma^*_{p-2} & \cdots & \gamma^*_0 & 0 & 0 \\
0 & 0 & \cdots & 0 & 1 & 0 \\
0 & 0 & \cdots & 0 & 0 & 1/3
\end{bmatrix}
$$ [^1]
Here, $\gamma^*_j$ represents the autocovariances of the stationary part of the transformed regressors. This expression reveals that the coefficients associated with the stationary variables ($\phi^*$) converge at a rate of $\sqrt{T}$, while the coefficient associated with the time trend ($Î´^*$) converges at a rate of $T^{3/2}$.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Let's consider a simple time series model with one stationary regressor ($x_t$) and a linear time trend. Suppose we have $T = 100$ observations.  The true model is:
>
> $$ y_t = 0.5 x_t + 0.2t + \epsilon_t $$
> where $x_t$ is a stationary AR(1) process with autocorrelation $\rho = 0.7$ and $\epsilon_t$ is white noise with variance $\sigma^2 = 1$.
>
> The transformation matrix G' would be such that the transformed regressors are orthogonal. Let's denote $\beta = [\phi, \delta]^T = [0.5, 0.2]^T$. In the transformed model $\beta^* = [\phi^*, \delta^*]^T$.
>
> The $Y_T$ matrix would be a 2x2 diagonal matrix where the first element is $\sqrt{100}=10$ and the second element is $100^{3/2} = 1000$. Thus:
> $$Y_T = \begin{bmatrix} 10 & 0 \\ 0 & 1000 \end{bmatrix}$$
>
>  Let's assume, for simplicity,  the sample autocovariances of the transformed stationary part are $\gamma_0^* = 1$ and $\gamma_1^* = 0.7$. Then $Q^*$ will look like this:
>
> $$ Q^* = \begin{bmatrix} 1 & 0 \\ 0 & 1/3 \end{bmatrix} $$
>
> After running a simulation we get $b^* = [\hat{\phi}^*, \hat{\delta}^*]^T = [0.48, 0.202]^T$.  Then,  $b^* - \beta^* = [-0.02, 0.002]^T$. So $Y_T(b^* - \beta^*)$ will be:
>
> $$Y_T(b^* - \beta^*) = \begin{bmatrix} 10 & 0 \\ 0 & 1000 \end{bmatrix}  \begin{bmatrix} -0.02 \\ 0.002 \end{bmatrix} =  \begin{bmatrix} -0.2 \\ 2 \end{bmatrix}$$
>
> The asymptotic distribution states that $Y_T (b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$. In this case:
>
> $$ \sigma^2 [Q^*]^{-1} = 1 \cdot \begin{bmatrix} 1 & 0 \\ 0 & 3 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 3 \end{bmatrix} $$
>
> So, asymptotically, the first element of $Y_T(b^* - \beta^*)$ is distributed as $N(0, 1)$, while the second element is distributed as $N(0,3)$. This exemplifies how the transformed model allows for different convergence rates. The estimator of the time trend converges much faster.

**Lemma 1**  The matrix $Q^*$ is positive definite.

*Proof Strategy:*  To show that $Q^*$ is positive definite, we need to demonstrate that for any non-zero vector $z$, the quadratic form $z'Q^*z$ is strictly positive. We can partition $z = [z_1', z_2, z_3]'$, where $z_1$ is a vector of size $p$ corresponding to the stationary part, and $z_2$ and $z_3$ correspond to the coefficients associated with the time trend and quadratic trend, respectively. The quadratic form can then be expressed as the sum of three parts. The first part corresponds to the stationary portion and is positive if the autocovariance matrix is positive definite (we will assume that). The second and third parts are $z_2^2$ and $\frac{1}{3}z_3^2$, respectively, which are clearly positive if $z_2$ and $z_3$ are not zero. Thus, the matrix $Q^*$ is positive definite.

*Proof:*
I.  Let $z$ be a non-zero vector partitioned as $z = [z_1', z_2, z_3]'$, where $z_1$ is a vector of size $p$, and $z_2$ and $z_3$ are scalars.
II. The quadratic form $z'Q^*z$ can be expressed as:
$$z'Q^*z = \begin{bmatrix} z_1' & z_2 & z_3 \end{bmatrix} \begin{bmatrix}
\Gamma^* & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1/3
\end{bmatrix} \begin{bmatrix} z_1 \\ z_2 \\ z_3 \end{bmatrix} $$
    where $\Gamma^*$ is the $p \times p$ autocovariance matrix of the stationary regressors, with entries $\gamma^*_j$.
III. This can be expanded as:
$$z'Q^*z = z_1' \Gamma^* z_1 + z_2^2 + \frac{1}{3}z_3^2 $$
IV. Since $\Gamma^*$ is the autocovariance matrix of a stationary process, it is assumed to be positive definite. Therefore, $z_1' \Gamma^* z_1 > 0$ for any non-zero $z_1$.
V. Also, $z_2^2 \geq 0$ and $\frac{1}{3}z_3^2 \geq 0$.
VI.  If $z \neq 0$, at least one of $z_1$, $z_2$, or $z_3$ is non-zero. If $z_1 \neq 0$, then $z_1'\Gamma^* z_1 > 0$. If $z_2 \neq 0$, then $z_2^2 > 0$. If $z_3 \neq 0$, then $\frac{1}{3}z_3^2 > 0$. Thus,  $z'Q^*z > 0$.
VII. Therefore, $Q^*$ is positive definite. â– 
   
**Theorem 1.1** Let $b$ be the OLS estimator of the original model and $\beta$ the vector of coefficients of the original model, then
$$
Y_T (b - \beta) \xrightarrow{d} N(0, \sigma^2 [G'Q^*G]^{-1}),
$$
where $G'$ is the transformation matrix such that $\beta^* = [G']^{-1} \beta$.

*Proof Strategy:* The proof follows directly from the asymptotic distribution of the transformed estimator, $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$. Using the relation $\beta^* = [G']^{-1} \beta$, we have $b^* = [G']^{-1} b$. Thus,
$ Y_T(b^* - \beta^*) = Y_T([G']^{-1}b - [G']^{-1}\beta) = Y_T[G']^{-1}(b-\beta)$.
Multiplying on the left by $G'$ yields:
$G'Y_T(b^* - \beta^*) = G'Y_T[G']^{-1}(b-\beta)$ and
$Y_T(b - \beta) = G'Y_T(b^* - \beta^*)$ and the final result follows from the result for $b^*$ by applying Slutsky's theorem. The asymptotic covariance matrix will be obtained by multiplying $[Q^*]^{-1}$ by $G$ and $G'$ on the right and left, respectively, since $b^* = (G')^{-1}b$.

*Proof:*
I. We know that $Y_T (b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$, where $b^*$ is the OLS estimator for the transformed model and $\beta^*$ is the true parameter vector for the transformed model.
II.  From the transformation, we have $\beta^* = [G']^{-1} \beta$, which implies $\beta = G' \beta^*$. Similarly, $b^* = [G']^{-1} b$, which implies $b = G' b^*$.
III. Substituting $b^*$ and $\beta^*$ in terms of $b$ and $\beta$ in the asymptotic distribution result:
     $$Y_T([G']^{-1}b - [G']^{-1}\beta) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$$
IV. Factoring out $[G']^{-1}$ from the left side gives:
    $$Y_T [G']^{-1} (b - \beta) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$$
V.  Multiplying both sides by $G'$ from the left:
    $$G' Y_T [G']^{-1} (b - \beta) \xrightarrow{d} G'N(0, \sigma^2 [Q^*]^{-1})$$
VI. We can rewrite $G' Y_T [G']^{-1}$ as $(G'Y_T[G']^{-1})(b-\beta)$. Multiplying both sides on the left by $G'$ we get: $$G'Y_T(b^* - \beta^*) = G'Y_T[G']^{-1}(b-\beta)$$ and since $(b^* - \beta^*) = (G')^{-1}(b-\beta)$ this gives us:
$$Y_T(b - \beta) = G'Y_T(b^* - \beta^*)$$
     This implies $G' Y_T [G']^{-1} = Y_T$.
     Therefore we get:
    $$Y_T(b-\beta) \xrightarrow{d} G'N(0, \sigma^2 [Q^*]^{-1})$$
VII. Now we need to analyze the distribution of $G'N(0, \sigma^2[Q^*]^{-1})$.
VIII. Because $Y_T(b^*-\beta^*)\xrightarrow{d}N(0,\sigma^2[Q^*]^{-1})$, and $Y_T(b-\beta) = G'Y_T(b^*-\beta^*)$ by Slutsky's theorem, the asymptotic distribution of $Y_T(b-\beta)$ is given by $N(0,\sigma^2G'[Q^*]^{-1}G)$. Since $[G'Q^*G]^{-1} = G'[Q^*]^{-1}G$ we have:
    $$Y_T (b - \beta) \xrightarrow{d} N(0, \sigma^2 [G'Q^*G]^{-1})$$
IX. Thus, the asymptotic distribution of $Y_T(b-\beta)$ is $N(0, \sigma^2[G'Q^*G]^{-1})$. â– 

> ðŸ’¡ **Exemplo NumÃ©rico (continuaÃ§Ã£o):**
> Let's continue with our previous example. Suppose we want to estimate the original model parameters, and we find $b=[\hat{\phi}, \hat{\delta}]^T = [0.49, 0.199]^T$. Thus, $b-\beta = [-0.01, -0.001]^T$.
>
> The transformation matrix $G$ is such that $\beta = G' \beta^*$.
> Let's assume for this example, we have $G' = \begin{bmatrix} 1 & 0 \\ -0.5 & 1 \end{bmatrix}$. We are making $G$ up to illustrate the example, because to compute it would require us to have the data for the regressors and time series of the model. Thus, in this case $\beta =  \begin{bmatrix} 1 & -0.5 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} \phi^* \\ \delta^* \end{bmatrix} =  \begin{bmatrix} \phi^* - 0.5\delta^* \\ \delta^* \end{bmatrix}$.
>
>
> In this case we would have:
> $$ [G'Q^*G]^{-1} = \begin{bmatrix} 1 & -0.5 \\ 0 & 1 \end{bmatrix}  \begin{bmatrix} 1 & 0 \\ 0 & 3 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ -0.5 & 1 \end{bmatrix} =   \begin{bmatrix} 1.75 & -1.5 \\ -1.5 & 3 \end{bmatrix} $$
>
> Then, the asymptotic variance of $Y_T(b - \beta)$ is $\sigma^2[G'Q^*G]^{-1}$, which equals to $\begin{bmatrix} 1.75 & -1.5 \\ -1.5 & 3 \end{bmatrix}$ in this example. Note that it is different than the asymptotic variance of the transformed model. Now, the asymptotic distribution of $Y_T(b-\beta)$ is given by
>
> $$ Y_T (b - \beta) \xrightarrow{d} N(0, \sigma^2 [G'Q^*G]^{-1}) = N(0, \begin{bmatrix} 1.75 & -1.5 \\ -1.5 & 3 \end{bmatrix})$$
>
> So we see that the parameters in the original model also have different asymptotic distributions, but that is now described by the matrix $[G'Q^*G]^{-1}$.

Moreover, Theorem 1.1 formalizes the intuition that the transformation proposed by Sims, Stock, and Watson allows the asymptotic analysis of OLS-estimated coefficients to be done taking into account the different convergence rates.

### Conclusion
The analysis of the asymptotic distribution of the OLS estimator for the transformed model reveals an important property: the coefficients associated with stationary terms and the time trend coefficient converge to their true versions at different rates. Specifically, the product of the estimated coefficient vector by the matrix $Y_T$ converges to a normal distribution with zero mean and covariance matrix $\sigma^2 [Q^*]^{-1}$. This result is fundamental for statistical inference in time series models with deterministic trends, as it allows us to perform hypothesis tests and construct confidence intervals for the model parameters, taking into account the different convergence rates. Furthermore, it formalizes the intuition that the time trend coefficient converges faster than the other parameters. The analysis carried out here expands the knowledge introduced in previous sections, preparing the ground for the analysis of autoregressive models around a deterministic trend [^1].

### References
[^1]: Provided text.
[^2]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113-44.
<!-- END -->
