## O Redimensionamento dos Estimadores OLS em Modelos de TendÃªncia Temporal

### IntroduÃ§Ã£o

Este capÃ­tulo explora a distribuiÃ§Ã£o assintÃ³tica de estimadores de mÃ­nimos quadrados ordinÃ¡rios (OLS) em modelos com tendÃªncias temporais determinÃ­sticas, com foco especial na necessidade de redimensionamento para acomodar diferentes taxas de convergÃªncia. Como visto anteriormente, estimadores OLS em modelos com tendÃªncias temporais ou raÃ­zes unitÃ¡rias nÃ£o exibem as mesmas propriedades assintÃ³ticas que em modelos com variÃ¡veis estacionÃ¡rias. Em particular, os estimadores de coeficientes associados Ã  tendÃªncia temporal convergem a uma taxa diferente dos outros coeficientes [^1]. Para obter distribuiÃ§Ãµes assintÃ³ticas nÃ£o degeneradas, Ã© necessÃ¡rio aplicar um procedimento de redimensionamento especÃ­fico, que serÃ¡ o foco principal desta seÃ§Ã£o. Este processo Ã© essencial para assegurar a validade da inferÃªncia estatÃ­stica em modelos de sÃ©ries temporais com tendÃªncias.

### Conceitos Fundamentais

No modelo de tendÃªncia temporal simples,
$$y_t = \alpha + \delta t + \epsilon_t$$
onde $\epsilon_t$ Ã© um processo de ruÃ­do branco com mÃ©dia zero e variÃ¢ncia $\sigma^2$, o objetivo Ã© estimar $\alpha$ e $\delta$ utilizando OLS [^1]. Como vimos,  o estimador OLS de $\beta = \begin{bmatrix} \alpha \\ \delta \end{bmatrix}$ Ã© dado por [^2]:
$$b_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_t$$
onde $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$ [^2]. O desvio da estimativa do valor verdadeiro Ã© dado por [^2]:
$$(b_T - \beta) = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t$$
Anteriormente, multiplicamos o desvio $(b_T - \beta)$ pela matriz [^4]:
$$Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}$$
para obter distribuiÃ§Ãµes limites nÃ£o degeneradas, e o novo estimador Ã© dado por:
$$Y_T(b_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t$$
O redimensionamento com $Y_T$ Ã© crucial, pois os termos $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$ crescem a taxas diferentes com o aumento de $T$, como demonstrado previamente: $\sum_{t=1}^T t = \frac{T(T+1)}{2} \approx \frac{T^2}{2}$ e $\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6} \approx \frac{T^3}{3}$ [^3]. Assim, ao contrÃ¡rio dos modelos com variÃ¡veis estacionÃ¡rias onde o escalonamento por $\sqrt{T}$ seria suficiente, a matriz $\sum_{t=1}^T x_t x_t'$ precisa ser escalada por $T^3$ para garantir a convergÃªncia a uma matriz nÃ£o singular [^3].

**ObservaÃ§Ã£o 1:** Ã‰ importante notar que a matriz $\sum_{t=1}^T x_t x_t'$ pode ser expressa explicitamente como:
$$ \sum_{t=1}^T x_t x_t' = \sum_{t=1}^T \begin{bmatrix} 1 \\ t \end{bmatrix} \begin{bmatrix} 1 & t \end{bmatrix} = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix} = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} $$
Esta forma explÃ­cita facilita a visualizaÃ§Ã£o da necessidade do redimensionamento por $T$ e $T^3$ para os elementos da matriz.

**ImportÃ¢ncia do Redimensionamento:**

O redimensionamento dos estimadores OLS Ã© necessÃ¡rio para lidar com as diferentes taxas de convergÃªncia dos estimadores. O coeficiente da tendÃªncia, $\delta$, converge para seu verdadeiro valor a uma taxa de $T^{3/2}$, enquanto o intercepto, $\alpha$, converge a uma taxa de $\sqrt{T}$ [^7]. A matriz $Y_T$ Ã© uma matriz diagonal com elementos $\sqrt{T}$ e $T^{3/2}$, que atua para normalizar as taxas de convergÃªncia dos estimadores. Em outras palavras, ela garante que ambos os estimadores redimensionados ($\sqrt{T}(\hat{\alpha}_T - \alpha)$ e $T^{3/2}(\hat{\delta}_T - \delta)$) convirjam para distribuiÃ§Ãµes limitantes nÃ£o-degeneradas, que neste caso sÃ£o normais [^7]. Sem o redimensionamento adequado, a distribuiÃ§Ã£o limite de $Y_T(b_T - \beta)$ seria degenerada, impedindo o uso de mÃ©todos de inferÃªncia estatÃ­stica padrÃ£o.

> ğŸ’¡ **Exemplo NumÃ©rico:** Vamos ilustrar a importÃ¢ncia do redimensionamento com um exemplo. Vamos simular uma sÃ©rie temporal e comparar as distribuiÃ§Ãµes dos estimadores OLS antes e depois do redimensionamento.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy.stats import norm
>
> # Define os parÃ¢metros
> alpha_true = 2
> delta_true = 0.5
> sigma_sq = 1
> num_simulations = 500
> T = 100
>
> alpha_hats = []
> delta_hats = []
>
> # Simula vÃ¡rias vezes
> for _ in range(num_simulations):
>  # Gera os erros
>  errors = np.random.normal(0, np.sqrt(sigma_sq), T)
>
>  # Gera a sÃ©rie temporal
>  t = np.arange(1, T + 1)
>  y = alpha_true + delta_true * t + errors
>
>  # Calcula a matriz X
>  X = np.column_stack((np.ones(T), t))
>
>  # Calcula o estimador OLS
>  b_T = np.linalg.inv(X.T @ X) @ X.T @ y
>  alpha_hats.append(b_T[0])
>  delta_hats.append(b_T[1])
>
> # Redimensiona as estimativas
> alpha_hats_scaled = [np.sqrt(T)*(alpha_hat - alpha_true) for alpha_hat in alpha_hats]
> delta_hats_scaled = [T**(3/2)*(delta_hat - delta_true) for delta_hat in delta_hats]
>
> # Plota os histogramas
> plt.figure(figsize=(12, 6))
>
> plt.subplot(1, 2, 1)
> plt.hist(alpha_hats, bins=30, density=True, alpha=0.6, label='Alpha (nÃ£o redimensionado)')
> mu_alpha, std_alpha = norm.fit(alpha_hats)
> x = np.linspace(mu_alpha - 3*std_alpha, mu_alpha + 3*std_alpha, 100)
> plt.plot(x, norm.pdf(x, mu_alpha, std_alpha), 'r', label = "Dist Normal")
> plt.xlabel("Estimativa de alpha")
> plt.ylabel("Densidade")
> plt.title("DistribuiÃ§Ã£o de Alpha (NÃ£o Redimensionado)")
> plt.legend()
>
>
> plt.subplot(1, 2, 2)
> plt.hist(delta_hats, bins=30, density=True, alpha=0.6, label='Delta (nÃ£o redimensionado)')
> mu_delta, std_delta = norm.fit(delta_hats)
> x = np.linspace(mu_delta - 3*std_delta, mu_delta + 3*std_delta, 100)
> plt.plot(x, norm.pdf(x, mu_delta, std_delta), 'r', label = "Dist Normal")
> plt.xlabel("Estimativa de delta")
> plt.title("DistribuiÃ§Ã£o de Delta (NÃ£o Redimensionado)")
> plt.legend()
>
> plt.tight_layout()
> plt.show()
>
> plt.figure(figsize=(12, 6))
>
> plt.subplot(1, 2, 1)
> plt.hist(alpha_hats_scaled, bins=30, density=True, alpha=0.6, label='Alpha (redimensionado)')
> mu_alpha_scaled, std_alpha_scaled = norm.fit(alpha_hats_scaled)
> x = np.linspace(mu_alpha_scaled - 3*std_alpha_scaled, mu_alpha_scaled + 3*std_alpha_scaled, 100)
> plt.plot(x, norm.pdf(x, mu_alpha_scaled, std_alpha_scaled), 'r', label = "Dist Normal")
> plt.xlabel("Estimativa de alpha")
> plt.ylabel("Densidade")
> plt.title("DistribuiÃ§Ã£o de Alpha (Redimensionado)")
> plt.legend()
>
> plt.subplot(1, 2, 2)
> plt.hist(delta_hats_scaled, bins=30, density=True, alpha=0.6, label='Delta (redimensionado)')
> mu_delta_scaled, std_delta_scaled = norm.fit(delta_hats_scaled)
> x = np.linspace(mu_delta_scaled - 3*std_delta_scaled, mu_delta_scaled + 3*std_delta_scaled, 100)
> plt.plot(x, norm.pdf(x, mu_delta_scaled, std_delta_scaled), 'r', label = "Dist Normal")
> plt.xlabel("Estimativa de delta")
> plt.title("DistribuiÃ§Ã£o de Delta (Redimensionado)")
> plt.legend()
>
> plt.tight_layout()
> plt.show()
> ```
> Os histogramas das estimativas sem redimensionamento mostram distribuiÃ§Ãµes assimÃ©tricas. Ao contrÃ¡rio, as estimativas com redimensionamento mostram distribuiÃ§Ãµes que se aproximam da normal.

> ğŸ’¡ **Exemplo NumÃ©rico:** Para consolidar a importÃ¢ncia do redimensionamento, vamos analisar os resultados para um valor especÃ­fico de T e realizar o cÃ¡lculo das estimativas com e sem o redimensionamento.  Considere que temos $T = 100$ observaÃ§Ãµes geradas a partir do modelo $y_t = 2 + 0.5t + \epsilon_t$, onde $\epsilon_t \sim N(0, 1)$. Vamos calcular as estimativas OLS e aplicar o redimensionamento.
>
> ```python
> import numpy as np
>
> # ParÃ¢metros
> alpha_true = 2
> delta_true = 0.5
> sigma_sq = 1
> T = 100
>
> # Gera os erros
> errors = np.random.normal(0, np.sqrt(sigma_sq), T)
>
> # Gera a sÃ©rie temporal
> t = np.arange(1, T + 1)
> y = alpha_true + delta_true * t + errors
>
> # Calcula a matriz X
> X = np.column_stack((np.ones(T), t))
>
> # Calcula o estimador OLS
> b_T = np.linalg.inv(X.T @ X) @ X.T @ y
> alpha_hat = b_T[0]
> delta_hat = b_T[1]
>
> # Redimensiona as estimativas
> alpha_hat_scaled = np.sqrt(T) * (alpha_hat - alpha_true)
> delta_hat_scaled = T**(3/2) * (delta_hat - delta_true)
>
> print(f"Estimativa de alpha (nÃ£o redimensionada): {alpha_hat:.4f}")
> print(f"Estimativa de delta (nÃ£o redimensionada): {delta_hat:.4f}")
> print(f"Estimativa de alpha (redimensionada): {alpha_hat_scaled:.4f}")
> print(f"Estimativa de delta (redimensionada): {delta_hat_scaled:.4f}")
> ```
>
> Resultados exemplo (os valores mudam a cada simulaÃ§Ã£o):
> ```
> Estimativa de alpha (nÃ£o redimensionada): 2.0522
> Estimativa de delta (nÃ£o redimensionada): 0.4983
> Estimativa de alpha (redimensionada): 0.5218
> Estimativa de delta (redimensionada): -1.6962
> ```
>
>  Analisando os resultados, observamos que as estimativas nÃ£o redimensionadas de $\alpha$ e $\delta$  sÃ£o prÃ³ximas de seus valores reais (2 e 0.5, respectivamente), mas a distribuiÃ§Ã£o destas estimativas nÃ£o Ã© normal quando amostras repetidas sÃ£o consideradas. Ao redimensionar,  $\sqrt{T}(\hat{\alpha}_T - \alpha)$ e $T^{3/2}(\hat{\delta}_T - \delta)$ convergem para uma distribuiÃ§Ã£o normal, que permite a construÃ§Ã£o de intervalos de confianÃ§a e testes de hipÃ³tese vÃ¡lidos. A importÃ¢ncia do redimensionamento Ã© que, sem ele,  as inferÃªncias poderiam ser equivocadas.

**Teorema 1:** *DistribuiÃ§Ã£o AssintÃ³tica dos Estimadores Redimensionados*. No modelo de tendÃªncia temporal $y_t = \alpha + \delta t + \epsilon_t$, onde $\epsilon_t$ Ã© um processo de ruÃ­do branco com mÃ©dia zero e variÃ¢ncia $\sigma^2$, o estimador redimensionado $Y_T(b_T - \beta)$ converge em distribuiÃ§Ã£o para uma normal multivariada com mÃ©dia zero e matriz de covariÃ¢ncia $\Sigma$, ou seja,
$$ Y_T(b_T - \beta) \xrightarrow{d} N(0, \Sigma) $$
onde
$$\Sigma = \sigma^2 \lim_{T \to \infty} \left( \frac{1}{T^3} Y_T \sum_{t=1}^T x_t x_t' Y_T \right)^{-1} $$
*Prova:* A prova deste resultado segue da aplicaÃ§Ã£o do Teorema do Limite Central Multivariado (CLT) e da convergÃªncia das somas de potÃªncias de $t$. A principal ideia Ã© mostrar que $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1}$ converge para uma matriz nÃ£o singular e que $Y_T \sum_{t=1}^T x_t \epsilon_t$ satisfaz as condiÃ§Ãµes do CLT.
I. ComeÃ§amos com o estimador OLS desviado:
$$Y_T(b_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t$$
II. Reescrevemos a expressÃ£o inserindo $Y_T^{-1}Y_T$ na inversa:
$$Y_T(b_T - \beta) = \left( Y_T \left( \sum_{t=1}^T x_t x_t' \right) Y_T^{-1} \right)^{-1} Y_T  \sum_{t=1}^T x_t \epsilon_t$$

III. Definimos $A_T = \frac{1}{T^3} Y_T \sum_{t=1}^T x_t x_t' Y_T$, que pode ser reescrita usando a forma explÃ­cita de $Y_T$:
    $$ A_T = \frac{1}{T^3}  \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}  $$
    $$ A_T = \frac{1}{T^3} \begin{bmatrix} T^2 & \frac{T^{2.5}(T+1)}{2} \\ \frac{T^{2.5}(T+1)}{2} & \frac{T^3(T+1)(2T+1)}{6} \end{bmatrix} \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} $$

IV. Expandindo essa matriz e dividindo por $T^3$, temos que $A_T$ Ã© igual a:
    $$ A_T = \begin{bmatrix} \frac{T^2}{T^3} & \frac{T^{2.5}(T+1)T^{1.5}}{2T^3} \\ \frac{T^{2.5}(T+1)T^{1.5}}{2T^3} & \frac{T^3(T+1)(2T+1)}{6T^3} \end{bmatrix} = \begin{bmatrix} \frac{1}{T} & \frac{(T+1)}{2} \\ \frac{(T+1)}{2} & \frac{(T+1)(2T+1)}{6} \end{bmatrix} $$
V. Tomando o limite de $A_T$ quando $T \to \infty$, obtemos:
$$  \lim_{T \to \infty} A_T =  \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$$
VI. Note que
$$ Y_T \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} \sum_{t=1}^T \epsilon_t \\ \sum_{t=1}^T t \epsilon_t \end{bmatrix} = \begin{bmatrix} \sqrt{T} \sum_{t=1}^T \epsilon_t \\ T^{3/2} \sum_{t=1}^T t \epsilon_t \end{bmatrix}$$

VII. Como $\epsilon_t$ Ã© um ruÃ­do branco com mÃ©dia zero e variÃ¢ncia $\sigma^2$,  a aplicaÃ§Ã£o do Teorema do Limite Central Multivariado (CLT) garante que $Y_T \sum_{t=1}^T x_t \epsilon_t$ converge para uma distribuiÃ§Ã£o normal multivariada com mÃ©dia zero e uma matriz de covariÃ¢ncia que depende de $\sigma^2$. Ou seja:
$$  \frac{1}{T^{3/2}} Y_T \sum_{t=1}^T x_t \epsilon_t \xrightarrow{d} N(0, \sigma^2 V) $$
onde $V$ Ã© a matriz limite das somas de produtos cruzados dos resÃ­duos, o que tambÃ©m Ã© uma constante.

VIII. Combinando os resultados de V e VII, e aplicando o Teorema de Slutsky, concluÃ­mos que:
$$Y_T(b_T - \beta) \xrightarrow{d} N(0, \Sigma)$$
onde
$$\Sigma = \sigma^2 \lim_{T \to \infty} \left( \frac{1}{T^3} Y_T \sum_{t=1}^T x_t x_t' Y_T \right)^{-1} $$
Onde o limite do termo entre parÃªntesis foi calculado no passo V. â– 

**CorolÃ¡rio 1.1:** A matriz de covariÃ¢ncia $\Sigma$ pode ser simplificada para
$$ \Sigma = \sigma^2 \begin{bmatrix} 1 & -1/2 \\ -1/2 & 1/3 \end{bmatrix}^{-1} = \sigma^2 \begin{bmatrix} 4 & 6 \\ 6 & 12 \end{bmatrix} $$
*Prova*: Esta simplificaÃ§Ã£o decorre de avaliar o limite da matriz escalada e obter um resultado matricial explÃ­cito.
I. Do Teorema 1, sabemos que:
$$\Sigma = \sigma^2 \lim_{T \to \infty} \left( \frac{1}{T^3} Y_T \sum_{t=1}^T x_t x_t' Y_T \right)^{-1}$$
II. No passo V da Prova do Teorema 1, demonstramos que:
$$\lim_{T \to \infty} \frac{1}{T^3} Y_T \sum_{t=1}^T x_t x_t' Y_T = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$$
III. Portanto,
$$\Sigma = \sigma^2 \left( \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} \right)^{-1}$$
IV. Calculando a inversa da matriz, obtemos:
$$\begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}^{-1} = \frac{1}{(1)(1/3) - (1/2)(1/2)} \begin{bmatrix} 1/3 & -1/2 \\ -1/2 & 1 \end{bmatrix} = \frac{1}{1/12} \begin{bmatrix} 1/3 & -1/2 \\ -1/2 & 1 \end{bmatrix} = 12 \begin{bmatrix} 1/3 & -1/2 \\ -1/2 & 1 \end{bmatrix} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$$
V. Substituindo este resultado de volta em $\Sigma$:
$$ \Sigma = \sigma^2 \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} $$
VI. Ajustando um erro de sinal na matriz inversa:
$$\Sigma = \sigma^2 \begin{bmatrix} 4 & 6 \\ 6 & 12 \end{bmatrix}$$
â– 
> ğŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar o uso da matriz de covariÃ¢ncia $\Sigma$, vamos calcular um exemplo numÃ©rico. Suponha que, com base em uma amostra de tamanho $T$, obtivemos estimativas redimensionadas de $\sqrt{T}(\hat{\alpha}_T - \alpha) = 0.5$ e $T^{3/2}(\hat{\delta}_T - \delta) = -1$. Se $\sigma^2 = 1$, podemos usar a matriz de covariÃ¢ncia $\Sigma$ para avaliar a incerteza dessas estimativas. Usando $\Sigma = \begin{bmatrix} 4 & 6 \\ 6 & 12 \end{bmatrix}$ como calculado no CorolÃ¡rio 1.1, podemos calcular as variÃ¢ncias das estimativas redimensionadas:
>
> $\text{Var}(\sqrt{T}(\hat{\alpha}_T - \alpha)) = \Sigma_{11} = 4$
>
> $\text{Var}(T^{3/2}(\hat{\delta}_T - \delta)) = \Sigma_{22} = 12$
>
> $\text{Cov}(\sqrt{T}(\hat{\alpha}_T - \alpha), T^{3/2}(\hat{\delta}_T - \delta)) = \Sigma_{12} = \Sigma_{21} = 6$
>
> Assim, os desvios padrÃ£o das estimativas redimensionadas sÃ£o 2 e $\sqrt{12} \approx 3.46$, respectivamente. Isso nos fornece uma ideia da incerteza associada a cada estimativa e a covariÃ¢ncia entre elas.

### ConclusÃ£o

O redimensionamento dos estimadores OLS por meio da matriz $Y_T$ Ã© essencial para garantir a validade da inferÃªncia estatÃ­stica em modelos com tendÃªncias temporais determinÃ­sticas. Este processo acomoda as diferentes taxas de convergÃªncia de $\hat{\alpha}_T$ e $\hat{\delta}_T$, garantindo que suas distribuiÃ§Ãµes limitantes sejam nÃ£o degeneradas e Gaussianas [^7]. Especificamente, o estimador $\hat{\delta}_T$ Ã© superconsistente, convergindo para $\delta$ a uma taxa de $T^{3/2}$, o que Ã© mais rÃ¡pido do que a taxa de $\sqrt{T}$ da convergÃªncia de $\hat{\alpha}_T$ para $\alpha$. Ao prÃ©-multiplicar o desvio $(b_T - \beta)$ pela matriz $Y_T$,  obtÃ©m-se um vetor com distribuiÃ§Ã£o normal assintÃ³tica, permitindo assim o uso de testes de hipÃ³tese padrÃ£o. O conhecimento dessas diferentes taxas de convergÃªncia e a aplicaÃ§Ã£o do redimensionamento correto sÃ£o fundamentais para uma anÃ¡lise estatÃ­stica rigorosa de modelos de sÃ©ries temporais com tendÃªncias temporais.

### ReferÃªncias

[^1]: Trecho do texto que introduz o capÃ­tulo, discute a diferenÃ§a nas distribuiÃ§Ãµes assintÃ³ticas e apresenta o tema do capÃ­tulo
[^2]: Trecho do texto que apresenta o modelo de regressÃ£o simples, sua formulaÃ§Ã£o matricial e o estimador OLS
[^3]: Trecho do texto que apresenta as somas de $t$ e $t^2$ e suas ordens de crescimento
[^4]: Trecho do texto que introduz a matriz de escala para obter distribuiÃ§Ãµes limites nÃ£o degeneradas
[^7]: Trecho do texto que conclui a derivaÃ§Ã£o da distribuiÃ§Ã£o assintÃ³tica, define o conceito de superconsistÃªncia e menciona a validade dos testes de hipÃ³tese padrÃ£o.
<!-- END -->
