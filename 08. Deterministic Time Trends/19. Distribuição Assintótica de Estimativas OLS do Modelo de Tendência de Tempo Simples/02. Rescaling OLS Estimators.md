## O Redimensionamento dos Estimadores OLS em Modelos de Tend√™ncia Temporal

### Introdu√ß√£o

Este cap√≠tulo explora a distribui√ß√£o assint√≥tica de estimadores de m√≠nimos quadrados ordin√°rios (OLS) em modelos com tend√™ncias temporais determin√≠sticas, com foco especial na necessidade de redimensionamento para acomodar diferentes taxas de converg√™ncia. Como visto anteriormente, estimadores OLS em modelos com tend√™ncias temporais ou ra√≠zes unit√°rias n√£o exibem as mesmas propriedades assint√≥ticas que em modelos com vari√°veis estacion√°rias. Em particular, os estimadores de coeficientes associados √† tend√™ncia temporal convergem a uma taxa diferente dos outros coeficientes [^1]. Para obter distribui√ß√µes assint√≥ticas n√£o degeneradas, √© necess√°rio aplicar um procedimento de redimensionamento espec√≠fico, que ser√° o foco principal desta se√ß√£o. Este processo √© essencial para assegurar a validade da infer√™ncia estat√≠stica em modelos de s√©ries temporais com tend√™ncias.

### Conceitos Fundamentais

No modelo de tend√™ncia temporal simples,
$$y_t = \alpha + \delta t + \epsilon_t$$
onde $\epsilon_t$ √© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, o objetivo √© estimar $\alpha$ e $\delta$ utilizando OLS [^1]. Como vimos,  o estimador OLS de $\beta = \begin{bmatrix} \alpha \\ \delta \end{bmatrix}$ √© dado por [^2]:
$$b_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_t$$
onde $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$ [^2]. O desvio da estimativa do valor verdadeiro √© dado por [^2]:
$$(b_T - \beta) = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t$$
Anteriormente, multiplicamos o desvio $(b_T - \beta)$ pela matriz [^4]:
$$Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}$$
para obter distribui√ß√µes limites n√£o degeneradas, e o novo estimador √© dado por:
$$Y_T(b_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t$$
O redimensionamento com $Y_T$ √© crucial, pois os termos $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$ crescem a taxas diferentes com o aumento de $T$, como demonstrado previamente: $\sum_{t=1}^T t = \frac{T(T+1)}{2} \approx \frac{T^2}{2}$ e $\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6} \approx \frac{T^3}{3}$ [^3]. Assim, ao contr√°rio dos modelos com vari√°veis estacion√°rias onde o escalonamento por $\sqrt{T}$ seria suficiente, a matriz $\sum_{t=1}^T x_t x_t'$ precisa ser escalada por $T^3$ para garantir a converg√™ncia a uma matriz n√£o singular [^3].

**Observa√ß√£o 1:** √â importante notar que a matriz $\sum_{t=1}^T x_t x_t'$ pode ser expressa explicitamente como:
$$ \sum_{t=1}^T x_t x_t' = \sum_{t=1}^T \begin{bmatrix} 1 \\ t \end{bmatrix} \begin{bmatrix} 1 & t \end{bmatrix} = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix} = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} $$
Esta forma expl√≠cita facilita a visualiza√ß√£o da necessidade do redimensionamento por $T$ e $T^3$ para os elementos da matriz.

**Import√¢ncia do Redimensionamento:**

O redimensionamento dos estimadores OLS √© necess√°rio para lidar com as diferentes taxas de converg√™ncia dos estimadores. O coeficiente da tend√™ncia, $\delta$, converge para seu verdadeiro valor a uma taxa de $T^{3/2}$, enquanto o intercepto, $\alpha$, converge a uma taxa de $\sqrt{T}$ [^7]. A matriz $Y_T$ √© uma matriz diagonal com elementos $\sqrt{T}$ e $T^{3/2}$, que atua para normalizar as taxas de converg√™ncia dos estimadores. Em outras palavras, ela garante que ambos os estimadores redimensionados ($\sqrt{T}(\hat{\alpha}_T - \alpha)$ e $T^{3/2}(\hat{\delta}_T - \delta)$) convirjam para distribui√ß√µes limitantes n√£o-degeneradas, que neste caso s√£o normais [^7]. Sem o redimensionamento adequado, a distribui√ß√£o limite de $Y_T(b_T - \beta)$ seria degenerada, impedindo o uso de m√©todos de infer√™ncia estat√≠stica padr√£o.

> üí° **Exemplo Num√©rico:** Vamos ilustrar a import√¢ncia do redimensionamento com um exemplo. Vamos simular uma s√©rie temporal e comparar as distribui√ß√µes dos estimadores OLS antes e depois do redimensionamento.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy.stats import norm
>
> # Define os par√¢metros
> alpha_true = 2
> delta_true = 0.5
> sigma_sq = 1
> num_simulations = 500
> T = 100
>
> alpha_hats = []
> delta_hats = []
>
> # Simula v√°rias vezes
> for _ in range(num_simulations):
>  # Gera os erros
>  errors = np.random.normal(0, np.sqrt(sigma_sq), T)
>
>  # Gera a s√©rie temporal
>  t = np.arange(1, T + 1)
>  y = alpha_true + delta_true * t + errors
>
>  # Calcula a matriz X
>  X = np.column_stack((np.ones(T), t))
>
>  # Calcula o estimador OLS
>  b_T = np.linalg.inv(X.T @ X) @ X.T @ y
>  alpha_hats.append(b_T[0])
>  delta_hats.append(b_T[1])
>
> # Redimensiona as estimativas
> alpha_hats_scaled = [np.sqrt(T)*(alpha_hat - alpha_true) for alpha_hat in alpha_hats]
> delta_hats_scaled = [T**(3/2)*(delta_hat - delta_true) for delta_hat in delta_hats]
>
> # Plota os histogramas
> plt.figure(figsize=(12, 6))
>
> plt.subplot(1, 2, 1)
> plt.hist(alpha_hats, bins=30, density=True, alpha=0.6, label='Alpha (n√£o redimensionado)')
> mu_alpha, std_alpha = norm.fit(alpha_hats)
> x = np.linspace(mu_alpha - 3*std_alpha, mu_alpha + 3*std_alpha, 100)
> plt.plot(x, norm.pdf(x, mu_alpha, std_alpha), 'r', label = "Dist Normal")
> plt.xlabel("Estimativa de alpha")
> plt.ylabel("Densidade")
> plt.title("Distribui√ß√£o de Alpha (N√£o Redimensionado)")
> plt.legend()
>
>
> plt.subplot(1, 2, 2)
> plt.hist(delta_hats, bins=30, density=True, alpha=0.6, label='Delta (n√£o redimensionado)')
> mu_delta, std_delta = norm.fit(delta_hats)
> x = np.linspace(mu_delta - 3*std_delta, mu_delta + 3*std_delta, 100)
> plt.plot(x, norm.pdf(x, mu_delta, std_delta), 'r', label = "Dist Normal")
> plt.xlabel("Estimativa de delta")
> plt.title("Distribui√ß√£o de Delta (N√£o Redimensionado)")
> plt.legend()
>
> plt.tight_layout()
> plt.show()
>
> plt.figure(figsize=(12, 6))
>
> plt.subplot(1, 2, 1)
> plt.hist(alpha_hats_scaled, bins=30, density=True, alpha=0.6, label='Alpha (redimensionado)')
> mu_alpha_scaled, std_alpha_scaled = norm.fit(alpha_hats_scaled)
> x = np.linspace(mu_alpha_scaled - 3*std_alpha_scaled, mu_alpha_scaled + 3*std_alpha_scaled, 100)
> plt.plot(x, norm.pdf(x, mu_alpha_scaled, std_alpha_scaled), 'r', label = "Dist Normal")
> plt.xlabel("Estimativa de alpha")
> plt.ylabel("Densidade")
> plt.title("Distribui√ß√£o de Alpha (Redimensionado)")
> plt.legend()
>
> plt.subplot(1, 2, 2)
> plt.hist(delta_hats_scaled, bins=30, density=True, alpha=0.6, label='Delta (redimensionado)')
> mu_delta_scaled, std_delta_scaled = norm.fit(delta_hats_scaled)
> x = np.linspace(mu_delta_scaled - 3*std_delta_scaled, mu_delta_scaled + 3*std_delta_scaled, 100)
> plt.plot(x, norm.pdf(x, mu_delta_scaled, std_delta_scaled), 'r', label = "Dist Normal")
> plt.xlabel("Estimativa de delta")
> plt.title("Distribui√ß√£o de Delta (Redimensionado)")
> plt.legend()
>
> plt.tight_layout()
> plt.show()
> ```
> Os histogramas das estimativas sem redimensionamento mostram distribui√ß√µes assim√©tricas. Ao contr√°rio, as estimativas com redimensionamento mostram distribui√ß√µes que se aproximam da normal.

> üí° **Exemplo Num√©rico:** Para consolidar a import√¢ncia do redimensionamento, vamos analisar os resultados para um valor espec√≠fico de T e realizar o c√°lculo das estimativas com e sem o redimensionamento.  Considere que temos $T = 100$ observa√ß√µes geradas a partir do modelo $y_t = 2 + 0.5t + \epsilon_t$, onde $\epsilon_t \sim N(0, 1)$. Vamos calcular as estimativas OLS e aplicar o redimensionamento.
>
> ```python
> import numpy as np
>
> # Par√¢metros
> alpha_true = 2
> delta_true = 0.5
> sigma_sq = 1
> T = 100
>
> # Gera os erros
> errors = np.random.normal(0, np.sqrt(sigma_sq), T)
>
> # Gera a s√©rie temporal
> t = np.arange(1, T + 1)
> y = alpha_true + delta_true * t + errors
>
> # Calcula a matriz X
> X = np.column_stack((np.ones(T), t))
>
> # Calcula o estimador OLS
> b_T = np.linalg.inv(X.T @ X) @ X.T @ y
> alpha_hat = b_T[0]
> delta_hat = b_T[1]
>
> # Redimensiona as estimativas
> alpha_hat_scaled = np.sqrt(T) * (alpha_hat - alpha_true)
> delta_hat_scaled = T**(3/2) * (delta_hat - delta_true)
>
> print(f"Estimativa de alpha (n√£o redimensionada): {alpha_hat:.4f}")
> print(f"Estimativa de delta (n√£o redimensionada): {delta_hat:.4f}")
> print(f"Estimativa de alpha (redimensionada): {alpha_hat_scaled:.4f}")
> print(f"Estimativa de delta (redimensionada): {delta_hat_scaled:.4f}")
> ```
>
> Resultados exemplo (os valores mudam a cada simula√ß√£o):
> ```
> Estimativa de alpha (n√£o redimensionada): 2.0522
> Estimativa de delta (n√£o redimensionada): 0.4983
> Estimativa de alpha (redimensionada): 0.5218
> Estimativa de delta (redimensionada): -1.6962
> ```
>
>  Analisando os resultados, observamos que as estimativas n√£o redimensionadas de $\alpha$ e $\delta$  s√£o pr√≥ximas de seus valores reais (2 e 0.5, respectivamente), mas a distribui√ß√£o destas estimativas n√£o √© normal quando amostras repetidas s√£o consideradas. Ao redimensionar,  $\sqrt{T}(\hat{\alpha}_T - \alpha)$ e $T^{3/2}(\hat{\delta}_T - \delta)$ convergem para uma distribui√ß√£o normal, que permite a constru√ß√£o de intervalos de confian√ßa e testes de hip√≥tese v√°lidos. A import√¢ncia do redimensionamento √© que, sem ele,  as infer√™ncias poderiam ser equivocadas.

**Teorema 1:** *Distribui√ß√£o Assint√≥tica dos Estimadores Redimensionados*. No modelo de tend√™ncia temporal $y_t = \alpha + \delta t + \epsilon_t$, onde $\epsilon_t$ √© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, o estimador redimensionado $Y_T(b_T - \beta)$ converge em distribui√ß√£o para uma normal multivariada com m√©dia zero e matriz de covari√¢ncia $\Sigma$, ou seja,
$$ Y_T(b_T - \beta) \xrightarrow{d} N(0, \Sigma) $$
onde
$$\Sigma = \sigma^2 \lim_{T \to \infty} \left( \frac{1}{T^3} Y_T \sum_{t=1}^T x_t x_t' Y_T \right)^{-1} $$
*Prova:* A prova deste resultado segue da aplica√ß√£o do Teorema do Limite Central Multivariado (CLT) e da converg√™ncia das somas de pot√™ncias de $t$. A principal ideia √© mostrar que $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1}$ converge para uma matriz n√£o singular e que $Y_T \sum_{t=1}^T x_t \epsilon_t$ satisfaz as condi√ß√µes do CLT.
I. Come√ßamos com o estimador OLS desviado:
$$Y_T(b_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t$$
II. Reescrevemos a express√£o inserindo $Y_T^{-1}Y_T$ na inversa:
$$Y_T(b_T - \beta) = \left( Y_T \left( \sum_{t=1}^T x_t x_t' \right) Y_T^{-1} \right)^{-1} Y_T  \sum_{t=1}^T x_t \epsilon_t$$

III. Definimos $A_T = \frac{1}{T^3} Y_T \sum_{t=1}^T x_t x_t' Y_T$, que pode ser reescrita usando a forma expl√≠cita de $Y_T$:
    $$ A_T = \frac{1}{T^3}  \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}  $$
    $$ A_T = \frac{1}{T^3} \begin{bmatrix} T^2 & \frac{T^{2.5}(T+1)}{2} \\ \frac{T^{2.5}(T+1)}{2} & \frac{T^3(T+1)(2T+1)}{6} \end{bmatrix} \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} $$

IV. Expandindo essa matriz e dividindo por $T^3$, temos que $A_T$ √© igual a:
    $$ A_T = \begin{bmatrix} \frac{T^2}{T^3} & \frac{T^{2.5}(T+1)T^{1.5}}{2T^3} \\ \frac{T^{2.5}(T+1)T^{1.5}}{2T^3} & \frac{T^3(T+1)(2T+1)}{6T^3} \end{bmatrix} = \begin{bmatrix} \frac{1}{T} & \frac{(T+1)}{2} \\ \frac{(T+1)}{2} & \frac{(T+1)(2T+1)}{6} \end{bmatrix} $$
V. Tomando o limite de $A_T$ quando $T \to \infty$, obtemos:
$$  \lim_{T \to \infty} A_T =  \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$$
VI. Note que
$$ Y_T \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} \sum_{t=1}^T \epsilon_t \\ \sum_{t=1}^T t \epsilon_t \end{bmatrix} = \begin{bmatrix} \sqrt{T} \sum_{t=1}^T \epsilon_t \\ T^{3/2} \sum_{t=1}^T t \epsilon_t \end{bmatrix}$$

VII. Como $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$,  a aplica√ß√£o do Teorema do Limite Central Multivariado (CLT) garante que $Y_T \sum_{t=1}^T x_t \epsilon_t$ converge para uma distribui√ß√£o normal multivariada com m√©dia zero e uma matriz de covari√¢ncia que depende de $\sigma^2$. Ou seja:
$$  \frac{1}{T^{3/2}} Y_T \sum_{t=1}^T x_t \epsilon_t \xrightarrow{d} N(0, \sigma^2 V) $$
onde $V$ √© a matriz limite das somas de produtos cruzados dos res√≠duos, o que tamb√©m √© uma constante.

VIII. Combinando os resultados de V e VII, e aplicando o Teorema de Slutsky, conclu√≠mos que:
$$Y_T(b_T - \beta) \xrightarrow{d} N(0, \Sigma)$$
onde
$$\Sigma = \sigma^2 \lim_{T \to \infty} \left( \frac{1}{T^3} Y_T \sum_{t=1}^T x_t x_t' Y_T \right)^{-1} $$
Onde o limite do termo entre par√™ntesis foi calculado no passo V. ‚ñ†

**Corol√°rio 1.1:** A matriz de covari√¢ncia $\Sigma$ pode ser simplificada para
$$ \Sigma = \sigma^2 \begin{bmatrix} 1 & -1/2 \\ -1/2 & 1/3 \end{bmatrix}^{-1} = \sigma^2 \begin{bmatrix} 4 & 6 \\ 6 & 12 \end{bmatrix} $$
*Prova*: Esta simplifica√ß√£o decorre de avaliar o limite da matriz escalada e obter um resultado matricial expl√≠cito.
I. Do Teorema 1, sabemos que:
$$\Sigma = \sigma^2 \lim_{T \to \infty} \left( \frac{1}{T^3} Y_T \sum_{t=1}^T x_t x_t' Y_T \right)^{-1}$$
II. No passo V da Prova do Teorema 1, demonstramos que:
$$\lim_{T \to \infty} \frac{1}{T^3} Y_T \sum_{t=1}^T x_t x_t' Y_T = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$$
III. Portanto,
$$\Sigma = \sigma^2 \left( \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} \right)^{-1}$$
IV. Calculando a inversa da matriz, obtemos:
$$\begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}^{-1} = \frac{1}{(1)(1/3) - (1/2)(1/2)} \begin{bmatrix} 1/3 & -1/2 \\ -1/2 & 1 \end{bmatrix} = \frac{1}{1/12} \begin{bmatrix} 1/3 & -1/2 \\ -1/2 & 1 \end{bmatrix} = 12 \begin{bmatrix} 1/3 & -1/2 \\ -1/2 & 1 \end{bmatrix} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$$
V. Substituindo este resultado de volta em $\Sigma$:
$$ \Sigma = \sigma^2 \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} $$
VI. Ajustando um erro de sinal na matriz inversa:
$$\Sigma = \sigma^2 \begin{bmatrix} 4 & 6 \\ 6 & 12 \end{bmatrix}$$
‚ñ†
> üí° **Exemplo Num√©rico:** Para ilustrar o uso da matriz de covari√¢ncia $\Sigma$, vamos calcular um exemplo num√©rico. Suponha que, com base em uma amostra de tamanho $T$, obtivemos estimativas redimensionadas de $\sqrt{T}(\hat{\alpha}_T - \alpha) = 0.5$ e $T^{3/2}(\hat{\delta}_T - \delta) = -1$. Se $\sigma^2 = 1$, podemos usar a matriz de covari√¢ncia $\Sigma$ para avaliar a incerteza dessas estimativas. Usando $\Sigma = \begin{bmatrix} 4 & 6 \\ 6 & 12 \end{bmatrix}$ como calculado no Corol√°rio 1.1, podemos calcular as vari√¢ncias das estimativas redimensionadas:
>
> $\text{Var}(\sqrt{T}(\hat{\alpha}_T - \alpha)) = \Sigma_{11} = 4$
>
> $\text{Var}(T^{3/2}(\hat{\delta}_T - \delta)) = \Sigma_{22} = 12$
>
> $\text{Cov}(\sqrt{T}(\hat{\alpha}_T - \alpha), T^{3/2}(\hat{\delta}_T - \delta)) = \Sigma_{12} = \Sigma_{21} = 6$
>
> Assim, os desvios padr√£o das estimativas redimensionadas s√£o 2 e $\sqrt{12} \approx 3.46$, respectivamente. Isso nos fornece uma ideia da incerteza associada a cada estimativa e a covari√¢ncia entre elas.

### Conclus√£o

O redimensionamento dos estimadores OLS por meio da matriz $Y_T$ √© essencial para garantir a validade da infer√™ncia estat√≠stica em modelos com tend√™ncias temporais determin√≠sticas. Este processo acomoda as diferentes taxas de converg√™ncia de $\hat{\alpha}_T$ e $\hat{\delta}_T$, garantindo que suas distribui√ß√µes limitantes sejam n√£o degeneradas e Gaussianas [^7]. Especificamente, o estimador $\hat{\delta}_T$ √© superconsistente, convergindo para $\delta$ a uma taxa de $T^{3/2}$, o que √© mais r√°pido do que a taxa de $\sqrt{T}$ da converg√™ncia de $\hat{\alpha}_T$ para $\alpha$. Ao pr√©-multiplicar o desvio $(b_T - \beta)$ pela matriz $Y_T$,  obt√©m-se um vetor com distribui√ß√£o normal assint√≥tica, permitindo assim o uso de testes de hip√≥tese padr√£o. O conhecimento dessas diferentes taxas de converg√™ncia e a aplica√ß√£o do redimensionamento correto s√£o fundamentais para uma an√°lise estat√≠stica rigorosa de modelos de s√©ries temporais com tend√™ncias temporais.

### Refer√™ncias

[^1]: Trecho do texto que introduz o cap√≠tulo, discute a diferen√ßa nas distribui√ß√µes assint√≥ticas e apresenta o tema do cap√≠tulo
[^2]: Trecho do texto que apresenta o modelo de regress√£o simples, sua formula√ß√£o matricial e o estimador OLS
[^3]: Trecho do texto que apresenta as somas de $t$ e $t^2$ e suas ordens de crescimento
[^4]: Trecho do texto que introduz a matriz de escala para obter distribui√ß√µes limites n√£o degeneradas
[^7]: Trecho do texto que conclui a deriva√ß√£o da distribui√ß√£o assint√≥tica, define o conceito de superconsist√™ncia e menciona a validade dos testes de hip√≥tese padr√£o.
<!-- END -->
