## Distribui√ß√£o Assint√≥tica de Estimativas OLS em Modelos de Tend√™ncia Temporal Determin√≠stica

### Introdu√ß√£o
Este cap√≠tulo explora a infer√™ncia estat√≠stica para processos univariados contendo ra√≠zes unit√°rias ou tend√™ncias de tempo determin√≠sticas, focando em um modelo de tend√™ncia de tempo simples. Como mencionado anteriormente, as distribui√ß√µes assint√≥ticas para coeficientes estimados em modelos com ra√≠zes unit√°rias ou tend√™ncias de tempo determin√≠sticas diferem de modelos com vari√°veis estacion√°rias [^1]. Este cap√≠tulo aborda essas diferen√ßas e introduz uma abordagem geral para obter distribui√ß√µes assint√≥ticas, particularmente para processos envolvendo tend√™ncias temporais determin√≠sticas, mas sem ra√≠zes unit√°rias [^1]. O foco principal ser√° a distribui√ß√£o assint√≥tica das estimativas de m√≠nimos quadrados ordin√°rios (OLS) de um modelo de tend√™ncia temporal simples, que ser√° expandido posteriormente para processos autorregressivos em torno de tend√™ncias temporais determin√≠sticas.

**Observa√ß√£o:** √â importante notar que, embora este cap√≠tulo se concentre em tend√™ncias temporais determin√≠sticas, a an√°lise de processos com ra√≠zes unit√°rias √© intimamente relacionada, pois ambos os casos levam a taxas de converg√™ncia assint√≥ticas n√£o convencionais. A diferen√ßa fundamental reside no fato de que os processos com ra√≠zes unit√°rias n√£o convergem para um valor fixo, enquanto os processos com tend√™ncias determin√≠sticas, como o que ser√° apresentado aqui, convergem em torno de uma trajet√≥ria determin√≠stica.

### Conceitos Fundamentais
O ponto de partida √© um modelo de tend√™ncia temporal simples [^1]:
$$y_t = \alpha + \delta t + \epsilon_t$$
onde $\epsilon_t$ √© um processo de ru√≠do branco. Assumindo que $\epsilon_t \sim N(0, \sigma^2)$, o modelo satisfaz as suposi√ß√µes cl√°ssicas de regress√£o [^1]. No entanto, ao contr√°rio dos modelos com vari√°veis estacion√°rias, as taxas de converg√™ncia assint√≥tica para os diferentes par√¢metros ( $\alpha$ e $\delta$) ser√£o distintas [^1].

> üí° **Exemplo Num√©rico:** Vamos gerar uma s√©rie temporal com $\alpha = 2$, $\delta = 0.5$, $\sigma^2 = 1$, e $T = 100$.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Define os par√¢metros
> alpha_true = 2
> delta_true = 0.5
> sigma_sq = 1
> T = 100
>
> # Gera os erros
> np.random.seed(42)
> errors = np.random.normal(0, np.sqrt(sigma_sq), T)
>
> # Gera a s√©rie temporal
> t = np.arange(1, T + 1)
> y = alpha_true + delta_true * t + errors
>
> # Visualiza a s√©rie temporal
> plt.plot(t, y)
> plt.xlabel("Tempo (t)")
> plt.ylabel("y_t")
> plt.title("S√©rie Temporal Gerada")
> plt.show()
> ```
> Essa visualiza√ß√£o mostra uma s√©rie temporal com uma tend√™ncia linear crescente e ru√≠do aleat√≥rio.

Para facilitar a an√°lise, o modelo acima √© reescrito na forma de um modelo de regress√£o padr√£o [^2]:
$$y_t = x_t'\beta + \epsilon_t$$
onde $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$ e $\beta = \begin{bmatrix} \alpha \\ \delta \end{bmatrix}$ [^2]. O estimador OLS para $\beta$, denotado por $b_T$, √© dado por [^2]:
$$b_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_t$$
O desvio da estimativa OLS do valor verdadeiro pode ser expresso como [^2]:
$$(b_T - \beta) = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t$$

> üí° **Exemplo Num√©rico:** Usando a s√©rie temporal gerada anteriormente, vamos calcular $b_T$ usando a f√≥rmula OLS.
> ```python
> # Calcula a matriz X
> X = np.column_stack((np.ones(T), t))
>
> # Calcula o estimador OLS
> b_T = np.linalg.inv(X.T @ X) @ X.T @ y
> print(f"Estimativas OLS: alpha_hat = {b_T[0]:.4f}, delta_hat = {b_T[1]:.4f}")
> ```
> Os resultados do exemplo mostram que o $\hat{\alpha}_T$ e $\hat{\delta}_T$  s√£o estimativas dos valores verdadeiros $\alpha$ e $\delta$.
>
> Para entender o passo a passo da computa√ß√£o, vamos mostrar os c√°lculos para um conjunto de dados pequeno, com T=3 e a s√©rie y=[2.8, 3.7, 5.2]
>
> $\text{Passo 1: Calcular X} = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \end{bmatrix}$
>
> $\text{Passo 2: Calcular }X^TX = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \end{bmatrix} = \begin{bmatrix} 3 & 6 \\ 6 & 14 \end{bmatrix}$
>
> $\text{Passo 3: Calcular } (X^TX)^{-1} =  \frac{1}{42-36} \begin{bmatrix} 14 & -6 \\ -6 & 3 \end{bmatrix} = \frac{1}{6} \begin{bmatrix} 14 & -6 \\ -6 & 3 \end{bmatrix} = \begin{bmatrix} 2.33 & -1 \\ -1 & 0.5 \end{bmatrix} $
>
> $\text{Passo 4: Calcular } X^Ty = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 2.8 \\ 3.7 \\ 5.2 \end{bmatrix} = \begin{bmatrix} 11.7 \\ 26.8 \end{bmatrix}$
>
> $\text{Passo 5: Calcular } b_T = (X^TX)^{-1}X^Ty = \begin{bmatrix} 2.33 & -1 \\ -1 & 0.5 \end{bmatrix} \begin{bmatrix} 11.7 \\ 26.8 \end{bmatrix} = \begin{bmatrix} 0.55 \\ 0.85 \end{bmatrix}$
>
> Isso significa que $\hat{\alpha}_T = 0.55$ e $\hat{\delta}_T = 0.85$.

Diferentemente da abordagem para vari√°veis estacion√°rias, onde se multiplica por $\sqrt{T}$ [^2], neste caso, √© preciso reconhecer que as somas de $t$ e $t^2$ t√™m ordens de crescimento diferentes. De fato, temos [^3]:
$$\sum_{t=1}^T t = \frac{T(T+1)}{2}$$
$$\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6}$$
As somas $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$ convergem para $T^2/2$ e $T^3/3$ respectivamente. Logo, a matriz $\sum_{t=1}^T x_t x_t'$ precisa ser escalada por $T^3$ para convergir para uma matriz n√£o singular.
Para obter distribui√ß√µes limites n√£o degeneradas, $\hat{\alpha}_T$ √© multiplicado por $\sqrt{T}$ e $\hat{\delta}_T$ por $T^{3/2}$. Isso √© equivalente a pr√©-multiplicar o desvio $(b_T - \beta)$ pela matriz [^4]:
$$Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}$$
Obtendo assim:
$$Y_T(b_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t \epsilon_t$$
$$Y_T(b_T - \beta) = \left( Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right) Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$$
A matriz  $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T$ converge em probabilidade para uma matriz n√£o singular $Q$ [^5]:
$$Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$$

**Lema 1:** A matriz $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T$ converge em probabilidade para a matriz $Q$.
*Prova:*
I.  Come√ßamos com a defini√ß√£o de $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$ e calculamos $\sum_{t=1}^T x_t x_t'$:
    $$\sum_{t=1}^T x_t x_t' = \sum_{t=1}^T \begin{bmatrix} 1 \\ t \end{bmatrix} \begin{bmatrix} 1 & t \end{bmatrix} = \sum_{t=1}^T \begin{bmatrix} 1 & t \\ t & t^2 \end{bmatrix} = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix} = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix}$$

II.  Multiplicamos a matriz acima por $Y_T$ e $Y_T$ transposto:
    $$Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T =  \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix}^{-1} \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}$$

III.  Calculamos a inversa da matriz do passo I:
    $$\left( \sum_{t=1}^T x_t x_t' \right)^{-1} =  \frac{1}{\frac{T^2(T+1)(2T+1)}{6} - \frac{T^2(T+1)^2}{4}} \begin{bmatrix} \frac{T(T+1)(2T+1)}{6} & -\frac{T(T+1)}{2} \\ -\frac{T(T+1)}{2} & T \end{bmatrix} = \frac{1}{\frac{T^4}{12} + O(T^3)}\begin{bmatrix} \frac{T^3}{3} + O(T^2) & -\frac{T^2}{2}+O(T) \\ -\frac{T^2}{2} + O(T) & T \end{bmatrix}$$

IV. Multiplicando $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T $:
$$Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T  = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} \frac{1}{\frac{T^4}{12} + O(T^3)}\begin{bmatrix} \frac{T^3}{3} + O(T^2) & -\frac{T^2}{2}+O(T) \\ -\frac{T^2}{2} + O(T) & T \end{bmatrix}\begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} = \frac{1}{\frac{T^4}{12} + O(T^3)} \begin{bmatrix} \frac{T^4}{3} + O(T^3) & -\frac{T^{7/2}}{2}+O(T^{5/2}) \\ -\frac{T^{7/2}}{2} + O(T^{5/2}) & T^4 \end{bmatrix} $$
$$= \begin{bmatrix} \frac{\frac{T^4}{3} + O(T^3)}{\frac{T^4}{12} + O(T^3)} & \frac{-\frac{T^{7/2}}{2}+O(T^{5/2})}{\frac{T^4}{12} + O(T^3)} \\ \frac{-\frac{T^{7/2}}{2} + O(T^{5/2})}{\frac{T^4}{12} + O(T^3)} & \frac{T^4}{\frac{T^4}{12} + O(T^3)} \end{bmatrix} $$
V.  Tomando o limite quando $T \to \infty$:
$$ \lim_{T\to\infty} Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T = \begin{bmatrix} \frac{1/3}{1/12} & \frac{-1/2}{1/12} \\ \frac{-1/2}{1/12} & \frac{1}{1/12} \end{bmatrix} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$$
   
VI. Multiplicando por $Y_T$ na forma $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T $:
$$\begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} \left( \sum_{t=1}^T x_t x_t' \right)^{-1}  \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} \xrightarrow{p} \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}^{-1} = Q$$
   Assim, $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T$ converge em probabilidade para $Q$ quando $T \to \infty$
   $$\begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} ^{-1} = \frac{1}{48-36} \begin{bmatrix} 12 & 6 \\ 6 & 4 \end{bmatrix} = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$$

    Com isso, a demonstra√ß√£o est√° completa.‚ñ†

O segundo termo em [^5] √© ent√£o:
$$Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} (1/\sqrt{T})\sum_{t=1}^T \epsilon_t \\ (1/T^{3/2})\sum_{t=1}^T t\epsilon_t \end{bmatrix}$$

Sob suposi√ß√µes padr√£o sobre $\epsilon_t$ (i.i.d., m√©dia zero, vari√¢ncia $\sigma^2$, e quarto momento finito), este vetor converge para uma normal multivariada assint√≥tica. Especificamente, o primeiro elemento satisfaz o Teorema do Limite Central [^5]:
$$(1/\sqrt{T}) \sum_{t=1}^T \epsilon_t \xrightarrow{d} N(0, \sigma^2)$$
Para o segundo elemento, √© observado que $\{(t/T)\epsilon_t\}$ √© uma sequ√™ncia de diferen√ßas martingales [^5]. A vari√¢ncia do segundo elemento converge para $\sigma^2/3$ [^6]. Finalmente, qualquer combina√ß√£o linear desses dois elementos √© assintoticamente gaussiana, implicando uma distribui√ß√£o gaussiana bivariada limite [^6].

**Lema 2:** O segundo termo $Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$ converge para uma distribui√ß√£o normal bivariada com m√©dia zero e matriz de covari√¢ncia espec√≠fica.
*Prova:*
I. Definimos $Z_T = Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t = \begin{bmatrix} (1/\sqrt{T})\sum_{t=1}^T \epsilon_t \\ (1/T^{3/2})\sum_{t=1}^T t\epsilon_t \end{bmatrix}$.

II.  Pelo Teorema do Limite Central (TLC) cl√°ssico, o primeiro componente $(1/\sqrt{T})\sum_{t=1}^T \epsilon_t$ converge em distribui√ß√£o para $N(0,\sigma^2)$, dado que $\epsilon_t$ √© iid com m√©dia zero e vari√¢ncia $\sigma^2$.

III. Para o segundo componente, $(1/T^{3/2})\sum_{t=1}^T t\epsilon_t$, definimos a sequ√™ncia $u_{t,T} = (t/T^{3/2})\epsilon_t$.  Como $\epsilon_t$ tem m√©dia zero, $E[u_{t,T}] = 0$.

IV.  Calculamos a vari√¢ncia de $u_{t,T}$: $E[u_{t,T}^2] = E[(t^2/T^3)\epsilon_t^2] = (t^2/T^3)E[\epsilon_t^2] = (t^2/T^3)\sigma^2$.

V.  A soma das vari√¢ncias √© $\sum_{t=1}^T E[u_{t,T}^2] = (1/T^3)\sigma^2 \sum_{t=1}^T t^2 = (1/T^3)\sigma^2 (T(T+1)(2T+1)/6) = \sigma^2(2T^3 + 3T^2 + T)/(6T^3) \rightarrow \sigma^2/3$ quando $T \rightarrow \infty$.

VI. Como $\{u_{t,T}\}$ √© uma sequ√™ncia de diferen√ßas martingales (dado que $E[\epsilon_t|\epsilon_{t-1},\epsilon_{t-2},...] = 0$), podemos aplicar o TLC para Martingales, o que nos d√° $(1/T^{3/2})\sum_{t=1}^T t\epsilon_t \xrightarrow{d} N(0,\sigma^2/3)$.

VII. Dado que qualquer combina√ß√£o linear dos componentes de $Z_T$ converge para uma distribui√ß√£o normal, podemos afirmar que $Z_T$ converge para uma distribui√ß√£o normal bivariada. Al√©m disso, a covari√¢ncia entre os dois componentes √© zero (porque os erros $\epsilon_t$ s√£o independentes entre si).

Portanto, conclu√≠mos que $Z_T$ converge para uma distribui√ß√£o normal bivariada com m√©dia zero e matriz de covari√¢ncia dada por $\begin{bmatrix} \sigma^2 & 0 \\ 0 & \sigma^2/3 \end{bmatrix}$, completando a prova. ‚ñ†

> üí° **Exemplo Num√©rico:** Para ilustrar o Lema 2, vamos calcular o segundo termo para um conjunto de dados simulados e verificar a converg√™ncia de suas vari√¢ncias:
> ```python
> def calculate_second_term(T, sigma_sq, seed):
>  np.random.seed(seed)
>  errors = np.random.normal(0, np.sqrt(sigma_sq), T)
>  t = np.arange(1, T + 1)
>  second_term = (1/T**(3/2)) * np.sum(t * errors)
>  return second_term
>
> # Simula v√°rias vezes para diferentes T
> num_simulations = 500
> T_values = [100, 500, 1000, 5000]
> results = {}
> sigma_sq = 1
>
> for T in T_values:
>     second_terms = [calculate_second_term(T, sigma_sq, i) for i in range(num_simulations)]
>     results[T] = {
>         "mean": np.mean(second_terms),
>         "variance": np.var(second_terms)
>     }
>
> for T, data in results.items():
>     print(f"Para T = {T}:")
>     print(f"  M√©dia: {data['mean']:.4f}")
>     print(f"  Vari√¢ncia: {data['variance']:.4f}")
>     print(f"  Vari√¢ncia Te√≥rica: {sigma_sq/3:.4f}")
> ```
> Os resultados mostrar√£o que a vari√¢ncia amostral do segundo termo converge para $\sigma^2/3$ quando T aumenta, como demonstrado no Lema 2.

### Conclus√£o
A distribui√ß√£o assint√≥tica de $Y_T(b_T - \beta)$ √© dada por [^7]:
$$Y_T(b_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1})$$
onde $Q$ √© a matriz definida acima. Explicitamente, isso implica que [^7]:
$$\begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q^{-1})$$
Este resultado demonstra que a estimativa do coeficiente da tend√™ncia de tempo, $\hat{\delta}_T$, √© superconsistente [^7]. Ou seja, n√£o s√≥ $\hat{\delta}_T$ converge para $\delta$, mas tamb√©m $T(\hat{\delta}_T - \delta)$ converge para zero. As estimativas de m√≠nimos quadrados ordin√°rios, ap√≥s o redimensionamento apropriado, convergem para uma distribui√ß√£o gaussiana assint√≥tica, onde os termos de erro devem seguir um processo de ru√≠do branco para satisfazer as suposi√ß√µes cl√°ssicas de regress√£o. O Teorema do Limite Central foi usado para demonstrar que a combina√ß√£o linear dos estimadores de m√≠nimos quadrados ordin√°rios redimensionados tamb√©m √© assintoticamente gaussiana, implicando que os testes de hip√≥tese padr√£o podem ser aplicados [^7].

> üí° **Exemplo Num√©rico:** Usando as estimativas obtidas anteriormente, podemos calcular as estimativas assint√≥ticas de seus desvios e seus intervalos de confian√ßa:
>
> ```python
> # Calcula Q inversa
> Q_inv = np.linalg.inv(np.array([[1, 1/2],[1/2, 1/3]]))
>
> # Calcula a matriz de covari√¢ncia assint√≥tica
> asymptotic_cov = sigma_sq * Q_inv
>
> # Calcula os desvios padr√£o assint√≥ticos
> std_alpha = np.sqrt(asymptotic_cov[0, 0] / T)
> std_delta = np.sqrt(asymptotic_cov[1, 1] / T**3)
>
> # Calcula os intervalos de confian√ßa (95%)
> z_score = 1.96
> ci_alpha = (b_T[0] - z_score * std_alpha, b_T[0] + z_score * std_alpha)
> ci_delta = (b_T[1] - z_score * std_delta, b_T[1] + z_score * std_delta)
>
> print(f"Intervalo de Confian√ßa para alpha: {ci_alpha}")
> print(f"Intervalo de Confian√ßa para delta: {ci_delta}")
> ```
> Este exemplo mostra como os intervalos de confian√ßa para os par√¢metros $\alpha$ e $\delta$ podem ser calculados usando a distribui√ß√£o assint√≥tica derivada.

**Teorema 1:**  Seja $y_t = \alpha + \delta t + \epsilon_t$, onde $\epsilon_t$ √© um processo de ru√≠do branco com $E[\epsilon_t]=0$, $Var[\epsilon_t] = \sigma^2$, e quarto momento finito, ent√£o as estimativas OLS $\hat{\alpha}_T$ e $\hat{\delta}_T$, adequadamente escaladas, convergem para uma distribui√ß√£o normal bivariada com m√©dia zero e matriz de covari√¢ncia $\sigma^2 Q^{-1}$, onde $Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$.
*Prova:*
I. Pelo Lema 1, sabemos que $Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T$ converge em probabilidade para a matriz $Q$.

II. Pelo Lema 2, sabemos que $Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$ converge em distribui√ß√£o para uma distribui√ß√£o normal bivariada com m√©dia zero e matriz de covari√¢ncia $\begin{bmatrix} \sigma^2 & 0 \\ 0 & \sigma^2/3 \end{bmatrix}$.

III.  O estimador OLS pode ser escrito como:
$$Y_T(b_T - \beta) =  \left( Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right)  Y_T^{-1} \sum_{t=1}^T x_t \epsilon_t$$

IV.  A distribui√ß√£o assint√≥tica de $Y_T(b_T - \beta)$ √© dada pelo produto de uma sequ√™ncia que converge em probabilidade por outra que converge em distribui√ß√£o. Portanto, temos:
$$Y_T(b_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1})$$
onde $Q^{-1}$ √© a inversa da matriz $Q$, que √© dada por $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$

V. Explicitamente, isso significa que:
$$\begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \xrightarrow{d} N\left(0, \sigma^2 \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}\right)$$

Este resultado mostra que as estimativas OLS, quando escaladas adequadamente, convergem para uma distribui√ß√£o normal bivariada, completando a prova do teorema.‚ñ†

### Refer√™ncias
[^1]: Trecho do texto que introduz o cap√≠tulo, discute a diferen√ßa nas distribui√ß√µes assint√≥ticas e apresenta o tema do cap√≠tulo
[^2]: Trecho do texto que apresenta o modelo de regress√£o simples, sua formula√ß√£o matricial e o estimador OLS
[^3]: Trecho do texto que apresenta as somas de $t$ e $t^2$ e suas ordens de crescimento
[^4]: Trecho do texto que introduz a matriz de escala para obter distribui√ß√µes limites n√£o degeneradas
[^5]: Trecho do texto que demonstra a converg√™ncia para uma matriz n√£o singular e apresenta o Teorema do Limite Central para o primeiro termo
[^6]: Trecho do texto que demonstra a converg√™ncia para uma normal multivariada assint√≥tica e a vari√¢ncia do segundo termo
[^7]: Trecho do texto que conclui a deriva√ß√£o da distribui√ß√£o assint√≥tica, define o conceito de superconsist√™ncia e menciona a validade dos testes de hip√≥tese padr√£o.
<!-- END -->
