## Testes de Hip√≥teses para o Modelo de Tend√™ncia Temporal Simples

### Introdu√ß√£o
Este cap√≠tulo continua a an√°lise de modelos com tend√™ncias temporais determin√≠sticas, com foco nos testes de hip√≥teses [^1, ^2, ^3]. J√° estabelecemos as distribui√ß√µes assint√≥ticas dos estimadores OLS em modelos com tend√™ncia linear e exploramos a abordagem geral de Sims, Stock e Watson para processos autorregressivos em torno da tend√™ncia [^1, ^2]. Uma quest√£o central √© se os testes t e F usuais, que s√£o v√°lidos para amostras finitas em modelos com inova√ß√µes gaussianas, permanecem v√°lidos assintoticamente em modelos com inova√ß√µes n√£o gaussianas, considerando as diferentes taxas de converg√™ncia dos estimadores [^1, ^2]. Nesta se√ß√£o, demonstraremos que, apesar das diferentes taxas de converg√™ncia de $\hat{\alpha}$ e $\hat{\delta}$, os testes t e F usuais mant√™m as mesmas distribui√ß√µes assint√≥ticas que em regress√µes estacion√°rias. Ou seja, os erros padr√£o dos estimadores compensam as diferentes taxas de converg√™ncia de forma que os testes estat√≠sticos t√™m um comportamento padr√£o [^8].

### Conceitos Fundamentais
Em modelos de regress√£o com inova√ß√µes gaussianas, os estimadores OLS s√£o tamb√©m gaussianos, e os testes t e F t√™m distribui√ß√µes exatas para amostras finitas [^8]. No entanto, em muitas aplica√ß√µes, as inova√ß√µes podem n√£o ser gaussianas, e, como j√° discutimos, os estimadores podem convergir para seus verdadeiros valores a taxas diferentes [^1, ^2, ^3]. Especificamente, no modelo com tend√™ncia temporal simples $y_t = \alpha + \delta t + \epsilon_t$, o estimador $\hat{\alpha}$ converge para $\alpha$ a uma taxa de $\sqrt{T}$, enquanto $\hat{\delta}$ converge para $\delta$ a uma taxa de $T^{3/2}$ [^5, ^7]. No entanto, mesmo com essas diferen√ßas nas taxas de converg√™ncia, os testes t e F usuais mant√™m sua validade assint√≥tica [^8]. A intui√ß√£o √© que os erros padr√£o dos estimadores tamb√©m incorporam taxas de converg√™ncia diferentes, cancelando a diferen√ßa de converg√™ncia dos par√¢metros no teste estat√≠stico. Isso significa que estat√≠sticas como $(\hat{\delta}_T - \delta) / \hat{\sigma}_{\delta_T}$, onde $\hat{\sigma}_{\delta_T}$ √© o erro padr√£o estimado de $\hat{\delta}_T$, ainda convergem em distribui√ß√£o para uma normal padr√£o [^8].

Primeiramente, consideremos o teste t para a hip√≥tese nula $H_0: \alpha = \alpha_0$. A estat√≠stica t para este teste √© dada por:
$$ t_T = \frac{\hat{\alpha}_T - \alpha_0}{s_T \sqrt{[1 \quad 0](X'X)^{-1} [1 \quad 0]'}} \quad [16.2.1] $$
onde $s_T^2$ √© o estimador OLS da vari√¢ncia do erro:
$$ s_T^2 = \frac{1}{T-2} \sum_{t=1}^T (y_t - \hat{\alpha}_T - \hat{\delta}_T t)^2 \quad [16.2.2] $$
e $(X'X)^{-1}$ √© a matriz inversa de $\sum_{t=1}^T x_t x_t'$ onde $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$ [^8].

Multiplicando o numerador e o denominador de [16.2.1] por $\sqrt{T}$, temos:
$$ t_T = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{s_T \sqrt{[\sqrt{T} \quad 0](X'X)^{-1} [\sqrt{T} \quad 0]'}} \quad [16.2.3] $$
Lembrando que $[\sqrt{T} \quad 0] = [1 \quad 0]Y_T$, e que $[1 \quad 0]Y_T (X'X)^{-1} Y_T[1 \quad 0]' \rightarrow [1 \quad 0] Q^{-1} [1 \quad 0]'$ onde $Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$, podemos reescrever o denominador:
$$ t_T = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{s_T \sqrt{[1 \quad 0]Q^{-1}[1 \quad 0]'}} \quad [16.2.5] $$
Sabemos que $\sqrt{T}(\hat{\alpha}_T - \alpha_0)$ converge em distribui√ß√£o para $N(0, \sigma^2 q^{11})$, onde $q^{11}$ √© o elemento (1,1) de $Q^{-1}$ e que $s_T^2 \overset{p}{\rightarrow} \sigma^2$ [^5, ^8]. Portanto, a estat√≠stica t converge em distribui√ß√£o para uma normal padr√£o:
$$ t_T \overset{d}{\rightarrow} N(0, 1) \quad [16.2.7] $$

> üí° **Exemplo Num√©rico:** Vamos ilustrar a validade assint√≥tica do teste t para $\alpha$ usando uma simula√ß√£o. Consideramos o modelo $y_t = \alpha + \delta t + \epsilon_t$, onde $\alpha = 2$, $\delta = 0.5$, e $\epsilon_t \sim N(0,1)$, $T=200$. Geramos 1000 amostras e testamos a hip√≥tese nula $H_0 : \alpha = 2$.
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
> from scipy.stats import t as student_t
> from scipy.stats import norm
>
> # Par√¢metros
> alpha = 2
> delta = 0.5
> sigma_sq = 1
> T = 200
> num_simulations = 1000
> alpha_0 = 2
>
> # Simula√ß√µes
> t_stats = np.zeros(num_simulations)
>
> for i in range(num_simulations):
>    t = np.arange(1, T + 1)
>    epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
>    y = alpha + delta * t + epsilon
>
>    X = np.column_stack((np.ones(T), t))
>    model = sm.OLS(y, X)
>    results = model.fit()
>    alpha_hat = results.params[0]
>    se_alpha = results.bse[0]
>
>    t_stats[i] = (alpha_hat - alpha_0) / se_alpha
>
> # Plot do histograma das estat√≠sticas t e distribui√ß√£o normal padr√£o
> plt.hist(t_stats, bins=30, density=True, alpha=0.6, color='blue', label='Simula√ß√µes')
> x = np.linspace(min(t_stats), max(t_stats), 100)
> plt.plot(x, norm.pdf(x, 0, 1), color='red', label='Normal Padr√£o')
> plt.title("Distribui√ß√£o da estat√≠stica t para $\alpha$")
> plt.legend()
> plt.show()
>
> # Contagem de quantas vezes a estat√≠stica t est√° fora da regi√£o de rejei√ß√£o do teste
> rejection_rate = np.mean(np.abs(t_stats) > student_t.ppf(0.975, T - 2))
> print(f"Taxa de rejei√ß√£o emp√≠rica (n√≠vel de signific√¢ncia de 5%): {rejection_rate:.4f}")
>
> print(f"M√©dia da estat√≠stica t: {np.mean(t_stats):.4f}")
> print(f"Desvio padr√£o amostral da estat√≠stica t: {np.std(t_stats):.4f}")
> ```
> O histograma das estat√≠sticas t simuladas se aproxima de uma distribui√ß√£o normal padr√£o, comprovando a validade do teste t para grandes amostras. A taxa de rejei√ß√£o se aproxima do n√≠vel de signific√¢ncia nominal de 5%.

De maneira similar, podemos analisar o teste t para a hip√≥tese nula $H_0: \delta = \delta_0$. A estat√≠stica t √© dada por:
$$ t_T = \frac{\hat{\delta}_T - \delta_0}{s_T \sqrt{[0 \quad 1](X'X)^{-1} [0 \quad 1]'}} $$
Multiplicando o numerador e denominador por $T^{3/2}$, temos:
$$ t_T = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{s_T \sqrt{[0 \quad T^{3/2}](X'X)^{-1} [0 \quad T^{3/2}]'}} $$
Usando novamente a rela√ß√£o $[0 \quad T^{3/2}] = [0 \quad 1] Y_T$, e $Y_T(X'X)^{-1} Y_T =  \left[ Y_T^{-1} \left( \sum_{t=1}^T x_t x_t' \right) Y_T^{-1} \right]^{-1} \rightarrow Q^{-1}$, temos:
$$ t_T = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{s_T \sqrt{[0 \quad 1]Q^{-1}[0 \quad 1]'}} $$
Como $T^{3/2}(\hat{\delta}_T - \delta_0)$ converge em distribui√ß√£o para $N(0, \sigma^2 q^{22})$, onde $q^{22}$ √© o elemento (2,2) de $Q^{-1}$ e $s_T^2$ converge em probabilidade para $\sigma^2$, a estat√≠stica t converge em distribui√ß√£o para uma normal padr√£o:
$$ t_T \overset{d}{\rightarrow} N(0, 1) $$
> üí° **Exemplo Num√©rico:** Vamos verificar a validade assint√≥tica do teste t para $\delta$. Usamos o mesmo modelo $y_t = \alpha + \delta t + \epsilon_t$, onde $\alpha = 2$, $\delta = 0.5$, e $\epsilon_t \sim N(0,1)$, $T=200$. Geramos 1000 amostras, desta vez testando a hip√≥tese nula $H_0: \delta = 0.5$.
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
> from scipy.stats import t as student_t
> from scipy.stats import norm
>
> # Par√¢metros
> alpha = 2
> delta = 0.5
> sigma_sq = 1
> T = 200
> num_simulations = 1000
> delta_0 = 0.5
>
> # Simula√ß√µes
> t_stats = np.zeros(num_simulations)
>
> for i in range(num_simulations):
>    t = np.arange(1, T + 1)
>    epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
>    y = alpha + delta * t + epsilon
>
>    X = np.column_stack((np.ones(T), t))
>    model = sm.OLS(y, X)
>    results = model.fit()
>    delta_hat = results.params[1]
>    se_delta = results.bse[1]
>
>    t_stats[i] = (delta_hat - delta_0) / se_delta
>
> # Plot do histograma das estat√≠sticas t e distribui√ß√£o normal padr√£o
> plt.hist(t_stats, bins=30, density=True, alpha=0.6, color='blue', label='Simula√ß√µes')
> x = np.linspace(min(t_stats), max(t_stats), 100)
> plt.plot(x, norm.pdf(x, 0, 1), color='red', label='Normal Padr√£o')
> plt.title("Distribui√ß√£o da estat√≠stica t para $\delta$")
> plt.legend()
> plt.show()
>
> # Contagem de quantas vezes a estat√≠stica t est√° fora da regi√£o de rejei√ß√£o do teste
> rejection_rate = np.mean(np.abs(t_stats) > student_t.ppf(0.975, T - 2))
> print(f"Taxa de rejei√ß√£o emp√≠rica (n√≠vel de signific√¢ncia de 5%): {rejection_rate:.4f}")
>
> print(f"M√©dia da estat√≠stica t: {np.mean(t_stats):.4f}")
> print(f"Desvio padr√£o amostral da estat√≠stica t: {np.std(t_stats):.4f}")
> ```
> O histograma das estat√≠sticas t simuladas se aproxima da distribui√ß√£o normal padr√£o. A taxa de rejei√ß√£o emp√≠rica se aproxima do n√≠vel de signific√¢ncia nominal de 5%, confirmando a validade do teste para grandes amostras.

Esses resultados demonstram que, embora $\hat{\alpha}$ e $\hat{\delta}$ convirjam a taxas diferentes, os erros padr√£o correspondentes compensam essa diferen√ßa, resultando em testes t que convergem para distribui√ß√µes normais padr√£o, mesmo com erros n√£o gaussianos. Portanto, os testes t usuais s√£o assintoticamente v√°lidos.

Essa mesma l√≥gica se aplica a testes de hip√≥teses envolvendo combina√ß√µes lineares de $\alpha$ e $\delta$. Por exemplo, para testar a hip√≥tese $H_0: r_1\alpha + r_2\delta = r$, a estat√≠stica t apropriada √©:
$$ t_T = \frac{r_1 \hat{\alpha}_T + r_2 \hat{\delta}_T - r}{s_T \sqrt{[r_1 \quad r_2](X'X)^{-1}[r_1 \quad r_2]'}} $$
Multiplicando o numerador e denominador pela taxa de converg√™ncia mais lenta, nesse caso $\sqrt{T}$, e aplicando os mesmos argumentos que no caso anterior, podemos mostrar que:
$$ t_T \overset{d}{\rightarrow} N(0, 1) $$
O resultado geral √© que o teste √© dominado pela taxa de converg√™ncia mais lenta, e a estat√≠stica t padr√£o √© assintoticamente v√°lida [^10].

> üí° **Exemplo Num√©rico:** Vamos ilustrar o teste t para combina√ß√µes lineares dos par√¢metros, considerando $H_0: \alpha + 2 \delta = 3$ com o mesmo modelo $y_t = \alpha + \delta t + \epsilon_t$, onde $\alpha = 2$, $\delta = 0.5$, e $\epsilon_t \sim N(0,1)$, $T=200$.
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
> from scipy.stats import t as student_t
> from scipy.stats import norm
>
> # Par√¢metros
> alpha = 2
> delta = 0.5
> sigma_sq = 1
> T = 200
> num_simulations = 1000
> r1 = 1
> r2 = 2
> r = 3
>
> # Simula√ß√µes
> t_stats = np.zeros(num_simulations)
>
> for i in range(num_simulations):
>    t = np.arange(1, T + 1)
>    epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
>    y = alpha + delta * t + epsilon
>
>    X = np.column_stack((np.ones(T), t))
>    model = sm.OLS(y, X)
>    results = model.fit()
>    alpha_hat = results.params[0]
>    delta_hat = results.params[1]
>    se_params = results.bse
>    cov_params = results.cov_params()
>
>    num = r1 * alpha_hat + r2 * delta_hat - r
>    den = np.sqrt( r1**2 * se_params[0]**2 + r2**2 * se_params[1]**2 + 2 * r1 * r2 * cov_params[0,1] )
>
>    t_stats[i] = num / den
>
> # Plot do histograma das estat√≠sticas t e distribui√ß√£o normal padr√£o
> plt.hist(t_stats, bins=30, density=True, alpha=0.6, color='blue', label='Simula√ß√µes')
> x = np.linspace(min(t_stats), max(t_stats), 100)
> plt.plot(x, norm.pdf(x, 0, 1), color='red', label='Normal Padr√£o')
> plt.title("Distribui√ß√£o da estat√≠stica t para $H_0: \alpha + 2\delta = 3$")
> plt.legend()
> plt.show()
>
> # Contagem de quantas vezes a estat√≠stica t est√° fora da regi√£o de rejei√ß√£o do teste
> rejection_rate = np.mean(np.abs(t_stats) > student_t.ppf(0.975, T - 2))
> print(f"Taxa de rejei√ß√£o emp√≠rica (n√≠vel de signific√¢ncia de 5%): {rejection_rate:.4f}")
>
> print(f"M√©dia da estat√≠stica t: {np.mean(t_stats):.4f}")
> print(f"Desvio padr√£o amostral da estat√≠stica t: {np.std(t_stats):.4f}")
> ```
> A estat√≠stica t simulada segue uma distribui√ß√£o normal padr√£o e sua taxa de rejei√ß√£o √© pr√≥xima ao n√≠vel de signific√¢ncia desejado.

Al√©m dos testes t, o cap√≠tulo aborda o teste F, que √© utilizado para testar hip√≥teses conjuntas sobre os par√¢metros do modelo. Para um teste com uma √∫nica restri√ß√£o, como $H_0: r_1\alpha + r_2\delta = r$, a estat√≠stica F √© o quadrado da estat√≠stica t [^9]. Essa rela√ß√£o tamb√©m se mant√©m assintoticamente, de forma que a distribui√ß√£o da estat√≠stica F se aproxima de uma distribui√ß√£o $\chi^2(1)$ quando o tamanho da amostra cresce.

De forma mais geral, para testar m√∫ltiplas hip√≥teses sobre os par√¢metros, $R\beta = r$, onde $R$ √© uma matriz de restri√ß√µes, $\beta$ √© o vetor de par√¢metros e $r$ √© um vetor de constantes, a estat√≠stica Wald para o teste F √© dada por:
$$ \chi^2 = (Rb - r)'[s^2 R(X'X)^{-1}R']^{-1}(Rb - r) \quad [16.3.19] $$

> üí° **Exemplo Num√©rico:** Para ilustrar o teste Wald, vamos simular e testar a hip√≥tese conjunta de que $\alpha = 2$ e $\delta = 0.5$. Considere um modelo com $\alpha = 2$, $\delta = 0.5$, $\sigma^2 = 1$ e $T = 200$.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
> from scipy.stats import chi2
>
> # Par√¢metros
> alpha = 2
> delta = 0.5
> sigma_sq = 1
> T = 200
> num_simulations = 1000
>
> # Valores para a hip√≥tese nula
> alpha_null = 2
> delta_null = 0.5
>
> # Simula√ß√µes
> chi2_stats = np.zeros(num_simulations)
>
> for i in range(num_simulations):
>   t = np.arange(1, T + 1)
>   epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
>   y = alpha + delta * t + epsilon
>
>   X = np.column_stack((np.ones(T), t))
>   model = sm.OLS(y, X)
>   results = model.fit()
>   b = results.params
>   s = results.scale
>   cov_mat = results.cov_params()
>
>   # Definindo a matriz R e o vetor r
>   R = np.eye(2)
>   r = np.array([alpha_null, delta_null])
>
>   # Calculando a estat√≠stica Wald
>   x2_stat = np.dot(np.dot(np.transpose(b - r), np.linalg.inv(s * (np.dot(np.dot(R, cov_mat), np.transpose(R))) )), (b-r))
>
>   chi2_stats[i] = x2_stat
>
> # Plot do histograma da estat√≠stica chi2
> plt.hist(chi2_stats, bins=30, density=True, alpha=0.6, color='blue', label='Simula√ß√µes')
> x = np.linspace(min(chi2_stats), max(chi2_stats), 100)
> plt.plot(x, chi2.pdf(x, 2), color='red', label='Qui-quadrado(2)')
> plt.title("Distribui√ß√£o da Estat√≠stica Wald")
> plt.legend()
> plt.show()
>
> # Taxa de rejei√ß√£o para um n√≠vel de signific√¢ncia de 5%
> rejection_rate = np.mean(chi2_stats > chi2.ppf(0.95, 2))
> print(f"Taxa de rejei√ß√£o emp√≠rica: {rejection_rate:.4f}")
>
> print(f"M√©dia amostral da estat√≠stica Wald: {np.mean(chi2_stats):.4f}")
> print(f"Vari√¢ncia amostral da estat√≠stica Wald: {np.var(chi2_stats):.4f}")
> ```
> O histograma da estat√≠stica $\chi^2$ simulada se aproxima de uma distribui√ß√£o $\chi^2$ com 2 graus de liberdade. A taxa de rejei√ß√£o emp√≠rica √© pr√≥xima do n√≠vel de signific√¢ncia de 5%.

A estat√≠stica $\chi^2$ se aproxima de uma distribui√ß√£o $\chi^2$ com $m$ graus de liberdade, onde $m$ √© o n√∫mero de restri√ß√µes, conforme demonstrado no cap√≠tulo 8 [^9].

**Proposi√ß√£o 1:** A estat√≠stica Wald [16.3.19], sob a hip√≥tese nula $R\beta = r$ e com as condi√ß√µes usuais de regularidade, converge em distribui√ß√£o para uma vari√°vel aleat√≥ria qui-quadrado com $m$ graus de liberdade, onde $m$ √© o n√∫mero de restri√ß√µes impostas por $R$.

*Prova:*
I. Sob a hip√≥tese nula $R\beta = r$, temos que $R\hat{\beta} - r = R(\hat{\beta} - \beta)$.
II. Sabemos que $\sqrt{T}(\hat{\beta} - \beta)$ converge em distribui√ß√£o para uma normal multivariada com m√©dia zero e matriz de covari√¢ncia dada por $\sigma^2 Q^{-1}$, onde $Q = \lim_{T \to \infty} \frac{1}{T}X'X$ (assumindo que o limite existe). Ou seja, $\sqrt{T}(\hat{\beta} - \beta) \overset{d}{\rightarrow} N(0, \sigma^2 Q^{-1})$.
III. Portanto, $\sqrt{T} R (\hat{\beta} - \beta) \overset{d}{\rightarrow} N(0, \sigma^2 RQ^{-1}R')$.
IV. A estat√≠stica de Wald pode ser reescrita como:
    $$\chi^2 = T(R\hat{\beta} - r)' [s^2 T R(X'X)^{-1}R']^{-1} (R\hat{\beta} - r)$$
V. Como $s^2 \overset{p}{\rightarrow} \sigma^2$ e $ \frac{1}{T}X'X \rightarrow Q $, podemos substituir esses valores na estat√≠stica de Wald:
   $$\chi^2 \approx T(R\hat{\beta} - r)' [\sigma^2 R Q^{-1}R']^{-1} (R\hat{\beta} - r)$$
VI.  Definindo $Z = \sqrt{T} R (\hat{\beta} - \beta)$, sabemos que $Z \overset{d}{\rightarrow} N(0, \sigma^2 RQ^{-1}R')$, ent√£o podemos reescrever a estat√≠stica de Wald como:
    $$\chi^2 \approx  Z'(\sigma^2 RQ^{-1}R')^{-1} Z$$
VII. Pela propriedade de formas quadr√°ticas de vari√°veis normais, temos que $Z'(\sigma^2 RQ^{-1}R')^{-1} Z$ converge em distribui√ß√£o para uma vari√°vel qui-quadrado com $m$ graus de liberdade, onde $m$ √© o n√∫mero de restri√ß√µes impostas pela matriz $R$.
VIII. Portanto, a estat√≠stica de Wald converge em distribui√ß√£o para uma vari√°vel qui-quadrado com $m$ graus de liberdade:
$$\chi^2 \overset{d}{\rightarrow} \chi^2(m)$$
‚ñ†

**Lema 1:** Se $\hat{\beta}$ √© o estimador OLS do vetor de par√¢metros $\beta$ no modelo $y = X\beta + \epsilon$, e se as condi√ß√µes de Gauss-Markov para o modelo se mant√™m, ent√£o o estimador da vari√¢ncia dos erros $s^2$ converge em probabilidade para a vari√¢ncia populacional $\sigma^2$, isto √©, $s^2 \overset{p}{\rightarrow} \sigma^2$.

*Prova:*
I. O estimador da vari√¢ncia dos erros √© dado por:
    $$s^2 = \frac{1}{T-k} \sum_{t=1}^T e_t^2$$
    onde $e_t = y_t - x_t'\hat{\beta}$ s√£o os res√≠duos OLS e $k$ √© o n√∫mero de par√¢metros no modelo.
II. Sabemos que a soma dos quadrados dos res√≠duos pode ser escrita como:
    $$\sum_{t=1}^T e_t^2 = \epsilon'M\epsilon$$
    onde $M = I - X(X'X)^{-1}X'$ √© a matriz de proje√ß√£o que aniquila o espa√ßo coluna de $X$, e $\epsilon$ √© o vetor de erros.
III. Assim, podemos reescrever o estimador de vari√¢ncia como:
    $$s^2 = \frac{\epsilon'M\epsilon}{T-k}$$
IV.  O valor esperado de $s^2$ √©:
    $$E[s^2] = E\left[\frac{\epsilon'M\epsilon}{T-k}\right] = \frac{1}{T-k}E[\text{tr}(\epsilon'M\epsilon)] = \frac{1}{T-k}E[\text{tr}(M\epsilon\epsilon')] = \frac{1}{T-k}\text{tr}(ME[\epsilon\epsilon'])$$
V. Sob as condi√ß√µes de Gauss-Markov, $E[\epsilon\epsilon'] = \sigma^2 I$, e como $M$ √© idempotente ($M = M^2$), temos:
    $$E[s^2] = \frac{1}{T-k}\text{tr}(M\sigma^2 I) = \frac{\sigma^2}{T-k}\text{tr}(M) = \frac{\sigma^2}{T-k} \text{tr}(I - X(X'X)^{-1}X') = \frac{\sigma^2}{T-k}(T - k) = \sigma^2$$
VI. Para mostrar a converg√™ncia em probabilidade, precisamos mostrar que a vari√¢ncia de $s^2$ converge para zero conforme $T$ tende ao infinito. Sabemos que:
    $$ Var(s^2) = Var \left( \frac{\epsilon'M\epsilon}{T-k} \right) = \frac{1}{(T-k)^2} Var(\epsilon'M\epsilon)$$
VII. Sob a condi√ß√£o de que $\epsilon_t$ tem momentos de quarta ordem finitos, pode-se demonstrar que:
    $$Var(\epsilon'M\epsilon) = 2\sigma^4 tr(M^2) = 2 \sigma^4 tr(M) = 2\sigma^4(T-k)$$
VIII. Portanto:
    $$Var(s^2) = \frac{2 \sigma^4 (T-k)}{(T-k)^2} = \frac{2\sigma^4}{T-k}$$
IX. Como $T-k$ tende para infinito conforme $T$ tende para infinito, a vari√¢ncia de $s^2$ converge para zero:
    $$\lim_{T \to \infty} Var(s^2) = 0$$
X. Pela desigualdade de Chebyshev, para qualquer $\varepsilon > 0$:
    $$P(|s^2 - E[s^2]| \ge \varepsilon) \le \frac{Var(s^2)}{\varepsilon^2}$$
XI. Como $\lim_{T \to \infty} Var(s^2) = 0$, temos que:
    $$\lim_{T \to \infty} P(|s^2 - \sigma^2| \ge \varepsilon) = 0$$
XII. Isso demonstra que $s^2$ converge em probabilidade para $\sigma^2$:
$$s^2 \overset{p}{\rightarrow} \sigma^2$$
‚ñ†

**Corol√°rio 1:**  Se considerarmos o modelo com tend√™ncia temporal simples $y_t = \alpha + \delta t + \epsilon_t$, e se os erros $\epsilon_t$ forem i.i.d. com m√©dia zero e vari√¢ncia $\sigma^2$, e se a condi√ß√£o de Gauss-Markov se mantiver, a estat√≠stica $s^2$ definida em [16.2.2] converge em probabilidade para $\sigma^2$.

*Prova:*
I. O modelo com tend√™ncia temporal simples $y_t = \alpha + \delta t + \epsilon_t$ se encaixa na forma geral do modelo de regress√£o linear $y = X\beta + \epsilon$.
II. As condi√ß√µes de Gauss-Markov s√£o satisfeitas, pois os erros $\epsilon_t$ s√£o i.i.d. com m√©dia zero e vari√¢ncia $\sigma^2$.
III. Pelo Lema 1, sabemos que o estimador da vari√¢ncia dos erros $s^2$ converge em probabilidade para a vari√¢ncia populacional $\sigma^2$ sob as condi√ß√µes de Gauss-Markov.
IV. Portanto, a estat√≠stica $s^2$ definida em [16.2.2] converge em probabilidade para $\sigma^2$:
$$s^2 \overset{p}{\rightarrow} \sigma^2$$
‚ñ†

Em resumo, os testes de hip√≥teses sobre os par√¢metros em modelos com tend√™ncias determin√≠sticas podem ser realizados utilizando a abordagem usual com as estat√≠sticas t e F, j√° que as mesmas mant√©m validade assint√≥tica, ainda que com diferentes taxas de converg√™ncia de seus estimadores. Essa equival√™ncia se d√° pelo comportamento assint√≥tico dos erros padr√£o dos estimadores que acomodam a diferen√ßa de converg√™ncia dos par√¢metros.

### Conclus√£o
Nesta se√ß√£o, mostramos que os testes t e F usuais s√£o assintoticamente v√°lidos para modelos de regress√£o com tend√™ncias temporais determin√≠sticas, mesmo quando as inova√ß√µes n√£o s√£o gaussianas [^8]. A transforma√ß√£o de Sims, Stock e Watson, discutida na se√ß√£o anterior, embora fundamental para derivar as distribui√ß√µes assint√≥ticas dos estimadores, n√£o √© necess√°ria para a aplica√ß√£o dos testes de hip√≥teses. Os erros padr√£o estimados compensam as diferentes taxas de converg√™ncia dos estimadores de forma que os testes t√™m um comportamento padronizado. Os resultados apresentados aqui fornecem uma base s√≥lida para a infer√™ncia estat√≠stica em modelos com tend√™ncias determin√≠sticas, e se estendem para processos autorregressivos em torno da tend√™ncia [^11]. Nos cap√≠tulos seguintes, exploraremos como essas t√©cnicas se estendem a modelos com ra√≠zes unit√°rias e outros tipos de n√£o estacionariedade [^2].

### Refer√™ncias
[^1]: Trecho do texto original que introduz o tema de distribui√ß√µes assint√≥ticas em modelos com tend√™ncias determin√≠sticas e a abordagem geral de Sims, Stock e Watson (1990).
[^2]: Trecho do texto original que menciona a utiliza√ß√£o de t√©cnicas diferentes daquelas usadas em modelos estacion√°rios e a relev√¢ncia do cap√≠tulo para estudos de processos n√£o estacion√°rios (Cap√≠tulos 17 e 18).
[^3]: Trecho do texto original que introduz a ideia de que as estat√≠sticas usuais de OLS tem as mesmas distribui√ß√µes assint√≥ticas de regress√µes estacion√°rias, mesmo com diferentes taxas de converg√™ncia.
[^4]: Trecho do texto original que demonstra a diverg√™ncia da matriz $(1/T) \sum x_t x_t'$ e a necessidade de dividir por $T^3$ em vez de $T$.
[^5]: Trecho do texto original que introduz a matriz $Y_T$ para o reescalonamento das vari√°veis e deriva a distribui√ß√£o assint√≥tica do segundo termo ap√≥s o reescalonamento.
[^7]: Trecho do texto original que afirma a superconsist√™ncia do estimador do coeficiente da tend√™ncia temporal.
[^8]: Trecho do texto original que explica que as estat√≠sticas t e F usuais s√£o v√°lidas assintoticamente quando os erros s√£o gaussianos e sugere que isso √© verdade em casos n√£o gaussianos.
[^9]: Trecho do texto original que explica o uso do teste F como o quadrado do teste t para restri√ß√µes lineares, e a converg√™ncia para uma distribui√ß√£o $\chi^2$.
[^10]: Trecho do texto original que explica que testes com restri√ß√µes em par√¢metros com diferentes taxas de converg√™ncia s√£o dominadas assintoticamente pelas taxas mais lentas.
[^11]: Trecho do texto original que descreve o modelo autorregressivo com tend√™ncia temporal e a transforma√ß√£o de Sims, Stock, e Watson.
<!-- END -->
