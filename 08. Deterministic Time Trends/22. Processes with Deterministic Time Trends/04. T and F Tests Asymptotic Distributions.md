## Testes de HipÃ³teses para o Modelo de TendÃªncia Temporal Simples

### IntroduÃ§Ã£o
Este capÃ­tulo continua a anÃ¡lise de modelos com tendÃªncias temporais determinÃ­sticas, com foco nos testes de hipÃ³teses [^1, ^2, ^3]. JÃ¡ estabelecemos as distribuiÃ§Ãµes assintÃ³ticas dos estimadores OLS em modelos com tendÃªncia linear e exploramos a abordagem geral de Sims, Stock e Watson para processos autorregressivos em torno da tendÃªncia [^1, ^2]. Uma questÃ£o central Ã© se os testes t e F usuais, que sÃ£o vÃ¡lidos para amostras finitas em modelos com inovaÃ§Ãµes gaussianas, permanecem vÃ¡lidos assintoticamente em modelos com inovaÃ§Ãµes nÃ£o gaussianas, considerando as diferentes taxas de convergÃªncia dos estimadores [^1, ^2]. Nesta seÃ§Ã£o, demonstraremos que, apesar das diferentes taxas de convergÃªncia de $\hat{\alpha}$ e $\hat{\delta}$, os testes t e F usuais mantÃªm as mesmas distribuiÃ§Ãµes assintÃ³ticas que em regressÃµes estacionÃ¡rias. Ou seja, os erros padrÃ£o dos estimadores compensam as diferentes taxas de convergÃªncia de forma que os testes estatÃ­sticos tÃªm um comportamento padrÃ£o [^8].

### Conceitos Fundamentais
Em modelos de regressÃ£o com inovaÃ§Ãµes gaussianas, os estimadores OLS sÃ£o tambÃ©m gaussianos, e os testes t e F tÃªm distribuiÃ§Ãµes exatas para amostras finitas [^8]. No entanto, em muitas aplicaÃ§Ãµes, as inovaÃ§Ãµes podem nÃ£o ser gaussianas, e, como jÃ¡ discutimos, os estimadores podem convergir para seus verdadeiros valores a taxas diferentes [^1, ^2, ^3]. Especificamente, no modelo com tendÃªncia temporal simples $y_t = \alpha + \delta t + \epsilon_t$, o estimador $\hat{\alpha}$ converge para $\alpha$ a uma taxa de $\sqrt{T}$, enquanto $\hat{\delta}$ converge para $\delta$ a uma taxa de $T^{3/2}$ [^5, ^7]. No entanto, mesmo com essas diferenÃ§as nas taxas de convergÃªncia, os testes t e F usuais mantÃªm sua validade assintÃ³tica [^8]. A intuiÃ§Ã£o Ã© que os erros padrÃ£o dos estimadores tambÃ©m incorporam taxas de convergÃªncia diferentes, cancelando a diferenÃ§a de convergÃªncia dos parÃ¢metros no teste estatÃ­stico. Isso significa que estatÃ­sticas como $(\hat{\delta}_T - \delta) / \hat{\sigma}_{\delta_T}$, onde $\hat{\sigma}_{\delta_T}$ Ã© o erro padrÃ£o estimado de $\hat{\delta}_T$, ainda convergem em distribuiÃ§Ã£o para uma normal padrÃ£o [^8].

Primeiramente, consideremos o teste t para a hipÃ³tese nula $H_0: \alpha = \alpha_0$. A estatÃ­stica t para este teste Ã© dada por:
$$ t_T = \frac{\hat{\alpha}_T - \alpha_0}{s_T \sqrt{[1 \quad 0](X'X)^{-1} [1 \quad 0]'}} \quad [16.2.1] $$
onde $s_T^2$ Ã© o estimador OLS da variÃ¢ncia do erro:
$$ s_T^2 = \frac{1}{T-2} \sum_{t=1}^T (y_t - \hat{\alpha}_T - \hat{\delta}_T t)^2 \quad [16.2.2] $$
e $(X'X)^{-1}$ Ã© a matriz inversa de $\sum_{t=1}^T x_t x_t'$ onde $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$ [^8].

Multiplicando o numerador e o denominador de [16.2.1] por $\sqrt{T}$, temos:
$$ t_T = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{s_T \sqrt{[\sqrt{T} \quad 0](X'X)^{-1} [\sqrt{T} \quad 0]'}} \quad [16.2.3] $$
Lembrando que $[\sqrt{T} \quad 0] = [1 \quad 0]Y_T$, e que $[1 \quad 0]Y_T (X'X)^{-1} Y_T[1 \quad 0]' \rightarrow [1 \quad 0] Q^{-1} [1 \quad 0]'$ onde $Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$, podemos reescrever o denominador:
$$ t_T = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{s_T \sqrt{[1 \quad 0]Q^{-1}[1 \quad 0]'}} \quad [16.2.5] $$
Sabemos que $\sqrt{T}(\hat{\alpha}_T - \alpha_0)$ converge em distribuiÃ§Ã£o para $N(0, \sigma^2 q^{11})$, onde $q^{11}$ Ã© o elemento (1,1) de $Q^{-1}$ e que $s_T^2 \overset{p}{\rightarrow} \sigma^2$ [^5, ^8]. Portanto, a estatÃ­stica t converge em distribuiÃ§Ã£o para uma normal padrÃ£o:
$$ t_T \overset{d}{\rightarrow} N(0, 1) \quad [16.2.7] $$

> ğŸ’¡ **Exemplo NumÃ©rico:** Vamos ilustrar a validade assintÃ³tica do teste t para $\alpha$ usando uma simulaÃ§Ã£o. Consideramos o modelo $y_t = \alpha + \delta t + \epsilon_t$, onde $\alpha = 2$, $\delta = 0.5$, e $\epsilon_t \sim N(0,1)$, $T=200$. Geramos 1000 amostras e testamos a hipÃ³tese nula $H_0 : \alpha = 2$.
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
> from scipy.stats import t as student_t
> from scipy.stats import norm
>
> # ParÃ¢metros
> alpha = 2
> delta = 0.5
> sigma_sq = 1
> T = 200
> num_simulations = 1000
> alpha_0 = 2
>
> # SimulaÃ§Ãµes
> t_stats = np.zeros(num_simulations)
>
> for i in range(num_simulations):
>    t = np.arange(1, T + 1)
>    epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
>    y = alpha + delta * t + epsilon
>
>    X = np.column_stack((np.ones(T), t))
>    model = sm.OLS(y, X)
>    results = model.fit()
>    alpha_hat = results.params[0]
>    se_alpha = results.bse[0]
>
>    t_stats[i] = (alpha_hat - alpha_0) / se_alpha
>
> # Plot do histograma das estatÃ­sticas t e distribuiÃ§Ã£o normal padrÃ£o
> plt.hist(t_stats, bins=30, density=True, alpha=0.6, color='blue', label='SimulaÃ§Ãµes')
> x = np.linspace(min(t_stats), max(t_stats), 100)
> plt.plot(x, norm.pdf(x, 0, 1), color='red', label='Normal PadrÃ£o')
> plt.title("DistribuiÃ§Ã£o da estatÃ­stica t para $\alpha$")
> plt.legend()
> plt.show()
>
> # Contagem de quantas vezes a estatÃ­stica t estÃ¡ fora da regiÃ£o de rejeiÃ§Ã£o do teste
> rejection_rate = np.mean(np.abs(t_stats) > student_t.ppf(0.975, T - 2))
> print(f"Taxa de rejeiÃ§Ã£o empÃ­rica (nÃ­vel de significÃ¢ncia de 5%): {rejection_rate:.4f}")
>
> print(f"MÃ©dia da estatÃ­stica t: {np.mean(t_stats):.4f}")
> print(f"Desvio padrÃ£o amostral da estatÃ­stica t: {np.std(t_stats):.4f}")
> ```
> O histograma das estatÃ­sticas t simuladas se aproxima de uma distribuiÃ§Ã£o normal padrÃ£o, comprovando a validade do teste t para grandes amostras. A taxa de rejeiÃ§Ã£o se aproxima do nÃ­vel de significÃ¢ncia nominal de 5%.

De maneira similar, podemos analisar o teste t para a hipÃ³tese nula $H_0: \delta = \delta_0$. A estatÃ­stica t Ã© dada por:
$$ t_T = \frac{\hat{\delta}_T - \delta_0}{s_T \sqrt{[0 \quad 1](X'X)^{-1} [0 \quad 1]'}} $$
Multiplicando o numerador e denominador por $T^{3/2}$, temos:
$$ t_T = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{s_T \sqrt{[0 \quad T^{3/2}](X'X)^{-1} [0 \quad T^{3/2}]'}} $$
Usando novamente a relaÃ§Ã£o $[0 \quad T^{3/2}] = [0 \quad 1] Y_T$, e $Y_T(X'X)^{-1} Y_T =  \left[ Y_T^{-1} \left( \sum_{t=1}^T x_t x_t' \right) Y_T^{-1} \right]^{-1} \rightarrow Q^{-1}$, temos:
$$ t_T = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{s_T \sqrt{[0 \quad 1]Q^{-1}[0 \quad 1]'}} $$
Como $T^{3/2}(\hat{\delta}_T - \delta_0)$ converge em distribuiÃ§Ã£o para $N(0, \sigma^2 q^{22})$, onde $q^{22}$ Ã© o elemento (2,2) de $Q^{-1}$ e $s_T^2$ converge em probabilidade para $\sigma^2$, a estatÃ­stica t converge em distribuiÃ§Ã£o para uma normal padrÃ£o:
$$ t_T \overset{d}{\rightarrow} N(0, 1) $$
> ğŸ’¡ **Exemplo NumÃ©rico:** Vamos verificar a validade assintÃ³tica do teste t para $\delta$. Usamos o mesmo modelo $y_t = \alpha + \delta t + \epsilon_t$, onde $\alpha = 2$, $\delta = 0.5$, e $\epsilon_t \sim N(0,1)$, $T=200$. Geramos 1000 amostras, desta vez testando a hipÃ³tese nula $H_0: \delta = 0.5$.
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
> from scipy.stats import t as student_t
> from scipy.stats import norm
>
> # ParÃ¢metros
> alpha = 2
> delta = 0.5
> sigma_sq = 1
> T = 200
> num_simulations = 1000
> delta_0 = 0.5
>
> # SimulaÃ§Ãµes
> t_stats = np.zeros(num_simulations)
>
> for i in range(num_simulations):
>    t = np.arange(1, T + 1)
>    epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
>    y = alpha + delta * t + epsilon
>
>    X = np.column_stack((np.ones(T), t))
>    model = sm.OLS(y, X)
>    results = model.fit()
>    delta_hat = results.params[1]
>    se_delta = results.bse[1]
>
>    t_stats[i] = (delta_hat - delta_0) / se_delta
>
> # Plot do histograma das estatÃ­sticas t e distribuiÃ§Ã£o normal padrÃ£o
> plt.hist(t_stats, bins=30, density=True, alpha=0.6, color='blue', label='SimulaÃ§Ãµes')
> x = np.linspace(min(t_stats), max(t_stats), 100)
> plt.plot(x, norm.pdf(x, 0, 1), color='red', label='Normal PadrÃ£o')
> plt.title("DistribuiÃ§Ã£o da estatÃ­stica t para $\delta$")
> plt.legend()
> plt.show()
>
> # Contagem de quantas vezes a estatÃ­stica t estÃ¡ fora da regiÃ£o de rejeiÃ§Ã£o do teste
> rejection_rate = np.mean(np.abs(t_stats) > student_t.ppf(0.975, T - 2))
> print(f"Taxa de rejeiÃ§Ã£o empÃ­rica (nÃ­vel de significÃ¢ncia de 5%): {rejection_rate:.4f}")
>
> print(f"MÃ©dia da estatÃ­stica t: {np.mean(t_stats):.4f}")
> print(f"Desvio padrÃ£o amostral da estatÃ­stica t: {np.std(t_stats):.4f}")
> ```
> O histograma das estatÃ­sticas t simuladas se aproxima da distribuiÃ§Ã£o normal padrÃ£o. A taxa de rejeiÃ§Ã£o empÃ­rica se aproxima do nÃ­vel de significÃ¢ncia nominal de 5%, confirmando a validade do teste para grandes amostras.

Esses resultados demonstram que, embora $\hat{\alpha}$ e $\hat{\delta}$ convirjam a taxas diferentes, os erros padrÃ£o correspondentes compensam essa diferenÃ§a, resultando em testes t que convergem para distribuiÃ§Ãµes normais padrÃ£o, mesmo com erros nÃ£o gaussianos. Portanto, os testes t usuais sÃ£o assintoticamente vÃ¡lidos.

Essa mesma lÃ³gica se aplica a testes de hipÃ³teses envolvendo combinaÃ§Ãµes lineares de $\alpha$ e $\delta$. Por exemplo, para testar a hipÃ³tese $H_0: r_1\alpha + r_2\delta = r$, a estatÃ­stica t apropriada Ã©:
$$ t_T = \frac{r_1 \hat{\alpha}_T + r_2 \hat{\delta}_T - r}{s_T \sqrt{[r_1 \quad r_2](X'X)^{-1}[r_1 \quad r_2]'}} $$
Multiplicando o numerador e denominador pela taxa de convergÃªncia mais lenta, nesse caso $\sqrt{T}$, e aplicando os mesmos argumentos que no caso anterior, podemos mostrar que:
$$ t_T \overset{d}{\rightarrow} N(0, 1) $$
O resultado geral Ã© que o teste Ã© dominado pela taxa de convergÃªncia mais lenta, e a estatÃ­stica t padrÃ£o Ã© assintoticamente vÃ¡lida [^10].

> ğŸ’¡ **Exemplo NumÃ©rico:** Vamos ilustrar o teste t para combinaÃ§Ãµes lineares dos parÃ¢metros, considerando $H_0: \alpha + 2 \delta = 3$ com o mesmo modelo $y_t = \alpha + \delta t + \epsilon_t$, onde $\alpha = 2$, $\delta = 0.5$, e $\epsilon_t \sim N(0,1)$, $T=200$.
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
> from scipy.stats import t as student_t
> from scipy.stats import norm
>
> # ParÃ¢metros
> alpha = 2
> delta = 0.5
> sigma_sq = 1
> T = 200
> num_simulations = 1000
> r1 = 1
> r2 = 2
> r = 3
>
> # SimulaÃ§Ãµes
> t_stats = np.zeros(num_simulations)
>
> for i in range(num_simulations):
>    t = np.arange(1, T + 1)
>    epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
>    y = alpha + delta * t + epsilon
>
>    X = np.column_stack((np.ones(T), t))
>    model = sm.OLS(y, X)
>    results = model.fit()
>    alpha_hat = results.params[0]
>    delta_hat = results.params[1]
>    se_params = results.bse
>    cov_params = results.cov_params()
>
>    num = r1 * alpha_hat + r2 * delta_hat - r
>    den = np.sqrt( r1**2 * se_params[0]**2 + r2**2 * se_params[1]**2 + 2 * r1 * r2 * cov_params[0,1] )
>
>    t_stats[i] = num / den
>
> # Plot do histograma das estatÃ­sticas t e distribuiÃ§Ã£o normal padrÃ£o
> plt.hist(t_stats, bins=30, density=True, alpha=0.6, color='blue', label='SimulaÃ§Ãµes')
> x = np.linspace(min(t_stats), max(t_stats), 100)
> plt.plot(x, norm.pdf(x, 0, 1), color='red', label='Normal PadrÃ£o')
> plt.title("DistribuiÃ§Ã£o da estatÃ­stica t para $H_0: \alpha + 2\delta = 3$")
> plt.legend()
> plt.show()
>
> # Contagem de quantas vezes a estatÃ­stica t estÃ¡ fora da regiÃ£o de rejeiÃ§Ã£o do teste
> rejection_rate = np.mean(np.abs(t_stats) > student_t.ppf(0.975, T - 2))
> print(f"Taxa de rejeiÃ§Ã£o empÃ­rica (nÃ­vel de significÃ¢ncia de 5%): {rejection_rate:.4f}")
>
> print(f"MÃ©dia da estatÃ­stica t: {np.mean(t_stats):.4f}")
> print(f"Desvio padrÃ£o amostral da estatÃ­stica t: {np.std(t_stats):.4f}")
> ```
> A estatÃ­stica t simulada segue uma distribuiÃ§Ã£o normal padrÃ£o e sua taxa de rejeiÃ§Ã£o Ã© prÃ³xima ao nÃ­vel de significÃ¢ncia desejado.

AlÃ©m dos testes t, o capÃ­tulo aborda o teste F, que Ã© utilizado para testar hipÃ³teses conjuntas sobre os parÃ¢metros do modelo. Para um teste com uma Ãºnica restriÃ§Ã£o, como $H_0: r_1\alpha + r_2\delta = r$, a estatÃ­stica F Ã© o quadrado da estatÃ­stica t [^9]. Essa relaÃ§Ã£o tambÃ©m se mantÃ©m assintoticamente, de forma que a distribuiÃ§Ã£o da estatÃ­stica F se aproxima de uma distribuiÃ§Ã£o $\chi^2(1)$ quando o tamanho da amostra cresce.

De forma mais geral, para testar mÃºltiplas hipÃ³teses sobre os parÃ¢metros, $R\beta = r$, onde $R$ Ã© uma matriz de restriÃ§Ãµes, $\beta$ Ã© o vetor de parÃ¢metros e $r$ Ã© um vetor de constantes, a estatÃ­stica Wald para o teste F Ã© dada por:
$$ \chi^2 = (Rb - r)'[s^2 R(X'X)^{-1}R']^{-1}(Rb - r) \quad [16.3.19] $$

> ğŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar o teste Wald, vamos simular e testar a hipÃ³tese conjunta de que $\alpha = 2$ e $\delta = 0.5$. Considere um modelo com $\alpha = 2$, $\delta = 0.5$, $\sigma^2 = 1$ e $T = 200$.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
> from scipy.stats import chi2
>
> # ParÃ¢metros
> alpha = 2
> delta = 0.5
> sigma_sq = 1
> T = 200
> num_simulations = 1000
>
> # Valores para a hipÃ³tese nula
> alpha_null = 2
> delta_null = 0.5
>
> # SimulaÃ§Ãµes
> chi2_stats = np.zeros(num_simulations)
>
> for i in range(num_simulations):
>   t = np.arange(1, T + 1)
>   epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
>   y = alpha + delta * t + epsilon
>
>   X = np.column_stack((np.ones(T), t))
>   model = sm.OLS(y, X)
>   results = model.fit()
>   b = results.params
>   s = results.scale
>   cov_mat = results.cov_params()
>
>   # Definindo a matriz R e o vetor r
>   R = np.eye(2)
>   r = np.array([alpha_null, delta_null])
>
>   # Calculando a estatÃ­stica Wald
>   x2_stat = np.dot(np.dot(np.transpose(b - r), np.linalg.inv(s * (np.dot(np.dot(R, cov_mat), np.transpose(R))) )), (b-r))
>
>   chi2_stats[i] = x2_stat
>
> # Plot do histograma da estatÃ­stica chi2
> plt.hist(chi2_stats, bins=30, density=True, alpha=0.6, color='blue', label='SimulaÃ§Ãµes')
> x = np.linspace(min(chi2_stats), max(chi2_stats), 100)
> plt.plot(x, chi2.pdf(x, 2), color='red', label='Qui-quadrado(2)')
> plt.title("DistribuiÃ§Ã£o da EstatÃ­stica Wald")
> plt.legend()
> plt.show()
>
> # Taxa de rejeiÃ§Ã£o para um nÃ­vel de significÃ¢ncia de 5%
> rejection_rate = np.mean(chi2_stats > chi2.ppf(0.95, 2))
> print(f"Taxa de rejeiÃ§Ã£o empÃ­rica: {rejection_rate:.4f}")
>
> print(f"MÃ©dia amostral da estatÃ­stica Wald: {np.mean(chi2_stats):.4f}")
> print(f"VariÃ¢ncia amostral da estatÃ­stica Wald: {np.var(chi2_stats):.4f}")
> ```
> O histograma da estatÃ­stica $\chi^2$ simulada se aproxima de uma distribuiÃ§Ã£o $\chi^2$ com 2 graus de liberdade. A taxa de rejeiÃ§Ã£o empÃ­rica Ã© prÃ³xima do nÃ­vel de significÃ¢ncia de 5%.

A estatÃ­stica $\chi^2$ se aproxima de uma distribuiÃ§Ã£o $\chi^2$ com $m$ graus de liberdade, onde $m$ Ã© o nÃºmero de restriÃ§Ãµes, conforme demonstrado no capÃ­tulo 8 [^9].

**ProposiÃ§Ã£o 1:** A estatÃ­stica Wald [16.3.19], sob a hipÃ³tese nula $R\beta = r$ e com as condiÃ§Ãµes usuais de regularidade, converge em distribuiÃ§Ã£o para uma variÃ¡vel aleatÃ³ria qui-quadrado com $m$ graus de liberdade, onde $m$ Ã© o nÃºmero de restriÃ§Ãµes impostas por $R$.

*Prova:*
I. Sob a hipÃ³tese nula $R\beta = r$, temos que $R\hat{\beta} - r = R(\hat{\beta} - \beta)$.
II. Sabemos que $\sqrt{T}(\hat{\beta} - \beta)$ converge em distribuiÃ§Ã£o para uma normal multivariada com mÃ©dia zero e matriz de covariÃ¢ncia dada por $\sigma^2 Q^{-1}$, onde $Q = \lim_{T \to \infty} \frac{1}{T}X'X$ (assumindo que o limite existe). Ou seja, $\sqrt{T}(\hat{\beta} - \beta) \overset{d}{\rightarrow} N(0, \sigma^2 Q^{-1})$.
III. Portanto, $\sqrt{T} R (\hat{\beta} - \beta) \overset{d}{\rightarrow} N(0, \sigma^2 RQ^{-1}R')$.
IV. A estatÃ­stica de Wald pode ser reescrita como:
    $$\chi^2 = T(R\hat{\beta} - r)' [s^2 T R(X'X)^{-1}R']^{-1} (R\hat{\beta} - r)$$
V. Como $s^2 \overset{p}{\rightarrow} \sigma^2$ e $ \frac{1}{T}X'X \rightarrow Q $, podemos substituir esses valores na estatÃ­stica de Wald:
   $$\chi^2 \approx T(R\hat{\beta} - r)' [\sigma^2 R Q^{-1}R']^{-1} (R\hat{\beta} - r)$$
VI.  Definindo $Z = \sqrt{T} R (\hat{\beta} - \beta)$, sabemos que $Z \overset{d}{\rightarrow} N(0, \sigma^2 RQ^{-1}R')$, entÃ£o podemos reescrever a estatÃ­stica de Wald como:
    $$\chi^2 \approx  Z'(\sigma^2 RQ^{-1}R')^{-1} Z$$
VII. Pela propriedade de formas quadrÃ¡ticas de variÃ¡veis normais, temos que $Z'(\sigma^2 RQ^{-1}R')^{-1} Z$ converge em distribuiÃ§Ã£o para uma variÃ¡vel qui-quadrado com $m$ graus de liberdade, onde $m$ Ã© o nÃºmero de restriÃ§Ãµes impostas pela matriz $R$.
VIII. Portanto, a estatÃ­stica de Wald converge em distribuiÃ§Ã£o para uma variÃ¡vel qui-quadrado com $m$ graus de liberdade:
$$\chi^2 \overset{d}{\rightarrow} \chi^2(m)$$
â– 

**Lema 1:** Se $\hat{\beta}$ Ã© o estimador OLS do vetor de parÃ¢metros $\beta$ no modelo $y = X\beta + \epsilon$, e se as condiÃ§Ãµes de Gauss-Markov para o modelo se mantÃªm, entÃ£o o estimador da variÃ¢ncia dos erros $s^2$ converge em probabilidade para a variÃ¢ncia populacional $\sigma^2$, isto Ã©, $s^2 \overset{p}{\rightarrow} \sigma^2$.

*Prova:*
I. O estimador da variÃ¢ncia dos erros Ã© dado por:
    $$s^2 = \frac{1}{T-k} \sum_{t=1}^T e_t^2$$
    onde $e_t = y_t - x_t'\hat{\beta}$ sÃ£o os resÃ­duos OLS e $k$ Ã© o nÃºmero de parÃ¢metros no modelo.
II. Sabemos que a soma dos quadrados dos resÃ­duos pode ser escrita como:
    $$\sum_{t=1}^T e_t^2 = \epsilon'M\epsilon$$
    onde $M = I - X(X'X)^{-1}X'$ Ã© a matriz de projeÃ§Ã£o que aniquila o espaÃ§o coluna de $X$, e $\epsilon$ Ã© o vetor de erros.
III. Assim, podemos reescrever o estimador de variÃ¢ncia como:
    $$s^2 = \frac{\epsilon'M\epsilon}{T-k}$$
IV.  O valor esperado de $s^2$ Ã©:
    $$E[s^2] = E\left[\frac{\epsilon'M\epsilon}{T-k}\right] = \frac{1}{T-k}E[\text{tr}(\epsilon'M\epsilon)] = \frac{1}{T-k}E[\text{tr}(M\epsilon\epsilon')] = \frac{1}{T-k}\text{tr}(ME[\epsilon\epsilon'])$$
V. Sob as condiÃ§Ãµes de Gauss-Markov, $E[\epsilon\epsilon'] = \sigma^2 I$, e como $M$ Ã© idempotente ($M = M^2$), temos:
    $$E[s^2] = \frac{1}{T-k}\text{tr}(M\sigma^2 I) = \frac{\sigma^2}{T-k}\text{tr}(M) = \frac{\sigma^2}{T-k} \text{tr}(I - X(X'X)^{-1}X') = \frac{\sigma^2}{T-k}(T - k) = \sigma^2$$
VI. Para mostrar a convergÃªncia em probabilidade, precisamos mostrar que a variÃ¢ncia de $s^2$ converge para zero conforme $T$ tende ao infinito. Sabemos que:
    $$ Var(s^2) = Var \left( \frac{\epsilon'M\epsilon}{T-k} \right) = \frac{1}{(T-k)^2} Var(\epsilon'M\epsilon)$$
VII. Sob a condiÃ§Ã£o de que $\epsilon_t$ tem momentos de quarta ordem finitos, pode-se demonstrar que:
    $$Var(\epsilon'M\epsilon) = 2\sigma^4 tr(M^2) = 2 \sigma^4 tr(M) = 2\sigma^4(T-k)$$
VIII. Portanto:
    $$Var(s^2) = \frac{2 \sigma^4 (T-k)}{(T-k)^2} = \frac{2\sigma^4}{T-k}$$
IX. Como $T-k$ tende para infinito conforme $T$ tende para infinito, a variÃ¢ncia de $s^2$ converge para zero:
    $$\lim_{T \to \infty} Var(s^2) = 0$$
X. Pela desigualdade de Chebyshev, para qualquer $\varepsilon > 0$:
    $$P(|s^2 - E[s^2]| \ge \varepsilon) \le \frac{Var(s^2)}{\varepsilon^2}$$
XI. Como $\lim_{T \to \infty} Var(s^2) = 0$, temos que:
    $$\lim_{T \to \infty} P(|s^2 - \sigma^2| \ge \varepsilon) = 0$$
XII. Isso demonstra que $s^2$ converge em probabilidade para $\sigma^2$:
$$s^2 \overset{p}{\rightarrow} \sigma^2$$
â– 

**CorolÃ¡rio 1:**  Se considerarmos o modelo com tendÃªncia temporal simples $y_t = \alpha + \delta t + \epsilon_t$, e se os erros $\epsilon_t$ forem i.i.d. com mÃ©dia zero e variÃ¢ncia $\sigma^2$, e se a condiÃ§Ã£o de Gauss-Markov se mantiver, a estatÃ­stica $s^2$ definida em [16.2.2] converge em probabilidade para $\sigma^2$.

*Prova:*
I. O modelo com tendÃªncia temporal simples $y_t = \alpha + \delta t + \epsilon_t$ se encaixa na forma geral do modelo de regressÃ£o linear $y = X\beta + \epsilon$.
II. As condiÃ§Ãµes de Gauss-Markov sÃ£o satisfeitas, pois os erros $\epsilon_t$ sÃ£o i.i.d. com mÃ©dia zero e variÃ¢ncia $\sigma^2$.
III. Pelo Lema 1, sabemos que o estimador da variÃ¢ncia dos erros $s^2$ converge em probabilidade para a variÃ¢ncia populacional $\sigma^2$ sob as condiÃ§Ãµes de Gauss-Markov.
IV. Portanto, a estatÃ­stica $s^2$ definida em [16.2.2] converge em probabilidade para $\sigma^2$:
$$s^2 \overset{p}{\rightarrow} \sigma^2$$
â– 

Em resumo, os testes de hipÃ³teses sobre os parÃ¢metros em modelos com tendÃªncias determinÃ­sticas podem ser realizados utilizando a abordagem usual com as estatÃ­sticas t e F, jÃ¡ que as mesmas mantÃ©m validade assintÃ³tica, ainda que com diferentes taxas de convergÃªncia de seus estimadores. Essa equivalÃªncia se dÃ¡ pelo comportamento assintÃ³tico dos erros padrÃ£o dos estimadores que acomodam a diferenÃ§a de convergÃªncia dos parÃ¢metros.

### ConclusÃ£o
Nesta seÃ§Ã£o, mostramos que os testes t e F usuais sÃ£o assintoticamente vÃ¡lidos para modelos de regressÃ£o com tendÃªncias temporais determinÃ­sticas, mesmo quando as inovaÃ§Ãµes nÃ£o sÃ£o gaussianas [^8]. A transformaÃ§Ã£o de Sims, Stock e Watson, discutida na seÃ§Ã£o anterior, embora fundamental para derivar as distribuiÃ§Ãµes assintÃ³ticas dos estimadores, nÃ£o Ã© necessÃ¡ria para a aplicaÃ§Ã£o dos testes de hipÃ³teses. Os erros padrÃ£o estimados compensam as diferentes taxas de convergÃªncia dos estimadores de forma que os testes tÃªm um comportamento padronizado. Os resultados apresentados aqui fornecem uma base sÃ³lida para a inferÃªncia estatÃ­stica em modelos com tendÃªncias determinÃ­sticas, e se estendem para processos autorregressivos em torno da tendÃªncia [^11]. Nos capÃ­tulos seguintes, exploraremos como essas tÃ©cnicas se estendem a modelos com raÃ­zes unitÃ¡rias e outros tipos de nÃ£o estacionariedade [^2].

### ReferÃªncias
[^1]: Trecho do texto original que introduz o tema de distribuiÃ§Ãµes assintÃ³ticas em modelos com tendÃªncias determinÃ­sticas e a abordagem geral de Sims, Stock e Watson (1990).
[^2]: Trecho do texto original que menciona a utilizaÃ§Ã£o de tÃ©cnicas diferentes daquelas usadas em modelos estacionÃ¡rios e a relevÃ¢ncia do capÃ­tulo para estudos de processos nÃ£o estacionÃ¡rios (CapÃ­tulos 17 e 18).
[^3]: Trecho do texto original que introduz a ideia de que as estatÃ­sticas usuais de OLS tem as mesmas distribuiÃ§Ãµes assintÃ³ticas de regressÃµes estacionÃ¡rias, mesmo com diferentes taxas de convergÃªncia.
[^4]: Trecho do texto original que demonstra a divergÃªncia da matriz $(1/T) \sum x_t x_t'$ e a necessidade de dividir por $T^3$ em vez de $T$.
[^5]: Trecho do texto original que introduz a matriz $Y_T$ para o reescalonamento das variÃ¡veis e deriva a distribuiÃ§Ã£o assintÃ³tica do segundo termo apÃ³s o reescalonamento.
[^7]: Trecho do texto original que afirma a superconsistÃªncia do estimador do coeficiente da tendÃªncia temporal.
[^8]: Trecho do texto original que explica que as estatÃ­sticas t e F usuais sÃ£o vÃ¡lidas assintoticamente quando os erros sÃ£o gaussianos e sugere que isso Ã© verdade em casos nÃ£o gaussianos.
[^9]: Trecho do texto original que explica o uso do teste F como o quadrado do teste t para restriÃ§Ãµes lineares, e a convergÃªncia para uma distribuiÃ§Ã£o $\chi^2$.
[^10]: Trecho do texto original que explica que testes com restriÃ§Ãµes em parÃ¢metros com diferentes taxas de convergÃªncia sÃ£o dominadas assintoticamente pelas taxas mais lentas.
[^11]: Trecho do texto original que descreve o modelo autorregressivo com tendÃªncia temporal e a transformaÃ§Ã£o de Sims, Stock, e Watson.
<!-- END -->
