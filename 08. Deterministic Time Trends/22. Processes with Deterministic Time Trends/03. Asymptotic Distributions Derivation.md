## Abordagem Geral para Deriva√ß√£o de Distribui√ß√µes Assint√≥ticas em Processos com Tend√™ncias Determin√≠sticas

### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise de modelos de regress√£o com tend√™ncias temporais determin√≠sticas, expandindo os conceitos introduzidos na se√ß√£o anterior [^1]. Enquanto a se√ß√£o precedente focou na distribui√ß√£o assint√≥tica dos estimadores OLS em um modelo de tend√™ncia linear simples, esta se√ß√£o introduz uma abordagem mais geral, proposta por Sims, Stock, e Watson (1990), que √© aplic√°vel a uma gama maior de modelos, incluindo aqueles com componentes autorregressivos em torno da tend√™ncia [^1]. O principal desafio, como j√° discutido, √© a exist√™ncia de diferentes taxas de converg√™ncia para os estimadores dos coeficientes, o que impede a aplica√ß√£o direta das t√©cnicas assint√≥ticas utilizadas em modelos com vari√°veis estacion√°rias [^1]. Abordaremos como a transforma√ß√£o do modelo de regress√£o para uma forma can√¥nica simplifica a deriva√ß√£o das distribui√ß√µes assint√≥ticas, permitindo uma an√°lise mais clara e concisa [^1]. Esta se√ß√£o expande os conceitos introduzidos anteriormente, abordando processos que envolvem componentes autorregressivos em torno da tend√™ncia.

### Conceitos Fundamentais
Como vimos anteriormente, a aplica√ß√£o direta de t√©cnicas assint√≥ticas para modelos com vari√°veis estacion√°rias em regress√µes com tend√™ncias temporais determin√≠sticas √© inadequada devido √†s diferentes taxas de converg√™ncia dos estimadores [^1]. A abordagem geral proposta por Sims, Stock, e Watson (1990) visa contornar esta dificuldade atrav√©s da transforma√ß√£o do modelo de regress√£o em uma forma can√¥nica. Esta transforma√ß√£o permite isolar os componentes do modelo que exibem diferentes taxas de converg√™ncia, tornando a an√°lise mais trat√°vel [^1]. Esta abordagem √© crucial em contextos n√£o estacion√°rios, onde as abordagens convencionais de s√©ries temporais n√£o se aplicam diretamente [^2].

O cap√≠tulo come√ßa com o exemplo mais simples de inova√ß√µes i.i.d. em torno de uma tend√™ncia determin√≠stica, explorando a distribui√ß√£o assint√≥tica dos estimadores dos coeficientes. A an√°lise revela que os estimadores de diferentes par√¢metros convergem para seus verdadeiros valores a taxas distintas, necessitando o reescalonamento das vari√°veis para obter distribui√ß√µes assint√≥ticas n√£o degeneradas. Essa necessidade de reescalonamento implica que diferentes estimadores de par√¢metros devem ser multiplicados por diferentes pot√™ncias de $T$ (o tamanho da amostra) para que suas distribui√ß√µes assint√≥ticas sejam bem comportadas [^4].

A transforma√ß√£o de um modelo de regress√£o em uma forma can√¥nica, como proposto por Sims, Stock e Watson, √© fundamental para simplificar a deriva√ß√£o de distribui√ß√µes assint√≥ticas em modelos com tend√™ncias temporais determin√≠sticas [^1]. A ideia central √© reescrever o modelo original, de forma que os regressores sejam compostos por vari√°veis com m√©dia zero e estacion√°rias, um termo constante e uma tend√™ncia temporal. Esta decomposi√ß√£o permite isolar os componentes com diferentes taxas de converg√™ncia, facilitando a aplica√ß√£o de ferramentas assint√≥ticas.

> üí° **Exemplo Num√©rico:** Vamos considerar o modelo apresentado na se√ß√£o anterior $y_t = \alpha + \delta t + \epsilon_t$ [^2], onde $\epsilon_t \sim N(0, \sigma^2)$. A partir desse modelo, podemos derivar o vetor $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$ e o vetor de par√¢metros $\beta = \begin{bmatrix} \alpha \\ \delta \end{bmatrix}$. O modelo de regress√£o √© dado por $y_t = x_t'\beta + \epsilon_t$ [^2]. Como discutido anteriormente, os estimadores OLS $\hat{\alpha}$ e $\hat{\delta}$ t√™m diferentes taxas de converg√™ncia: $\hat{\alpha}$ converge a uma taxa de $\sqrt{T}$ e $\hat{\delta}$ a uma taxa de $T^{3/2}$ [^5].
>
> Para ilustrar, vamos gerar dados sint√©ticos com $\alpha = 2$, $\delta = 0.5$, e $\sigma^2 = 1$, para $T = 100$ e estimar os par√¢metros via OLS:
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
>
> # Par√¢metros
> alpha = 2
> delta = 0.5
> sigma = 1
> T = 100
>
> # Gerando dados
> np.random.seed(42)  # Para reprodutibilidade
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, sigma, T)
> y = alpha + delta * t + epsilon
>
> # Criando a matriz de regressores X
> X = np.column_stack((np.ones(T), t))
>
> # Ajustando o modelo OLS
> model = sm.OLS(y, X)
> results = model.fit()
>
> # Estimativas dos par√¢metros
> alpha_hat, delta_hat = results.params
>
> # Imprimindo os resultados
> print(f"Estimativa de alpha: {alpha_hat:.4f}")
> print(f"Estimativa de delta: {delta_hat:.4f}")
>
> # Visualiza√ß√£o dos dados e da reta ajustada
> import matplotlib.pyplot as plt
>
> plt.figure(figsize=(8, 6))
> plt.scatter(t, y, label='Dados simulados')
> plt.plot(t, alpha_hat + delta_hat * t, color='red', label='Regress√£o OLS')
> plt.xlabel('Tempo (t)')
> plt.ylabel('y_t')
> plt.title('Regress√£o com Tend√™ncia Determin√≠stica')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
> A sa√≠da do c√≥digo acima mostra as estimativas de $\alpha$ e $\delta$ e um gr√°fico dos dados simulados com a reta de regress√£o. Observamos que, mesmo com dados simulados, as estimativas s√£o pr√≥ximas dos valores reais ($2$ e $0.5$). Este exemplo ilustra a aplica√ß√£o do modelo com tend√™ncia determin√≠stica e a obten√ß√£o dos estimadores via OLS, que convergem para os valores verdadeiros a diferentes taxas, conforme discutido.

**Lema 1** Para o modelo de regress√£o $y_t = \alpha + \delta t + \epsilon_t$, onde $\epsilon_t \sim iid(0, \sigma^2)$, os estimadores OLS $\hat{\alpha}$ e $\hat{\delta}$ s√£o dados por:
$$ \hat{\delta} = \frac{\sum_{t=1}^{T} (t - \bar{t})(y_t - \bar{y})}{\sum_{t=1}^{T} (t - \bar{t})^2} \quad \text{e} \quad \hat{\alpha} = \bar{y} - \hat{\delta}\bar{t} $$
onde $\bar{y} = \frac{1}{T} \sum_{t=1}^{T} y_t$ e $\bar{t} = \frac{1}{T} \sum_{t=1}^{T} t$.

*Prova*: Esta √© uma aplica√ß√£o padr√£o da f√≥rmula OLS para regress√£o com dois regressores, um termo constante e uma tend√™ncia linear. O lema pode ser comprovado minimizando a soma dos quadrados dos res√≠duos $\sum_{t=1}^{T} (y_t - \alpha - \delta t)^2$ em rela√ß√£o a $\alpha$ e $\delta$.
I. Come√ßamos com a fun√ß√£o de soma dos quadrados dos res√≠duos:
   $$S(\alpha, \delta) = \sum_{t=1}^{T} (y_t - \alpha - \delta t)^2$$
II. Para encontrar os valores de $\alpha$ e $\delta$ que minimizam $S$, tomamos as derivadas parciais em rela√ß√£o a $\alpha$ e $\delta$ e igualamos a zero:
   $$\frac{\partial S}{\partial \alpha} = -2 \sum_{t=1}^{T} (y_t - \alpha - \delta t) = 0$$
   $$\frac{\partial S}{\partial \delta} = -2 \sum_{t=1}^{T} t(y_t - \alpha - \delta t) = 0$$
III. Resolvendo a primeira equa√ß√£o para $\alpha$:
   $$\sum_{t=1}^{T} y_t - T\alpha - \delta \sum_{t=1}^{T} t = 0$$
   $$\alpha = \frac{1}{T} \sum_{t=1}^{T} y_t - \delta \frac{1}{T} \sum_{t=1}^{T} t = \bar{y} - \delta\bar{t}$$
IV. Substituindo $\alpha$ na segunda equa√ß√£o:
    $$\sum_{t=1}^{T} t(y_t - (\bar{y} - \delta\bar{t}) - \delta t) = 0$$
    $$\sum_{t=1}^{T} t(y_t - \bar{y}) - \delta \sum_{t=1}^{T} t(t - \bar{t}) = 0$$
    $$\sum_{t=1}^{T} t(y_t - \bar{y}) = \delta \sum_{t=1}^{T} t(t - \bar{t})$$
    $$\delta = \frac{\sum_{t=1}^{T} t(y_t - \bar{y})}{\sum_{t=1}^{T} t(t - \bar{t})}$$
V. Usando o fato de que $\sum_{t=1}^{T} t(y_t - \bar{y}) = \sum_{t=1}^{T} (t - \bar{t})(y_t - \bar{y})$ e $\sum_{t=1}^{T} t(t - \bar{t}) = \sum_{t=1}^{T} (t - \bar{t})^2$, temos:
   $$\delta = \frac{\sum_{t=1}^{T} (t - \bar{t})(y_t - \bar{y})}{\sum_{t=1}^{T} (t - \bar{t})^2}$$
VI. Conclu√≠mos que:
   $$\hat{\delta} = \frac{\sum_{t=1}^{T} (t - \bar{t})(y_t - \bar{y})}{\sum_{t=1}^{T} (t - \bar{t})^2} \quad \text{e} \quad \hat{\alpha} = \bar{y} - \hat{\delta}\bar{t}$$
‚ñ†

Para analisar um processo autorregressivo em torno de uma tend√™ncia determin√≠stica, considera-se um modelo geral do tipo:
$$ y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t \quad [16.3.1] $$
Onde $\epsilon_t$ √© um processo i.i.d. com m√©dia zero, vari√¢ncia $\sigma^2$ e momento quarto finito, e as ra√≠zes da equa√ß√£o $1 - \phi_1 z - \phi_2 z^2 - \ldots - \phi_p z^p = 0$ est√£o fora do c√≠rculo unit√°rio [^11].
Para analisar este modelo, √© necess√°rio transform√°-lo utilizando a metodologia de Sims, Stock, e Watson, que envolve adicionar e subtrair termos de forma a reescrever os regressores em termos de vari√°veis estacion√°rias, um termo constante e uma tend√™ncia temporal [^11]. Ao fazer isso, o modelo em [16.3.1] √© reescrito como:

$$ y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \ldots + \phi_p^* y_{t-p}^* + \epsilon_t \quad [16.3.3] $$
onde:
$$ \alpha^* = [\alpha(1 + \phi_1 + \phi_2 + \ldots + \phi_p) - \delta(\phi_1 + 2\phi_2 + \ldots + p\phi_p)] $$
$$ \delta^* = \delta(1 + \phi_1 + \phi_2 + \ldots + \phi_p) $$
$$ \phi_j^* = \phi_j $$
e
$$ y_{t-j}^* = y_{t-j} - \alpha - \delta (t-j) \quad \text{para } j=1, 2, \ldots, p \quad [16.3.4] $$

A ideia por tr√°s desta transforma√ß√£o √© decompor os regressores em partes com diferentes propriedades. Os termos $y_{t-j}^*$ s√£o zero-m√©dia e estacion√°rios [^11]. O termo $\alpha^*$ √© uma constante e o termo $\delta^* t$ representa a tend√™ncia temporal. Transformando os regressores dessa forma, isolamos os componentes do vetor de coeficientes que possuem diferentes taxas de converg√™ncia [^11]. Essa transforma√ß√£o √© crucial para obter as distribui√ß√µes assint√≥ticas.

De forma mais geral, o modelo original [16.3.1] pode ser escrito como
$$ y_t = x_t'\beta + \epsilon_t \quad [16.3.5] $$
onde
$$ x_t = \begin{bmatrix} y_{t-1} \\ y_{t-2} \\ \vdots \\ y_{t-p} \\ 1 \\ t \end{bmatrix} \quad \text{e} \quad \beta = \begin{bmatrix} \phi_1 \\ \phi_2 \\ \vdots \\ \phi_p \\ \alpha \\ \delta \end{bmatrix} \quad [16.3.6] $$

A transforma√ß√£o alg√©brica que leva √† forma [16.3.3] pode ser descrita como uma reescrita de [16.3.5] na forma
$$ y_t = x_t'G' (G')^{-1} \beta + \epsilon_t = [x_t^*]' \beta^* + \epsilon_t  \quad [16.3.7] $$
onde
$$  G' = \begin{bmatrix} 1 & 0 & 0 & \cdots & 0 & 0 & 0 \\ 0 & 1 & 0 & \cdots & 0 & 0 & 0 \\  \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & 1 & 0 & 0 \\ -\alpha + \delta & -\alpha + 2\delta & \cdots & -\alpha + p\delta & 1 & 0 \\ -\delta & -\delta & \cdots & -\delta & 0 & 1  \end{bmatrix} \quad [16.3.8] $$
$$  (G')^{-1} =  \begin{bmatrix} 1 & 0 & 0 & \cdots & 0 & 0 & 0 \\ 0 & 1 & 0 & \cdots & 0 & 0 & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & 1 & 0 & 0 \\  \alpha - \delta & \alpha - 2\delta & \cdots & \alpha - p\delta & 1 & 0 \\ \delta & \delta & \cdots & \delta & 0 & 1  \end{bmatrix} \quad [16.3.8] $$
e
$$ x_t^* = G x_t =  \begin{bmatrix} y_{t-1}^* \\ y_{t-2}^* \\ \vdots \\ y_{t-p}^* \\ 1 \\ t \end{bmatrix} \quad [16.3.9] $$
$$ \beta^* = (G')^{-1} \beta = \begin{bmatrix} \phi_1^* \\ \phi_2^* \\ \vdots \\ \phi_p^* \\ \alpha^* \\ \delta^* \end{bmatrix}  \quad [16.3.10] $$

O sistema em [16.3.7] √© apenas uma representa√ß√£o algebricamente equivalente do modelo de regress√£o [16.3.5]. √â importante notar que o estimador de $\beta^*$ baseado em uma regress√£o OLS de $y_t$ sobre $x_t^*$ √© dado por:

$$ b^* = \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \left( \sum_{t=1}^T x_t^* y_t \right) = (G')^{-1} b \quad [16.3.11] $$
onde $b$ √© o estimador OLS de $\beta$ em [16.3.5].

Essa transforma√ß√£o revela que a an√°lise da distribui√ß√£o assint√≥tica de $b^*$ √© mais simples do que a de $b$. Uma vez que a distribui√ß√£o assint√≥tica de $b^*$ √© encontrada, a distribui√ß√£o assint√≥tica de $b$ pode ser obtida invertendo a transforma√ß√£o em [16.3.12]:
$$ b = G'b^* \quad [16.3.12] $$

O ap√™ndice 16.A do cap√≠tulo demonstra que
$$ Y_T(b^* - \beta^*) \overset{d}{\rightarrow} N(0, \sigma^2[Q^*]^{-1}) \quad [16.3.13] $$
onde
$$ Y_T = \begin{bmatrix}
\sqrt{T} & 0 & \cdots & 0 & 0 & 0 \\
0 & \sqrt{T} & \cdots & 0 & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & \cdots & \sqrt{T} & 0 & 0 \\
0 & 0 & \cdots & 0 & \sqrt{T} & 0 \\
0 & 0 & \cdots & 0 & 0 & T^{3/2}
\end{bmatrix} \quad [16.3.14] $$
e $Q^*$ √© uma matriz que envolve as covari√¢ncias dos regressores transformados [^13].

> üí° **Exemplo Num√©rico:**  Vamos considerar o modelo AR(1) com tend√™ncia determin√≠stica dado por $y_t = \alpha + \delta t + \phi y_{t-1} + \epsilon_t$ para ilustrar a transforma√ß√£o. O modelo original pode ser escrito na forma $y_t = x_t' \beta + \epsilon_t$, onde $x_t = \begin{bmatrix} y_{t-1} \\ 1 \\ t \end{bmatrix}$ e $\beta = \begin{bmatrix} \phi \\ \alpha \\ \delta \end{bmatrix}$.
>
> Usando a transforma√ß√£o de Sims, Stock e Watson, podemos reescrever o modelo como $y_t = \alpha^* + \delta^* t + \phi^* y_{t-1}^* + \epsilon_t$, onde $y_{t-1}^* = y_{t-1} - \alpha - \delta(t-1)$.
>
> Nesse caso, ter√≠amos $x_t^* = \begin{bmatrix} y_{t-1}^* \\ 1 \\ t \end{bmatrix}$ e $\beta^* = \begin{bmatrix} \phi^* \\ \alpha^* \\ \delta^* \end{bmatrix}$, onde:
>
> $$ \alpha^* = \alpha(1 + \phi) - \delta \phi $$
> $$ \delta^* = \delta(1 + \phi) $$
> $$ \phi^* = \phi $$
>
> Para demonstrar, vamos simular um conjunto de dados, realizar a transforma√ß√£o e estimar os coeficientes originais e transformados.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LinearRegression
>
> # Simulando dados
> np.random.seed(42)
> T = 200
> alpha = 5
> delta = 0.2
> phi = 0.7
> sigma = 1.5
>
> epsilon = np.random.normal(0, sigma, T)
> y = np.zeros(T)
>
> # Simula√ß√£o usando modelo com tend√™ncia determin√≠stica e AR(1)
> for t in range(1, T):
>  y[t] = alpha + delta * t + phi * y[t-1] + epsilon[t]
>
> t_values = np.arange(1, T+1)
> # Criando matriz x_t para o modelo original
> X = np.column_stack((y[:-1], np.ones(T-1), t_values[:-1]))
>
> # Estimando o modelo original
> model_original = LinearRegression()
> model_original.fit(X, y[1:])
> phi_hat_original, alpha_hat_original, delta_hat_original = model_original.coef_[0], model_original.intercept_, model_original.coef_[2]
>
>
> # Transforma√ß√£o Sims, Stock e Watson
> y_star = np.zeros(T)
> for t in range(1, T):
>   y_star[t] = y[t-1] - alpha - delta * (t -1)
>
> # Cria√ß√£o da matriz x_t* para o modelo transformado
> X_star = np.column_stack((y_star[:-1], np.ones(T-1), t_values[:-1]))
>
> # Estimando o modelo transformado
> model_transformed = LinearRegression()
> model_transformed.fit(X_star, y[1:])
> phi_star_hat, alpha_star_hat, delta_star_hat = model_transformed.coef_[0], model_transformed.intercept_, model_transformed.coef_[2]
>
> print("Estimativas Originais:")
> print(f"phi_hat_original: {phi_hat_original:.4f}")
> print(f"alpha_hat_original: {alpha_hat_original:.4f}")
> print(f"delta_hat_original: {delta_hat_original:.4f}")
>
> print("\nEstimativas Transformadas:")
> print(f"phi_star_hat: {phi_star_hat:.4f}")
> print(f"alpha_star_hat: {alpha_star_hat:.4f}")
> print(f"delta_star_hat: {delta_star_hat:.4f}")
>
> # Calculando os par√¢metros originais a partir dos transformados
> alpha_hat_reconstructed = (alpha_star_hat + delta_star_hat * phi_star_hat) / (1 + phi_star_hat)
> delta_hat_reconstructed = delta_star_hat / (1+ phi_star_hat)
> print("\nPar√¢metros originais reconstru√≠dos a partir do modelo transformado:")
> print(f"alpha_hat_reconstructed: {alpha_hat_reconstructed:.4f}")
> print(f"delta_hat_reconstructed: {delta_hat_reconstructed:.4f}")
>
> plt.plot(t_values[1:], y[1:])
> plt.title('Simula√ß√£o com AR(1) e Tend√™ncia')
> plt.xlabel('Tempo')
> plt.ylabel('y_t')
> plt.show()
> ```

> Este exemplo demonstra como a transforma√ß√£o de Sims, Stock, e Watson permite reescrever o modelo AR(1) com tend√™ncia determin√≠stica,  mostrando a estimativa dos par√¢metros originais e transformados, al√©m da reconstru√ß√£o dos par√¢metros originais a partir dos transformados, confirmando que as duas abordagens s√£o equivalentes em termos de ajuste aos dados simulados.

Essa abordagem, ao transformar os regressores, permite isolar as componentes com diferentes taxas de converg√™ncia, resultando em um m√©todo geral para derivar distribui√ß√µes assint√≥ticas em regress√µes com vari√°veis n√£o estacion√°rias, como as que envolvem tend√™ncias temporais determin√≠sticas.  Em [16.3.13], o coeficiente da tend√™ncia temporal ($\delta^*$) converge a uma taxa de $T^{3/2}$, enquanto todos os outros coeficientes convergem a uma taxa de $\sqrt{T}$ [^13]. Este resultado generaliza a Proposi√ß√£o 16.1, onde $p = 0$ [^13].

**Teorema 1.1** (Converg√™ncia de Estimadores Transformados)
Dado o modelo transformado em [16.3.3], e sob as condi√ß√µes descritas para o erro $\epsilon_t$ e as ra√≠zes do polin√¥mio autorregressivo, o estimador OLS $b^*$ converge em probabilidade para $\beta^*$. Mais especificamente:

$$ b^* \overset{p}{\rightarrow} \beta^* $$

*Prova:*
Este resultado segue da aplica√ß√£o da Lei dos Grandes N√∫meros (LGN) e do Teorema de Slutsky. A matriz $\frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'}$ converge para uma matriz positiva definida, e o vetor $\frac{1}{T} \sum_{t=1}^T x_t^* y_t$ converge para um vetor, ambos com a ressalva do termo da tend√™ncia temporal que, ap√≥s reescalonamento converge em taxa diferente. Uma vez que a matriz de covari√¢ncias dos regressores transformados tem um limite n√£o singular e os erros s√£o i.i.d., podemos concluir que $b^* =  \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \left( \sum_{t=1}^T x_t^* y_t \right)$ converge em probabilidade para $\beta^*$. A parte que exige o reescalonamento est√° devidamente tratada pela matriz $Y_T$ em [16.3.14].
I. O estimador OLS $b^*$ √© definido como:
$$b^* = \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \left( \sum_{t=1}^T x_t^* y_t \right)$$
II. Podemos reescrever $b^*$ como:
   $$b^* = \left( \frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \left( \frac{1}{T} \sum_{t=1}^T x_t^* y_t \right)$$
III. Substitu√≠mos $y_t$ pelo modelo transformado em [16.3.3]:
    $$b^* = \left( \frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \left( \frac{1}{T} \sum_{t=1}^T x_t^* (x_t^{*'}\beta^* + \epsilon_t) \right)$$
IV. Expandindo a equa√ß√£o:
    $$b^* = \left( \frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \left( \frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'} \beta^* + \frac{1}{T} \sum_{t=1}^T x_t^* \epsilon_t \right)$$
V. Simplificando:
    $$b^* = \beta^* + \left( \frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \left( \frac{1}{T} \sum_{t=1}^T x_t^* \epsilon_t \right)$$
VI. Pela lei dos grandes n√∫meros, $\frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'}$ converge em probabilidade para uma matriz $Q^*$ positiva definida, onde $Q^* = E[x_t^* x_t^{*'}]$. Al√©m disso, $\frac{1}{T} \sum_{t=1}^T x_t^* \epsilon_t$ converge em probabilidade para zero, j√° que $E[x_t^* \epsilon_t]=0$.
VII. Portanto, pelo Teorema de Slutsky, temos:
  $$b^* \overset{p}{\rightarrow} \beta^* + (Q^*)^{-1} \cdot 0 = \beta^*$$
‚ñ†

**Corol√°rio 1.1** (Converg√™ncia de Estimadores Originais)
Dado o resultado do Teorema 1.1 e a rela√ß√£o $b = G'b^*$ em [16.3.12], o estimador OLS $b$ do modelo original em [16.3.5] converge em probabilidade para $\beta$:

$$ b \overset{p}{\rightarrow} \beta $$
*Prova:* A prova decorre diretamente da converg√™ncia em probabilidade de $b^*$ para $\beta^*$ (Teorema 1.1) e do fato de $b=G'b^*$ onde G' √© uma matriz de constantes. Se $b^* \overset{p}{\rightarrow} \beta^*$, ent√£o $G'b^* \overset{p}{\rightarrow} G'\beta^* = \beta$.
I. Sabemos que $b = G'b^*$.
II. Do Teorema 1.1, temos que $b^* \overset{p}{\rightarrow} \beta^*$.
III. Como $G'$ √© uma matriz de constantes, a multiplica√ß√£o por $G'$ preserva a converg√™ncia em probabilidade. Assim:
$$G'b^* \overset{p}{\rightarrow} G'\beta^*$$
IV. Pela defini√ß√£o da transforma√ß√£o, $G'\beta^* = \beta$.
V. Portanto,
$$b \overset{p}{\rightarrow} \beta$$
‚ñ†

### Conclus√£o
Nesta se√ß√£o, exploramos a abordagem de Sims, Stock e Watson para a deriva√ß√£o de distribui√ß√µes assint√≥ticas em processos com tend√™ncias temporais determin√≠sticas [^1]. Essa metodologia, atrav√©s da transforma√ß√£o do modelo de regress√£o em uma forma can√¥nica, simplifica a an√°lise ao isolar os componentes com diferentes taxas de converg√™ncia.  Discutimos a import√¢ncia do reescalonamento das vari√°veis para obter distribui√ß√µes assint√≥ticas n√£o degeneradas. Este m√©todo √© generaliz√°vel para outros modelos de s√©ries temporais n√£o estacion√°rias, incluindo aqueles com ra√≠zes unit√°rias, conforme ser√° abordado em cap√≠tulos subsequentes [^2].  O foco aqui foi em processos com tend√™ncias determin√≠sticas, mas sem ra√≠zes unit√°rias, o que implica que a transforma√ß√£o, embora crucial para an√°lise, n√£o altera as propriedades de converg√™ncia dos estimadores no sentido de superconsist√™ncia do estimador da tend√™ncia.

### Refer√™ncias
[^1]: Trecho do texto original que introduz o tema de distribui√ß√µes assint√≥ticas em modelos com tend√™ncias determin√≠sticas e a abordagem geral de Sims, Stock e Watson (1990).
[^2]: Trecho do texto original que menciona a utiliza√ß√£o de t√©cnicas diferentes daquelas usadas em modelos estacion√°rios e a relev√¢ncia do cap√≠tulo para estudos de processos n√£o estacion√°rios (Cap√≠tulos 17 e 18).
[^3]: Trecho do texto original que demonstra por indu√ß√£o as f√≥rmulas para $\sum t$ e $\sum t^2$ e introduz a f√≥rmula geral para $\sum t^v$.
[^4]: Trecho do texto original que demonstra a diverg√™ncia da matriz $(1/T) \sum x_t x_t'$ e a necessidade de dividir por $T^3$ em vez de $T$.
[^5]: Trecho do texto original que introduz a matriz $Y_T$ para o reescalonamento das vari√°veis e deriva a distribui√ß√£o assint√≥tica do segundo termo ap√≥s o reescalonamento.
[^6]: Trecho do texto original que explica como a distribui√ß√£o conjunta de combina√ß√µes lineares dos elementos do vetor em [16.1.21] resulta em uma distribui√ß√£o gaussiana bivariada.
[^7]: Trecho do texto original que afirma a superconsist√™ncia do estimador do coeficiente da tend√™ncia temporal.
[^11]: Trecho do texto original que descreve o modelo autorregressivo com tend√™ncia temporal e a transforma√ß√£o de Sims, Stock, e Watson, incluindo a forma das matrizes $G'$ e $(G')^{-1}$.
[^13]: Trecho do texto original que explica a taxa de converg√™ncia do estimador de $\delta^*$ e como isso generaliza a Proposi√ß√£o 16.1.
<!-- END -->
