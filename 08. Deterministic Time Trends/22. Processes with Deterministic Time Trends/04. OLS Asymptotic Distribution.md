## Distribui√ß√µes Assint√≥ticas de Estimadores OLS para Modelos de Tend√™ncia Determin√≠stica

### Introdu√ß√£o
Este cap√≠tulo continua a explora√ß√£o das peculiaridades da an√°lise de s√©ries temporais com tend√™ncias temporais determin√≠sticas, baseando-se nos conceitos desenvolvidos anteriormente [^1, ^2]. J√° vimos que a aplica√ß√£o direta de abordagens convencionais de regress√£o a modelos com tend√™ncias temporais leva a estimadores com diferentes taxas de converg√™ncia [^1].  Nas se√ß√µes precedentes, estabelecemos a necessidade de reescalonar as vari√°veis para obter distribui√ß√µes assint√≥ticas bem definidas, e introduzimos a transforma√ß√£o de Sims, Stock e Watson para abordar processos mais gerais com componentes autorregressivos [^2].  Nesta se√ß√£o, detalharemos o processo de obten√ß√£o das distribui√ß√µes assint√≥ticas dos estimadores OLS nesses modelos, enfatizando a aplica√ß√£o do teorema do limite central e do teorema de converg√™ncia de martingales, quando o ru√≠do √© um processo de ru√≠do branco. Este desenvolvimento √© crucial para infer√™ncia estat√≠stica e para testar hip√≥teses sobre os par√¢metros dos modelos com tend√™ncias temporais.

### Conceitos Fundamentais
Como visto anteriormente, a an√°lise de modelos de regress√£o com tend√™ncias temporais determin√≠sticas exige t√©cnicas espec√≠ficas devido √†s diferentes taxas de converg√™ncia dos estimadores OLS [^1, ^2]. Em particular, para o modelo de tend√™ncia linear simples $y_t = \alpha + \delta t + \epsilon_t$, estabelecemos que o estimador de $\alpha$ ($\hat{\alpha}$) converge para o verdadeiro valor $\alpha$ a uma taxa de $\sqrt{T}$, enquanto o estimador de $\delta$ ($\hat{\delta}$) converge para $\delta$ a uma taxa de $T^{3/2}$ [^4, ^7]. Para obter distribui√ß√µes assint√≥ticas n√£o degeneradas, √© necess√°rio multiplicar $\hat{\alpha}$ por $\sqrt{T}$ e $\hat{\delta}$ por $T^{3/2}$, como expresso na matriz $Y_T$ [^5].

Retomando a an√°lise do modelo de regress√£o com tend√™ncia determin√≠stica simples, temos o vetor dos estimadores OLS dado por:
$$ \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \left( \sum_{t=1}^T x_t y_t \right) $$
onde $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$. O desvio dos estimadores OLS em rela√ß√£o aos valores verdadeiros √©:
$$ \begin{bmatrix} \hat{\alpha}_T - \alpha \\ \hat{\delta}_T - \delta \end{bmatrix} = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \left( \sum_{t=1}^T x_t \epsilon_t \right) $$
A matriz $\left( \sum_{t=1}^T x_t x_t' \right)^{-1}$ diverge, sendo necess√°rio o reescalonamento por meio da multiplica√ß√£o por $Y_T$, de forma a obtermos:
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1} Y_T \left( \sum_{t=1}^T x_t \epsilon_t \right) $$
Ao analisar o primeiro termo na √∫ltima express√£o,
$$ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T = \left[ Y_T^{-1} \left( \sum_{t=1}^T x_t x_t' \right) Y_T^{-1} \right]^{-1} $$
vimos que esse termo converge para uma matriz $Q$ [^5]:
$$ \left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right]^{-1} \rightarrow Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix} \quad [16.1.19, 16.1.20] $$
> üí° **Exemplo Num√©rico:** Vamos ilustrar como essa matriz $Q$ √© calculada, considerando um tamanho de amostra $T=100$. Primeiro, calculamos $\sum_{t=1}^T x_t x_t'$:
> $$ \sum_{t=1}^{100} x_t x_t' = \sum_{t=1}^{100} \begin{bmatrix} 1 \\ t \end{bmatrix} \begin{bmatrix} 1 & t \end{bmatrix} = \sum_{t=1}^{100} \begin{bmatrix} 1 & t \\ t & t^2 \end{bmatrix} = \begin{bmatrix} \sum_{t=1}^{100} 1 & \sum_{t=1}^{100} t \\ \sum_{t=1}^{100} t & \sum_{t=1}^{100} t^2 \end{bmatrix} $$
> Usando as f√≥rmulas para soma de inteiros e soma de quadrados:
> $$ \sum_{t=1}^{100} 1 = 100, \quad \sum_{t=1}^{100} t = \frac{100(101)}{2} = 5050, \quad \sum_{t=1}^{100} t^2 = \frac{100(101)(201)}{6} = 338350 $$
> Assim,
> $$ \sum_{t=1}^{100} x_t x_t' = \begin{bmatrix} 100 & 5050 \\ 5050 & 338350 \end{bmatrix} $$
> Agora,  $Y_T^{-1} = \begin{bmatrix} 1/\sqrt{T} & 0 \\ 0 & 1/T^{3/2} \end{bmatrix}$ para T = 100
> $Y_{100}^{-1} = \begin{bmatrix} 1/10 & 0 \\ 0 & 1/1000 \end{bmatrix}$.  Calculamos:
> $$ Y_{100}^{-1} \left( \sum_{t=1}^{100} x_t x_t' \right) Y_{100}^{-1}  = \begin{bmatrix} 1/10 & 0 \\ 0 & 1/1000 \end{bmatrix} \begin{bmatrix} 100 & 5050 \\ 5050 & 338350 \end{bmatrix} \begin{bmatrix} 1/10 & 0 \\ 0 & 1/1000 \end{bmatrix}  = \begin{bmatrix} 1 & 5.05 \\ 5.05 & 338.35 \end{bmatrix} \begin{bmatrix} 1/10 & 0 \\ 0 & 1/1000 \end{bmatrix} = \begin{bmatrix} 0.1 & 0.00505 \\ 0.505 & 0.33835 \end{bmatrix} $$
> Calculando o inverso da matriz obtida no passo anterior:
> $$ \left[Y_{100}^{-1} \left( \sum_{t=1}^{100} x_t x_t' \right) Y_{100}^{-1} \right]^{-1} \approx \begin{bmatrix} 1.004 & 0.500 \\ 0.500 & 0.333 \end{bmatrix} $$
> Note que essa matriz se aproxima de Q = $\begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$ conforme o valor de T aumenta.

O segundo termo, por sua vez, torna-se:
$$ Y_T \left( \sum_{t=1}^T x_t \epsilon_t \right) = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{T^{3/2}} \sum_{t=1}^T t \epsilon_t \end{bmatrix} = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix} \quad [16.1.21] $$
Para demonstrar que o vetor acima √© assintoticamente Gaussiano, s√£o utilizadas duas ferramentas: o teorema do limite central e o teorema de converg√™ncia de martingales.

**Teorema do Limite Central (TLC)**

O teorema do limite central afirma que, sob certas condi√ß√µes, a distribui√ß√£o da m√©dia amostral de uma sequ√™ncia de vari√°veis aleat√≥rias independentes e identicamente distribu√≠das (i.i.d.) aproxima-se de uma distribui√ß√£o normal quando o tamanho da amostra aumenta. Mais precisamente, se $X_1, X_2, \ldots, X_T$ s√£o vari√°veis aleat√≥rias i.i.d. com m√©dia $\mu$ e vari√¢ncia $\sigma^2$, ent√£o a m√©dia amostral $\bar{X} = \frac{1}{T} \sum_{t=1}^T X_t$ tem a seguinte distribui√ß√£o assint√≥tica:
$$ \frac{\bar{X} - \mu}{\sigma/\sqrt{T}} \overset{d}{\rightarrow} N(0, 1) $$
Equivalentemente,
$$ \sqrt{T}(\bar{X} - \mu) \overset{d}{\rightarrow} N(0, \sigma^2) $$

No contexto do nosso modelo, o primeiro elemento do vetor [16.1.21] √©:
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t = \sqrt{T} \left( \frac{1}{T} \sum_{t=1}^T \epsilon_t \right) $$
Se $\epsilon_t$ s√£o i.i.d. com m√©dia zero e vari√¢ncia $\sigma^2$, ent√£o, pelo TLC, temos:
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \overset{d}{\rightarrow} N(0, \sigma^2) $$

**Prova do Teorema do Limite Central (TLC) aplicado ao primeiro termo do vetor [16.1.21]**
*Provaremos que se $\epsilon_t$ s√£o i.i.d. com m√©dia zero e vari√¢ncia $\sigma^2$, ent√£o $\frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \overset{d}{\rightarrow} N(0, \sigma^2)$*.
I. Dado que $\epsilon_t$ s√£o i.i.d. com m√©dia 0 e vari√¢ncia $\sigma^2$, definimos a m√©dia amostral $\bar{\epsilon} = \frac{1}{T} \sum_{t=1}^T \epsilon_t$.
II. Pelo Teorema do Limite Central, sabemos que $\sqrt{T}(\bar{\epsilon} - 0) \overset{d}{\rightarrow} N(0, \sigma^2)$, que √© equivalente a $\sqrt{T}\bar{\epsilon} \overset{d}{\rightarrow} N(0, \sigma^2)$.
III. Substituindo $\bar{\epsilon}$, temos $\sqrt{T} \left( \frac{1}{T} \sum_{t=1}^T \epsilon_t \right) \overset{d}{\rightarrow} N(0, \sigma^2)$.
IV. Simplificando, obtemos $\frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \overset{d}{\rightarrow} N(0, \sigma^2)$. ‚ñ†

> üí° **Exemplo Num√©rico:** Suponha que temos uma s√©rie de erros $\epsilon_t$ com $T=100$,  $\epsilon_t \sim N(0, \sigma^2)$, com $\sigma^2 = 2$. O TLC nos diz que $\frac{1}{\sqrt{100}} \sum_{t=1}^{100} \epsilon_t$ deve se aproximar de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia $\sigma^2 = 2$. Vamos simular e verificar:
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy.stats import norm
>
> # Par√¢metros
> T = 100
> sigma_sq = 2
> num_simulations = 1000
>
> # Simula√ß√µes
> sum_eps = np.zeros(num_simulations)
> for i in range(num_simulations):
>   eps = np.random.normal(0, np.sqrt(sigma_sq), T)
>   sum_eps[i] = np.sum(eps) / np.sqrt(T)
>
> # Plot do histograma e da distribui√ß√£o normal te√≥rica
> plt.hist(sum_eps, bins=30, density=True, alpha=0.6, color='blue', label='Simula√ß√µes')
> x = np.linspace(np.min(sum_eps), np.max(sum_eps), 100)
> plt.plot(x, norm.pdf(x, 0, np.sqrt(sigma_sq)), color='red', label='Normal Te√≥rica')
> plt.title("Distribui√ß√£o de $\frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t$")
> plt.legend()
> plt.show()
>
> print(f"M√©dia amostral: {np.mean(sum_eps):.4f}")
> print(f"Vari√¢ncia amostral: {np.var(sum_eps):.4f}")
> print(f"Vari√¢ncia te√≥rica: {sigma_sq:.4f}")
> ```
> A m√©dia amostral se aproxima de 0, e a vari√¢ncia amostral se aproxima de 2, como esperado pelo TLC.

**Teorema da Converg√™ncia de Martingales**

Um processo estoc√°stico $\{M_t\}$ √© uma martingale se $E[|M_t|] < \infty$ e $E[M_t | M_{t-1}, M_{t-2}, ...] = M_{t-1}$ para todo $t$. Uma sequ√™ncia de diferen√ßas de martingales √© um processo estoc√°stico $\{X_t\}$ tal que $X_t = M_t - M_{t-1}$ para alguma martingale $M_t$.

O teorema de converg√™ncia de martingales estabelece que, sob condi√ß√µes espec√≠ficas, a soma acumulada de uma sequ√™ncia de diferen√ßas de martingales converge para uma distribui√ß√£o normal. Em particular, se $\{X_t\}$ √© uma sequ√™ncia de diferen√ßas de martingales com vari√¢ncia condicional $E[X_t^2 | X_{t-1}, X_{t-2}, \ldots ] = \sigma_t^2$, e se a soma das vari√¢ncias condicionais converge:
$$ \frac{1}{T} \sum_{t=1}^T \sigma_t^2 \overset{p}{\rightarrow} \sigma^2 $$
e
$$ \frac{1}{T} \sum_{t=1}^T E[X_t^4] < \infty $$
ent√£o,
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^T X_t \overset{d}{\rightarrow} N(0, \sigma^2) $$

No contexto do nosso modelo, o segundo elemento do vetor [16.1.21] √©:
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t = \frac{1}{\sqrt{T}} \sum_{t=1}^T X_t $$
onde $X_t = \frac{t}{T} \epsilon_t$.  Observe que $\{X_t\}$ forma uma sequ√™ncia de diferen√ßas de martingales, uma vez que $E[\epsilon_t | \epsilon_{t-1}, \epsilon_{t-2}, ...] = 0$. A vari√¢ncia condicional de $X_t$ √©:
$$ \sigma_t^2 = E \left[ \left( \frac{t}{T} \epsilon_t \right)^2 \right] = \frac{t^2}{T^2} \sigma^2 $$
E, portanto:
$$ \frac{1}{T} \sum_{t=1}^T \sigma_t^2 = \frac{\sigma^2}{T^3} \sum_{t=1}^T t^2 \rightarrow \frac{\sigma^2}{3} $$
Logo, pelo teorema da converg√™ncia de martingales, temos:
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \overset{d}{\rightarrow} N(0, \frac{\sigma^2}{3}) $$

**Prova da Converg√™ncia via Teorema de Martingales para o segundo termo do vetor [16.1.21]**
*Provaremos que $\frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \overset{d}{\rightarrow} N(0, \frac{\sigma^2}{3})$*
I. Definimos $X_t = \frac{t}{T}\epsilon_t$. Queremos mostrar que $\frac{1}{\sqrt{T}} \sum_{t=1}^T X_t$ converge em distribui√ß√£o para uma normal com m√©dia 0 e vari√¢ncia $\frac{\sigma^2}{3}$.
II. Note que $\{X_t\}$ √© uma sequ√™ncia de diferen√ßas de martingales, pois $E[\epsilon_t | \epsilon_{t-1}, \epsilon_{t-2}, ...] = 0$.
III. Calculamos a vari√¢ncia condicional de $X_t$: $\sigma_t^2 = E[X_t^2|X_{t-1}, X_{t-2},...] = E[\left(\frac{t}{T}\epsilon_t\right)^2] = \frac{t^2}{T^2} E[\epsilon_t^2] = \frac{t^2}{T^2}\sigma^2$.
IV. Calculamos a m√©dia das vari√¢ncias condicionais: $\frac{1}{T}\sum_{t=1}^T \sigma_t^2 = \frac{1}{T}\sum_{t=1}^T \frac{t^2}{T^2}\sigma^2 = \frac{\sigma^2}{T^3}\sum_{t=1}^T t^2$.
V. Usando a f√≥rmula para a soma dos quadrados, $\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6}$, ent√£o $\frac{1}{T}\sum_{t=1}^T \sigma_t^2 = \frac{\sigma^2}{T^3} \frac{T(T+1)(2T+1)}{6} = \frac{\sigma^2}{6} \frac{(T+1)(2T+1)}{T^2}$
VI. Conforme $T \rightarrow \infty$, temos $\frac{(T+1)(2T+1)}{T^2} \rightarrow 2$, logo $\frac{1}{T}\sum_{t=1}^T \sigma_t^2 \rightarrow \frac{\sigma^2}{6} \cdot 2 = \frac{\sigma^2}{3}$.
VII. Assumindo que $\frac{1}{T} \sum_{t=1}^T E[X_t^4] < \infty$, podemos aplicar o teorema de converg√™ncia de martingales,
VIII. Portanto, $\frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \overset{d}{\rightarrow} N(0, \frac{\sigma^2}{3})$. ‚ñ†

> üí° **Exemplo Num√©rico:** Similar ao exemplo do TLC, vamos simular para um ru√≠do branco com $\sigma^2 = 2$ e $T=100$.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy.stats import norm
>
> # Par√¢metros
> T = 100
> sigma_sq = 2
> num_simulations = 1000
>
> # Simula√ß√µes
> sum_tx_eps = np.zeros(num_simulations)
> for i in range(num_simulations):
>     eps = np.random.normal(0, np.sqrt(sigma_sq), T)
>     t = np.arange(1, T + 1)
>     sum_tx_eps[i] = np.sum(t/T * eps) / np.sqrt(T)
>
> # Plot do histograma e da distribui√ß√£o normal te√≥rica
> plt.hist(sum_tx_eps, bins=30, density=True, alpha=0.6, color='blue', label='Simula√ß√µes')
> x = np.linspace(np.min(sum_tx_eps), np.max(sum_tx_eps), 100)
> plt.plot(x, norm.pdf(x, 0, np.sqrt(sigma_sq/3)), color='red', label='Normal Te√≥rica')
> plt.title("Distribui√ß√£o de $\frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t$")
> plt.legend()
> plt.show()
>
> print(f"M√©dia amostral: {np.mean(sum_tx_eps):.4f}")
> print(f"Vari√¢ncia amostral: {np.var(sum_tx_eps):.4f}")
> print(f"Vari√¢ncia te√≥rica: {sigma_sq/3:.4f}")
> ```
> A m√©dia amostral se aproxima de 0 e a vari√¢ncia se aproxima de $\sigma^2/3 = 2/3$, conforme o esperado.

**Observa√ß√£o 1**

√â importante destacar que a converg√™ncia demonstrada acima para o segundo termo no vetor [16.1.21] utilizando o teorema de converg√™ncia de martingales depende crucialmente de $\epsilon_t$ serem um ru√≠do branco, ou seja, n√£o autocorrelacionado. Se $\epsilon_t$ seguir um processo mais geral, como um processo ARMA, a an√°lise torna-se mais complexa, e o teorema de converg√™ncia de martingales em sua forma mais simples pode n√£o ser diretamente aplic√°vel. Nesses casos, √© necess√°rio usar resultados mais gerais para diferen√ßas de martingales ou outros teoremas limites que acomodem a depend√™ncia temporal das inova√ß√µes. A an√°lise de modelos com erros correlacionados ser√° abordada em se√ß√µes posteriores.

**Deriva√ß√£o da Distribui√ß√£o Assint√≥tica**

A partir do reescalonamento dos estimadores OLS e da aplica√ß√£o do TLC e do teorema da converg√™ncia de martingales, podemos concluir que o vetor
$$ \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix} $$
converge em distribui√ß√£o para uma normal multivariada com m√©dia zero e matriz de covari√¢ncia $\sigma^2Q$, onde $Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$ [^6]. Portanto, a distribui√ß√£o assint√≥tica do vetor de erros escalados √© dada por:
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \overset{d}{\rightarrow} N(0, \sigma^2 Q^{-1}) \quad [16.1.25] $$
onde
$$ Q^{-1} =  \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} $$

**Prova da distribui√ß√£o assint√≥tica do vetor de erros escalados:**
*Provaremos que $\begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \overset{d}{\rightarrow} N(0, \sigma^2 Q^{-1})$*
I. Da an√°lise anterior, sabemos que $\begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} =  Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1} Y_T \left( \sum_{t=1}^T x_t \epsilon_t \right)$.
II. Definimos  $A_T = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1}$, e  $B_T = Y_T \left( \sum_{t=1}^T x_t \epsilon_t \right)$.
III. Sabemos que $A_T \rightarrow Q^{-1}$, onde $Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$
IV. Tamb√©m sabemos que $B_T = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix}$ converge em distribui√ß√£o para uma normal com m√©dia 0 e matriz de covari√¢ncia $\sigma^2Q$
V. Portanto, podemos escrever $\begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} = A_T B_T$.
VI. Como $A_T$ converge para $Q^{-1}$ e $B_T$ converge para uma distribui√ß√£o normal com m√©dia 0 e matriz de covari√¢ncia $\sigma^2Q$, o produto $A_TB_T$ converge para uma distribui√ß√£o normal com m√©dia 0 e matriz de covari√¢ncia $Q^{-1}(\sigma^2Q)(Q^{-1})' = \sigma^2 Q^{-1}QQ^{-1} = \sigma^2 Q^{-1}$.
VII. Assim, $\begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \overset{d}{\rightarrow} N(0, \sigma^2 Q^{-1})$. ‚ñ†

> üí° **Exemplo Num√©rico:** Para ilustrar a distribui√ß√£o assint√≥tica dos estimadores escalados, vamos simular um modelo com $\alpha = 2$, $\delta = 0.5$, $\sigma^2 = 1$, e $T=200$. Simularemos 1000 vezes e calcularemos os estimadores OLS e seus erros escalados:
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
> from scipy.stats import multivariate_normal
>
> # Par√¢metros
> alpha = 2
> delta = 0.5
> sigma_sq = 1
> T = 200
> num_simulations = 1000
>
> # Simula√ß√µes
> alpha_hat_simulations = np.zeros(num_simulations)
> delta_hat_simulations = np.zeros(num_simulations)
>
> for i in range(num_simulations):
>    t = np.arange(1, T + 1)
>    epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
>    y = alpha + delta * t + epsilon
>
>    X = np.column_stack((np.ones(T), t))
>    model = sm.OLS(y, X)
>    results = model.fit()
>
>    alpha_hat_simulations[i] = results.params[0]
>    delta_hat_simulations[i] = results.params[1]
>
> # Escalando os erros
> scaled_alpha_errors = np.sqrt(T) * (alpha_hat_simulations - alpha)
> scaled_delta_errors = T**(3/2) * (delta_hat_simulations - delta)
>
> # Matriz de covari√¢ncia te√≥rica
> Q_inv = np.array([[4, -6], [-6, 12]])
> cov_matrix = sigma_sq * Q_inv
>
> # Plot da distribui√ß√£o conjunta
> x, y = np.mgrid[min(scaled_alpha_errors)-1:max(scaled_alpha_errors)+1:.01,
>                 min(scaled_delta_errors)-1:max(scaled_delta_errors)+1:.01]
> pos = np.dstack((x, y))
> rv = multivariate_normal(mean=[0, 0], cov=cov_matrix)
>
> plt.contourf(x, y, rv.pdf(pos), cmap='viridis')
> plt.scatter(scaled_alpha_errors, scaled_delta_errors, s=10, alpha=0.5, color='white', label='Simula√ß√µes')
> plt.xlabel('$\\sqrt{T}(\\hat{\\alpha} - \\alpha)$')
> plt.ylabel('$T^{3/2}(\\hat{\\delta} - \\delta)$')
> plt.title("Distribui√ß√£o conjunta dos erros escalados")
> plt.legend()
> plt.show()
>
> print(f"M√©dia amostral de sqrt(T)(alpha_hat - alpha): {np.mean(scaled_alpha_errors):.4f}")
> print(f"Vari√¢ncia amostral de sqrt(T)(alpha_hat - alpha): {np.var(scaled_alpha_errors):.4f}")
> print(f"M√©dia amostral de T^(3/2)(delta_hat - delta): {np.mean(scaled_delta_errors):.4f}")
> print(f"Vari√¢ncia amostral de T^(3/2)(delta_hat - delta): {np.var(scaled_delta_errors):.4f}")
> print(f"Covari√¢ncia amostral de sqrt(T)(alpha_hat - alpha) e T^(3/2)(delta_hat - delta): {np.cov(scaled_alpha_errors, scaled_delta_errors)[0, 1]:.4f}")
> print(f"Matriz de covari√¢ncia te√≥rica:\n {cov_matrix}")
> ```
> Os resultados das simula√ß√µes confirmam que os erros escalados se comportam de acordo com a distribui√ß√£o normal bivariada, com m√©dia pr√≥xima de zero e matriz de covari√¢ncia pr√≥xima da matriz $\sigma^2 Q^{-1}$.

**Corol√°rio 1**

O resultado acima, expressado em [16.1.25], permite obter distribui√ß√µes assint√≥ticas para fun√ß√µes lineares dos estimadores $\hat{\alpha}$ e $\hat{\delta}$. Por exemplo, para um vetor de constantes $a = [a_1, a_2]'$, a combina√ß√£o linear $a' \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} $ ter√° a seguinte distribui√ß√£o assint√≥tica:
$$
\begin{bmatrix} a_1 \sqrt{T} & a_2 T^{3/2} \end{bmatrix} \begin{bmatrix} \hat{\alpha}_T - \alpha \\ \hat{\delta}_T - \delta \end{bmatrix}  \overset{d}{\rightarrow} N(0, \sigma^2 a' Q^{-1} a)
$$
Este resultado √© fundamental para testar hip√≥teses lineares sobre os par√¢metros do modelo.

> üí° **Exemplo Num√©rico:** Suponha que queremos testar a hip√≥tese nula de que $\delta = 0.5$ em um modelo com $\alpha = 2$ e $\sigma^2 = 1$. Considere $a = [0, 1]$, de forma que o corol√°rio se torna:
> $$
> \begin{bmatrix} 0 \sqrt{T} & 1 T^{3/2} \end{bmatrix} \begin{bmatrix} \hat{\alpha}_T - \alpha \\ \hat{\delta}_T - \delta \end{bmatrix} \overset{d}{\rightarrow} N(0, \sigma^2 a' Q^{-1} a)
> $$
> $$ T^{3/2}(\hat{\delta}_T - \delta) \overset{d}{\rightarrow} N(0, \sigma^2  \begin{bmatrix} 0 & 1 \end{bmatrix} \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} \begin{bmatrix} 0 \\ 1 \end{bmatrix}) = N(0, 12\sigma^2)
> $$
> Vamos simular e verificar:
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
> from scipy.stats import norm
>
> # Par√¢metros
> alpha = 2
> delta = 0.5
> sigma_sq = 1
> T = 200
> num_simulations = 1000
>
> # Simula√ß√µes
> delta_hat_simulations = np.zeros(num_simulations)
>
> for i in range(num_simulations):
>   t = np.arange(1, T + 1)
>   epsilon = np.random.normal(0, np.sqrt(sigma_sq), T)
>   y = alpha + delta * t + epsilon
>
>   X = np.column_stack((np.ones(T), t))
>   model = sm.OLS(y, X)
>   results = model.fit()
>
>   delta_hat_simulations[i] = results.params[1]
>
> # Calculando a estat√≠stica de teste
> scaled_delta_errors = T**(3/2) * (delta_hat_simulations - delta)
>
> # Plot do histograma
> plt.hist(scaled_delta_errors, bins=30, density=True, alpha=0.6, color='blue', label='Simula√ß√µes')
> x = np.linspace(min(scaled_delta_errors), max(scaled_delta_errors), 100)
> plt.plot(x, norm.pdf(x, 0, np.sqrt(12*sigma_sq)), color='red', label='Normal Te√≥rica')
> plt.title("Distribui√ß√£o de $T^{3/2}(\\hat{\\delta} - \\delta)$")
> plt.legend()
> plt.show()
>
> print(f"M√©dia amostral da estat√≠stica: {np.mean(statistic):.4f}")
> print(f"Desvio padr√£o amostral da estat√≠stica: {np.std(statistic):.4f}")
> ```
>
> A execu√ß√£o deste c√≥digo produz um histograma da distribui√ß√£o de $T^{3/2}(\hat{\delta} - \delta)$ juntamente com a densidade normal te√≥rica, bem como a m√©dia e o desvio padr√£o amostral da estat√≠stica.
>
> ### Conclus√£o
>
> Este cap√≠tulo explorou em detalhes o conceito de estima√ß√£o pontual e suas propriedades, incluindo vi√©s, erro quadr√°tico m√©dio e consist√™ncia. Foi demonstrado como essas propriedades s√£o cruciais para avaliar a qualidade de um estimador. Al√©m disso, foi apresentado um exemplo pr√°tico utilizando simula√ß√£o para verificar a distribui√ß√£o assint√≥tica do estimador de um par√¢metro de um modelo AR(1). Este exemplo ilustra como a teoria estat√≠stica pode ser aplicada em cen√°rios reais para validar a performance de estimadores.
>
> O conte√∫do deste cap√≠tulo fornece uma base s√≥lida para entender e aplicar m√©todos de estima√ß√£o em diversas √°reas da estat√≠stica e econometria, al√©m de fornecer ferramentas para a an√°lise e avalia√ß√£o de estimadores em modelos mais complexos.
>
>
> <!-- END -->
