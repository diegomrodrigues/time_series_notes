## Processes with Deterministic Time Trends: Asymptotic Distribution of OLS Estimates

### Introdu√ß√£o
Este cap√≠tulo explora as particularidades da an√°lise de s√©ries temporais que envolvem **tend√™ncias temporais determin√≠sticas**, um cen√°rio onde as abordagens convencionais para regress√µes com vari√°veis estacion√°rias se mostram inadequadas [^1]. Em particular, examinamos como a aplica√ß√£o de **Ordinary Least Squares (OLS)** em modelos com tend√™ncias temporais determin√≠sticas resulta em taxas de converg√™ncia distintas para os diferentes par√¢metros estimados, o que exige m√©todos espec√≠ficos para a deriva√ß√£o de distribui√ß√µes assint√≥ticas. O cap√≠tulo se baseia nos conceitos de **distribui√ß√µes assint√≥ticas** e **taxas de converg√™ncia** previamente abordados em contextos de regress√£o com vari√°veis estacion√°rias, mas agora adaptando-os ao tratamento de tend√™ncias temporais determin√≠sticas. Ao longo do cap√≠tulo, faremos refer√™ncia a m√©todos introduzidos em cap√≠tulos anteriores, como o uso de estat√≠sticas t e F [^1], e expandiremos seu entendimento para processos n√£o estacion√°rios.

### Conceitos Fundamentais
Em modelos de regress√£o, os coeficientes s√£o frequentemente estimados usando **Ordinary Least Squares (OLS)**. No entanto, quando esses modelos incluem **ra√≠zes unit√°rias** ou **tend√™ncias temporais determin√≠sticas**, as distribui√ß√µes assint√≥ticas dos estimadores dos coeficientes n√£o podem ser derivadas da mesma maneira que para modelos com vari√°veis estacion√°rias [^1]. Uma das principais dificuldades surge porque os estimadores de diferentes par√¢metros podem apresentar **taxas de converg√™ncia assint√≥tica** distintas. Este cap√≠tulo aborda a ideia de diferentes taxas de converg√™ncia e desenvolve uma abordagem geral para obter distribui√ß√µes assint√≥ticas, conforme sugerido por Sims, Stock, e Watson (1990) [^1].

O foco inicial √© no caso mais simples: **inova√ß√µes i.i.d** em torno de uma tend√™ncia temporal determin√≠stica [^1]. Consideremos um modelo de tend√™ncia temporal simples dado por:
$$y_t = \alpha + \delta t + \epsilon_t$$
onde $\epsilon_t$ √© um ru√≠do branco. Se $\epsilon_t \sim N(0, \sigma^2)$, este modelo satisfaz as premissas cl√°ssicas de regress√£o, e as estat√≠sticas t e F padr√£o poderiam ser usadas em princ√≠pio [^1]. No entanto, para obter as distribui√ß√µes assint√≥ticas de $\hat{\alpha}$ e $\hat{\delta}$ quando $\epsilon_t$ n√£o √© necessariamente gaussiano, t√©cnicas diferentes das utilizadas em regress√µes estacion√°rias (como visto no Cap√≠tulo 8 [^1]) s√£o necess√°rias. O cap√≠tulo introduz essas t√©cnicas, que ser√£o √∫teis tanto para o estudo de tend√™ncias temporais quanto para analisar estimadores em processos n√£o estacion√°rios nos Cap√≠tulos 17 e 18 [^2].

> üí° **Exemplo Num√©rico:** Suponha que temos uma s√©rie temporal de 100 observa√ß√µes, onde $y_t$ representa a receita de uma empresa ao longo do tempo. O modelo de tend√™ncia temporal √© $y_t = 10 + 0.5t + \epsilon_t$, onde $\alpha = 10$ e $\delta = 0.5$. Simulamos dados para este modelo com $\epsilon_t \sim N(0, 2^2)$:
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> np.random.seed(42)
> T = 100
> alpha = 10
> delta = 0.5
> sigma = 2
>
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, sigma, T)
> y = alpha + delta * t + epsilon
>
> plt.plot(t, y)
> plt.xlabel("Tempo (t)")
> plt.ylabel("Receita (y_t)")
> plt.title("S√©rie Temporal com Tend√™ncia Determin√≠stica")
> plt.show()
> ```
> Este gr√°fico visualiza a s√©rie temporal com uma tend√™ncia crescente e ru√≠do aleat√≥rio ao redor da tend√™ncia. O objetivo do OLS √© estimar os par√¢metros $\alpha$ e $\delta$ a partir destes dados.

Para encontrar as distribui√ß√µes assint√≥ticas, o modelo [16.1.1] √© reescrito na forma padr√£o de regress√£o:
$$y_t = x_t'\beta + \epsilon_t$$
onde $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$ e $\beta = \begin{bmatrix} \alpha \\ \delta \end{bmatrix}$ [^2].

O estimador OLS de $\beta$, denotado por $b_T$, baseado em uma amostra de tamanho $T$ √©:
$$b_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \left( \sum_{t=1}^T x_t y_t \right)$$ [^2]

O desvio do estimador OLS em rela√ß√£o ao valor verdadeiro √© dado por:
$$(b_T - \beta) = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \left( \sum_{t=1}^T x_t \epsilon_t \right)$$ [^2]

A abordagem padr√£o, utilizada no Cap√≠tulo 8 [^2] para regress√µes com vari√°veis explicativas estacion√°rias, envolveria multiplicar a equa√ß√£o acima por $\sqrt{T}$. No entanto, essa abordagem n√£o pode ser diretamente aplicada a um modelo com tend√™ncia temporal determin√≠stica. Isso se deve ao fato de que, para $x_t$ e $\beta$ dados, a express√£o [16.1.6] assume a forma:
$$ \begin{bmatrix} \hat{\alpha}_T - \alpha \\ \hat{\delta}_T - \delta \end{bmatrix} = \begin{bmatrix} \sum_t 1 & \sum_t t \\ \sum_t t & \sum_t t^2 \end{bmatrix}^{-1} \begin{bmatrix} \sum_t \epsilon_t \\ \sum_t t\epsilon_t \end{bmatrix} $$ [^2].

√â poss√≠vel demonstrar, por indu√ß√£o, que [^3]:
$$ \sum_{t=1}^T t = \frac{T(T+1)}{2} \quad [16.1.9] $$
$$ \sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6} \quad [16.1.10] $$

**Prova das equa√ß√µes [16.1.9] e [16.1.10]:**

**Prova de [16.1.9]:**

Queremos provar que $\sum_{t=1}^T t = \frac{T(T+1)}{2}$.

I.  **Caso base:** Para $T=1$, $\sum_{t=1}^1 t = 1$ e $\frac{1(1+1)}{2} = 1$. Portanto, a f√≥rmula √© v√°lida para $T=1$.
II.  **Hip√≥tese indutiva:** Assumimos que a f√≥rmula √© v√°lida para algum $k \geq 1$, ou seja, $\sum_{t=1}^k t = \frac{k(k+1)}{2}$.
III. **Passo indutivo:** Devemos mostrar que a f√≥rmula tamb√©m √© v√°lida para $k+1$.
     \begin{align*}
     \sum_{t=1}^{k+1} t &= \sum_{t=1}^k t + (k+1) \\
     &= \frac{k(k+1)}{2} + (k+1)  \text{ (por hip√≥tese indutiva)}\\
     &= \frac{k(k+1) + 2(k+1)}{2} \\
     &= \frac{(k+1)(k+2)}{2}
     \end{align*}
Portanto, a f√≥rmula √© v√°lida para $k+1$. Por indu√ß√£o, a f√≥rmula $\sum_{t=1}^T t = \frac{T(T+1)}{2}$ √© v√°lida para todos os inteiros $T \geq 1$. ‚ñ†

**Prova de [16.1.10]:**

Queremos provar que $\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6}$.

I. **Caso base:** Para $T=1$, $\sum_{t=1}^1 t^2 = 1$ e $\frac{1(1+1)(2(1)+1)}{6} = \frac{1(2)(3)}{6} = 1$. Portanto, a f√≥rmula √© v√°lida para $T=1$.
II. **Hip√≥tese indutiva:** Assumimos que a f√≥rmula √© v√°lida para algum $k \geq 1$, ou seja, $\sum_{t=1}^k t^2 = \frac{k(k+1)(2k+1)}{6}$.
III. **Passo indutivo:** Devemos mostrar que a f√≥rmula tamb√©m √© v√°lida para $k+1$.
     \begin{align*}
     \sum_{t=1}^{k+1} t^2 &= \sum_{t=1}^k t^2 + (k+1)^2 \\
     &= \frac{k(k+1)(2k+1)}{6} + (k+1)^2 \text{ (por hip√≥tese indutiva)}\\
     &= \frac{k(k+1)(2k+1) + 6(k+1)^2}{6} \\
     &= \frac{(k+1)[k(2k+1) + 6(k+1)]}{6} \\
     &= \frac{(k+1)(2k^2 + k + 6k + 6)}{6} \\
     &= \frac{(k+1)(2k^2 + 7k + 6)}{6} \\
     &= \frac{(k+1)(k+2)(2k+3)}{6} \\
     &= \frac{(k+1)((k+1)+1)(2(k+1)+1)}{6}
     \end{align*}
Portanto, a f√≥rmula √© v√°lida para $k+1$. Por indu√ß√£o, a f√≥rmula $\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6}$ √© v√°lida para todos os inteiros $T \geq 1$. ‚ñ†

Portanto, o termo dominante em $\sum_t t$ √© $T^2/2$ e o termo dominante em $\sum_t t^2$ √© $T^3/3$. Da√≠,
$$ \frac{1}{T^2} \sum_{t=1}^T t \rightarrow \frac{1}{2} \quad [16.1.11] $$
$$ \frac{1}{T^3} \sum_{t=1}^T t^2 \rightarrow \frac{1}{3} \quad [16.1.12] $$
De forma geral, o termo dominante em $\sum_{t=1}^T t^v$ √© $T^{v+1}/(v+1)$ [^3]:
$$ \frac{1}{T^{v+1}} \sum_{t=1}^T t^v \rightarrow \frac{1}{v+1} \quad [16.1.13] $$
Essa express√£o pode ser verificada observando que $\frac{1}{T^{v+1}} \sum_{t=1}^T t^v = \frac{1}{T} \sum_{t=1}^T (\frac{t}{T})^v$ [^3], e o lado direito pode ser interpretado como uma aproxima√ß√£o da √°rea sob a curva $f(r) = r^v$.
No entanto, diferente do caso estacion√°rio, a matriz $(1/T) \sum_{t=1}^T x_t x_t'$ diverge [^4]. Para obter uma matriz convergente, [16.1.16] precisaria ser dividida por $T^3$, e n√£o por $T$ [^4]:
$$ T^{-3} \sum_{t=1}^T x_t x_t' \rightarrow \begin{bmatrix} 0 & 0 \\ 0 & \frac{1}{3} \end{bmatrix} $$
Essa matriz limite n√£o √© invert√≠vel. Isso demonstra que a abordagem usada para vari√°veis estacion√°rias n√£o se aplica diretamente. As estimativas OLS, $\hat{\alpha}_T$ e $\hat{\delta}_T$, possuem diferentes taxas de converg√™ncia assint√≥tica [^4]. Para obter distribui√ß√µes assint√≥ticas n√£o degeneradas, $\hat{\alpha}_T$ √© multiplicado por $\sqrt{T}$, enquanto $\hat{\delta}_T$ √© multiplicado por $T^{3/2}$. Isso equivale a pr√©-multiplicar [16.1.6] pela matriz:
$$ Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} $$
Obtendo [^5]:
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1} Y_T \left( \sum_{t=1}^T x_t \epsilon_t \right) $$
$$ = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T^{-1} Y_T \left( \sum_{t=1}^T x_t \epsilon_t \right)  $$

O primeiro termo na √∫ltima express√£o de [16.1.18], ap√≥s substituir [16.1.17] e [16.1.16], converge para uma matriz $Q$ [^5]:
$$ \left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right]^{-1} \rightarrow Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix} \quad [16.1.19, 16.1.20] $$

**Prova da converg√™ncia em [16.1.19] e defini√ß√£o da matriz Q em [16.1.20]:**

Para provar que $\left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right]^{-1} \rightarrow Q$, onde $Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$, primeiro precisamos calcular o termo dentro dos colchetes.

I. Sabemos que $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$, ent√£o $x_t x_t' = \begin{bmatrix} 1 & t \\ t & t^2 \end{bmatrix}$. Portanto, $\sum_{t=1}^T x_t x_t' = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix} = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix}$.

II. Aplicamos $Y_T$ e $Y_T^{-1}$:
\begin{align*}
    Y_T \left( \sum_{t=1}^T x_t x_t' \right) Y_T &= \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} \\
    &= \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} T\sqrt{T} & \frac{T(T+1)T^{3/2}}{2} \\ \frac{T(T+1)\sqrt{T}}{2} & \frac{T(T+1)(2T+1)T^{3/2}}{6} \end{bmatrix} \\
    &= \begin{bmatrix} T^2 & \frac{T^2(T+1)T}{2} \\ \frac{T^2(T+1)T}{2} & \frac{T^3(T+1)(2T+1)T^2}{6} \end{bmatrix} \\
    &= \begin{bmatrix} T^2 & \frac{T^3(T+1)}{2} \\ \frac{T^3(T+1)}{2} & \frac{T^5(T+1)(2T+1)}{6} \end{bmatrix}
\end{align*}

III. Calculamos o inverso:
$$ \left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right) Y_T \right]^{-1} = \left[ \begin{bmatrix} T^2 & \frac{T^3(T+1)}{2} \\ \frac{T^3(T+1)}{2} & \frac{T^5(T+1)(2T+1)}{6} \end{bmatrix} \right]^{-1} $$
O termo dominante de cada elemento √©:
\begin{align*}
    &\begin{bmatrix} T^2 & \frac{T^4}{2} \\ \frac{T^4}{2} & \frac{2T^7}{6} \end{bmatrix} = \begin{bmatrix} T^2 & \frac{T^4}{2} \\ \frac{T^4}{2} & \frac{T^7}{3} \end{bmatrix}
\end{align*}

IV. Calculamos a matriz inversa, usando o fato de que a inversa de $\begin{bmatrix} a & b \\ c & d \end{bmatrix}$ √© $\frac{1}{ad-bc}\begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$.
Assim, obtemos:

\begin{align*}
\left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right) Y_T \right]^{-1} =  \frac{1}{T^2 \frac{T^7}{3} - \frac{T^8}{4}}
\begin{bmatrix} \frac{T^7}{3} & -\frac{T^4}{2} \\ -\frac{T^4}{2} & T^2 \end{bmatrix} = \frac{1}{\frac{T^9}{12}} \begin{bmatrix} \frac{T^7}{3} & -\frac{T^4}{2} \\ -\frac{T^4}{2} & T^2 \end{bmatrix} = \begin{bmatrix} \frac{4}{T^2} & -6 \\ -6 & 12 \end{bmatrix}
\end{align*}
O termo dominante da inversa de $Y_T \left( \sum_{t=1}^T x_t x_t' \right) Y_T$ √© $\begin{bmatrix} \frac{12}{T^2} & -\frac{18}{T^3} \\ -\frac{18}{T^3} & \frac{12}{T^5} \end{bmatrix}$.

V. Tomando o limite quando $T \to \infty$:
$$ \left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right]^{-1} = \begin{bmatrix} \frac{T^7}{3} & -\frac{T^4}{2} \\ -\frac{T^4}{2} & T^2 \end{bmatrix} \frac{12}{T^9} = \begin{bmatrix} \frac{4}{T^2} & -\frac{6}{T^5} \\ -\frac{6}{T^5} & \frac{12}{T^7} \end{bmatrix} $$
e definindo
$$  \left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right) Y_T \right]^{-1} \rightarrow Q^{-1} =  \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix} $$

Portanto, a inversa dessa matriz √©:
$$ Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix} $$
Assim, $\left[ Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right]^{-1} \rightarrow Q$ como definido em [16.1.20]. ‚ñ†

> üí° **Exemplo Num√©rico:**  Vamos usar a amostra simulada anteriormente com $T=100$, onde os verdadeiros valores de $\alpha$ e $\delta$ s√£o $10$ e $0.5$ respectivamente e estimar os coeficientes.
> ```python
> from sklearn.linear_model import LinearRegression
>
> X = np.column_stack((np.ones(T), t))
> model = LinearRegression()
> model.fit(X, y)
>
> alpha_hat = model.intercept_
> delta_hat = model.coef_[1]
>
> print(f"Estimativa de alpha: {alpha_hat:.4f}")
> print(f"Estimativa de delta: {delta_hat:.4f}")
> ```
> Este c√≥digo calcula as estimativas de $\hat{\alpha}$ e $\hat{\delta}$ usando OLS. Os valores estimados ser√£o pr√≥ximos dos verdadeiros par√¢metros (10 e 0.5), mas n√£o exatamente iguais devido ao ru√≠do aleat√≥rio $\epsilon_t$.

O segundo termo, por sua vez, torna-se [^5]:
$$ Y_T \left( \sum_{t=1}^T x_t \epsilon_t \right) = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \end{bmatrix} \quad [16.1.21] $$
Sob suposi√ß√µes padr√£o sobre $\epsilon_t$, esse vetor √© assintoticamente gaussiano. Se, por exemplo, $\epsilon_t$ √© i.i.d. com m√©dia zero, vari√¢ncia $\sigma^2$, e momento finito de quarta ordem, ent√£o o primeiro elemento satisfaz o teorema do limite central:
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \overset{d}{\rightarrow} N(0, \sigma^2) $$
Para o segundo elemento, observe que $\{ (t/T) \epsilon_t \}$ √© uma sequ√™ncia de diferen√ßas de martingales que satisfaz as condi√ß√µes da Proposi√ß√£o 7.8 [^5]. Especificamente, sua vari√¢ncia √©:
$$ \sigma^2_t = E[(t/T) \epsilon_t]^2 = \sigma^2 \frac{t^2}{T^2} $$
onde:
$$ \frac{1}{T} \sum_{t=1}^T \sigma^2_t = \sigma^2 \frac{1}{T^3} \sum_{t=1}^T t^2 \rightarrow \frac{\sigma^2}{3} $$
Al√©m disso,
$$ \frac{1}{T} \sum_{t=1}^T [(\frac{t}{T}) \epsilon_t]^2 \overset{p}{\rightarrow} \frac{\sigma^2}{3} $$
e, portanto,
$$ \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t \overset{d}{\rightarrow} N(0, \frac{\sigma^2}{3}) $$
Qualquer combina√ß√£o linear dos elementos do vetor em [16.1.21] tamb√©m √© assintoticamente gaussiana, implicando que a distribui√ß√£o conjunta √© uma distribui√ß√£o gaussiana bivariada [^6].

> üí° **Exemplo Num√©rico:** Para a simula√ß√£o anterior, podemos calcular a m√©dia e a vari√¢ncia de  $\frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t$ e  $\frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t$, e verificar se aproximam-se dos valores te√≥ricos.
> ```python
> u1 = (1 / np.sqrt(T)) * np.sum(epsilon)
> u2 = (1 / np.sqrt(T)) * np.sum((t / T) * epsilon)
>
> print(f"Valor de u1: {u1:.4f}")
> print(f"Valor de u2: {u2:.4f}")
>
> # Para verificar a distribui√ß√£o, vamos simular v√°rias vezes
> num_simulations = 1000
> u1_simulations = np.zeros(num_simulations)
> u2_simulations = np.zeros(num_simulations)
>
> for i in range(num_simulations):
>     epsilon_sim = np.random.normal(0, sigma, T)
>     u1_simulations[i] = (1 / np.sqrt(T)) * np.sum(epsilon_sim)
>     u2_simulations[i] = (1 / np.sqrt(T)) * np.sum((t / T) * epsilon_sim)
>
> print(f"M√©dia de u1 (simula√ß√µes): {np.mean(u1_simulations):.4f}")
> print(f"Vari√¢ncia de u1 (simula√ß√µes): {np.var(u1_simulations):.4f}")
> print(f"M√©dia de u2 (simula√ß√µes): {np.mean(u2_simulations):.4f}")
> print(f"Vari√¢ncia de u2 (simula√ß√µes): {np.var(u2_simulations):.4f}")
>
> theoretical_var_u1 = sigma**2
> theoretical_var_u2 = sigma**2 / 3
> print(f"Vari√¢ncia te√≥rica de u1: {theoretical_var_u1:.4f}")
> print(f"Vari√¢ncia te√≥rica de u2: {theoretical_var_u2:.4f}")
>
> plt.figure(figsize=(12, 6))
> plt.subplot(1, 2, 1)
> plt.hist(u1_simulations, bins=30, density=True, alpha=0.6, color='blue')
> plt.title("Distribui√ß√£o de u1")
> plt.subplot(1, 2, 2)
> plt.hist(u2_simulations, bins=30, density=True, alpha=0.6, color='red')
> plt.title("Distribui√ß√£o de u2")
> plt.show()
> ```
> Aqui, observamos que a m√©dia amostral dos u1 e u2 simulados √© pr√≥xima de 0 e as vari√¢ncias convergem para $\sigma^2$ e $\sigma^2/3$ respectivamente. Os histogramas confirmam que as amostras de u1 e u2 seguem uma distribui√ß√£o normal.

Com base em [16.1.19] e [16.1.24], a distribui√ß√£o assint√≥tica de [16.1.18] √©:
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \overset{d}{\rightarrow} N(0, \sigma^2 Q^{-1}) $$
Esses resultados podem ser resumidos na seguinte proposi√ß√£o:

**Proposi√ß√£o 16.1:** *Se $y_t$ √© gerado de acordo com a tend√™ncia temporal determin√≠stica simples [16.1.1], onde $\epsilon_t$ √© i.i.d. com $E(\epsilon_t^2) = \sigma^2$ e $E(\epsilon_t^4) < \infty$, ent√£o:*
$$ \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \overset{d}{\rightarrow} N\left(0, \sigma^2 \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} \right) \quad [16.1.26] $$
Note que o estimador do coeficiente da tend√™ncia temporal, $\hat{\delta}_T$, √© superconsistente - n√£o apenas $\hat{\delta}_T \rightarrow \delta$, mas tamb√©m $T(\hat{\delta}_T - \delta) \overset{p}{\rightarrow} 0$ [^7].

> üí° **Exemplo Num√©rico:** Para ilustrar a Proposi√ß√£o 16.1, simulamos 1000 s√©ries temporais de tamanho $T=100$ com os par√¢metros do exemplo anterior ($\alpha=10$, $\delta=0.5$, $\sigma=2$). Estimamos $\hat{\alpha}_T$ e $\hat{\delta}_T$ para cada simula√ß√£o, calculamos $\sqrt{T}(\hat{\alpha}_T - \alpha)$ e $T^{3/2}(\hat{\delta}_T - \delta)$, e verificamos suas m√©dias, vari√¢ncias e covari√¢ncia.
> ```python
> num_simulations = 1000
> alpha_hat_simulations = np.zeros(num_simulations)
> delta_hat_simulations = np.zeros(num_simulations)
>
> for i in range(num_simulations):
>    epsilon_sim = np.random.normal(0, sigma, T)
>    y_sim = alpha + delta * t + epsilon_sim
>    model_sim = LinearRegression()
>    model_sim.fit(X, y_sim)
>    alpha_hat_simulations[i] = model_sim.intercept_
>    delta_hat_simulations[i] = model_sim.coef_[1]
>
> scaled_alpha_errors = np.sqrt(T) * (alpha_hat_simulations - alpha)
> scaled_delta_errors = T**(3/2) * (delta_hat_simulations - delta)
>
> cov_matrix = np.cov(scaled_alpha_errors, scaled_delta_errors)
>
> print(f"M√©dia de sqrt(T)(alpha_hat - alpha): {np.mean(scaled_alpha_errors):.4f}")
> print(f"Vari√¢ncia de sqrt(T)(alpha_hat - alpha): {np.var(scaled_alpha_errors):.4f}")
> print(f"M√©dia de T^(3/2)(delta_hat - delta): {np.mean(scaled_delta_errors):.4f}")
> print(f"Vari√¢ncia de T^(3/2)(delta_hat - delta): {np.var(scaled_delta_errors):.4f}")
> print(f"Covari√¢ncia entre sqrt(T)(alpha_hat - alpha) e T^(3/2)(delta_hat - delta): {cov_matrix[0, 1]:.4f}")
>
> theoretical_cov_matrix = sigma**2 * np.array([[4, -6], [-6, 12]])
> print("Matriz de covari√¢ncia te√≥rica:")
> print(theoretical_cov_matrix)
>
> plt.figure(figsize=(12,6))
> plt.subplot(1,2,1)
> plt.hist(scaled_alpha_errors, bins=30, density=True, alpha=0.6, color='blue')
> plt.title("Distribui√ß√£o de sqrt(T)(alpha_hat - alpha)")
> plt.subplot(1,2,2)
> plt.hist(scaled_delta_errors, bins=30, density=True, alpha=0.6, color='red')
> plt.title("Distribui√ß√£o de T^(3/2)(delta_hat - delta)")
> plt.show()
> ```
> Observamos que as m√©dias dos erros escalados s√£o pr√≥ximas de 0, as vari√¢ncias e covari√¢ncia s√£o pr√≥ximas dos valores te√≥ricos fornecidos pela Proposi√ß√£o 16.1, e os histogramas mostram que os estimadores seguem uma distribui√ß√£o normal assint√≥tica.

**Lema 16.1:** *A matriz $Q$ definida em [16.1.20] √© sim√©trica e positiva definida. Al√©m disso, sua inversa √© dada por*
$$ Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} $$

*Prova*: A simetria de $Q$ √© evidente pela sua constru√ß√£o. Para mostrar que $Q$ √© positiva definida, considere qualquer vetor $z = [z_1, z_2]'$. Ent√£o,
$$ z'Qz = \begin{bmatrix} z_1 & z_2 \end{bmatrix} \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix} \begin{bmatrix} z_1 \\ z_2 \end{bmatrix} = z_1^2 + z_1 z_2 + \frac{1}{3}z_2^2 = (z_1+\frac{1}{2}z_2)^2 + \frac{1}{12}z_2^2 $$
Como essa express√£o √© sempre n√£o-negativa e igual a zero apenas se $z_1 = z_2 = 0$, $Q$ √© positiva definida. A inversa de $Q$ pode ser calculada diretamente, ou verificando que $QQ^{-1} = I$.

**Prova da Inversa de Q:**

Queremos provar que a inversa de $Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$ √© $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$.

I.  Para encontrar a inversa de uma matriz 2x2 $A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$, podemos usar a f√≥rmula: $A^{-1} = \frac{1}{ad-bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$.
II.  Neste caso, $a=1$, $b=\frac{1}{2}$, $c=\frac{1}{2}$ e $d=\frac{1}{3}$.
III. Calculamos o determinante de $Q$: $det(Q) = ad - bc = (1)(\frac{1}{3}) - (\frac{1}{2})(\frac{1}{2}) = \frac{1}{3} - \\$\frac{1}{4} = \frac{4-3}{12} = \frac{1}{12}$.
IV. Como o determinante √© diferente de zero, $Q$ √© invert√≠vel.

**Calculando a Inversa**

Para encontrar a inversa $Q^{-1}$ de uma matriz $Q = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$, usamos a f√≥rmula:
$Q^{-1} = \frac{1}{ad-bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$

Neste caso, temos $Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$ e sabemos que $det(Q) = \frac{1}{12}$. Portanto,

$Q^{-1} = \frac{1}{\frac{1}{12}} \begin{bmatrix} \frac{1}{3} & -\frac{1}{2} \\ -\frac{1}{2} & 1 \end{bmatrix} = 12 \begin{bmatrix} \frac{1}{3} & -\frac{1}{2} \\ -\frac{1}{2} & 1 \end{bmatrix} = \begin{bmatrix} 12(\frac{1}{3}) & 12(-\frac{1}{2}) \\ 12(-\frac{1}{2}) & 12(1) \end{bmatrix} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$

Logo, $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$.

**Verifica√ß√£o**

Para verificar, multiplicamos $Q$ por $Q^{-1}$ e o resultado deve ser a matriz identidade $I = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$.

$Q \cdot Q^{-1} = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix} \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix} = \begin{bmatrix} 1(4) + \frac{1}{2}(-6) & 1(-6) + \frac{1}{2}(12) \\ \frac{1}{2}(4) + \frac{1}{3}(-6) & \frac{1}{2}(-6) + \frac{1}{3}(12) \end{bmatrix} = \begin{bmatrix} 4 - 3 & -6 + 6 \\ 2 - 2 & -3 + 4 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$

Como o resultado √© a matriz identidade, a inversa foi calculada corretamente.
<!-- END -->
