## InferÃªncia AssintÃ³tica para um Processo Autoregressivo em torno de uma TendÃªncia Temporal DeterminÃ­stica

### IntroduÃ§Ã£o
Este capÃ­tulo visa explorar a inferÃªncia estatÃ­stica para processos univariados que contÃªm uma raiz unitÃ¡ria, com foco especÃ­fico em processos autoregressivos em torno de uma tendÃªncia temporal determinÃ­stica [^1]. Como vimos anteriormente no capÃ­tulo 16, a anÃ¡lise de modelos de sÃ©ries temporais com tendÃªncias determinÃ­sticas requer uma abordagem diferenciada para a obtenÃ§Ã£o de distribuiÃ§Ãµes assintÃ³ticas dos estimadores, devido Ã s diferentes taxas de convergÃªncia dos parÃ¢metros envolvidos [^1]. O modelo autoregressivo com tendÃªncia temporal determinÃ­stica, expresso como $y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t$, onde $\epsilon_t$ representa um ruÃ­do branco, apresenta um desafio particular nesse contexto [^1]. Este capÃ­tulo irÃ¡ aprofundar a metodologia de Sims, Stock e Watson, que se baseia na transformaÃ§Ã£o dos regressores para isolar componentes estacionÃ¡rios, uma constante e uma tendÃªncia temporal.

### Conceitos Fundamentais
Em continuidade ao desenvolvimento de processos com tendÃªncias determinÃ­sticas, expandimos nossa anÃ¡lise para um processo autoregressivo (AR) de ordem *p* com uma tendÃªncia temporal determinÃ­stica, dado por [^1]:
$$ y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t, $$
onde $\epsilon_t$ Ã© um processo de ruÃ­do branco com mÃ©dia zero, variÃ¢ncia $\sigma^2$, e um quarto momento finito. Assume-se tambÃ©m que as raÃ­zes do polinÃ´mio $1 - \phi_1z - \phi_2z^2 - \ldots - \phi_pz^p = 0$ estÃ£o fora do cÃ­rculo unitÃ¡rio [^1].

**ObservaÃ§Ã£o 1** A condiÃ§Ã£o de que as raÃ­zes do polinÃ´mio caracterÃ­stico estejam fora do cÃ­rculo unitÃ¡rio Ã© crucial para garantir a estacionariedade do processo autoregressivo em torno da tendÃªncia determinÃ­stica, quando considerada sem a tendÃªncia. Isso assegura que as flutuaÃ§Ãµes em torno da tendÃªncia nÃ£o cresÃ§am indefinidamente ao longo do tempo.

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha um processo AR(1) com tendÃªncia: $y_t = 2 + 0.5t + 0.7y_{t-1} + \epsilon_t$. O polinÃ´mio caracterÃ­stico Ã© $1 - 0.7z = 0$, que tem uma raiz em $z = 1/0.7 \approx 1.43$, que estÃ¡ fora do cÃ­rculo unitÃ¡rio. Isso garante que o processo, sem a tendÃªncia ($y_t = 0.7y_{t-1} + \epsilon_t$), seria estacionÃ¡rio. Se a raiz fosse dentro do cÃ­rculo unitÃ¡rio, por exemplo, se o coeficiente fosse 1, a sÃ©rie teria uma raiz unitÃ¡ria e nÃ£o seria estacionÃ¡ria.

A metodologia de Sims, Stock e Watson envolve uma transformaÃ§Ã£o dos regressores do modelo original [^1]. O objetivo Ã© reescrever o modelo [16.3.1] em termos de variÃ¡veis aleatÃ³rias estacionÃ¡rias com mÃ©dia zero, uma constante e uma tendÃªncia temporal [^1]. Esta transformaÃ§Ã£o permite isolar os componentes do vetor de coeficientes OLS com diferentes taxas de convergÃªncia [^1]. Como visto em [16.3.2], somando e subtraindo $\phi_j [\alpha + \delta(t-j)]$ para $j = 1, 2, ..., p$ no lado direito do modelo, o modelo de regressÃ£o pode ser reescrito de forma equivalente como:
$$ y_t = \alpha (1 + \phi_1 + \phi_2 + \ldots + \phi_p) + \delta(1 + \phi_1 + 2\phi_2 + \ldots + p\phi_p) + \delta(\phi_1 + 2\phi_2 + \ldots + p\phi_p) + \phi_1[y_{t-1} - \alpha - \delta(t-1)] + \phi_2[y_{t-2} - \alpha - \delta(t-2)] + \ldots + \phi_p[y_{t-p} - \alpha - \delta(t-p)] + \epsilon_t. $$
Essa formulaÃ§Ã£o pode ser simplificada para [^1]:
$$ y_t = \alpha^* + \delta^*t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \ldots + \phi_p^* y_{t-p}^* + \epsilon_t, $$
onde:
- $\alpha^* = \alpha (1 + \phi_1 + \phi_2 + \ldots + \phi_p) - \delta(\phi_1 + 2\phi_2 + \ldots + p\phi_p)$
- $\delta^* = \delta (1 + \phi_1 + \phi_2 + \ldots + \phi_p)$
- $\phi_j^* = \phi_j$ para $j=1,2,\ldots,p$
- $y_{t-j}^* = y_{t-j} - \alpha - \delta(t-j)$ para $j=1,2,\ldots,p$

A transformaÃ§Ã£o dos regressores em [16.3.4], dada por $y_{t-j}^* = y_{t-j} - \alpha - \delta(t-j)$, Ã© crucial para a anÃ¡lise assintÃ³tica do modelo [^1]. Esta transformaÃ§Ã£o isola os componentes do modelo com diferentes taxas de convergÃªncia, facilitando a anÃ¡lise assintÃ³tica da regressÃ£o [^1].

> ğŸ’¡ **Exemplo NumÃ©rico:** Continuando com o exemplo AR(1): $y_t = 2 + 0.5t + 0.7y_{t-1} + \epsilon_t$, temos $\alpha = 2$, $\delta = 0.5$, e $\phi_1 = 0.7$.
>  - $\alpha^* = 2(1+0.7) - 0.5(0.7) = 2(1.7) - 0.35 = 3.4 - 0.35 = 3.05$
>  - $\delta^* = 0.5(1+0.7) = 0.5(1.7) = 0.85$
>  - $\phi_1^* = 0.7$
>  - $y_{t-1}^* = y_{t-1} - 2 - 0.5(t-1)$.
>   O modelo transformado Ã© entÃ£o $y_t = 3.05 + 0.85t + 0.7y_{t-1}^* + \epsilon_t$.
>
> Vamos ilustrar com alguns dados simulados, onde $\epsilon_t \sim N(0, 1)$. Suponha que temos $y_0 = 5$. Calculamos $y_1 = 2 + 0.5(1) + 0.7(5) + \epsilon_1 = 2 + 0.5 + 3.5 + \epsilon_1 = 6 + \epsilon_1$. Se $\epsilon_1 = 0.2$, entÃ£o $y_1 = 6.2$. Da mesma forma, $y_2 = 2 + 0.5(2) + 0.7(6.2) + \epsilon_2 = 2 + 1 + 4.34 + \epsilon_2 = 7.34 + \epsilon_2$. Se $\epsilon_2 = -0.5$, entÃ£o $y_2 = 6.84$. O valor de $y_1^*$ seria $y_1^* = y_1 - 2 - 0.5(1-1) = 6.2 - 2 = 4.2$. O valor de $y_2^*$ seria $y_2^* = y_2 - 2 - 0.5(2-1) = 6.84 - 2 - 0.5 = 4.34$.

Em notaÃ§Ã£o matricial, o modelo original [16.3.1] pode ser escrito como $y_t = x_t' \beta + \epsilon_t$ [^1], onde:

$$ x_t = \begin{bmatrix} y_{t-1} \\ y_{t-2} \\ \vdots \\ y_{t-p} \\ 1 \\ t \end{bmatrix}  \quad  \beta = \begin{bmatrix} \phi_1 \\ \phi_2 \\ \vdots \\ \phi_p \\ \alpha \\ \delta \end{bmatrix} $$

A transformaÃ§Ã£o algÃ©brica do modelo [16.3.5] para [16.3.3] pode ser descrita como uma reescrita na forma $y_t = x_t' G'(G')^{-1}\beta + \epsilon_t = x_t^{*'} \beta^* + \epsilon_t$ [^1], onde $x_t^* = G x_t$ e $\beta^* = (G')^{-1}\beta$. As matrizes $G'$ e $(G')^{-1}$ sÃ£o definidas em [16.3.8] e [16.3.9], respectivamente [^1].

O estimador de $\beta^*$ por OLS, denotado por $b^*$, Ã© dado por [16.3.11] como:
$$ b^* = \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* y_t = [G']^{-1} \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_t = [G']^{-1}b $$

Esta formulaÃ§Ã£o destaca que o estimador para a regressÃ£o transformada, $b^*$, Ã© uma transformaÃ§Ã£o linear do estimador do sistema original, $b$ [^1]. Os valores ajustados sÃ£o idÃªnticos nos dois modelos: $x_t^{*'} b^* = x_t' b$ [^1]. Embora a regressÃ£o transformada nÃ£o seja diretamente estimÃ¡vel pelos dados, a anÃ¡lise das propriedades da estimativa de OLS para o modelo transformado Ã© mais acessÃ­vel, e a distribuiÃ§Ã£o assintÃ³tica de $b$ pode ser obtida invertendo a transformaÃ§Ã£o [16.3.12]: $b = G'b^*$ [^1].

**Lema 1** A equivalÃªncia dos valores ajustados entre os modelos original e transformado, i.e., $x_t^{*'} b^* = x_t' b$, Ã© uma consequÃªncia direta da propriedade de invariÃ¢ncia dos ajustes por transformaÃ§Ãµes lineares na regressÃ£o. Esta propriedade garante que a transformaÃ§Ã£o utilizada nÃ£o afeta a capacidade preditiva do modelo, apenas reparametriza seus coeficientes.

O ApÃªndice 16.A demonstra que a distribuiÃ§Ã£o assintÃ³tica do estimador da regressÃ£o transformada Ã© dada por:

$$ Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2[Q^*]^{-1}) $$

onde $Y_T$ Ã© uma matriz diagonal que ajusta a taxa de convergÃªncia de cada coeficiente, conforme [16.3.14] [^1]. Especificamente, os coeficientes relacionados Ã s variÃ¡veis de mÃ©dia zero e covariÃ¢ncia estacionÃ¡ria convergem a uma taxa $\sqrt{T}$, enquanto o coeficiente relacionado Ã  tendÃªncia temporal converge a uma taxa $T^{3/2}$. A matriz $Q^*$ Ã© definida em [16.3.15], que captura as interrelaÃ§Ãµes entre os regressores transformados [^1].

> ğŸ’¡ **Exemplo NumÃ©rico:**  Suponha que, para um modelo AR(1) com tendÃªncia, tenhamos apÃ³s a transformaÃ§Ã£o que:
>
> $Y_T = \begin{bmatrix} \sqrt{T} & 0 & 0 \\ 0 & \sqrt{T} & 0 \\ 0 & 0 & T^{3/2} \end{bmatrix}$
>
>
> e que a matriz $[Q^*]^{-1}$ seja estimada como:
>
> $[Q^*]^{-1} = \begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ 0.05 & 0.2 & 1.8 \end{bmatrix}$
>
>Se $\sigma^2 = 1$, entÃ£o a matriz de covariÃ¢ncia assintÃ³tica de $Y_T(b^* - \beta^*)$ seria:
>
>$\sigma^2[Q^*]^{-1} = \begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ 0.05 & 0.2 & 1.8 \end{bmatrix}$
>
> Isso indica, por exemplo, que a variÃ¢ncia assintÃ³tica do estimador do coeficiente autoregressivo ($b_1^*$) Ã© 1.2 quando multiplicada pela taxa de convergÃªncia de $\sqrt{T}$, o que significa que a variÃ¢ncia do estimador original ($b_1^*$) Ã© da ordem de $\frac{1.2}{T}$. O coeficiente da tendÃªncia ($b_3^*$) por sua vez tem uma variÃ¢ncia assintÃ³tica de 1.8 quando multiplicada pela taxa de convergÃªncia de $T^{3/2}$, o que significa que sua variÃ¢ncia original ($b_3^*$) Ã© da ordem de $\frac{1.8}{T^3}$ que converge para zero muito mais rapidamente do que a variÃ¢ncia de $b_1^*$.
>
> Vamos supor que temos uma amostra de tamanho $T=100$. EntÃ£o, $Y_T = \begin{bmatrix} 10 & 0 & 0 \\ 0 & 10 & 0 \\ 0 & 0 & 1000 \end{bmatrix}$. A matriz de covariÃ¢ncia assintÃ³tica de $b^*$ serÃ¡ da ordem de:
>
>  $\frac{1}{T} \begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ 0.05 & 0.2 & 1.8 \end{bmatrix} = \begin{bmatrix} 0.012 & 0.001 & 0.00005 \\ 0.001 & 0.015 & 0.0002 \\ 0.00005 & 0.0002 & 0.0000018 \end{bmatrix}$
>
> Assim, a variÃ¢ncia assintÃ³tica do estimador do coeficiente autoregressivo ($b_1^*$) seria aproximadamente 0.012, a variÃ¢ncia do intercepto seria aproximadamente 0.015, e a variÃ¢ncia do estimador do coeficiente da tendÃªncia temporal ($b_3^*$) seria aproximadamente 0.0000018. Isso demonstra como a convergÃªncia Ã© mais rÃ¡pida para o coeficiente da tendÃªncia.

**Teorema 1** (DistribuiÃ§Ã£o AssintÃ³tica do Estimador do Modelo Original)
Dado que $b = G'b^*$ e conhecendo a distribuiÃ§Ã£o assintÃ³tica de $b^*$, a distribuiÃ§Ã£o assintÃ³tica do estimador $b$ do modelo original Ã© dada por:
$$Y_T (b - \beta) \xrightarrow{d} N(0, \sigma^2 G' [Q^*]^{-1} (G')^T ) $$
*Prova:*
Vamos demonstrar o Teorema 1:

I.  ComeÃ§amos com a relaÃ§Ã£o entre os estimadores do modelo original e transformado: $b = G'b^*$. Subtraindo $\beta$ de ambos os lados, e usando o fato de que $\beta = G' \beta^*$, obtemos:
    $$b - \beta = G'b^* - G' \beta^* = G'(b^* - \beta^*).$$

II.  Multiplicamos ambos os lados da equaÃ§Ã£o por $Y_T$:
     $$Y_T(b - \beta) = Y_T G'(b^* - \beta^*)$$

III.  Dado que $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2[Q^*]^{-1})$, podemos aplicar uma transformaÃ§Ã£o linear para obter a distribuiÃ§Ã£o assintÃ³tica de $Y_T(b - \beta)$. Ao multiplicar por $G'$ Ã  esquerda, a distribuiÃ§Ã£o assintÃ³tica Ã© transformada, usando propriedades de transformaÃ§Ãµes lineares de variÃ¡veis aleatÃ³rias normais, que preservam a normalidade:
     $$Y_T(b - \beta) \xrightarrow{d} N(0, G' \sigma^2[Q^*]^{-1} (G')^T).$$
     
IV.  Assim, a distribuiÃ§Ã£o assintÃ³tica do estimador $b$ do modelo original Ã© dada por:
    $$Y_T(b - \beta) \xrightarrow{d} N(0, \sigma^2 G' [Q^*]^{-1} (G')^T )$$
â– 

> ğŸ’¡ **Exemplo NumÃ©rico:**  Continuando com o exemplo anterior, se tivermos:
>
> $G' = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -1 & -1 & 1 \end{bmatrix}$
>
>e a matriz $[Q^*]^{-1}$ estimada anteriormente, podemos calcular:
>
>$G' [Q^*]^{-1} (G')^T =  \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -1 & -1 & 1 \end{bmatrix} \begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ 0.05 & 0.2 & 1.8 \end{bmatrix}  \begin{bmatrix} 1 & 0 & -1 \\ 0 & 1 & -1 \\ 0 & 0 & 1 \end{bmatrix} $
>
>Realizando as multiplicaÃ§Ãµes matriciais:
>
>$\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -1 & -1 & 1 \end{bmatrix} \begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ 0.05 & 0.2 & 1.8 \end{bmatrix} = \begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ -1.2-0.1+0.05 & -0.1-1.5+0.2 & -0.05-0.2+1.8 \end{bmatrix} =  \begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ -1.25 & -1.4 & 1.55 \end{bmatrix}$
>
>$\begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ -1.25 & -1.4 & 1.55 \end{bmatrix} \begin{bmatrix} 1 & 0 & -1 \\ 0 & 1 & -1 \\ 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 1.2 & 0.1 & -1.2-0.1+0.05 \\ 0.1 & 1.5 & -0.1-1.5+0.2 \\ -1.25 & -1.4 & 1.25+1.4+1.55 \end{bmatrix} = \begin{bmatrix} 1.2 & 0.1 & -1.25 \\ 0.1 & 1.5 & -1.4 \\ -1.25 & -1.4 & 4.2\end{bmatrix}$
>
>EntÃ£o, a matriz de covariÃ¢ncia assintÃ³tica de $Y_T(b - \beta)$ Ã©:
>
>$ \sigma^2 G' [Q^*]^{-1} (G')^T = \begin{bmatrix} 1.2 & 0.1 & -1.25 \\ 0.1 & 1.5 & -1.4 \\ -1.25 & -1.4 & 4.2\end{bmatrix}$
>
>Com esta matriz, podemos construir intervalos de confianÃ§a para os estimadores dos parÃ¢metros do modelo original ($b$), levando em consideraÃ§Ã£o as diferentes taxas de convergÃªncia.
>
>Por exemplo, se $\sigma^2$ for estimado como 0.8, a matriz de covariÃ¢ncia assintÃ³tica serÃ¡:
>
>$ 0.8  \begin{bmatrix} 1.2 & 0.1 & -1.25 \\ 0.1 & 1.5 & -1.4 \\ -1 & -1.12 & 3.36\end{bmatrix} = \begin{bmatrix} 0.96 & 0.08 & -1 \\ 0.08 & 1.2 & -1.12 \\ -1 & -1.12 & 3.36\end{bmatrix}$
>
> Usando a raiz quadrada dos elementos diagonais e multiplicando pela taxa de convergÃªncia $\frac{1}{\sqrt{T}}$ para o coeficiente autoregressivo, e por $\frac{1}{T^{3/2}}$ para o coeficiente da tendÃªncia, podemos calcular os erros padrÃ£o assintÃ³ticos. Se $T=100$, temos que a variÃ¢ncia assintÃ³tica do estimador do coeficiente autoregressivo Ã© aproximadamente $\frac{0.96}{100} = 0.0096$, e o erro padrÃ£o Ã© $\sqrt{0.0096} \approx 0.098$. Para o coeficiente da tendÃªncia, a variÃ¢ncia assintÃ³tica Ã© aproximadamente $\frac{3.36}{100^3} = 0.00000336$ e o erro padrÃ£o Ã© $\sqrt{0.00000336} \approx 0.00183$.

**Lema 1.1** (ConvergÃªncia da Matriz de CovariÃ¢ncia Amostral para a Matriz de CovariÃ¢ncia AssintÃ³tica)
    Sob condiÃ§Ãµes de regularidade, a matriz de covariÃ¢ncia amostral dos estimadores OLS, dada por $\hat{\Sigma}_b = \hat{\sigma}^2 \left( \sum_{t=1}^T x_t x_t' \right)^{-1}$, converge em probabilidade para a matriz de covariÃ¢ncia assintÃ³tica, $\sigma^2 G' [Q^*]^{-1} (G')^T$, apÃ³s ser devidamente ajustada pela matriz diagonal de taxas de convergÃªncia $Y_T^{-1}$ . Formalmente:
    $$ Y_T \hat{\Sigma}_b Y_T \xrightarrow{p} \sigma^2 G' [Q^*]^{-}^{-1} (G')^T $$
*Prova:*
I. ComeÃ§amos com a definiÃ§Ã£o da matriz de covariÃ¢ncia amostral dos estimadores OLS:
$$\hat{\Sigma}_b = \hat{\sigma}^2 \left( \sum_{t=1}^T x_t x_t' \right)^{-1} $$

II. Multiplicamos e dividimos a matriz de covariÃ¢ncia amostral pela quantidade $T$:
   $$\hat{\Sigma}_b = \hat{\sigma}^2 \left( T \left( \frac{1}{T}\sum_{t=1}^T x_t x_t' \right) \right)^{-1} = \hat{\sigma}^2 \frac{1}{T} \left( \frac{1}{T}\sum_{t=1}^T x_t x_t' \right)^{-1} $$

III. Agora multiplicamos a matriz de covariÃ¢ncia amostral Ã  esquerda e Ã  direita por $Y_T$:
    $$Y_T \hat{\Sigma}_b Y_T = Y_T \hat{\sigma}^2 \frac{1}{T} \left( \frac{1}{T}\sum_{t=1}^T x_t x_t' \right)^{-1} Y_T$$

IV. Utilizando a propriedade que $Y_T^{-1}Y_T = I$ (matriz identidade),  inserimos $Y_T^{-1}Y_T$ dentro da matriz inversa:
   $$Y_T \hat{\Sigma}_b Y_T = \hat{\sigma}^2 Y_T \frac{1}{T}  \left(  \frac{1}{T} \sum_{t=1}^T x_t x_t'  Y_T^{-1}Y_T  \right)^{-1}  Y_T = \hat{\sigma}^2 Y_T \frac{1}{T}  \left( Y_T^{-1} \left( \frac{1}{T} \sum_{t=1}^T Y_T x_t x_t' Y_T \right) Y_T^{-1} \right)^{-1} Y_T $$

V.  Sabemos que $x_t^* = G x_t$, entÃ£o podemos reescrever a expressÃ£o:
     $$Y_T \hat{\Sigma}_b Y_T = \hat{\sigma}^2 Y_T \frac{1}{T}  \left( Y_T^{-1} \left( \frac{1}{T} \sum_{t=1}^T  (G')^{-1} x_t^* x_t^{*'} (G)^{-1} Y_T \right) Y_T^{-1} \right)^{-1} Y_T $$
     
VI. A expressÃ£o $Y_T^{-1}\left(\frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'}\right)Y_T^{-1}$ converge em probabilidade para $Q^*$ [^1] e $\hat{\sigma}^2 \xrightarrow{p} \sigma^2$. A matriz $G'$ e $(G')^{-1}$ sÃ£o constantes. Portanto,
$$Y_T \hat{\Sigma}_b Y_T \xrightarrow{p}  \sigma^2  G' [Q^*]^{-1} (G')^T.$$
â– 

**CorolÃ¡rio 1** (Teste de HipÃ³tese sobre os ParÃ¢metros do Modelo Original)
    Para testar hipÃ³teses lineares sobre os parÃ¢metros do modelo original, tais como $R\beta = r$, onde $R$ Ã© uma matriz de restriÃ§Ãµes e $r$ Ã© um vetor de constantes, a estatÃ­stica de Wald, dada por:
     $$ W = (Rb - r)' (R \hat{\Sigma}_b R')^{-1} (Rb - r) $$
     Ã© assintoticamente distribuÃ­da como uma $\chi^2$ com *k* graus de liberdade, onde *k* Ã© o nÃºmero de restriÃ§Ãµes. Mais formalmente:
      $$ W \xrightarrow{d} \chi^2(k) $$
*Prova:*
I. Definimos a estatÃ­stica de Wald como:
$$W = (Rb - r)' (R \hat{\Sigma}_b R')^{-1} (Rb - r)$$

II. Sabemos que $Y_T(b-\beta) \xrightarrow{d} N(0, \sigma^2 G' [Q^*]^{-1} (G')^T)$, entÃ£o podemos escrever:
$$ Y_T (Rb - R\beta) \xrightarrow{d} N(0, R \sigma^2 G' [Q^*]^{-1} (G')^T R') $$

III. Sob a hipÃ³tese nula $R\beta=r$, temos $Y_T(Rb - r) \xrightarrow{d} N(0, R \sigma^2 G' [Q^*]^{-1} (G')^T R')$.

IV.  Pelo Lema 1.1, sabemos que $ Y_T \hat{\Sigma}_b Y_T \xrightarrow{p} \sigma^2 G' [Q^*]^{-1} (G')^T$, entÃ£o podemos escrever:
$$ Y_T (R\hat{\Sigma}_b R') Y_T \xrightarrow{p} R\sigma^2 G' [Q^*]^{-1} (G')^T R' $$

V. Usando o resultado acima, podemos escrever a estatÃ­stica de Wald como:
$$W = (Y_T(Rb - r))' (Y_T R \hat{\Sigma}_b R' Y_T)^{-1} (Y_T(Rb - r))$$

VI.  Como $Y_T(Rb - r)$ Ã© assintoticamente normal, e usando o resultado do Lema 1.1, a estatÃ­stica de Wald $W$ segue uma distribuiÃ§Ã£o $\chi^2$ com *k* graus de liberdade sob a hipÃ³tese nula:
$$ W \xrightarrow{d} \chi^2(k) $$
â– 
> ğŸ’¡ **Exemplo NumÃ©rico:**
> Suponha que temos um modelo AR(1) com tendÃªncia e queremos testar a hipÃ³tese nula de que o coeficiente da tendÃªncia ($\delta$) Ã© igual a zero. EntÃ£o, $R = \begin{bmatrix} 0 & 0 & 1 \end{bmatrix}$ e $r = [0]$.
>
> Se estimamos $b$ como $b = \begin{bmatrix} 0.7 \\ 2.1 \\ 0.02 \end{bmatrix}$,  $\hat{\Sigma}_b = \begin{bmatrix} 0.01 & 0.005 & 0.0001 \\ 0.005 & 0.02 & 0.0002 \\ 0.0001 & 0.0002 & 0.000004 \end{bmatrix}$
>
>EntÃ£o, $Rb - r = \begin{bmatrix} 0 & 0 & 1 \end{bmatrix}\begin{bmatrix} 0.7 \\ 2.1 \\ 0.02 \end{bmatrix} - 0 = 0.02$
>
> $R\hat{\Sigma}_b R' = \begin{bmatrix} 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 0.01 & 0.005 & 0.0001 \\ 0.005 & 0.02 & 0.0002 \\ 0.0001 & 0.0002 & 0.000004 \end{bmatrix} \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} = 0.000004 $
>
> A estatÃ­stica de Wald Ã©:
>
> $W = (0.02) (0.000004)^{-1} (0.02) = \frac{0.0004}{0.000004} = 100$
>
> Como temos uma restriÃ§Ã£o, a estatÃ­stica de Wald segue uma distribuiÃ§Ã£o $\chi^2$ com 1 grau de liberdade. O valor crÃ­tico da $\chi^2$ com 1 grau de liberdade e nÃ­vel de significÃ¢ncia de 5% Ã© 3.84. Como $W = 100 > 3.84$, rejeitamos a hipÃ³tese nula de que o coeficiente da tendÃªncia Ã© igual a zero.

### ConclusÃ£o
Em resumo, a metodologia de transformaÃ§Ã£o de regressores proposta por Sims, Stock e Watson, aplicada a um modelo autoregressivo em torno de uma tendÃªncia temporal determinÃ­stica, permite isolar as componentes com diferentes taxas de convergÃªncia [^1]. Ao transformar o modelo para uma forma que inclui variÃ¡veis estacionÃ¡rias, uma constante e uma tendÃªncia temporal, podemos derivar a distribuiÃ§Ã£o assintÃ³tica dos estimadores de OLS, conforme visto em [16.3.13]. Esta abordagem facilita a anÃ¡lise estatÃ­stica e a realizaÃ§Ã£o de testes de hipÃ³teses em modelos com raÃ­zes unitÃ¡rias e tendÃªncias determinÃ­sticas [^1].

Como explicitado em [16.3.13], os coeficientes das variÃ¡veis estacionÃ¡rias convergem para uma distribuiÃ§Ã£o Gaussiana a uma taxa de $\sqrt{T}$, enquanto os coeficientes da tendÃªncia temporal convergem para uma distribuiÃ§Ã£o Gaussiana a uma taxa de $T^{3/2}$ [^1]. Este ponto Ã© fundamental para realizar inferÃªncias estatÃ­sticas vÃ¡lidas [^1]. A estatÃ­stica de Wald para testes de hipÃ³teses sobre os parÃ¢metros do modelo AR com tendÃªncia temporal, conforme demonstrado em [16.3.20] e [16.3.21], Ã© assintoticamente vÃ¡lida, mesmo que nÃ£o se aplique a transformaÃ§Ã£o na prÃ¡tica, demonstrando a robustez da metodologia [^1]. Esta equivalÃªncia na distribuiÃ§Ã£o assintÃ³tica entre o modelo transformado e o modelo original, permite a construÃ§Ã£o de testes de hipÃ³teses vÃ¡lidos sem a necessidade de estimar a regressÃ£o transformada na prÃ¡tica [^1].

### ReferÃªncias
[^1]: Texto original fornecido.
<!-- END -->
