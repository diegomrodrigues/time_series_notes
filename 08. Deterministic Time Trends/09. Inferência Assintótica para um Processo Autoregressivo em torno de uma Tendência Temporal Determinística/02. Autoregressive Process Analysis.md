## InferÃªncia AssintÃ³tica para um Processo Autoregressivo em torno de uma TendÃªncia Temporal DeterminÃ­stica

### IntroduÃ§Ã£o
Este capÃ­tulo visa explorar a inferÃªncia estatÃ­stica para processos univariados que contÃªm uma raiz unitÃ¡ria, com foco especÃ­fico em processos autoregressivos em torno de uma tendÃªncia temporal determinÃ­stica [^1]. Como vimos anteriormente no capÃ­tulo 16, a anÃ¡lise de modelos de sÃ©ries temporais com tendÃªncias determinÃ­sticas requer uma abordagem diferenciada para a obtenÃ§Ã£o de distribuiÃ§Ãµes assintÃ³ticas dos estimadores, devido Ã s diferentes taxas de convergÃªncia dos parÃ¢metros envolvidos [^1]. O modelo autoregressivo com tendÃªncia temporal determinÃ­stica, expresso como $y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t$, onde $\epsilon_t$ representa um ruÃ­do branco, apresenta um desafio particular nesse contexto [^1]. Este capÃ­tulo irÃ¡ aprofundar a metodologia de Sims, Stock e Watson, que se baseia na transformaÃ§Ã£o dos regressores para isolar componentes estacionÃ¡rios, uma constante e uma tendÃªncia temporal.

### Conceitos Fundamentais
Em continuidade ao desenvolvimento de processos com tendÃªncias determinÃ­sticas, expandimos nossa anÃ¡lise para um processo autoregressivo (AR) de ordem *p* com uma tendÃªncia temporal determinÃ­stica, dado por [^1]:
$$ y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t, $$
onde $\epsilon_t$ Ã© um processo de ruÃ­do branco com mÃ©dia zero, variÃ¢ncia $\sigma^2$, e um quarto momento finito. Assume-se tambÃ©m que as raÃ­zes do polinÃ´mio $1 - \phi_1z - \phi_2z^2 - \ldots - \phi_pz^p = 0$ estÃ£o fora do cÃ­rculo unitÃ¡rio [^1].

**ObservaÃ§Ã£o 1** A condiÃ§Ã£o de que as raÃ­zes do polinÃ´mio caracterÃ­stico estejam fora do cÃ­rculo unitÃ¡rio Ã© crucial para garantir a estacionariedade do processo autoregressivo em torno da tendÃªncia determinÃ­stica, quando considerada sem a tendÃªncia. Isso assegura que as flutuaÃ§Ãµes em torno da tendÃªncia nÃ£o cresÃ§am indefinidamente ao longo do tempo.

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha um processo AR(1) com tendÃªncia: $y_t = 2 + 0.5t + 0.7y_{t-1} + \epsilon_t$. O polinÃ´mio caracterÃ­stico Ã© $1 - 0.7z = 0$, que tem uma raiz em $z = 1/0.7 \approx 1.43$, que estÃ¡ fora do cÃ­rculo unitÃ¡rio. Isso garante que o processo, sem a tendÃªncia ($y_t = 0.7y_{t-1} + \epsilon_t$), seria estacionÃ¡rio. Se a raiz fosse dentro do cÃ­rculo unitÃ¡rio, por exemplo, se o coeficiente fosse 1, a sÃ©rie teria uma raiz unitÃ¡ria e nÃ£o seria estacionÃ¡ria.

A metodologia de Sims, Stock e Watson envolve uma transformaÃ§Ã£o dos regressores do modelo original [^1]. O objetivo Ã© reescrever o modelo [16.3.1] em termos de variÃ¡veis aleatÃ³rias estacionÃ¡rias com mÃ©dia zero, uma constante e uma tendÃªncia temporal [^1]. Esta transformaÃ§Ã£o permite isolar os componentes do vetor de coeficientes OLS com diferentes taxas de convergÃªncia [^1]. Como visto em [16.3.2], somando e subtraindo $\phi_j [\alpha + \delta(t-j)]$ para $j = 1, 2, ..., p$ no lado direito do modelo, o modelo de regressÃ£o pode ser reescrito de forma equivalente como:
$$ y_t = \alpha (1 + \phi_1 + \phi_2 + \ldots + \phi_p) + \delta(1 + \phi_1 + 2\phi_2 + \ldots + p\phi_p) + \delta(\phi_1 + 2\phi_2 + \ldots + p\phi_p) + \phi_1[y_{t-1} - \alpha - \delta(t-1)] + \phi_2[y_{t-2} - \alpha - \delta(t-2)] + \ldots + \phi_p[y_{t-p} - \alpha - \delta(t-p)] + \epsilon_t. $$
Essa formulaÃ§Ã£o pode ser simplificada para [^1]:
$$ y_t = \alpha^* + \delta^*t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \ldots + \phi_p^* y_{t-p}^* + \epsilon_t, $$
onde:
- $\alpha^* = \alpha (1 + \phi_1 + \phi_2 + \ldots + \phi_p) - \delta(\phi_1 + 2\phi_2 + \ldots + p\phi_p)$
- $\delta^* = \delta (1 + \phi_1 + \phi_2 + \ldots + \phi_p)$
- $\phi_j^* = \phi_j$ para $j=1,2,\ldots,p$
- $y_{t-j}^* = y_{t-j} - \alpha - \delta(t-j)$ para $j=1,2,\ldots,p$

A transformaÃ§Ã£o dos regressores em [16.3.4], dada por $y_{t-j}^* = y_{t-j} - \alpha - \delta(t-j)$, Ã© crucial para a anÃ¡lise assintÃ³tica do modelo [^1]. Esta transformaÃ§Ã£o isola os componentes do modelo com diferentes taxas de convergÃªncia, facilitando a anÃ¡lise assintÃ³tica da regressÃ£o [^1].

> ðŸ’¡ **Exemplo NumÃ©rico:** Continuando com o exemplo AR(1): $y_t = 2 + 0.5t + 0.7y_{t-1} + \epsilon_t$, temos $\alpha = 2$, $\delta = 0.5$, e $\phi_1 = 0.7$.
>  - $\alpha^* = 2(1+0.7) - 0.5(0.7) = 2(1.7) - 0.35 = 3.4 - 0.35 = 3.05$
>  - $\delta^* = 0.5(1+0.7) = 0.5(1.7) = 0.85$
>  - $\phi_1^* = 0.7$
>  - $y_{t-1}^* = y_{t-1} - 2 - 0.5(t-1)$.
>   O modelo transformado Ã© entÃ£o $y_t = 3.05 + 0.85t + 0.7y_{t-1}^* + \epsilon_t$.

Em notaÃ§Ã£o matricial, o modelo original [16.3.1] pode ser escrito como $y_t = x_t' \beta + \epsilon_t$ [^1], onde:

$$ x_t = \begin{bmatrix} y_{t-1} \\ y_{t-2} \\ \vdots \\ y_{t-p} \\ 1 \\ t \end{bmatrix}  \quad  \beta = \begin{bmatrix} \phi_1 \\ \phi_2 \\ \vdots \\ \phi_p \\ \alpha \\ \delta \end{bmatrix} $$

A transformaÃ§Ã£o algÃ©brica do modelo [16.3.5] para [16.3.3] pode ser descrita como uma reescrita na forma $y_t = x_t' G'(G')^{-1}\beta + \epsilon_t = x_t^{*'} \beta^* + \epsilon_t$ [^1], onde $x_t^* = G x_t$ e $\beta^* = (G')^{-1}\beta$. As matrizes $G'$ e $(G')^{-1}$ sÃ£o definidas em [16.3.8] e [16.3.9], respectivamente [^1].

O estimador de $\beta^*$ por OLS, denotado por $b^*$, Ã© dado por [16.3.11] como:
$$ b^* = \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* y_t = [G']^{-1} \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_t = [G']^{-1}b $$

Esta formulaÃ§Ã£o destaca que o estimador para a regressÃ£o transformada, $b^*$, Ã© uma transformaÃ§Ã£o linear do estimador do sistema original, $b$ [^1]. Os valores ajustados sÃ£o idÃªnticos nos dois modelos: $x_t^{*'} b^* = x_t' b$ [^1]. Embora a regressÃ£o transformada nÃ£o seja diretamente estimÃ¡vel pelos dados, a anÃ¡lise das propriedades da estimativa de OLS para o modelo transformado Ã© mais acessÃ­vel, e a distribuiÃ§Ã£o assintÃ³tica de $b$ pode ser obtida invertendo a transformaÃ§Ã£o [16.3.12]: $b = G'b^*$ [^1].

**Lema 1** A equivalÃªncia dos valores ajustados entre os modelos original e transformado, i.e., $x_t^{*'} b^* = x_t' b$, Ã© uma consequÃªncia direta da propriedade de invariÃ¢ncia dos ajustes por transformaÃ§Ãµes lineares na regressÃ£o. Esta propriedade garante que a transformaÃ§Ã£o utilizada nÃ£o afeta a capacidade preditiva do modelo, apenas reparametriza seus coeficientes.

O ApÃªndice 16.A demonstra que a distribuiÃ§Ã£o assintÃ³tica do estimador da regressÃ£o transformada Ã© dada por:

$$ Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2[Q^*]^{-1}) $$

onde $Y_T$ Ã© uma matriz diagonal que ajusta a taxa de convergÃªncia de cada coeficiente, conforme [16.3.14] [^1]. Especificamente, os coeficientes relacionados Ã s variÃ¡veis de mÃ©dia zero e covariÃ¢ncia estacionÃ¡ria convergem a uma taxa $\sqrt{T}$, enquanto o coeficiente relacionado Ã  tendÃªncia temporal converge a uma taxa $T^{3/2}$. A matriz $Q^*$ Ã© definida em [16.3.15], que captura as interrelaÃ§Ãµes entre os regressores transformados [^1].

> ðŸ’¡ **Exemplo NumÃ©rico:**  Suponha que, para um modelo AR(1) com tendÃªncia, tenhamos apÃ³s a transformaÃ§Ã£o que:
>
> $Y_T = \begin{bmatrix} \sqrt{T} & 0 & 0 \\ 0 & \sqrt{T} & 0 \\ 0 & 0 & T^{3/2} \end{bmatrix}$
>
>
> e que a matriz $[Q^*]^{-1}$ seja estimada como:
>
> $[Q^*]^{-1} = \begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ 0.05 & 0.2 & 1.8 \end{bmatrix}$
>
>Se $\sigma^2 = 1$, entÃ£o a matriz de covariÃ¢ncia assintÃ³tica de $Y_T(b^* - \beta^*)$ seria:
>
>$\sigma^2[Q^*]^{-1} = \begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ 0.05 & 0.2 & 1.8 \end{bmatrix}$
>
> Isso indica, por exemplo, que a variÃ¢ncia assintÃ³tica do estimador do coeficiente autoregressivo ($b_1^*$) Ã© 1.2 quando multiplicada pela taxa de convergÃªncia de $\sqrt{T}$, o que significa que a variÃ¢ncia do estimador original ($b_1^*$) Ã© da ordem de $\frac{1.2}{T}$. O coeficiente da tendÃªncia ($b_3^*$) por sua vez tem uma variÃ¢ncia assintÃ³tica de 1.8 quando multiplicada pela taxa de convergÃªncia de $T^{3/2}$, o que significa que sua variÃ¢ncia original ($b_3^*$) Ã© da ordem de $\frac{1.8}{T^3}$ que converge para zero muito mais rapidamente do que a variÃ¢ncia de $b_1^*$.

**Teorema 1** (DistribuiÃ§Ã£o AssintÃ³tica do Estimador do Modelo Original)
Dado que $b = G'b^*$ e conhecendo a distribuiÃ§Ã£o assintÃ³tica de $b^*$, a distribuiÃ§Ã£o assintÃ³tica do estimador $b$ do modelo original Ã© dada por:
$$Y_T (b - \beta) \xrightarrow{d} N(0, \sigma^2 G' [Q^*]^{-1} (G')^T ) $$
*Prova:*
Vamos demonstrar o Teorema 1:

I.  ComeÃ§amos com a relaÃ§Ã£o entre os estimadores do modelo original e transformado: $b = G'b^*$. Subtraindo $\beta$ de ambos os lados, e usando o fato de que $\beta = G' \beta^*$, obtemos:
    $$b - \beta = G'b^* - G' \beta^* = G'(b^* - \beta^*).$$

II.  Multiplicamos ambos os lados da equaÃ§Ã£o por $Y_T$:
     $$Y_T(b - \beta) = Y_T G'(b^* - \beta^*)$$

III.  Dado que $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2[Q^*]^{-1})$, podemos aplicar uma transformaÃ§Ã£o linear para obter a distribuiÃ§Ã£o assintÃ³tica de $Y_T(b - \beta)$. Ao multiplicar por $G'$ Ã  esquerda, a distribuiÃ§Ã£o assintÃ³tica Ã© transformada, usando propriedades de transformaÃ§Ãµes lineares de variÃ¡veis aleatÃ³rias normais, que preservam a normalidade:
     $$Y_T(b - \beta) \xrightarrow{d} N(0, G' \sigma^2[Q^*]^{-1} (G')^T).$$
     
IV.  Assim, a distribuiÃ§Ã£o assintÃ³tica do estimador $b$ do modelo original Ã© dada por:
    $$Y_T(b - \beta) \xrightarrow{d} N(0, \sigma^2 G' [Q^*]^{-1} (G')^T )$$
â– 

> ðŸ’¡ **Exemplo NumÃ©rico:**  Continuando com o exemplo anterior, se tivermos:
>
> $G' = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -1 & -1 & 1 \end{bmatrix}$
>
>e a matriz $[Q^*]^{-1}$ estimada anteriormente, podemos calcular:
>
>$G' [Q^*]^{-1} (G')^T =  \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -1 & -1 & 1 \end{bmatrix} \begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ 0.05 & 0.2 & 1.8 \end{bmatrix}  \begin{bmatrix} 1 & 0 & -1 \\ 0 & 1 & -1 \\ 0 & 0 & 1 \end{bmatrix} $
>
>Realizando as multiplicaÃ§Ãµes matriciais:
>
>$\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -1 & -1 & 1 \end{bmatrix} \begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ 0.05 & 0.2 & 1.8 \end{bmatrix} = \begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ -1.2-0.1+0.05 & -0.1-1.5+0.2 & -0.05-0.2+1.8 \end{bmatrix} =  \begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ -1.25 & -1.4 & 1.55 \end{bmatrix}$
>
>$\begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ -1.25 & -1.4 & 1.55 \end{bmatrix} \begin{bmatrix} 1 & 0 & -1 \\ 0 & 1 & -1 \\ 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 1.2 & 0.1 & -1.2-0.1+0.05 \\ 0.1 & 1.5 & -0.1-1.5+0.2 \\ -1.25 & -1.4 & 1.25+1.4+1.55 \end{bmatrix} = \begin{bmatrix} 1.2 & 0.1 & -1.25 \\ 0.1 & 1.5 & -1.4 \\ -1.25 & -1.4 & 4.2\end{bmatrix}$
>
>EntÃ£o, a matriz de covariÃ¢ncia assintÃ³tica de $Y_T(b - \beta)$ Ã©:
>
>$ \sigma^2 G' [Q^*]^{-1} (G')^T = \begin{bmatrix} 1.2 & 0.1 & -1.25 \\ 0.1 & 1.5 & -1.4 \\ -1.25 & -1.4 & 4.2\end{bmatrix}$
>
>Com esta matriz, podemos construir intervalos de confianÃ§a para os estimadores dos parÃ¢metros do modelo original ($b$), levando em consideraÃ§Ã£o as diferentes taxas de convergÃªncia.

### ConclusÃ£o
Em resumo, a metodologia de transformaÃ§Ã£o de regressores proposta por Sims, Stock e Watson, aplicada a um modelo autoregressivo em torno de uma tendÃªncia temporal determinÃ­stica, permite isolar as componentes com diferentes taxas de convergÃªncia [^1]. Ao transformar o modelo para uma forma que inclui variÃ¡veis estacionÃ¡rias, uma constante e uma tendÃªncia temporal, podemos derivar a distribuiÃ§Ã£o assintÃ³tica dos estimadores de OLS, conforme visto em [16.3.13]. Esta abordagem facilita a anÃ¡lise estatÃ­stica e a realizaÃ§Ã£o de testes de hipÃ³teses em modelos com raÃ­zes unitÃ¡rias e tendÃªncias determinÃ­sticas [^1].
As taxas de convergÃªncia, conforme demonstrado em [16.3.13], sÃ£o cruciais para compreender o comportamento assintÃ³tico dos estimadores, com coeficientes de variÃ¡veis estacionÃ¡rias convergindo a $\sqrt{T}$, e o coeficiente da tendÃªncia temporal convergindo a $T^{3/2}$ [^1]. Essa distinÃ§Ã£o na convergÃªncia Ã© fundamental para construir testes estatÃ­sticos vÃ¡lidos nesse contexto.

### ReferÃªncias
[^1]: Texto original fornecido.
<!-- END -->
