## Infer√™ncia Assint√≥tica para um Processo Autoregressivo em torno de uma Tend√™ncia Temporal Determin√≠stica

### Introdu√ß√£o
Este cap√≠tulo visa explorar a infer√™ncia estat√≠stica para processos univariados que cont√™m uma raiz unit√°ria, com foco espec√≠fico em processos autoregressivos em torno de uma tend√™ncia temporal determin√≠stica [^1]. Como vimos anteriormente no cap√≠tulo 16, a an√°lise de modelos de s√©ries temporais com tend√™ncias determin√≠sticas requer uma abordagem diferenciada para a obten√ß√£o de distribui√ß√µes assint√≥ticas dos estimadores, devido √†s diferentes taxas de converg√™ncia dos par√¢metros envolvidos [^1]. O modelo autoregressivo com tend√™ncia temporal determin√≠stica, expresso como $y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t$, onde $\epsilon_t$ representa um ru√≠do branco, apresenta um desafio particular nesse contexto [^1]. Este cap√≠tulo ir√° aprofundar a metodologia de Sims, Stock e Watson, que se baseia na transforma√ß√£o dos regressores para isolar componentes estacion√°rios, uma constante e uma tend√™ncia temporal.

### Conceitos Fundamentais
Em continuidade ao desenvolvimento de processos com tend√™ncias determin√≠sticas, expandimos nossa an√°lise para um processo autoregressivo (AR) de ordem *p* com uma tend√™ncia temporal determin√≠stica, dado por [^1]:
$$ y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t, $$
onde $\epsilon_t$ √© um processo de ru√≠do branco com m√©dia zero, vari√¢ncia $\sigma^2$, e um quarto momento finito. Assume-se tamb√©m que as ra√≠zes do polin√¥mio $1 - \phi_1z - \phi_2z^2 - \ldots - \phi_pz^p = 0$ est√£o fora do c√≠rculo unit√°rio [^1].

**Observa√ß√£o 1** A condi√ß√£o de que as ra√≠zes do polin√¥mio caracter√≠stico estejam fora do c√≠rculo unit√°rio √© crucial para garantir a estacionariedade do processo autoregressivo em torno da tend√™ncia determin√≠stica, quando considerada sem a tend√™ncia. Isso assegura que as flutua√ß√µes em torno da tend√™ncia n√£o cres√ßam indefinidamente ao longo do tempo.

> üí° **Exemplo Num√©rico:** Suponha um processo AR(1) com tend√™ncia: $y_t = 2 + 0.5t + 0.7y_{t-1} + \epsilon_t$. O polin√¥mio caracter√≠stico √© $1 - 0.7z = 0$, que tem uma raiz em $z = 1/0.7 \approx 1.43$, que est√° fora do c√≠rculo unit√°rio. Isso garante que o processo, sem a tend√™ncia ($y_t = 0.7y_{t-1} + \epsilon_t$), seria estacion√°rio. Se a raiz fosse dentro do c√≠rculo unit√°rio, por exemplo, se o coeficiente fosse 1, a s√©rie teria uma raiz unit√°ria e n√£o seria estacion√°ria.

A metodologia de Sims, Stock e Watson envolve uma transforma√ß√£o dos regressores do modelo original [^1]. O objetivo √© reescrever o modelo [16.3.1] em termos de vari√°veis aleat√≥rias estacion√°rias com m√©dia zero, uma constante e uma tend√™ncia temporal [^1]. Esta transforma√ß√£o permite isolar os componentes do vetor de coeficientes OLS com diferentes taxas de converg√™ncia [^1]. Como visto em [16.3.2], somando e subtraindo $\phi_j [\alpha + \delta(t-j)]$ para $j = 1, 2, ..., p$ no lado direito do modelo, o modelo de regress√£o pode ser reescrito de forma equivalente como:
$$ y_t = \alpha (1 + \phi_1 + \phi_2 + \ldots + \phi_p) + \delta(1 + \phi_1 + 2\phi_2 + \ldots + p\phi_p) + \delta(\phi_1 + 2\phi_2 + \ldots + p\phi_p) + \phi_1[y_{t-1} - \alpha - \delta(t-1)] + \phi_2[y_{t-2} - \alpha - \delta(t-2)] + \ldots + \phi_p[y_{t-p} - \alpha - \delta(t-p)] + \epsilon_t. $$
Essa formula√ß√£o pode ser simplificada para [^1]:
$$ y_t = \alpha^* + \delta^*t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \ldots + \phi_p^* y_{t-p}^* + \epsilon_t, $$
onde:
- $\alpha^* = \alpha (1 + \phi_1 + \phi_2 + \ldots + \phi_p) - \delta(\phi_1 + 2\phi_2 + \ldots + p\phi_p)$
- $\delta^* = \delta (1 + \phi_1 + \phi_2 + \ldots + \phi_p)$
- $\phi_j^* = \phi_j$ para $j=1,2,\ldots,p$
- $y_{t-j}^* = y_{t-j} - \alpha - \delta(t-j)$ para $j=1,2,\ldots,p$

A transforma√ß√£o dos regressores em [16.3.4], dada por $y_{t-j}^* = y_{t-j} - \alpha - \delta(t-j)$, √© crucial para a an√°lise assint√≥tica do modelo [^1]. Esta transforma√ß√£o isola os componentes do modelo com diferentes taxas de converg√™ncia, facilitando a an√°lise assint√≥tica da regress√£o [^1].

> üí° **Exemplo Num√©rico:** Continuando com o exemplo AR(1): $y_t = 2 + 0.5t + 0.7y_{t-1} + \epsilon_t$, temos $\alpha = 2$, $\delta = 0.5$, e $\phi_1 = 0.7$.
>  - $\alpha^* = 2(1+0.7) - 0.5(0.7) = 2(1.7) - 0.35 = 3.4 - 0.35 = 3.05$
>  - $\delta^* = 0.5(1+0.7) = 0.5(1.7) = 0.85$
>  - $\phi_1^* = 0.7$
>  - $y_{t-1}^* = y_{t-1} - 2 - 0.5(t-1)$.
>   O modelo transformado √© ent√£o $y_t = 3.05 + 0.85t + 0.7y_{t-1}^* + \epsilon_t$.
>
> Vamos ilustrar com alguns dados simulados, onde $\epsilon_t \sim N(0, 1)$. Suponha que temos $y_0 = 5$. Calculamos $y_1 = 2 + 0.5(1) + 0.7(5) + \epsilon_1 = 2 + 0.5 + 3.5 + \epsilon_1 = 6 + \epsilon_1$. Se $\epsilon_1 = 0.2$, ent√£o $y_1 = 6.2$. Da mesma forma, $y_2 = 2 + 0.5(2) + 0.7(6.2) + \epsilon_2 = 2 + 1 + 4.34 + \epsilon_2 = 7.34 + \epsilon_2$. Se $\epsilon_2 = -0.5$, ent√£o $y_2 = 6.84$. O valor de $y_1^*$ seria $y_1^* = y_1 - 2 - 0.5(1-1) = 6.2 - 2 = 4.2$. O valor de $y_2^*$ seria $y_2^* = y_2 - 2 - 0.5(2-1) = 6.84 - 2 - 0.5 = 4.34$.

Em nota√ß√£o matricial, o modelo original [16.3.1] pode ser escrito como $y_t = x_t' \beta + \epsilon_t$ [^1], onde:

$$ x_t = \begin{bmatrix} y_{t-1} \\ y_{t-2} \\ \vdots \\ y_{t-p} \\ 1 \\ t \end{bmatrix}  \quad  \beta = \begin{bmatrix} \phi_1 \\ \phi_2 \\ \vdots \\ \phi_p \\ \alpha \\ \delta \end{bmatrix} $$

A transforma√ß√£o alg√©brica do modelo [16.3.5] para [16.3.3] pode ser descrita como uma reescrita na forma $y_t = x_t' G'(G')^{-1}\beta + \epsilon_t = x_t^{*'} \beta^* + \epsilon_t$ [^1], onde $x_t^* = G x_t$ e $\beta^* = (G')^{-1}\beta$. As matrizes $G'$ e $(G')^{-1}$ s√£o definidas em [16.3.8] e [16.3.9], respectivamente [^1].

O estimador de $\beta^*$ por OLS, denotado por $b^*$, √© dado por [16.3.11] como:
$$ b^* = \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* y_t = [G']^{-1} \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_t = [G']^{-1}b $$

Esta formula√ß√£o destaca que o estimador para a regress√£o transformada, $b^*$, √© uma transforma√ß√£o linear do estimador do sistema original, $b$ [^1]. Os valores ajustados s√£o id√™nticos nos dois modelos: $x_t^{*'} b^* = x_t' b$ [^1]. Embora a regress√£o transformada n√£o seja diretamente estim√°vel pelos dados, a an√°lise das propriedades da estimativa de OLS para o modelo transformado √© mais acess√≠vel, e a distribui√ß√£o assint√≥tica de $b$ pode ser obtida invertendo a transforma√ß√£o [16.3.12]: $b = G'b^*$ [^1].

**Lema 1** A equival√™ncia dos valores ajustados entre os modelos original e transformado, i.e., $x_t^{*'} b^* = x_t' b$, √© uma consequ√™ncia direta da propriedade de invari√¢ncia dos ajustes por transforma√ß√µes lineares na regress√£o. Esta propriedade garante que a transforma√ß√£o utilizada n√£o afeta a capacidade preditiva do modelo, apenas reparametriza seus coeficientes.

O Ap√™ndice 16.A demonstra que a distribui√ß√£o assint√≥tica do estimador da regress√£o transformada √© dada por:

$$ Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2[Q^*]^{-1}) $$

onde $Y_T$ √© uma matriz diagonal que ajusta a taxa de converg√™ncia de cada coeficiente, conforme [16.3.14] [^1]. Especificamente, os coeficientes relacionados √†s vari√°veis de m√©dia zero e covari√¢ncia estacion√°ria convergem a uma taxa $\sqrt{T}$, enquanto o coeficiente relacionado √† tend√™ncia temporal converge a uma taxa $T^{3/2}$. A matriz $Q^*$ √© definida em [16.3.15], que captura as interrela√ß√µes entre os regressores transformados [^1].

> üí° **Exemplo Num√©rico:**  Suponha que, para um modelo AR(1) com tend√™ncia, tenhamos ap√≥s a transforma√ß√£o que:
>
> $Y_T = \begin{bmatrix} \sqrt{T} & 0 & 0 \\ 0 & \sqrt{T} & 0 \\ 0 & 0 & T^{3/2} \end{bmatrix}$
>
>
> e que a matriz $[Q^*]^{-1}$ seja estimada como:
>
> $[Q^*]^{-1} = \begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ 0.05 & 0.2 & 1.8 \end{bmatrix}$
>
>Se $\sigma^2 = 1$, ent√£o a matriz de covari√¢ncia assint√≥tica de $Y_T(b^* - \beta^*)$ seria:
>
>$\sigma^2[Q^*]^{-1} = \begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ 0.05 & 0.2 & 1.8 \end{bmatrix}$
>
> Isso indica, por exemplo, que a vari√¢ncia assint√≥tica do estimador do coeficiente autoregressivo ($b_1^*$) √© 1.2 quando multiplicada pela taxa de converg√™ncia de $\sqrt{T}$, o que significa que a vari√¢ncia do estimador original ($b_1^*$) √© da ordem de $\frac{1.2}{T}$. O coeficiente da tend√™ncia ($b_3^*$) por sua vez tem uma vari√¢ncia assint√≥tica de 1.8 quando multiplicada pela taxa de converg√™ncia de $T^{3/2}$, o que significa que sua vari√¢ncia original ($b_3^*$) √© da ordem de $\frac{1.8}{T^3}$ que converge para zero muito mais rapidamente do que a vari√¢ncia de $b_1^*$.
>
> Vamos supor que temos uma amostra de tamanho $T=100$. Ent√£o, $Y_T = \begin{bmatrix} 10 & 0 & 0 \\ 0 & 10 & 0 \\ 0 & 0 & 1000 \end{bmatrix}$. A matriz de covari√¢ncia assint√≥tica de $b^*$ ser√° da ordem de:
>
>  $\frac{1}{T} \begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ 0.05 & 0.2 & 1.8 \end{bmatrix} = \begin{bmatrix} 0.012 & 0.001 & 0.00005 \\ 0.001 & 0.015 & 0.0002 \\ 0.00005 & 0.0002 & 0.0000018 \end{bmatrix}$
>
> Assim, a vari√¢ncia assint√≥tica do estimador do coeficiente autoregressivo ($b_1^*$) seria aproximadamente 0.012, a vari√¢ncia do intercepto seria aproximadamente 0.015, e a vari√¢ncia do estimador do coeficiente da tend√™ncia temporal ($b_3^*$) seria aproximadamente 0.0000018. Isso demonstra como a converg√™ncia √© mais r√°pida para o coeficiente da tend√™ncia.

**Teorema 1** (Distribui√ß√£o Assint√≥tica do Estimador do Modelo Original)
Dado que $b = G'b^*$ e conhecendo a distribui√ß√£o assint√≥tica de $b^*$, a distribui√ß√£o assint√≥tica do estimador $b$ do modelo original √© dada por:
$$Y_T (b - \beta) \xrightarrow{d} N(0, \sigma^2 G' [Q^*]^{-1} (G')^T ) $$
*Prova:*
Vamos demonstrar o Teorema 1:

I.  Come√ßamos com a rela√ß√£o entre os estimadores do modelo original e transformado: $b = G'b^*$. Subtraindo $\beta$ de ambos os lados, e usando o fato de que $\beta = G' \beta^*$, obtemos:
    $$b - \beta = G'b^* - G' \beta^* = G'(b^* - \beta^*).$$

II.  Multiplicamos ambos os lados da equa√ß√£o por $Y_T$:
     $$Y_T(b - \beta) = Y_T G'(b^* - \beta^*)$$

III.  Dado que $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2[Q^*]^{-1})$, podemos aplicar uma transforma√ß√£o linear para obter a distribui√ß√£o assint√≥tica de $Y_T(b - \beta)$. Ao multiplicar por $G'$ √† esquerda, a distribui√ß√£o assint√≥tica √© transformada, usando propriedades de transforma√ß√µes lineares de vari√°veis aleat√≥rias normais, que preservam a normalidade:
     $$Y_T(b - \beta) \xrightarrow{d} N(0, G' \sigma^2[Q^*]^{-1} (G')^T).$$
     
IV.  Assim, a distribui√ß√£o assint√≥tica do estimador $b$ do modelo original √© dada por:
    $$Y_T(b - \beta) \xrightarrow{d} N(0, \sigma^2 G' [Q^*]^{-1} (G')^T )$$
‚ñ†

> üí° **Exemplo Num√©rico:**  Continuando com o exemplo anterior, se tivermos:
>
> $G' = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -1 & -1 & 1 \end{bmatrix}$
>
>e a matriz $[Q^*]^{-1}$ estimada anteriormente, podemos calcular:
>
>$G' [Q^*]^{-1} (G')^T =  \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -1 & -1 & 1 \end{bmatrix} \begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ 0.05 & 0.2 & 1.8 \end{bmatrix}  \begin{bmatrix} 1 & 0 & -1 \\ 0 & 1 & -1 \\ 0 & 0 & 1 \end{bmatrix} $
>
>Realizando as multiplica√ß√µes matriciais:
>
>$\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -1 & -1 & 1 \end{bmatrix} \begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ 0.05 & 0.2 & 1.8 \end{bmatrix} = \begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ -1.2-0.1+0.05 & -0.1-1.5+0.2 & -0.05-0.2+1.8 \end{bmatrix} =  \begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ -1.25 & -1.4 & 1.55 \end{bmatrix}$
>
>$\begin{bmatrix} 1.2 & 0.1 & 0.05 \\ 0.1 & 1.5 & 0.2 \\ -1.25 & -1.4 & 1.55 \end{bmatrix} \begin{bmatrix} 1 & 0 & -1 \\ 0 & 1 & -1 \\ 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 1.2 & 0.1 & -1.2-0.1+0.05 \\ 0.1 & 1.5 & -0.1-1.5+0.2 \\ -1.25 & -1.4 & 1.25+1.4+1.55 \end{bmatrix} = \begin{bmatrix} 1.2 & 0.1 & -1.25 \\ 0.1 & 1.5 & -1.4 \\ -1.25 & -1.4 & 4.2\end{bmatrix}$
>
>Ent√£o, a matriz de covari√¢ncia assint√≥tica de $Y_T(b - \beta)$ √©:
>
>$ \sigma^2 G' [Q^*]^{-1} (G')^T = \begin{bmatrix} 1.2 & 0.1 & -1.25 \\ 0.1 & 1.5 & -1.4 \\ -1.25 & -1.4 & 4.2\end{bmatrix}$
>
>Com esta matriz, podemos construir intervalos de confian√ßa para os estimadores dos par√¢metros do modelo original ($b$), levando em considera√ß√£o as diferentes taxas de converg√™ncia.
>
>Por exemplo, se $\sigma^2$ for estimado como 0.8, a matriz de covari√¢ncia assint√≥tica ser√°:
>
>$ 0.8  \begin{bmatrix} 1.2 & 0.1 & -1.25 \\ 0.1 & 1.5 & -1.4 \\ -1 & -1.12 & 3.36\end{bmatrix} = \begin{bmatrix} 0.96 & 0.08 & -1 \\ 0.08 & 1.2 & -1.12 \\ -1 & -1.12 & 3.36\end{bmatrix}$
>
> Usando a raiz quadrada dos elementos diagonais e multiplicando pela taxa de converg√™ncia $\frac{1}{\sqrt{T}}$ para o coeficiente autoregressivo, e por $\frac{1}{T^{3/2}}$ para o coeficiente da tend√™ncia, podemos calcular os erros padr√£o assint√≥ticos. Se $T=100$, temos que a vari√¢ncia assint√≥tica do estimador do coeficiente autoregressivo √© aproximadamente $\frac{0.96}{100} = 0.0096$, e o erro padr√£o √© $\sqrt{0.0096} \approx 0.098$. Para o coeficiente da tend√™ncia, a vari√¢ncia assint√≥tica √© aproximadamente $\frac{3.36}{100^3} = 0.00000336$ e o erro padr√£o √© $\sqrt{0.00000336} \approx 0.00183$.

**Lema 1.1** (Converg√™ncia da Matriz de Covari√¢ncia Amostral para a Matriz de Covari√¢ncia Assint√≥tica)
    Sob condi√ß√µes de regularidade, a matriz de covari√¢ncia amostral dos estimadores OLS, dada por $\hat{\Sigma}_b = \hat{\sigma}^2 \left( \sum_{t=1}^T x_t x_t' \right)^{-1}$, converge em probabilidade para a matriz de covari√¢ncia assint√≥tica, $\sigma^2 G' [Q^*]^{-1} (G')^T$, ap√≥s ser devidamente ajustada pela matriz diagonal de taxas de converg√™ncia $Y_T^{-1}$ . Formalmente:
    $$ Y_T \hat{\Sigma}_b Y_T \xrightarrow{p} \sigma^2 G' [Q^*]^{-}^{-1} (G')^T $$
*Prova:*
I. Come√ßamos com a defini√ß√£o da matriz de covari√¢ncia amostral dos estimadores OLS:
$$\hat{\Sigma}_b = \hat{\sigma}^2 \left( \sum_{t=1}^T x_t x_t' \right)^{-1} $$

II. Multiplicamos e dividimos a matriz de covari√¢ncia amostral pela quantidade $T$:
   $$\hat{\Sigma}_b = \hat{\sigma}^2 \left( T \left( \frac{1}{T}\sum_{t=1}^T x_t x_t' \right) \right)^{-1} = \hat{\sigma}^2 \frac{1}{T} \left( \frac{1}{T}\sum_{t=1}^T x_t x_t' \right)^{-1} $$

III. Agora multiplicamos a matriz de covari√¢ncia amostral √† esquerda e √† direita por $Y_T$:
    $$Y_T \hat{\Sigma}_b Y_T = Y_T \hat{\sigma}^2 \frac{1}{T} \left( \frac{1}{T}\sum_{t=1}^T x_t x_t' \right)^{-1} Y_T$$

IV. Utilizando a propriedade que $Y_T^{-1}Y_T = I$ (matriz identidade),  inserimos $Y_T^{-1}Y_T$ dentro da matriz inversa:
   $$Y_T \hat{\Sigma}_b Y_T = \hat{\sigma}^2 Y_T \frac{1}{T}  \left(  \frac{1}{T} \sum_{t=1}^T x_t x_t'  Y_T^{-1}Y_T  \right)^{-1}  Y_T = \hat{\sigma}^2 Y_T \frac{1}{T}  \left( Y_T^{-1} \left( \frac{1}{T} \sum_{t=1}^T Y_T x_t x_t' Y_T \right) Y_T^{-1} \right)^{-1} Y_T $$

V.  Sabemos que $x_t^* = G x_t$, ent√£o podemos reescrever a express√£o:
     $$Y_T \hat{\Sigma}_b Y_T = \hat{\sigma}^2 Y_T \frac{1}{T}  \left( Y_T^{-1} \left( \frac{1}{T} \sum_{t=1}^T  (G')^{-1} x_t^* x_t^{*'} (G)^{-1} Y_T \right) Y_T^{-1} \right)^{-1} Y_T $$
     
VI. A express√£o $Y_T^{-1}\left(\frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'}\right)Y_T^{-1}$ converge em probabilidade para $Q^*$ [^1] e $\hat{\sigma}^2 \xrightarrow{p} \sigma^2$. A matriz $G'$ e $(G')^{-1}$ s√£o constantes. Portanto,
$$Y_T \hat{\Sigma}_b Y_T \xrightarrow{p}  \sigma^2  G' [Q^*]^{-1} (G')^T.$$
‚ñ†

**Corol√°rio 1** (Teste de Hip√≥tese sobre os Par√¢metros do Modelo Original)
    Para testar hip√≥teses lineares sobre os par√¢metros do modelo original, tais como $R\beta = r$, onde $R$ √© uma matriz de restri√ß√µes e $r$ √© um vetor de constantes, a estat√≠stica de Wald, dada por:
     $$ W = (Rb - r)' (R \hat{\Sigma}_b R')^{-1} (Rb - r) $$
     √© assintoticamente distribu√≠da como uma $\chi^2$ com *k* graus de liberdade, onde *k* √© o n√∫mero de restri√ß√µes. Mais formalmente:
      $$ W \xrightarrow{d} \chi^2(k) $$
*Prova:*
I. Definimos a estat√≠stica de Wald como:
$$W = (Rb - r)' (R \hat{\Sigma}_b R')^{-1} (Rb - r)$$

II. Sabemos que $Y_T(b-\beta) \xrightarrow{d} N(0, \sigma^2 G' [Q^*]^{-1} (G')^T)$, ent√£o podemos escrever:
$$ Y_T (Rb - R\beta) \xrightarrow{d} N(0, R \sigma^2 G' [Q^*]^{-1} (G')^T R') $$

III. Sob a hip√≥tese nula $R\beta=r$, temos $Y_T(Rb - r) \xrightarrow{d} N(0, R \sigma^2 G' [Q^*]^{-1} (G')^T R')$.

IV.  Pelo Lema 1.1, sabemos que $ Y_T \hat{\Sigma}_b Y_T \xrightarrow{p} \sigma^2 G' [Q^*]^{-1} (G')^T$, ent√£o podemos escrever:
$$ Y_T (R\hat{\Sigma}_b R') Y_T \xrightarrow{p} R\sigma^2 G' [Q^*]^{-1} (G')^T R' $$

V. Usando o resultado acima, podemos escrever a estat√≠stica de Wald como:
$$W = (Y_T(Rb - r))' (Y_T R \hat{\Sigma}_b R' Y_T)^{-1} (Y_T(Rb - r))$$

VI.  Como $Y_T(Rb - r)$ √© assintoticamente normal, e usando o resultado do Lema 1.1, a estat√≠stica de Wald $W$ segue uma distribui√ß√£o $\chi^2$ com *k* graus de liberdade sob a hip√≥tese nula:
$$ W \xrightarrow{d} \chi^2(k) $$
‚ñ†
> üí° **Exemplo Num√©rico:**
> Suponha que temos um modelo AR(1) com tend√™ncia e queremos testar a hip√≥tese nula de que o coeficiente da tend√™ncia ($\delta$) √© igual a zero. Ent√£o, $R = \begin{bmatrix} 0 & 0 & 1 \end{bmatrix}$ e $r = [0]$.
>
> Se estimamos $b$ como $b = \begin{bmatrix} 0.7 \\ 2.1 \\ 0.02 \end{bmatrix}$,  $\hat{\Sigma}_b = \begin{bmatrix} 0.01 & 0.005 & 0.0001 \\ 0.005 & 0.02 & 0.0002 \\ 0.0001 & 0.0002 & 0.000004 \end{bmatrix}$
>
>Ent√£o, $Rb - r = \begin{bmatrix} 0 & 0 & 1 \end{bmatrix}\begin{bmatrix} 0.7 \\ 2.1 \\ 0.02 \end{bmatrix} - 0 = 0.02$
>
> $R\hat{\Sigma}_b R' = \begin{bmatrix} 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 0.01 & 0.005 & 0.0001 \\ 0.005 & 0.02 & 0.0002 \\ 0.0001 & 0.0002 & 0.000004 \end{bmatrix} \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} = 0.000004 $
>
> A estat√≠stica de Wald √©:
>
> $W = (0.02) (0.000004)^{-1} (0.02) = \frac{0.0004}{0.000004} = 100$
>
> Como temos uma restri√ß√£o, a estat√≠stica de Wald segue uma distribui√ß√£o $\chi^2$ com 1 grau de liberdade. O valor cr√≠tico da $\chi^2$ com 1 grau de liberdade e n√≠vel de signific√¢ncia de 5% √© 3.84. Como $W = 100 > 3.84$, rejeitamos a hip√≥tese nula de que o coeficiente da tend√™ncia √© igual a zero.

### Conclus√£o
Em resumo, a metodologia de transforma√ß√£o de regressores proposta por Sims, Stock e Watson, aplicada a um modelo autoregressivo em torno de uma tend√™ncia temporal determin√≠stica, permite isolar as componentes com diferentes taxas de converg√™ncia [^1]. Ao transformar o modelo para uma forma que inclui vari√°veis estacion√°rias, uma constante e uma tend√™ncia temporal, podemos derivar a distribui√ß√£o assint√≥tica dos estimadores de OLS, conforme visto em [16.3.13]. Esta abordagem facilita a an√°lise estat√≠stica e a realiza√ß√£o de testes de hip√≥teses em modelos com ra√≠zes unit√°rias e tend√™ncias determin√≠sticas [^1].

Como explicitado em [16.3.13], os coeficientes das vari√°veis estacion√°rias convergem para uma distribui√ß√£o Gaussiana a uma taxa de $\sqrt{T}$, enquanto os coeficientes da tend√™ncia temporal convergem para uma distribui√ß√£o Gaussiana a uma taxa de $T^{3/2}$ [^1]. Este ponto √© fundamental para realizar infer√™ncias estat√≠sticas v√°lidas [^1]. A estat√≠stica de Wald para testes de hip√≥teses sobre os par√¢metros do modelo AR com tend√™ncia temporal, conforme demonstrado em [16.3.20] e [16.3.21], √© assintoticamente v√°lida, mesmo que n√£o se aplique a transforma√ß√£o na pr√°tica, demonstrando a robustez da metodologia [^1]. Esta equival√™ncia na distribui√ß√£o assint√≥tica entre o modelo transformado e o modelo original, permite a constru√ß√£o de testes de hip√≥teses v√°lidos sem a necessidade de estimar a regress√£o transformada na pr√°tica [^1].

### Refer√™ncias
[^1]: Texto original fornecido.
<!-- END -->
