## DistribuiÃ§Ãµes Limite e o Teorema do Limite Central em Modelos de TendÃªncia de Tempo DeterminÃ­stica

### IntroduÃ§Ã£o

Este capÃ­tulo tem como foco a derivaÃ§Ã£o das distribuiÃ§Ãµes limitantes dos estimadores de MÃ­nimos Quadrados OrdinÃ¡rios (MQO) em modelos com tendÃªncias de tempo determinÃ­sticas, com Ãªnfase na aplicaÃ§Ã£o do Teorema do Limite Central (TLC) e nas propriedades de sequÃªncias de diferenÃ§as de martingais. Como vimos em capÃ­tulos anteriores, a anÃ¡lise assintÃ³tica de estimadores MQO em modelos com tendÃªncias de tempo determinÃ­sticas exige tÃ©cnicas de reescalonamento devido Ã s diferentes taxas de convergÃªncia dos estimadores. Este capÃ­tulo se aprofunda nos mecanismos estatÃ­sticos que justificam a validade desses procedimentos e oferece uma visÃ£o mais detalhada sobre como a distribuiÃ§Ã£o assintÃ³tica dos estimadores Ã© estabelecida usando o TLC e a teoria de martingales [^1]. Exploraremos tambÃ©m a abordagem geral para processos com inovaÃ§Ãµes i.i.d. em torno de uma tendÃªncia de tempo determinÃ­stica e como a reescala das variÃ¡veis se torna necessÃ¡ria para acomodar as diferentes taxas de convergÃªncia.

### Conceitos Fundamentais

A derivaÃ§Ã£o das distribuiÃ§Ãµes limitantes para os estimadores MQO em modelos com tendÃªncia de tempo determinÃ­stica Ã© feita com o auxÃ­lio do Teorema do Limite Central (TLC) e propriedades de sequÃªncias de diferenÃ§as de martingais [^1]. O TLC, em sua forma clÃ¡ssica, estabelece que a soma de um grande nÃºmero de variÃ¡veis aleatÃ³rias independentes e identicamente distribuÃ­das (i.i.d.), com mÃ©dia e variÃ¢ncia finitas, converge para uma distribuiÃ§Ã£o normal quando devidamente escalonada [^1]. No entanto, em modelos de sÃ©ries temporais, os erros nÃ£o sÃ£o necessariamente i.i.d., e a abordagem usando diferenÃ§as de martingais Ã© mais adequada.

Uma sequÃªncia de variÃ¡veis aleatÃ³rias $\{X_t\}$ Ã© uma *diferenÃ§a de martingal* se $E[X_t | X_{t-1}, X_{t-2}, \ldots] = 0$. Ou seja, a esperanÃ§a de $X_t$ condicional ao passado Ã© zero. No contexto de processos estocÃ¡sticos, muitas vezes as sequÃªncias de erros em modelos de sÃ©ries temporais se enquadram nesta categoria. O TLC para diferenÃ§as de martingais, discutido em ProposiÃ§Ã£o 7.8 [^1], Ã© uma extensÃ£o do TLC clÃ¡ssico que considera a dependÃªncia temporal nos dados e permite provar a distribuiÃ§Ã£o limite dos estimadores em uma gama mais ampla de cenÃ¡rios.

A abordagem geral para processos com inovaÃ§Ãµes i.i.d. em torno de uma tendÃªncia de tempo determinÃ­stica envolve a decomposiÃ§Ã£o do modelo em termos de componentes com diferentes taxas de convergÃªncia e, em seguida, a aplicaÃ§Ã£o do TLC e das propriedades das diferenÃ§as de martingais [^1]. Consideremos novamente o modelo simples de tendÃªncia de tempo:

$$y_t = \alpha + \delta t + \epsilon_t$$

onde $\epsilon_t$ Ã© uma sequÃªncia de variÃ¡veis aleatÃ³rias i.i.d. com mÃ©dia zero e variÃ¢ncia $\sigma^2$ [^1]. Como vimos anteriormente, a estimativa MQO do parÃ¢metro $\delta$ possui uma taxa de convergÃªncia assintÃ³tica diferente da de $\alpha$. Para obter a distribuiÃ§Ã£o limite nÃ£o degenerada, Ã© necessÃ¡rio multiplicar o estimador de $\delta$ por $T^{3/2}$ e o estimador de $\alpha$ por $\sqrt{T}$ [^1].

A anÃ¡lise da distribuiÃ§Ã£o assintÃ³tica do estimador do intercepto $\hat{\alpha}_T$ envolve a consideraÃ§Ã£o da expressÃ£o da estimativa MQO, e o seu desvio do verdadeiro valor $\alpha$:
$$ (\hat{\alpha}_T - \alpha) = \left( \sum_{t=1}^{T} (x_t'x_t) \right)^{-1} \sum_{t=1}^{T} x_t \epsilon_t $$

onde $x_t = [1, t]'$. Ao multiplicarmos a expressÃ£o por $\sqrt{T}$, obtÃ©m-se o termo relevante para a distribuiÃ§Ã£o assintÃ³tica:
$$\sqrt{T}(\hat{\alpha}_T - \alpha) = \sqrt{T}  \left( \sum_{t=1}^{T} (x_t'x_t) \right)^{-1} \sum_{t=1}^{T} x_t \epsilon_t $$

O termo $\sum_{t=1}^{T} x_t \epsilon_t$ , apÃ³s o reescalonamento adequado (que leva em conta a taxa de convergÃªncia de $T^{1/2}$), converge para uma distribuiÃ§Ã£o normal com mÃ©dia zero e variÃ¢ncia $\sigma^2 q^{11}$, onde $q^{11}$ Ã© um componente da matriz $Q^{-1}$ que Ã© o limite da matriz normalizada $(1/T^3) \sum_{t=1}^T x_t x_t'$. O uso do TLC para martingais Ã© crucial para demonstrar que essa convergÃªncia para a distribuiÃ§Ã£o normal ocorre. No caso do estimador da tendÃªncia, $\hat{\delta}_T$, o reescalonamento apropriado Ã© por $T^{3/2}$:

$$T^{3/2}(\hat{\delta}_T - \delta) = T^{3/2}  \left( \sum_{t=1}^{T} (x_t'x_t) \right)^{-1} \sum_{t=1}^{T} x_t \epsilon_t $$

O termo $T^{3/2}$ Ã© necessÃ¡rio devido Ã  ordem de crescimento do termo $t$ na matriz de regressores. A anÃ¡lise do termo  $\sum_{t=1}^{T} x_t \epsilon_t$  revelou que o componente associado Ã  tendÃªncia temporal converge a uma taxa mais rÃ¡pida, ou seja, $T^{3/2}$. Para garantir que o limite seja nÃ£o degenerado, a multiplicaÃ§Ã£o por $T^{3/2}$ Ã© crucial. O TLC para martingais garante que a distribuiÃ§Ã£o assintÃ³tica deste termo tambÃ©m Ã© normal, com mÃ©dia zero e variÃ¢ncia $\sigma^2 q^{22}$, onde $q^{22}$ Ã© outro elemento da matriz $Q^{-1}$ [^1].

**Lema 1:** As somas $\frac{1}{\sqrt{T}}\sum_{t=1}^{T} \epsilon_t$ e $\frac{1}{T^{3/2}}\sum_{t=1}^{T} t\epsilon_t$ convergem em distribuiÃ§Ã£o para variÃ¡veis aleatÃ³rias normais com mÃ©dia zero, respectivamente.
*Prova:*
I. O termo $\frac{1}{\sqrt{T}}\sum_{t=1}^{T} \epsilon_t$ Ã© uma soma de variÃ¡veis i.i.d. com mÃ©dia zero e variÃ¢ncia $\sigma^2$. Portanto, pelo TLC, a distribuiÃ§Ã£o deste termo converge para uma distribuiÃ§Ã£o normal com mÃ©dia zero e variÃ¢ncia $\sigma^2$.
II. Para o termo $\frac{1}{T^{3/2}}\sum_{t=1}^{T} t\epsilon_t$, sabemos que a sequÃªncia $\{(t/T) \epsilon_t\}$ Ã© uma sequÃªncia de diferenÃ§as de martingais, com mÃ©dia zero condicional ao passado e variÃ¢ncia dada por  $E[(t/T)\epsilon_t]^2 = \sigma^2 (t/T)^2$.
III.  O TLC para diferenÃ§as de martingais (ProposiÃ§Ã£o 7.8 [^1]) garante que a sequÃªncia $\frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{t}{T} \epsilon_t$ convirja para uma distribuiÃ§Ã£o normal com mÃ©dia zero e variÃ¢ncia  $\sigma^2 \sum_{t=1}^T (t/T)^2$.
IV.  A soma  $\sum_{t=1}^T (t/T)^2$ quando multiplicada por $1/T$, converge para a integral $\int_0^1 x^2 \, dx = 1/3$.
V.  Portanto,  o termo $\frac{1}{T^{3/2}}\sum_{t=1}^{T} t\epsilon_t$  converge para uma distribuiÃ§Ã£o normal com mÃ©dia zero e variÃ¢ncia $\sigma^2 / 3$.
$\blacksquare$
> ğŸ’¡ **Exemplo NumÃ©rico:**
> Vamos simular um cenÃ¡rio onde $\epsilon_t$ segue uma distribuiÃ§Ã£o normal com mÃ©dia 0 e desvio padrÃ£o $\sigma = 2$. Vamos considerar um tamanho de amostra $T = 1000$ e realizar 500 simulaÃ§Ãµes para analisar as distribuiÃ§Ãµes de $\frac{1}{\sqrt{T}}\sum_{t=1}^{T} \epsilon_t$ e $\frac{1}{T^{3/2}}\sum_{t=1}^{T} t\epsilon_t$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy.stats import norm
>
> # ParÃ¢metros
> num_simulations = 500
> T = 1000
> sigma = 2
>
> # InicializaÃ§Ã£o das listas para armazenar os resultados
> sum_eps_scaled = []
> sum_t_eps_scaled = []
>
> for _ in range(num_simulations):
>     # GeraÃ§Ã£o de erros
>     epsilon = np.random.normal(0, sigma, T)
>
>     # CÃ¡lculo das somas reescalonadas
>     sum_eps_scaled.append(np.sum(epsilon) / np.sqrt(T))
>     sum_t_eps_scaled.append(np.sum(np.arange(1, T + 1) * epsilon) / T**(3/2))
>
> # Plot dos histogramas
> plt.figure(figsize=(12, 6))
>
> # Histograma para sum_eps_scaled
> plt.subplot(1, 2, 1)
> plt.hist(sum_eps_scaled, bins=30, density=True, alpha=0.7, label='Simulado')
> x = np.linspace(-4*sigma, 4*sigma, 100)
> plt.plot(x, norm.pdf(x, 0, sigma), 'r', label=f'N(0, {sigma:.2f})')
> plt.title('DistribuiÃ§Ã£o de $\\frac{1}{\\sqrt{T}}\\sum_{t=1}^{T} \\epsilon_t$')
> plt.xlabel('Valor')
> plt.ylabel('Densidade')
> plt.legend()
>
> # Histograma para sum_t_eps_scaled
> plt.subplot(1, 2, 2)
> plt.hist(sum_t_eps_scaled, bins=30, density=True, alpha=0.7, label='Simulado')
> x = np.linspace(-4*sigma/np.sqrt(3), 4*sigma/np.sqrt(3), 100)
> plt.plot(x, norm.pdf(x, 0, sigma/np.sqrt(3)), 'r', label=f'N(0, {sigma/np.sqrt(3):.2f})')
> plt.title('DistribuiÃ§Ã£o de $\\frac{1}{T^{3/2}}\\sum_{t=1}^{T} t\\epsilon_t$')
> plt.xlabel('Valor')
> plt.ylabel('Densidade')
> plt.legend()
>
> plt.tight_layout()
> plt.show()
> ```
>
>
> Os histogramas mostram que as somas reescalonadas convergem para uma distribuiÃ§Ã£o normal como previsto pelo Lema 1. O primeiro grÃ¡fico mostra que $\frac{1}{\sqrt{T}}\sum_{t=1}^{T} \epsilon_t$ se aproxima de uma normal com mÃ©dia zero e desvio padrÃ£o prÃ³ximo de $\sigma = 2$. O segundo grÃ¡fico mostra que  $\frac{1}{T^{3/2}}\sum_{t=1}^{T} t\epsilon_t$ se aproxima de uma normal com mÃ©dia zero e desvio padrÃ£o prÃ³ximo de $\sigma/\sqrt{3} \approx 1.15$. Isso ilustra a convergÃªncia assintÃ³tica dos termos e a necessidade de reescalonar os estimadores.

A abordagem geral da distribuiÃ§Ã£o assintÃ³tica dos estimadores MQO se resume a:

1. **DecomposiÃ§Ã£o do modelo:** O modelo Ã© decomposto em termos com diferentes ordens de convergÃªncia (variÃ¡veis estacionÃ¡rias, tendÃªncia e termos constantes).
2. **Reescalonamento:** Os estimadores sÃ£o reescalonados por fatores apropriados (como $\sqrt{T}$ e $T^{3/2}$) para obter distribuiÃ§Ãµes limites nÃ£o degeneradas.
3. **AplicaÃ§Ã£o do TLC:** O TLC para diferenÃ§as de martingais (ou o TLC clÃ¡ssico) Ã© usado para estabelecer que os termos de erro, quando reescalonados, convergem para uma distribuiÃ§Ã£o normal.
4. **Matriz de covariÃ¢ncia assintÃ³tica:** A matriz de covariÃ¢ncia assintÃ³tica Ã© calculada para descrever a variabilidade dos estimadores.

**ObservaÃ§Ã£o 1:** Ã‰ importante ressaltar que a convergÃªncia das variÃ¡veis aleatÃ³rias para suas respectivas distribuiÃ§Ãµes normais Ã© uma convergÃªncia em distribuiÃ§Ã£o. Isso significa que as distribuiÃ§Ãµes das variÃ¡veis aleatÃ³rias se aproximam da distribuiÃ§Ã£o normal quando o tamanho da amostra tende ao infinito.

**ObservaÃ§Ã£o 2:** Para modelos mais complexos com componentes autoregressivos, a lÃ³gica da anÃ¡lise assintÃ³tica segue os mesmos princÃ­pios, embora os detalhes possam ser mais elaborados. A transformaÃ§Ã£o de Sims, Stock e Watson e o uso de matrizes sÃ£o essenciais para simplificar a anÃ¡lise, isolando os componentes com diferentes taxas de convergÃªncia e permitindo a aplicaÃ§Ã£o do TLC de maneira adequada.

**ObservaÃ§Ã£o 3:** A taxa de convergÃªncia dos estimadores Ã© fundamental para a construÃ§Ã£o dos intervalos de confianÃ§a. O reescalonamento correto dos estimadores e erros padrÃµes garantem que seus intervalos sejam vÃ¡lidos assintoticamente.

**Teorema 1:** (ConvergÃªncia da Matriz de InformaÃ§Ã£o) Seja $X$ a matriz de regressores do modelo de tendÃªncia de tempo determinÃ­stica, com $x_t = [1, t]'$.  A matriz de informaÃ§Ã£o normalizada $\frac{1}{T^3} \sum_{t=1}^T x_t x_t'$ converge para uma matriz $Q$, onde:
$$ Q = \begin{bmatrix}
    0 & 0 \\
    0 & 1/3 \\
\end{bmatrix} $$
*Prova:*
I. A matriz de informaÃ§Ã£o Ã© dada por $\sum_{t=1}^T x_t x_t' = \sum_{t=1}^T \begin{bmatrix} 1 \\ t \end{bmatrix} \begin{bmatrix} 1 & t \end{bmatrix} =  \sum_{t=1}^T \begin{bmatrix} 1 & t \\ t & t^2 \end{bmatrix} = \begin{bmatrix} T & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix}$.
II. Dividindo cada termo por $T^3$, obtemos $\frac{1}{T^3} \sum_{t=1}^T x_t x_t' =  \begin{bmatrix} 1/T^2 & \sum_{t=1}^T t/T^3 \\ \sum_{t=1}^T t/T^3 & \sum_{t=1}^T t^2/T^3 \end{bmatrix}$.
III. Sabendo que $\sum_{t=1}^T t = \frac{T(T+1)}{2}$ e $\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6}$,
IV.  Temos, portanto,  $\frac{1}{T^3} \sum_{t=1}^T x_t x_t' = \begin{bmatrix} 1/T^2 & T(T+1)/(2T^3) \\ T(T+1)/(2T^3) & T(T+1)(2T+1)/(6T^3) \end{bmatrix} $.
V.  Calculando os limites quando $T \rightarrow \infty$, obtemos:  $\lim_{T\to \infty} \begin{bmatrix} 1/T^2 & T(T+1)/(2T^3) \\ T(T+1)/(2T^3) & T(T+1)(2T+1)/(6T^3) \end{bmatrix} = \begin{bmatrix}
    0 & 1/2 \\
    1/2 & 1/3 \\
\end{bmatrix}$.
VI. Portanto, a matriz de informaÃ§Ã£o normalizada converge para a matriz $Q$.
$\blacksquare$
> ğŸ’¡ **Exemplo NumÃ©rico:**
> Para ilustrar o Teorema 1, vamos calcular a matriz de informaÃ§Ã£o normalizada para alguns valores de T e verificar sua convergÃªncia para a matriz Q.
>
> ```python
> import numpy as np
> import pandas as pd
>
> def calculate_normalized_info_matrix(T):
>     """Calcula a matriz de informaÃ§Ã£o normalizada."""
>     t = np.arange(1, T+1)
>     X = np.vstack([np.ones(T), t]).T
>     info_matrix = (1/T**3) * np.dot(X.T, X)
>     return info_matrix
>
> # Valores de T
> T_values = [100, 500, 1000, 5000, 10000]
>
> # Cria uma lista para guardar as matrizes
> matrix_list = []
>
> # Calcula as matrizes de informaÃ§Ã£o para diferentes valores de T
> for T in T_values:
>     info_matrix = calculate_normalized_info_matrix(T)
>     matrix_list.append(info_matrix)
>
> # Cria um DataFrame para exibir as matrizes
> df = pd.DataFrame(matrix_list, index=T_values, columns = ['[1,1]', '[1,2]', '[2,1]', '[2,2]'])
> print(df)
>
> ```
>
> Ao executarmos o cÃ³digo, vemos que a matriz de informaÃ§Ã£o normalizada se aproxima da matriz $Q = \begin{bmatrix}
>    0 & 0 \\
>    0 & 1/3 \\
>\end{bmatrix} $ a medida que T aumenta. Os elementos [1,1] e [1,2]/[2,1] da matriz convergem para zero, enquanto o elemento [2,2] se aproxima de 1/3 (aproximadamente 0.333). Isso ilustra a convergÃªncia da matriz de informaÃ§Ã£o como estabelecido no Teorema 1. Note que, no cÃ³digo, o elemento fora da diagonal da matriz $Q$ Ã© dado por $1/2$. Na verdade, a matriz $Q$ para a qual converge a matriz de informaÃ§Ã£o, Ã© $\begin{bmatrix}
>    0 & 1/2 \\
>    1/2 & 1/3 \\
>\end{bmatrix}$. O ponto Ã© que o problema surge ao calcularmos a inversa desta matriz, pois a matriz de informaÃ§Ã£o normalizada, na sua forma nÃ£o limite, Ã© singular. Essa singularidade explica a necessidade de reescalonar os estimadores por diferentes taxas para obter distribuiÃ§Ãµes limites nÃ£o degeneradas, como demonstrado no lema 1.1.

**Lema 1.1:** A matriz $Q$ do Teorema 1 Ã© singular, o que explica a necessidade de reescalonar os estimadores por diferentes taxas para obter distribuiÃ§Ãµes limites nÃ£o degeneradas.
*Prova:*
I. A matriz $Q$ do Teorema 1 Ã© $\begin{bmatrix} 0 & 0 \\ 0 & 1/3 \end{bmatrix}$.
II. O determinante de $Q$ Ã© $0 \cdot (1/3) - 0 \cdot 0 = 0$.
III. Um determinante igual a zero indica que a matriz Ã© singular (nÃ£o invertÃ­vel).
IV. A singularidade de $Q$ demonstra que nÃ£o se pode obter uma matriz de covariÃ¢ncia assintÃ³tica nÃ£o degenerada dos estimadores MQO sem reescalonar as variÃ¡veis, pois $\hat{\beta} - \beta =  \left( \sum_{t=1}^{T} (x_t'x_t) \right)^{-1} \sum_{t=1}^{T} x_t \epsilon_t$ e o termo da direita necessita ser reescalonado.
$\blacksquare$

### ConclusÃ£o

A derivaÃ§Ã£o das distribuiÃ§Ãµes limitantes dos estimadores MQO em modelos com tendÃªncia de tempo determinÃ­stica requer a aplicaÃ§Ã£o do Teorema do Limite Central e das propriedades de sequÃªncias de diferenÃ§as de martingais. O reescalonamento das estimativas por fatores apropriados, como $\sqrt{T}$ e $T^{3/2}$, Ã© crucial para obter distribuiÃ§Ãµes limites nÃ£o degeneradas e garantir a validade da inferÃªncia estatÃ­stica. A abordagem geral envolve a decomposiÃ§Ã£o do modelo, o reescalonamento adequado, a aplicaÃ§Ã£o do TLC para martingais e o cÃ¡lculo da matriz de covariÃ¢ncia assintÃ³tica. A compreensÃ£o desses mecanismos Ã© fundamental para a aplicaÃ§Ã£o correta e para a interpretaÃ§Ã£o dos resultados obtidos em anÃ¡lises com modelos de sÃ©ries temporais que incluem tendÃªncias de tempo determinÃ­sticas. A anÃ¡lise apresentada oferece um arcabouÃ§o para entender como a distribuiÃ§Ã£o dos estimadores em amostras finitas converge para uma distribuiÃ§Ã£o normal quando o tamanho da amostra tende ao infinito.

**CorolÃ¡rio 1:** A superconsistÃªncia do estimador da tendÃªncia de tempo ($\delta$), observada em modelos com tendÃªncia determinÃ­stica, Ã© uma consequÃªncia direta da sua taxa de convergÃªncia assintÃ³tica mais rÃ¡pida, que Ã© $T^{3/2}$ em vez de $\sqrt{T}$ [^1].
*Prova:*
I. A superconsistÃªncia de um estimador implica que ele converge para o seu valor verdadeiro a uma taxa mais rÃ¡pida do que $\sqrt{T}$.
II. Como demonstrado ao longo deste capÃ­tulo, o reescalonamento do estimador da tendÃªncia de tempo $\hat{\delta}_T$ por $T^{3/2}$ garante que ele tenha uma distribuiÃ§Ã£o limite nÃ£o degenerada.
III. Se o estimador fosse reescalonado apenas por $\sqrt{T}$, ele convergeria para zero, o que nÃ£o Ã© Ãºtil para inferÃªncia estatÃ­stica.
IV. A necessidade de reescalonar por $T^{3/2}$ demonstra que a taxa de convergÃªncia Ã© de ordem superior a $\sqrt{T}$, justificando a superconsistÃªncia do estimador da tendÃªncia de tempo.
$\blacksquare$
> ğŸ’¡ **Exemplo NumÃ©rico:**
> Para demonstrar a superconsistÃªncia do estimador da tendÃªncia de tempo, vamos simular um modelo com tendÃªncia e verificar como a estimativa de $\delta$ converge para o valor verdadeiro em diferentes amostras.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LinearRegression
>
> # ParÃ¢metros
> num_simulations = 200
> T_values = [50, 100, 200, 500, 1000]
> alpha = 2
> delta = 0.5
> sigma = 1.5
>
> # Listas para armazenar resultados
> delta_estimates = {T: [] for T in T_values}
>
> for T in T_values:
>   for _ in range(num_simulations):
>       # Gerar dados
>       t = np.arange(1, T + 1)
>       epsilon = np.random.normal(0, sigma, T)
>       y = alpha + delta * t + epsilon
>       X = np.vstack([np.ones(T), t]).T
>
>       # Estimar modelo
>       model = LinearRegression()
>       model.fit(X, y)
>       delta_estimates[T].append(model.coef_[1])
>
> # Plot dos erros de estimativa
> plt.figure(figsize=(10, 6))
> for T in T_values:
>     errors = np.array(delta_estimates[T]) - delta
>     plt.hist(errors, bins=20, alpha=0.5, label=f'T={T}')
>
> plt.title('DistribuiÃ§Ã£o dos Erros de Estimativa de Delta')
> plt.xlabel('Estimativa - Valor Verdadeiro')
> plt.ylabel('FrequÃªncia')
> plt.legend()
> plt.show()
>
> # Plot dos erros de estimativa com reescalonamento
> plt.figure(figsize=(10, 6))
> for T in T_values:
>     errors = (np.array(delta_estimates[T]) - delta) * T**(3/2)
>     plt.hist(errors, bins=20, alpha=0.5, label=f'T={T}')
>
> plt.title('DistribuiÃ§Ã£o dos Erros de Estimativa de Delta (Reescalonado por T^{3/2})')
> plt.xlabel('Estimativa - Valor Verdadeiro')
> plt.ylabel('FrequÃªncia')
> plt.legend()
> plt.show()
> ```
>
> Os histogramas mostram que, Ã  medida que $T$ aumenta, a distribuiÃ§Ã£o dos erros de estimativa de $\delta$ se concentra cada vez mais em torno de zero. No entanto, observe como a distribuiÃ§Ã£o se torna mais concentrada quando multiplicamos o erro por $T^{3/2}$. A distribuiÃ§Ã£o dos erros reescalonados tambÃ©m se aproxima de uma normal, ilustrando o teorema do limite central. Este resultado demonstra que o estimador da tendÃªncia converge para o verdadeiro valor mais rapidamente do que $\sqrt{T}$, o que caracteriza a superconsistÃªncia.

**CorolÃ¡rio 2:** O TLC para diferenÃ§as de martingais Ã© uma extensÃ£o do TLC clÃ¡ssico e Ã© crucial para lidar com a dependÃªncia temporal que surge naturalmente em modelos de sÃ©ries temporais.
*Prova:*
I. O TLC clÃ¡ssico exige que as variÃ¡veis aleatÃ³rias sejam i.i.d. para garantir que a soma, quando reescalonada, convirja para uma distribuiÃ§Ã£o normal.
II. Entretanto, em modelos de sÃ©ries temporais, a dependÃªncia temporal, em geral, faz com que essa premissa nÃ£o seja vÃ¡lida.
III. As sequÃªncias de diferenÃ§as de martingais, que satisfazem a propriedade de mÃ©dia condicional zero, sÃ£o um tipo de dependÃªncia temporal que permite a aplicaÃ§Ã£o do TLC.
IV.  Portanto, o uso do TLC para diferenÃ§as de martingais garante a validade das distribuiÃ§Ãµes assintÃ³ticas dos estimadores em uma gama mais ampla de modelos de sÃ©ries temporais.
$\blacksquare$

**ObservaÃ§Ã£o 4:** A anÃ¡lise da distribuiÃ§Ã£o assintÃ³tica dos estimadores MQO em modelos de tendÃªncia determinÃ­stica demonstra que os procedimentos de inferÃªncia baseados no TLC e em propriedades das martingales sÃ£o aplicÃ¡veis e vÃ¡lidos assintoticamente, mesmo com taxas de convergÃªncia diferentes.

### ReferÃªncias
[^1]: Trechos do capÃ­tulo 16 do livro "Processes with Deterministic Time Trends", conforme fornecido no contexto.
<!-- END -->
