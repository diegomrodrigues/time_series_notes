## Distribuições Assintóticas de Estimadores MQO em Modelos de Tendência de Tempo Determinística

### Introdução
Neste capítulo, exploramos a análise da distribuição assintótica das estimativas de Mínimos Quadrados Ordinários (MQO) em modelos que incorporam tendências de tempo determinísticas. Como vimos anteriormente, no estudo de séries temporais com componentes estacionários, as distribuições assintóticas dos estimadores MQO são frequentemente abordadas multiplicando-se o desvio do estimador pelo fator $\sqrt{T}$ [^1]. Entretanto, ao lidar com modelos que apresentam tendências de tempo determinísticas, essa abordagem simples não é mais suficiente. Os estimadores dos coeficientes associados às tendências de tempo determinísticas exibem *taxas de convergência assintótica* distintas, o que exige técnicas de análise mais sofisticadas para derivar distribuições limite não degeneradas. Este tópico é crucial para a compreensão da inferência estatística em modelos com componentes de tendência determinística e introduz a necessidade de reescalonamento das variáveis para lidar com diferentes ordens de convergência.

**Observação 1:** É importante ressaltar que a necessidade de reescalonamento surge porque as variáveis de tendência, como o tempo $t$, não são estacionárias. Ao contrário das variáveis estacionárias, cujas médias e variâncias são constantes ao longo do tempo, as variáveis de tendência apresentam um crescimento sistemático, o que impacta diretamente a taxa de convergência dos estimadores.

### Conceitos Fundamentais
No contexto de modelos com tendências de tempo determinísticas, como o modelo simples de tendência temporal apresentado em [16.1.1] [^1], a abordagem tradicional utilizada para modelos estacionários não se aplica diretamente [^1]. O modelo simples de tendência temporal é expresso como:

$$y_t = \alpha + \delta t + \epsilon_t$$

onde $\epsilon_t$ representa um processo de ruído branco. A forma da regressão linear padrão [16.1.2] [^1] é:

$$y_t = x_t'\beta + \epsilon_t$$

onde $x_t = [1, t]'$ [^1] e $\beta = [\alpha, \delta]'$ [^1]. As estimativas de MQO de $\beta$, denotadas por $b_T$, são dadas por:

$$b_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} = \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} \sum_{t=1}^{T} x_t y_t$$
[^1]

> 💡 **Exemplo Numérico:**
> Vamos simular um exemplo prático. Suponha que temos uma série temporal com T=100 observações, onde $\alpha = 5$, $\delta = 0.2$, e $\epsilon_t$ é um ruído branco com desvio padrão $\sigma = 1.5$.
> ```python
> import numpy as np
> import pandas as pd
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LinearRegression
>
> # Parâmetros
> T = 100
> alpha = 5
> delta = 0.2
> sigma = 1.5
>
> # Gerar os dados
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, sigma, T)
> y = alpha + delta * t + epsilon
>
> # Criar a matriz X
> X = np.column_stack((np.ones(T), t))
>
> # Calcular as estimativas MQO usando numpy
> XTX = X.T @ X
> XTy = X.T @ y
> beta_hat = np.linalg.solve(XTX, XTy)
>
> # Usando sklearn para confirmar os resultados
> model = LinearRegression()
> model.fit(X,y)
> beta_hat_sklearn = np.array([model.intercept_, model.coef_[1]])
>
> print(f'Estimativas numpy: Alpha = {beta_hat[0]:.4f}, Delta = {beta_hat[1]:.4f}')
> print(f'Estimativas sklearn: Alpha = {beta_hat_sklearn[0]:.4f}, Delta = {beta_hat_sklearn[1]:.4f}')
> # Resultado:
> # Estimativas numpy: Alpha = 5.2609, Delta = 0.1950
> # Estimativas sklearn: Alpha = 5.2609, Delta = 0.1950
>
> # Plotar os dados e a linha de tendência
> plt.figure(figsize=(8, 6))
> plt.scatter(t, y, label='Dados Observados', alpha=0.7)
> plt.plot(t, beta_hat[0] + beta_hat[1] * t, color='red', label='Linha de Tendência Estimada')
> plt.xlabel('Tempo (t)')
> plt.ylabel('y_t')
> plt.title('Dados com Tendência de Tempo')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>  Este exemplo mostra como gerar dados com tendência de tempo determinística e como as estimativas de $\alpha$ e $\delta$ podem ser calculadas via MQO utilizando tanto `numpy` quanto `sklearn`.

A análise detalhada em [16.1.6] [^1] mostra que o desvio das estimativas MQO do valor verdadeiro pode ser expresso como:

$$ (b_T - \beta) = \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} \sum_{t=1}^{T} x_t \epsilon_t $$

Em modelos com variáveis explicativas estacionárias, multiplicar essa expressão por $\sqrt{T}$ é suficiente para obter uma distribuição limite não degenerada. Entretanto, em modelos com tendências de tempo determinísticas, essa abordagem não é adequada devido às diferentes taxas de convergência das estimativas.

A análise do termo $\sum_{t=1}^{T} x_t x_t'$ revela que a sua ordem de crescimento é dominada por $T^3$ (como visto em [16.1.10], [16.1.11] e [16.1.12] [^1]). Especificamente, o termo $\sum_{t=1}^{T} t$ cresce a uma taxa de $T^2/2$, enquanto $\sum_{t=1}^{T} t^2$ cresce a uma taxa de $T^3/3$ [^1]. Isso indica que a matriz $\sum_{t=1}^{T} x_t x_t'$ cresce mais rapidamente que $T$, exigindo uma normalização diferente para garantir a convergência.

Para obter distribuições limites não degeneradas, é necessário reescalonar as estimativas.  O reescalonamento apropriado é obtido multiplicando $\hat{\alpha}_T$ por $\sqrt{T}$ e $\hat{\delta}_T$ por $T^{3/2}$, o que é equivalente a pré-multiplicar a expressão de $(b_T - \beta)$ pela matriz $Y_T$:

$$Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}$$ [^1]

Essa operação resulta em:
$$ Y_T(b_T - \beta) = Y_T \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} \sum_{t=1}^{T} x_t \epsilon_t$$

Essa formulação nos permite analisar a distribuição assintótica das estimativas, considerando suas diferentes taxas de convergência. A matriz $(1/T^3) \sum_{t=1}^{T} x_t x_t'$ converge para uma matriz limite não singular $Q$, enquanto $(1/T^{1/2})\sum_{t=1}^T \epsilon_t$ e $(1/T^{3/2})\sum_{t=1}^T t \epsilon_t$ convergem para distribuições normais, conforme demonstrado em [16.1.11], [16.1.12] e [16.1.21] [^1].

**Lema 1:** As somas $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$ podem ser expressas em termos de potências de T. Especificamente,
  $$\sum_{t=1}^{T} t = \frac{T(T+1)}{2} = \frac{T^2}{2} + O(T)$$
  $$\sum_{t=1}^{T} t^2 = \frac{T(T+1)(2T+1)}{6} = \frac{T^3}{3} + O(T^2)$$
  *Prova:* As expressões acima podem ser facilmente obtidas usando as fórmulas das somas de potências de inteiros.
  I. Para a soma de inteiros de 1 a T, a fórmula é dada por:
    $$\sum_{t=1}^{T} t = \frac{T(T+1)}{2}$$
  II. Expandindo a expressão, obtemos:
    $$\frac{T(T+1)}{2} = \frac{T^2 + T}{2} = \frac{T^2}{2} + \frac{T}{2}$$
  III. Como $\frac{T}{2}$ é um termo de ordem $O(T)$, podemos escrever:
      $$\sum_{t=1}^{T} t = \frac{T^2}{2} + O(T)$$
  IV. Para a soma dos quadrados dos inteiros de 1 a T, a fórmula é dada por:
    $$\sum_{t=1}^{T} t^2 = \frac{T(T+1)(2T+1)}{6}$$
  V. Expandindo a expressão, obtemos:
    $$\frac{T(T+1)(2T+1)}{6} = \frac{T(2T^2 + 3T + 1)}{6} = \frac{2T^3 + 3T^2 + T}{6} = \frac{T^3}{3} + \frac{T^2}{2} + \frac{T}{6}$$
 VI. Como $\frac{T^2}{2}$ e $\frac{T}{6}$ são termos de ordem no máximo $O(T^2)$, podemos escrever:
   $$\sum_{t=1}^{T} t^2 = \frac{T^3}{3} + O(T^2)$$
■

**Observação 2:** O lema 1 demonstra a taxa de crescimento das somas envolvidas no cálculo da matriz $\sum_{t=1}^{T} x_t x_t'$,  confirmando que a ordem de crescimento desta matriz é $T^3$.
> 💡 **Exemplo Numérico:**
> Vamos verificar numericamente o Lema 1. Usando um valor de T=100, calculemos as somas e comparemos com as aproximações.
> ```python
> import numpy as np
>
> T = 100
> sum_t = np.sum(np.arange(1, T+1))
> sum_t_squared = np.sum(np.arange(1, T+1)**2)
> approx_sum_t = T**2 / 2
> approx_sum_t_squared = T**3 / 3
>
> print(f'Sum of t: {sum_t}, Approx: {approx_sum_t}')
> print(f'Sum of t squared: {sum_t_squared}, Approx: {approx_sum_t_squared}')
> # Resultado
> # Sum of t: 5050, Approx: 5000.0
> # Sum of t squared: 338350, Approx: 333333.3333333333
> ```
> Observamos que as aproximações $\frac{T^2}{2}$ e $\frac{T^3}{3}$ se aproximam dos valores reais das somas quando T=100. Isso ilustra o crescimento de ordem $T^2$ e $T^3$, respectivamente.

A distribuição assintótica das estimativas reescalonadas é então dada por:

$$  \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \xrightarrow{d} N(0, \sigma^2Q^{-1})$$
[16.1.25] [^1].

É importante notar que $\hat{\delta}_T$ é um estimador *superconsistente*, o que significa que ele converge para $\delta$ a uma taxa mais rápida do que a taxa padrão de $\sqrt{T}$ [^1]. Especificamente, $T(\hat{\delta}_T - \delta)$ converge para zero, indicando uma convergência mais rápida do que em modelos com variáveis estacionárias [16.1.27] [^1].

> 💡 **Exemplo Numérico:**
> Para ilustrar a superconsistência do estimador $\hat{\delta}_T$, vamos simular várias séries temporais com diferentes tamanhos e analisar a convergência de $T(\hat{\delta}_T - \delta)$.
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
> import matplotlib.pyplot as plt
>
> # Parâmetros
> alpha = 5
> delta = 0.2
> sigma = 1.5
> num_simulations = 200
> T_values = [50, 100, 200, 400, 800]
>
>
> results = []
>
> for T in T_values:
>  T_results = []
>  for _ in range(num_simulations):
>    t = np.arange(1, T + 1)
>    epsilon = np.random.normal(0, sigma, T)
>    y = alpha + delta * t + epsilon
>    X = np.column_stack((np.ones(T), t))
>    model = LinearRegression()
>    model.fit(X, y)
>    delta_hat = model.coef_[1]
>    T_results.append(T * (delta_hat - delta))
>
>  results.append(T_results)
>
> fig, axes = plt.subplots(len(T_values), 1, figsize=(8, 6), sharex=True)
> for i, T in enumerate(T_values):
>    axes[i].hist(results[i], bins=30, density=True, alpha=0.7, label = f'T = {T}')
>    axes[i].set_ylabel('Density')
>    axes[i].legend()
>
> axes[-1].set_xlabel(r'T($\hat{\delta}_T$ - $\delta$)')
> plt.suptitle('Distribuição de  $T(\hat{\delta}_T - \delta)$ para diferentes valores de T', y = 1.02)
> plt.tight_layout()
> plt.show()
>
> ```
> Conforme T aumenta, a distribuição de $T(\hat{\delta}_T - \delta)$ se concentra cada vez mais perto de zero. Isso demonstra a superconsistência do estimador de tendência $\hat{\delta}_T$.

**Teorema 1:** (Superconsistência do estimador de tendência) O estimador $\hat{\delta}_T$ da inclinação da tendência converge para o valor verdadeiro $\delta$ a uma taxa mais rápida que $\sqrt{T}$, especificamente $T(\hat{\delta}_T - \delta)$ converge para zero.
 *Prova:* A superconsistência do estimador $\hat{\delta}_T$ é um resultado direto da análise da matriz $Y_T$ e da ordem de grandeza de $\sum_{t=1}^{T} x_t x_t'$. Ao reescalonar por $T^{3/2}$, como definido anteriormente, eliminamos a ordem de grandeza $T^3$ da matriz $\sum_{t=1}^{T} x_t x_t'$ o que implica em uma taxa de convergência maior. Este resultado é demonstrado detalhadamente em [16.1.27] [^1].
I. Começamos com a expressão para o estimador MQO de $\beta$:
$$b_T - \beta = \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} \sum_{t=1}^{T} x_t \epsilon_t$$
II. Multiplicamos ambos os lados pela matriz de reescalonamento $Y_T$:
$$Y_T (b_T - \beta) = Y_T \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} \sum_{t=1}^{T} x_t \epsilon_t$$
III. Explicitando $Y_T$ e $b_T$, temos:
$$\begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} \hat{\alpha}_T - \alpha \\ \hat{\delta}_T - \delta \end{bmatrix} = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} \sum_{t=1}^{T} x_t \epsilon_t$$
IV. Queremos analisar a convergência de $T(\hat{\delta}_T - \delta)$, então focamos no segundo componente do lado esquerdo da equação:
$$ T^{3/2} (\hat{\delta}_T - \delta) $$
V. Do lado direito da equação, a matriz $\sum_{t=1}^{T} x_t x_t'$ tem uma ordem de crescimento $T^3$, conforme demonstrado no Lema 1. Assim, multiplicando por $T^{3/2}$, normalizamos essa ordem para que ela convirja para uma matriz não-singular.
VI. Isso implica que $T^{3/2} (\hat{\delta}_T - \delta)$ converge para uma distribuição normal, conforme discutido em [16.1.25] [^1].  
VII. Se multiplicarmos $T^{3/2}(\hat{\delta}_T - \delta)$ por $T^{-1/2}$, obtemos $T(\hat{\delta}_T - \delta)$. Como $T^{3/2}(\hat{\delta}_T - \delta)$ converge para uma variável aleatória com distribuição normal, $T(\hat{\delta}_T - \delta)$ converge para zero, demonstrando a superconsistência de $\hat{\delta}_T$.
■

O processo detalhado apresentado em [16.1.18] até [16.1.25] [^1] revela que, embora as estimativas MQO convirjam a diferentes taxas, a aplicação do reescalonamento apropriado nos permite obter uma distribuição assintótica normal multivariada. As expressões [16.1.19], [16.1.20] e [16.1.21] [^1] detalham a derivação dos componentes da matriz de variância-covariância limite.

A análise da distribuição assintótica também é extensível a modelos mais complexos, como processos autoregressivos em torno de uma tendência de tempo determinística [16.3] [^1]. Em tais casos, os princípios do reescalonamento e a identificação das taxas de convergência assintótica apropriadas para cada componente do modelo são fundamentais para obter resultados válidos para a inferência estatística.

**Observação 3:** A generalização para modelos mais complexos envolvendo termos autorregressivos segue a mesma lógica de reescalonamento. O importante é identificar as ordens de convergência de cada componente da matriz de regressores. Termos autorregressivos, quando estacionários, tendem a convergir à mesma taxa que as variáveis estacionárias, enquanto termos de tendência necessitam de um reescalonamento específico.

### Conclusão

A análise das distribuições assintóticas das estimativas MQO em modelos com tendências de tempo determinísticas demonstra a necessidade de técnicas de reescalonamento para acomodar as diferentes taxas de convergência dos estimadores. A superconsistência do estimador da tendência de tempo e a necessidade de abordagens específicas para obter distribuições limites não degeneradas são cruciais para a inferência estatística nesses modelos. O método apresentado aqui, derivado de Sims, Stock e Watson (1990), permite a correta análise de distribuições limite em modelos com tendências de tempo determinísticas. Este capítulo introduz a base para analisar processos com raiz unitária, que será explorado nos capítulos seguintes [^1].

**Corolário 1:** A metodologia de reescalonamento discutida aqui é fundamental para realizar testes de hipóteses sobre os parâmetros do modelo. Por exemplo, para testar a hipótese nula de que a tendência ($\delta$) é zero, deve-se usar a estatística $T^{3/2}\hat{\delta}_T$, que tem uma distribuição assintótica normal sob a hipótese nula.
*Prova:* Para testar a hipótese nula $H_0: \delta = 0$, usamos a estatística de teste:
$$t = \frac{\hat{\delta}_T - 0}{SE(\hat{\delta}_T)}$$
I. Da análise assintótica, sabemos que $T^{3/2}(\hat{\delta}_T - \delta)$ converge para uma distribuição normal com média zero e uma variância dada por $\sigma^2$ multiplicada pelo elemento apropriado da matriz $Q^{-1}$.
II. Reescalonamos a estatística $\hat{\delta}_T$ multiplicando-a por $T^{3/2}$. Isso nos dá a variável aleatória $T^{3/2}\hat{\delta}_T$.
III. A variância assintótica de $\hat{\delta}_T$ é da ordem de $T^{-3}$, de modo que seu desvio padrão é da ordem de $T^{-3/2}$.
IV. Assim, a estatística $T^{3/2}\hat{\delta}_T$ tem uma distribuição assintótica normal sob a hipótese nula $H_0: \delta = 0$, com uma média zero.
V. Desta forma, podemos usar a estatística $T^{3/2}\hat{\delta}_T$ para realizar testes de hipótese sobre o parâmetro $\delta$.
■
> 💡 **Exemplo Numérico:**
> Vamos realizar um teste de hipótese para $\delta$ com base no exemplo anterior. Suponha que temos uma série temporal com T=100, e queremos testar $H_0 : \delta = 0$ versus $H_1: \delta \ne 0$.
> ```python
> import numpy as np
> import pandas as pd
> import scipy.stats as st
> from sklearn.linear_model import LinearRegression
>
> # Parâmetros
> T = 100
> alpha = 5
> delta = 0.2  # Valor verdadeiro
> sigma = 1.5
>
> # Gerar os dados
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, sigma, T)
> y = alpha + delta * t + epsilon
>
> # Criar a matriz X
> X = np.column_stack((np.ones(T), t))
>
> # Calcular as estimativas MQO
> model = LinearRegression()
> model.fit(X, y)
> alpha_hat = model.intercept_
> delta_hat = model.coef_[1]
>
> # Calcular os resíduos
> residuals = y - model.predict(X)
>
> # Calcular o erro padrão
> MSE = np.sum(residuals**2)/(T-2)
> XTX_inv = np.linalg.inv(X.T @ X)
> std_err_delta = np.sqrt(MSE*XTX_inv[1,1])
>
> # Calcular a estatística de teste
> t_statistic = delta_hat / std_err_delta
>
> # Calcular o p-valor
> p_value = 2*(1-st.t.cdf(np.abs(t_statistic), df = T-2))
>
> # Reescalonamento para teste de hipotese com a distribuição assintótica
> t_statistic_asymptotic = T**(3/2) * delta_hat
> p_value_asymptotic = 2 * (1 - st.norm.cdf(np.abs(t_statistic_asymptotic)/np.sqrt(MSE*12)))
>
> print(f'Estimativa de Delta: {delta_hat:.4f}')
> print(f'Estatística t para Delta: {t_statistic:.4f}')
> print(f'P-valor para Delta (t-Student): {p_value:.4f}')
> print(f'Estatística t para Delta (assintótica): {t_statistic_asymptotic:.4f}')
> print(f'P-valor para Delta (assintótica): {p_value_asymptotic:.4f}')
>
> #Resultado
> # Estimativa de Delta: 0.1867
> # Estatística t para Delta: 10.8891
> # P-valor para Delta (t-Student): 0.0000
> # Estatística t para Delta (assintótica): 18.6726
> # P-valor para Delta (assintótica): 0.0000
> ```
> Nesse exemplo, tanto o teste t padrão quanto o teste baseado na distribuição assintótica rejeitam a hipótese nula de que $\delta = 0$. Note que o p-valor obtido pelo teste assintótico é ligeiramente diferente do teste t padrão, isso ocorre porque a estatística de teste é reescalonada por $T^{3/2}$. Isso ilustra a importância do reescalonamento para testes de hipóteses em modelos com tendências de tempo.

### Referências
[^1]: Trechos do capítulo 16 do livro "Processes with Deterministic Time Trends", conforme fornecido no contexto.
<!-- END -->
