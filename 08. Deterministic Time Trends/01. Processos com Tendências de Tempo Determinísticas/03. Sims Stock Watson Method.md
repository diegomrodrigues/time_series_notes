## A Transforma√ß√£o de Sims, Stock e Watson para An√°lise Assint√≥tica

### Introdu√ß√£o

Em continuidade √† an√°lise da distribui√ß√£o assint√≥tica de estimadores MQO em modelos de tend√™ncia de tempo determin√≠stica, exploramos o m√©todo de Sims, Stock e Watson (1990), o qual transforma o modelo de regress√£o original em uma forma can√¥nica que simplifica a an√°lise da distribui√ß√£o assint√≥tica [^1]. Essa transforma√ß√£o √© particularmente √∫til para isolar componentes do modelo que exibem diferentes taxas de converg√™ncia, permitindo uma an√°lise mais clara das propriedades assint√≥ticas dos estimadores. Conforme vimos anteriormente, a presen√ßa de tend√™ncias de tempo determin√≠sticas implica que os estimadores dos par√¢metros, como $\alpha$ e $\delta$ no modelo simples $y_t = \alpha + \delta t + \epsilon_t$ [^1], convergem para seus valores verdadeiros a taxas diferentes, necessitando reescalonamento para obter distribui√ß√µes limites n√£o degeneradas. A transforma√ß√£o de Sims, Stock e Watson oferece uma metodologia elegante para lidar com essa complexidade. Este cap√≠tulo se aprofunda na mec√¢nica da transforma√ß√£o e suas implica√ß√µes para a an√°lise assint√≥tica.

### Conceitos Fundamentais

O m√©todo de Sims, Stock e Watson aborda a complexidade das diferentes taxas de converg√™ncia transformando o modelo de regress√£o original em uma forma can√¥nica [^1]. Esta forma can√¥nica isola componentes com diferentes ordens de converg√™ncia, o que facilita a an√°lise da distribui√ß√£o assint√≥tica dos estimadores. Em ess√™ncia, a transforma√ß√£o envolve reescrever as vari√°veis regressores de uma forma que separa vari√°veis estacion√°rias, termos constantes e tend√™ncias de tempo.

Para ilustrar essa transforma√ß√£o, consideremos o modelo autoregressivo de ordem *p* em torno de uma tend√™ncia de tempo determin√≠stica, dado por [16.3.1] [^1]:

$$y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t$$

O objetivo da transforma√ß√£o √© reescrever esse modelo em termos de vari√°veis estacion√°rias de m√©dia zero (as defasagens de *y*), um termo constante e uma tend√™ncia de tempo, permitindo que as diferentes taxas de converg√™ncia sejam tratadas separadamente. O primeiro passo da transforma√ß√£o envolve adicionar e subtrair termos espec√≠ficos para reescrever o modelo [16.3.2] [^1]:

$$y_t = \alpha(1 + \phi_1 + \ldots + \phi_p) + \delta(1 + \phi_1 + 2\phi_2 + \ldots + p\phi_p)t - \delta(\phi_1 + 2\phi_2 + \ldots + p\phi_p) + \phi_1[y_{t-1} - \alpha - \delta(t-1)] + \ldots + \phi_p[y_{t-p} - \alpha - \delta(t-p)] + \epsilon_t$$
Essa reescrita introduz termos da forma $y_{t-j} - \alpha - \delta(t-j)$, que representam as defasagens das vari√°veis  removida da tend√™ncia. Agrupando os termos e definindo novos par√¢metros, chegamos ao modelo transformado [16.3.3] [^1]:

$$y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \ldots + \phi_p^* y_{t-p}^* + \epsilon_t$$
Onde:
  $$\alpha^* = \alpha(1 + \phi_1 + \ldots + \phi_p) - \delta(\phi_1 + 2\phi_2 + \ldots + p\phi_p)$$
  $$\delta^* = \delta(1 + \phi_1 + \ldots + \phi_p)$$
  $$\phi_j^* = \phi_j$$
e
  $$y_{t-j}^* = y_{t-j} - \alpha - \delta(t-j)$$
[16.3.4] [^1]

Esta transforma√ß√£o expressa o modelo original em fun√ß√£o de um termo constante ($\alpha^*$), um termo de tend√™ncia temporal ($\delta^* t$) e vari√°veis estacion√°rias $y_{t-j}^*$. Os termos $y_{t-j}^*$ s√£o defasagens de y ajustadas para a tend√™ncia determin√≠stica, resultando em um componente estacion√°rio. O modelo transformado √© essencialmente o mesmo que o original, mas √© expresso de uma maneira que facilita a an√°lise assint√≥tica dos par√¢metros estimados.

> üí° **Exemplo Num√©rico:**
> Para ilustrar a transforma√ß√£o, vamos considerar um modelo AR(1) com tend√™ncia de tempo determin√≠stica, com os seguintes valores:
>   $$y_t = 2 + 0.5t + 0.8y_{t-1} + \epsilon_t$$
>  onde $\epsilon_t$ s√£o ru√≠dos brancos com m√©dia zero e desvio padr√£o 1. Vamos gerar uma s√©rie temporal com 100 observa√ß√µes e aplicar a transforma√ß√£o de Sims, Stock e Watson para observar como os par√¢metros s√£o modificados. Usando as defini√ß√µes do modelo transformado, temos:
> $$\alpha^* = \alpha(1+\phi_1) - \delta \phi_1 = 2(1+0.8) - 0.5(0.8) = 3.2$$
> $$\delta^* = \delta(1+\phi_1) = 0.5(1+0.8) = 0.9$$
> $$\phi_1^* = \phi_1 = 0.8$$
> Assim, o modelo transformado seria:
> $$y_t = 3.2 + 0.9t + 0.8y_{t-1}^* + \epsilon_t$$
> onde $y_{t-1}^* = y_{t-1} - 2 - 0.5(t-1)$. Observe que os valores de $\alpha^*$ e $\delta^*$ s√£o diferentes de $\alpha$ e $\delta$, enquanto $\phi_1$ permanece igual. A transforma√ß√£o altera os par√¢metros do intercepto e da tend√™ncia, mas mant√©m o coeficiente autoregressivo. A pr√≥xima etapa √© ilustrar a transforma√ß√£o matricial.
>
>  Para demonstrar a transforma√ß√£o matricial, vamos definir a matriz G' para o nosso modelo AR(1) com tend√™ncia, onde p=1.
>
> $$G' = \begin{bmatrix}
> 1 & 0 & 0 \\
> -\alpha+\delta & 1 & 0 \\
> -\delta & 0 & 1
> \end{bmatrix} = \begin{bmatrix}
> 1 & 0 & 0 \\
> -2+0.5 & 1 & 0 \\
> -0.5 & 0 & 1
> \end{bmatrix} = \begin{bmatrix}
> 1 & 0 & 0 \\
> -1.5 & 1 & 0 \\
> -0.5 & 0 & 1
> \end{bmatrix}$$
>
> A inversa de G' √©:
>
> $$(G')^{-1} =  \begin{bmatrix}
> 1 & 0 & 0 \\
> 1.5 & 1 & 0 \\
> 0.5 & 0 & 1
> \end{bmatrix}$$
>
> Vamos supor que temos os dados: $y_0 = 3$, $y_1 = 5.8$, $y_2 = 8.5$.  Criamos o vetor $x_t$ para $t=1$ e $t=2$, com as vari√°veis $y_{t-1}, 1$ e $t$:
> $$x_1 = \begin{bmatrix}
> y_0 \\ 1 \\ 1
> \end{bmatrix} = \begin{bmatrix}
> 3 \\ 1 \\ 1
> \end{bmatrix}$$
>
> $$x_2 = \begin{bmatrix}
> y_1 \\ 1 \\ 2
> \end{bmatrix} = \begin{bmatrix}
> 5.8 \\ 1 \\ 2
> \end{bmatrix}$$
>
> Aplicando a transforma√ß√£o $x_t^* = (G')^{-1}x_t$:
>
> $$x_1^* = \begin{bmatrix}
> 1 & 0 & 0 \\
> 1.5 & 1 & 0 \\
> 0.5 & 0 & 1
> \end{bmatrix} \begin{bmatrix}
> 3 \\ 1 \\ 1
> \end{bmatrix} = \begin{bmatrix}
> 3 \\ 5.5 \\ 2.5
> \end{bmatrix}$$
>
> $$x_2^* = \begin{bmatrix}
> 1 & 0 & 0 \\
> 1.5 & 1 & 0 \\
> 0.5 & 0 & 1
> \end{bmatrix} \begin{bmatrix}
> 5.8 \\ 1 \\ 2
> \end{bmatrix} = \begin{bmatrix}
> 5.8 \\ 9.7 \\ 4.9
> \end{bmatrix}$$
>
> Os vetores $x_t^*$ transformados cont√™m os termos ajustados pela tend√™ncia. Este exemplo demonstra como a transforma√ß√£o matricial altera os regressores para facilitar a an√°lise assint√≥tica.
>
> Vamos agora realizar o exemplo utilizando Python:
>
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> T = 100
> alpha = 2
> delta = 0.5
> phi1 = 0.8
> sigma = 1
>
> # Gerar os dados
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, sigma, T)
> y = np.zeros(T)
> y[0] = alpha + delta*1 + epsilon[0]
> for i in range(1, T):
>   y[i] = alpha + delta*t[i] + phi1*y[i-1] + epsilon[i]
>
> # Criar matrizes X e G
> X = np.column_stack((y[0:T-1], np.ones(T-1), t[0:T-1]))
> G_prime = np.array([
>   [1, 0, 0],
>   [-alpha+delta, 1, 0],
>   [-delta, 0, 1]
> ])
> G_inv = np.linalg.inv(G_prime)
>
> # Aplicar a transforma√ß√£o
> X_star = X @ G_inv.T
>
> # Calcular as estimativas MQO
> model = LinearRegression(fit_intercept=False)
> model.fit(X_star,y[1:T])
> beta_star_hat = model.coef_
>
> # Calcular os estimadores do modelo original
> beta_hat = G_prime @ beta_star_hat
>
> print("Vetor transformado de par√¢metros (beta_star_hat):", beta_star_hat)
> print("Vetor original de par√¢metros (beta_hat):", beta_hat)
>
> #  Os resultados a seguir sao apenas uma demonstra√ß√£o, os valores
> #  estimados variam com a simula√ß√£o
> # Vetor transformado de par√¢metros (beta_star_hat): [ 2.0799  0.4436 -0.0449]
> # Vetor original de par√¢metros (beta_hat): [2.2573 0.4436 -0.0449]
> ```
> Este exemplo demonstra o uso do modelo transformado, o que permite a an√°lise da distribui√ß√£o assint√≥tica com a teoria de matrizes de transforma√ß√£o apresentada.
>

A transforma√ß√£o de Sims, Stock e Watson pode ser descrita algebricamente como uma reescrita do modelo original [16.3.5] [^1]  $y_t = x_t'\beta + \epsilon_t$,  para a forma [16.3.7] [^1] $y_t = x_t'G'(G')^{-1}\beta + \epsilon_t = x_t^{*'}\beta^* + \epsilon_t$, onde a matriz $G'$ √© definida como [16.3.8] [^1]:
$$G' = \begin{bmatrix}
1 & 0 & \ldots & 0 & 0 & 0 \\
0 & 1 & \ldots & 0 & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & \ldots & 1 & 0 & 0 \\
-\alpha+\delta & -\alpha+2\delta & \ldots & -\alpha+p\delta & 1 & 0 \\
-\delta & -\delta & \ldots & -\delta & 0 & 1
\end{bmatrix}$$

O vetor transformado de regressores, $x_t^*$, √© obtido como $x_t^*= (G')^{-1}x_t$ [16.3.9] [^1], onde $x_t$ √© definido em [16.3.6] [^1]:
$$x_t = \begin{bmatrix}
y_{t-1} \\ y_{t-2} \\ \vdots \\ y_{t-p} \\ 1 \\ t
\end{bmatrix}$$

e o vetor transformado de par√¢metros, $\beta^*$, √© obtido por $\beta^* = (G')^{-1}\beta$ [16.3.10] [^1]. Essa transforma√ß√£o matricial encapsula a reescrita do modelo original em termos de componentes estacion√°rios, um termo constante e um termo de tend√™ncia temporal. A transforma√ß√£o garante que as vari√°veis com diferentes taxas de converg√™ncia sejam tratadas de forma apropriada, permitindo que se obtenha uma distribui√ß√£o assint√≥tica para os estimadores.

**Observa√ß√£o 1:** O c√°lculo da matriz $G'$ e da transforma√ß√£o $x_t^* = (G')^{-1}x_t$ envolve manipula√ß√µes matriciais diretas e transforma√ß√µes nos dados. Em termos computacionais, essa transforma√ß√£o exige que os dados sejam previamente ajustados. Por exemplo, para computar $y_{t-j}^*$, os valores de $\alpha$ e $\delta$ teriam que ser usados, o que, na pr√°tica, implica em usar estimativas desses par√¢metros em vez dos valores verdadeiros.

√â crucial entender a natureza da matriz $\sum_{t=1}^T x_t x_t'$ para compreender a necessidade da transforma√ß√£o de Sims, Stock e Watson. Em modelos estacion√°rios, essa matriz, quando dividida por $T$, converge para uma matriz limitante n√£o singular. No entanto, como vimos, no caso de modelos com tend√™ncias de tempo determin√≠sticas, a matriz $\sum_{t=1}^T x_t x_t'$ cresce a uma taxa de $T^3$, o que significa que, ao inv√©s de dividir por T, deve-se dividir por $T^3$ para convergir a uma matriz limitante n√£o singular.

Apesar dessa converg√™ncia, a matriz limite obtida pela divis√£o por $T^3$ √© singular, inviabilizando o uso da abordagem tradicional de invers√£o de matrizes para obter as distribui√ß√µes assint√≥ticas.  √â aqui que a transforma√ß√£o de Sims, Stock e Watson se torna fundamental. Ao transformar os regressores, ela permite que o modelo seja reescrito de tal forma que a matriz $\sum_{t=1}^T x_t^* x_t^{*'}$ possa ser analisada de forma mais clara em termos de seus componentes de diferentes ordens de converg√™ncia. A matriz $Y_T$, usada no reescalonamento dos par√¢metros $\hat{\alpha}_T$ e $\hat{\delta}_T$, √© uma matriz diagonal onde os elementos na diagonal s√£o $\sqrt{T}$ e $T^{3/2}$, respectivamente [^1]. A multiplica√ß√£o de $(b_T - \beta)$ por $Y_T$ permite isolar os componentes da distribui√ß√£o assint√≥tica.

Essa transforma√ß√£o, conforme descrito em [16.1.17] [^1], envolve pr√©-multiplicar a express√£o $(b_T - \beta)$ pela matriz diagonal $Y_T$:
$$ Y_T = \begin{bmatrix}
\sqrt{T} & 0 \\
0 & T^{3/2}
\end{bmatrix}$$

**Lema 1:** A transforma√ß√£o de Sims, Stock e Watson, quando aplicada a um modelo com tend√™ncia de tempo determin√≠stica, isola os componentes com diferentes taxas de converg√™ncia, de forma que:
1. Os componentes associados √†s vari√°veis estacion√°rias convergem a uma taxa de $\sqrt{T}$.
2. O componente associado √† tend√™ncia de tempo ($\delta$) converge a uma taxa de $T^{3/2}$, indicando superconsist√™ncia.
3. O componente associado √† constante ($\alpha$) converge a uma taxa de $\sqrt{T}$.
*Prova:*
I. A transforma√ß√£o de Sims, Stock e Watson √© definida algebricamente como a opera√ß√£o que reescreve o modelo original usando a matriz G e sua inversa.
II. Ao aplicar essa transforma√ß√£o, isolamos os componentes com diferentes taxas de converg√™ncia, ou seja, a componente associada a constante,  a componente associada a tend√™ncia, e a componente associada as vari√°veis estacion√°rias.
III. Ao analisar a matriz transformada $\sum_{t=1}^T x_t^* x_t^{*'}$, observamos que os termos associados √†s vari√°veis estacion√°rias, ao serem multiplicados pela taxa de converg√™ncia de $\sqrt{T}$, convergem para uma matriz limite n√£o singular.
IV. Ao analisar a matriz transformada $\sum_{t=1}^T x_t^* x_t^{*'}$, observamos que os termos associados a tend√™ncia linear, ao serem multiplicados pela taxa de converg√™ncia de $T^{3/2}$, convergem para uma matriz limite n√£o singular.
V. O mesmo ocorre para a constante $\alpha$, que ao ser multiplicada por $\sqrt{T}$, converge para uma matriz limite n√£o singular.
VI. Portanto, ao reescalonar o vetor de par√¢metros $b_T$ por $Y_T$, e devido a converg√™ncia para uma matriz limitante n√£o singular, o teorema da converg√™ncia em distribui√ß√£o se aplica e o vetor resultante converge para uma distribui√ß√£o normal multivariada com covari√¢ncia apropriada.
$\blacksquare$

**Lema 1.1:** A transforma√ß√£o de Sims, Stock e Watson pode ser generalizada para modelos com tend√™ncias polinomiais de ordem superior.
*Prova:*
I. O modelo de tend√™ncia polinomial de ordem *k* pode ser descrito como $y_t = \alpha + \delta_1 t + \delta_2 t^2 + \ldots + \delta_k t^k + \phi_1 y_{t-1} + \ldots + \phi_p y_{t-p} + \epsilon_t$.
II. A transforma√ß√£o de Sims, Stock e Watson envolve a reescrita do modelo com base nas diferentes ordens de converg√™ncia dos componentes.
III. Ao estender a transforma√ß√£o para modelos com tend√™ncias polinomiais, a matriz G' deve ser adaptada para acomodar as diferentes ordens de converg√™ncia. Em particular, cada termo $t^k$ ter√° uma ordem de converg√™ncia de $T^{k+1/2}$, que dever√° ser considerada na constru√ß√£o da matriz de reescalonamento e da matriz G'.
IV. Assim, a forma can√¥nica obtida ap√≥s a transforma√ß√£o incluir√° termos de tend√™ncia com diferentes pot√™ncias de *t* (e suas correspondentes taxas de converg√™ncia), termos constantes e termos associados as vari√°veis estacion√°rias. O tratamento assint√≥tico segue os mesmos princ√≠pios do caso de tend√™ncia linear.
V. A constru√ß√£o da matriz G' em modelos com tend√™ncia polinomial √© uma extens√£o direta do modelo com tend√™ncia linear, onde cada termo adicional com pot√™ncia *k* ser√° adicionado na matriz.
VI. A matriz de reescalonamento $Y_T$ tamb√©m dever√° ser adaptada para incorporar as diferentes taxas de converg√™ncia, onde os elementos da diagonal passam a ser $\sqrt{T}, T^{3/2}, T^{5/2}, \ldots T^{k+1/2}$.
VII. Com isso, a transforma√ß√£o de Sims, Stock e Watson pode ser generalizada para tratar tend√™ncias polinomiais de ordem superior.
$\blacksquare$

### Conclus√£o

O m√©todo de Sims, Stock e Watson √© uma t√©cnica essencial para analisar a distribui√ß√£o assint√≥tica dos estimadores MQO em modelos com tend√™ncias de tempo determin√≠sticas. Ao transformar o modelo de regress√£o em uma forma can√¥nica, essa metodologia permite isolar os componentes que convergem a diferentes taxas, simplificando a deriva√ß√£o da distribui√ß√£o assint√≥tica. A transforma√ß√£o, que envolve manipula√ß√µes matriciais e transforma√ß√µes de dados, √© crucial para lidar com a n√£o estacionariedade das tend√™ncias de tempo. A matriz de covari√¢ncia dos estimadores MQO, ao contr√°rio de regress√µes estacion√°rias, exige divis√£o por $T^3$ para converg√™ncia, resultando em uma matriz limitante n√£o invert√≠vel, o que torna abordagens alternativas necess√°rias. A transforma√ß√£o de Sims, Stock e Watson resolve esse problema e permite an√°lises assint√≥ticas claras e precisas, fornecendo uma base s√≥lida para infer√™ncia estat√≠stica em modelos com tend√™ncias de tempo determin√≠sticas e processos autoregressivos com tend√™ncia. A import√¢ncia dessa transforma√ß√£o reside na sua capacidade de tornar trat√°vel a an√°lise de modelos com m√∫ltiplas taxas de converg√™ncia.

**Corol√°rio 1:**  A transforma√ß√£o de Sims, Stock e Watson, quando aplicada em modelos com componentes de tend√™ncia, n√£o apenas simplifica a deriva√ß√£o da distribui√ß√£o assint√≥tica, mas tamb√©m resulta em estimadores com caracter√≠sticas particulares de converg√™ncia, em especial a superconsist√™ncia para os par√¢metros associados a termos de tend√™ncia.
*Prova:* O teorema 1 detalhou como a transforma√ß√£o de Sims, Stock e Watson √© usada para definir os componentes com diferentes taxas de converg√™ncia.
I. Conforme descrito, a matriz de transforma√ß√£o G isola os componentes da regress√£o com diferentes taxas de converg√™ncia.
II. Ao aplicar esta transforma√ß√£o, obtemos um novo vetor de estimadores $\beta^*$ que pode ser usado para inferir o vetor de estimadores original $\beta$.
III. A superconsist√™ncia do estimador $\hat{\delta}$ da tend√™ncia de tempo √© uma consequ√™ncia direta da ordem de grandeza $T^3$ do denominador da express√£o dos desvios dos estimadores MQO.  Ao multiplicar por $T^{3/2}$, normalizamos essa ordem para que ela convirja para uma matriz n√£o-singular, o que acarreta em uma taxa de converg√™ncia maior para $\hat{\delta}$, que converge para seu valor verdadeiro a uma taxa mais r√°pida que $\sqrt{T}$.
IV. Os outros componentes, como $\alpha$ e $\phi$, ao serem multiplicados por $\sqrt{T}$, convergem para seus valores verdadeiros a uma taxa de converg√™ncia da ordem de $\sqrt{T}$.
$\blacksquare$

**Corol√°rio 1.1:** A transforma√ß√£o de Sims, Stock e Watson, devido √†s suas propriedades de isolamento das diferentes taxas de converg√™ncia, permite a deriva√ß√£o das distribui√ß√µes assint√≥ticas dos estimadores de par√¢metros mesmo em modelos com m√∫ltiplas tend√™ncias de tempo e componentes autoregressivos.
*Prova:*
I. A transforma√ß√£o de Sims, Stock e Watson, como demonstrado pelo Lema 1 e Lema 1.1, separa componentes com diferentes taxas de converg√™ncia, tanto tend√™ncias lineares como tend√™ncias polinomiais.
II. Devido a essa propriedade, a matriz $\sum_{t=1}^T x_t^* x_t^{*'}$  pode ser analisada separadamente por seus componentes com diferentes taxas de converg√™ncia.
III.  Ao reescalonar os estimadores originais $\beta$ atrav√©s da matriz $Y_T$, que contem na diagonal a ordem de converg√™ncia de cada componente, a distribui√ß√£o assint√≥tica resultante converge para uma distribui√ß√£o normal multivariada com covari√¢ncia apropriada.
IV.  Portanto, com a transforma√ß√£o, a deriva√ß√£o das distribui√ß√µes assint√≥ticas se torna direta e possibilita an√°lises de modelos complexos que incluem componentes autoregressivos e tend√™ncias de tempo de diferentes graus.
$\blacksquare$
### Refer√™ncias
[^1]: Trechos do cap√≠tulo 16 do livro "Processes with Deterministic Time Trends", conforme fornecido no contexto.
<!-- END -->
