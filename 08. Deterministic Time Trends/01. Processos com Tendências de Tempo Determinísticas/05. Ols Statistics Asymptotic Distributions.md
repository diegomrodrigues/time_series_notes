## Testes de Hipóteses com MQO em Modelos de Tendência de Tempo Determinística

### Introdução
Em continuidade aos tópicos anteriores sobre a análise assintótica de estimadores MQO em modelos com tendências de tempo determinísticas, este capítulo aborda a realização de testes de hipóteses utilizando as estatísticas *t* e *F* usuais [^1]. Exploramos que, apesar das diferentes taxas de convergência das estimativas, as estatísticas de teste baseadas em MQO mantêm suas distribuições assintóticas padrão sob certas condições. Isso significa que, embora as estimativas de parâmetros individuais, como $\hat{\alpha}_T$ e $\hat{\delta}_T$, convirjam a taxas distintas, as estatísticas construídas para testar hipóteses sobre esses parâmetros seguem as distribuições *t* e *F* usuais quando o tamanho da amostra tende ao infinito. Este capítulo é crucial para a aplicação prática da teoria desenvolvida anteriormente, mostrando que a inferência estatística em modelos com tendência determinística pode ser realizada de maneira análoga à inferência em modelos estacionários, após as devidas transformações e reescalonamentos.

### Conceitos Fundamentais

O objetivo principal deste capítulo é demonstrar que os testes de hipóteses construídos com estimadores MQO em modelos com tendência de tempo determinística mantêm validade assintótica [^1]. Especificamente, as estatísticas *t* e *F* calculadas a partir das estimativas MQO, apesar das diferentes taxas de convergência dos estimadores, convergem para as distribuições usuais sob certas condições. Esse resultado é essencial para a prática, pois permite que os pesquisadores utilizem os procedimentos de teste de hipóteses padrão, com a devida atenção à particularidade dos modelos de tendência.

Para verificar a validade assintótica das estatísticas *t*, vamos começar com o teste de hipótese para o coeficiente constante, $\alpha$. A hipótese nula é que $\alpha$ é igual a um valor específico $\alpha_0$, isto é, $H_0: \alpha = \alpha_0$ [^1]. A estatística *t* usual para esse teste é dada por:
$$t_{\alpha} = \frac{\hat{\alpha}_T - \alpha_0}{SE(\hat{\alpha}_T)}$$
onde $SE(\hat{\alpha}_T)$ é o erro padrão do estimador $\hat{\alpha}_T$ [^1].

Para obter a distribuição assintótica da estatística $t_{\alpha}$, multiplicamos o numerador e o denominador por $\sqrt{T}$ e utilizamos a matriz $Y_T$ definida anteriormente para reescalonar o vetor de estimadores:

$$t_{\alpha} = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{s_T \left[ [1 \, 0](X_T'X_T)^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} \right]^{1/2} }$$
onde $s_T^2$ é o estimador MQO da variância do erro $\epsilon_t$.

Utilizando a propriedade da transformação de Sims, Stock e Watson, podemos reescrever a expressão acima e, através de [16.2.6] [^1] e [16.1.19] [^1], podemos mostrar que:
$$t_\alpha =  \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{\sqrt{s_T^2 q^{11}}}$$
onde $q^{11}$ é o elemento (1,1) da matriz $Q^{-1}$, o limite da matriz de covariância de $(1/T^3)\sum_{t=1}^T x_t x_t'$ [^1].  Como $\sqrt{T}(\hat{\alpha}_T - \alpha_0)$ converge para uma distribuição normal com média zero e variância $\sigma^2 q^{11}$ [^1], e $s_T^2$ converge em probabilidade para $\sigma^2$, a estatística $t_\alpha$ converge para uma distribuição normal padrão $N(0,1)$ [^1]. Isso implica que o teste *t* usual para o coeficiente constante é assintoticamente válido.

> 💡 **Exemplo Numérico:**
> Para ilustrar, vamos simular um conjunto de dados com tendência determinística e aplicar o teste t para verificar a validade assintótica.
> ```python
> import numpy as np
> import pandas as pd
> import scipy.stats as st
> from sklearn.linear_model import LinearRegression
> import matplotlib.pyplot as plt
>
> # Parâmetros
> num_simulations = 200
> T = 200
> alpha = 5
> delta = 0.2
> sigma = 1.5
> alpha_0 = 5
>
> t_stats = []
> for _ in range(num_simulations):
>  # Gerar os dados
>  t = np.arange(1, T + 1)
>  epsilon = np.random.normal(0, sigma, T)
>  y = alpha + delta * t + epsilon
>
>  # Criar a matriz X
>  X = np.column_stack((np.ones(T), t))
>
>  # Calcular as estimativas MQO
>  model = LinearRegression()
>  model.fit(X, y)
>  alpha_hat = model.intercept_
>  delta_hat = model.coef_[1]
>
>  # Calcular os resíduos
>  residuals = y - model.predict(X)
>
>  # Calcular o erro padrão
>  MSE = np.sum(residuals**2)/(T-2)
>  XTX_inv = np.linalg.inv(X.T @ X)
>  std_err_alpha = np.sqrt(MSE*XTX_inv[0,0])
>
>  # Calcular a estatística de teste
>  t_statistic = (alpha_hat - alpha_0)/ std_err_alpha
>  t_stats.append(t_statistic)
>
>
> # Análise dos resultados
> df = pd.DataFrame({'t_statistic': t_stats})
>
> # Plotar o histograma da estatística t
> plt.figure(figsize=(8, 6))
> plt.hist(df['t_statistic'], bins=30, density=True, alpha=0.7, label = 't_statistic')
> x = np.linspace(-4, 4, 100)
> plt.plot(x, st.norm.pdf(x, 0, 1), label = 'N(0, 1)', color = 'r')
> plt.xlabel('Estatística t')
> plt.ylabel('Densidade')
> plt.title('Distribuição da Estatística t para alpha')
> plt.legend()
> plt.show()
>
> ```
> O histograma gerado pela simulação da estatística t se aproxima da distribuição normal padrão, ilustrando sua validade assintótica.
>
> Para um exemplo numérico específico, digamos que após rodar uma regressão em um conjunto de dados simulados, obtemos $\hat{\alpha}_T = 5.2$, $SE(\hat{\alpha}_T) = 0.15$, e queremos testar $H_0: \alpha = 5$. Então, a estatística t seria:
> $$t_{\alpha} = \frac{5.2 - 5}{0.15} = \frac{0.2}{0.15} \approx 1.33$$
>  Este valor de $t_{\alpha}$ pode então ser comparado com os valores críticos da distribuição normal padrão para decidir sobre a rejeição ou não da hipótese nula.

Um resultado similar pode ser obtido para o teste de hipótese para o coeficiente da tendência temporal, $\delta$. A hipótese nula agora é $H_0: \delta = \delta_0$. A estatística *t* correspondente é:
$$t_{\delta} = \frac{\hat{\delta}_T - \delta_0}{SE(\hat{\delta}_T)}$$
onde $SE(\hat{\delta}_T)$ é o erro padrão do estimador $\hat{\delta}_T$.
Multiplicamos o numerador e o denominador por $T^{3/2}$ para considerar a ordem de convergência de $\hat{\delta}_T$:

$$t_{\delta} = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{s_T \left[ [0 \, 1](X_T'X_T)^{-1} \begin{bmatrix} 0 \\ 1 \end{bmatrix} \right]^{1/2} }$$
Através de [16.2.6] [^1] e [16.1.19] [^1], podemos obter a distribuição limite de $t_\delta$ usando a matriz $Y_T$,  e a estatística resultante tem uma distribuição assintótica $N(0,1)$:
$$t_\delta =  \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\sqrt{s_T^2 q^{22}}}$$
onde $q^{22}$ é o elemento (2,2) da matriz $Q^{-1}$.
*Prova:*
I. A estatística *t* para o coeficiente $\delta$ é dada por $t_\delta = \frac{\hat{\delta}_T - \delta_0}{SE(\hat{\delta}_T)}$.
II. Multiplicamos o numerador e o denominador por $T^{3/2}$ para levar em consideração a taxa de convergência de $\hat{\delta}_T$, resultando em $t_{\delta} = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{T^{3/2}SE(\hat{\delta}_T)}$.
III. O erro padrão do estimador $\hat{\delta}_T$ é dado por $SE(\hat{\delta}_T) = s_T \left[ [0 \, 1](X_T'X_T)^{-1} \begin{bmatrix} 0 \\ 1 \end{bmatrix} \right]^{1/2}$.
IV. Substituindo na equação II, obtemos: $t_{\delta} = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{s_T \left[ [0 \, 1](X_T'X_T)^{-1} \begin{bmatrix} 0 \\ 1 \end{bmatrix} \right]^{1/2} }$.
V. Pela transformação de Sims, Stock e Watson, temos que $T^{3/2} \left[ [0 \, 1](X_T'X_T)^{-1} \begin{bmatrix} 0 \\ 1 \end{bmatrix} \right]^{1/2}$ converge para $\sqrt{q^{22}}$, e assim $t_\delta =  \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\sqrt{s_T^2 q^{22}}}$.
VI. Sabemos que $T^{3/2}(\hat{\delta}_T - \delta_0)$ converge para uma distribuição normal com média zero e variância $\sigma^2 q^{22}$, e $s_T^2$ converge em probabilidade para $\sigma^2$.
VII. Portanto, $t_\delta$ converge para uma distribuição normal padrão $N(0,1)$.
$\blacksquare$

> 💡 **Exemplo Numérico:**
> De forma semelhante, vamos simular dados e aplicar o teste t para $\delta$ para verificar sua distribuição assintótica.
> ```python
> import numpy as np
> import pandas as pd
> import scipy.stats as st
> from sklearn.linear_model import LinearRegression
> import matplotlib.pyplot as plt
>
> # Parâmetros
> num_simulations = 200
> T = 200
> alpha = 5
> delta = 0.2
> sigma = 1.5
> delta_0 = 0.2
>
> t_stats = []
> for _ in range(num_simulations):
>  # Gerar os dados
>  t = np.arange(1, T + 1)
>  epsilon = np.random.normal(0, sigma, T)
>  y = alpha + delta * t + epsilon
>
>  # Criar a matriz X
>  X = np.column_stack((np.ones(T), t))
>
>  # Calcular as estimativas MQO
>  model = LinearRegression()
>  model.fit(X, y)
>  alpha_hat = model.intercept_
>  delta_hat = model.coef_[1]
>
>  # Calcular os resíduos
>  residuals = y - model.predict(X)
>
>  # Calcular o erro padrão
>  MSE = np.sum(residuals**2)/(T-2)
>  XTX_inv = np.linalg.inv(X.T @ X)
>  std_err_delta = np.sqrt(MSE*XTX_inv[1,1])
>
>  # Calcular a estatística de teste
>  t_statistic = (delta_hat - delta_0)/ std_err_delta
>  t_stats.append(t_statistic)
>
>
> # Análise dos resultados
> df = pd.DataFrame({'t_statistic': t_stats})
>
> # Plotar o histograma da estatística t
> plt.figure(figsize=(8, 6))
> plt.hist(df['t_statistic'], bins=30, density=True, alpha=0.7, label = 't_statistic')
> x = np.linspace(-4, 4, 100)
> plt.plot(x, st.norm.pdf(x, 0, 1), label = 'N(0, 1)', color = 'r')
> plt.xlabel('Estatística t')
> plt.ylabel('Densidade')
> plt.title('Distribuição da Estatística t para delta')
> plt.legend()
> plt.show()
> ```
> O histograma da estatística t para o coeficiente $\delta$ também se aproxima da normal padrão, demonstrando a validade assintótica do teste.
>
> Para ilustrar com números, suponha que, em um conjunto de dados, $\hat{\delta}_T = 0.23$, $SE(\hat{\delta}_T) = 0.01$, e queremos testar $H_0: \delta = 0.2$. A estatística t seria:
> $$t_{\delta} = \frac{0.23 - 0.2}{0.01} = \frac{0.03}{0.01} = 3$$
> Assim como no exemplo de $\alpha$, este valor pode ser comparado com a distribuição normal padrão para verificar a significância estatística.

De forma análoga, é possível verificar a validade assintótica do teste *F* para uma hipótese conjunta sobre $\alpha$ e $\delta$. Especificamente, vamos considerar a hipótese nula de que um conjunto de restrições lineares sobre os coeficientes $\alpha$ e $\delta$ são verdadeiras, ou seja, $H_0: R\beta = r$, onde $R$ é uma matriz de restrições e $r$ é um vetor de constantes [^1].  A estatística *F* é dada por:

$$F = \frac{(R\hat{\beta} - r)' [R(X'X)^{-1} R']^{-1}(R\hat{\beta} - r)}{m s_T^2}$$

onde *m* é o número de restrições [^1].

A análise da distribuição assintótica da estatística *F* também envolve o reescalonamento apropriado dos estimadores e a utilização das propriedades das matrizes $X_T'X_T$. Ao multiplicar o numerador e denominador por $T$, e usando a propriedade da transformação de Sims, Stock e Watson, é possível mostrar que a estatística *F* converge para uma distribuição *qui-quadrado* com *m* graus de liberdade, dividida por *m*, quando o tamanho da amostra *T* tende ao infinito. Portanto, sob a hipótese nula, a estatística *F* tem uma distribuição assintótica *F* com *m* e *T-k* graus de liberdade (onde *k* é o número de parâmetros estimados).
*Prova:*
I. A estatística F para testar a hipótese conjunta $H_0: R\beta=r$ é dada por:
    $F = \frac{(R\hat{\beta} - r)' [R(X'X)^{-1} R']^{-1}(R\hat{\beta} - r)}{m s_T^2}$.
II. Multiplicamos o numerador e o denominador por $T$ para ajustar a taxa de convergência, resultando em:
    $F = \frac{T(R\hat{\beta} - r)' [R(X'X)^{-1} R']^{-1}(R\hat{\beta} - r)}{m T s_T^2}$.
III. Pela transformação de Sims, Stock e Watson, sabemos que $T(X'X)^{-1}$ converge para $Q^{-1}$, e $s_T^2$ converge em probabilidade para $\sigma^2$.
IV. Também sabemos que $T^{1/2}(\hat{\beta} - \beta)$ converge para uma distribuição normal com média zero e matriz de covariância $\sigma^2 Q^{-1}$. Assim, $T^{1/2}(R\hat{\beta} - R\beta)$ converge para uma distribuição normal com média zero e matriz de covariância $\sigma^2 R Q^{-1}R'$.
V. Sob a hipótese nula $R\beta = r$, a estatística $T(R\hat{\beta}-r)'[R(X'X)^{-1}R']^{-1}(R\hat{\beta}-r)$ converge para uma distribuição qui-quadrado com $m$ graus de liberdade.
VI.  Portanto, $\frac{T(R\hat{\beta} - r)' [R(X'X)^{-1} R']^{-1}(R\hat{\beta} - r)}{m T s_T^2}$ converge para uma distribuição qui-quadrado dividida pelos seus graus de liberdade, que é equivalente a uma distribuição F com m e T-k graus de liberdade.
$\blacksquare$

> 💡 **Exemplo Numérico:**
> Vamos agora verificar a distribuição da estatística F realizando um teste de hipótese conjunta. Vamos testar a hipótese nula de que $\alpha=5$ e $\delta = 0.2$.
> ```python
> import numpy as np
> import pandas as pd
> import scipy.stats as st
> from sklearn.linear_model import LinearRegression
> import matplotlib.pyplot as plt
>
> # Parâmetros
> num_simulations = 200
> T = 200
> alpha = 5
> delta = 0.2
> sigma = 1.5
>
> # Parâmetros da hipótese nula
> alpha_0 = 5
> delta_0 = 0.2
>
> f_stats = []
>
> for _ in range(num_simulations):
>  # Gerar os dados
>  t = np.arange(1, T + 1)
>  epsilon = np.random.normal(0, sigma, T)
>  y = alpha + delta * t + epsilon
>
>  # Criar a matriz X
>  X = np.column_stack((np.ones(T), t))
>
>  # Calcular as estimativas MQO
>  model = LinearRegression()
>  model.fit(X, y)
>  beta_hat = np.array([model.intercept_, model.coef_[1]])
>
>  # Calcular os resíduos
>  residuals = y - model.predict(X)
>
>  # Calcular o erro padrão
>  MSE = np.sum(residuals**2)/(T-2)
>
>  # Definir a matriz de restrições
>  R = np.array([[1, 0], [0, 1]])
>  r = np.array([alpha_0, delta_0])
>  # Calcular a estatística F
>  XTX_inv = np.linalg.inv(X.T @ X)
>  F_statistic = (R@beta_hat - r).T @ np.linalg.inv(R @ XTX_inv @ R.T) @ (R@beta_hat - r) / (2 * MSE)
>  f_stats.append(F_statistic)
>
> # Análise dos resultados
> df = pd.DataFrame({'f_statistic': f_stats})
>
> # Plotar o histograma da estatística F
> plt.figure(figsize=(8, 6))
> plt.hist(df['f_statistic'], bins=30, density=True, alpha=0.7, label = 'f_statistic')
> x = np.linspace(0, 10, 100)
> plt.plot(x, st.f.pdf(x, 2, T-2), label = 'F(2,T-2)', color = 'r')
> plt.xlabel('Estatística F')
> plt.ylabel('Densidade')
> plt.title('Distribuição da Estatística F para alpha e delta')
> plt.legend()
> plt.show()
> ```
> O histograma da estatística F simulada se aproxima da distribuição F teórica com 2 e T-2 graus de liberdade, ilustrando a validade assintótica do teste conjunto.
>
>  Para um exemplo numérico, suponha que temos as seguintes estimativas: $\hat{\alpha} = 5.3$, $\hat{\delta} = 0.22$, com uma matriz de covariância estimada de  $[R(X'X)^{-1}R']^{-1} = \begin{bmatrix} 0.01 & 0 \\ 0 & 0.0001 \end{bmatrix}$ e $MSE=2.25$.
> Testamos $H_0: \alpha=5, \delta=0.2$, com $R = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$ e $r = \begin{bmatrix} 5 \\ 0.2 \end{bmatrix}$.
>  A estatística F é calculada como:
>
> $\text{Step 1: } R\hat{\beta} - r = \begin{bmatrix} 5.3 \\ 0.22 \end{bmatrix} -  \begin{bmatrix} 5 \\ 0.2 \end{bmatrix} = \begin{bmatrix} 0.3 \\ 0.02 \end{bmatrix}$
>
> $\text{Step 2: } (R\hat{\beta} - r)' [R(X'X)^{-1} R']^{-1}(R\hat{\beta} - r) = \begin{bmatrix} 0.3 & 0.02 \end{bmatrix} \begin{bmatrix} 0.01 & 0 \\ 0 & 0.0001 \end{bmatrix}^{-1} \begin{bmatrix} 0.3 \\ 0.02 \end{bmatrix}$
>
>  $\text{Step 3: } = \begin{bmatrix} 0.3 & 0.02 \end{bmatrix} \begin{bmatrix} 100 & 0 \\ 0 & 10000 \end{bmatrix} \begin{bmatrix} 0.3 \\ 0.02 \end{bmatrix} = (0.3 * 100 * 0.3) + (0.02 * 10000 * 0.02) = 9 + 4 = 13$
>
>  $\text{Step 4: }  F = \frac{13}{2 * 2.25} =  \frac{13}{4.5} \approx 2.89$
> Este valor de F pode ser comparado com a distribuição F apropriada para determinar a significância estatística.

**Teorema 1:** (Validade Assintótica das Estatísticas t e F) As estatísticas *t* e *F* usuais, calculadas com base em estimadores MQO para modelos com tendências de tempo determinísticas, mantêm suas distribuições assintóticas padrão (normal padrão para *t* e *F* com graus de liberdade apropriados) quando o tamanho da amostra tende ao infinito.
*Prova:*
I. As estatísticas *t* e *F* são definidas como transformações dos estimadores MQO e seus respectivos erros padrões.
II. Como visto anteriormente, os estimadores MQO em modelos de tendência determinística convergem a taxas diferentes. Para analisar sua distribuição assintótica, é necessário usar a matriz de reescalonamento $Y_T$.
III. No caso da estatística *t*, a reescalonamento pelo $\sqrt{T}$ para o intercepto $\alpha$ e $T^{3/2}$ para a tendência $\delta$ é uma consequência direta das suas diferentes taxas de convergência. Ao reescalonar, garantimos que os termos convergem para distribuições normais com médias zero.
IV. Ao aplicar essa transformação, podemos mostrar que a estatística *t* (tanto para o coeficiente constante como para o da tendência) converge para uma distribuição normal padrão (com média zero e variância unitária).
V. No caso da estatística *F*, que é usada para testar hipóteses conjuntas sobre os parâmetros do modelo, o reescalonamento é análogo. Ao aplicar a transformação de Sims, Stock e Watson, a estatística *F* converge para uma distribuição *qui-quadrado* dividida por seus graus de liberdade, o que é equivalente à distribuição *F*.
VI. Portanto, as estatísticas *t* e *F* usuais, quando aplicadas em modelos com tendências de tempo determinísticas, convergem para suas distribuições assintóticas padrão.
$\blacksquare$

**Observação 1:** É importante ressaltar que a validade assintótica das estatísticas *t* e *F* se baseia no pressuposto de que os erros $\epsilon_t$ são i.i.d. com média zero e variância constante. A violação dessas suposições pode levar a resultados de teste inválidos. **Além disso, a presença de autocorrelação nos resíduos pode afetar a validade desses testes, exigindo ajustes nos erros padrões para garantir a inferência correta.**

**Teorema 1.1:** (Robustez das Estatísticas t e F a Heterocedasticidade) Se a suposição de homocedasticidade dos erros for violada, mas a sequência de erros $\epsilon_t$ ainda satisfizer as condições de estacionariedade fracamente dependente, as estatísticas t e F, quando calculadas usando erros padrões robustos à heterocedasticidade, também manterão sua validade assintótica.
*Prova:*
I. A heterocedasticidade, ou seja, a não constância da variância dos erros, pode levar a erros padrões inconsistentes para os estimadores MQO.
II. No entanto, utilizando estimadores de variância robustos à heterocedasticidade, como o estimador de White (HC), podemos obter erros padrões consistentes mesmo na presença de heterocedasticidade.
III. Esses estimadores de variância ajustam a matriz de covariância dos estimadores para levar em conta a possível variação da variância dos erros ao longo do tempo.
IV. Aplicando os erros padrões robustos na construção das estatísticas *t* e *F*, e mantendo os reescalonamentos de $\sqrt{T}$ e $T^{3/2}$ para os coeficientes, as mesmas converge para distribuições normais padrão (para *t*) e *F* (para *F*).
V. Portanto, as estatísticas *t* e *F* com erros padrões robustos são válidas assintoticamente, mesmo na presença de heterocedasticidade.
$\blacksquare$
> 💡 **Exemplo Numérico:**
> Vamos simular um exemplo de como usar erros padrão robustos. Para isso, vamos gerar dados com heterocedasticidade.
> ```python
> import numpy as np
> import pandas as pd
> import scipy.stats as st
> from sklearn.linear_model import LinearRegression
> import statsmodels.api as sm
> import matplotlib.pyplot as plt
>
> # Parâmetros
> num_simulations = 200
> T = 200
> alpha = 5
> delta = 0.2
>
> alpha_0 = 5
> delta_0 = 0.2
>
> t_stats_robust_alpha = []
> t_stats_robust_delta = []
>
> for _ in range(num_simulations):
>    # Gerar os dados
>    t = np.arange(1, T + 1)
>    sigma_t = 1.5 * (1 + 0.01 * t)  # Variância que aumenta com o tempo
>    epsilon = np.random.normal(0, sigma_t, T)
>    y = alpha + delta * t + epsilon
>
>    # Criar a matriz X
>    X = np.column_stack((np.ones(T), t))
>    X = sm.add_constant(X) # adicionar constante para o statsmodels
>
>    # Calcular as estimativas MQO com statsmodels para obter erros robustos
>    model = sm.OLS(y, X)
>    results = model.fit(cov_type='HC3')
>    alpha_hat = results.params[0]
>    delta_hat = results.params[1]
>    std_err_alpha_robust = results.bse[0]
>    std_err_delta_robust = results.bse[1]
>
>    # Calcular a estatística de teste
>    t_statistic_robust_alpha = (alpha_hat - alpha_0) / std_err_alpha_robust
>    t_statistic_robust_delta = (delta_hat - delta_0) / std_err_delta_robust
>    t_stats_robust_alpha.append(t_statistic_robust_alpha)
>    t_stats_robust_delta.append(t_statistic_robust_delta)
>
>
> # Análise dos resultados
> df_robust_alpha = pd.DataFrame({'t_statistic_robust': t_stats_robust_alpha})
> df_robust_delta = pd.DataFrame({'t_statistic_robust': t_stats_robust_delta})
>
> # Plotar o histograma da estatística t para alpha
> plt.figure(figsize=(12, 6))
> plt.subplot(1, 2, 1)
> plt.hist(df_robust_alpha['t_statistic_robust'], bins=30, density=True, alpha=0.7, label = 't_statistic_robust')
> x = np.linspace(-4, 4, 100)
> plt.plot(x, st.norm.pdf(x, 0, 1), label = 'N(0, 1)', color = 'r')
> plt.xlabel('Estatística t (robusto)')
> plt.ylabel('Densidade')
> plt.title('Distribuição da Estatística t para alpha (robusto)')
> plt.legend()
>
> # Plotar o histograma da estatística t para delta
> plt.subplot(1, 2, 2)
> plt.hist(df_robust_delta['t_statistic_robust'], bins=30, density=True, alpha=0.7, label = 't_statistic_robust')
> x = np.linspace(-4, 4, 100)
> plt.plot(x, st.norm.pdf(x, 0, 1), label = 'N(0, 1)', color = 'r')
> plt.xlabel('Estatística t (robusto)')
> plt.ylabel('Densidade')
> plt.title('Distribuição da Estatística t para delta (robusto)')
> plt.legend()
>
> plt.tight_layout()
> plt.show()
> ```
> Os histogramas da estatística t para $\alpha$ e $\delta$ calculadas com erros padrões robustos se aproximam da normal padrão, demonstrando que mesmo na presença de heterocedasticidade, podemos obter resultados válidos com o uso de erros padrões robustos.

### Conclusão

A análise detalhada neste capítulo demonstra que, apesar das complexidades introduzidas pelas tendências de tempo determinísticas, as estatísticas *t* e *F* usuais de MQO mantêm suas distribuições assintóticas padrão [^1]. Isso significa que, após realizar a análise de transformação e reescalonamento para os estimadores, a inferência estatística baseada nesses testes é válida. A aplicação da transformação de Sims, Stock e Watson e o reescalonamento adequado são cruciais para obter os resultados desejados. A compreensão dessas nuances é fundamental para uma análise correta de modelos de séries temporais que incluem tendências de tempo determinísticas. Este capítulo conclui a discussão dos processos com tendências determinísticas, fornecendo as ferramentas necessárias para a análise e inferência em tais modelos.

**Corolário 1:** A validade assintótica das estatísticas *t* e *F* em modelos com tendência determinística garante que os testes de hipótese realizados para verificar a significância dos coeficientes, sejam eles associados à constante, à tendência ou mesmo a restrições conjuntas entre eles, são estatisticamente válidos, desde que sejam aplicados os devidos ajustes (reescalonamento) para lidar com as diferentes taxas de convergência.
*Prova:*
I. A validade assintótica das estatísticas *t* e *F* significa que suas distribuições amostrais, quando o tamanho da amostra se torna grande, se aproximam das distribuições teóricas (normal padrão para o teste *t* e F para o teste *F*).
II. O reescalonamento da estatística t por $\sqrt{T}$ e $T^{3/2}$ para os coeficientes $\alpha$ e $\delta$, respectivamente, permite que se obtenha uma distribuição limite normal.
III. O uso da transformação de Sims, Stock e Watson e o reescalonamento correto para as estatísticas *F* também permite que se obtenha a distribuição limite *qui-quadrado*.
IV. Portanto, o uso dessas estatísticas para testes de hipótese é válido em amostras grandes, após as devidas transformações e reescalonamentos.
$\blacksquare$

**Corolário 2:** A validade assintótica das estatísticas *t* e *F* é fundamental para a construção de intervalos de confiança para os parâmetros dos modelos de tendência determinística, pois esses intervalos de confiança são baseados nos erros padrões dos estimadores, que por sua vez são componentes dessas estatísticas.
*Prova:*
I. A construção de intervalos de confiança é baseada na distribuição dos estimadores e seus erros padrão.
II. Como a distribuição da estatística t (que utiliza o erro padrão do estimador) converge para uma distribuição normal padrão, podemos construir intervalos de confiança para os parâmetros do modelo, utilizando essa distribuição.
III. Por exemplo, um intervalo de confiança de 95% para o parâmetro $\alpha$ será obtido usando a estimativa $\hat{\alpha}$ e o erro padrão $SE(\hat{\alpha})$, usando a distribuição normal padrão com quantil de 1.96.
IV. O mesmo argumento se aplica aos outros parâmetros e ao teste F para hipótese conjuntas.
$\blacksquare$
> 💡 **Exemplo Numérico:**
>  Suponha que após a análise, obtemos $\hat{\alpha} = 5.2$,$\hat{\beta} = 2.3$, e os erros padrões são $se(\hat{\alpha}) = 0.5$ e $se(\hat{\beta}) = 0.2$. Para um nível de significância de 5%, o intervalo de confiança de 95% para $\alpha$ seria $5.2 \pm 1.96 \times 0.5$, ou seja, $[4.22, 6.18]$. Similarmente, o intervalo de confiança de 95% para $\beta$ seria $2.3 \pm 1.96 \times 0.2$, ou seja, $[1.908, 2.692]$.

### 3. Inferência Causal

A regressão linear é uma ferramenta poderosa, mas é crucial entender suas limitações, especialmente em termos de causalidade.

#### 3.1. Correlação vs. Causalidade

É fundamental reconhecer que correlação não implica causalidade. A regressão linear pode mostrar uma relação estatística entre variáveis, mas não prova que uma variável causa mudanças na outra.

> ⚠️ **Atenção:**
>  A simples observação de uma relação entre $X$ e $Y$ não permite concluir que $X$ causa $Y$.

#### 3.2. Variáveis Omitidas e Viés

Um problema comum em regressões é o viés de variáveis omitidas. Se uma variável importante, correlacionada tanto com a variável dependente quanto com as variáveis independentes, for omitida da análise, os resultados podem ser enviesados.

> ❗ **Cuidado:**
>  A omissão de variáveis relevantes pode levar a conclusões errôneas sobre a relação causal entre as variáveis.

#### 3.3. Variáveis de Controle

Para lidar com o problema do viés de variáveis omitidas, é comum incluir variáveis de controle na análise. Estas variáveis capturam outras influências na variável dependente, permitindo isolar melhor o efeito da variável de interesse.

#### 3.4. Experimentos e Quase-Experimentos

A maneira mais robusta de estabelecer causalidade é através de experimentos controlados. No entanto, em muitas situações, experimentos não são viáveis. Nesses casos, técnicas de quase-experimentos podem ser usadas, como a variável instrumental ou a regressão de descontinuidade, para tentar identificar relações causais.

### 4. Extensões da Regressão Linear

A regressão linear tem várias extensões que permitem modelar relações mais complexas entre variáveis.

#### 4.1. Regressão Polinomial

Em vez de uma relação linear, a variável dependente pode ter uma relação curvilínea com as variáveis independentes. Nesses casos, podemos usar a regressão polinomial, onde incluímos termos polinomiais (quadráticos, cúbicos, etc.) das variáveis independentes.

#### 4.2. Regressão Logística

A regressão linear é adequada para variáveis dependentes contínuas. Para variáveis dependentes binárias (por exemplo, sim/não), usamos a regressão logística, que modela a probabilidade da variável dependente assumir um valor particular.

#### 4.3. Regressão com Interações

Às vezes, o efeito de uma variável independente sobre a variável dependente depende do valor de outra variável independente. Nesses casos, podemos incluir termos de interação na regressão.

> ✔️ **Nota:**
>  A escolha do modelo de regressão apropriado depende da natureza dos dados e da questão de pesquisa.

#### 4.4. Regressão com Efeitos Fixos e Aleatórios

Em dados com estrutura hierárquica, como dados longitudinais ou dados agrupados, podemos usar modelos de efeitos fixos ou aleatórios para modelar a variabilidade entre grupos ou indivíduos.

### 5. Conclusão

A regressão linear é uma ferramenta estatística versátil e amplamente utilizada. Dominar suas nuances e limitações é essencial para qualquer profissional que trabalhe com análise de dados. Ao longo deste capítulo, exploramos os fundamentos, a inferência, as nuances causais e as extensões da regressão linear, fornecendo uma base sólida para entender e aplicar essa técnica em uma variedade de contextos.

<!-- END -->
