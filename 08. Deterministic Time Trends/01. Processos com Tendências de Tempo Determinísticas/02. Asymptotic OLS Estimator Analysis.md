## Distribui√ß√µes Assint√≥ticas de Estimadores MQO em Modelos de Tend√™ncia de Tempo Determin√≠stica

### Introdu√ß√£o
Neste cap√≠tulo, exploramos a an√°lise da distribui√ß√£o assint√≥tica das estimativas de M√≠nimos Quadrados Ordin√°rios (MQO) em modelos que incorporam tend√™ncias de tempo determin√≠sticas. Como vimos anteriormente, no estudo de s√©ries temporais com componentes estacion√°rios, as distribui√ß√µes assint√≥ticas dos estimadores MQO s√£o frequentemente abordadas multiplicando-se o desvio do estimador pelo fator $\sqrt{T}$ [^1]. Entretanto, ao lidar com modelos que apresentam tend√™ncias de tempo determin√≠sticas, essa abordagem simples n√£o √© mais suficiente. Os estimadores dos coeficientes associados √†s tend√™ncias de tempo determin√≠sticas exibem *taxas de converg√™ncia assint√≥tica* distintas, o que exige t√©cnicas de an√°lise mais sofisticadas para derivar distribui√ß√µes limite n√£o degeneradas. Este t√≥pico √© crucial para a compreens√£o da infer√™ncia estat√≠stica em modelos com componentes de tend√™ncia determin√≠stica e introduz a necessidade de reescalonamento das vari√°veis para lidar com diferentes ordens de converg√™ncia.

**Observa√ß√£o 1:** √â importante ressaltar que a necessidade de reescalonamento surge porque as vari√°veis de tend√™ncia, como o tempo $t$, n√£o s√£o estacion√°rias. Ao contr√°rio das vari√°veis estacion√°rias, cujas m√©dias e vari√¢ncias s√£o constantes ao longo do tempo, as vari√°veis de tend√™ncia apresentam um crescimento sistem√°tico, o que impacta diretamente a taxa de converg√™ncia dos estimadores.

### Conceitos Fundamentais
No contexto de modelos com tend√™ncias de tempo determin√≠sticas, como o modelo simples de tend√™ncia temporal apresentado em [16.1.1] [^1], a abordagem tradicional utilizada para modelos estacion√°rios n√£o se aplica diretamente [^1]. O modelo simples de tend√™ncia temporal √© expresso como:

$$y_t = \alpha + \delta t + \epsilon_t$$

onde $\epsilon_t$ representa um processo de ru√≠do branco. A forma da regress√£o linear padr√£o [16.1.2] [^1] √©:

$$y_t = x_t'\beta + \epsilon_t$$

onde $x_t = [1, t]'$ [^1] e $\beta = [\alpha, \delta]'$ [^1]. As estimativas de MQO de $\beta$, denotadas por $b_T$, s√£o dadas por:

$$b_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} = \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} \sum_{t=1}^{T} x_t y_t$$
[^1]

> üí° **Exemplo Num√©rico:**
> Vamos simular um exemplo pr√°tico. Suponha que temos uma s√©rie temporal com T=100 observa√ß√µes, onde $\alpha = 5$, $\delta = 0.2$, e $\epsilon_t$ √© um ru√≠do branco com desvio padr√£o $\sigma = 1.5$.
> ```python
> import numpy as np
> import pandas as pd
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LinearRegression
>
> # Par√¢metros
> T = 100
> alpha = 5
> delta = 0.2
> sigma = 1.5
>
> # Gerar os dados
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, sigma, T)
> y = alpha + delta * t + epsilon
>
> # Criar a matriz X
> X = np.column_stack((np.ones(T), t))
>
> # Calcular as estimativas MQO usando numpy
> XTX = X.T @ X
> XTy = X.T @ y
> beta_hat = np.linalg.solve(XTX, XTy)
>
> # Usando sklearn para confirmar os resultados
> model = LinearRegression()
> model.fit(X,y)
> beta_hat_sklearn = np.array([model.intercept_, model.coef_[1]])
>
> print(f'Estimativas numpy: Alpha = {beta_hat[0]:.4f}, Delta = {beta_hat[1]:.4f}')
> print(f'Estimativas sklearn: Alpha = {beta_hat_sklearn[0]:.4f}, Delta = {beta_hat_sklearn[1]:.4f}')
> # Resultado:
> # Estimativas numpy: Alpha = 5.2609, Delta = 0.1950
> # Estimativas sklearn: Alpha = 5.2609, Delta = 0.1950
>
> # Plotar os dados e a linha de tend√™ncia
> plt.figure(figsize=(8, 6))
> plt.scatter(t, y, label='Dados Observados', alpha=0.7)
> plt.plot(t, beta_hat[0] + beta_hat[1] * t, color='red', label='Linha de Tend√™ncia Estimada')
> plt.xlabel('Tempo (t)')
> plt.ylabel('y_t')
> plt.title('Dados com Tend√™ncia de Tempo')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>  Este exemplo mostra como gerar dados com tend√™ncia de tempo determin√≠stica e como as estimativas de $\alpha$ e $\delta$ podem ser calculadas via MQO utilizando tanto `numpy` quanto `sklearn`.

A an√°lise detalhada em [16.1.6] [^1] mostra que o desvio das estimativas MQO do valor verdadeiro pode ser expresso como:

$$ (b_T - \beta) = \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} \sum_{t=1}^{T} x_t \epsilon_t $$

Em modelos com vari√°veis explicativas estacion√°rias, multiplicar essa express√£o por $\sqrt{T}$ √© suficiente para obter uma distribui√ß√£o limite n√£o degenerada. Entretanto, em modelos com tend√™ncias de tempo determin√≠sticas, essa abordagem n√£o √© adequada devido √†s diferentes taxas de converg√™ncia das estimativas.

A an√°lise do termo $\sum_{t=1}^{T} x_t x_t'$ revela que a sua ordem de crescimento √© dominada por $T^3$ (como visto em [16.1.10], [16.1.11] e [16.1.12] [^1]). Especificamente, o termo $\sum_{t=1}^{T} t$ cresce a uma taxa de $T^2/2$, enquanto $\sum_{t=1}^{T} t^2$ cresce a uma taxa de $T^3/3$ [^1]. Isso indica que a matriz $\sum_{t=1}^{T} x_t x_t'$ cresce mais rapidamente que $T$, exigindo uma normaliza√ß√£o diferente para garantir a converg√™ncia.

Para obter distribui√ß√µes limites n√£o degeneradas, √© necess√°rio reescalonar as estimativas.  O reescalonamento apropriado √© obtido multiplicando $\hat{\alpha}_T$ por $\sqrt{T}$ e $\hat{\delta}_T$ por $T^{3/2}$, o que √© equivalente a pr√©-multiplicar a express√£o de $(b_T - \beta)$ pela matriz $Y_T$:

$$Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}$$ [^1]

Essa opera√ß√£o resulta em:
$$ Y_T(b_T - \beta) = Y_T \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} \sum_{t=1}^{T} x_t \epsilon_t$$

Essa formula√ß√£o nos permite analisar a distribui√ß√£o assint√≥tica das estimativas, considerando suas diferentes taxas de converg√™ncia. A matriz $(1/T^3) \sum_{t=1}^{T} x_t x_t'$ converge para uma matriz limite n√£o singular $Q$, enquanto $(1/T^{1/2})\sum_{t=1}^T \epsilon_t$ e $(1/T^{3/2})\sum_{t=1}^T t \epsilon_t$ convergem para distribui√ß√µes normais, conforme demonstrado em [16.1.11], [16.1.12] e [16.1.21] [^1].

**Lema 1:** As somas $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$ podem ser expressas em termos de pot√™ncias de T. Especificamente,
  $$\sum_{t=1}^{T} t = \frac{T(T+1)}{2} = \frac{T^2}{2} + O(T)$$
  $$\sum_{t=1}^{T} t^2 = \frac{T(T+1)(2T+1)}{6} = \frac{T^3}{3} + O(T^2)$$
  *Prova:* As express√µes acima podem ser facilmente obtidas usando as f√≥rmulas das somas de pot√™ncias de inteiros.
  I. Para a soma de inteiros de 1 a T, a f√≥rmula √© dada por:
    $$\sum_{t=1}^{T} t = \frac{T(T+1)}{2}$$
  II. Expandindo a express√£o, obtemos:
    $$\frac{T(T+1)}{2} = \frac{T^2 + T}{2} = \frac{T^2}{2} + \frac{T}{2}$$
  III. Como $\frac{T}{2}$ √© um termo de ordem $O(T)$, podemos escrever:
      $$\sum_{t=1}^{T} t = \frac{T^2}{2} + O(T)$$
  IV. Para a soma dos quadrados dos inteiros de 1 a T, a f√≥rmula √© dada por:
    $$\sum_{t=1}^{T} t^2 = \frac{T(T+1)(2T+1)}{6}$$
  V. Expandindo a express√£o, obtemos:
    $$\frac{T(T+1)(2T+1)}{6} = \frac{T(2T^2 + 3T + 1)}{6} = \frac{2T^3 + 3T^2 + T}{6} = \frac{T^3}{3} + \frac{T^2}{2} + \frac{T}{6}$$
 VI. Como $\frac{T^2}{2}$ e $\frac{T}{6}$ s√£o termos de ordem no m√°ximo $O(T^2)$, podemos escrever:
   $$\sum_{t=1}^{T} t^2 = \frac{T^3}{3} + O(T^2)$$
‚ñ†

**Observa√ß√£o 2:** O lema 1 demonstra a taxa de crescimento das somas envolvidas no c√°lculo da matriz $\sum_{t=1}^{T} x_t x_t'$,  confirmando que a ordem de crescimento desta matriz √© $T^3$.
> üí° **Exemplo Num√©rico:**
> Vamos verificar numericamente o Lema 1. Usando um valor de T=100, calculemos as somas e comparemos com as aproxima√ß√µes.
> ```python
> import numpy as np
>
> T = 100
> sum_t = np.sum(np.arange(1, T+1))
> sum_t_squared = np.sum(np.arange(1, T+1)**2)
> approx_sum_t = T**2 / 2
> approx_sum_t_squared = T**3 / 3
>
> print(f'Sum of t: {sum_t}, Approx: {approx_sum_t}')
> print(f'Sum of t squared: {sum_t_squared}, Approx: {approx_sum_t_squared}')
> # Resultado
> # Sum of t: 5050, Approx: 5000.0
> # Sum of t squared: 338350, Approx: 333333.3333333333
> ```
> Observamos que as aproxima√ß√µes $\frac{T^2}{2}$ e $\frac{T^3}{3}$ se aproximam dos valores reais das somas quando T=100. Isso ilustra o crescimento de ordem $T^2$ e $T^3$, respectivamente.

A distribui√ß√£o assint√≥tica das estimativas reescalonadas √© ent√£o dada por:

$$  \begin{bmatrix} \sqrt{T}(\hat{\alpha}_T - \alpha) \\ T^{3/2}(\hat{\delta}_T - \delta) \end{bmatrix} \xrightarrow{d} N(0, \sigma^2Q^{-1})$$
[16.1.25] [^1].

√â importante notar que $\hat{\delta}_T$ √© um estimador *superconsistente*, o que significa que ele converge para $\delta$ a uma taxa mais r√°pida do que a taxa padr√£o de $\sqrt{T}$ [^1]. Especificamente, $T(\hat{\delta}_T - \delta)$ converge para zero, indicando uma converg√™ncia mais r√°pida do que em modelos com vari√°veis estacion√°rias [16.1.27] [^1].

> üí° **Exemplo Num√©rico:**
> Para ilustrar a superconsist√™ncia do estimador $\hat{\delta}_T$, vamos simular v√°rias s√©ries temporais com diferentes tamanhos e analisar a converg√™ncia de $T(\hat{\delta}_T - \delta)$.
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> alpha = 5
> delta = 0.2
> sigma = 1.5
> num_simulations = 200
> T_values = [50, 100, 200, 400, 800]
>
>
> results = []
>
> for T in T_values:
>  T_results = []
>  for _ in range(num_simulations):
>    t = np.arange(1, T + 1)
>    epsilon = np.random.normal(0, sigma, T)
>    y = alpha + delta * t + epsilon
>    X = np.column_stack((np.ones(T), t))
>    model = LinearRegression()
>    model.fit(X, y)
>    delta_hat = model.coef_[1]
>    T_results.append(T * (delta_hat - delta))
>
>  results.append(T_results)
>
> fig, axes = plt.subplots(len(T_values), 1, figsize=(8, 6), sharex=True)
> for i, T in enumerate(T_values):
>    axes[i].hist(results[i], bins=30, density=True, alpha=0.7, label = f'T = {T}')
>    axes[i].set_ylabel('Density')
>    axes[i].legend()
>
> axes[-1].set_xlabel(r'T($\hat{\delta}_T$ - $\delta$)')
> plt.suptitle('Distribui√ß√£o de  $T(\hat{\delta}_T - \delta)$ para diferentes valores de T', y = 1.02)
> plt.tight_layout()
> plt.show()
>
> ```
> Conforme T aumenta, a distribui√ß√£o de $T(\hat{\delta}_T - \delta)$ se concentra cada vez mais perto de zero. Isso demonstra a superconsist√™ncia do estimador de tend√™ncia $\hat{\delta}_T$.

**Teorema 1:** (Superconsist√™ncia do estimador de tend√™ncia) O estimador $\hat{\delta}_T$ da inclina√ß√£o da tend√™ncia converge para o valor verdadeiro $\delta$ a uma taxa mais r√°pida que $\sqrt{T}$, especificamente $T(\hat{\delta}_T - \delta)$ converge para zero.
 *Prova:* A superconsist√™ncia do estimador $\hat{\delta}_T$ √© um resultado direto da an√°lise da matriz $Y_T$ e da ordem de grandeza de $\sum_{t=1}^{T} x_t x_t'$. Ao reescalonar por $T^{3/2}$, como definido anteriormente, eliminamos a ordem de grandeza $T^3$ da matriz $\sum_{t=1}^{T} x_t x_t'$ o que implica em uma taxa de converg√™ncia maior. Este resultado √© demonstrado detalhadamente em [16.1.27] [^1].
I. Come√ßamos com a express√£o para o estimador MQO de $\beta$:
$$b_T - \beta = \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} \sum_{t=1}^{T} x_t \epsilon_t$$
II. Multiplicamos ambos os lados pela matriz de reescalonamento $Y_T$:
$$Y_T (b_T - \beta) = Y_T \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} \sum_{t=1}^{T} x_t \epsilon_t$$
III. Explicitando $Y_T$ e $b_T$, temos:
$$\begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} \begin{bmatrix} \hat{\alpha}_T - \alpha \\ \hat{\delta}_T - \delta \end{bmatrix} = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} \left( \sum_{t=1}^{T} x_t x_t' \right)^{-1} \sum_{t=1}^{T} x_t \epsilon_t$$
IV. Queremos analisar a converg√™ncia de $T(\hat{\delta}_T - \delta)$, ent√£o focamos no segundo componente do lado esquerdo da equa√ß√£o:
$$ T^{3/2} (\hat{\delta}_T - \delta) $$
V. Do lado direito da equa√ß√£o, a matriz $\sum_{t=1}^{T} x_t x_t'$ tem uma ordem de crescimento $T^3$, conforme demonstrado no Lema 1. Assim, multiplicando por $T^{3/2}$, normalizamos essa ordem para que ela convirja para uma matriz n√£o-singular.
VI. Isso implica que $T^{3/2} (\hat{\delta}_T - \delta)$ converge para uma distribui√ß√£o normal, conforme discutido em [16.1.25] [^1].  
VII. Se multiplicarmos $T^{3/2}(\hat{\delta}_T - \delta)$ por $T^{-1/2}$, obtemos $T(\hat{\delta}_T - \delta)$. Como $T^{3/2}(\hat{\delta}_T - \delta)$ converge para uma vari√°vel aleat√≥ria com distribui√ß√£o normal, $T(\hat{\delta}_T - \delta)$ converge para zero, demonstrando a superconsist√™ncia de $\hat{\delta}_T$.
‚ñ†

O processo detalhado apresentado em [16.1.18] at√© [16.1.25] [^1] revela que, embora as estimativas MQO convirjam a diferentes taxas, a aplica√ß√£o do reescalonamento apropriado nos permite obter uma distribui√ß√£o assint√≥tica normal multivariada. As express√µes [16.1.19], [16.1.20] e [16.1.21] [^1] detalham a deriva√ß√£o dos componentes da matriz de vari√¢ncia-covari√¢ncia limite.

A an√°lise da distribui√ß√£o assint√≥tica tamb√©m √© extens√≠vel a modelos mais complexos, como processos autoregressivos em torno de uma tend√™ncia de tempo determin√≠stica [16.3] [^1]. Em tais casos, os princ√≠pios do reescalonamento e a identifica√ß√£o das taxas de converg√™ncia assint√≥tica apropriadas para cada componente do modelo s√£o fundamentais para obter resultados v√°lidos para a infer√™ncia estat√≠stica.

**Observa√ß√£o 3:** A generaliza√ß√£o para modelos mais complexos envolvendo termos autorregressivos segue a mesma l√≥gica de reescalonamento. O importante √© identificar as ordens de converg√™ncia de cada componente da matriz de regressores. Termos autorregressivos, quando estacion√°rios, tendem a convergir √† mesma taxa que as vari√°veis estacion√°rias, enquanto termos de tend√™ncia necessitam de um reescalonamento espec√≠fico.

### Conclus√£o

A an√°lise das distribui√ß√µes assint√≥ticas das estimativas MQO em modelos com tend√™ncias de tempo determin√≠sticas demonstra a necessidade de t√©cnicas de reescalonamento para acomodar as diferentes taxas de converg√™ncia dos estimadores. A superconsist√™ncia do estimador da tend√™ncia de tempo e a necessidade de abordagens espec√≠ficas para obter distribui√ß√µes limites n√£o degeneradas s√£o cruciais para a infer√™ncia estat√≠stica nesses modelos. O m√©todo apresentado aqui, derivado de Sims, Stock e Watson (1990), permite a correta an√°lise de distribui√ß√µes limite em modelos com tend√™ncias de tempo determin√≠sticas. Este cap√≠tulo introduz a base para analisar processos com raiz unit√°ria, que ser√° explorado nos cap√≠tulos seguintes [^1].

**Corol√°rio 1:** A metodologia de reescalonamento discutida aqui √© fundamental para realizar testes de hip√≥teses sobre os par√¢metros do modelo. Por exemplo, para testar a hip√≥tese nula de que a tend√™ncia ($\delta$) √© zero, deve-se usar a estat√≠stica $T^{3/2}\hat{\delta}_T$, que tem uma distribui√ß√£o assint√≥tica normal sob a hip√≥tese nula.
*Prova:* Para testar a hip√≥tese nula $H_0: \delta = 0$, usamos a estat√≠stica de teste:
$$t = \frac{\hat{\delta}_T - 0}{SE(\hat{\delta}_T)}$$
I. Da an√°lise assint√≥tica, sabemos que $T^{3/2}(\hat{\delta}_T - \delta)$ converge para uma distribui√ß√£o normal com m√©dia zero e uma vari√¢ncia dada por $\sigma^2$ multiplicada pelo elemento apropriado da matriz $Q^{-1}$.
II. Reescalonamos a estat√≠stica $\hat{\delta}_T$ multiplicando-a por $T^{3/2}$. Isso nos d√° a vari√°vel aleat√≥ria $T^{3/2}\hat{\delta}_T$.
III. A vari√¢ncia assint√≥tica de $\hat{\delta}_T$ √© da ordem de $T^{-3}$, de modo que seu desvio padr√£o √© da ordem de $T^{-3/2}$.
IV. Assim, a estat√≠stica $T^{3/2}\hat{\delta}_T$ tem uma distribui√ß√£o assint√≥tica normal sob a hip√≥tese nula $H_0: \delta = 0$, com uma m√©dia zero.
V. Desta forma, podemos usar a estat√≠stica $T^{3/2}\hat{\delta}_T$ para realizar testes de hip√≥tese sobre o par√¢metro $\delta$.
‚ñ†
> üí° **Exemplo Num√©rico:**
> Vamos realizar um teste de hip√≥tese para $\delta$ com base no exemplo anterior. Suponha que temos uma s√©rie temporal com T=100, e queremos testar $H_0 : \delta = 0$ versus $H_1: \delta \ne 0$.
> ```python
> import numpy as np
> import pandas as pd
> import scipy.stats as st
> from sklearn.linear_model import LinearRegression
>
> # Par√¢metros
> T = 100
> alpha = 5
> delta = 0.2  # Valor verdadeiro
> sigma = 1.5
>
> # Gerar os dados
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, sigma, T)
> y = alpha + delta * t + epsilon
>
> # Criar a matriz X
> X = np.column_stack((np.ones(T), t))
>
> # Calcular as estimativas MQO
> model = LinearRegression()
> model.fit(X, y)
> alpha_hat = model.intercept_
> delta_hat = model.coef_[1]
>
> # Calcular os res√≠duos
> residuals = y - model.predict(X)
>
> # Calcular o erro padr√£o
> MSE = np.sum(residuals**2)/(T-2)
> XTX_inv = np.linalg.inv(X.T @ X)
> std_err_delta = np.sqrt(MSE*XTX_inv[1,1])
>
> # Calcular a estat√≠stica de teste
> t_statistic = delta_hat / std_err_delta
>
> # Calcular o p-valor
> p_value = 2*(1-st.t.cdf(np.abs(t_statistic), df = T-2))
>
> # Reescalonamento para teste de hipotese com a distribui√ß√£o assint√≥tica
> t_statistic_asymptotic = T**(3/2) * delta_hat
> p_value_asymptotic = 2 * (1 - st.norm.cdf(np.abs(t_statistic_asymptotic)/np.sqrt(MSE*12)))
>
> print(f'Estimativa de Delta: {delta_hat:.4f}')
> print(f'Estat√≠stica t para Delta: {t_statistic:.4f}')
> print(f'P-valor para Delta (t-Student): {p_value:.4f}')
> print(f'Estat√≠stica t para Delta (assint√≥tica): {t_statistic_asymptotic:.4f}')
> print(f'P-valor para Delta (assint√≥tica): {p_value_asymptotic:.4f}')
>
> #Resultado
> # Estimativa de Delta: 0.1867
> # Estat√≠stica t para Delta: 10.8891
> # P-valor para Delta (t-Student): 0.0000
> # Estat√≠stica t para Delta (assint√≥tica): 18.6726
> # P-valor para Delta (assint√≥tica): 0.0000
> ```
> Nesse exemplo, tanto o teste t padr√£o quanto o teste baseado na distribui√ß√£o assint√≥tica rejeitam a hip√≥tese nula de que $\delta = 0$. Note que o p-valor obtido pelo teste assint√≥tico √© ligeiramente diferente do teste t padr√£o, isso ocorre porque a estat√≠stica de teste √© reescalonada por $T^{3/2}$. Isso ilustra a import√¢ncia do reescalonamento para testes de hip√≥teses em modelos com tend√™ncias de tempo.

### Refer√™ncias
[^1]: Trechos do cap√≠tulo 16 do livro "Processes with Deterministic Time Trends", conforme fornecido no contexto.
<!-- END -->
