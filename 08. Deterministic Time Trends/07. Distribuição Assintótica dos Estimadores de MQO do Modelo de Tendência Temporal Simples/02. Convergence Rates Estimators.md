## Distribui√ß√£o Assint√≥tica dos Estimadores de MQO no Modelo de Tend√™ncia Temporal Simples: An√°lise das Taxas de Converg√™ncia e Distribui√ß√µes Limite

### Introdu√ß√£o
Este cap√≠tulo aprofunda a an√°lise da distribui√ß√£o assint√≥tica dos estimadores de M√≠nimos Quadrados Ordin√°rios (MQO) aplicados a um modelo de tend√™ncia temporal simples. Em continuidade com a discuss√£o sobre as dificuldades de aplicar a an√°lise assint√≥tica padr√£o para vari√°veis estacion√°rias [^1], aqui exploramos detalhadamente o comportamento dos estimadores quando confrontados com tend√™ncias temporais determin√≠sticas. A principal diferen√ßa em rela√ß√£o aos modelos com vari√°veis estacion√°rias reside nas taxas de converg√™ncia distintas dos estimadores dos coeficientes, o que exige uma abordagem metodol√≥gica adaptada [^1]. A an√°lise dos modelos de regress√£o envolvendo ra√≠zes unit√°rias ou tend√™ncias temporais determin√≠sticas exige um tratamento diferenciado, pois as distribui√ß√µes assint√≥ticas n√£o podem ser obtidas com as mesmas t√©cnicas usadas para vari√°veis estacion√°rias. Este cap√≠tulo explora como as taxas de converg√™ncia distintas dos diferentes par√¢metros impactam a an√°lise assint√≥tica, seguindo a abordagem proposta por Sims, Stock, e Watson (1990) [^1]. A an√°lise aqui desenvolvida serve de base para entender a complexidade da infer√™ncia em modelos de s√©ries temporais com ra√≠zes unit√°rias, tema dos pr√≥ximos cap√≠tulos.

### Conceitos Fundamentais
Revisitamos o modelo de tend√™ncia temporal simples:
$$y_t = \alpha + \delta t + \epsilon_t$$
onde $\epsilon_t$ √© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$ [^2]. A representa√ß√£o matricial do modelo, crucial para a an√°lise de MQO, √© dada por:
$$ y_t = x_t'\beta + \epsilon_t $$
onde $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$ e $\beta = \begin{bmatrix} \alpha \\ \delta \end{bmatrix}$ [^3]. O estimador de MQO, $b_T$, √© definido como:
$$b_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \left( \sum_{t=1}^T x_t y_t \right)$$ [^5]
E o desvio do estimador em rela√ß√£o ao valor verdadeiro √©:
$$(b_T - \beta) = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \left( \sum_{t=1}^T x_t \epsilon_t \right)$$ [^6]
Como j√° discutido, a aplica√ß√£o direta da metodologia utilizada para vari√°veis estacion√°rias, que envolve multiplicar o desvio $(b_T - \beta)$ por $\sqrt{T}$, n√£o √© adequada neste contexto [^1]. A raz√£o principal reside nas diferentes taxas de converg√™ncia de $\hat{\alpha}_T$ e $\hat{\delta}_T$. A an√°lise das somas envolvidas, $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$, demonstra que as parcelas dominantes s√£o $T^2/2$ e $T^3/3$, respectivamente [^11], [^12]. Essa constata√ß√£o indica que a matriz $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ diverge, necessitando que a matriz seja dividida por $T^3$ para convergir [^16]. Como demonstrado no *Lema 2.2*, $\frac{1}{T^3}\sum_{t=1}^T x_t x_t'$ converge para uma matriz n√£o singular.

> üí° **Exemplo Num√©rico:**
> Vamos considerar um exemplo pr√°tico com T = 100 amostras, onde simulamos dados de acordo com o modelo: $y_t = 2 + 0.5t + \epsilon_t$, com $\epsilon_t \sim N(0, 1)$. A matriz $X$ √© constru√≠da com uma coluna de 1s e uma coluna de 1 a 100.
> ```python
> import numpy as np
> import pandas as pd
>
> np.random.seed(42)
> T = 100
> t = np.arange(1, T + 1)
> X = np.column_stack((np.ones(T), t))
> alpha = 2
> delta = 0.5
> epsilon = np.random.normal(0, 1, T)
> y = alpha + delta * t + epsilon
>
> # C√°lculo do estimador de MQO
> beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
> alpha_hat, delta_hat = beta_hat
>
> print(f"Estimativa de alpha: {alpha_hat:.4f}")
> print(f"Estimativa de delta: {delta_hat:.4f}")
> ```
> Este c√≥digo simula os dados e estima os par√¢metros do modelo. Note que os valores estimados estar√£o pr√≥ximos aos valores verdadeiros (2 e 0.5) mas n√£o id√™nticos, devido √† aleatoriedade do termo de erro.

#### An√°lise Detalhada das Taxas de Converg√™ncia
Para analisar as taxas de converg√™ncia, o *Lema 2.2* nos mostra que a matriz $\left( \frac{1}{T^3} \sum_{t=1}^T x_t x_t' \right)^{-1}$ n√£o converge para a matriz $Q$ quando $T \rightarrow \infty$.
A necessidade de ajustar as taxas de converg√™ncia dos estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$ √© crucial para obter distribui√ß√µes assint√≥ticas n√£o degeneradas [^17]. Enquanto o estimador $\hat{\alpha}_T$ converge a uma taxa de $\sqrt{T}$, o estimador $\hat{\delta}_T$ converge a uma taxa mais r√°pida de $T^{3/2}$. Para acomodar essas diferentes taxas, a express√£o do desvio $(b_T - \beta)$ √© pr√©-multiplicada pela matriz:
$$Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}$$ [^17]
Essa transforma√ß√£o resulta em:
$$ Y_T (b_T - \beta) =  Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1}  \left( \sum_{t=1}^T x_t \epsilon_t \right) $$
$$ Y_T (b_T - \beta) = \left( Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right) Y_T \left( \sum_{t=1}^T x_t \epsilon_t \right) $$ [^18]
A matriz de momentos normalizada converge para uma matriz $Q$:
$$ Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix} $$ [^20]
A matriz $Y_T$, ao ser aplicada ao termo envolvendo os erros, transforma-o em:
$$ Y_T \left( \sum_{t=1}^T x_t \epsilon_t \right) = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{T^{3/2}} \sum_{t=1}^T t\epsilon_t \end{bmatrix} $$ [^21]
Sob condi√ß√µes usuais, esse vetor converge assintoticamente para uma distribui√ß√£o normal bivariada com m√©dia zero [^21]. O primeiro componente, $\frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t$, converge para $N(0, \sigma^2)$ [^21], e o segundo componente, $\frac{1}{T^{3/2}} \sum_{t=1}^T t\epsilon_t$, converge para uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\sigma^2/3$ [^22], [^23]. A covari√¢ncia entre esses componentes √© zero [^23]. Esses resultados s√£o cruciais para estabelecer a distribui√ß√£o limite dos estimadores.

> üí° **Exemplo Num√©rico (continua√ß√£o):**
> Para ilustrar a transforma√ß√£o com $Y_T$, vamos calcular $Y_T(b_T - \beta)$.  Usando os resultados do c√≥digo anterior, e assumindo $\alpha=2$ e $\delta=0.5$:
> ```python
> T = 100
> alpha_true = 2
> delta_true = 0.5
>
> YT = np.array([[np.sqrt(T), 0], [0, T**1.5]])
> beta_diff = np.array([alpha_hat - alpha_true, delta_hat - delta_true])
>
> YT_beta_diff = YT @ beta_diff
> print(f"YT(beta_hat - beta): \n{YT_beta_diff}")
> ```
> Os valores resultantes desta opera√ß√£o correspondem aos valores que convergem para uma distribui√ß√£o normal ap√≥s devida normaliza√ß√£o. O primeiro componente, $\sqrt{T}(\hat{\alpha} - \alpha)$, converge mais lentamente do que o segundo componente $T^{3/2}(\hat{\delta} - \delta)$.

**Lema 2.2** A matriz $ \frac{1}{T^3} \sum_{t=1}^T x_t x_t'$ converges to a matrix that, when scaled by $T^3$, converges to a non-singular matrix.

*Proof:*
I.  Recall that $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$. Therefore, $x_t x_t' = \begin{bmatrix} 1 & t \\ t & t^2 \end{bmatrix}$.
II.  We have:
$$\frac{1}{T^3} \sum_{t=1}^T x_t x_t' = \frac{1}{T^3} \sum_{t=1}^T \begin{bmatrix} 1 & t \\ t & t^2 \end{bmatrix} = \begin{bmatrix} \frac{1}{T^3} \sum_{t=1}^T 1 & \frac{1}{T^3} \sum_{t=1}^T t \\ \frac{1}{T^3} \sum_{t=1}^T t & \frac{1}{T^3} \sum_{t=1}^T t^2 \end{bmatrix}$$
III. Using known results for sums of powers of integers:
$$\frac{1}{T^3} \sum_{t=1}^T 1 = \frac{T}{T^3} = \frac{1}{T^2}$$
$$\frac{1}{T^3} \sum_{t=1}^T t = \frac{T(T+1)}{2T^3} = \frac{1}{2T} + \frac{1}{2T^2}$$
$$\frac{1}{T^3} \sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6T^3} = \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2}$$
IV. As $T \to \infty$, we have:
$$\lim_{T\to\infty} \frac{1}{T^3} \sum_{t=1}^T x_t x_t' = \lim_{T\to\infty} \begin{bmatrix} \frac{1}{T^2} & \frac{1}{2T} + \frac{1}{2T^2} \\ \frac{1}{2T} + \frac{1}{2T^2} & \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2} \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 0 & \frac{1}{3} \end{bmatrix}$$
V. Define $Q_T = \frac{1}{T^3}\sum_{t=1}^T x_t x_t'$. Then, the relevant matrix for the asymptotic distribution will be
$$Y_T Q_T^{-1} Y_T = \left( \frac{1}{T^3} Y_T^{-1}\sum_{t=1}^T x_t x_t' Y_T^{-1} \right)^{-1}$$
VI. We are interested in the limit of the inverse: $\left(  \frac{1}{T^3} Y_T \sum_{t=1}^T x_t x_t' Y_T \right)$. We need to compute
$$\frac{1}{T^3}Y_T \left( \sum_{t=1}^T x_t x_t' \right)Y_T = \frac{1}{T^3} \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} \left( \sum_{t=1}^T \begin{bmatrix} 1 & t \\ t & t^2 \end{bmatrix} \right) \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} $$
$$ = \frac{1}{T^3} \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}  \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix}  \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix} $$
$$ = \begin{bmatrix} \frac{1}{T^2} & \frac{1}{2T}+\frac{1}{2T^2} \\ \frac{1}{2T}+\frac{1}{2T^2} & \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2} \end{bmatrix} \begin{bmatrix} T & 0 \\ 0 & T^3 \end{bmatrix}$$
$$ = \begin{bmatrix} \frac{1}{T} & \frac{T}{2}+\frac{1}{2} \\ \frac{1}{2} + \frac{1}{2T} & \frac{T^2}{3} + \frac{T}{2} + \frac{1}{6}  \end{bmatrix} $$
$$ = \begin{bmatrix} \frac{1}{T^2} \sum_{t=1}^T 1  & \frac{1}{T^2}\sum_{t=1}^T t \\ \frac{1}{T^2}\sum_{t=1}^T t & \frac{1}{T^2}\sum_{t=1}^T t^2 \end{bmatrix}$$
VII. Then using the standard results on sums we get

$$\lim_{T \to \infty} \begin{bmatrix} \frac{1}{T} & \frac{1}{2} + \frac{1}{2T}  \\ \frac{1}{2} + \frac{1}{2T} & \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2} \end{bmatrix}  = \begin{bmatrix} 0 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$$

$$\lim_{T\to\infty} \frac{1}{T^3}Y_T \left( \sum_{t=1}^T x_t x_t' \right)Y_T = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3\end{bmatrix} = Q$$
Therefore, $\frac{1}{T^3} Y_T \sum_{t=1}^T x_t x_t' Y_T $ converges to Q.
‚ñ†

### Distribui√ß√µes Limite e Implica√ß√µes
Com base na an√°lise detalhada das taxas de converg√™ncia e da aplica√ß√£o da transforma√ß√£o $Y_T$, a distribui√ß√£o assint√≥tica dos estimadores de MQO ajustados √©:
$$ \begin{bmatrix} \sqrt{T} (\hat{\alpha}_T - \alpha) \\ T^{3/2} (\hat{\delta}_T - \delta) \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q^{-1}) $$ [^24], [^25]
onde $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$. A demonstra√ß√£o detalhada da converg√™ncia, incluindo a distribui√ß√£o limite dos estimadores, foi realizada. A an√°lise evidencia a necessidade de ajustar a an√°lise assint√≥tica, e como a matriz $Y_T$ √© crucial para lidar com as diferentes taxas de converg√™ncia de $\hat{\alpha}_T$ e $\hat{\delta}_T$. O resultado demonstra que, embora $\hat{\alpha}_T$ convirja a $\sqrt{T}$, $\hat{\delta}_T$ convirja a $T^{3/2}$ [^26].

> üí° **Exemplo Num√©rico:** Utilizando a simula√ß√£o apresentada na se√ß√£o anterior (com 1000 amostras de tamanho T=100), √© poss√≠vel verificar a distribui√ß√£o limite. Os histogramas das vari√°veis transformadas $\sqrt{T}(\hat{\alpha}_T - \alpha)$ e $T^{3/2}(\hat{\delta}_T - \delta)$ se aproximam de distribui√ß√µes normais, com m√©dias zero e vari√¢ncias $\sigma^2 Q^{-1}$, o que confirma os resultados te√≥ricos da distribui√ß√£o assint√≥tica.
>
> Para uma simula√ß√£o mais completa, podemos gerar m√∫ltiplas amostras e verificar a distribui√ß√£o dos estimadores transformados:
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> np.random.seed(42)
> num_simulations = 1000
> T = 100
> alpha_true = 2
> delta_true = 0.5
> alpha_hats = []
> delta_hats = []
>
> for _ in range(num_simulations):
>     t = np.arange(1, T + 1)
>     X = np.column_stack((np.ones(T), t))
>     epsilon = np.random.normal(0, 1, T)
>     y = alpha_true + delta_true * t + epsilon
>     beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
>     alpha_hats.append(beta_hat[0])
>     delta_hats.append(beta_hat[1])
>
> alpha_diffs = np.sqrt(T) * (np.array(alpha_hats) - alpha_true)
> delta_diffs = T**1.5 * (np.array(delta_hats) - delta_true)
>
> plt.figure(figsize=(12, 6))
> plt.subplot(1, 2, 1)
> plt.hist(alpha_diffs, bins=30, density=True, alpha=0.7, label='Simulated')
>
> mean_alpha_diff = np.mean(alpha_diffs)
> std_alpha_diff = np.std(alpha_diffs)
> x_alpha = np.linspace(mean_alpha_diff - 3 * std_alpha_diff, mean_alpha_diff + 3 * std_alpha_diff, 100)
> y_alpha = 1 / (std_alpha_diff * np.sqrt(2 * np.pi)) * np.exp(-(x_alpha - mean_alpha_diff)**2 / (2 * std_alpha_diff**2))
> plt.plot(x_alpha, y_alpha, color='red', label='Normal Approx')
> plt.title(r'Distribution of $\sqrt{T}(\hat{\alpha} - \alpha)$')
> plt.legend()
>
> plt.subplot(1, 2, 2)
> plt.hist(delta_diffs, bins=30, density=True, alpha=0.7, label='Simulated')
>
> mean_delta_diff = np.mean(delta_diffs)
> std_delta_diff = np.std(delta_diffs)
> x_delta = np.linspace(mean_delta_diff - 3 * std_delta_diff, mean_delta_diff + 3 * std_delta_diff, 100)
> y_delta = 1 / (std_delta_diff * np.sqrt(2 * np.pi)) * np.exp(-(x_delta - mean_delta_diff)**2 / (2 * std_delta_diff**2))
> plt.plot(x_delta, y_delta, color='red', label='Normal Approx')
> plt.title(r'Distribution of $T^{3/2}(\hat{\delta} - \delta)$')
> plt.legend()
>
> plt.tight_layout()
> plt.show()
>
> print(f"Variance of sqrt(T)(alpha_hat - alpha): {np.var(alpha_diffs):.2f} (Theoretical: {4*1:.2f})")
> print(f"Variance of T^(3/2)(delta_hat - delta): {np.var(delta_diffs):.2f} (Theoretical: {12*1:.2f})")
> ```
> Este c√≥digo gera histogramas dos estimadores transformados, com sobreposi√ß√£o das curvas normais te√≥ricas, e calcula as vari√¢ncias simuladas e as compara com as te√≥ricas ($4\sigma^2$ e $12\sigma^2$, com $\sigma^2=1$). Os histogramas mostram uma aproxima√ß√£o com as normais, e as vari√¢ncias estimadas est√£o pr√≥ximas dos valores esperados.

**Teorema 1** *Distribui√ß√£o Assint√≥tica dos Estimadores de MQO no Modelo de Tend√™ncia Temporal Simples*.

Dado o modelo $y_t = \alpha + \delta t + \epsilon_t$, onde $\epsilon_t$ √© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$, e o estimador de MQO $b_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix}$, ent√£o
$$ \begin{bmatrix} \sqrt{T} (\hat{\alpha}_T - \alpha) \\ T^{3/2} (\hat{\delta}_T - \delta) \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q^{-1}) $$
onde $Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$ e $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$.
*Proof:*
I. We start with the expression for the OLS estimator's deviation from the true value:
$$(b_T - \beta) = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \left( \sum_{t=1}^T x_t \epsilon_t \right)$$
II. Multiply both sides by $Y_T$:
$$Y_T (b_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \left( \sum_{t=1}^T x_t \epsilon_t \right)$$
III.  Multiply and divide by $T^3$ inside the inverse:
$$Y_T (b_T - \beta) =  \left( \frac{1}{T^3}  Y_T \left( \sum_{t=1}^T x_t x_t' \right) Y_T\right)^{-1} \frac{1}{T^3} Y_T  T^3 \left( \sum_{t=1}^T x_t \epsilon_t \right)$$
IV.  Using the result from Lemma 2.2, we know that $ \frac{1}{T^3} Y_T \left( \sum_{t=1}^T x_t x_t' \right) Y_T \xrightarrow{p} Q$ where $Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$. Thus,
$$Y_T (b_T - \beta) = \left( \frac{1}{T^3}  Y_T \left( \sum_{t=1}^T x_t x_t' \right) Y_T\right)^{-1} Y_T \left( \sum_{t=1}^T x_t \frac{\epsilon_t}{T^{3/2}} \right) T^3$$
V.  Also,
$$Y_T \left( \sum_{t=1}^T x_t \epsilon_t \right) = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{T^{3/2}} \sum_{t=1}^T t\epsilon_t \end{bmatrix} $$
VI. The terms inside the $Y_T$ matrix converge in distribution to a normal as the sample size increases.
VII.  Under standard assumptions about the error terms, we have:
$$\frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \xrightarrow{d} N(0, \sigma^2)$$
$$\frac{1}{T^{3/2}} \sum_{t=1}^T t\epsilon_t \xrightarrow{d} N(0, \frac{\sigma^2}{3})$$
VIII. The covariance between these two terms goes to zero. Therefore,
$$Y_T \left( \sum_{t=1}^T x_t \epsilon_t \right) \xrightarrow{d} N(0, \sigma^2 \begin{bmatrix} 1 & 0 \\ 0 & 1/3 \end{bmatrix})$$
IX. Combining this with the convergence of the matrix and using the continuous mapping theorem, we obtain:
$$Y_T (b_T - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1})$$
where $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$.
‚ñ†

**Corol√°rio 1.1** *Distribui√ß√µes Assint√≥ticas Marginais*.
As distribui√ß√µes assint√≥ticas marginais para os estimadores s√£o:
$$ \sqrt{T} (\hat{\alpha}_T - \alpha) \xrightarrow{d} N(0, 4\sigma^2) $$
$$ T^{3/2} (\hat{\delta}_T - \delta) \xrightarrow{d} N(0, 12\sigma^2) $$
*Proof:*
I.  From Theorem 1, we know that
$$ \begin{bmatrix} \sqrt{T} (\hat{\alpha}_T - \alpha) \\ T^{3/2} (\hat{\delta}_T - \delta) \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q^{-1}) $$
where $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$.
II. The marginal distributions are obtained by considering the diagonal elements of the variance-covariance matrix $\sigma^2 Q^{-1}$.
III. Thus, for the intercept:
$$ \sqrt{T} (\hat{\alpha}_T - \alpha) \xrightarrow{d} N(0, \sigma^2 \cdot 4) = N(0, 4\sigma^2) $$
IV. And for the slope:
$$ T^{3/2} (\hat{\delta}_T - \delta) \xrightarrow{d} N(0, \sigma^2 \cdot 12) = N(0, 12\sigma^2) $$
‚ñ†

### Conclus√£o
Este cap√≠tulo apresentou uma an√°lise detalhada da distribui√ß√£o assint√≥tica dos estimadores de MQO em um modelo de tend√™ncia temporal simples, enfatizando a necessidade de considerar as taxas de converg√™ncia distintas de $\hat{\alpha}_T$ e $\hat{\delta}_T$. A introdu√ß√£o da matriz $Y_T$ foi fundamental para ajustar a an√°lise assint√≥tica e obter distribui√ß√µes limites n√£o degeneradas. A an√°lise revelou que $\hat{\alpha}_T$ converge a uma taxa de $\sqrt{T}$, enquanto $\hat{\delta}_T$ converge a uma taxa mais r√°pida de $T^{3/2}$ [^26]. A compreens√£o dessas diferen√ßas √© essencial para a infer√™ncia correta em modelos de s√©ries temporais com tend√™ncias determin√≠sticas. Este estudo aprofundado serve como uma base s√≥lida para abordar a an√°lise de modelos de s√©ries temporais mais complexos, particularmente aqueles envolvendo ra√≠zes unit√°rias, que ser√£o explorados nos pr√≥ximos cap√≠tulos. As abordagens e os resultados aqui apresentados complementam o desenvolvimento te√≥rico anterior, fornecendo um entendimento mais completo e robusto das propriedades assint√≥ticas dos estimadores de MQO em modelos com tend√™ncias temporais determin√≠sticas.

### Refer√™ncias
[^1]:  Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113‚Äì44.
[^2]:  Se√ß√£o 16.1, par√°grafo 1.
[^3]:  Equa√ß√£o 16.1.2 e 16.1.3.
[^5]:  Equa√ß√£o 16.1.5.
[^6]:  Equa√ß√£o 16.1.6.
[^11]: Equa√ß√£o 16.1.11.
[^12]: Equa√ß√£o 16.1.12.
[^16]: Se√ß√£o 16.1, par√°grafo 9.
[^17]: Equa√ß√£o 16.1.17.
[^18]: Equa√ß√£o 16.1.18.
[^20]: Equa√ß√£o 16.1.20.
[^21]: Equa√ß√£o 16.1.21.
[^22]: Se√ß√£o 16.1, par√°grafos 13 e 14.
[^23]: Equa√ß√£o 16.1.23.
[^24]: Equa√ß√£o 16.1.24.
[^25]: Equa√ß√£o 16.1.25.
[^26]: Proposi√ß√£o 16.1.
<!-- END -->
