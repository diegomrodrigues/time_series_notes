## Distribui√ß√£o Assint√≥tica dos Estimadores de MQO em Modelos de Tend√™ncia Temporal Simples
### Introdu√ß√£o
Este cap√≠tulo explora os processos com tend√™ncias temporais determin√≠sticas, abordando especificamente a distribui√ß√£o assint√≥tica dos estimadores de M√≠nimos Quadrados Ordin√°rios (MQO) quando aplicados a um modelo de tend√™ncia temporal simples. Como mencionado anteriormente, os modelos de regress√£o envolvendo ra√≠zes unit√°rias ou tend√™ncias temporais determin√≠sticas apresentam desafios √∫nicos, pois as distribui√ß√µes assint√≥ticas dos estimadores dos coeficientes n√£o podem ser calculadas da mesma forma que para vari√°veis estacion√°rias [^1]. Este cap√≠tulo, em continuidade ao estudo de processos n√£o estacion√°rios, foca em como as taxas de converg√™ncia distintas dos diferentes par√¢metros influenciam na an√°lise assint√≥tica, seguindo a abordagem de Sims, Stock, e Watson (1990) [^1]. A an√°lise come√ßa com o exemplo mais simples de inova√ß√µes i.i.d. em torno de uma tend√™ncia temporal determin√≠stica, que serve como base para entender as complexidades da an√°lise assint√≥tica em modelos mais complexos com ra√≠zes unit√°rias, que ser√£o abordados nos pr√≥ximos cap√≠tulos [^1].

### Conceitos Fundamentais
O modelo de tend√™ncia temporal simples √© dado por:
$$y_t = \alpha + \delta t + \epsilon_t$$
onde $\epsilon_t$ √© um processo de ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$ [^2]. Este modelo satisfaz as premissas cl√°ssicas da regress√£o quando $\epsilon_t \sim N(0, \sigma^2)$. Para analisar a distribui√ß√£o assint√≥tica dos estimadores de MQO, √© √∫til reformular o modelo na forma padr√£o da regress√£o:
$$y_t = x_t'\beta + \epsilon_t$$
onde $x_t = \begin{bmatrix} 1 \\ t \end{bmatrix}$ e $\beta = \begin{bmatrix} \alpha \\ \delta \end{bmatrix}$ [^3]. O estimador de MQO de $\beta$, denotado por $b_T$, com base em uma amostra de tamanho $T$, √© dado por:
$$b_T = \begin{bmatrix} \hat{\alpha}_T \\ \hat{\delta}_T \end{bmatrix} = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \left( \sum_{t=1}^T x_t y_t \right)$$ [^5]
O desvio do estimador de MQO em rela√ß√£o ao valor verdadeiro pode ser expresso como:
$$ (b_T - \beta) = \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \left( \sum_{t=1}^T x_t \epsilon_t \right) $$ [^6]
Em regress√µes com vari√°veis explicativas estacion√°rias, a abordagem usual √© multiplicar essa express√£o por $\sqrt{T}$. No entanto, em modelos com tend√™ncia temporal determin√≠stica, essa abordagem n√£o √© direta devido a diferentes taxas de converg√™ncia para $\hat{\alpha}_T$ e $\hat{\delta}_T$ [^1].

> üí° **Exemplo Num√©rico:** Vamos considerar um exemplo simples com T=10, $\alpha = 2$, $\delta = 0.5$, e $\epsilon_t$ gerado a partir de uma distribui√ß√£o normal com m√©dia 0 e desvio padr√£o 1.
> ```python
> import numpy as np
> import pandas as pd
>
> np.random.seed(42)  # para reprodutibilidade
> T = 10
> alpha = 2
> delta = 0.5
> sigma = 1
>
> t = np.arange(1, T + 1)
> epsilon = np.random.normal(0, sigma, T)
> y = alpha + delta * t + epsilon
>
> X = np.vstack((np.ones(T), t)).T
> beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
>
> print(f"Estimativa de alpha: {beta_hat[0]:.4f}")
> print(f"Estimativa de delta: {beta_hat[1]:.4f}")
>
> df = pd.DataFrame({'t': t, 'y':y, 'x0':X[:,0], 'x1':X[:,1]})
> print(df)
> ```
> Este c√≥digo gera uma amostra de dados e calcula $\hat{\alpha}$ e $\hat{\delta}$ usando a f√≥rmula de MQO. Os resultados s√£o pr√≥ximos aos valores verdadeiros de $\alpha$ e $\delta$, mas as diferen√ßas s√£o influenciadas pelo ru√≠do $\epsilon_t$. Os valores calculados para $\hat{\alpha}$ e $\hat{\delta}$ demonstram o uso da f√≥rmula para $b_T$.
>
>
Para entender a dificuldade, √© preciso examinar as somas envolvidas:
$$\sum_{t=1}^T t = \frac{T(T+1)}{2}$$ [^9]
$$\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6}$$ [^10]
As parcelas dominantes em $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$ s√£o $T^2/2$ e $T^3/3$, respectivamente [^11], [^12]. Isso implica que a matriz $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ diverge quando $T$ tende ao infinito [^16]. Para obter uma matriz convergente, √© necess√°rio dividir por $T^3$ em vez de $T$ [^16]. **Lema 1** A matriz $\frac{1}{T^3}\sum_{t=1}^T x_t x_t'$ converge para uma matriz n√£o singular.

*Prova*:
I. A matriz $\frac{1}{T^3}\sum_{t=1}^T x_t x_t'$ pode ser expressa como:
$$ \frac{1}{T^3}\sum_{t=1}^T x_t x_t' = \frac{1}{T^3} \sum_{t=1}^T \begin{bmatrix} 1 \\ t \end{bmatrix} \begin{bmatrix} 1 & t \end{bmatrix} =  \frac{1}{T^3} \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix} $$

II. Usando as somas de pot√™ncias:
$$ \frac{1}{T^3}\sum_{t=1}^T x_t x_t' = \frac{1}{T^3} \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix} $$

III. Simplificando a matriz:
$$ \frac{1}{T^3}\sum_{t=1}^T x_t x_t' = \begin{bmatrix} \frac{1}{T^2} & \frac{T(T+1)}{2T^3} \\ \frac{T(T+1)}{2T^3} & \frac{T(T+1)(2T+1)}{6T^3} \end{bmatrix} $$

IV. Tomando o limite quando $T \to \infty$:
$$ \lim_{T\to\infty} \frac{1}{T^3}\sum_{t=1}^T x_t x_t' = \lim_{T\to\infty} \begin{bmatrix} \frac{1}{T^2} & \frac{T(T+1)}{2T^3} \\ \frac{T(T+1)}{2T^3} & \frac{T(T+1)(2T+1)}{6T^3} \end{bmatrix} = \begin{bmatrix} 0 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix} $$
A matriz resultante √© n√£o singular. ‚ñ†

> üí° **Exemplo Num√©rico:**  Para ilustrar o lema 1, vamos calcular a matriz $\frac{1}{T^3}\sum_{t=1}^T x_t x_t'$ com um valor espec√≠fico de $T$, por exemplo, $T=100$:
> ```python
> T = 100
> t = np.arange(1, T + 1)
> X = np.vstack((np.ones(T), t)).T
> matrix_sum = (X.T @ X) / T**3
> print(matrix_sum)
> ```
> O resultado da matriz para $T=100$ ser√° pr√≥xima de $\begin{bmatrix} 0 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}$.
> Repetindo o c√°lculo com T=1000, o resultado se aproxima ainda mais da matriz limite demonstrando a converg√™ncia.

√â importante notar que a matriz convergente para $Q$  √© dada pela express√£o que envolve $Y_T$, como mostrado posteriormente. O lema aqui apresentado √© um passo para ilustrar que $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ diverge e para mostrar a necessidade de normaliza√ß√£o com pot√™ncias de $T$.

A diferen√ßa nas taxas de converg√™ncia entre $\hat{\alpha}_T$ e $\hat{\delta}_T$ exige um ajuste na an√°lise assint√≥tica. Para chegar a distribui√ß√µes limites n√£o degeneradas, $\hat{\alpha}_T$ deve ser multiplicado por $\sqrt{T}$, enquanto $\hat{\delta}_T$ deve ser multiplicado por $T^{3/2}$ [^17]. Essa corre√ß√£o pode ser interpretada como a pr√©-multiplica√ß√£o de [16.1.6] pela matriz:
$$Y_T = \begin{bmatrix} \sqrt{T} & 0 \\ 0 & T^{3/2} \end{bmatrix}$$ [^17]
Aplicando essa transforma√ß√£o, a express√£o [16.1.6] torna-se:
$$ Y_T (b_T - \beta) = Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \left( \sum_{t=1}^T x_t \epsilon_t \right) = \left( Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right)^{-1} Y_T \left( \sum_{t=1}^T x_t \epsilon_t \right) $$ [^18]
O primeiro termo da express√£o acima, ap√≥s algumas manipula√ß√µes [^18], converge para uma matriz $Q$:
$$ Q = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix} $$ [^20]
O segundo termo, ap√≥s a transforma√ß√£o $Y_T$, resulta em:
$$ Y_T \left( \sum_{t=1}^T x_t \epsilon_t \right) = \begin{bmatrix} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t \\ \frac{1}{T^{3/2}} \sum_{t=1}^T t\epsilon_t \end{bmatrix} $$ [^21]

> üí° **Exemplo Num√©rico:** Vamos calcular a matriz $Y_T$ para $T=100$.
> ```python
> T = 100
> YT = np.array([[np.sqrt(T), 0], [0, T**(3/2)]])
> print(YT)
> ```
> A matriz $Y_T$ para $T=100$ √© usada para ajustar as taxas de converg√™ncia dos estimadores. Este c√°lculo destaca a import√¢ncia de $Y_T$ no processo de normaliza√ß√£o para an√°lise assint√≥tica.

**Lema 2** A matriz $ \left( \frac{1}{T^3} \sum_{t=1}^T x_t x_t' \right)^{-1} $ n√£o converge para a matriz $Q$ quando $T \rightarrow \infty$.

*Prova*:
I. Do *Lema 1*, sabemos que:
$$ \lim_{T\to\infty} \frac{1}{T^3}\sum_{t=1}^T x_t x_t' = \begin{bmatrix} 0 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix} $$

II. Se $\left( \frac{1}{T^3} \sum_{t=1}^T x_t x_t' \right)^{-1} $ convergir para $Q$, ent√£o
$ \left( \frac{1}{T^3} \sum_{t=1}^T x_t x_t' \right) $ deveria convergir para $Q^{-1}$.

III. Como $\begin{bmatrix} 0 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix} \neq \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{3} \end{bmatrix}^{-1}$
a afirma√ß√£o est√° provada.‚ñ†

A matriz que converge para $Q$ √©, como j√° definido no texto, $\left( Y_T \left( \sum_{t=1}^T x_t x_t' \right)^{-1} Y_T \right)^{-1}$.

Sob condi√ß√µes padr√£o sobre $\epsilon_t$, esse vetor converge assintoticamente para uma distribui√ß√£o normal bivariada com m√©dia zero. O primeiro componente, $\frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_t$, converge para $N(0, \sigma^2)$ pelo teorema do limite central [^21]. O segundo componente, $\frac{1}{T^{3/2}} \sum_{t=1}^T t\epsilon_t$, tamb√©m converge para uma distribui√ß√£o normal com m√©dia zero, com vari√¢ncia $\sigma^2/3$. A covari√¢ncia entre os dois termos √© zero [^22], [^23].
Portanto, a distribui√ß√£o assint√≥tica dos estimadores de MQO, ap√≥s o ajuste adequado com a matriz $Y_T$, pode ser expressa como:
$$ \begin{bmatrix} \sqrt{T} (\hat{\alpha}_T - \alpha) \\ T^{3/2} (\hat{\delta}_T - \delta) \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q^{-1}) $$ [^24], [^25]
onde $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$.

> üí° **Exemplo Num√©rico:** Para demonstrar a distribui√ß√£o assint√≥tica, vamos simular os estimadores de MQO para um n√∫mero grande de amostras e verificar se a distribui√ß√£o se aproxima de uma normal. Usando $\alpha=2$, $\delta=0.5$, e $\sigma=1$, vamos gerar 1000 amostras de tamanho T=100:
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> np.random.seed(42)
> num_simulations = 1000
> T = 100
> alpha = 2
> delta = 0.5
> sigma = 1
>
> alpha_hat_values = []
> delta_hat_values = []
>
> for _ in range(num_simulations):
>     t = np.arange(1, T + 1)
>     epsilon = np.random.normal(0, sigma, T)
>     y = alpha + delta * t + epsilon
>     X = np.vstack((np.ones(T), t)).T
>     beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
>     alpha_hat_values.append(beta_hat[0])
>     delta_hat_values.append(beta_hat[1])
>
> alpha_hat_values = np.array(alpha_hat_values)
> delta_hat_values = np.array(delta_hat_values)
>
> transformed_alpha = np.sqrt(T) * (alpha_hat_values - alpha)
> transformed_delta = T**(3/2) * (delta_hat_values - delta)
>
> plt.figure(figsize=(12, 6))
>
> plt.subplot(1, 2, 1)
> plt.hist(transformed_alpha, bins=30, density=True, alpha=0.6, color='blue', label='Transformed alpha')
> mu, std = 0, np.sqrt(sigma**2 * 4)
> x = np.linspace(mu - 3*std, mu + 3*std, 100)
> plt.plot(x, 1/(std * np.sqrt(2 * np.pi)) * np.exp( - (x - mu)**2 / (2 * std**2) ), color='red', label='Normal Distribution')
> plt.legend()
> plt.title('Distribution of Transformed Alpha')
>
> plt.subplot(1, 2, 2)
> plt.hist(transformed_delta, bins=30, density=True, alpha=0.6, color='green', label='Transformed delta')
> mu, std = 0, np.sqrt(sigma**2 * 12)
> x = np.linspace(mu - 3*std, mu + 3*std, 100)
> plt.plot(x, 1/(std * np.sqrt(2 * np.pi)) * np.exp( - (x - mu)**2 / (2 * std**2) ), color='red', label='Normal Distribution')
> plt.legend()
> plt.title('Distribution of Transformed Delta')
>
> plt.tight_layout()
> plt.show()
> ```
> Os histogramas dos estimadores transformados $\sqrt{T}(\hat{\alpha}_T - \alpha)$ e $T^{3/2}(\hat{\delta}_T - \delta)$ se aproximam da distribui√ß√£o normal com as vari√¢ncias te√≥ricas $\sigma^2 Q^{-1}$, confirmando o resultado da distribui√ß√£o assint√≥tica.

### Conclus√£o
Este cap√≠tulo demonstrou que a aplica√ß√£o direta de t√©cnicas de an√°lise assint√≥tica para vari√°veis estacion√°rias n√£o √© apropriada para modelos com tend√™ncias temporais determin√≠sticas. A necessidade de considerar as diferentes taxas de converg√™ncia dos estimadores de MQO, $\hat{\alpha}_T$ e $\hat{\delta}_T$, levou √† introdu√ß√£o da matriz $Y_T$ para ajustar a an√°lise assint√≥tica, resultando em distribui√ß√µes limites n√£o degeneradas. A an√°lise revelou que enquanto $\hat{\alpha}_T$ converge a uma taxa de $\sqrt{T}$, o estimador $\hat{\delta}_T$ converge a uma taxa mais r√°pida de $T^{3/2}$ [^26]. Essa distin√ß√£o √© fundamental para a correta infer√™ncia em modelos de s√©ries temporais com tend√™ncias determin√≠sticas e √© a base para o desenvolvimento de t√©cnicas mais complexas para an√°lise de processos com ra√≠zes unit√°rias que ser√£o discutidas em cap√≠tulos futuros [^1].

### Refer√™ncias
[^1]:  Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113‚Äì44.
[^2]:  Se√ß√£o 16.1, par√°grafo 1.
[^3]:  Equa√ß√£o 16.1.2 e 16.1.3.
[^5]:  Equa√ß√£o 16.1.5.
[^6]:  Equa√ß√£o 16.1.6.
[^9]:  Equa√ß√£o 16.1.9.
[^10]: Equa√ß√£o 16.1.10.
[^11]: Equa√ß√£o 16.1.11.
[^12]: Equa√ß√£o 16.1.12.
[^16]: Se√ß√£o 16.1, par√°grafo 9.
[^17]: Equa√ß√£o 16.1.17.
[^18]: Equa√ß√£o 16.1.18.
[^20]: Equa√ß√£o 16.1.20.
[^21]: Equa√ß√£o 16.1.21.
[^22]: Se√ß√£o 16.1, par√°grafos 13 e 14.
[^23]: Equa√ß√£o 16.1.23.
[^24]: Equa√ß√£o 16.1.24.
[^25]: Equa√ß√£o 16.1.25.
[^26]: Proposi√ß√£o 16.1.
<!-- END -->
