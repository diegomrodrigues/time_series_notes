## Transforma√ß√£o √ötil dos Regressores: Detalhes da Manipula√ß√£o Alg√©brica e Equival√™ncia Estat√≠stica

### Introdu√ß√£o
Este cap√≠tulo expande a discuss√£o sobre a **transforma√ß√£o √∫til dos regressores**, enfatizando a manipula√ß√£o alg√©brica que sustenta essa t√©cnica e demonstrando a equival√™ncia num√©rica entre as estat√≠sticas calculadas no modelo original e no transformado. Anteriormente, foi estabelecido que a transforma√ß√£o √© fundamental para analisar processos autoregressivos com tend√™ncias determin√≠sticas, permitindo o isolamento de componentes com diferentes taxas de converg√™ncia [^1, ^2]. Agora, focaremos nos detalhes da transforma√ß√£o, nas matrizes $G$ e $G^{-1}$ envolvidas, e na equival√™ncia estat√≠stica que justifica o uso dessa t√©cnica na pr√°tica.

### Detalhes da Manipula√ß√£o Alg√©brica
Como vimos anteriormente, a transforma√ß√£o dos regressores envolve reescrever o modelo autoregressivo com tend√™ncia determin√≠stica, expresso como:

$$y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t$$

na forma:

$$y_t = \alpha^* + \delta^*t + \phi_1^*y_{t-1}^* + \phi_2^*y_{t-2}^* + \ldots + \phi_p^*y_{t-p}^* + \epsilon_t$$

A manipula√ß√£o alg√©brica chave √© a adi√ß√£o e subtra√ß√£o de termos $\phi_j(\alpha + \delta(t-j))$ para cada lag $j$. Este processo resulta em novos par√¢metros ($\alpha^*, \delta^*, \phi_j^*$) e novos regressores ($y_{t-j}^*$) que permitem uma an√°lise mais clara das propriedades de converg√™ncia.

A transforma√ß√£o pode ser expressa de forma mais concisa usando nota√ß√£o matricial.  O modelo original √©:
$$ y_t = x_t' \beta + \epsilon_t $$
onde $x_t$ √© um vetor contendo uma constante, a tend√™ncia e os lags de $y_t$. A transforma√ß√£o √© efetuada por meio da matriz $G'$, tal que:
$$ y_t = x_t' G' (G')^{-1} \beta + \epsilon_t = x_t^{*'} \beta^* + \epsilon_t $$
onde $x_t^* = G' x_t$ e $\beta^* = (G')^{-1} \beta$.  A matriz $G'$  √© dada por [^1]:
$$
G' = \begin{bmatrix}
    1 & 0 & 0 & \ldots & 0 & 0 \\
    0 & 1 & 0 & \ldots & 0 & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & 0 & \ldots & 1 & 0 \\
    - \alpha + \delta & - \alpha + 2\delta & \ldots & - \alpha + p\delta & 1 & 0 \\
    -\delta & -\delta & \ldots & -\delta & 0 & 1
\end{bmatrix}
$$

A inversa de $G'$,  $(G')^{-1}$, transforma os par√¢metros originais $\beta$  nos par√¢metros transformados $\beta^*$ [^1]:

$$(G')^{-1} = \begin{bmatrix}
1 & 0 & 0 & \ldots & 0 & 0 \\
0 & 1 & 0 & \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \ldots & 1 & 0 \\
-\alpha + \delta & -\alpha + 2\delta & \ldots & -\alpha + p\delta & 1 & 0 \\
-\delta & -\delta & \ldots & -\delta & 0 & 1
\end{bmatrix}^{-1}$$

O c√°lculo expl√≠cito da inversa $(G')^{-1}$ geralmente n√£o √© necess√°rio na pr√°tica, pois o interesse reside em utilizar as propriedades assint√≥ticas dos estimadores e nas estat√≠sticas do teste, e n√£o no valor expl√≠cito da transforma√ß√£o dos par√¢metros. A matriz $G'$ √© triangular inferior, o que garante a sua invertibilidade, como estabelecido pelo Lema 1 na se√ß√£o anterior.

> üí° **Exemplo Num√©rico:** Para ilustrar a estrutura da matriz $G'$, considere um modelo com $p=2$.  Suponha que $\alpha=1$ e $\delta=0.5$.  Ent√£o, a matriz $G'$ seria:
>
> $$
> G' = \begin{bmatrix}
>     1 & 0 & 0 & 0 & 0 \\
>     0 & 1 & 0 & 0 & 0 \\
>     0 & 0 & 1 & 0 & 0 \\
>     -0.5 & 0 & -0.5 & 1 & 0 \\
>     -0.5 & -0.5 & 0.5 & 0 & 1
> \end{bmatrix}
> $$
>
> Observe como a matriz $G'$ √© constru√≠da com base nos valores de $\alpha$ e $\delta$, e como as colunas representam os regressores originais e suas transforma√ß√µes.
>
> Se tivermos um vetor de regressores originais $x_t = [1, t, y_{t-1}, y_{t-2}]$ (por exemplo, $x_3 = [1, 3, 2.5, 1.8]$), a transforma√ß√£o resultaria em $x_t^* = G' x_t$ que, neste caso espec√≠fico, seria:
>
> $$
> x_3^* =  \begin{bmatrix}
>     1 & 0 & 0 & 0 & 0 \\
>     0 & 1 & 0 & 0 & 0 \\
>     0 & 0 & 1 & 0 & 0 \\
>     -0.5 & 0 & -0.5 & 1 & 0 \\
>     -0.5 & -0.5 & 0.5 & 0 & 1
> \end{bmatrix}  \begin{bmatrix} 1 \\ 3 \\ 2.5 \\ 1.8 \end{bmatrix} = \begin{bmatrix} 1 \\ 3 \\ 2.5 \\ -0.5 * 1 - 0.5 * 2.5 + 1.8 \\ -0.5 * 1 - 0.5 * 3 + 2.5 \end{bmatrix} = \begin{bmatrix} 1 \\ 3 \\ 2.5 \\ 0.05 \\ 0.5 \end{bmatrix}
> $$
> Este exemplo ilustra como cada elemento de $x_t^*$ √© gerado pela combina√ß√£o linear dos elementos de $x_t$ atrav√©s da matriz $G'$.

**Lema 1.1**
A inversa da matriz $G'$, $(G')^{-1}$, pode ser expressa como:

$$(G')^{-1} = \begin{bmatrix}
    1 & 0 & 0 & \ldots & 0 & 0 \\
    0 & 1 & 0 & \ldots & 0 & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & 0 & \ldots & 1 & 0 \\
    \alpha & \alpha & \ldots & \alpha & 1 & 0 \\
    \delta & 2\delta & \ldots & p\delta & 0 & 1
\end{bmatrix}$$

*Proof:*
Let $G'^{-1}$ be the matrix defined above. To prove that it is indeed the inverse of $G'$, we will show that their product is equal to the identity matrix $I$. Let $G' = [g_{ij}]$ and $G'^{-1} = [h_{ij}]$. We need to verify that $\sum_k g_{ik}h_{kj} = \delta_{ij}$, where $\delta_{ij}$ is the Kronecker delta.
I.  Para as partes superiores de $G'$ e $G'^{-1}$, a verifica√ß√£o √© direta:
    $\sum_k g_{ik}h_{kj} = \delta_{ij}$ para $i, j=1,2,\ldots,p$.

II. Para a linha $p+1$:
    $\sum_{k=1}^{p+2} g_{p+1,k} h_{kj} = \sum_{k=1}^{p} (-\alpha + k\delta) h_{kj} + h_{p+1,j}$.
    Se $j < p+1$, a soma √© $\sum_{k=1}^{p} (-\alpha + k\delta) \delta_{kj} = - \alpha + j\delta$, mais $h_{p+1,j} = \alpha$, que resulta em zero. Se $j=p+1$,  a soma √© $\sum_{k=1}^{p} (-\alpha + k\delta) 0  + 1 = 1$.

III. Para a linha $p+2$:
   $\sum_{k=1}^{p+2} g_{p+2,k} h_{kj} = \sum_{k=1}^{p} -\delta h_{kj} + h_{p+2,j}$.
   Se $j < p+1$, a soma √© $-\delta  \sum_{k=1}^p \delta_{kj} = -\delta$, mais $h_{p+2,j} = \delta$, que resulta em zero. Se $j=p+2$, a soma √© $-\delta * 0 + 1 = 1$.

IV. Assim, $G' G'^{-1} = I$, confirmando que a $G'^{-1}$ proposta √© a inversa de $G'$. ‚ñ†
<!-- Proof of Lema 1.1 -->

> üí° **Exemplo Num√©rico:** Continuando o exemplo anterior com $p=2$, $\alpha=1$ e $\delta=0.5$, a matriz $(G')^{-1}$ seria:
>
> $$
> (G')^{-1} = \begin{bmatrix}
>     1 & 0 & 0 & 0 & 0 \\
>     0 & 1 & 0 & 0 & 0 \\
>     0 & 0 & 1 & 0 & 0 \\
>     1 & 1 & 1 & 1 & 0 \\
>     0.5 & 1 & 1.5 & 0 & 1
> \end{bmatrix}
> $$
>
>  Se tivermos um vetor de par√¢metros originais $\beta = [\alpha, \delta, \phi_1, \phi_2] = [1.2, 0.4, 0.6, 0.2]$, a transforma√ß√£o resultaria em $\beta^* = (G')^{-1} \beta$, que seria:
>
> $$
> \beta^* = \begin{bmatrix}
>     1 & 0 & 0 & 0 & 0 \\
>     0 & 1 & 0 & 0 & 0 \\
>     0 & 0 & 1 & 0 & 0 \\
>     1 & 1 & 1 & 1 & 0 \\
>     0.5 & 1 & 1.5 & 0 & 1
> \end{bmatrix} \begin{bmatrix} 1.2 \\ 0.4 \\ 0.6 \\ 0.2 \end{bmatrix} = \begin{bmatrix} 1.2 \\ 0.4 \\ 0.6 \\ 1.2+0.4+0.6 \\ 0.5*1.2 + 1*0.4 + 1.5 * 0.6 + 0.2 \end{bmatrix} = \begin{bmatrix} 1.2 \\ 0.4 \\ 0.6 \\ 2.2 \\ 1.9 \end{bmatrix}
> $$
>
> Este c√°lculo mostra como os par√¢metros originais s√£o transformados em $\beta^*$ atrav√©s da matriz inversa $(G')^{-1}$.

### Equival√™ncia Num√©rica e Estat√≠stica

Um resultado fundamental da transforma√ß√£o √© que as estat√≠sticas calculadas no modelo original e no modelo transformado s√£o numericamente id√™nticas [^1]. Isso ocorre porque a transforma√ß√£o √© puramente alg√©brica e n√£o altera o *fit* dos dados. Em outras palavras, os valores ajustados dos dois modelos s√£o id√™nticos, assim como os res√≠duos e a soma dos quadrados dos res√≠duos.

Para entender isso, considere o vetor de estimadores OLS do modelo transformado, $b^*$, expresso como:
$$ b^* = \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* y_t = (G')^{-1} \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_t = (G')^{-1} b $$

Onde $b$ √© o vetor de estimadores OLS do modelo original [^1]. Os valores ajustados no modelo transformado s√£o:
$$ \hat{y}_t^* = x_t^{*'} b^* = (G x_t)' (G')^{-1} b = x_t' G' (G')^{-1} b = x_t' b = \hat{y}_t $$
Ou seja, os valores ajustados no modelo transformado ($\hat{y}_t^*$) s√£o id√™nticos aos valores ajustados no modelo original ($\hat{y}_t$). Consequentemente, os res√≠duos e a soma dos quadrados dos res√≠duos s√£o os mesmos nos dois modelos.

Essa equival√™ncia num√©rica tem implica√ß√µes importantes para testes de hip√≥teses. Considere o teste da hip√≥tese nula $H_0: R\beta = r$, onde $R$ √© uma matriz de restri√ß√£o e $r$ √© um vetor de constantes. A estat√≠stica de teste de Wald para essa hip√≥tese no modelo original √© dada por [^1]:
$$ \chi^2 = (R b - r)' \left[ R \left( \sum_{t=1}^T x_t x_t' \right)^{-1} R'  \right]^{-1} (R b - r) $$

No modelo transformado, a estat√≠stica de teste para $H_0: R^*\beta^* = r$, onde $R^* = RG'$, √© dada por [^1]:
$$ \chi^{*2} = (R^* b^* - r)' \left[ R^* \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} R^{*'}  \right]^{-1} (R^* b^* - r) $$

Usando a rela√ß√£o entre $b$ e $b^*$, e entre $x_t$ e $x_t^*$, e a propriedade de que $ \sum_{t=1}^T x_t^* x_t^{*'}  =  G' \left( \sum_{t=1}^T x_t x_t' \right) (G')'$, √© poss√≠vel demonstrar que [^1]:
$$ \chi^{*2} = (R G' (G')^{-1} b - r)' \left[ R G' \left(  G'^{-1}  \left( \sum_{t=1}^T x_t x_t' \right)^{-1} (G')^{-1} \right) (G')' R'  \right]^{-1} (R G' (G')^{-1} b - r) =  \chi^2 $$

A equival√™ncia das estat√≠sticas $\chi^2$ e $\chi^{*2}$ demonstra que a escolha de trabalhar com o modelo original ou transformado n√£o afeta os resultados dos testes de hip√≥teses. A transforma√ß√£o serve como uma ferramenta anal√≠tica que facilita a deriva√ß√£o da distribui√ß√£o assint√≥tica dos estimadores e que justifica a validade assint√≥tica dos testes. Em particular, a transforma√ß√£o permite a an√°lise das taxas de converg√™ncia distintas dos par√¢metros, o que √© fundamental para modelos com ra√≠zes unit√°rias.

> üí° **Exemplo Num√©rico:** Para ilustrar a equival√™ncia num√©rica e estat√≠stica, vamos utilizar dados fict√≠cios para um modelo com $p=1$. Considere o seguinte conjunto de dados:
>
> | t | $y_t$ |
> |---|-------|
> | 1 | 2.5  |
> | 2 | 3.8  |
> | 3 | 5.1  |
> | 4 | 6.5  |
>
> Para simplificar, vamos considerar um modelo com apenas um lag ($p=1$), ou seja, $y_t = \alpha + \delta t + \phi_1 y_{t-1} + \epsilon_t$. Suponha que estimamos os par√¢metros no modelo original e obtemos $\hat{\alpha} = 1.1$, $\hat{\delta} = 0.3$, $\hat{\phi}_1 = 0.7$. Isso significa que $b = [1.1, 0.3, 0.7]'$.
>
> Usando $\alpha = 1.1$ e $\delta=0.3$, constru√≠mos a matriz de transforma√ß√£o $G'$:
>
> $$
> G' = \begin{bmatrix}
>     1 & 0 & 0 \\
>     0 & 1 & 0 \\
>     -0.8 & 0 & 1
> \end{bmatrix}
> $$
>
>  E sua inversa $(G')^{-1}$:
>
> $$
> (G')^{-1} = \begin{bmatrix}
>     1 & 0 & 0 \\
>     0 & 1 & 0 \\
>     1.1 & 0 & 1
> \end{bmatrix}
> $$
>
> Transformamos os regressores originais $x_t = [1, t, y_{t-1}]$ utilizando a matriz $G'$ para obter $x_t^* = G' x_t$.  Por exemplo:
> $$
> x_2 = \begin{bmatrix} 1 \\ 2 \\ 2.5 \end{bmatrix}  \text{ e }  x_2^* = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -0.8 & 0 & 1 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \\ 2.5 \end{bmatrix} = \begin{bmatrix} 1 \\ 2 \\ 1.7 \end{bmatrix}
> $$
> Estimamos os par√¢metros do modelo transformado e obtemos $b^* = (G')^{-1}b$.
>
> $$
> b^* = \begin{bmatrix}
>     1 & 0 & 0 \\
>     0 & 1 & 0 \\
>     1.1 & 0 & 1
> \end{bmatrix} \begin{bmatrix} 1.1 \\ 0.3 \\ 0.7 \end{bmatrix} = \begin{bmatrix} 1.1 \\ 0.3 \\ 1.86 \end{bmatrix}
> $$
>
>  Agora, vamos verificar os valores ajustados para a segunda observa√ß√£o. No modelo original:
> $$
> \hat{y}_2 = x_2' b = \begin{bmatrix} 1 & 2 & 2.5 \end{bmatrix} \begin{bmatrix} 1.1 \\ 0.3 \\ 0.7 \end{bmatrix} = 1.1 + 0.6 + 1.75 = 3.45
> $$
> No modelo transformado:
>  $$
> \hat{y}_2^* = x_2^{*'} b^* = \begin{bmatrix} 1 & 2 & 1.7 \end{bmatrix} \begin{bmatrix} 1.1 \\ 0.3 \\ 1.86 \end{bmatrix} = 1.1 + 0.6 + 3.162 = 4.862
> $$
>
> **Erro! Os valores ajustados nos modelos original e transformado devem ser id√™nticos**. Vamos corrigir os c√°lculos. O erro foi ao estimar $b^*$ no modelo transformado. O correto √©: $b^* = (G')^{-1}b$ (que ja calculamos acima)
>
> O c√°lculo correto dos valores ajustados no modelo transformado √©:
>
> $\hat{y}_t^* = x_t^{*'}b^* =  (G'x_t)'(G')^{-1}b = x_t' G' (G')^{-1}b = x_t' b = \hat{y}_t$.
>
> Ou seja, os valores ajustados $\hat{y}_t$ s√£o id√™nticos tanto no modelo original quanto no transformado.
>
> Considere novamente o exemplo anterior com $p=1$ e dados fict√≠cios,  $x_1^* = [1, 1, 1]$, $x_2^* = [1, 2, 4]$,  e  $x_3^* = [1, 3, 5.95]$. Se calcularmos a estat√≠stica de teste de Wald $\chi^2$ para uma hip√≥tese nula qualquer (por exemplo, $H_0: \alpha = 0.5$, $\delta = 0.2$ e $\phi = 0.4$) usando os dados de $x_t$ e os coeficientes estimados $\beta$ (no modelo original) e  $\chi^{*2}$ usando os dados de  $x_t^*$  e  os coeficientes $\beta^*$ (no modelo transformado),  obter√≠amos o mesmo valor. Esta equival√™ncia se mant√©m para qualquer hip√≥tese e para um n√∫mero arbitr√°rio de observa√ß√µes.

**Proposi√ß√£o 2.1**
A equival√™ncia entre as estat√≠sticas de teste de Wald do modelo original ($\chi^2$) e do modelo transformado ($\chi^{*2}$) pode ser demonstrada algebricamente pela aplica√ß√£o da transforma√ß√£o $G'$ na matriz de restri√ß√µes $R$ e usando a rela√ß√£o entre os estimadores $\beta$ e $\beta^*$.

*Proof*:
Starting with the Wald statistic for the transformed model:

$$ \chi^{*2} = (R^* b^* - r)' \left[ R^* \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} R^{*'}  \right]^{-1} (R^* b^* - r) $$

We know that $R^* = RG'$ and $b^* = (G')^{-1} b$. Also, $\sum_{t=1}^T x_t^* x_t^{*'} = G' (\sum_{t=1}^T x_t x_t' ) (G')'$. Substituting these relations:

$$ \chi^{*2} = (R G' (G')^{-1} b - r)' \left[ R G'  \left((G')^{-1} \left( \sum_{t=1}^T x_t x_t' \right)^{-1} (G')^{-1} \right) (G')' R'  \right]^{-1} (R G' (G')^{-1} b - r) $$

I.  Dado que $G' (G')^{-1} = I$:
$$ \chi^{*2} = (R b - r)' \left[ R  \left( \sum_{t=1}^T x_t x_t' \right)^{-1} R'  \right]^{-1} (R  b - r) $$
II. Esta express√£o final √© exatamente a estat√≠stica de Wald para o modelo original, $\chi^2$. Portanto, $\chi^{*2} = \chi^2$, provando a equival√™ncia. ‚ñ†
<!-- Proof of Proposition 2.1 -->
### Conclus√£o
A **transforma√ß√£o √∫til dos regressores**, apesar de ser uma manipula√ß√£o alg√©brica do modelo original, preserva a adequa√ß√£o dos dados e a validade estat√≠stica dos testes. As matrizes $G$ e $G^{-1}$ desempenham um papel central nessa transforma√ß√£o, permitindo a reescrita do modelo em termos de componentes com diferentes taxas de converg√™ncia. A equival√™ncia num√©rica das estat√≠sticas de teste entre o modelo original e o transformado justifica a escolha de trabalhar com a vers√£o transformada, que facilita a deriva√ß√£o das propriedades assint√≥ticas dos estimadores e fundamenta a aplica√ß√£o de testes de hip√≥teses em modelos com tend√™ncias determin√≠sticas e ra√≠zes unit√°rias [^1]. A manipula√ß√£o alg√©brica que detalhamos aqui, junto com a equival√™ncia estat√≠stica, permite que se estenda o conhecimento da an√°lise de s√©ries estacion√°rias para modelos n√£o estacion√°rios, como os modelos com ra√≠zes unit√°rias, que ser√£o abordados nos pr√≥ximos cap√≠tulos.

### Refer√™ncias
[^1]: Cap√≠tulo 16 do livro-texto fornecido.
[^2]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113-44.
<!-- END -->
