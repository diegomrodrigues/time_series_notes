## Transforma√ß√£o √ötil dos Regressores em Modelos com Tend√™ncias Determin√≠sticas

### Introdu√ß√£o
Este cap√≠tulo, construindo sobre o conceito de processos com tend√™ncias determin√≠sticas, explora em detalhes a **transforma√ß√£o √∫til dos regressores**, um m√©todo essencial para analisar processos autoregressivos em torno de uma tend√™ncia determin√≠stica. Em particular, esta se√ß√£o detalha a reescrita do modelo original em uma forma que isola os componentes com diferentes taxas de converg√™ncia, facilitando a deriva√ß√£o das distribui√ß√µes assint√≥ticas dos estimadores. Este m√©todo, seguindo a abordagem de Sims, Stock e Watson (1990), revela-se crucial para entender o comportamento de estimadores em modelos com ra√≠zes unit√°rias e tend√™ncias determin√≠sticas [^1, ^2].

### Conceitos Fundamentais

A se√ß√£o anterior introduziu o modelo autoregressivo geral em torno de uma tend√™ncia determin√≠stica [^1]:
$$y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \epsilon_t,$$
onde $\epsilon_t$ √© um ru√≠do branco i.i.d com m√©dia zero, vari√¢ncia $\sigma^2$ e momento quarto finito. Para analisar esse modelo com as ferramentas de an√°lise de s√©ries temporais estacion√°rias, uma transforma√ß√£o dos regressores √© necess√°ria. A ideia central √© reescrever o modelo de tal forma que ele contenha termos com m√©dia zero que sejam *covari√¢ncia-estacion√°rios*, al√©m de um termo constante e um termo de tend√™ncia linear. Essa reescrita permite isolar os componentes do vetor de coeficientes OLS com diferentes taxas de converg√™ncia [^1].

A transforma√ß√£o √∫til dos regressores envolve a adi√ß√£o e subtra√ß√£o de termos da forma $\phi_j(\alpha + \delta(t-j))$ para cada lag $j = 1, 2, \ldots, p$ no lado direito da equa√ß√£o original [^1]:

$$
\begin{aligned}
y_t &= \alpha(1 + \phi_1 + \phi_2 + \ldots + \phi_p) + \delta(1 + \phi_1 + \phi_2 + \ldots + \phi_p)t \\
&- \delta(\phi_1 + 2\phi_2 + \ldots + p\phi_p) + \phi_1[y_{t-1} - \alpha - \delta(t-1)] \\
&+ \phi_2[y_{t-2} - \alpha - \delta(t-2)] + \ldots + \phi_p[y_{t-p} - \alpha - \delta(t-p)] + \epsilon_t
\end{aligned}
$$
Essa manipula√ß√£o resulta em um novo modelo [^1]:
$$y_t = \alpha^* + \delta^*t + \phi_1^*y_{t-1}^* + \phi_2^*y_{t-2}^* + \ldots + \phi_p^*y_{t-p}^* + \epsilon_t,$$
onde:
$$
\begin{aligned}
\alpha^* &= \alpha(1 + \phi_1 + \phi_2 + \ldots + \phi_p) - \delta(\phi_1 + 2\phi_2 + \ldots + p\phi_p) \\
\delta^* &= \delta(1 + \phi_1 + \phi_2 + \ldots + \phi_p) \\
\phi_j^* &= \phi_j \quad \text{para} \quad j = 1, 2, \ldots, p,
\end{aligned}
$$
e os novos regressores s√£o definidos como:
$$
y_{t-j}^* = y_{t-j} - \alpha - \delta(t-j) \quad \text{para} \quad j = 1, 2, \ldots, p.
$$
Os regressores $y_{t-j}^*$ s√£o agora processos de m√©dia zero, facilitando a an√°lise de converg√™ncia dos coeficientes associados. Notavelmente, essa transforma√ß√£o n√£o altera a adequa√ß√£o geral do modelo, apenas a maneira como os regressores s√£o expressos [^1].

> üí° **Exemplo Num√©rico:** Para ilustrar, considere um modelo AR(1) com uma tend√™ncia determin√≠stica: $y_t = 2 + 0.5t + 0.7y_{t-1} + \epsilon_t$. Aqui, $\alpha = 2$, $\delta = 0.5$, e $\phi_1 = 0.7$.  Aplicando a transforma√ß√£o:
>
>$\alpha^* = 2(1 + 0.7) - 0.5(1*0.7) = 2(1.7) - 0.35 = 3.4 - 0.35 = 3.05$
>
>$\delta^* = 0.5(1 + 0.7) = 0.5(1.7) = 0.85$
>
>$\phi_1^* = 0.7$
>
>O regressor transformado √© $y_{t-1}^* = y_{t-1} - 2 - 0.5(t-1)$. Assim, o modelo transformado √©: $y_t = 3.05 + 0.85t + 0.7y_{t-1}^* + \epsilon_t$. Note como $\alpha^*$, $\delta^*$, e $y_{t-1}^*$ s√£o agora componentes isolados.

O objetivo desta transforma√ß√£o √© isolar componentes que possuem diferentes taxas de converg√™ncia. Especificamente, a estima√ß√£o OLS da equa√ß√£o transformada resulta em coeficientes para os regressores estacion√°rios $y^*_{t-i}$ que convergem a uma taxa de $\sqrt{T}$ para uma distribui√ß√£o gaussiana, enquanto os coeficientes $\alpha^*$ e $\delta^*$ convergem a taxas diferentes. O termo $\delta^*$ converge a uma taxa de $T^{3/2}$ e √© essencial para analisar os efeitos da tend√™ncia determin√≠stica. Os resultados obtidos para o modelo de tend√™ncia simples na Se√ß√£o 16.1 [^1] s√£o um caso especial desta transforma√ß√£o quando $p=0$.

**Proposi√ß√£o 1**
A transforma√ß√£o dos regressores, como apresentada, preserva a esperan√ßa condicional da s√©rie temporal, dado seu passado.
*Prova:*
I. Seja $Y_{t-1} = \{y_{t-1}, y_{t-2}, \ldots \}$ o hist√≥rico da s√©rie temporal at√© o per√≠odo $t-1$. No modelo original, temos
$$E[y_t | Y_{t-1}] = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p}.$$

II. Aplicando a transforma√ß√£o, o modelo equivalente √©
$$E[y_t | Y_{t-1}] = \alpha^* + \delta^* t + \phi_1^*y_{t-1}^* + \phi_2^*y_{t-2}^* + \ldots + \phi_p^*y_{t-p}^*.$$

III. Substituindo as defini√ß√µes de $\alpha^*$, $\delta^*$, $\phi_j^*$ e $y_{t-j}^*$, obtemos:
$$E[y_t | Y_{t-1}] = \left(\alpha(1 + \sum_{j=1}^p \phi_j) - \delta\sum_{j=1}^p j\phi_j\right) + \delta(1 + \sum_{j=1}^p \phi_j)t + \sum_{j=1}^p \phi_j(y_{t-j} - \alpha - \delta(t-j)).$$

IV. Expandindo e reagrupando os termos, temos:
$$E[y_t | Y_{t-1}] = \alpha + \alpha\sum_{j=1}^p \phi_j - \delta\sum_{j=1}^p j\phi_j + \delta t + \delta t \sum_{j=1}^p \phi_j + \sum_{j=1}^p \phi_j y_{t-j} - \alpha\sum_{j=1}^p \phi_j - \delta\sum_{j=1}^p \phi_j (t-j).$$

V. Simplificando os termos, cancelando termos iguais e reagrupando, obtemos:
$$E[y_t | Y_{t-1}] = \alpha + \delta t + \sum_{j=1}^p \phi_j y_{t-j} - \delta\sum_{j=1}^p j\phi_j + \delta\sum_{j=1}^p \phi_j j$$
$$E[y_t | Y_{t-1}] = \alpha + \delta t + \sum_{j=1}^p \phi_j y_{t-j}$$
que √© id√™ntico √† esperan√ßa condicional do modelo original. Portanto, a transforma√ß√£o preserva a esperan√ßa condicional. ‚ñ†

√â √∫til descrever essa transforma√ß√£o em termos de nota√ß√£o matricial. O modelo original pode ser escrito como [^1]:
$$ y_t = x_t' \beta + \epsilon_t, $$
onde $x_t$ √© um vetor de regressores (incluindo os lags $y_{t-i}$, uma constante e uma tend√™ncia) e $\beta$ √© o vetor de coeficientes.  A transforma√ß√£o √© equivalente a reescrever o modelo como:
$$ y_t = x_t' G' (G')^{-1} \beta + \epsilon_t = x_t^{*'} \beta^* + \epsilon_t. $$
Aqui, $G'$ √© uma matriz que efetua a transforma√ß√£o dos regressores, conforme definido em [16.3.8] e [16.3.9] [^1]:
$$
G' = \begin{bmatrix}
    1 & 0 & 0 & \ldots & 0 & 0 \\
    0 & 1 & 0 & \ldots & 0 & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & 0 & \ldots & 1 & 0 \\
    - \alpha + \delta & - \alpha + 2\delta & \ldots & - \alpha + p\delta & 1 & 0 \\
    -\delta & -\delta & \ldots & -\delta & 0 & 1
\end{bmatrix}
$$

e
$$
x_t^* = G x_t.
$$

O vetor $\beta^*$ √©, portanto,  $\beta^* = (G')^{-1} \beta$ [^1]. A estima√ß√£o OLS do modelo transformado √© dada por:

$$ b^* = \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* y_t = (G')^{-1} \left( \sum_{t=1}^T x_t x_t' \right)^{-1} \sum_{t=1}^T x_t y_t = (G')^{-1} b, $$
onde $b$ representa os estimadores OLS do modelo original [^1]. A forma transformada mant√©m a equival√™ncia alg√©brica do modelo e facilita a an√°lise das propriedades dos estimadores.

> üí° **Exemplo Num√©rico:**  Vamos considerar o exemplo anterior com $p=1$ e dados fict√≠cios. Suponha que, ap√≥s a transforma√ß√£o, temos $x_t^* = [1, t, y_{t-1}^*]'$. Vamos simplificar ainda mais considerando apenas tr√™s observa√ß√µes.
>
> Digamos que para t=1, $x_1 = [1, 1, y_0]$, $y_1 = 6.5$, e $x_1^* = [1, 1,  y_0^* = y_0 - 2 - 0.5(1-1)] = [1, 1, y_0 - 2]$
> para t=2, $x_2 = [1, 2, y_1]$, $y_2 = 8.95$, e $x_2^* = [1, 2, y_1^* = y_1 - 2 - 0.5(2-1)] = [1, 2, y_1 - 2.5]$
> para t=3, $x_3 = [1, 3, y_2]$, $y_3 = 10.565$, e $x_3^* = [1, 3, y_2^* = y_2 - 2 - 0.5(3-1)] = [1, 3, y_2 - 3]$
>
>Suponha que y0 = 3, y1 = 6.5, e y2 = 8.95. Ent√£o,
> $x_1^* = [1, 1, 1]$,
> $x_2^* = [1, 2, 4]$,
> $x_3^* = [1, 3, 5.95]$.
>
> $\beta = [\alpha, \delta, \phi_1]^T$  e $\beta^* = [\alpha^*, \delta^*, \phi_1^*]^T$
>
>A matriz $G'$ para p=1 e com os valores anteriores  $\alpha = 2$ e $\delta = 0.5$ √©:
>
> $G' = \begin{bmatrix} 1 & 0 & 0 \\  -2+0.5 & 1 & 0 \\ -0.5 & 0 & 1 \end{bmatrix}  = \begin{bmatrix} 1 & 0 & 0 \\  -1.5 & 1 & 0 \\ -0.5 & 0 & 1 \end{bmatrix}$
>
>A inversa de $G'$ √©
>
>$(G')^{-1} = \begin{bmatrix} 1 & 0 & 0 \\  1.5 & 1 & 0 \\ 0.5 & 0 & 1 \end{bmatrix}$
>
>
>Note que  $\beta^* = (G')^{-1} \beta$. Se assumirmos que os coeficientes $\beta$ obtidos por OLS no modelo original s√£o $\beta = [2, 0.5, 0.7]^T$, ent√£o:
> $\beta^* = \begin{bmatrix} 1 & 0 & 0 \\  1.5 & 1 & 0 \\ 0.5 & 0 & 1 \end{bmatrix}  \begin{bmatrix} 2 \\ 0.5 \\ 0.7 \end{bmatrix} = \begin{bmatrix} 2 \\ 3.5 \\ 1.7 \end{bmatrix} $
>
> Observe como o $\alpha^*$ e $\delta^*$ calculados anteriormente (3.05 e 0.85) n√£o s√£o exatamente iguais a $\beta^*$, pois aqui usamos um exemplo simplificado sem o c√°lculo da regress√£o em si. Os par√¢metros $\alpha^*$ e $\delta^*$  apresentados no exemplo anterior  s√£o as transforma√ß√µes te√≥ricas dos par√¢metros do modelo original.

**Lema 1**
A matriz $G'$ √© invert√≠vel, desde que $p$ seja finito, o que garante que a transforma√ß√£o √© sempre bem-definida.

*Prova:*
I. Para demonstrar que $G'$ √© invert√≠vel, √© suficiente verificar que o seu determinante √© n√£o-nulo.

II. A matriz $G'$ √© uma matriz triangular inferior, exceto pelas duas √∫ltimas linhas. No entanto, a parte que afeta o determinante √© a submatriz formada pelas $p+2$ primeiras linhas e colunas.

III. Essa submatriz √© triangular inferior com todos os elementos diagonais iguais a 1.

IV. Portanto, o determinante desta submatriz √© o produto dos elementos diagonais, que √© igual a 1.

V. Dado que a submatriz determinante de $G'$ √© n√£o nula, o determinante de $G'$ tamb√©m √© n√£o nulo.

VI. Concluindo, uma matriz com um determinante n√£o-nulo √© invert√≠vel, portanto, $G'$ √© invert√≠vel, e a transforma√ß√£o √© bem definida. ‚ñ†

### Conclus√£o

A **transforma√ß√£o √∫til dos regressores** √© uma ferramenta essencial para analisar processos autoregressivos com tend√™ncias determin√≠sticas. Ao reescrever o modelo em uma forma que separa os componentes com diferentes taxas de converg√™ncia, esta t√©cnica permite derivar as distribui√ß√µes assint√≥ticas dos estimadores e construir testes de hip√≥teses apropriados, como explorado nas pr√≥ximas se√ß√µes [^1]. A transforma√ß√£o estabelece uma ponte entre as ferramentas de an√°lise de s√©ries temporais estacion√°rias e o estudo de processos n√£o estacion√°rios, fundamentando a base para a an√°lise de modelos com ra√≠zes unit√°rias. Este m√©todo de transforma√ß√£o e an√°lise, conforme a metodologia de Sims, Stock e Watson (1990), √© um pilar na an√°lise de modelos de s√©ries temporais n√£o estacion√°rios [^1].

### Refer√™ncias
[^1]: Cap√≠tulo 16 do livro-texto fornecido.
[^2]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113-44.
<!-- END -->
