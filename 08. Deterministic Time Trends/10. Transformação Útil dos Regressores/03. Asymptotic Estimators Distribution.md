## Distribui√ß√£o Assint√≥tica dos Estimadores Transformados: Matrizes de Momentos e o Teorema do Limite Central

### Introdu√ß√£o

Este cap√≠tulo aprofunda a an√°lise da **transforma√ß√£o √∫til dos regressores**, focando na deriva√ß√£o da distribui√ß√£o assint√≥tica dos estimadores no modelo transformado. Nas se√ß√µes anteriores, estabelecemos a import√¢ncia dessa transforma√ß√£o para analisar processos autoregressivos com tend√™ncias determin√≠sticas, detalhando a manipula√ß√£o alg√©brica e a equival√™ncia estat√≠stica entre os modelos original e transformado [^1, ^2]. Agora, empregaremos resultados de matrizes de momentos e o teorema do limite central para deduzir formalmente a distribui√ß√£o assint√≥tica dos estimadores transformados, mostrando como essa abordagem √© essencial para estudar modelos de s√©ries temporais mais complexos, incluindo aqueles com ra√≠zes unit√°rias.

### Deriva√ß√£o da Distribui√ß√£o Assint√≥tica

A transforma√ß√£o dos regressores permite que o modelo autoregressivo com tend√™ncia determin√≠stica seja reescrito na forma [^1]:

$$y_t = \alpha^* + \delta^*t + \phi_1^*y_{t-1}^* + \phi_2^*y_{t-2}^* + \ldots + \phi_p^*y_{t-p}^* + \epsilon_t$$

onde $\alpha^*$, $\delta^*$ e $\phi_j^*$ s√£o os par√¢metros transformados, e $y_{t-j}^*$ s√£o os regressores transformados com m√©dia zero. O modelo pode ser expresso em nota√ß√£o matricial como [^1]:

$$ y_t = x_t^{*'} \beta^* + \epsilon_t $$

onde $x_t^*$ √© o vetor de regressores transformados e $\beta^*$ √© o vetor de par√¢metros transformados. O estimador OLS de $\beta^*$, denotado por $b^*$, √© dado por [^1]:

$$ b^* = \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* y_t $$

Para derivar a distribui√ß√£o assint√≥tica de $b^*$, √© conveniente expressar a diferen√ßa entre $b^*$ e o verdadeiro valor $\beta^*$ como [^1]:

$$ b^* - \beta^* = \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* \epsilon_t $$

Para analisar o comportamento assint√≥tico de $b^*$, multiplicamos ambos os lados por $\Upsilon_T$, uma matriz de escalonamento projetada para acomodar as diferentes taxas de converg√™ncia dos elementos de $\beta^*$. A matriz $\Upsilon_T$ √© definida como [^1]:

$$
\Upsilon_T = \begin{bmatrix}
    \sqrt{T} & 0 & 0 & \ldots & 0 & 0 \\
    0 & \sqrt{T} & 0 & \ldots & 0 & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & 0 & \ldots & \sqrt{T} & 0 \\
    0 & 0 & 0 & \ldots & 0 & T^{3/2}
\end{bmatrix}
$$
O objetivo da matriz $\Upsilon_T$ √© normalizar os elementos de $b^*$, de forma que a distribui√ß√£o limite n√£o seja degenerada. Note que, no caso de $p=0$, recuperamos a matriz de escalonamento apresentada na se√ß√£o 16.1 do livro [^1].

> üí° **Exemplo Num√©rico:** Para ilustrar, considere um cen√°rio com $T=100$ e $p=1$.  A matriz $\Upsilon_T$ seria:
>
> $$
> \Upsilon_{100} = \begin{bmatrix}
>     10 & 0 & 0 \\
>     0 & 10 & 0 \\
>     0 & 0 & 1000
> \end{bmatrix}
> $$
>
> Aqui, os dois primeiros elementos da diagonal s√£o $\sqrt{100} = 10$ e o √∫ltimo √© $100^{3/2} = 1000$.  Esta matriz ser√° usada para normalizar os par√¢metros da regress√£o transformada, onde os par√¢metros associados aos regressores defasados ($y_{t-1}^*$ e intercepto) convergem a uma taxa de $O(1/\sqrt{T})$ e a tend√™ncia convergem a uma taxa de $O(1/T^{3/2})$.

Multiplicando $b^* - \beta^*$ por $\Upsilon_T$, temos:

$$
\Upsilon_T(b^* - \beta^*) = \Upsilon_T \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* \epsilon_t
$$
Utilizando a nota√ß√£o $\Gamma_T = \sum_{t=1}^T x_t^* x_t^{*'}$, podemos reescrever a equa√ß√£o acima como:
$$
\Upsilon_T(b^* - \beta^*) = \left( \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1} \right)^{-1} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t
$$
Podemos expressar essa equa√ß√£o como:
$$
\Upsilon_T(b^* - \beta^*) = \left( \frac{1}{T} \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1} \right)^{-1} \frac{1}{\sqrt{T}} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t
$$

O pr√≥ximo passo √© derivar as distribui√ß√µes limites dos dois termos no lado direito da equa√ß√£o acima.

**Lema 2**
A matriz $\frac{1}{T} \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1}$ converge em probabilidade para uma matriz n√£o singular $Q^*$, tal que:

$$ \frac{1}{T} \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1} \xrightarrow{p} Q^* $$

*Prova:*
I.  A matriz $\Gamma_T$ √© definida como $\sum_{t=1}^T x_t^* x_t^{*'}$. Em [16.3.15] do cap√≠tulo 16 [^1], o livro define a matriz Q*. Para derivar o limite da matriz $\frac{1}{T} \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1}$, devemos analisar os componentes desta matriz quando T vai ao infinito.

II. O termo $\frac{1}{T} \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1}$ pode ser escrito como:

$$\frac{1}{T} \Upsilon_T^{-1} \left( \sum_{t=1}^T x_t^* x_t^{*'} \right) \Upsilon_T^{-1} =  \sum_{t=1}^T  \Upsilon_T^{-1} x_t^* x_t^{*'}  \Upsilon_T^{-1} \frac{1}{T} $$

III.  A matriz $\Upsilon_T^{-1}$ divide as linhas e colunas apropriadamente de $x_t^*$ para obter as diferentes taxas de converg√™ncia dos par√¢metros. Os elementos de $x_t^*$ s√£o formados pelos regressores transformados $y_{t-j}^*$  que s√£o estacion√°rios de m√©dia zero com um termo constante e uma tend√™ncia.

IV.  Desta forma, o termo $\frac{1}{T} \sum_{t=1}^T  \Upsilon_T^{-1} x_t^* x_t^{*'}  \Upsilon_T^{-1} $ converge para a matriz $Q^*$ definida como:
$$
Q^* = \begin{bmatrix}
 E[y_0^* y_0^{*'}] & E[y_0^* y_1^{*'}] & \ldots & E[y_0^* y_{p-1}^{*'}] & 0 & 0 \\
  E[y_1^* y_0^{*'}] & E[y_1^* y_1^{*'}] & \ldots & E[y_1^* y_{p-1}^{*'}] & 0 & 0 \\
    \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
   E[y_{p-1}^* y_0^{*'}] & E[y_{p-1}^* y_1^{*'}]  & \ldots &  E[y_{p-1}^* y_{p-1}^{*'}] & 0 & 0 \\
    0 & 0  & \ldots & 0 & 1 & 0 \\
     0 & 0 & \ldots & 0 & 0 & 1
\end{bmatrix}
$$

Onde $E[y_i^* y_j^{*'}]$ denota o valor esperado de $y_i^*y_j^*$. Note que $Q^*$ √© id√™ntica √† matriz Q* apresentada no Ap√™ndice 16A do livro [^1], exceto pela estrutura de $x_t^*$ (incluindo um termo constante e uma tend√™ncia). Como os termos  $E[y_i^* y_j^{*'}]$ s√£o covari√¢ncia estacion√°rios, o limite da matriz $\frac{1}{T} \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1}$ converge em probabilidade para a matriz $Q^*$. Como todos os elementos da diagonal de $Q^*$ s√£o positivos,  $Q^*$ √© uma matriz n√£o singular, o que garante a exist√™ncia de $(Q^*)^{-1}$.
‚ñ†

> üí° **Exemplo Num√©rico:**  Suponha que temos um modelo AR(1) com uma tend√™ncia, ou seja, $p=1$.  Nesse caso, a matriz $Q^*$ ter√° a seguinte estrutura:
>
>  $$
> Q^* = \begin{bmatrix}
>  E[y_0^{*2}] & E[y_0^* y_1^*] & 0 & 0 \\
>   E[y_1^* y_0^*] & E[y_1^{*2}] & 0 & 0 \\
>    0 & 0 & 1 & 0 \\
>      0 & 0 & 0 & 1
> \end{bmatrix}
> $$
>
>  Se os dados $y_t^*$ forem gerados por um processo estacion√°rio com $E[y_0^{*2}] = 2$,  $E[y_1^{*2}] = 2$,  e $E[y_0^* y_1^*] = 1$, a matriz $Q^*$ seria:
>
> $$
> Q^* = \begin{bmatrix}
>  2 & 1 & 0 & 0 \\
>  1 & 2 & 0 & 0 \\
>   0 & 0 & 1 & 0 \\
>   0 & 0 & 0 & 1
> \end{bmatrix}
> $$
> A converg√™ncia em probabilidade de $\frac{1}{T} \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1}$ para $Q^*$ significa que, para amostras grandes, a matriz de momentos normalizada se aproximar√° desta matriz espec√≠fica.

**Lema 2.1**
A matriz $Q^*$ √© sim√©trica e positiva definida.

*Prova:*
I. A matriz $Q^*$ √© sim√©trica porque $E[y_i^* y_j^{*'}] = E[y_j^* y_i^{*'}]$, o que implica que o elemento na linha $i$ e coluna $j$ √© igual ao elemento na linha $j$ e coluna $i$.

II. Para mostrar que $Q^*$ √© positiva definida, precisamos demonstrar que $z'Q^*z > 0$ para qualquer vetor $z \neq 0$.  Seja $z = [z_0, z_1, ..., z_p, z_{p+1}, z_{p+2}]'$. Ent√£o:

$$z'Q^*z = \sum_{i=0}^{p-1} \sum_{j=0}^{p-1} z_i z_j E[y_i^* y_j^{*'}] + z_{p+1}^2 + z_{p+2}^2$$

III.  O primeiro termo √© uma forma quadr√°tica envolvendo as covari√¢ncias dos regressores transformados. Como os regressores transformados s√£o linearmente independentes e t√™m vari√¢ncia positiva, a forma quadr√°tica √© estritamente positiva se pelo menos um dos $z_0, ..., z_{p-1}$ √© n√£o nulo. Os termos $z_{p+1}^2$ e $z_{p+2}^2$ s√£o n√£o negativos e estritamente positivos se pelo menos um de  $z_{p+1}$ ou $z_{p+2}$ for n√£o nulo.

IV.  Portanto, $z'Q^*z > 0$ para qualquer vetor $z \neq 0$, provando que $Q^*$ √© positiva definida.
‚ñ†

**Lema 3**
O vetor $\frac{1}{\sqrt{T}} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t$ converge em distribui√ß√£o para uma vari√°vel aleat√≥ria gaussiana multivariada com m√©dia zero e matriz de covari√¢ncia $\sigma^2 Q^*$:

$$ \frac{1}{\sqrt{T}} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t \xrightarrow{d} N(0, \sigma^2 Q^*) $$

*Prova:*
I.  O vetor $\frac{1}{\sqrt{T}} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t$  pode ser reescrito como:

$$\frac{1}{\sqrt{T}} \sum_{t=1}^T \Upsilon_T^{-1} x_t^* \epsilon_t$$
II. A partir da defini√ß√£o da matriz $\Upsilon_T$, os elementos do vetor $\frac{1}{\sqrt{T}} \Upsilon_T^{-1} x_t^* \epsilon_t$ s√£o:
    *  $\frac{1}{\sqrt{T}} y_{t-j}^* \epsilon_t$  para  $j=1,2,...,p$ que convergem para uma distribui√ß√£o normal com vari√¢ncia $\sigma^2 E(y_{t-j}^{*2})$, como demonstrado no cap√≠tulo 7.
    * $\frac{1}{\sqrt{T}} \epsilon_t$ que converge para uma distribui√ß√£o normal com vari√¢ncia $\sigma^2$, como demonstrado no cap√≠tulo 8.
    * $\frac{1}{T^{3/2}} t\epsilon_t$ que converge para uma distribui√ß√£o normal com vari√¢ncia $\frac{\sigma^2}{3}$, como demonstrado no cap√≠tulo 16.

III.  Uma vez que $y_{t-j}^*$ e $\epsilon_t$ s√£o i.i.d. e t√™m m√©dia zero,  os termos  $\frac{1}{\sqrt{T}} y_{t-j}^* \epsilon_t$ s√£o uma sequ√™ncia de diferen√ßas de martingala. A vari√¢ncia do termo  $\frac{1}{\sqrt{T}} x_t^* \epsilon_t$ converge para $\sigma^2Q^*$. Portanto, pelo teorema do limite central, o vetor $\frac{1}{\sqrt{T}} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t$ converge em distribui√ß√£o para uma vari√°vel aleat√≥ria gaussiana multivariada com m√©dia zero e matriz de covari√¢ncia $\sigma^2 Q^*$.
‚ñ†

> üí° **Exemplo Num√©rico:** Considere novamente o caso $p=1$ e suponha que $\sigma^2 = 1$ e usando a mesma matriz $Q^*$ do exemplo anterior.  Ent√£o, a distribui√ß√£o limite do vetor $\frac{1}{\sqrt{T}} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t$ √©:
>
> $$ \frac{1}{\sqrt{T}} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t \xrightarrow{d} N \left( \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix}
>  2 & 1 & 0 & 0 \\
>  1 & 2 & 0 & 0 \\
>   0 & 0 & 1 & 0 \\
>    0 & 0 & 0 & 1
> \end{bmatrix} \right) $$
>
>  Isso significa que, para amostras grandes, este vetor se comportar√° como uma distribui√ß√£o normal multivariada com m√©dia zero e a matriz de covari√¢ncia especificada.  Os elementos deste vetor est√£o relacionados aos res√≠duos do modelo, escalonados apropriadamente para que a sua vari√¢ncia n√£o v√° para zero no limite.

**Teorema 1**
O estimador $\Upsilon_T(b^* - \beta^*)$ converge em distribui√ß√£o para uma vari√°vel aleat√≥ria normal multivariada com m√©dia zero e matriz de covari√¢ncia $\sigma^2 (Q^*)^{-1}$.

$$
\Upsilon_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 (Q^*)^{-1})
$$

*Prova:*
I.  Come√ßamos com a express√£o j√° derivada:

$$
\Upsilon_T(b^* - \beta^*) = \left( \frac{1}{T} \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1} \right)^{-1} \frac{1}{\sqrt{T}} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t
$$

II. Do Lema 2, temos que $\left( \frac{1}{T} \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1} \right)^{-1} \xrightarrow{p} (Q^*)^{-1}$.
    
III. Do Lema 3, temos que $ \frac{1}{\sqrt{T}} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t \xrightarrow{d} N(0, \sigma^2 Q^*)$.
    
IV. Aplicando o Teorema de Slutsky, temos que o produto de uma sequ√™ncia que converge em probabilidade por uma sequ√™ncia que converge em distribui√ß√£o converge em distribui√ß√£o, e portanto:

$$
\Upsilon_T(b^* - \beta^*) \xrightarrow{d} (Q^*)^{-1} N(0, \sigma^2 Q^*)
$$

V.  Como o produto de uma matriz constante por uma vari√°vel normal √© tamb√©m uma vari√°vel normal, obtemos:
$$
\Upsilon_T(b^* - \beta^*) \xrightarrow{d} N(0, (Q^*)^{-1} \sigma^2 Q^* (Q^*)^{-1})
$$
VI.  Dado que $(Q^*)^{-1} Q^* (Q^*)^{-1} = (Q^*)^{-1}$, temos:
$$
\Upsilon_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 (Q^*)^{-1})
$$

‚ñ†
> üí° **Exemplo Num√©rico:**  Seguindo os exemplos anteriores, se $\sigma^2 = 1$ e usando a mesma matriz $Q^*$, podemos encontrar a inversa de $Q^*$, denotada por $(Q^*)^{-1}$:
>
> $$ (Q^*)^{-1} = \begin{bmatrix}
>  2 & 1 & 0 & 0 \\
>  1 & 2 & 0 & 0 \\
>   0 & 0 & 1 & 0 \\
>    0 & 0 & 0 & 1
> \end{bmatrix}^{-1} = \begin{bmatrix}
>  4/3 & -2/3 & 0 & 0 \\
>  -2/3 & 4/3 & 0 & 0 \\
>  0 & 0 & 1 & 0 \\
>   0 & 0 & 0 & 1
> \end{bmatrix}
> $$
> Ent√£o, a distribui√ß√£o assint√≥tica de $\Upsilon_T(b^* - \beta^*)$ √©:
>
> $$
> \Upsilon_T(b^* - \beta^*) \xrightarrow{d} N \left( \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix}
>  4/3 & -2/3 & 0 & 0 \\
>  -2/3 & 4/3 & 0 & 0 \\
>   0 & 0 & 1 & 0 \\
>    0 & 0 & 0 & 1
> \end{bmatrix} \right)
> $$
>
> Isso implica que os estimadores transformados, ap√≥s serem multiplicados pela matriz de escalonamento, convergem para uma distribui√ß√£o normal multivariada, com m√©dia zero e a matriz de covari√¢ncia calculada. A matriz de covari√¢ncia indica a vari√¢ncia dos par√¢metros e a correla√ß√£o entre eles.

Com esses resultados, podemos agora deduzir a distribui√ß√£o assint√≥tica de $\Upsilon_T(b^* - \beta^*)$:

$$ \Upsilon_T(b^* - \beta^*) = \left( \frac{1}{T} \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1} \right)^{-1} \frac{1}{\sqrt{T}} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t  $$

Pelo Lema 2, o termo $\left( \frac{1}{T} \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1} \right)^{-1}$ converge em probabilidade para $(Q^*)^{-1}$.  Pelo Lema 3, o termo $\frac{1}{\sqrt{T}} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t$ converge em distribui√ß√£o para  $N(0, \sigma^2 Q^*)$.  Portanto, utilizando o teorema de Slutsky:

$$ \Upsilon_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 (Q^*)^{-1} Q^* (Q^*)^{-1}) = N(0, \sigma^2 (Q^*)^{-1})  $$

Este resultado demonstra que $\Upsilon_T(b^* - \beta^*)$ converge em distribui√ß√£o para uma normal multivariada com m√©dia zero e matriz de covari√¢ncia  $\sigma^2 (Q^*)^{-1}$, como afirmado em [16.3.13] [^1].

### Conclus√£o

A deriva√ß√£o da distribui√ß√£o assint√≥tica dos estimadores transformados, utilizando resultados de matrizes de momentos e o teorema do limite central, solidifica a import√¢ncia da **transforma√ß√£o √∫til dos regressores** na an√°lise de modelos de s√©ries temporais com tend√™ncias determin√≠sticas. O uso da matriz de escalonamento $\Upsilon_T$ √© crucial para acomodar as diferentes taxas de converg√™ncia dos par√¢metros.  Este resultado permite a an√°lise de modelos mais complexos com diferentes ordens de integra√ß√£o e tend√™ncias, pavimentando o caminho para a an√°lise de modelos com ra√≠zes unit√°rias, como ser√° visto nos pr√≥ximos cap√≠tulos. A habilidade de derivar distribui√ß√µes assint√≥ticas usando esta abordagem estabelece um m√©todo robusto e flex√≠vel para a an√°lise de modelos de s√©ries temporais n√£o estacion√°rios.

### Refer√™ncias
[^1]: Cap√≠tulo 16 do livro-texto fornecido.
[^2]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113-44.
<!-- END -->
