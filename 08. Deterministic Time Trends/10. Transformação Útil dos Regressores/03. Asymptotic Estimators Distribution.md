## DistribuiÃ§Ã£o AssintÃ³tica dos Estimadores Transformados: Matrizes de Momentos e o Teorema do Limite Central

### IntroduÃ§Ã£o

Este capÃ­tulo aprofunda a anÃ¡lise da **transformaÃ§Ã£o Ãºtil dos regressores**, focando na derivaÃ§Ã£o da distribuiÃ§Ã£o assintÃ³tica dos estimadores no modelo transformado. Nas seÃ§Ãµes anteriores, estabelecemos a importÃ¢ncia dessa transformaÃ§Ã£o para analisar processos autoregressivos com tendÃªncias determinÃ­sticas, detalhando a manipulaÃ§Ã£o algÃ©brica e a equivalÃªncia estatÃ­stica entre os modelos original e transformado [^1, ^2]. Agora, empregaremos resultados de matrizes de momentos e o teorema do limite central para deduzir formalmente a distribuiÃ§Ã£o assintÃ³tica dos estimadores transformados, mostrando como essa abordagem Ã© essencial para estudar modelos de sÃ©ries temporais mais complexos, incluindo aqueles com raÃ­zes unitÃ¡rias.

### DerivaÃ§Ã£o da DistribuiÃ§Ã£o AssintÃ³tica

A transformaÃ§Ã£o dos regressores permite que o modelo autoregressivo com tendÃªncia determinÃ­stica seja reescrito na forma [^1]:

$$y_t = \alpha^* + \delta^*t + \phi_1^*y_{t-1}^* + \phi_2^*y_{t-2}^* + \ldots + \phi_p^*y_{t-p}^* + \epsilon_t$$

onde $\alpha^*$, $\delta^*$ e $\phi_j^*$ sÃ£o os parÃ¢metros transformados, e $y_{t-j}^*$ sÃ£o os regressores transformados com mÃ©dia zero. O modelo pode ser expresso em notaÃ§Ã£o matricial como [^1]:

$$ y_t = x_t^{*'} \beta^* + \epsilon_t $$

onde $x_t^*$ Ã© o vetor de regressores transformados e $\beta^*$ Ã© o vetor de parÃ¢metros transformados. O estimador OLS de $\beta^*$, denotado por $b^*$, Ã© dado por [^1]:

$$ b^* = \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* y_t $$

Para derivar a distribuiÃ§Ã£o assintÃ³tica de $b^*$, Ã© conveniente expressar a diferenÃ§a entre $b^*$ e o verdadeiro valor $\beta^*$ como [^1]:

$$ b^* - \beta^* = \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* \epsilon_t $$

Para analisar o comportamento assintÃ³tico de $b^*$, multiplicamos ambos os lados por $\Upsilon_T$, uma matriz de escalonamento projetada para acomodar as diferentes taxas de convergÃªncia dos elementos de $\beta^*$. A matriz $\Upsilon_T$ Ã© definida como [^1]:

$$
\Upsilon_T = \begin{bmatrix}
    \sqrt{T} & 0 & 0 & \ldots & 0 & 0 \\
    0 & \sqrt{T} & 0 & \ldots & 0 & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & 0 & \ldots & \sqrt{T} & 0 \\
    0 & 0 & 0 & \ldots & 0 & T^{3/2}
\end{bmatrix}
$$
O objetivo da matriz $\Upsilon_T$ Ã© normalizar os elementos de $b^*$, de forma que a distribuiÃ§Ã£o limite nÃ£o seja degenerada. Note que, no caso de $p=0$, recuperamos a matriz de escalonamento apresentada na seÃ§Ã£o 16.1 do livro [^1].

> ğŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar, considere um cenÃ¡rio com $T=100$ e $p=1$.  A matriz $\Upsilon_T$ seria:
>
> $$
> \Upsilon_{100} = \begin{bmatrix}
>     10 & 0 & 0 \\
>     0 & 10 & 0 \\
>     0 & 0 & 1000
> \end{bmatrix}
> $$
>
> Aqui, os dois primeiros elementos da diagonal sÃ£o $\sqrt{100} = 10$ e o Ãºltimo Ã© $100^{3/2} = 1000$.  Esta matriz serÃ¡ usada para normalizar os parÃ¢metros da regressÃ£o transformada, onde os parÃ¢metros associados aos regressores defasados ($y_{t-1}^*$ e intercepto) convergem a uma taxa de $O(1/\sqrt{T})$ e a tendÃªncia convergem a uma taxa de $O(1/T^{3/2})$.

Multiplicando $b^* - \beta^*$ por $\Upsilon_T$, temos:

$$
\Upsilon_T(b^* - \beta^*) = \Upsilon_T \left( \sum_{t=1}^T x_t^* x_t^{*'} \right)^{-1} \sum_{t=1}^T x_t^* \epsilon_t
$$
Utilizando a notaÃ§Ã£o $\Gamma_T = \sum_{t=1}^T x_t^* x_t^{*'}$, podemos reescrever a equaÃ§Ã£o acima como:
$$
\Upsilon_T(b^* - \beta^*) = \left( \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1} \right)^{-1} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t
$$
Podemos expressar essa equaÃ§Ã£o como:
$$
\Upsilon_T(b^* - \beta^*) = \left( \frac{1}{T} \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1} \right)^{-1} \frac{1}{\sqrt{T}} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t
$$

O prÃ³ximo passo Ã© derivar as distribuiÃ§Ãµes limites dos dois termos no lado direito da equaÃ§Ã£o acima.

**Lema 2**
A matriz $\frac{1}{T} \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1}$ converge em probabilidade para uma matriz nÃ£o singular $Q^*$, tal que:

$$ \frac{1}{T} \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1} \xrightarrow{p} Q^* $$

*Prova:*
I.  A matriz $\Gamma_T$ Ã© definida como $\sum_{t=1}^T x_t^* x_t^{*'}$. Em [16.3.15] do capÃ­tulo 16 [^1], o livro define a matriz Q*. Para derivar o limite da matriz $\frac{1}{T} \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1}$, devemos analisar os componentes desta matriz quando T vai ao infinito.

II. O termo $\frac{1}{T} \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1}$ pode ser escrito como:

$$\frac{1}{T} \Upsilon_T^{-1} \left( \sum_{t=1}^T x_t^* x_t^{*'} \right) \Upsilon_T^{-1} =  \sum_{t=1}^T  \Upsilon_T^{-1} x_t^* x_t^{*'}  \Upsilon_T^{-1} \frac{1}{T} $$

III.  A matriz $\Upsilon_T^{-1}$ divide as linhas e colunas apropriadamente de $x_t^*$ para obter as diferentes taxas de convergÃªncia dos parÃ¢metros. Os elementos de $x_t^*$ sÃ£o formados pelos regressores transformados $y_{t-j}^*$  que sÃ£o estacionÃ¡rios de mÃ©dia zero com um termo constante e uma tendÃªncia.

IV.  Desta forma, o termo $\frac{1}{T} \sum_{t=1}^T  \Upsilon_T^{-1} x_t^* x_t^{*'}  \Upsilon_T^{-1} $ converge para a matriz $Q^*$ definida como:
$$
Q^* = \begin{bmatrix}
 E[y_0^* y_0^{*'}] & E[y_0^* y_1^{*'}] & \ldots & E[y_0^* y_{p-1}^{*'}] & 0 & 0 \\
  E[y_1^* y_0^{*'}] & E[y_1^* y_1^{*'}] & \ldots & E[y_1^* y_{p-1}^{*'}] & 0 & 0 \\
    \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
   E[y_{p-1}^* y_0^{*'}] & E[y_{p-1}^* y_1^{*'}]  & \ldots &  E[y_{p-1}^* y_{p-1}^{*'}] & 0 & 0 \\
    0 & 0  & \ldots & 0 & 1 & 0 \\
     0 & 0 & \ldots & 0 & 0 & 1
\end{bmatrix}
$$

Onde $E[y_i^* y_j^{*'}]$ denota o valor esperado de $y_i^*y_j^*$. Note que $Q^*$ Ã© idÃªntica Ã  matriz Q* apresentada no ApÃªndice 16A do livro [^1], exceto pela estrutura de $x_t^*$ (incluindo um termo constante e uma tendÃªncia). Como os termos  $E[y_i^* y_j^{*'}]$ sÃ£o covariÃ¢ncia estacionÃ¡rios, o limite da matriz $\frac{1}{T} \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1}$ converge em probabilidade para a matriz $Q^*$. Como todos os elementos da diagonal de $Q^*$ sÃ£o positivos,  $Q^*$ Ã© uma matriz nÃ£o singular, o que garante a existÃªncia de $(Q^*)^{-1}$.
â– 

> ğŸ’¡ **Exemplo NumÃ©rico:**  Suponha que temos um modelo AR(1) com uma tendÃªncia, ou seja, $p=1$.  Nesse caso, a matriz $Q^*$ terÃ¡ a seguinte estrutura:
>
>  $$
> Q^* = \begin{bmatrix}
>  E[y_0^{*2}] & E[y_0^* y_1^*] & 0 & 0 \\
>   E[y_1^* y_0^*] & E[y_1^{*2}] & 0 & 0 \\
>    0 & 0 & 1 & 0 \\
>      0 & 0 & 0 & 1
> \end{bmatrix}
> $$
>
>  Se os dados $y_t^*$ forem gerados por um processo estacionÃ¡rio com $E[y_0^{*2}] = 2$,  $E[y_1^{*2}] = 2$,  e $E[y_0^* y_1^*] = 1$, a matriz $Q^*$ seria:
>
> $$
> Q^* = \begin{bmatrix}
>  2 & 1 & 0 & 0 \\
>  1 & 2 & 0 & 0 \\
>   0 & 0 & 1 & 0 \\
>   0 & 0 & 0 & 1
> \end{bmatrix}
> $$
> A convergÃªncia em probabilidade de $\frac{1}{T} \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1}$ para $Q^*$ significa que, para amostras grandes, a matriz de momentos normalizada se aproximarÃ¡ desta matriz especÃ­fica.

**Lema 2.1**
A matriz $Q^*$ Ã© simÃ©trica e positiva definida.

*Prova:*
I. A matriz $Q^*$ Ã© simÃ©trica porque $E[y_i^* y_j^{*'}] = E[y_j^* y_i^{*'}]$, o que implica que o elemento na linha $i$ e coluna $j$ Ã© igual ao elemento na linha $j$ e coluna $i$.

II. Para mostrar que $Q^*$ Ã© positiva definida, precisamos demonstrar que $z'Q^*z > 0$ para qualquer vetor $z \neq 0$.  Seja $z = [z_0, z_1, ..., z_p, z_{p+1}, z_{p+2}]'$. EntÃ£o:

$$z'Q^*z = \sum_{i=0}^{p-1} \sum_{j=0}^{p-1} z_i z_j E[y_i^* y_j^{*'}] + z_{p+1}^2 + z_{p+2}^2$$

III.  O primeiro termo Ã© uma forma quadrÃ¡tica envolvendo as covariÃ¢ncias dos regressores transformados. Como os regressores transformados sÃ£o linearmente independentes e tÃªm variÃ¢ncia positiva, a forma quadrÃ¡tica Ã© estritamente positiva se pelo menos um dos $z_0, ..., z_{p-1}$ Ã© nÃ£o nulo. Os termos $z_{p+1}^2$ e $z_{p+2}^2$ sÃ£o nÃ£o negativos e estritamente positivos se pelo menos um de  $z_{p+1}$ ou $z_{p+2}$ for nÃ£o nulo.

IV.  Portanto, $z'Q^*z > 0$ para qualquer vetor $z \neq 0$, provando que $Q^*$ Ã© positiva definida.
â– 

**Lema 3**
O vetor $\frac{1}{\sqrt{T}} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t$ converge em distribuiÃ§Ã£o para uma variÃ¡vel aleatÃ³ria gaussiana multivariada com mÃ©dia zero e matriz de covariÃ¢ncia $\sigma^2 Q^*$:

$$ \frac{1}{\sqrt{T}} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t \xrightarrow{d} N(0, \sigma^2 Q^*) $$

*Prova:*
I.  O vetor $\frac{1}{\sqrt{T}} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t$  pode ser reescrito como:

$$\frac{1}{\sqrt{T}} \sum_{t=1}^T \Upsilon_T^{-1} x_t^* \epsilon_t$$
II. A partir da definiÃ§Ã£o da matriz $\Upsilon_T$, os elementos do vetor $\frac{1}{\sqrt{T}} \Upsilon_T^{-1} x_t^* \epsilon_t$ sÃ£o:
    *  $\frac{1}{\sqrt{T}} y_{t-j}^* \epsilon_t$  para  $j=1,2,...,p$ que convergem para uma distribuiÃ§Ã£o normal com variÃ¢ncia $\sigma^2 E(y_{t-j}^{*2})$, como demonstrado no capÃ­tulo 7.
    * $\frac{1}{\sqrt{T}} \epsilon_t$ que converge para uma distribuiÃ§Ã£o normal com variÃ¢ncia $\sigma^2$, como demonstrado no capÃ­tulo 8.
    * $\frac{1}{T^{3/2}} t\epsilon_t$ que converge para uma distribuiÃ§Ã£o normal com variÃ¢ncia $\frac{\sigma^2}{3}$, como demonstrado no capÃ­tulo 16.

III.  Uma vez que $y_{t-j}^*$ e $\epsilon_t$ sÃ£o i.i.d. e tÃªm mÃ©dia zero,  os termos  $\frac{1}{\sqrt{T}} y_{t-j}^* \epsilon_t$ sÃ£o uma sequÃªncia de diferenÃ§as de martingala. A variÃ¢ncia do termo  $\frac{1}{\sqrt{T}} x_t^* \epsilon_t$ converge para $\sigma^2Q^*$. Portanto, pelo teorema do limite central, o vetor $\frac{1}{\sqrt{T}} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t$ converge em distribuiÃ§Ã£o para uma variÃ¡vel aleatÃ³ria gaussiana multivariada com mÃ©dia zero e matriz de covariÃ¢ncia $\sigma^2 Q^*$.
â– 

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere novamente o caso $p=1$ e suponha que $\sigma^2 = 1$ e usando a mesma matriz $Q^*$ do exemplo anterior.  EntÃ£o, a distribuiÃ§Ã£o limite do vetor $\frac{1}{\sqrt{T}} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t$ Ã©:
>
> $$ \frac{1}{\sqrt{T}} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t \xrightarrow{d} N \left( \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix}
>  2 & 1 & 0 & 0 \\
>  1 & 2 & 0 & 0 \\
>   0 & 0 & 1 & 0 \\
>    0 & 0 & 0 & 1
> \end{bmatrix} \right) $$
>
>  Isso significa que, para amostras grandes, este vetor se comportarÃ¡ como uma distribuiÃ§Ã£o normal multivariada com mÃ©dia zero e a matriz de covariÃ¢ncia especificada.  Os elementos deste vetor estÃ£o relacionados aos resÃ­duos do modelo, escalonados apropriadamente para que a sua variÃ¢ncia nÃ£o vÃ¡ para zero no limite.

**Teorema 1**
O estimador $\Upsilon_T(b^* - \beta^*)$ converge em distribuiÃ§Ã£o para uma variÃ¡vel aleatÃ³ria normal multivariada com mÃ©dia zero e matriz de covariÃ¢ncia $\sigma^2 (Q^*)^{-1}$.

$$
\Upsilon_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 (Q^*)^{-1})
$$

*Prova:*
I.  ComeÃ§amos com a expressÃ£o jÃ¡ derivada:

$$
\Upsilon_T(b^* - \beta^*) = \left( \frac{1}{T} \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1} \right)^{-1} \frac{1}{\sqrt{T}} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t
$$

II. Do Lema 2, temos que $\left( \frac{1}{T} \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1} \right)^{-1} \xrightarrow{p} (Q^*)^{-1}$.
    
III. Do Lema 3, temos que $ \frac{1}{\sqrt{T}} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t \xrightarrow{d} N(0, \sigma^2 Q^*)$.
    
IV. Aplicando o Teorema de Slutsky, temos que o produto de uma sequÃªncia que converge em probabilidade por uma sequÃªncia que converge em distribuiÃ§Ã£o converge em distribuiÃ§Ã£o, e portanto:

$$
\Upsilon_T(b^* - \beta^*) \xrightarrow{d} (Q^*)^{-1} N(0, \sigma^2 Q^*)
$$

V.  Como o produto de uma matriz constante por uma variÃ¡vel normal Ã© tambÃ©m uma variÃ¡vel normal, obtemos:
$$
\Upsilon_T(b^* - \beta^*) \xrightarrow{d} N(0, (Q^*)^{-1} \sigma^2 Q^* (Q^*)^{-1})
$$
VI.  Dado que $(Q^*)^{-1} Q^* (Q^*)^{-1} = (Q^*)^{-1}$, temos:
$$
\Upsilon_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 (Q^*)^{-1})
$$

â– 
> ğŸ’¡ **Exemplo NumÃ©rico:**  Seguindo os exemplos anteriores, se $\sigma^2 = 1$ e usando a mesma matriz $Q^*$, podemos encontrar a inversa de $Q^*$, denotada por $(Q^*)^{-1}$:
>
> $$ (Q^*)^{-1} = \begin{bmatrix}
>  2 & 1 & 0 & 0 \\
>  1 & 2 & 0 & 0 \\
>   0 & 0 & 1 & 0 \\
>    0 & 0 & 0 & 1
> \end{bmatrix}^{-1} = \begin{bmatrix}
>  4/3 & -2/3 & 0 & 0 \\
>  -2/3 & 4/3 & 0 & 0 \\
>  0 & 0 & 1 & 0 \\
>   0 & 0 & 0 & 1
> \end{bmatrix}
> $$
> EntÃ£o, a distribuiÃ§Ã£o assintÃ³tica de $\Upsilon_T(b^* - \beta^*)$ Ã©:
>
> $$
> \Upsilon_T(b^* - \beta^*) \xrightarrow{d} N \left( \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix}
>  4/3 & -2/3 & 0 & 0 \\
>  -2/3 & 4/3 & 0 & 0 \\
>   0 & 0 & 1 & 0 \\
>    0 & 0 & 0 & 1
> \end{bmatrix} \right)
> $$
>
> Isso implica que os estimadores transformados, apÃ³s serem multiplicados pela matriz de escalonamento, convergem para uma distribuiÃ§Ã£o normal multivariada, com mÃ©dia zero e a matriz de covariÃ¢ncia calculada. A matriz de covariÃ¢ncia indica a variÃ¢ncia dos parÃ¢metros e a correlaÃ§Ã£o entre eles.

Com esses resultados, podemos agora deduzir a distribuiÃ§Ã£o assintÃ³tica de $\Upsilon_T(b^* - \beta^*)$:

$$ \Upsilon_T(b^* - \beta^*) = \left( \frac{1}{T} \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1} \right)^{-1} \frac{1}{\sqrt{T}} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t  $$

Pelo Lema 2, o termo $\left( \frac{1}{T} \Upsilon_T^{-1} \Gamma_T \Upsilon_T^{-1} \right)^{-1}$ converge em probabilidade para $(Q^*)^{-1}$.  Pelo Lema 3, o termo $\frac{1}{\sqrt{T}} \Upsilon_T^{-1} \sum_{t=1}^T x_t^* \epsilon_t$ converge em distribuiÃ§Ã£o para  $N(0, \sigma^2 Q^*)$.  Portanto, utilizando o teorema de Slutsky:

$$ \Upsilon_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 (Q^*)^{-1} Q^* (Q^*)^{-1}) = N(0, \sigma^2 (Q^*)^{-1})  $$

Este resultado demonstra que $\Upsilon_T(b^* - \beta^*)$ converge em distribuiÃ§Ã£o para uma normal multivariada com mÃ©dia zero e matriz de covariÃ¢ncia  $\sigma^2 (Q^*)^{-1}$, como afirmado em [16.3.13] [^1].

### ConclusÃ£o

A derivaÃ§Ã£o da distribuiÃ§Ã£o assintÃ³tica dos estimadores transformados, utilizando resultados de matrizes de momentos e o teorema do limite central, solidifica a importÃ¢ncia da **transformaÃ§Ã£o Ãºtil dos regressores** na anÃ¡lise de modelos de sÃ©ries temporais com tendÃªncias determinÃ­sticas. O uso da matriz de escalonamento $\Upsilon_T$ Ã© crucial para acomodar as diferentes taxas de convergÃªncia dos parÃ¢metros.  Este resultado permite a anÃ¡lise de modelos mais complexos com diferentes ordens de integraÃ§Ã£o e tendÃªncias, pavimentando o caminho para a anÃ¡lise de modelos com raÃ­zes unitÃ¡rias, como serÃ¡ visto nos prÃ³ximos capÃ­tulos. A habilidade de derivar distribuiÃ§Ãµes assintÃ³ticas usando esta abordagem estabelece um mÃ©todo robusto e flexÃ­vel para a anÃ¡lise de modelos de sÃ©ries temporais nÃ£o estacionÃ¡rios.

### ReferÃªncias
[^1]: CapÃ­tulo 16 do livro-texto fornecido.
[^2]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113-44.
<!-- END -->
