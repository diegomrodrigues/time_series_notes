## Testes de Hip√≥teses para o Modelo de Tend√™ncia de Tempo Simples
### Introdu√ß√£o
Em continuidade ao estudo da distribui√ß√£o assint√≥tica dos estimadores de m√≠nimos quadrados ordin√°rios (OLS) em modelos com tend√™ncia de tempo determin√≠stica, este cap√≠tulo aborda a validade dos testes de hip√≥teses usuais para esses modelos. Como vimos anteriormente, os estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$ convergem a taxas diferentes, com $\hat{\alpha}_T$ convergindo a uma taxa de $T^{1/2}$ e $\hat{\delta}_T$ a $T^{3/2}$ [^1]. Apesar dessas diferentes taxas de converg√™ncia, os erros padr√£o, $\hat{\sigma}_{\hat{\alpha}_T}$ e $\hat{\sigma}_{\hat{\delta}_T}$, parecem apresentar um comportamento assint√≥tico compensat√≥rio, de modo que as estat√≠sticas $t$ para os testes de hip√≥teses, como $(\hat{\delta}_T - \delta)/\hat{\sigma}_{\hat{\delta}_T}$ s√£o assintoticamente $N(0,1)$ quando as inova√ß√µes s√£o gaussianas [^1]. Assim, surge a quest√£o sobre a validade desses testes para inova√ß√µes n√£o gaussianas, o que ser√° verificado nesta se√ß√£o.

### Conceitos Fundamentais
A validade assint√≥tica dos testes $t$ e $F$ para modelos com tend√™ncia de tempo √© demonstrada utilizando-se da distribui√ß√£o assint√≥tica dos estimadores OLS e de suas vari√¢ncias.  Inicialmente, consideremos o teste $t$ para a hip√≥tese nula $H_0: \alpha = \alpha_0$, que pode ser escrito como [^1]:

$$
t_T = \frac{\hat{\alpha}_T - \alpha_0}{\sqrt{s^2_T [1 \, 0](X'X)^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} }}
$$

onde $s^2_T$ √© o estimador OLS da vari√¢ncia do erro e $(X'X)^{-1}$ √© a matriz inversa de $(X'X)$ definida em [16.1.16] [^1]. Substituindo $(X'X)$ e multiplicando o numerador e denominador por $\sqrt{T}$, temos:

$$
t_T = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{\sqrt{s^2_T [\sqrt{T} \, 0](X'X)^{-1} \begin{bmatrix} \sqrt{T} \\ 0 \end{bmatrix} }}
$$

Utilizando [16.1.17] e [16.1.19], a express√£o acima pode ser reescrita como:

$$
t_T = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{\sqrt{s^2_T [1 \, 0] Q^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} }}
$$
onde Q √© dada por [16.1.20] [^1]. Dado que $\sqrt{T}(\hat{\alpha}_T - \alpha_0)$ √© assintoticamente $N(0, \sigma^2 q^{11})$, onde $q^{11}$ √© o elemento (1,1) da matriz $Q^{-1}$ [^1] e que $s^2_T$ converge para $\sigma^2$, a estat√≠stica $t_T$ converge em distribui√ß√£o para $N(0,1)$, validando assintoticamente o teste $t$ para $\alpha$.

> üí° **Exemplo Num√©rico:** Suponha que temos um modelo com tend√™ncia de tempo, $y_t = \alpha + \delta t + u_t$, e estimamos $\hat{\alpha}_T = 2.5$ com um erro padr√£o de $0.5$, $s_T^2 = 1.2$. Queremos testar a hip√≥tese nula $H_0: \alpha = 2$. O tamanho da amostra √© $T = 100$.
>
> Primeiro, calculamos a estat√≠stica t:
>
> $$
> t_T = \frac{\hat{\alpha}_T - \alpha_0}{\hat{\sigma}_{\hat{\alpha}_T}} = \frac{2.5 - 2}{0.5} = 1
> $$
>
> Agora, vamos usar a formula√ß√£o com as matrizes para verificar a consist√™ncia dos resultados, lembrando que $\hat{\sigma}_{\hat{\alpha}_T} = \sqrt{s^2_T [1 \, 0](X'X)^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} }$.
>
> Assumindo que $[1 \, 0]Q^{-1}\begin{bmatrix} 1 \\ 0 \end{bmatrix}  = 1$ e usando a f√≥rmula acima que $\hat{\sigma}_{\hat{\alpha}_T} = 0.5$ e  $s^2_T = 1.2$.
>
>
> $$
> t_T = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{\sqrt{s^2_T [1 \, 0] Q^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} }} = \frac{\sqrt{100}(2.5 - 2)}{\sqrt{1.2 * 1}}  = \frac{10 * 0.5}{\sqrt{1.2}} \approx 4.56
> $$
>  Note que o valor de $t_T$ com $\sqrt{T}$ no numerador √© diferente da estat√≠stica $t$ convencional, que usa $\hat{\sigma}_{\hat{\alpha}_T}$, e converge para uma distribui√ß√£o normal padr√£o conforme o tamanho da amostra aumenta.
>  Em nosso exemplo simplificado, para ilustrar o conceito, assumimos que $\hat{\sigma}_{\hat{\alpha}_T}$ foi calculado previamente. O valor 1 √© usado para ilustrar a aplica√ß√£o da f√≥rmula, e o valor de 4.56 √© o valor real da estat√≠stica. A decis√£o estat√≠stica depender√° do valor cr√≠tico da distribui√ß√£o Normal com um n√≠vel de signific√¢ncia definido. Se o valor absoluto da estat√≠stica t for maior que o valor cr√≠tico, rejeitamos a hip√≥tese nula.

De forma an√°loga, o teste $t$ para a hip√≥tese nula $H_0: \delta = \delta_0$ √© dado por [^1]:
$$
t_T = \frac{\hat{\delta}_T - \delta_0}{\sqrt{s^2_T [0 \, 1](X'X)^{-1} \begin{bmatrix} 0 \\ 1 \end{bmatrix} }}
$$

Multiplicando o numerador e o denominador por $T^{3/2}$, obtemos:
$$
t_T = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\sqrt{s^2_T [0 \, T^{3/2}](X'X)^{-1} \begin{bmatrix} 0 \\ T^{3/2} \end{bmatrix} }}
$$
Usando [16.1.17], a express√£o acima se torna:
$$
t_T = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\sqrt{s^2_T [0 \, 1] Q^{-1} \begin{bmatrix} 0 \\ 1 \end{bmatrix} }}
$$

Dado que $T^{3/2}(\hat{\delta}_T - \delta_0)$ √© assintoticamente $N(0, \sigma^2 q^{22})$, onde $q^{22}$ √© o elemento (2,2) da matriz $Q^{-1}$ [^1],  a estat√≠stica $t_T$ tamb√©m converge em distribui√ß√£o para $N(0,1)$, validando assintoticamente o teste $t$ para $\delta$.

> üí° **Exemplo Num√©rico:** Continuando com o exemplo anterior, suponha que estimamos $\hat{\delta}_T = 0.15$ com um erro padr√£o de $0.02$. Queremos testar a hip√≥tese nula $H_0: \delta = 0.1$. Temos $T=100$ e  $s^2_T = 1.2$.
>
> Calculamos a estat√≠stica t:
>
> $$
> t_T = \frac{\hat{\delta}_T - \delta_0}{\hat{\sigma}_{\hat{\delta}_T}} = \frac{0.15 - 0.1}{0.02} = 2.5
> $$
>
> Usando a formula√ß√£o com as matrizes,  assumindo que $[0 \, 1]Q^{-1}\begin{bmatrix} 0 \\ 1 \end{bmatrix}  = 1$, temos:
>
> $$
> t_T = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\sqrt{s^2_T [0 \, 1] Q^{-1} \begin{bmatrix} 0 \\ 1 \end{bmatrix} }} = \frac{100^{3/2}(0.15 - 0.1)}{\sqrt{1.2 * 1}} = \frac{1000*0.05}{\sqrt{1.2}} \approx 45.6
> $$
> Assim como no exemplo anterior, o valor 2.5 ilustra o c√°lculo da estat√≠stica t convencional, enquanto 45.6 ilustra a aplica√ß√£o da formula√ß√£o com as matrizes e as respectivas taxas de converg√™ncia.  A decis√£o estat√≠stica sobre rejeitar ou n√£o a hip√≥tese nula √© tomada com base no valor cr√≠tico da distribui√ß√£o Normal padr√£o, considerando o n√≠vel de signific√¢ncia.

Al√©m dos testes para par√¢metros individuais, podemos considerar testes de hip√≥teses conjuntas envolvendo $\alpha$ e $\delta$. Por exemplo, considere a hip√≥tese $H_0: r_1\alpha + r_2\delta = r$. A estat√≠stica de teste $t$ correspondente √© dada por [^1]:

$$
t_T = \frac{r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r}{\sqrt{s_T^2 \begin{bmatrix} r_1 & r_2 \end{bmatrix} (X'X)^{-1} \begin{bmatrix} r_1 \\ r_2 \end{bmatrix} }}
$$

Multiplicando o numerador e o denominador por $\sqrt{T}$ e utilizando as rela√ß√µes [16.1.17] e [16.1.19], temos:

$$
t_T = \frac{\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)}{\sqrt{s_T^2 \begin{bmatrix} r_1 & r_2/T^{3/2} \end{bmatrix} Q^{-1} \begin{bmatrix} r_1 \\ r_2/T^{3/2} \end{bmatrix} }}
$$

Como o componente que envolve $\hat{\delta}$ converge para zero mais r√°pido, o comportamento assint√≥tico da estat√≠stica $t$ √© dominado pelo componente que envolve $\hat{\alpha}$. Assim, a estat√≠stica $t$ converge em distribui√ß√£o para $N(0,1)$, justificando o uso dos testes usuais.

> üí° **Exemplo Num√©rico:** Suponha que queremos testar $H_0: \alpha + 2\delta = 2.8$. Temos $\hat{\alpha}_T = 2.5$, $\hat{\delta}_T = 0.15$, $r_1 = 1$, $r_2 = 2$, $r=2.8$ e $s_T^2 = 1.2$. Suponha que $\begin{bmatrix} 1 & 2 \end{bmatrix} (X'X)^{-1} \begin{bmatrix} 1 \\ 2 \end{bmatrix} = 0.3$, e temos $T=100$.
>
> Primeiro, calculamos a estat√≠stica t utilizando os par√¢metros e seus erros padr√µes:
>
> $$
> t_T = \frac{1*2.5 + 2*0.15 - 2.8}{\sqrt{1.2*0.3}} = \frac{2.5 + 0.3 - 2.8}{\sqrt{0.36}} = \frac{0}{0.6} = 0
> $$
>
> Agora, usando a formula√ß√£o com as matrizes e taxas de converg√™ncia:
>
>
> $$
> t_T = \frac{\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)}{\sqrt{s_T^2 \begin{bmatrix} r_1 & r_2/T^{3/2} \end{bmatrix} Q^{-1} \begin{bmatrix} r_1 \\ r_2/T^{3/2} \end{bmatrix} }}
> $$
> Suponha que  $\begin{bmatrix} 1 & 2/T^{3/2} \end{bmatrix} Q^{-1} \begin{bmatrix} 1 \\ 2/T^{3/2} \end{bmatrix} = 0.2 $. Ent√£o:
>
> $$
> t_T = \frac{\sqrt{100}(1*2.5 + 2*0.15 - 2.8)}{\sqrt{1.2 * 0.2 }} = \frac{10*(2.8-2.8)}{\sqrt{0.24}} = 0
> $$
>
> Novamente, o valor 0 reflete a aplica√ß√£o da f√≥rmula padr√£o, enquanto o valor 0 usando a formula√ß√£o com as matrizes demonstra o mesmo resultado dentro de uma estrutura com a taxa de converg√™ncia, ilustrando a converg√™ncia para uma distribui√ß√£o normal. O resultado da estat√≠stica t = 0 nos diz que n√£o h√° evid√™ncias para rejeitar a hip√≥tese nula, dado o n√≠vel de signific√¢ncia.

Outro exemplo importante √© o teste conjunto de hip√≥teses separadas para $\alpha$ e $\delta$ dado por:
$$
H_0: \begin{bmatrix} \alpha \\ \delta \end{bmatrix} = \begin{bmatrix} \alpha_0 \\ \delta_0 \end{bmatrix}
$$

A forma de Wald do teste de $\chi^2$ √© dada por:
$$
\chi^2_T = (\mathbf{b}_T - \mathbf{b}_0)' [s_T^2(X'X)^{-1}]^{-1} (\mathbf{b}_T - \mathbf{b}_0)
$$
onde $\mathbf{b}_T = [\hat{\alpha}_T \, \hat{\delta}_T ]'$ e $\mathbf{b}_0 = [\alpha_0 \, \delta_0]'$. Utilizando a propriedade de que $Y_T(X'X)^{-1}Y_T \rightarrow Q^{-1}$ [^1], a estat√≠stica $\chi^2$ √© assintoticamente $\chi^2(2)$.

> üí° **Exemplo Num√©rico:** Suponha que queremos testar a hip√≥tese conjunta $H_0: \begin{bmatrix} \alpha \\ \delta \end{bmatrix} = \begin{bmatrix} 2 \\ 0.1 \end{bmatrix}$. Temos $\hat{\alpha}_T = 2.5$, $\hat{\delta}_T = 0.15$ e $s_T^2 = 1.2$.  Suponha que $(X'X)^{-1} = \begin{bmatrix} 0.1 & 0.01 \\ 0.01 & 0.002 \end{bmatrix}$.
>
> Primeiro calculamos o vetor de diferen√ßas:
>
> $$\mathbf{b}_T - \mathbf{b}_0 = \begin{bmatrix} 2.5 \\ 0.15 \end{bmatrix} - \begin{bmatrix} 2 \\ 0.1 \end{bmatrix} = \begin{bmatrix} 0.5 \\ 0.05 \end{bmatrix}$$
>
> Agora, calculamos a inversa de $[s_T^2(X'X)^{-1}]$:
>
> $$[s_T^2(X'X)^{-1}]^{-1} = [1.2 * \begin{bmatrix} 0.1 & 0.01 \\ 0.01 & 0.002 \end{bmatrix}]^{-1} = \begin{bmatrix} 0.12 & 0.012 \\ 0.012 & 0.0024 \end{bmatrix}^{-1} \approx  \begin{bmatrix} 100 & -500 \\ -500 & 5000 \end{bmatrix} $$
>
> A estat√≠stica $\chi^2$ √© ent√£o:
>
> $$
> \chi^2_T = \begin{bmatrix} 0.5 & 0.05 \end{bmatrix}  \begin{bmatrix} 100 & -500 \\ -500 & 5000 \end{bmatrix} \begin{bmatrix} 0.5 \\ 0.05 \end{bmatrix}
> $$
> $$
> \chi^2_T = \begin{bmatrix} 0.5 & 0.05 \end{bmatrix}  \begin{bmatrix} 25 \\ -25 \end{bmatrix}  =  12.5 -1.25 = 11.25
> $$
> O valor de 11.25 √© a estat√≠stica $\chi^2$  que ser√° comparada com o valor cr√≠tico da distribui√ß√£o qui-quadrado com 2 graus de liberdade, dado um n√≠vel de signific√¢ncia pr√©-estabelecido, para decidir se rejeitamos ou n√£o a hip√≥tese conjunta.

**Observa√ß√£o 1:** √â importante notar que a converg√™ncia em distribui√ß√£o para $N(0,1)$ das estat√≠sticas $t$ para $\alpha$ e $\delta$  depende crucialmente da converg√™ncia de $s_T^2$ para $\sigma^2$. Esta converg√™ncia √© garantida pelas propriedades dos estimadores OLS sob as condi√ß√µes usuais, incluindo a hip√≥tese de que os erros s√£o i.i.d. com vari√¢ncia finita.

**Lema 1:** A converg√™ncia de $s_T^2$ para $\sigma^2$ √© uma consequ√™ncia direta da consist√™ncia do estimador OLS da vari√¢ncia do erro. Formalmente, se os erros do modelo s√£o i.i.d. com m√©dia zero e vari√¢ncia $\sigma^2$, ent√£o $plim_{T\to\infty} s_T^2 = \sigma^2$.
*Proof:* A prova segue da defini√ß√£o do estimador da vari√¢ncia do erro e das propriedades de converg√™ncia dos estimadores OLS sob as hip√≥teses padr√µes.

I. O estimador OLS da vari√¢ncia do erro √© dado por $s_T^2 = \frac{1}{T-k} \sum_{t=1}^{T} \hat{u}_t^2$, onde $\hat{u}_t$ s√£o os res√≠duos OLS e $k$ √© o n√∫mero de par√¢metros no modelo.

II. Os res√≠duos podem ser expressos como $\hat{u}_t = y_t - x_t'\hat{\beta}$, onde $y_t$ √© a vari√°vel dependente, $x_t$ √© o vetor de regressores e $\hat{\beta}$ s√£o os estimadores OLS dos par√¢metros.

III. Sob as condi√ß√µes de i.i.d. dos erros com m√©dia zero e vari√¢ncia $\sigma^2$, os estimadores OLS $\hat{\beta}$ s√£o consistentes, ou seja, $plim_{T\to\infty} \hat{\beta} = \beta$, onde $\beta$ s√£o os verdadeiros par√¢metros.

IV. Como os estimadores OLS s√£o consistentes e a fun√ß√£o $f(x) = x^2$ √© cont√≠nua, ent√£o $plim_{T\to\infty} \hat{u}_t^2 = plim_{T\to\infty}(y_t - x_t'\hat{\beta})^2 = (y_t - x_t'\beta)^2 = u_t^2$, onde $u_t$ s√£o os verdadeiros erros.

V. Pela lei dos grandes n√∫meros, $\frac{1}{T} \sum_{t=1}^{T} u_t^2 \xrightarrow{p} E(u_t^2) = \sigma^2$.

VI. Portanto, $plim_{T\to\infty} s_T^2 = plim_{T\to\infty} \frac{T}{T-k} \frac{1}{T} \sum_{t=1}^{T} \hat{u}_t^2 = 1 \cdot \sigma^2 = \sigma^2$, j√° que $k$ √© fixo. ‚ñ†

**Teorema 1:** (Robustez dos testes t em rela√ß√£o √† n√£o-normalidade) Sob as condi√ß√µes de i.i.d. para os erros com m√©dia zero e vari√¢ncia finita $\sigma^2$, mas sem a necessidade de gaussianidade, as estat√≠sticas t para os par√¢metros $\alpha$ e $\delta$ convergem em distribui√ß√£o para $N(0,1)$.
*Proof:*  A prova √© an√°loga √† apresentada no texto, utilizando o Teorema do Limite Central para vetores, que afirma que a soma de vari√°veis aleat√≥rias i.i.d. (ap√≥s devida normaliza√ß√£o) converge para uma distribui√ß√£o normal, mesmo que as vari√°veis originais n√£o sejam normalmente distribu√≠das. As derivadas apropriadas dos estimadores OLS podem ser expressas como somas de vari√°veis aleat√≥rias i.i.d. com vari√¢ncia finita, o que leva √† converg√™ncia para uma normal. A converg√™ncia de $s^2_T$ para $\sigma^2$ garante a validade assint√≥tica das estat√≠sticas $t$.

I. Come√ßamos com a defini√ß√£o da estat√≠stica t para $\alpha$:
    $$t_T = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{\sqrt{s^2_T [1 \, 0] Q^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} }} $$
II. Sabemos que $\sqrt{T}(\hat{\alpha}_T - \alpha_0)$ converge em distribui√ß√£o para $N(0, \sigma^2 q^{11})$, onde $q^{11}$ √© o elemento (1,1) de $Q^{-1}$, conforme resultado padr√£o para estimadores OLS com tend√™ncia.
III. Do Lema 1, temos que $s^2_T$ converge em probabilidade para $\sigma^2$.
IV. Portanto, o denominador $\sqrt{s^2_T [1 \, 0] Q^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} }$ converge em probabilidade para $\sqrt{\sigma^2 q^{11}}$.
V. Aplicando o Teorema de Slutsky, a raz√£o de uma vari√°vel aleat√≥ria que converge em distribui√ß√£o para uma normal e uma vari√°vel que converge em probabilidade para uma constante diferente de zero, tamb√©m converge em distribui√ß√£o para uma normal com a mesma distribui√ß√£o, ent√£o:
$$t_T = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{\sqrt{s^2_T [1 \, 0] Q^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} }} \xrightarrow{d} \frac{N(0,\sigma^2 q^{11})}{\sqrt{\sigma^2 q^{11}}} = N(0,1)$$
VI. O mesmo racioc√≠nio √© v√°lido para a estat√≠stica $t$ para $\delta$.
VII. Portanto, as estat√≠sticas $t$ para $\alpha$ e $\delta$ convergem em distribui√ß√£o para $N(0,1)$, mesmo sem a necessidade de gaussianidade dos erros. ‚ñ†

**Teorema 1.1:** A estat√≠stica t para a hip√≥tese conjunta $H_0: r_1\alpha + r_2\delta = r$ tamb√©m converge assintoticamente para uma distribui√ß√£o $N(0,1)$ sob as condi√ß√µes do Teorema 1.
*Proof:* A prova segue os mesmos argumentos apresentados no texto original, observando que o componente que envolve $\hat{\delta}$ converge para zero mais r√°pido, o que faz com que o comportamento assint√≥tico da estat√≠stica $t$ seja dominado pelo componente que envolve $\hat{\alpha}$, que converge para uma normal sob as condi√ß√µes de i.i.d. com vari√¢ncia finita, mas sem necessariamente gaussianidade para os erros.

I. A estat√≠stica t para a hip√≥tese conjunta √© dada por:
$$t_T = \frac{\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)}{\sqrt{s_T^2 \begin{bmatrix} r_1 & r_2/T^{3/2} \end{bmatrix} Q^{-1} \begin{bmatrix} r_1 \\ r_2/T^{3/2} \end{bmatrix} }} $$

II. Multiplicando e dividindo o numerador e o denominador por $\sqrt{q^{11}}$, e definindo $a = \frac{r_2}{T^{3/2}} $, temos:

$$t_T = \frac{\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)}{\sqrt{s_T^2 (r_1^2 q^{11} + 2r_1 a q^{12} + a^2 q^{22}) }} $$

III.  Sabemos que $\sqrt{T}(\hat{\alpha}_T - \alpha_0)$ converge para uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\sigma^2 q^{11}$ e $T^{3/2}(\hat{\delta}_T - \delta_0)$ converge para uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\sigma^2 q^{22}$ e, portanto, $T^{1/2}\hat{\delta}_T$ converge para zero, j√° que $\hat{\delta}_T$ converge a $T^{3/2}$

IV.  Ent√£o, o termo $r_2\hat{\delta}_T \sqrt{T} = r_2 \frac{\hat{\delta}_T}{T^{3/2}}T^2 \frac{1}{\sqrt{T}}$ tende a zero mais r√°pido do que $\sqrt{T}\hat{\alpha}_T$, logo, o comportamento assint√≥tico de $\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)$ √© dominado por $\sqrt{T}r_1\hat{\alpha}_T$, que converge para uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia $r_1^2\sigma^2 q^{11}$

V. Pelo Lema 1, $s_T^2$ converge para $\sigma^2$. Al√©m disso, como $a$ tende a zero, temos que o denominador se aproxima de $\sqrt{\sigma^2 r_1^2 q^{11}}$

VI.  Aplicando o Teorema de Slutsky, temos que:
$$t_T \xrightarrow{d} \frac{N(0, r_1^2\sigma^2 q^{11})}{\sqrt{\sigma^2 r_1^2 q^{11}}} = N(0,1)$$
VII. Portanto, a estat√≠stica $t$ para a hip√≥tese conjunta tamb√©m converge para uma distribui√ß√£o $N(0,1)$. ‚ñ†

### Conclus√£o
Em resumo, os testes $t$ e $F$ usuais, calculados com base nas estimativas de m√≠nimos quadrados ordin√°rios para modelos com tend√™ncia de tempo, s√£o assintoticamente v√°lidos, mesmo quando os estimadores apresentam diferentes taxas de converg√™ncia. Isso ocorre porque os erros padr√£o dos estimadores tamb√©m incorporam essas diferentes ordens de $T$, com um comportamento compensat√≥rio [^1]. Al√©m disso, a distribui√ß√£o assint√≥tica da estat√≠stica $t$ converge para uma distribui√ß√£o normal padr√£o, mesmo que os erros n√£o sejam gaussianos, justificando o uso dos testes de hip√≥tese usuais. Os resultados apresentados confirmam que os testes estat√≠sticos padr√£o s√£o aplic√°veis, n√£o apenas para modelos com vari√°veis estacion√°rias, mas tamb√©m para modelos com tend√™ncia de tempo determin√≠stica, desde que se utilizem as devidas transforma√ß√µes para obter distribui√ß√µes assint√≥ticas n√£o degeneradas.

### Refer√™ncias
[^1]:  Se√ß√£o 16.1 do texto fornecido.
<!-- END -->
