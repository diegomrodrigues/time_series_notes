## Testes de HipÃ³teses para o Modelo de TendÃªncia de Tempo Simples
### IntroduÃ§Ã£o
Em continuidade ao estudo da distribuiÃ§Ã£o assintÃ³tica dos estimadores de mÃ­nimos quadrados ordinÃ¡rios (OLS) em modelos com tendÃªncia de tempo determinÃ­stica, este capÃ­tulo aborda a validade dos testes de hipÃ³teses usuais para esses modelos. Como vimos anteriormente, os estimadores $\hat{\alpha}_T$ e $\hat{\delta}_T$ convergem a taxas diferentes, com $\hat{\alpha}_T$ convergindo a uma taxa de $T^{1/2}$ e $\hat{\delta}_T$ a $T^{3/2}$ [^1]. Apesar dessas diferentes taxas de convergÃªncia, os erros padrÃ£o, $\hat{\sigma}_{\hat{\alpha}_T}$ e $\hat{\sigma}_{\hat{\delta}_T}$, parecem apresentar um comportamento assintÃ³tico compensatÃ³rio, de modo que as estatÃ­sticas $t$ para os testes de hipÃ³teses, como $(\hat{\delta}_T - \delta)/\hat{\sigma}_{\hat{\delta}_T}$ sÃ£o assintoticamente $N(0,1)$ quando as inovaÃ§Ãµes sÃ£o gaussianas [^1]. Assim, surge a questÃ£o sobre a validade desses testes para inovaÃ§Ãµes nÃ£o gaussianas, o que serÃ¡ verificado nesta seÃ§Ã£o.

### Conceitos Fundamentais
A validade assintÃ³tica dos testes $t$ e $F$ para modelos com tendÃªncia de tempo Ã© demonstrada utilizando-se da distribuiÃ§Ã£o assintÃ³tica dos estimadores OLS e de suas variÃ¢ncias.  Inicialmente, consideremos o teste $t$ para a hipÃ³tese nula $H_0: \alpha = \alpha_0$, que pode ser escrito como [^1]:

$$
t_T = \frac{\hat{\alpha}_T - \alpha_0}{\sqrt{s^2_T [1 \, 0](X'X)^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} }}
$$

onde $s^2_T$ Ã© o estimador OLS da variÃ¢ncia do erro e $(X'X)^{-1}$ Ã© a matriz inversa de $(X'X)$ definida em [16.1.16] [^1]. Substituindo $(X'X)$ e multiplicando o numerador e denominador por $\sqrt{T}$, temos:

$$
t_T = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{\sqrt{s^2_T [\sqrt{T} \, 0](X'X)^{-1} \begin{bmatrix} \sqrt{T} \\ 0 \end{bmatrix} }}
$$

Utilizando [16.1.17] e [16.1.19], a expressÃ£o acima pode ser reescrita como:

$$
t_T = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{\sqrt{s^2_T [1 \, 0] Q^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} }}
$$
onde Q Ã© dada por [16.1.20] [^1]. Dado que $\sqrt{T}(\hat{\alpha}_T - \alpha_0)$ Ã© assintoticamente $N(0, \sigma^2 q^{11})$, onde $q^{11}$ Ã© o elemento (1,1) da matriz $Q^{-1}$ [^1] e que $s^2_T$ converge para $\sigma^2$, a estatÃ­stica $t_T$ converge em distribuiÃ§Ã£o para $N(0,1)$, validando assintoticamente o teste $t$ para $\alpha$.

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que temos um modelo com tendÃªncia de tempo, $y_t = \alpha + \delta t + u_t$, e estimamos $\hat{\alpha}_T = 2.5$ com um erro padrÃ£o de $0.5$, $s_T^2 = 1.2$. Queremos testar a hipÃ³tese nula $H_0: \alpha = 2$. O tamanho da amostra Ã© $T = 100$.
>
> Primeiro, calculamos a estatÃ­stica t:
>
> $$
> t_T = \frac{\hat{\alpha}_T - \alpha_0}{\hat{\sigma}_{\hat{\alpha}_T}} = \frac{2.5 - 2}{0.5} = 1
> $$
>
> Agora, vamos usar a formulaÃ§Ã£o com as matrizes para verificar a consistÃªncia dos resultados, lembrando que $\hat{\sigma}_{\hat{\alpha}_T} = \sqrt{s^2_T [1 \, 0](X'X)^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} }$.
>
> Assumindo que $[1 \, 0]Q^{-1}\begin{bmatrix} 1 \\ 0 \end{bmatrix}  = 1$ e usando a fÃ³rmula acima que $\hat{\sigma}_{\hat{\alpha}_T} = 0.5$ e  $s^2_T = 1.2$.
>
>
> $$
> t_T = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{\sqrt{s^2_T [1 \, 0] Q^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} }} = \frac{\sqrt{100}(2.5 - 2)}{\sqrt{1.2 * 1}}  = \frac{10 * 0.5}{\sqrt{1.2}} \approx 4.56
> $$
>  Note que o valor de $t_T$ com $\sqrt{T}$ no numerador Ã© diferente da estatÃ­stica $t$ convencional, que usa $\hat{\sigma}_{\hat{\alpha}_T}$, e converge para uma distribuiÃ§Ã£o normal padrÃ£o conforme o tamanho da amostra aumenta.
>  Em nosso exemplo simplificado, para ilustrar o conceito, assumimos que $\hat{\sigma}_{\hat{\alpha}_T}$ foi calculado previamente. O valor 1 Ã© usado para ilustrar a aplicaÃ§Ã£o da fÃ³rmula, e o valor de 4.56 Ã© o valor real da estatÃ­stica. A decisÃ£o estatÃ­stica dependerÃ¡ do valor crÃ­tico da distribuiÃ§Ã£o Normal com um nÃ­vel de significÃ¢ncia definido. Se o valor absoluto da estatÃ­stica t for maior que o valor crÃ­tico, rejeitamos a hipÃ³tese nula.

De forma anÃ¡loga, o teste $t$ para a hipÃ³tese nula $H_0: \delta = \delta_0$ Ã© dado por [^1]:
$$
t_T = \frac{\hat{\delta}_T - \delta_0}{\sqrt{s^2_T [0 \, 1](X'X)^{-1} \begin{bmatrix} 0 \\ 1 \end{bmatrix} }}
$$

Multiplicando o numerador e o denominador por $T^{3/2}$, obtemos:
$$
t_T = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\sqrt{s^2_T [0 \, T^{3/2}](X'X)^{-1} \begin{bmatrix} 0 \\ T^{3/2} \end{bmatrix} }}
$$
Usando [16.1.17], a expressÃ£o acima se torna:
$$
t_T = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\sqrt{s^2_T [0 \, 1] Q^{-1} \begin{bmatrix} 0 \\ 1 \end{bmatrix} }}
$$

Dado que $T^{3/2}(\hat{\delta}_T - \delta_0)$ Ã© assintoticamente $N(0, \sigma^2 q^{22})$, onde $q^{22}$ Ã© o elemento (2,2) da matriz $Q^{-1}$ [^1],  a estatÃ­stica $t_T$ tambÃ©m converge em distribuiÃ§Ã£o para $N(0,1)$, validando assintoticamente o teste $t$ para $\delta$.

> ðŸ’¡ **Exemplo NumÃ©rico:** Continuando com o exemplo anterior, suponha que estimamos $\hat{\delta}_T = 0.15$ com um erro padrÃ£o de $0.02$. Queremos testar a hipÃ³tese nula $H_0: \delta = 0.1$. Temos $T=100$ e  $s^2_T = 1.2$.
>
> Calculamos a estatÃ­stica t:
>
> $$
> t_T = \frac{\hat{\delta}_T - \delta_0}{\hat{\sigma}_{\hat{\delta}_T}} = \frac{0.15 - 0.1}{0.02} = 2.5
> $$
>
> Usando a formulaÃ§Ã£o com as matrizes,  assumindo que $[0 \, 1]Q^{-1}\begin{bmatrix} 0 \\ 1 \end{bmatrix}  = 1$, temos:
>
> $$
> t_T = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\sqrt{s^2_T [0 \, 1] Q^{-1} \begin{bmatrix} 0 \\ 1 \end{bmatrix} }} = \frac{100^{3/2}(0.15 - 0.1)}{\sqrt{1.2 * 1}} = \frac{1000*0.05}{\sqrt{1.2}} \approx 45.6
> $$
> Assim como no exemplo anterior, o valor 2.5 ilustra o cÃ¡lculo da estatÃ­stica t convencional, enquanto 45.6 ilustra a aplicaÃ§Ã£o da formulaÃ§Ã£o com as matrizes e as respectivas taxas de convergÃªncia.  A decisÃ£o estatÃ­stica sobre rejeitar ou nÃ£o a hipÃ³tese nula Ã© tomada com base no valor crÃ­tico da distribuiÃ§Ã£o Normal padrÃ£o, considerando o nÃ­vel de significÃ¢ncia.

AlÃ©m dos testes para parÃ¢metros individuais, podemos considerar testes de hipÃ³teses conjuntas envolvendo $\alpha$ e $\delta$. Por exemplo, considere a hipÃ³tese $H_0: r_1\alpha + r_2\delta = r$. A estatÃ­stica de teste $t$ correspondente Ã© dada por [^1]:

$$
t_T = \frac{r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r}{\sqrt{s_T^2 \begin{bmatrix} r_1 & r_2 \end{bmatrix} (X'X)^{-1} \begin{bmatrix} r_1 \\ r_2 \end{bmatrix} }}
$$

Multiplicando o numerador e o denominador por $\sqrt{T}$ e utilizando as relaÃ§Ãµes [16.1.17] e [16.1.19], temos:

$$
t_T = \frac{\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)}{\sqrt{s_T^2 \begin{bmatrix} r_1 & r_2/T^{3/2} \end{bmatrix} Q^{-1} \begin{bmatrix} r_1 \\ r_2/T^{3/2} \end{bmatrix} }}
$$

Como o componente que envolve $\hat{\delta}$ converge para zero mais rÃ¡pido, o comportamento assintÃ³tico da estatÃ­stica $t$ Ã© dominado pelo componente que envolve $\hat{\alpha}$. Assim, a estatÃ­stica $t$ converge em distribuiÃ§Ã£o para $N(0,1)$, justificando o uso dos testes usuais.

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que queremos testar $H_0: \alpha + 2\delta = 2.8$. Temos $\hat{\alpha}_T = 2.5$, $\hat{\delta}_T = 0.15$, $r_1 = 1$, $r_2 = 2$, $r=2.8$ e $s_T^2 = 1.2$. Suponha que $\begin{bmatrix} 1 & 2 \end{bmatrix} (X'X)^{-1} \begin{bmatrix} 1 \\ 2 \end{bmatrix} = 0.3$, e temos $T=100$.
>
> Primeiro, calculamos a estatÃ­stica t utilizando os parÃ¢metros e seus erros padrÃµes:
>
> $$
> t_T = \frac{1*2.5 + 2*0.15 - 2.8}{\sqrt{1.2*0.3}} = \frac{2.5 + 0.3 - 2.8}{\sqrt{0.36}} = \frac{0}{0.6} = 0
> $$
>
> Agora, usando a formulaÃ§Ã£o com as matrizes e taxas de convergÃªncia:
>
>
> $$
> t_T = \frac{\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)}{\sqrt{s_T^2 \begin{bmatrix} r_1 & r_2/T^{3/2} \end{bmatrix} Q^{-1} \begin{bmatrix} r_1 \\ r_2/T^{3/2} \end{bmatrix} }}
> $$
> Suponha que  $\begin{bmatrix} 1 & 2/T^{3/2} \end{bmatrix} Q^{-1} \begin{bmatrix} 1 \\ 2/T^{3/2} \end{bmatrix} = 0.2 $. EntÃ£o:
>
> $$
> t_T = \frac{\sqrt{100}(1*2.5 + 2*0.15 - 2.8)}{\sqrt{1.2 * 0.2 }} = \frac{10*(2.8-2.8)}{\sqrt{0.24}} = 0
> $$
>
> Novamente, o valor 0 reflete a aplicaÃ§Ã£o da fÃ³rmula padrÃ£o, enquanto o valor 0 usando a formulaÃ§Ã£o com as matrizes demonstra o mesmo resultado dentro de uma estrutura com a taxa de convergÃªncia, ilustrando a convergÃªncia para uma distribuiÃ§Ã£o normal. O resultado da estatÃ­stica t = 0 nos diz que nÃ£o hÃ¡ evidÃªncias para rejeitar a hipÃ³tese nula, dado o nÃ­vel de significÃ¢ncia.

Outro exemplo importante Ã© o teste conjunto de hipÃ³teses separadas para $\alpha$ e $\delta$ dado por:
$$
H_0: \begin{bmatrix} \alpha \\ \delta \end{bmatrix} = \begin{bmatrix} \alpha_0 \\ \delta_0 \end{bmatrix}
$$

A forma de Wald do teste de $\chi^2$ Ã© dada por:
$$
\chi^2_T = (\mathbf{b}_T - \mathbf{b}_0)' [s_T^2(X'X)^{-1}]^{-1} (\mathbf{b}_T - \mathbf{b}_0)
$$
onde $\mathbf{b}_T = [\hat{\alpha}_T \, \hat{\delta}_T ]'$ e $\mathbf{b}_0 = [\alpha_0 \, \delta_0]'$. Utilizando a propriedade de que $Y_T(X'X)^{-1}Y_T \rightarrow Q^{-1}$ [^1], a estatÃ­stica $\chi^2$ Ã© assintoticamente $\chi^2(2)$.

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que queremos testar a hipÃ³tese conjunta $H_0: \begin{bmatrix} \alpha \\ \delta \end{bmatrix} = \begin{bmatrix} 2 \\ 0.1 \end{bmatrix}$. Temos $\hat{\alpha}_T = 2.5$, $\hat{\delta}_T = 0.15$ e $s_T^2 = 1.2$.  Suponha que $(X'X)^{-1} = \begin{bmatrix} 0.1 & 0.01 \\ 0.01 & 0.002 \end{bmatrix}$.
>
> Primeiro calculamos o vetor de diferenÃ§as:
>
> $$\mathbf{b}_T - \mathbf{b}_0 = \begin{bmatrix} 2.5 \\ 0.15 \end{bmatrix} - \begin{bmatrix} 2 \\ 0.1 \end{bmatrix} = \begin{bmatrix} 0.5 \\ 0.05 \end{bmatrix}$$
>
> Agora, calculamos a inversa de $[s_T^2(X'X)^{-1}]$:
>
> $$[s_T^2(X'X)^{-1}]^{-1} = [1.2 * \begin{bmatrix} 0.1 & 0.01 \\ 0.01 & 0.002 \end{bmatrix}]^{-1} = \begin{bmatrix} 0.12 & 0.012 \\ 0.012 & 0.0024 \end{bmatrix}^{-1} \approx  \begin{bmatrix} 100 & -500 \\ -500 & 5000 \end{bmatrix} $$
>
> A estatÃ­stica $\chi^2$ Ã© entÃ£o:
>
> $$
> \chi^2_T = \begin{bmatrix} 0.5 & 0.05 \end{bmatrix}  \begin{bmatrix} 100 & -500 \\ -500 & 5000 \end{bmatrix} \begin{bmatrix} 0.5 \\ 0.05 \end{bmatrix}
> $$
> $$
> \chi^2_T = \begin{bmatrix} 0.5 & 0.05 \end{bmatrix}  \begin{bmatrix} 25 \\ -25 \end{bmatrix}  =  12.5 -1.25 = 11.25
> $$
> O valor de 11.25 Ã© a estatÃ­stica $\chi^2$  que serÃ¡ comparada com o valor crÃ­tico da distribuiÃ§Ã£o qui-quadrado com 2 graus de liberdade, dado um nÃ­vel de significÃ¢ncia prÃ©-estabelecido, para decidir se rejeitamos ou nÃ£o a hipÃ³tese conjunta.

**ObservaÃ§Ã£o 1:** Ã‰ importante notar que a convergÃªncia em distribuiÃ§Ã£o para $N(0,1)$ das estatÃ­sticas $t$ para $\alpha$ e $\delta$  depende crucialmente da convergÃªncia de $s_T^2$ para $\sigma^2$. Esta convergÃªncia Ã© garantida pelas propriedades dos estimadores OLS sob as condiÃ§Ãµes usuais, incluindo a hipÃ³tese de que os erros sÃ£o i.i.d. com variÃ¢ncia finita.

**Lema 1:** A convergÃªncia de $s_T^2$ para $\sigma^2$ Ã© uma consequÃªncia direta da consistÃªncia do estimador OLS da variÃ¢ncia do erro. Formalmente, se os erros do modelo sÃ£o i.i.d. com mÃ©dia zero e variÃ¢ncia $\sigma^2$, entÃ£o $plim_{T\to\infty} s_T^2 = \sigma^2$.
*Proof:* A prova segue da definiÃ§Ã£o do estimador da variÃ¢ncia do erro e das propriedades de convergÃªncia dos estimadores OLS sob as hipÃ³teses padrÃµes.

I. O estimador OLS da variÃ¢ncia do erro Ã© dado por $s_T^2 = \frac{1}{T-k} \sum_{t=1}^{T} \hat{u}_t^2$, onde $\hat{u}_t$ sÃ£o os resÃ­duos OLS e $k$ Ã© o nÃºmero de parÃ¢metros no modelo.

II. Os resÃ­duos podem ser expressos como $\hat{u}_t = y_t - x_t'\hat{\beta}$, onde $y_t$ Ã© a variÃ¡vel dependente, $x_t$ Ã© o vetor de regressores e $\hat{\beta}$ sÃ£o os estimadores OLS dos parÃ¢metros.

III. Sob as condiÃ§Ãµes de i.i.d. dos erros com mÃ©dia zero e variÃ¢ncia $\sigma^2$, os estimadores OLS $\hat{\beta}$ sÃ£o consistentes, ou seja, $plim_{T\to\infty} \hat{\beta} = \beta$, onde $\beta$ sÃ£o os verdadeiros parÃ¢metros.

IV. Como os estimadores OLS sÃ£o consistentes e a funÃ§Ã£o $f(x) = x^2$ Ã© contÃ­nua, entÃ£o $plim_{T\to\infty} \hat{u}_t^2 = plim_{T\to\infty}(y_t - x_t'\hat{\beta})^2 = (y_t - x_t'\beta)^2 = u_t^2$, onde $u_t$ sÃ£o os verdadeiros erros.

V. Pela lei dos grandes nÃºmeros, $\frac{1}{T} \sum_{t=1}^{T} u_t^2 \xrightarrow{p} E(u_t^2) = \sigma^2$.

VI. Portanto, $plim_{T\to\infty} s_T^2 = plim_{T\to\infty} \frac{T}{T-k} \frac{1}{T} \sum_{t=1}^{T} \hat{u}_t^2 = 1 \cdot \sigma^2 = \sigma^2$, jÃ¡ que $k$ Ã© fixo. â– 

**Teorema 1:** (Robustez dos testes t em relaÃ§Ã£o Ã  nÃ£o-normalidade) Sob as condiÃ§Ãµes de i.i.d. para os erros com mÃ©dia zero e variÃ¢ncia finita $\sigma^2$, mas sem a necessidade de gaussianidade, as estatÃ­sticas t para os parÃ¢metros $\alpha$ e $\delta$ convergem em distribuiÃ§Ã£o para $N(0,1)$.
*Proof:*  A prova Ã© anÃ¡loga Ã  apresentada no texto, utilizando o Teorema do Limite Central para vetores, que afirma que a soma de variÃ¡veis aleatÃ³rias i.i.d. (apÃ³s devida normalizaÃ§Ã£o) converge para uma distribuiÃ§Ã£o normal, mesmo que as variÃ¡veis originais nÃ£o sejam normalmente distribuÃ­das. As derivadas apropriadas dos estimadores OLS podem ser expressas como somas de variÃ¡veis aleatÃ³rias i.i.d. com variÃ¢ncia finita, o que leva Ã  convergÃªncia para uma normal. A convergÃªncia de $s^2_T$ para $\sigma^2$ garante a validade assintÃ³tica das estatÃ­sticas $t$.

I. ComeÃ§amos com a definiÃ§Ã£o da estatÃ­stica t para $\alpha$:
    $$t_T = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{\sqrt{s^2_T [1 \, 0] Q^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} }} $$
II. Sabemos que $\sqrt{T}(\hat{\alpha}_T - \alpha_0)$ converge em distribuiÃ§Ã£o para $N(0, \sigma^2 q^{11})$, onde $q^{11}$ Ã© o elemento (1,1) de $Q^{-1}$, conforme resultado padrÃ£o para estimadores OLS com tendÃªncia.
III. Do Lema 1, temos que $s^2_T$ converge em probabilidade para $\sigma^2$.
IV. Portanto, o denominador $\sqrt{s^2_T [1 \, 0] Q^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} }$ converge em probabilidade para $\sqrt{\sigma^2 q^{11}}$.
V. Aplicando o Teorema de Slutsky, a razÃ£o de uma variÃ¡vel aleatÃ³ria que converge em distribuiÃ§Ã£o para uma normal e uma variÃ¡vel que converge em probabilidade para uma constante diferente de zero, tambÃ©m converge em distribuiÃ§Ã£o para uma normal com a mesma distribuiÃ§Ã£o, entÃ£o:
$$t_T = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{\sqrt{s^2_T [1 \, 0] Q^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} }} \xrightarrow{d} \frac{N(0,\sigma^2 q^{11})}{\sqrt{\sigma^2 q^{11}}} = N(0,1)$$
VI. O mesmo raciocÃ­nio Ã© vÃ¡lido para a estatÃ­stica $t$ para $\delta$.
VII. Portanto, as estatÃ­sticas $t$ para $\alpha$ e $\delta$ convergem em distribuiÃ§Ã£o para $N(0,1)$, mesmo sem a necessidade de gaussianidade dos erros. â– 

**Teorema 1.1:** A estatÃ­stica t para a hipÃ³tese conjunta $H_0: r_1\alpha + r_2\delta = r$ tambÃ©m converge assintoticamente para uma distribuiÃ§Ã£o $N(0,1)$ sob as condiÃ§Ãµes do Teorema 1.
*Proof:* A prova segue os mesmos argumentos apresentados no texto original, observando que o componente que envolve $\hat{\delta}$ converge para zero mais rÃ¡pido, o que faz com que o comportamento assintÃ³tico da estatÃ­stica $t$ seja dominado pelo componente que envolve $\hat{\alpha}$, que converge para uma normal sob as condiÃ§Ãµes de i.i.d. com variÃ¢ncia finita, mas sem necessariamente gaussianidade para os erros.

I. A estatÃ­stica t para a hipÃ³tese conjunta Ã© dada por:
$$t_T = \frac{\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)}{\sqrt{s_T^2 \begin{bmatrix} r_1 & r_2/T^{3/2} \end{bmatrix} Q^{-1} \begin{bmatrix} r_1 \\ r_2/T^{3/2} \end{bmatrix} }} $$

II. Multiplicando e dividindo o numerador e o denominador por $\sqrt{q^{11}}$, e definindo $a = \frac{r_2}{T^{3/2}} $, temos:

$$t_T = \frac{\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)}{\sqrt{s_T^2 (r_1^2 q^{11} + 2r_1 a q^{12} + a^2 q^{22}) }} $$

III.  Sabemos que $\sqrt{T}(\hat{\alpha}_T - \alpha_0)$ converge para uma distribuiÃ§Ã£o normal com mÃ©dia zero e variÃ¢ncia $\sigma^2 q^{11}$ e $T^{3/2}(\hat{\delta}_T - \delta_0)$ converge para uma distribuiÃ§Ã£o normal com mÃ©dia zero e variÃ¢ncia $\sigma^2 q^{22}$ e, portanto, $T^{1/2}\hat{\delta}_T$ converge para zero, jÃ¡ que $\hat{\delta}_T$ converge a $T^{3/2}$

IV.  EntÃ£o, o termo $r_2\hat{\delta}_T \sqrt{T} = r_2 \frac{\hat{\delta}_T}{T^{3/2}}T^2 \frac{1}{\sqrt{T}}$ tende a zero mais rÃ¡pido do que $\sqrt{T}\hat{\alpha}_T$, logo, o comportamento assintÃ³tico de $\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)$ Ã© dominado por $\sqrt{T}r_1\hat{\alpha}_T$, que converge para uma distribuiÃ§Ã£o normal com mÃ©dia zero e variÃ¢ncia $r_1^2\sigma^2 q^{11}$

V. Pelo Lema 1, $s_T^2$ converge para $\sigma^2$. AlÃ©m disso, como $a$ tende a zero, temos que o denominador se aproxima de $\sqrt{\sigma^2 r_1^2 q^{11}}$

VI.  Aplicando o Teorema de Slutsky, temos que:
$$t_T \xrightarrow{d} \frac{N(0, r_1^2\sigma^2 q^{11})}{\sqrt{\sigma^2 r_1^2 q^{11}}} = N(0,1)$$
VII. Portanto, a estatÃ­stica $t$ para a hipÃ³tese conjunta tambÃ©m converge para uma distribuiÃ§Ã£o $N(0,1)$. â– 

### ConclusÃ£o
Em resumo, os testes $t$ e $F$ usuais, calculados com base nas estimativas de mÃ­nimos quadrados ordinÃ¡rios para modelos com tendÃªncia de tempo, sÃ£o assintoticamente vÃ¡lidos, mesmo quando os estimadores apresentam diferentes taxas de convergÃªncia. Isso ocorre porque os erros padrÃ£o dos estimadores tambÃ©m incorporam essas diferentes ordens de $T$, com um comportamento compensatÃ³rio [^1]. AlÃ©m disso, a distribuiÃ§Ã£o assintÃ³tica da estatÃ­stica $t$ converge para uma distribuiÃ§Ã£o normal padrÃ£o, mesmo que os erros nÃ£o sejam gaussianos, justificando o uso dos testes de hipÃ³tese usuais. Os resultados apresentados confirmam que os testes estatÃ­sticos padrÃ£o sÃ£o aplicÃ¡veis, nÃ£o apenas para modelos com variÃ¡veis estacionÃ¡rias, mas tambÃ©m para modelos com tendÃªncia de tempo determinÃ­stica, desde que se utilizem as devidas transformaÃ§Ãµes para obter distribuiÃ§Ãµes assintÃ³ticas nÃ£o degeneradas.

### ReferÃªncias
[^1]:  SeÃ§Ã£o 16.1 do texto fornecido.
<!-- END -->
