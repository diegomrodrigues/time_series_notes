## Testes de HipÃ³teses para o Modelo de TendÃªncia de Tempo Simples: AnÃ¡lise Detalhada da EstatÃ­stica t
### IntroduÃ§Ã£o
Como explorado anteriormente, o capÃ­tulo 16 introduz o conceito de diferentes taxas de convergÃªncia para estimadores de modelos com tendÃªncia de tempo determinÃ­stica [^1]. Especificamente, o estimador do intercepto ($\hat{\alpha}_T$) converge a uma taxa de $T^{1/2}$, enquanto o estimador do coeficiente da tendÃªncia ($\hat{\delta}_T$) converge a uma taxa de $T^{3/2}$ [^1].  Este capÃ­tulo busca aprofundar a anÃ¡lise da validade dos testes de hipÃ³teses usuais, particularmente o teste $t$, nesse contexto, focando na construÃ§Ã£o e nas propriedades assintÃ³ticas da estatÃ­stica de teste. Expandindo a discussÃ£o anterior sobre a convergÃªncia assintÃ³tica dos estimadores OLS, vamos nos concentrar na construÃ§Ã£o da estatÃ­stica *$t$* e sua distribuiÃ§Ã£o limite.

**Lema 1**  A convergÃªncia em probabilidade de $\hat{\sigma}^2_T$ para $\sigma^2$ Ã© crucial para a validade assintÃ³tica dos testes *$t$*. Este resultado, que decorre das propriedades assintÃ³ticas dos estimadores OLS e da Lei dos Grandes NÃºmeros, permite que as estatÃ­sticas *$t$* convirjam para uma distribuiÃ§Ã£o normal padrÃ£o, mesmo quando as distribuiÃ§Ãµes dos erros nÃ£o sÃ£o normais.

### Conceitos Fundamentais
O teste *$t$* para hipÃ³teses sobre os coeficientes em modelos de tendÃªncia de tempo Ã© construÃ­do pela razÃ£o entre o desvio do estimador do valor hipotÃ©tico e o seu erro padrÃ£o, com o numerador e o denominador multiplicados por fatores de escala apropriados para obter distribuiÃ§Ãµes limites nÃ£o degeneradas. Este processo Ã© essencial para lidar com as diferentes taxas de convergÃªncia dos estimadores [^1].

A estatÃ­stica $t$ para testar a hipÃ³tese nula $H_0: \alpha = \alpha_0$ Ã© dada por:

$$
t_T = \frac{\hat{\alpha}_T - \alpha_0}{\sqrt{s^2_T [1 \, 0](X'X)^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} }}
$$
onde $s^2_T$ Ã© o estimador OLS da variÃ¢ncia do erro e $(X'X)^{-1}$ Ã© a matriz inversa da matriz de momentos dos regressores, como definido em [16.1.16] [^1].  Para analisar o comportamento assintÃ³tico desta estatÃ­stica, multiplicamos tanto o numerador quanto o denominador por $\sqrt{T}$:
$$
t_T = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{\sqrt{s^2_T [\sqrt{T} \, 0](X'X)^{-1} \begin{bmatrix} \sqrt{T} \\ 0 \end{bmatrix} }}
$$
Essa multiplicaÃ§Ã£o pelo fator $\sqrt{T}$ Ã© crucial, pois ela adequa a escala da estatÃ­stica para que tenhamos um limite nÃ£o degenerado. Utilizando os resultados de [16.1.17] e [16.1.19], podemos reescrever a expressÃ£o acima:

$$
t_T = \frac{\sqrt{T}(\hat{\alpha}_T - \alpha_0)}{\sqrt{s^2_T [1 \, 0] Q^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} }}
$$
onde $Q$ Ã© a matriz limite definida em [16.1.20] [^1]. O numerador, $\sqrt{T}(\hat{\alpha}_T - \alpha_0)$, converge para uma distribuiÃ§Ã£o normal $N(0, \sigma^2 q^{11})$, onde $q^{11}$ Ã© o elemento (1,1) de $Q^{-1}$ [^1]. Simultaneamente, o denominador, $\sqrt{s^2_T [1 \, 0] Q^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} }$, converge em probabilidade para $\sqrt{\sigma^2 q^{11}}$. Pelo Teorema de Slutsky, a estatÃ­stica $t_T$ converge em distribuiÃ§Ã£o para $N(0,1)$, validando assintoticamente o teste para $\alpha$ mesmo sem a hipÃ³tese de normalidade dos erros, como discutido no Teorema 1 [^1].

> ðŸ’¡ **Detalhes da ConstrÃ§Ã£o da EstatÃ­stica t:** A multiplicaÃ§Ã£o por $\sqrt{T}$ no numerador e no denominador Ã© uma forma de normalizar a estatÃ­stica $t$, que, em sua forma original, tenderia para zero ou infinito Ã  medida que o tamanho da amostra aumenta. A razÃ£o pela qual escolhemos  $\sqrt{T}$ Ã© porque $\hat{\alpha}_T$ converge a essa taxa, de modo que $\sqrt{T}(\hat{\alpha}_T - \alpha_0)$ tem uma distribuiÃ§Ã£o limite nÃ£o degenerada. De forma anÃ¡loga, ao lidar com o coeficiente da tendÃªncia, $\hat{\delta}_T$,  multiplicamos o numerador e o denominador por $T^{3/2}$, jÃ¡ que essa Ã© a taxa de convergÃªncia de $\hat{\delta}_T$.

> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos simular dados para um modelo de tendÃªncia de tempo simples e calcular a estatÃ­stica t para $\alpha$.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
>
> # ParÃ¢metros verdadeiros
> alpha_true = 5
> delta_true = 0.2
> sigma_true = 1.5
> T = 200
>
> # GeraÃ§Ã£o dos dados
> t = np.arange(1, T + 1)
> X = np.column_stack((np.ones(T), t))
> errors = np.random.normal(0, sigma_true, T)
> y = alpha_true + delta_true * t + errors
>
> # EstimaÃ§Ã£o por OLS
> model = sm.OLS(y, X)
> results = model.fit()
>
> # Extraindo os resultados
> alpha_hat = results.params[0]
> delta_hat = results.params[1]
> s_squared = results.mse_resid
>
> # Matriz de covariÃ¢ncia dos coeficientes
> cov_matrix = results.cov_params()
>
> # CÃ¡lculo da estatÃ­stica t para alpha
> alpha_0 = 5  # HipÃ³tese nula para alpha
> t_stat_alpha = (alpha_hat - alpha_0) / np.sqrt(s_squared * cov_matrix[0, 0])
>
> # CÃ¡lculo da estatÃ­stica t escalada por sqrt(T) (versÃ£o para anÃ¡lise assintÃ³tica)
> t_stat_alpha_scaled = (np.sqrt(T) * (alpha_hat - alpha_0)) / np.sqrt(s_squared * cov_matrix[0, 0] * T)
>
> # Imprimindo os resultados
> print(f"Estimativa de alpha: {alpha_hat:.4f}")
> print(f"Estimativa de delta: {delta_hat:.4f}")
> print(f"Estimativa da variÃ¢ncia do erro: {s_squared:.4f}")
> print(f"EstatÃ­stica t para alpha: {t_stat_alpha:.4f}")
> print(f"EstatÃ­stica t para alpha (escalada): {t_stat_alpha_scaled:.4f}")
>
> # Comparando com o valor crÃ­tico de uma normal padrÃ£o
> from scipy.stats import norm
> critical_value = norm.ppf(0.975)  # Para um teste bicaudal de 5%
> print(f"Valor crÃ­tico (bicaudal, 5%): {critical_value:.4f}")
> if abs(t_stat_alpha) > critical_value:
>    print("Rejeita-se a hipÃ³tese nula para alpha")
> else:
>    print("NÃ£o se rejeita a hipÃ³tese nula para alpha")
>
> # DataFrame para anÃ¡lise
> results_df = pd.DataFrame({
>    'Parametro':['alpha', 'delta'],
>    'Estimativa':[alpha_hat, delta_hat],
>    'Erro PadrÃ£o':[np.sqrt(cov_matrix[0,0]), np.sqrt(cov_matrix[1,1])],
>    'EstatÃ­stica T':[results.tvalues[0], results.tvalues[1]],
>    'P-valor':[results.pvalues[0], results.pvalues[1]]
> })
> print("\nResultados da regressÃ£o:\n", results_df)
> ```
>
> Este cÃ³digo simula dados, estima os parÃ¢metros do modelo usando OLS (mÃ­nimos quadrados ordinÃ¡rios) e calcula a estatÃ­stica t para o intercepto. A versÃ£o escalada pela raiz de T ilustra como a normalizaÃ§Ã£o afeta a estatÃ­stica t, permitindo que ela convirja para uma distribuiÃ§Ã£o normal padrÃ£o quando o tamanho da amostra aumenta. Comparando a estatÃ­stica t com o valor crÃ­tico, podemos avaliar a significÃ¢ncia estatÃ­stica do estimador.  AlÃ©m disso, Ã© mostrado como a funÃ§Ã£o `summary()` do modelo OLS jÃ¡ retorna os resultados de interesse.

A estatÃ­stica $t$ para testar a hipÃ³tese nula $H_0: \delta = \delta_0$ Ã© similarmente construÃ­da:

$$
t_T = \frac{\hat{\delta}_T - \delta_0}{\sqrt{s^2_T [0 \, 1](X'X)^{-1} \begin{bmatrix} 0 \\ 1 \end{bmatrix} }}
$$
Multiplicando o numerador e denominador por $T^{3/2}$, temos:
$$
t_T = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\sqrt{s^2_T [0 \, T^{3/2}](X'X)^{-1} \begin{bmatrix} 0 \\ T^{3/2} \end{bmatrix} }}
$$
Usando [16.1.17], a expressÃ£o acima Ã© reescrita como:
$$
t_T = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\sqrt{s^2_T [0 \, 1] Q^{-1} \begin{bmatrix} 0 \\ 1 \end{bmatrix} }}
$$
Nesse caso, o numerador $T^{3/2}(\hat{\delta}_T - \delta_0)$ converge para uma distribuiÃ§Ã£o normal $N(0, \sigma^2 q^{22})$ e o denominador converge em probabilidade para $\sqrt{\sigma^2 q^{22}}$. Assim, pelo Teorema de Slutsky, a estatÃ­stica $t_T$ converge em distribuiÃ§Ã£o para $N(0,1)$, validando o teste para $\delta$ [^1].

**Prova para a ConvergÃªncia da EstatÃ­stica t para $\delta$**

Aqui, apresentamos uma prova detalhada passo a passo da convergÃªncia da estatÃ­stica $t$ para $\delta$ para uma distribuiÃ§Ã£o normal padrÃ£o.

I.  A estatÃ­stica $t$ para testar $H_0: \delta = \delta_0$ Ã© dada por:
$$ t_T = \frac{\hat{\delta}_T - \delta_0}{\sqrt{s^2_T [0 \, 1](X'X)^{-1} \begin{bmatrix} 0 \\ 1 \end{bmatrix} }} $$

II. Multiplicamos o numerador e o denominador por $T^{3/2}$:
$$ t_T = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\sqrt{s^2_T [0 \, T^{3/2}](X'X)^{-1} \begin{bmatrix} 0 \\ T^{3/2} \end{bmatrix} }} $$

III. Substituindo $(X'X)^{-1}$ por $Q^{-1}$ escalado por $T^{-2}$ (de [16.1.17]), obtemos:
$$ t_T = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\sqrt{s^2_T T^{-2}[0 \, T^{3/2}] Q^{-1} \begin{bmatrix} 0 \\ T^{3/2} \end{bmatrix} }} $$

IV. Simplificando a expressÃ£o, obtemos:
$$ t_T = \frac{T^{3/2}(\hat{\delta}_T - \delta_0)}{\sqrt{s^2_T [0 \, 1] Q^{-1} \begin{bmatrix} 0 \\ 1 \end{bmatrix} }} $$

V. Sabemos que $T^{3/2}(\hat{\delta}_T - \delta_0) \xrightarrow{d} N(0, \sigma^2 q^{22})$, onde $q^{22}$ Ã© o elemento (2,2) de $Q^{-1}$ [^1].

VI. TambÃ©m sabemos que $s^2_T \xrightarrow{p} \sigma^2$, e  $[0 \, 1] Q^{-1} \begin{bmatrix} 0 \\ 1 \end{bmatrix}$ Ã© igual a $q^{22}$.

VII. Portanto, o denominador converge em probabilidade para $\sqrt{\sigma^2 q^{22}}$.

VIII. Pelo Teorema de Slutsky, a estatÃ­stica $t_T$ converge em distribuiÃ§Ã£o para:
$$ \frac{N(0, \sigma^2 q^{22})}{\sqrt{\sigma^2 q^{22}}} = N(0,1) $$

IX. Assim, provamos que a estatÃ­stica $t_T$ para testar a hipÃ³tese $H_0: \delta = \delta_0$ converge em distribuiÃ§Ã£o para $N(0,1)$. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:** Continuando o exemplo anterior, vamos calcular a estatÃ­stica t para $\delta$.
>
> ```python
> import numpy as np
> import pandas as pd
> import statsmodels.api as sm
>
> # ParÃ¢metros verdadeiros (os mesmos do exemplo anterior)
> alpha_true = 5
> delta_true = 0.2
> sigma_true = 1.5
> T = 200
>
> # GeraÃ§Ã£o dos dados (os mesmos do exemplo anterior)
> t = np.arange(1, T + 1)
> X = np.column_stack((np.ones(T), t))
> errors = np.random.normal(0, sigma_true, T)
> y = alpha_true + delta_true * t + errors
>
> # EstimaÃ§Ã£o por OLS (os mesmos do exemplo anterior)
> model = sm.OLS(y, X)
> results = model.fit()
>
> # Extraindo os resultados (os mesmos do exemplo anterior)
> alpha_hat = results.params[0]
> delta_hat = results.params[1]
> s_squared = results.mse_resid
>
> # Matriz de covariÃ¢ncia dos coeficientes (os mesmos do exemplo anterior)
> cov_matrix = results.cov_params()
>
> # CÃ¡lculo da estatÃ­stica t para delta
> delta_0 = 0.2  # HipÃ³tese nula para delta
> t_stat_delta = (delta_hat - delta_0) / np.sqrt(s_squared * cov_matrix[1, 1])
>
> # CÃ¡lculo da estatÃ­stica t escalada por T^(3/2) (versÃ£o para anÃ¡lise assintÃ³tica)
> t_stat_delta_scaled = (T**(3/2) * (delta_hat - delta_0)) / np.sqrt(s_squared * cov_matrix[1, 1] * T**3)
>
> # Imprimindo os resultados
> print(f"Estimativa de alpha: {alpha_hat:.4f}")
> print(f"Estimativa de delta: {delta_hat:.4f}")
> print(f"Estimativa da variÃ¢ncia do erro: {s_squared:.4f}")
> print(f"EstatÃ­stica t para delta: {t_stat_delta:.4f}")
> print(f"EstatÃ­stica t para delta (escalada): {t_stat_delta_scaled:.4f}")
>
> # Comparando com o valor crÃ­tico de uma normal padrÃ£o
> from scipy.stats import norm
> critical_value = norm.ppf(0.975)  # Para um teste bicaudal de 5%
> print(f"Valor crÃ­tico (bicaudal, 5%): {critical_value:.4f}")
> if abs(t_stat_delta) > critical_value:
>    print("Rejeita-se a hipÃ³tese nula para delta")
> else:
>    print("NÃ£o se rejeita a hipÃ³tese nula para delta")
>
> # DataFrame para anÃ¡lise
> results_df = pd.DataFrame({
>    'Parametro':['alpha', 'delta'],
>    'Estimativa':[alpha_hat, delta_hat],
>    'Erro PadrÃ£o':[np.sqrt(cov_matrix[0,0]), np.sqrt(cov_matrix[1,1])],
>    'EstatÃ­stica T':[results.tvalues[0], results.tvalues[1]],
>    'P-valor':[results.pvalues[0], results.pvalues[1]]
> })
> print("\nResultados da regressÃ£o:\n", results_df)
> ```
>
> Este cÃ³digo calcula a estatÃ­stica t para o coeficiente de tendÃªncia ($\delta$) usando os mesmos dados simulados do exemplo anterior. Observe a escala por $T^{3/2}$, que Ã© a taxa de convergÃªncia de $\hat{\delta}_T$. Isso demonstra como a estatÃ­stica t Ã© ajustada para lidar com as diferentes taxas de convergÃªncia.  Novamente, comparamos com um valor crÃ­tico para avaliar a significÃ¢ncia estatÃ­stica, e construÃ­mos um dataframe para apresentar os principais resultados da anÃ¡lise.

**Lema 2** A convergÃªncia da matriz de momentos dos regressores, $\frac{X'X}{T^2} \xrightarrow{p} Q$, onde Q Ã© uma matriz definida positiva, Ã© um resultado fundamental que justifica a substituiÃ§Ã£o de $(X'X)^{-1}$ por $Q^{-1}$ no denominador das estatÃ­sticas *$t$* ao analisar suas propriedades assintÃ³ticas. Esta convergÃªncia Ã© uma consequÃªncia da estrutura especÃ­fica do modelo de tendÃªncia de tempo e da aplicaÃ§Ã£o da Lei dos Grandes NÃºmeros.

> ðŸ’¡ **Justificativa da MultiplicaÃ§Ã£o por $T^{3/2}$:** A escolha de $T^{3/2}$ para a estatÃ­stica t de $\delta$ Ã© devido Ã  taxa de convergÃªncia de $\hat{\delta}_T$. Ao multiplicar por $T^{3/2}$, estamos normalizando a estatÃ­stica para que ela possua uma distribuiÃ§Ã£o limite nÃ£o degenerada, o que permite a inferÃªncia estatÃ­stica. Essa manipulaÃ§Ã£o Ã© anÃ¡loga Ã  multiplicaÃ§Ã£o por $\sqrt{T}$ no caso de $\hat{\alpha}_T$, refletindo as diferentes taxas de convergÃªncia dos estimadores.

Para testar a hipÃ³tese conjunta $H_0: r_1\alpha + r_2\delta = r$, a estatÃ­stica *$t$* Ã© dada por:

$$
t_T = \frac{r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r}{\sqrt{s_T^2 \begin{bmatrix} r_1 & r_2 \end{bmatrix} (X'X)^{-1} \begin{bmatrix} r_1 \\ r_2 \end{bmatrix} }}
$$
Multiplicando o numerador e o denominador por $\sqrt{T}$, obtemos:

$$
t_T = \frac{\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)}{\sqrt{s_T^2 \begin{bmatrix} r_1 & r_2/T^{3/2} \end{bmatrix} Q^{-1} \begin{bmatrix} r_1 \\ r_2/T^{3/2} \end{bmatrix} }}
$$
O comportamento assintÃ³tico desta estatÃ­stica Ã© dominado por $\sqrt{T}\hat{\alpha}_T$, jÃ¡ que $\hat{\delta}_T$ converge mais rapidamente para o seu valor verdadeiro. Assim, a estatÃ­stica $t$ converge em distribuiÃ§Ã£o para $N(0,1)$. Este resultado, tambÃ©m discutido no Teorema 1.1, enfatiza como o teste Ã© dominado pela taxa de convergÃªncia mais lenta dos estimadores.

> ðŸ’¡ **ImplicaÃ§Ãµes da Taxa de ConvergÃªncia para Testes Conjuntos:** A taxa de convergÃªncia desempenha um papel crucial nos testes conjuntos, pois determina qual componente dominarÃ¡ o comportamento assintÃ³tico da estatÃ­stica de teste. Em nosso caso, como $\hat{\delta}_T$ converge mais rÃ¡pido do que $\hat{\alpha}_T$, Ã© a taxa de convergÃªncia de  $\hat{\alpha}_T$ que determinarÃ¡ a distribuiÃ§Ã£o limite da estatÃ­stica *$t$*.

**CorolÃ¡rio 1** A convergÃªncia da estatÃ­stica t para a hipÃ³tese conjunta $H_0: r_1\alpha + r_2\delta = r$ para uma distribuiÃ§Ã£o $N(0,1)$ Ã© uma consequÃªncia direta da aplicaÃ§Ã£o do Teorema de Slutsky e do fato de que o numerador da estatÃ­stica, quando apropriadamente escalado, converge para uma distribuiÃ§Ã£o normal, enquanto o denominador converge em probabilidade para uma constante. A dominÃ¢ncia da taxa de convergÃªncia de $\hat{\alpha}_T$ na distribuiÃ§Ã£o limite da estatÃ­stica t ressalta a importÃ¢ncia da escala correta ao lidar com estimadores que convergem a diferentes taxas.

> ðŸ’¡ **Exemplo NumÃ©rico:**  Vamos realizar um teste de hipÃ³tese conjunta, usando os mesmos dados simulados. Vamos testar $H_0: \alpha + 2\delta = 5.4$ (sabendo que os valores verdadeiros sÃ£o $\alpha=5$ e $\delta=0.2$, entÃ£o o valor correto Ã© 5.4).
>
> ```python
> import numpy as np
> import statsmodels.api as sm
> from scipy.stats import norm
>
> # ParÃ¢metros verdadeiros (os mesmos do exemplo anterior)
> alpha_true = 5
> delta_true = 0.2
> sigma_true = 1.5
> T = 200
>
> # GeraÃ§Ã£o dos dados (os mesmos do exemplo anterior)
> t = np.arange(1, T + 1)
> X = np.column_stack((np.ones(T), t))
> errors = np.random.normal(0, sigma_true, T)
> y = alpha_true + delta_true * t + errors
>
> # EstimaÃ§Ã£o por OLS (os mesmos do exemplo anterior)
> model = sm.OLS(y, X)
> results = model.fit()
>
> # Extraindo os resultados (os mesmos do exemplo anterior)
> alpha_hat = results.params[0]
> delta_hat = results.params[1]
> s_squared = results.mse_resid
> cov_matrix = results.cov_params()
>
> # Teste de hipÃ³tese conjunta
> r1 = 1
> r2 = 2
> r = 5.4
>
> # CÃ¡lculo da estatÃ­stica t para a hipÃ³tese conjunta
> numerator = r1 * alpha_hat + r2 * delta_hat - r
> denominator = np.sqrt(s_squared * np.dot(np.dot(np.array([r1,r2]), cov_matrix), np.array([r1,r2])))
> t_stat_joint = numerator / denominator
>
> # CÃ¡lculo da estatÃ­stica t escalada por sqrt(T) (versÃ£o para anÃ¡lise assintÃ³tica)
> denominator_scaled = np.sqrt(s_squared * np.dot(np.dot(np.array([r1,r2/T**(3/2)]), cov_matrix), np.array([r1,r2/T**(3/2)])) * T)
> t_stat_joint_scaled = (np.sqrt(T) * numerator) / denominator_scaled
>
> # Imprimindo os resultados
> print(f"EstatÃ­stica t para a hipÃ³tese conjunta: {t_stat_joint:.4f}")
> print(f"EstatÃ­stica t para a hipÃ³tese conjunta (escalada): {t_stat_joint_scaled:.4f}")
>
> # Comparando com o valor crÃ­tico de uma normal padrÃ£o
> critical_value = norm.ppf(0.975)  # Para um teste bicaudal de 5%
> print(f"Valor crÃ­tico (bicaudal, 5%): {critical_value:.4f}")
> if abs(t_stat_joint) > critical_value:
>    print("Rejeita-se a hipÃ³tese nula conjunta")
> else:
>    print("NÃ£o se rejeita a hipÃ³tese nula conjunta")
> ```
>
> Este exemplo mostra como calcular a estatÃ­stica t para um teste de hipÃ³tese conjunta envolvendo $\alpha$ e $\delta$. A estatÃ­stica t Ã© calculada usando as estimativas dos parÃ¢metros, seus erros padrÃ£o e os valores especificados para a hipÃ³tese nula.  Comparando com o valor crÃ­tico, Ã© possÃ­vel decidir se a hipÃ³tese nula conjunta deve ser rejeitada.

Finalmente, para o teste conjunto de hipÃ³teses separadas para $\alpha$ e $\delta$, usamos a estatÃ­stica $\chi^2$:
$$
\chi^2_T = (\mathbf{b}_T - \mathbf{b}_0)' [s_T^2(X'X)^{-1}]^{-1} (\mathbf{b}_T - \mathbf{b}_0)
$$
onde $\mathbf{b}_T = [\hat{\alpha}_T \, \hat{\delta}_T ]'$ e $\mathbf{b}_0 = [\alpha_0 \, \delta_0]'$.  Utilizando a propriedade de que $Y_T(X'X)^{-1}Y_T \rightarrow Q^{-1}$ [^1], a estatÃ­stica $\chi^2$ converge para uma distribuiÃ§Ã£o $\chi^2(2)$.

**ProposiÃ§Ã£o 1** (ExtensÃ£o para Testes com RestriÃ§Ãµes Lineares) Para o teste de uma hipÃ³tese linear geral sobre os coeficientes do modelo de tendÃªncia de tempo, dada por $H_0: R\mathbf{b} = \mathbf{r}$, onde $R$ Ã© uma matriz de restriÃ§Ãµes de dimensÃ£o $q \times 2$, e $\mathbf{b} = [\alpha, \delta]'$, a estatÃ­stica *$F$* apropriadamente construÃ­da converge para uma distribuiÃ§Ã£o $\chi^2(q)$ assintoticamente, dividida por seus graus de liberdade $q$. Mais precisamente, a estatÃ­stica F Ã© dada por:
$$
F_T = \frac{ (R\mathbf{\hat{b}}_T - \mathbf{r})' [s_T^2 R(X'X)^{-1}R']^{-1} (R\mathbf{\hat{b}}_T - \mathbf{r}) }{q}
$$

**Prova para a ConvergÃªncia da EstatÃ­stica F**

Aqui, fornecemos uma prova detalhada da convergÃªncia da estatÃ­stica $F$ para uma distribuiÃ§Ã£o qui-quadrado dividida por seus graus de liberdade.

I. A estatÃ­stica $F$ para testar a hipÃ³tese linear geral $H_0: R\mathbf{b} = \mathbf{r}$ Ã© dada por:
$$ F_T = \frac{ (R\mathbf{\hat{b}}_T - \mathbf{r})' [s_T^2 R(X'X)^{-1}R']^{-1} (R\mathbf{\hat{b}}_T - \mathbf{r}) }{q} $$
onde $\mathbf{b} = [\alpha, \delta]'$.

II. Sabemos que $\sqrt{T}(\hat{\mathbf{b}}_T - \mathbf{b})$ converge para uma distribuiÃ§Ã£o normal multivariada com mÃ©dia zero e matriz de covariÃ¢ncia $\sigma^2 Q^{-1}$, ou seja:
$$\sqrt{T}(\hat{\mathbf{b}}_T - \mathbf{b}) \xrightarrow{d} N(0, \sigma^2 Q^{-1})$$
Assim:
$$ \sqrt{T}(R\hat{\mathbf{b}}_T - R\mathbf{b}) \xrightarrow{d} N(0, \sigma^2 RQ^{-1}R') $$

III. Sob a hipÃ³tese nula $H_0$, $R\mathbf{b} = \mathbf{r}$, entÃ£o $\sqrt{T}(R\hat{\mathbf{b}}_T - \mathbf{r}) \xrightarrow{d} N(0, \sigma^2 RQ^{-1}R')$.

IV. Usando a propriedade que se $\mathbf{z} \sim N(0, \Sigma)$, entÃ£o $\mathbf{z}'\Sigma^{-1}\mathbf{z} \sim \chi^2(k)$, onde k Ã© a dimensÃ£o de $\mathbf{z}$,  e o fato de que $s^2_T \xrightarrow{p} \sigma^2$:
$$T(R\hat{\mathbf{b}}_T - \mathbf{r})' [\sigma^2 R(X'X)^{-1}R']^{-1} (R\hat{\mathbf{b}}_T - \mathbf{r})  \xrightarrow{d} \chi^2(q)$$

V. Dividindo pelo nÃºmero de restriÃ§Ãµes q e sabendo que $s^2_T$ converge em probabilidade para $\sigma^2$:
$$ \frac{ (R\mathbf{\hat{b}}_T - \mathbf{r})' [s_T^2 R(X'X)^{-1}R']^{-1} (R\mathbf{\hat{b}}_T - \mathbf{r}) }{q} \xrightarrow{d} \frac{\chi^2(q)}{q} $$

VI. Portanto, provamos que a estatÃ­stica $F_T$ converge em distribuiÃ§Ã£o para uma distribuiÃ§Ã£o qui-quadrado com $q$ graus de liberdade, dividida por $q$. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos calcular a estatÃ­stica F para testar a hipÃ³tese conjunta $H_0: \alpha = 5$ e $\delta = 0.2$.
>
> ```python
> import numpy as np
> import statsmodels.api as sm
> from scipy.stats import chi2
>
> # ParÃ¢metros verdadeiros (os mesmos do exemplo anterior)
> alpha_true = 5
> delta_true = 0.2
> sigma_true = 1.5
> T = 200
>
> # GeraÃ§Ã£o dos dados (os mesmos do exemplo anterior)
> t = np.arange(1, T + 1)
> X = np.column_stack((np.ones(T), t))
> errors = np.random.normal(0, sigma_true, T)
> y = alpha_true + delta_true * t + errors
>
> # EstimaÃ§Ã£o por OLS (os mesmos do exemplo anterior)
> model = sm.OLS(y, X)
> results = model.fit()
>
> # Extraindo os resultados (os mesmos do exemplo anterior)
> alpha_hat = results.params[0]
> delta_hat = results.params[1]
> s_squared = results.mse_resid
> cov_matrix = results.cov_params()
>
> # HipÃ³teses nulas
> alpha_0 = 5
> delta_0 = 0.2
>
> # Vetor de estimativas e hipÃ³teses nulas
> b_hat = np.array([alpha_hat, delta_hat])
> b_0 = np.array([alpha_0, delta_0])
>
> # Matriz de restriÃ§Ã£o
> R = np.eye(2) # Testamos alfa e delta individualmente
> r = b_0
>
> # Calculando a estatÃ­stica F
> numerator = np.dot(np.dot((R @ b_hat - r).T, np.linalg.inv(s_squared * R @ cov_matrix @ R.T)), (R @ b_hat - r))
> F_stat = numerator / 2 # q = 2, nÃºmero de restriÃ§Ãµes
>
> # Imprimindo os resultados
> print(f"EstatÃ­stica F: {F_stat:.4f}")
>
> # Comparando com o valor crÃ­tico de uma chi-quadrado
> degrees_freedom = 2
> critical_value = chi2.ppf(0.95, degrees_freedom)
> print(f"Valor crÃ­tico (chi-quadrado, 5%): {critical_value:.4f}")
>
> if F_stat > critical_value:
>    print("Rejeita-se a hipÃ³tese nula conjunta")
> else:
>    print("NÃ£o se rejeita a hipÃ³tese nula conjunta")
> ```
> Este exemplo mostra como calcular a estatÃ­stica F para um teste conjunto das hipÃ³teses $H_0: \alpha = \alpha_0$ e $H_0: \delta = \delta_0$. A estatÃ­stica F Ã© construÃ­da usando as estimativas dos parÃ¢metros, suas covariÃ¢ncias e os valores especificados sob a hipÃ³tese nula. O valor da estatÃ­stica F Ã© entÃ£o comparado com um valor crÃ­tico de uma distribuiÃ§Ã£o qui-quadrado para determinar se a hipÃ³tese nula conjunta deve ser rejeitada.

A demonstraÃ§Ã£o segue um raciocÃ­nio anÃ¡logo Ã  demonstraÃ§Ã£o de convergÃªncia da estatÃ­stica $\chi^2$, onde a matriz de covariÃ¢ncia assintÃ³tica dos estimadores Ã© utilizada para construir a estatÃ­stica de teste.

### ConclusÃ£o
A anÃ¡lise detalhada da estatÃ­stica $t$ para modelos com tendÃªncia de tempo revelou que as diferentes taxas de convergÃªncia dos estimadores requerem manipulaÃ§Ãµes especÃ­ficas para obter distribuiÃ§Ãµes limites nÃ£o degeneradas. As multiplicaÃ§Ãµes por $\sqrt{T}$ e $T^{3/2}$ sÃ£o essenciais para garantir que as estatÃ­sticas $t$ convirjam para distribuiÃ§Ãµes normais padrÃ£o, permitindo a realizaÃ§Ã£o de inferÃªncias estatÃ­sticas vÃ¡lidas.  Os resultados discutidos neste capÃ­tulo, que se baseiam na convergÃªncia dos estimadores e da variÃ¢ncia dos erros, confirmam que os testes estatÃ­sticos padrÃ£o sÃ£o aplicÃ¡veis, nÃ£o apenas para modelos com variÃ¡veis estacionÃ¡rias, mas tambÃ©m para modelos com tendÃªncia de tempo determinÃ­stica, demonstrando a robustez dos mÃ©todos estatÃ­sticos.
### ReferÃªncias
[^1]: SeÃ§Ã£o 16.1 do texto fornecido.
<!-- END -->
