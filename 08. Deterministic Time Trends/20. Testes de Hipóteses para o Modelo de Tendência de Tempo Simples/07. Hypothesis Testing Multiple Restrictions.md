## Testes de Hip√≥teses com M√∫ltiplas Restri√ß√µes e Taxas de Converg√™ncia em Modelos com Tend√™ncia de Tempo

### Introdu√ß√£o
Este cap√≠tulo aprofunda a discuss√£o sobre testes de hip√≥teses em modelos com tend√™ncia de tempo determin√≠stica, focando em restri√ß√µes lineares que envolvem m√∫ltiplos par√¢metros. Como visto em cap√≠tulos anteriores, os estimadores de m√≠nimos quadrados ordin√°rios (OLS) para modelos com tend√™ncia de tempo, especificamente $\hat{\alpha}_T$ e $\hat{\delta}_T$, convergem a taxas diferentes: $T^{1/2}$ e $T^{3/2}$, respectivamente [^1, ^2, ^3]. Embora os testes t e F padr√£o sejam assintoticamente v√°lidos, o comportamento assint√≥tico de testes com m√∫ltiplas restri√ß√µes, envolvendo par√¢metros com diferentes taxas de converg√™ncia, √© dominado pelos par√¢metros com as taxas de converg√™ncia mais lentas. Este fen√¥meno simplifica a an√°lise da distribui√ß√£o assint√≥tica de tais testes e permite a aplica√ß√£o de infer√™ncias estat√≠sticas v√°lidas.

### Conceitos Fundamentais
Ao testarmos hip√≥teses lineares que envolvem m√∫ltiplos par√¢metros em modelos com tend√™ncia de tempo, a assimetria nas taxas de converg√™ncia dos estimadores OLS impacta a distribui√ß√£o assint√≥tica da estat√≠stica de teste. Consideremos uma hip√≥tese nula linear geral da forma [^2]:

$$ H_0: R\mathbf{\beta} = \mathbf{r} $$

onde $R$ √© uma matriz de restri√ß√µes de dimens√£o $m \times (p+2)$, $\mathbf{\beta} = [\alpha, \delta]'$ √© o vetor de par√¢metros, e $\mathbf{r}$ √© um vetor de constantes de dimens√£o $m \times 1$. A estat√≠stica de teste t para essa hip√≥tese, como visto anteriormente, pode ser escrita como:

$$
t_T = \frac{R\hat{\mathbf{\beta}} - \mathbf{r}}{\sqrt{s_T^2 R(X'X)^{-1}R'}}
$$

onde $\hat{\mathbf{\beta}} = [\hat{\alpha}_T, \hat{\delta}_T]'$, $s_T^2$ √© o estimador da vari√¢ncia do erro, e $(X'X)^{-1}$ √© a inversa da matriz de momentos dos regressores [^2]. A an√°lise assint√≥tica de $t_T$ requer considera√ß√£o das diferentes taxas de converg√™ncia de $\hat{\alpha}_T$ e $\hat{\delta}_T$. Como demonstrado nas se√ß√µes anteriores, a estat√≠stica de teste pode ser expressa como [^2]:

$$
t_T = \frac{\sqrt{T}(R\hat{\mathbf{\beta}} - \mathbf{r})}{\sqrt{s_T^2 R(X'X)^{-1}R' T}}
$$

Ao analisarmos a distribui√ß√£o assint√≥tica de $t_T$, observamos que a taxa de converg√™ncia do numerador √© dominada pelo par√¢metro com a menor taxa de converg√™ncia, $\hat{\alpha}_T$, que converge a $T^{1/2}$ [^2]. Isso significa que o comportamento assint√≥tico da estat√≠stica de teste √© similar ao de um teste envolvendo apenas $\hat{\alpha}_T$, simplificando a an√°lise. De forma an√°loga ao teste $t$, para o teste $F$, a estat√≠stica de teste √© dada por [^2]:

$$
F_T = \frac{ (R\mathbf{\hat{b}}_T - \mathbf{r})' [s_T^2 R(X'X)^{-1}R']^{-1} (R\mathbf{\hat{b}}_T - \mathbf{r}) }{q}
$$
onde q √© o n√∫mero de restri√ß√µes na matriz R. Os par√¢metros do vetor $\mathbf{b}$ convergem a taxas diferentes, o que impacta na distribui√ß√£o assint√≥tica de $F_T$. A an√°lise assint√≥tica da estat√≠stica F requer transforma√ß√µes de escala apropriadas, an√°logas ao que foi apresentado na se√ß√£o anterior [^2]. Para tratar essa heterogeneidade nas taxas de converg√™ncia, multiplicamos e dividimos a estat√≠stica por fatores apropriados, dependendo se a restri√ß√£o √© sobre $\alpha$, $\delta$, ou ambos. Especificamente, para testar hip√≥teses lineares em que todos os par√¢metros envolvidos convergem a uma mesma taxa de converg√™ncia, √© preciso usar a transforma√ß√£o de Sims, Stock, e Watson [^1]. Essa transforma√ß√£o garante que as diferentes taxas de converg√™ncia sejam acomodadas e que a distribui√ß√£o assint√≥tica das estat√≠sticas de teste seja bem-definida, como demonstrado na se√ß√£o 16.3 [^1].

**Lema 1** (Domin√¢ncia da Taxa de Converg√™ncia Mais Lenta) A distribui√ß√£o assint√≥tica de uma estat√≠stica de teste envolvendo m√∫ltiplos par√¢metros em modelos com tend√™ncia de tempo √© dominada pela taxa de converg√™ncia do par√¢metro que converge mais lentamente. Se a hip√≥tese nula envolve restri√ß√µes sobre $\hat{\alpha}_T$ e $\hat{\delta}_T$, a taxa de converg√™ncia de $\hat{\alpha}_T$, ou seja, $T^{1/2}$, domina o comportamento assint√≥tico do teste.
*Proof:* A prova se baseia na an√°lise da distribui√ß√£o assint√≥tica da estat√≠stica de teste, utilizando o Teorema do Limite Central e o Teorema de Slutsky [^2]. Ao analisar o numerador da estat√≠stica de teste, que √© uma combina√ß√£o linear dos estimadores, a taxa de converg√™ncia do termo dominante √© aquela associada ao estimador que converge mais lentamente. Formalmente, dada a restri√ß√£o $H_0: r_1\alpha + r_2\delta = r$, a estat√≠stica $t$ √© dada por:
$$
t_T = \frac{r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r}{\sqrt{s_T^2 \begin{bmatrix} r_1 & r_2 \end{bmatrix} (X'X)^{-1} \begin{bmatrix} r_1 \\ r_2 \end{bmatrix} }}
$$
Multiplicando o numerador e denominador por $\sqrt{T}$, e usando [16.1.17] e [16.1.19], temos:
$$
t_T = \frac{\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)}{\sqrt{s_T^2 \begin{bmatrix} r_1 & r_2/T^{3/2} \end{bmatrix} Q^{-1} \begin{bmatrix} r_1 \\ r_2/T^{3/2} \end{bmatrix} }}
$$
I.  Reescrevemos a estat√≠stica de teste $t_T$ multiplicando o numerador e o denominador por $\sqrt{T}$:
    $$
    t_T = \frac{\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)}{\sqrt{s_T^2 \begin{bmatrix} r_1 & r_2 \end{bmatrix} (X'X)^{-1} \begin{bmatrix} r_1 \\ r_2 \end{bmatrix}T}}
    $$
II. Usando as transforma√ß√µes de escala apropriadas para as taxas de converg√™ncia, e usando [16.1.17] e [16.1.19], a matriz $(X'X)^{-1}$ √© reescalada como $Q^{-1}$:
    $$
     t_T = \frac{\sqrt{T}(r_1\hat{\alpha}_T + r_2\hat{\delta}_T - r)}{\sqrt{s_T^2 \begin{bmatrix} r_1 & r_2/T^{3/2} \end{bmatrix} Q^{-1} \begin{bmatrix} r_1 \\ r_2/T^{3/2} \end{bmatrix} }}
    $$
III. Como $\hat{\delta}_T$ converge a $T^{3/2}$, o termo $r_2\hat{\delta}_T\sqrt{T}$ converge para zero:  $\hat{\delta}_T \sqrt{T} = \hat{\delta}_T T^{-3/2}T^2 \frac{1}{\sqrt{T}}$ e $\hat{\delta}_T T^{3/2}$ converge para um valor finito, e os demais termos s√£o uma constante ou tendem para um n√∫mero finito. Portanto o numerador √© dominado pelo termo $\sqrt{T}r_1\hat{\alpha}_T$.
IV. Pelo Teorema de Slutsky, a estat√≠stica $t_T$ converge para uma distribui√ß√£o $N(0,1)$, onde a distribui√ß√£o limite √© aquela do termo com a taxa de converg√™ncia mais lenta [^2].
V. A prova se estende analogamente para a estat√≠stica $F$, onde a taxa de converg√™ncia do estimador de menor taxa determina a distribui√ß√£o limite. $\blacksquare$

> üí° **Exemplo Num√©rico:** Para ilustrar o Lema 1, considere o teste da hip√≥tese $H_0: \alpha + \delta = 5.2$. Suponha que temos um modelo linear com tend√™ncia de tempo, onde $y_t = \alpha + \delta t + u_t$. Geramos dados com $\alpha=5$, $\delta=0.2$ e erro $u_t \sim N(0,1)$, com $T=100$. As estimativas dos par√¢metros s√£o $\hat{\alpha} = 5.1$ e $\hat{\delta} = 0.18$. Usando $s^2 = 0.95$ e a matriz $Q^{-1}$ com os elementos $q^{11}=0.3$, $q^{12} = -0.005$ e $q^{22} = 0.001$, a estat√≠stica t sem transforma√ß√£o √©:
>
>  $$
> t_T = \frac{5.1 + 0.18 - 5.2}{\sqrt{0.95(0.3+2*(-0.005) + 0.001)}} \approx \frac{0.08}{\sqrt{0.95 * 0.291}} \approx 0.15
> $$
>
> Agora, a estat√≠stica t com transforma√ß√µes para as taxas de converg√™ncia √©:
>
> $$
> t_T = \frac{\sqrt{T}(\hat{\alpha}_T + \hat{\delta}_T - 5.2)}{\sqrt{s_T^2 \begin{bmatrix} 1 & 1/T^{3/2} \end{bmatrix} Q^{-1} \begin{bmatrix} 1 \\ 1/T^{3/2} \end{bmatrix} }}
> $$
>
> $$
> t_T = \frac{\sqrt{100}(5.1 + 0.18 - 5.2)}{\sqrt{0.95 \begin{bmatrix} 1 & 0.0001 \end{bmatrix} \begin{bmatrix} 0.3 & -0.005 \\ -0.005 & 0.001 \end{bmatrix} \begin{bmatrix} 1 \\ 0.0001 \end{bmatrix} }} = \frac{-0.2}{\sqrt{0.95 * 0.29}} \approx -0.37
> $$
>
> A estat√≠stica t, ap√≥s as transforma√ß√µes que levam em conta as taxas de converg√™ncia distintas, converge para uma distribui√ß√£o assint√≥tica $N(0,1)$.  Observe que o fator $T^{3/2}$ no componente $\delta$ faz com que este componente se torne menos relevante no numerador e denominador do teste estat√≠stico, o que demonstra que a distribui√ß√£o assint√≥tica √© dominada pela taxa de converg√™ncia mais lenta, que √© a do $\alpha$.  A decis√£o estat√≠stica depender√° da compara√ß√£o com o valor cr√≠tico da distribui√ß√£o normal padr√£o. Como $\alpha$ converge mais lentamente que $\delta$, a distribui√ß√£o assint√≥tica da estat√≠stica $t$ √© dominada pela distribui√ß√£o de $\alpha$.
>
> ```python
> import numpy as np
> from scipy.stats import norm
>
> # Dados do exemplo
> T = 100
> alpha_hat = 5.1
> delta_hat = 0.18
> s_squared = 0.95
> Q_inv = np.array([[0.3, -0.005], [-0.005, 0.001]])
>
> # Estat√≠stica t sem transforma√ß√£o
> t_stat_no_transf = (alpha_hat + delta_hat - 5.2) / np.sqrt(s_squared * (0.3 + 2 * (-0.005) + 0.001))
> print(f"Estat√≠stica t sem transforma√ß√£o: {t_stat_no_transf:.2f}")
>
> # Estat√≠stica t com transforma√ß√£o
> t_stat_transf = (np.sqrt(T) * (alpha_hat + delta_hat - 5.2)) / np.sqrt(s_squared * np.dot(np.array([1, 1/T**(3/2)]), np.dot(Q_inv, np.array([1, 1/T**(3/2)]))))
> print(f"Estat√≠stica t com transforma√ß√£o: {t_stat_transf:.2f}")
>
> # Valor cr√≠tico para um teste bicaudal com 5% de signific√¢ncia
> critical_value = norm.ppf(0.975)
> print(f"Valor cr√≠tico (bicaudal, 5%): {critical_value:.2f}")
>
> # Verifica√ß√£o da hip√≥tese
> if abs(t_stat_transf) > critical_value:
>     print("Rejeitar H0 com base na estat√≠stica t com transforma√ß√£o.")
> else:
>     print("N√£o rejeitar H0 com base na estat√≠stica t com transforma√ß√£o.")
> ```

**Teorema 1** (Distribui√ß√£o Assint√≥tica dos Testes com M√∫ltiplas Restri√ß√µes) Dada uma hip√≥tese nula linear $H_0: R\mathbf{\beta} = \mathbf{r}$, onde $R$ √© uma matriz $m \times (p+2)$ e $\mathbf{b} = [\alpha, \delta]'$, a estat√≠stica de Wald, constru√≠da como:

$$
\chi^2_T = (\mathbf{b}_T - \mathbf{b}_0)' [s_T^2(X'X)^{-1}]^{-1} (\mathbf{b}_T - \mathbf{b}_0)
$$

converge em distribui√ß√£o para uma vari√°vel aleat√≥ria qui-quadrado com $m$ graus de liberdade, denotada por $\chi^2(m)$, quando as restri√ß√µes envolvem par√¢metros com diferentes taxas de converg√™ncia.
*Proof:*
A demonstra√ß√£o formal deste teorema se baseia na aplica√ß√£o do Teorema do Limite Central e do Teorema de Slutsky, similar √† prova do Teorema da se√ß√£o anterior, mas com as devidas adapta√ß√µes para as taxas de converg√™ncia distintas. A transforma√ß√£o de Sims, Stock, e Watson √© utilizada para derivar a distribui√ß√£o assint√≥tica do vetor de estimadores.

I. A estat√≠stica de Wald √© definida como:
$$\chi^2_T = (\mathbf{b}_T - \mathbf{b}_0)' [s_T^2(X'X)^{-1}]^{-1} (\mathbf{b}_T - \mathbf{b}_0)$$
onde $\mathbf{b}_T$ √© o vetor de estimadores OLS e $\mathbf{b}_0$ √© o vetor de valores sob a hip√≥tese nula [^2].
II. Multiplicando e dividindo a estat√≠stica por $Y_T$, definida em [16.1.17] e [16.3.14], e usando [16.1.19], temos:
$$\chi^2_T = (Y_T(\mathbf{b}_T - \mathbf{b}_0))'[s_T^2Y_T(X'X)^{-1}Y_T]^{-1}(Y_T(\mathbf{b}_T - \mathbf{b}_0))$$
$$ \xrightarrow{d}  (Y_T(\mathbf{b}_T - \mathbf{b}_0))' [\sigma^2 Q^{-1}]^{-1} (Y_T(\mathbf{b}_T - \mathbf{b}_0))$$
III.  A estat√≠stica $Y_T(\mathbf{b}_T - \mathbf{b}_0)$ converge para uma distribui√ß√£o normal multivariada, com matriz de covari√¢ncia dada por $\sigma^2 Q^{-1}$, onde $Q$ √© definida em [16.1.20] [^1].
IV. Sob a hip√≥tese nula, $R\mathbf{b} = \mathbf{r}$, podemos reescrever a estat√≠stica $\chi^2_T$ como:
$$ \chi^2_T = (R\hat{\mathbf{b}}_T - R\mathbf{b})'[s_T^2 R(X'X)^{-1}R']^{-1} (R\hat{\mathbf{b}}_T - R\mathbf{b})  $$
V.  Considerando que $R\hat{\mathbf{b}}_T - R\mathbf{b}$ converge para uma distribui√ß√£o normal multivariada com matriz de covari√¢ncia $\sigma^2 RQ^{-1}R'$, e que $s_T^2$ converge em probabilidade para $\sigma^2$  [^2], temos:
$$ \chi^2_T \xrightarrow{d} \chi^2(m) $$
onde $m$ √© o n√∫mero de restri√ß√µes. $\blacksquare$

> üí° **Implica√ß√µes do Teorema:** O Teorema 1 implica que o teste de Wald √© assintoticamente v√°lido mesmo quando os par√¢metros envolvidos t√™m taxas de converg√™ncia distintas. A transforma√ß√£o de Sims, Stock, e Watson garante que o comportamento assint√≥tico do teste seja consistente com os testes de hip√≥teses padr√£o, ou seja, sua converg√™ncia para uma qui-quadrado, simplificando a infer√™ncia estat√≠stica em modelos com tend√™ncia de tempo.

**Corol√°rio 1** (Generaliza√ß√£o para Restri√ß√µes Lineares)  A distribui√ß√£o assint√≥tica da estat√≠stica de Wald n√£o depende da especifica√ß√£o exata da matriz de restri√ß√£o R, desde que as restri√ß√µes sejam lineares e o n√∫mero de restri√ß√µes seja menor que o n√∫mero de par√¢metros. A aplica√ß√£o do Teorema 1 se estende para qualquer conjunto de restri√ß√µes lineares em que todos os par√¢metros envolvidos est√£o na forma padr√£o.
*Proof:* A prova segue diretamente do Teorema 1, observando que a converg√™ncia da estat√≠stica de Wald para uma distribui√ß√£o qui-quadrado depende apenas do comportamento assint√≥tico da forma quadr√°tica da diferen√ßa entre os estimadores e os valores hipot√©ticos. Sob a hip√≥tese nula, essa forma quadr√°tica converge para uma distribui√ß√£o qui-quadrado com graus de liberdade iguais ao n√∫mero de restri√ß√µes lineares, independentemente da forma espec√≠fica da matriz de restri√ß√£o, desde que seja de dimens√£o $m \times (p+2)$ e rank $m$, e que as condi√ß√µes de regularidade para a aplica√ß√£o do Teorema do Limite Central para vetores sejam satisfeitas. $\blacksquare$

> üí° **Exemplo Num√©rico:** Vamos calcular o teste de Wald para a hip√≥tese conjunta de que $\alpha = 5$ e $\delta = 0.2$. Os dados s√£o gerados da mesma forma que no exemplo anterior, com $T=100$, e agora temos $\hat{\alpha} = 5.05$, $\hat{\delta} = 0.21$, e $s^2=1.1$, e a matriz de covari√¢ncia dos par√¢metros $(X'X)^{-1} = \begin{bmatrix} 0.2 & -0.005 \\ -0.005 & 0.001 \end{bmatrix}$.
>
>  A estat√≠stica de Wald √© calculada como:
>
> $\text{Step 1: } \mathbf{b}_T - \mathbf{b}_0 = \begin{bmatrix} 5.05 - 5 \\ 0.21 - 0.2 \end{bmatrix} = \begin{bmatrix} 0.05 \\ 0.01 \end{bmatrix}$
>
> $\text{Step 2: } (X'X)^{-1} = \begin{bmatrix} 0.2 & -0.005 \\ -0.005 & 0.001 \end{bmatrix} $
>
> $\text{Step 3: } s_T^2(X'X)^{-1} = 1.1 \times \begin{bmatrix} 0.2 & -0.005 \\ -0.005 & 0.001 \end{bmatrix} = \begin{bmatrix} 0.22 & -0.0055 \\ -0.0055 & 0.0011 \end{bmatrix}$
>
> $\text{Step 4: } [s_T^2(X'X)^{-1}]^{-1} = \begin{bmatrix} 0.22 & -0.0055 \\ -0.0055 & 0.0011 \end{bmatrix}^{-1} \approx \begin{bmatrix} 4.76 & 23.8 \\ 23.8 & 952.38 \end{bmatrix}$
>
> $\text{Step 5: } \chi^2_T = \begin{bmatrix} 0.05 & 0.01 \end{bmatrix} \begin{bmatrix} 4.76 & 23.8 \\ 23.8 & 952.38 \end{bmatrix} \begin{bmatrix} 0.05 \\ 0.01 \end{bmatrix} =  4.76 * 0.05^2 + 2 * 23.8 * 0.05 * 0.01 + 952.38 * 0.01^2 = 0.0119 + 0.0238 + 0.095238 \approx 0.131$
>
> A estat√≠stica de Wald √© $\chi^2_T \approx 0.131$, que √© bem menor do que o valor cr√≠tico da distribui√ß√£o qui-quadrado com 2 graus de liberdade, que para um n√≠vel de signific√¢ncia de 5% √© igual a 5.99. Portanto, n√£o rejeitamos a hip√≥tese nula conjunta.
>
> ```python
> import numpy as np
> from scipy.stats import chi2
>
> # Dados do exemplo
> alpha_hat = 5.05
> delta_hat = 0.21
> s_squared = 1.1
> cov_matrix = np.array([[0.2, -0.005], [-0.005, 0.001]])
>
> # Valores sob H0
> alpha_0 = 5
> delta_0 = 0.2
>
> # Step 1: b_hat - b_0
> b_diff = np.array([alpha_hat - alpha_0, delta_hat - delta_0])
>
> # Step 2: (X'X)^-1 (j√° fornecido)
>
> # Step 3: s^2 * (X'X)^-1
> s_cov_matrix = s_squared * cov_matrix
>
> # Step 4: Inversa de s^2 * (X'X)^-1
> inv_s_cov_matrix = np.linalg.inv(s_cov_matrix)
>
> # Step 5: Estat√≠stica de Wald
> chi2_stat = np.dot(b_diff, np.dot(inv_s_cov_matrix, b_diff))
>
> print(f"Estat√≠stica de Wald: {chi2_stat:.3f}")
>
> # Valor cr√≠tico para qui-quadrado com 2 graus de liberdade (5% signific√¢ncia)
> critical_value = chi2.ppf(0.95, 2)
> print(f"Valor cr√≠tico (qui-quadrado, 2 graus de liberdade, 5%): {critical_value:.2f}")
>
> # Verifica√ß√£o da hip√≥tese
> if chi2_stat > critical_value:
>    print("Rejeitar H0 com base no teste de Wald.")
> else:
>    print("N√£o rejeitar H0 com base no teste de Wald.")
> ```
> Este resultado demonstra a validade do teste de Wald em cen√°rios com par√¢metros com diferentes taxas de converg√™ncia.

**Proposi√ß√£o 1** (Teste F com Restri√ß√µes Lineares) Dada a hip√≥tese nula $H_0: R\mathbf{\beta} = \mathbf{r}$, a estat√≠stica de teste F, definida como:

$$
F_T = \frac{ (R\mathbf{\hat{b}}_T - \mathbf{r})' [s_T^2 R(X'X)^{-1}R']^{-1} (R\mathbf{\hat{b}}_T - \mathbf{r}) }{m}
$$
onde $m$ √© o n√∫mero de restri√ß√µes, converge em distribui√ß√£o para uma distribui√ß√£o F com $m$ e $T-p-2$ graus de liberdade, ou assintoticamente para uma distribui√ß√£o $\chi^2(m)/m$ quando $T \to \infty$, desde que as condi√ß√µes de regularidade para aplica√ß√£o do Teorema do Limite Central e a transforma√ß√£o de Sims, Stock, e Watson sejam satisfeitas.

*Proof:* A prova segue de maneira an√°loga ao Teorema 1, utilizando as transforma√ß√µes de Sims, Stock e Watson para ajustar as diferentes taxas de converg√™ncia.
I. A estat√≠stica F √© definida como:
    $$
    F_T = \frac{ (R\mathbf{\hat{b}}_T - \mathbf{r})' [s_T^2 R(X'X)^{-1}R']^{-1} (R\mathbf{\hat{b}}_T - \mathbf{r}) }{m}
    $$
II. Sob a hip√≥tese nula, e com o uso das transforma√ß√µes adequadas, o numerador converge para uma distribui√ß√£o qui-quadrado com $m$ graus de liberdade.
III. O denominador √© o n√∫mero de restri√ß√µes, $m$, que √© uma constante.
IV.  O termo $s_T^2$ converge para $\sigma^2$, a vari√¢ncia do erro.
V. Assim, a estat√≠stica F converge para uma distribui√ß√£o $F_{m, T-p-2}$.
VI. Alternativamente, quando $T$ tende para o infinito, a estat√≠stica F converge para uma vari√°vel qui-quadrado dividida pelo n√∫mero de restri√ß√µes, $\chi^2(m)/m$, que √© o limite da distribui√ß√£o F quando o denominador vai para infinito. $\blacksquare$

> üí° **Conex√£o com o Teste de Wald:** A Proposi√ß√£o 1 estabelece uma conex√£o direta entre o teste F e o teste de Wald. Quando $T \to \infty$, a estat√≠stica F se aproxima da estat√≠stica de Wald dividida pelo n√∫mero de restri√ß√µes, demonstrando que ambos os testes fornecem resultados assintoticamente equivalentes para hip√≥teses lineares em modelos com tend√™ncia de tempo.

**Lema 1.1** (Converg√™ncia do Estimador da Vari√¢ncia) O estimador da vari√¢ncia do erro $s_T^2$ converge em probabilidade para a verdadeira vari√¢ncia do erro, $\sigma^2$. Isto √©, $s_T^2 \xrightarrow{p} \sigma^2$.

*Proof:* A demonstra√ß√£o baseia-se na lei dos grandes n√∫meros e nas propriedades dos res√≠duos OLS.
I.  O estimador da vari√¢ncia do erro $s_T^2$ √© definido como:
    $$
    s_T^2 = \frac{1}{T-p-2}\sum_{i=1}^T \hat{u}_i^2
    $$
    onde $\hat{u}_i$ s√£o os res√≠duos OLS.
II. Como os res√≠duos s√£o uma fun√ß√£o dos dados observados e os par√¢metros estimados, e os par√¢metros convergem para seus valores verdadeiros.
III. Os res√≠duos tamb√©m convergem para os verdadeiros erros do modelo.
IV. A lei dos grandes n√∫meros garante que a m√©dia amostral dos quadrados dos res√≠duos converge em probabilidade para a vari√¢ncia populacional.
V. Portanto, temos $s_T^2 \xrightarrow{p} \sigma^2$. Essa propriedade √© fundamental para a validade assint√≥tica das estat√≠sticas de teste. $\blacksquare$

### Conclus√£o
Em resumo, as diferentes taxas de converg√™ncia dos estimadores OLS em modelos com tend√™ncia de tempo, embora complexas, n√£o impedem a aplica√ß√£o dos testes de hip√≥teses usuais, quando apropriadamente normalizadas. O comportamento assint√≥tico de testes com m√∫ltiplas restri√ß√µes √© dominado pelos par√¢metros com as taxas de converg√™ncia mais lentas, simplificando a an√°lise da distribui√ß√£o assint√≥tica. O teste de Wald, ao levar em conta as diferentes taxas de converg√™ncia dos estimadores, mant√©m a sua distribui√ß√£o assint√≥tica qui-quadrado, permitindo que infer√™ncias estat√≠sticas v√°lidas sejam realizadas. A aplica√ß√£o da transforma√ß√£o de Sims, Stock, e Watson permite que a distribui√ß√£o limite das estat√≠sticas de teste seja derivada de maneira an√°loga √† dos testes de modelos de regress√£o com par√¢metros estacion√°rios.

### Refer√™ncias
[^1]: Se√ß√£o 16.3 do texto fornecido.
[^2]: Se√ß√£o 16.2 do texto fornecido.
[^3]: Se√ß√£o 16.1 do texto fornecido.
<!-- END -->
