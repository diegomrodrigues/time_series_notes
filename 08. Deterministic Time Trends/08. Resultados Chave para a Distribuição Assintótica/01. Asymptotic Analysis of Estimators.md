## An√°lise Assint√≥tica dos Estimadores de MQO em Modelos com Tend√™ncias Temporais Determin√≠sticas

### Introdu√ß√£o
Este cap√≠tulo explora as propriedades assint√≥ticas dos estimadores de M√≠nimos Quadrados Ordin√°rios (MQO) em modelos que incluem tend√™ncias temporais determin√≠sticas, expandindo o conhecimento adquirido em cap√≠tulos anteriores sobre regress√µes com vari√°veis estacion√°rias [^1]. Em particular, focaremos na deriva√ß√£o das distribui√ß√µes assint√≥ticas dos estimadores e nas taxas de converg√™ncia, demonstrando como os resultados obtidos se conectam com a teoria cl√°ssica de regress√£o, ao mesmo tempo em que introduzem novas t√©cnicas necess√°rias para lidar com n√£o estacionariedades. O objetivo principal √© fornecer uma base s√≥lida para a compreens√£o da infer√™ncia estat√≠stica em modelos de s√©ries temporais com tend√™ncias determin√≠sticas.

### Conceitos Fundamentais
Como discutido na Se√ß√£o 16.1 [^1], os coeficientes de modelos de regress√£o envolvendo tend√™ncias temporais determin√≠sticas, como no modelo simples de tend√™ncia temporal $y_t = \alpha + \delta t + \epsilon_t$ [^1], n√£o podem ser tratados da mesma maneira que os modelos com vari√°veis estacion√°rias. A principal distin√ß√£o reside nas diferentes *taxas de converg√™ncia* assint√≥tica dos estimadores. Enquanto em modelos estacion√°rios os estimadores convergem para seus verdadeiros valores a uma taxa de $T^{-1/2}$ [^1], em modelos com tend√™ncias temporais as taxas de converg√™ncia podem ser distintas para diferentes par√¢metros, exigindo t√©cnicas de an√°lise mais refinadas [^1].

> üí° **Exemplo Num√©rico:** Considere um modelo de regress√£o simples com uma vari√°vel estacion√°ria $x_t$ e um termo de erro $\epsilon_t$, onde $y_t = \beta x_t + \epsilon_t$. Se usarmos 100 observa√ß√µes, o erro padr√£o do estimador de $\beta$, $\hat{\beta}$, √© proporcional a $1/\sqrt{100} = 0.1$. Se aumentarmos o n√∫mero de observa√ß√µes para 10.000, o erro padr√£o cai para $1/\sqrt{10000} = 0.01$. Isso ilustra a taxa de converg√™ncia de $T^{-1/2}$ para modelos estacion√°rios. Em contrapartida, em modelos com tend√™ncias temporais, diferentes par√¢metros podem ter taxas de converg√™ncia distintas, como ser√° visto a seguir.

A Se√ß√£o 16.1 detalha que para o modelo de tend√™ncia temporal simples, o estimador $\hat{\alpha}_T$ do intercepto, $\alpha$, converge a uma taxa de $T^{-1/2}$ e o estimador $\hat{\delta}_T$ do coeficiente da tend√™ncia temporal, $\delta$, converge a uma taxa de $T^{-3/2}$ [^1]. Isso √© demonstrado atrav√©s da an√°lise da matriz de momentos amostrais, $\sum_{t=1}^T x_t x_t'$, e sua converg√™ncia quando escalonada adequadamente [^1]. Especificamente, a matriz de momentos necessita ser dividida por $T^3$ em vez de $T$ para garantir uma matriz limite convergente [^1].

> üí° **Exemplo Num√©rico:** Suponha que temos dados gerados pelo modelo $y_t = 2 + 0.5t + \epsilon_t$, onde $\epsilon_t$ √© um ru√≠do branco com vari√¢ncia $\sigma^2 = 1$. Simulando dados com $T = 100$, o estimador $\hat{\alpha}_T$ do intercepto ter√° um erro padr√£o pr√≥ximo de $1/\sqrt{100}=0.1$, enquanto o estimador $\hat{\delta}_T$ da tend√™ncia ter√° um erro padr√£o pr√≥ximo de $1/\sqrt{100^3}=0.001$. Isso ilustra que o coeficiente da tend√™ncia converge mais rapidamente para seu valor verdadeiro do que o intercepto.

Para obter distribui√ß√µes assint√≥ticas n√£o degeneradas, o estimador $\hat{\alpha}_T$ √© multiplicado por $\sqrt{T}$ e o estimador $\hat{\delta}_T$ por $T^{3/2}$ [^1]. Essa opera√ß√£o, que pode ser vista como a pr√©-multiplica√ß√£o de $[16.1.6]$ ou $[16.1.8]$ pela matriz $Y_T$ [^1]:
$$
Y_T = \begin{bmatrix}
\sqrt{T} & 0 \\
0 & T^{3/2}
\end{bmatrix}
$$
permite que os termos resultantes convirjam para uma distribui√ß√£o normal multivariada [^1]. A Se√ß√£o 16.1.1 detalha a necessidade de se multiplicar os desvios dos estimadores pelos fatores adequados para se chegar a uma distribui√ß√£o limite n√£o degenerada [^1].

**Lema 1.1:** A matriz $Y_T$ definida acima √© uma matriz diagonal que escala os estimadores de acordo com suas respectivas taxas de converg√™ncia. Mais geralmente, para um modelo com tend√™ncia temporal polinomial de grau $p$, onde $x_t = [1, t, t^2, \ldots, t^p]'$, a matriz de escalonamento apropriada seria uma matriz diagonal $Y_T = \text{diag}(T^{1/2}, T^{3/2}, T^{5/2}, \ldots, T^{(2p+1)/2})$. Essa matriz generaliza a matriz $Y_T$ definida anteriormente e permite lidar com modelos com tend√™ncias mais complexas.

> üí° **Exemplo Num√©rico:** Considere um modelo com tend√™ncia quadr√°tica: $y_t = \beta_0 + \beta_1 t + \beta_2 t^2 + \epsilon_t$. Neste caso, a matriz $Y_T$ seria $\text{diag}(\sqrt{T}, T^{3/2}, T^{5/2})$. O estimador de $\beta_0$ seria escalonado por $\sqrt{T}$, o de $\beta_1$ por $T^{3/2}$ e o de $\beta_2$ por $T^{5/2}$. Essa escalagem garante que todos os estimadores tenham distribui√ß√µes assint√≥ticas n√£o degeneradas.

A Se√ß√£o 16.1.2 demonstra que, apesar das diferentes taxas de converg√™ncia, as estat√≠sticas $t$ e $F$ usuais t√™m as mesmas distribui√ß√µes assint√≥ticas das regress√µes estacion√°rias [^1]. Essa propriedade decorre de um comportamento assint√≥tico compensat√≥rio nos termos de erro padr√£o, que, ao serem escalonados de forma adequada, levam √† converg√™ncia para uma distribui√ß√£o normal padr√£o [^1].

**Caixa de destaque:**
>√â crucial entender que a converg√™ncia dos estimadores e das estat√≠sticas $t$ e $F$ em modelos com tend√™ncias temporais determin√≠sticas requer um tratamento cuidadoso das taxas de converg√™ncia. As multiplica√ß√µes por $\sqrt{T}$ e $T^{3/2}$ s√£o essenciais para obter distribui√ß√µes assint√≥ticas n√£o degeneradas [^1].

A Se√ß√£o 16.1.1 detalha a abordagem para encontrar a distribui√ß√£o limite, focando na multiplica√ß√£o do desvio do estimador MQO por $\sqrt{T}$ [^1]:
$$
\sqrt{T}(b_T - \beta) = \left[(1/T) \sum_{t=1}^T x_t x_t' \right]^{-1} \left( (1/\sqrt{T}) \sum_{t=1}^T x_t \epsilon_t \right)
$$
onde $x_t$ √© o vetor de regressores e $\epsilon_t$ √© o erro do modelo [^1]. O objetivo √© aplicar o Teorema do Limite Central para mostrar que a parte da direita da equa√ß√£o converge para uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia que depende de matrizes de momento [^1]. A se√ß√£o 16.1.1  mostra detalhadamente que, para uma tend√™ncia determin√≠stica, a matriz $(1/T) \sum_{t=1}^T x_t x_t'$ n√£o converge para uma matriz n√£o-singular, ent√£o √© necess√°rio usar uma matriz $Y_T$ como apresentado acima para realizar a mudan√ßa de escala e obter a converg√™ncia [^1].

**Lemma 1:** Para o modelo com tend√™ncia temporal determin√≠stica simples, a matriz de momentos escalonada $(1/T^3) \sum_{t=1}^T x_t x_t'$ converge para uma matriz n√£o-singular [^1]:
$$
\lim_{T \to \infty} \frac{1}{T^3} \sum_{t=1}^T x_t x_t' = \begin{bmatrix} 0 & 0 \\ 0 & 1/3 \end{bmatrix}
$$
Essa matriz limite, apesar de ser singular, leva a uma distribui√ß√£o normal multivariada n√£o degenerada quando os estimadores s√£o escalonados adequadamente [^1].

> üí° **Exemplo Num√©rico:** Vamos calcular a matriz de momentos escalonada para $T = 100$ no modelo de tend√™ncia simples $y_t = \alpha + \delta t + \epsilon_t$. Temos $x_t = [1, t]'$.
>
> $\text{Step 1: Calculate } \sum_{t=1}^T x_t x_t'$:
> $$
\sum_{t=1}^{100} x_t x_t' = \begin{bmatrix} \sum_{t=1}^{100} 1 & \sum_{t=1}^{100} t \\ \sum_{t=1}^{100} t & \sum_{t=1}^{100} t^2 \end{bmatrix} = \begin{bmatrix} 100 & \frac{100 \cdot 101}{2} \\ \frac{100 \cdot 101}{2} & \frac{100 \cdot 101 \cdot 201}{6} \end{bmatrix} = \begin{bmatrix} 100 & 5050 \\ 5050 & 338350 \end{bmatrix}
> $$
>
> $\text{Step 2: Calculate } \frac{1}{T^3} \sum_{t=1}^T x_t x_t'$:
>
>$$
\frac{1}{100^3}\sum_{t=1}^{100} x_t x_t' = \frac{1}{1000000} \begin{bmatrix} 100 & 5050 \\ 5050 & 338350 \end{bmatrix} = \begin{bmatrix} 0.0001 & 0.00505 \\ 0.00505 & 0.33835 \end{bmatrix}
>$$
>
>  $\text{Step 3: Compare with the limit matrix } \begin{bmatrix} 0 & 0 \\ 0 & 1/3 \end{bmatrix}$: Observe que o elemento (2,2) est√° pr√≥ximo de 1/3, enquanto os outros elementos convergem para 0 quando T aumenta.

**Prova do Lema 1:**

I. Para o modelo de tend√™ncia temporal simples, temos $x_t = [1, t]'$. Portanto, a matriz $\sum_{t=1}^T x_t x_t'$ √© dada por:
$$
\sum_{t=1}^T x_t x_t' = \sum_{t=1}^T \begin{bmatrix} 1 \\ t \end{bmatrix} \begin{bmatrix} 1 & t \end{bmatrix} = \sum_{t=1}^T \begin{bmatrix} 1 & t \\ t & t^2 \end{bmatrix} = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix}
$$

II. Sabemos que $\sum_{t=1}^T 1 = T$, $\sum_{t=1}^T t = \frac{T(T+1)}{2}$, e $\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6}$.

III. Substituindo as somas, obtemos:
$$
\sum_{t=1}^T x_t x_t' = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix}
$$

IV. Multiplicando por $\frac{1}{T^3}$, temos:
$$
\frac{1}{T^3} \sum_{t=1}^T x_t x_t' = \begin{bmatrix} \frac{1}{T^2} & \frac{T(T+1)}{2T^3} \\ \frac{T(T+1)}{2T^3} & \frac{T(T+1)(2T+1)}{6T^3} \end{bmatrix} = \begin{bmatrix} \frac{1}{T^2} & \frac{1}{2T} + \frac{1}{2T^2} \\ \frac{1}{2T} + \frac{1}{2T^2} & \frac{1}{3} + \frac{1}{2T} + \frac{1}{6T^2} \end{bmatrix}
$$

V. Tomando o limite quando $T \to \infty$, chegamos a:
$$
\lim_{T \to \infty} \frac{1}{T^3} \sum_{t=1}^T x_t x_t' = \begin{bmatrix} 0 & 0 \\ 0 & 1/3 \end{bmatrix}
$$
‚ñ†

A Se√ß√£o 16.1.2 explica que a matriz $Q$ em $[16.1.20]$ √© dada por
$$
Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}
$$
e, usando [16.1.24], a distribui√ß√£o assint√≥tica de $[16.1.18]$ √© expressa como
$$
\begin{bmatrix} \sqrt{T} (\hat{\alpha}_T - \alpha) \\ T^{3/2} (\hat{\delta}_T - \delta) \end{bmatrix} \xrightarrow{d} N(0, \sigma^2 Q^{-1})
$$
onde $\xrightarrow{d}$ indica a converg√™ncia em distribui√ß√£o. Isso estabelece formalmente a distribui√ß√£o assint√≥tica dos estimadores escalonados, mostrando que a aplica√ß√£o do Teorema do Limite Central resulta em uma distribui√ß√£o normal multivariada [^1].

> üí° **Exemplo Num√©rico:** Continuando o exemplo com $y_t = 2 + 0.5t + \epsilon_t$ e $\sigma^2 = 1$, a matriz $Q$ √© $Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$. A inversa de $Q$ √© $Q^{-1} = \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}$. Portanto, a distribui√ß√£o assint√≥tica dos estimadores escalonados √©:
>
> $$
\begin{bmatrix} \sqrt{T} (\hat{\alpha}_T - 2) \\ T^{3/2} (\hat{\delta}_T - 0.5) \end{bmatrix} \xrightarrow{d} N\left(0, \begin{bmatrix} 4 & -6 \\ -6 & 12 \end{bmatrix}\right)
$$
>
> Isso significa que, para $T$ suficientemente grande, $\sqrt{T}(\hat{\alpha}_T - 2)$ se aproxima de uma normal com m√©dia 0 e vari√¢ncia 4, enquanto $T^{3/2}(\hat{\delta}_T - 0.5)$ se aproxima de uma normal com m√©dia 0 e vari√¢ncia 12. A covari√¢ncia entre os estimadores, ap√≥s a escalagem, √© -6.

**Proposi√ß√£o 2:** A matriz $Q$ que aparece na distribui√ß√£o assint√≥tica dos estimadores pode ser generalizada para o modelo com tend√™ncia polinomial de grau $p$. Seja $x_t = [1, t, t^2, \ldots, t^p]'$, ent√£o a matriz $Q$ √© dada por $Q_{ij} = \frac{1}{i+j-1}$, para $i, j = 1, \ldots, p+1$. A prova segue diretamente da generaliza√ß√£o do Lema 1, usando a mesma abordagem de soma de s√©ries, combinada com a defini√ß√£o de $Y_T$ definida no Lema 1.1.

**Prova da Proposi√ß√£o 2:**

I. Considere um modelo com tend√™ncia polinomial de grau $p$, onde $x_t = [1, t, t^2, \ldots, t^p]'$. A matriz de momentos √© dada por $\sum_{t=1}^T x_t x_t'$.

II. O elemento $(i,j)$ da matriz $\sum_{t=1}^T x_t x_t'$ √© $\sum_{t=1}^T t^{i-1}t^{j-1} = \sum_{t=1}^T t^{i+j-2}$.

III. Pelo resultado geral para a soma de s√©ries $\sum_{t=1}^T t^\nu \approx \frac{T^{\nu+1}}{\nu+1}$ (Se√ß√£o 16.1.3), temos que $\sum_{t=1}^T t^{i+j-2} \approx \frac{T^{i+j-1}}{i+j-1}$.

IV. Para escalar adequadamente, dividimos a matriz de momentos por $T^{i+j-1}$ para cada elemento $(i,j)$. Como o estimador correspondente ao termo $t^{i-1}$ √© multiplicado por $T^{(2i-1)/2}$, o termo geral a ser usado para escalar √© $T^{(2i-1)/2}T^{(2j-1)/2} = T^{(i+j-1)}$. Portanto, os elementos da matriz escalonada por $Y_T$ ser√£o:
$$\frac{1}{T^{i+j-1}}\sum_{t=1}^T t^{i+j-2}$$

V. Aplicando o limite quando $T \to \infty$, temos:
$$
\lim_{T \to \infty} \frac{1}{T^{i+j-1}} \sum_{t=1}^T t^{i+j-2} = \frac{1}{i+j-1}
$$
VI. Portanto, a matriz $Q$ √© dada por $Q_{ij} = \frac{1}{i+j-1}$, para $i, j = 1, \ldots, p+1$.
‚ñ†

A Se√ß√£o 16.1.3 explora a deriva√ß√£o de um padr√£o geral para o termo l√≠der de somat√≥rios do tipo $\sum_{t=1}^T t^\nu$ [^1], resultando no termo geral:
$$
\frac{1}{T^{\nu+1}} \sum_{t=1}^T t^\nu \rightarrow \frac{1}{\nu + 1}
$$
que √© crucial para encontrar os componentes da matriz $Q$ [^1]. Esses resultados de soma de s√©ries s√£o fundamentais para o estabelecimento de que a matriz de momentos escalonada converge para a matriz $Q$ [^1]. A Se√ß√£o 16.1.3 detalha o uso da fun√ß√£o $f(r) = r^\nu$ para aproximar a √°rea sob a curva com o objetivo de calcular o limite do somat√≥rio [^1].

> üí° **Exemplo Num√©rico:** Para ilustrar a aproxima√ß√£o da soma por uma integral, considere $\sum_{t=1}^T t^2$. A aproxima√ß√£o da soma pela integral de $f(r) = r^2$ entre 0 e $T$ √© $\int_0^T r^2 dr = \frac{T^3}{3}$. Dividindo por $T^3$, obtemos o limite $\frac{1}{3}$, conforme apresentado na teoria.

**Teorema 3:** Para modelos com tend√™ncia temporal determin√≠stica polinomial de grau $p$, ou seja, $y_t = \beta_0 + \beta_1 t + \beta_2 t^2 + \ldots + \beta_p t^p + \epsilon_t$, os estimadores de MQO escalonados por $Y_T = \text{diag}(T^{1/2}, T^{3/2}, T^{5/2}, \ldots, T^{(2p+1)/2})$ convergem em distribui√ß√£o para uma normal multivariada com m√©dia zero e matriz de covari√¢ncia $\sigma^2 Q^{-1}$, onde a matriz $Q$ √© definida na Proposi√ß√£o 2. A prova segue das mesmas ideias apresentadas na se√ß√£o 16.1, combinadas com as extens√µes dos Lema 1.1 e Proposi√ß√£o 2.

> üí° **Exemplo Num√©rico:** Vamos considerar um modelo com tend√™ncia quadr√°tica: $y_t = \beta_0 + \beta_1 t + \beta_2 t^2 + \epsilon_t$, com $\beta_0 = 1$, $\beta_1 = 0.2$, $\beta_2 = 0.01$ e $\epsilon_t \sim N(0, 0.5^2)$. A matriz $Q$ para este modelo √©
>
> $$
Q = \begin{bmatrix} 1 & 1/2 & 1/3 \\ 1/2 & 1/3 & 1/4 \\ 1/3 & 1/4 & 1/5 \end{bmatrix}
> $$
>
> A inversa de $Q$ √©:
>
> $$
Q^{-1} = \begin{bmatrix} 9 & -36 & 30 \\ -36 & 192 & -180 \\ 30 & -180 & 180 \end{bmatrix}
> $$
>
> Os estimadores escalonados convergem para:
>
> $$
\begin{bmatrix} \sqrt{T} (\hat{\beta}_0 - \beta_0) \\ T^{3/2} (\hat{\beta}_1 - \beta_1) \\ T^{5/2} (\hat{\beta}_2 - \beta_2) \end{bmatrix} \xrightarrow{d} N\left(0, 0.5^2 \begin{bmatrix} 9 & -36 & 30 \\ -36 & 192 & -180 \\ 30 & -180 & 180 \end{bmatrix}\right)
> $$
> Isso mostra como a matriz $Q$ e sua inversa s√£o usadas para caracterizar a distribui√ß√£o assint√≥tica dos estimadores em modelos com tend√™ncias polinomiais.

**Prova do Teorema 3:**
I. O modelo √© dado por $y_t = \beta_0 + \beta_1 t + \beta_2 t^2 + \ldots + \beta_p t^p + \epsilon_t$, que pode ser escrito em forma matricial como $y = X\beta + \epsilon$, onde $X$ √© a matriz de regressores.

II. O estimador de MQO √© dado por $\hat{\beta} = (X'X)^{-1}X'y$. O desvio do estimador √© $\hat{\beta} - \beta = (X'X)^{-1}X'\epsilon$.

III. Multiplicando o desvio do estimador pela matriz de escalonamento $Y_T$, temos:
$Y_T(\hat{\beta} - \beta) = Y_T (X'X)^{-1}X'\epsilon$.

IV. Reescrevendo $Y_T(\hat{\beta} - \beta)$ como $Y_T(X'X)^{-1}Y_T^{-1}Y_TX'\epsilon$, e usando o fato de que $Y_T$ √© uma matriz diagonal, podemos escrever:
$$Y_T(\hat{\beta} - \beta) = (Y_T^{-1}X'X Y_T^{-1})^{-1} Y_T X'\epsilon$$

V. Pelo Lema 1.1 e Proposi√ß√£o 2, sabemos que $\frac{1}{T}Y_T^{-1}X'XY_T^{-1}$ converge para a matriz $Q$ quando $T \to \infty$.
$$\lim_{T \to \infty} \frac{1}{T}Y_T^{-1}X'XY_T^{-1}=Q$$

VI. O termo $Y_TX'\epsilon$ √© dado por uma soma de vari√°veis aleat√≥rias, que, quando multiplicada pela matriz de escala e dividida por $\sqrt{T}$, converge em distribui√ß√£o para uma distribui√ß√£o normal multivariada com m√©dia zero.
$$\frac{1}{\sqrt{T}}Y_TX'\epsilon \xrightarrow{d} N(0,\sigma^2 Q)$$

VII. Usando o resultado de que $\frac{1}{T}Y_T^{-1}X'XY_T^{-1}$ converge para $Q$ e aplicando o Teorema do Limite Central, obtemos:
$$Y_T(\hat{\beta} - \beta) \xrightarrow{d} N(0, \sigma^2 Q^{-1})$$
‚ñ†

### Conclus√£o
Este cap√≠tulo apresentou uma an√°lise detalhada da distribui√ß√£o assint√≥tica dos estimadores de MQO em modelos com tend√™ncias temporais determin√≠sticas. Foi demonstrado que a aplica√ß√£o de teoremas do limite central, combinada com um tratamento cuidadoso das taxas de converg√™ncia e das t√©cnicas de escalonamento, leva a uma distribui√ß√£o normal multivariada para os estimadores. Os resultados sobre a soma de s√©ries e a identifica√ß√£o dos termos dominantes nas somas s√£o essenciais para a deriva√ß√£o dessas distribui√ß√µes assint√≥ticas. Os resultados deste cap√≠tulo s√£o cruciais para a realiza√ß√£o de testes de hip√≥teses e infer√™ncia estat√≠stica em modelos de s√©ries temporais com componentes de tend√™ncia determin√≠stica.

### Refer√™ncias
[^1]: Trechos do texto fornecido.
<!-- END -->
