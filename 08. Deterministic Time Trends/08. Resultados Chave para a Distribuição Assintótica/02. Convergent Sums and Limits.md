## Resultados Chave para a DistribuiÃ§Ã£o AssintÃ³tica em Modelos com TendÃªncias Temporais DeterminÃ­sticas

### IntroduÃ§Ã£o
Este capÃ­tulo aprofunda a anÃ¡lise assintÃ³tica dos estimadores de MÃ­nimos Quadrados OrdinÃ¡rios (MQO) em modelos que incorporam tendÃªncias temporais determinÃ­sticas, com foco particular no comportamento das somas de sÃ©ries envolvidas na derivaÃ§Ã£o das distribuiÃ§Ãµes limites [^1]. Como explorado em capÃ­tulos anteriores, o comportamento assintÃ³tico dos estimadores em modelos de sÃ©ries temporais com tendÃªncias determinÃ­sticas difere significativamente do que Ã© observado em modelos estacionÃ¡rios [^1]. As taxas de convergÃªncia variam entre os parÃ¢metros, exigindo uma anÃ¡lise mais refinada para garantir distribuiÃ§Ãµes limites nÃ£o degeneradas [^1]. O objetivo central deste capÃ­tulo Ã© analisar o termo $(1/T)\sum(t/T)^v$, demonstrar sua convergÃªncia para $1/(v + 1)$, e delinear como este resultado e os somatÃ³rios especÃ­ficos $\sum t$ e $\sum t^2$, junto com o padrÃ£o geral para somas de $t^v$, desempenham um papel crucial na determinaÃ§Ã£o das propriedades assintÃ³ticas dos estimadores de MQO.

### Conceitos Fundamentais
Como vimos anteriormente, em modelos de regressÃ£o com tendÃªncias temporais, a anÃ¡lise assintÃ³tica nÃ£o pode ser feita usando as mesmas ferramentas de modelos estacionÃ¡rios [^1]. A diferenÃ§a crucial reside nas taxas de convergÃªncia dos estimadores e na necessidade de escalonar a matriz de momentos para obter uma matriz limite nÃ£o-singular [^1]. Especificamente, ao analisar modelos com uma tendÃªncia linear, $y_t = \alpha + \delta t + \epsilon_t$, notamos que o estimador de $\delta$ converge a uma taxa mais rÃ¡pida ($T^{-3/2}$) do que o estimador de $\alpha$ ($T^{-1/2}$) [^1].

A chave para a obtenÃ§Ã£o da distribuiÃ§Ã£o assintÃ³tica estÃ¡ na anÃ¡lise de somatÃ³rios como $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$ e suas generalizaÃ§Ãµes [^1]. A SeÃ§Ã£o 16.1.3 detalha o padrÃ£o geral para o somatÃ³rio $\sum_{t=1}^T t^\nu$, onde o termo dominante Ã© dado por $T^{\nu+1}/(\nu+1)$ [^1]. Este resultado Ã© essencial para determinar a matriz de escalonamento adequada, $Y_T$, que garante a convergÃªncia para uma distribuiÃ§Ã£o normal multivariada [^1].

O termo $(1/T)\sum_{t=1}^T (t/T)^\nu$ representa a aproximaÃ§Ã£o da Ã¡rea sob a curva $f(r)=r^\nu$ [^1]. Ao tomar o limite quando $T \to \infty$, este somatÃ³rio converge para a integral de 0 a 1 de $r^\nu$, que Ã© dada por $1/(\nu+1)$ [^1]. Este resultado Ã© fundamental para entender o comportamento dos somatÃ³rios nos limites [^1].
> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos calcular o termo $(1/T)\sum_{t=1}^T (t/T)^\nu$ para $\nu = 1$ e diferentes valores de $T$.
>
> Para $T = 10$:
>
> $\frac{1}{10} \sum_{t=1}^{10} (t/10)^1 = \frac{1}{10} \left( \frac{1}{10} + \frac{2}{10} + \ldots + \frac{10}{10} \right) = \frac{1}{10} \frac{10(10+1)}{2 \cdot 10} = \frac{1}{10} \cdot \frac{55}{10} = 0.55$
>
> Para $T = 100$:
>
> $\frac{1}{100} \sum_{t=1}^{100} (t/100)^1 = \frac{1}{100} \frac{100(100+1)}{2 \cdot 100} = \frac{1}{100} \cdot \frac{5050}{100} = 0.505$
>
> Para $T = 1000$:
>
> $\frac{1}{1000} \sum_{t=1}^{1000} (t/1000)^1 = \frac{1}{1000} \frac{1000(1000+1)}{2 \cdot 1000} =  \frac{1}{1000} \cdot \frac{500500}{1000} = 0.5005$
>
> Como podemos ver, Ã  medida que $T$ aumenta, o valor se aproxima de $1/(\nu + 1) = 1/2 = 0.5$. Isso demonstra a convergÃªncia do termo para o valor teÃ³rico.

**ProposiÃ§Ã£o 1.1:** O termo $(1/T)\sum_{t=1}^T (t/T)^\nu$ converge para $1/(\nu+1)$ quando $T \to \infty$.
*Prova:*
I. Considere a funÃ§Ã£o $f(r) = r^\nu$, onde $r$ varia entre 0 e 1.
II. A soma $(1/T)\sum_{t=1}^T (t/T)^\nu$ pode ser vista como uma soma de Ã¡reas de retÃ¢ngulos de largura $1/T$ e altura $(t/T)^\nu$, aproximando a Ã¡rea sob a curva $f(r) = r^\nu$.
III. No limite, quando $T \to \infty$, essa soma converge para a integral definida de $f(r)$ entre 0 e 1:
$$\lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^T (t/T)^\nu = \int_0^1 r^\nu dr$$
IV. Calculando a integral, temos:
$$\int_0^1 r^\nu dr = \left[ \frac{r^{\nu+1}}{\nu+1} \right]_0^1 = \frac{1}{\nu+1}$$
Portanto, $(1/T)\sum_{t=1}^T (t/T)^\nu \rightarrow 1/(\nu+1)$ quando $T \to \infty$. â– 

A SeÃ§Ã£o 16.1.3 tambÃ©m apresenta resultados especÃ­ficos para somatÃ³rios que sÃ£o essenciais para o desenvolvimento teÃ³rico [^1]. As seguintes somas sÃ£o diretamente utilizadas na anÃ¡lise da matriz de momentos:
$$
\sum_{t=1}^T t = \frac{T(T+1)}{2}
$$
$$
\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6}
$$
Estes resultados sÃ£o usados para derivar os termos dominantes e a matriz limite $Q$. As fÃ³rmulas podem ser verificadas por induÃ§Ã£o.

**Lemma 1.2:** As somas de sÃ©ries $\sum_{t=1}^T t$ e $\sum_{t=1}^T t^2$ tÃªm como termos dominantes $T^2/2$ e $T^3/3$, respectivamente.
*Prova:*
I. Para $\sum_{t=1}^T t = \frac{T(T+1)}{2}$, o termo dominante Ã© $T^2/2$ e o termo de ordem inferior Ã© $T/2$.
II. Para $\sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6}$, expandindo a expressÃ£o, obtemos $\frac{2T^3 + 3T^2 + T}{6}$. O termo dominante Ã© $2T^3/6 = T^3/3$, enquanto os termos de ordem inferior sÃ£o $3T^2/6$ e $T/6$.
III. Portanto, $\frac{1}{T^2} \sum_{t=1}^T t \rightarrow \frac{1}{2}$ e $\frac{1}{T^3} \sum_{t=1}^T t^2 \rightarrow \frac{1}{3}$ quando $T \to \infty$. â– 
> ðŸ’¡ **Exemplo NumÃ©rico:** Consideremos um exemplo com $T=100$.
>
> Para $\sum_{t=1}^T t$:
>
> $\sum_{t=1}^{100} t = \frac{100(100+1)}{2} = 5050$
>
> $\frac{1}{T^2}\sum_{t=1}^{100} t = \frac{5050}{100^2} = \frac{5050}{10000} = 0.505$, que se aproxima de $1/2$ quando $T$ aumenta.
>
> Para $\sum_{t=1}^T t^2$:
>
> $\sum_{t=1}^{100} t^2 = \frac{100(100+1)(2\cdot 100+1)}{6} = 338350$
>
> $\frac{1}{T^3}\sum_{t=1}^{100} t^2 = \frac{338350}{100^3} = \frac{338350}{1000000} = 0.33835$, que se aproxima de $1/3$ quando $T$ aumenta.

**Lemma 1.3:** A soma $\sum_{t=1}^T t^3$ tem como termo dominante $T^4/4$.
*Prova:*
I. A soma $\sum_{t=1}^T t^3$ pode ser expressa como $\frac{T^2(T+1)^2}{4}$.
II. Expandindo esta expressÃ£o, obtemos $\frac{T^4 + 2T^3 + T^2}{4}$.
III. O termo dominante Ã©, portanto, $T^4/4$.
IV. Logo, $\frac{1}{T^4} \sum_{t=1}^T t^3 \rightarrow \frac{1}{4}$ quando $T \to \infty$. â– 
> ðŸ’¡ **Exemplo NumÃ©rico:**  Vamos usar $T=50$ para ilustrar a soma $\sum_{t=1}^T t^3$:
>
> $\sum_{t=1}^{50} t^3 = \frac{50^2(50+1)^2}{4} = \frac{2500 \cdot 2601}{4} = 1625625$
>
> $\frac{1}{T^4} \sum_{t=1}^{50} t^3 = \frac{1625625}{50^4} = \frac{1625625}{6250000} = 0.2601$, que se aproxima de $1/4$ quando $T$ aumenta.

O resultado geral para o somatÃ³rio $\sum_{t=1}^T t^\nu$, como apresentado na SeÃ§Ã£o 16.1.3 [^1], Ã© que o termo dominante Ã© $T^{\nu+1}/(\nu+1)$. Este resultado, junto com os somatÃ³rios especÃ­ficos apresentados acima, Ã© crucial para a derivaÃ§Ã£o da matriz $Q$ e da matriz de escalonamento $Y_T$ [^1].

**Caixa de destaque:**
>A aproximaÃ§Ã£o de somas por integrais e a identificaÃ§Ã£o dos termos dominantes nos somatÃ³rios sÃ£o elementos chave na anÃ¡lise assintÃ³tica dos estimadores de MQO em modelos com tendÃªncias temporais determinÃ­sticas. O termo $(1/T)\sum_{t=1}^T (t/T)^\nu$ converge para $1/(\nu+1)$ e a forma geral das somas $\sum_{t=1}^T t^\nu$ Ã© fundamental para entender o comportamento limite.

### Desenvolvimento
Como visto na introduÃ§Ã£o, a convergÃªncia de estimadores em modelos de sÃ©ries temporais com tendÃªncias requer uma atenÃ§Ã£o cuidadosa ao escalonamento da matriz de momentos [^1]. A necessidade de usar uma matriz de escalonamento $Y_T$ resulta diretamente do comportamento dos somatÃ³rios. A matriz $Y_T$ garante que o limite da matriz de momentos escalonada seja uma matriz nÃ£o singular [^1].
Na SeÃ§Ã£o 16.1.1, o modelo de tendÃªncia simples $y_t = \alpha + \delta t + \epsilon_t$ Ã© apresentado e a matriz de momentos $\sum_{t=1}^T x_t x_t'$, onde $x_t = [1, t]'$, Ã© dada por:
$$
\sum_{t=1}^T x_t x_t' = \begin{bmatrix} \sum_{t=1}^T 1 & \sum_{t=1}^T t \\ \sum_{t=1}^T t & \sum_{t=1}^T t^2 \end{bmatrix}
$$

Usando os resultados de somatÃ³rio apresentados acima, temos:
$$
\sum_{t=1}^T x_t x_t' = \begin{bmatrix} T & \frac{T(T+1)}{2} \\ \frac{T(T+1)}{2} & \frac{T(T+1)(2T+1)}{6} \end{bmatrix}
$$
Dividindo cada termo pelo fator apropriado de T (i.e., $T^2$ para os elementos (1,1) e (2,1) e por $T^3$ para o elemento (2,2)), temos que $\frac{1}{T} \sum_{t=1}^T x_t x_t'$ diverge quando $T \rightarrow \infty$. No entanto, ao dividir por $T^3$ (ou escalar pela matriz $Y_T$), chegamos Ã  matriz limite nÃ£o-singular:
$$
\lim_{T \to \infty} \frac{1}{T^3} \sum_{t=1}^T x_t x_t' = \begin{bmatrix} 0 & 0 \\ 0 & 1/3 \end{bmatrix}
$$

A matriz $Q$ que aparece na distribuiÃ§Ã£o assintÃ³tica dos estimadores Ã© dada por:
$$
Q = \lim_{T \to \infty} \frac{1}{T} Y_T^{-1} \sum_{t=1}^T x_t x_t' Y_T^{-1}
$$
onde $Y_T = \text{diag}(\sqrt{T}, T^{3/2})$ para o modelo de tendÃªncia simples.
> ðŸ’¡ **Exemplo NumÃ©rico:** Para o modelo $y_t = \alpha + \delta t + \epsilon_t$, vamos considerar $T = 100$.
>
> A matriz de momentos $\sum_{t=1}^T x_t x_t'$ Ã©:
>
>  $\begin{bmatrix} 100 & \frac{100(101)}{2} \\ \frac{100(101)}{2} & \frac{100(101)(201)}{6} \end{bmatrix} = \begin{bmatrix} 100 & 5050 \\ 5050 & 338350 \end{bmatrix}$
>
> A matriz de escalonamento Ã© $Y_T = \text{diag}(\sqrt{100}, 100^{3/2}) = \text{diag}(10, 1000)$
>
> A matriz $Y_T^{-1}$ Ã© $\text{diag}(0.1, 0.001)$
>
> $Y_T^{-1} \sum_{t=1}^T x_t x_t' Y_T^{-1} = \begin{bmatrix} 0.1 & 0 \\ 0 & 0.001 \end{bmatrix} \begin{bmatrix} 100 & 5050 \\ 5050 & 338350 \end{bmatrix} \begin{bmatrix} 0.1 & 0 \\ 0 & 0.001 \end{bmatrix} = \begin{bmatrix} 0.1 & 0 \\ 0 & 0.001 \end{bmatrix} \begin{bmatrix} 100 & 5050 \\ 5050 & 338350 \end{bmatrix} \begin{bmatrix} 0.1 & 0 \\ 0 & 0.001 \end{bmatrix} = \begin{bmatrix} 10 & 5.05 \\ 5.05 & 338.35 \end{bmatrix}  \begin{bmatrix} 0.1 & 0 \\ 0 & 0.001 \end{bmatrix} =  \begin{bmatrix} 1 & 0.00505 \\ 0.505 & 0.33835 \end{bmatrix}$
>
>  $\frac{1}{T}Y_T^{-1}\sum_{t=1}^T x_t x_t'Y_T^{-1} = \frac{1}{100} \begin{bmatrix} 1 & 0.00505 \\ 0.505 & 0.33835 \end{bmatrix} = \begin{bmatrix} 0.01 & 0.0000505 \\ 0.00505 & 0.0033835 \end{bmatrix}$
>
> No limite, quando $T$ tende a infinito, essa matriz converge para:
>
> $\lim_{T \to \infty} \frac{1}{T} Y_T^{-1} \sum_{t=1}^T x_t x_t' Y_T^{-1} = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}$

No caso do modelo de tendÃªncia simples, a matriz $Q$ Ã© calculada na SeÃ§Ã£o 16.1.2 como:
$$
Q = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix}
$$
Para o modelo de tendÃªncia polinomial de grau $p$, a matriz $Q$ Ã© dada por $Q_{ij} = 1/(i+j-1)$. Essa matriz surge diretamente do padrÃ£o geral para as somas dos termos $t^\nu$ e a convergÃªncia de $(1/T) \sum_{t=1}^T (t/T)^\nu$ [^1].

**Teorema 2.1:** Para modelos com tendÃªncia temporal polinomial de grau $p$, onde $y_t = \beta_0 + \beta_1 t + \beta_2 t^2 + \ldots + \beta_p t^p + \epsilon_t$, os estimadores de MQO escalonados por $Y_T = \text{diag}(T^{1/2}, T^{3/2}, T^{5/2}, \ldots, T^{(2p+1)/2})$ convergem em distribuiÃ§Ã£o para uma normal multivariada com mÃ©dia zero e matriz de covariÃ¢ncia $\sigma^2 Q^{-1}$, onde $Q_{ij} = 1/(i+j-1)$ para $i,j = 1, ..., p+1$.

*Prova:*
I. O modelo Ã© dado por $y_t = X\beta + \epsilon_t$ onde $X$ Ã© a matriz de regressores, e $\beta = [\beta_0, \beta_1, ..., \beta_p]$.
II. O estimador de MQO Ã© dado por $\hat{\beta} = (X'X)^{-1}X'y$.
III. Para obter a distribuiÃ§Ã£o assintÃ³tica, multiplicamos $(\hat{\beta} - \beta)$ pela matriz de escalonamento $Y_T$ apropriada:
$Y_T (\hat{\beta} - \beta) = Y_T(X'X)^{-1}X'\epsilon_t$.
IV. A partir da anÃ¡lise de somatÃ³rios, e de que $\frac{1}{T} \sum_{t=1}^T (t/T)^v$ converge para $\frac{1}{v+1}$, sabemos que $\frac{1}{T} Y_T^{-1} X'X Y_T^{-1}$ converge para $Q$, onde $Q_{ij} = 1/(i+j-1)$.
V.  Portanto, podemos escrever:
$Y_T (\hat{\beta} - \beta)  = \left(  \frac{1}{T} Y_T^{-1} X'X Y_T^{-1}\right)^{-1} \left(\frac{1}{\sqrt{T}} Y_T X' \epsilon_t \right)$
VI. Pelo Teorema do Limite Central,  $\frac{1}{\sqrt{T}} Y_T X' \epsilon_t$ converge para uma normal com mÃ©dia zero e matriz de covariÃ¢ncia $\sigma^2 Q$. Portanto,
$Y_T (\hat{\beta} - \beta)$ converge para uma distribuiÃ§Ã£o normal multivariada com mÃ©dia zero e matriz de covariÃ¢ncia $\sigma^2 Q^{-1}$.â– 

**CorolÃ¡rio 2.2:** No caso especÃ­fico de um modelo de tendÃªncia quadrÃ¡tica, $y_t = \beta_0 + \beta_1 t + \beta_2 t^2 + \epsilon_t$, a matriz de escalonamento Ã© dada por $Y_T = \text{diag}(T^{1/2}, T^{3/2}, T^{5/2})$, e a matriz $Q$ Ã© dada por:
$$
Q = \begin{bmatrix} 1 & 1/2 & 1/3 \\ 1/2 & 1/3 & 1/4 \\ 1/3 & 1/4 & 1/5 \end{bmatrix}
$$
*Prova:*
I.  Para um modelo de tendÃªncia quadrÃ¡tica, os regressores sÃ£o $x_t = [1, t, t^2]'$.
II. A matriz $Q$ Ã© dada por $Q_{ij} = 1/(i+j-1)$, onde $i, j = 1, 2, 3$.
III. Calculando os elementos da matriz, obtemos:
$Q_{11} = 1/(1+1-1) = 1$
$Q_{12} = Q_{21} = 1/(1+2-1) = 1/2$
$Q_{13} = Q_{31} = 1/(1+3-1) = 1/3$
$Q_{22} = 1/(2+2-1) = 1/3$
$Q_{23} = Q_{32} = 1/(2+3-1) = 1/4$
$Q_{33} = 1/(3+3-1) = 1/5$
IV. Portanto, a matriz $Q$ resultante Ã©:
$$
Q = \begin{bmatrix} 1 & 1/2 & 1/3 \\ 1/2 & 1/3 & 1/4 \\ 1/3 & 1/4 & 1/5 \end{bmatrix}
$$
V. A matriz de escalonamento Ã© $Y_T = \text{diag}(T^{1/2}, T^{3/2}, T^{5/2})$ como demonstrado em Teorema 2.1. â– 
> ðŸ’¡ **Exemplo NumÃ©rico:** Para um modelo de tendÃªncia quadrÃ¡tica $y_t = \beta_0 + \beta_1 t + \beta_2 t^2 + \epsilon_t$ com $T = 50$, a matriz de escalonamento seria $Y_T = \text{diag}(\sqrt{50}, 50^{3/2}, 50^{5/2})$. Usando as somas dos termos, podemos montar a matriz de momentos $\sum_{t=1}^T x_t x_t'$, onde $x_t = [1, t, t^2]'$. ApÃ³s o escalonamento apropriado e tomando o limite quando $T$ tende a infinito, a matriz $Q$ converge para a matriz apresentada no corolÃ¡rio.
>
> Usando os somatÃ³rios, podemos escrever a matriz de momentos como:
> $$ \sum_{t=1}^T x_t x_t' = \begin{bmatrix} \sum_{t=1}^{50} 1 & \sum_{t=1}^{50} t & \sum_{t=1}^{50} t^2 \\ \sum_{t=1}^{50} t & \sum_{t=1}^{50} t^2 & \sum_{t=1}^{50} t^3 \\ \sum_{t=1}^{50} t^2 & \sum_{t=1}^{50} t^3 & \sum_{t=1}^{50} t^4 \end{bmatrix} = \begin{bmatrix} 50 & 1275 & 42925 \\ 1275 & 42925 & 1625625 \\ 42925 & 1625625 & 658043825 \end{bmatrix} $$
> A matriz de escalonamento Ã© $Y_T = \text{diag}(50^{1/2}, 50^{3/2}, 50^{5/2}) \approx \text{diag}(7.07, 353.55, 17677.67)$.
>
> Para obter a matriz $Q$ a partir da matriz de momentos, necessitamos prÃ©-multiplicar e pÃ³s-multiplicar por $Y_T^{-1}$ e dividir por $T$.
>
> $Q = \lim_{T \to \infty} \frac{1}{T} Y_T^{-1} \sum_{t=1}^T x_t x_t' Y_T^{-1}$.
>
> O processo de cÃ¡lculo desta expressÃ£o para um $T$ finito Ã© complexo, mas ao tomar o limite,  a matriz $Q$ converge para $\begin{bmatrix} 1 & 1/2 & 1/3 \\ 1/2 & 1/3 & 1/4 \\ 1/3 & 1/4 & 1/5 \end{bmatrix}$

### ConclusÃ£o
Este capÃ­tulo enfatizou a importÃ¢ncia de analisar o comportamento das somas de sÃ©ries para entender a distribuiÃ§Ã£o assintÃ³tica dos estimadores de MQO em modelos com tendÃªncias temporais determinÃ­sticas. O termo $(1/T)\sum_{t=1}^T (t/T)^\nu$ converge para $1/(\nu+1)$, representando a Ã¡rea sob a curva $f(r) = r^\nu$, e  Ã© essencial para determinar os componentes da matriz $Q$. Os resultados especÃ­ficos para somas como $\sum t$, $\sum t^2$ e o padrÃ£o geral para $\sum t^\nu$, desempenham um papel crucial no escalonamento adequado dos estimadores e na derivaÃ§Ã£o das suas distribuiÃ§Ãµes assintÃ³ticas. Estes resultados formam a base para a inferÃªncia estatÃ­stica em modelos de sÃ©ries temporais que incorporam tendÃªncias temporais determinÃ­sticas.

### ReferÃªncias
[^1]: Trechos do texto fornecido.
<!-- END -->
