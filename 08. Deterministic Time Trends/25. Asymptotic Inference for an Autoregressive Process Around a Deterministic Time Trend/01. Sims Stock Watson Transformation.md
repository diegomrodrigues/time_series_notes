## Asymptotic Inference for an Autoregressive Process Around a Deterministic Time Trend: Transforming Regressors

### Introdu√ß√£o

Em continuidade ao estudo de processos com tend√™ncias de tempo determin√≠sticas, esta se√ß√£o aprofunda a an√°lise de modelos autorregressivos (AR) em torno de tais tend√™ncias. Como vimos anteriormente, no contexto de modelos com tend√™ncias de tempo simples, a an√°lise das propriedades assint√≥ticas dos estimadores de m√≠nimos quadrados ordin√°rios (OLS) requer t√©cnicas espec√≠ficas devido √†s diferentes taxas de converg√™ncia dos estimadores. Expandindo o conceito apresentado, este cap√≠tulo agora aborda processos autorregressivos mais complexos, onde uma tend√™ncia de tempo determin√≠stica coexiste com din√¢micas autorregressivas. O objetivo principal √© empregar uma transforma√ß√£o nos regressores, proposta por Sims, Stock e Watson (1990), para isolar componentes com diferentes taxas de converg√™ncia: termos constantes, tend√™ncias de tempo e vari√°veis aleat√≥rias estacion√°rias de m√©dia zero [^1]. Esta t√©cnica permitir√° aplicar os resultados obtidos para modelos de tend√™ncia de tempo simples e estend√™-los para modelos AR mais complexos [^1].

### Conceitos Fundamentais

O ponto de partida √© o modelo autorregressivo geral com tend√™ncia de tempo determin√≠stica:
$$
y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \epsilon_t
$$
onde $\epsilon_t$ √© um ru√≠do branco com m√©dia zero e vari√¢ncia $\sigma^2$ [^1]. Para analisar este modelo, empregamos a transforma√ß√£o de vari√°veis sugerida por Sims, Stock e Watson (1990), que envolve adicionar e subtrair termos para reescrever a equa√ß√£o em uma forma que facilite a an√°lise assint√≥tica [^1]. Especificamente, adicionamos e subtra√≠mos $\phi_j[\alpha + \delta(t-j)]$ para cada $j=1,2,\ldots,p$ no lado direito da equa√ß√£o, o que leva a:

$$
\begin{aligned}
y_t = & \alpha(1 + \phi_1 + \phi_2 + \dots + \phi_p) + \delta(1 + \phi_1 + 2\phi_2 + \dots + p\phi_p)  \\
& - \delta(\phi_1 + 2\phi_2 + \dots + p\phi_p) + \phi_1[y_{t-1} - \alpha - \delta(t-1)] \\
& + \phi_2[y_{t-2} - \alpha - \delta(t-2)] + \dots + \phi_p[y_{t-p} - \alpha - \delta(t-p)] + \epsilon_t.
\end{aligned}
$$

Esta express√£o pode ser escrita de forma mais compacta como:

$$
y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \dots + \phi_p^* y_{t-p}^* + \epsilon_t,
$$

onde
$$
\begin{aligned}
\alpha^* &= [\alpha(1 + \phi_1 + \phi_2 + \dots + \phi_p) - \delta(\phi_1 + 2\phi_2 + \dots + p\phi_p)] \\
\delta^* &= \delta(1 + \phi_1 + \phi_2 + \dots + \phi_p) \\
\phi_j^* &= \phi_j \\
y_{t-j}^* &= y_{t-j} - \alpha - \delta(t-j)  \quad \text{para } j = 1, 2, \ldots, p
\end{aligned}
$$
[^1].

Essa transforma√ß√£o isola os componentes do modelo original, que agora consistem em um termo constante ($\alpha^*$), uma tend√™ncia de tempo ($\delta^* t$) e vari√°veis aleat√≥rias estacion√°rias de m√©dia zero ($y_{t-j}^*$) [^1].

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um modelo AR(1) com uma tend√™ncia de tempo:
>
> $$y_t = 1 + 0.2t + 0.8y_{t-1} + \epsilon_t$$
>
> Aqui, $\alpha = 1$, $\delta = 0.2$ e $\phi_1 = 0.8$.  Ap√≥s a transforma√ß√£o, temos:
>
> $$\alpha^* = \alpha(1 + \phi_1) - \delta\phi_1 = 1(1 + 0.8) - 0.2(0.8) = 1.8 - 0.16 = 1.64$$
> $$\delta^* = \delta(1 + \phi_1) = 0.2(1 + 0.8) = 0.2(1.8) = 0.36$$
>
>  E o modelo transformado √©:
>
> $$y_t = 1.64 + 0.36t + 0.8(y_{t-1} - 1 - 0.2(t-1)) + \epsilon_t$$
>
> Observe que $\phi_1^* = \phi_1 = 0.8$ e $y_{t-1}^* = y_{t-1} - \alpha - \delta(t-1)$.  Este exemplo ilustra como a transforma√ß√£o isola a tend√™ncia linear e o componente AR.

Para formalizar a transforma√ß√£o algebricamente, o modelo original [16.3.1] √© reescrito na forma matricial como:

$$
y_t = x_t' \beta + \epsilon_t
$$

onde $x_t = [y_{t-1}, y_{t-2}, \ldots, y_{t-p}, 1, t]' $ e $\beta = [\phi_1, \phi_2, \ldots, \phi_p, \alpha, \delta]' $ [^1]. A transforma√ß√£o √© ent√£o expressa como:
$$
y_t = x_t' G' [G']^{-1} \beta + \epsilon_t = x_t^{*'} \beta^* + \epsilon_t
$$
onde $x_t^* = G x_t$ e $\beta^* = [G']^{-1} \beta$ [^1]. A matriz $G$ e sua inversa s√£o definidas como:
$$
G' =
\begin{bmatrix}
1 & 0 & \dots & 0 & 0 & 0 \\
0 & 1 & \dots & 0 & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & \dots & 1 & 0 & 0 \\
- \alpha + \delta & - \alpha + 2\delta & \dots & - \alpha + p\delta & 1 & 0 \\
- \delta & - \delta & \dots & - \delta & 0 & 1
\end{bmatrix}
$$
e
$$
[G']^{-1} =
\begin{bmatrix}
1 & 0 & \dots & 0 & 0 & 0 \\
0 & 1 & \dots & 0 & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & \dots & 1 & 0 & 0 \\
\alpha - \delta & \alpha - 2\delta & \dots & \alpha - p\delta & 1 & 0 \\
\delta & \delta & \dots & \delta & 0 & 1
\end{bmatrix}
$$
[^1].

Essa representa√ß√£o matricial torna expl√≠cita a transforma√ß√£o que leva dos regressores originais para os componentes estacion√°rios, a constante e a tend√™ncia de tempo [^1]. A estimativa de m√≠nimos quadrados ordin√°rios de $\beta^*$ √© dada por:
$$
b^* = [(\sum_{t=1}^T x_t^* x_t^{*'})]^{-1} (\sum_{t=1}^T x_t^* y_t) = [G']^{-1} [(\sum_{t=1}^T x_t x_t')]^{-1} (\sum_{t=1}^T x_t y_t) = [G']^{-1} b
$$
onde $b$ √© o estimador OLS dos coeficientes originais [^1]. √â importante notar que os valores ajustados do modelo transformado s√£o id√™nticos aos do modelo original.

> üí° **Exemplo Num√©rico:**
>
>  Continuando o exemplo anterior com um AR(1), vamos supor que tenhamos 3 per√≠odos de dados: $y_1 = 2.0$, $y_2 = 2.5$, $y_3 = 3.0$.  Vamos construir as matrizes e ilustrar a transforma√ß√£o para este caso simples. Primeiro, os regressores n√£o transformados e o vetor $y$ seriam:
>
> $X = \begin{bmatrix} 1 & 1 & 2 \\ 1 & 2 & 2.5 \\ 1 & 3 & 3 \end{bmatrix}$  e  $Y = \begin{bmatrix} 2 \\ 2.5 \\ 3 \end{bmatrix}$ (Assumindo que o lag inicial $y_0$ j√° est√° dispon√≠vel)
>
> O vetor $x_t$ corresponde √†s linhas de $X$ e $\beta = [\phi_1, \alpha, \delta]$
>
> Para este caso, $p=1$ e a matriz $G'$ e sua inversa ficam:
>
>
> $$
> G' =
> \begin{bmatrix}
> 1 & 0 & 0 \\
> - \alpha + \delta & 1 & 0 \\
> - \delta & 0 & 1
> \end{bmatrix}
> =
> \begin{bmatrix}
> 1 & 0 & 0 \\
> -1 + 0.2 & 1 & 0 \\
> - 0.2 & 0 & 1
> \end{bmatrix}
> =
> \begin{bmatrix}
> 1 & 0 & 0 \\
> -0.8 & 1 & 0 \\
> - 0.2 & 0 & 1
> \end{bmatrix}
> $$
>
> $$
> [G']^{-1} =
> \begin{bmatrix}
> 1 & 0 & 0 \\
> \alpha - \delta & 1 & 0 \\
> \delta & 0 & 1
> \end{bmatrix}
> =
> \begin{bmatrix}
> 1 & 0 & 0 \\
> 1 - 0.2 & 1 & 0 \\
>  0.2 & 0 & 1
> \end{bmatrix}
> =
>  \begin{bmatrix}
> 1 & 0 & 0 \\
> 0.8 & 1 & 0 \\
>  0.2 & 0 & 1
> \end{bmatrix}
> $$
>
> Ent√£o a matriz transformada √© dada por $X^* = X G$
>
> $X^* = \begin{bmatrix} 1 & 1 & 2 \\ 1 & 2 & 2.5 \\ 1 & 3 & 3 \end{bmatrix} \begin{bmatrix}
> 1 & -0.8 & -0.2 \\
> 0 & 1 & 0 \\
> 0 & 0 & 1
> \end{bmatrix} = \begin{bmatrix} 1 & 0.2 & 1.8 \\ 1 & 1.2 & 2.3 \\ 1 & 2.2 & 2.8 \end{bmatrix}$
>
> Observe que a primeira coluna de $X$ √© mantida (correspondente ao lag $y_{t-1}$), a segunda coluna passa a ser $t - ( \alpha - \delta)$ e a terceira coluna (a tend√™ncia linear) permanece $t$. Este exemplo ilustra a transforma√ß√£o matricial.

Essa transforma√ß√£o possibilita a an√°lise das distribui√ß√µes assint√≥ticas dos estimadores OLS, pois os coeficientes associados aos componentes estacion√°rios ($\phi_j^*$) convergem a uma taxa de $\sqrt{T}$ para suas distribui√ß√µes gaussianas limites, enquanto os coeficientes associados √† tend√™ncia ($\delta^*$) convergem mais rapidamente, com uma taxa de $T^{3/2}$ [^1]. Os resultados obtidos em se√ß√µes anteriores para modelos de tend√™ncias de tempo simples s√£o um caso particular desta estrutura mais geral [^1].
A matriz $Y_T$ utilizada para reescalar os estimadores √© dada por:
$$
Y_T =
\begin{bmatrix}
\sqrt{T} & 0 & \dots & 0 & 0 & 0 \\
0 & \sqrt{T} & \dots & 0 & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & \dots & \sqrt{T} & 0 & 0 \\
0 & 0 & \dots & 0 & \sqrt{T} & 0 \\
0 & 0 & \dots & 0 & 0 & T^{3/2}
\end{bmatrix}
$$
[^1].

> üí° **Exemplo Num√©rico:**
>
>  Para ilustrar a matriz $Y_T$, considere novamente o exemplo com um AR(1) e $T=100$.  Nesse caso, $Y_T$ seria:
>
> $$
> Y_{100} =
> \begin{bmatrix}
> \sqrt{100} & 0 & 0 \\
> 0 & \sqrt{100} & 0 \\
> 0 & 0 & 100^{3/2}
> \end{bmatrix}
> =
> \begin{bmatrix}
> 10 & 0 & 0 \\
> 0 & 10 & 0 \\
> 0 & 0 & 1000
> \end{bmatrix}
> $$
>
> Esta matriz reescala os estimadores, fazendo com que o coeficiente $\phi_1$ e o intercepto convirjam a uma velocidade $\sqrt{T}$, enquanto o coeficiente da tend√™ncia converge a uma velocidade $T^{3/2}$. Isso garante que todos os par√¢metros tenham uma distribui√ß√£o limite n√£o degenerada.

**Lema 1**
Let $X = [x_1, x_2, \dots, x_T]'$, $X^* = [x_1^*, x_2^*, \dots, x_T]'$, and $Y = [y_1, y_2, \dots, y_T]'$. Then $X^* = XG$. Furthermore, the transformed OLS estimator $b^*$ can be expressed as:

$$b^* = (X^{*'}X^*)^{-1}X^{*'}Y = (G'X'XG)^{-1}G'X'Y = (G')^{-1}(X'X)^{-1}X'Y = (G')^{-1}b$$

*Prova:*
I.  A rela√ß√£o $x_t^* = Gx_t$ implica $X^* = XG$ pelo empilhamento dos vetores $x_t^*$ em uma matriz.
II.  Usando isso, e a defini√ß√£o de $b^*$ temos: $b^* = (X^{*'}X^*)^{-1}X^{*'}Y = ((XG)'XG)^{-1}(XG)'Y = (G'X'XG)^{-1}G'X'Y$.
III.  Usando a propriedade de invers√£o de matriz para produtos podemos escrever $(G'X'XG)^{-1} = G^{-1}(X'X)^{-1}(G')^{-1}$.
IV. Ent√£o, $b^* = G^{-1}(X'X)^{-1}(G')^{-1}G'X'Y = (G^{-1})(X'X)^{-1}X'Y = (G')^{-1} (X'X)^{-1}X'Y = (G')^{-1}b$.
‚ñ†

**Teorema 1.1**
Under the assumptions of the model, we can state the following asymptotic result:

$$
Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \Sigma)
$$

where $\Sigma$ is the asymptotic covariance matrix. This result implies the estimators associated with the stationary components, corresponding to the first $p$ entries of $\beta^*$, are $\sqrt{T}$-consistent and asymptotically normal, while the estimator of the trend coefficient, the last entry of $\beta^*$, has a faster rate of convergence, with order $T^{3/2}$, also being asymptotically normal.

*Proof Strategy:* This result extends the results known for simpler trend models to the AR case, taking into account the transformation of the regressors and using the appropriate scaling matrix $Y_T$. The proof relies on the properties of the transformation, the asymptotic behavior of the sums of squares and cross-products of the transformed regressors, and the central limit theorem for martingale differences. The application of the $Y_T$ scaling matrix is essential to capture the differing rates of convergence of the estimators.

**Observa√ß√£o 1**
A matriz $G$ √© uma matriz de transforma√ß√£o que nos permite trabalhar com as vari√°veis transformadas $x_t^*$, que s√£o uma combina√ß√£o linear dos regressores originais, mas com a propriedade de que elas se comportam assintoticamente de forma diferente. Ao aplicar a transforma√ß√£o $G$, a matriz dos regressores $X$ √© decomposta em seus componentes estacion√°rios e n√£o estacion√°rios, facilitando a an√°lise assint√≥tica do estimador de m√≠nimos quadrados. A matriz $Y_T$ por sua vez, garante que todas as estimativas sejam reescalonadas de forma que convirjam em velocidade apropriada, permitindo estudar a sua distribui√ß√£o limite conjunta.

**Lema 2**
The transformed regressors can be expressed as $x_t^* = [y_{t-1} - \alpha - \delta(t-1), y_{t-2} - \alpha - \delta(t-2), \dots, y_{t-p} - \alpha - \delta(t-p), 1, t ]'$. These represent the original regressors adjusted for the deterministic trend, along with a constant and a time trend component. The first $p$ elements are stationary around zero if the original process $y_t$ is stationary around a deterministic trend.

*Prova:*
I.  Esta express√£o segue diretamente da defini√ß√£o de $x_t^* = G x_t$ e da estrutura das matrizes $G$ e $x_t$.
II.  Multiplicando as matrizes obt√©m-se o resultado.
III. A propriedade de estacionariedade se mant√©m se o processo AR em si for estacion√°rio em torno de uma tend√™ncia determin√≠stica, uma vez que os termos $y_{t-j} - \alpha - \delta(t-j)$ isolam o comportamento estoc√°stico em torno do componente determin√≠stico.
‚ñ†

**Corol√°rio 1**
Given that the estimator $b^*$ is consistent, if we denote the true $\beta^*$ as  $\beta^* = [\phi_1, \dots, \phi_p, \alpha^*, \delta^*]'$, we have that the transformed parameters $\alpha^*$, and $\delta^*$ are consistently estimated by the respective components of $b^*$, and this allows one to back out consistent estimators for the original parameters $\alpha$ and $\delta$.

*Prova:*
I. Sabemos que $b^* = (G')^{-1} b$, ent√£o $b = G'b^*$.
II. Por defini√ß√£o temos $b = [\hat{\phi_1}, \dots, \hat{\phi_p}, \hat{\alpha}, \hat{\delta}]$ e $b^* = [\hat{\phi_1^*}, \dots, \hat{\phi_p^*}, \hat{\alpha^*}, \hat{\delta^*}]$.
III. Usando a estrutura de $G'$ e dada a consist√™ncia dos par√¢metros transformados, segue-se que $\hat{\alpha}$ e $\hat{\delta}$ s√£o consistentes.
‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Suponha que, ap√≥s a estima√ß√£o, obtivemos:
>
> $$b^* = [\hat{\phi_1^*}, \hat{\alpha^*}, \hat{\delta^*}] = [0.78, 1.7, 0.35]$$
>
> Usando o corol√°rio, podemos obter estimativas para os par√¢metros originais. Lembrando que  $b = G'b^*$, com os valores de $G'$  do exemplo anterior temos:
>
> $$ b =
> \begin{bmatrix}
> 1 & -0.8 & -0.2 \\
> 0 & 1 & 0 \\
> 0 & 0 & 1
> \end{bmatrix}
> \begin{bmatrix}
> 0.78 \\ 1.7 \\ 0.35
> \end{bmatrix} =
> \begin{bmatrix}
> 0.78 - 0.8(1.7) - 0.2(0.35) \\ 1.7 \\ 0.35
> \end{bmatrix} =
> \begin{bmatrix}
> -0.63 \\ 1.7 \\ 0.35
> \end{bmatrix}
> $$
>
>
> Isso corresponderia a:
>
>   $$\hat{\phi_1} = 0.78$$
>
>   $$\hat{\alpha} =  \hat{\alpha^*} - \hat{\delta^*} =  1.7 - 0.35 = 1.35  $$
>
>   $$\hat{\delta} = \frac{\hat{\delta^*}}{1 + \hat{\phi_1^*}} = \frac{0.35}{1 + 0.78} \approx 0.196 $$
>
>
>  Note que esses valores estimados podem diferir dos valores reais, mas s√£o consistentes. A consist√™ncia garante que, com um n√∫mero suficiente de dados, eles convergir√£o para os valores reais.

### Conclus√£o

Nesta se√ß√£o, detalhamos a transforma√ß√£o de regressores proposta por Sims, Stock e Watson (1990) para modelar processos autorregressivos em torno de tend√™ncias de tempo determin√≠sticas. Essa transforma√ß√£o √© crucial para isolar os componentes do modelo original em termos de vari√°veis estacion√°rias de m√©dia zero, um termo constante e uma tend√™ncia de tempo, permitindo uma an√°lise mais clara e precisa das taxas de converg√™ncia e distribui√ß√µes assint√≥ticas dos estimadores OLS. Os resultados obtidos aqui generalizam os resultados anteriores para modelos de tend√™ncia de tempo simples, abrindo caminho para a an√°lise de modelos ainda mais complexos em se√ß√µes posteriores. Al√©m disso, a decomposi√ß√£o em componentes estacion√°rios e n√£o estacion√°rios simplifica a an√°lise da distribui√ß√£o assint√≥tica dos estimadores e das propriedades dos testes de hip√≥teses [^1].

### Refer√™ncias

[^1]:  Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113-44.
<!-- END -->
