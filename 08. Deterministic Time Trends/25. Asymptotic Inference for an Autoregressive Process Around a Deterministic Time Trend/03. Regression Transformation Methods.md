## Asymptotic Inference for an Autoregressive Process Around a Deterministic Time Trend: Hypothesis Testing

### Introdução

Dando continuidade à análise de modelos autorregressivos em torno de tendências de tempo determinísticas, e tendo explorado em detalhe as propriedades assintóticas dos estimadores OLS após a aplicação da transformação de Sims, Stock e Watson (1990), este capítulo se concentra na construção e análise de testes de hipóteses. Como vimos nas seções anteriores, a transformação dos regressores simplifica a análise das distribuições limites e das taxas de convergência, mas a inferência estatística requer a capacidade de testar hipóteses sobre os parâmetros de interesse, tanto nos modelos transformados quanto nos originais. Para atingir este objetivo, esta seção detalha como usar as propriedades assintóticas já estabelecidas para desenvolver testes de hipóteses válidos. Abordaremos especificamente como as transformações lineares da matriz dos regressores afetam a construção dos testes e como as distribuições assintóticas derivadas para os parâmetros transformados podem ser usadas para inferir as propriedades assintóticas dos parâmetros originais.

### Conceitos Fundamentais

Como vimos nos capítulos anteriores, o modelo autorregressivo com tendência de tempo determinística pode ser escrito como:
$$
y_t = \alpha + \delta t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \epsilon_t
$$
[^1]. Após a transformação dos regressores, o modelo é reescrito como:
$$
y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \dots + \phi_p^* y_{t-p}^* + \epsilon_t
$$
onde $\alpha^*$, $\delta^*$, $\phi_j^*$ e $y_{t-j}^*$ são os parâmetros e variáveis transformados [^1]. As distribuições assintóticas dos estimadores $\hat{\alpha}^*$, $\hat{\delta}^*$ e $\hat{\phi_j^*}$ são mais simples de analisar, com $\hat{\phi_j^*}$ convergindo à taxa $\sqrt{T}$ e $\hat{\delta}^*$ convergindo à taxa $T^{3/2}$, e seus estimadores são assintoticamente independentes [^2]. No entanto, em muitas situações, o interesse reside em testar hipóteses sobre os parâmetros originais $\alpha$, $\delta$ e $\phi_j$. Para tal, necessitamos de uma ponte entre as distribuições dos estimadores transformados e dos estimadores originais.

> 💡 **Revisão Rápida:**
>
> Relembrando as transformações de variáveis, a matriz $G'$ conecta os parâmetros transformados com os parâmetros originais, tal que $\beta = G' \beta^*$, onde $\beta$ é o vetor com os parâmetros originais e $\beta^*$ é o vetor com os parâmetros transformados. O estimador OLS dos parâmetros originais, $b$, está relacionado com o estimador transformado, $b^*$, pela relação $b = G' b^*$. A matriz $G'$ e a sua inversa são dadas por:
>
> $$
> G' =
> \begin{bmatrix}
> 1 & 0 & \dots & 0 & 0 & 0 \\
> 0 & 1 & \dots & 0 & 0 & 0 \\
> \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
> 0 & 0 & \dots & 1 & 0 & 0 \\
> - \alpha + \delta & - \alpha + 2\delta & \dots & - \alpha + p\delta & 1 & 0 \\
> - \delta & - \delta & \dots & - \delta & 0 & 1
> \end{bmatrix}
> $$
>
> $$
> [G']^{-1} =
> \begin{bmatrix}
> 1 & 0 & \dots & 0 & 0 & 0 \\
> 0 & 1 & \dots & 0 & 0 & 0 \\
> \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
> 0 & 0 & \dots & 1 & 0 & 0 \\
> \alpha - \delta & \alpha - 2\delta & \dots & \alpha - p\delta & 1 & 0 \\
> \delta & \delta & \dots & \delta & 0 & 1
> \end{bmatrix}
> $$

A principal ideia é que, embora a transformação torne a distribuição assintótica dos estimadores mais simples, a matriz de transformação $G'$ nos permite realizar inferência sobre os parâmetros originais. As propriedades assintóticas derivadas para o estimador transformado $b^*$, combinadas com a matriz de transformação $G'$, nos permitem recuperar as propriedades assintóticas do estimador original $b$ e realizar testes de hipóteses válidos.

> 💡 **Exemplo Numérico:**
>
>  Considerando um modelo AR(1) com tendência, os parâmetros originais são $\beta = [\phi_1, \alpha, \delta]'$, e os parâmetros transformados são $\beta^* = [\phi_1^*, \alpha^*, \delta^*]'$. Os estimadores OLS associados são $b = [\hat{\phi_1}, \hat{\alpha}, \hat{\delta}]'$ e $b^* = [\hat{\phi_1^*}, \hat{\alpha^*}, \hat{\delta^*}]'$, respectivamente.
>  A relação entre esses estimadores é dada por $b = G' b^*$, ou seja:
>  $$
> \begin{bmatrix}
> \hat{\phi_1} \\
> \hat{\alpha} \\
> \hat{\delta}
> \end{bmatrix} =
> \begin{bmatrix}
> 1 & 0 & 0 \\
> - \alpha + \delta & 1 & 0 \\
> - \delta & 0 & 1
> \end{bmatrix}
> \begin{bmatrix}
> \hat{\phi_1^*} \\
> \hat{\alpha^*} \\
> \hat{\delta^*}
> \end{bmatrix}
> $$
>  Essa relação expressa como os estimadores originais podem ser recuperados a partir dos estimadores transformados. Observe que os elementos da matriz $G'$ dependem dos valores dos parâmetros originais.  Em uma situação prática, esses parâmetros são substituídos por estimativas. Em nosso contexto, como a matriz $G'$ não depende do tamanho amostral $T$, a substituição por estimativas consistentes não afeta as propriedades assintóticas.
>
> Suponha que tenhamos as seguintes estimativas obtidas após aplicar a transformação: $\hat{\phi_1^*} = 0.75$, $\hat{\alpha^*} = 1.8$, e $\hat{\delta^*} = 0.4$ e estimativas consistentes para $\hat{\alpha} = 2$ e $\hat{\delta} = 0.5$. Então a matriz $G'$ é estimada como:
> $$
> \hat{G}' =
> \begin{bmatrix}
> 1 & 0 & 0 \\
> -2 + 0.5 & 1 & 0 \\
> -0.5 & 0 & 1
> \end{bmatrix}
> =
> \begin{bmatrix}
> 1 & 0 & 0 \\
> -1.5 & 1 & 0 \\
> -0.5 & 0 & 1
> \end{bmatrix}
> $$
> Com isso, os estimadores originais são recuperados da seguinte forma:
> $$
> b = \hat{G}' b^* =
> \begin{bmatrix}
> 1 & 0 & 0 \\
> -1.5 & 1 & 0 \\
> -0.5 & 0 & 1
> \end{bmatrix}
> \begin{bmatrix}
> 0.75 \\
> 1.8 \\
> 0.4
> \end{bmatrix}
> =
> \begin{bmatrix}
> 0.75 \\
> -1.5 * 0.75 + 1.8 \\
> -0.5 * 0.75 + 0.4
> \end{bmatrix}
> =
> \begin{bmatrix}
> 0.75 \\
> 0.675 \\
> 0.025
> \end{bmatrix}
> $$
>  Logo,  $\hat{\phi_1} = 0.75$,  $\hat{\alpha} = 0.675$ e $\hat{\delta} = 0.025$. Observe que os estimadores transformados podem ser utilizados para obter os estimadores originais.

Para construir testes de hipóteses, recorremos à forma geral da hipótese nula:
$$
H_0: R\beta = r
$$
onde $R$ é uma matriz de restrições (de dimensão $m \times (p+2)$), $r$ é um vetor de constantes (de dimensão $m \times 1$), e $m$ é o número de restrições.  O teste Wald para essa hipótese pode ser expresso como:
$$
\chi^2 = (Rb - r)' [s^2 R(X'X)^{-1} R']^{-1} (Rb - r)
$$
onde $s^2$ é o estimador da variância do erro. O objetivo agora é mostrar como a transformação dos regressores nos permite analisar a distribuição assintótica desta estatística de teste.

**Teorema 3.1**
A estatística de teste Wald para a hipótese nula $H_0: R\beta = r$ pode ser reescrita em termos dos parâmetros e regressores transformados, e a estatística assim construída converge assintoticamente para uma distribuição Qui-Quadrado com $m$ graus de liberdade, onde $m$ é o número de restrições.

*Prova:*
I. O teste Wald para a hipótese nula $H_0: R\beta = r$ é dado por
$$
\chi^2 = (Rb - r)' [s^2 R(X'X)^{-1} R']^{-1} (Rb - r)
$$
II. Substituindo $b = G'b^*$ e $\beta = G'\beta^*$, temos que $Rb = RG'b^*$ e $R\beta = RG'\beta^*$.
III. Definindo $R^* = RG'$, podemos reescrever a hipótese nula como $R^*\beta^* = r$, e o teste Wald torna-se:
$$
\chi^2 = (R^*b^* - r)' [s^2 R^*(X^{*'}X^*)^{-1} R^{*'}]^{-1} (R^*b^* - r)
$$
IV.  A distribuição assintótica da estatística de teste baseia-se na distribuição assintótica de $b^*$, já estabelecida no capítulo anterior, e no lema de Slutsky.  Usando o Teorema 2.1, $Y_T(b^*-\beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$ e usando o lema de Slutsky, obtemos que a estatística acima converge em distribuição para uma Qui-Quadrado com $m$ graus de liberdade:
$$
\chi^2 \xrightarrow{d} \chi^2(m)
$$
■

> 💡 **Exemplo Numérico:**
>
>  Considerando um modelo AR(1) com tendência linear e a hipótese nula $H_0 : \delta = 0$, podemos expressar essa restrição matricialmente usando: $R = [0, 0, 1]$ e $r = 0$. O teste de hipótese será realizado através da seguinte estatística:
>  $$
> \chi^2 = (\hat{\delta} - 0)' [s^2 [0, 0, 1](X'X)^{-1} [0, 0, 1]']^{-1} (\hat{\delta} - 0)
> $$
>  Usando as propriedades assintóticas, e a relação $b=G'b^*$, podemos reescrever o teste como:
> $$
> \chi^2 = (\hat{\delta}^* (1+\hat{\phi_1}) - 0)' [s^2[0, 0, 1] (X^{*'}X^*)^{-1} [0, 0, 1]']^{-1} (\hat{\delta}^* (1+\hat{\phi_1}) - 0)
> $$
>  A estatística de teste, ao ser avaliada numericamente, produzirá o mesmo resultado tanto utilizando o modelo transformado quanto o original, como podemos verificar a seguir:
>
>   Vamos supor que os dados tenham sido estimados e as seguintes estimativas tenham sido obtidas:  $\hat{\phi_1} = 0.70$, $\hat{\alpha} = 2.1$, $\hat{\delta} = 0.03$, $s^2 = 1.2$  e a matriz $(X'X)^{-1}$ é:
>
> $$
> (X'X)^{-1} =
> \begin{bmatrix}
>  0.02 & -0.01 & 0.001 \\
>  -0.01 & 0.02 & -0.005\\
>  0.001 & -0.005 & 0.001
> \end{bmatrix}
> $$
>
>  O teste da hipótese nula $H_0 : \delta = 0$ é dado por:
>  $$
>  \chi^2 = (0.03)' [1.2  [0, 0, 1]  \begin{bmatrix}
>  0.02 & -0.01 & 0.001 \\
>  -0.01 & 0.02 & -0.005\\
>  0.001 & -0.005 & 0.001
> \end{bmatrix}  [0, 0, 1]']^{-1} (0.03) = (0.03)'  [1.2 (0.001)]^{-1} (0.03) \approx 7.5
>  $$
>  Sob a hipótese nula, a estatística de teste converge para uma distribuição Qui-Quadrado com 1 grau de liberdade.
>
>   Usando o modelo transformado, obtemos as estimativas  $\hat{\phi_1^*} = 0.70$, $\hat{\alpha^*} = 2.1 - 0.03 = 2.07$ e $\hat{\delta^*} = 0.03/(1+0.70) = 0.0176$
>
>   Calculando a matriz $(X^{*'}X^*)^{-1}$, obtemos:
>  $$
> (X^{*'}X^*)^{-1} =
> \begin{bmatrix}
>  0.02 & -0.01 & 0 \\
>  -0.01 & 0.02 & 0\\
>  0 & 0 & 0.0005
> \end{bmatrix}
> $$
>
>   A estatística de teste é agora calculada usando o estimador transformado:
>  $$
>  \chi^2 =  (0.0176 \times (1+0.7))'[1.2 [0, 0, 1]  \begin{bmatrix}
>  0.02 & -0.01 & 0 \\
>  -0.01 & 0.02 & 0\\
>  0 & 0 & 0.0005
> \end{bmatrix}  [0, 0, 1]']^{-1} (0.0176 \times (1+0.7)) \approx 7.5
>  $$
>  Como esperado, o valor da estatística de teste obtido com o modelo transformado é idêntico ao valor obtido com o modelo original.  Portanto, o teste de hipóteses permanece válido mesmo quando aplicado após as transformações. Este exemplo ilustra como a transformação dos regressores preserva as propriedades assintóticas do teste.

**Observação 1**
Este teorema assegura que os testes de hipóteses construídos usando as propriedades assintóticas dos estimadores transformados são assintoticamente válidos. Além disso, a estatística de teste é numericamente idêntica tanto no modelo original quanto no transformado, mesmo que as distribuições assintóticas dos estimadores sejam mais fáceis de analisar após a transformação.

**Lema 3.1**
Em modelos que envolvem parâmetros com diferentes taxas de convergência (como os modelos com tendência de tempo), os testes de hipóteses são dominados assintoticamente pelos parâmetros com as taxas de convergência mais lentas. Em outras palavras, quando uma hipótese nula envolve parâmetros que convergem para seus valores verdadeiros em diferentes taxas, o resultado do teste será mais sensível ao parâmetro que converge mais lentamente.

*Prova:*
I.  Considere uma hipótese que envolve parâmetros que convergem à taxa $\sqrt{T}$ e outro à taxa $T^{3/2}$.  A estatística de teste envolve os desvios desses parâmetros de seus valores sob a hipótese nula, escalonados por suas variâncias estimadas.
II. Como o parâmetro que converge à taxa $T^{3/2}$ se aproxima do seu valor verdadeiro muito mais rapidamente do que aquele que converge à taxa $\sqrt{T}$, a sua contribuição na estatística de teste se torna assintoticamente desprezível, pois o termo que envolve o erro da estimativa de $T^{3/2}$ converge para zero mais rapidamente.
III. Portanto, a estatística de teste acaba sendo dominada pelo parâmetro que converge mais lentamente, e o resultado do teste é essencialmente o mesmo que seria obtido se o parâmetro que converge mais rapidamente fosse conhecido.
■

> 💡 **Exemplo Numérico:**
>
> Considere um teste de hipótese que envolva tanto o parâmetro da tendência $\delta$ quanto o parâmetro autorregressivo $\phi_1$. Pelo lema 3.1, o teste será dominado pelo parâmetro que converge mais lentamente, que é  $\phi_1$. Isto ocorre porque  $\delta$ converge muito mais rapidamente para o seu valor verdadeiro do que $\phi_1$,  a sua variação assintótica é menor e, portanto, a sua contribuição para o teste será muito menor no limite.
>
> Vamos supor que estejamos testando a hipótese nula $H_0: \phi_1 = 0$ e $\delta = 0$, onde o parâmetro $\phi_1$ é um parâmetro autorregressivo do modelo AR(1) com tendência e o parâmetro $\delta$ é o coeficiente associado à tendência linear. Como o estimador de  $\delta$ converge mais rápido, o teste será essencialmente o mesmo que seria obtido se $\delta$ fosse conhecido. Em outras palavras, o teste será dominado pela incerteza na estimativa de  $\phi_1$, e será quase idêntico ao teste que apenas avalia a hipótese nula  $H_0: \phi_1 = 0$. A informação adicional sobre a tendência $\delta$ praticamente não afeta a inferência sobre $\phi_1$.
>
> Esse resultado se mostra útil em aplicações práticas porque simplifica a construção dos testes, pois em muitos casos não é necessário levar em conta todas as diferentes taxas de convergência explicitamente.

**Observação 2**
O lema 3.1 é fundamental para simplificar a análise de testes de hipóteses em modelos com diferentes taxas de convergência. Ele nos diz que, para fins de inferência, os testes são essencialmente determinados pelos parâmetros que convergem mais lentamente, e, portanto, em modelos com componentes estacionários e não-estacionários, as inferências assintóticas podem ser simplificadas.

**Corolário 3.1**
O teste usual de OLS para uma restrição linear sobre os parâmetros, tanto no modelo original quanto no transformado, será assintoticamente válido mesmo na presença de diferentes taxas de convergência. Isto implica que os testes padrão, como o teste t e o teste F, podem ser usados mesmo em modelos com tendência de tempo determinística, desde que se usem os erros padrão apropriados, e que a estatística de teste seja construída usando a forma geral da hipótese $H_0: R\beta = r$.

*Prova:*
I.  A validade assintótica do teste OLS segue da aplicação do Teorema 3.1, que demonstra que a estatística de teste converge para uma distribuição Qui-Quadrado sob a hipótese nula.
II.  As diferentes taxas de convergência são automaticamente acomodadas pelas transformações que relacionam os parâmetros originais aos parâmetros transformados.
III.  O lema 3.1 reforça a ideia de que os testes de hipóteses são dominados assintoticamente pelos parâmetros que convergem mais lentamente, garantindo assim a validade do teste mesmo em modelos com diferentes taxas de convergência.
IV.  A escolha da matriz de transformação $G$ não afeta a validade assintótica do teste de hipóteses, apenas a precisão dos estimadores.
■

> 💡 **Exemplo Numérico:**
>
>  Vamos considerar o mesmo exemplo do teste  $H_0: \delta = 0$ em um modelo AR(1) com tendência linear. O Corolário 3.1 afirma que podemos usar o teste t padrão (ou F) para avaliar esta hipótese nula e que o teste será válido assintoticamente, tanto no modelo original quanto no transformado.
>
>  Suponha que, após a estimação, obtivemos o estimador OLS para $\delta$ como sendo  $\hat{\delta} = 0.03$, e um erro padrão estimado de  $0.01$, usando a estimativa da matriz de variância-covariância assintótica, $\hat{V}$. O teste t para a hipótese $H_0: \delta = 0$ é dado por:
> $$
> t = \frac{\hat{\delta}}{se(\hat{\delta})} = \frac{0.03}{0.01} = 3
> $$
>  Sob a hipótese nula, esta estatística convergirá para uma distribuição normal padrão. Se usarmos um nível de significância de 5%, podemos rejeitar a hipótese nula se o valor absoluto da estatística t for maior do que 1.96. No nosso caso, como $3 > 1.96$, rejeitamos a hipótese nula que o coeficiente da tendência linear é zero, e concluímos que há evidência de uma tendência na série.
>
>  É importante notar que este resultado se mantém mesmo que as distribuições dos estimadores originais e transformados sejam diferentes. A transformação apenas nos permite obter a matriz de covariância assintótica dos estimadores. No entanto, o corolário 3.1 nos garante que o teste t usual, construído usando as estimativas obtidas no modelo original, é válido assintoticamente.

**Observação 3**
Os resultados obtidos neste capítulo reforçam a ideia de que, embora a transformação dos regressores simplifique a análise das distribuições assintóticas e taxas de convergência, as propriedades assintóticas dos testes de hipóteses usuais são preservadas. Em particular, o corolário 3.1 demonstra que os testes padrão, como o teste t, são assintoticamente válidos em modelos com tendências determinísticas, o que justifica o uso de ferramentas inferenciais padrão mesmo quando estamos trabalhando com dados não estacionários.

**Teorema 3.2**
Sob condições de regularidade, o teste da razão de verossimilhança (LR) para a hipótese nula $H_0 : R\beta = r$ converge assintoticamente para uma distribuição Qui-Quadrado com $m$ graus de liberdade, onde $m$ é o número de restrições, e a estatística do teste LR é assintoticamente equivalente à estatística do teste Wald.

*Prova:*
I.  O teste LR é definido como a diferença entre o valor máximo da função de log-verossimilhança sob a hipótese nula e o valor máximo sem restrições, multiplicado por -2.
II.  Sob condições de regularidade e usando a expansão de Taylor, pode-se mostrar que a estatística LR é assintoticamente equivalente à estatística de teste de Wald. Isto significa que a diferença entre as duas estatísticas converge para zero quando o tamanho amostral aumenta.
III. Uma vez que a estatística do teste de Wald converge para uma distribuição Qui-Quadrado com m graus de liberdade, a estatística do teste LR também converge para a mesma distribuição.
IV.   Essa equivalência assintótica garante que tanto o teste LR como o teste Wald podem ser usados em testes de hipóteses sobre os parâmetros do modelo.
■

**Lema 3.2**
Se a hipótese nula envolver apenas os parâmetros que convergem à taxa $\sqrt{T}$, a estatística de teste, seja ela Wald ou LR, será assintoticamente idêntica à estatística obtida se o parâmetro que converge mais rapidamente (à taxa $T^{3/2}$) fosse conhecido.

*Prova:*
I. Este lema estende o Lema 3.1, mostrando que se a restrição de hipótese envolver apenas parâmetros que convergem à taxa $\sqrt{T}$, o parâmetro que converge mais rápido, como a tendência, não afeta assintoticamente a distribuição do teste.
II.  A estatística do teste envolve o desvio dos estimadores em relação aos valores nulos, escalonado pelas suas variâncias estimadas. Como os estimadores de $\delta$ convergem muito mais rápido, a sua variância será desprezível no limite, relativamente à variância dos parâmetros que convergem mais lentamente.
III. Assim, o teste se comporta assintoticamente como se o parâmetro que converge rapidamente fosse conhecido, simplificando a análise do teste.
■

> 💡 **Exemplo Numérico:**
>
> Suponha que estamos testando a hipótese nula $H_0: \phi_1 = 0$ em um modelo AR(1) com tendência linear. Pelo Lema 3.2, a estatística do teste de hipótese será assintoticamente a mesma que seria obtida se o parâmetro da tendência linear, $\delta$, fosse conhecido. Isso ocorre porque o estimador de $\delta$ converge muito mais rapidamente do que o estimador de $\phi_1$. Portanto, a incerteza sobre $\delta$ se torna desprezível no limite e não afeta o resultado do teste sobre $\phi_1$.
>  Assim, a estatística de teste para $H_0: \phi_1 = 0$ pode ser construída considerando $\delta$ como conhecido, simplificando a inferência.  Este resultado é importante porque demonstra que podemos focar em realizar inferências sobre os parâmetros de interesse sem nos preocuparmos com as diferentes taxas de convergência.

**Corolário 3.2**
Em modelos com componentes não estacionários, mas nos quais se aplica a transformação de Sims, Stock e Watson (1990), as conclusões obtidas sobre os parâmetros que convergem à taxa $\sqrt{T}$ serão assintoticamente idênticas às que seriam obtidas em um modelo sem tendência, desde que se utilize o estimador transformado e a transformação seja feita corretamente.

*Prova:*
I.  A transformação de Sims, Stock e Watson (1990) remove a tendência determinística, resultando em parâmetros transformados que têm distribuições assintóticas mais simples.
II.  Pelo lema 3.2, os testes sobre os parâmetros que convergem à taxa $\sqrt{T}$ (como os parâmetros autorregressivos) são dominados pela incerteza associada a esses parâmetros.
III.  Como a transformação remove a não-estacionariedade causada pela tendência, a distribuição assintótica dos estimadores transformados associados aos parâmetros estacionários será a mesma que se obteria em um modelo sem tendência.
IV.  Portanto, os testes de hipóteses sobre os parâmetros autorregressivos serão assintoticamente idênticos aos que se obteriam se não houvesse tendência, uma vez que os efeitos da tendência são eliminados pela transformação.
■

> 💡 **Exemplo Numérico:**
>
>  Considerando um modelo AR(1) com tendência linear e o teste da hipótese nula $H_0: \phi_1 = 0$. Pelo Corolário 3.2, o teste sobre o parâmetro $\phi_1$ será assintoticamente idêntico ao teste que seria obtido em um modelo AR(1) sem tendência, após a transformação de Sims, Stock e Watson (1990). Isto significa que, para fins de inferência sobre $\phi_1$, podemos essencialmente ignorar a presença da tendência determinística, pois o estimador transformado já incorpora a correção necessária para a presença da tendência.
>
> Este resultado simplifica a análise de dados com tendência, pois podemos usar os métodos de inferência já conhecidos para modelos estacionários, desde que seja feita a transformação adequada dos regressores.  Essa simplificação é crucial para a aplicação prática dos modelos em séries temporais.

**Observação 4**
Os resultados adicionais aqui apresentados (Teorema 3.2, Lema 3.2 e Corolário 3.2) fornecem uma visão mais aprofundada sobre a aplicação de testes de hipóteses em modelos com tendência determinística. Eles mostram que o teste da razão de verossimilhança é assintoticamente equivalente ao teste Wald e que, em testes de parâmetros que convergem mais lentamente, os parâmetros que convergem mais rapidamente não exercem influência assintótica. Além disso, mostramos que a transformação de Sims, Stock e Watson (1990) permite a aplicação direta de métodos inferenciais para modelos estacionários sobre os parâmetros transformados, simplificando a análise estatística.

### Conclusão

Esta seção detalhou a construção e análise de testes de hipóteses em modelos autorregressivos com tendências determinísticas. Demonstramos que a transformação de Sims, Stock e Watson (1990), embora inicialmente utilizada para simplificar a análise das distribuições assintóticas, não interfere na validade assintótica dos testes de hipóteses sobre os parâmetros originais. Vimos que a estatística de teste Wald pode ser reescrita tanto nos parâmetros transformados quanto nos originais, mantendo a mesma distribuição limite. Além disso, demonstramos que os testes de hipóteses são dominados assintoticamente pelos parâmetros que convergem mais lentamente. Finalmente, mostramos que os testes t e F usuais podem ser aplicados em modelos com tendências determinísticas, desde que se utilizem os erros padrão apropriados. Em resumo, esta seção estabeleceu uma base sólida para a inferência estatística em modelos autorregressivos com tendências determinísticas, utilizando as propriedades assintóticas obtidas pela transformação dos regressores. Os resultados aqui apresentados complementam as seções anteriores e mostram como usar os conceitos discutidos para realizar inferências estatísticas válidas e construir testes de hipóteses em modelos mais complexos [^1].

### Referências
[^1]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113-44.
[^2]:  Fuller, Wayne A. 1976. *Introduction to Statistical Time Series*. New York: Wiley.
<!-- END -->
