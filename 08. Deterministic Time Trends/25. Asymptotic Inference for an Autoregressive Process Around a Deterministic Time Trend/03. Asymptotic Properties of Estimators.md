## Asymptotic Inference for an Autoregressive Process Around a Deterministic Time Trend: Limiting Distributions and Convergence Rates

### IntroduÃ§Ã£o

Em continuidade Ã  discussÃ£o sobre a transformaÃ§Ã£o de regressores em modelos autorregressivos com tendÃªncias determinÃ­sticas, este capÃ­tulo aprofunda a anÃ¡lise das propriedades assintÃ³ticas dos estimadores obtidos apÃ³s a aplicaÃ§Ã£o da tÃ©cnica de Sims, Stock e Watson (1990) [^1]. Conforme visto anteriormente, a transformaÃ§Ã£o dos regressores em um termo constante, uma tendÃªncia de tempo e componentes estacionÃ¡rios de mÃ©dia zero, desempenha um papel crucial na anÃ¡lise das distribuiÃ§Ãµes limites e taxas de convergÃªncia dos estimadores de mÃ­nimos quadrados ordinÃ¡rios (OLS). O objetivo principal desta seÃ§Ã£o Ã© demonstrar como a transformaÃ§Ã£o facilita a obtenÃ§Ã£o de distribuiÃ§Ãµes assintÃ³ticas mais simples e como as diferentes taxas de convergÃªncia dos estimadores influenciam a inferÃªncia estatÃ­stica. Expandindo os conceitos abordados, iremos focar na anÃ¡lise das propriedades assintÃ³ticas dos estimadores resultantes da transformaÃ§Ã£o, em particular nas diferentes taxas de convergÃªncia dos coeficientes associados aos componentes estacionÃ¡rios e Ã  tendÃªncia de tempo, e na sua independÃªncia assintÃ³tica.

### Conceitos Fundamentais

Conforme estabelecido na seÃ§Ã£o anterior, a transformaÃ§Ã£o dos regressores resulta no modelo:
$$
y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \dots + \phi_p^* y_{t-p}^* + \epsilon_t
$$
onde:
$$
\begin{aligned}
\alpha^* &= [\alpha(1 + \phi_1 + \phi_2 + \dots + \phi_p) - \delta(\phi_1 + 2\phi_2 + \dots + p\phi_p)] \\
\delta^* &= \delta(1 + \phi_1 + \phi_2 + \dots + \phi_p) \\
\phi_j^* &= \phi_j \\
y_{t-j}^* &= y_{t-j} - \alpha - \delta(t-j) \quad \text{para } j = 1, 2, \ldots, p
\end{aligned}
$$
[^1]. O estimador OLS para esse modelo Ã© denotado por $b^*$, que corresponde Ã  estimativa dos coeficientes transformados [^1]. A relaÃ§Ã£o entre os estimadores transformados $b^*$ e os estimadores originais $b$ Ã© dada por $b = G' b^*$, onde $G'$ Ã© a matriz de transformaÃ§Ã£o detalhada na seÃ§Ã£o anterior [^1].

> ğŸ’¡ **RevisÃ£o RÃ¡pida:**
>
>  Recordando do capÃ­tulo anterior, os regressores originais sÃ£o transformados para isolar os componentes estocÃ¡sticos e determinÃ­sticos, permitindo uma anÃ¡lise separada de suas propriedades. A transformaÃ§Ã£o envolve uma combinaÃ§Ã£o linear dos regressores originais, expressa pela matriz $G'$. Os coeficientes resultantes da transformaÃ§Ã£o sÃ£o designados por $\alpha^*$, $\delta^*$ e $\phi_j^*$, representando os efeitos do componente constante, da tendÃªncia de tempo e das variÃ¡veis defasadas, respectivamente. O ponto chave Ã© que o termo $y_{t-j}^*$ Ã© estacionÃ¡rio (quando $y_t$ Ã© estacionÃ¡rio em torno de uma tendÃªncia linear), enquanto o termo $t$ representa uma tendÃªncia linear.

O objetivo agora Ã© analisar as propriedades assintÃ³ticas do estimador transformado $b^*$. Uma propriedade crucial da transformaÃ§Ã£o Ã© que ela simplifica a anÃ¡lise da distribuiÃ§Ã£o limite dos estimadores OLS. Os coeficientes associados Ã s variÃ¡veis estacionÃ¡rias de mÃ©dia zero ($ \phi_1^*, \phi_2^*, \dots, \phi_p^*$) convergem para uma distribuiÃ§Ã£o gaussiana Ã  taxa de $\sqrt{T}$, enquanto o coeficiente da tendÃªncia de tempo ($\delta^*$) converge mais rapidamente, Ã  taxa de $T^{3/2}$ [^1]. Esta diferenÃ§a nas taxas de convergÃªncia Ã© uma consequÃªncia direta da presenÃ§a da tendÃªncia de tempo e resulta em diferentes ordens de grandeza para as variÃ¢ncias assintÃ³ticas dos estimadores. A matriz de escalonamento $Y_T$ Ã© crucial para "normalizar" essas taxas de convergÃªncia:
$$
Y_T =
\begin{bmatrix}
\sqrt{T} & 0 & \dots & 0 & 0 & 0 \\
0 & \sqrt{T} & \dots & 0 & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & \dots & \sqrt{T} & 0 & 0 \\
0 & 0 & \dots & 0 & \sqrt{T} & 0 \\
0 & 0 & \dots & 0 & 0 & T^{3/2}
\end{bmatrix}
$$
[^1].

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Para um modelo AR(1) com tendÃªncia linear, a matriz de escalonamento $Y_T$ seria:
>
> $$
> Y_T =
> \begin{bmatrix}
> \sqrt{T} & 0 & 0 \\
> 0 & \sqrt{T} & 0 \\
> 0 & 0 & T^{3/2}
> \end{bmatrix}
> $$
>
> Isso significa que o estimador do coeficiente AR(1) ($\phi_1^*$) e o intercepto ($\alpha^*$) convergem para seus verdadeiros valores Ã  uma taxa de $\sqrt{T}$, enquanto o estimador do coeficiente da tendÃªncia linear ($\delta^*$) converge Ã  uma taxa muito mais rÃ¡pida, de $T^{3/2}$. A diferenÃ§a nas taxas de convergÃªncia implica que o estimador $\delta^*$ Ã© mais preciso em amostras grandes em comparaÃ§Ã£o com $\phi_1^*$ e $\alpha^*$.
>
> Suponha que tenhamos uma sÃ©rie temporal com $T=100$. Nesse caso, $\sqrt{T} = 10$ e $T^{3/2} = 1000$. Para o mesmo desvio padrÃ£o, o erro padrÃ£o de  $\hat{\delta^*}$ Ã© cerca de 100 vezes menor que os erros padrÃ£o de $\hat{\phi_1^*}$ e $\hat{\alpha^*}$. Isso demonstra numericamente como a taxa de convergÃªncia de $T^{3/2}$ resulta em estimativas mais precisas para o coeficiente de tendÃªncia do que a taxa de convergÃªncia de $\sqrt{T}$ para os outros coeficientes.

A distribuiÃ§Ã£o assintÃ³tica do estimador transformado $b^*$ Ã© expressa como:
$$
Y_T (b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})
$$
onde $Q^*$ Ã© a matriz limite de $\frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'}$ [^1]. Essa expressÃ£o formaliza a ideia de que, apÃ³s o reescalonamento adequado, os estimadores convergem para uma distribuiÃ§Ã£o normal, com uma matriz de covariÃ¢ncia especÃ­fica.

**Teorema 2.1**
Sob as condiÃ§Ãµes do modelo, a distribuiÃ§Ã£o assintÃ³tica do estimador transformado Ã© dada por
$$
Y_T (b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})
$$
onde:
  - $Y_T$ Ã© a matriz de escalonamento, como definida anteriormente.
  - $b^*$ Ã© o estimador OLS dos parÃ¢metros transformados.
  - $\beta^*$ Ã© o vetor dos parÃ¢metros verdadeiros transformados.
  - $\sigma^2$ Ã© a variÃ¢ncia do termo de erro.
  - $Q^*$ Ã© o limite da matriz de covariÃ¢ncia amostral dos regressores transformados, isto Ã©,
$$
 Q^* = \lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'}
$$
e tem a forma
$$
Q^*=
\begin{bmatrix}
 \gamma_0^* & \gamma_1^* & \dots & \gamma_{p-1}^* & 0 & 0 \\
 \gamma_1^* & \gamma_0^* & \dots & \gamma_{p-2}^* & 0 & 0 \\
 \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
 \gamma_{p-1}^* & \gamma_{p-2}^* & \dots & \gamma_0^* & 0 & 0 \\
 0 & 0 & \dots & 0 & 1 & 0 \\
 0 & 0 & \dots & 0 & 0 & 1/3
\end{bmatrix}
$$
onde $\gamma_j^* = E(y_t^* y_{t-j}^*)$.

*Prova (EsboÃ§o):*
I.  A demonstraÃ§Ã£o se baseia em resultados assintÃ³ticos para processos estocÃ¡sticos.  Primeiro, verifica-se que  $\frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'} \xrightarrow{p} Q^*$, usando resultados para processos estacionÃ¡rios.
II.  Em seguida, verifica-se que  $\frac{1}{\sqrt{T}} \sum_{t=1}^T x_t^* \epsilon_t$ converge em distribuiÃ§Ã£o para uma normal multivariada com mÃ©dia zero e matriz de covariÃ¢ncia $\sigma^2Q^*$, usando o Teorema do Limite Central para Martingales (Proposition 7.8).
III. A distribuiÃ§Ã£o assintÃ³tica de $Y_T(b^* - \beta^*)$ Ã© entÃ£o obtida combinando os dois resultados anteriores e aplicando o lema de Slutsky.
â– 

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
>  Vamos ilustrar o Teorema 2.1 com o exemplo numÃ©rico de um modelo AR(1) com tendÃªncia.  Considere os estimadores $b^* = [\hat{\phi_1^*}, \hat{\alpha^*}, \hat{\delta^*}]$ e o vetor de parÃ¢metros verdadeiro  $\beta^* = [\phi_1^*, \alpha^*, \delta^*]$. O Teorema 2.1 afirma que
>
> $$
> \begin{bmatrix}
> \sqrt{T}(\hat{\phi_1^*} - \phi_1^*) \\
> \sqrt{T}(\hat{\alpha^*} - \alpha^*) \\
> T^{3/2}(\hat{\delta^*} - \delta^*)
> \end{bmatrix}
> \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})
> $$
>
> onde $\sigma^2$ Ã© a variÃ¢ncia do termo de erro e $[Q^*]^{-1}$ Ã© a matriz inversa de $Q^*$.   Note que os coeficientes  $\hat{\phi_1^*}$ e  $\hat{\alpha^*}$ convergem com velocidade  $\sqrt{T}$, enquanto o coeficiente  $\hat{\delta^*}$ converge com velocidade $T^{3/2}$. A forma especÃ­fica de $Q^*$ depende das propriedades de $y_t^*$.
>
> Suponha que simulamos uma sÃ©rie temporal $y_t$ de tamanho $T=1000$ usando um modelo AR(1) com tendÃªncia, onde $\phi_1 = 0.7$, $\alpha = 2$, $\delta = 0.5$, e a variÃ¢ncia do erro $\sigma^2 = 1$. ApÃ³s a transformaÃ§Ã£o, obtemos $b^* = [\hat{\phi_1^*}, \hat{\alpha^*}, \hat{\delta^*}]$. Estimamos $Q^*$ usando os dados e obtivemos a seguinte matriz:
>
> $$
> \hat{Q}^*=
> \begin{bmatrix}
>  1.96 & 1.37 & 0 \\
>  1.37 & 1.96 & 0 \\
>  0 & 0 & 1 & 0 \\
>  0 & 0 & 0 & 0.33
> \end{bmatrix}
> $$
>  Calculando a inversa de $\hat{Q}^*$ e multiplicando por $\sigma^2 = 1$, obtemos:
>
> $$
> \sigma^2 [\hat{Q}^*]^{-1} =
> \begin{bmatrix}
>  1.53 & -1.07 & 0 & 0\\
>  -1.07 & 1.53 & 0 & 0\\
>  0 & 0 & 1 & 0 \\
>  0 & 0 & 0 & 3
> \end{bmatrix}
> $$
>
>  Esta matriz Ã© a variÃ¢ncia assintÃ³tica dos estimadores reescalonados. Observe que a variÃ¢ncia do estimador de $\delta^*$ (Ãºltima entrada diagonal) Ã© muito menor do que as variÃ¢ncias dos estimadores de $\phi_1^*$ e $\alpha^*$, o que reflete a taxa de convergÃªncia mais rÃ¡pida. AlÃ©m disso, as covariÃ¢ncias entre os estimadores de $\phi_1^*$ e $\alpha^*$ sÃ£o nÃ£o-nulas, o que indica que eles estÃ£o correlacionados, enquanto todos os outros elementos fora da diagonal sÃ£o 0, indicando independÃªncia assintÃ³tica entre o estimador da tendÃªncia e os outros coeficientes.

**ObservaÃ§Ã£o 2**
Este teorema estabelece que, apÃ³s o escalonamento apropriado, o estimador transformado $b^*$ tem uma distribuiÃ§Ã£o normal assintÃ³tica, o que Ã© essencial para inferÃªncia estatÃ­stica. AlÃ©m disso, as diferentes taxas de convergÃªncia dos coeficientes, em $\sqrt{T}$ para os componentes estacionÃ¡rios e $T^{3/2}$ para a tendÃªncia, sÃ£o explicitamente capturadas pela matriz $Y_T$.

**Teorema 2.2**
Os coeficientes estimados apÃ³s a transformaÃ§Ã£o, ou seja os coeficientes que afetam os termos estacionÃ¡rios $\phi_j^*$, e o coeficiente da tendÃªncia $\delta^*$, sÃ£o assintoticamente independentes.

*Prova (EsboÃ§o):*
I.  A independÃªncia assintÃ³tica dos estimadores  $\phi_j^*$ e $\delta^*$ Ã© uma consequÃªncia da transformaÃ§Ã£o de Sims, Stock e Watson e das propriedades assintÃ³ticas do estimador OLS. Como os termos estocÃ¡sticos na matriz $Q^*$, referentes Ã s variÃ¡veis defasadas ($y_{t-1}^*, y_{t-2}^*, \ldots, y_{t-p}^*$),  nÃ£o sÃ£o correlacionados com a tendÃªncia $t$ no limite, os estimadores OLS dos coeficientes correspondentes tendem a ser assintoticamente independentes. A forma da matriz $Q^*$ explicita o resultado.
II. A independÃªncia assintÃ³tica Ã© ainda mais reforÃ§ada pelo fato de que os termos estocÃ¡sticos convergem Ã  taxa $\sqrt{T}$ e a tendÃªncia converge Ã  taxa $T^{3/2}$, levando a um efeito de independÃªncia entre as distribuiÃ§Ãµes assintÃ³ticas dos seus estimadores.
â– 

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> No contexto do nosso modelo AR(1) com tendÃªncia, o Teorema 2.2 implica que $\hat{\phi_1^*}$ Ã© assintoticamente independente de $\hat{\delta^*}$. Isso significa que a variabilidade assintÃ³tica na estimativa do coeficiente AR nÃ£o Ã© influenciada pela variabilidade assintÃ³tica na estimativa do coeficiente da tendÃªncia. A consequÃªncia prÃ¡tica Ã© que inferÃªncias sobre esses coeficientes podem ser feitas de forma separada.
>
> Por exemplo, considere que em um estudo empÃ­rico, apÃ³s a aplicaÃ§Ã£o da transformaÃ§Ã£o e estimaÃ§Ã£o, obtemos $\hat{\phi_1^*} = 0.75$ com um erro padrÃ£o de 0.05 e $\hat{\delta^*} = 0.6$ com um erro padrÃ£o de 0.002. De acordo com o Teorema 2.2, podemos construir intervalos de confianÃ§a para esses estimadores de forma independente. Um intervalo de confianÃ§a de 95% para $\phi_1^*$ pode ser calculado como $0.75 \pm 1.96 * 0.05$, enquanto o intervalo de confianÃ§a de 95% para $\delta^*$ pode ser calculado como $0.6 \pm 1.96 * 0.002$. A independÃªncia assintÃ³tica nos permite usar essas estimativas de variÃ¢ncia independentemente para inferÃªncia sobre cada parÃ¢metro. AlÃ©m disso, um teste de hipÃ³teses sobre $\phi_1^*$, como $H_0: \phi_1^* = 0$, pode ser conduzido sem considerar a precisÃ£o da estimativa de $\delta^*$, e vice-versa.

**CorolÃ¡rio 2.1**
A distribuiÃ§Ã£o assintÃ³tica dos estimadores dos coeficientes originais  $\phi_j, \alpha, \delta$  pode ser obtida aplicando a matriz de transformaÃ§Ã£o $G'$ aos resultados do teorema 2.1. Ou seja:

$$
Y_T(b - \beta) \xrightarrow{d} N(0, \sigma^2 G'[Q^*]^{-1}G)
$$
onde $\beta$ Ã© o vetor com os parÃ¢metros originais e $b$ Ã© o vetor com os estimadores dos parÃ¢metros originais.

*Prova:*
I.  Sabemos que $b = G'b^*$.
II. Usando o teorema 2.1, temos $Y_T (b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$.
III.  Aplicando a transformaÃ§Ã£o $b = G'b^*$ segue-se que $Y_T(b - \beta) = Y_T(G'b^* - G'\beta^*) = G'Y_T (b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 G'[Q^*]^{-1}G)$.
â– 

**Lema 2.1**
A matriz $Q^*$ Ã© positiva definida.

*Prova (EsboÃ§o):*
I.  A matriz $Q^*$ pode ser decomposta em blocos, onde o bloco superior esquerdo corresponde Ã  matriz de autocovariÃ¢ncias do processo estacionÃ¡rio $y_t^*$, o bloco inferior direito Ã© uma matriz diagonal com elementos positivos (1 e 1/3), e os blocos fora da diagonal sÃ£o nulos.
II. A matriz de autocovariÃ¢ncias de um processo estacionÃ¡rio Ã© sempre positiva semidefinida.  Sob certas condiÃ§Ãµes, ela Ã© positiva definida.
III. A soma de uma matriz positiva definida com uma matriz diagonal com elementos positivos resulta em uma matriz positiva definida. Portanto, $Q^*$ Ã© positiva definida.
â– 

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Para ilustrar que a matriz $Q^*$ Ã© positiva definida, vamos analisar um caso simplificado de um modelo AR(1) com tendÃªncia. Neste caso, a matriz $Q^*$ tem a seguinte estrutura:
>
> $$
> Q^*=
> \begin{bmatrix}
>  \gamma_0^* & 0 & 0 \\
>  0 & 1 & 0 \\
>  0 & 0 & 1/3
> \end{bmatrix}
> $$
>
> onde $\gamma_0^* = E(y_t^{*2})$. Como $y_t^*$ Ã© um processo estacionÃ¡rio, $\gamma_0^*$ Ã© a sua variÃ¢ncia e, portanto, um valor positivo. Assim, $Q^*$ Ã© uma matriz diagonal com elementos positivos, o que a torna positiva definida. A positividade definida garante que a variÃ¢ncia assintÃ³tica dos estimadores (que envolve a inversa de $Q^*$) seja sempre bem definida e positiva, o que Ã© essencial para a realizaÃ§Ã£o de testes de hipÃ³teses e construÃ§Ã£o de intervalos de confianÃ§a. AlÃ©m disso, o fato de $Q^*$ ser positiva definida garante que a matriz de covariÃ¢ncia assintÃ³tica dos estimadores tambÃ©m o seja.

**ProposiÃ§Ã£o 2.1**
Os estimadores originais $b$ tambÃ©m sÃ£o assintoticamente normais. Mais precisamente, temos:

$$
Y_T (b - \beta) \xrightarrow{d} N(0, V)
$$
onde $V = \sigma^2 G' [Q^*]^{-1} G$ Ã© a matriz de covariÃ¢ncia assintÃ³tica dos estimadores originais.

*Prova:*
I.  Pelo CorolÃ¡rio 2.1, temos  $Y_T(b - \beta) \xrightarrow{d} N(0, \sigma^2 G'[Q^*]^{-1}G)$.
II. Definindo $V = \sigma^2 G' [Q^*]^{-1} G$, temos o resultado desejado.
â– 

**ObservaÃ§Ã£o 3**
A ProposiÃ§Ã£o 2.1 mostra que os estimadores originais, apÃ³s o devido escalonamento, tambÃ©m convergem para uma distribuiÃ§Ã£o normal, com uma matriz de covariÃ¢ncia que depende da matriz de transformaÃ§Ã£o $G'$ e da matriz $Q^*$. Isto Ã© fundamental para realizar inferÃªncia estatÃ­stica sobre os parÃ¢metros originais. AlÃ©m disso, como $Q^*$ Ã© positiva definida (Lema 2.1) e $G'$ tem posto completo, a matriz de covariÃ¢ncia assintÃ³tica $V$ tambÃ©m serÃ¡ positiva definida.

**Teorema 2.3**
Os estimadores OLS $b$ sÃ£o consistentes para os parÃ¢metros verdadeiros $\beta$, ou seja $b \xrightarrow{p} \beta$.

*Prova:*
I. Pelo Teorema 2.1, sabemos que $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$.  Isso implica que  $b^* \xrightarrow{p} \beta^*$.
II. Sabemos que $b = G' b^*$ e $\beta = G' \beta^*$.
III. Pela continuidade da transformaÃ§Ã£o linear, temos que $b = G'b^* \xrightarrow{p} G' \beta^* = \beta$.
â– 

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Para ilustrar a consistÃªncia dos estimadores, vamos considerar um experimento de simulaÃ§Ã£o em um modelo AR(1) com tendÃªncia, com $\phi_1 = 0.7$, $\alpha = 2$ e $\delta = 0.5$. Vamos simular sÃ©ries temporais com tamanhos de amostra $T = 100$, $T = 500$ e $T = 1000$ e estimar os parÃ¢metros usando OLS apÃ³s a transformaÃ§Ã£o.
>
> | Tamanho da Amostra (T) | $\hat{\phi_1}$ (estimativa) | $\hat{\alpha}$ (estimativa) | $\hat{\delta}$ (estimativa) |
> |---|---|---|---|
> | 100 | 0.68  | 2.15 | 0.48 |
> | 500 | 0.72 | 1.98 | 0.51 |
> | 1000 | 0.705 | 2.03 | 0.503 |
>
>  Como podemos observar na tabela, Ã  medida que o tamanho da amostra aumenta, as estimativas convergem para os verdadeiros valores dos parÃ¢metros. Este exemplo ilustra a convergÃªncia em probabilidade dos estimadores OLS $b$ para os parÃ¢metros verdadeiros $\beta$, conforme estabelecido no Teorema 2.3.

**CorolÃ¡rio 2.2**
Se a variÃ¢ncia do erro $\sigma^2$ Ã© estimada de forma consistente por $\hat{\sigma}^2$, entÃ£o o estimador da variÃ¢ncia assintÃ³tica dos estimadores originais $V$ pode ser estimado consistentemente por $\hat{V} = \hat{\sigma}^2 G' [\hat{Q}^*]^{-1} G$, onde  $\hat{Q}^*$ Ã© um estimador consistente de $Q^*$.

*Prova:*
I.  Pelo Teorema 2.3,  $b \xrightarrow{p} \beta$.
II. Um estimador consistente de  $\sigma^2$ Ã© dado por  $\hat{\sigma}^2 = \frac{1}{T-k} \sum_{t=1}^T (y_t - x_t' b)^2$, onde $k$ Ã© o nÃºmero de regressores.
III. Pela lei dos grandes nÃºmeros, temos que $\hat{Q}^* \xrightarrow{p} Q^*$.
IV. Aplicando o lema de Slutsky e a continuidade da transformaÃ§Ã£o linear na expressÃ£o de $V$,  temos $\hat{V} \xrightarrow{p} V$.
â– 

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos considerar o exemplo do modelo AR(1) com tendÃªncia e os dados simulados no exemplo do Teorema 2.3.  Para $T=1000$, obtivemos as seguintes estimativas dos parÃ¢metros: $\hat{\phi_1} = 0.705$, $\hat{\alpha} = 2.03$, e $\hat{\delta} = 0.503$. AlÃ©m disso, a estimativa da variÃ¢ncia do erro foi $\hat{\sigma}^2=0.98$.
>  Podemos calcular $\hat{Q}^*$ com os dados observados e obter um estimador consistente. Suponha que, apÃ³s o cÃ¡lculo, obtivemos:
>
> $$
> \hat{Q}^* =
> \begin{bmatrix}
>  2.00 & 1.41 & 0 \\
>  1.41 & 2.00 & 0 \\
>  0 & 0 & 1 & 0 \\
>  0 & 0 & 0 & 0.33
> \end{bmatrix}
> $$
>
>  A matriz de transformaÃ§Ã£o $G'$ (que Ã© especÃ­fica para o modelo AR(1) com tendÃªncia) Ã© dada por:
>
>  $$
>  G' = \begin{bmatrix}
>  1 & 1 & 0 \\
>  0 & -1 & 1 \\
>  1 & 0 & 0
>  \end{bmatrix}
>  $$
>
> EntÃ£o, usando a fÃ³rmula $\hat{V} = \hat{\sigma}^2 G' [\hat{Q}^*]^{-1} G$, podemos calcular uma estimativa consistente da matriz de covariÃ¢ncia assintÃ³tica. Substituindo os valores obtidos, obtemos:
>
>  $$
>  \hat{V} = 0.98 \times
>  \begin{bmatrix}
>  1 & 1 & 0 \\
>  0 & -1 & 1 \\
>  1 & 0 & 0
>  \end{bmatrix}
>  \begin{bmatrix}
>  1.52 & -1.07 & 0 & 0\\
>  -1.07 & 1.52 & 0 & 0\\
>   0 & 0 & 1 & 0 \\
>  0 & 0 & 0 & 3
> \end{bmatrix}
>  \begin{bmatrix}
>  1 & 0 & 1 \\
>  1 & -1 & 0 \\
>  0 & 1 & 0
>  \end{bmatrix}
>  $$
>  O cÃ¡lculo resulta em:
>
> $$
> \hat{V} \approx
> \begin{bmatrix}
>  0.99 & -0.05 & -0.12\\
>  -0.05 & 0.28 & 0.01\\
>  -0.12 & 0.01 & 0.19
> \end{bmatrix}
> $$
>
>  A matriz $\hat{V}$ fornece as variÃ¢ncias assintÃ³ticas estimadas para os estimadores dos parÃ¢metros originais. A raiz quadrada da diagonal principal de $\hat{V}$ sÃ£o os erros padrÃ£o dos estimadores. Estes erros padrÃ£o podem ser usados para construir intervalos de confianÃ§a e realizar testes de hipÃ³teses sobre os parÃ¢metros $\phi_1$, $\alpha$ e $\delta$. O fato de que  $\hat{V}$ converge em probabilidade para $V$ garante que a inferÃªncia estatÃ­stica realizada usando esta matriz seja vÃ¡lida assintoticamente.

### ConclusÃ£o

Nesta seÃ§Ã£o, estabelecemos as distribuiÃ§Ãµes assintÃ³ticas dos estimadores OLS apÃ³s a aplicaÃ§Ã£o da transformaÃ§Ã£o de Sims, Stock e Watson. Demonstramos que os coeficientes associados aos componentes estacionÃ¡rios convergem para uma distribuiÃ§Ã£o gaussiana Ã  taxa de $\sqrt{T}$, enquanto o coeficiente da tendÃªncia de tempo converge mais rapidamente, Ã  taxa de $T^{3/2}$. AlÃ©m disso, comprovamos que, apÃ³s a transformaÃ§Ã£o, os estimadores dos componentes estacionÃ¡rios e da tendÃªncia de tempo sÃ£o assintoticamente independentes, o que simplifica a inferÃªncia estatÃ­stica. Em suma, esta seÃ§Ã£o formaliza os resultados da transformaÃ§Ã£o de regressores em um modelo autorregressivo com tendÃªncia determinÃ­stica, fornecendo uma base sÃ³lida para a anÃ¡lise assintÃ³tica dos estimadores e sua aplicaÃ§Ã£o em testes de hipÃ³teses. Os resultados aqui apresentados formam a base para a anÃ¡lise de modelos mais complexos e sÃ£o fundamentais para a compreensÃ£o da inferÃªncia estatÃ­stica em sÃ©ries temporais nÃ£o estacionÃ¡rias.

### ReferÃªncias
[^1]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113-44.
<!-- END -->
