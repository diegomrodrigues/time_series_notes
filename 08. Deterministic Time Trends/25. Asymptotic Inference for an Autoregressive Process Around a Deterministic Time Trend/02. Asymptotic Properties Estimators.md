## Asymptotic Inference for an Autoregressive Process Around a Deterministic Time Trend: Limiting Distributions and Convergence Rates

### Introdu√ß√£o

Em continuidade √† discuss√£o sobre a transforma√ß√£o de regressores em modelos autorregressivos com tend√™ncias determin√≠sticas, este cap√≠tulo aprofunda a an√°lise das propriedades assint√≥ticas dos estimadores obtidos ap√≥s a aplica√ß√£o da t√©cnica de Sims, Stock e Watson (1990) [^1]. Conforme visto anteriormente, a transforma√ß√£o dos regressores em um termo constante, uma tend√™ncia de tempo e componentes estacion√°rios de m√©dia zero, desempenha um papel crucial na an√°lise das distribui√ß√µes limites e taxas de converg√™ncia dos estimadores de m√≠nimos quadrados ordin√°rios (OLS). O objetivo principal desta se√ß√£o √© demonstrar como a transforma√ß√£o facilita a obten√ß√£o de distribui√ß√µes assint√≥ticas mais simples e como as diferentes taxas de converg√™ncia dos estimadores influenciam a infer√™ncia estat√≠stica. Expandindo os conceitos abordados, iremos focar na an√°lise das propriedades assint√≥ticas dos estimadores resultantes da transforma√ß√£o, em particular nas diferentes taxas de converg√™ncia dos coeficientes associados aos componentes estacion√°rios e √† tend√™ncia de tempo, e na sua independ√™ncia assint√≥tica.

### Conceitos Fundamentais

Conforme estabelecido na se√ß√£o anterior, a transforma√ß√£o dos regressores resulta no modelo:
$$
y_t = \alpha^* + \delta^* t + \phi_1^* y_{t-1}^* + \phi_2^* y_{t-2}^* + \dots + \phi_p^* y_{t-p}^* + \epsilon_t
$$
onde:
$$
\begin{aligned}
\alpha^* &= [\alpha(1 + \phi_1 + \phi_2 + \dots + \phi_p) - \delta(\phi_1 + 2\phi_2 + \dots + p\phi_p)] \\
\delta^* &= \delta(1 + \phi_1 + \phi_2 + \dots + \phi_p) \\
\phi_j^* &= \phi_j \\
y_{t-j}^* &= y_{t-j} - \alpha - \delta(t-j) \quad \text{para } j = 1, 2, \ldots, p
\end{aligned}
$$
[^1]. O estimador OLS para esse modelo √© denotado por $b^*$, que corresponde √† estimativa dos coeficientes transformados [^1]. A rela√ß√£o entre os estimadores transformados $b^*$ e os estimadores originais $b$ √© dada por $b = G' b^*$, onde $G'$ √© a matriz de transforma√ß√£o detalhada na se√ß√£o anterior [^1].

> üí° **Revis√£o R√°pida:**
>
>  Recordando do cap√≠tulo anterior, os regressores originais s√£o transformados para isolar os componentes estoc√°sticos e determin√≠sticos, permitindo uma an√°lise separada de suas propriedades. A transforma√ß√£o envolve uma combina√ß√£o linear dos regressores originais, expressa pela matriz $G'$. Os coeficientes resultantes da transforma√ß√£o s√£o designados por $\alpha^*$, $\delta^*$ e $\phi_j^*$, representando os efeitos do componente constante, da tend√™ncia de tempo e das vari√°veis defasadas, respectivamente. O ponto chave √© que o termo $y_{t-j}^*$ √© estacion√°rio (quando $y_t$ √© estacion√°rio em torno de uma tend√™ncia linear), enquanto o termo $t$ representa uma tend√™ncia linear.

O objetivo agora √© analisar as propriedades assint√≥ticas do estimador transformado $b^*$. Uma propriedade crucial da transforma√ß√£o √© que ela simplifica a an√°lise da distribui√ß√£o limite dos estimadores OLS. Os coeficientes associados √†s vari√°veis estacion√°rias de m√©dia zero ($ \phi_1^*, \phi_2^*, \dots, \phi_p^*$) convergem para uma distribui√ß√£o gaussiana √† taxa de $\sqrt{T}$, enquanto o coeficiente da tend√™ncia de tempo ($\delta^*$) converge mais rapidamente, √† taxa de $T^{3/2}$ [^1]. Esta diferen√ßa nas taxas de converg√™ncia √© uma consequ√™ncia direta da presen√ßa da tend√™ncia de tempo e resulta em diferentes ordens de grandeza para as vari√¢ncias assint√≥ticas dos estimadores. A matriz de escalonamento $Y_T$ √© crucial para "normalizar" essas taxas de converg√™ncia:
$$
Y_T =
\begin{bmatrix}
\sqrt{T} & 0 & \dots & 0 & 0 & 0 \\
0 & \sqrt{T} & \dots & 0 & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & \dots & \sqrt{T} & 0 & 0 \\
0 & 0 & \dots & 0 & \sqrt{T} & 0 \\
0 & 0 & \dots & 0 & 0 & T^{3/2}
\end{bmatrix}
$$
[^1].

> üí° **Exemplo Num√©rico:**
>
> Para um modelo AR(1) com tend√™ncia linear, a matriz de escalonamento $Y_T$ seria:
>
> $$
> Y_T =
> \begin{bmatrix}
> \sqrt{T} & 0 & 0 \\
> 0 & \sqrt{T} & 0 \\
> 0 & 0 & T^{3/2}
> \end{bmatrix}
> $$
>
> Isso significa que o estimador do coeficiente AR(1) ($\phi_1^*$) e o intercepto ($\alpha^*$) convergem para seus verdadeiros valores √† uma taxa de $\sqrt{T}$, enquanto o estimador do coeficiente da tend√™ncia linear ($\delta^*$) converge √† uma taxa muito mais r√°pida, de $T^{3/2}$. A diferen√ßa nas taxas de converg√™ncia implica que o estimador $\delta^*$ √© mais preciso em amostras grandes em compara√ß√£o com $\phi_1^*$ e $\alpha^*$.
>
> Suponha que tenhamos uma s√©rie temporal com $T=100$. Nesse caso, $\sqrt{T} = 10$ e $T^{3/2} = 1000$. Para o mesmo desvio padr√£o, o erro padr√£o de  $\hat{\delta^*}$ √© cerca de 100 vezes menor que os erros padr√£o de $\hat{\phi_1^*}$ e $\hat{\alpha^*}$. Isso demonstra numericamente como a taxa de converg√™ncia de $T^{3/2}$ resulta em estimativas mais precisas para o coeficiente de tend√™ncia do que a taxa de converg√™ncia de $\sqrt{T}$ para os outros coeficientes.

A distribui√ß√£o assint√≥tica do estimador transformado $b^*$ √© expressa como:
$$
Y_T (b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})
$$
onde $Q^*$ √© a matriz limite de $\frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'}$ [^1]. Essa express√£o formaliza a ideia de que, ap√≥s o reescalonamento adequado, os estimadores convergem para uma distribui√ß√£o normal, com uma matriz de covari√¢ncia espec√≠fica.

**Teorema 2.1**
Sob as condi√ß√µes do modelo, a distribui√ß√£o assint√≥tica do estimador transformado √© dada por
$$
Y_T (b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})
$$
onde:
  - $Y_T$ √© a matriz de escalonamento, como definida anteriormente.
  - $b^*$ √© o estimador OLS dos par√¢metros transformados.
  - $\beta^*$ √© o vetor dos par√¢metros verdadeiros transformados.
  - $\sigma^2$ √© a vari√¢ncia do termo de erro.
  - $Q^*$ √© o limite da matriz de covari√¢ncia amostral dos regressores transformados, isto √©,
$$
 Q^* = \lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'}
$$
e tem a forma
$$
Q^*=
\begin{bmatrix}
 \gamma_0^* & \gamma_1^* & \dots & \gamma_{p-1}^* & 0 & 0 \\
 \gamma_1^* & \gamma_0^* & \dots & \gamma_{p-2}^* & 0 & 0 \\
 \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
 \gamma_{p-1}^* & \gamma_{p-2}^* & \dots & \gamma_0^* & 0 & 0 \\
 0 & 0 & \dots & 0 & 1 & 0 \\
 0 & 0 & \dots & 0 & 0 & 1/3
\end{bmatrix}
$$
onde $\gamma_j^* = E(y_t^* y_{t-j}^*)$.

*Prova (Esbo√ßo):*
I.  A demonstra√ß√£o se baseia em resultados assint√≥ticos para processos estoc√°sticos.  Primeiro, verifica-se que  $\frac{1}{T} \sum_{t=1}^T x_t^* x_t^{*'} \xrightarrow{p} Q^*$, usando resultados para processos estacion√°rios.
II.  Em seguida, verifica-se que  $\frac{1}{\sqrt{T}} \sum_{t=1}^T x_t^* \epsilon_t$ converge em distribui√ß√£o para uma normal multivariada com m√©dia zero e matriz de covari√¢ncia $\sigma^2Q^*$, usando o Teorema do Limite Central para Martingales (Proposition 7.8).
III. A distribui√ß√£o assint√≥tica de $Y_T(b^* - \beta^*)$ √© ent√£o obtida combinando os dois resultados anteriores e aplicando o lema de Slutsky.
‚ñ†

> üí° **Exemplo Num√©rico:**
>
>  Vamos ilustrar o Teorema 2.1 com o exemplo num√©rico de um modelo AR(1) com tend√™ncia.  Considere os estimadores $b^* = [\hat{\phi_1^*}, \hat{\alpha^*}, \hat{\delta^*}]$ e o vetor de par√¢metros verdadeiro  $\beta^* = [\phi_1^*, \alpha^*, \delta^*]$. O Teorema 2.1 afirma que
>
> $$
> \begin{bmatrix}
> \sqrt{T}(\hat{\phi_1^*} - \phi_1^*) \\
> \sqrt{T}(\hat{\alpha^*} - \alpha^*) \\
> T^{3/2}(\hat{\delta^*} - \delta^*)
> \end{bmatrix}
> \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})
> $$
>
> onde $\sigma^2$ √© a vari√¢ncia do termo de erro e $[Q^*]^{-1}$ √© a matriz inversa de $Q^*$.   Note que os coeficientes  $\hat{\phi_1^*}$ e  $\hat{\alpha^*}$ convergem com velocidade  $\sqrt{T}$, enquanto o coeficiente  $\hat{\delta^*}$ converge com velocidade $T^{3/2}$. A forma espec√≠fica de $Q^*$ depende das propriedades de $y_t^*$.
>
> Suponha que simulamos uma s√©rie temporal $y_t$ de tamanho $T=1000$ usando um modelo AR(1) com tend√™ncia, onde $\phi_1 = 0.7$, $\alpha = 2$, $\delta = 0.5$, e a vari√¢ncia do erro $\sigma^2 = 1$. Ap√≥s a transforma√ß√£o, obtemos $b^* = [\hat{\phi_1^*}, \hat{\alpha^*}, \hat{\delta^*}]$. Estimamos $Q^*$ usando os dados e obtivemos a seguinte matriz:
>
> $$
> \hat{Q}^*=
> \begin{bmatrix}
>  1.96 & 1.37 & 0 \\
>  1.37 & 1.96 & 0 \\
>  0 & 0 & 1 & 0 \\
>  0 & 0 & 0 & 0.33
> \end{bmatrix}
> $$
>  Calculando a inversa de $\hat{Q}^*$ e multiplicando por $\sigma^2 = 1$, obtemos:
>
> $$
> \sigma^2 [\hat{Q}^*]^{-1} =
> \begin{bmatrix}
>  1.53 & -1.07 & 0 & 0\\
>  -1.07 & 1.53 & 0 & 0\\
>  0 & 0 & 1 & 0 \\
>  0 & 0 & 0 & 3
> \end{bmatrix}
> $$
>
>  Esta matriz √© a vari√¢ncia assint√≥tica dos estimadores reescalonados. Observe que a vari√¢ncia do estimador de $\delta^*$ (√∫ltima entrada diagonal) √© muito menor do que as vari√¢ncias dos estimadores de $\phi_1^*$ e $\alpha^*$, o que reflete a taxa de converg√™ncia mais r√°pida. Al√©m disso, as covari√¢ncias entre os estimadores de $\phi_1^*$ e $\alpha^*$ s√£o n√£o-nulas, o que indica que eles est√£o correlacionados, enquanto todos os outros elementos fora da diagonal s√£o 0, indicando independ√™ncia assint√≥tica entre o estimador da tend√™ncia e os outros coeficientes.

**Observa√ß√£o 2**
Este teorema estabelece que, ap√≥s o escalonamento apropriado, o estimador transformado $b^*$ tem uma distribui√ß√£o normal assint√≥tica, o que √© essencial para infer√™ncia estat√≠stica. Al√©m disso, as diferentes taxas de converg√™ncia dos coeficientes, em $\sqrt{T}$ para os componentes estacion√°rios e $T^{3/2}$ para a tend√™ncia, s√£o explicitamente capturadas pela matriz $Y_T$.

**Teorema 2.2**
Os coeficientes estimados ap√≥s a transforma√ß√£o, ou seja os coeficientes que afetam os termos estacion√°rios $\phi_j^*$, e o coeficiente da tend√™ncia $\delta^*$, s√£o assintoticamente independentes.

*Prova (Esbo√ßo):*
I.  A independ√™ncia assint√≥tica dos estimadores  $\phi_j^*$ e $\delta^*$ √© uma consequ√™ncia da transforma√ß√£o de Sims, Stock e Watson e das propriedades assint√≥ticas do estimador OLS. Como os termos estoc√°sticos na matriz $Q^*$, referentes √†s vari√°veis defasadas ($y_{t-1}^*, y_{t-2}^*, \ldots, y_{t-p}^*$),  n√£o s√£o correlacionados com a tend√™ncia $t$ no limite, os estimadores OLS dos coeficientes correspondentes tendem a ser assintoticamente independentes. A forma da matriz $Q^*$ explicita o resultado.
II. A independ√™ncia assint√≥tica √© ainda mais refor√ßada pelo fato de que os termos estoc√°sticos convergem √† taxa $\sqrt{T}$ e a tend√™ncia converge √† taxa $T^{3/2}$, levando a um efeito de independ√™ncia entre as distribui√ß√µes assint√≥ticas dos seus estimadores.
‚ñ†

> üí° **Exemplo Num√©rico:**
>
> No contexto do nosso modelo AR(1) com tend√™ncia, o Teorema 2.2 implica que $\hat{\phi_1^*}$ √© assintoticamente independente de $\hat{\delta^*}$. Isso significa que a variabilidade assint√≥tica na estimativa do coeficiente AR n√£o √© influenciada pela variabilidade assint√≥tica na estimativa do coeficiente da tend√™ncia. A consequ√™ncia pr√°tica √© que infer√™ncias sobre esses coeficientes podem ser feitas de forma separada.
>
> Por exemplo, considere que em um estudo emp√≠rico, ap√≥s a aplica√ß√£o da transforma√ß√£o e estima√ß√£o, obtemos $\hat{\phi_1^*} = 0.75$ com um erro padr√£o de 0.05 e $\hat{\delta^*} = 0.6$ com um erro padr√£o de 0.002. De acordo com o Teorema 2.2, podemos construir intervalos de confian√ßa para esses estimadores de forma independente. Um intervalo de confian√ßa de 95% para $\phi_1^*$ pode ser calculado como $0.75 \pm 1.96 * 0.05$, enquanto o intervalo de confian√ßa de 95% para $\delta^*$ pode ser calculado como $0.6 \pm 1.96 * 0.002$. A independ√™ncia assint√≥tica nos permite usar essas estimativas de vari√¢ncia independentemente para infer√™ncia sobre cada par√¢metro. Al√©m disso, um teste de hip√≥teses sobre $\phi_1^*$, como $H_0: \phi_1^* = 0$, pode ser conduzido sem considerar a precis√£o da estimativa de $\delta^*$, e vice-versa.

**Corol√°rio 2.1**
A distribui√ß√£o assint√≥tica dos estimadores dos coeficientes originais  $\phi_j, \alpha, \delta$  pode ser obtida aplicando a matriz de transforma√ß√£o $G'$ aos resultados do teorema 2.1. Ou seja:

$$
Y_T(b - \beta) \xrightarrow{d} N(0, \sigma^2 G'[Q^*]^{-1}G)
$$
onde $\beta$ √© o vetor com os par√¢metros originais e $b$ √© o vetor com os estimadores dos par√¢metros originais.

*Prova:*
I.  Sabemos que $b = G'b^*$.
II. Usando o teorema 2.1, temos $Y_T (b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$.
III.  Aplicando a transforma√ß√£o $b = G'b^*$ segue-se que $Y_T(b - \beta) = Y_T(G'b^* - G'\beta^*) = G'Y_T (b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 G'[Q^*]^{-1}G)$.
‚ñ†

**Lema 2.1**
A matriz $Q^*$ √© positiva definida.

*Prova (Esbo√ßo):*
I.  A matriz $Q^*$ pode ser decomposta em blocos, onde o bloco superior esquerdo corresponde √† matriz de autocovari√¢ncias do processo estacion√°rio $y_t^*$, o bloco inferior direito √© uma matriz diagonal com elementos positivos (1 e 1/3), e os blocos fora da diagonal s√£o nulos.
II. A matriz de autocovari√¢ncias de um processo estacion√°rio √© sempre positiva semidefinida.  Sob certas condi√ß√µes, ela √© positiva definida.
III. A soma de uma matriz positiva definida com uma matriz diagonal com elementos positivos resulta em uma matriz positiva definida. Portanto, $Q^*$ √© positiva definida.
‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar que a matriz $Q^*$ √© positiva definida, vamos analisar um caso simplificado de um modelo AR(1) com tend√™ncia. Neste caso, a matriz $Q^*$ tem a seguinte estrutura:
>
> $$
> Q^*=
> \begin{bmatrix}
>  \gamma_0^* & 0 & 0 \\
>  0 & 1 & 0 \\
>  0 & 0 & 1/3
> \end{bmatrix}
> $$
>
> onde $\gamma_0^* = E(y_t^{*2})$. Como $y_t^*$ √© um processo estacion√°rio, $\gamma_0^*$ √© a sua vari√¢ncia e, portanto, um valor positivo. Assim, $Q^*$ √© uma matriz diagonal com elementos positivos, o que a torna positiva definida. A positividade definida garante que a vari√¢ncia assint√≥tica dos estimadores (que envolve a inversa de $Q^*$) seja sempre bem definida e positiva, o que √© essencial para a realiza√ß√£o de testes de hip√≥teses e constru√ß√£o de intervalos de confian√ßa. Al√©m disso, o fato de $Q^*$ ser positiva definida garante que a matriz de covari√¢ncia assint√≥tica dos estimadores tamb√©m o seja.

**Proposi√ß√£o 2.1**
Os estimadores originais $b$ tamb√©m s√£o assintoticamente normais. Mais precisamente, temos:

$$
Y_T (b - \beta) \xrightarrow{d} N(0, V)
$$
onde $V = \sigma^2 G' [Q^*]^{-1} G$ √© a matriz de covari√¢ncia assint√≥tica dos estimadores originais.

*Prova:*
I.  Pelo Corol√°rio 2.1, temos  $Y_T(b - \beta) \xrightarrow{d} N(0, \sigma^2 G'[Q^*]^{-1}G)$.
II. Definindo $V = \sigma^2 G' [Q^*]^{-1} G$, temos o resultado desejado.
‚ñ†

**Observa√ß√£o 3**
A Proposi√ß√£o 2.1 mostra que os estimadores originais, ap√≥s o devido escalonamento, tamb√©m convergem para uma distribui√ß√£o normal, com uma matriz de covari√¢ncia que depende da matriz de transforma√ß√£o $G'$ e da matriz $Q^*$. Isto √© fundamental para realizar infer√™ncia estat√≠stica sobre os par√¢metros originais. Al√©m disso, como $Q^*$ √© positiva definida (Lema 2.1) e $G'$ tem posto completo, a matriz de covari√¢ncia assint√≥tica $V$ tamb√©m ser√° positiva definida.

**Teorema 2.3**
Os estimadores OLS $b$ s√£o consistentes para os par√¢metros verdadeiros $\beta$, ou seja $b \xrightarrow{p} \beta$.

*Prova:*
I. Pelo Teorema 2.1, sabemos que $Y_T(b^* - \beta^*) \xrightarrow{d} N(0, \sigma^2 [Q^*]^{-1})$.  Isso implica que  $b^* \xrightarrow{p} \beta^*$.
II. Sabemos que $b = G' b^*$ e $\beta = G' \beta^*$.
III. Pela continuidade da transforma√ß√£o linear, temos que $b = G'b^* \xrightarrow{p} G' \beta^* = \beta$.
‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar a consist√™ncia dos estimadores, vamos considerar um experimento de simula√ß√£o em um modelo AR(1) com tend√™ncia, com $\phi_1 = 0.7$, $\alpha = 2$ e $\delta = 0.5$. Vamos simular s√©ries temporais com tamanhos de amostra $T = 100$, $T = 500$ e $T = 1000$ e estimar os par√¢metros usando OLS ap√≥s a transforma√ß√£o.
>
> | Tamanho da Amostra (T) | $\hat{\phi_1}$ (estimativa) | $\hat{\alpha}$ (estimativa) | $\hat{\delta}$ (estimativa) |
> |---|---|---|---|
> | 100 | 0.68  | 2.15 | 0.48 |
> | 500 | 0.72 | 1.98 | 0.51 |
> | 1000 | 0.705 | 2.03 | 0.503 |
>
>  Como podemos observar na tabela, √† medida que o tamanho da amostra aumenta, as estimativas convergem para os verdadeiros valores dos par√¢metros. Este exemplo ilustra a converg√™ncia em probabilidade dos estimadores OLS $b$ para os par√¢metros verdadeiros $\beta$, conforme estabelecido no Teorema 2.3.

**Corol√°rio 2.2**
Se a vari√¢ncia do erro $\sigma^2$ √© estimada de forma consistente por $\hat{\sigma}^2$, ent√£o o estimador da vari√¢ncia assint√≥tica dos estimadores originais $V$ pode ser estimado consistentemente por $\hat{V} = \hat{\sigma}^2 G' [\hat{Q}^*]^{-1} G$, onde  $\hat{Q}^*$ √© um estimador consistente de $Q^*$.

*Prova:*
I.  Pelo Teorema 2.3,  $b \xrightarrow{p} \beta$.
II. Um estimador consistente de  $\sigma^2$ √© dado por  $\hat{\sigma}^2 = \frac{1}{T-k} \sum_{t=1}^T (y_t - x_t' b)^2$, onde $k$ √© o n√∫mero de regressores.
III. Pela lei dos grandes n√∫meros, temos que $\hat{Q}^* \xrightarrow{p} Q^*$.
IV. Aplicando o lema de Slutsky e a continuidade da transforma√ß√£o linear na express√£o de $V$,  temos $\hat{V} \xrightarrow{p} V$.
‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar o exemplo do modelo AR(1) com tend√™ncia e os dados simulados no exemplo do Teorema 2.3.  Para $T=1000$, obtivemos as seguintes estimativas dos par√¢metros: $\hat{\phi_1} = 0.705$, $\hat{\alpha} = 2.03$, e $\hat{\delta} = 0.503$. Al√©m disso, a estimativa da vari√¢ncia do erro foi $\hat{\sigma}^2=0.98$.
>  Podemos calcular $\hat{Q}^*$ com os dados observados e obter um estimador consistente. Suponha que, ap√≥s o c√°lculo, obtivemos:
>
> $$
> \hat{Q}^* =
> \begin{bmatrix}
>  2.00 & 1.41 & 0 \\
>  1.41 & 2.00 & 0 \\
>  0 & 0 & 1 & 0 \\
>  0 & 0 & 0 & 0.33
> \end{bmatrix}
> $$
>
>  A matriz de transforma√ß√£o $G'$ (que √© espec√≠fica para o modelo AR(1) com tend√™ncia) √© dada por:
>
>  $$
>  G' = \begin{bmatrix}
>  1 & 1 & 0 \\
>  0 & -1 & 1 \\
>  1 & 0 & 0
>  \end{bmatrix}
>  $$
>
> Ent√£o, usando a f√≥rmula $\hat{V} = \hat{\sigma}^2 G' [\hat{Q}^*]^{-1} G$, podemos calcular uma estimativa consistente da matriz de covari√¢ncia assint√≥tica. Substituindo os valores obtidos, obtemos:
>
>  $$
>  \hat{V} = 0.98 \times
>  \begin{bmatrix}
>  1 & 1 & 0 \\
>  0 & -1 & 1 \\
>  1 & 0 & 0
>  \end{bmatrix}
>  \begin{bmatrix}
>  1.52 & -1.07 & 0 & 0\\
>  -1.07 & 1.52 & 0 & 0\\
>   0 & 0 & 1 & 0 \\
>  0 & 0 & 0 & 3
> \end{bmatrix}
>  \begin{bmatrix}
>  1 & 0 & 1 \\
>  1 & -1 & 0 \\
>  0 & 1 & 0
>  \end{bmatrix}
>  $$
>  O c√°lculo resulta em:
>
> $$
> \hat{V} \approx
> \begin{bmatrix}
>  0.99 & -0.05 & -0.12\\
>  -0.05 & 0.28 & 0.01\\
>  -0.12 & 0.01 & 0.19
> \end{bmatrix}
> $$
>
>  A matriz $\hat{V}$ fornece as vari√¢ncias assint√≥ticas estimadas para os estimadores dos par√¢metros originais. A raiz quadrada da diagonal principal de $\hat{V}$ s√£o os erros padr√£o dos estimadores. Estes erros padr√£o podem ser usados para construir intervalos de confian√ßa e realizar testes de hip√≥teses sobre os par√¢metros $\phi_1$, $\alpha$ e $\delta$. O fato de que  $\hat{V}$ converge em probabilidade para $V$ garante que a infer√™ncia estat√≠stica realizada usando esta matriz seja v√°lida assintoticamente.

### Conclus√£o

Nesta se√ß√£o, estabelecemos as distribui√ß√µes assint√≥ticas dos estimadores OLS ap√≥s a aplica√ß√£o da transforma√ß√£o de Sims, Stock e Watson. Demonstramos que os coeficientes associados aos componentes estacion√°rios convergem para uma distribui√ß√£o gaussiana √† taxa de $\sqrt{T}$, enquanto o coeficiente da tend√™ncia de tempo converge mais rapidamente, √† taxa de $T^{3/2}$. Al√©m disso, comprovamos que, ap√≥s a transforma√ß√£o, os estimadores dos componentes estacion√°rios e da tend√™ncia de tempo s√£o assintoticamente independentes, o que simplifica a infer√™ncia estat√≠stica. Em suma, esta se√ß√£o formaliza os resultados da transforma√ß√£o de regressores em um modelo autorregressivo com tend√™ncia determin√≠stica, fornecendo uma base s√≥lida para a an√°lise assint√≥tica dos estimadores e sua aplica√ß√£o em testes de hip√≥teses. Os resultados aqui apresentados formam a base para a an√°lise de modelos mais complexos e s√£o fundamentais para a compreens√£o da infer√™ncia estat√≠stica em s√©ries temporais n√£o estacion√°rias.

### Refer√™ncias
[^1]: Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. "Inference in Linear Time Series Models with Some Unit Roots." *Econometrica* 58:113-44.
<!-- END -->
