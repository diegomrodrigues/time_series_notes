### Introdu√ß√£o

Este cap√≠tulo expande a an√°lise do modelo GARCH(p, q), focando em sua representa√ß√£o equivalente como um modelo ARMA (Autoregressive Moving Average) aplicado aos *shocks* quadrados. Essa perspectiva fornece *insights* valiosos sobre a estrutura de depend√™ncia temporal da volatilidade e facilita a aplica√ß√£o de ferramentas de an√°lise de s√©ries temporais bem estabelecidas. Construindo sobre os conceitos previamente introduzidos, como *volatility clustering*, persist√™ncia e condi√ß√µes de estacionariedade, este cap√≠tulo aprofunda a rela√ß√£o entre os par√¢metros do GARCH(p, q) e a estrutura ARMA resultante.

### Conceitos Fundamentais

Recordando a defini√ß√£o fundamental do modelo GARCH(p, q) [^2]:

$$\varepsilon_t|\psi_{t-1} \sim N(0, h_t) \qquad (1)$$
$$h_t = \alpha_0 + \sum_{i=1}^{q} \alpha_i\varepsilon_{t-i}^2 + \sum_{i=1}^{p} \beta_ih_{t-i} \qquad (2)$$

Onde $\varepsilon_t$ √© o *shock*, $h_t$ √© a vari√¢ncia condicional, $\alpha_0$, $\alpha_i$ e $\beta_i$ s√£o os par√¢metros do modelo, *p* √© a ordem do componente GARCH e *q* √© a ordem do componente ARCH.

**Deriva√ß√£o da Representa√ß√£o ARMA Equivalente**

Uma representa√ß√£o equivalente do modelo GARCH(p, q), conforme destacado por Bollerslev [^4], √© dada por:

$$\varepsilon_t^2 = \alpha_0 + \sum_{i=1}^{q} \alpha_i\varepsilon_{t-i}^2 + \sum_{j=1}^{p} \beta_j\nu_{t-j} + \nu_t \qquad (3)$$

Onde $\nu_t$ √© definido como:

$$\nu_t = \varepsilon_t^2 - h_t = (\eta_t^2 - 1)h_t \qquad (4)$$

E $\eta_t \sim i.i.d. N(0,1)$ √© um *shock* i.i.d. normal padr√£o [^4]. Uma propriedade crucial desta formula√ß√£o √© que $\nu_t$ √© serialmente n√£o correlacionado com m√©dia zero [^4], ou seja, $E[\nu_t] = 0$ e $Cov(\nu_t, \nu_{t-k}) = 0$ para todo $k \neq 0$.

**Prova da N√£o Correla√ß√£o Serial de ŒΩt:**

Para demonstrar que $\nu_t$ √© serialmente n√£o correlacionado, precisamos mostrar que $E[\nu_t] = 0$ e $Cov(\nu_t, \nu_{t-k}) = 0$ para todo $k \neq 0$.

I. **Prova de $E[\nu_t] = 0$:**

   $$E[\nu_t] = E[\varepsilon_t^2 - h_t]$$
   $$E[\nu_t] = E[E[\varepsilon_t^2|\psi_{t-1}] - h_t]$$
   $$E[\nu_t] = E[h_t - h_t] = E[0] = 0$$

II. **Prova de $Cov(\nu_t, \nu_{t-k}) = 0$ para $k \neq 0$:**

   Precisamos mostrar que $E[\nu_t\nu_{t-k}] = 0$ para $k \neq 0$, uma vez que $E[\nu_t] = 0$.

   $$E[\nu_t\nu_{t-k}] = E[(\varepsilon_t^2 - h_t)(\varepsilon_{t-k}^2 - h_{t-k})]$$

   $$E[\nu_t\nu_{t-k}] = E[E[(\varepsilon_t^2 - h_t)(\varepsilon_{t-k}^2 - h_{t-k})|\psi_{t-1}]] \quad \text{para } k > 0$$

   Como $\varepsilon_{t-k}^2$ e $h_{t-k}$ pertencem ao conjunto de informa√ß√µes $\psi_{t-1}$, podemos tratar-los como constantes ao condicionar em $\psi_{t-1}$:

   $$E[\nu_t\nu_{t-k}] = E[(\varepsilon_{t-k}^2 - h_{t-k})E[\varepsilon_t^2 - h_t|\psi_{t-1}]] = E[(\varepsilon_{t-k}^2 - h_{t-k})E[\nu_t|\psi_{t-1}]]$$

   Como $E[\varepsilon_t^2 - h_t|\psi_{t-1}] = E[\varepsilon_t^2|\psi_{t-1}] - h_t = h_t - h_t = 0$, temos:

   $$E[\nu_t\nu_{t-k}] = E[(\varepsilon_{t-k}^2 - h_{t-k}) \cdot 0] = 0$$

Um argumento semelhante se aplica para $k < 0$. Portanto, $\nu_t$ √© serialmente n√£o correlacionado com m√©dia zero. ‚ñ†

Com esta representa√ß√£o em m√£os, podemos derivar a forma ARMA equivalente. Substituindo (4) em (2):

$$\varepsilon_t^2 = \alpha_0 + \sum_{i=1}^{q} \alpha_i\varepsilon_{t-i}^2 + \sum_{i=1}^{p} \beta_i(\varepsilon_{t-i}^2 - \nu_{t-i}) + \nu_t$$
$$\varepsilon_t^2 = \alpha_0 + \sum_{i=1}^{q} \alpha_i\varepsilon_{t-i}^2 + \sum_{i=1}^{p} \beta_i\varepsilon_{t-i}^2 - \sum_{i=1}^{p} \beta_i\nu_{t-i} + \nu_t$$
$$\varepsilon_t^2 = \alpha_0 + \sum_{i=1}^{\max\{p,q\}} (\alpha_i + \beta_i)\varepsilon_{t-i}^2 - \sum_{i=1}^{p} \beta_i\nu_{t-i} + \nu_t \qquad (5)$$

Na equa√ß√£o (5), os par√¢metros $\alpha_i$ s√£o definidos como zero para $i > q$ e os par√¢metros $\beta_i$ s√£o definidos como zero para $i > p$. Esta equa√ß√£o representa um modelo ARMA($\max\{p, q\}, p$) para o processo $\varepsilon_t^2$, com o lado autoregressivo dependendo da soma dos par√¢metros ARCH e GARCH, e o lado de m√©dia m√≥vel dependendo dos par√¢metros GARCH [^4].

> üí° **Exemplo Num√©rico:** Suponha que temos um modelo GARCH(1,1) com par√¢metros $\alpha_0 = 0.01$, $\alpha_1 = 0.1$ e $\beta_1 = 0.8$. Isso significa que a volatilidade de hoje depende de 10% do *shock* quadrado de ontem e 80% da volatilidade de ontem. A representa√ß√£o ARMA(1,1) equivalente para $\varepsilon_t^2$ √©:
>
> $$\varepsilon_t^2 = 0.01 + (0.1 + 0.8)\varepsilon_{t-1}^2 - 0.8\nu_{t-1} + \nu_t$$
>
> Simplificando, temos:
>
> $$\varepsilon_t^2 = 0.01 + 0.9\varepsilon_{t-1}^2 - 0.8\nu_{t-1} + \nu_t$$
>
> Aqui, o coeficiente autoregressivo √© 0.9 e o coeficiente de m√©dia m√≥vel √© -0.8. Isso significa que $\varepsilon_t^2$ √© altamente dependente do seu valor anterior, refletindo a persist√™ncia da volatilidade.
>
> Para simular este processo em Python:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Define os par√¢metros do modelo GARCH(1,1)
> alpha_0 = 0.01
> alpha_1 = 0.1
> beta_1 = 0.8
>
> # N√∫mero de simula√ß√µes
> num_simulations = 250
>
> # Inicializa os arrays para armazenar os resultados
> epsilon_squared = np.zeros(num_simulations)
> nu = np.zeros(num_simulations)
> h = np.zeros(num_simulations)
>
> # Inicializa os primeiros valores
> h[0] = alpha_0 / (1 - alpha_1 - beta_1)  # Vari√¢ncia incondicional
> epsilon_squared[0] = h[0] + np.random.normal(0, np.sqrt(h[0]))
> nu[0] = epsilon_squared[0] - h[0]
>
> # Simula o processo GARCH(1,1)
> for t in range(1, num_simulations):
>     h[t] = alpha_0 + alpha_1 * epsilon_squared[t-1] + beta_1 * h[t-1]
>     epsilon_t = np.random.normal(0, np.sqrt(h[t]))
>     epsilon_squared[t] = epsilon_t**2
>     nu[t] = epsilon_squared[t] - h[t]
>
> # Plota epsilon_squared
> plt.figure(figsize=(10, 6))
> plt.plot(epsilon_squared, label='Œµ_t^2')
> plt.title('Simula√ß√£o do Processo ARMA Equivalente de um GARCH(1,1)')
> plt.xlabel('Tempo')
> plt.ylabel('Valor')
> plt.grid(True)
> plt.legend()
> plt.show()
>
> # Plota h_t
> plt.figure(figsize=(10, 6))
> plt.plot(h, label='h_t')
> plt.title('Volatilidade Condicional (h_t) na Simula√ß√£o GARCH(1,1)')
> plt.xlabel('Tempo')
> plt.ylabel('Valor')
> plt.grid(True)
> plt.legend()
> plt.show()
>
> # Plota nu_t
> plt.figure(figsize=(10, 6))
> plt.plot(nu, label='ŒΩ_t')
> plt.title('Shocks ŒΩ_t na Simula√ß√£o GARCH(1,1)')
> plt.xlabel('Tempo')
> plt.ylabel('Valor')
> plt.grid(True)
> plt.legend()
> plt.show()
> ```
>
> Esta simula√ß√£o demonstra como os *shocks* quadrados e a volatilidade condicional evoluem ao longo do tempo no modelo GARCH(1,1). A visualiza√ß√£o de $\nu_t$ ajuda a confirmar sua natureza de ru√≠do branco.

**Implica√ß√µes da Representa√ß√£o ARMA**

A representa√ß√£o ARMA equivalente oferece diversas implica√ß√µes importantes:

1.  ***Identifica√ß√£o do Modelo:*** A estrutura de autocorrela√ß√£o dos *shocks* quadrados ($\varepsilon_t^2$) pode ser analisada usando as ferramentas padr√£o para modelos ARMA (ACF e PACF). Isso auxilia na identifica√ß√£o das ordens apropriadas *p* e *q* para o modelo GARCH subjacente [^8].

2.  ***Estimativa de Par√¢metros:*** T√©cnicas de estima√ß√£o para modelos ARMA, como o m√©todo de m√°xima verossimilhan√ßa (MLE), podem ser adaptadas para estimar os par√¢metros $\alpha_0$, $\alpha_i$ e $\beta_i$ do modelo GARCH atrav√©s da estima√ß√£o do modelo ARMA equivalente.

3.  ***Previs√£o da Volatilidade:*** A representa√ß√£o ARMA pode ser usada para prever os *shocks* quadrados futuros, o que, por sua vez, pode ser usado para prever a volatilidade futura. As t√©cnicas de previs√£o de ARMA s√£o bem estabelecidas e podem ser aplicadas diretamente √† representa√ß√£o ARMA equivalente.

**Lema 1 (Par√¢metros do GARCH e Coeficientes do ARMA):** No modelo ARMA equivalente para $\varepsilon_t^2$, o coeficiente autoregressivo de defasagem *i* √© dado por $(\alpha_i + \beta_i)$, enquanto o coeficiente de m√©dia m√≥vel de defasagem *i* √© dado por $-\beta_i$.

*Prova:* Este resultado segue diretamente da deriva√ß√£o da representa√ß√£o ARMA equivalente na equa√ß√£o (5).

I. Come√ßando com a equa√ß√£o (5):
   $$\varepsilon_t^2 = \alpha_0 + \sum_{i=1}^{\max\{p,q\}} (\alpha_i + \beta_i)\varepsilon_{t-i}^2 - \sum_{i=1}^{p} \beta_i\nu_{t-i} + \nu_t $$

II. Reorganizando os termos para identificar os coeficientes autoregressivos e de m√©dia m√≥vel:
   $$\varepsilon_t^2 = \alpha_0 +  \left[ (\alpha_1 + \beta_1)\varepsilon_{t-1}^2 + (\alpha_2 + \beta_2)\varepsilon_{t-2}^2 + \ldots \right] - \left[ \beta_1\nu_{t-1} + \beta_2\nu_{t-2} + \ldots \right] + \nu_t$$

III. Desta equa√ß√£o, fica claro que o coeficiente autoregressivo na defasagem *i* (ou seja, o coeficiente de $\varepsilon_{t-i}^2$) √© $(\alpha_i + \beta_i)$.

IV. Similarmente, o coeficiente de m√©dia m√≥vel na defasagem *i* (ou seja, o coeficiente de $\nu_{t-i}$) √© $-\beta_i$.

V. Portanto, no modelo ARMA equivalente para $\varepsilon_t^2$, o coeficiente autoregressivo na defasagem *i* √© dado por $(\alpha_i + \beta_i)$, enquanto o coeficiente de m√©dia m√≥vel na defasagem *i* √© dado por $-\beta_i$. ‚ñ†

> üí° **Exemplo Num√©rico:** Vamos analisar os coeficientes de um modelo GARCH(1,1) com $\alpha_0 = 0.05$, $\alpha_1 = 0.2$ e $\beta_1 = 0.6$.
>
> 1.  **Coeficiente Autoregressivo:** O coeficiente autoregressivo de defasagem 1 √© $(\alpha_1 + \beta_1) = 0.2 + 0.6 = 0.8$. Isso significa que 80% da volatilidade passada (medida pelo quadrado dos *shocks*) influencia a volatilidade atual.
> 2.  **Coeficiente de M√©dia M√≥vel:** O coeficiente de m√©dia m√≥vel de defasagem 1 √© $-\beta_1 = -0.6$. Isso indica que a inova√ß√£o da volatilidade passada ($\nu_{t-1}$) tem um impacto negativo na volatilidade atual.
>
> Para visualizar como esses coeficientes afetam o processo, podemos gerar o ACF (Fun√ß√£o de Autocorrela√ß√£o) dos *shocks* quadrados simulados. Um ACF que decai lentamente refor√ßa a ideia de persist√™ncia da volatilidade.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> import statsmodels.api as sm
> from statsmodels.graphics.tsaplots import plot_acf
>
> # Define os par√¢metros do modelo GARCH(1,1)
> alpha_0 = 0.05
> alpha_1 = 0.2
> beta_1 = 0.6
>
> # N√∫mero de simula√ß√µes
> num_simulations = 250
>
> # Inicializa os arrays para armazenar os resultados
> epsilon_squared = np.zeros(num_simulations)
> nu = np.zeros(num_simulations)
> h = np.zeros(num_simulations)
>
> # Inicializa os primeiros valores
> h[0] = alpha_0 / (1 - alpha_1 - beta_1)  # Vari√¢ncia incondicional
> epsilon_squared[0] = h[0] + np.random.normal(0, np.sqrt(h[0]))
> nu[0] = epsilon_squared[0] - h[0]
>
> # Simula o processo GARCH(1,1)
> for t in range(1, num_simulations):
>     h[t] = alpha_0 + alpha_1 * epsilon_squared[t-1] + beta_1 * h[t-1]
>     epsilon_t = np.random.normal(0, np.sqrt(h[t]))
>     epsilon_squared[t] = epsilon_t**2
>     nu[t] = epsilon_squared[t] - h[t]
>
> # Plota o ACF de epsilon_squared
> plt.figure(figsize=(12, 6))
> plot_acf(epsilon_squared, lags=20, ax=plt.gca())
> plt.title('Fun√ß√£o de Autocorrela√ß√£o (ACF) de Œµ_t^2')
> plt.xlabel('Defasagens')
> plt.ylabel('Autocorrela√ß√£o')
> plt.grid(True)
> plt.show()
> ```
>
> O ACF resultante mostrar√° um decaimento gradual, confirmando a persist√™ncia da volatilidade impl√≠cita nos coeficientes.

> üí° **Exemplo Num√©rico:** Para um modelo GARCH(2,1), a representa√ß√£o ARMA(2,1) √©:
>
> $$\varepsilon_t^2 = \alpha_0 + (\alpha_1 + \beta_1)\varepsilon_{t-1}^2 + \alpha_2 \varepsilon_{t-2}^2 - \beta_1 \nu_{t-1} + \nu_t$$
>
> Aqui, o coeficiente autoregressivo de $\varepsilon_{t-1}^2$ √© $(\alpha_1 + \beta_1)$, o coeficiente autoregressivo de $\varepsilon_{t-2}^2$ √© $\alpha_2$, e o coeficiente de m√©dia m√≥vel de $\nu_{t-1}$ √© $-\beta_1$.

**Lema 1.1 (Consequ√™ncia da N√£o Correla√ß√£o Serial de ŒΩt):** Dado que $\nu_t$ √© serialmente n√£o correlacionado com m√©dia zero, $E[\varepsilon_t^2 \nu_{t-k}] = 0$ para todo $k > 0$.

*Prova:*
$E[\varepsilon_t^2 \nu_{t-k}] = E[(h_t + \nu_t) \nu_{t-k}] = E[h_t \nu_{t-k}] + E[\nu_t \nu_{t-k}]$. Como $h_t$ est√° no conjunto de informa√ß√µes $\psi_{t-1}$, ent√£o $h_t$ √© independente de $\nu_{t-k}$ para $k>0$. Portanto, $E[h_t \nu_{t-k}] = E[h_t]E[\nu_{t-k}] = 0$. Tamb√©m sabemos que $E[\nu_t \nu_{t-k}] = 0$ para $k \neq 0$. Assim, $E[\varepsilon_t^2 \nu_{t-k}] = 0$ para todo $k > 0$. ‚ñ†

**Condi√ß√µes de Estacionariedade em Termos da Representa√ß√£o ARMA**

As condi√ß√µes de estacionariedade para o modelo GARCH(p, q) podem ser expressas em termos das ra√≠zes do polin√¥mio autoregressivo na representa√ß√£o ARMA equivalente. Especificamente, para que o processo GARCH(p, q) seja estacion√°rio, todas as ra√≠zes do polin√¥mio autoregressivo devem estar fora do c√≠rculo unit√°rio.

O polin√¥mio autoregressivo $A(L)$ √© definido como:
$$A(L) = 1 - \sum_{i=1}^{\max\{p,q\}} (\alpha_i + \beta_i)L^i$$

As ra√≠zes do polin√¥mio $A(L)$ s√£o os valores de $z$ para os quais $A(z) = 0$. Para garantir a estacionariedade, √© necess√°rio que $|z_i| > 1$ para todas as ra√≠zes $z_i$ do polin√¥mio $A(L)$.

**Teorema 1 (Estacionariedade e Ra√≠zes do Polin√¥mio Autoregressivo):** Um processo GARCH(p, q) √© estacion√°rio se, e somente se, todas as ra√≠zes do polin√¥mio autoregressivo $A(L)$ estiverem fora do c√≠rculo unit√°rio.

*Prova:* A prova deste teorema se baseia na teoria de estacionariedade para modelos ARMA. Um processo ARMA √© estacion√°rio se, e somente se, todas as ra√≠zes do seu polin√¥mio autoregressivo estiverem fora do c√≠rculo unit√°rio. Como o processo GARCH(p, q) pode ser representado como um processo ARMA para os *shocks* quadrados, a mesma condi√ß√£o se aplica para a estacionariedade do processo GARCH.

I. **Estabelecendo a conex√£o com a teoria ARMA:**
   Um processo ARMA √© estacion√°rio se e somente se as ra√≠zes do seu polin√¥mio autoregressivo estiverem fora do c√≠rculo unit√°rio. Este √© um resultado bem estabelecido na teoria de s√©ries temporais.

II. **Ligando GARCH √† representa√ß√£o ARMA:**
   J√° mostramos que um processo GARCH(p, q) pode ser reescrito como um processo ARMA($\max\{p,q\}, p$) para $\varepsilon_t^2$. Esta representa√ß√£o permite que apliquemos resultados da teoria ARMA diretamente ao processo GARCH.

III. **Aplicando a condi√ß√£o de estacionariedade ARMA ao GARCH:**
   Como o processo GARCH(p, q) √© estacion√°rio se e somente se sua representa√ß√£o ARMA equivalente for estacion√°ria, segue-se que a condi√ß√£o de estacionariedade para o GARCH(p, q) √© a mesma da sua representa√ß√£o ARMA.

IV. **Definindo o polin√¥mio autoregressivo:**
   O polin√¥mio autoregressivo $A(L)$ para a representa√ß√£o ARMA do GARCH(p, q) √© dado por:
   $$A(L) = 1 - \sum_{i=1}^{\max\{p,q\}} (\alpha_i + \beta_i)L^i$$

V. **Concluindo a prova:**
   Portanto, para que o processo GARCH(p, q) seja estacion√°rio, √© necess√°rio e suficiente que todas as ra√≠zes do polin√¥mio autoregressivo $A(L)$ estejam fora do c√≠rculo unit√°rio, ou seja, $|z_i| > 1$ para todas as ra√≠zes $z_i$. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere um modelo GARCH(1,1) com $\alpha_1 = 0.3$ e $\beta_1 = 0.6$. A representa√ß√£o ARMA(1,1) equivalente √©:
>
> $\varepsilon_t^2 = \alpha_0 + 0.9\varepsilon_{t-1}^2 - 0.6\nu_{t-1} + \nu_t$
>
> O polin√¥mio autoregressivo √© $A(L) = 1 - 0.9L$. A raiz deste polin√¥mio √© $z = 1/0.9 \approx 1.11$. Como $|z| > 1$, o processo GARCH(1,1) √© estacion√°rio.
>
> Agora, considere outro modelo GARCH(1,1) com $\alpha_1 = 0.5$ e $\beta_1 = 0.7$. Neste caso, $\alpha_1 + \beta_1 = 1.2 > 1$. O polin√¥mio autoregressivo √© $A(L) = 1 - 1.2L$. A raiz √© $z = 1/1.2 \approx 0.83$. Como $|z| < 1$, este processo GARCH(1,1) **n√£o √© estacion√°rio**.
>
> Para ilustrar o comportamento n√£o estacion√°rio, podemos simular ambos os processos e observar a diferen√ßa:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Define os par√¢metros para o processo estacion√°rio
> alpha_1_stationary = 0.3
> beta_1_stationary = 0.6
> alpha_0_stationary = 0.01
>
> # Define os par√¢metros para o processo n√£o estacion√°rio
> alpha_1_nonstationary = 0.5
> beta_1_nonstationary = 0.7
> alpha_0_nonstationary = 0.01
>
> # N√∫mero de simula√ß√µes
> num_simulations = 100
>
> # Inicializa os arrays para armazenar os resultados (estacion√°rio)
> epsilon_squared_stationary = np.zeros(num_simulations)
> h_stationary = np.zeros(num_simulations)
>
> # Inicializa os arrays para armazenar os resultados (n√£o estacion√°rio)
> epsilon_squared_nonstationary = np.zeros(num_simulations)
> h_nonstationary = np.zeros(num_simulations)
>
> # Inicializa os primeiros valores (estacion√°rio)
> h_stationary[0] = alpha_0_stationary / (1 - alpha_1_stationary - beta_1_stationary)
> epsilon_squared_stationary[0] = h_stationary[0] + np.random.normal(0, np.sqrt(h_stationary[0]))
>
> # Inicializa os primeiros valores (n√£o estacion√°rio)
> h_nonstationary[0] = alpha_0_nonstationary / (1 - alpha_1_nonstationary - beta_1_nonstationary)
> epsilon_squared_nonstationary[0] = h_nonstationary[0] + np.random.normal(0, np.sqrt(h_nonstationary[0]))
>
> # Simula o processo GARCH(1,1) (estacion√°rio)
> for t in range(1, num_simulations):
>     h_stationary[t] = alpha_0_stationary + alpha_1_stationary * epsilon_squared_stationary[t-1] + beta_1_stationary * h_stationary[t-1]
>     epsilon_t = np.random.normal(0, np.sqrt(h_stationary[t]))
>     epsilon_squared_stationary[t] = epsilon_t**2
>
> # Simula o processo GARCH(1,1) (n√£o estacion√°rio)
> for t in range(1, num_simulations):
>     h_nonstationary[t] = alpha_0_nonstationary + alpha_1_nonstationary * epsilon_squared_nonstationary[t-1] + beta_1_nonstationary * h_nonstationary[t-1]
>     epsilon_t = np.random.normal(0, np.sqrt(h_nonstationary[t]))
>     epsilon_squared_nonstationary[t] = epsilon_t**2
>
> # Plota os resultados
> plt.figure(figsize=(14, 7))
> plt.plot(epsilon_squared_stationary, label='Estacion√°rio (Œ±1=0.3, Œ≤1=0.6)')
> plt.plot(epsilon_squared_nonstationary, label='N√£o Estacion√°rio (Œ±1=0.5, Œ≤1=0.7)')
> plt.title('Compara√ß√£o de Processos GARCH(1,1) Estacion√°rios e N√£o Estacion√°rios')
> plt.xlabel('Tempo')
> plt.ylabel('Œµ_t^2')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> O gr√°fico resultante mostrar√° que o processo estacion√°rio oscila em torno de um n√≠vel constante, enquanto o processo n√£o estacion√°rio tende a explodir ou apresentar um comportamento inst√°vel.

**Teorema 1.1 (Condi√ß√£o Suficiente para Estacionariedade em GARCH(1,1)):** Para um modelo GARCH(1,1), uma condi√ß√£o suficiente para estacionariedade √© que $\alpha_1 + \beta_1 < 1$.

*Prova:* Para um modelo GARCH(1,1), o polin√¥mio autoregressivo √© $A(L) = 1 - (\alpha_1 + \beta_1)L$. A raiz deste polin√¥mio √© $z = 1/(\alpha_1 + \beta_1)$. Para estacionariedade, devemos ter $|z| > 1$, o que implica $|1/(\alpha_1 + \beta_1)| > 1$. Isso √© equivalente a $|\alpha_1 + \beta_1| < 1$. Como $\alpha_1$ e $\beta_1$ s√£o n√£o negativos, podemos remover o valor absoluto, resultando em $\alpha_1 + \beta_1 < 1$. ‚ñ†

**Observa√ß√£o 1:** A condi√ß√£o $\alpha_1 + \beta_1 < 1$ no GARCH(1,1) tamb√©m est√° relacionada com a persist√™ncia da volatilidade. Quanto mais pr√≥ximo de 1 for a soma dos par√¢metros, mais tempo leva para os choques de volatilidade desaparecerem.

### Conclus√£o

A representa√ß√£o ARMA equivalente do modelo GARCH(p, q) oferece uma perspectiva valiosa para a an√°lise da volatilidade, permitindo a aplica√ß√£o de ferramentas bem estabelecidas de s√©ries temporais. Ao expressar o modelo GARCH em termos de um processo ARMA para os *shocks* quadrados, √© poss√≠vel analisar a estrutura de autocorrela√ß√£o, estimar os par√¢metros e prever a volatilidade de forma mais eficiente. A condi√ß√£o de estacionariedade para o modelo GARCH pode ser expressa em termos das ra√≠zes do polin√¥mio autoregressivo na representa√ß√£o ARMA equivalente, garantindo a estabilidade do modelo.

### Refer√™ncias

[^1]: Engle, R.F., 1982, Autoregressive conditional heteroskedasticity with estimates of the variance of U.K. inflation, Econometrica 50, 987-1008.
[^2]: Bollerslev, T., 1986, Generalized autoregressive conditional heteroskedasticity, Journal of Econometrics 31, 307-327.
[^3]: Bollerslev, T., 1986, Generalized autoregressive conditional heteroskedasticity, Journal of Econometrics 31, 307-327.
[^4]: Bollerslev, T., 1986, Generalized autoregressive conditional heteroskedasticity, Journal of Econometrics 31, 307-327.
[^5]: Ling, S., and McAleer, M. (2002). Necessary and sufficient condition for the existence of the fourth moment of GARCH(p,q) processes. *Journal of Econometrics*, *110*(2), 317-339.
[^6]: Bollerslev, T., 1986, Generalized autoregressive conditional heteroskedasticity, Journal of Econometrics 31, 307-327.
[^7]: Bollerslev, T., 1986, Generalized autoregressive conditional heteroskedasticity, Journal of Econometrics 31, 307-327.
[^8]: Bollerslev, T., 1986, Generalized autoregressive conditional heteroskedasticity, Journal of Econometrics 31, 307-327.
<!-- END -->