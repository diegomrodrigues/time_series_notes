## O Modelo de Regressão GARCH: Incorporando a Heteroscedasticidade Condicional na Modelagem Estatística

### Introdução
Este capítulo explora a extensão do modelo GARCH para incorporar um componente de regressão, resultando no modelo de regressão GARCH. Esta estrutura permite que a heteroscedasticidade condicional seja modelada em conjunto com a relação entre uma variável dependente e variáveis explicativas. Construindo sobre os conceitos fundamentais do modelo GARCH, incluindo a definição formal, condições de estacionariedade, representação ARMA equivalente e representação ARCH(∞), este capítulo detalha a formulação do modelo de regressão GARCH, seus métodos de estimação e suas aplicações em diversas áreas.

### Conceitos Fundamentais

Começamos relembrando a definição formal do modelo GARCH(p, q) [^2]:

$$\varepsilon_t|\psi_{t-1} \sim N(0, h_t) \qquad (1)$$
$$h_t = \alpha_0 + \sum_{i=1}^{q} \alpha_i\varepsilon_{t-i}^2 + \sum_{i=1}^{p} \beta_ih_{t-i} \qquad (2)$$

No modelo de regressão GARCH, o *shock* $\varepsilon_t$ é definido como o erro de um modelo de regressão linear [^3]:

$$\varepsilon_t = y_t - x_t'b \qquad (3)$$

Onde:

*   $y_t$ é a variável dependente no tempo *t*.
*   $x_t$ é um vetor de variáveis explicativas no tempo *t*.
*   $b$ é um vetor de parâmetros desconhecidos que quantificam a relação entre as variáveis explicativas e a variável dependente.

Substituindo a equação (3) na equação (2), obtemos o modelo de regressão GARCH:

$$h_t = \alpha_0 + \sum_{i=1}^{q} \alpha_i(y_{t-i} - x_{t-i}'b)^2 + \sum_{i=1}^{p} \beta_ih_{t-i} \qquad (4)$$

A equação (4) define a variância condicional $h_t$ como uma função das variáveis explicativas defasadas ($x_{t-i}$) e dos parâmetros da regressão ($b$), além dos parâmetros GARCH usuais ($\alpha_0$, $\alpha_i$, $\beta_i$).

**Interpretações e Implicações**

O modelo de regressão GARCH permite que a variância dos erros na regressão varie ao longo do tempo, dependendo tanto de erros quadrados passados (componente ARCH) quanto de variâncias condicionais passadas (componente GARCH), enquanto também considera o impacto das variáveis explicativas [^3]. Este modelo é particularmente útil quando a variância dos erros na regressão não é constante, o que é comum em séries temporais financeiras e econômicas.

> 💡 **Exemplo Numérico:** Considere um modelo de regressão para os retornos de ações ($y_t$) em função de um índice de mercado ($x_t$). Se a volatilidade dos retornos de ações variar ao longo do tempo, um modelo de regressão GARCH pode capturar essa heteroscedasticidade condicional e fornecer estimativas mais precisas dos parâmetros da regressão.
>
> Suponha que temos os seguintes dados para os retornos de ações e o índice de mercado para os primeiros 5 dias:
>
> | Tempo (t) | Retorno da Ação ($y_t$) | Índice de Mercado ($x_t$) |
> |-----------|-----------------------|------------------------|
> | 1         | 0.01                  | 0.005                   |
> | 2         | -0.005                | -0.002                  |
> | 3         | 0.015                 | 0.01                    |
> | 4         | 0.008                 | 0.006                   |
> | 5         | -0.01                 | -0.008                  |
>
> Primeiro, podemos ajustar um modelo de regressão linear simples: $y_t = b_0 + b_1 x_t + \varepsilon_t$.  Usando mínimos quadrados, estimamos $b_0 = 0.001$ e $b_1 = 0.8$.
>
> Agora, calculamos os resíduos: $\varepsilon_t = y_t - (0.001 + 0.8x_t)$.
>
> | Tempo (t) | Retorno da Ação ($y_t$) | Índice de Mercado ($x_t$) | Resíduo ($\varepsilon_t$) |
> |-----------|-----------------------|------------------------|-----------------------|
> | 1         | 0.01                  | 0.005                   | 0.005                  |
> | 2         | -0.005                | -0.002                  | -0.0044               |
> | 3         | 0.015                 | 0.01                    | 0.006                  |
> | 4         | 0.008                 | 0.006                   | 0.0022               |
> | 5         | -0.01                 | -0.008                  | -0.0046               |
>
> Para verificar se há efeitos GARCH, podemos usar o teste LM de Engle. Regredimos $\varepsilon_t^2$ em $\varepsilon_{t-1}^2$. Precisaríamos de mais dados para obter resultados estatisticamente significativos, mas este exemplo ilustra os primeiros passos.  Suponha que, com uma amostra maior, o teste LM indicasse a presença de efeitos GARCH.  Então, um modelo de regressão GARCH seria mais apropriado.

**Estimação do Modelo de Regressão GARCH**

A estimação do modelo de regressão GARCH geralmente é realizada utilizando o método da máxima verossimilhança (MLE) [^9]. O processo envolve maximizar a função de log-verossimilhança, que é construída com base na suposição de que os *shocks* $\varepsilon_t$ são normalmente distribuídos condicionalmente [^9].

A função de log-verossimilhança para uma amostra de *T* observações é dada por [^9]:

$$L(\theta) = T^{-1} \sum_{t=1}^{T} l_t(\theta) \qquad (5)$$

Onde $\theta = (b', \omega')$ é o vetor de parâmetros a serem estimados, com $b$ sendo o vetor de parâmetros da regressão e $\omega = (\alpha_0, \alpha_1, ..., \alpha_q, \beta_1, ..., \beta_p)$ o vetor de parâmetros GARCH [^9]. A função $l_t(\theta)$ é a função de log-verossimilhança para a observação *t*, dada por [^9]:

$$l_t(\theta) = -\frac{1}{2}\log(h_t) - \frac{1}{2}\frac{\varepsilon_t^2}{h_t} = -\frac{1}{2}\log(h_t) - \frac{1}{2}\frac{(y_t - x_t'b)^2}{h_t} \qquad (6)$$

Onde $h_t$ é a variância condicional calculada recursivamente utilizando a equação (4).

A maximização da função de log-verossimilhança requer o uso de métodos numéricos de otimização. As derivadas da função de log-verossimilhança em relação aos parâmetros $b$ e $\omega$ são necessárias para implementar esses métodos.

**Derivadas da Função de Log-Verossimilhança**

As derivadas da função de log-verossimilhança em relação aos parâmetros da variância $\omega$ e aos parâmetros da média $b$ são dadas por:

$$\frac{\partial l_t}{\partial \omega} = \frac{1}{2}\left(\frac{1}{h_t} \frac{\partial h_t}{\partial \omega} - \frac{\varepsilon_t^2}{h_t^2}\frac{\partial h_t}{\partial \omega}\right) = \frac{1}{2}\frac{\partial h_t}{\partial \omega}\left(\frac{1}{h_t} - \frac{\varepsilon_t^2}{h_t^2}\right) \qquad (7)$$

$$\frac{\partial l_t}{\partial b} = \frac{1}{2}h_t^{-1}2\varepsilon_t (-x_t) + \frac{\varepsilon_t^2}{2h_t^2} \frac{\partial h_t}{\partial b} = - \frac{\varepsilon_t x_t}{h_t} + \frac{\varepsilon_t^2}{2h_t^2} \frac{\partial h_t}{\partial b} \qquad (8)$$

Onde $\frac{\partial h_t}{\partial \omega}$ e $\frac{\partial h_t}{\partial b}$ representam as derivadas da variância condicional em relação aos parâmetros $\omega$ e $b$, respectivamente. Estas derivadas podem ser calculadas recursivamente utilizando a equação (4) [^9].

**Cálculo Recursivo das Derivadas da Variância Condicional**

Para implementar o método da máxima verossimilhança, precisamos calcular as derivadas de $h_t$ em relação a $\omega$ e $b$. Derivando a equação (4) em relação a $\omega$:

$$\frac{\partial h_t}{\partial \omega} = z_t + \sum_{i=1}^{p} \beta_i \frac{\partial h_{t-i}}{\partial \omega} \qquad (9)$$

Onde $z_t = \frac{\partial}{\partial \omega} \left[ \alpha_0 + \sum_{i=1}^{q} \alpha_i(y_{t-i} - x_{t-i}'b)^2\right]$ depende dos parâmetros específicos a serem estimados.

Derivando a equação (4) em relação a $b$:

$$\frac{\partial h_t}{\partial b} = -2\sum_{i=1}^{q} \alpha_i(y_{t-i} - x_{t-i}'b)x_{t-i} + \sum_{i=1}^{p} \beta_i \frac{\partial h_{t-i}}{\partial b} \qquad (10)$$

As equações (9) e (10) fornecem um esquema recursivo para calcular as derivadas da variância condicional em relação aos parâmetros $\omega$ e $b$, respectivamente. Para iniciar a recursão, precisamos de valores iniciais para $\frac{\partial h_{t-i}}{\partial \omega}$ e $\frac{\partial h_{t-i}}{\partial b}$ para $i = 1, \ldots, p$. Normalmente, esses valores iniciais são definidos como zero.

> 💡 **Exemplo Numérico:** Considere um modelo de regressão GARCH(1,1) com a seguinte especificação:
>
> $$y_t = b_0 + b_1 x_t + \varepsilon_t$$
> $$h_t = \alpha_0 + \alpha_1 \varepsilon_{t-1}^2 + \beta_1 h_{t-1}$$
>
> Neste caso, $\omega = (\alpha_0, \alpha_1, \beta_1)$ e $b = (b_0, b_1)$. As derivadas da função de log-verossimilhança em relação aos parâmetros são dadas pelas equações (7) e (8), com as derivadas da variância condicional dadas por:
>
> $$\frac{\partial h_t}{\partial \alpha_0} = 1 + \beta_1 \frac{\partial h_{t-1}}{\partial \alpha_0}$$
> $$\frac{\partial h_t}{\partial \alpha_1} = \varepsilon_{t-1}^2 + \beta_1 \frac{\partial h_{t-1}}{\partial \alpha_1}$$
> $$\frac{\partial h_t}{\partial \beta_1} = h_{t-1} + \beta_1 \frac{\partial h_{t-1}}{\partial \beta_1}$$
>
> $$\frac{\partial h_t}{\partial b_0} = -2\alpha_1 \varepsilon_{t-1} + \beta_1 \frac{\partial h_{t-1}}{\partial b_0}$$
> $$\frac{\partial h_t}{\partial b_1} = -2\alpha_1 \varepsilon_{t-1} x_{t-1} + \beta_1 \frac{\partial h_{t-1}}{\partial b_1}$$
>
> Esses cálculos mostram como as derivadas da variância condicional dependem das derivadas passadas e dos valores dos *shocks* e variáveis explicativas.
>
> Para implementar a estimação MLE em Python, podemos usar a biblioteca `arch`:
>
> ```python
> import numpy as np
> import pandas as pd
> from arch import arch_model
>
> # Gera dados simulados
> np.random.seed(42)
> T = 250
> b_0 = 0.1
> b_1 = 0.5
> alpha_0 = 0.01
> alpha_1 = 0.2
> beta_1 = 0.6
>
> x = np.random.normal(0, 1, T)
> epsilon = np.zeros(T)
> h = np.zeros(T)
>
> # Inicializa h
> h[0] = alpha_0 / (1 - alpha_1 - beta_1)
>
> y = np.zeros(T)
> for t in range(1, T):
>     y[t] = b_0 + b_1 * x[t] + epsilon[t-1]
>     h[t] = alpha_0 + alpha_1 * epsilon[t-1]**2 + beta_1 * h[t-1]
>     epsilon[t] = np.random.normal(0, np.sqrt(h[t]))
>
> data = pd.DataFrame({'y': y, 'x': x})
>
> # Ajusta o modelo de regressão GARCH(1,1)
> am = arch_model(data['y'], x=data[['x']], vol='Garch', p=1, o=0, q=1, dist='Normal')
> res = am.fit(disp='off')
>
> print(res.summary())
> ```
>
> A saída do código Python fornece um resumo estatístico da estimação do modelo GARCH(1,1) com regressão.  Os parâmetros estimados ($b_0$, $b_1$, $\alpha_0$, $\alpha_1$, $\beta_1$) e seus respectivos erros padrão, estatísticas z e valores p são exibidos. O resumo também inclui informações sobre a função de log-verossimilhança, critérios de informação (AIC, BIC) e testes de diagnóstico para os resíduos.  Esses resultados ajudam a avaliar a significância estatística dos parâmetros e a adequação do modelo aos dados. Por exemplo, um $\alpha_1$ significativo indica a presença de efeitos ARCH, enquanto um $\beta_1$ significativo indica efeitos GARCH.

**Independência Assintótica**

Sob certas condições de regularidade, os estimadores de máxima verossimilhança dos parâmetros da média ($b$) e dos parâmetros da variância ($\omega$) são assintoticamente independentes [^9]. Isso significa que a estimação de $b$ não é afetada pela estimação de $\omega$ e vice-versa. Essa independência assintótica simplifica a inferência estatística e a construção de testes de hipóteses.

**Teorema 1 (Independência Assintótica):** Sob condições de regularidade, os estimadores de máxima verossimilhança $\hat{b}$ e $\hat{\omega}$ são assintoticamente independentes.

*Proof Strategy:* A prova se baseia nas propriedades assintóticas dos estimadores de máxima verossimilhança e nas condições de regularidade que garantem a validade da aproximação normal assintótica. A chave é mostrar que os elementos fora da diagonal da matriz de informação são zero [^9]. Isso implica que a estimação dos parâmetros da média ($b$) não é afetada pela estimação dos parâmetros da variância ($\omega$) e vice-versa. Formalmente,

I. A matriz de informação é definida como a esperança negativa da matriz Hessiana da função de log-verossimilhança.
   $$I(\theta) = -E \left[ \frac{\partial^2 L(\theta)}{\partial \theta \partial \theta'} \right]$$

II. Sob condições de regularidade, a matriz de informação é consistente para o inverso da matriz de covariância assintótica dos estimadores MLE. Portanto, se mostrarmos que os elementos da matriz de informação correspondentes à covariância entre $\hat{b}$ e $\hat{\omega}$ são zero, teremos demonstrado a independência assintótica.

III. Os elementos relevantes da matriz de informação são dados por:
    $$I_{b\omega} = -E \left[ \frac{\partial^2 L(\theta)}{\partial b \partial \omega'} \right]$$

IV. Calculando a derivada cruzada, podemos mostrar que $E \left[ \frac{\partial^2 L(\theta)}{\partial b \partial \omega'} \right] = 0$. Este cálculo envolve manipular as derivadas da função de log-verossimilhança e utilizar as propriedades da expectativa condicional e a independência entre os *shocks* e a variância condicional.  Para ilustrar, considere que, sob certas condições de regularidade (ver [^9] para uma discussão completa), temos:

$$\frac{\partial^2 l_t}{\partial b \partial \omega'} = \frac{\partial}{\partial \omega'} \left( - \frac{\varepsilon_t x_t}{h_t} + \frac{\varepsilon_t^2}{2h_t^2} \frac{\partial h_t}{\partial b} \right)$$
$$= \frac{\varepsilon_t x_t}{h_t^2} \frac{\partial h_t}{\partial \omega'} + \frac{\varepsilon_t^2}{2} \left( -\frac{2}{h_t^3} \frac{\partial h_t}{\partial \omega'} \frac{\partial h_t}{\partial b} + \frac{1}{h_t^2} \frac{\partial^2 h_t}{\partial b \partial \omega'} \right)$$
Sob a suposição de que $E[\varepsilon_t | \psi_{t-1}] = 0$, onde $\psi_{t-1}$ representa a informação disponível no tempo $t-1$, o valor esperado da derivada cruzada é zero:
$$E \left[ \frac{\partial^2 l_t}{\partial b \partial \omega'} | \psi_{t-1} \right] = 0$$
Isso ocorre porque os termos que envolvem $\varepsilon_t$ têm valor esperado condicional zero.

V. Conclusão: Como $I_{b\omega} = 0$, a matriz de informação é bloco diagonal, o que implica que $\hat{b}$ e $\hat{\omega}$ são assintoticamente independentes.  $\blacksquare$

**Testes para GARCH**

É importante testar a presença de efeitos GARCH nos resíduos de um modelo de regressão antes de estimar um modelo de regressão GARCH [^10]. Vários testes estão disponíveis para este propósito, incluindo o teste de Engle (1982) e o teste de Lagrange Multiplier (LM) [^10]. Esses testes verificam se há dependência condicional na variância dos resíduos.

O teste LM para GARCH envolve a regressão dos resíduos quadrados de um modelo de regressão linear em suas próprias defasagens e, em seguida, testar a significância dos coeficientes dessas defasagens. Se os coeficientes forem significativamente diferentes de zero, isso fornece evidências de efeitos GARCH.

**Teorema 2 (Teste LM para GARCH):** O teste LM para GARCH é baseado na regressão dos resíduos quadrados de um modelo de regressão linear em suas próprias defasagens e testar a significância conjunta dos coeficientes dessas defasagens. A estatística de teste LM é assintoticamente distribuída como uma qui-quadrado com *q* graus de liberdade, onde *q* é o número de defasagens incluídas na regressão auxiliar.

*Proof Strategy:* A prova envolve derivar a estatística de teste LM sob a hipótese nula de que não há efeitos GARCH e mostrar que ela segue uma distribuição qui-quadrado assintoticamente.

I. Sob a hipótese nula de ausência de efeitos GARCH, a variância condicional é constante e igual à variância incondicional dos resíduos.  Isto implica que $h_t = \sigma^2$ para todo $t$, onde $\sigma^2$ é constante.

II. A estatística de teste LM é construída como *T* vezes o R-quadrado da regressão dos resíduos quadrados em suas defasagens, onde *T* é o tamanho da amostra.  Este resultado decorre da teoria geral dos testes LM.

III. Sob a hipótese nula, a estatística de teste LM segue uma distribuição qui-quadrado com *q* graus de liberdade, onde *q* é o número de defasagens incluídas na regressão auxiliar. Este resultado se baseia nas propriedades assintóticas da estatística de teste LM e na distribuição dos estimadores de mínimos quadrados sob a hipótese nula. Podemos demonstrar isso formalmente usando o teorema de convergência de Slutsky e o teorema do limite central.

IV. Se o valor da estatística de teste LM for maior do que o valor crítico da distribuição qui-quadrado no nível de significância escolhido, rejeitamos a hipótese nula de ausência de efeitos GARCH.  $\blacksquare$

Para complementar o Teorema 2, podemos formalizar o teste LM para GARCH como um procedimento algorítmico.

**Algoritmo 1 (Teste LM para GARCH)**

1.  **Estimar o modelo de regressão linear:** Estime o modelo $y_t = x_t'b + \varepsilon_t$ por mínimos quadrados ordinários (OLS) e obtenha os resíduos $\hat{\varepsilon}_t$.

2.  **Calcular os resíduos quadrados:** Calcule os resíduos quadrados $\hat{\varepsilon}_t^2$.

3.  **Regressão auxiliar:** Regresse os resíduos quadrados $\hat{\varepsilon}_t^2$ em um intercepto e em *q* defasagens dos resíduos quadrados:
    $$\hat{\varepsilon}_t^2 = \gamma_0 + \gamma_1 \hat{\varepsilon}_{t-1}^2 + \gamma_2 \hat{\varepsilon}_{t-2}^2 + \cdots + \gamma_q \hat{\varepsilon}_{t-q}^2 + v_t$$

4.  **Calcular a estatística de teste LM:** Calcule a estatística de teste LM como $LM = T \cdot R^2$, onde *T* é o tamanho da amostra e $R^2$ é o coeficiente de determinação da regressão auxiliar.

5.  **Determinar o valor-p:** Compare a estatística de teste LM com um valor crítico da distribuição qui-quadrado com *q* graus de liberdade. Calcule o valor-p correspondente à estatística de teste LM.

6.  **Decisão:** Se o valor-p for menor do que o nível de significância escolhido (por exemplo, 0.05), rejeite a hipótese nula de ausência de efeitos GARCH. Caso contrário, não rejeite a hipótese nula.

> 💡 **Exemplo Numérico:**  Para ilustrar o Teste LM, vamos usar os resíduos calculados no exemplo anterior e realizar o teste com uma defasagem (q=1).
>
> 1. **Resíduos Quadrados:**  Calculamos os quadrados dos resíduos:
>
>    | Tempo (t) | Resíduo ($\varepsilon_t$) | Resíduo Quadrado ($\varepsilon_t^2$) |
>    |-----------|-----------------------|------------------------------------|
>    | 1         | 0.005                  | 0.000025                           |
>    | 2         | -0.0044               | 0.00001936                         |
>    | 3         | 0.006                  | 0.000036                           |
>    | 4         | 0.0022               | 0.00000484                         |
>    | 5         | -0.0046               | 0.00002116                         |
>
> 2.  **Regressão Auxiliar:** Regredimos $\hat{\varepsilon}_t^2$ em um intercepto e $\hat{\varepsilon}_{t-1}^2$:  $\hat{\varepsilon}_t^2 = \gamma_0 + \gamma_1 \hat{\varepsilon}_{t-1}^2 + v_t$. Para este pequeno conjunto de dados, a regressão resultaria em:
>
>     $$\hat{\varepsilon}_t^2 =  0.00001 + 0.4 \hat{\varepsilon}_{t-1}^2$$
>
>     (Note:  Este é um resultado ilustrativo com poucos dados. Em uma análise real, precisaríamos de muitos mais pontos de dados).
>
> 3. **Calcular a Estatística LM:**  Suponha que o R-quadrado ($R^2$) da regressão auxiliar é 0.3.  Com T=5, a estatística LM é $LM = T \cdot R^2 = 5 \cdot 0.3 = 1.5$.
>
> 4. **Determinar o Valor-p:** Comparamos a estatística LM (1.5) com o valor crítico de uma distribuição qui-quadrado com 1 grau de liberdade. O valor crítico para $\alpha = 0.05$ é 3.84. O valor-p é aproximadamente 0.22.
>
> 5. **Decisão:** Como o valor-p (0.22) é maior que 0.05, não rejeitamos a hipótese nula de ausência de efeitos GARCH.
>
> Em um cenário real, a análise seria realizada com um conjunto de dados muito maior e usando software estatístico para obter resultados mais precisos.

Além dos testes para GARCH, é útil discutir as condições sob as quais as derivadas da função de log-verossimilhança são bem definidas, garantindo a validade da estimação por máxima verossimilhança.

**Lema 1 (Existência das Derivadas da Função de Log-Verossimilhança):** Se o modelo de regressão GARCH(p, q) satisfaz as condições de estacionariedade e ergodicidade, e se as variáveis explicativas $x_t$ têm momentos finitos até a ordem quatro, então as derivadas da função de log-verossimilhança em relação aos parâmetros $b$ e $\omega$ existem e são finitas em probabilidade.

*Proof Strategy:* A prova envolve verificar se as condições para a existência das derivadas da função de log-verossimilhança são satisfeitas.

I. As condições de estacionariedade e ergodicidade garantem que a variância condicional $h_t$ é estocasticamente finita e positiva.  As condições de estacionariedade para um GARCH(p,q) garantem que a variância incondicional do processo é finita. A ergodicidade garante que as médias amostrais convergem para as médias populacionais à medida que o tamanho da amostra aumenta.

II. Os momentos finitos das variáveis explicativas garantem que os termos $(y_{t-i} - x_{t-i}'b)$ têm momentos finitos até a ordem quatro.  Isso é necessário porque a função de log-verossimilhança envolve termos que são quadráticos em $(y_{t-i} - x_{t-i}'b)$.

III. Utilizando as equações (7) e (8), podemos mostrar que as derivadas da função de log-verossimilhança são funções contínuas de $h_t$, $(y_{t-i} - x_{t-i}'b)$ e $x_t$.  A continuidade é essencial para garantir que pequenas mudanças nos parâmetros resultem em pequenas mudanças na função de log-verossimilhança e suas derivadas.

IV. Combinando os resultados anteriores, podemos concluir que as derivadas da função de log-verossimilhança existem e são finitas em probabilidade.  $\blacksquare$

### Conclusão

O modelo de regressão GARCH fornece uma estrutura flexível e poderosa para modelar a heteroscedasticidade condicional em modelos de regressão. Ao permitir que a variância dos erros varie ao longo do tempo, dependendo tanto dos erros quadrados passados quanto das variâncias condicionais passadas, o modelo de regressão GARCH captura a dinâmica complexa da volatilidade em séries temporais financeiras e econômicas. Os métodos de estimação e teste para o modelo de regressão GARCH são bem estabelecidos e amplamente utilizados na prática.

### Referências

[^1]: Engle, R.F., 1982, Autoregressive conditional heteroskedasticity with estimates of the variance of U.K. inflation, Econometrica 50, 987-1008.
[^2]: Bollerslev, T., 1986, Generalized autoregressive conditional heteroskedasticity, Journal of Econometrics 31, 307-327.
[^3]: Bollerslev, T., 1986, Generalized autoregressive conditional heteroskedasticity, Journal of Econometrics 31, 307-327.
[^4]: Bollerslev, T., 1986, Generalized autoregressive conditional heteroskedasticity, Journal of Econometrics 31, 307-327.
[^5]: Ling, S., and McAleer, M. (2002). Necessary and sufficient condition for the existence of the fourth moment of GARCH(p,q) processes. *Journal of Econometrics*, *110*(2), 317-339.
[^6]: Bollerslev, T., 1986, Generalized autoregressive conditional heteroskedasticity, Journal of Econometrics 31, 307-327.
[^7]: Bollerslev, T., 1986, Generalized autoregressive conditional heteroskedasticity, Journal of Econometrics 31, 307-327.
[^8]: Bollerslev, T., 1986, Generalized autoregressive conditional heteroskedasticity, Journal of Econometrics 31, 307-327.
[^9]: Bollerslev, T., 1986, Generalized autoregressive conditional heteroskedasticity, Journal of Econometrics 31, 307-327.
[^10]: Engle, R.F., 1982, Autoregressive conditional heteroskedasticity with estimates of the variance of U.K. inflation, Econometrica 50, 987-1008.
<!-- END -->