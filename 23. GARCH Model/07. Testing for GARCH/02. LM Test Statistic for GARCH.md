## O Teste LM para Efeitos GARCH: Detalhes da EstatÃ­stica do Teste

### IntroduÃ§Ã£o
Em continuidade Ã  discussÃ£o sobre testes para efeitos GARCH (Generalized Autoregressive Conditional Heteroskedasticity), e aprofundando o conceito do Teste de Multiplicador de Lagrange (LM) introduzido no tÃ³pico anterior, este capÃ­tulo se concentrarÃ¡ na estatÃ­stica do teste LM propriamente dita, especificamente a estatÃ­stica $\xi_{LM} = f_0'Z_0(Z_0'Z_0)^{-1}Z_0'f_0$ [^1]. SerÃ£o detalhadas as componentes da estatÃ­stica, sua derivaÃ§Ã£o e interpretaÃ§Ã£o, ressaltando as nuances e implicaÃ§Ãµes prÃ¡ticas para a modelagem de sÃ©ries temporais.

### Conceitos Fundamentais

Como estabelecido, o teste LM oferece uma abordagem formal para verificar a presenÃ§a de efeitos GARCH, complementando ferramentas informais [^1]. A estatÃ­stica do teste, $\xi_{LM}$, quantifica a significÃ¢ncia da inclusÃ£o de parÃ¢metros GARCH em um modelo condicionalmente heteroscedÃ¡stico.

A estatÃ­stica do teste LM para a hipÃ³tese nula $H_0: \omega_2 = 0$, onde $\omega_2$ representa os parÃ¢metros GARCH que estamos testando, Ã© definida como [^1]:

$$
\xi_{LM} = f_0'Z_0(Z_0'Z_0)^{-1}Z_0'f_0
$$

Vamos analisar cada componente desta estatÃ­stica:

1.  **O Vetor** $f_0$:

    Este vetor, $f_0 = (\epsilon_1^2h_1^{-1} - 1, ..., \epsilon_T^2h_T^{-1} - 1)'$, captura os desvios dos erros quadrÃ¡ticos normalizados em relaÃ§Ã£o ao valor esperado sob a hipÃ³tese nula [^1]. Cada elemento deste vetor quantifica a diferenÃ§a entre o erro quadrÃ¡tico normalizado observado ($\epsilon_t^2h_t^{-1}$) e o valor esperado sob a hipÃ³tese nula, que Ã© 1. Em outras palavras, $f_{0,t} = \epsilon_t^2h_t^{-1} - 1$ mede o quÃ£o bem a variÃ¢ncia condicional estimada sob a hipÃ³tese nula ($h_t$) se alinha com o erro quadrÃ¡tico observado ($\epsilon_t^2$). Se os valores de $f_{0,t}$ forem consistentemente grandes (em valor absoluto), isso sugere que a hipÃ³tese nula de ausÃªncia de efeitos GARCH pode ser falsa, e que a variÃ¢ncia condicional verdadeira pode ter uma dinÃ¢mica nÃ£o capturada pelo modelo sob a hipÃ³tese nula.

    Sob a hipÃ³tese nula, $\mathbb{E}[\epsilon_t^2| \psi_{t-1}] = h_t$, e portanto, $\mathbb{E}[\epsilon_t^2h_t^{-1}| \psi_{t-1}] = 1$. Assim, se a hipÃ³tese nula for verdadeira, o vetor $f_0$ deve ter uma mÃ©dia prÃ³xima de zero.

    > ðŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar, considere uma sÃ©rie temporal com $T = 3$. Suponha que sob a hipÃ³tese nula (e.g., um modelo ARCH(0) ou ausÃªncia de efeitos ARCH), estimamos $h_t = \hat{\alpha}_0 = 1.0$ para todos os *t*. Se observarmos $\epsilon_1^2 = 1.5$, $\epsilon_2^2 = 0.8$, e $\epsilon_3^2 = 1.2$, entÃ£o o vetor $f_0$ seria:
    >
    > $f_0 = (1.5/1.0 - 1, 0.8/1.0 - 1, 1.2/1.0 - 1)' = (0.5, -0.2, 0.2)'$.
    >
    > Este exemplo quantifica como os desvios dos erros quadrÃ¡ticos normalizados em relaÃ§Ã£o Ã  sua esperanÃ§a sÃ£o calculados, formando a base para o cÃ¡lculo da estatÃ­stica LM. Valores maiores (em magnitude) dos elementos de $f_0$ sugerem evidÃªncias contra a hipÃ³tese nula.
    >
    > ```python
    > import numpy as np
    >
    > # Dados do exemplo
    > T = 3
    > h_t_hat = np.array([1.0, 1.0, 1.0])
    > epsilon_squared = np.array([1.5, 0.8, 1.2])
    >
    > # Calcula f_0
    > f_0 = (epsilon_squared / h_t_hat) - 1
    >
    > print("Vetor f_0:", f_0)
    > ```

2.  **A Matriz** $Z_0$:

    A matriz $Z_0$ Ã© definida como $Z_0 = (\frac{\partial h_1}{\partial \omega}, \frac{\partial h_2}{\partial \omega}, \ldots, \frac{\partial h_T}{\partial \omega})'$ [^1], onde cada coluna representa as derivadas parciais da variÃ¢ncia condicional $h_t$ em relaÃ§Ã£o aos parÃ¢metros $\omega$, avaliadas sob a hipÃ³tese nula. Estas derivadas quantificam como a variÃ¢ncia condicional responde a pequenas mudanÃ§as nos parÃ¢metros sob a hipÃ³tese nula. Em outras palavras, $Z_0$ representa a sensibilidade da variÃ¢ncia condicional aos parÃ¢metros do modelo sob a hipÃ³tese nula. A transposta $Z_0'$ entÃ£o organiza estas derivadas por perÃ­odo *t*, formando uma matriz *T x k*, onde *k* Ã© o nÃºmero de parÃ¢metros em $\omega$.

    > ðŸ’¡ **Exemplo NumÃ©rico:** Para um modelo ARCH(1), a variÃ¢ncia condicional Ã© $h_t = \alpha_0 + \alpha_1 \epsilon_{t-1}^2$, e $\omega = (\alpha_0, \alpha_1)'$. As derivadas parciais sÃ£o:
    >
    > $\frac{\partial h_t}{\partial \alpha_0} = 1$
    >
    > $\frac{\partial h_t}{\partial \alpha_1} = \epsilon_{t-1}^2$
    >
    > Sob a hipÃ³tese nula de ausÃªncia de efeitos ARCH(1) (ou seja, $\alpha_1 = 0$), temos $\omega = \alpha_0$ e $h_t = \alpha_0$. Neste caso, $Z_0$ seria uma matriz de dimensÃµes *T x 1* (assumindo que estamos avaliando a derivada em relaÃ§Ã£o a $\alpha_0$), com todos os elementos iguais a 1, $Z_0 = (1, 1, ..., 1)'$.
    >
    > Assim, a matriz $Z_0$ captura a sensibilidade da variÃ¢ncia condicional aos parÃ¢metros do modelo sob a hipÃ³tese nula.
    >
    > ```python
    > import numpy as np
    >
    > # Dados do exemplo
    > T = 3
    >
    > # Z_0 para ARCH(1) sob H0: alpha_1 = 0
    > Z_0 = np.ones((T, 1))
    >
    > print("Matriz Z_0:\n", Z_0)
    > ```
    >
    > ðŸ’¡ **Exemplo NumÃ©rico Estendido:** Considere um modelo GARCH(1,1) da forma $h_t = \alpha_0 + \alpha_1 \epsilon_{t-1}^2 + \beta_1 h_{t-1}$. Sob a hipÃ³tese nula de que $\alpha_1 = \beta_1 = 0$, temos $h_t = \alpha_0$. O vetor de parÃ¢metros Ã© $\omega = (\alpha_0, \alpha_1, \beta_1)'$. As derivadas parciais sÃ£o:
    >
    > $\frac{\partial h_t}{\partial \alpha_0} = 1$
    >
    > $\frac{\partial h_t}{\partial \alpha_1} = \epsilon_{t-1}^2$
    >
    > $\frac{\partial h_t}{\partial \beta_1} = h_{t-1}$
    >
    > Sob a hipÃ³tese nula, $\alpha_1 = 0$ e $\beta_1 = 0$, portanto as derivadas parciais em relaÃ§Ã£o a $\alpha_1$ e $\beta_1$ sÃ£o 0. Logo, $Z_0$ Ã© uma matriz *T x 1* (assumindo que estamos avaliando a derivada em relaÃ§Ã£o a $\alpha_0$), com todos os elementos iguais a 1, $Z_0 = (1, 1, ..., 1)'$.
    > ```python
    > import numpy as np
    >
    > # Dados do exemplo
    > T = 3
    >
    > # Z_0 para GARCH(1,1) sob H0: alpha_1 = beta_1 = 0
    > Z_0 = np.ones((T, 1))
    >
    > print("Matriz Z_0:\n", Z_0)
    > ```

    **ProposiÃ§Ã£o 1:** Para um modelo GARCH(p,q), a matriz $Z_0$ pode ser construÃ­da iterativamente utilizando as derivadas parciais da variÃ¢ncia condicional em relaÃ§Ã£o aos parÃ¢metros do modelo, avaliadas sob a hipÃ³tese nula de ausÃªncia de efeitos GARCH.

    *EstratÃ©gia da Prova:* A prova envolve a aplicaÃ§Ã£o da regra da cadeia para calcular as derivadas parciais da variÃ¢ncia condicional em relaÃ§Ã£o a cada parÃ¢metro do modelo GARCH(p,q). Sob a hipÃ³tese nula, os parÃ¢metros GARCH sÃ£o iguais a zero, simplificando as derivadas e permitindo a construÃ§Ã£o iterativa da matriz $Z_0$.

    **Prova da ProposiÃ§Ã£o 1:**
    I. Considere um modelo GARCH(p,q) geral:
    $$h_t = \alpha_0 + \sum_{i=1}^{q} \alpha_i \epsilon_{t-i}^2 + \sum_{j=1}^{p} \beta_j h_{t-j}$$
    Onde $\alpha_i$ sÃ£o os coeficientes ARCH e $\beta_j$ sÃ£o os coeficientes GARCH.  O vetor de parÃ¢metros Ã© $\omega = (\alpha_0, \alpha_1, \ldots, \alpha_q, \beta_1, \ldots, \beta_p)'$.

    II. Sob a hipÃ³tese nula de ausÃªncia de efeitos GARCH, temos $\alpha_1 = \ldots = \alpha_q = \beta_1 = \ldots = \beta_p = 0$.  Portanto, $h_t = \alpha_0$ para todo $t$.

    III. Calcule as derivadas parciais de $h_t$ em relaÃ§Ã£o a cada parÃ¢metro em $\omega$:
    $$\frac{\partial h_t}{\partial \alpha_0} = 1$$
    $$\frac{\partial h_t}{\partial \alpha_i} = 0, \quad \text{para } i = 1, \ldots, q$$
    $$\frac{\partial h_t}{\partial \beta_j} = 0, \quad \text{para } j = 1, \ldots, p$$

    IV. Construa a matriz $Z_0$. Cada linha de $Z_0$ corresponde a um tempo $t$, e cada coluna corresponde Ã  derivada parcial de $h_t$ em relaÃ§Ã£o a um elemento de $\omega$, avaliada sob a hipÃ³tese nula.  Assim, $Z_0$ Ã© uma matriz $T \times (1+p+q)$.

    V. Devido Ã  hipÃ³tese nula, as derivadas parciais em relaÃ§Ã£o a $\alpha_i$ e $\beta_j$ sÃ£o zero.  Portanto, a matriz $Z_0$ terÃ¡ a seguinte forma:
    $$Z_0 = \begin{bmatrix}
    1 & 0 & \cdots & 0 \\
    1 & 0 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    1 & 0 & \cdots & 0
    \end{bmatrix}$$
    Esta Ã© uma matriz $T \times (1+p+q)$ onde a primeira coluna Ã© composta por 1s e as demais colunas sÃ£o compostas por 0s.

    VI. Podemos construir $Z_0$ iterativamente. Inicialize $Z_0$ como uma matriz de zeros de dimensÃ£o $T \times (1+p+q)$.  Em seguida, defina a primeira coluna de $Z_0$ como um vetor de 1s.
    Portanto, para um modelo GARCH(p,q) sob a hipÃ³tese nula, a matriz $Z_0$ Ã© construÃ­da iterativamente, e as derivadas parciais da variÃ¢ncia condicional em relaÃ§Ã£o aos parÃ¢metros do modelo, avaliadas sob a hipÃ³tese nula de ausÃªncia de efeitos GARCH, sÃ£o utilizadas. â– 

3.  **A Matriz** $(Z_0'Z_0)^{-1}$:

    Esta Ã© a inversa da matriz $Z_0'Z_0$. Esta matriz Ã© crucial para o teste LM pois fornece uma estimativa da matriz de covariÃ¢ncia dos parÃ¢metros sob a hipÃ³tese nula [^1]. A inversa de $Z_0'Z_0$ Ã© usada para ajustar a estatÃ­stica do teste pela variabilidade dos parÃ¢metros estimados sob a hipÃ³tese nula. Matrizes de covariÃ¢ncia menores indicam estimativas mais precisas dos parÃ¢metros e, portanto, um teste mais poderoso.

    > ðŸ’¡ **Exemplo NumÃ©rico:** No exemplo anterior com $Z_0 = (1, 1, ..., 1)'$, a matriz $Z_0'Z_0$ seria a soma dos quadrados dos elementos de $Z_0$, que Ã© simplesmente o nÃºmero de observaÃ§Ãµes *T*. Portanto, $(Z_0'Z_0)^{-1} = 1/T$.
    >
    > Este exemplo demonstra como a matriz $(Z_0'Z_0)^{-1}$ estÃ¡ relacionada Ã  precisÃ£o das estimativas sob a hipÃ³tese nula.
    >
    > ```python
    > import numpy as np
    >
    > # Dados do exemplo
    > T = 3
    > Z_0 = np.ones((T, 1))
    >
    > # Calcula Z_0'Z_0
    > Z_0_transpose_Z_0 = np.transpose(Z_0) @ Z_0
    >
    > # Calcula (Z_0'Z_0)^{-1}
    > Z_0_transpose_Z_0_inv = 1 / Z_0_transpose_Z_0
    >
    > print("(Z_0'Z_0)^{-1}:", Z_0_transpose_Z_0_inv)
    > ```

4.  **A EstatÃ­stica** $\xi_{LM}$:

    A estatÃ­stica $\xi_{LM} = f_0'Z_0(Z_0'Z_0)^{-1}Z_0'f_0$ combina as informaÃ§Ãµes dos desvios dos erros quadrÃ¡ticos normalizados ($f_0$) com a sensibilidade da variÃ¢ncia condicional aos parÃ¢metros sob a hipÃ³tese nula ($Z_0$) e ajusta para a variabilidade dos parÃ¢metros estimados ($(Z_0'Z_0)^{-1}$). A estatÃ­stica $\xi_{LM}$ quantifica a significÃ¢ncia da inclusÃ£o de parÃ¢metros GARCH em um modelo condicionalmente heteroscedÃ¡stico [^1]. Sob a hipÃ³tese nula, $\xi_{LM}$ segue uma distribuiÃ§Ã£o qui-quadrado com *r* graus de liberdade, onde *r* Ã© o nÃºmero de parÃ¢metros em $\omega_2$ [^1].

    Um valor alto da estatÃ­stica $\xi_{LM}$ indica que os desvios dos erros quadrÃ¡ticos normalizados ($f_0$) sÃ£o sistematicamente relacionados com as derivadas da variÃ¢ncia condicional em relaÃ§Ã£o aos parÃ¢metros ($Z_0$), e que essa relaÃ§Ã£o Ã© estatisticamente significativa apÃ³s ajustar pela variabilidade dos parÃ¢metros estimados ($(Z_0'Z_0)^{-1}$). Neste caso, rejeitarÃ­amos a hipÃ³tese nula de ausÃªncia de efeitos GARCH.

    > ðŸ’¡ **Exemplo NumÃ©rico:** Para um modelo ARCH(1) sob a hipÃ³tese nula $\alpha_1 = 0$, temos que $f_0$ mede o desvio dos erros quadrÃ¡ticos normalizados e $Z_0$ representa a sensibilidade da variÃ¢ncia condicional ao parÃ¢metro $\alpha_0$. Se a estatÃ­stica $\xi_{LM}$ for alta, isso indica que os desvios dos erros quadrÃ¡ticos normalizados estÃ£o relacionados com a sensibilidade da variÃ¢ncia condicional a $\alpha_0$, e que essa relaÃ§Ã£o Ã© estatisticamente significativa.
    >
    > ```python
    > import numpy as np
    > from scipy.stats import chi2
    >
    > # Dados dos exemplos anteriores
    > T = 3
    > h_t_hat = np.array([1.0, 1.0, 1.0])
    > epsilon_squared = np.array([1.5, 0.8, 1.2])
    > f_0 = (epsilon_squared / h_t_hat) - 1
    > Z_0 = np.ones((T, 1))
    > Z_0_transpose_Z_0_inv = 1 / T
    >
    > # Calcula a estatÃ­stica LM
    > xi_LM = f_0.T @ Z_0 @ Z_0_transpose_Z_0_inv @ Z_0.T @ f_0
    >
    > # Calcula o p-valor
    > r = 1  # Graus de liberdade (nÃºmero de parÃ¢metros restritos)
    > p_value = 1 - chi2.cdf(xi_LM, r)
    >
    > print("EstatÃ­stica LM:", xi_LM)
    > print("P-valor:", p_value)
    >
    > # InterpretaÃ§Ã£o
    > alpha = 0.05  # NÃ­vel de significÃ¢ncia
    > if p_value < alpha:
    >     print("Rejeitamos a hipÃ³tese nula de ausÃªncia de efeitos ARCH.")
    > else:
    >     print("NÃ£o rejeitamos a hipÃ³tese nula de ausÃªncia de efeitos ARCH.")
    > ```

    **Teorema 1:** A estatÃ­stica do Teste LM, $\xi_{LM}$, Ã© assintoticamente equivalente Ã  estatÃ­stica do teste da razÃ£o de verossimilhanÃ§as (LR) para a hipÃ³tese nula de ausÃªncia de efeitos GARCH.

    *EstratÃ©gia da Prova:* A prova envolve expandir a funÃ§Ã£o de log-verossimilhanÃ§a sob a hipÃ³tese alternativa (com efeitos GARCH) em torno das estimativas sob a hipÃ³tese nula (sem efeitos GARCH) usando uma expansÃ£o de Taylor de segunda ordem. Mostra-se que a estatÃ­stica LR resultante Ã© assintoticamente igual a $\xi_{LM}$.

   **Prova do Teorema 1:**

    I. Seja $L(\theta)$ a funÃ§Ã£o de log-verossimilhanÃ§a do modelo GARCH, onde $\theta = (\theta_1, \theta_2)$ Ã© o vetor de parÃ¢metros, com $\theta_1$ representando os parÃ¢metros estimados sob a hipÃ³tese nula e $\theta_2$ os parÃ¢metros GARCH que estamos testando (e que sÃ£o restritos a zero sob a hipÃ³tese nula).

    II. A estatÃ­stica do teste da razÃ£o de verossimilhanÃ§as (LR) Ã© dada por:
    $$LR = 2[L(\hat{\theta}) - L(\tilde{\theta})]$$
    Onde $\hat{\theta}$ sÃ£o as estimativas de mÃ¡xima verossimilhanÃ§a sob a hipÃ³tese alternativa (sem restriÃ§Ãµes) e $\tilde{\theta}$ sÃ£o as estimativas sob a hipÃ³tese nula ($\theta_2 = 0$).

    III. Expanda $L(\hat{\theta})$ em torno de $\tilde{\theta}$ usando uma expansÃ£o de Taylor de segunda ordem:
    $$L(\hat{\theta}) \approx L(\tilde{\theta}) + (\hat{\theta} - \tilde{\theta})' \nabla L(\tilde{\theta}) + \frac{1}{2} (\hat{\theta} - \tilde{\theta})' \nabla^2 L(\tilde{\theta}) (\hat{\theta} - \tilde{\theta})$$
    Onde $\nabla L(\tilde{\theta})$ Ã© o vetor gradiente (primeira derivada) e $\nabla^2 L(\tilde{\theta})$ Ã© a matriz Hessiana (segunda derivada) da funÃ§Ã£o de log-verossimilhanÃ§a, ambos avaliados em $\tilde{\theta}$.

    IV. Sob a hipÃ³tese nula, o vetor gradiente para os parÃ¢metros restritos $\theta_2$ Ã© zero no ponto de mÃ¡ximo (pelas condiÃ§Ãµes de primeira ordem): $\nabla_{\theta_2} L(\tilde{\theta}) = 0$. Portanto, o termo linear na expansÃ£o de Taylor se anula.

    V. A estatÃ­stica LR se torna:
    $$LR = 2[L(\hat{\theta}) - L(\tilde{\theta})] \approx (\hat{\theta} - \tilde{\theta})' [-\nabla^2 L(\tilde{\theta})] (\hat{\theta} - \tilde{\theta})$$
    Observe que $-\nabla^2 L(\tilde{\theta})$ Ã© a matriz de informaÃ§Ã£o de Fisher, que denotamos por $I(\tilde{\theta})$.

    VI. Particionando a matriz de informaÃ§Ã£o de Fisher e o vetor de parÃ¢metros, temos:
    $$I(\tilde{\theta}) = \begin{bmatrix}
    I_{11} & I_{12} \\
    I_{21} & I_{22}
    \end{bmatrix}, \quad (\hat{\theta} - \tilde{\theta}) = \begin{bmatrix}
    \hat{\theta}_1 - \tilde{\theta}_1 \\
    \hat{\theta}_2 - 0
    \end{bmatrix}$$
    Onde $I_{ij}$ sÃ£o os blocos da matriz de informaÃ§Ã£o correspondentes aos parÃ¢metros $\theta_i$ e $\theta_j$.

    VII. Sob a hipÃ³tese nula, a estimativa de mÃ¡xima verossimilhanÃ§a de $\theta_2$ Ã© assintoticamente:
    $$\hat{\theta}_2 \approx I_{22}^{-1} \nabla_{\theta_2} L(\tilde{\theta})$$
    Como $\nabla_{\theta_2} L(\tilde{\theta})$ Ã© o vetor $f_0$ e $I_{22}$ estÃ¡ relacionado a $Z_0'Z_0$, podemos escrever a estatÃ­stica LR como:
    $$LR \approx f_0'Z_0(Z_0'Z_0)^{-1}Z_0'f_0 = \xi_{LM}$$

    VIII. Portanto, a estatÃ­stica do Teste LR Ã© assintoticamente equivalente Ã  estatÃ­stica do Teste LM:
    $$LR \approx \xi_{LM}$$
    Isso completa a prova. â– 

A estatÃ­stica do teste LM tambÃ©m pode ser expressa como $\xi_{LM} = T \cdot R^2$, onde $R^2$ Ã© o coeficiente de correlaÃ§Ã£o mÃºltipla ao quadrado entre $f_0$ e $Z_0$ [^1]. Esta formulaÃ§Ã£o oferece uma interpretaÃ§Ã£o intuitiva: a estatÃ­stica do teste Ã© proporcional Ã  quantidade de variaÃ§Ã£o em $f_0$ que Ã© explicada por $Z_0$.

### ConclusÃ£o

Em conclusÃ£o, a estatÃ­stica do teste LM, $\xi_{LM} = f_0'Z_0(Z_0'Z_0)^{-1}Z_0'f_0$, Ã© uma ferramenta fundamental para testar a presenÃ§a de efeitos GARCH em modelos de sÃ©ries temporais. Compreender suas componentes e sua derivaÃ§Ã£o Ã© essencial para interpretar corretamente os resultados do teste e tomar decisÃµes informadas sobre a adequaÃ§Ã£o do modelo [^1]. A estatÃ­stica $\xi_{LM}$ quantifica a significÃ¢ncia da inclusÃ£o de parÃ¢metros GARCH em um modelo condicionalmente heteroscedÃ¡stico, considerando os desvios dos erros quadrÃ¡ticos normalizados, a sensibilidade da variÃ¢ncia condicional aos parÃ¢metros sob a hipÃ³tese nula, e a variabilidade dos parÃ¢metros estimados.

### ReferÃªncias

[^1]: Bollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. *Journal of Econometrics, 31*(3), 307-327.
<!-- END -->