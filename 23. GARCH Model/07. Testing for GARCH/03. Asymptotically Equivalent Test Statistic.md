## O Teste LM para Efeitos GARCH: Abordagem via $R^2$

### Introdu√ß√£o

Este cap√≠tulo d√° continuidade √† discuss√£o sobre o Teste de Multiplicador de Lagrange (LM) para detec√ß√£o de efeitos GARCH (Generalized Autoregressive Conditional Heteroskedasticity), explorando uma formula√ß√£o assintoticamente equivalente da estat√≠stica do teste, $\xi_{LM} = T \cdot R^2$ [^1]. Esta abordagem, baseada no coeficiente de determina√ß√£o ($R^2$), oferece uma interpreta√ß√£o intuitiva e facilita a aplica√ß√£o pr√°tica do teste. Ser√£o detalhados os fundamentos desta formula√ß√£o, sua rela√ß√£o com a estat√≠stica original, e exemplos pr√°ticos para ilustrar seu uso.

### Conceitos Fundamentais

Como apresentado nos cap√≠tulos anteriores, o teste LM oferece uma abordagem formal para verificar a presen√ßa de efeitos GARCH. A estat√≠stica original, $\xi_{LM} = f_0'Z_0(Z_0'Z_0)^{-1}Z_0'f_0$, quantifica a signific√¢ncia da inclus√£o de par√¢metros GARCH [^1]. Uma formula√ß√£o assintoticamente equivalente, e frequentemente mais pr√°tica, √© expressa em termos do coeficiente de determina√ß√£o ($R^2$) [^1]:

$$
\xi_{LM} = T \cdot R^2
$$

Onde:

*   $T$ √© o tamanho da amostra.
*   $R^2$ √© o coeficiente de correla√ß√£o m√∫ltipla ao quadrado entre o vetor $f_0$ e a matriz $Z_0$ [^1].

Esta formula√ß√£o da estat√≠stica do teste LM pode ser obtida a partir da realiza√ß√£o de uma regress√£o auxiliar. A regress√£o auxiliar consiste em regredir os elementos do vetor $f_0$ sobre as colunas da matriz $Z_0$. O $R^2$ obtido desta regress√£o √© ent√£o multiplicado pelo tamanho da amostra ($T$) para obter a estat√≠stica do teste LM.

1.  **O Coeficiente de Determina√ß√£o ($R^2$):**

    O $R^2$ na regress√£o auxiliar mede a propor√ß√£o da vari√¢ncia no vetor $f_0$ que √© explicada pela matriz $Z_0$ [^1]. Um $R^2$ alto indica que as colunas da matriz $Z_0$ conseguem explicar uma por√ß√£o significativa da variabilidade no vetor $f_0$, sugerindo que a hip√≥tese nula de aus√™ncia de efeitos GARCH √© falsa. Em outras palavras, se a matriz $Z_0$ (que cont√©m as derivadas da vari√¢ncia condicional em rela√ß√£o aos par√¢metros) conseguir prever os desvios dos erros quadr√°ticos normalizados (capturados em $f_0$), ent√£o temos evid√™ncias de que a vari√¢ncia condicional est√° mal especificada sob a hip√≥tese nula.

    > üí° **Exemplo Num√©rico:** Considere uma regress√£o linear simples onde estamos tentando explicar os valores de $f_0$ usando $Z_0$. Se obtivermos um $R^2$ de 0.60, isso significa que 60% da variabilidade em $f_0$ √© explicada por $Z_0$. Um $R^2$ t√£o alto sugere uma rela√ß√£o forte entre $f_0$ e $Z_0$, indicando que a inclus√£o de efeitos GARCH pode ser apropriada.
    >
    > ```python
    > import numpy as np
    > from sklearn.linear_model import LinearRegression
    > from sklearn.metrics import r2_score
    >
    > # Dados do exemplo (substitua com seus valores reais)
    > f_0 = np.array([0.5, -0.2, 0.2, 0.3, -0.1])  # Vetor f_0
    > Z_0 = np.array([[1], [1], [1], [1], [1]])  # Matriz Z_0 (exemplo simples)
    >
    > # Ajusta a regress√£o linear
    > model = LinearRegression()
    > model.fit(Z_0, f_0)
    >
    > # Faz as previs√µes
    > f_0_predicted = model.predict(Z_0)
    >
    > # Calcula o R-quadrado
    > r_squared = r2_score(f_0, f_0_predicted)
    >
    > print("R-quadrado:", r_squared)
    > ```

    **Teorema 1:** Sob a hip√≥tese nula de aus√™ncia de efeitos GARCH, a distribui√ß√£o assint√≥tica da estat√≠stica $\xi_{LM} = T \cdot R^2$ √© a mesma da estat√≠stica $\xi_{LM} = f_0'Z_0(Z_0'Z_0)^{-1}Z_0'f_0$.

    *Estrat√©gia da Prova:* A prova envolve mostrar que, sob a hip√≥tese nula, a regress√£o de $f_0$ sobre $Z_0$ converge para a mesma distribui√ß√£o assint√≥tica que a estat√≠stica original do teste LM. Isso √© alcan√ßado utilizando a teoria assint√≥tica de m√≠nimos quadrados ordin√°rios (OLS) e demonstrando a equival√™ncia entre a estat√≠stica $T \cdot R^2$ e a estat√≠stica original do teste LM.

    *Prova do Teorema 1:*

    I. Seja $f_0$ o vetor de res√≠duos normalizados e $Z_0$ a matriz de regressores, ambos definidos como anteriormente.

    II. A estat√≠stica do teste LM baseada em OLS √© obtida da regress√£o auxiliar:
        $$f_0 = Z_0 \beta + u$$
        onde $\beta$ √© o vetor de coeficientes e $u$ √© o vetor de erros.

    III. A estimativa de m√≠nimos quadrados ordin√°rios (OLS) para $\beta$ √©:
        $$\hat{\beta} = (Z_0'Z_0)^{-1}Z_0'f_0$$

    IV. O vetor de res√≠duos √©:
        $$\hat{u} = f_0 - Z_0 \hat{\beta} = f_0 - Z_0(Z_0'Z_0)^{-1}Z_0'f_0$$

    V. O $R^2$ da regress√£o auxiliar √© definido como:
        $$R^2 = 1 - \frac{\hat{u}'\hat{u}}{f_0'f_0}$$

    VI. Substituindo $\hat{u}$ na express√£o para $R^2$, obtemos:
        $$R^2 = 1 - \frac{[f_0 - Z_0(Z_0'Z_0)^{-1}Z_0'f_0]'[f_0 - Z_0(Z_0'Z_0)^{-1}Z_0'f_0]}{f_0'f_0}$$
        $$R^2 = \frac{f_0'Z_0(Z_0'Z_0)^{-1}Z_0'f_0}{f_0'f_0}$$
    (Nota: Esta etapa envolve simplificar a express√£o do $R^2$ expandindo o produto interno e usando as propriedades de proje√ß√µes ortogonais.)

    VII. Multiplicando ambos os lados da equa√ß√£o por $T$, obtemos a estat√≠stica do teste LM baseada em $R^2$:
        $$T \cdot R^2 = T \cdot \frac{f_0'Z_0(Z_0'Z_0)^{-1}Z_0'f_0}{f_0'f_0}$$

    VIII. Sob a hip√≥tese nula, $f_0'f_0$ converge para *T* √† medida que o tamanho da amostra aumenta. Portanto, podemos aproximar a estat√≠stica do teste LM como:

    $$T \cdot R^2 \approx \frac{T}{T} f_0'Z_0(Z_0'Z_0)^{-1}Z_0'f_0 = f_0'Z_0(Z_0'Z_0)^{-1}Z_0'f_0 = \xi_{LM}$$
    Portanto, $T \cdot R^2$ √© assintoticamente equivalente a $\xi_{LM}$, ou seja, ambas as estat√≠sticas convergem para a mesma distribui√ß√£o assint√≥tica. ‚ñ†

    > üí° **Exemplo Num√©rico:** Para ilustrar o Teorema 1, suponha que tenhamos os seguintes dados: $T = 100$, $f_0'f_0 = 98$, e $f_0'Z_0(Z_0'Z_0)^{-1}Z_0'f_0 = 15$. Ent√£o, $R^2 = 15/98 \approx 0.153$. A estat√≠stica do teste LM seria $T \cdot R^2 = 100 \cdot 0.153 = 15.3$, que √© aproximadamente igual a $f_0'Z_0(Z_0'Z_0)^{-1}Z_0'f_0 = 15$. Este exemplo num√©rico valida a equival√™ncia assint√≥tica entre as duas estat√≠sticas do teste.

    **Teorema 1.1:** Se o modelo estimado sob a hip√≥tese nula apresentar erros com distribui√ß√£o normal, ent√£o a estat√≠stica do teste $\xi_{LM}$ ter√° uma converg√™ncia mais r√°pida para a distribui√ß√£o qui-quadrado.

    *Estrat√©gia da Prova:* A prova baseia-se na demonstra√ß√£o de que a normalidade dos erros implica em momentos de ordem superior que convergem mais rapidamente para seus valores assint√≥ticos sob a hip√≥tese nula, resultando em uma melhor aproxima√ß√£o da distribui√ß√£o qui-quadrado para amostras finitas.

    *Prova do Teorema 1.1:*

    I. Assumindo normalidade dos erros, os momentos de quarta ordem dos res√≠duos normalizados podem ser expressos em termos dos momentos de segunda ordem.

    II. Sob a hip√≥tese nula e com erros normalmente distribu√≠dos, a matriz de informa√ß√£o observada converge mais rapidamente para a matriz de informa√ß√£o esperada.

    III. Esta converg√™ncia mais r√°pida implica que a estat√≠stica do teste LM, que √© uma fun√ß√£o da matriz de informa√ß√£o, tamb√©m converge mais rapidamente para sua distribui√ß√£o assint√≥tica qui-quadrado. Formalmente, isso pode ser demonstrado utilizando expans√µes de Edgeworth ou Cornish-Fisher para aproximar a distribui√ß√£o da estat√≠stica do teste LM.

    IV. Portanto, a estat√≠stica do teste $\xi_{LM}$ se aproxima da distribui√ß√£o qui-quadrado mais rapidamente quando os erros s√£o normalmente distribu√≠dos. ‚ñ†

    > üí° **Exemplo Num√©rico:** Para exemplificar o Teorema 1.1, imagine que simulamos dois conjuntos de dados: um com erros normais e outro com erros n√£o-normais (por exemplo, uma distribui√ß√£o t de Student com graus de liberdade baixos). Ao aplicar o teste LM em ambos os conjuntos de dados, com um tamanho de amostra moderado (digamos, $T=150$), observamos que a estat√≠stica do teste LM para os dados com erros normais se aproxima mais da distribui√ß√£o qui-quadrado te√≥rica em compara√ß√£o com os dados com erros n√£o-normais. Isso pode ser visualizado atrav√©s de histogramas das estat√≠sticas do teste LM para m√∫ltiplas simula√ß√µes, sobrepostos √† densidade qui-quadrado te√≥rica.
    >
    > ```python
    > import numpy as np
    > import statsmodels.api as sm
    > from scipy.stats import chi2, t
    > import matplotlib.pyplot as plt
    >
    > # Par√¢metros
    > T = 150
    > num_simulations = 500
    > degrees_of_freedom = 5  # Para a distribui√ß√£o t de Student
    >
    > # Simula√ß√µes para erros normais
    > lm_stats_normal = []
    > for _ in range(num_simulations):
    >     f_0 = np.random.normal(0, 1, T)
    >     Z_0 = np.random.normal(0, 1, (T, 1))
    >     model = sm.OLS(f_0, Z_0)
    >     results = model.fit()
    >     r_squared = results.rsquared
    >     lm_stat = T * r_squared
    >     lm_stats_normal.append(lm_stat)
    >
    > # Simula√ß√µes para erros n√£o-normais (t de Student)
    > lm_stats_t = []
    > for _ in range(num_simulations):
    >     f_0 = t.rvs(df=degrees_of_freedom, size=T)
    >     Z_0 = np.random.normal(0, 1, (T, 1))
    >     model = sm.OLS(f_0, Z_0)
    >     results = model.fit()
    >     r_squared = results.rsquared
    >     lm_stat = T * r_squared
    >     lm_stats_t.append(lm_stat)
    >
    > # Cria√ß√£o dos histogramas
    > plt.figure(figsize=(12, 6))
    >
    > # Histograma para erros normais
    > plt.subplot(1, 2, 1)
    > plt.hist(lm_stats_normal, bins=30, density=True, alpha=0.6, label='Erros Normais')
    >
    > # Sobreposi√ß√£o da densidade qui-quadrado
    > x = np.linspace(chi2.ppf(0.01, 1), chi2.ppf(0.99, 1), 100)
    > plt.plot(x, chi2.pdf(x, 1), 'r-', lw=2, label='Qui-quadrado (1 df)')
    >
    > plt.title('Estat√≠stica LM com Erros Normais')
    > plt.xlabel('Estat√≠stica LM')
    > plt.ylabel('Densidade')
    > plt.legend()
    >
    > # Histograma para erros n√£o-normais (t de Student)
    > plt.subplot(1, 2, 2)
    > plt.hist(lm_stats_t, bins=30, density=True, alpha=0.6, label='Erros t de Student')
    >
    > # Sobreposi√ß√£o da densidade qui-quadrado
    > x = np.linspace(chi2.ppf(0.01, 1), chi2.ppf(0.99, 1), 100)
    > plt.plot(x, chi2.pdf(x, 1), 'r-', lw=2, label='Qui-quadrado (1 df)')
    >
    > plt.title('Estat√≠stica LM com Erros t de Student')
    > plt.xlabel('Estat√≠stica LM')
    > plt.ylabel('Densidade')
    > plt.legend()
    >
    > plt.tight_layout()
    > plt.show()
    > ```

2.  **Graus de Liberdade:**

    Assim como na formula√ß√£o original, a estat√≠stica $\xi_{LM} = T \cdot R^2$ segue uma distribui√ß√£o qui-quadrado com *r* graus de liberdade sob a hip√≥tese nula [^1]. Aqui, *r* representa o n√∫mero de restri√ß√µes impostas pela hip√≥tese nula, o que corresponde ao n√∫mero de par√¢metros que estamos testando em $\omega_2$. Na pr√°tica, *r* √© o n√∫mero de colunas da matriz $Z_0$.

    > üí° **Exemplo Num√©rico:** Se estivermos testando a presen√ßa de efeitos ARCH(1), teremos um √∫nico par√¢metro de interesse ($\alpha_1$). Portanto, ter√≠amos *r* = 1 grau de liberdade.

### Aplica√ß√£o Pr√°tica

Para aplicar o teste LM usando a formula√ß√£o com $R^2$, siga os seguintes passos:

1.  **Estime o modelo sob a hip√≥tese nula:** Estime o modelo de s√©rie temporal sob a hip√≥tese nula de aus√™ncia de efeitos GARCH. Isso fornecer√° as estimativas dos par√¢metros necess√°rios para calcular o vetor $f_0$ e a matriz $Z_0$.
2.  **Calcule o vetor** $f_0$: Calcule o vetor $f_0$ usando os res√≠duos e as vari√¢ncias condicionais estimadas sob a hip√≥tese nula.
3.  **Calcule a matriz** $Z_0$: Calcule a matriz $Z_0$ das derivadas parciais da vari√¢ncia condicional em rela√ß√£o aos par√¢metros, avaliadas sob a hip√≥tese nula.
4.  **Realize a regress√£o auxiliar:** Regrida os elementos do vetor $f_0$ sobre as colunas da matriz $Z_0$.
5.  **Obtenha o $R^2$:** Obtenha o coeficiente de determina√ß√£o ($R^2$) da regress√£o auxiliar.
6.  **Calcule a estat√≠stica do teste:** Calcule a estat√≠stica do teste LM multiplicando o $R^2$ pelo tamanho da amostra ($T$): $\xi_{LM} = T \cdot R^2$.
7.  **Compare com o valor cr√≠tico:** Compare a estat√≠stica do teste com o valor cr√≠tico de uma distribui√ß√£o qui-quadrado com *r* graus de liberdade. Se a estat√≠stica do teste for maior que o valor cr√≠tico, rejeite a hip√≥tese nula de aus√™ncia de efeitos GARCH.

> üí° **Exemplo Num√©rico:** Suponha que, ap√≥s realizar a regress√£o auxiliar em uma amostra de tamanho $T = 200$, obtemos um $R^2 = 0.08$. A estat√≠stica do teste LM seria $\xi_{LM} = 200 \cdot 0.08 = 16$. Se estivermos testando para efeitos ARCH(1) (com 1 grau de liberdade), e o valor cr√≠tico da distribui√ß√£o qui-quadrado com 1 grau de liberdade no n√≠vel de signific√¢ncia de 5% for 3.84, rejeitar√≠amos a hip√≥tese nula de aus√™ncia de efeitos ARCH.

### C√≥digo Python para Implementa√ß√£o

O c√≥digo Python a seguir ilustra como implementar o teste LM usando a formula√ß√£o com $R^2$:

```python
import numpy as np
import statsmodels.api as sm
from scipy.stats import chi2

def lm_test_r2(f_0, Z_0, T):
    """
    Realiza o teste LM para efeitos GARCH usando a formula√ß√£o com R^2.

    Args:
        f_0 (np.ndarray): Vetor dos res√≠duos normalizados.
        Z_0 (np.ndarray): Matriz das derivadas da vari√¢ncia condicional.
        T (int): Tamanho da amostra.

    Returns:
        float: Estat√≠stica do teste LM.
        float: p-valor do teste.
    """

    # Regress√£o auxiliar
    model = sm.OLS(f_0, Z_0)
    results = model.fit()

    # Obt√©m o R-quadrado
    r_squared = results.rsquared

    # Calcula a estat√≠stica do teste LM
    lm_stat = T * r_squared

    # Calcula os graus de liberdade
    r = Z_0.shape[1]  # N√∫mero de colunas de Z_0

    # Calcula o p-valor
    p_value = 1 - chi2.cdf(lm_stat, r)

    return lm_stat, p_value

# Exemplo de uso (substitua com seus dados reais)
T = 200
f_0 = np.random.normal(0, 1, T) # Simula dados para f_0
Z_0 = np.random.normal(0, 1, (T, 1)) # Simula dados para Z_0

lm_stat, p_value = lm_test_r2(f_0, Z_0, T)

print("Estat√≠stica LM:", lm_stat)
print("P-valor:", p_value)
```

Este c√≥digo utiliza a biblioteca `statsmodels` para realizar a regress√£o auxiliar e obter o $R^2$. Em seguida, calcula a estat√≠stica do teste LM e o p-valor, permitindo a avalia√ß√£o da hip√≥tese nula.

**Corol√°rio 1**: Se a matriz $Z_0'Z_0$ for singular, ent√£o o $R^2$ da regress√£o auxiliar ser√° indefinido e o teste LM n√£o poder√° ser aplicado.

*Prova*: A prova decorre diretamente do fato de que o $R^2$ √© definido em termos da inversa da matriz $Z_0'Z_0$. Se $Z_0'Z_0$ √© singular, sua inversa n√£o existe, e portanto o $R^2$ e a estat√≠stica do teste LM n√£o podem ser calculados.

I. A estat√≠stica $R^2$ √© calculada utilizando a express√£o $R^2 = 1 - \frac{\hat{u}'\hat{u}}{f_0'f_0}$, onde $\hat{u}$ √© o vetor de res√≠duos da regress√£o auxiliar de $f_0$ em $Z_0$.

II. O vetor de res√≠duos $\hat{u}$ √© dado por $\hat{u} = f_0 - Z_0\hat{\beta}$, onde $\hat{\beta} = (Z_0'Z_0)^{-1}Z_0'f_0$ √© o estimador de m√≠nimos quadrados.

III. Se a matriz $Z_0'Z_0$ √© singular, ent√£o sua inversa $(Z_0'Z_0)^{-1}$ n√£o existe.

IV. Sem a inversa de $Z_0'Z_0$, o estimador $\hat{\beta}$ n√£o pode ser calculado, e consequentemente, o vetor de res√≠duos $\hat{u}$ e a estat√≠stica $R^2$ tamb√©m n√£o podem ser determinados.

V. Portanto, se a matriz $Z_0'Z_0$ for singular, o $R^2$ da regress√£o auxiliar ser√° indefinido e o teste LM n√£o poder√° ser aplicado. ‚ñ†

> üí° **Exemplo Num√©rico:** Suponha que $Z_0$ √© uma matriz $5 \times 2$ onde as duas colunas s√£o linearmente dependentes (e.g., uma coluna √© um m√∫ltiplo da outra). Ent√£o $Z_0'Z_0$ ser√° uma matriz $2 \times 2$ singular. Isso pode ser verificado calculando o determinante de $Z_0'Z_0$; se o determinante for zero, a matriz √© singular e a regress√£o auxiliar n√£o pode ser realizada.
>
> ```python
> import numpy as np
>
> # Matriz Z_0 com colunas linearmente dependentes
> Z_0 = np.array([[1, 2], [2, 4], [3, 6], [4, 8], [5, 10]])
>
> # Calcula Z_0'Z_0
> Z_0_transpose_Z_0 = np.transpose(Z_0) @ Z_0
>
> # Calcula o determinante
> determinant = np.linalg.det(Z_0_transpose_Z_0)
>
> print("Determinante de Z_0'Z_0:", determinant)
>
> if np.isclose(determinant, 0):
>     print("A matriz Z_0'Z_0 √© singular.")
> else:
>     print("A matriz Z_0'Z_0 n√£o √© singular.")
> ```

**Proposi√ß√£o 1:** A estat√≠stica do teste LM baseada em $R^2$ √© invariante a transforma√ß√µes lineares n√£o singulares da matriz $Z_0$.

*Prova:*
Seja $Z_1 = Z_0 A$, onde $A$ √© uma matriz n√£o singular. Ent√£o, a estat√≠stica do teste LM usando $Z_1$ em vez de $Z_0$ √© baseada no $R^2$ da regress√£o de $f_0$ em $Z_1$. O $R^2$ desta regress√£o √© dado por:

$R^2 = \frac{f_0'Z_1(Z_1'Z_1)^{-1}Z_1'f_0}{f_0'f_0}$

Substituindo $Z_1 = Z_0 A$:

$R^2 = \frac{f_0'Z_0A(A'Z_0'Z_0A)^{-1}A'Z_0'f_0}{f_0'f_0}$

$R^2 = \frac{f_0'Z_0A(A^{-1}(Z_0'Z_0)^{-1}(A')^{-1})A'Z_0'f_0}{f_0'f_0}$

$R^2 = \frac{f_0'Z_0(Z_0'Z_0)^{-1}Z_0'f_0}{f_0'f_0}$

Este √© o mesmo $R^2$ obtido pela regress√£o de $f_0$ em $Z_0$. Portanto, a estat√≠stica do teste LM, $T \cdot R^2$, √© invariante a transforma√ß√µes lineares n√£o singulares de $Z_0$. ‚ñ†

> üí° **Exemplo Num√©rico:** Seja $Z_0 = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$ e $A = [2]$. Ent√£o $Z_1 = Z_0A = \begin{bmatrix} 2 \\ 4 \\ 6 \end{bmatrix}$. Regredir $f_0$ em $Z_0$ e $f_0$ em $Z_1$ resultar√° no mesmo $R^2$, demonstrando a invari√¢ncia da estat√≠stica do teste LM.
>
> ```python
> import numpy as np
> import statsmodels.api as sm
>
> # Dados de exemplo
> f_0 = np.array([1, 2, 3])
> Z_0 = np.array([[1], [2], [3]])
> A = np.array([[2]])
> Z_1 = Z_0 @ A
>
> # Regress√£o com Z_0
> model_0 = sm.OLS(f_0, Z_0)
> results_0 = model_0.fit()
> r_squared_0 = results_0.rsquared
>
> # Regress√£o com Z_1
> model_1 = sm.OLS(f_0, Z_1)
> results_1 = model_1.fit()
> r_squared_1 = results_1.rsquared
>
> print("R^2 com Z_0:", r_squared_0)
> print("R^2 com Z_1:", r_squared_1)
>
> # Verifica se os R^2 s√£o iguais
> print("Os R^2 s√£o iguais:", np.isclose(r_squared_0, r_squared_1))
> ```

### Conclus√£o

A formula√ß√£o do Teste de Multiplicador de Lagrange (LM) baseada no coeficiente de determina√ß√£o ($R^2$), $\xi_{LM} = T \cdot R^2$, oferece uma abordagem alternativa e intuitiva para detec√ß√£o de efeitos GARCH. Esta formula√ß√£o simplifica a aplica√ß√£o pr√°tica do teste e facilita a interpreta√ß√£o dos resultados. Ao realizar uma regress√£o auxiliar e obter o $R^2$, podemos quantificar a propor√ß√£o da varia√ß√£o nos res√≠duos normalizados que √© explicada pelas derivadas da vari√¢ncia condicional, fornecendo evid√™ncias para a rejei√ß√£o ou n√£o da hip√≥tese nula de aus√™ncia de efeitos GARCH [^1].

### Refer√™ncias

[^1]: Bollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. *Journal of Econometrics, 31*(3), 307-327.
<!-- END -->