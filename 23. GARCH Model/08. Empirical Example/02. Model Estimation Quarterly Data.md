Wie bereits eingef√ºhrt, konzentriert sich dieser Abschnitt auf die Sch√§tzung eines Modells f√ºr die Inflationsunsicherheit in den Vereinigten Staaten unter Verwendung des GARCH-Modells. Aufbauend auf den vorherigen Abschnitten werden hier die spezifischen Aspekte der Datengrundlage und der angewandten Sch√§tzmethode detailliert betrachtet [^1].

Das Modell wird anhand viertelj√§hrlicher Daten aus dem Zeitraum 1948.2 bis 1983.4 gesch√§tzt. Dies ergibt eine Stichprobengr√∂√üe von insgesamt 143 Beobachtungen [^1]. Die Sch√§tzung erfolgt mit der Ordinary Least Squares (OLS)-Methode [^1]. Im Folgenden werden die Details der OLS-Sch√§tzung erl√§utert.

**Die OLS Sch√§tzung**

Die Ordinary Least Squares (OLS)-Methode ist eine weit verbreitete Technik zur Sch√§tzung der Parameter eines linearen Regressionsmodells [^1]. Das Ziel der OLS-Sch√§tzung ist es, die Summe der quadrierten Differenzen zwischen den beobachteten und den vorhergesagten Werten zu minimieren [^1]. Mathematisch ausgedr√ºckt bedeutet dies, dass die Parameter $\beta$ so gew√§hlt werden, dass die folgende Zielfunktion minimiert wird:

$$
\min_\beta \sum_{t=1}^{T} (y_t - x_t'\beta)^2
$$

wobei:
*   $y_t$ der beobachtete Wert der abh√§ngigen Variablen zum Zeitpunkt $t$ ist,
*   $x_t$ ein Vektor der beobachteten Werte der unabh√§ngigen Variablen zum Zeitpunkt $t$ ist,
*   $\beta$ ein Vektor der zu sch√§tzenden Parameter ist, und
*   $T$ die Anzahl der Beobachtungen ist.

Im Kontext des aktuellen Modells ist $y_t = \pi_t$ (die Inflationsrate) und $x_t$ enth√§lt die verz√∂gerten Werte von $\pi_t$ [^1].

> üí° **Exemplo Num√©rico:** Betrachten wir ein vereinfachtes Modell mit nur einem Lag:
>
> $$
> \pi_t = \beta_0 + \beta_1 \pi_{t-1} + \epsilon_t
> $$
>
> Um $\beta_0$ und $\beta_1$ mit OLS zu sch√§tzen, w√ºrden wir die Summe der quadrierten Residuen minimieren:
>
> $$
> \min_{\beta_0, \beta_1} \sum_{t=2}^{T} (\pi_t - \beta_0 - \beta_1 \pi_{t-1})^2
> $$
>
> Die OLS-Sch√§tzer f√ºr $\beta_0$ und $\beta_1$ sind gegeben durch:
>
> $$
> \hat{\beta}_1 = \frac{\sum_{t=2}^{T} (\pi_t - \bar{\pi})(\pi_{t-1} - \bar{\pi}_{-1})}{\sum_{t=2}^{T} (\pi_{t-1} - \bar{\pi}_{-1})^2}
> $$
>
> $$
> \hat{\beta}_0 = \bar{\pi} - \hat{\beta}_1 \bar{\pi}_{-1}
> $$
>
> wobei $\bar{\pi}$ der Mittelwert von $\pi_t$ und $\bar{\pi}_{-1}$ der Mittelwert von $\pi_{t-1}$ ist.
>
> Konkret nehmen wir an, wir haben die folgenden Inflationsdaten f√ºr f√ºnf aufeinanderfolgende Quartale (vereinfacht): $\pi = [2, 3, 5, 4, 6]$.  Wir m√∂chten $\pi_t$ auf $\pi_{t-1}$ regressieren.
>
> $\pi_t$: [3, 5, 4, 6]
>
> $\pi_{t-1}$: [2, 3, 5, 4]
>
> $\bar{\pi} = (3+5+4+6)/4 = 4.5$
>
> $\bar{\pi}_{-1} = (2+3+5+4)/4 = 3.5$
>
> $\hat{\beta}_1 = \frac{(3-4.5)(2-3.5) + (5-4.5)(3-3.5) + (4-4.5)(5-3.5) + (6-4.5)(4-3.5)}{(2-3.5)^2 + (3-3.5)^2 + (5-3.5)^2 + (4-3.5)^2} = \frac{2.25 - 0.25 - 0.75 + 0.75}{2.25 + 0.25 + 2.25 + 0.25} = \frac{2}{5} = 0.4$
>
> $\hat{\beta}_0 = 4.5 - 0.4 * 3.5 = 4.5 - 1.4 = 3.1$
>
> Daher ist die gesch√§tzte Regressionsgleichung: $\pi_t = 3.1 + 0.4\pi_{t-1}$. Das bedeutet, dass, wenn die Inflation im vorherigen Quartal um 1 Prozentpunkt steigt, wir erwarten, dass die Inflation im aktuellen Quartal um 0.4 Prozentpunkte steigt, zus√§tzlich zu einem Basisniveau von 3.1 Prozent.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Inflationsdaten
> pi = np.array([2, 3, 5, 4, 6])
>
> # Berechnung der Mittelwerte
> pi_mean = np.mean(pi[1:])
> pi_lag_mean = np.mean(pi[:-1])
>
> # Berechnung von beta_1
> beta_1_numerator = np.sum((pi[1:] - pi_mean) * (pi[:-1] - pi_lag_mean))
> beta_1_denominator = np.sum((pi[:-1] - pi_lag_mean)**2)
> beta_1 = beta_1_numerator / beta_1_denominator
>
> # Berechnung von beta_0
> beta_0 = pi_mean - beta_1 * pi_lag_mean
>
> print(f"Gesch√§tztes beta_0: {beta_0}")
> print(f"Gesch√§tztes beta_1: {beta_1}")
>
> # Visualisierung
> plt.figure(figsize=(10, 6))
> plt.scatter(pi[:-1], pi[1:], label="Datenpunkte")
>
> # Regressionslinie
> x = np.linspace(min(pi[:-1]), max(pi[:-1]), 100)
> y = beta_0 + beta_1 * x
> plt.plot(x, y, color='red', label=f'Regressionslinie: y = {beta_0:.2f} + {beta_1:.2f}x')
>
> plt.title("OLS-Sch√§tzung der Inflation auf ihren Lag")
> plt.xlabel("Inflation im vorherigen Quartal (œÄ_{t-1})")
> plt.ylabel("Inflation im aktuellen Quartal (œÄ_t)")
> plt.grid(True)
> plt.legend()
> plt.show()
> ```

**Teorema 1 (Gauss-Markov Theorem)** Unter den Annahmen des klassischen linearen Regressionsmodells (Linearit√§t, Exogenit√§t, Homoskedastizit√§t und keine Autokorrelation der Fehlerterme) sind die OLS-Sch√§tzer die besten linearen erwartungstreuen Sch√§tzer (BLUE).

**Corol√°rio 1.1** Wenn die Fehlerterme zus√§tzlich normalverteilt sind, dann sind die OLS-Sch√§tzer auch die Maximum-Likelihood-Sch√§tzer (MLE) und somit effizient.

*Beweis (Corol√°rio 1.1):* Wir beweisen, dass unter der Annahme normalverteilter Fehlerterme die OLS-Sch√§tzer Maximum-Likelihood-Sch√§tzer (MLE) sind.

I. Angenommen, die Fehlerterme $\epsilon_t$ sind unabh√§ngig und identisch normalverteilt mit einem Mittelwert von 0 und einer konstanten Varianz $\sigma^2$, d. h. $\epsilon_t \sim N(0, \sigma^2)$.

II. Die Wahrscheinlichkeitsfunktion f√ºr eine einzelne Beobachtung $y_t$ ist gegeben durch:
    $$
    L(y_t; \beta, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_t - x_t'\beta)^2}{2\sigma^2}\right)
    $$

III. Die Likelihood-Funktion f√ºr die gesamte Stichprobe (unter der Annahme unabh√§ngiger Beobachtungen) ist das Produkt der Likelihood-Funktionen f√ºr jede Beobachtung:
     $$
     L(\mathbf{y}; \beta, \sigma^2) = \prod_{t=1}^{T} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_t - x_t'\beta)^2}{2\sigma^2}\right)
     $$

IV. Die Log-Likelihood-Funktion ist:
     $$
     \log L(\mathbf{y}; \beta, \sigma^2) = -\frac{T}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=1}^{T} (y_t - x_t'\beta)^2
     $$

V. Um die MLE-Sch√§tzer f√ºr $\beta$ zu finden, maximieren wir die Log-Likelihood-Funktion in Bezug auf $\beta$. Da der erste Term nicht von $\beta$ abh√§ngt, entspricht die Maximierung der Log-Likelihood-Funktion der Minimierung des zweiten Terms, der die Summe der quadrierten Residuen ist:
    $$
    \max_\beta \log L(\mathbf{y}; \beta, \sigma^2) \Leftrightarrow \min_\beta \sum_{t=1}^{T} (y_t - x_t'\beta)^2
    $$

VI. Dies ist genau die Zielfunktion der OLS-Sch√§tzung. Daher sind die OLS-Sch√§tzer unter der Annahme normalverteilter Fehlerterme auch die Maximum-Likelihood-Sch√§tzer. ‚ñ†

**Standardfehler**

Die Genauigkeit der OLS-Sch√§tzungen wird durch ihre Standardfehler quantifiziert [^1]. Die Standardfehler geben ein Ma√ü f√ºr die Variabilit√§t der Sch√§tzungen an [^1]. Kleine Standardfehler deuten auf pr√§zisere Sch√§tzungen hin, w√§hrend gro√üe Standardfehler auf gr√∂√üere Unsicherheit hinweisen [^1].

Die Standardfehler der OLS-Sch√§tzungen werden √ºblicherweise wie folgt berechnet:

$$
\text{SE}(\hat{\beta}) = \sqrt{\text{diag}((X'X)^{-1} \hat{\sigma}^2)}
$$

wobei:
*   $X$ die Matrix der unabh√§ngigen Variablen ist,
*   $\hat{\sigma}^2 = \frac{1}{T-k} \sum_{t=1}^{T} (y_t - x_t'\hat{\beta})^2$ die gesch√§tzte Varianz des Fehlers ist, und
*   $k$ die Anzahl der Parameter im Modell ist.

Es ist wichtig zu beachten, dass die hier gezeigten Standardfehler unter der Annahme einer Homoskedastizit√§t (konstante Varianz) berechnet werden. Dies kann jedoch in vielen √∂konomischen Zeitreihen nicht der Fall sein, was die Verwendung heteroskedastie-konsistenter Standardfehler erforderlich macht [^1].

> üí° **Exemplo Num√©rico:** Um die Berechnung der Standardfehler zu veranschaulichen, nehmen wir an, wir sch√§tzen das gleiche einfache Inflationsmodell wie zuvor: $\pi_t = \beta_0 + \beta_1 \pi_{t-1} + \epsilon_t$. Wir haben unsere Beispiel-Inflationsdaten von vorher: $\pi = [2, 3, 5, 4, 6]$.
>
> $\text{X} = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 5 \\ 1 & 4 \end{bmatrix}$, $\text{y} = \begin{bmatrix} 3 \\ 5 \\ 4 \\ 6 \end{bmatrix}$
>
> Wir haben bereits gesch√§tzt: $\hat{\beta} = \begin{bmatrix} 3.1 \\ 0.4 \end{bmatrix}$
>
> 1.  **Berechne die Residuen:** $\hat{\epsilon} = y - X\hat{\beta}$
>
> $\hat{\epsilon} = \begin{bmatrix} 3 \\ 5 \\ 4 \\ 6 \end{bmatrix} - \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 5 \\ 1 & 4 \end{bmatrix} \begin{bmatrix} 3.1 \\ 0.4 \end{bmatrix} = \begin{bmatrix} 3 \\ 5 \\ 4 \\ 6 \end{bmatrix} - \begin{bmatrix} 3.9 \\ 4.3 \\ 5.1 \\ 4.7 \end{bmatrix} = \begin{bmatrix} -0.9 \\ 0.7 \\ -1.1 \\ 1.3 \end{bmatrix}$
>
> 2.  **Berechne die gesch√§tzte Fehlervarianz:** $\hat{\sigma}^2 = \frac{1}{T-k} \sum_{t=1}^{T} \hat{\epsilon}_t^2$
>
> $\hat{\sigma}^2 = \frac{(-0.9)^2 + (0.7)^2 + (-1.1)^2 + (1.3)^2}{4 - 2} = \frac{0.81 + 0.49 + 1.21 + 1.69}{2} = \frac{4.2}{2} = 2.1$
>
> 3.  **Berechne $(X'X)^{-1}$:**
>
> $X'X = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 2 & 3 & 5 & 4 \end{bmatrix} \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 5 \\ 1 & 4 \end{bmatrix} = \begin{bmatrix} 4 & 14 \\ 14 & 54 \end{bmatrix}$
>
> $(X'X)^{-1} = \frac{1}{(4*54 - 14*14)} \begin{bmatrix} 54 & -14 \\ -14 & 4 \end{bmatrix} = \frac{1}{216 - 196} \begin{bmatrix} 54 & -14 \\ -14 & 4 \end{bmatrix} = \frac{1}{20} \begin{bmatrix} 54 & -14 \\ -14 & 4 \end{bmatrix} = \begin{bmatrix} 2.7 & -0.7 \\ -0.7 & 0.2 \end{bmatrix}$
>
> 4.  **Berechne die Standardfehler:** $\text{SE}(\hat{\beta}) = \sqrt{\text{diag}((X'X)^{-1} \hat{\sigma}^2)}$
>
> $\text{SE}(\hat{\beta}) = \sqrt{\text{diag}(\begin{bmatrix} 2.7 & -0.7 \\ -0.7 & 0.2 \end{bmatrix} * 2.1)} = \sqrt{\text{diag}(\begin{bmatrix} 5.67 & -1.47 \\ -1.47 & 0.42 \end{bmatrix})} = \begin{bmatrix} \sqrt{5.67} \\ \sqrt{0.42} \end{bmatrix} = \begin{bmatrix} 2.38 \\ 0.65 \end{bmatrix}$
>
> Die Standardfehler sind also SE($\hat{\beta}_0$) = 2.38 und SE($\hat{\beta}_1$) = 0.65.  Dies gibt uns ein Gef√ºhl f√ºr die Unsicherheit bei der Sch√§tzung unserer Koeffizienten.  Ein gr√∂√üerer Standardfehler bedeutet eine gr√∂√üere Unsicherheit.
>
> ```python
> import numpy as np
> from numpy.linalg import inv
>
> # Daten
> X = np.array([[1, 2], [1, 3], [1, 5], [1, 4]])
> y = np.array([3, 5, 4, 6])
>
> # OLS-Sch√§tzung (wie zuvor)
> beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
>
> # Residuen berechnen
> epsilon_hat = y - X @ beta_hat
>
> # Gesch√§tzte Fehlervarianz
> sigma_hat_squared = np.sum(epsilon_hat**2) / (len(y) - X.shape[1])
>
> # (X'X)^-1 berechnen
> XTX_inv = np.linalg.inv(X.T @ X)
>
> # Standardfehler berechnen
> SE = np.sqrt(np.diag(XTX_inv * sigma_hat_squared))
>
> print(f"Gesch√§tzte Koeffizienten: {beta_hat}")
> print(f"Gesch√§tzte Standardfehler: {SE}")
> ```

**Heteroskedastie-konsistente Standardfehler**
In der Gegenwart von Heteroskedastizit√§t sind die OLS-Sch√§tzer selbst weiterhin unverzerrt und konsistent, aber die herk√∂mmlichen Standardfehler sind nicht mehr g√ºltig [^1]. Um diesem Problem zu begegnen, werden h√§ufig heteroskedastie-konsistente Standardfehler verwendet [^1]. Diese Standardfehler sind robust gegen√ºber Heteroskedastizit√§t und liefern zuverl√§ssigere Inferenz [^1].

Eine g√§ngige Methode zur Berechnung heteroskedastie-konsistenter Standardfehler ist der White-Sch√§tzer, der wie folgt definiert ist:

$$
\text{SE}_{\text{HC}}(\hat{\beta}) = \sqrt{\text{diag}((X'X)^{-1} \left(\sum_{t=1}^{T} x_t x_t' \hat{\epsilon}_t^2 \right) (X'X)^{-1})}
$$

wobei $\hat{\epsilon}_t = y_t - x_t'\hat{\beta}$ die gesch√§tzten Residuen sind.

> üí° **Exemplo Num√©rico:** Nehmen wir an, wir haben folgende Matrix $X'X$:
>
> $X'X = \begin{bmatrix} 143 & 0 \\ 0 & 100 \end{bmatrix}$
>
> Daraus ergibt sich:
>
> $(X'X)^{-1} = \begin{bmatrix} 1/143 & 0 \\ 0 & 1/100 \end{bmatrix} = \begin{bmatrix} 0.00699 & 0 \\ 0 & 0.01 \end{bmatrix}$
>
> Nehmen wir an $\hat{\sigma}^2 = 0.3$, dann:
>
> $\text{SE}(\hat{\beta}) = \sqrt{\text{diag}((X'X)^{-1} \hat{\sigma}^2)} = \sqrt{\text{diag}(\begin{bmatrix} 0.00699 & 0 \\ 0 & 0.01 \end{bmatrix} \cdot 0.3)} = \sqrt{\text{diag}(\begin{bmatrix} 0.0021 & 0 \\ 0 & 0.003 \end{bmatrix})} = \begin{bmatrix} 0.0458 & 0 \\ 0 & 0.0548 \end{bmatrix}$
>
> Angenommen, $\sum_{t=1}^{T} x_t x_t' \hat{\epsilon}_t^2 = \begin{bmatrix} 50 & 0 \\ 0 & 40 \end{bmatrix}$
>
> $SE_{HC}(\hat{\beta}) = \sqrt{\text{diag}((X'X)^{-1} (\sum_{t=1}^{T} x_t x_t' \hat{\epsilon}_t^2) (X'X)^{-1})} = \sqrt{\text{diag}(\begin{bmatrix} 0.00699 & 0 \\ 0 & 0.01 \end{bmatrix} \begin{bmatrix} 50 & 0 \\ 0 & 40 \end{bmatrix} \begin{bmatrix} 0.00699 & 0 \\ 0 & 0.01 \end{bmatrix})} = \sqrt{\text{diag}(\begin{bmatrix} 0.3495 & 0 \\ 0 & 0.4 \end{bmatrix} \begin{bmatrix} 0.00699 & 0 \\ 0 & 0.01 \end{bmatrix})} = \sqrt{\text{diag}(\begin{bmatrix} 0.0024 & 0 \\ 0 & 0.004 \end{bmatrix})} = \begin{bmatrix} 0.0489 & 0 \\ 0 & 0.0632 \end{bmatrix}$
>
> Betrachten wir die Inflation von oben, aber nehmen wir an, es liegt Heteroskedastizit√§t vor, und wir m√ºssen White's Heteroskedastie-konsistente Standardfehler verwenden.
>
> Zuerst m√ºssen wir $x_t x_t' \hat{\epsilon}_t^2$ f√ºr jede Beobachtung berechnen und dann summieren.  Zur Erinnerung: $X = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 5 \\ 1 & 4 \end{bmatrix}$, $\hat{\epsilon} = \begin{bmatrix} -0.9 \\ 0.7 \\ -1.1 \\ 1.3 \end{bmatrix}$.
>
> $\sum_{t=1}^{4} x_t x_t' \hat{\epsilon}_t^2 =  \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 5 \\ 1 & 4 \end{bmatrix} diag((-0.9)^2, (0.7)^2, (-1.1)^2, (1.3)^2) \begin{bmatrix} 1 & 1 & 1 & 1 \\ 2 & 3 & 5 & 4 \end{bmatrix} = \begin{bmatrix} 4.43 & 14.11 \\ 14.11 & 46.71 \end{bmatrix}$
>
> Dann k√∂nnen wir die heteroskedastie-konsistenten Standardfehler berechnen:
>
> $\text{SE}_{\text{HC}}(\hat{\beta}) = \sqrt{\text{diag}((X'X)^{-1} \left(\sum_{t=1}^{T} x_t x_t' \hat{\epsilon}_t^2 \right) (X'X)^{-1})} = \sqrt{\text{diag}(\begin{bmatrix} 2.7 & -0.7 \\ -0.7 & 0.2 \end{bmatrix} \begin{bmatrix} 4.43 & 14.11 \\ 14.11 & 46.71 \end{bmatrix} \begin{bmatrix} 2.7 & -0.7 \\ -0.7 & 0.2 \end{bmatrix})} = \begin{bmatrix} 2.74 \\ 0.79 \end{bmatrix}$
>
> Vergleicht man diese mit den vorherigen Standardfehlern (2.38, 0.65), so sind die heteroskedastie-konsistenten Standardfehler etwas gr√∂√üer.  Dies deutet darauf hin, dass die Ber√ºcksichtigung von Heteroskedastizit√§t zu konservativeren Schlussfolgerungen f√ºhren kann (d. h. breitere Konfidenzintervalle).
>
> ```python
> import numpy as np
> from numpy.linalg import inv
>
> # Daten (wie zuvor)
> X = np.array([[1, 2], [1, 3], [1, 5], [1, 4]])
> y = np.array([3, 5, 4, 6])
>
> # OLS-Sch√§tzung (wie zuvor)
> beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
>
> # Residuen berechnen
> epsilon_hat = y - X @ beta_hat
>
> # White's Heteroskedastie-konsistente Varianz-Kovarianzmatrix berechnen
> T = len(y)
> k = X.shape[1]
> XTX_inv = np.linalg.inv(X.T @ X)
> V_HC = XTX_inv @ (X.T @ np.diag(epsilon_hat**2) @ X) @ XTX_inv
>
> # Heteroskedastie-konsistente Standardfehler berechnen
> SE_HC = np.sqrt(np.diag(V_HC))
>
> print(f"Heteroskedastie-konsistente Standardfehler: {SE_HC}")
> ```

Die Verwendung von OLS zur Sch√§tzung von Modellen mit heteroskedastischen Fehlern ist nicht optimal, aber weit verbreitet [^1]. Die Maximum-Likelihood-Sch√§tzung des gesamten GARCH-Modells ist effizienter [^1].

**Teorema 2 (Konsistenz der OLS-Sch√§tzer)** Unter milden Regularit√§tsbedingungen, insbesondere wenn die Fehlerterme einen bedingten Mittelwert von Null und eine endliche Varianz haben und die Regressoren nicht perfekt kollinear sind, konvergieren die OLS-Sch√§tzer in Wahrscheinlichkeit gegen die wahren Parameterwerte, wenn die Stichprobengr√∂√üe gegen unendlich geht.

**Proposi√ß√£o 1 (Eigenschaften von White's Heteroskedastie-konsistentem Sch√§tzer)** Der White-Sch√§tzer ist asymptotisch √§quivalent zum OLS-Sch√§tzer unter Homoskedastizit√§t, aber er ist robust gegen√ºber Heteroskedastizit√§t unbekannter Form.

**Lema 1 (Bedeutung von Lag-L√§nge)** Die Wahl der optimalen Lag-L√§nge f√ºr die Inflation in der Regressionsgleichung ist entscheidend. Zu kurze Lag-L√§ngen k√∂nnen zu ausgelassenen Variablen f√ºhren, w√§hrend zu lange Lag-L√§ngen zu unn√∂tigem Verlust von Freiheitsgraden und m√∂glicherweise zu Multikollinearit√§t f√ºhren k√∂nnen. Informationskriterien wie das Akaike Information Criterion (AIC) oder das Bayesian Information Criterion (BIC) k√∂nnen verwendet werden, um die optimale Lag-L√§nge zu bestimmen.

### Datenquelle

Die Daten f√ºr den impliziten GNP-Deflator (GD) stammen aus der Citibank Economic Database und dem U.S. Department of Commerce, Survey of Current Business, Vol. 64, No. 9, September 1984 [^1].

### Zusammenfassung
Dieser Abschnitt hat die Datengrundlage und die Sch√§tzmethode f√ºr das Modell der Inflationsunsicherheit beschrieben [^1]. Es wurde die OLS-Sch√§tzung und die Berechnung von Standardfehlern, einschlie√ülich heteroskedastie-konsistenter Standardfehler, diskutiert. Dar√ºber hinaus wurde die Datenquelle f√ºr den impliziten GNP-Deflator angegeben.

### Referenzen
[^1]: Tim Bollerslev, *Generalized Autoregressive Conditional Heteroskedasticity*
<!-- END -->