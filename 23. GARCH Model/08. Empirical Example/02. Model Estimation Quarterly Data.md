Wie bereits eingeführt, konzentriert sich dieser Abschnitt auf die Schätzung eines Modells für die Inflationsunsicherheit in den Vereinigten Staaten unter Verwendung des GARCH-Modells. Aufbauend auf den vorherigen Abschnitten werden hier die spezifischen Aspekte der Datengrundlage und der angewandten Schätzmethode detailliert betrachtet [^1].

Das Modell wird anhand vierteljährlicher Daten aus dem Zeitraum 1948.2 bis 1983.4 geschätzt. Dies ergibt eine Stichprobengröße von insgesamt 143 Beobachtungen [^1]. Die Schätzung erfolgt mit der Ordinary Least Squares (OLS)-Methode [^1]. Im Folgenden werden die Details der OLS-Schätzung erläutert.

**Die OLS Schätzung**

Die Ordinary Least Squares (OLS)-Methode ist eine weit verbreitete Technik zur Schätzung der Parameter eines linearen Regressionsmodells [^1]. Das Ziel der OLS-Schätzung ist es, die Summe der quadrierten Differenzen zwischen den beobachteten und den vorhergesagten Werten zu minimieren [^1]. Mathematisch ausgedrückt bedeutet dies, dass die Parameter $\beta$ so gewählt werden, dass die folgende Zielfunktion minimiert wird:

$$
\min_\beta \sum_{t=1}^{T} (y_t - x_t'\beta)^2
$$

wobei:
*   $y_t$ der beobachtete Wert der abhängigen Variablen zum Zeitpunkt $t$ ist,
*   $x_t$ ein Vektor der beobachteten Werte der unabhängigen Variablen zum Zeitpunkt $t$ ist,
*   $\beta$ ein Vektor der zu schätzenden Parameter ist, und
*   $T$ die Anzahl der Beobachtungen ist.

Im Kontext des aktuellen Modells ist $y_t = \pi_t$ (die Inflationsrate) und $x_t$ enthält die verzögerten Werte von $\pi_t$ [^1].

> 💡 **Exemplo Numérico:** Betrachten wir ein vereinfachtes Modell mit nur einem Lag:
>
> $$
> \pi_t = \beta_0 + \beta_1 \pi_{t-1} + \epsilon_t
> $$
>
> Um $\beta_0$ und $\beta_1$ mit OLS zu schätzen, würden wir die Summe der quadrierten Residuen minimieren:
>
> $$
> \min_{\beta_0, \beta_1} \sum_{t=2}^{T} (\pi_t - \beta_0 - \beta_1 \pi_{t-1})^2
> $$
>
> Die OLS-Schätzer für $\beta_0$ und $\beta_1$ sind gegeben durch:
>
> $$
> \hat{\beta}_1 = \frac{\sum_{t=2}^{T} (\pi_t - \bar{\pi})(\pi_{t-1} - \bar{\pi}_{-1})}{\sum_{t=2}^{T} (\pi_{t-1} - \bar{\pi}_{-1})^2}
> $$
>
> $$
> \hat{\beta}_0 = \bar{\pi} - \hat{\beta}_1 \bar{\pi}_{-1}
> $$
>
> wobei $\bar{\pi}$ der Mittelwert von $\pi_t$ und $\bar{\pi}_{-1}$ der Mittelwert von $\pi_{t-1}$ ist.
>
> Konkret nehmen wir an, wir haben die folgenden Inflationsdaten für fünf aufeinanderfolgende Quartale (vereinfacht): $\pi = [2, 3, 5, 4, 6]$.  Wir möchten $\pi_t$ auf $\pi_{t-1}$ regressieren.
>
> $\pi_t$: [3, 5, 4, 6]
>
> $\pi_{t-1}$: [2, 3, 5, 4]
>
> $\bar{\pi} = (3+5+4+6)/4 = 4.5$
>
> $\bar{\pi}_{-1} = (2+3+5+4)/4 = 3.5$
>
> $\hat{\beta}_1 = \frac{(3-4.5)(2-3.5) + (5-4.5)(3-3.5) + (4-4.5)(5-3.5) + (6-4.5)(4-3.5)}{(2-3.5)^2 + (3-3.5)^2 + (5-3.5)^2 + (4-3.5)^2} = \frac{2.25 - 0.25 - 0.75 + 0.75}{2.25 + 0.25 + 2.25 + 0.25} = \frac{2}{5} = 0.4$
>
> $\hat{\beta}_0 = 4.5 - 0.4 * 3.5 = 4.5 - 1.4 = 3.1$
>
> Daher ist die geschätzte Regressionsgleichung: $\pi_t = 3.1 + 0.4\pi_{t-1}$. Das bedeutet, dass, wenn die Inflation im vorherigen Quartal um 1 Prozentpunkt steigt, wir erwarten, dass die Inflation im aktuellen Quartal um 0.4 Prozentpunkte steigt, zusätzlich zu einem Basisniveau von 3.1 Prozent.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Inflationsdaten
> pi = np.array([2, 3, 5, 4, 6])
>
> # Berechnung der Mittelwerte
> pi_mean = np.mean(pi[1:])
> pi_lag_mean = np.mean(pi[:-1])
>
> # Berechnung von beta_1
> beta_1_numerator = np.sum((pi[1:] - pi_mean) * (pi[:-1] - pi_lag_mean))
> beta_1_denominator = np.sum((pi[:-1] - pi_lag_mean)**2)
> beta_1 = beta_1_numerator / beta_1_denominator
>
> # Berechnung von beta_0
> beta_0 = pi_mean - beta_1 * pi_lag_mean
>
> print(f"Geschätztes beta_0: {beta_0}")
> print(f"Geschätztes beta_1: {beta_1}")
>
> # Visualisierung
> plt.figure(figsize=(10, 6))
> plt.scatter(pi[:-1], pi[1:], label="Datenpunkte")
>
> # Regressionslinie
> x = np.linspace(min(pi[:-1]), max(pi[:-1]), 100)
> y = beta_0 + beta_1 * x
> plt.plot(x, y, color='red', label=f'Regressionslinie: y = {beta_0:.2f} + {beta_1:.2f}x')
>
> plt.title("OLS-Schätzung der Inflation auf ihren Lag")
> plt.xlabel("Inflation im vorherigen Quartal (π_{t-1})")
> plt.ylabel("Inflation im aktuellen Quartal (π_t)")
> plt.grid(True)
> plt.legend()
> plt.show()
> ```

**Teorema 1 (Gauss-Markov Theorem)** Unter den Annahmen des klassischen linearen Regressionsmodells (Linearität, Exogenität, Homoskedastizität und keine Autokorrelation der Fehlerterme) sind die OLS-Schätzer die besten linearen erwartungstreuen Schätzer (BLUE).

**Corolário 1.1** Wenn die Fehlerterme zusätzlich normalverteilt sind, dann sind die OLS-Schätzer auch die Maximum-Likelihood-Schätzer (MLE) und somit effizient.

*Beweis (Corolário 1.1):* Wir beweisen, dass unter der Annahme normalverteilter Fehlerterme die OLS-Schätzer Maximum-Likelihood-Schätzer (MLE) sind.

I. Angenommen, die Fehlerterme $\epsilon_t$ sind unabhängig und identisch normalverteilt mit einem Mittelwert von 0 und einer konstanten Varianz $\sigma^2$, d. h. $\epsilon_t \sim N(0, \sigma^2)$.

II. Die Wahrscheinlichkeitsfunktion für eine einzelne Beobachtung $y_t$ ist gegeben durch:
    $$
    L(y_t; \beta, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_t - x_t'\beta)^2}{2\sigma^2}\right)
    $$

III. Die Likelihood-Funktion für die gesamte Stichprobe (unter der Annahme unabhängiger Beobachtungen) ist das Produkt der Likelihood-Funktionen für jede Beobachtung:
     $$
     L(\mathbf{y}; \beta, \sigma^2) = \prod_{t=1}^{T} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_t - x_t'\beta)^2}{2\sigma^2}\right)
     $$

IV. Die Log-Likelihood-Funktion ist:
     $$
     \log L(\mathbf{y}; \beta, \sigma^2) = -\frac{T}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=1}^{T} (y_t - x_t'\beta)^2
     $$

V. Um die MLE-Schätzer für $\beta$ zu finden, maximieren wir die Log-Likelihood-Funktion in Bezug auf $\beta$. Da der erste Term nicht von $\beta$ abhängt, entspricht die Maximierung der Log-Likelihood-Funktion der Minimierung des zweiten Terms, der die Summe der quadrierten Residuen ist:
    $$
    \max_\beta \log L(\mathbf{y}; \beta, \sigma^2) \Leftrightarrow \min_\beta \sum_{t=1}^{T} (y_t - x_t'\beta)^2
    $$

VI. Dies ist genau die Zielfunktion der OLS-Schätzung. Daher sind die OLS-Schätzer unter der Annahme normalverteilter Fehlerterme auch die Maximum-Likelihood-Schätzer. ■

**Standardfehler**

Die Genauigkeit der OLS-Schätzungen wird durch ihre Standardfehler quantifiziert [^1]. Die Standardfehler geben ein Maß für die Variabilität der Schätzungen an [^1]. Kleine Standardfehler deuten auf präzisere Schätzungen hin, während große Standardfehler auf größere Unsicherheit hinweisen [^1].

Die Standardfehler der OLS-Schätzungen werden üblicherweise wie folgt berechnet:

$$
\text{SE}(\hat{\beta}) = \sqrt{\text{diag}((X'X)^{-1} \hat{\sigma}^2)}
$$

wobei:
*   $X$ die Matrix der unabhängigen Variablen ist,
*   $\hat{\sigma}^2 = \frac{1}{T-k} \sum_{t=1}^{T} (y_t - x_t'\hat{\beta})^2$ die geschätzte Varianz des Fehlers ist, und
*   $k$ die Anzahl der Parameter im Modell ist.

Es ist wichtig zu beachten, dass die hier gezeigten Standardfehler unter der Annahme einer Homoskedastizität (konstante Varianz) berechnet werden. Dies kann jedoch in vielen ökonomischen Zeitreihen nicht der Fall sein, was die Verwendung heteroskedastie-konsistenter Standardfehler erforderlich macht [^1].

> 💡 **Exemplo Numérico:** Um die Berechnung der Standardfehler zu veranschaulichen, nehmen wir an, wir schätzen das gleiche einfache Inflationsmodell wie zuvor: $\pi_t = \beta_0 + \beta_1 \pi_{t-1} + \epsilon_t$. Wir haben unsere Beispiel-Inflationsdaten von vorher: $\pi = [2, 3, 5, 4, 6]$.
>
> $\text{X} = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 5 \\ 1 & 4 \end{bmatrix}$, $\text{y} = \begin{bmatrix} 3 \\ 5 \\ 4 \\ 6 \end{bmatrix}$
>
> Wir haben bereits geschätzt: $\hat{\beta} = \begin{bmatrix} 3.1 \\ 0.4 \end{bmatrix}$
>
> 1.  **Berechne die Residuen:** $\hat{\epsilon} = y - X\hat{\beta}$
>
> $\hat{\epsilon} = \begin{bmatrix} 3 \\ 5 \\ 4 \\ 6 \end{bmatrix} - \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 5 \\ 1 & 4 \end{bmatrix} \begin{bmatrix} 3.1 \\ 0.4 \end{bmatrix} = \begin{bmatrix} 3 \\ 5 \\ 4 \\ 6 \end{bmatrix} - \begin{bmatrix} 3.9 \\ 4.3 \\ 5.1 \\ 4.7 \end{bmatrix} = \begin{bmatrix} -0.9 \\ 0.7 \\ -1.1 \\ 1.3 \end{bmatrix}$
>
> 2.  **Berechne die geschätzte Fehlervarianz:** $\hat{\sigma}^2 = \frac{1}{T-k} \sum_{t=1}^{T} \hat{\epsilon}_t^2$
>
> $\hat{\sigma}^2 = \frac{(-0.9)^2 + (0.7)^2 + (-1.1)^2 + (1.3)^2}{4 - 2} = \frac{0.81 + 0.49 + 1.21 + 1.69}{2} = \frac{4.2}{2} = 2.1$
>
> 3.  **Berechne $(X'X)^{-1}$:**
>
> $X'X = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 2 & 3 & 5 & 4 \end{bmatrix} \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 5 \\ 1 & 4 \end{bmatrix} = \begin{bmatrix} 4 & 14 \\ 14 & 54 \end{bmatrix}$
>
> $(X'X)^{-1} = \frac{1}{(4*54 - 14*14)} \begin{bmatrix} 54 & -14 \\ -14 & 4 \end{bmatrix} = \frac{1}{216 - 196} \begin{bmatrix} 54 & -14 \\ -14 & 4 \end{bmatrix} = \frac{1}{20} \begin{bmatrix} 54 & -14 \\ -14 & 4 \end{bmatrix} = \begin{bmatrix} 2.7 & -0.7 \\ -0.7 & 0.2 \end{bmatrix}$
>
> 4.  **Berechne die Standardfehler:** $\text{SE}(\hat{\beta}) = \sqrt{\text{diag}((X'X)^{-1} \hat{\sigma}^2)}$
>
> $\text{SE}(\hat{\beta}) = \sqrt{\text{diag}(\begin{bmatrix} 2.7 & -0.7 \\ -0.7 & 0.2 \end{bmatrix} * 2.1)} = \sqrt{\text{diag}(\begin{bmatrix} 5.67 & -1.47 \\ -1.47 & 0.42 \end{bmatrix})} = \begin{bmatrix} \sqrt{5.67} \\ \sqrt{0.42} \end{bmatrix} = \begin{bmatrix} 2.38 \\ 0.65 \end{bmatrix}$
>
> Die Standardfehler sind also SE($\hat{\beta}_0$) = 2.38 und SE($\hat{\beta}_1$) = 0.65.  Dies gibt uns ein Gefühl für die Unsicherheit bei der Schätzung unserer Koeffizienten.  Ein größerer Standardfehler bedeutet eine größere Unsicherheit.
>
> ```python
> import numpy as np
> from numpy.linalg import inv
>
> # Daten
> X = np.array([[1, 2], [1, 3], [1, 5], [1, 4]])
> y = np.array([3, 5, 4, 6])
>
> # OLS-Schätzung (wie zuvor)
> beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
>
> # Residuen berechnen
> epsilon_hat = y - X @ beta_hat
>
> # Geschätzte Fehlervarianz
> sigma_hat_squared = np.sum(epsilon_hat**2) / (len(y) - X.shape[1])
>
> # (X'X)^-1 berechnen
> XTX_inv = np.linalg.inv(X.T @ X)
>
> # Standardfehler berechnen
> SE = np.sqrt(np.diag(XTX_inv * sigma_hat_squared))
>
> print(f"Geschätzte Koeffizienten: {beta_hat}")
> print(f"Geschätzte Standardfehler: {SE}")
> ```

**Heteroskedastie-konsistente Standardfehler**
In der Gegenwart von Heteroskedastizität sind die OLS-Schätzer selbst weiterhin unverzerrt und konsistent, aber die herkömmlichen Standardfehler sind nicht mehr gültig [^1]. Um diesem Problem zu begegnen, werden häufig heteroskedastie-konsistente Standardfehler verwendet [^1]. Diese Standardfehler sind robust gegenüber Heteroskedastizität und liefern zuverlässigere Inferenz [^1].

Eine gängige Methode zur Berechnung heteroskedastie-konsistenter Standardfehler ist der White-Schätzer, der wie folgt definiert ist:

$$
\text{SE}_{\text{HC}}(\hat{\beta}) = \sqrt{\text{diag}((X'X)^{-1} \left(\sum_{t=1}^{T} x_t x_t' \hat{\epsilon}_t^2 \right) (X'X)^{-1})}
$$

wobei $\hat{\epsilon}_t = y_t - x_t'\hat{\beta}$ die geschätzten Residuen sind.

> 💡 **Exemplo Numérico:** Nehmen wir an, wir haben folgende Matrix $X'X$:
>
> $X'X = \begin{bmatrix} 143 & 0 \\ 0 & 100 \end{bmatrix}$
>
> Daraus ergibt sich:
>
> $(X'X)^{-1} = \begin{bmatrix} 1/143 & 0 \\ 0 & 1/100 \end{bmatrix} = \begin{bmatrix} 0.00699 & 0 \\ 0 & 0.01 \end{bmatrix}$
>
> Nehmen wir an $\hat{\sigma}^2 = 0.3$, dann:
>
> $\text{SE}(\hat{\beta}) = \sqrt{\text{diag}((X'X)^{-1} \hat{\sigma}^2)} = \sqrt{\text{diag}(\begin{bmatrix} 0.00699 & 0 \\ 0 & 0.01 \end{bmatrix} \cdot 0.3)} = \sqrt{\text{diag}(\begin{bmatrix} 0.0021 & 0 \\ 0 & 0.003 \end{bmatrix})} = \begin{bmatrix} 0.0458 & 0 \\ 0 & 0.0548 \end{bmatrix}$
>
> Angenommen, $\sum_{t=1}^{T} x_t x_t' \hat{\epsilon}_t^2 = \begin{bmatrix} 50 & 0 \\ 0 & 40 \end{bmatrix}$
>
> $SE_{HC}(\hat{\beta}) = \sqrt{\text{diag}((X'X)^{-1} (\sum_{t=1}^{T} x_t x_t' \hat{\epsilon}_t^2) (X'X)^{-1})} = \sqrt{\text{diag}(\begin{bmatrix} 0.00699 & 0 \\ 0 & 0.01 \end{bmatrix} \begin{bmatrix} 50 & 0 \\ 0 & 40 \end{bmatrix} \begin{bmatrix} 0.00699 & 0 \\ 0 & 0.01 \end{bmatrix})} = \sqrt{\text{diag}(\begin{bmatrix} 0.3495 & 0 \\ 0 & 0.4 \end{bmatrix} \begin{bmatrix} 0.00699 & 0 \\ 0 & 0.01 \end{bmatrix})} = \sqrt{\text{diag}(\begin{bmatrix} 0.0024 & 0 \\ 0 & 0.004 \end{bmatrix})} = \begin{bmatrix} 0.0489 & 0 \\ 0 & 0.0632 \end{bmatrix}$
>
> Betrachten wir die Inflation von oben, aber nehmen wir an, es liegt Heteroskedastizität vor, und wir müssen White's Heteroskedastie-konsistente Standardfehler verwenden.
>
> Zuerst müssen wir $x_t x_t' \hat{\epsilon}_t^2$ für jede Beobachtung berechnen und dann summieren.  Zur Erinnerung: $X = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 5 \\ 1 & 4 \end{bmatrix}$, $\hat{\epsilon} = \begin{bmatrix} -0.9 \\ 0.7 \\ -1.1 \\ 1.3 \end{bmatrix}$.
>
> $\sum_{t=1}^{4} x_t x_t' \hat{\epsilon}_t^2 =  \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 5 \\ 1 & 4 \end{bmatrix} diag((-0.9)^2, (0.7)^2, (-1.1)^2, (1.3)^2) \begin{bmatrix} 1 & 1 & 1 & 1 \\ 2 & 3 & 5 & 4 \end{bmatrix} = \begin{bmatrix} 4.43 & 14.11 \\ 14.11 & 46.71 \end{bmatrix}$
>
> Dann können wir die heteroskedastie-konsistenten Standardfehler berechnen:
>
> $\text{SE}_{\text{HC}}(\hat{\beta}) = \sqrt{\text{diag}((X'X)^{-1} \left(\sum_{t=1}^{T} x_t x_t' \hat{\epsilon}_t^2 \right) (X'X)^{-1})} = \sqrt{\text{diag}(\begin{bmatrix} 2.7 & -0.7 \\ -0.7 & 0.2 \end{bmatrix} \begin{bmatrix} 4.43 & 14.11 \\ 14.11 & 46.71 \end{bmatrix} \begin{bmatrix} 2.7 & -0.7 \\ -0.7 & 0.2 \end{bmatrix})} = \begin{bmatrix} 2.74 \\ 0.79 \end{bmatrix}$
>
> Vergleicht man diese mit den vorherigen Standardfehlern (2.38, 0.65), so sind die heteroskedastie-konsistenten Standardfehler etwas größer.  Dies deutet darauf hin, dass die Berücksichtigung von Heteroskedastizität zu konservativeren Schlussfolgerungen führen kann (d. h. breitere Konfidenzintervalle).
>
> ```python
> import numpy as np
> from numpy.linalg import inv
>
> # Daten (wie zuvor)
> X = np.array([[1, 2], [1, 3], [1, 5], [1, 4]])
> y = np.array([3, 5, 4, 6])
>
> # OLS-Schätzung (wie zuvor)
> beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
>
> # Residuen berechnen
> epsilon_hat = y - X @ beta_hat
>
> # White's Heteroskedastie-konsistente Varianz-Kovarianzmatrix berechnen
> T = len(y)
> k = X.shape[1]
> XTX_inv = np.linalg.inv(X.T @ X)
> V_HC = XTX_inv @ (X.T @ np.diag(epsilon_hat**2) @ X) @ XTX_inv
>
> # Heteroskedastie-konsistente Standardfehler berechnen
> SE_HC = np.sqrt(np.diag(V_HC))
>
> print(f"Heteroskedastie-konsistente Standardfehler: {SE_HC}")
> ```

Die Verwendung von OLS zur Schätzung von Modellen mit heteroskedastischen Fehlern ist nicht optimal, aber weit verbreitet [^1]. Die Maximum-Likelihood-Schätzung des gesamten GARCH-Modells ist effizienter [^1].

**Teorema 2 (Konsistenz der OLS-Schätzer)** Unter milden Regularitätsbedingungen, insbesondere wenn die Fehlerterme einen bedingten Mittelwert von Null und eine endliche Varianz haben und die Regressoren nicht perfekt kollinear sind, konvergieren die OLS-Schätzer in Wahrscheinlichkeit gegen die wahren Parameterwerte, wenn die Stichprobengröße gegen unendlich geht.

**Proposição 1 (Eigenschaften von White's Heteroskedastie-konsistentem Schätzer)** Der White-Schätzer ist asymptotisch äquivalent zum OLS-Schätzer unter Homoskedastizität, aber er ist robust gegenüber Heteroskedastizität unbekannter Form.

**Lema 1 (Bedeutung von Lag-Länge)** Die Wahl der optimalen Lag-Länge für die Inflation in der Regressionsgleichung ist entscheidend. Zu kurze Lag-Längen können zu ausgelassenen Variablen führen, während zu lange Lag-Längen zu unnötigem Verlust von Freiheitsgraden und möglicherweise zu Multikollinearität führen können. Informationskriterien wie das Akaike Information Criterion (AIC) oder das Bayesian Information Criterion (BIC) können verwendet werden, um die optimale Lag-Länge zu bestimmen.

### Datenquelle

Die Daten für den impliziten GNP-Deflator (GD) stammen aus der Citibank Economic Database und dem U.S. Department of Commerce, Survey of Current Business, Vol. 64, No. 9, September 1984 [^1].

### Zusammenfassung
Dieser Abschnitt hat die Datengrundlage und die Schätzmethode für das Modell der Inflationsunsicherheit beschrieben [^1]. Es wurde die OLS-Schätzung und die Berechnung von Standardfehlern, einschließlich heteroskedastie-konsistenter Standardfehler, diskutiert. Darüber hinaus wurde die Datenquelle für den impliziten GNP-Deflator angegeben.

### Referenzen
[^1]: Tim Bollerslev, *Generalized Autoregressive Conditional Heteroskedasticity*
<!-- END -->