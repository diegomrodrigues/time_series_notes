O presente cap√≠tulo dedica-se ao estudo aprofundado do processo GARCH(1,1), um modelo amplamente utilizado na an√°lise de s√©ries temporais para capturar a heteroscedasticidade condicional, ou seja, a volatilidade vari√°vel ao longo do tempo. Como uma especializa√ß√£o do modelo GARCH(p,q) [^1, ^2], o GARCH(1,1) oferece uma estrutura simples, por√©m eficaz, para modelar *clustering* de volatilidade, um fen√¥meno comum em dados financeiros e econ√¥micos.

### Conceitos Fundamentais

O modelo **GARCH(1,1)**, como apresentado em [^5], √© definido pelas seguintes equa√ß√µes:

$$
\epsilon_t|\Psi_{t-1} \sim N(0, h_t)
$$

$$
h_t = \alpha_0 + \alpha_1\epsilon_{t-1}^2 + \beta_1h_{t-1}
$$

onde:

*   $\epsilon_t$ representa o *erro ou inova√ß√£o* no tempo $t$, condicional ao conjunto de informa√ß√µes $\Psi_{t-1}$ dispon√≠veis at√© o tempo $t-1$.
*   $h_t$ denota a *vari√¢ncia condicional* no tempo $t$, que √© modelada como uma fun√ß√£o dos erros quadrados passados e das vari√¢ncias condicionais passadas.
*   $\alpha_0 > 0$ √© o *n√≠vel constante* da vari√¢ncia condicional.
*   $\alpha_1 \geq 0$ √© o *coeficiente de ARCH*, que mede a sensibilidade da vari√¢ncia condicional √†s inova√ß√µes passadas ($\epsilon_{t-1}^2$).
*   $\beta_1 \geq 0$ √© o *coeficiente de GARCH*, que mede a persist√™ncia da volatilidade, ou seja, o quanto a vari√¢ncia condicional passada ($h_{t-1}$) influencia a vari√¢ncia condicional atual.
*   $N(0, h_t)$ denota uma distribui√ß√£o *Normal com m√©dia zero e vari√¢ncia* $h_t$. Embora a distribui√ß√£o condicional seja frequentemente assumida como Normal, outras distribui√ß√µes podem ser utilizadas [^3].

**Interpreta√ß√£o dos Par√¢metros:**

*   $\alpha_0$: Representa o n√≠vel base da volatilidade. Quanto maior $\alpha_0$, maior a volatilidade m√©dia do processo.
*   $\alpha_1$: Indica a sensibilidade da volatilidade a choques recentes. Um valor alto de $\alpha_1$ implica que grandes choques passados levar√£o a um aumento significativo na volatilidade atual.
*   $\beta_1$: Mede a persist√™ncia da volatilidade. Quanto maior $\beta_1$, mais tempo a volatilidade permanecer√° alta ap√≥s um choque.

> üí° **Exemplo Num√©rico:**
> Suponha que estimamos um modelo GARCH(1,1) para retornos di√°rios de uma a√ß√£o e obtivemos os seguintes par√¢metros: $\alpha_0 = 0.01$, $\alpha_1 = 0.10$, e $\beta_1 = 0.85$. Isso significa que:
> *   O n√≠vel base da volatilidade (vari√¢ncia di√°ria) √© de 0.01.
> *   10% de qualquer choque passado (retorno di√°rio inesperado) √© incorporado na volatilidade atual.
> *   85% da volatilidade do dia anterior persiste para o dia atual.
> Se o choque de ontem ($\epsilon_{t-1}^2$) foi de 4 (o que corresponderia a um retorno di√°rio de 2%), e a vari√¢ncia condicional de ontem ($h_{t-1}$) foi 1, ent√£o a vari√¢ncia condicional de hoje ($h_t$) seria calculada como:
> $h_t = 0.01 + 0.10 * 4 + 0.85 * 1 = 0.01 + 0.4 + 0.85 = 1.26$
> A volatilidade di√°ria (desvio padr√£o) seria $\sqrt{1.26} \approx 1.12$.

**Condi√ß√µes de Estacionariedade e Exist√™ncia de Momentos:**

Para que o processo GARCH(1,1) seja *wide-sense stationary*, √© necess√°rio que a seguinte condi√ß√£o seja satisfeita [^5]:

$$
\alpha_1 + \beta_1 < 1
$$

Esta condi√ß√£o garante que a vari√¢ncia incondicional do processo seja finita e constante ao longo do tempo.

*Prova:*

I. Come√ßamos pela defini√ß√£o do processo GARCH(1,1):
    $$h_t = \alpha_0 + \alpha_1\epsilon_{t-1}^2 + \beta_1h_{t-1}$$

II. Tomamos o valor esperado incondicional de ambos os lados:
    $$E[h_t] = E[\alpha_0 + \alpha_1\epsilon_{t-1}^2 + \beta_1h_{t-1}]$$

III. Assumindo estacionariedade, $E[h_t] = E[h_{t-1}] = h$ e $E[\epsilon_{t-1}^2] = E[h_{t-1}] = h$, temos:
     $$h = \alpha_0 + \alpha_1 h + \beta_1 h$$

IV. Resolvendo para $h$:
     $$h(1 - \alpha_1 - \beta_1) = \alpha_0$$
     $$h = \frac{\alpha_0}{1 - \alpha_1 - \beta_1}$$

V. Para que a vari√¢ncia incondicional $h$ seja finita e positiva (dado que $\alpha_0 > 0$), √© necess√°rio que o denominador seja positivo:
    $$1 - \alpha_1 - \beta_1 > 0$$
    $$\alpha_1 + \beta_1 < 1$$

Portanto, a condi√ß√£o para estacionariedade √© $\alpha_1 + \beta_1 < 1$ ‚ñ†

> üí° **Exemplo Num√©rico:**
> Usando os par√¢metros do exemplo anterior ($\alpha_0 = 0.01$, $\alpha_1 = 0.10$, e $\beta_1 = 0.85$), verificamos a condi√ß√£o de estacionariedade:
> $\alpha_1 + \beta_1 = 0.10 + 0.85 = 0.95 < 1$.
> Portanto, o processo √© estacion√°rio.
> A vari√¢ncia incondicional seria:
> $h = \frac{0.01}{1 - 0.10 - 0.85} = \frac{0.01}{0.05} = 0.2$.
> Isso significa que, a longo prazo, a vari√¢ncia m√©dia do processo ser√° de 0.2.
> Se tiv√©ssemos $\alpha_1 = 0.20$ e $\beta_1 = 0.90$, ent√£o $\alpha_1 + \beta_1 = 1.10 > 1$, e o processo n√£o seria estacion√°rio. A vari√¢ncia incondicional tenderia ao infinito.

Adicionalmente, a exist√™ncia de momentos superiores imp√µe restri√ß√µes adicionais sobre os par√¢metros. Especificamente, para a exist√™ncia do 2m-√©simo momento (onde m √© um inteiro positivo), a seguinte condi√ß√£o deve ser satisfeita [^5]:

$$
\mu(\alpha_1, \beta_1, m) = \sum_{j=0}^{m} \binom{m}{j} a_j \alpha_1^j \beta_1^{m-j} < 1
$$

onde $a_j = \prod_{i=1}^{j} (2i - 1)$, com $a_0 = 1$ [^5].

**Observa√ß√£o:** A condi√ß√£o para a exist√™ncia do quarto momento (m=2) √© dada por $\alpha_1^2(3) + 2\alpha_1\beta_1 + \beta_1^2 < 1$.

*Prova:*

I. Para m=2, temos:
    $$\mu(\alpha_1, \beta_1, 2) = \sum_{j=0}^{2} \binom{2}{j} a_j \alpha_1^j \beta_1^{2-j} < 1$$

II. Expandindo a soma:
    $$\mu(\alpha_1, \beta_1, 2) = \binom{2}{0} a_0 \alpha_1^0 \beta_1^{2} + \binom{2}{1} a_1 \alpha_1^1 \beta_1^{1} + \binom{2}{2} a_2 \alpha_1^2 \beta_1^{0} < 1$$

III. Calculando os coeficientes binomiais e os valores de $a_j$:
     *  $\binom{2}{0} = 1$, $a_0 = 1$
     *  $\binom{2}{1} = 2$, $a_1 = \prod_{i=1}^{1} (2i - 1) = 1$
     *  $\binom{2}{2} = 1$, $a_2 = \prod_{i=1}^{2} (2i - 1) = (2(1) - 1)(2(2) - 1) = 1 \cdot 3 = 3$

IV. Substituindo os valores na equa√ß√£o:
     $$\mu(\alpha_1, \beta_1, 2) = 1 \cdot 1 \cdot 1 \cdot \beta_1^2 + 2 \cdot 1 \cdot \alpha_1 \cdot \beta_1 + 1 \cdot 3 \cdot \alpha_1^2 \cdot 1 < 1$$
     $$\beta_1^2 + 2\alpha_1\beta_1 + 3\alpha_1^2 < 1$$

Portanto, a condi√ß√£o para a exist√™ncia do quarto momento √© $3\alpha_1^2 + 2\alpha_1\beta_1 + \beta_1^2 < 1$ ‚ñ†

> üí° **Exemplo Num√©rico:**
> Com os par√¢metros $\alpha_1 = 0.10$ e $\beta_1 = 0.85$, verificamos a exist√™ncia do quarto momento:
> $3\alpha_1^2 + 2\alpha_1\beta_1 + \beta_1^2 = 3(0.10)^2 + 2(0.10)(0.85) + (0.85)^2 = 3(0.01) + 0.17 + 0.7225 = 0.03 + 0.17 + 0.7225 = 0.9225 < 1$.
> Portanto, o quarto momento existe.
> Se $\alpha_1 = 0.3$ e $\beta_1 = 0.8$, ent√£o:
> $3\alpha_1^2 + 2\alpha_1\beta_1 + \beta_1^2 = 3(0.3)^2 + 2(0.3)(0.8) + (0.8)^2 = 3(0.09) + 0.48 + 0.64 = 0.27 + 0.48 + 0.64 = 1.39 > 1$.
> Nesse caso, o quarto momento n√£o existiria.

**Lema 1** Se $\alpha_1 + \beta_1 < 1$, ent√£o existe um $m > 1$ tal que $\mu(\alpha_1, \beta_1, m) < 1$.

*Prova:* Esta condi√ß√£o garante que a vari√¢ncia incondicional √© finita. No entanto, a exist√™ncia de momentos superiores requer condi√ß√µes mais restritivas sobre os par√¢metros. A prova formal envolve analisar o comportamento assint√≥tico da fun√ß√£o $\mu(\alpha_1, \beta_1, m)$ quando $m$ tende ao infinito.

**Teorema 2** Se $\epsilon_t|\Psi_{t-1} \sim t(\nu, 0, h_t)$, onde $t(\nu, 0, h_t)$ denota uma distribui√ß√£o t-Student com $\nu$ graus de liberdade, m√©dia 0 e vari√¢ncia $h_t$, ent√£o a fun√ß√£o de log-verossimilhan√ßa √© dada por:

$$
l_t(\theta) = log\left(\frac{\Gamma(\frac{\nu+1}{2})}{\Gamma(\frac{\nu}{2})\sqrt{\pi(\nu-2)h_t}}\right) - \frac{\nu+1}{2}log\left(1 + \frac{\epsilon_t^2}{(\nu-2)h_t}\right)
$$

onde $\Gamma(.)$ denota a fun√ß√£o gama.

*Prova:* A demonstra√ß√£o segue da fun√ß√£o densidade de probabilidade da distribui√ß√£o t-Student com m√©dia 0 e vari√¢ncia $h_t$. A fun√ß√£o de log-verossimilhan√ßa √© obtida somando os logaritmos das densidades para cada observa√ß√£o.

> üí° **Exemplo Num√©rico:**
> Suponha que temos um modelo GARCH(1,1) com distribui√ß√£o t-Student e estimamos $\nu=5$, $h_t = 2$, e $\epsilon_t = 1.5$. Ent√£o,
>
> $l_t(\theta) = log\left(\frac{\Gamma(\frac{5+1}{2})}{\Gamma(\frac{5}{2})\sqrt{\pi(5-2)(2)}}\right) - \frac{5+1}{2}log\left(1 + \frac{(1.5)^2}{(5-2)(2)}\right)$
>
> $l_t(\theta) = log\left(\frac{\Gamma(3)}{\Gamma(2.5)\sqrt{6\pi}}\right) - 3log\left(1 + \frac{2.25}{6}\right)$
>
> Usando valores aproximados para a fun√ß√£o gama ($\Gamma(3) = 2! = 2$ e $\Gamma(2.5) \approx 1.33$)
>
> $l_t(\theta) \approx log\left(\frac{2}{1.33\sqrt{6\pi}}\right) - 3log\left(1 + 0.375\right)$
>
> $l_t(\theta) \approx log(0.308) - 3log(1.375)$
>
> $l_t(\theta) \approx -1.177 - 3(0.318) \approx -1.177 - 0.954 = -2.131$
>
> Este valor representa a contribui√ß√£o desta observa√ß√£o para a fun√ß√£o de log-verossimilhan√ßa total.

**Teorema 2.1** A estat√≠stica do teste de raz√£o de verossimilhan√ßas (LR) para comparar um modelo GARCH(1,1) com distribui√ß√£o normal condicional com um modelo GARCH(1,1) com distribui√ß√£o t-Student condicional √© dada por:

$$LR = 2(L_{t} - L_{N})$$

Onde $L_{t}$ √© o valor m√°ximo da fun√ß√£o de log-verossimilhan√ßa sob a distribui√ß√£o t-Student e $L_{N}$ √© o valor m√°ximo da fun√ß√£o de log-verossimilhan√ßa sob a distribui√ß√£o normal. Sob a hip√≥tese nula de que a distribui√ß√£o normal √© a correta, a estat√≠stica LR segue uma distribui√ß√£o qui-quadrado com um grau de liberdade (correspondente ao par√¢metro adicional $\nu$ da distribui√ß√£o t-Student).

*Prova:* A demonstra√ß√£o √© baseada na teoria assint√≥tica dos testes de raz√£o de verossimilhan√ßas. O teste LR compara a verossimilhan√ßa dos dados sob duas hip√≥teses: uma hip√≥tese nula (distribui√ß√£o normal) e uma hip√≥tese alternativa (distribui√ß√£o t-Student). A estat√≠stica LR mede a diferen√ßa entre as log-verossimilhan√ßas maximizadas sob cada hip√≥tese. Sob certas condi√ß√µes de regularidade, a estat√≠stica LR converge para uma distribui√ß√£o qui-quadrado assintoticamente.

**Proposi√ß√£o 3** A autocorrela√ß√£o amostral dos quadrados dos res√≠duos ($\epsilon_t^2$) pode ser usada para verificar a adequa√ß√£o do modelo GARCH(1,1). Se o modelo for adequado, as autocorrela√ß√µes dos res√≠duos padronizados (${\epsilon_t^2}/{h_t}$) devem ser pr√≥ximas de zero.

*Prova:* Se o modelo GARCH(1,1) captura adequadamente a depend√™ncia temporal na volatilidade, os res√≠duos padronizados devem ser aproximadamente independentes e identicamente distribu√≠dos (iid). Portanto, suas autocorrela√ß√µes devem ser insignificantes. Desvios significativos das autocorrela√ß√µes de zero indicam que o modelo pode n√£o estar capturando toda a depend√™ncia temporal na volatilidade e pode ser necess√°rio considerar um modelo mais complexo ou uma distribui√ß√£o condicional diferente.

> üí° **Exemplo Num√©rico:**
> Suponha que ajustamos um modelo GARCH(1,1) aos retornos di√°rios de uma a√ß√£o e calculamos as autocorrela√ß√µes dos res√≠duos padronizados ao quadrado. Obtemos as seguintes autocorrela√ß√µes para os primeiros 5 lags:
>
> | Lag | Autocorrela√ß√£o |
> |-----|----------------|
> | 1   | 0.05           |
> | 2   | 0.02           |
> | 3   | -0.01          |
> | 4   | 0.03           |
> | 5   | -0.02          |
>
> Se o tamanho da amostra for grande (por exemplo, T = 1000), podemos usar o teste de Ljung-Box para verificar se as autocorrela√ß√µes s√£o significativamente diferentes de zero. A estat√≠stica de Ljung-Box √© dada por:
>
> $Q(m) = T(T+2)\sum_{k=1}^{m} \frac{\hat{\rho}_k^2}{T-k}$
>
> onde $\hat{\rho}_k$ √© a autocorrela√ß√£o amostral no lag k, e m √© o n√∫mero m√°ximo de lags considerados. Sob a hip√≥tese nula de que n√£o h√° autocorrela√ß√£o, Q(m) segue uma distribui√ß√£o qui-quadrado com m graus de liberdade.
>
> Para m = 5:
>
> $Q(5) = 1000(1000+2) \left( \frac{0.05^2}{999} + \frac{0.02^2}{998} + \frac{(-0.01)^2}{997} + \frac{0.03^2}{996} + \frac{(-0.02)^2}{995} \right)$
>
> $Q(5) \approx 1002000 \left( \frac{0.0025}{999} + \frac{0.0004}{998} + \frac{0.0001}{997} + \frac{0.0009}{996} + \frac{0.0004}{995} \right)$
>
> $Q(5) \approx 1002000 (2.5 \times 10^{-6} + 0.4 \times 10^{-6} + 0.1 \times 10^{-6} + 0.9 \times 10^{-6} + 0.4 \times 10^{-6}) \approx 1002000 (4.3 \times 10^{-6}) \approx 4.3086$
>
> O valor cr√≠tico para uma distribui√ß√£o qui-quadrado com 5 graus de liberdade a um n√≠vel de signific√¢ncia de 5% √© aproximadamente 11.07. Como Q(5) < 11.07, n√£o rejeitamos a hip√≥tese nula de que as autocorrela√ß√µes s√£o insignificantes. Portanto, o modelo GARCH(1,1) parece ser adequado para capturar a depend√™ncia temporal na volatilidade desta s√©rie.

**Interpreta√ß√£o em Termos de Mem√≥ria:**

O processo GARCH(1,1) pode ser reescrito como um processo ARCH de ordem infinita (ARCH($\infty$)) [^3]:

$$
h_t = \frac{\alpha_0}{1 - \beta_1} + \alpha_1 \sum_{i=1}^{\infty} \beta_1^{i-1} \epsilon_{t-i}^2
$$

Esta representa√ß√£o revela que a vari√¢ncia condicional atual √© uma m√©dia ponderada de todos os erros quadrados passados, com pesos que decaem exponencialmente com a defasagem. O par√¢metro $\beta_1$ controla a velocidade de decaimento, e, portanto, a "mem√≥ria" do processo. Quanto maior $\beta_1$, mais lenta a taxa de decaimento e mais longa a mem√≥ria do processo.

*Prova:*

I. Come√ßamos pela equa√ß√£o do GARCH(1,1):
    $$h_t = \alpha_0 + \alpha_1\epsilon_{t-1}^2 + \beta_1h_{t-1}$$

II. Substitu√≠mos recursivamente $h_{t-1}$:
    $$h_t = \alpha_0 + \alpha_1\epsilon_{t-1}^2 + \beta_1(\alpha_0 + \alpha_1\epsilon_{t-2}^2 + \beta_1h_{t-2})$$
    $$h_t = \alpha_0 + \alpha_1\epsilon_{t-1}^2 + \beta_1\alpha_0 + \beta_1\alpha_1\epsilon_{t-2}^2 + \beta_1^2h_{t-2}$$

III. Continuamos a substitui√ß√£o recursiva por $n$ vezes:
     $$h_t = \alpha_0\sum_{i=0}^{n-1} \beta_1^i + \alpha_1\sum_{i=0}^{n-1} \beta_1^i \epsilon_{t-i-1}^2 + \beta_1^n h_{t-n}$$

IV. Se o processo √© estacion√°rio ($\beta_1 < 1$), ent√£o $\lim_{n \to \infty} \beta_1^n h_{t-n} = 0$.  Al√©m disso, $\sum_{i=0}^{\infty} \beta_1^i = \frac{1}{1 - \beta_1}$. Portanto:
    $$h_t = \alpha_0\sum_{i=0}^{\infty} \beta_1^i + \alpha_1\sum_{i=0}^{\infty} \beta_1^i \epsilon_{t-i-1}^2$$
    $$h_t = \frac{\alpha_0}{1 - \beta_1} + \alpha_1\sum_{i=1}^{\infty} \beta_1^{i-1} \epsilon_{t-i}^2$$

Assim, obtemos a representa√ß√£o ARCH($\infty$) do processo GARCH(1,1) ‚ñ†

> üí° **Exemplo Num√©rico:**
> Usando os par√¢metros $\alpha_0 = 0.01$, $\alpha_1 = 0.10$, e $\beta_1 = 0.85$, a representa√ß√£o ARCH($\infty$) seria:
>
> $h_t = \frac{0.01}{1 - 0.85} + 0.10 \sum_{i=1}^{\infty} (0.85)^{i-1} \epsilon_{t-i}^2$
>
> $h_t = \frac{0.01}{0.15} + 0.10 [\epsilon_{t-1}^2 + 0.85\epsilon_{t-2}^2 + (0.85)^2\epsilon_{t-3}^2 + ... ]$
>
> $h_t = 0.0667 + 0.10 [\epsilon_{t-1}^2 + 0.85\epsilon_{t-2}^2 + 0.7225\epsilon_{t-3}^2 + ... ]$
>
> Isso mostra que o choque mais recente ($\epsilon_{t-1}^2$) tem um peso de 0.10, o choque de dois per√≠odos atr√°s ($\epsilon_{t-2}^2$) tem um peso de 0.085, o choque de tr√™s per√≠odos atr√°s ($\epsilon_{t-3}^2$) tem um peso de 0.07225, e assim por diante. Os pesos decaem exponencialmente, indicando que os choques mais recentes t√™m um impacto maior na volatilidade atual.

**M√©dia e Mediana do Lag:**

A m√©dia do lag (tempo m√©dio que a volatilidade leva para retornar ao seu n√≠vel m√©dio) na equa√ß√£o da vari√¢ncia condicional √© dada por [^5]:

$$
\zeta = \frac{\alpha_1}{1 - \beta_1} = \sum_{i=1}^{\infty} i \delta_i
$$

onde $\delta_i$ s√£o os coeficientes da representa√ß√£o ARCH($\infty$).

> üí° **Exemplo Num√©rico:**
> Com $\alpha_1 = 0.10$ e $\beta_1 = 0.85$, a m√©dia do lag √©:
> $\zeta = \frac{0.10}{1 - 0.85} = \frac{0.10}{0.15} = 0.667$.
> Isso significa que, em m√©dia, a volatilidade leva cerca de 0.67 per√≠odos para retornar ao seu n√≠vel m√©dio ap√≥s um choque.

A mediana do lag √© dada por [^6]:

$$
v = -\frac{log2}{log\beta_1}
$$

> üí° **Exemplo Num√©rico:**
> Com $\beta_1 = 0.85$, a mediana do lag √©:
> $v = -\frac{log2}{log0.85} = -\frac{0.6931}{-0.1625} \approx 4.26$.
> Isso significa que metade do impacto de um choque na volatilidade desaparece ap√≥s aproximadamente 4.26 per√≠odos.

**Rela√ß√£o com Modelos ARMA:**

O processo GARCH(1,1) pode ser interpretado como um modelo ARMA aplicado √† s√©rie temporal dos erros quadrados.  Se definirmos $\nu_t = \epsilon_t^2 - h_t$, ent√£o [^4]:

$$
\epsilon_t^2 = \alpha_0 + (\alpha_1 + \beta_1)\epsilon_{t-1}^2 + \nu_t - \beta_1\nu_{t-1}
$$

Isto mostra que $\epsilon_t^2$ segue um processo ARMA(1,1).

*Prova:*
I. Come√ßamos pela equa√ß√£o do GARCH(1,1):
    $$h_t = \alpha_0 + \alpha_1\epsilon_{t-1}^2 + \beta_1h_{t-1}$$

II. Rearranjamos a equa√ß√£o para expressar $\alpha_0$:
    $$\alpha_0 = h_t - \alpha_1\epsilon_{t-1}^2 - \beta_1h_{t-1}$$

III. Substitu√≠mos a express√£o para $\alpha_0$ na equa√ß√£o $\epsilon_t^2 = h_t + \nu_t$:
     $$ \epsilon_t^2 = h_t + \nu_t =  \alpha_0 + \alpha_1\epsilon_{t-1}^2 + \beta_1h_{t-1} + \nu_t$$
     $$ \epsilon_t^2 =  h_t = \alpha_0 + \alpha_1\epsilon_{t-1}^2 + \beta_1h_{t-1} + \nu_t$$

IV. Usamos o fato de que $\nu_t = \epsilon_t^2 - h_t$:
     $$ h_{t-1} = \epsilon_{t-1}^2 - \nu_{t-1}$$
     $$ \epsilon_t^2 = \alpha_0 + \alpha_1\epsilon_{t-1}^2 + \beta_1(\epsilon_{t-1}^2 - \nu_{t-1}) + \nu_t$$
     $$ \epsilon_t^2 = \alpha_0 + (\alpha_1 + \beta_1)\epsilon_{t-1}^2 + \nu_t - \beta_1\nu_{t-1}$$
Esta equa√ß√£o √© a representa√ß√£o de um processo ARMA(1,1) para $\epsilon_t^2$ ‚ñ†

**Estima√ß√£o:**

Os par√¢metros do modelo GARCH(1,1) s√£o geralmente estimados por *Maximum Likelihood Estimation (MLE)* [^1]. Sob a suposi√ß√£o de normalidade condicional, a fun√ß√£o de log-verossimilhan√ßa √© dada por:

$$
L(\theta) = \frac{1}{T} \sum_{t=1}^{T} l_t(\theta)
$$

onde:

$$
l_t(\theta) = -\frac{1}{2}log(h_t) - \frac{1}{2}\frac{\epsilon_t^2}{h_t}
$$

e $\theta = (\alpha_0, \alpha_1, \beta_1)$ √© o vetor de par√¢metros a serem estimados. A maximiza√ß√£o da fun√ß√£o de log-verossimilhan√ßa requer o uso de m√©todos num√©ricos iterativos.

> üí° **Exemplo Num√©rico:**
> Suponha que temos uma s√©rie temporal de retornos com T = 1000 observa√ß√µes. Ap√≥s otimizar a fun√ß√£o de log-verossimilhan√ßa, obtemos as seguintes estimativas de par√¢metros: $\hat{\alpha}_0 = 0.015$, $\hat{\alpha}_1 = 0.12$, e $\hat{\beta}_1 = 0.83$.
> Para calcular a log-verossimilhan√ßa para a primeira observa√ß√£o (t=1), precisamos de um valor inicial para $h_0$. Podemos usar a vari√¢ncia amostral dos retornos como um valor inicial. Suponha que a vari√¢ncia amostral seja 0.2. Ent√£o:
> $h_1 = \hat{\alpha}_0 + \hat{\alpha}_1\epsilon_0^2 + \hat{\beta}_1h_0 = 0.015 + 0.12\epsilon_0^2 + 0.83(0.2)$. Assumindo $\epsilon_0^2 = h_0 = 0.2$:
> $h_1 = 0.015 + 0.12(0.2) + 0.83(0.2) = 0.015 + 0.024 + 0.166 = 0.205$.
> Suponha que $\epsilon_1 = 0.5$. Ent√£o:
> $l_1(\theta) = -\frac{1}{2}log(h_1) - \frac{1}{2}\frac{\epsilon_1^2}{h_1} = -\frac{1}{2}log(0.205) - \frac{1}{2}\frac{(0.5)^2}{0.205} = -\frac{1}{2}(-1.585) - \frac{1}{2}\frac{0.25}{0.205} = 0.7925 - 0.6097 = 0.1828$.
> Repetimos esse c√°lculo para todas as T observa√ß√µes e somamos os resultados para obter a fun√ß√£o de log-verossimilhan√ßa total:
> $L(\theta) = \frac{1}{1000}\sum_{t=1}^{1000} l_t(\theta)$.

√â importante notar que a escolha da distribui√ß√£o condicional para $\epsilon_t$ impacta significativamente a fun√ß√£o de log-verossimilhan√ßa e, consequentemente, a estimativa dos par√¢metros. Outras distribui√ß√µes comuns incluem a distribui√ß√£o t-Student (mencionada anteriormente), que permite modelar caudas mais pesadas do que a distribui√ß√£o normal, e a distribui√ß√£o Generalizada de Erro (GED), que oferece flexibilidade para modelar diferentes formas de cauda.

**Teorema 2.1** A estat√≠stica do teste de raz√£o de verossimilhan√ßas (LR) para comparar um modelo GARCH(1,1) com distribui√ß√£o normal condicional com um modelo GARCH(1,1) com distribui√ß√£o t-Student condicional √© dada por:

$$LR = 2(L_{t} - L_{N})$$

Onde $L_{t}$ √© o valor m√°ximo da fun√ß√£o de log-verossimilhan√ßa sob a distribui√ß√£o t-Student e $L_{N}$ √© o valor m√°ximo da fun√ß√£o de log-verossimilhan√ßa sob a distribui√ß√£o normal. Sob a hip√≥tese nula de que a distribui√ß√£o normal √© a correta, a estat√≠stica LR segue uma distribui√ß√£o qui-quadrado com um grau de liberdade (correspondente ao par√¢metro adicional $\nu$ da distribui√ß√£o t-Student).

*Prova:* A demonstra√ß√£o √© baseada na teoria assint√≥tica dos testes de raz√£o de verossimilhan√ßas. O teste LR compara a verossimilhan√ßa dos dados sob duas hip√≥teses: uma hip√≥tese nula (distribui√ß√£o normal) e uma hip√≥tese alternativa (distribui√ß√£o t-Student). A estat√≠stica LR mede a diferen√ßa entre as log-verossimilhan√ßas maximizadas sob cada hip√≥tese. Sob certas condi√ß√µes de regularidade, a estat√≠stica LR converge para uma distribui√ß√£o qui-quadrado assintoticamente.

**Proposi√ß√£o 3** A autocorrela√ß√£o amostral dos quadrados dos res√≠duos ($\epsilon_t^2$) pode ser usada para verificar a adequa√ß√£o do modelo GARCH(1,1). Se o modelo for adequado, as autocorrela√ß√µes dos res√≠duos padronizados (${\epsilon_t^2}/{h_t}$) devem ser pr√≥ximas de zero.

*Prova:* Se o modelo GARCH(1,1) captura adequadamente a depend√™ncia temporal na volatilidade, os res√≠duos padronizados devem ser aproximadamente independentes e identicamente distribu√≠dos (iid). Portanto, suas autocorrela√ß√µes devem ser insignificantes. Desvios significativos das autocorrela√ß√µes de zero indicam que o modelo pode n√£o estar capturando toda a depend√™ncia temporal na volatilidade e pode ser necess√°rio considerar um modelo mais complexo ou uma distribui√ß√£o condicional diferente.

### Conclus√£o

O modelo GARCH(1,1) representa uma ferramenta poderosa e flex√≠vel para a modelagem de volatilidade em s√©ries temporais. Sua simplicidade, aliada √† capacidade de capturar *clustering* de volatilidade e mem√≥ria longa, o tornam uma escolha popular em diversas aplica√ß√µes, particularmente em finan√ßas. A interpreta√ß√£o dos par√¢metros e o entendimento das condi√ß√µes de estacionariedade s√£o cruciais para a aplica√ß√£o e interpreta√ß√£o corretas do modelo.

### Refer√™ncias
[^1]: Bollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. *Journal of Econometrics, 31*(3), 307-327.
[^2]: Engle, R. F. (1982). Autoregressive conditional heteroskedasticity with estimates of the variance of United Kingdom inflation. *Econometrica, 50*(4), 987-1007.
[^3]: Ver se√ß√£o 2 do artigo de Bollerslev (1986)
[^4]: Ver equa√ß√£o (7) do artigo de Bollerslev (1986)
[^5]: Ver se√ß√£o 3 do artigo de Bollerslev (1986)
[^6]: Harvey, A.C. (1982). The econometric analysis of time series (Philip Allen, Oxford).
<!-- END -->