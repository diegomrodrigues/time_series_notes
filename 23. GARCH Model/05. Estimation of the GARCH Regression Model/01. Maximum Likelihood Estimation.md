Maximum Likelihood Estimation in GARCH Regression Models

### Introdu√ß√£o
Este cap√≠tulo se aprofunda no processo de **Estima√ß√£o de M√°xima Verossimilhan√ßa (MLE)** para modelos de **Regress√£o GARCH (Generalized Autoregressive Conditional Heteroskedastic)**. Construindo sobre a introdu√ß√£o dos modelos GARCH [^1] e sua import√¢ncia na modelagem de s√©ries temporais com vari√¢ncia condicional vari√°vel, focaremos em como a MLE √© implementada para estimar os par√¢metros desses modelos. A t√©cnica MLE desempenha um papel crucial na infer√™ncia estat√≠stica, fornecendo estimativas consistentes e assintoticamente eficientes dos par√¢metros do modelo [^9].

### Conceitos Fundamentais
A **MLE** √© um m√©todo estat√≠stico para estimar os par√¢metros de um modelo de probabilidade, maximizando uma fun√ß√£o de verossimilhan√ßa, que representa a probabilidade dos dados observados em fun√ß√£o dos par√¢metros do modelo [^1]. No contexto dos modelos de regress√£o GARCH, o objetivo √© estimar os par√¢metros que governam tanto a equa√ß√£o da m√©dia condicional quanto a equa√ß√£o da vari√¢ncia condicional.

Dado um modelo de regress√£o GARCH (1), (2) e (3) [^3], e seguindo a nota√ß√£o em [^9], seja $z_t^t = (1, \epsilon_{t-1}^2,..., \epsilon_{t-q}^2, h_{t-1},..., h_{t-p})$, e $\omega' = (\alpha_0, \alpha_1,..., \alpha_q, \beta_1,..., \beta_p)$. Defina $\theta \in \Theta$, onde $\theta = (b', \omega')$ e $\Theta$ √© um subespa√ßo compacto de um espa√ßo Euclidiano tal que $\epsilon_t$ possua momentos finitos [^9]. Denote os verdadeiros par√¢metros por $\theta_0$, onde $\theta_0 \in \text{int } \Theta$ [^9]. Podemos reescrever o modelo como $\epsilon_t = y_t - x_t'b$, com $\epsilon_t|\psi_{t-1} \sim N(0, h_t)$, e $h_t = z_t'\omega$ [^9].

A fun√ß√£o de log-verossimilhan√ßa para uma amostra de $T$ observa√ß√µes, omitindo uma constante, √© dada por [^9]:

$$
L_T(\theta) = T^{-1} \sum_{t=1}^{T} l_t(\theta),
$$

onde

$$
l_t(\theta) = -\frac{1}{2} \log h_t - \frac{1}{2} \epsilon_t^2 h_t^{-1}.
$$

Esta equa√ß√£o [^9] quantifica a adequa√ß√£o dos par√¢metros do modelo aos dados observados. O primeiro termo, $-\frac{1}{2} \log h_t$, reflete a penalidade para alta volatilidade, enquanto o segundo termo, $-\frac{1}{2} \epsilon_t^2 h_t^{-1}$, penaliza os res√≠duos grandes em rela√ß√£o √† volatilidade estimada. A soma dessas penalidades sobre todas as observa√ß√µes $T$ fornece uma medida geral da adequa√ß√£o do modelo.

> üí° **Exemplo Num√©rico:**
> Suponha que temos as seguintes observa√ß√µes para um modelo GARCH(1,1):
> $y_t = 0.1y_{t-1} + \epsilon_t$
> $h_t = 0.1 + 0.3\epsilon_{t-1}^2 + 0.6h_{t-1}$
>
> Para a primeira observa√ß√£o (t=1), assumimos $y_0 = 0$, $\epsilon_0 = 0$, e $h_0 = 0.2$ (um valor inicial para a vari√¢ncia).
> Para a segunda observa√ß√£o (t=2), suponha que $y_1 = 0.5$. Ent√£o, $\epsilon_1 = y_1 - 0.1y_0 = 0.5 - 0.1(0) = 0.5$.
> Agora, $h_1 = 0.1 + 0.3\epsilon_0^2 + 0.6h_0 = 0.1 + 0.3(0)^2 + 0.6(0.2) = 0.1 + 0 + 0.12 = 0.22$.
>
> Usando a fun√ß√£o de log-verossimilhan√ßa,
> $l_1(\theta) = -\frac{1}{2} \log h_1 - \frac{1}{2} \epsilon_1^2 h_1^{-1} = -\frac{1}{2} \log(0.22) - \frac{1}{2} (0.5)^2 (0.22)^{-1} \approx 0.759 - 0.568 = 0.191$
>
> Para a terceira observa√ß√£o (t=3), suponha que $y_2 = 0.2$. Ent√£o, $\epsilon_2 = y_2 - 0.1y_1 = 0.2 - 0.1(0.5) = 0.2 - 0.05 = 0.15$.
> Agora, $h_2 = 0.1 + 0.3\epsilon_1^2 + 0.6h_1 = 0.1 + 0.3(0.5)^2 + 0.6(0.22) = 0.1 + 0.075 + 0.132 = 0.307$.
>
> $l_2(\theta) = -\frac{1}{2} \log h_2 - \frac{1}{2} \epsilon_2^2 h_2^{-1} = -\frac{1}{2} \log(0.307) - \frac{1}{2} (0.15)^2 (0.307)^{-1} \approx 0.593 - 0.037 = 0.556$
>
> Para uma amostra de tamanho T=2:
> $L_T(\theta) = \frac{1}{2} (l_1(\theta) + l_2(\theta)) = \frac{1}{2} (0.191 + 0.556) = \frac{1}{2}(0.747) \approx 0.3735$
>
> Este exemplo ilustra como calcular os valores da fun√ß√£o de log-verossimilhan√ßa para as primeiras duas observa√ß√µes de uma s√©rie temporal, dados os par√¢metros do modelo GARCH(1,1).  Para obter as estimativas de m√°xima verossimilhan√ßa, este processo seria repetido para todas as observa√ß√µes na amostra e, em seguida, um algoritmo de otimiza√ß√£o seria usado para encontrar os valores dos par√¢metros que maximizam a fun√ß√£o de log-verossimilhan√ßa.

Derivando com rela√ß√£o ao par√¢metro de vari√¢ncia $\omega$ [^9], obtemos:

$$
\frac{\partial l_t}{\partial \omega} = \left[-\frac{1}{2} + \frac{\epsilon_t^2}{2h_t}\right] \frac{\partial h_t}{\partial \omega} \frac{1}{h_t},
$$

e

$$
\frac{\partial^2 l_t}{\partial \omega \partial \omega'} = \left[-\frac{1}{h_t}\right] \frac{\partial h_t}{\partial \omega} \frac{\partial h_t}{\partial \omega'} + \left[-\frac{1}{2} + \frac{\epsilon_t^2}{2h_t}\right] \left[-\frac{1}{h_t^2} \right] \frac{\partial h_t}{\partial \omega} \frac{\partial h_t}{\partial \omega'} + \left[-\frac{1}{2} + \frac{\epsilon_t^2}{2h_t}\right] \frac{1}{h_t} \frac{\partial^2 h_t}{\partial \omega \partial \omega'}.
$$

A partir dessas derivadas [^9], pode-se implementar algoritmos de otimiza√ß√£o para encontrar os valores de $\omega$ que maximizam a fun√ß√£o de log-verossimilhan√ßa.

Da mesma forma, a diferencia√ß√£o em rela√ß√£o aos par√¢metros da m√©dia produz [^9]:

$$
\frac{\partial l_t}{\partial b} = \epsilon_t x_t h_t^{-1} + \frac{1}{2}h_t^{-1} \frac{\partial h_t}{\partial b},
$$

$$
\frac{\partial^2 l_t}{\partial b \partial b'} = -h_t^{-1}x_t'x_t - \frac{1}{2}h_t^{-2} x_t'x_t \frac{\partial h_t}{\partial b} + \left( \frac{\epsilon_t^2}{h_t} -1 \right) x_t x_t' \left( \frac{1}{2}\frac{\partial h_t}{\partial b} \right).
$$

Aqui, $\frac{\partial h_t}{\partial \omega} = z_t + \sum_{i=1}^p \beta_i \frac{\partial h_{t-i}}{\partial \omega}$ [^10]. O c√°lculo dessas derivadas [^9] √© fundamental para implementar m√©todos de otimiza√ß√£o para estimar os par√¢metros do modelo GARCH. A complexidade reside na natureza recursiva da vari√¢ncia condicional $h_t$, que requer considera√ß√£o cuidadosa ao calcular as derivadas.

> üí° **Exemplo Num√©rico:**
> Continuamos com o modelo GARCH(1,1) do exemplo anterior e calculamos as primeiras derivadas para a segunda observa√ß√£o (t=2).
>
> Temos: $y_t = 0.1y_{t-1} + \epsilon_t$, $h_t = 0.1 + 0.3\epsilon_{t-1}^2 + 0.6h_{t-1}$.
> Para t=2, $y_2 = 0.2$, $\epsilon_2 = 0.15$, e $h_2 = 0.307$. Seja $x_t = y_{t-1}$, ent√£o $x_2 = y_1 = 0.5$.
> Os par√¢metros s√£o $b = 0.1$, $\omega = (\alpha_0, \alpha_1, \beta_1) = (0.1, 0.3, 0.6)$.
>
> Primeiro, calculamos $\frac{\partial l_2}{\partial b}$:
> $\frac{\partial l_2}{\partial b} = \epsilon_2 x_2 h_2^{-1} = (0.15)(0.5)(0.307)^{-1} \approx 0.244$.
>
> Agora, calculamos $\frac{\partial h_t}{\partial \omega}$:
> $\frac{\partial h_t}{\partial \alpha_0} = 1$, $\frac{\partial h_t}{\partial \alpha_1} = \epsilon_{t-1}^2$, $\frac{\partial h_t}{\partial \beta_1} = h_{t-1}$.
> Para t=2:
> $\frac{\partial h_2}{\partial \alpha_0} = 1$, $\frac{\partial h_2}{\partial \alpha_1} = \epsilon_1^2 = (0.5)^2 = 0.25$, $\frac{\partial h_2}{\partial \beta_1} = h_1 = 0.22$.
>
> Ent√£o, calculamos $\frac{\partial l_2}{\partial \omega}$:
> $\frac{\partial l_2}{\partial \omega} = \left[-\frac{1}{2} + \frac{\epsilon_2^2}{2h_2}\right] \frac{\partial h_2}{\partial \omega} \frac{1}{h_2} = \left[-\frac{1}{2} + \frac{(0.15)^2}{2(0.307)}\right] \frac{\partial h_2}{\partial \omega} \frac{1}{0.307} \approx [-0.5 + 0.0366] \frac{\partial h_2}{\partial \omega} \frac{1}{0.307} \approx -1.50 \frac{\partial h_2}{\partial \omega}$.
> Portanto:
> $\frac{\partial l_2}{\partial \alpha_0} \approx -1.50(1) = -1.50$,
> $\frac{\partial l_2}{\partial \alpha_1} \approx -1.50(0.25) = -0.375$,
> $\frac{\partial l_2}{\partial \beta_1} \approx -1.50(0.22) = -0.33$.
>
> Estas derivadas s√£o usadas em algoritmos de otimiza√ß√£o para estimar os par√¢metros do modelo.  Em cada itera√ß√£o do algoritmo, as derivadas s√£o calculadas e usadas para atualizar as estimativas dos par√¢metros.

√â importante notar que a inclus√£o da parte recursiva √© a √∫nica diferen√ßa de Engle (1982) [^10]. A condi√ß√£o $B(1) < 1$ garante que (21) seja est√°vel [^10].

**Proposi√ß√£o 1.** *Sob condi√ß√µes de regularidade apropriadas (ver [^9] e refer√™ncias l√° contidas), o estimador de m√°xima verossimilhan√ßa $\hat{\theta}$ √© consistente e assintoticamente normal, isto √©,*

$$
\sqrt{T}(\hat{\theta} - \theta_0) \xrightarrow{d} N(0, A^{-1}BA^{-1}),
$$

*onde* $A = E\left[ \frac{-\partial^2 l_t(\theta_0)}{\partial \theta \partial \theta'} \right]$ *e* $B = E\left[ \frac{\partial l_t(\theta_0)}{\partial \theta} \frac{\partial l_t(\theta_0)}{\partial \theta'} \right]$.

*Observa√ß√£o:* A matriz $A$ √© a matriz de informa√ß√£o de Fisher, e $B$ √© a matriz de vari√¢ncia assint√≥tica do escore. Se o modelo for corretamente especificado, ent√£o $A = B$, e a matriz de vari√¢ncia assint√≥tica do estimador de m√°xima verossimilhan√ßa se reduz a $A^{-1}$. No entanto, em muitas aplica√ß√µes, o modelo pode n√£o ser corretamente especificado, e √© importante usar o estimador robusto $A^{-1}BA^{-1}$ para a matriz de vari√¢ncia assint√≥tica.

**Lema 1.** *Se* $E[\epsilon_t^4] < \infty$, *ent√£o* $E[h_t^2] < \infty$.

*Prova:* Da equa√ß√£o da vari√¢ncia condicional $h_t = \alpha_0 + \sum_{i=1}^q \alpha_i \epsilon_{t-i}^2 + \sum_{j=1}^p \beta_j h_{t-j}$, temos

$$
E[h_t^2] = E\left[ \left( \alpha_0 + \sum_{i=1}^q \alpha_i \epsilon_{t-i}^2 + \sum_{j=1}^p \beta_j h_{t-j} \right)^2 \right].
$$

Expandindo e usando a desigualdade de Cauchy-Schwarz, obtemos

$$
E[h_t^2] \leq C + \sum_{i=1}^q \alpha_i^2 E[\epsilon_{t-i}^4] + \sum_{j=1}^p \beta_j^2 E[h_{t-j}^2],
$$

onde $C$ √© uma constante. Se $E[\epsilon_t^4] < \infty$ e $\sum_{j=1}^p \beta_j^2 < 1$, ent√£o $E[h_t^2] < \infty$.

<!-- Teorema 1 -->
**Teorema 1.** *Sob as condi√ß√µes da Proposi√ß√£o 1 e do Lema 1, o estimador de m√°xima verossimilhan√ßa para os par√¢metros do modelo GARCH possui momentos finitos de at√© ordem dois.*

*Prova:* A prova segue diretamente da Proposi√ß√£o 1 e do Lema 1. Uma vez que o estimador de m√°xima verossimilhan√ßa √© consistente e assintoticamente normal, seus momentos finitos existem. Al√©m disso, uma vez que $E[\epsilon_t^4] < \infty$ implica $E[h_t^2] < \infty$, os momentos do processo GARCH tamb√©m s√£o finitos. A combina√ß√£o desses resultados garante que o estimador de m√°xima verossimilhan√ßa para os par√¢metros do modelo GARCH possua momentos finitos de at√© ordem dois.

Al√©m do c√°lculo das derivadas, a implementa√ß√£o pr√°tica da MLE em modelos GARCH requer aten√ß√£o a algumas quest√µes importantes. Inicializa√ß√£o dos par√¢metros, por exemplo, pode impactar significativamente a converg√™ncia do algoritmo de otimiza√ß√£o. Frequentemente, √© √∫til come√ßar com valores plaus√≠veis para os par√¢metros, como aqueles obtidos por meio de m√©todos mais simples como o m√©todo dos momentos.

> üí° **Exemplo Num√©rico:**
> Para um modelo GARCH(1,1), uma inicializa√ß√£o razo√°vel dos par√¢metros poderia ser:
> *   $\alpha_0$: Uma pequena constante positiva (ex: 0.01), representando a vari√¢ncia incondicional.
> *   $\alpha_1$: Um valor entre 0 e 1 (ex: 0.1), representando o efeito do choque quadrado anterior sobre a vari√¢ncia atual.
> *   $\beta_1$: Um valor entre 0 e 1 (ex: 0.8), representando a persist√™ncia da volatilidade.
>
> Esses valores garantem que a vari√¢ncia seja positiva e que a soma $\alpha_1 + \beta_1$ seja menor que 1, satisfazendo a condi√ß√£o de estacionariedade. Uma m√° inicializa√ß√£o, como valores negativos ou $\alpha_1 + \beta_1 > 1$, pode levar a n√£o converg√™ncia ou estimativas inv√°lidas.

Al√©m disso, a verifica√ß√£o da estacionariedade e positividade da vari√¢ncia condicional √© crucial durante o processo de otimiza√ß√£o. Restri√ß√µes nos par√¢metros, como $\alpha_i > 0$ e $\beta_j > 0$, e a condi√ß√£o $\sum_{i=1}^q \alpha_i + \sum_{j=1}^p \beta_j < 1$, devem ser impostas para garantir que o modelo seja bem comportado.

> üí° **Exemplo Num√©rico:**
> Suponha que, ap√≥s algumas itera√ß√µes do algoritmo de otimiza√ß√£o, obtemos as seguintes estimativas: $\alpha_0 = 0.005$, $\alpha_1 = 0.4$, e $\beta_1 = 0.7$.
>
> Verificamos a condi√ß√£o de estacionariedade: $\alpha_1 + \beta_1 = 0.4 + 0.7 = 1.1$. Como essa soma √© maior que 1, o modelo n√£o √© estacion√°rio. Nesse caso, precisamos impor a restri√ß√£o $\alpha_1 + \beta_1 < 1$ durante a otimiza√ß√£o. Podemos fazer isso usando um algoritmo de otimiza√ß√£o com restri√ß√µes ou reparametrizando o modelo para garantir que essa condi√ß√£o seja satisfeita.

### Conclus√£o
A **MLE** √© um m√©todo poderoso para estimar os par√¢metros dos modelos de regress√£o GARCH [^1]. Ao maximizar a fun√ß√£o de log-verossimilhan√ßa, podemos obter estimativas consistentes e assintoticamente eficientes dos par√¢metros do modelo. Este cap√≠tulo forneceu um tratamento detalhado do processo de MLE, incluindo as equa√ß√µes para a fun√ß√£o de log-verossimilhan√ßa e suas derivadas. Al√©m disso, a estimativa consistente da matriz de informa√ß√£o √© dada pelo an√°logo da amostra do √∫ltimo termo em (20) [^9].

### Refer√™ncias
[^1]: Bollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. *Journal of Econometrics*, *31*(3), 307-327.
[^3]: A defini√ß√£o do modelo GARCH(p,q) apresentada na p√°gina 309 do artigo de Bollerslev (1986).
[^9]: A discuss√£o sobre a estima√ß√£o da regress√£o GARCH e a fun√ß√£o de log-verossimilhan√ßa na p√°gina 315 do artigo de Bollerslev (1986).
[^10]: A discuss√£o sobre a estima√ß√£o da regress√£o GARCH na p√°gina 316 do artigo de Bollerslev (1986).
<!-- END -->