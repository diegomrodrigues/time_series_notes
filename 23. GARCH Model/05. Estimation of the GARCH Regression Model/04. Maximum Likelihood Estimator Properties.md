### Estimadores Assintoticamente Normais e Consistentes em Modelos GARCH

### Introdu√ß√£o
Este cap√≠tulo detalha as propriedades assint√≥ticas do **Estimador de M√°xima Verossimilhan√ßa (MLE)** em modelos de regress√£o **GARCH (Generalized Autoregressive Conditional Heteroskedastic)**, especificamente sob condi√ß√µes de regularidade [^1]. Construindo sobre os cap√≠tulos anteriores que introduziram o modelo GARCH [^1], o processo de **MLE** e algoritmos de otimiza√ß√£o iterativos [^11], este cap√≠tulo enfoca a consist√™ncia e a normalidade assint√≥tica do estimador MLE. Esses s√£o resultados te√≥ricos cruciais que fornecem a base para infer√™ncia estat√≠stica e testes de hip√≥teses nos modelos GARCH.

### Conceitos Fundamentais
Como estabelecido nos cap√≠tulos anteriores, a **MLE** √© um m√©todo para estimar os par√¢metros $\theta$ de um modelo GARCH maximizando a fun√ß√£o de log-verossimilhan√ßa $L_T(\theta)$ [^1]. O objetivo √© encontrar um estimador $\hat{\theta}$ que capture da melhor forma as caracter√≠sticas dos dados observados.

Sob condi√ß√µes de regularidade apropriadas, o estimador MLE $\hat{\theta}_T$ possui propriedades assint√≥ticas desej√°veis, como consist√™ncia e normalidade assint√≥tica [^1]. Esses resultados permitem que estat√≠sticos e econometristas fa√ßam infer√™ncias sobre os verdadeiros valores dos par√¢metros e conduzam testes de hip√≥teses.

**Defini√ß√£o 1 (Consist√™ncia):** Um estimador $\hat{\theta}_T$ √© consistente para $\theta_0$ se, para qualquer $\epsilon > 0$,

$$
\lim_{T \to \infty} P(|\hat{\theta}_T - \theta_0| > \epsilon) = 0.
$$

Em outras palavras, um estimador consistente converge em probabilidade para o verdadeiro valor do par√¢metro √† medida que o tamanho da amostra tende ao infinito.

**Teorema 1 (Consist√™ncia do MLE):** Sob condi√ß√µes de regularidade apropriadas, o estimador de m√°xima verossimilhan√ßa $\hat{\theta}_T$ para os par√¢metros do modelo GARCH √© consistente para o verdadeiro valor $\theta_0$.

*Prova:* A prova da consist√™ncia do MLE para modelos GARCH √© tecnicamente envolvida e se baseia em v√°rios resultados de teoria assint√≥tica. As condi√ß√µes de regularidade garantem que a fun√ß√£o de log-verossimilhan√ßa seja bem comportada e que o MLE tenha propriedades estat√≠sticas desej√°veis.

I. **Identifica√ß√£o:** A verdadeira distribui√ß√£o de probabilidade dos dados deve ser identific√°vel de forma √∫nica dentro da classe de modelos considerados. Em outras palavras, diferentes valores de par√¢metros devem levar a diferentes distribui√ß√µes de probabilidade.

II. **Continuidade e Diferenciabilidade:** A fun√ß√£o de log-verossimilhan√ßa deve ser cont√≠nua e diferenci√°vel em rela√ß√£o aos par√¢metros. Isso permite o uso de t√©cnicas de otimiza√ß√£o baseadas em gradiente.

III. **Dom√≠nio de Par√¢metros Compacto:** O espa√ßo de par√¢metros deve ser um conjunto compacto. Isso garante que o algoritmo de otimiza√ß√£o tenha uma solu√ß√£o.

IV. **Converg√™ncia Uniforme:** A fun√ß√£o de log-verossimilhan√ßa amostral deve convergir uniformemente para sua expectativa em probabilidade.

V. **Exist√™ncia de Momentos:** Certos momentos dos dados devem ser finitos.

VI. Uma vez que as condi√ß√µes de regularidade s√£o satisfeitas, pode-se aplicar o Teorema de Identifica√ß√£o e Consist√™ncia de Wald, que fornece resultados gerais para a consist√™ncia de estimadores de m√°xima verossimilhan√ßa.

VII. Portanto, sob condi√ß√µes de regularidade apropriadas, o estimador de m√°xima verossimilhan√ßa $\hat{\theta}_T$ para os par√¢metros do modelo GARCH √© consistente para o verdadeiro valor $\theta_0$. ‚ñ†

Para complementar o Teorema 1, podemos formular um lema que estabelece uma condi√ß√£o suficiente para a identifica√ß√£o em modelos GARCH:

**Lema 1 (Identifica√ß√£o em Modelos GARCH):** Se para dois vetores de par√¢metros distintos $\theta_1$ e $\theta_2$, as fun√ß√µes de verossimilhan√ßa correspondentes $L_T(\theta_1)$ e $L_T(\theta_2)$ s√£o diferentes para todo $T$ suficientemente grande, ent√£o o modelo GARCH √© identificado.

*Prova:* A prova segue diretamente da defini√ß√£o de identifica√ß√£o. Se diferentes valores de par√¢metros levam a diferentes fun√ß√µes de verossimilhan√ßa, ent√£o o verdadeiro valor do par√¢metro pode ser identificado de forma √∫nica a partir dos dados observados. ‚ñ†

> üí° **Exemplo Num√©rico:**
> Suponha que simulamos dados de um modelo GARCH(1,1) com $\alpha_0 = 0.1$, $\alpha_1 = 0.2$ e $\beta_1 = 0.7$. Estimamos o modelo usando MLE para tamanhos de amostra crescentes (por exemplo, $T = 100, 500, 1000, 5000$). Observamos que, √† medida que $T$ aumenta, as estimativas de $\hat{\alpha}_0$, $\hat{\alpha}_1$ e $\hat{\beta}_1$ se aproximam cada vez mais dos verdadeiros valores. Este exemplo ilustra a propriedade de consist√™ncia do MLE em modelos GARCH.
>
> Suponha que temos dados simulados de um modelo GARCH(1,1):
> ```python
> import numpy as np
> from scipy.optimize import minimize
>
> # Gera dados simulados de um modelo GARCH(1,1)
> def garch_sim(alpha0, alpha1, beta1, T):
>     np.random.seed(42)
>     epsilon = np.random.randn(T)
>     h = np.zeros(T)
>     h[0] = alpha0 / (1 - beta1 - alpha1)  # Inicializa a vari√¢ncia
>     y = np.zeros(T)
>
>     for t in range(1, T):
>         h[t] = alpha0 + alpha1 * (y[t-1]**2) + beta1 * h[t-1]
>         y[t] = np.sqrt(h[t]) * epsilon[t]
>
>     return y
>
> # Define a fun√ß√£o de log-verossimilhan√ßa
> def garch_log_likelihood(params, data):
>     alpha0, alpha1, beta1 = params
>     T = len(data)
>     h = np.zeros(T)
>     h[0] = alpha0 / (1 - beta1 - alpha1)
>     log_likelihood = 0
>
>     for t in range(1, T):
>         h[t] = alpha0 + alpha1 * (data[t-1]**2) + beta1 * h[t-1]
>         log_likelihood += -0.5 * np.log(h[t]) - 0.5 * (data[t]**2) / h[t]
>
>     return -log_likelihood
>
> # Verdadeiros par√¢metros
> alpha0_true = 0.1
> alpha1_true = 0.2
> beta1_true = 0.7
>
> # Tamanho da amostra
> T = 1000
>
> # Simula os dados
> data = garch_sim(alpha0_true, alpha1_true, beta1_true, T)
>
> # Estima os par√¢metros usando MLE
> initial_guess = [0.05, 0.1, 0.6]  # chute inicial
> bounds = ((0, None), (0, 1), (0, 1))  # Restri√ß√µes de par√¢metros
> result = minimize(garch_log_likelihood, initial_guess, args=(data,),
>                   method='L-BFGS-B', bounds=bounds)
>
> # Extrai as estimativas
> alpha0_hat, alpha1_hat, beta1_hat = result.x
>
> # Imprime os resultados
> print("Verdadeiro alpha0:", alpha0_true)
> print("Estimativa de alpha0:", alpha0_hat)
> print("Verdadeiro alpha1:", alpha1_true)
> print("Estimativa de alpha1:", alpha1_hat)
> print("Verdadeiro beta1:", beta1_true)
> print("Estimativa de beta1:", beta1_hat)
> ```

**Defini√ß√£o 2 (Normalidade Assint√≥tica):** Um estimador $\hat{\theta}_T$ √© assintoticamente normal se

$$
\sqrt{T}(\hat{\theta}_T - \theta_0) \xrightarrow{d} N(0, \Sigma),
$$

onde $\Sigma$ √© a matriz de covari√¢ncia assint√≥tica e $\xrightarrow{d}$ denota a converg√™ncia em distribui√ß√£o.

A normalidade assint√≥tica implica que, para tamanhos de amostra grandes, a distribui√ß√£o do estimador MLE se aproxima de uma distribui√ß√£o normal com m√©dia $\theta_0$ e matriz de covari√¢ncia $\Sigma/T$. Este resultado √© crucial para construir intervalos de confian√ßa e realizar testes de hip√≥teses.

**Teorema 2 (Normalidade Assint√≥tica do MLE):** Sob condi√ß√µes de regularidade apropriadas, o estimador de m√°xima verossimilhan√ßa $\hat{\theta}_T$ para os par√¢metros do modelo GARCH √© assintoticamente normal com matriz de covari√¢ncia $\Sigma = A^{-1}BA^{-1}$, onde $A$ e $B$ s√£o definidos como em [^9].

*Prova:* A prova da normalidade assint√≥tica do MLE para modelos GARCH se baseia no Teorema do Limite Central (TLC) e em v√°rios resultados de teoria assint√≥tica. As condi√ß√µes de regularidade garantem que o TLC pode ser aplicado e que a matriz de covari√¢ncia assint√≥tica pode ser consistentemente estimada.

I. **Condi√ß√µes de Regularidade:** As mesmas condi√ß√µes de regularidade para a consist√™ncia s√£o necess√°rias, al√©m de algumas condi√ß√µes adicionais:

    *   As terceiras derivadas da fun√ß√£o de log-verossimilhan√ßa devem ser limitadas.

    *   A matriz de informa√ß√£o de Fisher deve ser n√£o singular.

II. **Teorema do Limite Central (TLC):** Sob condi√ß√µes de regularidade, o vetor de escore, que √© a soma das derivadas da fun√ß√£o de log-verossimilhan√ßa, segue um TLC:
$$
\frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{\partial l_t(\theta_0)}{\partial \theta} \xrightarrow{d} N(0, B)
$$
onde $B$ √© a matriz de vari√¢ncia assint√≥tica do escore.

III. **Expans√£o de Taylor:** Expandimos o vetor de escore em torno do verdadeiro valor do par√¢metro $\theta_0$:
$$
\frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{\partial l_t(\hat{\theta}_T)}{\partial \theta} = \frac{1}{\sqrt{T}} \sum_{t=1}^T \frac{\partial l_t(\theta_0)}{\partial \theta} + \left[ \frac{1}{T} \sum_{t=1}^T \frac{\partial^2 l_t(\theta^*)}{\partial \theta \partial \theta'} \right] \sqrt{T} (\hat{\theta}_T - \theta_0)
$$
onde $\theta^*$ est√° entre $\hat{\theta}_T$ e $\theta_0$.

IV. **Identidade de Informa√ß√£o:** Sob correta especifica√ß√£o do modelo, a identidade de informa√ß√£o se mant√©m, o que implica que $A = B$, onde $A$ √© a matriz de informa√ß√£o de Fisher.

V. **Converg√™ncia:** Usando a consist√™ncia de $\hat{\theta}_T$ e a lei dos grandes n√∫meros, mostramos que:
$$
\frac{1}{T} \sum_{t=1}^T \frac{\partial^2 l_t(\theta^*)}{\partial \theta \partial \theta'} \xrightarrow{p} -A
$$

VI. **Distribui√ß√£o Assint√≥tica:** Finalmente, combinando os resultados acima, obtemos a normalidade assint√≥tica do MLE:
$$
\sqrt{T}(\hat{\theta}_T - \theta_0) \xrightarrow{d} N(0, A^{-1}BA^{-1})
$$
Sob correta especifica√ß√£o, $A = B$, ent√£o a matriz de covari√¢ncia assint√≥tica simplifica para $A^{-1}$. ‚ñ†

Com base no Teorema 2, podemos derivar um corol√°rio que fornece a distribui√ß√£o assint√≥tica de uma fun√ß√£o dos estimadores MLE:

**Corol√°rio 2.1:** Seja $g(\theta)$ uma fun√ß√£o continuamente diferenci√°vel de $\theta$. Ent√£o, sob as mesmas condi√ß√µes do Teorema 2,

$$
\sqrt{T}(g(\hat{\theta}_T) - g(\theta_0)) \xrightarrow{d} N(0, \nabla g(\theta_0)' A^{-1} \nabla g(\theta_0)),
$$

onde $\nabla g(\theta_0)$ √© o gradiente de $g(\theta)$ avaliado em $\theta_0$.

*Prova:* A prova segue diretamente do Teorema 2 e do m√©todo delta. Usando uma expans√£o de Taylor de primeira ordem de $g(\hat{\theta}_T)$ em torno de $\theta_0$, obtemos:

I. Expans√£o de Taylor de primeira ordem:
$$
g(\hat{\theta}_T) \approx g(\theta_0) + \nabla g(\theta_0)' (\hat{\theta}_T - \theta_0).
$$

II. Multiplicando por $\sqrt{T}$:
$$
\sqrt{T}(g(\hat{\theta}_T) - g(\theta_0)) \approx \sqrt{T} \nabla g(\theta_0)' (\hat{\theta}_T - \theta_0).
$$

III. Aplicando o Teorema 2 e a propriedade de converg√™ncia cont√≠nua:
$$
\sqrt{T}(g(\hat{\theta}_T) - g(\theta_0)) \xrightarrow{d} N(0, \nabla g(\theta_0)' A^{-1} \nabla g(\theta_0)).
$$

Portanto, demonstramos o resultado desejado. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Utilizando os dados simulados do exemplo anterior, calculamos um intervalo de confian√ßa de 95% para $\alpha_1$ usando a propriedade de normalidade assint√≥tica do MLE.
> ```python
> import numpy as np
> from scipy.optimize import minimize
> from scipy.stats import norm
>
> # Gera dados simulados de um modelo GARCH(1,1)
> def garch_sim(alpha0, alpha1, beta1, T):
>     np.random.seed(42)
>     epsilon = np.random.randn(T)
>     h = np.zeros(T)
>     h[0] = alpha0 / (1 - beta1 - alpha1)  # Inicializa a vari√¢ncia
>     y = np.zeros(T)
>
>     for t in range(1, T):
>         h[t] = alpha0 + alpha1 * (y[t-1]**2) + beta1 * h[t-1]
>         y[t] = np.sqrt(h[t]) * epsilon[t]
>
>     return y
>
> # Define a fun√ß√£o de log-verossimilhan√ßa
> def garch_log_likelihood(params, data):
>     alpha0, alpha1, beta1 = params
>     T = len(data)
>     h = np.zeros(T)
>     h[0] = alpha0 / (1 - beta1 - alpha1)
>     log_likelihood = 0
>
>     for t in range(1, T):
>         h[t] = alpha0 + alpha1 * (data[t-1]**2) + beta1 * h[t-1]
>         log_likelihood += -0.5 * np.log(h[t]) - 0.5 * (data[t]**2) / h[t]
>
>     return -log_likelihood
>
> # Verdadeiros par√¢metros
> alpha0_true = 0.1
> alpha1_true = 0.2
> beta1_true = 0.7
>
> # Tamanho da amostra
> T = 1000
>
> # Simula os dados
> data = garch_sim(alpha0_true, alpha1_true, beta1_true, T)
>
> # Estima os par√¢metros usando MLE
> initial_guess = [0.05, 0.1, 0.6]  # chute inicial
> bounds = ((0, None), (0, 1), (0, 1))  # Restri√ß√µes de par√¢metros
> result = minimize(garch_log_likelihood, initial_guess, args=(data,),
>                   method='L-BFGS-B', bounds=bounds,
>                   options={'disp': False}) # suprime a sa√≠da
>
> # Extrai as estimativas
> alpha0_hat, alpha1_hat, beta1_hat = result.x
>
> # Calcula a matriz Hessiana (aproxima√ß√£o)
> hessian_approx = result.hess_inv.todense()
>
> # Calcula o erro padr√£o para alpha1
> var_alpha1 = hessian_approx[1, 1]  # assume que a hessiana √© a inversa da matriz de informa√ß√£o
> se_alpha1 = np.sqrt(var_alpha1 / T)
>
> # Define o n√≠vel de signific√¢ncia
> alpha = 0.05
>
> # Calcula o valor cr√≠tico para um intervalo de confian√ßa de 95%
> z_critical = norm.ppf(1 - alpha / 2)
>
> # Calcula o intervalo de confian√ßa
> ci_lower = alpha1_hat - z_critical * se_alpha1
> ci_upper = alpha1_hat + z_critical * se_alpha1
>
> # Imprime os resultados
> print("Estimativa de alpha1:", alpha1_hat)
> print("Erro padr√£o de alpha1:", se_alpha1)
> print("Intervalo de confian√ßa de 95% para alpha1:", (ci_lower, ci_upper))
> ```

Onde:
* result.hess_inv fornece a inversa da matriz Hessiana, que podemos usar para aproximar a matriz de covari√¢ncia assint√≥tica do estimador MLE.
* se_alpha1 calcula o erro padr√£o para $\alpha_1$ com base na inversa da Hessiana e no tamanho da amostra T.
* z_critical √© o valor cr√≠tico da distribui√ß√£o normal padr√£o para um n√≠vel de signific√¢ncia de Œ± = 0.05, que √© usado para calcular o intervalo de confian√ßa.
* ci_lower e ci_upper s√£o os limites inferior e superior do intervalo de confian√ßa de 95% para Œ±1.
* A execu√ß√£o do c√≥digo fornecer√° a estimativa de Œ±1, seu erro padr√£o e o intervalo de confian√ßa de 95%.

Em seguida, com base nessas estimativas, constru√≠mos um intervalo de confian√ßa de 95% para $\alpha_1$ usando a propriedade de normalidade assint√≥tica do MLE. Se o intervalo de confian√ßa contiver o verdadeiro valor de $\alpha_1$, isso fornece evid√™ncias adicionais da precis√£o e confiabilidade do estimador MLE.

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar o Corol√°rio 2.1, suponha que estejamos interessados na persist√™ncia do modelo GARCH(1,1), que pode ser definida como $\rho = \alpha_1 + \beta_1$. Podemos usar o m√©todo delta para estimar a distribui√ß√£o assint√≥tica de $\hat{\rho} = \hat{\alpha}_1 + \hat{\beta}_1$. O gradiente de $\rho$ com rela√ß√£o a $\theta = (\alpha_0, \alpha_1, \beta_1)'$ √© $\nabla \rho(\theta) = (0, 1, 1)'$. Portanto, a vari√¢ncia assint√≥tica de $\sqrt{T}(\hat{\rho} - \rho_0)$ √© dada por $(0, 1, 1) A^{-1} (0, 1, 1)'$, onde $A$ √© a matriz de informa√ß√£o de Fisher.
>
> Usando as estimativas do exemplo anterior:
> ```python
> import numpy as np
> from scipy.optimize import minimize
> from scipy.stats import norm
>
> # Gera dados simulados de um modelo GARCH(1,1)
> def garch_sim(alpha0, alpha1, beta1, T):
>     np.random.seed(42)
>     epsilon = np.random.randn(T)
>     h = np.zeros(T)
>     h[0] = alpha0 / (1 - beta1 - alpha1)  # Inicializa a vari√¢ncia
>     y = np.zeros(T)
>
>     for t in range(1, T):
>         h[t] = alpha0 + alpha1 * (y[t-1]**2) + beta1 * h[t-1]
>         y[t] = np.sqrt(h[t]) * epsilon[t]
>
>     return y
>
> # Define a fun√ß√£o de log-verossimilhan√ßa
> def garch_log_likelihood(params, data):
>     alpha0, alpha1, beta1 = params
>     T = len(data)
>     h = np.zeros(T)
>     h[0] = alpha0 / (1 - beta1 - alpha1)
>     log_likelihood = 0
>
>     for t in range(1, T):
>         h[t] = alpha0 + alpha1 * (data[t-1]**2) + beta1 * h[t-1]
>         log_likelihood += -0.5 * np.log(h[t]) - 0.5 * (data[t]**2) / h[t]
>
>     return -log_likelihood
>
> # Verdadeiros par√¢metros
> alpha0_true = 0.1
> alpha1_true = 0.2
> beta1_true = 0.7
>
> # Tamanho da amostra
> T = 1000
>
> # Simula os dados
> data = garch_sim(alpha0_true, alpha1_true, beta1_true, T)
>
> # Estima os par√¢metros usando MLE
> initial_guess = [0.05, 0.1, 0.6]  # chute inicial
> bounds = ((0, None), (0, 1), (0, 1))  # Restri√ß√µes de par√¢metros
> result = minimize(garch_log_likelihood, initial_guess, args=(data,),
>                   method='L-BFGS-B', bounds=bounds,
>                   options={'disp': False}) # suprime a sa√≠da
>
> # Extrai as estimativas
> alpha0_hat, alpha1_hat, beta1_hat = result.x
>
> # Calcula a matriz Hessiana (aproxima√ß√£o)
> hessian_approx = result.hess_inv.todense()
>
> # Calcula o erro padr√£o para alpha1
> var_alpha1 = hessian_approx[1, 1]  # assume que a hessiana √© a inversa da matriz de informa√ß√£o
> se_alpha1 = np.sqrt(var_alpha1 / T)
>
> # Calcula o erro padr√£o para beta1
> var_beta1 = hessian_approx[2, 2]
> se_beta1 = np.sqrt(var_beta1 / T)
>
> # Calcula a estimativa de rho
> rho_hat = alpha1_hat + beta1_hat
>
> # Calcula o erro padr√£o de rho usando o m√©todo delta
> # Var(rho) = Var(alpha1) + Var(beta1) + 2*Cov(alpha1, beta1)
> cov_alpha1_beta1 = hessian_approx[1, 2]
> var_rho = var_alpha1 + var_beta1 + 2 * cov_alpha1_beta1
> se_rho = np.sqrt(var_rho / T)
>
> # Define o n√≠vel de signific√¢ncia
> alpha = 0.05
>
> # Calcula o valor cr√≠tico para um intervalo de confian√ßa de 95%
> z_critical = norm.ppf(1 - alpha / 2)
>
> # Calcula o intervalo de confian√ßa
> ci_lower = rho_hat - z_critical * se_rho
> ci_upper = rho_hat + z_critical * se_rho
>
> # Imprime os resultados
> print("Estimativa de rho:", rho_hat)
> print("Erro padr√£o de rho:", se_rho)
> print("Intervalo de confian√ßa de 95% para rho:", (ci_lower, ci_upper))
> ```

√â importante enfatizar que as propriedades assint√≥ticas do MLE valem sob condi√ß√µes de regularidade apropriadas [^1]. Se essas condi√ß√µes n√£o forem satisfeitas, as propriedades assint√≥ticas do MLE podem n√£o se manter e m√©todos de infer√™ncia alternativos podem ser necess√°rios.

Al√©m dos resultados assint√≥ticos para o MLE, √© √∫til considerar a consist√™ncia e normalidade assint√≥tica de estimadores alternativos, como o **Estimador de M√≠nimos Quadrados (OLS)**, quando aplic√°vel.

**Proposi√ß√£o 3 (Consist√™ncia e Normalidade Assint√≥tica do OLS em Modelos Lineares):** Sob as condi√ß√µes usuais de Gauss-Markov, o estimador OLS em um modelo linear √© o melhor estimador linear n√£o viesado (BLUE). Al√©m disso, √© consistente e assintoticamente normal.

*Prova:* A prova da consist√™ncia e normalidade assint√≥tica do OLS sob as condi√ß√µes de Gauss-Markov √© bem estabelecida.

I. **Modelo Linear:** Considere o modelo linear $y = X\beta + \epsilon$, onde $y$ √© o vetor de vari√°veis dependentes, $X$ √© a matriz de regressores, $\beta$ √© o vetor de par√¢metros e $\epsilon$ √© o vetor de erro.

II. **Condi√ß√µes de Gauss-Markov:** As condi√ß√µes de Gauss-Markov s√£o:
    * $E[\epsilon] = 0$
    * $Var(\epsilon) = \sigma^2 I$, onde $I$ √© a matriz identidade.
    * $X$ √© uma matriz de posto completo.

III. **Estimador OLS:** O estimador OLS √© dado por $\hat{\beta} = (X'X)^{-1}X'y$.

IV. **N√£o Viesado:** Sob as condi√ß√µes de Gauss-Markov, $E[\hat{\beta}] = \beta$, o que significa que o estimador OLS √© n√£o viesado.

V. **Melhor Estimador Linear N√£o Viesado (BLUE):** O Teorema de Gauss-Markov afirma que $\hat{\beta}$ √© o BLUE.

VI. **Consist√™ncia:** Sob as condi√ß√µes de Gauss-Markov e a condi√ß√£o adicional de que $\frac{1}{n}X'X$ converge para uma matriz definida positiva, $\hat{\beta}$ √© consistente, ou seja, $\hat{\beta} \xrightarrow{p} \beta$.

VII. **Normalidade Assint√≥tica:** Se assumirmos que os erros s√£o normalmente distribu√≠dos, ou seja, $\epsilon \sim N(0, \sigma^2 I)$, ent√£o $\hat{\beta}$ √© normalmente distribu√≠do. Mesmo sem a suposi√ß√£o de normalidade, $\hat{\beta}$ √© assintoticamente normal devido ao Teorema do Limite Central. Especificamente,
$$
\sqrt{n}(\hat{\beta} - \beta) \xrightarrow{d} N(0, \sigma^2 (X'X/n)^{-1}).
$$

Portanto, sob as condi√ß√µes usuais de Gauss-Markov, o estimador OLS √© consistente e assintoticamente normal. ‚ñ†

Embora os modelos GARCH n√£o sejam lineares, esta proposi√ß√£o fornece um contraste √∫til e destaca a import√¢ncia de condi√ß√µes de regularidade para garantir boas propriedades assint√≥ticas dos estimadores.

### Conclus√£o
A consist√™ncia e a normalidade assint√≥tica do estimador de m√°xima verossimilhan√ßa s√£o resultados fundamentais na estima√ß√£o de modelos GARCH [^1]. Eles fornecem a base para infer√™ncia estat√≠stica e testes de hip√≥teses. Entender as condi√ß√µes de regularidade sob as quais esses resultados se mant√™m √© crucial para aplicar e interpretar corretamente os modelos GARCH na pr√°tica.

### Refer√™ncias
[^1]: Bollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. *Journal of Econometrics*, *31*(3), 307-327.
[^9]: A discuss√£o sobre a estima√ß√£o da regress√£o GARCH e a fun√ß√£o de log-verossimilhan√ßa na p√°gina 315 do artigo de Bollerslev (1986).
[^10]: A discuss√£o sobre a estima√ß√£o da regress√£o GARCH na p√°gina 316 do artigo de Bollerslev (196).
[^11]: Berndt, E.K., Hall, B.H., Hall, R.E. e Hausman, J.A. (1974). Estimation inference in nonlinear structural models, Annals of Economic and Social Measurement, no. 4, 653-665.
<!-- END -->