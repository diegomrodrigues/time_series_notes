## Distribuição Assintótica do Estimador de Π em Modelos VAR
### Introdução
Este capítulo aprofunda a análise de modelos de Vetores Autorregressivos (VAR), com foco especial na distribuição assintótica dos estimadores dos coeficientes, representados pela matriz $\Pi$. Como vimos anteriormente, a estimação de modelos VAR envolve regressões OLS e a inferência estatística desempenha um papel crucial na avaliação da significância dos resultados. Este capítulo avança na discussão da estimação do VAR, explorando como as propriedades assintóticas do estimador de $\Pi$ permitem a inferência estatística, mesmo quando as inovações não são gaussianas.

### Conceitos Fundamentais
Como estabelecido anteriormente, um modelo VAR pode ser expresso de forma compacta usando a notação [^1]:
$$ y_t = \Pi'x_t + \epsilon_t $$
onde $y_t$ é um vetor de variáveis endógenas, $x_t$ contém um termo constante e lags de $y_t$, e $\epsilon_t$ é o vetor de inovação. O objetivo é estimar a matriz de coeficientes $\Pi$. A estimação por Máxima Verossimilhança (MLE) para $\Pi$, como derivado em [^2], é dada por:
$$ \hat{\Pi}' = \left[\sum_{t=1}^{T} y_t x_t'\right] \left[\sum_{t=1}^{T} x_t x_t'\right]^{-1} $$
que pode ser vista como o análogo amostral da projeção linear populacional de $y_t$ sobre um termo constante e $x_t$ [^2]. Notavelmente, cada linha de $\hat{\Pi}'$ corresponde ao vetor de coeficientes estimados por uma regressão OLS de um dos elementos de $y_t$ sobre $x_t$.

A distribuição assintótica de $\hat{\Pi}$ é fundamental para testes de hipóteses e construção de intervalos de confiança. A Proposição 11.1 [^3] estabelece essa distribuição assintótica:
**Proposição 11.1**: Considere o modelo
$$ y_t = c + \Phi_1 y_{t-1} + \Phi_2 y_{t-2} + \dots + \Phi_p y_{t-p} + \epsilon_t $$
onde $\epsilon_t$ são i.i.d com média zero e variância $\Omega$, e $E(\epsilon_i \epsilon_j \epsilon_l \epsilon_m) < \infty$ para todos os $i$, $j$, $l$, e $m$. Além disso, assume-se que as raízes de $|I_n - \Phi_1 z - \Phi_2 z^2 - \dots - \Phi_p z^p| = 0$ estão fora do círculo unitário. Seja $k = np+1$ e $x_t = [1, y_{t-1}', \dots, y_{t-p}']'$ e $\hat{\Pi}_T = vec(\hat{\Pi}')$ o vetor de coeficientes obtidos por regressão OLS. Finalmente, seja $\Pi = vec(\Pi')$. Então:
1.  $\frac{1}{T} \sum_{t=1}^T x_t x_t' \overset{p}{\rightarrow} Q$ onde $Q = E(x_t x_t')$ [^3]
2.  $\hat{\Pi}_T \overset{p}{\rightarrow} \Pi$ [^3]
3.  $\hat{\Omega}_T \overset{p}{\rightarrow} \Omega$, onde $\hat{\Omega}_T = \frac{1}{T} \sum_{t=1}^T \hat{\epsilon}_t \hat{\epsilon}_t'$
4.  $\sqrt{T}(\hat{\Pi}_T - \Pi) \overset{d}{\rightarrow} N(0, (\Omega \otimes Q^{-1}))$ [^3]

O resultado (a) estabelece que a matriz de covariância amostral dos regressores converge em probabilidade para a sua expectativa, denotada por Q.  O resultado (b) estabelece a consistência do estimador $\Pi$, indicando que $\hat{\Pi}_T$ converge em probabilidade para o seu valor populacional $\Pi$. O resultado (c) mostra que o estimador da matriz de variância-covariância dos resíduos converge em probabilidade para a matriz $\Omega$. O resultado (d) formaliza a distribuição assintótica dos estimadores dos coeficientes. O estimador empírico $\hat{\Pi}_T$ é assintoticamente normalmente distribuído, com média igual ao verdadeiro valor populacional $\Pi$ e matriz de variância-covariância igual a $(\Omega \otimes Q^{-1})$. O símbolo $\otimes$ denota o produto de Kronecker.

Este resultado é notável porque estabelece que, sob condições suaves, a distribuição dos estimadores dos parâmetros do VAR são assintoticamente normais, independentemente da distribuição das inovações, desde que os momentos de quarta ordem sejam finitos.  Isso permite o uso de resultados de inferência estatística padrão, como testes t e F, que são baseados na aproximação normal para avaliar a significância dos coeficientes estimados. Note também que mesmo que as inovações não sejam gaussianas, a consistência e a normalidade assintótica ainda se mantêm.

Em particular, para o j-ésimo conjunto de coeficientes $\hat{\pi}_i$ (a j-ésima linha de $\hat{\Pi}'$), o resultado (d) implica que:
$$ \sqrt{T}(\hat{\pi}_{j,T} - \pi_j) \overset{d}{\rightarrow} N(0, \sigma_j^2 Q^{-1}) $$
onde $\sigma_j^2$ é a variância da inovação da j-ésima equação, que é estimada consistentemente pela média dos resíduos quadrados da j-ésima equação [^3].  Assim, cada conjunto de coeficientes é assintoticamente normal, com variância estimada consistentemente pelos resíduos da respectiva regressão.

Essa distribuição assintótica permite construir intervalos de confiança e realizar testes de hipóteses sobre os coeficientes, de maneira análoga à abordagem de regressão OLS.  A matriz Q pode ser estimada consistentemente por $[(1/T) \sum_{t=1}^T x_t x_t']^{-1}$ [^3], de modo que se pode usar as formulas tradicionais do OLS para construir os intervalos de confiança.

Uma generalização deste resultado é a distribuição assintótica para combinações lineares de parâmetros do VAR. Uma hipótese mais geral da forma $R\Pi=r$, onde $R$ e $r$ são, respectivamente, uma matriz e um vetor de constantes, pode ser testada usando uma generalização da forma de Wald do teste χ² de OLS:
$$ \chi^2(m) = (R \hat{\Pi} - r)' [R (\hat{\Omega}_T \otimes Q_T^{-1}) R']^{-1} (R \hat{\Pi} - r),$$
onde $m$ é o número de linhas de R, ou seja, o número de restrições testadas [^3]. Essa estatística possui uma distribuição assintótica qui-quadrado com $m$ graus de liberdade.

Além de $\Pi$, o estimador da matriz de covariância dos erros $\Omega$ também é de interesse.  Como visto, a MLE para $\Omega$ é dada por [^2]:
$$ \hat{\Omega} = \frac{1}{T} \sum_{t=1}^T \hat{\epsilon}_t \hat{\epsilon}_t' $$
onde $\hat{\epsilon}_t = y_t - \hat{\Pi}'x_t$ são os resíduos da regressão OLS.  A consistência e distribuição assintótica de $\hat{\Omega}$ são garantidas pela proposição [^3]. É importante notar que a distribuição assintótica de $\hat{\Pi}$ e $\hat{\Omega}$ é derivada sob a condição de que as inovações $\epsilon_t$ sejam independentes e identicamente distribuídas (i.i.d) com média zero e variância $\Omega$, e que o 4º momento das inovações seja finito. A demonstração da Proposição 11.1 [^3] utiliza as propriedades de convergência em probabilidade e em distribuição, juntamente com o teorema de Slutsky, para estabelecer esses resultados.

### Conclusão
Este capítulo apresentou a distribuição assintótica do estimador da matriz de coeficientes $\Pi$ em modelos VAR. A Proposição 11.1 estabelece formalmente que, mesmo em modelos com inovações não Gaussianas, os estimadores de $\Pi$ são consistentes e assintoticamente normais, com uma matriz de variância-covariância que pode ser estimada consistentemente através das fórmulas OLS. Esses resultados fundamentam a inferência estatística em modelos VAR, permitindo a realização de testes de hipóteses e a construção de intervalos de confiança para os coeficientes. A generalização para restrições lineares sobre $\Pi$ demonstra a aplicabilidade da estatística de Wald em testes de hipóteses sobre as relações entre variáveis em modelos VAR. Esta análise fornece uma base sólida para análise e interpretação dos resultados de modelos VAR. Além disso, a Proposição 11.1 assegura que os erros padrão dos estimadores de $\Pi$ e $\Omega$ podem ser calculados usando as fórmulas OLS tradicionais.

### Referências
[^1]:  Seção 11.1
[^2]:  Seção 11.1, especificamente a equação [11.1.11] e [11.1.27]
[^3]:  Proposição 11.1, Seção 11.1, páginas 298 e 299
<!-- END -->
