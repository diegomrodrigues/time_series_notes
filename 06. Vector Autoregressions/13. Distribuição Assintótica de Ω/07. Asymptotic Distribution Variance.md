## Distribuição Assintótica de $\Omega$: Derivadas Matriciais e Operadores 'vec' e 'vech'
### Introdução
Este capítulo dedica-se à derivação da distribuição assintótica da matriz de covariância dos erros, $\Omega$, em modelos de Vetores Autorregressivos (VAR), com foco no uso de derivadas matriciais e operadores 'vec' e 'vech' para lidar com a estrutura simétrica de $\Omega$. Como discutido nos capítulos anteriores, a obtenção de erros padrão para os estimadores dos parâmetros em modelos VAR é essencial para realizar inferências estatísticas. A matriz $\Omega$, sendo uma matriz de covariância, é simétrica, o que implica que alguns de seus elementos são redundantes. Para lidar com essa redundância e derivar a distribuição assintótica de maneira eficiente, utilizamos derivadas matriciais e os operadores 'vec' (que transforma uma matriz em um vetor empilhando suas colunas) e 'vech' (que transforma uma matriz simétrica em um vetor empilhando suas colunas não redundantes).  [^3, 10, 11]

### Derivadas Matriciais e a Estimação de $\Omega$
A estimação de máxima verossimilhança (MLE) para modelos VAR envolve a maximização da função de verossimilhança em relação aos parâmetros do modelo, incluindo a matriz de covariância dos erros $\Omega$. A função de log-verossimilhança para um modelo VAR, avaliada nos estimadores de máxima verossimilhança (MLE) para $\Pi$, é dada por:
$$ \mathcal{L}(\Omega, \hat{\Pi}) = - \frac{Tn}{2} \log(2\pi) + \frac{T}{2} \log|\Omega^{-1}| - \frac{1}{2} \sum_{t=1}^T \epsilon_t' \Omega^{-1} \epsilon_t $$ [^4, 11.1.25]
onde $n$ é o número de variáveis, $T$ é o tamanho da amostra e $\epsilon_t$ são os resíduos do modelo. Para encontrar o estimador de máxima verossimilhança de $\Omega$, é necessário maximizar essa função de verossimilhança em relação a $\Omega$. Isto é feito calculando as derivadas da função de verossimilhança em relação aos elementos de $\Omega$ e igualando a zero.

#### Derivada da Forma Quadrática
A derivação da forma quadrática na função de log-verossimilhança envolve o cálculo da derivada de:
$$ \sum_{t=1}^T (y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t) $$
onde $y_t$ é um vetor de variáveis, $\Pi$ é a matriz de coeficientes, e $x_t$ é um vetor de regressores. Utilizando as regras de derivadas matriciais, podemos expressar a derivada da forma quadrática em relação aos elementos de $\Omega^{-1}$.  A expressão [11.1.20]  apresenta um resultado útil:
$$ \frac{\partial x'Ax}{\partial A} = xx' $$
[^10, 11.1.20]
Essa derivada é utilizada para encontrar o estimador de $\Pi$.

#### Derivada do Determinante
Outro resultado fundamental envolve a derivada do determinante de uma matriz, expressa na equação [11.1.21]:
$$ \frac{\partial \log|A|}{\partial a_{ij}} = a^{ji} $$
[^10, 11.1.21]
onde $a_{ij}$ são os elementos de uma matriz $A$ e $a^{ji}$ são os elementos da inversa de $A$, ou seja, $A^{-1}$. Em notação matricial, esse resultado é dado em [11.1.22]:
$$ \frac{\partial \log|A|}{\partial A} = (A^{-1})' $$
[^10, 11.1.22]
Essas derivadas são essenciais para obter os estimadores de máxima verossimilhança de $\Omega$.

### Operadores 'vec' e 'vech'
A matriz de covariância $\Omega$ é simétrica, o que significa que alguns de seus elementos são redundantes, dado que $\sigma_{ij} = \sigma_{ji}$. Para lidar com essa redundância e para construir os erros padrão de forma eficiente, usamos os operadores 'vec' e 'vech'.

#### Operador 'vec'
O operador 'vec' transforma uma matriz em um vetor, empilhando suas colunas. Se $A$ é uma matriz $m \times n$, então $vec(A)$ é um vetor $mn \times 1$ obtido ao empilhar as colunas de $A$ sequencialmente. Formalmente:
$$ vec(A) = \begin{bmatrix} a_{11} \\ a_{21} \\ \vdots \\ a_{m1} \\ a_{12} \\ a_{22} \\ \vdots \\ a_{mn} \end{bmatrix} $$
#### Operador 'vech'
O operador 'vech', por sua vez, transforma uma matriz simétrica em um vetor empilhando os elementos únicos (não redundantes), ou seja, os elementos da diagonal principal e os elementos abaixo ou acima da diagonal principal.  Se $A$ é uma matriz simétrica $n \times n$, então $vech(A)$ é um vetor $\frac{n(n+1)}{2} \times 1$ obtido ao empilhar as colunas da matriz.  Formalmente:
$$ vech(A) = \begin{bmatrix} a_{11} \\ a_{21} \\ \vdots \\ a_{n1} \\ a_{22} \\ a_{32} \\ \vdots \\ a_{nn} \end{bmatrix} $$
O operador 'vech' é crucial para modelar a simetria de $\Omega$ e criar um vetor contendo as variâncias e covariâncias únicas.

### Distribuição Assintótica de  $\hat{\Omega}$ utilizando 'vec' e 'vech'
O uso dos operadores 'vec' e 'vech' é crucial para expressar a distribuição assintótica de $\hat{\Omega}$. Como derivado em capítulos anteriores [^11], a distribuição assintótica de $\hat{\Omega}$ é dada por:
$$ \sqrt{T} [vech(\hat{\Omega}_T) - vech(\Omega)] \xrightarrow{d} N(0, \Sigma_{22}) $$
onde $\xrightarrow{d}$ denota convergência em distribuição e $\Sigma_{22}$ é a matriz de covariância assintótica. Para o caso geral de dimensão n,  a matriz $\Sigma_{22}$  pode ser expressa como:
$$ \Sigma_{22} = 2D^+ (\Omega \otimes \Omega)(D^+)' $$
[^11, 11.1.48]
onde $D^+$ é o pseudoinverso da matriz de duplicação $D$. Essa representação, usando a matriz de duplicação $D$, é crucial para lidar com a redundância de elementos em uma matriz simétrica.

#### Matriz de Duplicação
A matriz de duplicação $D_n$ (onde $n$ é o número de variáveis no VAR) é uma matriz que transforma o vetor *vech*($\Omega$) em *vec*($\Omega$). Ela tem dimensão $n^2 \times \frac{n(n+1)}{2}$. A relação entre os operadores é definida como:
$$ vec(\Omega) = D_n vech(\Omega) $$
[^10, 11.1.43]
O pseudoinverso da matriz de duplicação, $D^+$, é definido como $D^+ = (D_n' D_n)^{-1} D_n'$ e transforma um vetor *vec* para um vetor *vech* e é usado na construção da matriz de variância covariância assintótica $\Sigma_{22}$.

### Importância das Derivadas Matriciais e Operadores
A combinação de derivadas matriciais e dos operadores 'vec' e 'vech' permite:
-   **Derivação Eficiente**: Permite derivar expressões para a distribuição assintótica de estimadores de forma mais concisa.
-   **Manipulação Simples**: Facilita a manipulação das matrizes e vetores envolvidos, reduzindo a complexidade computacional.
-   **Inclusão da Simetria**: Incorpora a simetria da matriz de covariância dos erros, o que é fundamental para a correta derivação dos erros padrão.
-  **Construção de Testes**: Permite criar testes estatísticos para hipóteses sobre os elementos de $\Omega$.
- **Implementação Computacional**: Simplifica a implementação computacional da estimação e inferência em modelos VAR, permitindo que cálculos sejam feitos de forma mais rápida, com um menor custo computacional e que seja possível realizar testes e análises em modelos mais complexos.

### Conclusão
Neste capítulo, exploramos a distribuição assintótica de $\Omega$ utilizando derivadas matriciais e os operadores 'vec' e 'vech', destacando sua importância para lidar com a simetria da matriz de covariância. A combinação dessas ferramentas proporciona uma maneira eficiente e precisa para obter erros padrão e realizar inferências estatísticas em modelos VAR. A utilização da matriz de duplicação e o produto de Kronecker são essenciais para expressar a variabilidade das estimativas de variâncias e covariâncias, permitindo uma melhor compreensão da estrutura de dependência dos erros. Compreender a derivação da distribuição assintótica, usando essas ferramentas, é fundamental para a modelagem de séries temporais multivariadas e a análise de dados empíricos com modelos VAR.
### Referências
[^3]:  *“The likelihood function is calculated in the same way as for a scalar auto-regression."* [11.1.3]
[^10]: *“The next task is to calculate the maximum likelihood estimate of 2. Here two results from matrix calculus will prove helpful..."* [11.1.44]
[^11]:  *“A proof of this proposition is provided in Appendix 11.A to this chapter."* [11.1.35]
[^4]: *“When evaluated at the MLE ÎÛ, the log likelihood [11.1.10] is..."* [11.1.25]
<!-- END -->
