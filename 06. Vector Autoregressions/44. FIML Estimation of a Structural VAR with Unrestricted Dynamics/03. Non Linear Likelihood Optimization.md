## Estimação FIML de VARs Estruturais: Derivação da Verossimilhança e Otimização

### Introdução

Este capítulo visa detalhar ainda mais a estimação de máxima verossimilhança de informação completa (FIML) em Modelos de Vetores Autorregressivos (VAR) estruturais com dinâmica irrestrita, com um foco específico na forma da função de verossimilhança e nos desafios relacionados à sua otimização, como a necessidade de um bom algoritmo e o cálculo de derivadas. Dando continuidade aos conceitos de [^SECTION_PLACEHOLDER], exploraremos como a complexidade da função de verossimilhança não linear e não convexa exige abordagens computacionais sofisticadas, especialmente o cálculo eficiente das derivadas da função objetivo.

### Derivação da Função de Verossimilhança e a Não-Convexidade

Na estimação de VARs estruturais, conforme discutido em capítulos anteriores, o objetivo é encontrar os parâmetros estruturais que melhor descrevem os dados observados. Isso é feito maximizando a função de verossimilhança, que mede a plausibilidade dos parâmetros dado os dados. No entanto, a forma da função de verossimilhança para modelos VAR estruturais com dinâmica irrestrita é não linear e não convexa.

Como vimos na seção anterior, partimos da função de log-verossimilhança [^11.6.29]:
$$ \mathcal{L}(B_0, D, \Pi) = -(Tn/2) \log(2\pi) - (T/2) \log |B_0^{-1} D (B_0^{-1})'| - (1/2) \sum_{t=1}^T [y_t - \Pi'x_t]' [B_0^{-1} D (B_0^{-1})']^{-1} [y_t - \Pi'x_t] $$
e, após manipulação e substituição das estimativas OLS para $ \Pi $, chegamos a
$$  \mathcal{L}(B_0, D, \hat{\Pi}) = -(Tn/2) \log(2\pi) + (T/2) \log |B_0^{-1}|^2 - (T/2) \log |D| - (T/2) \text{trace}\{(B_0D^{-1}B_0')^{-1} \hat{\Omega} \} $$

A não linearidade decorre da maneira como os parâmetros estruturais ($B_0$ e $D$) entram na função de verossimilhança. A não convexidade implica que a função de verossimilhança pode ter múltiplos máximos locais e um máximo global. Nesse cenário, algoritmos de otimização padrão podem ficar presos em um máximo local, e a garantia de que a solução encontrada é global se torna um desafio. Além disso, as derivadas dessa função apresentam termos não lineares, aumentando ainda mais a complexidade da sua otimização.

### A Necessidade de um Bom Algoritmo de Otimização
A forma complexa da função de verossimilhança reforça a importância do uso de um bom algoritmo de otimização. Algoritmos que se baseiam apenas no valor da função em um ponto específico podem falhar em encontrar a solução ótima em modelos com muitos parâmetros e não convexidade. Por isso, algoritmos de otimização que utilizam informações sobre o gradiente e a Hessiana da função objetivo, como o método de Newton-Raphson [^SECTION_PLACEHOLDER], se tornam importantes.

Como vimos em [^SECTION_PLACEHOLDER], o método de Newton-Raphson usa a seguinte regra iterativa:
$$ \theta^{(k+1)} = \theta^{(k)} - [H(\theta^{(k)})]^{-1} g(\theta^{(k)}) $$
onde $ \theta^{(k)} $ é o vetor de parâmetros na iteração k, $ g(\theta^{(k)}) $ é o gradiente, e $ H(\theta^{(k)}) $ é a matriz Hessiana da função objetivo. A eficiência desse método depende da precisão e da eficiência computacional do cálculo do gradiente e da Hessiana.

### Cálculo Eficiente das Derivadas da Função Objetivo
O cálculo das derivadas da função objetivo, em particular a matriz Hessiana, é um passo computacional caro na otimização da função de log-verossimilhança para VARs estruturais. Abordagens tradicionais que utilizam aproximações numéricas podem ser lentas e imprecisas. No entanto, o uso de técnicas de diferenciação automática oferece uma maneira mais eficiente e precisa de calcular essas derivadas.

**Diferenciação Automática:**
A diferenciação automática é um conjunto de técnicas para avaliar numericamente as derivadas de uma função definida por um programa de computador. Ao contrário da diferenciação numérica, que aproxima as derivadas, a diferenciação automática utiliza as regras da cadeia da derivada para calcular as derivadas de forma exata. Essa técnica se torna particularmente útil na otimização da função de log-verossimilhança do modelo estrutural VAR, dada a sua complexidade.
Ao usar diferenciação automática, o cálculo das derivadas analíticas da seção [^11.B] se torna automatizado computacionalmente, evitando erros de derivação e permitindo a construção de algoritmos mais eficientes.

**Implementação de Derivadas:**
Para implementar a diferenciação automática em um algoritmo de otimização, é necessário:
1.  **Definir a Função Objetivo:** Primeiro, é preciso definir formalmente a função de log-verossimilhança em termos dos parâmetros estruturais $B_0$ e $D$, como mostrado na seção anterior.
2. **Implementar um Sistema de Derivação Automática:** Em seguida, um sistema de diferenciação automática, como os disponíveis em linguagens de programação como Python (com bibliotecas como JAX ou TensorFlow) ou Julia (com Zygote), é utilizado para calcular as derivadas de primeira e segunda ordem da função objetivo.
3. **Usar um Otimizador:** Finalmente, um otimizador baseado em gradiente (como o método de Newton-Raphson ou seus variantes) utiliza essas derivadas para encontrar iterativamente as estimativas de máxima verossimilhança.

### Vantagens da Diferenciação Automática
*   **Precisão:** Ao contrário das aproximações numéricas, a diferenciação automática calcula as derivadas de forma exata (a menos de erros de ponto flutuante), o que pode ser especialmente importante para funções de log-verossimilhança complexas e sensíveis.
*   **Eficiência:** Os algoritmos de diferenciação automática, geralmente, são mais eficientes do que a derivação numérica, especialmente em modelos com muitos parâmetros.
*   **Automatização:** A diferenciação automática automatiza o processo de cálculo de derivadas, o que reduz a chance de erros e acelera o desenvolvimento e teste de novos modelos.

### Conclusão

A estimação FIML de VARs estruturais com dinâmica irrestrita apresenta desafios computacionais significativos devido à forma não linear e não convexa da função de verossimilhança. O uso de um bom algoritmo de otimização, como o método de Newton-Raphson ou suas variantes, e o cálculo eficiente das derivadas da função objetivo são cruciais para a obtenção de estimativas robustas e precisas. A diferenciação automática oferece uma abordagem eficiente e precisa para calcular as derivadas, automatizando o processo de otimização e tornando essa estimação mais viável.

### Referências

[^SECTION_PLACEHOLDER]:  Previous topic.
[^11.6.29]: Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press.
[^11.6.28]: Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press.
[^11.6.30]: Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press.
[^11.B]: Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press.
<!-- END -->
