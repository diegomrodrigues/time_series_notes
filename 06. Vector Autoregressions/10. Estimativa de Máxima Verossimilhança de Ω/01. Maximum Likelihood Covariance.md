## Estimativa de Máxima Verossimilhança da Matriz de Covariância $\Omega$

### Introdução
Este capítulo avança no tópico de *Vector Autoregressions (VAR)*, focando especificamente na obtenção da estimativa de máxima verossimilhança (MLE) para a matriz de covariância $\Omega$. A compreensão deste processo é crucial para a análise e inferência estatística em modelos VAR. Em continuidade aos conceitos estabelecidos anteriormente sobre a função de verossimilhança e estimativas de máxima verossimilhança[^1], vamos agora nos aprofundar na derivação da MLE para $\Omega$, utilizando os resultados de cálculo matricial apresentados.

### Conceitos Fundamentais
A função de verossimilhança, que maximizamos para obter os estimadores de máxima verossimilhança, é definida condicionalmente aos valores de *y* observados até o tempo *t-1* [^1]. A densidade condicional da *t-ésima* observação é dada por [^1]:
$$
f(y_t | y_{t-1}, y_{t-2}, \ldots, y_{t-p}; \theta) = (2\pi)^{-n/2} |\Omega|^{-1/2} \exp[(-1/2)(y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t)]
$$
onde *x* é um vetor que contém uma constante e *p* lags de cada elemento de *y*. A função de log-verossimilhança para uma amostra completa de tamanho *T* é então [^1]:
$$
L(\theta) = - (Tn/2)\log(2\pi) + (T/2)\log|\Omega^{-1}| - (1/2) \sum_{t=1}^{T} (y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t)
$$
Como vimos anteriormente, a MLE para $\Pi$, denotada por $\hat{\Pi}$, é obtida por regressões OLS das variáveis *y* em uma constante e nos *p* lags de todas as variáveis do sistema [^1]. O objetivo agora é encontrar a MLE para $\Omega$.

Para obter a MLE de $\Omega$, maximizamos a função de log-verossimilhança quando avaliada em $\hat{\Pi}$, a MLE de $\Pi$. Essa função é dada por [^1]:
$$
L(\Omega, \hat{\Pi}) = -(Tn/2) \log(2\pi) + (T/2) \log|\Omega^{-1}| - (1/2) \sum_{t=1}^T \hat{\epsilon}_t'\Omega^{-1}\hat{\epsilon}_t
$$
onde $\hat{\epsilon}_t = y_t - \hat{\Pi}'x_t$ representa os resíduos da regressão OLS.

Para encontrar o valor de $\Omega$ que maximiza essa função, necessitamos diferenciar *L* em relação a $\Omega^{-1}$ e igualar a zero. Utilizaremos os seguintes resultados de cálculo matricial [^1]:
1.  $\frac{\partial x'Ax}{\partial A} = xx'$
2.  $\frac{\partial \log|A|}{\partial A} = (A^{-1})'$

Aplicando esses resultados, temos:
$$
\frac{\partial L(\Omega, \hat{\Pi})}{\partial \Omega^{-1}} = (T/2)\Omega - (1/2)\sum_{t=1}^{T} \hat{\epsilon}_t \hat{\epsilon}_t'
$$
Igualando a derivada a zero, encontramos que a MLE para $\Omega$ é dada por [^1]:
$$
\hat{\Omega} = \frac{1}{T} \sum_{t=1}^{T} \hat{\epsilon}_t \hat{\epsilon}_t'
$$
Essa expressão indica que a MLE de $\Omega$ é a matriz de covariância amostral dos resíduos das regressões OLS, calculada como a média dos produtos externos dos resíduos. É importante ressaltar que, embora a derivação seja feita para uma matriz $\Omega$ irrestrita, o resultado para a MLE será sempre simétrico e positivo-definido, o que garante a validade da estimativa [^1].

### Conclusão
Neste capítulo, derivamos a MLE da matriz de covariância $\Omega$ em um modelo VAR, demonstrando que ela é simplesmente a matriz de covariância amostral dos resíduos das regressões OLS. Este resultado é fundamental para a realização de inferências e testes de hipóteses no contexto de modelos VAR. No próximo passo, iremos explorar como utilizar essas estimativas de máxima verossimilhança, especificamente na construção de testes de razão de verossimilhança e no tratamento de restrições nos modelos VAR [^1].

### Referências
[^1]: Contexto fornecido.
<!-- END -->
