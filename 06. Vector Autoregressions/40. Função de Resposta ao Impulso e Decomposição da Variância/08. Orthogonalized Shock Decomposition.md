## Ortogonalização de Choques em Modelos VAR: Uma Análise Detalhada para Interpretações Causais

### Introdução

Modelos de Vetores Auto-Regressivos (VAR), como visto anteriormente [^1], [^2], são ferramentas poderosas para analisar as interações dinâmicas entre múltiplas séries temporais. A análise da **Função de Resposta ao Impulso (IRF)** é fundamental nesse contexto, pois permite visualizar a resposta de cada variável a um choque específico ao longo do tempo. No entanto, a interpretação da IRF como uma relação *causal*, é complexa devido à possível correlação contemporânea entre os resíduos do modelo VAR. Como tal, a  **ortogonalização de choques** emerge como uma técnica essencial para permitir interpretações mais causais das IRFs. Este capítulo aprofundará a metodologia e as implicações da ortogonalização de choques em modelos VAR, focando na decomposição da matriz de covariância dos resíduos em matrizes triangulares, complementando as abordagens já apresentadas [^3], [^4].

### A Necessidade da Ortogonalização

Como discutido nos capítulos anteriores, um modelo VAR pode ser expresso em forma de médias móveis (MA) [^3], com o sistema sendo impulsionado por um conjunto de inovações (resíduos) $\epsilon_t$, tal que  $ y_t = \mu + \epsilon_t + \Psi_1 \epsilon_{t-1} + \Psi_2 \epsilon_{t-2} + \dots $. No entanto, essas inovações, que representam a parte não explicada das variáveis, podem ser contemporaneamente correlacionadas, o que torna difícil isolar o efeito de um choque específico em uma variável sobre as outras variáveis do sistema. Em particular, a matriz de covariância $\Omega = E(\epsilon_t\epsilon_t')$ que representa a variabilidade das inovações no tempo t, não é, em geral, uma matriz diagonal.

A ortogonalização dos choques visa transformar essas inovações correlacionadas $\epsilon_t$ em um conjunto de choques ortogonais $u_t$, que são não correlacionados entre si e com o histórico das variáveis, permitindo que a análise da IRF se torne mais interpretável [^5]. A ideia é que, usando os choques ortogonalizados, um choque na variável $i$ irá influenciar a variável $j$ *apenas* através da estrutura do modelo VAR, e não por meio de correlações contemporâneas com choques em outras variáveis.

### A Decomposição da Matriz de Covariância

O método mais comum para ortogonalizar choques em modelos VAR é através da decomposição de Cholesky. A decomposição de Cholesky é uma fatorização da matriz de covariância $\Omega$ em uma matriz triangular inferior $P$ (com entradas positivas na diagonal) e sua transposta tal que $\Omega = PP'$. Em [11.4.21], vimos que essa matriz pode ser escrita como $\Omega = AD^{1/2}D^{1/2}A' = PP'$ onde $D$ é uma matriz diagonal e $A$ é uma matriz triangular inferior com unidades na diagonal [^6]. A matriz  $P$ permite definir um vetor de choques ortogonalizados $u_t$ a partir das inovações $\epsilon_t$:

$$
u_t = P^{-1} \epsilon_t
$$

em que  $ E(u_t u_t') = I$. Com isso, a representação em médias móveis do modelo VAR passa a ser escrita como:
$$
y_t = \mu + A u_t + \Psi_1 A u_{t-1} + \Psi_2 A u_{t-2} + \ldots
$$
e a matriz da resposta ao impulso ortogonalizada é dada por $\Psi_s A$, em vez de $\Psi_s$.

**Procedimento da Decomposição de Cholesky**

O processo de decomposição de Cholesky envolve uma ordenação sequencial das variáveis, onde a primeira variável no ordenamento não é influenciada contemporaneamente pelas outras variáveis, a segunda variável é influenciada contemporaneamente pela primeira, mas não pelas demais, e assim sucessivamente [^7]. Ou seja, a decomposição de Cholesky assume uma estrutura hierárquica na qual a variável em uma determinada posição no vetor pode ser afetada por variáveis anteriores, mas não pelas variáveis em posições subsequentes, ou seja, não existe feedback contemporâneo.

A matriz de decomposição triangular inferior $P$, onde  $\Omega = PP'$,  pode ser calculada usando o algoritmo de Cholesky, que é baseado em operações simples sobre a matriz $\Omega$.  O elemento diagonal de $P$ na linha $i$ representa o desvio padrão do choque ortogonalizado da variável $i$, e os elementos fora da diagonal representam o impacto dos choques passados sobre as variáveis. Conforme expresso em [11.4.21], definindo  $D^{1/2}$ como a matriz diagonal cujos elementos são a raíz quadrada dos elementos de  $D$, podemos usar  $P=A D^{1/2}$ para reescrever o modelo com choques ortogonalizados, onde  $E(u_t u_t')=I$.

### Interpretação da Resposta ao Impulso Ortogonalizada

Com a matriz de choques ortogonalizados $u_t$, é possível analisar a função de resposta ao impulso (IRF) ortogonalizada, que agora reflete as respostas das variáveis do modelo VAR a choques independentes. Em outras palavras, a resposta de uma variável a um choque em outra variável é interpretada como um efeito "puro", descontado da possível correlação contemporânea entre as inovações [^5]. A resposta da variável $i$ a um choque ortogonalizado na variável $j$ no horizonte $s$ será dada por $\Psi_s A$.
A forma de calcular as respostas aos choques ortogonalizados, seguindo [11.4.22], será de:

$$ \frac{\partial y_{t+s}}{\partial v_{t}} = \Psi_s P $$

onde $v_t = D^{-1/2} u_t $ é um choque normalizado.

A IRF ortogonalizada permite analisar o impacto de cada choque em cada variável e, ao contrário da IRF tradicional (não ortogonalizada), ela busca identificar mecanismos causais subjacentes aos dados, e não apenas padrões de previsão. No entanto, esta interpretação causal deve ser utilizada com cautela, pois ela depende das escolhas de identificação feitas para a ortogonalização.

### Limitações e Alternativas da Ortogonalização

A decomposição de Cholesky, apesar de sua popularidade, possui limitações importantes, como discutido em [^7]. A principal delas é a dependência da ordenação das variáveis. Como a decomposição de Cholesky impõe uma estrutura hierárquica, a interpretação causal das IRFs ortogonalizadas depende da validade dessa ordenação. Uma ordenação diferente das variáveis pode levar a conclusões diferentes sobre a transmissão de choques. A validade de uma ordenação particular deve ser justificada em termos de conhecimento teórico ou empírico.

Existem alternativas à decomposição de Cholesky, como a utilização de restrições de curto e longo prazo para obter uma matriz de decomposição de $\Omega$ que seja triangular, mas com as condições de ortogonalização sobre uma estrutura causal pre-estabelecida. Alternativamente, é possível, utilizar técnicas de identificação baseadas em modelos estruturais, conforme explorado em [11.6.32], onde as restrições são impostas sobre o comportamento contemporâneo das variáveis e que podem ser utilizadas para obter uma matriz que possibilite interpretar o significado de choques ortogonalizados específicos [^9]. Estas técnicas oferecem uma maior flexibilidade para explorar diferentes hipóteses sobre a estrutura do modelo.

### Implicações Teóricas e Práticas

A ortogonalização de choques é uma etapa crucial para análises que visam obter interpretações causais em modelos VAR. É através desta metodologia que os choques podem ser relacionados a eventos específicos ou alterações na economia, permitindo que as IRFs possam ser interpretadas como mecanismos de transmissão de choques. No entanto, as limitações, especialmente a dependência da ordenação, devem ser consideradas com cautela, e o uso de métodos de identificação estrutural devem ser considerados.

Na prática, a escolha do método de ortogonalização (de Cholesky ou outras técnicas) deve ser baseada nos objetivos da análise e nas premissas teóricas sobre a relação entre as variáveis. A robustez dos resultados deve ser avaliada através da comparação com diferentes ordenações e/ou diferentes técnicas de identificação.

### Conclusão

A ortogonalização de choques é essencial para a obtenção de IRFs com interpretações causais em modelos VAR. A decomposição da matriz de covariância, especialmente através do método de Cholesky, transforma os resíduos do modelo em um conjunto de choques não correlacionados, que permitem a avaliação de impactos "puros" de uma variável sobre as outras no sistema. A escolha da ordem da decomposição de Cholesky, assim como outras abordagens de identificação, pode impactar as conclusões, e o uso de métodos alternativos de ortogonalização e restrições estruturais podem ser necessárias. O conhecimento sobre os métodos de ortogonalização e suas limitações é essencial para o uso adequado da análise da IRF em modelos VAR e para uma melhor compreensão das interações dinâmicas entre múltiplas séries temporais.

### Referências

[^1]: *A discussão de modelos VAR como uma abordagem para analisar relações dinâmicas entre múltiplas variáveis, e a sua utilidade sem impor fortes restrições teóricas.*
[^2]: *Os modelos VAR são úteis para analisar relações dinâmicas entre múltiplas variáveis sem impor fortes restrições teóricas sobre suas relações.*
[^3]: *Um modelo VAR pode ser expressa em forma de média móvel (MA).*
[^4]: *A matriz  $\Psi_s$ quantifica como um choque de uma unidade na variável j no tempo t afeta a variável i no tempo t+s, mantendo todas as outras inovações constantes.*
[^5]: *A ortogonalização dos choques visa transformar essas inovações correlacionadas em um conjunto de choques ortogonais.*
[^6]: *A decomposição de Cholesky para ortogonalizar IRFs. *
[^7]: *A ordenação das variáveis na decomposição de Cholesky impacta o resultado.*
[^8]: *O erro de previsão em s períodos e a análise da variância do erro de previsão no horizonte s.*
[^9]: *Técnicas de identificação baseadas em modelos estruturais.*
## 11.8. Previsão com Modelos VAR
### Introdução
Expandindo os conceitos de representação, estimação e análise de respostas de impulso discutidos anteriormente, focaremos na utilização de modelos **VAR** para previsão. O modelo VAR, em sua essência, é uma ferramenta valiosa para prever o comportamento futuro de um conjunto de variáveis inter-relacionadas. Como vimos, um modelo VAR captura as relações dinâmicas entre as variáveis, permitindo-nos fazer previsões sobre seus valores futuros, condicionadas a seus valores passados [^1].

### Construindo Previsões
A previsão com um modelo VAR envolve o uso de valores passados das variáveis para estimar seus valores futuros. Especificamente, dado um modelo VAR da forma [^1]:

$$ y_t = c + \Phi_1y_{t-1} + \Phi_2y_{t-2} + \ldots + \Phi_py_{t-p} + \epsilon_t $$,

onde $y_t$ é um vetor de variáveis no tempo t, $c$ é um vetor de interceptos, $\Phi_i$ são matrizes de coeficientes e $\epsilon_t$ é um vetor de erros, uma previsão de um passo à frente, $\hat{y}_{t+1|t}$, pode ser construída usando os valores observados até o tempo *t* como:

$$\hat{y}_{t+1|t} = \hat{c} + \hat{\Phi}_1y_t + \hat{\Phi}_2y_{t-1} + \ldots + \hat{\Phi}_py_{t-p+1} $$,

onde $\hat{c}$ e $\hat{\Phi}_i$ são as estimativas obtidas por OLS [^1].

Para previsões de múltiplos passos à frente, podemos usar a previsão anterior como um valor passado para gerar a próxima previsão e assim por diante. Por exemplo, uma previsão de dois passos à frente, $\hat{y}_{t+2|t}$, seria construída como:

$$ \hat{y}_{t+2|t} = \hat{c} + \hat{\Phi}_1\hat{y}_{t+1|t} + \hat{\Phi}_2y_t + \ldots + \hat{\Phi}_py_{t-p+2} $$.

Este processo de utilizar previsões anteriores para gerar previsões futuras é chamado de previsão iterativa ou recursiva. A precisão das previsões de modelos VAR depende fortemente da qualidade dos dados e da escolha do número apropriado de lags [^1, 4, 14].

### Erros de Previsão
Os erros de previsão em modelos VAR são inerentemente relacionados aos choques inovativos $\epsilon_t$. Como a previsão de um passo à frente é construída com base em informações disponíveis no tempo *t*, o erro de previsão desse passo à frente é exatamente a inovação no tempo $t+1$, ou seja, $\epsilon_{t+1}$. Para previsões de múltiplos passos à frente, os erros de previsão tornam-se mais complicados, pois incluem todos os choques inovativos acumulados no horizonte de previsão. Por exemplo, o erro de previsão de dois passos à frente seria:
$$ y_{t+2} - \hat{y}_{t+2|t} = \epsilon_{t+2} + \Phi_1\epsilon_{t+1} $$.
A análise desses erros de previsão nos permite avaliar a incerteza associada às previsões e entender o impacto dos choques inovativos no sistema [^8].

### Avaliando a Precisão da Previsão
A precisão das previsões de um modelo VAR pode ser avaliada utilizando várias métricas. Algumas das métricas mais comuns incluem a Raiz do Erro Quadrático Médio (RMSE), o Erro Absoluto Médio (MAE) e o Erro Percentual Absoluto Médio (MAPE). Essas métricas fornecem uma medida da precisão das previsões em diferentes escalas. Adicionalmente, a análise da decomposição da variância do erro de previsão, que é discutida na seção [11.5] [^8], pode fornecer insights sobre a importância relativa de diferentes choques inovativos na geração das variações de previsão [^8, 9].

### Considerações Práticas
É importante notar que os modelos VAR são apenas aproximações da realidade, e a precisão da previsão é limitada pela qualidade dos dados, a especificação do modelo e a natureza do sistema que está sendo modelado. Ao usar modelos VAR para previsão, devemos estar cientes de que as previsões são mais precisas para horizontes de curto prazo e tornam-se menos precisas à medida que o horizonte de previsão aumenta. Além disso, é crucial realizar uma avaliação minuciosa do ajuste do modelo e verificar a robustez das previsões através de diferentes conjuntos de dados e especificaçõs do modelo [^14].
Por exemplo, em aplicações econômicas, pode-se usar modelos VAR para prever variáveis como Produto Interno Bruto (PIB), taxas de inflação e desemprego, com base em seus valores históricos e nas interrelações entre elas. Em finanças, os modelos VAR podem ser usados para prever o comportamento de ações, taxas de câmbio e outras variáveis financeiras [^1].

### Conclusão
Este capítulo forneceu uma exploração abrangente dos modelos VAR, incluindo sua construção, estimação, interpretação e uso para previsão. Vimos que os modelos VAR são uma ferramenta poderosa para a análise de séries temporais multivariadas, permitindo capturar as complexas relações dinâmicas entre variáveis. Embora os modelos VAR tenham suas limitações, eles continuam sendo uma ferramenta valiosa para a análise e previsão em diversas áreas de estudo [^1].

### Referências
[^1]: Veja a Seção 11.1 para definições e informações sobre modelos VAR.
[^4]: Veja a Seção 11.2 para as implicações alternativas da causalidade de Granger.
[^8]: Veja a Seção 11.5 para uma discussão sobre a decomposição da variância do erro de previsão.
[^9]: Veja a Seção 11.6 para mais informações sobre modelos estruturais VAR.
[^14]: Veja a Seção 11.7 para uma discussão sobre erros padrão.
<!-- END -->
