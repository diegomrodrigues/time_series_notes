## Matriz de Covariância Assintótica $\Sigma_{22}$ e a Matriz de Duplicação em Modelos VAR
### Introdução
Este capítulo continua a exploração da distribuição assintótica do estimador da matriz de covariância $\Omega$ em modelos Vetoriais Autorregressivos (VAR), com foco específico na matriz $\Sigma_{22}$, que descreve as covariâncias assintóticas entre os estimadores das variâncias e covariâncias de $\Omega$. Expandindo as discussões anteriores, iremos introduzir o conceito da matriz de duplicação e demonstrar como ela pode ser utilizada para expressar $\Sigma_{22}$ de forma mais concisa, facilitando inferências estatísticas e cálculos [^3].

### Conceitos Fundamentais
Na análise de modelos VAR, a matriz de covariância dos resíduos, $\Omega$, é um elemento crucial que descreve a dispersão e as correlações entre as inovações do sistema [^1]. A estimação de $\Omega$ é realizada via regressões OLS, resultando em um estimador $\hat{\Omega}$.  Como discutido anteriormente, a distribuição assintótica de $\hat{\Omega}$ é necessária para realizar inferências estatísticas. O estudo da matriz $\Sigma_{22}$, que representa a matriz de covariância assintótica de $vech(\hat{\Omega})$, torna-se fundamental para entendermos como os estimadores das variâncias e covariâncias em $\Omega$ se relacionam probabilisticamente [^3].

A matriz $\Sigma_{22}$, expressa em notação mais detalhada como:
$$
(\sigma_{il}\sigma_{jm} + \sigma_{im}\sigma_{jl}),
$$
onde $\sigma_{ij}$ é um elemento de $\Omega$, captura as covariâncias assintóticas entre os estimadores de variância e covariância dos resíduos do modelo VAR [^3].

Entretanto, uma representação mais compacta de $\Sigma_{22}$ pode ser obtida através da matriz de duplicação, que facilita os cálculos e a análise da estrutura de covariância. A matriz de duplicação, denotada por $D_n$, é uma matriz que transforma o vetor vech(Ω) no vetor vec(Ω), ou seja, $vec(\Omega) = D_n vech(\Omega)$ [^3]. Essa transformação é útil porque o operador vec empilha as colunas da matriz, enquanto o operador vech empilha apenas os elementos da diagonal e abaixo dela, explorando a simetria da matriz $\Omega$.

O pseudoinverso da matriz de duplicação, denotado por $D_n^+$, é definido de forma que $D_n^+ D_n = I$. Usando essas definições, podemos reescrever a matriz $\Sigma_{22}$ de uma maneira mais concisa e elegante:
$$ \Sigma_{22} = 2D_n^+ (\Omega \otimes \Omega) (D_n^+)' $$
onde $\otimes$ denota o produto de Kronecker, e $D_n^+$ o pseudoinverso da matriz de duplicação [^3].

A vantagem dessa representação é que ela expressa $\Sigma_{22}$ em termos de operações matriciais fundamentais, facilitando a implementação computacional e a análise teórica. A matriz de duplicação e seu pseudoinverso capturam a redundância presente na estrutura simétrica de $\Omega$, permitindo expressar a matriz de covariância de $vech(\hat{\Omega})$ de forma concisa e eficiente.

Para exemplificar a construção de $D_n$, considere o caso onde n=2. Então, a matriz de duplicação é dada por:
$$
D_2 = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}
$$
de forma que, se $vec(\Omega) = [\sigma_{11}, \sigma_{21}, \sigma_{12}, \sigma_{22}]'$
e $vech(\Omega) = [\sigma_{11}, \sigma_{21}, \sigma_{22}]'$, temos que
$vec(\Omega) = D_2 vech(\Omega)$ [^3].

O pseudoinverso $D_n^+$ satisfaz $D_n^+ D_n = I$, de modo que $vech(\Omega) = D_n^+ vec(\Omega)$.
Para o caso n=2, o pseudoinverso $D_2^+$ é dado por:
$$
D_2^+ = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1/2 & 0 \\ 0 & 1/2 & 0 \\ 0 & 0 & 1 \end{bmatrix}
$$
Observamos que esta matriz satisfaz a condição $D_2^+ D_2 = I$ [^3].
### Implicações para Inferência Estatística
A representação de $\Sigma_{22}$ usando a matriz de duplicação tem implicações importantes para a inferência estatística em modelos VAR. Ao expressar a matriz de covariância assintótica de forma mais compacta, torna-se mais fácil realizar cálculos e construir testes de hipóteses envolvendo os elementos de $\Omega$. Por exemplo, ao testar se certas covariâncias em $\Omega$ são zero, o uso da matriz de duplicação facilita a derivação da distribuição assintótica dos estatísticos de teste.

Além disso, a representação de $\Sigma_{22}$ em termos de matrizes fundamentais como $\Omega$ e a matriz de duplicação possibilita a análise teórica e a derivação de propriedades assintóticas dos estimadores de forma mais elegante e eficiente.

### Conclusão
A matriz $\Sigma_{22}$, que captura as relações entre os estimadores das variâncias e covariâncias do VAR, desempenha um papel crucial na inferência estatística. A utilização da matriz de duplicação oferece uma forma concisa e computacionalmente eficiente de expressar $\Sigma_{22}$, o que facilita a análise teórica e prática dos modelos VAR [^3]. Este resultado é fundamental para a compreensão da distribuição assintótica dos estimadores de $\Omega$ e para a construção de testes de hipóteses e intervalos de confiança, que são passos essenciais na modelagem de séries temporais multivariadas.
### Referências
[^1]: Texto referente à seção anterior do capítulo, que discute a estimação e teste de hipóteses em VAR.
[^2]: Texto extraído da página 300, onde é introduzido o operador "vec"
[^3]: Texto extraído das páginas 301-302, onde a distribuição assintótica de $\Omega$ e os operadores "vec" e "vech" são abordados em detalhes e introduzidos para análise.
### 11.7.1. Abordagens Alternativas para Cálculo de Erros Padrão para a Função de Resposta ao Impulso Não Ortogonalizada

A matriz de derivadas $G_s$ pode ser calculada alternativamente numericamente, como segue. Primeiro, usamos as estimativas OLS $\hat{\pi}$ para calcular $\hat{\Psi}_s(\hat{\pi})$ para $s = 1, 2, ..., m$. Em seguida, aumentamos o valor do i-ésimo elemento de $\pi$ por uma pequena quantidade $\Delta$, mantendo todos os outros elementos constantes e avaliamos $\hat{\Psi}_s(\hat{\pi} + e_i\Delta)$ para $s = 1, 2, ..., m$, onde $e_i$ denota a i-ésima coluna de $I_{nk}$. Então, o vetor $(n^2 \times 1)$
$$ \frac{\hat{\Psi}_s(\hat{\pi} + e_i\Delta) - \hat{\Psi}_s(\hat{\pi})}{\Delta} $$
fornece uma estimativa da i-ésima coluna de $G_s$. Ao realizar avaliações separadas da sequência $\hat{\Psi}_s(\hat{\pi} + e_i\Delta)$ para cada $i = 1, 2, ..., nk$, todas as colunas de $G_s$ podem ser preenchidas.

Métodos de Monte Carlo também podem ser usados para inferir a distribuição de $\hat{\Psi}_s(\hat{\pi})$. Aqui, geraríamos aleatoriamente um vetor $(nk \times 1)$ extraído de uma distribuição $N(\hat{\pi}, (1/T)(\hat{\Omega}\otimes Q^{-1}))$. Denotamos este vetor por $\pi^{(1)}$, e calculamos $\hat{\Psi}_s(\pi^{(1)})$. Desenhe um segundo vetor $\pi^{(2)}$ da mesma distribuição e calcule $\hat{\Psi}_s(\pi^{(2)})$. Repita isso por, digamos, 10.000 simulações separadas. Se 9500 dessas simulações resultarem em um valor para o primeiro elemento de $\hat{\Psi}_s$ que está entre $\psi_{s,l}$ e $\psi_{s,u}$, então $[\psi_{s,l}, \psi_{s,u}]$ pode ser usado como um intervalo de confiança de 95% para o primeiro elemento de $\hat{\Psi}_s$.

Runkle (1987) empregou uma abordagem relacionada baseada em *bootstrapping*. A ideia por trás do *bootstrapping* é obter uma estimativa da distribuição da pequena amostra de $\hat{\pi}$ sem assumir que as inovações $\epsilon_t$ sejam Gaussianas. Para implementar este procedimento, primeiro estima-se o VAR e salvam-se as estimativas dos coeficientes $\hat{\Pi}$ e os resíduos ajustados $\{\hat{\epsilon}_1, \hat{\epsilon}_2, ..., \hat{\epsilon}_T\}$. Em seguida, considere uma variável aleatória artificial $u_t$ que tem probabilidade $(1/T)$ de assumir cada um dos valores particulares $\{\hat{\epsilon}_1, \hat{\epsilon}_2, ..., \hat{\epsilon}_T\}$. A esperança é que a distribuição de $u_t$ seja semelhante à distribuição da verdadeira população $\epsilon_t$'s. Então, faz-se um sorteio aleatório dessa distribuição (denotado $u_t^{(1)}$), e usa-se isso para construir a primeira inovação em uma amostra artificial; isto é, estabeleça
$$ y_1^{(1)} = \hat{c} + \hat{\Phi}_1 y_0 + \hat{\Phi}_2 y_{-1} + \ldots + \hat{\Phi}_p y_{-p+1} + u_t^{(1)} $$
onde $y_0, y_{-1}, ..., y_{-p+1}$ denotam os valores de pré-amostra de $y$ que foram realmente observados nos dados históricos. Fazendo um segundo sorteio $u_t^{(2)}$, gere
$$ y_2^{(1)} = \hat{c} + \hat{\Phi}_1 y_1^{(1)} + \hat{\Phi}_2 y_{0} + \ldots + \hat{\Phi}_p y_{-p+2} + u_t^{(2)} $$
Observe que esse segundo sorteio é com reposição; isto é, há uma chance de $(1/T)$ de que $u_t^{(2)}$ seja exatamente o mesmo que $u_t^{(1)}$. Prosseguindo desta maneira, uma amostra completa $\{y_1^{(1)}, y_2^{(1)}, ..., y_T^{(1)}\}$ pode ser gerada. Um VAR pode ser ajustado por OLS a esses dados simulados (novamente pegando os valores de pré-amostra de $y$ como seus valores históricos), produzindo uma estimativa $\hat{\Psi}_s^{(1)}$. A partir desta estimativa, a magnitude $\hat{\Psi}_s^{(1)}$ pode ser calculada. Em seguida, gere um segundo conjunto de $T$ sorteios da distribuição de $u_t$, denotado por $\{u_1^{(2)}, u_2^{(2)}, ..., u_T^{(2)}\}$, ajuste $\hat{\pi}^{(2)}$ a esses dados por OLS, e calcule $\hat{\Psi}_s^{(2)}$. Uma série de 10.000 dessas simulações podem ser realizadas, e um intervalo de confiança de 95% para $\hat{\Psi}_s$ pode então ser inferido a partir do intervalo que inclui 95% dos valores para $\hat{\Psi}_s^{(i)}$.

### 11.7.2. Erros Padrão para Parâmetros de um VAR Estrutural
Recapitulando da Proposição 11.2 e equação [11.1.48] que se as inovações são gaussianas,
$$ \sqrt{T}[vech(\hat{\Omega}) - vech(\Omega)] \xrightarrow{d} N(0, 2D_n^+ (\Omega \otimes \Omega) (D_n^+)'). $$
As estimativas dos parâmetros de um VAR estrutural ($\hat{B_0}$ e $\hat{D}$) são determinadas como funções implícitas de $\hat{\Omega}$ de
$$ \hat{\Omega} = \hat{B_0} \hat{D} (\hat{B_0}')'.$$
Como na equação [11.6.34], os elementos desconhecidos de $B_0$ são resumidos por um vetor ($\mathit{n}_B \times 1$) $\theta_B$ com $vec(B_0) = S_B\theta_B + s_B$. Similarmente, como em [11.6.35], assume-se que $vec(D) = S_D\theta_D + s_D$ para $\theta_D$ um vetor ($\mathit{n}_D \times 1$). Segue da Proposição 7.4 que
$$ \sqrt{T}(\hat{\theta}_{B,T} - \theta_B) \xrightarrow{d} N(0, 2 G_B D^+ (\Omega \otimes \Omega) (D^+) G_B'), $$
$$ \sqrt{T}(\hat{\theta}_{D,T} - \theta_D) \xrightarrow{d} N(0, 2 G_D D^+ (\Omega \otimes \Omega) (D^+) G_D'), $$
onde
$$ G_B = \frac{\partial \theta_B}{\partial[vech(\Omega)]'} \quad (n_B \times n^*), $$
$$ G_D = \frac{\partial \theta_D}{\partial[vech(\Omega)]'} \quad (n_D \times n^*) $$
e $n^* = n(n + 1)/2$.
A equação [11.6.38] deu uma expressão para a matriz $[n^* \times (\mathit{n}_B + \mathit{n}_D)]$
$$ J = \left[ \frac{\partial vech(\Omega)}{\partial \theta_B'}, \frac{\partial vech(\Omega)}{\partial \theta_D'} \right].$$
Notamos ali que se o modelo é para ser identificado, as colunas desta matriz devem ser linearmente independentes. No caso apenas-identificado, $n^* = (n_B + n_D)$ e $J^{-1}$ existe.

### 11.7.3. Erros Padrão para Funções de Resposta ao Impulso Ortogonalizadas

A Seção 11.6 descreveu o cálculo da seguinte matriz $(n \times n)$:
$$ H_s = \Psi_s B_0'.$$
O elemento da linha $i$, coluna $j$ desta matriz mede o efeito da $j$-ésima perturbação estrutural ($u_{jt}$) na $i$-ésima variável no sistema $(y_{i,t+s})$ após um atraso de $s$ períodos. Colete essas magnitudes em um vetor $(n^2 \times 1)$ $h_s = vec(H_s)$. Assim, os primeiros $n$ elementos de $h_s$ dão o efeito de $u_{1t}$ em $y_{1,t+s}$, os próximos $n$ elementos dão o efeito de $u_{2t}$ em $y_{2,t+s}$, e assim por diante.
Como $\Psi_s$ é uma função de $\pi$ e como $B_0$ é uma função de $vech(\Omega)$, as distribuições de ambos os coeficientes auto-regressivos e das variâncias afetam a distribuição assintótica de $h_s$. Segue da Proposição 11.2 que, com inovações gaussianas,
$$ \sqrt{T}(h_{s,T} - h_s) \xrightarrow{d} N(0, [\Xi_{\pi}'\ \Xi_{\Omega}']
\begin{bmatrix}
\Omega \otimes Q^{-1} & 0 \\ 0 & 2D_n^+ (\Omega \otimes \Omega) (D_n^+)'
\end{bmatrix}
\begin{bmatrix}
\Xi_{\pi} \\ \Xi_{\Omega}
\end{bmatrix}),
$$
onde o Apêndice 11.B demonstra que
$$ \Xi_{\pi} = \frac{\partial h_s}{\partial \pi'} = [I_n \otimes (B_0')']G_s, $$
$$ \Xi_{\Omega} = \frac{\partial h_s}{\partial [vech(\Omega)]'} = - [H_s' \otimes (B_0^{-1})'] D_n S_{B_0} G_D, $$
Aqui, $G_s$ é a matriz dada em [11.7.5], $G$ é a matriz dada em [11.7.11], e $S_{B_0}$ é uma matriz $(n^2 \times n_B)$ que pega os elementos de $\theta_B$ e os coloca na posição correspondente para construir $vec(B_0)$:
$$ vec(B_0) = S_B \theta_B + s_B. $$
Para o exemplo de oferta e demanda em [11.6.24] até [11.6.26],
$$ S_{B_0} = 
\begin{bmatrix}
-1 & 0 & 0 \\
0 & -1 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & -1 \\
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & -1 \\
0 & 0 & 0
\end{bmatrix}.
$$

### 11.7.4. Experiência Prática com Erros Padrão

Na prática, os erros padrão para inferências dinâmicas baseadas em VARs geralmente acabam sendo desapontadoramente grandes (veja Runkle, 1987, e Lütkepohl, 1990).
<!-- END -->
