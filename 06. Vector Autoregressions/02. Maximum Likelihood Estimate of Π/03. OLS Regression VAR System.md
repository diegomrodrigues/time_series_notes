## Estimativa de Máxima Verossimilhança de Π via Regressão OLS em VARs
### Introdução
Este capítulo aprofunda a análise da estimativa de máxima verossimilhança (MLE) da matriz Π em modelos de Vetores Auto-regressivos (VAR), com um foco particular na equivalência entre a MLE e a regressão de mínimos quadrados ordinários (OLS). Como vimos anteriormente, a matriz Π contém os termos constantes e coeficientes auto-regressivos, desempenhando um papel central na modelagem das relações dinâmicas entre séries temporais multivariadas [^1].  Este tópico visa clarificar a derivação da MLE e sua conexão com a regressão OLS.

### Conceitos Fundamentais
Como já estabelecido, a função de verossimilhança condicional para um modelo VAR pode ser escrita como [^1]:
$$
f(y_t | y_{t-1}, y_{t-2}, \ldots, y_{t-p}; \theta) \sim N(\Pi'x_t, \Omega),
$$
onde $x_t$ é um vetor que contém um termo constante e as defasagens de $y$, e Π' é a matriz que contém os parâmetros a serem estimados. A função de log-verossimilhança para a amostra completa é, então, [^1]:
$$
\mathcal{L}(\theta) = - \frac{Tn}{2} \log(2\pi) + \frac{T}{2} \log|\Omega^{-1}| - \frac{1}{2} \sum_{t=1}^T (y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t)
$$
A estimativa de máxima verossimilhança de Π, denotada por $\hat{\Pi}'$, é obtida maximizando a função de log-verossimilhança ou, equivalentemente, minimizando o termo quadrático da soma [^1]:
$$
\sum_{t=1}^T (y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t)
$$
Anteriormente, foi demonstrado que a solução para $\hat{\Pi}'$ é dada por [^1]:
$$
\hat{\Pi}' = \left[ \sum_{t=1}^T y_t x_t' \right] \left[ \sum_{t=1}^T x_t x_t' \right]^{-1}
$$
Este resultado, como notamos, corresponde à projeção linear amostral de $y_t$ em uma constante e em $x_t$.  Agora, vamos detalhar o argumento que mostra que a MLE de Π corresponde a executar regressões OLS separadas para cada equação do sistema VAR [^1].

### Equivalência entre MLE e Regressão OLS
Para mostrar a equivalência entre a MLE de Π e regressões OLS, podemos manipular a soma na função de log-verossimilhança.  Primeiramente, reescrevemos o termo dentro da soma, substituindo a matriz de coeficientes populacional Π pelo seu estimador $\hat{\Pi}$  e definindo $\hat{\epsilon}_t$ como os resíduos da regressão OLS de $y_t$ em $x_t$:
$$
\sum_{t=1}^T (y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t) = \sum_{t=1}^T (\hat{\epsilon}_t + (\hat{\Pi}' - \Pi')x_t)'\Omega^{-1}(\hat{\epsilon}_t + (\hat{\Pi}' - \Pi')x_t)
$$
Expandindo esta expressão, obtemos [^1]:
$$
\sum_{t=1}^T  (\hat{\epsilon}_t'\Omega^{-1}\hat{\epsilon}_t + \hat{\epsilon}_t'\Omega^{-1}(\hat{\Pi}' - \Pi')x_t + x_t'(\hat{\Pi}' - \Pi')'\Omega^{-1}\hat{\epsilon}_t + x_t'(\hat{\Pi}' - \Pi')'\Omega^{-1}(\hat{\Pi}' - \Pi')x_t)
$$
A primeira parte dessa soma é um termo constante que depende dos resíduos OLS. O segundo e terceiro termos são nulos, pois os resíduos OLS são ortogonais aos regressores, o que significa que $\sum_{t=1}^T \hat{\epsilon}_t x_t' = 0$ [^1]. Resta, então, o último termo, que é quadrático em relação a $(\hat{\Pi}' - \Pi')$.  Assim, a função de log-verossimilhança é equivalente a minimizar o seguinte termo [^1]:
$$
\sum_{t=1}^T \hat{\epsilon}_t'\Omega^{-1}\hat{\epsilon}_t  + \sum_{t=1}^T x_t'(\hat{\Pi}' - \Pi')'\Omega^{-1}(\hat{\Pi}' - \Pi')x_t
$$
A minimização deste termo com respeito a $\Pi'$ leva à mesma solução que a regressão OLS. Como $\Omega^{-1}$ é uma matriz definida positiva, o último termo é minimizado se $\hat{\Pi}'$ corresponde aos estimadores OLS para cada equação do sistema VAR [^1].  Isto ocorre porque cada linha $j$ da matriz $\hat{\Pi}'$ é obtida através da minimização do termo quadrático da função de log-verossimilhança, o que equivale a uma regressão OLS de $y_{jt}$ em $x_t$.

Este resultado crucial demonstra que a estimativa de máxima verossimilhança da matriz Π em um sistema VAR é computacionalmente direta, pois pode ser obtida através de regressões OLS para cada equação do sistema [^1].  Em outras palavras, o problema de otimização para maximizar a verossimilhança se desdobra em $n$ problemas de otimização separados, cada um correspondendo a uma regressão OLS. Isso torna a estimativa de modelos VAR muito mais tratável do ponto de vista computacional [^1].

Cada linha de $\hat{\Pi}'$ representa o vetor de coeficientes que minimizam a soma dos quadrados dos resíduos correspondentes para uma determinada equação no VAR, que é precisamente o que uma regressão OLS faz [^1]. Isso estabelece uma forte ligação entre a teoria da máxima verossimilhança e as técnicas de regressão padrão, mostrando que a MLE pode ser alcançada através de procedimentos de regressão bem estabelecidos.

### Conclusão
Em resumo, este tópico estabeleceu formalmente que a estimativa de máxima verossimilhança (MLE) da matriz Π em modelos VAR é equivalente a realizar regressões OLS separadas para cada equação no sistema. A manipulação da função de log-verossimilhança mostrou que a solução que maximiza a verossimilhança coincide com a solução que minimiza os resíduos de regressão OLS para cada variável dependente em relação a uma constante e aos lags das variáveis do sistema. Este resultado não só simplifica a estimativa dos modelos VAR, mas também fornece uma intuição sobre a natureza da solução da MLE, que se alinha com a projeção linear da população.  Esta abordagem computacionalmente eficiente permite uma análise robusta e prática de modelos VAR em aplicações de séries temporais multivariadas [^1].

### Referências
[^1]: Texto fornecido para análise
<!-- END -->
