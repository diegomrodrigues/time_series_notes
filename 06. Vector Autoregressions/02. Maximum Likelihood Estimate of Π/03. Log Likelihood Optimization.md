## O Processo de Otimização da Função de Log-Verossimilhança para a MLE de Π em VARs
### Introdução
Este capítulo dedica-se à exploração detalhada do processo de otimização da função de log-verossimilhança para a obtenção da estimativa de máxima verossimilhança (MLE) da matriz Π em modelos Vetores Auto-regressivos (VAR). Como vimos nos tópicos anteriores, a matriz Π contém os coeficientes auto-regressivos e o termo constante do modelo, e sua estimativa precisa é fundamental para a análise das relações dinâmicas entre séries temporais multivariadas [^1]. Este capítulo irá dissecar o processo de otimização, demonstrando como manipulações algébricas e propriedades de matrizes levam à conclusão de que a MLE de Π corresponde à realização de regressões de Mínimos Quadrados Ordinários (OLS).

### Desenvolvimento da Otimização da Log-Verossimilhança
Como já discutido, a função de log-verossimilhança condicional para um modelo VAR é dada por [^1]:
$$
\mathcal{L}(\theta) = - \frac{Tn}{2} \log(2\pi) + \frac{T}{2} \log|\Omega^{-1}| - \frac{1}{2} \sum_{t=1}^T (y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t)
$$
onde $y_t$ é o vetor de variáveis no tempo $t$, $x_t$ é o vetor de regressores incluindo uma constante e defasagens de $y_t$, e $\Omega$ é a matriz de covariância dos erros. A MLE de Π, que denotamos por $\hat{\Pi}$, é obtida maximizando esta função, o que é equivalente a minimizar o termo quadrático [^1]:
$$
\sum_{t=1}^T (y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t)
$$
Para mostrar a equivalência com a regressão OLS, é crucial manipular este termo.  Primeiramente, vamos adicionar e subtrair $\hat{\Pi}'x_t$ dentro da expressão, onde $\hat{\Pi}'$ representa a MLE de $\Pi'$:
$$
\sum_{t=1}^T (y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t) = \sum_{t=1}^T (y_t - \hat{\Pi}'x_t + \hat{\Pi}'x_t - \Pi'x_t)'\Omega^{-1}(y_t - \hat{\Pi}'x_t + \hat{\Pi}'x_t - \Pi'x_t)
$$
Reescrevendo a expressão, obtemos
$$
\sum_{t=1}^T (\hat{\epsilon}_t + (\hat{\Pi}' - \Pi')x_t)'\Omega^{-1}(\hat{\epsilon}_t + (\hat{\Pi}' - \Pi')x_t)
$$
onde $\hat{\epsilon}_t = y_t - \hat{\Pi}'x_t$ são os resíduos da regressão OLS de $y_t$ em $x_t$. Expandindo esta expressão, temos
$$
\sum_{t=1}^T (\hat{\epsilon}_t'\Omega^{-1}\hat{\epsilon}_t + \hat{\epsilon}_t'\Omega^{-1}(\hat{\Pi}' - \Pi')x_t + x_t'(\hat{\Pi}' - \Pi')'\Omega^{-1}\hat{\epsilon}_t + x_t'(\hat{\Pi}' - \Pi')'\Omega^{-1}(\hat{\Pi}' - \Pi')x_t)
$$
Pela propriedade de ortogonalidade dos resíduos da regressão OLS, os termos $\sum_{t=1}^T \hat{\epsilon}_t'\Omega^{-1}(\hat{\Pi}' - \Pi')x_t$  e $\sum_{t=1}^T x_t'(\hat{\Pi}' - \Pi')'\Omega^{-1}\hat{\epsilon}_t$ são iguais a zero. Assim, a expressão se reduz a
$$
\sum_{t=1}^T \hat{\epsilon}_t'\Omega^{-1}\hat{\epsilon}_t + \sum_{t=1}^T x_t'(\hat{\Pi}' - \Pi')'\Omega^{-1}(\hat{\Pi}' - \Pi')x_t
$$
O primeiro termo não depende de $\Pi'$ e, portanto, não influencia a minimização.  O segundo termo, que é uma forma quadrática, é minimizado quando $\hat{\Pi}'$ se aproxima de $\Pi'$. No entanto, este termo envolve a matriz $\Omega$, que ainda precisa ser estimada, o que torna a minimização não direta [^1]. Para simplificar, vamos usar a propriedade do traço de matrizes.  Como a expressão $\sum_{t=1}^T x_t'(\hat{\Pi}' - \Pi')'\Omega^{-1}(\hat{\Pi}' - \Pi')x_t$ é um escalar, ela é igual ao seu traço. Usando a propriedade de que  $tr(AB) = tr(BA)$ e $tr(A) = tr(A')$, podemos escrever o traço da seguinte forma:
$$
tr\left( \sum_{t=1}^T x_t'(\hat{\Pi}' - \Pi')'\Omega^{-1}(\hat{\Pi}' - \Pi')x_t\right) = tr\left(\Omega^{-1} \sum_{t=1}^T (\hat{\Pi}' - \Pi')x_t x_t'(\hat{\Pi}' - \Pi')'\right)
$$
O termo que precisamos minimizar envolve a matriz de covariância $\Omega$. Uma abordagem comum é assumir que $\Omega$ é conhecida, embora na prática ela seja estimada.  Alternativamente, podemos minimizar este termo usando o fato de que, para uma matriz positiva definida $A$ e uma matriz qualquer $B$, a forma quadrática $tr(B'AB)$ é minimizada quando $B=0$. Assim, para minimizar o termo $\sum_{t=1}^T x_t'(\hat{\Pi}' - \Pi')'\Omega^{-1}(\hat{\Pi}' - \Pi')x_t$, necessitamos que $(\hat{\Pi}' - \Pi')=0$, ou seja, $\hat{\Pi}' = \Pi'$.
Para ver a conexão com OLS, note que cada linha $j$ de $\hat{\Pi}'$, denotada por $\hat{\pi}_j'$, corresponde aos parâmetros da equação $j$ e satisfaz as condições de primeira ordem das equações normais da regressão OLS:
$$
\hat{\pi}_j' = \left(\sum_{t=1}^T y_{jt} x_t'\right)\left(\sum_{t=1}^T x_t x_t'\right)^{-1}
$$
Portanto, a minimização da função de log-verossimilhança implica em obter estimadores de cada equação do modelo VAR através de regressões OLS individuais. Cada linha $j$ de $\hat{\Pi}'$ é o vetor de coeficientes estimado na regressão OLS da variável $y_{jt}$ em $x_t$ [^1].

### Conclusão
Através da manipulação algébrica da função de log-verossimilhança e da utilização da propriedade do traço, demonstramos que o processo de otimização da MLE para a matriz Π em um modelo VAR é equivalente a realizar regressões OLS separadas para cada equação do sistema.  Este resultado é fundamental porque transforma um problema de otimização complexo em um conjunto de problemas de otimização de regressão mais simples, permitindo a aplicação de técnicas bem conhecidas de regressão OLS. Esta equivalência não só simplifica o processo computacional, mas também fornece uma ligação teórica entre a estimativa de máxima verossimilhança e métodos estatísticos tradicionais, reforçando a robustez da análise de modelos VAR em séries temporais multivariadas [^1].

### Referências
[^1]: Texto fornecido para análise
<!-- END -->
