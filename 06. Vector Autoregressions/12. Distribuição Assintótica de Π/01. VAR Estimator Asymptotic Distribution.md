## Distribuição Assintótica do Estimador de Π em Modelos VAR

### Introdução
Este capítulo se aprofunda na análise da **distribuição assintótica** do estimador de **Π** em modelos Vector Autoregressive (VAR), um tópico fundamental para a inferência estatística em séries temporais multivariadas. A distribuição assintótica de **Π** revela importantes propriedades de convergência e normalidade, que são cruciais para a validação e interpretação dos resultados obtidos a partir de modelos VAR. Conectando-se com os temas previamente abordados, como a estimação por máxima verossimilhança (MLE) de **П** [^1], esta seção expande o conhecimento, focando na distribuição dos estimadores em amostras grandes e sua relevância para testes de hipóteses e construção de intervalos de confiança.

### Conceitos Fundamentais
A análise da distribuição assintótica de **Π** é baseada em alguns conceitos cruciais. Primeiramente, o estimador de máxima verossimilhança (MLE) de **Π**, denotado por **Π̂**, é obtido através da maximização da função de verossimilhança [^1]. Este estimador, sob certas condições, converge em probabilidade para o verdadeiro valor de **Π**. A importância da convergência assintótica reside no fato de que, para amostras grandes, **Π̂** torna-se um estimador confiável do verdadeiro parâmetro populacional.

A **Proposição 11.1** estabelece as condições sob as quais essa convergência ocorre. Especificamente, as condições incluem que os erros $\epsilon_t$ sejam independentes e identicamente distribuídos com média zero e matriz de covariância $\Omega$, e que as raízes da equação polinomial característica do VAR estejam fora do círculo unitário [^2]. Essa condição assegura a estacionariedade do processo, um pré-requisito para a aplicação da teoria assintótica.

A **normalidade assintótica** do estimador de **П** é outro resultado chave. A Proposição 11.1 (d) [^2] implica que o vetor de coeficientes da *i*-ésima regressão em um VAR converge para uma distribuição normal com média no parâmetro populacional e matriz de covariância estimada por $(\sigma^2)Q^{-1}$. Essa matriz de covariância é composta por $Q^{-1}$, a inversa da matriz de variância das variáveis explicativas, e $\sigma^2$, que é a variância do erro da regressão. Como resultado, as estatísticas *t* e *F* das regressões OLS são assintoticamente válidas [^2].

De forma mais geral, a **Proposição 11.1 (d)** [^2] também demonstra que sob as condições de estacionariedade, a raiz quadrada de T (o tamanho da amostra) vezes a diferença entre o estimador de MLE **Π̂** e o verdadeiro parâmetro **П** converge em distribuição para uma normal multivariada com média zero e matriz de covariância dada por $(\Omega \otimes Q^{-1})$, onde $\otimes$ denota o produto de Kronecker. Essa convergência é formalizada da seguinte forma:
$$ \sqrt{T}(\hat{\Pi} - \Pi) \xrightarrow{d} \mathcal{N}(0, (\Omega \otimes Q^{-1}))$$
Essa convergência é de extrema importância para a realização de inferência estatística sobre os coeficientes de modelos VAR.

O produto de Kronecker, denotado por $\otimes$, desempenha um papel crucial na definição da matriz de covariância assintótica. Ele combina as informações de variância-covariância dos erros ($\Omega$) com a variância das variáveis explicativas ($Q^{-1}$), produzindo uma estrutura de covariância que descreve a relação entre todos os parâmetros do VAR. Em termos práticos, essa matriz é usada para calcular os desvios padrões dos parâmetros estimados, possibilitando a construção de intervalos de confiança e testes de hipóteses [^2].

A **matriz Q** é definida como a esperança do produto das variáveis explicativas:
$$Q = E(x_t x_t')$$
E é estimada através da média amostral:
$$\hat{Q} = \frac{1}{T}\sum_{t=1}^{T}x_t x_t'$$
Onde $x_t$ representa o vetor que contém um termo constante e *p* defasagens de cada uma das variáveis do sistema.

Além disso, o texto menciona o uso da forma usual do OLS para obter a variância dos coeficientes, onde $s^2$ [^2] é utilizado no lugar de $\sigma^2$, sendo $s^2$ a estimativa da variância do erro, dada por:
$$s^2 = \frac{1}{T-k} \sum_{t=1}^{T} \hat{\epsilon}_t^2 $$
E, como destacado no texto, *k = np + 1*. Embora $s^2$ e $\sigma^2$ sejam assintoticamente equivalentes, $s^2$ é preferível para amostras menores [^2].

A **Proposição 11.1** também estabelece que as estatísticas *t* e *F* padrão aplicadas aos coeficientes de qualquer equação no VAR são assintoticamente válidas. Isso permite que testes de hipóteses e construção de intervalos de confiança sejam realizados da mesma maneira que em modelos lineares clássicos [^2].

Uma importante implicação da distribuição assintótica é que ela permite testar hipóteses sobre combinações lineares de coeficientes de diferentes equações do VAR. A generalização do teste Wald, expressa em [11.1.38], possibilita testar hipóteses da forma $R\pi = r$, onde $R$ é uma matriz de restrições e $r$ é um vetor de constantes [^2]. A estatística do teste tem uma distribuição assintótica $\chi^2$ com graus de liberdade iguais ao número de restrições.

### Conclusão
A distribuição assintótica do estimador de **П** em modelos VAR é um pilar fundamental para a inferência estatística nesses modelos. A convergência e a normalidade assintótica dos estimadores permitem a realização de testes de hipóteses e a construção de intervalos de confiança de forma consistente. Os resultados apresentados neste capítulo, baseados na Proposição 11.1,  [1], consolidam o entendimento de que os estimadores de máxima verossimilhança em modelos VAR, sob condições razoáveis, são assintoticamente válidos para a inferência. Complementarmente, é apresentado o teste de Wald como ferramenta para testes mais complexos e para explorar relações entre as diferentes variáveis do sistema [^2].

### Referências
[^1]: ... *[Refere-se ao contexto onde se trata da estimação por máxima verossimilhança de П]*
[^2]: ... *[Refere-se aos trechos onde a Proposição 11.1, e suas implicações são discutidas]*
<!-- END -->
