## Decomposição da Log-Verossimilhança em Modelos VAR para Estimação de Máxima Verossimilhança

### Introdução

Este capítulo explora a técnica de decomposição da função de log-verossimilhança em modelos de Vetores Autorregressivos (VAR), particularmente em contextos onde restrições como a exogeneidade de bloco são impostas. Como discutido anteriormente, a estimação de máxima verossimilhança (MLE) em modelos VAR restritos envolve a imposição de restrições teóricas para derivar estimadores consistentes e eficientes [^1]. A abordagem de decomposição da log-verossimilhança oferece uma forma simplificada de abordar problemas de otimização complexos, dividindo a função de verossimilhança conjunta em partes mais manejáveis: log-verossimilhança marginal e condicional.

### Decomposição da Log-Verossimilhança

Para simplificar a estimação de máxima verossimilhança em modelos VAR com restrições, como a exogeneidade de bloco, a função de log-verossimilhança conjunta pode ser decomposta na soma das log-verossimilhanças marginal e condicional [^1].
Considere o modelo VAR com duas variáveis $y_{1t}$ ($n_1 \times 1$) e $y_{2t}$ ($n_2 \times 1$) sob a restrição de exogeneidade de bloco, como definido anteriormente [^1]:

$$ y_{1t} = c_1 + A_1x_{1t} + \epsilon_{1t} $$
$$ y_{2t} = c_2 + B_1x_{1t} + B_2x_{2t} + \epsilon_{2t} $$

Onde $x_{1t}$ e $x_{2t}$ contêm os lags de $y_{1t}$ e $y_{2t}$, respectivamente e a exogeneidade de bloco de $y_1$ em relação a $y_2$ implica que a matriz $A_2 = 0$.

A função de log-verossimilhança conjunta pode ser expressa como:
$$ \log f(y_t | x_t; \theta) = \log f(y_{1t} | x_t; \theta) + \log f(y_{2t} | y_{1t}, x_t; \theta) $$
onde $\theta$ representa todos os parâmetros do modelo. O primeiro termo da soma representa a log-verossimilhança marginal de $y_{1t}$:
$$ l_{1t} = \log f(y_{1t} | x_t; \theta) = -(n_1/2)\log(2\pi) - \frac{1}{2}\log|\Omega_{11}| - \frac{1}{2}(y_{1t}-c_1 - A_1x_{1t})'\Omega_{11}^{-1}(y_{1t}-c_1 - A_1x_{1t}) $$
Essa expressão é baseada em uma distribuição gaussiana para $y_{1t}$ condicionado a $x_t$, onde $ \Omega_{11} $ representa a matriz de variância-covariância condicional de $y_{1t}$.

O segundo termo corresponde à log-verossimilhança condicional de $y_{2t}$ dado $y_{1t}$ e $x_t$:
$$ l_{2t} = \log f(y_{2t} | y_{1t}, x_t; \theta) = -(n_2/2)\log(2\pi) - \frac{1}{2}\log|H| - \frac{1}{2}(y_{2t} - m_{2t})'H^{-1}(y_{2t} - m_{2t}) $$
onde:
$$ H = \Omega_{22} - \Omega_{21} \Omega_{11}^{-1} \Omega_{12} $$
$$ m_{2t} = E(y_{2t}|x_t) + \Omega_{21} \Omega_{11}^{-1} [y_{1t} - E(y_{1t}|x_t)] $$

Aqui, $H$ é a matriz de variância-covariância condicional de $y_{2t}$ dado $y_{1t}$ e $x_t$, e $m_{2t}$ é a média condicional de $y_{2t}$.  $ \Omega_{12} $ é a matriz de covariância entre $y_{1t}$ e $y_{2t}$.

**Otimização da Log-Verossimilhança Decomposta:**
Ao decompor a função de log-verossimilhança dessa forma, a otimização se torna mais tratável. A log-verossimilhança total é dada por:
$$ L(\theta) = \sum_{t=1}^T (l_{1t} + l_{2t}) $$
onde $T$ é o número de observações.

A imposição da restrição $A_2 = 0$ torna a função de verossimilhança total tratável e os parâmetros podem ser estimados em duas etapas, explorando a independência dos termos. Na primeira etapa, os parâmetros $c_1$, $A_1$ e $\Omega_{11}$ podem ser estimados pela maximização de $\sum_{t=1}^T l_{1t}$ , o que se resume em uma regressão de OLS de $y_{1t}$ sobre os lags de $y_{1t}$ [^1]:
$$ y_{1t} = c_1 + A_1x_{1t} + \epsilon_{1t} $$

Na segunda etapa, os parâmetros restantes ($d$, $D_0$, $D_1$, $D_2$, e $H$) são estimados da maximização da função de log-verossimilhança condicional $\sum_{t=1}^T l_{2t}$. Note que $d$, $D_0$, $D_1$ e $D_2$ são funções dos parâmetros originais $c_2$, $B_1$, $B_2$, $\Omega_{11}$, $\Omega_{12}$ e $\Omega_{22}$. Na prática, isso também é equivalente a uma regressão de OLS da equação para $y_{2t}$ [^1]:
$$ y_{2t} = d + D_0y_{1t} + D_1x_{1t} + D_2x_{2t} + v_{2t} $$

Essa abordagem de duas etapas simplifica significativamente a otimização da função de log-verossimilhança, transformando um problema de otimização conjunta em duas regressões de OLS separadas, o que facilita a obtenção dos estimadores MLE sob a restrição de exogeneidade de bloco [^1].

### Benefícios da Decomposição
A decomposição da função de log-verossimilhança proporciona as seguintes vantagens:
- **Simplificação da Otimização:** Ao separar a log-verossimilhança conjunta em termos marginal e condicional, a estimação de máxima verossimilhança é transformada em problemas de otimização mais simples que podem ser resolvidos por OLS.
- **Entendimento do Modelo:** A decomposição explícita permite um entendimento mais claro de como diferentes variáveis são relacionadas, tanto marginalmente quanto condicionalmente.
- **Eficiência Computacional:** O uso de OLS para cada equação separadamente é computacionalmente mais eficiente do que tentar maximizar a função de verossimilhança conjunta, particularmente em sistemas grandes.
- **Flexibilidade:** Essa abordagem pode ser utilizada em conjunto com diferentes restrições, fornecendo um modelo flexível para análise econométrica.

### Conclusão

A técnica de decomposição da função de log-verossimilhança, ao transformar um problema complexo em uma série de otimizações menores, apresenta uma ferramenta poderosa na estimação de máxima verossimilhança de modelos VAR restritos. Ao separar a função de log-verossimilhança conjunta em componentes marginal e condicional, este método não só simplifica o processo de estimação, mas também oferece um entendimento mais aprofundado da relação entre as variáveis no modelo, com particular aplicabilidade em situações de restrições como a exogeneidade de bloco. Esse procedimento resulta em estimadores de máxima verossimilhança eficientes e consistentes. A capacidade de decompor a função de verossimilhança não apenas facilita a estimação mas também oferece uma visão clara da estrutura do modelo e das inter-relações entre suas variáveis, contribuindo significativamente para a análise econométrica.
### Referências
[^1]: [11.3.  Maximum Likelihood Estimation of Restricted Vector Autoregressions]
<!-- END -->
