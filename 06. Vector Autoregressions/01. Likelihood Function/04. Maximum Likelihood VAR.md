## O Estimador de Máxima Verossimilhança em Modelos VAR

### Introdução

Este capítulo aborda em detalhes o processo de obtenção do estimador de máxima verossimilhança (MLE) para os parâmetros de um modelo Vetorial Autorregressivo (VAR). Exploraremos como a maximização da função de verossimilhança logarítmica leva às estimativas dos coeficientes e da matriz de covariância, usando cálculos matriciais complexos para chegar a uma solução analítica.

### Conceitos Fundamentais

O objetivo da estimação de máxima verossimilhança é encontrar os valores dos parâmetros do modelo (representados por $\Pi'$ e $\Omega$) que maximizam a função de verossimilhança logarítmica $\mathcal{L}(\Theta)$, que foi detalhada no capítulo anterior [^1]:
$$
\mathcal{L}(\Theta) = -\frac{Tn}{2} \log(2\pi) + \frac{T}{2} \log|\Omega^{-1}| - \frac{1}{2} \sum_{t=1}^{T} (y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t)
$$
O processo de maximização da log-verossimilhança é realizado em duas etapas principais: primeiro, encontramos o MLE para $\Pi$, e depois, para $\Omega$.

**1. MLE para $\Pi$:**

Considerando a primeira parte do problema, que é encontrar o MLE de $\Pi$, que contém o termo constante $c$ e os coeficientes autorregressivos $\Phi$,  observamos que a expressão para o MLE de $\Pi$ é dada por [^1]:
$$
\hat{\Pi}' = \left[ \sum_{t=1}^T y_t x_t' \right] \left[ \sum_{t=1}^T x_t x_t' \right]^{-1}
$$
Esta expressão pode ser vista como uma analogia amostral da projeção linear da população de $y_t$ sobre uma constante e $x_t$. A $j$-ésima linha de  $\hat{\Pi}'$ é dada por:
$$
\hat{\Pi}'_j = \left[ \sum_{t=1}^T y_{jt} x_t' \right] \left[ \sum_{t=1}^T x_t x_t' \right]^{-1}
$$
Essa expressão indica que o MLE dos coeficientes para a $j$-ésima equação do VAR é encontrado através de uma regressão OLS de $y_{jt}$ em $x_t$ [^1].

Para verificar [11.1.11], escrevemos a soma que aparece no último termo de [11.1.10] como [^1]:
$$
\sum_{t=1}^T (y_t - \Pi'x_t)' \Omega^{-1} (y_t - \Pi'x_t)
$$
Em seguida, decompomos a expressão de modo que tenhamos o resíduo amostral $\hat{\epsilon}_t$ resultante da regressão OLS de $y_t$ em $x_t$, e $\hat{\Pi}$ como o estimador OLS de $\Pi$ [^1]:

$$
\sum_{t=1}^T \left[\hat{\epsilon}_t + (\hat{\Pi} - \Pi)'x_t\right]' \Omega^{-1} \left[\hat{\epsilon}_t + (\hat{\Pi} - \Pi)'x_t\right]
$$
Expandindo essa expressão, temos:

$$
\sum_{t=1}^T \hat{\epsilon}_t' \Omega^{-1} \hat{\epsilon}_t + 2 \sum_{t=1}^T \hat{\epsilon}_t' \Omega^{-1} (\hat{\Pi} - \Pi)'x_t + \sum_{t=1}^T x_t' (\hat{\Pi} - \Pi) \Omega^{-1} (\hat{\Pi} - \Pi)' x_t
$$
O termo do meio na equação acima é igual a zero, pois os resíduos da regressão OLS são ortogonais às variáveis explicativas (ou seja, $\sum_{t=1}^T \hat{\epsilon}_t x_t' = 0$) [^1]. A expressão então se simplifica para:

$$
\sum_{t=1}^T \hat{\epsilon}_t' \Omega^{-1} \hat{\epsilon}_t + \sum_{t=1}^T x_t' (\hat{\Pi} - \Pi) \Omega^{-1} (\hat{\Pi} - \Pi)' x_t
$$
O segundo termo é positivo para qualquer sequência $x_t$, a menos que $\hat{\Pi} = \Pi$. Minimizar essa expressão requer que $\hat{\Pi} = \hat{\Pi}$, que corresponde às estimativas de OLS. Assim, concluímos que a regressão OLS fornece os estimadores de máxima verossimilhança para os coeficientes de um modelo VAR [^1].

**2. MLE para $\Omega$:**

Para encontrar o MLE de $\Omega$, derivamos a log-verossimilhança em relação à matriz inversa de $\Omega$,  $\Omega^{-1}$, usando resultados de cálculo matricial [^1]. A derivada da log-verossimilhança em relação a $\Omega^{-1}$ é:

$$
\frac{\partial \mathcal{L}(\Theta)}{\partial \Omega^{-1}} = \frac{T}{2} \Omega - \frac{1}{2} \sum_{t=1}^{T} (y_t - \Pi'x_t)(y_t - \Pi'x_t)'
$$
Definindo os resíduos amostrais da regressão OLS como $\hat{\epsilon}_t = y_t - \hat{\Pi}'x_t$, e igualando a derivada a zero, obtemos:
$$
\hat{\Omega}^{-1} = \frac{1}{T} \sum_{t=1}^T \hat{\epsilon}_t\hat{\epsilon}_t'
$$
Onde $\hat{\Omega}$ é o estimador de máxima verossimilhança para a matriz de covariância, sendo uma média amostral do produto externo dos resíduos da regressão OLS [^1].  Note que esse resultado é consistente com o fato de que $\Omega$ é simétrica e positiva definida [^1].

### Conclusão

Neste capítulo, detalhamos o processo de obtenção do estimador de máxima verossimilhança para um modelo VAR. Demonstramos como, por meio da maximização da função de log-verossimilhança, chegamos a estimativas para os coeficientes do modelo através de regressões OLS e para a matriz de covariância, através da média amostral do produto externo dos resíduos. O processo envolve cálculos matriciais, porém fornece estimativas consistentes e eficientes para os parâmetros do modelo VAR. Este processo de estimação é um componente fundamental para a análise e interpretação de modelos VAR.

### Referências
[^1]: Seções "Maximum Likelihood Estimate of II" e "The Maximum Likelihood Estimate of $\Omega$" do texto fornecido.
<!-- END -->
