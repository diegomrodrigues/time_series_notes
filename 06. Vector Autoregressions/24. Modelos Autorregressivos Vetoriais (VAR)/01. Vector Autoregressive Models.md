## Modelos Autorregressivos Vetoriais (VAR)

### Introdução
Este capítulo explora os modelos autorregressivos vetoriais (VAR), uma ferramenta fundamental para a análise de múltiplas séries temporais. Em contraste com modelos univariados, que focam em uma única série, os modelos VAR permitem examinar as interdependências dinâmicas entre várias séries temporais simultaneamente. Como visto anteriormente [^1], a análise de séries temporais muitas vezes se beneficia ao considerar as relações entre diferentes variáveis. Modelos VAR são particularmente úteis quando se suspeita que várias séries temporais influenciam umas às outras, e são um componente crucial em macroeconometria e econometria financeira [^2]. Neste contexto, vamos detalhar a construção, estimação, e testes de hipóteses em modelos VAR.

### Conceitos Fundamentais
Um modelo VAR generaliza o conceito de autorregressão para um sistema de múltiplas séries temporais. Em sua forma mais básica, um modelo VAR de ordem *p*, ou VAR(*p*), modela cada variável em um vetor *y* como uma função linear de seus próprios valores passados e dos valores passados das outras variáveis do sistema [^1]. Formalmente, podemos expressar um VAR(*p*) como:
$$ y_t = c + \Phi_1 y_{t-1} + \Phi_2 y_{t-2} + \ldots + \Phi_p y_{t-p} + \epsilon_t$$
onde:
*   $y_t$ é um vetor *n x 1* de séries temporais no tempo *t*.
*   $c$ é um vetor *n x 1* de constantes.
*   $\Phi_1, \Phi_2, \ldots, \Phi_p$ são matrizes *n x n* de coeficientes autorregressivos.
*   $\epsilon_t$ é um vetor *n x 1* de ruídos brancos, com média zero e matriz de covariância $\Omega$.

O termo $p$ denota a ordem da autorregressão. A intuição por trás do modelo VAR é que o valor atual de cada série temporal é afetado pelos seus próprios valores passados e também pelos valores passados das outras séries temporais no sistema [^1]. A análise VAR é útil para capturar as interdependências dinâmicas entre as séries.

Para facilitar a análise, é útil expressar o modelo VAR em uma forma mais compacta. Defina um vetor $x_t$ que contém uma constante e *p* lags de cada elemento de *y* [^1]:

$$
x_t =
\begin{bmatrix}
1 \\ y_{t-1} \\ y_{t-2} \\ \vdots \\ y_{t-p}
\end{bmatrix}
$$

Aqui, $x_t$ é um vetor $[(np+1)\times 1]$. Seja $\Pi'$ uma matriz $[n \times (np+1)]$ definida por:

$$\Pi' = [c \ \Phi_1 \ \Phi_2 \ \ldots \ \Phi_p]$$

Com esta notação, a média condicional [^1] pode ser escrita como:

$$E(y_t|y_{t-1}, y_{t-2}, \ldots, y_{t-p}) = \Pi'x_t$$

e o modelo VAR pode ser compactamente expresso como:

$$y_t | y_{t-1}, y_{t-2}, \ldots, y_{t-p+1} \sim N(\Pi'x_t, \Omega)$$

A densidade condicional da t-ésima observação é dada por [^1]:
$$f(y_t | y_{t-1}, y_{t-2},\ldots,y_{t-p+1};\theta) = (2\pi)^{-n/2} |\Omega|^{-1/2} \exp\left(-\frac{1}{2}(y_t - \Pi'x_t)' \Omega^{-1} (y_t - \Pi'x_t) \right)$$

Onde $\theta$ representa todos os parâmetros do modelo.

A função de *likelihood* para a amostra completa, condicionada em $y_0, y_{-1}, \ldots, y_{-p+1}$ é o produto das densidades condicionais individuais [^1]:
$$L(\theta) = \prod_{t=1}^{T}f(y_t | y_{t-1}, y_{t-2},\ldots,y_{t-p+1};\theta)$$
$$L(\theta) =  \prod_{t=1}^{T} (2\pi)^{-n/2} |\Omega|^{-1/2} \exp\left(-\frac{1}{2}(y_t - \Pi'x_t)' \Omega^{-1} (y_t - \Pi'x_t) \right)$$
e o log-likelihood é:
$$l(\theta) = -\frac{Tn}{2} \log(2\pi) + \frac{T}{2} \log|\Omega^{-1}| - \frac{1}{2} \sum_{t=1}^T (y_t - \Pi'x_t)' \Omega^{-1} (y_t - \Pi'x_t)$$ [^1]

Para estimar os parâmetros do modelo VAR, podemos usar o método da Máxima Verossimilhança (MLE). O MLE para $\Pi$ é dado por [^2]:
$$\hat{\Pi}' = \left( \sum_{t=1}^T y_t x_t' \right) \left( \sum_{t=1}^T x_t x_t' \right)^{-1}$$

Que pode ser interpretado como uma regressão linear de cada variável em *y* em uma constante e *p* lags de todas as variáveis [^2].
Para verificar isso, podemos escrever a soma que aparece no último termo da função log-verossimilhança da seguinte forma [^2]:
$$\sum_{t=1}^T (y_t - \Pi'x_t)' \Omega^{-1} (y_t - \Pi'x_t) = \sum_{t=1}^T (\hat{\epsilon_t} + (\hat{\Pi}'-\Pi')x_t)' \Omega^{-1} (\hat{\epsilon_t} + (\hat{\Pi}'-\Pi')x_t)$$
Expandindo [^2]:
$$\sum_{t=1}^T \hat{\epsilon_t}' \Omega^{-1} \hat{\epsilon_t} + 2\sum_{t=1}^T \hat{\epsilon_t}' \Omega^{-1} (\hat{\Pi}'-\Pi')x_t  + \sum_{t=1}^T x_t' (\hat{\Pi}'-\Pi')' \Omega^{-1} (\hat{\Pi}'-\Pi')x_t$$

Onde $\hat{\epsilon_t}$ é o resíduo da regressão OLS de $y_t$ em $x_t$, $\hat{\epsilon_t} = y_t - \hat{\Pi}'x_t$ [^2]. Como $\sum_{t=1}^T \hat{\epsilon_t} x_t' = 0$, o termo do meio acima é zero. Assim, a soma pode ser escrita [^2]:
$$\sum_{t=1}^T \hat{\epsilon_t}' \Omega^{-1} \hat{\epsilon_t} + \sum_{t=1}^T x_t' (\hat{\Pi}'-\Pi')' \Omega^{-1} (\hat{\Pi}'-\Pi')x_t$$

Podemos ainda simplificar o último termo usando o operador "trace" [^2]:
$$\sum_{t=1}^T x_t' (\hat{\Pi}'-\Pi')' \Omega^{-1} (\hat{\Pi}'-\Pi')x_t = \text{trace} \left[ \Omega^{-1} (\hat{\Pi}'-\Pi')\sum_{t=1}^T x_t x_t' (\hat{\Pi}'-\Pi')' \right]$$
Como os resíduos da regressão OLS são ortogonais às variáveis explicativas, a soma da função log-verossimilhança é minimizada quando $\Pi = \hat{\Pi}$, comprovando que a estimativa OLS maximiza a função de verossimilhança.

A estimativa de máxima verossimilhança para $\Omega$ é dada por [^2]:
$$\hat{\Omega} = \frac{1}{T} \sum_{t=1}^T \hat{\epsilon}_t \hat{\epsilon}_t'$$

A qual é a matriz de covariância amostral dos resíduos da regressão OLS [^2]. A diagonal de $\hat{\Omega}$ contém as variâncias dos erros de cada equação do VAR e os elementos fora da diagonal representam as covariâncias entre os erros das equações diferentes [^2].

Para testar hipóteses sobre os coeficientes do modelo VAR, podemos usar o teste da razão de verossimilhança. Se temos uma hipótese nula $H_0$ contra uma alternativa $H_1$, o teste LR é definido como [^2]:
$$LR = 2(l(\hat{\theta}) - l(\hat{\theta}_0))$$
Onde $l(\hat{\theta})$ é o valor do log-likelihood com todos os parâmetros do modelo estimados (não restritos) e $l(\hat{\theta}_0)$ é o valor da log-verossimilhança com as restrições da hipótese nula ($H_0$) aplicadas [^2]. Sob a hipótese nula, este teste é assintoticamente distribuído como uma $\chi^2$ com graus de liberdade iguais ao número de restrições impostas.

Uma questão importante ao utilizar modelos VAR é a seleção da ordem $p$ da autorregressão. Critérios de informação como o AIC (Critério de Informação de Akaike) ou o BIC (Critério de Informação Bayesiano) são comumente utilizados para escolher a ordem apropriada [^2].

Os resultados assintóticos para os estimadores de máxima verossimilhança são de que eles são consistentes mesmo se os erros não forem Gaussianos [^2].

### Conclusão
Modelos VAR são uma ferramenta poderosa para analisar as relações dinâmicas entre múltiplas séries temporais. As estimativas obtidas usando OLS ou MLE são consistentes e assintoticamente válidas. As técnicas de teste de hipóteses discutidas permitem uma análise detalhada das relações entre as variáveis. Modelos VAR são uma base para muitas outras técnicas de análise de séries temporais. O próximo passo lógico é a extensão destes modelos para incorporar restrições que reflitam conhecimento prévio ou teoria econômica [^3].

### Referências
[^1]: Trechos da página 292.
[^2]: Trechos das páginas 293-297.
[^3]: Trechos das páginas 308-311.
<!-- END -->
