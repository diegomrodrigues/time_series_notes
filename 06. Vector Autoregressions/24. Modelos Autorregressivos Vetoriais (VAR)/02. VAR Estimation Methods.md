## Estimação em Modelos VAR: Abordagens OLS e MLE com Restrições
### Introdução

Como discutido anteriormente, modelos autorregressivos vetoriais (VAR) são ferramentas essenciais para analisar as relações dinâmicas entre múltiplas séries temporais [^1]. A estimação desses modelos pode ser realizada por meio de várias abordagens, cada uma com suas particularidades e adequações [^1]. Este capítulo se aprofunda em duas abordagens principais: a estimação por mínimos quadrados ordinários (OLS) e a estimação por máxima verossimilhança (MLE), com foco especial em como restrições podem ser incorporadas no processo de estimação. Em particular, exploraremos o caso onde modelos VAR são expressos com certas restrições em seus coeficientes.

### Estimação por Mínimos Quadrados Ordinários (OLS)
A estimação por mínimos quadrados ordinários (OLS) é uma abordagem comum e simples para estimar os coeficientes de um modelo VAR [^2]. Como vimos, o modelo VAR pode ser escrito de forma compacta como:
$$y_t = \Pi'x_t + \epsilon_t$$
onde $y_t$ é um vetor *n x 1* de séries temporais, $\Pi$ é a matriz de coeficientes e $x_t$ é um vetor contendo uma constante e os lags das variáveis [^1]. A abordagem OLS estima cada equação do VAR independentemente das outras [^2]. Isso significa que, para cada variável em *y*, é realizada uma regressão linear onde esta variável é a dependente e as variáveis em $x_t$ são as independentes [^2]. A intuição aqui é que cada variável é explicada por seus próprios lags e os lags das outras variáveis do sistema [^1].

A grande vantagem da estimação OLS é a sua simplicidade e facilidade de implementação [^1]. No entanto, ela tem algumas limitações, como a incapacidade de impor restrições complexas entre os coeficientes de diferentes equações, por exemplo.

### Estimação por Máxima Verossimilhança (MLE) com Restrições
A estimação por máxima verossimilhança (MLE) é uma abordagem mais geral que permite a incorporação de restrições nos parâmetros do modelo [^2]. No contexto do modelo VAR, podemos impor restrições nas matrizes de coeficientes autorregressivos $\Phi_1, \Phi_2, \ldots, \Phi_p$. Como exemplo, considere um sistema de duas variáveis, $y_{1t}$ e $y_{2t}$, onde $y_{1t}$ é modelado em função de seus lags e dos lags de $y_{2t}$, mas $y_{2t}$ depende apenas dos seus próprios lags:
$$
\begin{bmatrix}
y_{1t} \\ y_{2t}
\end{bmatrix} =
\begin{bmatrix}
c_1 \\ c_2
\end{bmatrix} +
\begin{bmatrix}
\phi_{11} & \phi_{12} \\ 0 & \phi_{22}
\end{bmatrix}
\begin{bmatrix}
y_{1t-1} \\ y_{2t-1}
\end{bmatrix} +
\begin{bmatrix}
\epsilon_{1t} \\ \epsilon_{2t}
\end{bmatrix}
$$
Aqui, estamos impondo que o efeito de $y_{1t-1}$ sobre $y_{2t}$ é zero.
Para encontrar as estimativas de máxima verossimilhança (MLE) em um VAR com restrições, podemos usar as propriedades da distribuição normal dos erros para construir a função de verossimilhança e maximizar sob as restrições impostas [^2]. Em particular, o log-likelihood para o modelo VAR é dado por [^2]:
$$l(\theta) = -\frac{Tn}{2} \log(2\pi) + \frac{T}{2} \log|\Omega^{-1}| - \frac{1}{2} \sum_{t=1}^T (y_t - \Pi'x_t)' \Omega^{-1} (y_t - \Pi'x_t)$$
onde, como já visto, $\theta$ representa o conjunto de parâmetros do modelo, incluindo os elementos da matriz $\Pi$ (que contém as constantes e os coeficientes autorregressivos) e da matriz $\Omega$.

Ao impormos restrições aos coeficientes, estamos efetivamente criando uma versão restrita da função de verossimilhança [^2]. A maximização da função log-verossimilhança restrita geralmente não pode ser resolvida analiticamente, e é necessário o uso de métodos numéricos. A questão chave aqui é que podemos reescrever as variáveis para que a estimação de MLE sob certas restrições seja tão simples quanto OLS em um sistema sem restrições.

Vamos examinar este processo sob a restrição de block exogeneity [^3]. Assuma que os parâmetros do modelo VAR são separados em dois grupos de variáveis, $y_{1t}$ ($n_1 \times 1$) e $y_{2t}$ ($n_2 \times 1$), onde $y_t = [y_{1t}', y_{2t}']'$. A especificação VAR pode ser escrita como:
$$y_{1t} = c_1 + A_1 x_{1t} + A_2 x_{2t} + \epsilon_{1t}$$
$$y_{2t} = c_2 + B_1 x_{1t} + B_2 x_{2t} + \epsilon_{2t}$$
onde $x_{1t}$ são os lags de $y_{1t}$, $x_{2t}$ são os lags de $y_{2t}$, e as matrizes $A_1$, $A_2$, $B_1$, e $B_2$ contêm os coeficientes autorregressivos. Se a hipótese nula é de que as variáveis em $y_{1t}$ não são influenciadas pelas variáveis em $y_{2t}$, então $A_2 = 0$. Neste caso, dizemos que $y_{1t}$ é block-exógeno com respeito a $y_{2t}$ [^3].

Quando $A_2 = 0$, podemos maximizar o log-likelihood separadamente para as duas equações [^3]. Primeiro, estimamos os parâmetros da equação para $y_{1t}$ restrita a $A_2 = 0$:
$$y_{1t} = c_1 + A_1 x_{1t} + \epsilon_{1t}$$
A matriz $\Pi$ é dada por $[c_1 \ A_1]$, e as estimativas de máxima verossimilhança para $c_1$ e $A_1$ podem ser obtidas por regressão OLS da equação acima [^3]. Se $\hat{\Pi}_1 = [c_1, A_1]$ representa as estimativas, e $\hat{\Omega}_{11}$ a matriz de covariância dos resíduos, temos a parte do log-likelihood associada a $y_{1t}$ é dada por [^3]:
$$l_1(\theta_1) = -\frac{Tn_1}{2} \log(2\pi) + \frac{T}{2} \log|\hat{\Omega}_{11}^{-1}| - \frac{Tn_1}{2}$$
onde $n_1$ é a dimensão de $y_{1t}$ e $\theta_1$ representa as estimativas para $c_1, A_1$, e $\Omega_{11}$.

Em um segundo passo, estimamos a equação para $y_{2t}$. Nesse caso, a média condicional é dada por [^3]:

$$E(y_{2t}|y_{1t}, x_{1t}, x_{2t}) = c_2 + B_1 x_{1t} + B_2 x_{2t} + \Omega_{21}\Omega_{11}^{-1}(y_{1t} - (c_1 + A_1 x_{1t}))$$

Definimos o resíduo da equação para $y_{1t}$ como:
$$ \hat{\epsilon}_{1t} = y_{1t} - \hat{c}_1 - \hat{A}_1 x_{1t}$$

Então, o modelo para $y_{2t}$ pode ser escrito como [^3]:
$$ y_{2t} = d + D_0 \hat{\epsilon}_{1t} + D_1 x_{1t} + D_2 x_{2t} + \nu_{2t}$$
onde:
$$d = c_2 - \Omega_{21}\Omega_{11}^{-1} c_1$$
$$D_0 = \Omega_{21}\Omega_{11}^{-1}$$
$$D_1 = B_1 - \Omega_{21}\Omega_{11}^{-1}A_1$$
$$D_2 = B_2$$

As estimativas de $d$, $D_0$, $D_1$, e $D_2$ podem ser obtidas por OLS desta equação. A matriz de covariância dos resíduos $\nu_{2t}$ é $\hat{H}$ e a parte do log-likelihood associada a $y_{2t}$ é dada por [^3]:
$$l_2(\theta_2) = -\frac{Tn_2}{2} \log(2\pi) + \frac{T}{2} \log|\hat{H}^{-1}| - \frac{Tn_2}{2}$$
onde $n_2$ é a dimensão de $y_{2t}$ e $\theta_2$ representa as estimativas para $d, D_0, D_1, D_2$ e $H$. O log-likelihood completo é a soma das duas partes $l(\theta) = l_1(\theta_1) + l_2(\theta_2)$. As estimativas para as matrizes originais $c_2, B_1, B_2, \Omega_{21}$ são recuperadas dos estimadores acima [^3].

É importante notar que os resíduos $\nu_{2t}$ são ortogonais aos regressores, e que são ortogonais aos resíduos estimados na equação para $y_{1t}$ [^3].
O teste de razão de verossimilhança (LR) é um teste para a hipótese de que $A_2 = 0$ [^3].

### Conclusão
Enquanto a estimação OLS oferece uma abordagem direta e computacionalmente eficiente para modelos VAR, a estimação por máxima verossimilhança (MLE) com restrições possibilita a incorporação de informações adicionais ao modelo, resultando em estimativas mais precisas quando essas restrições são válidas [^3]. A capacidade de lidar com restrições, como a block-exogeneidade, torna o MLE uma abordagem poderosa para analisar sistemas complexos de séries temporais [^3].

### Referências
[^1]: Trechos das páginas 292-293
[^2]: Trechos das páginas 293-297
[^3]: Trechos das páginas 309-311
<!-- END -->
