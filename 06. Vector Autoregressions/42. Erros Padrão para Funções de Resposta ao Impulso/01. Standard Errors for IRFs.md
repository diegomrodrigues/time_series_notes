## Erros Padrão para Funções de Resposta ao Impulso
### Introdução
Este capítulo aborda a computação de **erros padrão para funções de resposta ao impulso (IRFs)**, um tópico crucial na análise de modelos de Vetores Autorregressivos (VARs). Como vimos anteriormente, os IRFs descrevem a resposta de um sistema a um choque em uma de suas variáveis [^1]. No entanto, essas respostas são estimadas a partir de dados amostrais e, portanto, estão sujeitas a incerteza. Para quantificar essa incerteza, é necessário calcular os erros padrão dos IRFs. Este capítulo explora abordagens analíticas e numéricas para esse cálculo, complementando o estudo da estimação e testes de hipóteses em VARs já apresentado.

### Conceitos Fundamentais
Como discutido na Seção 11.4, a matriz de IRF em um atraso *s*, denotada por $\Psi_s$, é construída a partir do conhecimento dos coeficientes autorregressivos [^1]. Em situações práticas, esses coeficientes não são conhecidos com certeza, mas devem ser estimados por meio de regressões OLS [^1]. Ao utilizar os valores estimados desses coeficientes para calcular as funções de resposta ao impulso, é essencial reportar os erros padrão dessas estimativas [^1].

Para isso, adota-se a notação da Proposição 11.1, onde *k* = *np* + 1 denota o número de coeficientes em cada equação do VAR, e *n* = vec(Π) denota o vetor (nk x 1) de parâmetros para todas as equações [^1]. Os primeiros *k* elementos de π fornecem o termo constante e os coeficientes autorregressivos da primeira equação, os próximos *k* elementos fornecem os parâmetros da segunda equação e assim por diante. Seja  $\psi_s = \text{vec}(\Psi_s)$ denotando o vetor ($n^2$ x 1) dos coeficientes de média móvel associados ao atraso *s* [^1]. Os primeiros *n* elementos de  $\psi_s$ são dados pela primeira linha de $\Psi_s$ e identificam a resposta de $y_{1t+s}$ para $\varepsilon_t$, os *n* elementos seguintes por sua vez, correspondem a resposta de $y_{2t+s}$ para $\varepsilon_t$ e assim por diante. Dada os coeficientes autorregressivos em π, o VAR pode ser simulado para calcular $\psi_s$. Assim, $\psi_s$ pode ser vista como uma função não-linear de π, representada por  $\psi_s(\pi)$, onde $\psi_s: \mathbb{R}^{kn} \rightarrow \mathbb{R}^{n^2}$ [^1].

Com as estimativas OLS,  $\hat{\pi}$,  gera-se a estimativa 
$\hat{\psi}_s = \psi_s(\hat{\pi})$ [^1]. Com isso, sob as condições da Proposição 11.1, tem-se que  $\sqrt{T}(\hat{\pi} - \pi)$ converge em distribuição para $X$, onde $X \sim N(0, (\Omega \otimes Q^{-1}))$ [^1].

Os erros padrão para $\hat{\psi}_s$ podem ser calculados usando a Proposição 7.4:
$$
\sqrt{T}(\hat{\psi}_{s,T} - \psi_{s}) \xrightarrow{L} G_sX,
$$
onde
$$
G_s = \frac{\partial \psi_s(\pi)}{\partial \pi'}
$$
[11.7.2] e
$$
\sqrt{T}(\hat{\psi}_{s,T} - \psi_{s}) \xrightarrow{L} N(0, G_s(\Omega \otimes Q^{-1})G_s').
$$
[11.7.3]

Os erros padrão para um coeficiente de resposta ao impulso estimado são dados pela raiz quadrada do elemento diagonal associado de $(1/T)G_s(\Omega \otimes Q^{-1})G_s'$, onde
$$
Q_T = (1/T) \sum_{t=1}^{T} x_t x_t'
$$
e *x* e Ω são definidos na Proposição 11.1 [^1].

Para aplicar esse resultado, é preciso obter uma expressão para a matriz $G_s$ em [11.7.2] [^1]. O Apêndice 11.B estabelece que a sequência $\{G_s\}_{s=1}^r$ pode ser calculada iterativamente por:
$$
G_s = [I_n, (0_n' \otimes I_n) \Psi_{s-1}' , \Psi_{s-2}',..., \Psi_{s-p}'] + \sum_{i=1}^{p} (\Phi_i \otimes I_n)G_{s-i}
$$
[11.7.4]
Aqui $0_n$ denota um vetor de zeros de tamanho (n x 1). A iteração é inicializada com $G_{-1} = G_{-2} = \ldots = G_{-p+1} = 0_n$, entende-se que $\Psi_0 = I_n$ e $\Psi_s = 0_n$ para s < 0 [^1].

Uma solução de forma fechada para [11.7.4] é dada por:
$$
G_s = \sum_{i=1}^{s} [\Psi_{s-i} \otimes (0_n' , \Psi_1', \Psi_2',..., \Psi_{p-i+1}')]
$$
[11.7.5]

#### Abordagens Alternativas para o Cálculo de Erros Padrão

A matriz de derivadas $G_s$ pode ser calculada numericamente como segue [^1]. Primeiramente, utiliza-se estimativas de OLS,  $\hat{\pi}$, para calcular $\hat{\psi}_s(\hat{\pi})$ para s = 1, 2, ..., m. Em seguida, aumenta-se o valor do i-ésimo elemento de $\hat{\pi}$ por uma pequena quantidade Δ, mantendo os outros elementos constantes, e avalia-se $\psi_s(\hat{\pi} + e_i\Delta)$ para s = 1, 2, ..., m, onde $e_i$ denota a i-ésima coluna de $I_{nk}$. Então o vetor ($n^2$ x 1)
$$
\frac{\psi_s(\hat{\pi} + e_i\Delta) - \psi_s(\hat{\pi})}{\Delta}
$$
dá uma estimativa da i-ésima coluna de $G_s$. Realizando avaliações separadas da sequência $\psi_s(\hat{\pi} + e_i\Delta)$ para cada *i* = 1, 2, ..., *nk*, todas as colunas de $G_s$ podem ser computadas.

Métodos de Monte Carlo também podem ser usados para inferir a distribuição de $\hat{\psi}_s(\hat{\pi})$ [^1]. Aqui, um vetor (nk x 1) seria gerado aleatoriamente a partir de uma distribuição $N(\pi, (1/T)(\Omega \otimes Q^{-1}))$ [^1]. Este vetor, denotado por $\pi^{(1)}$, seria utilizado para calcular $\psi_s(\pi^{(1)})$.  Um segundo vetor, $\pi^{(2)}$, seria então gerado a partir da mesma distribuição e utilizado para computar  $\psi_s(\pi^{(2)})$. O processo se repete por 10.000 simulações separadas. Se 9500 dessas simulações resultarem em um valor do primeiro elemento de $\psi_s$ entre  $\psi_{s,i}$ e  $\psi_{s,u}$ então  $[\psi_{s,i},  \psi_{s,u}]$ pode ser usado como um intervalo de confiança de 95% para o primeiro elemento de $\psi_s$.

Runkle (1987) empregou uma abordagem relacionada baseada em *bootstrapping*, com o objetivo de obter uma estimativa da distribuição de amostra pequena de $\hat{\pi}$ sem assumir que as inovações $\varepsilon_t$ são gaussianas [^1]. Para implementar esse procedimento, estima-se o VAR, salvando as estimativas de coeficientes  $\hat{\pi}$, e os resíduos ajustados $\{\hat{\varepsilon}_1, \hat{\varepsilon}_2, \ldots, \hat{\varepsilon}_T\}$. Em seguida, considera-se uma variável aleatória artificial *u* que tem probabilidade (1/T) de assumir cada um dos valores $\{\hat{\varepsilon}_1, \hat{\varepsilon}_2, \ldots, \hat{\varepsilon}_T\}$ [^1].

### Conclusão
Este capítulo detalhou os procedimentos para quantificar a incerteza associada às funções de resposta ao impulso (IRFs) em modelos VARs, tanto por meio de abordagens analíticas quanto numéricas. As abordagens analíticas envolvem o cálculo de derivadas, enquanto as abordagens numéricas envolvem simulações de Monte Carlo ou *bootstrapping*. Este material complementa o conhecimento prévio sobre estimação e testes de hipóteses em modelos VARs, proporcionando uma compreensão mais profunda das ferramentas necessárias para a análise de séries temporais multivariadas e preparando o terreno para o capítulo que se segue, que abordará como a escolha de uma determinada ordenação das variáveis afeta as conclusões sobre causalidade e dinâmica.

### Referências
[^1]:  _Trechos relevantes do texto fornecido._
<!-- END -->
