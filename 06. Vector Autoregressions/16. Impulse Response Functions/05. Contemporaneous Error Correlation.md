## Correlação Contemporânea nos Choques em Modelos VAR e a Matriz de Covariância $\Omega$

### Introdução
Este capítulo aprofunda a análise das funções de resposta a impulso (IRFs) em modelos Vetor Autorregressivos (VAR), focando especificamente nas implicações da correlação contemporânea entre os choques ou inovações do sistema. Como vimos anteriormente, a matriz $\Psi_s$ representa a resposta do sistema a um choque em um período $t$, mas a suposição de que esses choques são ortogonais é uma simplificação que nem sempre é válida. Na prática, as inovações em modelos VAR frequentemente exibem correlação contemporânea, o que significa que um choque em uma variável tem um efeito não apenas sobre ela mesma, mas também sobre outras variáveis no mesmo período [^11.4.1]. Este capítulo visa explorar a natureza desta correlação através da análise da matriz de covariância $\Omega$ das inovações, e como essa informação é fundamental para interpretar as respostas a impulso.

### A Natureza da Correlação Contemporânea

Em um modelo VAR, a representação geral é dada por [^11.1.1]:

$$ y_t = c + \Phi_1y_{t-1} + \Phi_2y_{t-2} + \ldots + \Phi_py_{t-p} + \epsilon_t $$

onde $y_t$ é um vetor de *n* variáveis endógenas, $c$ é um vetor constante, $\Phi_i$ são as matrizes de coeficientes autorregressivos, e $\epsilon_t$ é um vetor de inovações ou choques. A matriz de covariância contemporânea dessas inovações, $\Omega$, é definida como:

$$ \Omega = E[\epsilon_t\epsilon_t'] $$

Em geral, $\Omega$ não é uma matriz diagonal, o que indica que as inovações não são ortogonais. Isso significa que $\epsilon_{i,t}$ (o choque na variável $i$ no período $t$) é correlacionada com $\epsilon_{j,t}$ (o choque na variável $j$ no mesmo período $t$).

**Implicações da Correlação Contemporânea**

A existência da correlação contemporânea tem implicações importantes na interpretação das funções de resposta a impulso:

1.  **Choques Não-Isolados:** Quando as inovações são correlacionadas, um choque em uma variável não é um evento isolado. Ele se propaga contemporaneamente para outras variáveis, mesmo que não haja uma relação direta de causa e efeito.

2. **Dificuldade na Interpretação:** Se não levarmos a correlação em conta, a análise das IRFs torna-se ambígua, já que um choque observado na variável $i$ pode ser contemporaneamente confundido com o efeito de um choque na variável $j$.

3.  **Necessidade de Ortogonalização:** Para analisar as respostas a impulso como o efeito de choques isolados, é necessária uma transformação das inovações originais em um conjunto de choques não correlacionados (ortogonalizados) [^11.4.2].

### A Matriz de Covariância $\Omega$ e suas Propriedades

A matriz $\Omega$ desempenha um papel fundamental na caracterização da correlação contemporânea. As suas propriedades são:

1.  **Simetria:** $\Omega$ é uma matriz simétrica, ou seja, o elemento da i-ésima linha e j-ésima coluna é igual ao elemento da j-ésima linha e i-ésima coluna: $\Omega_{ij} = \Omega_{ji}$.

2.  **Elementos Diagonais:** Os elementos da diagonal principal, $\Omega_{ii}$, representam as variâncias dos choques nas variáveis individuais ($\epsilon_i$).

3.  **Elementos Fora da Diagonal:** Os elementos fora da diagonal, $\Omega_{ij}$, representam as covariâncias entre os choques nas variáveis $i$ e $j$ ($\epsilon_i$ e $\epsilon_j$).

4.  **Definida Positiva:** $\Omega$ é uma matriz definida positiva, o que significa que todos os seus autovalores são positivos. Isso garante que a variância de qualquer combinação linear das inovações seja positiva.

**Implicações da Matriz $\Omega$**
A matriz $\Omega$ fornece informações sobre como diferentes choques são correlacionados entre si no tempo *t*. Se $\Omega_{ij}$ é positiva, um choque na variável *i* é associado a um choque de mesmo sinal na variável *j*. Se $\Omega_{ij}$ é negativa, os choques nas variáveis *i* e *j* têm sinais contrários. Se $\Omega_{ij}$ é zero, os choques são não correlacionados. O conhecimento da estrutura das correlações contemporâneas é fundamental para que possamos entender a dinâmica do nosso modelo VAR.

### Técnicas de Ortogonalização

Para lidar com a correlação contemporânea, é comum empregar técnicas de ortogonalização que geram um conjunto de inovações transformadas, $u_t$, que são não correlacionadas e que representam choques únicos e isolados. As duas principais técnicas de ortogonalização são:

1.  **Decomposição de Cholesky:** Essa técnica decompõe a matriz $\Omega$ em um produto de uma matriz triangular inferior $A$ e sua transposta, $\Omega = AA'$. Os choques ortogonalizados são dados por:

    $$ u_t = A^{-1}\epsilon_t $$
   
    Essa abordagem impõe uma estrutura recursiva ao modelo, onde as variáveis que aparecem primeiro na ordenação afetam as variáveis que vêm depois, e as variáveis que vêm depois não afetam as que vieram antes. A escolha da ordenação afeta a interpretação, e ela deve ser baseada em uma teoria que a justifique.

2.  **Outras Transformações:** Existem outras maneiras de ortogonalizar os choques que não impõem a estrutura recursiva. Estas transformações podem ter uma interpretação econômica, ou podem ser puramente estatísticas. Alguns exemplos são: a decomposição espectral e a decomposição de valores singulares.

### Impacto da Ortogonalização nas IRFs

A ortogonalização transforma a matriz de inovações, e as funções de resposta a impulso serão agora calculadas usando essas inovações transformadas. As IRFs ortogonalizadas,  resultantes do uso de uma transformação linear $u_t = A^{-1}\epsilon_t$,  são definidas como:

$$ \frac{\partial y_{t+s}}{\partial u_{t}} = \Psi_s A $$
e representam a resposta do sistema a choques não correlacionados.

### Interpretação das IRFs com Ortogonalização
1. Choques Estruturais: o uso da ortogonalização transforma os choques de forma que agora eles possam ter uma interpretação econômica mais forte. Por exemplo, ao ortogonalizar a oferta e a demanda, podemos analisar o efeito de choques de cada uma dessas forças no sistema.
2. Comparação de Modelos: as IRFs ortogonalizadas dependem da transformação linear aplicada, e diferentes transformações levam a diferentes respostas. O uso de diferentes ortogonalizações deve se basear em hipóteses específicas sobre o sistema em estudo. A escolha entre elas depende da teoria por trás do modelo e das relações que são objeto de análise.
3.  A escolha da ortogonalização afeta os resultados. Deve-se apresentar as respostas a impulso para diferentes ordenações das variáveis, quando a decomposição de Cholesky for utilizada, para avaliar a sensibilidade dos resultados.

### Conclusão
A correlação contemporânea entre os choques em modelos VAR é uma característica fundamental que precisa ser levada em consideração para uma interpretação correta das funções de resposta a impulso. A matriz de covariância $\Omega$ quantifica essa correlação, fornecendo informações sobre a relação entre os choques do sistema, e as técnicas de ortogonalização oferecem uma maneira de isolar e interpretar os efeitos dos choques. A escolha do método de ortogonalização, bem como a interpretação das funções de resposta a impulso, devem ser feitas com cuidado e justificadas teoricamente para que as análises possam ser consideradas sólidas e confiáveis.

### Referências
[^11.4.1]: Hamilton, James D. (1994). *Time Series Analysis*. Princeton University Press.
[^11.1.1]: Hamilton, James D. (1994). *Time Series Analysis*. Princeton University Press.
[^11.4.2]: Lütkepohl, Helmut. (2005). *New Introduction to Multiple Time Series Analysis*. Springer.
## 11.8. Exercises

This section provides exercises to reinforce the concepts presented in this chapter. These exercises range from basic calculations to more involved analytical derivations, designed to solidify your understanding of vector autoregressions and related techniques.

**Exercise 11.1:**
Consider a bivariate VAR(1) model given by:
$$
\begin{bmatrix} y_{1t} \\ y_{2t} \end{bmatrix} = \begin{bmatrix} 0.5 & 0.2 \\ 0.1 & 0.7 \end{bmatrix} \begin{bmatrix} y_{1t-1} \\ y_{2t-1} \end{bmatrix} + \begin{bmatrix} \epsilon_{1t} \\ \epsilon_{2t} \end{bmatrix}
$$
where the innovations are distributed as:
$$
\begin{bmatrix} \epsilon_{1t} \\ \epsilon_{2t} \end{bmatrix} \sim N \left( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 & 0.5 \\ 0.5 & 2 \end{bmatrix} \right)
$$
(a) Calculate the first three impulse response matrices, $\Psi_0$, $\Psi_1$, and $\Psi_2$.
(b) Compute the variance-covariance matrix of the two-step ahead forecast error.
(c) Using Cholesky decomposition, compute the orthogonalized impulse-response matrices for the first two periods.
(d) Comment on the differences between the non-orthogonalized and orthogonalized responses.

**Exercise 11.2:**
Given the following VAR(2) model:
$$
\begin{bmatrix} y_{1t} \\ y_{2t} \end{bmatrix} = \begin{bmatrix} 0.4 & 0.1 \\ -0.2 & 0.3 \end{bmatrix} \begin{bmatrix} y_{1t-1} \\ y_{2t-1} \end{bmatrix} + \begin{bmatrix} 0.2 & -0.1 \\ 0.1 & 0.2 \end{bmatrix} \begin{bmatrix} y_{1t-2} \\ y_{2t-2} \end{bmatrix} + \begin{bmatrix} \epsilon_{1t} \\ \epsilon_{2t} \end{bmatrix}
$$
with  $E(\epsilon_t \epsilon_t') = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$,  investigate the Granger causality between $y_{1t}$ and $y_{2t}$. Formulate the null hypothesis and conduct the F-test.

**Exercise 11.3:**
Suppose we have a three-variable VAR(1) model:
$$
\begin{bmatrix} y_{1t} \\ y_{2t} \\ y_{3t} \end{bmatrix} = \begin{bmatrix} a_{11} & a_{12} & 0 \\ a_{21} & a_{22} & a_{23} \\ 0 & a_{32} & a_{33} \end{bmatrix} \begin{bmatrix} y_{1t-1} \\ y_{2t-1} \\ y_{3t-1} \end{bmatrix} + \begin{bmatrix} \epsilon_{1t} \\ \epsilon_{2t} \\ \epsilon_{3t} \end{bmatrix}
$$
where $E(\epsilon_t \epsilon_t') = \Omega$. Discuss the implications of the imposed restrictions for
(a) block exogeneity of $y_1$ with respect to $y_2$ and $y_3$
(b) instantaneous causality between $y_2$ and $y_3$.

**Exercise 11.4:**
Consider a VAR model with two variables, $y_{1t}$ and $y_{2t}$. Suppose you estimate a VAR(2) model and want to test the hypothesis that the second lag of $y_2$ does not affect the first variable $y_{1t}$. Formally specify the null hypothesis, describe how to conduct a likelihood ratio test, and describe the asymptotic distribution of the test statistic.

**Exercise 11.5:**
Using the data described in exercise 11.2, suppose you wish to test the joint hypothesis that the second lags of both variables are not significant in any of the equations. Describe the necessary steps to perform the Wald test.

**Exercise 11.6:**
Derive the expressions for $\frac{\partial \Psi_s}{\partial \pi}$ for $s=1$ and $s=2$ for a VAR(1) process. Use the method outlined in [11.7.4].

**Exercise 11.7:**
Discuss the advantages and disadvantages of using orthogonalized versus non-orthogonalized impulse-response functions.

**Exercise 11.8:**
In the context of structural VAR, elaborate on the order and rank conditions for identification. Use the model described in [11.6.24] - [11.6.26] as a demonstration.

**Exercise 11.9:**
Compare and contrast the use of VAR models for (a) forecasting, (b) structural analysis. Provide specific examples of how the techniques could be applied.

**Exercise 11.10:**
Describe the basic steps of a Monte Carlo simulation to obtain standard errors of impulse response functions for a given VAR process.

## 11.9 Conclusion

This chapter has provided a comprehensive overview of Vector Autoregressions, a powerful tool for modeling and analyzing multivariate time series data. We have explored the theoretical foundations of VAR models, including their representation, estimation, and testing procedures. We have also examined the practical applications of VARs in various settings, such as forecasting, impulse response analysis, and structural identification. Furthermore, we discussed common pitfalls and the importance of careful model selection and validation. The exercises provided offer an opportunity to apply these techniques and deepen understanding.

This concludes our exploration of vector autoregressions, providing a strong foundation for further studies in time series econometrics and dynamic modeling. We encourage you to engage further with the provided references and to explore the wider academic literature to expand your knowledge and competence in this area.

### References
[^11.1.1]: Hamilton, James D. (1994). *Time Series Analysis*. Princeton University Press.
[^11.4.2]: Lütkepohl, Helmut. (2005). *New Introduction to Multiple Time Series Analysis*. Springer.
<!-- END -->
