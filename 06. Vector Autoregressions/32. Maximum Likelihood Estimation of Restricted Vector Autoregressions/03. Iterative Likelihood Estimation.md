## Estimação Iterativa de Máxima Verossimilhança em Modelos VAR com Restrições Gerais
### Introdução
Este capítulo se dedica à estimação de máxima verossimilhança (MLE) para modelos de Vetores Autorregressivos (VAR) sob restrições mais gerais, explorando um procedimento iterativo que envolve regressões OLS e a matriz de variância-covariância dos erros. Em contraste com os capítulos anteriores, que trataram de modelos irrestritos e com restrições de exogeneidade de bloco, aqui abordamos sistemas VAR que não se encaixam em estruturas bloco-recursivas [^1, ^2]. O foco é detalhar um método prático para a obtenção de estimativas em sistemas complexos e restritos, combinando a simplicidade das regressões OLS com ajustes iterativos para garantir a convergência para os estimadores de máxima verossimilhança [^2].

### Conceitos Fundamentais
Como nos casos anteriores, o objetivo é estimar modelos VAR com restrições além da exogeneidade de bloco, em que os parâmetros não podem ser estimados diretamente via regressões OLS separadas. Suponha um sistema de equações VAR, com um vetor $y_t$, escrito da forma:
$$ y_t = \mathcal{X}_t \beta + \epsilon_t $$
onde $\mathcal{X}_t$ é uma matriz de variáveis explicativas, $\beta$ é o vetor de coeficientes e $\epsilon_t$ é o vetor de erros [^1, 11.3.28]. As restrições podem ser quaisquer restrições sobre os coeficientes, $\beta$, que não podem ser expressas de forma recursiva em blocos. A função de log-verossimilhança pode ser escrita como:
$$ \mathcal{L}(\beta, \Omega) = -(Tn/2) \log(2\pi) + (T/2) \log|\Omega^{-1}| - \frac{1}{2}\sum_{t=1}^T (y_t - \mathcal{X}_t \beta)'\Omega^{-1}(y_t - \mathcal{X}_t \beta) $$ [11.3.29]
onde $n$ é a dimensão de $y_t$, $T$ o tamanho da amostra e $\Omega$ a matriz de variância-covariância dos erros. A maximização desta função de verossimilhança com respeito a $\beta$ e $\Omega$ é feita através da minimização de
$$ \sum_{t=1}^T (y_t - \mathcal{X}_t \beta)'\Omega^{-1}(y_t - \mathcal{X}_t \beta) $$ [11.3.30]
quando o termo em $\log|\Omega^{-1}|$ não for considerado.

O método aqui introduzido, requer uma transformação dos dados. Primeiro, decompomos $\Omega^{-1}$ em $L'L$, onde $L$ é uma matriz triangular inferior, e definimos $\tilde{y}_t = L y_t$ e $\tilde{\mathcal{X}}_t = L \mathcal{X}_t$ [^1]. Com essa transformação, o problema de minimização se torna:
$$ \sum_{t=1}^T (\tilde{y}_t - \tilde{\mathcal{X}}_t \beta)'(\tilde{y}_t - \tilde{\mathcal{X}}_t \beta) $$ [11.3.31]
que é um problema de regressão OLS onde o vetor de coeficientes $\beta$ pode ser estimado por:
$$ \hat{\beta} = \left(\sum_{t=1}^T \tilde{\mathcal{X}}_t' \tilde{\mathcal{X}}_t\right)^{-1} \sum_{t=1}^T \tilde{\mathcal{X}}_t'\tilde{y}_t $$
Esta é uma regressão OLS em um conjunto de variáveis transformadas [^1, 11.3.32].
No entanto, para construir as variáveis $\tilde{y}_t$ e $\tilde{\mathcal{X}}_t$ é necessário conhecer a matriz $L$ e, por consequência, a matriz $\Omega$. Como $\Omega$ é também desconhecido, a solução envolve um processo iterativo, que é resumido nas seguintes etapas:

1. **Inicialização:** Começamos com uma estimativa inicial dos coeficientes $\beta$, que pode ser obtida por regressões OLS separadas de cada equação, assumindo que as variáveis são ortogonais [^1, 11.3.32].
2. **Estimação da matriz de variância-covariância:** Calculamos a matriz de variância-covariância dos resíduos utilizando a estimativa inicial de $\beta$:
$$ \hat{\Omega}^{(0)} = \frac{1}{T} \sum_{t=1}^T [y_t - \mathcal{X}_t \hat{\beta}^{(0)}][y_t - \mathcal{X}_t \hat{\beta}^{(0)}]' $$ [11.3.32].
3. **Transformação dos dados:** Encontramos uma matriz $L^{(0)}$ tal que $(L^{(0)})'L^{(0)} = [\hat{\Omega}^{(0)}]^{-1}$, geralmente por fatoração de Cholesky [^1]. Usamos $L^{(0)}$ para transformar os dados como:
$$\tilde{y}_t^{(0)} = L^{(0)}y_t \quad \text{e} \quad \tilde{\mathcal{X}}_t^{(0)} = L^{(0)} \mathcal{X}_t$$
4. **Nova estimativa dos coeficientes:** Rodamos uma regressão OLS das variáveis transformadas para encontrar uma nova estimativa para os coeficientes $\beta$:
$$ \hat{\beta}^{(1)} = \left(\sum_{t=1}^T \tilde{\mathcal{X}}_t^{(0)'} \tilde{\mathcal{X}}_t^{(0)}\right)^{-1} \sum_{t=1}^T \tilde{\mathcal{X}}_t^{(0)'}\tilde{y}_t^{(0)} $$ [11.3.32].
5. **Iteração:** Repetimos os passos 2 a 4 até que a estimativa de $\beta$ convirja. Ou seja, até que a diferença entre $\hat{\beta}^{(k)}$ e $\hat{\beta}^{(k-1)}$ seja menor do que um dado limite.

Este procedimento iterativo converge para os estimadores de máxima verossimilhança, ainda que o estimador na primeira iteração já possua distribuição assintótica similar ao estimador final [^1].

### Conclusão
Este capítulo descreveu um método iterativo para estimação de máxima verossimilhança em modelos VAR sujeitos a restrições gerais que não admitem um procedimento OLS simples, como no caso de exogeneidade de bloco. O método envolve a transformação dos dados utilizando a matriz de variância-covariância dos erros, e a aplicação de regressões OLS iterativamente até que os estimadores convirjam. Este processo demonstra como modelos VAR mais complexos podem ser estimados com o uso de recursos computacionais e um bom entendimento das propriedades das regressões OLS e dos estimadores de máxima verossimilhança [^1, ^2]. Além disso, evidencia a relação entre os estimadores OLS e MLE, como a forma iterativa de aplicar OLS para encontrar os parâmetros.
### Referências
[^1]: *Trechos relevantes do texto fornecido.*
[^2]: *Trechos relevantes do contexto anterior.*
<!-- END -->
