## Erros Padrão para Funções de Resposta ao Impulso Não Ortogonalizadas: Uma Abordagem Detalhada

### Introdução

Em continuidade à discussão sobre a análise de modelos de Vetores Auto-regressivos (VAR), este capítulo se dedica a detalhar o cálculo dos **erros padrão das funções de resposta ao impulso (IRFs)**, um aspecto crucial para a inferência estatística em modelos VAR [^1, ^2]. Como vimos anteriormente [^1, ^2], as IRFs descrevem a resposta de uma variável em um sistema VAR a um choque (inovação) em outra variável ao longo do tempo. No entanto, as estimativas das IRFs são baseadas em estimativas dos coeficientes do VAR que estão sujeitas a incertezas. Portanto, é fundamental quantificar essa incerteza por meio do cálculo dos erros padrão e construir intervalos de confiança para as IRFs. Neste capítulo, detalharemos o cálculo dos erros padrão para IRFs não ortogonalizadas, focando na abordagem das derivadas analíticas, e discutindo alternativas computacionais como derivadas numéricas, métodos de Monte Carlo e *bootstrapping*.

### Derivação Analítica dos Erros Padrão para IRFs Não Ortogonalizadas

A Seção 11.4 introduziu a construção das matrizes de resposta ao impulso $\Psi_s$ a partir do conhecimento dos coeficientes auto-regressivos [^1]. Na prática, esses coeficientes não são conhecidos, mas sim *estimados* via regressões OLS, o que introduz incerteza nas estimativas das IRFs. Assim, é essencial quantificar essa incerteza. Os erros padrão podem ser obtidos usando a abordagem de *derivadas analíticas*, que explora a relação entre a distribuição assintótica dos coeficientes estimados do VAR e a distribuição assintótica das IRFs [^2].

Começamos com a notação da Proposição 11.1 [^2], definindo $k = np+1$ como o número de coeficientes em cada equação do VAR e $\pi = vec(\Pi)$ como o vetor $(nk \times 1)$ de todos os coeficientes. Os primeiros $k$ elementos de $\pi$ representam o termo constante e os coeficientes auto-regressivos da primeira equação, e assim por diante. Definindo $\psi_s = vec(\Psi_s)$ como o vetor $(n^2 \times 1)$ dos coeficientes da média móvel associados ao lag $s$, este vetor é uma função não linear de $\pi$, denotada por $\psi_s(\pi)$, onde $\psi_s: R^{nk} \rightarrow R^{n^2}$ [^2]. Em termos matemáticos, essa função $\psi_s$ representa uma transformação do espaço de coeficientes VAR para o espaço das IRFs.

As IRFs são estimadas substituindo-se $\pi$ pelas estimativas OLS $\hat{\pi}$, gerando a estimativa $\hat{\psi}_s = \psi_s(\hat{\pi})$ [^2]. Pela Proposição 11.1, temos que $\sqrt{T}(\hat{\pi} - \pi) \xrightarrow{L} X$, onde $X \sim N(0, (\Omega \otimes Q^{-1}))$ [^2]. Esta é uma propriedade chave, pois estabelece que a distribuição assintótica do vetor de parâmetros estimados do VAR é normal, com uma média igual ao verdadeiro valor do parâmetro e matriz de variância-covariância dada por $\Omega \otimes Q^{-1}$. Os erros padrão de $\psi_s$ podem ser calculados aplicando a Proposição 7.4 [^2], que usa a regra da cadeia para aproximar a distribuição de funções de estimadores assintoticamente normais:
$$ \sqrt{T}(\hat{\psi}_{s,T} - \psi_s) \xrightarrow{L} G_s X $$
onde
$$ G_s = \left.\frac{\partial \psi_s(\pi)}{\partial \pi'}\right|_{\pi=\hat{\pi}} $$
[11.7.2]
e
$$ \sqrt{T}(\hat{\psi}_{s,T} - \psi_s) \xrightarrow{L} N(0, G_s(\Omega \otimes Q^{-1})G_s') $$
[11.7.3]
Esta expressão é fundamental, pois ela estabelece que a distribuição assintótica do vetor de IRFs também é normal, com uma matriz de variância-covariância dada por $G_s(\Omega \otimes Q^{-1})G_s'$. O erro padrão de um coeficiente específico de resposta ao impulso é dado pela raiz quadrada do elemento diagonal correspondente de $ (1/T) G_s(\Omega \otimes Q^{-1})G_s' $ [^2].

Para aplicar esses resultados, precisamos de uma expressão para a matriz $G_s$ em [11.7.2]. O Apêndice 11.B apresenta que a sequência $\{G_s\}_{s=1}^{\infty}$ pode ser calculada iterativamente através de:
$$ G_s = [I_n, (0_n' \otimes \Psi_{s-1}), \ldots, (0_n' \otimes \Psi_{s-p})] + \sum_{i=1}^p(\Phi_i \otimes I_n)G_{s-i} $$
[11.7.4]
onde $0_n$ denota um vetor $(n \times 1)$ de zeros, $\Psi_i$ é a matriz de resposta ao impulso do lag $i$, e $\Phi_i$ são os coeficientes do VAR no lag $i$. A iteração é inicializada com $G_0 = I_n$, $G_{-1} = G_{-2} = \ldots = G_{-p+1} = 0$. Note que $\Psi_0 = I_n$ e $\Psi_s = 0$ para $s < 0$. A expressão em [11.7.4] mostra como as matrizes de derivadas $G_s$ são construídas iterativamente, a partir das matrizes de resposta ao impulso em lags anteriores e dos coeficientes do modelo VAR.
Uma solução de forma fechada para [11.7.4] é dada por:
$$ G_s = \sum_{i=1}^s \left[\Psi_{s-i} (0_n' \otimes I_n) \ldots (0_n' \otimes \Psi_{s-i-p+1}) \right] $$
[11.7.5]
Essa expressão fornece uma maneira mais direta de calcular a matriz $G_s$ para qualquer lag $s$ e evita a necessidade de realizar a iteração em [11.7.4]. A solução em forma fechada, [11.7.5], é particularmente útil para implementar cálculos computacionais.

### Abordagens Alternativas para Cálculo dos Erros Padrão para IRFs não Ortogonalizadas

Embora a abordagem das derivadas analíticas seja crucial para a compreensão teórica e a inferência assintótica dos erros padrão das IRFs, ela pode ser difícil de implementar devido à complexidade das expressões envolvidas. Em tais casos, abordagens alternativas são frequentemente usadas [^2]:

1.  **Derivadas Numéricas:** Como alternativa à abordagem analítica, a matriz de derivadas $G_s$ pode ser obtida por *derivação numérica* [^2]. Este método evita a necessidade de calcular explicitamente as derivadas analíticas, que podem ser complexas ou intratáveis analiticamente. Para implementar este método, primeiro, obtêm-se as estimativas OLS $\hat{\psi}_s(\pi)$ para $s = 1, 2, \ldots, m$, onde $m$ define o número máximo de lags considerados para calcular as IRFs. Em seguida, para calcular uma aproximação numérica da matriz de derivadas em relação ao $i$-ésimo elemento de $\pi$, denotado por $\pi_i$, incrementa-se este elemento por um pequeno valor $\Delta$, mantendo todos os outros elementos constantes, formando um novo vetor $\pi + e_i\Delta$, onde $e_i$ é um vetor de zeros com um '1' na posição $i$ e $\Delta$ é um valor pequeno (por exemplo, $\Delta = 0.001$). Avalia-se $\hat{\psi}_s(\pi + e_i \Delta)$ para $s = 1, 2, \ldots, m$. A derivada numérica para a $i$-ésima coluna de $G_s$ é aproximada por:
$$ \frac{\hat{\psi}_s(\pi + e_i \Delta) - \hat{\psi}_s(\pi)}{\Delta} $$
     O resultado dessa operação é um vetor com $n^2$ componentes. Repetindo esse processo para cada $i = 1, 2, \ldots, nk$, obtém-se a matriz de derivadas aproximada $G_s$ com dimensões $n^2 \times nk$ [^2]. Este método de derivadas numéricas, embora forneça apenas uma aproximação, pode ser computacionalmente mais rápida do que a abordagem analítica e evitar a complexidade das derivadas analíticas. A escolha de um valor adequado para $\Delta$ é importante, pois valores muito pequenos podem levar a erros de arredondamento, enquanto valores muito grandes podem levar a uma aproximação grosseira da derivada. Na prática, é comum usar valores de $\Delta$ próximos de 0.001.
2.  **Métodos de Monte Carlo:** Métodos de Monte Carlo podem ser usados para obter uma estimativa da distribuição de $\hat{\psi}_s(\pi)$ [^2]. Para isso, simula-se a distribuição assintótica dos parâmetros do VAR, gerando aleatoriamente um vetor $(nk \times 1)$ a partir de uma distribuição normal com média $\hat{\pi}$ e matriz de variância-covariância $(1/T)(\Omega \otimes Q^{-1})$, denotando esse vetor por $\pi^{(1)}$ e calcula-se $\hat{\psi}_s(\pi^{(1)})$. Em seguida, repete-se o processo com vetores $\pi^{(2)}, \pi^{(3)}, \ldots, \pi^{(N)}$ independentes, calculando $\hat{\psi}_s(\pi^{(i)})$ para cada um deles. Os valores de $\hat{\psi}_s(\pi^{(i)})$ podem então ser usados para construir intervalos de confiança e outros testes de hipóteses. É importante salientar que a implementação eficiente de métodos de Monte Carlo frequentemente envolve paralelização, pois o método exige um grande número de simulações (tipicamente, $N \geq 1000$) para obter uma distribuição de $\hat{\psi}_s(\pi)$ precisa [^2].
3.  **Bootstrapping:** Runkle (1987) propôs o uso de *bootstrapping* como uma alternativa robusta para a derivação analítica e os métodos de Monte Carlo, particularmente quando as suposições de normalidade podem não ser válidas [^2].  O método de *bootstrap* é uma técnica de reamostragem que permite aproximar a distribuição amostral de um estimador sem fazer suposições sobre a distribuição da população subjacente. No contexto da estimação dos erros padrão das IRFs, o procedimento de bootstrap envolve as seguintes etapas:
      1.  Estima-se o modelo VAR usando a amostra de dados observada.
      2.  Salvam-se os resíduos estimados $\hat{\epsilon}_1, \ldots, \hat{\epsilon}_T$.
      3.  Gera-se uma variável aleatória artificial $u_t$ que toma os valores $\hat{\epsilon}_1, \ldots, \hat{\epsilon}_T$ com probabilidade $(1/T)$ cada, realizando uma amostragem com reposição.
      4.  Com essa amostra artificial de resíduos $u_t$,  gera-se um conjunto de dados simulado através da estrutura do modelo VAR, utilizando os valores observados das variáveis para inicializar o processo.
      5.  Estima-se um novo modelo VAR com essa amostra simulada, gerando uma estimativa $\hat{\psi}_s^{(1)}$ das IRFs.
      6. Repete-se os passos de 2 a 5 $N$ vezes para obter um conjunto de estimativas  $\hat{\psi}_s^{(i)}$ para $i = 1, \ldots, N$, onde $N$ é um número grande (tipicamente, $N \geq 1000$). Essas estimativas são usadas para construir intervalos de confiança por meio de métodos não-paramétricos. O procedimento de bootstrap permite obter uma estimativa da distribuição de $\hat{\psi}_s(\pi)$ sob condições mais gerais do que as que são assumidas na abordagem das derivadas analíticas.
    Para implementar o bootstrap de maneira eficiente, é fundamental realizar a randomização e o paralelismo, em particular para grandes conjuntos de dados ou para modelos VAR complexos. A randomização permite a geração de várias amostras aleatórias independentes dos resíduos. O paralelismo, por sua vez, permite que as estimações do VAR e o cálculo das IRFs sejam feitos simultaneamente. Essas estratégias reduzem o tempo de processamento, tornando o método de *bootstrap* uma opção viável na prática [^2].

### Erros Padrão para as Funções de Resposta ao Impulso Ortogonalizadas

Para calcular os erros padrão das IRFs *ortogonalizadas* [^2], introduzimos a seguinte notação:
$$ H_s = \Psi_s B_0 $$
[11.7.12]
onde $H_s$ é uma matriz $(n \times n)$ cuja entrada na linha $i$, coluna $j$ representa o efeito do choque estrutural $u_t$ na variável $i$ em $t+s$ [^2]. Cada linha representa uma função de resposta ao impulso (IRF). Ao coletar essas entradas em um vetor $(n^2 \times 1)$, obtemos $h_s = vec(H_s)$. Os erros padrão de $h_s$ podem ser derivados como:
$$ \sqrt{T}(h_{s,T} - h_s) \xrightarrow{L} N(0, \Xi [\Omega \otimes Q^{-1}] \Xi' + 2D_n (\Omega \otimes \Omega) (D_n)') $$
[11.7.13]

onde $\Xi_{\pi} = \partial h_s / \partial \pi' = [I_n (B_0')']G_s$ e $\Xi_{\Omega} = \partial h_s / \partial vec(\Omega)' = -[H_s'(B_0^{-1})']S_n G_{\Omega}$ [^2].
O ponto crucial aqui é como a incerteza dos parâmetros originais do VAR é traduzida na incerteza das IRFs ortogonalizadas. As derivadas analíticas, apresentadas em [11.7.13], são usadas para obter a matriz de variância-covariância assintótica, que quantifica essa incerteza. As matrizes $\Xi_\pi$ e $\Xi_\Omega$ são derivadas, respectivamente, em relação aos parâmetros VAR e à matriz de covariância dos resíduos. A complexidade adicionada no cálculo dos erros padrão para as IRFs ortogonalizadas reside em como a matriz $B_0$ (que representa a identificação dos choques estruturais) é obtida, e como a sua incerteza também se propaga para as IRFs.

### Conclusão

O cálculo dos erros padrão para as IRFs em modelos VAR é fundamental para a inferência estatística, permitindo avaliar a precisão das estimativas das funções de resposta e construir intervalos de confiança. Este capítulo apresentou diferentes métodos para obter esses erros padrão, incluindo abordagens analíticas e numéricas, além dos métodos de Monte Carlo e *bootstrapping*. As abordagens analíticas são baseadas em derivadas da função, enquanto abordagens alternativas incluem simulações de Monte Carlo ou métodos de *bootstrapping*. A escolha do método mais adequado depende das características do modelo VAR, dos objetivos da análise, do poder computacional disponível, da precisão desejada e de suposições sobre a distribuição dos erros [^2].  Em geral, abordagens numéricas e de reamostragem são consideradas boas opções para analisar a sensibilidade dos resultados a possíveis violações das suposições do modelo e também para calcular os erros padrão para modelos VAR com restrições complexas. A abordagem analítica, por sua vez, fornece um entendimento mais profundo de como a incerteza nas estimativas dos parâmetros do modelo VAR se propaga para as IRFs.
### Referências
[^1]: Capítulo anterior sobre Vetores Autoregressivos (VAR).
[^2]: Contexto fornecido para criação do capítulo.
<!-- END -->
