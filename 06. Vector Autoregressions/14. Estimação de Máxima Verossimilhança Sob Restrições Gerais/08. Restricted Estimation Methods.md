## Estimação de Máxima Verossimilhança Sob Restrições Gerais em Modelos VAR

### Introdução
Este capítulo expande o conceito de **estimação de máxima verossimilhança** (MLE) apresentado anteriormente em modelos de vetores autorregressivos (VAR) não restritos, discutindo agora a estimativa sob restrições gerais nos coeficientes. Enquanto a seção anterior focava em sistemas VAR onde cada equação tinha as mesmas variáveis explicativas, esta seção aborda o cenário onde restrições, que não podem ser expressas de forma recursiva por blocos, são impostas aos coeficientes do modelo. Este problema de otimização com restrições é um tema central no contexto de modelos VAR e necessita de uma abordagem mais sofisticada para obter estimativas consistentes e eficientes dos parâmetros. O foco principal desta seção é a **implementação computacional** da MLE sob restrições gerais, detalhando um processo iterativo que envolve a avaliação da função de verossimilhança e suas derivadas. Também exploraremos como métodos de otimização numérica podem ser usados para encontrar os estimadores que maximizam a função sob as restrições impostas.

### Conceitos Fundamentais

Em sua forma mais geral, um VAR com restrições nos coeficientes pode ser visto como um sistema de regressões aparentemente não relacionadas, conforme analisado por Zellner (1962) [^1]. Considerando um vetor *$y$* que é descrito por um modelo VAR com restrições, temos:

$y_{1t} = x'_{1t}\beta_1 + \epsilon_{1t}$
$y_{2t} = x'_{2t}\beta_2 + \epsilon_{2t}$
$...$
$y_{nt} = x'_{nt}\beta_n + \epsilon_{nt}$

Aqui, $x_{it}$ é um vetor $(k_i \times 1)$ contendo um termo constante e defasagens das variáveis que aparecem na i-ésima equação do VAR. O sistema é composto por *n* equações, onde $k_i$ denota o número de regressores na i-ésima equação. O objetivo é estimar os parâmetros $\beta_i$ e a matriz de covariância $\Omega$ que maximizam a função de verossimilhança. É importante notar que, diferentemente de um VAR não restrito, onde todas as equações compartilham o mesmo conjunto de variáveis explicativas, em um VAR com restrições gerais, cada equação pode conter um subconjunto diferente de regressores e os coeficientes podem ter relações específicas entre eles.

Como explorado anteriormente, a representação vetorial do modelo VAR torna a abordagem de máxima verossimilhança mais clara. As equações do modelo VAR podem ser escritas em forma matricial, de acordo com [^1]:

$y_t = \mathcal{X}_t \beta + \epsilon_t$,

onde $y_t$ é um vetor $(n \times 1)$ de variáveis endógenas, $\mathcal{X}_t$ é uma matriz $(n \times k)$ de regressores, $\beta$ é um vetor $(k \times 1)$ de coeficientes combinados e $\epsilon_t$ é um vetor $(n \times 1)$ de termos de erro. Aqui, $k = \sum_{i=1}^n k_i$ é o número total de regressores nas *n* equações. Esta representação unifica o modelo VAR em uma única equação vetorial, o que facilita a aplicação de técnicas de otimização para estimar os parâmetros desconhecidos.

A função de verossimilhança, como visto anteriormente [^1], pode ser escrita como:

$$
\mathcal{L}(\beta, \Omega) = - \frac{Tn}{2}\log(2\pi) + \frac{T}{2}\log|\Omega^{-1}| - \frac{1}{2} \sum_{t=1}^T (y_t - \mathcal{X}_t\beta)'\Omega^{-1}(y_t - \mathcal{X}_t\beta)
$$
onde $\mathcal{X}_t$ é uma matriz que contém os regressores correspondentes a cada equação do VAR e $\beta$ é um vetor que contém todos os coeficientes. Esta função representa a probabilidade dos dados observados dado os parâmetros do modelo.

O objetivo da **estimação de máxima verossimilhança (MLE)** é encontrar os valores de $\beta$ e $\Omega$ que maximizam a função de verossimilhança, ou equivalentemente, minimizam a função quadrática:
$$
\sum_{t=1}^T (y_t - \mathcal{X}_t\beta)'\Omega^{-1}(y_t - \mathcal{X}_t\beta).
$$

Para isso, como sugerido em [^1], usamos uma transformação que envolve a decomposição de Cholesky de $\Omega^{-1}$ como $L'L$, onde L é uma matriz triangular inferior:
$$
\sum_{t=1}^T (y_t - \mathcal{X}_t\beta)'\Omega^{-1}(y_t - \mathcal{X}_t\beta) = \sum_{t=1}^T (\tilde{y}_t - \tilde{\mathcal{X}}_t\beta)'(\tilde{y}_t - \tilde{\mathcal{X}}_t\beta)
$$
onde $\tilde{y}_t = Ly_t$ e $\tilde{\mathcal{X}}_t = L\mathcal{X}_t$.
Essa transformação permite que a minimização da função log-verossimilhança, que é o objetivo da MLE, seja realizada através de uma **regressão OLS de $\tilde{y}$ sobre $\tilde{\mathcal{X}}$**, agrupando todas as equações em uma única regressão, conforme mencionado em [^1]. Esta abordagem transforma o problema de otimização com restrições em um problema de regressão OLS padrão, simplificando o processo de estimação. É neste ponto que a escolha de uma estrutura inicial para $\beta$ se torna crucial, e o uso de estimativas de OLS como estágio inicial agiliza o processo iterativo.

Em termos computacionais, a estimação sob restrições gerais é implementada por meio de um **processo iterativo**. O primeiro passo é obter estimativas iniciais dos parâmetros. Estas estimativas iniciais, $\beta(0)$,  podem ser obtidas, como mencionado anteriormente [^1], por meio de uma sequência de regressões OLS separadas em cada equação do sistema VAR. A matriz de covariância dos resíduos, $\hat{\Omega}(0)$, é estimada utilizando os resíduos destas regressões iniciais. Em seguida, uma matriz $L(0)$ é obtida através da fatorização de Cholesky de $(\hat{\Omega}(0))^{-1}$, tal que $(L(0))'L(0) = (\hat{\Omega}(0))^{-1}$. Uma vez obtido $L(0)$, as variáveis originais são transformadas como $\tilde{y}_t(0) = L(0)y_t$ e $\tilde{\mathcal{X}}_t(0) = L(0)\mathcal{X}_t$. Então, uma regressão OLS agrupada de $\tilde{y}_t(0)$ sobre $\tilde{\mathcal{X}}_t(0)$ fornece uma nova estimativa $\beta(1)$.

O próximo passo crucial é a **avaliação da função de verossimilhança** para os parâmetros estimados. O procedimento continua iterativamente, onde a estimativa dos parâmetros é atualizada a cada iteração, usando um método de otimização numérica, tal como um método de gradiente ou Newton-Raphson. Em cada iteração, a função de verossimilhança e suas derivadas (gradiente e hessiana) são avaliadas para determinar a direção na qual os parâmetros devem ser ajustados para aumentar o valor da função de verossimilhança. As equações [11.3.13] e [11.3.14] apresentadas em [^1], juntamente com as equações [11.3.17] e [11.3.18], são essenciais neste processo, mostrando como as derivadas da função podem ser obtidas e como a função log-verossimilhança pode ser representada de uma forma que simplifica sua avaliação.

A **convergência do procedimento iterativo** é fundamental para garantir que as estimativas obtidas sejam precisas e confiáveis. A condição de parada comum é expressa através de um critério de tolerância, como discutido anteriormente, com o processo iterativo continuando até que a diferença entre as estimativas de $\beta$ em iterações sucessivas seja menor que a tolerância pré-definida $\delta$.  A tolerância $\delta$ atua como um limite para a variação dos parâmetros em iterações consecutivas, garantindo que o processo pare quando a estimativa do parâmetro estiver suficientemente próxima da estimativa que maximiza a função de verossimilhança. O ajuste adequado da tolerância é um aspecto crítico da implementação computacional, uma vez que impacta a precisão, a eficiência computacional e a prevenção de *overfitting*.

Os **métodos de otimização numérica**, que podem ser utilizados no procedimento iterativo incluem métodos de gradiente, como o método do gradiente descendente, e métodos de segunda ordem, como o método de Newton-Raphson e suas variações, como BFGS (Broyden-Fletcher-Goldfarb-Shanno). Estes métodos utilizam informações sobre a derivada da função de verossimilhança para encontrar o máximo, com o método de Newton-Raphson, em particular, requerendo o cálculo da hessiana. Os métodos de otimização numérica são usados para atualizar iterativamente as estimativas dos parâmetros até que um ponto de convergência seja alcançado, ou seja, quando a função de verossimilhança atinge um valor máximo sob as restrições impostas.

A **otimização para diferentes configurações de hardware** envolve a implementação dos métodos de estimação iterativa com o uso de bibliotecas otimizadas, como BLAS (Basic Linear Algebra Subprograms), LAPACK (Linear Algebra PACKage), ou utilizando processamento paralelo por meio de CUDA ou OpenCL, para GPUs e processamento paralelo, respectivamente. A utilização de algoritmos otimizados para operações matriciais em álgebra linear, juntamente com a paralelização,  é fundamental para o desempenho computacional em larga escala, especialmente ao lidar com modelos VAR com muitas variáveis e um número grande de observações.

### Conclusão

A estimação de máxima verossimilhança sob restrições gerais em modelos VAR envolve a implementação de um processo iterativo que combina transformações de dados, regressões OLS agrupadas e métodos de otimização numérica para maximizar a função de verossimilhança. O ponto crítico para este processo é a implementação da etapa de convergência, que deve ser feita com atenção para garantir a precisão das estimativas e a eficiência computacional. A utilização de métodos de otimização numérica e a implementação cuidadosa do procedimento iterativo são essenciais para obter estimativas confiáveis dos parâmetros do modelo sob as restrições impostas. A otimização para diferentes configurações de hardware pode acelerar significativamente o processo computacional, especialmente quando lidamos com conjuntos de dados de grande dimensão. A escolha do método de otimização adequado e a implementação da etapa de convergência são essenciais para a precisão das estimativas e a eficiência computacional da abordagem.

### Referências
[^1]: *Zellner, A. (1962). An Efficient Method of Estimating Seemingly Unrelated Regressions and Tests for Aggregation Bias. Journal of the American Statistical Association, 57(298), 348–368.*
<!-- END -->
