## Estimação de Máxima Verossimilhança Sob Restrições Gerais em Modelos VAR

### Introdução
Este capítulo expande o conceito de **estimação de máxima verossimilhança** (MLE) apresentado anteriormente em modelos de vetores autorregressivos (VAR) não restritos, discutindo agora a estimativa sob restrições gerais nos coeficientes. Enquanto a seção anterior focava em sistemas VAR onde cada equação tinha as mesmas variáveis explicativas, esta seção aborda o cenário onde restrições, que não podem ser expressas de forma recursiva por blocos, são impostas aos coeficientes do modelo. Este problema de otimização com restrições é um tema central no contexto de modelos VAR e necessita de uma abordagem mais sofisticada para obter estimativas consistentes e eficientes dos parâmetros. O foco principal desta seção é detalhar o **procedimento iterativo** utilizado na MLE com restrições gerais, que envolve um algoritmo de *pooling* de dados para combinar todas as equações em um único sistema e otimizar o desempenho computacional usando uma estimativa OLS como um estágio inicial para agilizar o processo. Além disso, vamos discutir a importância da **etapa de convergência** no procedimento iterativo, detalhando como um algoritmo de tolerância bem definido deve ser implementado para assegurar a precisão e eficácia da estimativa e inferência do modelo, juntamente com considerações sobre otimização para diferentes configurações de hardware.

### Conceitos Fundamentais

Em sua forma mais geral, um VAR com restrições nos coeficientes pode ser visto como um sistema de regressões aparentemente não relacionadas, conforme analisado por Zellner (1962) [^1]. Considerando um vetor *$y$* que é descrito por um modelo VAR com restrições, temos:

$y_{1t} = x'_{1t}\beta_1 + \epsilon_{1t}$
$y_{2t} = x'_{2t}\beta_2 + \epsilon_{2t}$
$...$
$y_{nt} = x'_{nt}\beta_n + \epsilon_{nt}$

Aqui, $x_{it}$ é um vetor $(k_i \times 1)$ contendo um termo constante e defasagens das variáveis que aparecem na i-ésima equação do VAR. O sistema é composto por *n* equações, onde $k_i$ denota o número de regressores na i-ésima equação. O objetivo é estimar os parâmetros $\beta_i$ e a matriz de covariância $\Omega$ que maximizam a função de verossimilhança. É importante notar que, diferentemente de um VAR não restrito, onde todas as equações compartilham o mesmo conjunto de variáveis explicativas, em um VAR com restrições gerais, cada equação pode conter um subconjunto diferente de regressores e os coeficientes podem ter relações específicas entre eles.

Como explorado anteriormente, a representação vetorial do modelo VAR torna a abordagem de máxima verossimilhança mais clara. As equações do modelo VAR podem ser escritas em forma matricial, de acordo com [^1]:

$y_t = \mathcal{X}_t \beta + \epsilon_t$,

onde $y_t$ é um vetor $(n \times 1)$ de variáveis endógenas, $\mathcal{X}_t$ é uma matriz $(n \times k)$ de regressores, $\beta$ é um vetor $(k \times 1)$ de coeficientes combinados e $\epsilon_t$ é um vetor $(n \times 1)$ de termos de erro. Aqui, $k = \sum_{i=1}^n k_i$ é o número total de regressores nas *n* equações. Esta representação unifica o modelo VAR em uma única equação vetorial, o que facilita a aplicação de técnicas de otimização para estimar os parâmetros desconhecidos.

A função de verossimilhança, como visto anteriormente [^1], pode ser escrita como:

$$
\mathcal{L}(\beta, \Omega) = - \frac{Tn}{2}\log(2\pi) + \frac{T}{2}\log|\Omega^{-1}| - \frac{1}{2} \sum_{t=1}^T (y_t - \mathcal{X}_t\beta)'\Omega^{-1}(y_t - \mathcal{X}_t\beta)
$$
onde $\mathcal{X}_t$ é uma matriz que contém os regressores correspondentes a cada equação do VAR e $\beta$ é um vetor que contém todos os coeficientes. Esta função representa a probabilidade dos dados observados dado os parâmetros do modelo.

O objetivo da **estimação de máxima verossimilhança (MLE)** é encontrar os valores de $\beta$ e $\Omega$ que maximizam a função de verossimilhança, ou equivalentemente, minimizam a função quadrática:

$$
\sum_{t=1}^T (y_t - \mathcal{X}_t\beta)'\Omega^{-1}(y_t - \mathcal{X}_t\beta).
$$

Para isso, como sugerido em [^1], usamos uma transformação que envolve a decomposição de Cholesky de $\Omega^{-1}$ como $L'L$, onde L é uma matriz triangular inferior:
$$
\sum_{t=1}^T (y_t - \mathcal{X}_t\beta)'\Omega^{-1}(y_t - \mathcal{X}_t\beta) = \sum_{t=1}^T (\tilde{y}_t - \tilde{\mathcal{X}}_t\beta)'(\tilde{y}_t - \tilde{\mathcal{X}}_t\beta)
$$
onde $\tilde{y}_t = Ly_t$ e $\tilde{\mathcal{X}}_t = L\mathcal{X}_t$.
Essa transformação permite que a minimização da função log-verossimilhança, que é o objetivo da MLE, seja realizada através de uma **regressão OLS de $\tilde{y}$ sobre $\tilde{\mathcal{X}}$**, agrupando todas as equações em uma única regressão, conforme mencionado em [^1]. Esta abordagem transforma o problema de otimização com restrições em um problema de regressão OLS padrão, simplificando o processo de estimação. É neste ponto que a escolha de uma estrutura inicial para $\beta$ se torna crucial, e o uso de estimativas de OLS como estágio inicial agiliza o processo iterativo.

A minimização da função é, portanto, feita através de um **algoritmo iterativo**, onde o vetor de parâmetros $\beta$ em um modelo VAR com restrições gerais é estimado por meio de OLS agrupado, em cada iteração. O método consiste na minimização da soma dos quadrados ponderados dos resíduos, onde a matriz de pesos é a inversa da matriz de covariância dos resíduos.

A otimização do processo computacional se dá através de um **método iterativo**, no qual uma estimativa OLS é utilizada como um estágio inicial, conforme sugerido em [^1]. De acordo com as informações fornecidas, o método consiste em usar o resultado das regressões OLS de cada equação do sistema separadamente como um ponto de partida.  Esta estratégia não apenas fornece valores iniciais razoáveis para os parâmetros, como também melhora o desempenho computacional do processo iterativo. O procedimento inicia com estimativas OLS separadas $\beta(0)$ de cada equação do VAR e calcula a matriz de covariância dos resíduos $\hat{\Omega}(0)$. Em seguida, uma matriz $L(0)$ tal que $(L(0))'L(0) = (\hat{\Omega}(0))^{-1}$ é obtida usando a decomposição de Cholesky. As variáveis originais são transformadas para $\tilde{y}_t(0) = L(0)y_t$ e $\tilde{\mathcal{X}}_t(0) = L(0)\mathcal{X}_t$. Após esta transformação, uma regressão OLS agrupada de $\tilde{y}_t(0)$ sobre $\tilde{\mathcal{X}}_t(0)$ fornece uma nova estimativa $\beta(1)$. **A etapa de convergência deste processo é crucial** e envolve a definição de uma **tolerância $\delta$** para verificar quando o processo iterativo deve parar. Por exemplo, uma condição de parada comum é:

$$
||\beta^{(i+1)} - \beta^{(i)}|| < \delta
$$

onde $||\cdot||$ denota uma norma (e.g., Euclidiana) e $\beta^{(i)}$ denota o vetor de parâmetros estimado na i-ésima iteração. Este processo é repetido até que as estimativas convirjam dentro da tolerância $\delta$, garantindo que a estimativa de máxima verossimilhança seja alcançada de forma eficiente sob as restrições impostas. Um valor de $\delta$ muito alto pode resultar na convergência prematura e em estimativas imprecisas, enquanto um valor de $\delta$ muito baixo pode tornar o processo computacionalmente caro e aumentar o risco de *overfitting*.

A **matriz de covariância assintótica** dos coeficientes $\beta$,  permite realizar testes de hipóteses sobre os parâmetros do modelo, como visto em seções anteriores. A obtenção desta matriz se dá pela expressão:

$$
E[(\hat{\beta} - \beta)(\hat{\beta} - \beta)'] = \left[ \sum_{t=1}^T (\mathcal{X}'_t \Omega^{-1} \mathcal{X}_t) \right]^{-1}
$$
onde $\hat{\beta}$ representa a estimativa do vetor de parâmetros $\beta$. Esta expressão é fundamental para avaliar a precisão das estimativas e realizar inferência estatística no modelo VAR. A forma dessa matriz é apresentada em [^1], expressa como:

$$
E(\hat{\beta} - \beta)(\hat{\beta} - \beta)' =  \left[ \sum_{t=1}^T (\tilde{\mathcal{X}}_t' \tilde{\mathcal{X}}_t) \right]^{-1}
$$

Para obter o resultado em [^1], é necessário usar o fato de que o resíduo da regressão OLS agrupada possui uma matriz de covariância igual à matriz identidade, como indicado em [^1].  A partir dessas propriedades, a matriz de covariância assintótica pode ser obtida a partir dos regressores transformados. Este resultado é crucial para a inferência sobre os parâmetros, pois permite avaliar a incerteza associada às estimativas dos coeficientes.

O procedimento de estimação iterativo, como discutido, pode ser otimizado para diferentes configurações de hardware. Por exemplo, a fatorização de Cholesky e a multiplicação de matrizes, operações computacionalmente intensivas, podem ser implementadas usando bibliotecas otimizadas para GPU (Graphics Processing Unit), como CUDA ou OpenCL, proporcionando ganhos significativos de desempenho computacional. Além disso, a paralelização das regressões OLS nas diferentes iterações pode acelerar ainda mais o processo, particularmente em sistemas com múltiplos núcleos de processamento.

### Conclusão

A estimativa de máxima verossimilhança sob restrições gerais em modelos VAR é um processo iterativo que combina a transformação dos dados, regressões OLS agrupadas e uma etapa de convergência bem definida para garantir estimativas precisas e eficientes dos parâmetros. A escolha do critério de convergência e da tolerância é crucial para equilibrar precisão e eficiência computacional. A matriz de covariância assintótica obtida através do procedimento descrito em seções anteriores fornece a base para testes de hipótese e inferência estatística, enquanto a otimização para diferentes configurações de hardware pode acelerar significativamente o processo computacional. A utilização das representações alternativas da função log-verossimilhança facilita a abordagem da otimização em etapas diferentes, o que é essencial para modelos complexos.

### Referências
[^1]: *Zellner, A. (1962). An Efficient Method of Estimating Seemingly Unrelated Regressions and Tests for Aggregation Bias. Journal of the American Statistical Association, 57(298), 348–368.*
<!-- END -->
