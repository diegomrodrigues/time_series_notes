## Estimação de Máxima Verossimilhança Sob Restrições Gerais em Modelos VAR
### Introdução
Este capítulo expande o conceito de **estimação de máxima verossimilhança** (MLE) apresentado anteriormente em modelos de vetores autorregressivos (VAR) não restritos, discutindo agora a estimativa sob restrições gerais nos coeficientes. Enquanto a seção anterior focava em sistemas VAR onde cada equação tinha as mesmas variáveis explicativas, esta seção aborda o cenário onde restrições, que não podem ser expressas de forma recursiva por blocos, são impostas aos coeficientes do modelo. Este problema de otimização com restrições é um tema central no contexto de modelos VAR e necessita de uma abordagem mais sofisticada para obter estimativas consistentes e eficientes dos parâmetros. Como veremos, a **função log-verossimilhança** para este modelo pode ser escrita de forma a facilitar a estimação dos parâmetros, permitindo uma abordagem sistemática para lidar com restrições arbitrárias sobre os coeficientes.

### Conceitos Fundamentais

Em sua forma mais geral, um VAR com restrições nos coeficientes pode ser visto como um sistema de regressões aparentemente não relacionadas, conforme analisado por Zellner (1962) [^1]. Considerando um vetor *y* que é descrito por um modelo VAR com restrições, temos:

$y_{1t} = x'_{1t}\beta_1 + \epsilon_{1t}$
$y_{2t} = x'_{2t}\beta_2 + \epsilon_{2t}$
$...$
$y_{nt} = x'_{nt}\beta_n + \epsilon_{nt}$

Aqui, $x_{it}$ é um vetor $(k_i \times 1)$ contendo um termo constante e defasagens das variáveis que aparecem na i-ésima equação do VAR. O sistema é composto por n equações, onde $k_i$ denota o número de regressores na i-ésima equação. O objetivo é estimar os parâmetros $\beta_i$ e a matriz de covariância $\Omega$ que maximizam a função de verossimilhança.

A função de verossimilhança, como visto anteriormente [^1], pode ser escrita como:

$$
\mathcal{L}(\beta, \Omega) = - \frac{Tn}{2}\log(2\pi) + \frac{T}{2}\log|\Omega^{-1}| - \frac{1}{2} \sum_{t=1}^T (y_t - \mathcal{X}_t\beta)'\Omega^{-1}(y_t - \mathcal{X}_t\beta)
$$
onde $\mathcal{X}_t$ é uma matriz que contém os regressores correspondentes a cada equação do VAR e $\beta$ é um vetor que contém todos os coeficientes.
Em um cenário sem restrições, a maximização da função de verossimilhança pode ser alcançada por meio de regressões OLS separadas para cada equação do sistema. Entretanto, quando são impostas restrições sobre os parâmetros, a abordagem padrão de OLS não é mais apropriada.

O problema da estimação de máxima verossimilhança sob restrições gerais em modelos VAR é, portanto, um problema de otimização com restrições. Este problema envolve a busca de um vetor de parâmetros $\beta$ e a matriz de covariância $\Omega$ que maximizam a função de verossimilhança logarítmica, respeitando as restrições impostas. Essas restrições podem ser lineares ou não lineares, envolvendo combinações de coeficientes e podem originar de teorias econômicas, hipóteses estatísticas ou outras fontes de conhecimento.

Para lidar com essas restrições, uma abordagem comum é usar métodos iterativos que empregam condições de primeira ordem e, em seguida, atualizar as estimativas dos parâmetros, respeitando as restrições impostas. O método de Newton-Raphson ou o algoritmo de Fisher Scoring são algumas das técnicas iterativas frequentemente usadas.

Como vimos em [^1], se $\Omega^{-1}$ for escrita como $L'L$, a função a ser minimizada é:
$$
\sum_{t=1}^T(y_t - \mathcal{X}_t\beta)'\Omega^{-1}(y_t - \mathcal{X}_t\beta) = \sum_{t=1}^T(\tilde{y}_t - \tilde{\mathcal{X}}_t\beta)'(\tilde{y}_t - \tilde{\mathcal{X}}_t\beta)
$$
onde $\tilde{y}_t = Ly_t$ e $\tilde{\mathcal{X}}_t = L\mathcal{X}_t$.
Assim, a MLE é obtida pela regressão OLS de $\tilde{y}_t$ sobre $\tilde{\mathcal{X}}_t$, agrupando todas as equações.

Para implementar este procedimento iterativo, começamos com estimativas iniciais dos parâmetros, como sugerido em [^1]. Por exemplo, podem ser obtidas estimativas iniciais $\beta(0)$ a partir de regressões OLS separadas para cada equação do VAR. Em seguida, a matriz $\Omega$ é estimada utilizando os resíduos das regressões OLS, como em [^1]:
$$
\hat{\Omega}(0) = \frac{1}{T}\sum_{t=1}^T (y_t - \mathcal{X}_t\beta(0))(y_t - \mathcal{X}_t\beta(0))'
$$

Em seguida, encontramos uma matriz $L(0)$ tal que $(L(0))'L(0) = (\hat{\Omega}(0))^{-1}$ usando a fatorização de Cholesky, e formamos $\tilde{y}_t(0) = L(0)y_t$ e $\tilde{\mathcal{X}}_t(0) = L(0)\mathcal{X}_t$.
A regressão OLS agrupada de $\tilde{y}_t(0)$ sobre $\tilde{\mathcal{X}}_t(0)$ fornece a nova estimativa $\beta(1)$. Repetindo este processo iterativamente, as estimativas convergem para as estimativas de máxima verossimilhança, sob as restrições impostas, conforme discutido em [^1].

No contexto da estimativa sob restrições gerais, as equações [11.3.9] a [11.3.12] e [11.3.17] e [11.3.18] são importantes para entender a decomposição da função de verossimilhança em termos de parâmetros restritos. Essas equações mostram como a função de verossimilhança pode ser expressa em termos das matrizes A, B e das matrizes de covariância, que são diretamente relacionadas aos parâmetros do modelo. A estratégia de estimar primeiro parâmetros sob condições restritas, e posteriormente estimar os parâmetros não restritos com base na estrutura restrita fornece uma forma eficiente de otimizar o modelo. Especificamente, notemos que as equações [11.3.13] a [11.3.15] fornecem uma representação alternativa para a função de verossimilhança que é particularmente útil quando lidamos com restrições. Aqui, a função log-verossimilhança é reescrita como a soma de duas partes, $l_{1t}$ e $l_{2t}$, facilitando o tratamento de restrições que afetam apenas um subconjunto de equações no modelo.

$$
\log f_{y_t|x_t} (y_t, x_t; \theta) = l_{1t} + l_{2t}
$$

onde
$$
l_{1t} = -\frac{n_1}{2} \log(2\pi) - \frac{1}{2} \log|\Omega_{11}| - \frac{1}{2}(y_{1t} - c_1 - A_1x_t - A_2x_{2t})'\Omega_{11}^{-1}(y_{1t} - c_1 - A_1x_t - A_2x_{2t})
$$
e
$$
l_{2t} = -\frac{n_2}{2} \log(2\pi) - \frac{1}{2} \log|H| - \frac{1}{2}(y_{2t} - d - D_0y_{1t} - D_1x_t - D_2x_{2t})'H^{-1}(y_{2t} - d - D_0y_{1t} - D_1x_t - D_2x_{2t})
$$

Este formato permite tratar o problema de estimação como um problema de otimização separada, primeiro estimando parâmetros que aparecem apenas em $l_{1t}$ e depois os parâmetros que aparecem em $l_{2t}$.

### Conclusão
A estimação de máxima verossimilhança sob restrições gerais em modelos VAR apresenta um desafio adicional em relação à estimativa de modelos não restritos. No entanto, a compreensão das técnicas e métodos iterativos usados para lidar com as restrições permite obter estimativas consistentes e eficientes dos parâmetros. A abordagem iterativa, começando com estimativas iniciais dos parâmetros e atualizando-as com base nas condições de primeira ordem e restrições impostas, garante uma solução ótima para o problema de otimização. Adicionalmente, a decomposição da função de verossimilhança permite explorar como a matriz de coeficientes e a matriz de covariância são relacionadas, resultando em estratégias eficientes de otimização para o modelo VAR. A representação da função log-verossimilhança em componentes separados facilita a estimação sob restrições gerais, permitindo estimar parâmetros em blocos e em estágios diferentes. Esta abordagem é essencial para lidar com as complexidades inerentes à modelagem econométrica com restrições.

### Referências
[^1]: *Zellner, A. (1962). An Efficient Method of Estimating Seemingly Unrelated Regressions and Tests for Aggregation Bias. Journal of the American Statistical Association, 57(298), 348–368.*
<!-- END -->
