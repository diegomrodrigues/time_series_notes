## Estimação de Máxima Verossimilhança Sob Restrições Gerais em Modelos VAR

### Introdução
Este capítulo expande o conceito de **estimação de máxima verossimilhança** (MLE) apresentado anteriormente em modelos de vetores autorregressivos (VAR) não restritos, discutindo agora a estimativa sob restrições gerais nos coeficientes. Enquanto a seção anterior focava em sistemas VAR onde cada equação tinha as mesmas variáveis explicativas, esta seção aborda o cenário onde restrições, que não podem ser expressas de forma recursiva por blocos, são impostas aos coeficientes do modelo. Este problema de otimização com restrições é um tema central no contexto de modelos VAR e necessita de uma abordagem mais sofisticada para obter estimativas consistentes e eficientes dos parâmetros. Como veremos, a **função log-verossimilhança** para este modelo pode ser escrita de forma a facilitar a estimação dos parâmetros, permitindo uma abordagem sistemática para lidar com restrições arbitrárias sobre os coeficientes. O método de minimização da função log-verossimilhança, neste cenário, é implementado através de uma regressão OLS de *y* em *x*, agrupando todas as equações em uma única regressão.

### Conceitos Fundamentais

Em sua forma mais geral, um VAR com restrições nos coeficientes pode ser visto como um sistema de regressões aparentemente não relacionadas, conforme analisado por Zellner (1962) [^1]. Considerando um vetor *$y$* que é descrito por um modelo VAR com restrições, temos:

$y_{1t} = x'_{1t}\beta_1 + \epsilon_{1t}$
$y_{2t} = x'_{2t}\beta_2 + \epsilon_{2t}$
$...$
$y_{nt} = x'_{nt}\beta_n + \epsilon_{nt}$

Aqui, $x_{it}$ é um vetor $(k_i \times 1)$ contendo um termo constante e defasagens das variáveis que aparecem na i-ésima equação do VAR. O sistema é composto por *n* equações, onde $k_i$ denota o número de regressores na i-ésima equação. O objetivo é estimar os parâmetros $\beta_i$ e a matriz de covariância $\Omega$ que maximizam a função de verossimilhança. É importante notar que, diferentemente de um VAR não restrito, onde todas as equações compartilham o mesmo conjunto de variáveis explicativas, em um VAR com restrições gerais, cada equação pode conter um subconjunto diferente de regressores e os coeficientes podem ter relações específicas entre eles.

Como explorado anteriormente, a representação vetorial do modelo VAR torna a abordagem de máxima verossimilhança mais clara. As equações do modelo VAR podem ser escritas em forma matricial, de acordo com [^1]:

$y_t = \mathcal{X}_t \beta + \epsilon_t$,

onde $y_t$ é um vetor $(n \times 1)$ de variáveis endógenas, $\mathcal{X}_t$ é uma matriz $(n \times k)$ de regressores, $\beta$ é um vetor $(k \times 1)$ de coeficientes combinados e $\epsilon_t$ é um vetor $(n \times 1)$ de termos de erro. Aqui, $k = \sum_{i=1}^n k_i$ é o número total de regressores nas *n* equações. Esta representação unifica o modelo VAR em uma única equação vetorial, o que facilita a aplicação de técnicas de otimização para estimar os parâmetros desconhecidos.

A função de verossimilhança, como visto anteriormente [^1], pode ser escrita como:

$$
\mathcal{L}(\beta, \Omega) = - \frac{Tn}{2}\log(2\pi) + \frac{T}{2}\log|\Omega^{-1}| - \frac{1}{2} \sum_{t=1}^T (y_t - \mathcal{X}_t\beta)'\Omega^{-1}(y_t - \mathcal{X}_t\beta)
$$
onde $\mathcal{X}_t$ é uma matriz que contém os regressores correspondentes a cada equação do VAR e $\beta$ é um vetor que contém todos os coeficientes. Esta função representa a probabilidade dos dados observados dado os parâmetros do modelo.

O objetivo da **estimação de máxima verossimilhança (MLE)** é encontrar os valores de $\beta$ e $\Omega$ que maximizam a função de verossimilhança, ou equivalentemente, minimizam a função quadrática:
$$
\sum_{t=1}^T (y_t - \mathcal{X}_t\beta)'\Omega^{-1}(y_t - \mathcal{X}_t\beta).
$$

Para isso, como sugerido em [^1], usamos uma transformação que envolve a decomposição de Cholesky de $\Omega^{-1}$ como $L'L$, onde L é uma matriz triangular inferior:
$$
\sum_{t=1}^T (y_t - \mathcal{X}_t\beta)'\Omega^{-1}(y_t - \mathcal{X}_t\beta) = \sum_{t=1}^T (\tilde{y}_t - \tilde{\mathcal{X}}_t\beta)'(\tilde{y}_t - \tilde{\mathcal{X}}_t\beta)
$$
onde $\tilde{y}_t = Ly_t$ e $\tilde{\mathcal{X}}_t = L\mathcal{X}_t$.
Essa transformação permite que a minimização da função log-verossimilhança, que é o objetivo da MLE, seja realizada através de uma **regressão OLS de $\tilde{y}$ sobre $\tilde{\mathcal{X}}$**, agrupando todas as equações em uma única regressão, conforme mencionado em [^1]. Esta abordagem transforma o problema de otimização com restrições em um problema de regressão OLS padrão, simplificando o processo de estimação.

O **vetor de parâmetros $\beta$** em um modelo VAR com restrições gerais é estimado por meio de OLS agrupado. O método consiste na minimização da soma dos quadrados ponderados dos resíduos, onde a matriz de pesos é a inversa da matriz de covariância dos resíduos.

Em um contexto iterativo, como discutido em [^1], o procedimento começa com estimativas iniciais dos parâmetros, por exemplo, as estimativas de OLS separadas para cada equação, $\beta(0)$, e da matriz de covariância, $\hat{\Omega}(0)$, calculada usando os resíduos dessas regressões. Após obter a matriz $L(0)$ tal que $L(0)'L(0) = (\hat{\Omega}(0))^{-1}$ pela fatorização de Cholesky, transformamos as variáveis como $\tilde{y}_t(0) = L(0)y_t$ e $\tilde{\mathcal{X}}_t(0) = L(0)\mathcal{X}_t$. Em seguida, a regressão OLS agrupada de $\tilde{y}_t(0)$ sobre $\tilde{\mathcal{X}}_t(0)$ fornece a nova estimativa $\beta(1)$. Este processo é repetido iterativamente até que as estimativas convirjam para os valores que minimizam a função de log-verossimilhança.

Para uma compreensão mais completa da conexão entre a função log-verossimilhança e a abordagem de OLS agrupado, considere que a função log-verossimilhança para um sistema VAR restrito pode ser decomposta em duas partes, $l_{1t}$ e $l_{2t}$, como mostrado em [^1] nas equações [11.3.13] a [11.3.15]:
$$
\log f_{y_t|x_t} (y_t, x_t; \theta) = l_{1t} + l_{2t}
$$
onde
$$
l_{1t} = -\frac{n_1}{2} \log(2\pi) - \frac{1}{2} \log|\Omega_{11}| - \frac{1}{2}(y_{1t} - c_1 - A_1x_t - A_2x_{2t})'\Omega_{11}^{-1}(y_{1t} - c_1 - A_1x_t - A_2x_{2t})
$$
e
$$
l_{2t} = -\frac{n_2}{2} \log(2\pi) - \frac{1}{2} \log|H| - \frac{1}{2}(y_{2t} - d - D_0y_{1t} - D_1x_t - D_2x_{2t})'H^{-1}(y_{2t} - d - D_0y_{1t} - D_1x_t - D_2x_{2t})
$$
Esta representação permite uma abordagem em dois estágios para a estimativa, em que os parâmetros em $l_{1t}$ são estimados primeiro (usando OLS), e depois esses resultados são usados para estimar parâmetros em $l_{2t}$. Assim, a abordagem de mínimos quadrados ordinários é aplicada não apenas aos parâmetros não transformados, mas também às variáveis transformadas. Isso garante que os resultados sejam equivalentes aos obtidos pela maximização da função de verossimilhança, como discutido em [^1].

### Conclusão

A estimativa de máxima verossimilhança em modelos VAR com restrições gerais nos coeficientes pode ser feita através de uma regressão OLS agrupada após uma transformação das variáveis. Este método envolve a minimização da soma dos quadrados ponderados dos resíduos, onde a matriz de pesos é a inversa da matriz de covariância dos resíduos. A utilização da fatorização de Cholesky e de técnicas iterativas permitem a estimativa consistente dos parâmetros sob restrições. A representação vetorial do modelo e a decomposição da função de log-verossimilhança são essenciais para a aplicação deste método. A abordagem discutida permite que o problema de estimação complexo seja transformado em um problema de OLS, simplificando a implementação e garantindo a obtenção de estimativas de máxima verossimilhança sob as restrições impostas.

### Referências
[^1]: *Zellner, A. (1962). An Efficient Method of Estimating Seemingly Unrelated Regressions and Tests for Aggregation Bias. Journal of the American Statistical Association, 57(298), 348–368.*
<!-- END -->
