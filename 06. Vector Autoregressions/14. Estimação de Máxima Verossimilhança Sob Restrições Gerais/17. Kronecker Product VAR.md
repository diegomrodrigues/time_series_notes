## Estimação de Máxima Verossimilhança Sob Restrições Gerais em Modelos VAR: Construção de Variáveis via Álgebra Linear Avançada

### Introdução
Este capítulo aprofunda a discussão sobre a **estimação de máxima verossimilhança** (MLE) em modelos de vetores autorregressivos (VAR) com restrições gerais nos coeficientes, com um foco específico na **construção das variáveis** para a regressão OLS agrupada, um passo crucial no processo iterativo de MLE sob restrições. Nos capítulos anteriores, estabelecemos o arcabço teórico e algorítmico para a MLE com restrições, enfatizando a importância da avaliação da função de verossimilhança, da otimização numérica e da convergência, bem como o uso de algoritmos paralelizáveis para análise em grande escala. Esta seção visa detalhar os aspectos de **álgebra linear avançada** envolvidos na construção das variáveis que alimentam a regressão OLS agrupada, mostrando o uso do **produto de Kronecker** e das **transformações lineares** nos dados, passos esses essenciais para uma implementação eficiente e correta deste método de estimação.

### Conceitos Fundamentais
Como visto anteriormente, um modelo VAR com restrições gerais pode ser expresso como um sistema de regressões aparentemente não relacionadas [^1]:

$y_{1t} = x'_{1t}\beta_1 + \epsilon_{1t}$
$y_{2t} = x'_{2t}\beta_2 + \epsilon_{2t}$
$...$
$y_{nt} = x'_{nt}\beta_n + \epsilon_{nt}$

onde $y_{it}$ são as variáveis endógenas, $x_{it}$ são os vetores de regressores (incluindo termo constante e defasagens), $\beta_i$ são os vetores de coeficientes, e $\epsilon_{it}$ são os termos de erro para cada equação. O objetivo principal é estimar os parâmetros $\beta_i$ e a matriz de covariância $\Omega$ que maximizam a função de verossimilhança.

A função de verossimilhança é dada por [^1]:

$$
\mathcal{L}(\beta, \Omega) = - \frac{Tn}{2}\log(2\pi) + \frac{T}{2}\log|\Omega^{-1}| - \frac{1}{2} \sum_{t=1}^T (y_t - \mathcal{X}_t\beta)'\Omega^{-1}(y_t - \mathcal{X}_t\beta)
$$

onde $y_t$ é um vetor $(n \times 1)$ de variáveis endógenas, $\mathcal{X}_t$ é uma matriz $(n \times k)$ de regressores, $\beta$ é um vetor $(k \times 1)$ de coeficientes combinados e $\epsilon_t$ é um vetor $(n \times 1)$ de termos de erro. Aqui, $k = \sum_{i=1}^n k_i$ é o número total de regressores nas *n* equações.

A estratégia para realizar a MLE sob restrições gerais envolve a transformação dos dados, tal como descrito por Zellner (1962) [^1]. O objetivo é transformar a função de verossimilhança para que o problema de otimização possa ser resolvido através de uma regressão OLS agrupada. Para tal, a matriz de covariância $\Omega$ é decomposta utilizando a fatoração de Cholesky $\Omega^{-1} = L'L$, onde $L$ é uma matriz triangular inferior. A função quadrática a ser minimizada passa a ser:
$$
\sum_{t=1}^T (y_t - \mathcal{X}_t\beta)'\Omega^{-1}(y_t - \mathcal{X}_t\beta) = \sum_{t=1}^T (\tilde{y}_t - \tilde{\mathcal{X}}_t\beta)'(\tilde{y}_t - \tilde{\mathcal{X}}_t\beta)
$$
onde $\tilde{y}_t = Ly_t$ e $\tilde{\mathcal{X}}_t = L\mathcal{X}_t$.
Em termos computacionais, a construção de $\tilde{y}_t$ e $\tilde{\mathcal{X}}_t$ é um passo crucial. Vamos detalhar agora como a álgebra linear avançada é usada para tal.

O primeiro passo no procedimento iterativo de MLE sob restrições gerais é a obtenção de estimativas iniciais de $\beta$ via regressão OLS separada em cada equação do modelo VAR. Estas estimativas iniciais resultam em uma matriz de covariância dos resíduos $\hat{\Omega}^{(0)}$.  A partir desta matriz é possível obter uma matriz triangular inferior $L^{(0)}$ tal que $(L^{(0)})'L^{(0)} = (\hat{\Omega}^{(0)})^{-1}$. Com esta matriz em mãos,  a variável dependente transformada  $\tilde{y}_t^{(0)}$ e a matriz de regressores transformada $\tilde{\mathcal{X}}_t^{(0)}$ podem ser construídas.

Para representar o processo de transformação de uma forma mais clara, vamos definir o vetor $y_t$ e a matriz de regressores $\mathcal{X}_t$ como:

$$
y_t = \begin{bmatrix}
y_{1t} \\ y_{2t} \\ \vdots \\ y_{nt}
\end{bmatrix} \quad \text{e} \quad \mathcal{X}_t = \begin{bmatrix}
x'_{1t} & 0 & \cdots & 0 \\
0 & x'_{2t} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & x'_{nt}
\end{bmatrix}
$$

O objetivo é transformar $y_t$ e $\mathcal{X}_t$ em $\tilde{y}_t = L y_t$ e $\tilde{\mathcal{X}}_t = L \mathcal{X}_t$. A aplicação da transformação linear $L$, que é uma matriz triangular inferior,  aos dados envolve operações de álgebra linear. O resultado de $\tilde{y}_t = Ly_t$ é direto pois consiste em uma multiplicação de matrizes. A construção de $\tilde{\mathcal{X}}_t = L \mathcal{X}_t$ envolve um processo mais elaborado.

O produto de Kronecker, denotado por $\otimes$, é utilizado para criar uma representação das variáveis que pode ser usada na regressão OLS agrupada. O produto de Kronecker de duas matrizes, $A$ de dimensão $(m \times n)$ e $B$ de dimensão $(p \times q)$, é uma matriz de dimensão $(mp \times nq)$ formada por blocos que são o resultado da multiplicação de cada elemento de $A$ pela matriz $B$:

$$
A \otimes B = \begin{bmatrix}
a_{11}B & a_{12}B & \cdots & a_{1n}B \\
a_{21}B & a_{22}B & \cdots & a_{2n}B \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1}B & a_{m2}B & \cdots & a_{mn}B
\end{bmatrix}
$$

A matriz de regressores $\mathcal{X}_t$ pode ser expressa através da criação de um vetor de empilhamento de matrizes $x_t$, onde o vetor $x_t$ empilha o vetor de regressores de cada equação com $x_t = [x_{1t}, x_{2t}, ..., x_{nt}]'$. A matriz de regressão $\mathcal{X}_t$ pode ser escrita como uma combinação do produto de Kronecker e um operador seletor. Usando o produto de Kronecker, podemos construir a matriz $\tilde{\mathcal{X}}_t$ através de uma combinação de matrizes de seleção e de produtos de Kronecker.

Primeiro, definimos a matriz de seleção $S_i$ que seleciona a i-ésima equação do vetor y. Esta matriz de seleção é uma matriz de dimensão $(1 \times n)$, em que o elemento na i-ésima posição da matriz é 1 e todos os demais são 0.

$$
S_i = [0, \ldots, 0, 1, 0, \ldots, 0],
$$

onde o 1 está na i-ésima posição. Usando o produto de Kronecker com as matrizes de seleção $S_i$ e uma matriz identidade $I$ de dimensão $(k_i \times k_i)$, podemos expressar as linhas da matriz $\mathcal{X}_t$ como:

$$
x'_i = (S_i \otimes I_{k_i})x_t.
$$

Para obter $\tilde{\mathcal{X}}_t = L\mathcal{X}_t$,  cada elemento da matriz L será utilizado para combinar as linhas da matriz $\mathcal{X}_t$, gerando as novas variáveis que serão utilizadas na regressão OLS agrupada. Em outras palavras, os elementos da matriz $L$ atuam como pesos que determinam as transformações lineares nas variáveis.

A matriz $\tilde{\mathcal{X}}_t$ é obtida através de transformações lineares em $\mathcal{X}_t$, com os elementos de $L$ atuando como pesos. Isso é, para cada linha $i$ da matriz $L$,  as linhas de  $\mathcal{X}_t$ serão multiplicadas pelos elementos $L_{ij}$. O resultado final é uma matriz de regressores transformada $\tilde{\mathcal{X}}_t$ que é usada na regressão OLS agrupada.

A **transformação linear**  implica que, para a matriz de regressores original $\mathcal{X}_t$, que tem uma estrutura de blocos diagonais,  a matriz transformada $\tilde{\mathcal{X}}_t$ terá elementos que são combinações lineares dos regressores originais, resultando numa matriz com blocos fora da diagonal principal.  A aplicação da matriz $L$ em $\mathcal{X}_t$ acarreta em transformações nas variáveis, que, como vimos, são uma combinação linear das variáveis originais ponderadas pelos elementos da matriz $L$.
A matriz resultante $\tilde{\mathcal{X}}_t = L\mathcal{X}_t$  é utilizada na regressão OLS agrupada para estimar o vetor de parâmetros $\beta$ de forma iterativa, conforme explorado nas seções anteriores.

A estratégia de usar OLS como estágio inicial, como mencionado anteriormente [^1], se revela computacionalmente eficiente. O uso de OLS para obter $\beta(0)$ e $\Omega(0)$ como um ponto de partida ajuda a minimizar o número de iterações necessárias para convergir para o máximo da função de verossimilhança.

### Conclusão
A construção das variáveis para a regressão OLS agrupada em modelos VAR com restrições gerais é um processo sofisticado que envolve álgebra linear avançada, em particular, o uso do produto de Kronecker e transformações lineares. O produto de Kronecker é usado para construir uma representação vetorial das variáveis e os elementos da matriz $L$ são usados para gerar combinações lineares dos regressores,  permitindo que a MLE seja implementada de forma eficiente através de regressões OLS agrupadas. A transformação linear dos regressores via a matriz $L$ é essencial, permitindo que o problema de otimização seja resolvido por uma regressão OLS padrão, com resultados consistentes e eficientes. A implementação correta dessas transformações é um passo crítico para a estimação precisa dos parâmetros em modelos VAR com restrições gerais, sendo de extrema importância a compreensão e o uso adequado da álgebra linear avançada neste contexto.

### Referências
[^1]: *Zellner, A. (1962). An Efficient Method of Estimating Seemingly Unrelated Regressions and Tests for Aggregation Bias. Journal of the American Statistical Association, 57(298), 348–368.*
<!-- END -->
