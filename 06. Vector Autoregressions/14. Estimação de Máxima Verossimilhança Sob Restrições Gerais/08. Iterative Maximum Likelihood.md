## Estimação de Máxima Verossimilhança Sob Restrições Gerais em Modelos VAR

### Introdução
Este capítulo expande o conceito de **estimação de máxima verossimilhança** (MLE) apresentado anteriormente em modelos de vetores autorregressivos (VAR) não restritos, discutindo agora a estimativa sob restrições gerais nos coeficientes. Enquanto a seção anterior focava em sistemas VAR onde cada equação tinha as mesmas variáveis explicativas, esta seção aborda o cenário onde restrições, que não podem ser expressas de forma recursiva por blocos, são impostas aos coeficientes do modelo. Este problema de otimização com restrições é um tema central no contexto de modelos VAR e necessita de uma abordagem mais sofisticada para obter estimativas consistentes e eficientes dos parâmetros. O foco principal desta seção é detalhar o **procedimento iterativo** utilizado na MLE com restrições gerais, que envolve um algoritmo de *pooling* de dados para combinar todas as equações em um único sistema e otimizar o desempenho computacional usando uma estimativa OLS como um estágio inicial para agilizar o processo.

### Conceitos Fundamentais

Em sua forma mais geral, um VAR com restrições nos coeficientes pode ser visto como um sistema de regressões aparentemente não relacionadas, conforme analisado por Zellner (1962) [^1]. Considerando um vetor *$y$* que é descrito por um modelo VAR com restrições, temos:

$y_{1t} = x'_{1t}\beta_1 + \epsilon_{1t}$
$y_{2t} = x'_{2t}\beta_2 + \epsilon_{2t}$
$...$
$y_{nt} = x'_{nt}\beta_n + \epsilon_{nt}$

Aqui, $x_{it}$ é um vetor $(k_i \times 1)$ contendo um termo constante e defasagens das variáveis que aparecem na i-ésima equação do VAR. O sistema é composto por *n* equações, onde $k_i$ denota o número de regressores na i-ésima equação. O objetivo é estimar os parâmetros $\beta_i$ e a matriz de covariância $\Omega$ que maximizam a função de verossimilhança. É importante notar que, diferentemente de um VAR não restrito, onde todas as equações compartilham o mesmo conjunto de variáveis explicativas, em um VAR com restrições gerais, cada equação pode conter um subconjunto diferente de regressores e os coeficientes podem ter relações específicas entre eles.

Como explorado anteriormente, a representação vetorial do modelo VAR torna a abordagem de máxima verossimilhança mais clara. As equações do modelo VAR podem ser escritas em forma matricial, de acordo com [^1]:

$y_t = \mathcal{X}_t \beta + \epsilon_t$,

onde $y_t$ é um vetor $(n \times 1)$ de variáveis endógenas, $\mathcal{X}_t$ é uma matriz $(n \times k)$ de regressores, $\beta$ é um vetor $(k \times 1)$ de coeficientes combinados e $\epsilon_t$ é um vetor $(n \times 1)$ de termos de erro. Aqui, $k = \sum_{i=1}^n k_i$ é o número total de regressores nas *n* equações. Esta representação unifica o modelo VAR em uma única equação vetorial, o que facilita a aplicação de técnicas de otimização para estimar os parâmetros desconhecidos.

A função de verossimilhança, como visto anteriormente [^1], pode ser escrita como:

$$
\mathcal{L}(\beta, \Omega) = - \frac{Tn}{2}\log(2\pi) + \frac{T}{2}\log|\Omega^{-1}| - \frac{1}{2} \sum_{t=1}^T (y_t - \mathcal{X}_t\beta)'\Omega^{-1}(y_t - \mathcal{X}_t\beta)
$$
onde $\mathcal{X}_t$ é uma matriz que contém os regressores correspondentes a cada equação do VAR e $\beta$ é um vetor que contém todos os coeficientes. Esta função representa a probabilidade dos dados observados dado os parâmetros do modelo.

O objetivo da **estimação de máxima verossimilhança (MLE)** é encontrar os valores de $\beta$ e $\Omega$ que maximizam a função de verossimilhança, ou equivalentemente, minimizam a função quadrática:

$$
\sum_{t=1}^T (y_t - \mathcal{X}_t\beta)'\Omega^{-1}(y_t - \mathcal{X}_t\beta).
$$

Para isso, como sugerido em [^1], usamos uma transformação que envolve a decomposição de Cholesky de $\Omega^{-1}$ como $L'L$, onde L é uma matriz triangular inferior:
$$
\sum_{t=1}^T (y_t - \mathcal{X}_t\beta)'\Omega^{-1}(y_t - \mathcal{X}_t\beta) = \sum_{t=1}^T (\tilde{y}_t - \tilde{\mathcal{X}}_t\beta)'(\tilde{y}_t - \tilde{\mathcal{X}}_t\beta)
$$
onde $\tilde{y}_t = Ly_t$ e $\tilde{\mathcal{X}}_t = L\mathcal{X}_t$.
Essa transformação permite que a minimização da função log-verossimilhança, que é o objetivo da MLE, seja realizada através de uma **regressão OLS de $\tilde{y}$ sobre $\tilde{\mathcal{X}}$**, agrupando todas as equações em uma única regressão, conforme mencionado em [^1]. Esta abordagem transforma o problema de otimização com restrições em um problema de regressão OLS padrão, simplificando o processo de estimação. É neste ponto que a escolha de uma estrutura inicial para $\beta$ se torna crucial, e o uso de estimativas de OLS como estágio inicial agiliza o processo iterativo.

A minimização da função é, portanto, feita através de um **algoritmo iterativo**, onde o vetor de parâmetros $\beta$ em um modelo VAR com restrições gerais é estimado por meio de OLS agrupado, em cada iteração. O método consiste na minimização da soma dos quadrados ponderados dos resíduos, onde a matriz de pesos é a inversa da matriz de covariância dos resíduos.

A otimização do processo computacional se dá através de um **método iterativo**, no qual uma estimativa OLS é utilizada como um estágio inicial, conforme sugerido em [^1]. De acordo com as informações fornecidas, o método consiste em usar o resultado das regressões OLS de cada equação do sistema separadamente como um ponto de partida.  Esta estratégia não apenas fornece valores iniciais razoáveis para os parâmetros, como também melhora o desempenho computacional do processo iterativo. O procedimento inicia com estimativas OLS separadas $\beta(0)$ de cada equação do VAR e calcula a matriz de covariância dos resíduos $\hat{\Omega}(0)$. Em seguida, uma matriz $L(0)$ tal que $(L(0))'L(0) = (\hat{\Omega}(0))^{-1}$ é obtida usando a decomposição de Cholesky. As variáveis originais são transformadas para $\tilde{y}_t(0) = L(0)y_t$ e $\tilde{\mathcal{X}}_t(0) = L(0)\mathcal{X}_t$. Após esta transformação, uma regressão OLS agrupada de $\tilde{y}_t(0)$ sobre $\tilde{\mathcal{X}}_t(0)$ fornece uma nova estimativa $\beta(1)$. Este processo é repetido até a convergência, garantindo que a estimativa de máxima verossimilhança seja alcançada de forma eficiente sob as restrições impostas.

A estratégia de estimar primeiro parâmetros sob condições restritas, e posteriormente estimar os parâmetros não restritos com base na estrutura restrita, conforme mencionado em [^1]  fornece uma forma eficiente de otimizar o modelo. Isso pode ser observado nas equações [11.3.13]-[11.3.15] e [11.3.17]-[11.3.18], que mostram como a função de verossimilhança pode ser expressa em termos de parâmetros restritos e não restritos, e como o OLS pode ser aplicado para obter as estimativas de parâmetros.

### Conclusão

A estimativa de máxima verossimilhança sob restrições gerais em modelos VAR envolve um procedimento iterativo que utiliza um algoritmo de *pooling* de dados para estimar conjuntamente os coeficientes do VAR e a matriz de covariância dos resíduos. Ao transformar o problema de otimização com restrições em uma regressão OLS agrupada, este método permite uma implementação computacional mais eficiente. A utilização da fatorização de Cholesky e a representação vetorial do modelo juntamente com a decomposição da função log-verossimilhança transformam um problema complexo em etapas mais simples e eficazes, garantindo que as estimativas de máxima verossimilhança sejam obtidas de forma eficiente e com respeito às restrições impostas. O uso de estimativas OLS como estágio inicial otimiza o desempenho computacional do processo iterativo, garantindo uma solução eficiente para o problema de otimização com restrições.

### Referências
[^1]: *Zellner, A. (1962). An Efficient Method of Estimating Seemingly Unrelated Regressions and Tests for Aggregation Bias. Journal of the American Statistical Association, 57(298), 348–368.*
<!-- END -->
