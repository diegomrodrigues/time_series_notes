## Estimativa de Máxima Verossimilhança dos Coeficientes em Modelos VAR via Regressão OLS

### Introdução
Este capítulo explora em detalhes a obtenção das estimativas de **máxima verossimilhança** (MLE) para os **coeficientes** de um modelo de vetores autorregressivos (VAR), demonstrando como estas estimativas são obtidas por meio de regressões de **mínimos quadrados ordinários** (OLS). Como vimos anteriormente, o método de máxima verossimilhança é uma ferramenta fundamental na estimação de parâmetros em modelos estatísticos, e a MLE é o valor do parâmetro que maximiza a função de verossimilhança [^1]. Este capítulo se aprofunda na aplicação deste método a modelos VAR, destacando a conexão com as técnicas de regressão OLS e demonstrando como as dependências temporais entre as variáveis são capturadas por meio das defasagens. Essa discussão complementa o desenvolvimento anterior e prepara o terreno para análises mais avançadas envolvendo testes de hipóteses e inferências estatísticas.

### O Modelo VAR e a Função de Verossimilhança
Em um modelo VAR, o valor de uma variável no instante *t* ($y_t$) é modelado como uma combinação linear dos seus valores passados mais um termo de erro aleatório $\epsilon_t$ [^2]. Formalmente, a relação fundamental do modelo VAR é dada por:
$$
y_t = c + \Phi_1 y_{t-1} + \Phi_2 y_{t-2} + \dots + \Phi_p y_{t-p} + \epsilon_t
$$
[11.1.3]
onde $c$ é um vetor de constantes, $\Phi_1, \Phi_2, \dots, \Phi_p$ são matrizes de coeficientes autorregressivos, e $\epsilon_t$ é um vetor de ruído branco multivariado com distribuição normal $N(0, \Omega)$.
Para obter as estimativas de máxima verossimilhança dos parâmetros, é necessário maximizar a função de log-verossimilhança, derivada no capítulo anterior [^3]:
$$
\mathcal{L}(\theta) = -(Tn/2) \log(2\pi) + (T/2) \log|\Omega^{-1}| - \frac{1}{2} \sum_{t=1}^{T} (y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t)
$$
[11.1.10]
onde:
$$
x_t = \begin{bmatrix} 1 \\ y_{t-1} \\ y_{t-2} \\ \vdots \\ y_{t-p} \end{bmatrix}
$$
[11.1.5]
e
$$
\Pi' = \begin{bmatrix} c & \Phi_1 & \Phi_2 & \dots & \Phi_p \end{bmatrix}
$$
[11.1.6]
O objetivo agora é encontrar os valores de $\Pi$ que maximizam $\mathcal{L}(\theta)$.

### MLE por Regressão OLS
Conforme derivado no texto, a MLE de $\Pi$, que inclui o termo constante *c* e os coeficientes autorregressivos $\Phi_1, \Phi_2, \dots, \Phi_p$, pode ser obtida por meio de regressões OLS [^4]. Mais especificamente, para a *j*-ésima equação no modelo VAR, a MLE dos coeficientes $\Pi'_j$ pode ser encontrada a partir da regressão de $y_{jt}$ em relação ao vetor $x_t$:
$$
\hat{\Pi}'_j = \left[ \sum_{t=1}^{T} y_{jt}x_t' \right] \left[ \sum_{t=1}^{T} x_tx_t' \right]^{-1}
$$
[11.1.11]
Essa expressão revela que a *j*-ésima linha da matriz $\Pi'$ corresponde aos coeficientes da regressão OLS de $y_{jt}$ sobre o vetor $x_t$. Assim, para cada uma das *n* variáveis em $y_t$, é preciso realizar uma regressão OLS de forma separada.

A regressão OLS em cada equação do VAR captura as dependências temporais entre as variáveis e seus valores passados. Ao incluir um termo constante e *p* defasagens de todas as variáveis no sistema, a regressão OLS leva em consideração a influência das defasagens de cada variável sobre o valor atual da variável em questão. É importante observar que a mesma matriz de regressores, $x_t$, é utilizada em todas as *n* regressões.
É possível também expressar a regressão OLS em termos de toda a matriz $\Pi$ com a seguinte notação
$$
\hat{\Pi} = \left[ \sum_{t=1}^{T} y_tx_t' \right] \left[ \sum_{t=1}^{T} x_tx_t' \right]^{-1}
$$
Onde o resultado da regressão OLS é uma matriz com todas as estimativas dos parâmetros do modelo VAR.

### Interpretação dos Resultados da Regressão OLS
As estimativas obtidas por regressão OLS representam os efeitos marginais das defasagens de cada variável sobre o valor corrente da variável em questão, dados os valores passados de todas as outras variáveis do sistema. Por exemplo, na primeira equação do VAR:
$$
y_{1t} = c_1 + \Phi_{1,1} y_{t-1} + \Phi_{1,2} y_{t-2} + \dots + \Phi_{1,p} y_{t-p} + \epsilon_{1t}
$$
o coeficiente $\Phi_{1,1}$ representa a influência da primeira defasagem da variável 1 sobre o valor corrente da variável 1, dados os valores passados de todas as outras variáveis no sistema. Similarmente, o coeficiente $\Phi_{1,2}$ representa a influência da segunda defasagem da variável 1 sobre o valor corrente da variável 1, e assim por diante.
Para a segunda equação do VAR, teríamos
$$
y_{2t} = c_2 + \Phi_{2,1} y_{t-1} + \Phi_{2,2} y_{t-2} + \dots + \Phi_{2,p} y_{t-p} + \epsilon_{2t}
$$
e, em geral, para a *i*-ésima equação do VAR
$$
y_{it} = c_i + \Phi_{i,1} y_{t-1} + \Phi_{i,2} y_{t-2} + \dots + \Phi_{i,p} y_{t-p} + \epsilon_{it}
$$
Este processo se repete para cada uma das *n* equações do sistema.

### Conexão com a Função de Verossimilhança
A conexão entre a regressão OLS e a MLE reside no fato de que, sob a suposição de que os erros têm distribuição normal, o vetor de parâmetros que maximiza a função de verossimilhança é o mesmo vetor de parâmetros que minimiza a soma dos erros ao quadrado.  Para verificar essa conexão, derivamos a soma dos erros ao quadrado em relação aos parâmetros do modelo:
$$
\sum_{t=1}^{T}(y_{t} - \Pi'x_{t})'(y_{t} - \Pi'x_{t})
$$
A minimização dessa função leva às mesmas estimativas de $\Pi$ obtidas através da maximização da função de log-verossimilhança, dada em [11.1.11] e [11.1.12]. Isso justifica o uso da regressão OLS para a estimação dos parâmetros do modelo VAR.
De forma mais precisa, derivando a função de log-verossimilhança em relação a $\Pi$, temos:
$$
\frac{\partial \mathcal{L}(\theta)}{\partial \Pi} = \Omega^{-1}\sum_{t=1}^{T}(y_{t} - \Pi'x_{t})x'_t = 0
$$
Isolando $\Pi'$, obtemos:
$$
\Pi' = \left[ \sum_{t=1}^{T} y_{t}x_t' \right] \left[ \sum_{t=1}^{T} x_tx_t' \right]^{-1}
$$
que é exatamente a mesma expressão obtida por regressão OLS.

### Implicações para Modelagem e Inferência
A equivalência entre a estimação por máxima verossimilhança e a regressão OLS simplifica significativamente a aplicação prática de modelos VAR, uma vez que as rotinas de regressão OLS são amplamente disponíveis em softwares estatísticos. Além disso, as propriedades estatísticas dos estimadores obtidos por OLS são bem conhecidas e podem ser utilizadas para inferência estatística.

### Conclusão
A obtenção das estimativas de máxima verossimilhança dos coeficientes de um modelo VAR é realizada através de regressões OLS de cada variável em relação ao seu passado, o que captura as dependências temporais das variáveis do sistema. Este capítulo solidifica a conexão entre a MLE e a regressão OLS, simplificando a aplicação de modelos VAR. Com essa compreensão, estamos aptos a avançar para as próximas etapas na modelagem VAR, como testes de hipóteses, análise de causalidade e funções de resposta ao impulso, que dependem destas estimativas de parâmetros.

### Referências
[^1]: "the likelihood function" and the value of 0 that maximizes [11.1.2] as the “maximum likelihood estimate.”"
[^2]: "c + \Phi_1y_{t-1} + \Phi_2y_{t-2} + \cdots + \Phi_py_{t-p} plus a N(0, \Omega) variable."
[^3]:  "A função de verossimilhança para uma amostra completa, condicionada a observações iniciais, é o produto das densidades condicionais individuais, que é fundamental para a estimação de parâmetros."
[11.1.3]:  "c + \Phi_1y_{t-1} + \Phi_2y_{t-2} + \cdots + \Phi_py_{t-p} plus a N(0, \Omega) variable."
[11.1.5]:  "Let $x_t$ denote a vector containing a constant term and p lags of each of the elements of y:"
[11.1.6]:  "Let $\Pi'$ denote the following [n x (np + 1)] matrix: $\Pi'$ = [c  $\Phi_1$  $\Phi_2$ ... $\Phi_p$]."
[11.1.10]: "L($\theta$) =  $\sum_{t=1}^T$ log f($y_t$| $y_{t-1}$, $y_{t-2}$, ..., $y_{t-p+1}$; $\theta$) = - (Tn/2) log(2$\pi$) + (T/2) log|-$\Omega^{-1}$| - (1/2) $\sum_{t=1}^T$ ($y_t$ - $\Pi'x_t$)'$\Omega^{-1}$($y_t$ - $\Pi'x_t$)."
[11.1.11]:   "$\Pi'$ =  [$\sum_{t=1}^T y_tx'_t$][$\sum_{t=1}^T x_tx'_t$]$^{-1}$"
[11.1.12]: " The jth row of $\Pi'$ is  $\hat{\Pi}'_j$ = [$\sum^T_{t=1} y_{jt}x'_t$][$\sum^T_{t=1} x_tx'_t$]$^{-1}$. which is just the estimated coefficient vector from an OLS regression of $y_{jt}$ on $x_t$."
<!-- END -->
