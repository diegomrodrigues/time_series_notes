## A Construção da Função de Verossimilhança para Modelos VAR: Uma Abordagem Detalhada

### Introdução
Este capítulo dedica-se a uma análise aprofundada do cálculo da **função de verossimilhança** para modelos de vetores autorregressivos (VAR). O processo de derivação da função de verossimilhança para modelos VAR é análogo ao utilizado em modelos autorregressivos escalares, com a principal diferença sendo a natureza multivariada das variáveis envolvidas [^1]. A função de verossimilhança é um elemento crucial para a estimação de parâmetros e testes de hipóteses em modelos VAR [^2]. Ao detalharmos a construção dessa função, torna-se mais claro como as dependências temporais entre múltiplas variáveis são modeladas, e como os dados são utilizados para obter as melhores estimativas dos parâmetros. Este capítulo expande a discussão iniciada nos capítulos anteriores [^3], fornecendo uma compreensão completa dos fundamentos teóricos necessários para a aplicação de modelos VAR.

### Derivação da Função de Verossimilhança em Modelos VAR
A construção da função de verossimilhança em modelos VAR inicia-se com a definição da distribuição condicional de $y_t$ dado o seu passado. Como estabelecido anteriormente, em um modelo VAR, o valor de $y_t$ no instante *t* é modelado como uma combinação linear dos seus valores passados mais um termo de erro aleatório $\epsilon_t$ [^4]. Formalmente, essa relação é expressa como:
$$
y_t = c + \Phi_1 y_{t-1} + \Phi_2 y_{t-2} + \dots + \Phi_p y_{t-p} + \epsilon_t
$$
[11.1.3]
onde $c$ é um vetor de constantes, $\Phi_1, \Phi_2, \dots, \Phi_p$ são matrizes de coeficientes autorregressivos, e $\epsilon_t$ é um vetor de ruído branco multivariado com distribuição normal $N(0, \Omega)$.

Para facilitar a manipulação matemática e obter uma expressão mais compacta, define-se o vetor $x_t$:
$$
x_t = \begin{bmatrix} 1 \\ y_{t-1} \\ y_{t-2} \\ \vdots \\ y_{t-p} \end{bmatrix}
$$
[11.1.5]
e a matriz $\Pi'$:
$$
\Pi' = \begin{bmatrix} c & \Phi_1 & \Phi_2 & \dots & \Phi_p \end{bmatrix}
$$
[11.1.6]
Com essas definições, podemos reescrever o modelo VAR de forma mais concisa:
$$
y_t = \Pi'x_t + \epsilon_t
$$
A **média condicional** de $y_t$, dado o seu passado, torna-se $\Pi'x_t$, e a distribuição condicional de $y_t$ é normal multivariada:
$$
y_t | y_{t-1}, y_{t-2}, \dots, y_{t-p+1} \sim N(\Pi'x_t, \Omega)
$$
[11.1.7]
A **densidade condicional** correspondente a esta distribuição é dada por:

$$
f(y_t | y_{t-1}, y_{t-2}, \dots, y_{t-p+1}; \theta) = (2\pi)^{-n/2} |\Omega|^{-1/2} \exp\left(-\frac{1}{2}(y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t)\right)
$$
[11.1.8]
onde $\theta$ representa o conjunto de parâmetros a serem estimados ($\Pi$ e $\Omega$).

A **função de verossimilhança** para a amostra completa, condicionada às observações iniciais $y_0, y_{-1}, \dots, y_{-p+1}$, é dada pelo produto das densidades condicionais individuais:
$$
f(y_T, y_{T-1}, \dots, y_1 | y_0, y_{-1}, \dots, y_{-p+1}; \theta) = \prod_{t=1}^{T} f(y_t | y_{t-1}, y_{t-2}, \dots, y_{t-p+1}; \theta)
$$
[11.1.9]
Essa expressão representa a probabilidade de observar a sequência completa de dados, dadas as observações iniciais e os parâmetros do modelo VAR.
A **função de log-verossimilhança** é obtida tomando o logaritmo da função de verossimilhança:
$$
\mathcal{L}(\theta) = \sum_{t=1}^{T} \log f(y_t | y_{t-1}, y_{t-2}, \dots, y_{t-p+1}; \theta)
$$
[11.1.10]
Substituindo a expressão da densidade condicional, obtemos:
$$
\mathcal{L}(\theta) = -(Tn/2) \log(2\pi) + (T/2) \log|\Omega^{-1}| - \frac{1}{2} \sum_{t=1}^{T} (y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t)
$$
[11.1.10]
Essa é a função de log-verossimilhança para um modelo VAR, e é o ponto de partida para a estimação dos parâmetros $\Pi$ e $\Omega$.

### Comparação com Modelos Autorregressivos Escalares
O cálculo da função de verossimilhança para modelos VAR é similar ao processo para modelos autorregressivos escalares. No entanto, existem algumas diferenças cruciais:

1.  **Natureza Multivariada:** Em modelos VAR, $y_t$ e $\epsilon_t$ são vetores, enquanto em modelos autorregressivos escalares são escalares. Isso implica que, em modelos VAR, $\Omega$ é uma matriz de covariância, e não uma simples variância escalar.

2.  **Estrutura dos Parâmetros:** Em modelos VAR, os coeficientes autorregressivos $\Phi_1, \Phi_2, \dots, \Phi_p$ são matrizes, e não escalares. Isso reflete a interação e dependência entre as múltiplas variáveis do sistema.

3.  **Complexidade Computacional:** Devido à natureza multivariada do modelo VAR, o cálculo das estimativas de máxima verossimilhança (MLE) envolve uma maior complexidade computacional.

Apesar dessas diferenças, os princípios subjacentes à construção da função de verossimilhança são os mesmos: condicionar o valor presente aos valores passados, expressar a densidade condicional e calcular o produto dessas densidades ao longo do tempo.

### Implicações Práticas e Teóricas
A construção detalhada da função de verossimilhança é essencial para diversas aplicações e implicações teóricas em modelos VAR:

1.  **Estimação por Máxima Verossimilhança (MLE):**  A função de verossimilhança é maximizada em relação aos parâmetros do modelo ($\Pi$ e $\Omega$) para obter estimativas consistentes e eficientes desses parâmetros.

2.  **Inferência Estatística:**  A forma da função de verossimilhança permite derivar as distribuições assintóticas dos estimadores MLE e realizar testes de hipóteses sobre os parâmetros do modelo.

3.  **Seleção de Modelos:** A função de verossimilhança é utilizada em conjunto com critérios de informação como AIC e BIC para comparar modelos com diferentes números de defasagens e selecionar a melhor estrutura para o modelo VAR.

4.  **Simulação:** A função de verossimilhança define a distribuição a partir da qual amostras podem ser geradas através de simulação.

### Conclusão

O cálculo da função de verossimilhança para modelos VAR, embora similar ao processo para modelos AR escalares, envolve uma complexidade adicional devido à natureza multivariada das variáveis envolvidas. A função de verossimilhança, expressa como o produto das densidades condicionais individuais, é o ponto de partida para a estimação e inferência em modelos VAR. A compreensão detalhada da estrutura dessa função é fundamental para a aplicação correta e interpretação significativa dos resultados obtidos. Com isso, os fundamentos necessários para os testes de hipóteses, análise de causalidade e funções de impulso-resposta são estabelecidos.

### Referências
[^1]:  "The likelihood function is calculated in the same way as for a scalar auto-regression."
[^2]: "...the likelihood function and the value of 0 that maximizes [11.1.2] as the “max-imum likelihood estimate.”
[^3]: "Este capítulo aprofunda a discussão sobre modelos de vetores autorregressivos (VAR), com foco na derivação da **função de verossimilhança** e na obtenção da **estimativa de máxima verossimilhança** (MLE)."
[11.1.3]: "c + $\Phi_1y_{t-1}$ + $\Phi_2y_{t-2}$ + $\cdots$ + $\Phi_py_{t-p}$ plus a N(0, $\Omega$) variable."
[11.1.5]: "Let $x_t$ denote a vector containing a constant term and p lags of each of the elements of y:"
[11.1.6]:  "Let $\Pi'$ denote the following [n x (np + 1)] matrix: $\Pi'$ = [c  $\Phi_1$  $\Phi_2$ ... $\Phi_p$]."
[11.1.7]:  "$y_t | y_{t-1}, y_{t-2}, \ldots, y_{t-p+1} \sim N(\Pi'x_t, \Omega)$."
[11.1.8]:  "$f(y_t | y_{t-1}, y_{t-2}, \ldots, y_{t-p+1}; \theta) = (2\pi)^{-n/2}|\Omega|^{-1/2} \exp[(-1/2)(y_{t} - \Pi'x_{t})'\Omega^{-1}(y_{t} - \Pi'x_{t})]$."
[11.1.9]: "The joint density of observations 1 through t conditioned on $y_0, y_{-1},..., y_{-p+1}$ satisfies $f(y_T, y_{T-1}, \ldots, y_1 | y_0, y_{-1}, \ldots, y_{-p+1}; \theta) = \prod_{t=1}^{T} f(y_t | y_{t-1}, y_{t-2}, \ldots, y_{t-p+1}; \theta)$."
[11.1.10]: "$\mathcal{L}(\theta) =  \sum_{t=1}^{T} \log f(y_t| y_{t-1}, y_{t-2}, \ldots, y_{t-p+1}; \theta) = - (Tn/2) \log(2\pi) + (T/2) \log|-\Omega^{-1}| - (1/2) \sum_{t=1}^{T} (y_{t} - \Pi'x_{t})'\Omega^{-1}(y_{t} - \Pi'x_{t})$."
<!-- END -->
