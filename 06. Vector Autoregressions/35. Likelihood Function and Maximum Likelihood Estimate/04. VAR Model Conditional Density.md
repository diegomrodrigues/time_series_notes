## A Densidade Condicional em Modelos VAR: Uma Análise Detalhada

### Introdução
Este capítulo foca na análise detalhada da **densidade condicional** para a *t*-ésima observação em um modelo de vetores autorregressivos (VAR). Conforme abordado nos capítulos anteriores, a densidade condicional é um componente fundamental na construção da função de verossimilhança e, consequentemente, na estimação de parâmetros [^1]. Ao compreender a forma dessa densidade, que se revela uma distribuição normal multivariada, estabelecemos as bases para modelar a probabilidade de um conjunto de observações dadas as informações do passado. Este estudo complementa o desenvolvimento anterior sobre a função de verossimilhança e prepara o terreno para a análise de modelos VAR com mais profundidade.

### A Densidade Condicional em Detalhe

A densidade condicional para a *t*-ésima observação em um modelo VAR descreve a distribuição de probabilidade do vetor $y_t$, dado o conhecimento dos valores passados de $y$. Conforme estabelecido anteriormente [^2], a relação fundamental é expressa como:
$$
y_t = c + \Phi_1 y_{t-1} + \Phi_2 y_{t-2} + \dots + \Phi_p y_{t-p} + \epsilon_t
$$
[11.1.3]
onde $\epsilon_t$ é um vetor de ruído branco multivariado com distribuição normal $N(0, \Omega)$. Utilizando a notação compacta introduzida em [^3], podemos reescrever essa relação como:
$$
y_t = \Pi'x_t + \epsilon_t
$$
onde:
$$
x_t = \begin{bmatrix} 1 \\ y_{t-1} \\ y_{t-2} \\ \vdots \\ y_{t-p} \end{bmatrix}
$$
[11.1.5]
e
$$
\Pi' = \begin{bmatrix} c & \Phi_1 & \Phi_2 & \dots & \Phi_p \end{bmatrix}
$$
[11.1.6]

A média condicional de $y_t$, dado $x_t$, é dada por $\Pi'x_t$, e a distribuição condicional de $y_t$ é:
$$
y_t | y_{t-1}, y_{t-2}, \dots, y_{t-p+1} \sim N(\Pi'x_t, \Omega)
$$
[11.1.7]
Essa notação compacta facilita a representação da média condicional e da variância condicional do processo.

A **densidade condicional** de $y_t$, dado os valores passados, é então expressa como uma distribuição normal multivariada:
$$
f(y_t | y_{t-1}, y_{t-2}, \dots, y_{t-p+1}; \theta) = (2\pi)^{-n/2} |\Omega|^{-1/2} \exp\left(-\frac{1}{2}(y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t)\right)
$$
[11.1.8]
Nessa expressão:
*   $(2\pi)^{-n/2}$ é um fator de normalização, onde $n$ é a dimensão do vetor $y_t$.
*   $|\Omega|^{-1/2}$ é o determinante da matriz de covariância $\Omega$, elevado a -1/2, e representa a dispersão da distribuição.
*   $\exp(-\frac{1}{2}(y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t))$ é o núcleo da distribuição normal multivariada, e quantifica a probabilidade de observar $y_t$ dada a média condicional e a variância condicional.

Essa densidade condicional descreve a probabilidade de observar um valor específico de $y_t$, dados os valores anteriores e os parâmetros do modelo. Ela é uma função de $y_t$, de $\Pi$, e de  $\Omega$, representando um conceito essencial na construção da função de verossimilhança.

### Componentes da Densidade Condicional
A densidade condicional é uma função da diferença entre o valor observado de $y_t$ e a média condicional $\Pi'x_t$, ponderada pela matriz de covariância $\Omega$. Os principais componentes dessa densidade incluem:
*   **Média Condicional (Π'x<sub>t</sub>):**  Representa a melhor estimativa linear de $y_t$, dada a informação passada. Os coeficientes $\Pi$ determinam como os valores passados de $y$ influenciam o valor atual.
*   **Matriz de Covariância (Ω):**  Descreve a variância dos resíduos $\epsilon_t$ e as covariâncias entre os resíduos das diferentes equações do VAR. Ela quantifica a incerteza associada às previsões do modelo.
*   **Fator Exponencial:** O núcleo da distribuição normal, que atribui maiores probabilidades a valores de $y_t$ mais próximos da média condicional e menores probabilidades para valores mais distantes.

A forma da densidade condicional, que é uma normal multivariada, implica em uma série de resultados importantes. Por exemplo, a média condicional de $y_t$ é linear em relação aos valores passados, e a variância condicional é constante, assumindo que os parâmetros do modelo não variam com o tempo.

### Implicações para a Estimação e Inferência
A densidade condicional é o bloco construtor fundamental para a função de verossimilhança, que é definida como o produto das densidades condicionais ao longo do tempo [^4]. Ao maximizar essa função de verossimilhança, obtemos as estimativas de máxima verossimilhança (MLE) dos parâmetros do modelo, $\Pi$ e $\Omega$. Assim, a forma da densidade condicional tem implicações diretas para:

*   **Estimação de Parâmetros:**  A forma normal multivariada implica que as estimativas de máxima verossimilhança podem ser obtidas usando regressões de mínimos quadrados ordinários (OLS), como vimos em capítulos anteriores [^5].
*   **Testes de Hipóteses:**  A forma normal da densidade possibilita o uso de testes estatísticos, como o teste da razão de verossimilhança e testes de Wald, para verificar hipóteses sobre os parâmetros do modelo.
*   **Construção de Intervalos de Confiança:**  O conhecimento da distribuição condicional das observações permite a construção de intervalos de confiança para as previsões e parâmetros do modelo.

### Conclusão
A densidade condicional da *t*-ésima observação em um modelo VAR é uma distribuição normal multivariada, cuja forma é determinada pelos parâmetros do modelo, $\Pi$ e $\Omega$. Ela quantifica a probabilidade de observar um valor de $y_t$, dados os seus valores passados e os parâmetros do modelo. A compreensão da densidade condicional é fundamental para a construção da função de verossimilhança, a qual, por sua vez, é essencial para a estimação de parâmetros e a realização de inferências sobre o modelo VAR. Este tópico estabelece as bases para a análise detalhada de modelos VAR e a implementação de testes de hipóteses. O próximo passo será a análise da distribuição assintótica dos estimadores.

### Referências
[^1]: "The likelihood function is calculated in the same way as for a scalar auto-regression."
[^2]: "c + Φ₁y<sub>t-1</sub> + Φ₂y<sub>t-2</sub> + ··· + Φ<sub>p</sub>y<sub>t-p</sub> plus a N(0, Ω) variable."
[^3]: "Let x<sub>t</sub> denote a vector containing a constant term and p lags of each of the elements of y:"
[11.1.3]: "c + Φ₁y<sub>t-1</sub> + Φ₂y<sub>t-2</sub> + ··· + Φ<sub>p</sub>y<sub>t-p</sub> plus a N(0, Ω) variable."
[11.1.5]: "Let x<sub>t</sub> denote a vector containing a constant term and p lags of each of the elements of y:"
[11.1.6]: "Let Π' denote the following [n x (np + 1)] matrix: Π' = [c  Φ₁  Φ₂ ... Φ<sub>p</sub>]."
[11.1.7]:  "y<sub>t</sub> | y<sub>t-1</sub>, y<sub>t-2</sub>, ..., y<sub>t-p+1</sub> ~ N(Π'x<sub>t</sub>, Ω)."
[11.1.8]: "f(y<sub>t</sub> | y_{t-1}, y_{t-2}, ..., y_{t-p+1}; θ) = (2\pi)^{-n/2}|\Omega|^{-1/2} \exp[(-1/2)(y_{t} - \Pi'x_{t})'\Omega^{-1}(y_{t} - \Pi'x_{t})]."
[11.1.9]: "The joint density of observations 1 through t conditioned on y<sub>0</sub>, y<sub>-1</sub>,..., y<sub>-p+1</sub> satisfies f(y<sub>T</sub>, y<sub>T-1</sub>, ..., y<sub>1</sub> | y<sub>0</sub>, y<sub>-1</sub>, ..., y<sub>-p+1</sub>; θ) = \prod_{t=1}^{T} f(y_{t} | y_{t-1}, y_{t-2}, ..., y_{t-p+1}; θ)."
[11.1.10]: "L(θ) =  \sum_{t=1}^{T} \log f(y_{t}| y_{t-1}, y_{t-2}, ..., y_{t-p+1}; θ) = - (Tn/2) \log(2\pi) + (T/2) \log|-\Omega^{-1}| - (1/2) \sum_{t=1}^{T} (y_{t} - \Pi'x_{t})'\Omega^{-1}(y_{t} - \Pi'x_{t})."
[11.1.25]:  "L($\Omega$, $\hat{\Pi}$) = - (Tn/2) log(2$\pi$) + (T/2) log|$\Omega^{-1}$| - (1/2) $\sum_{t=1}^{T}$ $\hat{\epsilon}$'$_t$$\Omega^{-1}$$\hat{\epsilon}$$_t$."
<!-- END -->
