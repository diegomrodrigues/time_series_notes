## A Função de Verossimilhança e a Estimativa de Máxima Verossimilhança em Modelos VAR

### Introdução
Este capítulo aprofunda a discussão sobre modelos de vetores autorregressivos (VAR), com foco na derivação da **função de verossimilhança** e na obtenção da **estimativa de máxima verossimilhança** (MLE). O conceito de função de verossimilhança, introduzido brevemente em [^1], é fundamental para a estimação de parâmetros em modelos estatísticos. Expandiremos essa ideia para o contexto de VAR, detalhando o processo de construção da função e como ela leva à MLE. Os conceitos aqui desenvolvidos complementam a discussão anterior, proporcionando uma base sólida para os próximos tópicos do capítulo.

### Conceitos Fundamentais

A **função de verossimilhança** para um modelo VAR é construída de maneira análoga à de um modelo autorregressivo escalar, conforme mencionado em [^1]. Condicional aos valores de *y* observados até o instante *t-1*, o valor de *y* no instante *t* é igual a uma constante, mais uma variável com distribuição *$N(0, \Omega)$*. Essa relação é expressa como:

$$
y_t = c + \Phi_1 y_{t-1} + \Phi_2 y_{t-2} + \dots + \Phi_p y_{t-p} + \epsilon_t, \quad \epsilon_t \sim N(0, \Omega)
$$
[11.1.3]

Para simplificar a notação, define-se um vetor *$x_t$* que contém um termo constante e *p* defasagens de cada elemento de *y*:

$$
x_t = \begin{bmatrix} 1 \\ y_{t-1} \\ y_{t-2} \\ \vdots \\ y_{t-p} \end{bmatrix}
$$
[11.1.5]

Com isso, *$x_t$* é um vetor de dimensão ((np+1)x1). Também se define uma matriz *$\Pi'$* de dimensão (nx(np+1)):

$$
\Pi' = \begin{bmatrix} c & \Phi_1 & \Phi_2 & \dots & \Phi_p \end{bmatrix}
$$
[11.1.6]

A média condicional de *$y_t$* pode ser expressa como *$\Pi'x_t$*. A notação permite reescrever [11.1.4] de forma mais compacta:

$$
y_t | y_{t-1}, y_{t-2}, \dots, y_{t-p+1} \sim N(\Pi'x_t, \Omega)
$$
[11.1.7]

A **densidade condicional** da *$t$*-ésima observação é dada por:

$$
f(y_t | y_{t-1}, y_{t-2}, \dots, y_{t-p+1}; \theta) = (2\pi)^{-n/2} |\Omega|^{-1/2} \exp\left(-\frac{1}{2}(y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t)\right)
$$
[11.1.8]

A **densidade conjunta** das observações de 1 até *T*, condicionada a *$y_0, y_{-1}, ..., y_{-p+1}$*, é o produto das densidades condicionais individuais:

$$
f(y_T, y_{T-1}, \dots, y_1 | y_0, y_{-1}, \dots, y_{-p+1}; \theta) = \prod_{t=1}^{T} f(y_t | y_{t-1}, y_{t-2}, \dots, y_{t-p+1}; \theta)
$$
[11.1.9]

A **função de log-verossimilhança** da amostra é encontrada substituindo [11.1.8] em [11.1.9] e tomando o logaritmo:

$$
\mathcal{L}(\theta) = \sum_{t=1}^{T} \log f(y_t | y_{t-1}, y_{t-2}, \dots, y_{t-p+1}; \theta) \\
= -(Tn/2) \log(2\pi) + (T/2) \log|\Omega^{-1}| - \frac{1}{2} \sum_{t=1}^{T} (y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t)
$$
[11.1.10]

O objetivo da **estimação de máxima verossimilhança** (MLE) é encontrar os valores dos parâmetros *$\Pi$* e *$\Omega$* que maximizam a função de verossimilhança. A MLE de *$\Pi$* é dada por:

$$
\hat{\Pi}' = \left[\sum_{t=1}^{T} y_t x_t'\right] \left[\sum_{t=1}^{T} x_t x_t'\right]^{-1}
$$
[11.1.11]
Essa expressão pode ser vista como um análogo amostral da projeção linear populacional de *$y_t$* sobre um termo constante e *$x_t$*. A j-ésima linha de *$\Pi'$* é:

$$
\hat{\Pi}'_j = \left[\sum_{t=1}^{T} y_{jt} x_t'\right] \left[\sum_{t=1}^{T} x_t x_t'\right]^{-1}
$$
[11.1.12]

que é o vetor de coeficientes estimados por meio de uma regressão de mínimos quadrados ordinários (OLS) de *$y_{jt}$* em *$x_t$*. Assim, as estimativas de máxima verossimilhança dos coeficientes para a j-ésima equação de um VAR são obtidas por meio de uma regressão OLS de *$y_{jt}$* em um termo constante e *p* defasagens de todas as variáveis do sistema [^1].

Para verificar [11.1.11], o somatório no último termo de [11.1.10] pode ser reescrito como:

$$
\sum_{t=1}^{T} (y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t) = \sum_{t=1}^{T} [(y_t - \hat{\Pi}'x_t) + (\hat{\Pi}'x_t - \Pi'x_t)]'\Omega^{-1}[(y_t - \hat{\Pi}'x_t) + (\hat{\Pi}'x_t - \Pi'x_t)]
$$
$$
= \sum_{t=1}^{T} [\hat{\epsilon}_t + (\hat{\Pi} - \Pi)'x_t]'\Omega^{-1}[\hat{\epsilon}_t + (\hat{\Pi} - \Pi)'x_t]
$$
[11.1.13]
onde *$\hat{\epsilon}_t$* representa o resíduo amostral da regressão OLS de *$y_t$* em *$x_t$*:
$$
\hat{\epsilon}_t = y_t - \hat{\Pi}'x_t
$$
[11.1.14]

Expandindo a expressão [11.1.13], temos:

$$
\sum_{t=1}^{T} (y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t) = \sum_{t=1}^{T} \hat{\epsilon}_t'\Omega^{-1}\hat{\epsilon}_t + 2 \sum_{t=1}^{T} \hat{\epsilon}_t' \Omega^{-1} (\hat{\Pi} - \Pi)'x_t + \sum_{t=1}^{T} x_t'(\hat{\Pi} - \Pi) \Omega^{-1} (\hat{\Pi} - \Pi)'x_t
$$
[11.1.15]
O termo do meio em [11.1.15] é um escalar, portanto, pode ser alterado utilizando o operador "trace":

$$
\sum_{t=1}^{T} 2 \hat{\epsilon}_t' \Omega^{-1} (\hat{\Pi} - \Pi)'x_t =  trace \left[ \sum_{t=1}^{T} 2 \hat{\epsilon}_t' \Omega^{-1} (\hat{\Pi} - \Pi)'x_t \right] = trace \left[ 2 \Omega^{-1} (\hat{\Pi} - \Pi)'\sum_{t=1}^{T} x_t \hat{\epsilon}_t'\right]
$$
[11.1.16]
Os resíduos amostrais de uma regressão OLS são ortogonais às variáveis explicativas, o que significa que  $\sum_{t=1}^{T} x_t \hat{\epsilon}_t' = 0$. Logo, [11.1.16] é identicamente zero, e [11.1.15] se simplifica para:

$$
\sum_{t=1}^{T} (y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t) = \sum_{t=1}^{T} \hat{\epsilon}_t'\Omega^{-1}\hat{\epsilon}_t + \sum_{t=1}^{T} x_t'(\hat{\Pi} - \Pi) \Omega^{-1} (\hat{\Pi} - \Pi)'x_t
$$
[11.1.17]
Como *$\Omega$* é uma matriz definida positiva, *$\Omega^{-1}$* também é. Definindo o vetor *(nx1)*:

$$
x_t^* = (\hat{\Pi} - \Pi)'x_t
$$

O último termo em [11.1.17] torna-se:

$$
\sum_{t=1}^{T} x_t'(\hat{\Pi} - \Pi) \Omega^{-1} (\hat{\Pi} - \Pi)'x_t = \sum_{t=1}^{T} (x_t^*)' \Omega^{-1} x_t^*
$$

Essa expressão é positiva para qualquer sequência {*$x_t$*}, exceto quando *$x_t^*=0$* para todo *$t$*. O menor valor que [11.1.17] pode assumir é atingido quando *$x_t^*=0$*, o que ocorre quando *$\Pi = \hat{\Pi}$*. Portanto, [11.1.10] é maximizado quando *$\Pi = \hat{\Pi}$*, o que estabelece que as regressões OLS fornecem as estimativas de máxima verossimilhança dos coeficientes de um vetor autorregressivo.

### Conclusão

A função de verossimilhança é uma ferramenta essencial para a estimação de parâmetros em modelos VAR. A derivação da função de verossimilhança e a obtenção da estimativa de máxima verossimilhança (MLE) demonstram a forte ligação entre os conceitos de regressão OLS e a estimação de modelos VAR. Através da construção da função de verossimilhança e da sua análise, compreende-se que a MLE de $\hat{\Pi}$, nos modelos VAR, pode ser obtida utilizando regressões OLS, o que facilita a aplicação do método. O próximo passo natural é avaliar as propriedades estatísticas dessas estimativas, e isso será feito nos próximos capítulos. Este tópico foi essencial para a compreensão dos fundamentos da estimação em modelos VAR, e a discussão continuará com a análise da distribuição assintótica das estimativas.

### Referências
[^1]:  "...the likelihood function and the value of 0 that maximizes [11.1.2] as the “max-imum likelihood estimate.” The likelihood function is calculated in the same way as for a scalar auto-regression."
[11.1.3]: "c + Φ₁y<sub>t-1</sub> + Φ₂y<sub>t-2</sub> + ··· + Φ<sub>p</sub>y<sub>t-p</sub> plus a N(0, Ω) variable."
[11.1.5]: "Let x<sub>t</sub> denote a vector containing a constant term and p lags of each of the elements of y:"
[11.1.6]:  "Let Π' denote the following [n x (np + 1)] matrix: Π' = [c  Φ₁  Φ₂ ... Φ<sub>p</sub>]."
[11.1.7]: "y<sub>t</sub> | y<sub>t-1</sub>, y<sub>t-2</sub>, ..., y<sub>t-p+1</sub> ~ N(Π'x<sub>t</sub>, Ω)."
[11.1.8]: "f(y<sub>t</sub> | y<sub>t-1</sub>, y<sub>t-2</sub>, ..., y<sub>t-p+1</sub>; θ) = (2π)<sup>-n/2</sup>|Ω|<sup>-1/2</sup> exp[(-1/2)(y<sub>t</sub> - Π'x<sub>t</sub>)'Ω<sup>-1</sup>(y<sub>t</sub> - Π'x<sub>t</sub>)]."
[11.1.9]: "The sample log likelihood is found by substituting [11.1.8] into [11.1.9] and taking logs:"
[11.1.10]: "L(θ) =  ∑<sub>t=1</sub><sup>T</sup> log f(y<sub>t</sub>| y<sub>t-1</sub>, y<sub>t-2</sub>, ..., y<sub>t-p+1</sub>; θ) = - (Tn/2) log(2π) + (T/2) log|-Ω<sup>-1</sup>| - (1/2) ∑<sub>t=1</sub><sup>T</sup> (y<sub>t</sub> - Π'x<sub>t</sub>)'Ω<sup>-1</sup>(y<sub>t</sub> - Π'x<sub>t</sub>)."
[11.1.11]: " Consider first the MLE of Π, which contains the constant term c and auto-regressive coefficients Φ<sub>j</sub>. This turns out to be given by Π' = [∑<sup>T</sup><sub>t=1</sub> y<sub>t</sub>x'<sub>t</sub>][∑<sup>T</sup><sub>t=1</sub> x<sub>t</sub>x'<sub>t</sub>]<sup>-1</sup>."
[11.1.12]: "The jth row of Π' is  $\hat{\Pi}'_j$ = [∑<sup>T</sup><sub>t=1</sub> y<sub>jt</sub>x'<sub>t</sub>][∑<sup>T</sup><sub>t=1</sub> x<sub>t</sub>x'<sub>t</sub>]<sup>-1</sup>."
[11.1.13]: " To verify [11.1.11], write the sum appearing in the last term in [11.1.10] as ∑<sub>t=1</sub><sup>T</sup>(y<sub>t</sub> - Π'x<sub>t</sub>)'Ω<sup>-1</sup>(y<sub>t</sub> - Π'x<sub>t</sub>)  =  ∑<sub>t=1</sub><sup>T</sup>[ (y<sub>t</sub> - $\hat{\Pi}'$x<sub>t</sub>) + ($\hat{\Pi}'$x<sub>t</sub> - Π'x<sub>t</sub>) ]'Ω<sup>-1</sup>[(y<sub>t</sub> - $\hat{\Pi}'$x<sub>t</sub>) + ($\hat{\Pi}'$x<sub>t</sub> - Π'x<sub>t</sub>)]."
[11.1.14]: "where the jth element of the (n x 1) vector $\hat{\epsilon}$<sub>t</sub> is the sample residual for observation t from an OLS regression of y<sub>jt</sub> on x<sub>t</sub>:  $\hat{\epsilon}$<sub>t</sub> = y<sub>t</sub> - $\hat{\Pi}'$x<sub>t</sub>."
[11.1.15]: "Expression [11.1.13] can be expanded as   ∑<sub>t=1</sub><sup>T</sup>(y<sub>t</sub> - Π'x<sub>t</sub>)'Ω<sup>-1</sup>(y<sub>t</sub> - Π'x<sub>t</sub>) =  ∑<sub>t=1</sub><sup>T</sup>$\hat{\epsilon}$'<sub>t</sub>Ω<sup>-1</sup>$\hat{\epsilon}$<sub>t</sub> + 2∑<sub>t=1</sub><sup>T</sup>$\hat{\epsilon}$'<sub>t</sub>Ω<sup>-1</sup>($\hat{\Pi}$ - Π)'x<sub>t</sub>  + ∑<sup>T</sup><sub>t=1</sub> x'<sub>t</sub>($\hat{\Pi}$ - Π)Ω<sup>-1</sup>($\hat{\Pi}$ - Π)'x<sub>t</sub>."
[11.1.16]: "by applying the "trace" operator:   ∑<sub>t=1</sub><sup>T</sup> 2 $\hat{\epsilon}$'<sub>t</sub> Ω<sup>-1</sup> ($\hat{\Pi}$ - Π)'x<sub>t</sub> = trace[ 2 Ω<sup>-1</sup> ($\hat{\Pi}$ - Π)'∑<sub>t=1</sub><sup>T</sup> x<sub>t</sub>$\hat{\epsilon}$'<sub>t</sub>]."
[11.1.17]: "The sample log likelihood is found by substituting [11.1.8] into [11.1.9] and taking logs: ∑<sub>t=1</sub><sup>T</sup>(y<sub>t</sub> - Π'x<sub>t</sub>)'Ω<sup>-1</sup>(y<sub>t</sub> - Π'x<sub>t</sub>)  =  ∑<sub>t=1</sub><sup>T</sup> $\hat{\epsilon}$'<sub>t</sub>Ω<sup>-1</sup>$\hat{\epsilon}$<sub>t</sub> +  ∑<sub>t=1</sub><sup>T</sup> x'<sub>t</sub>($\hat{\Pi}$ - Π)Ω<sup>-1</sup>($\hat{\Pi}$ - Π)'x<sub>t</sub>."
<!-- END -->
