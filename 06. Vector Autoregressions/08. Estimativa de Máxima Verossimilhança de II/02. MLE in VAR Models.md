## Estimativa de Máxima Verossimilhança de Π em Modelos VAR
### Introdução
Em continuidade à discussão sobre modelos de vetores autorregressivos (VAR), este capítulo aprofunda a obtenção da **estimativa de máxima verossimilhança (MLE)** para a matriz de parâmetros Π. Como vimos anteriormente, os modelos VAR são ferramentas poderosas para analisar as inter-relações dinâmicas entre múltiplas variáveis [^1]. A MLE de Π é um passo fundamental para a estimação precisa dos parâmetros do modelo, permitindo inferências estatísticas válidas e previsões confiáveis. Este capítulo irá explorar a derivação matemática da MLE de Π, demonstrando que ela corresponde à solução de regressões de mínimos quadrados ordinários (OLS) separadas para cada equação do sistema VAR.

### Conceitos Fundamentais
A **função de verossimilhança condicional** para um modelo VAR é dada por [^1]:
$$
f(y_t | y_{t-1}, y_{t-2}, \ldots, y_{t-p}; \theta) \sim N(\Pi'x_t, \Omega)
$$
onde $x_t$ é um vetor contendo um termo constante e $p$ lags de cada elemento de $y$, e Π' é a matriz de parâmetros que contém o termo constante 'c' e os coeficientes autorregressivos Φ [^1].
O objetivo é encontrar a **MLE de Π**, denotada como $\hat{\Pi}$, que maximiza a função de verossimilhança, ou equivalentemente, minimiza o negativo do log-verossimilhança. O log da função de verossimilhança amostral é dado por [^1]:
$$
L(\theta) = -\frac{Tn}{2} \log(2\pi) + \frac{T}{2} \log|\Omega^{-1}| - \frac{1}{2} \sum_{t=1}^{T} (y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t)
$$
A MLE de Π é obtida maximizando esta função com respeito a Π. De acordo com [^1], a **MLE de Π** é dada por:
$$
\hat{\Pi}' = \left[ \sum_{t=1}^{T} y_t x_t' \right] \left[ \sum_{t=1}^{T} x_t x_t' \right]^{-1}
$$
Esta expressão [11.1.11] pode ser vista como o **análogo amostral da projeção linear da população** de $y_t$ em uma constante e $x_t$, como discutido em [4.1.23] [^1]. A *j-ésima linha de $\hat{\Pi}'$ é dada por*:
$$
\hat{\pi}'_j = \left[ \sum_{t=1}^{T} y_{jt} x_t' \right] \left[ \sum_{t=1}^{T} x_t x_t' \right]^{-1}
$$
Esta expressão [11.1.12] representa o vetor de coeficientes estimados de uma **regressão OLS** de $y_{jt}$ em $x_t$. Portanto, a MLE dos coeficientes para a *j-ésima equação de um VAR* são encontradas através de uma **regressão OLS de $y_{jt}$ em uma constante e $p$ lags de todas as variáveis do sistema** [^1]. Para verificar a equação [11.1.11], podemos reescrever a soma que aparece no último termo do log da função de verossimilhança [11.1.10] da seguinte forma [^1]:
$$
\sum_{t=1}^{T} (y_t - \Pi'x_t)' \Omega^{-1} (y_t - \Pi'x_t) = \sum_{t=1}^{T} (y_t - \hat{\Pi}'x_t + \hat{\Pi}'x_t - \Pi'x_t)' \Omega^{-1} (y_t - \hat{\Pi}'x_t + \hat{\Pi}'x_t - \Pi'x_t)
$$
Expandindo:
$$
\sum_{t=1}^{T} [\hat{\epsilon}_t + (\hat{\Pi} - \Pi)'x_t]' \Omega^{-1} [\hat{\epsilon}_t + (\hat{\Pi} - \Pi)'x_t]
$$
onde $\hat{\epsilon}_t = y_t - \hat{\Pi}'x_t$ é o resíduo amostral da regressão OLS de $y_t$ em $x_t$ [^1].  Expandindo novamente:
$$
 \sum_{t=1}^{T} \hat{\epsilon}_t' \Omega^{-1} \hat{\epsilon}_t + 2 \sum_{t=1}^{T} \hat{\epsilon}_t' \Omega^{-1} (\hat{\Pi} - \Pi)'x_t + \sum_{t=1}^{T} x_t' (\hat{\Pi} - \Pi) \Omega^{-1} (\hat{\Pi} - \Pi)'x_t
$$
O termo do meio da equação acima é um escalar, e pode ser reescrito usando o operador 'trace' [^1]:
$$
2 \sum_{t=1}^{T} \hat{\epsilon}_t' \Omega^{-1} (\hat{\Pi} - \Pi)'x_t = trace\left[ 2 \sum_{t=1}^{T} x_t'(\hat{\Pi} - \Pi) \Omega^{-1}\hat{\epsilon}_t \right]
$$
Ainda, os resíduos de uma regressão OLS são ortogonais às variáveis explicativas, isso significa que  $\sum_{t=1}^{T} x_t \hat{\epsilon}_t' = 0$, e também $\sum_{t=1}^{T} \hat{\epsilon}_t x_t' = 0$. Então, o termo do meio da equação acima se torna 0. Assim, a expressão se simplifica para [^1]:
$$
\sum_{t=1}^{T} \hat{\epsilon}_t' \Omega^{-1} \hat{\epsilon}_t + \sum_{t=1}^{T} x_t' (\hat{\Pi} - \Pi) \Omega^{-1} (\hat{\Pi} - \Pi)'x_t
$$
Como $\Omega$ é uma matriz definida positiva, $\Omega^{-1}$ também é, assim, o termo  $x_t' (\hat{\Pi} - \Pi) \Omega^{-1} (\hat{\Pi} - \Pi)'x_t$ é não negativo, e será 0 quando $\hat{\Pi} = \Pi$. Portanto, o valor mínimo para a expressão ocorre quando $\hat{\Pi} = \Pi$, o que confirma que a estimativa obtida por regressão OLS fornece a MLE da matriz de coeficientes Π [^1].

### Conclusão
Este capítulo demonstrou detalhadamente como a **MLE de Π** em modelos VAR pode ser obtida através de uma abordagem de regressão OLS. A equivalência entre a MLE e as regressões OLS simplifica o processo de estimação, tornando os modelos VAR acessíveis para análises empíricas e aplicações práticas. A compreensão dessas relações matemáticas fornece uma base sólida para avançar nos próximos tópicos, que envolvem testes de hipóteses e inferências sobre as relações dinâmicas entre as variáveis do sistema. A seguir, será discutida a estimação da matriz de covariância dos erros ($\Omega$) e os testes de hipóteses associados aos parâmetros do VAR, expandindo as ferramentas analíticas para modelos VAR.

### Referências
[^1]:  Trechos do contexto fornecido, incluindo as equações (11.1.3), (11.1.4), (11.1.5), (11.1.6), (11.1.7), (11.1.8), (11.1.10), (11.1.11), (11.1.12), (11.1.13), (11.1.14), (11.1.15), (11.1.16) e (11.1.17)
<!-- END -->
