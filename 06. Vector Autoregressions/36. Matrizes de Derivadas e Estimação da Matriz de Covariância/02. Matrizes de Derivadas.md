## Matrizes de Derivadas e Estimação da Matriz de Covariância

### Introdução
Este capítulo avança na discussão de **Vector Autoregressions (VAR)**, explorando em profundidade os métodos de estimação, com foco na obtenção de estimativas de máxima verossimilhança (MLE) para os parâmetros do modelo, particularmente a **matriz de covariância** ($\Omega$). Como vimos anteriormente, em modelos VAR,  a compreensão da estrutura de dependência entre as variáveis é crucial para uma modelagem precisa [^1]. O tópico atual detalha o uso de **derivadas de matrizes** e do operador **traço** na estimação da matriz de covariância $\Omega$ em modelos VAR não restritos. Este aprofundamento  complementa os tópicos anteriores ao fornecer um entendimento matemático mais sólido de como os estimadores são derivados.

### Conceitos Fundamentais

Em modelos VAR, a função de verossimilhança desempenha um papel crucial na estimação dos parâmetros. A função de verossimilhança condicional para a $t$-ésima observação, dada pelas observações até $t-1$, é expressa como [^1]:

$$ f(y_t | y_{t-1}, y_{t-2}, \ldots, y_{t-p}; \theta) = (2\pi)^{-n/2} |\Omega|^{-1/2} \exp\left(-\frac{1}{2}(y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t)\right) $$

onde $y_t$ é um vetor de variáveis, $x_t$ é um vetor contendo um termo constante e $p$ lags de cada elemento de $y$, $\Pi$ é a matriz de parâmetros e $\Omega$ é a matriz de covariância dos resíduos. A função de log-verossimilhança amostral é então dada por [^1]:

$$
L(\theta) = -\frac{Tn}{2}\log(2\pi) + \frac{T}{2}\log|\Omega^{-1}| - \frac{1}{2} \sum_{t=1}^T (y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t)
$$

Para encontrar o **MLE de $\Pi$**, podemos utilizar uma transformação que simplifica a minimização da soma de quadrados. Através do desenvolvimento apresentado em [^1], o estimador de $\Pi$ é dado por:
$$
\hat{\Pi}' = \left[ \sum_{t=1}^T y_t x_t' \right] \left[ \sum_{t=1}^T x_t x_t' \right]^{-1}
$$
Essa estimativa é equivalente à aplicação de regressão OLS em cada equação do VAR [^1]. O próximo passo é obter o estimador de máxima verossimilhança para a matriz de covariância $\Omega$.

Para derivar o **MLE de $\Omega$**, é necessário calcular a derivada da função de log-verossimilhança em relação aos elementos de $\Omega^{-1}$. As seguintes ferramentas de **cálculo matricial** são cruciais [^1]:

1.  A derivada de uma forma quadrática $x'Ax$ em relação a um elemento $a_{ij}$ de uma matriz $A$ (onde $A$ é uma matriz *não simétrica* e não restrita) é dada por:

    $$\frac{\partial x'Ax}{\partial a_{ij}} = x_i x_j$$ [^1]

     Esta derivada, quando expressa em termos matriciais para todos os $n^2$ elementos da matriz, é dada por:
      $$\frac{\partial x'Ax}{\partial A} = xx'$$ [^1]

2.  A derivada do logaritmo do determinante de uma matriz $A$ (onde $A$ é não simétrica e não restrita com determinante positivo) em relação a um elemento $a_{ij}$ é dada por:
    $$
    \frac{\partial \log|A|}{\partial a_{ij}} = (A^{-1})_{ji}
    $$ [^1]
    onde $(A^{-1})_{ji}$ é o elemento da linha $j$ e coluna $i$ de $A^{-1}$. Expressando em termos matriciais:
    $$
    \frac{\partial \log|A|}{\partial A} = (A^{-1})'
    $$ [^1]

Utilizando essas derivadas, podemos calcular a derivada parcial da log-verossimilhança em relação a $\Omega^{-1}$. Usando as derivadas em [^1] (11.1.20) e [^1] (11.1.22) obtemos:

$$
\frac{\partial L(\Omega, \hat{\Pi})}{\partial \Omega^{-1}} = \frac{T}{2} \Omega - \frac{1}{2} \sum_{t=1}^T \hat{\epsilon}_t \hat{\epsilon}_t'
$$
onde $\hat{\epsilon}_t$ são os resíduos do modelo, $ \hat{\epsilon}_t = y_t - \hat{\Pi}'x_t$.  Maximizando a log-verossimilhança implica em igualar a derivada a zero, e então, o **MLE de $\Omega$** é dado por:
$$
\hat{\Omega} = \frac{1}{T} \sum_{t=1}^T \hat{\epsilon}_t \hat{\epsilon}_t'
$$ [^1]
que é a matriz de covariância amostral dos resíduos.

Para verificar que o estimador de $\Omega$ é o MLE, é necessário reescrever a soma que aparece no último termo da função de verossimilhança e usar o operador **traço**.  Como demonstrado em [^1], a soma pode ser expressa como:

$$
\sum_{t=1}^T (y_t - \Pi'x_t)' \Omega^{-1} (y_t - \Pi'x_t) = \sum_{t=1}^T \hat{\epsilon}_t' \Omega^{-1} \hat{\epsilon}_t + \sum_{t=1}^T \hat{x}_t' (\hat{\Pi} - \Pi) \Omega^{-1} (\hat{\Pi} - \Pi)' \hat{x}_t
$$

O operador **traço** de uma matriz é a soma dos elementos na diagonal principal, e possui a propriedade crucial de que  $\text{trace}(AB) = \text{trace}(BA)$ quando o produto $AB$ e $BA$ estão bem definidos.  A importância do operador traço nesse contexto deriva do fato que, se $a$ é um escalar, então $a = \text{trace}(a)$. Usando o operador traço, podemos reescrever a soma  como [^1]:

$$
\sum_{t=1}^T (y_t - \Pi'x_t)' \Omega^{-1} (y_t - \Pi'x_t) = \text{trace} \left[ \sum_{t=1}^T \hat{\epsilon}_t' \Omega^{-1} \hat{\epsilon}_t \right]
$$
e ao aplicar as propriedades do traço em [^1], obtemos:

$$
\text{trace} \left[\sum_{t=1}^T \hat{\epsilon}_t' \Omega^{-1} \hat{\epsilon}_t \right] = \text{trace} \left[\Omega^{-1} \sum_{t=1}^T \hat{\epsilon}_t \hat{\epsilon}_t'\right]
$$
Usando que os resíduos da regressão OLS são ortogonais às variáveis explicativas, temos que o segundo termo da soma se anula. Portanto, a função de log-verossimilhança se simplifica para [^1]:
$$
L(\theta) = - \frac{Tn}{2} \log(2\pi) + \frac{T}{2} \log|\Omega^{-1}| - \frac{1}{2} \text{trace} \left[\Omega^{-1} \sum_{t=1}^T \hat{\epsilon}_t \hat{\epsilon}_t'\right]
$$
Agora, vamos focar no cálculo de derivadas para encontrar o **MLE de $\Omega$**. A estratégia é encontrar o máximo da log-verossimilhança.  Para isso, precisamos calcular a derivada da função de log-verossimilhança em relação aos elementos de $\Omega^{-1}$. De acordo com as ferramentas de cálculo matricial descritas acima, obtemos:
$$
\frac{\partial L(\theta)}{\partial \Omega^{-1}} = \frac{T}{2} \Omega - \frac{1}{2} \sum_{t=1}^T \hat{\epsilon}_t \hat{\epsilon}_t'
$$
Igualando a zero para obter o máximo, chegamos novamente ao resultado do estimador de máxima verossimilhança para $\Omega$:
$$
\hat{\Omega} = \frac{1}{T} \sum_{t=1}^T \hat{\epsilon}_t \hat{\epsilon}_t'
$$
Como $\Omega$ é uma matriz definida positiva, $\Omega^{-1}$ também o é. Essa condição assegura que  $\sum_{t=1}^T x_t'(\hat{\Pi} - \Pi) \Omega^{-1} (\hat{\Pi} - \Pi)' x_t \ge 0$ (para qualquer sequência de $\hat{x}_t$), o que significa que ao escolher $\hat{\Pi}=\Pi$,  minimizamos o último termo em [^1] (11.1.17) (a soma dos quadrados ponderados pelos elementos da matriz $\Omega^{-1}$).  Portanto, a maximização da log-verossimilhança leva a resultados que podem ser alcançados usando as regressões de OLS para cada equação do VAR [^1].
A matriz $\Omega$ estimada também é simétrica e definida positiva, e os elementos da diagonal principal da matriz representam a variância dos erros de cada equação no VAR, enquanto os termos fora da diagonal representam a covariância entre os erros das diferentes equações.

### Conclusão
Este capítulo detalhou o uso de derivadas de matrizes e do operador traço para obter o estimador de máxima verossimilhança para a matriz de covariância $\Omega$ em modelos VAR. As derivadas de formas quadráticas e logaritmos de determinantes de matrizes são essenciais para derivar as condições de primeira ordem para maximizar a função de log-verossimilhança. O operador traço oferece uma forma eficiente de manipular expressões envolvendo matrizes, simplificando as demonstrações e derivações. A combinação dessas técnicas com o conceito de  regressão OLS permite uma compreensão matemática rigorosa da obtenção dos estimadores dos parâmetros de um modelo VAR não restrito. Este conhecimento se baseia nos conceitos fundamentais de VAR e da função de verossimilhança que foram apresentados em capítulos anteriores.
O próximo passo natural é a análise de **VARs restritos**, que exploraremos nos próximos capítulos.

### Referências
[^1]:  [11.1.2], [11.1.3], [11.1.4], [11.1.5], [11.1.6], [11.1.7], [11.1.8], [11.1.9], [11.1.10], [11.1.11], [11.1.12], [11.1.13], [11.1.14], [11.1.15], [11.1.16], [11.1.17], [11.1.20], [11.1.21], [11.1.22]
<!-- END -->
