## A Matriz que Maximiza a Verossimilhança é Simétrica e Definida Positiva em Modelos VAR

### Introdução
Este capítulo tem como objetivo demonstrar formalmente que a **matriz** $\Omega$ que maximiza a função de verossimilhança em modelos **Vector Autoregressions (VAR)** é não apenas simétrica, mas também **definida positiva**. Este resultado é fundamental na análise de séries temporais, pois garante que as estimativas da matriz de covariância sejam coerentes com a teoria estatística, e construi sobre as discussões de **derivadas de matrizes**, do operador **traço** e da ligação entre **máxima verossimilhança (MLE)** e **mínimos quadrados (OLS)** em capítulos anteriores [^1]. Ao provar que a matriz $\hat{\Omega}$, obtida pela maximização da função de log-verossimilhança, é simétrica e definida positiva, asseguramos que as estimativas resultantes sejam estatisticamente válidas e consistentes.

### Conceitos Fundamentais
Como vimos em capítulos anteriores, a função de log-verossimilhança para um modelo VAR é dada por [^1]:

$$
L(\theta) = -\frac{Tn}{2}\log(2\pi) + \frac{T}{2}\log|\Omega^{-1}| - \frac{1}{2} \sum_{t=1}^T (y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t)
$$

onde $y_t$ é o vetor de variáveis endógenas, $x_t$ é o vetor de variáveis explicativas, $\Pi$ é a matriz de coeficientes e $\Omega$ é a matriz de covariância dos resíduos. Os estimadores de máxima verossimilhança são obtidos maximizando esta função, como já detalhado nos capítulos anteriores [^1].

O foco deste capítulo é a matriz $\Omega$, que representa a variabilidade e a dependência entre os resíduos do modelo VAR. Já mostramos que o estimador de máxima verossimilhança de $\Omega$ é dado por [^1]:
$$
\hat{\Omega} = \frac{1}{T} \sum_{t=1}^T \hat{\epsilon}_t \hat{\epsilon}_t'
$$
onde $\hat{\epsilon}_t = y_t - \hat{\Pi}'x_t$ são os resíduos do modelo e $\hat{\Pi}$ é o estimador de máxima verossimilhança (equivalente a OLS) para a matriz de coeficientes $\Pi$.

Para que este estimador seja válido, a matriz $\hat{\Omega}$ deve ser **simétrica** e **definida positiva**. Uma matriz simétrica é aquela em que seus elementos $a_{ij}$ e $a_{ji}$ são iguais. Uma matriz definida positiva é aquela em que para qualquer vetor não nulo $z$, a forma quadrática $z'Az$ é sempre estritamente positiva.  Estas propriedades asseguram que a matriz de covariância de um modelo VAR seja uma representação válida das variâncias e covariâncias das variáveis do modelo, de acordo com a teoria estatística.

### Demonstração da Simetria e da Definição Positiva de $\hat{\Omega}$

1.  **Simetria:**
    Para demonstrar que $\hat{\Omega}$ é simétrica, precisamos mostrar que $\hat{\Omega} = \hat{\Omega}'$. O estimador de $\Omega$ é dado por:
    $$
    \hat{\Omega} = \frac{1}{T} \sum_{t=1}^T \hat{\epsilon}_t \hat{\epsilon}_t'
    $$
    Tomando a transposta de $\hat{\Omega}$, obtemos:
    $$
    \hat{\Omega}' = \left( \frac{1}{T} \sum_{t=1}^T \hat{\epsilon}_t \hat{\epsilon}_t' \right)' = \frac{1}{T} \sum_{t=1}^T (\hat{\epsilon}_t \hat{\epsilon}_t')' = \frac{1}{T} \sum_{t=1}^T (\hat{\epsilon}_t')' \hat{\epsilon}_t' = \frac{1}{T} \sum_{t=1}^T \hat{\epsilon}_t \hat{\epsilon}_t' = \hat{\Omega}
    $$
    pois $\hat{\epsilon}_t$ é um vetor e $(\hat{\epsilon}_t')'=\hat{\epsilon}_t$ e $(\hat{\epsilon}_t \hat{\epsilon}_t')' = \hat{\epsilon}_t \hat{\epsilon}_t'$. Isso demonstra que $\hat{\Omega}$ é uma matriz simétrica.

2.  **Definição Positiva:**
    Para demonstrar que $\hat{\Omega}$ é definida positiva, precisamos mostrar que para qualquer vetor não nulo $z$, $z'\hat{\Omega}z > 0$. Usando o estimador $\hat{\Omega}$, temos:
    $$
    z'\hat{\Omega}z = z' \left( \frac{1}{T} \sum_{t=1}^T \hat{\epsilon}_t \hat{\epsilon}_t' \right) z = \frac{1}{T} \sum_{t=1}^T z' \hat{\epsilon}_t \hat{\epsilon}_t' z
    $$
   Reescrevendo, obtemos:
   $$ z'\hat{\Omega}z = \frac{1}{T} \sum_{t=1}^T (z' \hat{\epsilon}_t) ( \hat{\epsilon}_t' z) = \frac{1}{T} \sum_{t=1}^T  ( \hat{\epsilon}_t' z )' ( \hat{\epsilon}_t' z )= \frac{1}{T} \sum_{t=1}^T \| \hat{\epsilon}_t'z \|^2$$
   onde $\| \hat{\epsilon}_t'z \|^2$ é o quadrado da norma do vetor $(\hat{\epsilon}_t'z)$.  Como a norma de um vetor é sempre não negativa, o quadrado da norma é sempre não negativa.   Como estamos somando quadrados de normas para todas as observações,  a expressão é sempre maior ou igual a zero.  Além disso, se assumirmos que os resíduos são não nulos para, pelo menos, uma observação, então a soma será estritamente positiva.

    Portanto, para qualquer vetor não nulo $z$, temos $z'\hat{\Omega}z > 0$, demonstrando que $\hat{\Omega}$ é uma matriz definida positiva. Esta propriedade é crucial para assegurar a validade estatística das estimativas em modelos VAR.  O fato de a matriz de covariância ser definida positiva garante que as variâncias das variáveis sejam sempre positivas e que as combinações lineares das variáveis sejam consistentes com a teoria estatística.

### Conclusão

Este capítulo demonstrou formalmente que o estimador de máxima verossimilhança da matriz de covariância $\Omega$ em modelos VAR, dado por $\hat{\Omega} = \frac{1}{T} \sum_{t=1}^T \hat{\epsilon}_t \hat{\epsilon}_t'$, é simétrico e definido positivo. Estas propriedades são essenciais para a validade das estimativas e para a consistência da análise de modelos econométricos.  A demonstração da simetria e da definição positiva da matriz, usando o conceito de forma quadrática, o operador transposto e as propriedades de normas de vetores, reforça a compreensão dos fundamentos matemáticos da estimação em modelos VAR, com base nos resultados apresentados nos capítulos anteriores. Este resultado estabelece um marco importante para a discussão de testes de hipóteses e análise de modelos VAR, fornecendo uma base sólida para os próximos capítulos.

### Referências
[^1]: [11.1.27], [11.1.28], [11.1.29], [11.1.30]
<!-- END -->
