## A Derivada do Determinante de uma Matriz e sua Importância em Modelos VAR

### Introdução

Este capítulo expande a discussão sobre modelos **Vector Autoregressions (VAR)**, aprofundando o conceito de **derivada do determinante de uma matriz** e sua relevância na estimação de parâmetros em modelos econométricos. Construindo sobre os conceitos de **derivadas de formas quadráticas**, **matrizes de derivadas** e o **operador traço** abordados em capítulos anteriores [^1], este capítulo explora especificamente como a derivada do determinante de uma matriz é utilizada na derivação de estimadores de máxima verossimilhança (MLE) para modelos VAR não restritos, com um foco na matriz de covariância $\Omega$. O objetivo é oferecer uma compreensão profunda de como as ferramentas de cálculo matricial são aplicadas na prática para otimizar a função de verossimilhança e obter estimativas consistentes e eficientes para os parâmetros do modelo VAR.

### Conceitos Fundamentais

Como vimos anteriormente, a função de log-verossimilhança para um modelo VAR é dada por [^1]:

$$
L(\theta) = -\frac{Tn}{2}\log(2\pi) + \frac{T}{2}\log|\Omega^{-1}| - \frac{1}{2} \sum_{t=1}^T (y_t - \Pi'x_t)'\Omega^{-1}(y_t - \Pi'x_t)
$$

onde $y_t$ é o vetor de variáveis endógenas, $x_t$ é o vetor de variáveis explicativas (incluindo lags), $\Pi$ é a matriz de coeficientes e $\Omega$ é a matriz de covariância dos resíduos. O termo $\log|\Omega^{-1}|$ é crucial e seu tratamento exige o conhecimento da derivada do determinante de uma matriz.

A derivada do logaritmo do determinante de uma matriz $A$ em relação a um elemento $a_{ij}$ de $A$ (onde $A$ é uma matriz não simétrica e não restrita com determinante positivo) é dada por [^1]:
$$
\frac{\partial \log|A|}{\partial a_{ij}} = (A^{-1})_{ji}
$$
onde $(A^{-1})_{ji}$ é o elemento da linha $j$ e coluna $i$ da matriz inversa de $A$. Em termos matriciais, esta derivada é expressa como:
$$
\frac{\partial \log|A|}{\partial A} = (A^{-1})'
$$
Este resultado é essencial para a derivação do MLE para a matriz de covariância $\Omega$ em modelos VAR. A necessidade desta derivada surge quando tratamos do termo $\frac{T}{2}\log|\Omega^{-1}|$ na função de log-verossimilhança. Ao buscar o máximo da função de log-verossimilhança, derivamos esse termo em relação à matriz de covariância, e o resultado desta derivada é diretamente influenciado pela derivada do logaritmo do determinante de uma matriz.

Para derivar este resultado, é útil relembrar a fórmula para o determinante de uma matriz $A$ (equação [A.4.10] no Apêndice A do livro):
$$ |A| = \sum_{j=1}^n (-1)^{i+j} a_{ij} |A_{ij}| $$
onde $|A_{ij}|$ é o determinante da submatriz formada pela remoção da $i$-ésima linha e $j$-ésima coluna de $A$.  A derivada do determinante de $A$ em relação ao elemento $a_{ij}$ é:
$$ \frac{\partial |A|}{\partial a_{ij}} = (-1)^{i+j} |A_{ij}| $$

A derivada do logaritmo do determinante é dada por:
$$ \frac{\partial \log|A|}{\partial a_{ij}} = \frac{1}{|A|} \frac{\partial |A|}{\partial a_{ij}} = \frac{1}{|A|} (-1)^{i+j} |A_{ij}| $$
que corresponde ao elemento $(j, i)$ da inversa de $A$, ou seja, $(A^{-1})_{ji}$. Expressando em termos matriciais, temos:
$$ \frac{\partial \log|A|}{\partial A} = (A^{-1})' $$

### Aplicação na Estimação de Máxima Verossimilhança
Na estimação de modelos VAR, a matriz $\Omega^{-1}$ aparece dentro do logaritmo e na forma quadrática na função de log-verossimilhança. Para obter o estimador de máxima verossimilhança para $\Omega$, precisamos calcular as derivadas parciais da função de log-verossimilhança em relação a $\Omega^{-1}$ e igualá-las a zero. A derivada do termo $\frac{T}{2} \log|\Omega^{-1}|$ com relação a $\Omega^{-1}$, utilizando as ferramentas descritas acima é:
$$
\frac{\partial}{\partial \Omega^{-1}} \left( \frac{T}{2} \log|\Omega^{-1}| \right) = \frac{T}{2} (\Omega^{-1})^{-1'} = \frac{T}{2} \Omega
$$
Combinando este resultado com a derivada do termo quadrático e igualando a zero, como demonstrado em [^1], obtemos o estimador de máxima verossimilhança para $\Omega$:
$$
\hat{\Omega} = \frac{1}{T} \sum_{t=1}^T \hat{\epsilon}_t \hat{\epsilon}_t'
$$
onde $\hat{\epsilon}_t = y_t - \hat{\Pi}'x_t$ são os resíduos do modelo VAR.
O resultado para $\hat{\Pi}$ como apresentado em capítulos anteriores é obtido por minimização da soma de quadrados, o que é equivalente a fazer OLS em cada equação do VAR [^1].

Este resultado destaca a importância da derivada do determinante de uma matriz na derivação de estimadores de máxima verossimilhança em modelos econométricos. A derivada do determinante é usada quando se calcula as condições de primeira ordem da função de log-verossimilhança. As propriedades de cálculo matricial nos permitem simplificar as expressões, facilitando a derivação dos estimadores.

O uso consistente dessas derivadas e do operador traço (como visto em capítulos anteriores) permite derivar os estimadores de máxima verossimilhança de forma sistemática e elegante.  Esses resultados enfatizam como as ferramentas do cálculo matricial se tornam indispensáveis para otimização e estimação de modelos econométricos.

### Conclusão

Este capítulo detalhou o conceito de derivada do determinante de uma matriz e sua aplicação na estimação de parâmetros de modelos VAR. A derivada do logaritmo do determinante de uma matriz desempenha um papel central na derivação do estimador de máxima verossimilhança para a matriz de covariância $\Omega$.  As ferramentas do cálculo matricial, combinadas com o operador traço, fornecem uma estrutura matemática rigorosa para a compreensão da estimação em modelos VAR, construindo sobre os conceitos previamente estabelecidos em tópicos anteriores.  O próximo passo na construção da teoria de VARs será o estudo da estimação de modelos VARs com restrições.

### Referências
[^1]: [11.1.20], [11.1.21], [11.1.22], [11.1.23], [11.1.24]
<!-- END -->
