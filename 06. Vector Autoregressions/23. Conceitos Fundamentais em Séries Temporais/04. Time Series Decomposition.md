## Decomposição de Séries Temporais: Uma Análise Detalhada

### Introdução

Em continuidade aos estudos sobre séries temporais, este capítulo se concentrará na **decomposição de séries temporais**, uma técnica fundamental que visa separar uma série temporal em suas componentes constituintes: **tendência**, **sazonalidade** e **resíduo**. Como os capítulos anteriores já abordaram cada um desses componentes individualmente[^2], este capítulo aprofundará a metodologia de decomposição, explorando diferentes métodos e sua relevância, particularmente no contexto de modelos Vetores Auto-regressivos (VAR), onde o entendimento das interações entre os componentes é crucial para uma modelagem eficaz [^1]. A análise da decomposição em si permite entender melhor o comportamento da série temporal, isolando os fatores que influenciam suas variações.

### Metodologias de Decomposição de Séries Temporais

A decomposição de séries temporais é uma ferramenta essencial para analisar dados que evoluem no tempo. Ao isolar os componentes da série, é possível construir modelos mais precisos e interpretar os resultados com mais clareza. A escolha da metodologia de decomposição depende da natureza da série temporal e dos objetivos da análise.

**1. Decomposição Clássica:**

A decomposição clássica, já introduzida no capítulo anterior, é um método intuitivo que separa a série temporal em três componentes: tendência (*T*), sazonalidade (*S*) e resíduo (*R*). A decomposição clássica pode ser aditiva ou multiplicativa.

*   **Decomposição Aditiva:** Assume que as componentes se somam para formar a série temporal:

    $$y_t = T_t + S_t + R_t$$

    Nesse caso, assume-se que a componente sazonal tem uma amplitude constante ao longo do tempo. A tendência é estimada por uma média móvel ou um modelo de regressão. A sazonalidade é estimada através de médias móveis sobre os valores da série em cada período sazonal. Os resíduos são calculados subtraindo as componentes de tendência e sazonalidade da série original.

*   **Decomposição Multiplicativa:** Assume que as componentes se multiplicam:

    $$y_t = T_t \times S_t \times R_t$$

    Na decomposição multiplicativa, assume-se que a amplitude da componente sazonal varia proporcionalmente ao nível da tendência. A tendência é estimada da mesma forma que na decomposição aditiva, mas a sazonalidade é calculada como uma razão entre os valores da série e a tendência estimada.  Os resíduos são obtidos dividindo a série original pelas componentes de tendência e sazonalidade.

    A escolha entre modelos aditivos ou multiplicativos depende das características da série. Se a amplitude das flutuações sazonais variar com o tempo, o modelo multiplicativo pode ser mais adequado.

**2. Decomposição STL (Seasonal-Trend decomposition using Loess):**

A decomposição STL é uma técnica mais robusta e flexível, baseada na aplicação iterativa de um filtro Loess (Locally Estimated Scatterplot Smoothing) para obter uma estimativa da tendência e da sazonalidade. O método STL tem a vantagem de permitir sazonalidade que varia ao longo do tempo, sendo, portanto, mais flexível do que a decomposição clássica.

*   **Processo Iterativo:** O STL decompõe a série temporal através de um processo iterativo. Primeiro, estima-se uma componente de tendência. Em seguida, a série com tendência removida é utilizada para estimar a componente sazonal. A série original é então ajustada pela remoção da tendência e sazonalidade. O processo é repetido até que os componentes convirjam.
*   **Flexibilidade:** O STL é capaz de lidar com sazonalidades que mudam ao longo do tempo e resiste bem a outliers.
*  **Componentes:**  O modelo STL é aditivo e divide a série em três componentes:
$$y_t = T_t + S_t + R_t$$
Onde $T_t$ é a componente de tendência, $S_t$ é a componente sazonal e $R_t$ a componente de resíduo.

**3. Decomposição por Filtros de Hodrick-Prescott:**

O filtro de Hodrick-Prescott (HP) é um método para decompor uma série temporal em componentes de tendência e resíduo, ideal para separar as flutuações de curto prazo das tendências de longo prazo. Ele não isola uma componente sazonal. O filtro HP minimiza a seguinte função:
$$\sum_{t=1}^T (y_t - \tau_t)^2 + \lambda \sum_{t=2}^{T-1} \left[ (\tau_{t+1} - \tau_t) - (\tau_t - \tau_{t-1}) \right]^2$$
Onde $y_t$ é a série original, $\tau_t$ é a componente de tendência e $\lambda$ é um parâmetro que controla o grau de suavização da tendência. Valores altos de $\lambda$  resultam em uma tendência mais suave e um valor menor  de $\lambda$ permite que a tendência capture também os ciclos de médio prazo.

A desvantagem do filtro HP é que ele não é projetado para isolar a sazonalidade e gera uma componente de tendência que pode ser sensível a outliers.

**4. Modelos Estruturais de Séries Temporais:**

Modelos estruturais de séries temporais, ou modelos *State-Space*, usam uma abordagem mais sofisticada para decompor a série em várias componentes. Esses modelos são particularmente eficazes para modelar tendências e sazonalidades variáveis ao longo do tempo:

*   **Componentes de Estado:** A série temporal é modelada através de um modelo de estado, composto por equações de estado e equações de observação. A evolução da tendência e da sazonalidade é modelada por meio de variáveis de estado que seguem processos estocásticos específicos.
*   **Componentes:** O modelo pode decompor a série em:
    *  **Nível (Tendência):** Variação do nível base da série.
    *  **Inclinação (Tendência):** Variação da taxa de mudança da tendência.
    * **Sazonalidade:** Variações periódicas repetitivas.
    *  **Ruído:** Variações não explicadas pelas outras componentes.

    Os parâmetros do modelo são estimados através do Filtro de Kalman, o que permite lidar com componentes que se alteram ao longo do tempo e também lidar com valores ausentes na série temporal.

*   **Flexibilidade:** Modelos estruturais são altamente flexíveis e permitem incorporar informações prévias sobre as propriedades dos componentes da série temporal.

**5. Análise de Componentes Independentes (ICA):**

A análise de componentes independentes (ICA) é uma técnica que pode ser usada para decompor uma série temporal em fontes ou componentes independentes, onde a independência estatística é um critério para separação. A análise ICA é útil quando as componentes não são necessariamente ortogonais e podem não ser identificadas por métodos de decomposição clássica. A abordagem ICA assume que a série temporal é uma mistura linear de componentes independentes, procurando separá-los utilizando algoritmos de otimização. Embora menos comum em comparação com outras metodologias, a análise ICA representa uma abordagem alternativa para decomposição, principalmente se a suposição de independência de componentes for plausível.

### Relevância da Decomposição para Modelos VAR

A decomposição de séries temporais tem importância fundamental na análise e modelagem de Vetores Auto-regressivos (VAR):

*   **Tratamento da Não-Estacionariedade:** A decomposição permite tratar explicitamente a não-estacionariedade das séries temporais por meio da remoção da tendência e sazonalidade, tornando as séries mais adequadas para modelagem VAR.
*   **Melhora da Interpretabilidade:** Ao decompor as séries, os modelos VAR podem ser interpretados de forma mais clara, com o foco na análise das relações entre os resíduos e nas interdependências de curto prazo.
*  **Remoção de Sazonalidade:** A remoção da sazonalidade antes da modelagem VAR permite que o modelo se concentre nos padrões dinâmicos das séries, não sendo confundidos por efeitos sazonais.
*  **Análise do Impacto de Componentes:**  Ao decompor as séries, os modelos VAR podem ser utilizados para entender o impacto das componentes na dinâmica da série.
*  **Previsões mais acuradas:** Ao decompor séries temporais, é possível criar modelos preditivos mais precisos em todos os níveis (tendência, sazonalidade, resíduo), que podem ser combinados para melhorar a previsão da série total.

A escolha do método de decomposição adequado deve considerar as características de cada série e os objetivos da modelagem VAR.

### Conclusão

A decomposição de séries temporais é uma técnica essencial para separar e analisar as diferentes componentes que influenciam o comportamento de uma série temporal, desde a modelagem de tendências lineares e não lineares, até o tratamento de sazonalidades complexas. As metodologias, que vão da decomposição clássica ao uso de modelos estruturais e análise espectral, permitem uma abordagem flexível e robusta, adequada a diferentes tipos de dados. Em modelos VAR, a aplicação de uma técnica de decomposição adequada é fundamental para a obtenção de resultados robustos e confiáveis. A análise de cada componente de forma separada e a compreensão de como eles interagem podem melhorar significativamente a precisão das previsões e a capacidade de obter *insights* a partir de dados de séries temporais.

### Referências

[^1]: "...a vector containing a constant term and p lags of each
of the elements of y..."
[^2]:  "A não-estacionariedade, por outro lado, sugere que essas propriedades variam, exigindo técnicas de tratamento antes da modelagem."
## Análise de Causalidade de Granger Bivariada
### Introdução
Em continuidade ao conceito de **vetores autorregressivos (VAR)** apresentado no capítulo anterior, este capítulo explora a relação de causalidade entre variáveis, focando em testes de causalidade de Granger. O VAR é uma ferramenta poderosa para modelar as inter-relações dinâmicas entre múltiplas séries temporais. No entanto, para compreender as complexas relações causais dentro de um VAR, é necessário um entendimento mais profundo das técnicas de análise de causalidade.

### Conceitos Fundamentais
#### Formulação Matemática do Modelo VAR
O modelo VAR, como visto anteriormente, é fundamental para entendermos as interações entre variáveis. Formalmente, um modelo VAR de ordem *p*, VAR(*p*), pode ser expresso como:

$$y_t = c + \Phi_1y_{t-1} + \Phi_2y_{t-2} + \ldots + \Phi_p y_{t-p} + \epsilon_t$$

Onde:

*   $y_t$ é um vetor $n \times 1$ de variáveis endógenas no tempo *t*.
*   $c$ é um vetor $n \times 1$ de constantes.
*   $\Phi_i$ são matrizes de coeficientes de tamanho $n \times n$.
*   $\epsilon_t$ é um vetor $n \times 1$ de inovações (erros) com média zero e matriz de covariância $\Omega$.[^1]

#### Causalidade de Granger
O conceito de **Causalidade de Granger** explora se uma variável *x* pode ajudar a prever outra variável *y*. Especificamente, dizemos que *y* não causa no sentido de Granger *x* se, para todos os *s* > 0, o erro quadrático médio da previsão de $x_{t+s}$ usando apenas os valores passados de *x* é o mesmo que usar valores passados de *x* e *y*. Formalmente:

$$MSE[E(x_{t+s} | x_t, x_{t-1}, \ldots)] = MSE[E(x_{t+s} | x_t, x_{t-1}, \ldots, y_t, y_{t-1}, \ldots)]$$

Equivalentemente, *x* é considerada exógena com relação a *y* no sentido de série temporal se [^1]:

$$MSE[\hat{E}(x_{t+s}|x_t, x_{t-1},\ldots)] = MSE[\hat{E}(x_{t+s}|x_t, x_{t-1},\ldots, y_t, y_{t-1},\ldots)]$$

Isso implica que os valores passados de *y* não contêm nenhuma informação linear útil para prever valores futuros de *x*.

#### Implicações da Causalidade de Granger
Num sistema VAR bivariado, onde *x* e *y* são as variáveis, *y* não causa *x* se as matrizes de coeficientes $\Phi_j$ forem triangulares inferiores para todos os *j*:

$$\begin{bmatrix} x_t \\ y_t \end{bmatrix} = \begin{bmatrix} c_1 \\ c_2 \end{bmatrix} + \sum_{j=1}^{p} \begin{bmatrix} \Phi_{11}^{(j)} & 0 \\ \Phi_{21}^{(j)} & \Phi_{22}^{(j)} \end{bmatrix} \begin{bmatrix} x_{t-j} \\ y_{t-j} \end{bmatrix} + \begin{bmatrix} \epsilon_{1t} \\ \epsilon_{2t} \end{bmatrix}$$

Neste sistema, o forecast de um passo à frente para *x* depende apenas dos seus próprios lags, não dos lags de *y*. Em contrapartida, *y* pode depender dos lags de *x*.

#### Testes Econométricos para Causalidade de Granger
Testes empíricos sobre causalidade de Granger podem ser baseados em três abordagens:

1.  **Abordagem VAR:** Testar se as matrizes de coeficientes são triangulares inferiores.
2.  **Abordagem MA(∞):** Analisar se as matrizes de médias móveis são triangulares inferiores.
3.  **Abordagem de projeção linear:** Projetar *y* em passados, presentes e futuros de *x*.

A abordagem mais simples, e frequentemente preferida, é a baseada na especificação VAR. Para implementar o teste, assumimos um comprimento de lag específico *p* e estimamos:

$$x_t = c_1 + \alpha_1x_{t-1} + \alpha_2x_{t-2} + \ldots + \alpha_px_{t-p} + \beta_1y_{t-1} + \beta_2y_{t-2} + \ldots + \beta_py_{t-p} + u_t$$

Em seguida, fazemos um teste *F* da hipótese nula:

$$H_0 : \beta_1 = \beta_2 = \ldots = \beta_p = 0$$

Rejeitamos a hipótese nula se o teste *F* ou o teste de razão de verossimilhança indicar que os coeficientes de *y* são conjuntamente significativos.

####  Problemas e Ajustes
Os testes de causalidade de Granger podem ser sensíveis à escolha do comprimento do lag (*p*) e à presença de não estacionariedade. Os testes baseados na forma de Sims podem ser afetados por autocorrelação nos erros, o que exige o uso de erros padrão consistentes com a autocorrelação ou transformações de mínimos quadrados generalizados [^1]. Uma modificação comum ao teste de razão de verossimilhança é substituir o multiplicador T por T-k, onde k é o número de parâmetros estimados por equação.

#### Distribuição Assintótica
Estimadores de máxima verossimilhança para coeficientes VAR, mesmo que as inovações não sejam Gaussianas, são consistentes. Os erros padrão podem ser calculados com base em fórmulas OLS usuais, como em:

$$\sqrt{T}(\hat{\pi}_i - \pi_i) \xrightarrow{d} N(0, \sigma_i^2 Q^{-1})$$

Onde:
*  $\hat{\pi}_i$ é o estimador de máxima verossimilhança do coeficiente da i-ésima equação
* $\pi_i$ é o verdadeiro valor do coeficiente
* $\sigma_i^2$ é a variância da inovação na i-ésima equação
*  $Q$ é a matriz de variância-covariância das variáveis exógenas.

### Conclusão
Este capítulo estabeleceu os fundamentos teóricos e práticos para a análise de causalidade de Granger em um contexto bivariado. A causalidade de Granger é uma ferramenta útil para avaliar relações de previsão, mas suas interpretações como causalidade "verdadeira" devem ser vistas com cautela. Abordagens alternativas e considerações sobre estrutura de dados são cruciais para análises mais robustas. A análise dos testes de razão de verossimilhança e as abordagens de estimação de máxima verossimilhança, junto com os seus respectivos pressupostos e limitações, são importantes para garantir uma adequada aplicação destas técnicas.
### Referências
[^1]: Capítulo 11 "Vector Autoregressions" do livro não especificado no contexto.
[^2]: Conteúdo anterior do contexto
<!-- END -->
