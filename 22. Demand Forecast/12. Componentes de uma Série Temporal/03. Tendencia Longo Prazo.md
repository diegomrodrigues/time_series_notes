## Decomposi√ß√£o de S√©ries Temporais: An√°lise Aprofundada da Componente de Tend√™ncia

### Introdu√ß√£o
Em continuidade √† nossa discuss√£o sobre a decomposi√ß√£o de s√©ries temporais, este cap√≠tulo aprofunda a modelagem matem√°tica e estat√≠stica da componente de **tend√™ncia** (trend), que representa o movimento de longo prazo na demanda [^3]. Como vimos anteriormente, a tend√™ncia pode ser linear ou n√£o linear, influenciada por fatores como mudan√ßas demogr√°ficas, concorr√™ncia e transforma√ß√µes sociais [^3]. Exploraremos em detalhes como modelar essa componente usando regress√£o linear, regress√£o polinomial e suaviza√ß√£o por m√©dias m√≥veis, incluindo as nuances matem√°ticas e estat√≠sticas de cada m√©todo. Al√©m disso, vamos incluir uma an√°lise da formula√ß√£o do modelo de espa√ßo de estados, juntamente com o uso do Filtro de Kalman.

### Regress√£o Linear: Modelagem Matem√°tica e Estimativa de Par√¢metros

Como introduzido no cap√≠tulo anterior, a **regress√£o linear** modela a tend√™ncia como uma linha reta, cuja equa√ß√£o geral √© dada por $T_t = a + bt$, onde $T_t$ √© a tend√™ncia no per√≠odo $t$, $a$ √© o intercepto e $b$ √© o coeficiente angular [^Proposi√ß√£o 1.1]. Vamos detalhar a formula√ß√£o matricial e a estimativa dos par√¢metros $a$ e $b$.

#### Formula√ß√£o Matricial
A regress√£o linear pode ser expressa em nota√ß√£o matricial como:
$$Y = X\beta + \epsilon$$
onde:
-   $Y$ √© o vetor de valores observados da s√©rie temporal.
-   $X$ √© a matriz de design, onde a primeira coluna √© um vetor de 1's (para o intercepto $a$) e a segunda coluna cont√©m os valores de tempo $t$.
-   $\beta$ √© o vetor de coeficientes de regress√£o (incluindo $a$ e $b$).
-   $\epsilon$ √© o vetor de erros aleat√≥rios.

A matriz de design $X$ assume a forma:
$$ X = \begin{bmatrix} 1 & t_1 \\ 1 & t_2 \\ \vdots & \vdots \\ 1 & t_n \end{bmatrix} $$
onde $t_i$ representa o tempo no ponto de dados $i$.

#### Estimativa dos Par√¢metros
O vetor de coeficientes $\beta$, que cont√©m $a$ e $b$, √© estimado minimizando a soma dos quadrados dos erros (SSE) [^Prova da Estimativa de Œ≤]:
$$ SSE = \epsilon^T \epsilon = (Y - X\beta)^T(Y - X\beta) $$
A estimativa de $\beta$, denotada como $\hat{\beta}$, √© obtida atrav√©s da seguinte equa√ß√£o:
$$\hat{\beta} = (X^TX)^{-1}X^TY$$
onde $(X^TX)^{-1}$ √© a inversa da matriz $(X^TX)$.

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal com os seguintes valores de demanda ao longo de 5 per√≠odos: $Y = [20, 25, 30, 33, 36]$. Os tempos correspondentes s√£o $t = [1, 2, 3, 4, 5]$. Queremos ajustar um modelo de regress√£o linear para estimar a tend√™ncia.
>
> 1. **Constru√ß√£o da Matriz X:**
> ```python
> import numpy as np
>
> t = np.array([1, 2, 3, 4, 5])
> X = np.vstack([np.ones(len(t)), t]).T
> print("Matriz X:\n", X)
> ```
> Isso nos d√°:
> $$X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \\ 1 & 5 \end{bmatrix}$$
> 2. **Cria√ß√£o do vetor Y:**
> ```python
> Y = np.array([20, 25, 30, 33, 36])
> print("Vetor Y:\n", Y)
> ```
> 3. **C√°lculo de $X^TX$:**
> ```python
> XtX = X.T @ X
> print("X^T * X:\n", XtX)
> ```
> Resultado:
> $$X^TX = \begin{bmatrix} 5 & 15 \\ 15 & 55 \end{bmatrix}$$
> 4. **C√°lculo da inversa de $(X^TX)^{-1}$:**
> ```python
> XtX_inv = np.linalg.inv(XtX)
> print("(X^T * X)^-1:\n", XtX_inv)
> ```
> Resultado:
> $$(X^TX)^{-1} = \begin{bmatrix} 1.1 & -0.3 \\ -0.3 & 0.1 \end{bmatrix}$$
> 5. **C√°lculo de $X^TY$:**
> ```python
> XtY = X.T @ Y
> print("X^T * Y:\n", XtY)
> ```
> Resultado:
> $$X^TY = \begin{bmatrix} 144 \\ 451 \end{bmatrix}$$
> 6. **C√°lculo de $\hat{\beta}$:**
> ```python
> beta_hat = XtX_inv @ XtY
> print("Beta_hat:\n", beta_hat)
> ```
> Resultado:
> $$\hat{\beta} = \begin{bmatrix} 18.4 \\ 4.0 \end{bmatrix}$$
> Portanto, $\hat{a} = 18.4$ e $\hat{b} = 4.0$. O modelo de regress√£o linear para a tend√™ncia √© $T_t = 18.4 + 4t$. Isso significa que, em m√©dia, a demanda aumenta em 4 unidades por per√≠odo, partindo de um n√≠vel base de 18.4 no per√≠odo inicial (t=0).
> 7. **Visualiza√ß√£o do ajuste:**
>
> ```mermaid
>   graph LR
>      A[Dados Observados] --> B(Regress√£o Linear);
>      B --> C{Trend Line};
>      C --> D(Visualiza√ß√£o);
> ```
> A linha de tend√™ncia pode ser visualizada com os dados observados para avaliar o ajuste do modelo.
>
> 8.  **C√°lculo dos valores ajustados:**
>
> ```python
> fitted_values = X @ beta_hat
> print("Valores Ajustados:\n", fitted_values)
> ```
> Resultado:
> $$ \text{fitted_values} = [22.4, 26.4, 30.4, 34.4, 38.4]$$
>
> 9. **C√°lculo dos res√≠duos:**
>
> ```python
> residuals = Y - fitted_values
> print("Res√≠duos:\n", residuals)
> ```
> Resultado:
> $$\text{residuals} = [-2.4, -1.4, -0.4, -1.4, -2.4]$$
>
> Uma an√°lise dos res√≠duos pode ser feita para avaliar a adequa√ß√£o do modelo.
> Uma regress√£o linear fornece um ponto de partida para entender a tend√™ncia de um conjunto de dados.

**Lema 1.1:** A estimativa de $\beta$ obtida pela f√≥rmula $\hat{\beta} = (X^TX)^{-1}X^TY$ √© um estimador de m√≠nimos quadrados n√£o viesado.

**Prova do Lema 1.1:**
I.  Partindo do modelo de regress√£o linear $Y = X\beta + \epsilon$, onde $E[\epsilon]=0$ e $Var(\epsilon)=\sigma^2I$, sendo $I$ a matriz identidade.

II.  A estimativa de $\beta$ √© dada por $\hat{\beta} = (X^TX)^{-1}X^TY$. Substituindo a express√£o de $Y$:
    $$\hat{\beta} = (X^TX)^{-1}X^T(X\beta + \epsilon) = (X^TX)^{-1}X^TX\beta + (X^TX)^{-1}X^T\epsilon$$
    Como $(X^TX)^{-1}X^TX=I$, a identidade:
    $$\hat{\beta} = \beta + (X^TX)^{-1}X^T\epsilon$$

III.  Para verificar se o estimador √© n√£o viesado, calculamos a esperan√ßa matem√°tica de $\hat{\beta}$:
    $$E[\hat{\beta}] = E[\beta + (X^TX)^{-1}X^T\epsilon] = E[\beta] + E[(X^TX)^{-1}X^T\epsilon]$$
    Como $\beta$ √© um vetor de par√¢metros fixos, $E[\beta] = \beta$. Como $X$ √© uma matriz de valores fixos e $E[\epsilon]=0$, temos:
    $$E[(X^TX)^{-1}X^T\epsilon] = (X^TX)^{-1}X^TE[\epsilon] = (X^TX)^{-1}X^T0 = 0$$
    Portanto,
    $$E[\hat{\beta}] = \beta$$
    Isto mostra que a estimativa $\hat{\beta}$ √© um estimador n√£o viesado.

IV. Para calcular a vari√¢ncia do estimador $\hat{\beta}$, usamos a propriedade $Var(AX) = A Var(X)A^T$, onde A √© uma constante.
    $$Var(\hat{\beta}) = Var(\beta + (X^TX)^{-1}X^T\epsilon) = Var((X^TX)^{-1}X^T\epsilon)$$
    Como $Var(\epsilon)=\sigma^2 I$:
    $$Var(\hat{\beta}) = (X^TX)^{-1}X^T Var(\epsilon)  ((X^TX)^{-1}X^T)^T = (X^TX)^{-1}X^T \sigma^2 I X(X^TX)^{-1} = \sigma^2(X^TX)^{-1}$$
   Portanto, a vari√¢ncia do estimador √© $\sigma^2(X^TX)^{-1}$.
    Assim, demonstramos que o estimador de m√≠nimos quadrados $\hat{\beta}$ √© n√£o viesado e possui vari√¢ncia $\sigma^2(X^TX)^{-1}$.‚ñ†

**Lema 1.2:** A matriz $(X^TX)$ √© invers√≠vel se e somente se as colunas de X s√£o linearmente independentes.

**Prova do Lema 1.2:**
I. Suponha que as colunas de X s√£o linearmente dependentes. Ent√£o existe um vetor $c \neq 0$ tal que $Xc = 0$.
II. Multiplicando por $X^T$, temos $X^TXc = X^T0 = 0$.
III. Isso significa que existe um vetor n√£o nulo $c$ tal que $(X^TX)c=0$, o que implica que a matriz $X^TX$ n√£o √© invers√≠vel.
IV. Agora, suponha que a matriz $X^TX$ n√£o √© invers√≠vel. Ent√£o existe um vetor $c \neq 0$ tal que $(X^TX)c = 0$.
V. Multiplicando por $c^T$ temos $c^T(X^TX)c = 0$, ou $(Xc)^T(Xc)=0$, o que implica que $Xc=0$
VI. Portanto, as colunas de X s√£o linearmente dependentes, completando a prova.‚ñ†

**Lema 1.3:** A estimativa de $\hat{\beta}$ √© √∫nica se e somente se as colunas de X s√£o linearmente independentes.

**Prova do Lema 1.3:**
I. Se as colunas de X s√£o linearmente independentes, ent√£o a matriz $X^TX$ √© invert√≠vel (pelo Lema 1.2) e a estimativa $\hat{\beta}=(X^TX)^{-1}X^TY$ √© √∫nica.
II. Reciprocamente, se a estimativa $\hat{\beta}$ √© √∫nica, ent√£o a matriz $X^TX$ deve ser invert√≠vel. Caso contr√°rio, se $X^TX$ n√£o fosse invert√≠vel, ter√≠amos a possibilidade de m√∫ltiplos vetores $\hat{\beta}$ que minimizam SSE, contradizendo a unicidade da estimativa. Assim, se a estimativa $\hat{\beta}$ √© √∫nica, as colunas de X devem ser linearmente independentes, completando a prova.‚ñ†

### Regress√£o Polinomial: Modelagem de Tend√™ncias N√£o Lineares

Quando a tend√™ncia n√£o √© linear, a **regress√£o polinomial** oferece uma alternativa, representando a tend√™ncia como uma curva [^Proposi√ß√£o 1.1]. A equa√ß√£o geral de um polin√¥mio de grau *n* para a tend√™ncia √©:
$$ T_t = a_0 + a_1t + a_2t^2 + \ldots + a_nt^n $$
onde $a_0, a_1, \ldots, a_n$ s√£o os par√¢metros do polin√¥mio.

#### Formula√ß√£o Matricial
Semelhante √† regress√£o linear, a regress√£o polinomial pode ser formulada em nota√ß√£o matricial:
$$ Y = X\beta + \epsilon $$
Neste caso, a matriz de design $X$ inclui as pot√™ncias de $t$:
$$ X = \begin{bmatrix} 1 & t_1 & t_1^2 & \dots & t_1^n \\ 1 & t_2 & t_2^2 & \dots & t_2^n \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & t_m & t_m^2 & \dots & t_m^n \end{bmatrix} $$
O vetor $\beta$ √© dado por $[a_0, a_1, \dots, a_n]^T$, representando os coeficientes do polin√¥mio.
#### Estimativa dos Par√¢metros
A estimativa dos par√¢metros $\hat{\beta}$ √© realizada da mesma forma que na regress√£o linear:
$$\hat{\beta} = (X^TX)^{-1}X^TY$$
A complexidade do modelo aumenta com o grau *n* do polin√¥mio, o que pode levar a overfitting (sobreajuste) se o grau for muito elevado ou se a quantidade de dados n√£o for suficiente.
A sele√ß√£o adequada do grau do polin√¥mio pode ser feita atrav√©s de m√©todos como valida√ß√£o cruzada ou o uso de crit√©rios de informa√ß√£o como AIC e BIC. A valida√ß√£o cruzada divide os dados em diferentes grupos e avalia o desempenho do modelo em cada um, prevenindo o overfitting. Crit√©rios de informa√ß√£o, como o AIC e o BIC, adicionam uma penaliza√ß√£o √† complexidade do modelo, preferindo modelos mais simples que se ajustem bem aos dados, balanceando entre ajuste e complexidade.

> üí° **Exemplo Num√©rico:** Considere os mesmos dados de demanda, mas agora vamos ajustar um polin√¥mio de grau 2: $T_t = a_0 + a_1t + a_2t^2$. Usando $Y=[20, 25, 30, 33, 36]$ e $t=[1,2,3,4,5]$, a matriz X torna-se:
>
> $$ X = \begin{bmatrix} 1 & 1 & 1^2 \\ 1 & 2 & 2^2 \\ 1 & 3 & 3^2 \\ 1 & 4 & 4^2 \\ 1 & 5 & 5^2 \end{bmatrix} = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 4 \\ 1 & 3 & 9 \\ 1 & 4 & 16 \\ 1 & 5 & 25 \end{bmatrix} $$
>
> ```python
> import numpy as np
>
> t = np.array([1, 2, 3, 4, 5])
> Y = np.array([20, 25, 30, 33, 36])
> X = np.vstack([np.ones(len(t)), t, t**2]).T
> print("Matriz X:\n", X)
>
> XtX = X.T @ X
> print("X^T * X:\n", XtX)
>
> XtX_inv = np.linalg.inv(XtX)
> print("(X^T * X)^-1:\n", XtX_inv)
>
> XtY = X.T @ Y
> print("X^T * Y:\n", XtY)
>
> beta_hat = XtX_inv @ XtY
> print("Beta_hat:\n", beta_hat)
>
> fitted_values = X @ beta_hat
> print("Valores Ajustados:\n", fitted_values)
>
> residuals = Y - fitted_values
> print("Res√≠duos:\n", residuals)
> ```
> Os resultados s√£o:
> $$ \hat{\beta} = \begin{bmatrix} 19.2 \\ 1.3 \\ 0.3 \end{bmatrix} $$
> Os valores ajustados s√£o: $[20.8, 25.4, 29.4, 32.8, 35.6]$. Os res√≠duos s√£o: $[-0.8, -0.4, 0.6, 0.2, 0.4]$. O modelo ajustado √© $T_t = 19.2 + 1.3t + 0.3t^2$. Observe como os res√≠duos s√£o menores em magnitude para este modelo polinomial comparado ao exemplo da regress√£o linear. Este modelo captura melhor a curvatura dos dados.

### Suaviza√ß√£o por M√©dias M√≥veis: Abordagem N√£o Param√©trica
A **suaviza√ß√£o por m√©dias m√≥veis** (SMA) √© um m√©todo n√£o param√©trico que suaviza as flutua√ß√µes de curto prazo para revelar a tend√™ncia [^Proposi√ß√£o 1.1]. A m√©dia m√≥vel simples para um per√≠odo $t$ √© calculada como:
$$ SMA_t = \frac{1}{k} \sum_{i=t-k+1}^t Y_i $$
onde *k* √© o tamanho da janela da m√©dia m√≥vel.

#### Vantagens da SMA
-   **Simplicidade:** F√°cil de calcular e implementar.
-   **Redu√ß√£o de ru√≠do:** Suaviza as varia√ß√µes aleat√≥rias de curto prazo, expondo a tend√™ncia subjacente.

#### Desvantagens da SMA
-   **Defasagem:** A SMA tende a apresentar uma defasagem em rela√ß√£o √† tend√™ncia real, especialmente quando ela muda rapidamente.
-   **Sensibilidade ao tamanho da janela:** A escolha de *k* pode afetar o resultado final. Valores de *k* muito pequenos podem n√£o suavizar o suficiente o ru√≠do, enquanto valores muito grandes podem suavizar a tend√™ncia excessivamente, perdendo nuances importantes.
-   **Perda de informa√ß√£o nos extremos:** As primeiras e √∫ltimas observa√ß√µes da s√©rie temporal n√£o podem ser suavizadas, devido √† necessidade de valores passados e futuros dentro da janela, o que leva a perda de informa√ß√£o nesses pontos.

A m√©dia m√≥vel ponderada (WMA), que discutimos no cap√≠tulo anterior [^Lema 1.1], oferece uma extens√£o da SMA, permitindo dar pesos diferentes a cada observa√ß√£o dentro da janela. A f√≥rmula para a WMA √©:
$$ WMA_t = \sum_{i=t-k+1}^t w_i Y_i $$
onde $w_i$ s√£o os pesos atribu√≠dos a cada observa√ß√£o, com $\sum_{i=t-k+1}^t w_i = 1$.

> üí° **Exemplo Num√©rico:** Usando os dados $Y = [20, 25, 30, 33, 36]$, vamos calcular a m√©dia m√≥vel simples com janela de tamanho k=3.
>
>  -   $SMA_3 = \frac{20 + 25 + 30}{3} = 25$
>  -   $SMA_4 = \frac{25 + 30 + 33}{3} = 29.33$
>  -   $SMA_5 = \frac{30 + 33 + 36}{3} = 33$
>
>  Note que os dois primeiros pontos n√£o podem ser calculados com k=3, exemplificando a perda de informa√ß√£o nos extremos.  A SMA suaviza os valores, mas com uma defasagem. Para k = 2:
>
> - $SMA_2 = \frac{20 + 25}{2} = 22.5$
> - $SMA_3 = \frac{25 + 30}{2} = 27.5$
> - $SMA_4 = \frac{30 + 33}{2} = 31.5$
> - $SMA_5 = \frac{33 + 36}{2} = 34.5$
> Para k=2, menos suaviza√ß√£o e menos defasagem s√£o observadas.
>
> Uma m√©dia m√≥vel ponderada com pesos [0.2, 0.3, 0.5] e janela k=3 seria calculada como:
>
> - $WMA_3 = 0.2*20 + 0.3*25 + 0.5*30 = 26.5$
> - $WMA_4 = 0.2*25 + 0.3*30 + 0.5*33 = 30$
> - $WMA_5 = 0.2*30 + 0.3*33 + 0.5*36 = 33.9$

##### Suaviza√ß√£o Exponencial
A **suaviza√ß√£o exponencial** √© outra t√©cnica de suaviza√ß√£o que oferece uma alternativa para a SMA e WMA. Em ess√™ncia, a suaviza√ß√£o exponencial calcula uma m√©dia ponderada dos valores passados, onde os pesos decrescem exponencialmente √† medida que os dados ficam mais antigos. A suaviza√ß√£o exponencial √© modelada como:
$$ ES_t = \alpha Y_{t-1} + (1-\alpha)ES_{t-1} $$
Onde $\alpha$ √© o fator de suaviza√ß√£o, sendo $0 < \alpha < 1$. A escolha de $\alpha$ controla o peso dado aos dados mais recentes em rela√ß√£o aos dados mais antigos. Valores de $\alpha$ pr√≥ximos de 1 d√£o mais peso aos dados recentes, tornando o modelo mais sens√≠vel a mudan√ßas, enquanto valores pr√≥ximos a 0 d√£o mais peso aos dados antigos, tornando o modelo mais est√°vel.
Uma desvantagem da suaviza√ß√£o exponencial √© que ela n√£o modela explicitamente a tend√™ncia e sazonalidade.

> üí° **Exemplo Num√©rico:** Usando os dados $Y = [20, 25, 30, 33, 36]$, vamos calcular a suaviza√ß√£o exponencial com $\alpha = 0.6$. Inicializamos $ES_1 = Y_1$.
>
> - $ES_2 = 0.6*20 + 0.4*20 = 20$
> - $ES_3 = 0.6*25 + 0.4*20 = 23$
> - $ES_4 = 0.6*30 + 0.4*23 = 27.2$
> - $ES_5 = 0.6*33 + 0.4*27.2 = 30.7$
>
> Com $\alpha = 0.2$, com a mesma inicializa√ß√£o:
>
> - $ES_2 = 0.2*20 + 0.8*20 = 20$
> - $ES_3 = 0.2*25 + 0.8*20 = 21$
> - $ES_4 = 0.2*30 + 0.8*21 = 22.8$
> - $ES_5 = 0.2*33 + 0.8*22.8 = 24.8$
>
> Valores maiores de $\alpha$ (0.6) tornam o modelo mais sens√≠vel a mudan√ßas recentes nos dados, enquanto valores menores (0.2) resultam em maior suaviza√ß√£o.

**Proposi√ß√£o 1.1:** A suaviza√ß√£o exponencial simples assume que a s√©rie temporal n√£o possui tend√™ncia. Para s√©ries com tend√™ncia, pode-se utilizar a suaviza√ß√£o exponencial dupla.

**Prova da Proposi√ß√£o 1.1:**
I. A suaviza√ß√£o exponencial simples, como definida acima, estima o n√≠vel da s√©rie, mas n√£o sua tend√™ncia, j√° que considera que o n√≠vel √© constante ao longo do tempo.

II. A suaviza√ß√£o exponencial dupla adiciona uma componente de tend√™ncia, modelando-a como um valor vari√°vel no tempo. As equa√ß√µes da suaviza√ß√£o exponencial dupla s√£o:
   $$ S_t = \alpha Y_t + (1-\alpha)(S_{t-1} + b_{t-1}) $$
   $$ b_t = \beta(S_t - S_{t-1}) + (1-\beta)b_{t-1} $$
  Onde $S_t$ √© o n√≠vel suavizado e $b_t$ √© a estimativa da tend√™ncia no tempo $t$, $\alpha$ e $\beta$ s√£o par√¢metros de suaviza√ß√£o.

III. Portanto, a suaviza√ß√£o exponencial simples n√£o √© adequada para s√©ries com tend√™ncia, e o uso da suaviza√ß√£o exponencial dupla se faz necess√°rio para capturar tanto o n√≠vel quanto a tend√™ncia da s√©rie.‚ñ†

### Modelos de Espa√ßo de Estados e Filtro de Kalman
Os modelos de espa√ßo de estados fornecem uma estrutura flex√≠vel para modelar sistemas din√¢micos, incluindo aqueles com componentes de tend√™ncia. Eles s√£o definidos por duas equa√ß√µes principais: a equa√ß√£o de estado e a equa√ß√£o de observa√ß√£o.

A equa√ß√£o de estado descreve como o estado do sistema evolui ao longo do tempo:
$$ \mathbf{x}_{t+1} = \mathbf{F}\mathbf{x}_t + \mathbf{G}\mathbf{w}_t $$

A equa√ß√£o de observa√ß√£o relaciona o estado do sistema √†s observa√ß√µes:
$$ \mathbf{y}_t = \mathbf{H}\mathbf{x}_t + \mathbf{v}_t $$

Para modelar uma tend√™ncia, podemos incluir uma vari√°vel para a tend√™ncia no vetor de estado:
$$ \mathbf{x}_t = \begin{bmatrix} \text{n√≠vel}_t \\ \text{tend√™ncia}_t \end{bmatrix} $$

O filtro de Kalman usa essas equa√ß√µes para estimar iterativamente o estado do sistema, incluindo a tend√™ncia.

#### Passos do Filtro de Kalman
1.  **Previs√£o:**
    -   Estado previsto:
        $$ \hat{\mathbf{x}}_{t|t-1} = \mathbf{F} \hat{\mathbf{x}}_{t-1|t-1} $$
    -   Covari√¢ncia do estado prevista:
        $$ \mathbf{P}_{t|t-1} = \mathbf{F} \mathbf{P}_{t-1|t-1} \mathbf{F}^T + \mathbf{G} \mathbf{Q} \mathbf{G}^T $$

2.  **Atualiza√ß√£o:**
    -   Ganho de Kalman:
        $$ \mathbf{K}_t = \mathbf{P}_{t|t-1} \mathbf{H}^T (\mathbf{H} \mathbf{P}_{t|t-1} \mathbf{H}^T + \mathbf{R})^{-1} $$
    -   Estado atualizado:
        $$ \hat{\mathbf{x}}_{t|t} = \hat{\mathbf{x}}_{t|t-1} + \mathbf{K}_t (\mathbf{y}_t - \mathbf{H} \hat{\mathbf{x}}_{t|t-1}) $$
    -   Covari√¢ncia do estado atualizada:
        $$ \mathbf{P}_{t|t} = (\mathbf{I} - \mathbf{K}_t \mathbf{H}) \mathbf{P}_{t|t-1} $$

O filtro de Kalman permite estimar o estado subjacente (incluindo tend√™ncia), usando as medi√ß√µes ruidosas. Os modelos de espa√ßo de estados e o Filtro de Kalman, portanto, oferecem uma abordagem mais flex√≠vel e poderosa para lidar com componentes de tend√™ncia, permitindo a combina√ß√£o com outras componentes (sazonalidade, ciclo) e modelos mais avan√ßados.

> üí° **Exemplo Num√©rico:** Vamos simplificar e apresentar um exemplo com valores hipot√©ticos para ilustrar o processo do filtro de Kalman para estimar uma tend√™ncia.
>
> Suponha que temos um modelo onde o estado $\mathbf{x}_t$ √© composto pelo n√≠vel e pela tend√™ncia. Vamos come√ßar com:
>
> - Estado inicial: $\mathbf{x}_{0|0} = \begin{bmatrix} 20 \\ 2 \end{bmatrix}$ (n√≠vel inicial 20, tend√™ncia inicial 2)
> - Covari√¢ncia inicial: $\mathbf{P}_{0|0} = \begin{bmatrix} 1 & 0 \\ 0 & 0.1 \end{bmatrix}$
> - Matriz de transi√ß√£o de estado: $\mathbf{F} = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$ (o n√≠vel aumenta pela tend√™ncia, a tend√™ncia mant√©m-se)
> - Matriz de ru√≠do do processo: $\mathbf{G} = \begin{bmatrix} 0.5 & 0 \\ 0 & 0.1 \end{bmatrix}$
> - Matriz de covari√¢ncia do ru√≠do do processo: $\mathbf{Q} = \begin{bmatrix} 0.01 & 0 \\ 0 & 0.001 \end{bmatrix}$
> - Matriz de observa√ß√£o: $\mathbf{H} = \begin{bmatrix} 1 & 0 \end{bmatrix}$ (observamos apenas o n√≠vel)
> - Vari√¢ncia do ru√≠do da observa√ß√£o: $\mathbf{R} = [0.5]$
>
> Suponha que a observa√ß√£o no tempo $t=1$ √© $\mathbf{y}_1 = 23$.
>
> 1. **Previs√£o:**
>     - Estado previsto:
>       $$\hat{\mathbf{x}}_{1|0} = \mathbf{F} \hat{\mathbf{x}}_{0|0} = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 20 \\ 2 \end{bmatrix} = \begin{bmatrix} 22 \\ 2 \end{bmatrix}$$
>     - Covari√¢ncia do estado prevista:
>        $$\mathbf{P}_{1|0} = \mathbf{F} \mathbf{P}_{0|0} \mathbf{F}^T + \mathbf{G} \mathbf{Q} \mathbf{G}^T$$
>       $$\mathbf{P}_{1|0} = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 0.1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix} + \begin{bmatrix} 0.5 & 0 \\ 0 & 0.1 \end{bmatrix}  \begin{bmatrix} 0.01 & 0 \\ 0 & 0.001 \end{bmatrix} \begin{bmatrix} 0.5 & 0 \\ 0 & 0.1 \end{bmatrix}^T$$
>        $$\mathbf{P}_{1|0} \approx \begin{bmatrix} 1.1025 & 0.1 \\ 0.1 & 0.101 \end{bmatrix}$$
> 2. **Atualiza√ß√£o:**
>     - Ganho de Kalman:
>      $$ \mathbf{K}_1 = \mathbf{P}_{1|0} \mathbf{H}^T (\mathbf{H} \mathbf{P}_{1|0} \mathbf{H}^T + \mathbf{R})^{-1} $$
>      $$ \mathbf{K}_1 = \begin{bmatrix} 1.1025 & 0.1 \\ 0.1 & 0.101 \end{bmatrix} \begin{bmatrix} 1 \\ 0 \end{bmatrix}  ( \begin{bmatrix} 1 & 0 \end{bmatrix} \begin{bmatrix} 1.1025 & 0.1 \\ 0.1 & 0.101 \end{bmatrix} \begin{bmatrix} 1 \\ 0 \end{bmatrix} + 0.5 )^{-1} $$
>       $$\mathbf{K}_1 =  \begin{bmatrix} 1.1025 \\ 0.1 \end{bmatrix}  (1.1025+0.5)^{-1}  = \begin{bmatrix} 0.688 \\ 0.062 \end{bmatrix}$$
>     - Estado atualizado:
>      $$ \hat{\mathbf{x}}_{1|1} = \hat{\mathbf{x}}_{1|0} + \mathbf{K}_1 (\mathbf{y}_1 - \mathbf{H} \hat{\mathbf{x}}_{1|0}) $$
>     $$ \hat{\mathbf{x}}_{1|1} = \begin{bmatrix} 22 \\ 2 \end{bmatrix} + \begin{bmatrix} 0.688 \\ 0.062 \end{bmatrix} (23 - \begin{bmatrix} 1 & 0 \end{bmatrix} \begin{bmatrix} 22 \\ 2 \end{bmatrix} ) = \begin{bmatrix} 22 \\ 2 \end{bmatrix} + \begin{bmatrix} 0.688 \\ 0.062 \end{bmatrix} (23-22) = \begin{bmatrix} 22.688 \\ 2.062 \end{bmatrix}$$
>     - Covari√¢ncia do estado atualizada:
>        $$ \mathbf{P}_{1|1} = (\mathbf{I} - \mathbf{K}_1 \mathbf{H}) \mathbf{P}_{1|0} $$
>       $$ \mathbf{P}_{1|1} = \left( \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} - \begin{bmatrix} 0.688 \\ 0.062 \end{bmatrix} \begin{bmatrix} 1 & 0 \end{bmatrix} \right) \begin{bmatrix} 1.1025 & 0.1 \\ 0.1 & 0.101 \end{bmatrix} = \begin{bmatrix} 0.347 & 0.031 \\ 0.031 & 0.1 \end{bmatrix}$$
>
> Ap√≥s essa atualiza√ß√£o, a estimativa do n√≠vel passou de 22 para 22.688, e a tend√™ncia foi ligeiramente ajustada para 2.062. Este processo √© repetido para cada nova observa√ß√£o, permitindo que o filtro de Kalman rastreie a tend√™ncia e o n√≠vel da s√©rie temporal.

**Teorema 1:** Se o modelo de espa√ßo de estados √© linear e Gaussiano, o filtro de Kalman fornece a estimativa √≥tima do estado no sentido de m√≠nimos quadrados e tamb√©m no sentido de m√°xima verossimilhan√ßa.

**Prova do Teorema 1:**
I.  Se o modelo de espa√ßo de estados √© linear, as equa√ß√µes de estado e observa√ß√£o s√£o lineares em rela√ß√£o aos estados e ru√≠dos. Ou seja, $\mathbf{x}_{t+1} = \mathbf{F}\mathbf{x}_t + \mathbf{G}\mathbf{w}_t$ e $\mathbf{y}_t = \mathbf{H}\mathbf{x}_t + \mathbf{v}_t$, onde $\mathbf{w}_t$ e $\mathbf{v}_t$ s√£o ru√≠dos gaussianos com m√©dia zero.

II.  Se os ru√≠dos s√£o gaussianos, e o estado inicial $\mathbf{x}_0$ tamb√©m √© gaussiano, ent√£o todos os estados $\mathbf{x}_t$ s√£o gaussianos e todasas distribui√ß√µes $\mathbf{x}_t$ s√£o completamente descritas pelas suas m√©dias e matrizes de covari√¢ncia. Assumindo que $\mathbf{x}_0 \sim \mathcal{N}(\boldsymbol{\mu}_0, \mathbf{\Sigma}_0)$ e que $\mathbf{v}_t \sim \mathcal{N}(\mathbf{0}, \mathbf{R}_t)$, ent√£o, recursivamente:

\begin{align*}
\boldsymbol{\mu}_t &= \mathbf{F}_t \boldsymbol{\mu}_{t-1} + \mathbf{b}_t \\
\mathbf{\Sigma}_t &= \mathbf{F}_t \mathbf{\Sigma}_{t-1} \mathbf{F}_t^T + \mathbf{R}_t
\end{align*}

Estas equa√ß√µes descrevem a evolu√ß√£o da m√©dia e covari√¢ncia do estado ao longo do tempo, dadas as din√¢micas do sistema ($\mathbf{F}_t$, $\mathbf{b}_t$) e o ru√≠do do processo ($\mathbf{R}_t$).

### Modelo de Observa√ß√£o

Similarmente, assumimos que as medi√ß√µes $\mathbf{y}_t$ s√£o tamb√©m afetadas por um ru√≠do $\mathbf{w}_t$:

$$ \mathbf{y}_t = \mathbf{H}_t \mathbf{x}_t + \mathbf{c}_t + \mathbf{w}_t$$

Onde:
-   $\mathbf{H}_t$ √© a matriz de observa√ß√£o no instante $t$.
-   $\mathbf{c}_t$ √© um vetor de deslocamento.
-   $\mathbf{w}_t$ s√£o ru√≠dos gaussianos com m√©dia zero.

Assumindo que $\mathbf{w}_t \sim \mathcal{N}(\mathbf{0}, \mathbf{Q}_t)$, e que $\mathbf{x}_t$ √© gaussiano, ent√£o $\mathbf{y}_t$ √© tamb√©m gaussiano:

$$ \mathbf{y}_t | \mathbf{x}_t \sim \mathcal{N}(\mathbf{H}_t \mathbf{x}_t + \mathbf{c}_t, \mathbf{Q}_t) $$

### Infer√™ncia

O objetivo √© estimar o estado $\mathbf{x}_t$ com base nas medi√ß√µes $\mathbf{y}_{1:t}$. Para isso, podemos usar o filtro de Kalman, que calcula a distribui√ß√£o preditiva e a distribui√ß√£o de atualiza√ß√£o:

**Previs√£o:**

\begin{align*}
\boldsymbol{\mu}_{t|t-1} &= \mathbf{F}_t \boldsymbol{\mu}_{t-1|t-1} + \mathbf{b}_t \\
\mathbf{\Sigma}_{t|t-1} &= \mathbf{F}_t \mathbf{\Sigma}_{t-1|t-1} \mathbf{F}_t^T + \mathbf{R}_t
\end{align*}

**Atualiza√ß√£o:**
Primeiro calculamos o ganho de Kalman:

$$ \mathbf{K}_t = \mathbf{\Sigma}_{t|t-1} \mathbf{H}_t^T (\mathbf{H}_t \mathbf{\Sigma}_{t|t-1} \mathbf{H}_t^T + \mathbf{Q}_t)^{-1} $$

Em seguida, atualizamos a m√©dia e a covari√¢ncia:

\begin{align*}
\boldsymbol{\mu}_{t|t} &= \boldsymbol{\mu}_{t|t-1} + \mathbf{K}_t (\mathbf{y}_t - \mathbf{H}_t \boldsymbol{\mu}_{t|t-1} - \mathbf{c}_t) \\
\mathbf{\Sigma}_{t|t} &= (\mathbf{I} - \mathbf{K}_t \mathbf{H}_t) \mathbf{\Sigma}_{t|t-1}
\end{align*}

Onde:
- $\boldsymbol{\mu}_{t|t-1}$ e $\mathbf{\Sigma}_{t|t-1}$ s√£o a m√©dia e covari√¢ncia do estado predito no instante $t$, dado o hist√≥rico at√© $t-1$.
- $\boldsymbol{\mu}_{t|t}$ e $\mathbf{\Sigma}_{t|t}$ s√£o a m√©dia e covari√¢ncia do estado atualizado no instante $t$, dado o hist√≥rico at√© $t$.

### Inicializa√ß√£o

O filtro de Kalman precisa ser inicializado com valores iniciais para a m√©dia e a covari√¢ncia do estado: $\boldsymbol{\mu}_{0|0} = \boldsymbol{\mu}_0$ e $\mathbf{\Sigma}_{0|0} = \mathbf{\Sigma}_0$.

<!-- END -->
