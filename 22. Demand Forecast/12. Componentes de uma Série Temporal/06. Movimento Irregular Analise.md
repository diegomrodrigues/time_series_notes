## Decomposi√ß√£o de S√©ries Temporais: An√°lise Aprofundada da Componente de Movimento Irregular

### Introdu√ß√£o
Em nossa an√°lise da decomposi√ß√£o de s√©ries temporais, o presente cap√≠tulo aborda a componente de **movimento irregular**, tamb√©m conhecida como componente aleat√≥ria ou res√≠duo estoc√°stico [^4, ^5]. Diferentemente da tend√™ncia, ciclo e sazonalidade, o movimento irregular representa varia√ß√µes aleat√≥rias e imprevis√≠veis nos dados, n√£o capturadas pelas outras componentes [^4]. Compreender a natureza do movimento irregular √© crucial para avaliar a qualidade de um modelo de previs√£o e para evitar o *overfitting* [^5]. Este cap√≠tulo explora a modelagem estat√≠stica da componente irregular como um ru√≠do branco, a verifica√ß√£o da distribui√ß√£o dos res√≠duos e o uso do **filtro de Kalman** para mitigar seu impacto.

### Modelagem Estat√≠stica do Movimento Irregular

O movimento irregular √© caracterizado como uma **vari√°vel aleat√≥ria** com propriedades estat√≠sticas espec√≠ficas. Em muitos casos, assume-se que o movimento irregular segue uma distribui√ß√£o normal com m√©dia zero e vari√¢ncia constante, caracterizando o **ru√≠do branco** [^Proposi√ß√£o 4.1].

#### Ru√≠do Branco
Um ru√≠do branco $\epsilon_t$ √© definido por:
1. **M√©dia Zero:** $E[\epsilon_t] = 0$, para todo $t$. Isso significa que, em m√©dia, o componente irregular n√£o causa um desvio sistem√°tico da demanda.
2. **Vari√¢ncia Constante (Homocedasticidade):** $Var[\epsilon_t] = \sigma^2$, para todo $t$. A amplitude das varia√ß√µes aleat√≥rias √© constante ao longo do tempo.
3. **N√£o Autocorrela√ß√£o:** $Cov[\epsilon_t, \epsilon_s] = 0$, para todo $t \neq s$. Os valores do ru√≠do em diferentes instantes s√£o independentes entre si.

Estas propriedades caracterizam um processo puramente aleat√≥rio, sem nenhum padr√£o temporal. No entanto, em aplica√ß√µes reais, o movimento irregular raramente √© perfeitamente branco, podendo apresentar ligeiras depend√™ncias ou desvios da normalidade.

> üí° **Exemplo Num√©rico:** Considere uma s√©rie temporal simulada de 100 pontos. Vamos simular o movimento irregular como um ru√≠do branco, utilizando uma distribui√ß√£o normal com m√©dia 0 e desvio padr√£o 2:
```python
import numpy as np
import matplotlib.pyplot as plt

# Simular ru√≠do branco
N = 100
np.random.seed(42)  # para reprodutibilidade
ruido_branco = np.random.normal(0, 2, N)

# Plotar o ru√≠do
plt.figure(figsize=(10, 5))
plt.plot(ruido_branco)
plt.axhline(y=0, color='r', linestyle='--', label='M√©dia Zero')
plt.title("Ru√≠do Branco Simulado")
plt.xlabel("Tempo")
plt.ylabel("Ru√≠do")
plt.legend()
plt.grid(True)
plt.show()
print(f"M√©dia do Ru√≠do: {np.mean(ruido_branco):.3f}")
print(f"Desvio Padr√£o do Ru√≠do: {np.std(ruido_branco):.3f}")
```
O gr√°fico mostra uma sequ√™ncia de valores aleat√≥rios oscilando em torno de zero, simulando o ru√≠do branco. A m√©dia emp√≠rica e o desvio padr√£o podem ser calculados para confirmar as propriedades do ru√≠do branco. A an√°lise estat√≠stica dos dados simulados, mostram que a m√©dia se aproxima de zero e o desvio padr√£o se aproxima do valor definido (2.0).

**Lema 1.1:** Se o modelo de previs√£o √© adequado, os res√≠duos (diferen√ßa entre os valores observados e os valores previstos) devem se comportar como um ru√≠do branco.

**Prova do Lema 1.1:**
I. Se um modelo de previs√£o captura corretamente todas as componentes sistem√°ticas da s√©rie temporal (tend√™ncia, ciclo e sazonalidade), os res√≠duos devem ser o que resta dos dados, ou seja, somente o movimento irregular.
II.  Se um padr√£o sistem√°tico, seja na m√©dia ou na vari√¢ncia, √© encontrado nos res√≠duos, ent√£o isso indica que o modelo n√£o capturou toda a informa√ß√£o presente nos dados. Por exemplo, uma tend√™ncia nos res√≠duos indica que a tend√™ncia original n√£o foi modelada corretamente. Similarmente, uma mudan√ßa na vari√¢ncia ao longo do tempo indica que o modelo √© heteroced√°stico.
III.  Se os res√≠duos se comportam como um ru√≠do branco, significa que n√£o existe mais padr√£o presente nos dados e, portanto, o modelo √© adequado. ‚ñ†

**Lema 1.2:** A autocovari√¢ncia de um ru√≠do branco √© zero para qualquer defasagem n√£o nula.

**Prova do Lema 1.2:**
I. A autocovari√¢ncia de um processo estoc√°stico $\epsilon_t$ com defasagem $k$ √© definida como $Cov(\epsilon_t, \epsilon_{t-k}) = E[(\epsilon_t - E[\epsilon_t])(\epsilon_{t-k} - E[\epsilon_{t-k}])]$.
II. Para um ru√≠do branco, $E[\epsilon_t] = 0$ para todo $t$. Assim, a autocovari√¢ncia se reduz a $Cov(\epsilon_t, \epsilon_{t-k}) = E[\epsilon_t \epsilon_{t-k}]$.
III. Pela defini√ß√£o de ru√≠do branco, os valores em diferentes instantes s√£o independentes, ou seja, $Cov[\epsilon_t, \epsilon_s] = 0$ para $t \neq s$. Portanto, para $k \neq 0$, temos que $Cov(\epsilon_t, \epsilon_{t-k}) = E[\epsilon_t \epsilon_{t-k}] = 0$.
IV. A autocovari√¢ncia √© zero para qualquer defasagem n√£o nula, demonstrando a aus√™ncia de correla√ß√£o temporal no ru√≠do branco. ‚ñ†

### Verifica√ß√£o da Distribui√ß√£o dos Res√≠duos

Para avaliar a qualidade de um modelo de previs√£o, √© essencial verificar se a distribui√ß√£o dos res√≠duos se aproxima de uma distribui√ß√£o normal e se suas propriedades est√£o de acordo com a defini√ß√£o de ru√≠do branco.

#### An√°lise da Normalidade

A normalidade dos res√≠duos √© uma propriedade desej√°vel, embora n√£o obrigat√≥ria, em modelos de previs√£o, pois a distribui√ß√£o normal simplifica a an√°lise estat√≠stica e as infer√™ncias. A normalidade pode ser verificada por meio de:
1. **Histograma:** Um histograma dos res√≠duos deve ter um formato aproximadamente sim√©trico e em forma de sino, caracter√≠stico da distribui√ß√£o normal.
2. **Gr√°fico de Probabilidade Normal (Q-Q plot):**  Os pontos devem se alinhar aproximadamente em torno de uma linha reta, indicando que a distribui√ß√£o dos dados √© similar √† distribui√ß√£o normal. Desvios significativos da linha reta indicam desvios da normalidade.
3. **Testes de Normalidade:** Testes estat√≠sticos como o teste de Shapiro-Wilk e o teste de Kolmogorov-Smirnov, que quantificam a concord√¢ncia entre a distribui√ß√£o dos res√≠duos e a distribui√ß√£o normal.

> üí° **Exemplo Num√©rico:** Criemos uma s√©rie temporal simulada e avaliemos a normalidade dos res√≠duos.
```python
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

# Simular uma s√©rie temporal (sem estrutura) e calcular res√≠duos
N = 100
np.random.seed(42)
y = 10*np.random.normal(0, 2, N) # simulando res√≠duos de um modelo
# Verificar normalidade dos res√≠duos
# Histogram
plt.hist(y, bins=20, density=True, alpha=0.6, color='blue', label='Histograma')
plt.title("Histograma dos Res√≠duos")
plt.xlabel("Res√≠duos")
plt.ylabel("Densidade")
plt.show()
# Q-Q plot
plt.figure(figsize=(10, 5))
stats.probplot(y, dist="norm", plot=plt)
plt.title("Gr√°fico de Probabilidade Normal (Q-Q Plot)")
plt.show()
# Teste de Shapiro-Wilk
stat, p = stats.shapiro(y)
print(f"Teste de Shapiro-Wilk: Estat√≠stica = {stat:.3f}, p-valor = {p:.3f}")

```
O histograma e o Q-Q plot mostram como os res√≠duos se distribuem, evidenciando a proximidade com a distribui√ß√£o normal, enquanto o teste de Shapiro-Wilk quantifica a adequa√ß√£o da distribui√ß√£o √† normal. Um p-valor alto (por exemplo, > 0.05) indica que n√£o h√° evid√™ncia estat√≠stica significativa para rejeitar a hip√≥tese nula de normalidade.

#### Verifica√ß√£o da Aus√™ncia de Autocorrela√ß√£o
A aus√™ncia de autocorrela√ß√£o nos res√≠duos indica que n√£o h√° padr√µes residuais na s√©rie temporal. Esta condi√ß√£o pode ser verificada por meio de:
1. **Fun√ß√£o de Autocorrela√ß√£o (ACF):** A ACF dos res√≠duos deve mostrar valores pr√≥ximos a zero para todas as defasagens, indicando que os res√≠duos em diferentes instantes n√£o s√£o correlacionados.
2. **Fun√ß√£o de Autocorrela√ß√£o Parcial (PACF):** Similar √† ACF, a PACF tamb√©m deve mostrar valores pr√≥ximos a zero para todas as defasagens, indicando a aus√™ncia de autocorrela√ß√£o.
3. **Teste de Ljung-Box:** Este teste estat√≠stico avalia a autocorrela√ß√£o nos res√≠duos para um determinado n√∫mero de defasagens. Um p-valor alto indica a aus√™ncia de autocorrela√ß√£o.

> üí° **Exemplo Num√©rico:** Usando a mesma s√©rie temporal simulada do exemplo anterior, vamos avaliar a autocorrela√ß√£o dos res√≠duos.
```python
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import acf, pacf
from statsmodels.stats.diagnostic import acorr_ljungbox

# Simular uma s√©rie temporal (sem estrutura) e calcular res√≠duos
N = 100
np.random.seed(42)
y = 10*np.random.normal(0, 2, N) # simulando res√≠duos de um modelo

# Calcular ACF e PACF
acf_values = acf(y, nlags=20)
pacf_values = pacf(y, nlags=20)
# Plotar ACF e PACF
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.stem(np.arange(len(acf_values)), acf_values, use_line_collection=True)
plt.xlabel("Lag")
plt.ylabel("Autocorrela√ß√£o")
plt.title("Fun√ß√£o de Autocorrela√ß√£o (ACF)")

plt.subplot(1, 2, 2)
plt.stem(np.arange(len(pacf_values)), pacf_values, use_line_collection=True)
plt.xlabel("Lag")
plt.ylabel("Autocorrela√ß√£o Parcial")
plt.title("Fun√ß√£o de Autocorrela√ß√£o Parcial (PACF)")
plt.tight_layout()
plt.show()

# Teste de Ljung-Box
lb_stat, p_value = acorr_ljungbox(y, lags=[10])
print(f"Teste de Ljung-Box: Estat√≠stica = {lb_stat[0]:.3f}, p-valor = {p_value[0]:.3f}")

```
Os gr√°ficos da ACF e PACF devem mostrar valores pr√≥ximos a zero, exceto no lag 0, que representa a autocorrela√ß√£o do res√≠duo consigo mesmo (que √© sempre 1). O teste de Ljung-Box indica que n√£o h√° autocorrela√ß√£o significativa.  Um p-valor alto (por exemplo, > 0.05) indica que n√£o h√° evid√™ncia estat√≠stica para rejeitar a hip√≥tese nula de que os res√≠duos n√£o s√£o autocorrelacionados.

#### Verifica√ß√£o da Homocedasticidade
A homocedasticidade (vari√¢ncia constante) √© verificada atrav√©s da an√°lise dos res√≠duos ao longo do tempo. Se a vari√¢ncia dos res√≠duos aumenta ou diminui ao longo do tempo, isso indicar√° a presen√ßa de heterocedasticidade. Podemos verificar atrav√©s de:
1. **Gr√°fico de res√≠duos versus valores ajustados (fitted values):** Um gr√°fico dos res√≠duos versus os valores previstos deve mostrar uma dispers√£o aleat√≥ria sem nenhum padr√£o sistem√°tico. Se a dispers√£o aumentar ou diminuir ao longo do eixo de valores previstos, isso indica heterocedasticidade.
2. **Testes de heterocedasticidade:** Testes estat√≠sticos como o teste de Breusch-Pagan e o teste de White, que quantificam a homocedasticidade dos res√≠duos.

> üí° **Exemplo Num√©rico:** Vamos usar uma s√©rie temporal simulada com heterocedasticidade e avaliar a dispers√£o dos res√≠duos.
```python
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm

# Simular dados com res√≠duos heteroced√°sticos
N = 100
np.random.seed(42)
t = np.arange(N)
y_fitted = 0.2*t + 2
y_heteroskedastic = y_fitted + np.random.normal(0, 0.1 * t, N) # desvio aumenta com t

# Gr√°fico de res√≠duos vs fitted
model = sm.OLS(y_heteroskedastic, sm.add_constant(t)).fit() # modelo linear simples
residuals = model.resid
fitted_values = model.fittedvalues
plt.figure(figsize=(10, 5))
plt.scatter(fitted_values, residuals)
plt.axhline(0, color='r', linestyle='--')
plt.xlabel("Valores Ajustados")
plt.ylabel("Res√≠duos")
plt.title("Gr√°fico de Res√≠duos vs Valores Ajustados")
plt.show()

```
O gr√°fico dos res√≠duos versus os valores ajustados mostra que a dispers√£o dos res√≠duos aumenta com os valores ajustados, indicando a presen√ßa de heterocedasticidade. Testes de homocedasticidade, como o teste de Breusch-Pagan (n√£o apresentado aqui) podem ser utilizados para quantificar o problema.

### Filtro de Kalman e Mitiga√ß√£o do Movimento Irregular

O filtro de Kalman, em conjunto com modelos de espa√ßo de estados, fornece uma estrutura flex√≠vel para lidar com a componente irregular e otimizar as estimativas da s√©rie temporal, como discutido anteriormente [^Corol√°rio 4.1].  O filtro de Kalman trata a componente irregular como um ru√≠do nas observa√ß√µes e utiliza as equa√ß√µes de atualiza√ß√£o para reduzir o impacto deste ru√≠do nas estimativas dos estados.  Desta forma, o filtro de Kalman n√£o remove explicitamente o ru√≠do da s√©rie temporal, mas pondera as observa√ß√µes, de acordo com a incerteza, de forma que o ru√≠do tenha um impacto menor nas estimativas do estado.

Ao definir o modelo de espa√ßo de estados, assume-se que o ru√≠do de observa√ß√£o $\mathbf{v}_t$ (que corresponde √† componente irregular) tem uma distribui√ß√£o normal com m√©dia zero e matriz de covari√¢ncia $\mathbf{R}$.  O Filtro de Kalman minimiza a vari√¢ncia do erro de estima√ß√£o ao ponderar as previs√µes do modelo com as novas observa√ß√µes, reduzindo assim o impacto da irregularidade nas estimativas do estado (incluindo tend√™ncia e sazonalidade).
O ru√≠do do processo $\mathbf{w}_t$, por outro lado, modela a incerteza na evolu√ß√£o da tend√™ncia ou de outros componentes da s√©rie temporal.
O filtro de Kalman estima iterativamente o estado subjacente, combinando informa√ß√µes passadas (predi√ß√£o) e presentes (observa√ß√£o), minimizando a incerteza associada ao movimento irregular.

**Proposi√ß√£o 3.1:** O filtro de Kalman minimiza a vari√¢ncia do erro de estima√ß√£o e, portanto, reduz o impacto do movimento irregular na estimativa do estado. A magnitude da redu√ß√£o depende dos par√¢metros de ru√≠do de medi√ß√£o ($\mathbf{R}$) e do ru√≠do do processo ($\mathbf{Q}$).

**Prova da Proposi√ß√£o 3.1:**
I. O filtro de Kalman estima o estado minimizando a vari√¢ncia do erro de estima√ß√£o. A vari√¢ncia do erro de estima√ß√£o da m√©dia dos estados ap√≥s a atualiza√ß√£o √© dado por:
  $$\mathbf{P}_{t|t} = (\mathbf{I} - \mathbf{K}_t \mathbf{H}) \mathbf{P}_{t|t-1}$$
II. A vari√¢ncia do erro de estima√ß√£o √© atualizada em cada passo atrav√©s do ganho de Kalman, que depende da rela√ß√£o entre a incerteza na predi√ß√£o e a incerteza na medi√ß√£o, dada por
   $$ \mathbf{K}_t = \mathbf{P}_{t|t-1} \mathbf{H}^T (\mathbf{H} \mathbf{P}_{t|t-1} \mathbf{H}^T + \mathbf{R})^{-1} $$
III.  Se a vari√¢ncia do ru√≠do de medi√ß√£o $\mathbf{R}$ √© alta, o filtro de Kalman d√° menor peso √† observa√ß√£o e mais peso √† previs√£o do modelo. Se a vari√¢ncia do ru√≠do do processo $\mathbf{Q}$ for alta, o filtro d√° maior peso √†s novas observa√ß√µes, e menor peso √† previs√£o.
IV.  A vari√¢ncia do erro de estima√ß√£o $\mathbf{P}_{t|t}$ converge para zero se o sistema √© observ√°vel e control√°vel, como demonstrado no cap√≠tulo anterior [^Teorema 3.1].
V.  Portanto, o filtro de Kalman minimiza a vari√¢ncia do erro de estima√ß√£o e, por consequ√™ncia, reduz o impacto da componente irregular nas estimativas de estado. A magnitude desta redu√ß√£o depende dos par√¢metros $\mathbf{Q}$ e $\mathbf{R}$. Um $\mathbf{R}$ menor torna o filtro mais sens√≠vel a novas observa√ß√µes (e, portanto, mais sens√≠vel ao ru√≠do), enquanto um $\mathbf{Q}$ menor faz com que a estima√ß√£o de estado seja menos sens√≠vel a mudan√ßas. ‚ñ†

**Proposi√ß√£o 3.2:** O filtro de Kalman, sob certas condi√ß√µes de estabilidade, produz uma estimativa √≥tima no sentido de que minimiza o erro quadr√°tico m√©dio da estimativa do estado.

**Prova da Proposi√ß√£o 3.2:**
I. O filtro de Kalman √© derivado a partir da minimiza√ß√£o do erro quadr√°tico m√©dio (EQM) da estimativa do estado. Em cada passo de tempo, o filtro encontra o ganho de Kalman que minimiza a vari√¢ncia do erro de estima√ß√£o, como detalhado em [^Proposi√ß√£o 3.1].
II. A atualiza√ß√£o das estimativas de estado e da matriz de covari√¢ncia de erro √© feita de forma a que o EQM da estimativa seja minimizado em cada passo.
III. Se o modelo de espa√ßo de estados (definido pelas matrizes $\mathbf{F}$, $\mathbf{H}$, $\mathbf{Q}$ e $\mathbf{R}$) for adequado e o sistema for observ√°vel e control√°vel, o filtro de Kalman converge para uma estimativa √≥tima, de acordo com a teoria de controle √≥timo.
IV. A prova formal da otimalidade do filtro de Kalman envolve a utiliza√ß√£o de c√°lculo variacional e teoria de controle linear. No entanto, o resultado pode ser interpretado intuitivamente como o balan√ßo ideal entre a informa√ß√£o obtida das observa√ß√µes e a informa√ß√£o obtida da din√¢mica do sistema.
V. Portanto, sob certas condi√ß√µes de estabilidade, o filtro de Kalman produz uma estimativa √≥tima que minimiza o erro quadr√°tico m√©dio. ‚ñ†

> üí° **Exemplo Num√©rico:** Vamos simular uma s√©rie temporal com tend√™ncia, sazonalidade e ru√≠do, e aplicar o filtro de Kalman para mitigar o ru√≠do e melhorar a estimativa do estado. Usaremos um exemplo mais complexo com uma tend√™ncia e uma componente sazonal com um per√≠odo de 10, de forma que as matrizes de transi√ß√£o de estado ser√£o:
>
> $$\mathbf{F} = \begin{bmatrix} 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\ \end{bmatrix}$$
> $$\mathbf{H} = \begin{bmatrix} 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \end{bmatrix}, \text{ se } t \text{ mod } 10 = 0$$
> $$\mathbf{H} = \begin{bmatrix} 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \end{bmatrix}, \text{ se } t \text{ mod } 10 = 1$$
> ...e assim por diante at√© o √∫ltimo per√≠odo.
```python
import numpy as np
import matplotlib.pyplot as plt
from pykalman import KalmanFilter
# Gerar dados com tend√™ncia, sazonalidade e ru√≠do
N = 200
t = np.arange(N)
freq_sazonal = 0.1
y_true = 0.5 * t + 5 * np.sin(2 * np.pi * t/10) # tendencia + sazonalidade
y_observed = y_true + np.random.normal(0, 2, N) # ru√≠do

# Par√¢metros do filtro de Kalman (simplificado)
Q = 0.1*np.eye(12)  # Ru√≠do do processo (incerteza na evolu√ß√£o dos estados)
R = 1.0  # Ru√≠do de medi√ß√£o (incerteza na observa√ß√£o)
x0 = np.zeros(12) # estado inicial
P0 = np.eye(12)*10  # incerteza inicial

# Matrizes do espa√ßo de estados
F = np.eye(12)
F[0, 1] = 1
H_matrices = []
for i in range(10):
    H_temp = np.zeros((1,12))
    H_temp[0,0] = 1
    H_temp[0, i+2] = 1
    H_matrices.append(H_temp)

# Inicializa√ß√£o do Filtro de Kalman
kf = KalmanFilter(
    transition_matrices=F,
    initial_state_mean=x0,
    initial_state_covariance=P0,
    transition_covariance=Q,
    observation_covariance=R
)
state_means = []
for i in range(N):
   H = H_matrices[i%10]
   kf.observation_matrices = H
   state_mean, state_covariance = kf.filter_update(state_means if len(state_means) >0 else x0,
                                                         P0 if len(state_means) >0 else P0,
                                                         y_observed[i])
   state_means.append(state_mean)

estimates = np.array(state_means)[:,0]
# Visualizar
plt.figure(figsize=(10, 5))
plt.plot(t, y_true, label='Sinal Verdadeiro (sem ru√≠do)')
plt.plot(t, y_observed, label='Sinal com Ru√≠do')
plt.plot(t, estimates, label='Filtro de Kalman')
plt.legend()
plt.xlabel("Tempo")
plt.ylabel("Amplitude")
plt.title("Filtro de Kalman com Tend√™ncia, Sazonalidade e Ru√≠do")
plt.show()
```
O gr√°fico mostra como o filtro de Kalman suaviza a s√©rie temporal, reduzindo o impacto do ru√≠do e revelando a tend√™ncia e sazonalidade subjacentes. Os par√¢metros de ru√≠do do processo ($\mathbf{Q}$) e medi√ß√£o ($\mathbf{R}$) precisam ser definidos adequadamente para garantir um bom desempenho do filtro. O c√≥digo implementa um modelo simplificado para ilustra√ß√£o do filtro. Na pr√°tica, o modelo de espa√ßo de estados e o filtro de Kalman podem ser mais complexos para acomodar um modelo preciso da din√¢mica do processo, e para que a converg√™ncia seja garantida.

### Conclus√£o
A componente de movimento irregular representa as varia√ß√µes aleat√≥rias e imprevis√≠veis nos dados que n√£o s√£o explicadas por outros componentes das s√©ries temporais [^4, ^5]. Embora n√£o seja poss√≠vel prever o comportamento desta componente, √© essencial model√°-la adequadamente, avaliando a distribui√ß√£o dos res√≠duos, a aus√™ncia de autocorrela√ß√£o e a vari√¢ncia constante. O filtro de Kalman, usado em conjunto com modelos de espa√ßo de estados, oferece uma abordagem flex√≠vel e poderosa para mitigar o impacto do movimento irregular, ponderando as novas observa√ß√µes de acordo com a incerteza inerente ao ru√≠do, melhorando a precis√£o das estimativas e previs√µes [^Proposi√ß√£o 3.1]. Compreender a natureza do movimento irregular e aplicar as t√©cnicas adequadas para lidar com ele √© crucial para a constru√ß√£o de modelos de previs√£o mais robustos e precisos em sistemas de sa√∫de. No pr√≥ximo cap√≠tulo, exploraremos modelos associativos para previs√£o de demanda.

### Refer√™ncias
[^1]: ... *Cap√≠tulo 3: Forecasting Demand*
[^2]: ... *Box 3.1 OM in Practice!*
[^3]: ... *Componentes of a Time Series*
[^4]: ... *3. Seasonality.*
[^5]: ... *4. Irregular movement.*
[^Proposi√ß√£o 4.1]: ... *Previous Topics: Decomposi√ß√£o de S√©ries Temporais: Tend√™ncia, Ciclo, Sazonalidade e Irregularidade*
[^Lema 2.1]: ... *Previous Topics: Decomposi√ß√£o de S√©ries Temporais: An√°lise Aprofundada da Componente de Ciclo*
[^Teorema 3.1]: ... *Previous Topics: Decomposi√ß√£o de S√©ries Temporais: An√°lise Aprofundada da Componente de Ciclo*
[^Corol√°rio 4.1]: ... *Previous Topics: Decomposi√ß√£o de S√©ries Temporais: An√°lise Aprofundada da Componente de Ciclo*
<!-- END -->
