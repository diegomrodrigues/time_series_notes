## A Equa√ß√£o da Reta de Regress√£o e o M√©todo dos M√≠nimos Quadrados

### Introdu√ß√£o
Em continuidade ao estudo da **proje√ß√£o de tend√™ncia linear**, e como vimos anteriormente [^1, 2], vamos aprofundar na compreens√£o da **equa√ß√£o da reta de regress√£o** e no **m√©todo dos m√≠nimos quadrados**, elementos centrais para a constru√ß√£o de modelos preditivos baseados em regress√£o linear. Como j√° estabelecido, o modelo de regress√£o linear busca expressar a rela√ß√£o entre uma vari√°vel dependente (demanda) e uma vari√°vel independente (tempo) atrav√©s de uma linha reta [^2].

### Conceitos Fundamentais
A equa√ß√£o que descreve essa linha √© fundamental para a realiza√ß√£o de previs√µes. Essa equa√ß√£o, como j√° mencionamos, √© dada por:
$$ \hat{y} = a + bx $$
onde:
- $\hat{y}$ √© o valor previsto da demanda.
- $a$ √© o intercepto, ou seja, o valor de $\hat{y}$ quando $x$ √© igual a zero.
- $b$ √© a inclina√ß√£o da reta, indicando a varia√ß√£o em $\hat{y}$ para cada unidade de aumento em $x$.
- $x$ √© o valor da vari√°vel independente, representando o tempo.

O objetivo do **m√©todo dos m√≠nimos quadrados** √© encontrar os valores de $a$ e $b$ que minimizem a soma dos quadrados das diferen√ßas entre os valores reais ($y_i$) e os valores previstos ($ \hat{y_i}$) da vari√°vel dependente. Essa soma dos quadrados dos res√≠duos, $J$, √© definida como:

$$ J = \sum_{i=1}^{n} (y_i - \hat{y_i})^2 = \sum_{i=1}^{n} (y_i - (a + bx_i))^2 $$

Para encontrar os valores de $a$ e $b$ que minimizam $J$, derivamos $J$ em rela√ß√£o a $a$ e $b$ e igualamos a zero, conforme a prova apresentada anteriormente [^2]. As f√≥rmulas resultantes s√£o:

$$ a = \bar{y} - b\bar{x} $$

$$ b = \frac{\sum xy - n \bar{x} \bar{y}}{\sum x^2 - n \bar{x}^2} $$

onde:
- $\bar{y}$ √© a m√©dia dos valores observados da demanda.
- $\bar{x}$ √© a m√©dia dos valores do tempo.
- $n$ √© o n√∫mero de observa√ß√µes.

> üí° **Exemplo Num√©rico:** Vamos supor que temos os seguintes dados de demanda (`y`) ao longo do tempo (`x`):

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.array([1, 2, 3, 4, 5])
y = np.array([2, 4, 5, 4, 5])
n = len(x)
x_mean = np.mean(x)
y_mean = np.mean(y)

xy_sum = np.sum(x*y)
x_squared_sum = np.sum(x**2)


b = (xy_sum - n * x_mean * y_mean) / (x_squared_sum - n * x_mean**2)
a = y_mean - b * x_mean
print(f"Intercepto (a): {a:.2f}")
print(f"Inclina√ß√£o (b): {b:.2f}")

y_predicted = a + b * x

plt.scatter(x, y, label='Dados reais')
plt.plot(x, y_predicted, color='red', label='Reta de regress√£o')
plt.xlabel('Tempo (x)')
plt.ylabel('Demanda (y)')
plt.title('Regress√£o Linear')
plt.legend()
plt.grid(True)
plt.show()
```
> Neste exemplo, calculamos o intercepto (a) como aproximadamente 2.20 e a inclina√ß√£o (b) como 0.60. Isso significa que, para cada unidade de tempo que passa, a demanda aumenta em 0.60 unidades, e quando x √© zero, a demanda estimada √© 2.20. O gr√°fico mostra a dispers√£o dos dados reais e a reta de regress√£o ajustada.

> üí° **Detalhamento do Intercepto (a):** O intercepto $a$ representa o valor previsto da demanda quando o tempo ($x$) √© zero. Este valor √© um ponto de refer√™ncia na an√°lise, e sua interpreta√ß√£o deve ser feita com cautela. Em algumas situa√ß√µes, o intercepto pode n√£o ter significado pr√°tico. Por exemplo, se estivermos modelando a demanda em um per√≠odo de tempo espec√≠fico, o valor $a$ pode representar uma demanda hipot√©tica antes do in√≠cio do per√≠odo analisado [^2].

> üí° **Detalhamento da Inclina√ß√£o (b):** A inclina√ß√£o $b$ quantifica a taxa de mudan√ßa na demanda por cada unidade de tempo. Uma inclina√ß√£o positiva significa que a demanda aumenta com o tempo, enquanto uma inclina√ß√£o negativa indica que a demanda diminui com o tempo. A magnitude de $b$ indica a for√ßa dessa rela√ß√£o, com valores maiores representando uma mudan√ßa mais r√°pida na demanda [^2].

### Prova do M√©todo dos M√≠nimos Quadrados
Como j√° apresentada, vamos revisar a prova do m√©todo dos m√≠nimos quadrados para determinar $a$ e $b$.
Come√ßamos com a fun√ß√£o de custo, $J$:
$$J = \sum_{i=1}^{n} (y_i - \hat{y_i})^2 = \sum_{i=1}^{n} (y_i - (a + bx_i))^2$$

**I. Encontrando a Inclina√ß√£o (b)**
1. Substitu√≠mos $a$ por $\bar{y} - b\bar{x}$:
$$J = \sum_{i=1}^{n} (y_i - (\bar{y} - b\bar{x} + bx_i))^2 = \sum_{i=1}^{n} (y_i - \bar{y} - b(x_i - \bar{x}))^2$$

2. Derivamos $J$ em rela√ß√£o a $b$ e igualamos a zero:
$$\frac{\partial J}{\partial b} = -2 \sum_{i=1}^{n} (y_i - \bar{y} - b(x_i - \bar{x}))(x_i - \bar{x}) = 0$$

3. Simplificamos a equa√ß√£o:
$$\sum_{i=1}^{n} (y_i - \bar{y})(x_i - \bar{x}) - b \sum_{i=1}^{n} (x_i - \bar{x})^2 = 0$$

4. Isolamos $b$:
$$b \sum_{i=1}^{n} (x_i - \bar{x})^2 = \sum_{i=1}^{n} (y_i - \bar{y})(x_i - \bar{x})$$
$$b = \frac{\sum_{i=1}^{n} (y_i - \bar{y})(x_i - \bar{x})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}$$
Expandindo os somat√≥rios e usando as propriedades, obtemos a f√≥rmula final para $b$:

$$ b = \frac{\sum xy - n \bar{x} \bar{y}}{\sum x^2 - n \bar{x}^2} $$
$\blacksquare$

**II. Encontrando o Intercepto (a)**
1.  Como a linha de regress√£o passa pelo ponto $(\bar{x}, \bar{y})$, substitu√≠mos na equa√ß√£o da reta:
$$\bar{y} = a + b\bar{x}$$

2.  Isolamos $a$:
$$a = \bar{y} - b\bar{x}$$
$\blacksquare$

**Lema 1** *A express√£o para b pode ser reescrita em termos da covari√¢ncia e vari√¢ncia amostral.*

**Prova:**
I. Recordando que a covari√¢ncia amostral entre x e y √© dada por:
$$Cov(x,y) = \frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{n}$$
e a vari√¢ncia amostral de x √©:
$$Var(x) = \frac{\sum_{i=1}^{n}(x_i-\bar{x})^2}{n}$$

II. Podemos reescrever a f√≥rmula de b:
$$b = \frac{\sum_{i=1}^{n} (y_i - \bar{y})(x_i - \bar{x})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}$$

III. Multiplicando o numerador e o denominador por $\frac{1}{n}$, temos:
$$b = \frac{\frac{1}{n}\sum_{i=1}^{n} (y_i - \bar{y})(x_i - \bar{x})}{\frac{1}{n}\sum_{i=1}^{n} (x_i - \bar{x})^2}$$

IV. Substituindo pelas defini√ß√µes de covari√¢ncia e vari√¢ncia amostral:
$$b = \frac{Cov(x,y)}{Var(x)}$$

Dessa forma, a inclina√ß√£o da reta de regress√£o √© dada pela raz√£o entre a covari√¢ncia amostral de x e y, e a vari√¢ncia amostral de x. $\blacksquare$

> üí° **Exemplo Num√©rico:**  Usando os mesmos dados anteriores, podemos calcular a covari√¢ncia entre `x` e `y` e a vari√¢ncia de `x`:

```python
import numpy as np

x = np.array([1, 2, 3, 4, 5])
y = np.array([2, 4, 5, 4, 5])
n = len(x)

x_mean = np.mean(x)
y_mean = np.mean(y)


covariance_xy = np.sum((x - x_mean) * (y - y_mean)) / n
variance_x = np.sum((x - x_mean)**2) / n

b_alternative = covariance_xy / variance_x

print(f"Covari√¢ncia(x,y): {covariance_xy:.2f}")
print(f"Vari√¢ncia(x): {variance_x:.2f}")
print(f"Inclina√ß√£o (b) usando cov/var: {b_alternative:.2f}")
```
> Os c√°lculos mostram que a covari√¢ncia √© 1.20, a vari√¢ncia de x √© 2.0, e a inclina√ß√£o `b` usando covari√¢ncia sobre vari√¢ncia √© tamb√©m 0.60, confirmando o resultado anterior.

### Considera√ß√µes Adicionais
- **Interpreta√ß√£o do Modelo:** √â crucial interpretar corretamente os valores de $a$ e $b$ no contexto do problema. O intercepto √© o ponto de partida da linha, enquanto a inclina√ß√£o determina como a demanda varia com o tempo. A interpreta√ß√£o errada desses valores pode levar a decis√µes equivocadas.

- **Extrapola√ß√£o:** √â importante ser cauteloso ao usar a equa√ß√£o da reta para prever a demanda em per√≠odos muito distantes dos dados utilizados no treinamento do modelo. Como a proje√ß√£o de tend√™ncia linear assume uma rela√ß√£o linear, prever a demanda muito al√©m do escopo dos dados de treinamento pode levar a erros.
- **Valida√ß√£o das Premissas:** As suposi√ß√µes da regress√£o linear [^2] precisam ser verificadas para garantir a validade dos resultados. Se os pressupostos de linearidade, normalidade, homocedasticidade e independ√™ncia dos res√≠duos n√£o forem v√°lidas, ent√£o √© necess√°rio utilizar m√©todos de previs√£o diferentes, ou efetuar transforma√ß√µes nos dados para melhorar o ajuste do modelo.

**Teorema 1** *A soma dos res√≠duos de um modelo de regress√£o linear ajustado pelo m√©todo dos m√≠nimos quadrados √© zero.*

**Prova:**
I. A partir da dedu√ß√£o da equa√ß√£o do intercepto, temos que:
$$a = \bar{y} - b\bar{x}$$
e tamb√©m que:
$$\bar{y} = a + b\bar{x}$$

II. A equa√ß√£o da reta de regress√£o no ponto $(\bar{x}, \bar{y})$ √© dada por:
$$\hat{y_i} = a + bx_i$$

III. O res√≠duo para cada ponto √© dado por:
$$e_i = y_i - \hat{y_i}$$

IV. A soma dos res√≠duos √© ent√£o:
$$ \sum_{i=1}^n e_i = \sum_{i=1}^n (y_i - \hat{y_i}) =  \sum_{i=1}^n (y_i - (a + bx_i)) $$

V. Substituindo $a$:
$$ \sum_{i=1}^n e_i = \sum_{i=1}^n (y_i - (\bar{y} - b\bar{x} + bx_i)) = \sum_{i=1}^n (y_i - \bar{y} - b(x_i - \bar{x})) $$
VI. Distribuindo o somat√≥rio:
$$\sum_{i=1}^n e_i = \sum_{i=1}^n (y_i - \bar{y}) - b\sum_{i=1}^n (x_i - \bar{x})$$

VII. Como a soma dos desvios em rela√ß√£o √† m√©dia √© sempre zero, temos:
$$\sum_{i=1}^n (y_i - \bar{y}) = 0$$
e
$$\sum_{i=1}^n (x_i - \bar{x}) = 0$$

VIII. Portanto:
$$ \sum_{i=1}^n e_i = 0 - b(0) = 0 $$
Isso prova que a soma dos res√≠duos de um modelo de regress√£o linear ajustado pelo m√©todo dos m√≠nimos quadrados √© zero. $\blacksquare$

> üí° **Exemplo Num√©rico:** Vamos calcular os res√≠duos do exemplo anterior e verificar que sua soma √© zero:

```python
import numpy as np

x = np.array([1, 2, 3, 4, 5])
y = np.array([2, 4, 5, 4, 5])
n = len(x)

x_mean = np.mean(x)
y_mean = np.mean(y)

xy_sum = np.sum(x*y)
x_squared_sum = np.sum(x**2)


b = (xy_sum - n * x_mean * y_mean) / (x_squared_sum - n * x_mean**2)
a = y_mean - b * x_mean


y_predicted = a + b * x
residuals = y - y_predicted
print(f"Res√≠duos: {residuals}")
print(f"Soma dos res√≠duos: {np.sum(residuals):.5f}")
```

> O exemplo demonstra que a soma dos res√≠duos √© muito pr√≥xima de zero, dentro de um n√≠vel de precis√£o aceit√°vel devido a arredondamentos computacionais.
### Conclus√£o
A equa√ß√£o da reta de regress√£o, com seu intercepto e inclina√ß√£o, √© o cora√ß√£o da proje√ß√£o de tend√™ncia linear. O m√©todo dos m√≠nimos quadrados √© a ferramenta matem√°tica que nos permite encontrar os par√¢metros ideais para essa equa√ß√£o. A compreens√£o profunda desses elementos √© essencial para a constru√ß√£o e interpreta√ß√£o correta de modelos de previs√£o de demanda baseados em regress√£o linear. A precis√£o das previs√µes dependem da correta interpreta√ß√£o das suposi√ß√µes da regress√£o linear, com vistas √† implementa√ß√£o de m√©todos mais robustos se necess√°rio.

### Refer√™ncias
[^1]: Chapter 3, p. 57
[^2]: Previous Topic
<!-- END -->
