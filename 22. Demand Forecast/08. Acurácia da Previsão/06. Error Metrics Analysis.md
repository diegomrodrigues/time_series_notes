## Avalia√ß√£o Comparativa de Modelos de Previs√£o: Sele√ß√£o Baseada em M√©tricas de Erro

### Introdu√ß√£o
Como explorado nos cap√≠tulos anteriores [^1], a previs√£o de demanda √© um processo complexo que envolve a escolha e a aplica√ß√£o de diferentes modelos e t√©cnicas. A sele√ß√£o do modelo mais adequado √© crucial para garantir previs√µes precisas e para facilitar a tomada de decis√µes estrat√©gicas. As m√©tricas de erro, como o Desvio M√©dio Absoluto (MAD), o Erro Quadr√°tico M√©dio (MSE) e o Erro Percentual Absoluto M√©dio (MAPE), desempenham um papel fundamental na avalia√ß√£o e compara√ß√£o do desempenho desses modelos [^6]. Este cap√≠tulo visa fornecer uma an√°lise detalhada de como usar essas m√©tricas para selecionar o melhor modelo de previs√£o para um cen√°rio espec√≠fico, com base nos resultados dos c√°lculos de erros de previs√£o.

### Conceitos Fundamentais
A avalia√ß√£o de modelos de previs√£o envolve a compara√ß√£o das m√©tricas de erro calculadas para cada modelo em um conjunto de dados de teste ou valida√ß√£o [^6]. O modelo com o menor erro (seja MAD, MSE ou MAPE, dependendo do contexto) √© considerado o mais preciso para aquele cen√°rio espec√≠fico. O objetivo √© identificar o modelo que melhor se ajusta aos dados hist√≥ricos e que, portanto, tem maior probabilidade de gerar previs√µes mais precisas para o futuro.

**Observa√ß√£o 12:** A sele√ß√£o do modelo n√£o deve se basear apenas em uma √∫nica m√©trica, mas sim em uma combina√ß√£o das informa√ß√µes fornecidas pelo MAD, MSE e MAPE, e uma an√°lise contextual que leve em conta a natureza do problema e os custos associados a diferentes tipos de erros.

#### Compara√ß√£o das M√©tricas MAD, MSE e MAPE
Como discutido em cap√≠tulos anteriores [^1], cada m√©trica de erro tem propriedades e aplica√ß√µes distintas:

*   **MAD:** Mede a magnitude m√©dia dos erros de previs√£o em unidades absolutas. √â √∫til para entender a escala m√©dia dos erros, sendo menos sens√≠vel a *outliers* [^6].

*   **MSE:** Mede a m√©dia dos quadrados dos erros de previs√£o. Penaliza erros maiores, sendo mais adequado quando grandes erros t√™m consequ√™ncias mais graves [^6]. O MSE √© expressa em unidades ao quadrado.

*   **MAPE:** Mede a magnitude m√©dia dos erros de previs√£o como uma porcentagem da demanda real. Facilita a compara√ß√£o entre conjuntos de dados com diferentes escalas, mas √© inst√°vel quando a demanda real √© zero ou pr√≥xima de zero [^6].

**Teorema 6:**  N√£o existe uma m√©trica de erro "universalmente melhor". A escolha entre MAD, MSE e MAPE depende do contexto espec√≠fico do problema e das consequ√™ncias dos diferentes tipos de erros.
*Prova:*
I. MAD, MSE e MAPE s√£o m√©tricas que avaliam a acur√°cia de previs√£o de maneiras distintas, cada uma com suas vantagens e desvantagens.
II. O MAD √© uma m√©trica simples e intuitiva que quantifica a magnitude m√©dia dos erros absolutos, fornecendo uma medida geral da precis√£o.
III. O MSE enfatiza a penaliza√ß√£o de erros maiores, tornando-o mais adequado quando erros grandes t√™m um custo ou consequ√™ncia desproporcionalmente maior do que erros menores.
IV. O MAPE expressa a precis√£o em termos percentuais, o que facilita a compara√ß√£o entre conjuntos de dados com escalas diferentes. Contudo, o MAPE pode se tornar inst√°vel quando a demanda √© nula ou pr√≥xima de zero, ou quando a demanda real j√° est√° expressa em porcentagem ou taxa.
V.  N√£o existe uma m√©trica "universalmente melhor", pois a escolha ideal depende da natureza da previs√£o, das consequ√™ncias de erros espec√≠ficos e das prefer√™ncias do analista, n√£o existindo uma m√©trica melhor em todos os casos. ‚ñ†

**Lema 6.1:** Em cen√°rios onde a import√¢ncia de todos os erros √© similar, e sem consequ√™ncias maiores associadas a erros maiores, o MAD tende a ser a m√©trica mais indicada, fornecendo uma vis√£o geral da magnitude dos erros.
*Prova:*
I. O MAD calcula a m√©dia dos erros absolutos, tratando todos os erros da mesma maneira.
II. Em situa√ß√µes onde n√£o h√° penalidades por erros maiores, a magnitude geral do erro √© o fator mais relevante para a escolha do modelo.
III. Portanto, quando n√£o h√° prefer√™ncias por erros espec√≠ficos, o MAD tende a ser a m√©trica mais adequada, por fornecer uma medida geral do desempenho do modelo sem priorizar um tipo de erro em detrimento de outro. ‚ñ†

**Lema 6.2:** Em cen√°rios em que grandes erros t√™m custos significativamente maiores do que erros pequenos, o MSE tende a ser a m√©trica mais apropriada para a sele√ß√£o do modelo de previs√£o.
*Prova:*
I. O MSE calcula a m√©dia dos erros ao quadrado, penalizando mais os erros maiores.
II. Em situa√ß√µes onde as consequ√™ncias dos erros maiores s√£o muito piores, essa caracter√≠stica do MSE alinha a avalia√ß√£o da precis√£o do modelo com a prioriza√ß√£o dada pela severidade de tais erros.
III. Portanto, o MSE tende a ser a m√©trica mais apropriada nesses cen√°rios, onde a penaliza√ß√£o por grandes erros √© relevante para a escolha do modelo. ‚ñ†

**Lema 6.3:** O MAPE pode ser usado para comparar modelos de previs√£o em diferentes escalas, fornecendo uma medida da precis√£o expressa em termos percentuais. No entanto, deve ser usado com cautela, pois √© inst√°vel quando a demanda √© zero ou pr√≥xima de zero.
*Prova:*
I.  O MAPE expressa os erros de previs√£o como um percentual da demanda real, o que torna poss√≠vel comparar modelos em diferentes escalas.
II. Essa normaliza√ß√£o atrav√©s da divis√£o pela demanda real, torna o MAPE invariante para transforma√ß√µes lineares que preservem a origem.
III.  No entanto, o MAPE √© inst√°vel quando a demanda real √© nula ou pr√≥xima de zero, porque a divis√£o por zero ou n√∫meros muito pequenos podem levar a valores muito grandes ou indeterminados, conforme discutido anteriormente [^1].
IV. Portanto, o MAPE deve ser usado com cautela, e outras m√©tricas devem ser consideradas quando existe a possibilidade da demanda ser pr√≥xima de zero. ‚ñ†

**Proposi√ß√£o 1:**  Uma alternativa ao MAPE quando a demanda pode ser zero √© o SMAPE (Symmetric Mean Absolute Percentage Error), que usa a m√©dia da demanda real e da demanda prevista como denominador, tornando a m√©trica mais est√°vel perto de zero.
*Defini√ß√£o:*
O SMAPE √© definido como:
$$SMAPE = \frac{1}{n}\sum_{t=1}^{n} \frac{|y_t - \hat{y}_t|}{(|y_t| + |\hat{y}_t|)/2} * 100$$,
onde $y_t$ √© a demanda real no per√≠odo $t$, e $\hat{y}_t$ √© a demanda prevista no per√≠odo $t$.
*Prova:*
I. Ao utilizar a m√©dia de $|y_t|$ e $|\hat{y}_t|$ no denominador, o SMAPE evita a divis√£o por zero, que pode ocorrer no MAPE quando a demanda real ($y_t$) √© zero ou muito pequena.
II.  O SMAPE tamb√©m oferece uma interpreta√ß√£o percentual da magnitude dos erros, similar ao MAPE, facilitando a compara√ß√£o entre diferentes s√©ries.
III. Em situa√ß√µes onde tanto a demanda real como a prevista podem estar perto de zero, o SMAPE oferece uma avalia√ß√£o mais est√°vel e confi√°vel do desempenho do modelo do que o MAPE.
IV. Portanto, o SMAPE √© uma alternativa √∫til ao MAPE quando a demanda real pode estar perto de zero. ‚ñ†
> üí° **Exemplo Num√©rico:** Vamos comparar o MAPE e SMAPE em um cen√°rio onde a demanda real pode ser zero. Suponha que tenhamos as seguintes previs√µes e demandas reais para tr√™s per√≠odos:
>
> | Per√≠odo | Demanda Real ($y_t$) | Previs√£o ($\hat{y}_t$) |
> |---------|---------------------|----------------------|
> | 1       | 100                 | 110                 |
> | 2       | 50                  | 45                  |
> | 3       | 0                   | 5                   |
>
> **C√°lculo do MAPE:**
>
> $$\text{MAPE} = \frac{1}{n}\sum_{t=1}^{n} \frac{|y_t - \hat{y}_t|}{|y_t|} * 100$$
>
> $$\text{MAPE} = \frac{1}{3} \left( \frac{|100-110|}{100} + \frac{|50-45|}{50} + \frac{|0-5|}{0} \right) * 100$$
>
> $$\text{MAPE} = \frac{1}{3} \left( \frac{10}{100} + \frac{5}{50} + \frac{5}{0} \right) * 100$$
>
> Note que a divis√£o por zero no terceiro per√≠odo torna o MAPE indefinido neste caso.
>
> **C√°lculo do SMAPE:**
>
> $$\text{SMAPE} = \frac{1}{n}\sum_{t=1}^{n} \frac{|y_t - \hat{y}_t|}{(|y_t| + |\hat{y}_t|)/2} * 100$$
>
> $$\text{SMAPE} = \frac{1}{3} \left( \frac{|100-110|}{(100+110)/2} + \frac{|50-45|}{(50+45)/2} + \frac{|0-5|}{(0+5)/2} \right) * 100$$
>
> $$\text{SMAPE} = \frac{1}{3} \left( \frac{10}{105} + \frac{5}{47.5} + \frac{5}{2.5} \right) * 100$$
>
> $$\text{SMAPE} \approx \frac{1}{3} (0.095 + 0.105 + 2) * 100 \approx \frac{2.2}{3} * 100 \approx 73.33\%$$
>
> Como pode ser visto, o SMAPE fornece uma m√©trica mais est√°vel e definida, mesmo quando a demanda real √© zero, enquanto o MAPE n√£o. Isso demonstra a utilidade do SMAPE em cen√°rios com demanda nula ou pr√≥xima de zero.

### Processo de Sele√ß√£o de Modelos Baseado em M√©tricas de Erro
O processo de sele√ß√£o do modelo de previs√£o mais adequado envolve as seguintes etapas:

1.  **Definir o objetivo da previs√£o:** O primeiro passo √© identificar claramente o objetivo da previs√£o e as consequ√™ncias associadas a diferentes tipos de erros.
2.  **Escolher m√©tricas de avalia√ß√£o:** Com base no objetivo da previs√£o, escolha as m√©tricas de erro mais adequadas (MAD, MSE ou MAPE, ou uma combina√ß√£o delas).
3.  **Aplicar os modelos:** Aplique diferentes modelos de previs√£o aos dados hist√≥ricos e calcule as m√©tricas de erro para cada modelo no conjunto de dados de teste.
4.  **Comparar os resultados:** Compare os resultados das m√©tricas de erro entre os diferentes modelos.
5.  **Selecionar o modelo:** Selecione o modelo com as menores m√©tricas de erro, levando em considera√ß√£o o contexto e as propriedades de cada m√©trica.
6.  **Validar o modelo:** Verifique se as suposi√ß√µes do modelo s√£o v√°lidas e se os res√≠duos seguem um padr√£o aleat√≥rio. Ajuste o modelo, se necess√°rio.

> üí° **Exemplo Num√©rico:** Comparando tr√™s modelos (SMA, WMA e Suaviza√ß√£o Exponencial) e suas respectivas m√©tricas de erro.
>
> Suponha que temos tr√™s modelos de previs√£o diferentes, SMA (m√©dia m√≥vel simples), WMA (m√©dia m√≥vel ponderada) e suaviza√ß√£o exponencial, que foram aplicados aos mesmos dados de demanda, gerando as seguintes m√©tricas de erro:
>
> | Modelo                | MAD  | MSE     | MAPE  |
> |-----------------------|------|---------|-------|
> | SMA (n=3)             | 198.22  | 51665.90   | 14.36%|
> | WMA (pesos 0.5, 0.3, 0.2)  | 178.44 | 41964.53 | 12.72% |
> | Suaviza√ß√£o Exponencial (Œ± = 0.4) | 180.52    | 44397.08  | 13.19% |
>
> Para escolher o melhor modelo, analisamos o seguinte:
> * **MAD:** O modelo WMA apresenta o menor MAD (178.44), sugerindo que seus erros de previs√£o t√™m, em m√©dia, a menor magnitude.
> * **MSE:**  O modelo WMA apresenta o menor MSE (41964.53), o que indica que os erros maiores s√£o mais penalizados nesse modelo.
> * **MAPE:** O modelo WMA apresenta o menor MAPE (12.72%), indicando que os erros de previs√£o est√£o em torno de 12.72% da demanda real.
>
> Neste cen√°rio, o modelo WMA apresenta o melhor resultado em todas as m√©tricas, sugerindo que √© o mais apropriado.

**Observa√ß√£o 13:** Nem sempre o mesmo modelo ser√° o melhor para todas as m√©tricas. √â importante analisar os resultados das m√©tricas de forma integrada e considerar o contexto espec√≠fico do problema. Se, por exemplo, grandes erros t√™m um impacto desproporcionalmente alto no resultado, o modelo com o menor MSE pode ser prefer√≠vel, mesmo que o seu MAD seja ligeiramente maior que o de outro modelo. J√° se o objetivo √© um modelo com erros percentuais menores, o modelo com menor MAPE pode ser prefer√≠vel, a menos que haja valores de demanda muito pr√≥ximos de zero.

**Corol√°rio 6.1:** A combina√ß√£o do uso de v√°rias m√©tricas (MAD, MSE e MAPE) e de uma an√°lise contextual √© fundamental para escolher o melhor modelo de previs√£o para o cen√°rio espec√≠fico.
*Prova:*
I. Cada m√©trica oferece uma perspectiva diferente do desempenho do modelo de previs√£o.
II.  O uso de v√°rias m√©tricas permite uma avalia√ß√£o mais abrangente e robusta do desempenho do modelo.
III. A an√°lise contextual permite levar em considera√ß√£o fatores relevantes que podem afetar a escolha da m√©trica mais adequada e, portanto, a escolha do modelo.
IV.  Portanto, uma an√°lise integrada e contextual das m√©tricas de erro √© crucial para escolher o modelo de previs√£o mais adequado para cada cen√°rio espec√≠fico. ‚ñ†

**Corol√°rio 6.2:** Quando a demanda pode ser zero ou muito pr√≥xima de zero, deve-se considerar o uso do SMAPE em conjunto com outras m√©tricas, como o MAD e o MSE, para uma avalia√ß√£o mais robusta do modelo.
*Prova:*
I. O MAPE pode apresentar resultados inst√°veis e pouco confi√°veis quando a demanda √© zero ou muito pr√≥xima de zero.
II. O SMAPE corrige essa instabilidade ao utilizar a m√©dia dos valores absolutos da demanda real e prevista no denominador, o que o torna mais adequado nessas situa√ß√µes.
III.  A combina√ß√£o do SMAPE com outras m√©tricas como MAD e MSE oferece uma vis√£o mais abrangente da performance do modelo, considerando tanto a magnitude dos erros como o seu comportamento percentual.
IV. Portanto, √© aconselh√°vel usar o SMAPE em conjunto com outras m√©tricas quando a demanda pode ser zero ou muito pequena. ‚ñ†

### An√°lise da Distribui√ß√£o de Erros
√â importante verificar se os res√≠duos do modelo selecionado seguem um padr√£o aleat√≥rio, sem vi√©s ou autocorrela√ß√£o, e se a sua distribui√ß√£o se aproxima de uma normal [^1]. A an√°lise de res√≠duos √© um passo crucial na avalia√ß√£o da qualidade do modelo de previs√£o e permite detectar poss√≠veis problemas ou defici√™ncias que possam levar a previs√µes enviesadas ou menos precisas.
Conforme discutido no cap√≠tulo anterior, padr√µes nos res√≠duos, aus√™ncia de normalidade, ou a presen√ßa de heterocedasticidade, podem indicar a necessidade de ajuste do modelo, ou at√© mesmo a necessidade da escolha de outro modelo.

**Observa√ß√£o 14:** Al√©m da an√°lise visual dos res√≠duos, testes estat√≠sticos como o teste de Shapiro-Wilk para normalidade, o teste de Ljung-Box para autocorrela√ß√£o e testes para heterocedasticidade, podem auxiliar a identificar problemas nos res√≠duos.

> üí° **Exemplo Num√©rico:** Suponha que temos um modelo que gera as seguintes previs√µes e res√≠duos:
>
> | Per√≠odo | Demanda Real ($y_t$) | Previs√£o ($\hat{y}_t$) | Res√≠duo ($e_t = y_t - \hat{y}_t$) |
> |---------|---------------------|----------------------|--------------------------------|
> | 1       | 100                 | 105                  | -5                             |
> | 2       | 110                 | 112                  | -2                             |
> | 3       | 120                 | 118                  | 2                              |
> | 4       | 130                 | 133                  | -3                             |
> | 5       | 140                 | 138                  | 2                              |
> | 6       | 150                 | 152                  | -2                             |
> | 7       | 160                 | 157                  | 3                              |
> | 8       | 170                 | 173                  | -3                             |
> | 9       | 180                 | 178                  | 2                              |
> | 10      | 190                 | 192                  | -2                             |
>
> Podemos analisar os res√≠duos:
> *   **M√©dia dos res√≠duos:** A m√©dia dos res√≠duos √© $(-5 -2 + 2 -3 + 2 -2 + 3 -3 + 2 -2) / 10 = -1$, o que sugere um pequeno vi√©s.
> *   **Visualiza√ß√£o dos res√≠duos:** Ao plotar os res√≠duos ao longo do tempo, podemos verificar se h√° padr√µes como autocorrela√ß√£o ou heterocedasticidade. Um gr√°fico de res√≠duos sem padr√µes indica um modelo melhor ajustado.
> *   **Teste de Shapiro-Wilk:** Aplicando o teste de Shapiro-Wilk, podemos verificar se os res√≠duos se aproximam de uma distribui√ß√£o normal. Em Python, usando `scipy.stats`, temos:
>
> ```python
> import numpy as np
> from scipy.stats import shapiro
>
> residuos = np.array([-5, -2, 2, -3, 2, -2, 3, -3, 2, -2])
> stat, p = shapiro(residuos)
> print('estat√≠stica de teste=%.3f, p=%.3f' % (stat, p))
> alpha = 0.05
> if p > alpha:
>     print('Amostra parece Gaussiana (falha ao rejeitar H0)')
> else:
>     print('Amostra n√£o parece Gaussiana (rejeita H0)')
> ```
>
> Se o p-valor for menor que 0.05, podemos rejeitar a hip√≥tese nula de que os res√≠duos s√£o normalmente distribu√≠dos.
>
> A an√°lise detalhada dos res√≠duos √© crucial para garantir que o modelo n√£o tenha problemas que afetem a qualidade das previs√µes.

### T√©cnicas de Valida√ß√£o
Ap√≥s selecionar um modelo, √© necess√°rio validar seu desempenho em um conjunto de dados de teste ou valida√ß√£o, utilizando t√©cnicas como:

1. **Valida√ß√£o Cruzada:** A valida√ß√£o cruzada divide os dados em v√°rios conjuntos de treinamento e teste, utilizando os diferentes conjuntos para estimar e validar o modelo. Isso garante uma avalia√ß√£o robusta da capacidade de generaliza√ß√£o do modelo para dados n√£o vistos.
2. **Holdout Method:** O holdout method divide os dados em um conjunto de treinamento e um conjunto de teste, utilizando o conjunto de teste para validar o modelo.
3. **Rolling Forecast:** O rolling forecast utiliza uma janela deslizante de dados para estimar os par√¢metros do modelo e fazer as previs√µes, garantindo uma avalia√ß√£o mais realista da sua performance ao longo do tempo.

**Lema 7:** A utiliza√ß√£o de t√©cnicas de valida√ß√£o robustas, como valida√ß√£o cruzada ou rolling forecast, √© fundamental para garantir a capacidade de generaliza√ß√£o do modelo para dados n√£o vistos, resultando em previs√µes mais confi√°veis e precisas em cen√°rios reais.
*Prova:*
I. A valida√ß√£o cruzada divide os dados em v√°rias partes, utilizando cada parte para treinamento e teste do modelo, garantindo que o modelo seja avaliado em diferentes amostras.
II. O holdout method divide os dados em um conjunto de treinamento e um conjunto de teste, mas n√£o avalia o modelo em diferentes amostras.
III. O rolling forecast simula uma situa√ß√£o de previs√£o em tempo real, com os dados sendo alimentados em per√≠odos consecutivos, permitindo uma avalia√ß√£o mais realista da performance do modelo ao longo do tempo.
IV.  A utiliza√ß√£o de t√©cnicas robustas de valida√ß√£o ajuda a prevenir o sobreajuste (overfitting), garantindo que o modelo n√£o esteja simplesmente decorando os dados de treinamento, mas seja capaz de gerar previs√µes precisas para dados n√£o vistos. ‚ñ†

> üí° **Exemplo Num√©rico:** Ilustrando o m√©todo de Valida√ß√£o Cruzada (k-fold)
>
> Suponha que temos uma s√©rie temporal com 10 per√≠odos de dados. Para aplicar a valida√ß√£o cruzada k-fold com k=5, dividimos os dados em 5 grupos, ou folds.  Em cada itera√ß√£o, 4 folds s√£o usados para treinamento e 1 para teste:
>
> *   **Fold 1:** Treino nos per√≠odos 3, 4, 5, 6, 7, 8, 9, 10; Teste nos per√≠odos 1, 2.
> *   **Fold 2:** Treino nos per√≠odos 1, 2, 5, 6, 7, 8, 9, 10; Teste nos per√≠odos 3, 4.
> *   **Fold 3:** Treino nos per√≠odos 1, 2, 3, 4, 7, 8, 9, 10; Teste nos per√≠odos 5, 6.
> *   **Fold 4:** Treino nos per√≠odos 1, 2, 3, 4, 5, 6, 9, 10; Teste nos per√≠odos 7, 8.
> *   **Fold 5:** Treino nos per√≠odos 1, 2, 3, 4, 5, 6, 7, 8; Teste nos per√≠odos 9, 10.
>
> Em cada fold, o modelo √© treinado e avaliado, gerando uma m√©trica de erro (e.g., MSE). O resultado final da valida√ß√£o cruzada √© a m√©dia dessas m√©tricas, fornecendo uma avalia√ß√£o mais robusta do desempenho do modelo.
>
> ```mermaid
> graph LR
>     A[Dados Totais] --> B(Fold 1: Treino 3-10, Teste 1-2);
>     A --> C(Fold 2: Treino 1-2, 5-10, Teste 3-4);
>     A --> D(Fold 3: Treino 1-4, 7-10, Teste 5-6);
>     A --> E(Fold 4: Treino 1-6, 9-10, Teste 7-8);
>     A --> F(Fold 5: Treino 1-8, Teste 9-10);
>     B --> G(Avalia√ß√£o);
>     C --> G;
>     D --> G;
>     E --> G;
>     F --> G;
>     G --> H{M√©dia das M√©tricas};
> ```
>
> No *Rolling Forecast*, em vez de folds, separamos uma janela temporal de treinamento e uma de teste, e "rolamos" essa janela ao longo da s√©rie. Por exemplo, se tivermos 12 per√≠odos, podemos treinar com os 8 primeiros e testar os 4 seguintes, e depois usar os per√≠odos de 2 a 9 para treinar, e testar 10 a 12, e assim por diante.

### Conclus√£o
A sele√ß√£o do melhor modelo de previs√£o envolve um processo cuidadoso de an√°lise e compara√ß√£o de diferentes modelos com base em m√©tricas de erro como MAD, MSE e MAPE [^6]. A decis√£o final depende do contexto espec√≠fico do problema, da an√°lise dos res√≠duos e do desempenho do modelo em conjuntos de dados de teste. Ao aplicar m√©tricas e t√©cnicas de valida√ß√£o adequadas, gestores e analistas podem garantir a sele√ß√£o dos modelos de previs√£o mais confi√°veis e tomar decis√µes mais informadas, otimizando a aloca√ß√£o de recursos e melhorando a efici√™ncia operacional.

### Refer√™ncias
[^1]: Cap√≠tulos anteriores sobre acur√°cia de previs√£o, an√°lise de res√≠duos, MAD, MSE e MAPE.
[^6]: Cap√≠tulo 3, Se√ß√£o sobre Acur√°cia da Previs√£o.
<!-- END -->
