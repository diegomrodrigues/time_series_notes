## Modelos Associativos: Uma An√°lise Detalhada

### Introdu√ß√£o
Este cap√≠tulo, dando continuidade √† discuss√£o sobre previs√£o de demanda, aprofunda-se nos **modelos associativos**, uma abordagem que expande a an√°lise dos modelos de s√©ries temporais, considerando que a demanda √© afetada n√£o apenas pelo tempo, mas tamb√©m por outras vari√°veis relevantes [^1]. Ao contr√°rio dos modelos de s√©ries temporais, que se baseiam em padr√µes hist√≥ricos de demanda, os modelos associativos exploram rela√ß√µes causais entre a demanda e outras vari√°veis, como pre√ßo, localiza√ß√£o, demografia e outros fatores externos [^1]. Este cap√≠tulo explora a fundo a l√≥gica, a matem√°tica e a aplica√ß√£o destes modelos, construindo sobre conceitos previamente estabelecidos.

### Conceitos Fundamentais
Os **modelos associativos** diferem dos modelos de s√©ries temporais, pois partem da premissa de que a demanda √© uma fun√ß√£o complexa, influenciada por v√°rios fatores al√©m do tempo [^1]. O objetivo desses modelos √© identificar e quantificar essas rela√ß√µes causais para criar previs√µes mais robustas e adapt√°veis.

**Vari√°veis Independentes e Dependentes**
O modelo associativo busca a rela√ß√£o matem√°tica entre a vari√°vel dependente (demanda) e uma ou mais vari√°veis independentes ou preditoras. Essas vari√°veis independentes podem incluir pre√ßo, localiza√ß√£o, dados demogr√°ficos e fatores externos [^1].
A regress√£o linear m√∫ltipla √© uma t√©cnica chave para modelar essas rela√ß√µes, expressando a vari√°vel dependente como uma combina√ß√£o linear das vari√°veis independentes [^1].

**T√©cnicas de Modelagem Associativa**
A **regress√£o linear m√∫ltipla** √© fundamental nos modelos associativos, permitindo expressar a vari√°vel dependente como uma combina√ß√£o linear de vari√°veis independentes. Outras t√©cnicas, como a **regress√£o polinomial**, podem ser utilizadas para rela√ß√µes n√£o lineares [^1].

**Proposi√ß√£o 1**
A **regress√£o polinomial** √© usada para modelar rela√ß√µes n√£o lineares entre a demanda e as vari√°veis independentes. A equa√ß√£o geral para um modelo de regress√£o polinomial de grau *p* √© dada por:

$$
\hat{y} = a + b_1x + b_2x^2 + \ldots + b_px^p
$$

onde $\hat{y}$ √© o valor previsto da demanda, $a$ √© o intercepto, e $b_1$, $b_2$, ..., $b_p$ s√£o os coeficientes de regress√£o das pot√™ncias da vari√°vel independente $x$. Esta t√©cnica √© mais adequada em cen√°rios onde a rela√ß√£o linear n√£o √© suficiente, capturando rela√ß√µes curvil√≠neas entre as vari√°veis. A escolha do grau do polin√¥mio (*p*) deve ser cuidadosa para evitar *overfitting*.

> üí° **Exemplo Num√©rico:** Como exemplificado anteriormente, a demanda por um produto, em fun√ß√£o do pre√ßo, pode ser modelada por uma regress√£o polinomial, mostrando que a queda na demanda se acentua quando o pre√ßo est√° mais alto. Se a regress√£o polinomial de grau 2 √© dada por $\hat{y} = 1000 - 50x + 0.5x^2$, podemos observar que com o pre√ßo ($x$) de 10, a demanda ($\hat{y}$) √© 550, enquanto que com o pre√ßo de 20, a demanda √© de 200.
<br>

A equa√ß√£o geral para um modelo de **regress√£o linear m√∫ltipla**, previamente apresentada em [^1] como [3.14], √©:

$$
\hat{y} = a + b_1x_1 + b_2x_2 + \ldots + b_nx_n
$$

onde $\hat{y}$ √© a demanda prevista, $a$ √© o intercepto, e $b_1$, $b_2$, ..., $b_n$ s√£o os coeficientes de regress√£o correspondentes √†s vari√°veis independentes $x_1$, $x_2$, ..., $x_n$.

**Interpreta√ß√£o dos Coeficientes de Regress√£o**

Os coeficientes de regress√£o ($b$) em [3.14] indicam como cada vari√°vel independente afeta a demanda. Coeficientes positivos indicam que um aumento na vari√°vel independente leva a um aumento na demanda, enquanto coeficientes negativos indicam uma rela√ß√£o inversa. A magnitude do coeficiente indica a intensidade dessa rela√ß√£o. Modelos n√£o lineares apresentam interpreta√ß√µes mais complexas, pois o efeito da vari√°vel independente sobre a demanda varia com o valor dessa vari√°vel.

**Exemplo:**
Como visto em [^2], a demanda por leitos em lares de idosos aumenta com o aumento da popula√ß√£o acima de 65 anos e diminui com o aumento de pessoas abaixo da linha da pobreza. Um modelo de regress√£o linear m√∫ltipla seria:

$$
\hat{y} = 2905.43 + 19.92x_1 - 63.17x_2
$$

Onde:
* $\hat{y}$ = N√∫mero previsto de leitos em lares de idosos.
* $x_1$ = Popula√ß√£o com 65 anos ou mais (em milhares).
* $x_2$ = N√∫mero de pessoas abaixo da linha da pobreza (em milhares).

Um aumento de 1.000 pessoas na popula√ß√£o com 65 anos ou mais resulta em um aumento de aproximadamente 19,92 leitos, enquanto um aumento de 1.000 pessoas abaixo da linha da pobreza diminui a demanda em aproximadamente 63,17 leitos.

> üí° **Exemplo Num√©rico:** Usando este modelo, vamos analisar como calcular e como avaliar o impacto de varia√ß√µes nas vari√°veis independentes:
>
> Suponha que temos $x_1 = 100$ (100 mil idosos) e $x_2 = 20$ (20 mil abaixo da linha de pobreza). A demanda prevista ser√°: $\hat{y} = 2905.43 + 19.92(100) - 63.17(20) = 3634.03$.
>
> Se aumentarmos $x_1$ para 110 (110 mil), mantendo $x_2$ em 20, temos: $\hat{y} = 2905.43 + 19.92(110) - 63.17(20) = 3833.23$. Um aumento de 10 mil idosos gera um aumento de aproximadamente 199 leitos.
>
> Se aumentarmos $x_2$ para 30 (30 mil), mantendo $x_1$ em 100, temos: $\hat{y} = 2905.43 + 19.92(100) - 63.17(30) = 3002.33$. Um aumento de 10 mil pessoas abaixo da linha da pobreza causa uma diminui√ß√£o de cerca de 632 leitos.
<br>
**Teorema 1**
Em modelos de regress√£o linear m√∫ltipla, a signific√¢ncia estat√≠stica de cada vari√°vel independente pode ser avaliada usando testes de hip√≥teses, como o teste t. A hip√≥tese nula (H0) √© que o coeficiente de regress√£o (b) da vari√°vel independente √© igual a zero, indicando que a vari√°vel n√£o tem efeito na vari√°vel dependente. O teste t avalia a probabilidade (p-valor) de observar os dados, assumindo que H0 √© verdadeira. Se o p-valor for menor que um n√≠vel de signific√¢ncia (Œ±), como 0.05, rejeita-se H0, concluindo que a vari√°vel independente tem um efeito estatisticamente significativo sobre a demanda.

**Prova:** O teste t para cada coeficiente de regress√£o, $b_i$, √© calculado como $t = \frac{b_i}{SE(b_i)}$, onde $SE(b_i)$ √© o erro padr√£o do coeficiente $b_i$. O p-valor associado ao valor de t √© ent√£o comparado com o n√≠vel de signific√¢ncia. Se p-valor < Œ±, a hip√≥tese nula de que $b_i = 0$ √© rejeitada, indicando que a vari√°vel $x_i$ tem um impacto significativo em $\hat{y}$.
<br>
I. O teste t √© usado para determinar se o coeficiente de regress√£o $b_i$ √© estatisticamente diferente de zero.
II. A estat√≠stica de teste t √© calculada como $t = \frac{b_i}{SE(b_i)}$, onde $SE(b_i)$ √© o erro padr√£o do coeficiente $b_i$.
III. A hip√≥tese nula $H_0$ √© que $b_i = 0$, o que significa que a vari√°vel independente $x_i$ n√£o tem efeito sobre a vari√°vel dependente $\hat{y}$.
IV. Um valor de p associado ao valor da estat√≠stica t √© determinado.
V. Se o valor de p √© menor que o n√≠vel de signific√¢ncia $\alpha$ (geralmente 0,05), ent√£o rejeitamos a hip√≥tese nula.
VI. Rejeitar a hip√≥tese nula significa que existe evid√™ncia suficiente para concluir que $b_i \neq 0$ e que a vari√°vel independente $x_i$ tem um efeito estatisticamente significativo sobre $\hat{y}$. ‚ñ†

> üí° **Exemplo Num√©rico:** Suponha que, ao executar uma regress√£o linear m√∫ltipla para prever as vendas de um produto, encontramos o seguinte:
>
> *   Coeficiente para a vari√°vel 'pre√ßo' ($b_1$): -25.0
> *   Erro padr√£o do coeficiente do pre√ßo ($SE(b_1)$): 5.0
>
>   O valor de t √©: $t = \frac{-25.0}{5.0} = -5.0$
>
>   Se o p-valor associado a esse valor de t (usando uma distribui√ß√£o t com graus de liberdade apropriados) for menor que 0.05, rejeitamos a hip√≥tese nula, concluindo que o 'pre√ßo' tem um efeito estatisticamente significativo nas vendas.
>
>  Por outro lado, se tiv√©ssemos o seguinte:
>  *  Coeficiente para a vari√°vel 'n√∫mero de concorrentes' ($b_2$): 2.0
>  *  Erro padr√£o do coeficiente do n√∫mero de concorrentes ($SE(b_2$)): 2.5
>
>   O valor de t seria:  $t = \frac{2.0}{2.5} = 0.8$
>
> Se o p-valor para este valor de t for maior que 0.05, n√£o rejeitamos a hip√≥tese nula e n√£o temos evid√™ncias suficientes para concluir que a vari√°vel 'n√∫mero de concorrentes' tem um efeito significativo nas vendas. Isso sugere que o n√∫mero de concorrentes, neste modelo, n√£o tem um impacto estatisticamente relevante nas vendas.

**Proposi√ß√£o 1.1**
Uma extens√£o importante da regress√£o polinomial, especialmente √∫til em modelagem de demanda, √© a utiliza√ß√£o de *splines*. *Splines* permitem ajustar diferentes polin√¥mios em intervalos distintos da vari√°vel independente, oferecendo maior flexibilidade em modelar comportamentos n√£o lineares complexos. O modelo *spline*  √© composto por fun√ß√µes polinomiais conectadas em pontos espec√≠ficos (n√≥s), criando uma curva suave.
Em um modelo *spline* c√∫bico, a fun√ß√£o entre dois n√≥s √© um polin√¥mio de terceiro grau, e as derivadas de primeira e segunda ordem s√£o cont√≠nuas nos n√≥s, garantindo a suavidade da curva. Essa abordagem √© particularmente √∫til quando a rela√ß√£o entre a demanda e a vari√°vel independente muda de forma significativa em diferentes faixas de valores da vari√°vel independente.
<br>

**Ajuste e Valida√ß√£o do Modelo**

O coeficiente de determina√ß√£o $R^2$ mede a adequa√ß√£o de um modelo associativo, indicando a propor√ß√£o da varia√ß√£o na vari√°vel dependente explicada pelo modelo. Um $R^2$ pr√≥ximo de 1 indica que o modelo explica uma grande parte da varia√ß√£o, enquanto um $R^2$ pr√≥ximo de 0 indica que o modelo explica pouco da varia√ß√£o [^1]. √â fundamental reconhecer que correla√ß√£o n√£o implica causalidade, e outros crit√©rios, como o *R¬≤ ajustado* e o erro quadr√°tico m√©dio (RMSE), devem ser considerados.

**Lema 1**
O *R¬≤ ajustado* penaliza a inclus√£o de vari√°veis que n√£o contribuem significativamente para a explica√ß√£o da variabilidade da vari√°vel dependente, prevenindo o *overfitting*. Ele √© calculado como:

$$
R_{ajustado}^2 = 1 - \frac{(1-R^2)(n-1)}{n-p-1}
$$

onde $n$ √© o n√∫mero de observa√ß√µes e $p$ √© o n√∫mero de vari√°veis independentes no modelo. O *R¬≤ ajustado* fornece uma avalia√ß√£o mais precisa do desempenho do modelo, especialmente quando o n√∫mero de vari√°veis independentes √© grande, prevenindo o *overfitting*.

> üí° **Exemplo Num√©rico:** Um modelo de regress√£o com 100 observa√ß√µes ($n=100$), 3 vari√°veis independentes ($p=3$) e $R^2$ de 0.75 tem o $R^2$ ajustado de $R_{ajustado}^2 = 1 - \frac{(1 - 0.75)(100 - 1)}{100 - 3 - 1} = 0.7422$. Esse ajuste mostra que a inclus√£o de vari√°veis que n√£o melhoram o ajuste pode ser penalizada, oferecendo uma avalia√ß√£o mais precisa do modelo.

**Prova:**
A formula do *R¬≤ ajustado*, como previamente demonstrado,  √© dada por:

$$R_{ajustado}^2 = 1 - \frac{\frac{SS_{res}}{n-p-1}}{\frac{SS_{tot}}{n-1}} = 1 - \frac{(1-R^2)(n-1)}{n-p-1}$$

onde $SS_{res}$ √© a soma dos quadrados dos res√≠duos, $SS_{tot}$ √© a soma total dos quadrados, $n$ √© o n√∫mero de observa√ß√µes e $p$ √© o n√∫mero de vari√°veis independentes no modelo. O R¬≤ ajustado penaliza a inclus√£o de vari√°veis que n√£o contribuem significativamente para explicar a variabilidade da vari√°vel dependente, prevenindo o *overfitting*. ‚ñ†
I. O $R^2$ ajustado √© definido como: $R_{ajustado}^2 = 1 - \frac{MS_{res}}{MS_{tot}}$, onde $MS_{res}$ √© o quadrado m√©dio dos res√≠duos e $MS_{tot}$ √© o quadrado m√©dio total.
II. O quadrado m√©dio dos res√≠duos √© calculado como: $MS_{res} = \frac{SS_{res}}{n-p-1}$, onde $SS_{res}$ √© a soma dos quadrados dos res√≠duos, $n$ √© o n√∫mero de observa√ß√µes, e $p$ √© o n√∫mero de vari√°veis independentes.
III. O quadrado m√©dio total √© calculado como: $MS_{tot} = \frac{SS_{tot}}{n-1}$, onde $SS_{tot}$ √© a soma total dos quadrados.
IV. O $R^2$ √© definido como $R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$.
V. Substituindo $SS_{res} = (1 - R^2)SS_{tot}$ e combinando as equa√ß√µes:
$R_{ajustado}^2 = 1 - \frac{\frac{SS_{res}}{n-p-1}}{\frac{SS_{tot}}{n-1}} = 1 - \frac{\frac{(1 - R^2)SS_{tot}}{n-p-1}}{\frac{SS_{tot}}{n-1}} = 1 - \frac{(1 - R^2)(n-1)}{n-p-1}$
VI. Essa f√≥rmula mostra que o $R^2$ ajustado penaliza a adi√ß√£o de vari√°veis que n√£o melhoram significativamente o modelo, pois $n-p-1$ diminui quando p aumenta, e portanto $R_{ajustado}^2$ tamb√©m diminui se a inclus√£o de novas vari√°veis n√£o reduzir os erros do modelo proporcionalmente. ‚ñ†

**Lema 2**
O erro quadr√°tico m√©dio (RMSE) mede a diferen√ßa entre os valores previstos e os valores reais, calculado como:

$$
RMSE = \sqrt{\frac{\sum_{i=1}^{n}(\hat{y}_i - y_i)^2}{n}}
$$

Onde $\hat{y}_i$ s√£o os valores previstos, $y_i$ s√£o os valores reais e $n$ √© o n√∫mero de observa√ß√µes. O RMSE quantifica a magnitude dos erros de previs√£o, com valores menores indicando melhor ajuste.

> üí° **Exemplo Num√©rico:** Em um modelo de previs√£o de demanda, com as seguintes previs√µes e valores reais:
>
>  | Observa√ß√£o (i) | Valor Real ($y_i$) | Valor Previsto ($\hat{y}_i$) | Erro ($y_i - \hat{y}_i$) | Erro Quadr√°tico  ($(\hat{y}_i - y_i)^2$) |
> |-----------------|--------------------|--------------------------|----------------------|------------------------------------|
> | 1               | 100                | 105                      | -5                   | 25                                 |
> | 2               | 120                | 118                      | 2                    | 4                                  |
> | 3               | 110                | 108                      | 2                    | 4                                  |
> | 4               | 130                | 135                      | -5                   | 25                                 |
> | 5               | 125                | 122                      | 3                    | 9                                  |
>
> O RMSE seria $\sqrt{\frac{25 + 4 + 4 + 25 + 9}{5}} = \sqrt{\frac{67}{5}} \approx 3.66$. Isso indica que, em m√©dia, as previs√µes do modelo diferem dos valores reais em cerca de 3.66 unidades.

**Prova:**
Como j√° demonstrado, a f√≥rmula do RMSE √© dada por:

$$RMSE = \sqrt{\frac{\sum_{i=1}^{n}(\hat{y}_i - y_i)^2}{n}}$$

Esta f√≥rmula quantifica a diferen√ßa m√©dia entre os valores previstos e os valores reais, atrav√©s da raiz quadrada do erro quadr√°tico m√©dio. ‚ñ†
I. O erro quadr√°tico m√©dio (RMSE) √© uma medida da magnitude dos erros de previs√£o.
II. Para cada observa√ß√£o $i$, o erro √© calculado como a diferen√ßa entre o valor previsto $\hat{y}_i$ e o valor real $y_i$, ou seja, $\hat{y}_i - y_i$.
III. O erro √© elevado ao quadrado, $(\hat{y}_i - y_i)^2$, para garantir que erros positivos e negativos sejam considerados.
IV. A m√©dia dos erros quadr√°ticos √© calculada: $\frac{\sum_{i=1}^{n}(\hat{y}_i - y_i)^2}{n}$.
V. A raiz quadrada da m√©dia dos erros quadr√°ticos √© calculada para obter o RMSE: $RMSE = \sqrt{\frac{\sum_{i=1}^{n}(\hat{y}_i - y_i)^2}{n}}$.
VI. O RMSE representa a magnitude m√©dia dos erros de previs√£o em unidades da vari√°vel dependente. ‚ñ†

### Vantagens e Desafios dos Modelos Associativos

**Vantagens:**
Os modelos associativos fornecem uma vis√£o causal da demanda, s√£o flex√≠veis, oferecem previs√µes mais robustas e suportam decis√µes estrat√©gicas [^1]. Eles identificam as causas subjacentes da varia√ß√£o na demanda, incorporam diversas vari√°veis, minimizam os efeitos de eventos aleat√≥rios e orientam decis√µes estrat√©gicas [^1].

**Desafios:**
Esses modelos apresentam desafios como complexidade na constru√ß√£o, necessidade de dados de alta qualidade, problemas de multicolinearidade e dificuldades com rela√ß√µes n√£o lineares. A necessidade de conhecimento estat√≠stico e an√°lise de dados, a qualidade dos dados, a multicolinearidade e a limita√ß√£o para rela√ß√µes n√£o lineares s√£o desafios significativos.

**Observa√ß√£o 1**
A **multicolinearidade**, que ocorre quando as vari√°veis independentes s√£o correlacionadas, pode ser detectada pela an√°lise de matriz de correla√ß√£o e pelo c√°lculo do *Variance Inflation Factor* (VIF). O VIF para a vari√°vel independente *j* √© dado por:

$$
VIF_j = \frac{1}{1 - R_j^2}
$$

Onde $R_j^2$ √© o coeficiente de determina√ß√£o da regress√£o de $x\_j$ sobre as outras vari√°veis independentes. VIFs maiores que 5 ou 10 indicam multicolinearidade, que podem gerar problemas na estimativa dos coeficientes. A solu√ß√£o pode ser remover vari√°veis correlacionadas, usar t√©cnicas de regulariza√ß√£o ou obter mais dados.

> üí° **Exemplo Num√©rico:**  Modelando a demanda com gasto em publicidade ($x_1$), visualiza√ß√µes de v√≠deo promocional ($x_2$), e visitas ao site ($x_3$), observamos que $x_2$ e $x_3$ s√£o correlacionadas. Usando $R^2$ de 0.85 para x2 e 0.78 para x3 nas suas regress√µes auxiliares, obtemos $VIF_{x_2} = \frac{1}{1 - 0.85} = 6.67$ e $VIF_{x_3} = \frac{1}{1 - 0.78} = 4.54$. O VIF de $x_2$ (6.67) indica um problema de multicolinearidade, enquanto o VIF de $x_3$ (4.54) √© menor mas ainda necessita monitoramento.

**Prova:**
Como previamente demonstrado, a f√≥rmula do VIF √©:

$$VIF_j = \frac{1}{1 - R_j^2}$$

onde $R_j^2$ √© o coeficiente de determina√ß√£o da regress√£o da vari√°vel independente $x_j$ em rela√ß√£o a todas as outras vari√°veis independentes no modelo. O VIF quantifica o aumento da vari√¢ncia dos coeficientes devido √† multicolinearidade. Valores altos indicam forte multicolinearidade, afetando a precis√£o dos coeficientes do modelo. ‚ñ†
I.  A multicolinearidade ocorre quando as vari√°veis independentes em um modelo de regress√£o s√£o altamente correlacionadas.
II.  O VIF √© usado para quantificar a gravidade da multicolinearidade para cada vari√°vel independente.
III.  Para uma vari√°vel independente $x_j$, o VIF √© calculado como $VIF_j = \frac{1}{1 - R_j^2}$, onde $R_j^2$ √© o coeficiente de determina√ß√£o da regress√£o de $x_j$ sobre as outras vari√°veis independentes.
IV.   Um valor de $R_j^2$ pr√≥ximo a 1 indica que a vari√°vel independente $x_j$ √© altamente correlacionada com as outras vari√°veis independentes.
V.   Se $R_j^2$ √© alto, ent√£o $1-R_j^2$ ser√° pequeno, e $VIF_j$ ser√° grande, indicando forte multicolinearidade.
VI. Um VIF alto indica que a vari√¢ncia do coeficiente de regress√£o para $x_j$ est√° inflacionada devido √† multicolinearidade. ‚ñ†

**Lema 3**
Para mitigar a multicolinearidade, uma t√©cnica comum √© a **regulariza√ß√£o**, que adiciona um termo de penalidade √† fun√ß√£o de custo da regress√£o. Duas t√©cnicas comuns s√£o a regress√£o *ridge*, que usa a norma L2 dos coeficientes de regress√£o, e a regress√£o *lasso*, que usa a norma L1. A regress√£o *ridge* minimiza:
$$ \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p}b_j^2 $$
E a regress√£o *lasso* minimiza:
$$ \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p}|b_j| $$
Onde $\lambda$ √© o par√¢metro de regulariza√ß√£o, que controla a intensidade da penalidade, e b s√£o os coeficientes de regress√£o.
A regress√£o *ridge* reduz os coeficientes de vari√°veis correlacionadas, enquanto *lasso* pode zerar alguns coeficientes, atuando tamb√©m como um m√©todo de sele√ß√£o de vari√°veis.

> üí° **Exemplo Num√©rico:** Vamos considerar um problema de regress√£o linear m√∫ltipla onde temos duas vari√°veis independentes ($x_1$ e $x_2$) altamente correlacionadas, e a vari√°vel dependente ($y$). Vamos comparar os resultados da regress√£o linear padr√£o (OLS), regress√£o Ridge e regress√£o Lasso:
>
> Suponha que, ap√≥s o ajuste de um modelo de regress√£o linear m√∫ltipla usando OLS, obtivemos os seguintes coeficientes: $b_1 = 5$ e $b_2 = 4$. A soma dos quadrados dos res√≠duos (SSR) foi 100.
>
> Agora, vamos aplicar a regress√£o Ridge com $\lambda = 0.5$. Os novos coeficientes podem ser, por exemplo, $b_1 = 3$ e $b_2 = 2.5$.  O novo SSR passa a 120. A fun√ß√£o de custo minimizada na regress√£o Ridge √©: $120 + 0.5 * (3^2 + 2.5^2) = 120 + 0.5 * (9 + 6.25) = 120 + 7.625 = 127.625$.
>
> Em seguida, aplicamos a regress√£o Lasso com $\lambda = 0.5$. Os coeficientes resultantes podem ser: $b_1 = 2$ e $b_2 = 0$. O novo SSR passa a 130.  A fun√ß√£o de custo minimizada na regress√£o Lasso √©: $130 + 0.5 * (2 + 0) = 130 + 1 = 131$.
>
> A tabela abaixo compara os resultados:
>
> | M√©todo   | Coeficiente $b_1$ | Coeficiente $b_2$ | SSR  | Fun√ß√£o de Custo |
> | -------- | --------------- | --------------- | ---- | -------------- |
> | OLS      | 5               | 4               | 100  | 100            |
> | Ridge  | 3               | 2.5             | 120  | 127.625        |
> | Lasso    | 2               | 0               | 130  | 131           |
>
> A regress√£o Ridge reduziu a magnitude dos coeficientes, enquanto a regress√£o Lasso reduziu $b_2$ a zero, eliminando $x_2$ do modelo e atuando como um m√©todo de sele√ß√£o de vari√°veis. Observe que a fun√ß√£o de custo (incluindo o termo de penalidade) √© o que √© efetivamente minimizado, n√£o apenas o SSR.

**Prova:**
A regress√£o ridge adiciona um termo de penaliza√ß√£o √† fun√ß√£o de custo, penalizando valores altos dos coeficientes de regress√£o, o que ajuda a mitigar a multicolinearidade. O termo de penaliza√ß√£o √© a norma L2 dos coeficientes ($ \lambda \sum_{j=1}^{p}b_j^2 $). De forma semelhante, a regress√£o lasso adiciona a norma L1 dos coeficientes ($ \lambda \sum_{j=1}^{p}|b_j|$), que tem como efeito secund√°rio a sele√ß√£o de vari√°veis por zerar coeficientes. ‚ñ†
I. A regulariza√ß√£o √© uma t√©cnica usada para mitigar a multicolinearidade adicionando um termo de penaliza√ß√£o √† fun√ß√£o de custo da regress√£o.
II. A regress√£o ridge adiciona um termo de penaliza√ß√£o baseado na norma L2 dos coeficientes, minimizando $ \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p}b_j^2 $, onde $\lambda$ controla a for√ßa da penaliza√ß√£o.
III. O termo de penaliza√ß√£o $ \lambda \sum_{j=1}^{p}b_j^2 $ reduz a magnitude dos coeficientes, especialmente para vari√°veis correlacionadas, mitigando o problema de multicolinearidade.
IV. A regress√£o lasso adiciona um termo de penaliza√ß√£o baseado na norma L1 dos coeficientes, minimizando $ \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p}|b_j| $, onde $\lambda$ controla a for√ßa da penaliza√ß√£o.
V. O termo de penaliza√ß√£o $ \lambda \sum_{j=1}^{p}|b_j| $ tem o efeito adicional de reduzir alguns coeficientes a zero, atuando como um m√©todo de sele√ß√£o de vari√°veis e mitigando tamb√©m a multicolinearidade. ‚ñ†

### Conclus√£o
Os modelos associativos s√£o essenciais para prever a demanda em ambientes complexos, onde a demanda √© influenciada por m√∫ltiplos fatores [^1]. A regress√£o linear m√∫ltipla e outras t√©cnicas permitem quantificar essas rela√ß√µes, criando modelos mais robustos. A combina√ß√£o de modelos de s√©ries temporais e modelos associativos resulta em previs√µes mais precisas, auxiliando na tomada de decis√µes e otimiza√ß√£o das opera√ß√µes.

### Refer√™ncias

[^1]: Heizer, J., Render, B., & Munson, C. (2020). *Operations management: Sustainability and supply chain management* (13th ed.). Pearson.
[^2]: Stulz, N., Pichler, E.-M., Kawohl, W., & Hepp, U. (2018). The gravitational force of mental health services: Distance decay effects in a rural Swiss service area. *BMC Health Services Research*, *18*(1), 81. doi:10.1186/s12913-018-2888-1.
<!-- END -->
