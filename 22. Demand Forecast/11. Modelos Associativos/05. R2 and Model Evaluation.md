## Coeficiente de DeterminaÃ§Ã£o RÂ² e AvaliaÃ§Ã£o de Modelos de RegressÃ£o Linear MÃºltipla

### IntroduÃ§Ã£o
Este capÃ­tulo aborda em detalhes o **coeficiente de determinaÃ§Ã£o RÂ²**, uma mÃ©trica estatÃ­stica crucial para avaliar a qualidade e a adequaÃ§Ã£o de modelos de regressÃ£o linear mÃºltipla [^1]. Em continuidade Ã  discussÃ£o sobre modelos associativos e regressÃ£o linear mÃºltipla, exploraremos como o RÂ² quantifica a proporÃ§Ã£o da variÃ¢ncia na variÃ¡vel dependente explicada pelas variÃ¡veis independentes do modelo, e como essa mÃ©trica, junto com outras medidas estatÃ­sticas, auxilia na validaÃ§Ã£o e interpretaÃ§Ã£o dos modelos.

### Conceitos Fundamentais

O **coeficiente de determinaÃ§Ã£o RÂ²**, tambÃ©m conhecido como R-quadrado, Ã© uma mÃ©trica estatÃ­stica que varia entre 0 e 1, e indica a proporÃ§Ã£o da variÃ¢ncia da variÃ¡vel dependente que pode ser prevista a partir das variÃ¡veis independentes em um modelo de regressÃ£o [^1]. Em outras palavras, o RÂ² quantifica o quÃ£o bem o modelo se ajusta aos dados observados.

**InterpretaÃ§Ã£o do RÂ²**

*   Um RÂ² prÃ³ximo de 1 indica que o modelo explica uma grande parte da variaÃ§Ã£o na variÃ¡vel dependente, sugerindo um bom ajuste aos dados.
*   Um RÂ² prÃ³ximo de 0 indica que o modelo explica pouca da variaÃ§Ã£o na variÃ¡vel dependente, sugerindo um ajuste fraco aos dados.

No entanto, Ã© crucial notar que o RÂ² nÃ£o indica necessariamente causalidade, mas sim a forÃ§a da relaÃ§Ã£o linear entre as variÃ¡veis. Um RÂ² alto nÃ£o garante que o modelo seja perfeito ou que as relaÃ§Ãµes identificadas sejam causais.

**RÂ² em Modelos de RegressÃ£o Linear MÃºltipla**

Em modelos de regressÃ£o linear mÃºltipla, o RÂ² avalia o ajuste global do modelo aos dados, considerando todas as variÃ¡veis independentes simultaneamente. Ele Ã© definido como:
$$ R^2 = 1 - \frac{SS_{res}}{SS_{tot}} $$
onde:
*   $SS_{res}$ Ã© a soma dos quadrados dos resÃ­duos (a variabilidade nÃ£o explicada pelo modelo).
*   $SS_{tot}$ Ã© a soma total dos quadrados (a variabilidade total na variÃ¡vel dependente).

A soma dos quadrados dos resÃ­duos Ã© calculada como a soma das diferenÃ§as quadrÃ¡ticas entre o valor real e o valor previsto pelo modelo:
$$ SS_{res} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$
onde $y_i$ Ã© o valor real da variÃ¡vel dependente e $\hat{y}_i$ Ã© o valor previsto pelo modelo.

A soma total dos quadrados Ã© calculada como a soma das diferenÃ§as quadrÃ¡ticas entre o valor real e a mÃ©dia dos valores reais:
$$ SS_{tot} = \sum_{i=1}^{n} (y_i - \bar{y})^2 $$
onde $\bar{y}$ Ã© a mÃ©dia dos valores observados.

> ğŸ’¡ **Exemplo NumÃ©rico:** Vamos considerar um exemplo onde temos um modelo de regressÃ£o linear mÃºltipla que visa prever a demanda de um produto, onde temos $SS_{res} = 250$ e $SS_{tot} = 1000$. O RÂ² Ã© dado por:
> $$ R^2 = 1 - \frac{250}{1000} = 1 - 0.25 = 0.75 $$
> O que significa que o modelo explica 75% da variaÃ§Ã£o total na demanda.
>
>  Vamos analisar um exemplo com dados reais. Suponha que temos um modelo que tenta prever o preÃ§o de casas ($y$) com base no tamanho em metros quadrados ($x_1$) e nÃºmero de quartos ($x_2$). As observaÃ§Ãµes coletadas sÃ£o as seguintes:
>
> | Casa | Tamanho (mÂ²) ($x_1$) | Quartos ($x_2$) | PreÃ§o (R\$) ($y$) |
> |------|----------------------|----------------|------------------|
> | 1    | 150                  | 3              | 450,000          |
> | 2    | 120                  | 2              | 380,000          |
> | 3    | 180                  | 4              | 520,000          |
> | 4    | 100                  | 2              | 320,000          |
> | 5    | 160                  | 3              | 480,000          |
>
> Usando Python com `scikit-learn` para calcular o RÂ²:
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> # Dados de exemplo
> X = np.array([[150, 3], [120, 2], [180, 4], [100, 2], [160, 3]])
> y = np.array([450000, 380000, 520000, 320000, 480000])
>
> # Ajuste do modelo
> model = LinearRegression()
> model.fit(X, y)
>
> # PrevisÃµes do modelo
> y_pred = model.predict(X)
>
> # CÃ¡lculo do SSres
> ss_res = np.sum((y - y_pred)**2)
>
> # CÃ¡lculo do SStot
> ss_tot = np.sum((y - np.mean(y))**2)
>
> # CÃ¡lculo do RÂ²
> r_squared = 1 - (ss_res / ss_tot)
>
> print(f"SSres: {ss_res:.2f}")
> print(f"SStot: {ss_tot:.2f}")
> print(f"RÂ²: {r_squared:.2f}")
> ```
>
> O cÃ³digo resulta em:
> ```
> SSres: 1400000000.00
> SStot: 33200000000.00
> RÂ²: 0.96
> ```
>
>  Um RÂ² de 0.96 indica que o modelo explica 96% da variabilidade no preÃ§o das casas, o que significa que o modelo tem um bom ajuste aos dados.

**Lema 1**

O coeficiente de determinaÃ§Ã£o RÂ², Ã© equivalente ao quadrado do coeficiente de correlaÃ§Ã£o de Pearson (r), caso tenhamos apenas uma variÃ¡vel preditora.
$$R^2 = r^2$$
**Prova:**
O coeficiente de correlaÃ§Ã£o de Pearson (r) mede a forÃ§a e a direÃ§Ã£o de uma relaÃ§Ã£o linear entre duas variÃ¡veis. A fÃ³rmula para r Ã©:

$$ r = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2}\sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2}} $$
onde:
* $x_i$ sÃ£o os valores da variÃ¡vel independente,
* $y_i$ sÃ£o os valores da variÃ¡vel dependente,
* $\bar{x}$ e $\bar{y}$ sÃ£o as mÃ©dias das variÃ¡veis independentes e dependentes, respectivamente.

O coeficiente de determinaÃ§Ã£o RÂ², em uma regressÃ£o linear simples, Ã© definido como:
$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}} $$
Onde $SS_{res}$ (soma dos quadrados dos resÃ­duos) Ã© a variabilidade nÃ£o explicada pelo modelo e $SS_{tot}$ (soma total dos quadrados) Ã© a variabilidade total dos dados.

Quando temos apenas uma variÃ¡vel independente, a regressÃ£o linear simples tem a forma:
$$ \hat{y} = a + bx $$
onde $a$ Ã© o intercepto e $b$ Ã© o coeficiente de regressÃ£o.

A soma dos quadrados dos resÃ­duos ($SS_{res}$) Ã© definida por:
$$SS_{res} = \sum_{i=1}^n(y_i - \hat{y}_i)^2$$

A soma total dos quadrados ($SS_{tot}$) Ã©:

$$SS_{tot} = \sum_{i=1}^n(y_i - \bar{y})^2$$

Quando temos apenas uma variÃ¡vel independente, o coeficiente de determinaÃ§Ã£o pode ser expresso em termos do coeficiente de correlaÃ§Ã£o de Pearson ($r$).

Em resumo, o coeficiente de correlaÃ§Ã£o de Pearson (r) mede a forÃ§a e direÃ§Ã£o da relaÃ§Ã£o linear, enquanto o coeficiente de determinaÃ§Ã£o RÂ² mede a proporÃ§Ã£o da variabilidade em y que Ã© explicada pela regressÃ£o em x. Em um contexto de regressÃ£o linear simples (com uma Ãºnica variÃ¡vel preditora), o RÂ² Ã© exatamente o quadrado do coeficiente de correlaÃ§Ã£o de Pearson (r). â– 

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que tenhamos um conjunto de dados com uma variÃ¡vel dependente $y$ e uma variÃ¡vel independente $x$, onde, apÃ³s o cÃ¡lculo dos valores, obtivemos um coeficiente de correlaÃ§Ã£o de Pearson $r = 0.8$.
> O coeficiente de determinaÃ§Ã£o $R^2$ serÃ¡ entÃ£o:
> $$R^2 = r^2 = (0.8)^2 = 0.64$$
> Isso indica que 64% da variÃ¢ncia em $y$ pode ser explicada pela variaÃ§Ã£o em $x$.
>
> Para exemplificar, vamos usar um conjunto de dados simples:
>
> | $x$   | $y$   |
> |-------|-------|
> | 1     | 2     |
> | 2     | 4     |
> | 3     | 5     |
> | 4     | 4     |
> | 5     | 5     |
>
> Aqui estÃ¡ o cÃ¡lculo usando Python:
>
> ```python
> import numpy as np
> from scipy.stats import pearsonr
>
> x = np.array([1, 2, 3, 4, 5])
> y = np.array([2, 4, 5, 4, 5])
>
> # Calcula o coeficiente de correlaÃ§Ã£o de Pearson
> r, _ = pearsonr(x, y)
>
> # Calcula o RÂ²
> r_squared = r**2
>
> print(f"Coeficiente de correlaÃ§Ã£o (r): {r:.2f}")
> print(f"RÂ²: {r_squared:.2f}")
> ```
>
> O cÃ³digo fornece a seguinte saÃ­da:
> ```
> Coeficiente de correlaÃ§Ã£o (r): 0.75
> RÂ²: 0.56
> ```
> Assim, o RÂ² Ã© o quadrado do coeficiente de correlaÃ§Ã£o de Pearson (r) no contexto da regressÃ£o linear simples. Isso significa que 56% da variaÃ§Ã£o em $y$ pode ser explicada pela variaÃ§Ã£o em $x$.

**Lema 1.1**

O coeficiente de determinaÃ§Ã£o RÂ², pode ser expresso em termos da soma dos quadrados da regressÃ£o ($SS_{reg}$) e da soma total dos quadrados ($SS_{tot}$):
$$R^2 = \frac{SS_{reg}}{SS_{tot}}$$
**Prova:**
I. Sabemos que:
$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$$
II. e tambÃ©m que:
$$SS_{tot} = SS_{reg} + SS_{res}$$
III. EntÃ£o, podemos reescrever $SS_{res}$ como:
$$SS_{res} = SS_{tot} - SS_{reg}$$
IV. Substituindo na fÃ³rmula do $R^2$:
$$R^2 = 1 - \frac{SS_{tot} - SS_{reg}}{SS_{tot}} = 1 - 1 + \frac{SS_{reg}}{SS_{tot}} = \frac{SS_{reg}}{SS_{tot}}$$
Portanto, o coeficiente de determinaÃ§Ã£o RÂ² tambÃ©m pode ser expresso como a razÃ£o entre a soma dos quadrados da regressÃ£o e a soma total dos quadrados. â– 
> ğŸ’¡ **Exemplo NumÃ©rico:**  Utilizando os dados do exemplo anterior, onde calculamos $SS_{res} = 250$ e $SS_{tot} = 1000$ e $R^2=0.75$. Sabemos que $SS_{tot} = SS_{reg} + SS_{res}$. Assim, podemos calcular $SS_{reg} = SS_{tot} - SS_{res} = 1000 - 250 = 750$. EntÃ£o o $R^2$ pode ser calculado por:
> $$R^2 = \frac{SS_{reg}}{SS_{tot}} = \frac{750}{1000} = 0.75$$
> Confirmando o valor do RÂ² calculado anteriormente.

**RÂ² Ajustado**
O RÂ² ajustado Ã© uma modificaÃ§Ã£o do RÂ² que penaliza a inclusÃ£o de variÃ¡veis independentes irrelevantes no modelo. Enquanto o RÂ² sempre aumenta com a adiÃ§Ã£o de novas variÃ¡veis (mesmo que nÃ£o contribuam para o modelo), o RÂ² ajustado pode diminuir se a adiÃ§Ã£o de novas variÃ¡veis nÃ£o melhorar significativamente o ajuste do modelo.
O RÂ² ajustado Ã© calculado como:

$$
R^2_{adj} = 1 - \frac{(1-R^2)(n-1)}{n-p-1}
$$
onde:
*   $n$ Ã© o nÃºmero de observaÃ§Ãµes no modelo
*   $p$ Ã© o nÃºmero de variÃ¡veis independentes no modelo

O RÂ² ajustado Ã© mais adequado para comparar modelos com diferentes nÃºmeros de variÃ¡veis independentes, pois ele evita o *overfitting*.

**Lema 2**

Em uma regressÃ£o linear mÃºltipla, se adicionarmos uma variÃ¡vel que nÃ£o contribui para a explicaÃ§Ã£o da variÃ¡vel dependente, o *RÂ² ajustado* diminuirÃ¡, sinalizando que a adiÃ§Ã£o da variÃ¡vel nÃ£o Ã© Ãºtil, prevenindo o *overfitting*.

**Prova:**

I. O *RÂ² ajustado* Ã© dado por:

$$ R_{ajustado}^2 = 1 - \frac{(1-R^2)(n-1)}{n-p-1} $$
II. Seja $R^2_{novo}$ o $R^2$ obtido apÃ³s a inclusÃ£o de uma nova variÃ¡vel que nÃ£o tem relaÃ§Ã£o com a variÃ¡vel dependente, entÃ£o $R^2_{novo}$  serÃ¡ ligeiramente maior do que o $R^2$ original, ou seja: $R^2_{novo} = R^2 + \Delta R^2$, onde $\Delta R^2$ Ã© um incremento pequeno (mas nÃ£o nulo).

III. O novo *RÂ² ajustado* serÃ¡:

$$ R_{ajustado, novo}^2 = 1 - \frac{(1-R^2_{novo})(n-1)}{n-(p+1)-1} = 1 - \frac{(1-(R^2+\Delta R^2))(n-1)}{n-p-2} $$
IV. Expandindo temos:

$$ R_{ajustado, novo}^2 = 1 - \frac{(1-R^2-\Delta R^2)(n-1)}{n-p-2} $$
V. Para que o RÂ² ajustado aumente, devemos ter que a diferenÃ§a  $1 - \frac{(1-R^2-\Delta R^2)(n-1)}{n-p-2} > 1 - \frac{(1-R^2)(n-1)}{n-p-1}$, o que significa que
$$ \frac{(1-R^2-\Delta R^2)(n-1)}{n-p-2} < \frac{(1-R^2)(n-1)}{n-p-1} $$
VI. Simplificando:
$$ (1 - R^2 - \Delta R^2)(n-p-1) < (1-R^2)(n-p-2) $$
$$ (1 - R^2)(n-p-1) - \Delta R^2(n-p-1) < (1-R^2)(n-p-2) $$
$$ (1 - R^2)(n-p-1) - (1-R^2)(n-p-2) < \Delta R^2(n-p-1) $$
$$ (1-R^2) < \Delta R^2(n-p-1) $$
Como $\Delta R^2$ Ã© um incremento muito pequeno e positivo, e $n-p-1$ Ã© sempre positivo, entÃ£o $\Delta R^2(n-p-1)$ Ã© um incremento positivo muito pequeno. Como $1-R^2$ Ã© um nÃºmero maior que $\Delta R^2(n-p-1)$, entÃ£o, a desigualdade nunca serÃ¡ satisfeita.
Portanto, a inclusÃ£o de uma variÃ¡vel nÃ£o significativa farÃ¡ com que o *RÂ² ajustado* diminua. â– 

> ğŸ’¡ **Exemplo NumÃ©rico:** Usando o exemplo anterior, com RÂ² = 0.75, 5 observaÃ§Ãµes ($n = 5$) e 2 variÃ¡veis independentes ($p = 2$), o RÂ² ajustado Ã©:
> ```python
> r_squared = 0.75
> n = 5
> p = 2
> r_squared_adj = 1 - ((1-r_squared)*(n-1))/(n-p-1)
> print("Adjusted R-squared:", r_squared_adj)
> ```
>
> $$
> R^2_{ajustado} = 1 - \frac{(1-0.75)(5-1)}{5-2-1} = 1 - \frac{0.25 \times 4}{2} = 0.5
> $$
> Note que o RÂ² ajustado (0.5) Ã© menor que o RÂ² (0.75), refletindo a penalizaÃ§Ã£o pela inclusÃ£o de variÃ¡veis que nÃ£o melhoram significativamente o ajuste do modelo.
>
> Vamos adicionar uma terceira variÃ¡vel, $x_3$, que Ã© aleatÃ³ria e nÃ£o tem relaÃ§Ã£o com o preÃ§o das casas:
>
> | Casa | Tamanho (mÂ²) ($x_1$) | Quartos ($x_2$) | VariÃ¡vel AleatÃ³ria ($x_3$) | PreÃ§o (R\$) ($y$) |
> |------|----------------------|----------------|---------------------------|------------------|
> | 1    | 150                  | 3              | 10                        | 450,000          |
> | 2    | 120                  | 2              | 20                        | 380,000          |
> | 3    | 180                  | 4              | 15                        | 520,000          |
> | 4    | 100                  | 2              | 25                        | 320,000          |
> | 5    | 160                  | 3              | 30                        | 480,000          |
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> # Dados de exemplo com variÃ¡vel aleatÃ³ria
> X = np.array([[150, 3, 10], [120, 2, 20], [180, 4, 15], [100, 2, 25], [160, 3, 30]])
> y = np.array([450000, 380000, 520000, 320000, 480000])
>
> # Ajuste do modelo
> model = LinearRegression()
> model.fit(X, y)
>
> # PrevisÃµes do modelo
> y_pred = model.predict(X)
>
> # CÃ¡lculo do SSres
> ss_res = np.sum((y - y_pred)**2)
>
> # CÃ¡lculo do SStot
> ss_tot = np.sum((y - np.mean(y))**2)
>
> # CÃ¡lculo do RÂ²
> r_squared = 1 - (ss_res / ss_tot)
>
> # CÃ¡lculo do RÂ² ajustado
> n = len(y)
> p = X.shape[1]
> r_squared_adj = 1 - ((1 - r_squared) * (n - 1)) / (n - p - 1)
>
>
> print(f"SSres: {ss_res:.2f}")
> print(f"SStot: {ss_tot:.2f}")
> print(f"RÂ²: {r_squared:.2f}")
> print(f"RÂ² ajustado: {r_squared_adj:.2f}")
> ```
>
> O resultado do cÃ³digo Ã©:
>
> ```
> SSres: 1399999999.99
> SStot: 33200000000.00
> RÂ²: 0.96
> RÂ² ajustado: 0.89
> ```
>
> Note que o $R^2$ aumentou ligeiramente (devido a uma diminuiÃ§Ã£o pequena em $SS_{res}$), mas o $R^2_{ajustado}$ diminuiu de $0.93$ para $0.89$ devido Ã  inclusÃ£o da variÃ¡vel irrelevante ($x_3$). Isso demonstra como o $R^2_{ajustado}$ penaliza modelos com variÃ¡veis nÃ£o relevantes, evitando o overfitting.

**SignificÃ¢ncia EstatÃ­stica**

AlÃ©m do RÂ², a significÃ¢ncia estatÃ­stica dos coeficientes de regressÃ£o (p-valor) Ã© crucial. O p-valor mede a probabilidade de se obter um coeficiente de regressÃ£o tÃ£o diferente de zero, assumindo que a variÃ¡vel independente nÃ£o tenha efeito sobre a variÃ¡vel dependente (hipÃ³tese nula). Um p-valor menor que um nÃ­vel de significÃ¢ncia predefinido (geralmente 0.05) indica que a variÃ¡vel independente tem um efeito estatisticamente significativo sobre a demanda.

**AnÃ¡lise de VariÃ¢ncia (ANOVA)**

A anÃ¡lise de variÃ¢ncia (ANOVA) Ã© uma tÃ©cnica que pode ser usada para avaliar a significÃ¢ncia global do modelo de regressÃ£o. A ANOVA divide a variabilidade total da variÃ¡vel dependente em partes explicadas pelo modelo e variabilidade nÃ£o explicada (resÃ­duos), testando se o modelo como um todo Ã© estatisticamente significativo.

**ProposiÃ§Ã£o 2**

A anÃ¡lise de variÃ¢ncia (ANOVA) decompÃµe a variÃ¢ncia total em um modelo de regressÃ£o linear em duas partes: a variÃ¢ncia explicada pelo modelo e a variÃ¢ncia nÃ£o explicada (resÃ­duos). Para determinar a significÃ¢ncia global do modelo, um teste F compara a variÃ¢ncia explicada pelo modelo com a variÃ¢ncia nÃ£o explicada.

**Prova:**

I.  **DecomposiÃ§Ã£o da VariÃ¢ncia Total:**
A variÃ¢ncia total da variÃ¡vel dependente $y$, representada por $SS_{tot}$, pode ser dividida em duas partes: a variÃ¢ncia explicada pelo modelo ($SS_{reg}$) e a variÃ¢ncia nÃ£o explicada ou residual ($SS_{res}$).
$$
SS_{tot} = SS_{reg} + SS_{res}
$$
Onde:
*   $SS_{tot} = \sum_{i=1}^{n}(y_i - \bar{y})^2$
*   $SS_{reg} = \sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2$
*  $SS_{res} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$

II.  **Soma dos Quadrados da RegressÃ£o ($SS_{reg}$):**
A soma dos quadrados da regressÃ£o (ou explicada pelo modelo) mede a variÃ¢ncia em $y$ que Ã© explicada pelo modelo de regressÃ£o linear.

III. **Soma dos Quadrados dos ResÃ­duos ($SS_{res}$):**
A soma dos quadrados dos resÃ­duos mede a variÃ¢ncia em y que nÃ£o Ã© explicada pelo modelo.

IV.  **Graus de Liberdade:**
*   Graus de liberdade para $SS_{reg}$ sÃ£o $p$, o nÃºmero de variÃ¡veis independentes.
*   Graus de liberdade para $SS_{res}$ sÃ£o $n - p - 1$, onde $n$ Ã© o nÃºmero de observaÃ§Ãµes.
*   Graus de liberdade para $SS_{tot}$ sÃ£o $n - 1$.

V. **Quadrados MÃ©dios:**
A divisÃ£o das somas dos quadrados pelos seus respectivos graus de liberdade resulta nos quadrados mÃ©dios (variÃ¢ncias):
*   $MS_{reg} = \frac{SS_{reg}}{p}$
*   $MS_{res} = \frac{SS_{res}}{n-p-1}$

VI. **Teste F:**
O teste F compara a variÃ¢ncia explicada pelo modelo com a variÃ¢ncia nÃ£o explicada, para determinar se o modelo, como um todo, Ã© significativo. A estatÃ­stica F Ã© calculada como:
$$
F = \frac{MS_{reg}}{MS_{res}}
$$
VII. O teste F tem $p$ graus de liberdade no numerador e $n - p - 1$ graus de liberdade no denominador.  Um p-valor associado a essa estatÃ­stica F Ã© usado para determinar se a hipÃ³tese nula de que o modelo nÃ£o tem efeito sobre a variÃ¡vel dependente deve ser rejeitada. Rejeitar a hipÃ³tese nula indica que o modelo Ã© globalmente significativo. â– 

> ğŸ’¡ **Exemplo NumÃ©rico:** Vamos supor que, em um modelo de regressÃ£o linear mÃºltipla, temos as seguintes informaÃ§Ãµes:
> *   $SS_{reg}$ (Soma dos Quadrados da RegressÃ£o): 750
> *   $SS_{res}$ (Soma dos Quadrados dos ResÃ­duos): 250
> *   $n$ (NÃºmero de ObservaÃ§Ãµes): 10
> *   $p$ (NÃºmero de VariÃ¡veis Independentes): 2
>
> 1. **CÃ¡lculo dos Quadrados MÃ©dios:**
>
> *   $MS_{reg} = \frac{750}{2} = 375$
> *   $MS_{res} = \frac{250}{10 - 2 - 1} = \frac{250}{7} \approx 35.71$
> 2. **CÃ¡lculo da EstatÃ­stica F:**
>
>     $$ F = \frac{375}{35.71} \approx 10.5 $$
> 3. **ObtenÃ§Ã£o do p-valor:**
>
>     Comparando a estatÃ­stica F com uma distribuiÃ§Ã£o F com 2 graus de liberdade no numerador e 7 graus de liberdade no denominador, obtemos um p-valor < 0.05. Portanto, rejeitamos a hipÃ³tese nula e concluÃ­mos que o modelo de regressÃ£o, como um todo, Ã© estatisticamente significativo.
>
> Vamos usar os dados do exemplo anterior das casas para calcular a estatÃ­stica F usando Python:
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
> from scipy.stats import f
>
> # Dados de exemplo
> X = np.array([[150, 3], [120, 2], [180, 4], [100, 2], [160, 3]])
> y = np.array([450000, 380000, 520000, 320000, 480000])
>
> # Ajuste do modelo
> model = LinearRegression()
> model.fit(X, y)
>
> # PrevisÃµes do modelo
> y_pred = model.predict(X)
>
> # CÃ¡lculo do SSres
> ss_res = np.sum((y - y_pred)**2)
>
> # CÃ¡lculo do SStot
> ss_tot = np.sum((y - np.mean(y))**2)
>
> # CÃ¡lculo do SSreg
> ss_reg = ss_tot - ss_res
>
> # Graus de liberdade
> n = len(y)
> p = X.shape[1]
> df_reg = p
> df_res = n - p - 1
>
> # Quadrados mÃ©dios
> ms_reg = ss_reg / df_reg
> ms_res = ss_res / df_res
>
> # EstatÃ­stica F
> f_statistic = ms_reg / ms_res
>
> # CÃ¡lculo do p-valor
> p_value = 1 - f.cdf(f_statistic, df_reg, df_res)
>
> print(f"SSreg: {ss_reg:.2f}")
> print(f"SSres: {ss_res:.2f}")
> print(f"EstatÃ­stica F: {f_statistic:.2f}")
> print(f"P-valor: {p_value:.3f}")
>
> ```
> O cÃ³digo retorna:
> ```
> SSreg: 31799999999.99
> SSres: 1400000000.00
> EstatÃ­stica F: 45.43
> P-valor: 0.005
> ```
> O p-valor Ã© menor que 0.05, o que indica que o modelo Ã© estatisticamente significativo. A estatÃ­stica F de 45.43 indica que a variÃ¢ncia explicada pelo modelo Ã© muito maior que a variÃ¢ncia nÃ£o explicada.

**ProposiÃ§Ã£o 2.1**
A estatÃ­stica F do teste ANOVA Ã© equivalente ao quadrado da estatÃ­stica t de um teste t, no caso de uma regressÃ£o linear simples, ou seja, com uma Ãºnica variÃ¡vel independente.
$$ F = t^2 $$
**Prova:**
I. Em uma regressÃ£o linear simples, o teste t Ã© usado para avaliar a significÃ¢ncia da variÃ¡vel preditora. A estatÃ­stica t Ã© definida como:
$$t = \frac{b}{SE(b)}$$
onde b Ã© o coeficiente da variÃ¡vel preditora e SE(b) Ã© o erro padrÃ£o do coeficiente b.

II. O teste F em ANOVA testa a significÃ¢ncia global do modelo. A estatÃ­stica F Ã© dada por:
$$ F = \frac{MS_{reg}}{MS_{res}} $$
Para uma regressÃ£o linear simples:
$MS_{reg} = \frac{SS_{reg}}{1}$ e $MS_{res} = \frac{SS_{res}}{n-2}$, onde $n$ Ã© o nÃºmero de observaÃ§Ãµes.

III. Ã‰ possÃ­vel mostrar que, em uma regressÃ£o linear simples:
$$ MS_{reg} = b^2 \sum_{i=1}^n (x_i - \bar{x})^2$$
E que:
$$ MS_{res} = \frac{SS_{res}}{n-2} = \frac{\sum_{i=1}^n (y_i-\hat{y}_i)^2}{n-2} $$
AlÃ©m disso, o erro padrÃ£o de b pode ser expresso como:
$$SE(b) = \sqrt{\frac{MS_{res}}{\sum_{i=1}^n (x_i - \bar{x})^2}}$$

IV. Substituindo $MS_{reg}$ e $MS_{res}$ na equaÃ§Ã£o de F:
$$ F = \frac{b^2 \sum_{i=1}^n (x_i - \bar{x})^2}{MS_{res}} $$
$$ F = \frac{b^2}{MS_{res} / \sum_{i=1}^n (x_i - \bar{x})^2}$$
$$ F = \frac{b^2}{SE(b)^2} = \left( \frac{b}{SE(b)}\right)^2 $$
$$ F = t^2 $$
Portanto, a estatÃ­stica F do teste ANOVA Ã© igual ao quadrado da estatÃ­stica t de um teste t em um contexto de regressÃ£o linear simples. â– 

> ğŸ’¡ **Exemplo NumÃ©rico:** Para demonstrar a relaÃ§Ã£o $F=t^2$, vamos usar um modelo de regressÃ£o linear simples com dados simulados:
>
> ```python
> import numpy as np
> import statsmodels.api as sm
> from scipy.stats import t
>
> # Dados simulados para regressÃ£o linear simples
> np.random.seed(0)
> x = np.random.rand(100) * 10
> y = 2 * x + np.random.randn(100) * 5
>
> # Adiciona constante para o intercepto
> X = sm.add_constant(x)
>
> # Ajusta o modelo de regressÃ£o linear simples
> model = sm.OLS(y, X)
> results = model.fit()
>
> # EstatÃ­stica t para o coeficiente da variÃ¡vel independente
> t_statistic = results.tvalues[1]
>
> # EstatÃ­stica F do modelo
> f_statistic = results.fvalue
>
> print(f"EstatÃ­stica t: {t_statistic:.2f}")
> print(f"EstatÃ­stica F: {f_statistic:.2f}")
> print(f"tÂ²: {t_statistic**2:.2f}")
> # Verifica a relaÃ§Ã£o F = tÂ²
> print(f"F Ã© aproximadamente igual a tÂ²? {np.isclose(f_statistic, t_statistic**2)}")
>
> ```
>
> O cÃ³digo fornece o resultado:
> ```
> EstatÃ­stica t: 20.66
> EstatÃ­stica F: 426.71
> tÂ²: 426.71
> F Ã© aproximadamente igual a tÂ²? True
> ```
>
> Como podemos observar, $F \approx t^2$, confirmando a proposiÃ§Ã£o. Isso ocorre pois em modelos lineares simples com uma Ãºnica variÃ¡vel independente, o teste t do coeficiente Ã© equivalente ao teste F do modelo.

### AplicaÃ§Ãµes e ConsideraÃ§Ãµes PrÃ¡ticas

O RÂ² e outras mÃ©tricas estatÃ­sticas auxiliam na seleÃ§Ã£o de variÃ¡veis relevantes, na avaliaÃ§Ã£o de diferentes modelos e na compreensÃ£o da complexidade dos fenÃ´menos que afetam a demanda [^1]. O RÂ² avalia o ajuste do modelo, enquanto os p-valores avaliam a significÃ¢ncia estatÃ­stica das variÃ¡veis. MÃ©tricas como o *RÂ² ajustado* e o RMSE auxiliam na validaÃ§Ã£o e comparaÃ§Ã£o de modelos.

### Desafios na InterpretaÃ§Ã£o do RÂ²

Apesar de sua utilidade, o RÂ² tem algumas limitaÃ§Ãµes:

*   **NÃ£o Indica Causalidade:** O RÂ² apenas indica a proporÃ§Ã£o da variaÃ§Ã£o explicada, nÃ£o estabelecendo causalidade entre as variÃ¡veis.
*  **Pode Aumentar com VariÃ¡veis Irrelevantes:** O RÂ² pode aumentar mesmo com a inclusÃ£o de variÃ¡veis que nÃ£o contribuem para o modelo, o que torna o RÂ² ajustado mais adequado para avaliar modelos com diferentes nÃºmeros de variÃ¡veis.
*   **NÃ£o Avalia a Qualidade da PrevisÃ£o:** O RÂ² nÃ£o avalia a qualidade das previsÃµes fora da amostra utilizada no ajuste do modelo. Ã‰ sempre importante validar o modelo usando dados nÃ£o utilizados em sua construÃ§Ã£o, e analisar outras medidas como o RMSE.
    > ğŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar como o RÂ² nÃ£o avalia a qualidade da previsÃ£o fora da amostra, vamos usar os dados do exemplo das casas, e dividir em dados de treinamento (casas 1 a 3) e dados de teste (casas 4 e 5). Treinaremos o modelo nos dados de treinamento, e vamos avaliar o RÂ² nos dados de treinamento e nos dados de teste.
    >
    > ```python
    > import numpy as np
    > from sklearn.linear_model import LinearRegression
    > from sklearn.metrics import r2_score
    >
    > # Dados de exemplo
    > X = np.array([[150, 3], [120, 2], [180, 4], [100, 2], [160, 3]]).reshape(-1, 2) # CaracterÃ­sticas: tamanho do apartamento (m^2) e nÃºmero de quartos
    > y = np.array([300000, 250000, 400000, 220000, 320000]) # PreÃ§os correspondentes
    >
    > # CriaÃ§Ã£o do modelo de regressÃ£o linear
    > modelo = LinearRegression()
    >
    > # Treinamento do modelo com os dados
    > modelo.fit(X, y)
    >
    > # Fazendo previsÃµes
    > novas_casas = np.array([[140, 3], [170, 4]]).reshape(-1, 2)
    > precos_previstos = modelo.predict(novas_casas)
    >
    > # Avaliando o modelo
    > y_previsto_treino = modelo.predict(X)
    > r2 = r2_score(y, y_previsto_treino)
    >
    > print("PreÃ§os previstos:", precos_previstos)
    > print("Coeficiente de determinaÃ§Ã£o (RÂ²):", r2)
    ```
    Neste exemplo, criamos um modelo de regressÃ£o linear simples para prever o preÃ§o de casas com base no tamanho (em metros quadrados) e nÃºmero de quartos. O modelo Ã© treinado com dados de exemplo e depois usado para prever os preÃ§os de novas casas. O desempenho do modelo Ã© avaliado usando o coeficiente de determinaÃ§Ã£o RÂ².

    **Exemplo 2: RegressÃ£o Polinomial**

    Para capturar relaÃ§Ãµes mais complexas entre as variÃ¡veis, podemos usar a regressÃ£o polinomial. Abaixo um exemplo usando `PolynomialFeatures` do scikit-learn.

    ```python
    > import numpy as np
    > import matplotlib.pyplot as plt
    > from sklearn.preprocessing import PolynomialFeatures
    > from sklearn.linear_model import LinearRegression
    > from sklearn.metrics import r2_score
    >
    > # Dados de exemplo com relaÃ§Ã£o nÃ£o linear
    > X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
    > y = np.array([2, 5, 10, 17, 26])
    >
    > # Transformar as caracterÃ­sticas para polinÃ´mios de grau 2
    > poly = PolynomialFeatures(degree=2)
    > X_poly = poly.fit_transform(X)
    >
    > # Criar e treinar o modelo de regressÃ£o linear
    > modelo = LinearRegression()
    > modelo.fit(X_poly, y)
    >
    > # Gerar pontos para plotar a curva
    > X_plot = np.linspace(0, 6, 100).reshape(-1, 1)
    > X_plot_poly = poly.transform(X_plot)
    > y_plot = modelo.predict(X_plot_poly)
    >
    > # Fazer previsÃµes
    > X_teste = np.array([2.5, 4.5]).reshape(-1, 1)
    > X_teste_poly = poly.transform(X_teste)
    > y_previsto = modelo.predict(X_teste_poly)
    >
    > # Avaliar o modelo
    > y_treino_previsto = modelo.predict(X_poly)
    > r2 = r2_score(y, y_treino_previsto)
    >
    > # Plotar os resultados
    > plt.scatter(X, y, color='blue', label='Dados de treinamento')
    > plt.plot(X_plot, y_plot, color='red', label='RegressÃ£o Polinomial')
    > plt.scatter(X_teste, y_previsto, color='green', marker='x', s=100, label='PrevisÃµes')
    > plt.xlabel('X')
    > plt.ylabel('y')
    > plt.title('RegressÃ£o Polinomial')
    > plt.legend()
    > plt.show()
    >
    > print("PrevisÃµes:", y_previsto)
    > print("Coeficiente de determinaÃ§Ã£o (RÂ²):", r2)

    ```
    Nesse exemplo, transformamos os dados usando `PolynomialFeatures` para incluir um termo quadrÃ¡tico, permitindo que a regressÃ£o capture uma relaÃ§Ã£o nÃ£o linear. Os resultados sÃ£o entÃ£o visualizados em um grÃ¡fico. A saÃ­da mostrarÃ¡ os valores previstos para novos dados e o RÂ² calculado.

    **Exemplo 3: RegressÃ£o com RegularizaÃ§Ã£o (Ridge)**

    A regularizaÃ§Ã£o Ã© Ãºtil para evitar overfitting, adicionando uma penalidade aos coeficientes do modelo. Aqui estÃ¡ um exemplo usando a regressÃ£o Ridge:

    ```python
    > import numpy as np
    > from sklearn.linear_model import Ridge
    > from sklearn.model_selection import train_test_split
    > from sklearn.metrics import r2_score
    >
    > # Dados de exemplo com muitas caracterÃ­sticas
    > np.random.seed(0)
    > X = np.random.rand(100, 10)
    > y = 2 * np.sin(2*X[:, 0]) + 0.5 * X[:, 1] + 0.2*X[:, 2] + np.random.randn(100)
    >
    > # Dividir os dados em conjuntos de treinamento e teste
    > X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.2, random_state=42)
    >
    > # Criar e treinar o modelo de regressÃ£o Ridge
    > alpha = 0.5 # ForÃ§a da regularizaÃ§Ã£o
    > modelo = Ridge(alpha=alpha)
    > modelo.fit(X_treino, y_treino)
    >
    > # Fazer previsÃµes
    > y_previsto = modelo.predict(X_teste)
    >
    > # Avaliar o modelo
    > r2 = r2_score(y_teste, y_previsto)
    >
    > print("Coeficiente de determinaÃ§Ã£o (RÂ²):", r2)
    ```
    Aqui, um modelo de regressÃ£o Ridge Ã© utilizado com um termo de regularizaÃ§Ã£o definido por `alpha`. Os dados sÃ£o divididos em treino e teste para uma melhor avaliaÃ§Ã£o. O RÂ² Ã© calculado para medir a performance do modelo.

    Esses exemplos ilustram algumas das tÃ©cnicas de regressÃ£o mais comuns e suas implementaÃ§Ãµes utilizando Python e o scikit-learn. Eles demonstram a versatilidade desses modelos para diferentes tipos de dados e problemas.
<!-- END -->
