## Coeficiente de Determina√ß√£o R¬≤ e Avalia√ß√£o de Modelos de Regress√£o Linear M√∫ltipla

### Introdu√ß√£o
Este cap√≠tulo aborda em detalhes o **coeficiente de determina√ß√£o R¬≤**, uma m√©trica estat√≠stica crucial para avaliar a qualidade e a adequa√ß√£o de modelos de regress√£o linear m√∫ltipla [^1]. Em continuidade √† discuss√£o sobre modelos associativos e regress√£o linear m√∫ltipla, exploraremos como o R¬≤ quantifica a propor√ß√£o da vari√¢ncia na vari√°vel dependente explicada pelas vari√°veis independentes do modelo, e como essa m√©trica, junto com outras medidas estat√≠sticas, auxilia na valida√ß√£o e interpreta√ß√£o dos modelos.

### Conceitos Fundamentais

O **coeficiente de determina√ß√£o R¬≤**, tamb√©m conhecido como R-quadrado, √© uma m√©trica estat√≠stica que varia entre 0 e 1, e indica a propor√ß√£o da vari√¢ncia da vari√°vel dependente que pode ser prevista a partir das vari√°veis independentes em um modelo de regress√£o [^1]. Em outras palavras, o R¬≤ quantifica o qu√£o bem o modelo se ajusta aos dados observados.

**Interpreta√ß√£o do R¬≤**

*   Um R¬≤ pr√≥ximo de 1 indica que o modelo explica uma grande parte da varia√ß√£o na vari√°vel dependente, sugerindo um bom ajuste aos dados.
*   Um R¬≤ pr√≥ximo de 0 indica que o modelo explica pouca da varia√ß√£o na vari√°vel dependente, sugerindo um ajuste fraco aos dados.

No entanto, √© crucial notar que o R¬≤ n√£o indica necessariamente causalidade, mas sim a for√ßa da rela√ß√£o linear entre as vari√°veis. Um R¬≤ alto n√£o garante que o modelo seja perfeito ou que as rela√ß√µes identificadas sejam causais.

**R¬≤ em Modelos de Regress√£o Linear M√∫ltipla**

Em modelos de regress√£o linear m√∫ltipla, o R¬≤ avalia o ajuste global do modelo aos dados, considerando todas as vari√°veis independentes simultaneamente. Ele √© definido como:
$$ R^2 = 1 - \frac{SS_{res}}{SS_{tot}} $$
onde:
*   $SS_{res}$ √© a soma dos quadrados dos res√≠duos (a variabilidade n√£o explicada pelo modelo).
*   $SS_{tot}$ √© a soma total dos quadrados (a variabilidade total na vari√°vel dependente).

A soma dos quadrados dos res√≠duos √© calculada como a soma das diferen√ßas quadr√°ticas entre o valor real e o valor previsto pelo modelo:
$$ SS_{res} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$
onde $y_i$ √© o valor real da vari√°vel dependente e $\hat{y}_i$ √© o valor previsto pelo modelo.

A soma total dos quadrados √© calculada como a soma das diferen√ßas quadr√°ticas entre o valor real e a m√©dia dos valores reais:
$$ SS_{tot} = \sum_{i=1}^{n} (y_i - \bar{y})^2 $$
onde $\bar{y}$ √© a m√©dia dos valores observados.

> üí° **Exemplo Num√©rico:** Vamos considerar um exemplo onde temos um modelo de regress√£o linear m√∫ltipla que visa prever a demanda de um produto, onde temos $SS_{res} = 250$ e $SS_{tot} = 1000$. O R¬≤ √© dado por:
> $$ R^2 = 1 - \frac{250}{1000} = 1 - 0.25 = 0.75 $$
> O que significa que o modelo explica 75% da varia√ß√£o total na demanda.
>
>  Vamos analisar um exemplo com dados reais. Suponha que temos um modelo que tenta prever o pre√ßo de casas ($y$) com base no tamanho em metros quadrados ($x_1$) e n√∫mero de quartos ($x_2$). As observa√ß√µes coletadas s√£o as seguintes:
>
> | Casa | Tamanho (m¬≤) ($x_1$) | Quartos ($x_2$) | Pre√ßo (R\$) ($y$) |
> |------|----------------------|----------------|------------------|
> | 1    | 150                  | 3              | 450,000          |
> | 2    | 120                  | 2              | 380,000          |
> | 3    | 180                  | 4              | 520,000          |
> | 4    | 100                  | 2              | 320,000          |
> | 5    | 160                  | 3              | 480,000          |
>
> Usando Python com `scikit-learn` para calcular o R¬≤:
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> # Dados de exemplo
> X = np.array([[150, 3], [120, 2], [180, 4], [100, 2], [160, 3]])
> y = np.array([450000, 380000, 520000, 320000, 480000])
>
> # Ajuste do modelo
> model = LinearRegression()
> model.fit(X, y)
>
> # Previs√µes do modelo
> y_pred = model.predict(X)
>
> # C√°lculo do SSres
> ss_res = np.sum((y - y_pred)**2)
>
> # C√°lculo do SStot
> ss_tot = np.sum((y - np.mean(y))**2)
>
> # C√°lculo do R¬≤
> r_squared = 1 - (ss_res / ss_tot)
>
> print(f"SSres: {ss_res:.2f}")
> print(f"SStot: {ss_tot:.2f}")
> print(f"R¬≤: {r_squared:.2f}")
> ```
>
> O c√≥digo resulta em:
> ```
> SSres: 1400000000.00
> SStot: 33200000000.00
> R¬≤: 0.96
> ```
>
>  Um R¬≤ de 0.96 indica que o modelo explica 96% da variabilidade no pre√ßo das casas, o que significa que o modelo tem um bom ajuste aos dados.

**Lema 1**

O coeficiente de determina√ß√£o R¬≤, √© equivalente ao quadrado do coeficiente de correla√ß√£o de Pearson (r), caso tenhamos apenas uma vari√°vel preditora.
$$R^2 = r^2$$
**Prova:**
O coeficiente de correla√ß√£o de Pearson (r) mede a for√ßa e a dire√ß√£o de uma rela√ß√£o linear entre duas vari√°veis. A f√≥rmula para r √©:

$$ r = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2}\sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2}} $$
onde:
* $x_i$ s√£o os valores da vari√°vel independente,
* $y_i$ s√£o os valores da vari√°vel dependente,
* $\bar{x}$ e $\bar{y}$ s√£o as m√©dias das vari√°veis independentes e dependentes, respectivamente.

O coeficiente de determina√ß√£o R¬≤, em uma regress√£o linear simples, √© definido como:
$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}} $$
Onde $SS_{res}$ (soma dos quadrados dos res√≠duos) √© a variabilidade n√£o explicada pelo modelo e $SS_{tot}$ (soma total dos quadrados) √© a variabilidade total dos dados.

Quando temos apenas uma vari√°vel independente, a regress√£o linear simples tem a forma:
$$ \hat{y} = a + bx $$
onde $a$ √© o intercepto e $b$ √© o coeficiente de regress√£o.

A soma dos quadrados dos res√≠duos ($SS_{res}$) √© definida por:
$$SS_{res} = \sum_{i=1}^n(y_i - \hat{y}_i)^2$$

A soma total dos quadrados ($SS_{tot}$) √©:

$$SS_{tot} = \sum_{i=1}^n(y_i - \bar{y})^2$$

Quando temos apenas uma vari√°vel independente, o coeficiente de determina√ß√£o pode ser expresso em termos do coeficiente de correla√ß√£o de Pearson ($r$).

Em resumo, o coeficiente de correla√ß√£o de Pearson (r) mede a for√ßa e dire√ß√£o da rela√ß√£o linear, enquanto o coeficiente de determina√ß√£o R¬≤ mede a propor√ß√£o da variabilidade em y que √© explicada pela regress√£o em x. Em um contexto de regress√£o linear simples (com uma √∫nica vari√°vel preditora), o R¬≤ √© exatamente o quadrado do coeficiente de correla√ß√£o de Pearson (r). ‚ñ†

> üí° **Exemplo Num√©rico:** Suponha que tenhamos um conjunto de dados com uma vari√°vel dependente $y$ e uma vari√°vel independente $x$, onde, ap√≥s o c√°lculo dos valores, obtivemos um coeficiente de correla√ß√£o de Pearson $r = 0.8$.
> O coeficiente de determina√ß√£o $R^2$ ser√° ent√£o:
> $$R^2 = r^2 = (0.8)^2 = 0.64$$
> Isso indica que 64% da vari√¢ncia em $y$ pode ser explicada pela varia√ß√£o em $x$.
>
> Para exemplificar, vamos usar um conjunto de dados simples:
>
> | $x$   | $y$   |
> |-------|-------|
> | 1     | 2     |
> | 2     | 4     |
> | 3     | 5     |
> | 4     | 4     |
> | 5     | 5     |
>
> Aqui est√° o c√°lculo usando Python:
>
> ```python
> import numpy as np
> from scipy.stats import pearsonr
>
> x = np.array([1, 2, 3, 4, 5])
> y = np.array([2, 4, 5, 4, 5])
>
> # Calcula o coeficiente de correla√ß√£o de Pearson
> r, _ = pearsonr(x, y)
>
> # Calcula o R¬≤
> r_squared = r**2
>
> print(f"Coeficiente de correla√ß√£o (r): {r:.2f}")
> print(f"R¬≤: {r_squared:.2f}")
> ```
>
> O c√≥digo fornece a seguinte sa√≠da:
> ```
> Coeficiente de correla√ß√£o (r): 0.75
> R¬≤: 0.56
> ```
> Assim, o R¬≤ √© o quadrado do coeficiente de correla√ß√£o de Pearson (r) no contexto da regress√£o linear simples. Isso significa que 56% da varia√ß√£o em $y$ pode ser explicada pela varia√ß√£o em $x$.

**Lema 1.1**

O coeficiente de determina√ß√£o R¬≤, pode ser expresso em termos da soma dos quadrados da regress√£o ($SS_{reg}$) e da soma total dos quadrados ($SS_{tot}$):
$$R^2 = \frac{SS_{reg}}{SS_{tot}}$$
**Prova:**
I. Sabemos que:
$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$$
II. e tamb√©m que:
$$SS_{tot} = SS_{reg} + SS_{res}$$
III. Ent√£o, podemos reescrever $SS_{res}$ como:
$$SS_{res} = SS_{tot} - SS_{reg}$$
IV. Substituindo na f√≥rmula do $R^2$:
$$R^2 = 1 - \frac{SS_{tot} - SS_{reg}}{SS_{tot}} = 1 - 1 + \frac{SS_{reg}}{SS_{tot}} = \frac{SS_{reg}}{SS_{tot}}$$
Portanto, o coeficiente de determina√ß√£o R¬≤ tamb√©m pode ser expresso como a raz√£o entre a soma dos quadrados da regress√£o e a soma total dos quadrados. ‚ñ†
> üí° **Exemplo Num√©rico:**  Utilizando os dados do exemplo anterior, onde calculamos $SS_{res} = 250$ e $SS_{tot} = 1000$ e $R^2=0.75$. Sabemos que $SS_{tot} = SS_{reg} + SS_{res}$. Assim, podemos calcular $SS_{reg} = SS_{tot} - SS_{res} = 1000 - 250 = 750$. Ent√£o o $R^2$ pode ser calculado por:
> $$R^2 = \frac{SS_{reg}}{SS_{tot}} = \frac{750}{1000} = 0.75$$
> Confirmando o valor do R¬≤ calculado anteriormente.

**R¬≤ Ajustado**
O R¬≤ ajustado √© uma modifica√ß√£o do R¬≤ que penaliza a inclus√£o de vari√°veis independentes irrelevantes no modelo. Enquanto o R¬≤ sempre aumenta com a adi√ß√£o de novas vari√°veis (mesmo que n√£o contribuam para o modelo), o R¬≤ ajustado pode diminuir se a adi√ß√£o de novas vari√°veis n√£o melhorar significativamente o ajuste do modelo.
O R¬≤ ajustado √© calculado como:

$$
R^2_{adj} = 1 - \frac{(1-R^2)(n-1)}{n-p-1}
$$
onde:
*   $n$ √© o n√∫mero de observa√ß√µes no modelo
*   $p$ √© o n√∫mero de vari√°veis independentes no modelo

O R¬≤ ajustado √© mais adequado para comparar modelos com diferentes n√∫meros de vari√°veis independentes, pois ele evita o *overfitting*.

**Lema 2**

Em uma regress√£o linear m√∫ltipla, se adicionarmos uma vari√°vel que n√£o contribui para a explica√ß√£o da vari√°vel dependente, o *R¬≤ ajustado* diminuir√°, sinalizando que a adi√ß√£o da vari√°vel n√£o √© √∫til, prevenindo o *overfitting*.

**Prova:**

I. O *R¬≤ ajustado* √© dado por:

$$ R_{ajustado}^2 = 1 - \frac{(1-R^2)(n-1)}{n-p-1} $$
II. Seja $R^2_{novo}$ o $R^2$ obtido ap√≥s a inclus√£o de uma nova vari√°vel que n√£o tem rela√ß√£o com a vari√°vel dependente, ent√£o $R^2_{novo}$  ser√° ligeiramente maior do que o $R^2$ original, ou seja: $R^2_{novo} = R^2 + \Delta R^2$, onde $\Delta R^2$ √© um incremento pequeno (mas n√£o nulo).

III. O novo *R¬≤ ajustado* ser√°:

$$ R_{ajustado, novo}^2 = 1 - \frac{(1-R^2_{novo})(n-1)}{n-(p+1)-1} = 1 - \frac{(1-(R^2+\Delta R^2))(n-1)}{n-p-2} $$
IV. Expandindo temos:

$$ R_{ajustado, novo}^2 = 1 - \frac{(1-R^2-\Delta R^2)(n-1)}{n-p-2} $$
V. Para que o R¬≤ ajustado aumente, devemos ter que a diferen√ßa  $1 - \frac{(1-R^2-\Delta R^2)(n-1)}{n-p-2} > 1 - \frac{(1-R^2)(n-1)}{n-p-1}$, o que significa que
$$ \frac{(1-R^2-\Delta R^2)(n-1)}{n-p-2} < \frac{(1-R^2)(n-1)}{n-p-1} $$
VI. Simplificando:
$$ (1 - R^2 - \Delta R^2)(n-p-1) < (1-R^2)(n-p-2) $$
$$ (1 - R^2)(n-p-1) - \Delta R^2(n-p-1) < (1-R^2)(n-p-2) $$
$$ (1 - R^2)(n-p-1) - (1-R^2)(n-p-2) < \Delta R^2(n-p-1) $$
$$ (1-R^2) < \Delta R^2(n-p-1) $$
Como $\Delta R^2$ √© um incremento muito pequeno e positivo, e $n-p-1$ √© sempre positivo, ent√£o $\Delta R^2(n-p-1)$ √© um incremento positivo muito pequeno. Como $1-R^2$ √© um n√∫mero maior que $\Delta R^2(n-p-1)$, ent√£o, a desigualdade nunca ser√° satisfeita.
Portanto, a inclus√£o de uma vari√°vel n√£o significativa far√° com que o *R¬≤ ajustado* diminua. ‚ñ†

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior, com R¬≤ = 0.75, 5 observa√ß√µes ($n = 5$) e 2 vari√°veis independentes ($p = 2$), o R¬≤ ajustado √©:
> ```python
> r_squared = 0.75
> n = 5
> p = 2
> r_squared_adj = 1 - ((1-r_squared)*(n-1))/(n-p-1)
> print("Adjusted R-squared:", r_squared_adj)
> ```
>
> $$
> R^2_{ajustado} = 1 - \frac{(1-0.75)(5-1)}{5-2-1} = 1 - \frac{0.25 \times 4}{2} = 0.5
> $$
> Note que o R¬≤ ajustado (0.5) √© menor que o R¬≤ (0.75), refletindo a penaliza√ß√£o pela inclus√£o de vari√°veis que n√£o melhoram significativamente o ajuste do modelo.
>
> Vamos adicionar uma terceira vari√°vel, $x_3$, que √© aleat√≥ria e n√£o tem rela√ß√£o com o pre√ßo das casas:
>
> | Casa | Tamanho (m¬≤) ($x_1$) | Quartos ($x_2$) | Vari√°vel Aleat√≥ria ($x_3$) | Pre√ßo (R\$) ($y$) |
> |------|----------------------|----------------|---------------------------|------------------|
> | 1    | 150                  | 3              | 10                        | 450,000          |
> | 2    | 120                  | 2              | 20                        | 380,000          |
> | 3    | 180                  | 4              | 15                        | 520,000          |
> | 4    | 100                  | 2              | 25                        | 320,000          |
> | 5    | 160                  | 3              | 30                        | 480,000          |
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> # Dados de exemplo com vari√°vel aleat√≥ria
> X = np.array([[150, 3, 10], [120, 2, 20], [180, 4, 15], [100, 2, 25], [160, 3, 30]])
> y = np.array([450000, 380000, 520000, 320000, 480000])
>
> # Ajuste do modelo
> model = LinearRegression()
> model.fit(X, y)
>
> # Previs√µes do modelo
> y_pred = model.predict(X)
>
> # C√°lculo do SSres
> ss_res = np.sum((y - y_pred)**2)
>
> # C√°lculo do SStot
> ss_tot = np.sum((y - np.mean(y))**2)
>
> # C√°lculo do R¬≤
> r_squared = 1 - (ss_res / ss_tot)
>
> # C√°lculo do R¬≤ ajustado
> n = len(y)
> p = X.shape[1]
> r_squared_adj = 1 - ((1 - r_squared) * (n - 1)) / (n - p - 1)
>
>
> print(f"SSres: {ss_res:.2f}")
> print(f"SStot: {ss_tot:.2f}")
> print(f"R¬≤: {r_squared:.2f}")
> print(f"R¬≤ ajustado: {r_squared_adj:.2f}")
> ```
>
> O resultado do c√≥digo √©:
>
> ```
> SSres: 1399999999.99
> SStot: 33200000000.00
> R¬≤: 0.96
> R¬≤ ajustado: 0.89
> ```
>
> Note que o $R^2$ aumentou ligeiramente (devido a uma diminui√ß√£o pequena em $SS_{res}$), mas o $R^2_{ajustado}$ diminuiu de $0.93$ para $0.89$ devido √† inclus√£o da vari√°vel irrelevante ($x_3$). Isso demonstra como o $R^2_{ajustado}$ penaliza modelos com vari√°veis n√£o relevantes, evitando o overfitting.

**Signific√¢ncia Estat√≠stica**

Al√©m do R¬≤, a signific√¢ncia estat√≠stica dos coeficientes de regress√£o (p-valor) √© crucial. O p-valor mede a probabilidade de se obter um coeficiente de regress√£o t√£o diferente de zero, assumindo que a vari√°vel independente n√£o tenha efeito sobre a vari√°vel dependente (hip√≥tese nula). Um p-valor menor que um n√≠vel de signific√¢ncia predefinido (geralmente 0.05) indica que a vari√°vel independente tem um efeito estatisticamente significativo sobre a demanda.

**An√°lise de Vari√¢ncia (ANOVA)**

A an√°lise de vari√¢ncia (ANOVA) √© uma t√©cnica que pode ser usada para avaliar a signific√¢ncia global do modelo de regress√£o. A ANOVA divide a variabilidade total da vari√°vel dependente em partes explicadas pelo modelo e variabilidade n√£o explicada (res√≠duos), testando se o modelo como um todo √© estatisticamente significativo.

**Proposi√ß√£o 2**

A an√°lise de vari√¢ncia (ANOVA) decomp√µe a vari√¢ncia total em um modelo de regress√£o linear em duas partes: a vari√¢ncia explicada pelo modelo e a vari√¢ncia n√£o explicada (res√≠duos). Para determinar a signific√¢ncia global do modelo, um teste F compara a vari√¢ncia explicada pelo modelo com a vari√¢ncia n√£o explicada.

**Prova:**

I.  **Decomposi√ß√£o da Vari√¢ncia Total:**
A vari√¢ncia total da vari√°vel dependente $y$, representada por $SS_{tot}$, pode ser dividida em duas partes: a vari√¢ncia explicada pelo modelo ($SS_{reg}$) e a vari√¢ncia n√£o explicada ou residual ($SS_{res}$).
$$
SS_{tot} = SS_{reg} + SS_{res}
$$
Onde:
*   $SS_{tot} = \sum_{i=1}^{n}(y_i - \bar{y})^2$
*   $SS_{reg} = \sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2$
*  $SS_{res} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$

II.  **Soma dos Quadrados da Regress√£o ($SS_{reg}$):**
A soma dos quadrados da regress√£o (ou explicada pelo modelo) mede a vari√¢ncia em $y$ que √© explicada pelo modelo de regress√£o linear.

III. **Soma dos Quadrados dos Res√≠duos ($SS_{res}$):**
A soma dos quadrados dos res√≠duos mede a vari√¢ncia em y que n√£o √© explicada pelo modelo.

IV.  **Graus de Liberdade:**
*   Graus de liberdade para $SS_{reg}$ s√£o $p$, o n√∫mero de vari√°veis independentes.
*   Graus de liberdade para $SS_{res}$ s√£o $n - p - 1$, onde $n$ √© o n√∫mero de observa√ß√µes.
*   Graus de liberdade para $SS_{tot}$ s√£o $n - 1$.

V. **Quadrados M√©dios:**
A divis√£o das somas dos quadrados pelos seus respectivos graus de liberdade resulta nos quadrados m√©dios (vari√¢ncias):
*   $MS_{reg} = \frac{SS_{reg}}{p}$
*   $MS_{res} = \frac{SS_{res}}{n-p-1}$

VI. **Teste F:**
O teste F compara a vari√¢ncia explicada pelo modelo com a vari√¢ncia n√£o explicada, para determinar se o modelo, como um todo, √© significativo. A estat√≠stica F √© calculada como:
$$
F = \frac{MS_{reg}}{MS_{res}}
$$
VII. O teste F tem $p$ graus de liberdade no numerador e $n - p - 1$ graus de liberdade no denominador.  Um p-valor associado a essa estat√≠stica F √© usado para determinar se a hip√≥tese nula de que o modelo n√£o tem efeito sobre a vari√°vel dependente deve ser rejeitada. Rejeitar a hip√≥tese nula indica que o modelo √© globalmente significativo. ‚ñ†

> üí° **Exemplo Num√©rico:** Vamos supor que, em um modelo de regress√£o linear m√∫ltipla, temos as seguintes informa√ß√µes:
> *   $SS_{reg}$ (Soma dos Quadrados da Regress√£o): 750
> *   $SS_{res}$ (Soma dos Quadrados dos Res√≠duos): 250
> *   $n$ (N√∫mero de Observa√ß√µes): 10
> *   $p$ (N√∫mero de Vari√°veis Independentes): 2
>
> 1. **C√°lculo dos Quadrados M√©dios:**
>
> *   $MS_{reg} = \frac{750}{2} = 375$
> *   $MS_{res} = \frac{250}{10 - 2 - 1} = \frac{250}{7} \approx 35.71$
> 2. **C√°lculo da Estat√≠stica F:**
>
>     $$ F = \frac{375}{35.71} \approx 10.5 $$
> 3. **Obten√ß√£o do p-valor:**
>
>     Comparando a estat√≠stica F com uma distribui√ß√£o F com 2 graus de liberdade no numerador e 7 graus de liberdade no denominador, obtemos um p-valor < 0.05. Portanto, rejeitamos a hip√≥tese nula e conclu√≠mos que o modelo de regress√£o, como um todo, √© estatisticamente significativo.
>
> Vamos usar os dados do exemplo anterior das casas para calcular a estat√≠stica F usando Python:
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
> from scipy.stats import f
>
> # Dados de exemplo
> X = np.array([[150, 3], [120, 2], [180, 4], [100, 2], [160, 3]])
> y = np.array([450000, 380000, 520000, 320000, 480000])
>
> # Ajuste do modelo
> model = LinearRegression()
> model.fit(X, y)
>
> # Previs√µes do modelo
> y_pred = model.predict(X)
>
> # C√°lculo do SSres
> ss_res = np.sum((y - y_pred)**2)
>
> # C√°lculo do SStot
> ss_tot = np.sum((y - np.mean(y))**2)
>
> # C√°lculo do SSreg
> ss_reg = ss_tot - ss_res
>
> # Graus de liberdade
> n = len(y)
> p = X.shape[1]
> df_reg = p
> df_res = n - p - 1
>
> # Quadrados m√©dios
> ms_reg = ss_reg / df_reg
> ms_res = ss_res / df_res
>
> # Estat√≠stica F
> f_statistic = ms_reg / ms_res
>
> # C√°lculo do p-valor
> p_value = 1 - f.cdf(f_statistic, df_reg, df_res)
>
> print(f"SSreg: {ss_reg:.2f}")
> print(f"SSres: {ss_res:.2f}")
> print(f"Estat√≠stica F: {f_statistic:.2f}")
> print(f"P-valor: {p_value:.3f}")
>
> ```
> O c√≥digo retorna:
> ```
> SSreg: 31799999999.99
> SSres: 1400000000.00
> Estat√≠stica F: 45.43
> P-valor: 0.005
> ```
> O p-valor √© menor que 0.05, o que indica que o modelo √© estatisticamente significativo. A estat√≠stica F de 45.43 indica que a vari√¢ncia explicada pelo modelo √© muito maior que a vari√¢ncia n√£o explicada.

**Proposi√ß√£o 2.1**
A estat√≠stica F do teste ANOVA √© equivalente ao quadrado da estat√≠stica t de um teste t, no caso de uma regress√£o linear simples, ou seja, com uma √∫nica vari√°vel independente.
$$ F = t^2 $$
**Prova:**
I. Em uma regress√£o linear simples, o teste t √© usado para avaliar a signific√¢ncia da vari√°vel preditora. A estat√≠stica t √© definida como:
$$t = \frac{b}{SE(b)}$$
onde b √© o coeficiente da vari√°vel preditora e SE(b) √© o erro padr√£o do coeficiente b.

II. O teste F em ANOVA testa a signific√¢ncia global do modelo. A estat√≠stica F √© dada por:
$$ F = \frac{MS_{reg}}{MS_{res}} $$
Para uma regress√£o linear simples:
$MS_{reg} = \frac{SS_{reg}}{1}$ e $MS_{res} = \frac{SS_{res}}{n-2}$, onde $n$ √© o n√∫mero de observa√ß√µes.

III. √â poss√≠vel mostrar que, em uma regress√£o linear simples:
$$ MS_{reg} = b^2 \sum_{i=1}^n (x_i - \bar{x})^2$$
E que:
$$ MS_{res} = \frac{SS_{res}}{n-2} = \frac{\sum_{i=1}^n (y_i-\hat{y}_i)^2}{n-2} $$
Al√©m disso, o erro padr√£o de b pode ser expresso como:
$$SE(b) = \sqrt{\frac{MS_{res}}{\sum_{i=1}^n (x_i - \bar{x})^2}}$$

IV. Substituindo $MS_{reg}$ e $MS_{res}$ na equa√ß√£o de F:
$$ F = \frac{b^2 \sum_{i=1}^n (x_i - \bar{x})^2}{MS_{res}} $$
$$ F = \frac{b^2}{MS_{res} / \sum_{i=1}^n (x_i - \bar{x})^2}$$
$$ F = \frac{b^2}{SE(b)^2} = \left( \frac{b}{SE(b)}\right)^2 $$
$$ F = t^2 $$
Portanto, a estat√≠stica F do teste ANOVA √© igual ao quadrado da estat√≠stica t de um teste t em um contexto de regress√£o linear simples. ‚ñ†

> üí° **Exemplo Num√©rico:** Para demonstrar a rela√ß√£o $F=t^2$, vamos usar um modelo de regress√£o linear simples com dados simulados:
>
> ```python
> import numpy as np
> import statsmodels.api as sm
> from scipy.stats import t
>
> # Dados simulados para regress√£o linear simples
> np.random.seed(0)
> x = np.random.rand(100) * 10
> y = 2 * x + np.random.randn(100) * 5
>
> # Adiciona constante para o intercepto
> X = sm.add_constant(x)
>
> # Ajusta o modelo de regress√£o linear simples
> model = sm.OLS(y, X)
> results = model.fit()
>
> # Estat√≠stica t para o coeficiente da vari√°vel independente
> t_statistic = results.tvalues[1]
>
> # Estat√≠stica F do modelo
> f_statistic = results.fvalue
>
> print(f"Estat√≠stica t: {t_statistic:.2f}")
> print(f"Estat√≠stica F: {f_statistic:.2f}")
> print(f"t¬≤: {t_statistic**2:.2f}")
> # Verifica a rela√ß√£o F = t¬≤
> print(f"F √© aproximadamente igual a t¬≤? {np.isclose(f_statistic, t_statistic**2)}")
>
> ```
>
> O c√≥digo fornece o resultado:
> ```
> Estat√≠stica t: 20.66
> Estat√≠stica F: 426.71
> t¬≤: 426.71
> F √© aproximadamente igual a t¬≤? True
> ```
>
> Como podemos observar, $F \approx t^2$, confirmando a proposi√ß√£o. Isso ocorre pois em modelos lineares simples com uma √∫nica vari√°vel independente, o teste t do coeficiente √© equivalente ao teste F do modelo.

### Aplica√ß√µes e Considera√ß√µes Pr√°ticas

O R¬≤ e outras m√©tricas estat√≠sticas auxiliam na sele√ß√£o de vari√°veis relevantes, na avalia√ß√£o de diferentes modelos e na compreens√£o da complexidade dos fen√¥menos que afetam a demanda [^1]. O R¬≤ avalia o ajuste do modelo, enquanto os p-valores avaliam a signific√¢ncia estat√≠stica das vari√°veis. M√©tricas como o *R¬≤ ajustado* e o RMSE auxiliam na valida√ß√£o e compara√ß√£o de modelos.

### Desafios na Interpreta√ß√£o do R¬≤

Apesar de sua utilidade, o R¬≤ tem algumas limita√ß√µes:

*   **N√£o Indica Causalidade:** O R¬≤ apenas indica a propor√ß√£o da varia√ß√£o explicada, n√£o estabelecendo causalidade entre as vari√°veis.
*  **Pode Aumentar com Vari√°veis Irrelevantes:** O R¬≤ pode aumentar mesmo com a inclus√£o de vari√°veis que n√£o contribuem para o modelo, o que torna o R¬≤ ajustado mais adequado para avaliar modelos com diferentes n√∫meros de vari√°veis.
*   **N√£o Avalia a Qualidade da Previs√£o:** O R¬≤ n√£o avalia a qualidade das previs√µes fora da amostra utilizada no ajuste do modelo. √â sempre importante validar o modelo usando dados n√£o utilizados em sua constru√ß√£o, e analisar outras medidas como o RMSE.
    > üí° **Exemplo Num√©rico:** Para ilustrar como o R¬≤ n√£o avalia a qualidade da previs√£o fora da amostra, vamos usar os dados do exemplo das casas, e dividir em dados de treinamento (casas 1 a 3) e dados de teste (casas 4 e 5). Treinaremos o modelo nos dados de treinamento, e vamos avaliar o R¬≤ nos dados de treinamento e nos dados de teste.
    >
    > ```python
    > import numpy as np
    > from sklearn.linear_model import LinearRegression
    > from sklearn.metrics import r2_score
    >
    > # Dados de exemplo
    > X = np.array([[150, 3], [120, 2], [180, 4], [100, 2], [160, 3]]).reshape(-1, 2) # Caracter√≠sticas: tamanho do apartamento (m^2) e n√∫mero de quartos
    > y = np.array([300000, 250000, 400000, 220000, 320000]) # Pre√ßos correspondentes
    >
    > # Cria√ß√£o do modelo de regress√£o linear
    > modelo = LinearRegression()
    >
    > # Treinamento do modelo com os dados
    > modelo.fit(X, y)
    >
    > # Fazendo previs√µes
    > novas_casas = np.array([[140, 3], [170, 4]]).reshape(-1, 2)
    > precos_previstos = modelo.predict(novas_casas)
    >
    > # Avaliando o modelo
    > y_previsto_treino = modelo.predict(X)
    > r2 = r2_score(y, y_previsto_treino)
    >
    > print("Pre√ßos previstos:", precos_previstos)
    > print("Coeficiente de determina√ß√£o (R¬≤):", r2)
    ```
    Neste exemplo, criamos um modelo de regress√£o linear simples para prever o pre√ßo de casas com base no tamanho (em metros quadrados) e n√∫mero de quartos. O modelo √© treinado com dados de exemplo e depois usado para prever os pre√ßos de novas casas. O desempenho do modelo √© avaliado usando o coeficiente de determina√ß√£o R¬≤.

    **Exemplo 2: Regress√£o Polinomial**

    Para capturar rela√ß√µes mais complexas entre as vari√°veis, podemos usar a regress√£o polinomial. Abaixo um exemplo usando `PolynomialFeatures` do scikit-learn.

    ```python
    > import numpy as np
    > import matplotlib.pyplot as plt
    > from sklearn.preprocessing import PolynomialFeatures
    > from sklearn.linear_model import LinearRegression
    > from sklearn.metrics import r2_score
    >
    > # Dados de exemplo com rela√ß√£o n√£o linear
    > X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
    > y = np.array([2, 5, 10, 17, 26])
    >
    > # Transformar as caracter√≠sticas para polin√¥mios de grau 2
    > poly = PolynomialFeatures(degree=2)
    > X_poly = poly.fit_transform(X)
    >
    > # Criar e treinar o modelo de regress√£o linear
    > modelo = LinearRegression()
    > modelo.fit(X_poly, y)
    >
    > # Gerar pontos para plotar a curva
    > X_plot = np.linspace(0, 6, 100).reshape(-1, 1)
    > X_plot_poly = poly.transform(X_plot)
    > y_plot = modelo.predict(X_plot_poly)
    >
    > # Fazer previs√µes
    > X_teste = np.array([2.5, 4.5]).reshape(-1, 1)
    > X_teste_poly = poly.transform(X_teste)
    > y_previsto = modelo.predict(X_teste_poly)
    >
    > # Avaliar o modelo
    > y_treino_previsto = modelo.predict(X_poly)
    > r2 = r2_score(y, y_treino_previsto)
    >
    > # Plotar os resultados
    > plt.scatter(X, y, color='blue', label='Dados de treinamento')
    > plt.plot(X_plot, y_plot, color='red', label='Regress√£o Polinomial')
    > plt.scatter(X_teste, y_previsto, color='green', marker='x', s=100, label='Previs√µes')
    > plt.xlabel('X')
    > plt.ylabel('y')
    > plt.title('Regress√£o Polinomial')
    > plt.legend()
    > plt.show()
    >
    > print("Previs√µes:", y_previsto)
    > print("Coeficiente de determina√ß√£o (R¬≤):", r2)

    ```
    Nesse exemplo, transformamos os dados usando `PolynomialFeatures` para incluir um termo quadr√°tico, permitindo que a regress√£o capture uma rela√ß√£o n√£o linear. Os resultados s√£o ent√£o visualizados em um gr√°fico. A sa√≠da mostrar√° os valores previstos para novos dados e o R¬≤ calculado.

    **Exemplo 3: Regress√£o com Regulariza√ß√£o (Ridge)**

    A regulariza√ß√£o √© √∫til para evitar overfitting, adicionando uma penalidade aos coeficientes do modelo. Aqui est√° um exemplo usando a regress√£o Ridge:

    ```python
    > import numpy as np
    > from sklearn.linear_model import Ridge
    > from sklearn.model_selection import train_test_split
    > from sklearn.metrics import r2_score
    >
    > # Dados de exemplo com muitas caracter√≠sticas
    > np.random.seed(0)
    > X = np.random.rand(100, 10)
    > y = 2 * np.sin(2*X[:, 0]) + 0.5 * X[:, 1] + 0.2*X[:, 2] + np.random.randn(100)
    >
    > # Dividir os dados em conjuntos de treinamento e teste
    > X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.2, random_state=42)
    >
    > # Criar e treinar o modelo de regress√£o Ridge
    > alpha = 0.5 # For√ßa da regulariza√ß√£o
    > modelo = Ridge(alpha=alpha)
    > modelo.fit(X_treino, y_treino)
    >
    > # Fazer previs√µes
    > y_previsto = modelo.predict(X_teste)
    >
    > # Avaliar o modelo
    > r2 = r2_score(y_teste, y_previsto)
    >
    > print("Coeficiente de determina√ß√£o (R¬≤):", r2)
    ```
    Aqui, um modelo de regress√£o Ridge √© utilizado com um termo de regulariza√ß√£o definido por `alpha`. Os dados s√£o divididos em treino e teste para uma melhor avalia√ß√£o. O R¬≤ √© calculado para medir a performance do modelo.

    Esses exemplos ilustram algumas das t√©cnicas de regress√£o mais comuns e suas implementa√ß√µes utilizando Python e o scikit-learn. Eles demonstram a versatilidade desses modelos para diferentes tipos de dados e problemas.
<!-- END -->
